DO CONTEMPORARY CATE MODELS
CAPTURE REAL-WORLD HETEROGENEITY?
FINDINGS FROM A LARGE-SCALE BENCHMARKS
HainingYu YizhouSun
Amazon DepartmentofComputerScience,UCLA
hainiy@amazon.com yzsun@cs.ucla.edu
ABSTRACT
Wepresentunexpectedfindingsfromalarge-scalebenchmarkstudyevaluating
ConditionalAverageTreatmentEffect(CATE)estimationalgorithms. Byrunning
16modernCATEmodelsacross43,200datasets,wefindthat: (a)62%ofCATEes-
timateshaveahigherMeanSquaredError(MSE)thanatrivialzero-effectpredictor,
renderingthemineffective;(b)indatasetswithatleastoneusefulCATEestimate,
80%stillhavehigherMSEthanaconstant-effectmodel;and(c)Orthogonality-
basedmodelsoutperformothermodelsonly30%ofthetime,despitewidespread
optimismabouttheirperformance. Thesefindingsexposesignificantlimitationsin
currentCATEmodelsandsuggestampleopportunitiesforfurtherresearch.
Ourfindingsstemfromanovelapplicationofobservationalsampling,originally
developedtoevaluateAverageTreatmentEffect(ATE)estimatesfromobserva-
tionalmethodswithexperimentdata. ToadaptobservationalsamplingforCATE
evaluation,weintroduceastatisticalparameter,Q,equaltoMSEminusaconstant
and preserves the ranking of models by their MSE. We then derive a family of
samplestatistics,collectivelycalledQˆ,thatcanbecomputedfromreal-worlddata.
WeprovethatQˆ isaconsistentestimatorofQundermildtechnicalconditions.
Whenusedinobservationalsampling,Qˆ isunbiasedandasymptoticallyselects
the model withthe smallest MSE. To ensure the benchmarkreflects real-world
heterogeneity,wehandpickdatasetswhereoutcomescomefromfieldratherthan
simulation. Bycombiningthenewobservationalsamplingmethod,newstatistics,
andreal-worlddatasets,thebenchmarkprovidesauniqueperspectiveonCATE
estimatorperformanceanduncovergapsincapturingreal-worldheterogeneity.
1 INTRODUCTION
ConditionalAverageTreatmentEffect(CATE)modelsareincreasinglyusedtoanswercausalinfer-
encequestionsinfieldssuchasmedicine,economics,andpolicy. Buthowwelldothesemodels
capturereal-worldheterogeneity? Wepresentunexpectedfindingsfromalarge-scalebenchmark
studyoncontemporaryCATEestimationalgorithms. Basedon43,200datasetsand16CATEmodels,
wefind: (a)62%ofCATEestimateshaveahigherMeanSquaredError(MSE)thanatrivialesti-
matorthatconsistentlypredictszeroeffect,renderingthemineffective;(b)incaseswhereatleast
oneusefulCATEestimateexists,80%havehigherMSEthanaconstant-effectestimator;and(c)
orthogonality-basedmodelsoutperformothermodelsonly30%ofthetime. Thesefindingsraise
importantquestionsaboutthecurrentmodels’abilitytofullyreflectthecomplexitiesofreal-world
heterogeneity.
Rather than introducing new models, our benchmark study focuses on evaluating current CATE
estimationmodels. ThepastdecadehasseensignificantadvancementsinCATEestimation,with
new methods emerging from statistics, econometrics, and machine learning (see Chernozhukov
etal.(2017);Athey&Imbens(2016);Kennedy(2023);Shalitetal.(2017);Alaa&vanderSchaar
(2017);Chernozhukovetal.(2023);Künzeletal.(2019)). Widelyavailableandeasy-to-usetools
likeEconML(Battocchietal.,2019)andDoubleML(Bachetal.,2022)havemadeCATEmodels
1
4202
tcO
9
]LM.tats[
1v12070.0142:viXraaccessibletouserswithminimalexpertise,leadingtotheirbroadapplicationinhigh-stakesbusiness
andscientificdecisions,whereaccuracyiscritical.
Understandingreal-worldmodelaccuracyischallengingbecauseCATEestimationmodelslackaccess
togroundtruthCATE.Tocompensate,thesemodelsrelyonestimatesofpotentialoutcomesand/or
propensity,knownasnuisancefunctions. Asaresult,modelsfacetwokeyrisks-inaccuratepotential
outcomeestimationandinaccuratepropensityestimation-forcingdifficulttrade-offs.Whenpotential
outcomeriskapproacheszero,thegroundtruthCATEestimatecanberecovered,whichisthecore
ideabehindS-learnerandT-learnermodels(Künzeletal.,2019). Conversely,whenpropensityis
known, theHorvitz-Thompsonestimator(Horvitz&Thompson,1952)providesunbiasedCATE
estimates. MostcontemporaryCATEmodelsattemptahybridapproach,estimatingbothpotential
outcomes and propensities to generate the final CATE estimate. The error of nuisance estimates
impactstheaccuracyofCATEestimates. Tominimizetheeffectoferrors,modernCATEmodels
employlossfunctionswithrobustnessguarantees,ensuringthaterrorsinnuisanceestimatesdonot
haveafirst-ordereffectontheCATEestimate. Suchguaranteescomefromafamilyofcloselyrelated
theories,includingbutnotlimitedtodoublyrobustness(Kennedy,2023),NeymanOrthogonality
(Chernozhukovetal.,2017;Nie&Wager,2020;Foster&Syrgkanis,2023),andInfluencefunction
(Alaa&VanDerSchaar,2019). Werefertosuchmodelsfromthesetheoriesasorthogonality-based
models. Thesetheoriesrelyonstringentassumptionsregardingsmoothness,Lipschitzcontinuity,
sparsity,convexity,andorthogonalityofthetruepotentialoutcomes,propensities,andlossfunctions.
Whilemathematicallyelegant,theireffectivenessistypicallyvalidatedusingsimulationdata.
Despitetheoreticalguarantees,evaluatingCATEmodelsinpracticeremainsasignificantchallenge.
CATEmodelevaluationmethodsaimtoassesstheaccuracyofCATEestimationmodelsbutencounter
thesamechallengesasthemodelstheyevaluate. Ideally,wewouldcomputetheMSEforCATE
estimatesandrankestimatorsbytheirMSErelativetothegroundtruthCATE,aprocessknownas
oracleranking. However,inmostreal-worlddatasets,onlythefactualoutcomeisobserved,notthe
counterfactual,makingthegroundtruthCATEunknownunlessthecounterfactualgenerationprocess
isexplicitlyknown. Itiswidelyacceptedthatwithoutobservingbothfactualandcounterfactual
outcomes,groundtruthCATEcannotbecomputed,makingitdifficulttoselectthemostaccurate
CATE estimators (Curth & van der Schaar, 2021; 2023; Neal et al., 2021; Mahajan et al., 2023).
Without understanding CATE estimate accuracy, users cannot effectively evaluate the quality of
estimatorsortheriskofinaccurateestimates.
ToaddresschallengeofevaluatingCATEmodelswithoutgroundtruthCATE,twomainapproaches
areused,eachwithdrawbacks. ThefirstapproachsimulatespotentialoutcomesandranksCATE
estimatorsbytheirMSEonsemi-syntheticdatasets,effectivelyremovingtheriskofpotentialoutcome
estimation(seeHill(2011);Shalitetal.(2017);Künzeletal.(2019);Diemertetal.(2021)). But
doubtsremainaboutwhetherpromisingsimulationresultstranslatetoequallypromisingoutcomes
inreal-worldcases. ThesecondapproachranksCATEestimatorsusingproxylossfunctions(see
detailsinSection2),particularlythosewithdoublyrobustorNeymanorthogonalproperties. This
approachattemptstomanagebothpotentialoutcomeandpropensityestimationrisksbyexploiting
orthogonalitypropertiesintheloss.Doubtsremainaboutproxylossfunctions,particularlyconcerning
theirstringentassumptionrequirement,finitesampleproperty,andself-servingbias(Curth&vander
Schaar,2023). ThelastbiasoccurswhenaCATEestimatorisevaluatedusingalossfunctionsharing
commonassumptions. Forexample,ifweuseR-losstoscoreCATEestimatorsandfindR-learner
(whichoptimizesR-loss)performsbest,wecannotdeterminewhetherR-learnerhasindeedthelowest
MSEorsimplysharesassumptionswithR-loss. Thissituationisanalogoustoasportsplayeralso
actingasthereferee. Notethat,thissituationisuniquetocausalinferencewheregroundtruthis
missingintestdataset,makingitimpossibletocomputeMSEtherelikeonewoulddoinsupervised
learning. To summarize, from a risk perspective, current CATE evaluation methods either try to
removepotentialoutcomeestimationrisk,ormanagebothoutcomeestimationandpropensityrisks.
Meanwhile,fewworkexploresremovingtheriskofpropensityestimation.
GiventhelimitationsofcurrentCATEevaluationmethods,weseeknewapproachesthatcanassess
CATEestimatorperformanceonreal-worldheterogeneousdatawhilerelyingonfewerandsimpler
assumptions. Drawingfromtheriskdiscussion, weask: couldeliminatingtheriskofpropensity
estimation be the solution? At first glance, this seems counter-intuitive. Observational methods
inherentlyworkwithunknownpropensity,thustheriskcannotberemoved. Thisiswherethemethod
ofobservationalsamplingcomesin(LaLonde,1986;Gentzeletal.,2021). Inobservationalsampling,
anobservationaldatasetiscreatedbysamplingfromanRandomizedControlledTrial(RCT)dataset
2throughacarefullydesignedprocessthatintroducesselectionbias. ThisapproachallowsCATE
estimatorstobetrainedontheobservationalsub-sample(withpropensityestimationrisk)whiletheir
performanceisevaluatedusingthefullRCTdata(withoutpropensityestimationrisk).
The history of observational sampling dates back as long as the field of causal inference itself.
Forinstance,LaLonde(1986)usedittoconstructtheIHDPdatasetforevaluatingATEestimates
from observational data. Since then, large RCT datasets have become more available in certain
domains (Gordon et al., 2019; 2022). However, few researchers (except Gentzel et al. (2021))
exploreobservationalsamplingforCATEevaluation,asmainstreamresearchoftenreliesonsmall
semi-syntheticdatasets,whichhavethedrawbacksdiscussedearlier.
Recognizingtheuntappedpotentialofobservationalsampling,wehypothesizethatitoffersoppor-
tunitiestodevelopnewstatisticsforidentifyingthemostaccurateCATEestimatorsandassessing
theirabilitytocapturereal-worldheterogeneity. Thisformsthecentralhypothesisofourresearch.
Torigorouslytestthis,weaimtodevelopnewtheoreticalresultsandcreateabenchmarkprocedure
usingobservationalsamplingforCATEevaluation.
ThispaperoffersthreekeycontributionstothefieldofCATEevaluation:
1. BenchmarkFindings: Ourprimarycontributionisnewfindingsfromthelarge-scalebenchmark
study,evaluatingsixteencontemporaryCATEestimationmethodsacross43,200testdatasetswithreal-
worldheterogeneity. Asnotedintheopeningparagraph,thesefindingsrevealsignificantlimitations
incurrentCATEmodels,particularlyincapturingreal-worldheterogeneity. Theyhighlighttheneed
forfurtherresearchandimprovementsinthisarea.
2. NewEvaluationMetrics: OursecondcontributionisnewCATEevaluationmetrics. Wedefinea
newstatisticalparameter,Q,whichequalsMSEminusaconstant,anddevelopafamilyofstatistics,
collectively called Qˆ, that converge to Q. We prove that Qˆ converges in probability to Q under
mildtechnicalconditions. ForRCTdata,wefurtherdemonstratethatQˆ isunbiasedandachieves
√
a O(1/ N) asymptotic convergence rate. Additionally, we introduce a control-variates-based
framework to reduce the variance of Qˆ, showing that common CATE estimation losses, such as
R-lossandDR-loss,arespecialcasesofthisframework.
3. NovelEvaluationProcedure: OurthirdcontributionisanovelCATEevaluationprocedurebased
onobservationalsamplingandthenewlydevelopedQ. ThismethodallowsforthetrainingofCATE
estimatorsonobservationalsub-samplesandevaluatestheirperformanceusingQˆ onthefullRCT
dataset. Unlikepreviousbenchmarks,ourapproachdoesnotrelyonsimulatedpotentialoutcomes,
addressingconcernsaboutreal-worldheterogeneityandmitigatingtheriskofself-servingbias.
2 PRELIMINARY AND RELATED WORK
2.1 PRELIMINARIES
We formalize our problem setting using the potential outcomes framework Rubin (2005). All
notations can be found in Table 2 in Appendix A. Let (X,T,Y) be a tuple of random variables
following distribution Π, where X ∈ X is the pre-treatment covariates, Y ∈ R is the observed
outcome,andT ∈ {0,1}isthetreatmentassignment. Eachtupleisassociatedwithtwopotential
outcomesY(0)andY(1). However,weobserveonlytheoutcomeassociatedtothefactualtreatment
T ∈ {0,1},Y = Y(T). Wedenoteµ(0)(x) = E[Y(0)|X = x]andµ(1)(x) = E[Y(1)|X = x]as
theexpectedpotentialoutcomefunctionsgiventhecovariatex,ande(x) = Pr(T = 1|X = x)as
thetreatmentpropensityfunction. TheConditionalAverageTreatmentEffect(CATE)isthendefined
as: τ(x) = E[Y(1)−Y(0)|X = x] = µ(1)(x)−µ(0)(x). TheAverageTreatmentEffect(ATE)is
thenτ =E [τ(X)].
ATE X
Let D denote a dataset with N i.i.d samples {(x ,t ,y )} drawn from Π. When the treatment
n n n
assignmentisindependentofcovariates,i.e.,T ⊥X,wecallsuchdatasetaRCTdataset. OnanRCT
dataset,wedefinetheconstanttreatmentpropensityE =e(x)=Pr(T =1|X =x)=Pr(T =1)
1
andE =1−E .
0 1
ThegoalofCATEestimationistotrainaCATEestimatorτˆ(x)usinganobservationaldatasetthat
approximatesτ(x)asmuchaspossible. GivenatrainedCATEestimatorτˆ(·) : X → R,thegoal
3ofCATEevaluationistoevaluatethequalityofτˆ(·)bycomparingittoτ(·). Onecommonlyused
evaluationcriterionisitsMSE,alsoknownasPrecisioninEstimatingHeterogeneousEffects(PEHE)
(Hill,2011): P(τˆ(·)) = E [(τ(X)−τˆ(X))2],whichisafunctionalthatmapsanestimatorτˆto
X
anon-negativerealnumber. Notethat,MSEcanbecalculatedonlywhenτ(·)isavailable,which
requires the observation of both factual and counterfactual outcomes. To ensure that effects are
identifiablefromobservationaldata,werelyonthestandardignorabilityassumptions(Rosenbaum&
Rubin,1983):
Assumption 2.1. (i) Consistency: for a sample with treatment assignment T, we observe the
associated potential outcome, i.e. Y = Y(T). (ii) Unconfoundedness: there are no unobserved
confounders,sothatY(0),Y(1)⊥T|X. (iii)Overlap: treatmentassignmentisnon-deterministic,
i.e.,0<Pr(T =1|X =x)<1.
2.2 RELATEDWORK
CATEestimation.ThereexistmanymethodstoconstructCATEestimatorτˆ(x).Herewecoverthree
mostpopularstrategies. Thefirstoutcomepredictionstrategypredictspotentialoutcomesµ(0)(x)
andµ(1)(x),andusestheirdifferenceastheCATEestimate,i.e.,τˆ(x)=µ˜(1)(x)−µ˜(0)(x). 1 This
approach essentially minimizes L OP(µ˜(0),µ˜(1)) = N1 (cid:80) n(cid:0) y n−µ˜(tn)(x n)(cid:1)2 by any regression
model. ExamplesincludeS-learner,whichregressY onX andT,andT-learner,whichregressY
onX forT =0andT =1separately(Künzeletal.,2019). Solvingtheminimizationproblemof
argmin L yieldstheestimatorofτˆ (x)=µ˜(1)(x)−µ˜(0)(x).Forobservationaldatasets,
µ˜(0),µ˜(1) OP OP
learningasharedrepresentationϕ(x)forbothtreatmentgroupscanimproveCATEestimates.Thisap-
proachregressesY onϕ(X)toestimatepotentialoutcomes;Johanssonetal.(2018)providesbounds
ongeneralizationerror. Dragonnet(Shietal.,2019),avariationofthisapproach,learnstherepresen-
tationsofµ(0)(ϕ(x)),µ(1)(ϕ(x)),ande(ϕ(x))usingathree-headneuralnetwork. Thelossfunction
(cid:104) (cid:105)
fordragonnetisL (µ˜(0),µ˜(1))= 1 (cid:80) (y −µ˜(t ,ϕ(x )))2+λBCE(t ,e˜(ϕ(x ))) . Solving
RL N n n n n n n
theproblemofargmin L yieldtheestimatorτˆ (x)=µ˜(1)(ϕ(x))−µ˜(0)(ϕ(x)).
ϕ,µ˜(0),µ˜(1),e˜ RL RL
Thesecondsemi-parametricregressionstrategyestimatesCATEbasedontransformedoutcomes.
When the true model is Y = f(X) + Tτ(X) + ϵ, it can be rewritten as Y − m(X) = (T −
e(X))τ(X)+ϵwherem(x)=E[Y|X =x]=f(x)+e(x)τ(x). Thisreformulationthenestimates
τ(x)byregressingtransformedoutcomeY −m(X)ontransformedcovariateT −e(X). Robinson
√
(1988)andsubsequentworkshowthattheseestimatesare N-consistent. Historically,thisapproach
carried different names such as residual-on-residual, partialling-out estimators, Double Machine
Learning(Chernozhukovetal.,2017),andNeymanorthogonality(Newey,1994),tonameafew.
(cid:104) (cid:105)
Initssimplestform,thelossfunctionisL (τˆ)= 1 (cid:80) ((y −m˜(x ))−(t −e˜(x ))τˆ(x ))2
R N n n n n n n
with plug-in estimates m˜(x) and e˜(x); this is called R-loss in Nie & Wager (2020). Minimizing
L yieldstheestimatorτˆ (x). Anotableextensionofthismethodiscausalforests(Atheyetal.,
R R
2018),whichadaptivelypartitionthedatatomaximizethedifferencebetweenCATEestimatesfrom
differenttreepartitions,improvingtheaccuracyoftheestimates.
ThethirdapproachisInverse-PropensityWeighting(IPW).IPWisbasedontheHorvitz-Thompson
(cid:16) (cid:17)
estimator, defined as η(x,t,y) = t − (1−t) y. When the propensity score e(x) is
e(x) 1−e(x)
known, this estimator is an unbiased estimate of τ(x) (Horvitz & Thompson, 1952). That
is, E[η(X,T,Y)|X = x] = τ(x). However, IPW is known to have high variance (Robins
et al., 1994). To reduce variance, Kennedy (2023) suggests constructing doubly robust loss
(cid:16) (cid:17)
L (τˆ) = 1 (cid:80) [η(x ,t ,y )+γ(x ,t )−τˆ(x )]2, where γ(x,t) = 1− t µ˜(1)(x)−
DR N n n n n n n n e˜(x)
(cid:16) (cid:17)
1− 1−t µ˜(0)(x)isashorthandfunctionwithplug-inestimatesµ˜(1),µ˜(0)ande˜(x)thatwewill
1−e˜(x)
uselater. Solvingtheproblemofargmin L yieldstheDoublyRobustestimatorτˆ (x).
τˆ DR DR
When explaining the strategies mentioned above, we omit the details for sample splitting and
regularizationtoimprovereadability. ModernimplementationofthesemethodsusestandardML
1Letf beagroundtruthfunctiondefiningthedatagenerationprocess;f isoftenunobservable.Weusefˆto
representthemainestimator,andf˜torepresenttheplug-in.
4regressionandclassificationmodelsascomponents. Indifferentliteraturecomponentsarecalled
plug-ins,baselearners,ornuisancefunctions.
CATEmodelevaluation. ThereisextensiveliteratureonCATEmodelevaluation. Inprinciple,any
scorefunctionS(τˆ)canrankandevaluateCATEestimators. However,thekeyquestioniswhether
therankinghelpsusersidentifythebestestimatorsfortheirneeds. Whilethispaperfocusesonscore
functionsthatrankbyMSE,it’susefultofirstexplorethebroaderlandscapeofscorefunctions.
The first category of score functions includes hypothesis testing statistics. Studies such as Cher-
nozhukovetal.(2023);Bartolomeisetal.(2024);Hussainetal.(2023)developstatisticstodetect
heterogeneity,unobservedconfounding,ortransportability.ThesestatisticscanrankCATEestimators
butdonotguaranteefindingtheestimatorwithsmallerMSE;thismakesthemunsuitableforgeneral
CATEevaluation. Forinstance,theBLPstatisticfromChernozhukovetal.(2023)isineffectivewhen
theCATEestimatorhassmallvariance. Thesecondcategorycoversrank-basedmetrics,commonly
usedinupliftmodelingandCATEcalibration. ExamplesincludeRadcliffe(2007);Dwivedietal.
(2020);Yadlowskyetal.(2023);Imai&Li(2021);Xu&Yadlowsky(2022). Whileusefulinspecific
contexts, these metrics lack a direct connection to MSE. For example, the Qini index (Radcliffe,
2007)assignsthesamescoretotwoestimatorsτˆ(x)andτˆ(x)+1,eveniftheirMSEdiffers. The
third category consists of score functions that compare CATE estimators to ATE estimates from
experimentaldata,asdiscussedinGentzeletal.(2021)andrelatedwork.
Now,letusturntomethodsdesignedtofindmodelwithsmallestMSE;seeCurth&vanderSchaar
(2021;2023);Mahajanetal.(2023);Nealetal.(2021)forreviews. Themostcommonapproachis
simulationusingsemi-syntheticdatasets,suchasIHDPandJobs(Hill,2011;LaLonde,1986),where
simulatedpotentialoutcomesmakeMSEcalculationfeasible. Extensionsofthisapproachinclude
generativemodelsforsyntheticdata(Nealetal.,2021;Atheyetal.,2020;Parikhetal.,2022). As
notedbefore,simulationzeroesoutreal-worldheterogeneity,thepreciseriskwewanttoevaluate.
Anotherstrategyishold-outvalidation, whichconstructsCATEestimationlossfunctionsontest
datasets. Forinstance,byestimatingpotentialoutcomesµ˜(0)(x)andµ˜(1)(x),onecancomputethe
proxy τ˜(x) and calculate the MSE as L (τˆ) = 1 (cid:80) (τˆ(x )−τ˜(x ))2. Other loss functions
PL N n n n
canalsoapply;theorem15.2.1inChernozhukovetal.(2024)isanexample. Theprimaryriskwith
this approach is still self-serving bias: model may unfairly benefit from being judged by losses
that favor its own design. Furthermore, hold-out validation introduces complexity, as there are
numerouschoices: baseregressionmodels,hyperparameters,regularization,sample-splitting,and
biascorrectiontechniques(e.g.,Neymanorthogonality(Newey,1994),influencefunctions(Alaa&
VanDerSchaar,2019)). Thesechoicesfurtheraggravatetheself-servingbias.
Recentstudies(Curth&vanderSchaar,2023;Mahajanetal.,2023;Nealetal.,2021;Atheyetal.,
2020;Parikhetal.,2022),largelybasedonsemi-syntheticdatasets,haveevaluatedvariousCATE
evaluationcriteria,includingL ,L ,L ,andL . However,consensusisyetform.
OP R DR PL
3 THE PROPOSED EVALUATION METRIC Q
Asdiscussed,existingCATEevaluationcriteriafallshortofselectingthebestmodels,particularly
whenitcomestocapturingreal-worldheterogeneity.Thismakesitdifficultforpractitionerstochoose
themostsuitablemodelsandpreventsthecommunityfrommakingsubstantialbreakthroughsin
CATEestimation. WeproposetobreakthisdilemmabyintroducinganewstatisticalparameterQ,
equaltoMSEminusaconstant,andafamilyofstatisticsQˆ thatcanbecomputedfromreal-world
data. WeprovethatQˆ isaconsistentestimatortoQ; whentreatmentpropensityisknown(asin
observationalsampling),Qˆ isunbiased. ThusQˆ asymptoticallypreservesthesameorderasMSE
whenrankingdifferentCATEmodels. Wealsodiscussthegeneralizationproperty,variancereduction
strategies,andwaystouseQˆ toevaluateCATEestimators.
3.1 QANDITSSTATISTICALESTIMATORQˆ
ForagivenCATEestimatorτˆ,werefactorMSEP(τˆ)intothreeparts:
P(τˆ)=E [(τ(X)−τˆ(X))2]= E [τ2(X)] +E [τˆ2(X)]−2E [τ(X)τˆ(X)] (1)
X X X X
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
unobservableconstant canbeapproximatedfromreal-worlddataset
5Thefirstpart,P = E [τ2(X)],isunobservablebutindependentfromtheCATEestimator,thus
1 X
canbedroppedwhenweevaluaterelativeperformanceofmodels. Thetwootherparts,P (τˆ) =
2
E [τˆ2(X)],andP (τˆ) = E [τ(X)τˆ(X)],canbeapproximatedwithconfidence. Thisyieldsthe
X 3 X
statisticalparameterthatdrivesoraclemodelranking:
Q(τˆ)=P (τˆ)−2P (τˆ)=P(τˆ)−P (2)
2 3 1
Letq(x,t,y;τˆ)=τˆ2(x)−2τˆ(x)η(x,t,y),whereηistheshorthandforHorwitz-Thompsonestimator,
andletq (τˆ)=q(x ,t ,y ;τˆ)forn-thsample. Wenowdefinethesamplestatistic:
n n n n
Qˆ(τˆ)= 1 (cid:88) q (τˆ)= 1 (cid:88) [τˆ2(x )−2τˆ(x )η(x ,t ,y )] (3)
N n N n n n n n
n n
NotethatwecancomputethevalueofQˆ withoutcounterfactualgroundtruth. Itfollowsthat:
Lemma 3.1. Unbiasedness. When the propensity function P(T = 1|X = x) = e(x) is known,
E[Qˆ(τˆ)]=Q(τˆ).
SeeAppendixBforallproofs.
Remark3.2. RelationshipwithorthogonalML.BothLemma3.1andTheorem3.4belowarecon-
nected to orthogonal ML methods (Foster & Syrgkanis, 2023). For example, Theorem 15.2.1 in
Chernozhukov et al. (2024) discuss similar CATE evaluation techniques based on orthogonality
assumptions,requiringthetripleproductofthepropensityerror,plug-inoutcomeestimateerror,and
√
thedifferencebetweentwoCATEestimatestoconvergeatanO(1/ N)rate.
WhileLemma3.1mayinitiallyappearsimilartoresultsinorthogonalML(byzero-ingoutpropensity
risk), itisessentialtoestablishtheseresultswithoutrelyingonorthogonalMLassumptions. As
discussedinSection1,CATEevaluationshouldbebasedonfewerandsimplerassumptionsthan
CATEestimation. Furthermore,asSection4willshow,orthogonality-basedestimatorsoftenfail
tocapturereal-worldheterogeneity,raisingdoubtsabouttheirreliabilityinCATEevaluation. This
makesitcriticaltodevelopresultsonstrongerfoundation. Toourknowledge,ourresultisthefirstto
provideasymptoticguaranteesfororaclerankingundersuchgeneralconditions.
Remark3.3. LocalEffect. Lemma3.1canbeeasilyextendedtothecasewhereQisweightedby
functionw(x) > 0. Thatis,Qˆ(τ;w) = (cid:80) (cid:2) w(x )(τˆ2(x )−2η(x ,t ,y )τˆ(x ))(cid:3) /(cid:80) w(x )
n n n n n n n n n
isanunbiasedestimatorofQ(τˆ;w)=E [w(X)(τˆ2(X)−2τˆ(X)τ(X))]. Thisresultisusefulwhen
X
onepartofthedistributionismoreimportantthanothers.
BasedonLemma3.1,WecanestablishtheconsistencyofQˆ whenpropensityneedstobeestimated:
Theorem 3.4. Consistency. Assume propensity e(x) and its estimate eˆ(x) are both bounded
away from zero and way on their support: 0 < e¯ ≤ e(x),eˆ(x) ≤ 1 − e¯ < 1. Also assume
lim Pr(E [|eˆ (x)−e(X)|]>ϵ)=0forallϵ>0. Wehaveforallϵ>0,
n→∞ X n
lim Pr(|Qˆ (τˆ)−Q(τˆ)|>ϵ)=0 (4)
n
n→∞
Intuitively,Qˆ arerelativeperformancemetrics. Meanwhile,theycanbeusedtomeasureabsolute
performanceofCATEestimators,inthreewaysbelow. WewilldemonstratetheiruseinSection4
Benchmark.
Remark3.5. Degeneracy. ACATEestimatorisuselessifQ(τˆ)≥0;whenthishappens,wecallthe
estimatorisdegenerate. Toseethat,letτˆ =0bea(trivial)CATEestimatorthatestimatesnoCATE
0
constantly. IfQ(τˆ)≥0,theCATEestimatorτˆhasahigherMSEthanτˆ . Thissuggeststhemodelis
0
useless. Asaresult,Qˆ(τˆ)≥0canbeusedtodetectsevereerrorsinCATEestimation.
Remark3.6. Heterogeneityscreening. Secondly,letQ(τˆ )beaconstanteffect(ATE)estimator. A
B
CATEestimatorwithQ(τˆ)≥Q(τˆ )isuselessasitsMSEishigherthanaconstanteffectestimator.
B
Remark3.7. ApproximateMSE.Finally,wecanconstructanMSEestimate,Pˆ(τˆ),bydecorating
Pˆ(τˆ) with plug-in estimates of potential outcomes µ˜(0)(x) and µ˜(1)(x) as follows. Pˆ(τˆ) helps
usunderstandtheerrormagnitudeofCATEestimatorandretainssamerankingpropertyasQˆ(τˆ)
Pˆ(τˆ)=Qˆ(τˆ)+ 1 (cid:80) (µ˜(1)(x)−µ˜(0)(x))2.
N n
6DoesCATEevaluationresultgeneralizetonewdistributions? Weanswerinthenexttwotheorems:
A scientist with access to a dataset generated by one distribution may want to use it to find the
bestCATEestimatorforasecond,differentbutrelateddistribution,withoutthecostofseconddata
selection. Theorem3.8belowshowsthatwecanusedatafromonedistributiontoestimateQon
another, via Inverse Propensity Weighting, when the density ratio between two distributions are
knownorcanbereliablyestimated.
Theorem3.8. GeneralizationviaInversePropensityWeighting. LetΠ andΠ betwodatadis-
1 2
tributions sharing the same τ(x). Let X and X be their marginal distribution of X with den-
1 2
sityρ (x)andρ (x)respectively. Alsoassumebothdistributionssharecommonsupportandlet
1 2
ζ(x)=ρ (x)/ρ (x)bethedensityratio. Let{(x ,t ,y )}(1≤n≤N)beN i.i.dsamplesdrawn
2 1 n n n
fromΠ . WehaveQ(τˆ;X )=E (cid:2)1 (cid:80) ζ(x )(cid:2) τˆ2(x )−2η(x ,t ,y )τˆ(x )(cid:3)(cid:3) .
1 2 Π1 N n n n n n n n
Whendensityratioisdifficulttoestimate, orwhenthepotentialoutcomedistributionchanges, a
scientistmaywonderifthebestCATEestimatoridentifiedforonedistributionisalsothebestfor
another. Theorem3.9belowstatesthattheCATEestimatorwithsmallerQononedistributionisalso
theCATEestimatorwithsmallerQonthesecond,whenthetwodistributionsarecloseenough:
Theorem 3.9. Ranking Generalization. Let Π and Π be two different joint distribution of
1 2
X,Y(0),Y(1). Letτˆ andτˆ betwodeterministicCATEestimators. Leth (x,y ,y ;τˆ)=τˆ2(X)−
1 2 0 0 1
2τˆ(x)(y −y ))beashorthandfunction. LetD (Π ,Π ) := sup |E [h(X,Y(0),Y(1))]−
1 0 H 1 2 h∈H Π1
E [h(X,Y(0),Y(1))]|<∆betheIntegralProbabilityMetricboundedbyafiniteconstant∆,where
Π2
Hisasetofreal-valuedfunctionssuchthath (τˆ ),h (τˆ )∈H.WhenQ(τˆ ;Π )−Q(τˆ ;Π )≥2∆,
0 1 0 2 1 1 2 1
wehaveQ(τˆ ;Π )−Q(τˆ ;Π )>0.
1 2 2 2
3.2 IMPROVEDRESULTSFOROBSERVATIONALSAMPLING
In case of observational sampling (to be used in Section 4), results can be further improved. In
observationalsampling,wesampleasubsetfromexperimentdatatotrainCATEestimators. Wethen
evaluatetheCATEestimatorsontheremainingRCTsub-sample. Theseresultshelpthebenchmark.
Firstweexploreopportunitiesofvariancereduction. EvenwithLemma3.1andTheorem3.4the
variance of Qˆ can still be large in finite-sample settings, driven by the high-variance nature of
Horwitz-Thompsonestimatorη. Inthissectionweprovideageneralcontrolvariatesframeworkto
reducevarianceofQˆwhilepreservingitsdesirableunbiasedproperty. Tostart,weintroducethebasic
conceptsofcontrolvariates(Glynn&Szechtman,2002). LetU beareal-valuedrandomvariableand
wewanttoestimatesitsmeanE[U]. WecanusethesamplemeanestimatorU¯ =(cid:80) u /N where
n n
u arei.i.dsamplesofU. Supposethatthereexistsazero-meanrandomvariableV,E[V]=0. Then,
n
thecontrolvariateU¯(θ)=(cid:80) (u +θv )/N =U¯ +θ(cid:80) v /N isalsoanunbiasedestimatorof
n n n n n
E[U]. Moreover,thevariance-minimizingchoiceisθ∗ =−Cov(U,V)/Var[V].
ToapplycontrolvariatesonQˆ,notethatQˆ = 1 (cid:80) q isthesamplemeanofq(X,T,Y;τˆ). Let
N n n
r(x,t,y;τˆ)beacontrolvariatesfunctionwithzeromean,i.e.,E[r(X,T,Y;τˆ)]=0. Therefore
Qˆ(r(·);τˆ)=
1 (cid:88)(cid:104)
q(x ,t ,y ;τˆ)+θr(x ,t ,y
;τˆ)(cid:105)
(5)
N n n n n n n
n
hasthesameexpectationasQˆ(τˆ),i.e.,E[Qˆ(r(·);τˆ)=E[Qˆ(τˆ)].
NextweshowthatlocationinvarianceandcommonlyusedCATEestimationlossesarespecialcases
ofthiscontrolvariatesframework.
Proposition3.10. LocationInvariance. AssumeX ⊥T. Letthelocationinvariancecontrolvariates
(cid:16) (cid:17)
functionber (x,t,y;τˆ)=2 t − (1−t) τˆ(x). Qˆ(r )=Qˆ+θ 1 (cid:80) r (x ,t ,y ;τˆ)isan
LI E1 E0 LI N n LI n n n
unbiasedestimatorofQ.
Proposition3.11. DoublyRobustloss. AssumeX ⊥ T. Definethecontrolvariatesfunctionas
r (x,t,y;τˆ) = −2γ(x,t)τˆ(x)andQˆ(r ) = Qˆ + 1 (cid:80) r (x ,t ,y ;τˆ), whereγ(x,t)is
DR DR N n DR n n n
theshorthandfunctiondefinedinSection2. WehaveE[Qˆ(r )] = QandL (τˆ) = Qˆ(r )+
DR DR DR
(cid:104) (cid:105)2
1 (cid:80) η(x ,t ,y )+γ(x ,t ) isequaltoQˆ(r )plusaconstantindependentfromτˆ.
N n n n n n n DR
7Proposition3.12. R-loss.AssumeX ⊥T.Definethecontrolvariatesfunctionasr (x,t)=−4(1−
R
2t)m˜(x)τˆ(x)andQˆ(r )=Qˆ+ 1 (cid:80) r (x ,t ,y ;τˆ). WehaveE[Qˆ(r )]=Q. Moreoverwhen
R N n R n n n R
E = Pr(T = 1) = 0.5, L (τˆ) = Qˆ(rR) + 1 (cid:80) (y −m˜(x ))2 is equal to Qˆ(r )/4 plus a
1 R 4 N n n n R
constantindependentfromτˆ.
VariationsofQˆ arerank-preservingwhenusedtoevaluateCATEmodels. Whenthedatasetgrows
large,theirdifferencedisappears. Whichvarianttousedependsontheoreticalandpracticalconsider-
ations: theoriginalQˆ andQˆ(r )aremodel-freeandeasytoimplement. Meanwhile,thevariance
LI
reductionvariantsQˆ(r),includingitsspecialcasesQˆ(r )andQˆ(r ),offersthepotentialbenefit
R DR
ofevenlowervariance,atthepriceoffittingandsavingtheextraplug-inestimators. Finally,notethat
Chernozhukovetal.(2023)provedresultssimilartoPropositions3.12and3.11,basedonstringent
orthogonalityassumptions;theirresultdonotgeneralizetoothercontrolvariates.
√
Finally,weprovethatallvariantsofQˆ achievesO(1/ N)convergencerate:
Theorem3.13. ConvergenceRate. AssumeY isaboundedrandomvariable,τˆ(x)isabounded
function,propensityscoreisbounded,i.e.,0<e¯<e(x)=Pr(T =1|X =x)<1−e¯<1,andthat
√
thecontrolvariatefunctionr(x,t,y;τˆ)isbounded. Wehave N(Qˆ(r;τˆ)−Q)→N(0,σ2(r,τˆ))
whereσ2(r,τˆ)isthefinitevarianceforq(r;τˆ).
4 BENCHMARK AND FINDINGS
4.1 BENCHMARKDESIGNVIAOBSERVATIONALSAMPLING
Now that we have studied the statistical properties of variations of Qˆ, we are ready to use it to
evaluateCATEestimationmodelsusingreal-worlddatasets. Weemphasizethattheevaluationis
morethanapost-mortemexamination. Theorems3.8and3.9suggeststhatresultsobtainedfromone
distributioncangeneralizetoanewoneundertherightconditions. PerformanceofCATEestimators
onacarefullyselectedportfoliosofobservationalsamplingstudyarepredictiveindicatorstotheir
futureperformanceonnewandsimilardistributions.
Datasetgeneration. WeusetwelvelargeRCTdatasetsforthisevaluationstudy. Theyarelistedin
Table4. Thesedatasetswereselectedtorepresentdiversereal-worlddatagenerationprocesses;the
rationalefortheirinclusionandadditionaldetailsareinAppendixE.
Observationalsampling. ForeachRCTdatasetD,wesampleittogeneratetheestimationD with
est
selectionbias,andanevaluationRCTdatasetD ;thereisnooverlapbetweenthem. SeeAppendix
eval
Ffordetails. Wevarythreesamplingparameters,4variationsinestimationdatasetsize,3variations
intreatment%,and3variationsinassignmentmechanismnonlinearity;thisresultsin36settings. For
everysetting,wesampleD andD jointly100times,yielding3,600pairsofD andD .
est eval est eval
Repeatingsameprocessforthe12RCTdatasetsyields12×3,600=43,200benchmarkdatasets.
Estimationmodelselection. Weevaluate16CATEestimationmodelsonD . Thesemodelsinclude
est
variationsofS,R,andTlearners(Künzeletal.,2019),DoublyRobustlearners(Kennedy,2023),
DoubleMachineLearningmodels(Chernozhukovetal.,2017;Nie&Wager,2020),representation
learning-based models (Shi et al., 2019), and causal random forest models (Athey et al., 2018).
Weusetheformat<model-name>.<base-learner>.<details>asthemodelcodewhen
presentingresults. FullmodeldetailscanbefoundinTable3inAppendixC.Weusecodefrom
Curth&vanderSchaar(2023);Curth(2023);Battocchietal.(2019)forreproducibility.
EstimationandEvaluation. Wetrain16modelslistedinTable3onD . SeeAppendixCfordetails.
est
WethenevaluatethetrainedmodelsonD ,usingQˆ(r ).
eval DR
4.2 FINDINGS
Table1summarizesthebenchmarkfindings. Foreachdataset,wecalculateQˆ foreverymodel. Out
of43,200datasets,41,499(96%)haveatleastonenon-degeneratemodelwithQˆ(τˆ)<0. Forthese
datasets,modelsarerankedfrombest(rank1forthemostnegativeQˆ)toworst. Amodel"wins"ifit
ranks1,andits"winshare"reflectshowoftenitoutperformsothermodels. Wealsocomputeeach
model’saveragedegeneraterate.
8Table1: Modelcomparison: summaryof43,200datasets
Model Wins Winshare Degenerate Degeneraterate Avgrank
s.xgb.cv 10,491 25.5% 2,600 6.3% 4.4
s.ridge.cv 5,327 12.9% 12,837 31.2% 4.2
dragon.nn 4,976 12.1% 18,021 43.8% 5.1
s.ext.ridge.cv 4,582 11.1% 20,760 50.4% 5.6
dml.elastic 3,413 8.3% 19,913 48.4% 5.6
dml.lasso 3,279 8.0% 19,916 48.4% 5.7
s.ext.xgb.cv 2,648 6.4% 21,344 51.8% 6.9
r.ridge.cv 2,532 6.2% 25,209 61.2% 8.7
dr.ridge.cv 2,499 6.1% 24,384 59.2% 7.1
t.ridge.cv 1,780 4.3% 26,383 64.1% 8.2
dr.xgb.cv 476 1.2% 29,409 71.4% 9.9
cforest 209 0.5% 31,286 76.0% 10.5
t.xgb.cv 187 0.5% 31,561 76.7% 11.4
r.xgb.cv 110 0.3% 34,814 84.6% 12.4
dml.xgb - 0.0% 40,741 99.0% 15.9
dml.linear - 0.0% 38,796 94.2% 14.3
ThebenchmarkrevealscriticalinsightsintothecurrentlandscapeofCATEestimationmodels:
1. CATEmodelsproducesdegenerateestimatorsmorethanhalfthetime. Wefound62%offitted
CATE estimators were degenerate, highlighting the need for problem-specific fine-tuning. This
suggests that using using Qˆ(τˆ) ≤ 0 as a model selection guardrail is crucial for avoiding poor-
performingestimators,whenpossible.
2. CATEestimatorsfailtooutperformaconstant-effectbenchmark80%ofthetime. WeuseDouble
MLwithaLassobaselearner(dml.lasso)toconstructτˆ ,aconstant-effectestimator. Among
B
25,440 datasets with non-degenerate τˆ < 0, only 20% of CATE estimators (τˆ) outperform τˆ .
B B
Thisfindingisstriking,giventhatthesemethodsareexplicitlydesignedtocaptureheterogeneity. It
alsohighlightstheunderappreciatedvalueofheterogeneitydetectionmethods(Crumpetal.,2008;
Chernozhukovetal.,2023),whichdeservesignificantlymoreattention.
3. Orthogonality-basedlearnersunderperform. Despitetheirtheoreticaladvantages,thesemodels
(modelnamedml,r,dr,andcforest)haveanaveragedegeneraterateof71%,andwinonly
30%ofthetime. Thisunderperformanceraisesconcernsabouttheself-servingbiasinherentinusing
theirproxylossesasCATEevaluationcriteria.
4.Nosinglemodelconsistentlyoutperformsothers.Amongthesixteenmodelsevaluated,s.xgb.cv
hadthehighestwinshareat25.5%. Unlikepriorstudies,ourfindingsarebasedonreal-worlddata
ratherthansimulatedoutcomes,reinforcingtherelevanceoftheseresultsforpracticalapplications.
Detailed performance analysis across datasets, data size, treatment proportion, and assignment
complexityisavailableinAppendixH.1.
4.3 RESULTVALIDITYANDCONSIDERATIONS
Weweresurprisedbythefindings. Whileweanticipatedvariationin(relative)accuracy, wedid
expectcontemporaryCATEestimatorstoprovidegenerallyusefulestimates. Beforeconcluding
thattheseresultsreflectfundamentalissueswithCATEestimation,weconsiderseveralalternative
explanations:
IsQˆ reallyperformingoracleranking? WepresentsimulationresultsontheagreementbetweenQˆ
andMSEP whenusedtoselectbestmodels. Wetestedusingsemi-syntheticdatasetsbasedonthe
Hillstromdataset(Hillstrom,2008). Syntheticpotentialoutcomesandtreatmentsweregenerated,and
thedatasetwassplitintoanestimationset(D )andanevaluationset(D )ofvaryingsizes(1,000
est eval
to64,000samples). Wetrainedthesame16CATEmodelsonD andevaluatedthemonD
est eval
usingQˆ variantsastheevaluationcriteria. ToassesstheaccuracyofQˆ,wecomparedmodelrankings
from Qˆ with oracle rankings available in the simulated data using ranking metrics; see Figure 1
9forMRRandAppendixDformoredetails. Aspredictedbytheory,theagreementbetweenQˆ and
theoracleimprovedwithlargerevaluationdatasets, andQˆ consistentlyoutperformedalternative
evaluationmetrics. SeeAppendixD.2formoreresults.
mrr vs test dataset s ze on h llstrom
1.0
oracle MSE aga n() S-learner (xgb) proxy loss
0.9
Q̂ MSE against S-learner (lr) proxy loss
0.8
0.7 Q̂(rLI) MSE against T-learner (xgb) proxy loss
MSE against T-learner (lr) proxy loss
0.6 Q̂(rR) w )h lr ba(e learner
Qini
0.5 Q̂(rR) w )h xgb ba(e learner MSE against ATE
0.4
0.3 Q̂(rDR) w )h lr ba(e learner Calibration Score
0.2 Q̂(rDR) w )h xgb ba(e learner
0.1 Ou)come pred c) on accuracy
0.0
1k 2k 4k 8k 16k 32k 64k
Dataset s ze
Figure1: RankingagreementbetweenQˆ variantsandoracle. X-axisrepresentsevaluationdatasize;
y-axisrepresentsMRR.
Implementation Accuracy. The possibility of implementation errors is a valid concern, but we
minimizetheriskbyre-usingthecodebase(Curth,2023)thathasbeenusedforrecentlarge-scale
benchmarks(Curth&vanderSchaar,2023). WereliedonexistingCATEestimatorsandevaluation
criteria when possible and used EconML for additional implementations. We are committed to
releasingourcodefollowingproperapprovaltofurtherensuretransparencyandreproducibility.
Model Selection. Our evaluation focused on 16 widely used CATE models; they span the major
strategiesinCATEestimationdiscussedinSection2.2.Whileresourceconstraintslimitedthenumber
ofmodelswecouldinclude,thisselectionoffersarepresentativeevaluationofcontemporarymethods.
However, weacknowledge thatadditionalmodels, particularly fromdeeplearningandGaussian
Processapproaches(Alaa&vanderSchaar,2017),couldprovidefurtherinsights.
Context-SpecificGeneralizability. Whilethedatasetsusedinourbenchmarkmaynotcoverevery
researcher’sspecificneeds,theyrepresentadiverserangeofreal-worlddatagenerationprocesses.
OurresultsexposesignificantrisksinCATEestimation,particularlyforpractitionerswithoutthe
deep domain expertise necessary for rigorous model fine-tuning. These findings provide crucial
insightsintothelimitationsofwidelyusedCATEmethodsincapturingreal-worldheterogeneity.
5 CONCLUSIONS
WeintroduceanewapproachtoevaluatingCATEestimatorsusingobservationalsampling,centered
aroundthestatisticalparameterQtoidentifytheestimatorwiththelowestMSE.TheQˆ familyof
statisticsareunbiasedestimatorsofQ,computablefromreal-worldRCTdatasetswithoutrelyingon
simulatedpotentialoutcomes. Underthecontrolvariatesframework,weshowthatcommonCATE
estimationlossesarespecialcasesofQˆ,andthatthismethodgeneralizestonewandobservational
distributions.
However,themostimportantcontributionofthisworkistheempiricalfindingsthemselves.Ourlarge-
scaleevaluationofsixteenCATEmodelsacross43,200datasetsbuiltfromreal-worlddatareveals
astrikingpattern: 62%ofCATEestimates(71%fororthogonalitybasedmodels)performworse
thanatrivialzero-effectpredictor,andamongcaseswithusefulestimates,80%failtooutperforma
constant-effectmodel. Furthermore,orthogonality-basedmodelsonlyoutperformnon-orthogonality
models30%ofthetime. ThesefindingshighlightimportantlimitationsincurrentCATEestimation
researchandemphasizetheneedforcontinuedinnovationanddevelopmentinthisarea.
REFERENCES
AhmedAlaaandMihaelaVanDerSchaar. Validatingcausalinferencemodelsviainfluencefunctions.
InKamalikaChaudhuriandRuslanSalakhutdinov(eds.),Proceedingsofthe36thInternational
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,
10pp.191–201.PMLR,09–15Jun2019. URLhttps://proceedings.mlr.press/v97/
alaa19a.html.
AhmedM.AlaaandMihaelavanderSchaar. Bayesianinferenceofindividualizedtreatmenteffects
usingmulti-taskgaussianprocesses,2017.
SusanAtheyandGuidoImbens. Recursivepartitioningforheterogeneouscausaleffects. Proceedings
oftheNationalAcademyofSciences,113(27):7353–7360,2016.
SusanAthey,JulieTibshirani,andStefanWager. Generalizedrandomforests,2018.
SusanAthey,GuidoImbens,JonasMetzger,andEvanMunro. Usingwassersteingenerativeadver-
sarialnetworksforthedesignofmontecarlosimulations,2020.
PhilippBach,VictorChernozhukov,MalteS.Kurz,andMartinSpindler. DoubleML–Anobject-
orientedimplementationofdoublemachinelearninginPython. JournalofMachineLearning
Research,23(53):1–6,2022. URLhttp://jmlr.org/papers/v23/21-0862.html.
Piersilvio De Bartolomeis, Javier Abad, Konstantin Donhauser, and Fanny Yang. Hidden yet
quantifiable: Alowerboundforconfoundingstrengthusingrandomizedtrials,2024.
KeithBattocchi,EleanorDillon,MaggieHei,GregLewis,PaulOka,MirunaOprescu,andVasilis
Syrgkanis. EconML:APythonPackageforML-BasedHeterogeneousTreatmentEffectsEstima-
tion. https://github.com/py-why/EconML,2019. Version0.x.
VictorChernozhukov,DenisChetverikov,MertDemirer,EstherDuflo,ChristianHansen,Whitney
Newey,andJamesRobins. Double/debiasedmachinelearningfortreatmentandcausalparameters,
2017.
VictorChernozhukov,MertDemirer,EstherDuflo,andIvánFernández-Val. Fisher-schultzlecture:
Genericmachinelearninginferenceonheterogenoustreatmenteffectsinrandomizedexperiments,
withanapplicationtoimmunizationinindia,2023.
Victor Chernozhukov, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis.
Applied causal inference powered by ml and ai, 2024. URL https://arxiv.org/abs/
2403.02467.
RichardK.Crump,V.JosephHotz,GuidoW.Imbens,andOscarA.Mitnik. NonparametricTestsfor
TreatmentEffectHeterogeneity. TheReviewofEconomicsandStatistics,90(3):389–405,082008.
doi: 10.1162/rest.90.3.389. URLhttps://doi.org/10.1162/rest.90.3.389.
Alicia Curth. Cateselection: Sklearn-style implementations of model selection criteria for cate
estimation. https://github.com/AliciaCurth/CATESelection, 2023. Accessed:
2023-05-08.
AliciaCurthandMihaelavanderSchaar.Doinggreatatestimatingcate?ontheneglectedassumptions
inbenchmarkcomparisonsoftreatmenteffectestimators,2021.
AliciaCurthandMihaelavanderSchaar. Insearchofinsights,notmagicbullets: Towardsdemystifi-
cationofthemodelselectiondilemmainheterogeneoustreatmenteffectestimation,2023.
MichaelDavern,ReneBautista,JeremyFreese,PamelaHerd,andStephenL.Morgan. Generalsocial
survey1972-2022.[machine-readabledatafile].,2023.
EustacheDiemert,ArtemBetlei,ChristopheRenaudin,Massih-RezaAmini,ThéophaneGregoir,
andThibaudRahier. Alargescalebenchmarkforindividualtreatmenteffectpredictionanduplift
modeling,2021.
RaazDwivedi,YanShuoTan,BritonPark,MianWei,KevinHorgan,DavidMadigan,andBinYu.
Stablediscoveryofinterpretablesubgroupsviacalibrationincausalstudies,2020.
BrunoFerman. Readingthefineprint: Informationdisclosureinthebraziliancreditcardmarket.
ManagementScience,62(12):3534–3548,2015.
DylanJ.FosterandVasilisSyrgkanis. Orthogonalstatisticallearning,2023.
11AmandaGentzel,PurvaPruthi,andDavidJensen. Howandwhytouseexperimentaldatatoevaluate
methodsforobservationalcausalinference,2021.
PeterW.GlynnandRobertoSzechtman. Somenewperspectivesonthemethodofcontrolvariates. In
MonteCarloandQuasi-MonteCarloMethods2000,pp.27–49,Berlin,Heidelberg,2002.Springer
BerlinHeidelberg.
BrettR.Gordon,FlorianZettelmeyer,NehaBhargava,andDanChapsky.Acomparisonofapproaches
toadvertisingmeasurement: Evidencefrombigfieldexperimentsatfacebook. MarketingScience,
38(2):193–225,2019.
BrettR.Gordon,RobertMoakler,andFlorianZettelmeyer. Closeenough? alarge-scaleexploration
ofnon-experimentalapproachestoadvertisingmeasurement,2022.
JenniferHill. Bayesiannonparametricmodelingforcausalinference. JournalofComputationaland
GraphicalStatistics,20:217–240,032011. doi: 10.1198/jcgs.2010.08162.
Kevin Hillstrom. The minethatdata e-mail analytics and data mining chal-
lenge, 2008. URL https://blog.minethatdata.com/2008/03/
minethatdata-e-mail-analytics-and-data.html.
D. G. Horvitz and D. J. Thompson. A generalization of sampling without replacement from a
finiteuniverse. JournaloftheAmericanStatisticalAssociation,47(260):663–685,1952. ISSN
01621459.
ZeshanHussain,Ming-ChiehShih,MichaelOberst,IlkerDemirel,andDavidSontag. Falsification
ofinternalandexternalvalidityinobservationalstudiesviaconditionalmomentrestrictions,2023.
KosukeImaiandMichaelLingzhiLi. Experimentalevaluationofindividualizedtreatmentrules.
JournaloftheAmericanStatisticalAssociation,118(541):242–256,June2021. ISSN1537-274X.
doi: 10.1080/01621459.2021.1923511. URLhttp://dx.doi.org/10.1080/01621459.
2021.1923511.
IST Collaborative Group and P.A.G. Sandercock. The international stroke trial (ist): A ran-
domised trial of aspirin, subcutaneous heparin, both, or neither among 19 435 patients with
acuteischaemicstroke. TheLancet,349(9065):1569–1581,May1997. ISSN0140-6736. doi:
10.1016/S0140-6736(97)04011-7.
FredrikD.Johansson,UriShalit,andDavidSontag. Learningrepresentationsforcounterfactual
inference,2018.
EdwardH.Kennedy. Towardsoptimaldoublyrobustestimationofheterogeneouscausaleffects,
2023.
Sören R. Künzel, Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. Metalearners for estimating
heterogeneoustreatmenteffectsusingmachinelearning. ProceedingsoftheNationalAcademy
of Sciences, 116(10):4156–4165, feb 2019. doi: 10.1073/pnas.1804597116. URL https:
//doi.org/10.1073%2Fpnas.1804597116.
RobertJ.LaLonde. Evaluatingtheeconometricevaluationsoftrainingprogramswithexperimental
data. The American Economic Review, 76(4):604–620, 1986. ISSN 00028282. URL http:
//www.jstor.org/stable/1806062.
DivyatMahajan,IoannisMitliagkas,BradyNeal,andVasilisSyrgkanis. Empiricalanalysisofmodel
selectionforheterogeneouscausaleffectestimation,2023.
Brady Neal, Chin-Wei Huang, and Sunand Raghupathi. Realcause: Realistic causal inference
benchmarking,2021.
WhitneyK.Newey. Theasymptoticvarianceofsemiparametricestimators. Econometrica,62(6):
1349–1382,1994.
XinkunNieandStefanWager. Quasi-oracleestimationofheterogeneoustreatmenteffects,2020.
12HarshParikh,CarlosVarjao,LouiseXu,andEricTchetgenTchetgen. Validatingcausalinference
methods. InKamalikaChaudhuri,StefanieJegelka,LeSong,CsabaSzepesvari,GangNiu,and
Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning,
volume162ofProceedingsofMachineLearningResearch,pp.17346–17358.PMLR,17–23Jul
2022. URLhttps://proceedings.mlr.press/v162/parikh22a.html.
NicholasRadcliffe. Usingcontrolgroupstotargetonpredictedlift: Buildingandassessinguplift
model. 2007. URLhttps://api.semanticscholar.org/CorpusID:22535399.
JamesM.Robins,AndreaRotnitzky,andLuePingZhao. Estimationofregressioncoefficientswhen
someregressorsarenotalwaysobserved. JournaloftheAmericanStatisticalAssociation,89(427):
846–866,1994.
P.M.Robinson. Root-n-consistentsemiparametricregression. Econometrica,56(4):931–954,1988.
ISSN00129682,14680262. URLhttp://www.jstor.org/stable/1912705.
PaulR.RosenbaumandDonaldB.Rubin. Thecentralroleofthepropensityscoreinobservational
studiesforcausaleffects. Biometrika,70(1):41–55,041983.
DonaldB.Rubin. Causalinferenceusingpotentialoutcomes: Design,modeling,decisions. Journal
oftheAmericanStatisticalAssociation,100(469):322–331,2005. ISSN01621459. URLhttp:
//www.jstor.org/stable/27590541.
UriShalit,FredrikD.Johansson,andDavidSontag. Estimatingindividualtreatmenteffect: general-
izationboundsandalgorithms,2017.
Claudia Shi, David M. Blei, and Victor Veitch. Adapting neural networks for the estimation of
treatmenteffects,2019.
StefanWager,WenfeiDu,JonathanTaylor,andRobertJ.Tibshirani. High-dimensionalregression
adjustmentsinrandomizedexperiments. ProceedingsoftheNationalAcademyofSciences,113
(45):12673–12678,2016. doi: 10.1073/pnas.1614732113. URLhttps://www.pnas.org/
doi/abs/10.1073/pnas.1614732113.
YizheXuandSteveYadlowsky. Calibrationerrorforheterogeneoustreatmenteffects,2022.
Steve Yadlowsky, Scott Fleming, Nigam Shah, Emma Brunskill, and Stefan Wager. Evaluating
treatmentprioritizationrulesviarank-weightedaveragetreatmenteffects,2023.
13A NOTATION TABLE
Letf beagroundtruthfunctiondefiningthedatagenerationprocess;f isoftenunobservable. We
usefˆtorepresentthemainestimator,andf˜torepresenttheplug-in.
Notation Definition
X Pre-treatmentrandomvector
T Binarytreatment
Y(0)andY(1) Potentialoutcomes
Y =Y(T) Outcome
µ(0)(x)andµ(1)(x) Expectationsofpotentialoutcomes
µ˜(0)(x)andµ˜(1)(x) Plug-inestimatesofpotentialoutcomes
τ(x)=µ(1)(x)−µ(0)(x) groundtruthCATEfunction
τˆ(x) CATEestimate
e(x)=Pr(T =1|X =x) Propensityfunction
e˜(x) Plug-inestimateofe(x)
E TreatmentprobabilityinRCT,i.e.,E =Pr(T =1). AlsoE =1−E
1 1 0 1
D Adataset,alistof(X,T,Y)tuples
N Numberofsamplesindataset
(x ,t ,y ) n-thsampleinD
n n n
D Theestimationdataset,asubsetofD
est
D Theevaluationdataset,asubsetofD
eval
P MeanSquaredErrorofCATEestimator. AlsoknownasPEHE
Q Thestatisticalparameter;alsothepartofMSEthatdependsonτˆ
Qˆ ThesamplestatisticscomputedondatasetD
r(·) Thezero-meancontrolvariatefunction
Qˆ(r) Thesampledstatisticswithcontrolvariatesfunctionr
L Rloss
R
L DRloss
DR
m(x) E[Y|X =x],usedinRloss
m˜(x) Plug-inestimateofm(x)
η(x,t,y) ShorthandfunctionusedforIPWestimator
γ(x,t) ShorthandfunctionusedinDRlearner
Table2: Notations
B PROOFS
B.1 PROOFOFLEMMA3.1
Proof. Toseethat,notice
P (τˆ) = E [τˆ(x)τ(x)]
3 X
= E [τˆ(x)E [η(X,T,Y)]]
X T,Y|X
= E [E [τˆ(x)η(X,T,Y)]]
X T,Y|X
= E [τˆ(x)η(X,T,Y)]
X,T,Y
(cid:104) 1 (cid:88) (cid:105)
= E τˆ(x )η(x ,t ,y )
N n n n n
n
14wherethesecondequationisduetounbiasednessofHorwitz-Thompsonestimatorη. Itfollowsthat
Q = P −2P
2 3
(cid:104) 1 (cid:88) (cid:105)
= E (τˆ(x )−2τˆ(x )η(x ,t ,y ))
N n n n n n
n
(cid:104) 1 (cid:88) (cid:105)
= E q(x ,t ,y ;τˆ)
N n n n
n
= E[Qˆ]
ThusE[Qˆ]=Qholdsaslongasthegroundtruthpropensityfunctione(x)isknown,evenifitisnot
constant.
RemarkB.1. TheproofcanbeextendedtootherunbiasedCATEestimators.
B.2 PROOFOFTHEOREM3.4
Proof. First,notice
(cid:20) (cid:21)
Qˆ(e(x))−Qˆ(eˆ(x)) = 1 (cid:88) τˆ2(x )−2τˆ(x )( t n − 1−t n )y (6)
N n n e(x ) 1−e(x ) n
n n
n
(cid:20) (cid:21)
− 1 (cid:88) τˆ2(x )−2τˆ(x )( t n − 1−t n )y (7)
N n n eˆ(x ) 1−eˆ(x ) n
n n
n
(cid:18) (cid:19)
= 1 (cid:88) 2τˆ(x )y t n − t n + 1−t n − 1−t n (8)
N n n eˆ(x ) e(x ) 1−e(x ) 1−eˆ(x )
n n n n
n
Itfollowsthat
|Qˆ(e(x))−Qˆ(eˆ(x))| (9)
(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)(cid:19)
≤ N1 (cid:88) |2τˆ(x n)y n| (cid:12) (cid:12) (cid:12)eˆ(t xn
)
− e(t xn )(cid:12) (cid:12) (cid:12)+(cid:12) (cid:12) (cid:12)1−1− e(t xn
)
− 1−1− eˆ(t xn )(cid:12) (cid:12)
(cid:12)
(10)
n n n n
n
(cid:18) (cid:19)
= 1 (cid:88) |2τˆ(x )y | t n |eˆ(x )−e(x )|+ 1−t n |eˆ(x )−e(x ()1|1)
N n n e(x )eˆ(x ) n n (1−e(x ))(1−e(x )) n n
n n n n
n
(cid:40) (cid:18) (cid:19) (cid:41)
≤ 1 (cid:88) |2τˆ(x )y | t n + 1−t n |e(x )−e(x )| (12)
N n n e(x )eˆ(x ) (1−e(x ))(1−e(x )) n n
n n n n
n
1 (cid:88)
≤ C |e(x )−eˆ(x )| (13)
1N n n
n
= C (E [|e(X)−eˆ(X)|]+ε ) (14)
1 X 1
≤ C (E [|e(X)−eˆ(X)|]+|ε |) (15)
1 X 1
where
(cid:18) (cid:19)
1 1
C =4max , max|τˆ(x )y | (16)
1 e¯2 (1−e¯)2 n n n
isaconstantand
1 (cid:88)
ε =E [|e(X)−eˆ(X)|]− |e(x )−eˆ(x )| (17)
1 X N n n
n
√
ε isazero-meanrandomvariablewithasymptoticvarianceontheorderofo(1/ N)duetoCentral
1
LimitTheorem. Asaresultwehavelim Pr(ε >ϵ)=0. Itfollowsthat
n→∞ 1
lim Pr(E|e(X)−eˆ(X)|>ε)=0 (18)
n→∞
15Combiningthetwoyields
lim Pr(|Qˆ(e(x))−Qˆ(eˆ(x))|>ϵ)=0 (19)
n→∞
Secondly,byTheorem3.13,wehave
√
N(Qˆ(e(x))−Q(e(x)))→N(0,σ2) (20)
Itfollowsthat
lim Pr(|Qˆ(e(x))−Q(e(x))|>ϵ)=0 (21)
n→∞
Finally,noticethat
Qˆ(eˆ(x))−Q(e(x))=[Qˆ(eˆ(x))−Qˆ(e(x))]+[Qˆ(e(x))−Q(e(x))] (22)
isthesumoftwoparts. Theconvergenceforthefirstpartisgivenby(19)andthesecondpartby
(21). Itfollowsthat
lim Pr(|Qˆ(eˆ(x))−Q(e(x))|>ϵ)=0 (23)
n→∞
B.3 PROOFOFTHEOREM3.8
Proof. RecallQ=P −2P . Wehave:
2 3
P (τˆ;X ) = E [τˆ2(X)] (24)
2 2 X2
= E [ζ(X)τˆ2(X)] (25)
X1
(cid:34) (cid:35)
1 (cid:88)
= E ζ(x )τˆ2(x ) (26)
Π1 N n n
n
wherethesecondequationisisduetoinversepropensityweightingandthedefinitionofdensityratio,
andthethirdequationisduetotheunbiasednessnatureofsamplemean. Similarly,
P (τˆ;X ) = E [τ(X)τˆ(x)] (27)
3 2 X2
= E [ζ(X)τ(X)τˆ(x)] (28)
X1
= E [ζ(X)η(X)τˆ(x)] (29)
X1
(cid:34) (cid:35)
1 (cid:88)
= E ζ(x )η(x ,t ,y )τ(x ) (30)
Π1 N n n n n n
n
Combiningthetwoyields
(cid:34) (cid:35)
Q(τˆ;X )=E 1 (cid:88) ζ(x )(cid:2) τˆ2(x )−2η(x ,t ,y )τˆ(x )(cid:3) (31)
2 Π1 N n n n n n n
n
Tosummarize,ifwecomputeQˆ(τˆ)onΠ andweightitbyIPWdensityratioζ,wegetanunbiased
1
estimatorofQ(τˆ)onΠ .
2
B.4 PROOFOFTHEOREM3.9
Proof. Firstnotethat,foragivenCATEestimatorτˆ,thedifferencebetweenQ(τˆ)underΠ andΠ
1 2
isboundedby∆:
|Q(τˆ;Π )−Q(τˆ;Π )| = |E [τˆ2(X)−2τˆ(X)τ(X)]−E [τˆ2(X)−2τˆ(X)τ(X)]|
1 2 X1 X2
= |E [τˆ2(X)−2τˆ(X)E [Y(1)−Y(0)]]−E [τˆ2(X)−2τˆ(X)E [Y(1)−Y(0)]]|
X1 Y1| X2 Y2
= |E [h (X,Y(0),Y(1))]−E [h (X,Y(0),Y(1))]|
Π1 0 Π2 0
≤ sup|E [h(X,Y(0),Y(1))]−E [h(X,Y(0),Y(1))]|
Π1 Π2
h∈H
= D (Π ,Π )
H 1 2
< ∆
16wherethethirdequalityisduetothedefinitionofh andthefifthstepisduetothedefinitionofIPM
0
D (Π ,Π ).
H 1 2
LetusassumewehavetwoCATEestimators,τˆ (x)andτˆ (x),and
1 2
Q(τˆ;Π )(τˆ )−Q(τˆ;Π )(τˆ )≥2∆ (32)
1 1 1 2
Itfollowsthat
Q(τˆ ;Π )−Q(τˆ ;Π ) > Q(τˆ ;Π )−∆−(Q(τˆ ;Π )+∆)
1 2 2 2 1 1 2 1
> Q(τˆ ;Π )−Q(τˆ ;Π )−2∆
1 1 2 1
> 0
Thatis,τˆ isalsobetteronΠ .
2 2
B.5 PROOFOFPROPOSITION3.10
Proof. FirstweshowthatE[r (X,T,Y;τˆ]=0,i.e.,itisacontrolvariatesfunction.Thisisobvious
LI
because
(cid:20)(cid:18) (cid:19) (cid:21)
T (1−T)
E[r (X,T,Y;τˆ)] = 2θE − τˆ(X) (33)
LI E E
1 0
(cid:20) (cid:21)
T (1−T)
= 2θE − E[τ(ˆ X)] (34)
E E
1 0
=
2θ(1−1)E[τ(ˆ
X)] (35)
= 0 (36)
wherethefirststepisbydefinitionofr (·),andthesecondstepisbypropertyofRCTdataset.
LI
Itfollowsthat
(cid:34) (cid:35)
E[Qˆ(r )] = E Qˆ+ 1 (cid:88) r (x ,t ,y ;τˆ) (37)
LI N LI n n n
n
= E[Qˆ]+E[r (X,T,Y;τˆ)] (38)
LI
= E[Qˆ] (39)
= Q (40)
B.6 PROOFOFPROPOSITION3.11
Proof. FirstweproveE[r (X,T,Y;τˆ)]=0. Firstnotethat,
DR
(cid:104) T (cid:105) T
E (1− )µˆ(1)(X)τˆ(X) = E[(1− )]E[µˆ(1)(X)τˆ(X)] (41)
E E
1 1
= (1−1)E[µˆ(1)(X)τˆ(X)] (42)
= 0 (43)
Similarly
(cid:104) 1−T (cid:105)
E 1− µˆ(0)(X)τˆ(X) =0 (44)
1−E
0
CombiningtheresultsaboveyieldsE(r (X,T,Y;τˆ)]=0. ItfollowsthatE[Qˆ ]=E[Qˆ]=Q
DR DR
17NextweshowthatL isalinearfunctionofQˆ(τˆ). Recall
DR
L (τˆ) = 1 (cid:88) [η(x ,t ,y )+γ(x ,t )−τˆ(x )]2 (45)
DR N n n n n n n
n
=
1 (cid:88)(cid:110)
[(η(x ,t ,y )+γ(x ,t )]2+τˆ2(x )−2[η(x ,t ,y )+γ(x ,t
)]τˆ(x(4)(cid:111)
6)
N n n n n n n n n n n n n
n
=
1 (cid:88)(cid:104)
η(x ,t ,y )+γ(x ,t
)(cid:105)2
+Qˆ+
1 (cid:88)(cid:104)
r (x ,t )τˆ(x
)(cid:105)
(47)
N n n n n n N DR n n n
n n
= (cid:88) 1 (cid:104) η(x ,t ,y )+γ(x ,t )(cid:105)2 +Qˆ(r ) (48)
N n n n n n DR
n
wherethefirsttermisindependentfromτˆandthuscanbeomittedforrankingpurposes.
B.7 PROOFOFPROPOSITION3.12
Proof. Firstweprovethezero-meanproperty:
E[(1−2T)m(X)τˆ(X)] = E[m(X)τˆ(X)]E[1−2T] (49)
= E[m(X)τˆ(X)]·0 (50)
= 0 (51)
ItfollowsthatE[Qˆ ]=E[Qˆ]=Q.
R
Next,notethatwhenE =E =e˜(x)=0.5
1 0
Qˆ = 1 (cid:88)(cid:2) τˆ2(x )+4(1−2t )y τˆ(x )(cid:3) (52)
N n n n n
n
Itfollowsthat
L (τˆ) =
1 (cid:88)(cid:104)
((y −m˜(x ))−(t −e˜(x ))τˆ(x
))2(cid:105)
(53)
R N n n n n n
n
= 1 (cid:88)(cid:2) (y −m˜(x ))2+(t −e˜(x ))2τˆ2(x )−2(y −m˜(x ))(t −e˜(x ))τˆ(x )(cid:3) (54)
N n n n n n n n n n n
n
1 (cid:88) 1 (cid:88)(cid:104) (cid:105)
= (y −m˜(x ))2+ (t −e˜(x ))2τˆ2(x )−2(y −m˜(x ))(t −e˜(x ))τˆ(x ) (55)
N n n N n n n n n n n n
n n
1 (cid:88) 1 (cid:88)(cid:104)1 (cid:105)
= (y −m˜(x ))2+ τˆ2(x )+(y −m˜(x ))(1−2t )τˆ(x ) (56)
N n n N 4 n n n n n
n n
1 (cid:88) 1 (cid:88)(cid:104) (cid:105)
= (y −m˜(x ))2+ τˆ2(x )+4(y −m˜(x ))(1−2t )τˆ(x ) (57)
N n n 4N n n n n n
n n
(cid:40) (cid:41)
1 (cid:88) 1 (cid:88)(cid:104) (cid:105) (cid:88)
= (y −m˜(x ))2+ τˆ2(x )+4y (1−2t )τˆ(x ) − 4m˜(x )(1−2t )τˆ(x ()58)
N n n 4N n n n n n n n
n n n
= 1 (cid:88) (y −m˜(x ))2+ 1 Qˆ(r ) (59)
N n n 4 R
n
wherethefirsttermisaconstantwithoutimpactonranking.
B.8 PROOFOFTHEOREM3.13
Proof. Basedontheassumptions,itiseasytoseethattherandomvariableη(X,T,Y)isbounded.
Combining this with the boundedness of τˆ(x) we get q(x,t,y|τ) = τˆ2(x)−2τˆ(x)η(x,t,y) is
bounded. Itfollowthatq(r;τˆ)=q+risalsoboundedandthushavefiniteexpectationandfinite
variance. Letusdenoteitsvarianceasσ2(r,τˆ). ByLindeberg–LévyCLT,thesamplemeanQˆ(r;τˆ)
√
convergesindistributiontoitsexpectationQ: N(Qˆ(r;τˆ)−Q)→N(0,σ2(r,τˆ))
18C DETAILED CONFIGURATION OF CATE ESTIMATION MODELS
We train sixteen CATE estimation models listed in Table 3. This includes two S-learners, two
T-learners, two R-learners, two Doubly Robust learners, four Double ML learners, a causal tree
(forest)learner,andonerepresentationlearninglearner,discussedinSection2.
We hope the selection covers mainstream CATE estimation methods. We include meta-learner
(e.g.,SandTlearnersinKünzeletal.(2019))inourempiricalstudy,becausetheyrepresentthe
outcomepredictionstrategy,arguablythemostsimpleanddirectmethodsforcausalinference. We
includeDoubleMLmethods(Chernozhukovetal.,2017)becausetheyrepresenteconometric/semi-
parametricviewofcausalinference,aswellastherecentdevelopmentoforthogonalanddebiased
MachineLearning. WeincludeDoublyRobustlearners(Kennedy,2023)becausetheyrepresent
InversePropensityWeighting(Robinsetal.,1994),aclassiccausalinferencetechnique. Weinclude
causalforestbecausetheyareoneoftheearliestML-basedworkwiththeoreticalguarantee. Finally,
weincludeDragonNettorepresentrecenttrendusingdeeplearningandrepresentationlearningfor
causalinference.
OurlimitedresourcepreventsusfromincludingmoreCATEestimationmethods. Asaresultwedo
notclaimthislisttobecompleteor“optimal”. Duetoresourceconstraint,wewereunabletoinclude
morevariationsofdeeplearningmodelsfollowingShalitetal.(2017),causaltreemodelsfollowing
Atheyetal.(2018),orGaussianProcessmodelsfollowingAlaa&vanderSchaar(2017). Thelist
maynotbe“optimal”becausesomeofthemodelingapproachesarerelated,mostnotablybetween
DoubleMachineLearningandRlearners.
Table3: CATEestimationModels
CATEestimationMethod BaseLearner ModelCode
DRlearner RidgeRegression dr.ridge.cv
DRlearner XGBoost dr.xgb.cv
Rlearner RidgeRegression r.ridge.cv
Rlearner XGBoost r.xgb.cv
Slearner RidgeRegression s.ridge.cv
Slearner XGBoost s.xgb.cv
Slearner RidgeRegression s.ext.ridge.cv
Slearner XGBoost s.ext.xgb.cv
Tlearner RidgeRegression t.ridge.cv
Tlearner XGBoost t.xgb.cv
DoubleML LinearRegression dml.linear
DoubleML Lasso dml.lasso
DoubleML ElasticNet dml.elastic
DoubleML XGBoost dml.xgb
GeneralizedCausalForest RandomForest cforest
RepresentationLearning NeuralNet dragon.nn
Notethat,s.ext.xgb.cvands.ext.ridge.cvmodelsarevariantsofSlearnerswherethe
interactionX·T areconstructedexplicitlyasmodelinputs.
WeusecodebaseinCurth(2023)formodelestimationandevaluation.Outofthesixteenmodelslisted
inTable3,eightmeta-learners(S-learners,T-learners,R-learners)andtwoDoublyRobustlearners
directlycomefromCurth(2023)implementation. FollowingCurth&vanderSchaar(2023),weuse
twobaselearners,linearregression(implementedasansklearnRidgeCVobject),andXGBoost
(implemented as an XGBoost XGBRegressor object with grid search implemented by sklearn
GridSearchCV).SeeCurth(2023)fordetails. ThefiveDoubleMachineLearninglearnersare
implementedusingLinearDMLandNonParamDMLclassesinEconMLpackage,usingsklearn
GradientBoostingRegressorasthepotentialoutcomelearnerandrespectivebaselearneras
theresidualmodellearner.
AlltreatmentpropensityestimatorsareimplementedusingsklearnRandomForestClassifier
class;weclippropensityoutputsbetween0.05and0.95.
19D SECTION 4.3 SUPPLEMENTAL DETAILS
D.1 SEMI-SYNTHETICDATASETGENERATIONDETAILS
Wefollowthestepsbelowtotransformrawcovariatesintofeaturex:
• Applyone-hotencodingonallcategoricalcovariates
• Linearlyscaleallfloatcovariatesbetween0and1
• Columestackallcovariatestogenerateafeaturevector
• Iffeaturevectorhasmorethan100elements,randomlyselect100.
Wefollowthestepsbelowtogeneratesyntheticoutcome:
• Generatetworandomvectorsβ andβ withdiscretevaluesof[0,1,2,3,4]anddiscrete
0 1
probabilityof[0.5,0.2,0.15,0.1,0.05]
• Computetransformedfeatureandoutcomeusingtheoneoffollowingthreeapproaches:
- Linear. First compute z (x) = x,z (x) = ex then generate µ (x) = βTz (x) and
0 1 0 0 0
µ
1
=eβ 1Tz1(x)
- Interaction. First compute z (x) = [x x ,x x ,...,x x ] and z (x) =
0 0 1 1 2 D−1 0 1
[x x ,x x ,...,x x ]thengenerateµ (x)=βTz (x)andµ =βTz (x)
0 2 1 3 D−1 1 0 0 0 1 1 1
- Sine. First compute z (x) = [x x ,x x ,...,x x ] and z (x) =
0 0 1 1 2 D−1 0 1
[x x ,x x ,...,x x ]thengenerateµ (x)=cos(βTz (x))andµ =sin(βTz (x))
0 2 1 3 D−1 1 0 0 0 1 1 1
• Scaleµ (x)andµ (x)tohavezeromeanandunitstandarddeviation
0 1
• generatey = µ (x)+N(0,1)andy = µ (x)+N(0,1)+τ,whereτ isnowtheATE
0 0 1 1
estimate.
Wegeneratesynthetictreatmentasfollows:
• Generaterandomvectorβ
T
• CalculatePr(T|X =x)= 1
1+eβTx+1.
• SampleT usingthePr(t|x)
D.2 AGREEMENTBETWEENMODELSELECTIONCRITERIAANDORACLE
mrr prec1 spearman
1.0 1.0 1.0 oracle
Q̂
0.9 0.9 0.9 Q̂(rLI)
0.8 0.8 0.8 Q̂(rR) with lr base learner
Q̂(rR) wi(h xgb base learner
0.7 0.7 0.7
Q̂(rDR) wi(h lr base learner
0.6 0.6 0.6 Q̂(rDR) wi(h xgb base learner
O)(come predic(ion acc)racy
0.5 0.5 0.5 MSE agains( S-learner (xgb)
MSE agains( S-learner (lr)
0.4 0.4 0.4
MSE agains( T-learner (xgb)
0.3 0.3 0.3 MSE agains( T-learner (lr)
Qini
0.2 0.2 0.2 MSE agains( ATE
Calibra(ion Score
0.1 0.1 0.1
0.0 0.0 0.0
1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k
Figure2: Interactiontransformation;τ =2.0
20mrr prec1 spearman
1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8
0.7 0.7 0.7
0.6 0.6 0.6
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
0.0 0.0 0.0
1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k
oracle Q̂(rR) with lr base learner Q̂(rDR) wi(h xgb base learner MSE agains( S-learner pl)g-in (lr base learner) MSE agains( T-learner pl)g-in (lr base learner) MSE agains( ATE
Q̂ Q̂(rR) wi(h xgb base learner O)(come predic(ion acc)racy MSE agains( T-learner pl)g-in (xgb base learner) Qini Calibra(ion Sscore
Q̂(rLI) Q̂(rDR) wi(h lr base learner MSE agains( S-learner pl)g-in (xgb base learner)
Figure3: Sinetransformation;τ =0.5
mrr prec1 spearman
1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8
0.7 0.7 0.7
0.6 0.6 0.6
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
0.0 0.0 0.0
1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k
oracle Q̂(rR) with lr base learner Q̂(rDR) wi(h xgb base learner MSE agains( S-learner pl)g-in (lr base learner) MSE agains( T-learner pl)g-in (lr base learner) MSE agains( ATE
Q̂ Q̂(rR) wi(h xgb base learner O)(come predic(ion acc)racy MSE agains( T-learner pl)g-in (xgb base learner) Qini Calibra(ion Sscore
Q̂(rLI) Q̂(rDR) wi(h lr base learner MSE agains( S-learner pl)g-in (xgb base learner)
Figure4: Lineartransformation;τ =0.5
21E DATASET INTRODUCTION
WeusetwelveRCTdatasetsinSection4.3and4,listedinTable4. Inthissectionwediscussthe
rationaleofdataselection,andprovideabriefintroductiontoindividualdatasetsanddatahandling.
Thegoalforthedatasetselectionistoensurethat,collectively,theybetterrepresentrealapplications
ofcausalinferencethanthosedatasetsstudiedbycurrentstateoftheart(e.g,Curth&vanderSchaar
(2023);Mahajanetal.(2023);Nealetal.(2021)): includingIHDP,ACIC,andLaLonde. Weachieve
thatbyapplyingthefollowingfourfactorswhenselectingthedatasets:
• Real-worldheterogeneity. Weselectdatasetscollectedfromreal-world,andforgodatasets
withsimulatedoutcomes,toallowevaluationperformanceofCATEestimatoronreal-world
heterogeneity. Incomparison, simulatedoutcomeonIHDPorACICdonotachievethe
samerigor.
• Datasetsize. wepreferlargedatasetstoallowtheasymptoticpropertyofQˆ tokickin. The
smallestintheselectionsandercockhas 19,000samples.
• Diversity. Thedatasetsarecuratedtocoveradiversesources. Theydifferindomains(e.g.,
marketingincriteoandhilstrom,consumerbehaviorinferman,medicalsciencein
sandercock,andsociologyandpoliticalscienceinGSSdatasets),geography(GSSfrom
theUnitedStates,fermanfromBrazil,sandercockfromEurope,andcriteofrom
Russia),andformofexperiments(traditionalRCTinsandercock,onlineA/Btestingin
criteoandhillstrom,andfieldsurveyinGSS)
• Priorwork. Weselectdatasetspreviouslystudiedbycausalinferenceandrelatedliterature.
ForexamplecriteoisusedforupliftmodelinginDiemertetal.(2021);thenatfareis
usedforregressionadjustmentinWageretal.(2016)
Table4: RCTDatasets
Dataset Samples Treatment% Features References
criteo 13,979,592 85% 12 Diemertetal.(2021)
ferman 103,116 82% 9 Ferman(2015)
hillstrom 64,000 67% 12 Hillstrom(2008)
sandercock 19,435 50% 24 ISTCollaborativeGroup&Sandercock(1997)
nataid 51,957 50% 20 Davernetal.(2023)
natarms 51,987 50% 20 Davernetal.(2023)
natcity 51,915 50% 20 Davernetal.(2023)
natcrime 51,977 50% 20 Davernetal.(2023)
natdrug 51,961 50% 20 Davernetal.(2023)
nateduc 52,017 50% 20 Davernetal.(2023)
natenvir 52,027 50% 20 Davernetal.(2023)
natfare 51,993 50% 20 Davernetal.(2023)
Criteo dataset Diemert et al. (2021) captures advertising related online shopping behavior for
13,979,592 web users (identified by a browser cookie) in RCT. Each user is randomly assigned
to either treatment or control group. Pre-assignment user activities before assignment is used to
constructcovariates. Ifauserisintreatment,theyaresubjecttoanadexposure;iftheyareincontrol
group,theyareexposetothead. Thedatasettracksmultipleoutcomessuchasvisitsandconversion.
Weusevisitastheoutcomeinthecurrentanalysis.
HillstromdatasetHillstrom(2008)containsemailmarketingrelatedactivityfor64,000shoppers
whohadpurchaserecordswithinayear. Throughrandomization,onethirdoftheshoppersreceive
a marketing e-mail campaign featuring Men’s merchandise; one third receive an email featuring
women’s;andthelastonethirdreceivednomarketingemail. Covariatesincludepastpurchasehistory,
gender,geolocation,etc. Inthecurrentpaper,wecombinethetwogroupswhoreceivemarketing
emailintoonetreatmentgroup;theremaininggroup(whoreceivesnomarketinemail)isthecontrol
group. Weusevisitastheoutcomevariable.
22SandercockdatasetISTCollaborativeGroup&Sandercock(1997)includesdataon19,435patients
with acute stroke. Patients in treatment group are treated with aspirin; patients in control group
are not. Covariates include age, gender, and other medical information. The binary outcome is
whetherpatientisdeadordependentonotherpeopleforactivitiesofdailylivingatsixmonthsafter
randomisation.
FermandatasetFerman(2015)includesshoppingactivityrelatedtocreditcardpaymentplanon
103,116customersofaBraziliancreditcompany. Customersarerandomlyassignedintothreegroups:
34,743customersinthefirstgroupwereofferedamenuofpaymentplanswithinterestrateequalto
6:39%,49,573customersinsecondgroupwereofferedplanswithinterestrateequalto9:59%,and
thethirdgroupof18,800customersdidnotreceiveanypaymentplanoffer. Theoutcomeiswhether
thecustomerdefaultswithin12monthsaftertheoffer. Wecombinethefirstandsecondgroupinto
onetreatmentgroup.
GSSdatasetsincluderesponsestoeightquestionsfrommorethan50,000respondentssurveyedby
TheGeneralSocialSurvey(GSS)Davernetal.(2023)between1986and2022;responsetoeach
questionconsistutesaRCTdataset. GSSisanannualsociologicalsurveycreatedin1972bythe
NationalOpinionResearchCenter(NORC)attheUniversityofChicago. Itcollectsinformation
biannually and keeps a historical record of the concerns, experiences, attitudes, and practices of
residentsoftheUnitedStates. GSSSurveyregularlyincluderandomizedwordingexperimentsto
captureheterogeneityinrespondent’sopiniononsocialissues. Foragivenrandomizedquestion,
thequestionvariationformsdifferenttreatmentarms,theanswertothequestionformstheoutcome.
GSSalsocollectshundredsofhigh-qualitydemographicvariables,capturingdemographic,work,
family and spouse, household, racial, and region related information. These variables become
thepre-treatmentcovariates. Weuseeightwordingexperiments(nataid,natarms,natcity,
natcrime,natdrug,nateduc,natenvir,natfare))toconstructthebinaryoutcome. Its
valueisequalto1ifandonlyifwhenarespondentanswers“toomuch”toaquestion,and0otherwise.
F SECTION 4 ADDITIONAL DETAILS ON OBSERVATIONAL SAMPLING
ForeveryoriginaldatasetinTable4,wevarythreeparameterswhengeneratingD : firstwesetthe
est
estimationdatasetsizetobeoneofthefollowingvalue[1000,2000,4000,8000],totestifcertain
modelsperformbetterwithmore(orless)data. Secondly,wesettheexpectedtreatment%tobeone
ofthefollowingvalues[0.1,0.5,0.9],totestifcertainmodelsaresensitivetotreatmentimbalance. 2
Third,weuseaMLPingeneratingassignmentmechanism,andsetthenumberofMLPlayerstobe
1,2,or3. Thistestsifmodelsaresensitivetononlinearityinassignmentmechanism. Weenumerate
allparametercombinations,leadingto4×3×3=36settings. Foreverysetting,wesampleD
est
andD jointly100times. Thisgives3,600pairsofD andD . Repeatingsameprocessfor
eval est eval
the12datasetsyields12×3,600=43,200datasets.
Wetrainmodelonsmallestimationdatasetswithselectionbias,andevaluatemodelperformance
onlargeunbiasedRCTdatasets. ThestartingpointisanRCTdatasetD. WefirstrandomlysplitD
intoD andD−D . WethensampleD−D togetestimationdataset: foreverysample
eval eval eval
(x,t,y),definearandomvariableK ∈{0,1}withPr(K =1|T =t,X =x)=G(t,x). Wekeep
then-thsampleinD ifandonlyifk = 1. G(t,x)isthebiasingfunctionsinceitintroduces
est n
selectionbiastotheoriginalRCTdataset. ThiscreatestheestimationdatasetD ,asubsetofD
est
withselectionbias. WeapplyanyCATEestimationmethodonD toobtainanCATEestimator
est
τˆ(x),anduseτˆ(x)onD tocomputeQˆ. Fig. 5illustratestheprocess. Notethat,inestimation
eval
dataset,thetreatmentisafunctionofcovariates; inevaluationdataset,thetreatmentisrandomly
generatedbasedonstandardbinarydistribution. Foreverydatasetandevaluationdatasetsize,we
repeatsimulation100times.
ThecompletealgorithmofcreatingD issummarizedbelow:
est
2Treatment%of0.9canbedifferentfrom0.1becausedifferentpotentialoutcomesmaybedifferentin
real-worlddatasets.
23Figure5: Overallapproach
Algorithm1CreatingestimationdatasetD
est
Input: RCTdatasetD
Input: functionG(t,x),0<G(t,x)<1
Output: ObservationaldatasetD
est
D =∅
est
for everysampleninDdo
SampleabinaryrandomvariableK withPr(K =1|X =x ,T =t )=G(t ,x )
n n n n n
IfK =1,addsamplentoD
n est
endfor
ReturnD
est
Notethat,thebiasingfunctionG(t,x)functiongeneratesthefollowingassignmentmechanismfor
D :
est
Pr(T =1|X =x,K =1) (60)
Pr(T =1,X =x,K =1)
= (61)
Pr(X =x,K =1)
Pr(T =1,X =x,K =1)
= (62)
Pr(X =x,K =1)
Pr(T =1)Pr(X =x)Pr(K =1|X =x,T =1)
= (63)
Pr(X =x)Pr(K =1|X =x)
Pr(T =1)Pr(X =x)Pr(K =1|X =x,T =1)
= (64)
Pr(X =x)(Pr(T =1)Pr(K =1|X =x,T =1)+Pr(T =0)Pr(K =1|X =x,T =0))
Pr(T =1)G(x,1)
= (65)
Pr(T =1)G(x,1)+Pr(T =0)G(x,0)
1
= (66)
1+ Pr(T=0)G(x,0)
Pr(T=1)G(x,1)
Asaresult,T isdependentonX,achievingselectionbiasonD .
est
24Notethat
Pr(X ≤x|K =1) = Pr(X ≤x|K =1,T =1)Pr(T =1)+Pr(X ≤x|K =1,T =0)Pr(T =(607))
Pr(x≤x,K =1,T =1) Pr(X ≤x,K =1,T =0)
= + (68)
Pr(K =1,T =1) Pr(K =1,T =0)
(cid:82)x (cid:82)x
f(x)G(x,1)dx f(x)G(x,0)dx
= 0 + 0 (69)
(cid:82)∞ (cid:82)∞
f(x)G(x,1)dx f(x)G(x,0)dx
0 0
(cid:82)x (cid:82)x
f(x)G(x,1)dx f(x)G(x,0)dx
= 0 + 0 (70)
E [G(X,1)] E [G(X,0)]
X X
wheref isdensityofX onΠ. Itfollowsthat
(cid:18) (cid:19)
G(x,1) G(x,0)
f (x)=f(x) + (71)
est E [G(X,1)] E [G(X,0)]
X X
ThecompleteCATEestimatorevaluationalgorithmissummarizedbelow.
Algorithm2SelectingbestCATEEstimator
Input: AlistofACATEestimationmodelsa=1,2,...,A
Input: RCTdatasetD
Input: BiasingfunctionG(t,x)
Output: a∗,1≤a∗ ≤Athebestperformingmodel
RandomlysplitDintoD andD
train eval
GenerateD fromD usingGasthebiasingfunction
est train
for modela,1≤a≤Ado
TrainaCATEestimatorτˆ (x)usingdataD
a est
Computeq oneverysample(x ,t ,y )∈D
n n n n eval
ComputeQˆ(τˆ ,D )
a eval
endfor
Returna∗ =argmin Qˆ(τˆ ,D )
a a eval
25G SECTION 4 RESULTS FOR ALL RCT DATASETS COMBINED
Model win share vs. dataset size: all
100%
s.xgb.cv
s.ridge.cv
dragon.nn
80% s.ext.ridge.cv
dml.elastic
dml.lasso
s.ext.xgb.cv
60%
r.ridge.cv
dr.ridge.cv
t.ridge.cv
40% dr.xgb.cv
cforest
t.xgb.cv
r.xgb.cv
20%
dml.xgb
dml.linear
0%
all 1k 2k 4k 8k
Training dataset size
Figure6: Modelwinsharebytrainingdatasetsize: allRCTdatasets
Model win share vs. mean treatment %: all
100%
s.xgb.cv
s.ridge.cv
dragon.nn
80% s.ext.ridge.cv
dml.elastic
dml.lasso
s.ext.xgb.cv
60%
r.ridge.cv
dr.ridge.cv
t.ridge.cv
40% dr.xgb.cv
cforest
t.xgb.cv
r.xgb.cv
20%
dml.xgb
dml.linear
0%
all 0.1 0.5 0.9
Mean treatment %
Figure7: Modelwinsharebytreatmentratio: allRCTdatasets
26
erahs
niW
erahs
niWModel win share vs. complexity: all
100%
s.xgb.cv
s.ridge.cv
dragon.nn
80% s.ext.ridge.cv
dml.elastic
dml.lasso
s.ext.xgb.cv
60%
r.ridge.cv
dr.ridge.cv
t.ridge.cv
40% dr.xgb.cv
cforest
t.xgb.cv
r.xgb.cv
20%
dml.xgb
dml.linear
0%
all 1 2 3
Complexity
Figure8: Modelwinsharebyassignmentmechanismcomplexity: allRCTdatasets
Win vs. degenerate rates: all
100% s.xgb.cv
s.ridge.cv
dragon.nn
s.ext.ridge.cv
80%
dml.elastic
dml.lasso
s.ext.xgb.cv
60% r.ridge.cv
dr.ridge.cv
t.ridge.cv
dr.xgb.cv
40%
cforest
t.xgb.cv
r.xgb.cv
20% dml.xgb
dml.linear
0.0% 5.0% 10.0% 15.0% 20.0% 25.0%
Win rate
Figure9: Winsharevs. degeneraterate,bymodel,allRCTdatasets
27
erahs
niW
etar
etarenegeDH SECTION 4 DATASET LEVEL STATISTICS
H.1 SUMMARYOFDATASETSPECIFICBEHAVIOR
Doesmodels’relativeperformancevarybyamountoftrainingdata,treatmentimbalance,orlevel
of nonlinearity in assignment mechanism? Looking at the aggregated results (Appendix G), we
foundmoderatefluctuationofmodelperformancewhenvaryingthesedrivers. Thisislikelybecause
variationsaveragesout. Datasetlevelstatisticsshowsadifferentpicture. Wepresentselectedresults
inthissectionandleavemoreinAppendixH.
Training dataset size can have large impact on model performance. On criteo, Win share of
R-learner(r.ridge.cv)increasefrom7%with1kestimationdata,to26%with8kestimation
data. SeeFigure10.
Model win share vs. dataset size: criteo
100%
s.xgb.cv
r.ridge.cv
dragon.nn
80% s.ridge.cv
s.ext.ridge.cv
s.ext.xgb.cv
dr.ridge.cv
60%
dml.elastic
dml.lasso
cforest
40% r.xgb.cv
t.ridge.cv
dr.xgb.cv
dml.linear
20%
dml.xgb
t.xgb.cv
0%
all 1k 2k 4k 8k
Training dataset size
Figure10: Trainingsizeoncriteo
Treatmentimbalancecanhavelargeimpactonmodelperformance. Onnatcity,winshareofDR
learneris40%withbalancedtreatment,and16%whentreatedratiois0.9. SeeFigure11.
Levelofnonlinearityinassignmentmechanism,wefound,haslimitedimpactonmodelperformance.
Thisispartly,wethink,becauseourpropensityestimator(RandomForestwithpropensityclipping)
isflexibleenoughtofitdifferentdegreesofnonlinearity. SeeFigure8inAppendix.
28
erahs
niWModel win share vs. mean treatment %: natcity
100%
dr.ridge.cv
dragon.nn
dml.elastic
80% s.ext.ridge.cv
dml.lasso
t.ridge.cv
s.xgb.cv
60%
r.ridge.cv
dr.xgb.cv
s.ridge.cv
40% t.xgb.cv
s.ext.xgb.cv
cforest
r.xgb.cv
20%
dml.xgb
dml.linear
0%
all 0.1 0.5 0.9
Mean treatment %
Figure11: treatment%onnatcity
29
erahs
niWH.2 CRITEO
Win vs. degenerate rates: criteo
100% s.xgb.cv
r.ridge.cv
90% dragon.nn
s.ridge.cv
80% s.ext.ridge.cv
s.ext.xgb.cv
dr.ridge.cv
70%
dml.elastic
dml.lasso
60%
cforest
r.xgb.cv
50%
t.ridge.cv
dr.xgb.cv
40%
dml.linear
dml.xgb
30% t.xgb.cv
20%
0.0% 2.5% 5.0% 7.5% 10.0% 12.5% 15.0%
Win rate
Figure12: Winsharevs. degeneratepercentage,bymodeloncriteo
Model win share vs. dataset size: criteo
100%
s.xgb.cv
r.ridge.cv
dragon.nn
80% s.ridge.cv
s.ext.ridge.cv
s.ext.xgb.cv
dr.ridge.cv
60%
dml.elastic
dml.lasso
cforest
40% r.xgb.cv
t.ridge.cv
dr.xgb.cv
dml.linear
20%
dml.xgb
t.xgb.cv
0%
all 1k 2k 4k 8k
Training dataset size
Figure13: Modelwinsharebyestimationdata,criteo
30
etar
etarenegeD
erahs
niWModel win share vs. mean treatment %: criteo
100%
s.xgb.cv
r.ridge.cv
dragon.nn
80% s.ridge.cv
s.ext.ridge.cv
s.ext.xgb.cv
dr.ridge.cv
60%
dml.elastic
dml.lasso
cforest
40% r.xgb.cv
t.ridge.cv
dr.xgb.cv
dml.linear
20%
dml.xgb
t.xgb.cv
0%
all 0.1 0.5 0.9
Mean treatment %
Figure14: Modelwinsharebytreatmentratio,criteo
Model win share vs. complexity: criteo
100%
s.xgb.cv
r.ridge.cv
dragon.nn
80% s.ridge.cv
s.ext.ridge.cv
s.ext.xgb.cv
dr.ridge.cv
60%
dml.elastic
dml.lasso
cforest
40% r.xgb.cv
t.ridge.cv
dr.xgb.cv
dml.linear
20%
dml.xgb
t.xgb.cv
0%
all 1 2 3
Complexity
Figure15: Modelwinsharebyassignmentmechanismcomplexity,criteo
31
erahs
niW
erahs
niWH.3 HILLSTROM
Win vs. degenerate rates: hillstrom
100% dragon.nn
s.ridge.cv
dml.elastic
dml.lasso
80%
s.ext.ridge.cv
s.xgb.cv
r.ridge.cv
60%
s.ext.xgb.cv
dr.ridge.cv
dr.xgb.cv
40% t.ridge.cv
r.xgb.cv
t.xgb.cv
dml.xgb
20%
cforest
dml.linear
0%
0.0% 5.0% 10.0% 15.0% 20.0%
Win rate
Figure16: Winsharevs. degeneratepercentage,bymodelonhillstrom
Model win share vs. dataset size: hillstrom
100%
dragon.nn
s.ridge.cv
dml.elastic
80% dml.lasso
s.ext.ridge.cv
s.xgb.cv
r.ridge.cv
60%
s.ext.xgb.cv
dr.ridge.cv
dr.xgb.cv
40% t.ridge.cv
r.xgb.cv
t.xgb.cv
dml.xgb
20%
cforest
dml.linear
0%
all 1k 2k 4k 8k
Training dataset size
Figure17: Modelwinsharebyestimationdata,hillstrom
32
etar
etarenegeD
erahs
niWModel win share vs. mean treatment %: hillstrom
100%
dragon.nn
s.ridge.cv
dml.elastic
80% dml.lasso
s.ext.ridge.cv
s.xgb.cv
r.ridge.cv
60%
s.ext.xgb.cv
dr.ridge.cv
dr.xgb.cv
40% t.ridge.cv
r.xgb.cv
t.xgb.cv
dml.xgb
20%
cforest
dml.linear
0%
all 0.1 0.5 0.9
Mean treatment %
Figure18: Modelwinsharebytreatmentratio,hillstrom
Model win share vs. complexity: hillstrom
100%
dragon.nn
s.ridge.cv
dml.elastic
80% dml.lasso
s.ext.ridge.cv
s.xgb.cv
r.ridge.cv
60%
s.ext.xgb.cv
dr.ridge.cv
dr.xgb.cv
40% t.ridge.cv
r.xgb.cv
t.xgb.cv
dml.xgb
20%
cforest
dml.linear
0%
all 1 2 3
Complexity
Figure19: Modelwinsharebyassignmentmechanismcomplexity,hillstrom
33
erahs
niW
erahs
niWH.4 FERMAN
Win vs. degenerate rates: ferman
100% s.xgb.cv
s.ext.xgb.cv
s.ridge.cv
dml.elastic
80%
dml.lasso
dragon.nn
r.xgb.cv
60% dr.ridge.cv
dr.xgb.cv
dml.xgb
cforest
40%
dml.linear
r.ridge.cv
s.ext.ridge.cv
20% t.ridge.cv
t.xgb.cv
0%
0% 20% 40% 60% 80%
Win rate
Figure20: Winsharevs. degeneratepercentage,bymodelonferman
Model win share vs. dataset size: ferman
100%
s.xgb.cv
s.ext.xgb.cv
s.ridge.cv
80% dml.elastic
dml.lasso
dragon.nn
r.xgb.cv
60%
dr.ridge.cv
dr.xgb.cv
dml.xgb
40% cforest
dml.linear
r.ridge.cv
s.ext.ridge.cv
20%
t.ridge.cv
t.xgb.cv
0%
all 1k 2k 4k 8k
Training dataset size
Figure21: Modelwinsharebyestimationdata,ferman
34
etar
etarenegeD
erahs
niWModel win share vs. mean treatment %: ferman
100%
s.xgb.cv
s.ext.xgb.cv
s.ridge.cv
80% dml.elastic
dml.lasso
dragon.nn
r.xgb.cv
60%
dr.ridge.cv
dr.xgb.cv
dml.xgb
40% cforest
dml.linear
r.ridge.cv
s.ext.ridge.cv
20%
t.ridge.cv
t.xgb.cv
0%
all 0.1 0.5 0.9
Mean treatment %
Figure22: Modelwinsharebytreatmentratio,ferman
Model win share vs. complexity: ferman
100%
s.xgb.cv
s.ext.xgb.cv
s.ridge.cv
80% dml.elastic
dml.lasso
dragon.nn
r.xgb.cv
60%
dr.ridge.cv
dr.xgb.cv
dml.xgb
40% cforest
dml.linear
r.ridge.cv
s.ext.ridge.cv
20%
t.ridge.cv
t.xgb.cv
0%
all 1 2 3
Complexity
Figure23: Modelwinsharebyassignmentmechanismcomplexity,ferman
35
erahs
niW
erahs
niWH.5 SANDERCOCK
Win vs. degenerate rates: sandercock
100% s.ridge.cv
s.xgb.cv
dml.lasso
dml.elastic
80% s.ext.xgb.cv
dragon.nn
cforest
r.xgb.cv
60%
dr.xgb.cv
s.ext.ridge.cv
dr.ridge.cv
r.ridge.cv
40%
t.xgb.cv
dml.linear
dml.xgb
t.ridge.cv
20%
0.0% 5.0% 10.0% 15.0% 20.0% 25.0%
Win rate
Figure24: Winsharevs. degeneratepercentage,bymodelonsandercock
Model win share vs. dataset size: sandercock
100%
s.ridge.cv
s.xgb.cv
dml.lasso
80% dml.elastic
s.ext.xgb.cv
dragon.nn
cforest
60%
r.xgb.cv
dr.xgb.cv
s.ext.ridge.cv
40% dr.ridge.cv
r.ridge.cv
t.xgb.cv
dml.linear
20%
dml.xgb
t.ridge.cv
0%
all 1k 2k 4k 8k
Training dataset size
Figure25: Modelwinsharebyestimationdata,sandercock
36
etar
etarenegeD
erahs
niWModel win share vs. dataset size: sandercock
100%
s.ridge.cv
s.xgb.cv
dml.lasso
80% dml.elastic
s.ext.xgb.cv
dragon.nn
cforest
60%
r.xgb.cv
dr.xgb.cv
s.ext.ridge.cv
40% dr.ridge.cv
r.ridge.cv
t.xgb.cv
dml.linear
20%
dml.xgb
t.ridge.cv
0%
all 1k 2k 4k 8k
Training dataset size
Figure26: Modelwinsharebytreatmentratio,sandercock
Model win share vs. complexity: sandercock
100%
s.ridge.cv
s.xgb.cv
dml.lasso
80% dml.elastic
s.ext.xgb.cv
dragon.nn
cforest
60%
r.xgb.cv
dr.xgb.cv
s.ext.ridge.cv
40% dr.ridge.cv
r.ridge.cv
t.xgb.cv
dml.linear
20%
dml.xgb
t.ridge.cv
0%
all 1 2 3
Complexity
Figure27: Modelwinsharebyassignmentmechanismcomplexity,sandercock
37
erahs
niW
erahs
niWH.6 NATAID
Win vs. degenerate rates: nataid
100% s.ridge.cv
dragon.nn
dml.lasso
dml.elastic
80%
r.ridge.cv
s.xgb.cv
dr.ridge.cv
60% s.ext.ridge.cv
dr.xgb.cv
s.ext.xgb.cv
40% t.ridge.cv
r.xgb.cv
dml.xgb
dml.linear
20%
cforest
t.xgb.cv
0%
0.0% 5.0% 10.0% 15.0% 20.0% 25.0%
Win rate
Figure28: Winsharevs. degeneratepercentage,bymodelonnataid
Model win share vs. dataset size: nataid
100%
s.ridge.cv
dragon.nn
dml.lasso
80% dml.elastic
r.ridge.cv
s.xgb.cv
dr.ridge.cv
60%
s.ext.ridge.cv
dr.xgb.cv
s.ext.xgb.cv
40% t.ridge.cv
r.xgb.cv
dml.xgb
dml.linear
20%
cforest
t.xgb.cv
0%
all 1k 2k 4k 8k
Training dataset size
Figure29: Modelwinsharebyestimationdata,nataid
38
etar
etarenegeD
erahs
niWModel win share vs. mean treatment %: nataid
100%
s.ridge.cv
dragon.nn
dml.lasso
80% dml.elastic
r.ridge.cv
s.xgb.cv
dr.ridge.cv
60%
s.ext.ridge.cv
dr.xgb.cv
s.ext.xgb.cv
40% t.ridge.cv
r.xgb.cv
dml.xgb
dml.linear
20%
cforest
t.xgb.cv
0%
all 0.1 0.5 0.9
Mean treatment %
Figure30: Modelwinsharebytreatmentratio,nataid
Model win share vs. complexity: nataid
100%
s.ridge.cv
dragon.nn
dml.lasso
80% dml.elastic
r.ridge.cv
s.xgb.cv
dr.ridge.cv
60%
s.ext.ridge.cv
dr.xgb.cv
s.ext.xgb.cv
40% t.ridge.cv
r.xgb.cv
dml.xgb
dml.linear
20%
cforest
t.xgb.cv
0%
all 1 2 3
Complexity
Figure31: Modelwinsharebyassignmentmechanismcomplexity,nataid
39
erahs
niW
erahs
niWH.7 NATARMS
Win vs. degenerate rates: natarms
100% s.xgb.cv
s.ridge.cv
dragon.nn
dml.elastic
80% dml.lasso
r.ridge.cv
s.ext.xgb.cv
dr.ridge.cv
60%
dr.xgb.cv
s.ext.ridge.cv
cforest
dml.linear
40%
dml.xgb
r.xgb.cv
t.ridge.cv
20% t.xgb.cv
0.0% 5.0% 10.0% 15.0% 20.0% 25.0% 30.0%
Win rate
Figure32: Winsharevs. degeneratepercentage,bymodelonnatarms
Model win share vs. dataset size: natarms
100%
s.xgb.cv
s.ridge.cv
dragon.nn
80% dml.elastic
dml.lasso
r.ridge.cv
s.ext.xgb.cv
60%
dr.ridge.cv
dr.xgb.cv
s.ext.ridge.cv
40% cforest
dml.linear
dml.xgb
r.xgb.cv
20%
t.ridge.cv
t.xgb.cv
0%
all 1k 2k 4k 8k
Training dataset size
Figure33: Modelwinsharebyestimationdata,natarms
40
etar
etarenegeD
erahs
niWModel win share vs. mean treatment %: natarms
100%
s.xgb.cv
s.ridge.cv
dragon.nn
80% dml.elastic
dml.lasso
r.ridge.cv
s.ext.xgb.cv
60%
dr.ridge.cv
dr.xgb.cv
s.ext.ridge.cv
40% cforest
dml.linear
dml.xgb
r.xgb.cv
20%
t.ridge.cv
t.xgb.cv
0%
all 0.1 0.5 0.9
Mean treatment %
Figure34: Modelwinsharebytreatmentratio,natarms
Model win share vs. complexity: natarms
100%
s.xgb.cv
s.ridge.cv
dragon.nn
80% dml.elastic
dml.lasso
r.ridge.cv
s.ext.xgb.cv
60%
dr.ridge.cv
dr.xgb.cv
s.ext.ridge.cv
40% cforest
dml.linear
dml.xgb
r.xgb.cv
20%
t.ridge.cv
t.xgb.cv
0%
all 1 2 3
Complexity
Figure35: Modelwinsharebyassignmentmechanismcomplexity,natarms
41
erahs
niW
erahs
niWH.8 NATCITY
Win vs. degenerate rates: natcity
100% dr.ridge.cv
dragon.nn
dml.elastic
80% s.ext.ridge.cv
dml.lasso
t.ridge.cv
s.xgb.cv
60%
r.ridge.cv
dr.xgb.cv
s.ridge.cv
40% t.xgb.cv
s.ext.xgb.cv
cforest
20% r.xgb.cv
dml.xgb
dml.linear
0%
0.0% 5.0% 10.0% 15.0% 20.0%
Win rate
Figure36: Winsharevs. degeneratepercentage,bymodelonnatcity
Model win share vs. dataset size: natcity
100%
dr.ridge.cv
dragon.nn
dml.elastic
80% s.ext.ridge.cv
dml.lasso
t.ridge.cv
s.xgb.cv
60%
r.ridge.cv
dr.xgb.cv
s.ridge.cv
40% t.xgb.cv
s.ext.xgb.cv
cforest
r.xgb.cv
20%
dml.xgb
dml.linear
0%
all 1k 2k 4k 8k
Training dataset size
Figure37: Modelwinsharebyestimationdata,natcity
42
etar
etarenegeD
erahs
niWModel win share vs. mean treatment %: natcity
100%
dr.ridge.cv
dragon.nn
dml.elastic
80% s.ext.ridge.cv
dml.lasso
t.ridge.cv
s.xgb.cv
60%
r.ridge.cv
dr.xgb.cv
s.ridge.cv
40% t.xgb.cv
s.ext.xgb.cv
cforest
r.xgb.cv
20%
dml.xgb
dml.linear
0%
all 0.1 0.5 0.9
Mean treatment %
Figure38: Modelwinsharebytreatmentratio,natcity
Model win share vs. complexity: natcity
100%
dr.ridge.cv
dragon.nn
dml.elastic
80% s.ext.ridge.cv
dml.lasso
t.ridge.cv
s.xgb.cv
60%
r.ridge.cv
dr.xgb.cv
s.ridge.cv
40% t.xgb.cv
s.ext.xgb.cv
cforest
r.xgb.cv
20%
dml.xgb
dml.linear
0%
all 1 2 3
Complexity
Figure39: Modelwinsharebyassignmentmechanismcomplexity,natcity
43
erahs
niW
erahs
niWH.9 NATCRIME
Win vs. degenerate rates: natcrime
100% t.ridge.cv
dr.ridge.cv
r.ridge.cv
s.ext.ridge.cv
80%
s.xgb.cv
dml.elastic
dml.lasso
60%
dragon.nn
dr.xgb.cv
s.ridge.cv
40% t.xgb.cv
s.ext.xgb.cv
r.xgb.cv
cforest
20%
dml.xgb
dml.linear
0%
0.0% 5.0% 10.0% 15.0% 20.0%
Win rate
Figure40: Winsharevs. degeneratepercentage,bymodelonnatcrime
Model win share vs. dataset size: natcrime
100%
t.ridge.cv
dr.ridge.cv
r.ridge.cv
80% s.ext.ridge.cv
s.xgb.cv
dml.elastic
dml.lasso
60%
dragon.nn
dr.xgb.cv
s.ridge.cv
40% t.xgb.cv
s.ext.xgb.cv
r.xgb.cv
cforest
20%
dml.xgb
dml.linear
0%
all 1k 2k 4k 8k
Training dataset size
Figure41: Modelwinsharebyestimationdata,natcrime
44
etar
etarenegeD
erahs
niWModel win share vs. mean treatment %: natcrime
100%
t.ridge.cv
dr.ridge.cv
r.ridge.cv
80% s.ext.ridge.cv
s.xgb.cv
dml.elastic
dml.lasso
60%
dragon.nn
dr.xgb.cv
s.ridge.cv
40% t.xgb.cv
s.ext.xgb.cv
r.xgb.cv
cforest
20%
dml.xgb
dml.linear
0%
all 0.1 0.5 0.9
Mean treatment %
Figure42: Modelwinsharebytreatmentratio,natcrime
Model win share vs. complexity: natcrime
100%
t.ridge.cv
dr.ridge.cv
r.ridge.cv
80% s.ext.ridge.cv
s.xgb.cv
dml.elastic
dml.lasso
60%
dragon.nn
dr.xgb.cv
s.ridge.cv
40% t.xgb.cv
s.ext.xgb.cv
r.xgb.cv
cforest
20%
dml.xgb
dml.linear
0%
all 1 2 3
Complexity
Figure43: Modelwinsharebyassignmentmechanismcomplexity,natcrime
45
erahs
niW
erahs
niWH.10 NATDRUG
Win vs. degenerate rates: natdrug
100% dragon.nn
s.ext.ridge.cv
s.xgb.cv
s.ridge.cv
80%
dml.lasso
dml.elastic
r.ridge.cv
60%
dr.ridge.cv
t.ridge.cv
s.ext.xgb.cv
40% dr.xgb.cv
cforest
dml.xgb
dml.linear
20%
r.xgb.cv
t.xgb.cv
0%
0.0% 5.0% 10.0% 15.0% 20.0%
Win rate
Figure44: Winsharevs. degeneratepercentage,bymodelonnatdrug
Model win share vs. dataset size: natdrug
100%
dragon.nn
s.ext.ridge.cv
s.xgb.cv
80% s.ridge.cv
dml.lasso
dml.elastic
r.ridge.cv
60%
dr.ridge.cv
t.ridge.cv
s.ext.xgb.cv
40% dr.xgb.cv
cforest
dml.xgb
dml.linear
20%
r.xgb.cv
t.xgb.cv
0%
all 1k 2k 4k 8k
Training dataset size
Figure45: Modelwinsharebyestimationdata,natdrug
46
etar
etarenegeD
erahs
niWModel win share vs. mean treatment %: natdrug
100%
dragon.nn
s.ext.ridge.cv
s.xgb.cv
80% s.ridge.cv
dml.lasso
dml.elastic
r.ridge.cv
60%
dr.ridge.cv
t.ridge.cv
s.ext.xgb.cv
40% dr.xgb.cv
cforest
dml.xgb
dml.linear
20%
r.xgb.cv
t.xgb.cv
0%
all 0.1 0.5 0.9
Mean treatment %
Figure46: Modelwinsharebytreatmentratio,natdrug
Model win share vs. complexity: natdrug
100%
dragon.nn
s.ext.ridge.cv
s.xgb.cv
80% s.ridge.cv
dml.lasso
dml.elastic
r.ridge.cv
60%
dr.ridge.cv
t.ridge.cv
s.ext.xgb.cv
40% dr.xgb.cv
cforest
dml.xgb
dml.linear
20%
r.xgb.cv
t.xgb.cv
0%
all 1 2 3
Complexity
Figure47: Modelwinsharebyassignmentmechanismcomplexity,natdrug
47
erahs
niW
erahs
niWH.11 NATEDUC
Model win share vs. complexity: nateduc
100%
s.xgb.cv
s.ext.xgb.cv
s.ridge.cv
80% dragon.nn
s.ext.ridge.cv
dml.elastic
dml.lasso
60%
r.ridge.cv
dr.ridge.cv
dr.xgb.cv
40% t.ridge.cv
cforest
dml.xgb
dml.linear
20%
r.xgb.cv
t.xgb.cv
0%
all 1 2 3
Complexity
Figure48: Winsharevs. degeneratepercentage,bymodelonnateduc
Win vs. degenerate rates: nateduc
100% s.xgb.cv
s.ext.xgb.cv
s.ridge.cv
dragon.nn
80%
s.ext.ridge.cv
dml.elastic
dml.lasso
60%
r.ridge.cv
dr.ridge.cv
dr.xgb.cv
40% t.ridge.cv
cforest
dml.xgb
20% dml.linear
r.xgb.cv
t.xgb.cv
0%
0% 10% 20% 30% 40% 50% 60% 70% 80%
Win rate
Figure49: Modelwinsharebyestimationdata,nateduc
48
erahs
niW
etar
etarenegeDModel win share vs. dataset size: nateduc
100%
s.xgb.cv
s.ext.xgb.cv
s.ridge.cv
80% dragon.nn
s.ext.ridge.cv
dml.elastic
dml.lasso
60%
r.ridge.cv
dr.ridge.cv
dr.xgb.cv
40% t.ridge.cv
cforest
dml.xgb
dml.linear
20%
r.xgb.cv
t.xgb.cv
0%
all 1k 2k 4k 8k
Training dataset size
Figure50: Modelwinsharebytreatmentratio,nateduc
Model win share vs. mean treatment %: nateduc
100%
s.xgb.cv
s.ext.xgb.cv
s.ridge.cv
80% dragon.nn
s.ext.ridge.cv
dml.elastic
dml.lasso
60%
r.ridge.cv
dr.ridge.cv
dr.xgb.cv
40% t.ridge.cv
cforest
dml.xgb
dml.linear
20%
r.xgb.cv
t.xgb.cv
0%
all 0.1 0.5 0.9
Mean treatment %
Figure51: Modelwinsharebyassignmentmechanismcomplexity,nateduc
49
erahs
niW
erahs
niWH.12 NATENVIR
Win vs. degenerate rates: natenvir
100% s.xgb.cv
s.ridge.cv
dragon.nn
dml.elastic
80%
dml.lasso
s.ext.xgb.cv
r.ridge.cv
60%
s.ext.ridge.cv
dr.ridge.cv
t.ridge.cv
40% dr.xgb.cv
cforest
dml.xgb
dml.linear
20%
r.xgb.cv
t.xgb.cv
0%
0.0% 5.0% 10.0% 15.0% 20.0% 25.0% 30.0% 35.0%
Win rate
Figure52: Winsharevs. degeneratepercentage,bymodelonnatenvir
Model win share vs. dataset size: natenvir
100%
s.xgb.cv
s.ridge.cv
dragon.nn
80% dml.elastic
dml.lasso
s.ext.xgb.cv
r.ridge.cv
60%
s.ext.ridge.cv
dr.ridge.cv
t.ridge.cv
40% dr.xgb.cv
cforest
dml.xgb
dml.linear
20%
r.xgb.cv
t.xgb.cv
0%
all 1k 2k 4k 8k
Training dataset size
Figure53: Modelwinsharebyestimationdata,natenvir
50
etar
etarenegeD
erahs
niWModel win share vs. mean treatment %: natenvir
100%
s.xgb.cv
s.ridge.cv
dragon.nn
80% dml.elastic
dml.lasso
s.ext.xgb.cv
r.ridge.cv
60%
s.ext.ridge.cv
dr.ridge.cv
t.ridge.cv
40% dr.xgb.cv
cforest
dml.xgb
dml.linear
20%
r.xgb.cv
t.xgb.cv
0%
all 0.1 0.5 0.9
Mean treatment %
Figure54: Modelwinsharebytreatmentratio,natenvir
Model win share vs. complexity: natenvir
100%
s.xgb.cv
s.ridge.cv
dragon.nn
80% dml.elastic
dml.lasso
s.ext.xgb.cv
r.ridge.cv
60%
s.ext.ridge.cv
dr.ridge.cv
t.ridge.cv
40% dr.xgb.cv
cforest
dml.xgb
dml.linear
20%
r.xgb.cv
t.xgb.cv
0%
all 1 2 3
Complexity
Figure55: Modelwinsharebyassignmentmechanismcomplexity,natenvir
51
erahs
niW
erahs
niWH.13 NATFARE
Win vs. degenerate rates: natfare
s.ext.ridge.cv
t.ridge.cv
80%
dr.ridge.cv
dml.elastic
dml.lasso
60% s.ext.xgb.cv
dr.xgb.cv
t.xgb.cv
r.ridge.cv
40% dragon.nn
s.ridge.cv
s.xgb.cv
cforest
20%
r.xgb.cv
dml.xgb
dml.linear
0%
0% 10% 20% 30% 40% 50% 60%
Win rate
Figure56: Winsharevs. degeneratepercentage,bymodelonnatfare
Model win share vs. dataset size: natfare
100%
s.ext.ridge.cv
t.ridge.cv
dr.ridge.cv
80% dml.elastic
dml.lasso
s.ext.xgb.cv
dr.xgb.cv
60%
t.xgb.cv
r.ridge.cv
dragon.nn
40% s.ridge.cv
s.xgb.cv
cforest
r.xgb.cv
20%
dml.xgb
dml.linear
0%
all 1k 2k 4k 8k
Training dataset size
Figure57: Modelwinsharebyestimationdata,natfare
52
etar
etarenegeD
erahs
niWModel win share vs. mean treatment %: natfare
100%
s.ext.ridge.cv
t.ridge.cv
dr.ridge.cv
80% dml.elastic
dml.lasso
s.ext.xgb.cv
dr.xgb.cv
60%
t.xgb.cv
r.ridge.cv
dragon.nn
40% s.ridge.cv
s.xgb.cv
cforest
r.xgb.cv
20%
dml.xgb
dml.linear
0%
all 0.1 0.5 0.9
Mean treatment %
Figure58: Modelwinsharebytreatmentratio,natfare
Model win share vs. complexity: natfare
100%
s.ext.ridge.cv
t.ridge.cv
dr.ridge.cv
80% dml.elastic
dml.lasso
s.ext.xgb.cv
dr.xgb.cv
60%
t.xgb.cv
r.ridge.cv
dragon.nn
40% s.ridge.cv
s.xgb.cv
cforest
r.xgb.cv
20%
dml.xgb
dml.linear
0%
all 1 2 3
Complexity
Figure59: Modelwinsharebyassignmentmechanismcomplexity,natfare
53
erahs
niW
erahs
niW