One Initialization to Rule them All: Fine-tuning via
Explained Variance Adaptation
FabianPaischer1* LukasHauzenberger1* ThomasSchmied1
BenediktAlkin1,3 MarcPeterDeisenroth2 SeppHochreiter1,3
1ELLISUnit,LITAILab,InstituteforMachineLearning,JKULinz,Austria
2UniversityCollegeLondon
3NXAIGmbH,Linz,Austria
paischer@ml.jku.at
Abstract
Foundation models (FMs) are pre-trained on large-scale datasets and then fine-
tunedonadownstreamtaskforaspecificapplication. Themostsuccessfuland
mostcommonlyusedfine-tuningmethodistoupdatethepre-trainedweightsvia
a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are
usuallyinitializedatrandomwithauniformrankdistributionacrossmodelweights.
Recentworksfocusonweight-driveninitializationorlearningofadaptiveranks
duringtraining. Bothapproacheshaveonlybeeninvestigatedinisolation,resulting
inslowconvergenceorauniformrankdistribution,inturnleadingtosub-optimal
performance. WeproposetoenhanceLoRAbyinitializingthenewweightsina
data-drivenmannerbycomputingsingularvaluedecompositiononminibatches
of activation vectors. Then, we initialize the LoRA matrices with the obtained
right-singularvectorsandre-distributeranksamongallweightmatricestoexplain
the maximal amount of variance and continue the standard LoRA fine-tuning
procedure. ThisresultsinournewmethodExplainedVarianceAdaptation(EVA).
WeapplyEVAtoavarietyoffine-tuningtasksrangingfromlanguagegeneration
andunderstandingtoimageclassificationandreinforcementlearning.EVAexhibits
fasterconvergencethancompetitorsandattainsthehighestaveragescoreacrossa
multitudeoftasksperdomain.
1 Introduction
Foundationmodels(Bommasanietal.,2021,FMs)areusuallytrainedonlarge-scaledataandthen
fine-tuned towards a particular downstream task. This training paradigm has led to significant
advancementsintherealmoflanguagemodeling(OpenAI,2023;Touvronetal.,2023a;Reidetal.,
2024), computer vision (Dehghani et al., 2023; Oquab et al., 2023), and reinforcement learning
(Brohan et al., 2023; Zitkovich et al., 2023). With an increasing number of model parameters,
the process of fine-tuning becomes prohibitively expensive. This results in the need for efficient
alternativestofine-tuningallparametersofthepre-trainedmodel.
Parameter-efficientfine-tuning(PEFT)approachesarecommonlyusedasaneffectivealternativeto
fullfine-tuning(FFT).PEFTmethodsmodifythepre-trainedmodelbyintroducingasmallnumber
ofnewtrainableparameters,whilethepre-trainedweightsremainfrozen. Thisleadstoasubstantial
reductionincomputationalcost,bothintermsoftimeandspace. Aparticularlysuccessfulapproach,
LoRA(Huetal.,2022),introducesnewweightsintheformofalow-rankdecompositionforeach
weightmatrixinthepre-trainedmodel. Aftertraining,thenewweightscanbereadilymergedinto
*Equalcontribution
4202
tcO
9
]GL.sc[
1v07170.0142:viXraFigure1: Left: EVAperformssingularvaluedecompositiononactivationvectorsforthefirstfew
minibatchestoobtainasuitableinitializationfortheLoRAmatrixA. Right: AfterinitializingA,
weallocaterankstomaximizetheexplainedvariancethroughoutthemodelandcontinuethestandard
LoRAfine-tuningprocedure,whereW iskeptfrozenandonlyAandBaretrained.
thepre-trainedweightswithoutanyadditionalinferencelatency. Recentresearchhasexploredtwo
mainavenuesforenhancingLoRA:weight-driveninitializationandadaptiverankallocationduring
training(seeTable1). Whileweight-driveninitializationmethodshaveshownpromise,theytypically
relyonauniformrankdistributionacrosspre-trainedweights. Further,theyareconstrainedtothe
informationstoredinthepre-trainedweights. Finally,existingadaptiverankallocationtechniques
initializelow-rankmatricesrandomly.
WeproposeanewmethodbasedonLoRAthatcombinesadaptiverankallocationwithdata-driven
initializationbyleveraginginformationfromthedownstreamtaskathand. Certainactivationpatterns
ofFMshaveshowntobecrucialformodelperformance(Sunetal.,2024). Therefore,weleverage
minibatchesofactivationscomputedondownstreamdatatoinitializeLoRAweights. Tothisend,
wepropagateminibatchesofthefine-tuningdatathroughthemodelandcomputethesingularvalue
decomposition(SVD)onactivationvectorstoobtaintheright-singularvectors. Wethensorttheright-
singularvectorsindescendingorderaccordingtothevariancetheyexplain. Finally,weleveragethe
top-kcomponentsaccordingtoagivenrankbudgetforinitializingLoRA.Thisresultsinaneffective
initialization,that(i)isdata-drivenbyleveraginginformationfromthedownstreamtask,and(ii)
allocatesrankstopre-trainedweightstomaximizetheexplainedvariancethroughoutthemodel. We
calltheresultingmethodEVA,whichisshortforExplainedVarianceAdaptation. Importantly,this
procedurecanbeperformedwithinthefirstfewminibatchesofLoRAfine-tuningwithoutsignificant
computationaloverhead.
WedemonstratethebenefitsofEVAonanarrayofdownstreamtasks,namelylanguagegeneration
andunderstanding,imageclassification,andreinforcementlearning(RL).EVAconsistentlyimproves
average performance across a multitude of tasks on each domain compared to LoRA and other
recentlyproposedinitializationorrankredistributionmethods. Forlanguagegeneration,wefine-tune
7B-9B parameter language models on math and reasoning tasks, where EVA attains the highest
averageperformance. Further,onasetoflanguageunderstandingtasks,EVAimprovestheaverage
performancecomparedtocompetitors. Onimageclassificationwefine-tuneapre-trainedvision
transformer (Dosovitskiy et al., 2021) on a set of 19 diverse tasks. We find that EVA attains the
highestaveragescoreandimprovesoverLoRAandestablishedextensionsthereof,withmostgains
onin-domaindata. ForourRLexperimentsweconductfine-tuningoncontinuouscontroltasksand
findthatEVAsignificantlyexceedsperformanceofLoRAandevenexceedsperformanceoffull
fine-tuning(FFT)whencombinedwithDoRA(Liuetal.,2024a).Finally,weconductablationstudies
todemonstratethatthecombinationofdirectionandscaleofEVAleadstothebestperformance.
2 RelatedWork
LoRA(Huetal.,2022)hassparkedwidespreadinterestinleveraginglow-rankdecompositionsfor
fine-tuningduetoitssimplicity. BuildingonthesuccessofLoRA,anumberofothervariantshave
beenproposed(Kopiczkoetal.,2024;Zietal.,2023;Babakniyaetal.,2023;Dettmersetal.,2023;Li
2Table1: ComparisonofEVAtoexistinginitializationschemesforLoRA.Existingworkseitherfocus
onweight-driveninitializationoradaptiverankallocation. EVAcombinesdata-driveninitialization
withadaptiverankallocationtoenhanceconvergenceanddownstreamperformance.
Method Initialization Adaptiveranks
LoRA(Huetal.,2022) Random ✗
AdaLoRA(Zhangetal.,2023a) Random ✓
PiSSA(Mengetal.,2024) Weight-driven ✗
OLoRA(Büyükakyüz,2024) Weight-driven ✗
EVA(Ours) Data-driven ✓
etal.,2023;Nikdanetal.,2024;Liuetal.,2024a;Zhangetal.,2023a;Hayouetal.,2024b;Chavan
etal.,2023). ThemostsimilarvariantstoEVAareAdaLoRA(Zhangetal.,2023a)andPiSSA(Meng
etal.,2024). AdaLoRAadaptivelyaltersthenumberofranksforLoRAmatricesduringfine-tuning.
Othermorerecentapproacheslearngatestoswitchranksonoroffduringfine-tuning(Liuetal.,
2024b;Meoetal.,2024). Incontrast,thedata-driveninitializationallowsEVAtoredistributeranks
for each LoRA matrix prior to fine-tuning by leveraging a small number of minibatches of data.
PiSSAinitializesLoRAmatrixAviathetopsingularvectorsofthepre-trainedweightmatrices.
Contrary,EVAinitializesAviatheright-singularvectorsofminibatchesofactivationvectorsand
isthereforedata-driven. SinceEVAmostlyconstitutesaneffectiveinitialization,itcanbereadily
pluggedintootherLoRAvariantssuchasDoRA(Liuetal.,2024a).
Initialization of LoRA matrices Common initialization schemes forneural networks(He et al.,
2015;Glorot&Bengio,2010)weredesignedtostabilizetrainingofdeepneuralnetworksbasedon
activationfunctionsanddepth. InthecontextofPEFT,Huetal.(2022)andLiuetal.(2022)explored
data-driveninitializationbyeitherpre-trainingonadifferenttaskfirst,orbyunsupervisedpre-training
onthetaskathand. Contrary,EVAdoesnotrequireanygradientupdatesteps,thereforeitismuch
moreefficient. Similarly,Nikdanetal.(2024)utilizeawarm-upstageinLoRAfine-tuning,where
gradientswithrespecttoLoRAweightsareusedtoinitializeasparsematrixforsparseadaptation
(Sungetal.,2021)incombinationwithLoRA.Alternatively,Babakniyaetal.(2023)initializeLoRA
matricesusingSVDonweightmatricesobtainedafterafewstepsoffullfine-tuningforfederated
learning with heterogeneous data. Meng et al. (2024) use the main directions of the pre-trained
weightstoinitializetheLoRAmatrices. Incontrast,EVAtakesadata-drivenapproachtoinitialize
theLoRAmatrices,insteadofrelyingoncomponentsofthepre-trainedweights. Similarinitialization
schemes were proposed by Mishkin & Matas (2016); Krähenbühl et al. (2016) for training deep
networksfromscratch.
Increasing efficiency of LoRA Several works have investigated how to further break down the
complexityofLoRAforfine-tuningFMs. Kopiczkoetal.(2024)decreasethememorycomplexity
ofLoRAbyinitializingbothAandB atrandomandkeepingthemfrozenwhilemerelytraining
newly-introducedscalingvectors. Thisway,onlyrandomseedsforinitializingAandBneedtobe
stored. Anotherfruitfulavenueisquantization(Dettmersetal.,2022),whichhasbeensuccessfully
combinedwithLoRAfine-tuning(Dettmersetal.,2023). OtherLoRAvariants(Nikdanetal.,2024;
Valipouretal.,2023;Mengetal.,2024)alsoprovidequantizedversions. Ithasalsobeenshownthat
properinitializationforquantizationresultsinimprovedfine-tuningperformance(Lietal.,2023).
3 Method
EVAaimsatinitializingLoRAweightsinadata-drivenmannerbyleveragingdatafromthedown-
streamtask. SinceEVAbuildsonlow-rankdecompositionofweightmatricesasinLoRA(Huetal.,
2022), we first briefly explain LoRA in Section 3.1. In Section 3.2, we describe how we obtain
aneffectiveinitializationforthelow-rankdecompositionofLoRAmatricesviaSVDonactivation
vectors. Thisenablesanadaptiveassignmentofranksacrossalllayerstomaximizetheexplained
variancethroughoutthepre-trainedmodelWeexplainthisinmoredetailinSection3.3.
3EVA Random PiSSA OLoRA
0.28 0.28 EVA PiSSA OLoRA Random
0.26 0.8
0.26
0.24 0.7
0.24 0.22 0.6
0.20
0.22 0.5
0.18
0.20 0K 2K 4K 0.4
0.3
0.18
0.2
0.16
0.1
0.14 0K 2K 4K 6K 8K 10K 12K 14K 16K 18K 20K 22K 0.0
Update Steps
Figure2:Left:Traininglossforfine-tuningLlama-3.1-8BontheMetaMathQAdataset.Wecompare
EVAtorecentlyproposedinitializationmethodsOLoRA,PiSSA,andrandominitialization(LoRA).
We show mean and standard deviation across three random seeds. Right: Mean and standard
deviationofgradientnormforEVA,PiSSA,OLoRAandRandominitializationofLoRAmatrices.
EVAexhibitssignificantlylargergradientnormleadingtofasterconvergence.
3.1 Low-RankAdaptation(LoRA)
LoRAaddsnewtrainableweightswhicharecomputedviaanouterproductoflow-rankmatrices(Hu
etal.,2022). Thisismotivatedbythelowintrinsicdimensionalityoflanguagemodels(Aghajanyan
etal.,2021)andreliesontheassumptionthatthegradientsduringfine-tuningarealsooflowrank
(Gur-Arietal.,2018;Zhangetal.,2023b;Gauchetal.,2022). Inthefollowing,weexplainLoRA
inmoredetail. Letx∈Rd×1betheinputtoapre-trainedweightmatrixW ∈Rk×d. Then,LoRA
introducesnewweightmatricesAandBasalow-rankdecomposition
h=Wx+BAx, (1)
whereB ∈Rk×r andA∈Rr×d. Therankrisahyperparameterwithr ≪k. Duringfine-tuning,
W remainsfrozenandonlyAandB areupdated. UsuallyB isinitializedwithzeros,suchthat
fine-tuningstartsfromthepre-trainedmodel. Aisusuallyinitializedatrandom. Additionally,Hu
etal.(2022)introduceahyperparamterαwhichisusedtoscaleBAxby α.
r
3.2 Data-drivenInitializationofLow-RankAdaptation
Our aim is to find an effective initialization for the low-rank matrix A in a data-driven manner
to maximize performance on the downstream task. To this end, we perform SVD on batches of
activation vectors X ∈ Rb×d to obtain the right-singular values, which constitute the directions
that capture most of the variance. This procedure is done during the initial training stage where
wepropagateminibatchesofdatathroughthemodelandincrementallyupdatetheright-singular
vectors. More formally, we collect batches of activations Xi for N pre-trained weight matrices
Wi ∈{W0,W1,...,WN}thatwechoosetoupdatefine-tune. Subsequently,wecomputetheSVD
oneachXitoobtaintheright-singularvectorsvi andrespectivesingularvaluesσi as
j,: j
r
(cid:88)
Xi = ui σivi . (2)
:,j j j,:
j=1
Importantly,wecomputetheSVDincrementallyoneachminibatchoffine-tuningdataandupdate
vi aftereachforwardpassthroughthemodel. Aftereachstepwecheckwhethervi hasconverged.
:r,: :r,:
Tothisend,wemeasurethecolumn-wisecosinesimilaritybetweensubsequentcomputationsofv
:r,:
anddetermineconvergencebasedonathresholdτ. Iftheright-singularvalueshaveconverged,i.e.
cossim(vi,t−1,vi,t) ≥ τ ∀ 1 ⩽ j ⩽ r,weinitializeAi = vi andexcludethecorresponding
j,: j,: :r,:
weightmatrixfromsubsequentSVDcomputations. Wecontinuethisprocedureuntilallvi have
:r,:
converged.
4
ssoL
gniniarT
mroN
tneidarGThecomputationofSVDintroducescomputationaloverheadintheinitialtrainingstage. Sincewe
do not require gradient computation or storing of optimizer states, there is no overhead in terms
ofmemory. SVDhasatimecomplexityofO(min(b2d,bd2))whichcanbereducedtoO(k2b)for
k << d by randomly choosing k columns from X as introduced in Halko et al. (2011). Let T
bethenumberofminibatchesuntilallcomponentsareconvergedforN weightmatrices,thenthe
timecomplexityisO(NTk2b). Inotherwords,thecomplexityscaleslinearlywiththenumberof
weightmatricesandthenumberofminibatches. TospeedupthecomputationofSVD,weprovidean
implementationthatrunsentirelyonGPU.
3.3 AdaptiveRankAllocation
ThesingularvaluesobtainedbySVDprovideanestimateofthevariancethatisexplainedbytheir
components. Leveragingthisinformation,wecanredistributeranksacrossweightmatricesofthe
pre-trainedmodelsuchthatthemaximumamountofvarianceisexplained. Thiscanbedoneby
allocatingmorerankstolayersthatpropagatemoreinformation,i.e.,explainmorevariance. More
formally,thevarianceexplainedbyeachcomponentinvi isgivenbytheirexplainedvarianceratio
j,:
σi2
ξi = j , (3)
j (M −1)||σi||
1
where||·|| denotestheℓ norm,σiisavectorcontainingallrsingularvalues,andM isthetotal
1 1
numberofsamplesusedfortheincrementalSVDcomputation. Next,wesortthecomponentsvi for
j,:
eachweightmatrixindescendingorderaccordingtotheirexplainedvarianceratioξi. Finally,we
j
assignrankstopre-traiendweightsuntilwereachacertainrankbudget.
Additionally, we introduce a hyperparameter
ρ∈[1,∞)whichcontrolstheuniformityofthe
Algorithm1Fine-tuningviaEVA
rankdistribution. ρdeterminesthenumberof
ranksthatwecomputeduringSVDandincreas- Input: FMψ(·),ρ,rankr,datasetD
ingρallowsforanincreasinglyheterogeneous 1: whilenot all_converged(ψ)do
rankdistribution. Thatis,foreachWiwecom- 2: X ←ψ(next(D)) ▷getactivations
puterρcomponentsinitiallymeaningweobtain 3: V new,ξ ←SVD(X,ρr)
Nrρcomponentsintotal. Fortheredistribution 4: ifisclose(V old,v new)then
we only use the top Nr components accord- 5: wrap_and_initialize(W j,V new)
ing to their explained variance ratio ξi. Thus, 6: endif
j
settingρ = 1, resultsinauniformrankdistri- 7: V old ←V new
butionasinLoRA,butinitializedaccordingto 8: endwhile
EVA.Therefore,ρprovidesuswiththemeans 9: redistribute_ranks(ψ,ξ,V new)
tochangetherankdistributioninacontrolled 10: lora_finetune(ψ,X)
mannerpriortofine-tuningattheinitialization
stage,asopposedtolearningitthroughoutthe
trainingprocessasdoneinpriorworks(Zhangetal.,2023a;Valipouretal.,2023;Meoetal.,2024).
Inpracticewefoundthattheredistributionconvergesforvaluesofρ>2(seeAppendixG).Finally,
weinitializeBwithzerosandperformthestandardLoRAfine-tuning,asrecommendedinHayou
etal.(2024a). InAlgorithm1weprovidepseudocodeforEVA.
4 Experiments
First, we elaborate on implementation details of EVA in Section 4.1. Then, we show results for
fine-tuninglargelanguagemodels(LLMs)onmathandreasoningtasksinSection4.2andlanguage
understandingtasksinSection4.3.FurtherweshowresultsforimageclassificationinSection4.4and
decisionmakingtasksinSection4.5. Finally,inSection4.6wedemonstratethatthecomputational
overhead induced by EVA over LoRA is negligible and that incremental SVD converges and is
invarianttobatchorderandbatchsize.
4.1 ImplementationDetails
WefollowthestandardLoRAtrainingprocedurefromHuetal.(2022). SimilartoKalajdzievski
(2023), we found LoRA training to be very sensitive to the scaling parameter α. Therefore, we
5Table2: ComparisonofLoRAandDoRAtodifferentinitializationandrankre-distributionmethods
onNLGtasks. Wereportaverageperformanceacrossthreeseedsandrespectivestandarddeviationin
Table11. EVA+DoRAandEVAconsistentlyattainthehighestaverageperformanceacrossalltasks.
Model Method BoolQ PIQA SIQA HellaSwag Winogrande ARC-e ARC-c OBQA Avg.
LoRA 67.2 83.9 82.0 94.7 84.0 87.8 74.1 84.0 82.2
AdaLoRA 74.8 82.2 80.5 93.3 79.4 86.1 71.1 80.6 81.0
PiSSA 62.6 84.8 81.2 94.5 84.8 87.8 74.8 85.4 82.0
OLoRA 68.7 84.8 82.2 95.0 85.0 88.1 74.9 85.2 82.9
Llama-2-7B
EVA 71.2 85.2 82.1 95.2 84.5 88.9 75.6 85.0 83.4
DoRA 68.3 85.1 82.2 94.9 84.3 88.7 74.8 86.3 83.1
EVA+DoRA 73.5 85.3 82.4 95.2 84.8 88.9 76.0 87.3 84.2
LoRA 85.7 90.3 83.0 96.9 88.4 94.2 84.8 90.1 89.2
AdaLoRA 83.9 89.5 81.7 96.2 86.3 93.7 82.7 86.8 87.6
PiSSA 72.9 87.3 81.6 95.3 87.8 91.7 81.2 87.6 85.7
OLoRA 86.0 90.4 83.9 97.0 88.6 94.5 84.7 90.3 89.4
Llama-3.1-8B
EVA 85.5 90.8 83.3 97.1 88.6 94.7 85.7 89.5 89.4
DoRA 86.2 90.8 83.4 96.9 88.6 94.3 84.9 89.4 89.3
EVA+DoRA 85.8 90.8 83.9 97.1 89.2 94.4 85.9 90.5 89.7
LoRA 88.3 92.9 85.2 97.8 92.3 97.2 89.9 94.4 92.2
AdaLoRA 87.3 91.8 84.6 97.3 91.3 97.0 90.0 92.6 91.5
PiSSA 81.4 90.0 82.5 95.5 89.0 93.6 83.5 90.8 88.3
OLoRA 87.7 92.5 85.2 97.5 92.5 96.6 88.7 93.7 91.8
Gemma-2-9B
EVA 88.6 93.0 85.3 97.9 92.8 97.5 90.5 94.5 92.5
DoRA 88.3 92.6 84.9 97.7 92.2 97.1 89.9 94.5 92.1
EVA+DoRA 88.6 93.1 85.1 97.9 92.5 97.3 89.6 94.8 92.4
setα = 1sincewefoundthistobethemoststablesettingandadditionallytunethelearningrate.
We apply EVA to pre-trained weights only, i.e., we do not initialize newly introduced classifier
heads. FollowingZhangetal.(2023a),weapplyLoRAtoallpre-trainedweightmatricesexcept
for the embedding layer. For EVA we always search over ρ ∈ {1,2} to cover both uniform and
non-uniformrankallocationandreportthebestscore. Allmodelsweusedarepubliclyavailableon
thehuggingfacehub(Wolfetal.,2020). Fortheimplementationofbaselinesweleveragethewidely
usedPEFTlibrary(Mangrulkaretal.,2022). Acrossexperimentswehighlightthehighestscoresin
boldfaceandunderlinethesecond-highest.
Table3: ComparisonofEVAtootherinitialization
andadaptiverankmethodsonGSM8KandMATH
4.2 LanguageGeneration
datasets. Wereportmeanandstandarddeviation
acrossthreerandomseeds.
We fine-tune three different LLMs, namely
Llama-2-7B (Touvron et al., 2023b),
Llama-3.1-8B (Dubey et al., 2024), and Model Method GSM8K MATH
Gemma-2-9B(Rivièreetal.,2024)oncommon LoRA 59.7 10.9
±.8 ±.2
sense and math reasoning benchmarks. For AdaLoRA 56.9 9.6
±.4 ±.2
common sense reasoning we follow Liu et al. PiSSA 61.1 ±.3 12.6 ±.4
(2024a)andamalgamateatrainingsetconsist- Llama-2-7B OLoRA 60.7 ±.5 11.8 ±.3
EVA 61.9 13.1
ingofBoolQ(Christopheretal.,2019),PIQA ±.5 ±.3
DoRA 59.8 11.5
(Bisk et al., 2020), SIQA (Sap et al., 2019), ±.5 ±.2
EVA+DoRA 62.5 13.4
HellaSwag (Zellers et al., 2019), Winogrande ±.8 ±.01
(Sakaguchi et al., 2020), ARC-e and ARC-c LoRA 78.3 ±.6 30.1 ±.5
AdaLoRA 76.9 28.9
(Clark et al., 2018) and OpenBookQA (Mi- ±.2 ±.7
PiSSA 78.8 29.5
haylovetal.,2018). Weapplyallmethodslisted ±.2 ±.5
OLoRA 78.0 31.0
inTable1toallthreemodelsandadditionally Llama-3.1-8B ±.1 ±.7
EVA 78.8 31.2
addacomparisontoDoRA(Liuetal.,2024a) ±.3 ±.3
DoRA 77.9 30.2
±.1 ±.5
and EVA+DoRA, which combines EVA with
EVA+DoRA 79.1 30.8
±.5 ±.4
DoRA.Wetrainallmethodswithrankr =16
LoRA 83.4 40.7
andalearningrateof5e−4forthreerandom ±.9 ±.2
AdaLoRA 83.5 41.1
seeds. Furtherdetailsonthefine-tuningsettings ±.5 ±.4
PiSSA 79.8 34.9
±.5 ±.2
can be found in Appendix B. We present OLoRA 82.2 39.4
Gemma-2-9B ±.2 ±.6
our results in Table 2. For Llama-2-7B and EVA 83.6 41.5
±.8 ±.3
Llama-3.1-8B EVA+DoRA (ρ = 1) is the DoRA 82.5 39.7
±.6 ±.4
bestperformingmethodonaveragewhilealso EVA+DoRA 82.9 ±.3 40.0 ±.6
6Table4: ComparisonofallmethodsforRoBERTa (top)andDeBERTav3 (bottom)onGLUE
Large Base
tasks. WereportmeanandstandarddeviationofMatthew’scorrelationforCoLA,Pearsoncorrelation
forSTS-B,matchedaccuracyforMNLI,andaccuracyforremainingtasks. ForCoLA,RTE,MRPC,
andSTS-Bweaverageoverfiveseedsandfortheremainingtasksoverthreeseeds.
Method MNLI QNLI QQP SST2 CoLA MRPC RTE STS-B Avg
FFT 90.2 94.7 92.2 96.4 68.0 90.9 86.6 92.4 88.93
LoRA 90.7 94.8 92.0 96.2 69.1 91.1 88.1 92.3 89.29
±.1 ±.1 ±.0 ±.3 ±.5 ±.6 ±1.1 ±.1
AdaLoRA 90.5 94.8 90.6 96.1 68.2 90.7 84.4 91.8 88.39
±.1 ±.2 ±.1 ±.2 ±.7 ±.6 ±.9 ±.1
PiSSA 90.1 94.7 91.0 96.1 68.7 90.4 87.6 92.5 88.89
±.1 ±.0 ±.0 ±.2 ±1.3 ±.6 ±.5 ±.3
OLoRA 90.9 95.0 92.0 96.3 69.0 91.0 87.9 92.4 89.32
±.1 ±.1 ±.2 ±.3 ±1.5 ±1.0 ±1.2 ±.1
EVA 90.8 95.0 92.1 96.2 69.5 91.4 88.8 92.6 89.55
±.1 ±.2 ±.1 ±.1 ±1.4 ±.8 ±1.2 ±.1
DoRA 89.5 94.6 89.9 96.1 69.3 91.0 88.4 92.4 88.90
±.1 ±.1 ±.1 ±.1 ±.8 ±.6 ±1.2 ±.1
FFT 90.1 94.0 92.4 95.6 69.2 89.5 83.8 91.6 88.28
LoRA 90.5 94.3 92.4 95.2 72.0 91.4 88.9 91.7 89.64
±.1 ±.1 ±.1 ±.3 ±1.3 ±.7 ±.5 ±.1
AdaLoRA 90.8 94.6 92.2 96.1 71.5 90.7 88.1 91.8 89.46
PiSSA 90.1 94.1 91.8 95.8 72.7 90.9 86.5 91.6 89.19
±.3 ±.1 ±.1 ±.1 ±1.7 ±.6 ±1.2 ±.2
OLoRA 90.5 94.4 92.6 96.2 72.0 91.6 89.1 92.0 89.80
±.1 ±.1 ±.1 ±.2 ±1.0 ±.7 ±.9 ±.2
EVA 90.6 94.4 92.4 96.2 72.5 91.8 89.4 92.0 89.91
±.1 ±.1 ±.04 ±.2 ±1.3 ±.6 ±.7 ±.2
DoRA 89.0 94.1 88.0 94.6 70.3 91.9 87.8 91.8 88.44
±.2 ±.1 ±.1 ±.4 ±.5 ±.6 ±.7 ±.1
exhibiting the best individual scores on most
tasks. ForGemma-2-9B,EVAwithadaptiveranks(ρ=2)yieldsthehighestperformance. EVAas
wellasEVA+DoRAareconsistentlyamongthebestperformingmethodsonallindividualtasks.
ThishighlightstheeffectivenessofEVA’sdata-driveninitializationandrankallocation.
Forthemathfine-tuningexperiments,wefine-tuneallmodelsontheMetaMathQAdataset(Yuetal.,
2024)foroneepochwiththesamehyperparametersthatweusedforthecommonsensereasoning
benchmarksandreporttheresultsinTable3. WeobservethatEVAattainsthehighestperformance
ontheGSM8KdatasetforGemma-2-9Busingρ=2. ForLlama-2-7BandLlama-3.1-8Bthebest
performingmethodisEVA+DoRAusingρ=1closelyfollowedbyEVA.OnMATH,EVA+DoRA
performsbestforLlama-2-7Bwithρ = 1,whileEVAattainsthehighestscoreforLlama-3.1-8B
withρ=1andGemma-2-9Bwithρ=2. Theseresultsindicatesthattheperformanceofadaptive
rankallocationdependsontheselectedmodel. Wefurtheranalyzetheresultingrankdistributionsfor
differentvaluesofρforLlama-2-7BandtheireffectondownstreamperformanceinAppendixG.
Finally,weprovideadditionalresultsforLlama-2-7Boncodefine-tuningtasksinAppendixB.
4.3 LanguageUnderstanding
WetrainRoBERTa (Liuetal.,2019)andDeBERTav3 (Heetal.,2023)ontheGLUEbench-
Large Base
mark(Wangetal.,2019). TheGLUEbenchmarkcompriseseightdownstreamtasks,suchasnatural
languageinference,orsentimentanalysis. Additionallytolearningrate,wealsosearchoverdifferent
rankswithinamaximalrankbudget(r = 16). Forfurtherdetailsaboutdatasets,implementation,
orhyperparameters,wereferthereadertoAppendixC.WealsoaddFFTasabaseline,butneglect
EVA+DoRAduetotimeconstraintsandreportMatthew’scorrelationforCoLA,Pearsoncorrelation
forSTS-B,andaccuracyfortheremainingtasksinTable4. ForRoBERTa ,EVAattainsthe
Large
highest scores on QNLI, CoLA, MRPC, RTE, and STS-B, leading to the highest average score.
Interestingly,DoRAusuallyonlyslightlyimprovesoverLoRAonlowresourcetasks(RTE,MRPC),
whileperformingworseinhighresourcetasks(MNLI,QNLI,QQP,SST2). WealsocompareLoRA
toEVAinTable14inAppendixCfordifferentrankbudgets, whereEVAconsistentlyimproves
overLoRA.ForDeBERTav3 ,EVAreachesthehighestscoresonSST2,RTE,andSTS-B,again
Base
leadingtothehighestaveragescoreacrossalltasks. Wevisualizeresultingrankdistributionpatterns
ofEVAfordifferentGLUEtasksinAppendixC.Moreranksareassignedtohigherlayersofthe
query,key,andvalueprojectionsintheself-attention,whiletheremainingweightsoftenreceivea
lowernumberofranks. Thisisaconsistentpatternforboth,DeBERTav3 andRoBERTa .
Base Large
4.4 ImageClassification
WeinvestigatetheefficacyofEVAontheVTAB-1K(Zhaietal.,2019)benchmark,whichhasbeen
widelyusedtoevaluatePEFTmethods. VTAB-1Kcomprises19imageclassificationtasksthatare
7Table 5: Fine-tuning DINOv2-g/14 on the VTAB-1K benchmark. Best average performance is
highlightedinboldface. Wereportaverageaccuracyacrossfiveseeds.
Natural Specialized Structured
FFT 73.1 89.7 78.4 99.7 92.2 89.5 55.5 74.8 95.0 88.2 70.5 93.6 64.2 63.6 68.8 92.0 64.3 50.2 56.8 76.8
LoRA 85.9 92.2 82.2 99.7 94.5 64.1 63.6 88.8 97.0 92.6 76.6 97.7 65.3 62.1 83.6 90.6 63.0 37.1 52.3 78.4
AdaLoRA 85.4 92.5 81.4 99.7 95.2 90.5 62.2 87.1 96.4 91.2 76.6 94.4 64.4 60.3 83.7 85.4 61.0 32.9 46.0 78.2
PiSSA 85.5 93.6 82.3 99.7 94.6 92.8 62.3 87.1 96.6 91.9 76.3 95.0 66.3 63.2 84.9 90.5 60.1 36.3 48.6 79.4
OLoRA 85.5 93.0 82.1 99.7 95.1 78.3 62.1 86.7 96.3 91.9 76.8 94.3 66.0 62.4 71.3 89.0 60.9 34.3 49.5 77.6
EVA 85.6 93.9 82.2 99.7 95.9 93.2 63.6 86.8 96.6 92.3 76.1 96.1 65.1 61.1 83.3 91.4 61.6 35.0 55.0 79.7
DoRA 85.9 92.7 82.1 99.7 95.2 34.4 61.4 88.6 96.8 92.4 76.8 97.6 65.4 62.7 84.4 43.2 63.1 37.8 52.6 74.4
EVA+DoRA 86.2 92.1 81.9 99.7 94.9 93.8 62.4 88.3 96.6 92.6 76.7 97.2 65.5 54.1 83.7 93.3 62.3 37.5 54.5 79.6
Table6: Resultsforsingletaskfine-tuningexperimentsontheMeta-Worldbenchmark. Wereport
meansuccessratesandstandarderroracrossthreeseedsforeverytask.
FFT 1.0 0.97 1.0 0.77 0.87 1.0 1.0 1.0 0.63 1.0 0.92
±.0 ±.03 ±.0 ±.05 ±.05 ±.0 ±.0 ±.0 ±.03 ±.0
LoRA 1.0 1.0 1.0 0.6 0.63 1.0 1.0 1.0 0.4 1.0 0.86
±.0 ±.0 ±.0 ±.05 ±.1 ±.0 ±.0 ±.0 ±.09 ±.0
AdaLoRA 1.0 0.97 1.0 0.4 0.57 0.97 0.97 1.0 0.13 1.0 0.80
±.0 ±.03 ±.0 ±.09 ±.1 ±.03 ±.03 ±.0 ±.07 ±.0
PiSSA 1.0 1.0 1.0 0.43 0.57 1.0 1.0 1.0 0.53 1.0 0.85
±.0 ±.0 ±.0 ±0.11 ±0.03 ±.0 ±.0 ±.0 ±0.1 ±.0
OLoRA 1.0 0.97 1.0 0.57 0.63 1.0 1.0 1.0 0.6 1.0 0.88
±.0 ±0.03 ±.0 ±0.1 ±0.03 ±.0 ±.0 ±.0 ±0.12 ±.0
EVA 1.0 0.97 1.0 0.63 0.77 1.0 1.0 1.0 0.63 1.0 0.90
±.0 ±.03 ±.0 ±.03 ±.05 ±.0 ±.0 ±.0 ±.07 ±.0
DoRA 1.0 1.0 1.0 0.6 1.0 1.0 1.0 1.0 0.67 1.0 0.93
±.0 ±.0 ±.0 ±1.2 ±.0 ±.0 ±.0 ±.0 ±1.5 ±.0
EVA+DoRA 1.0 1.0 1.0 0.8 1.0 1.0 1.0 1.0 0.63 1.0 0.94
±.0 ±.0 ±.0 ±.08 ±.0 ±.0 ±.0 ±.0 ±.03 ±.0
dividedintonaturalimages,specializedimages(medicalimagesandremotesensing),andstructured
images(e.g.orientationprediction,depthestimationorobjectcounting).Wefine-tuneaDINOv2-g/14
model (Oquab et al., 2023) that consists of around 1.1B parameters. For implementation details
orhyperparameterswereferthereadertoAppendixD.OurresultsareshowninTable5andwe
additionallyreporterrorbarsinTable17. EVAandEVA+DoRAattainthebestandsecond-best
averageaccuracyacrossalltasks,respectively. Interestingly,EVAmainlyimprovesovercompetitors
onthenaturaltasks,i.e. in-domaindatasets. LoRAperformsbestonthespecializedtasksandfull
fine-tuning(FFT)performsbestonthestructuredtask. However,bothLoRAandFFTperformworse
ontheremainingtasks,leadingtoaworseaveragescorecomparedtoEVAandEVA+DoRA.
4.5 DecisionMaking
Wefollowthesingletaskfine-tuningexperimentsinSchmiedetal.(2023)andfine-tuneaDecision
Transformer(Chenetal.,2021a,DT)ontheMeta-Worldbenchmarksuite(Yuetal.,2020). Meta-
Worldconsistsofadiversesetof50tasksforroboticmanipulation,suchasobjectmanipulation,
grasping, or pushing buttons. We split Meta-World according to Wolczyk et al. (2021) into 40
pre-trainingtasks(MT40)and10fine-tuningtasks(CW10). Wepre-traina12MparameterDTon
MT40andfine-tuneitontheCW10holdouttasks. Wereportsuccessratesandstandarderrorsfor
eachtaskofCW10inTable6.WeobservethatEVAsignificantlyreducesthatgapbetweenLoRAand
FFT.Furthermore,DoRAperformsparticularlywellinthisexperimentandexceedsFFTperformance.
Finally,ourEVA+DoRAevenimprovesuponDoRAandattainsthebestaverageperformanceacross
8
001rafiC
esolc-tecuaf
101hcetlaC DTD
remmah
201rewolF steP
sserp-eldnah
NHVS 793nuS
gulpnu-gep
noylemaC TASoruE
kcab-hsup
54csiseR yhtaponiteR
hsup
tnuoC-rvelC tsiD-rvelC
llaw-hsup
baLMD
ecalp-flehs
tsiD-ITTIK coL-rpSd
llup-kcits
irO-rpSd
mizA-BRONs
esolc-wodniw
elE-BRONs egarevA
egarevAIncremental SVD convergence
3500
3000
2500
2000
1500
4 1000
8
500 16
32
0
0 100 200 300 400
Time in seconds
Figure3: Left: TimeinsecondsuntilconvergenceofincrementalSVDcomponentsfordifferent
batchsizesforLlama-2-7BontheMetaMathQAdataset. Thedashedlineindicatesthetotalnumber
of components. Right: Average cosine similarity between SVD components across 10 random
seedsforpermutingthebatchorder. Thefirst10componentsremainmostlyconsistentacrossall
permutations. Whiletheremainingcomponentsvary,theystronglycorrelatewitheachother.
alltasks. WereportresultsfordifferentrankbudgetsinTable19,aswellasimplementationdetails
andhyperparametersinAppendixE.
4.6 SVDConvergenceAnalysis
Thedata-driveninitializationofEVAreliesonincrementalSVDonminibatchesofactivationsin
theinitialtrainingstage. InFigure3,left,weshowthatthisprocessconvergesforLlama-2-7Bon
MetaMathQAfordifferentminibatchsizes. Usingaminibatchsizeof4thecomputationforEVA’s
initializationlastsforapproximately80seconds,whichcorrespondstoaround90minibatches. Fora
batchsizeof32thecomputationoftheSVDcomponentstakesaround500seconds.InFigure3,right,
weadditionallyshow,thatthemaincomponentsobtainedviaSVDmostlyremainconsistentacross
differentbatchordersforabatchsizeof4,againforLlama-2-7BonMetaMathQA.Tothisend,we
plotcosinesimilaritybetweencomponentsobtainedviaincrementalSVDafterrankredistribution.
These results indicate that these models exhibit certain activation patterns that remain consistent
acrossdifferentbatchorderswhichleadtoarobustinitializationforEVA.Wealsoshowthatthe
componentsfordifferentbatchsizesconvergetomostlythesamefinalinitializationinAppendixF.
4.7 AblationStudies
Finally,weconductablationstudiesonEVAtoinvestigateimportantfactorsthatcontributetoits
performance. Specifically,weinvestigatetheimpactofscaleanddirections. Tothisend,weusethe
VTAB-1Kdatasetbecauseitcomprisesadiversesetoftasksandallowsforasystematicinvestigation
on in-domain data (natural), and out-of-distribution data (specialized and structured). We report
resultsforourablationstudiesinTable7andexplainthedifferentsettingsinthefollowingparagraphs.
Effectofscale. Toinvestigatetheeffectofscaleontheinitialization,weaddasettingwhichuses
whitening(EVA-whiten). Whiteningscalestheinitializationbythereciprocaloftheireigenvalues,
which alters scale, but preserves directions. We found that whitening can significantly improve
performanceonstructured(out-of-distribution)tasksevenleadingtoaslightlyhigheraveragescore
thanEVA.Thisindicatesthatscaleisespeciallyimportantforstructureddata. However,EVA-whiten
experiencesaslightperformancedroponnaturalandspecializedtasks.
Effectofdirections. Toaddresstheimportanceofthedirectionsofthecomponents,werandomly
permuteitsrows(EVA-perm). Thispreservesscalewhilecorruptingdirectionsandℓ normofA.
2
Additionally,weaddasettingwherewerandomlyrotateA(EVA-rot),whichpreservesℓ norm,but
2
altersdirections. Wefindthatalteringdirectionsleadstoaperformancedroponthestructuredtasks,
whilechangingℓ normleadstoadroponthenaturaltasks. Both,EVA-permandEVA-rotleadto
2
worseaverageperformanceacrossalltaskscomparedtoEVA.
9
stnenopmoc
degrevnoc
fo
rebmuNEffect of rank redistribution. We conduct an experiment in which we randomly initialize A
afterperformingrankredistribution(LoRA-redist). Thissettinggivesinsightsontheeffectofthe
redistributionandwhetheritsbenefitsareboundtoEVA.Theredistributionhasapositiveeffect
onLoRAonthenaturaltasks,butanegativeeffectonbothstructuredandspecializedtasks. This
illustratesthatrankredistributionismostbeneficialincombinationwithEVA’sinitializationofA.
Table7: Group-wiseaveragesforDINOv2-G/14
5 DiscussionandLimitations
ablationstudiesontheVTAB-1Kbenchmark.
Method Nat. Spec. Struct. All
Alternative data-driven initialization
schemes. Wealsoinvestigatedalternativedata LoRA 83.2 88.8 69.0 78.4
driveninitializationschemes. Suchalternatives LoRA-redist 87.3 88.0 68.2 79.4
include, but are not limited to, Kernel-PCA EVA-whiten 87.5 87.5 69.1 79.8
(Schölkopfetal.,1997)orLinearDiscriminant EVA-rot 87.7 88.0 68.2 79.6
Analysis (Fisher, 1936, LDA). While Kernel- EVA-perm 87.4 87.8 68.3 79.5
PCAcanaccountfornon-linearitiesinthedata, EVA 87.7 87.9 68.6 79.7
itscaleswiththenumberofdatapoints. Inour
settingweperformSVDonminibatchesofsequences,therefore,thenumberofdatapointsgrows
fast,makingKernel-PCAimpractical. LDAprojectsthedataontoasubspacethatmaximizeslinear
separabilitybetweenclasses. Suchaninitializationschemeisparticularlyinterestingforclassification
taskslikeGLUEorVTAB-1K.However,weobservedontheGLUEtasksthattheLDAprojection
matrixneverconverges.
AdditionallatencyofSVD.EVAleadstoperformanceimprovementsoverLoRA,butintroduces
additional latency in the beginning of training for computing the data-driven initialization. We
demonstrated that this process converges quickly. In Appendix F we also show that this process
is mostly invariant to the batch size, meaning that smaller batch sizes may be used for the SVD
computation. Thisresultsinanadditionalcostintherangeof100secondsforaLlama-2-7Bmodel,
whichisnegligible. Further,theSVDcomputationdoesnotrequirebackpropagationandstoringof
optimizerstates. Hence,thereisnooverheadwithrespecttomemory.
Whatmethodperformswellonwhichtasks? Throughoutallofourexperiments,weobservedthat
EVAisthemoststablemethodandconsistentlyimprovesaveragescoresacrosstasksforalldomains
comparedtocompetitors. Interestingly,DoRAonlyoutperformedLoRAonexperimentswithlarger
modelsandonRLtasks. Furthermore,FFTperformedparticularlywellonout-of-distributiontasks
inourimageclassificationexperiments,butoftenperformsworseonin-domainorlowresourcetasks.
Contrary,EVAconsistentlyadvancesaverageperformanceonawiderangeoftasks,establishingits
potentialasstate-of-the-artfine-tuningmethod.
Reproducibility. Weprovidethesourcecodetoreproduceallourexperiments(seeAppendixAfor
moredetails). Further,weintegrateEVAintothewidelyusedPEFTlibrary(Mangrulkaretal.,2022).
6 ConclusionandBroaderImpact
We propose a novel method named Explained Variance Adaptation (EVA), extending the widely
used LoRA with data-driven initialization and rank re-distribution. We initialize LoRA matrices
in a data-driven manner by performing SVD on minibatches of activation vectors. Further, we
re-distributeranksacrossweightmatricesaccordingtotheamountofvariancetheyexplain. Inthis
regard, wealsointroduceahyperparameterthatallowsforacontrolledinvestigationofdifferent
rankdistributions. Thereby,inEVAwebindthebenefitsofadaptiverankallocationanddata-driven
initialization,resultinginoneinitializationtorulethemall. Wedemonstrateperformancegainsof
EVAoverLoRAandinitializationschemesthereofonavarietyofdomains,rangingfromlanguage
tovisionandRL.EVAvariantsconsistentlyreachthehighestaverageperformanceonawiderange
oftasksacrossalldomains.
We believe that EVA sheds a novel view on LoRA fine-tuning, where initialization of the newly
introducedweightsisguidedbythedownstreamdataandcanhaveasignificantimpactonfuture
research on fine-tuning of foundation models. In the future, we aim at investigating the effect
ofincorporatinggradientinformationinEVAandquantization,aswellasalternativedata-driven
initializationschemes.
10Acknowledgments
WeacknowledgeEuroHPCJointUndertakingforawardingusaccesstoVegaatIZUM,Slovenia,
KarolinaatIT4Innovations,CzechRepublic,MeluXinaatLuxProvide,Luxembourg,Leonardoat
CINECA,Italy,MareNostrum5atBSC,Spain.TheELLISUnitLinz,theLITAILab,theInstitutefor
MachineLearning,aresupportedbytheFederalStateUpperAustria. WethanktheprojectsMedical
CognitiveComputingCenter(MC3),INCONTROL-RL(FFG-881064),PRIMAL(FFG-873979),
S3AI(FFG-872172),DLforGranularFlow(FFG-871302),EPILEPSIA(FFG-892171),AIRIFG9-N
(FWF-36284,FWF-36235),AI4GreenHeatingGrids(FFG-899943),INTEGRATE(FFG-892418),
ELISE(H2020-ICT-2019-3ID:951847),Stars4Waters(HORIZON-CL6-2021-CLIMATE-01-01).
We thank NXAI GmbH, Audi.JKU Deep Learning Center, TGW LOGISTICS GROUP GMBH,
SiliconAustriaLabs(SAL),FILLGesellschaftmbH,AnylineGmbH,Google,ZFFriedrichshafen
AG,RobertBoschGmbH,UCBBiopharmaSRL,MerckHealthcareKGaA,VerbundAG,GLS(Univ.
Waterloo),SoftwareCompetenceCenterHagenbergGmbH,BorealisAG,TÜVAustria,Frauscher
Sensonic,TRUMPFandtheNVIDIACorporation. FabianPaischeracknowledgestravelsupport
fromELISE(GAno951847)
References
Aghajanyan,A.,Gupta,S.,andZettlemoyer,L. Intrinsicdimensionalityexplainstheeffectivenessof
languagemodelfine-tuning. InZong,C.,Xia,F.,Li,W.,andNavigli,R.(eds.),Proceedingsofthe
59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternational
JointConferenceonNaturalLanguageProcessing,ACL/IJCNLP2021,(Volume1: LongPapers),
VirtualEvent,August1-6,2021,pp.7319–7328.AssociationforComputationalLinguistics,2021.
doi: 10.18653/v1/2021.acl-long.568.
Austin,J.,Odena,A.,Nye,M.,Bosma,M.,Michalewski,H.,Dohan,D.,Jiang,E.,Cai,C.,Terry,M.,
Le,Q.,etal. Programsynthesiswithlargelanguagemodels. arXivpreprintarXiv:2108.07732,
2021.
Babakniya,S.,Elkordy,A.R.,Ezzeldin,Y.H.,Liu,Q.,Song,K.,El-Khamy,M.,andAvestimehr,S.
Slora: Federatedparameterefficientfine-tuningoflanguagemodels. CoRR,abs/2308.06522,2023.
doi: 10.48550/ARXIV.2308.06522.
Beattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., Küttler, H., Lefrancq, A.,
Green, S., Valdés, V., Sadik, A., Schrittwieser, J., Anderson, K., York, S., Cant, M., Cain, A.,
Bolton,A.,Gaffney,S.,King,H.,Hassabis,D.,Legg,S.,andPetersen,S. Deepmindlab. CoRR,
abs/1612.03801,2016.
Bisk,Y.,Zellers,R.,Bras,R.L.,Gao,J.,andChoi,Y. Piqa: Reasoningaboutphysicalcommonsense
innaturallanguage. InThirty-FourthAAAIConferenceonArtificialIntelligence,2020.
Bommasani,R.,Hudson,D.A.,Adeli,E.,Altman,R.B.,Arora,S.,vonArx,S.,Bernstein,M.S.,
Bohg,J.,Bosselut,A.,Brunskill,E.,Brynjolfsson,E.,Buch,S.,Card,D.,Castellon,R.,Chatterji,
N.S.,Chen,A.S.,Creel,K.,Davis,J.Q.,Demszky,D.,Donahue,C.,Doumbouya,M.,Durmus,
E.,Ermon,S.,Etchemendy,J.,Ethayarajh,K.,Fei-Fei,L.,Finn,C.,Gale,T.,Gillespie,L.,Goel,
K.,Goodman,N.D.,Grossman,S.,Guha,N.,Hashimoto,T.,Henderson,P.,Hewitt,J.,Ho,D.E.,
Hong,J.,Hsu,K.,Huang,J.,Icard,T.,Jain,S.,Jurafsky,D.,Kalluri,P.,Karamcheti,S.,Keeling,
G.,Khani,F.,Khattab,O.,Koh,P.W.,Krass,M.S.,Krishna,R.,Kuditipudi,R.,andetal. Onthe
opportunitiesandrisksoffoundationmodels. CoRR,abs/2108.07258,2021.
Brohan,A.,Brown,N.,Carbajal,J.,Chebotar,Y.,Dabis,J.,Finn,C.,Gopalakrishnan,K.,Hausman,
K.,Herzog,A.,Hsu,J.,Ibarz,J.,Ichter,B.,Irpan,A.,Jackson,T.,Jesmonth,S.,Joshi,N.J.,Julian,
R.,Kalashnikov,D.,Kuang,Y.,Leal,I.,Lee,K.,Levine,S.,Lu,Y.,Malla,U.,Manjunath,D.,
Mordatch,I.,Nachum,O.,Parada,C.,Peralta,J.,Perez,E.,Pertsch,K.,Quiambao,J.,Rao,K.,
Ryoo,M.S.,Salazar,G.,Sanketi,P.R.,Sayed,K.,Singh,J.,Sontakke,S.,Stone,A.,Tan,C.,Tran,
H.T.,Vanhoucke,V.,Vega,S.,Vuong,Q.,Xia,F.,Xiao,T.,Xu,P.,Xu,S.,Yu,T.,andZitkovich,
B. RT-1: roboticstransformerforreal-worldcontrolatscale. InBekris,K.E.,Hauser,K.,Herbert,
S.L.,andYu,J.(eds.),Robotics: ScienceandSystemsXIX,Daegu,RepublicofKorea,July10-14,
2023,2023. doi: 10.15607/RSS.2023.XIX.025.
11Büyükakyüz, K. Olora: Orthonormal low-rank adaptation of large language models. CoRR,
abs/2406.01775,2024. doi: 10.48550/ARXIV.2406.01775.
Chavan, A., Liu, Z., Gupta, D. K., Xing, E. P., and Shen, Z. One-for-all: Generalized lora for
parameter-efficientfine-tuning. CoRR,abs/2306.07967,2023. doi: 10.48550/ARXIV.2306.07967.
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and
Mordatch,I. Decisiontransformer: Reinforcementlearningviasequencemodeling. Advancesin
neuralinformationprocessingsystems,34:15084–15097,2021a.
Chen,M.,Tworek,J.,Jun,H.,Yuan,Q.,deOliveiraPinto,H.P.,Kaplan,J.,Edwards,H.,Burda,
Y.,Joseph,N.,Brockman,G.,Ray,A.,Puri,R.,Krueger,G.,Petrov,M.,Khlaaf,H.,Sastry,G.,
Mishkin,P.,Chan,B.,Gray,S.,Ryder,N.,Pavlov,M.,Power,A.,Kaiser,L.,Bavarian,M.,Winter,
C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss,
A., Guss, W.H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S.,
Saunders, W., Hesse, C., Carr, A.N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford,
A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D.,
McCandlish,S.,Sutskever,I.,andZaremba,W. Evaluatinglargelanguagemodelstrainedoncode,
2021b.
Cheng,G.,Han,J.,andLu,X. Remotesensingimagesceneclassification: Benchmarkandstateof
theart. Proc.IEEE,105(10):1865–1883,2017. doi: 10.1109/JPROC.2017.2675998.
Christopher,C.,Kenton,L.,Ming-Wei,C.,Tom,K.,Michael,C.,andKristina,T. Boolq: Exploring
thesurprisingdifficultyofnaturalyes/noquestions. InNAACL,2019.
Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A. Describing textures in the
wild. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014,
Columbus, OH, USA, June 23-28, 2014, pp. 3606–3613. IEEE Computer Society, 2014. doi:
10.1109/CVPR.2014.461.
Clark, K., Luong, M., Le, Q. V., and Manning, C. D. ELECTRA: pre-training text encoders as
discriminatorsratherthangenerators.In8thInternationalConferenceonLearningRepresentations,
ICLR2020,AddisAbaba,Ethiopia,April26-30,2020.OpenReview.net,2020.
Clark,P.,Cowhey,I.,Etzioni,O.,Khot,T.,Sabharwal,A.,Schoenick,C.,andTafjord,O. Thinkyou
havesolvedquestionanswering? tryarc,theai2reasoningchallenge. arXiv:1803.05457v1,2018.
Cobbe,K.,Kosaraju,V.,Bavarian,M.,Chen,M.,Jun,H.,Kaiser,L.,Plappert,M.,Tworek,J.,Hilton,
J.,Nakano,R.,Hesse,C.,andSchulman,J. Trainingverifierstosolvemathwordproblems,2021.
Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv
preprintarXiv:2307.08691,2023.
Dehghani,M.,Djolonga,J.,Mustafa,B.,Padlewski,P.,Heek,J.,Gilmer,J.,Steiner,A.P.,Caron,
M.,Geirhos,R.,Alabdulmohsin,I.,Jenatton,R.,Beyer,L.,Tschannen,M.,Arnab,A.,Wang,X.,
Ruiz,C.R.,Minderer,M.,Puigcerver,J.,Evci,U.,Kumar,M.,vanSteenkiste,S.,Elsayed,G.F.,
Mahendran,A.,Yu,F.,Oliver,A.,Huot,F.,Bastings,J.,Collier,M.,Gritsenko,A.A.,Birodkar,V.,
Vasconcelos,C.N.,Tay,Y.,Mensink,T.,Kolesnikov,A.,Pavetic,F.,Tran,D.,Kipf,T.,Lucic,M.,
Zhai,X.,Keysers,D.,Harmsen,J.J.,andHoulsby,N. Scalingvisiontransformersto22billion
parameters. InKrause,A.,Brunskill,E.,Cho,K.,Engelhardt,B.,Sabato,S.,andScarlett,J.(eds.),
InternationalConferenceonMachineLearning,ICML2023,23-29July2023,Honolulu,Hawaii,
USA,volume202ofProceedingsofMachineLearningResearch,pp.7480–7512.PMLR,2023.
Dettmers,T.,Lewis,M.,Belkada,Y.,andZettlemoyer,L. Gpt3.int8(): 8-bitmatrixmultiplicationfor
transformersatscale. InKoyejo,S.,Mohamed,S.,Agarwal,A.,Belgrave,D.,Cho,K.,andOh,A.
(eds.),AdvancesinNeuralInformationProcessingSystems,volume35,pp.30318–30332.Curran
Associates,Inc.,2022.
Dettmers,T.,Pagnoni,A.,Holtzman,A.,andZettlemoyer,L. Qlora: Efficientfinetuningofquantized
llms. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.),
AdvancesinNeuralInformationProcessingSystems36:AnnualConferenceonNeuralInformation
ProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December10-16,2023,2023.
12Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,T.,Dehghani,
M.,Minderer,M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,andHoulsby,N. Animageisworth16x16
words: Transformersforimagerecognitionatscale. In9thInternationalConferenceonLearning
Representations,ICLR2021,VirtualEvent,Austria,May3-7,2021.OpenReview.net,2021.
Dubey,A.,Jauhri,A.,Pandey,A.,Kadian,A.,Al-Dahle,A.,Letman,A.,Mathur,A.,Schelten,A.,
Yang,A.,Fan,A.,Goyal,A.,Hartshorn,A.,Yang,A.,Mitra,A.,Sravankumar,A.,Korenev,A.,
Hinsvark,A.,Rao,A.,Zhang,A.,Rodriguez,A.,Gregerson,A.,Spataru,A.,Rozière,B.,Biron,
B.,Tang,B.,Chern,B.,Caucheteux,C.,Nayak,C.,Bi,C.,Marra,C.,McConnell,C.,Keller,C.,
Touret, C., Wu, C., Wong, C., Ferrer, C.C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D.,
Livshits,D.,Esiobu,D.,Choudhary,D.,Mahajan,D.,Garcia-Olano,D.,Perino,D.,Hupkes,D.,
Lakomkin,E.,AlBadawy,E.,Lobanova,E.,Dinan,E.,Smith,E.M.,Radenovic,F.,Zhang,F.,
Synnaeve,G.,Lee,G.,Anderson,G.L.,Nail,G.,Mialon,G.,Pang,G.,Cucurell,G.,Nguyen,H.,
Korevaar,H.,Xu,H.,Touvron,H.,Zarov,I.,Ibarra,I.A.,Kloumann,I.M.,Misra,I.,Evtimov,
I.,Copet,J.,Lee,J.,Geffert,J.,Vranes,J.,Park,J.,Mahadeokar,J.,Shah,J.,vanderLinde,J.,
Billock,J.,Hong,J.,Lee,J.,Fu,J.,Chi,J.,Huang,J.,Liu,J.,Wang,J.,Yu,J.,Bitton,J.,Spisak,J.,
Park,J.,Rocca,J.,Johnstun,J.,Saxe,J.,Jia,J.,Alwala,K.V.,Upasani,K.,Plawiak,K.,Li,K.,
Heafield,K.,Stone,K.,andetal. Thellama3herdofmodels. CoRR,abs/2407.21783,2024. doi:
10.48550/ARXIV.2407.21783.
Fei-Fei,L.,Fergus,R.,andPerona,P. One-shotlearningofobjectcategories. IEEETrans.Pattern
Anal.Mach.Intell.,28(4):594–611,2006. doi: 10.1109/TPAMI.2006.79.
Fisher,R.A.Theuseofmultiplemeasurementsintaxonomicproblems. AnnalsEugenics,7:179–188,
1936.
Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu,
J.,LeNoac’h,A.,Li,H.,McDonell,K.,Muennighoff,N.,Ociepa,C.,Phang,J.,Reynolds,L.,
Schoelkopf,H.,Skowron,A.,Sutawika,L.,Tang,E.,Thite,A.,Wang,B.,Wang,K.,andZou,A.
Aframeworkforfew-shotlanguagemodelevaluation,072024.
Gauch,M.,Beck,M.,Adler,T.,Kotsur,D.,Fiel,S.,Eghbal-zadeh,H.,Brandstetter,J.,Kofler,J.,
Holzleitner, M., Zellinger, W., Klotz, D., Hochreiter, S., and Lehner, S. Few-shot learning by
dimensionalityreductioningradientspace. InChandar,S.,Pascanu,R.,andPrecup,D.(eds.),
ConferenceonLifelongLearningAgents,CoLLAs2022,22-24August2022,McGillUniversity,
Montréal, Québec, Canada, volume 199 of Proceedings of Machine Learning Research, pp.
1043–1064.PMLR,2022.
Geiger,A.,Lenz,P.,Stiller,C.,andUrtasun,R. Visionmeetsrobotics: TheKITTIdataset. Int.J.
RoboticsRes.,32(11):1231–1237,2013. doi: 10.1177/0278364913491297.
Glorot,X.andBengio,Y. Understandingthedifficultyoftrainingdeepfeedforwardneuralnetworks.
InTeh,Y.W.andTitterington,D.M.(eds.),ProceedingsoftheThirteenthInternationalConference
onArtificialIntelligenceandStatistics,AISTATS2010,ChiaLagunaResort,Sardinia,Italy,May
13-15,2010,volume9ofJMLRProceedings,pp.249–256.JMLR.org,2010.
Gur-Ari, G., Roberts, D. A., and Dyer, E. Gradient descent happens in a tiny subspace. CoRR,
abs/1812.04754,2018.
Halko, N., Martinsson, P., and Tropp, J. A. Finding structure with randomness: Probabilistic
algorithmsforconstructingapproximatematrixdecompositions. SIAMRev.,53(2):217–288,2011.
doi: 10.1137/090771806.
Hayou,S.,Ghosh,N.,andYu,B. Theimpactofinitializationonlorafinetuningdynamics,2024a.
Hayou,S.,Ghosh,N.,andYu,B. Lora+: Efficientlowrankadaptationoflargemodels,2024b.
He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level
performanceonimagenetclassification. In2015IEEEInternationalConferenceonComputer
Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 1026–1034. IEEE Computer
Society,2015. doi: 10.1109/ICCV.2015.123.
13He,P.,Gao,J.,andChen,W. Debertav3: Improvingdebertausingelectra-stylepre-trainingwith
gradient-disentangledembeddingsharing. InTheEleventhInternationalConferenceonLearning
Representations,ICLR2023,Kigali,Rwanda,May1-5,2023.OpenReview.net,2023.
Helber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat: A novel dataset and deep learning
benchmarkforlanduseandlandcoverclassification. IEEEJ.Sel.Top.Appl.EarthObs.Remote.
Sens.,12(7):2217–2226,2019. doi: 10.1109/JSTARS.2019.2918242.
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora:
Low-rankadaptationoflargelanguagemodels. InTheTenthInternationalConferenceonLearning
Representations,ICLR2022,VirtualEvent,April25-29,2022.OpenReview.net,2022.
Hu, Z., Wang, L., Lan, Y., Xu, W., Lim, E.-P., Bing, L., Xu, X., Poria, S., and Lee, R. LLM-
adapters: An adapter family for parameter-efficient fine-tuning of large language models. In
Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,
pp. 5254–5276, Singapore, December 2023. Association for Computational Linguistics. doi:
10.18653/v1/2023.emnlp-main.319.
Johnson,J.,Hariharan,B.,vanderMaaten,L.,Fei-Fei,L.,Zitnick,C.L.,andGirshick,R.B.CLEVR:
Adiagnosticdatasetforcompositionallanguageandelementaryvisualreasoning. In2017IEEE
ConferenceonComputerVisionandPatternRecognition,CVPR2017,Honolulu,HI,USA,July
21-26,2017,pp.1988–1997.IEEEComputerSociety,2017. doi: 10.1109/CVPR.2017.215.
KaggleandEyePacs. Kagglediabeticretinopathydetection,July2015.
Kalajdzievski,D. Arankstabilizationscalingfactorforfine-tuningwithlora. CoRR,abs/2312.03732,
2023. doi: 10.48550/ARXIV.2312.03732.
Kopiczko, D. J., Blankevoort, T., and Asano, Y. M. ELoRA: Efficient low-rank adaptation with
randommatrices. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
Krähenbühl,P.,Doersch,C.,Donahue,J.,andDarrell,T. Data-dependentinitializationsofconvo-
lutionalneuralnetworks. InBengio,Y.andLeCun,Y.(eds.),4thInternationalConferenceon
LearningRepresentations,ICLR2016,SanJuan,PuertoRico,May2-4,2016,ConferenceTrack
Proceedings,2016.
Krizhevsky,A. Learningmultiplelayersoffeaturesfromtinyimages. CoRR,pp.32–33,2009.
LeCun, Y., Huang, F. J., and Bottou, L. Learning methods for generic object recognition with
invariancetoposeandlighting. In2004IEEEComputerSocietyConferenceonComputerVision
andPatternRecognition(CVPR2004),withCD-ROM,27June-2July2004,Washington,DC,
USA,pp.97–104.IEEEComputerSociety,2004. doi: 10.1109/CVPR.2004.144.
Li,Y.,Yu,Y.,Liang,C.,He,P.,Karampatziakis,N.,Chen,W.,andZhao,T. Loftq: Lora-fine-tuning-
aware quantization for large language models. CoRR, abs/2310.08659, 2023. doi: 10.48550/
ARXIV.2310.08659.
Liu,H.,Tam,D.,Muqeeth,M.,Mohta,J.,Huang,T.,Bansal,M.,andRaffel,C. Few-shotparameter-
efficientfine-tuningisbetterandcheaperthanin-contextlearning. InKoyejo,S.,Mohamed,S.,
Agarwal,A.,Belgrave,D.,Cho,K.,andOh,A.(eds.),AdvancesinNeuralInformationProcessing
Systems35: AnnualConferenceonNeuralInformationProcessingSystems2022,NeurIPS2022,
NewOrleans,LA,USA,November28-December9,2022,2022.
Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct?
rigorousevaluationoflargelanguagemodelsforcodegeneration. InThirty-seventhConferenceon
NeuralInformationProcessingSystems,2023.
Liu,S.,Wang,C.,Yin,H.,Molchanov,P.,Wang,Y.F.,Cheng,K.,andChen,M. Dora: Weight-
decomposedlow-rankadaptation. CoRR,abs/2402.09353,2024a. doi: 10.48550/ARXIV.2402.
09353.
Liu,Y.,Ott,M.,Goyal,N.,Du,J.,Joshi,M.,Chen,D.,Levy,O.,Lewis,M.,Zettlemoyer,L.,and
Stoyanov,V. Roberta: ArobustlyoptimizedBERTpretrainingapproach. CoRR,abs/1907.11692,
2019.
14Liu,Z.,Lyn,J.,Zhu,W.,Tian,X.,andGraham,Y. Alora: Allocatinglow-rankadaptationforfine-
tuninglargelanguagemodels. InDuh,K.,Gómez-Adorno,H.,andBethard,S.(eds.),Proceedings
of the 2024 Conference of the North American Chapter of the Association for Computational
Linguistics: HumanLanguageTechnologies(Volume1: LongPapers),NAACL2024,MexicoCity,
Mexico,June16-21,2024,pp.622–641.AssociationforComputationalLinguistics,2024b. doi:
10.18653/V1/2024.NAACL-LONG.35.
Loshchilov,I.andHutter,F. Fixingweightdecayregularizationinadam. CoRR,abs/1711.05101,
2017.
Mangrulkar,S.,Gugger,S.,Debut,L.,Belkada,Y.,Paul,S.,andBossan,B. Peft: State-of-the-art
parameter-efficientfine-tuningmethods,2022.
Matthey,L.,Higgins,I.,Hassabis,D.,andLerchner,A. dsprites: Disentanglementtestingsprites
dataset. https://github.com/deepmind/dsprites-dataset/,2017.
Meng,F.,Wang,Z.,andZhang,M. Pissa: Principalsingularvaluesandsingularvectorsadaptation
oflargelanguagemodels,2024.
Meo,C.,Sycheva,K.,Goyal,A.,andDauwels,J. Bayesian-lora: Lorabasedparameterefficient
fine-tuningusingoptimalquantizationlevelsandrankvaluestroughdifferentiablebayesiangates.
CoRR,abs/2406.13046,2024. doi: 10.48550/ARXIV.2406.13046.
Micikevicius,P.,Narang,S.,Alben,J.,Diamos,G.,Elsen,E.,Garcia,D.,Ginsburg,B.,Houston,M.,
Kuchaiev,O.,Venkatesh,G.,etal. Mixedprecisiontraining. arXivpreprintarXiv:1710.03740,
2017.
Mihaylov,T.,Clark,P.,Khot,T.,andSabharwal,A. Canasuitofarmorconductelectricity? anew
datasetforopenbookquestionanswering. InEMNLP,2018.
Mishkin, D. and Matas, J. All you need is a good init. In Bengio, Y. and LeCun, Y. (eds.), 4th
InternationalConferenceonLearningRepresentations,ICLR2016,SanJuan,PuertoRico,May
2-4,2016,ConferenceTrackProceedings,2016.
Netzer,Y.,Wang,T.,Coates,A.,Bissacco,A.,Wu,B.,andNg,A.Y. Readingdigitsinnaturalimages
with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised
FeatureLearning2011,2011.
Nikdan,M.,Tabesh,S.,andAlistarh,D. Rosa: Accurateparameter-efficientfine-tuningviarobust
adaptation. CoRR,abs/2401.04679,2024. doi: 10.48550/ARXIV.2401.04679.
Nilsback, M.andZisserman, A. Automatedflowerclassificationoveralargenumber ofclasses.
InSixthIndianConferenceonComputerVision,Graphics&ImageProcessing,ICVGIP2008,
Bhubaneswar,India,16-19December2008,pp.722–729.IEEEComputerSociety,2008. doi:
10.1109/ICVGIP.2008.47.
OpenAI. GPT-4technicalreport. CoRR,abs/2303.08774,2023. doi: 10.48550/ARXIV.2303.08774.
Oquab,M.,Darcet,T.,Moutakanni,T.,Vo,H.,Szafraniec,M.,Khalidov,V.,Fernandez,P.,Haziza,
D.,Massa,F.,El-Nouby,A.,Assran,M.,Ballas,N.,Galuba,W.,Howes,R.,Huang,P.,Li,S.,
Misra, I., Rabbat, M. G., Sharma, V., Synnaeve, G., Xu, H., Jégou, H., Mairal, J., Labatut, P.,
Joulin,A.,andBojanowski,P. Dinov2: Learningrobustvisualfeatureswithoutsupervision. CoRR,
abs/2304.07193,2023. doi: 10.48550/ARXIV.2304.07193.
Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. V. Cats and dogs. In 2012 IEEE
ConferenceonComputerVisionandPatternRecognition,Providence,RI,USA,June16-21,2012,
pp.3498–3505.IEEEComputerSociety,2012. doi: 10.1109/CVPR.2012.6248092.
Paszke,A.,Gross,S.,Massa,F.,Lerer,A.,Bradbury,J.,Chanan,G.,Killeen,T.,Lin,Z.,Gimelshein,
N.,Antiga,L.,Desmaison,A.,Köpf,A.,Yang,E.Z.,DeVito,Z.,Raison,M.,Tejani,A.,Chil-
amkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style,
high-performance deep learning library. In Wallach, H. M., Larochelle, H., Beygelzimer, A.,
d’Alché-Buc,F.,Fox,E.B.,andGarnett,R.(eds.),AdvancesinNeuralInformationProcessing
Systems32: AnnualConferenceonNeuralInformationProcessingSystems2019,NeurIPS2019,
December8-14,2019,Vancouver,BC,Canada,pp.8024–8035,2019.
15Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are
unsupervisedmultitasklearners. CoRR,2019.
Reid,M.,Savinov,N.,Teplyashin,D.,Lepikhin,D.,Lillicrap,T.P.,Alayrac,J.,Soricut,R.,Lazaridou,
A.,Firat,O.,Schrittwieser,J.,Antonoglou,I.,Anil,R.,Borgeaud,S.,Dai,A.M.,Millican,K.,
Dyer, E., Glaese, M., Sottiaux, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Molloy, J., Chen,
J., Isard, M., Barham, P., Hennigan, T., McIlroy, R., Johnson, M., Schalkwyk, J., Collins, E.,
Rutherford,E.,Moreira,E.,Ayoub,K.,Goel,M.,Meyer,C.,Thornton,G.,Yang,Z.,Michalewski,
H.,Abbas,Z.,Schucher,N.,Anand,A.,Ives,R.,Keeling,J.,Lenc,K.,Haykal,S.,Shakeri,S.,
Shyam,P.,Chowdhery,A.,Ring,R.,Spencer,S.,Sezener,E.,andetal. Gemini1.5: Unlocking
multimodalunderstandingacrossmillionsoftokensofcontext. CoRR,abs/2403.05530,2024. doi:
10.48550/ARXIV.2403.05530.
Rivière,M.,Pathak,S.,Sessa,P.G.,Hardin,C.,Bhupatiraju,S.,Hussenot,L.,Mesnard,T.,Shahriari,
B.,Ramé,A.,Ferret,J.,Liu,P.,Tafti,P.,Friesen,A.,Casbon,M.,Ramos,S.,Kumar,R.,Lan,
C.L.,Jerome,S.,Tsitsulin,A.,Vieillard,N.,Stanczyk,P.,Girgin,S.,Momchev,N.,Hoffman,
M., Thakoor, S., Grill, J., Neyshabur, B., Bachem, O., Walton, A., Severyn, A., Parrish, A.,
Ahmad,A.,Hutchison,A.,Abdagic,A.,Carl,A.,Shen,A.,Brock,A.,Coenen,A.,Laforge,A.,
Paterson, A., Bastian, B., Piot, B., Wu, B., Royal, B., Chen, C., Kumar, C., Perry, C., Welty,
C.,Choquette-Choo,C.A.,Sinopalnikov,D.,Weinberger,D.,Vijaykumar,D.,Rogozinska,D.,
Herbison, D., Bandy, E., Wang, E., Noland, E., Moreira, E., Senter, E., Eltyshev, E., Visin, F.,
Rasskin, G., Wei, G., Cameron, G., Martins, G., Hashemi, H., Klimczak-Plucinska, H., Batra,
H., Dhand, H., Nardini, I., Mein, J., Zhou, J., Svensson, J., Stanway, J., Chan, J., Zhou, J. P.,
Carrasqueira,J.,Iljazi,J.,Becker,J.,Fernandez,J.,vanAmersfoort,J.,Gordon,J.,Lipschultz,J.,
Newlan,J.,Ji,J.,Mohamed,K.,Badola,K.,Black,K.,Millican,K.,McDonell,K.,Nguyen,K.,
Sodhia,K.,Greene,K.,Sjösund,L.L.,Usui,L.,Sifre,L.,Heuermann,L.,Lago,L.,andMcNealus,
L. Gemma2: Improvingopenlanguagemodelsatapracticalsize. CoRR,abs/2408.00118,2024.
doi: 10.48550/ARXIV.2408.00118.
Sakaguchi,K.,Bras,R.L.,Bhagavatula,C.,andChoi,Y. Winogrande: Anadversarialwinograd
schemachallengeatscale. InTheThirty-FourthAAAIConferenceonArtificialIntelligence,AAAI
2020,TheThirty-SecondInnovativeApplicationsofArtificialIntelligenceConference,IAAI2020,
TheTenthAAAISymposiumonEducationalAdvancesinArtificialIntelligence,EAAI2020,New
York,NY,USA,February7-12,2020,pp.8732–8740.AAAIPress,2020. doi: 10.1609/AAAI.
V34I05.6399.
Sap,M.,Rashkin,H.,Chen,D.,Bras,R.L.,andChoi,Y. Socialiqa: Commonsensereasoningabout
socialinteractions. CoRR,abs/1904.09728,2019.
Schmied,T.,Hofmarcher,M.,Paischer,F.,Pascanu,R.,andHochreiter,S. Learningtomodulate
pre-trainedmodelsinRL. InOh,A.,Naumann,T.,Globerson,A.,Saenko,K.,Hardt,M.,and
Levine,S.(eds.),AdvancesinNeuralInformationProcessingSystems36: AnnualConferenceon
NeuralInformationProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December
10-16,2023,2023.
Schölkopf,B.,Smola,A.,andMüller,K.-R. Kernelprincipalcomponentanalysis. InGerstner,W.,
Germond,A.,Hasler,M.,andNicoud,J.-D.(eds.),ArtificialNeuralNetworks—ICANN’97,pp.
583–588,Berlin,Heidelberg,1997.SpringerBerlinHeidelberg. ISBN978-3-540-69620-9.
Sun,M.,Chen,X.,Kolter,J.Z.,andLiu,Z. Massiveactivationsinlargelanguagemodels. InFirst
ConferenceonLanguageModeling,2024.
Sung,Y.,Nair,V.,andRaffel,C. Trainingneuralnetworkswithfixedsparsemasks. InRanzato,
M.,Beygelzimer,A.,Dauphin,Y.N.,Liang,P.,andVaughan,J.W.(eds.),AdvancesinNeural
InformationProcessingSystems34:AnnualConferenceonNeuralInformationProcessingSystems
2021,NeurIPS2021,December6-14,2021,virtual,pp.24193–24205,2021.
Todorov,E.,Erez,T.,andTassa,Y. Mujoco: Aphysicsengineformodel-basedcontrol. In2012
IEEE/RSJinternationalconferenceonintelligentrobotsandsystems,pp.5026–5033.IEEE,2012.
Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,M.,Lacroix,T.,Rozière,B.,Goyal,N.,
Hambro,E.,Azhar,F.,Rodriguez,A.,Joulin,A.,Grave,E.,andLample,G. Llama: Openand
16efficientfoundationlanguagemodels. CoRR,abs/2302.13971,2023a. doi: 10.48550/ARXIV.2302.
13971.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra,
S.,Bhargava,P.,Bhosale,S.,Bikel,D.,Blecher,L.,Canton-Ferrer,C.,Chen,M.,Cucurull,G.,
Esiobu,D.,Fernandes,J.,Fu,J.,Fu,W.,Fuller,B.,Gao,C.,Goswami,V.,Goyal,N.,Hartshorn,
A.,Hosseini,S.,Hou,R.,Inan,H.,Kardas,M.,Kerkez,V.,Khabsa,M.,Kloumann,I.,Korenev,
A., Koura, P.S., Lachaux, M., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X.,
Mihaylov,T.,Mishra,P.,Molybog,I.,Nie,Y.,Poulton,A.,Reizenstein,J.,Rungta,R.,Saladi,K.,
Schelten,A.,Silva,R.,Smith,E.M.,Subramanian,R.,Tan,X.E.,Tang,B.,Taylor,R.,Williams,
A.,Kuan,J.X.,Xu,P.,Yan,Z.,Zarov,I.,Zhang,Y.,Fan,A.,Kambadur,M.,Narang,S.,Rodriguez,
A.,Stojnic,R.,Edunov,S.,andScialom,T. Llama2: Openfoundationandfine-tunedchatmodels.
CoRR,abs/2307.09288,2023b. doi: 10.48550/ARXIV.2307.09288.
Valipour,M.,Rezagholizadeh,M.,Kobyzev,I.,andGhodsi,A. Dylora: Parameter-efficienttuningof
pre-trainedmodelsusingdynamicsearch-freelow-rankadaptation. InVlachos,A.andAugenstein,
I. (eds.), Proceedings of the 17th Conference of the European Chapter of the Association for
Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pp. 3266–3279.
AssociationforComputationalLinguistics,2023. doi: 10.18653/V1/2023.EACL-MAIN.239.
Veeling, B. S., Linmans, J., Winkens, J., Cohen, T., and Welling, M. Rotation equivariant cnns
fordigitalpathology. InFrangi,A.F.,Schnabel,J.A.,Davatzikos,C.,Alberola-López,C.,and
Fichtinger,G.(eds.),MedicalImageComputingandComputerAssistedIntervention-MICCAI
2018 - 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings,
PartII,volume11071ofLectureNotesinComputerScience,pp.210–218.Springer,2018. doi:
10.1007/978-3-030-00934-2\_24.
Wang,A.,Singh,A.,Michael,J.,Hill,F.,Levy,O.,andBowman,S.R. GLUE:Amulti-taskbench-
markandanalysisplatformfornaturallanguageunderstanding. In7thInternationalConferenceon
LearningRepresentations,ICLR2019,NewOrleans,LA,USA,May6-9,2019.OpenReview.net,
2019.
Wołczyk, M., Zaja˛c, M., Pascanu, R., Kucin´ski, Ł., and Miłos´, P. Continual world: A robotic
benchmark for continual reinforcement learning. Advances in Neural Information Processing
Systems,34:28496–28510,2021.
Wolczyk, M., Zajkac, M., Pascanu, R., Kucin´ski, L., and Milos´, P. Continual world: A robotic
benchmark for continual reinforcement learning. Advances in Neural Information Processing
Systems,34:28496–28510,2021.
Wolf,T.,Debut,L.,Sanh,V.,Chaumond,J.,Delangue,C.,Moi,A.,Cistac,P.,Rault,T.,Louf,R.,
Funtowicz,M.,Davison,J.,Shleifer,S.,vonPlaten,P.,Ma,C.,Jernite,Y.,Plu,J.,Xu,C.,LeScao,
T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. Transformers: State-of-the-art natural
languageprocessing. InProceedingsofthe2020ConferenceonEmpiricalMethodsinNatural
LanguageProcessing: SystemDemonstrations,pp.38–45,Online,October2020.Associationfor
ComputationalLinguistics. doi: 10.18653/v1/2020.emnlp-demos.6.
Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., andTorralba, A. SUNdatabase: Large-scalescene
recognitionfromabbeytozoo. InTheTwenty-ThirdIEEEConferenceonComputerVisionand
Pattern Recognition, CVPR 2010, San Francisco, CA, USA, 13-18 June 2010, pp. 3485–3492.
IEEEComputerSociety,2010. doi: 10.1109/CVPR.2010.5539970.
Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J.T., Li, Z., Weller, A., andLiu, W.
Metamath: Bootstrapyourownmathematicalquestionsforlargelanguagemodels. InTheTwelfth
InternationalConferenceonLearningRepresentations,ICLR2024,Vienna,Austria,May7-11,
2024.OpenReview.net,2024.
Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. Meta-world: A
benchmarkandevaluationformulti-taskandmetareinforcementlearning. InConferenceonrobot
learning,pp.1094–1100.PMLR,2020.
17Zellers,R.,Holtzman,A.,Bisk,Y.,Farhadi,A.,andChoi,Y. Hellaswag: Canamachinereallyfinish
yoursentence? InProceedingsofthe57thAnnualMeetingoftheAssociationforComputational
Linguistics,2019.
Zhai,X.,Puigcerver,J.,Kolesnikov,A.,Ruyssen,P.,Riquelme,C.,Lucic,M.,Djolonga,J.,Pinto,
A.S.,Neumann,M.,Dosovitskiy,A.,Beyer,L.,Bachem,O.,Tschannen,M.,Michalski,M.,Bous-
quet,O.,Gelly,S.,andHoulsby,N. Thevisualtaskadaptationbenchmark. CoRR,abs/1910.04867,
2019.
Zhang,Q.,Chen,M.,Bukharin,A.,He,P.,Cheng,Y.,Chen,W.,andZhao,T. Adaptivebudgetallo-
cationforparameter-efficientfine-tuning. InTheEleventhInternationalConferenceonLearning
Representations,ICLR2023,Kigali,Rwanda,May1-5,2023.OpenReview.net,2023a.
Zhang,Z.,Liu,B.,andShao,J. Fine-tuninghappensintinysubspaces: Exploringintrinsictask-
specificsubspacesofpre-trainedlanguagemodels. InRogers,A.,Boyd-Graber,J.,andOkazaki,
N.(eds.),Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguis-
tics (Volume 1: Long Papers), pp. 1701–1713, Toronto, Canada, July 2023b. Association for
ComputationalLinguistics. doi: 10.18653/v1/2023.acl-long.95.
Zheng,T.,Zhang,G.,Shen,T.,Liu,X.,Lin,B.Y.,Fu,J.,Chen,W.,andYue,X. Opencodeinterpreter:
Integratingcodegenerationwithexecutionandrefinement. https://arxiv.org/abs/2402.14658,2024.
Zi, B., Qi, X., Wang, L., Wang, J., Wong, K., and Zhang, L. Delta-lora: Fine-tuning high-rank
parameterswiththedeltaoflow-rankmatrices. CoRR,abs/2309.02411,2023. doi: 10.48550/
ARXIV.2309.02411.
Zitkovich,B.,Yu,T.,Xu,S.,Xu,P.,Xiao,T.,Xia,F.,Wu,J.,Wohlhart,P.,Welker,S.,Wahid,A.,
Vuong,Q.,Vanhoucke,V.,Tran,H.T.,Soricut,R.,Singh,A.,Singh,J.,Sermanet,P.,Sanketi,P.R.,
Salazar,G.,Ryoo,M.S.,Reymann,K.,Rao,K.,Pertsch,K.,Mordatch,I.,Michalewski,H.,Lu,Y.,
Levine,S.,Lee,L.,Lee,T.E.,Leal,I.,Kuang,Y.,Kalashnikov,D.,Julian,R.,Joshi,N.J.,Irpan,
A.,Ichter,B.,Hsu,J.,Herzog,A.,Hausman,K.,Gopalakrishnan,K.,Fu,C.,Florence,P.,Finn,
C.,Dubey,K.A.,Driess,D.,Ding,T.,Choromanski,K.M.,Chen,X.,Chebotar,Y.,Carbajal,J.,
Brown,N.,Brohan,A.,Arenas,M.G.,andHan,K. RT-2: vision-language-actionmodelstransfer
webknowledgetoroboticcontrol. InTan,J.,Toussaint,M.,andDarvish,K.(eds.),Conferenceon
RobotLearning,CoRL2023,6-9November2023,Atlanta,GA,USA,volume229ofProceedings
ofMachineLearningResearch,pp.2165–2183.PMLR,2023.
18Supplementary Material
FabianPaischer1* LukasHauzenberger1* ThomasSchmied1
BenediktAlkin1,3 MarcPeterDeisenroth2 SeppHochreiter1,3
1ELLISUnit,LITAILab,InstituteforMachineLearning,JKULinz,Austria
2UniversityCollegeLondon
3NXAIGmbH,Linz,Austria
paischer@ml.jku.at
Contents
A ReproducibilityStatement 20
B Naturallanguagegeneration 20
B.1 Implementationdetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.2 Hyperparametersearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.3 Additionalresults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C Naturallanguageunderstanding 22
C.1 DatasetStatistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.2 ImplementationDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.3 Hyperparametersearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.4 Additionalresults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D ImageClassification 25
D.1 Datasetstatistics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.2 Implementationdetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.3 Hyperparametersearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
D.4 Additionalresults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
E DecisionMaking 31
E.1 Datasetstatistics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
E.2 Implementationdetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
E.3 Hyperparametersearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
E.4 Additionalresults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
F SVDconvergenceanalysis 33
F.1 BatchSizeinvariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
F.2 ExcludingignoredtokensforSVD . . . . . . . . . . . . . . . . . . . . . . . . . . 33
G Rankre-distributionanalysis 34
19A ReproducibilityStatement
We provide the source code to reproduce our experiments for language generation and un-
derstanding, as well as our RL experiments at https://github.com/ml-jku/EVA. For im-
age classification we used custom implementations that are available at https://github.com/
BenediktAlkin/vtab1k-pytorch. We also integrated EVA into the widely used PEFT library
(seehttps://github.com/sirluk/peft). Aminimalexampleforfine-tuninganymodelavail-
ableonthehuggingfacehubcanbefoundathttps://github.com/sirluk/peft/blob/main/
examples/eva_finetuning/eva_finetuning.py.
B Naturallanguagegeneration
WefollowtheexperimentsconductedinHuetal.(2023)andfine-tuneLlama-2-7B,Llama-3.1-8B
andGemma-2-9Bon8commonsensereasoningtaskswithqastyleprompts. Wekeeptheoriginal
prompttemplatesunchangedasidefromtwominormodifications: ForBoolQweprependthethe
passagefieldbeforethequestionandforWinoGrandeweaddaline"Answerformat: ..."analogous
totheotherprompts. AsdonebyHuetal.(2023)aswellasLiuetal.(2024a)weperformjoint
finetuningonall8tasks. Wefurthermoreevaluatethepre-trainedmodelsmentionedaboveonthe
mathematicalreasoningtasksGSM8K(Cobbeetal.,2021)andMath(Yuetal.,2024)afterfinetuning
on MetaMathQA (Yu et al., 2024) as done in Meng et al. (2024). We keep the original prompt
templateforfinetuningandevaluation. Foralldatasetswerunfinetuningforoneepoch.
B.1 Implementationdetails
ForfinetuningourcodebaseleveragespeftimplementationsofadaptermethodsLoRA,AdaLoRA,
PiSSA, OLoRA and DoRA. The initialization step for EVA is a custom implementation but for
finetuning we can reformulate EVA as a LoRA adapter leveraging the rank_pattern argument of
peft.LoraConfig. ForevaluationweleveragescriptsprovidedbytheMetaMathgithubrepository(Yu
etal.,2024)formathreasoningtasks. Forcommonsensereasoningwemakeuseofthelmevaluation
harnessproject(Gaoetal.,2024)anddefinecustomtasksusingthefinetuningprompts. FortheSVD
computationforjointfinetuningonthecommonsensereasoningtasksweexperimentwithrandom
andstratifiedsamplingofexamplesfromthe8tasksanddonotnoticeadifferenceinperformance.
AlltrainingandevaluationrunsforLlama-2-7Bweredoneon4A100GPUs. RunsforLlama-3.1-8B
andGemma-2-9Butilizedtwodifferentnodes,onewith4A100GPUsandonewith4H200GPUs.
B.2 Hyperparametersearch
The reported results on language generation Table9: hyperparametersforfinetuningoncom-
tasks in Table 2 and Table 3 are the best set- monsensereasoningandmathreasoning
tingbasedonagridsearchoverdifferentlearn-
Training
ingrates. Weapplyadapterstoalllinearlayers
including the language modelling head. Fur- Optimizer AdamW
thermorewesetα=1forallourexperiments. WeightDecay 0.0
WeuseAdamWwithweightdecayandalinear LoraDropout 0.0
learningrateschedulewithwarm-up. Wetrain BatchSize 32
for1epochandusethefinalcheckpointforeval- #Epoch 1
uation. Allhyperparametersaresummarizedin LRSchedule Linear
Table9 Warmupratio 0.03
LabelSmooth 0.0
B.3 Additionalresults LearningRate 5e-4
LoRADim 16
In addition to the results presented in Table 2 LoRAα 1
and Table 3 we also fine-tune Llama-2-7B on BatchSizeSVD(EVA) 16
theCode-FeedbackdatasetZhengetal.(2024) τ cossim0.99
consistingofmulti-turnconversationsbetween
Inference
userandAIAssistant. Duetolimitedcomputa-
tionalresourcesandthelongsequencelengthsof BeamSize 1.0
theexamplesinthisdatasetwedonotfine-tune LengthPenalty 1.0
repetitionpenalty 1.0
20Table 8: Prompt templates with examples (red) used for finetuning on common sense and math
reasoningtasks.
Dataset Fine-tuningDataTemplate
BoolQ Passage: Drinkinginpublic–Drinkinginpublicismostcommonlyaccepted.
Afterreadingthispassage,pleaseanswerthefollowingquestionwithtrueor
false,question: canyoudrinkonthestreetinchina
Answerformat: true/false
thecorrectansweristrue
PIQA Pleasechoosethecorrectsolutiontothequestion: Whenboilingbutter,when
it’sready,youcan
Solution1: Pouritontoaplate
Solution2: Pouritintoajar
Answerformat: solution1/solution2
thecorrectanswerissolution2
SIQA Pleasechoosethecorrectanswertothequestion: Carsonrelocatedsomewhere
new. HowwouldyoudescribeCarson?
Answer1: mobile
Answer2: anxious
Answer3: lonely
Answerformat: answer1/answer2/answer3
thecorrectanswerisanswer1
HellaSwag Pleasechoosethecorrectendingtocompletethegivensentence: Playing
drums: Peoplearestandingbehindlargedrums. Aman
Ending1: isplayingabagpipe.
Ending2: startstoplayaroundthedrums.
Ending3: beginsplayingadrumset.
Ending4: beginsplayingthedrums.
Answerformat: ending1/ending2/ending3/ending4
thecorrectanswerisending4
WinoGrande Pleasechoosethecorrectanswertofillintheblanktocompletethegiven
sentence: IanvolunteeredtoeatDennis’smenudoafteralreadyhavingabowl
because_despisedeatingintestine.
Option1: Ian
Option2: Dennis
Answerformat: option1/option2
thecorrectanswerisoption2
ARC-e& Pleasechoosethecorrectanswertothequestion: Whichfactorwillmost
ARC-c likelycauseapersontodevelopafever?
Answer1: alegmusclerelaxingafterexercise
Answer2: abacterialpopulationinthebloodstream
Answer3: severalviralparticlesontheskin
Answer4: carbohydratesbeingdigestedinthestomach
Answerformat: answer1/answer2/answer3/answer4
thecorrectanswerisanswer2
OBQA Pleasechoosethecorrectanswertothequestion: Thesunisresponsiblefor
Answer1: puppieslearningnewtricks
Answer2: childrengrowingupandgettingold
Answer3: flowerswiltinginavase
Answer4: plantssprouting,bloomingandwilting
Answerformat: answer1/answer2/answer3/answer4
thecorrectanswerisanswer4
MetaMathQA Belowisaninstructionthatdescribesatask. Writearesponsethat
appropriatelycompletestherequest.
###Instruction:
Whatisthevalueofthecosineof90degrees?
###Response:
s$\\boxed{0}$.Theansweris: 0
21Llama-3.1-8BandGemma-2-9BoranyDoRA
variants. Weevaluatethefine-tunedcheckpoints
onfourcodingbenchmarks: MBPPAustinetal.
(2021),HumanEvalChenetal.(2021b),MBPP+
andHumanEval+Liuetal.(2023). Theresults
are presented in Table 10. EVA shows the best performance on MBPP and MBPP+ while also
exhibitinggoodperformanceonHumanEvalandHumanEval+. Onthelattertwodatasets,PiSSA
isthebestperformingmethod. Forfinetuningweuseamaximumsequencelengthof2028with
right-sidetruncation. Fordecodingwesetthetemperatureto0.2andtop_pto0.7
Table10: ComparisonofEVAtootherinitializationandrankre-distributionschemesoncodefine-
tuningdatasets. Wereportmeanandstandarddeviationacrossthreerandomseeds.
Method MBPP HumanEval MBPP+ HumanEval+
LoRA 22.2 18.9 30.7 18.9
±1.1 ±0.6 ±1.1 ±0.6
AdaLoRA 21.5 17.1 29.4 17.1
±0.2 ±0.0 ±0.7 ±0.0
PiSSA 22.8 19.9 30.8 19.9
±1.2 ±0.9 ±0.7 ±0.9
OLoRA 22.3 18.9 32.4 18.9
±0.6 ±0.0 ±0.4 ±0.0
EVA 22.9 18.9 32.6 18.9
±0.7 ±1.2 ±0.6 ±1.2
In Table 11 we report the standard deviation across three seeds from the results in Table 2. For
Llama-3.1-8BandGemma-2-9BEVAhasthesmallestaveragestandarddeviationacrosstasks. For
Llama-2-7BthestandardthevarianceofEVAisonlyslightlyaboveaverageincomparisontoother
methods,mainlyduetothehighstandarddeviationontheBoolQdataset.
C Naturallanguageunderstanding
C.1 DatasetStatistics
ThedatasetstatisticsforeachtaskintheGLUEbenchmark(Wangetal.,2019)areshowninTable12.
Generally,GLUEcontainsfourlow-resourcedatasets(RTE,MRPC,STS-B,andCoLA)andfour
highresourcedatasets(SST-2,QNLI,QQP,MNLI).WhileCoLAandSST-2relyonsinglesentence
classification, STS-B evaluates for similarity and the remaining tasks are based on pairwise text
classification.
C.2 ImplementationDetails
We base our implementation on the codebase of LoRA1. For these experiments, we initially pre-
computeourinitializationpriortothefine-tuningstageandstoreitasacheckpoint. However,wealso
providethepossibilitytodirectlycomputetheinitializationduringthefine-tuningstage,asdonefor
ourexperimentsonVTAB-1kandMeta-World. Bydefault,wealwaysoffloadthecomputationofthe
initialcheckpointtoCPUtosaveVRAM.WeranallourexperimentsonnodeswithfourA100GPUs
andusedPyTorch’sdata-distributedparallelfunctionality(Paszkeetal.,2019). Runtimesranges
fromaslittleas10minutesperrunforsmallerdatasets(RTE,STS-B)toaround15hoursforthe
largestdatasets(QQP,MNLI).
C.3 Hyperparametersearch
ForLoRAandEVA,wesearchoverthenumberofranksr ∈{2,4,6,8}anddifferentlearningrates
η ∈{1e−3,4e−4,1e−4}forRoBERTa andη ∈{4e−3,1e−3,4e−4}forDeBERTav3 .
Large Base
Wereportthebesthyperparametersettingsforboth,RoBERTa andDeBERTav3 forLoRA
Large Base
andEVAinTable13. ForAdaLoRA,wesearchoverthesameranksandalwaysstartinitialranks
withr+4thatarethenredistributedduringtraining. Additionally,wesearchoverthesamelearning
ratesasfortheotherLoRAvariants. Further,weintroducehyperparametersthatresultinadditional
speed-upofourinitialization,namelyathresholdτ thatconsiderscomponentsasconverged,and
athresholdδthatstopscomputationoftheinitializationwhenacertainpercentageofcomponents
1https://github.com/microsoft/LoRA
22Table11: Standarddeviationacrossthreeseedsoncommonsensereasoningtasks.
Model Method BoolQ PIQA SIQA HellaSwag Winogrande ARC-e ARC-c OBQA
LoRA 1.498 0.252 0.233 0.102 0.658 0.072 0.489 0.822
AdaLoRA 1.315 0.251 0.182 0.098 0.392 0.362 0.106 0.899
PiSSA 0.358 0.294 0.138 0.096 0.298 0.386 0.494 1.117
OLoRA 4.938 0.190 0.524 0.062 0.652 0.339 0.672 0.660
Llama-2-7B
EVA 3.858 0.336 0.210 0.059 0.453 0.221 0.358 0.189
DoRA 2.599 0.290 0.483 0.113 0.244 0.215 0.489 0.525
EVA+DoRA 5.281 0.273 0.293 0.034 0.853 0.110 0.494 0.249
LoRA 0.472 0.194 0.419 0.070 0.197 0.052 0.563 0.189
AdaLoRA 0.510 0.044 0.261 0.040 0.392 0.201 0.804 0.748
PiSSA 6.516 0.373 0.603 0.195 0.707 0.325 0.245 0.589
OLoRA 0.298 0.245 0.397 0.057 0.451 0.173 0.329 0.189
Llama-3.1-8B
EVA 0.109 0.320 0.125 0.022 0.591 0.110 0.241 0.189
DoRA 0.225 0.112 0.315 0.014 0.260 0.119 0.698 0.000
EVA+DoRA 0.225 0.168 0.121 0.117 0.392 0.105 0.175 0.249
LoRA 0.095 0.277 0.386 0.062 0.324 0.072 0.070 0.589
AdaLoRA 0.088 0.353 0.217 0.033 0.098 0.209 0.106 0.432
PiSSA 2.761 0.286 0.214 0.109 0.621 0.447 0.121 0.163
OLoRA 0.066 0.451 0.501 0.099 0.501 0.267 0.448 0.573
Gemma-2-9B
EVA 0.275 0.136 0.111 0.094 0.260 0.119 0.040 0.249
DoRA 0.189 0.420 0.301 0.074 0.419 0.091 0.000 0.499
EVA+DoRA 0.132 0.296 0.490 0.070 0.037 0.150 0.715 0.340
Table 12: GLUE benchmark suite statistics and evaluation metric for each corpus sorted by the
numberofexamplesinthetrainingset.
Corpus #Train #Dev #Test Metric
RTE 2.5k 276 3k Accuracy
MRPC 3.7k 408 1.7k Accuracy
STS-B 7k 1.5k 1.4k Pearsoncorrelation
CoLA 8.5k 1k 1k Matthew’scorrelation
SST-2 67k 872 1.8k Accuracy
QNLI 108k 5.7k 5.7k Accuracy
QQP 364k 40k 391k Accuracy
MNLI 393k 20k 20k Accuracy
haveconverged. Bydefault,wesetτ =0.99andδ =1,i.e. weonlystopwhenallcomponentsare
converged,andtheyarealmostexactlythesame. Theseparametersprovideadditionalleewayto
speeduptheinitializationstageofEVA.
WehaveexploredthesensitivityofLoRAtodifferentinitializationschemesandfoundthat,similar
tootherprominentinitializationschemes(Heetal.,2015;Glorot&Bengio,2010),scaleplaysan
importantrolealongwithdirections. Originally,(Huetal.,2022)proposetosetα=2r,however,
wefoundthatthisparameterisquitesensitiveasalsoshownin(Kalajdzievski,2023). Similarly,
differentranksleadtoverydifferentresultsondifferentdownstreamtasks. Therefore,wesuggestto
alwayssearchovermoreranksandchoosethebestperformingoneiftherequiredcomputebudgetis
available. WealsoexperimentedwithdifferentlearningratesfortheAandBmatricesasproposed
in(Hayouetal.,2024b),however,thisdidnotresultinconsistentimprovements. Instead,wefound
thatlearningratesforLoRA-styletrainingcanbesurprisinglyhigh(4e−3forDeBERTav3 ),
Base
whileforlargermodelsthelearningrateneedstobeapproximatelyamagnitudesmaller. Asimple
recipethatworkedconsistentlywell,wassettingα=1,whichresultsinasimilarscalingfactorasin
Kalajdzievski(2023),andsearchingoverasetofsmalllearningratesforlargermodelsandhigher
learningratesforsmallerones. ForEVA,theonlytunablehyperparameteristherankbudget,which
werecommendtotunealongwiththefine-tuninglearningrate.
23Table13:ThebesthyperparametersRoBERTa andDeBERTav3 thatwerefoundviagridsearch
Large Base
foreachtaskoftheGLUEbenchmark.
Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B
Optimizer AdamW
WarmupRatio 0.06
LRSchedule Linear
BatchSize 8 16 8 8 8 8 16 8
#Epochs 10 10 20 20 10 20 20 10
RoBERTa
Large LoRArank 2 8 8 4 8 4 2 2
LoRA
Learningrate 4e-4 1e-3 4e-4 1e-3 1e-3 1e-3 1e-3 4e-4
LoRAα 1
MaxSeq.Len. 512
DDPGPUs 4
BatchSize 8 16 8 8 8 8 16 8
#Epochs 10 10 20 20 10 20 20 10
RoBERTa
Large LoRArank 2 2 4 2 16 8 4 4
EVA
Learningrate 4e-4 1e-3 4e-4 1e-3 4e-4 1e-3 1e-3 1e-3
LoRAα 1
MaxSeq.Len. 512
DDPGPUs 4
BatchSize 32 32 16 32 64 32 32 16
#Epochs 30 60 30 80 25 25 80 40
DeBERTav3
Base LoRArank 8 4 4 8 16 4 4 8
LoRA
Learningrate 4e-4 1e-3 4e-3 4e-3 4e-3 4e-3 4e-3 4e-3
LoRAα 1
MaxSeq.Len. 512
DDPGPUs 4
BatchSize 32 32 16 32 64 32 32 16
DeBERTav3 #Epochs 30 60 30 80 25 25 80 40
Base
EVA LoRArank 8 2 4 8 16 4 2 2
Learningrate 4e-4 4e-4 4e-3 4e-3 4e-3 4e-3 4e-3 4e-3
LoRAα 1
MaxSeq.Len. 512
DDPGPUs 4
C.4 Additionalresults
We report additional results for EVA compared to LoRA for different rank budgets in Table 14.
We find that EVA consistently outperforms LoRA for different rank budgets. This demonstrates
the effectiveness of EVA among different compute budgets. Further, we show additional rank
redistributionsfortheCoLA,MRPC,RTE,andSTSBtasksfordifferentforr =2(Figure4),r =4
(Figure5),r =8(Figure6),andr =16(Figure7)forboth,RoBERTa andDeBERTav3 .The
Large Base
distributionsforthedifferentmodelsshowdifferentpatterns.ForDeBERTav3 thehigherattention
Base
layersusuallyreceivemoreranksthanlowerones. ForCoLA,thereisalsoahighnumberofranksin
theveryfirstlayer. ForRoBERTa itseemstobetheopposite,astheveryfirstlayersconsistently
Large
receivemorerankscomparedtolaterlayers. Thereisalsoanotabledifferenceacrosstasksforboth
models,whichdemonstratestheflexibilityofEVAtoallocateranksdependentonthedownstream
task. Interestingly,forahigherinitialrank(r =16),theredistributionforDeBERTav3 putsmore
Base
emphasisonfine-tuningtheself-attentionspecificweightmatrices. ThisisnottrueforRoBERTa ,
Large
asW alsoreceivesplentyofranksacrossalltasks. Overall,therankredistributionincursdifferent
f1
fine-tuningparadigmsdependingonthetaskandtheinitialrank.
Additionally,weshowresultsfordifferentrankredistributionsthatweobtainbyusingalternative
measuresforexplainedvariance. Specifically,wecompareEVAtousing,(i),theraweigenvalues
(EVA-Raw),and(ii),normalizingbythemaximumeigenvalue(EVA-Max). Wereportresultsfor
RoBERTa onfouroftheGLUEtasks,namelyCoLA,RTE,MRPC,andSTS-BinTable15. Our
Large
24Table14: ComparisonofLoRAtoEVAusingRoBERTa onalltasksfromGLUEforequalrank
Large
budgets. MeanandstandarddeviationofMatthew’scorrelationforCoLA,pearsoncorrelationfor
STS-B,andaccuracyforremainingdatasetsonthedevelopmentsetacross5seedsareshown.
Method CoLA MRPC RTE STS-B MNLI QNLI QQP SST-2 Avg
LoRA 68.0 90.9 88.1 92.3 91.9 94.8 90.6 96.1 89.09
r=2 ±1.4 ±.8 ±1.1 ±.1 ±.1 ±.3 ±.1 ±.1
EVA 69.1 90.8 88.2 92.5 90.8 94.9 91.9 96.2 89.30
r=2 ±1.4 ±.5 ±.7 ±.1 ±.1 ±.1 ±.1 ±.1
LoRA 69.1 90.7 86.9 92.3 90.6 94.7 92.0 96.0 89.04
r=4 ±.5 ±.7 ±.2 ±.1 ±.1 ±.2 ±.0 ±.1
EVA 69.5 91.4 88.8 92.6 90.7 94.9 91.8 96.1 89.48
r=4 ±1.4 ±.8 ±1.3 ±.1 ±.0 ±.1 ±.0 ±.1
LoRA 68.8 91.1 87.1 92.2 90.6 94.8 91.8 96.2 89.08
r=8 ±1.0 ±.6 0.7 ±.2 ±.2 ±.1 ±.0 ±.3
EVA 69.0 91.1 88.4 92.6 90.6 94.9 92.1 96.1 89.35
r=8 ±1.4 ±.4 ±.6 ±.3 ±.1 ±.1 ±.1 ±.2
LoRA 68.4 90.5 88.0 92.3 90.6 94.8 91.9 96.1 89.08
r=16 ±1.0 ±.5 ±.5 ±.1 ±.1 ±.1 ±.1 ±.1
EVA 69.1 91.2 88.0 92.6 90.7 95.0 91.8 96.2 89.33
r=16 ±.8 ±.8 ±.5 ±.2 ±.0 ±.2 ±.0 ±.1
Table15: ComparisonofLoRAtoEVA,EVA-Raw,andEVA-MaxforRoBERTa ontheGLUE
Large
tasksCoLA,MRPC,RTE,andSTS-B.WereportmeanandstandarddeviationofMatthew’scor-
relationforCoLA,pearsoncorrelationforSTS-B,matchedaccuracyforMNLI,andaccuracyfor
remainingtasksacross5seeds.
Method CoLA MRPC RTE STS-B Avg
LoRA 69.1 91.1 88.1 92.3 85.2
±.5 ±0.6 ±1.1 ±0.1
EVA 69.5 91.4 88.8 92.6 85.6
±1.4 ±0.8 ±1.2 ±0.1
EVA-Raw 69.4 91.0 88.2 92.5 85.3
±1.1 ±0.9 ±0.3 ±0.2
EVA-Max 69.1 91.2 88.4 92.5 85.3
±0.5 ±0.5 ±1.2 ±0.2
resultsshowthatwhileEVA-RawandEVA-MaxslighthlyimproveuponLoRA,theyperformworse
onaveragethanEVA.
D ImageClassification
D.1 Datasetstatistics
TheVTAB-1Kbenchmarkconsistsof19datasets,eachcontainingasubsetof1000examplesof
theirrespectivesamples. WesummarizethedatasetstatisticsforeachdatasetinTable16. Whilethe
originaltrainsizesofthedatasetsvarydrastically,the1Ksubsetprovidesequaldatasetsacrosstasks.
Thenumberofclassesalsovariesfromaslittleastwotoalmost400.
D.2 Implementationdetails
Weimplementedacustompipelinetofine-tuneDINOv2-L/14onVTAB-1KthatsupportsLoRA,
DoRAandEVA.TotrainAdaLora, PiSSAandOLoRA,weintegratetheirimplementationfrom
thepeftlibrary(Mangrulkaretal.,2022)intoourpipeline. Thispipelineisdesignedtobehighly
parallelizableandtobeexecutedonindividualGPUs. AsingleevaluationrunofaL/14model(all
19 datasets with hyperparameter tuning and evaluation) takes roughly 160 A100 GPU-hours but
canbeeasilyparallelized. Ag/14runtakesroughly140H100GPU-hours. Asingleevaluationrun
consistsof1140hyperparametertuningruns(19datasets*5learningrates*4ranks*3seeds)and
95evaluationruns(19datasets*5seeds). Detailstohyperparametertuningaredescribedbelow.
WeusetheoriginalDINOv2models(Oquabetal.,2023)andtrainaclassificationheadontopof
the[CLS]token,whereweinitializetheclassificationheadweightswithanormaldistributionwith
σ =2e-5andbiaswithzeros. Wetraintheclassificationhead,LoRAmatricesandbiases. Imagesare
resizedto224×224resolutionwithbi-cubicinterpolationandnormalizedwiththeper-channelmean
andvarianceofImageNet. Wetrainallmodelsinbfloat16precisionusingtheAdamWoptimizerwith
aweightdecayof0.05for30epochs. Weuseacosinelearningrateschedulewithalinearwarm-up
25CoLA CoLA
MRPC MRPC
RTE RTE
STSB STSB
Figure 4: Rank distribution after initialization with EVA on four tasks of the GLUE benchmark
(CoLA,MRPC,RTE,STSB)forDeBERTav3 (left)andRoBERTa (right)withinitialrank
Base Large
r =2.
26CoLA CoLA
MRPC MRPC
RTE RTE
STSB STSB
Figure 5: Rank distribution after initialization with EVA on four tasks of the GLUE benchmark
(CoLA,MRPC,RTE,STSB)forDeBERTav3 (left)andRoBERTa (right)withinitialrank
Base Large
r =4.
27CoLA CoLA
MRPC MRPC
RTE RTE
STSB STSB
Figure 6: Rank distribution after initialization with EVA on four tasks of the GLUE benchmark
(CoLA,MRPC,RTE,STSB)forDeBERTav3 (left)andRoBERTa (right)withinitialrank
Base Large
r =8.
28CoLA CoLA
MRPC MRPC
RTE RTE
STSB STSB
Figure 7: Rank distribution after initialization with EVA on four tasks of the GLUE benchmark
(CoLA,MRPC,RTE,STSB)forDeBERTav3 (left)andRoBERTa (right)withinitialrank
Base Large
r =16.
29Table16: Category,trainsizeandclassesoftheVTAB-1Kdataset.
Category Dataset Trainsize Classes
Natural Caltech101(Fei-Feietal.,2006) 3060 102
Natural CIFAR-100(Krizhevsky,2009) 50000 100
Natural DTD(Cimpoietal.,2014) 3760 47
Natural Flowers102(Nilsback&Zisserman,2008) 2040 102
Natural Pets(Parkhietal.,2012) 3680 37
Natural Sun397(Xiaoetal.,2010) 87003 397
Natural SVHN(Netzeretal.,2011) 73257 10
Specialized EuroSAT(Helberetal.,2019) 21600 10
Specialized Resisc45(Chengetal.,2017) 25200 45
Specialized PatchCamelyon(Veelingetal.,2018) 294912 2
Specialized Retinopathy(Kaggle&EyePacs,2015) 46032 5
Structured Clevr/count(Johnsonetal.,2017) 70000 8
Structured Clevr/distance(Johnsonetal.,2017) 70000 6
Structured dSprites/location(Mattheyetal.,2017) 663552 16
Structured dSprites/orientation(Mattheyetal.,2017) 663552 16
Structured SmallNORB/azimuth(LeCunetal.,2004) 36450 18
Structured SmallNORB/elevation(LeCunetal.,2004) 36450 9
Structured DMLab(Beattieetal.,2016) 88178 6
Structured KITTI/distance(Geigeretal.,2013) 5711 4
Table17: StandarddeviationsfortheVTAB-1Kresults(Table5)over5seeds.
Natural Specialized Structured
FFT 1.5 1.1 1.6 0.0 0.4 1.2 0.9 14.9 0.4 0.6 2.7 1.7 0.9 1.2 23.6 0.5 0.4 1.6 1.9 3.0
LoRA 0.2 0.4 0.2 0.0 0.3 36.4 0.1 0.5 0.3 0.1 0.4 0.2 0.3 0.5 1.2 0.4 0.4 0.7 0.4 2.3
AdaLoRA 0.0 0.2 0.4 0.0 0.1 0.4 0.1 0.3 0.3 0.2 0.3 0.3 0.2 0.3 0.8 0.8 0.3 0.3 0.4 0.3
PiSSA 0.2 0.4 0.3 0.0 0.2 0.5 0.2 0.7 0.2 0.1 0.4 0.3 0.4 0.2 0.7 0.3 0.5 0.4 0.5 0.3
OLoRA 0.3 0.3 0.4 0.0 0.3 29.4 0.1 0.3 0.1 0.2 0.2 0.5 0.1 0.3 24.6 0.3 0.4 0.3 0.8 3.1
EVA 0.2 0.5 0.2 0.0 0.1 0.3 0.1 0.3 0.2 0.3 0.4 0.5 0.3 0.6 0.6 0.5 0.5 0.2 0.5 0.3
DoRA 0.1 0.2 0.5 0.0 0.2 29.7 0.4 0.7 0.1 0.2 0.4 0.4 0.3 0.3 0.6 36.2 0.5 0.3 0.3 3.8
EVA+DoRA 0.2 1.3 0.6 0.0 0.3 0.5 0.3 0.4 0.2 0.3 0.3 0.4 0.4 12.8 1.3 2.5 0.3 0.6 0.6 1.2
forthefirst3epochs. Batchsizeissetto64whereweusegradientaccumulationifthebatchsize
doesnotfitintoGPUmemory. Fullfine-tuningusesalayer-wiselrdecay(Clarketal.,2020)of0.75.
D.3 Hyperparametersearch
We first fine-tune on the 800 train samples of VTAB-1K datasets to find the best learning rate
for the task. We sweep over learning_rate ∈ {2.5e-3,1e-3,7.5e-4,5e-4,2.5e-4} and rank ∈
{2,4,8,16}andaveragetheaccuracyonthe200validationsamplesover3differentseedstochoose
thebestlearningrateandrankforeachdataset. Forevaluation,wetrainontheunionoftrainand
validationsetusing5differentseedsandreporttheaverageaccuracyonthetestset.
D.4 Additionalresults
TocomplementourmainresultsinTable5,wereporttherespectivestandarddeviationsinTable17.
30
001rafiC 101hcetlaC
DTD
201rewolF
steP
NHVS 793nuS noylemaC TASoruE 54csiseR yhtaponiteR tnuoC-rvelC tsiD-rvelC baLMD tsiD-ITTIK coL-rpSd irO-rpSd
mizA-BRONs
elE-BRONs egarevAE DecisionMaking
E.1 Datasetstatistics
Meta-World(Yuetal.,2020)isanestablishedbenchmarkinRLformulti-taskcontinuouscontrol.
The benchmark consists of 50 challenging robotics tasks simulated using a Sawyer robotic arm
intheMuJoCophysicsengine(Todorovetal.,2012). All50tasksinMeta-Worldsharethesame
underlyingroboticarm. Therefore,alltasksshareacommonstate(39-dimensionalcontinuousvector)
andaction-space(6-dimensional). TherewardfunctionsinMeta-Worldaredenseandbasedonthe
distanceoftheroboticarmtothegoallocationorobjects. Allepisodeslastfor200environment
interactions.
ForourexperimentsonMeta-World,weleveragethedatasetsreleasedbySchmiedetal.(2023). We
followWołczyketal.(2021)andSchmiedetal.(2023),andsplitthe50tasksinto40pre-training
tasks(MT40)and10fine-tuningtasks(CW10). TheCW10tasksare:
hammer-v2, push-wall-v2, faucet-close-v2, push-back-v2, stick-pull-v2,
stick-pull-v2, handle-press-side-v2, push-v2, shelf-place-v2, window-close-v2,
andpeg-unplug-side-v2.
Thedatasetscontain2Mtransitionsforeveryofthe50tasks,amountingto80Mtransitions(320M
tokens)acrossalltrainingtasks. TheaveragesuccessrateandrewardsacrossallMT40tasksare84%
and1414.62,respectively. WelistthestatisticspertaskinTable18.
E.2 Implementationdetails
WeimplementedourpipelinethatsupportstrainingforMeta-Worldontopofthecode-baseprovided
bySchmiedetal.(2023). OurcustomimplementationsupportstrainingLoRA,DoRAandEVA.
Furthermore,weleveragethepeftlibrary(Mangrulkaretal.,2022)totraintheremainingmethods.
ForourexperimentsonMeta-World,weuseaGPT2-likenetworkarchitecture(Radfordetal.,2019)
with4Transformerlayers,8heads,andhiddendimensionof512resultingin16Mparameters. We
useacontextof50timesteps,whichamountstoasequencelengthof200,aseachtimestepcontains
states,actions,rewardsandRTGs. Weembedstates,actions,rewardsandreturn-to-gos(RTGs)using
separatelinearembeddinglayerspermodality,asproposedbyChenetal.(2021a). Wetrainwitha
batchsizeof128usingaconstantlearningrateof1e−4,4000linearwarm-upstepsfollowedbya
cosinedecayto1e−6,usingtheAdamWoptimizer(Loshchilov&Hutter,2017). Weemploygradient
clippingof0.25,weightdecayof0.01,andadropoutrateof0.2. OurDTimplementationemploys
globalpositionembedding. Foreverytask,wesetthetargetreturntothemaximumreturnachieved
intherespectivetrainingdatasets,asproposedby(Schmiedetal.,2023). Furthermore,weemploy
mixed-precision(Micikeviciusetal.,2017)andflash-attention(Dao,2023)tospeed-uptraining.
Wefirstpre-trainaDTonallMT40tasks(80Mtransitions)for1Mupdatesvianext-actionprediction
byminimizingthemean-squarederror. Theresultingpre-trainedmodelattainsanaveragesuccess
rateof80%acrossallMT40tasks. Thenwefine-tunetheDToneachoftheCW10down-stream
tasksfor100Kupdateswiththesamesetofhyperparametersasusedforpre-training. Werunallour
experimentsonapublicresearchclusterwith4xA100-40GBGPUnodes. Asinglefine-tuningrun
withEVAforonetasktakesroughly1hourononeA100.
E.3 Hyperparametersearch
Inlinewithpreviousexperiments,wetunetherankforLoRA,DoRA,AdaLoraandEVA,rank∈
{2,4,8,16}. Further,wesweepoverthesamelearningratesasfortheGLUEtasks.
E.4 Additionalresults
In Table 19, we show the full comparison for all methods on CW10. EVA+DoRA consistently
outperformsallcompetitorsforthedifferentrankbudgets.
31Table18: DatasetstatisticsforallMT40tasksfromSchmiedetal.(2023).
Task |S| |A| SuccessRate Reward
assembly-v2 39 4 0.0 1206.9
basketball-v2 39 4 0.9 1375.95
bin-picking-v2 39 4 0.0 474.81
box-close-v2 39 4 0.0 759.15
button-press-topdown-v2 39 4 1.0 1299.24
button-press-topdown-wall-v2 39 4 1.0 1296.16
button-press-v2 39 4 1.0 1430.44
button-press-wall-v2 39 4 1.0 1508.16
coffee-button-v2 39 4 1.0 1499.17
coffee-pull-v2 39 4 1.0 1313.88
coffee-push-v2 39 4 0.6 508.14
dial-turn-v2 39 4 0.8 1674.29
disassemble-v2 39 4 1.0 1396.55
door-close-v2 39 4 1.0 1535.4
door-lock-v2 39 4 1.0 1712.65
door-open-v2 39 4 1.0 1544.32
door-unlock-v2 39 4 1.0 1733.64
drawer-close-v2 39 4 1.0 1845.92
drawer-open-v2 39 4 1.0 1710.65
faucet-open-v2 39 4 0.9 1727.98
hand-insert-v2 39 4 1.0 1607.17
handle-press-v2 39 4 1.0 1854.79
handle-pull-side-v2 39 4 1.0 1613.72
handle-pull-v2 39 4 1.0 1581.75
lever-pull-v2 39 4 1.0 1449.05
peg-insert-side-v2 39 4 1.0 1545.19
pick-out-of-hole-v2 39 4 1.0 1435.64
pick-place-v2 39 4 0.0 6.59
pick-place-wall-v2 39 4 0.1 702.59
plate-slide-back-side-v2 39 4 1.0 1766.24
plate-slide-back-v2 39 4 1.0 1773.56
plate-slide-side-v2 39 4 1.0 1663.35
plate-slide-v2 39 4 1.0 1667.35
reach-v2 39 4 1.0 1858.99
reach-wall-v2 39 4 1.0 1831.14
soccer-v2 39 4 0.4 445.84
stick-push-v2 39 4 1.0 1470.71
sweep-into-v2 39 4 1.0 1761.69
sweep-v2 39 4 1.0 1458.35
window-open-v2 39 4 1.0 1537.59
Average - - 0.84±0.34 1414.62±439.39
32Table19: Rank-wisecomparisonforallmethodsonCW10. Wefine-tunea12MDTon10tasks
individuallyandreportthemeansuccessrates/rewards(±standarderror)foreverytask.
Method Rank
FFT - 0.97±0.03 0.93±0.03 1.0±0.0 0.6±0.05 0.7±0.12 1.0±0.0 0.93±0.03 1.0±0.0 0.57±0.07 1.0±0.0 0.87±0.03
LoRA 2 1.0±0.0 1.0±0.0 1.0±0.0 0.6±0.05 0.57±0.07 0.97±0.03 0.93±0.03 1.0±0.0 0.37±0.1 1.±0.0 0.84±0.04
4 1.0±0.0 0.97±0.03 1.0±0.0 0.47±0.12 0.63±0.1 0.97±0.03 1.0±0.0 1.0±0.0 0.23±0.12 1.0±0.0 0.83±0.05
8 1.0±0.0 0.97±0.03 1.0±0.0 0.43±0.05 0.4±0.09 0.97±0.03 0.93±0.03 1.0±0.0 0.23±0.12 1.0±0.0 0.79±0.06
16 1.0±0.0 0.97±0.03 1.0±0.0 0.43±0.03 0.47±0.03 1.0±0.0 0.97±0.03 1.0±0.0 0.4±0.09 1.0±0.0 0.82±0.05
DoRA 2 1.0±0.0 1.0±0.0 1.0±0.0 0.57±0.05 1.0±0.0 1.0±0.0 1.0±0.0 1.0±0.0 0.33±0.11 1.0±0.0 0.89±0.04
4 1.0±0.0 1.0±0.0 1.0±0.0 0.6±0.12 1.0±0.0 1.0±0.0 1.0±0.0 1.0±0.0 0.43±0.12 1.0±0.0 0.9±0.04
8 1.0±0.0 1.0±0.0 1.0±0.0 0.47±0.12 0.93±0.05 1.0±0.0 1.0±0.0 1.0±0.0 0.57±0.15 1.0±0.0 0.9±0.04
16 1.0±0.0 1.0±0.0 1.0±0.0 0.57±0.12 1.0±0.0 1.0±0.0 1.0±0.0 1.0±0.0 0.67±0.15 1.0±0.0 0.92±0.03
AdaLoRA 2 1.0±0.0 0.97±0.03 1.0±0.0 0.37±0.05 0.37±0.05 0.93±0.05 0.97±0.03 1.0±0.0 0.13±0.07 1.0±0.0 0.77±0.06
4 1.0±0.0 0.97±0.03 1.0±0.0 0.37±0.07 0.57±0.1 0.97±0.03 0.9±0.08 1.0±0.0 0.13±0.07 1.0±0.0 0.79±0.06
8 1.0±0.0 0.97±0.03 1.0±0.0 0.3±0.05 0.57±0.14 0.93±0.03 0.87±0.07 1.0±0.0 0.0±0.0 1.0±0.0 0.76±0.06
16 1.0±0.0 0.97±0.03 1.0±0.0 0.4±0.09 0.57±0.12 0.97±0.03 0.93±0.05 1.0±0.0 0.0±0.0 1.0±0.0 0.78±0.06
oLoRA 2 1.0±0.0 0.9±0.05 1.0±0.0 0.47±0.03 0.33±0.03 0.97±0.03 0.970.03 1.0±0.0 0.27±0.11 1.0±0.0 0.79±0.05
4 1.0±0.0 0.9±0.05 1.0±0.0 0.43±0.03 0.63±0.12 1.0±0.0 1.00.0 1.0±0.0 0.6±0.12 1.0±0.0 0.86±0.04
8 1.0±0.0 0.97±0.03 1.0±0.0 0.57±0.1 0.5±0.08 1.0±0.0 1.00.0 1.0±0.0 0.53±0.14 1.0±0.0 0.86±0.04
16 1.0±0.0 0.97±0.03 1.0±0.0 0.4±0.05 0.63±0.03 1.0±0.0 1.00.0 1.0±0.0 0.43±0.05 1.0±0.0 0.84±0.04
PiSSa 2 1.0±0.0 0.97±0.03 1.0±0.0 0.43±0.11 0.53±0.07 0.97±0.03 0.90.08 1.0±0.0 0.33±0.17 1.0±0.0 0.81±0.05
4 1.0±0.0 1.0±0.0 1.0±0.0 0.37±0.07 0.7±0.05 0.97±0.03 1.00.0 1.0±0.0 0.07±0.05 1.0±0.0 0.81±0.06
8 1.0±0.0 0.97±0.03 1.0±0.0 0.3±0.0 0.57±0.03 0.97±0.03 1.00.0 1.0±0.0 0.53±0.1 1.0±0.0 0.83±0.05
16 1.0±0.0 0.93±0.03 1.0±0.0 0.33±0.12 0.47±0.03 1.0±0.0 0.970.03 1.0±0.0 0.47±0.11 1.0±0.0 0.82±0.05
EVA 2 1.0±0.0 0.97±0.03 1.0±0.0 0.43±0.07 0.77±0.05 0.97±0.03 1.0±0.0 1.0±0.0 0.63±0.07 1.0±0.0 0.88±0.04
4 1.0±0.0 0.97±0.03 1.0±0.0 0.43±0.05 0.47±0.12 1.0±0.0 0.97±0.03 1.0±0.0 0.23±0.05 1.0±0.0 0.81±0.05
8 1.0±0.0 0.97±0.03 1.0±0.0 0.63±0.03 0.7±0.08 1.0±0.0 1.0±0.0 1.0±0.0 0.23±0.03 1.0±0.0 0.85±0.05
16 1.0±0.0 0.97±0.03 1.0±0.0 0.53±0.03 0.77±0.07 1.0±0.0 1.0±0.0 1.0±0.0 0.0±0.0 1.0±0.0 0.83±0.06
EVA+DoRA 2 1.0±0.0 1.0±0.0 1.0±0.0 0.8±0.08 0.97±0.03 1.0±0.0 1.0±0.0 1.0±0.0 0.43±0.12 1.0±0.0 0.92±0.03
4 1.0±0.0 1.0±0.0 1.0±0.0 0.8±0.05 0.93±0.03 1.0±0.0 1.0±0.0 1.0±0.0 0.63±0.03 1.0±0.0 0.94±0.02
8 1.0±0.0 1.0±0.0 1.0±0.0 0.63±0.19 0.87±0.07 1.0±0.0 1.0±0.0 1.0±0.0 0.57±0.03 1.0±0.0 0.91±0.04
16 1.0±0.0 1.0±0.0 1.0±0.0 0.67±0.2 1.0±0.0 1.0±0.0 1.0±0.0 1.0±0.0 0.5±0.16 1.0±0.0 0.92±0.04
F SVDconvergenceanalysis
F.1 BatchSizeinvariance
WeconductananalysisontheconvergenceofthecomponentsobtainedviaSVD.Specifically,we
investigatethedifferenceincomponentsaccordingtocosinesimilarityacrossdifferentbatchsizes.
Previouslywehaveseenthatthecomponentsobtainedacrossdifferentbatchorderingsareheavily
correlated. InFigure8wevisualizethecosinesimilaritiesbetweentheSVDcomponentsfordifferent
batchsizes,namely4,8,16,and32forLlama-2-7BontheMetaMathQAdataset. Weobservethat
thecomponentscorrelatestronglyandremainmostlyinvarianttothebatchsize. Thisindicatesthat
smallerbatchsizesmaybeusedforobtainingtheinitializationwhichresultsinlesscomputational
overhead. InthecaseofLlama-2-7BonMetaMathQA,thismeansthatwecanuseabatchsizeof4
sinceitinducesacomputationaloverheadofaround100seconds. Afterwardswecancontinuethe
fine-tuningprocesswithalargerbatchsize.
F.2 ExcludingignoredtokensforSVD
ForsomedatasetswenoticethatmaskingouttokensfortheSVDcomputationwhichareignoredfor
thelosscalculationduringfinetuningcanbeadvantageous. Thiscanhoweverresultinasignificant
reductionoftheeffectivebatchsizeforSVDifthenumberofcompletiontokensissmall.Anexample
wherethisisthecaseinourexperimentsarethecommonsensereasoningtaskswhichhavelong
promptsbutcompletiontokensareonlyonewordpersample. Thissettingcanleadtocaseswere
SVDdoesnotconvergeforlowerbatchsizes. Wethereforedonotmaskouttheprompttokensforthe
commonsensereasoningtasks,butapplymaskingforthemathandcodingtasks. Forthemulti-turn
codingtasks(Table10),wefoundmaskingtosignificantlyimprovetheperformance. Inthissetup
wemaskoutalltokensexceptfortheassistanttokens.
33
esolc-tecuaf remmah edis-sserp-eldnah edis-gulpnu-gep kcab-hsup
hsup
llaw-hsup ecalp-flehs llup-kcits esolc-wodniw egarevAFigure 8: Average cosine similarity between components obtained via SVD on minibatches of
activationvectorsacrossdifferentbatchsizes. Thecomponentsstronglycorrelateindicatingthatthe
SVDcomputationismostlyinvarianttothebatchsizeandreturnsmostlythesamecomponents.
G Rankre-distributionanalysis
Toilluminatetherankre-distributionprocess,wevisualizetheresultingranksforeachweightmatrix
after SVD for Llama-2-7B on the MetaMathQA dataset for different values of ρ. Setting ρ = 1
resultsinauniformrankdistributionasinstandardLoRA.However,settingρ>1altersthenumber
ofranksperweightmatrix. InFigure9wevisualizethenumberofranksassignedtoeachweight
matrixfordifferentvaluesofρ > 1andinFigure10wevisualizethecorrespondingdeltas. Both
visualizationsclearlyillustratethatthemostchangeoccursforvaluesofρ<1.5. Settingρtohigher
valuesresultsinlessandlesschange. Interestingly,someranksstillchangewhengoingfromρ=2.5
toρ = 3. Finally,weconducthyperparametersearchinwhichwesearchoverdifferentvaluesof
ρ∈{1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.5,3}. WereporttheresultsinFigure11. Wefind
thatforLlama-2-7BonMetaMathQAauniformdistributionperformsfavorably. Thesecond-best
performanceissharedbyρ=1.5andρ=2. Therefore,wealwayssearchforρ=1andρ=2for
allourremainingexperimentswhenweapplyEVAandselectthebestperformingone.
34Figure 9: The resulting rank allocation per weight matrix in each layer for Llama-2-7B on the
MetaMathQA dataset with different values of ρ. The first row represents a uniform distribution
whereeachweightmatrixreceivesthesamerankr =16. Themostchangeoccursforρ<1.5. The
re-distributionconvergesforlargervaluesofρ.
Figure10: DeltasbetweenrankdistributionsperweightmatrixineachlayerforLlama-2-7Bonthe
MetaMathQAdatasetwithdifferentvaluesofρ. Thefirstrowrepresentsauniformdistributionwhere
eachweightmatrixreceivesthesamerankr =16. Themostchangeoccursintherangeρ∈[1,1.5].
Largervaluesofρdonotinduceadditionalsignificantchangestotherankdistribution.
35Llama-2-7B on MetaMath
0.62
0.5970.6040.608 0.594 0.61 0.5970.599 0.6 0.607 0.61 0.6070.603
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 2.5 3
Rho
Figure11: Accuracyfordifferentvaluesofρwhenfine-tuningLlama-2-7BontheMetaMathQA
dataset.
36
ycaruccA