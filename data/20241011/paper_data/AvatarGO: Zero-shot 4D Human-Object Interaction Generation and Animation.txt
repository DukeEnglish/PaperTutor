AVATARGO: ZERO-SHOT 4D HUMAN-OBJECT INTER-
ACTION GENERATION AND ANIMATION
YukangCao1∗ LiangPan2†‡ KaiHan3 Kwan-YeeK.Wong3 ZiweiLiu1†
1S-Lab,NanyangTechnologicalUniversity,2ShanghaiAILaboratory,3TheUniversityofHongKong
https://yukangcao.github.io/AvatarGO/
ABSTRACT
Recentadvancementsindiffusionmodelshaveledtosignificantimprovements
inthegenerationandanimationof4Dfull-bodyhuman-objectinteractions(HOI).
Nevertheless,existingmethodsprimarilyfocusonSMPL-basedmotiongeneration,
whichislimitedbythescarcityofrealisticlarge-scaleinteractiondata. Thiscon-
straintaffectstheirabilitytocreateeverydayHOIscenes. Thispaperaddressesthis
challengeusingazero-shotapproachwithapre-traineddiffusionmodel. Despite
thispotential,achievingourgoalsisdifficultduetothediffusionmodel’slackof
understandingof“where”and“how”objectsinteractwiththehumanbody. To
tackletheseissues,weintroduceAvatarGO,anovelframeworkdesignedtogener-
ateanimatable4DHOIscenesdirectlyfromtextualinputs. Specifically,1)forthe
“where”challenge,weproposeLLM-guidedcontactretargeting,whichemploys
Lang-SAMtoidentifythecontactbodypartfromtextprompts,ensuringprecise
representationofhuman-objectspatialrelations. 2)Forthe“how”challenge,we
introducecorrespondence-awaremotionoptimizationthatconstructsmotion
fieldsforbothhumanandobjectmodelsusingthelinearblendskinningfunction
from SMPL-X. Our framework not only generates coherent compositional mo-
tions,butalsoexhibitsgreaterrobustnessinhandlingpenetrationissues. Extensive
experimentswithexistingmethodsvalidateAvatarGO’ssuperiorgenerationand
animationcapabilitiesonavarietyofhuman-objectpairsanddiverseposes. Asthe
firstattempttosynthesize4Davatarswithobjectinteractions,wehopeAvatarGO
couldopennewdoorsforhuman-centric4Dcontentcreation.
1 INTRODUCTION
Thecreationof4Dhuman-objectinteraction(HOI)holdsimmensesignificanceacrossawiderange
ofindustries,includingaugmented/virtualreality(AR/VR)andgamedevelopment,asitformsthe
foundationofthe4Dvirtualworld. Traditionally,developingsuchmodelshasrequiredextensive
humaneffortandspecializedengineeringexpertise. Fortunately,thankstothecollectionsofHOI
datasets(Lietal.,2023b;Bhatnagaretal.,2022;Jiangetal.,2023a)andtherecentadvancementsin
diffusionmodels(Sahariaetal.,2022;Rameshetal.,2022;Balajietal.,2022;Stability.AI,2022;
2023),existingHOIgenerativetechniques(Zhangetal.,2022;2023;2024;Shafiretal.,2023;Kapon
et al., 2024; Chen et al., 2024a) have exhibited promising capabilities by generating 4D human
motionswithobjectinteractionsfromtextualinputs. Nonetheless,thesemethodsprimarilyfocus
onSMPL-based(Loperetal.,2015;Pavlakosetal.,2019)motiongeneration,whichstrugglesto
capturetherealisticappearanceofsubjectsencounteredineverydaylife. AlthoughInterDreamer(Xu
etal.,2024b)hasrecentlyproposedtogeneratetext-aligned4DHOIsequencesinazero-shotmanner,
their output is still largely constrained by the SMPL model. This highlights a pressing need for
morerealisticandgeneralizablemethodstailoredspecificallytomodel4Dhuman-objectinteractive
content. Wetaketheinitiativeandshowcasethepotentialofaddressingthischallengebyleveraging
the3Dgenerativemethodsinazero-shotmanner.
In recent times, 3D generative methods (Poole et al., 2022; Tang et al., 2023; Liu et al., 2023c;
Lin et al., 2023; Wang et al., 2023d; Cao et al., 2023b; Liao et al., 2023) and Large Language
∗PartoftheworkhasbeendonewheninterningatShanghaiAILaboratory. †Correspondingauthors
‡Projectlead
1
4202
tcO
9
]VC.sc[
1v46170.0142:viXra8 11 13 16 29
Bodybuildercarryingadumbbelinhis<hand>
Goku in Dragon Ball Seriesgraspingatorchinhis<hand>
Iron Man holdingan axe of Thor inhis<hand>
GokuinDragon Narutoin
AlbertEinstein Joker– HarryPotter– StevenPaul
BallSeries– NarutoSeries–
–box* microphone* bearheadhat* Jobs–iPhone*
AK-47* football*
Figure 1: Examples of 4D animation results obtained via AvatarGO. AvatarGO effectively
producesdiversehuman-objectcompositionswithcorrectspatialcorrelationsandcontactareas. It
AlbertEinsteinholding
achievesajbooxiinnhits<haanndi>mationofhumansandobjectswhileavoidingpenetrationissues.
Models (LLMs) (Wu et al., 2023a) have garnered increasing interest. These progressives have
ledtothedevelopmentoftext-guided3Dcompositionalgenerationtechniquesthatarecapableof
AlbertEinsteinholding GokuinDragonBallSeries Jokerholdingaboxin WolfgangAmadeus
compreahboexinnhdis<ihnangd>intrichaoldtiengarneAKl-a47tiinohins<hsanad>ndcreahtisi<nhangd>complMeoxzart3hoDldingsacboettnleesincorporatingmultiplesubjects.
inhis<hand>
Notably,GraphDreamer(Gaoetal.,2023)utilizesLLMstoconstructagraphwherenodesrepresent
objectsandedgesdenotetheirrelations. ComboVerse(ChenheoldS tit ne gave an ln.P ia ,Pu hl o2J no e0b ins 2his4b)prWoonpad deor umwsbom beea lsn inh hosl id spinagtial-aware
<hand> <hand>
scoredistillationsamplingtoamplifythespatialcorrelation. Subsequentstudies(Epsteinetal.,2024;
Zhouetal.,2024)furtherexplorethepotentialofjointlyoptimizinglayoutstocompositedifferent
components.
Despitethepromisingperformancedemonstratedbyexistingmethods,theyencountertwomajor
challengesingenerating4DHOIscenes: 1)Incorrectcontactarea: WhileLLMsexcelatcapturing
the relationships, optimization with diffusion models faces difficulties in accurately defining the
contact area between various objects, particularly those with complex articulated structures like
humanbodies. AlthougheffortslikeInterFusion(Daietal.,2024)haveconstructed2Dhuman-object
interactiondatasetstoretrievehumanposesfromtextprompts,theystillencounterchallengesin
definingtheoptimalcontactbodypartsforcasesoutsidethetrainingdistribution. 2)Limitationsin
4Dcompositionalanimation: WhileexistingtechniqueslikeDreamGaussian4D(Renetal.,2023)
andTC4D(Bahmanietal.,2024)employvideodiffusionmodels(Blattmannetal.,2023;Guoetal.,
2023a)toanimate3Dstaticscenes,theyoftentreattheentiresceneasonesubjectduringoptimization,
leadingtounrealisticanimationresults. DespiteinitiativeslikeComp4D(Xuetal.,2024a),which
utilizetrajectoriestoanimate3Dobjectsindividually,modelingcontactbetweenvarioussubjects
remainsachallenge.
Inthispaper,weproposeAvatarGO,anovelframeworkforcompositional4Davatargenerationwith
objectinteractions. Bytakingthetextpromptsasinputs,weassumethatthe3Dhumanandobject
modelsaswellasthehumanmotionsequencescanbeindividuallygeneratedbyadoptingexisting
2generativetechniques(Tangetal.,2023;Liuetal.,2023d;Zhangetal.,2023;2024). Specifically,we
adoptDreamGaussian4D(Renetal.,2023)asourbaselineconsideringitssuperiortrainingefficiency
andfocusonaddressingthechallengesassociatedwithhuman-objectinteractions. Toachievethis
objective,AvatarGOintegratestwokeyinnovationstolearn“where”and“how”theobjectshould
interactwiththehumanbody: 1)LLM-guidedcontactretargeting. Giventhelimitedavailability
ofhuman-objectinteractionimagesinthe2Ddatasetusedfordiffusionmodeltraining,it’sdifficult
to identify the most appropriate contact area between humans and objects. To tackle this issue,
weproposeleveragingLang-SAM(lan,2023)toidentifythecontactbodypartfromtextprompts,
whichservesastheinitializationfortheoptimizationprocedure. 2)Correspondence-awaremotion
optimization. Buildingupontheobservationthatpenetrationisabsentinstaticcompositedmodels,
weintroducecorrespondence-awaremotionoptimizationthatleveragesSMPL-Xasanintermediary
tomaintainthecorrespondencebetweenhumansandobjectswhentheyareanimatedtoanewpose,
thusdemonstratinggreaterrobustnessinhandlingpenetrationissues.
WethoroughlyassessAvatarGObycompositingdiversepairsof3Dhumansandobjectsandanimating
themacrossvariousmotionsequences(seeFig.1). Ourexperimentalresultsshowthatourmethod
excelsatidentifyingoptimalcontactareasandexhibitsgreaterrobustnessinhandlingpenetration
issuesduringanimation,significantlyoutperformingexistingtechniques. Wewillmakeourcode
publiclyavailable.
2 RELATED WORK
3DContentGeneration. Leveragingadvancesindiffusion-basedtext-to-2Dimagegeneration(Sa-
hariaetal.,2022;Rameshetal.,2022;Balajietal.,2022;Stability.AI,2022;2023),DreamFusion
introducedScoreDistillationSampling(SDS)togenerate3Dcontentviapre-trainedmodels,uti-
lizingtechnologieslikeNeRF(Mildenhalletal.,2020),DMTET(Shenetal.,2021),3DGaussian
Splatting(Kerbletal.,2023)). Subsequentresearchhasfocusedonenhancingoutputquality(Lin
etal.,2023;Chenetal.,2023b;Wangetal.,2023d),controllinggenerationprocesses(Metzeretal.,
2022;Seoetal.,2023), improvingtrainingefficiency(Wangetal.,2023a;Wuetal.,2024;Tang
etal.,2023),andextendingcapabilitieson3Dtexturing(Richardsonetal.,2023;Caoetal.,2023a;
Chenetal.,2023a;Tangetal.,2024b). Addressing3Dhumanbodycomplexity,recentstudies(Cao
etal.,2023b;c;Liaoetal.,2023;Jiangetal.,2023b;Huangetal.,2023b;Kolotourosetal.,2023;
Zeng et al., 2023; Huang et al., 2023a) have been proposed for creating controllable 3D human
avatars,althoughthesestillrequiresignificantinput-specifictrainingtime. Theproliferationoflarge
3Ddatasets(Deitkeetal.,2023;2024;Wuetal.,2023b)haspropelled3Dgenerationtechniques
forward. Notably,Zero-1-to-3(Liuetal.,2023c),Zero123++(Shietal.,2023a),andMVDream(Shi
etal.,2023b)use2Ddiffusionmodelstogenerateconsistentmulti-viewimages,servingasinputs
for efficient 3D model generation tools like SyncDreamer (Liu et al., 2023e), Wonder3D (Long
etal.,2023),One-2-3-45(Liuetal.,2023b;a),UniDream(Liuetal.,2023f),MVDiffusion++(Tang
etal.,2024c),andMake-Your-3D(Liuetal.,2024). Additionally,buildingontransformer(Vaswani
et al., 2017) and image processor advancements (e.g., DINO (Caron et al., 2021; Oquab et al.,
2023)),LargeReconstructionModels(Hongetal.,2023;Wangetal.,2023b;Xuetal.,2023;Li
etal.,2023a)implementtransformer-basedarchitecturestoderive3Dtri-planetokensfromimage
features. 3DTopia(Hongetal.,2024)useshybriddiffusionpriorstoproducehigh-fidelity3Dobjects.
Meanwhile,methodslikeLGM(Tangetal.,2024a),CRM(Wangetal.,2024),andGRM(Yinghao
etal.,2024)explorevarious3Drepresentationsforimprovedperformance,suchas3DGaussian
Splatting(Kerbletal.,2023)andFlexiCube(Shenetal.,2023). Despitetheseadvances,challenges
remainingeneratingcomplexcompositional3Dscenes.
3DCompositionalGeneration. Toaddressthecompositionalnatureof3Dcontent,afewefforts
have been made recently. Epstein et al (Epstein et al., 2024) and GALA3D (Zhou et al., 2024)
propose optimizing component layouts for integrated object scenes. ComboVerse (Chen et al.,
2024b)introducesspatial-awarescoredistillationsampling(SSDS)toeffectivelylearnobjectspatial
relations. GraphDreamer(Gaoetal.,2023)useslargelanguagemodelstoformgraphstructures
wherenodesandedgesrepresentobjectsandtheirrelationships,respectively,showingpromising
results. Challengesremaininmodelinginteractionsbetweenhumansandobjects. InterFusion(Dai
etal.,2024)developsa2Ddatasetforhuman-objectinteractions,enablingtext-guidedposeretrieval
andscenegeneration. However,thisapproachlacksprecisecontroloverinteractionareasandisnot
readilyadaptableto4Dscenarios.
3(I) Text-driven3Dhumanandobjectcomposition 4DAvatarGenerationwith
Trainableparameters ObjectInteractions
LLM-guidedcontactretargeting Spatial-awareSDS
StableDiffusion
Co rm ep ndos ei rt ii no gnal ℒ"∗
#"
Scale(
Rotationℛ
Translation' *c
SMPL-Xparameters Bodybuilderholdingadumbbelinhis<hand>
(II) Correspondence-aware(CA)motionfield
StableDiffusion
Humananimation
x!
)
)
+
,*
)
,
+
)*
)
*
+
+* x"
ℒ"∗
#"
+ * ,
LinearBlendSkinning Non-rigid motion Co rm ep ndos ei rt ii no gnal C mor or te is onpon opd te imnc ie z- aa tw ioa nre ℒ%&
Objectanimation
x! Av Le Bra Sge TrR ao nt sa lt aio tin onℛ
!
x" ℒ"#"
Texture-Structure
Trainableparameters jointdiffusion
Figure2: OverviewofAvatarGO.AvatarGOtakesthetextpromptsasinputtogenerate4Davatars
with object interactions. At the core of our network are: 1) Text-driven 3D human and object
compositionthatemployslargelanguagemodelstoretargetthecontactareasfromtextsandspatial-
awareSDStocompositethe3Dmodels. 2)Correspondence-awaremotionoptimizationwhichjointly
optimizestheanimationforhumansandobjects. Iteffectivelymaintainsthespatialcorrespondence
duringanimation,addressingthepenetrationissues.
4DContentGeneration. Recentadvancesinvideodiffusionmodelsandscoredistillationsampling
have spurred a variety of 4D scene generation techniques. Make-A-Video3D (MAV3D) (Singer
etal.,2023)utilizesHexPlanefeaturesfor4Drepresentations. 4D-fy(Bahmanietal.,2023)and
DreamGaussian4D(Renetal.,2023)employmulti-stageoptimizationpipelinestotransformstatic
3Dintodynamic4Dscenes. Dream-in-4D(Zhengetal.,2023)allowsforpersonalized4Dgeneration
usingimageguidance,whileConsistent4D(Jiangetal.,2023c)usesvideoinputswithRIFE(Huang
et al., 2022) and a super-resolution module for scene creation. 4DGen (Yin et al., 2023) and
AnimatableDreamer(Wangetal.,2023c)focusoncontrollablemotiongenerationviadrivingvideos.
More recently, Comp4D (Xu et al., 2024a) and TC4D (Bahmani et al., 2024) have introduced
trajectory-basedapproachesforcreating4Dcompositionalscenes. Whilethesetechnologiesshow
promise,theyoftenstruggletoproduce4Davatarsthateffectivelyinteractwithobjects. Although
GAvatar(Yuanetal.,2023)excelsin4Dhumananimation, itsobjectinteractioncapabilitiesare
limited.
3 METHODOLOGY
Givenagenerated3Davatarandaspecific3Dobject,AvatarGOgeneratescompositional4Davatars
withobjectinteractionsbasedontextinstructions. Inthesubsequentsections, wefirstintroduce
thepreliminaries(inSec.3.1),includingstatic3Dcontentgenerationandparametrichumanmodel
SMPL-X.Next,wewilldescribethekeycomponentsofAvatarGO,including(1)text-driven3D
humanandobjectcomposition(inSec.3.2),and(2)correspondence-awaremotionoptimizationfor
achievingsynchronizedhumanandobjectanimation(inSec.3.3). TheoverviewofAvatarGOis
showninFig.2.
3.1 PRELIMINARIES
3DModelGeneration. Recently,DreamGaussian(Tangetal.,2023)showcasespromisingresults
withlargelyimprovedtrainingefficiencybyincorporatingtwomajorcomponents:
4
MAS-gnaL noitazilaitini(1)3DGaussianSplatting(3DGS).3DGS(Kerbletal.,2023)directlydefinesthe3Dspacethrough
asetofGaussiansparameterizedbytheir3Dpositionµ,opacityα,anisotropiccovarianceΣ,and
sphericalharmoniccoefficientssh. Theshtermisusedtocapturetheview-dependentappearanceof
thesceneandΣcanbedecomposedto:
Σ=RSSTRT, (1)
whereRistherotationmatrixexpressedbyaquaternionq ∈ SO(3),andS isthescalingmatrix,
representedbya3Dvectors. Essentially,eachGaussiancenteredatpoint(mean)µisdefinedas:
G(x,µ)=e− 21(x−µ)TΣ−1(x−µ), (2)
wherexisthe3Dquerypoint.
Forrenderingthe3DGaussiansontothe2Dimagespace,3DGSincorporatesatile-basedrasterizer
andpoint-basedα-blendrendering. Specifically,thecolorC(u)ofapixelucanbecalculatedas:
i−1
(cid:88) (cid:89)
C(u)= T c α SH(sh ,v), T =G(x,µ ) (1−α G(x,µ )), (3)
i i i i i i j j
i∈N j=1
whereT representsthetransmittance,SHdenotesthesphericalharmonicfunction,andvindicatesthe
viewingdirection.ByoptimizingtheGaussianattributes{G:µ,q,s,σ,c}anddynamicallyadjusting
thedensityof3DGaussians(i.e.,densifyingandpruning),DreamGaussianachieveshigh-quality
generationsfromeithertextualorvisualinputs.
(2) Score Distillation Sampling (SDS). Starting with the latent feature z extracted from a 3DGS
renderingx,SDSintroducesrandomnoiseϵtoz,yieldinganoisylatentvariablez . Thisvariableis
t
thenprocessedbyapre-traineddenoisingfunctionϵ (z ;y,t)toestimatetheaddednoise. TheSDS
ϕ t
lossthencalculatesthedifferencebetweenpredictedandaddednoise,withitsgradientcalculatedby:
(cid:20) (cid:21)
∂z ∂x
∇ L (ϕ,g(θ))=E w(t)(ϵ (z ;y,t)−ϵ) , (4)
θ SDS t,ϵ∼N(0,1) ϕ t ∂x ∂θ
whereydenotesthetextembedding,w(t)weightsthelossfromnoiselevelt. Wedonotapplythe
meshextractionandtextureoptimizationproposedinDreamGaussiantoobtainthe3Dmodels.
SMPL-X(Loperetal.,2015;Pavlakosetal.,2019). Withposeparameterθ,shapeparameterβ,
andexpressionparameterϕasinputs,SMPL-Xmapsthecanonicalmodeltotheobservationspace:
M(β,θ,ϕ)=LBS(T(β,θ,ϕ),J(β),θ,W), (5a)
T(β,θ,ϕ)=T+B (β)+B (ϕ)+B (θ), (5b)
s e p
where M denotes the function defining the mesh model of a human body, and T represents the
transformedvertices. W standsforblendweights,B ,B ,andB arefunctionsrespectivelyfor
s e p
shape,expression,andposeblendshapes. LBS(·)indicatesthelinearblendskinningfunctionthat
poseseachbodyvertexofSMPL-Xaccordingto:
K
(cid:88)
v =G·v , G = w G (θ,j ), (6)
o c k k k
k=1
where v and v represent SMPL-X vertices under the canonical pose and observation space,
c o
respectively. w istheskinningweight,G (θ,j )istheaffinedeformationthatmapsthek-thjoint
k k k
j fromthecanonicalspacetoobservationspace,andK denotesthenumberofneighboringjoints.
k
3.2 TEXT-DRIVEN3DHUMANANDOBJECTCOMPOSITION
WiththehelpofDreamGaussian(Tangetal.,2023),weefficientlygeneratethe3DavatarG andthe
h
3DobjectG individuallybasedon3DGSandSDS(discussedinSec.3.1). Wenoticedthateven
o
withmanualadjustments,suchasrescalingandrotatingthe3Dobjects,it’sdifficulttodirectlyrigthe
generated3Dhumanandobjectmodelsaccurately(seeAppx.F).Therefore, westrivetoseamlessly
compositeG andG basedonthetextpromptinthisstage. Specifically,theGaussianattributesof
h o
G andG wouldbeoptimized,aswellasthreetrainableglobalparametersofG ,includingrotation
h o o
R∈R4,scalingfactorS ∈R,andthetranslationmatrixT ∈R3:
X :=S·(X ·R+T), (7)
Go Go
5whereX isthesetofstaticGaussianpoints.
Go
However,solelyutilizingSDSforoptimizationcouldfrequentlyleadtodisproportionaterelationships
anderroneouscontactareas(seeFig.3). Thisissuecanbeattributedtotwopotentialfactors: (1)the
absenceofemphasisonwordsdescribinghuman-objectinteraction,whichdecreasesthemodel’s
abilitytocomprehendtherelationshipsbetweenhumansandobjects;(2)thecomplexityinherentin
humansubjects,posingchallengesforthediffusionmodeltoidentifythemostsuitablecontactareas
(seeSec.4.3).
Spatial-aware SDS (SSDS). Following ComboVerse (Chen et al., 2024b), we employ SSDS to
facilitatethecompositional3Dgenerationbetweenthehumanandtheobject. Specifically,SSDS
augments the SDS with a spatial relationship between the human and the object by scaling the
attentionmapsofthedesignatedtokens<token∗>withaconstantfactorc(wherec>1):
(cid:26) c·ATT , if <token>=<token∗>,
ATT:= <token> (8)
ATT , otherwise.
<token>
Here,<token∗>correspondstothetokensencodingthehuman-objectinteractionterm,suchas
<‘holding’>,whichcanbeidentifiedthroughLargeLanguageModels(LLMs)orspecifiedby
theuser. Consequently,thespatial-awareSDSlosscanbewrittenas:
(cid:20) (cid:21)
∂z ∂x
∇ L (ϕ∗,g(θ))=E w(t)(ϵ (z ;y,t)−ϵ) , (9)
θ SSDS t,ϵ∼N(0,1) ϕ∗ t ∂x ∂θ
whereϕ∗denotesthepre-traineddenoisingfunctionwiththeadjustedattentionmaps.
LLM-guidedContactRetargeting. Whilespatial-awareSDScouldbenefitinunderstandingspatial
correlations,itstillfacesdifficultiesinidentifyingthemostappropriatecontactarea(SeeFig.3),
whichservesasakeycomponentforhuman-objectinteraction. Accordingtoourstudies(seeAppx.E
forvisualization),thediffusionmodelstrugglestoaccuratelyestimatecontacts,eveninthe2Dimages
generatedforhuman-objectinteraction. Totacklethisissue,weproposeleveragingLang-SAM(lan,
2023)toidentifythecontactareafromtextprompts. Specifically,startingfromthe3Dhumanmodel
G ,werenderitfromafrontalviewpointtoproducetheimageI. Thisimage,alongsidetextual
h
inputs,undergoesLang-SAMmodeltoderive2DsegmentationmasksM:
LangSAM(I,<body-part>)→M, (10)
where <body-part> represents the text describing the human body part, such as <‘hand’>.
Subsequently,weback-projectthe2Dsegmentationlabelsontothe3DGaussiansviainverserender-
ing(Chenetal.,2023c). Specifically,foreachpixeluonthesegmentationmaps,weupdatethemask
value(0or1)backtotheGaussiansvia:
(cid:88)
w = o (u)×T (u)×M(u), (11)
i i i
i∈N
wherew representstheweightofthei-thGaussian,N isthecollectionofGaussiansthatcanbe
i
projectedontothepixelu. o(·),T(·),andM(·)respectivelydenotetheopacity,transmittance,and
segmentationmaskvalue. Followingtheweightupdates,weassesswhetheraGaussiancorresponds
to the segmented region of the human body part by comparing its weight against a pre-defined
thresholda. WetheninitializethetranslationparameterT accordingto:
(cid:88)
T =(wT ∗µ)/ w, (12)
wherew ={w ,...,w |w =0/1}∈RN×1,µ={µ ,...,µ }∈RN×3,andN isthenumberof
1 N i 1 N
GaussainpointswithinthehumanmodelG .
h
3.3 CORRESPONDENCE-AWAREMOTIONFIELD
Followingthecompositionalintegrationof3Dhumansandobjects,animatingthemsynchronously
presentsanadditionalchallengeowingtopotentialpenetrationissues. Thisproblemstemsfromthe
absenceofawell-definedmotionfieldfortheobject. Tothisend,weestablishthemotionfieldsfor
bothhumanandobjectmodelsusingthelinearblendskinningfunctionfromSMPL-X(asinEq.6),
6Jokerholdingamicrophoneinhishand
GokuinDragonBallSeriesholdingatorchinhishand
KratosinGodofWarholdinganaxeofThorinhishand
HumanGaussian GraphDreamer Ours (Var-A) Ours
Figure3: Comparisonson3Dcompositionalgenerations.
andproposeacorrespondence-awaremotionoptimizationaimedatoptimizingthetrainableglobal
parametersoftheobjectmodel,i.e.,rotation(R)andtranslation(T),toimproverobustnessagainst
penetrationissuesbetweenhumansandobjects.
Human Animation. Given the motion sequence, we first construct a deformation field, which
consistsoftwocomponents: (1)articulateddeformationutilizingtheSMPL-Xlinearblendskinning
functionLBS(·),and(2)non-rigidmotionlearningtheoffsetbasedonHexPlanefeatures(Cao&
Johnson,2023),todeformthepointx fromthecanonicalspacetox intheobservationspace:
c o
x =G·x +MLP(F(x ,t)), (13)
o c c
whereF(·)denotestheHexPlane-basedfeatureextractionnetwork,andtindicatesthetimestamp.
WederiveG fromtheclosetcanonicalSMPL-Xvertextox .
c
ObjectAnimation. Similartothehumananimation,wecalculatethedeformationmatrixG foreach
c
GaussianpointxwithintheobjectmodelG basedonitsclosestcanonicalSMPL-Xvertex. Given
o
ourexperimentaldefinitionof3Dobjectsasrigidbodies,wethencomputetheiraveragetoestablish
theintermediatemotionfieldfortheobject:
(cid:80)
G
X
=G′
·X ,
G′
=
i∈[1,M] ci,
(14)
o c c c M
whereX = {x ,...,x },X = {x ,...,x },andM isthetotalnumberofGaussianpoints
o o1 oM c c1 cM
withinG . AlthoughanimatingtheobjectdirectlyusingSMPL-Xlinearblendskinningmayseem
o
likeasimplesolution,itcanresultinpenetrationissuesbetweenthehumanandtheobject(seeFig.6).
Thischallengearisesprimarilyfromtheabsenceofproperconstraintstomaintainthecorrespondence
betweenthesetwomodels.
Correspondence-aware Motion Optimization. Drawing insight from the fact that our method
isrobustinhandlingpenetrationissuesinstaticcompositedmodelsacrossvariousscenarios,we
proposeacorrespondence-awaremotionoptimizationtopreservethecorrespondencebetweenhuman
andobject,therebyaddressingthepenetrationproblem. Specifically,weextendtheabovemotion
field(Eq.14)toincludetwoadditionaltrainableparametersRandT:
X :=X ·R+T. (15)
o o
whereX isobtainedinEq.14.RatherthannaïvelyoptimizingtheparametersviaSDS,weproposea
o
novelcorrespondence-awaretrainingobjectivethatleveragesSMPL-Xasanintermediarytomaintain
thecorrespondencebetweenhumanandobjectwhentheyareanimatedtoanewpose:
L =MSE(G ,G ), G ={G ,...,G }, G ={G ,...,G } (16)
CA c o c c0 cM o o0 oM
7Bodybuilderholdingadumbbelinhishand
GokuinDragonBallSeriesholdingatorchinhishand
Steven Paul Jobsholdingan iPhone inhishand
DreamGaussian4D HumanGaussian* TC4D Ours(Var-B) Ours
Figure 4: Comparisons on 4D avatar animation with object interactions. ‘∗’ indicates that
HumanGaussiandirectlyemploystheSMPLLBSfunctionforanimation.
whereG andG isrespectivelyderivedbasedonx ,x andtheircorrespondingSMPL-Xmodels.
ci oi ci oi
Inadditiontoourcorrespondence-awareloss,wealsoincorporatethespatial-awareSDSasinEq.9
andthetexture-structurejointSDSfromHumanGaussian(Liuetal.,2023d)toenhancetheoverall
quality:
(cid:20) (cid:19)
∂z ∂x
∇ L (ϕ,g(θ))=λ ·E w(t)(ϵ (z ;y,t)−ϵ ) x
θ SDS Steven1 Pault ,Jϵo∼bNsh(0o,l1d)ingan iPhoϕne inxthishand x ∂x ∂θ
(cid:20) (cid:21)
∂z ∂d
+λ ·E w(t)(ϵ (z ;y,t)−ϵ ) d , (17)
2 t,ϵ∼N(0,1) ϕ dt d ∂d ∂θ
whereλ andλ arehyper-parameterstobalancetheimpactofstructuralandtexturallosses,whiled
1 2
denotesthedepthrenderings.
Iron Man holdingan axe of Thor inhishand
Theoveralllossfunctiontooptimizethe4Danimativesceneisthengivenby:
L=λ ·L +λ ·L +λ ·L , (18)
CA CA SDS SDS SSDS SSDS
whereλ ,λ ,andλ representsweightstobalancetherespectivelosses.
CA SDS SSDS
Hulk holdinga golden cudgel inhishand
DreamGaussian4D HumanGaussian* TC4D Ours(Var-B) Ours
4 EXPERIMENTS
We now validate the effectiveness and capability of our proposed framework to animate various
3D avatar-object pairs with different poses and provide comparisons with existing 3D and 4D
compositionalgenerationmethods.
Implementation Details. We follow DreamGaussian4D (Ren et al., 2023) to implement the 3D
Gaussian Splatting (Kerbl et al., 2023) and the HexPlane (Cao & Johnson, 2023) in our method.
Weutilizethepre-trainedTexture-StructurejointdiffusionmodelfromHumanGaussian(Liuetal.,
2023d)andversion2.1ofStableDiffusion(Stability.AI,2022)torespectivelycalculatetheSDS
andspatial-awareSDSinourimplementation. Typically,foreach3Davatar-objectpair,wetrainthe
3Dstagewithabatchsizeof16for400epochs,andthe4Dstagewithabatchsizeof10for400
epochs. Thetrainingtakesaround10minutesforthe3Dstageand20minutesforthe4Dstageona
singleNVIDIAA100GPU.WeuseAdam(Kingma&Ba,2015)optimizerforback-propagation.
AdditionalimplementationdetailscanbefoundintheAppx.B.
Comparison Methods for 3D Static Generation. We first compare the 3D static generation
results with HumanGaussian (Liu et al., 2023d) and GraphDreamer (Gao et al., 2023). Since
8ComboVerse (Chen et al., 2024b) lacks an official code release and relies on image inputs, we
comparestaticAvatarGOwithanalternativevariant,i.e.,“Ours(Var-A)”,byonlyusingthespatial-
awarescoredistillationsampling(SSDS)inComboVersetocomposite3Dhumansandavatars. We
cannotcomparewithGALA3Dastheirsourcecodeisnotpubliclyaccessible.
Comparison Methods for 4D Animation. Since there are no specific methods tailored for 4D
avataranimationwithobjectinteractions,weaccessAvatarGO’sefficacyagainstthreerecent4D
generationtechniques(i.e.,DreamGaussian4D(Renetal.,2023),HumanGaussian(Liuetal.,2023d),
andTC4D(Bahmanietal.,2024)),aswellasonealternativevariant“Ours(Var-B)”. Toimplement
Var-B, we utilize human hand motion sequences as trajectories to guide the transformation of
3DobjectsandfollowComp4DtointegratethevideodiffusionmodeltocomputeSDS.Because
InterDreamer(Xuetal.,2024b)andInterFusion(Daietal.,2024)havenotreleasedtheircode,we
couldnotincludetheirresultsforcomparison. Seemoremotivationfordesigning“Ours(Var-A)”
and“Ours(Var-B)”inAppx.C.
4.1 QUALITATIVEEVALUATIONS
4D Avatar Generation with Object Interaction. In Fig. 1, we present a diverse collection of
avatar-object pairs that are animated to different poses. These renderings consistently showcase
high-fidelityresultsfromvariousviewpoints.ThankstoourproposedLLM-guidedcontactretargeting
andcorrespondence-awaremotionoptimization,ourmethodcandeliverappropriatehuman-object
interactionsanddemonstratesuperiorrobustnesstothepenetrationissues.
Comparisonon3DGeneration. Weprovidequalitativecomparisonswithexistingmethodson3D
generationinFig.3. Wecanobserve: 1)withouttheaidofLLMs, HumanGaussianstrugglesto
determinethespatialcorrelationsbetweenhumansandobjects;2)Despiteusinggraphstoestablish
relationships, GraphDreamer is confused by the meaningful contact, resulting in unsatisfactory
outcomes. 3)OptimizingR,S,andT withonlySSDSisinadequatetomovetheobjecttothecorrect
area. Conversely,AvatarGOconsistentlyoutperformswithprecisehuman-objectinteractions.
Comparisons on 4D Animation. In Fig. 4, we compare our 4D animation results with SOTA
methods. Wetaketherenderingfromour3DcompositionsstageastheinputforDreamGaussian4D.
Thefollowingobservationscanbemade: 1)Evenwithhuman-objectinteractionimages,Dream-
Gaussian4D,whichemploysvideodiffusionmodels,struggleswithanimatingthecompositedscene.
2)DirectanimationviaSMPLLBSfunction,asinHumanGaussian,tendstoyieldunsmoothresults,
especiallyforthearms. 3)TC4DfacessimilarissuesastheDreamGaussian4D.Meanwhile,ittreats
theentiresceneasasingleentity,lackingbothlocalandlarge-scalemotionsforindividualobjects. 4)
Onemaythinkapplyingtrajectorytoobjectsseemslikeasimplesolution(asinComp4D).However,
asseenin“Ours(Var-B)”,itcandisruptspatialcorrelationsbetweenhumansandobjects. These
points further validate the necessity of AvatarGO. Our method can consistently deliver superior
resultswithcorrectrelationshipsandbetterrobustnesstopenetrationissuesSeetheAppx.A,H,J,K
formorecomparisons.
4.2 QUANTITATIVEEVALUATIONS
CLIP-basedMetrics. WeuseCLIP-basedmet- Table1: QuantitativeEvaluation.
rics(CLIP-Score(CLIP-S),CLIPimagesimilar-
ity(CLIP-Image),andCLIPDirectionalSimi- GraphDreamer TC4D HumanGaussian (VO au rr -s A) (VO au rr -s B) (stO au tr is c) Ours
l 2a 0r 2it 2y )( )C wL iI tP h-D CS L) I( PB -r Vo io Tk -s Le /1t 4al. m,2 o0 d2 e3 l; .G Aal met oa nl g., C CCL LLI IIP PP- --I DSm S↑a ↑ge↑ 9 8 18 . .0 7.4 9 14 1 18 9 59 . .. 8 25 4 80 28 433 .. 7. 69 193 29 057 .. 9. 38 168 3 29 0 52 . .. 5 91 7 01 39 3(cid:58)23 3(cid:58).. .(cid:58)24 8(cid:58)75
0
39 2(cid:58) (cid:58)22 8(cid:58) (cid:58).. .(cid:58) (cid:58)82 0(cid:58) (cid:58)40
3
them,CLIP-Smeasuresthesimilaritybetweentextsandtheircorrespondingmodels,CLIP-Image
denotesthesimilaritybetweencompositionalmodelsandhumanmodels,andCLIP-DSrepresents
thealignmentbetweenchangesintextcaptions(e.g.,“IronMan”to“IronManholdinganaxeofThor
inhishand”)andcorrespondingchangesinimages. ThroughTab.1,ourmethodmaintainthehuman
identityinthecompositedscenes(seeCLIP-Image). Notethat“Ours(Var-A)”andGraphDreamer
isslightlybetterforthismetricastheystruggletodothecomposition(seeFig.3). Meanwhile,“Ours”
and“Ours(static)”consistentlyachievebetterresultsthanHumanGaussianandothervariants,further
affirmingtheobjectivesuperiorityofAvatarGO.
UserStudiesWefurtherconductuserstudiestocomparewithDreamGaussian4D,HumanGaussian,
TC4D,and“Ours(Var-A)”.24Volunteersratedthesemethodsindependentlybasedonsevencriteria
9w/SDFdistanceloss w/SDFlabelloss Ours w/o ℛ, #, ℒ w/o ℒ Ours
!" !"
IronManholdinganaxeinhishand
Figure6: Analysisofcorrespondence-awaremotionfield.
from1(worst)to5(best):(1)Levelofpenetration;(2)Accuracyoftherelativescalebetweenhumans
andobjects;(3)Accuracyofcontact;(4)Motionquality;(5)Motionamount;(6)Textalignment;(7)
Overallperformance. DetailedresultshavebeenpresentedinTab.5. Keyobservationsinclude: 1)
BothDreamGaussian4DandHumanGaussianhavedifficultyprovidingsatisfactoryoutcomesfor
human-objectinteraction(HOI)scenes. 2)AlthoughTC4DperformswellwithHOIgenerations,it
onlyproducesglobalmotions,leadingtolessoptimalmotionqualityandquantitycomparedtoour
method. Ourfinaldesignconsistentlydeliverssuperiorresultsforallsevencriteria,outperforming
theothermethodsacrosstheboard.
4.3 ABLATIONSTUDIES
AnalysisofLLM-guidedContactRetargeting. Wefirstconductevaluationstovalidatetheefficacy
ofemployingLang-SAMforretargetingtheaccuratecontactarea. SeeFig.3. Bycomparing“Ours
(Var-A)”andOurs,wecanconcludethatwithoutLang-SAM,themodelstrugglestoproducecorrect
human-objectinteractioninthe3Dcompositionalgeneration.
Figure5: Userstudies.
Analysis of Correspondence-aware Motion
Field. InFig.6,wefirstcompareourproposed GaD usr se ia am n4D GH au um ssa ian n TC4D (VO au rr -s B) Ours
training objectives L with two alternative Levelofpenetration↑ 1.267 1.084 4.236 1.537 4.872
CA Accuracyofrelativescale↑ 1.183 1.092 4.308 3.947 4.788
strategies: 1)“SDFdistanceloss”whichmini- Accuracyofcontact↑ 1.654 1.137 4.412 2.137 4.802
Motionquality↑ 1.321 2.156 1.947 1.673 4.592
mizesthechangeofsigneddistancefield(SDF) Motionamount↑ 2.118 3.781 1.517 4.159 4.934
Textalignment↑ 2.047 1.918 4.515 2.462 4.767
betweenobjectsandhumanswhentheyarean- OverallPerformance↑ 3.467 1.633 4.033 2.033 4.869
imatedtoanewpose,and2)“SDFlabelloss”
thatsupervisethelabelofSDFinstead. Thesecomparisonsdemonstratetheeffectivenessofour
proposedmethodformaintainingspatialcorrelationsduringtheanimations.Additionally,wevalidate
ourmodel’sdesignbyfurthercomparingitwithtwovariants: 1)“w/oR,T,L ”whichdisables
CA
the trainable parameters R, T, Eq. 15, and our proposed loss L . This setting represents the
CA
scenariowheretheobjectismoveddirectlywiththecontactpoint. and2)“w/oL ”whichtrains
CA
the animation network solely with SDS loss (L∗ , L ). These comparisons underscore the
SDS SDS
necessityofthesecomponentsinachieving4Danimationwithbetterrobustnesstothepenetration
issues.
Analysis of Spatial-aware SDS.
We finally assess the effectiveness
of spatial-aware SDS (SSDS) and
presenttheresultsinFig.7. Notably,
weobservethatSSDSplaysacrucial
Captain America holdinga flute inhishand Woody in Toy Story holdinga microphone
roleinpreventingtheoptimizationof w/oℒ!∗ "!(static) Ours (static) w/oℒ!∗ "! (dyni an mh icis ) hand Ours
R, S, T from vanishing during 3D Figure7: AnalysisofSpatial-awareSDS.
compositionalgeneration. Addition-
ally,thereisadropinthequalityoftheanimatedavatarswhendisablingSSDS.
5 CONCLUSIONS
Inthispaper,wehaveintroducedAvatarGO,thefirstattemptfortext-guided4Davatargeneration
with object interactions. Within AvatarGO, we proposed to employ large language model for
comprehendingthemostsuitablecontactareabetweenhumansandobjects. Wealsopresenteda
novelcorrespondence-awaremotionoptimizationthatutilizesSMPL-Xasanintermediarytoenhance
themodel’sresiliencetopenetrationissueswhenanimating3Dhumansandobjectsintonewposes.
Extensiveevaluationsdemonstratedthatourmethodhasachievedhigh-fidelity4Danimationsacross
diverse3Davatar-objectpairsandposes,surpassingcurrentstate-of-the-artsbyalargemargin.
Limitations. Whileopeningnewdoorsforhuman-centric4Dcontentgeneration,weacknowledge
AvatarGO has certain limitations: 1) Our pipeline operates under the assumption of rigid-body
dynamicsfor3Dobjects,makingitunsuitableforanimatingnon-rigidcontentsuchasflags;2)our
10method presumes that continuous contact between objects and avatars, making it challenges for
taskslike"Dribblingthebasketball,"wherethehumanandobjectinevitablydisconnectatcertain
points. Nevertheless,ourcurrentapproachdoesnotcoverallpossiblescenarios,iteffectivelyhandles
continuouscontactandrigidconnections,whicharecommonlyencounteredinreal-worldapplications.
REFERENCES
Language segment anything. https://github.com/paulguerrero/lang-sam.git,
2023. 3,6
Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter
Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B Lindell. 4d-fy:
Text-to-4dgenerationusinghybridscoredistillationsampling. arXivpreprintarXiv:2311.17984,
2023. 4
SherwinBahmani,XianLiu,YifanWang,IvanSkorokhodov,VictorRong,ZiweiLiu,XihuiLiu,
JeongJoonPark,SergeyTulyakov,GordonWetzstein,etal. Tc4d: Trajectory-conditionedtext-to-
4dgeneration. arXivpreprintarXiv:2403.17920,2024. 2,4,9,19
YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat,JiamingSong,KarstenKreis,MiikaAittala,
TimoAila,SamuliLaine,BryanCatanzaro,etal. ediffi: Text-to-imagediffusionmodelswithan
ensembleofexpertdenoisers. arXivpreprintarXiv:2211.01324,2022. 1,3
BharatLalBhatnagar,XianghuiXie,IlyaPetrov,CristianSminchisescu,ChristianTheobalt,and
GerardPons-Moll. Behave: Datasetandmethodfortrackinghumanobjectinteractions. InIEEE
ConferenceonComputerVisionandPatternRecognition(CVPR).IEEE,jun2022. 1
AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Dominik
Lorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion: Scaling
latentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023. 2
TimBrooks,AleksanderHolynski,andAlexeiAEfros. Instructpix2pix: Learningtofollowimage
editinginstructions. InIEEEConferenceonComputerVisionandPatternRecognition,2023. 9
AngCaoandJustinJohnson. Hexplane: Afastrepresentationfordynamicscenes. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.130–141,2023. 7,8,
18
TianshiCao,KarstenKreis,SanjaFidler,NicholasSharp,andKangxueYin. Texfusion: Synthesizing
3dtextureswithtext-guidedimagediffusionmodels.InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pp.4169–4181,2023a. 3
YukangCao,Yan-PeiCao,KaiHan,YingShan,andKwan-YeeKWong. Dreamavatar: Text-and-
shapeguided3dhumanavatargenerationviadiffusionmodels. arXivpreprintarXiv:2304.00916,
2023b. 1,3
YukangCao,Yan-PeiCao,KaiHan,YingShan,andKwan-YeeKWong. Guide3d: Create3davatars
fromtextandimageguidance. arXivpreprintarXiv:2308.09705,2023c. 3
MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski,and
ArmandJoulin. Emergingpropertiesinself-supervisedvisiontransformers. InProceedingsofthe
IEEE/CVFinternationalconferenceoncomputervision,pp.9650–9660,2021. 3
Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nießner.
Text2tex: Text-driventexturesynthesisviadiffusionmodels. arXivpreprintarXiv:2303.11396,
2023a. 3
Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and
appearanceforhigh-qualitytext-to-3dcontentcreation. arXivpreprintarXiv:2303.13873,2023b.
3
11RuiChen,MingyiShi,ShaoliHuang,PingTan,TakuKomura,andXuelinChen. Tamingdiffusion
probabilisticmodelsforcharactercontrol. InSIGGRAPH,2024a. 1
YiwenChen,ZilongChen,ChiZhang,FengWang,XiaofengYang,YikaiWang,ZhongangCai,Lei
Yang,HuapingLiu,andGuoshengLin. Gaussianeditor: Swiftandcontrollable3deditingwith
gaussiansplatting. arXivpreprintarXiv:2311.14521,2023c. 6
Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui Jia, and Ziwei Liu. Comboverse:
Compositional 3d assets creation using spatially-aware diffusion guidance. arXiv preprint
arXiv:2403.12409,2024b. 2,3,6,9
Sisi Dai, Wenhao Li, Haowen Sun, Haibin Huang, Chongyang Ma, Hui Huang, Kai Xu, and
RuizhenHu. Interfusion: Text-drivengenerationof3dhuman-objectinteraction. arXivpreprint
arXiv:2403.15612,2024. 2,3,9
MattDeitke,DustinSchwenk,JordiSalvador,LucaWeihs,OscarMichel,EliVanderBilt,Ludwig
Schmidt,KianaEhsani,AniruddhaKembhavi,andAliFarhadi. Objaverse: Auniverseofanno-
tated3dobjects. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.13142–13153,2023. 3
MattDeitke,RuoshiLiu,MatthewWallingford,HuongNgo,OscarMichel,AdityaKusupati,Alan
Fan,ChristianLaforte,VikramVoleti,SamirYitzhakGadre,etal. Objaverse-xl: Auniverseof
10m+3dobjects. AdvancesinNeuralInformationProcessingSystems,36,2024. 3
DaveEpstein,BenPoole,BenMildenhall,AlexeiAEfros,andAleksanderHolynski. Disentangled
3dscenegenerationwithlayoutlearning. arXivpreprintarXiv:2402.16936,2024. 2,3
Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.
Stylegan-nada: Clip-guided domain adaptation of image generators. ACM Transactions on
Graphics(TOG),2022. 9
GegeGao,WeiyangLiu,AnpeiChen,AndreasGeiger,andBernhardSchölkopf. Graphdreamer:
Compositional3dscenesynthesisfromscenegraphs. arXivpreprintarXiv:2312.00093,2023. 2,
3,8,19,22
Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Chongyang Ma, Weiming Hu,
ZhengjunZha,HaibinHuang,PengfeiWan,etal. I2v-adapter: Ageneralimage-to-videoadapter
forvideodiffusionmodels. arXivpreprintarXiv:2312.16693,2023a. 2
Yuan-ChenGuo,Ying-TianLiu,RuizhiShao,ChristianLaforte,VikramVoleti,GuanLuo,Chia-
HaoChen,Zi-XinZou,ChenWang,Yan-PeiCao,andSong-HaiZhang. threestudio: Aunified
frameworkfor3dcontentgeneration. https://github.com/threestudio-project/
threestudio,2023b. 17,18
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. InIEEEConferenceonComputerVisionandPatternRecognition,2016. 18
FangzhouHong,JiaxiangTang,ZiangCao,MinShi,TongWu,ZhaoxiChen,TengfeiWang,Liang
Pan,DahuaLin,andZiweiLiu. 3dtopia: Largetext-to-3dgenerationmodelwithhybriddiffusion
priors. arXivpreprintarXiv:2403.02234,2024. 3
YicongHong,KaiZhang,JiuxiangGu,SaiBi,YangZhou,DifanLiu,FengLiu,KalyanSunkavalli,
TrungBui,andHaoTan. Lrm: Largereconstructionmodelforsingleimageto3d. arXivpreprint
arXiv:2311.04400,2023. 3
Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, Ying Feng, Yebin Liu, and Qing Wang.
Humannorm: Learningnormaldiffusionmodelforhigh-qualityandrealistic3dhumangeneration.
arXivpreprintarXiv:2310.01406,2023a. 3
YukunHuang, JiananWang, AilingZeng, HeCao, XianbiaoQi, YukaiShi, Zheng-JunZha, and
Lei Zhang. Dreamwaltz: Make a scene with complex 3d animatable avatars. arXiv preprint
arXiv:2305.12529,2023b. 3
12ZheweiHuang,TianyuanZhang,WenHeng,BoxinShi,andShuchangZhou. Real-timeintermediate
flowestimationforvideoframeinterpolation. InEuropeanConferenceonComputerVision,pp.
624–642.Springer,2022. 4
NanJiang,TengyuLiu,ZhexuanCao,JiemingCui,ZhiyuanZhang,YixinChen,HeWang,Yixin
Zhu,andSiyuanHuang. Full-bodyarticulatedhuman-objectinteraction. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pp.9365–9376,2023a. 1
RuixiangJiang,CanWang,JingboZhang,MengleiChai,MingmingHe,DongdongChen,andJing
Liao. Avatarcraft: Transformingtextintoneuralhumanavatarswithparameterizedshapeandpose
control. arXivpreprintarXiv:2303.17606,2023b. 3
YanqinJiang,LiZhang,JinGao,WeiminHu,andYaoYao. Consistent4d: Consistent360{\deg}
dynamicobjectgenerationfrommonocularvideo. arXivpreprintarXiv:2311.02848,2023c. 4
RoyKapon,GuyTevet,DanielCohen-Or,andAmitHBermano. Mas:Multi-viewancestralsampling
for 3d motion generation using 2d diffusion. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pp.1965–1974,2024. 1
BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis. 3dgaussiansplatting
forreal-timeradiancefieldrendering. ACMTransactionsonGraphics(ToG),42(4):1–14,2023. 3,
5,8
DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InInternational
ConferenceonLearningRepresentations,2015. 8
Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Fieraru,
and Cristian Sminchisescu. Dreamhuman: Animatable 3d avatars from text. arXiv preprint
arXiv:2306.09329,2023. 3
Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan
Sunkavalli,GregShakhnarovich,andSaiBi.Instant3d:Fasttext-to-3dwithsparse-viewgeneration
andlargereconstructionmodel. arXivpreprintarXiv:2311.06214,2023a. 3
JiamanLi,JiajunWu,andCKarenLiu. Objectmotionguidedhumanmotionsynthesis. ACMTrans.
Graph.,42(6),2023b. 1
TingtingLiao,HongweiYi,YuliangXiu,JiaxaingTang,YangyiHuang,JustusThies,andMichaelJ
Black. Tada! texttoanimatabledigitalavatars. arXivpreprintarXiv:2308.10899,2023. 1,3
Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,XiaohuiZeng,XunHuang,Karsten
Kreis,SanjaFidler,Ming-YuLiu,andTsung-YiLin. Magic3d: High-resolutiontext-to-3dcontent
creation. InIEEEConferenceonComputerVisionandPatternRecognition,2023. 1,3
FangfuLiu,HanyangWang,WeiliangChen,HaowenSun,andYueqiDuan. Make-your-3d: Fastand
consistentsubject-driven3dcontentgeneration. arXivpreprintarXiv:2403.09625,2024. 3
MinghuaLiu,RuoxiShi,LinghaoChen,ZhuoyangZhang,ChaoXu,XinyueWei,HanshengChen,
Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with
consistentmulti-viewgenerationand3ddiffusion. arXivpreprintarXiv:2311.07885,2023a. 3
MinghuaLiu,ChaoXu,HaianJin,LinghaoChen,ZexiangXu,HaoSu,etal.One-2-3-45:Anysingle
imageto3dmeshin45secondswithoutper-shapeoptimization. arXivpreprintarXiv:2306.16928,
2023b. 3
RuoshiLiu,RundiWu,BasileVanHoorick,PavelTokmakov,SergeyZakharov,andCarlVondrick.
Zero-1-to-3: Zero-shotoneimageto3dobject. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pp.9298–9309,2023c. 1,3
XianLiu,XiaohangZhan,JiaxiangTang,YingShan,GangZeng,DahuaLin,XihuiLiu,andZiwei
Liu. Humangaussian: Text-driven3dhumangenerationwithgaussiansplatting. arXivpreprint
arXiv:2311.17061,2023d. 3,8,9,19,22,23
13YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,LingjieLiu,TakuKomura,andWenpingWang.
Syncdreamer: Generatingmultiview-consistentimagesfromasingle-viewimage. arXivpreprint
arXiv:2309.03453,2023e. 3
ZexiangLiu,YangguangLi,YoutianLin,XinYu,SidaPeng,Yan-PeiCao,XiaojuanQi,Xiaoshui
Huang, Ding Liang, and Wanli Ouyang. Unidream: Unifying diffusion priors for relightable
text-to-3dgeneration. arXivpreprintarXiv:2312.08754,2023f. 3
Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma,
Song-HaiZhang,MarcHabermann,ChristianTheobalt,etal. Wonder3d: Singleimageto3dusing
cross-domaindiffusion. arXivpreprintarXiv:2310.15008,2023. 3
MatthewLoper,NaureenMahmood,JavierRomero,GerardPons-Moll,andMichaelJ.Black. SMPL:
Askinnedmulti-personlinearmodel. ACMTrans.Graphics,Asia,2015. 1,5
Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for
shape-guidedgenerationof3dshapesandtextures. arXivpreprintarXiv:2211.07600,2022. 3
BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,and
RenNg. Nerf: Representingscenesasneuralradiancefieldsforviewsynthesis. InEuropean
ConferenceonComputerVision,2020. 3
Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2: Learning
robustvisualfeatureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023. 3
GeorgiosPavlakos,VasileiosChoutas,NimaGhorbani,TimoBolkart,AhmedA.A.Osman,Dimitrios
Tzionas,andMichaelJ.Black. Expressivebodycapture: 3dhands,face,andbodyfromasingle
image. InIEEEConferenceonComputerVisionandPatternRecognition,2019. 1,5
BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall. Dreamfusion: Text-to-3dusing2d
diffusion. InInternationalConferenceonLearningRepresentations,2022. 1
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,2022. 1,3
JiaweiRen,LiangPan,JiaxiangTang,ChiZhang,AngCao,GangZeng,andZiweiLiu. Dreamgaus-
sian4d: Generative4dgaussiansplatting. arXivpreprintarXiv:2312.17142,2023. 2,3,4,8,9,17,
19,23
EladRichardson,GalMetzer,YuvalAlaluf,RajaGiryes,andDanielCohen-Or. Texture: Text-guided
texturingof3dshapes. arXivpreprintarXiv:2302.01721,2023. 3
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed
KamyarSeyedGhasemipour,BurcuKaragolAyan,SSaraMahdavi,RaphaGontijoLopes,etal.
Photorealistictext-to-imagediffusionmodelswithdeeplanguageunderstanding. arXivpreprint
arXiv:2205.11487,2022. 1,3
JunyoungSeo,WooseokJang,Min-SeopKwak,JaehoonKo,HyeonsuKim,JunhoKim,Jin-Hwa
Kim,JiyoungLee,andSeungryongKim. Let2ddiffusionmodelknow3d-consistencyforrobust
text-to-3dgeneration. arXivpreprintarXiv:2303.07937,2023. 3
Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H Bermano. Human motion diffusion as a
generativeprior. arXivpreprintarXiv:2303.01418,2023. 1
TianchangShen,JunGao,KangxueYin,Ming-YuLiu,andSanjaFidler. Deepmarchingtetrahedra:
ahybridrepresentationforhigh-resolution3dshapesynthesis. InAdvancesinNeuralInformation
ProcessingSystems,2021. 3
TianchangShen,JacobMunkberg,JonHasselgren,KangxueYin,ZianWang,WenzhengChen,Zan
Gojcic,SanjaFidler,NicholasSharp,andJunGao.Flexibleisosurfaceextractionforgradient-based
meshoptimization. ACMTransactionsonGraphics(TOG),42(4):1–16,2023. 3
14RuoxiShi,HanshengChen,ZhuoyangZhang,MinghuaLiu,ChaoXu,XinyueWei,LinghaoChen,
ChongZeng, andHaoSu. Zero123++: asingleimagetoconsistentmulti-viewdiffusionbase
model. arXivpreprintarXiv:2310.15110,2023a. 3
YichunShi,PengWang,JianglongYe,LongMai,KejieLi,andXiaoYang. Mvdream: Multi-view
diffusionfor3dgeneration. arXiv:2308.16512,2023b. 3
UrielSinger,ShellySheynin,AdamPolyak,OronAshual,IuriiMakarov,FilipposKokkinos,Naman
Goyal,AndreaVedaldi,DeviParikh,JustinJohnson,etal. Text-to-4ddynamicscenegeneration.
arXivpreprintarXiv:2301.11280,2023. 4
Stability.AI. Stable diffusion. https://stability.ai/blog/
stable-diffusion-public-release,2022. 1,3,8
Stability.AI. Stability AI releases DeepFloyd IF, a powerful text-to-image model
that can smartly integrate text into images. https://stability.ai/blog/
deepfloyd-if-text-to-image-model,2023. 1,3
JiaxiangTang, JiaweiRen, HangZhou, ZiweiLiu, andGangZeng. Dreamgaussian: Generative
gaussiansplattingforefficient3dcontentcreation. arXivpreprintarXiv:2309.16653,2023. 1,3,4,
5
Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm:
Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint
arXiv:2402.05054,2024a. 3
JiaxiangTang,RuijieLu,XiaokangChen,XiangWen,GangZeng,andZiweiLiu. Intex: Interactive
text-to-texturesynthesisviaunifieddepth-awareinpainting. arXivpreprintarXiv:2403.11878,
2024b. 3
Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas
Chandra, Yasutaka Furukawa, and Rakesh Ranjan. Mvdiffusion++: A dense high-resolution
multi-view diffusion model for single or sparse-view 3d object reconstruction. arXiv preprint
arXiv:2402.12712,2024c. 3
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessing
systems,30,2017. 3
PeihaoWang,ZhiwenFan,DejiaXu,DilinWang,SreyasMohan,ForrestIandola,RakeshRanjan,
YileiLi,QiangLiu,ZhangyangWang,etal. Steindreamer: Variancereductionfortext-to-3dscore
distillationviasteinidentity. arXivpreprintarXiv:2401.00604,2023a. 3
PengWang,HaoTan,SaiBi,YinghaoXu,FujunLuan,KalyanSunkavalli,WenpingWang,Zexiang
Xu, and Kai Zhang. Pf-lrm: Pose-free large reconstruction model for joint pose and shape
prediction. arXivpreprintarXiv:2311.12024,2023b. 3
XinzhouWang,YikaiWang,JunliangYe,ZhengyiWang,FuchunSun,PengkunLiu,LingWang,Kai
Sun,XintongWang,andBinHe. Animatabledreamer: Text-guidednon-rigid3dmodelgeneration
andreconstructionwithcanonicalscoredistillation. arXivpreprintarXiv:2312.03795,2023c. 4
ZhengyiWang,ChengLu,YikaiWang,FanBao,ChongxuanLi,HangSu,andJunZhu. Prolific-
dreamer: High-fidelityanddiversetext-to-3dgenerationwithvariationalscoredistillation. arXiv
preprintarXiv:2305.16213,2023d. 1,3
ZhengyiWang,YikaiWang,YifeiChen,ChendongXiang,ShuoChen,DajiangYu,ChongxuanLi,
HangSu,andJunZhu. Crm: Singleimageto3dtexturedmeshwithconvolutionalreconstruction
model. arXivpreprintarXiv:2403.05034,2024. 3
TianyuWu,ShizhuHe,JingpingLiu,SiqiSun,KangLiu,Qing-LongHan,andYangTang. Abrief
overviewofchatgpt: Thehistory,statusquoandpotentialfuturedevelopment. IEEE/CAAJournal
ofAutomaticaSinica,10(5):1122–1136,2023a. 2
15TongWu,JiaruiZhang,XiaoFu,YuxinWang,JiaweiRen,LiangPan,WayneWu,LeiYang,Jiaqi
Wang,ChenQian,etal. Omniobject3d: Large-vocabulary3dobjectdatasetforrealisticperception,
reconstructionandgeneration. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.803–814,2023b. 3
Zike Wu, Pan Zhou, Xuanyu Yi, Xiaoding Yuan, and Hanwang Zhang. Consistent3d: Towards
consistenthigh-fidelitytext-to-3dgenerationwithdeterministicsamplingprior. arXivpreprint
arXiv:2401.09050,2024. 3
Dejia Xu, Hanwen Liang, Neel P Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos N Plataniotis,
andZhangyangWang. Comp4d: Llm-guidedcompositional4dscenegeneration. arXivpreprint
arXiv:2403.16993,2024a. 2,4,18
Sirui Xu, Ziyin Wang, Yu-Xiong Wang, and Liang-Yan Gui. Interdreamer: Zero-shot text to 3d
dynamichuman-objectinteraction. arXivpreprintarXiv:2403.19652,2024b. 1,9
YinghaoXu, HaoTan, FujunLuan, SaiBi, PengWang, JiahaoLi, ZifanShi, KalyanSunkavalli,
GordonWetzstein, ZexiangXu, etal. Dmv3d: Denoisingmulti-viewdiffusionusing3dlarge
reconstructionmodel. arXivpreprintarXiv:2311.09217,2023. 3
YuyangYin,DejiaXu,ZhangyangWang,YaoZhao,andYunchaoWei. 4dgen: Grounded4dcontent
generationwithspatial-temporalconsistency. arXivpreprintarXiv:2312.17225,2023. 4
XuYinghao,ShiZifan,YifanWang,ChenHansheng,YangCeyuan,PengSida,ShenYujun,and
WetzsteinGordon. Grm: Largegaussianreconstructionmodelforefficient3dreconstructionand
generation,2024. 3
Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan Kautz, and Umar
Iqbal. Gavatar: Animatable 3d gaussian avatars with implicit mesh learning. arXiv preprint
arXiv:2312.11461,2023. 4
YifeiZeng,YuanxunLu,XinyaJi,YaoYao,HaoZhu,andXunCao. Avatarbooth: High-qualityand
customizable3dhumanavatargeneration. arXivpreprintarXiv:2306.09864,2023. 3
LvminZhangandManeeshAgrawala. Addingconditionalcontroltotext-to-imagediffusionmodels.
arXivpreprintarXiv:2302.05543,2023. 19
MingyuanZhang,ZhongangCai,LiangPan,FangzhouHong,XinyingGuo,LeiYang,andZiwei
Liu. Motiondiffuse: Text-drivenhumanmotiongenerationwithdiffusionmodel. arXivpreprint
arXiv:2208.15001,2022. 1
MingyuanZhang,XinyingGuo,LiangPan,ZhongangCai,FangzhouHong,HuirongLi,LeiYang,
andZiweiLiu. Remodiffuse: Retrieval-augmentedmotiondiffusionmodel. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pp.364–373,2023. 1,3
MingyuanZhang,DaishengJin,ChenyangGu,FangzhouHong,ZhongangCai,JingfangHuang,
Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, et al. Large motion model for unified
multi-modalmotiongeneration. arXivpreprintarXiv:2404.01284,2024. 1,3
YufengZheng,XuetingLi,KokiNagano,SifeiLiu,OtmarHilliges,andShaliniDeMello. Aunified
approachfortext-andimage-guided4dscenegeneration. arXivpreprintarXiv:2311.16854,2023.
4
Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun,
andMing-HsuanYang. Gala3d: Towardstext-to-3dcomplexscenegenerationvialayout-guided
generativegaussiansplatting. arXivpreprintarXiv:2402.07207,2024. 2,3
16A VIDEO RESULTS
Tobettervisualizethegeneratedresults,weofferanimproveddemonstrationofourmethodthrough
rotatedvideosinthesupplementarymaterials. Toaccessthisdemonstration,pleaseopenthefile
named“index.html”providedinthesupplementary.
B IMPLEMENTATION DETAILS
OurnetworkisbuiltupontheofficialimplementationofDreamGaussian4D(Renetal.,2023)and
Threestudio(Guoetal.,2023b)(anopen-source3Dgenerativeproject).
Toensureeasyreproducibility,wefirstincludeallthehyperparametersforour3Dcompositionstage
inTab.2.
Table2: Hyper-parametersofAvatarGO-3Dcompositionstage.
Cameradistancerange 2.
Radius 2.0
Camerasetting
Elevationrange (-30,30)
FoVrange 49.1
Resolutionfor0-120epochs (128,128)
Rendersetting Resolutionfor120-240iters (256,256)
Resolutionfor240-400iters (512,512)
Guidancescale 7.5
trange (0.01,0.97)
Diffusionsetting Minimalsteppercent 0.01
Maximalsteppercent 0.97
√
ω(t) αt(1−αt)
RotationR torch.normal(mean=[0.5,0.5,0.5,0.5],std=0.1)
Initialization TranslationT 0.0
ScaleS torch.normal(mean=1.0,std=0.3)
RotationR 0.005
Learningrate TranslationT 0.005
ScaleS 0.005
LLM-guidedcontactretargeting thresholda 1e-7
Trainingobjectives λ∗ SDS 1.0
Hardware GPU 1×NVIDIAA100(80GB)
17Table3: Hyper-parametersofAvatarGO-4Danimataionstage.
Cameradistancerange 2.
Radius 2.0
Camerasetting
Elevationrange (-30,30)
FoVrange 49.1
Resolutionfor0-120epochs (128,128)
Rendersetting Resolutionfor120-240iters (256,256)
Resolutionfor240-400iters (512,512)
Guidancescale 7.5
trange (0.01,0.97)
DiffusionsettingtocalculateL∗ SDS Minimalsteppercent 0.01
Maximalsteppercent 0.97
√
ω(t) αt(1−αt)
Guidancescale 7.5
Guidancerescale 0.75
trange (0.02,0.98)
Minimalsteppercent 0.02
DiffusionsettingtocalculateLSDS Maximalsteppercent 0.98
gradientclip [0,1.5,2.0,1000]
gradientclippixel True
gradientclipthreshold 1.0
√
ω(t) αt(1−αt)
RotationR [-0.16,-0.16,-0.16,0.5]
Initialization
TranslationT 0.0
RotationR 0.001
Learningrate
TranslationT 0.001
λCA 1e+3
Trainingobjectives λ∗ 1.0
SDS
λSDS 1.0
Hardware GPU 1×NVIDIAA100(80GB)
Inthe4Danimationstage,weapplyHexPlane(Cao&Johnson,2023)toproducefeaturesfrompoint
positionx andtimestampt,followedbyanMLPtopredicttheoffsetforGaussianattributes,i.e.,
c
pointlocationx,scalingmatrixs,rotationmatrixR. Specifically,theHexPlaneencoderliftsthe
inputstoahigherfrequencydimensionF((x ,t)) ∈ R128,whiletheMLPissettothedefaultin
c
DreamGaussian4DwithResNet(Heetal.,2016).
Tofurtherensureeasyreproducibility,wefirstincludeallthehyperparametersforour4Danimation
stageinTab.3 Theotherhyper-parametersaresettobethedefaultofDreamGaussian4D(Guoetal.,
2023b).
C MORE EXPLANATION ON DESIGNING “OURS (VAR-A)” AND “OURS
(VAR-B)”
“Ours(Var-A)”:ThisisaversionwherewehavedisabledtheLang-SAMinitializationinour3D
staticcompositionalgeneration. Comparingthiswithourfinalmethodshowsthatwithoutassistance
fromLang-SAM,thediffusionmodelstrugglestoaccuratelyinterprethuman-objectimages.
“Ours (Var-B)”: While Comp4D (Xu et al., 2024a) separates 3D scenes into two components
and applies trajectories to one component for compositional 4D generation, it leaves the other
component static. This method is not suitable for our scenarios where both humans and objects
are dynamic. Therefore, we design "Ours (Var-B)" by adopting the Comp4D strategy: allowing
the object to follow a trajectory while the human moves independently. Specifically, we replace
ourcorrespondence-awaremotionsupervision,asdefinedinEq.16,withSDSsupervisionstrategy
via the video diffusion model used in Comp4D. Comparing this approach with our final method
demonstrates that our correspondence-aware motion supervision more effectively preserves the
relationshipbetweenhumansandobjectsthroughouttheanimationprocess.
18D TRAINING COMPLEXITY
Inourstudy,ourresults,detailedinboththemainpaperandtheAppendix,involvetrainingthe3D
stagefor400epochsonasingleNVIDIAA100GPU,takingapproximately10minutes. Similarly,
the 4D stage requires roughly 20 minutes of training on the same GPU. To compare with other
methods: 1)Intheexperimentsfor3Dcompositionalgeneration,HumanGaussian(Liuetal.,2023d)
demandsapproximately2hourstocomplete3600epochs;GraphDreamer(Gaoetal.,2023)adopts
atwo-stagetrainingapproach,withthecoarsestagetakingroughly3hoursfor10000epochsand
thefinestagerequiringaround6hoursfor20000epochs. 2)Additionally,inourexperimentswith
4Danimation,DreamGaussian4D(Renetal.,2023)completestrainingoftheir3-stagenetworkin
around10minutes;TC4D(Bahmanietal.,2024)demandsapproximately1hourforthefirststage
over10000epochs,3hoursforthesecondstageover20000epochs,androughly30hoursforthe
thirdstageover30000epochs.
E 2D HUMAN-OBJECT INTERACTION IMAGE GENERATION
Becauseofthelimitedavailabilityofhuman-objectinteractionimageswithinthe2Ddatasetutilized
fortrainingdiffusionmodels,existingmodelsencounterchallengesinaccuratelycapturingthespatial
dynamicsandcontactbetweenhumansandobjects. ThislimitationisevidentinFigure8,where
wenoticedthatduringtheprocessof2Dimagegeneration,thediffusionmodelwouldstruggleto
createsuchimages. Thisinadequacysignificantlyhamperstheabilityofdiffusionmodelstogenerate
realistic3Dhuman-objectinteractions.
Ultramanholdingan IronManholdingahat Obamaholdinghatin
Posecondition
axeinhishand inhishand hislefthand
Figure8: Examplegenerationofhuman-objectinteractionimages. Imagesgeneratedbypose-
conditionedControlNetZhang&Agrawala(2023)
19F DIRECT RIGGING OF 3D OBJECT AND HUMAN MODELS
Weconductedexperimentsbydirectlypositioningthe3Dobjectsinareasonablepositionrelative
tothehumans. AsshowninFig.9,withoutfurtheradjustmentssuchasrescalingorrotating,the
relationshipsbetweenhumansandobjectsarenotaccuratelydepicted. Penetrationissueswillalso
existinsomeexamples. Evenwithmanualadjustments,suchasrescalingandrotatingthe3Dobjects,
significant human effort is required, and the interactions between humans and objects still lack
accuracy. Forinstance,Fig.9illustratesthathumansfrequentlyappearwithopenhands,whichfails
toconvincingly"hold"theobjectsandsignificantlyunderminestheuserexperience.
IronManholdinganaxeinhishand StevenPaulJobsholdanaxeinhishand
Directrigging Directrigging
Ours Ours
humansandobjects humansandobjects
Figure9: Evaluationbydirectlyrigginghumansandobjects
IronManholdinganaxeinhishand StevenPaulJobsholdanaxeinhishand
Directrigging Directrigging
G A huNmaAnLsaYnSdIoSbjeBctYs DETERMIONuIrNs G THE ANIMhAumTaInOsNandOoFbjeOctBsJECT BY OONuLrsY THE
CONTACT PART
IronManholdinganaxeinhishand IronManholdinganaxeinhishand
IronManholdinganaxeinhishand Bodybuilderholdingadumbbelinhishand
UsingSMPLpose UsingSMPL-Xpose
Ours Ours
fromcontacthumanpart fromcontacthumanpart
StevenPaulJobsholdanaxeinhishand Bodybuilderholdingadumbbelinhishand
DirecFtriiggugirneg10: EvaluationbyusingSMPL-XUpsoinsgeSfMroPmL-Xcopnosteacthumanpart
Ours Ours
humansandobjects fromcontacthumanpart
Weconductedexperimentsusingthecontactpartofthehumanbodytodeterminetheobject’smotion.
TheresultsareshowninFig.10.Wefoundthatthisapproachworkswellwhentheobjectispositioned
farfromthebody,butitcanencounterpenetrationissueswhentheobjectisclosetothebody(see
"Bodybuilderholdingadumbbelinhishand"). Wewillincorporatethisdiscussionintotheupdated
paper.
20H COMPARISONS WITH AVATARCRAFT, DREAMWALTZ AND DREAMAVATAR
InFig.11,weprovidequalitativecomparisonswithAvatarCraft,DreamWaltz,andDreamAvatar. We
observedthatAvatarCraftandDreamAvatararehighlyconstrainedbytheSMPLpriormodel,making
itdifficultforthemtocreatehumanmodelswitheffectiveobjectinteractions. WhileDreamWaltzcan
generatesomeobjectinteractions,theseinteractionsareofteninaccurate. Additionally,DreamWaltz
hastroublemaintainingproperinteractionsthroughouttheanimation,aspresentedinFig.12.
Kratos in god of war holding an axe in his hand
Kratos in god of war holding an axe in his hand
Joker holding a microphone in his hand
DreamWaltz JoAkvera tharoCldrainftg a microphone inD rheiasm hAanvadtar Ours
DreamWaltz AvatarCraft DreamAvatar Ours
Figure11: QualitativecomparisonswithDreamWaltz,AvatarCraft,andDreamAvatar
Iron Man holding an axe in his hand
Iron Man holding an axe in his hand
Figure12: EvaluationonDreamWaltz’sanimatedresults
I SOCIETAL IMPACT.
Theprogressin4DavatargenerationwithobjectinteractionsholdspromisefornumerousAR/VR
applications, yet also raises concerns regarding potential misuse, such as creating misleading or
nonexistenthuman-objectpairings. Weadvocateforresponsibleresearchanddeployment,promoting
opennessandtransparencyinpracticestomitigateanypotentialnegativeconsequences.
21J MORE COMPARISONS ON 3D GENERATION
We provide more qualitative comparisons with HumanGaussian (Liu et al., 2023d), Graph-
Dreamer (Gao et al., 2023), and “Ours (Var-A)” in Fig. 13. These results serve to reinforce
theclaimsmadeinSec.4ofthemainpaper,providingfurtherevidenceofthesuperiorperformance
ofAvatarGOincompositing3Dhumanandobjectmodels.
Bodybuilderholdingadumbbelinhishand
Hulkholdingagoldencudgelinhishand
IronManholdinganaxeofThorinhishand
WolfgangAmadeusMozartholdingacupinhishand
NarutoinNarutoSeriesholdinganAK-47inhishand
WonderWomanholdingadumbbelinhishand
WoodyinToyStoryholdingamicrophoneinhishand
HumanGaussian GraphDreamer Ours (Var-A) Ours
Figure13: Comparisonson3Dcompositionalgenerations.
22K MORE COMPARISONS ON 4D ANIMATION
We further provide more qualitative comparisons of 4D animation with DreamGaussian4D (Ren
etal.,2023),HumanGaussian(Liuetal.,2023d),and“Ours(Var-B)”.Theresultscanbefound
inFig.14. ThesecomparisonsfurtherdemonstratethesuperiorityofAvatarGOinmaintainingthe
spatialcorrelationduringanimationsandinaddressingthepenetrationissues.
Bodybuilderholdingadumbbelinhishand
GokuinDragonBallSeriesholdingatorchinhishand
Jokerholdingamicrophoneinhishand
KratosinGodofWarholdaTorchinhishand
WolfgangAmadeusMozartholdingacupinhishand
NarutoinNarutoSeriesholdinganAK-47inhishand
DreamGaussian4D HumanGaussian* Ours (Var-B) Ours
Figure14: Comparisonson4Danimation.
23