Preprint.
DO BETTER LANGUAGE MODELS HAVE CRISPER VISION?
JonaRuthardt1,GertjanJ.Burghouts2,SergeBelongie3,YukiM.Asano1
1FunAILab,UniversityofTechnologyNuremberg
2IntelligentImaging,TNO
3DepartmentofComputerScience,UniversityofCopenhagen
ABSTRACT
How well do text-only Large Language Models (LLMs) grasp the visual world?
As LLMs are increasingly used in computer vision, addressing this question be-
comesbothfundamentalandpertinent. However,existingstudieshaveprimarily
focused on limited scenarios, such as their ability to generate visual content or
clustermultimodaldata. Tothisend,weproposetheVisualTextRepresentation
Benchmark(ViTeRB)toisolatekeypropertiesthatmakelanguagemodelswell-
aligned with the visual world. With this, we identify large-scale decoder-based
LLMsasidealcandidatesforrepresentingtextinvision-centriccontexts,counter
to the current practice of utilizing text encoders. Building on these findings, we
propose ShareLock, an ultra-lightweight CLIP-like model. By leveraging pre-
computable frozen features from strong vision and language models, ShareLock
achieves an impressive 51% accuracy on ImageNet despite utilizing just 563k
image-caption pairs. Moreover, training requires only 1 GPU hour (or 10 hours
including the precomputation of features) – orders of magnitude less than prior
methods. Codewillbereleased.
1 INTRODUCTION
LargeLanguageModels(LLMs)aresolelypretrainedonunimodaltextualdata,yettheyareincreas-
inglyincorporatedintosystemsthatperceiveandinteractwiththenaturalworld(Ahnetal.,2022;
Driessetal.,2023;Wayve,2023). Thelackofdirectsensoryexperienceraisesfundamentalques-
tions to which extent such models can develop a meaningful and accurate understanding of visual
reality. Dothesemodelsmerelyregurgitatevisuallyrelevantfactualknowledgefromtheirtraining
corpus,ordotheyforminternalrepresentationsthatcorrespondtoreal-worldphenomena? Despite
thesuccessfulintegrationofLLMsintolarge-scaleVision-LanguageModels(VLMs),itisdifficult
tojudgethevisualcapabilitiesalreadyinherenttoLLMsthisway.Thisisnotonlybecauseofwidely
varyingtrainingrecipesandproprietarydatasourcesbutparticularlyduetofine-tuningwithpaired
image-textdata,whichdilutesanyvisualknowledgealreadycontainedintext-onlymodels.
In contrast, Sharma et al. (2024) and Huh et al. (2024) more immediately assess the visual nature
of LLMs and highlight a non-trivial degree of visual understanding and cross-modal alignment.
These works do this by compiling proxy tasks or measures such as generating code to represent
visualconcepts(Sharmaetal.,2024)orcorrelatingvisualwithlanguage-basedrepresentations(Huh
etal.,2024). However,therelianceonhighlyconstrainedandsynthetictaskswithlimitedpractical
significancefailstogaugetheaptitudeofLLMswhendeployedinmorerealisticsettings.
Tothisend, weproposetheVisualTextRepresentationBenchmark(ViTeRB),anovelbenchmark
that directly measures performance on the downstream task of zero-shot open-vocabulary image
classification,aspopularisedbyCLIP(Radfordetal.,2021). Thisenablesustoquantifythevisual
understandingoflanguagemodelsandtheirabilitytoencodetextforvision-centrictasks.Toprevent
concept leakage during the training stage – a significant factor underlying the robust “zero-shot”
performance of many VLMs (Fang et al., 2022; Udandarao et al., 2024; Parashar et al., 2024) –
wereverttothetraditionalnotionofzero-shotlearning(ZSL)whereseenandunseenconceptscan
be strictly delineated and are disjoint (cf. (Lampert et al., 2009)). With the advent of VLMs like
CLIP (Radford et al., 2021), these formerly strict assumptions have been watered down in favor
ofscalingtolargevolumesofwebdatathatlikelycontainmostbuttherarestentitiesandobjects.
Consequently, by enforcing a clear training and evaluation protocol, we can accurately assess the
truegeneralizationcapabilitiesfacilitatedbythelanguageembeddings.
1
4202
tcO
9
]LC.sc[
1v37170.0142:viXraPreprint.
8B
45 13B 8B
8B 11B
7B 8B 7B
7B 2B 7B
40 1.5B
7B
1.5B 13B
1.3B
Flan UL2 2B
35 3B
73 BB 0.5B
3B
T5-XL 1.7B 1.7B 0.5B Model Family
30 Llama 3
Phi-3
Qwen2
BLOOM
Model Variant Falcon
25
Base Gemma
Instruct Vicuna v1.5
BERT Base
20 30 40 50 60 70 80
Massive Multitask Language Understanding (MMLU) Score
Figure1: ViTeRBperformancerelativetoMMLUscores. Modelcapabilityonlanguagetasksis
predictiveofvisualtransferperformanceasmeasuredonourVisualTextRepresentationBenchmark
(R2: 0.101 and −0.121 (excl./incl. Phi-3 models)). MMLU scores are taken from the original
publicationsortheChatbotArenaLLMLeaderboard(Chiangetal.,2024).
UsingtheViTeRBbenchmark, weinvestigatewhatpropertiesanddesignchoicesenablelanguage
models to be effectively leveraged in vision-centric tasks. As one of our key results, we find that
featuresextractedfromdecoder-basedLLMsaremoreeffectivecomparedtoencoder-basedembed-
dings. Intriguingly,wefindthatgeneralLLMcapability,asmeasuredthroughMMLU(Hendrycks
et al., 2021), correlates positively with the model’s ViTeRB visual performance, as shown in Fig-
ure 1. Even off-the-shelf, text-only LLMs without embedding-specific fine-tuning demonstrate
strongvisualrepresentationabilities.
Based on these findings, we propose “Shared Vision-Language-Locked Tuning” (ShareLock), a
straightforward late-fusion VLM that leverages the expressive representations of frozen models
across both modalities. With vast streams of unimodal data available for large-scale unsupervised
pretraining,ourresearchquestionishowtooptimallyexploitthisresourceandinvestigatehowlittle
image-textpaireddata,andthusweakhumansupervision,isneededtoachievecompetitiveresults.
OurextensiveevaluationofShareLock demonstratestheeffectivenessofourapproachinavariety
oftasks. ShareLockoutperformsexistingmethodstrainedonthesamedata,suchasCLIP(Radford
etal.,2021)orLiT(Zhaietal.,2022), byasignificantmarginonclassificationproblemsandper-
formscompetitivelyonretrievalandcompositionalreasoningproblems. Withafractionofthedata
andlearnableparameters,ourmethodapproachestheperformanceofCLIPmodelsfullyoptimized
onordersofmagnitudemoredata. Moreover,byonlytrainingasingleMLPontopoffrozenrepre-
sentations,ShareLockisanextremelylightweightframeworkthatallowsustotrainourmodelwith
batchsizesof16konasingleA100GPU.
Summarizing,themaincontributionsofthisworkareasfollows:
• WeintroduceViTeRB,aprotocolthatstrictlycontrolspriorconceptexposure,enablingthe
assessmentoftruevisualzero-shotunderstandingoflanguagemodels.
• Ourbenchmarkhighlightsdecoder-basedLLMsaseffectivesourcesofvisualknowledge,
withsemanticallymeaningfulrepresentationsdirectlyextractablefromtheirinternalstates.
• WeproposeShareLock,alightweightmethodthatalignsfrozenunimodalfeatures,achiev-
ingstate-of-the-artdataefficiencyandsuperiorperformancecomparedtopreviousmodels.
2
]%[
erocS
kramhcneB
noitatneserpeR
txeT
lausiVPreprint.
2 RELATED WORK
Visualunderstandingoflargelanguagemodels. Manypreviousworks(Liuetal.,2023;Wang
et al., 2023; Li et al., 2023) enable LLMs to interact with visual information by mapping image
featuresintothetokenembeddingspaceofthelanguagemodel,anapproachthatrequiresextensive
alignmentonmulti-modalcorpora. However,LLMscanalsoinferandreasonaboutvisualcontent
without explicit multi-modal training (Bowman, 2023). By transcribing images into text form us-
ingseparateVLMs,LLMscanbenaturallyinterfacedvialanguage(Hakimov&Schlangen,2023).
Sharmaetal.(2024)taskedLLMstodrawcommonobjectsandscenesusingsimpleshapes,indicat-
ingpresentspatialunderstandingandillustratingthatLLMscanconceptualizereal-worldsettings.
VariousworkshighlighttheplausibilityandutilityofLLM-generateddescriptionsofobjectsinthe
contextofimageclassificationanddemonstratethatLLMspossessencyclopedicknowledgeabout
visual characteristics (Pratt et al., 2022; Menon & Vondrick, 2023; Yang et al., 2022; Saha et al.,
2024). Thesecapabilitiessuggestthattheextensivepretrainingonlargevolumesofdiversetextual
dataaidsthevisualunderstandingofLLMs. Promptedbycorrelationsbetweensemanticrepresen-
tationsinthelanguageandvisionspace,Huhetal.(2024)arguethattheembeddingspacesofneural
networksconvergetowardsasharedrepresentationofrealityirrespectiveoftheconcreteoptimiza-
tionobjectives,modelarchitectures,anddatautilizedduringtraining. Similarly,weinvestigatethe
degreeofvisualalignmentinherenttoexclusivelylanguage-basedrepresentationsbutassessthisin
thepracticallymorerelevantcontextofzero-shotimageclassificationanddesignarigorousbench-
marktomeasurethetruegeneralizationcapabilitiesfacilitatedbylanguageembeddings.
Data-efficient CLIP-like models. Prevailing VLMs heavily rely on large-scale corpora. While
theoriginalCLIP(Radfordetal.,2021)wastrainedon400Mimage-textpairs,ALIGN(Jiaetal.)
forwent extensive data cleansing, utilizing a total of 1.8B samples and showing that the noisiness
of web-scraped data can be offset through scale. However, more recent work suggests improving
the data quality rather than quantity as the more promising alley towards better performance, and
a litany of filtration methods has been proposed as a result (Schuhmann et al., 2021; Mahmoud
etal.,2024;Joshietal.,2024;Yuetal.,2023). Suchinvestigationsaimatidentifyingdatasubsets
thateffectivelyfacilitategeneralizationwhilekeepingthetrainingrecipesfixed(Gadreetal.,2023).
Additionally,advanceshavebeenmadeinthemodelarchitectureandtrainingregimetoimprovedata
andcomputationalefficiency. Ithasbeenshownthatevensmallerlanguageencoderswithnotably
fewer layers can perform similarly to more expressive language models (Cui et al., 2022). Zhai
etal.(2022)leveragerepresentationsofpretrainedimageencodersandonlytunethetextencoder.
LilT (Khan & Fu, 2023) takes this further by exploiting pretrained encoders for both vision and
languagemodalities. However,onlyasmallsubsetofparametersandadditionalalignmentmodules
are unlocked and optimized instead of full fine-tuning. Keeping both pretrained encoders entirely
frozen, ASIF (Norelli et al., 2023) aligns their representations in a training-free manner with only
afewmillionimage-textpairs. Comparedtotheseworks,ourapproachfocusesonmaximizingthe
utilityofexistingunimodalmodelsbyaligningthemwithminimalcomputeandlimitedpaireddata.
3 ShareLock: SHARED VISION-LANGUAGE-LOCKED TUNING
InspiredbytheefficiencyandeffectivenessoflatefusionarchitecturesinCLIP-likemodels,Share-
Lock comprises two separate encoders for vision and language inputs. The outputs of either en-
coderϕ(·)aresubsequentlymappedintoasharedd-dimensionallatentspacethroughaprojection
p(·). The latent representation for a given input image x or caption t is therefore computed by
i i
z = p (ϕ (x )) ∈ Rd andz = p (ϕ (t )) ∈ Rd,respectively. Duetothenormalization
img img img i txt txt txt i
followingtheprojection,thecosinesimilaritybetweentwoembeddingsz andz isgivenbytheir
i j
dotproduct(i.e.,sim(z ,z )=⟨z ,z ⟩). Duringtraining,thecontrastivelossencouragesthemodel
i j i j
tomaximizethesimilaritybetweenembeddingsofcorrectimage-captionpairingswhiledecreasing
thesimilarityofnon-correspondingpairs. Foranimage-captionpairiinabatchwithN items,itis
givenby
exp(cid:0) sim(zi ,zi)/τ(cid:1)
L(i)=−log m n , (1)
(cid:80)N exp(sim(zi ,zj)/τ)
j=1 m n
forbothalternatedmodalitiespairings(m,n) ∈ {(txt,img),(img,txt)}andwithτ beingafixed
temperatureparameter.GivenasetofclassesCandtheircorrespondingtextualclassrepresentations
3Preprint.
CLIP LiT ShareLock
Image Text Image Text Image Text • Both backbones frozen
• Decoder LLM for text
representations
• 16k batchsizeon 1GPU
fc fc fc MLP
Figure2: ModelSchematicofLateFusionVLMs. ComparedtopriorworkslikeCLIPandLiT,
weproposeShareLock,whichutilizesfrozenpretrainedrepresentationsforbothmodalities,allow-
ing extremely efficient training. ShareLock also benefits from progress in the LLM domain by
representingtextwithdecoder-onlyLLMs,suchasLlama-3.
f(·)(e.g.,“a photo of a <class name>”),thepredictedclasscˆforasamplex isobtained
i
viacˆ=argmax⟨z ,p (ϕ (f(c)))⟩.
img txt txt
c∈C
Deviatingfrompriorworks,ShareLockleveragesfrozenpretrainedmodelsinboththevisionaswell
aslanguagecomponents,ascanbeseeninFigure2. Asthealignmentofthetwomodalitiesisstill
necessary,onlythelightweightprojectionnetworksp(·)areoptimized.
4 VISUAL TEXT REPRESENTATION BENCHMARK
TheobjectiveofourproposedvisualalignmentbenchmarkViTeRBistoassesshowlanguagemod-
els facilitate generalization to novel concepts. It retains the model architecture and optimization
objectives of ShareLock but places restrictions on the data akin to traditional ZSL approaches (cf.
Lampert et al. (2009)). To rigorously attest to the true generalization performance without be-
ing affected by concept leakage through supervision with arbitrary image-caption pairs, we split
conventional image classification datasets into sets of seen classes S as well as unseen classes U,
ensuringthatS∩U=∅. Toprovidecoverageacrossnaturalandhumanartifacts(e.g.,aircraftsand
animals), coarse and fine-grained categories (e.g., zebra vs. dolphin and fish crow vs. American
crow), and different scales (40 ≤ |S| ≤ 1000), the reported scores are averaged per-class accu-
racies over U across four datasets. Namely, AWA2 (Xian et al., 2017), CUB (Wah et al., 2011),
FGVCAircraft(Majietal.,2013),andImageNet+areselectedfortheircomplementarycharacteris-
tics. ImageNet+definestheImageNet-1kclassesasseenconceptsandtreatsthe500mostpopulated
classes(i.e.,highestnumberoftrainingsamples)ofImageNet-21kasunseenones. ForAWA2and
CUB, we utilize the splits proposed by Xian et al. (2017) while randomly assigning aircraft types
into50seenand20unseenclasses.
Astheclassificationperformanceonunseenclassesisprimarilycontingentonthevalidityandse-
mantic continuity of the class representation, the proposed setup can assess the visual alignment
of language embeddings. In the absence of image-specific captions, text-based class representa-
tions f(y ) are used as supervision signals during training and for zero-shot transfer during infer-
i
ence. Besides the template-based targets proposed by Radford et al. (2021) that solely substitute
the respective class names, we generate more comprehensive auxiliary information about classes
(e.g.,visualdescriptions)usingtheinstruction-tunedversionoftheLlama-38Bmodelandacquire
human-curatedinformationfromWikipedia(detailsprovidedinA.2).
5 LANGUAGE MODELS FOR VISUAL ZERO-SHOT GENERALIZATION
Utilizing the Visual Text Representation Benchmark(ViTeRB), we investigate the impact of spe-
cific design choices to identify critical factors that promote generalization and inform subsequent
decisionswhenbuildingalocked-image-locked-textmodel.
LLMsarecomprehensiverepositoriesforreal-worldknowledge. Whilesimpletemplateshave
proveneffectiveclassificationtargetsonlarge-scaleCLIP-likemodels(Radfordetal.,2021;LAION
AI,2022),trainingwithconventionalimageclassificationdatasetsopensupthepossibilityofusing
alternativesemanticclassrepresentationsaswell. Especiallytheclass-wisesupervisioncombined
with limited diversity and number of concepts can impede vision-language alignment. Therefore,
4Preprint.
Table 1: Classification results of ViTeRB benchmark for various language models. Decoder-
based language models outperform encoder-based architectures across all types of input data.
Llama-38BisusedforLLMgeneratedWikipediaarticlesanddescriptions.
Type LanguageModel Reference ClassNames LLMDescription LLMWikip.Articles Wikip.Articles
BERT-Large Devlinetal.(2019) 18.3 15.9 20.8 22.9
T5-XL Raffeletal.(2020) 33.6 37.8 37.9 40.7
Flan-UL2 Tay(2024) 37.0 43.4 42.6 41.9
SentenceT5-XXL Nietal.(2022) 39.5 44.7 44.3 41.6
NV-Embed Leeetal.(2024) 40.5 42.9 47.5 45.9
Gemma7B Gemmaetal.(2024) 39.7 33.7 45.1 43.4
Llama-38B Dubeyetal.(2024) 40.2 43.8 44.9 44.3
wefirstutilizeViTeRBwithdifferenttypesoftextualclassrepresentationstogaugehowtheirnature
andinformationcontentaffectthemodel’sgeneralizationability. Moredetailsaboutthecharacter-
istics and acquisition of these class representations are provided in Section A.2 of the appendix.
Depending on the type of language model, text features are obtained from a special CLS token or
thelasttexttoken,asshowninFigure3. TheresultsaresummarizedinTable1.
Inlinewithpreviousstudies(Prattetal.,2022;Menon&Vondrick,2023;Yangetal.,2022;Saha
et al., 2024), we find that the addition of auxiliary information, such as class descriptions, results
in improved performance for most language models. This is even true for the Llama-3 model,
which was used to generate the description data, resembling findings from Chain-of-thought (Wei
et al., 2022), where model performance increases with response length. We also find that LLM-
generated articles describing a class in the style of Wikipedia (LLM Wikip Articles in Table A.2
can provide strong targets during multi-modal alignment, achieving the best overall performance
of 47.5%. Interestingly, relying on strictly human-curated data in the form of actual Wikipedia
articles tends to lower scores, for example, from 47.5% → 45.9% and 44.9% → 44.3%, for NV-
Embed and Llama-3. Thus, LLMs can effectively absorb and interpolate substantial amounts of
factualinformationfromtheirtrainingdata,positioningthemasvaluablesourcesofvisuallyrelevant
knowledge.
Decodersoutperformencodersinvisualconceptrepresentation. Anewinsightresultingfrom
thisanalysisisthecompetitivenessofdecoder-basedlanguagemodelsforrepresentingvisualcon-
cepts. Comparedtoencoders, weshowthatrepresentinginputswithdecoderscanresultinhigher
performanceforvisualtasks,mirroringarecentlyemergingtrendinthelanguagedomain(Leeetal.,
2024;Springeretal.,2024). NV-Embed(Leeetal.,2024),amodeltunedexplicitlyforembedding
text, emerges as the best performer across various types of input data with a maximum perfor-
manceof47.5%. However,evenoff-the-shelveLLMslikeGemmaorLlamamanagetooutperform
encoder-basedmodelsandtrailNV-EmbedwithViTeRBscoresof45.1%and44.9%,respectively.
Fine-tuning regimes have minimal impact on visual alignment. LLMs are often subject to ad-
ditionaltask-specificfine-tuning. Table2comparesdifferentmodelsandtheirderivativestunedon
instruction and multi-modal data. While vision-tuned variants are available for Phi-3 (Microsoft,
2024), Qwen2(Wangetal.,2024), andVicunav1.5(Zhengetal.,2023), XTuner’sLLaVAmodel
(XTunerContributors,2023)constitutesthesourceforthevisualLlama-3variant. Acrossallmod-
els,withtheexceptionofLlama-3,theimpactoffine-tuningisminor,typicallyshiftingperformance
by only a few decimal points in the case of Phi-3 and Vicuna. Interestingly, Llama-3’s perfor-
mance declines significantly after fine-tuning (−1.7 and −10.4 for instruction and visual tuning),
contrasting with the generally stable results of other models. Neither instruction-based nor visual
fine-tuning shows a clear and consistent advantage in improving overall performance. Ultimately,
thebasemodel’sarchitectureandtrainingregimearemoresignificantindeterminingperformance
thanpost-hocfine-tuningstrategies.
LLMperformancecorrelateswithvisualperformance. InFigure1,wecomparevariousLLMs
by their ViTeRB performance, as well as their MMLU (Hendrycks et al., 2021) score, which is a
common metric to measure LLM performance. We find that the capability of language models is
positively correlated with their ability to perform well on the visual ViTeRB tasks. Since models
steadilyimproveinthelanguagedomain,thisbenchmarkwillbeusefultoassesswhetherthetrend
ofincreasingvisualunderstandingwillcontinueinfutureLLMmodels.Ifthisholds,ShareLockcan
piggybackoffandbenefitfromdevelopmentsintheLLMdomain.
5
.cnE
.ceDPreprint.
Table 2: Classification results of ViTeRB benchmark for different fine-tuning regimes. Fine-
tuning has minimal impact on visual alignment of LLM representations. Llama-3 is a notable ex-
ceptionwithasignificantperformancedecreaseforinstruct-andvision-tunedvariants.
Fine-Tuning Llama-38B Phi-3Mini Qwen27B Llama-2/Vicuna7B
None 44.4 n/a 40.2 42.5
Instruct 42.7 36.9 41.8 42.3
Visual 34.0 37.0 40.3 42.6
A notable outlier is presented by the Phi-3 model family, which score comparably low ViTeRB
resultsgiventheirMMLUscores. Thisdiscrepancylikelyillustratestheeffectsoftheextensivedata
curationandsyntheticdatacreationutilizedinPhi3,whichmightremovevisualinformationtofavor
tokens that promote reasoning abilities. Thus, a lack of exposure to sufficient factual knowledge
aboutreal-worldconditionsmayimpedetheformationofvisuallyinformedrepresentations.
6 IMAGE-CAPTION PRETRAINING EXPERIMENTS
The previous section has provided us with prerequisite insights to propose ShareLock and moti-
vatedthechoicetoleveragethestrongvisuallyalignedrepresentationsofLLMsinthecontextofa
CLIP-likemodel. Forgoingthestrictzero-shotsetupandmovingtowardlarger-scaleimage-caption
datasets, we intend to explore how well these observations translate in the context of a general-
purposeVLMandwhetheronlyoptimizingalightweightnetworkontopoffrozenfeaturesissuf-
ficienttocompetewithfullpretrainingorfine-tuning. Thisanalysiswillillustratethecurrentupper
limitsofutilizingunimodalfoundationmodelsasbuildingblockswhileapplyingminimaladditional
computeandmulti-modaldatatoachievehigh-performingVLMs.
6.1 EXPERIMENTALSETUP
Pretrained Vision and Language Models. Given its strong performance, broad pretraining
regime, and popularity, the ViT-B/14 variant of the DINOv2 model family (Oquab et al., 2023)
isusedasthedefaultvisionbackboneunlessnotedotherwise. Languagefeaturesareextractedfrom
aLlama-38BLLMthroughlasttokenpooling,asshowninFigure3.ForLiTbaselines,weinitialize
the language encoder with pretrained BERT weights (Devlin et al., 2019), in accordance with the
originalimplementation(Zhaietal.,2022). WhencomparingLiT,ASIF,andShareLockmodelsin
thefollowing,theexactsamepre-computedinputfeatures(barringthelanguagecomponentofLiT).
Projection Networks. As in Zhai et al. (2022), no transforma-
tions are applied to the vision features. The MLP projection net-
work after the language model comprises four layers. Between
consecutive layers, inputs are normalized via Batch Normaliza-
tion (Ioffe & Szegedy, 2015) and fed into a ReLU non-linearity.
Dropout (Srivastava et al., 2014) with p = 0.2 is applied during
training. Wehavealsoexploredmoresophisticatedprojectionnet-
works,butfoundtheMLPtooverallprovidebestperformances,see
detailsandablationsinAppendixA.3.
Figure 3: Text features. We
Datasets. Ourinvestigationfocusesonminimizingtheamountof
obtain the final text features
paired data required and explores how unimodal embeddings can
byprocessingthelastcaption
driverobustmultimodalperformancewithminimalsupervisionand
tokenwithanMLP.
alignment. As a result, our evaluation is limited to comparably
small paired datasets. COCO Captions. Containing human an-
notations for around 80k images, COCO Captions (Chen et al., 2015) is a small but high-quality
multimodal dataset. As multiple captions per image are available, a random caption is sampled
duringeachiteration. CC3M.TheConceptualCaptionsdataset(Sharmaetal.,2018)wasbuiltby
scraping image-alt-text pairs from websites, applying filters to remove noisy or mismatched data.
Duetoexpiredlinks, ourversionofCC3Mcontainsaround2.8Mimage-textpairs. Wealsousea
smallersubsetfilteredformorebalancedconceptcoveragefortheLLaVAVLM(Liuetal.,2023).
6Preprint.
CC12M. Expanding the scale and diversity of CC3M, CC12M (Changpinyo et al., 2021) is the
largest dataset used to train and evaluate our model, offering insights into performance at higher
datascales. Ourdatasetversioncontainsapproximately8.5Mimage-textpairs.
Training. ThetrainingsetuplargelyfollowstheCC12MconfigurationofLiT(Zhaietal.,2022)
andusestheAdamoptimizer(Kingma&Ba,2014)withalearningrateof10−3andaweightdecay
of10−4. Gradientclippingtoaglobalnormof1isapplied. TheCLIPloss(Radfordetal.,2021)
with τ = 0.07 is employed, and models are trained until convergence on a validation split sets
in, which is around 5k optimization steps with a batch size of 16,384 – regardless of dataset size.
Features of the frozen vision and language models are initially precomputed and stored for direct
re-useinsubsequentepochs.
Trainingspeedandstorage. OnasingleA10040GBGPU,theprecomputationoflanguagefea-
tures with LLama3 8B takes around 8 hours for the CC3M-Llava subset containing 563k image-
captionpairs. TheDINOv2featuresareobtainedin1GPUhour,andthefinalmultimodaloptimiza-
tion of the MLP also takes around 1 GPU hour, bringing the total training time to approximately
10 GPU hours. In terms of storage, the original dataset requires over 80GB of storage, while our
precomputedfeaturesonlyrequirearound12GB.
Evaluation. WeemployacomprehensivesuiteofVLMevaluationstoassessandcompareShare-
Lock’scapabilitiesacrossawiderangeoftasks. BasedonthepubliclyavailableCLIPBenchmark
(LAION AI, 2022), we gauge the models’ zero-shot classification and retrieval abilities across di-
versedatasetsandprovidequalitativetext-to-imageretrievalresultsonImageNetforCC3Mtrained
models. Additionally, the challenging compositionality Winoground task (Thrush et al., 2022) is
explored.
Benchmarks. We compare our proposed method against a variety of existing VLMs with a par-
ticularemphasisondata-efficientalignmentapproaches. AlongsidetheoriginalViT-B/16variantof
CLIP(Radfordetal.,2021),wetestagainstseveralCLIP-likemodelstrainedonpublicdatasetsof
different scales (Fan et al., 2023; Gadre et al., 2023). Using pretrained models to their advantage,
weassesshowShareLockstacksupagainstLiT(Zhaietal.,2022)andASIF(Norellietal.,2023).
6.2 COMPARISONTOSTATE-OF-THE-ART
Comparison to prior works on IN-1k. Taking the ImageNet-1k zero-shot classification perfor-
mance as the principal benchmark for model performance, ShareLock clearly outperforms other
models trained with similar amounts of data, as demonstrated in Table 3. Compared to the small-
scale CC3M CLIP model (Fan et al., 2023), ShareLock performs notably better, achieving an ac-
curacy 52.1% vs. 16.0%. Adding LLM-based features further proves effective when consider-
ing the 44.1% accuracy of LiT (Zhai et al., 2022), which utilizes the same vision backbone as
ShareLock. Ouroptimization-basedalignmentalsoconsistentlyoutperformsthetraining-freeASIF
(Norellietal.,2023)method,whichreliesonalargereferencedatasetwithdiverseconceptscovered
for performance. As the dataset size increases, fine-tuning encoders becomes more feasible. Yet,
ShareLockstillmaintainsperformancegainsof3%−18%toLiTandCLIPevenforCC12M.
Robustness. To evaluate the robustness of the VLMs under distribution shifts, the ImageNet-1k
classificationobjectiveisrepeatedwithvisualout-of-distributioninputs.AsseeninTable3,columns
‘IN-v2’,‘IN-R’,etc.,ShareLockstillcomparesfavorablytopreviousapproaches.Onaverage,itsur-
passesothermodelstrainedwithdatasetscomparableinscaleandapproachesvanillaCLIPmodels
trainedwithordersofmagnitudemoredata(8.5Mvs. 400MfortheoriginalCLIP).
Fine-grained classification. As shown in Table 4, the strong unimodal features of ShareLock
similarly contribute positively to fine-grained problems. Here, ShareLock outperforms CLIP, LiT,
and DataComp models by large margins on 8/12 evaluations. We also find that on these datasets,
themodelsbenefitmuchmorenoticeablyfromincreaseddatascale,e.g. from10.6%onFlowersto
48.8%whenincreasingthedataset100×. Intuitively,exposuretoamorediverseandnuancedsetof
concepts makes methods more capable of performing fine-grained classification. Nonetheless, the
7Preprint.
Table 3: Frozen CLIP-like zero-shot classification on ImageNet variants. ShareLock outper-
formsCLIP,LiTandASIFbaselinesacross21/24ImageNetevaluationsandachievesperformances
competitivewithmodelsthatutilizesignificantlymorepaireddata,suchasCommonPool-L(384M).
TrainingDataset TestDataset
Model Average
Size Name IN-1k IN-V2 IN-R IN-A INSketch ObjectNet
LiT 83k COCOCaptions 23.3 20.8 34.4 21.1 18.4 29.2 24.5
ASIF 83k COCOCaptions 9.4 8.7 14.4 8.8 6.9 16.1 10.7
ShareLock 83k COCOCaptions 32.2 28.6 36.6 22.8 22.4 30.4 28.8
LiT 563k CC3MSubset 41.7 37.5 59.2 44.4 32.4 40.7 42.6
ASIF 563k CC3MSubset 21.6 20.5 27.7 24.4 14.9 21.5 21.8
ShareLock 563k CC3MSubset 50.5 45.8 60.5 47.0 36.9 41.1 47.0
CLIP 2.8M CC3M 16.0 13.2 17.6 3.6 6.4 8.2 10.8
LiT 2.8M CC3M 44.1 39.3 62.7 45.6 34.8 43.3 45.0
ShareLock 2.8M CC3M 52.1 47.1 64.1 50.9 39.0 43.1 49.4
DataComp-LAION 3.84M CommonPool-S 3.0 2.7 4.4 1.5 1.3 3.7 2.8
CLIP 12M CC12M 41.6 35.4 52.6 10.7 28.8 24.0 32.2
LiT 8.5M CC12M 56.2 49.9 70.3 52.8 43.9 47.8 53.5
ShareLock 8.5M CC12M 59.1 53.2 68.8 53.4 44.5 46.7 54.3
DataComp-LAION 38.4M CommonPool-M 23.0 18.9 28.0 4.3 15.1 17.7 17.8
DataComp-LAION 384M CommonPool-L 55.3 47.9 65.0 20.2 43.2 46.5 46.3
CLIP 400M Proprietary 68.4 61.8 77.6 50.1 48.2 55.4 60.2
Table4: Zero-shotclassificationonfine-graineddatasets. Fine-grainedproblemsrelyheavilyon
large-scaledata;still,ShareLockperformscompetitivelywithothermodelstrainedonthesamedata.
TrainingDataset TestDataset
Model Average
Size Name Aircraft Pets Flowers Cars EuroSAT
LiT 83k COCOCaptions 1.6 28.8 7.7 1.8 21.9 12.3
ASIF 83k COCOCaptions 2.8 7.0 1.6 1.3 21.5 6.8
ShareLock 83k COCOCaptions 3.0 20.6 10.6 9.2 25.1 13.7
CLIP 2.8M CC3M 1.4 13.0 10.8 0.8 12.9 7.8
LiT 2.8M CC3M 2.1 28.5 35.9 3.0 34.4 20.8
ShareLock 2.8M CC3M 6.5 43.1 32.8 4.4 27.9 22.9
DataComp-LAION 3.84M CommonPool-S 1.4 4.0 1.8 1.6 15.8 4.9
CLIP 12M CC12M 2.5 64.2 36.7 24.1 20.9 29.7
LiT 8.5M CC12M 5.0 74.4 48.2 13.2 35.3 35.2
ShareLock 8.5M CC12M 8.3 66.6 48.8 11.5 40.7 36.7
DataComp-LAION 38.4M CommonPool-M 1.7 29.9 22.4 22.0 18.8 18.9
DataComp-LAION 384M CommonPool-L 7.1 77.8 53.3 67.7 41.0 49.4
CLIP 400M Proprietary 24.4 89.0 71.2 64.7 55.9 61.0
effectivenessofourmethodinleveragingauxiliaryknowledgecontainedinLLMrepresentationsis
demonstratedthroughsurpassingalternativemethodsonthesametrainingdatasets.
Compositionality. LatefusionVLMshavelongstruggledwithnuancedtextualscenedescriptions
or fine-grained compositional differences as tested through benchmarks like Winoground (Thrush
etal.,2022)orSugarCrepe(Hsiehetal.,2023). WhileourresultsarecompetitiveinWinoground,
outscoring CLIP and LiT by a couple of percentage points in 5/6 cases (see Table 5), the reliance
oncapablepretrainedmodelssofarfailedtomaterializeinasignificantabove-randomperformance
forthechallengingcompositionalitytask. ShareLocksharessimilarlimitationsaspreviousmethods
andremainsfarfromreachinghuman-levelperformance.
Data scaling. In Figure 4, we show the performance of various CLIP-like methods and models
withincreasingimage-captiondatasetsizes. Wefindthatstartingfromscratch,vanillaCLIPmodels
require orders of magnitude more data to achieve similar performance levels compared to Share-
Lock. ThisremainstrueevenformoresophisticateddatafilteringmethodsasusedintheDataComp
models. While sharing a similar improvement trajectory, suggesting comparable scaling charac-
teristics, ShareLock alsoconsistentlyoutperformsLiT.Thisunderlinesthatfeaturesofextensively
trained unimodal models possess a semantic understanding that can be efficiently aligned across
modalitieswithminimalpaireddata.
Limitations As shown in the previous section, ShareLock outperforms existing methods across
zero-shotclassificationtasks. However,wefindthatitperformslesscompetitivelyinotherproblem
8Preprint.
Table5: Compositionalreasoning. Strongfrozenlanguagefeaturesalonedonotaddresstheshort-
comings inherent to prior CLIP-like models when it comes to spacial or conceptual relationships.
Forspace,thefulltableincludingCC3MmodelperformancesisprovidedintheAppendix.
TrainingDataset Winoground
Model
Size Name Text Image Group
Human 89.5 88.5 85.5
Chance 25.0 25.0 16.7
LiT 83k COCOCaptions 25.0 5.8 2.8
ASIF 83k COCOCaptions 18.8 9.0 5.3
ShareLock 83k COCOCaptions 21.0 11.8 6.5
CLIP 12M CC12M 22.3 9.5 5.3
LiT 8.5M CC12M 24.3 6.5 4.8
ShareLock 8.5M CC12M 26.3 12.8 5.3
DataComp-LAION 38.4M CommonPool-M 25.0 8.3 6.3
DataComp-LAION 384M CommonPool-L 27.0 9.5 7.0
CLIP 400M Propriatary 30.8 10.8 8.3
ShareLock (ours)
60
LiT
CLIP
50 DataComp-LAION
DataComp
40
30
20
10
0
105 106 107 108 109
Dataset Size [Number of Unique Image-Caption Pairs]
Figure4:Scalingofzero-shotperformanceacrossdatasetsizes.ShareLockachievesbest-in-class
performanceusingsignificantlyfewerimage-captionpairscomparedtoCLIPandLiTmodels.
settings. Aswebelievethatsuchnegativeresultscanyieldusefulinsightsintotheworkingsofour
methodtotheresearchcommunity,weexplicitlyhighlightsomepersistinglimitations.
Forretrievaltasks, havingmoretunablemodelcomponentscanbeadvantageous, asseeninTable
6. Here, CLIP and LiT frequently achieve higher scores compared to ShareLock with a relative
advantageof10.4and1.1percentagepointsacrossevaluationdatasetsfortheCC12M-trainedvari-
ants, respectively. This may be due to the reduced internal post-hoc adaptation capacity during
contrastive alignment of frozen task-unspecific textual representations. However, when utilizing
retrieval-specificLLMfeaturessuchasfromNV-Embed(Leeetal.,2024),theShareLockparadigm
experiencesasignificantboostinretrievalabilitiesofupto17%,asshowninTable8.
6.3 QUALITATIVERESULTS
Inadditiontotheextensivesuiteofquantitativeevaluations,wepresentseveralqualitativeresultsin
Figure5toillustratetheeffectivenessofourmethod. Acrossdiversetextualprompts,ourapproach
demonstrates strong alignment between textual and visual representations. Compared to versions
ofCLIPandLiTalsotrainedonCC3M,wefindthatShareLockgenerallyperformsbetterforfine-
grained(i.e.,“aphotoofaBMW.”)andmoreabstract(i.e.,“[...] heavyseas.”)prompts.
9
]%[
ecnamrofreP
k1-teNegamIPreprint.
Table6: Recall@5scoresforimageandtextretrieval. Previousmodelswithencoder-basedand
fullyfine-tunedlanguagefeaturesachievebetterperformanceinmostretrievaltasks. Forspace,the
fulltableincludingCC3MmodelperformancesisprovidedintheAppendix.
TrainingDataset Flickr8k Flickr30k MSCOCO Average
Model
Size Name T →I I→T T →I I→T T →I I→T T →I I→T
LiT 83k COCOCaptions 57.5 77.1 61.3 80.6 50.7 69.1 56.5 75.6
ASIF 83k COCOCaptions 10.4 20.6 12.1 24.9 8.9 17.1 10.4 20.9
ShareLock 83k COCOCaptions 50.5 69.6 56.9 78.4 27.0 41.9 50.2 69.5
CLIP 12M CC12M 73.1 84.5 73.9 86.3 51.0 65.4 66.0 78.7
LiT 8.5M CC12M 60.0 72.8 69.1 82.1 36.5 53.4 55.2 69.4
ShareLock 8.5M CC12M 55.0 73.0 67.1 81.7 32.7 50.1 51.6 68.3
DataComp-LAION 38.4M CommonPool-M 39.4 52.6 39.2 52.4 26.0 35.9 34.9 47.0
DataComp-LAION 384M CommonPool-L 78.1 89.0 81.0 90.7 57.6 71.7 72.2 83.8
CLIP 400M Propriatary 82.9 91.4 85.6 96.2 58.4 76.8 75.6 88.1
CLIP LiT ShareLock
AAA ppphhhoootttooo ooofff aaa bbbaaannnaaannnaaa...
Sim: 0.62 Sim: 0.52 Sim: 0.52 Sim: 0.67 Sim: 0.67 Sim: 0.66 Sim: 0.71 Sim: 0.69 Sim: 0.68
AAA pppeeerrrsssooonnn hhhooorrrssseeebbbaaaccckkk
rrriiidddiiinnnggg...
Sim: 0.48 Sim: 0.47 Sim: 0.46 Sim: 0.57 Sim: 0.56 Sim: 0.52 Sim: 0.53 Sim: 0.49 Sim: 0.49
AAA llliiiggghhhttthhhooouuussseee cccaaauuuggghhhttt iiinnn
hhheeeaaavvvyyy ssseeeaaasss...
Sim: 0.47 Sim: 0.46 Sim: 0.46 Sim: 0.68 Sim: 0.66 Sim: 0.64 Sim: 0.69 Sim: 0.67 Sim: 0.65
AAA ppphhhoootttooo ooofff aaa BBBMMMWWW...
Sim: 0.48 Sim: 0.47 Sim: 0.46 Sim: 0.47 Sim: 0.47 Sim: 0.46 Sim: 0.56 Sim: 0.56 Sim: 0.55
Figure5: Comparisonontext-to-imageretrieval. Weshowqualitativetop-3retrievalresultsfor
CLIP, LiT and ShareLock all trained on CC3M. Green border color indicates correctly retrieved
samples.
Table 7: Ablation of the vision encoder used in ShareLock. Strong and comprehensive image
featuresareessentialforgeneralization.
Avg.ClassificationScores Avg.RetrievalScore WinoGround
VisionEncoder Dataset
Robustness Fine-Grained T →I I→T Text Image Group
ResNet101 IN-1k(sup.) 28.0 16.5 28.3 44.9 21.8 15.3 8.8
ViT-B/16 IN-1k(sup.) 36.2 14.8 36.0 47.3 21.8 10.8 6.0
DINOViT-B/16 IN-1k(unsup.) 25.8 16.1 38.8 54.5 21.5 15.3 8.8
ShareLock(DINOv2) Various(unsup.) 49.4 22.9 48.2 62.7 22.8 15.8 9.0
6.4 ABLATIONS
As the nature of the frozen input features is of great significance in the ShareLock paradigm, the
choiceofvisionandlanguageencodersisablated. Inaddition,alternativeprojectionnetworkarchi-
tecturesarecomparedinSectionA.3. AllablationsareperformedontheCC3Mdataset.
Vision encoder. ShareLock is agnostic to the specific type of vision encoder employed. There-
fore,weablatedifferentchoicesofsupervisedandunsupervisedsupervisionapproachesaswellas
different model architectures in Table 7. As ShareLock projects language embeddings into the vi-
sionspace,themodelchoiceispivotal,asdemonstratedbytheDINOv2backboneyieldingalmost
double the classification scores compared to the worst performing encoder and a 36% lead over
the second-best vision transformer. Models supervised on datasets limited in scope clearly show
reduced robustness and less generality compared to DINOv2, illustrating how generalization can
benefitfrombroadpretrainingacrossvariousconcepts–evenwithoutexplicitsupervision.
Languagemodel. Similarly,wecomparevariationsofShareLockusingdifferentlanguagemodels
and list their performance in Table 8. These results illustrate the effectiveness of decoder-based
approachespreviouslyhighlightedbytheViTeRBbenchmarkinSection5. Despiteservingasthe
startingpointinLiTmodels,theBERTencoderfailstoachievecompetitiveresultswithoutanyfine-
10Preprint.
Table8:Ablationoflanguagemodelusedfordual-lockedtuning. Decoder-basedlanguagemod-
elsarekeytoenablethestrongperformanceofShareLock.
Avg.ClassificationScores Avg.RetrievalScore WinoGround
LanguageModel
Robustness Fine-Grained T →I I→T Text Image Group
BERT-Base 36.2 7.2 36.6 50.3 19.5 9.0 5.0
ShareLock(NV-Embed) 50.9 25.8 56.5 69.2 27.0 14.8 8.3
ShareLockdefault(Llama-38B) 49.4 22.9 48.2 62.7 22.8 15.8 9.0
tuning. Incontrast,frozendecoder-basedrepresentationsconsistentlyoutperformtheirBERT-based
counterparts,withimprovementsrangingfrom40%to450%. Thisdemonstratestheexpressiveness
andhighinformationcontentofstrongLLMrepresentationsobtainedthroughtext-onlypretraining.
7 CONCLUSION
WeintroduceViTeRB,abenchmarkforevaluatingthevisualcapabilitiesandalignmentoflanguage
models. With it, we show that LLM quality, measured by MMLU, correlates with visual under-
standing,anddecoder-basedLLMsexcelinextractingvisuallyinformedrepresentations. Building
on these insights, we propose ShareLock, a simple CLIP-like VLM that leverages the large-scale
pretrainingandinternalknowledgeoffrozenLLMs. Ourmethodachievesstrongperformancesand
requires fewer image-caption pairs than models like CLIP or LiT for similar performances. Com-
binedwithitsextremelyfasttrainingtime,thisworkhighlightsthepotentialoffrozendecoder-only
LLMsforvision-languagetasks.
8 ACKNOWLEDGEMENTS
We acknowledge the ELLIS Unit Amsterdam for providing funding for a research visit to Copen-
hagen. We thanks SURF for providing GPU cluster access and support during this project. This
workwassupportedinpartbythePioneerCentreforAI,DNRFgrantnumberP1.
11Preprint.
REFERENCES
MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes,ByronDavid,Chelsea
Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho, Jasmine Hsu,
JulianIbarz,BrianIchter,AlexIrpan,EricJang,RosarioMJaureguiRuano,KyleJeffrey,Sally
Jesmonth,NikhilJayantJoshi,RyanC.Julian,DmitryKalashnikov,YuhengKuang,Kuang-Huei
Lee,SergeyLevine,YaoLu,LindaLuu,CarolinaParada,PeterPastor,JornellQuiambao,Kan-
ishkaRao, JarekRettinghouse, DiegoMReyes, PierreSermanet, NicolasSievers, ClaytonTan,
Alexander Toshev, Vincent Vanhoucke, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan
Yan. Doasican,notasisay: Groundinglanguageinroboticaffordances. InCoRL,2022.
SamBowman. Eightthingstoknowaboutlargelanguagemodels. 2023.
SoravitChangpinyo,PiyushSharma,NanDing,andRaduSoricut. Conceptual12M:Pushingweb-
scaleimage-textpre-trainingtorecognizelong-tailvisualconcepts. InCVPR,2021.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and
C.LawrenceZitnick. Microsoftcococaptions: Datacollectionandevaluationserver,2015.
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li,
Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica.
Chatbotarena: Anopenplatformforevaluatingllmsbyhumanpreference,2024.
Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, and Jing Shao. Democratizing contrastive
language-imagepre-training: Aclipbenchmarkofdata,model,andsupervision,2022.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. InNAACL,2019.
Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,
AyzaanWahid,JonathanTompson,QuanVuong,TianheYu,WenlongHuang,YevgenChebotar,
PierreSermanet, DanielDuckworth, SergeyLevine, VincentVanhoucke, KarolHausman, Marc
Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied
multimodallanguagemodel. 2023.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels.
arXivpreprint: 2407.21783,2024.
Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training
withlanguagerewrites. InNeurIPS,2023.
AlexFang,GabrielIlharco,MitchellWortsman,YuWan,VaishaalShankar,AchalDave,andLud-
wigSchmidt.Datadeterminesdistributionalrobustnessincontrastivelanguageimagepre-training
(clip). InICML,2022.
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao
Nguyen,RyanMarten,MitchellWortsman,DhrubaGhosh,JieyuZhang,EyalOrgad,RahimEn-
tezari,GiannisDaras,SarahPratt,VivekRamanujan,YonatanBitton,KalyaniMarathe,Stephen
Mussmann,RichardVencu,MehdiCherti,RanjayKrishna,PangWeiKoh,OlgaSaukh,Alexan-
der Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh,
AlexDimakis,JeniaJitsev,YairCarmon,VaishaalShankar,andLudwigSchmidt. Datacomp: In
searchofthenextgenerationofmultimodaldatasets. NeurIPS,2023.
Team Gemma, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya
Pathak,LaurentSifre,MorganeRivie`re,MihirSanjayKale,JulietteLove,etal. Gemma: Open
modelsbasedongeminiresearchandtechnology. arXivpreprint: 2403.08295,2024.
SherzodHakimovandDavidSchlangen.Imagesinlanguagespace:Exploringthesuitabilityoflarge
languagemodelsforvision&languagetasks. InFindingsoftheAssociationforComputational
Linguistics: ACL2023,2023.
DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,andJacob
Steinhardt. Measuringmassivemultitasklanguageunderstanding. InICLR,2021.
12Preprint.
Cheng-YuHsieh,JieyuZhang,ZixianMa,AniruddhaKembhavi,andRanjayKrishna. Sugarcrepe:
Fixing hackable benchmarks for vision-language compositionality. In NeurIPS - Datasets and
BenchmarksTrack,2023.
Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation
hypothesis. ICML,2024.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducinginternalcovariateshift. InICML,2015.
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan
Sung, ZhenLi, andTomDuerig. Scalingupvisualandvision-languagerepresentationlearning
withnoisytextsupervision. InICML.
Siddharth Joshi, Arnav Jain, Ali Payani, and Baharan Mirzasoleiman. Data-efficient contrastive
multi-modallearning: Prioritizingdataqualityoverquantity. AISTATS,2024.
Zaid Khan and Yun Fu. Contrastive alignment of vision to language through parameter-efficient
transferlearning. InTheEleventhInternationalConferenceonLearningRepresentations,2023.
DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. CoRR,2014.
LAIONAI.Clipbenchmark.https://github.com/LAION-AI/CLIP_benchmark,2022.
Christoph H. Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object
classesbybetween-classattributetransfer. CVPR,2009.
Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catan-
zaro, andWeiPing. Nv-embed: Improvedtechniquesfortrainingllmsasgeneralistembedding
models,2024.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. InICML,2023.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,
2023.
Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather, and
AriMorcos. Sieve: Multimodaldatasetpruningusingimagecaptioningmodels. CVPR,2024.
S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of
aircraft. 2013.
SachitMenonandCarlVondrick. Visualclassificationviadescriptionfromlargelanguagemodels.
InICLR,2023.
Microsoft. Phi-3technicalreport: Ahighlycapablelanguagemodellocallyonyourphone,2024.
JianmoNi,GustavoHernandezAbrego,NoahConstant,JiMa,KeithHall,DanielCer,andYinfei
Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In ACL,
2022.
Antonio Norelli, Marco Fumero, Valentino Maiorca, Luca Moschella, Emanuele Rodola, and
Francesco Locatello. Asif: Coupled data turns unimodal models to multimodal without train-
ing. NeurIPS,2023.
MaximeOquab,Timothe´eDarcet,TheoMoutakanni,HuyV.Vo,MarcSzafraniec,VasilKhalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,RussellHowes,Po-Yao
Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran,
NicolasBallas,GabrielSynnaeve,IshanMisra,HerveJegou,JulienMairal,PatrickLabatut,Ar-
mandJoulin,andPiotrBojanowski. Dinov2:Learningrobustvisualfeatureswithoutsupervision.
TMLR,2023.
ShubhamParashar,ZhiqiuLin,TianLiu,XiangjueDong,YananLi,DevaRamanan,JamesCaver-
lee,andShuKong. Theneglectedtailsinvision-languagemodels. InCVPR,2024.
13Preprint.
SarahPratt,RosanneLiu,andAliFarhadi. Whatdoesaplatypuslooklike? generatingcustomized
promptsforzero-shotimageclassification. ICCV,2022.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision. In ICML,
2021.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. JournalofMachineLearningResearch,2020.
OindrilaSaha,GrantVanHorn,andSubhransuMaji. Improvedzero-shotclassificationbyadapting
vlmswithtextdescriptions. InCVPR,2024.
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,
AarushKatta,TheoCoombes,JeniaJitsev,andAranKomatsuzaki. Laion-400m:Opendatasetof
clip-filtered400millionimage-textpairs. DataCentricAIWorkshopNeurIPS,2021.
PiyushSharma,NanDing,SebastianGoodman,andRaduSoricut. Conceptualcaptions:Acleaned,
hypernymed,imagealt-textdatasetforautomaticimagecaptioning. InACL,2018.
PratyushaSharma, TamarRottShaham, ManelBaradad, StephanieFu, AdrianRodriguez-Munoz,
ShivamDuggal,PhillipIsola,andAntonioTorralba. Avisioncheck-upforlanguagemodels. In
CVPR,2024.
JacobMitchellSpringer,SuhasKotha,DanielFried,GrahamNeubig,andAditiRaghunathan. Rep-
etitionimproveslanguagemodelembeddings. arXivpreprint: 2402.15449,2024.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JMLR,2014.
YiTay. Anewopensourceflan20bwithul2,2024.
Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and
Candace Ross. Winoground: Probing vision and language models for visio-linguistic composi-
tionality. InCVPR,2022.
Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi,
Samuel Albanie, and Matthias Bethge. No ”zero-shot” without exponential data: Pretraining
conceptfrequencydeterminesmultimodalmodelperformance. 2024.
C.Wah,S.Branson,P.Welinder,P.Perona,andS.Belongie. Caltech-ucsdbirds-200-2011. Techni-
calreport,CaliforniaInstituteofTechnology,2011.
PengWang, ShuaiBai, SinanTan, ShijieWang, ZhihaoFan, JinzeBai, KeqinChen, XuejingLiu,
JialinWang,WenbinGe,YangFan,KaiDang,MengfeiDu,XuanchengRen,RuiMen,Dayiheng
Liu,ChangZhou,JingrenZhou,andJunyangLin.Qwen2-vl:Enhancingvision-languagemodel’s
perceptionoftheworldatanyresolution. arXivpreprint: 2409.12191,2024.
WenhaiWang, ZheChen, XiaokangChen, JiannanWu, XizhouZhu, GangZeng, PingLuo, Tong
Lu,JieZhou,YuQiao,andJifengDai. VisionLLM:Largelanguagemodelisalsoanopen-ended
decoderforvision-centrictasks. InNeurIPS,2023.
Wayve. Lingo-1: Exploringnaturallanguageforautonomousdriving,2023.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,
Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language
models. InNeurIPS,2022.
YongqinXian, ChristophLampert, BerntSchiele, andZeynepAkata. Zero-shotlearning-acom-
prehensiveevaluationofthegood,thebadandtheugly. PAMI,2017.
XTunerContributors. Xtuner: Atoolkitforefficientlyfine-tuningllm. https://github.com/
InternLM/xtuner,2023.
14Preprint.
Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark
Yatskar. Language in a bottle: Language model guided concept bottlenecks for interpretable
imageclassification. CVPR,2022.
HaichaoYu,YuTian,SateeshKumar,LinjieYang,andHengWang. Thedevilisinthedetails: A
deepdiveintotherabbitholeofdatafiltering,2023.
XiaohuaZhai,XiaoWang,BasilMustafa,AndreasSteiner,DanielKeysers,AlexanderKolesnikov,
andLucasBeyer. Lit: Zero-shottransferwithlocked-imagetexttuning. CVPR,2022.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.
Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on
NeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2023.
15Preprint.
A APPENDIX
A.1 REPRODUCIBILITYSTATEMENT
Weacknowledgeandemphasizetheimportanceofreproducibilityinourworkandtakeactivemea-
sures to facilitate reproducibility efforts. Besides providing comprehensive documentation of our
methodsthroughoutthemainpaper,withadditionaldetailsinthesupplementarymaterials,wewill
publishsourcecodefortheproposedShareLockmodel.
A.2 TEXTUALCLASSREPRESENTATIONSFORVISUALTEXTREPRESENTATION
BENCHMARK
More details about the characteristics and acquisition of these class representations are provided
in Section A.2 of the appendix. Besides the template-based targets proposed by Radford et al.
(2021)thatsolelysubstitutetherespectiveclassnames,wegeneratemorecomprehensiveauxiliary
informationaboutclasses(e.g.,visualdescriptions)usingtheinstruction-tunedversionoftheLlama-
3-8Bmodelandacquirehuman-curatedinformationfromWikipedia(detailsprovidedinA.2).
Class representations are essential for facilitating the knowledge transfer between classes in the
traditionaldefinitionofzero-shotlearning.Comparedtoattributesorotherformsofclasssemantics,
language-based class representations are more conveniently accessible at various scales and may
come in diverse manifestations. The advent of LLMs adds further possibilities for generating and
obtaining such auxiliary information. The following paragraphs specify the respective properties
andacquisitionprocess. Here,allLLM-basedclassrepresentationsaregeneratedusingtheinstruct-
tunedversionofLLama-38B.
ClassNames. Asetof80human-engineeredprompttemplatesinthestyleof"a photo of a
<class name>"areadoptedfromRadfordetal.(2021).
Description. ThistypeofclassrepresentationisgeneratedbytaskinganLLMtogenerateshort,
one-sentence descriptions of how a given class looks like. Multiple descriptions are generated for
eachclassbyslightlyvaryingtheLLMpromptandutilizingdifferentseedsasaformofaugmenta-
tion.
Wikipedia Page. Being a comprehensive and mostly factually correct source of information,
Wikipediaconstitutesaninterestingsourceofauxiliaryinformationinthecontextofzero-shotclas-
sification.Toobtainclass-articlecorrespondences,classnamesareautomaticallymatchedwithpage
names, after which additional manual quality checks are performed. Nonetheless, an ideal match
doesnotalwaysexistduetohighclassspecificityorgenerality,inwhichcasesuperordinatearticles
areconsideredortemplate-basedfallbacksareemployed.
LLM-basedWikipediaStyleArticles. Despitebeingspecificallypromptedforarticlesmimick-
ingWikipedia, theLlama-3-generatedtextstendtoshowsignificantdifferencesinstylecompared
totheirrealcounterparts.
As the lengthy nature of Wikipedia(-style) articles might dilute the information content captured
bythelanguageembeddings,thetextsaresplitintoindividualsentences,whichareusedastargets
during training. For all types of class representations, predictions are made by aggregating class
scoresthroughaveragingoverallindividualclass-specifictexts.
A.3 PROJECTIONNETWORKARCHITECTURE
Themulti-layerperceptron(MLP)projectionnetworksofShareLockasintroducedinSection3are
conceivably simple. As these are the only unfrozen and tunable parts of the model architecture
and thus responsible for aligning vision and language inputs, they are of particular significance to
aptlyprocessandtransformtheinputs. FollowingZhaietal.(2022),notransformationtothevision
inputsisappliedforanyofthearchitectures. Withahiddensizeof4096andfourlayers,theMLP
processingthelanguagefeaturescomprisesapproximately53Mparameters.
16Preprint.
InadditiontothestraightforwardMLP-basednetworks,alsomoresophisticatedTransformer-based
architecturesareinspiredbyrecentworks. FirstintroducedaspartoftheBLIP-2model(Lietal.,
2023),theQ-FormerisalightweightTransformer-basedmodelthatextractsfeaturesfromaninput
modalityusingcross-attentionwithlearnablequerytokens. Similarly,albeitintroducedinadiffer-
entcontext,NV-Embed(Leeetal.,2024)usesalatentattentionlayertopoollanguagetokensand
receiveaglobalembedding. Slightadjustmentsaremadetobothbaselinearchitecturestobettersuit
late-fusionvision-languagemodeling. Thehyperparameterswereselectedbasedontheimplemen-
tationdetails suggestedinthe originalpublications andtoapproximately matchtheMLP baseline
inlearnableparametercount. BoththeQ-FormerandtheNV-Embedprojectionnetworkshaveato-
kendimensionof1024intheTransformerpartsofthemodels,eightlearnablequeries(Q-Former),
and key/values (NV-Embed). Whereas the Q-Former consists of 3 blocks and 4 attention heads,
NV-Embedcomprisesatotaloffourlayerswitheightcross-attentionheadseach.
The choice of projection network architecture is ablated in Table 9. While no single architecture
consistentlyscoresbest,theMLP-basedShareLockconfigurationperformscompetitivelycompared
to NV-Embed and QFormer throughout the evaluation cases. Additionally, Transformer-based ar-
chitecturesentailincreasedcomputationalcomplexityduetothemoreevolvedattentionmechanism
andprocessingofmoretokens,makingMLPsanattractivechoicefromanefficiencyperspectiveas
well. These results suggest that the additional information contained across all tokens of an input
isnotsignificantlymoreadjuvantcomparedtosolelyconsideringthelasttokenrepresentationasis
donewiththeMLP.
Table9: AblationoftheprojectionnetworkarchitecturestunedaspartofShareLocktraining.
SimpleMLPsperformcompetitivelycomparedtomoreadvancedTransformer-basedarchitectures.
Avg.ClassificationScores Avg.RetrievalScore WinoGround
Architecture
Robustness Fine-Grained T →I I →T Text Image Group
NV-Embed 41.9 20.3 49.5 65.8 21.5 10.0 6.0
QFormer 48.3 26.8 49.4 65.2 24.5 14.0 8.5
ShareLock(MLP) 49.4 22.9 48.2 62.7 22.8 15.8 9.0
A.4 SUPPLEMENTARYQUANTITATIVERESULTS
The following tables include additional results and analyses that were omitted in the main body
of the paper due to space constraints. These supplementary results offer extended insights from
additionalmodelvariantsandfurtherbuttresspreviouslydrawnconclusions.
Table10: Extendedresultsforzero-shotclassificationonImageNetvariants.
TrainingDataset TestDataset
Model Average
Size Name IN-1k IN-V2 IN-R IN-A INSketch ObjectNet
LiT 83k COCOCaptions 23.3 20.8 34.4 21.1 18.4 29.2 24.5
ASIF 83k COCOCaptions 9.4 8.7 14.4 8.8 6.9 16.1 10.7
ShareLock 83k COCOCaptions 32.2 28.6 36.6 22.8 22.4 30.4 28.8
LiT 563k CC3MSubset 41.7 37.5 59.2 44.4 32.4 40.7 42.6
ASIF 563k CC3MSubset 21.6 20.5 27.7 24.4 14.9 21.5 21.8
ShareLock 563k CC3MSubset 50.5 45.8 60.5 47.0 36.9 41.1 47.0
CLIP 2.8M CC3M 16.0 13.2 17.6 3.6 6.4 8.2 10.8
LiT 2.8M CC3M 44.1 39.3 62.7 45.6 34.8 43.3 45.0
ShareLock 2.8M CC3M 52.1 47.1 64.1 50.9 39.0 43.1 49.4
DataComp-LAION 3.84M CommonPool-S 3.0 2.7 4.4 1.5 1.3 3.7 2.8
CLIP 12M CC12M 41.6 35.4 52.6 10.7 28.8 24.0 32.2
LiT 8.5M CC12M 56.2 49.9 70.3 52.8 43.9 47.8 53.5
ShareLock 8.5M CC12M 59.1 53.2 68.8 53.4 44.5 46.7 54.3
DataComp 12.8M CommonPool-S 2.7 2.3 4.1 1.4 1.1 3.7 2.5
DataComp 128M CommonPool-M 17.5 14.4 19.8 3.9 9.5 15.8 13.5
DataComp-LAION 38.4M CommonPool-M 23.0 18.9 28.0 4.3 15.1 17.7 17.8
DataComp-LAION 384M CommonPool-L 55.3 47.9 65.0 20.2 43.2 46.5 46.3
CLIP 400M Proprietary 68.4 61.8 77.6 50.1 48.2 55.4 60.2
DataComp 1.28B CommonPool-L 45.9 39.2 52.7 15.9 34.5 41.1 38.2
DataComp-LAION 3.84B CommonPool-XL 75.4 68.5 87.0 57.0 63.5 68.5 70.0
DataComp 12.8B CommonPool-XL 72.3 65.1 85.9 56.4 61.1 70.6 68.6
17Preprint.
Table11: Extendedresultsforzero-shotclassificationonfine-graineddatasets.
TrainingDataset TestDataset
Model Average
Size Name Aircraft Pets Flowers Cars EuroSAT
LiT 83k COCOCaptions 1.6 28.8 7.7 1.8 21.9 12.3
ASIF 83k COCOCaptions 2.8 7.0 1.6 1.3 21.5 6.8
ShareLock 83k COCOCaptions 3.0 20.6 10.6 9.2 25.1 13.7
LiT 563k CC3MSubset 1.1 22.8 27.5 4.1 25.5 16.2
ASIF 563k CC3MSubset 2.1 11.7 6.4 2.3 19.5 8.4
ShareLock 563k CC3MSubset 8.4 38.3 33.3 5.4 29.4 23.0
CLIP 2.8M CC3M 1.4 13.0 10.8 0.8 12.9 7.8
LiT 2.8M CC3M 2.1 28.5 35.9 3.0 34.4 20.8
ShareLock 2.8M CC3M 6.5 43.1 32.8 4.4 27.9 22.9
DataComp-LAION 3.84M CommonPool-S 1.4 4.0 1.8 1.6 15.8 4.9
CLIP 12M CC12M 2.5 64.2 36.7 24.1 20.9 29.7
LiT 8.5M CC12M 5.0 74.4 48.2 13.2 35.3 35.2
ShareLock 8.5M CC12M 8.3 66.6 48.8 11.5 40.7 36.7
DataComp 12.8M CommonPool-S 0.8 4.4 2.3 1.4 14.9 4.8
DataComp-LAION 38.4M CommonPool-M 1.7 29.9 22.4 22.0 18.8 18.9
DataComp 128M CommonPool-M 1.3 16.8 8.1 13.3 25.4 13.0
DataComp-LAION 384M CommonPool-L 7.1 77.8 53.3 67.7 41.0 49.4
CLIP 400M Proprietary 24.4 89.0 71.2 64.7 55.9 61.0
DataComp 1.28B CommonPool-L 3.3 56.2 39.4 60.5 33.4 38.6
DataComp-LAION 3.84B CommonPool-XL 93.1 77.1 28.7 89.2 73.8 72.4
DataComp 12.8B CommonPool-XL 19.5 90.6 71.6 89.3 68.9 68.0
Table12: Extendedresultsforimageandtextretrieval.
TrainingDataset Flickr8k Flickr30k MSCOCO Average
Model
Size Name T →I I→T T →I I→T T →I I→T T →I I→T
LiT 83k COCOCaptions 57.5 77.1 61.3 80.6 50.7 69.1 56.5 75.6
ASIF 83k COCOCaptions 10.4 20.6 12.1 24.9 8.9 17.1 10.4 20.9
ShareLock 83k COCOCaptions 50.5 69.6 56.9 78.4 27.0 41.9 50.2 69.5
LiT 563k CC3MSubset 51.2 67.2 60.7 74.4 28.4 45.8 46.8 62.5
ASIF 563k CC3MSubset 10.3 20.3 15.9 29.6 5.7 12.1 10.6 20.7
ShareLock 563k CC3MSubset 49.9 64.6 57.9 73.9 29.8 45.9 44.9 60.1
CLIP 2.8M CC3M 43.5 56.9 40.4 54.7 25.3 30.9 36.4 47.5
LiT 2.8M CC3M 60.1 76.6 69.3 81.4 35.9 53.6 55.1 70.5
ShareLock 2.8M CC3M 54.9 70.1 60.1 74.2 29.5 43.9 48.2 62.7
DataComp-LAION 3.84M CommonPool-S 7.8 11.4 6.7 9.1 3.6 4.4 6.1 8.3
CLIP 12M CC12M 73.1 84.5 73.9 86.3 51.0 65.4 66.0 78.7
LiT 8.5M CC12M 60.0 72.8 69.1 82.1 36.5 53.4 55.2 69.4
ShareLock 8.5M CC12M 55.0 73.0 67.1 81.7 32.7 50.1 51.6 68.3
DataComp 12.8M CommonPool-S 8.1 12.3 6.9 9.9 3.5 5.7 6.2 9.3
DataComp-LAION 38.4M CommonPool-M 39.4 52.6 39.2 52.4 26.0 35.9 34.9 47.0
DataComp 128M CommonPool-M 30.7 42.3 31.4 40.7 19.4 30.0 27.2 37.7
DataComp-LAION 384M CommonPool-L 78.1 89.0 81.0 90.7 57.6 71.7 72.2 83.8
CLIP 400M Propriatary 82.9 91.4 85.6 96.2 58.4 76.8 75.6 88.1
DataComp 1.28B CommonPool-L 64.3 78.6 69.9 81.4 45.7 60.2 60.0 73.4
DataComp-LAION 3.84B CommonPool-XL 90.9 96.1 92.9 99.0 71.4 84.6 85.1 93.2
DataComp 12.8B CommonPool-XL 84.6 92.1 86.4 94.6 63.1 77.1 78.0 87.9
A.5 SUPPLEMENTARYQUALITATIVERESULTS
Figure6providesadditionalqualitativeinsightsintotheretrievalabilityofCLIP,LiT,andShareLock
modelstrainedonCC3M.
18Preprint.
Table13: Extendedresultsforcompositionalreasoning.
TrainingDataset Winoground SugarCrepe
Model
Size Name Text Image Group Replace Swap Add
Human 89.5 88.5 85.5 99.6 99.5 99.0
Chance 25.0 25.0 16.7 50.0 50.0 50.0
LiT 83k COCOCaptions 25.0 5.8 2.8 78.7 65.1 75.7
ASIF 83k COCOCaptions 18.8 9.0 5.3 49.1 44.3 46.8
ShareLock 83k COCOCaptions 21.0 11.8 6.5 70.5 55.4 68.4
LiT 563k CC3MSubset 24.3 8.3 5.5 69.5 57.7 67.2
ASIF 563k CC3MSubset 18.3 13.3 7.3 58.7 52.1 56.8
ShareLock 563k CC3MSubset 20.0 13.8 6.8 62.4 50.6 60.8
CLIP 2.8M CC3M 21.3 9.5 6.0 67.0 56.6 63.3
LiT 2.8M CC3M 23.8 6.0 4.5 74.0 62.4 73.6
ShareLock 2.8M CC3M 22.8 15.8 9.0 63.0 54.0 62.3
DataComp-LAION 3.84M CommonPool-S 19.3 12.0 7.5 56.5 53.6 58.1
CLIP 12M CC12M 22.3 9.5 5.3 77.5 61.9 73.5
LiT 8.5M CC12M 24.3 6.5 4.8 74.1 62.0 77.6
ShareLock 8.5M CC12M 26.3 12.8 5.3 66.3 53.1 65.5
DataComp 12.8M CommonPool-S 17.3 5.5 2.3 57.7 51.7 56.4
DataComp-LAION 38.4M CommonPool-M 25.0 8.3 6.3 69.1 56.7 66.2
DataComp 128M CommonPool-M 24.3 4.5 3.0 65.5 53.4 65.5
DataComp-LAION 384M CommonPool-L 27.0 9.5 7.0 79.8 62.8 79.3
CLIP 400M Propriatary 30.8 10.8 8.3 80.0 62.7 73.0
DataComp 1.28B CommonPool-L 24.0 6.5 4.3 73.4 58.7 75.2
DataComp-LAION 3.84B CommonPool-XL 34.0 11.8 10.0 79.7 58.7 81.4
DataComp 12.8B CommonPool-XL 28.8 7.5 6.0 84.3 66.7 87.5
19Preprint.
CLIP LiT ShareLock
AAA ppphhhoootttooo ooofff aaa bbbaaannnaaannnaaa...
Sim: 0.62 Sim: 0.53 Sim: 0.52 Sim: 0.67 Sim: 0.67 Sim: 0.66 Sim: 0.74 Sim: 0.74 Sim: 0.72
AAA pppeeerrrsssooonnn hhhooorrrssseeebbbaaaccckkk
rrriiidddiiinnnggg...
Sim: 0.45 Sim: 0.44 Sim: 0.44 Sim: 0.59 Sim: 0.56 Sim: 0.51 Sim: 0.65 Sim: 0.65 Sim: 0.64
AAA llliiiggghhhttthhhooouuussseee cccaaauuuggghhhttt iiinnn
hhheeeaaavvvyyy ssseeeaaasss...
Sim: 0.47 Sim: 0.46 Sim: 0.46 Sim: 0.68 Sim: 0.66 Sim: 0.64 Sim: 0.69 Sim: 0.67 Sim: 0.65
AAA ppphhhoootttooo ooofff aaa BBBMMMWWW...
Sim: 0.48 Sim: 0.47 Sim: 0.46 Sim: 0.47 Sim: 0.47 Sim: 0.46 Sim: 0.56 Sim: 0.56 Sim: 0.55
AAA cccaaarrr pppaaarrrkkkeeeddd ooonnn ttthhheee
ssstttrrreeeeeettt...
Sim: 0.49 Sim: 0.48 Sim: 0.47 Sim: 0.46 Sim: 0.46 Sim: 0.45 Sim: 0.52 Sim: 0.50 Sim: 0.49
AAA lllooonnneeelllyyy bbbeeennnccchhh iiinnn aaa qqquuuiiieeettt
pppaaarrrkkk
Sim: 0.50 Sim: 0.45 Sim: 0.45 Sim: 0.71 Sim: 0.67 Sim: 0.65 Sim: 0.75 Sim: 0.70 Sim: 0.69
AAA hhheeellliiicccooopppttteeerrr lllaaannndddiiinnnggg ooonnn aaa
bbbuuuiiillldddiiinnnggg...
Sim: 0.46 Sim: 0.45 Sim: 0.45 Sim: 0.52 Sim: 0.51 Sim: 0.51 Sim: 0.56 Sim: 0.54 Sim: 0.54
AAA mmmaaannn fffiiissshhhiiinnnggg bbbyyy aaa lllaaakkkeee
wwwiiittthhh mmmooouuunnntttaaaiiinnnsss iiinnn ttthhheee
bbbaaaccckkkgggrrrooouuunnnddd
Sim: 0.42 Sim: 0.41 Sim: 0.40 Sim: 0.58 Sim: 0.54 Sim: 0.47 Sim: 0.53 Sim: 0.49 Sim: 0.47
TTTwwwooo dddiiiffffffeeerrreeennnttt bbbrrreeeeeedddsss ooofff
dddooogggsss sssiiittttttiiinnnggg nnneeexxxttt tttooo eeeaaaccchhh
ooottthhheeerrr...
Sim: 0.45 Sim: 0.45 Sim: 0.44 Sim: 0.51 Sim: 0.48 Sim: 0.44 Sim: 0.59 Sim: 0.51 Sim: 0.49
AAA rrrooouuunnnddd ooobbbjjjeeecccttt...
Sim: 0.48 Sim: 0.48 Sim: 0.48 Sim: 0.30 Sim: 0.30 Sim: 0.30 Sim: 0.33 Sim: 0.31 Sim: 0.31
AAA ppphhhoootttooo ttthhhaaattt iiinnncccllluuudddeeesss aaa
mmmiiirrrrrrooorrr...
Sim: 0.46 Sim: 0.46 Sim: 0.45 Sim: 0.67 Sim: 0.60 Sim: 0.54 Sim: 0.57 Sim: 0.47 Sim: 0.46
AAA vvviiinnntttaaagggeee---lllooooookkkiiinnnggg ppphhhoootttooo...
Sim: 0.47 Sim: 0.46 Sim: 0.45 Sim: 0.41 Sim: 0.40 Sim: 0.38 Sim: 0.51 Sim: 0.50 Sim: 0.49
AAA ppphhhoootttooo ooofff aaa bbbrrroookkkeeennn
ooobbbjjjeeecccttt...AAA fffuuunnnnnnyyy---lllooooookkkiiinnnggg
hhhooottt aaaiiirrr bbbaaalllllloooooonnn...
Sim: 0.53 Sim: 0.53 Sim: 0.52 Sim: 0.61 Sim: 0.61 Sim: 0.61 Sim: 0.36 Sim: 0.35 Sim: 0.35
Figure6: Qualitativecomparisonontext-to-imageretrieval(ImageNet-1k).
20