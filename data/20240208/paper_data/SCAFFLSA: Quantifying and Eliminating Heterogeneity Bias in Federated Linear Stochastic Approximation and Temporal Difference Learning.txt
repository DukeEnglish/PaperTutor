SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated
Linear Stochastic Approximation and Temporal Difference Learning
PaulMangold*1 SergeySamsonov*2 SafwanLabbi1 IlyaLevin2 RedaAlami3 AlexeyNaumov2
EricMoulines14
Abstract lossminimizationtasks. Thesetasksoftenbenefitfroma
particularstructure(e.g. finite-sum,overparameterization),
Inthispaper,weperformanon-asymptoticanaly-
thatallowtheuseofad-hoctechniquestohandlegradients’
sisofthefederatedlinearstochasticapproxima-
stochasticity.Inthisstudy,thefocusisshiftedtothestochas-
tion(FedLSA)algorithm. Weexplicitlyquantify
ticapproximation(SA)paradigm(Robbins&Monro,1951).
the bias introduced by local training with het-
Unlikestochasticgradientscenarios,thestochasticoracles
erogeneous agents, and investigate the sample
inSAdonotoriginatefromthegradientofalossfunction.
complexity of the algorithm. We show that the
Consequently,analyzingSAtechniquesrequiresanovelset communication complexity of FedLSA scales
ofanalyticaltools. Thesetools,despitetheirimportancein
polynomiallywiththedesiredprecisionϵ,which
federatedsettings,haveyettobeexplored.
limits the benefits of federation. To overcome
this,weproposeSCAFFLSA,anovelvariantof Inthismanuscript,wediveintothefieldoffederatedlinear
FedLSA,thatusescontrolvariatestocorrectthe stochasticapproximation(federatedLSA)withaparticular
biasoflocaltraining,andproveitsconvergence focusontheheterogeneityofparticipatingagents. Hetero-
withoutassumptionsonstatisticalheterogeneity. geneityposesamajorchallenge:ifnotcarefullymanaged,it
Weapplytheproposedmethodologytofederated canseverelyaffecttheperformanceoffederatedalgorithms,
temporaldifferencelearningwithlinearfunction limitingthepracticalutilityofsuchmethodsinreal-world
approximation, and analyze the corresponding applications. ThemaingoaloffederatedLSAistosolve
complexityimprovements. alinearsystemofequations, where(i)thesystemmatrix
andthecorrespondingtargetareonlyaccessibleviastochas-
tic oracles, and (ii) these oracles are distributed over an
1.Introduction ensembleofheterogeneousagents. Thisproblemscanbe
solvedusingtheFedLSA procedure,whichperformsLSA
FederatedLearning(FL)(McMahanetal.,2017;Konecˇny`
locally,withoccasionalconsensussteps. Animportantap-
etal.,2016)representsaparadigmshiftinmachinelearning,
plicationoffederatedLSAisfederatedpolicyevaluation
leveragingdecentralizeddatasetsacrossdifferentdevicesor
in reinforcement learning. This is usually carried out us-
agents, whileensuringthatthedataremainsatitssource.
ingtemporaldifferencelearning(TD);seeSutton(1988).
Thisfederatedapproachtomodeltraininginvolvestheit-
IntheTDparadigm,thestochasticoraclesinvolvedinthe
erative refinement of a global model through a sequence
processarecharacterizedbyhighvariance,apropertythat
ofinteractionswithlocaloracles. Theseoracles,working
highlightsthepotentialbenefitsofpromotingcooperation
with individual agent datasets, provide updates to a cen-
betweenagentstospeedupevaluation. Consequently,itis
tralizedserver. Theserverthenintegratestheseupdatesto
thestochasticnatureoftheseoraclesthatoftenprovestobe
incrementallyimproveandevolvetheglobalmodel.
themostcriticalconstraintinsuchenvironments.
The predominant research in federated learning (FL) fo-
Inparticular,thereisanotablestrandofresearch,initiated
cusesonstochasticgradientmethodstailoredtodistributed
byKarimireddyetal.(2020),thatdealswithheterogeneity
*Equalcontribution 1E´colePolytechnique,Palaiseau,France usingcontrolvariates. However,theeffectivenessofthese
2HSEUniversity,Russia3TechnologyInnovationInstitute,9639 approachesremainsunclearwhenthelocaloraclehashigh
Masdar City, Abu Dhabi, United Arab Emirates 4Mohamed variance; seeMishchenkoetal.(2022). Sincesuchnoisy
bin Zayed University of Artificial Intelligence, Abu Dhabi, environmentsareparticularlyrelevantforTDlearning,this
United Arab Emirates. Correspondence to: Paul Mangold
raisesacriticalquestion: Shouldtheusageofcontrolvari-
<paul.mangold@polytechnique.edu>,SergeySamsonov<svsam-
atesprovablyimprovethecommunicationcomplexityofthe
sonov@hse.ru>.
FederatedSAprocedure?
1
4202
beF
6
]LM.tats[
1v41140.2042:viXraSCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Inthispaper,weattempttoprovideapositiveanswertothis Overall,astrongfocushasbeenputonthecelebratedFeder-
questioninthespecificsettingoffederatedLSAwithhet- atedAveraging(FedAvg)algorithm(McMahanetal.,2017).
erogeneousagents. Ourprimarycontributionisthreefold: FedAvg aimstoreducecommunicationthroughlocaltrain-
ing,whichcauseslocaldriftwhenagentsareheterogeneous
• Inspired by the study of Wang et al. (2022), we in- (Zhaoetal.,2018). Sampleandcommunicationcomplexity
vestigatethenon-asymptoticbehavioroftheFedLSA ofFedAvg havebeenstudiedunderdifferentassumptions,
algorithm. Ouranalysisdelineatestheexactrelation- withhomogeneousagents(Lietal.,2020;Haddadpour&
ship between the mean squared error (MSE) of the Mahdavi, 2019) and heterogeneous agents (Khaled et al.,
FedLSA procedure and three key factors: the num- 2020;Koloskovaetal.,2020). Itwasalsoshowntoyield
beroflocalupdates, themagnitudeofthelocalstep linearspeed-upinthenumberofagentswhengradientare
size,andthenumberofagents. Furthermore,wede- stochastic(Quetal.,2021). Inpractice, FedAvg exhibits
riveananalyticalformulationthatcapturestheintrinsic muchbetterperformancethanpredictedbymosttheoreti-
heterogeneitybias. calanalyseswhenagentsareheterogeneous(Collinsetal.,
2021;Reddietal.,2020). Thisphenomenonhasbeenstud-
• Tocorrecttheheterogeneitybias,weproposeandan- iedtheoreticallybyWangetal.(2022);Pateletal.(2023),
alyze SCAFFLSA, a variant of FedLSA that lever- whoproposednewmeasuresofheterogeneity,thataremore
agescontrolvariates,withtwodifferentcommunica- adaptedtoFedAvg. Ouranalysisoffederatedlinearstochas-
tionstrategies: first,therandomizedcommunication ticapproximationextendstheseideastothelinearstochastic
scheme, which is similar to the framework used in approximationsetting.
proximalfederatedmethods(seee.g. Mishchenkoetal.
AnotherlineofworkaimstocorrectFedAvg’sbiasbyin-
(2022)),andsecond,thedeterministic-temporalcom-
troducingcontrolvariates,thatcompensateforheterogene-
municationpattern,wherecommunicationissystem-
ity. ThiswasproposedbyKarimireddyetal.(2020),with
aticallyactivatedattheendofblocksofH localstep.
the Scaffold method, and was then studied by Gorbunov
Thisensurescompatibilitywithdifferentoperational
et al. (2021); Mitra et al. (2021), recovering the conver-
paradigmstypicaloffederatedlearningenvironments.
gence rate of gradient descent (with a single local step),
• We show how the proposed methodology can be ap- regardlessofheterogeneity. ItwaslatershownwithProx-
plied to federated policy evaluation problems in re- Skip (Mishchenko et al., 2022) that such methods indeed
inforcement learning in heterogeneous settings. We acceleratethetraining. However,andcontrarilytoScaffold,
focusonthesituationoffederatedTDlearningwithlin- theylosethelinearspeed-upinthenumberofagentswhen
earfunctionalapproximationandestablishtightfinite- gradientsarestochastic. Multipleothermethodswithaccel-
sampleupperboundsofthemeansquareerror,improv- eratedrateshavebeenproposed(Malinovskyetal.,2022;
ingoverexistinganalyses. Condatetal.,2022;Condat&Richta´rik,2022;Grudzien´
etal.,2023;Hu&Huang,2023),althoughalllosethelinear
speed-up. Inspired from these works, we propose a new
Thispaperisorganizedasfollows. Wefirstdiscussrelated
methodforfederatedlinearstochasticapproximation,with
workinSection2. WethenintroducefederatedLSAand
improvedratesatthecostofthelinearspeed-up.
TDinSection3,andanalyzeitinSection4. Section5then
introducesanovelstrategyformitigatingthebias. Finally, Single-agentandFederatedTD-learning.Temporaldiffer-
weillustrateourresultsnumericallyinSection6. ence(TD)learninghavealonghistoryinpolicyevaluation
(Sutton, 1988; Dann et al., 2014), with classical asymp-
Notationsanddefinitions. Intherestofthepaper,wewill
toticanalysesinthelinearfunctionapproximationsetting
usethefollowingnotations.FormatrixAwedenoteby∥A∥
(Tsitsiklis&VanRoy,1997;Suttonetal.,2009). Multiple
itsoperatornorm. SettingN forthenumberofcommuni-
cating clients, we use the notation E [a ]= 1 (cid:80)N a non-asymptoticanalysesofthemeansquarederror(MSE)
c c N c=1 c ofTD-typealgorithmshavebeenproposed(Bhandarietal.,
for the average over different clients. For the matrix
A=A⊤ ⪰0,A∈Rd×d andv√ectorx∈Rd wedefinethe 2018;Dalaletal.,2018;Patiletal.,2023;Lietal.,2023b;
Samsonovetal.,2023). Recently,significantattentionhas
correspondingnorm∥x∥ = x⊤Ax. Forthesequences
A
a andb wewritea ≲b ifthereexistaconstantc>0 beendrawntofederatedreinforcementlearning(Limetal.,
n n n n
2020;Qietal.,2021;Xie&Song,2023)andtoFederated
suchthata ≤cb forc>0.
n n
TDlearningwithlinearfunctionapproximation. Khodada-
dianetal.(2022);DalFabbroetal.(2023);Liu&Olshevsky
2.RelatedWork
(2023)analyseditwithverylittlecommunication,underthe
strongassumptionthatagentsareidentical. FederatedTD
FederatedLearning. Apartfromtheseminalworkfrom
wasstudiedwithheterogeneousagents,firstwithoutlocal
Doan(2020)onfederatedstochasticapproximation,most
training(Doan,2020),thenwithlocaltraining,butnolinear
oftheFLliteratureisdevotedtofederatedgradientmethods.
2SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
speed-up(Doanetal.,2019;Jinetal.,2022). Morerecently, Algorithm1FedLSA
Wang et al. (2023) analyzed federated TD with heteroge- Input: η >0,θ ∈Rd,T,N,H >0
0
neousagents,localtraining,andlinearspeed-upinnumber fort=0toT −1do
ofagentsinalow-heterogeneitysetting.Unfortunately,their Initializeθ =θ
t,0 t
analysissuffersfromunavoidablebias. Additionally,itsde- forc=1toN do
pendenceonnoiselevelscanbepessimistic,andrequires forh=1toH do
theservertoprojectaggregatediteratestoaballofunknown ReceiveZc andperformlocalupdate:
t,h
radius. In contrast, we give sharp bounds on the conver-
θ =θc −η(Ac(Zc )−bc(Zc )) (2)
genceoffederatedTD,withlinearspeed-up,andexplicit t,h t,h−1 t,h t,h
characterizationofthebias. Wealsoproposeastrategyto endfor
mitigatethisbias,allowingforextendedlocaltraining. endfor
Average: θ = 1 (cid:80)N θc (3)
t+1 N c=1 t,H
3.FederatedLSAandTDlearning endfor
Inthispaper,westudythefederatedlinearstochasticapprox-
imation problem, where N agents aim to collaboratively
Durmusetal.,2022)forLSAtoanalyzethemeansquared
solveasystemwiththefollowingfinitesumstructure
erroroftheestimatesofAlgorithm1. Forthispurpose,we
(cid:0)1 (cid:80)N A¯c(cid:1) θ = 1 (cid:80)N b¯c , (1) rewritelocalupdates(2)as
N c=1 ⋆ N c=1
θc −θc =(I−ηA(Zc ))(θc −θc)−ηεc(Zc ), (4)
whereforc ∈ [N],A¯c ∈ Rd×d,b¯c ∈ Rd,andweassume t,h ⋆ t,h t,h−1 ⋆ t,h
thesolutionθ ⋆tobeunique. Wesetthesystemmatrixand wherewehavesetb˜c(z)=bc(z)−b¯c,A˜c(z)=Ac(z)−
correspondingright-handside,respectively,as
A¯c,anddefinedthenoisevariableateachθc,
⋆
A¯ = 1 (cid:80)N A¯c , b¯ = 1 (cid:80)N b¯c .
N c=1 N c=1 εc(z)=A˜c(z)θc−b˜c(z).
⋆
In linear stochastic approximation, neither matrices A¯c
nor vectors b¯c are observed directly. Instead, each agent Running the recursion (4) until the start of local training,
c ∈ [N] has access to its own observation sequence weobtainthat
{(Ac(Z kc),bc(Z kc))} k∈N. For each c ∈ [N], Ac : Z →
θc −θc =Γ(c,η){θc −θc}−η(cid:80)H Γ(c,η) εc(Zc ),
Rd×d and bc : Z → Rd are measurable functions, and t,H ⋆ t,1:H t,0 ⋆ h=1 t,h+1:H t,h
(Z kc) n∈N isani.i.d.sequence,withvaluesinastatespace whereweintroducedthenotation
(Z,Z)anddistributionπ satisfyingE[Ac(Zc)]=A¯cand
c 1
E[bc(Z 1c)]=b¯c. Inthefollowing,weassumethatagents’ Γ( t,c m,η :)
n
=(cid:81)n h=m(I−ηA(Z tc ,h)), 1≤m≤n≤H ,
observationsequencesareindependentfromeachother,and
thateachlocalsystemA¯cθ ⋆c =b¯chasauniquesolutionθ ⋆c. with the convention Γ(c,η) = I for m > n. Now we
t,m:n
In the federated setting, agents can only communicate proceed with the classical assumptions, under which we
throughacentralserver,andsuchcommunicationsaregen- analyzetheLSAerrordynamics:
erally costly. This makes problem (1) challenging, as it A1. For each agent c the observations (Z kc) k∈N are i.i.d.
mustbesolvedcollaborativelybyallagents. Tothisend, randomvariablestakingvaluesin(Z,Z)withadistribution
weproposeFedLSA,whereagentsperformlocalupdates, π satisfying E [Ac(Zc)] = A¯c and E [b(Zc)] = b¯c.
c πc 1 πc 1
thatareaggregatedperiodicallytolimitthecommunication Moreover,foreachc∈[N]thefollowingmatricesexist
burden. During round t > 0, agents start from a shared
valueθ tandperformH >0localupdates,forh=1toH, Σc ε =(cid:82) Zεc(z)εc(z)⊤dπ c(z), (5)
Σc =(cid:82) (A˜c(z))⊤A˜c(z)dπ (z), (6)
θc =θc −η(Ac(Zc )−bc(Zc )), A˜ Z c
t,h t,h−1 t,h t,h
andC =sup ∥A¯c∥ <∞.
wherewesetθc =θ ,andusethealiasZc ≡Zc A c∈[N]
t,0 t t,h Ht+h−1
to simplify notations. Agents then send θ t,H to the cen- A2. Thereexista>0,η ∞ >0,suchthatη ∞a≤1/2,and
tral server, that aggregates them as θ
t
= N1 (cid:80)N c=1θ
t−1,H
forη ∈(0;η ∞),c∈[N],u∈Rd,h∈N,
andsendsitbacktoallagents. Wedescribethecomplete
E1/2(cid:2) ∥Γ(c,η)u∥2(cid:3) ≤(1−ηa)h∥u∥ . (7)
FedLSA procedureinAlgorithm1. t,1:h
Stochastic expansions for local updates. We use the AssumptionA1isclassicalinfinite-timestudiesofLSAit-
errorexpansionframework(seee.g. Aguechetal.,2000; erates;seee.g. Srikant&Ying(2019);Durmusetal.(2022).
3SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
IntheconsideredapplicationtotheFederatedTDlearning, PropertiesofΣc areextremelyimportantforouranalysis,
φ
the random matrices A˜c(z) and vectors εc(z) are almost sincewedefinetheoptimalapproximationparameterθc as
⋆
surebounded,sotheyautomaticallyadmitfinitemoments.
NotethatthematrixΣc from(5)measuresthenoiselevel
θ ⋆c ∈argmin θ∈RdE s∼µc[∥V θc,π(s)−Vc,π(s)∥2]. (8)
ε
atthelocaloptimumθc. Verifyingtheexponentialstabil- With the linear functional approximation (LFA) we can
⋆
ityassumptionA2iscrucialfortheconvergenceofLSA writethefederatedTD(0)asaparticularsettingofFedLSA
algorithmsinceitautomaticallyimpliesthatthetransient algorithm. HerewefollowotherpapersofTD(0)withLFA
componentoftheerrorθ˜(tr) decreasesexponentiallyfast; (Patil et al., 2023; Wang et al., 2023), and leave detailed
t
seeGuo&Ljung(1995);Priouret&Veretenikov(1998). In descriptiontoAppendixD.WeputtheassumptionsonΣc
φ
practicalapplications,thisassumptionrequirescarefulveri- andgenerativemechanism,whichareclassicalinTD(Patil
fication,withparticularattentiontothewaytheparameter etal.,2023;Lietal.,2023a;Samsonovetal.,2023).
ain(7)scaleswithinstance-dependentquantities. Impor- TD1. Tuples(Sc,Ac,Sc )aregeneratedi.i.d.withSc ∼
t t t+1 t
tantly,A2impliesthestabilityofthedeterministicmatrix µc,Ac ∼π(·|s),Sc ∼Pc (·|Sc,Ac).
t t+1 MDP t t
product,thatis,foranyu∈Rdwehave TD2. Matrices Σc are non-degenerate with the minimal
φ
eigenvalue ν = min λ (Σc) > 0. Moreover, the
∥(I−ηA¯c)hu∥ ≤(1−ηa)h∥u∥ . c∈[N] min φ
featuremappingφ(·)satisfiessup ∥φ(s)∥≤1.
s∈S
InSection5,wewillsometimesrequireafinerassumption,
The generative model assumption TD1 is used in many
whichwestatehereforeaseofreference.
previousworks;see,e.g.Dalaletal.(2018);Lietal.(2023a);
A3. Thereexistconstantsa,L > 0,suchthatforanyη ∈ Patiletal.(2023). Wenotethatitispossibletogeneralize
(0,1/L),c∈[N],itholdsforZ ∼π c,that this assumption to the more realistic setting of on-policy
aI≼E[1(Ac(Z)+Ac(Z)⊤)]≼ 1E[Ac(Z)⊤Ac(Z)]. evaluationoverasingletrajectoryleveragingtheMarkovian
2 L noisedynamics,followingWangetal.(2023). However,we
ThisassumptionisslightlymorerestrictivethanA2,since leaveitasadirectionforthefuturework. AssumptionTD
whenverA3holds,A2alsoholdswiththesameconstanta 2allowstoensuretheuniquenessoftheoptimalparameter
(seePatiletal.,2023;Samsonovetal.,2023). θc in(8). UnderTD1andTD2wecheckthegeneralLSA
⋆
assumptionsA1-A3,andthefollowingstatementholds.
Federatedtemporal-differencelearning. Wenowcon- Lemma 3.1. Assume TD1-TD2. Then the sequence of
sidertheparticularexampleoffederatedTD-learningwith TD(0)updatessatisfiesassumptionsA1-A3with
linearfunctionapproximation. InthissettingweobserveN
MarkovDecisionProcesses(S,A,Pc MDP,rc,γ)withcom- C A =1+γ , ∥Σc A˜∥ ≤2(1+γ)2 , (9)
monstateandactionspaces. Here,γ isadiscountingfactor, Tr(Σc)≤2(1+γ)2(cid:0) ∥θc∥2+1(cid:1) , (10)
ε ⋆
andeachoftheagentshasitsownenvironmentdynamics
Pc . ThestatespaceS isassumedtobeacompletemetric a= (1− 2γ)ν , η ∞ = (1− 4γ) , L= (1−1+ γγ )2ν . (11)
MDP
space. ThedynamicsofeachMDPisencapsulatedbythe
MarkovkernelPc (B|s,a),whichspecifiestheprobabil- Proof of the bounds (9)–(11) can be found in Samsonov
MDP
ity of transitioning from state s to a set B ∈ B(S) upon etal.(2023)orPatiletal.(2023). Forreader’sconvenience
taking action a. We denote by rs : S ×A → [0,1] the weprovideadetailedargumentinAppendixD.Inthecon-
rewardfunction(assumedtobedeterministicforsimplicity) sideredsettingthelocaloptimumparameterθ ⋆c corresponds
andbyπ(·|s)anagent’spolicy(assumedtobethesamefor tothe(unique)solutionofthesystemA¯cθ ⋆c = b¯c ,where
allagents). Onthecontrary,thedynamicsPc andreward weset,respectively,
MDP
functionrs arespecifictoeachagent. Theobjectiveisto A¯c =E [ϕ(s){ϕ(s)−γϕ(s′)}⊤] (12)
s∼µc,s′∼Pπ,c(·|s)
computetheagent’svaluefunctionunderthepolicyπ
b¯c =E [ϕ(s)r(s,a)].
s∼µc,a∼π(·|s)
Vc,π(s)=E[(cid:80)∞ γtrc(Sc,Ac)] ,
t=0 t t In case of a single-agent TD(0) the quantity of interest,
where Sc = s, and for t ∈ N, Ac ∼ π(·|Sc) and whichmeasuresthequalityofparameterθcomparedtoθ ⋆c
Sc ∼0 Pc (·|Sc,Ac). We considet r linear funt ctional isarguably(seePatiletal.,2023;Wangetal.,2023)notthe
apt+ pr1 oximatM ioD nP fort Vc,t
π(s) with a feature mapping φ :
norm∥θ−θ ⋆c∥2,butrather
S → Rd, i.e. we aim to approximate the value function ∥θ−θc∥2 =E [∥Vc,π(s)−Vc,π(s)∥2].
by Vc,π(s) = φ⊤(s)θ, θ ∈ Rd. For c ∈ [N], we set µc ⋆ Σc φ s∼µc θ θ ⋆c
θ
theinvariantdistributionoverS inducedby(π,Pc ),and Inoursetting,theoptimalparameterθ ⋆correspondstothe
definethedesignmatrixΣc φas MDP averagedsystem N1 (cid:80)N c=1A¯cθ ⋆ = N1 (cid:80)N c=1b¯c. Thissys-
temdoesnotnaturallyrelatetotheiteratesofTD(0)algo-
Σc =E [φ(Sc)φ(Sc)⊤]∈Rd×d . rithmwithdynamicsfromsometransitionkernelPπ,c(·|s),
φ µc 0 0
4SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
c∈[N]. Forthisreason,wewillpresentourresultsusing withthenotationsΓ¯(η) =E[Γ¯(η) ]= 1 (cid:80)N (I−ηA¯c)H
H s,H N c=1
theeuclideannorm∥·∥. and∆(η) =(cid:8)(cid:81)t Γ¯(η)(cid:9) −(Γ¯(η))t−s. Thefirstterm,
H,s,t i=s+1 i,H H
θ˜(tr)givestherateatwhichtheinitialerrorisforgotten. The
4.AnalysisoftheFedLSA algorithm t
termsθ˜(bi,bi) andθ˜(fl,bi) representthebiasandfluctuation
t t
Inthissection,weanalyzetheFedLSA algorithm. First, duetostatisticalheterogeneityacrossagents. Notethatin
weexpressθ asafunctionofθ . Then,westudythecon- thespecialcasewhereagentsarehomogeneous(i.e. A¯c =
T 0
vergencerateofthealgorithm. Finally,wediscusssample A¯ forallc∈[N]),thesetwotermsvanish. Finally,theterm
andcommunicationcomplexityofthealgorithm. θ˜ t(fl)depictsthefluctuationsofθ taroundthesolutionθ ⋆.
StochasticexpansionforFedLSA.Toderiveanexpression ConvergencerateofFedLSA.First,weanalyzetherateat
ofθ asafunctionofθ ,westartfromtheexpansionsfrom whichFedLSA convergestoθ˜(bi,bi)+θ .Thetwofollowing
T 0 t ⋆
Section3. Usingthefactθc = θ ,andemployingthe quantities,thatareduetoheterogeneityandstochasticityof
t,0 t−1
globalaveragingprocedure(3),weobtainthat localestimators,willplayacentralroleinthisrate,
θ t−θ ⋆ =Γ¯( t,η H){θ t−1−θ ⋆}+ρ¯ H +τ¯ t,H +ηφ¯ t,H , (13) v˜ heter =E c[∥Σc A˜∥∥θ ⋆c−θ ⋆∥2], σ¯ ε =E c[Tr(Σc ε)]. (18)
wherewehavedefined The quantities v˜ heter and σ¯ ε corresponds to the different
sourcesofnoiseintheerrordecomposition(17). Indeed,σ¯
ε
Γ¯(η) = 1 (cid:80)N Γ(c,η) , (14) isrelatedtothevarianceoflocalLSAiteratesoneachof
t,H N c=1 t,1:H
ρ¯ = 1 (cid:80)N (I−(I−ηA¯c)H){θc−θ }, theagents,whilev˜ hetercontrolsbiasfluctuationtermθ˜ t(fl,bi).
H N c=1 ⋆ ⋆ Notealsothatinthecentralizedsetting(i.e. N = 1)the
τ¯ t,H = N1 (cid:80)N c=1{(I−ηA¯c)H −Γ( t,c 1,η :H)}{θ ⋆c−θ ⋆}, termv˜ heter vanishes,butnotthetermσ¯ ε. Wenowproceed
φ¯ =−1 (cid:80)N (cid:80)H Γ(c,η) εc(Zc ). withtheanalysisoftheMSEofFedLSA’siterates.
t,H N c=1 h=1 t,h+1:H t,h
Theorem4.1. AssumeA1andA2. Thenforanystepsize
The transient term Γ¯( t,η H)(θ
t−1
− θ ⋆), responsible for the η ∈(0,η ∞)itholdsthat
r that ee flo uf cf to ur ag te iott nin tg ert mhe ηp φ¯re t,v Hio ,u rs eflit ee cr ta it nio gn the err oo sr cθ ilt l− a1 tio− nsθ ⋆ o, fa tn hd e E1 2(cid:2) ∥θ t−θ˜ t(bi,bi)−θ ⋆∥2(cid:3)≲(cid:113) η av˜ Nheter +(cid:113) η aσ¯ Nε
iteratesaroundθ ,aresimilartotheonesfromthestandard (cid:113)
LSAerrordecom⋆ position. Thetwoadditionaltermsin(13) + E c[ H∥Σ Nc A˜∥]∥ρ¯ aH∥ +(1−ηa)tH∥θ 0−θ ⋆∥ ,
reflect the heterogeneity bias. This bias is composed of
twoparts: thetruebiasρ¯ H,whichisnon-random,andits wherethebiasθ˜ t(bi,bi)convergesto(I−Γ¯( Hη))−1ρ¯ H atarate
fluctuationsτ¯ . Notethat,ifH = 1,thenthebiasterm ∥θ˜(bi,bi)−(I−Γ¯(η))−1ρ¯ ∥≤(1−ηa)tH∥(I−Γ¯(η))−1∥∥ρ¯ ∥.
t,H t H H H H
vanishes,as
Theproofofthisresultreliesonboundingcarefullyeachof
ρ¯ 1 = N1 (cid:80)N c=1A¯c{θ ⋆c−θ ⋆}=0, (15) thetermsfrom(17).Wegivethedetailedproofwithexplicit
constantsinAppendixA.Importantly,thefluctuationand
whichshowsthat,whendoingasinglelocalstep,thebias
heterogeneitytermsscaleinlinearlywithN,whichallows
duetolocaltrainingdisappears. WhenH ≥2,weshowin
touselargerstep-sizethaninthesingle-agentsetting.
LemmaC.3thatthebiastermcanbeboundedby
Remark4.2. WhenN =1,FedLSA revertstothecentral-
∥ρ¯ ∥ ≤ η2H2 (cid:80)N exp(ηH∥A¯c∥)∥θc−θ ∥ . (16) izedalgorithm: thebiastermρ¯ H anditsfluctuationv˜ heter
H N c=1 ⋆ ⋆ vanishinTheorem4.1,yieldingthelast-iteratebound
Thisboundcannotbeimprovedwithoutfurtherassumptions.
E1/2(cid:2)∥θ −θ ∥2(cid:3)≲(cid:112) ησ¯ /a+(1−ηa)tH∥θ −θ ∥ ,
Toanalyzethecomplexityandcommunicationcomplexity t ⋆ ε 0 ⋆
ofFedLSA,weruntherecurrence(13)toobtain
whichisknowntobesharpinitsdependenceonηforsingle-
θ −θ =θ˜(tr)+θ˜(bi,bi)+θ˜(fl,bi)+θ˜(fl) , (17) agentLSA(seeTheorem5inDurmusetal.(2021)).
t ⋆ t t t t
TheseboundscanbeinstantiatedforfederatedTD(0):using
wherewehavedefined
theboundsfromLemma3.1,weobtainthefollowingresult.
θ˜ t(tr) =(cid:81)t s=1Γ¯( sη ,H) {θ 0−θ ⋆}, Corollary4.3. AssumeTD1andTD2. Thenforanystep
θ˜(bi,bi) =(cid:80)t (cid:0) Γ¯(η)(cid:1)t−s ρ¯ , sizeη ∈(0,1− 4γ),theiteratesoffederatedTD(0)satisfy
t s=1 H H
(cid:113)
θ˜ t(fl,bi) =(cid:80)t s=1(cid:81)t i=s+1Γ¯( i,η H)τ¯
s,H
+∆( Hη ,) s,tρ¯
H
, E1/2(cid:2) ∥θ t−θ˜ t(bi,bi)−θ ⋆∥2(cid:3)≲ η(E c[∥θ ⋆c−θ (⋆ 1∥ −2] γ∨ )( ν1 N+E c[∥θ ⋆c∥2]))
θ˜ t(fl) =η(cid:80)t s=1(cid:81)t i=s+1Γ¯( i,η H)φ¯ s,H , +(cid:113) H1
N
(1∥ −ρ¯H γ∥
)ν
+(1− η(1− 2γ)ν)tH∥θ 0−θ ⋆∥ .
5SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
The right hand side of Corollary 4.3 scales linearly with Then,toachieveE(cid:2) ∥θ −θ ∥2(cid:3) <ϵ2therequirednumber
T ⋆
N,allowingforlinearspeed-up. Thisisinlinewithrecent ofcommunicationsforfederatedTD(0)is
resultsonfederatedTD(0),thatshowlinearspeed-upeither
withoutlocaltraining(DalFabbroetal.,2023)oruptoa T =O(cid:16)(cid:16) 1 ∨ E c[∥θ ⋆c−θ⋆∥](cid:17) log∥θ0−θ⋆∥(cid:17) .
(1−γ)2ν (1−γ)2ν2ϵ ϵ
possiblylargebiasterm(Wangetal.,2023)(seeanalysisof
theirTheorem2). Next,wewillseethatourtighteranalysis
Corollary4.6showsthat, evenwhenagentsareheteroge-
ofthebiasallowstoproveconvergenceoffederatedTD(0)
neous, it is possible for federated TD(0) to converge to
evenunderheterogeneity.
θ witharbitraryprecision. Thisfollowsfromourprecise
⋆
Sample and communication complexity of FedLSA. characterizationofthebiasoffederatedTD(0)withlocal
Webeginwiththeanalysiswithoutlocaltraining(thatis, training,anditsdependenceontheproductηH. Nonethe-
H =1). There,thebiastermdisappears,andaboveresults less,thenumberofroundsT stillhastoscalepolynomially
directlygiveasimplifiedsamplecomplexitybound. inϵ−1. Weproposeawayoffixingthisinthenextsection.
Corollary4.4. AssumeA1andA2. LetH =1,0<ϵ<1.
Setη = aNϵ2 ∧η .Then,toachieveE(cid:2) ∥θ −θ ∥2(cid:3) ≤ϵ2, 5.Bias-CorrectedFederatedLSA
v˜heter∨σ¯ε ∞ T ⋆
therequirednumberofcommunicationsis
WenowintroducetheStochasticControlledAveragingfor
T =O(cid:0)(cid:0) 1 ∨ v˜heter∨σ¯ε(cid:1) log∥θ0−θ⋆∥(cid:1) . FederatedLSAalgorithm(SCAFFLSA),animprovedver-
aη∞ Na2ϵ2 ϵ
sionofFedLSAthatmitigatesclientdriftusingcontrolvari-
Here,weobtainalinearspeed-upintermsofsamplecom- ates. ThismethodisinspiredbyProxSkip(seeMishchenko
plexity. This is expected, since when H = 1, FedLSA et al., 2022), and more specifically Scaffnew, its instance
amountstousingastochasticoraclewithreducedvariance,
tailoredtominimizesumsofstronglyconvexfunctions. In
allowing for larger step-size. Now, we proceed with the SCAFFLSA,eachagentc∈[N]keepsalocalvariableξc,
k
moredelicatesettingwhereH >1,andheterogeneitybias
thatremainsconstantbetweensuccessivecommunication
doesnotvanish. Weobtainthefollowingbounds. rounds. Attimestepk,agentsperformalocalupdateonthe
Corollary 4.5. Assume A1 and A2. Let H > 1, and currentestimatesoftheparameters(θc )N asfollows
k−1 c=1
0 ste< psϵ ize< η( =√ v˜ Ohet (cid:0)er∨σ¯ aε NE ϵc 2a[∥θ ⋆c ∧− ηθ⋆∥ (cid:1)]) .2 T/5 he∨ n,E tc o[∥ aaθ c⋆c C h− A ieθ⋆ v∥ e] E. (cid:2)S ∥e θt th −e θˆ kc =θˆ kc −1−η(Ac(Z kc)θˆ kc −1−bc(Z kc)−ξ kc −1).
θ
∥2(cid:3) <ϵ2therev˜ qhe ute ir∨ reσ¯ dε num∞ berofcommunicationsisT Then,theycommunicateaccordingtothevalueofavariable
⋆
B ∈{0,1},thatissetbyoneofthefollowingrules.
k
T =O(cid:16)(cid:16) 1 ∨ E c[∥θ ⋆c−θ⋆∥](cid:17) log∥θ0−θ⋆∥(cid:17) , (19) H1. B k ∼Bernoulli(p)arei.i.d.,withp∈[0,1].
aη∞ a2ϵ ϵ
H2. B =1ifkisamultipleofH >0,B =0otherwise.
k k
wherethenumberoflocalstepsissetto
When B = 1, agents (i) communicate to the central
k
H =O(cid:0) v˜heter∨σ¯ε 1 (cid:1) . server (CS), then (ii) the CS averages local iterates, and
E c[∥θ ⋆c−θ⋆∥]Nϵ
(iii) agents update their local control variates. We de-
InCorollary4.5,thetotalnumberoforaclecallsscalesas
scribe the procedure in Algorithm 2. In the following,
TH
=O(cid:0)v˜heter∨σ¯ε log∥θ0−θ⋆∥(cid:1)
, we study SCAFFLSA under H 1 and H 2. We estab-
Na2ϵ2 ϵ
lish finite-time bounds for the MSE of the parameters of
whichcorrespondstothenumberofiterationsofthesyn- interest E[1 (cid:80)N ∥θc −θ ∥2]. To this end, we define
N c=1 K ⋆
chronousversionoftheLSAmethodpredictedbyCorol- the ideal control variates at the global solution, given by
lary4.4.Thus,despitethebiasduetoheterogeneity,itisstill ξc =A¯cθ −b¯c =A¯c(θ −θc). Usingξc,wecanrewrite
⋆ ⋆ ⋆ ⋆ ⋆
possibletoachievelinearspeed-up. Thisisnotablythecase thelocalupdateas
whencommunicatingregularlyenough,sothatthe”physi-
caltime”ofthelocaliterations,ηH,scalesasηH =O(ϵ). θˆc −θ =(I−ηAc(Zc))(θc −θ )+
k ⋆ k k−1 ⋆
Therefore,toachieveprecisionϵ2ontheMSE,thenumber
η(ξc −ξc)−ηωc(Zc), (20)
ofcommunicationsT in(19)mustscalepolynomiallywith k−1 ⋆ k
ϵ−1. Inthenextsection,wewillshowthatthisdrawback where we defined ωc(z) = A˜c(z)θ −b˜c(z). Under A
⋆
canbeovercomebyusingappropriatecontrolvariates. This 1, it has finite covariance Σc = (cid:82) ωc(z)ωc(z)⊤dπ (z).
willallowtode-biasthealgorithm,whichwillinturnallow Similarlyto(18),weusethenω otationZ σ¯ =E [Tr(Σcc )].
ω c ω
tochooseηH ofconstantorderwithrespecttoϵ.
Analysis under H1. In the stochastic communication
NowwestatethecommunicationboundoffederatedTD(0).
paradigmdescribedinH1,theagentsaveragetheirrespec-
Corollary 4.6. Assume TD 1 and TD 2. Then for any tive local parameters with a predefined probability p dur-
0<ϵ< g (1 1( −θ ⋆c γ,θ )ν⋆) withg
1
=O((1+∥θ ⋆∥)E c[∥θ ⋆c−θ ⋆∥]). ing each iteration. It is noteworthy that this framework,
6SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Algorithm2SCAFFLSA:StochasticControlledFedLSA We provide the detailed proof of these two statements in
Input: η >0,θ ,ξc ∈Rd,T,N,H,p>0 AppendixB.1.InCorollary5.2itisshownthattheinclusion
0 0
Set: K =T/p(ifH1),K =TH (ifH2) ofcontrolvariatesinlocalupdatesreducestherequiredcom-
fork =1toK do municationroundsbyasignificantamount,quantifiableas
(cid:112)
forc=1toN do O(min( L/a,1/aϵ)). Comparedtothescenariosdescribed
ReceiveZc andperformlocalupdate: inSection4,thisimprovementmeansagainbyafactorof
k
θˆc =θˆc −η(Ac(Zc)θˆc −bc(Zc)−ξc )
1/a. Atthesametime, however,thisalsomeansalossof
k k−1 k k−1 k k−1 thelinearaccelerationfactorintheformof1/N. Thisphe-
endfor nomenonisprimarilyduetotheinherentpropertiesofthe
ChooseB k accordingtoH1orH2 analyticalmethodused,asweshowinAppendixB.1. Note
ifB k =1then that,underTD2andTD1theexpectednumberofcommu-
Averagelocaliterates: θc = 1 (cid:80)N θˆc nicationroundsforSCAFFLSA appliedtofederatedTD(0)
k N c=1 k
(cid:40)
ξc =ξc + p(θc −θˆc) ifH1 algorithmscales,following(21),as
Update: k k−1 η k k
ξc =ξc + 1 (θc −θˆc) ifH2 √
k k−1 ηH k k T =O(cid:0) max(cid:0) 1 ,1+ E c[∥θ ⋆c∥2](cid:1) log(cid:0)ψ0(cid:1)(cid:1) ,
else (1−γ)3/2ν ϵ(1−γ)ν ϵ2
Set: θc =θˆc,ξc =ξc
k k k k−1 whichimprovesuniformlyoverthenumberofcommunica-
endif
tionsrequiredbyFedLSA thatwepresentedintheprevious
endfor
section. Nonetheless,thenumberofcommunicationrounds
for SCAFFLSA still scales with the inverse of ϵ. This
as specified in Algorithm 2, does not include an inner seemstobeduetothefactthatourstep-by-stepanalysisis
loop. Thisstructuralsimplification,originallyintroducedin looseinhigh-precisionregimes,wherethestepsizeisvery
Mishchenkoetal.(2022),significantlystreamlinestheanal- small. Forthisreason,westudyintherestofthissection
ysis: itcansimplybeperformedbyformulatingadescent anothervariantofSCAFFLSA,whichismoreadaptedtoa
lemmathatrevolvesaroundaparticularLyapunovfunction, block-by-blockanalysis,alikeFedLSA.
AnalysisunderH2. Inthedeterministic-temporalcommu-
ψ = 1 (cid:80)N ∥θc −θ ∥2+ η2 1 (cid:80)N ∥ξc −ξc∥2 .
k N c=1 k ⋆ p2 N c=1 k ⋆ nicationparadigmH2,agentsaveragetheirlocalparameters
afterafixednumberoflocalupdatesH. Inthisscenario,the
Examiningtheexpectedone-stepimprovementofthisfunc-
simplifiedanalysisinSection5isnotapplicable. Instead,
tion effectively captures the dynamics of both the local
wedecomposethealgorithmblock-by-block. Similarlyto
parameterupdatesandthecontrolvariates. Thisprovidesa
the analysis of FedLSA, we use (20) to describe the se-
clearwaytostudytheconvergencepropertiesandgeneral
quenceofaggregatediteratesas
behaviorofthealgorithm. However,thisone-stepanalysis
requiresthestrongerassumptionA3onA¯c. Wenowestab-
θ −θ = 1 (cid:80)N (cid:2) Γ(c,η)(θ −θ )
lishfinitetime-bounds,fromwhichwededucesampleand t+1 ⋆ N c=1 t,1:H t ⋆
communicationcomplexityofAlgorithm2underH1. +η(cid:80)H Γ(c,η) (ξc−ξc)−η(cid:80)H Γ(c,η) ωc(Zc )(cid:3) ,
h=1 t,h+1:H t ⋆ h=1 t,h+1:H t,h
Theorem5.1(MSEbound). AssumeA1,A3,andH1.Then,
foranystepsizeη ≤ 1/2Landnumberofglobaliteration
wheretistheintegerpartofk/H,andh=k mod H (we
K =T/p>0,itholds,withζ
=min(cid:0) ηa,p2(cid:1)
,
restatethealgorithmwiththesenotationsinAppendixB.2).
Usingthisexpression,wecananalyzeSCAFFLSA under
E[1 (cid:80)N ∥θc −θ ∥2]≤(cid:0) 1−ζ(cid:1)K ψ + 2η2 σ¯ , H2,bystudyingthefollowingLyapunovfunction
N c=1 K ⋆ 0 ζ ω
whereψ =∥θ −θ ∥2+ η2E [∥A¯c(θc−θ )∥2]. ψ t =∥θ t−θ ⋆∥2+ η2 NH2 (cid:80)N c=1∥ξ tc−ξ ⋆c∥2 ,
0 0 ⋆ p2 c ⋆ ⋆
Corollary5.2(Iterationcomplexity). AssumeA1,A3,and whichisdefinedastheerrorinθ estimationoncommunica-
H1. Letϵ > 0. Setη =
O(cid:0) min(cid:0)1,ϵ2a(cid:1)(cid:1)
andp =
√
ηa
tionrounds,aswellastheavera⋆
geerroroncontrolvariates.
(sothatζ = ηa). Then,E[1 (cid:80)N L ∥θσ¯ cω −θ ∥2] ≤ ϵ2 for Wenowstatetheconvergencerateanditerationcomplexity
N c=1 K ⋆
anexpectednumberofcommunicationrounds ofAlgorithm2underthemoregeneralAssumptionA2.
(cid:16) (cid:16)(cid:113) √ (cid:17) (cid:16) (cid:17)(cid:17) Theorem5.3. AssumeA1,A2,H2. Letη,H >0suchthat
T =O max L a, ϵσ¯ aω log ψ ϵ20 , (21) η ≤ min(1/CA,η ∞), and H ≤ min(a/32ηC2 A,1/η,1/ηa).
Setξc =0forallc∈[N]. Thenwehave
whereψ isdefinedinTheorem5.1,withanexpectednum- 0
0
beroflocalupdates E[∥θ −θ ∥2]≤(cid:0) 1− ηaH(cid:1)TE[ψ ]+ 16ησ¯ ,
T ⋆ 4 0 a ω
(cid:113) √
1/p=O(cid:0) max(cid:0) L a, ϵσ¯ aω(cid:1)(cid:1) , fort∈[T]. whereψ
0
=∥θ 0−θ ⋆∥2+η2H2E c[∥A¯c(θ ⋆c−θ ⋆)∥2].
7SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Corollary 5.4. Let ϵ > 0. Assume ϵ2 ≤ 32σ¯ω, a2 ≤
Em [i ∥n θ(1, −32 θC ∥2 A 2) ]. ≤S ϵe 2t wη he= ntO he(m nuin m( bη e∞ r, oa fϵ c2 o/σ¯ mω m)a ) uC . nA iW cae tih oa nv ie
s
102 11 00 13 F Se Cd AL FS FA
LSA
T ⋆ 100
10 1
T =O(cid:0)C2 A log(cid:0)ψ0(cid:1)(cid:1) , 10 2 10 3
a2 ϵ2
10 4 10 5
withψ =O(∥θ −θ ∥2+ a2 E [∥A¯c(θc−θ )∥2]),and
0 0 ⋆ C2 c ⋆ ⋆ 0 20 40 60 80 100 0 20 40 60 80 100
thenumberoflocalupdates A Communication rounds Communication rounds
(a) Lowheterogeneity. (b) Highheterogeneity.
H =O(cid:0) max(cid:0) a , σ¯ω (cid:1)(cid:1) .
η∞C2
A
ϵ2C2
A Figure1.Meansquarederrorforestimationoftheglobaloptimal
WeprovethesetwostatementsinAppendixB.2. InCorol- parameterofthevaluefunction,forFedLSA andSCAFFLSA
instantiatedforTD(0)onGarnetproblems. Foreachalgorithm,
lary5.4,weshowthatwithsmallenoughstep-size,thetotal
weplottheaverageMSEandstandarddeviationover5runs.
numberofcommunicationdependsonlylogarithmicallyon
theprecisionϵ. ThisisinstarkcontrastwithAlgorithm1,
wherethenecessityofcontrollingthebias’magnitudepre- row so that they sum to 1. We generate N = 100 such
vents from scaling H with 1/ϵ2. Thus, in high precision environments,withϵ=0.0002,andrunAlgorithms1and2
regime (i.e.small ϵ and η), using control variates reduces with parameters η = 0.01,H = 10000 and N = 100.
communication complexity compared to FedLSA. How- WegivetheresultsinFigure1(a),whereweseethatboth
ever, we note that, as in Corollary 5.2, we lose the linear algorithmsbehavesimilarly.Thisisduetothefactthatinthe
speed-up in 1/N. This result translates into the following lowheterogeneityregimelocaliterationscauselittleclient
communicationcomplexityboundforfederatedTD(0). drift, making bias small: there, the noise from stochastic
Corollary 5.5. Assume TD1 and TD2 and let 0 < ϵ ≤ oraclesdominates(i.e. v˜ heterandρ¯ H aresmallerthanσ¯ ε).
(cid:112) 8E [1+∥θc∥2]/((1−γ)ν). Then, in order to achieve
E[∥θc −θ ∥2]⋆ ≤ϵ2,therequirednumberofcommunication Highheterogeneityregime. Tosimulatehighlyheteroge-
T ⋆
neous environments, we generate N = 100 independent
forfederatedTD(0)algorithmis
Garnetenvironments,withthesameparametersasabove.
(cid:16) (cid:16) (cid:17)(cid:17)
T =O 1 log ψ0 , WeshowtheresultsinFigure1(b),wheretheFedLSA stops
(1−γ)2ν2 ϵ2 makingprogressduetoheterogeneitybias, whileSCAF-
whereψ =O(∥θ −θ ∥2+(1−γ)2ν2E [∥θc−θ ∥2]). FLSA continuestowardstheglobalsolutionoftheproblem,
0 0 ⋆ c ⋆ ⋆ untilnoisestartstodominate. Notethat,accordingtothe
Corollary5.5confirmsthat,whenappliedtoTD(0),SCAF- theory,wecouldalwaysdecreasethestepsizeofFedLSA
FLSA’scommunicationcomplexitydependsonlylogarith- toobtainmorepreciseresults. However,thiswouldrequire
micallyonheterogeneityandonthedesiredprecision. In morecommunicationrounds,whichisnotalwayspossible.
contrastwithexistingmethodsforfederatedTD(0)(Doan
etal.,2019;Jinetal.,2022;Wangetal.,2023),itconverges
7.Conclusion
tothesolutionofsystem(1)evenwithmanylocalsteps.
Inthispaper,westudiedfederatedlinearstochasticapproxi-
6.NumericalExperiments mation,withlocaltrainingandheterogeneousagents. We
analyzedthecomplexityofFedLSA algorithm. Wecare-
Inthissection,weillustratethepracticalperformanceofour
fullyanalyzeditsbias,inducedbylocaltraining,andshowed
twoalgorithmsfordifferentlevelsofheterogeneity. Tothis
howtoparameterizethealgorithmtocontrolit.Wethenpro-
end,weconsidertheclassicalGarnetproblem(Archibald
posedSCAFFLSA,analgorithmthatcorrectsFedLSA’s
etal.,1995),inthesimplifiedversionproposedbyGeistetal.
biaswithcontrolvariates,andstudieditscomplexity. Both
(2014). Theseproblemsarecharacterizedbythenumberof
methodsareappliedtofederatedTDlearningwithlinear
statesn, numberofactionsa, andbranchingfactorb(i.e.
functionapproximation,forwhichwegiveexplicitcommu-
thenumberofneighborsofeachstateintheMDP).Weset
nicationcomplexitybounds. WithouranalysisofSCAF-
these values to n = 30, a = 2 and b = 2, and aim, for
FLSA,weimproveiterationandcommunicationcomplexity
illustration, to evaluate the value function of the uniform
comparedtoFedLSA intermsofkeyproblemparameters
policy,thatchoosesarandomactionateachtimestep.
and desired precision, but we lose the linear speed-up in
number of agents. The same problem appears in many
Low heterogeneity regime. We generate an instance of
existinganalysesofgradient-basedfederatedlearningalgo-
Garnetwithmentionedparameters,andperturbitbyadding
rithms, making it a challenging future research direction.
small uniform noise ε ∼ U([0,ϵ]) to each non-zero
s,a,s′
elementofthetransitionmatrix. Thenwenormalizeeach
8
2||
t ||
2||
t ||SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
ImpactStatement Dalal,G.,Szo¨re´nyi,B.,Thoppe,G.,andMannor,S. Finite
sampleanalysesforTD(0)withfunctionapproximation.
Thispaperpresentsworkwhosegoalistoadvancethefield
InThirty-SecondAAAIConferenceonArtificialIntelli-
of Machine Learning. There are many potential societal
gence,2018.
consequences of our work, none which we feel must be
specificallyhighlightedhere. Dann, C., Neumann, G., Peters, J., et al. Policy evalua-
tion with temporal differences: A survey and compari-
Acknowledgement son. JournalofMachineLearningResearch,15:809–883,
2014.
TheworkofP.M.andS.LhasbeensupportedbyTechnol-
Doan,T.,Maguluri,S.,andRomberg,J. Finite-TimeAnal-
ogyInnovationInstitute(TII),projectFed2Learn. Thework
ysisofDistributedTD(0)withLinearFunctionApprox-
ofEricMoulineshasbeenpartlyfundedbytheEuropean
imation on Multi-Agent Reinforcement Learning. In
Union(ERC-2022-SYG-OCEAN-101071601). IlyaLevin,
Proceedings of the 36th International Conference on
AlexeyNaumovandSergeySamsonovweresupportedby
Machine Learning, pp. 1626–1635. PMLR, May 2019.
HSEUniversityBasicResearchProgram. Viewsandopin-
URLhttps://proceedings.mlr.press/v97/
ionsexpressedarehoweverthoseoftheauthor(s)onlyand
doan19a.html. ISSN:2640-3498.
donotnecessarilyreflectthoseoftheEuropeanUnionor
theEuropeanResearchCouncilExecutiveAgency. Neither
Doan, T. T. Local stochastic approximation: A uni-
theEuropeanUnionnorthegrantingauthoritycanbeheld
fied view of federated learning and distributed multi-
responsibleforthem.
taskreinforcementlearningalgorithms. arXivpreprint
arXiv:2006.13460,2020.
References
Durmus,A.,Moulines,E.,Naumov,A.,Samsonov,S.,Sca-
Aguech, R., Moulines, E., and Priouret, P. On a pertur- man,K.,andWai,H.-T. Tighthighprobabilitybounds
bation approach for the analysis of stochastic tracking for linear stochastic approximation with fixed stepsize.
algorithms. SIAMJournalonControlandOptimization, In Ranzato, M., Beygelzimer, A., Nguyen, K., Liang,
39(3):872–899,2000. P.S.,Vaughan,J.W.,andDauphin,Y.(eds.),Advancesin
NeuralInformationProcessingSystems,volume34,pp.
Archibald, T., McKinnon, K., and Thomas, L. On the 30063–30074.CurranAssociates,Inc.,2021.
generationofmarkovdecisionprocesses. Journalofthe
Durmus,A.,Moulines,E.,Naumov,A.,andSamsonov,S.
OperationalResearchSociety,46(3):354–361,1995.
Finite-timehigh-probabilityboundsforpolyak-ruppert
averaged iterates of linear stochastic approximation.
Bhandari,J.,Russo,D.,andSingal,R. Afinitetimeanal-
arXivpreprintarXiv:2207.04475,2022.
ysisoftemporaldifferencelearningwithlinearfunction
approximation. InConferenceOnLearningTheory,pp.
Geist, M., Scherrer, B., et al. Off-policy learning with
1691–1692,2018.
eligibilitytraces: asurvey. J.Mach.Learn.Res.,15(1):
289–333,2014.
Collins,L.,Hassani,H.,Mokhtari,A.,andShakkottai,S.
Exploitingsharedrepresentationsforpersonalizedfeder- Gorbunov,E.,Hanzely,F.,andRichta´rik,P. Localsgd: Uni-
ated learning. In International conference on machine fiedtheoryandnewefficientmethods. InInternational
learning,pp.2089–2099.PMLR,2021. Conference on Artificial Intelligence and Statistics, pp.
3556–3564.PMLR,2021.
Condat,L.andRichta´rik,P. Randprox: Primal-dualopti-
Grudzien´,M.,Malinovsky,G.,andRichta´rik,P. Can5th
mizationalgorithmswithrandomizedproximalupdates.
generation local training methods support client sam-
arXivpreprintarXiv:2207.12891,2022.
pling? yes! In International Conference on Artificial
IntelligenceandStatistics,pp.1055–1092.PMLR,2023.
Condat, L., Agarsky, I., and Richta´rik, P. Provably dou-
blyacceleratedfederatedlearning: Thefirsttheoretically
Guo, L. and Ljung, L. Exponential stability of general
successfulcombinationoflocaltrainingandcompressed
tracking algorithms. IEEE Transactions on Automatic
communication. arXivpreprintarXiv:2210.13277,2022.
Control,40(8):1376–1387,1995.
DalFabbro,N.,Mitra,A.,andPappas,G.J. Federatedtd Haddadpour,F.andMahdavi,M. OntheConvergenceof
learningoverfinite-rateerasurechannels:Linearspeedup LocalDescentMethodsinFederatedLearning,Decem-
undermarkoviansampling.IEEEControlSystemsLetters, ber 2019. URL http://arxiv.org/abs/1910.
2023. 14425. arXiv:1910.14425[cs,stat].
9SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Hu,Z.andHuang,H. Tighteranalysisforproxskip. InIn- Malinovsky,G.,Yi,K.,andRichta´rik,P. Variancereduced
ternationalConferenceonMachineLearning,pp.13469– proxskip: Algorithm,theoryandapplicationtofederated
13496.PMLR,2023. learning. Advances in Neural Information Processing
Systems,35:15176–15189,2022.
Jin,H.,Peng,Y.,Yang,W.,Wang,S.,andZhang,Z. Feder-
atedReinforcementLearningwithEnvironmentHetero- McMahan,B.,Moore,E.,Ramage,D.,Hampson,S.,and
geneity. InProceedingsofThe25thInternationalConfer- yArcas,B.A. Communication-efficientlearningofdeep
enceonArtificialIntelligenceandStatistics,pp.18–37. networks from decentralized data. In Artificial intelli-
PMLR, May 2022. URL https://proceedings. genceandstatistics,pp.1273–1282.PMLR,2017.
mlr.press/v151/jin22a.html. ISSN: 2640-
Mishchenko,K.,Malinovsky,G.,Stich,S.,andRichta´rik,P.
3498.
Proxskip: Yes! localgradientstepsprovablyleadtocom-
Karimireddy,S.P.,Kale,S.,Mohri,M.,Reddi,S.,Stich,S., municationacceleration! finally! InInternationalCon-
andSuresh,A.T. Scaffold: Stochasticcontrolledaverag-
ferenceonMachineLearning,pp.15750–15769.PMLR,
ingforfederatedlearning. InInternationalconferenceon 2022.
machinelearning,pp.5132–5143.PMLR,2020.
Mitra, A., Jaafar, R., Pappas, G.J., andHassani, H. Lin-
ear convergence in federated learning: Tackling client
Khaled, A., Mishchenko, K., and Richta´rik, P. Tighter
heterogeneityandsparsegradients. AdvancesinNeural
theoryforlocalsgdonidenticalandheterogeneousdata.
InformationProcessingSystems,34:14606–14619,2021.
InInternationalConferenceonArtificialIntelligenceand
Statistics,pp.4519–4529.PMLR,2020.
Patel, K. K., Glasgow, M., Wang, L., Joshi, N., and Sre-
bro, N. On the still unreasonable effectiveness of fed-
Khodadadian,S.,Sharma,P.,Joshi,G.,andMaguluri,S.T.
erated averaging for heterogeneous distributed learn-
Federatedreinforcementlearning: Linearspeedupunder
ing. In Federated Learning and Analytics in Practice:
markovian sampling. In International Conference on
Algorithms, Systems, Applications, and Opportunities,
MachineLearning,pp.10997–11057.PMLR,2022.
2023. URLhttps://openreview.net/forum?
id=vhS68bKv7x.
Koloskova,A.,Loizou,N.,Boreiri,S.,Jaggi,M.,andStich,
S. Aunifiedtheoryofdecentralizedsgdwithchanging
Patil,G.,Prashanth,L.,Nagaraj,D.,andPrecup,D. Finite
topologyandlocalupdates. InInternationalConference
timeanalysisoftemporaldifferencelearningwithlinear
onMachineLearning,pp.5381–5393.PMLR,2020.
functionapproximation:Tailaveragingandregularisation.
InInternationalConferenceonArtificialIntelligenceand
Konecˇny`,J.,McMahan,H.B.,Ramage,D.,andRichta´rik,P.
Statistics,pp.5438–5448.PMLR,2023.
Federatedoptimization: Distributedmachinelearningfor
on-deviceintelligence. arXivpreprintarXiv:1610.02527,
Priouret,P.andVeretenikov,A. Aremarkonthestability
2016. oftheLMStrackingalgorithm. Stochasticanalysisand
applications,16(1):119–129,1998.
Li,G.,Wu, W., Chi,Y., Ma,C.,Rinaldo,A.,andWei,Y.
Sharp high-probability sample complexities for policy Qi, J., Zhou, Q., Lei, L., and Zheng, K. Federated rein-
evaluationwithlinearfunctionapproximation,2023a. forcementlearning: Techniques,applications,andopen
challenges. arXivpreprintarXiv:2108.11887,2021.
Li, T., Sahu, A. K., Talwalkar, A., and Smith, V. Feder-
atedlearning: Challenges,methods,andfuturedirections. Qu,Z.,Lin,K.,Li,Z.,andZhou,J. Federatedlearning’s
IEEEsignalprocessingmagazine,37(3):50–60,2020. blessing: Fedavg has linear speedup. In ICLR 2021-
WorkshoponDistributedandPrivateMachineLearning
Li,T.,Lan,G.,andPananjady,A. Acceleratedandinstance- (DPML),2021.
optimalpolicyevaluationwithlinearfunctionapproxima-
tion. SIAMJournalonMathematicsofDataScience,5 Reddi, S., Charles, Z., Zaheer, M., Garrett, Z., Rush, K.,
(1):174–200,2023b. doi: 10.1137/21M1468668. URL Konecˇny`,J.,Kumar,S.,andMcMahan,H.B. Adaptive
https://doi.org/10.1137/21M1468668. federatedoptimization. arXivpreprintarXiv:2003.00295,
2020.
Lim,H.-K.,Kim,J.-B.,Heo,J.-S.,andHan,Y.-H.Federated
reinforcement learning for training control policies on Robbins, H. and Monro, S. A Stochastic Approxima-
multipleiotdevices. Sensors,20(5):1359,2020. tion Method. The Annals of Mathematical Statis-
tics, 22(3):400 – 407, 1951. doi: 10.1214/aoms/
Liu,R.andOlshevsky,A. Distributedtd(0)withalmostno 1177729586. URL https://doi.org/10.1214/
communication. IEEEControlSystemsLetters,2023. aoms/1177729586.
10SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Samsonov, S., Tiapkin, D., Naumov, A., and Moulines,
E. Finite-sample analysis of the Temporal Difference
Learning. arXivpreprintarXiv:2310.14286,2023.
Srikant,R.andYing,L. Finite-timeerrorboundsforlinear
stochasticapproximationandTDlearning. InConference
onLearningTheory,pp.2803–2830.PMLR,2019.
Sutton,R.S.Learningtopredictbythemethodsoftemporal
differences. Machinelearning,3:9–44,1988.
Sutton,R.S.,Maei,H.R.,Precup,D.,Bhatnagar,S.,Sil-
ver,D.,Szepesva´ri,C.,andWiewiora,E. Fastgradient-
descent methods for temporal-difference learning with
linearfunctionapproximation. InProceedingsofthe26th
annualinternationalconferenceonmachinelearning,pp.
993–1000,2009.
Tsitsiklis,J.N.andVanRoy,B. Ananalysisoftemporal-
differencelearningwithfunctionapproximation. IEEE
TransactionsonAutomaticControl,42(5):674–690,May
1997. ISSN2334-3303. doi: 10.1109/9.580874.
Wang,H.,Mitra,A.,Hassani,H.,Pappas,G.J.,andAnder-
son,J. Federatedtemporaldifferencelearningwithlinear
functionapproximationunderenvironmentalheterogene-
ity. arXivpreprintarXiv:2302.02212,2023.
Wang, J., Das, R., Joshi, G., Kale, S., Xu, Z., and
Zhang, T. On the unreasonable effectiveness of feder-
atedaveragingwithheterogeneousdata. arXivpreprint
arXiv:2206.04723,2022.
Xie, Z. and Song, S. Fedkl: Tackling data heterogene-
ityinfederatedreinforcementlearningbypenalizingkl
divergence. IEEEJournalonSelectedAreasinCommu-
nications,41(4):1227–1242,2023.
Zhao,Y.,Li,M.,Lai,L.,Suda,N.,Civin,D.,andChandra,
V. Federatedlearningwithnon-iiddata. arXivpreprint
arXiv:1806.00582,2018.
11SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
A.AnalysisofFederatedLinearStochasticApproximation
For the analysis we need to define two filtration: F+ := σ(Zc ,t ≥ s,k ≥ h,1 ≤ c ≤ N) (future events) and
s,h t,k
F− :=σ(Zc ,t≤s,k ≤h,1≤c≤N)(precedingevents). RecallthatthelocalLSAupdatesarewrittenas
s,h t,k
θc −θc =(I−ηA(Zc ))(θc −θc)−ηεc(Zc ).
t,h ⋆ t,h t,h−1 ⋆ t,h
PerformingH localstepsandtakingaverage,weendupwiththedecomposition(17),whichweduplicatehereforuser’s
convenience:
θ −θ =θ˜(tr)+θ˜(bi,bi)+θ˜(fl,bi)+θ˜(fl) , (22)
t ⋆ t t t t
wherewehavedefined
θ˜(tr) =(cid:81)t Γ¯(η) {θ −θ }, (23)
t s=1 s,H 0 ⋆
θ˜(bi,bi) =(cid:80)t (cid:0) Γ¯(η)(cid:1)t−s ρ¯ ,
t s=1 H H
θ˜(fl,bi) =(cid:80)t (cid:81)t Γ¯(η)τ¯ +∆(η) ρ¯ ,
t s=1 i=s+1 i,H s,H H,s,t H
θ˜(fl) =η(cid:80)t (cid:81)t Γ¯(η)φ¯ .
t s=1 i=s+1 i,H s,H
Nowweneedtoupperboundeachofthetermsindecomposition(22). Thisisdoneinasequenceoflemmasbelow: θ˜(fl)
t
isboundedinLemmaA.1,θ˜(fl,bi) inLemmaA.2,θ˜(tr) inLemmaA.3,andθ˜(bi,bi) inLemmaA.4. Thenwecombinethe
t t t
boundsinordertostateaversionofTheorem4.1withexplicitconstantsinTheoremA.5.
LemmaA.1. AssumeA1andA2. Then,foranystepsizeη ∈(0,η )itholds
∞
E(cid:2) ∥θ˜(fl)∥2(cid:3) ≤ ησ¯ ε .
t aN(1−e−2)
Proof. We start from the decomposition (23). With the definition of θ˜ t(fl) and EF s+ +1,1(cid:104)(cid:8)(cid:81)t i=s+1Γ¯( i,η H)(cid:9) φ¯ s,H(cid:105) = 0, we
obtainthat
t t
E(cid:2) ∥θ˜(fl)∥2(cid:3) =η2(cid:88) E(cid:2) ∥(cid:8) (cid:89) Γ¯(η)(cid:9)
φ¯
∥2(cid:3)
.
t i,H s,H
s=1 i=s+1
Now,usingtheassumptionA2andMinkowski’sinequality,weobtainthat
t N t−1 t−1
E1/2(cid:2) ∥(cid:8) (cid:89) Γ¯(η)(cid:9)
φ¯
∥2(cid:3)
≤
1 (cid:88) E1/2∥(cid:2) Γ¯(c,η)(cid:8) (cid:89) Γ¯(η)(cid:9)
φ¯
∥2(cid:3)( ≤a) (1−ηa)HE1/2(cid:2) ∥(cid:8) (cid:89) Γ¯(η)(cid:9)
φ¯
∥2(cid:3)
.
i,H s,H N t,H i,H s,H i,H s,H
i=s+1 c=1 i=s+1 i=s+1
(24)
In(a)appliedA2conditionallyonF− . Hence,byinductionwegetfromthepreviousformulasthat
t−1,H
t
E(cid:2) ∥θ˜(fl)∥2(cid:3) ≤η2(cid:88) (1−ηa)HE[∥φ¯ ∥2]. (25)
t s,H
s=1
NowweproceedwithboundingE(cid:2)
∥φ¯
∥2(cid:3)
. Indeed,sincetheclientsareindependent,wegetusing(14)that
s,H
E(cid:2) ∥φ¯ ∥2(cid:3) = 1 (cid:88)N E(cid:2) ∥(cid:88)H Γ(c,η) εc(Zc )∥2(cid:3)
s,H N2 c=1 h=1 s,h+1:H s,h
(cid:20) (cid:21)
= 1 (cid:88)N (cid:88)H E(cid:2) ∥Γ(c,η) εc(Zc )∥2(cid:3)
N2 c=1 h=1 s,h+1:H s,h
≤ 1 (cid:88)N (cid:88)H (1−ηa)2(H−h)E(cid:2) ∥εc(Zc )∥2(cid:3) .
N2 c=1 h=1 s,h
Therefore,using(5)andthefollowinginequality,
H−1
(cid:88) 1
(1−ηa)2h ≤H ∧ , forallη ≥0,suchthatηa≤1,
ηa
h=0
12SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
weget
E(cid:2) ∥φ¯ ∥2(cid:3) ≤ 1 (cid:16) H ∧ 1 (cid:17) σ¯ .
s,H N ηa ε
Pluggingthisinequalityin(25),weget
E(cid:2) ∥θ˜(fl)∥2(cid:3) ≤ E c[Tr(Σc ε)](cid:16) η2H ∧ η(cid:17)(cid:88)t (cid:2) (1−ηa)2H(t−s)(cid:3)
t N a
s=1
σ¯ (cid:16) η(cid:17) 1
≤ ε η2H ∧
N a 1−(1−ηa)2H
ησ¯ 1
≤ ε (ηaH ∧1) ,
aN 1−e−2ηaH
whereweusedadditionally
e−2x ≤1−x≤e−x , (26)
whichisvalidforx∈[0;1/2]. Nowitremainstonoticethat
x∧1 1
≤
1−e−2x 1−e−2
foranyx>0.
Weproceedwithanalyzingthefluctuationofthetruebiascomponentoftheerrorθ definedin(23). Thefirststeptowards
t
thisistoobtaintherespectiveboundforτ¯ ,s ∈ {1,...,T},whereτ¯ isdefinedin(14). Nowweprovideanupper
s,H s,H
boundforθ˜(fl,bi):
t
LemmaA.2. AssumeA1andA2. Then,foranystepsizeη ∈(0,η )itholds
∞
(cid:113)
(cid:114) 2 E ∥Σc ∥∥ρ¯ ∥
E1/2(cid:2) ∥θ˜(fl,bi)∥2(cid:3) ≤ 2ηv˜ heter + c A˜ H .
t Na aH1/2N1/2
Proof. Recallthatθ˜(fl,bi)isgiven(see(23))by
t
t t (cid:32) t t (cid:33)
θ˜(fl,bi) =(cid:88) (cid:89) Γ¯(η)τ¯ + (cid:88)(cid:8) (cid:89) Γ¯(η)(cid:9) −(Γ¯(η))t−s ρ¯ , (27)
t i,H s,H i,H H H
s=1i=s+1 s=1 i=s+1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
T1 T2
whereτ¯ andρ¯ aredefinedin(14). WebeginwithboundingT . Inordertodoitwefirstneedtoboundτ¯ . Sincethe
s,H H 1 s,H
differentagentsareindependent,wehave
N
E[∥τ¯ ∥2]= 1 (cid:88) E[∥((I−ηA¯c)H −Γ(c,η) ){θc−θ }∥2]. (28)
s,H N2 s,1:H ⋆ ⋆
c=1
ApplyingLemmaC.1andthefactthat(cid:8) (I−ηA¯c)h−1A˜c(Zc )Γ(c,η) (θc−θ )(cid:9)H isamartingale-differencew.r.t.
s,h s,(h+1):H ⋆ ⋆ h=1
F− ,wegetthat
s,h
E[∥((I−ηA¯c)H −Γ(c,η) ){θc−θ }∥2]
s,1:H ⋆ ⋆
H
=η2E[∥(cid:88) (I−ηA¯c)h−1A˜c(Zc )Γ(c,η) {θc−θ }∥2]
s,h s,(h+1):H ⋆ ⋆
h=1
H
=η2(cid:88) E[∥(I−ηA¯c)h−1A˜c(Zc )Γ(c,η) {θc−θ }∥2]
s,h s,(h+1):H ⋆ ⋆
h=1
H
≤η2(cid:88) (1−ηa)2(h−1){θc−θ }⊤E[(Γ(c,η) )⊤(A˜c(Zc ))⊤A˜c(Zc )Γ(c,η) ]{θc−θ }.
⋆ ⋆ s,(h+1):H s,h s,h s,(h+1):H ⋆ ⋆
h=1
13SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
UsingthetowerpropertyconditionallyonF+ ,weget
s,h+1
E[(Γ(c,η) )⊤(A˜c(Zc ))⊤A˜c(Zc )Γ(c,η) ]=E[(Γ(c,η) )⊤Σc Γ(c,η) ],
s,(h+1):H s,h s,h s,(h+1):H s,(h+1):H A˜ s,(h+1):H
whereΣc A˜ isthenoisecovariancematrixdefinedin(6). Sinceforanyvectoru∈Rdwehave∥u∥ Σc A˜ ≤∥Σc A˜∥1/2∥u∥,we
get
H
E[∥((I−ηA¯c)H −Γ(c,η) ){θc−θ }∥2]≤η2(cid:88) (1−ηa)2(h−1)E(cid:2) ∥Γ(c,η) {θc−θ }∥2 (cid:3) (29)
s,1:H ⋆ ⋆ s,(h+1):H ⋆ ⋆ Σc
A˜
h=1
H
≤η2∥Σc ∥(cid:88) (1−ηa)2(h−1)E(cid:2) ∥Γ(c,η) {θc−θ }∥2(cid:3)
A˜ s,(h+1):H ⋆ ⋆
h=1
≤Hη2(1−ηa)2(H−1)∥Σc ∥∥θc−θ ∥2 .
A˜ ⋆ ⋆
Combiningtheaboveboundsin(28)yieldsthat
Hη2(1−ηa)2(H−1)(cid:80)N ∥Σc ∥∥θc−θ ∥2
E(cid:2)
∥τ¯
∥2(cid:3)
≤
c=1 A˜ ⋆ ⋆
. (30)
s,H N2
Thus,proceedingasin(24)togetherwith(30),weget
E[∥T ∥2]=(cid:88)t E[∥(cid:89)t Γ¯(η)τ¯ ∥2]
1 i,H s,H
s=1 i=s+1
Hη2(1−ηa)2(H−1)(cid:80)N ∥Σc ∥∥θc−θ ∥2
≤(cid:88)t c=1 A˜ ⋆ ⋆ (1−ηa)2H(t−s)
s=1 N2
Hη2(1−ηa)2(H−1)
≤ E [∥Σc ∥∥θc−θ ∥2]
(1−(1−ηa)2H)N c A˜ ⋆ ⋆
η Haηe−2Haη
≤ E [∥Σc ∥∥θc−θ ∥2]
aN(1−ηa)2 1−e−2Haη c A˜ ⋆ ⋆
2η
≤ E [∥Σc ∥∥θc−θ ∥2].
Na c A˜ ⋆ ⋆
Intheboundaboveweused(26)togetherwiththebound
xe−2x 1
≤ ,x≥0.
1−e−2x 2
Nowweboundthesecondpartofθ˜(fl,bi)in(27),thatis,T . Tobeginwith,westartwithapplyingLemmaC.1andweget
t 2
foranys∈{1,...,t}andi∈{s+1,...,t},that
t t t
(cid:8) (cid:89) Γ¯(η)(cid:9) −(Γ¯(η))t−s)ρ¯ = (cid:88) (cid:8) (cid:89) Γ¯(η)(cid:9) (Γ¯(η) −Γ¯(η))(Γ¯(η))i−s−1ρ¯ .
i,H H H r,H i,H H H H
i=s+1 i=s+1 r=i+1
Notethat,
t
EF i+ +1,1[(cid:8) (cid:89) Γ¯( rη ,H)(cid:9) (Γ¯( i,η H) −Γ¯( Hη))(Γ¯( Hη))i−s−1ρ¯ H]=0 (31)
r=i+1
Proceedingasin(29),wegetusingindependencebetweenagentsforanyu∈Rd,
N
E[∥(Γ¯(η) −Γ¯(η))u∥2]= 1 E[∥(cid:88) (Γ(c,η) −(I−ηA¯c)H)u∥2]
i,H H N2 s,1:H
c=1
N
= 1 (cid:88) E[∥(Γ(c,η) −(I−ηA¯c)H)u∥2]
N2 s,1:H
c=1
Hη2(1−ηa)2(H−1) (cid:32) 1 (cid:88)N (cid:33)
≤ ∥Σc ∥ ∥u∥2 .
N N A˜
c=1
14SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Hence,using(31),weget
t Hη2(1−ηa)2H(t−s)−2E ∥Σc ∥
E[∥(cid:0)(cid:8) (cid:89) Γ¯(η)(cid:9) −(Γ¯(η))t−s(cid:1) ρ¯ ∥2]= c A˜ ∥ρ¯ ∥2 .
i,H H H N H
i=s+1
Combiningtheaboveestimatesin(27),andusingMinkowski’sinequality,weget
H1/2η (cid:113) (cid:88)t−1
E1/2[∥T ∥2]≤ E ∥Σc ∥∥ρ¯ ∥ (1−ηa)H(t−s)
2 (1−ηa)N1/2 c A˜ H
s=1
2 Haηe−Haη(cid:113)
≤ E ∥Σc ∥∥ρ¯ ∥
aH1/2N1/2 1−e−Haη c A˜ H
2 (cid:113)
≤ E ∥Σc ∥∥ρ¯ ∥ ,
aH1/2N1/2 c A˜ H
whereweusedthatηa≤1/2and
xe−x
≤1, x≥0.
1−e−x
andthestatementfollows.
LemmaA.3. AssumeA1andA2. Thenforanystepsizeη ∈(0,η )wehave
∞
E1/2[∥θ˜(tr)∥2]≤(1−ηa)tH∥θ −θ ∥
t 0 ⋆
Proof. Proceedingasin(24)foranyu∈Rdwehave
t
E1/2[∥(cid:89) Γ¯(η) u∥2]≤(1−ηa)tH∥u∥
s,H
s=1
Usingthisresultforu=θ −θ wegetthestatement.
0 ⋆
LemmaA.4. AssumeA1andA2. Thenforanyη ∈(0,η )wehave
∞
∥θ˜(bi,bi)−(I−Γ¯(η))−1ρ¯ ∥ ≤(1−ηa)tH∥(I−Γ¯(η))−1∥∥ρ¯ ∥
t H H H H
Proof. UsingA2andMinkowski’sinequalitty,weget
∥θ˜(bi,bi)−(I−Γ¯(η))−1ρ¯ ∥ =∥(I−Γ¯(η))−1(Γ¯(η))tρ¯ ∥
t H H H H H
≤∥(I−Γ¯(η))−1∥∥(Γ¯(η))tρ¯ ∥
H H H
N
≤∥(I−Γ¯(η))−1∥ 1 (cid:88) ∥(I−ηA¯c)H(Γ¯(η))t−1ρ¯ ∥
H N H H
c=1
≤(1−ηa)H∥(I−Γ¯(η))−1∥∥(Γ¯(η))t−1ρ¯ ∥
H H H
andthestatementfollows.
TheoremA.5. AssumeA1andA2. Thenforanystepsizeη ∈(0,η )itholdsthat
∞
(cid:113)
E1/2(cid:2) ∥θ −θ˜(bi,bi)−θ ∥2(cid:3)
≤(cid:114)
ησ¯ ε
+(cid:114)
2ηv˜ heter +
2 E c∥Σc A˜∥∥ρ¯ H∥
+(1−ηa)tH∥θ −θ ∥ , (32)
t t ⋆ aN(1−e−2) Na aH1/2N1/2 0 ⋆
wherethebiasθ˜(bi,bi)convergesto(I−Γ¯(η))−1ρ¯ atarate
t H H
∥θ˜(bi,bi)−(I−Γ¯(η))−1ρ¯ ∥ ≤(1−ηa)tH∥(I−Γ¯(η))−1∥∥ρ¯ ∥ .
t H H H H
15SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Proof. ProoffollowsbycombiningtheresultsLemmaA.1-LemmaA.4above.
CorollaryA.6. AssumeA1andA2. LetH = 1,thenforany0 < ϵ < 1,inordertoachieveE(cid:2) ∥θ −θ ∥2(cid:3) ≤ ϵ2 the
T ⋆
requirednumberofcommunicationsis
(cid:18) (cid:19)
v˜ ∨σ¯ ∥θ −θ ∥
T =O heter ε log 0 ⋆
Na2ϵ2 ϵ
numberofcommunications,settingthestepsize
aNϵ2
η = . (33)
0 v˜ ∨σ¯
heter ε
Proof. Boundingthefirsttwotermsindecomposition(32)wegetthatthestepsizeshouldsatisfy
aNϵ2
η ≤ .
v˜ ∨σ¯
heter ε
Fromthelasttermwehave
1 ∥θ −θ ∥ v˜ ∨σ¯ ∥θ −θ ∥
t≥ log 0 ⋆ ≥ heter ε log 0 ⋆
aη ϵ Na2ϵ2 ϵ
Corollary A.7. Assume A1 and A2. For any 0 ≤ ϵ ≤ C− A1E c∥θ ⋆c−θ⋆∥ ∨(cid:16)√ v˜heter∨σ¯εE c∥θ ⋆c−θ⋆∥(cid:17)2/5 in order to achieve
a a
E(cid:2) ∥θ −θ ∥2(cid:3) <ϵ2therequirednumberofcommunicationsis
T ⋆
(cid:18)E ∥θc−θ ∥ ∥θ −θ ∥(cid:19)
T =O c ⋆ ⋆ log 0 ⋆ , (34)
a2ϵ ϵ
settingthestepsize
(cid:18) aNϵ2 (cid:19)
η =O
v˜ ∨σ¯
heter ε
andnumberoflocaliterations
(cid:18) (cid:19)
v˜ ∨σ¯
H =O heter ε
NϵE ∥θc−θ ∥
c ⋆ ⋆
Proof. Weaimtoboundseparatelyallthetermsinther.h.s. ofTheorem4.1. Notethatitrequirestosetη ∈(0;η )withη
0 0
givenin(33)inordertofulfillthebounds
(cid:114) (cid:114)
ηv˜ ησ¯
heter ≲ε, ε ≲ε.
aN aN
Now,weshouldboundthebiasterm
E1/2[∥θ˜(bi,bi)∥2]≤(1+(1−ηa)tH)∥(I−Γ¯(η))−1ρ¯ ∥ ≤2∥(I−Γ¯(η))−1ρ¯ ∥ .
t H H H H
Thus,usingtheNeumanseries,wecanboundthenormofthetermaboveas
∞ ∞
∥(I−Γ¯(η))−1ρ¯ ∥ =∥(cid:88) (Γ¯(η))kρ¯ ∥ ≤(cid:88) (1−ηa)Hk∥ρ¯ ∥ ≤ ∥ρ¯ H∥ .
H H H H H 1−(1−ηa)H
k=0 k=0
Hence,usingtheboundof(16),weget
2∥ρ¯ ∥ ηaH ηHE [exp(ηH∥A¯c∥)∥θc−θ ∥]
E1/2[∥θ˜(bi,bi)∥2]≤ H ≤ c ⋆ ⋆
t 1−(1−ηa)H 1−(1−ηa)H a
2ηHE [exp(ηH∥A¯c∥)∥θc−θ ∥] ηHE [∥θc−θ ∥]
≤ c ⋆ ⋆ ≲ c ⋆ ⋆ ,
a a
16SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Algorithm3SCAFFLSA-H1: StochasticControlledFedLSA withprobabilisticcommunication
Input: η >0,θ ,ξc ∈Rd,T,N,H,p>0
0 0
Set: K =T/p
fork =1toK do
forc=1toN do
ReceiveZc andperformlocalupdate:
k
θˆc =θˆc −η(Ac(Zc)θˆc −bc(Zc)−ξc )
k k−1 k k−1 k k−1
endfor
DrawB ∼Bernoulli(p)
k
ifB =1then
k
Averagelocaliterates: θc = 1 (cid:80)N θˆc
k N c=1 k
Update: ξc =ξc + p(θc −θˆc)
k k−1 η k k
else
Set: θc =θˆc,ξc =ξc
k k k k−1
endif
endfor
whereweusedthefactthatthestepsizeηischoseninordertosatisfyηHC ≤1.ThusinordertofulfillE1/2[∥θ˜(bi,bi)∥2]≲
A t
εweneedtochooseηandH suchthat
ηHE [∥θc−θ ∥]≤εa.
c ⋆ ⋆
Itremainstoboundtheterm
(cid:113)E
c∥Σc A˜∥∥ρ¯H∥
. Usingtheboundof(16),weget
aH1/2N1/2
(cid:113) (cid:113)
E c∥Σc A˜∥∥ρ¯ H∥ ≤(cid:114)
η
×
E c∥Σc A˜∥(ηH)3/2 ≲ε5/2(cid:114)
1 a
.
aH1/2N1/2 N a v˜ ∨σ¯ E [∥θc−θ ∥]
heter ε c ⋆ ⋆
Hence,itremainstocombinetheboundsaboveinordertogetthesamplecomplexityresult(34).
CorollaryA.8. AssumeTD1andTD2. Thenforany
√
0≤ϵ≤
2(cid:16)√
2(1+γ) E c∥θ ⋆c−θ⋆∥2∨(1+E c[∥θ⋆∥2])E c[∥θ
⋆c−θ⋆∥](cid:17)2/5
∨ 2E c[∥θ ⋆c−θ⋆∥] ,
(1−γ)ν (1−γ)ν(1+γ)
inordertoachieveE(cid:2) ∥θ −θ ∥2(cid:3) <ϵ2therequirednumberofcommunicationsforfederatedTD(0)algorithmis
T ⋆
T =O(cid:16)(cid:16) 1 ∨ E c[∥θ ⋆c−θ⋆∥](cid:17) log∥θ0−θ⋆∥(cid:17) .
(1−γ)2ν (1−γ)2ν2ϵ ϵ
B.FederatedLinearStochasticApproximationwithControlVariates
B.1.Probabilisticcommunication(AssumptionH1)
Tomitigatethebiascausedbylocaltraining,wemayusecontrolvariates. Weassumeinthissectionthatateachiteration
wechoose,withprobabilityp,whetheragentsshouldcommunicateornot. Considerthefollowingalgorithm,wherefor
k =1,...,T/p,wecompute
θˆc =θc −η(Ac(Zc)θc −bc(Zc)−ξc )
k k−1 k k−1 k k−1
i.e. weupdatethelocalparameterswithLSAadjustedwithacontrolvariateξc . Thiscontrolvariateisinitializedtozero,
k−1
andupdatedaftereachcommunicationround. WedrawaBernoullirandomvariableB withsuccessprobabilitypandthen
k
updatetheparameterasfollows:
(cid:40) θ¯ = 1 (cid:80)N θˆc B =1
θc = k N c=1 k k
k θˆc B =0
k k
17SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Wethenupdatethecontrolvariate
p
ξc =ξc + (θc −θˆc). (35)
k k−1 η k k
wherewehavesetξc =0. Forclarity,werestateAlgorithm2underthisassumptionasAlgorithm3.
0
Byconstruction,θc isthecurrentvalueofparametertheparameterattimek. Thecontrolvariatestaysconstantbetweentwo
k
successiveconsensussteps. Notethat,forallk ∈N,(cid:80)N ξc =0. Indeed,ifB =0,foranyc∈{1,...,N},ξc =ξc .
c=1 k k k k−1
Additionally,ifB =1,wehavefrom(35)that
k
N N
1 (cid:88) ξc = 1 (cid:88) ξc + p (θ¯ −θ¯ )=0.
N k N k−1 η k k
c=1 c=1
Wenowproceedtotheproof,whichamountstoconstructingacommonLyapunovfunctionforthesequences{θ kc} k∈Nand
{ξ kc} k∈N. DefinetheLyapunovfunction,
1 (cid:88)N η2 1 (cid:88)N
ψ = ∥θc −θ ∥2+ ∥ξc −ξc∥2 ,
k N k ⋆ p2 N k ⋆
c=1 c=1
whereθ isthesolutionofA¯θ =b¯,andξc =A¯c(θ −θc). Anaturalmeasureofheterogeneityisthengivenby
⋆ ⋆ ⋆ ⋆ ⋆
N N
∆ = 1 (cid:88) ∥ξc∥2 = 1 (cid:88) ∥A¯c(θc−θ )∥2 .
heter N ⋆ N ⋆ ⋆
c=1 c=1
LemmaB.1(Onestepprogress). AssumeA1andA2(2). Assumethatη ≤ 1 . Theiteratesofthealgorithmdescribed
2L
abovesatisfy
E[ψ ]≤(cid:16) 1−min(cid:0) ηa,p2(cid:1)(cid:17) E[ψ ]+ 2η2 (cid:88)N Tr(Σc).
k k−1 N ε
c=1
Proof. Decompositionoftheupdate. Remarkthattheupdatecanbereformulatedas
θˆc −θ =(I−ηAc(Zc))(θc −θ )+η(ξc −ξc)−ηωc(Zc), (36)
k ⋆ k k−1 ⋆ k−1 ⋆ k
whereωc(z)=A˜c(z)θ −b˜c(z). Thiscomesfromthefactthat,forallz,
⋆
bc(z)+ξc =b¯c+b˜c(z)+ξc
k−1 k−1
=A¯cθc+b˜c(z)+ξc
⋆ k−1
=A¯cθ +b˜c(z)+ξc −ξc
⋆ k−1 ⋆
=Ac(z)θ −A˜c(z)θ +b˜c(z)+ξc −ξc
⋆ ⋆ k−1 ⋆
=Ac(z)θ −ωc(z)+ξc −ξc .
⋆ k−1 ⋆
Expressionofcommunicationsteps. Usingthat(cid:80)N ξc =0and(cid:80)N ξc =0,weget
c=1 k−1 c=1 ⋆
N N
1 (cid:88) ∥θc −θ ∥2 =1 (B )∥θ¯ −θ ∥2+1 (B ) 1 (cid:88) ∥θˆc −θ ∥2
N k ⋆ {1} k k ⋆ {0} k N k ⋆
c=1 c=1
N N N
=1 (B )∥ 1 (cid:88) (θˆc − η ξc )− 1 (cid:88) (θ − η ξc)∥2+1 (B ) 1 (cid:88) ∥θˆc −θ ∥2 .
{1} k N k p k−1 N ⋆ p ⋆ {0} k N k ⋆
c=1 c=1 c=1
ThefirsttermcanbeupperboundedbyusingLemmaC.4,whichgives
(cid:40) N N (cid:41)
1 (B )∥θ¯ −θ ∥2 =1 (B ) 1 (cid:88) ∥θˆc − η (ξc −ξc)−θ ∥2− 1 (cid:88) ∥θ¯ −(θˆc − η ξc )+ η ξc∥2
{1} k k ⋆ {1} k N k p k−1 ⋆ ⋆ N k k p k−1 p ⋆
c=1 c=1
=1 (B
)(cid:40) 1 (cid:88)N
∥θˆc −
η
(ξc −ξc)−θ ∥2−
η2 1 (cid:88)N
∥ξc
−ξc∥2(cid:41)
.
{1} k N k p k−1 ⋆ ⋆ p2 N k ⋆
c=1 c=1
18SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Wenowexpandthefirsttermintheright-handsideofthepreviousequation. Thisgives
1 (cid:88)N
∥θˆc −
η
(ξc −ξc)−θ ∥2 =
1 (cid:88)N (cid:26)
∥θˆc −θ ∥2−
2η
⟨ξc −ξc, θˆc −θ ⟩+
η2
∥ξc
−ξc∥2(cid:27)
,
N k p k−1 ⋆ ⋆ N k ⋆ p k−1 ⋆ k ⋆ p2 k−1 ⋆
c=1 c=1
whichyields
1 (B ){ψ }=1 (B
)(cid:40)
∥θ¯ −θ ∥2+
η2 1 (cid:88)N
∥ξc
−ξc∥2(cid:41)
{1} k k {1} k k ⋆ p2 N k ⋆
c=1
=1 (B
)(cid:40) 1 (cid:88)N
∥θˆc −θ ∥2−
2η
⟨ξc −ξc, θˆc −θ ⟩+
η2 1 (cid:88)N
∥ξc
−ξc∥2(cid:41)
. (37)
{1} k N k ⋆ p k−1 ⋆ k ⋆ p2 N k−1 ⋆
c=1 c=1
Ontheotherhand,notethat
(cid:40) 1 (cid:88)N η2 1 (cid:88)N (cid:41)
1 (B ){ψ }=1 (B ) ∥θc −θ ∥2+ ∥ξc −ξc∥2
{0} k k {0} k N k ⋆ p2 N k ⋆
c=1 c=1
=1 (B
)(cid:40) 1 (cid:88)N
∥θˆc −θ ∥2+
η2 1 (cid:88)N
∥ξc
−ξc∥2(cid:41)
. (38)
{0} k N k ⋆ p2 N k−1 ⋆
c=1 c=1
Bycombining(38)and(37),weget
1 (cid:88)N η2 1 (cid:88)N
ψ = ∥θc −θ ∥2+ ∥ξc −ξc∥2
k N k ⋆ p2 N k ⋆
c=1 c=1
=
1 (cid:88)N
∥θˆc −θ
∥2−2η
1 (B )⟨ξc −ξc, θˆc −θ ⟩+
η2 1 (cid:88)N
∥ξc −ξc∥2 . (39)
N k ⋆ p {1} k k−1 ⋆ k ⋆ p2 N k−1 ⋆
c=1 c=1
Progressinlocalupdates. Wenowboundthefirsttermofthesumin(39). Forc∈[N],(36)gives
∥θˆc −θ ∥2 =∥(I−ηAc(Zc))(θc −θ )+η(ξc −ξc)−ηωc(Zc)∥2
k ⋆ k k−1 ⋆ k−1 ⋆ k
=∥(I−ηAc(Zc)){θc −θ }−ηωc(Zc)∥2+η2∥ξc −ξc∥2
k k ⋆ k k−1 ⋆
+2η⟨ξc −ξc, (I−ηAc(Zc)){θc −θ }−ηωc(Zc)⟩
k−1 ⋆ k k ⋆ k
=∥(I−ηAc(Zc)){θc −θ }−ηωc(Zc))∥2+2η⟨ξc −ξc, θˆc −θ ⟩−η2∥ξc −ξc∥2 . (40)
k k ⋆ k k−1 ⋆ k ⋆ k−1 ⋆
(cid:124) (cid:123)(cid:122) (cid:125)
T1
Definetheσ-algebraG =σ(B ,s≤k−1,Zc,s≤k−1,c∈[N]). WenowboundtheconditionalexpectationofT
k−1 s s 1
EGk−1[T 1]=EGk−1(cid:2) ∥(I−ηAc(Z kc)){θ kc −θ ⋆}∥2−2η⟨(I−ηAc(Z kc)){θ kc −θ ⋆}, ωc(Z kc)⟩+η2∥ωc(Z kc)∥2(cid:3)
=EGk−1(cid:2) ∥(I−ηAc(Z kc)){θ kc −θ ⋆}∥2+2η2⟨Ac(Z kc){θ kc −θ ⋆}, ωc(Z kc)⟩+η2∥ωc(Z kc)∥2(cid:3) ,
whereweusedthefactthat⟨I, ωc(Zc)⟩ = 0. UsingYoung’sinequalityforproducts,andLemmaC.5withη ≤ 1 and
k 2L
u=θc −θ ,wethenobtain
k ⋆
EGk−1[T 1]≤EGk−1(cid:2) ∥(I−ηAc(Z kc)){θ kc −θ ⋆}∥2+η2∥Ac(Z kc){θ kc −θ ⋆}∥2+η2∥ωc(Z kc)∥2+η2∥ωc(Z kc)∥2(cid:3)
≤(1−ηa)∥θ kc −θ ⋆∥2−η( L1 −2η)EGk−1(cid:2) ∥Ac(Z kc){θ kc −θ ⋆}∥2(cid:3) +2η2EGk−1(cid:2) ∥ωc(Z kc)∥2(cid:3) . (41)
Plugging(41)in(40)andusingtheassumptionη ≤ 1 ,weobtain
2L
(cid:104) (cid:105)
EGk−1 ∥θˆ kc −θ ⋆∥2−2η⟨ξ kc −1−ξ ⋆c, θˆ kc −θ ⋆⟩ ≤(1−ηa)∥θ kc −θ ⋆∥2−η2∥ξ kc −1−ξ ⋆c∥2+2η2Tr(Σc ε). (42)
19SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
BoundingtheLyapunovfunction. Takingthecondtionalexpectationof(39)andusing(42)forc=1toN,weobtainthe
followingboundontheLyapunovfunction
EGk−1[ψ k]=
N1 (cid:88)N EGk−1(cid:104)
∥θˆ kc −θ ⋆∥2−2η⟨ξ kc −1−ξ ⋆c, θˆ kc −θ
⋆⟩(cid:105)
+
η p22 N1 (cid:88)N
∥ξ kc −1−ξ ⋆c∥2
c=1 c=1
≤ 1 (cid:88)N (cid:2) (1−ηa)∥θc −θ ∥2−η2∥ξc −ξc∥2+2η2Tr(Σc)(cid:3) + η2 1 (cid:88)N ∥ξc −ξc∥2
N k ⋆ k−1 ⋆ ε p2 N k−1 ⋆
c=1 c=1
1 (cid:88)N η2 1 (cid:88)N 2η2 (cid:88)N
=(1−ηa) ∥θc −θ ∥2+(1−p2) ∥ξc −ξc∥2+ Tr(Σc),
N k ⋆ p2 N k−1 ⋆ N ε
c=1 c=1 c=1
andtheresultoftheLemmafollowsfromtheTowerproperty.
TheoremB.2(Convergencerate). AssumeA1andA2(2). Then,foranyη ≤ 1 andT >0,itholds
2L
E[ψ ]≤(cid:0) 1−ζ(cid:1)K(cid:18) ∥θ −θ ∥2+ η2 ∆ (cid:19) + 2η2 1 (cid:88)N Tr(Σc),
K 0 ⋆ p2 heter ζ N ε
c=1
whereζ
=min(cid:0) ηa,p2(cid:1)
.
CorollaryB.3(Iterationcomplexity). Letϵ>0. Setη =min(cid:0) 1 , ϵ2a(cid:1) andp=√ ηa(sothatζ =ηa). Then,E[ψ ]≤ϵ2
2L 8σ¯ε K
aslongasthenumberofiterationsis
K
≥max(cid:18) 2L
,
4σ¯
ε
(cid:19) log(cid:32) ∥θ 0−θ ⋆∥2+min(cid:0) 2a1 L, 8ϵ σ¯2 ε(cid:1) ∆ heter(cid:33)
,
a ϵ2a2 2ϵ2
whichcorrespondstoanexpectednumberofcommunicationrounds
T
≥max(cid:32)(cid:114) 2L ,(cid:114) 4σ¯
ε
(cid:33) log(cid:32) ∥θ 0−θ ⋆∥2+min(cid:0) 2a1 L, 8ϵ σ¯2 ε(cid:1) ∆ heter(cid:33)
.
a ϵ2a2 2ϵ2
TheoremB.4(Nolinearspeedupintheprobabilisticcommunicationsettingwithcontrolvariates). Theboundsobtainedin
TheoremB.2areminimaxoptimaluptoconstantsthatareindependentfromtheproblem. Precisely,forevery(p,η)there
existsaFLSAproblemsuchthat
E[ψ ]=(cid:0)
1−ζ(cid:1)K(cid:18)
∥θ −θ ∥2+
η2
∆
(cid:19)
+
2η2
σ¯ ,
K 0 ⋆ p2 heter ζ ε
wherewehavedefinedζ
=min(cid:0) 2ηa,p2(cid:1)
.
Proof. Defineforallc∈[N],
A¯c =aI, b¯c =b u,
c
whereuisavectorwhomallcoordinatesareequalto1. Wealsoconsiderthesequenceofi.i.drandomvariables(Zc)such
k
thatthatforallc∈[N]and0≤t≤T,Zc followsaRademacherdistribution. Moreover,wedefine
k
Ac(Zc)=A¯c , bc(Zc)=b¯c+Zcu.
k k k
Inparticularthisimplies
ωc(z)=Zcu.
k
20SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Algorithm4SCAFFLSA-H2: StochasticControlledFedLSA withdeterministiccommunication
Input: η >0,θ ,ξc ∈Rd,T,N,H
0 0
Set: K =TH
fort=1toT do
forc=1toN do
forh=1toH do
ReceiveZc andperformlocalupdate:
t,h
θˆc =θˆc −η(Ac(Zc )θˆc −bc(Zc )−ξc)
t,h t,h−1 t,h t,h−1 t,h t
endfor
Averagelocaliterates: θc = 1 (cid:80)N θˆc
t+1 N c=1 t,H
Update: ξc =ξc+ 1 (θc −θˆc )
t+1 t ηH t+1 t,H
endfor
endfor
WefollowthesameproofofLemmaB.1untilthechainofequalitiesbreaks. Thereby,westartfrom
(cid:88)N η2 (cid:88)N
E[ψ ]=E[ ∥θc −θ ∥2+ ∥ξc −ξc∥2]
k k ⋆ p2 k ⋆
c=1 c=1
(cid:88)N η2 (cid:88)N
=E[ ∥(I−ηAc(Zc)){θc −θ }−ηωc(Zc))∥2+(1−p2) ∥ξc −ξc∥2]
k k−1 ⋆ k p2 k−1 ⋆
c=1 c=1
=E[(cid:88)N
∥(I−ηA¯c){θc −θ
}−ηωc(Zc))∥2+(1−p2)η2 (cid:88)N
∥ξc −ξc∥2]
k−1 ⋆ k p2 k−1 ⋆
c=1 c=1
(cid:88)N η2 (cid:88)N
=E[ (1−ηa)2∥θc −θ ∥2+η2∥ωc(Zc)∥2+(1−p2) ∥ξc −ξc∥2]
k−1 ⋆ k p2 k−1 ⋆
c=1 c=1
whereweusedthatAc(Zc)=A¯c. Unrollingtherecursiongivesthedesiredresult.
k
B.2.Deterministiccommunication(AssumptionH2)
Considerthesamealgorithmasinthepreviouspart,exceptthatthevariablesB arechosensuchthatB = 1ifk isa
k k
multipleofH andB =1otherwise. ThisamountstomakingblocksoflocalupdatesofconstantsizeH. Forclarity,we
k
thusdefinetastheintegerpartofk/H andh=k mod H. Thus,foragivent≥0,eachagentc∈[N]performsH local
updatesasfollows
θˆc =θˆc −η(Ac(Z )θˆc −bc(Z )−ξc),
t,h t,h−1 t,h t,h−1 t,h t
whereθˆ =θ andcontrolvariatesξcremainconstant. Attheendofeachblock,theglobaliterateandcontrolvariatesare
t,0 t t
updatedas
N
θ = 1 (cid:88) θc , ξc =ξc+ 1 (θ −θˆc ).
t+1 N t,H t+1 t ηH t+1 t,H
c=1
Forclarity,werestateAlgorithm2underthisassumptioninAlgorithm4. WeConsidertheLyapunovfunction,
η2H2 (cid:88)N
ψ =∥θ −θ ∥2+ ∥ξc−ξc∥2 ,
t t ⋆ N t ⋆
c=1
whichisnaturallydefinedastheerrorinθ estimationoncommunicationrounds, andtheaverageerroronthecontrol
⋆
variates.
21SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
TheoremB.5. AssumeA1andA2.Letη,HsuchthatηaH ≤1,ηH ≤1,η∥A¯c∥ ≤1,andη ≤ a ,
16Hmaxc∈[N]{∥A¯c∥2+∥Σc A˜∥}
andsetξ 0c =0forallc∈[N]. Then,thesequence(ψ t) t∈Nsatisfies,forallt≥0,
(cid:18) ηaH(cid:19)t
16η
E[ψ ]≤ 1− E[ψ ]+ σ¯ , (43)
t 4 0 a ε
whereψ =∥θ −θ ∥2+ η2H2 (cid:80)N ∥A¯c(θc−θ )∥2.
0 0 ⋆ N c=1 ⋆ ⋆
Proof. Expressionoflocalupdates. Similarlyto(36),werewritethelocalupdatesas,fort≥0,h≥0,c∈[N],
θˆc −θ =(I−ηAc(Zc ))(θˆc −θ )+η(ξc−ξc)−ηωc(Zc ),
t,h ⋆ t,h t,h−1 ⋆ t ⋆ t,h
whereωc(z)=A˜c(z)θ −b˜c(z). Unrollingthisrecursion,weget,foreachc∈[N],
⋆
H H
θc −θ =Γ(c,η)(θ −θ )+η(cid:88) Γ(c,η) (ξc−ξc)−η(cid:88) Γ(c,η) ωc(Zc ). (44)
t,H ⋆ t,1:H t ⋆ t,h+1:H t ⋆ t,h+1:H t,h
h=1 h=1
Expression of the Lyapunov function. Since the sum control variates is (cid:80)N ξc = (cid:80)N ξc = 0, we have θ =
t=1 t t=1 ⋆ t+1
1 (cid:80)N θc = 1 (cid:80)N θc −ηH(ξc−ξc). ApplyingLemmaC.4,weobtain
N c=1 t,H N c=1 t,H t ⋆
N
∥θ −θ ∥2 =∥ 1 (cid:88) θˆc −θ −ηH(ξc−ξc)∥2
t+1 ⋆ N t,H ⋆ t ⋆
c=1
N N
= 1 (cid:88) ∥θˆc −θ −ηH(ξc−ξc)∥2− 1 (cid:88) ∥θ −θc +ηH(ξc−ξc)∥2
N t,H ⋆ t ⋆ N t+1 t,H t ⋆
c=1 c=1
=
1 (cid:88)N
∥θˆc −θ −ηH(ξc−ξc)∥2−
η2H2 (cid:88)N
∥ξc −ξc∥2 ,
N t,H ⋆ t ⋆ N t+1 ⋆
c=1 c=1
sinceξc =ξc+ 1 (θ −θc ). Adding η2H2 (cid:80)N ∥ξc −ξc∥2onbothsidesandusing(44),weobtain
t+1 t ηH t+1 t,H N c=1 t+1 ⋆
N
ψ = 1 (cid:88) ∥θˆc −θ −ηH(ξc−ξc)∥2
t+1 N t,H ⋆ t ⋆
c=1
N H H
= 1 (cid:88) ∥Γ(c,η)(θ −θ )+η(cid:88) Γ(c,η) (ξc−ξc)−η(cid:88) Γ(c,η) ωc(Zc )−ηH(ξc−ξc)∥2
N t,1:H t ⋆ t,h+1:H t ⋆ t,h+1:H t,h t ⋆
c=1 h=1 h=1
N H
= 1 (cid:88) ∥Γ(c,η)(θ −θ )−ηH(I− 1Cc )(ξc−ξc)−η(cid:88) Γ(c,η) ωc(Zc )∥2 ,
N t,1:H t ⋆ H η,H t ⋆ t,h+1:H t,h
c=1 h=1
wherewedefinedCc =(cid:80)H Γ(c,η) . Expandingthenormgives
η,H h=1 t,h+1:H
N H
ψ = 1 (cid:88) ∥Γ(c,η)(θ −θ )−ηH(I− 1Cc )(ξc−ξc)∥2+∥η(cid:88) Γ(c,η) ωc(Zc )∥2
t+1 N t,1:H t ⋆ H η,H t ⋆ t,h+1:H t,h
c=1(cid:124) (cid:123)(cid:122) (cid:125) h=1
T 0c (cid:124) (cid:123)(cid:122) (cid:125)
Tc
1 (45)
H H
−2η⟨Γ(c,η)(θ −θ ), (cid:88) Γ(c,η) ωc(Zc )⟩+2ηH⟨(I− 1Cc )(ξc−ξc), η(cid:88) Γ(c,η) ωc(Zc )⟩ .
t,1:H t ⋆ t,h+1:H t,h H η,H t ⋆ t,h+1:H t,h
h=1 h=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Tc Tc
2 3
Inthefollowing,wewillusethefiltrationofalleventsuptostept,F :=σ(Zc ,0≤s≤t,0≤h≤H,1≤c≤N).
t s,h
22SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
BoundingTc. UsingYoung’sinequalitywithα >0,andAssumptionA2,wecanbound
0 0
E[Tc]≤E[(1+α )∥Γ(c,η)(θ −θ )∥2]+(1+α−1)η2H2E[∥(I− 1Cc )(ξc−ξc)∥2]
0 0 t,1:H t ⋆ 0 H η,H t ⋆
≤(1+α )(1−ηa)2HE[∥θ −θ ∥2]+(1+α−1)η2H2E[∥(I− 1Cc )(ξc−ξc)∥2].
0 t ⋆ 0 H η,H t ⋆
UsingLemmaC.6andYoung’sinequalityagain,andsinceη∥A¯c∥ ≤1wecanbound
E[∥(I− 1Cc )(ξc−ξc)∥2]=E[EFt−1(cid:2) ∥(I− 1Cc )(ξc−ξc)∥2(cid:3) ]
H η,H t ⋆ H η,H t ⋆
≤E[EFt−1(cid:2) ∥I− 1Cc ∥2(cid:3)E[∥ξc−ξc∥2]
H η,H t ⋆
(cid:16) (cid:17)
≤ 1[exp((H −1)η∥A¯c∥)−1]2+2η2(H −1)2∥Σc ∥ E[∥ξc−ξc∥2]
2 A˜ t ⋆
(cid:16) (cid:17)
≤ 2η2(H −1)2∥A¯c∥2+2η2(H −1)2∥Σc ∥ E[∥ξc−ξc∥2].
A˜ t ⋆
Wethusobtain:
E[Tc]≤(1+α )(1−ηa)2HE[∥θ −θ ∥2]+2(1+α−1)(∥A¯c∥2+∥Σc∥)η4H4E[∥ξc−ξc∥2]. (46)
0 0 t ⋆ 0 ε t ⋆
BoundingTc. SincetheZ areallindependent,wehave:
1 t,hc
H H
E[Tc]=E[∥η(cid:88) Γ(c,η) ωc(Zc )∥2]=η2(cid:88) E[∥Γ(c,η) ωc(Zc )∥2].
1 t,h+1:H t,h t,h+1:H t,h
h=1 h=1
UsingAssumptionA2,webound
H
(cid:88)
E[Tc]≤η2 E[∥ωc(Zc )∥2]=η2HTr(Σc). (47)
1 t,h ε
h=1
BoundingTc. Since(I−ηA¯c)H isindependentfromallZ ,wehaveforanyα >0,
2 t,h 2
(cid:20) (cid:21)
|EFt−1[Tc]|=|2ηEFt−1 ⟨(Γ(c,η) −(I−ηA¯c)H)(θ −θ ), (cid:88)H Γ(c,η) ωc(Zc )⟩ |
2 t,1:H t ⋆ h=1 t,h+1:H t,h
≤α EFt−1(cid:104) ∥(Γ(c,η) −(I−ηA¯c)H)(θ −θ )∥2(cid:105) + η2 EFt−1(cid:20) ∥(cid:88)H Γ(c,η) ωc(Zc )∥2(cid:21) . (48)
2 t,1:H t ⋆ α 2 h=1 t,h+1:H t,h
To bound the expectation of the first term in (48), we bound the following conditional expectation for u ∈ Rd, using
Minkowski’sinequalityandLemmaC.1,
H
E1/2[∥(Γ(c,η) −(I−ηA¯c)H)u∥2]≤E1/2[∥η(cid:88) Γ(c,η) (Ac(Z )−A¯c)(I−ηA¯c)H−h−1u∥2]
t,1:H t,1:h−1 t,h
h=1
H
≤η(cid:88) E1/2[∥Γ(c,η) (Ac(Z )−A¯c)(I−ηA¯c)H−h−1u∥2].
t,1:h−1 t,h
h=1
FromassumptionA2,wethenobtain
H
E1/2[∥(Γ(c,η) −(I−ηA¯c)H)u∥2]≤η(cid:88) (1−ηa)h−1E1/2[∥(Ac(Z )−A¯c)(I−ηA¯c)H−h−1u∥2]
t,1:H t,h
h=1
H
≤η(cid:88) (1−ηa)h−1∥Σc ∥1/2∥(I−ηA¯c)H−h−1u∥2
A˜
h=1
H
(cid:88)
≤η (1−ηa)H∥Σc ∥1/2∥u∥2 ≤ηH∥Σc ∥1/2∥u∥ . (49)
A˜ A˜
h=1
23SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Wethenboundthesecondtermof(48),usingtheindependenceoftheZc ,
t,h
E[∥(cid:88)H Γ(c,η) ωc(Zc )∥2]=(cid:88)H E∥Γ(c,η) ωc(Zc )∥2 ≤(cid:88)H E[∥ωc(Zc )∥2]=HTr(Σc), (50)
h=1 t,h+1:H t,h h=1 t,h+1:H t,h h=1 t,h ε
whichleadstothefollowinginequality
E[Tc]≤α η2H2∥Σc ∥E[∥θ −θ ∥2]+ η2H Tr(Σc). (51)
2 2 A˜ t ⋆ α2 ε
BoundingTc. LetC¯c =(cid:80)H (I−ηA¯c)H−h. SinceIandC¯c areindependentfromallZ ,wehave,foranyα >0,
3 η,H h=1 η,H t,h 3
(cid:34) H (cid:35)
|EFt−1[Tc]|=2|EFt−1 ⟨η(C¯c −Cc )(ξc−ξc), η(cid:88) Γ(c,η) ωc(Zc )⟩ |
3 η,H η,H t ⋆ t,h+1:H t,h
h=1
≤α η2EFt−1(cid:2) ∥(C¯c −Cc )(ξc−ξc)∥2(cid:3) +
η2 EFt−1(cid:34) ∥(cid:88)H
Γ(c,η) ωc(Zc
)∥2(cid:35)
. (52)
3 η,H η,H t ⋆ α t,h+1:H t,h
3
h=1
From(50),wecanboundtheexpectationofthesecondtermby
η2H
Tr(Σ ). Wenowproceedasinthederivationof(49)to
α3 ε
boundthefirsttermfrom(52). Foranyvectoru∈Rd,Minkowski’sinequalityandLemmaC.2give
H
E1/2[∥(C¯c −Cc )u∥2]=E1/2[∥(cid:88) (Γ(c,η) −(I−ηA¯c)H−h)u∥2]
η,H η,H t,h+1:H
h=1
H H
=ηE1/2[∥(cid:88) (cid:88) Γ(c,η) (Ac(Z )−A¯c)(I−ηA¯c)H−h′ u∥2]
t,h+1:h′−1 t,h′
h=1h′=h+1
H H
=η(cid:88) (cid:88) E1/2[∥Γ(c,η) (Ac(Z )−A¯c)(I−ηA¯c)H−h′ u∥2].
t,h+1:h′−1 t,h′
h=1h′=h+1
Then,usingassumptionA2,weobtain
H H
E1/2[∥(C¯c −Cc )u∥2]≤η(cid:88) (cid:88) (1−ηa)H−h−1∥Σc ∥1/2∥u∥ ≤ η H(H −1)∥Σc ∥1/2∥u∥ .
η,H η,H A˜ 2 A˜
h=1h′=h+1
Wethushavethefollowingbound
η4 η2H
E[Tc]≤ α H2(H −1)2∥Σc ∥E[∥ξc−ξc∥2]+ Tr(Σc). (53)
3 4 3 A˜ t ⋆ α ε
3
Boundingψ . Wecannowboundψ byplugging(46),(47),(51)and(53)intheexpectationof(45)
t+1 t+1
N
1 (cid:88)
E[ψ ]= E[Tc+Tc+Tc+Tc]
t+1 N 0 1 2 3
c=1
(cid:16) (cid:17)
≤ (1+α )(1−ηa)2H +α η2H2E [∥Σc ∥] E[∥θ −θ ∥2]
0 2 c A˜ t ⋆
N
+
N1 (cid:88)(cid:16)
2(1+α 0−1)(∥A¯c∥2+∥Σc A˜∥)+ α 43∥Σc
A˜∥(cid:17)
η4H4E[∥ξ tc−ξ
⋆c∥2]+(cid:16)
1+ α1
2
+ α1
3(cid:17)
η2Hσ¯
ε
c=1
(cid:16) (cid:17)
≤ (1+α )(1−ηa)2H +α η2H2E [∥Σc ∥] E[∥θ −θ ∥2]
0 2 c A˜ t ⋆
(cid:18) (cid:19) N
+ 2(1+α−1)max{∥A¯c∥2+∥Σc ∥}+ α3 max{∥Σc ∥} η4H4 1 (cid:88) E[∥ξc−ξc∥2]
0 c∈[N] A˜ 4 c∈[N] A˜ N t ⋆
c=1
(cid:16) (cid:17)
+ 1+ 1 + 1 η2Hσ¯ . (54)
α2 α3 ε
24SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Now,wesetα = ηaH,α = a ,α = 1. SinceηaH ≤ 1,ηH ≤ 1,andη ≤ a we
0 2 2 4ηHE c[∥Σc A˜∥] 3 16Hmaxc∈[N]{∥A¯c∥2+∥Σc A˜∥}
have
(1+α )(1−ηa)2H +α η2H2E [∥Σc ∥]≤(1+ ηaH)(1−ηaH)+ ηaH ≤1− ηaH ,
0 2 c A˜ 2 4 4
η2H2(2(1+α−1)max{∥A¯c∥2+∥Σc ∥}+ α3 max{∥Σc ∥})≤η2H2(3+ 4 )max{∥A¯c∥2+∥Σc ∥}≤ 1 ,
0
c∈[N]
A˜ 4
c∈[N]
A˜ ηaH
c∈[N]
A˜ 2
1+ 1 + 1 ≤2+
2ηHE c[∥Σc A˜∥]
≤4.
α2 α3 a
Usingtheseinequalities,wecansimplify(54)as
(cid:16) (cid:17)
E[ψ ]≤ 1− ηaH E[∥θ −θ ∥2]+ 1η2H2E[∥ξ−ξ ∥2]+4η2Hσ¯
t+1 4 t ⋆ 2 ⋆ ε
(cid:16) (cid:16) (cid:17)(cid:17)
≤ 1−min 1,ηaH E[ψ ]+4η2Hσ¯
2 4 t ε
(cid:16) (cid:17)
= 1− ηaH E[ψ ]+4η2Hσ¯ ,
4 t ε
wherethelastinequalitycomesfromηaH ≤1. Applyingthisinequalityiterativelygives
(cid:18) ηaH(cid:19)T
16η
E[ψ ]≤ 1− E[ψ ]+ σ¯ ,
T 4 0 a ε
whichistheresultofthetheorem.
CorollaryB.6. Letϵ > 0. Assumeϵ2 ≤ 32σ¯ε ,a ≤ 1anda2 ≤ 16max {∥A¯c∥2+∥Σc ∥}. Inorderto
amaxc∈[N]{∥A¯c∥} c∈[N] A˜
achieve∥θ −θ ∥2 ≤ϵ2,therequirednumberofcommunicationis
T ⋆
T =
64max c∈[N]{∥A¯c∥2+∥Σc A˜∥} log(cid:16)2∥θ 0−θ ⋆∥2+ 256maxc∈[N]{a ∥2 A¯c∥2+∥Σc A˜∥}2∆ heter(cid:17)
,
a2 ϵ2
usingthestepsize
(cid:18) aϵ2 (cid:19)
η =min η , ,
∞ 32σ¯
ε
andthenumberoflocaliterations
(cid:16) (cid:17)
amax 1 ,2σ¯ε
H =
16η∞ aϵ2
.
max {∥A¯c∥2+∥Σc ∥}
c∈[N] A˜
Proof. First,werequirethatthesecondtermoftheright-handsideof(43)isboundedby ϵ2 ,thatis 16ησ¯ ≤ ϵ2 . This
2 a ε 2
requiresthat
(cid:18) aϵ2 (cid:19)
η ≤min η , ,
∞ 32σ¯
ε
(cid:16) (cid:17)
whichalsoensuresthatη ≤ ∥A¯1
c∥
forallc∈[N]sinceϵ2 ≤ amaxc∈3 [2 Nσ¯ ]ε {∥A¯c∥}. Wethussetη =min η ∞, 3a 2ϵ σ¯2
ε
,andlook
atthelargestH possible. TheconditionsηaH ≤1,ηH ≤1,andη ≤ a givethat
16Hmaxc∈[N]{∥A¯c∥2+∥Σc A˜∥}
(cid:32) (cid:33)
1 1 a
H ≤min , ,
η aη 16ηmax {∥A¯c∥2+∥Σc ∥}
c∈[N] A˜
(cid:32) (cid:33)
1 1 a
= min 1, , .
η a 16max {∥A¯c∥2+∥Σc ∥}
c∈[N] A˜
25SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Sincea≤1anda2 ≤16max {∥A¯c∥2+∥Σc ∥},thesmallesttermoftheminimumisthethirdoneandwecantake
c∈[N] A˜
(cid:16) (cid:17)
H =
a
=
amax 161 η∞,2 aσ¯ ϵ2ε
.
16ηmax {∥A¯c∥2+∥Σc ∥} max {∥A¯c∥2+∥Σc ∥}
c∈[N] A˜ c∈[N] A˜
ToobtainE[ψ ]≤ϵ2,weneedtotakeT sothat(1− ηaH)Tψ ≤ ϵ2 ,whichgives
T 4 0 2
T ≥
4 log(2ψ
0)=
64max c∈[N]{∥A¯c∥2+∥Σc A˜∥} log(cid:16)2∥θ 0−θ ⋆∥2+ 256maxc∈[N]{a ∥2 A¯c∥2+∥Σc A˜∥}2∆ heter(cid:17)
,
ηaH ϵ2 a2 ϵ2
andtheresultfollows.
C.Technicalproofs
LemmaC.1. Foranymatrix-valuedsequences(U n) n∈N,(V n) n∈NandforanyM ∈N,itholdsthat:
M M M k−1 M
(cid:89) (cid:89) (cid:88) (cid:89) (cid:89)
U − V = { U }(U −V ){ V }.
k k j k k j
k=1 k=1 k=1 j=1 j=k+1
LemmaC.2(Stabilityofthedeterministicproduct). AssumeA2. Then,foranyu∈Rdandh∈N,
∥(I−ηA¯c)hu∥ ≤(1−ηa)h∥u∥ .
Proof. Since(Zc ) arei.i.d,weget
t,h 1≤h≤H
E(cid:2) Γ(c,η)u(cid:3) =E(cid:2)(cid:81)h (I−ηA(Zc ))u(cid:3) =(cid:81)h E(cid:2) I−ηA(Zc )(cid:3) u=(I−ηA¯c)hu.
t,1:h l=1 t,l l=1 t,l
Theproofthenfollowsfromtheelementaryinequality:foranysquare-integrablerandomvectorU,∥E[U]∥ ≤(E[∥U∥2])1/2.
LemmaC.3. Recallthatρ¯ = 1 (cid:80)N (I−(I−ηA¯c)H){θc−θ },itsatisfies
H N c=1 ⋆ ⋆
∥ρ¯ ∥ ≤
η2H2 (cid:88)N
exp(ηH∥A¯c∥)∥θc−θ ∥ (55)
H N ⋆ ⋆
c=1
Proof. Usingtheidentity,
H−2 (cid:18) (cid:19)
(cid:88) H
1−(1−u)H =Hu−u2 (−1)k uk
k+2
k=0
andtheinequality(cid:0) H (cid:1) ≤(cid:0)H−2(cid:1) H2,wegetthat
k+2 k
(cid:12) (cid:12)
(cid:12)H (cid:88)−2 (cid:18) H (cid:19) (cid:12) H2 H (cid:88)−2(cid:18) H −2(cid:19) H2
(cid:12) (−1)k uk(cid:12)≤ |u|k ≤ exp((H −2)|u|)
(cid:12) k+2 (cid:12) 2 k 2
(cid:12) (cid:12)
k=0 k=0
Using(15),wefinallyget(55).
Lemma C.4. Let (x )N , and (y )N be N vectors of Rd. Denote x¯ = (1/N)(cid:80)N x and y¯ = (1/N)(cid:80)N y .
i i=1 i i=1 N i=1 i N i=1 i
Then,
N N
(cid:88) (cid:88)
N∥x¯ −y¯ ∥2 = ∥x −y ∥2− ∥x −x¯ −(y −y¯ )∥2
N N i i i N i N
i=1 i=1
26SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Proof. Definex=[x⊤,...,x⊤]⊤andy=[y⊤,...,y⊤]⊤ ∈RNd. DefinebyPtheorthogonalprojectoron
1 N 1 N
E =(cid:8) x∈RNd :x=[x⊤,...,x⊤]⊤,x∈Rd(cid:9) .
We show that Px = [x¯⊤,...,x¯⊤]⊤. Note indeed that for any z = [z⊤,...,z⊤]⊤ ∈ E, we get (with a slight abuse of
N N
notations,⟨·, ·⟩denotesthescalarproductinRNdandRd)
N
(cid:88)
⟨x−Px, z⟩= {⟨x , z⟩−⟨x¯ , z⟩}=0.
i N
i=1
TheprooffollowsfromPythagorasidentitywhichshowsthat
∥Px−Py∥2 =∥x−y∥2−∥(x−Px)−(y−Py∥2
LemmaC.5. AssumeA3. LetZ bearandomvariabletakingvaluesinastatespace(Z,Z)withdistributionπ . Setη ≥0,
c
thenforanyvectoru∈Rd,wehave
E[∥(I−ηAc(Z))u∥2]≤(1−ηa)∥u∥2−η(1 −η)E[∥Ac(Z)u∥2].
L
Proof. First,remarkthat
∥(I−ηAc(Z))u∥2 =u⊤(I−ηAc(Z))⊤(I−ηAc(Z))u
=u⊤(cid:0) I−2η(1(Ac(Z)+Ac(Z)⊤))+η2Ac(Z)⊤Ac(Z)(cid:1)
u.
2
SincewehaveE[1(Ac(Z)+Ac(Z)⊤)]≽aIandE[1(Ac(Z)+Ac(Z)⊤)]≽ 1E[Ac(Z)⊤Ac(Z)],weobtain
2 2 L
E[∥(I−ηAc(Z))u∥2]=u⊤u−2ηu⊤E[1(Ac(Z)+Ac(Z)⊤)]u+η2u⊤E[Ac(Z)⊤Ac(Z)]u
2
≤∥u∥2−ηa∥u∥2− ηu⊤E[Ac(Z)⊤Ac(Z)]u+η2u⊤E[Ac(Z)⊤Ac(Z)]u
L
=(1−ηa)∥u∥2−η(1 −η)u⊤E[Ac(Z)⊤Ac(Z)]u,
L
whichgivestheresult.
LemmaC.6. LetH >0,η >0suchthat0≤η∥A¯c∥ ≤1forallc∈[N]. Thenitholdsthat
E1/2[∥I−C(t,c)∥2]≤ 1[exp((H −1)η∥A¯c∥)−1]+η(H −1)∥Σc ∥1/2 .
η,H 2 A˜
Proof. UsingMinkowski’sinequality,weget
E[∥I−C(t,c)∥2]=E[∥I− 1 (cid:88)H Γ(c,η) ∥2] (56)
η,H H h=1 t,h+1:H
≤∥I− 1 (cid:88)H (I−ηA¯c)h−1∥+ 1 E1/2[∥(cid:88)H (I−ηA¯c)H+1−h−Γ(c,η) ∥2].
H h=1 H h=1 t,h+1:H
Let’snowestablishanupperboundforthefirstterminthepreviousinequality
∥I− 1 (cid:88)H (I−ηA¯c)h−1∥ =∥I− (A¯c)−1 (I−(I−ηA¯c)H∥
H h=1 ηH
=∥I− (A¯c)−1 (I−(cid:88)H (cid:0)H(cid:1) (−ηA¯c)h)∥
ηH h=0 h
=∥I− (A¯c)−1 (cid:88)H (cid:0)H(cid:1) (−ηA¯c)h∥
ηH h=1 h
=∥I− 1 (cid:88)H−1(cid:0) H (cid:1) (−ηA¯c)h∥
H h=0 h+1
=∥1 (cid:88)H−1(cid:0) H (cid:1) (−ηA¯c)h∥ .
H h=1 h+1
27SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Usingtriangleinequality,weobtain
∥I− 1 Cc ∥ ≤ 1 (cid:88)H−1(cid:0) H (cid:1) ∥ηA¯c∥h .
H η,H H h=1 h+1
Furthermore
1 (cid:88)H−1(cid:0) H (cid:1) ∥ηA¯c∥h ≤ 1(cid:88)H−1(cid:0)H−1(cid:1) ∥ηA¯c∥h ≤ 1[exp((H −1)η∥A¯c∥)−1].
H h=1 h+1 2 h=1 h 2
ApplyingMinkowski’sinequalityandLemmaC.1tothesecondtermontherightsideoftheinequality(56)yields
1 E1/2[∥(cid:88)H (I−ηA¯c)H+1−h−Γ(c,η) ∥2]
H h=1 t,h+1:H
≤ 1 (cid:88)H E1/2[∥(I−ηA¯c)H+1−h−Γ(c,η) ∥2]
H h=1 t,h+1:H
= η (cid:88)H E1/2[∥(cid:88)H (I−ηA¯c)k−1−(h+1)(Ac(Zc )−A¯c)Γ(c,η) ∥2]
H h=1 k=h+1 t,k t,k+1:H
≤ η (cid:88)H (cid:88)H E1/2∥(I−ηA¯c)k−1−(h+1)(Ac(Zc )−A¯c)Γ(c,η) ∥2]
H h=1 k=h+1 t,k t,k+1:H
η (cid:88)H
≤ (H −h)(1−ηa)H−h−1∥Σc ∥1/2
H h=1 A˜
≤η(H −1)∥Σc ∥1/2 .
A˜
D.TDlearningasafederatedLSAproblem
In this section we specify TD(0) as a particular instance of the LSA algorithm. In the setting of linear functional
approximation the problem of estimating Vπ(s) reduces to the problem of estimating θ ∈ Rd, which can be done
⋆
viatheLSAprocedure. Fortheagentc∈[N]thek-thsteprandomnessisgivenbythetupleZc =(Sc,Ac,Sc ). With
k k k k+1
slightabuseofnotation,wewriteAc insteadofA(Zc ),andbc insteadofb(Zc ). ThenthecorrespondingLSAupdate
t,h t,h t,h t,h
equationwithconstantstepsizeηcanbewrittenas
θc =θc −η(Ac θc −bc ),
t,h t,h−1 t,h t,h−1 t,h
whereAc andbc aregivenby
t,h t,h
Ac =ϕ(Sc ){ϕ(Sc )−γϕ(Sc )}⊤ ,
t,h t,h t,h t,h+1
(57)
bc =ϕ(Sc )rc(Sc ,Ac ).
t,h t,h t,h t,h
RespectivespecialisationofFedLSAalgorithmisstatedinAlgorithm5.
Thecorrespondinglocalagent’ssystemwritesasA¯cθc =b¯c,wherewehave,respectively,
⋆
A¯c =E [ϕ(s){ϕ(s)−γϕ(s′)}⊤]
s∼µc,s′∼Pπ(·|s)
b¯c =E [ϕ(s)rc(s,a)].
s∼µc,a∼π(·|s)
The authors of (Wang et al., 2023) study the corresponding virtual MDP dynamics with P˜ = N−1(cid:80)N Pc , r˜ =
c=1 MDP
N−1(cid:80)N rc. Next,introducingtheinvariantdistributionofthekernelµ˜oftheaveragedstatekernel
c=1
N (cid:90)
P˜ (B|s)=N−1(cid:88) Pc (B|s,a)π(da|s),
π MDP
c=1 A
wehaveθ˜asanoptimalparametercorrespondingtothesystemA˜θ˜=˜b. Here
A˜=E [ϕ(s){ϕ(s)−γϕ(s′)}⊤]
s∼µ˜,s′∼˜Pπ(·|s)
˜b=E [ϕ(s)r˜(s,a)].
s∼µ˜,a∼π(·|s)
28SCAFFLSA:QuantifyingandEliminatingBiasinFederatedLSAandTDLearning
Algorithm5FedLSA appliedtoTD(0)settingwithlinearfunctionalapproximation
Input: η >0,θ ∈Rd,T,N,H >0
0
fort=0toT −1do
Initializeθ =θ
t,0 t
forc=1toN do
forh=1toH do
Receivetuple(Sc ,Ac ,Sc )followingTD1andperformlocalupdate:
t,h t,h t,h+1
θc =θc −η(Ac θc −bc ),
t,h t,h−1 t,h t,h−1 t,h
whereAc andbc aregivenin(57)
t,h t,h
endfor
endfor
Average: θ = 1 (cid:80)N θc (58)
t+1 N c=1 t,H
endfor
D.1.ProofofLemma3.1.
Theproofbelowcloselyfollows(Patiletal.,2023)(Lemma7)and(Samsonovetal.,2023)(Lemma1). Indeed,withTD2
and(12)weget
∥Ac∥ ≤(1+γ)
1
almostsurely,whichimplies∥A¯c∥ ≤1+γ foranyc∈[N]. Thisimplies,usingthedefinitionofΣc ,that
A˜
∥Σc ∥ =∥E[{Ac}⊤Ac]−{A¯c}⊤A¯c∥ ≤2(1+γ)2 ,
A˜ 1 1
andthebound(9)follows. Nextweobservethat
Tr(Σc)=E[∥(Ac −A¯c)θc−(bc −b¯c)∥2]≤2{θc}⊤E[{Ac}⊤Ac]θc+2E[(rs(Ss,Ac))2Tr(φ(Sc)φ⊤(Sc))]
ε 1 ⋆ 1 ⋆ 1 1 ⋆ 0 0 0 0
≤2(1+γ)2{θc}⊤Σ [c]θc+2≤2(1+γ)2(cid:0) ∥θc∥2+1(cid:1) ,
⋆ φ ⋆ ⋆
wherethelatterinequalityfollowsfromTD2,andthus(10)holds. Inordertocheckthelastequation(11),wenotefirst
that the bound for a and η readily follows from the ones presented in (Patil et al., 2023)[Lemma 5] and (Patil et al.,
∞
2023)[Lemma7]. TocheckassumptionA3,notefirstthat,withs∼µc,s′ ∼Pπ(·|s),wehave
Ac+{Ac}⊤ =φ(s){φ(s)−γφ(s′)}⊤+{φ(s)−γφ(s′)}φ(s)⊤ =2φ(s)φ(s)⊤−γ{φ(s)φ(s′)⊤+φ(s′)φ(s)⊤}
⪯(2+γ)φ(s)φ(s)⊤+γφ(s′)φ(s′)⊤ ,
whereweadditionallyusedthat
−(uu⊤+vv⊤)⪯uv⊤+vu⊤ ⪯(uu⊤+vv⊤)
foranyu,v ∈Rd. Thus,wegetthat
E[Ac+{Ac}⊤]⪯2(1+γ)Σc .
φ
Therestoftheprooffollowsfromthefactthat
E[{Ac}⊤Ac]⪰{A¯c}⊤A¯c ⪰(1−γ)2λ Σc ,
1 1 min φ
whichisprovene.g. in(Lietal.,2023a)(Lemma5)or(Samsonovetal.,2023)(Lemma7).
29