CAST: CLUSTERING SELF-ATTENTION USING SURRO-
GATE TOKENS FOR EFFICIENT TRANSFORMERS
AdjornvanEngelenhoven,NicolaStrisciuglio&EstefaníaTalavera
DepartmentofComputerScience
UniversityofTwente(TheNetherlands)
ABSTRACT
TheTransformerarchitecturehasshowntobeapowerfultoolforawiderange
of tasks. It is based on the self-attention mechanism, which is an inherently
computationally expensive operation with quadratic computational complexity:
memory usage and compute time increase quadratically with the length of the
inputsequences,thuslimitingtheapplicationofTransformers. Inthiswork,we
propose a novel Clustering self-Attention mechanism using Surrogate Tokens
(CAST),tooptimizetheattentioncomputationandachieveefficienttransformers.
CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix,
used to cluster the input sequence and generate novel cluster summaries. The
self-attentionfromwithineachclusteristhencombinedwiththeclustersummaries
ofotherclusters,enablinginformationflowacrosstheentireinputsequence. CAST
improvesefficiencybyreducingthecomplexityfromO(N2)toO(αN)whereN
isthesequencelength,andαisconstantaccordingtothenumberofclustersand
samplespercluster. WeshowthatCASTperformsbetterthanorcomparabletothe
baselineTransformersonlong-rangesequencemodelingtasks,whilealsoachieving
higherresultsontimeandmemoryefficiencythanotherefficienttransformers.
1 INTRODUCTION
TheTransformerarchitecture(Vaswanietal.,2017)hasrevolutionizedmanyfieldswithinmachine
learningsuchastranslation(Vaswanietal.,2017),summarization(Miller,2019),textgeneration
(Chenetal.,2019),sentimentclassification(Sunetal.,2019),andalsotaskslikeimageclassification
(Dosovitskiyetal.,2020),objectdetection(Liuetal.,2021b),andproteinfolding(Jumperetal.,
2021). Theself-attentionmechanismstandsatthecoreofitsstrengths. ItallowstheTransformerto
directlymodellong-rangedependencieswithinasequencewithouttheneedforahiddenstatelikein
recurrentneuralnetworks(Hochreiter&Schmidhuber,1997). However,theself-attentionmechanism
hasaninherentlargememorycost,sinceitscomplexitygrowsquadraticallywiththeinputsequence
length. Withthesememoryrequirementsandtheever-increasingsizeoflargelanguagemodels,such
astheGPTseries(Brownetal.,2020;OpenAI,2023)andLLaMA(Touvronetal.,2023),aneed
formoreefficientattentionmechanismshasemerged(Daoetal.,2022). Currentimplementationsof
moreefficientself-attentionmechanismscanberoughlygroupedintothefollowingcategories: (1)
applyself-attentiononsubsetsoftheinputsequences(sparsification)(Ainslieetal.,2020;Darasetal.,
2020;Kitaevetal.,2020;Maetal.,2023;Tayetal.,2020b;Zaheeretal.,2021),(2)approximatethe
self-attentionmechanismwithalowercomplexity(Choromanskietal.,2020;Liuetal.,2021a;Wang
etal.,2020),and(3)removeself-attentioninfavorofalowercomplexitysimilaroperation(Guetal.,
2022;Lee-Thorpetal.,2021;Smithetal.,2023;Tolstikhinetal.,2021).
Inthispaper,weintroduceanewefficientvariantofself-attention,namedClusteringAttentionusing
SurrogateTokens(CAST)–seeFigure1. Itappliesclusteringtoself-attentionandintroducestwo
novelideas, namely1)learnableclusteringoftokens, and2)clustersummaries, whichallowfor
informationtoflowbetweentokensfromdifferentclusterswithinthesameattentionhead. CAST
learns to cluster tokens that would have a strong connection in the original attention matrix by
clusteringbasedonasimilaritymatrixbetweenthesurrogatetokens,queries,andkeys. Standard
self-attentionisappliedwithinclusters,whereitsresultiscombinedwithclustersummariesbasedon
apreviouslycreatedsimilaritymatrix. Thisallowsforeachtokentoretrieveinformationfromthe
restofthesequence,improvingstabilityandperformance. CASTissignificantlyfasterthanother
1
4202
beF
6
]GL.sc[
1v93240.2042:viXraFigure 1: Sketch of the proposed method. The colors red and blue correspond to two different
clusters. Withthequeries(Q),keys(K),andvalues(V),wecreatethesurrogatetokensimilarities
A (similaritybetweenthequeriesandsurrogatetokens)andA (similaritybetweenthekeysand
q k
surrogatetokens).TheyarecombinedtocreateafinalsimilarityA foreachtokentoeachcluster.We
g
thenusethisclusteringoftokensandcreatetheclusteredqueries(Q ),keys(K ),andvalues(V ).
g g g
Withineachcluster,self-attentionisappliedresultinginR . Furthermore,A isalsoclustered
intra k
andmatrixmultipliedwithV tocreateasummaryperclusterresultinginR . TheresultsR
g inter intra
andR arethencombinedusingA astheweightsforaweightedsum,resultinginR. Another
inter q
linearprojectionOisthenappliedonRandpassedontothefeedforwardlayeroftheTransformer.
efficientTransformervariants,andmatchesorimprovestheperformanceofastandardTransformer
onlong-sequencemodelingtasks.
Thecontributionofthispaperisanovelefficientapproachtoreplacetheself-attentioninTransform-
ersbasedonlearningtokenclustersinanunsupervisedmanner.
2 RELATED WORKS
Priorresearchonefficientcomputationofself-attentionhasfocusedonchunkingattention,clustering
attention,andthecurrentstate-of-the-art,structured-state-space-basedmodels.
Chunkingattention. Oneobviouswaytosolvethequadraticcomplexityofself-attentionistochunk
thegivensequenceintosmallerpiecesandapplyself-attentionwithinthosepieces. Thisisknownas
LocalAttention(Luongetal.,2015). However,bychunkingthesequence,noinformationcanbe
passedbetweenchunks,causingadecreaseintaskperformancebelowthatofthestandardTransformer.
Severalworkshavesparsifiedtheattentionmatrixbywindowingorchunkingthesequence(Ainslie
etal.,2020;Beltagyetal.,2020;Childetal.,2019;Luongetal.,2015;Zaheeretal.,2021). Some
optedforapplyingattentioninaslidingwindowmanner,buttheuseofglobalattentiontospecial
tokens,suchasthe"CLS",isalsocommonamongtheoriginalefficientTransformersoftheLRA
benchmark(Ainslieetal.,2020;Beltagyetal.,2020;Zaheeretal.,2021). Thesemodelsalsousethe
"CLS"tokenforthefinalclassification,allowingallpartsofthesequencetocontributetothefinal
result. BigBird(Zaheeretal.,2021)combinedglobalattention,windowattention,andrandomblocks
ofattentiontoachievestate-of-the-artperformanceontheLRAbenchmark. Despitetheefficiency
gains of the chunking of self-attention, it does not necessarily model long-range dependencies
well. Multipleroundsofself-attentioncanbenecessarytocreatealargeenoughreceptivefieldto
modellong-rangedependencies. Althoughchunkinghasshownitseffectiveness,itdoesnotmodel
long-rangedependencieswell,sincenoinformationcanflowfromdistantpartsoftheinputsequence.
Clusteringattention. Onewaytoeasilymodellong-rangedependenciesistheclusteringofthe
inputsequencewhichisonlypartiallydependentontheorderoftheinput. Specifically,theReformer
(Kitaev et al., 2020) and its descendant SMYRF (Daras et al., 2020) both use locality-sensitive
hashing(LSH)toapplyclusteringtotheinputsequenceandthenapplyaformofattention. The
Reformerfirstusestheconstraintofthequeriesandkeysbeingequalsuchthattheattentionmatrix
is symmetric. Then to create the clusters they define a random matrix R ∈ Rdh×Nc/2, which is
then matrix multiplied with query-key. The query keys are then clustered based on the result of
argmax([X R⊕−X R]).Thesymbol⊕standsforconcatenation,whileX standsfortheshared
qk qk qk
query-keyrepresentation. Theresultingclustersareofdifferentsizes,makingitdifficulttocompute
themonconventionalhardware,harmingefficiency,f.i. ontheLongRangeArenaBenchmark.
2SMYRF efficiently computes asymmetric clustering of queries and keys, that is a query is not
necessarilyclusteredwithitscorrespondingkey. UnliketheReformer,SMYRFalsocreatesbalanced
clustersofconstantsizeandthusachievingbettercomputationalefficiency. Althoughboththese
clusteringTransformersdomodellong-rangedependencies,clusteringalsocreatesproblems. The
randominitializationofthenetworkcausesqueriesandkeystobeclusteredrandomlyatfirst. Asa
result,theweightupdateforqueriesandkeysisbasedonlyontheinformationthatisinsidesingle
clusters. Furthermore,gradientscouldbeunstablewhenaqueryorkeyswitchesfromoneclusterto
another. Ideally,anefficientTransformerretainstheoriginalstrengthoftheTransformer,namelythe
informationflowthroughouttheentireinputsequenceviatheself-attentionmechanism.Ourapproach
keepsthisinformationflowandintroducesmorestabilitythroughtheuseofclustersummaries,which
actasanindicatorofthetypeofinformationthatcanbeobtainedinagivencluster.
StructuredStateSpaceModels. MorerecentworkonefficientsequencemodelsincludesStructured
StateSpaceModels(SSSM),suchasS4(Guetal.,2022),S5(Smithetal.,2023),andMEGA(Ma
etal.,2023)whicharecurrentlythestate-of-the-artonlong-rangesequencemodelingbenchmarks.
SSSMsdonotusetheself-attentionmechanism,andrelyonalearnablestatespacetocapturerelevant
dependenciesinasequence. However,wedonotinvestigatethesetypesofarchitecturesfurther,as
theyarenolongerrelatedtotheTransformerarchitectureandthustheself-attentionmechanism.
3 CLUSTERING ATTENTION USING SURROGATE TOKENS
We propose CAST, Clustering Attention using Surrogate Tokens, that learns token clusters and
optimizesthecomputationofattentioninTransformerarchitectures. Wepresentitsversionswith
single-headedandmulti-headedself-attention,anddiscussthecomputationalcomplexity. Avisual-
izationofCASTisinFigure1,andamodule-baseddescriptionisinAppendixA.1. Moreover,a
nomenclaturewithsymboldefinitionsandpseudocodesareinAppendicesA.2andA.3.2,respectively.
3.1 INTUITION
Aquery(Q)andkey(K)whichareinthesamedirectionwithalargeenoughmagnitudewillendup
withalargescoreintheself-attentionmatrixA . Thisrelationshipcanbeexploitedforclustering
K
bydefiningsomestaticclusteringdirections,determiningthesimilarityofallqueriesandallkeys
withtheseclusteringdirections,andthenclusteringbasedonthissimilarity. However,thisapproach
hastwoproblems: (1)whenclustering,directionsarerandomlyinitialized,andtheirconfiguration
mightnotbeoptimalforthetaskthatistrainedon,and(2)whentrainingisstarted,queriesandkeys
areclusteredrandomly. Consequentially,thegradientofthequeriesandkeysisonlybasedonthe
self-attentionwithintheircluster,makingitimpossibleforqueriesandkeysfromdifferentclustersto
alignthemselvesaccordingtotheloss.
To alleviate this problem, we design CAST to ensure that the clustering directions are learnable
andthateachtokenreceivesinformationfromallclusters. InCAST,surrogatetokensrepresentthe
learnableclusteringdirectionsandareusedasasurrogateforfindingsimilarqueriesandkeys. The
weightofeachclusterisbasedonthesimilarityofitsqueryandtheclusteringdirection. Withinthe
clusterofacertaintoken,weapplyself-attention. Forotherclusters,clustersummariesarecreated,
basedonthesimilarityofatokenkeywiththedirectionoftheclusteritbelongsto.
3.2 SINGLE-HEADCLUSTERINGATTENTIONUSINGSURROGATETOKENS
CASTisanextensionoftheself-attentionmechanismintheTransformerarchitecture(Vaswanietal.,
2017). Wefirstcreatequery-key-valuecombinationsfromtheinputsequenceX∈RN×d,whereN
istheinputsequencelength,anddthefeatureembeddingdimension:
Q=XW , K=XW , V=XW , ∈RN×d, (1)
q k v
whereW ,W ,W ∈Rd×darelearnableparametersforthequeries(Q),keys(K),andvalues(V)
q k v
respectively.Tocreateclustersandlowerthecomputationalcomplexity,wedefinelearnablesurrogate
tokensS∈RNc×d,whereN cindicatesthenumberofclusters. Thesurrogatetokensrepresentthe
learnableclusteringdirectionsandareusedasasurrogateforfindingsimilarqueriesandkeys. Then
wecomputethesimilaritymatriceswiththesurrogatetokensforthequeries(A )andthekeys(A ).
q k
Wecombinethesesimilaritymatricesusingaratioσ(φ):1−σ(φ)basedonalineartransformation
3ofX,whereσindicatesthesigmoidfunction.
A =QST,A =KST ∈RN×Nc
q k
φ=XW +b ∈RN×1
φ φ
A =σ(φ)⊙f (A )+(1−σ(φ))⊙f (A ) ∈RN×Nc, (2)
g 2 q 2 k
where σ(φ) is a sigmoid function applied to a linear transformation of X, which is represented
as XW +b . Where W ∈ Rd×1 and b ∈ R1 are learnable parameters. The function f(·)
φ φ φ φ
indicatesanattentionfunction,whichinthispaperincludestheclassicalsoftmaxandtheLaplace
function from MEGA (Ma et al., 2023). In the case of softmax, f (·) indicates that the softmax
i
is applied over the dimension i of the matrix. Here the softmax is applied to the dimensions
holdingcorrespondingtothedifferentclusters. Thesymbol⊙representselement-wisemultiplication.
Subsequently,thecalculatedsimilaritiesareusedtoclustertheinputsequenceusingaclustering
mechanismG,computedasG : RN×Nc,RN×∗ → RNc×κ×∗,where∗indicatesanygivenshape,
andκindicatesthesizeofacluster. Furthermore,letG−1indicatethereverseofthefunctionG,such
thatG−1 :RN×Nc,RNc×κ×∗ →RN×∗,whereintheeventofaninputiscontainedintwoclusters
thesumiscalculated. Then,standardself-attentionisappliedwithineachclusterasfollows:
(cid:32) (cid:33)
Q KT
R =f g g V ∈RNc×κ×d, (3)
intra τ g
whereQ =G(A ,Q),K =G(A ,K),V =G(A ,V),andτ isascalardependingontheused
g g g g g g
attentionfunction. HereR indicatestheresultofattentionwithintheclusters.
intra
Tocreateagradientbetweentokensfromdifferentclustersweapplyattentionbetweenclustersas
well. Todothis,wedefinevaluesummariesR ,whichisaweightedsumofallvalueswithin
inter
eachclusterwheretheweightsA arebasedonA andφasfollows:
inter k
(cid:18) (cid:19)
A ⊙ϕ(−φ)
A =G A , k I′ ∈RNc×κ×1,
inter g τ Nc
k
R =f (A )VT ∈RNc×1×d, (4)
inter 2 inter g
where I N′
c
indicates the expanded identity matrix I
Nc
such that I N′
c
∈ BNc×Nc×1, the function
ϕ(x)=Softplus(x)+1(Zhengetal.,2015),andτ isascalingfactor. WeuseA andrtocreate
k Q
anattentionmatrixusedasaweightedsumforthevaluesummariesandattentionwithinclusters:
(cid:18) (cid:19)
A ⊙ϕ(φ)
A =f q ∈RN×Nc,
sum 3 τ
q
A =(A ⊙Mˆ) ∈RN×Nc,
inter sum
A =G(A ,A ⊙M) ∈RN×Nc,
intra g sum
R=G−1(A ,A R )+A R ∈RN×d, (5)
g intra intra inter inter
where M ∈ (0,1)N×Nc is a mask where M
i,j
= 1 if X
i
∈ G(A g,X)
j
and M
i,j
= 0 if X
i
∈/
G(A ,X) . Asaresultofthisoperation,RisaweightedsumaccordingtoA oftheattention
g j sum
within clusters (R ) and the summaries of the clusters (R ). The final output O is then
intra inter
calculatedasO=RW ∈RN×d,whereW ∈Rd×darelearnableparameters. Oisthenpassedon
o o
totherestofthestandardTransformerarchitecture.
Clustering. The clustering mechanism is an integral part of CAST, and serves to group inter-
importanttokensoftheinputsequence. Wemeasuretheinter-importanceasthesimilarityscoresin
theattentionmatrixA . Wedefinetwoclusteringmechanismstomaximizethesimilaritypercluster:
g
A) Top-K Clustering Mechanism. The Top-K clustering mechanism is a naive approach to
clusteringtheinputsequence,theindicesofthelargestK elementsinA aretakenperclusterand
g
used to index the original sequence. Because Top-K simply maximizes the similarity scores per
clusterseparately,itispossibleforanytokentobecontainedinanywherebetween0andN clusters.
c
This attribute of Top-K can be useful in case padding is used, by setting the similarity scores of
paddingto0,itcanbeensuredthatpaddingisnevertakenintoconsiderationwhenapplyingattention
withinclusters. However,instaticsequencedomains,likeimages,itcanalsocausecertainpartsof
theinputsequencetoneverbeclustered.
4Figure2: ThepracticaldifferencebetweentheTop-K andSATop-K clusteringmechanisms. Here,S
indicatestheclusteringdirectionoftwosurrogatetokens. Theblueandgreendashedcirclesindicate
theclustersthattheTop-K andSATop-K clusteringmechanismswouldcreate,respectively.
B)SingleAssignmentTop-KClusteringMechanism. Thisapproachhastheconstraintthatevery
partofthesequencecanonlybeassignedtoasinglecluster,andensurethateverytokenispartof
theresultofCASTandthushasagradient. Weimplementtheconstraintbyclusteringtokensin
descendingorderaccordingtotheirmaximumscoreinA . Whenaclusterhasreachedthedesired
g
size,wenolongerassigntokenstothiscluster.
3.3 MULTI-HEADCLUSTERINGATTENTIONUSINGSURROGATETOKENS
ToapplyCASTinamulti-headedscenario,thesurrogatetokensSarealsosplitintomultipleheads
such that S ∈ RNc×h×dh, where h is the number of heads, and d
h
= hd. The score A
g
is then
computedasfollows:
A =QST,A =KST ∈RN×h×Nc,
q k
φ=XW +b ∈RN×1,
φ φ
(cid:88)
As =σ(φ)⊙f ( A ) ∈RN×Nc,
q 2 q:,h,:
h
(cid:88)
As =(1−σ(φ))⊙f ( A )∈RN×Nc,
k 2 k:,h,:
h
A =As+As ∈RN×Nc. (6)
g q k
Inshort,wesumthesimilarityscoresA andA overtheheaddimensiontogetthesimilarityof
q k
each token to each cluster. After this step CAST works as described in Section 3.2, but with an
addedconstantdimensionh. BeforetheresultOiscalculated,theresultofthedifferentheadsRis
concatenatedsuchthatR∈RN×d.
3.4 COMPLEXITY
WiththeuseofCAST,theoriginalquadraticcomplexityoftheself-attentionmechanismissignifi-
cantlyreduced. ThecomplexityofCASTwithoutaddedconstantsregardingthenumberoflayers,
batch size, and hidden dimensions is O(αN). Here α = max(κ,N2), where κ is the number of
c
elementsinacluster,andN thenumberofclusters. Here,thecomplexityO(Nκ)isderivedfrom
c
thecomputationofR beingN κ2,whichcanberewrittenasNκ. ThecomplexityO(NN2)is
intra c c
derivedfromthecomputationofR . Here,wehavesettherelationofκtobeκ= N ,although
inter Nc
technicallynotnecessarythiswouldallowforeachtokentobeclustered. Theoretically,thememory
usageislowestwithaconfigurationwhereN2 =κ.
c
4 EXPERIMENTAL SETUP
WeusetheLongRangeArenaBenchmark(Tayetal.,2020c)(LRA)toevaluatetheperformance
of CAST, and compare the results with those of other methods for efficient Transformers. The
LRAbenchmarkiscomposedofsixcomplextasksondifferentdatamodalitiesandsequencelength
(1K-16Ktokens), consideredtotheoreticallyrepresentvariouschallengingtasksandcomplexity
levels. TheLRAdatasetandtrainingruleswereproposedwiththeaimofallowingresearchersto
compareefficientTransformerswithouttheneedforlargecomputationalpower. Wefurtherperform
anablationstudyonthesurrogatetokenstodeterminehowthenumberofclustersinfluencesthe
5performance,peakmemoryusage,andthetrainingstepsefficiency. Lastly,weanalyzethelearned
clusterstogaininsightsintowhyCASTworks.
Thetasksweconsiderservetoinvestigatethecapabilityofmodelstodealwithadiverserangeofdata
modalitiesandstructures,suchasnaturallanguage,images,andmathematics. TheLRAbenchmark
iscurrentlybeingusedasthemainbenchmarkforefficientTransformersandlong-rangesequence
modeling. TheevaluationmetricforallthetasksinLRAisclassificationaccuracy. Moredetailson
thepertinenceoftheLRAanditssixtaskscanbefoundinAppendixA.4.
4.1 EXPERIMENTS
Wecarriedoutexperimentsonavarietyofhardware,andexpandeduponinmoredetailperexperiment
wheresignificant. Forreproducibilityandfaircomparison,wekeepthenumberoflayersandfeatures
comparabletothoseusedinefficientTransformersintheoriginalLRApaper(Tayetal.,2020c).
CASTefficiency. WeevaluatetheefficiencyofCASTbyrunningitontheTexttaskofLRAwitha
varyingsequencelengthof1K,2K,3K,and4K.Foreachofthesesequencetasks,wedeterminethe
peakmemoryusageandthenumberoftrainingstepspersecondrelativetotheoriginalTransformer
architecture. ForcomparisonwithotherefficientTransformerswetaketheirperformancereportedin
theoriginalLRApaper(Tayetal.,2020c). WeensurethatCASTandtheTransformerusetheexact
samehyperparameters,suchasthenumberoflayers,thenumberofheads,andthesizeofthefeature
space. CASTusesaclustersizeof200throughoutallsequencelengths. Allexperimentsregarding
memoryandtimeefficiencywererunonasingleA40GPU.
LongRangeArenaperformance. WeevaluatetheperformanceofCASTontheLRAdataset
byperformingasmallhyperparametersweep. Intotal,werantenfull-lengthtrainingsessionsper
task,wherethecheckpointwiththelowestvalidationlosswasusedtoevaluatetheperformanceof
CAST.InAppendixA.5,amoredetaileddescriptionregardingthehyperparameterscanbeviewed.
Furthermore,aWeights&Biases(Biewald,2020)reportwithallhyperparametersandlosscurves
canbefoundhere1.
Clusteringablation. Wefurtherperformanablationstudyonhowthenumberofsurrogatetokens,
i.e. thenumberofclusters,affectstheperformance,peakmemoryusage,andnumberoftraining
steps per second. We also investigate whether there is a difference in using the Top-K or Single
AssignmentTop-K clusteringmechanismsintheImagetask. Forthisablation,weusetheTextand
Imagetaskstodeterminewhetherthereisadifferencebetweenmodalities. Foreachtask,wetake
thebest-performingmodelsfromthehyperparametersweepbutvarytheclustersizeκsuchthat
κ∈{32,64,128,256,512}.
Visualanalysisonclusters. Lastly,weperformavisualanalysisonthelearnedclustersinthe
ImagetaskoftheLRAdataset. Fromtheablations,wetakeasinglemodelwithtwoCASTlayers
andeightsurrogatetokens. Wethenvisualizewhichtokensareclusteredtogetherandhaveamore
in-depthlookattheobtainedsimilarityscoresA .
g
5 RESULTS AND DISCUSSION
5.1 LONGRANGEARENAEFFICIENCY
In Table 1, we compare the speed and memory efficiency of several notable architectures. We
observethatCASTwithTop-K issignificantlyfastercomparedtoboththeoriginalTransformerand
otherefficientTransformersforallsequencelengths,withitbeing6.18timesfasterduringtraining
thantheoriginalTransformeronasequencelengthof4K.Furthermore,CASTneedsslightlyless
memorythanotherefficientTransformers,onlyneeding10%ofthememorycomparedtotheoriginal
Transformerarchitectureatasequencelengthof4K.TheuseofSATop-K lowersthespeedofCAST
significantlybutdoesnotaffectthememoryefficiency. Furtherresultsregardingtheefficiencyduring
inferencecanbefoundinAppendixA.6.1.
5.2 LONGRANGEARENAPERFORMANCE
Table2reportstheperformanceresultsofCASTcomparedtothoseofthebaselineTransformerand
itsefficientvariations,andthecurrentstate-of-the-artmodels. CASTachievesperformancebetween
thatofthestatespacemodelsandtheotherefficientTransformers. Althoughstructuredstatespace
1Linktobepubliclyavailableuponacceptance
6Table1: SpeedandMemoryefficiencyoftheLRABenchmarkwiththeaverageperformance(Avg.).
TheTransformerandCASTwerecreatedusingthesamehyperparameters. Abatchsizeof25was
usedandCASTusesaconstantclustersizeof200. SpeedandMemoryincrease/decreasearereported
relativetotheresultsoftheoriginalTransformerarchitecture. Modelsannotatedwiththe†symbol
hadtheirrelativespeedandmemorytakenfromtheLRAbenchmark(Tayetal.,2020c).
StepsPerSecond↑ PeakMemoryUsage↓ Avg.
Model
1K 2K 3K 4K 1K 2K 3K 4K Performance
Transformer(Vaswanietal.,2017) 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 57.71
Reformer†(Kitaevetal.,2020) 0.5 0.4 0.7 0.8 0.56 0.37 0.28 0.24 50.56
SinkhornTrans.†(Tayetal.,2020b) 1.1 1.6 2.9 3.8 0.55 0.31 0.21 0.16 51.23
Performer†(Choromanskietal.,2020) 1.2 1.9 3.8 5.7 0.44 0.22 0.15 0.11 51.18
Luna-16(Maetal.,2021) 1.2 1.8 3.7 5.5 0.44 0.23 0.17 0.10 59.55
S4(Guetal.,2022) - - - 4.8 - - - 0.14 86.09
MEGA(Maetal.,2023) - - - 2.9 - - - 0.31 88.21
MEGA-Chunk(Maetal.,2023) - - - 5.5 - - - 0.13 85.66
CAST(Top-K) 1.76 3.25 4.48 6.18 0.33 0.18 0.13 0.10 59.32
CAST(SATop-K) 1.47 2.24 2.33 2.62 0.33 0.18 0.13 0.10 57.57
.
Table2: TheperformanceofdifferentarchitecturesontheLongRangeArenabenchmarkinclassifi-
cationaccuracy. Wedividetheseworksin(A)TransformerarchitecturesthatdonotuseStructured
StateSpacesoranyderivationofthis,and(B)ArchitecturesusingStructuredStateSpaces. (A-Top)
TheoriginalTransformerarchitecture. (A-Middle)EfficientTransformerarchitecturesthatcameout
withtheLRAbenchmark. (A-Bottom)NotablemodelsthatcameoutafterthereleaseoftheLRA
benchmark. (B)ArchitecturesusingStructuredStateSpaces. herethesymbol†indicatesthatthe
resultscamefromtheoriginalpaperfromtheLRAdataset(Tayetal.,2020c). Furthermore, the
symbol×indicatesthattheTransformervarianteitherranoutofmemoryand−indicatesthatresults
werenotreported.
Model Year ListOps Text Retrieval Image Pathfinder Path-X Avg.
Random 10.00 50.00 50.00 10.00 50.00 50.00 36.67
(A)TransformerBasedArchitectures
Transformer†(Vaswanietal.,2017) 2017 36.37 64.27 57.46 42.44 71.40 × 53,66
Transformer(re-impl(Maetal.,2021)) 2017 37.11 65.21 79.14 42.94 71.83 × 57.71
LocalAtt.†(Tayetal.,2020c) 2017 15.82 52.98 53.39 41.46 66.63 × 46.71
SparseTrans.†(Childetal.,2019) 2019 17.07 63.58 59.59 44.24 71.71 × 51.03
Performer†(Choromanskietal.,2020) 2020 18.01 65.40 53.82 42.77 77.05 × 51.18
Reformer†(Kitaevetal.,2020) 2020 37.27 56.10 53.40 38.07 68.50 × 50.56
SinkhornTrans.†(Tayetal.,2020b) 2020 33.67 61.20 53.83 41.23 67.45 × 51.23
BigBird†(Zaheeretal.,2021) 2021 36.05 64.02 59.29 40.83 74.87 × 54.18
FNet(Lee-Thorpetal.,2021) 2021 35.33 65.11 59.61 38.67 77.80 × 54,42
Luna-16(Maetal.,2021) 2021 37.43 65.74 79.38 46.39 78.36 - 59.55
CASTTop-K(Ours) 2023 39.90 65.45 78.01 52.37 70.18 × 59.32
CASTSATop-K(Ours) 2023 40.70 65.13 74.64 52.78 62.22 × 57.57
(B)StructuredStateSpaceArchitectures
S4(Guetal.,2022) 2021 59.60 86.82 90.90 88.65 94.20 96.35 86.09
S5(Smithetal.,2023) 2022 62.15 89.31 91.40 88.00 95.33 98.58 87.46
MEGA-Chunk(Maetal.,2023) 2022 58.76 90.19 90.97 85.80 94.41 93.81 85.66
MEGA(Maetal.,2023) 2022 63.14 90.43 91.25 90.44 96.01 97.98 88.21
modelsarestate-of-the-art,theycannotbedirectlycomparedtootherefficientTransformerssince
theyapplyglobalconvolutionsandarenotsolelyrelyingonattention. CASThasarelativelyhigh
scorefortheImagetaskandarelativelylowscoreforthePathfindertaskcomparedtothatofthe
otherefficientTransformers. ThelowscoreofthePathfindertaskcouldbeexplainedbythefactthat
manyofthepixelsinthePathfinderimageareblack,whichmakestheirquery-keypairssimilarand
putinthesamecluster. AnextendedversionofTable2canbefoundinAppendixA.6.2.
5.3 CLUSTERINGABLATION
Clusteringmechanisms. Figure3showsthedifferenceinperformance,memoryfootprint,and
timeefficiencybetweentheclusteringmechanismsTop-K andSATop-K ontheTextandImagetasks
7(a) (b) (c)
(d) (e) (f)
Figure3: AblationsontheclustersizeusingCASTwithTop-K ClusteringMechanism(blue)and
SingleAssignmentTop-K ClusteringMechanism(orange)ontheTextandImagetasksoftheLRA
benchmarkagainst(a&d)theperformance,(b&e)thepeakmemoryallocated,and(c&f)thetime
efficiency,respectively.
of LRA. In Figure 3d, it can be seen that the choice in the clustering mechanism slightly affects
theresultingperformanceontheImagetaskataclustersizeof128and256. Itcanbeobserved
fromFigure3bandFigure3e,thattheclustermechanismdoesnotaffectthePeakMemoryUsage.
Furthermore, it can be seen from Figure 3c and Figure 3f that the Top-K clustering mechanism
isoverallsignificantlyfasterthantheSATop-K clusteringmechanism. TheSATop-Kclustering
mechanisminparticularismuchslowerwhenusingsmallclustersizesonlargeinputsequences,like
fortheTexttask.
Performance. InFigure3aandFigure3d,weshowacomparisonoftheeffectofclustersizesand
clustermechanismontheperformanceofCASTonTextandImagetaskoftheLRAdataset. For
theTexttask,theclustersizedoesnotsignificantlyimpacttheresultingaccuracy,althoughaslight
increaseinaccuracycanbeobservedatalargerclustersize. However,clustersizedoesimpactthe
performanceoftheImagetasksignificantlyforbothTop-KandSATop-K.Itcanbeobservedthatthe
performanceontheImagetaskdipsaroundaclustersizeof64to128,butpeaksataclustersizeof
32and256.
Peakmemoryusage. InFigure3bandFigure3e,weshowmeasurementsoftheinfluenceofthe
clustersizesonthepeakmemoryusageoftheImageandTexttask. Atitslowest,CASTonlyuses
around1.35gigabytesofmemoryfortheImageand6.3gigabytesofmemoryfortheTexttask. The
memorycurvesrepresentaquadraticrelationship,withanincreaseinmemorywhenthenumberof
clustersbecomestoolarge,whichwasexpectedfromSection3.4. Forbothtasks,itcanbeseenthat
theleastamountofmemoryisusedwhenthenumberofclustersandtheclustersizeisclosetothe
relationN2 =κ. However,knowingthatCASTachievessimilarperformanceacrossdifferentcluster
c
sizes,wecanusetheclustersizethatminimizesthememoryfootprintwithoutalargedecreasein
performance.
Timeefficiency. InFigure3candFigure3f,wereportmeasurementsoftheinfluenceofthecluster
sizeandclusteringmechanismonthetrainingstepspersecondoftheTextandImagetask. Itcanbe
observedthatthenumberoftrainingstepspersecondfortheSATop-Kclusteringmechanism(orange)
issignificantlylowerthanthatofthestandardTop-K clusteringmechanism,especiallyatsmaller
clustersizes. ThisisduetotheconstraintofSATop-K mustensureeverytokeniscontainedonlyin
onecluster. However,knowingthatthechangeofperformancebetweenclusteringmechanismsand
8(a)ImageoftheLRAImagetask. (b)VisualizationsoflearnedclustersofCASTperlayer.
Figure4: VisualizationsofthelearnedclustersofaCASTmodelwithSATop-K ontheLRAImage
task. Thenumberofclustersis8. (a)Anexampleimage. (b-Left)Clusteredpixels,whereeachcolor
representsadifferentcluster. ExamplescoresforclustersofA (b-Middle&Right),eachimage
g
correspondstoadifferentclusterforthefirst(b-Top)andlastlayer(b-Bottom),respectively.
clustersizesissmall,theTop-K clusteringmechanismcanbechosenataclustersizethatmaximizes
thenumberofstepspersecond.
5.4 VISUALANALYSISONCLUSTERS
The clusters created by CAST do seem to hold visuospatial information on image tasks. More
specifically,CASTseemstoseparatebackgroundfromforegroundinimages. InFigure6a,weshow
anexampleofaninputimagefromtheImagetaskwhichdepictsahorseanditsrider. InFigure
4b,theclusteredpixelsofthetwolayersofCASTaredepicted,whereeachcolorcorrespondsto
oneoftheclusters. Inthefirstlayer,wecanobservethattheclustersareapproximatelyslicesofthe
originalimage. Inthelastlayer,itcanbeobservedthatthebackgroundandforegroundoftheimage
areroughlyseparatedindifferentclusters. Thisbehaviorisobservedformostoftheimagesinthe
Imagetask–seeAppendixA.6.3formoreexamples. Wefurtheranalyzetheclustersbyvisualizing
thescoresofA inFigure4b,wheretheseparationofforegroundandbackgroundismoreevident,
g
togetherwiththeseparationpersliceoftheimage.
5.5 LIMITATIONS
CASTissignificantlyfasterthanothermethodsbasedonefficienttransformers.Intermsofbenchmark
results,however,efficienttransformersincludingCASTperformlowerthanStructuredStateSpace
models. While a direct comparison is unfair, as the architecture and working principle of these
methodsaredifferentfromtransformers,withhighercomplexity,wereporttheirresultsforthesake
ofcompleteness. AcurrentlimitationofCASTistheabsenceofadecodingversionforgenerative
natural language. While the focus of the paper is however on the optimization of the attention
computationviaanovelclusteringofsurrogatetokensapproaches,weforeseethatCASTcouldbe
adapted using asymmetric clustering and casual masking to create a decoder and be deployed in
generativemodelsaswell.
6 CONCLUSIONS
WepresentCAST,amoreefficientdrop-inreplacementforself-attention,whichlowersthecomplexity
ofcomputingtheself-attentioninTransformers. Thesolutionthatweproposeisbasedonclustering
surrogatetokens,anovelapproachinwhichtheclusterdirectionsarelearnable,incontrastwithstatic,
algorithmicallydefinedclusterdirectionsofpreviousworks. Whileourcontributionispotentially
general,andapplicabletomanytasks,wefocusonanalyzingitsimpacttowardsimprovingefficient
computationofself-attentionintransformers,especiallyatinferencetime,whilemaintainingthe
highresultsofstandardtransformerarchitectures. Ourexperimentsdemonstratethatthememory
andcomputationalefficiencyofCASTissignificantlybetterthanotherefficientTransformers(e.g.
Reformer(Kitaevetal.,2020), Performer(Choromanskietal.,2020))intheLongRangeArena
efficiencybenchmark. CASTuseslessmemorythanexistingmethodsforalltestedsequencelengths,
beinguptoabout6×fasterthanandusing10%ofthememoryoftheoriginalTransformer. Our
futureworkwillexploreincreasingtheefficiencyofCASTbyparallelizingattentionwithinclusters,
andfindingmoreefficientclusteringmechanisms.
9REFERENCES
Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham,
AnirudhRavula,SumitSanghai,QifanWang,andLiYang. Etc: Encodinglongandstructured
inputsintransformers,2020.
IzBeltagy,MatthewE.Peters,andArmanCohan. Longformer: Thelong-documenttransformer,
2020.
LukasBiewald. Experimenttrackingwithweightsandbiases,2020. URLhttps://www.wandb.
com/. Softwareavailablefromwandb.com.
TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielM.Ziegler,
JeffreyWu,ClemensWinter,ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever,andDarioAmodei. Languagemodelsarefew-shotlearners,2020.
Yen-ChunChen,ZheGan,YuCheng,JingzhouLiu,andJingjingLiu. Distillingknowledgelearned
inbertfortextgeneration,2019. URLhttps://arxiv.org/abs/1911.03829.
LeiCheng,RuslanKhalitov,TongYu,andZhirongYang. Classificationoflongsequentialdatausing
circulardilatedconvolutionalneuralnetworks,2022.
RewonChild,ScottGray,AlecRadford,andIlyaSutskever. Generatinglongsequenceswithsparse
transformers,2019. URLhttps://arxiv.org/abs/1904.10509.
KrzysztofChoromanski,ValeriiLikhosherstov,DavidDohan,XingyouSong,AndreeaGane,Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy
Colwell, and Adrian Weller. Rethinking attention with performers, 2020. URL https://
arxiv.org/abs/2009.14794.
TriDao,DanielY.Fu,StefanoErmon,AtriRudra,andChristopherRé. Flashattention: Fastand
memory-efficientexactattentionwithio-awareness,2022.
TriDao,DanielY.Fu,KhaledK.Saab,ArminW.Thomas,AtriRudra,andChristopherRé. Hungry
hungryhippos: Towardslanguagemodelingwithstatespacemodels,2023.
Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G. Dimakis. Smyrf: Efficient
attentionusingasymmetricclustering,2020.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,
andNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionatscale,
2020. URLhttps://arxiv.org/abs/2010.11929.
AlbertGu,KaranGoel,andChristopherRé. Efficientlymodelinglongsequenceswithstructured
statespaces,2022.
Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and
DanielaRus. Liquidstructuralstate-spacemodels,2022.
SeppHochreiterandJürgenSchmidhuber. Longshort-termmemory. NeuralComputation,9(8):
1735–1780,1997.
John M. Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-
neberger,KathrynTunyasuvunakool,RussBates,AugustinZídek,AnnaPotapenko,AlexBridg-
land, Clemens Meyer, Simon A A Kohl, Andy Ballard, Andrew Cowie, Bernardino Romera-
Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David A.
Reiman,EllenClancy,MichalZielinski,MartinSteinegger,MichalinaPacholska,TamasBergham-
mer,SebastianBodenstein,DavidSilver,OriolVinyals,AndrewW.Senior,KorayKavukcuoglu,
PushmeetKohli,andDemisHassabis. Highlyaccurateproteinstructurepredictionwithalphafold.
Nature,596:583–589,2021.
10Ruslan Khalitov, Tong Yu, Lei Cheng, and Zhirong Yang. Sparse factorization of large square
matrices,2021.
RuslanKhalitov,TongYu,LeiCheng,andZhirongYang. Chordmixer: Ascalableneuralattention
modelforsequenceswithdifferentlengths,2023.
NikitaKitaev,ŁukaszKaiser,andAnselmLevskaya. Reformer: Theefficienttransformer,2020.
URLhttps://arxiv.org/abs/2001.04451.
AlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. pp.32–33,2009. URL
https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
JamesLee-Thorp,JoshuaAinslie,IlyaEckstein,andSantiagoOntanon. Fnet: Mixingtokenswith
fouriertransforms. arXivpreprintarXiv:2105.03824,2021.
YuhongLi,TianleCai,YiZhang,DemingChen,andDebadeeptaDey. Whatmakesconvolutional
modelsgreatonlongsequencemodeling?,2022.
DrewLinsley,JunkyungKim,VijayVeerabadran,andThomasSerre. Learninglong-rangespatial
dependencieswithhorizontalgated-recurrentunits. CoRR,abs/1805.08315,2018. URLhttp:
//arxiv.org/abs/1805.08315.
Liu Liu, Zheng Qu, Zhaodong Chen, Yufei Ding, and Yuan Xie. Transformer acceleration with
dynamicsparseattention. CoRR,abs/2110.11299,2021a. URLhttps://arxiv.org/abs/
2110.11299.
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows, 2021b. URL
https://arxiv.org/abs/2103.14030.
Minh-ThangLuong,HieuPham,andChristopherD.Manning. Effectiveapproachestoattention-
basedneuralmachinetranslation,2015. URLhttps://arxiv.org/abs/1508.04025.
XuezheMa,XiangKong,SinongWang,ChuntingZhou,JonathanMay,HaoMa,andLukeZettle-
moyer. Luna: Linearunifiednestedattention,2021. URLhttps://arxiv.org/abs/2106.
01540.
XuezheMa,ChuntingZhou,XiangKong,JunxianHe,LiangkeGui,GrahamNeubig,JonathanMay,
andLukeZettlemoyer. Mega: Movingaverageequippedgatedattention,2023.
AndrewL.Maas,RaymondE.Daly,PeterT.Pham,DanHuang,AndrewY.Ng,andChristopherPotts.
Learningwordvectorsforsentimentanalysis. InProceedingsofthe49thAnnualMeetingofthe
AssociationforComputationalLinguistics: HumanLanguageTechnologies-Volume1,HLT’11,
pp.142–150,Stroudsburg,PA,USA,2011.AssociationforComputationalLinguistics. ISBN978-
1-932432-87-9. URLhttp://dl.acm.org/citation.cfm?id=2002472.2002491.
DerekMiller. Leveragingbertforextractivetextsummarizationonlectures,2019. URLhttps:
//arxiv.org/abs/1906.04165.
OpenAI. Gpt-4technicalreport,2023.
DragomirR.Radev,PradeepMuthukrishnan,VahedQazvinian,andAmjadAbu-Jbara. Theaclan-
thologynetworkcorpus. Lang.Resour.Eval.,47(4):919–944,dec2013. ISSN1574-020X. doi: 10.
1007/s10579-012-9211-2. URLhttps://doi.org/10.1007/s10579-012-9211-2.
JimmyT.H.Smith,AndrewWarrington,andScottW.Linderman. Simplifiedstatespacelayersfor
sequencemodeling,2023.
Chi Sun, Luyao Huang, and Xipeng Qiu. Utilizing bert for aspect-based sentiment analysis via
constructingauxiliarysentence,2019. URLhttps://arxiv.org/abs/1903.09588.
Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer:
Rethinkingself-attentionintransformermodels,2020a. URLhttps://arxiv.org/abs/
2005.00743.
11YiTay,DaraBahri,LiuYang,DonaldMetzler,andDa-ChengJuan. Sparsesinkhornattention,2020b.
URLhttps://arxiv.org/abs/2002.11296.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
LiuYang,SebastianRuder,andDonaldMetzler. Longrangearena: Abenchmarkforefficient
transformers,2020c.
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-
terthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and
AlexeyDosovitskiy. Mlp-mixer: Anall-mlparchitectureforvision,2021.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,Armand
Joulin,EdouardGrave,andGuillaumeLample. Llama: Openandefficientfoundationlanguage
models,2023.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,Lukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed,2017.
SinongWang,BelindaZ.Li,MadianKhabsa,HanFang,andHaoMa. Linformer: Self-attention
withlinearcomplexity,2020.
TongYu,RuslanKhalitov,LeiCheng,andZhirongYang. Paramixer: Parameterizingmixinglinksin
sparsefactorsworksbetterthandot-productself-attention,2022.
ManzilZaheer,GuruGuruganesh,AvinavaDubey,JoshuaAinslie,ChrisAlberti,SantiagoOntanon,
PhilipPham,AnirudhRavula,QifanWang,LiYang,andAmrAhmed. Bigbird: Transformersfor
longersequences,2021.
Hao Zheng, Zhanlei Yang, Wenju Liu, Jizhong Liang, and Yanpeng Li. Improving deep neural
networks using softplus units. In 2015 International Joint Conference on Neural Networks
(IJCNN),pp.1–4,2015. doi: 10.1109/IJCNN.2015.7280459.
A APPENDIX
A.1 MODULARIZEDVISUALISATIONOFCAST
In Figure 5, a modularized verison of the visualization of CAST can be seen. In Figure 5a, we
applyIntraClusterAttentionbyclusteringtheinputX basedonthevaluesinA ,andthenapply
g
self-attention within the created clusters. In Figure 5b, we create the cluster summaries R ,
inter
whichisbasedonasumvaluesweightedbasedonthethekey-surrogatetokensimilaritymatrixA .
k
InFigure5c,welastlycombinetheresultsR andR basedonthequery-surrogatetoken
intra inter
similaritymatrixA ,resultingintheoutputofCAST.
q
A.2 NOMENCLATURE
WeincludeanomenclatureinTable3toaidinthereadingofSection3.2. Herethesymbolsaregiven
togetherwithwhattheyarerepresenting.
Table3: AnomenclatureofsymbolsusedintheequationsdefiningCAST.
Symbol Meaning/Representation
Aq Thedotproductsimilaritybetweenthequeries(Q)andthesurrogatetokens(S).
Ak Thedotproductsimilaritybetweenthekeys(K)andthesurrogatetokens(S).
ϕ Alearnedvaluesimilartothequeries/keys,representswhetheritismoreimportanttoshareorreceiveinformation.
Ag
ThecombinedsimilarityofAqandAk,whereAqhasmoreweightifphiishigh,
andAkhasmoreweightwhenphiislow.Usedasthebasisforclustering.
Xg,Qg,Kg,Vg Theclusteredtokens,queries,keys,andvalues.
Rintra Theresultofself-attentionwithineachcluster.
Avalue Theweightsfortheweightedsumoftheclustersummaries.
Rinter Theclustersummaries.
12(a)IntraClusterAttention (b) Creation of the cluster summaries R , which is a
inter
weightedsumofeachcluster’svaluesbasedontheweightsin
A .
k
(c)ThecombiningofR andR us-
inter intra
ingA astheweightsfortheweightedsum.
q
Figure5: Amodularizedsketchoftheproposedmethod. Here,somedetailsareomittedtomakeit
easiertoread. (a)showsintra-clusterself-attention,(b)showsthecreationoftheclustersummaries
R ,and(c)showshowR andR arecombined.
inter inter intra
A.3 DETAILSOFTHECLUSTERINGMECHANISMS
Inthissection,wedescribeourproposedTop-K clusteringmechanismandtheSATop-K clustering
mechanism.
A.3.1 TOP-KCLUSTERING
TheTop-K clusteringmechanismgroupstheindiceswiththelargestsimilarityscoresinA ,itallows
g
foratokentobeclusteredintotwoclusters,butalsoforatokennottobeclusteredatall. Aformal
definitionoftheTop-K clusteringmechanismisinAlgorithm1,whereA∈RN×Nc isthesimilarity
scoresforeachtokentoeachcluster,andX ∈RN×∗isamatrixoffeaturevectorsthatwewishto
cluster,where∗indicatesanyshape.
Algorithm1ImplementationoftheproposedTop-K clusteringmechanism.
InputX,A
OutputC
1: functionSATop-K(A,X)
2: C ={C 1...C Nc} ▷Initializeresult
3: I,A top =Top-K(A) ▷Gettheindicesofthelargestvaluespercluster
4: fori←1toN cdo
5: forj ←1to N do
Nc
6: i token =I i,j
7: C i.insert(X itoken)
8: endfor
9: endfor
10: endfunction
A.3.2 SINGLEASSIGNMENTTOP-KCLUSTERING
ThesingleassignmentTop-K clusteringmechanismhastheconstraintthateachtokenisassignedto
onlyasinglecluster,whilealsomaximizingthetotalsimilarityforallclusterscombined. TheSA
Top-K clusteringmechanismisformallydefinedinAlgorithm2,whereA∈RN×Nc representsthe
13similarityscoresforeachtokentoeachcluster,andX ∈RN×∗representsamatrixoffeaturevectors
thatwewishtocluster.
Algorithm2ImplementationoftheproposedSingleAssignmentTop-K clusteringmechanism.
InputX,A
OutputC
1: functionSATop-K(A,X)
2: Ac,Ic =sort 2(A) ▷Sortfromhighesttolowestcluster
3: Ar,Ir =sort 1(Ac) ▷Sortfromhighesttolowesttoken
4: C ={C 1...C Nc} ▷Initializeresult
5: M =0N ▷InitializeAssignmentMask
6: fori←1toN cdo
7: forj ←1toN do
8: j token =I jr
9: i cluster =I jc
token
10: ifM j =1orlength(C icluster)= NN
c
then
11: continueforloop
12: endif
13: C icluster.insert(X jtoken)
14: M jtoken =1
15: endfor
16: endfor
17: returnC
18: endfunction
A.4 LONGRANGEARENABENCHMARK
TheLongRangeArena(LRA)benchmarkcontainssixtasks[ListOps,Text,Retrieval,Image,Path,
andPath-X]thatrepresentadiverseandintricatespectrumofchallenges.Eachtaskdemandsadistinct
setofskills,rangingfromsemanticunderstandingandreasoningtoimagecomprehensionandlogical
manipulation. Thisdiverseselectionoftasksaimstoassessthecapabilitiesofarchitectures,ensuring
a comprehensive evaluation that goes beyond singular skill acquisition. Furthermore, the LRA
benchmarkholdsauniquepositionwithintheresearchcommunityasitisthecommonbenchmark
forcomparingefficiencyandperformance,suchasforthearchitecturesinLuna(Maetal.,2021),
MEGA(Maetal.,2023),andS4(Guetal.,2022). Thiswidespreadadoptionsignifiesaconsensus
amongresearchersregardingitssuitabilityforassessingtheperformanceofexperimentalframeworks,
includingourproposedCAST.Next,wedescribehowthesesixtasksaretreated.
ListOps. TheListOpsdatasetwascreatedfortestingtheparsingabilityoflatenttreemodels,buta
largerversionisnowusedintheLRAtotestthecapabilityofTransformerstolearnthehierarchical
structures. Thedataisasequenceoftokensrepresentingalargemathematicaloperationonlistsof
numbers. Thenumbers0to9areavailableasboththeinputoftheoperationsandthefinalresult.
Therearefourbasemathematicaloperations:
• MAX:Thelargestvalueinagivenlist.
• MIN:Thesmallestvalueinagivenlist.
• MED:Themedianvalueinagivenlist.
• SUMMOD:Thesumofthelistmodule10.
In the LRA the maximal length of the input sequence is set to 2K tokens. This is a ten-way
classificationtaskwhereaccuracyisusedastheevaluationmetric.
Text. TheTexttasktakestheIMDbreviewssentimentclassificationtask(Maasetal.,2011)andthe
charactersastokensintheinputsequence. Themaximumlengthoftheinputsequencesistruncated
orpaddedto4Ktokens. Thistaskisabinaryclassificationtaskwithaccuracyasitsmetric.
Retrieval. For the Retrieval task the ACL Anthology Network dataset (Radev et al., 2013) is
used. Forthisdataset,thetaskistodeterminewhethertwopapersarelinkedbyacitation. Both
papersarepassedtotheTransformervariant,creatingcompressedrepresentations,whicharethen
14combinedandpassedintoaclassificationhead. WiththissetuptheRetrievaltaskcanbeconsidered
abinaryclassificationtask. Tomakethetaskmorechallenging,character-leveltokenslikeinthetext
classificationtaskareusedinthesetup. Asequencelengthof4Ktokensisusedperdocumenttheuse
of8Ktokensperexample.
Image. TheImagetasktakestheCIFAR-10dataset(Krizhevsky,2009)asitsbase. Theimages
are first greyscaled into a single channel with each pixel having an 8-bit pixel intensity as its
representation. Thisresultsina32×32imagewhichisunrolledintoa1-Dsequence,thissequence
isthenusedasinputforaten-wayclassificationtask.
Pathfinder. ThePathfindertask(Linsleyetal.,2018)consistsofimagesof32×32wheretwo
dots,representedbycircles,areconnectedbydashedlines. Amodelisrequiredtomakeabinary
decisionofwhetherthetwodotsareconnectedbythedashedlines,however,therearealsodistraction
linesthatarenotconnectedtoanyofthedots. Justlikeintheimageclassificationtasktheimageis
unrolledintoasequencelengthof1024andusedasinputforthistask.
Path-X. ThePath-XtaskisamoreextremecaseoftheoriginalPathfinder,insteadoftheimage
being32×32itis128×128makingthesequencelength16timeslargerthantheoriginal. Apart
fromthesizethistaskisexactlythesameastheoriginalPathfinder. Itshouldbenotedthatthistask
hasnotyetbeenachievedwithahigher-than-randomaccuracywiththeconstraintsoftheLRA.
A.5 EXPERIMENTDETAILS
Foralltasks,wefollowthestandardsgivenintheoriginalLongRangeArenapaper(Tayetal.,2020c)
regardingthedataprocessingandtasksetup. Forourchoicesinmosthyperparameters,weusedthe
currentstate-of-the-art,MEGA(Maetal.,2023),asourbaselineregardingthenumberofweight
updates. Furthermore,weusetheirdatasplitsregardingalltasks. Thefinalhyperparametersused
forourreportedaccuracyareinTable4. ForboththereportedperformanceofTop-K andSATop-K
thesamehyperparametersareused. Generalhyperparametersincludetheaveragingoftheoutput
featuresoverthesequencefortheclassificationfeatures,theuseoflinearfeatureembeddingsfor
pixeltasks,theuseofsinusoidalpositionalembeddingsforalltasks,andanextranormalizationlayer
ontheoutputfeatureswhenpre-normalizationisused.
Table4: FinalhyperparametersforthebestperformingCASTmodelsinourhyperparametersweep.
Here,DepthindicatesthenumberofTransformerblocks,hthenumberofheads,dthenumberof
featuresintheself-attentionblock,d ,thenumberoffeaturesinthefeedforwardblock,d the
ff emb
numberoffeaturesintheembedding,N thenumberofclusters,Normthetypeofnormalization
c
beingused,BSthebatch-size,LRthelearningrate,WDtheweightdecay,andEpochsthenumberof
epochsthatweretrainedfor.
Task Depth h d d d N Norm Pre-norm BS LR WD Epochs
ff emb c
ListOps 4 8 64 128 256 10 Layer False 64 1e−3 1e−2 60
Text 4 4 64 128 256 20 Scale False 25 1e−3 1e−2 25
Retrieval 2 8 256 256 256 20 Layer False 8 1e−2 1e−2 5
Image 2 2 128 128 256 16 Batch True 50 5e−3 1e−2 200
Pathfinder 2 2 32 32 64 16 Batch True 128 1e−3 1e−2 200
A.6 DETAILEDRESULTS
In this section, we go into more depth regarding the results of CAST. We first give an extensive
overviewofotherlong-rangesequencemodelingarchitectures,andthenshowmoreexamplesofthe
visualanalysisthatwasdoneontheImagetask.
A.6.1 LONGRANGEARENAEFFICIENCY
We further compare the relative efficiency of CAST Top-K with the vanilla Transformer during
inferencetimeinTable5. Here,weobservethatduringinferenceCASTisstillsignificantlyfaster
andmorememoryefficienctthanthevanillaTransformer.
A.6.2 LONGRANGEARENAPERFORMANCE
InTable6,wereportadetailedlistofresultsofdifferentefficientTransformervariants,andother
long-rangesequencemodelsontheLongRangeArenabenchmark. Wedividethesemodelsinto
15Table5: SpeedandMemoryefficiencyoftheLRABenchmarkduringinference.
StepsPerSecond↑ PeakMemoryUsage↓
Model
1K 2K 3K 4K 1K 2K 3K 4K
Transformer 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
CAST(Top-K) 1.87 3.95 5.27 6.91 0.280 0.150 0.102 0.081
thefollowingcategories: TransformerBasedArchitectures,StructuredStateSpaceArchitectures,
andOtherArchitectures. WecanseethatamongtheTransformerBasedArchitectures(A)Luna
(Maetal.,2021)andCASTsimilarlystrongperformance. WhenitcomestotheStructuredState
SpaceArchitectures(B),itcanbeobservedthatallmodelsperformsimilarly,withMEGA(Maetal.,
2023)beingslightlybetterthantherest. Asfortheothertypesofarchitectures, theyneitheruse
self-attention nor structured state spaces to "mix" their input sequence. Among them, the recent
ChordMixer (Khalitov et al., 2023) stands out, ChordMixer was created for handling data with
extremelylongsequencelengths(intheorderof100Ktokens),buthasshownimpressiveresultson
theLRAbenchmarktoo.
Table6: TheperformanceofdifferentarchitecturesontheLongRangeArenabenchmarkinclassifi-
cationaccuracy. Wedividetheseworksinto(A)TransformerarchitecturesthatdonotuseStructured
StateSpacesoranyderivationofthis,(B)ArchitecturesusingStructuredStateSpaces,and(C)Other
typesofarchitectures. The(A)-relatedmodelsaregroupedas;(A-Top)TheoriginalTransformer
architecture,(A-Middle)efficientTransformerarchitecturesthatcameoutwiththeLRAbenchmark,
and(A-Bottom)notablemodelsthatcameoutafterthereleaseoftheLRAbenchmark. Herethe
symbol†indicatesthattheresultscamefromtheoriginalpaperfromtheLRAdataset(Tayetal.,
2020c). Furthermore,thesymbol×indicatesthattheTransformervarianteitherranoutofmemory
and−indicatesthatresultswerenotreported.
Model Year ListOps Text Retrieval Image Pathfinder Path-X Avg.
Random 10.00 50.00 50.00 10.00 50.00 50.00 36.67
(A)TransformerBasedArchitectures
Transformer†(Vaswanietal.,2017) 2017 36.37 64.27 57.46 42.44 71.40 × 53.66
Transformer(re-impl(Maetal.,2021)) 2017 37.11 65.21 79.14 42.94 71.83 × 57.71
SparseTrans.†(Childetal.,2019) 2019 17.07 63.58 59.59 44.24 71.71 × 51.03
LocalAtt.†(Tayetal.,2020c) 2020 15.82 52.98 53.39 41.46 66.63 × 46,71
Reformer†(Kitaevetal.,2020) 2020 37.27 56.10 53.40 38.07 68.50 × 50.56
SinkhornTrans.†(Tayetal.,2020b) 2020 33.67 61.20 53.83 41.23 67.45 × 51.23
Performer†(Choromanskietal.,2020) 2020 18.01 65.40 53.82 42.77 77.05 × 51.18
Linformer†(Wangetal.,2020) 2020 35.70 53.94 52.27 38.56 76.34 × 51.36
Longformer†(Beltagyetal.,2020) 2020 35.63 62.85 56.89 42.22 69.71 × 53.46
Synthesizer†(Tayetal.,2020a) 2021 36.99 61.68 54.67 41.61 69.45 × 52.40
BigBird†(Zaheeretal.,2021) 2021 36.05 64.02 59.29 40.83 74.87 × 54.18
Luna-16(Maetal.,2021) 2021 37.43 65.74 79.38 46.39 78.36 - 59.55
Luna-128(Maetal.,2021) 2021 38.01 65.74 79.55 47.47 78.89 - 59,94
Luna-256(Maetal.,2021) 2021 37.98 65.78 79.56 47.86 78.55 - 59,96
PSF(Khalitovetal.,2021) 2021 38.85 77.32 - 45.01 80.49 - 56.95
FNet(Lee-Thorpetal.,2021) 2021 35.33 65.11 59.61 38.67 77.80 × 54.42
CASTTop-K(Ours) 2023 39.90 65.45 78.01 52.37 70.18 × 59.32
CASTSATop-K(Ours) 2023 40.70 65.13 74.64 52.78 62.22 × 57.57
(B)StructuredStateSpaceArchitectures
S4(Guetal.,2022) 2021 59.60 86.82 90.90 88.65 94.20 96.35 86.09
H3(Daoetal.,2023) 2022 57.50 88.20 91.00 87.30 93.00 91.80 84.80
Liquid-S4(Hasanietal.,2022) 2022 62.75 89.02 91.20 89.50 94.8 96.66 87.32
SGConv(Lietal.,2022) 2022 61.45 89.20 91.11 87.97 95.46 97.83 87.17
S5(Smithetal.,2023) 2022 62.15 89.31 91.40 88.00 95.33 98.58 87.46
MEGA-Chunk(Maetal.,2023) 2022 58.76 90.19 90.97 85.80 94.41 93.81 85.66
MEGA(Maetal.,2023) 2022 63.14 90.43 91.25 90.44 96.01 97.98 88.21
(C)OtherArchitectures
Paramixer(Yuetal.,2022) 2022 39.57 83.32 - 46.58 80.49 - 58.33
CDIL(Chengetal.,2022) 2022 - 87.61 84.27 64.49 91.00 - 64.56
ChordMixer(Khalitovetal.,2023) 2023 60.12 88.82 89.98 90.17 96.69 98.63 87.40
16(a)ExampleImageof (b)ClusteredImage, (c)ClusteredImage,
theLRAImagetask. 8hashes,firstlayer. 8hashes,lastlayer.
Figure6: AvisualizationoflearnedclusterswhenusingLSHAttention. Here6ashowstheoriginal
inputimage,6bshowsthewaypixelsareclusteredinthefirstlayerofLSHAttention,and6cshows
thewaypixelsareclusteredinthelastlayerofLSHAttention
A.6.3 FURTHERVISUALANALYSIS
AdditionalvisualizationsoftheclusterscreatedfordifferentsamplesfromtheImagetaskofLRA,
can be seen in Figure 7 (a horse), Figure 8 (a deer), and Figure 9 (an automobile). For each of
thesefigures,subfigure(a)showstheoriginalinputimage,(b)showstheassignmentofclustersfor
eachpixelinthefirstlayer,and(c)showstheassignmentofclustersforeachpixelinthelastlayer.
Subfigure(d)showsforeachclusterthescoreinA thateachtokenhadforthefirstlayer. Subfigure
g
(e)showsforeachclusterthescoreinA thateachpixelhadforthelastlayer.
g
Forthementionedsampleimages,itcanbeseenthattheinthefirstlayer,i.e. inFigures7d,8d,
andFigure9d,eachclusterroughlyclustersthesamepixels. Thisbehaviorcouldoccur,becausethe
positionalembeddingsaremostprominentinthefirstlayer,causingthesurrogatetokenstocluster
basedonthispositionalembedding. Furthermore,italsoshowsthatCASTlearnstoclusterslices
oftheimagefirst,similartoconvolution. InFigures7e,8eand,Figure9e,thescoresinA forthe
g
lastlayercanbeseen. Thislayer(e)showsmoreimage-specificclustering. Forinstance,fromthese
scores,wecanobservetheoutlineandinverseoutlineofahorse,adeerinaforest,andanautomobile,
respectively. Weinterpretthisastheseparationofbackgroundandforeground. Inthecaseofthe
deer,Figure8e,weobserveamoreroughoutline,whichcanbeduetothefactthatthebackground
andforegroundofthisimagearemuchmoresimilar.
A.6.4 REFORMERVISUALANALYSIS
TodeterminewhetherthevisuospatialinformationcontainedintheclusteringofCASTisgeneral
featureofclusteringTransformersliketheReformerandSMYRF,wetrainedaReformermodel
ontheCIFAR-10datasetandinspectedtheclusteredimages. InFigure6, anexampleimagefor
clusteringcreatedbytheReformer’sLSHAttentioncanbeseen.
17(a) Example Image of the LRA(b) Clustered Image, N = 8,(c) Clustered Image, N = 8,
c c
Imagetask. FirstLayer. LastLayer.
(d)ScoresofA perclusterforthefirstlayerofaCASTmodeltrainedontheImagetaskofLRA.Here,each
g
imagecorrespondstoacluster,i.e.asinglecolumnofA .
g
(e)ScoresofA perclusterforthelastlayerofaCASTmodeltrainedontheImagetaskofLRA.Here,each
g
imagecorrespondstoacluster,i.e.asinglecolumnofA .
g
Figure7: AvisualizationlearnedclustersindifferentlayersofCAST.Here,(a)isasamplefromthe
ImageofLRA,depictingahorsewitharider,(b)isanimagerepresentingtheclusteredpixelsinthe
firstlayerofCAST,(c)isanimagerepresentingtheclusteredpixelsinthelastlayerofCAST,(d)the
scoresinA foreverypixelineachoftheeightclustersinthefirstlayerofCAST,(e)thescoresin
g
A foreverypixelineachoftheeightclustersinthelastlayerofCAST.
g
18(a) Example Image of the LRA(b) Clustered Image, N = 8,(c) Clustered Image, N = 8,
c c
Imagetask. FirstLayer. LastLayer.
(d)ScoresofA perclusterforthefirstlayerofaCASTmodeltrainedontheImagetaskofLRA.Here,each
g
imagecorrespondstoacluster,i.e.asinglecolumnofA .
g
(e)ScoresofA perclusterforthelastlayerofaCASTmodeltrainedontheImagetaskofLRA.Here,each
g
imagecorrespondstoacluster,i.e.asinglecolumnofA .
g
Figure8: AvisualizationofthelearnedclustersindifferentlayersofCAST.Here,(a)isasample
fromtheImageofLRA,depictingadeerinaforest,(b)isanimagerepresentingtheclusteredpixels
inthefirstlayerofCAST,(c)isanimagerepresentingtheclusteredpixelsinthelastlayerofCAST,
(d)thescoresinA foreverypixelineachoftheeightclustersinthefirstlayerofCAST,(e)the
g
scoresinA foreverypixelineachoftheeightclustersinthelastlayerofCAST.
g
19(a) Example Image of the LRA(b) Clustered Image, N = 8,(c) Clustered Image, N = 8,
c c
Imagetask. FirstLayer. LastLayer.
(d)ScoresofA perclusterforthefirstlayerofaCASTmodeltrainedontheImagetaskofLRA.Here,each
g
imagecorrespondstoacluster,i.e.asinglecolumnofA .
g
(e)ScoresofA perclusterforthelastlayerofaCASTmodeltrainedontheImagetaskofLRA.Here,each
g
imagecorrespondstoacluster,i.e.asinglecolumnofA .
g
Figure9: AvisualizationlearnedclustersindifferentlayersofCAST.Here,(a)isasamplefromthe
ImageofLRA,depictinganautomobile,(b)isanimagerepresentingtheclusteredpixelsinthefirst
layerofCAST,(c)isanimagerepresentingtheclusteredpixelsinthelastlayerofCAST,(d)the
scoresinA foreverypixelineachoftheeightclustersinthefirstlayerofCAST,(e)thescoresin
g
A foreverypixelineachoftheeightclustersinthelastlayerofCAST.
g
20