Joint Intrinsic Motivation for Coordinated Exploration in
Multi-Agent Deep Reinforcement Learning
ExtendedversionoftheextendedabstractpaperpublishedatAAMAS2024
MaximeToquebiau∗ NicolasBredeche†
ECEParis&SorbonneUniversité,CNRS,ISIR SorbonneUniversité,CNRS,ISIR
F-75005Paris,France F-75005Paris,France
maxime.toquebiau@gmail.com nicolas.bredeche@sorbonne-universite.fr
FaïzBenamar Jae-YunJun†
SorbonneUniversité,CNRS,ISIR ECEParis
F-75005Paris,France Paris,France
faiz.ben_amar@sorbonne-universite.fr jaeyunjk@gmail.com
ABSTRACT tacklelong-standingproblemsinMASsuchascreditassignmentor
Multi-agentdeepreinforcementlearning(MADRL)problemsoften partialobservability[6,11,22].Thesetechniquesareabletosolve
encounterthechallengeofsparserewards.Thischallengebecomes verycomplexmulti-agenttaskssuchasautonomousdriving[25]
evenmorepronouncedwhencoordinationamongagentsisneces- orreal-timestrategyvideogames[17].However,majorissuesstill
sary.Asperformancedependsnotonlyononeagent’sbehaviorbut remainwiththeseapproaches,suchastheproblemofrelativeover-
ratheronthejointbehaviorofmultipleagents,findinganadequate generalization[30,32]whereagentsstruggletofindtheoptimal
solutionbecomessignificantlyharder.Inthiscontext,agroupof jointpolicybecauselocalpoliciesareattractedtowardssuboptimal
agentscanbenefitfromactivelyexploringdifferentjointstrategies areasofthesearchspace.Thismakesmostalgorithmsinefficient
inordertodeterminethemostefficientone.Inthispaper,wepro- intaskswheretheoptimalstrategyrequiresstrongcoordination
poseanapproachforrewardingstrategieswhereagentscollectively amongagents.Relativeovergeneralizationcanbedescribedasa
exhibitnovelbehaviors.WepresentJIM(JointIntrinsicMotiva- problemofexplorationofthejoint-statespace:asthesuccessof
tion),amulti-agentintrinsicmotivationmethodthatfollowsthe theMASdependsonthecoordinationofmultipleagents,exploring
centralizedlearningwithdecentralizedexecutionparadigm.JIM thejoint-observationspaceisrequiredtodiscoveroptimaljoint
rewardsjointtrajectoriesbasedonacentralizedmeasureofnovelty behaviors.Inthispaper,weaddressthequestionofhowtoexplore
designedtofunctionincontinuousenvironments.Wedemonstrate thejoint-statespacetoefficientlydiscoversuperiorcoordinated
thestrengthsofthisapproachbothinasyntheticenvironmentde- strategiesforsolvingthetaskathand.
signedtorevealshortcomingsofstate-of-the-artMADRLmethods, Insingle-agentRL,theproblemofexplorationhasbeenstudied
andinsimulatedrobotictasks.Resultsshowthatjointexploration tosolvehardexplorationtaskswherepositiverewardsignalsare
iscrucialforsolvingtaskswheretheoptimalstrategyrequiresa verysparse.Onesolutionistouseintrinsicmotivation[10,18,24]
highlevelofcoordination. toinciteagentstoexploreunknownpartsoftheenvironment.In
additiontotheenvironmentreward,agentsaregivenanauxiliary
Keywords. Multi-agentSystems,DeepReinforcementLearning, rewardrelatedtothenoveltyofencounteredstates.Maximizing
IntrinsicMotivation thisintrinsicrewardleadsagentstovisitpreviouslyunexplored
regionsoftheenvironment,ultimatelydiscoveringnewsolutions
1 INTRODUCTION
tothetask.ThesemethodshaveshowngreatsuccessinhelpingRL
Onecrucialaspectofhumanintelligenceisitsabilitytoactcoinci- agentssolvehardexplorationtasks[2,19].
dentallywithotherhumanbeings,toeithercooperateorcompetein Inthemulti-agentsetting,intrinsicobjectiveshavealsobeen
agiventask.Thishasledresearcherstostudyreinforcementlearn- studiedtoinducedifferentkindsofbehaviorsinagentssuchasco-
ing(RL)inthecontextofmulti-agentsystems(MAS),wheremulti- ordinatedexploration[8],socialinfluence[9,29]oralignmentwith
pleartificialagentsinteractwiththeirenvironmentandeachother otheragents’expectations[13].However,previousworkshaveonly
whileconcurrentlylearningtoperformatask[11,28].However, usedlocalobservationstogenerateintrinsicrewards.Withpartial
havingmultipleagentsintheenvironmentmakestheRLprocess observability,localobservationsoftenlackcrucialinformationto
significantlymoredifficultforseveralreasons[33].Inparticular, fullyunderstandthecurrentconfigurationoftheenvironment.In
theglobalrewarddependsontheactionsofseveralindependent thecontextofexploration,anintrinsicrewardbasedonlyonlocal
agents,whichmakesthesearchfortheoptimaljointpolicymore observationswillleadtoeachagentexploringtheirownobserva-
complicated. tionspace,withoutconsideringthecurrentstateofotheragents.
Recently,multi-agentdeepreinforcementlearning(MADRL)ap- Thiscanresultininefficientexplorationincooperativetaskswhere
proacheshavecombinedadvancementsinRLanddeeplearningto thesuccessoftheMASdependsonthecoordinationofallagents.
∗Contactauthor:maxime.toquebiau@gmail.com.
†Authorscontributedequallytothepaper.
4202
beF
6
]AM.sc[
1v27930.2042:viXraMaximeToquebiau,NicolasBredeche,FaïzBenamar,andJae-YunJun
Inthispaper,weintroduceanovelmulti-agentexplorationap- modelswillyieldlownoveltyforstatessimilartowhattheyhave
proachcalledJointIntrinsicMotivation(JIM)whichcanbecom- trainedonwhileproducinghighnoveltyforunknownpartsofthe
bined with any MADRL algorithm that follows the centralized environment.RIDE[20]andNovelD[35]userespectivelyICMand
trainingwithdecentralizedexecutionparadigm(CTDE).JIMex- RNDtocomputearewardfromthedifferenceofnoveltybetween
ploitscentralizedinformationtomotivateagentstoexplorenew thenextstateandthecurrentstate,pushingtheagentstoalways
coordinatedbehaviors.Inordertocomputejointnovelty,JIMbuilds seeknovelstates.Similarly,NGU[2]andE3B[7]useclustering
fromtwostate-of-the-artapproaches:NovelD[35]forexploring techniquestorewardstatesthataredistantfrompreviousstates.
unknownpartsoftheenvironment,andE3B[7]forhavingmore Finally,asimilarapproachisproposedbyAGAC[5]whichtrains
diversetrajectories.Addingthisauxiliaryrewardtotheagents’ anadversarialpolicytopredictthemainpolicy’soutput,thelatter
objectiveincitesthemtodiversifytheircollectivebehavioruntil beingrewardedwiththeformer’spredictionerror.
theyhaveafairknowledgeoftheenvironmentandcanfocuson InMADRL,recentworkshavedemonstratedtheeffectiveness
themaintaskathand. ofintrinsicrewardsinpromotingdesirablebehaviorsingroups
Todemonstratetheadvantagesofourapproach,wefirstdesign of agents. One example is social influence [9, 29] that rewards
asimpletestenvironmenttoshowcaseaclearexampleofrelative agentsforperformingactionsthathaveasignificantimpacton
overgeneralization.Weshowthatthestate-of-the-artalgorithm otheragents.Maetal.[13]proposeanintrinsicrewardbasedon
QMIX[22]strugglesinthisscenarioandthatmotivatingtheex- theaveragealignmentwithotheragents’expectations,promoting
plorationofcoordinatedbehaviorhelpssolvethetask.Next,we morepredictablebehaviorsinagents.Lupuetal.[12]proposeto
validatetheseresultsinacontinuousvirtualenvironment,showing rewardpoliciesthatperformdiversetrajectoriesincomparisonto
thatcoordinationtasksbenefitfromjointexploration.Finally,fur- apopulationofagents,whichisshowntohelptrainagentstobe
theranalysisisconductedtoconfirmthestrengthandscalability moreversatile.Duetal.[4]useintrinsicobjectivesasacreditas-
ofourapproach. signmenttechnique.Finally,IqbalandSha[8]proposeanapproach
forcoordinatedexplorationusingseveralmetricsforestimating
thenoveltyofobservationsthatdependonallagents’pastexpe-
2 RELATEDWORKS
riences.However,theirmodeliscomputationallyexpensiveand
Inrecentyears,deepreinforcementlearningtechniqueshavebeen doesnotaddresstheexplorationofthejoint-observationspace,
usedinthecontextofMAStotacklelong-standingissuesinmulti- whichcanbeproblematicforhardexplorationtaskswhererelative
agentlearning.Successfulsingle-agentRLapproacheshavebeen overgeneralizationcanoccur.
adaptedtotheCTDEframework[11,34],usingacentralizedvalue Inthispaper,weaddressthechallengeofrelativeovergener-
function to guide the training of decentralized policies. Recent alizationbyrewardingagentsforexploringthejoint-observation
studieshaveinvestigatedtheproblemofcreditassignment[6]in space.Inthefollowingsections,wewillpresentthenecessaryfor-
MADRL,i.e.,distributingtheglobalrewardamongagentsbased malbackgroundandanoverviewoftheproposedalgorithmthat
on their participation. Value factorization methods also do this implementsjointintrinsicmotivation.
implicitly[27],combiningtheoutputoflocalvaluefunctionsinto
acentralizedonethatpredictsthecurrentvalueofthesystem.In
3 BACKGROUND
particular,QMIX[22]usesaseparatenetworktopredicttheQ-value
ofthejointaction,giventheoutputoflocalQ-valuesandtheglobal 3.1 Dec-POMDP
stateoftheenvironment.QMIXhasestablisheditselfasalong- Todescribecooperativemulti-agenttasks,weusethedefinitionof
standingstate-of-the-artapproach,despiteitsinherentlimitations decentralizedpartially-observableMarkovdecisionprocess(Dec-
thatseveralworkshavetriedtosurpass[21,26].However,MADRL POMDP)[16],definedasatuple⟨S,A,𝑇,O,𝑂,𝑅,𝑛,𝛾⟩with𝑛being
algorithmshavebeenshowntosufferfromtheproblemofrelative thenumberofagents.Sdescribesthesetofglobalstates𝑠ofthe
overgeneralization[31,32].Sofar,fewworkshaveaddressedthis environment. O is the set of joint observations, with one joint
problem:Weietal.[31]proposemaximumentropyRLtoexplore observationo={𝑜 1,...,𝑜 𝑛}∈O,andAthesetofjointactions,with
thejoint-actionspace,andMAVEN[14]augmentsQMIXusinga onejointactiona = {𝑎 1,...,𝑎 𝑛} ∈ A.𝑇 isthetransitionfunction
hierarchicalpolicytoguidetheexplorationofjointbehaviors. definingtheprobability𝑃(𝑠′|𝑠,a)totransitionfromstate𝑠tonext
Apromisingapproachtoovercomerelativeovergeneralization state𝑠′withthejointactiona.𝑂istheobservationfunctiondefining
istointrinsicallymotivateagentstoexploretheirenvironment, theprobability𝑃(o|a,𝑠′)toobservethejointobservationoafter
ultimatelydiscoveringtheoptimalrewardsignals.Insingle-agent takingjointactionaandendingupin𝑠′.𝑅 : O×A → Risthe
RL,curiosityhasbeendefinedtohelpagentssolvehardexploration rewardfunctionproducingateachtimesteptherewardsharedby
tasks[10,18,24]byrewardingthevisitationofstatesconsidered allagents.Finally,𝛾 ∈ [0,1)isthediscountfactorcontrollingthe
asnovel.Formeasuringnovelty,severalmethodshaveusedthe importanceofimmediaterewardsagainstfuturegains.
erroroftrainablepredictionmodels.TheIntrinsicCuriosityModule
(ICM)[19]trainsamodelofenvironmentdynamicsandusesthe
3.2 Intrinsicrewards
predictionerrorasameasureofnovelty.RandomNetworkDis-
tillation(RND)[3]usesatargetnetworkthatproducesarandom InSection2,weintroducedintrinsicmotivationasawaytoincite
encodingofthestateandtrainsapredictornetworktogenerate agentstoactivelyexploretheirenvironment.Tothisend,ateach
thesameencoding,thepredictionerrorbeingthemeasureofnov- timestep𝑡,agentsreceiveanaugmentedreward𝑟 𝑡 =𝑟 𝑡ext+𝛽𝑟 𝑡int,
elty.Theideabehindthesetwoapproachesisthattheprediction where𝑟 𝑡extistheextrinsicrewardgivenbytheenvironment,𝑟 𝑡intJointIntrinsicMotivationforCoordinatedExplorationinMADRL
istheintrinsicreward,and𝛽isahyperparametercontrollingthe
weightoftheintrinsicrewardintheagents’objective.
Inthissection,wedescribethreemethodsofintrinsicrewards
fromtheliteraturethatwewilluselaterinSection4.2.
Random Network Distillation (RND). In RND, Burda et al. [3]
computenoveltyusingtwoneuralnetworkswiththesamearchi-
tecture:atargetnetwork𝜙andapredictornetwork𝜙′.Thetarget’s 𝐴 𝐵 𝐶
parametersareinitializedrandomlyandfixed.Ittakesasinputthe 𝐴 10 −5 −5
state𝑠 𝑡 andproducesarandomembedding𝜙(𝑠 𝑡).Thepredictoris 𝐵 −5 7 7
𝐶 −5 7 7
trainedtooutputthesameembedding,minimizingtheEuclidean
distance: (a) (b)
𝑅𝑁𝐷 𝑡(𝑠 𝑡)=∥𝜙(𝑠 𝑡)−𝜙′(𝑠 𝑡)∥ 2. (1)
Thisdistanceisusedasameasureofthenoveltyofstate𝑠 𝑡 andis Figure1:Twoexamplesofrelativeovergeneralization:(a)
payoff matrix of a social dilemma game, (b) heat-map of
givenasanintrinsicrewardtoagents.
therewardfunctionintherel_overgenenvironmentfortwo
NoveltyDifference(NovelD). Zhangetal.[35]builduponRNDto agents,with𝐷 =40and𝛿 =30.DetailsinSection6.1.
deviseanoveltycriteriontermedNovelD.Itisdefinedasfollows:
𝑁(𝑠 𝑡,𝑠 𝑡+1)=max[𝑅𝑁𝐷(𝑠 𝑡+1)−𝛼𝑅𝑁𝐷(𝑠 𝑡),0]×
Then,wedefineourintrinsicrewardandexplainhowitisusedin
×1{𝑁 𝑒(𝑠 𝑡+1)=1}, (2) amulti-agentsettingwithJIM.
with𝛼 ascalingfactorand𝑁 𝑒 anepisodiccountofvisitedstates.
4.1 Thechallengeofcoordinatedactions
Thefirstpartisthecoreofthenoveltycriterion.ItusesRNDto
rewardagentsforpositivegainsinnoveltybetweenthecurrent Addressinghardexplorationenvironmentsischallengingbecause
andthenextstates.Thesecondpartisanepisodicrestrictionthat ofthesparsepositiverewardsignalsthatexisttoguidetheagent’s
ensurestherewardisgivenonlywhenstate𝑠 𝑡+1isobservedforthe learningprocess.ThisbecomesevenworsewithMASasthecom-
firsttimeinthisepisode.ThisrestrictionlimitstheuseofNovelD pletionofataskdependsontheactionsofmultipleindependent
todiscretestatespacesasitreliesonanexplicitcountofvisited agents.Whenstrongcoordinationisneeded,agentswillstruggle
states. to find the optimal strategy and settle for an easier suboptimal
jointstrategy,whichisaproblemknownasrelativeovergeneral-
ExplorationviaEllipticalEpisodicBonuses(E3B). WithE3B,Henaff
ization[30,32].Figure1aprovidesanexampleofasocialdilemma
etal.[7]proposeanepisodicbonusbasedonthepositionofthe
gamewhererelativeovergeneralizationoccurs.Theoptimalstrat-
observedstatewithrespecttoanellipsethatfitsallstatesprevi-
egyrequiresbothagentstochooseactionA.Butifonlyoneagent
ouslyencounteredinthecurrentepisode.Formally,itiscomputed
choosesactionA,thepayoffisverybad.Therefore,agentswill
asfollows:
independentlyprefertotakeactionsBorC,asactionAmostoften
𝑏(𝑠 𝑡)=𝜓(𝑠 𝑡)⊤𝐶 𝑡− −1 1𝜓(𝑠 𝑡), (3)
leadstosub-optimaloutcomes.
with InMAS,thiscanbeseenasaproblemofill-coordinatedexplo-
𝐶
𝑡−1=𝑡 ∑︁−1
𝜓(𝑠 𝑖)𝜓(𝑠 𝑖)⊤+𝜆𝐼, (4)
r oa ft ji oo in n. tA ps ols iu cic ec se is ss rd ee qp ue in red ds io nn oc ro do errd ti on dat ise cd ob ve eh ra wv hio icr hs, oe nx ep slo lr ea at dio tn
o
𝑖=1
optimalreturns.IntheexampleofFigure1a,exploringindependent
where𝐼 istheidentitymatrixand𝜆 ascalarcoefficient.𝜓 isan
strategieswillleadtoultimatelychoosingsuboptimalactionsas
embeddingnetworktrainedusinganinversedynamicsmodel[19]:
theyindividuallymayyieldbetterexpectedreturns.Ontheother
embeddingsoffollowingstates𝜓(𝑠 𝑡) and𝜓(𝑠 𝑡+1) areusedbya
hand,wearguethatuniformlyexploringjointactionswouldenable
separate neural network trained to predict the action 𝑎 𝑡 taken agentstochooseoptimaljointstrategiesmoreoftenandconse-
betweenthesestates.Asaresultofthistrainingprocess,𝜓 encodes
quentlylearnmoreefficientindividualbehaviors.Theapproach
partsoftheobservationthatarecontrollablebytheagents(details
describedinthefollowingtwosectionsimplementsanalgorithm
in[7]).Intuitively,𝑏 canbeunderstoodasageneralizationofa
thatefficientlyrewardsagentsforexploringthejoint-observation
count-basedepisodicbonusforacontinuousstatespace.Statesthat
space,inordertoconsistentlyfindoptimalstrategies.
areclosetopreviouslyencounteredstatesinthecurrentepisode
willyieldlowbonuses,whereasstatesthatareverydifferentwill
4.2 Double-timescaleIntrinsicReward
producehighbonuses.
Similarlytopreviousworksonsingle-agentintrinsicmotivation[2],
4 ALGORITHM wedefineanoveltymetricthatcombinestwoexplorationcriteria
workingatdifferenttimescales:
Inthissection,weintroducetheJointIntrinsicMotivation(JIM)ex-
plorationcriterionforcoordinatedmulti-agentexploration.Firstly, • Alife-longexplorationcriterion(LLEC)thatcaptures
wedescribethemotivationbehindourapproachbyprovidinga hownovelisthecurrentobservationwithrespecttoall
detaileddescriptionoftheproblemofrelativeovergeneralization. observationssincethebeginningoftraining.MaximeToquebiau,NicolasBredeche,FaïzBenamar,andJae-YunJun
• Anepisodicexplorationcriterion(EEC)thatcaptures
thedifferencebetweenthecurrentobservationandallpre-
viousobservationsinthecurrentepisode.
Intuitively,thelife-longrewardmotivatesagentstosearchfornever-
experienced parts of the environment. Meanwhile, the episodic
bonusinducesmorediversetrajectories.Thesetwoelementswill
feedeachotherandreinforceagentstoefficientlyexploretheir
environment.
Concretely,foreachtransitionfromstate𝑠 𝑡 tothenextstate
𝑠 𝑡+1,wedefinethedouble-timescaleintrinsicrewardasfollows:
𝑟 𝑡(𝑠 𝑡,𝑠 𝑡+1)=𝑁 𝐿𝐿𝐸𝐶(𝑠 𝑡,𝑠 𝑡+1)×𝑁 𝐸𝐸𝐶(𝑠 𝑡+1), (5)
withthelife-longnovelty𝑁 𝐿𝐿𝐸𝐶 inspiredfromNovelD[35](see
Eq.(2)):
Figure2:ArchitecturefortheJointIntrinsicMotivation(JIM)
𝑁 𝐿𝐿𝐸𝐶(𝑠 𝑡,𝑠 𝑡+1)=max[𝑅𝑁𝐷(𝑠 𝑡+1)−𝛼𝑅𝑁𝐷(𝑠 𝑡),0], (6) algorithm.JIMhasonlyoneintrinsicmotivationmodulefor
with𝛼 ascalingfactorand𝑅𝑁𝐷thenoveltymeasure(seeEq.(1)). the whole multi-agent system, computing novelty of the
Further,theepisodicnovelty𝑁 𝐸𝐸𝐶 usesthebonusfromE3B[7] jointobservationo𝑡.However,agentsonlyusetheirlocal
observationtochoosetheiraction.
(seeEq.(3)):
𝑁 𝐸𝐸𝐶(𝑠 𝑡+1)=√︁ 2𝑏(𝑠 𝑡+1). (7)
We remove the episodic restriction of NovelD as it relies on
agent,JIMcomputesonlyoneintrinsicreward.Thisrequiresfewer
anepisodiccountofvisitedstates.Thismakesitimpracticalina
parametersandmakesitpossibletocapturenoveltyattheteam
continuousstatespace,asonestateisveryunlikelytobevisited
level,ratherthanattheindividuallevel.Asagentsarerewardedby
twice.Instead,wescalethelife-longnoveltyusingtheelliptical
thenoveltyofthejointobservation,theywilllearntosearchfor
episodic bonus𝑏 from E3B [7]. This bonus acts as an episodic
newcombinationsofobservationswithotheragentsofthesystem,
restrictionbyscaling𝑁 𝐿𝐿𝐸𝐶 upordown,dependingonthenovelty ratherthanonlyexploringtheirlocal-observationspace.
ofthecurrentstatecomparedtowhathasbeenobservedduringthe
AsJIMusesjointobservationsforcomputingtheintrinsicreward,
currentepisode.As𝑏providesverylargebonusesanddecreases
itcanbeassociatedwithanyMADRLalgorithmthatfitsinthe
veryfast,weuse√︁ 2𝑏(𝑠 𝑡+1)tobothsmoothoutlargevaluesand
CTDEparadigm.Thesealgorithmsusuallyemployacentralized
increasesmallones. valuefunction[11,22,34]thatlooksatthejointobservationto
Combiningthesetworewardsmakesitpossibletotaketheben- predict the value of the agents’ actions. Such centralized value
efitsofboth.𝑁 𝐿𝐿𝐸𝐶 pushesagentstoexploreregionsofthestate functionswillbeabletoassociaterewardsprovidedbyJIMtonew
spacethatarenotwell-knowntoagents.Meanwhile,𝑁 𝐸𝐸𝐶 favors configurationsinthejointobservationspace,thusinducingagents
diversetrajectories,incitingagentstoalwaysseeknewobservations toactivelysearchfortheseconfigurations.
duringasingleepisode.Astheagentsexploretheirenvironment, Onecouldnotethatthejointobservationhastwonotabledraw-
thepredictionerrorofRND(seeEq.(1))slowlydecreases.Thus, backs:thenumberofdimensionsgrowslinearlywiththenumberof
𝑁 𝐿𝐿𝐸𝐶 decreasesaswell,tendingtowardzero,allowingagentsto agentsandthereisariskofcapturingredundantinformation.These
progressivelyfocusontheextrinsicreward.Finally,astheepisodic issuesarebothalleviatedbyusingembeddingnetworkstocapture
restrictiondoesnotrelyonanyexplicitcountofvisitedstates,it thecurrentstateofthejointobservationintoamorecondensed
canbeusedincontinuousstatespaces. latentrepresentation.Both𝑁 𝐿𝐿𝐸𝐶 and𝑁 𝐸𝐸𝐶 useembeddingnet-
works,respectively𝜙 and𝜓 (asdescribedinSection3.2),toencode
4.3 TheJointIntrinsicMotivationalgorithm
thejointobservation.Thisallowsforamorecontrollablenumber
Buildingfromtheintrinsicrewardintroducedpreviously,wepro- ofparametersinJIM,asonlythedimensionoftheinputlayersof
posetheJointIntrinsicMotivation(JIM)algorithmtoinciteMADRL 𝜙 and𝜓 dependonthesizeofthejointobservation.Furthermore,
agentstoexplorethejoint-observationspace.Ateachtimestep,all embeddingnetworkslearntocastawayuselessorredundantinfor-
agentsreceivethesameglobalreward𝑟 𝑡 =𝑟 𝑡ext+𝛽𝑟 𝑡𝐽𝐼𝑀 ,where𝑟 𝑡ext mationinordertoproduceafaithful,morecompactrepresentation
istheextrinsicrewardgivenbytheenvironment,𝑟 𝑡𝐽𝐼𝑀 isourjoint ofthejointobservation.Itisimportanttonotethatboth𝜙 and𝜓
explorationcriterion,and𝛽isahyper-parametercontrollingthe wereoriginallyused(respectivelyinRND[3]andE3B[7])with
rawpixelimagesasinput,showingthesignificantdimensionality
weightoftheintrinsicreward.TheexplorationcriterioninJIMuses
reductioncapabilitiesofthesetechniques.
thedouble-timescaleintrinsicrewarddefinedearliertocompute
thenoveltyofthejointobservation:
5 IMPLEMENTATIONDETAILS
𝑟 𝑡𝐽𝐼𝑀 (o𝑡,o𝑡+1)=𝑁 𝐿𝐿𝐸𝐶(o𝑡,o𝑡+1)×𝑁 𝐸𝐸𝐶(o𝑡+1), (8)
Inthenextsection,weuseJIMwithQMIX[22].Weusethedefault
whereo𝑡 = {𝑜 𝑡𝑖} 0≤𝑖≤𝑁,i.e.,theconcatenationofalllocalobser- QMIXarchitectureandhyperparameters,alongwithprioritized
vations.Figure2showsthearchitectureforJIM.Comparedtoa experiencereplay[23].Inallexperiments,wecomparethreealgo-
localmethodthatwoulduseoneintrinsicmotivationmoduleper rithms:JointIntrinsicMotivationforCoordinatedExplorationinMADRL
(a)easy(𝛿 =30) (b)hard(𝛿 =40) (c)veryhard(𝛿 =50)
Figure3:PerformanceofvariantsofQMIXintherel_overgenenvironment,withthreelevelsofdifficulty.Ontop,weshow
theheatmapsrepresentingtherewardfunctionineachinstance,wherethedifficultyisdictatedbythewidthcoefficientof
theoptimalrewardspike𝛿 (asdefinedinEq.(9)).Increasing𝛿 leadstoasmalleroptimalrewardspike.Belowisshownthe
performanceduringtrainingofQMIXwithnointrinsicreward(QMIX),localintrinsicmotivation(QMIX+LIM),andjoint
intrinsicmotivation(QMIX+JIM)(meanandstandarddeviationshownfor15runseach).Weseethataslightdecreaseinthe
sizeoftheoptimalrewardspikeresultsinaconsiderableincreaseinthedifficultyofthetask.
• QMIX+JIM,augmentingQMIXwithjointexploration,as inacontinuousenvironmentandshowthatexploringthejoint-
showninFigure2anddescribedinSection4.2. observationspacehelpssolvecooperativetasks.Next,wepresent
• QMIX+LIM,adegradedversionofQMIX+JIMwherelo- anablationstudybycomparingJIMwithtwosimplerversionsthat
cal(ratherthanglobal)intrinsicmotivationisused.Each eachlackoneofthetwoexplorationcriteriadescribedinSection4.2,
agentgeneratesitsownintrinsicrewardbasedsolelyonits showingtheadvantageofcombiningthetwo.Finally,weshowin
localobservation,usingthesamerewarddefinitionasJIM Section6.4thatJIMremainsrelevantwhenproblemsarescaledup.
(seeSection4.2).ThearchitectureforLIM(LocalIntrinsic
Motivation)isdescribedinAppendixA. 6.1 Addressingrelativeovergeneralization
• Theoriginalstate-of-the-artQMIXalgorithm[22]withno
6.1.1 Environment definition. To demonstrate how joint explo-
intrinsicmotivation,usedasabaseline.
rationhelpssolvetheproblemofrelativeovergeneralization,we
Notethattheonlydifferencebetweenthesethreealgorithmsis designasimpletestenvironmentthatexpandstheexampleshown
thedefinitionoftherewardfunctiongiventoeachagentduring inFigure1a.Inthisenvironmentcalledrel_overgen,twoagents
training.Theactualtrainingandexecutionalgorithmsareidentical. canmoveonadiscreteone-dimensionalaxiswith𝐷possiblepo-
ToensureafaircomparisonbetweenJIMandLIM,weusedif- sitions.Thetwoagentsaredenotedbytheirposition,namelyx
ferentvaluesforsomespecifichyperparameters(e.g.,dimensionof (forthefirstagent)andy(forthesecondagent).Ateachtimestep,
thehiddenlayers)inthetwoversionsinorderforthemtohavea agentsobservetheirpositionasaone-hotvector(e.g.,foragentx,
similarnumberoftrainableparameters.Allhyperparametersused 𝑜 𝑡x={𝑜 𝑡x,𝑖 =1ifx=𝑖, 0otherwise} 0≤𝑖<𝐷)andcanchoosebetween
inourexperimentsarelistedinAppendixBT˙hecodeusedtorun
threeactions:moveinonedirectionortheother,orstayinposition.
allexperimentsisfreelyavailableonline1.
Theyreceivearewardcorrespondingtotheircombinedposition:
6 EXPERIMENTS 𝑟 𝑡ext(𝑥,𝑦;𝛿)=max(cid:16) 𝑅+− 𝐷𝛿 (cid:2) (𝑥−𝑟 𝑥+)2+(𝑦−𝑟 𝑦+)2(cid:3),
Inthissection,wepresentasetofexperimentstoevaluatethe (9)
explorationcriterionofJIMwhenusedalongthestate-of-the-art
𝑅−− 81 𝐷(cid:2) (𝑥−𝑟 𝑥−)2+(𝑦−𝑟 𝑦−)2(cid:3)(cid:17) .
QMIXalgorithm[22].First,weshowtheresultsinasyntheticdis-
creteenvironmentwheretheproblemofrelativeovergeneralization TheresultofthisformulaisdisplayedinFigure1b.Thereward
canbeartificiallytunedandobservethatJIMhelpsalleviatethis combines two hyperboles in opposite corners: one narrow that
issue.Then,wetestourapproachonpseudo-realisticrobotictasks
culminatesat𝑅+atposition(𝑟 𝑥+,𝑟 𝑦+),andanothermuchwiderthat
plateausat𝑅− atposition(𝑟 𝑥−,𝑟 𝑦−).Wesettheoptimalreward𝑅+
1https://github.com/MToquebiau/Joint-Intrinsic-Motivation to12andthesuboptimal𝑅− to0.ThewidthoftheoptimalrewardMaximeToquebiau,NicolasBredeche,FaïzBenamar,andJae-YunJun
spikeiscontrolledbytheparameter𝛿:ahigher𝛿 valueyieldsa
narrowerspike.
Thegoaloftheagentsistofindwheretogotomaximizethe
globalreward.Thewidesuboptimalhyperboleisdeceptiveasit
isanobviouspathforagentstominimizetheirloss.Theoptimal
rewardspikeisdifficulttofindbecauseitcoversasmallportion
ofthestatespace,butitguaranteesmuchgreaterreturns.Wecan
varythedifficultyofthetaskbychangingthewidthofthisoptimal
(a) (b)
rewardspike:thenarrowerthespike,theharderitistofind.
Inthisenvironment,weexpectMADRLmethodstostruggleto
Figure4:ScreenshotsofourcustomtasksintheMPE,agents
findtheoptimalrewardspike.Exploringlocalstatescouldhelp
arethesmallgreycircles.(a)cooperativeboxpushingscenario:
butwouldnotbesufficienttoconsistentlysolvethetask.Asthe
agentsmustdelivertheobject(greencircleinthemiddle)
dimension𝐷ofthelocal-statespaceisfairlysmall,noveltyrewards
tothelandmarkinthebottomrightcorner.(b)coordinated
willquicklyvanishandwillnothelpagentsfindtheoptimalreward
placementscenario:agentsmustnavigatetopositionthem-
spike.Exploringthejoint-observationspaceadequatelyisrequired
selvesonthecoloredcirclesrepresentinglandmarks.
inordertoconsistentlyfindoptimalrewards.AsJIMwillreward
explorationuntilallcombinedpositions(𝑥,𝑦)arevisitedseveral
times,agentswillvisittheoptimalrewardspikemoreoften,thus
helpingthemtolearntheoptimalcoordinatedstrategy.
6.1.2 Results. TheresultsshowninFigure3confirmthehypothe-
sesformulatedintheprevioussection.Weshowtheperformance
ofQMIX,QMIX+LIM,andQMIX+JIMacross15independentruns
each.Further,wepresentresultsinthreedifficultylevelsdictated
bythewidthoftheoptimalrewardspike.Theresultsclearlydemon-
stratetheimportanceofexploringthejoint-statespace.QMIXalone
managestogetapositiverewardontheeasyscenario,butitsperfor-
manceisbothlowerandwithalargerstandarddeviationcompared
tothetwootheralgorithms.Intheharderscenarios,QMIX’sper-
Figure5:TrainingcurvesofthethreevariantsofQMIXinthe
formancedegradesstrongly,neverfindinganypositiverewardin
cooperativeboxpushingtask,withthemeanandstandard
thehardestcase.JIMclearlyimprovestheperformance.Intheeasy
deviationacross11runseach.
scenario,QMIX+JIMconsistentlygoesfortheoptimalrewardspike.
Inthehardersettings,itstillperformswellonaverage,eveninthe
"veryhard"scenariowheretheoptimalrewardspikecoversonly
showsascreenshotofthisscenario.Atthestartofeachepisode,the
0.013%ofallcombinedpositions.TheresultsofQMIX+LIMshow
landmarkisrandomlyplacedinanyoneofthefourcorners.The
thatexploringthelocal-observationspacehelpsagentsfindthe
initialpositionsoftheagentsandtheobjectarerandomlyset.If
optimalrewardspikemoreoften.However,itperformsworsethan
theagentsmanagetopushtheobjectandplaceitonthelandmark,
JIMasitdoesnotensurethatallcombinedpositionsaresufficiently
theepisodeendsandtheyreceivearewardof+100.Agentsalsoget
explored.Thisshowsthatexploringthejoint-observationspaceis
asmallpenaltyof-1ateachtimesteptorewardfasterstrategies.
crucialtoallowagentstodiscoveroptimalcoordinatedbehaviors.
Thisrewardispurposefullydefinedtobeverysparse,inorderto
studyhowexplorationhelpsinthiskindofsituation.
6.2 Coordinationtasksinacontinuous
The second scenario is a cooperative placement task where
environment
agentsmustpositionthemselvesoverlandmarksinordertomaxi-
6.2.1 Environmentdefinitionandsetups. Next,westudyhowJIM mizetheirreward.AsshowninFigure4b,therearetwosetsofthree
scalestomorerealisticcontinuousenvironmentsandmorecomplex coloredlandmarks.Therewardgivenateachtimestepdepends
tasks.Weusethemulti-agentparticleenvironment2(MPE)[11,15] on the placement of the agents on the landmarks. The optimal
tosimulatecooperativerobotictasksthatrequireahighdegreeof stateishavingbothagentsplacedontheredlandmarks,yieldinga
coordination.ThestatespaceofMPEiscontinuous:inoursetups, rewardof+10ateachtimestep.Theblueandyellowlandmarks
agentsreceiveasobservationavectorwiththeirpositioninthetwo- actasdeceivingrewards,yieldingmuchsmallerrewards(+2for
dimensionalspaceand,foralltheotherentitiesintheenvironment, blue,+1foryellow).Toincreasethedeceivingaspectoftheblue
theirrelativepositionandvelocity.Agentsnavigateinaclosed andyellowlandmarks,wealsorewardagentscollectivelyby+0.5if
two-by-two-meterareabychoosingbetweenfivediscreteactions: onlyoneofthemstandsononeofthesetwocolors.Thisleadstoa
moveinanyfourcardinaldirectionsorstayinplace. needforcoordinationbetweenagents,astheywilllocallyfindthat
Thefirsttaskisacooperativebox-pushingtaskthatrequires goingonblueoryellowlandmarkssystematicallyleadstoasmall
agentstopushanobjectandplaceitontopofalandmark.Figure4a reward.Onlyifagentsexploretheirenvironmentwell,willthey
discoverthattheyneedtobebothonredtogettheoptimalreward
2https://github.com/openai/multiagent-particle-envs signal.Importantly,thisscenariofeaturespartialobservability,withJointIntrinsicMotivationforCoordinatedExplorationinMADRL
(a) (b) (a) (b)
Figure6:PerformanceofthethreevariantsofQMIXintheco- Figure7:AblationstudyofJIMinthecoordinatedplacement
ordinatedplacementtask.(a)showsthetrainingcurveswith task.(a)showsthetrainingcurveswiththemeanandstan-
themeanandstandarddeviationacross11independentruns darddeviationacross11independentrunseach,while(b)
each,while(b)displaystheperformanceofeachindepen- displaystheperformanceofeachindependentrunatthelast
dentrunatthelastiterationoftraining.Dashedlineson(b) iterationoftraining.Thetwoablatedversionsonlyfeature
indicatetheoptimal(unattainable)levelofreturnobtained oneofthetwoexplorationcriteriadefinedinSection4.2:
withdifferentstrategies:red,blue,andyellowlinesrepresent JIM-EECfor𝑁 𝐸𝐸𝐶 andJIM-LLECfor𝑁 𝐿𝐿𝐸𝐶.Theresultsshow
thereturnobtainedifbothagentsareonlandmarksofthe theimportanceofcombiningthetwocriteria.
relatedcolorduring100steps(thedurationofanepisode),
andthegreylinesimilarlydescribesonlyoneagentbeing
eitheronblueoryellow.
6bshowsthatLIMarguablyperformsworse.Tworunsmanage
tofindtheoptimalstrategy,butLIMoftenperformspoorlywith
agentsonlyhavinginformationaboutentitiesclosetothem(less
onlyoneagentonablueoryellowlandmark.Thisdemonstrates
thansixtycentimetersfromthem),theothersbeingmaskedwith
thatexploringthespaceoflocalobservationscanbehelpful,asit
neutralvalues.Thismeansthatagentsdonotnecessarilyseewhich
pushesagentstoexploretheenvironment.However,exploringlocal
landmarktheotheragentgoesto3.
observationscanalsobemisleadingastheydonotcontainallthe
6.2.2 Results. Resultsoftraininginthecooperativeboxpushing informationaboutthecurrentstateoftheenvironment.WithJIM,
scenarioareshowninFigure5(medianandconfidenceinterval exploringthejoint-observationspaceclearlyimprovesthequality
shownfor11runseach).First,weobservethatQMIXaloneper- ofthechosenstrategies.Morethanhalfofthetime,QMIX+JIM
formsverypoorlyasitisunabletofindthesolutiontothetask. findstheoptimalrewardsignalandlearnsaneffectivestrategyto
Thehighsparsityoftherewardfunctionmakesitimpossiblefor goonredlandmarks,showingthatJIMallowsformoreefficient
agentstodiscovertheobjectivewithrandomexplorationofthe explorationofcoordinatedbehaviors.Whenagentsdonotfindthe
environment.Second,weseethatJIMandLIMachievesimilarlev- optimalstrategy,theystickwiththebestsub-optimalstrategyto
elsofperformance.Whilecoordinationcanhelpagentsperform gobothonblue.Thisshowsthatagentsbenefitfromexploringthe
well,itisactuallynotarequirementforthistask.Infact,oneagent spaceofjointobservationsastheyaredirectlylinkedtotheob-
aloneisabletopushtheobjectandplaceitonthelandmark.Thus, tainedreward,whereaslocalobservationslackcrucialinformation
exploringthespaceofjointconfigurationsisnothelpfulinthissce- tounderstandtheglobalreward.
nario.Thisshowshoweverthatactivelyexploringtheenvironment AnotheradvantageofJIMisthesimplicityofitsarchitecture.
iscrucialintaskswheretherewardfunctionisverysparse. While LIM and similar approaches in previous works [4, 8, 29]
Conversely,experimentsinthecoordinatedplacementtaskde- requirecomputingoneintrinsicrewardforeachagent,JIMonly
monstratewelltheimportanceofexploringjointly.Figure6ashows computesoneintrinsicrewardforthewholegroupofagents.This
thetrainingcurvesofQMIX,QMIX+LIM,andQMIX+JIM,with11 makesJIMsignificantlymoreefficienttorun,withLIMbeingap-
independentrunseach.Figure6bshowstheperformanceofeach proximately 24% slower than JIM to train (see Appendix D for
runatthelastiterationoftraining.Thecoloreddashedlinesgive details).
aninsightintothelevelofstrategylearnedbyeachrun.These
levelsofstrategycanbevisualizedwithexampletrajectoriesdis- 6.3 Ablationstudy
playedinFigure8.QMIXalonealmostalwaysgoesfortheblue
Inthisablationstudy,JIMiscomparedwithtwoablatedversions
landmarks,whilesometimessettlingfortheyellowones.Thisindi- ofthereward:onewithonlytheepisodicexplorationcriterion𝑁
𝐸𝐸𝐶
catesthatwithoutactivelyexploringtheenvironment,QMIXgets
(JIM-EEC) and one with only the life-long exploration criterion
stuckbecauseofdeceptiverewardsandisunabletofindtheoptimal 𝑁 𝐿𝐿𝐸𝐶 (JIM-LLEC).NotethatJIM-LLECisactuallyequivalentto
strategy.WhileQMIX+LIMseemsslightlybetterthanQMIXonthe
NovelD[35]inthisenvironmentastheepisodicrestrictionofNov-
trainingcurves,theindividualrunperformanceshowninFigure
elD(seeSection3.2)wouldbeineffectiveinacontinuousenviron-
3SeeAppendixCforadetaileddescriptionofthesetwotasks. mentsuchasMPE.Tocomparethesethreeversionsproperly,weMaximeToquebiau,NicolasBredeche,FaïzBenamar,andJae-YunJun
(a) (b) (c) (d)
Figure8:Examplesoftrajectoriesinthecoordinatedplace-
menttask.Theypresentdifferentlevelsofstrategy,with(a)>
(b)>(c)>(d):(a)optimalstrategywithbothagentsonred,(b)
bothonblue,(c)bothonyellow,and(d)oneonblue/yellow.
scaletheintrinsicrewardsofthetwoablatedmodelstobeatasim-
ilarmagnitudeastheintrinsicrewardgeneratedbyJIM.Figure7
showstheresultsoftrainingtheseversionsinthecoordinatedplace-
menttask,with11independentrunseach.Bothablatedalgorithms
performsignificantlyworsethanJIM.First,theepisodicbonusof
JIM-EECalonelacksthemotivationfordiscoveringunseenconfig-
urations.Thus,itexploreslessandisnotabletofindtheoptimal
solutiontothetask.Meanwhile,withouttheepisodicrestriction,
JIM-LLECdevelopslessefficientexplorationstrategies.Thiscon-
firmsthat,asshowninarecentstudy[1],theepisodicrestriction
implementedinNovelDandotherintrinsicrewards[2,20]iscrucial Figure9:TrainingperformanceofQMIX,QMIX+LIMand
fordevelopingefficientexplorationstrategies.Overall,thisproves QMIX+JIM in the four-agent version of rel_overgen. Top
theimportanceofcombiningthetwostagesofexplorationdefined graphshowsthemeanandstandarddeviationacross11runs
in𝑁 𝐿𝐿𝐸𝐶 and𝑁 𝐸𝐸𝐶. each,bottomgraphdisplaysallsingleruns(QMIXindotted
linesforclarity,QMIX+LIMisomittedasitnevergoesbeyond
6.4 Scalinguptomoreagents thesuboptimalstrategy).
Finally, we evaluate JIM in a scenario with four agents using a
modifiedversionoftherel_overgenenvironmentintroducedin
Section6.1(seeAppendixEformoredetails).Asinthetwo-agent theoptimalsolution,advocatingforthebenefitsofusingaglobal,
version,therewardfunctionhasahighrewardspikeinonecorner ratherthanlocal,intrinsicrewardforexploration.
ofthespaceandalowrewardplateauintheothercorner,making
7 CONCLUSION
relativeovergeneralizationpronetoarise.Withmoreagents,the
numberofdimensionsofthejointobservationincreaseslinearly Inthispaper,wepresentanalgorithmforjointintrinsicmotivation
(inthiscase:from80dimensionswithtwoagentsto160withfour (JIM),whichisthefirstmethodtorewardactiveexplorationof
agents) while the number of possible states increases exponen- thejoint-observationspace.Itcanbeintegratedtoenhanceany
tially,makingthesearchfortheoptimalstrategysignificantlymore Multi-Agent Deep Reinforcement Learning algorithm that uses
challenging. centralizedtrainingwithdecentralizedexecution.Bycombining
Figure9showstheresultsinthisscenariofor11independent JIM with the state-of-the-art QMIX algorithm, we demonstrate
runseach.Aswithpreviousexperiments,JIMimprovestheperfor- thatitoutperformstheoriginalQMIXimplementationaswellasa
manceofQMIXbyupgradingitsexplorationcapabilities.Takinga modifiedQMIXalgorithmusingsingle-agentintrinsicrewards.We
closerlookattheresultsrevealsthatQMIXandQMIX+LIMfindthe showthatactiveexplorationisakeycomponentformulti-agent
optimalsolutioninrespectively2and0runsoutofthe11,while learning in environments with sparse rewards. Moreover, joint
QMIX+JIMfindstheoptimalsolutionin8outof11runsinthe explorationenablesthediscoveryofoptimalcoordinatedbehaviors
allocatedtime(cf.Figure9-bottom).Thetwopositiveresultsfor thatwouldbehardtofindotherwiseastheynecessitateahighlevel
QMIXcanbeattributedtobeneficialinitialconditionsasQMIX ofcoordinationbetweenagents.
neverreachestheoptimalperformanceotherwise.Thisisnotthe Thisshowstheimportanceofusingjointobservationsinthe
casewithQMIX+JIM,whichshowsrobustnesstoinitialconditions processofcomputingintrinsicrewardsforamulti-agentsystem.
thankstoactiveexplorationoftheenvironment.Infact,theimpact Infact,thejointobservationisthebestestimateoftheglobalstate
ofJIMbecomesevidentwhenlookingatcurvesfromindividual oftheenvironmentavailablefortheagents.Usingitallowsmore
runs,whereweconsistentlyobservesignificantperformanceen- efficientlearningofmulti-agentjointbehaviorsandiscomputation-
hancementssubsequenttoaminorinitialdropinefficiency.This allylessexpensivethanhavingtocomputelocalintrinsicrewards
phenomenonreflectsadeliberateshifttowardsexploringnovel foreachagent.Theseresultsshouldencourageresearchonhow
approacheswhenthesystemwouldotherwiseremainstagnant. jointobservationscanbeusedinotherkindsofintrinsicrewards
This is unique to JIM, as QMIX+LIM never succeeds in finding toshapetheagents’behaviorfurther.JointIntrinsicMotivationforCoordinatedExplorationinMADRL
REFERENCES
MichaelPetrov,HenriqueP.d.O.Pinto,JonathanRaiman,TimSalimans,Jeremy
[1] AlainAndres,EstherVillar-Rodriguez,andJavierDelSer.2022.AnEvaluation Schlatter,JonasSchneider,SzymonSidor,IlyaSutskever,JieTang,FilipWolski,
StudyofIntrinsicMotivationTechniquesappliedtoReinforcementLearning andSusanZhang.2019.Dota2withLargeScaleDeepReinforcementLearning.
overHardExplorationEnvironments.InarXiv:2205.11184. InarXiv:1912.06680.
[2] AdriàPuigdomènechBadia,PabloSprechmann,AlexVitvitskyi,DanielGuo, [18] Pierre-YvesOudeyerandFredericKaplan.2007.WhatisIntrinsicMotivation?A
BilalPiot,StevenKapturowski,OlivierTieleman,MartinArjovsky,Alexander TypologyofComputationalApproaches.InFrontiersinneurorobotics,Vol.1.6.
Pritzel,AndrewBolt,andCharlesBlundell.2020. NeverGiveUp:Learning https://doi.org/10.3389/neuro.12.006.2007
DirectedExplorationStrategies.In8thInternationalConferenceonLearning [19] DeepakPathak,PulkitAgrawal,AlexeiA.Efros,andTrevorDarrell.2017.
Representations. https://openreview.net/forum?id=Sye57xStvB Curiosity-drivenExplorationbySelf-supervisedPrediction.InProceedingsofthe
[3] YuriBurda,HarrisonEdwards,AmosStorkey,andOlegKlimov.2019.Exploration 34thInternationalConferenceonMachineLearning,Vol.PMLR70.2778–2787.
byRandomNetworkDistillation.In7thInternationalConferenceonLearning [20] RobertaRaileanuandTimRocktäschel.2020.RIDE:RewardingImpact-DrivenEx-
Representations. plorationforProcedurally-GeneratedEnvironments.In9thInternationalConfer-
[4] Yali Du, Lei Han, Meng Fang, Tianhong Dai, Ji Liu, and Dacheng enceonLearningRepresentations. https://openreview.net/forum?id=rkg-TJBFPB
[21] Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. 2020.
Tao. 2019. LIIR: Learning Individual Intrinsic Reward in Multi-Agent
WeightedQMIX:ExpandingMonotonicValueFunctionFactorisationforDeep
Reinforcement Learning. In Advances in Neural Information Process-
Multi-AgentReinforcementLearning.InAdvancesinNeuralInformationProcess-
ing Systems, Vol. 32. https://proceedings.neurips.cc/paper/2019/hash/
ingSystems33.10199–10210. https://proceedings.neurips.cc/paper/2020/hash/
07a9d3fed4c5ea6b17e80258dee231fa-Abstract.html
73a427badebe0e32caa2e1fc7530b7f3-Abstract.html
[5] YannisFlet-Berliac,JohanFerret,OlivierPietquin,PhilippePreux,andMatthieu
[22] TabishRashid,MikayelSamvelyan,ChristianSchroeder,GregoryFarquhar,Jakob
Geist.2021.AdversariallyGuidedActor-Critic.In9thInternationalConference
Foerster,andShimonWhiteson.2018.QMIX:MonotonicValueFunctionFac-
onLearningRepresentations. https://hal.inria.fr/hal-03167169
torisationforDeepMulti-AgentReinforcementLearning.InProceedingsofthe
[6] JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,and
35thInternationalConferenceonMachineLearning,Vol.PMLR80.4295–4304.
ShimonWhiteson.2018. CounterfactualMulti-AgentPolicyGradients.In
http://proceedings.mlr.press/v80/rashid18a.html
ProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.32. https:
[23] TomSchaul,JohnQuan,IoannisAntonoglou,andDavidSilver.2016.Prioritized
//ojs.aaai.org/index.php/AAAI/article/view/11794
ExperienceReplay.In4thInternationalConferenceonLearningRepresentations.
[7] MikaelHenaff,RobertaRaileanu,MinqiJiang,andTimRocktäschel.2022.Ex-
[24] JürgenSchmidhuber.1991.APossibilityforImplementingCuriosityandBore-
plorationviaEllipticalEpisodicBonuses.InAdvancesinNeuralInformation
dominModel-BuildingNeuralControllers.InProceedingsoftheInternational
ProcessingSystems,Vol.35.37631–37646. https://openreview.net/forum?id=Xg-
ConferenceonSimulationofAdaptiveBehavior:FromAnimalstoAnimats.222–
yZos9qJQ
227.
[8] ShariqIqbalandFeiSha.2019.CoordinatedExplorationviaIntrinsicRewardsfor
[25] ShaiShalev-Shwartz,ShakedShammah,andAmnonShashua.2016.Safe,Multi-
Multi-AgentReinforcementLearning.InarXiv:1905.12127. https://openreview.
Agent,ReinforcementLearningforAutonomousDriving.InarXiv:1610.03295.
net/forum?id=rkltE0VKwH
[26] KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYung
[9] NatashaJaques,AngelikiLazaridou,EdwardHughes,CaglarGulcehre,PedroA.
Yi.2019.QTRAN:LearningtoFactorizewithTransformationforCooperative
Ortega,DJStrouse,JoelZ.Leibo,andNandodeFreitas.2019.SocialInfluenceas
Multi-AgentReinforcementLearning.InProceedingsofthe36thInternational
IntrinsicMotivationforMulti-AgentDeepReinforcementLearning.InProceed-
ConferenceonMachineLearning,Vol.PMLR97.5887–5896.
ingsofthe36thInternationalConferenceonMachineLearning,Vol.97.3040–3049.
[27] PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,Vini-
[10] JoelLehmanandKennethO.Stanley.2011.Abandoningobjectives:Evolution
ciusZambaldi,MaxJaderberg,MarcLanctot,NicolasSonnerat,JoelZ.Leibo,
throughthesearchfornoveltyalone.InEvolutionaryComputation,Vol.19.189–
KarlTuyls,andThoreGraepel.2018.Value-DecompositionNetworksForCo-
223.
operativeMulti-AgentLearningBasedOnTeamReward.InProceedingsofthe
[11] RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch.2017.
17thInternationalConferenceonAutonomousAgentsandMultiAgentSystems.
Multi-AgentActor-CriticforMixedCooperative-CompetitiveEnvironments.In
2085–2087.
AdvancesinNeuralInformationProcessingSystems,Vol.30.
[28] MingTan.1993. Multi-AgentReinforcementLearning:Independentversus
[12] AndreiLupu,BrandonCui,HengyuanHu,andJakobFoerster.2021.Trajectory
CooperativeAgents.InProceedingsoftheTenthInternationalConferenceon
DiversityforZero-ShotCoordination.InProceedingsofthe38thInternational
MachineLearning.330–337.
ConferenceonMachineLearning,Vol.139.7204–7213. https://proceedings.mlr.
[29] TonghanWang,JianhaoWang,YiWu,andChongjieZhang.2020. Influence-
press/v139/lupu21a.html
BasedMulti-AgentExploration.In8thInternationalConferenceonLearning
[13] ZixianMa,RoseEWang,LiFei-Fei,MichaelS.Bernstein,andRanjayKrishna.
Representations. https://openreview.net/forum?id=BJgy96EYvr
2022. ELIGN:ExpectationAlignmentasaMulti-AgentIntrinsicReward.In
[30] ErmoWeiandSeanLuke.2016. LenientLearninginIndependent-Learner
AdvancesinNeuralInformationProcessingSystems,Vol.35.8304–8317. https:
StochasticCooperativeGames.InTheJournalofMachineLearningResearch,
//openreview.net/forum?id=uPyNR2yPoe
Vol.17.2914–2955.
[14] AnujMahajan,TabishRashid,MikayelSamvelyan,andShimonWhiteson.2019.
[31] ErmoWei,DrewWicke,DavidFreelan,andSeanLuke.2018.MultiagentSoft
MAVEN:Multi-AgentVariationalExploration.InAdvancesinNeuralInforma-
Q-Learning.InarXiv:1804.09817.
tionProcessingSystems,Vol.32. https://proceedings.neurips.cc/paper/2019/file/
[32] RudolfPaulWiegand.2003.AnAnalysisofCooperativeCoevolutionaryAlgorithms.
f816dc0acface7498e10496222e9db10-Paper.pdf
Ph.D.Dissertation.GeorgeMasonUniversity.
[15] IgorMordatchandPieterAbbeel.2018.EmergenceofGroundedCompositional
[33] MichaelWooldridge.2009.Anintroductiontomultiagentsystems.Johnwiley&
LanguageinMulti-AgentPopulations.InProceedingsoftheAAAIConference
sons.
onArtificialIntelligence. https://www.aaai.org/ocs/index.php/AAAI/AAAI18/
[34] ChaoYu,AkashVelu,EugeneVinitsky,YuWang,AlexandreBayen,andYiWu.
paper/view/17007
2021.TheSurprisingEffectivenessofPPOinCooperative,Multi-AgentGames.
[16] FransA.OliehoekandChristopherAmato.2016. AConciseIntroductionto
InarXiv:2103.01955.
DecentralizedPOMDPs. Springer. https://www.ccis.northeastern.edu/home/
[35] TianjunZhang,HuazheXu,XiaolongWang,YiWu,KurtKeutzer,JosephE
camato/publications/OliehoekAmato16book.pdf
Gonzalez, and Yuandong Tian. 2021. NovelD: A Simple yet Effective
[17] OpenAI,ChristopherBerner,GregBrockman,BrookeChan,VickiCheung,Prze-
Exploration Criterion. In Advances in Neural Information Processing Sys-
mysławDębiak,ChristyDennison,DavidFarhi,QuirinFischer,ShariqHashme,
tems,Vol.34.25217–25230. https://proceedings.neurips.cc/paper/2021/file/
ChrisHesse,RafalJózefowicz,ScottGray,CatherineOlsson,JakubPachocki,
d428d070622e0f4363fceae11f4a3576-Paper.pdfMaximeToquebiau,NicolasBredeche,FaïzBenamar,andJae-YunJun
APPENDICES
A LOCALINTRINSICMOTIVATION
Figure10:Architectureforlocalintrinsicmotivation(LIM).Eachagenthasitsownmoduleforcomputinganintrinsicreward
basedonitslocalobservation.
B HYPERPARAMETERS
Table2:Hyperparametersusedinthecooperativebox
Table1:Hyperparametersusedintherel_overgenenvironment.
pushingscenario.
Algorithm
Hyperparameter Algorithm
JIM JIM4agents LIM Hyperparameter
JIM LIM
Intrinsicrewardweight𝛽 1
Intrinsicrewardweight𝛽 1 1
HEn i Sdc cdo aed ln ii nn d gg im fd ai cm 𝐷 toℎ𝐷 r𝑖𝑑𝜙 𝛼𝑑/ 𝑒𝜓 𝑛 16 24 8 0.526 5 ,4 6 0.6 3 62 4 HEn idc do ed nin dg imdim 𝐷 ℎ𝐷 𝑖𝑑𝜙 𝑑/ 𝑒𝜓 𝑛 16 24 8 3 62 4
Scalingfactor𝛼 0.5
Intrinsicrewardlearningrate𝛼
𝑖𝑛𝑡
0.0001 0.0001,0.0002 0.0001
Intrinsicrewardlearningrate𝛼
𝑖𝑛𝑡
0.0001
Table3:Hyperparametersusedinthecoordinatedplacementscenario.
Algorithm
Hyperparameter
JIM LIM JIM-LLEC JIM-EEC
Intrinsicrewardweight𝛽 1,2,4 1,4,8 1,3 0.1,1
Encodingdim𝐷 𝜙/𝜓 64 36 64 64
Hiddendim𝐷 ℎ𝑖𝑑𝑑𝑒𝑛 512 256 512 512
Scalingfactor𝛼 0.5
Intrinsicrewardlearningrate𝛼
𝑖𝑛𝑡
0.0001
Tables1to3presentthevalueschosenforhyperparameters.Weonlylisthyperparametersspecifictotheintrinsicrewardmodule,
asforQMIXweusethedefaulthyperparametersdescribedintheoriginalpaper[22].Whenweperformedsomesearchoveraspecific
hyperparameter,weputthelistofallthevalueswetriedandputthebestoneinbold.Here,wedetailthedifferentparameterspresented:
• Intrinsicrewardweight𝛽:weightoftheintrinsicreward𝑟 𝑖againsttheextrinsicreward𝑟 𝑒intherewardgiventoagents:𝑟 𝑡 =𝑟 𝑡𝑒+𝛽𝑟 𝑡𝑖𝑛𝑡 .
• Encodingdimension𝐷 𝜙/𝜓:dimensionoftheoutputoftheembeddingnetworks𝜙 and𝜓 usedin𝑁 𝐿𝐿𝐸𝐶 and𝑁 𝐸𝐸𝐶 respectively(see
Section3).JointIntrinsicMotivationforCoordinatedExplorationinMADRL
• Hiddendimension𝐷 ℎ𝑖𝑑𝑑𝑒𝑛:dimensionofthehiddenlayersintheembeddingnetwork𝜙and𝜓 usedin𝑁 𝐿𝐿𝐸𝐶 and𝑁 𝐸𝐸𝐶 respectively.
• Scalingfactor𝛼:parameterusedinthedefinitionof𝑁 𝐿𝐿𝐸𝐶 (seeEq.(6)).Itcontrolshowmuchnoveltygainwewantagentstofind
betweeneachstep.
• Intrinsicrewardlearningrate𝛼 𝑖𝑛𝑡:learningrateusedfortrainingtheintrinsicrewardmodule.
C DETAILSONCUSTOMTASKSINTHEMULTI-AGENTPARTICLEENVIRONMENT
C.1 Cooperativeboxpushing
Figure11:Cooperativeboxpushingtask,agentsarethesmallgreycircles,thegreencircleinthemiddleisanobjecttodeliver
tothelandmarkinthebottomrightcorner.
Thecooperativeboxpushingtaskrequirespushingaroundobjectandplacingitontopofalandmark.Theenvironmentisa2x2meter
areawithwallsonthesidesthatblockentitiesfrommovingaway.Ateachtimestep,anagentgetsasobservation:
• itspersonaldata:itspositionandvelocitypos self,𝑥,pos self,𝑦,vel self,𝑥,vel self,𝑦,
• dataabouttheotheragent:abooleanindicatingiftheotheragentisvisibleornot,therelativeposition,andthevelocityofthis
agentis_visible agent,distagent,𝑥,distagent,𝑦,velagent,𝑥,velagent,𝑦,
• dataabouttheobject:abooleanindicatingiftheobjectisvisibleornot, therelativeposition,andthevelocityofthis object:
is_visible object,dist object,𝑥,dist object,𝑦,vel object,𝑥,vel object,𝑦,
• anddataaboutthelandmark:abooleanindicatingifthelandmarkisvisibleornotandthenumberofthecorneritislocatedinto
(from1to4):is_visible ,corner .
landmark landmark
Thus,theobservationisavectorofdimension16containingthisinformation.Relativepositionsofotherentities(agentorobject)are
actuallythedistancetotheagent,normalizedbytheirrangeofobservation,i.e.,
distagent,𝑥 =
pos agent,𝑥 −pos self,𝑥
.
obs_range
Theenvironmentcanbeeitherfullyobservableorpartiallyobservable.Inthelattercase,agentshavearangeofobservationof60centimeters
aroundthem.Whenagentsorobjectsareoutsidethisrange,theirrelativepositionismaskedwithonesandtheirvelocitywithzeros.When
thelandmarkisoutsidetherangeofobservation,thecornernumberismaskedwithzero.Inthefullyobservablecase,theobservationrange
issetto2.83meters,i.e.,thelargestdistancepossibletohaveinthis2x2meterarea.
Therewardfunctionisverysparse.Ateachtimestep,agentsreceiveapenaltyof0.1,plusapenaltyof2ifthereisacollisionbetween
agents.Ifthetaskiscompleted,i.e.,thecenteroftheobjectisplacedintheareaofthelandmark,theagentsgetarewardof100andthe
episodeends.
Atthestartofeachepisode,thepositionsofallentitiesintheenvironmentarerandomlyset:theagentsandtheobjectarerandomly
placedinsidetheenvironment,andthelandmarkisplacedinoneofthefourcornersofthemap.MaximeToquebiau,NicolasBredeche,FaïzBenamar,andJae-YunJun
C.2 Coordinatedplacement
Figure12:Coordinatedplacementtask,agentsarethesmallgreycircles,thecoloredcirclesrepresentlandmarkstheagents
havetonavigateontogainrewards.
Thecoordinatedplacementtaskrequiresnavigatingontopoflandmarksandchoosingtherightlandmarkcolorsinordertomaximizethe
obtainedreward.Theenvironmentisa2x2meterareawithwallsonthesidesthatblockentitiesfrommovingaway.Ateachtimestep,an
agentgetsasobservation:
• itspersonaldata:itspositionandvelocitypos self,𝑥,pos self,𝑦,vel self,𝑥,vel self,𝑦,
• dataabouttheotheragent:abooleanindicatingiftheotheragentisvisibleornot,therelativeposition,andthevelocityofthis
agentis_visible agent,distagent,𝑥,distagent,𝑦,velagent,𝑥,velagent,𝑦,,
• foreachlandmarkintheenvironment:abooleanindicatingifthelandmarkisvisibleornot,therelativepositionofthislandmark,
anditscolorasaone-hotencoding:is_visible landmark,dist landmark,𝑥,dist landmark,𝑦,is_red,is_blue,is_yellow.
Thus,theobservationisavectorofdimension43containingthisinformation.Relativepositionsofotherentities(agentorlandmarks)are
actuallythedistancetotheagent,normalizedbytheirrangeofobservation,i.e.,
distagent,𝑥 =
pos agent,𝑥 −pos self,𝑥
.
obs_range
Thisscenarioispartiallyobservable.Agentshavearangeofobservationof60centimetersaroundthem.Whenagentsorobjectsareoutside
thisrange,theirrelativepositionismaskedwithonesandtheirvelocitywithzeros.Forlandmarksoutsideoftheobservationrange,the
colorismaskedwithzeros.
Therewardgivenateachtimestepdependsonlyonwhichlandmarkhasanagentplacedontop:
• iftwoagentsareplacedontopofredlandmarks,𝑟 𝑡𝑒 =10,
• iftwoagentsareplacedontopofbluelandmarks,𝑟 𝑡𝑒 =2,
• iftwoagentsareplacedontopofyellowlandmarks,𝑟 𝑡𝑒 =1,
• ifonlyoneagentisplacedontopofeitherablueoryellowlandmark,𝑟 𝑡𝑒 =0.5,
• else,𝑟 𝑡𝑒 =0.
Atthestartofeachepisode,agentsareplacedrandomlyonthehorizontallineinthemiddleofthemap,i.e.,pos=(𝑥 =uniform(−1,1),𝑦=
0).Thelandmarksarealwaysinthesamepositions,withthecolorsinthesameorder,asdisplayedinFigure12.JointIntrinsicMotivationforCoordinatedExplorationinMADRL
D EXECUTIONTIMES
Figure13:TrainingcurvesofQMIX,QMIX+LIM,andQMIX+JIMinthecoordinatedplacementtaskwithexecutiontimeonthe
x-axis.QMIXtakesonaverage6.5hourstotrainduring10millionsteps,whileQMIX+JIMtakes8.5hoursandQMIX+LIM10.5
hours.
E N-AGENTrel_overgenENVIRONMENT
Tostudytheproblemofrelativeovergeneralizationwithmorethantwoagents,weextendtherel_overgenenvironmenttoaccept𝑁 agents.
Todoso,wemodifytherewarddefinitiongiveninthepaper(seeEq.(9)):
(cid:32) 𝑁 𝑁 (cid:33)
𝑟 𝑡ext(p;𝛿)=max 𝑅+− 𝐷𝛿 ∑︁ (𝑝
𝑖
−𝑟 𝑖+)2,𝑅−− 81
𝐷
∑︁ (𝑝
𝑖
−𝑟 𝑖−)2 ,
𝑖=0 𝑖=0
withp={𝑝 𝑖} 0<𝑖≤𝑁 thepositionsoftheagents,𝛿thecoefficientcontrollingthesizeoftheoptimalrewardspike,𝐷thedimensionofeach
agent’sstate,𝑅+themaximumvalueoftheoptimalrewardspikeplacedatpositionr+ = {𝑟 𝑖+} 0<𝑖≤𝑁 and𝑅− themaximumvalueofthe
suboptimalplateauplacedatpositionr− ={𝑟 𝑖−} 0<𝑖≤𝑁.Thisformulayieldsthesameresultsasthetwo-dimensionalexamplesshowninthe
paperbutinaN-dimensionalspace.
Addingagentsincreasesthecomplexityofthetaskexponentially.Tocompensateforthis,wehavetomaketheoptimalrewardspike
largerforthetasktobesolvablebyQMIX.Inthe4-agentexperiments,weuse𝛿 =0.9.
Finally,withfouragentswehadtolowertheinitialvalueofthe𝜖parameterofQMIXforits𝜖-greedystrategy.Wefoundthatincreasing
thenumberofagentsledtobadresultswiththedefault0.3initialvalueof𝜖.Withthishyperparametersetto0.1,theresultsweresignificantly
better.Thisislikelyduetothefactthatagentschooseseparatelyiftheyexploreorexploit(atleastthatisthecaseinourimplementation),
meaningthatincreasingthenumberofagentsleadstohavingmorerandomnessintheselectionofeachjointaction.