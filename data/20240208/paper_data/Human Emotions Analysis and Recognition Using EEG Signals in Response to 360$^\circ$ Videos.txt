Human Emotions Analysis and Recognition Using EEG Signals in
Response to 360° Videos
Haseeb ur Rahman Abbasi1, Zeeshan Rashid 1, Muhammad Majid 1, and Syed Muhammad Anwar2
Abstract—Emotion recognition (ER) technology is an in- thedetectionofdepressiondisorders[4],andtheexploration
tegral part for developing innovative applications such as of schizophrenia [5] to name a few.
drowsinessdetectionandhealthmonitoringthatplaysapivotal
Virtual reality (VR) is a technology that uses computer-
role in contemporary society. This study delves into ER using
generatedsimulatedenvironments,thatgiveuserstheillusion
electroencephalography(EEG),withinimmersivevirtualreality
(VR) environments. There are four main stages in our pro- of real physical exposure. VR is widely used in different
posed methodology including data acquisition, pre-processing, fields like education, medicine, entertainment, defense, mar-
feature extraction, and emotion classification. Acknowledg- keting, real estate, and many more [6]. Environments that
ing the limitations of existing 2D datasets, we introduce a
are difficult to realize can be simulated using VR. The
groundbreaking 3D VR dataset to elevate the precision of
2021 launch of Horizon Worlds by Meta Platforms has
emotion elicitation. Leveraging the Interaxon Muse headband
for EEG recording and Oculus Quest 2 for VR stimuli, we igniteddebatesonthesocietalimplicationsoftheMetaverse,
meticulously recorded data from 40 participants, prioritizing described as "the layer between you and reality." Meta
subjects without reported mental illnesses. Pre-processing en- envisions seamlessly integrating avatars and holograms in
tailsrigorouscleaning,uniformtruncation,andtheapplication
a 3D virtual shared world for work and social interactions
of a Savitzky-Golay filter to the EEG data. Feature extraction
[7]. Although VR technology is making progress rapidly,
encompassesacomprehensiveanalysisofmetricssuchaspower
spectraldensity,correlation,rationalanddivisionalasymmetry, it has not yet reached the peak of its development, and its
and power spectrum. To ensure the robustness of our model, limitations cannot be reliably predicted for now.
we employed a 10-fold cross-validation, revealing an average At present, most emotion recognition systems are data-
validation accuracy of 85.54%, with a noteworthy maximum
dependent. Many datasets have been proposed in the litera-
accuracy of 90.20% in the best fold. Subsequently, the trained
ture for research. Prominent publicly accessible datasets for
model demonstrated a commendable test accuracy of 82.03%,
promising favorable outcomes. emotionrecognition,suchasDEAP[8],DREAMER[9],and
ASCERTAIN [10], incorporate EEG signals in conjunction
I. INTRODUCTION with other physiological data. These datasets used videos,
Emotion recognition, a technology with diverse applica- music, and images to elicit different emotions. The main
tions, is currently employed in driving drowsiness detec- limitation of these datasets is that all these use stimuli that
tion, workload evaluation, and health monitoring, signifi- are 2D, non-immersive, and hence they lack the feel of
cantly impacting society. It involves both emotion elicitation presence for users when they interact with them. So, a 3D
and classification. Emotion recognition not only portrays virtual environment is required for more effective emotion
behavioral and mental states but also strengthens human- elicitation.
computer interaction. Positive emotions indicate a healthy Studieshaveshownthatemotionselicitedusingimmersive
state, contrasting with negative emotions which could even virtualenvironmentsarebetterthanthoseinducedusingnon-
be linked to conditions such as depression and an elevated immersive methods [11]. Aelee Kim et al [12] showed that
suicide risk [1]. Recent studies favor physiological signals VRgivesamoreimmersiveexperienceandgreateremotional
like electroencephalography (EEG) over visible signs (like responsetohorrorfilms.TheeffectivenessofVRasemotion
speech, and facial expressions), considering them more ac- elicitation stimuli has been proven [13-16]. In [17], a VR-
curateindicatorsofgenuineemotionsduetotheirconnection based EEG dataset named as VREED, was presented and
to the central nervous system. There has been a significant usedtorecognizedifferentemotions.Thedatasetiscurrently
interest in EEG based analysis, due to its potential to offer in the embargo phase and is not yet available publicly. In
a straightforward, cost-effective, portable, and user-friendly [18], emotion recognition based on VR was conducted to
solution for emotion identification [2]. Therefore, EEG finds classify four classes of emotions. Another study [19] was
extensiveapplicationindiversebiomedicalcontexts,playing conducted for emotion recognition in response to VR using
a crucial role in diverse tasks such as stress assessment [3], data from frontal EEG electrodes.
Duetolimitedresearchinemotionrecognitioninresponse
*Thisworkwasnotsupportedbyanyorganization to VR and the unavailability of a physiological signals
1HaseeburRahmanAbbasi,ZeeshanRashidandMuhammadMajidare
dataset, this paper acquires a new dataset for emotion anal-
withDepartmentofComputerEngineering,UniversityofEngineeringand
Technology,Taxila,Pakistan. ysis using EEG signals in response to VR environments. In
2Syed Muhammad Anwar is with with Sheikh Zayed Institute for the near future, interactions with VR technology is expected
Pediatric Surgical Innovation, Children’s National Hospital, Washington,
to increase manifold, as leading IT industries have already
DC and School of Medicine and Health Sciences, George Washington
University,Washington,DC. started developing VR-based applications. So, our work is
4202
beF
6
]CH.sc[
1v24140.2042:viXraFig. 1: The proposed method for emotion recognition in response to 360° videos
a significant step forward to minimize the gap towards and arousal ratings obtained through the Self-Assessment
human emotion analysis and VR interactions. The main Manikin (SAM) scale [19]. The videos whose ratings were
contributions of this paper are, farthestfromtheoriginwereselected.Fromthefirstquadrant
1) Wecurateanewdatasetforemotionrecognitionusing of the valence arousal scale, we selected video numbers 50,
EEG signals acquired in response to VR content i.e., 62, 69, and 52. Video numbers 33, 32, 27, and 22 were
360° videos. selectedfromthesecondquadrant.Forthethirdquadrant,we
2) We further perform emotion recognition utilizing vari- selected 15, 3, 1, and 14, and for the fourth quadrant 20, 65,
ations in EEG signals in response to 360° video using 68,and21wereselectedfrom[19].Eachquadrantrepresents
frequency domain features. happy, angry, sad, and relaxed emotions respectively. These
The remaining part of the paper is organized as follows. selected videos were then organized into four sessions, each
Section II explains the proposed methodology for this study. comprising of four videos. The design of each session was
Section III presents results and discussion and a conclusion aimed to maintain a total duration of approximately 15 min-
is drawn in section IV. utes, considering user fatigue, as studies suggest discomfort
after prolonged viewing periods [19].
II. OURPROPOSEDMETHODOLOGY
3) Experimental Procedure:ForEEGrecordings,partic-
Anoverflowworkflowfortheproposedstrategyforhuman ipants were taken to temperature-regulated room with con-
emotion analysis and recognition in response to a virtual sistentlightingconditions.Inthissetting,anoverviewofthe
reality environment (360° videos) is shown in Figure 1. The experimental process was presented, and participants were
four main steps are: 1)data acquisition, 2) pre-processing, 3) invited to sign the consent form. Furthermore, individuals
feature extraction, and 4) emotion classification. were requested to complete a demographic questionnaire to
supply details about their age, gender, and any pertinent
A. Data Acquisition
information related to mental health. Participants were in-
1)ParticipantsandApparatus:Datawererecordedfrom
formed that they had the freedom to quit the experiment
a total of 40 subjects (24 males and 16 females). The age
at any time of their choosing. The experiments for this
range was between 18 to 35 years. Subjects did not report
study are designed following the Helsinki Declaration and
any mental illness before recording. For EEG recording, we
the study was approved by the Board of Advanced Studies
employedtheInteraxonMuseheadband,aversatileanduser-
Research and Technological Development at the University
friendly EEG recording system with four channels located
of Engineering and Technology, Taxila.
at AF7, AF8, TP9, and TP10 positions. The Muse headband
B. Data Pre-Processing
records EEG data at 256 Hz and connects to a smartphone
viaBluetoothfordatatransmission.Topresentthestimulito The recorded data was transferred to a computer for pre-
subjects, a head-mounted display (HMD) from Meta named processing. Initially, missing data points from raw EEG
Oculus Quest 2 with six degrees of freedom with realistic channels were generated using window-based averaging to
precision, 1832×1920 resolution per eye, and with 90 Hz ensuredatacleanliness.Although,therecordeddatahadvery
refresh rate was used. fewofthesepoints.Subsequently,thecleaneddatafileswere
2) Stimuli Selection: Sixteen videos, four from each uniformly truncated to match the duration of each video.
quadrant were chosen from a publicly available immer- Following this, a Savitzky-Golay filter with a third-order
sive VR video database, each accompanied by valence polynomial and a window size of 11 was applied to smoothTABLE I: PERFORMANCE COMPARISON OF DIFFERENT
SVMKERNELSFORFOURCLASSCLASSIFICATION.
Kernel Average Maximum TestAccuracy
Function accuracy Accuracyon onBestFold
across10folds thebestfold
RBF 70.49% 76.92% 64.84%
Linear 72.82% 82.69% 75%
Gaussian 69.52% 75% 64.84%
Polynomial 85.54% 90.20% 82.03%
outpotentialoutliersinthedata.Furthermore,fivefrequency (a) (b)
bands:delta(0–4Hz),theta(4–7Hz),alpha(8–12Hz),beta
(12–30Hz),andgamma(30–50Hz)wereobtainedfromraw
EEG channels data.
C. Features Extraction
To analyze the recorded EEG data, distinct frequency do-
main feature groups were derived from each channel. These
included power spectral density (PSD), correlation (C), di-
visional asymmetry (DASM), rational asymmetry (RASM),
and power spectrum (PS). In particular, PSD characterizes
thepower distributionacross specificfrequency ranges. Fea-
tures from this group included the mean and variance of the
(c) (d)
PSD for each channel. This feature group comprised 8 fea-
Fig. 2: Confusion matrix on the test set for (a) RBF, (b)
tures. Correlation, a statistical measure reflecting the degree
linear, (c) Gaussian, (d) polynomial. kernels
ofvariationbetweentwovalues,wascomputedforasymmet-
ric channels of the left and right hemispheres—specifically,
(TP9, TP10) and (AF7, AF8). Two features were computed
for this feature group. DASM represented the variance in linear, Gaussian, and polynomial kernels. Using a polyno-
absolute power between asymmetric channels of the brain mial kernel we got an average cross-validation accuracy of
hemispheres, while RASM denoted the ratio of absolute 85.54% with a maximum accuracy of 90.20% on the best
power between left and right hemisphere channels. Four fold.Ontestingthebestmodelwiththetestset,weachieved
featureswereextractedfromthesefeaturegroups.Thepower a remarkable accuracy of 82.03%. Table I compares the
spectrum involved the average absolute power across four results of different kernels for four class classifications. The
scalp electrodes in the five frequency bands of the EEG sig- 2nd-degree polynomial kernel outperforms all other kernels.
nal. 20 Features were extracted from this group, comprising
Fig. 2. shows the confusion matrix for all four kernels
a total feature vector length of 34 features.
used for 4-class classification on the test set. In evaluating
D. Classification the performance of the four kernel functions (RBF, Linear,
Gaussian, and Polynomial) on a four-class classification
For classification, we used a machine learning algorithm
problem (Happy, Angry, Sad, Relaxed), distinct strengths
named Support Vector Machine (SVM). In SVM, data items
and weaknesses emerge. The Polynomial kernel exhibits the
are positioned in an n-dimensional space. The classifica-
highest average precision 0.82, recall 0.81, and F1 score of
tion of these data points involves the identification of a
0.82 across all classes, showcasing its superior performance.
hyperplane, strategically separating the classes for optimal
On the other hand, the Linear kernel demonstrates balanced
distinction. SVM employs an iterative training algorithm to
precision across classes. The RBF and Gaussian kernels
pinpointthemostadvantageoushyperplane,workingtowards
perform similarly, excelling in distinguishing instances in
theminimizationofanerrorfunction.UsingSVM,weclassi-
the "Happy" and "Relaxed" classes but facing challenges in
fied four classes of emotions, based on videos selected from
accurately classifying instances in the "Angry" and "Sad"
four quadrants. We applied different kernel functions like
categories.
radialbasisfunction(RBF),Gaussian,linear,andpolynomial
to compare results. Table II presents the performance comparison of our pro-
posed method with recent studies conducted for the classifi-
III. RESULTSANDDISCUSSION
cationofemotionsinresponsetoVRcontent.Studiesarese-
To train the machine learning model, we split the feature lectedforcomparisonastheyusedVR360-degreevideosfor
vector into 80 percent training and 20 percent testing sets. emotionelicitationstimuli.Thecomparisonwasmadebased
10-fold cross-validation was applied to the training set. We on the number of participants, the number of EEG channels
then tested the test set on the model trained with the best used, and the type of classification. A study conducted in
fold(withmaximumaccuracy).SVMwastrainedusingRBF, [15], that used VR stimuli to elicit the target emotions. EEGTABLE II: PERFORMANCE COMPARISON OF OUR PRO-
[2] S. Alarcao and M. Fonseca, "Emotions Recognition Using EEG
POSED METHOD WITH SOME EXISTING STUDIES THAT Signals:ASurvey",IEEETransactionsonAffectiveComputing,vol.
USED VR CONTENT IN EMOTION RECOGNITION USING 10,no.3,pp.374-393,2019.
[3] P. Samal and R. Singla, ”EEG Based Stress Level Detection During
EEGSIGNALS.
Gameplay,” 2021 2nd Global Conference for Advancement in Tech-
nology(GCAT),pp.1-4,2021.
EEG
Method Subjects Accuracy Classifier Type [4] S. Mantri, D. Patil, P. Agrawal, and V. Wadhai, ”Non-invasive EEG
channels
signalprocessingframeworkforreal-timedepressionanalysis",2015
DER-
4 SAIIntelligentSystemsConference(IntelliSys),pp.518-521,2015.
VREEG 32 4 85.01% SVM
class [5] Sun, J., Tang, Y., Lim, K.O., Wang, J., Tong, S., Li, H. and He, B.,
[17]
”Abnormal Dynamics of EEG Oscillations in Schizophrenia Patients
VREED 3
25 60 71.35% SVM on Multiple Time Scales,” in IEEE Transactions on Biomedical
[18] class
Engineering,vol.61,no.6,pp.1756-1764,2014.
Based
[6] A.Ptukhin,K.Serkov,A.KhrushkovandE.Bozhko,"Prospectsand
on
moderntechnologiesinthedevelopmentofVR/AR,"2018UralSym-
[15] 60 10-20 75% SVM Valance
posiumonBiomedicalEngineering,RadioelectronicsandInformation
and
Technology(USBEREIT),Yekaterinburg,Russia,pp.169-173,2018.
Arousal
[7] Dwivedi,Y.K.,Hughes,L.,Baabdullah,A.M.,Ribeiro-Navarrete,S.,
4
Proposed 40 4 85.54% SVM Giannakis, M., Al-Debei, M.M., Dennehy, D., Metri, B., Buhalis,
class
D., Cheung, C.M. and Conboy, K., "Metaverse beyond the hype:
Multidisciplinaryperspectivesonemergingchallenges,opportunities,
andagendaforresearch,practiceandpolicy",InternationalJournalof
InformationManagement,vol.66,p.102542,2022.
and ECG signals were recorded for 60 participants. Features
[8] Koelstra, S., Muhl, C., Soleymani, M., Lee, J.S., Yazdani, A.,
were extracted from the recorded signals using principal Ebrahimi,T.,Pun,T.,Nijholt,A.andPatras,I.,"DEAP:ADatabase
component analysis (PCA). Extracted features were then for Emotion Analysis; Using Physiological Signals", IEEE Transac-
tionsonAffectiveComputing,vol.3,no.1,pp.18-31,2012.
trainedusingSVMtorecognizethedesiredemotions.Leave-
[9] S.KatsigiannisandN.Ramzan,"DREAMER:ADatabaseforEmotion
One-Subject-Out(LOSO)cross-validationmethodwasused. Recognition Through EEG and ECG Signals from Wireless Low-
The model achieved an accuracy of 75.0% for arousal and costOff-the-ShelfDevices",IEEEJournalofBiomedicalandHealth
Informatics,vol.22,no.1,pp.98-107,2018.
71.0% accuracy for the valence dimension. In [17], a VR-
[10] R. Subramanian, J. Wache, M. Abadi, R. Vieriu, S. Winkler and
based EEG dataset named DER-VREED was presented. 25 N. Sebe,"ASCERTAIN: Emotion and Personality Recognition Using
participants (15 males and 10 females) were engaged in the Commercial Sensors", IEEE Transactions on Affective Computing,
vol.9,no.2,pp.147-160,2018.
experiment. 60 3D videos were used each 4 sec long, and
[11] D.Liaoetal.,"DesignandEvaluationofAffectiveVirtualRealitySys-
20 videos for each positive, negative, and neutral emotion. temBasedonMultimodalPhysiologicalSignalsandSelf-Assessment
A 64-channel wireless EEG device was used to collect the Manikin",IEEEJournalofElectromagnetics,RF,andMicrowavesin
MedicineandBiology,vol.4,no.3,pp.16-224,2020.
signals. The dataset is currently in the embargo phase and
[12] Kim, A., Chang, M., Choi, Y., Jeon, S. and Lee, K., "The effect
isnotyetavailablepublicly.AnotherVR-basedEEGdataset of immersion on emotional responses to film viewing in a virtual
presentedin[18]thattargetsfourclassesofemotionsnamely environment", In 2018 IEEE Conference on Virtual Reality and 3D
UserInterfaces(VR),pp.601-602,2018.
happy, scared, calm, and bored. 32 individuals participated
[13] B. Meuleman and D. Rudrauf, "Induction and Profiling of Strong
and watched 39 3D VR videos. Multi-ComponentialEmotionsinVirtualReality",IEEETransactions
onAffectiveComputing,vol.12,no.1,pp.189-202,2021.
IV. CONCLUSION [14] Liao, D., Shu, L., Liang, G., Li, Y., Zhang, Y., Zhang, W. and Xu,
X.,"Designandevaluationofaffectivevirtualrealitysystembasedon
In this paper, we used VR 360-degree videos as emotion
multimodalphysiologicalsignalsandself-assessmentmanikin",IEEE
elicitation stimuli for collecting an EEG dataset, for human Journal of Electromagnetics, RF and Microwaves in Medicine and
emotions analysis and recognition. From raw EEG signals, Biology,vol.4,no.3,pp.216-224,2019.
[15] Marín-Morales, J., Higuera-Trujillo, J.L., Greco, A., Guixeres, J.,
we extracted five feature groups named correlation, power
Llinares, C., Scilingo, E.P., Alcañiz, M. and Valenza, G., "Affective
spectral density, power spectrum, rational asymmetry, and computing in virtual reality: emotion recognition from brain and
divisional asymmetry. These features were then used to heartbeat dynamics using wearable sensors", Scientific reports, vol.
8,no.1,p.13657,2018.
train SVM with different kernels to classify four classes
[16] Yu, M., Xiao, S., Hua, M., Wang, H., Chen, X., Tian, F. and Li, Y.,
of emotions. Results confirmed that the polynomial kernel "EEG-basedemotionrecognitioninanimmersivevirtualrealityenvi-
outperformed other kernels with a maximum average ac- ronment: From local activity to brain network features", Biomedical
SignalProcessingandControl,vol.72,p.103349,2022.
curacy of 85.54% on 10-fold cross-validation and 82.03%
[17] N. Suhaimi, J. Mountstephens and J. Teo, "A Dataset for Emo-
accuracy on testing. This paper not only bridges a gap in tion Recognition Using Virtual Reality and EEG (DER-VREEG):
VR-based emotion datasets but also establishes a foundation Emotional State Classification Using Low-Cost Wearable VR-EEG
Headsets", Big Data and Cognitive Computing, vol. 6, no. 1, p. 16,
for the integration of emotion recognition into future VR
2022.
technologies. In the future, we intend to extend our work [18] T. Xu, R. Yin, L. Shu and X. Xu, "Emotion Recognition Using
to multimodal studies and use other physiological signals as FrontalEEGinVRAffectiveScenes,"2019IEEEMTT-SInternational
MicrowaveBiomedicalConference(IMBioC),Nanjing,China,2019,
well along with EEG recordings.
pp.1-4.
[19] Li, B.J., Bailenson, J.N., Pines, A., Greenleaf, W.J. and Williams,
REFERENCES L.M.,"ApublicdatabaseofimmersiveVRvideoswithcorresponding
ratingsofarousal,valence,andcorrelationsbetweenheadmovements
[1] Joy,E.,Joseph,R.B.,Lakshmi,M.B.,Joseph,W.,&Rajeswari,M. and self report measures", Frontiers in psychology, vol. 8, p.2116,
“Recent survey on emotion recognition using physiological signals.” 2017.
7thInternationalConferenceonAdvancedComputingandCommuni-
cationSystems(ICACCS),pp.1858-1863,2021.