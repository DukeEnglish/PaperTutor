EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters
QuanSun1* JinshengWang1∗ QiyingYu1,2∗ YufengCui1
FanZhang1 XiaosongZhang1 XinlongWang1
1BeijingAcademyofArtificialIntelligence 2TsinghuaUniversity
∗equalcontribution
code&models:baaivision/EVA/EVA-CLIP-18B
Abstract
Scaling up contrastive language-image pretraining
(CLIP) is critical for empowering both vision and multi-
modal models. We present EVA-CLIP-18B, the largest
andmostpowerfulopen-sourceCLIPmodeltodate, with
18-billion parameters. With only 6-billion training sam-
plesseen,EVA-CLIP-18Bachievesanexceptional80.7%
zero-shot top-1 accuracy averaged across 27 widely rec-
ognized image classification benchmarks, outperforming
itsforerunnerEVA-CLIP(5-billionparameters)andother
open-sourceCLIPmodelsbyalargemargin. Remarkably,
weobserveaconsistentperformanceimprovementwiththe
model size scaling of EVA-CLIP, despite maintaining a
constanttrainingdatasetof2-billionimage-textpairsfrom
LAION-2BandCOYO-700M.Thisdatasetisopenlyavail-
Figure1:ScalingbehaviorofEVA-CLIPwithzero-shotclassi-
ableandmuchsmallerthanthein-housedatasets(e.g.,DFN-
ficationperformanceaveragedacross27imageclassification
5B, WebLI-10B) employed in other state-of-the-art CLIP
benchmarks,comparedwiththecurrentstate-of-the-artandlargest
models. EVA-CLIP-18BdemonstratesthepotentialofEVA-
CLIPmodels(224px). Thediameterofeachcircledemonstrates
style[30,29,63]weak-to-strongvisualmodelscaling. With theforwardGFLOPs×thenumberoftrainingsamplesseen.The
ourmodelweightsmadepubliclyavailable,wehopetofa- performanceofEVA-CLIPconsistentlyimprovesasscalingup.
cilitatefutureresearchinvisionandmultimodalfoundation
models.
ThispaperintroducesEVA-CLIP-18B,thelargestopen-
source CLIP model with 18-billion parameters to narrow
1.Introduction
thisgap. EVA-CLIP[63]open-sourcesaseriesofeffective
andefficientCLIPmodels,whichhavebeenleveragedasthe
RecentyearswitnessedtherapidgrowthofLargeMul-
visionfoundationbynumerousimpactfulworksacross2D/
timodalModels(LMMs)[3,64,62,69,5,46],withCLIP
3Dvisionandmultimodalmodeling[42,78,77,50,69,64].
models [53,19,63,43,75,28,17]servingasafoundational
WefurtherscaleupEVA-CLIPtothissignificantparameter
visionencodertodeliverrobustandtransferablevisualrep-
sizebuildinguponthescalingphilosophyofEVA[30,29]
resentations,andLargeLanguageModels(LLMs)[65,54]
andEVA-CLIP[63].Withmerely6-billiontrainingsamples
servingasageneralinterfaceforreasoningacrossdifferent
seenandtrainedonpubliclyavailabledatasets,EVA-CLIP-
modalities. However, as LLMs have scaled up to around
18Bachievestheexceptional80.7%averagezero-shottop-
100Bparametersorhigher[11,20,65],theadoptedvision
1 accuracy on 27 widely recognized image classification
foundation models continue to operate at a much smaller
benchmarks,significantlysurpassingitsforerunnerEVA-02-
scale,laggingfarbehindtheLLMs.
CLIP-E/14+(5-billionparameters)andotheropen-source
*Correspondencetowangxinlong@baai.ac.cn CLIPmodels. Besides,themodelshavenotexhibitedany
1
4202
beF
6
]VC.sc[
1v25240.2042:viXratotal image text samples imagebatch imagecls. videocls. retrieval
model+ #param.#param.#param. data seen size size gpusfortraining avg.acc. avg.acc. MR
EVA-01-CLIP-g/14+[63] 1.1B 1.0B 124M LAION-400M[59] 11B 2242 41k 256×A100(40GB) 72.2 66.2 80.9
EVA-01-CLIP-g/14+[63] 1.3B 1.0B 354M Merged-2B[63] 11B 2242 114k 112×A100(40GB) 75.1 68.8 85.3
OpenCLIP-G/14+[2] 2.5B 1.8B 695M LAION-2B[58] 39B 2242 160k 512×A100(80GB) 76.2 68.7 85.7
InternVL-C+[17] 14.0B 6.0B 8.0B custom[17] 29B 2242 164k 640×A100(80GB) 78.0 73.7 86.6
DFN5B-CLIP-H/14+[28] 1.0B 632M 354M DFN-5B[28] 39B 2242 79k TPUv4 78.3 67.0 86.6
EVA-02-CLIP-E/14+[63] 5.0B 4.4B 695M LAION-2B[58] 9B 2242 144k 144×A100(80GB) 78.7 72.1 85.7
DFN5B-CLIP-H/14+[28] 1.0B 632M 354M DFN-5B[28] 5B 3782 79k TPUv4 79.2 68.4 87.2
EVA-CLIP-8B+ 8.1B 7.5B 695M Merged-2B[63] 9B 2242 178k 384×A100(40GB) 79.4 73.6 86.2
EVA-CLIP-18B+ 18.1B 17.5B 695M Merged-2B+ 6B 2242 108k 360×A100(40GB) 80.7 75.0 87.8
Table 1: CLIP model configurations and zero-shot performance on 33 benchmarks including 27 image classification, 4 video
classificationand2image-textretrievaldatasets.DFN-5B[28]are5Bimagesfilteredfromapoolof43Buncuratedimage-textpairs
consistingof12.8Bimage-textpairsfromCommonPool-12.8B[32]and30Badditionalpublicimage-textpairs.Thedatasetusedfortraining
InternVL-C[17]iscustommixtures,seedetailin[17].
method
EVA-01-CLIP-g/14+ 78.571.573.692.567.672.3 98.388.762.687.774.232.428.991.765.861.773.852.274.593.549.349.994.258.470.398.985.7 72.2
EVA-01-CLIP-g/14+ 79.372.574.192.768.475.3 99.190.172.089.574.739.931.890.770.267.873.256.079.793.766.562.494.958.671.499.584.7 75.1
OpenCLIP-G/14+ 80.473.669.392.869.973.0 98.387.571.689.475.053.634.994.973.069.171.159.681.593.162.763.695.365.372.698.587.4 76.2
InternVL-C+ 83.277.383.895.774.380.6 99.493.180.689.576.353.335.194.469.270.879.456.285.895.365.548.796.368.474.499.480.0 78.0
DFN5B-CLIP-H/14+ 83.577.471.792.972.876.7 98.890.585.889.577.071.434.495.877.470.765.254.792.595.867.765.296.554.876.198.981.5 78.3
EVA-02-CLIP-E/14+ 82.175.782.194.772.279.6 99.393.274.790.575.358.737.094.777.668.275.959.084.594.967.764.496.062.675.799.387.9 78.7
DFN5B-CLIP-H/14+ 84.378.379.693.673.379.6 98.890.583.688.977.472.537.996.080.570.961.156.191.696.267.969.696.855.575.999.181.9 79.2
EVA-CLIP-8B+ 83.577.785.295.374.381.2 99.392.384.889.676.260.541.794.879.071.068.956.186.495.570.958.196.466.275.399.385.1 79.4
EVA-CLIP-18B+ 83.877.987.395.774.782.2 99.493.883.089.877.759.743.194.979.972.179.859.386.095.872.465.296.167.576.999.685.8 80.7
Table2:EVA-CLIPzero-shotimageclassificationperformanceon27datasets.Wereporttop-1accuracyonalldatasets.Thebestresults
areinboldandthesecondbestareunderlined.
signalofperformancesaturation,sheddinglightonfurther effectivenessofourmodel-scalespecificscalingphilosophy,
scaling of vision models. An intuitive demonstration is althoughscalingupdatasetscanfurtherunleashthescaling
showninFigure1. powerofourmethod.
ThesuccessfultrainingofEVA-CLIP-18Bexemplifies Specifically,inthiswork,wepre-trainalargeEVAmodel
thepotentialoftheEVA-stylevisualmodelscalingphiloso- namedEVA-18BusingasmallEVA-CLIP(EVA-02-CLIP-
phy.Wekeepopen-sourcingthetrainingcodeandweightsof E/14+)[63]astheteacher,whichistrainedtoreconstruct
ourmodelstoencouragefurtherresearchandempowerthe masked image-text aligned vision features from EVA-02-
developmentofvisionandmultimodalfoundationmodels. CLIP-E/14+. EVA-18BomitsbiastermsofQKVprojec-
tions and uses RMSNorm [76] instead of LayerNorm [4]
2.Weak-to-StrongVisionScaling followingLLaMA[65]. Subsequently,weleveragetheEVA
model as the vision encoder initialization for EVA-CLIP
Ourscaling-upprocedureisguidedbytheprinciplesof pre-trainingwiththeimage-textcontrastivelearningobjec-
EVA[30]andEVA-CLIP[63]. TheEVAphilosophyfor tive. Besides,wealsointroduceasmallercounterpart,EVA-
scalingvisualmodelsfollowsaweak-to-strongparadigm, CLIP-8B,whichundergoessimilarpre-trainingmethodolo-
designedtoimprovevisualmodelsthroughastrategicpro- gies. Notably,ourexperimentsdemonstratesustainedperfor-
gression.ThisprocessbeginswithalargeEVAvisionmodel manceimprovementwiththeprogressiveweak-teach-strong
distillingknowledgefromasmallEVA-CLIPmodel,which scalingupofEVA-CLIP.
inturnservesasthevisionencoderinitializationtostabilize
and accelerate the training of a larger EVA-CLIP. After 3.Experiments
that,theclosed-loopscaling-upcyclecontinuesandalarger
EVAisdistilledout. Throughoutourmodelscalingcycle, Settings. Following EVA-CLIP [63], we initialized the
thetrainingdatasetremainslargelyfixedtodemonstratethe model with pre-trained vision and text encoders. Specifi-
2
]62[K1-teNegamI ]75[2V-teNegamI
]63[.vdA-teNegamI ]53[.neR-teNegamI ]86[.ekS-teNegamI
]8[teNtcejbO ]04[01-RAFIC ]04[001-RAFIC ]14[TSINM ]13[101hcetlaC ]27[793NUS
]74[tfarcriACVGF
]35[112-yrtnuoC ]93[sraCdrofnatS ]9[pansdriB
]12[DTD
]43[TASoruE ]33[3102REF ]94[201-srewolF ]01[101-dooF ]16[BRSTG
]76[maCP ]15[steP
]35[2TSSderedneR
]81[54CSISER ]32[01-LTS ]72[7002COV .cca1-pot.gva.zero-shottextretrieval zero-shotimageretrieval
Flickr30K COCO Flickr30K COCO
method+ R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 MR
EVA-01-CLIP-g/14+ 87.9 98.0 99.5 61.7 83.2 89.9 72.5 91.5 95.4 44.5 69.1 77.7 80.9
EVA-01-CLIP-g/14+ 93.3 99.5 99.9 69.4 88.3 93.2 79.2 95.2 97.3 51.1 74.7 82.5 85.3
OpenCLIP-G/14+ 93.5 99.3 99.7 69.0 87.8 93.1 80.9 95.1 97.2 52.6 76.1 83.6 85.7
EVA-02-CLIP-E/14+ 94.3 99.6 99.8 69.4 88.6 93.3 79.7 94.9 97.3 52.5 75.9 83.4 85.7
EVA-CLIP-8B+ 95.6 99.6 99.9 70.3 89.3 93.9 80.8 95.5 97.6 53.0 76.0 83.4 86.2
DFN5B-CLIP-H/14+ 92.9 99.3 99.9 72.3 90.2 94.6 80.1 95.2 97.3 53.9 78.0 85.6 86.6
InternVL-C+ 93.8 99.7 100.0 70.3 89.2 93.8 82.1 96.0 98.1 54.1 77.1 84.8 86.6
DFN5B-CLIP-H/14+ 93.6 99.3 99.6 71.8 90.4 94.9 82.1 96.0 97.9 55.6 79.2 86.3 87.2
EVA-CLIP-18B+ 96.7 99.7 100.0 73.6 90.9 95.0 83.3 96.3 98.3 56.2 78.5 85.6 87.8
Table3:Zero-shotretrievalperformanceonFlickr30K[74]andCOCO[45].
cally, we employ a pre-trained EVA-18B [30, 29] as the imageencoder textencoder #params
method layerswidthheadslayerswidthheadsimage text total
visionencoderandEVA-02-CLIP-E/14+ [63]forthetext
EVA-CLIP-8B+ 32 4096 32 32 1280 20 7.5B 695M 8.1B
encoder. WeadopttheLAMBoptimizer[73]withβ =0.9,
1 EVA-CLIP-18B+ 48 5120 40 32 1280 20 17.5B695M18.1B
β =0.95,andaweightdecayof0. Weapplydifferentlearn-
2
ingratesandlayerdecayratestothevisionencoderandtext Table4:Architectureconfigurations.
encodertoensureoptimaltraining. Wesetthepeaklearning
rate as 4e-4 and 4e-5 for the vision encoder and the text
Zero-ShotImageClassification. Weshowtheexceptional
encoderrespectively,with2000warm-upsteps. Afterwards,
performanceofEVA-CLIPonall27zero-shotimageclassi-
the learning rates decay to 0 with a cosine schedule. The
ficationbenchmarksinTable2. EVA-CLIP-18Bachieves
learningratelayerdecayratesareconfiguredas0.9and0.75
80.7%top-1accuracyaveragedacrossall27benchmarks.
forthevisionandtextencoders. Thetemperatureparameter
Theseresultssignificantlyoutperformthepreviousbestopen-
remainsconstantat0.01. Further, weusetheDeepSpeed
sourcedDFN5B-CLIP-H/14+[28]by+1.5%,andthelargest
optimizationlibrary[56]withZeROstage-3partition[55],
existingCLIPmodel,InternVL-C[17],by+2.7%. ForBird-
gradientcheckpointing[16]andflashattention[24]toopti-
snapdataset,thedownloadwaslimitedto2195testimages
mizethetrainingcost.
duetobrokenlinks.
Dataset. OurMerged-2Bdatasetconsistsof1.6billionsam-
method+ #Frames UCF-101K-400K-600K-700 avg.
ples from LAION-2B [58] and 0.4 billion samples from
EVA-01-CLIP-g/14+ 1 76.0 65.4 64.5 58.8 66.2
COYO-700M [12]. Note that the use of a subset from
DFN5B-CLIP-H/14+ 1 78.2 65.2 65.5 59.2 67.0
LAION-2Bisnottheresultofdeliberatefiltering,butrather
DFN5B-CLIP-H/14+ 1 79.2 66.7 67.0 60.7 68.4
duetoimagedownloadingfailures. Theuseof0.4billion
OpenCLIP-G/14+ 1 80.5 67.1 66.9 60.3 68.7
COYO-700Msamplesaimstocomplementthenumberof
EVA-01-CLIP-g/14+ 1 78.9 67.3 67.3 61.5 68.8
trainingsamplestonearlythesameasLAION-2B.Merged-
EVA-02-CLIP-E/14+ 1 83.1 70.7 70.0 64.4 72.1
2B+consistsofallsamplesfromMerged-2B,alongwithad-
EVA-CLIP-8B+ 1 85.7 71.3 71.2 66.1 73.6
ditional20millionsamplesfromLAION-COCO[1]and23
InternVL-C+ 1 85.2 71.8 71.7 66.4 73.7
millionsamplesfromMerged-videoincludingVideoCC[48], EVA-CLIP-18B+ 1 86.0 72.9 72.9 68.2 75.0
InternVid[70]andWebVid-10M[6]. Merged-videoisin- EVA-CLIP-18B+ 8 88.2 79.3 79.2 72.1 79.7
cludedattheendofthetrainingprocess. EVA-CLIP-18B+ 16 88.4 79.4 79.4 72.2 79.8
EVA-CLIP-18Bpre-trainswith5.4billionsamplesfrom
Table5:EVA-CLIPzero-shotvideoclassificationperformance.
Merged-2Bseenwith50%ofpatchdropoutratio[44],0.6
We report top1 accuracy for UCF-101 [60], average of top1
billion samples from Merged-2B and 20 million samples
andtop5accuracyforKinetics-400[15], Kinetics-600[13]and
fromLAION-COCOwithoutpatchdropout,and24million Kinetics-700[14].
samplesfromMerged-videowith50%ofpatchdropoutratio.
Evaluation. Weevaluateon33widelyuseddatasetsacross Zero-ShotVideoClassification. Wereportthetop-1accu-
image, video classification and image-text retrieval. All racyforUCF-101[60]andthemeanoftop-1andtop-5accu-
datasets used to evaluate EVA-CLIP-18B are reported in racyforKinetics-400[15],Kinetics-600[13]andKinetics-
Table11. Weutilizethespecifiedprompttemplatesfollow- 700[14]. InTable5wedemonstratethatEVA-CLIP-18B
ing[53,38]. alsooutperformsotherCLIPmodelsonzero-shotvideoclas-
3method+ IN-1K IN-A IN-R IN-V2 IN-Sketch ObjectNet ∆↓ avg.acc.
DFN5B-CLIP-H/14+ 83.5 71.7 92.9 77.4 72.8 76.7 4.4 79.2
OpenCLIP-G/14+ 80.4 69.3 92.8 73.6 69.9 73.0 3.9 76.5
SigLIP-SO[75](reported) 82.0 71.9 95.1 76.1 74.0 70.6 3.7 78.3
DFN5B-CLIP-H/14+ 84.3 79.6 93.6 78.3 73.3 79.6 2.8 81.5
EVA-01-CLIP-g/14+ 78.5 73.6 92.5 71.5 67.6 72.3 2.5 76.0
EVA-01-CLIP-g/14+ 79.3 74.1 92.7 72.5 68.4 75.3 2.2 77.1
BASIC-L[52](reported) 85.7 85.6 95.7 80.6 76.1 82.3 1.4 84.3
SigLIP-SO+[75](reported) 83.0 82.5 95.8 77.2 74.5 77.0 1.3 81.7
EVA-02-CLIP-E/14+ 82.1 82.1 94.7 75.7 72.2 79.6 1.0 81.1
InternVL-C+ 83.2 83.8 95.7 77.3 74.3 80.6 0.7 82.5
EVA-CLIP-8B+ 83.5 85.2 95.3 77.7 74.3 81.2 0.6 82.9
EVA-CLIP-18B+ 83.8 87.3 95.7 77.9 74.7 82.2 0.2 83.6
(a) Zero-shotperformanceonImageNetvariantsandObjectNet.“avg.acc.”:theaveragedtop-1accuracyondifferentImageNetvariants(i.e.,IN-{1K,V2,
ReaL,Adv.,Ren.,Ske.}),andObjectNet.“∆↓”:Thegapbetweentheaveragedtop-1accuracyandtheImageNet-1Ktop-1accuracy(thelowerthebetter).
EVA-CLIPsuffersfromthesmallestperformancedrop(only0.2%top-1accuracygapforEVA-CLIP-18B)whileEVA-CLIP-18Bachieves83.6%top-1
accuracyaveragedonall6benchmarks.
method
BASIC-L[52](reported) 85.7 80.6 85.6 95.7 76.1 82.3 97.5 82.3 40.3 76.2 59.2 64.6 51.0 95.1 59.6 72.7 99.6 76.7(77.8)
EVA-CLIP-18B+ 83.8 77.9 87.3 95.7 74.7 82.2 99.4 93.8 83.0 77.7 79.9 72.1 79.8 95.8 65.2 76.9 99.6 84.9(84.1)
-1.9 -2.7 +1.7 +0.0 -1.4 -0.1 +1.9 +11.5 +42.7 +1.5 +20.7 +7.5 +28.8 +0.7 +5.6 +4.2 +0.0 +8.2(+6.3)
(b) ComparisonEVA-CLIP-18B’szero-shotimageclassificationperformancewithBASIC-L[52]on17datasets.Ourreportincludesthetop-1accuracy
foralldatasets,consideringthatBASIC-Lonlyprovidedtop-1accuracyforthesespecific17datasets.()istheaveragetop-1accuracyremovingBirdsnapdue
tothedifferenttestsizebetweenEVA-CLIP-18BandBASIC-L.EVA-CLIP-18BoutperformsBASIC-Lwithanotablemarginof+8.2(+6.3)inaveragetop-1
accuracy,despiteexhibitinglowerperformanceonImageNetvariants.
Table6:RobustnessevaluationofCLIPmodelsandcomparisonwithBASIC-L[52]on17Benchmarks.
sificationbenchmarksbyalargemargin. Whensampling doesnotnecessarilyleadtobetteroverallperformance,as
asinglecenterframepervideo,EVA-CLIP-18Bachieves evidencedinTable6b,whereBASIC-L[52]exhibitshigher
accuraciesof86.0%,72.9%,72.9%,and68.2%acrossthe ImageNet-related top-1 accuracy but considerably lower
fourevaluatedbenchmarks. Further,whenuniformlysample overall average top-1 accuracy compared to EVA-CLIP-
8 or 16 frames per video, we observe an improvement of 18B across a broader range of datasets and distributions,
+4.7%/+4.8%averagedacrossfourbenchmarkscompared showingadifferenceof-8.2%.
tothesingle-framesetting.
LinearProbingonImageNet-1K.InTable7,wepresent
Zero-ShotImage-TextRetrieval. InTable3,wereportthe
the results of linear probing on ImageNet-1K [26]. EVA-
zero-shotimageandtextretrievalresultsonFlickr30K[74]
CLIP-18B achieves an average top-1 accuracy of 88.9%,
andCOCO[45]. EVA-CLIP-18Bachievesanaveragere-
surpassingInternVL-C[17]by0.7%.
callof87.8%acrossallretrievalbenchmarks,significantly
outperformingcompetitors.
method #param top1acc.
Robustness. In Table 6, we demonstrate that scaling up
OpenCLIP-G/14(reported) 1.8B 86.2
EVA-CLIPsignificantlyenhancestherobustnessofvisual
EVA-01-CLIP-g/14+ 1.0B 86.5
representations. EVA-CLIPsuffersfromthesmallestper-
EVA-02-CLIP-E/14+ 4.4B 88.1
formancedrop(∆↓)betweenImageNet-1KandImageNet
InternVL-C(reported) 5.9B 88.2
variantsincludingadversarialones,withmerely0.2%top-1
EVA-CLIP-8B+ 7.5B 88.5
accuracygapforEVA-CLIP-18B.
EVA-CLIP-18B+ 17.5B 88.9
Foramorerobustandcomprehensiveevaluationofro-
bustnessandzero-shotperformance,itisadvisabletoinclude Table7: LinearProbingonImageNet-1K[26]. Thetop-1ac-
morebenchmarkscoveringmoreimagedistributions. How- curacyshowsacontinuousimprovementwiththescalingupof
ever,wewanttonotethathigherImageNettop-1accuracy EVA-CLIP.
4
]62[K1-teNegamI ]75[2V-teNegamI
]63[.vdA-teNegamI ]53[.neR-teNegamI ]86[.ekS-teNegamI
]8[teNtcejbO ]04[01-RAFIC ]04[001-RAFIC ]14[TSINM ]27[793NUS ]9[pansdriB
]12[DTD
]43[TASoruE ]01[101-dooF
]76[maCP
]81[54CSISER ]32[01-LTS
.cca1-pot.gva.3D Representation. We adopt the Uni3D [77] setting to method+ resolution IN-1KIN-AIN-RIN-V2IN-Ske.ObjectNet avg.
exploretheeffectivenessofscalingupteachers. Withthe EVA-CLIP-8B+ 224×224 83.5 85.2 95.3 77.7 74.3 81.2 82.9
scaling up of EVA-CLIP in Table 8, we observe consis- EVA-CLIP-8B+ 448×448 83.8 88.7 95.4 77.7 74.1 82.9 83.8
tent improvements in 3D representation learning capabili- +0.3 +3.5 +0.1 +0.0 -0.2 +1.7 +0.9
ties. Further,Uni3D-baseequippedwithEVA-CLIP-18B EVA-CLIP-18B+ 224×224 83.8 87.3 95.7 77.9 74.7 82.2 83.6
setsnewrecordsonModelNet[71]andScanObjectNN[66] EVA-CLIP-18B+ 336×336 83.9 88.9 95.6 78.2 74.3 83.6 84.1
benchmarks. +0.1 +1.6 -0.1 +0.3 -0.4 +1.4 +0.5
Table10:Increasingresolution.Wereportzero-shotperformance
teacher data O-LVIS MNet40 ScanObjNN
onImageNetvariantsandObjectNet.
OpenCLIP-G/14+ w/oLVIS 44.5 85.8 58.9
EVA-02-CLIP-E/14+ w/oLVIS 45.8 86.1 61.7
EVA-CLIP-8B+ w/oLVIS 46.2 87.3 62.7
5.Conclusion
EVA-CLIP-18B+ w/oLVIS 47.0 87.6 65.3
EVA-02-CLIP-E/14+ Ensembled 51.7 86.3 63.8 We present EVA-CLIP-18B, the currently largest and
EVA-CLIP-18B+ Ensembled 53.2(+1.5)88.6(+2.3) 67.8(+4.0) mostperformantopen-sourcedCLIPmodelwith18-billion
parameters. WeshowthatfollowingEVA’sweak-to-strong
Table8: EVA-CLIP-18Benhanceszero-shot3dclassification
visionscalingprinciple,wecanfurtherscaleupCLIPmod-
performance.WeuseUni3D-base[77]asthebaselineandscale
elstoanewrecordandadvanceSOTAonmultipleprevalent
theteacherfrom5Bto18B.Wereporttop-1accuracyonObjaverse-
LVIS[25],ModelNet40[71]andScanObjectNN[66]. benchmarksacrossimage,videoand3Ddomains. Impor-
tantly,wedemonstratethatscalingupthesizeofEVA-CLIP
modelsconsistentlyboostsperformancewithnosignofsat-
uration,sheddinglightonfuturevisionmodelscaling.
4.AblationStudies
References
Video Data. In Table 9, we conduct ablations on EVA-
[1] Laioncoco: 600msyntheticcaptionsfromlaion2b-en. https:
CLIP-18B’szero-shotperformance,comparingresultswhen
//laion.ai/blog/laion-coco/.3
trained with and without Merged-Video. The training ob-
[2] Reaching80zero-shotaccuracywithopenclip:Vit-g/14trainedon
jectiveforthevideodataalignswiththatofimages,encom-
laion-2b. https://laion.ai/blog/giant-openclip/.
passingtheextractionoffeaturesfromvideowhere8frames 2
areuniformlysampled. Themeanofall[CLS]embeddings
[3] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,
servesasarepresentationforthevideo.Theoutcomesreveal IainBarr,YanaHasson,KarelLenc,ArthurMensch,KatieMillican,
substantialperformanceimprovementsassociatedwithtrain- MalcolmReynolds,etal. Flamingo:avisuallanguagemodelfor
ingusingMerged-Video. Thezero-shotperformance,aver- few-shotlearning.arXivpreprintarXiv:2204.14198,2022.1
agedacrossUCF-101[60]andKinetics-400[15]/600[13] [4] JimmyLeiBa,JamieRyanKiros,andGeoffreyE.Hinton.Layer
normalization.arXivpreprintarXiv:1607.06450,2016.2
/700[14],indicatesagainof+0.7forevaluationwithone
middleframeand+0.8forevaluationwith8frames. [5] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,Peng
Wang,JunyangLin,ChangZhou,andJingrenZhou. Qwen-vl:A
versatilevision-languagemodelforunderstanding,localization,text
classification retrieval reading,andbeyond.arXivpreprintarXiv:2308.12966,2023.1
imagevideo(#F1)video(#F8) avg.recall
[6] Max Bain, Arsha Nagrani, Gu¨l Varol, and Andrew Zisserman.
w/ovideodata 80.7 74.3 78.9 87.9 Frozenintime: Ajointvideoandimageencoderforend-to-end
w/videodata 80.7 75.0(+0.7) 79.7(+0.8) 87.8(-0.1) retrieval.InProceedingsoftheIEEE/CVFInternationalConference
Table9:Videodataenhanceszero-shotvideoclassificationper- onComputerVision,pages1728–1738,2021.3
formance. Werespectivelyreportperformancesaveragedon27 [7] HangboBao,LiDong,andFuruWei. Beit: Bertpre-trainingof
imageclassificationbenchmarks,4videobenchmarksand2image- imagetransformers.arXivpreprintarXiv:2106.08254,2021.9
textretrievalbenchmarks. [8] AndreiBarbu,DavidMayo,JulianAlverio,WilliamLuo,Christo-
pherWang,DanGutfreund,JoshTenenbaum,andBorisKatz.Ob-
jectnet:Alarge-scalebias-controlleddatasetforpushingthelimits
ImageResolution. InTable10,weinvestigatetheimpactof ofobjectrecognitionmodels.InNeurIPS,2019.2,4,9,10
largerimageresolutionsonzero-shotperformance. Notably, [9] ThomasBerg,JiongxinLiu,SeungWooLee,MichelleLAlexander,
DavidWJacobs,andPeterNBelhumeur. Birdsnap:Large-scale
there is an average top-1 accuracy gain of +0.9 when the
fine-grainedvisualcategorizationofbirds.InCVPR,2014.2,4,9,
resolutionincreasesfrom2242to4482forEVA-CLIP-8B.
10
Similarly,anincreasefrom2242to3362resultsinagainof
[10] LukasBossard,MatthieuGuillaumin,andLucVanGool.Food-101–
+0.5,evenwhentrainedwithlowglobalbatchsizesof24k
miningdiscriminativecomponentswithrandomforests.InECCV,
forEVA-CLIP-8B+and23kforEVA-CLIP-18B+. 2014.2,4,9,10
5[11] TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah, [23] AdamCoates,AndrewNg,andHonglakLee.Ananalysisofsingle-
Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav layernetworksinunsupervisedfeaturelearning.InAISTAT,2011.
Shyam,GirishSastry,AmandaAskell,SandhiniAgarwal,Ariel 2,4,9,10
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
[24] TriDao,DanielY.Fu,StefanoErmon,AtriRudra,andChristopher
AdityaRamesh,DanielM.Ziegler,JeffreyWu,ClemensWinter,
Re´.Flashattention:Fastandmemory-efficientexactattentionwith
ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,Scott
io-awareness,2022.3
Gray,BenjaminChess,JackClark,ChristopherBerner,SamMcCan-
dlish,AlecRadford,IlyaSutskever,andDarioAmodei.Language [25] MattDeitke,DustinSchwenk,JordiSalvador,LucaWeihs,Oscar
modelsarefew-shotlearners. arXivpreprintarXiv:2005.14165, Michel,EliVanderBilt,LudwigSchmidt,KianaEhsani,Aniruddha
2020.1 Kembhavi,andAliFarhadi.Objaverse:Auniverseofannotated3d
objects.InProceedingsoftheIEEE/CVFConferenceonComputer
[12] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee,
VisionandPatternRecognition,pages13142–13153,2023.5
Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-
text pair dataset. https://github.com/kakaobrain/ [26] JiaDeng, WeiDong, RichardSocher, Li-JiaLi, KaiLi, andLi
coyo-dataset,2022.3 Fei-Fei. Imagenet:Alarge-scalehierarchicalimagedatabase. In
CVPR,2009.2,4,9,10
[13] JoaoCarreira,EricNoland,AndrasBanki-Horvath,ChloeHillier,
andAndrewZisserman. Ashortnoteaboutkinetics-600. arXiv [27] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
preprintarXiv:1808.01340,2018.3,5,9 and A. Zisserman. ”the PASCAL Visual Object Classes
Challenge 2007 (VOC2007) Results. ”http://www.pascal-
[14] JoaoCarreira,EricNoland,ChloeHillier,andAndrewZisserman.A
network.org/challenges/VOC/voc2007/workshop/index.html”,2007.
shortnoteonthekinetics-700humanactiondataset.arXivpreprint
2,9,10
arXiv:1907.06987,2019.3,5,9
[28] AlexFang,AlbinMadappallyJose,AmitJain,LudwigSchmidt,
[15] JoaoCarreiraandAndrewZisserman. Quovadis,actionrecogni-
AlexanderToshev,andVaishaalShankar. Datafilteringnetworks.
tion?anewmodelandthekineticsdataset. InCVPR,2017. 3,5,
arXivpreprintarXiv:2309.17425,2023.1,2,3
9
[29] YuxinFang,QuanSun,XinggangWang,TiejunHuang,Xinlong
[16] TianqiChen,BingXu,ChiyuanZhang,andCarlosGuestrin.Train-
Wang, andYueCao. Eva-02: Avisualrepresentationforneon
ingdeepnetswithsublinearmemorycost,2016.3
genesis.arXivpreprintarXiv:2303.11331,2023.1,3
[17] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,Sen
[30] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,Xing-
Xing,MuyanZhong,QinglongZhang,XizhouZhu,LeweiLu,Bin
gang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva:
Li,PingLuo,TongLu,YuQiao,andJifengDai.Internvl:Scalingup
Exploringthelimitsofmaskedvisualrepresentationlearningat
visionfoundationmodelsandaligningforgenericvisual-linguistic
scale.arXivpreprintarXiv:2211.07636,2022.1,2,3
tasks.arXivpreprintarXiv:2312.14238,2023.1,2,3,4
[31] LiFei-Fei,RobFergus,andPietroPerona. Learninggenerative
[18] GongCheng,JunweiHan,andXiaoqiangLu.Remotesensingimage
visualmodelsfromfewtrainingexamples:Anincrementalbayesian
sceneclassification:Benchmarkandstateoftheart.Proceedingsof
approachtestedon101objectcategories.InCVPRW,2004.2,9,10
theIEEE,2017.2,4,9,10
[32] SamirYitzhakGadre,GabrielIlharco,AlexFang,JonathanHayase,
[19] MehdiCherti,RomainBeaumont,RossWightman,MitchellWorts-
GeorgiosSmyrnis,ThaoNguyen,RyanMarten,MitchellWortsman,
man,GabrielIlharco,CadeGordon,ChristophSchuhmann,Ludwig
DhrubaGhosh,JieyuZhang,EyalOrgad,RahimEntezari,Gian-
Schmidt,andJeniaJitsev.Reproduciblescalinglawsforcontrastive
nisDaras,SarahPratt,VivekRamanujan,YonatanBitton,Kalyani
language-imagelearning,2022.1
Marathe,StephenMussmann,RichardVencu,MehdiCherti,Ran-
[20] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten jayKrishna,PangWeiKoh,OlgaSaukh,AlexanderRatner,Shu-
Bosma,GauravMishra,AdamRoberts,PaulBarham,HyungWon ranSong,HannanehHajishirzi,AliFarhadi,RomainBeaumont,
Chung,CharlesSutton,SebastianGehrmann,ParkerSchuh,Kensen SewoongOh,AlexDimakis,JeniaJitsev,YairCarmon,Vaishaal
Shi,SashaTsvyashchenko,JoshuaMaynez,AbhishekRao,Parker Shankar,andLudwigSchmidt.Datacomp:Insearchofthenextgen-
Barnes,YiTay,NoamShazeer,VinodkumarPrabhakaran,Emily erationofmultimodaldatasets.arXivpreprintarXiv:2304.14108,
Reif,NanDu,BenHutchinson,ReinerPope,JamesBradbury,Ja- 2023.2
cob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju
[33] Ian J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron
Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Hen-
Courville, MehdiMirza, BenHamner, WillCukierski, Yichuan
rykMichalewski,XavierGarcia,VedantMisra,KevinRobinson,
Tang,DavidThaler,Dong-HyunLee,etal. Challengesinrepre-
LiamFedus,DennyZhou,DaphneIppolito,DavidLuan,Hyeon-
sentationlearning:Areportonthreemachinelearningcontests.In
taekLim,BarretZoph,AlexanderSpiridonov,RyanSepassi,David
ICONIP,2013.2,9,10
Dohan,ShivaniAgrawal,MarkOmernick,AndrewM.Dai,Thanu-
malayanSankaranarayanaPillai,MariePellat,AitorLewkowycz, [34] PatrickHelber,BenjaminBischke,AndreasDengel,andDamian
EricaMoreira,RewonChild,OleksandrPolozov,KatherineLee, Borth.Eurosat:Anoveldatasetanddeeplearningbenchmarkfor
ZongweiZhou,XuezhiWang,BrennanSaeta,MarkDiaz,Orhan landuseandlandcoverclassification.IEEEJ.Sel.Top.Appl.Earth
Firat,MicheleCatasta,JasonWei,KathyMeier-Hellstern,Douglas Obs.RemoteSens.,2019.2,4,9,10
Eck,JeffDean,SlavPetrov,andNoahFiedel. Palm:Scalinglan-
[35] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath,
guagemodelingwithpathways.arXivpreprintarXiv:2204.02311,
FrankWang,EvanDorundo,RahulDesai,TylerZhu,SamyakPara-
2022.1
juli,MikeGuo,etal. Themanyfacesofrobustness: Acritical
[21] M.Cimpoi,S.Maji,I.Kokkinos,S.Mohamed,,andA.Vedaldi. analysisofout-of-distributiongeneralization.InCVPR,2021.2,4,
Describingtexturesinthewild.InCVPR,2014.2,4,9,10 9,10
[22] KevinClark,Minh-ThangLuong,QuocVLe,andChristopherD [36] DanHendrycks,KevinZhao,StevenBasart,JacobSteinhardt,and
Manning.ELECTRA:Pre-trainingtextencodersasdiscriminators DawnSong.Naturaladversarialexamples.InCVPR,2021.2,4,9,
ratherthangenerators.arXivpreprintarXiv:2003.10555,2020.9 10
6[37] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q [56] JeffRasley,SamyamRajbhandari,OlatunjiRuwase,andYuxiong
Weinberger.Deepnetworkswithstochasticdepth.InECCV,2016. He.Deepspeed:Systemoptimizationsenabletrainingdeeplearning
9 modelswithover100billionparameters.InKDD,2020.3,9
[38] GabrielIlharco,MitchellWortsman,RossWightman,CadeGor- [57] BenjaminRecht,RebeccaRoelofs,LudwigSchmidt,andVaishaal
don,NicholasCarlini,RohanTaori,AchalDave,VaishaalShankar, Shankar.Doimagenetclassifiersgeneralizetoimagenet?,2019.2,
Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali 4,9,10
Farhadi, andLudwigSchmidt. Openclip. https://github.
[58] ChristophSchuhmann,RomainBeaumont,RichardVencu,Cade
com/mlfoundations/open_clip,2021.3
Gordon,RossWightman,MehdiCherti,TheoCoombes,Aarush
[39] JonathanKrause,MichaelStark,JiaDeng,andLiFei-Fei.3dobject
Katta,ClaytonMullis,MitchellWortsman,etal.Laion-5b:Anopen
representationsforfine-grainedcategorization.InICCVW,2013.2,
large-scaledatasetfortrainingnextgenerationimage-textmodels.
9,10 arXivpreprintarXiv:2210.08402,2022.2,3
[40] AlexKrizhevsky,GeoffreyHinton,etal.Learningmultiplelayers
[59] ChristophSchuhmann,RichardVencu,RomainBeaumont,Robert
offeaturesfromtinyimages.2009.2,4,9,10
Kaczmarczyk,ClaytonMullis,AarushKatta,TheoCoombes,Je-
[41] YannLeCun,Le´onBottou,YoshuaBengio,andPatrickHaffner. nia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset
Gradient-basedlearningappliedtodocumentrecognition.Proceed- of clip-filtered 400 million image-text pairs. arXiv preprint
ingsoftheIEEE,1998.2,4,9,10 arXiv:2111.02114,2021.2
[42] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2:
[60] KhurramSoomro,AmirRoshanZamir,andMubarakShah.Ucf101:
Bootstrappinglanguage-imagepre-trainingwithfrozenimageen-
Adatasetof101humanactionsclassesfromvideosinthewild.
codersandlargelanguagemodels.arXivpreprintarXiv:2301.12597,
arXivpreprintarXiv:1212.0402,2012.3,5,9
2023.1
[61] JohannesStallkamp,MarcSchlipsing,JanSalmen,andChristian
[43] XianhangLi,ZeyuWang,andCihangXie.Clipa-v2:Scalingclip
Igel.Manvs.computer:Benchmarkingmachinelearningalgorithms
trainingwith81.1arXivpreprintarXiv:2306.15658,2023.1
fortrafficsignrecognition.Neuralnetworks,2012.2,9,10
[44] YanghaoLi,HaoqiFan,RonghangHu,ChristophFeichtenhofer,
[62] QuanSun,YufengCui,XiaosongZhang,FanZhang,QiyingYu,
andKaimingHe.Scalinglanguage-imagepre-trainingviamasking,
ZhengxiongLuo,YuezeWang,YongmingRao,JingjingLiu,Tiejun
2022.3,9
Huang, andXinlongWang. Generativemultimodalmodelsare
[45] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,Pietro
in-contextlearners.arXivpreprintarXiv:2312.13286,2023.1
Perona,DevaRamanan,PiotrDolla´r,andCLawrenceZitnick.Mi-
crosoftcoco:Commonobjectsincontext.InEuropeanconference [63] QuanSun,YuxinFang,LedellWu,XinlongWang,andYueCao.
oncomputervision,pages740–755.Springer,2014.3,4,9 Eva-clip: Improvedtrainingtechniquesforclipatscale. arXiv
preprintarXiv:2303.15389,2023.1,2,3
[46] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visual
instructiontuning.arXivpreprintarXiv:2304.08485,2023.1 [64] QuanSun,QiyingYu,YufengCui,FanZhang,XiaosongZhang,
YuezeWang, HongchengGao, JingjingLiu, TiejunHuang, and
[47] SubhransuMaji,EsaRahtu,JuhoKannala,MatthewBlaschko,and
XinlongWang. Generativepretraininginmultimodality. arXiv
AndreaVedaldi.Fine-grainedvisualclassificationofaircraft.arXiv
preprintarXiv:2307.05222,2023.1
preprintarXiv:1306.5151,2013.2,9,10
[48] ArshaNagrani,PaulHongsuckSeo,BryanSeybold,AnjaHauth, [65] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,
SantiagoManen,ChenSun,andCordeliaSchmid.Learningaudio- Marie-Anne Lachaux, Timothe´e Lacroix, Baptiste Rozie`re, Na-
videomodalitiesfromimagecaptions,2022.3 manGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,Ar-
mand Joulin, Edouard Grave, and Guillaume Lample. Llama:
[49] Maria-ElenaNilsbackandAndrewZisserman.Automatedflower
Openandefficientfoundationlanguagemodels. arXivpreprint
classificationoveralargenumberofclasses.InICVGIP,2008.2,9,
arXiv:2302.13971,2023.1,2
10
[66] MikaelaAngelinaUy,Quang-HieuPham,Binh-SonHua,Thanh
[50] TingPan,LuluTang,XinlongWang,andShiguangShan.Tokenize
Nguyen,andSai-KitYeung.Revisitingpointcloudclassification:
anythingviaprompting.arXivpreprintarXiv:2312.09128,2023.1
Anewbenchmarkdatasetandclassificationmodelonreal-world
[51] OmkarM.Parkhi,AndreaVedaldi,AndrewZisserman,andC.V.
data.InProceedingsoftheIEEE/CVFinternationalconferenceon
Jawahar.Catsanddogs.InCVPR,2012.2,9,10
computervision,pages1588–1597,2019.5
[52] HieuPham,ZihangDai,GolnazGhiasi,HanxiaoLiu,AdamsWei
[67] BastiaanSVeeling,JasperLinmans,JimWinkens,TacoCohen,and
Yu, Minh-ThangLuong, MingxingTan, andQuocVLe. Com-
MaxWelling. Rotationequivariantcnnsfordigitalpathology. In
bined scaling for zero-shot transfer learning. arXiv preprint
MICCAI,2018.2,4,9,10
arXiv:2111.10050,2021.4
[68] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing.
[53] AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh,
Learningrobustglobalrepresentationsbypenalizinglocalpredictive
Gabriel Goh, SandhiniAgarwal, Girish Sastry, Amanda Askell,
power.NeurIPS,2019.2,4,9,10
PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision.InICML,2021.1,2,3, [69] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,
9,10 YanWang,JunhuiJi,ZhuoyiYang,LeiZhao,XixuanSong,etal.
[54] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,Sharan Cogvlm: Visual expert for pretrained language models. arXiv
Narang,MichaelMatena,YanqiZhou,WeiLi,PeterJLiu,etal. preprintarXiv:2311.03079,2023.1
Exploringthelimitsoftransferlearningwithaunifiedtext-to-text [70] YiWang,YinanHe,YizhuoLi,KunchangLi,JiashuoYu,XinMa,
transformer.JMLR,2020.1
XinhaoLi,GuoChen,XinyuanChen,YaohuiWang,ConghuiHe,
[55] SamyamRajbhandari,JeffRasley,OlatunjiRuwase,andYuxiong PingLuo,ZiweiLiu,YaliWang,LiminWang,andYuQiao.Intern-
He.Zero:Memoryoptimizationstowardtrainingtrillionparameter vid:Alarge-scalevideo-textdatasetformultimodalunderstanding
models.InSC20,2020.3,9 andgeneration,2024.3
7[71] ZhirongWu,ShuranSong,AdityaKhosla,FisherYu,Linguang
Zhang,XiaoouTang,andJianxiongXiao. 3dshapenets: Adeep
representationforvolumetricshapes. InProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition,pages1912–
1920,2015.5
[72] JianxiongXiao,JamesHays,KristaAEhinger,AudeOliva,and
AntonioTorralba.Sundatabase:Large-scalescenerecognitionfrom
abbeytozoo.InCVPR,2010.2,4,9,10
[73] YangYou,JingLi,SashankReddi,JonathanHseu,SanjivKumar,
SrinadhBhojanapalli,XiaodanSong,JamesDemmel,KurtKeutzer,
andCho-JuiHsieh. Largebatchoptimizationfordeeplearning:
Trainingbertin76minutes,2019.3,9
[74] PeterYoung, AliceLai, MicahHodosh, andJuliaHockenmaier.
Fromimagedescriptionstovisualdenotations:Newsimilaritymet-
ricsforsemanticinferenceovereventdescriptions.TACL,2014.3,
4,9
[75] XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucas
Beyer.Sigmoidlossforlanguageimagepre-training.arXivpreprint
arXiv:2303.15343,2023.1,4
[76] BiaoZhangandRicoSennrich.Rootmeansquarelayernormaliza-
tion.arXivpreprintarXiv:1910.07467,2019.2
[77] JunshengZhou,JinshengWang,BaoruiMa,Yu-ShenLiu,Tiejun
Huang,andXinlongWang.Uni3d:Exploringunified3drepresenta-
tionatscale.arXivpreprintarXiv:2310.06773,2023.1,5
[78] DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedEl-
hoseiny.Minigpt-4:Enhancingvision-languageunderstandingwith
advancedlargelanguagemodels.arXivpreprintarXiv:2304.10592,
2023.1
8EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters
Supplementary Material
config EVA-CLIP-{8B,8B+}
imageenc.weightinit. EVA-8B/EVA-CLIP-8B
Dataset ClassesTestsizeEvaluationMetric
textenc.weightinit. EVA-02-CLIP-E/14+/EVA-CLIP-8B
ImageNet-1K[26] 1000 50,000 accuracy image-textdata Merged-2B
imageenc.peaklearningrate 4e-4/2e-4
ImageNet-V2[57] 1000 10,000 accuracy
imageenc.layer-wiselrdecay[22,7] 0.9/0.85
ImageNet-Adversarial[36] 1000 7,500 accuracy
textenc.peaklearningrate 4e-5/2e-5
ImageNet-R(endition)[35] 1000 30,000 accuracy textenc.layer-wiselrdecay[22,7] 0.75
ImageNet-Sketch[68] 1000 50,899 accuracy learningrateschedule cosinedecay
ObjectNet[8] 1000 50,273 accuracy optimizer LAMB[73]
CIFAR-10[40] 10 10,000 accuracy optimizerhyper-parameters β1,β2,ϵ=0.9,0.95,1e-6
weightdecay 0
CIFAR-100[40] 100 10,000 accuracy
inputresolution 2242/4482
MNIST[41] 10 10,000 accuracy patchsize 142
Caltech101[31] 101 9144 accuracy batchsize 178k/24k
SUN397[72] 397 108,754 accuracy samplesseen 9B/800M
FGVCAircraft[47] 100 3,333 accuracy dropimagepatch[44] 0.5/0.0
droppath[37] 0.0
Country-211[53] 211 21,100 accuracy
randomresizedcrop (0.9,1)
StanfordCars[39] 196 8,041 accuracy
numericalprecision DeepSpeedbf16[56]
Birdsnap[9] 500 2,195 accuracy ZeROoptimizer[55] stage3
DescribableTextures[21] 47 1,880 accuracy
EuroSAT[34] 10 27,000 accuracy Table12:EVA-CLIP-8BandEVA-CLIP-8B+trainingsettings.
FacialEmotionRecognition2013[33]8 3,574 accuracy
OxfordFlowers102[49] 102 6,149 accuracy config EVA-CLIP-{18B,18B+}
Food-101[10] 102 25,250 accuracy imageenc.weightinit. EVA-18B/EVA-CLIP-18B
GTSRB[61] 43 12,630 accuracy textenc.weightinit. EVA-02-CLIP-E/14+/EVA-CLIP-18B
PatchCamelyon[67] 2 32,768 accuracy image-textdata Merged-2B+
imageenc.peaklearningrate 4e-4/2e-4
Oxford-IIITPets[51] 37 3,669 accuracy
imageenc.layer-wiselrdecay[22,7] 0.9/0.85
RenderedSST2[53] 2 1,821 accuracy
textenc.peaklearningrate 4e-5/2e-5
RESISC45[18] 45 31,500 accuracy textenc.layer-wiselrdecay[22,7] 0.75
STL-10[23] 10 8000 accuracy learningrateschedule cosinedecay
PascalVOC2007Classification[27] 20 4,952 accuracy optimizer LAMB[73]
optimizerhyper-parameters β1,β2,ϵ=0.9,0.95,1e-6
UC-F101[60] 101 11,213 accuracy weightdecay 0
Kinetics-400[15] 400 19,240 mean(top1,top5) inputresolution 2242/3362
Kinetics-600[13] 600 29,788 mean(top1,top5) patchsize 142
batchsize 108k/23k
Kinetics-700[14] 700 33,966 mean(top1,top5)
samplesseen 6B/400M
Flickr30K[74] - 1000 recall dropimagepatch[44] 0.5/0.0
COCO[45] - 5000 recall droppath[37] 0.0
randomresizedcrop (0.9,1)
numericalprecision DeepSpeedbf16[56]
Table11:DatasetsusedtoevaluateEVA-CLIPmodels.
ZeROoptimizer[55] stage3
Table13:EVA-CLIP-18BandEVA-CLIP-18B+trainingsettings.
6.TrainingSettings
WepresentdetailedtrainingsettingsofEVA-CLIP-8B
ofthesetwoimagetransformationsinzero-shotevaluations.
andEVA-CLIP-18BinTabs.12and13.
Notably,thereexistsasignificantperformancegapbetween
thetwotransformations,observedparticularlyinzero-shot
7.ImageTransformationsforEvaluation
imageclassificationonObjectNet[8]andVOC2007[27],
Twoprevalentimagetransformationsutilizedinzero-shot andzero-shotretrievalonFlickr30K[74]andCOCO[45].
evaluationare: 1)directresizingofimagestoafixedsize, EVA-CLIP-18B shows robustness with almost the same
suchas224×224,and2)resizingimagesbasedontheshort- averageaccuracyacrossdifferentimagetransformationsin
estside,followedbycentercroppingtoachieveafixedsize. zero-shotimage/videoclassification.
InTable14,ourstudysystematicallyinvestigatestheimpact Forzero-shotimageclassificationandvideoclassification,
9method
DFN5B-CLIP-H/14+* 84.077.879.692.972.479.6 98.890.583.688.777.064.936.195.780.570.961.156.191.696.167.869.696.755.575.999.178.2 78.5
DFN5B-CLIP-H/14+† 84.378.379.393.673.373.5 98.890.583.688.977.472.537.996.080.370.961.156.191.496.267.969.696.855.575.999.181.9 78.9
EVA-CLIP-18B* 83.777.987.395.674.482.2 99.493.883.089.477.558.441.894.979.971.979.859.385.995.872.465.296.067.576.899.682.4 80.4
EVA-CLIP-18B† 83.877.786.295.774.776.2 99.493.883.089.877.759.743.194.978.472.179.859.386.095.772.365.296.167.576.999.685.8 80.4
(a) Impactofimagetransformationsonzero-shotimageclassificationperformance.Differenttransformationscansignificantlyinfluencezero-shotimage
classificationperformance,particularlyforObjectNet[8].EVA-CLIP-18Bshowsrobustnesswiththesameaveragetop-1accuracyacrossdifferentimage
transformations.
method+ #Frames UCF-101K-400K-600K-700 avg.acc.
DFN5B-CLIP-H/14+* 1 78.5 65.2 66.0 59.2 67.2
DFN5B-CLIP-H/14+† 1 79.2 66.7 67.0 60.7 68.4
EVA-CLIP-18B* 1 86.0 72.2 72.6 67.4 74.6
EVA-CLIP-18B† 1 85.6 72.9 72.9 68.2 74.9
EVA-CLIP-18B* 8 88.2 79.3 79.2 72.0 79.7
EVA-CLIP-18B† 8 87.9 79.2 79.1 72.1 79.6
(b) Impactofimagetransformsonzero-shotvideoclassificationperformance.
zero-shottextretrieval zero-shotimageretrieval
Flickr30K COCO Flickr30K COCO
method+ R@1R@5R@10 R@1R@5R@10 R@1R@5R@10 R@1R@5R@10 MR
DFN5B-CLIP-H/14+* 92.3 99.1 99.7 70.6 89.6 94.4 80.7 95.5 97.7 54.1 78.0 85.4 86.4
DFN5B-CLIP-H/14+† 93.6 99.3 99.6 71.8 90.4 94.9 82.1 96.0 97.9 55.6 79.2 86.3 87.2
EVA-CLIP-18B* 95.4 99.5 99.8 72.8 89.7 94.3 83.2 95.9 97.8 55.6 77.9 85.3 86.7
EVA-CLIP-18B† 96.7 99.7 100.0 73.6 90.9 95.0 83.3 96.3 98.3 56.2 78.5 85.6 87.8
(c) Impactofimagetransformsonzero-shotretrievalperformance.
Table14: Impactofimagetransformationsonzero-shotevaluation. †denotesthedirectresizingofimagestoafixedsize,while*
indicatesresizingimagesbasedontheshortestsideandsubsequentlycentercroppingtoachieveafixedsize.
wepresentresultsobtainedbyselectingthebest-performing
transformation between the two. In the case of zero-shot
retrievaltasks,wespecificallychoosethetransformationthat
involvesdirectresizingofimagestoafixedsize.
10
]62[K1-teNegamI ]75[2V-teNegamI
]63[.vdA-teNegamI ]53[.neR-teNegamI ]86[.ekS-teNegamI
]8[teNtcejbO ]04[01-RAFIC ]04[001-RAFIC ]14[TSINM ]13[101hcetlaC ]27[793NUS
]74[tfarcriACVGF
]35[112-yrtnuoC ]93[sraCdrofnatS ]9[pansdriB
]12[DTD
]43[TASoruE ]33[3102REF ]94[201-srewolF ]01[101-dooF ]16[BRSTG
]76[maCP ]15[steP
]35[2TSSderedneR
]81[54CSISER ]32[01-LTS ]72[7002COV .cca1-pot.gva.