CogCoM: Train Large Vision-Language Models Diving into Details through
Chain of Manipulations
JiQi1‡ MingDing2 WeihanWang1‡ YushiBai1‡ QingsongLv2 WenyiHong1‡
BinXu1 LeiHou1 JuanziLi1 YuxiaoDong1 JieTang1
qj20@mails.tsinghua.edu.cn, ming.ding@zhipuai.cn
Abstract b<latexit sha1_base64="YsAbQHKXhdd0WG03Y0D8n2fbcl8=">AAACxnicjVHLTsJAFD3UF+ILdemmkZi4alokFXZENywxyiNBQtphwIbSNu1UQ4iJP+BWP834B/oX3hlLogui07Q9c+49Z+be60a+lwjTfM9pK6tr6xv5zcLW9s7uXnH/oJ2Eacx4i4V+GHddJ+G+F/CW8ITPu1HMnanr8447uZTxzj2PEy8MbsQs4v2pMw68kcccQdS1OygPiiXTKFds27J1ArZpViSonpm12pluGaZaJWSrGRbfcIshQjCkmIIjgCDsw0FCTw8WTETE9TEnLibkqTjHIwqkTSmLU4ZD7IS+Y9r1MjagvfRMlJrRKT69MSl1nJAmpLyYsDxNV/FUOUt2mfdcecq7zejvZl5TYgXuiP1Lt8j8r07WIjBCVdXgUU2RYmR1LHNJVVfkzfUfVQlyiIiTeEjxmDBTykWfdaVJVO2yt46Kf6hMyco9y3JTfMpb0oAXU9SXg3bZsGzDuqqU6hfZqPM4wjFOaZ7nqKOBJlrkPcYzXvCqNbRAS7WH71Qtl2kO8WtpT1+KqpBd</latexit>2 Grounding(the man in black top) b<latexit sha1_base64="k/+edjw63BcBkak5pVpf4fe8LfI=">AAACxnicjVHLToNAFD3FV62vqks3xMbEFQFLkO4a3XRZo30ktWmATispBQKDpmlM/AG3+mnGP9C/8M5IE100OgQ4c+49Z+be68aBn3Jdfy8oK6tr6xvFzdLW9s7uXnn/oJ1GWeKxlhcFUdJ1nZQFfsha3OcB68YJc6ZuwDru5FLEO/csSf0ovOGzmPWnzjj0R77ncKKu3YExKFd0zazalmmrumZVddOqEbCNmgCGpstVQb6aUfkNtxgigocMUzCE4IQDOEjp6cGAjpi4PubEJYR8GWd4RIm0GWUxynCIndB3TLtezoa0F56pVHt0SkBvQkoVJ6SJKC8hLE5TZTyTzoJd5j2XnuJuM/q7udeUWI47Yv/SLTL/qxO1cIxgyxp8qimWjKjOy10y2RVxc/VHVZwcYuIEHlI8IexJ5aLPqtSksnbRW0fGP2SmYMXey3MzfIpb0oAXU1SXg/aZZliacWVW6hf5qIs4wjFOaZ7nqKOBJlrkPcYzXvCqNJRQyZSH71SlkGsO8WspT1+9YJBz</latexit>1
b<latexit sha1_base64="k/+edjw63BcBkak5pVpf4fe8LfI=">AAACxnicjVHLToNAFD3FV62vqks3xMbEFQFLkO4a3XRZo30ktWmATispBQKDpmlM/AG3+mnGP9C/8M5IE100OgQ4c+49Z+be68aBn3Jdfy8oK6tr6xvFzdLW9s7uXnn/oJ1GWeKxlhcFUdJ1nZQFfsha3OcB68YJc6ZuwDru5FLEO/csSf0ovOGzmPWnzjj0R77ncKKu3YExKFd0zazalmmrumZVddOqEbCNmgCGpstVQb6aUfkNtxgigocMUzCE4IQDOEjp6cGAjpi4PubEJYR8GWd4RIm0GWUxynCIndB3TLtezoa0F56pVHt0SkBvQkoVJ6SJKC8hLE5TZTyTzoJd5j2XnuJuM/q7udeUWI47Yv/SLTL/qxO1cIxgyxp8qimWjKjOy10y2RVxc/VHVZwcYuIEHlI8IexJ5aLPqtSksnbRW0fGP2SmYMXey3MzfIpb0oAXU1SXg/aZZliacWVW6hf5qIs4wjFOaZ7nqKOBJlrkPcYzXvCqNJRQyZSH71SlkGsO8WspT1+9YJBz</latexit>1
Vision-Language Models (VLMs) have demon-
Grounding(pillar near the man at b <latexit sha1_base64="k/+edjw63BcBkak5pVpf4fe8LfI=">AAACxnicjVHLToNAFD3FV62vqks3xMbEFQFLkO4a3XRZo30ktWmATispBQKDpmlM/AG3+mnGP9C/8M5IE100OgQ4c+49Z+be68aBn3Jdfy8oK6tr6xvFzdLW9s7uXnn/oJ1GWeKxlhcFUdJ1nZQFfsha3OcB68YJc6ZuwDru5FLEO/csSf0ovOGzmPWnzjj0R77ncKKu3YExKFd0zazalmmrumZVddOqEbCNmgCGpstVQb6aUfkNtxgigocMUzCE4IQDOEjp6cGAjpi4PubEJYR8GWd4RIm0GWUxynCIndB3TLtezoa0F56pVHt0SkBvQkoVJ6SJKC8hLE5TZTyTzoJd5j2XnuJuM/q7udeUWI47Yv/SLTL/qxO1cIxgyxp8qimWjKjOy10y2RVxc/VHVZwcYuIEHlI8IexJ5aLPqtSksnbRW0fGP2SmYMXey3MzfIpb0oAXU1SXg/aZZliacWVW6hf5qIs4wjFOaZ7nqKOBJlrkPcYzXvCqNJRQyZSH71SlkGsO8WspT1+9YJBz</latexit> 1 ) b<latexit sha1_base64="YsAbQHKXhdd0WG03Y0D8n2fbcl8=">AAACxnicjVHLTsJAFD3UF+ILdemmkZi4alokFXZENywxyiNBQtphwIbSNu1UQ4iJP+BWP834B/oX3hlLogui07Q9c+49Z+be60a+lwjTfM9pK6tr6xv5zcLW9s7uXnH/oJ2Eacx4i4V+GHddJ+G+F/CW8ITPu1HMnanr8447uZTxzj2PEy8MbsQs4v2pMw68kcccQdS1OygPiiXTKFds27J1ArZpViSonpm12pluGaZaJWSrGRbfcIshQjCkmIIjgCDsw0FCTw8WTETE9TEnLibkqTjHIwqkTSmLU4ZD7IS+Y9r1MjagvfRMlJrRKT69MSl1nJAmpLyYsDxNV/FUOUt2mfdcecq7zejvZl5TYgXuiP1Lt8j8r07WIjBCVdXgUU2RYmR1LHNJVVfkzfUfVQlyiIiTeEjxmDBTykWfdaVJVO2yt46Kf6hMyco9y3JTfMpb0oAXU9SXg3bZsGzDuqqU6hfZqPM4wjFOaZ7nqKOBJlrkPcYzXvCqNbRAS7WH71Qtl2kO8WtpT1+KqpBd</latexit>2 stratedtheirwidespreadviabilitythankstoexten-
sivetraininginaligningvisualinstructionstoan- CropZoomIn( b<latexit sha1_base64="YsAbQHKXhdd0WG03Y0D8n2fbcl8=">AAACxnicjVHLTsJAFD3UF+ILdemmkZi4alokFXZENywxyiNBQtphwIbSNu1UQ4iJP+BWP834B/oX3hlLogui07Q9c+49Z+be60a+lwjTfM9pK6tr6xv5zcLW9s7uXnH/oJ2Eacx4i4V+GHddJ+G+F/CW8ITPu1HMnanr8447uZTxzj2PEy8MbsQs4v2pMw68kcccQdS1OygPiiXTKFds27J1ArZpViSonpm12pluGaZaJWSrGRbfcIshQjCkmIIjgCDsw0FCTw8WTETE9TEnLibkqTjHIwqkTSmLU4ZD7IS+Y9r1MjagvfRMlJrRKT69MSl1nJAmpLyYsDxNV/FUOUt2mfdcecq7zejvZl5TYgXuiP1Lt8j8r07WIjBCVdXgUU2RYmR1LHNJVVfkzfUfVQlyiIiTeEjxmDBTykWfdaVJVO2yt46Kf6hMyco9y3JTfMpb0oAXU9SXg3bZsGzDuqqU6hfZqPM4wjFOaZ7nqKOBJlrkPcYzXvCqNbRAS7WH71Qtl2kO8WtpT1+KqpBd</latexit> 2 , 4 times) I<latexit sha1_base64="C08P0sL86c4e7PRYQrhFA2bf210=">AAACxnicjVHLTsJAFD3UF+ILdemmkZi4aqaKPHZEN7jDKI8ECWnLgA2lbdqphhATf8CtfprxD/QvvDOWRBdEp2l759xzzsy91w49NxaMvWe0peWV1bXsem5jc2t7J7+714qDJHJ40wm8IOrYVsw91+dN4QqPd8KIWxPb4217fCHz7XsexW7g34hpyHsTa+S7Q9exBEHXl32zny8wo1Q0WfFUZ0axUqmeyaDKWLnCdNNgahWQrkaQf8MtBgjgIMEEHD4ExR4sxPR0YYIhJKyHGWERRa7KczwiR9qEWJwYFqFj+o5o101Rn/bSM1Zqh07x6I1IqeOINAHxIorlabrKJ8pZoou8Z8pT3m1Kfzv1mhAqcEfoX7o58786WYvAEBVVg0s1hQqR1TmpS6K6Im+u/6hKkENImIwHlI8odpRy3mddaWJVu+ytpfIfiilRuXdSboJPeUsa8HyK+uKgdWKYJcO8KhZq5+moszjAIY5pnmXUUEcDTfIe4RkveNXqmq8l2sM3Vcukmn38WtrTF1YekEc=</latexit>1
swers. However,thisconclusivealignmentleads
Focus on I<latexit sha1_base64="C08P0sL86c4e7PRYQrhFA2bf210=">AAACxnicjVHLTsJAFD3UF+ILdemmkZi4aqaKPHZEN7jDKI8ECWnLgA2lbdqphhATf8CtfprxD/QvvDOWRBdEp2l759xzzsy91w49NxaMvWe0peWV1bXsem5jc2t7J7+714qDJHJ40wm8IOrYVsw91+dN4QqPd8KIWxPb4217fCHz7XsexW7g34hpyHsTa+S7Q9exBEHXl32zny8wo1Q0WfFUZ0axUqmeyaDKWLnCdNNgahWQrkaQf8MtBgjgIMEEHD4ExR4sxPR0YYIhJKyHGWERRa7KczwiR9qEWJwYFqFj+o5o101Rn/bSM1Zqh07x6I1IqeOINAHxIorlabrKJ8pZoou8Z8pT3m1Kfzv1mhAqcEfoX7o58786WYvAEBVVg0s1hQqR1TmpS6K6Im+u/6hKkENImIwHlI8odpRy3mddaWJVu+ytpfIfiilRuXdSboJPeUsa8HyK+uKgdWKYJcO8KhZq5+moszjAIY5pnmXUUEcDTfIe4RkveNXqmq8l2sM3Vcukmn38WtrTF1YekEc=</latexit> 1 to answer the question
modelstoignorecriticalvisualreasoning,andfur-
I<latexit sha1_base64="Dmpt8GEE6+k1qUxrNT+OzN28VIU=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0U3cV7QNqKcl0WoemSUgmSimCP+BWP038A/0L74xTUIvohCRnzr3nzNx7/TgQqXSc15w1N7+wuJRfLqysrq1vFDe3GmmUJYzXWRREScv3Uh6IkNelkAFvxQn3Rn7Am/7wTMWbtzxJRRReyXHMOyNvEIq+YJ4k6vK863SLJafs6GXPAteAEsyqRcUXXKOHCAwZRuAIIQkH8JDS04YLBzFxHUyISwgJHee4R4G0GWVxyvCIHdJ3QLu2YUPaK89UqxmdEtCbkNLGHmkiyksIq9NsHc+0s2J/855oT3W3Mf194zUiVuKG2L9008z/6lQtEn2c6BoE1RRrRlXHjEumu6Jubn+pSpJDTJzCPYonhJlWTvtsa02qa1e99XT8TWcqVu2Zyc3wrm5JA3Z/jnMWNA7K7lHZvTgsVU7NqPPYwS72aZ7HqKCKGurkPcAjnvBsVa3Qyqy7z1QrZzTb+Lashw+dlo/3</latexit>0 therresultinfailuresonmeticulousvisualprob-
lemsandunfaithfulresponses. Inthispaper,we
I<latexit sha1_base64="C08P0sL86c4e7PRYQrhFA2bf210=">AAACxnicjVHLTsJAFD3UF+ILdemmkZi4aqaKPHZEN7jDKI8ECWnLgA2lbdqphhATf8CtfprxD/QvvDOWRBdEp2l759xzzsy91w49NxaMvWe0peWV1bXsem5jc2t7J7+714qDJHJ40wm8IOrYVsw91+dN4QqPd8KIWxPb4217fCHz7XsexW7g34hpyHsTa+S7Q9exBEHXl32zny8wo1Q0WfFUZ0axUqmeyaDKWLnCdNNgahWQrkaQf8MtBgjgIMEEHD4ExR4sxPR0YYIhJKyHGWERRa7KczwiR9qEWJwYFqFj+o5o101Rn/bSM1Zqh07x6I1IqeOINAHxIorlabrKJ8pZoou8Z8pT3m1Kfzv1mhAqcEfoX7o58786WYvAEBVVg0s1hQqR1TmpS6K6Im+u/6hKkENImIwHlI8odpRy3mddaWJVu+ytpfIfiilRuXdSboJPeUsa8HyK+uKgdWKYJcO8KhZq5+moszjAIY5pnmXUUEcDTfIe4RkveNXqmq8l2sM3Vcukmn38WtrTF1YekEc=</latexit>1
proposeChainofManipulations,amechanism I<latexit sha1_base64="Dmpt8GEE6+k1qUxrNT+OzN28VIU=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0U3cV7QNqKcl0WoemSUgmSimCP+BWP038A/0L74xTUIvohCRnzr3nzNx7/TgQqXSc15w1N7+wuJRfLqysrq1vFDe3GmmUJYzXWRREScv3Uh6IkNelkAFvxQn3Rn7Am/7wTMWbtzxJRRReyXHMOyNvEIq+YJ4k6vK863SLJafs6GXPAteAEsyqRcUXXKOHCAwZRuAIIQkH8JDS04YLBzFxHUyISwgJHee4R4G0GWVxyvCIHdJ3QLu2YUPaK89UqxmdEtCbkNLGHmkiyksIq9NsHc+0s2J/855oT3W3Mf194zUiVuKG2L9008z/6lQtEn2c6BoE1RRrRlXHjEumu6Jubn+pSpJDTJzCPYonhJlWTvtsa02qa1e99XT8TWcqVu2Zyc3wrm5JA3Z/jnMWNA7K7lHZvTgsVU7NqPPYwS72aZ7HqKCKGurkPcAjnvBsVa3Qyqy7z1QrZzTb+Lashw+dlo/3</latexit>0 Focus on I<latexit sha1_base64="Dmpt8GEE6+k1qUxrNT+OzN28VIU=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0U3cV7QNqKcl0WoemSUgmSimCP+BWP038A/0L74xTUIvohCRnzr3nzNx7/TgQqXSc15w1N7+wuJRfLqysrq1vFDe3GmmUJYzXWRREScv3Uh6IkNelkAFvxQn3Rn7Am/7wTMWbtzxJRRReyXHMOyNvEIq+YJ4k6vK863SLJafs6GXPAteAEsyqRcUXXKOHCAwZRuAIIQkH8JDS04YLBzFxHUyISwgJHee4R4G0GWVxyvCIHdJ3QLu2YUPaK89UqxmdEtCbkNLGHmkiyksIq9NsHc+0s2J/855oT3W3Mf194zUiVuKG2L9008z/6lQtEn2c6BoE1RRrRlXHjEumu6Jubn+pSpJDTJzCPYonhJlWTvtsa02qa1e99XT8TWcqVu2Zyc3wrm5JA3Z/jnMWNA7K7lHZvTgsVU7NqPPYwS72aZ7HqKCKGurkPcAjnvBsVa3Qyqy7z1QrZzTb+Lashw+dlo/3</latexit> 0 to answer the question
thatenablesVLMstosolveproblemswithase-
riesofmanipulations,whereeachmanipulation
referstoanoperationonthevisualinput,either
fromintrinsicabilities(e.g.,grounding)acquired Figure1.In comparison with existing vision-language models,
CogCoMperformsthemultiplestepsofevidentialreasoningwith
throughpriortrainingorfromimitatinghuman-
chainofmanipulations(CoM)toachievethefinalanswer.
like behaviors(e.g., zoomin). Thismechanism
encouragesVLMstogeneratefaithfulresponses
withevidentialvisualreasoning,andpermitsusers
totraceerrorcausesintheinterpretablepaths.We TheresearchemployingVLMsasfoundationmodels(Bai
thustrainCogCoM,ageneral17BVLMwitha etal.,2023;Sunetal.,2023b;Wangetal.,2023b)usually
memory-basedcompatiblearchitectureendowed involvestwomainstagesoftraining,wherethefirststage
this reasoning mechanism. Experiments show cultivatesintrinsicvisualunderstandingthroughexposureto
that our model achieves the state-of-the-art per- massiveimage-captionpairs,andthesecondstageendows
formanceacross8benchmarksfrom3categories, the models with problem-solving capabilities through an
and a limited number of training steps with the instruction tuning. Some other studies (Dai et al., 2023;
dataswiftlygainsacompetitiveperformance.The Chenetal.,2023b;Zhangetal.,2023b)directlyperform
codeanddataarepubliclyavailableatthisurl. thesecondstagefortheapplicablescenes.
However,existingtuningmethodstrainmodelstorespond
toinstructionswithconclusivelinguisticanswersuponvi-
1.Introduction
sualinputs,whichleadsmodelstoignoretheessentialvisual
reasoning and further results in failures in meticulous vi-
BenefitingfromtheadvantageofLargeLanguageModels
sualproblems,unfaithfulresponses,andevenhallucinations.
(LLMs)inbroadworldknowledge,largeVisionLanguage
ForexampleinFigure1,wetestthetopperformingmodel
Models(VLMs)(Alayracetal.,2022;Wangetal.,2023b)
CogVLM(Wangetal.,2023b)aboutthedetailsintheimage
that are further trained to understand vision have demon-
(i.e.,textswrittenonpillar),anditdirectlygivesanincor-
stratedviabilitiesonbroadscenarios,suchasvisualquestion
rectanswer(i.e.,NOSMOKING),mostlikelyfrombiasto
answering(Liuetal.,2023b),visualgrounding(Pengetal.,
visualorlinguisticpriors(i.e.,typicalsceneswithpillarin
2023),opticalcharacterrecognition(Zhangetal.,2023b).
office). Theabsenceofthisevidentialreasoningwithvisual
1TsinghuaUniversity2ZhipuAI‡DoneasinternatZhipuAI. evidenceleadstoarashresponse(Hwangetal.,2023).
Correspondenceto:BinXu<xubin@tsinghua.edu.cn>,JieTang
<jietang@tsinghua.edu.cn>. Humanssolvethemeticulousvisualproblemsbymarking
orprocessingthegivenimagesforconvenienceandrigor,
1
4202
beF
6
]VC.sc[
1v63240.2042:viXra
Q:
in
black
top?
What
is written
on
the
pillar
in
front
of
the
man
CogCOM
VLM
A:
QUICK
DEPOSIT.
SMOKING
The
letters
written
on
the
pillar
are
NOwhichwerefertoasmanipulations. Forexample,wemay proposedreasoningexaminationbenchmarkAutoCoM-test.
findtargetsbysequentiallylocatingreferences,andconcen- The results show that our model achieves the superior or
trateonsubtledetailsbyzoominginacorrespondingregion. competitiveperformanceacrosstheboard. Andtheinvesti-
MostoftheVLMshavedevelopednumerousintrinsicmulti- gationontheproposedtestbedshowsthatCogCoMswiftly
modalcapabilities(e.g.,groundingboxes,recognizingtexts) achievescompetitiveperformancewithafewtrainingsteps
duringthefirststageoftraining. Byfurtherimitatingthe byincorporatingtheproducedreasoningchains.
foundationalhuman-likebehaviours(e.g.,cropping,zoom
in),modelshavethepotentialtoperformevidentialvisual
2.Methodology
reasoningforsolvingproblems.
Inthissection,weintroducethemethodologyofCogCoM.
Therearetwomajorchallengesthatpreventusfromtraining
WefirstintroducetheterminologydefinitionrelatedtoCoM
modelswiththismechanism.First,itiscrucialtoeffectively
in Section 2.1. In Section 2.2, we illustrate the data pro-
produce abundant training data with the evidential visual
ductionprocess(Figure2left),whichinvolvesalinguistic
reasoningpathsfromexistinglinguisticinstruction-answer
annotatortoprovidemanipulations-assistedsolvingsteps,
pairs. Second,itisdifficulttobuildageneralmechanism
thevisualannotatorstosupplythemanipulationsreturns,
with various manipulations, to train VLMs of dedicated
andthefinaltraversaltogatherviablepaths. InSection2.3,
architectureswhilepreservingtheirpresetcapabilities.
weillustratemodeltraining(Figure2right),wherewecon-
Inthispaper,weproposeChainofManipulations(CoM), duct a compatible devising to train a general multi-turns
a general mechanism that enables VLMs to perform ev- VLMwithchainofmanipulations.
idential visual reasoning with a series of manipulations,
where each manipulation is applied to the visual input to 2.1.Terminology
acquiredifferenttypesofvisualcontents(e.g.,boxes,texts,
We first introduce the formal definition of the terminolo-
images). Wefirstconstructanautomaticdataproduction
giesanddatastructureforanunambiguousunderstanding.
framework based on existing image-question-answer cor-
pus. Alinguisticannotator1 whoisgrantedtoutilizeaset Wedefinethemanipulationsasaflexiblesetthatincludes
functions from a foundational predefined set, as well as
ofmanipulationsisengagedtoprovidereasoningstepsfor
functionsdevisedbymodelsthemselvesduringinference
agivenquestion,andfundamentalvisualtoolsarefurther
to accommodate in-context learning. We thus predefine
employedtoacquirethecorrespondingreturnsrequestedby
the a set of manipulations functions that VLMs can de-
themanipulations.Atraversalonthetreebranchedbypossi-
velop, either from prior training or by imitating human
blereturnsofthemanipulationsisthenperformed,toobtain
behaviors: Grounding(tgt) bbx, OCR(tgt)
thefinalfeasiblepathsthatleadtothecorrectanswerbythe
M ⊆{ → →
txt, Calculate(tgt) num, Counting(tgt) num,
lastmanipulationreturns. Basedontheproduceddata,we
→ →
CropZoomIn(bbx,x) img , where the parameters or
presentCogCoM,a17BVLMtrainedwithamemory-based
→ }
returns tgt,bbx,txt,num,x,img refer to the bounding
compatiblearchitectureandafusionoffourcategoriesof
boxes,zoomratio,image,targetdescription,number,and
data,todevelopthegeneralandreasoningmultimodalcapa-
text,respectively.
bilities. Themodelperformsreasoningbyactivelyadopting
multiplemanipulationstoacquirevisualcontents(e.g.,refer- GivenalanguagequestionQuponaninitialinputimage
entialregionsbbx 1,bbx 2,andnewimageimg 1),andfinally I 0,ageneralvision-languagemodelequippedwithChain
achievestheconclusiveanswer. Inaddition,duetothelack of Manipulations (CoM) ς solves the problem to achieve
ofevaluationresources,wefurtherintroduceatestbedwith thecorrespondinganswerasVLM(ς I ,Q) A,whereς
0
meticulousvisualproblemsinvolvingreasoningpaths,and referstoachainofevidentialreason| ingste⇒ ps2,
akeypoints-awaremetrictoinvestigatethecorrectnessof
ς =(step ,step ,...)
thefinalansweraswellasthesolvingprocess. 1 2
(1)
step =(f ,desc ), f
Weconductextensiveexperimentson8benchmarksfrom3 i i i i ∈M
categories of capabilities, including detailed visual ques-
wheref referstoainstantiatedmanipulationfunctionfrom
tion answering with TextVQA (Singh et al., 2019), ST- i
a set of manipulations definition , and desc refers to
VQA(Bitenetal.,2019),TallyVQA(Acharyaetal.,2019), M i
alanguagedescriptionincludingtheexecutionofthema-
and GQA (Hudson & Manning, 2019), visual grounding
nipulation. Thisdefinitionexplicitlydeclaresthesymbolic
with RefCOCO (Yu et al., 2016), RefCOCO+(Yu et al.,
executionofthemanipulationsf ,andalsocompatiblewith
2016),andRefCOCOg(Maoetal.,2016),andthehalluci- i
theexistingfree-forminstruction-answerdatastructurewith
nationvalidationwithPOPE(Lietal.,2023d),andalsoa
thelinguisticdescriptionsdesc .
i
1ALLMprependedwithtaskrequirementsisusedastheanno-
2Weusethesymbolς (i.e.,sigma)todenotetheCoMchain
tator,whoprovidessolvingstepswithoutknowinganswers.
throughoutthispaperforconvenience.
2Data Production Model Training
<latexit sha1_base64="luFztvg4pkmUxMB01yKJs5viirE=">AAACznicjVHLSsNAFD2Nr/quunQTLIKrkoioy6Ibly3YB9Qik+m0DebFZFIopbj1B9zqZ4l/oH/hnTEFtYhOSHLm3HPuzL3XSwI/VY7zWrAWFpeWV4qra+sbm1vbpZ3dZhpnkosGj4NYtj2WisCPREP5KhDtRAoWeoFoeXeXOt4aCZn6cXStxonohmwQ+X2fM0VU5yZkashZMKlPb0tlp+KYZc8DNwdl5KsWl15wgx5icGQIIRBBEQ7AkNLTgQsHCXFdTIiThHwTF5hijbwZqQQpGLF39B3QrpOzEe11ztS4OZ0S0CvJaeOQPDHpJGF9mm3imcms2d9yT0xOfbcx/b08V0iswpDYv3wz5X99uhaFPs5NDT7VlBhGV8fzLJnpir65/aUqRRkS4jTuUVwS5sY567NtPKmpXfeWmfibUWpW73muzfCub0kDdn+Ocx40jyvuacWtn5SrF/moi9jHAY5onmeo4go1NEzHH/GEZ6tmjaypdf8ptQq5Zw/flvXwAY5Ck+4=</latexit> : What is written on the pillar in front of the man in black top? <latexit sha1_base64="Lz9T70LZH4j8t/WotJtOLW80Cn4=">AAACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl147KCrYW2yGQ6rcG8mEwKpRS3/oBb/SzxD/QvvDOmoBbRCUnOnHvOnbn3ekngp8pxXgvW3PzC4lJxubSyura+Ud7caqZxJrlo8DiIZctjqQj8SDSUrwLRSqRgoReIa+/uXMevh0KmfhxdqVEiuiEbRH7f50wR1e6ETN1yFoxPJzflilN1zLJngZuDCvJVj8sv6KCHGBwZQghEUIQDMKT0tOHCQUJcF2PiJCHfxAUmKJE3I5UgBSP2jr4D2rVzNqK9zpkaN6dTAnolOW3skScmnSSsT7NNPDOZNftb7rHJqe82or+X5wqJVbgl9i/fVPlfn65FoY8TU4NPNSWG0dXxPEtmuqJvbn+pSlGGhDiNexSXhLlxTvtsG09qate9ZSb+ZpSa1XueazO861vSgN2f45wFzYOqe1R1Lw8rtbN81EXsYBf7NM9j1HCBOhqm4494wrNVt4bWxLr/lFqF3LONb8t6+ABoMpPe</latexit> : QUICK DEPOSIT <latexit sha1_base64="KN0S1bKjKmsgCuZK2vWtgtEr8eA=">AAACznicjVHLSsNAFD3GV62vqks3wSK4KonYpu6KbtwIFawW2iKTcWyDeZFMCqUUt/6AW/0s8Q/0L7wzTUEXRSckOXPuOXfm3uvGvpdKy/pYMBaXlldWC2vF9Y3Nre3Szu5NGmUJFy0e+VHSdlkqfC8ULelJX7TjRLDA9cWt+3iu4rdDkaReFF7LUSx6AeuH3oPHmSSq0w2YHHDmjy8nd6WyVak5du24bmrgVE+moHrqmHbF0quMfDWj0ju6uEcEjgwBBEJIwj4YUno6sGEhJq6HMXEJIU/HBSYokjcjlSAFI/aRvn3adXI2pL3KmWo3p1N8ehNymjgkT0S6hLA6zdTxTGdW7LzcY51T3W1EfzfPFRArMSD2L99M+V+fqkXiAXVdg0c1xZpR1fE8S6a7om5u/qhKUoaYOIXvKZ4Q5to567OpPamuXfWW6finVipW7XmuzfClbkkDnk3RnA9ujit2rWJfnZQbZ/moC9jHAY5ong4auEATLd3xF7zizWgaQ2NiPE2lxkLu2cOvZTx/A1+/lEk=</latexit> : Predefined Manipulations <latexit sha1_base64="mJeJclI8TdpbSWcC7W9vjITcNvc=">AAAC0HicjVHLSsNAFD2Nr/quunQTLIKrkoioy6Ib3VWxD6ilTKbTNpiXyUQspYhbf8CtfpX4B/oX3hlTUIvohCRnzr3nzNx7nchzE2lZrzljanpmdi4/v7C4tLyyWlhbryVhGnNR5aEXxg2HJcJzA1GVrvREI4oF8x1P1J2rYxWv34g4ccPgQg4i0fJZL3C7LmeSqNalz2SfM294OmrLdqFolSy9zElgZ6CIbFXCwgsu0UEIjhQ+BAJIwh4YEnqasGEhIq6FIXExIVfHBUZYIG1KWYIyGLFX9O3RrpmxAe2VZ6LVnE7x6I1JaWKbNCHlxYTVaaaOp9pZsb95D7WnutuA/k7m5RMr0Sf2L9048786VYtEF4e6BpdqijSjquOZS6q7om5ufqlKkkNEnMIdiseEuVaO+2xqTaJrV71lOv6mMxWr9jzLTfGubkkDtn+OcxLUdkv2fsk+2yuWj7JR57GJLezQPA9QxgkqqJL3NR7xhGfj3Lg17oz7z1Qjl2k28G0ZDx/Fb5TN</latexit> It
Q A M
<latexit sha1_base64="/KcGymOf6Y2mVdHFOKgNt1vAXiY=">AAAC0HicjVHLSsNAFD2Nr/quunQTLIKrkoioy6Ib3VWxD6ilTKbTNpiXyUQspYhbf8CtfpX4B/oX3hlTUIvohCRnzr3nzNx7nchzE2lZrzljanpmdi4/v7C4tLyyWlhbryVhGnNR5aEXxg2HJcJzA1GVrvREI4oF8x1P1J2rYxWv34g4ccPgQg4i0fJZL3C7LmeSqNalz2SfM294Ompb7ULRKll6mZPAzkAR2aqEhRdcooMQHCl8CASQhD0wJPQ0YcNCRFwLQ+JiQq6OC4ywQNqUsgRlMGKv6NujXTNjA9orz0SrOZ3i0RuT0sQ2aULKiwmr00wdT7WzYn/zHmpPdbcB/Z3MyydWok/sX7px5n91qhaJLg51DS7VFGlGVcczl1R3Rd3c/FKVJIeIOIU7FI8Jc60c99nUmkTXrnrLdPxNZypW7XmWm+Jd3ZIGbP8c5ySo7Zbs/ZJ9tlcsH2WjzmMTW9iheR6gjBNUUCXvazziCc/GuXFr3Bn3n6lGLtNs4NsyHj4AI++UiQ==</latexit> I0 b<latexit sha1_base64="HfMht1uhZfNRE3AH4G7DyqFOHtw=">AAACyHicjVHLSsNAFD3GV31XXboJFsFVmNQ2tbuiG3GlYFXQUpJ0WgfzIpmopbjxB9zql4l/oH/hnTEFXRSdkOTOueecmXuvlwQik4y9TxnTM7Nz86WFxaXlldW18vrGeRbnqc/bfhzE6aXnZjwQEW9LIQN+maTcDb2AX3i3hyp/ccfTTMTRmRwmvBO6g0j0he9Kgtqe99CtdssVZtnNusOYyaxq06ntORQ0mozV66ZtMb0qKNZJXH7DNXqI4SNHCI4IkuIALjJ6rmCDISGsgxFhKUVC5zkesUjanFicGC6ht/Qd0O6qQCPaK89Mq306JaA3JaWJHdLExEspVqeZOp9rZ4VO8h5pT3W3If29wiskVOKG0L90Y+Z/daoWiT72dQ2Cako0oqrzC5dcd0Xd3PxRlSSHhDAV9yifUuxr5bjPptZkunbVW1fnPzRToWrvF9wcn+qWNODxFM3JwXnVsh3LPq1VWgfFqEvYwjZ2aZ4NtHCEE7TJW+AZL3g1jo3EuDeG31RjqtBs4tcynr4A48ORTQ==</latexit>bx2 b<latexit sha1_base64="HfMht1uhZfNRE3AH4G7DyqFOHtw=">AAACyHicjVHLSsNAFD3GV31XXboJFsFVmNQ2tbuiG3GlYFXQUpJ0WgfzIpmopbjxB9zql4l/oH/hnTEFXRSdkOTOueecmXuvlwQik4y9TxnTM7Nz86WFxaXlldW18vrGeRbnqc/bfhzE6aXnZjwQEW9LIQN+maTcDb2AX3i3hyp/ccfTTMTRmRwmvBO6g0j0he9Kgtqe99CtdssVZtnNusOYyaxq06ntORQ0mozV66ZtMb0qKNZJXH7DNXqI4SNHCI4IkuIALjJ6rmCDISGsgxFhKUVC5zkesUjanFicGC6ht/Qd0O6qQCPaK89Mq306JaA3JaWJHdLExEspVqeZOp9rZ4VO8h5pT3W3If29wiskVOKG0L90Y+Z/daoWiT72dQ2Cako0oqrzC5dcd0Xd3PxRlSSHhDAV9yifUuxr5bjPptZkunbVW1fnPzRToWrvF9wcn+qWNODxFM3JwXnVsh3LPq1VWgfFqEvYwjZ2aZ4NtHCEE7TJW+AZL3g1jo3EuDeG31RjqtBs4tcynr4A48ORTQ==</latexit>bx2 G C<latexit sha1_base64="5GIlD1frWH3idT/Eewp1XutvIGM=">AAAC7HicjVHLTttAFD0YaHm1DbBkMyKqoJvIdiLaJaILWIJEIFISRbYzOKM4Hms8bhNFfAI7dhVbfqDb8h2IP4C/4M7gSHSB2rFsnzn3njNz7w2zROTadR/mnPmFxXfvl5ZXVtc+fPxUWd84y2WhIt6MZCJVKwxynoiUN7XQCW9ligejMOHn4fC7iZ//4CoXMj3Vk4x3R0GcigsRBZqoXmWno/mYdNNDJYu0L9L4clfH+gvrKBEPdKCU/MnCcNyrVN1ao+H79TqzoO7tvQDf85lXc+2qolzHsnKPDvqQiFBgBI4UmnCCADk9bXhwkRHXxZQ4RUjYOMclVkhbUBanjIDYIX1j2rVLNqW98cytOqJTEnoVKRk+k0ZSniJsTmM2Xlhnw77lPbWe5m4T+oel14hYjQGx/9LNMv9XZ2rRuMA3W4OgmjLLmOqi0qWwXTE3Z6+q0uSQEWdwn+KKcGSVsz4zq8lt7aa3gY0/2kzDmn1U5hZ4MrekAc+myN4GZ37N26t5J43q/kE56iVsYRu7NM+v2McRjtEk7yv8xh/cOalz7fxybl5SnblSs4m/lnP7DLpmoFE=</latexit> <latexit sha1_base64="glNiHPts8Mly1mjsEsAIuQV0Gis=">AAAC63icjVHLTsMwEBzCq7wLHLlYVCC4VClqC9wquHAEiQISRSgJJlgkceQ4PFTxB9y4Ia78AFf4D8QfwF+wNqkEBwSOkqxnZ8beXT+NRKZd963P6R8YHBoujYyOjU9MTpWnZ/YymauAtwMZSXXgexmPRMLbWuiIH6SKe7Ef8X3/fNPk9y+4yoRMdvV1yo9iL0zEqQg8TdBxebGj+RXpupsyT7RIwpslHepl1lEiPNOeUvKSJXl8XK641Xqzsd5cYyZouo0G+0Lqq6xWde2qoFjbsvyKDk4gESBHDI4EmuIIHjJ6DlGDi5SwI3QJUxQJm+e4wShpc2JxYniEntM3pN1hgSa0N56ZVQd0SkSvIiXDAmkk8RTF5jRm87l1Nuhv3l3rae52TX+/8IoJ1Tgj9C9dj/lfnalF4xRrtgZBNaUWMdUFhUtuu2Juzr5VpckhJczEJ5RXFAdW2eszs5rM1m5669n8u2Ua1OyDgpvjw9ySBtybIvs92Fup1prV2k690tooRl3CHOaxRPNcRQtb2EabvG/xhGe8OLFz59w7D19Up6/QzOLHch4/ASwYoCM=</latexit> oro uu nn tid ni gn (g t( gt tg )t) !! nb ub mx
b<latexit sha1_base64="RWWYa5PFtCXkZDtvP4QDF3c3RAQ=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZI+YpdFN+Kqgn1ALSVJp3UwTUIyUUtx4w+41S8T/0D/wjtjCrooOiHJnXPPOTP3XjfyeSJM8z2nLS2vrK7l1wsbm1vbO8XdvXYSprHHWl7oh3HXdRLm84C1BBc+60Yxcyauzzru7ZnMd+5YnPAwuBLTiPUnzjjgI+45gqCW6z4MrEGxZBpWza6Uq7pp1GyrbFYoqFcrlbKtW4apVgnZaobFN1xjiBAeUkzAEEBQ7MNBQk8PFkxEhPUxIyymiKs8wyMKpE2JxYjhEHpL3zHtehka0F56Jkrt0Sk+vTEpdRyRJiReTLE8TVf5VDlLdJH3THnKu03p72ZeE0IFbgj9Szdn/lcnaxEYoa5q4FRTpBBZnZe5pKor8ub6j6oEOUSEyXhI+ZhiTynnfdaVJlG1y946Kv+hmBKVey/jpviUt6QBz6eoLw7aZcOyDeuyWmqcZqPO4wCHOKZ5nqCBczTRIm+OZ7zgVbvQIu1em35TtVym2cevpT19AcNgkT8=</latexit>bx1 b<latexit sha1_base64="RWWYa5PFtCXkZDtvP4QDF3c3RAQ=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZI+YpdFN+Kqgn1ALSVJp3UwTUIyUUtx4w+41S8T/0D/wjtjCrooOiHJnXPPOTP3XjfyeSJM8z2nLS2vrK7l1wsbm1vbO8XdvXYSprHHWl7oh3HXdRLm84C1BBc+60Yxcyauzzru7ZnMd+5YnPAwuBLTiPUnzjjgI+45gqCW6z4MrEGxZBpWza6Uq7pp1GyrbFYoqFcrlbKtW4apVgnZaobFN1xjiBAeUkzAEEBQ7MNBQk8PFkxEhPUxIyymiKs8wyMKpE2JxYjhEHpL3zHtehka0F56Jkrt0Sk+vTEpdRyRJiReTLE8TVf5VDlLdJH3THnKu03p72ZeE0IFbgj9Szdn/lcnaxEYoa5q4FRTpBBZnZe5pKor8ub6j6oEOUSEyXhI+ZhiTynnfdaVJlG1y946Kv+hmBKVey/jpviUt6QBz6eoLw7aZcOyDeuyWmqcZqPO4wCHOKZ5nqCBczTRIm+OZ7zgVbvQIu1em35TtVym2cevpT19AcNgkT8=</latexit>bx1 OC <<l la at te ex xi it t s sh ha a1 1_ _b ba as se e6 64 4= =" "m 8g Cp K/ F0 LZ +l 3a gm E0 mv 0N u9 b2 fe p1 4O YB z2 C1 jU 6L LM vL Mb /RA O= E" => "A >A AA AC A8 CH 5i nc ij cV jH VL HT Lt Tt sA JF AD F1 Dx 3X Uz NS 7U 5N Qm l2 2X 63 qY xw Aa QI 3V pE BV AR CT LZ o1 1m un 3S PHY mF IN C2 AI kB QG 0C pS Yh BC Jy pn SY 2k mz Ui 5u 92 hx Lx hu 2M 52 8K 6O 4I 9j Qu fu cu 6u qY ct Ys /f 06 DL /Z w8 jB te jI SP X4 RC B+ d4M Jz qh 2S ZW 8U 6T 9t 5W 8L zb cP en 5H 3v QP 4m 5b Gn 03 rh Pn ek Ui MC Tu U2 96 MV zz sX 3n vw 5c BN eH Xj F5 p8 es WP ca 20 sv rP V1 9t5 E/ Qq SL xx c8 Vt nV UB DI LU xs BV 18 xV 44 6k YE x6 3k 1O Ww l6 VD xg 6i rc Bh 44 KT Zw gu 8d d8 jM 9N Wc c8 wS aA GN KE 19 y4 6P ZJ i9 Hsm j3 gv n/ 8K uV bS kF Lk Wt Gq d9 sP /c n3 X6 ec 7B an kE qm hR 2i ZI qK sN pF 2E Tn Xj pf Rc sD ez Ha Ze 7k cm 52 20 Zr Om 7X Z+ lR PM wt X3J lz /t ab QC gc RL Xr pO rp yu W/ 7Y UQ zI Wl y4 pr eA KO pl X5 KD xc Zm G0 pv Qi sk R0 JX QR Jb lv Dt If W1 3P p/ ln kM ag yn Tb In Pw Ox G3w JO j2 o3 Im 4t CV Ly G7 Em Aq wj +W Jr Gm Ex Pc NY io JA 6h GJ iC jK AU QS kM hG cR CQ yR PN iO BE CK GC ug 45 ww yg 3e SX JO MT 2E pH iWN xG Gn GC TI ak xb AJ /z rj 2D aH NX dS Il Wp JT /F 2K yS jM Pg Sd ak pL df Om 8H eZ gH VF pZ Dv SR x3 Qn 5g qW AV 8h g3 RR hK dQ Zm q9 pi 4pQ 7M Fq 26 VS uR wl kK 7c 5I Hm 29 VO HY ej 7Z of bW +2 Tb eC AL 2v JG lf eW g0 T9 +z 5t dl uP n5 Ph l5 fZ nU aS pq Fz oE om s9 9l X+ Q6 Oe neb m/ k6 Lk Nw qt OG ri cN x0 Cb XQ W2 XC Va Es 3o Nt 3Y 96 Uq JL cK kp hf JS Ed 7s hX Dc sn UP F1 YV Vl cS ra xH 3n 0D 2i tD Sh bx SR tX ahC vO er 2n jP re +Z oW TU M1 Vh qa /z Ze u9 kD hW vz j8 U2 9m 2Y Sa B1 ju yy ej oK jr kf ZE Xj Bb Tk zl hD XX Kg ++ cR Fb rY KY 7H hG 8y k0 ovI 58 7t Hb J8 r9 av Rb om 35 lV Wo s1 I7 8C jG n7 Kz BF KG 3s n2 dz 4g x0 g1 t8 ex ji b5 56 x5 bP z0 wd Yv j/ 9E +H pF R4 i5 ry Rf bj Og D/ Xnf MO p7 6V +K Ad EW Ta lV n7 Xj o3 =n <J /+ l3 aQ tI eb x1 i6 tG >Q</latexit> r Co RpZ (to go tm )I !n(b txbx t,x) !img ViT Encoder❄ M<latexit sha1_base64="BjPDv/IoisN4jPySDXrxkBnEZqQ=">AAACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040aoYGuhLTKZTmswLyaTQinFrT/gVj9L/AP9C++MKahFdEKSM+eec2fuvV4S+KlynNeCNTe/sLhUXC6trK6tb5Q3t5ppnEkuGjwOYtnyWCoCPxIN5atAtBIpWOgF4tq7O9Px66GQqR9HV2qUiG7IBpHf9zlTRLU7IVO3nAXji8lNueJUHbPsWeDmoIJ81ePyCzroIQZHhhACERThAAwpPW24cJAQ18WYOEnIN3GBCUrkzUglSMGIvaPvgHbtnI1or3Omxs3plIBeSU4be+SJSScJ69NsE89MZs3+lntscuq7jejv5blCYhVuif3LN1X+16drUejjxNTgU02JYXR1PM+Sma7om9tfqlKUISFO4x7FJWFunNM+28aTmtp1b5mJvxmlZvWe59oM7/qWNGD35zhnQfOg6h5V3cvDSu00H3URO9jFPs3zGDWco46G6fgjnvBs1a2hNbHuP6VWIfds49uyHj4AhL6T6g==</latexit> + Q<latexit sha1_base64="luFztvg4pkmUxMB01yKJs5viirE=">AAACznicjVHLSsNAFD2Nr/quunQTLIKrkoioy6Ibly3YB9Qik+m0DebFZFIopbj1B9zqZ4l/oH/hnTEFtYhOSHLm3HPuzL3XSwI/VY7zWrAWFpeWV4qra+sbm1vbpZ3dZhpnkosGj4NYtj2WisCPREP5KhDtRAoWeoFoeXeXOt4aCZn6cXStxonohmwQ+X2fM0VU5yZkashZMKlPb0tlp+KYZc8DNwdl5KsWl15wgx5icGQIIRBBEQ7AkNLTgQsHCXFdTIiThHwTF5hijbwZqQQpGLF39B3QrpOzEe11ztS4OZ0S0CvJaeOQPDHpJGF9mm3imcms2d9yT0xOfbcx/b08V0iswpDYv3wz5X99uhaFPs5NDT7VlBhGV8fzLJnpir65/aUqRRkS4jTuUVwS5sY567NtPKmpXfeWmfibUWpW73muzfCub0kDdn+Ocx40jyvuacWtn5SrF/moi9jHAY5onmeo4go1NEzHH/GEZ6tmjaypdf8ptQq5Zw/flvXwAY5Ck+4=</latexit>
b<latexit sha1_base64="3BVUemwIkNR4isyNX94YODCAX+0=">AAACzHicjVHLTsMwEBzCq5RXgSOXiAqJU0ha+uCG4AIXBBJtQVBVSWrAIi85DqKquPIDXOG7EH8Af8HapBIcKnCUZD07M/bueknAU2nb7xPG5NT0zGxhrji/sLi0XFpZbadxJnzW8uMgFueem7KAR6wluQzYeSKYG3oB63h3ByrfuWci5XF0JgcJ64buTcSvue9Kgi4876FX3T7qOb1S2bZ267VmtWLaltOoOpUmBdWG3ajtmI5l61VGvk7i0huu0EcMHxlCMESQFAdwkdJzCQc2EsK6GBImKOI6z/CIImkzYjFiuITe0feGdpc5GtFeeaZa7dMpAb2ClCY2SRMTT1CsTjN1PtPOCh3nPdSe6m4D+nu5V0ioxC2hf+lGzP/qVC0S12jqGjjVlGhEVefnLpnuirq5+aMqSQ4JYSruU15Q7GvlqM+m1qS6dtVbV+c/NFOhau/n3Ayf6pY04NEUzfFBu2I5dcs53Snv7eejLmAdG9iieTawh0OcoEXeIZ7xglfj2JDG0Hj8phoTuWYNv5bx9AUC1ZKC</latexit>bx3/I1
t<latexit sha1_base64="3uQ8QX2fue98Q2WsirfBsNLdvpE=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0I64qmLagpSTTaR2aJmEyUUtx4w+41S8T/0D/wjtjCmoRnZDkzLn3nJl7b5CEIlWO81qwZmbn5heKi6Wl5ZXVtfL6RiONM8m4x+Iwlq3AT3koIu4poULeSiT3h0HIm8HgRMebN1ymIo4u1Cjh7aHfj0RPMF8R5ak71XE75YpTdcyyp4GbgwryVY/LL7hCFzEYMgzBEUERDuEjpecSLhwkxLUxJk4SEibOcY8SaTPK4pThEzugb592lzkb0V57pkbN6JSQXklKGzukiSlPEtan2SaeGWfN/uY9Np76biP6B7nXkFiFa2L/0k0y/6vTtSj0cGRqEFRTYhhdHctdMtMVfXP7S1WKHBLiNO5SXBJmRjnps200qald99Y38TeTqVm9Z3luhnd9Sxqw+3Oc06CxV3UPqu75fqV2nI+6iC1sY5fmeYgaTlGHR94Cj3jCs3VmJdatNfpMtQq5ZhPflvXwAYXbkSM=</latexit>xt1
Adapter + Embedding❄
n-th LM Block ❄
Layer Norm
MLP
G q m crue eaen anse tipt er ia o u dt n l ae o, t ns ib o eo y n sl v so .i pn frtg oio ms nt ae M <latexit sha1_base64="BjPDv/IoisN4jPySDXrxkBnEZqQ=">AAACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040aoYGuhLTKZTmswLyaTQinFrT/gVj9L/AP9C++MKahFdEKSM+eec2fuvV4S+KlynNeCNTe/sLhUXC6trK6tb5Q3t5ppnEkuGjwOYtnyWCoCPxIN5atAtBIpWOgF4tq7O9Px66GQqR9HV2qUiG7IBpHf9zlTRLU7IVO3nAXji8lNueJUHbPsWeDmoIJ81ePyCzroIQZHhhACERThAAwpPW24cJAQ18WYOEnIN3GBCUrkzUglSMGIvaPvgHbtnI1or3Omxs3plIBeSU4be+SJSScJ69NsE89MZs3+lntscuq7jejv5blCYhVuif3LN1X+16drUejjxNTgU02JYXR1PM+Sma7om9tfqlKUISFO4x7FJWFunNM+28aTmtp1b5mJvxmlZvWe59oM7/qWNGD35zhnQfOg6h5V3cvDSu00H3URO9jFPs3zGDWco46G6fgjnvBs1a2hNbHuP6VWIfds49uyHj4AhL6T6g==</latexit> lp l y s u ofo s rr in a a g n g c ei h wve a lyn in of Linguistic Annotator (LLM) Visual Annot Q<latexit sha1_base64="luFztvg4pkmUxMB01yKJs5viirE=">AAACznicjVHLSsNAFD2Nr/quunQTLIKrkoioy6Ibly3YB9Qik+m0DebFZFIopbj1B9zqZ4l/oH/hnTEFtYhOSHLm3HPuzL3XSwI/VY7zWrAWFpeWV4qra+sbm1vbpZ3dZhpnkosGj4NYtj2WisCPREP5KhDtRAoWeoFoeXeXOt4aCZn6cXStxonohmwQ+X2fM0VU5yZkashZMKlPb0tlp+KYZc8DNwdl5KsWl15wgx5icGQIIRBBEQ7AkNLTgQsHCXFdTIiThHwTF5hijbwZqQQpGLF39B3QrpOzEe11ztS4OZ0S0CvJaeOQPDHpJGF9mm3imcms2d9yT0xOfbcx/b08V0iswpDYv3wz5X99uhaFPs5NDT7VlBhGV8fzLJnpir65/aUqRRkS4jTuUVwS5sY567NtPKmpXfeWmfibUWpW73muzfCub0kDdn+Ocx40jyvuacWtn5SrF/moi9jHAY5onmeo4go1NEzHH/GEZ6tmjaypdf8ptQq5Zw/flvXwAY5Ck+4=</latexit>ators (Tools) n-th Memory Block Visual Weigh Lts anguage Weights❄
S St te ep p1 2: : G G < <l la at te ex xi it t s sh ha a1 1_ _b ba as se e6 64 4= =" "b H2 0b OQ oA cC 4R Z8 /s Zp Mo jF 6X HU If yI BD cR oH Va 6k bv 4G A4 +W C4 U4 A= =" "> >A AA AA AC C7 73 3i ic cj jV VH HL LS Ss sN NA AF FD D3 2G NV 73 /1 qX qX ub no QJ TF Lq IG K6 6K KI Em kI Ru dh VR ld 06 oL UK sC FV qc 4G KW Vk kq qT RT jd HD ED wN zx cM Tn LE RB l8 uV I/ /c uO Hd MO n3 bP vo 0D Bb tv /U ov Xx 4D h/ /Q ov X/ 3D hO nG jM OE AH Do 0h QO lS Jn zD pn x3 7n zj 5N mz 57 9/ /W pT Ji xK Ff PK lc OZ M7 87 Fr qP 66 9B /w Ya HH Bh ow es Kj Qo 42 OP jj YE +5 MF VR mx ae mm tY 5/ LF RZ Sk YM DW VC g0 9Q Ek JZ OC SH Bv 7p 6e Uy si 4M je Gs rp Kr 6i 4K i2 dG pE Bi Im 5d rf X1 9I i3 Ob 3g 7H p2 x/ sp 6+ vM nE /p Ok Zy Mk pX F8 vZ K6 u6 6S CF Ti tj q6 e4 2U Hx Mb j/ 3P nA gU K0 aQ K1 ai p0 at W1 Gx Yc h5 3J S1 99 Tu aW lI yo Ot Ib WP jA 84 Pv Ly By Rp WU qT ZX ne Wx xL In Xn lY 4U oZ j6 wU p4 xs Yz X2 t/ +f 5O 1m m2 ty Vy kW qn OI xp Xj Hl Lv P0 sT nu cD Hk No QI RV r9 6V 2U RX ex kC JH DS b0 QI gB EM Cj BQ DB GU wM wM xR FT Oi EC Ih H5 lS Je 6I D7 uh Hw Ck QB UD LX cQ EI X8 r4 ES SY Ui Lb cO xM BM kl uR Uk Sm Rb tU RR la mj MD MI j/ xY iY T/ +q kG bt 0D uv 4K w2 ZZ 2j P2 a2 aj 8M /1 U6 qo AB MO 6i Je ai JV Xp kL tS Lx GQ PB Gp kB Ee 5Z UK nw CP +s j0 T2 b8 xc Dw P4 ja r/ NY n3 f7 v5 H7 vx G1 UH 9e +7 to SL 3+ 8f /e 93 2W oJ TV qe 3g BQ C+ 75 Ff +u 6I j/ 8O z/ /O 6l n2 QL tQ Ch sv dr Yp Mg zZ VO wN qS iW kG x0 jd KU 4F uu yU Ft 0m yu 0q xJ Vv 9b cn /+ tq TS Vp YF oD cQ Ep uz IG 0L bY lp FL cw Eo gF 6R Mf 8v qT PZ PN tp tr GU k1 pK n5 b7 d6 W5 8n /4 Ei X8 0n yU mr ZN v4 UH +e yW H6 MG zV v3 O1 pL bG 0r oD D7 df 7Z +w P/ 8w Cf f5 ay qx FV X2 et lu 4L us 4r sp lY 23 vN rf +N aQ iF Hz MG YE se 5Z LZ Nr An 8G Vj 1a Hw Dg Fy rp Zq R5 JH +2 8F re 3z Oz Mg B0 jT 9q ax Zr dq W0 3b d6 W/ LY f9 v1 qe Vr YL hN 1b 8P z4 gs yq 7y L7 uN 30 gM Bp Ho +T 6o E= 8< </ /l la at te ex xi it t> > r r o o u u n n d d i i n n g g ( ( t t g g t t 1 2 ) ) ! ! b b b b x x 1 2 , , U aU lo tss c b <latexit sha1_base64="RWWYa5PFtCXkZDtvP4QDF3c3RAQ=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZI+YpdFN+Kqgn1ALSVJp3UwTUIyUUtx4w+41S8T/0D/wjtjCrooOiHJnXPPOTP3XjfyeSJM8z2nLS2vrK7l1wsbm1vbO8XdvXYSprHHWl7oh3HXdRLm84C1BBc+60Yxcyauzzru7ZnMd+5YnPAwuBLTiPUnzjjgI+45gqCW6z4MrEGxZBpWza6Uq7pp1GyrbFYoqFcrlbKtW4apVgnZaobFN1xjiBAeUkzAEEBQ7MNBQk8PFkxEhPUxIyymiKs8wyMKpE2JxYjhEHpL3zHtehka0F56Jkrt0Sk+vTEpdRyRJiReTLE8TVf5VDlLdJH3THnKu03p72ZeE0IFbgj9Szdn/lcnaxEYoa5q4FRTpBBZnZe5pKor8ub6j6oEOUSEyXhI+ZhiTynnfdaVJlG1y946Kv+hmBKVey/jpviUt6QBz6eoLw7aZcOyDeuyWmqcZqPO4wCHOKZ5nqCBczTRIm+OZ7zgVbvQIu1em35TtVym2cevpT19AcNgkT8=</latexit>ii a nn b t gg xe 1 GG . . ), rr too t oh uu e nn fi ndd p dii o nn s gg ..i (( ,t t t i thh o hee n e p m i rs i el a la san ut r lb <latexit sha1_base64="RWWYa5PFtCXkZDtvP4QDF3c3RAQ=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZI+YpdFN+Kqgn1ALSVJp3UwTUIyUUtx4w+41S8T/0D/wjtjCrooOiHJnXPPOTP3XjfyeSJM8z2nLS2vrK7l1wsbm1vbO8XdvXYSprHHWl7oh3HXdRLm84C1BBc+60Yxcyauzzru7ZnMd+5YnPAwuBLTiPUnzjjgI+45gqCW6z4MrEGxZBpWza6Uq7pp1GyrbFYoqFcrlbKtW4apVgnZaobFN1xjiBAeUkzAEEBQ7MNBQk8PFkxEhPUxIyymiKs8wyMKpE2JxYjhEHpL3zHtehka0F56Jkrt0Sk+vTEpdRyRJiReTLE8TVf5VDlLdJH3THnKu03p72ZeE0IFbgj9Szdn/lcnaxEYoa5q4FRTpBBZnZe5pKor8ub6j6oEOUSEyXhI+ZhiTynnfdaVJlG1y946Kv+hmBKVey/jpviUt6QBz6eoLw7aZcOyDeuyWmqcZqPO4wCHOKZ5nqCBczTRIm+OZ7zgVbvQIu1em35TtVym2cevpT19AcNgkT8=</latexit>i i t n nb a x n b f r1 t l o a r. nc etk g o is ofh n ti hr it se ) b <latexit sha1_base64="HfMht1uhZfNRE3AH4G7DyqFOHtw=">AAACyHicjVHLSsNAFD3GV31XXboJFsFVmNQ2tbuiG3GlYFXQUpJ0WgfzIpmopbjxB9zql4l/oH/hnTEFXRSdkOTOueecmXuvlwQik4y9TxnTM7Nz86WFxaXlldW18vrGeRbnqc/bfhzE6aXnZjwQEW9LIQN+maTcDb2AX3i3hyp/ccfTTMTRmRwmvBO6g0j0he9Kgtqe99CtdssVZtnNusOYyaxq06ntORQ0mozV66ZtMb0qKNZJXH7DNXqI4SNHCI4IkuIALjJ6rmCDISGsgxFhKUVC5zkesUjanFicGC6ht/Qd0O6qQCPaK89Mq306JaA3JaWJHdLExEspVqeZOp9rZ4VO8h5pT3W3If29wiskVOKG0L90Y+Z/daoWiT72dQ2Cako0oqrzC5dcd0Xd3PxRlSSHhDAV9yifUuxr5bjPptZkunbVW1fnPzRToWrvF9wcn+qWNODxFM3JwXnVsh3LPq1VWgfFqEvYwjZ2aZ4NtHCEE7TJW+AZL3g1jo3EuDeG31RjqtBs4tcynr4A48ORTQ==</latexit>t m o b xa n 2 A<latexit sha1_base64="/T/YLaO9AY6UEpfHnTU/ygqt5M8=">AAAC0HicjVHLSsNAFD2Nr/quunQTLIKrkoioy6obl1XsA2opk+m0DeZlMhFLKeLWH3CrXyX+gf6Fd8YU1CI6IcmZc+85M/deJ/LcRFrWa86Ymp6ZncvPLywuLa+sFtbWa0mYxlxUeeiFccNhifDcQFSlKz3RiGLBfMcTdefqRMXrNyJO3DC4kINItHzWC9yuy5kkqnXpM9nnzBsejdpWu1C0SpZe5iSwM1BEtiph4QWX6CAERwofAgEkYQ8MCT1N2LAQEdfCkLiYkKvjAiMskDalLEEZjNgr+vZo18zYgPbKM9FqTqd49MakNLFNmpDyYsLqNFPHU+2s2N+8h9pT3W1Afyfz8omV6BP7l26c+V+dqkWii0Ndg0s1RZpR1fHMJdVdUTc3v1QlySEiTuEOxWPCXCvHfTa1JtG1q94yHX/TmYpVe57lpnhXt6QB2z/HOQlquyV7v2Sf7RXLx9mo89jEFnZongco4xQVVMn7Go94wrNxbtwad8b9Z6qRyzQb+LaMhw8Q15SB</latexit> .0 b<latexit sha1_base64="wf9aHC2+VJkHDfvYU/0ZxLHsvQ4=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6rLoRlxVMG2hlpJMp3VoXiQTtRQ3/oBb/TLxD/QvvDOmoBbRCUnOnHvPmbn3erEvUmlZrwVjbn5hcam4XFpZXVvfKG9uNdMoSxh3WORHSdtzU+6LkDtSSJ+344S7gefzljc6VfHWDU9SEYWXchzzbuAOQzEQzJVEOZ5316v1yhWraullzgI7BxXkqxGVX3CFPiIwZAjAEUIS9uEipacDGxZi4rqYEJcQEjrOcY8SaTPK4pThEjui75B2nZwNaa88U61mdIpPb0JKE3ukiSgvIaxOM3U8086K/c17oj3V3cb093KvgFiJa2L/0k0z/6tTtUgMcKxrEFRTrBlVHctdMt0VdXPzS1WSHGLiFO5TPCHMtHLaZ1NrUl276q2r4286U7Fqz/LcDO/qljRg++c4Z0GzVrUPq/bFQaV+ko+6iB3sYp/meYQ6ztCAQ94Cj3jCs3FuxMatMf5MNQq5ZhvflvHwATI5kQA=</latexit>bx2b<latexit sha1_base64="MGZVLUPfO1JBj5ou6+m0nbxL/Ig=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0I64qmLagpSTTaR2aJmEyUUtx4w+41S8T/0D/wjtjCmoRnZDkzLn3nJl7b5CEIlWO81qwZmbn5heKi6Wl5ZXVtfL6RiONM8m4x+Iwlq3AT3koIu4poULeSiT3h0HIm8HgRMebN1ymIo4u1Cjh7aHfj0RPMF8R5QXBXcftlCtO1THLngZuDirIVz0uv+AKXcRgyDAERwRFOISPlJ5LuHCQENfGmDhJSJg4xz1KpM0oi1OGT+yAvn3aXeZsRHvtmRo1o1NCeiUpbeyQJqY8SVifZpt4Zpw1+5v32Hjqu43oH+ReQ2IVron9SzfJ/K9O16LQw5GpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hLcUmYGeWkz7bRpKZ23VvfxN9Mpmb1nuW5Gd71LWnA7s9xToPGXtU9qLrn+5XacT7qIrawjV2a5yFqOEUdHnkLPOIJz9aZlVi31ugz1Srkmk18W9bDBy/ZkP8=</latexit>bx b<latexit sha1_base64="SxfSvzv42IdpDU8dACsAC6r4gQ4=">AAACyHicjVHLTsJAFD3UF+ILdemmkZi4IgMSkR3RjXGFiQgJEtKWASf0lXaqEuLGH3CrX2b8A/0L74zFaIjRadqeOfeeM3PvtUNXxJKx14wxN7+wuJRdzq2srq1v5De3LuMgiRzedAI3iNq2FXNX+LwphXR5O4y45dkub9mjExVv3fAoFoF/Icch73rW0BcD4ViSqKZt3/XKvXyBFWs1xsrMJFCtsNqBWSoyvb5AAelqBPkXXKGPAA4SeODwIQm7sBDT00EJDCFxXUyIiwgJHee4R460CWVxyrCIHdF3SLtOyvq0V56xVjt0iktvREoTe6QJKC8irE4zdTzRzor9zXuiPdXdxvS3Uy+PWIlrYv/STTP/q1O1SAxwpGsQVFOoGVWdk7okuivq5ua3qiQ5hMQp3Kd4RNjRymmfTa2Jde2qt5aOv+lMxaq9k+YmeFe3pAHPjHMWXJaLpcNi6bxSqB+no85iB7vYp3lWUccpGmiSt8AjnvBsnBmhcWuMP1ONTKrZxo9lPHwArViRNQ==</latexit>1 bx2 b<latexit sha1_base64="SxfSvzv42IdpDU8dACsAC6r4gQ4=">AAACyHicjVHLTsJAFD3UF+ILdemmkZi4IgMSkR3RjXGFiQgJEtKWASf0lXaqEuLGH3CrX2b8A/0L74zFaIjRadqeOfeeM3PvtUNXxJKx14wxN7+wuJRdzq2srq1v5De3LuMgiRzedAI3iNq2FXNX+LwphXR5O4y45dkub9mjExVv3fAoFoF/Icch73rW0BcD4ViSqKZt3/XKvXyBFWs1xsrMJFCtsNqBWSoyvb5AAelqBPkXXKGPAA4SeODwIQm7sBDT00EJDCFxXUyIiwgJHee4R460CWVxyrCIHdF3SLtOyvq0V56xVjt0iktvREoTe6QJKC8irE4zdTzRzor9zXuiPdXdxvS3Uy+PWIlrYv/STTP/q1O1SAxwpGsQVFOoGVWdk7okuivq5ua3qiQ5hMQp3Kd4RNjRymmfTa2Jde2qt5aOv+lMxaq9k+YmeFe3pAHPjHMWXJaLpcNi6bxSqB+no85iB7vYp3lWUccpGmiSt8AjnvBsnBmhcWuMP1ONTKrZxo9lPHwArViRNQ==</latexit>bxb<latexit sha1_base64="qIYQDeRMW7OoRDRjSVocIIZPcYk=">AAACyHicjVHLTsJAFD3UF+ILdemmkZi4IlMkIjuiG+MKExESJKQtA04obdNOVULc+ANu9cuMf6B/4Z2xGA0xOk3bM+fec2buvU7oiVgy9pox5uYXFpeyy7mV1bX1jfzm1mUcJJHLG27gBVHLsWPuCZ83pJAeb4URt0eOx5vO8ETFmzc8ikXgX8hxyDsje+CLvnBtSVTDce66VjdfYMVqlbESMwlUyqx6YFpFptcXKCBd9SD/giv0EMBFghE4fEjCHmzE9LRhgSEkroMJcREhoeMc98iRNqEsThk2sUP6DmjXTlmf9soz1mqXTvHojUhpYo80AeVFhNVppo4n2lmxv3lPtKe625j+Tuo1Ilbimti/dNPM/+pULRJ9HOkaBNUUakZV56Yuie6Kurn5rSpJDiFxCvcoHhF2tXLaZ1NrYl276q2t4286U7Fq76a5Cd7VLWnAM+OcBZelonVYtM7LhdpxOuosdrCLfZpnBTWcoo4GeQs84gnPxpkRGrfG+DPVyKSabfxYxsMHqviRNA==</latexit> 2bx1 b<latexit sha1_base64="SxfSvzv42IdpDU8dACsAC6r4gQ4=">AAACyHicjVHLTsJAFD3UF+ILdemmkZi4IgMSkR3RjXGFiQgJEtKWASf0lXaqEuLGH3CrX2b8A/0L74zFaIjRadqeOfeeM3PvtUNXxJKx14wxN7+wuJRdzq2srq1v5De3LuMgiRzedAI3iNq2FXNX+LwphXR5O4y45dkub9mjExVv3fAoFoF/Icch73rW0BcD4ViSqKZt3/XKvXyBFWs1xsrMJFCtsNqBWSoyvb5AAelqBPkXXKGPAA4SeODwIQm7sBDT00EJDCFxXUyIiwgJHee4R460CWVxyrCIHdF3SLtOyvq0V56xVjt0iktvREoTe6QJKC8irE4zdTzRzor9zXuiPdXdxvS3Uy+PWIlrYv/STTP/q1O1SAxwpGsQVFOoGVWdk7okuivq5ua3qiQ5hMQp3Kd4RNjRymmfTa2Jde2qt5aOv+lMxaq9k+YmeFe3pAHPjHMWXJaLpcNi6bxSqB+no85iB7vYp3lWUccpGmiSt8AjnvBsnBmhcWuMP1ONTKrZxo9lPHwArViRNQ==</latexit>bx2 K<latexit sha1_base64="1AVZvMXUJX/idhDddIAMQnEbopY=">AAAC5HicjVHLSsNAFD2Nr1pfVZcuDBbBRQmJiLosuhHcVLAPaEtJ0mkNzYtkIpbSpTt34tYfcKvfIv6B/oV3xhSqRXRCknPPvefM3LlW6Dox1/W3jDIzOze/kF3MLS2vrK7l1zeqcZBENqvYgRtEdcuMmev4rMId7rJ6GDHTs1xWs/qnIl+7ZlHsBP4lH4Ss5Zk93+k6tsmJaue3m5zd8OE5G4zaelGdiIyiqmlaO1/QNV0udRoYKSggXeUg/4omOghgI4EHBh+csAsTMT0NGNAREtfCkLiIkCPzDCPkSJtQFaMKk9g+fXsUNVLWp1h4xlJt0y4uvREpVeySJqC6iLDYTZX5RDoL9jfvofQUZxvQ30q9PGI5roj9Szeu/K9O9MLRxbHswaGeQsmI7uzUJZG3Ik6uTnTFySEkTuAO5SPCtlSO71mVmlj2Lu7WlPl3WSlYEdtpbYIPcUoasPFznNOguq8Zh5pxcVAonaSjzmILO9ijeR6hhDOUUSHvWzzhGS9KV7lT7pWHr1Ilk2o28W0pj5+boJst</latexit> ey0,Key1,... SELF-ATTN Visual Weight Ls anguage Weights❄
Step3: G <latexit sha1_base64="61ZzdnoXO7upFU9SmdB0Ja42I2o=">AAAC73icjVHLSsNAFD2N73fVpZtgEdRFSayoS9GFLhVsK1gpSTrGwTQTJxMfFP/BnTtx6w+41b8Q/0D/wjtjCj4QnZDkzLn3nJl7r59EPFWO81Kwenr7+gcGh4ZHRsfGJ4qTU7VUZDJg1UBEQu77XsoiHrOq4ipi+4lkXtuPWN0/2dTx+hmTKRfxnrpM2GHbC2N+xANPEdUsLjYUuyBdZ0uKLG7xOLyaV6FqVhYakofHypNSnNu+f9GsNIslp+yYZf8Ebg5KyNeOKD6jgRYEAmRogyGGIhzBQ0rPAVw4SIg7RIc4SYibOMMVhkmbURajDI/YE/qGtDvI2Zj22jM16oBOieiVpLQxRxpBeZKwPs028cw4a/Y3747x1He7pL+fe7WJVTgm9i9dN/O/Ol2LwhHWTA2cakoMo6sLcpfMdEXf3P5UlSKHhDiNWxSXhAOj7PbZNprU1K5765n4q8nUrN4HeW6GN31LGrD7fZw/QW2p7K6U3d3l0vpGPupBzGAW8zTPVaxjGzuokvc1HvCIJ+vUurFurbuPVKuQa6bxZVn370zNoT4=</latexit> r o u n d i n g ( t g t 3 ) ! b b x 3 , U ins b<latexit sha1_base64="HfMht1uhZfNRE3AH4G7DyqFOHtw=">AAACyHicjVHLSsNAFD3GV31XXboJFsFVmNQ2tbuiG3GlYFXQUpJ0WgfzIpmopbjxB9zql4l/oH/hnTEFXRSdkOTOueecmXuvlwQik4y9TxnTM7Nz86WFxaXlldW18vrGeRbnqc/bfhzE6aXnZjwQEW9LIQN+maTcDb2AX3i3hyp/ccfTTMTRmRwmvBO6g0j0he9Kgtqe99CtdssVZtnNusOYyaxq06ntORQ0mozV66ZtMb0qKNZJXH7DNXqI4SNHCI4IkuIALjJ6rmCDISGsgxFhKUVC5zkesUjanFicGC6ht/Qd0O6qQCPaK89Mq306JaA3JaWJHdLExEspVqeZOp9rZ4VO8h5pT3W3If29wiskVOKG0L90Y+Z/daoWiT72dQ2Cako0oqrzC5dcd0Xd3PxRlSSHhDAV9yifUuxr5bjPptZkunbVW1fnPzRToWrvF9wcn+qWNODxFM3JwXnVsh3LPq1VWgfFqEvYwjZ2aZ4NtHCEE7TJW+AZL3g1jo3EuDeG31RjqtBs4tcynr4A48ORTQ==</latexit> i bn xg 2 G ) r to ou fin nd din .g .,( t th he e t re ex st u w ltari ntt te rn e go in o nth ie s b<latexit sha1_base64="/s4AI4yOoSfQlmcVZM3q9QZdNDI=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZK+3RXdiKsK9gG1lCSd1sE0CclELcWNP+BWv0z8A/0L74wp6KLohCR3zj3nzNx7ndDjsTDN94y2tLyyupZdz21sbm3v5Hf32nGQRC5ruYEXRF3HjpnHfdYSXHisG0bMnjge6zi3ZzLfuWNRzAP/SkxD1p/YY5+PuGsLglqO8zAoDfIF0zipVuqlom4aVq1kFesUlGpmrVLWLcNUq4B0NYP8G64xRAAXCSZg8CEo9mAjpqcHCyZCwvqYERZRxFWe4RE50ibEYsSwCb2l75h2vRT1aS89Y6V26RSP3oiUOo5IExAvoliepqt8opwlush7pjzl3ab0d1KvCaECN4T+pZsz/6uTtQiMUFc1cKopVIiszk1dEtUVeXP9R1WCHELCZDykfESxq5TzPutKE6vaZW9tlf9QTInKvZtyE3zKW9KA51PUFwftomFVDeuyXGicpqPO4gCHOKZ51tDAOZpokTfHM17wql1ooXavTb+pWibV7OPX0p6+AO+3kVI=</latexit>p bi l xl a 3 r . b<latexit sha1_base64="hZG6VHgd7RXWpU+4TsdWqZIIsxY=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIVdVl0I64qmLZQS0mm0zo0L5KJWoobf8Ctfpn4B/oX3hlTUIvohCRnzr3nzNx7vdgXqbSs14IxMzs3v1BcLC0tr6yuldc3GmmUJYw7LPKjpOW5KfdFyB0ppM9bccLdwPN50xueqnjzhiepiMJLOYp5J3AHoegL5kqiHM+76+53yxWraullTgM7BxXkqx6VX3CFHiIwZAjAEUIS9uEipacNGxZi4joYE5cQEjrOcY8SaTPK4pThEjuk74B27ZwNaa88U61mdIpPb0JKEzukiSgvIaxOM3U8086K/c17rD3V3Ub093KvgFiJa2L/0k0y/6tTtUj0caxrEFRTrBlVHctdMt0VdXPzS1WSHGLiFO5RPCHMtHLSZ1NrUl276q2r4286U7Fqz/LcDO/qljRg++c4p0Fjr2ofVu2Lg0rtJB91EVvYxi7N8wg1nKEOh7wFHvGEZ+PciI1bY/SZahRyzSa+LePhAzSZkQE=</latexit>bx3 V<latexit sha1_base64="95TKWukBzyYQwOOsTS9wREZuKnM=">AAAC6HicjVHLSsNAFD2Nr1pfVZdugkVwUUIioi6LblxWsA9oiyTptMZOk5BMxFL6Ae7ciVt/wK1+ifgH+hfeGVOwFtEJSe4995wzc+c6IfdiYZpvGW1mdm5+IbuYW1peWV3Lr29U4yCJXFZxAx5EdceOGfd8VhGe4KweRszuO5zVnN6JrNeuWRR7gX8uBiFr9e2u73U81xYEXeQLTcFuxLBq84SNLsyiPpFbRd0wDGKZhqmWPh1YaVBAuspB/hVNtBHARYI+GHwIijlsxPQ0YMFESFgLQ8IiijxVZxghR9qEWIwYNqE9+nYpa6SoT7n0jJXapV04vREpdeyQJiBeRLHcTVf1RDlL9DfvofKUZxvQ30m9+oQKXBL6l27M/K9O9iLQwZHqwaOeQoXI7tzUJVG3Ik+uf+tKkENImIzbVI8odpVyfM+60sSqd3m3tqq/K6ZEZe6m3AQf8pQ0YOvnOKeD6p5hHRjW2X6hdJyOOostbGOX5nmIEk5RRoW8b/GEZ7xoV9qddq89fFG1TKrZxMTSHj8BWCec/Q==</latexit> alue0,Value1,... K<latexit sha1_base64="L60kNDzMRwZEVn/lSNyuTsFdZ9k=">AAAC4XicjVHLSsNAFD2Nr1pfUZe6CBbBhZRERF0W3QhuKtgH1FKS6bSGpklIJmIp3bhzJ279Abf6M+If6F94Z5qCWkQnJDn33HvOzJ3rhJ4bC9N8y2hT0zOzc9n53MLi0vKKvrpWiYMkYrzMAi+Iao4dc8/1eVm4wuO1MOJ2z/F41emeyHz1mkexG/gXoh/yRs/u+G7bZbYgqqlvXgp+IwZnvD9sil1jFFVsL+EUN/W8WTDVMiaBlYI80lUK9FdcooUADAl64PAhCHuwEdNThwUTIXENDIiLCLkqzzFEjrQJVXGqsInt0rdDUT1lfYqlZ6zUjHbx6I1IaWCbNAHVRYTlbobKJ8pZsr95D5SnPFuf/k7q1SNW4IrYv3Tjyv/qZC8CbRypHlzqKVSM7I6lLom6FXly40tXghxC4iRuUT4izJRyfM+G0sSqd3m3tsq/q0rJypiltQk+5ClpwNbPcU6Cyl7BOihY5/v54nE66iw2sIUdmuchijhFCWXyvsUTnvGiMe1Ou9ceRqVaJtWs49vSHj8BSwCblA==</latexit> eyt,Valuet Q<latexit sha1_base64="H9sLI9wGKyvcWKrO7gvtvNa/aao=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl047JF+4C2liSd1tC8mEzEUAri1h9wqz8l/oH+hXfGFNQiOiHJmXPvOTP3Xjvy3FgYxmtOm5tfWFzKLxdWVtfWN4qbW404TLjD6k7ohbxlWzHz3IDVhSs81oo4s3zbY017dCbjzRvGYzcMLkUasa5vDQN34DqWIOqqI9itGNcSxtNJT/SKJaNsqKXPAjMDJWSrGhZf0EEfIRwk8MEQQBD2YCGmpw0TBiLiuhgTxwm5Ks4wQYG0CWUxyrCIHdF3SLt2xga0l56xUjt0ikcvJ6WOPdKElMcJy9N0FU+Us2R/8x4rT3m3lP525uUTK3BN7F+6aeZ/dbIWgQFOVA0u1RQpRlbnZC6J6oq8uf6lKkEOEXES9ynOCTtKOe2zrjSxql321lLxN5UpWbl3stwE7/KWNGDz5zhnQeOgbB6VzdphqXKajTqPHexin+Z5jArOUUWdvDke8YRn7UJLtTvt/jNVy2WabXxb2sMHp5qVjw==</latexit> ueryt
Step4: C <latexit sha1_base64="QA+RhXzVhkMBAd/4AoV0OYVQEag=">AAAC4nicjVHLSsNAFD2Nr/qOuhQhWAQFKYkWdSm60Z2CbcUHJUlHDU0yYTIRS+nKnTtx6w+41Y8R/0D/wjtjCj4QnZDkzLn3nJl7r5eEQSpt+6Vg9PUPDA4Vh0dGx8YnJs2p6VrKM+Gzqs9DLg49N2VhELOqDGTIDhPB3MgLWd1rbat4/ZKJNODxgWwn7DRyz+PgLPBdSVTDnDuR7Ip0nW3BkyPOo924u+h5V43VZauy1DBLdtnWy/oJnByUkK89bj7jBE1w+MgQgSGGJBzCRUrPMRzYSIg7RYc4QSjQcYYuRkibURajDJfYFn3PaXecszHtlWeq1T6dEtIrSGlhgTSc8gRhdZql45l2Vuxv3h3tqe7Wpr+Xe0XESlwQ+5eul/lfnapF4gwbuoaAako0o6rzc5dMd0Xd3PpUlSSHhDiFmxQXhH2t7PXZ0ppU16566+r4q85UrNr7eW6GN3VLGrDzfZw/QW2l7KyVnf1KaXMrH3URs5jHIs1zHZvYwR6q5H2NBzziyWgaN8atcfeRahRyzQy+LOP+HamTmtk=</latexit> r o p Z o o m I n ( b b x 3 , 4 ) ! <latexit sha1_base64="2LswZlMM15UTGqp2V+A6N+wNFqA=">AAACznicjVHLTsJAFD3UF+ILdemmkZi4Iq0x6pLoxiUmAiZATFsGmFA6zXSKIYS49Qfc6mcZ/0D/wjtjSVRidJq2Z849587ce/045IlynNectbC4tLySXy2srW9sbhW3d+qJSGXAaoEIhbzxvYSFPGI1xVXIbmLJvKEfsoY/uNDxxojJhIvoWo1j1h56vYh3eeApopotyXt95Ukp7m6LJafsmGXPAzcDJWSrKoovaKEDgQAphmCIoAiH8JDQ04QLBzFxbUyIk4S4iTNMUSBvSipGCo/YAX17tGtmbER7nTMx7oBOCemV5LRxQB5BOklYn2abeGoya/a33BOTU99tTH8/yzUkVqFP7F++mfK/Pl2LQhdnpgZONcWG0dUFWZbUdEXf3P5SlaIMMXEadyguCQfGOeuzbTyJqV331jPxN6PUrN4HmTbFu74lDdj9Oc55UD8quydl9+q4VDnPRp3HHvZxSPM8RQWXqKJmOv6IJzxbVWtkTa37T6mVyzy7+Lashw/pB5QU</latexit> I<latexit sha1_base64="QRAYKeCs4EYLlqr7Zz969VaQGEc=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0U3cV7QNqKcl0WoemSUgmSimCP+BWP038A/0L74xTUIvohCRnzr3nzNx7/TgQqXSc15w1N7+wuJRfLqysrq1vFDe3GmmUJYzXWRREScv3Uh6IkNelkAFvxQn3Rn7Am/7wTMWbtzxJRRReyXHMOyNvEIq+YJ4k6vK863aLJafs6GXPAteAEsyqRcUXXKOHCAwZRuAIIQkH8JDS04YLBzFxHUyISwgJHee4R4G0GWVxyvCIHdJ3QLu2YUPaK89UqxmdEtCbkNLGHmkiyksIq9NsHc+0s2J/855oT3W3Mf194zUiVuKG2L9008z/6lQtEn2c6BoE1RRrRlXHjEumu6Jubn+pSpJDTJzCPYonhJlWTvtsa02qa1e99XT8TWcqVu2Zyc3wrm5JA3Z/jnMWNA7K7lHZvTgsVU7NqPPYwS72aZ7HqKCKGurkPcAjnvBsVa3Qyqy7z1QrZzTb+Lashw+f9o/4</latexit> 1 , U ans din zg o C or mo p inZ o ito bm yI n 4( tb<latexit sha1_base64="/s4AI4yOoSfQlmcVZM3q9QZdNDI=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZK+3RXdiKsK9gG1lCSd1sE0CclELcWNP+BWv0z8A/0L74wp6KLohCR3zj3nzNx7ndDjsTDN94y2tLyyupZdz21sbm3v5Hf32nGQRC5ruYEXRF3HjpnHfdYSXHisG0bMnjge6zi3ZzLfuWNRzAP/SkxD1p/YY5+PuGsLglqO8zAoDfIF0zipVuqlom4aVq1kFesUlGpmrVLWLcNUq4B0NYP8G64xRAAXCSZg8CEo9mAjpqcHCyZCwvqYERZRxFWe4RE50ibEYsSwCb2l75h2vRT1aS89Y6V26RSP3oiUOo5IExAvoliepqt8opwlush7pjzl3ab0d1KvCaECN4T+pZsz/6uTtQiMUFc1cKopVIiszk1dEtUVeXP9R1WCHELCZDykfESxq5TzPutKE6vaZW9tlf9QTInKvZtyE3zKW9KA51PUFwftomFVDeuyXGicpqPO4gCHOKZ51tDAOZpokTfHM17wql1ooXavTb+pWibV7OPX0p6+AO+3kVI=</latexit> i mb x e 3 s , t 4 o) gto e tc nro ewp i it m o an g b<latexit sha1_base64="/s4AI4yOoSfQlmcVZM3q9QZdNDI=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZK+3RXdiKsK9gG1lCSd1sE0CclELcWNP+BWv0z8A/0L74wp6KLohCR3zj3nzNx7ndDjsTDN94y2tLyyupZdz21sbm3v5Hf32nGQRC5ruYEXRF3HjpnHfdYSXHisG0bMnjge6zi3ZzLfuWNRzAP/SkxD1p/YY5+PuGsLglqO8zAoDfIF0zipVuqlom4aVq1kFesUlGpmrVLWLcNUq4B0NYP8G64xRAAXCSZg8CEo9mAjpqcHCyZCwvqYERZRxFWe4RE50ibEYsSwCb2l75h2vRT1aS89Y6V26RSP3oiUOo5IExAvoliepqt8opwlush7pjzl3ab0d1KvCaECN4T+pZsz/6uTtQiMUFc1cKopVIiszk1dEtUVeXP9R1WCHELCZDykfESxq5TzPutKE6vaZW9tlf9QTInKvZtyE3zKW9KA51PUFwftomFVDeuyXGicpqPO4gCHOKZ51tDAOZpokTfHM17wql1ooXavTb+pWibV7OPX0p6+AO+3kVI=</latexit> e b x I<latexit sha1_base64="3MvpiuL8JcXNT4OrPGF91CDSxfU=">AAACxnicjVHLTsMwEBzCu7wKHLlEVEicIieUR28ILnADQQGpIJS4brGaJpHjgCqExA9whU9D/AH8BWuTSnBA4CjJenZm7N2NsljmmrG3EWd0bHxicmq6MjM7N79QXVw6y9NCcdHkaZyqiyjMRSwT0dRSx+IiUyLsR7E4j3r7Jn9+K1Qu0+RUDzJx1Q+7iexIHmqCTg6v/etqjXkBC+qNwGXeRiNgbIuCzfomYxuu7zG7aijXUVp9xSXaSMFRoA+BBJriGCFyelrwwZARdoV7whRF0uYFHlAhbUEsQYyQ0B59u7RrlWhCe+OZWzWnU2J6FSldrJEmJZ6i2Jzm2nxhnQ36m/e99TR3G9A/Kr36hGrcEPqXbsj8r87UotHBjq1BUk2ZRUx1vHQpbFfMzd1vVWlyyAgzcZvyimJulcM+u1aT29pNb0Obf7dMg5o9L7kFPswtacDDKbq/B2eB5295/nG9trtXjnoKK1jFOs1zG7s4wBGa5N3FE57x4hw4iVM4d19UZ6TULOPHch4/ASFJkDA=</latexit> 1 3 , . t<latexit sha1_base64="3uQ8QX2fue98Q2WsirfBsNLdvpE=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0I64qmLagpSTTaR2aJmEyUUtx4w+41S8T/0D/wjtjCmoRnZDkzLn3nJl7b5CEIlWO81qwZmbn5heKi6Wl5ZXVtfL6RiONM8m4x+Iwlq3AT3koIu4poULeSiT3h0HIm8HgRMebN1ymIo4u1Cjh7aHfj0RPMF8R5ak71XE75YpTdcyyp4GbgwryVY/LL7hCFzEYMgzBEUERDuEjpecSLhwkxLUxJk4SEibOcY8SaTPK4pThEzugb592lzkb0V57pkbN6JSQXklKGzukiSlPEtan2SaeGWfN/uY9Np76biP6B7nXkFiFa2L/0k0y/6vTtSj0cGRqEFRTYhhdHctdMtMVfXP7S1WKHBLiNO5SXBJmRjnps200qald99Y38TeTqVm9Z3luhnd9Sxqw+3Oc06CxV3UPqu75fqV2nI+6iC1sY5fmeYgaTlGHR94Cj3jCs3VmJdatNfpMtQq5ZhPflvXwAYXbkSM=</latexit>I<latexit sha1_base64="QRAYKeCs4EYLlqr7Zz969VaQGEc=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0U3cV7QNqKcl0WoemSUgmSimCP+BWP038A/0L74xTUIvohCRnzr3nzNx7/TgQqXSc15w1N7+wuJRfLqysrq1vFDe3GmmUJYzXWRREScv3Uh6IkNelkAFvxQn3Rn7Am/7wTMWbtzxJRRReyXHMOyNvEIq+YJ4k6vK863aLJafs6GXPAteAEsyqRcUXXKOHCAwZRuAIIQkH8JDS04YLBzFxHUyISwgJHee4R4G0GWVxyvCIHdJ3QLu2YUPaK89UqxmdEtCbkNLGHmkiyksIq9NsHc+0s2J/855oT3W3Mf194zUiVuKG2L9008z/6lQtEn2c6BoE1RRrRlXHjEumu6Jubn+pSpJDTJzCPYonhJlWTvtsa02qa1e99XT8TWcqVu2Zyc3wrm5JA3Z/jnMWNA7K7lHZvTgsVU7NqPPYwS72aZ7HqKCKGurkPcAjnvBsVa3Qyqy7z1QrZzTb+Lashw+f9o/4</latexit> xt1 1
Step5: O <latexit sha1_base64="76HXZux32y8Z7IeCXjMCYR2Nwsc=">AAAC6XicjVHLSsNAFD3GV31XXboJVqFuSiKiLovduPOB1UIrJUmndWheTCZaKf6AO3fi1h9wqz8i/oH+hXfGCGoRnZDkzLn3nJl7rxv7PJGW9TJkDI+Mjo3nJianpmdm5/LzC8dJlAqPVb3Ij0TNdRLm85BVJZc+q8WCOYHrsxO3W1Hxk3MmEh6FR/IyZqeB0wl5m3uOJKqZX2lI1iNdf69yeFXkQadprzUE75xJR4jowpQ92bSb+YJVsvQyB4GdgQKytR/ln9FACxE8pAjAEEIS9uEgoacOGxZi4k7RJ04Q4jrOcIVJ0qaUxSjDIbZL3w7t6hkb0l55Jlrt0Sk+vYKUJlZJE1GeIKxOM3U81c6K/c27rz3V3S7p72ZeAbESZ8T+pfvM/K9O1SLRxraugVNNsWZUdV7mkuquqJubX6qS5BATp3CL4oKwp5WffTa1JtG1q946Ov6qMxWr9l6Wm+JN3ZIGbP8c5yA4Xi/ZmyX7YKNQ3slGncMSllGkeW6hjF3so0re13jAI56MrnFj3Bp3H6nGUKZZxLdl3L8DgiyeRw==</latexit> C R ( i m g 1 ) ! t x t 1 , U ths ei n img aO gC eR I<latexit sha1_base64="3MvpiuL8JcXNT4OrPGF91CDSxfU=">AAACxnicjVHLTsMwEBzCu7wKHLlEVEicIieUR28ILnADQQGpIJS4brGaJpHjgCqExA9whU9D/AH8BWuTSnBA4CjJenZm7N2NsljmmrG3EWd0bHxicmq6MjM7N79QXVw6y9NCcdHkaZyqiyjMRSwT0dRSx+IiUyLsR7E4j3r7Jn9+K1Qu0+RUDzJx1Q+7iexIHmqCTg6v/etqjXkBC+qNwGXeRiNgbIuCzfomYxuu7zG7aijXUVp9xSXaSMFRoA+BBJriGCFyelrwwZARdoV7whRF0uYFHlAhbUEsQYyQ0B59u7RrlWhCe+OZWzWnU2J6FSldrJEmJZ6i2Jzm2nxhnQ36m/e99TR3G9A/Kr36hGrcEPqXbsj8r87UotHBjq1BUk2ZRUx1vHQpbFfMzd1vVWlyyAgzcZvyimJulcM+u1aT29pNb0Obf7dMg5o9L7kFPswtacDDKbq/B2eB5295/nG9trtXjnoKK1jFOs1zG7s4wBGa5N3FE57x4hw4iVM4d19UZ6TULOPHch4/ASFJkDA=</latexit> ( 1 I<latexit sha1_base64="3MvpiuL8JcXNT4OrPGF91CDSxfU=">AAACxnicjVHLTsMwEBzCu7wKHLlEVEicIieUR28ILnADQQGpIJS4brGaJpHjgCqExA9whU9D/AH8BWuTSnBA4CjJenZm7N2NsljmmrG3EWd0bHxicmq6MjM7N79QXVw6y9NCcdHkaZyqiyjMRSwT0dRSx+IiUyLsR7E4j3r7Jn9+K1Qu0+RUDzJx1Q+7iexIHmqCTg6v/etqjXkBC+qNwGXeRiNgbIuCzfomYxuu7zG7aijXUVp9xSXaSMFRoA+BBJriGCFyelrwwZARdoV7whRF0uYFHlAhbUEsQYyQ0B59u7RrlWhCe+OZWzWnU2J6FSldrJEmJZ6i2Jzm2nxhnQ36m/e99TR3G9A/Kr36hGrcEPqXbsj8r87UotHBjq1BUk2ZRUx1vHQpbFfMzd1vVWlyyAgzcZvyimJulcM+u1aT29pNb0Obf7dMg5o9L7kFPswtacDDKbq/B2eB5295/nG9trtXjnoKK1jFOs1zG7s4wBGa5N3FE57x4hw4iVM4d19UZ6TULOPHch4/ASFJkDA=</latexit> , 1 t ) h er e rc eo sug ln t i iz se “th Qe U t Ie Cx Kt w Dr Eit Pte On S o ITnA<latexit sha1_base64="zM4q66wQW3lqPnlAeNIlhWc5UK4=">AAAC0HicjVHLSsNAFD2Nr/quunQTLIKrkoioy6obl1XsA2opk+m0DeZlMhFLKeLWH3CrXyX+gf6Fd8YU1CI6IcmZc+85M/deJ/LcRFrWa86Ymp6ZncvPLywuLa+sFtbWa0mYxlxUeeiFccNhifDcQFSlKz3RiGLBfMcTdefqRMXrNyJO3DC4kINItHzWC9yuy5kkqnXpM9nnzBsejdp2u1C0SpZe5iSwM1BEtiph4QWX6CAERwofAgEkYQ8MCT1N2LAQEdfCkLiYkKvjAiMskDalLEEZjNgr+vZo18zYgPbKM9FqTqd49MakNLFNmpDyYsLqNFPHU+2s2N+8h9pT3W1Afyfz8omV6BP7l26c+V+dqkWii0Ndg0s1RZpR1fHMJdVdUTc3v1QlySEiTuEOxWPCXCvHfTa1JtG1q94yHX/TmYpVe57lpnhXt6QB2z/HOQlquyV7v2Sf7RXLx9mo89jEFnZongco4xQVVMn7Go94wrNxbtwad8b9Z6qRyzQb+LaMhw8TN5SC</latexit> ” .1 A<latexit sha1_base64="Lz9T70LZH4j8t/WotJtOLW80Cn4=">AAACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl147KCrYW2yGQ6rcG8mEwKpRS3/oBb/SzxD/QvvDOmoBbRCUnOnHvOnbn3ekngp8pxXgvW3PzC4lJxubSyura+Ud7caqZxJrlo8DiIZctjqQj8SDSUrwLRSqRgoReIa+/uXMevh0KmfhxdqVEiuiEbRH7f50wR1e6ETN1yFoxPJzflilN1zLJngZuDCvJVj8sv6KCHGBwZQghEUIQDMKT0tOHCQUJcF2PiJCHfxAUmKJE3I5UgBSP2jr4D2rVzNqK9zpkaN6dTAnolOW3skScmnSSsT7NNPDOZNftb7rHJqe82or+X5wqJVbgl9i/fVPlfn65FoY8TU4NPNSWG0dXxPEtmuqJvbn+pSlGGhDiNexSXhLlxTvtsG09qate9ZSb+ZpSa1XueazO861vSgN2f45wFzYOqe1R1Lw8rtbN81EXsYBf7NM9j1HCBOhqm4494wrNVt4bWxLr/lFqF3LONb8t6+ABoMpPe</latexit> A<latexit sha1_base64="veN4EvEHuo6iohugj2nSrwf7lEM=">AAAC0HicjVHLSsNAFD2Nr/quunQTLIKrkoioy6obl1XsA2opk+m0DeZlMhFLKeLWH3CrXyX+gf6Fd8YU1CI6IcmZc+85M/deJ/LcRFrWa86Ymp6ZncvPLywuLa+sFtbWa0mYxlxUeeiFccNhifDcQFSlKz3RiGLBfMcTdefqRMXrNyJO3DC4kINItHzWC9yuy5kkqnXpM9nnzBsejdqyXShaJUsvcxLYGSgiW5Ww8IJLdBCCI4UPgQCSsAeGhJ4mbFiIiGthSFxMyNVxgREWSJtSlqAMRuwVfXu0a2ZsQHvlmWg1p1M8emNSmtgmTUh5MWF1mqnjqXZW7G/eQ+2p7jagv5N5+cRK9In9SzfO/K9O1SLRxaGuwaWaIs2o6njmkuquqJubX6qS5BARp3CH4jFhrpXjPptak+jaVW+Zjr/pTMWqPc9yU7yrW9KA7Z/jnAS13ZK9X7LP9orl42zUeWxiCzs0zwOUcYoKquR9jUc84dk4N26NO+P+M9XIZZoNfFvGwweyV5TF</latexit> t <latexit sha1_base64="0k0+D4mHvZwQZuyk7AOOwscazgU=">AAAC1nicjVHLSsNAFD3Gd32lunQTLIIglEREXRbd6E7B1kItZTKONjQvkolSQt2JW3/ArX6S+Af6F94ZU1CL6IQkZ86958zce93Y91Jp269jxvjE5NT0zGxpbn5hccksLzfSKEu4qPPIj5Kmy1Lhe6GoS0/6ohknggWuL87c3oGKn12LJPWi8FT2Y9EO2FXoXXqcSaI6Zvk8YLLLmZ8fDTq53HQGHbNiV229rFHgFKCCYh1H5gvOcYEIHBkCCISQhH0wpPS04MBGTFwbOXEJIU/HBQYokTajLEEZjNgefa9o1yrYkPbKM9VqTqf49CaktLBOmojyEsLqNEvHM+2s2N+8c+2p7tanv1t4BcRKdIn9SzfM/K9O1SJxiT1dg0c1xZpR1fHCJdNdUTe3vlQlySEmTuELiieEuVYO+2xpTaprV71lOv6mMxWr9rzIzfCubkkDdn6OcxQ0tqrOTtU52a7U9otRz2AVa9igee6ihkMco07eN3jEE56NpnFr3Bn3n6nGWKFZwbdlPHwALmSWeg==</latexit> It+1 I<latexit sha1_base64="Xzwl2eLe8WruyuQkQHioGGKvH10=">AAAC3HicjVHLSsNAFD3GV31XXbhwEyyCIJRE1La7ohvdKVgt2FqSdFoH0yROJoVSunMnbv0Bt/o94h/oX3hnmoIuik5Icubce87cO9eNfB5Ly/qYMCanpmdmM3PzC4tLyyvZ1bXLOEyExype6Iei6jox83nAKpJLn1UjwZyO67Mr9+5Yxa+6TMQ8DC5kL2L1jtMOeIt7jiSqkd04bfTlrj2oBezerHUdEYTylgftRjZn5Q/sUtHaM4egVBqCQqFo2nlLrxzSdRZm31FDEyE8JOiAIYAk7MNBTM81bFiIiKujT5wgxHWcYYB50iaUxSjDIfaOvm3aXadsQHvlGWu1R6f49ApSmtgmTUh5grA6zdTxRDsrdpx3X3uq2nr0d1OvDrESt8T+pRtl/lenepFooah74NRTpBnVnZe6JPpWVOXmj64kOUTEKdykuCDsaeXonk2tiXXv6m4dHf/UmYpVey/NTfClqqQBj6ZojgeXe3n7MG+f7+fKR+moM9jEFnZongWUcYIzVHT9L3jFm3FjPBiPxtMw1ZhINev4tYznb1M4mUs=</latexit>t+16=?
,
Figure2.Theautomaticdatasynthesisframework(left): alinguisticannotator(LLM)taughtwithtaskrequirementsandusageof
manipulations(prompt)isfirstaskedtoprovidesolvingstepsforaquestionQ,andthevisualannotators(Tools)arethenengagedto
replacethemanipulationsreturns,followedbyafinaltraversalonthetreebranchedbythepossiblereturnsisperformedtofindfeasible
pathsterminatinganswerA.AcompatibleVLMarchitecture(right):ahistoricmemoryrepresentationismaintainedthroughoutaround
ofmulti-turnstraining,wheret-thturnisevokedbyanproductionofnewimageI .
t
2.2.DataProduction ments,usageofmanipulations,andoutputdataformat,and
further manually annotate 5 demonstrations for a stable
Based on the definition of data structure, we introduce a
generation. Thedetailedimplementationsareavailableat
dataproductionframework,thatcouldefficientlysynthesize
AppendixB.3.
CoMdatauponimage-orientedquestion-answerpairs,and
is also capable of producing high-quality annotations by Wethenemployessentialvisualannotatorstosupplythere-
replacingthelinguisticandvisualannotatorswithhuman turnsofmanipulationsrequestedinthesolvingstepsthrough
labour. Wedescribethedetaileddataproductionapproach exactly performing the corresponding manipulations. By
withtheimplementationforautomaticdatasynthesisinthis empiricallyanalyzingthemanipulationsfrombothprede-
paper. finedsetandnewlycreatedones(referstoAppendixB.2for
adetailedstatistics),werevealthegroundingandOCRare
2.2.1.DATAGENERATION twofundamentalmanipulations,andmostoftheotherscan
beconsequentlyderived(e.g.,CropZoomInalongaregion
Given a general corpus = (I,Q,A) consisting of
D { } ofbox,Countinguponrecognizedboxes,andCalculatefor
tripletsamplesofimagesandcorrespondingvisualquestion-
theidentifiedformula). Therefore,weemploytworeliable
answerpairs,ourautomaticdatasynthesisframeworkcon-
visualtools,GroundingDINO(Liuetal.,2023c)andPad-
sistsofalinguisticannotatorandseveralvisualannotators
dleOCR(Duetal.,2020),anddeveloptheimplementations
accordingtothemanipulations. ForaquestionQineach
ofthesemanipulations.
sample,wefirstengagethelinguisticannotatortogenerate
manipulations-assistedsolvingstepswiththeCoMformat Weexecutethemanipulationsalongwiththereasoningsteps,
(f ,desc ),wherethecorrespondingreturnsoftheinstanti- whichturnsthereasoningstepsintoatree ,astheinputof
i i T
atedmanipulationsexecutionsaresetwithvariablesasplace- currentmanipulationf 1(x 1)mayrelyononeofthemultiple
holders. In this paper, we adopt GPT4 (OpenAI, 2023a), returnsofpreviousmanipulationf 2 x 2,i.e.,x 1 relyon
→
alargelanguagemodelwithreliablelanguageunderstand- x 2(e.g.,step2forfindingpillarsinFigure2). Thedesigned
ingandgenerationabilitiesasthelinguisticannotator. We prompt,andacorrespondinggenerationwithlinguisticand
designacomprehensivepromptincludingthetaskrequire- visualresultsareavailableatAppendixfigure6.
3
! !!2.2.2.DATAPROCESSING cretely, the pre-trained EVA2-CLIP-E (Sun et al., 2023a)
with 4B parameters and Vicuna-7B-v1.5 (Chiang et al.,
Thetree ,thatisrootedfromthequestionQandbranched
T 2023) are adopted as the vision encoder and LLM back-
by the manipulations returns may contain negative paths
bone,respectively. Atwo-layerMLP(SwiGLU(Shazeer,
thatcannotleadtothecorrectanswerA,inwhichthenodes
2020))isfurtherengagedtomaptheoutputofthevision
maycomefromerrorannotationsorirrelevantreferences.
encoderintothelinguisticspaceoftheLLMbackbone. The
Wethenperformatraversaloneachproducedtreewiththe
visualexpertmoduleaddsthevision-specificweightsinto
DepthFirstSearch(DFS)tofindallpositivepaths
{Pi |Pi ∈ theattentionlayerandfeed-forwardlayerofeachblockin
,i=1,2,... ,whereeachpath isterminatedwiththe
T } Pi theLLMbackbone,resultinginatotalof6.5Badditional
finalanswerAasthereturnofthelastmanipulation.
parametersforthedeepfusionofmodalities.
The operation of zoom in refers to an intellective human
Basedonthisgeneralarchitecture,wedevelopamemory-
behaviour, which could not be generated from the auto-
based multi-turns multi-images VLM approach. Specif-
matic annotators appropriately and requires a cogitative
ically, for a round of images-based multi-turns sample
decisionaccordingtothevisualscene. Weleverageasim-
[(I ,Q ,A )t = 1,2,...], we keep the accumulated KV
ple but effective strategy to compensate the CropZoomIn t t t |
memoriesofeachlayeroftheLLMbackbonethroughthis
manipulation. Duringthetraversalontree ,atthenode
T around. Andateachturntfortrainingandinference,we
of manipulation f(x(bbx)) that involves a box bbx in the
calculatetheattentionfunctionattas:
input x, we first calculate the ratio r of the current im-
age’s area to the box’s area, and then insert the manipu- att(X)=softmax(Q t√K′ tT )V′
t
lation CropZoomIn(bbx,n) in current step with the sat- d
(3)
isfiability condition (n is determined according to r, e.g., K′ =trunc(concat(K ,K ,...,K ))
t 0 1 t
n = 2 if r 36). Therefore, the final resultant chains V′ =trunc(concat(V ,V ,...,V ))
≥ t 0 1 t
ς ς := ,i = 1,2,... are obtained by assigning the
i i i
{ | P }
c po atr hre s.sp To hn ed oin vg erv aa lllu de as tao -pf rr oe da us co in ni gng als gt oe rp its hw mit ih spth ree sp eno ts ei dtiv ine w anh der te heQ
Kt
∈′,VRs ′×di Rsq (su ×e tr )y ×dre rp er fe es ren tota tt hio en co of ncc au tr er ne an tt iola nye or f,
AppendixB.1. accumulatet drept r∈ esentationsandwillbefurthertruncatedif
thesequencelengths tgreaterthanapredefinedthreshold.
Toadapttothetrainingofimages-basedmulti-turnsVLMs,
×
At the turn t > 0, the question Q is sampled from a set
we can easily convert a steps-based chain to an images- t
of hard prompts (asking model to focus on history), and
basedchainbypartitioningstepsaccordingtothemanip-
theimageI iscroppedfromI andisamplifiedwiththe
ulations returns of images and merge the corresponding t t−1
BicubicInterpolation(Keys,1981).
descriptions(e.g.,basedontheimagesofI ,I ,thechain
0 1
isconvertedintoς (A ,A )inFigure2),
0 1
→ 2.3.2.TRAINING
ς [(I 0,Q),(I 1,A 1),(I 2,A 2),...] (2) TheproposedCogCoM-17Breliesontwomainstagesof
→
training,todevelopthecapabilitiesofgeneralmultimodal
where I t is the manipulation return from the t 1 step, task-solvingaswellastheevidentialvisualreasoning.
−
and A is the sequential concatenation of elements in
t
(desc i(t−1),..,desc i(t)]withtheindexmappingfromttoi. FirstStageofPre-Training Thisstageconsistsoftwo
ordinal sub-phases of training for foundational visual un-
We implement this proposed data production framework
derstandingandgroundedgeneration. Followingthepre-
on3existingdatasetsthatrequiremeticulousrecognition
training of CogVLM (Wang et al., 2023b), we first train
and object counting, TextVQA (Singh et al., 2019), ST-
modelon1.5Bimage-textpairscleanedfromtheLAION-
VQA(Bitenetal.,2019),andTDIUC(Shresthaetal.,2019),
2B(Schuhmannetal.,2022)andCOYO-700M(Byeonetal.,
tobuild70KCoMchains. Thedetailedstatisticsofthedata
2022)with120,000iterationsandbatchsizeof8,192. We
generationareavailableatAppendixB.2.
thentrainmodelon40Mgroundedimage-question-answer
triplescleanedfromLAION-115M(Lietal.,2023c)with
2.3.ModelTraining
60,000iterationsandbatchsizeof1,024,whereeachnoun
2.3.1.ARCHITECTURE phrase in the answer is followed by a list of coordinates
[[x ,y ,x ,y ],...]3 referring the phrase to the grounded
We use the same model architecture as CogVLM (Wang 0 0 1 1
objectsintheimage. Bothphasesadoptthenexttokenpre-
etal.,2023b),ageneralVLMapproachthatinvolvesfour
dictionobjective, andtrainthe6.5Bparametersofvisual
fundamentalcomponents:(1)aVisionEncoder,(2)anMLP
experts.
Adapter, (3) an LLM Backbone, and (4) a Visual Expert
Module, for a reliable multimodal understanding. Con- 3x ,y ∈[000,999]refertothenormalizedpixelcoordinates.
i i
4SecondStageofAlignment Thisstagefurthertrainsthe 3.1.ExperimentsonDetailedVQA
modeltoalignwithhumanpreferencesonsolvingpractical
VLMs have demonstrated the well-known superiority in
visualproblems. WefusetheproducedCoMdatawith3
visualsceneswithsalientcontentunderstanding. Weevalu-
typesofcorpus,includingMultiInstruct(Xuetal.,2022),
atetheeffectivenessofCogCoMonVQAsonmeticulous
LLaVAR (Zhang et al., 2023b), and ShareGPT4V (Chen
understanding,whichtypicallyrequiremodelstoperform
etal.,2023c),referringtheabilitiesofinstruction-following,
multiple actions (find, read) or multiple reasoning steps
texts-recognizing,anddetailed-captioning. Thisfusionre-
(recognizing and then calculating). Following previous
sultsinatotalof570K(I,Q,A)samples,wheretheanswer
studies(Wangetal.,2023b),wetrainourmodelobtained
AinCoMdataconsistsofmultipleturns. Forthetraining
fromthefirst-phaseofstage-1onafusionofdata,including
data of CoM, we randomly prepend the questions with a
aninstructioncorpusofMultiInstruct,13publiclyavailable
lunchingprompt4 PM toquestionsQ=PM+Qasking
VQA datasets (only using training set), a newly created
modelstooptionallyusemanipulationsfortheadaptionof
VQA dataset built through promoting GPT4-V (OpenAI,
expliciteliciting. Weempiricallyshowthatthemodelcan
2023b)forimage-orientedquestion-answergeneration,and
effectivelylearntheevidentialvisualreasoningbyingesting
theCoMcorpus. ThistrainingresultsinageneralistVQA
thisportionofCoMdata. Wetrain14,000iterationswith
modelincorporatingCoMreasoning. ForallexistingVQA
abatchsizeof160,wherethelearningratereaches10−5
tasks, we directly prompt CogCoM with given questions
after280stepsofwarm-upandthendecayslinearly. The
andexaminethecorrectnessofoutputtedanswers.
parametersof6.5Bvisualexpertsaretrainedwiththeobjec-
tiveofnexttokenprediction. Thesetwostagesoftraining
3.1.1.GQA,TEXTVQA,ST-VQA,TALLYVQA
resultinourstandardversionofCogCoMinvolvingboth
chatandreasoningcapabilities. Moretrainingdetailsare Settings The task of visual question answering aims to
availableatAppendixC.2. evaluatethecorrectnessofanswergeneratedbyamodelby
givenaquestiontowardsanimage. GQAisacompositional
3.Experiment VQAbenchmarkwithdiversereasoningquestionscoming
fromsemanticfunctionalprograms. TallyVQAisanobjects
Toquantitativelyvalidatethesuitabilityandefficiencyofthe countingbenchmarkwithhuman-annotatedcomplexcount-
proposedmethod,weconductexperimentson9benchmarks ingquestionsinvolvingchallengingnon-zerocounterparts.
corresponding to 3 categories of multimodal capabilities, TextVQAandST-VQAaretwotextsunderstandingbench-
andanewlyproposedtestbedincorporatingtheevidential marksrequiringmodelstoanswerquestionsthroughtextual
reasoningpathswithakeypoints-awaremetric. Following cuesonimages. Weusetheofficialevaluationscriptsfor
previousworks,wetraintwogeneralistversionsofCogCoM GQAandTallyVQA,whichcalculatetheaccuracyscoreby
foradaptingtothedifferentscenariosofVisualQuestion theExactMatching(EM)betweenmodelpredictionsand
AnsweringandVisualGrounding,andevaluatethestandard answers. ForTextVQAandST-VQA,wesubmitourmodel
versionwithaqualitativeanalysis(Hwangetal.,2023). predictionstotheofficialonlinewebsitesforcalculatingthe
accuracywithVQAScoremetric(Antoletal.,2015).
• Detailed Visual Question Answering. This task
involves models to perform detailed reasoning or
recognition on images. We use 4 prominent bench- Results The experimental results are shown in Table 2.
marks including, GQA (Hudson & Manning, 2019), CogCoMachievesthestate-of-the-artperformanceincom-
TextVQA(Singhetal.,2019),ST-VQA(Bitenetal., parison with all generalist models across the board, and
2019),andTallyVQA(Acharyaetal.,2019). significantlysurpassthebaselineofthepreviousbestmodel
onmultiplebenchmarks. Onthedatasetsrequiringcomplex
• Visual Grounding. Visual grounding evaluates the reasoninganddetailedtextsrecognition,CogCoMobtains
crucialabilitiesofVLMsonmeticulouspositionunder- theresultsthatarecomparabletothespecialistSOTAs.
standing. Weevaluateourmodelon3standardbench-
Specifically, our model improves by 5.97 accuracy score
marks,RefCOCO(Yuetal.,2016),RefCOCO+(Yu
compared to the baseline model CogVLM on the GQA
etal.,2016),andRefCOCOg(Maoetal.,2016).
benchmark,suggestingthatthetrainingincorporatedwith
theevidentialvisualreasoningchainseffectivelyenhance
• Hallucination Examination. We also evaluate our
thegeneralreasoningabilityofmodels. Onthecounting
modelonaspecificbenchmarkPOPE(Lietal.,2023d)
benchmarkofTallyVQA,ourmodeloutperformsthebase-
thatexploresthecrucialissueofvisualobjectshalluci-
lineby4.2and2.1inaccuracyscorewithsimpleandcom-
nation,toinvestigatethehelpfulnessofincorporating
plexquestions,respectively. Thisresultdemonstratesthe
evidentialvisualreasoningwithmodeltraining.
effectivenessoftherigorousgroundedlearningonthesce-
4SeeAppendixC.1forexamples. narios requiring specific objects count calculations. On
5GQA TallyVQA TextVQA ST-VQA
Type Model
test-balanced simple complex test test
Flamingo(Alayracetal.,2022) - - - 54.1 -
GIT(Wangetal.,2022a) - - - 59.8 -
Generalist
GI2(Wangetal.,2022a) - - - 67.3 -
BLIP-2(Lietal.,2023c) 44.7† - - - 21.7
InstructBLIP(Daietal.,2023) 49.5† - - - 50.7†
Qwen-VL(Baietal.,2023) 59.3 - - 63.8 -
CogVLM(Wangetal.,2023b) 65.2 79.8 68.0 69.7 61.0
CogCoM 71.7 84.0 70.1 71.1 70.0
Specialist 72.1 86.0 75.6 71.4 86.0
SOTAs (CFR) (PaLI-X) (PaLI-X) (PaLI-X) (SMoLA)
Table1.PerformanceonVisualQuestionAnsweringbenchmarks,wheretheresultslabeledwith†refertothefew-shotsetting.CogCoM
achievesSOTAacrosstheboard,anddemonstratestheeffectivenessonthevisualreasoningandscenetextsrecognitionbenchmarks.
Settings TomeasurethecorrectnessofCoMchains,we
CogCoM/answer
60.00 CogCoM/explanation introduceakeypoints-awaremetric. Theintuitionisthat
55.59 wecareaboutthekeyelementsincludingactions(i.e.,ma-
nipulationname),targets(i.e.,manipulationinput),andvi-
sualcontents(i.e.,manipulationreturns)ofeachstepinthe
40.00 path,aswellasthelogicalexecutionorderofmanipulations.
Given a pair of chain-answer annotation (c,a) and corre-
30.00 spondingmodelprediction(c′,a′),wefirstsequentiallyex-
tractthekeyelementsfromcandc′toconstructtwoordered
20.00
lists, and then replace the elements in the lists with their
fixedindicesinaBag-of-Elements = c c′ toresultin
7.27 listsofkandk′. WethuscalculatethE escor∪ easthenormal-
Zero shot/answer
2.15 ized Levenshtein Distance s = Levenshtein(k,k′)/N
c
0 2k 4k 6k 8k
Training Steps between the two lists, where N is the maximum length
betweenkandk′. Weadoptthissimplediscretizationstrat-
Figure3.Results shown training CogCoM only using the 70K
egy with low time complexity to concentrate on the key
COM data without original supervised data. CogCoM swiftly
points as well as the solving order. We further consider
achievescompetitiveperformancewithafewtrainingsteps,and
theexplanationscoresincreaseproportionally. the linguistic matching of paragraphs by calculating the
BLEU (Papineni et al., 2002) score between two chains
s = BLEU(c,c′),andthefinalsoreisaweightedcombi-
p
the detailed texts recognition benchmarks TextVQA and nationasacc=(0.6 s c+0.4 s p)/2.
× ×
ST-VQA, CogCoM surpassed the baseline by 1.4 and 9
Results Wetrainourfirst-stagemodelonlyusingthepro-
pointsinaccuracy,respectively. Thisresultdemonstrates
posed70KCoMdatawithoutanyoriginalsupervisionfor
theeffectivenessoftheproposedapproachinsolvingdetails
qualitativelyevaluatetheeffectivenessofthechains. The
recognitionproblem.
resultsareshowninFigure3. The2.15accuracyscorerefers
tothezero-shotperformancebypromptingthefirst-stage
3.1.2.AUTOCOM-TEST
modeldirectly. Wecanseethatbytrainingincorporating
Duetothelackofresourcetoexplorethecorrectnessofthe with the COM chains, our model can swiftly achieve the
evidentialreasoning,weruntheproposeddataproduction competitiveperformancewith48.41accuracyscorewith2k
approachonthevalidationsetofTextVQA,acorpusthat trainingsteps,andobtaintheoptimalresultof55.59with8K
containssufficientquestionsandgoldenanswerstoachieve steps. Theexplanationscoresgraduallyimprovealongwith
885 positive CoM chains. It is noteworthy that eliciting themodelperformance,indicatingthatsuccessfulreason-
CoMchainsdonotraisethedifficultyofansweringvisual ingstepscontributetotheachievingoffinalanswer. Since
questions,butoffersanopportunitytoexaminethecorrect- thisstudycalculatestheexplanationscoreasasequentially
ness of intermediate reasoning process. The distribution matchingofthediscretizedelements,whichover-penalizes
of chains and manipulations of the generated testbed are theapproximateresults(e.g.,similarboxes),furtherresult-
providedinAppendixB.2 inginsmalldiscrepanciesinscoresandindistinctcurves.
6
erocS
ycaruccARefCOCO RefCOCO+ RefCOCOg
Type Model
val test-A test-B val test-A test-B val test
OFA-L*(Wangetal.,2022b) 79.96 83.67 76.39 68.29 76.00 61.75 67.57 67.58
Shikra-7B(Chenetal.,2023b) 87.01 90.61 80.24 81.60 87.36 72.12 82.27 82.19
Shikra-13B(Chenetal.,2023b) 87.83 91.11 81.81 82.89 87.79 74.41 82.64 83.16
Generalist Qwen-VL(Baietal.,2023) 89.36 92.26 85.34 83.12 88.25 77.21 85.58 85.48
CogVLM(Wangetal.,2023b) 92.51 93.95 88.73 87.52 91.81 81.43 89.46 90.09
CogCoM 92.34 94.57 89.15 88.19 92.80 82.08 89.32 90.45
Specialist 92.64 94.33 91.46 88.77 92.21 83.23 89.22 89.37
SOTAs (UNINEXT) (UNINEXT) (UNINEXT) (ONE-PEACE) (ONE-PEACE) (ONE-PEACE) (ONE-PEACE) (UNINEXT-H)
Table2. ResultsonVisualGroundingbenchmarks,wherethespecialistSOTAsarequotedfrom (Baietal.,2023).
Method POPE Results The results are shown in Figure 2. CogCoM
achieves the best performance in 6 out of all 8 sub-sets.
BLIP-2(Lietal.,2023c) -
Giventhatourmodelistrainedonafusionoftheinstruction
Otter(Lietal.,2023a) -
following,visualgrounding,andCoMcorpustogainbroad
MiniGPT4(Zhuetal.,2023) 70.4
applicability,thisresultindicatesthatourmodelexhibitsa
InstructBLIP(Daietal.,2023) 77.3
superiorgroundingabilitieswhileofferspotentialtosolve
LLaVA(Liuetal.,2023b) 66.3
a variety of tasks. In addition, CogCoM achieves perfor-
LLaMA-Adapterv2(Gaoetal.,2023) -
manceonparwiththespecialistSOTAs,demonstratingthat
DreamLLM(Dongetal.,2023) 76.5
incorporatingevidentialreasoningwithgroundingmanipu-
LLaVA-1.5(Liuetal.,2023a) 84.5
lationsintothegeneraltrainingcanenhancethemultimodal
Emu(Sunetal.,2023b) -
capabilitieswhilepreservingthemasteryofgrounding.
CogVLM 87.2
CogCoM 87.8
3.3.ExperimentsonHallucinationExamination
Table3.Evaluation on integrated and hallucination assessment Tofurtherinvestigatethehelpfulnessofincorporatingthe
(adversarialsubset). Weusetheoriginalquestionsastheinput evidentialvisualreasoningonalleviatingmultimodalhal-
promptsforevaluation.
lucinations,wefurtherevaluateCogCoMonPOPE,atypi-
calbenchmarkthatevaluatetheobjectshallucinationissue
facedbyVLMs. WeuseourgeneralistVQAmodelforthis
3.2.ExperimentsonVisualGrounding
evaluation,andobtainmodelpredictionsbydirectlyasking
Thetaskofvisualgroundingrequiresmodelstoprecisely thecorrespondingquestionsintheoriginaldataset.
providethecorrespondingcoordinatesofregionsinanim-
age based on the given target expression, which is a cru-
Settings POPEaimstoinvestigatetheobjectshallucina-
cial aspect of detailed position understanding of VLMs
tionofVLMsbyaskingtheexistenceofobjectsinanimage
and is attracted extensive attention. Following the exist-
withbinaryquestions(i.e.,yes/no). Weusethechallenging
ingwork(Wangetal.,2023b),wetrainourmodelobtained
adversarial version of the dataset for standard evaluation.
bythefirststageonafusionofdatasets,includinganinstruc-
Forthecalculationofscores,weusetheofficialevaluation
tioncorpusMultiInstruct,ahigh-qualitygroundingVQA
scriptstocalculatetheF scoreofpredictions.
1
corpusintroduceinCogVLM,andtheproposedCoMdata.
Thistrainingresultsinageneralistgroundingmodelthat
Results TheexperimentalresultsonthePOPEbenchmark
isexcellingatvisualgroundingwhilecapableofreasoning.
areshowninTable3. WecanseethatCogCoMachievesthe
Forallbenchmarks,wepromptCogOMinachatmanner
superiorperformanceincomparisonwithpreviousVLMs
toaskthemodeltoprovidegroundedcoordinates,suchas
onthebenchmark,demonstratingtheimprovements.Specif-
“Whereis expr answerin[x0,y0,x1,y1]format.”,where
⟨ ⟩ ically,CogCoMimprovestheperformanceandachievesthe
the expr referstothetargetexpression.
⟨ ⟩ superiorresultonPOPincomparisonwithbaselineandthe
Settings WeevaluateCogCoMonthreewell-established previous VLMs. As we train CogCOM with a fusion of
visualgroundingbenchmarks. Weusethestandardevalua- diversifiedcapabilities,thisresultssuggestthatourmodel
tionmetric,thatconsidersapredictionascorrectwhenthe performs well on general multimodal tasks with a lower
intersection-over-union(IoU)betweenthepredictedbound- susceptibilitytohallucination,whilepreservingVQA,in-
ingboxandgroundtruthisgreaterthan0.5. structionfollowing,andreasoningabilities.
7Q: What is the brand of the plane? Q: What time does this photo show? Q: what is the smallest part of the pie chart?
A: AirFrance A(cid:29)(cid:3)1:45 A: pa hospital
Using grounding(the plane) to find the plane in current Applying grounding(the clock) to find the clock in this Based on grounding(the pie chart) to find the pie chart
image, and there are two regions [[255,217,312,245], image, resulting in the region of [[814, 531, 940,1062]]. in current picture, and the region that I found is
[193,113,481,208]] referring planes that can be found. [[534,203,847,406 ]].
Use crop_and_zoomin([[814, 531, 940,10
As the plane in [[193,113,481,208]] is the most notable 62]], 2) to crop this image along region [[81 Using grounding(the smallest part of the pie chart in
one, using grounding(the logo in region 4, 531, 940,1062]] and then zoom in it by [[534,203,847,406]]) to find the region of the smallest
[[193,113,481,208]]) to locate the logo of the plane in two times of its original size to yield a new part, and the result is [[554,369,693, 737]].
given region, resulting in [[246,142,307,163]]. image, and re-input this new image.
Using crop_and_zoomin([[534,203,847,406
Using crop_and_zoomin( [[246,142,307,16 Using read_time(the clock in the new image) to ]], 3) to crop this picture and then resize the
3]],3)to crop the current image and magnify recognize the time on the clock, and the result is 1:45. cropped by 3 times to get a new image.
it by three times over to create a new image.
Recognize the content in the new image, and the final
Recognize the brand name of the logo in the new image, result is "pa hospital".
resulting in AirFrance.
Q: How many wheels of the truck can be seen in this picture? Q: What might be the name of one team in this match? Q: what is the name of the song on the sheet of music?
A: ten A: Dbacks A: across the distance
Using grounding(all individual wheels of the truck) to Using grounding(the name of teams) to find regions Using grounding(the sheet of music) to localize the
locate all individual wheels in this picture, resulting in the corresponding to the name of teams in current image, sheet of music in this picture, and the resulting region is
boxes of [[56, 256, 96, 513], [105, 240, 144, 480], [223, resulting in [[0, 199, 91, 397], [368, 83, 438, 166], [445, [[ 0,348,788,696]].
238, 249, 475], [253, 238, 281, 476], [301, 229, 331, 458], 81, 497, 161]].
[338, 226, 364, 452], [367, 227, 379, 454], [382, 228, 397, Using grounding(the title of the sheet in region
457], [395, 228, 411, 457], [416, 222, 443, 444]]. In comparison with other two regions, the [0,99,91,397] [[0,348,788,696]]) to focus on the title of the sheet in the
should include the name of one of the teams. corresponding region, and the region that should be
Based on the identified boxes of wheels, the number of focused on is [[641,498,682,995 ]].
visible wheels of the truck in this picture is ten. Using crop_and_zoomin([[0,99,91,397]], 2)
to crop the current image and resize it by Adopt crop_and_zoomin([[641,498,682,995
two times to get new image, and re-input it. ]], 2) to crop this region and zoom in it two
times over, resulting in a new image.
Identify the text referring to the team name in the new
image, resulting in Dbacks. Recognize the name of the song by looking at the new
image, and the answer is across the distance.
Figure4.CogCoMperforms reasoning withdetailsrecognition,readingtime,understandingcharts,countingobjects,andreadingtexts.
3.4.QualitativeAnalysis 4.Conclusion
WeinvestigatetheevidentialreasoningcapabilityofCog- Thispaperstudytheproblemspresentedbytheconclusive-
CoMonscenariosthatrequiresdifferenttypesofmeticulous alignment training of Vision-Language Models (VLMs),
reasoning, including recognizing textual details, reading andproposeageneralmechanism,ChainofManipulations
time,understandingchartsandcountingobjects. Theresults (CoM),thatenablesVLMstosolveproblemsbyactively
are shown in Figure 4. The first case demonstrates that manipulating visual inputs to acquire essential contents.
CogCoMfindstheregioncorrespondingtotheplanelogo Werealizethismethodologybybuildinganefficientdata
throughtwostepsofgroundingandthenachievestheanswer production framework that engages linguistic and visual
basedonzoominginthecroppedregion. Thesecondcase annotatorstosynthesisCoMchains,andamemory-based
illustratestheabilityofCogCoMinreadingtime,bylocat- architecturewhichiscompatiblewithexistingmodels. A
ingthedevicethatdisplaystimeandthentransformingthe 17BVLM,CogCoMwiththememory-basedarchitecture
timeintowordsbasedontheread timnemanipulation. In istrainedonthedatafusionincorporatingtheimplemented
theforthexample,CogCoMfirstidentifiesallvisibletruck CoM chains. Experiments with quantitative results on 8
wheelsthroughgrounding,andthenleveragesthecounting benchmarksandqualitativeanalysisdemonstratetheeffec-
manipulationtoconsolidatethetotalnumber. tivenessofthemethodinsolvingdetailedvisualproblems.
85.Limitations fortraininglargeautoregressivevision-languagemodels.
arXivpreprint,2023.
Though we try to develop an accurate and robust frame-
workthatengagesremarkableLLMtoprovidebasicsolving Bai,J.,Bai,S.,Yang,S.,Wang,S.,Tan,S.,Wang,P.,Lin,J.,
steps,adoptsreliablevisualtoolstoobtainvisualcontents, Zhou,C.,andZhou,J. Qwen-vl: Afrontierlargevision-
andthenacquiresfeasible pathsbasedontraversal, there languagemodelwithversatileabilities. arXivpreprint,
are still limitations in our methodology that we hope to 2023.
improveinthefuture. First, Wefindthatthediversityof
linguisticsolvingstepsisinsufficient, andtheinaccuracy Biten,A.F.,Tito,R.,Mafla,A.,Gomez,L.,Rusinol,M.,
of visual tools (e.g., the rough granularity of grounding Valveny,E.,Jawahar,C.,andKaratzas,D. Scenetextvi-
boxes, OCR failures on slant letters) will lead to a large sualquestionanswering.InProceedingsoftheIEEE/CVF
amountofnegativepaths(effectivelyutilizingthesepaths internationalconferenceoncomputervision,2019.
wouldbeneficial). Wesuggesttopromotetheselimitations
Byeon,M.,Park,B.,Kim,H.,Lee,S.,Baek,W.,andKim,
withdedicatepromptsandimprovedvisualtools. Second,
S. Coyo-700m: Image-textpairdataset,2022.
ourcurrentmodelre-inputthemanipulatedimageswitha
setofhardprompts, whichmaybringspeedlosses. This
Changpinyo,S.,Sharma,P.,Ding,N.,andSoricut,R. Con-
isexpectedtobeimprovedbyimplementingthephysical
ceptual12m: Pushingweb-scaleimage-textpre-training
manipuationsintothecalculationsinvectorspace.
to recognize long-tail visual concepts. In Proceedings
of the IEEE/CVF Conference on Computer Vision and
6.Impacts PatternRecognition,2021.
Thisworkpresentsageneralvisualreasoningmechanism Chen, D., Liu, J., Dai, W., and Wang, B. Visual in-
thatalleviatetheproblemscausedbyexistingconclusion- struction tuning with polite flamingo. arXiv preprint
alignmenttrainingforVLMs,introducesadataproduction arXiv:2307.01003,2023a.
frameworkinvolvingLLMsandvisualtoolsasreliablean-
notators, and devises a memory-based compatible VLM Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and
architecture. Weexpectthisworktobringthreebenefitsto Zhao,R. Shikra:Unleashingmultimodalllm’sreferential
thecommunity. First,theproposedvisualreasoningmecha- dialoguemagic. arXivpreprint,2023b.
nismmaypushtheprogressofVLMsinsolvingcomplex
Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J.,
visual problems. Second, the introduced data production
Zhao, F., and Lin, D. Sharegpt4v: Improving large
frameworkmaybeappliedtowidespreadtrainingscenarios
multi-modalmodelswithbettercaptions. arXivpreprint
topromotethedevelopmentofcurrentdata-drivenmachine
arXiv:2311.12793,2023c.
learning. Third,wehopethatthememory-basedarchitec-
turewillbehelpfulforVLMsinmulti-turnlongcontexts.
Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A.,
Padlewski, P., Salz, D., Goodman, S., Grycner, A.,
References Mustafa, B., Beyer, L., et al. Pali: A jointly-scaled
multilingual language-image model. In The Eleventh
Acharya,M.,Kafle,K.,andKanan,C. Tallyqa: Answering
InternationalConferenceonLearningRepresentations,
complexcountingquestions. InProceedingsoftheAAAI
2022.
conferenceonartificialintelligence,2019.
Chen,X.,Djolonga,J.,Padlewski,P.,Mustafa,B.,Chang-
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,
pinyo,S.,Wu,J.,Ruiz,C.R.,Goodman,S.,Wang,X.,
Hasson,Y.,Lenc,K.,Mensch,A.,Millican,K.,Reynolds,
Tay,Y.,etal. Pali-x: Onscalingupamultilingualvision
M.,etal.Flamingo:avisuallanguagemodelforfew-shot
andlanguagemodel. arXivpreprint,2023d.
learning. Advances in Neural Information Processing
Systems,2022. Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
H.,Zheng,L.,Zhuang,S.,Zhuang,Y.,Gonzalez,J.E.,
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., etal. Vicuna: Anopen-sourcechatbotimpressinggpt-4
Zitnick, C. L., and Parikh, D. Vqa: Visual question with90%*chatgptquality. Seehttps://vicuna.lmsys.org
answering. In Proceedings of the IEEE international (accessed14April2023),2023.
conferenceoncomputervision,2015.
Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li,
Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., B.,Fung,P.,andHoi,S. Instructblip: Towardsgeneral-
Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, purposevision-languagemodelswithinstructiontuning.
S., et al. Openflamingo: An open-source framework arxiv2023. arXivpreprintarXiv:2305.06500.
9Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, Li,B.,Zhang,Y.,Chen,L.,Wang,J.,Pu,F.,Yang,J.,Li,C.,
B.,Fung,P.,andHoi,S. Instructblip: Towardsgeneral- andLiu,Z. Mimic-it: Multi-modalin-contextinstruction
purposevision-languagemodelswithinstructiontuning. tuning. arXivpreprintarXiv:2306.05425,2023a.
arxiv2023. arXivpreprint,2023.
Li,B.,Zhang,Y.,Chen,L.,Wang,J.,Yang,J.,andLiu,Z.
Dong,R.,Han,C.,Peng,Y.,Qi,Z.,Ge,Z.,Yang,J.,Zhao, Otter: Amulti-modalmodelwithin-contextinstruction
L., Sun, J., Zhou, H., Wei, H., et al. Dreamllm: Syn- tuning. arXivpreprintarXiv:2305.03726,2023b.
ergisticmultimodalcomprehensionandcreation. arXiv
Li,J.,Li,D.,Savarese,S.,andHoi,S.Blip-2:Bootstrapping
preprintarXiv:2309.11499,2023.
language-imagepre-trainingwithfrozenimageencoders
Du,Y.,Li,C.,Guo,R.,Yin,X.,Liu,W.,Zhou,J.,Bai,Y., andlargelanguagemodels. arXivpreprint,2023c.
Yu,Z.,Yang,Y.,Dang,Q.,etal. Pp-ocr: Apracticalultra
Li,Y.,Du,Y.,Zhou,K.,Wang,J.,Zhao,W.X.,andWen,
lightweightocrsystem.arXivpreprintarXiv:2009.09941,
J.-R. Evaluating object hallucination in large vision-
2020.
language models. arXiv preprint arXiv:2305.10355,
Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., 2023d.
Zhang,W.,Lu,P.,He,C.,Yue,X.,etal. Llama-adapter
Li,Y.,Zhang,C.,Yu,G.,Wang,Z.,Fu,B.,Lin,G.,Shen,
v2: Parameter-efficientvisualinstructionmodel. arXiv
C.,Chen,L.,andWei,Y. Stablellava: Enhancedvisual
preprint,2023.
instructiontuningwithsynthesizedimage-dialoguedata.
Hong,W.,Wang,W.,Lv,Q.,Xu,J.,Yu,W.,Ji,J.,Wang, arXivpreprintarXiv:2308.10253,2023e.
Y., Wang, Z., Dong, Y., Ding, M., et al. Cogagent: A
Lin,T.-Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ra-
visual language model for gui agents. arXiv preprint
manan,D.,Dolla´r,P.,andZitnick,C.L. Microsoftcoco:
arXiv:2312.08914,2023.
Commonobjectsincontext. InComputerVision–ECCV
Huang,K.-H.,Zhou,M.,Chan,H.P.,Fung,Y.R.,Wang,Z., 2014: 13thEuropeanConference,Zurich,Switzerland,
Zhang,L.,Chang,S.-F.,andJi,H. Dolvlmsunderstand September6-12,2014,Proceedings,PartV13,2014.
charts? analyzingandcorrectingfactualerrorsinchart
Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved base-
captioning. arXivpreprintarXiv:2312.10160,2023a.
lines with visual instruction tuning. arXiv preprint
Huang,S.,Dong,L.,Wang,W.,Hao,Y.,Singhal,S.,Ma, arXiv:2310.03744,2023a.
S., Lv, T., Cui, L., Mohammed, O. K., Liu, Q., et al.
Liu, H., Li, C., Wu, Q., andLee, Y.J. Visualinstruction
Languageisnotallyouneed: Aligningperceptionwith
tuning. arXivpreprint,2023b.
languagemodels. arXivpreprint,2023b.
Liu,S.,Zeng,Z.,Ren,T.,Li,F.,Zhang,H.,Yang,J.,Li,C.,
Hudson,D.A.andManning,C.D. Gqa: Anewdatasetfor
Yang,J.,Su,H.,Zhu,J.,etal. Groundingdino: Marry-
real-worldvisualreasoningandcompositionalquestion
ingdinowithgroundedpre-trainingforopen-setobject
answering. InProceedingsoftheIEEE/CVFconference
detection. arXivpreprintarXiv:2303.05499,2023c.
oncomputervisionandpatternrecognition,2019.
Mao,J.,Huang,J.,Toshev,A.,Camburu,O.,Yuille,A.L.,
Hwang, A., Head, A., and Callison-Burch, C. Grounded
andMurphy,K. Generationandcomprehensionofunam-
intuitionofgpt-vision’sabilitieswithscientificimages.
biguousobjectdescriptions. InProceedingsoftheIEEE
arXivpreprint,2023.
conferenceoncomputervisionandpatternrecognition,
Jia,C.,Yang,Y.,Xia,Y.,Chen,Y.-T.,Parekh,Z.,Pham,H., 2016.
Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up
OpenAI,R. Gpt-4technicalreport.arxiv2303.08774. View
visualandvision-languagerepresentationlearningwith
inArticle,2023a.
noisy text supervision. In International conference on
machinelearning,2021. OpenAI,R. Gpt-4v(ision)systemcard. Citekey: gptvision.,
2023b.
Keys,R. Cubicconvolutioninterpolationfordigitalimage
processing. IEEEtransactionsonacoustics,speech,and Ordonez,V.,Kulkarni,G.,andBerg,T. Im2text:Describing
signalprocessing,1981. imagesusing1millioncaptionedphotographs. Advances
inneuralinformationprocessingsystems,2011.
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,
Kravitz,J.,Chen,S.,Kalantidis,Y.,Li,L.-J.,Shamma, Papineni, K., Roukos, S., Ward, T., andZhu, W.-J. Bleu:
D.A.,etal. Visualgenome: Connectinglanguageand a method for automatic evaluation of machine transla-
visionusingcrowdsourceddenseimageannotations. In- tion. InProceedingsofthe40thannualmeetingofthe
ternationaljournalofcomputervision,2017. AssociationforComputationalLinguistics,2002.
10Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, Wang,W.,Lv,Q.,Yu,W.,Hong,W.,Qi,J.,Wang,Y.,Ji,
S.,andWei,F. Kosmos-2: Groundingmultimodallarge J.,Yang,Z.,Zhao,L.,Song,X.,etal. Cogvlm: Visual
languagemodelstotheworld. arXivpreprint,2023. expert for pretrained language models. arXiv preprint,
2023b.
Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk,
R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,
Komatsuzaki, A. Laion-400m: Open dataset of clip- Khashabi,D.,andHajishirzi,H. Self-instruct: Aligning
filtered 400 million image-text pairs. arXiv preprint languagemodelwithselfgeneratedinstructions. arXiv
arXiv:2111.02114,2021. preprintarXiv:2212.10560,2022c.
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wu, P. and Xie, S. V*: Guided visual search as a
Wightman,R.,Cherti,M.,Coombes,T.,Katta,A.,Mullis, core mechanism in multimodal llms. arXiv preprint
C., Wortsman, M., et al. Laion-5b: An open large- arXiv:2312.14135,2023.
scaledatasetfortrainingnextgenerationimage-textmod-
els. AdvancesinNeuralInformationProcessingSystems, Xu, Z., Shen, Y., and Huang, L. Multiinstruct: Improv-
2022. ingmulti-modalzero-shotlearningviainstructiontuning.
arXivpreprintarXiv:2212.10773,2022.
Sharma,P.,Ding,N.,Goodman,S.,andSoricut,R. Con-
ceptualcaptions: Acleaned,hypernymed,imagealt-text Yin,S.,Fu,C.,Zhao,S.,Xu,T.,Wang,H.,Sui,D.,Shen,Y.,
datasetforautomaticimagecaptioning. InProceedings Li,K.,Sun,X.,andChen,E. Woodpecker: Hallucination
ofthe56thAnnualMeetingoftheAssociationforCom- correctionformultimodallargelanguagemodels. arXiv
putationalLinguistics(Volume1: LongPapers),2018. preprintarXiv:2310.16045,2023.
Shazeer, N. Glu variants improve transformer. arXiv Yu,L., Poirson,P.,Yang,S., Berg,A.C.,andBerg,T.L.
preprintarXiv:2002.05202,2020. Modelingcontextinreferringexpressions. InComputer
Vision–ECCV2016: 14thEuropeanConference,Amster-
Shrestha, R., Kafle, K., andKanan, C. Answerthemall!
dam, The Netherlands, October 11-14, 2016, Proceed-
towarduniversalvisualquestionansweringmodels. In
ings,PartII14,2016.
ProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,2019. Yu, Q., Li, J., Wei, L., Pang, L., Ye, W., Qin, B., Tang,
S.,Tian,Q.,andZhuang,Y. Hallucidoctor: Mitigating
Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,
hallucinatory toxicity in visual instruction data. arXiv
Batra, D., Parikh, D., and Rohrbach, M. Towards vqa
preprintarXiv:2311.13614,2023.
modelsthatcanread. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition, Zeng, Y., Zhang, H., Zheng, J., Xia, J., Wei, G., Wei, Y.,
2019. Zhang,Y.,andKong,T. Whatmattersintrainingagpt4-
style language model with multimodal inputs? arXiv
Sun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. Eva-
preprintarXiv:2307.02469,2023.
clip: Improvedtrainingtechniquesforclipatscale. arXiv
preprintarXiv:2303.15389,2023a.
Zhang,S.,Sun,P.,Chen,S.,Xiao,M.,Shao,W.,Zhang,W.,
Chen,K.,andLuo,P. Gpt4roi: Instructiontuninglarge
Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y.,
language model on region-of-interest. arXiv preprint
Gao, H., Liu, J., Huang, T., and Wang, X. Generative
arXiv:2307.03601,2023a.
pretraininginmultimodality. arXivpreprint,2023b.
Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Zhang,Y.,Zhang,R.,Gu,J.,Zhou,Y.,Lipka,N.,Yang,D.,
Z., Liu, C., and Wang, L. Git: A generative image-to- andSun,T. Llavar: Enhancedvisualinstructiontuning
texttransformerforvisionandlanguage. arXivpreprint fortext-richimageunderstanding. arXivpreprint,2023b.
arXiv:2205.14100,2022a.
Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.
Wang,P.,Yang,A.,Men,R.,Lin,J.,Bai,S.,Li,Z.,Ma,J., Minigpt-4: Enhancing vision-language understanding
Zhou,C.,Zhou,J.,andYang,H. Ofa: Unifyingarchitec- with advanced large language models. arXiv preprint,
tures,tasks,andmodalitiesthroughasimplesequence-to- 2023.
sequencelearningframework. 2022b.
Wang,W.,Chen,Z.,Chen,X.,Wu,J.,Zhu,X.,Zeng,G.,
Luo,P.,Lu,T.,Zhou,J.,Qiao,Y.,etal. Visionllm: Large
languagemodelisalsoanopen-endeddecoderforvision-
centrictasks. arXivpreprint,2023a.
11A.RelatedWorks
A.1.LargeVision-LangaugeModelsasFoundations
Most of LVLMs rely on the training on publicly available image-caption pairs, including ALIGN (Jia et al., 2021),
MSCOCO(Linetal.,2014),VG(Krishnaetal.,2017),CC3M(Sharmaetal.,2018),CC12M(Changpinyoetal.,2021),
SBU(Ordonezetal.,2011),LAION2B(Schuhmannetal.,2022),LAION400M(Schuhmannetal.,2021). Startingfrom
Flamingo (Alayrac et al., 2022), a series of LVLMs have focused on training the adaptation layers to align the visual
representationtothefrozenLLMsonamixtureofimage-textpairswiththeabovecorpus,includingBLIP2(Lietal.,2023c),
KOSMOS(Huangetal.,2023b),andOpenFlamingo(Awadallaetal.,2023). Inspiredbysuccessofinstructiontuningin
LLMs(Wangetal.,2022c),alineofworkshavedevotedeffortstobuildvision-orientedinstruction-answerpairsthrough
GPT4andtrainmodelsforimitation,suchasLLAVA(Liuetal.,2023b),Otter(Lietal.,2023b),VisionLLM(Wangetal.,
2023a),MultiInstruct(Xuetal.,2022),Lynx(Zengetal.,2023),InstructBLIP(Daietal.),CleverFlamingo(Chenetal.,
2023a)andStableLLaVA(Lietal.,2023e). Recently,researchershaveproventheefficiencyofdevelopingLVLMswith
twostagesoftraining,thefirststageofabundantpretrainingonimage-captionpairsandthesecondstageofalignmenton
image-question-answertriples,suchasPALI(Chenetal.,2022),PaLI-X(Chenetal.,2023d),Qwen-VL(Baietal.,2023),
andCogVLM(Wangetal.,2023b).
A.2.LargeVision-LanguageModelswithReasoning
TofurtherenhancetheabilityofLVLMsinsolvinghigh-levelvisualproblems,researchfocusingonvariousaspectsof
reasoningisattractingbroadattention. Wesimplydivideexistingstudiesintotreebroadcategories. Thefirstlineofresearch
focusonenhancetrainmodelswithamasteryofcross-modalgroundedreasoning,wheregroundedinstruction-following
supervisionisbuildthroughpublicvisualgroundingdatasetorGPT4-Vfortraining,includingKOSMOS-2(Pengetal.,
2023),Shikra(Chenetal.,2023b),andGPT4ROI(Zhangetal.,2023a). Thesecondaspectofeffortshavebeendevoted
intopromotingmodelstounderstandartificialvisualscenes,suchasfigures,charts,andreceipts. Thesestudiesincludes
CogAgent(Hongetal.,2023)andCHARTVE(Huangetal.,2023a). Someotherstudiesaddressthecrucialproblemof
hallucinationinLVLMswithcounterfactualorinterpretablereasoning(Yuetal.,2023;Yinetal.,2023). V*(Wu&Xie,
2023)alsocontributeseffortstoenhancethedetailsrecognitionofVLMsbasedtheLLM-guidedsearchingprocess.
12B.DetailsofDataProduction
In this section, we further introduce the details of CoM data production, with the overall algorithm of a pseudo code,
anexampleofthesolvingstepsgenerationwithLLMandcorrespondingguideline,anexampleofthereasoningchains
completion with visual tools. We also list the details of data statistics for the synthesised training data as well as the
evaluationdataofAutoCoM-test,followedbyalimitationanalysisforthecurrentdataproductionmethod.
B.1.TheAlgorithmforAReproducibleDataProduction
We provide the pseudocode of the CoM synthesis algorithm to clearly explain the process of data generation, thereby
facilitatingunderstandingandreproduction1.
Algorithm1SynthesisingChainofManipulations

Manipulations: f :x y f

{
i
→ |
i
∈M}
1: Define: LinguisticAnnotator :Ψ //WeuseGPT4inthiswork
L
VisualAnnotator
:Ψ //WeusePaddleOCRandGroundingDINOinthiswork
V
2: Input: ImageI,QuestionQ,AnswerA
3: //LinguisticAnnotation
4: PromptΨ withguidelinePLtogeneratereasoningsteps:
L
(cid:40)
ς =(steps ,steps ,...)
ς =Ψ (QPL), where 1 2 (4)
L
| steps =(f ,desc )
i i i
5: Definetree
T
6: fori=1to ς do
| |
7: Extractx ,y instantiatedwithf instep
i i i i
8: ExtractreferentialboxesBfromx
i
9: forbinBdo
10: LeverageΨ toacquirecorrespondingvisualcontenty′ =Ψ(x I,b),andapplyy totree
V i i | i
.level[i].append(y ) (5)
i
T
11: endfor
12: endfor
13: Traverse toobtainpositivechainsthatleadstogivenanswerwithterminalreturn
T
[ς ,ς ,...]=DFS( A) (6)
1 2
T|
14: Return[ς ,ς ,...]
1 2
13B.2.DataStatistics
WedevelopastrategytoextractpredicatephrasesbasedconstituencyparsingwithStandordCoreNLP,inwhichweextract
verb,conjunction-connectedverbphrase,preposition-connectedverbphrase.
BesidesthestandardCoMdataincorporatingmanipulationswithexplicitvisualevidences,theproposeddatasynthesising
frameworkiscompatibleofproducingimplicitvisualreasoningstepsstep′ =(desc )withoutinvolvingthemanipulations.
i i
WetherebyalsobuildthispartialCoMdataonthecorpusconsistingofabsurdvisualquestions(i.e.,askingunanswerable
questionsbasedonthegivenimage)tofurtherresistthetoxichallucinations. Specifically,givenanimageI withaquestion
Q,wepromptGPT-4V(OpenAI,2023b)tosolvethequestionstep-by-steptoacquirethereasoningchains.
DataSource #QAs #Chains #Steps/Chain #ManipulationsTypes/Chain
TextVQA(Bitenetal.,2019) 10782 13766 2.93 2.41
ST-VQA(Singhetal.,2019) 4814 3959 2.88 2.43
TDIUC-count(Shresthaetal.,2019) 53547 54523 2.35 0.74
TDIUC-absurd(Shresthaetal.,2019) 11677 11677 4.09 -
AutoCoM-test 4609 8612 3.26 2.18
Table4. DetailedstatisticsthethetrainingdataandevaluationdatasynthesisedwithCoMproduction.
0.06
0.05
0.04
0.03
0.02
0.01
0.00
locate r ie d da ed en tt eif ry m li on oe k o f bo sr e ar nv ae el xyz ae min ce o zu on ot cm o min pare ff oi cn ud s on us ze o eo xm it nr ta rec rt ep cr o se g et n ai rz ce zh of oomr on n lot oe ro ek
u
s na et da er rc sh ta prn od vidme ake li cst h ee nc hk a cn oc ne mdu ec a ct s hu er ce k fmor at ac nh sw coer n cs oul nt v ce or nt dfi er sm cr ii nb s ce p ale cc ut late look ope sn ta nt oe r te ef e dr co ow nn sid ge ar et sh tie tmr raat ne sla rt ee cord
Figure5.Distributionofthetop-50generatedmanipulationsoutoftotal465basedon5-shotprompting,wherethefirstthreebarsare
scaledwith20%forasmoothvisualizationofalldata.
14B.3.DetailsofLinguistic/VisualAnnotations
Inthiswork,weadopttheGPT4-turboasthelinguisticannotatorforgeneratingproblemssolvingsteps,andtheAPIcallwas
conductedduringtheperiodof2023.9-2023.12. Forthevisualannotators,weleveragethethecurrentlybest-performing
tools,GroundingDINOandPaddleOCR,toacquireallvisualcontentsrequestedbythemanipulations.Foracleardescription
totheproductionsettingandresults,weillustratetheguidingprompt,andanexample-basedlinguisticannotationresultsas
wellasthevisualannotationresultsinFigure6.
B.4.LimitationAnalysis
Fortheimplementeddataframework,weengagetheremarkableLLMtoprovidebasicsolvingsteps,adopttworeliable
visualtools(i.e.,GroundingDINOandPaddleOCR)toacquirecorrespondingvisualcontents,andthenperformthetraversal
toachievefeasiblereasoningpaths,whichensuresthecorrectnessandrobustnessofdatasynthesizing. However,wealso
findthattherearethreemajorlimitationscausedbytheemployedmodelsandcouldbeimprovedinfuture:
• Thelackofdiversityinlinguisticreasoningsteps. The5-shotpromptingtotheGPT-4gainsastablesolvingsteps,but
italsoresultsinthedescriptionsforexecutingmanipulationsorgeneralthinkingaresimilar. Wesuggestthatthiscan
beaddressedbyemployingdiversifiedpromptsorrequirements.
• Theinaccuracyofvisualtools. Wefindthatthereareaconsiderableamountofnegativepathscausedbythefailures
ofvisualtools, suchastheroughgranularityofboundingboxesandtheerrorrecognitionofslatedlettersorlong
sentences. Thisissuecanberelievedbyimprovingthesemanticunderstandingcapabilitiesofvisualtools.
C.DetailsofTraining
C.1.LaunchingPrompts
• Please solve the problem gradually via a chain of manipulations, where in each step you can selectively
adopt one of the following manipulations GROUNDING(a phrase) boxes, OCR(an image or a region) texts,
→ →
CROP AND ZOOMIN(aregionongivenimage) new image,CALCULATE(acomputabletarget) numbers,or
→ →
inventanewmanipulation,ifthatseemshelpful. QUESTION
{ }
• Pleasetackleagivenquestioninastepbystepmanner. Foreachsteponeofthefollowingmanipulations(depictedas
Name(Input) Retrun)canbeoptionallyused: GROUNDING(aphrase) boxes,OCR(animageoraregion) texts,
→ → →
CROP AND ZOOMIN(aregionongivenimage) new image,CALCULATE(acomputabletarget) numbers,or
→ →
developanewmanipulationyourself(ifitisindeedrequired). QUESTION
{ }
• Pleasegothroughthequestionincrementallywithchainofmanipulations(optionallyusemanipulationwhenneeded)
suchasGROUNDING(aphrase) boxes,OCR(animageoraregion) texts,CROP AND ZOOMIN(aregionon
→ →
givenimage) new image,CALCULATE(acomputabletarget) numbers,andcreateanewmanipulationifnecessary.
→ →
QUESTION
{ }
C.2.Trainingsettings
Parameters Stage1-1 State1-2 Stage-2
Objective nexttokenprediction nexttokenprediction nexttokenprediction
Images 1.5B 40M 570K
Batchsize 8192 1024 160
Iterations 120,000 60000 14000
Optimizer AdamW AdamW AdamW
Learningrate 1e-4 1e-5 1e-5
Warmupsteps 7200 1200 280
Trainableweights 6.5Bvisualexpert 6.5Bvisualexpert 6.5Bvisualexpert
Table5. Trainingdetailsofallstages.
D.DetailsofRepresentativeCases 15i-th OCR manipulation, that recognize the natural texts written on the target `tgt`, and return the recognized
OCR_i(tgt)->txt_i: texts `txt_i`.
calculate(tgt)->res_i: i-th calculate manipulation, that calculate the formula specified by the target `tgt` in current image, and return
the calculation result `res_i`.
grounding_i(tgt)->bbx_i: i-th grounding manipulation, that locates the object(s) specified by the target noun phrase `tgt` in current
image, and return the resulting bounding box(es) as `bbx_i` where each box is represented by the top-left
and bottom-right coordinates.
crop_and_zoomin_i(bbx)->img_i: i-th crop_and_zoomin manipulation which is useful to identify small and subtle objects in image, that first crops
the current image using the box `bbx` defined by the top-left and bottom-right coordinates, and then zoom in
the cropped image by two times and finally return the resulting image `img_i`.
You are a visual assistant capable of generating solving steps for image-oriented visual questions. In each step, you can optionally use a
manipulation to operate the image, which can be used to acquire specific information from the image or to acquire the processed new image
(please be aware that these manipulations will not actually be performed when you generate the solving steps). The manipulation can be one from
the predefined ones, or can be a new one you create yourself (should there indeed be a need), where the predefined manipulations with their
descriptions are listed below:\n\n{MANIPULATIONS}.\n\nGiven a question Q abount an image, please generate a series of essential solving
steps, where the output of each step is a tuple consisting of a Manipulation (leave it to None if the current step doesn't involve any manipulation)
and a Description: (1) Manipulation f(x)->y, that is the manipulation `f` targeting `x` to obtain specific information or image `y`; (2) Description,
which is a sentence describing the current solving step.\n Please adhere the following format: given an input of 'Q: xxx. The essential solving
steps are: ', the output should like 'Step 1: (Manipulation, Description); Step 2: (Manipulation, Description); ...'.\n\n There are several examples:\n
{DEMONSTRATIONS}\n\n Q: {QUESTION} The essential solving steps are:
Manipulations Definition and Linguistic Annotation Guideline
I<latexit sha1_base64="Eeipg0SaSzO/bMIo2fhu3xEhWXY=">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0owhSsWkLtUgyndbYvEwmYi2u/AG3+mPiH+hfeGdMQS2iE5KcOfeeM3PvdSLPTYRhvOa0icmp6Zn8bGFufmFxqbi8Uk/CNGbcYqEXxk3HTrjnBtwSrvB4M4q57Tsebzj9Axlv3PA4ccOgJgYRb/t2L3C7LrMFUfWjk6pVO7soloyyoZY+DswMlJCtalh8wTk6CMGQwgdHAEHYg42EnhZMGIiIa2NIXEzIVXGOexRIm1IWpwyb2D59e7RrZWxAe+mZKDWjUzx6Y1Lq2CBNSHkxYXmaruKpcpbsb95D5SnvNqC/k3n5xApcEvuXbpT5X52sRaCLPVWDSzVFipHVscwlVV2RN9e/VCXIISJO4g7FY8JMKUd91pUmUbXL3toq/qYyJSv3LMtN8S5vSQM2f45zHNS3yuZO2TzdLlX2s1HnsYZ1bNI8d1HBIaqwyPsKj3jCs3asXWu32t1nqpbLNKv4trSHD4zXkSA=</latexit>NPUTS Question: What number police station is on the building?
Step 1: grounding_1(the building)->bbx_1, Locate the building in the image and return the bounding box of the building as `bbx_1`.
Step 2: (grounding_2(number police station on Identify the number of the police station on the building in box `bbx_1` and return the Linguistic
O<latexit sha1_base64="z5pJ6KZLmsSkixjimeA0//1F+qY=">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040K0YmMrWkqSTuvQvEgmQind+gNu9bvEP9C/8M44glpEJyQ5c+49Z+be6yUBz4RlvRSMqemZ2bnifGlhcWl5pby6dpnFeeozx4+DOG15bsYCHjFHcBGwVpIyN/QC1vQGRzLevGNpxuOoIYYJa4duP+I97ruCqKszp1F3Ghedk065YlUttcxJYGtQgV71uPyMG3QRw0eOEAwRBOEALjJ6rmHDQkJcGyPiUkJcxRnGKJE2pyxGGS6xA/r2aXet2Yj20jNTap9OCehNSWliizQx5aWE5WmmiufKWbK/eY+Up7zbkP6e9gqJFbgl9i/dZ+Z/dbIWgR4OVA2cakoUI6vztUuuuiJvbn6pSpBDQpzEXYqnhH2l/OyzqTSZql321lXxV5UpWbn3dW6ON3lLGrD9c5yT4HKnau9V7fPdSu1Qj7qIDWxim+a5jxqOUYdD3iEe8Ign49QQxsgYf6QaBa1Zx7dl3L8Dgf2SSg==</latexit> UTPUTSL the building in box `bbx_1`)->bbx_2, bounding box of the number as `bbx_2`. Annotation
Step 3: (OCR_1(number in region `bbx_2`) Recognize the number in the region `bbx_2` and return the recognized number as `txt_1`.
->txt_1,
Question: What number police station is on the building?
Answer: 43
I<latexit sha1_base64="Eeipg0SaSzO/bMIo2fhu3xEhWXY=">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0owhSsWkLtUgyndbYvEwmYi2u/AG3+mPiH+hfeGdMQS2iE5KcOfeeM3PvdSLPTYRhvOa0icmp6Zn8bGFufmFxqbi8Uk/CNGbcYqEXxk3HTrjnBtwSrvB4M4q57Tsebzj9Axlv3PA4ccOgJgYRb/t2L3C7LrMFUfWjk6pVO7soloyyoZY+DswMlJCtalh8wTk6CMGQwgdHAEHYg42EnhZMGIiIa2NIXEzIVXGOexRIm1IWpwyb2D59e7RrZWxAe+mZKDWjUzx6Y1Lq2CBNSHkxYXmaruKpcpbsb95D5SnvNqC/k3n5xApcEvuXbpT5X52sRaCLPVWDSzVFipHVscwlVV2RN9e/VCXIISJO4g7FY8JMKUd91pUmUbXL3toq/qYyJSv3LMtN8S5vSQM2f45zHNS3yuZO2TzdLlX2s1HnsYZ1bNI8d1HBIaqwyPsKj3jCs3asXWu32t1nqpbLNKv4trSHD4zXkSA=</latexit>NPUTS
O<latexit sha1_base64="z5pJ6KZLmsSkixjimeA0//1F+qY=">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040K0YmMrWkqSTuvQvEgmQind+gNu9bvEP9C/8M44glpEJyQ5c+49Z+be6yUBz4RlvRSMqemZ2bnifGlhcWl5pby6dpnFeeozx4+DOG15bsYCHjFHcBGwVpIyN/QC1vQGRzLevGNpxuOoIYYJa4duP+I97ruCqKszp1F3Ghedk065YlUttcxJYGtQgV71uPyMG3QRw0eOEAwRBOEALjJ6rmHDQkJcGyPiUkJcxRnGKJE2pyxGGS6xA/r2aXet2Yj20jNTap9OCehNSWliizQx5aWE5WmmiufKWbK/eY+Up7zbkP6e9gqJFbgl9i/dZ+Z/dbIWgR4OVA2cakoUI6vztUuuuiJvbn6pSpBDQpzEXYqnhH2l/OyzqTSZql321lXxV5UpWbn3dW6ON3lLGrD9c5yT4HKnau9V7fPdSu1Qj7qIDWxim+a5jxqOUYdD3iEe8Ign49QQxsgYf6QaBa1Zx7dl3L8Dgf2SSg==</latexit> UTPUTSL
Visual
bbx_1 bbx_1 bbx_2 bbx_1 bbx_2 Annotation
txt_1
O<latexit sha1_base64="gNwd0WkqYb/+16EBVpkoNrWmwYE=">AAACz3icjVHLTsJAFD3UF+ILdemmkZi4Iq0x6pLoxoWJECmQACHtMGBDaZt2qiEE49YfcKt/ZfwD/QvvjCVRidFp2p45954zc+91Qs+NhWG8ZrS5+YXFpexybmV1bX0jv7lVi4MkYtxigRdEDceOuef63BKu8HgjjLg9dDxedwZnMl6/4VHsBn5VjELeHtp93+25zBZEtS6tatmqXnXGtYtJJ18wioZa+iwwU1BAuspB/gUtdBGAIcEQHD4EYQ82YnqaMGEgJK6NMXERIVfFOSbIkTahLE4ZNrED+vZp10xZn/bSM1ZqRqd49Eak1LFHmoDyIsLyNF3FE+Us2d+8x8pT3m1Efyf1GhIrcE3sX7pp5n91shaBHk5UDS7VFCpGVsdSl0R1Rd5c/1KVIIeQOIm7FI8IM6Wc9llXmljVLntrq/ibypSs3LM0N8G7vCUN2Pw5zllQOyiaR0WzclgonaajzmIHu9ineR6jhHOUYZF3iEc84VmraLfanXb/maplUs02vi3t4QMXQZO2</latexit> UTPUTSVL
Locate the building in the image and return Identify the number of the police station on Recognize the number in the region `bbx_2`
the bounding box of the building as `bbx_1`. the building in box `bbx_1` and return the and return the recognized number as `txt_1`.
bounding box of the number as `bbx_2`.
An Example to show the linguistic annotation results and Visual annotation results
Figure6. Anexampleshowstheconfiguration,inputs,outputsofthelinguisticannotationandvisualannotation.
16
MANIPULATIONS
REQUIREMENTS
(PROMPT)CoM:
Given a question, please solve the question step-by-step with a chain of manipulations, where in each step you can optionally use one of the following manipulations
(defined as Name(Input)->Return) on the image: GROUNDING(a phrase)->boxes, OCR(an image or a region)->texts, CROP_AND_ZOOMIN(a region on given
image)->new_image, CALCULATE(a computable target)->numbers, or a new manipulation created by yourself (if it is indeed needed).
Question: What is written on the traffic sign near the bus?
Model: Using GROUNDING(the traffic sign near the bus) to locate the traffic sign near the bus
in current image, which is [[084,157,926,703;676,146,740,273]]. Utilizing OCR(texts in region
[[084,157,926,703]]) to read the texts written on the traffic sign within region [[084,157,926,703]],
resulting Taxi. So ultimately, the conclusive answer to the question in discussion is Taxi.
Visual Grounding:
Question: Find the region in image that "a traffic sign" describes.
Model: [[678,145,739,270]
Grounded Captioning:
Question: The goal of this task is to generate description for one part of the image. The part is specified by [[678,145,739,270]].
Model: A no tax sign
Image Captioning:
Question: In this task, you are given an image and you will need to generate some text to describe it. Please give me a concise answer.
Answer: Taxi bus with rainbow colors on side.
Multi Choice:
Question: Select the color that does not appear in the rainbow sign of this bus. (1) red. (2) yellow. (3) pink. (4) black. Please give me a direct answer.
Answer: black
Detailed Captioning:
Question: Explain the visual content of the image in great detail.
Answer: The image captures a scene on a city street, where a white bus with red and blue accents is parked. The bus bears the text "Trafalgar" in bold red
letters across its side, indicating it's likely part of a tourist or travel company. The bus is positioned next to a traffic sign that reads "TAXI", suggesting it might
be waiting for passengers or ready to embark on a journey. In the background, there are buildings and trees, adding depth to the urban landscape. The overall
impression is one of a typical day in an urban setting, with a bus waiting for its passengers amidst the hustle and bustle of city life.
Figure7.CogCoMdemonstratestheflexiblecapabilitiesforadaptingtodifferentmultimodalscenarios, includingevidentialvisual
reasoning,VisualGrounding,GroundedCaptioning,ImageCaptioning,MultiChoice,andDetailedCaptioning.
17