Attention with Markov: A Framework for Principled Analysis of Transformers
via Markov Chains
AshokVardhanMakkuva*1 MarcoBondaschi*1 AdwayGirish1 AlliotNagle2 MartinJaggi1 HyejiKim†2
MichaelGastpar†1
Abstract natural languages (Vaswani et al., 2017; Radford &
Narasimhan,2018;Devlinetal.,2018). Inadditiontothe
Inrecentyears,attention-basedtransformershave
innovativearchitectureandthemodelscale,oneofthekey
achievedtremendoussuccessacrossavarietyof
ingredientsbehindtheirsuccessisthegenerativepretraining
disciplines including natural languages. A key
procedure. Capitalizing on their efficient processing of
ingredientbehindtheirsuccessisthegenerative sequentialdata, thesemodelsarepretrainedonlargetext
pretrainingprocedure,duringwhichthesemod-
corporaonthenext-tokenprediction(NTP)objective.
elsaretrainedonalargetextcorpusinanauto-
regressive manner. To shed light on this phe- Such trained models generalize to a wide assortment of
nomenon,weproposeanewframeworkthatal- naturallanguagetaskswithminimalfine-tuning,achieving
lowsboththeoryandsystematicexperimentsto impressiveperformance(Brownetal.,2020). Giventheir
studythesequentialmodelingcapabilitiesoftrans- success, there is tremendous interest and active research
formers through the lens of Markov chains. In- inunderstandingtransformermodels. Forexample,Garg
spiredbytheMarkovianityofnaturallanguages, etal.(2022);VonOswaldetal.(2023)studytheirin-context
wemodelthedataasaMarkoviansourceanduti- learningcapabilities,Yunetal.(2020)establishtheirrep-
lize this framework to systematically study the resentationpower, andHewitt&Manning(2019);Vig&
interplaybetweenthedata-distributionalproper- Belinkov(2019)probeattentionoutputstounderstandthe
ties, thetransformerarchitecture, thelearntdis- attention mechanism. While these results offer valuable
tribution, and the final model performance. In insights, a precise characterization of the NTP objective
particular,wetheoreticallycharacterizetheloss on sequential learning capabilities of transformers is not
landscapeofsingle-layertransformersandshow wellunderstood. Thismotivatesthefollowingfundamental
theexistenceofglobalminimaandbadlocalmin- question: Howdotransformerslearnfromsequentialdata?
imacontingentuponthespecificdatacharacteris-
Toaddressthisquestion,inthispaperweintroduceanew
ticsandthetransformerarchitecture. Backedby
frameworkforaprincipledtheoreticalandempiricalanal-
experiments,wedemonstratethatourtheoretical
ysisoftransformersviaMarkovchains. Thekeyideaun-
findingsareincongruencewiththeempiricalre-
derpinningourapproachisthemodelingofsequentialinput
sults. Wefurtherinvestigatethesefindingsinthe
data as a Markov process, inspired by the Markovianity
broader context of higher order Markov chains
of natural languages. In fact, the idea of using Markov
anddeeperarchitectures,andoutlineopenprob-
processesforlanguagemodelinghasarichhistorydating
lemsinthisarena. Codeisavailableathttps:
backtoseminalworksofShannon(1948;1951). Thisfea-
//github.com/Bond1995/Markov.
tureallowsforasystematicstudyoftheinterplaybetween
thedata-distributionalproperties,thetransformerarchitec-
ture,andthefinalmodelperformanceinamathematically
1.Introduction
tractableway(Secs.3and4).
Attention-basedtransformershavebeenattheforefrontof Moreconcretely,wemakethefollowingcontributions:
recentbreakthroughsinavarietyofdisciplines,including
• Weprovideanovelframeworkforaprecisetheoretical
*Equal contribution. †Equal contribution. 1School of Com-
andempiricalstudyoftransformersviaMarkovchains
puterandCommunicationSciences,EPFL,Lausanne,Switzerland
2DepartmentofElectricalandComputerEngineering,UTAustin, (Sec.2.3).
Austin,TX,USA.Correspondenceto:AshokVardhanMakkuva
• We characterize the loss landscape of single-layer
<ashok.makkuva@epfl.ch>.
transformerswithfirst-orderMarkovchains,highlight-
ing the effect of the data distribution and the model
1
4202
beF
6
]GL.sc[
1v16140.2042:viXraAttentionwithMarkov
architecture(Sec.3). 2.1.Transformers
Transformersaresequence-to-sequencemappingsthattake
• Weempiricallyinvestigatehowourfindingstranslate
a sequence of discrete symbols as input and output the
to higher order Markov chains and deeper architec-
prediction probabilities for the next symbol in an auto-
tures, and outline interesting open problems in this
regressive manner (Vaswani et al., 2017). Functionally,
area(Sec.4).
theyfirstembedtheinputsequenceintoaEuclideanspace
andthenapplytheself-attentionlayerandthetoken-wise
Ourmainfindingsandobservationsare: feed-forwardlayeronthesesequenceofembeddings. Fi-
nally,alinearoperationconvertstheseembeddingstologits,
whicharethenusedforthenext-tokenprediction. Herewe
• Forfirst-orderMarkovchains,weobservethatweight
considerdecoder-onlymodelswithacausalattentionmask,
tying and the transition probabilities play a key role
whicharewidelyusedinpractice(Brownetal.,2020). We
onthelosslandscape(Thms.1and2).
omitthelayernormfortheoreticalanalysissinceitsinflu-
enceismarginalinthesettingsweconsider(Sec.3).
• Inparticular,weprovethatweighttyingcanintroduce
bad local minima for single-layer transformers For the ease of exposition, let us consider a single-layer
(Thm. 2). Increasing the depth, however, solves the transformer with a single-head attention whose input vo-
issue(Sec.3.3). cabularyX isofsize2, i.e. X = {0,1}. Let{x n}N n=1 ∈
{0,1}N beaninputsequenceoflengthN. Thenforeach
n ∈ [N], the Transformer operations are mathematically
• Forhigher-orderMarkovchains,thetransformerfails
givenby(Fig.1a):
to learn the correct transition probabilities unless a
maskingwindowisused(Sec.4). x
n
=x ne 1+(1−x n)e 0+p
(cid:101)n
∈Rd, (Embedding)
(cid:88)
• Interestingly, we observe that increasing the depth
y
n
=x n+W
O
att n,i·W
V
x
i
∈Rd, (Attention)
i∈[n]
doesnothelplearninghigher-orderMarkovchains,in
contrasttothefirst-ordercase(Sec.4). z n =y n+W 2ReLU(W 1y n)∈Rd, (FF)
logit
n
=⟨a,z n⟩+b ∈R, (Linear)
Notation. Scalarsaredenotedbyitaliclowercaseletters f θ¯(xn 1)≜P θ¯(x
n+1
=1|xn 1)= σ(logit n). (Prediction)
like x,y and Euclidean vectors and matrices are denoted (cid:124) (cid:123)(cid:122) (cid:125)
∈[0,1]
by bold ones x,y,M, etc. We use ∥ · ∥ to denote the
ℓ -normforEuclideanvectorsandFrobeniusnormforma- Hereθ¯ ≜(e ,e ,...,b,a)denotesthefulllistoftheTrans-
2 1 0
trices. [k] ≜ {1,...,k},andforasequence(x ) ,de- formerparameters. distheembeddingdimension,e and
n n≥1 1
fine xm ≜ (x ,...,x ) if k ≥ 1 and (x ,...,x ) oth- e inRdarethetoken-embeddingscorrespondingtox =1
k k m 1 m 0 n
erwise. Forz ∈ R,thesigmoidσ(z) ≜ 1/(1+e−z)and andx =0respectively,andp isthepositionalencoding.
n (cid:101)n
ReLU(z)≜max(0,z).ForeventsAandB,P(A)denotes We have matrices W ∈ Rd×m and W ∈ Rm×d, and
O V
theprobabilityofAwhereasP(A|B)theconditionalprob- the attention weights att ∈ (0,1) are computed using
n,i
ability. Let(x,y)beapairofdiscreterandomvariableson thequeryandkeymatrices. W ∈Rd×r andW ∈Rr×d
2 1
[k]×[k]withtheprobabilitymassfunction(pmf)ofxbeing are the weight matrices in the FF layer, whereas a ∈ Rd
p = (p ,...,p ) ∈ [0,1]k. ThenitsShannonentropyis andb∈Raretheweightandbiasparametersforthelinear
x 1 k
definedasH(x)=H(p
)≜−(cid:80)
p logp . Thecon- layers. Sincethevocabularyhasonlytwosymbols,itsuf-
x i∈[k] i i
ditionalentropyisdefinedtobeH(y|x)≜H(x,y)−H(x). ficestocomputetheprobabilityforthesymbol1usingthe
Theentropyrateofastochasticprocess(x n) n≥1isdefined logits: f θ¯(xn 1)≜P θ¯(x n+1 =1|xn 1)=σ(logit n)∈[0,1].
as lim H(xn)/n. We simply write x = y to mean Wereferto§AforadetailedexpositionoftheTransformer.
n→∞ 1
P(x=y)=1. WealsousetheshorthandP(y =j |x)for The Transformer parameters θ¯ are trained via the cross-
P(y =j |x=x) as a function of the random variable x. entropylossonthenext-tokenprediction:
Finally,forp∈(0,1),thebinaryentropyfunctionh(·)isde-
finedash(p)≜H(p,1−p)=−plogp−(1−p)log(1−p). L(θ¯)≜− N1 (cid:88) E
xn
1+1[x n+1·logf θ¯(xn 1)+
(1)
n∈[N]
2.Backgroundandproblemformulation (1−x n+1)·log(1−f θ¯(xn 1))],
WeprovidebackgroundonTransformersandMarkovchains where the expectation is over the data distribution of the
andthenpresentourframeworktostudytransformersvia sequence{x }N . Inpractice,itisreplacedbytheempiri-
n n=1
Markovchains. calaveragesacrossthesequences{x }N sampledfrom
n n=1
2AttentionwithMarkov
f θ¯(x1 1) ... f θ¯(xn 1) ... f θ¯(xN
1
)
p
σ(·) σ(·) σ(·)
logit ... logit ... logit
1 n N
1−p 0 1 1−q
Linear Linear Linear
z 1 ... z n ... z N q
FF FF FF
(cid:126)
(cid:119)
y 1 ... y n ... y N (cid:127)
Attention
(cid:20) (cid:21)
1−p p
(x |x ) ∼P(p,q)≜ ,
x ... x ... x n+1 n n≥1 q 1−q
1 n N
Embedding Embedding Embedding with P ij
=P(cid:0)
x n+1 =j |x n
=i(cid:1)
, i,j ∈{0,1}.
x 1 ... x n ... x N ∈{0,1}
(b) State transition diagram and Markov kernel for a first-order
(a)Thetransformermodelwithbinaryinputdata:foreachxn,the Markov chain P(p,q) with flipping probabilities P = p and
1 01
next-bitpredictionprobabilityisf θ¯(xn 1)=P θ¯(x
n+1
=1|xn 1). P
10
=q.
Figure1. TheTransformerarchitectureandtheMarkovianinputsequence.
thecorpus,withstochasticoptimizerslikeSGDorAdam stateandnoneofthepast:
(Kingma&Ba,2015)usedtoupdatethemodelparameters.
P ≜P(x =j |x =i)
ij n+1 n
2.2.MarkovChains =P(cid:0) x =j |x =i, xn−1 =in−1(cid:1) ,
n+1 n 1 1
Markovchainsareoneofthemostwidelyusedmodelsto
foranyi ,...,i ,i,j ∈X andn≥1. TheMarkovker-
1 n−1
studystochastic(random)processesthatevolvewithtime
nelP =(P )governsthetransitiondynamicsofthepro-
ij
(Norris,1997). Owingtotheirsimplicityandrichnessin cess: ifπ(n) ∈ [0,1]|X| denotestheprobabilitylawofx
n
structure,theyareubiquitousinavarietyoffieldsranging attimen,thenπ(n+1) =π(n)·P. Ofparticularinterestto
frombiology,physics,andeconomics,tofinance. Thekey usinthispaperisthekernelP(p,q)≜[1−p, p; q, 1−q]
definingpropertyoftheseprocessesisMarkovianity,aka
on the binary state space with the flipping probabilities
“memorylessness”: futureevolutionisonlyinfluencedby
P =pandP =q,forp,q ∈(0,1). Fig.1billustrates
01 10
themostrecentstate(s). Formally,wesaythat(x ) isa
n n≥1 thesame.Herewerefertothesump+qastheswitchingfac-
discrete-time(time-homogeneous)Markovchainoforder
tor. Wedenoteafirst-orderbinaryMarkovchain(x )
n n≥1
mona(finite)statespaceX,ifitevolvesaccordingtothe
withthetransitionkernelP(p,q)andstartingwithaninitial
conditionaldistributiongivenby law π(1) as (x ) ∼ (π(1),P(p,q)). When the initial
n n≥1
distribution is understood from context, we simply write
P(cid:0) x =j |xn =in(cid:1)
n+1 1 1 (x n+1 | x n) n≥1 ∼ P(p,q). Forthisprocess, theentropy
=P(cid:0) x =j |xn =in (cid:1) , rateequalsH(x |x )= 1 (qh(p)+ph(q)),whichis
n+1 n−m+1 n−m+1 n+1 n p+q
independentofn.
foranyi ,...,i ,j ∈ X,n ≥ 1. Thatis,theevolutionof
1 n Stationary distribution. A stationary distribution of a
theprocessisinfluencedonlybythemmostrecentstates.
Markovchainisadistributionπ onX thatisinvariantto
Time-homogeneityreferstothefactthatthetransistiondy-
the transition dynamics, i.e. if π(n) = π, then we have
namicsareinvarianttopositionn. Markovchainscanalso
π(n+1) = πP = π aswell. Consequently,π(m) = π for
be equivalently represented by a state transition diagram
all m ≥ n. That is, once the Markov chain reaches the
oratransitionprobabilitymatrix,calledtheMarkovkernel.
stationarydistributionπ,itcontinuestohavethesamedis-
Fig.1billustratesthisformemorym=1andX ={0,1}.
tributionforever—itreacheda“steadystate”andismixed.
First-order Markov chains. First-order Markov chains Its prominence lies in the fact that for a wide class of
areMarkovchainswith(order)memorym=1. Forthese Markovchains,startingwithanydistributionπ(1),thelaw
processes,thenextstateisinfluencedonlybythecurrent attimen,π(n),convergestothestationarydistributionπ
3AttentionwithMarkov
asn→∞,i.e. πisthelimitingdistribution. Itsexistence Loss. We consider the cross-entropy population loss be-
anduniquenesscanbeguaranteedunderfairlygeneralcon- tweenthepredictedprobabilityf θ¯(xn 1)andthecorrespond-
ditions(Norris,1997),andinparticularforP(p,q)when ing ground truth symbol x across all the positions
n+1
p,q ̸=0,1. ForP(p,q),thestationarydistributionisgiven n∈[N],asinEq.(1). Itisimportanttonotethatwhilethe
byπ(p,q)≜(π ,π )= 1 (q,p). Thehighertheflipping underlyingdata{x }N isMarkovian,theTransformeris
0 1 p+q n n=1
probabilityq,thehigherthelikelihoodforthechaintobein agnostictothisfactanditcanpotentiallyutilizethefullpast
thestate0atthesteadystate. Similarlyforthestate1. We forthenext-symbolpredictionviaf θ¯(xn 1).
canverifythatπindeedsatisfiesπP =π. Forbrevity,we
Objective. GiventheaforementionedMarkoviandataand
dropthedependenceon(p,q)andsimplywriteπ andP
thetransformerarchitecture,weareinterestedincharacter-
whentheparametersareclearfromcontext.
izing how the cross-entropy loss on the next-symbol pre-
dictionenablesthemodeltolearntheMarkovianstructure
2.3.Framework: TransformersviaMarkovchains
inherenttothedata.Tothisend,westudythelosslandscape
We present our mathematical framework for a principled ofthetransformersandhighlighttheimportantroleofthe
analysisoftransformersviaMarkovchains. Wefocuson datacharacteristicsandthemodelarchitecture.
first-orderMarkoviandataandsingle-layertransformersin
Sec.3. InSec.4,weconsiderhigher-orderprocessesand 3.First-orderMarkovchains: theoryto
deeperarchitectures. Westartwiththeassumptionsforour
practice
theoreticalanalysisinSec.3.
We present our main results for the first-order Markov
Data. We assume that the input sequence
chains. Inparticular,wetheoreticallyandempiricallychar-
{x }N ∈{0,1}N,whereN ≥1issomefixedsequence
n n=1 acterizetheinterplaybetweentheMarkoviandata,thetrans-
length, is generated from the class of first-order Markov
formerarchitecture,itslosslandscape,andthelearntdistri-
chainswithkernelP(p,q),andthatitisalreadymixed,i.e.
bution. Interestingly,weobservethatweighttyingplaysa
{x }N ∼(π(p,q),P(p,q)). Themotivationbehindthis
n n=1 keyroleinthischaracterizationandhenceweconsiderthe
datamodelingassumptionisthatitallowsforamathemati-
scenarioswithandwithoutweighttyingseparately.
callytractablecharacterizationofsequentialinputdatawhile
respectingtheMarkovianstructureofnaturallanguages.
3.1.Withweighttying
Model.Weconsiderasingle-layertransformerwithasingle-
Whentheembeddingandlinearlayersaretied,i.e. e=a,
headattention,withoutlayernorm. ExperimentsinSec.3
our analysis reveals that the switching factor p+q has a
usethesamemodel. Astheinputisbinary,theEmbedding
significantimpactonthelosslandscapeofthetransformers:
layercanbesimplifiedto
if the switching factor is greater than one, there exist
x n =x ne 1+(1−x n)e 0+p (cid:101)n
(Uni-embedding)
b coa rd relo spc oa nl dm sin ti oma thθ eπ mw ah ro gs ine ap lre sd taic tit oio nn arp yro db ia sb tri il bit uy tif oθ nπ( π·)
,
=x e+p ,
n n ignoringthepastandthepresent(Thm.2andFig.2d). On
theother hand, there alwaysexists aglobalminimum θ
wheree≜e −e istheembeddingvectorandp ≜e + ⋆
1 0 n 0 whose prediction f (·) corresponds to the true Markov
p isthenewpositionalencoding. Notethatx ∈ {0,1} θ⋆
(cid:101)n n kernel(Thm.1andFig.2b). Weformallystatetheseresults.
and hence the embedding is either e+p or just p de-
n n
pendingonx n. TheotherlayersarethesameasinSec.2.1: Theorem 1 (Global minimum). Let the input sequence
be {x }N ∼ (π(p,q),P(p,q)) for some fixed (p,q) ∈
n n=1
x n ∈{0,1}−U −n −i −-e −m −b −e −dd −i −n →g x n −A −t −te −n −ti −o →n y n (2) ( a0 n,1 ex) p2. licT ih te cn onf so tr rua cl tl io(p n, sq u) c, hth ther ae tie tx ii sst as ga loθ b⋆ al∈ mR inD im− ud mw fi oth r
y n −F −→F z n −L −i −ne −a →r logit n −P −r −e −di −c −tio −→n f θ¯(xn 1). thepopulationlossL(·)inEq.(1),i.e.
(i) L(θ)≥L(θ )forallθ ∈RD−d.
⋆
Letθ¯ ≜(e,{p }N ,...,b,a)∈RD denotethejointlist
n n=1 Further,θ satisfies:
of the parameters from all the layers, with D being the ⋆
totaldimensionality. Intraininglargelanguagemodels,it (ii) P (x =1|xn) = P(x =1|x ), the
θ⋆ n+1 1 n+1 n
isacommonpracticetotietheembeddingandlinearlayer Markovkernel.
weights, i.e. a = e, referred to as weight tying (Press & (iii) L(θ )=H(x |x ),theentropyrateoftheMarkov
⋆ n+1 n
Wolf, 2017). In this case, the list of all parameters, θ = chain.
(e = a,{p }N ,...,b) ∈ RD−d,sinceaisnolongera
n n=1 (iv) ∇L(θ )=0,i.e. θ isastationarypoint.
⋆ ⋆
freeparameter. Westudybothweight-tiedandgeneralcases
inSec.3.1andSec.3.2respectively. Remark1. Whiletherecouldexistotherglobalminimain
4AttentionwithMarkov
0.70 0.30 0.80
W i t h o u t w e i g h t t y i n g W i t h o u t w e i g h t t y i n g W i t h o u t w e i g h t t y i n g W i t h o u t w e i g h t t y i n g W i t h w e i g h t t y i n g W i t h w e i g h t t y i n g W i t h w e i g h t tying 0.6 W i t h w e i g h t t y i n g
0.65 E n t r o p y r a t e 0.25 T r a n s i t i o n p r o b a b i l i t y p 0.75 E n t r o p y rate Tr a n s i t i o n p robabilityp E n t r o p y ofstationarydistribution St a t i o n a r y p r o b a b i l i t y π 1
0.60 0.20 0.70 0.5
0.55 0.15 0.65
0.4
0.50 0 20 40 60 80 100 0.10 0 20 40 60 80 100 0.60 0 20 40 60 80 100 0 20 40 60 80 100
Itera tion Indexk: xnk =0 Itera tion Indexk:x nk =0
(a)Testloss(p+q<1) (b)Predictedprobability(p+q<1) (c)Testloss(p+q>1) (d)Predictedprobability(p+q>1)
Figure2.Effectofweighttyingontestlossandpredictedprobabilitiesf (xnk)forzeroindices{n }100 suchthatx =0.For(a),(b):
θ 1 k k=1 nk
p=0.2,q=0.3.Thetestlossalwaysconvergestoaglobalminimum,andthepredictedprobabilityisp.For(c),(d):p=0.5,q=0.8.
Withweighttying,thelossconvergestoalocalminimum,andthepredictedprobabilityisπ =p/(p+q).Withoutweighttying,we
1
predictthecorrectprobabilitypandconvergetoaglobalminimum.
principle,Thm.1highlightsaspecificwaytorealizethis. whileitisnotfullyclearwhythetrainingalgorithm
convergestolowranksolutions, theydoindeedpro-
Proof. (Sketch)Thekeyideahereistoshowthatanyθsat- videacanonicalandsimplewaytorealizetheMarkov
isfyingf (xn)=P(x =1|x )isaglobalminimum kernel,asillustratedin§C.2.
θ 1 n+1 n
andisastationarypointwiththelossbeingtheentropyrate
(Lemmas.1and2). Toconstructsuchaθ, weutilizethe Further, weshowin§C.2thatplugginginthenumerical
facttheMarkovkernelisonlyafunctionofx andthuswe values obtained from the average of five runs, the proba-
n
canignorethepastinformationintheAttentionlayerusing bility given by our formula matches the theory, i.e. the
onlytheskip. Wedeferthefullproofto§B. modellearnstocorrectlyoutputtheMarkovkernelproba-
bilities. Indeed,Fig.2ashowsthatthetestlossofthemodel
convergestothetheoreticalglobalminimum(Thm.1),the
Empirical evidence and interpretatation. As demon-
entropy rate of the source, when p = 0.2 and q = 0.3
strated in the proof above, a canonical way to realize
(p+q <1). Forthepredictionprobability,wefocusonthe
the Markov kernel by the single-layer transformer is to
zeropositionsn = n suchthatx = 0. Fig.2bshows
rely only on the current symbol x
n
and ignore the past thatirrespectiveofthk
eindexk
andnk
thepastxnk−1, ifthe
in the Attention layer. We now empirically confirm this 1
currentx is0,themodelcorrectlypredictstheprobability
fact. For our experiments, we use the single-layer trans- nk
forthenextbitx being1,whichequalsptheoretically
former (Table 1) and consider the results corresponding nk+1
(Fig. 1b). More precisely, f (xnk−1,x = 0) = p for
tothebestsetofhyper-parametersafteragridsearch(Ta- θ 1 nk
allxnk−1 andk,inlinewithproperty(ii)ofThm.1. We
ble 2). In particular, for p = 0.2,q = 0.3, and d = 4, 1
observeasimilarphenomenonwithx =1andprediction
we generate sequences {x n}N
n=1
∼ (π(p,q),P(p,q)) of
probability q. This indicates that
thenk
model has learnt to
lengthN = 1024andtrainthetransformerparametersθ
capturethefactthatthedataisfirst-orderMarkovianand
(weight-tied)tominimizethecross-entropylossinEq.(1).
henceonlyutilizesx forpredictingx .
Atinference,weinterprettheattentionlayerandobserve n n+1
thattherelativemagnitudeoftheattentioncontributionto LetL ≜ L(θ )betheglobalminimallossfromThm.1.
⋆ ⋆
the final attention output y is negligible, i.e. the ratio Nowwepresentourresultsonthebadlocalminima.
n
(cid:80)
∥W att ·W x ∥/∥y ∥ ≈ 0.01, when aver-
O i∈[n] n,i V i n Theorem2(Badlocalminimum). Lettheinputsequence
agedacross5runs. Hence,theattentioncontributioncanbe be {x }N ∼ (π(p,q),P(p,q)) for some fixed (p,q) ∈
n n=1
neglectedcomparedtotheskip-connection,i.e. y n ≈ x n. (0,1)2. Ifp+q > 1,thereexistsanexplicitθ π ∈ RD−d
Usingthisapproximation,in§C.2wederiveaformulafor
suchthatitisabadlocalminimumforthelossL(·),i.e.
thefinalpredictedprobabilityf (xn)asitislearntbythe
θ 1
network. Thisformularevealsinterestinginsightsaboutthe (i) there exists a neighborhood B(θ π,r) with r > 0
learntparametersofthetransformer: such that L(θ) ≥ L(θ π) for all θ ∈ B(θ π,r), with
L(θ )>L .
π ⋆
• Thepositionalembeddingp isconstantacrossn,i.e. Further,θ satisfies:
n π
itisindependentofthesequenceposition,reflecting
(ii) P (x =1|xn) = P(x =1) = π , the
thefactthatithaslearnttocapturethehomogeneityof θπ n+1 1 n+1 1
marginaldistribution.
theMarkovianchainjustfromthedata.
(iii) L(θ ) = H(x ) = H(π), the entropy of the
π n+1
• Theweightmatricesareallapproximatelyrank-one; marginal.
5
LssoltseT
)kn
1x(θf
noitciderP
LssoltseT
)kn
1x(θf
noitciderPAttentionwithMarkov
(iv) ∇L(θ )=0,i.e. θ isastationarypoint. 3.2.Withoutweighttying
π π
Remark 2. Since L(θ ) = H(x ) and Now we let the token-embedding e ∈ Rd and the linear
π n+1
L = H(x |x ), the optimality gap L(θ ) − L = weighta ∈ Rd beindependentparameters. Here,similar
⋆ n+1 n π ⋆
H(x ) − H(x |x ) = I(x ;x ) ≥ 0, where totheweight-tiedcase,weobservethattherealwaysexists
n+1 n+1 n n n+1
I(x ;x ) is the mutual information between x and aglobalminimumwhichisacanonicalextensionofθ to
n n+1 n ⋆
x (Cover&Thomas,2006). Itequalszeroifandonlyif thislargerspace. Interestingly,whenp+q >1,theearlier
n+1
x andx areindependent,whichhappensforp+q =1 localminimumθ becomesasaddlepoint.
n n+1 π
(sinceP(x =1|x )=x (1−p−q)+p).
n+1 n n RecallthatD isthetotalparameterdimensionalityinthe
non-weight-tied scenario where the full list of parame-
ters θ¯ = (e,{p }N ,...,b,a) ∈ RD and θ = (e =
n n=1
Proof. (Sketch)Themainideabehindconstructingθ π is a,{p }N ,...,b)∈RD−disthelistofparametersinthe
n n=1
that if we set e = a = 0 in the Linear layer, the model
weight-tiedcase.
ignorestheinputsalltogetherandoutputsaconstantprob-
Theorem 3 (Global minimum). Consider the same set-
ability, andinparticularπ bychoosingthebiasbappro-
1
priately, i.e. f (xn) = π forallxn,n. Forthisθ it’s ting as in Thm. 1. Then for all (p,q), if θ ⋆ = (e ⋆ =
θπ 1 1 1 π a ,...,b ) ∈ RD−d is a global minimum for the loss
easytoshowthatL(θ )=H(π)andthatit’sastationary ⋆ ⋆
π L(·) in the weight-tied scenario, then its extension θ¯ ≜
point. FurtherweshowthattheHessianatθ followsthe ⋆
structure(cid:20) H 0α 0 0(cid:21) whereH α ≻0whenpπ +q >1,and ( iθ n⋆ th,a e⋆ n) on∈ -wR eD ighis t-ta il es do ca asg el .o Fba ul rtm hein r,im θ¯ ⋆um satf io sr fieL s( t· h) ein saR mD e
thatitimpliesthelocalminimalityofθ . Wedeferthefull properties(ii)–(iv)asinThm.1.
π
proofto§B.3. Theorem4(Saddlepoint). Considerthesamesettingasin
Thm.2andforp+q >1,letθ =(e =a ,...,b )∈
π π π π
RD−d be the corresponding bad local minimum for the
Empirical evidence and interpretation. The proof of loss L(·) in the weight-tied scenario. Then its extension
Thm.2abovehighlightsthatwhenthelinearweightais θ¯ ≜ (θ ,a ) ∈ RD isasaddlepointforL(·)inRD in
π π π
zero in θ, it serves as a bad local minimum. We confirm the non-weight-tied case. Further, θ¯ satisfies the same
π
this fact empirically. Using the same weight-tied setting properties(ii)–(iv)asinThm.2.
as before but with the flipping probabilities p = 0.5 and
q =0.8instead,weobservethatthemagnitudeofthevector Empirical evidence and interpretation. In view of the
a is approximately 0.01 whereas that of z is 4.0. Thus, theoreticalresultsabove,removingweighttyingispossibly
n
⟨a,z ⟩ ≈ 0.04 , while the bias term b ≈ −0.4 implying beneficial: thebadlocalminimumintheweight-tiedcase
n
σ(⟨a,z ⟩+b)≈σ(b). Hencethefinalpredictionreturned forp+q > 1suddenlybecomesasaddlepointwhenthe
n
bythenetworkonlydependsonthebiasoftheLinearlayer, weighttyingisremoved. Weobserveasimilarphenomenon
totally independent of the input sequence xN. Fig. 2d il- empirically: as shown in Fig. 2c, when not weight-tied,
1
lustratesthisfactandshowsthatthemodelalwayspredicts the model’s test loss converges to the entropy rate of the
thestationaryprobabilityπ atallpositionsinthesequence, sourcewhenp+q >1,incontrasttotheweight-tiedcase,
1
independentoftheinput. Hereweplotonlyfortheindices possiblyescapingthesaddlepoint(Thm.4). Thefactthat
ksuchthatx =0forthesakeofcomparisonwithweight- themodelcorrectlylearnstheMarkoviankernelisfurther
nk
tiedcasebutweobservethesamephenomenonforx =1, demonstratedinFig.2d. Figs.2aand2ctogetherhighlight
nk
i.e. f (xn) = π forallxn,n,thusverifyingproperty(ii) thatthemodelalways(empirically)convergestotheglobal
θ 1 1 1
ofThm.2. Similarly,Fig.2cdemonstratesthatthemodel minimum in the non-weight-tied case irrespective of the
testlossconvergestotheentropyofthestationarydistribu- switchingfactorp+q.
tionH(π)insteadoftheglobalminimumL givenbythe
⋆
entropyrateofthesource(Thm.2-(iii)). 3.3.Higherdepth
Thm.2shouldbeinterpretedinthelightthatitguarantees Forthesingle-layertransformer,theaforementionedresults
theexistenceofbadlocalminimaonlyforp+q > 1(in highlight the significance of the switching factor and the
syncwiththeexperimentsaswell). Whiletherecouldexist weight tying on the loss landscape. In stark contrast, we
suchminimaevenwhenp+q <1,weempiricallyobserve empirically observe that for transformers of depth 2 and
thatthemodelalwaysconvergestotheglobalminimumin beyond,thelosscurveeventuallyreachestheglobalmini-
thisscenario(Fig.2a). Likewise,whileThm.1guarantees mumregardlessofthesefactors(§C.3). Interestingly,we
theexistenceofglobalminimaforall(p,q)andinparticular observe during training that it first reaches a plateau at a
forp+q >1,empiricallythemodeloftenconvergestobad lossvaluecorrespondingtoH(π)andafterafewadditional
localminimaashighlightedabove(Fig.2c). iterations,itfurtherdropsdownuntilitreachestheglobal
6AttentionwithMarkov
minimum(theentropyrate). Thissuggeststhat,whilethere incontrasttoorder1(Sec.3.3).
couldstillbelocalminima,increasingthenumberoflayers
Maskinghelps. Nonetheless, ourexperimentsshowthat
positivelyaffectsthelosscurvatureatthebadlocalminima
themodelisabletolearnthecorrectMarkoviankernelby
inamannermakingiteasiertoescapeandreachtheglobal
limiting the scope of its attention to fewer past symbols.
minimum. Theoreticallycharacterizingthisphenomenon
This can be done by simply tuning the masking in the
forhigherdepthsisaninterestingopendirection.
attentionlayer,i.e. wechangeEq.(Attention)to
4.Higher-orderMarkovchains n
(cid:88)
y n =x n+W O att n,i·W V x i, (Attention W)
We now shift our focus to Markov chains with memory i=n−W+1
m ≥ 2. Surprisingly, even for m = 2, we empirically
whereW isthenumberofpastsymbolsweattendto. We
observethatthemodeldoesnotlearnthecorrectconditional
callthisspanW themaskwindow. Ourexperimentsshow
probabilities,regardlessofthetransformerdepth. However,
thatreducingW dramaticallyimprovestheperformanceof
we show that a limited masking window, instead of the
themodel.InFig.3bweplotthefinaltestlossversusW,for
fullcausalmaskthatisusuallyusedinpractice,helpsthe
differentnumbersoflayersℓ.Theseresultsillustratethatthe
modeltolearnthetrueMarkoviankernel. Wemakethese
losscorrectlyconvergestotheentropyrateforamoderate
observationsonthefulltransformerarchitecture,including
W (e.g.,W ≲50forasingle-layermodel),includingwhen
layernorm,withandwithoutweight-tying.
W ≫ 2, the order of the Markov chain. However, when
W crossesaparticularthreshold,wenoticeaphasechange
4.1.Order=2
withthemodelunabletolearnthecorrectkernel,reverting
Data model. A second-order Markov chain on {0,1} is, to a bad local minimum corresponding to the stationary
in general, governed by four parameters (one probability distributionπ,i.e. lossequallingH(π). Fig.3bhighlights
value for each possible pair (x ,x )). For the ease of theprogressiveperformancedegradationofthemodelasW
n n−1
interpretationofourresultsinlightofthoseinSec.3,we increasesforℓ=1,fromcorrectlypredictingpatW =40
consider a special class of second-order Markov chains, toincorrectlypredictingπ atW = 100, atalmostevery
1
namely one where x is only influenced by x , and zeroindexkwithx =0. Interestingly,increasingthe
n+1 n−1 nk−1
notbyx : depthℓdoesnotresolvethisissue,andinsteadexacerbatesit:
n
inFig.3b,asℓincreases,themaximummaskwindowW for
P(x =j |xn =in)=P(x =j |x =i ),
n+1 1 1 n+1 n−1 n−1 whichthemodelcorrectlyoutputsthekernelgetssmaller.
for all i ,...,i ,j ∈ X,n ≥ 1. Clearly, such Markov
1 n
chains are defined by only two parameters, for which 4.2.Order>2
we can use the same kernel P(p,q) as in the first-order
WecanonicallyextendtheconstructioninSec.4.1togen-
case. Formally, we construct a second-order Markov
erateMarkovchainsoforderm > 2, i.e. wegeneratem
chain (x ) ∼ (π(p,q),P (p,q)) by interleaving two
n n≥1 2 independentMarkovchainsandinterleavethemtoobtain
independent Markov chains (x¯ ) ,(x¯ ) , each
1,n n≥1 2,n n≥1 (x ) oforderm. Experimentsshowthatthesamemask-
initialized with π and following the dynamics according n n≥1
ingissuefromsecond-orderchainsstillpersistsforhigher
toP(p,q),i.e. x = x¯ andx = x¯ k ≥ 1,with
2k−1 1,k 2k 2,k orders. Furthermore,limitingthemaskingwithamaskwin-
(x¯ ),(x¯ )∼(π,P). Thisisthesimplestsecond-order
1,k 2,k dowW stillhasthesamepositiveeffectforordersgreater
Markov chain that retains the fundamental properties of
than2. ThemaximumwindowW requiredforthemodelto
second-order chains. In fact, the model has to learn the
correctlylearnthetransitionprobabilities,pandq,seems
non-trivialfactthatitspredictionforthenextsymbolshould
nottochangeastheorderincreases(Fig.5in§D).
dependnotonthecurrenttoken,butratherthepreviousone.
Empiricalfindings. Empirically,weobservethatincreas- 5.Relatedwork
ingtheorderfrom1to2significantlyimpactsthelearning
capabilitiesofthetransformer. Inparticular,wenoticethat There is tremendous interest and active research in un-
the transformer (with or without weight tying) is unable derstandingtransformermodelsfromvariousperspectives
to correctly predict the conditional probabilities p and q (Weiss et al. (2021); Giannou et al. (2023); Oymak et al.
evenwhenp+q < 1, andinsteadpredictsthestationary (2023);Lietal.(2023a);Fuetal.(2023);Nocietal.(2023);
probabilities,asinthebadlocalminimaseenforfirst-order Tianetal.(2023)andreferencestherein). Yunetal.(2020);
Markovchainswhenp+q >1(Fig.2c). Thesameconclu- Pérezetal.(2021);Weietal.(2022);Malach(2023);Jiang
sionholdsevenafteranexhaustivegrid-searchontraining &Li(2023)demonstratetherepresentationcapabilitiesof
andarchitecturehyperparameters(Table2). Surprisingly, transformersandshowpropertiessuchasuniversalapproxi-
evenincreasingthedepthofthetransformerdoesnothelp mationandTuring-completeness. Anotherlineofinquiry
7AttentionwithMarkov
W =2 0.675
0.6 W =40 ℓ = 1
W =80 ℓ = 4
W =100 0.650 ℓ = 8
0.5
T r a n s i t i o n p r o b a b i l i t y p E n t r o p y r a t e
S t a t i o n a r yprobabilityπ1 0.625 H ( π )
0.4
0.600
0.3 0.575
0.2 0.550
0 20 40 60 80 100 0 20 40 60 80 100
Indexk:xnk−1=0 MaskwindowW
(a)Predictedprobability (b)Testloss
Figure3.EffectofthemaskwindowW onthepredictedprobabilityandthetestlossforsecond-order{x }N ∼(π,P (0.2,0.3)).
n n=1 2
(a):Fordepthℓ=1,predictedprobabilityf (xnk)atpositions{n }100 :x =0fordifferentvaluesofW.AsW increases,the
θ 1 k k=1 nk−1
predictiongraduallychangesfromthecorrectoutputpatW =2,totheincorrectstationaryprobabilityπ atW =100.(b):Finaltest
1
lossvs.W fordifferentnumbersoflayersℓ.AsW increases,wetransitionfromthelossconvergingtoaglobalminimumcorresponding
tothetransitionprobabilities,tothelossconvergingtoabadlocalminimumcorrespondingtothestationarydistributionπ.
(Elhageetal.,2021;Snelletal.,2021;Wangetal.,2023; backs in encoder-only models, which is in line with our
Gevaetal.,2023)ismechanisticinterpretability,i.e.reverse- observations that removing weight tying is beneficial in
engineering transformer operations on specific synthetic decoder-onlymodelswithMarkovianinputdata.
tasks(e.g.,matrixinversionandeigenvaluedecomposition
Ontheotherhand,givensamplesfromanunknowndistri-
inCharton(2022),modularadditioninNandaetal.(2023))
bution,theclassicalproblemsofdistributionestimationand
tounderstandthetransformercomponentsbuttheyusually
next-samplepredictionarewell-studied,datingbackmany
lack theoretical guarantees, as opposed to ours. Li et al.
decades(Good,1953;Krichevsky&Trofimov,1981;Xie&
(2023c)studieshowtransformerslearnsemanticstructures
Barron,1997;Kamathetal.,2015). Inparticular,whenthe
acrosswordswhileweareinterestedinhowtheylearnse-
underlyingdistributionisknowntobeMarkovian,Billings-
quentialityininputdata. Tarzanaghetal.(2023a;b)takean
ley(1961);Willemsetal.(1995);Haoetal.(2018);Wolfer
optimization-theoreticperspectivetostudytrainingdynam-
&Kontorovich(2019)providetheoreticaloptimalityguar-
icsandcharacterizeimplicitbiasesintransformermodels
anteesonthesamplecomplexityandloss(regret)associated
trainedwithgradientdescent. Incontrast,wecharacterize
withlearningtheMarkovchainparametersviafrequency
thelocalandglobalminimaofthelosslandscapeofthese
andsample-basedestimators. Whilewealsoconsidernext-
models under sequential input data. Dong et al. (2023);
sample(symbol)predictionforMarkovchains,weuseaspe-
Akyüreketal.(2023);VonOswaldetal.(2023);Xieetal.
cificparametricfamilyintheformoftransformersforthis
(2021);Baietal.(2023);Lietal.(2023b);Gargetal.(2022)
taskinordertobetterinterpretandunderstandthesemodels.
studyin-contextlearning,i.e. theabilityofthetransformer
to extract the desired task from just a few representative
examples. In this direction, our work is most closely re- 6.ConclusionandOpenquestions
latedtoBiettietal.(2023),buttheunderlyingpremiseand
Inthiswork,weprovideanovelframeworkforasystem-
motivation are quite different. While their goal is to an-
atictheoreticalandempiricalstudyofthesequentialmod-
alyzein-contextlearningusingbigrams(Markovchains),
elingcapabilitiesoftransformersthroughMarkovchains.
we present a systematic framework for a theoretical and
Leveragingthisframework, wetheoreticallycharacterize
empiricalstudyoftransformers,especiallywithregardto
the loss landscape of single-layer transformers and show
theirsequentiallearningcapabilities. Recently,Grau-Moya
theexistenceofglobalminimaandbadlocalminimacon-
etal.(2024)usedatageneratedfromMarkovchains,among
tingentuponthespecificdatacharacteristicsandthetrans-
otherdatasources, tostudyifmeta-learningcanapproxi-
formerarchitecture,andindependentlyverifythembyex-
mateSolomonoffInduction. Chungetal.(2021)provide
periments. Wefurtherrevealinterestinginsightsforhigher
empiricalevidencetosuggestthatweighttyinghasdraw-
orderMarkovchainsanddeeperarchitectures.
8
)kn
1x(
θf
noitciderP
LssoltseTAttentionwithMarkov
Webelieveourframeworkprovidesanewavenueforaprin- Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B.,
cipledstudyoftransformers. Inparticular,someinteresting Sun,X.,Xu,J.,Li,L.,andSui,Z. Asurveyonin-context
openquestionsinthisdirectioninclude: learning, 2023. URL https://arxiv.org/abs/
2301.00234.
• Characterizingthelearningdynamicsofgradient-based
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph,
algorithmsinoursetup.
N.,Mann,B.,Askell,A.,Bai,Y.,Chen,A.,Conerly,T.,
• Obtaining tight parametric rates for estimation of DasSarma,N.,Drain,D.,Ganguli,D.,Hatfield-Dodds,
Markovchainsusingtransformermodels. Thisfacil- Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L.,
itates a systematic comparison of transformers with Ndousse,K.,Amodei,D.,Brown,T.,Clark,J.,Kaplan,J.,
existingoptimalestimatorsandtheirrates. McCandlish,S.,andOlah,C. Amathematicalframework
for transformer circuits. Transformer Circuits Thread,
• Furtherinvestigationintotheroleofweighttyingin
2021. URL https://transformer-circuits.
decoder-onlymodels.
pub/2021/framework/index.html.
References Fu,H.,Guo,T.,Bai,Y.,andMei,S. Whatcanasingleat-
tentionlayerlearn? Astudythroughtherandomfeatures
Akyürek, E., Schuurmans, D., Andreas, J., Ma, T., and lens.InThirty-seventhConferenceonNeuralInformation
Zhou, D. What learning algorithm is in-context learn- ProcessingSystems,2023.
ing? Investigationswithlinearmodels. InTheEleventh
InternationalConferenceonLearningRepresentations, Garg, S., Tsipras, D., Liang, P. S., and Valiant, G. What
2023. cantransformerslearnin-context? Acasestudyofsim-
ple function classes. Advances in Neural Information
Bai,Y.,Chen,F.,Wang,H.,Xiong,C.,andMei,S. Trans-
ProcessingSystems,35:30583–30598,2022.
formersasstatisticians:Provablein-contextlearningwith
in-contextalgorithmselection. InWorkshoponEfficient Geva, M., Bastings, J., Filippova, K., and Globerson, A.
SystemsforFoundationModels@ICML2023,2023. Dissectingrecalloffactualassociationsinauto-regressive
languagemodels. InThe2023ConferenceonEmpirical
Bietti,A.,Cabannes,V.,Bouchacourt,D.,Jegou,H.,and
MethodsinNaturalLanguageProcessing,2023.
Bottou,L. Birthofatransformer: Amemoryviewpoint.
InThirty-seventhConferenceonNeuralInformationPro- Giannou, A., Rajput, S., Sohn, J.-Y., Lee, K., Lee, J. D.,
cessingSystems,2023. and Papailiopoulos, D. Looped transformers as pro-
grammable computers. In Proceedings of the 40th In-
Billingsley,P. StatisticalmethodsinMarkovchains. The
ternationalConferenceonMachineLearning,pp.11398–
Annals of Mathematical Statistics, 32(1):12–40, 1961.
11442,23–29Jul2023.
ISSN00034851.
Good,I.J. Thepopulationfrequenciesofspeciesandthe
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
estimationofpopulationparameters. Biometrika,40(3-
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
4):237–264,121953. ISSN0006-3444. doi: 10.1093/
Askell,A.,etal. Languagemodelsarefew-shotlearners.
biomet/40.3-4.237.
Advancesinneuralinformationprocessingsystems,33:
1877–1901,2020. Grau-Moya,J.,Genewein,T.,Hutter,M.,Orseau,L.,Delé-
tang,G.,Catt,E.,Ruoss,A.,Wenliang,L.K.,Mattern,
Charton,F. Whatismymathtransformerdoing? –Three
C., Aitchison, M., and Veness, J. Learning universal
resultsoninterpretabilityandgeneralization,2022. URL
predictors,2024. URLhttps://arxiv.org/abs/
https://arxiv.org/abs/2211.00170.
2401.14953.
Chung,H.W.,Fevry,T.,Tsai,H.,Johnson,M.,andRuder,
Hao,Y.,Orlitsky,A.,andPichapati,V. Onlearningmarkov
S. Rethinking embedding coupling in pre-trained lan-
chains. InAdvancesinNeuralInformationProcessing
guagemodels. InInternationalConferenceonLearning
Systems,volume31,pp.646–655,2018.
Representations,2021.
Hewitt, J. and Manning, C. D. A structural probe for
Cover,T.M.andThomas,J.A. Elementsofinformation
finding syntax in word representations. In Proceed-
theory. JohnWiley&Sons,2ndedition,2006.
ings of the 2019 Conference of the North American
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K.BERT: Chapter of the Association for Computational Linguis-
Pre-training of deep bidirectional transformers for lan- tics: Human Language Technologies, Volume 1 (Long
guage understanding, 2018. URL https://arxiv. and Short Papers), pp. 4129–4138, June 2019. URL
org/abs/1810.04805. https://aclanthology.org/N19-1419.
9AttentionwithMarkov
Horn,R.A.andJohnson,C.R. Matrixanalysis. Cambridge Pagliardini,M. GPT-2modularcodebaseimplementation.
universitypress,2012. https://github.com/epfml/llm-baselines,2023.
Jiang,H.andLi,Q. Approximationtheoryoftransformer Press, O. and Wolf, L. Using the output embedding to
networks for sequence modeling, 2023. URL https: improve language models. In Proceedings of the 15th
//arxiv.org/abs/2305.18475. Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 2, Short
Kamath,S.,Orlitsky,A.,Pichapati,D.,andSuresh,A.T.On Papers, pp. 157–163, April 2017. URL https://
learningdistributionsfromtheirsamples. InProceedings aclanthology.org/E17-2025.
ofThe28thConferenceonLearningTheory,volume40,
pp.1066–1100,03–06Jul2015. Pérez,J.,Barceló,P.,andMarinkovic,J.AttentionisTuring-
complete. Journal of Machine Learning Research, 22
Kingma, D. and Ba, J. Adam: A method for stochastic
(75):1–35,2021.
optimization. InInternationalConferenceonLearning
Representations(ICLR),2015. Radford, A. and Narasimhan, K. Improving language
understanding by generative pre-training. 2018.
Krichevsky,R.andTrofimov,V. Theperformanceofuniver- URL https://api.semanticscholar.org/
salencoding. IEEETransactionsonInformationTheory, CorpusID:49313245.
27(2):199–207,1981.
Shannon,C.E. Amathematicaltheoryofcommunication.
Li, H., Wang, M., Liu, S., and Chen, P.-Y. A theoretical
TheBellsystemtechnicaljournal,27(3):379–423,1948.
understandingofshallowvisiontransformers: Learning,
generalization,andsamplecomplexity. InTheEleventh Shannon,C.E. Predictionandentropyofprintedenglish.
InternationalConferenceonLearningRepresentations, TheBellSystemTechnicalJournal,30(1):50–64,1951.
2023a.
Snell,C.,Zhong,R.,Klein,D.,andSteinhardt,J. Approx-
Li,Y.,Ildiz,M.E.,Papailiopoulos,D.,andOymak,S.Trans- imating how single head attention learns, 2021. URL
formersasalgorithms: generalizationandstabilityinin- https://arxiv.org/abs/2103.07601.
contextlearning.InProceedingsofthe40thInternational
Tarzanagh, D.A., Li, Y., Thrampoulidis, C., andOymak,
ConferenceonMachineLearning,2023b.
S. Transformersassupportvectormachines. InNeurIPS
Li,Y.,Li,Y.,andRisteski,A. Howdotransformerslearn 2023 Workshop on Mathematics of Modern Machine
topicstructure: towardsamechanisticunderstanding. In Learning,2023a.
Proceedingsofthe40thInternationalConferenceonMa-
Tarzanagh,D.A.,Li,Y.,Zhang,X.,andOymak,S. Max-
chineLearning,2023c.
margintokenselectioninattentionmechanism. InThirty-
Malach,E. Auto-regressivenext-tokenpredictorsareuni- seventh Conference on Neural Information Processing
versallearners,2023. Systems,2023b.
Nanda,N.,Chan,L.,Lieberum,T.,Smith,J.,andSteinhardt, Tian,Y.,Wang,Y.,Chen,B.,andDu,S.S. Scanandsnap:
J. Progressmeasuresforgrokkingviamechanisticinter- Understandingtrainingdynamicsandtokencomposition
pretability. InTheEleventhInternationalConferenceon in1-layertransformer. InConferenceonParsimonyand
LearningRepresentations,2023. Learning(RecentSpotlightTrack),2023.
Noci,L.,Li,C.,Li,M.B.,He,B.,Hofmann,T.,Maddison, Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
C.J.,andRoy,D.M. Theshapedtransformer: Attention L.,Gomez,A.N.,Kaiser,Ł.,andPolosukhin,I. Atten-
modelsintheinfinitedepth-and-widthlimit. InThirty- tionisallyouneed. InAdvancesinNeuralInformation
seventh Conference on Neural Information Processing ProcessingSystems,pp.5998–6008,2017.
Systems,2023.
Vig,J.andBelinkov,Y. Analyzingthestructureofatten-
Norris,J.R.MarkovChains.CambridgeSeriesinStatistical tioninatransformerlanguagemodel. InProceedingsof
and Probabilistic Mathematics. Cambridge University the2019ACLWorkshopBlackboxNLP:Analyzingand
Press,1997. InterpretingNeuralNetworks for NLP, pp.63–76, Au-
gust 2019. URL https://aclanthology.org/
Oymak,S.,Rawat,A.S.,Soltanolkotabi,M.,andThram- W19-4808.
poulidis, C. On the role of attention in prompt-tuning.
InProceedingsofthe40thInternationalConferenceon VonOswald,J.,Niklasson,E.,Randazzo,E.,Sacramento,
MachineLearning,2023. J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov,
10AttentionwithMarkov
M. Transformers learn in-context by gradient descent.
In International Conference on Machine Learning, pp.
35151–35174,2023.
Wang,K.R.,Variengien,A.,Conmy,A.,Shlegeris,B.,and
Steinhardt,J.Interpretabilityinthewild:acircuitforindi-
rectobjectidentificationinGPT-2small. InTheEleventh
InternationalConferenceonLearningRepresentations,
2023.
Wei, C., Chen, Y., and Ma, T. Statistically meaningful
approximation: a case study on approximating Turing
machineswithtransformers. InAdvancesinNeuralInfor-
mationProcessingSystems,volume35,pp.12071–12083,
2022.
Weiss,G.,Goldberg,Y.,andYahav,E. Thinkingliketrans-
formers. InInternationalConferenceonMachineLearn-
ing,pp.11080–11090,2021.
Willems,F.,Shtarkov,Y.,andTjalkens,T. Thecontext-tree
weightingmethod: basicproperties. IEEETransactions
onInformationTheory,41(3):653–664,1995.
Wolfer, G. and Kontorovich, A. Minimax learning of er-
godicMarkovchains. InProceedingsofthe30thInter-
national Conference on Algorithmic Learning Theory,
volume98,pp.904–930,22–24Mar2019.
Xie,Q.andBarron,A. Minimaxredundancyfortheclassof
memorylesssources. IEEETransactionsonInformation
Theory,43(2):646–657,1997.
Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An
explanationofin-contextlearningasimplicitBayesian
inference. arXivpreprintarXiv:2111.02080,2021. URL
https://arxiv.org/abs/2111.02080.
Yun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S., and
Kumar, S. Are transformers universal approximators
of sequence-to-sequence functions? In International
ConferenceonLearningRepresentations,2020.
11AttentionwithMarkov
Organization. Theappendixisorganizedasfollows:
• App.AdetailstheTransformerarchitecture,especiallythatoftheattentionmechanism.
• App.Bprovidestheproofsforourtheoreticalresultsonfirst-orderMarkovchains.
• App.Ccontainsadditionalexperimentaldetailsandresultsforfirst-orderMarkovchains, whereasApp.Dcovers
higherorders.
A.TheTransformerarchitecture
WedescribetheTransformerarchitecturefromSec.2.1indetail,usingtheembeddinglayersimplicationfromSec.2.3:
x
n
=x ne+p
n
∈Rd, (Uni-embedding)
(cid:88)
y
n
=x n+W
O
att n,i·W
V
x
i
∈Rd, (Attention)
i∈[n]
z
n
=y n+W 2ReLU(W 1y n)∈Rd, (FF)
logit
n
=⟨a,z n⟩+b ∈R, (Linear)
f θ¯(xn 1)≜P θ¯(x
n+1
=1|xn 1)= σ(logit n). (Prediction)
(cid:124) (cid:123)(cid:122) (cid:125)
∈[0,1]
(i)Embedding: Thediscretetokensx =1andx =0aremappedtothetoken-embeddingseand0inRdrespectively,
n n
wheredistheembeddingdimension. Thepositionalembeddingp ∈Rdencodesthepositionalinformation(varieswith
n
n). Thesumofthesetwoembeddingsconstitutesthefinalinputembeddingx ∈Rd.
n
(ii)Attention: Theattentionlayercanbeviewedasmapppingaqueryandasetofkey-valuepairstoanoutput,which
areallvectors(Vaswanietal.,2017). Thatis, ontopoftheskip-connectionx , theoutputy ∈ Rd iscomputedasa
n n
weighted sum of the values v ≜ W x . The weight assigned to each value, att , is computed by a compatibility
i V i n,i
functionofthequeryvectorq ≜W x andthecorrespondingkeyvectorsk ≜W x foralli∈[n]. Moreprecisely,
n Q n √ i K i
att ≜ softmax((⟨q ,k ⟩,...,⟨q ,k ⟩)/ d) . W ∈ Rm×d aretherespectivekey,query,andvaluematrices,
n,i n 1 n n i K,Q,V
andW ∈Rd×mistheprojectionmatrix. Formulti-headedattention,thesameoperationisperformedonmultipleparallel
O
heads,whoseoutputsareadditivelycombined.
(iii) Feed-forward (FF): The FF transformation consists of a skip-connection and a single-hidden layer with ReLU
activationandweightmatricesW ∈ Rd×r,andW ∈ Rr×d. TheFFlayerisappliedtoken-wiseoneachy ∈ Rd to
2 1 n
outputz ∈Rdwiththesamedimensionality.
n
(iv)Linear: Thelinearlayertransformsthefinaloutputembeddingz toascalarlogit ∈R,withtheweightparameter
n n
a∈Rdandthebiasb∈R.
(v) Prediction: The sigmoid activation finally converts the scalar logits to probabilities for the next-token predic-
tion. Since the vocabulary has only two symbols, it suffices to compute the probability for the symbol 1: f θ¯(xn 1) ≜
P θ¯(x
n+1
=1|xn 1)=σ(logit n)∈[0,1]. Moregenerally,thelogitsareofthesamedimensionalityasthevocabularyand
areconvertedtothepredictionprobabilitiesusingasoftmaxlayer, whichsimplifiestothesigmoidforthebinarycase.
Likewise,thereareasmanytoken-embeddingsasthewordsinthevocabularyandseverallayersofmulti-headedattention
andFFoperationsareappliedalternativelyontheinputembeddingstocomputethefinallogits.
Finally, The Transformer parameters θ¯ ≜ (e,{p }N ,...,b,a) ∈ RD are trained via the cross-entropy loss on the
n n=1
next-tokenprediction:
L(θ¯)≜− N1 (cid:88) E
xn
1+1[x n+1·logf θ¯(xn 1)+(1−x n+1)·log(1−f θ¯(xn 1))], (3)
n∈[N]
12AttentionwithMarkov
B.ProofsofSec.3
WenowpresentourproofsforthetechnicalresultsinSec.3. Towardsthis,wefirstestablishtwousefullemmasontheloss
functionL(·)andthecorrespondinggradientcomputation. Letθ¯ =(e,{p }N ,...,b,a)∈RD bethelistofparameters
n n=1
inthenon-weight-tiedcaseandθ = (e = a,{p }N ,...,b) ∈ RD−d intheweight-tiedcase. Withaslightabuseof
n n=1
notation,byw ∈θ¯wemeanaspecificparameterwamongtheset{e,p ,...,p ,...,b,a}.Sincetheweight-tiedscenario
1 N
isaspecialcaseofthenon-weight-tiedonewithe=a,wedirectlypresenttheresultsforthegeneralnon-weight-tiedcase,
butbothlemmasholdforθ ∈RD−daswell. Firstwestartwiththelossfunction.
Lemma1(LossasKLdivergence). Lettheinputsequencebe{x }N ∼(π(p,q),P(p,q))forsomefixed(p,q)∈(0,1)2,
n n=1
θ¯ = (e,{p }N ,...,b,a) ∈ RD bethefulllistofthetransformerparameters, andL(θ¯)bethecorrespondingcross-
n n=1
entropylossinEq.(1). ThenthelossfunctionL(·)isequivalenttotheKLdivergencebetweentheMarkovkernelandthe
predicteddistribution,i.e.
L(θ¯)= N1 (cid:88) E
xn
1
[D KL(P(x
n+1
=·|x n) ∥P θ¯(x
n+1
=·|xn 1))]+H(x n+1|x n),
(4)
n∈[N]
whereD (P ∥Q)istheKLdivergencebetweentwodistributionsP andQ,andH(x |x )istheentropyrateofthe
KL n+1 n
Markovchain.
Remark 3. Consequently, Eq. (4) highlights that any parameter θ¯ with the predicted probability f θ¯(xn 1) =
P(x =1|x ) is a global minimum for the loss L as D (· ∥ ·) ≥ 0 (Cover & Thomas, 2006). We utilize this
n+1 n KL
factintheproofofThm.1below.
Proof. Wedefertheproofto§B.6.
Lemma 2 (Gradient computation). Consider the same data and parameter setting as in Lemma 1 and L(θ¯) be the
cross-entropylossinEq.(1). Thenforanyparameterw ∈θ¯,
∇ wL(θ¯)=− N1 (cid:88) E
xn
1+1(cid:2) (x n+1−f θ¯(xn 1))·∇ w(cid:0) a⊤z n+b(cid:1)(cid:3)
n∈[N]
(5)
=− N1 (cid:88) E
xn
1
(cid:2) (P(x
n+1
=1|x n)−f θ¯(xn 1))·∇ w(cid:0) a⊤z n+b(cid:1)(cid:3) .
n∈[N]
Remark4. Eq.(5)highlightsthatanyparameterθ¯ withthepredictedprobabilityf θ¯(xn 1)=P(x
n+1
=1|x n)isalsoa
stationarypointforthelossL. WeutilizethisfactintheproofofThm.1below.
Proof. Wedefertheproofto§B.7.
WenowdetailtheproofsoftheoremsinSec.3. WeprovetheglobalminimumresultinThm.1intwoparts,separatelyfor
thecaseswhenp+q ≤1andp+q >1.
B.1.ProofofThm.1forp+q ≤1
Proof. Weassumethatp+q ≤1andthatweuseweighttying,i.e. thelistofparametersθ =(e=a,{p }N ,...,b)∈
n n=1
RD−d. ThusinviewofLemma1andLemma2,itfollowsthatanyθsatisfyingf (xn)=P(x =1|x )isaglobal
θ 1 n+1 n
minimumwithlossequallingtheentropyrate,andisastationarypoint. Henceitsufficestoconstructsuchaθ.
Tobuildourintuitiontowardsdesigningθ ,recallthattheMarkovkernelP(x =1|x )canbesuccintlywrittenas
⋆ n+1 n
P(x =1|x ) = x (1−p−q)+p. Toensurethatf (xn) = x (1−p−q)+p,itsufficesforthetransformerto
n+1 n n θ 1 n
utilizeonlytheinformationfromthecurrentsymbolx andignorethepastxn−1. Inviewofthetransformerarchitecture
n 1
in§A,anaturalwaytorealizethisistoletW
O
= 0andW
2
= 0inAttentionandFFrespectively. Thisimpliesthat
z = y = x = x e+p . Hence the logits are given by logit = ⟨e,z ⟩+b = x ∥e∥2 +⟨e,p ⟩+b. Since
n n n n n n n n n
f (xn)=σ(logit )anditequalstheMarkovkernel,wehavethat
θ 1 n
σ(logit )=σ(x ∥e∥2+⟨e,p ⟩+b)=x (1−p−q)+p.
n n n n
13AttentionwithMarkov
Rewriting,
(cid:18) (cid:19)
x (1−p−q)+p
x ∥e∥2+⟨e,p ⟩+b=log n , x ∈{0,1}.
n n (1−p)(1−x )+qx n
n n
Substitutingx =0andx =1,wefurthersimiplifyto
n n
(cid:18) (cid:19)
p
⟨e,p ⟩+b=log ,
n 1−p
(cid:18) (cid:19)
1−q
∥e∥2+⟨e,p ⟩+b=log .
n q
Subtractingboththeequationsweobtainthataglobalminimumθshouldsatisfy
(cid:18) (cid:19)
(1−p)(1−q)
∥e∥2 =log ,
pq
(6)
(cid:18) (cid:19)
p
⟨e,p ⟩+b=log .
n 1−p
Notetheabovechoiceofeiswell-definedsince (1−p)(1−q) >1whenp+q <1andhencelog(1−p)(1−q) >0. Whilethere
pq pq
existinfinitelymanysolutionsfor(e,p ,b)satisfyingEq.(6),acanonicalsuchsolutionfortheglobalminimumθ =θ is
n ⋆
(cid:32) (cid:115) (cid:33)
1 (1−p)(1−q) p
θ = e=a=1 log ,{p =0}N ,W =0,W ,W =0,W ,b=log , (7)
⋆ d pq n n=1 O K,Q,V 2 1 1−p
where 1 ∈ RD−d denotes the all-one vector, the position embeddings p are set to zero, W ∈ Rm×d, and
n K,Q,V
W ∈Rr×dcanbesettoanyarbitraryvalue. Thisconcludestheexplicitconstructionofθ andtheproof.
1 ⋆
B.2.ProofofThm.1forp+q >1
Proof. We use a similar idea as in the proof for the p+q ≤ 1 case by constructing a θ ∈ RD−d satisfying f (xn) =
θ 1
P(x =1|x )=x (1−p−q)+p. However,inthiscaseweneedtousetheReLUcomponentoftheFFmechanism
n+1 n n
unliketheearliercasewherewesetW =0. Nowwestartwithconstructingθ .
2 ⋆
Lettheembeddinge = a = 1andthepositionalencodingp = −11foralln ≥ 1,where1 ∈ Rd denotestheall-one
n 2
vector. Thusx
n
=α n1withα
n
=+1
2
whenx
n
=1andα
n
=−1
2
whenx
n
=0. NowletW
O
=0intheAttentionlayer.
Hencey
n
=x
n
=α n1. FortheFFlayer,letW 1andW 2besuchthat(tobedeterminedlater)
W ReLU(W y )=β 1,
2 1 n n
andhence
z =x +W ReLU(W y )=α 1+β 1=(α +β )1.
n n 2 1 n n n n n
Thusthelogitsaregivenbylogit =σ(⟨a,z ⟩+b)=σ(d(α +β )+b).Sincef (xn)=σ(logit )=P(x =1|x ),
n n n n θ 1 n n+1 n
wehavethat
σ(logit )=σ(d(α +β )+b)=x (1−p−q)+p, x ∈{0,1}.
n n n n n
Rewriting,
(cid:18) (cid:19)
x (1−p−q)+p
d(α +β )+b=log n , x ∈{0,1}.
n n (1−p)(1−x )+qx n
n n
Substitutingx =0andx =1,anddenotingcorrespondingβ’sbyβ andβ (withaslightabuseofnotation),wefurther
n n 1 0
simiplifyto
(cid:18) (cid:19) (cid:18) (cid:19)
1 p
d − +β +b=log , (8)
2 0 1−p
14AttentionwithMarkov
(cid:18) (cid:19) (cid:18) (cid:19)
1 1−q
d +β +b=log .
2 1 q
Subtractingboththeaboveequationsweobtain
(cid:18) (cid:19)
(1−p)(1−q)
d(1+β −β )=log .
1 0 pq (9)
(cid:124) (cid:123)(cid:122) (cid:125)
<0whenp+q>1
Nowitsufficestofindβ andβ satisfyingEq.(9). Recallthatβ obeysW ReLU(W y )=β 1. LetW =w11⊤
1 0 n 2 1 n n 1
andW =−W⊤forsomew ∈R. Sincey =α 1,wehave
2 1 n n
−w11⊤ReLU(w11⊤α 1)=β 1.
n n
Simplifying,
1
β =−w2d·1⊤ReLU(α 1), α =± .
n n n 2
Thusβ =0(correspondingtox =0andα =−1)andβ =−w2d2 (otherwise). SubstitutingtheminEq.(9),wehave
0 n n 2 1 2
w2d2 1 (cid:18) (1−p)(1−q)(cid:19)
1− = ·log .
2 d pq
Letw =w beasolutiontotheaboveequation,i.e.
⋆
(cid:115)
(cid:18) (cid:18) (cid:19)(cid:19)
2 1 (1−p)(1−q)
w = 1− ·log .
⋆ d2 d pq
(cid:16) (cid:17)
Bysubstitutingβ =0inEq.(8)weobtainthebiasb =log p + d. Piecingeverythingtogether,let
0 ⋆ 1−p 2
(cid:16) (cid:17)
θ = e=a=1,{p =(−1/2)1}N ,W =0,W ,W =w 11⊤,W =−W⊤,b=b , (10)
⋆ n n=1 O K,Q,V 1 ⋆ 2 1 ⋆
andwearedone.
B.3.ProofofThm.2
Proof. Firstweconstructanexplicitθ ∈RD−dsuchthatitsatifiesproperties(ii)–(iv)ofThm.2i.e. itisastationarypoint
π
withlossvaluebeingtheentropyofthemarginalH(π)andthatitcapturesthemarginaldistributionP(x =1)=π .
n+1 1
ThenwecomputeitsHessianandshowthatitisalocalminimumforp+q > 1thusprovingproperty(i). Ontheother
hand,thesameθ couldeitherbealocalminimumorsaddlepointforp+q <1. Westartwiththeconstruction.
π
Recall that the full set of the Transformer parameters in the weight-tied case is given by θ = (e =
a,{p }N ,W ,W ,W ,W ,b)∈RD−d. Defineθ ∈RD−dtobe
n n=1 O K,Q,V 2 1 π
(cid:18) (cid:18) (cid:19)(cid:19)
p
θ = e=a=0,{p }N ,W =0,W ,W =0,W ,b=log , (11)
π n n=1 O K,Q,V 2 1 q
where{p }N ⊂Rd,W ∈Rm×d,andW ∈Rr×dcanbesettoanyarbitraryvalue. Nowwestartwithproperty
n n=1 K,Q,V 1
(ii).
(ii): f (xn)=P (x =1|xn)=P(x =1)=π .
θπ 1 θπ n+1 1 n+1 1
Sincea = 0,itfollowsfrom(Linear)and(Prediction)layersthatf θπ(xn 1) = σ(b) = σ(log(p/q)) = p+p
q
= π 1. Inother
words,themodelignoresalltheinputsandoutputsaconstantprobabilityπ .
1
(iii): L(θ )=H(x )=H(π).
π n+1
15AttentionwithMarkov
Sincef (·)=π =E[x ],itfollowsfromEq.(3)that
θπ 1 n+1
1 (cid:88)
L(θ )=− E [x ·logf (xn)+(1−x )·log(1−f (xn))]
π N xn 1+1 n+1 θπ 1 n+1 θπ 1
n∈[N]
1 (cid:88)
=− E [x ·logπ +(1−x )·logπ ]
N xn 1+1 n+1 1 n+1 0
n∈[N]
1 (cid:88)
= [−π logπ −π logπ ]
N 1 1 0 0
n∈[N]
=H(π)=H(x ).
n+1
(iv): ∇L(θ )=0.
π
Atθ =θ ,theindividuallayeroutputsoftheTransformer(§A)aregivenby
π
x ∈{0,1}−U −n −i −-e −m −b −e −dd −i −n →g x =p −A −t −te −n −ti −o →n y =p −F −→F z =p −L −i −ne −a →r logit =b−P −r −e −di −c −tio −→n f (xn)=π .
n n n n n n n n θπ 1 1
Inotherwords,noneofthelayeroutputsdependontheinputsequence{x }N . InviewofthisfactandE[x ]=π ,
n n=1 n+1 1
usingLemma2thegradientwithrespecttoaofLatθ =θ isgivenby
π
∇ L=− 1 (cid:88) E (cid:2) (x −f (xn))·∇ (cid:0) a⊤z +b(cid:1)(cid:3)
a N xn 1+1 n+1 θπ 1 a n
n∈[N]
1 (cid:88)
=− E [(x −π )(z +∇ z ·a)]
N xn 1+1 n+1 1 n a n
n∈[N]
(a ==0) − 1 (cid:88) E [(x −π )·p ]
N xn+1 n+1 1 n
n∈[N]
=0.
Similarly,forb:
∇ L=− 1 (cid:88) E (cid:2) (x −f (xn))·∇ (cid:0) a⊤z +b(cid:1)(cid:3) =− 1 (cid:88) E [x −π ]=0.
b N xn 1+1 n+1 θπ 1 b n N xn+1 n+1 1
n∈[N] n∈[N]
Foranyotherparameterw ∈ θ apartfroma,b,weseefromEq.(5)thatthegradient∇ Lhastheterm∇ (a⊤z ) =
w w n
(∇ z )·ainsidetheexpectationE [...]. Sincea=0,thisequalszeroandhence∇ L=0.
w n xn+1 w
1
Together∇L(θ )=0.
π
(i): θ isabadlocalminimumforLwhenp+q >1.
π
Towards establishing this, we first let α = (b,a) and β = ({p }N ,W ,W ,W ,W ) be two different sets
n n=1 O K,Q,V 2 1
ofparameterscomprisingθ,i.e. θ = (α,β)andcomputetheHessianH ≜ ∇(2)L(θ)| andshowthatithasthe
π θ=θπ
followingblock-diagonalstructure:
(cid:20) (cid:21)
H 0
H = α ,
π 0 0
whereH correspondstotheHessianwithrespecttotheparametersaandbinα. Furtherweshowthatifp+q > 1,
α
H ≻0i.e. itispositive-definite. Thishelpsusinestablishingthatθ alocalminimum. NowwestartwiththeHessian
α π
computation.
Hessiancomputation. WefirstcomputetheHessianwithrespecttoα.
16AttentionwithMarkov
FromLemma2,wehavethatsecondderivativewithrespecttobatθ =θ isgivenby
π
 
∇( b2)L=∇ b(∇ bL)=∇ b− N1 (cid:88) E
xn
1+1[x n+1−f θ(xn 1)]
n∈[N]
( =a) 1 (cid:88) E(cid:2) f (xn)(1−f (xn))·∇ (cid:0) a⊤z +b(cid:1)(cid:3)
N θ 1 θ 1 b n
n∈[N]
(θ= =θπ) 1 (cid:88) E[π π ]
N 1 0
n∈[N]
=π π >0,
0 1
where(a)followsfromthefactthat∇ f (xn) = ∇ σ(a⊤z +b) = f (xn)(1−f (xn))·∇ (cid:0) a⊤z +b(cid:1) . Nowwe
b θ 1 b n θ 1 θ 1 b n
computethesecondderivativewithrespecttoa. FromLemma2,weobtain
 
∇( a2)L=∇ a(∇ aL)=∇ a− N1 (cid:88) E xn 1+1(cid:2) (x n+1−f θ(xn 1))·∇ a(a⊤z n)(cid:3) 
n∈[N]
 
1 (cid:88)
=∇ a−
N
E[(x n+1−f θ(xn 1))(z n+(∇ az n)·a)]
n∈[N]
( =a) 1 (cid:88) E(cid:2)
f (1−f )(z +(∇ z )·a)(z +(∇ z
)·a)⊤(cid:3)
N θ θ n a n n a n
n∈[N]
1 (cid:88)
− E[(x −f (xn))(2∇ z )],
N n+1 θ 1 a n
n∈[N]
where(a)followsfromthegradientoftheproductruleandthefactthat∇ f (xn)=f (xn)(1−f (xn))(z +(∇ z )·a).
a θ 1 θ 1 θ 1 n a n
Atθ =θ ,thisfurthersimplifiesto
π
∇(2)L( =b) 1 (cid:88) (cid:0)E[π π ·p p⊤]−2E[(x −π )x I](cid:1)
a N 1 0 n n n+1 1 n
n∈[N]
( =c) 1 (cid:88) (cid:0) (π π )·p p⊤)−2E[(x (1−p−q)+p−π )x I](cid:1)
N 0 1 n n n 1 n
n∈[N]
= 1 (cid:88) (cid:0) (π π )·p p⊤)−2E[x (π −q)I](cid:1)
N 0 1 n n n 0
n∈[N]
= 1 (cid:88) (cid:0) (π π )·p p⊤)−2π (π −q)I](cid:1)
N 0 1 n n 1 0
n∈[N]
 
(cid:88) p p⊤ (cid:18) q (cid:19)
=π 0π 1 n Nn −2 1−
π
I
0
n∈[N]
 
( =d) π 0π 1 (cid:88) p n Np⊤ n +2(p+q−1)I,
n∈[N]
where(b)followsfromthefactthat∇ z =x Iatθ =θ whereIistheidentitymatrixisRd×d,(c)fromtheobservation
a n n π
thatE[x |x ]=x (1−p−1)+p,and(d)fromthefactthatπ = q . Nowwecomputethecross-derivativeofsecond
n+1 n n 0 p+q
order∇ L. Again,invokingLemma2,
a,b
 
1 (cid:88)
∇ abL=∇ a(∇ bL)=∇ a−
N
E
xn
1+1[x n+1−f θ(xn 1)]
n∈[N]
17AttentionwithMarkov
1 (cid:88)
= E[f (1−f )(z +(∇ z )·a)]
N θ θ n a n
n∈[N]
(θ= =θπ) 1 (cid:88) E[π π ·p ]
N 1 0 n
n∈[N]
 
(cid:88) p
=π 0π 1 Nn .
n∈[N]
Piecingalltheresultstogetherweobtainthatforα=(b,a),itscorrespondingHessianisgivenby
(cid:20) 1 u⊤(cid:21) (cid:88) p (cid:88) p p⊤
H ≜∇(2)L(α ,β )=π π , u≜ n,V ≜ n n +2(p+q−1)I. (12)
α α π π 0 1 u V N N
n∈[N] n∈[N]
WenowshowthattheHessianH ≜∇(2)L(α ,β )=0. Recallthatβ =({p }N ,W ,W ,W ,W ). For
β β π π n n=1 O K,Q,V 2 1
anyw ,w ∈β,Lemma2impliesthat
1 2
 
1 (cid:88)
∇ w1w2L=∇ w1(∇ w2L)=∇ w1−
N
E[(x n+1−f θ(xn 1)(∇ w2z n·a))]
n∈[N]
( =a) 1 (cid:88) E(cid:2)
f (1−f )(∇ z ·a)(∇ z
·a)⊤(cid:3)
N θ θ w2 n w1 n
n∈[N]
(θ= =θπ)
0,
where(a)followsfromthefactthat∇ f (xn)=∇ σ(a⊤z +b)=f (xn)(1−f (xn))(∇ z ·a). ThusH =0.
w1 θ 1 w1 n θ 1 θ 1 w1 n β
Similarly,wecanshowthatH =∇ L=0andhenceH =H⊤ =0. Thus,
αβ αβ βα αβ
(cid:20) (cid:21)
H 0
H =∇(2)L(θ )= α
π π 0 0
NowitremainstoshowthatH ispositive-definitewhenp+q >1anditimpliesthatθ isalocalminimum.
α π
(cid:20)
1
u⊤(cid:21)
(cid:80)
Positive-definitenss of H . Recall from Eq. (12) that H = , where u = p /N,V =
α α u V n∈[N] n
(cid:80) p p⊤/N +2(p+q −1)I. From the characterization of positive-definiteness by Schur’s complement (Horn
n∈[N] n n
&Johnson,2012),wehavethatH ≻0⇔1>0andV −uu⊤ ≻0. Wehavethat
α
  ⊤
(cid:88) p p⊤ (cid:88) p (cid:88) p
V −uu⊤ =2(p+q−1)I+ n n − n  n 
N N N
n∈[N] n∈[N] n∈[N]
  ⊤
(cid:88) 1 (cid:88) p (cid:88) p
=2(p+q−1)I+
N
p n− Nn p n− Nn 
n∈[N] n∈[N] n∈[N]
=2(p+q−1)I+Cov({p }N ),
n n=1
(cid:16) (cid:17)(cid:16) (cid:17)⊤
where Cov({p }N ) = (cid:80) 1 p −(cid:80) p n p −(cid:80) p n is the covariance matrix of the set
n n=1 n∈[N] N n n∈[N] N n n∈[N] N
{p }N and hence positive semi-definite. Thus if p + q > 1, we have that 2(p + q − 1)I ≻ 0 and together, we
n n=1
obtainthatV −uu⊤ ≻0. HenceH ≻0. Nowitremainstoshowthatθ isalocalminimum.
α π
H is positive-definite implies θ is a local minimum. Since H ≻ 0, let H ≽ λI for some λ > 0 (in fact
α π α α
λ=2(p+q−1)works). Sinceθ =(α,β)∈RD−d,interpretL(θ)=L(α,β)asafunctionoftwovariablesαandβ
withappropriatedimensions. WeknowthefollowingfactsaboutL(·,·):
18AttentionwithMarkov
• Fact1. α(cid:55)→L(α,β )hasalocalminimum(asafunctionofonevariable)atα=α (sinceH ≻0).
π π α
• Fact2. β (cid:55)→L(α ,β)isconstantinβ(sincea =0,theprobabilityf (xn)isconstantw.r.t.z andhencew.r.t.β
π π θ 1 n
(Linear)).
(cid:20) (cid:21)
H 0
• Fact3. ∇L(α ,β )=0andH =∇(2)L(α ,β )= α withH ≽λI.
π π π π π 0 0 α
Usingthesefactsnowweshowthat(α ,β )=θ isalsoalocalminimumintwo-variables.Weprovethisbycontradiction.
π π π
Supposethat(α ,β )isnotalocalminimumforL(·,·). Withoutlossofgenerality,byashiftofcordinatestreatθ asthe
π π π
origin,i.e. (α =0,β =0)isnotalocalminimumforL(·,·). Thenthereexistsanunitdirectiond=(u,v)∈RD−d
π π
with∥d∥2 =∥u∥2+∥v∥2 =1andan0<ε <1suchthat
0
L(εd)<L(0), ∀0<ε≤ε <1. (13)
0
Clearly∥u∥>0,otherwiseitwillcontradictFact2. Usingthedefinitionofdirectional-derivative,wehavethat
⟨∇L(εd),d⟩−⟨∇L(0),d⟩
d⊤∇(2)L(0,0)d= lim
ε→0 ε
⟨∇L(εd),d⟩
= lim .
ε→0 ε
Ontheotherhand,usingtheHessianstructuretheLHSequalsd⊤∇(2)L(0,0)d≥λ∥u∥2 ≜K >0. Thus
1
⟨∇L(εd),d⟩
lim =K >0.
ε→0 ε 1
Thusthereexistsanε >0andK >0suchthat
1
⟨∇L(εd),d⟩
≥K, ∀0<ε≤ε ,
ε 1
whichimplies
⟨∇L(εd),d⟩≥Kε, ∀0≤ε≤ε .
1
Definingthefunctiong :R →Rasg(ε)=L(εd),weobtainthatg′(ε)=⟨∇L(εd),d⟩≥Kεfor0≤ε≤ε . Usingthe
+ 1
fundamentaltheoremofCalculus,wehavethatforany0≤ε≤ε ,
1
(cid:90) ε
g(ε)−g(0)= g′(t)dt
0
(cid:90) ε
≥ Ktdt
0
Kε2
= .
2
Thusg(ε)=L(εd)≥L(0)+ Kε2 forall0≤ε≤ε whereasL(εd)<L(0)forall0<ε<ε fromEq.(13). Choosing
2 1 0
ε =min(ε ,ε ),wehaveacontradictionfor0<ε<ε . Thus0≡θ =(α ,β )isalocalminimum.
⋆ 0 1 ⋆ π π π
B.4.ProofofThm.3
Proof. By extending θ ∈ RD−d to θ¯ ≜ (θ ,a ) ∈ RD, it follows from the Transformer architecture in § A that
⋆ ⋆ ⋆ ⋆
P anθ¯ d⋆ §(x Bn .+ 11 e= sta1 bl| isx hn 1 ,) pr= edP icθ ti⋆ o( nx pn r+ o1 ba= bil1 it| yx en 1 q) ua= lliP ng(x thn e+ k1 e= rne1 l| isx an s) u, ft fih ce ieM na tr ck oo nv dik tie or nne fl o. rA gls ot bh ae l-p or po to imf ao lf itT y.h Hm e. n1 cein θ¯§B is.2
a
⋆
globalminimumforL(·)inRD.
19AttentionwithMarkov
B.5.ProofofThm.4
Proof. Since θ¯ ≜ (θ ,a ) ∈ RD is a canonical extension of θ = (e = a ,...,b ) ∈ RD−d, which is a local-
π π π π π π π
minimumforL(·)inRD−d,followingthesame-stepsforthegradientcomputationandprobabilityevaluationasinproof
ofThm.2in§B.3,itimmediatelyfollowsthatθ¯ alsosatisfiesproperties(ii)-(iv),i.e. it’sastationarypoint,itcaptures
π
themarginal,sinceP θ¯ π(x n+1 =1|xn 1)=P θπ(x n+1 =1|xn 1)=P(x n+1 =1)=π,andhenceitslossequalsentropy
ofstationarydistributionH(π). Inasimilarfashion,theHessiancomputationisessentiallythesameexceptforaslight
differenceintheHessianstructure,i.e.
(cid:20) (cid:21)  1 u⊤ 0 
H 0
H(θ¯ π)≜∇(2)L(θ¯ π)= 0α
0
, H
α
=π 0π 1u V (p+q−1)I,
0 (p+q−1)I 0
where u ≜ (cid:80) p n,V ≜ (cid:80) p np⊤ n. In the weight-tied case, we observe that the matrix V also contains the
n∈[N] N n∈[N] N
(p+q−1)I termswhichinthenon-weight-tiedcasegetsde-coupled(duetoseparateeandaparameters). Infact,H
α
correspondstotheHessianw.r.ttheparametersα=(b,a,e),i.e. H =∇(2)L| . NowitremainstoshowthatH is
indefiniteandhenceθ¯ asaddlepoint.
α α α=απ α
π
Clearly,H cannotbenegativedefinitesincewithd = (1,0,...,0),wehaved⊤H d = π π > 0for(p,q) ∈ (0,1).
α α 0 1
Nowweshowthatitcannotbepositivedefiniteeither. Denoting
H =π π (cid:20) 1 b⊤(cid:21) , b≜(cid:20)(cid:80) n∈[N] p Nn(cid:21) , C ≜(cid:34) (cid:80) n∈[N] p n Np⊤ n (p+q−1)I(cid:35) .
α 0 1 b C 0 (p+q−1)I 0
Usingthecharacterizationofpositive-definitenessbySchur’scomplement(Horn&Johnson,2012),wehavethatH ≻
α
0⇔1>0andC−bb⊤ ≻0. Thiscanbefurthersimplifiedto
M ≜C−bb⊤ =(cid:34) (cid:80) n∈[N] p n Np⊤ n (p+q−1)I(cid:35) −(cid:20)(cid:80) n∈[N] p Nn(cid:21)(cid:104) (cid:80) p⊤ n 0(cid:105)
(p+q−1)I 0 0 n∈[N] N
(cid:20) Cov({p }N ) (p+q−1)I(cid:21)
= n n=1 ,
(p+q−1)I 0
(cid:16) (cid:17)(cid:16) (cid:17)⊤
where Cov({p }N ) = (cid:80) 1 p −(cid:80) p n p −(cid:80) p n is the covariance matrix of the set
n n=1 n∈[N] N n n∈[N] N n n∈[N] N
{p }N . Now we show that M cannot be positive definite. Suppose not. Then there exists a λ > 0 such that
n n=1
v⊤Mv ≥λ∥v∥2forallv =(v ,v )∈R2d. Thisfurtherimpliesthat
1 2
v⊤Cov({p }N )v +2(p+q−1)⟨v ,v ⟩≥λ∥v∥2, ∀v ,v ∈Rd.
1 n n=1 1 1 2 1 2
Takingv =0theaboveinequalityimplesthatλ∥v ∥2 ≤0forallv ∈Rd,whichisacontradiction. HenceM cannotbe
1 2 2
positivedefiniteandconsequentlyneithercanH .
α
B.6.ProofofLemma1
Proof. ConsiderthelossfunctionL(·)giveninEq.(1). Wecanrewriteitasfollows:
L(θ¯)=− N1 (cid:88) E
xn
1+1[x n+1·logf θ¯(xn 1)+(1−x n+1)·log(1−f θ¯(xn 1))]
n∈[N]
1 (cid:88) (cid:104) (cid:105)
=−
N
E
xn
1
E
xn+1|xn
1[x n+1]·logf θ¯(xn 1)+E
xn+1|xn
1[1−x n+1]·log(1−f θ¯(xn 1))
n∈[N]
=−
1 (cid:88)
E
(cid:104)
P(x =1|x )log
f θ¯(xn 1)
+P(x =0|x )log
1−f θ¯(xn 1)
N xn 1 n+1 n P(x =1|x ) n+1 n P(x =0|x )
n+1 n n+1 n
n∈[N]
1 1 (cid:105)
−P(x =1|x )log −P(x =0|x )log
n+1 n P(x =1|x ) n+1 n P(x =0|x )
n+1 n n+1 n
20AttentionwithMarkov
=−
1 (cid:88)
E
(cid:104)
P(x =1|x )log
f θ¯(xn 1)
+P(x =0|x )log
1−f θ¯(xn 1) (cid:105)
N xn 1 n+1 n P(x =1|x ) n+1 n P(x =0|x )
n+1 n n+1 n
n∈[N]
(cid:104) (cid:104) 1 1 (cid:105)(cid:105)
−E E P(x =1|x )log +P(x =0|x )log
xn xn 1−1|xn n+1 n P(x
n+1
=1|x n) n+1 n P(x
n+1
=0|x n)
=−
1 (cid:88)
E
(cid:104)
P(x =1|x )log
f θ¯(xn 1)
+P(x =0|x )log
1−f θ¯(xn 1) (cid:105)
N xn 1 n+1 n P(x =1|x ) n+1 n P(x =0|x )
n+1 n n+1 n
n∈[N]
(cid:104) 1 1 (cid:105)
−E P(x =1|x )log +P(x =0|x )log .
xn n+1 n P(x =1|x ) n+1 n P(x =0|x )
n+1 n n+1 n
Sincef θ¯(xn 1)=P θ¯(x
n+1
=1|x n),wehavethatthefirsttermaboveis
P(x =1|x )log
f θ¯(xn 1)
+P(x =0|x )log
1−f θ¯(xn 1)
n+1 n P(x =1|x ) n+1 n P(x =0|x )
n+1 n n+1 n
=−D KL(P(x
n+1
=·|x n) ∥P θ¯(x
n+1
=·|xn 1)).
Further,observethatthesecondtermisexactlytheentropyrateH(x |x ). Hence,theaboveexpressionfortheloss
n+1 n
reducesto
L(θ¯)= N1 (cid:88) E
xn
1
[D KL(P(x
n+1
=·|x n) ∥P θ¯(x
n+1
=·|xn 1))]+H(x n+1|x n),
n∈[N]
andwearedone.
B.7.ProofofLemma2
Proof. Itsufficestoshowthatforanycomponentθ¯ ofθ¯ ∈RD,
j
(cid:20) (cid:21)
∂∂
θ¯
jL(θ¯)=− N1 (cid:88) E
xn 1+1
(x n+1−f θ¯(xn 1))· ∂∂
θ¯
j
(cid:0) a⊤z n+b(cid:1)
n∈[N]
(cid:20) (cid:21)
=− N1 (cid:88) E
xn
1
(P(x
n+1
=1|x n)−f θ¯(xn 1))· ∂∂
θ¯
(cid:0) a⊤z n+b(cid:1) .
j
n∈[N]
RecallfromEq.(3)thatL(·)isgivenby
L(θ¯)=− N1 (cid:88) E
xn
1+1[x n+1·logf θ¯(xn 1)+(1−x n+1)·log(1−f θ¯(xn 1))],
n∈[N]
whichimpliesthat
(cid:20) (cid:21)
∂∂
θ¯
jL(θ¯)=− N1 (cid:88) E
xn 1+1
x n+1· ∂∂
θ¯
j
logf θ¯(xn 1)+(1−x n+1)· ∂∂
θ¯
j
log(1−f θ¯(xn 1))
n∈[N]
(cid:20) (cid:21)
1 (cid:88) 1 ∂ 1 ∂
=−
N
n∈[N]E
xn 1+1
x n+1·
f θ¯(xn 1)∂θ¯
jf θ¯(xn 1)+(1−x n+1)·
1−f θ¯(xn 1)∂θ¯
j(1−f θ¯(xn 1)) .
Sincef θ¯(xn 1)=σ(a⊤z n+b),wefirstnotethatthederivativeofσisgivenbyσ′(z)= (1+e e− −z
z)2
=σ(z)(1−σ(z)). Hence,
thederivative ∂∂
θ¯
jf θ¯(xn 1)canbewrittenas
∂∂
θ¯
f θ¯(xn 1)= ∂∂
θ¯
σ(a⊤z n+b)=σ(a⊤z n+b)(cid:2) 1−σ(a⊤z n+b)] ∂∂
θ¯
(a⊤z n+b)
j j j
=f θ¯(xn 1)(cid:2) 1−f θ¯(xn 1)(cid:3) ∂∂
θ¯
(a⊤z n+b).
j
21AttentionwithMarkov
Pluggingthisintotheaboveexpression,wehave
(cid:20) (cid:21)
∂∂
θ¯
jL(θ¯)=− N1 (cid:88) E
xn 1+1
x n+1·(cid:2) 1−f θ¯(xn 1)(cid:3) ∂∂
θ¯
j(a⊤z n+b)−(1−x n+1)·f θ¯(xn 1) ∂∂
θ¯
j(a⊤z n+b)
n∈[N]
(cid:20) (cid:21)
=− N1 (cid:88) E
xn 1+1
x n+1·(cid:2) 1−f θ¯(xn 1)(cid:3) ∂∂
θ¯
j(a⊤z n+b)−(1−x n+1)·f θ¯(xn 1) ∂∂
θ¯
j(a⊤z n+b)
n∈[N]
(cid:20) (cid:21)
=− N1 (cid:88) E
xn 1+1
(x n+1−f θ¯(xn 1))· ∂∂
θ¯
j
(cid:0) a⊤z n+b(cid:1)
n∈[N]
(cid:20) (cid:21)
=− N1 (cid:88) E
xn
1
(E
xn+1|xn
1[x n+1]−f θ¯(xn 1))· ∂∂
θ¯
(cid:0) a⊤z n+b(cid:1)
j
n∈[N]
(cid:20) (cid:21)
=− N1 (cid:88) E
xn
1
(P(x
n+1
=1|x n)−f θ¯(xn 1))· ∂∂
θ¯
(cid:0) a⊤z n+b(cid:1) ,
j
n∈[N]
andwearedone.
22AttentionwithMarkov
C.Additionalresultsforfirst-orderMarkovchains
C.1.Modelarchitectureandhyper-parameters
Table1. Parametersinthetransformerarchitecturewiththeirshape.
Parameter Matrixshape
transformer.wte 2×d
transformer.wpe N ×d
transformer.h.ln_1(×ℓ) d×1
transformer.h.attn.c_attn(×ℓ) 3d×d
transformer.h.attn.c_proj(×ℓ) d×d
transformer.h.ln_2(×ℓ) d×1
transformer.h.mlp.c_fc(×ℓ) 4d×d
transformer.h.mlp.c_proj(×ℓ) d×4d
transformer.ln_f d×1
Table2. Settingsandparametersforthetransformermodelusedintheexperiments.
Dataset k-thorderbinaryMarkovsource
Architecture BasedontheGPT-2architectureasimplementedin(Pagliardini,2023)
Batchsize Grid-searchedin{16,50}
Accumulationsteps 1
Optimizer AdamW(β =0.9,β =0.95)
1 2
Learningrate 0.001
Scheduler Cosine
#Iterations 8000
Weightdecay 1×10−3
Dropout 0
Sequencelength Grid-searchedin{512,1024,2048}
Embeddingdimension Grid-searchedin{4,8,16,32,64}
Transformerlayers Between1and6dependingontheexperiment
Attentionheads Grid-searchedin{1,2,4,8}
Maskwindow Between2andfullcausalmaskingdependingontheexperiment
Repetitions 3or5
C.2.Empiricalformulaforp+q <1
Inthissectionwecomputethefunctionf (xn)thatgivesthenext-symbolprobabilitypredictedbythenetwork,usingthe
θ 1
valuesoftheweightmatricesobtainedfiveindependentexperimentruns. Bysubstitutingtheempiricalweightsintothe
transformerarchitecturefrom§A,i.e.
x
n
=x ne+p
n
∈Rd, (Uni-embedding)
(cid:88)
y
n
=x n+W
O
att n,i·W
V
x
i
∈Rd, (Attention)
i∈[n]
z
n
=y n+W 2ReLU(W 1y n)∈Rd, (FF)
logit
n
=⟨a,z n⟩+b ∈R, (Linear)
f θ(xn 1)≜P θ(x
n+1
=1|xn 1)=σ(logit n). (Prediction)
Wecanobtainanexplicitexpressionforf (xn)asitisactuallylearnedbythemodel. Wenowanalyzeeachsectionofthe
θ 1
modelarchitectureseparately.
23AttentionwithMarkov
Embedding. Allthefiveindependentrunsshowthatthewordembeddingvectorehasthestructure
e=e·v (14)
wherev = (v ,...,v )issuchthatv ∈ {−1,+1}foralli,i.e.,v ∈ {−1,1}d,andeissomeconstant. Moreover,the
1 d i
positionalembeddingsareapproximatelyconstantacrosspositionsn,andtheyshareasimilarstructuretoe. Inparticular,
wealwayshavethat
p =p=p·v ∀n (15)
n
forsomeconstantp∈R. Furthermore,theconstantsarealwayssuchthatp<0ande+p>0.
Attention. Acrossalltheruns,weobservethatthecontributionoftheattentionmechanismisnegligiblecomparedtothe
skip-connection. Inparticular,weobservethat
(cid:80)
∥W att ·W x ∥
O i∈[n] n,i V i
≈0.01 (16)
∥y ∥
n
uniformlyforalln. Therefore,wecanusetheapproximation
y ≈x ∀n. (17)
n n
FF.FortheMLPlayer,weobservethatW andW haveaclearjointstructure. Infact,weempiricallyseethat
1 2
W =w ·w·vT (18)
1 1
wherevisagainthesamevectorasinEq.(14),w ∈{−1,1}r andw ∈R. Hence,W isarank-onematrix. Ascustomary
1 1
intheGPT-2model,forourexperimentsweusedr =4d=16. Furthermore,weseethat
W =WT. (19)
2 1
Duetothisstructureandtheformulafory describedabove,wehave
n
W y =W x =w d(ex +p)w (20)
1 n 1 n 1 n
Letnowr =ReLU(W y ). Duetothefactthatp<0ande+p>0,wehavethat,ifx =1,
1 n n
(cid:40)
e+p, ifw =1,
r = i (21)
i
0, ifw =−1.
i
Whileifx =0,
n
(cid:40)
0, ifw =1,
r = i (22)
i
−p, ifw =−1.
i
Letβ =(cid:80)r 1 . SinceW =WT =w v·wT,wehavethat,forr =W r,
i=1 {wi=1} 2 1 1 (cid:101) 2
(cid:40)
w2d(e+p)β·v, ifx =1,
r = 1 n (23)
(cid:101) w2dp(r−β)·v, ifx =0.
1 n
Ormorecompactly,
r =w2d(ex +p)((2β−r)x +r−β)·v, (24)
(cid:101) 1 n n
and
z =y +r =(ex +p)(1+w2d((2β−r)x +r−β))·v (25)
n n (cid:101) n 1 n
Linear. Sincea=eduetoweight-tying,wehave
logit =ed(ex +p)(1+w2d((2β−r)x +r−β))+b (26)
n n 1 n
24AttentionwithMarkov
Prediction. Wecannowplugintheempiricalvaluesobtainedbyaveragingfiveindependentruns. Thenumericalresults
thatwegetare
e=0.3618
p=−0.1539
w =0.3264 (27)
1
b=−0.1229
β =5
PluggingthesenumbersintoEq.(26),weget
(cid:40)
0.8191, ifx =1,
logit = n (28)
n −1.3897, ifx =0.
n
Hence,byapplyingthesigmoidfunctiontothelogitvalues,weobtainthepredictedprobabilities
(cid:40)
σ(0.8191)=0.694, ifx =1,
f (xn)=P (x =1|x )= n (29)
θ 1 θ n+1 n σ(−1.3897)=0.199, ifx =0.
n
Thenumericalresultscorrespondalmostexactlytotheexpectedtheoreticalvaluesof1−q =0.7andp=0.2.
C.3.Empiricalresultsforfirst-orderMarkovchainswithdepth≥2
0.80
ℓ = 1
ℓ = 2
0.75
ℓ = 4
ℓ = 8
E n t r o p y rate
E n t r o p y ofstationarydistribution
0.70
0.65
0.60
0 25 50 75 100 125 150 175 200
Inter ation
Figure4.First-orderMarkovchains:Beneficialeffectofthenumberoftransformerlayersℓonthetestlossconvergenceoftransformer
modelswithweight-tying,inthecasep+q>1(p=0.5,q=0.8).Single-layermodelstypicallyconvergetothebadlocalminimum.
Onthecontrary,whilethelocalminimumremainspresent,higher-depthmodelsescapemoreeasilyfromit,eventuallyconvergingtothe
globalminimum.
25
LssoltseTAttentionwithMarkov
D.Additionalresultsforhigher-orderMarkovchains
0.675
0.650
0.625 O r d e r 2
O r d e r 3
O r d e r 6
0.600 E n t r o p y r a t e
H ( π )
0.575
0.550
0 20 40 60 80 100
MaskwindowW
Figure5.EffectofmaskwindowW onthefinaltestlossforasingle-layertransformermodel,asafunctionoftheorderoftheMarkov
sourcegeneratingthedata.Thebehaviorseemstobeessentiallyindependentoftheorderofthesource.
26
LssoltseT