“Task Success” is not Enough: Investigating the Use of Video-Language Models
as Behavior Critics for Catching Undesirable Agent Behaviors
LinGuan*1 YifanZhou*1 DenisLiu1 YantianZha2 HeniBenAmor1 SubbaraoKambhampati1
Abstract 1.Introduction
Large-scale generative models are shown to be Large-scale pre-trained generative models, such as large
useful for sampling meaningful candidate solu- language models (LLMs), exhibit impressive capabilities
tions,yettheyoftenoverlooktaskconstraintsand acrossabroadspectrumoftasks,includinglanguagemodel-
userpreferences. Theirfullpowerisbetterhar- ingandcodegeneration. However,despitetheirsuccessin
nessedwhenthemodelsarecoupledwithexter- numerousapplications,ithasbeenobservedthattheyoften
nal verifiers and the final solutions are derived failtoproducedesiredoutputsinasingleattempt. Forex-
iterativelyorprogressivelyaccordingtothever- ample,indailyAI-assistedwritingscenarios,usersusually
ification feedback. In the context of embodied samplemultipleoutputsfromanLLMandprovidecorrec-
AI, verification often solely involves assessing tivefeedbackinamulti-turnmanneruntilthedesiredwriting
whethergoalconditionsspecifiedintheinstruc- isobtained. Similarly, inAI-basedmathematicsproblem
tionshavebeenmet.Nonetheless,fortheseagents solving,thestate-of-the-artperformance(Romera-Paredes
to be seamlessly integrated into daily life, it is et al., 2023; Trinh et al., 2024) is achieved by repeatedly
crucial to account for a broader range of con- (re-)promptingLLMswithverificationfeedbackfrompro-
straints and preferences beyond bare task suc- gramexecutorsandscriptedevaluator. Likewise,inseveral
cess(e.g.,arobotshouldgraspbreadwithcareto LLMs-driventaskplanningframeworks(Valmeekametal.,
avoidsignificantdeformations). However,given 2023b;Guanetal.,2023), candidateplanssampledfrom
the unbounded scope of robot tasks, it is infea- LLMsarevalidatedbyexternalmodel-basedverifiers,e.g.,
sibletoconstructscriptedverifiersakintothose toseeifthereisanyunfulfilledpreconditionateachstep.
usedforexplicit-knowledgetaskslikethegame Overall,whilelarge-scalegenerativemodelsarecapableof
ofGoandtheoremproving. Thisbegstheques- generatingmeaningfulcandidatesolutions,theytendtocon-
tion: whennosoundverifierisavailable,canwe sistentlyoverlooktaskconstraintsanduserpreferences. To
use large vision and language models (VLMs), fullyharnesstheirpower,ithasbecomecommonpracticeto
whichareapproximatelyomniscient,asscalable couplethemwithexternalcriticsorverifiers(e.g.,humans
Behavior Critics to catch undesirable robot be- orprograms)suchthatthemodelscaniterativelyrefinetheir
haviorsinvideos? Toanswerthis, wefirstcon- outputsaccordingtothecritiquesorverificationfeedback.
struct a benchmark that contains diverse cases
Motivated by the success of large pre-trained models in
of goal-reaching yet undesirable robot policies.
other domains, robotics and embodied AI communities
Then,wecomprehensivelyevaluateVLMcritics
haveinitiatedeffortsinbuildinggeneral-purposelanguage-
togainadeeperunderstandingoftheirstrengths
conditionedpolicymodels(Padalkaretal.,2023;Teametal.,
andfailuremodes. Basedontheevaluation,we
2023;Nairetal.,2022). Althoughtheversatilityandreli-
provideguidelinesonhowtoeffectivelyutilize
abilityofstate-of-the-artpolicymodelsarenotyetonpar
VLMcritiquesandshowcaseapracticalwayto
withmodelslikeLLMs,theircapabilitiescontinuetogrow
integratethefeedbackintoaniterativeprocessof
as researchers enrich robot-specific datasets (Zhao et al.,
policyrefinement. Thedatasetandcodebaseare
2023a)anddevisemethodologiesthatcanleverageinternet-
released at: https://guansuns.github.
scaleembodiment-agnosticdata(Yangetal.,2023;Duetal.,
io/pages/vlm-critic.
2023b). Nevertheless,evenifdatasparsityisaddressed,we
envisionthatlargepolicymodelswouldstillfacethesame
*Equal contribution 1School of Computing & AI, Arizona
limitations as we observe in LLMs development — the
StateUniversity2DepartmentofComputerScience, University
modelsmaynotproducedesiredpoliciesinoneshot. Sev-
of Maryland, College Park. Correspondence to: Lin Guan
<lguan9@asu.edu>. eralfactorscanresultinsuchinaccuracies: (a)theintrinsic
stochasticityoflargeparametricmodels;(b)theinevitable
Copyright2024bytheauthor(s).
1
4202
beF
6
]IA.sc[
1v01240.2042:viXraInvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
Figure1. ExamplesofGPT-4Vcriticaccuratelycatchingundesirablebehaviors.FailurecasescanbefoundinFig.3.
noise in large-scale datasets, especially in collective data theundesirabilitypresentedinvideosofrobotbehaviors?
sourcesliketheinternet;and(c)theadoptionoffullygoal- Furthermore,givenVLMsmayexhibitinaccuraciesinany
driven policy optimization mechanisms – in other words, usecase,understandingthepatternoftheiroutputsbecomes
thecurrentliteraturetypicallyonlyverifiesthesatisfaction crucialpriortotheirintegrationintoanypolicygeneration
ofgoalconditionsspecifiedinsymbolicornatural-language system.
instructions(e.g.,byensuringhighCLIP-stylesimilaritybe-
Toanswerthesequestions,weconductthefirstcomprehen-
tweenimageobservationsandthelanguageinstructions(Ma
sivestudyonleveragingVLMs(i.e.,GPT-4VandGemini
etal.,2022;Cuietal.,2022;Nairetal.,2022;Rocamonde
Prointhiswork)asbehaviorcriticstofreelycommenton
etal.,2023)). However,sincelanguageinstructions(orany
undesirablebehaviorspresentedinvideos. Wefirstcollect
abstractedrepresentationoftheworld)arealwaysanincom-
real-worldvideosthatencompassdiverserobotbehaviors
pletespecificationofgoalconditionsanduserpreferences
whicharegoal-reachingbutundesirable(Fig. 1). Thenwe
(Guanetal.,2022a),afullygoal-drivenlearningmechanism
use the videos as a benchmark to investigate the feasibil-
would encourage “shortcuts” and overlook undesirability
ityofVLMcritics,withanemphasisontestinghowmany
in the agent’s behaviors (Fig. 1). For example, when in-
of the undesirable behaviors are covered by the critiques
structedto“handapairofscissorstoaperson,”therobot
(i.e., recall rate) and how many of the critiques are valid
holdsthepointyendofthescissorstowardstheperson. In
(i.e.,precisionrate). Inadditiontoscalarmetrics,wealso
anotherexample,therobotforcefullyopensacabinetdoor,
manuallyexamineeachcritiqueanddevelopataxonomyto
potentiallycausingdamagetothedoorhinge, evenifthe
characterizethefailuremodes. Ourevaluationindicatesthat
damageisnotimmediatelynoticeable.
GPT-4V can identify a significant portion of undesirable
Therefore, inthecontextofembodiedAI,toformaloop robot behaviors (with a recall rate of 69%). However, it
ofpolicy(re-)generationandverification(Fig. 2),itisnec- alsogeneratescritiquesthatcontainaconsiderableamount
essarytoaccountforbothgoalreachabilityandabroader ofhallucinatedinformation(resultinginaprecisionrateof
range of common constraints and preferences. However, 62%),mainlyduetolimitedvisual-groundingcapability.
unlikeexplicit-knowledgetasks(e.g.,thegameofGoand
Basedonthesefindings,wediscusshowtoeffectivelyuti-
theorem proving) wherein it is feasible to list out all the
lizeVLMcritiques. Wedemonstratethat,inanidealcase,
rulesandconstraints,giventheunboundedscopeofrobot
byprovidingGPT-4Vwithgroundingfeedbackthatverifies
tasks,itisintractabletoexhaustivelyarticulatetheconsid-
theoccurrencesofeventsmentionedinthecritiques,itcan
erationsandundesirability. Thisraisesaquestion: inthe
“refine”itsoutputsandachieveaprecisionrateofover98%
absenceofasoundverifier,canweuselargevisionandlan-
(withaminorimpactontherecall). Lastly,whilethiswork
guagemodels(VLMs)likeGPT-4V(Achiametal.,2023),
doesnottakeastrongstanceonhowthecritiquesshould
whichareconsideredapproximatelyomniscient,asscalable
be integrated into closed-loop policy-generation systems,
Behavior Critics to catch, at least to a significant extent,
2InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
and models (Guan et al., 2023; Wong et al., 2023), con-
structingrewardfunctions(Xieetal.,2023;Maetal.,2023;
Yuetal.,2023),automatingtaskdesign(Ahnetal.,2024),
andexplainingexecutionfailures(Liuetal.,2023). More
comprehensivesurveyscanbefoundin(Firoozietal.,2023;
Zengetal.,2023).Amongalltheusecases,rewardconstruc-
tionandfailureexplanationaremostrelevanttothiswork.
LLMs-basedrewardconstructionalsoelaboratesondesir-
abilityandundesirabilityinarewardfunction. However,its
focusisonhowtocreateaneasily-optimizabledensereward
functionbyadjustingtherelativeweightsofapre-defined
setoffeaturesorbymodifyingthelogicofaPythonreward
program.Differently,behaviorcriticsemphasizeknowledge
acquisitionbyproposingnewfactorsthatareoverlookedin
thecurrentbehavior. Thesefactorsmaybeusedtoexpand
Figure2.Positioningofthiswork. Thisfigurealsoillustratesa the feature set and be incorporated into a refined reward
loopofpolicy(re-)generationandverification. function. InfailureexplanationframeworkslikeREFLECT
(Liuetal.,2023)andreflexion(Shinnetal.,2023),thefocus
isonidentifyingthecausesoferrorsthatresultintermina-
tionofplanexecutionandpreventtheagentfromreaching
goalstates. Instead,ourworkarguesthattherecanstillbe
wedopresentacandidateframeworkusingarealrobotin
undesirabilityingoal-reachingbehaviorsespeciallygiven
fivehouseholdscenarios,whereinaCode-as-Policiesagent
thathumaninstructionsalwaysunder-specifycommoncon-
(Liangetal.,2023)iterativelyrefinesthepolicyaccording
straintsandhumanpreferences.Thepositioningofourwork
toVLMcritiquesontherollouts.
isillustratedinFig. 2.
2.RelatedWork Roboticfoundationmodels. Roboticfoundationmodels,
suchastheRTseries(Zitkovichetal.,2023;Padalkaretal.,
Coupling generative models with verifiers. Generative 2023)andOcto(Teametal.,2023),aretypicallywaypoint-
models,likeLLMs,aregeneraliststhatcanserveaspow- basedpolicymodelsthatconnectsemantictaskspecifica-
erfulheuristicsfornumerousproblems. However,toeffec- tionstolow-levelmotion. Critics,includingourbehavior
tivelyexplorethesolutionspaceandselectaworkingsolu- critics,andacontinuously-improvedpolicymodelcancom-
tionwithguaranteedcorrectness,itisnecessarytocouple plementeachotherinthesensethatadesiredbehaviorcan
thesemodelswithexternalsoundverifiers. Suchgenerator- bemoreefficientlyderivedfromthepolicywithinaguided
verifierparadigmsarewidelyadopted(Valmeekametal., generationframework.
2023b;Guanetal.,2023;Romera-Paredesetal.,2023;Trinh
etal.,2024).
3.ProblemSetting
TheuseofAIcritiquesandfeedback. Itmaybeinfeasible
Weconsiderasettingwhereanagent(e.g.,amotionplanner)
toconstructformalverifiersfortasksthataretacitorhave
tries to solve a task specified with a language instruction
awidescope. Inthesesituations,peoplehaveexploredthe
i. The task is represented as a finite-horizon discounted
option of using AI models to criticize the outputs, either
MDPM=⟨S,A,R,T,S ,γ⟩,whereS isthestatespace
generatedbythemselvesorbyothermodels(Baietal.,2022; 0
comprisingfeaturesthatencapsulatenecessaryinformation
Leeetal.,2023;Wangetal.,2023;Yuanetal.,2024;Ahn
abouttheenvironment,theagent,and,optionally,thehis-
et al., 2024). Different from existing works that employ
tory; A is the set of actions; T : S ×A×S → [0,1] is
AI models to assess how well each output conforms to a
theworlddynamicmodel;S isthesetofpossibleinitial
predefined “constitution,” this study focuses on utilizing 0
states; R is the reward function; and γ is the discounted
AImodelstoidentifyoverlookedaspectsofundesirability
factor. Instructionicorrespondstoasymbolicspecification
(e.g.,tocompletetheconstitution). Overall,theuseofAI
ofgoalconditions,whichcharacterizesasetofabsorbing
feedbackoffersimprovedscalabilitybutalsocomesatthe
goalstatesS ⊂ S. Withoutlossofgenerality, R canbe
expenseoflosingcorrectnessguarantees. g
constructedas1(s∈S )fors∈S,whichencouragesthe
g
LLMsandVLMsfordecisionmaking. LLMsandVLMs agenttoreachagoalstatewiththeleasttimestepsifγ <1.
havebeenusedinvariousaspectsofdecision-making,such LetS∗ ⊂S denotethesetofstatesthatsatisfyallthecon-
g g
as generating heuristic plans (Ahn et al., 2022; Du et al., ditionsiniaswellascommonhumanpreferencesP. Due
2023a; Zhao et al., 2023b), acquiring domain knowledge
3InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
tothepotentialincompletenessofsymbolicrepresentations, with justification. Pairwise preferences can complement
instructionimayallowforundesirablegoal-satisfyingstates verbalcritiqueswhentheundesirabilityinvolvessubtleand
S− =S \S∗. Consideringtheunboundedortacitnature tacitcomponents. Forexample,whenselectingatrajectory
g g g
ofP,ourprimaryobjectiveistoinvestigatehowwellVLMs thatopensadoorless“forcefully”fromasetofvirtualroll-
canapproximateP anddetermineifanystatesisinS−. outs,itwouldbeeasiertousepreferencelabels. However,
g
wedonotethatverbalcritiquesstillplayacentralrolein
this study as they provide the most informative signal to
4.Methodology
guideanagenttowarddesirablebehaviors. Formoredetails
Webeginbydescribingthepromptusedtoobtainbehavior onpreferenceelicitation,seeAppx. A.1.
critiques from VLMs. Next, as the core contribution of
this paper, we will explain how we collect and construct 4.2.BenchmarkingVLMsbehaviorcritics
ourbenchmarkwhichconsistsofvideoclipsdemonstrating
Data collection. In our benchmark, a video of subopti-
suboptimalyetgoal-reachingpoliciesindiversehousehold
malpolicymayfeatureoneormorenegativeevents. These
tasks. Accordingly,weintroducethemetricsandtaxonomy
negativeeventsarebasedonundesirabilityobservedinpub-
forcharacterizingthestrengthsandfailuremodesofVLM
licrobotvideos(e.g.,onhomepagesofrelevantacademic
critics. Finally,weconcludethissectionwithadiscussion
projects)anddemonstrationdatasets(Padalkaretal.,2023)
onpracticalwaystoutilizeandintegrateVLMcritiques.
aswellasourownexperienceoftestingrobotpolicies(with
bothpublicandproprietarymodels). Alltasksetupstake
4.1.ObtainingbehaviorcritiquesfromVLMs
placeinreal-worldhouseholdenvironments.
Fig. 9inAppendixshowstheprompttemplateforinstruct-
To facilitate and ensure the safety of data collection, the
ingVLMstogeneratecritiquesoverundesirablebehaviors.
majorityofvideoswererecordedbyreproducingdifferent
Ithas4keycomponents: (a)abriefdescriptionofthetask
policieswithahuman-controlledgrabbertool(Fig. 1). The
of being a behavior critic; (b) two textual examples that
grabber tool closely resembles a robot gripper in appear-
demonstrate the expected output format for cases where
ance. Furthermore,wehavetakencaretocropthevideosto
thereareandarenotundesirablebehaviorsinavideo;(c)
eliminateanyvisiblecuesindicatingmanualoperationof
framesfromthevideotobecriticized;and(d)ashortnote
the“gripper”. Thissetuphasalsobeenemployedinseveral
explainingthattheselectedframesmaintainthesamerela-
visualimitationwork(Youngetal.,2021;Parietal.,2021).
tiveorderingasintheoriginalvideoandthattheremaybe
zeroormultipleundesirablebehaviorspresentedinavideo. Benchmarkstructure. Wecollected175videosintotal,
forming114testcases. Eachtestcasehasallthefollowing
Each⟨frame ⟩tagintheprompttemplatecorrespondstoan
i components: (a) a negative video that features the unde-
image. Inourbenchmark,eachvideoshowcasesapolicy
sirablebehaviors;(b)anothernegativevideothatexhibits
forashort-horizonmanipulationtask,andwefindthat30
the same undesirable behaviors; (c) a positive video that
framesadequatelycapturetheessenceofeachpolicy.Hence,
demonstratesasatisfactorypolicy–theinclusionofposi-
each input contains a maximum of 30 frames, and each
tiveexamplesaimstoassessthefrequencyofVLMcritics
frame undergoes preprocessing to ensure that its longest
producingfalsealarms;(d)alanguageinstructionthatspec-
side does not exceed 512 pixels. Also, adjustments may
ifiesthetask–policyshowninapositivevideoissupposed
need to be made based on the VLM being used. For ex-
to satisfy the goal conditions given in the corresponding
ample,GeminiProcurrentlyonlysupports16framesper
instruction;(e)alistofhuman-annotatedundesirablebehav-
conversation.
iorsthatarepresentedintherespectivenegativevideos;and
Moreover, the examples used to illustrate the output for- (f)anarrativedescriptionofthenegativevideo,coveringkey
mat are fixed across all test cases. We acknowledge the actionsthatperformthetaskasanticipatedandthosethat
possibilityofmarginallyimprovingoverallperformanceby resultinundesirableoutcomes(seeexamplesinApp. A.4).
retrievingmorecontextuallyrelevantexamplesfromapool Theinclusionofnarrationistoexaminewhetherthetext
ofpre-recordedundesirablebehaviors. However,itisim- backbonesofexistingmulti-modalmodelshavesufficient
portanttorememberthatthemainreasonforusingVLMs knowledgetodifferentiatebetweenundesirableandaligned
asbehaviorcriticsisthedifficultyinelaboratingonthepo- agentbehaviors.
tentialundesirability. Therefore,itiscrucialtoassessthe
Evaluationmetrics. Recallandprecisionarefundamen-
performanceofbehaviorcriticsunderan“unforeseeable”
talmetricsinassessingbehaviorcritics. Recallmeasures
setting.
the proportion of the most critical negative events, as an-
In addition to having a behavior critic comment on indi- notatedbyhumans,thattheVLMcriticssuccessfullyiden-
vidualvideosseparately,wealsoexperimentwithhavinga tify. Precision, on the other hand, quantifies the fraction
criticcomparepairsoftrajectoriesandprovidepreferences ofthecritiquesthataccuratelyreflectthenegativeevents
4InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
stances,anoperationalizableexplanationisinherentlypart
ofthedescription,forexample,“the robot gripper made
contact with the plate while trying to grasp the carrot.”
(b)Visualgroundingerrors: visualgroundingerrorsare
instanceswhereacriticreferstoeventsthatarenotpresented
in the video. This is also known as hallucination in the
contextofperceptionandvideounderstanding. Notethat
perceptionerrorscanoccurthroughouttheentirecritique
–critiquingbehaviornotdepictedinthevideo–orwithin
fragmentsofacritique,forinstance,fabricatingabehavior
toexplainthecauseofanegativeevent.
(c)Causalunderstandingerrors:thistypeoferrorpertains
tocaseswhereacriticmisconstruesthecauseofnegative
events(ifanexplanationisprovidedaspartofthecritique).
Figure3.FailurecasesofGPT-4Vcritic. Moreexamplescanbe
Sucherrorsmayarisewhenthecriticerroneouslylinkstwo
foundinFig.6inAppendix.
independent events or when the critic attributes a nonex-
istent event as the cause. The latter case is also labelled
as a visual grounding error – different error types in our
taxonomyarenotmutuallyexclusive.
withinthevideos. Notethatthecalculationofprecisionrate (d)Incompleteunderstandingoftheproblem:inthistype
involves manual assessment because, by definition, “pre- ofinaccuracy,thecriticcarelesslyjudgesaneventasnega-
cision”shouldpermitthecriticstosuggestvalidcritiques tivewhileitmaynotbeundesirableundercertainconditions.
evenwhentheydonotassociatewithanyhuman-annotated Thisisarelativelyrareerrortype. InthecaseofGPT-4V,
negativeevent. only two critiques fell into this category. One instance
criticizestherobotfor“releasingthescissorsabovetheta-
Furthermore, in order to gain deeper insights that scalar
ble when handing them to a person, potentially causing
metricsalonecannotprovide,wealsoconductedathorough
them to fall and injure the person or damage the table”.
examination and annotation of individual critiques based
However,inthevideo,thepersoncomfortablyandfirmly
on the properties they exhibit. We adhere to a taxonomy
received the scissors from the robot’s gripper. Therefore,
that allows us to delineate the success cases and failure
releasing the scissors above the table did not necessarily
modes. Whileourtaxonomymaynotbeexhaustivelycom-
exhibitriskybehavior,consideringtheperson’sreaction.
prehensive,wefinditsufficientlycapturespatternsofVLM
critiquesthatweobservedinourevaluations. Thedimen- Inadditiontotheaforementionederrortypes, wealsoin-
sionswetakeintoaccountinclude: vestigated whether the behavior critic provides incorrect
“advice”thatleadstheagentawayfromanoptimalpolicy.
(a)Critiqueoperationalizability: acritiqueisexpectedto
An example scenario could be when the critique hinders
identifynegativeeventsinasensethattheeventitself,not
arobotfromreachinganobject,contradictingthetaskre-
supplementary information such as underlying causes, is
quirementofdeliveringtheobjecttoaperson. Fortunately,
preciselydescribed. However,anintriguingquestionarises
GPT-4V never exhibit such errors in our evaluation, and
astowhetheracritiquecanextendbeyondmeredescription
asaresult,thisparticularcategoryisdeliberatelyexcluded
toprovideanoperationalizableexplanationforthecause.
fromourtaxonomyandfutureanalysistoavoidpotential
Here,anoperationalizableexplanationreferstoanexplana-
confusion.
tionthatcanbedirectlyimplementedbyanagenttorectify
thesuboptimalpolicy,withoutrequiringextrahuman-like
4.3.Utilizingbehaviorcritiques
reasoningaboutthecause. Anexampleofanunoperational-
izablecritiqueis“therobotliftedthebagwithoutsecuring
Theprimaryobjectiveofourbenchmarkistoshedlighton
the contents, leading to the carrots falling onto the ta-
theoutputpatternsofVLMcriticsandprovideguidelines
ble,”whichdoesnotpinpointtherootcause,i.e.,therobot’s
onhowtoutilizethem.AsshowninSec. 5.1,critiquesfrom
failuretosecurelyholdorsealthebag’sopening. Another
GPT-4Vexhibitthefollowingcharacteristics: (a)GPT-4V
example(Fig. 3)is“the robot caused pasta sauce to drip
canidentifyasignificantportionofundesirableevents,but
onto the counter while transferring the spoon from the
italsoproducesinaccuratecritiques,asreflectedinitsrel-
jar to the bowl,”whichdoesnotidentifythatthecoreissue
ativelylowprecisionrate;and(b)visualgroundingerrors
liesintherobotnotwaitingfortheremnantstodripbackto
arecommonandcontributetothemajorityofinaccuracies,
thejarbeforemovingthespoon. Notethatinsomecircum-
5InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
meaningthatGPT-4V’scritiquesoftencontainhallucinated area of research. Some works have been able to utilize
information. Therefore, from the perspective of critique- verbalfeedbackinspecificproblemsetups(Sharmaetal.,
takingagents, whileattemptingtoaddressthese“halluci- 2022;Cuietal.,2023;Buckeretal.,2023;Zhaetal.,2023;
nated”critiquesmaynotnecessarilydeviateanagentfrom Miyaokaetal.,2023;Yowetal.,2024). Forinstance,RBAs
optimalbehaviors,itcouldbeawasteoftime. Additionally, (Guanetal.,2022b)andAlignDiff(Dongetal.,2023)learn
sinceVLMcriticsmaycriticizeoptimalbehaviorsbasedon conditional reward or policy models that can implement
hallucinatedevents,asysteminvolvingsuchcriticswillfail relativeconceptslike“takebiggerstride.”However,there
toprovideaclearnotionofconvergence,i.e.,determining is currently no robot policy model that demonstrates the
whetheradesirablebehaviorisachieved. Theremainderof samelevelofgeneralityandflexibilityasmodelsinother
thissectionwilldiscusshowtoaddressthesechallenges. domains,suchasLLMs.
Augmenting VLM critics with grounding feedback. Inthiswork,weremainneutralonthechoiceofunderlying
Whenitcomestotaskslikeidentifyingpotentialnegative modelstodrivetheagent, asourmainfocusisonverbal
events and their causes, currently there may not be more critiques. Nevertheless, we present a viable closed-loop
performantmodelsthanstate-of-the-artVLMs. However, systemwhereinCode-as-Policies(Liangetal.,2023)isem-
this does not hold true for visual grounding. For one, as ployedtocontroltherobot. InCode-as-Policies(CaP),an
notedintheOpenAIofficialdocumentandseveralempir- LLM“programs”therobot’smotion byinvokingandad-
ical studies (Chen et al., 2024), GPT-4V may not match justinginputparametersofapredefinedlibraryofcontrol
the performance of specialized vision models in specific primitiveAPIs. Withthissetup,aclosed-loopsystemcan
typesofvisualgroundingtasks(Zhangetal.,2022;Kirillov beconstructedbyrepeatingthesesteps: (a)CaPsynthesizes
etal.,2023). Moreover,accuratelygroundingcertainevents acontrolprogramaspolicy;(b)therobotexecutesthepro-
mayrequireadditionalinformationfrommodalitiesother gramintheenvironmentor,ideally,afaithfulsimulator;(c)
thanimages. Forexample,depthmapscanprovidemoredi- avideorecordingoftherolloutiscollectedandsenttothe
rectinformationaboutspatialrelationshipsbetweenobjects, VLMcriticforreview;and(d)theprocessterminatesifno
suchascollisions,distances,orrelativepositions. Similarly, undesirablebehaviorisdetected;otherwise,theCaPagent
audioandrobotsensoryreadingscanbemoreinformative isinformedofthenegativeevents.
about the contact that the robot’s gripper exerts on other
Apartfromdirectlytakingverbalcritiques,onemayalsodis-
objects.
cardthetextualdescriptionoftheundesirablebehaviorsand
Basedontheaboveanalysis,weproposeapracticalpipeline convertthecritiquesintoscalartrainingsignalslikebinary
toleverageVLMcriticsbyincorporatinggroundingfeed- preferencelabels,e.g.,byassumingatrajectorywithfewer
backfromasuiteofspecializedgroundingmodules. Firstly, undesirableeventsispreferredoverothers. Thepreference
events in the critiques are extracted and sent to the most labelscanthenbeusedwithinframeworkslikepreference-
suitablespecializedmodel(s)tochecktheirexistenceinthe basedRL(Christianoetal.,2017;Zhangetal.,2019;Hejna
robot’s behavior. The process of model selection can be etal.,2023). Here,wewillnotdelveintothisdirectionas
automatedwithanLLM(Zengetal.,2022;Wuetal.,2023). theinabilitytoacceptexplicitverbalguidanceisafunda-
Next,thegroundingfeedback,whichtypicallyprovidesone mentalcauseofhighsamplecomplexityinpreference-based
bitofextrainformation(i.e.,whetheraneventispresent), policylearning,especiallyinhard-explorationproblemslike
is returned to the VLM critic to continue the critique- continuouscontrol(Kambhampati,2021).
extraction dialogue. As we will see in Sec. 5.3, when
thegroundingfeedbackisperfect,aprecisionrateofover
5.EmpiricalEvaluation
98%canbeachieved. Inessence,thisaugmentedpipeline
sharesthesameprinciplesastool-augmentedLLMs(Parisi This section will elaborate on the evaluation results, fo-
etal.,2022;Schicketal.,2023)andretrieval-augmented cusingparticularlyonresultsobtainedwithGPT-4Vasof
generation (Guu et al., 2020), wherein LLMs and VLMs January2024. Alongside,anevaluationofGeminihasbeen
serveasahubtomanipulateinformationfrommoreaccurate conductedbutislimitedbecauseGeminiProis currently
sources(Zengetal.,2022;Chenetal.,2024). Inthiswork, restrictedtoamaximumof16imagesperconversation. For
VLMcriticsinitiallysuggestpotentialnegativeeventsby clarity, Gemini’s results are presented in Appx. A.2. It
retrievingapproximatelyfromitsvastinternalknowledge is important to note that the objective of this study is to
and then refine the outputs with more precise grounding investigate the feasibility of using state-of-the-art VLMs
tools(whicharenotnecessarilygenerativemodels). as behavior critics, rather than establishing comparisons
betweendifferentmodels.
Integratingcritiquesintoaclosed-loopsystem. Thede-
velopmentofgeneral-purposepolicymodelsthatcanaccept
verbalinstructionsandcorrectivefeedbackisanongoing
6InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
Acc. 5.2.Insightsintothedistributionoferrortypes
Metrics Recall Precision
Positive Negative
Sec. 4.2outlinesthetaxonomyoferrorsthatisusedhereto
GPT-4V 6.14% 92.98% 69.42% 62.21%
analyzetheoutputpatternsofGPT-4Vcritic(onnegative
GPT-4V-Augmented 98.24% 78.07% 62.80% 98.02% samples). Fig. 4 displays the number of errors per cate-
gory. Notably,within262critiques,wefind116instances
Table1.EvaluationresultsofGPT-4Vcritic.GPT-4V-Augmented
ofvisual-groundingerrors,highlightingamajorlimitation
refers to the experiment of augmenting GPT-4V with perfect
ofGPT-4Vinunderstandingvideos. Visualgroundinger-
groundingfeedback(Sec.5.3).
rorsalsooccursignificantlymorefrequentlycomparedto
other error types. However, we would interpret this as a
positive finding since grounding-related errors are more
manageabletoaddressthanothers(seediscussionsinSec.
4.3). Additionally,wefindthatonly69.05%ofthecritiques
capturing undesirability can accurately identify both the
negativeeventsandtheircauses(byprovidingoperational-
izableexplanations). Hence,atpresent,itismorereliableto
considerthecritiqueasameansofdetectingundesirability
Figure4.Distributionsoferrortypes.GPT-4V-Augmentedrefers ratherthanprovidingactionableadvice.
totheexperimentofaugmentingGPT-4Vwithperfectgrounding
feedback(Sec.5.3). 5.3.AugmentingVLMcriticswithgroundingfeedback
MostanalysessofarsuggesttheneedtoaugmentVLMs
with improved grounding capabilities (more discussions
in Sec. 4.3). As a preliminary study, we assess whether
5.1.Recallandprecision GPT-4Vcriticcanrefineitsoutputwhenperfectgrounding
feedbackisprovided(byhumanparticipantsforexperimen-
Table1presentstherecallandprecisionratesofGPT-4V
tal purposes). To achieve this, we continue the critique-
critic.TheseresultsarebasedonthedirectoutputfromGPT-
elicitationconversationwithfeedbackmessagesmentioning
4Vpriortoanypost-filtering. Intermsofrecall,GPT-4V
that“A perfect perception model has been used to verify
achievesarateof69.42%. Wenotethatgiventheextensive
the existence of events in the critiques.”Eachgrounding
poolofpotentialnegativeeventswithinhouseholdtasks,a
feedbackisprovidedintheformatof“The following event
recallrateof69.42%alreadysignifiesadecentperformance
is not detected: ⟨event description⟩”.
andahighvalueinsavinghumaneffortstoeliminateunde-
sirability. Nonetheless,thisnumberalsosuggestsroomfor Results are shown in Table 1 and Fig. 4. With perfect
improvement,suchasfine-tuningthemodelwithacollective grounding feedback, the precision rate increases signifi-
setofundesirableevents. Comparedtorecall,theprecision cantlytoover98%, althoughthereisaslightdecreasein
rateof62.21%highlightsmorepracticalchallenges. Asig- recall. Uponmanualexamination, wefindthatthisisbe-
nificantportionofthecritiquesdonotaccuratelycoverany causeGPT-4Vtendstosimplydeletecritiquesthatcontain
undesirableeventsinthevideos,emphasizingtheneedfor “hallucinated”informationinsteadofrefiningthem(e.g.,by
post-filtering(Sec. 4.3andSec. 5.3). generatingabetterguessofcauses). Moreover,grounding
feedbackalsosubstantiallyreducestheoccurrenceoffalse
Notethattherecall-precisionevaluationisconductedsolely
alarmsonpositivesamples(evidencedbytheaccuracyof
withvideoscontainingnegativeevents(hereafterreferredto
98.24%indeterminingthepresenceofundesirability.).
as“negativesamples”). Itisalsoimportanttoexaminehow
GPT-4Vcommentsonvideosshowcasingpreferablebehav-
5.4.Additionalevaluations
iors(hereafterreferredtoas“positivesamples”). Table1
showsGPT-4V’saccuracyinidentifyingthepresenceofun- Thissubsectionfocusesontwoadditionalexperiments. The
desirableeventsamongbothnegativeandpositivesamples. firstexperimentaimstoassessthefeasibilityofcomplement-
TheresultsclearlyillustratethatGPT-4Valwaysprovides ing verbal critiqueswith preference labels(see Sec. 4.1).
criticism,irrespectiveofthepresenceorabsenceofundesir- Ourresultsshowthat(a)whencontrastinganegativesample
ablebehaviors. Uponreviewingthe“critiques”onpositive withapositivesample,in95.61%oftime,GPT-4Vmanages
samples,wefindthatalmostallofthemareinvalidasthey topreferthepositiveone;(b)whencomparingpairsofneg-
criticize“hallucinated”negativeevents. Thisresultfurther ativesamplesorpairsofpositivesamples,GPT-4Valways
confirmsthatGPT-4Vsuffersfromlimitedvisualgrounding establishesinvalidrankingswithinthepairsby“hallucinat-
capability,anditscritiquesshouldnotbedirectlyusedasan ing”thatonesampleisperfectandoneisflawed,asseen
indicationof“optimality”.
7InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
Bread Scissor Snack sionallygeneratespoliciesthatpointthesharpendofthe
Grasping Handover Picking
scissorsatahuman. Afterreceivingonecritique,theagent
managestorotatetheend-effectorby180degrees. Inlifting
Initial
anopenedbag,theagentoccasionallymakesthemistakeof
Policy
grippingthebottomofthebag,leadingtospills. Afterafew
attempts(3inavg.) tograspdifferentedgesofthebag,the
agentmanagestoholdtheopening. Inbreadgrasping,the
Updated
agentinitiallyappliesexcessiveforce,resultinginsignifi-
Policy
cantdeformationofthebread.After∼5iterations,theagent
managestoreducetheforcetoaminimalvalue. Likewise,
inknifeplacing,theagentstartswithapolicythatcauses
Figure5.Illustrationsoftheinitialpoliciesandtheupdatedpoli-
forcefulcontactbetweentheknifetipandthecuttingboard.
ciesinresponsetoverbalcritiques.Amoredetailedvisualization
After2iterations,theagentsuccessfullylowersitsgripper
canbefoundinFig.7inAppendix.
and rotates the gripper by 90 degrees, ensuring a gentle
placementoftheknifeandpreventingcontactbetweenthe
tipandthecuttingboard.
initsjustifications. ThepositiveaspectisthatGPT-4Vcan TherearealsoinstanceswhereCaPfailstoreachaprefer-
accurately select near-optimal behaviors over suboptimal ablebehavior. Inascenariowheretherobotistaskedwith
oneseventhoughittendstoestablishunnecessaryorderings takingaspoonoutofajar,theCaPagentneglectsthestep
inothercases.Moredetailsofthisexperimentareexplained of waiting for the sauce residue to drip back into the jar
inAppx. A.1. beforerepositioningthespoon. Uponreceivinganunopera-
tionalizablecritique“therobotcausedpastasaucetodrip
In the second experiment, instead of feeding videos into
onto the counter while transferring the spoon from the
GPT-4V,weprovideanarrationofthevideo(seeSec. 4.2
jartothebowl”,theCaPagentcontinuestomovethespoon
and Appx. A.2) and ask GPT-4 to identify which steps
immediatelyafterextractingit,albeitviaanalternativepath.
containundesirablebehaviors. Thegoalofthisstudyisto
Thisrevealstheoverallperformancemayalsobelimitedby
verifythat(a)thetextbackboneofGPT-4Vhasgoodunder-
theplanner’scapabilitytoinfercausesofnegativeevents.
standingofwhatconstitutesundesirableandwell-aligned
behaviors, and(b)theprimaryobstaclelieswithinvisual
6.Conclusion
grounding. Out of 114 test cases, GPT-4 accurately lo-
cates and explains the undesirable behavior in 110 cases.
Weconductedathoroughinvestigationintothefeasibilityof
We note that although this may not be a rigorous evalu-
usingVLMsasbehaviorcriticstoidentifycommonundesir-
ation as human-written narrations already provide strong
abilityinvideosofrobotbehaviors. WeevaluatedGPT-4V
informationfiltering,itdoesshedsomelightonthecurrent
onourbenchmarkthatcoversdiversecasesofundesirability.
bottleneckofVLMs.
Ourfindingsnotonlydemonstratethefeasibilitybutalso
comprehensivelycharacterizethestrengthsandlimitations
5.5.Demonstratingaclosed-loopsystemwithCaP ofGPT-4Vcritic,whichprovidepracticalguidelinesforef-
fectivelyutilizingthecritiques. Lastly,wealsodemonstrate
In this subsection, we showcase a candidate closed-loop
howtoalignthebehaviorsofaCode-as-Policiesagentwith
systemwithCode-as-Policiesastheplanner. Wewouldreit-
ourcritic.
eratethatwedonotrestricttheselectionofpolicymodels,
andthemainreasonforemployingCaPistheunderlying Going forward, it would be important to understand the
LLM’sstrongabilitytoadaptinresponsetoverbalcritiques, connectionsbetweenthisworkandotherstudiesthatshow
eventhoughCaPmaybelessexpressivecomparedtoother LLMscannotreliablyverifyplans(Valmeekametal.,2023a;
waypoint-based approaches (Padalkar et al., 2023; Team Stechlyetal.,2023).Ourhypothesisisthat,inourtestcases,
etal.,2023). thebehaviorcriticcanidentifyundesirabilitywithlocalin-
formation without the need to reason over long horizons
We use a UR5 robot and Robotiq-85 gripper with opera-
and capture interactions between subgoals. It would be
tionalspacecontrol. Accordingly,theCaPagentisprovided
beneficial to further expand the benchmark in this direc-
withAPIsthatcancontroltherobottotranslatefreelyon
tion. Additionally,itwouldbemeaningfultotestifwecan
xyz,rotateregardingaspecifiedaxis,andadjustthegripper
turnthecriticintoastronger“universal”human-preference
(seedetailsinAppx. A.3). Weconsiderthefollowingtasks
“enforcer”byfine-tuningaVLMwithundesirabilitydata
withintable-topsetups: (a)scissorhandover;(b)liftingan
collectedthroughcollaborativeeffortsfromthecommunity.
openedbag;(c)breadgrasping;(d)knifeplacing;and(e)
spoon picking. In scissor handover, the CaP agent occa-
8InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
References reinforcementlearningwithlargelanguagemodels.arXiv
preprintarXiv:2302.06692,2023a.
Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,
Aleman,F.L.,Almeida,D.,Altenschmidt,J.,Altman,S., Du,Y.,Yang,M.,Dai,B.,Dai,H.,Nachum,O.,Tenenbaum,
Anadkat,S.,etal. Gpt-4technicalreport. arXivpreprint J.B.,Schuurmans,D.,andAbbeel,P. Learninguniversal
arXiv:2303.08774,2023. policiesviatext-guidedvideogeneration. arXivpreprint
arXiv:2302.00111,2023b.
Ahn,M.,Brohan,A.,Brown,N.,Chebotar,Y.,Cortes,O.,
David,B.,Finn,C.,Fu,C.,Gopalakrishnan,K.,Hausman, Firoozi,R.,Tucker,J.,Tian,S.,Majumdar,A.,Sun,J.,Liu,
K.,etal. Doasican,notasisay: Groundinglanguage W., Zhu, Y., Song, S., Kapoor, A., Hausman, K., et al.
inroboticaffordances. arXivpreprintarXiv:2204.01691, Foundationmodelsinrobotics: Applications,challenges,
2022. andthefuture. arXivpreprintarXiv:2312.07843,2023.
Ahn,M.,Dwibedi,D.,Finn,C.,Arenas,M.G.,Gopalakr- Guan,L.,Sreedharan,S.,andKambhampati,S. Leveraging
ishnan,K.,Hausman,K.,Ichter,B.,Irpan,A.,Joshi,N., approximate symbolic models for reinforcement learn-
Julian,R.,etal. Autort: Embodiedfoundationmodelsfor ing via skill diversity. In International Conference on
largescaleorchestrationofroboticagents. arXivpreprint MachineLearning,pp.7949–7967.PMLR,2022a.
arXiv:2401.12963,2024.
Guan,L.,Valmeekam,K.,andKambhampati,S. Relative
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., behavioralattributes: Fillingthegapbetweensymbolic
Jones,A.,Chen,A.,Goldie,A.,Mirhoseini,A.,McKin- goalspecificationandrewardlearningfromhumanpref-
non, C., etal. Constitutionalai: Harmlessnessfromai erences. arXivpreprintarXiv:2210.15906,2022b.
feedback. arXivpreprintarXiv:2212.08073,2022.
Guan,L.,Valmeekam,K.,Sreedharan,S.,andKambham-
pati, S. Leveraging pre-trained large language models
Bucker,A.,Figueredo,L.,Haddadin,S.,Kapoor,A.,Ma,S.,
to construct and utilize world models for model-based
Vemprala,S.,andBonatti,R. Latte: Languagetrajectory
taskplanning. InThirty-seventhConferenceonNeural
transformer. In2023IEEEInternationalConferenceon
Information Processing Systems, 2023. URL https:
RoboticsandAutomation(ICRA),pp.7287–7294.IEEE,
//openreview.net/forum?id=zDbsSscmuj.
2023.
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.
Chen, B., Xu, Z., Kirmani, S., Ichter, B., Driess, D., Flo-
Retrieval augmented language model pre-training. In
rence,P.,Sadigh,D.,Guibas,L.,andXia,F. Spatialvlm:
Internationalconferenceonmachinelearning,pp.3929–
Endowingvision-languagemodelswithspatialreasoning
3938.PMLR,2020.
capabilities. arXivpreprintarXiv:2401.12168,2024.
Hejna, J., Rafailov, R., Sikchi, H., Finn, C., Niekum, S.,
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg,
Knox,W.B.,andSadigh,D. Contrastiveprefencelearn-
S.,andAmodei,D. Deepreinforcementlearningfrom
ing: Learning from human feedback without rl. arXiv
humanpreferences. Advancesinneuralinformationpro-
preprintarXiv:2310.13639,2023.
cessingsystems,30,2017.
Kambhampati,S. Polanyi’srevengeandai’snewromance
Cui,Y.,Niekum,S.,Gupta,A.,Kumar,V.,andRajeswaran,
withtacitknowledge. CommunicationsoftheACM,64
A. Canfoundationmodelsperformzero-shottaskspecifi-
(2):31–32,2021.
cationforrobotmanipulation? InLearningforDynamics
andControlConference,pp.893–905.PMLR,2022. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C.,
Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C.,
Cui,Y.,Karamcheti,S.,Palleti,R.,Shivakumar,N.,Liang, Lo, W.-Y., et al. Segment anything. arXiv preprint
P., and Sadigh, D. No, to the right: Online language arXiv:2304.02643,2023.
correctionsforroboticmanipulationviasharedautonomy.
InProceedingsofthe2023ACM/IEEEInternationalCon- Lee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T.,
ferenceonHuman-RobotInteraction,pp.93–101,2023. Bishop, C., Carbune, V., and Rastogi, A. Rlaif: Scal-
ingreinforcementlearningfromhumanfeedbackwithai
Dong,Z.,Yuan,Y.,Hao,J.,Ni,F.,Mu,Y.,Zheng,Y.,Hu, feedback. arXivpreprintarXiv:2309.00267,2023.
Y.,Lv,T.,Fan,C.,andHu,Z. Aligndiff:Aligningdiverse
Liang,J.,Huang,W.,Xia,F.,Xu,P.,Hausman,K.,Ichter,B.,
humanpreferencesviabehavior-customisablediffusion
Florence,P.,andZeng,A. Codeaspolicies: Language
model. arXivpreprintarXiv:2310.02054,2023.
model programs for embodied control. In 2023 IEEE
Du,Y.,Watkins,O.,Wang,Z.,Colas,C.,Darrell,T.,Abbeel, International Conference on Robotics and Automation
P., Gupta, A., and Andreas, J. Guiding pretraining in (ICRA),pp.9493–9500.IEEE,2023.
9InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
Liu, Z., Bahety, A., and Song, S. Reflect: Summarizing Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K.R.,
robotexperiencesforfailureexplanationandcorrection. and Yao, S. Reflexion: Language agents with verbal
arXivpreprintarXiv:2306.15724,2023. reinforcementlearning. InThirty-seventhConferenceon
NeuralInformationProcessingSystems,2023.
Ma,Y.J.,Sodhani,S.,Jayaraman,D.,Bastani,O.,Kumar,
V.,andZhang,A. Vip: Towardsuniversalvisualreward Stechly, K., Marquez, M., and Kambhampati, S. GPT-
andrepresentationviavalue-implicitpre-training. arXiv 4 doesn’t know it’s wrong: An analysis of iterative
preprintarXiv:2210.00030,2022. prompting for reasoning problems. In NeurIPS 2023
Foundation Models for Decision Making Workshop,
Ma,Y.J.,Liang,W.,Wang,G.,Huang,D.-A.,Bastani,O., 2023. URLhttps://openreview.net/forum?
Jayaraman, D., Zhu, Y., Fan, L., and Anandkumar, A. id=PMtZjDYB68.
Eureka: Human-levelrewarddesignviacodinglargelan-
guagemodels. arXivpreprintarXiv:2310.12931,2023. Team, O. M., Ghosh, D., Walke, H., Pertsch, K., Black,
K.,Mees,O.,Dasari,S.,Hejna,J.,Xu,C.,Luo,J.,etal.
Miyaoka, Y., Inoue, M., and Nii, T. Chatmpc: Natural Octo: Anopen-sourcegeneralistrobotpolicy,2023.
language based mpc personalization. arXiv preprint
arXiv:2309.05952,2023. Trinh,T.H.,Wu,Y.,Le,Q.V.,He,H.,andLuong,T. Solv-
ingolympiadgeometrywithouthumandemonstrations.
Nair,S.,Rajeswaran,A.,Kumar,V.,Finn,C.,andGupta, Nature,625(7995):476–482,2024.
A. R3m: A universal visual representation for robot
manipulation. arXivpreprintarXiv:2203.12601,2022. Valmeekam, K., Marquez, M., and Kambhampati, S. In-
vestigatingtheeffectivenessofself-critiquinginLLMs
Padalkar,A.,Pooley,A.,Jain,A.,Bewley,A.,Herzog,A., solvingplanningtasks.InNeurIPS2023FoundationMod-
Irpan,A.,Khazatsky,A.,Rai,A.,Singh,A.,Brohan,A., elsforDecisionMakingWorkshop,2023a. URLhttps:
etal. Openx-embodiment: Roboticlearningdatasetsand //openreview.net/forum?id=gGQfkyb0KL.
rt-xmodels. arXivpreprintarXiv:2310.08864,2023.
Valmeekam,K.,Marquez,M.,Sreedharan,S.,andKamb-
Pari,J.,Shafiullah,N.M.,Arunachalam,S.P.,andPinto, hampati, S. On the planning abilities of large lan-
L. Thesurprisingeffectivenessofrepresentationlearning guagemodels-acriticalinvestigation. InThirty-seventh
for visual imitation. arXiv preprint arXiv:2112.01511, ConferenceonNeuralInformationProcessingSystems,
2021. 2023b.URLhttps://openreview.net/forum?
id=X6dEqXIsEW.
Parisi,A.,Zhao,Y.,andFiedel,N. Talm: Toolaugmented
language models. arXiv preprint arXiv:2205.12255, Wang, T., Yu, P., Tan, X. E., O’Brien, S., Pasunuru,
2022. R., Dwivedi-Yu, J., Golovneva, O., Zettlemoyer, L.,
Fazel-Zarandi, M., and Celikyilmaz, A. Shepherd: A
Rocamonde, J., Montesinos, V., Nava, E., Perez, E., and
critic for language model generation. arXiv preprint
Lindner, D. Vision-language models are zero-shot re-
arXiv:2308.04592,2023.
wardmodelsforreinforcementlearning. arXivpreprint
arXiv:2310.12921,2023. Wong,L.,Mao,J.,Sharma,P.,Siegel,Z.S.,Feng,J.,Ko-
rneev,N.,Tenenbaum,J.B.,andAndreas,J. Learning
Romera-Paredes, B., Barekatain, M., Novikov, A., Ba-
adaptiveplanningrepresentationswithnaturallanguage
log, M., Kumar, M. P., Dupont, E., Ruiz, F. J. R., El-
guidance. arXivpreprintarXiv:2312.08566,2023.
lenberg, J., Wang, P., Fawzi, O., Kohli, P., and Fawzi,
A. Mathematicaldiscoveriesfromprogramsearchwith Wu,C.,Yin,S.,Qi,W.,Wang,X.,Tang,Z.,andDuan,N.
large language models. Nature, 2023. doi: 10.1038/ Visualchatgpt: Talking,drawingandeditingwithvisual
s41586-023-06924-6. foundation models. arXiv preprint arXiv:2303.04671,
2023.
Schick,T.,Dwivedi-Yu,J.,Dess`ı,R.,Raileanu,R.,Lomeli,
M.,Zettlemoyer,L.,Cancedda,N.,andScialom,T. Tool- Xie,T.,Zhao,S.,Wu,C.H.,Liu,Y.,Luo,Q.,Zhong,V.,
former: Languagemodels canteach themselvesto use Yang, Y., and Yu, T. Text2reward: Automated dense
tools. arXivpreprintarXiv:2302.04761,2023. reward function generation for reinforcement learning.
arXivpreprintarXiv:2309.11489,2023.
Sharma,P.,Sundaralingam,B.,Blukis,V.,Paxton,C.,Her-
mans,T.,Torralba,A.,Andreas,J.,andFox,D. Correct- Yang,M.,Du,Y.,Ghasemipour,K.,Tompson,J.,Schuur-
ing robot plans with natural language feedback. arXiv mans,D.,andAbbeel,P. Learninginteractivereal-world
preprintarXiv:2204.05186,2022. simulators. arXivpreprintarXiv:2310.06114,2023.
10InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
Young,S.,Gandhi,D.,Tulsiani,S.,Gupta,A.,Abbeel,P.,
andPinto,L. Visualimitationmadeeasy. InConference
onRobotLearning,pp.1992–2005.PMLR,2021.
Yow, J., Garg, N. P., Ramanathan, M., Ang, W. T., et al.
Extract–explainabletrajectorycorrectionsfromlanguage
inputsusingtextualdescriptionoffeatures.arXivpreprint
arXiv:2401.03701,2024.
Yu,W.,Gileadi,N.,Fu,C.,Kirmani,S.,Lee,K.-H.,Are-
nas,M.G.,Chiang,H.-T.L.,Erez,T.,Hasenclever,L.,
Humplik,J.,etal. Languagetorewardsforroboticskill
synthesis. arXivpreprintarXiv:2306.08647,2023.
Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J.,
andWeston,J. Self-rewardinglanguagemodels. arXiv
preprintarXiv:2401.10020,2024.
Zeng,A.,Attarian,M.,Ichter,B.,Choromanski,K.,Wong,
A.,Welker,S.,Tombari,F.,Purohit,A.,Ryoo,M.,Sind-
hwani,V.,etal. Socraticmodels: Composingzero-shot
multimodal reasoning with language. arXiv preprint
arXiv:2204.00598,2022.
Zeng,F.,Gan,W.,Wang,Y.,Liu,N.,andYu,P.S. Large
languagemodelsforrobotics: Asurvey. arXivpreprint
arXiv:2311.07226,2023.
Zha,L.,Cui,Y.,Lin,L.-H.,Kwon,M.,Arenas,M.G.,Zeng,
A.,Xia,F.,andSadigh,D. Distillingandretrievinggener-
alizableknowledgeforrobotmanipulationvialanguage
corrections. arXivpreprintarXiv:2311.10678,2023.
Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni,
L.M.,andShum,H.-Y.Dino:Detrwithimproveddenois-
inganchorboxesforend-to-endobjectdetection. arXiv
preprintarXiv:2203.03605,2022.
Zhang,R.,Torabi,F.,Guan,L.,Ballard,D.H.,andStone,
P. Leveraginghumanguidancefordeepreinforcement
learningtasks. arXivpreprintarXiv:1909.09906,2019.
Zhao, T. Z., Kumar, V., Levine, S., and Finn, C. Learn-
ing fine-grained bimanual manipulation with low-cost
hardware. arXivpreprintarXiv:2304.13705,2023a.
Zhao,Z.,Lee,W.S.,andHsu,D. Largelanguagemodelsas
commonsenseknowledgeforlarge-scaletaskplanning.
arXivpreprintarXiv:2305.14078,2023b.
Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,
Wu,J.,Wohlhart,P.,Welker,S.,Wahid,A.,etal. Rt-2:
Vision-language-actionmodelstransferwebknowledge
toroboticcontrol. InConferenceonRobotLearning,pp.
2165–2183.PMLR,2023.
11InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
A.Appendix
Figure6. AdditionalfailurecasesofGPT-4Vcritic.
A.1.Preferenceelicitation
Fig. 8showstheprompttemplateforinstructingVLMstoprovidepreferencesovertrajectorypairs. Itfollowsasimilar
structureasthepromptforcritiqueelicitation,withtheadditionofframesfromoneextravideo.Similartocritiqueelicitation
(Sec. 4.1), the examples used to illustrate the output format are fixed across all test cases. There are three examples
correspondingtothefollowingconditions: (a)oneofthetrajectoriesispreferabletotheother; (b)neithertrajectoryis
desirable;and(c)bothtrajectoriesaredesirable.
AsmentionedinSec. 4.2,foreachtask,wehaveonepositivesamplealongwithtwonegativesamplesthatexhibitthesame
undesirablebehavior(s). Accordingly,intheevaluation,wetestwhetherGPT-4Vcangiveaccuratepreferencelabelsin
thefollowingcases: (a)whenpairsofpositiveandnegativesamples(denotedaspositive-negative)aregiven;(b)pairsof
negativesamples(denotedasnegative-negative)aregiven;and(c)pairsofpositivesamples(denotedaspositive-positive)are
given. Notethat,tocreatepositive-positivepairs,wesimplyhorizontallyflipeachframeofapositivesample. Alsonotethat
fornegative-negativesamples,weexpectGPT-4Vtonotestablishanyorderingwithinthepairsunlessvalidjustifications
areprovided. Forpositive-positivesamples,givenhowthepairsareformed,weexpectGPT-4Vtostatebothvideosare
12InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
Time
Initial
Policy
Scissor
Handover GPT-4V Critic: The robot is handing over the scissors with the points facing towards the human, which could potentially cause injury.
Updated
Policy
Initial
Policy
Snack
Grasping GPT-4V Critic: The candies spilled out from an already present opening in the bag.
Updated
Policy
Initial
Policy
Bread
Grasping GPT-4V Critic: The robot's gripper appears to compress the bread too much, potentially deforming it.
Updated
Policy
Initial
Policy
GPT-4V Critic: The robot's gripper is positioned with the sharp edge of the knife facing upwards during the release, which is not the
safest orientation for placing a knife onto a surface.
Updated
Knife
Policy
Putting Down
Iter 1
GPT-4V Critic: The robot's gripper releases the knife from a height that could cause the knife to bounce or change position on the
cutting board, potentially posing a safety risk.
Updated
Policy
Iter 2
Figure7. Visualizationofthetasksandpolicies.
13InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
Figure8. Preference-elicitationprompt.
Recall Precision
GPT-4V(Preference) 65.29% 60.65%
Table2. RecallrateandprecisionrateofcritiqueswhenGPT-4Vcontrastspositivesampleswithnegativesamples
equallypreferred.
Table3summarizestheevaluationresults(i.e.,accuracyscoresofpreferencelabels). ResultssuggestthatGPT-4Vcan
selectthepositivesamplesfrompositive-negativepairswithhighaccuracy. However,intwootherconditions,GPT-4V
showssubparperformance. GPT-4Valwaysestablishesinvalidrankingswithinnegative-negativeandpositive-positivepairs
by“hallucinating”thatonesampleisperfectandoneisflawed,asseeninitsjustifications. Alongwithaccuracyscores,we
alsoreporttherecallandprecisionrates(Table2),andtheerrortypeswithinthejustifications(Fig. 10)generatedwhen
contrastingnegativesampleswithpositivesamples. TheresultsresemblethoseobtainedwhenGPT-4Vcriticizesindividual
videos,indicatingthatthewaysofextractingcritiquesdonotsignificantlyaltertheoutputpattern.
A.2.ResultsofGeminiPro
Table4showstheperformanceofGeminicriticintermsofcritiqueaccuracy,recall,andprecision. Theevaluationsetupis
identicaltothatofGPT-4V(Sec. 5.1). ComparedtoGPT-4V,Geminitendstobe“over-optimistic”aboutthevideo,meaning
thatitmorefrequentlystatesthereisnoundesirabilitypresented. However,thisover-optimismalsoleadstoasignificantly
lowerrecallrate,asmanyundesirableeventsarenotcaptured. Overall,theresultsindicatethatthereisstillaperformance
gapbetweentheProversionofGeminiandGPT-4V,suggestingthatGPT-4Vcurrentlyisabetterchoiceforbehaviorcritics.
WedidnotevaluateGeminiProforpreferenceelicitationduetotherestrictiononthenumberofframesperconversion.
positive-negative positive-positive negative-negative
GPT-4V(Preference) 95.61% 3.7% 0%
Table3. AccuracyscoresofpreferencelabelsgeneratedbyGPT-4V
14InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
Figure9. Critique-elicitationprompt.
Acc.
Metrics Recall Precision
Positive Negative
GeminiPro 71.92% 42.98% 18.18% 44.9%
Table4. EvaluationresultsofGeminiProcritic.
A.3.Robotexperimentdetails
WeprovidethefollowingprimitivecontrolAPIstoCaPagent:
• get xy(object),whichqueriesatop-downcameraandpre-trainedvision-languagemodelstogetthelocationofan
objectonthetable.
• move gripper to(x, y, z),whichmovestherobotend-effectortothedesiredx,y,zinthespace.
• control gripper(force, position),whichcontrolsthegripperandcloseittoindicatedpositionwithspecifiedforce.
• rotate(degree),whichrotatestheend-effectoraccordingtotheinputdegrees.
• pause(second),whichdoesnothingandholdtherobotforindicatedseconds.
WestrictlyfollowthepromptstructureoutlinedintheCode-as-Policiespaper. Whenitcomestocodeexamplesinprompts,
weuseexamplesdrawnfromapoolofpoliciesfortaskswithinthesamecategory. Forinstance,forbreadgraspingand
spoonpicking,examplesmaycomefromtaskssuchaspickingupaCokecan,pickingupabagofchips,andpickingup
breadpackagedindifferentways. Similarly,forscissorhandover,theexampletasksincludehandingoveratomato,handing
overacupofwater,andhandingoverapairofscissorsfromdifferentinitialpositions. Forknifeputtingdown,theexamples
areputting-relatedones,suchasputtingdownabananaontoabook,puttingdownanapple,andputtingdownaknifeheld
indiversewaysbytherobotinitially.
A.4.Examplesofhuman-writtennarrations
Instruction: pickuptheceramicbowlandputinthesmallboxforstoringceramichouseholditems
• t=0: The robot’s gripper reaches toward the ceramic bowl
15InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
Figure10. Distributionsoferrortypeswithinpreferencejustifications.
• t=1: The robot’s gripper closes its gripper on the rim of the ceramic bowl, securely grasping it
• t=2: The robot lifts the bowl to a few inches above the top of the container; the robot twists its gripper such
that the bottom of the bowl faces to the right
• t=3: The robot drops the bowl into the container; it lands on its side, bounces, and then lands upright in the
container
• t=4: The robot’s gripper exits the frame
Instruction: usethespatulatostir-frythefoodthoroughly
• t=0: The robot’s gripper grasps a spatula and prepares to stir-fry food in a pan; the sound of the gas-powered
flame fills the otherwise silent room
• t=1: The robot begins to stir-fry the food; a yellow vegetable jumps out of the pan and lands on the grating
• t=2: The robot continues to stir-fry the food; the vegetables turn and flip over inside the pan
Instruction: movetheturnerandplaceitattheleftedgeofthetable
• t=0: the robot’s gripper approaches the spatula and hovers over its handle
• t=1: the robot closes its gripper and securely grasps the spatula
• t=2: the robot lifts the gripper and bumps into the pot with eggs in it, moving it a few inches closer to the edge
of the table
• t=3: the robot, still gripping the spatula, moves around the pot and places the spatula on the other side of the
pot
• t=4: the robot withdraws its gripper from the frame
Instruction: putthefacialcleanserontothewhitetray
• t=0: The robot’s gripper approaches a bottle of lotion which sits by itself on a tray
• t=1: The robot’s gripper pumps lotion; the lotion falls partially onto the bottle and also onto the tray
• t=2: The robot’s gripper adjusts itself to securely grasp the bottle of lotion
• t=3: The robot lifts the bottle of lotion and places it into the white tray with all the other lotions
Instruction: poursomeorangejuiceintotheguest’sglass
• t=0: the robot’s gripper is firmly grasping a glass of orange juice
16InvestigatingtheUseofVideo-LanguageModelsasBehaviorCriticsforCatchingUndesirableAgentBehaviors
• t=1: the robot, while approaching the cup, spills some orange juice onto the table
• t=2: the robot’s gripper tilts the glass to pour the juice into the cup
• t=3: the robot pours all the orange juice from the glass into the cup
• t=4: the robot’s gripper tilts the glass back to right side up and lifts the glass out of frame
17