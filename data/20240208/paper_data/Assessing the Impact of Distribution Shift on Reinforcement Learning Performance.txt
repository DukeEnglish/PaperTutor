Assessing the Impact of Distribution Shift on
Reinforcement Learning Performance
TedFujimoto1∗JoshuaSuetterlein1SamratChatterjee1,2AuroopGanguly2,1
1PacificNorthwestNationalLaboratory 2NortheasternUniversity
Abstract
Researchinmachinelearningismakingprogressinfixingitsownreproducibility
crisis. Reinforcementlearning(RL),inparticular,facesitsownsetofuniquechal-
lenges. Comparisonofpointestimates,andplotsthatshowsuccessfulconvergence
to the optimal policy during training, may obfuscate overfitting or dependence
ontheexperimentalsetup. AlthoughresearchersinRLhaveproposedreliability
metricsthataccountforuncertaintytobetterunderstandeachalgorithm’sstrengths
andweaknesses,therecommendationsofpastworkdonotassumethepresenceof
out-of-distributionobservations. Weproposeasetofevaluationmethodsthatmea-
suretherobustnessofRLalgorithmsunderdistributionshifts. Thetoolspresented
herearguefortheneedtoaccountforperformanceovertimewhiletheagentis
actinginitsenvironment. Inparticular,werecommendtimeseriesanalysisasa
methodofobservationalRLevaluation. Wealsoshowthattheuniquepropertiesof
RLandsimulateddynamicenvironmentsallowustomakestrongerassumptions
to justify the measurement of causal impact in our evaluations. We then apply
thesetoolstosingle-agentandmulti-agentenvironmentstoshowtheimpactof
introducingdistributionshiftsduringtesttime. Wepresentthismethodologyasa
firststeptowardrigorousRLevaluationinthepresenceofdistributionshifts.
1 Introduction
ThefieldofRLhasenjoyedsomespectacularrecentadvancements,likereachingsuperhumanlevelsat
boardgames[Silveretal.,2016,2018,Bakhtinetal.,2022],sailboatracing[McKinseyandCompany,
2021],multiplayerpoker[BrownandSandholm,2019],andreal-timestrategygames[Berneretal.,
2019,Vinyalsetal.,2019]. TransitioningRLtowardarigorousscience,however,hasbeenamore
complicatedjourney. Likeotherfieldsinmachinelearning,progressinRLmightbecompromised
byalackoffocusonreproducibilitycombinedwithmoreemphasisonbest-caseperformance. To
remedythisproblem,reliabilitymetricshavebeenproposedtoimprovereproducibilitybymaking
RLevaluationmorerigorous. Someexamplesincludethedispersionandriskoftheperformance
distribution[Chanetal.,2020],andinterquartilemeanwithscoredistributions[Agarwaletal.,2021].
Pastwork,however,doesnotassumethepresenceofdistributionshiftduringtesttime.
In general, distribution shift in machine learning occurs when there is a difference between the
trainingandtestdistributions,whichcansignificantlyimpactperformancewhenthemachinelearning
systemisdeployedintherealworld[Kohetal.,2021]. Insupervisedlearning,distributionshiftcould
causeadecreaseinaccuracy. WewillfocusondistributionshiftinRL,whichcouldcauseadecline
inexpectedreturns. ThisimpactonperformanceisasymptomofoverfittingindeepRL,whichis
aproblemthatrequirescarefullydesignedevaluationprotocolsfordetection[Zhangetal.,2018].
Whiletherearemanytypesofdistributionshift,wewillfocusontest-timeadversarialexamplesand
theintroductionofnewagentsinmulti-agentadhocteamwork.
∗CorrespondingAuthor:ted.fujimoto@pnnl.gov
WorkshoponRegulatableMachineLearningatthe37thConferenceonNeuralInformationProcessingSystems
(RegML@NeurIPS2023).
4202
beF
5
]GL.sc[
1v09530.2042:viXraTakinginspirationfromcarcrashtestsandsafetyratingsfromtrustedorganizations,liketheNational
HighwayTrafficSafetyAdministration(NHTSA),webelieveRLwouldbenefitfromtechniquesthat
evaluaterobustperformanceaftertraining. Here,wecontributesomerecommendationsforevaluation
protocolsforRLagentsthatencounterdistributionshiftattesttime. Specifically,werecommend(1)
thecomparisonoftimeseriesforecastingmodelsofagentperformance,(2)usingpredictionintervals
tocapturethedistributionanduncertaintyoffutureperformance,and(3)counterfactualanalysis
whendistributionshifthasbeenappliedbytheexperimenter. Webelievethisstress-testmethodology
isapromisingstarttowardsreliablecomparisonofpretrainedRLagentsexposedtodistributionshift
inbothsingleandmulti-agentenvironments.
Insection2,wementionpastrelatedworkondistributionshiftandRLreproducibility. Insection3,
wearguewhytimeseriesanalysisisneededforevaluationofRLalgorithmsunderdistributionshift.
Insection4,weoutlineourrecommendationsforRLevaluationwithtimeseriesanalysis. Insection
5,weprovideexamplesofsuchanalysesinsingleandmulti-agentRL.Insection6,weconclude
withsuggestionsforfutureresearchinRLevaluationanddescribehowitcanadvanceMLsafetyand
regulation.
2 RelatedWork
Distribution(ordataset)shiftoccurswhenthedatadistributionattrainingtimediffersfromthedata
distributionattesttime[Quinonero-Candelaetal.,2008]. Thefocusofthispaperisnottodetect
distributionshift[Rabanseretal.,2019],buttoassumeitexistswhilemeasuringagentperformance.
Therehasbeenworkonreproducibilityunderdistributionshiftforsupervisedlearning[Kohetal.,
2021], but RL will be the focus here. In particular, we focus on how overfitting to the training
environmentcanaffecttheagent’sperformanceduringevaluationinatestenvironment[Zhangetal.,
2018]. Althoughwearetakingatimeseriesperspectiveinthispaper,wearenotproposinganew
methodofmachinelearningtotraintimeseriesmodels[Ahmedetal.,2010,Masinietal.,2023].
Furthermore,wearenotproposingamethodofcausalmachinelearning[Petersetal.,2017]. Thatis,
wearenotadvocatingamodelthatlearnscausalrepresentations. Furthermore,wedonotclaimto
bethefirsttouseinterventionsinsimulationsforRL,orotherareasofmachinelearning[Ahmed
et al., 2021, Lee et al., 2021, Verma and Srivastava, 2021, Lee et al., 2023, Verma et al., 2023].
We contribute a methodology that includes counterfactual time series analysis, using the impact
visualizationstrategyfromBrodersenetal.[2015],asawaytomeasuretheimpactofdistribution
shift after training. We will use time series and counterfactual analysis only as methods of RL
performanceevaluation.
Anydistributionshiftcanbeusedinourmethodology. Duetotheinterestandresearchprogressin
adversarialattacksandadhocteamwork,thedistributionshiftsweinvestigateareadversarialattacks
onimages(Atarigameobservations)andagentswitchinginmulti-agentenvironments. Therehas
beenextensiveresearchonadversarialattacksonsupervisedlearningmodels,likesupportvector
machines and neural networks [Huang et al., 2011, Biggio et al., 2012, Goodfellow et al., 2014,
Kurakinetal.,2016]. WhilewewillfocusontheadversarialattacksproposedinHuangetal.[2017],
therehasbeenrelatedresearchonadversarialattacksinsingle-agent[KosandSong,2017,Pattanaik
etal.,2017,Rakhshaetal.,2020,Zhangetal.,2020],andmulti-agentRL[Gleaveetal.,2019,Ma
etal.,2019,Figuraetal.,2021,Fujimotoetal.,2021,Casperetal.,2022,Cuietal.,2022]. Another
setofexperimentswilltesttheadhocteamworkofthegroupofagents[Stoneetal.,2010,Barrett
andStone,2015,Rahmanetal.,2021,Mirskyetal.,2022],whereagentswitchingwillbetreatedasa
distributionshiftamongthegroup.
RigorousevaluationofRLalgorithmsisstillatopicthatrequiresfurtherinvestigation. Henderson
etal.[2018]showthatevensubtledifferences,likerandomseedsandcodeimplementation,canaffect
thetrainingperformanceofdeepRLagents. Engstrometal.[2020]giveamorethoroughstudyof
RLimplementationandprovideevidencethatseeminglyirrelevantcode-leveloptimizationsmightbe
themainreasonwhyProximalPolicyOptimization[Schulmanetal.,2017]tendstoperformbetter
thanTrustRegionPolicyOptimization[Schulmanetal.,2015]. Colasetal.[2018]showhowthe
numberofrandomseedsrelatestotheprobabilityofstatisticalerrorswhenmeasuringperformance
inthecontextofdeepRL.TheinherentbrittlenessincurrentdeepRLalgorithmscallsintoquestion
thereproducibilityofsomepublishedresults. ThishasledtoproposalsforrigorousandreliableRL
evaluationtechniquesgroundedinstatisticalpractice. Chanetal.[2020]recommendedreliability
metrics like interquartile range (IQR) and conditional value at risk (CVaR). Jordan et al. [2020]
2Comparing Evaluation Performance over Time
20.0 Agent 1
17.5 Agent 2
Agent 3
15.0
12.5
10.0
7.5
5.0
2.5
0.0
0 2 4 6 8 10
Episodes
Figure1: Inthissimplifiedplotofagentperformanceinthepresenceofworseningdistributionshifts
overtime. Allthreeagentshaveaveragereturnsof10. Itisclear,however,thatagent3istheleast
desiredagentovertime. Eventhoughagent3startsoutwiththehighestaveragereturns,itseems
to have overfit to the training environment and fails to maintain its superior performance. Point
estimatesalonewouldnotcapturethisbehavior.
suggestedmetricslikeperformancepercentiles,proposedagame-theoreticapproachtoquantifying
performanceuncertainty,anddevelopedatechniquetoquantifytheuncertaintythroughouttheentire
evaluationprocedure. Agarwaletal.[2021]recommendedstratifiedbootstrapconfidenceintervals,
scoredistributions,andinterquartilemeans. Formulti-agentcooperativeRL,Gorsaneetal.[2022]
proposedastandardperformanceevaluationprotocolusingRLrecommendationsfrompastpapers.
What sets our methodology apart from past work is the emphasis on time series analysis of RL
performancewithdistributionshiftduringtesttime. Chanetal.[2020]hasincludedafterlearning
metricsintheirwork,butfocusesonperformancevariabilityfromtrainedpolicyrolloutsalone,which
isunlikelytocaptureperformanceinthepresenceofsignificantenvironmentchanges.
3 TheNeedtoMeasurePerformanceOverTime
3.1 ATimeSeriesPerspective
Wearguefortimeseriesanalysisasasolutiontotheproblemsthatpointestimatesandconfidence
intervalsalonecannotfix. Becausewearenotcertainoftheagent’sfutureperformance,forecasting
itisneededifweassumethetrainingandtestenvironmentsarenotidentical. Pointestimatesalone
canfailtoexplaindecreasesinperformanceattesttime. InFigure1,weassumethreeagentsthatact
inanenvironmentexperienceincreasingdistributionshiftasthenumberofepisodescontinues. All
agentsachievethesameaveragereturns,butthereisacleardifferenceinperformanceovertime. In
thishypotheticalexample,Agent3seemstohaveoverfittothetrainingenvironmentbecauseitstarts
highbutendsasthelowestperformer. Inthelongerterm,Agent2ispreferablebecauseitsdecrease
inperformanceisslower,andeventuallyoutperformsAgent3. Agent1representstheidealRLagent
becauseitsperformanceovertimeneverdecreases.
Within-episodeperformanceisdifficulttomeasurebecauseitrequiresknowledgeoftheparticular
environment’s states to know when to make an intervention. To make our methodology more
general,wemeasuretheimpactofdistributionshiftovermanyepisodes,whereXA(t)istheexpected
performanceatanepisodet.Thisallowsustomeasurethepositive(ornegative)impactofdistribution
shiftsovertime. Anexampleofthiscouldbemeasuringtheenergyusageorcustomersatisfactionof
aRL-trainedHVACsystemeachdayoveraweekifexposedtodistributionshift.
3.2 RLandtheFundamentalProblemofCausalInference
Practitionersofcausalinferencemustrememberitsfundamentalproblem: Ifweweretoexposea
unitutosomeintervention,wewillneverseetheworldwhereweneverexposedtheunitutothat
intervention[Holland,1986]. Thatis,observingindividualtreatmenteffectisimpossible. Thishas
notdiscouragedresearchersfromdevelopingmethodsthatcircumventthisproblem[Spirtesetal.,
2000, Pearl, 2009, Imbens and Rubin, 2015, Peters et al., 2017]. These methods usually require
assumptionsthatmanagethemessinessoftherealworld,whichmakesitpossibletoconstructvalid
3
snruteR
egarevAargumentsinfavorofcausalinference. RLagentsinsimulatedenvironments,however,allowusto
makeassumptionsthatmightbetoostrongforlesspredictableunits(likehumansoranimals). In
adeterministicenvironment,withagivenrandomseed,regardlessofhowmanytimesyoureacha
states,theagentwillchoosethesameactionandreceivethesamerewardatstates. Thatis,RL
agentsindeterministicenvironmentswithfixedrandomseedsallowustoseewhathappenswhen
wechoosetointerveneornot. ThisdoesnotcontradictHendersonetal.[2018],wheretheyargue
thatdifferentrandomseedscanleadtodifferentperformancesandbehaviors. Here,weclaimthat
differentRLagentswiththeirownrandomseedmayexhibitdifferentbehaviors,butwillrepeatthose
fixedbehaviors. NowthatweareconcernedwithRLevaluationovertime,wecanmakethefollowing
assumption:
RLFixedSeedAssumption3.1. LetT,suchthat1≤T ≤N,bethetimeaninterventionoccurs.
LetGdesignategroupssuchthatG=1indicatesthetreatmentgroupandG=0indicatesthecontrol
group. ConsideratimeseriesoutcomeXA(t),whereAisaRLagentinasimulateddeterministic
environmentM. LetXA(t<T)betheperformancebeforetimeT.Then,
E[XA(t<T)|G=1]=E[XA(t<T)|G=0] (1)
LetXA (t)betheperformancemeasurementasabovewiththecounterfactualinterventionU =u.
U=u
WedefineU = 1andU = 0asbeingexposedtoanintervention(e.g.,distributionshift)andnot
exposed,respectively. Then,onaverage,theperformanceofthecontrolgroupisthecounterfactual
performanceofthetreatmentgroupasifithadnotbeenexposedtotheintervention:
E[XA(t)|G=0]=E[XA (t)|G=1] (2)
U=0
This assumption basically says that if we have a fixed set of random seeds2 in a deterministic
environment,thentheexpectedreturnsofthecontrolgroupisthesameastheexpectedreturnsofthe
counterfactualtreatmentgroupwherenoout-of-distributioninterventioneveroccurred. Thismakes
intuitivesensebecausebothgroupswillexhibitidenticalbehaviorindeterministicenvironmentsif
nooutsideinfluenceisintroduced. ThisishelpfulwhenjustifyingcausalinferenceintheAppendix.
4 RecommendationsforTimeSeriesEvaluation
4.1 Ifyoucancontrolwhenthedistributionshiftoccurs,measurethecausalimpact
Similartocarcrashtestsincontrolledsettings,weproposeanevaluationmethodwheretheexperi-
mentercancontrolwhenaRLagentexperiencesashiftindistribution. First,takeatrainedRLagent
andhaveitinteractinitsenvironmentuntilsometime(orepisode)T,whichwillbethepre-treatment
period. AttimeT,thepost-treatmentperiodstartswiththeexperimenterappliesashiftindistribution.
Suchdistributionshiftsincludeadversarialexamples[Goodfellowetal.,2014]. Inthemulti-agent
cooperativetasks,suddenreplacementofagentscanalsocauseshiftsindistribution[Mirskyetal.,
2022]. Theexperimenterlogstheperformancescoresateachpointintime. Whentheagentreaches
theendoftheexperiment,evaluatethecausalimpactfromthecounterfactualmodel(theRLagent
controlgroup)andthetreatmentgroup’spost-treatmentperformanceusingdifference-in-differences
(DiD)[Cunningham,2021,Huntington-Klein,2021].Theresultswillshowhowmuchthedistribution
shiftimpactedtheperformanceoftheagentassumingacounterfactualmodelthatrepresentsthe
agent’sperformanceifthedistributionshiftneverhappened. Wewillassumethetrainedagentshave
achievedaflat(slope=0)trendinrawperformance(asopposedtonoisyorrandom)intheabsence
ofdistributionshiftattesttime.
2ReproducibilitycanbedifficulttoachievewhenrunningonaGPU.Wediscussthisfurtherandhowto
controlforitintheAppendix.
4Prediction Trends of Agent Performance Prediction Trends of Agent Performance
10
6
4
5
2
0 0
2
4 5
6 Agent 1 Agent 1
8 A A Stg g ae e rn n t t t o 2 3 f prediction trend lines 10 A A Stg g ae e rn n t t t o 2 3 f prediction trend lines
0 20 40 60 80 100 0 20 40 60 80 100
Episodes Episodes
Figure 2: Left: The differences in performance are clear because the prediction intervals do not
overlapattheendoftheplot. Hence,Agent1hasthebestperformancebecausetheforecastisnot
decreasingandthepredictionintervalissmall. Right: Here,allagentshavenoisierperformance.
EventhoughAgent3stillhasadownwardtrend,itbrieflyspikesuptomatchAgent1’sperformance.
Hence, we want prediction intervals that anticipate this uncertainty by showing interval overlap
betweenagentperformanceovertime. ThereisnolongerasignificantdifferencebetweenAgents2
and3becausetheirpredictionintervalsoverlapateverytimestep.
Toshowtheimpactofthedistributionshift,weusethetemplateoftime-seriesimpactplotsfrom
Brodersenetal.[2015]. Thistemplateconsistsofthreepanels: anoriginalplot,apointwiseplot,and
acumulativeplot. Intheoriginalplot,therawperformanceisshown. Thepointwiseplotshowsthe
differencebetweentheobserveddataandthecounterfactualpredictions,whichistheinferredcausal
impactofintervention. Thecumulativeplotshowsthecumulativeimpactoftheinterventionover
time. InsteadofBrodersenetal.’schoiceofusingBayesianstructuraltime-seriesmodels,weuseDiD
becausewearemeasuringonlyonevariable(returns). ThejustificationforDiDinthismethodology
isprovidedinthemethodssectionoftheAppendix.
4.2 Otherwise,compareagentsusingsimpletimeseriestrendswithpredictionintervals
Here, we assume the experimenter has no control over when the distribution shift will occur. It
isalsopossiblethatdistributionshiftmaynotoccuratcertaintimes. Suchscenariosareintended
to be a closer representation of real-world RL agent deployment. This implies, without further
assumptions,anobservationalstudyisthebestwecando.Ifwewanttoaccountfortheuncertaintyof
theperformancetrends,itwouldbepreferabletounderstandthedistributionofvaluesandwherewe
expectourforecastingmodeltogeneratethenextdatapointsampled[Buteikis,2020]. Wepropose
comparisonofRLperformanceusingtimeseriestrends,likeHolt’sLinearDampedTrendMethod
[GardnerJrandMcKenzie,1985,Holt,2004,HyndmanandAthanasopoulos,2018],withprediction
intervals. AnexampleisprovidedinFigure2. Here,wecancomparetheperformanceofagents
trainedondifferentRLalgorithms. TheideabehindthesegraphsisthesameasinFigure1. The
maindifferencesarethatweusetimeseriesforecastsandpredictionintervalstoshowthepredicted
performancetrendofeachagent. Here,theperformancesbetweenepisodes0-50arethemeasured
performances of the agents in the environment. The plots at episodes 50-100 are not measured
performance,butpredictiontrendsoffutureperformancewithpredictionintervals. Thisvisualizes
robustperformancebyshowinguncertaintyinpredictedaveragereturnsovertime. Ideally,theagent
thatperformsthebestwouldhavethehighesttrendlineandthesmallestpredictioninterval. Using
timeseriesforecastswithpredictionintervalsaremeanttobeanimprovementoverpointestimates
withconfidenceintervalstobettershowuncertaintyinpredictedaveragereturnsovertime. Since
wewillbeusingHolt’slineardampedtrendmethod,95%predictionintervalsmightbetoonarrow
[Hyndman,2014]. Hence,wewilldefaultto99%predictionintervals. Likeinsection3,weinterpret
eachtimepointasanepisode,andXA(t)representstheRLagent’sperformanceduringthatepisode.
FurtherdescriptionofthismethodisprovidedinthemethodssectionoftheAppendix.
InastylesimilartoGorsaneetal.[2022],wesummarizethepreviousrecommendationsintothe
followingprotocol:
5
snruteR
egarevA
snruteR
egarevAAProtocolforMeasuringPerformanceinthePresenceofDistributionShift
Input: Environments E ...E , Algorithms A ...A with corresponding time series
1 K 1 M
metricXA(t),DistributionShiftsd ...d .
1 N
1. Evaluationparameters-defaults
• Numberofepisodesforimpactandobservationalplots.
• Numberofepisodesforforecastingwithpredictionintervals(=100).
• Numberofrandomseeds/runs(=10fromAgarwaletal.[2021]).
2a. Ifyoucancontrolwhenthedistributionshiftoccurs,measurethecausalimpact
• Measurethecausalimpactofeachdistributionshiftinterventiond usingdifference-
n
in-differenceswithplotsofthe(i)rawperformance,(ii)pointwiseimpact,and(iii)
cumulativeimpact(fromBrodersenetal.[2015]).
• Foreachpair(E ,d ),algorithmA performsbetterthanA inthepresenceofd
k n i j n
ifA ’scumulativeimpactofaveragereturnsishigherthanA ’sattheendofthe
i j
experiment.
2b. Otherwise,compareagentsusingtimeseriestrendswithpredictionintervals
• Inthepresenceofadistributionshift,plotaforecast,usingHolt’sLinearDamped
TrendMethod,with99%predictionintervalsover100episodesafterthelastmea-
suredperformance(fromHolt[2004]).
• Foreachpair(E ,d ),algorithmA achievessignificantlyhighertrendinperfor-
k n i
mancethanA inthepresenceofd ifA hasahigherforecastpredictionlinethan
j n i
A withnooverlapbetweenthecorrespondingpredictionintervals.
j
3. Reporting
• Experiment Parameters: Report all distribution shift implementations, hyperpa-
rameters, code-level optimizations, computational requirements, and framework
details.
• Plots: ForeachenvironmentE ...E ,createplotsfrom2aor2b(preferablyboth)
1 K
foreachalgorithmA ...A inthepresenceofdistributionshiftsd ...d .
1 M 1 N
• Analysis: Provideanalysisfortheplotsandderiveconclusions.
5 ExamplesofRLTime-seriesAnalysis
Inourtimeseriesevaluation,wemodelthescenarioasdeployingtheagent(s)outintheintended
environment. In this scenario, we have the pretrained agents run over a number of episodes. As
mentionedpreviously,thecausalimpactprocedurecanbeinterpretedastheRLversionofcarcrash
tests,whereRLagent(s)aredeployedinacontrolledenvironmentandundergorepeatedperformance
testing. Whenevaluatingthecausalimpactofthedistributionshift, weintroducetheshiftatthe
halfwaypoint. Wemeasuretheperformanceofthetreatmentandcontrolgroups,thenmeasurethe
differenceandcumulativeimpactoftheshift. Inbothsingleandmulti-agentsettings,weusethe
causalimpactplottemplatefromBrodersenetal.[2015].
In the observational case, we can interpret having the agents run over a number of episodes as
deployingtheagentintotheenvironmenteachdaywhilerecordingitsperformance. Thedistribution
shiftsintheobservationalevaluationshappenrandomly,sothehumanmaintainershavenocontrol
overhowtheshiftsareintroduced. Whenplottingtheseevaluations,thesingle-agentandmulti-agent
casesusedifferentapproachestorandomlyintroducingdistributionshift. Inthesingle-agentcase,
we define a probability threshold for adversarial attacks. If the random number generator (like
random.random()inPython)givesanumberabovethethreshold,theAtarigameimageisattacked.
In the multi-agent case, we use a strategy similar to Rahman et al. [2021], where the number of
stepsanagentisswitchedoutisdrawnfromauniformdistribution. Figure4providessomeAtari
gameexamplesofobservationalstudies. Weuse10randomseedsforbothcausalandobservational
time-seriesevaluation. MoreinformationontheplotsisprovidedintheAppendix.
6Agent: a2c Env: BreakoutNoFrameskip-v4 Agent: ppo Env: BreakoutNoFrameskip-v4
100 100
50 50
0 0
0 0
20 20
0 0
2000
2000
4000
0 100 200 300 400 0 100 200 300 400
Episode Episode
Counterfactual Epsilon 0.05 Epsilon 0.5 Counterfactual Epsilon 0.05 Epsilon 0.5
Epsilon 0.01 Epsilon 0.1 Epsilon 1.0 Epsilon 0.01 Epsilon 0.1 Epsilon 1.0
Agent: a2c Env: PongNoFrameskip-v4 Agent: ppo Env: PongNoFrameskip-v4
20 20
0 0
20 20
0 0
20 20
40 40
0 0
1000 1000
0 20 40 60 80 0 20 40 60 80
Episode Episode
Counterfactual Epsilon 0.05 Epsilon 0.5 Counterfactual Epsilon 0.05 Epsilon 0.5
Epsilon 0.01 Epsilon 0.1 Epsilon 1.0 Epsilon 0.01 Epsilon 0.1 Epsilon 1.0
Figure3:ThecausalimpactplotsshownhereillustratetheimpactofFGSMadversarialattacksonRL
agentstrainedonthePongAtarigame. EachrowrepresentsanAtarigame. Eachcolumnrepresentsa
RLalgorithm(A2CorPPO).Theoriginalplotshereshowtherollingmeanoftherewardsovertime.
Thepointwiseplotsshowthedifferencebetweenthecounterfactualperformanceandtheperformance
whentheagentisattacked. Thecumulativeperformanceisthesummationoftherewardsgainedor
lostovertime. Asexpected,theperformancetendstodropasϵincreases.
5.1 AnalysisonAdversarialAttacksonA2CandPPOAgents
Inthisanalysis,wemeasuretheperformanceofRLagentstrainedonAtarigames[Bellemareetal.,
2013]inthepresenceofadversarialattacks. InFigure3andtheAtariimpactgraphsinAppendixD,
weusepretrainedA2CandPPOpretrainedagentsprovidedbyRLBaselines3Zoo(RLZoo)[Raffin,
2020]implementedinStable-Baselines3[Raffinetal.,2021].
ForadversarialattacksontheAtarigameimages,weusetheFastGradientSignMethod(FGSM)
[Goodfellowetal.,2014]. Inmostcases,likeinFigure3,theadversarialattacksworkasexpected
andgenerallydecreasestheperformanceasϵincreases. Onenoticeablepatternisthattheagents
significantlydeviatefromthebaselineperformancewhenϵ ≥ 0.5. PPOperformanceisgenerally
equal or worse than A2C after the distribution shift intervention at the halfway point. There are
games,likeQbertandSpaceInvaders,wherePPOachievesmuchworsecumulativereturnsthanA2C
whenexposedtothehighestvaluesofϵ. AnotherstrangefindingwasthattheRLZooA2Cagent
showednon-decreasing(orslightlyincreasing)cumulativereturnsinAsteroidsandRoadRunner.
OneconclusionthatcanbedrawnfromthesefindingsisthatthepretrainedRLZooA2Cismore
robustthanPPOagainstconstantadversarialexamples.
The observational plots, like in Figure 4 and Appendix D, tell a more nuanced story. Except for
Pong,wherethePPOperformancetrendsaresignificantlybetterthanA2C,PPO’sforecaststrends
arehigherthanA2C’sbutthereisoverlapbetweenthepredictionintervals. ItseemsthatthePPO
agentsareabletorecoverandchoosebetteractionswhennotobservingadversarialexamples. This
isconsistentwiththeRLBaselines3Zoobenchmarkscores,wherePPOscoreshigherthanA2Cin
everyAtarigame(withoutthepresenceofadversarialattacks). Hence,weconcludethatRLZooPPO
agentshavetrendsthatperform(notsignificantly)betterinthepresenceofrandomattacks,butthe
7
sdraweR
sdraweR
evitalumuC
evitalumuC
lanigirO
esiwtnioP
lanigirO
esiwtnioP
sdraweR
sdraweR
evitalumuC
evitalumuC
lanigirO
esiwtnioP
lanigirO
esiwtnioPPrediction Trends in PongNoFrameskip-v4 with FGSM(=0.01) Prediction Trends in PongNoFrameskip-v4 with FGSM(=0.50)
21 21
20
20
19
19 A PP2 OC 18
Start of prediction trend lines 17
18 16
15
17 14 A PP2 OC
Start of prediction trend lines
0 25 50 75 100 125 150 175 200 13 0 25 50 75 100 125 150 175 200
Episode Episode
Prediction Trends in BreakoutNoFrameskip-v4 with FGSM(=0.01) Prediction Trends in BreakoutNoFrameskip-v4 with FGSM(=0.50)
60 40
40 30
20 20
0 10
20 A PP2 OC 0 A PP2 OC
Start of prediction trend lines Start of prediction trend lines
0 100 200 300 400 500 600 0 100 200 300 400 500 600
Episode Episode
Figure4: Theseplotstaketherollingmean(window=25)oftheobservationalperformanceupto
a certain time point with some probability of being in the presence of adversarial attacks. After
thattimepoint,itshowsthetimeseriesforecastwith99%predictionintervals. TopRow: Inthe
PongNoFrameskip-v4plots,PPOperformssignificantlybetterandhassmallerpredictionintervals.
Theplotontheright,however,usesattackswithhigherϵ,wherebothagentshavelargerprediction
intervals. BottomRow: IntheBreakoutNoFrameskip-v4plots,thepredictionintervalsoverlap. One
noticeabledifferenceisthatthestrongerattackscausethePPOintervaltoshrinkinsizewhilethe
meanrewardsareslightlyless. Thisdecreaseinvariabilityaccountsforthedecreaseinthemaximum
rewardsachieved.
impactplotsshowclearsignsofoverfittinginsomeenvironments. Ontheotherhand,A2Cagents
tendtoberelativelymorerobustthanPPOagainstadversarialattacksinAtarigames.
5.2 AnalysisonAgentSwitchinginPowerGridworld
Inthemulti-agentcase(Figure5),wemeasuretheperformanceofacooperativegroupof5decen-
tralizedPPOagentsinthepresenceofadhocagentswitching. Themulti-agentenvironmentwe
useisaframeworkforpower-systems-focusedsimulationscalledPowerGridworld[Biagionietal.,
2022]. AsseenontheimpactplotsinFigure5,weinvestigatewhathappenswhen1to3agentsin
thegroupareswitchedoutwiththecorrespondingnumberofoutsidepretrainedoruntrainedagents.
Theoutsidepretrainedagentsweretrainedasaseparategroupof5agentsthatachieveascoresimilar
totheoriginalgroup. Inthiscase,therewasaslightimpactwhenswitchingout1or2agents. The
performancedramaticallydecreasedwhen3agentswereswitchedout,whichisamajorityofthe
agents.Replacingoneagentofthegroupwithoneuntrainedagent,ascanbeseeninFigure5,ismuch
worsethanswitchingout3agentswithoutsidepretrainedagents. TheobservationalPowerGridworld
plotsinAppendixDprovidecomplementaryresults. Hence,inthisPowerGridworldenvironment,
wecanconcludethat(i)switchingoutandreplacingthemajorityofagents(includingpretrained)
cansignificantlydiminishperformance,and(ii)replacingevenonetrainedagentwithanuntrained
agentcanbeworsethanswitchingoutthemajoritywithoutsidetrainedagents. Whenswitchingthe
originalagentswithsimilarly-trainedoutsideagents,itseemsthatgroupperformanceisrobustwhen
onlyswitchingoutaminorityoftheagents,butissignificantlylessrobustwhenamajorityofagents
isswitchedout. Infurtherstudies,itwouldbeinterestingtoseeifthisbehaviorcanbegeneralizedto
otherdecentralizedgroups,andifadhocteamworktrainingmethodscanimprovetheseresults.
8
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevAReplaced with Trained Agents Replaced with Untrained Agents
220 0
Counterfactual Counterfactual
240 1 Agent Switched 2500 1 Agent Switched
2 Agents Switched 2 Agents Switched
260 3 Agents Switched 5000 3 Agents Switched
0 0
Difference Difference
20 1 Agent Switched 2500 1 Agent Switched
2 Agents Switched 2 Agents Switched
40 3 Agents Switched 5000 3 Agents Switched
0 0
Cumulative Sum Cumulative Sum
1000 1 Agent Switched 1 Agent Switched
2 Agents Switched 200000 2 Agents Switched
2000 3 Agents Switched 3 Agents Switched
0 20 40 60 80 100 0 20 40 60 80 100
Episode Episode
Figure5: Theplotshereshowtheimpactoftheadhocswitchingofagentsinagroupof5inthe
PowerGridworldenvironment. LeftColumn: Wereplace1, 2, or3agentsoutofthegroupof5
withagentsthattrainedwithadifferentgroup. Whilethereislittlechangewhenonlyreplacing1
or2agents,weseethatperformancedramaticallydecreaseswhen3agentshavebeenswitchedout.
RightColumn: Weseethatjustswitchingout1agentinthegroupwith1untrainedagentcausesa
significantlylargedecreaseingroupperformance.
6 Conclusion
Inthiswork,weshowthatrelyingonpointestimates,evenalternativesthataremorereliableandless
biasedthanwhatistypicallyusedinpractice,arelimitedintheirabilitytoevaluateRLperformance
inthepresenceofdistributionshifts. Inparticular,wearguethatthemethodologyproposedhere
provides more of a necessary emphasis on test-time evaluation, and can assess both single and
multi-agentperformance. If,orwhen,wechoosetodeployRLagentsintotherealworld,howsuch
agents perform after training will matter more than during training. The decline in performance
duringadhocagentswitchinginthePowerGridworldshowswhatcouldhappenifweneedtostart
replacingintelligentagentstoensureenergyusageisminimized. Suchexperimentsrevealaworkflow
thatwilllikelybeclosertorepresentingreal-worldsafetychecksandmaintenanceofAIsystems.
Ourmethodologymightalsobeusefulforinvestigatinggeneralsingleormulti-agentRLbehavior.
There is still work to accomplish in ensuring RL research is reproducible. One weakness is that
the generality of our protocol makes it difficult to make environment-specific conclusions. For
example,inanopen-endedgamelikeMinecraft[Johnsonetal.,2016,Gussetal.,2019,Fanetal.,
2022], we would also want task-specific explanations when the agent accomplishes a goal in a
massively multitask environment. Another weakness of our approach is that it does not account
for non-deterministic environments. Investigating if more complex time series models would be
appropriateincertainscenariosisanothernextstep. Timeseriesclusteringandmotifs[Mueenetal.,
2009,Imanietal.,2021]canbeusedtobetterunderstandhowdifferenttypesofdistributionshift
canaffectRLperformance. Onecouldalsoinvestigateiftimeseriesensemblemodels[Wichard
and Ogorzalek, 2004] provide better forecasts than simpler models. Developing a methodology
that provides environment-specific explanations, accounts for more assumptions, and uses more
sophisticatedtimeseriesmodelscanbeinvestigatedinfutureresearch.
Nations,liketheUnitedStates[WhiteHouse,2022],havesignedlegislationtoinvestinthemodern-
izationofinfrastructureandcommunitiestomeetthegrowingchallengesofthe21stcentury(e.g.,
cybersecurityandclimatechange). RLisadvancingtheautomationofcomplexdecisionmaking
inreal-worldinfrastructuresystems(e.g., energyandtransportation). LikeotherareasinAI,the
roleofRLincriticalinfrastructureandotherhigh-consequenceapplicationsrequiresrobust,reliable,
repeatable, and standardized evaluation protocols [European Union, 2021, Biden Jr, 2023]. The
methodologyweproposehere,inspiredbytheNHTSAratingsstandards,isastarttowardageneral,
standardizedprotocolforevaluatingpretrainedRLperformanceovertime.
Acknowledgments
ThisresearchwassupportedbytheNationalInfrastructureSimulationandAnalysisCenter(NISAC),
aprogramoftheU.S.CybersecurityandInfrastructureSecurityAgency’s(CISA’s)NationalRisk
9
sdraweR
evitalumuC
lanigirO
esiwtnioPManagement Center. Pacific Northwest National Laboratory is operated by Battelle Memorial
InstitutefortheU.S.DepartmentofEnergyunderContractNo. DE-AC05-76RL01830.
References
RishabhAgarwal,MaxSchwarzer,PabloSamuelCastro,AaronCCourville,andMarcBellemare.
Deepreinforcementlearningattheedgeofthestatisticalprecipice.Advancesinneuralinformation
processingsystems,34:29304–29320,2021.
Nesreen K Ahmed, Amir F Atiya, Neamat El Gayar, and Hisham El-Shishiny. An empirical
comparisonofmachinelearningmodelsfortimeseriesforecasting. Econometricreviews,29(5-6):
594–621,2010.
Ossama Ahmed, Frederik Träuble, Anirudh Goyal, Alexander Neitz, Manuel Wuthrich, Yoshua
Bengio,BernhardSchölkopf,andStefanBauer. Causalworld: Aroboticmanipulationbenchmark
forcausalstructureandtransferlearning.InInternationalConferenceonLearningRepresentations,
2021. URLhttps://openreview.net/forum?id=SK7A5pdrgov.
AntonBakhtin,NoamBrown,EmilyDinan,GabrieleFarina,ColinFlaherty,DanielFried,Andrew
Goff,JonathanGray,HengyuanHu,AthulPaulJacob,etal. Human-levelplayinthegameof
diplomacybycombininglanguagemodelswithstrategicreasoning.Science,378(6624):1067–1074,
2022.
Nikhil Barhate. Minimal pytorch implementation of proximal policy optimization. https://
github.com/nikhilbarhate99/PPO-PyTorch,2021.
Samuel Barrett and Peter Stone. Cooperating with unknown teammates in complex domains: A
robotsoccercasestudyofadhocteamwork. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume29,2015.
MarcGBellemare,YavarNaddaf,JoelVeness,andMichaelBowling. Thearcadelearningenviron-
ment: Anevaluationplatformforgeneralagents. JournalofArtificialIntelligenceResearch,47:
253–279,2013.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław De˛biak, Christy
Dennison,DavidFarhi,QuirinFischer,ShariqHashme,ChrisHesse,etal. Dota2withlargescale
deepreinforcementlearning. arXivpreprintarXiv:1912.06680,2019.
David Biagioni, Xiangyu Zhang, Dylan Wald, Deepthi Vaidhynathan, Rohit Chintala, Jennifer
King, and Ahmed S. Zamzam. Powergridworld: A framework for multi-agent reinforcement
learninginpowersystems. InProceedingsoftheThirteenthACMInternationalConferenceon
FutureEnergySystems,e-Energy’22,page565–570,NewYork,NY,USA,2022.Association
for Computing Machinery. ISBN 9781450393973. doi: 10.1145/3538637.3539616. URL
https://doi.org/10.1145/3538637.3539616.
Joseph Biden Jr. Executive order on the safe, secure, and trustworthy development and use of
artificial intelligence. White House, 2023. URL https://www.whitehouse.gov/briefing-
room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-
and-trustworthy-development-and-use-of-artificial-intelligence/. [Accessed
November2,2023].
Battista Biggio, B Nelson, P Laskov, et al. Poisoning attacks against support vector machines.
InProceedingsofthe29thInternationalConferenceonMachineLearning,ICML2012,pages
1807–1814.ArXive-prints,2012.
Kay H Brodersen, Fabian Gallusser, Jim Koehler, Nicolas Remy, and Steven L Scott. Inferring
causalimpactusingbayesianstructuraltime-seriesmodels. TheAnnalsofAppliedStatistics,pages
247–274,2015.
NoamBrownandTuomasSandholm. Superhumanaiformultiplayerpoker. Science,365(6456):
885–890,2019.
AButeikis. Practicaleconometricsanddatascience. VilniusUniversity: Vilnius,Lithuania,2020.
10StephenCasper,DylanHadfield-Menell,andGabrielKreiman. White-boxadversarialpoliciesin
deepreinforcementlearning. arXivpreprintarXiv:2209.02167,2022.
Stephanie CY Chan, Samuel Fishman, Anoop Korattikara, John Canny, and Sergio Guadarrama.
Measuringthereliabilityofreinforcementlearningalgorithms. InInternationalConferenceon
LearningRepresentations,2020.
CédricColas,OlivierSigaud,andPierre-YvesOudeyer. Howmanyrandomseeds? statisticalpower
analysisindeepreinforcementlearningexperiments. arXivpreprintarXiv:1806.08295,2018.
Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Introduction to
algorithms. MITpress,2022.
JiaxunCui,XiaomengYang,MulongLuo,GeunbaeLee,PeterStone,Hsien-HsinSLee,Benjamin
Lee, GEdwardSuh, WenjieXiong, andYuandongTian. Macta: Amulti-agentreinforcement
learningapproachforcachetimingattacksanddetection.InTheEleventhInternationalConference
onLearningRepresentations,2022.
ScottCunningham. Causalinference: Themixtape. Yaleuniversitypress,2021.
LoganEngstrom,AndrewIlyas,ShibaniSanturkar,DimitrisTsipras,FirdausJanoos,LarryRudolph,
andAleksanderMadry. Implementationmattersindeeppolicygradients: Acasestudyonppoand
trpo. arXivpreprintarXiv:2005.12729,2020.
EuropeanUnion. Layingdownharmonisedrulesonartificialintelligence(artificialintelligenceact)
andamendingcertainunionlegislativeacts.proposalforaregulationoftheeuropeanparliamentand
of the council, 2021. URL https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=
CELEX:52021PC0206.
LinxiFan,GuanzhiWang,YunfanJiang,AjayMandlekar,YuncongYang,HaoyiZhu,AndrewTang,
De-AnHuang,YukeZhu,andAnimaAnandkumar. Minedojo: Buildingopen-endedembodied
agentswithinternet-scaleknowledge. AdvancesinNeuralInformationProcessingSystems,35:
18343–18362,2022.
Martin Figura, Krishna Chaitanya Kosaraju, and Vijay Gupta. Adversarial attacks in consensus-
basedmulti-agentreinforcementlearning. In2021AmericanControlConference(ACC),pages
3050–3055.IEEE,2021.
TedFujimoto,TimothyDoster,AdamAttarian,JillBrandenberger,andNathanHodas. Reward-free
attacksinmulti-agentreinforcementlearning. arXivpreprintarXiv:2112.00940,2021.
EveretteSGardnerJrandEDMcKenzie. Forecastingtrendsintimeseries. Managementscience,31
(10):1237–1246,1985.
AdamGleave,MichaelDennis,CodyWild,NeelKant,SergeyLevine,andStuartRussell. Adver-
sarialpolicies: Attackingdeepreinforcementlearning. InInternationalConferenceonLearning
Representations,2019.
IanJGoodfellow,JonathonShlens,andChristianSzegedy. Explainingandharnessingadversarial
examples. arXivpreprintarXiv:1412.6572,2014.
RihabGorsane,OmaymaMahjoub,RuanJohndeKock,RolandDubb,SiddarthSingh,andArnu
Pretorius. Towardsastandardisedperformanceevaluationprotocolforcooperativemarl. Advances
inNeuralInformationProcessingSystems,35:5510–5521,2022.
WilliamHGuss,BrandonHoughton,NicholayTopin,PhillipWang,CaydenCodel,ManuelaVeloso,
and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. arXiv
preprintarXiv:1907.13440,2019.
PeterHenderson,RiashatIslam,PhilipBachman,JoellePineau,DoinaPrecup,andDavidMeger.
Deepreinforcementlearningthatmatters. InProceedingsoftheAAAIconferenceonartificial
intelligence,volume32,2018.
PaulWHolland. Statisticsandcausalinference. JournaloftheAmericanstatisticalAssociation,81
(396):945–960,1986.
11Charles C Holt. Forecasting seasonals and trends by exponentially weighted moving averages.
Internationaljournalofforecasting,20(1):5–10,2004.
LingHuang,AnthonyDJoseph,BlaineNelson,BenjaminIPRubinstein,andJDougTygar. Ad-
versarialmachinelearning. InProceedingsofthe4thACMworkshoponSecurityandartificial
intelligence,pages43–58,2011.
SandyHuang,NicolasPapernot,IanGoodfellow,YanDuan,andPieterAbbeel. Adversarialattacks
onneuralnetworkpolicies. arXivpreprintarXiv:1702.02284,2017.
J.D.Hunter. Matplotlib: A2dgraphicsenvironment. ComputinginScience&Engineering,9(3):
90–95,2007. doi: 10.1109/MCSE.2007.55.
NickHuntington-Klein. Theeffect: Anintroductiontoresearchdesignandcausality. CRCPress,
2021.
RobJHyndman. Predictionintervalstoonarrow,Oct2014. URLhttps://robjhyndman.com/
hyndsight/narrow-pi/.
RobJHyndmanandGeorgeAthanasopoulos. Forecasting: principlesandpractice. OTexts,2018.
Shima Imani, Alireza Abdoli, and Eamonn Keogh. Time2cluster: Clustering time series using
neighborinformation. InTimeSeriesWorkshopatthe38thInternationalConferenceonMachine
Learning(ICML),2021.
GuidoWImbensandDonaldBRubin. Causalinferenceinstatistics,social,andbiomedicalsciences.
CambridgeUniversityPress,2015.
MatthewJohnson,KatjaHofmann,TimHutton,andDavidBignell. Themalmoplatformforartificial
intelligenceexperimentation. InIjcai,pages4246–4247,2016.
ScottJordan,YashChandak,DanielCohen,MengxueZhang,andPhilipThomas. Evaluatingthe
performance of reinforcement learning algorithms. In International Conference on Machine
Learning,pages4962–4973.PMLR,2020.
Hoki Kim. Torchattacks: A pytorch repository for adversarial attacks. arXiv preprint
arXiv:2010.01950,2020.
PangWeiKoh,ShioriSagawa,HenrikMarklund,SangMichaelXie,MarvinZhang,AkshayBal-
subramani,WeihuaHu,MichihiroYasunaga,RichardLanasPhillips,IrenaGao,etal. Wilds: A
benchmarkofin-the-wilddistributionshifts. InInternationalConferenceonMachineLearning,
pages5637–5664.PMLR,2021.
Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. arXiv preprint
arXiv:1705.06452,2017.
AlexeyKurakin,IanJGoodfellow,andSamyBengio. Adversarialmachinelearningatscale. In
InternationalConferenceonLearningRepresentations,2016.
Tabitha E Lee, Jialiang Alan Zhao, Amrita S Sawhney, Siddharth Girdhar, and Oliver Kroemer.
Causalreasoninginsimulationforstructureandtransferlearningofrobotmanipulationpolicies.
In2021IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages4776–4782.
IEEE,2021.
TabithaEdithLee,ShivamVats,SiddharthGirdhar,andOliverKroemer. SCALE:Causallearning
anddiscoveryofrobotmanipulationskillsusingsimulation. In7thAnnualConferenceonRobot
Learning,2023. URLhttps://openreview.net/forum?id=WmF-fagWdD.
Markus Löning, Franz Király, Tony Bagnall, Matthew Middlehurst, Sajaysurya Ganesh, George
Oastler,JasonLines,MartinWalter,ViktorKaz,LukaszMentel,chrisholder,LeonidasTsaprounis,
RNKuhns, Mirae Parker, Taiwo Owoseni, Patrick Rockenschaub, danbartl, jesellier, eenticott
shell,CiaranGilbert,GuzalBulatova,Lovkush,PatrickSchäfer,StanislavKhrapov,KatieBuch-
horn,KejsiTake,ShivanshSubramanian,SveaMarieMeyer,AidenRushbrooke,andBethrice.
sktime/sktime: v0.13.4,September2022. URLhttps://doi.org/10.5281/zenodo.7117735.
12Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. Policy poisoning in batch reinforcement
learningandcontrol. AdvancesinNeuralInformationProcessingSystems,32,2019.
RicardoPMasini,MarceloCMedeiros,andEduardoFMendes. Machinelearningadvancesfortime
seriesforecasting. Journalofeconomicsurveys,37(1):76–111,2023.
McKinsey and Company, Mar 2021. URL https://www.mckinsey.com/capabilities/
mckinsey-digital/how-we-help-clients/flying-across-the-sea-propelled-by-
ai.
ReuthMirsky,IgnacioCarlucho,ArrasyRahman,ElliotFosong,WilliamMacke,MohanSridharan,
Peter Stone, and Stefano V Albrecht. A survey of ad hoc teamwork research. In Multi-Agent
Systems: 19thEuropeanConference,EUMAS2022,Düsseldorf,Germany,September14–16,2022,
Proceedings,pages275–293.Springer,2022.
RahaMoraffah,ParasSheth,MansoorehKarami,AnchitBhattacharya,QianruWang,AniqueTahir,
AdrienneRaglin,andHuanLiu. Causalinferencefortimeseriesanalysis: Problems,methodsand
evaluation. KnowledgeandInformationSystems,63:3041–3085,2021.
AbdullahMueen,EamonnKeogh,QiangZhu,SydneyCash,andBrandonWestover. Exactdiscovery
oftimeseriesmotifs. InProceedingsofthe2009SIAMinternationalconferenceondatamining,
pages473–484.SIAM,2009.
AnayPattanaik,ZhenyiTang,ShuijingLiu,GauthamBommannan,andGirishChowdhary. Robust
deepreinforcementlearningwithadversarialattacks. arXivpreprintarXiv:1712.03632,2017.
JudeaPearl. Causality. Cambridgeuniversitypress,2009.
JonasPeters,DominikJanzing,andBernhardSchölkopf. Elementsofcausalinference: foundations
andlearningalgorithms. TheMITPress,2017.
JoaquinQuinonero-Candela,MasashiSugiyama,AntonSchwaighofer,andNeilDLawrence.Dataset
shiftinmachinelearning. MitPress,2008.
StephanRabanser,StephanGünnemann,andZacharyLipton. Failingloudly: Anempiricalstudyof
methodsfordetectingdatasetshift. AdvancesinNeuralInformationProcessingSystems,32,2019.
AntoninRaffin. Rlbaselines3zoo. https://github.com/DLR-RM/rl-baselines3-zoo,2020.
Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah
Dormann.Stable-baselines3:Reliablereinforcementlearningimplementations.JournalofMachine
LearningResearch,22(268):1–8,2021. URLhttp://jmlr.org/papers/v22/20-1364.html.
MuhammadARahman,NiklasHopner,FilipposChristianos,andStefanoVAlbrecht. Towardsopen
adhocteamworkusinggraph-basedpolicylearning. InInternationalConferenceonMachine
Learning,pages8776–8786.PMLR,2021.
AminRakhsha,GoranRadanovic,RatiDevidze,XiaojinZhu,andAdishSingla. Policyteaching
viaenvironmentpoisoning: Training-timeadversarialattacksagainstreinforcementlearning. In
InternationalConferenceonMachineLearning,pages7974–7984.PMLR,2020.
JohnSchulman,SergeyLevine,PieterAbbeel,MichaelJordan,andPhilippMoritz. Trustregion
policyoptimization. InInternationalconferenceonmachinelearning,pages1889–1897.PMLR,
2015.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriessche,
JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,etal. Mastering
thegameofgowithdeepneuralnetworksandtreesearch. nature,529(7587):484–489,2016.
13DavidSilver,ThomasHubert,JulianSchrittwieser,IoannisAntonoglou,MatthewLai,ArthurGuez,
MarcLanctot,LaurentSifre,DharshanKumaran,ThoreGraepel,etal. Ageneralreinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):
1140–1144,2018.
PeterSpirtes,ClarkNGlymour,andRichardScheines. Causation,prediction,andsearch. MIT
press,2000.
PeterStone,GalKaminka,SaritKraus,andJeffreyRosenschein. Adhocautonomousagentteams:
Collaboration without pre-coordination. In Proceedings of the AAAI Conference on Artificial
Intelligence,volume24,pages1504–1509,2010.
PulkitVermaandSiddharthSrivastava. Learningcausalmodelsofautonomousagentsusinginterven-
tions. arXivpreprintarXiv:2108.09586,2021.
Pulkit Verma, Rushang Karia, and Siddharth Srivastava. Autonomous capability assessment of
sequential decision-making systems in stochastic settings. In Thirty-seventh Conference on
Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=
VqclD6Nfaj.
OriolVinyals,IgorBabuschkin,WojciechMCzarnecki,MichaëlMathieu,AndrewDudzik,Junyoung
Chung,DavidHChoi,RichardPowell,TimoEwalds,PetkoGeorgiev,etal. Grandmasterlevelin
starcraftiiusingmulti-agentreinforcementlearning. Nature,575(7782):350–354,2019.
MichaelL.Waskom. seaborn: statisticaldatavisualization. JournalofOpenSourceSoftware,6(60):
3021,2021. doi: 10.21105/joss.03021. URLhttps://doi.org/10.21105/joss.03021.
WhiteHouse. Aguidebooktothebipartisaninfrastructurelawforstate,local,tribal,andterritorial
governments,andotherpartners. WhiteHouse,2022. URLhttps://www.whitehouse.gov/wp-
content/uploads/2022/05/BUILDING-A-BETTER-AMERICA-V2.pdf. [AccessedNovember
2,2023].
JorgDWichardandMaciejOgorzalek. Timeseriespredictionwithensemblemodels. In2004IEEE
internationaljointconferenceonneuralnetworks(IEEECat.No.04CH37541),volume2,pages
1625–1630.IEEE,2004.
Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overfitting in deep
reinforcementlearning. arXivpreprintarXiv:1804.06893,2018.
Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks
againstreinforcementlearning. InInternationalConferenceonMachineLearning,pages11225–
11234.PMLR,2020.
14Appendix
A Methods
WeprovidethefollowingdefinitionofRLagenttime-seriesperformancemeasurements,similarto
howperformanceismeasuredinRLtrainingresearch,thatwillberelevantinthenextsubsection:
Definition 1. Let XA(t) the time series performance for t = 1...N, where A is an RL agent
in a simulated deterministic environment M. For each XA(t), there is an associated sample of
performancemeasurementsxA(t)=(xA,1(t),...,xA,M(t)). Here,theiinxA,i(t)isarandomseed
i∈S,whereSisafinitesetofM randomseeds. Hence,wedefine:
(cid:80) xA,i(t)
XA(t)=E[xA,i(t)]= i∈S (3)
M
whichistheexpectedperformance3attimetoveralltherandomseedsinS.
A.1 ComparisonofSimpleTimeSeriesForecastingTrends
TocompareRLalgorithmsattesttime,wecomparetheperformanceforecastsofeachRLagent. This
recommendationisinspiredbytheuseoftimecomplexityorrunningtimetomeasuretheefficiency
ofanalgorithm[Cormenetal.,2022]. AsillustratedinFigure1,lookingatperformancetrendsofRL
agentsisaninformativewaytoobserveoverfittingduringtesttime. Here,weusesimpletimeseries
forecastsovermorecomplextimeseriesmodelsthatmightbetterpredicttheperformancetrend. The
reasonisthatonecannotmaketoomanystrong,generalassumptions(e.g.,seasonality,howmanypast
valuestouseforautoregression)onthetimeseriestrendofagentperformanceinallenvironments.
Fortest-timedistributionshift,weonlyassumethetrendwillbepessimisticandnotincrease. Thisis
reasonableifweassumetheidealcaseiswhenperformanceneverdecreases(likeAgent1inFigure
1). WedefaulttousingHolt’slineardampedtrendmethod[GardnerJrandMcKenzie,1985,Holt,
2004,HyndmanandAthanasopoulos,2018]tomodelthetrendofagentperformancewheninthe
presenceofdistributionshift. Thedampingparameterdiscouragesconstantincreasingtrends,which
willlikelygivemoreaccurateforecaststhanHolt’smethodwithoutdamping. Researchersevaluating
theirownexperiments,however,mayfindthatmorecomplexmodelswouldbemoreappropriatefor
theirownstudiesiftheyseeperformancetrendsthatwarrantsuchassumptions.
Here,wedefineHolt’slineardampedtrendmethod[HyndmanandAthanasopoulos,2018]:
Letyˆ betheshort-handestimatefory basedondatay ...y . Thetermℓ istheestimate
t+h|t t+h 1 t t
oftheleveloftheseriesattimetwithasmoothingparameterαsuchthat0 ≤ α ≤ 1. Theterm
b istheestimateofthetrend(slope)oftheseriesattimetwithasmoothingparameterβ∗ such
t
that0 ≤ β∗ ≤ 1. Letϕbethedampingparametersuchthat0 < ϕ < 1. Thenthetrendmethodis
representedastheforecastequationandsmoothingequationsbelow:
yˆ =ℓ +(ϕ+ϕ2+···+ϕh)b
t+h|t t t
ℓ =αy +(1−α)(ℓ +ϕb )
t t t−1 t−1
b =β∗(ℓ −ℓ )+(1−β∗)ϕb
t t t−1 t−1
A.2 PredictionIntervalsoverFuturePerformance
Alongwithtimeseriesforecasts, werecommendpredictionintervalsoverfutureaveragereturns
[HyndmanandAthanasopoulos,2018]. Thecombinationofsimpletimeseriesforecastsandpredic-
tionintervalsmapouttherangeofaveragereturnsovertime. Asstatedpreviously,pointestimates
withconfidenceintervalsarenotenoughtocapturetherangeofreturnsovertime. Predictionintervals
actasacomplimenttothetimeseriesforecastingbyvisualizingtheuncertaintyofthepossiblefuture
averagereturns. Whenapplyingbothofthesemethods,wecanspecifythemostrobustRLalgorithm
asthetrendwiththemostoptimisticforecastonperformanceandthesmallestpredictioninterval.
Here,weassumethereexistsdistributionshiftsbutdonotnecessarilyknowwhenorwhere.
3Weuseexpectedperformancejustasasimpledefault. Forexample,inaccordancewithAgarwaletal.
[2021],onecouldreplaceexpectedperformancewiththeinterquartilemean.
15A.3 Difference-in-differencesAnalysisforRLPerformance
Difference-in-differences(DiD)measuresthecausaleffectbetweenatreatmentandcontrolgroup,
wherethetreatmentgroupisexposedtoaninterventionatacertainpointintime[Cunningham,2021,
Huntington-Klein,2021]. Intuitively,itrepresentshowmuchmorethetreatmentgroupwasaffected
bytheinterventionatsometimetcomparedtothechangeoftheunaffectedcontrolgroupatthesame
timet. Sincewearedealingwithtimeseries,wewilladheretotheDiDformulationinMoraffah
etal.[2021]:
Definition2. Ift<T andt>T denotethepre-andpost-treatmentperiods,respectively,thenwe
cancalculatetheDiDmeasureusingtheaveragetreatmenteffectmetricoveratimeseriesX(t)as
follows:
DiD ={E[X(t>T)|G=1]−E[X(t<T)|G=1]}−{E[X(t>T)|G=0]−E[X(t<T)|G=0]}
(4)
whereGindicatesthetreatmentgroup(G=1)andthecontrolgroup(G=0).
AnymethodologythatreliesonDiDmustsatisfytheparalleltrendsassumption,whichsaysthatif
notreatmenthadoccurred,thedifferencebetweenthetreatedgroupandtheuntreatedgroupwould
havestayedthesameinthepost-treatmentperiodasitwasinthepre-treatmentperiod[Huntington-
Klein,2021]. RLagentsindeterministicenvironmentswithfixedseedstriviallysatisfythisbecause
theagentwilltakethesameactionateachstateandreceivethesamerewardregardlessofhowmany
timestheevaluationisrepeated.
From the RL fixed seed assumption, DiD simplifies to the equation below when evaluating RL
performance:
DiD =E[XA(t>T)|G=1]−E[XA(t>T)|G=0] (5)
whereAistheRLagentbeingevaluated. Thisfollowsfromthepre-treatmentaveragescanceling
eachotherout. Hence,weonlyneedtomeasurethepost-treatmenteffectofRLperformance. Using
theRLfixedseedassumption,DiDbecomesthefollowing:
DiD =E[XA(t>T)|G=1]−E[XA (t>T)|G=1] (6)
U=0
Equations5and6saythefollowing: MeasuringtheDiDeffectisequivalenttomeasuringtheaverage
timeseriespost-treatmenteffectofagentAbetweenthetreatmentandcontrolgroup. Iftheagents
inboththetreatmentandcontrolgroupshavethesamefixedrandomseeds, wecaninterpretthe
performancemeasurementsofthecontrolgroupasthecounterfactualofthetreatmentgroupasifthe
treatmentgroupwasneverexposedtothedistributionshiftintervention. Hence,wehaveshownthat
theRLfixedseedassumptionjustifiescausalinferenceinourtimeseriesanalysis.
B FurtherNotesontheRLFixedSeedAssumption
Reproducibility can be difficult to achieve when running on a GPU. As described in PyTorch’s
webpage on reproducibility, even identical seeds might not provide reproducible results. Some
reasons include nondeterministic algorithms that improve performance and the use of different
hardwarecanaffecttheselectionofsuchalgorithms. Tocontrolthesesourcesofrandomnessinour
experiments,weadheretothereproducibilitysuggestionsprovidedonthewebpage.
C FurtherNotesonEnvironmentsUsedinEvaluations
Allplotsuse10randomseeds. Inmulti-agentsettings,eachagentsharesthesameseedduringan
evaluationrun. Forexample,ifourseednumbersare1and42,thenthefirstevaluationrunsets
C.1 AtariGames
ThegamesoffocusareasubsetofAtarigamesAsteroidsNoFrameskip-v4,BeamRiderNoFrameskip-
v4, BreakoutNoFrameskip-v4, MsPacmanNoFrameskip-v4, PongNoFrameskip-v4,
16QbertNoFrameskip-v4, RoadRunnerNoFrameskip-v4, SeaquestNoFrameskip-v4, and
SpaceInvadersNoFrameskip-v4. The agents we evaluate are pretrained agents from RL
Baselines3Zoo[Raffin,2020]thatareavailableonSB3’sHuggingfacerepositoryofmodels. The
adversarialattacks(FGSM)wereimplementedintorchattacks[Kim,2020].
We evaluate each agent over 100 games per Atari game. In the causal impact plots, the attacker
intervenesatthe50thgame. Here,sincegamescoresaccumulateacrosslives,wedefineepisodesas
thelifeoftheplayer. Forexample,Pongonlygivesonelife,whichgivesusatotalof100episodes.
InQbert,theplayerisgiven4lives. Hence,theattackerintervenesatepisode200because(50games)
*(4episodes/game)=200episodes(orlives). Thisreasoningisalsowhythereare400episodesin
theQbertexperiments. Thisstrategydeliberatelyavoidsthenoisyartifactsthatcanemergefromthe
timeseriesdatawhenaccumulatedperformancesuddenlydropswheneachgameends.
C.2 PowerGridworld
PowerGridworldisamodular,customizableframeworkforbuildingpowersystemsenvironments
totrainRLagents. Becauseofthis,weuseanenvironmentprovidedinoneoftheexamplescripts.
TheclassnameiscalledCoordinatedMultiBuildingControlEnv,whichisamulti-agentcoor-
dinationenvironment. Inadditiontotheoriginalagent-levelreward,grid-levelreward/penaltyand
system-levelconstraint(s)areconsidered. Inparticular,weconsiderthevoltageconstraints: agents
needtocoordinatesothecommonbusvoltageiswithintheANSIC.84.1limit. Iftheconstraintsare
notsatisfied,thevoltageviolationpenaltywillbesharedbyallagents. Theagentsinthisscenarioare
minimalimplementations[Barhate,2021]ofPPO.
C.3 TimeSeriesTools
TrendsandpredictionintervalswereimplementedinthePythonpackagesktime[Löningetal.,2022].
OtherPythonvisualizationtoolsincludeMatplotlib[Hunter,2007]andSeaborn[Waskom,2021].
D MorePlots
D.1 AtariGameCausalImpactPlots
Agent: a2c Env: AsteroidsNoFrameskip-v4 Agent: ppo Env: AsteroidsNoFrameskip-v4
10 30
20
5
10
5 2.5
0
0.0
5
2.5
100 0
0
100 500
0 50 100 150 200 250 300 350 0 50 100 150 200 250 300 350 400
Episode Episode
Counterfactual Epsilon 0.05 Epsilon 0.5 Counterfactual Epsilon 0.05 Epsilon 0.5
Epsilon 0.01 Epsilon 0.1 Epsilon 1.0 Epsilon 0.01 Epsilon 0.1 Epsilon 1.0
17
snruteR
egarevA
esiwtnioP
evitalumuC
lanigirO
snruteR
egarevA
evitalumuC
lanigirO
esiwtnioPAgent: a2c Env: BeamRiderNoFrameskip-v4 Agent: ppo Env: BeamRiderNoFrameskip-v4
50 50
25 25
0 0
0 0
20 20
0 0
1000
2000 2000
0 50 100 150 200 250 300 0 50 100 150 200 250 300
Episode Episode
Counterfactual Epsilon 0.05 Epsilon 0.5 Counterfactual Epsilon 0.05 Epsilon 0.5
Epsilon 0.01 Epsilon 0.1 Epsilon 1.0 Epsilon 0.01 Epsilon 0.1 Epsilon 1.0
Agent: a2c Env: BreakoutNoFrameskip-v4 Agent: ppo Env: BreakoutNoFrameskip-v4
100 100
50 50
0 0
0 0
20 20
0 0
2000
2000
4000
0 100 200 300 400 0 100 200 300 400
Episode Episode
Counterfactual Epsilon 0.05 Epsilon 0.5 Counterfactual Epsilon 0.05 Epsilon 0.5
Epsilon 0.01 Epsilon 0.1 Epsilon 1.0 Epsilon 0.01 Epsilon 0.1 Epsilon 1.0
Agent: a2c Env: MsPacmanNoFrameskip-v4 Agent: ppo Env: MsPacmanNoFrameskip-v4
100
100
50 50
0
0
20 20
0 0
1000 1000
2000
2000
0 50 100 150 200 250 0 50 100 150 200 250
Episode Episode
Counterfactual Epsilon 0.05 Epsilon 0.5 Counterfactual Epsilon 0.05 Epsilon 0.5
Epsilon 0.01 Epsilon 0.1 Epsilon 1.0 Epsilon 0.01 Epsilon 0.1 Epsilon 1.0
Agent: a2c Env: PongNoFrameskip-v4 Agent: ppo Env: PongNoFrameskip-v4
20 20
0 0
20 20
0 0
20 20
40 40
0 0
1000 1000
0 20 40 60 80 0 20 40 60 80
Episode Episode
Counterfactual Epsilon 0.05 Epsilon 0.5 Counterfactual Epsilon 0.05 Epsilon 0.5
Epsilon 0.01 Epsilon 0.1 Epsilon 1.0 Epsilon 0.01 Epsilon 0.1 Epsilon 1.0
18
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
evitalumuC
evitalumuC
evitalumuC
evitalumuC
esiwtnioP
lanigirO
esiwtnioP
lanigirO
esiwtnioP
lanigirO
esiwtnioP
lanigirO
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
evitalumuC
evitalumuC
evitalumuC
evitalumuC
esiwtnioP
lanigirO
esiwtnioP
lanigirO
esiwtnioP
lanigirO
esiwtnioP
lanigirOAgent: a2c Env: QbertNoFrameskip-v4 Agent: ppo Env: QbertNoFrameskip-v4
200 50
25 100
0 0
0
0
10 50
0
0
5000
2000 10000
0 50 100 150 200 250 300 350 0 50 100 150 200 250 300 350
Episode Episode
Counterfactual Epsilon 0.05 Epsilon 0.5 Counterfactual Epsilon 0.05 Epsilon 0.5
Epsilon 0.01 Epsilon 0.1 Epsilon 1.0 Epsilon 0.01 Epsilon 0.1 Epsilon 1.0
Agent: a2c Env: RoadRunnerNoFrameskip-v4 Agent: ppo Env: RoadRunnerNoFrameskip-v4
10
5 5
0 0
2 0
0 2
0
100
200
0
400
0 50 100 150 200 250 0 50 100 150 200 250
Episode Episode
Counterfactual Epsilon 0.05 Epsilon 0.5 Counterfactual Epsilon 0.05 Epsilon 0.5
Epsilon 0.01 Epsilon 0.1 Epsilon 1.0 Epsilon 0.01 Epsilon 0.1 Epsilon 1.0
Agent: a2c Env: SeaquestNoFrameskip-v4 Agent: ppo Env: SeaquestNoFrameskip-v4
20 20
10 10
0 0
10 10
0 0
2000 2000
0 50 100 150 200 250 300 350 0 50 100 150 200 250 300 350
Episode Episode
Counterfactual Epsilon 0.05 Epsilon 0.5 Counterfactual Epsilon 0.05 Epsilon 0.5
Epsilon 0.01 Epsilon 0.1 Epsilon 1.0 Epsilon 0.01 Epsilon 0.1 Epsilon 1.0
Agent: a2c Env: SpaceInvadersNoFrameskip-v4 Agent: ppo Env: SpaceInvadersNoFrameskip-v4
40
50
20
0
0 0
20
20
0 0
1000
1000 2000
0 25 50 75 100 125 150 175 200 0 50 100 150 200
Episode Episode
Counterfactual Epsilon 0.05 Epsilon 0.5 Counterfactual Epsilon 0.05 Epsilon 0.5
Epsilon 0.01 Epsilon 0.1 Epsilon 1.0 Epsilon 0.01 Epsilon 0.1 Epsilon 1.0
19
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
evitalumuC
evitalumuC
evitalumuC
evitalumuC
esiwtnioP
lanigirO
esiwtnioP
esiwtnioP
esiwtnioP
lanigirO
lanigirO
lanigirO
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
evitalumuC
evitalumuC
evitalumuC
evitalumuC
lanigirO
esiwtnioP
lanigirO
esiwtnioP
lanigirO
esiwtnioP
lanigirO
esiwtnioPD.2 ObservationalPlots
Allobservationalplotsshowforecastswithpredictionintervals100episodesafterthelastmeasure-
ment.
D.2.1 PowerGridworld
PGW Prediction Trends by Switching with Pretrained Agents PGW Prediction Trends by Switching with Untrained Agents
180 Group performance with 1 agent switched Group performance with 1 agent switched
G Gr ro ou up p p Pee rr ff oo rr mm aa nn cc ee ww ii tt hh 32 aa gg ee nn tt ss ss ww ii tt cc hh ee dd 2000 G Gr ro ou up p p Pee rr ff oo rr mm aa nn cc ee ww ii tt hh 32 aa gg ee nn tt ss ss ww ii tt cc hh ee dd
200 Start of prediction trend lines Start of prediction trend lines
0
220
2000
240
4000
260
6000
280
8000
300
10000
0 50 100 150 200 250 300 350 400 0 50 100 150 200 250 300 350 400
Episode Episode
Figure6: PowerGridworldobservationalplots. Left: Comparingrandomswitchingwithpretrained
agentsateachepisode. Right: Comparingrandomswitchingwithuntrainedagentedateachepisode.
D.2.2 AtariGames
Prediction Trends in AsteroidsNoFrameskip-v4 with FGSM(=0.01) Prediction Trends in AsteroidsNoFrameskip-v4 with FGSM(=0.05)
16
16
14
14
12
12
10
10
8
8
6
6
4
4 A2C A2C
PPO 2 PPO
2 Start of prediction trend lines Start of prediction trend lines
0 100 200 300 400 500 0 100 200 300 400 500
Episode Episode
Prediction Trends in AsteroidsNoFrameskip-v4 with FGSM(=0.10) Prediction Trends in AsteroidsNoFrameskip-v4 with FGSM(=0.50)
17.5 16
15.0 14
12.5 12
10.0 10
8
7.5
6
5.0
4
2.5 A2C A2C
0.0 P SP taO rt of prediction trend lines 2 P SP taO rt of prediction trend lines
0 100 200 300 400 500 0 100 200 300 400 500
Episode Episode
Prediction Trends in AsteroidsNoFrameskip-v4 with FGSM(=1.00)
16
14
12
10
8
6
4
2 A2C
PPO
0 Start of prediction trend lines
0 100 200 300 400 500
Episode
20
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevAPrediction Trends in BeamRiderNoFrameskip-v4 with FGSM(=0.01) Prediction Trends in BeamRiderNoFrameskip-v4 with FGSM(=0.05)
30
27.5
25.0 25
22.5
20.0 20
17.5
15
15.0
12.5
A2C 10 A2C
10.0 PPO PPO
Start of prediction trend lines Start of prediction trend lines
0 50 100 150 200 250 300 350 400 0 50 100 150 200 250 300 350 400
Episode Episode
Prediction Trends in BeamRiderNoFrameskip-v4 with FGSM(=0.10) Prediction Trends in BeamRiderNoFrameskip-v4 with FGSM(=0.50)
30 30
25 25
20
20
15
15
10
10
A2C A2C
5 PPO PPO
Start of prediction trend lines 5 Start of prediction trend lines
0 50 100 150 200 250 300 350 400 0 50 100 150 200 250 300 350 400
Episode Episode
Prediction Trends in BeamRiderNoFrameskip-v4 with FGSM(=1.00)
30
25
20
15
A2C
10 P SP taO rt of prediction trend lines
0 50 100 150 200 250 300 350 400
Episode
Prediction Trends in BreakoutNoFrameskip-v4 with FGSM(=0.01) Prediction Trends in BreakoutNoFrameskip-v4 with FGSM(=0.05)
60
60
40 40
20 20
0 0
20 A PP2 OC A PP2 OC
Start of prediction trend lines 20 Start of prediction trend lines
0 100 200 300 400 500 600 0 100 200 300 400 500 600
Episode Episode
Prediction Trends in BreakoutNoFrameskip-v4 with FGSM(=0.10) Prediction Trends in BreakoutNoFrameskip-v4 with FGSM(=0.50)
40
40
30
30
20
20
10
10
A PP2 OC 0 A PP2 OC
0 Start of prediction trend lines Start of prediction trend lines
0 100 200 300 400 500 600 0 100 200 300 400 500 600
Episode Episode
Prediction Trends in BreakoutNoFrameskip-v4 with FGSM(=1.00)
40
30
20
10
A2C
PPO
Start of prediction trend lines
0 100 200 300 400 500 600
Episode
21
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevAPrediction Trends in MsPacmanNoFrameskip-v4 with FGSM(=0.01) Prediction Trends in MsPacmanNoFrameskip-v4 with FGSM(=0.05)
100 A PP2 OC 100 A PP2 OC
90 Start of prediction trend lines 90 Start of prediction trend lines
80 80
70 70
60 60
50 50
40 40
30 30
0 50 100 150 200 250 300 350 400 0 50 100 150 200 250 300 350 400
Episode Episode
Prediction Trends in MsPacmanNoFrameskip-v4 with FGSM(=0.10) Prediction Trends in MsPacmanNoFrameskip-v4 with FGSM(=0.50)
100 A PP2 OC 100 A PP2 OC
Start of prediction trend lines Start of prediction trend lines
90 90
80 80
70 70
60 60
50 50
40 40
30 30
0 50 100 150 200 250 300 350 400 0 50 100 150 200 250 300 350 400
Episode Episode
Prediction Trends in MsPacmanNoFrameskip-v4 with FGSM(=1.00)
100 A PP2 OC
Start of prediction trend lines
90
80
70
60
50
40
30
0 50 100 150 200 250 300 350 400
Episode
Prediction Trends in PongNoFrameskip-v4 with FGSM(=0.01) Prediction Trends in PongNoFrameskip-v4 with FGSM(=0.05)
21 21
20
20
19
19 A PP2 OC 18
Start of prediction trend lines
17
18
16
17 15 A PP2 OC
Start of prediction trend lines
0 25 50 75 100 125 150 175 200 14 0 25 50 75 100 125 150 175 200
Episode Episode
Prediction Trends in PongNoFrameskip-v4 with FGSM(=0.10) Prediction Trends in PongNoFrameskip-v4 with FGSM(=0.50)
21 21
20 20
19
19
18
18
17
17
16
16 15
15 A PP2 OC 14 A PP2 OC
Start of prediction trend lines Start of prediction trend lines
14 0 25 50 75 100 125 150 175 200 13 0 25 50 75 100 125 150 175 200
Episode Episode
Prediction Trends in PongNoFrameskip-v4 with FGSM(=1.00)
21
20
19
18
17
16
15
A2C
14 PPO
Start of prediction trend lines
0 25 50 75 100 125 150 175 200
Episode
22
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevAPrediction Trends in QbertNoFrameskip-v4 with FGSM(=0.01) Prediction Trends in QbertNoFrameskip-v4 with FGSM(=0.05)
250 A2C A2C
PPO 140 PPO
200 Start of prediction trend lines Start of prediction trend lines
120
150
100
100
80
50
60
0
40
50
20
100
0
150
0 100 200 300 400 500 0 100 200 300 400 500
Episode Episode
Prediction Trends in QbertNoFrameskip-v4 with FGSM(=0.10) Prediction Trends in QbertNoFrameskip-v4 with FGSM(=0.50)
300 A PP2 OC 140
Start of prediction trend lines
120
200 100
100 80
60
0
40
100 20
0 A PP2 OC
200 20 Start of prediction trend lines
0 100 200 300 400 500 0 100 200 300 400 500
Episode Episode
Prediction Trends in QbertNoFrameskip-v4 with FGSM(=1.00)
150
100
50
0
50 A PP2 OC
Start of prediction trend lines
0 100 200 300 400 500
Episode
Prediction Trends in RoadRunnerNoFrameskip-v4 with FGSM(=0.01) Prediction Trends in RoadRunnerNoFrameskip-v4 with FGSM(=0.05)
A2C
9 PPO
Start of prediction trend lines
8 8
7
6
6
5 4
4
3 2 A2C
PPO
2 Start of prediction trend lines
0 50 100 150 200 250 300 350 400 0 0 50 100 150 200 250 300 350 400
Episode Episode
Prediction Trends in RoadRunnerNoFrameskip-v4 with FGSM(=0.10) Prediction Trends in RoadRunnerNoFrameskip-v4 with FGSM(=0.50)
A2C A2C
PPO PPO
Start of prediction trend lines Start of prediction trend lines
8 8
6 6
4
4
2
2
0 50 100 150 200 250 300 350 400 0 50 100 150 200 250 300 350 400
Episode Episode
Prediction Trends in RoadRunnerNoFrameskip-v4 with FGSM(=1.00)
A2C
PPO
Start of prediction trend lines
8
6
4
2
0 50 100 150 200 250 300 350 400
Episode
23
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevAPrediction Trends in SeaquestNoFrameskip-v4 with FGSM(=0.01) Prediction Trends in SeaquestNoFrameskip-v4 with FGSM(=0.05)
25
A PP2 OC 26 A PP2 OC
24 Start of prediction trend lines 25 Start of prediction trend lines
24
23
23
22 22
21
21
20
20 19
0 100 200 300 400 500 0 100 200 300 400 500
Episode Episode
Prediction Trends in SeaquestNoFrameskip-v4 with FGSM(=0.10) Prediction Trends in SeaquestNoFrameskip-v4 with FGSM(=0.50)
A2C A2C
PPO 25 PPO
23.0 Start of prediction trend lines Start of prediction trend lines
24
22.5
23
22.0
22
21.5
21
21.0
20
20.5 19
0 100 200 300 400 500 0 100 200 300 400 500
Episode Episode
Prediction Trends in SeaquestNoFrameskip-v4 with FGSM(=1.00)
A2C
24.0 PPO
Start of prediction trend lines
23.5
23.0
22.5
22.0
21.5
21.0
20.5
0 100 200 300 400 500
Episode
Prediction Trends in SpaceInvadersNoFrameskip-v4 with FGSM(=0.01) Prediction Trends in SpaceInvadersNoFrameskip-v4 with FGSM(=0.05)
60 A2C 50 A2C
50 P SP taO rt of prediction trend lines P SP taO rt of prediction trend lines
40 40
30
30
20
10 20
0
10 10
20 0
0 50 100 150 200 250 0 50 100 150 200 250 300
Episode Episode
Prediction Trends in SpaceInvadersNoFrameskip-v4 with FGSM(=0.10) Prediction Trends in SpaceInvadersNoFrameskip-v4 with FGSM(=0.50)
60 A2C
PPO 35
50 Start of prediction trend lines
30
40
25
30 20
20 15
10 10
0 5 A2C
0 PPO
10 Start of prediction trend lines
0 50 100 150 200 250 300 0 50 100 150 200 250
Episode Episode
Prediction Trends in SpaceInvadersNoFrameskip-v4 with FGSM(=1.00)
60 A2C
PPO
50 Start of prediction trend lines
40
30
20
10
0
10
0 50 100 150 200 250 300
Episode
24
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevA
snruteR
egarevAE HyperparametersforPowerGridworldPPOTraining
NumberofTrainingEpisodes: 2,000,000
MaximumNumberofStepsperEpisode: 1000
StartingStandardDeviationforActionDistribution: 1.0
StandardDeviationforActionDistributionDecayRate: 0.05
MinimumStandardDeviationforActionDistribution: 0.1
StandardDeviationDecayFrequencyintimesteps: 250,000
Policy: 2-layerMultilayerPerceptron
HiddenUnits: 128perlayer
Policyisupdatedevery2000timesteps. Duringthattimestep,thepolicyupdated5epochs.
Clipparameter: 0.2
Discountfactorγ: 0.99
ActorLearningRate: 0.0003
CriticLearningRate: 0.001
Maingrouprandomseed: 0
Outsidegrouprandomseed: 1
25