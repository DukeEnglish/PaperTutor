Linear-time Minimum Bayes Risk Decoding with Reference Aggregation
JannisVamvas and RicoSennrich
DepartmentofComputationalLinguistics,UniversityofZurich
{vamvas,sennrich}@cl.uzh.ch
Abstract wecombinerepresentationsofthereferencesinto
an aggregate reference representation, which we
Minimum Bayes Risk (MBR) decoding is a
then use for utility estimation. Our proposed ap-
textgenerationtechniquethathasbeenshown
proximation still relies on MC sampling, but on
toimprovethequalityofmachinetranslations,
alowerlevel: RatherthancomputinganMCesti-
butisexpensive,evenifasampling-basedap-
mate of the expected utility, we compute an MC
proximationisused. Besidesrequiringalarge
numberofsampledsequences,itrequiresthe estimate of the “true” reference representation in
pairwisecalculationofautilitymetric,which thefeaturespaceofthegivenutilitymetric. Since
has quadratic complexity. In this paper, we thisestimateonlyneedstobecomputedonce,our
proposetoapproximatepairwisemetricscores approach has linear complexity in the number of
withscorescalculatedagainstaggregatedrefer-
sampledhypothesesandreferences.
ence representations. This changes the com-
Wereportempiricalresultsforfourtranslation
plexity of utility estimation from O(n2) to
O(n), while empirically preserving most of
directionsandtwoutilitymetrics: CHRF (Popovic´,
thequalitygainsofMBRdecoding. Werelease 2015),whichisbasedoncharactern-gramoverlap,
oursourcecode.1 and COMET (Reietal.,2020),aneuralnetwork
trained with examples of human translation qual-
1 Introduction ity judgments. For CHRF, we find that reference
aggregation reduces the time needed for comput-
Theideaofgeneratingtranslationsbymaximizing
ingtheutilityof1024samplesby99.5%,without
ametricoftranslationquality(KumarandByrne,
affectingtranslationquality. ForCOMET,metric
2004) has recently been revived in the context
accuracydoesdecreasewithaggregation,buttoa
ofneuralmachinetranslation. Insampling-based
lesserextentthanwithsimplyreducingthenumber
MBRdecoding(EikemaandAziz,2020),manyhy-
ofreferences. DependingontheCOMET model,
pothesesaresampledfromthemodeldistribution,
computation time is reduced by 95–99%, which
andtheirexpectedutilityisestimatedusingMonte
makes reference aggregation an efficient method
Carlo (MC) sampling. This approach has been
forhypothesispruningwith COMET.
showntoimprovetranslationqualitycomparedto
beam search, especially when neural metrics are
2 BackgroundandRelatedWork
usedforutilityestimation(Freitagetal.,2022).
Estimating utility through MC sampling has Sampling-based MBR (Eikema and Aziz, 2020)
quadratic complexity in the number of samples, selectsatranslationhyp∗ outofasetoftranslation
whichlimitspracticalapplication. Previouswork hypotheses hyp ,...,hyp ∈ hyps by maximiz-
1 n
suggested pruning the number of samples based ing(expected)utility:
onacheapermetricorasmallernumberofrefer-
ences(EikemaandAziz,2022;ChengandVlachos, hyp∗ = argmaxutility(hyp). (1)
2023). In this paper, we propose reference ag- hyp∈hyps
gregation,analternativeefficiencytechniquethat
Thesetofhypothesesissampledfromthemodel
exploitsthefactthatmostcommonmetricsrepre-
distribution p(hyp|src). Eikema and Aziz (2020)
sent text sequences in averageable form, e.g., as
proposetoapproximatetheutilityusingMCsam-
n-gram statistics or as embeddings. Specifically,
pling: sample a set of pseudo-references refs =
1https://github.com/ZurichNLP/mbr {ref ,...,ref } ∼ p(ref|src)fromthemodeland
1 m
4202
beF
6
]LC.sc[
1v15240.2042:viXracalculateametricagainsteachsampledreference: “true”reference,whichwethenuseforapproximat-
ingtheexpectedutilityofeachsampledhypothe-
1 (cid:88)
utility(hyp) ≈ metric(hyp,ref). (2) sis. Importantly,thecomputationalcomplexityof
m
ref∈refs our approach is in O(|hyps| + |refs|) rather than
O(|hyps|·|refs|);seeAppendixDforadiscussion.
Formachinetranslation,typicalsuchmetricsare
CHRF (Popovic´,2015)andBLEU(Papinenietal.,
3.1 ApplicationtochrFMetric
2002), which are based on n-gram statistics, or
CHRF (Popovic´, 2015) is defined as an F-score
neuralmetricssuchas COMET (Reietal.,2020)
overcharactern-grams:
and BLEURT (Sellametal.,2020).
Alineofresearchhasfocusedonimprovingthe
(1+β2)·CHRP·CHRR
efficiency of sampling-based MBR. Eikema and
CHRF
β
=
β2·CHRP+CHRR
, (5)
Aziz (2022) propose coarse-to-fine MBR, which where
prunesthehypothesesbasedonacheapermetric, |hyp∩ref| |hyp∩ref|
andN-by-SMBR,whichusesfewerreferencesthan
CHRP = and CHRR = ,
|hyp| |ref|
hypotheses. Cheng and Vlachos (2023) propose
and the parameter β controls the relative impor-
confidence-basedpruning,wherethenumberofhy-
tanceofprecisionandrecall. Therepresentations
pothesesisiterativelyreducedbasedonanincreas-
hypandref arebagsofn-grams,i.e.,objectsthat
ingnumberofreferences. JinnaiandAriu(2024)
mapeachn-gramtoitscountinthestring.
interpret sampling-based MBR as an instance of
WeapplyreferenceaggregationtoCHRFbyav-
medoididentificationandapplyanestablishedap-
eragingthecountsofn-gramsacrossallreferences:
proximationalgorithmtothisproblem. Finally,a
1 (cid:93)
lineofworkusesMBRoutputsasatrainingreward, ref = ref, (6)
m
avoiding the inefficiency of MBR during deploy-
ref∈refs
ment(Finkelsteinetal.,2023;Yangetal.,2023). (cid:85)
where is an operation that sums up the counts
3 ReferenceAggregation of each n-gram. We then approximate the ex-
pected utility of a hypothesis by calculating
Ourapproachisbasedontheobservationthatmost CHRF β(hyp,ref). AppendixAprovidesamorefor-
metricsthatarecommonlyusedforMBRmakeuse
maldefinitionofreferenceaggregationfor CHRF.
of feature representations that can be aggregated.
Forexample,then-gramstatisticsusedby CHRF 3.2 ApplicationtoCOMETMetric
canbeaggregatedbyaveragingthecountsofthe COMET (Reietal.,2020)isapre-trainedTrans-
n-grams across all references; and the sentence formermodel(Vaswanietal.,2017)thathasbeen
embeddingsusedby COMET canbeaggregated fine-tunedtopredicthumanjudgmentsoftransla-
bycalculatinganaveragesentenceembedding. tionquality. Inthispaper,wefocusontheEstima-
For simplicity, we re-use the above notation, tormodelarchitecture,whichdirectlyestimatesa
where hyp is a hypothesis and ref is a reference, quality score given a hypothesis, a reference and
but we now assume that they are represented in thesourcesequence. COMETseparatelyencodes
anaverageableform. Wethencombinethesetof thesethreeinputsintofixed-sizeembeddings:
referencesrefsintoanaggregaterepresentationref:
hyp, ref, src = emb(hyp), emb(ref), emb(src).
1 (cid:88)
ref = ref. (3) The three embeddings are then fed into a feed-
m
ref∈refs forwardmodule,whichoutputsascalarscore:
Weapproximatetheexpectedtheutilityofasam- comet(hyp) = score(hyp, ref, src). (7)
pledhypothesisbycalculatingasinglemetricscore
We apply reference aggregation to COMET by
againstthisaggregaterepresentation:
averagingthereferenceembeddings:
utility(hyp) ≈ metric(hyp,ref). (4) 1 (cid:88)
ref = emb(ref), (8)
m
Likewithstandardsampling-basedMBR,itispos- ref∈refs
sible to interpret this approximation as MC sam- calculatingasinglescoreperhypothesis:
pling: By averaging over representations of sam-
comet(hyp) ≈ score(hyp, ref, src). (9)
pledreferences,weestimatearepresentationoftheAccuracyofefficiencymethodswith CHRF asutilitymetric
EN–DE DE–EN EN–RU RU–EN
100%
50%
0%
n n n n ... 1 n n n n ... 1 n n n n ... 1 n n n n ... 1
1 2 4 8 1 2 4 8 1 2 4 8 1 2 4 8
Numberofeffectivereferences(logscale) Referenceaggregation N-by-S
Accuracyofefficiencymethodswith COMET-22asutilitymetric
EN–DE DE–EN EN–RU RU–EN
100%
50%
0%
n n n n ... 1 n n n n ... 1 n n n n ... 1 n n n n ... 1
1 2 4 8 1 2 4 8 1 2 4 8 1 2 4 8
Numberofeff. references(logscale) Referenceaggregation N-by-S CHRF
Figure1: HowaccuratelydoMBRefficiencymethodsapproximatestandardMBR?Inthisvalidationexperiment
onnewstest21,wegraduallyincreaseefficiencybyusingfewerreferencesforpairwiseutilityestimation–eitherby
subsamplingthereferences(N-by-S;EikemaandAziz,2022)orbyaggregatingtheirrepresentationsusingpartial
aggregation(Section3.3). Wereporttop-20accuracy,whichdescribeshowoftenanefficiencymethodranksthe
correcthypothesis(asselectedbystandardMBR)amongthetop20hypotheses. Anefficiencymethodwithahigh
top-20accuracycouldbeusedforpruningthenumberofhypothesesto20beforestandardMBRisapplied.
3.3 PartialAggregation sampling(Hewittetal.,2022)withϵ = 0.02. For
MBR,wegenerate1024samplespersegmentusing
Tobetterunderstandthelossofaccuracyincurred
epsilonsamplingandre-usethesamesamplesas
byaggregation,weexperimentwithpartialaggre-
references. Whilethisapproachdoesnotguarantee
gation, where we vary the number of references
thattheestimationoftheexpectedutilityisunbi-
that are combined into an average. Given a num-
ased(EikemaandAziz,2022), ithasempirically
bersofeffectivereferencesthatshouldbeusedfor
beenfoundtoworkwell(Freitagetal.,2023).
utilityestimation,wepartitionthesetofreferences
into s subsets and create an aggregate reference
Models We use open-source NMT models
for each subset. Appendix B presents a formal
trained for the EN–DE, DE–EN, EN–RU and RU–
descriptionofpartialaggregation. EN translation directions (Ng et al., 2019).2 The
authors provide an ensemble of four models per
3.4 Aggregate-to-fineMBR
direction, but we restrict our experiments to one
Analogouslytocoarse-to-fineMBR(Eikemaand single model per direction. We use the Fairseq
Aziz,2022),weevaluateanaggregate-to-fineMBR codebase(Ottetal.,2019)formodelinference.
approach. Specifically,weusetheaggregaterefer-
encetoprunethenumberofhypothesesto20ina
Metrics ForestimatingtheutilitieswithCHRF,
firststep. Inasecondstep,weusestandardMBR
we use a custom implementation of CHRF3 that
is equivalent to SacreBLEU (Post, 2018) with
toselectthebesthypothesisfromtheprunedset. A
default settings4. As COMET model, we use
formaldescriptionisprovidedinAppendixC.
COMET-22(Reietal.,2022a);becausethismodel
4 ExperimentalSetup
2Themodelsweretrainedwithalabelsmoothingofϵ=
0.1(Szegedyetal.,2016),whichisacommonchoiceinNMT.
Data Weusenewstest21(Akhbardehetal.,2021)
SomepreviousstudiesofMBRtrainedcustommodelswithout
as validation data and newstest22 (Kocmi et al., labelsmoothing(e.g.,EikemaandAziz,2020).Wearguethat
thisisonlynecessaryifunbiasedutilityestimatesaresought
2022)astestdata.
throughancestralsampling,andshouldbelessofaconcern
withepsilonsampling.
GenerationParameters Asbaselines,weevalu- 3https://github.com/jvamvas/fastChrF
atebeamsearchwithabeamsizeof4andepsilon 4chrF2|#:1|case:mixed|eff:yes|nc:6|nw:0|space:no|v:2.0.0
.cca02-poT
.cca02-poTEN–DE DE–EN EN–RU RU–EN Avg. Time(utility/total)
Beamsearch(size4) 76.16 72.56 68.50 75.47 73.17 - / 0.2s
Epsilonsampling(ϵ = 0.02) 73.39 69.70 65.79 72.13 70.25 - / 0.2s
MBRwith CHRF metric
–standardMBR 76.03 72.73 69.52 75.51 73.44 15.0s /19.8s
–referenceaggregation 75.95 72.79 69.46 75.45 73.41 0.1s / 4.9s
–aggregate-to-fineMBR 76.02 72.80 69.54 75.47 73.46 0.4s / 5.2s
MBRwith COMET-22metric
–standardMBR 77.64 73.57 72.40 76.11 74.93 23.1s /27.9s
–referenceaggregation 77.21 73.36 72.05 76.05 74.67 1.1s / 5.9s
–aggregate-to-fineMBR 77.54 73.52 72.29 76.13 74.87 1.5s / 6.3s
Table1: Testresultsonnewstest22,usingBLEURT-20forautomaticevaluation. Weuse1024samples/references
forMBR.Inthelastcolumn,wereporttheaveragetimeneededfortranslatingasegment.
was not trained on annotations of newstest21 or or without reference aggregation). We perform
newstest22,atrain–testoverlapcanberuledout. anautomaticevaluationusing BLEURT-20 (Sel-
lam et al., 2020), chosen because it is unrelated
MeasuringTimeEfficiency Weestimatewall-
totheutilitymetricsweuseforMBR. CHRF and
clocktimebasedonapartofthesegments,usinga
COMET scoresarereportedinAppendixE.
systemequippedwithanNVIDIAGeForceRTX
Theresultsshowthatreferenceaggregationnar-
3090andanAMDEPYC774264-coreprocessor.
rows the efficiency gap between MBR and beam
searchwhilepreservingmostofthequalitygainof
5 Results
standardMBR.Referenceaggregationspeedsup
5.1 Validationresults utilityestimationby99.5%for CHRF and95.1%
Figure 1 evaluates how accurately MBR effi- for COMET-22, reducing the total time needed
ciency methods approximate standard MBR. We fortranslationby75.5%and78.8%,respectively.
report top-20 accuracy, motivated by the idea Using an aggregate-to-fine approach has a lower
of coarse-to-fine MBR: any method with perfect lossofqualityandstillreducesthetotaltranslation
top-20accuracycouldbeusedforpruningthehy- timeby73.6–77.4%.
pothesissetto20withoutaffectingquality. Results Referenceaggregationisthusasuccessfulstrat-
fortop-1accuracyarereportedinAppendixG.5 egytoovercomethequadraticcomplexityofMBR.
For CHRF, we observe that reference aggrega- However,itisstillslowerthanbeamsearch,asthe
tionisParetosuperiortoN-by-S,maintainingnear- cost of sampling is now the dominant factor. Fu-
perfecttop-20accuracyevenifasingleaggregate tureworkcouldfocusonsamplingefficiency,e.g.,
referenceisused. ForCOMET,referenceaggrega- byusingfewerhypotheses,improvedcaching,or
tioncausessomelossofaccuracy,butoutperforms speculativesamplingapproaches(Leviathanetal.,
N-by-S in the s ≤ 16 range, where efficiency is 2023;Chenetal.,2023).
highest. Inaddition,wefindthatreferenceaggre-
6 Conclusion
gationapproximatesstandard(pairwise) COMET
much better than using CHRF as a coarse metric We proposed reference aggregation, a technique
does,providingaclearmotivationforaggregate-to- thatbooststheefficiencyofMBRdecodingbyshift-
fineMBRasanalternativetocoarse-to-fineMBR. ingtheMCsamplingfromtheutilityestimationto
thereferencerepresentation. Experimentsonma-
5.2 Testresults
chinetranslationshowedthatreferenceaggregation
InTable1,wereporttestresultsfornewstest22,fo- speedsuputilityestimationbyupto99.5%while
cusingonacomparisonbetweenfastbaselinealgo- minimallyaffectingtranslationquality,depending
rithms(beamsearchandsampling)andMBR(with ontheutilitymetricused. Thisreducesthegapto
beam search and makes MBR more practical for
5AccuracywasproposedbyChengandVlachos(2023)as
anevaluationmetricforMBRefficiencymethods. large-scaleapplications.Limitations Cristina España-Bonet, Angela Fan, Christian Fe-
dermann, Markus Freitag, Yvette Graham, Ro-
Thisworkhastwomainlimitations: man Grundkiewicz, Barry Haddow, Leonie Harter,
Kenneth Heafield, Christopher Homan, Matthias
1. Referenceaggregationrequiresautilitymetric Huck,KwabenaAmponsah-Kaakyire,JungoKasai,
DanielKhashabi,KevinKnight,TomKocmi,Philipp
basedonaverageablerepresentations.
Koehn, Nicholas Lourie, Christof Monz, Makoto
Morishita,MasaakiNagata,AjayNagesh,Toshiaki
2. Fortrainedmetrics,theeffectivenessofaggre-
Nakazawa,MatteoNegri,SantanuPal,AllahseraAu-
gationneedstobeevaluatedempirically.
gusteTapo,MarcoTurchi,ValentinVydrin,andMar-
cosZampieri.2021. Findingsofthe2021conference
Wehavedemonstratedthatreferenceaggregation onmachinetranslation(WMT21). InProceedingsof
is a viable technique for MBR with CHRF and theSixthConferenceonMachineTranslation,pages
1–88,Online.AssociationforComputationalLinguis-
COMET,leadingtoaconsiderablespeed-upwith
tics.
minor quality losses. In the case of CHRF, refer-
ence aggregation entails a slight modification of ChantalAmrheinandRicoSennrich.2022. Identifying
themetricdefinition,butisotherwiseexactandnot weaknessesinmachinetranslationmetricsthrough
minimum Bayes risk decoding: A case study for
an approximation. We thus expect that reference
COMET. In Proceedings of the 2nd Conference
aggregationcouldbeappliedinastraightforward
of the Asia-Pacific Chapter of the Association for
manner to other lexical overlap metrics such as ComputationalLinguisticsandthe12thInternational
CHRF++ (Popovic´, 2017) and BLEU (Papineni JointConferenceonNaturalLanguageProcessing
(Volume1: LongPapers),pages1125–1141,Online
etal.,2002).
only.AssociationforComputationalLinguistics.
For COMET, which is a trained metric, refer-
ence aggregation involves the averaging of fixed- Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,
sizesentenceembeddings. Weempiricallystudied Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. 2023. Accelerating large language model
thelossofaccuracyincurredbythisaveragingand
decodingwithspeculativesampling.
found that there is a favorable trade-off between
speed and accuracy for the COMET models we JuliusChengandAndreasVlachos.2023. Fastermin-
imum Bayes risk decoding with confidence-based
evaluated. We recommend that future work vali-
pruning. InProceedingsofthe2023Conferenceon
datestheeffectivenessofreferenceaggregationfor
Empirical Methods in Natural Language Process-
othertrainedmetrics. ing,pages12473–12480,Singapore.Associationfor
WhileCHRFandCOMETareamongthemost ComputationalLinguistics.
commonlyusedmetricsforMBR,previouswork
BryanEikemaandWilkerAziz.2020. IsMAPdecoding
hasalsoproposedmetricsthatarenotbasedonav-
allyouneed? theinadequacyofthemodeinneural
erageablereferencerepresentations. Forexample, machinetranslation. InProceedingsofthe28thInter-
BLEURT (Sellam et al., 2020), a trained metric nationalConferenceonComputationalLinguistics,
pages4506–4520,Barcelona,Spain(Online).Inter-
that was shown to be effective for MBR (Freitag
nationalCommitteeonComputationalLinguistics.
et al., 2022), is based on a cross-encoder archi-
tecturethatcreatesajointrepresentationforeach BryanEikemaandWilkerAziz.2022. Sampling-based
hypothesis–referencepair. Futureworkcouldinves- approximations to minimum Bayes risk decoding
for neural machine translation. In Proceedings of
tigateinwhatform,ifatall,referenceaggregation
the2022ConferenceonEmpiricalMethodsinNatu-
canbeappliedtocross-encoderarchitectures.
ralLanguageProcessing,pages10978–10993,Abu
Dhabi,UnitedArabEmirates.AssociationforCom-
Acknowledgments putationalLinguistics.
WethankClaraMeisterforhelpfuldiscussionsand MaraFinkelstein,SubhajitNaskar,MehdiMirzazadeh,
feedbackonthemanuscript. Thisworkwasfunded ApurvaShah,andMarkusFreitag.2023. MBRand
QEfinetuning: Training-timedistillationofthebest
bytheSwissNationalScienceFoundation(project
andmostexpensivedecodingmethods.
MUTAMUR;no.213976).
MarkusFreitag,BehroozGhorbani,andPatrickFernan-
des. 2023. Epsilon sampling rocks: Investigating
References samplingstrategiesforminimumBayesriskdecod-
ing for machine translation. In Findings of theAs-
Farhad Akhbardeh, Arkady Arkhangorodsky, Mag- sociation for Computational Linguistics: EMNLP
dalena Biesialska, Ondˇrej Bojar, Rajen Chatter- 2023,pages9198–9209,Singapore.Associationfor
jee, Vishrav Chaudhary, Marta R. Costa-jussa, ComputationalLinguistics.MarkusFreitag,DavidGrangier,QijunTan,andBowen sequencemodeling. InProceedingsofthe2019Con-
Liang. 2022. High quality rather than high model ferenceoftheNorthAmericanChapteroftheAssocia-
probability: MinimumBayesriskdecodingwithneu- tionforComputationalLinguistics(Demonstrations),
ralmetrics. TransactionsoftheAssociationforCom- pages48–53,Minneapolis,Minnesota.Association
putationalLinguistics,10:811–825. forComputationalLinguistics.
NunoM.Guerreiro,RicardoRei,DaanvanStigt,Luisa KishorePapineni,SalimRoukos,ToddWard,andWei-
Coheur, Pierre Colombo, and André F. T. Martins. JingZhu.2002. Bleu: amethodforautomaticevalu-
2023. xcomet: Transparentmachinetranslationeval- ationofmachinetranslation. InProceedingsofthe
uationthroughfine-grainederrordetection. 40thAnnualMeetingoftheAssociationforCompu-
tational Linguistics, pages 311–318, Philadelphia,
John Hewitt, Christopher Manning, and Percy Liang. Pennsylvania,USA.AssociationforComputational
2022. Truncation sampling as language model Linguistics.
desmoothing. InFindingsoftheAssociationforCom-
putationalLinguistics: EMNLP2022,pages3414– Maja Popovic´. 2015. chrF: character n-gram F-score
3427,AbuDhabi,UnitedArabEmirates.Association forautomaticMTevaluation. InProceedingsofthe
forComputationalLinguistics. TenthWorkshoponStatisticalMachineTranslation,
pages 392–395, Lisbon, Portugal. Association for
YuuJinnaiandKaitoAriu.2024. Hyperparameter-free ComputationalLinguistics.
approachforfasterminimumbayesriskdecoding.
Maja Popovic´. 2017. chrF++: words helping charac-
DonaldEKnuth.1997. Artofcomputerprogramming, tern-grams. InProceedingsoftheSecondConfer-
Volume 2: Seminumerical algorithms, 3rd edition. enceonMachineTranslation,pages612–618,Copen-
Addison-Wesley. hagen,Denmark.AssociationforComputationalLin-
guistics.
Tom Kocmi, Rachel Bawden, Ondˇrej Bojar, Anton
MattPost.2018. AcallforclarityinreportingBLEU
Dvorkovich, Christian Federmann, Mark Fishel,
scores. InProceedingsoftheThirdConferenceon
Thamme Gowda, Yvette Graham, Roman Grund-
MachineTranslation: ResearchPapers,pages186–
kiewicz,BarryHaddow,RebeccaKnowles,Philipp
191, Brussels, Belgium. Association for Computa-
Koehn,ChristofMonz,MakotoMorishita,Masaaki
tionalLinguistics.
Nagata,ToshiakiNakazawa,MichalNovák,Martin
Popel,andMajaPopovic´.2022. Findingsofthe2022
Ricardo Rei, José G. C. de Souza, Duarte Alves,
conference on machine translation (WMT22). In
ChrysoulaZerva,AnaCFarinha,TaisiyaGlushkova,
ProceedingsoftheSeventhConferenceonMachine
AlonLavie,LuisaCoheur,andAndréF.T.Martins.
Translation(WMT),pages1–45,AbuDhabi,United
2022a. COMET-22: Unbabel-IST2022submission
ArabEmirates(Hybrid).AssociationforComputa-
for the metrics shared task. In Proceedings of the
tionalLinguistics.
SeventhConferenceonMachineTranslation(WMT),
pages578–585,AbuDhabi,UnitedArabEmirates
ShankarKumarandWilliamByrne.2004. Minimum
(Hybrid).AssociationforComputationalLinguistics.
Bayes-riskdecodingforstatisticalmachinetransla-
tion. InProceedingsoftheHumanLanguageTech-
RicardoRei, AnaCFarinha, JoséG.C.deSouza, Pe-
nologyConferenceoftheNorthAmericanChapter
droG.Ramos,AndréF.T.Martins,LuisaCoheur,and
of the Association for Computational Linguistics:
Alon Lavie. 2022b. Searching for COMETINHO:
HLT-NAACL 2004, pages 169–176, Boston, Mas-
The little metric that could. In Proceedings of the
sachusetts,USA.AssociationforComputationalLin-
23rd Annual Conference of the European Associa-
guistics.
tionforMachineTranslation, pages61–70, Ghent,
Belgium.EuropeanAssociationforMachineTrans-
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
lation.
2023. Fast inference from transformers via spec-
ulativedecoding. InProceedingsofthe40thInter- RicardoRei,CraigStewart,AnaCFarinha,andAlon
nationalConferenceonMachineLearning,volume Lavie.2020. COMET:AneuralframeworkforMT
202ofProceedingsofMachineLearningResearch, evaluation. InProceedingsofthe2020Conference
pages19274–19286.PMLR. onEmpiricalMethodsinNaturalLanguageProcess-
ing(EMNLP),pages2685–2702,Online.Association
Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,
forComputationalLinguistics.
MichaelAuli,andSergeyEdunov.2019. Facebook
FAIR’s WMT19 news translation task submission. ThibaultSellam,DipanjanDas,andAnkurParikh.2020.
InProceedingsoftheFourthConferenceonMachine BLEURT: Learning robust metrics for text genera-
Translation (Volume 2: Shared Task Papers, Day tion. InProceedingsofthe58thAnnualMeetingof
1),pages314–319,Florence,Italy.Associationfor theAssociationforComputationalLinguistics,pages
ComputationalLinguistics. 7881–7892,Online.AssociationforComputational
Linguistics.
MyleOtt,SergeyEdunov,AlexeiBaevski,AngelaFan,
SamGross,NathanNg,DavidGrangier,andMichael Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Auli. 2019. fairseq: A fast, extensible toolkit for JonShlens,andZbigniewWojna.2016. Rethinkingthe inception architecture for computer vision. In scoresforeachn-gramorder:
Proceedings of the IEEE conference on computer
visionandpatternrecognition,pages2818–2826. 1 (cid:88)n
CHRP(hyp, ref) = CHRP i(hyp, ref),
n
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob i=1
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz (13)
Kaiser,andIlliaPolosukhin.2017. Attentionisall n
1 (cid:88)
youneed. InAdvancesinNeuralInformationPro- CHRR(hyp, ref) = CHRR i(hyp, ref).
cessingSystems,volume30.CurranAssociates,Inc. n
i=1
(14)
GuangyuYang,JinghongChen,WeizheLin,andBill
Byrne.2023. Directpreferenceoptimizationforneu-
WhenCHRFisusedasautilitymetricinastan-
ral machine translation with minimum bayes risk
dardMBRsetting,theexpectedutilityofahypothe-
decoding.
sisisestimatedbasedonaset{ref(1),...,ref(m)}
ofmreferences:
m
1 (cid:88)
A FormalDefinitionof
utility(hyp) =
m
CHRF β(hyp, ref(k)).
k=1
ReferenceAggregationforChrF (15)
Incontrast,referenceaggregationfirstcalculates
The CHRF metric (Popovic´, 2015) is a harmonic
thearithmeticmeanofthereferencebags:
meanofprecisionandrecallscores:
m m
1 (cid:88) (k) 1 (cid:88) (k)
(1+β2)·CHRP·CHRR ref = [
m
ref
1
,...,
m
ref |V|], (16)
CHRF
β
=
β2·CHRP+CHRR
. (10)
k=1 k=1
andestimatestheutilityas:
Internally, CHRF converts hypotheses and refer-
ences into bags of character n-grams. Such bags utility agg(hyp) = CHRF β(hyp, ref). (17)
canberepresentedasmultisets(Knuth,1997,Sec-
tion4.6.3)oras(sparse)vectors. Wewillusevec- Note that the only mathematical difference be-
tornotationinthisformaldefinition,whichallows tween pairwise calculation of chrF and using the
us to define reference aggregation with standard aggregatereferenceisthattheF-scoreisaveraged
vectoroperations. across sentences in the pairwise calculation, and
Let hyp ∈ R|V| and ref ∈ R|V| be bags repre- computedovertheglobalprecisionandrecallwith
referenceaggregation.
senting a hypothesis and a reference, where V is
thevocabularyofallcharactern-gramsuptomax-
B FormalDefinitionof
imum order n, and the entries hyp and ref are
j j PartialAggregation
thecountsofn-gramj ∈ V inthehypothesisand
reference,respectively. Weconceptualizepartialaggregationasfollows:
Foragivenn-gramorderi ∈ {1,...,n},preci-
1. The set of individual references contains m
sionandrecallaredefinedas:
references.
(cid:80) min(hyp ,ref ) 2. We randomly partition the set of references
CHRP i(hyp, ref) = j∈V (cid:80)i j j , intosgroupsofequalsize.
hyp
j∈Vi j
3. Eachgroupiscombinedintoanaveragerefer-
(11)
encerepresentation,resultinginsaggregate
(i) (s)
(cid:80) referencesref ,...,ref .
min(hyp ,ref )
CHRR i(hyp, ref) = j∈V (cid:80)i j j ,
ref Theexpectedutilityofeachsampledhypothesisis
j∈Vi j
(12) thenapproximatedastheaveragemetricscoreover
allaggregatereferences:
where V is the set of all character n-grams of or-
i
s
der i. Overall precision and recall are calculated 1 (cid:88) (i)
utility(hyp) ≈ metric(hyp,ref ). (18)
asthearithmeticmeanoftheprecisionandrecall s
i=1Like with N-by-S MBR, the parameter s can weprovideamoredetailedanalysisfor CHRF and
beseenasthenumberofeffectivereferencesthat COMET.
determines the computational complexity of the Above, we stated that utility estimation with
utilityestimation. Thecases = mcorrespondsto these metrics usually has two stages: feature ex-
standardMBR,whereeachsampledhypothesisis tractionandscoring. Thefeatureextractionstageis
comparedtoeachreferenceinapairwisefashion. notaffectedbyreferenceaggregation,andprevious
Thecases = 1correspondstothefullaggregation workhasalreadyremarkedthatreferencefeatures
approach, where a single aggregate reference is canbeextractedonceandre-usedforallhypothe-
createdfromallreferences. ses(AmrheinandSennrich,2022). Ifthereference
setisidenticaltothesetofhypotheses,thefeature
C FormalDefinitionofAggregate-to-fine
extractionstageisinO(n),otherwiseO(n+m).
MBR
ThescoringstageofCHRFisdominatedbythe
element-wiseminimumfunctioninEqs.11and12
Aggregate-to-fineMBRisaspecialcaseofcoarse-
(or,ifthebagsofn-gramsarerepresentedasmul-
to-fineMBR(EikemaandAziz,2022),whichuses
tisets,bytheintersectionoperationhyp∩ref). Be-
acheapproxyutilityfunctiontoprunethenumber
cause this operation is performed separately for
of hypotheses. In the case of aggregate-to-fine
eachhypothesis–referencepair,thecomplexityis
MBR, the proxy utility function is based on an
inO(nm). Referenceaggregationreducesthecom-
aggregatereferencerepresentation.
plexitytoO(n+m),giventhattheaggregateref-
Thegeneraldefinitionofcoarse-to-fineMBRis
erencecanbecomputedonceandthenre-usedfor
as follows: Given the original set of sampled hy-
pothesesH¯(x)andaproxyutilityfunctionu ,
allhypotheses.6
proxy
The same analysis applies to COMET. With
coarse-to-fineMBRselectsasubsetofT hypothe-
standardMBR,Eq.7isevaluatedforeachhypoth-
ses:
H¯ (x) := top-T u (hyp). (19) esis–referencepair;withreferenceaggregation,it
T proxy
hyp∈H¯(x) is only evaluated once for each hypothesis. The
aggregatereferenceembeddingscanbecomputed
Inthesecondstep,theutilityofeachhypothesisin
onceandre-usedforallhypotheses.
theprunedsetisestimatedusingthefine-grained
In practice, the runtime of utility estimation is
utilityfunctionu :
target
affectedbyadditionalfactors. Theremaybedupli-
yC2F := argmax u (hyp). (20) catesamongthesamples,sothenumberofscores
target
hyp∈H¯ T(x) thateffectivelyneedtobecomputedcanvary. In
addition,mostaspectsofutilityestimationcanbe
When experimenting with aggregate-to-fine
computed in parallel, which makes the effective
MBR, we re-use the same utility metric for both
runtimehighlyimplementation-dependent.
steps,butfirstwithanaggregatereferenceandthen
withthefullsetofreferences:
u (hyp) = metric(hyp,ref), (21)
proxy
1 (cid:88)
u (hyp) = metric(hyp,ref). (22)
target
m
ref∈refs
Notethatusingthesamemetricinbothstepsis
not strictly necessary, but has the advantage that
the features (e.g., embeddings) only need to be
computedonce.
6ForCHRF,referenceaggregationcanresultinanaggre-
D ComplexityAnalysis gatebagofn-gramsthatislargerthatthebagsoftheindi-
vidualreferences;inthetheoreticalworstcase,whereallthe
references are disjoint, even in an aggregate bag that is m
Generally,referenceaggregationreducesthecom-
timeslarger. However,thisisahighlyunlikelyscenarioin
plexityofutilityestimationfromO(nm)toO(n+ practice,sincedifferenttranslationsofthesamesourcewill
m), where n is the number of hypotheses and m havesubstantialoverlap,andevenif|ref|≫|ref|,thecostof
intersectiononlydependson|hyp|,assumingthataconstant-
isthenumberofreferences. Theexactcomplexity
timehashtableisusedtocheckwhethereachiteminhypis
dependsonthespecificsoftheutilitymetric. Here, containedinref.E ExtendedTestResults
CHRF Cometinho COMET-22 XCOMET-XXL BLEURT-20
Beamsearch(size4) 58.6 56.0 84.3 92.2 73.2
Epsilonsampling(ϵ = 0.02) 52.6 45.3 81.9 89.4 70.3
MBRwith CHRF metric
–standardMBR 59.8 58.3 84.5 91.8 73.4
–referenceaggregation 59.8 58.2 84.5 91.7 73.4
–aggregate-to-fineMBR 59.8 58.3 84.5 91.8 73.5
MBRwithCometinhometric
–standardMBR 57.5 65.1 85.1 92.5 74.0
–referenceaggregation 57.8 64.5 85.0 92.4 73.9
–aggregate-to-fineMBR 57.5 65.0 85.1 92.5 74.0
MBRwith COMET-22metric
–standardMBR 57.3 60.8 87.1 93.7 74.9
–referenceaggregation 57.7 60.8 86.8 93.4 74.7
–aggregate-to-fineMBR 57.4 60.8 87.0 93.7 74.9
Coarse-to-fineMBR
–standard CHRF to COMET-22 59.3 60.1 85.8 93.0 74.4
–aggregate CHRF to COMET-22 59.4 60.2 85.8 93.0 74.4
Table 2: Extended results on newstest22. In this table, we include Cometinho (Rei et al., 2022b) as util-
ity metric, which is a distilled COMET model. Furthermore, as an additional evaluation metric, we report
XCOMET-XL(Guerreiroetal.,2023). Weaveragetheevaluationscoresacrossthefourtranslationdirections.
F DataStatistics
#Segments #Samplespersegment #Uniquesamplespersegment
newstest21 EN–DE 1002 1024 874.2
newstest21 DE–EN 1000 1024 716.9
newstest21 EN–RU 1002 1024 896.7
newstest21 RU–EN 1000 1024 727.3
newstest22 EN–DE 2037 1024 697.5
newstest22 DE–EN 1984 1024 671.4
newstest22 EN–RU 2037 1024 750.2
newstest22 RU–EN 2016 1024 726.3
Table3: Statisticsforthedatasetsusedinthispaper. Wesample1024hypothesespersourcesegmentusingepsilon
samplingandfindthatmostofthesamplesareunique.G Top-1AccuracyofEfficiencyMethods
Utilitymetric: CHRF
EN–DE DE–EN EN–RU RU–EN
100%
50%
0%
n n n n ... 1 n n n n ... 1 n n n n ... 1 n n n n ... 1
1 2 4 8 1 2 4 8 1 2 4 8 1 2 4 8
Numberofeffectivereferences(logscale) Referenceaggregation N-by-S
Utilitymetric: COMET-22
EN–DE DE–EN EN–RU RU–EN
100%
50%
0%
n n n n ... 1 n n n n ... 1 n n n n ... 1 n n n n ... 1
1 2 4 8 1 2 4 8 1 2 4 8 1 2 4 8
Numberofeff. references(logscale) Referenceaggregation N-by-S CHRF
Figure2: Top-1accuracyofMBRefficiencymethodsonnewstest21,analogoustoFigure1.
H ValidationResultsforCometinho
Top-20accuracy
EN–DE DE–EN EN–RU RU–EN
100%
50%
0%
n n n n ... 1 n n n n ... 1 n n n n ... 1 n n n n ... 1
1 2 4 8 1 2 4 8 1 2 4 8 1 2 4 8
Numberofeff. references(logscale) Referenceaggregation N-by-S CHRF
Top-1accuracy
EN–DE DE–EN EN–RU RU–EN
100%
50%
0%
n n n n ... 1 n n n n ... 1 n n n n ... 1 n n n n ... 1
1 2 4 8 1 2 4 8 1 2 4 8 1 2 4 8
Numberofeff. references(logscale) Referenceaggregation N-by-S CHRF
Figure3: AccuracyofMBRefficiencymethodsonnewstest21whenusingtheCometinhomodel(Reietal.,2022b)
asutilitymetric.
ycarucca1-poT
ycarucca1-poT
ycarucca02-poT
ycarucca1-poT