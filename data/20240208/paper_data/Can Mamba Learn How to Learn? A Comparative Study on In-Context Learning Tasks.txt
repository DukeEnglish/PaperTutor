Can Mamba Learn How to Learn?
A Comparative Study on In-Context Learning Tasks
JonghoPark1, JaeseungPark2∗, ZheyangXiong3, NayoungLee3, JaewoongCho1,
SametOymak4, KangwookLee1,3, DimitrisPapailiopoulos1,3
1KRAFTON,2SeoulNationalUniversity,
3UniversityofWisconsin-Madison,4UniversityofMichigan,AnnArbor
Abstract
State-spacemodels(SSMs),suchasMambaGu&Dao(2023),havebeenproposed
asalternativestoTransformernetworksinlanguagemodeling,byincorporating
gating,convolutions,andinput-dependenttokenselectiontomitigatethequadratic
cost of multi-head attention. Although SSMs exhibit competitive performance,
their in-context learning (ICL) capabilities, a remarkable emergent property of
modernlanguagemodelsthatenablestaskexecutionwithoutparameteroptimiza-
tion,remainunderexploredcomparedtoTransformers. Inthisstudy,weevaluate
theICLperformanceofSSMs,focusingonMamba,againstTransformermodels
acrossvarioustasks. OurresultsshowthatSSMsperformcomparablytoTrans-
formersinstandardregressionICLtasks,whileoutperformingthemintaskslike
sparseparitylearning. However,SSMsfallshortintasksinvolvingnon-standard
retrievalfunctionality. Toaddresstheselimitations,weintroduceahybridmodel,
MambaFormer,thatcombinesMambawithattentionblocks,surpassingindividual
modelsintaskswheretheystruggleindependently.Ourfindingssuggestthathybrid
architecturesofferpromisingavenuesforenhancingICLinlanguagemodels.
1 Introduction
Modernlargelanguagemodels(LLMs)exhibitremarkablein-contextlearning(ICL)capabilities,
enablingthemtolearnnewtaskswithafewdemonstrationsandwithoutfurtherweightfine-tuning.
Although the exact emergence mechanism of these capabilities warrants further theoretical and
empiricalinvestigation(Chanetal.,2022;Weietal.,2022;Minetal.,2022b;Schaefferetal.,2023),
experimentsonlargerTransformer-basedmodelsconsistentlydemonstratethattheirICLcapabilities
improveastraininglossreduces(Brownetal.,2020;Kaplanetal.,2020;Muennighoffetal.,2023).
Meta-learning,or“learningtolearn,”hasbeenextensivelystudied(Schmidhuberetal.,1997;Ravi
&Larochelle,2016)andrecentlyregainedinterestinthecontextofICL,particularlyconcerning
Transformermodels(Vaswanietal.,2017). Gargetal.(2022),forexample,proposedvariousICL
tasks,suchaslearninglinearregression,andevaluatedtheabilityoftransformerstoperformthem
whenspecificallytrainedtodoso. Ontheotherhand,Minetal.(2022a)studiedfine-tuninglanguage
modelstoexplicitlylearnandperformICL.Followingthesefootsteps,numerousresearchstudies
havebeendedicatedtounderstandingthemechanicsofAttentionthatenablesuchmeta-learning
capabilities,eitherthroughconstructiveargumentsorextensiveexperimentalinvestigation(Akyürek
etal.,2022;Lietal.,2023b;vonOswaldetal.,2023b;Baietal.,2023;Yangetal.,2023a;Lietal.,
2023a;vonOswaldetal.,2023a).
∗ThisworkwasdoneduringaninternshipatKRAFTON.
Email:<jongho.park@krafton.com>.Correspondence:<dimitris@papail.io>
Preprint.Underreview.
4202
beF
6
]GL.sc[
1v84240.2042:viXraTransformer Mamba MambaFormer
Linearregression ✓ ✓ ✓
Sparselinearregression ✓ ✓ ✓
2NNregression ✓ ✓ ✓
DecisionTree ✓ ▲ ✓
Orthogonal-outlierregression ✓ ▲ ✓
Many-outlierregression ▲ ✓ ✓
Sparseparity ✗ ✓ ✓
Chain-of-ThoughtI/O ✓ ✓ ✓
Vector-valuedMQAR ✓ ✗ ✓
Table1: ModelperformancesonvariousICLtasks. Welabelthemodel’sperformancewith✓ifthe
modelperformsonparwithotherbaselinemodels,✗ifthemodelfailstolearnthetask,and▲if
thereexistsperformancegapcomparedtoothermodels. Transformerfailsinlearningsparseparity
Mambafailsinvector-valuedMQAR.OurproposedMambaFormersucceedsinalltasks.
AsTransformerlanguagemodelsarecurrentlytheonlylargemodelsthathavebeenreportedtobe
capableofICLinpractice,thisraisesthequestion:
Canattention-freemodelsperformICL?
Thisquestionholdsmerit,especiallyconsideringthatseveralrecentstudieshaveattemptedtomove
beyondattention-basednetworksduetotheirquadraticcost(Guetal.,2022b;Daoetal.,2022;Gu&
Dao,2023;Polietal.,2023;Pengetal.,2023;Sunetal.,2023;Yangetal.,2023b). Inthiswork,we
focusspecificallyonstate-spacemodels(SSMs),andparticularlyMamba(Gu&Dao,2023). Mamba
wasrecentlydemonstratedtobehighlyefficientwhileachievingnearstate-of-the-artperformancein
standardpretraininglanguagedatasets,suchasthePile(Gaoetal.,2020),butatsmallermodelscales
(e.g.,upto3billionparameters),surpassingtransformersandotherattention-freearchitecturesacross
variouslanguageandnon-languagetasks. However,ICLcapabilitiesusuallyemergeatscalesbeyond
3billionparameters. Asaresult,thepotentialoftheseattention-freemodelstoperformICLremains
underexplored,astestingsuchhypothesesusuallyrequiresscalingbeyondthe7billionparameter
level. Nonetheless, wecanstillinvestigatesmall-scaleICLcapabilitiesbyspecificallytraininga
modeltoperformin-contextlearning,followingtheapproachofGargetal.(2022).
Contributions. Inthisstudy,weintroduceadiversesetofICLtaskstoevaluatetheperformance
ofTransformerandvariousSSMs,includingstate-of-the-artmodelslikeMambaandS4(Guetal.,
2022b). Our findings reveal that most of these SSMs can effectively perform ICL, matching the
performanceofTransformersacrossmultipletasks. However,Mambademonstratessomelimitations
inlearningdecisiontreesandretrievaltasks(asalsonotedby(Aroraetal.,2023)),butcanoutperform
TransformersinothercomplexICLtasks,suchassparseparity,whereTransformermodelsstruggle.
PerformanceofdifferentmodelsoneachtaskissummarizedinTable1.
Since there seem to be tasks where either family of models is better, we explore the impact of
interleaving SSM blocks with multi-head attention blocks, similar to (Gu & Dao, 2023). We
introduceMambaFormer,anovelhybridarchitecturethatintegratesMambaandAttentionlayers,
whileeliminatingtheneedforpositionalencodings,asshowninFigure1. MambaFormerseems
toleveragethestrengthsofbothMambaandTransformers,exhibitinggoodperformanceacrossall
evaluatedICLtasksandsimultaneouslylearningsparseparityandretrieval.
We believe that our findings underscore the importance of broadening the understanding of ICL
beyondTransformers,assignificantprogresshasbeenmadeinthecontextofattention-freearchitec-
tures.
Weacknowledgethatalimitationofourstudyliesinthefocusonnon-languageICLtasksandsmaller
models. ItispossiblethatanarchitecturalcomparisonbetweenSSMsandtransformersformore
generalICLtasksinactuallanguagesettingsathigherparametercountsmightnotbeyieldthesame
observations as we offer here. Nevertheless, our results indicate that, apart from its difficulty in
someretrievaltasks,similartothosenotedby(Aroraetal.,2023),thereseemstobenofundamental
obstacleforMambatoperformin-contextlearning.
2Outputs
Trainingdata
Mamba
prompts:
Multi-Head
Attention
Mamba
Inputs
Figure1: MambaFormerisahybridarchitecturethatreplacesMLPblockswithinthetransformer
withMambablocks. Importantly,thearchitecturealsostartswithaMambablockanddoesnotuse
positional encoding. In our ICL evaluations, we find that MambaFormer consistently achieves a
best-of-both-worldsperformancecomparedtoTransformerandMamba.
2 RelatedWork
Transformer-based in-context Learning. The role of attention in ICL has been the focus of
boththeoreticalandempiricalresearch. Studieshaveprimarilyfocusedonmeta-learning(Ravi&
Larochelle,2016;Minetal.,2022a),whereoneexplicitlytrainsforICL.Notably,Gargetal.(2022)
haveexaminedtransformersinin-contextregressiontasks,fromlearninglinearregressiontolearning
decisiontrees. Subsequentworkshavesuggestedthatattentionmaymimicvariousoptimization
algorithms(Akyüreketal.,2022;vonOswaldetal.,2023b;Daietal.,2023).Infact,Ahnetal.(2023);
Mahankalietal.(2023)haveprovablyshownthatgradientdescentisoptimalinlinearregressionICL
forlinearattention.
Whilethesesettingsmightappearsimplisticanddetachedfromlanguagemodels, Bhattamishra
etal.(2023)showedthatafrozenGPT-2canimplementthenearestneighboralgorithm,drawing
connectionsbetweentheICLinexistinglanguagemodelsandthestylizedsettingoftrainingfor
ICLfromrandominitialization. Furthermore, Olssonetal.(2022)alsoempiricallydemonstratethat
“inductionheads”,whichareattentionheadsthatsolveasimpleretrievalproblem,correlatewithICL
behavior,providingastrongconnectionbetweenretrievalandICL.
Sub-quadraticarchitectures. Thenumberofeffectivefloatingpointoperationsinanattention
layerscalesquadraticallywithrespecttotheinputsequencelength. Numerousapproximationsor
alternativemodelarchitectureshavebeenproposedtoovercomethequadraticdependence. These
range from approximating attention mechanisms (Beltagy et al., 2020; Wang et al., 2020) to the
developmentofnovelrecurrentconvolutionalmodelssuchasstructuredstate-spacemodels(Guetal.,
2022b).
S4(Guetal.,2022a)isafamilyofsequencemodelscharacterizedbyadiscretizedstate-spacemodel
h =Ah +Bx , y =Ch , (1)
t t−1 t t t
whereh representsthehiddenstateand(A,B,C)areinput-independent(transformed)parameters.
t
Therecurrenceisexpressibleasaconvolution,enablingnear-linearcomplexityusingFastFourier
Transform. Viewed in this framework, Linear Transformers (Katharopoulos et al., 2020), which
employlinearattentionwithoutsoftmax,canbeseenasavariantoflinearSSM.
Buildinguponthisconcept,H3(Daoetal.,2022),whichintegratesanS4withdualgatedconnections.
TherecentMamba(Gu&Dao,2023)departsfromthestandardSSMbyintroducingaselection
3mechanismthatmakes(A,B,C)inEquation(1)dependentonx ,whichallowsforinput-dependent
t
sequencemixing.
There are other notable attention-free models such as Hyena (Poli et al., 2023), RWKV (Peng
etal.,2023),RetNet(Sunetal.,2023),andGLA(Yangetal.,2023b). Despiteofstate-of-the-art
performanceformodelslikeMamba,Aroraetal.(2023)havedemonstratedthatsubquadraticmodels
stilllagbehindattentiononmulti-queryrecalltasks,whichisageneralizationoftheinductionhead
task(Olssonetal.,2022).
InXieetal.(2021),theauthorsproposedasyntheticlanguage-basedin-contextlearningdatasetand
showthattransformersandLSTMsarecapableofICL.Moreover,Akyüreketal.(2024)suggested
a langauge based ICL benchmark, by training on regular languages generated by random finite
automata,andalsounderscoredthegapbetweenTransformersandsubquadraticcomplexitymodels.
3 ExperimentalSetup
WeevaluatetheICLcapabilitiesofSSMsandTransformersbytrainingeachmodelfromscratchon
eachspecifictask,detailedinSection3.1. Section3.2outlinestheICLandrelatedtasksinvestigated
inourstudy. WeprovideabriefsummaryofourtasksinthefollowingTable2.
Task dim(d) points(N) Example/FunctionSampling Task-specific
Linearregression 20 41 x,w∼N(0,I d) –
SparseLinearregression 20 101 x,w∼N(0,I d),sparsity(w)←k k=3
2NNregression 20 101 W(1),W(2)∼N(0,1) –
ij ij
DecisionTree 20 101 x,Leaf∼N(0,1),non_leaf∼{1,...,d} depth=4
Orthogonal-outlierregression 20 101 x,w∼N(0,I d),u,v∼w⊥ p=0.5
Many-outlierregression 20 512 x∼N(0,I)w.p.1−p,else(x,y)=(1,1) p=0.9
SparseParity 10 140 x∼{−1,1}d,y=(cid:81) x[j] k=2
j∈I
Chain-of-ThoughtI/O 10 101 x∼N(0,I d),W ij∼N(0,2/k),v∼N(0,I k) h=8
Vector-valuedMQAR 20 128 k,v∼Unif(Sd−1) 32k-vpairs
Table2:SummaryofTasks.Allmodelsaretrainedfor500,000iterations(exceptforthevector-valued
MQAR;seeAppendixB.2).
3.1 ModelTrainingforIn-contextLearning
WetrainmodelstolearnspecificfunctionclassesF in-context.Trainingbeginsbygeneratingrandom
prompts: selectingafunctionf ∈F fromdistributionD andsamplingasequenceofrandominputs
F
x ,...,x ∈Rdi.i.d. fromD . Here,N anddrepresentthenumberofin-contextexamplesand
1 N X
thedimensionofx ,respectively. TheseinputscreatethepromptP =(x ,f(x ),...,x ,f(x )).
i 1 1 N N
Wetrainthemodelf ,parameterizedbyθ,byminimizingtheexpectedlossoverallprompts:
θ
(cid:34) N−1 (cid:35)
1 (cid:88)
minE ℓ(f (Pi),f(x )) , (2)
θ P N θ i
i=1
where Pi := (x ,f(x ),...,x ,f(x ),x ) and ℓ(·,·) isa lossfunction. Forf : Rd → R, we
1 1 i i i+1
appendd−1zerostof(x). WeuseappropriatelossfunctionsforeachICLtask.
Modelarchitecture. WeprimarilyfocusonSSMs,including(1)Mamba(Gu&Dao,2023),astate-
of-the-artSSMmodelwithselectionmechanism;(2)S4(Guetal.,2022a),alineartime-invariant
counterparttoMamba;and(3)S4-Mamba,avariantwhereMamba’sinput-dependentS6isreplaced
withinput-independentS4. TheprimarydifferencesbetweentheS4modelslieintheapplicationof
multiplicativegatingandthemoduleorder.2
Training. Wetraineachmodelbysamplingabatchofrandompromptsateachtrainingstepand
updatingthemodelparametersusingAdamoptimizer(Kingma&Ba,2014). Weuseabatchsizeof
64andtrainedfor500,000iterations(exceptforthevector-valuedMQARtask;seeAppendixB.2).
2https://github.com/state-spaces/s4/blob/main/models/s4
4Evaluation. Weevaluatemodelperformanceonin-contextlearningusingtaskanddatadistributions
D and D consistent with training. A function and a sequence of N inputs are sampled from
F X
D and D , respectively, to generate a test prompt P = (x ,f(x ),...,x ,f(x )). We
F X test 1 1 N N
create1,280promptsandmeasuretheempiricalmeanofEq.(2)acrossthemforin-contextlearning
performance.
Toplotperformanceasmodelcapacitygrows,wecalculatethetotalfloatingpointoperations(FLOPs)
usedfortraining.ThecalculationforTransformerandMambacanbefoundin AppendixC,whichare
basedon(Kaplanetal.,2020;Gu&Dao,2023). Modelconfigurationsandtrainingimplementation
detailsareprovidedinAppendixA.
3.2 In-contextlearningtasks
We provide an overview of the ICL and related tasks investigated in this study. Some tasks are
adaptedfrom(Gargetal.,2022),andwefollowthesettingsoutlinedintheirwork. Thetasksare
summarizedinTable2.
3.2.1 Learningregression
Forallregressiontasks,in-contextexamplesx aresampledfromtheGaussiandistributionN(0,I ),
i d
whereI isthed×didentitymatrix. Weusethesquarederrorlossformodeltraining.
d
Linearregression. WeexaminetheclassoflinearfunctionsF = {f|f(x) = w⊤x,w ∈ Rd}
wherewissampledfromtheGaussiandistributionN(0,I ). Wesetd=20.
d
Sparselinearregression. Thesettingisidenticaltolinearregression,exceptthatwissampled
fromN(0,I ),afterwhichkcoordinatesarerandomlyretainedinw,andtherestaresettozero. We
d
setk =3.
Two-layer neural network. We consider the class of two-layer ReLU neural networks F =
{f|f(x) = W(2)σ(cid:0) W(1)x(cid:1) }, whereW(2) ∈ R1×h,W(1) ∈ Rh×d, andσ(·) = max(0,·)isthe
ReLUfunction. EachelementoftheweightmatricesisindependentlydrawnfromN(0,1). Weuse
d=20andh=100.
DecisionTree Weconsiderafullbinarytreewithafixeddepthandinputx∈Rd. Leafnodevalues
aresampledfromN(0,1),andtherestaresampleduniformlyfrom{1,...,d},functioningasindices
ofx. Atagivennon-leafnode,wemovetotherightifx[i]>0,whereiisthesampledindex,and
otherwisemovetotheleft. yistheleafnodevaluewhenthetraversalterminates.
3.2.2 Learningwithoutliers
Theproblemsthatbelongtothisfamilyadoptthebasicsettingofthestandardlinearregressiontask.
Withafixedprobabilityp,eachpairof(x ,f(x ))inthepromptisreplacedwith“dummy”vectors
i i
whichareeitheroutofthetrainingdistribution,orconfoundersdesignedtoincreasethecomplexity
ofthetask. Wetestp ∈ {0.5,0.9}asreplacementprobabilitiesfortasksdescribedbelow. During
training,wedonotcomputethelossforthereplacedoutliers.
Orthogonal-outlier regression. Each pair of (x ,f(x )) is randomly replaced with ((a u +
i i x
b v)/(a2 +b2),(a u+b v)/(a2 +b2)),whereu,v ∈ w⊥. (u,v) := (w −proj (w ),w −
x x x y y x x 1 w 1 2
proj (w ))andw andw aresampledfromN(0,I )andthecoefficientsa ,b ,a ,b areinde-
w 2 1 2 d x x y y
pendentlysampledfromN(0,1).
Many-outlierregression. Inthissetting,x andf(x )arerandomlyreplacedwithad-dimensional
i i
vectorofones{1}dandanone-hotvector[1,0,...,0],respectively,withprobability90%. Here,we
testlongersequencesofN =512.
3.2.3 Learningdiscretefunctions
Sparse parity. Following the setting from Bhattamishra et al. (2023), we consider the class of
(cid:81)
functionsF ={f|f(x)= x [j]},wherex [j]denotesthej-thelementofthevectorx andS
j∈S i i i
isasubsetof{1,...,d}withthesizek. Eachx issampleduniformlyatrandomfrom{−1,1}d,
i
5andS ofsizekisrandomlysampledfromtheset{1,...,d}. Forthistask,wetrainamodelusing
thecross-entropylossandevaluatethemodelusingabinaryindicatorforaccuracy,whichassigns1
tocorrectpredictionsand0toincorrectones.
3.2.4 LearningChain-of-Thought
Chain-of-Thought-I/O. Following the setting from Li et al. (2023b), we consider the class of
two-layerReLUneuralnetworksF ={f|f(x)=W(2)σ(cid:0) W(1)x(cid:1) },whereW(2) ∈R1×h,W(1) ∈
Rh×d,andσ(·)istheReLUfunction. Wesetd = 10,andh = 8. Weadditionallyinterleavethe
intermediatehiddenfeatures =σ(cid:0) W(1)x (cid:1) inourinputtrainingsequenceinaChain-of-Thought
i i
style. Giventheinputsequence(x ,s ,f(x ),··· ,x ,s ,f(x ),x ),themodelisevaluatedon
1 1 1 N N N test
thefinaloutputpredictionyˆ basedontheinputsequenceandtheintermediatelayerpredictionˆs .
test
3.2.5 Learningretrieval
Vector-valuedmulti-queryassociativerecall
Wetestthemodel’sabilitytodomulti-queryassociativerecall(MQAR)(Aroraetal.,2023). While
MQARisnotanICLtask,model’sabilitytodoassociativerecall(AR)ishighlyrelatedtomodel’s
abilitytolearnin-context(Olssonetal.,2022). Tobettermeasurethemodel’sabilitytoretrieve
informationfromcontext,weconsideravariantofMQARsuchthatkeysandvaluesarevector-valued
soeachvectorcanbeseenasa“uniquetoken”andtheretrievalaccuracycanbemeasuredbythe
meansquarederrorbetweenretrievedvectorsandtargetvectors. Specifically,inthistask,themodel
is given a sequence of key-value pairs of vectors {k ,v ,...,k ,v }, where k ,v ∈ Sd−1 are
1 1 n n i i
sampleduniformlyfromtheunitd-sphere. Thequeryconsistsofsequenceofvectors{q ,...,q }.
1 m
Foreachqueryq ,thereexistssome1≤l≤nsuchthatq =k . Themodelmustlearntooutput
j j l
v associatedwiththequeryq foreachofthequeries,producingmoutputstotal. Wetrainamodel
l j
usingthesquarederrorerror.
4 Experimentresults
Inthissection,wedemonstratethatMambacanbetrainedfromscratchtoperformvariousICLtasks.
Furthermore,weidentifyspecifictasksinwhichonemodelperformsbetterthantheotherandvice
versa.
4.1 Mambacanin-contextlearn!
AsshowninFigure2,MambaconsistentlyoutperformsitsmoresimplecounterpartsS4-Mambaand
S4.Insimpletaskssuchaslinearregression,thegapbetweenMambaandS4-Mambaismuchsmaller
thanthatofS4-MambaandS4. GiventhatthemaindifferencebetweenMambaandS4-Mambaisthe
input-dependentselectionmechanism,appropriategatingandstackingofMLPs(i.e.,thedifference
betweenS4-MambaandS4)seemtobemoresignificantforsuchtasks. However,incomparison,
input-dependentselectionmakesmeaningfulprogressformorecomplextaskssuchas2NNregression
andlearningdecisiontrees.
MambacanalsoperformonparwithTransformerevenasthetotalFLOPsscaleup. Thisissurprising
giventhatTransformerandattentionhavebeenthefocusofmanypreviousworksforitsuniqueICL
capability. Moreover,Mambatendstoperformbetterinsmallerparametersettingswhencontrolling
forequaldepth,i.e.,keepingthenumberofattention,MLP,andMambablocksequivalent.
4.2 PerformancegapsinmorecomplexICLtasks
WealsoconsiderafamilyofmorecomplexICLtasks,namelylearningdecisiontree,sparseparity,
andChain-of-Thought(Figures2and4). ThefigureshowsthatTransformerscansolveDecision
TreeandVector-valuedMQAR,whileMambacannot. InSparseParitytaskofFigure5,however,
TransformerisunabletolearnthefunctionfamilywhileMambacan.
Filteringoutliersinregression. Orthogonal-outlierregressionandmany-outlierregression,like
other outlier tasks, focus on the model’s ability to learn to ignore dummy vectors, either by the
6S4 S4-Mamba MambaFormer Standard Mamba Transformer
Linear Regression ICL: eval at 30 examples Many-outlier Linear ICL: eval at 20th clean example
101 Sparse Linear Regression: eval at 10 examples
101
100
100
101
100
102
1015 1016 1017 1018 1017 1018
FLOPs FLOPs
Sparse Linear Regression: eval at 20 examples Chain-of-Thought-I/O: eval at 100 examples
100
100
101 1015 1016 1011017 1018
FLOPs
102
102
1015 1016 1017 1018 1015 1016 1017
FLOPs FLOPs
Decision Tree ICL: eval at 100 examples
2NN Regression ICL: eval at 100 examples
100
6×100
101
4×100
3×100
102
1015 1016 1017 1018 1016 1017 1018
FLOPs FLOPs
Figure2: AsuiteofICLtasksranforTransformer,Mamba,S4,andhybridarchitectureswhereeach
colorrepresentsadifferentarchitecture. Moretransparentpointsindicateearlierstagesoftraining;
plottedmodelsaretrainedinbetween[100k,500k]iterations. Werepresenteachmodelintermsof
itsnumberoffloatingpointoperations(FLOPs)usedfortraining.
factthatthex ∈w⊥,orbythefactthaty isavectorinsteadofazero-paddedscalarvalue. This
i i
explicitlyrequiresthemodelstolookatthepreviousinputsequences,anddiscovertheproperties
thatdistinguishthedummyvectorsfromtrainingexampleswhilelearningtheclassoffunctionsthe
trainingpromptrepresents.
Fororthogonal-outlierregressiontaskwitharelativelyshortsequencelengthof101(seeTable2for
taskdescriptions),MambaperformsonparwithTransformer,asseeninFigure3. Interestingly,for
many-outlierregressionwherewetestonasequencelengthof512and90%all-onesreplacement,
MambasignificantlyoutperformsTransformers. ThisisalsoinlinewithwhatGu&Dao(2023)
7
ssoL
erauqS
ssoL
erauqS
ssoL
erauqS
ssoL
erauqS
ssoL
erauqS
ssoL
erauqS
ssoL
erauqS101 Scaling Filter Orthogonal Regres Ms aio mn
ba
101 Scaling Filter Orthogonal Regres Ms aio mn
ba
Transformer Transformer
Standard Hybrid Standard Hybrid
MambaFormer MambaFormer
100
100
101
101
102
1016 1017 1018 1016 1017 1018
FLOPs FLOPs
Figure3: ScalingcurveswithrespecttothetotalnumberofFLOPsonOrthogonal-outlierRegression
Task.
report, inwhichMambafaresbetterfortheinductiontaskforlongsequencelengths. Thesetwo
resultsindicatethatMambahasnosignificantissuewithfilteringoutunnecessaryinformation,while
retainingtheabilitytolearnlinearregressionin-context.
tf_standard tf_small tf_tiny tf, k=4 tf, k=8 tf, k=16
mamba_standard mamba_small mamba_tiny mamba, k=4 mamba, k=8 mamba, k=16
Chain-of-Thought-I/O Chain-of-Thought-I/O
101 101
100 100
10 1 10 1
10 2 10 2
0 20 40 60 80 100 0 20 40 60 80 100
in-context examples in-context examples
Figure 4: Performance of Transformer and Mamba models on the Chain-of-Thought-I/O task.
Experimentsonvaryingthemodelsize(left)andvaryingthehiddendimension(right).
Chain-of-ThoughtI/O. Figure4showsthatMambamodelsarecapableofin-contextlearning
in a chain-of-thought manner, performing comparably to Transformer models across the tested
configurations. In smaller model configurations, Mamba models exhibit superior performance
comparedtoTransformermodels. However,asmodelsizeincreases,Transformermodelsbeginto
surpassMambamodels. TheperformanceofTransformermodelsremainsrelativelystableacross
differentproblemsizes,whileMambamodels’performanceissignificantlyinfluencedbythesizeof
thehiddenlayer. Specifically,MambamodelsexceloverTransformermodelsatsmallerproblem
sizes(i.e.,smallerhiddendimensions),buttheiradvantagediminishesastheproblemsizeexpands.
4.3 Challengesinparityandretrieval
We run vector-valued MQAR on two settings: (1) 32 key-value pairs with 16 queries and (2) 32
key-value pairs with 4 queries. From Table 3, we can see that Mamba fails to retrieve vectors
accuratelyasthemeansquarederrorforretrievingnormedvectorsaregreaterthan0.1inallcases.
Asasidenote, allmodelstrainedwith16querieshavelowertestlossthanmodelstrainedwith4
queries. Apossibleexplanationisthat,forasinglesequenceofdatathatrepresentsanMQARtask,
wecanthinkofeach(q,v)pairasa“trainingsample”,soasequencewith16queriescontainsmore
“trainingsamples”thanthatofasequencewith4queries. Thisalsoshowsthathavingmorequeries
doesnotnecessarilymakethetaskharder.
8
rorre
derauqs
tohs-03
@
ssoL
erauqS
tohs-05
@
ssoL
erauqS
rorre
derauqsEmbeddingdimension(d) 64 128
Mamba 7.23e-1 1.50e-1
6MambaBlocks+1Standard Hybrid 1.54e-3 5.86e-5
Transformerw/oPE 7.61e-5 5.55e-5
Transformerw/PE 3.99e-5 2.46e-7
MambaFormer 1.03e-5 3.79e-7
(a)32key-valuepairswith16queries.
Embeddingdimension(d) 64 128
Mamba 8.64e-1 1.64e-1
6MambaBlocks+1Standard Hybrid 1.99e-2 1.37e-2
Transformerw/oPE 1.14e-3 8.66e-5
Transformerw/PE 5.17e-6 8.76e-7
MambaFormer 7.30e-6 3.37e-6
(b)32key-valuepairswith4queries.
Table3: Testloss(meansquarederror)onvector-valuedMQARandrespectivemodelconfigurations.
WetestforbothTransformerwithPositionalEncoding(PE)andwithout. Allmodelshave4layers
withroughlythesamenumberofparameters.Anexceptionis“6MambaBlocks+1StandardHybrid”
model,butwestillconsideritas(equivalently)having4layerssince6MambaBlocksareequivalent
to3MambalayersasdescribedinFigure6.
Sparse Parity ICL Evaluation
Transformer
1.0
S4-Mamba
Mamba
0.8 S4
0.6
0.4
0 50 100
in-context examples
Figure5: AlthoughTransformerfailstoconverge,MambaandS4-Mambacanlearnsparseparity
ofd=10andk =2. Eachmodelistrainedwithanembeddingdimensionof256anddepthof12
layers(approximately10millionparameters)upto500kiterations. Transformerfailedtolearneven
uptoanembeddingdimensionof768and24layers.
WhileMambafailsonsimpleretrievaltaskssuchasMQAR,thetablesturnforthetaskoflearning
sparse parity (Figure 5). Transformer fails to do better than random guessing, in line with the
empiricalevidenceofBhattamishraetal.(2023). WeconfirmthisisthecaseforTransformersizesof
embeddingdimensionsupto768andupto24layerswhentrainedforatmost1millioniterations.
However,Mambasucceedsinthistaskwithease,solvingsparseparityfor(d,k)=(10,2)witha
networkassmallas2layers. Evenmoresurprisingly,S4-Mambaisabletosolveparityaswell;this
maymeanthatproperconvolutionorgatingmaybemoreimportantthaninput-dependentselection.
Our result hints at that the initial (causal) convolution that Mamba provides before the attention
layermaybecrucialtosolvingparities,asimilarphenomenonobservedforVisionTransformersin
computervisiontasks(Yuetal.,2022).
Itisknownthatanyalgorithmforlearningparitiesrequireseitherasuper-linearmemoryofω(d)ora
super-polynomialnumberofsamplesind(Raz,2016;Koletal.,2017). WhileTransformerisknown
tohavebettermemoryduetoitsquadraticattentionmechanism,ourresultsonlearningsparseparities
bringsforththequestiononhowdifferentarchitecturesmayutilizeitsmemorydifferentlyinterms
9
ycaruccAoffunctionapproximation. Weleavethetheoreticalandempiricalquestionofwhicharchitectural
componentallowsforlearningparitiesasanavenueforfurtherstudy.
5 TheAdvantageofHybridArchitecturesforIn-contextLearning
Outputs Outputs Outputs Outputs
Feed
Mamba Mamba Mamba
Forward
Multi-Head Multi-Head Multi-Head
Mamba
Attention Attention Attention
Positional Positional
Embedding Embedding
Mamba
Inputs Inputs Inputs
Inputs
(a) Transformer (b) Mamba (c) Standard Hybrid (d) MambaFormer
Figure6: ModelArchitectures. (a)and(b)denotethestandardTransformerandMambaarchitectures.
(c)denotesthehybridarchitectureofMambaandAttentionblocks,followingthedesignproposed
inGu&Dao(2023). (d)demonstratestheproposedarchitecture, namelyMambaFormer, which
replacesthePositionalEncodingwithaMambablock. Forconvenience,wedenote2blocksofeither
Mamba,Multi-headAttention,oraFeedForwardNetworkas1layer.
In the previous section, we have observed that Transformers perform better than SSMs in some
tasks,suchaslearningdecisiontreesorretrieval,whileSSMsexcelinothers,suchaslearningsparse
paritiesorlearningheavy-outlierlinearregression,possiblyduetoitsrecurrentnature. However,can
weachievethebestofbothworldswithoutsacrificingperformanceinoursuiteofICLtasks?
Weanswerthisintheaffirmative;thatwecanindeedreachcompetitiveperformanceinoursuiteofICL
tasks,achievingperformancecomparabletothatofTransformersandMamba,whilesimultaneously
excelling in specific tasks that either fail in. We can achieve strong performance by interleaving
AttentionandMamba,whereakeyingredientishavingMambaasthefirstlayer.
Inthissection,weinvestigatetwohybridarchitecturesthatcombineTransformerandMamba,namely
Standard HybridandMambaFormerasillustratedinFigure6. Standard Hybridisthearchitecture
ofinterleavingMHAandMambabyreplacingtheMLPblockwithMamba. MambaFormerisnearly
identicaltoStandard HybridbutwithanadditionalMambablockasitsinitiallayerandnoparticular
positionalencoding. Althoughmanyworkshavefoundthatinterleavingmulti-headattentionand
LTISSMsbeneficial(Zuoetal.,2022;Mehtaetal.,2022;Pilaultetal.,2023),interestinglyGu&
Dao(2023)havenotfoundsignificantbenefitsofinterleaving. Inthefollowingresults,weshowthat
interleavingwithMambaasitsinitiallayercanhelpsolvebothsparseparityandretrieval,eachtask
unsolvablebyMambaandTransformer.
5.1 Simultaneouslylearningparitiesandretrieval
AshighlightedinBhattamishraetal.(2023);Baraketal.(2022),learningsparseparityin-context
seemstobedifficultforTransformerandsomeSSMslikeHyena.Yetinterestingly,asseeninFigure7,
MambaFormersuccessfullylearnsparityasquicklyasMambaintermsofsamplecomplexity. While
theStandard Hybridmodelisalsocapable,itexhibitsmuchworsesampleefficiency.
WeperformanablationstudybyequippingTransformerwithaninitialMambablockwithoutany
positionalencoding. AlthoughthisvariantTransformeronlyhasfewerMambablocksthanStandard
Hybrid, itsolvesparityalmostasefficientlyasMamba. Notonlydoesthisshowusthatorderof
layersininterleavingmatter,asshowninPressetal.(2022),butalsothatMambacancomplement
TransformerwithouthurtingperformanceinICL.Thisresultbringsupintriguingdifferencebetween
thefunctionlearningcapabilitiesofAttentionandMamba;weleavethisquestionupforfurtherstudy.
10Sparse Parity ICL Convergence
500000
400000
300000
200000
100000
0
105 106
Total Parameters (non-embedding)
Transformer MambaFormer
Mamba TF w/ initial Mamba
Standard Hybrid
Figure7: Medianconvergencetimeoflearningparityover5randomseedsformax. 500kiterations,
where500kconvergencetimesignifiesfailedlearning. HavingtheinitiallayerasMambaisessential
forefficientlylearningparities. TestedmodelconfigurationsarespecifiedinAppendixA.
Closingthegapinretrieval. ThegapbetweenMambaandTransformerinvector-valuedMQAR
taskislargelyduetothefactthatMamba(asanSSM)compressescontextintosmallerstateswhen
generatingoutput,whiletheAttentionmechanisminTransformerdoesnotcompressthecontext. The
amountofinformationaboutthecontextMambahasateachstatedependsonthedimensionofhidden
state(asthehiddenstatescapturetheimportantinformationinthecontext)anditischallengingifthe
taskistoaccuratelyretrieveaspecificpartofthecontextbyaquerythatisplacedafterthecontext.
Toclosethegapinthevector-valuedMQARtaskbetweenMambaandTransformer,andwithout
sacrificingtoomuchoftheefficiency,weaddoneattentionlayerwithinlayersofMambablocks. In
particular,inaMambamodelof4layers(8Mambablocksstackedhomogeneously),wereplacethe
middletwoblockswithStandard Hybrid(w/opositionalembedding). AsshowninTable3,Mamba
modelgainsasignificantimprovementinvector-valuedMQARbyhavingoneStandard Hybrid. We
furthertestMambaFormeronthesametaskandfindthatMambaFormeralmostentirelyclosesthe
gaptotransformerinvector-valuedMQARtask.
5.2 All-in-oneICLperformance
WhileMambaFormersucceedsintwotasksthatwereeitherdeemeddifficultforMambaforTrans-
former,italsoperformsequallyaswellasTransformerandMambaintherestofoursuiteofICL
tasks. InFigure2,weseethatMambaFormerandStandard Hybridbothlearndecisiontreesaswell
asTransformer,evenatlargerparametersizes. Evenmoresurprisingly,MambaFormerefficiently
learnslinearregressionbetterthanbothmodelseveninthepresenceof90%noisydatainMany-
outlierregression,asaMambaFormertrainedon100kiterations(<1017FLOPs)performsaswell
asmodelstrainedwith10timesthenumberofFLOPs.
In conclusion, we find the best of both worlds within our diverse array of ICL tasks; a hybrid
architecturethatcansolveasdifficultproblemsasretrievalandparity,whileperformingonparwith
Transformer and Mamba in other ICL tasks. Given our results, it will be interesting to see how
hybridarchitecturesperforminotherkindsofICLtasks,suchasthosediscussedin(Xieetal.,2021;
Akyüreketal.,2024).
11
)snoitareti(
emiT
ecnegrevnoC
naideM6 Discussion
Inthiswork,wehaveprovidedacomprehensiveinvestigationofin-contextlearningwithstate-space
models(SSMs)andcontrastedthemwiththetransformerarchitecture. Ourstudyhasrevealedthat
SSMs,especiallyMamba,arecapablein-contextlearners.Ontheotherhand,ourevaluationsrevealed
thatneitherSSMsnortransformersaregreatatalltasks,specifically,SSMsstrugglewithdecision
treelearningandretrievaltaskswhereastransformersstrugglewithsparseparity. Thishasledus
tothehybridarchitectureMambaFormerwhichachievesabest-of-both-worldsperformanceonour
ICLsuite.
Futureresearchdirectionsincludeexploring(1)howperformanceonourICLsuitecorrelateswith
generallanguagemodelingcapabilities,suchasperplexityonstandardNLPbenchmarks,(2)the
potential for developing more effective architectures by integrating elements from transformers,
SSMs, and gating mechanisms, (3) identifying architectural features that contribute to effective
in-contextlearning,and(4)assessingtheimpactofMambaFormerandotherinnovativearchitectures
onlanguagemodelingperformance.
12References
Ahn,K.,Cheng,X.,Daneshmand,H.,andSra,S. Transformerslearntoimplementpreconditioned
gradientdescentforin-contextlearning. arXivpreprintarXiv:2306.00297,2023. 3
Akyürek,E.,Schuurmans,D.,Andreas,J.,Ma,T.,andZhou,D.Whatlearningalgorithmisin-context
learning? investigationswithlinearmodels. InTheEleventhInternationalConferenceonLearning
Representations,2022. 1,3
Akyürek,E.,Wang,B.,Kim,Y.,andAndreas,J. In-contextlanguagelearning: Architecturesand
algorithms. arXiv preprint arXiv:2401.12973, 2024. URL https://arxiv.org/abs/2401.
12973. 4,11
Arora,S.,Eyuboglu,S.,Timalsina,A.,Johnson,I.,Poli,M.,Zou,J.,Rudra,A.,andRé,C. Zoology:
Measuringandimprovingrecallinefficientlanguagemodels. arXivpreprintarXiv:2312.04927,
2023. URLhttps://arxiv.org/abs/2312.04927. 2,4,6
Bai,Y.,Chen,F.,Wang,H.,Xiong,C.,andMei,S. Transformersasstatisticians: Provablein-context
learningwithin-contextalgorithmselection. arXivpreprintarXiv:2306.04637,2023. 1
Barak, B., Edelman, B., Goel, S., Kakade, S., Malach, E., and Zhang, C. Hidden progress in
deeplearning: Sgdlearnsparitiesnearthecomputationallimit. AdvancesinNeuralInformation
ProcessingSystems,35:21750–21764,2022. 10
Beltagy,I.,Peters,M.E.,andCohan,A.Longformer:Thelong-documenttransformer.arXivpreprint
arXiv:2004.05150,2020. 3
Bhattamishra, S., Patel, A., Blunsom, P., and Kanade, V. Understanding in-context learning in
transformersandllmsbylearningtolearndiscretefunctions. arXivpreprintarXiv:2310.03016,
2023. URLhttps://arxiv.org/abs/2310.03016. 3,5,9,10
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,Dhariwal,P.,Neelakantan,A.,Shyam,
P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural
informationprocessingsystems,33:1877–1901,2020. 1
Chan,S.,Santoro,A.,Lampinen,A.,Wang,J.,Singh,A.,Richemond,P.,McClelland,J.,andHill,
F. Datadistributionalpropertiesdriveemergentin-contextlearningintransformers. Advancesin
NeuralInformationProcessingSystems,35:18878–18891,2022. 1
Dai,D.,Sun,Y.,Dong,L.,Hao,Y.,Ma,S.,Sui,Z.,andWei,F. Whycangptlearnin-context? lan-
guagemodelssecretlyperformgradientdescentasmeta-optimizers. InFindingsoftheAssociation
forComputationalLinguistics: ACL2023,pp.4005–4019,2023. 3
Dao, T., Fu, D. Y., Saab, K. K., Thomas, A. W., Rudra, A., and Ré, C. Hungry hungry hippos:
Towardslanguagemodelingwithstatespacemodels. arXivpreprintarXiv:2212.14052, 2022.
URLhttps://arxiv.org/abs/2212.14052. 2,3
Gao,L.,Biderman,S.,Black,S.,Golding,L.,Hoppe,T.,Foster,C.,Phang,J.,He,H.,Thite,A.,
Nabeshima,N.,Presser,S.,andLeahy,C. Thepile: An800gbdatasetofdiversetextforlanguage
modeling,2020. 2
Garg, S., Tsipras, D., Liang, P. S., and Valiant, G. What can transformers learn in-context? a
casestudyofsimplefunctionclasses. AdvancesinNeuralInformationProcessingSystems,35:
30583–30598,2022. 1,2,3,5,16
Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv
preprintarXiv:2312.00752,2023. URLhttps://arxiv.org/abs/2312.00752. 1,2,3,4,5,7,
10,16
Gu, A., Goel, K., Gupta, A., and Ré, C. On the parameterization and initialization of diagonal
state space models. In Advances in Neural Information Processing Systems, volume 35, pp.
35971–35983,2022a. 3,4
13Gu, A., Goel, K., and Ré, C. Efficiently modeling long sequences with structured state spaces.
InTheTenthInternationalConferenceonLearningRepresentations,ICLR2022,2022b. URL
https://openreview.net/forum?id=uYLFoz1vlAC. 2,3
Kaplan,J.,McCandlish,S.,Henighan,T.,Brown,T.B.,Chess,B.,Child,R.,Gray,S.,Radford,A.,
Wu,J.,andAmodei,D.Scalinglawsforneurallanguagemodels.arXivpreprintarXiv:2001.08361,
2020. 1,5
Katharopoulos,A.,Vyas,A.,Pappas,N.,andFleuret,F. Transformersarernns: Fastautoregressive
transformerswithlinearattention.InProceedingsofthe37thInternationalConferenceonMachine
Learning,ICML2020,volume119ofProceedingsofMachineLearningResearch,pp.5156–5165,
2020. URLhttp://proceedings.mlr.press/v119/katharopoulos20a.html. 3
Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014. 4,16
Kol,G.,Raz,R.,andTal,A. Time-spacehardnessoflearningsparseparities. InProceedingsofthe
49thAnnualACMSIGACTSymposiumonTheoryofComputing,pp.1067–1080,2017. 9
Li,Y.,Ildiz,M.E.,Papailiopoulos,D.,andOymak,S. Transformersasalgorithms: Generalization
andstabilityinin-contextlearning,2023a. 1
Li,Y.,Sreenivasan,K.,Giannou,A.,Papailiopoulos,D.,andOymak,S. Dissectingchain-of-thought:
Compositionalitythroughin-contextfilteringandlearning. InThirty-seventhConferenceonNeural
InformationProcessingSystems,2023b. 1,6,16
Mahankali,A.,Hashimoto,T.B.,andMa,T. Onestepofgradientdescentisprovablytheoptimal
in-contextlearnerwithonelayeroflinearself-attention. arXivpreprintarXiv:2307.03576,2023. 3
Mehta,H.,Gupta,A.,Cutkosky,A.,andNeyshabur,B. Longrangelanguagemodelingviagated
statespaces. arXivpreprintarXiv:2206.13947,2022. 10
Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H. Metaicl: Learning to learn in context.
In Proceedings of the 2022 Conference of the North American Chapter of the Association for
ComputationalLinguistics: HumanLanguageTechnologies,pp.2791–2809,2022a. 1,3
Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., andZettlemoyer, L. Re-
thinking the role of demonstrations: What makes in-context learning work? arXiv preprint
arXiv:2202.12837,2022b. URLhttps://arxiv.org/abs/2202.12837. 1
Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Tazi, N., Piktus, A., Pyysalo, S., Wolf,
T.,andRaffel,C. Scalingdata-constrainedlanguagemodels. InThirty-seventhConferenceon
NeuralInformationProcessingSystems, 2023. URLhttps://openreview.net/forum?id=
j5BuTrEj35. 1
Olsson,C.,Elhage,N.,Nanda,N.,Joseph,N.,DasSarma,N.,Henighan,T.,Mann,B.,Askell,A.,
Bai,Y.,andetal.,A.C. In-contextlearningandinductionheads. arXivpreprintarXiv:2209.11895,
2022. URLhttps://arxiv.org/abs/2209.11895. 3,4,6
Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M.,
Grella,M.,Kiran,K.G.,etal. Rwkv: Reinventingrnnsforthetransformerera. arXivpreprint
arXiv:2305.13048,2023. URLhttps://arxiv.org/abs/2305.13048. 2,4
Pilault,J.,Fathi,M.,Firat,O.,Pal,C.,Bacon,P.-L.,andGoroshin,R. Block-statetransformers. In
Thirty-seventhConferenceonNeuralInformationProcessingSystems,2023. 10
Poli,M.,Massaroli,S.,Nguyen,E.,Fu,D.Y.,Dao,T.,Baccus,S.,Bengio,Y.,Ermon,S.,andRé,C.
Hyenahierarchy:Towardslargerconvolutionallanguagemodels.arXivpreprintarXiv:2302.10866,
2023. URLhttps://arxiv.org/abs/2302.10866. 2,4
Press,O.,Zhang,M.,Min,S.,Schmidt,L.,Smith,N.A.,andLewis,M. Measuringandnarrowing
thecompositionalitygapinlanguagemodels. arXivpreprintarXiv:2210.03350,2022. 10
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are
unsupervisedmultitasklearners. OpenAIblog,1(8):9,2019. 16
14Ravi,S.andLarochelle,H.Optimizationasamodelforfew-shotlearning.InInternationalconference
onlearningrepresentations,2016. 1,3
Raz,R. Fastlearningrequiresgoodmemory: Atime-spacelowerboundforparitylearning. In2016
IEEE57thAnnualSymposiumonFoundationsofComputerScience(FOCS),pp.266–275.IEEE,
2016. 9
Schaeffer, R., Miranda, B., and Koyejo, S. Are emergent abilities of large language models a
mirage? InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023. URL
https://openreview.net/forum?id=ITw9edRDlD. 1
Schmidhuber,J.,Zhao,J.,andWiering,M. Shiftinginductivebiaswithsuccess-storyalgorithm,
adaptivelevinsearch,andincrementalself-improvement. MachineLearning,28:105–130,1997. 1
Sun,Y.,Dong,L.,Huang,S.,Ma,S.,Xia,Y.,Xue,J.,Wang,J.,andWei,F. Retentivenetwork: A
successortotransformerforlargelanguagemodels. arXivpreprintarXiv:2307.08621,2023. URL
https://arxiv.org/abs/2307.08621. 2,4
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,
L., and Polosukhin, I. Attention is all you need. In Advances in Neural Informa-
tion Processing Systems 30: Annual Conference on Neural Information Processing Systems
2017,pp.5998–6008,2017. URLhttps://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. 1
vonOswald,J.,Niklasson,E.,Randazzo,E.,Sacramento,J.,Mordvintsev,A.,Zhmoginov,A.,and
Vladymyrov,M. Transformerslearnin-contextbygradientdescent. InInternationalConference
onMachineLearning,pp.35151–35174.PMLR,2023a. 1
vonOswald,J.,Niklasson,E.,Schlegel,M.,Kobayashi,S.,Zucchet,N.,Scherrer,N.,Miller,N.,
Sandler,M.,Vladymyrov,M.,Pascanu,R.,etal. Uncoveringmesa-optimizationalgorithmsin
transformers. arXivpreprintarXiv:2309.05858,2023b. URLhttps://arxiv.org/abs/2309.
05858. 1,3
Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear
complexity. arXivpreprintarXiv:2006.04768,2020. 3
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M.,
Zhou, D., Metzler, D., et al. Emergent abilities of large language models. arXiv preprint
arXiv:2206.07682,2022. 1
Xie,S.M.,Raghunathan,A.,Liang,P.,andMa,T. Anexplanationofin-contextlearningasimplicit
bayesianinference. InInternationalConferenceonLearningRepresentations,2021. 4,11
Yang,L.,Lee,K.,Nowak,R.,andPapailiopoulos,D. Loopedtransformersarebetteratlearning
learningalgorithms,2023a. 1
Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with
hardware-efficienttraining. arXivpreprintarXiv:2312.06635,2023b. 2,4
Yu,W.,Luo,M.,Zhou,P.,Si,C.,Zhou,Y.,Wang,X.,Feng,J.,andYan,S. Metaformerisactually
whatyouneedforvision. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pp.10819–10829,2022. 9
Zuo,S.,Liu,X.,Jiao,J.,Charles,D.,Manavoglu,E.,Zhao,T.,andGao,J. Efficientlongsequence
modelingviastatespaceaugmentedtransformer. arXivpreprintarXiv:2212.08136,2022. 10
15A ExperimentalSetup
A.1 ModelArchitectures
We focus on decoder-only Transformer models, particularly those from the GPT-2 family (Rad-
ford et al., 2019), Mamba (Gu & Dao, 2023), and their Hybrid variants, including Standard and
MambaFormerconfigurations. Thesemodelsareevaluatedacrossarangeofsizes,asdetailedin
Table4. TransformerlayersconsistofaMulti-HeadAttention(MHA)blockfollowedbyaMultilayer
Perceptron(MLP)block.MambamodelsconsistoftwoMambablocksperlayer.TheHybridvariants
mergetheseapproaches,combiningasingleMHAblockwithaMambablock. ForMHAblocks,we
use8numberofheads. RefertoFigure6foravisualizationofthearchitecturesconsidered.
A.2 ModelTraining
WetrainallofourmodelsonA100-SXM4-40GBGPUsfor500,000trainingstepsonalltasks. We
use Adam optimizer Kingma & Ba (2014) with a fixed learning rate. The default value is set to
0.0001,followingthedefaultlearningrateinGargetal.(2022),andsearchvariouslearningrates
in{5e−5,1e−4,2e−4,4e−4}. Weobservethatthetrainingprocedureisthemostsensitiveto
choosingtherightlearningrate. Inparticular,asthenumberofparametersofthemodelsincreases,
thetrainingprocedureispronetogradientexplosions,especiallyinMambaandhybridarchitecutres.
Hence,weclipthegradientnorm,withvaluesin{5.0,10.0,50.0}.
Asforthetrainandtestdata,wefixthedimensionofxtobe20,andfixthebatchsizetobe64. As
suggestedinGargetal.(2022),wealsoobservethatcurriculumiscrucialincertainICLtasks. We
adoptacurriculumof15stepsevery2000stepsbothonthedimensionofxandthenumberofpoints
(halfthelengthofthetrainingprompt).
Table4: DifferentconfigurationsofModels.
#layers embeddim
Standard 12 768
Small 8 512
X-small 4 256
XX-small 2 128
B ImplementationDetails
ThissectionfurtherelaboratesonthetaskdescriptionsfromSection3.
B.1 Chain-of-Thought-I/Oresults
Table5presentstheconfigurationsfortheChain-of-Thought-I/Otaskusinga2-layerReLUneural
network,followingthesetupdescribedbyLietal.(2023b). Inthemodelscaleexperiment,theinput
dimensiond=10andhiddenlayerdimensionk =8areheldconstantwhilevaryingthemodelscale.
Additionally,thehiddendimensionkisvariedamong4,8,16whilefixingthemodelscaletosmall
toidentifytheeffectofproblemscale.
Table5: ModelconfigurationsforChain-of-Thought-I/Oexperiments.
Model #layers embeddim #heads(MHA)
Standard 12 256 8
Small 6 128 4
Tiny 3 64 2
16B.2 Vector-valuedMQAR
Thetrainingsetconsistsof300,000trainingsamples. Wetrainfor64epochswithbatchsizeof
64andevaluateonatestsetof3,000samples. Foreachsetting,wesweepwithlearningratesin
np.logspace(-4,-2,4)andreportthebestresultamongalllearningrates.
B.3 Orthogonal-outlierRegression
We run Orthogonal-outlier regression on all four model architectures, with varying number of
parameters. ThecurvesabovegenerallycapturethetrendthatMambaandTransformerperformon
parwitheachother,andStandardHybridandMambaFormershowbettercurvescomparedtothe
vanillamodels.
OnecurvethatstandsoutwouldbeMamba’slosscurveinthesmallerregime. Asafuturedirection,
wehavetestedtradingoffthedepthandwidthwhilefixingthetotalnumberofparameterswiththe
smallestMambamodelconfigurationinTable6.
Table6: VariousMambamodelconfigurationswithfixednumberofparameters.
Model #layers embeddim
Standard 4 128
2 192
8 96
16 64
Thebestperformingmodelconfigurationwaswith16layers,andembeddingdimensionof64. This
suggeststhatwithfixednumberofparameters,increasingthedepthmayboostthedownstreamtask
performanceonICLtasks.
C FLOPscomputation
We count the number of multiplications in a Mamba block and a Transformer block in Table 7
and Table 8. We assume batch size B = 1. To calculate FLOPs, we multiply the number of
multiplicationsby6toaccountforthemultiply-accumulatecostinbothforwardandbackwardpass.
NotethataStandard HybridblockisanattentionblockstackedwithaMambablock,sothenumber
ofmultiplicationsinaStandard Hybridblockis10LD2+2L2D,ignoringthelinearterms.
Table7: NumberofmultiplicationsinaTransformerblock.
Numberofmultiplications
QKVprojection 3LD2
OuterproductandmultiplyV 2L2D
Outerprojection LD2
FFNwithffw_width=4 8LD2
Table8: NumberofmultiplicationsinaMambablock. Unlessspecifiedotherwise,weassumeE =2
andR=2.
Numberofmultiplications
Inputprojection 2LED2
SSM 7LEDN +2RLED
Outputprojection LED2
17