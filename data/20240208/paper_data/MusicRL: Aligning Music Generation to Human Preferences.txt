2024-2-7
MusicRL: Aligning Music Generation to Human
Preferences
GeoffreyCideron,SertanGirgin,MauroVerzetti,DamienVincent,MatejKastelic,Zal√°nBorsos,Brian
McWilliams,VictorUngureanu,OlivierBachem,OlivierPietquin1,MatthieuGeist1,L√©onardHussenot,
NeilZeghidour2 andAndreaAgostinelli
GoogleDeepMind
WeproposeMusicRL,thefirstmusicgenerationsystemfinetunedfromhumanfeedback. Appreciation
oftext-to-musicmodelsisparticularlysubjectivesincetheconceptofmusicalityaswellasthespecific
intention behind a caption are user-dependent (e.g. a caption such as ‚Äúupbeat workout music‚Äù can
map to a retro guitar solo or a technopop beat). Not only this makes supervised training of such
modelschallenging,butitalsocallsforintegratingcontinuoushumanfeedbackintheirpost-deployment
finetuning. MusicRLisapretrainedautoregressiveMusicLM(Agostinellietal.,2023)modelofdiscrete
audiotokensfinetunedwithreinforcementlearningtomaximizesequence-levelrewards. Wedesign
rewardfunctionsrelatedspecificallytotext-adherenceandaudioqualitywiththehelpfromselected
raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect
a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from
HumanFeedback(RLHF),wetrainMusicRL-U,thefirsttext-to-musicmodelthatincorporateshuman
feedbackatscale. HumanevaluationsshowthatbothMusicRL-RandMusicRL-Uarepreferredtothe
baseline. Ultimately,MusicRL-RUcombinesthetwoapproachesandresultsinthebestmodelaccording
tohumanraters. Ablationstudiesshedlightonthemusicalattributesinfluencinghumanpreferences,
indicatingthattextadherenceandqualityonlyaccountforapartofit. Thisunderscorestheprevalenceof
subjectivityinmusicalappreciationandcallsforfurtherinvolvementofhumanlistenersinthefinetuning
ofmusicgenerationmodels. Samplescanbefoundatgoogle-research.github.io/seanet/musiclm/rlhf/.
1. Introduction
Generativemodellingofmusichasexperiencedaleapforward: whileitwasuntilrecentlyeitherlimited
tothefinemodellingofindividualinstruments(D√©fossezetal.,2018;Engeletal.,2017,2020)orthe
coarse generation of polyphonic music (Dhariwal et al., 2020), models can now handle open-ended,
high-fidelity text-controlled music generation (Agostinelli et al., 2023; Copet et al., 2023; Forsgren
andMartiros,2022;Liuetal.,2023). Inparticular,text-to-musicsystemssuchasMusicLM(Agostinelli
et al., 2023) and MusicGen (Copet et al., 2023) build on audio language models, as they cast the
generativeprocessasanautoregressivepredictiontaskinthediscreterepresentationspaceofaneural
audiocodec(D√©fossezetal.,2022;Zeghidouretal.,2022). Whilethisapproachhasdemonstratedits
ability to generate realistic speech (Borsos et al., 2023a,b; Wang et al., 2023), sound events (Kreuk
et al., 2022) and music, it suffers from a few shortcomings. First, the next-token prediction task used
to train these systems ‚Äî while generic enough to model arbitrary audio signals ‚Äî lacks any prior
knowledge about musicality that could bias those towards generating music that is more appealing
to listeners. Second, while the temperature sampling used at inference allows for generating diverse
audio from a single text caption, this diversity is only desirable along certain axes such as melody
or performance, while musicality and adherence to the prompt should remain consistently high.
1NowatCohere
2NowatKyutai
Correspondingauthor(s):gcideron@google.com,agostinelli@google.com
4202
beF
6
]GL.sc[
1v92240.2042:viXraMusicRL:AligningMusicGenerationtoHumanPreferences
Win Tie Loss
MusicRL-R vs. MusicLM 65.0 22.1 12.9
MusicRL-U vs. MusicLM 58.6 21.2 20.2
MusicRL-U vs. MusicRL-R 36.0 33.0 31.0
MusicRL-RU vs. MusicLM 66.7 22.9 10.4
MusicRL-RU vs. MusicRL-U 43.9 29.7 26.4
MusicRL-RU vs. MusicRL-R 42.6 35.6 21.8
Figure 1 | Results of the qualitative side-by-side evaluation for the RLHF finetuned models. In each X
vs. Y comparison, the green bar corresponds to the percentage of times model X was preferred, the
yellowbartothepercentageoftiesandtheredbartothepercentageoftimesmodelYwaspreferred.
MusicRL-R is the MusicLM model finetuned on quality and text adherence reward. MusicRL-U is
finetunedonarewardmodelofuserpreferences. MusicRL-RUisfinetunedsequentiallyonqualityand
adherencetotextandthenonarewardmodelofuserpreferences. WhileeveryRLHFfinetunedversion
of MusicLM significantly outperforms MusicLM, MusicRL-R and MusicRL-U achieve comparable
performance, while MusicRL-RU is overall the preferred model.
These fundamental issues of autoregressive generative models have been extensively observed and
addressedinthecontextoflanguagemodelling. Forexample,severalworkshaveexploredfinetuning
machine translation models to maximise the BLEU score (Ranzato et al., 2016; Wu et al., 2016)
or summarization models to improve the relevant ROUGE metric (Ranzato et al., 2016; Roit et al.,
2023; Wu and Hu, 2018). Such metrics are typically sequence-level, and evaluate the output of a
non-differentiable sampling process (e.g., greedy decoding, temperature sampling). This is typically
circumvented by using a reinforcement learning method which models the metric of interest of a
reward function and the generative model as a policy. The underlying algorithmic similarity between
such text generation systems and autoregressive music models suggests that ‚Äî given the proper
reward functions‚Äî one could use reinforcement learning to improve music generation.
Music generated given a prompt should exhibit three properties: adherence to the input text, high
acoustic quality (absence of artifacts), and ‚Äúmusicality‚Äù or general pleasantness. Automatic metrics
have been proposed to quantify the text adherence like Classifier KLD (Yang et al., 2022) or MuLan
Cycle Consistency (Agostinelli et al., 2023) as well as acoustic quality with Fr√©chet Audio Distance
(Kilgour et al., 2019). Such metrics could be used as reward functions. Yet, designing automatic
proxiestomeasuremusicalityischallenging. Mostofthepreviousapproaches(Guimaraesetal.,2017;
Jaquesetal.,2017;Kotecha,2018;Latifetal.,2023)relyoncomplexmusictheoryrules,arerestricted
tospecificmusicaldomains(e.g.,classicalpiano)andonlypartiallyalignwithhumanpreferences. This
gapbetweenautomaticmetricsandhumanpreferenceshasagainbeenextensivelystudiedinlanguage
modelling,withRLHF(ReinforcementLearningfromHumanPreferences)becomingthedefactoway
of aligning conversational models (Achiam et al., 2023; Team et al., 2023) with human feedback.
Human preferences as referred in previous work (Ouyang et al., 2022; Stiennon et al., 2020) mainly
referstothepreferencesofraters. Ratersmaynotberepresentativeofthepopulationinteractingwith
themodel(e.g. ratingservicessuchasAmazonMechanicalTurk3 usesaglobalworkforce). Especially
in the context of music, this population gap can have a significant impact on the preferences (Trehub
et al., 2015). Collecting large scale user preferences data could help bridge the population gap by
3https://www.mturk.com/
2MusicRL:AligningMusicGenerationtoHumanPreferences
collecting considerably more interactions in contrast with raters.
In this work, we introduce MusicRL, a text-to-music generative model finetuned with reinforcement
learning. StartingfromaMusicLMbaseline,weuseanautomaticmeasureoftextadherenceaswellasa
newacousticfidelitymetricasrewardfunctionstoperformRLfinetuning. Humanevaluationsindicate
that generations from the resulting MusicRL-R are preferred over those from MusicLM 83% of the
time, as measured by ùë§ùëñùëõ/(ùë§ùëñùëõ+ùëôùëúùë†ùë†). Then, to explicitly align the model with human judgment, we
collectadatasetofpairwisepreferencesfromusersinteractingwithMusicLMtofitarewardmodel. Ab-
lationstudiesontherewardmodeltrainedonuserinteractiondatademonstratethatuserpreferences
stronglycorrelatewithmusicality. Extensivehumanevaluationsrevealthatthemusicgenerationscom-
ingfromtheresultingMusicRL-Uarepreferredoverthebasemodel74%ofthetime. Finally,wecom-
bineautomaticrewardsandhumanfeedbacktofinetuneMusicRL-RintoMusicRL-RUandshowthat
thismodeloutperformsallalternativesmorethan62%ofthetime. Tothebestofourknowledge,this
workisthefirstattemptatleveraginghumanfeedbackatscaletoimproveanaudiogenerativemodel.
2. Related Work
Music generation. While earlier approches to musical audio generation were limited in terms
of producing high quality outputs (Dhariwal et al., 2020) or semantically consistent long audios
(Hawthorne et al., 2022), recent research has achieved a level of quality that allows for an enjoyable
listeningexperience. Afirstlineofworkcaststhetaskofmusicgenerationascategoricalpredictionin
the discrete token space provided by a neural audio codec (D√©fossez et al., 2022; Zeghidour et al.,
2022),andtrainsaTransformer-based(Vaswanietal.,2017)modelfornexttokenprediction(Borsos
etal.,2023a)orparalleltokendecoding(Borsosetal.,2023b;Garciaetal.,2023;Parkeretal.,2024).
Combiningthisgenerativebackbonewithtext-conditioningeitherthroughatextencoderortext-audio
embeddings (Elizalde et al., 2022; Huang et al., 2022) provides high-quality text-to-music models
(Agostinellietal.,2023;Copetetal.,2023). Aparallellineofworkreliesondiffusionmodelsandcasts
the task of music generation as denoising of audio waveforms and spectrograms (Huang et al., 2023)
or learned latent representations (Liu et al., 2023; Schneider et al., 2023). In both cases, the models
are trained offline on a collection of existing musical recordings and inference is run in a stochastic
fashion (e.g. diffusion or temperature sampling), which provides diversity but also uncertainty on
the outputs (e.g. in terms of text-adherence or quality). Previous work (Kharitonov et al., 2023) has
circumvented this issue by sampling many sequences, ranking them with a score function (e.g. a
reference-free audio quality estimator) and returning the best candidate. This considerably increases
inference cost and requires well-defined score functions.
MusicRL addresses these limitations by finetuning a MusicLM (Agostinelli et al., 2023) model with
reinforcement learning, using reward functions derived from automatic metrics, small scale high-
quality human ratings, and large scale user feedback. To the best of our knowledge, MusicRL is the
first music generation system that shows the benefits from integrating feedback from hundreds of
thousands of users.
RL-finetuning of music generation models. Most previous works in RL-finetuning music generation
models involve designing handmade reward signals based on principles of music theory (Guimaraes
et al., 2017; Jaques et al., 2017; Kotecha, 2018; Latif et al., 2023) or simple patterns like repetitions
(Karbasietal.,2021). Jaquesetal.(2017)usesasetofrulesinspiredbyamelodiccompositiontheory
(Gauldin, 1988) (e.g., stay in key, play motifs and repeat them, avoid excessively repeating notes)
in combination with a KL regularization term. These approaches have several limitations: the rule
sets can be incomplete or contradictory, practitioners must find the correct balance between different
rewards, and the rules themselves derive from music theory, which is an imperfect approximation of
3MusicRL:AligningMusicGenerationtoHumanPreferences
Figure2 | Givenadatasetofmusiccaptions,MusicLMgeneratesaudiosamplesthatarescoredwitha
reward function. The RL algorithm finetune the model to maximise the received reward.
human musical preferences. Jiang et al. (2020) finetune an online music accompaniment generation
modelwithfourrewardmodelslearnedfromdataandrule-basedrewardthatassign-1whenanoteis
excessively repeated. Each reward model corresponds to the probability of a chunk of the generation
given a context (the context and the chunk to predict is different for each reward). These rewards
are learned with a masked language model (Devlin et al., 2019) loss on a music dataset. Yet, such
methodsonlyapplytorestrictedmusicaldomains(e.g. monophonicpiano)orsymbolicgeneration. In
contrastwithpreviouswork,MusicRLlearnshumanpreferencesfromitsownrawaudiogenerations.
This allows for improving music generation across the whole spectrum of musical genres and styles,
from lo-fi hip-hop to orchestral symphonies and modal jazz.
RL from human feedback. RLHF recently became a critical step in the training of conversational
models used in applications such as Bard (Gemini Team, 2023) or GPT-4 (OpenAI, 2023). RLHF has
firstbeenappliedtosolveAtarigames(Christianoetal.,2017)beforebeingusedwidely,forexample
in natural language tasks (Bai et al., 2022; Jaques et al., 2019; Ouyang et al., 2022; Stiennon et al.,
2020; Ziegler et al., 2019) or in image generation (Lee et al., 2023; Wallace et al., 2023). Wallace
et al. (2023) uses Direct Optimisation Algorithm (DPO) (Rafailov et al., 2023) to finetune a diffusion
model on human preference data. To the best of our knowledge, we are the first to apply RLHF to
music generation models.
3. Method
3.1. MusicLM
MusicLM (Agostinelli et al., 2023) is an autoregressive model for generating music from text de-
scriptions. Following the design of AudioLM (Borsos et al., 2023a), MusicLM relies on two different
types of audio representations for generation: semantic tokens, which are quantized representations
of masked audio language models such as w2v-BERT (Chung et al., 2021) and acoustic tokens, the
discrete representations produced by neural audio codecs such as SoundStream (Zeghidour et al.,
2022). Whilethesemantictokensensurethelong-termstructuralcoherenceofthegenerationprocess,
the acoustic tokens allow for high-quality synthesis. To ensure high-bitrate reconstructions, Sound-
Stream uses residual vector quantization (RVQ) ‚Äî a stack of vector quantizers where each quantizer
operates on the residual produced by the previous quantizers ‚Äî to discretize the continuous audio
representations, imposing a hierarchical structure on the acoustic tokens. Additionally, MusicLM
reliesonMuLan(Huangetal.,2022),ajointmusic-textcontrastivemodel,forconditioningtheaudio
generation task on descriptive text.
MusicLM was initially introduced as a 3-stage Transformer-based autoregressive model. The first
stage learns the mapping between MuLan and semantic tokens. The second stage predicts the first
4MusicRL:AligningMusicGenerationtoHumanPreferences
levels from the output of the SoundStream RVQ (coarse acoustic tokens) from MuLan and semantic
tokens. The last stage predicts the remaining SoundStream RVQ levels (fine acoustic tokens) from
coarse acoustic tokens.
For the purpose of RL finetuning, we choose to optimise the semantic and coarse acoustic modelling
stages, which are the most important contributors to acoustic quality, adherence to the text and
overall appeal of the generated music. We address the challenges of jointly optimizing semantic and
coarse acoustic modelling by using a single autoregressive stage that operates on frame-interleaved
semanticandacoustictokens. WhilesimplifyingtheRLsetupandproblemformulation,thisapproach
increasesmodeledtokensequencelength. Weaddressthiswithahierarchicaltransformer,similarlyto
Lee et al. (2022); Yang et al. (2023); Yu et al. (2023). Finally, instead of the original autoregressive
fine acoustic modelling stage of MusicLM, we use Soundstorm (Borsos et al., 2023b) for achieving
efficient parallel generation.
For simplicity, by referring to MusicLM in this work, we refer only to the autoregressive modelling
stage of interleaved semantic and coarse acoustic tokens, which is the text conditioned modelling
stage that can be finetuned with RL.
3.2. RL finetuning procedure
We use the standard formulation of RL in the context of finetuning large language models as done in
previouswork(Ziegleretal.,2019). Figure2illustratestheRLtrainingloop. Theagentactsaccording
toitspolicyùúã ùúÉ withùúÉtheweightsthatparameterizethepolicy. Thepolicyisanautoregressivemodel
taking as input ùëé 0,...,ùëé ùë°‚àí1, the sequence of previously generated tokens and outputs a probability
distributionoverthenextaction,i.e.,thenexttokentopick: ùëé ùë° ‚àº ùúã ùúÉ(.|ùëé 0...ùëé ùë°‚àí1). TheRLfinetuning
phaseaimsatmaximizingùîºùúã ùúÉ[(cid:205) ùë°ùëü(ùëé 0...ùëé ùë°)] withùëüagivenrewardfunction. WeuseaKLregularized
version of the REINFORCE algorithm (Jaques et al., 2017; Williams, 1992) to update the policy
weights. Given a trajectory (ùëé ùë°) ùë°ùëá
=0
and denoting ùë† ùë° = (ùëé 0...ùëé ùë°‚àí1), the corresponding policy gradient
objective to maximise is
ùëá ùëá
‚àëÔ∏Å ‚àëÔ∏Å
ùïÅ(ùúÉ) = (1‚àíùõº)[ logùúã ùúÉ(ùëé ùë°|ùë† ùë°)( ùëü(ùë† ùëñ)‚àíùëâ ùúô(ùë† ùë°))]
ùë°=0 ùëñ=ùë°
ùëá
‚àëÔ∏Å‚àëÔ∏Å
‚àíùõº [log(ùúã ùúÉ(ùëé|ùë† ùë°)/ùúã
ùúÉ
(ùëé|ùë† ùë°))],
0
ùë°=0 ùëé‚ààùê¥
with ùê¥ the action space which here corresponds to the codebook, ùõº the KL regularization strength,
and ùëâ ùúô the baseline. The baseline value function ùëâ ùúô is used to decrease the variance in the policy
gradient objective (Sutton and Barto, 2018) and it is trained to estimate the mean return of the
current policy. The baseline is learned as follows:
ùëá
‚àëÔ∏Å ‚àëÔ∏Å
minùîºùúã ( ùëü(ùë† ùëò)‚àíùëâ ùúô(ùë† ùë°))2.
ùúô ùúÉ
ùë° ùëò=ùë°
BoththepolicyandthevaluefunctionareinitializedfromtheinitialMusicLMcheckpointwithweight
ùúÉ .
0
3.3. Reward Signals
Text adherence. We derive a reward model for text adherence from pretrained MuLan (Huang
et al., 2022) embeddings. MuLan is a contrastive audio-text embedding model trained on music clips
5MusicRL:AligningMusicGenerationtoHumanPreferences
Figure 3 | The AI Test Kitchen MusicLM interface. The user can write a prompt or choose from
suggestions. Each prompt generates two 20s clips, and the user can label their favorite clip among
the two with a trophy.
and weakly-associated, free-form text annotations. We compute the cosine similarity between the
text embedding of the input prompt and the audio embedding of the generated music, resulting in
a reward value in [‚àí1;1]. We refer to this metric as MuLan score. Because our models generate
30-second audios, while MuLan is trained on 10-second audio clips, we divide each audio into three
segments, we calculate MuLan scores for each segment, and we average the results.
Acoustic quality. Another main attribute of musical generation is acoustic quality, e.g. whether a clip
sounds like a professional recording or is contaminated with artifacts. We rely on a reference-free
quality estimator trained to predict the human Mean Opinion Score (MOS - between 1 and 5) of a
20 second music clip. We train the model on a mix of human-created and MusicLM-generated music
clips, whereeachclipwasratedby3raters. Theraters weretaskedtojudgeonlytheacousticquality,
to avoid confounding factors such as musicality. We refer to this metric as the quality score. Because
our models generate 30-second clips, we compute quality scores on the first 20 seconds and on the
last 20 seconds, and average the two scores.
User preferences. We deploy the pretrained text-to-music MusicLM model through the AITK web-
based interface 4 to a large scale userbase. We choose to collect feedback through pairwise compar-
isons(Christianoetal.,2017): whenauserseizesaprompt,wegeneratetwo20scandidateclipsand
lettheuseroptionallyassignatrophytooneofthem. Animportantdesignchoiceimpliedbythispro-
cessistheabsenceofspecificinstructions,whichisintendednottobiasuserstowardsprecisemusicalat-
tributesandrathercommunicatetheiroverallsubjectivetaste. Weonlyconsiderpreferencesfromusers
thatlistentobothgenerations. Afterfiltering,weobtainadatasetofpairwiseuserdataofsize300,000.
This dataset minimizes the biases that often arise from human raters (as detailed in Appendix C).
Our reward model takes as input the caption‚Äôs text and corresponding audio tokens and outputs
a scalar score. This model is trained with a Bradley-Terry Model (Bradley and Terry, 1952) as in
4https://aitestkitchen.withgoogle.com/
6MusicRL:AligningMusicGenerationtoHumanPreferences
Christiano et al. (2017), which enables learning a pointwise ELO score from pairwise preferences.
It is initialized with the MusicLM checkpoint, as first results demonstrated that, starting from scratch,
the reward model was not able to do better than chance at predicting human preferences. We split
the user preference dataset into a train split of size 285,000 and an evaluation split of size 15,000.
After training for 10,000 steps on batches of 32 pairs, the reward model achieves 60% of accuracy
on the evaluation set (see Figure 6).
To pre-assess the performance of the reward model, we conduct an internal small-scale human
evaluation on 156 audio comparisons from the user preference dataset. In 60% of cases, our team‚Äôs
preferences aligned with the established preferences in the dataset. This result is comparable to
the performance of the reward model. Furthermore, this low agreement rate highlights the inher-
ent subjectivity in judging music preferences, compared to domains such as summarization where
Stiennon et al. (2020) estimated at 73-77% the agreement rate for the OpenAI human preference
dataset. When finetuning MusicLM on the user preference reward model, since our models generate
30-second audios, we average the scores computed from the first and last 20 seconds of audio.
4. Experimental Setup
4.1. Datasets
Given the pretrained reward signals as described in Section 3.3, the RL finetuning step uses a dataset
exclusively composed of captions, used for prompting all MusicLM-based models. Consequently, no
ground-truthaudioisinvolvedinthefinetuningprocess. WefollowthesameprocedureasHuangetal.
(2023)forsyntheticallygeneratingcaptionsfromthreesources. WeusetheLaMDAmodel(Thoppilan
etal.,2022)togeneratedescriptionsof150,000popularsongs. Afterprovidingsongtitlesandartists,
LaMDA‚Äôsresponsesareprocessedinto4milliondescriptivesentencesaboutthemusic. Wesplit10,028
captions from MusicCaps (Agostinelli et al., 2023) into 35,333 single sentences describing music.
Furthermore, we collect 23,906 short-form music tags from MusicCaps. Additionally, we extend the
previous captions with the 300,000 prompts collected from users, as described in Section 3.3. We
randomly split the data, using 90% for training and 10% for evaluation.
4.2. Training procedure
Inthefollowingexperiments,weRL-finetunetheMusicLMmodelwiththesameRLalgorithmandthe
same hyperparameters. The common decoding scheme is temperature sampling with temperature
ùëá = 0.99. The temperature was chosen with subjective inspection to have a good quality-diversity
tradeoff for the generations of MusicLM. The RL-finetuned models differs only with the reward
function employed during their training process.
MusicRL-R. We RL-finetune MusicLM for 20,000 training steps (1) with the MuLan reward, (2)
with the quality reward, and (3) with a linear combination of the MuLan and the quality reward:
the resulting models are respectively called MusicRL-MuLan, MusicRL-Quality, and MusicRL-R.
Throughout our experiments, we normalize the quality reward from [1;5] to [0;1] as preliminary
experiments have shown that the combination of the MuLan and the quality reward gives the best
resultswhenbothrewardsareonthesamescale. Westilldisplayinfigurestheun-normalizedscores.
MusicRL-U.WeRL-finetuneMusicLMfor5000trainingstepswiththeuserpreferencerewardmodel
to obtain a model that we call MusicRL-U.
MusicRL-RU. To combine all the reward signals, we RL-finetune MusicRL-R for1000 training steps
ontheuserpreferencerewardmodel. Forthisexperiment,theKLregularizationiscomputedbetween
7MusicRL:AligningMusicGenerationtoHumanPreferences
themodelbeingfinetunedandMusicRL-R.TheresultingmodeliscalledMusicRL-RU.Wefindthat
the sequential approach of first finetuning on MuLan and quality and then finetuning on the user
preference reward outperforms learning from the three rewards at the same time. We hypothesize
this comes from the fact that it takes a small number of gradient steps (under 2000) before over
optimizing on the user preference reward while it takes around 10,000 steps to optimise the other
rewards. Moreover, using the user preference reward model in a final stage in this matter may allow
the model to align better on the human preferences.
4.3. Evaluation
The main metrics we report in our experiments are the quality reward, the MuLan reward, and the
userpreferencerewardmodel. Wereportthemetricseitheragainstthetrainingsteptoshowprogress
along the training, or against the KL divergence to the base model. This is typically used as a proxy
to measure the distance to the base checkpoint and thus the retention of the original capabilities of
the model (Christiano et al., 2017; Roit et al., 2023).
For the qualitative evaluation, we use 101 diverse, internally-collected prompts, representing a
balancedrangeofmusicalgenres(seeAppendixAforthefulllist). Weusethesepromptstogenerate
audio samples from each evaluated model. We select raters for their experience listening to varied
musical styles (>6 years) and fluency in written English. During the qualitative evaluation, raters
are presented with two audio clips generated by different models using the same text prompt. We
ask raters to rate each clip on a scale of 1 to 5, considering adherence to the text prompt, acoustic
quality and overall appeal to the audio clip. Each comparison is performed by three different raters,
totaling 303 ratings per model comparison. From these ratings, we compute a win rate metric which
is defined as ùë§ùëñùëõ/(ùë§ùëñùëõ+ùëôùëúùë†ùë†).
4.4. Checkpoint selection
For all RL-finetuned models, we select the best checkpoint by inspecting the quantitative results and
listening to the music generations. For MusicRL-R, MusicRL-U, and MusicRL-RU we respectively
choose the checkpoint after 10,000 training steps, 2000 training steps, and 1000 training steps.
5. Results
We aim to answer the following questions: (1) Can RL-finetuning on MuLan and quality rewards
improve the generation quality of text-to-music models such as MusicLM? (2) Can RLHF improve the
alignment of the generated music to generic preferences from users? (3) Is it possible to combine all
reward signals to further improve performance?
5.1. Quantitative Results
In all quantitative evaluations, we analyse model progress during RL finetuning by tracking scores
of rewards against the KL divergence from the initial model. Regardless of whether we train with a
single reward model or a combination of both as in MusicRL-R, we evaluate model performance on
all reward signals.
Figure 4 shows that RL-finetuning successfully optimises both quality and MuLan scores. Specifically,
finetuning on the quality reward alone leads to the greatest increase in quality score (from 3.5 MOS
to 4.6 MOS), and a smaller increase in the MuLan score (from 0.58 to 0.61). Conversely, finetuning
on only the MuLan reward maximises the MuLan score (from 0.58 to 0.71), with a less pronounced
8MusicRL:AligningMusicGenerationtoHumanPreferences
0.775
4.6
0.750
4.4
0.725
4.2 0.700
0.675
4.0
0.650
3.8 0.625
MusicRL-MuLan MusicRL-U 0.600 MusicRL-MuLan MusicRL-U
3.6 MusicRL-Quality MusicRL-RU MusicRL-Quality MusicRL-RU
MusicRL-R start RU 0.575 MusicRL-R start RU
0 10 20 30 40 50 60 0 10 20 30 40 50 60
KL divergence KL divergence
Figure4 | Quality(left)orMuLanscore(right)vsKLdivergencefortheRL-finetunedmodels. TheKL
divergenceiscomputedbetweentheRL-finetunedmodelsandMusicLMexceptforMusicRL-RUwhere
the KL divergence is computed against MusicRL-R. The black cross corresponds to the checkpoint
used to start the training of MusicRL-RU. RL-finetuning successfully optimises the quality and the
MuLan scores (MusicRL-R). Additionally, optimizing the user preference reward (MusicRL-RU,
MusicRL-RU) improves the quality score while marginally decreasing the MuLan score.
2.0
MusicRL-MuLan
MusicRL-Quality
1.5
MusicRL-R
MusicRL-U
1.0 MusicRL-RU
start RU
0.5
0.0
0.5
1.0
1.5
0 10 20 30 40 50 60
KL divergence
Figure 5 | User Preference Reward Model Score for the different RL-finetuned models. The KL
divergenceiscomputedbetweentheRL-finetunedmodelsandMusicLMexceptforMusicRL-RUwhere
the KL divergence is computed against MusicRL-R. The black cross corresponds to the checkpoint
used to start the training of MusicRL-RU. RL-finetuning successfully improves the user preference
reward model score of the generations (see MusicRL-U and MusicRL-RU curves). When trained on
other rewards (MuLan and/or quality) the user preference reward model score slightly improves.
quality scoreimprovement (from 3.5 MOS to4.1 MOS). Leveraging both quality and MuLan rewards
significantly improves both scores (quality: 3.5 MOS to 4.4 MOS; MuLan: 0.58 to 0.71), while
marginally increasing KL divergence. Given the promising and stable performance in simultaneously
optimizing MuLan and quality scores, we perform qualitative evaluations only on MusicRL-R.
Figure 8 (in Appendix B) shows that after 10,000 finetuning steps on the quality reward, the reward
model trained on user preference begins assigning lower scores to music samples. This suggests that
finetuningsolelyonthequalityrewardispronetorewardover-optimisation(Costeetal.,2023;Jiang
et al., 2020; Ram√© et al., 2024).
9
erocS
ytilauQ
erocS
ledoM
draweR
ecnereferP
resU
erocS
naLuMMusicRL:AligningMusicGenerationtoHumanPreferences
Model MOS # wins
MusicLM 3.07 133
MusicRL-R 3.54 362
MusicRL-U 3.54 372
MusicRL-RU 3.82 460
Table 1 | Average mean opinion score (MOS) and number of wins across all rating tasks, for each
model. The music generated from the RL-finetuned models are significantly scored higher in average
thanthe onesfromMusicLM.The bestperforming modelboth intermofMOSandnumberofwinsis
MusicRL-RU.
Figure 5 demonstrates that finetuning with the user preference reward model significantly improves
generation scores, increasing them from -1.5 to over 1.5. Figure 4 shows that despite not training on
the quality reward, the quality score increases from 3.5 MOS to 4 MOS. The MuLan score slightly
decreases from 0.58 to 0.55. Yet, Figure 7 highlights that over-optimizing the user preference reward
model can drastically reduce the MuLan score. Overall, this suggests that user preference feedback
particularly enhances audio quality while having minimal impact on text adherence.
Figure 5 shows that optimizing the user preference reward model on a model finetuned for 10,000
steps on quality and MuLan improves the user preference reward model score significantly. Figure 4
shows that the quality score slightly increases while the MuLan score slightly decreases, which
confirms the impact of the user preference reward model observed in the previous paragraph.
5.2. Qualitative Results
Figure 1 presents human rater evaluations of pairwise comparisons between all possible model com-
binationsacrossMusicLM,MusicRL-R,MusicRL-UandMusicRL-RU.WhencomparedtoMusicLM,
MusicRL-R wins 65% of the time, ties 22.1% and loses 12.9%. This translates into a 83% win rate
in favor of MusicRL-R. MusicRL-U is also strongly preferred over MusicLM as it achieves a 74%
win rate against MusicLM. The best performing model overall is MusicRL-RU. When compared to
MusicLM, MusicRL-RU is strongly preferred by the raters with a 87% win rate. When compared
to the other RL-finetuned models, MusicRL-RU achieves a win rate of 66% against MusicRL-R,
and 62% against MusicRL-U. All results described above are statistically significant according to a
post-hoc analysis using the Wilcoxon signed-rank test (Rey and Neuh√§user, 2011).
Table 1 summarises results from all qualitative evaluations by showing average mean opinion score
(MOS) and number of wins across all rating tasks, for each model. On both metrics, all RL-finetuned
models outperform MusicLM, with MusicRL-RU being the best performing model.
Lastly, MusicRL-R and MusicRL-U perform comparably according to raters, as shown in Figure 1
and Table 1.
5.3. Takeaway
Our results demonstrate several key findings: (1) MusicRL-R shows that RL-finetuning on text
adherence and quality rewards improves the generation quality of MusicLM; (2) MusicRL-U con-
firms the ability to leverage generic user preferences data to improve MusicLM; (3) MusicRL-RU
outperforms all other models, demonstrating that the above reward signals are complementary and
can be combined for the highest performance.
10MusicRL:AligningMusicGenerationtoHumanPreferences
0.60
0.58
0.56
0.54
0.52
3s 20s Quality Predictor
5s No Text MuLan Predictor
0.50 10s
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Epochs over the training dataset
Figure6 | Ablationsontheuserpreferencerewardmodel. Therewardmodelislearnedeitherwithno
text tokens (No Text) or with a cropped version of the input audio (i.e. 10s, 5s, 3s). While dropping
the text tokens does not significantly impact the accuracy of the reward model, cropping the audio
substantially degrades performance. This suggests that text adherence and audio quality are not the
primary factors influencing user audio preferences, as additionally shown by the low accuracy when
using text adherence based (MuLan) or audio quality based predictors for user preference.
6. Understanding Human Feedback Through the Lens of the Reward Model
Inthissection,weanalyserewardmodelaccuracytouncoverthespecificmusicelementsthatinfluence
user preferences. This analysis directly addresses our research question: What is the user paying
attention to when rating the audio?
We categorise generated music into three components which might drive the users‚Äô choice on their
audio preferences: (1) text adherence, (2) audio quality, and (3) musicality. In particular, defining
and modelling musicality is a complex task, which underscores our focus on human feedback as a
solution, moving beyond rule-based limitations.
6.1. Importance of the text input
Toisolatetheimpactoftextonpairwisepreferenceevaluation,wedroptexttokenswhiletrainingthe
reward model. Accuracy remains stable as shown by Figure 6. Additionally, we measure how often
the music clip with the highest MuLan score corresponds to the preferred one. On the evaluation set,
theseindicatorsonlymatch51.6%ofthetime,whichisveryclosetorandomaccuracy. Overall,these
findings indicate that adherence to the text prompt was not a primary driver of human preference in
our experiment. This aligns with our quantitative results in Section 5.1, which show no significant
improvement in text adherence as measured by MuLan, when training MusicRL-U.
6.2. Importance of the audio quality
Sinceaudioqualityremainsrelativelyconsistentwithinageneratedsequence,afewsecondsofaudio
should provide sufficient information to evaluate this aspect. We train reward models on different
inputaudiotokenslengthcorrespondingto10,5,and3seconds. AsshowninFigure6theevaluation
accuracyonpairwisepreferencedecreasesaswereducethelengthoftheinputtokens,droppingfrom
60 to 56% when using 3-5 seconds of input audio. The significant accuracy decrease suggests that
othermusicalcomponentsplayacomplementaryroleinuserpreference. Additionally,wereplicatethe
11
ycaruccAMusicRL:AligningMusicGenerationtoHumanPreferences
analysis done in 6.1 and measure how often the music clip with the highest quality score is preferred.
As shown in Figure 6 the quality predictor achieves 53.3% accuracy on the evaluation dataset. These
findings indicate that audio quality is not the only driver of human preference, while being a better
signal than text adherence. This is consistent with our quantitative results in Section 5.1, where
training MusicRL-U improves marginally on the quality score. Overall, this analysis shows that user
preference is influenced by music elements which go beyond text adherence and audio quality.
7. Limitations and Future Work
Aligningfeedbackandevaluation. Whentrainingonuserpreferencedata,alimitationofourcurrent
setupisthepopulationgapbetweenthosewhoprovidefeedbacktoimprovethemodel(generalusers)
and those who assess the results (selected raters). A direction for future work is to directly measure
the perceived improvements from the user‚Äôs perspective.
Using on-policy data. For the reasons explained in Section 3.1, in this work we collected user
preferences on a different version of MusicLM compared to the one used for RL finetuning. A clear
path for improvement is to iteratively collect on-policy data (data generated by the model that is
being finetuned) and use it to update the model. Eventually, this would allow for real integrated
feedbackwherefinetunedmodelsarecontinuouslydeployedtocollectnewfeedbackwhileimproving
the user experience.
Refining the user preference dataset. Several interesting research directions involve refining
the large user interaction dataset. For instance, identifying and retaining examples where users
express a confident and clear preference could reduce noise and improve the overall dataset quality.
Furthermore, focusing on techniques to train robust reward models on smaller, but highly relevant
datasets could facilitate research directions such as model personalization for specific users.
8. Conclusion
In this work, we introduce MusicRL, the first text-to-music generative model aligned with human
preferences. In a first set of experiments, we derive sequence-level reward functions that inform on
the adherence to the caption as well as the acoustic quality. When finetuning a pretrained MusicLM
model to optimise these rewards with RL, the quantitative and qualitative results show consistent
improvements over the pretrained baseline. We then show for the first time that we can align music
generationwithgenericpreferencesfromusers. Todoso,wecollect300,000usergeneratedcaptions
andaudiopairsalongwithpairwisepreferencesthroughawebinterface. Weleveragethislarge-scale
feedback to train a reward model and improve our model through RLHF, again consistently outper-
forming the baseline. Lastly, we combine all reward signals to produce the highest performing model.
Additional analysis indicates that the signal extracted from user preferences contains information
beyondtextadherenceandaudioquality. Thishighlightsthesubjectiveandcomplexnatureofmusical
appeal,emphasizingthevalueofintegratinguserfeedbackwhenimprovingmusicgenerationmodels.
Acknowledgments and Contributions
Acknowledgments. We thank the AI Test Kitchen team of Google for their contributions in designing
and deploying the MusicLM experience to users at scale: Sai Kiran Gorthi, Kristin Yim, Phillip Maier,
Sejal Khatri, Shirley Leung, Yilun Liu, Yi Yao and Elias Roman. We thank other members and contrib-
utors of MusicLM: Timo Denk, Mauricio Zuluaga, Marco Tagliasacchi, Matt Sharifi, Michael Dooley,
Christian Frank and Hema Manickavasagam. We also thank Robert Dadashi, Nando de Freitas and
12MusicRL:AligningMusicGenerationtoHumanPreferences
Doug Eck for their valuable feedback on earlier drafts of the paper. Finally, we thank the individuals
whodesignedandbuilttheRLtraininginfrastructureusedinthispaper: JohanFerret,NinoVieillard,
Alexis Jacq, Sabela Ramos, Piotr Stanczyk, Danila Sinopalnikov, Am√©lie H√©liou, and Nikola Momchev.
We are also grateful to all the users of MusicLM for their valuable feedback and their contribution
in creating MusicRL.
Contributions. Geoffrey Cideron and Andrea Agostinelli (main investigators for the project), Neil
Zeghidour(corecontributionsoninitialdesign,supervision,qualityrewardmodelling,paperwriting),
Sertan Girgin (core infrastructure), Mauro Verzetti (infrastructure, core quality reward modelling),
L√©onard Hussenot (supervision, core paper writing), Victor Ungureanu (core quality reward mod-
elling),MatejKastelic(userdatainfrastructure),Zal√°nBorsos,BrianMcWilliamsandDamienVincent
(core MusicLM modelling), Olivier Bachem and Olivier Pietquin (supervision, initial design), and
Matthieu Geist (supervision).
References
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,
S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts,
M. Tagliasacchi, M. Sharifi, N. Zeghidour, and C. Frank. Musiclm: Generating music from text,
2023.
Y.Bai,A.Jones,K.Ndousse,A.Askell,A.Chen,N.DasSarma,D.Drain,S.Fort,D.Ganguli,T.Henighan,
et al. Training a helpful and harmless assistant with reinforcement learning from human feedback.
arXiv preprint arXiv:2204.05862, 2022.
Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul,
D. Grangier, M. Tagliasacchi, et al. Audiolm: a language modeling approach to audio generation.
IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023a.
Z. Borsos, M. Sharifi, D. Vincent, E. Kharitonov, N. Zeghidour, and M. Tagliasacchi. Soundstorm:
Efficient parallel audio generation, 2023b.
R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired
comparisons. Biometrika, 39, 1952.
P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning
from human preferences. Advances in neural information processing systems, 30, 2017.
Y. Chung, Y. Zhang, W. Han, C. Chiu, J. Qin, R. Pang, and Y. Wu. W2v-bert: Combining contrastive
learningandmaskedlanguagemodelingforself-supervisedspeechpre-training. arXiv:2108.06209,
2021.
J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. D√©fossez. Simple and
controllable music generation, 2023.
T.Coste,U.Anwar,R.Kirk,andD.Krueger. Rewardmodelensembleshelpmitigateoveroptimization.
arXiv preprint, 2023.
A. D√©fossez, N. Zeghidour, N. Usunier, L. Bottou, and F. R. Bach. SING: symbol-to-instrument
neural generator. In S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference
13MusicRL:AligningMusicGenerationtoHumanPreferences
on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr√©al,
Canada, pages 9055‚Äì9065, 2018. URL https://proceedings.neurips.cc/paper/2018/
hash/56dc0997d871e9177069bb472574eb29-Abstract.html.
J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova.BERT:Pre-trainingofdeepbidirectionaltransformers
for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171‚Äì4186, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://aclanthology.org/N19-1423.
P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever. Jukebox: A generative model
for music. arXiv:2005.00341, 2020.
A. D√©fossez, J. Copet, G. Synnaeve, and Y. Adi. High fidelity neural audio compression.
arXiv:2210.13438, 2022.
B. Elizalde, S. Deshmukh, M. A. Ismail, and H. Wang. Clap: Learning audio concepts from natural
language supervision, 2022.
J. H. Engel, C. Resnick, A. Roberts, S. Dieleman, M. Norouzi, D. Eck, and K. Simonyan. Neural
audio synthesis of musical notes with wavenet autoencoders. In D. Precup and Y. W. Teh, editors,
Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,
Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1068‚Äì
1077. PMLR, 2017. URL http://proceedings.mlr.press/v70/engel17a.html.
J. H. Engel, L. Hantrakul, C. Gu, and A. Roberts. DDSP: differentiable digital signal processing. In
International Conference on Learning Representations (ICLR), 2020.
S. Forsgren and H. Martiros. Riffusion - Stable diffusion for real-time music generation, 2022. URL
https://riffusion.com/about.
H.F.Garcia,P.Seetharaman,R.Kumar,andB.Pardo. Vampnet: Musicgenerationviamaskedacoustic
token modeling, 2023.
R. Gauldin. A practical approach to eighteenth-century counterpoint. Prentice-Hall, 1988.
G. Gemini Team. Gemini: A family of highly capable multimodal models. 2023.
G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias, and A. Aspuru-Guzik. Objective-
reinforced generative adversarial networks (organ) for sequence generation models. arXiv preprint
arXiv:1705.10843, 2017.
C. Hawthorne, A. Jaegle, C. Cangea, S. Borgeaud, C. Nash, M. Malinowski, S. Dieleman, O. Vinyals,
M.M.Botvinick,I.Simon,H.Sheahan,N.Zeghidour,J.Alayrac,J.Carreira,andJ.H.Engel. General-
purpose, long-context autoregressive modeling with perceiver AR. In K. Chaudhuri, S. Jegelka,
L.Song,C.Szepesv√°ri,G.Niu,andS.Sabato,editors,InternationalConferenceonMachineLearning
(ICML), 2022.
Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y. Li, and D. P. W. Ellis. Mulan: A joint embedding of music
audio and natural language. In International Society for Music Information Retrieval Conference
(ISMIR), 2022.
14MusicRL:AligningMusicGenerationtoHumanPreferences
Q. Huang, D. S. Park, T. Wang, T. I. Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank,
et al. Noise2music: Text-conditioned music generation with diffusion models. arXiv preprint
arXiv:2302.03917, 2023.
N. Jaques, S. Gu, D. Bahdanau, J. M. Hern√°ndez-Lobato, R. E. Turner, and D. Eck. Sequence tutor:
Conservativefine-tuningofsequencegenerationmodelswithkl-control. InInternationalConference
on Machine Learning, pages 1645‚Äì1654. PMLR, 2017.
N. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, A. Lapedriza, N. Jones, S. Gu, and R. Picard.
Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv
preprint arXiv:1907.00456, 2019.
N.Jiang,S.Jin,Z.Duan,andC.Zhang. Rl-duet: Onlinemusicaccompanimentgenerationusingdeep
reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, volume 34,
pages 710‚Äì718, 2020.
S. M. Karbasi, H. S. Haug, M.-K. Kvalsund, M. J. Krzyzaniak, and J. T√∏rresen. A generative model for
creatingmusicalrhythmswithdeepreinforcementlearning. 2ndConferenceonAIMusicCreativity,
2021.
E. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi,
and N. Zeghidour. Speak, read and prompt: High-fidelity text-to-speech with minimal supervision.
Transactions of the Association for Computational Linguistics, 11:1703‚Äì1718, 2023. URL https:
//api.semanticscholar.org/CorpusID:256627687.
K. Kilgour, M. Zuluaga, D. Roblek, and M. Sharifi. Fr√©chet audio distance: A reference-free metric for
evaluating music enhancement algorithms. In INTERSPEECH, 2019.
N. Kotecha. Bach2bach: generating music using a deep reinforcement learning approach. arXiv
preprint arXiv:1812.01060, 2018.
F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D√©fossez, J. Copet, D. Parikh, Y. Taigman, and Y. Adi.
Audiogen: Textually guided audio generation, 2022.
S. Latif, H. Cuay√°huitl, F. Pervez, F. Shamshad, H. S. Ali, and E. Cambria. A survey on deep rein-
forcement learning for audio-based applications. Artificial Intelligence Review, 56(3):2193‚Äì2240,
2023.
D. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. Autoregressive image generation using residual
quantization. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages 11523‚Äì11532, 2022.
K. Lee, H. Liu, M. Ryu, O. Watkins, Y. Du, C. Boutilier, P. Abbeel, M. Ghavamzadeh, and S. S. Gu.
Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023.
D.J.Lewkowicz. Theconceptofecologicalvalidity: Whatareitslimitationsandisitbadtobeinvalid?
Infancy, 2(4):437‚Äì450, 2001.
H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. P. Mandic, W. Wang, and M. D. Plumbley. Audioldm: Text-
to-audio generation with latent diffusion models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt,
S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29
July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages
21450‚Äì21474. PMLR, 2023. URL https://proceedings.mlr.press/v202/liu23f.html.
OpenAI. Gpt-4 technical report. 2023.
15MusicRL:AligningMusicGenerationtoHumanPreferences
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,
A. Ray, et al. Training language models to follow instructions with human feedback. Advances in
Neural Information Processing Systems, 35:27730‚Äì27744, 2022.
J. D. Parker, J. Spijkervet, K. Kosta, F. Yesiler, B. Kuznetsov, J.-C. Wang, M. Avent, J. Chen, and D. Le.
Stemgen: A music generation model that listens, 2024.
R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference
optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290,
2023.
A. Ram√©, N. Vieillard, L. Hussenot, R. Dadashi, G. Cideron, O. Bachem, and J. Ferret. Warm: On the
benefits of weight averaged reward models, 2024.
M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural
networks. In Y. Bengio and Y. LeCun, editors, 4th International Conference on Learning Representa-
tions, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL
http://arxiv.org/abs/1511.06732.
D. Rey and M. Neuh√§user. Wilcoxon-signed-rank test. In M. Lovric, editor, International Encyclopedia
of Statistical Science, pages 1658‚Äì1659. Springer, 2011. doi: 10.1007/978-3-642-04898-2\_616.
URL https://doi.org/10.1007/978-3-642-04898-2_616.
P. Roit, J. Ferret, L. Shani, R. Aharoni, G. Cideron, R. Dadashi, M. Geist, S. Girgin, L. Hussenot,
O. Keller, N. Momchev, S. R. Garea, P. Stanczyk, N. Vieillard, O. Bachem, G. Elidan, A. Hassidim,
O. Pietquin, and I. Szpektor. Factually consistent summarization via reinforcement learning with
textual entailment feedback. In A. Rogers, J. L. Boyd-Graber, and N. Okazaki, editors, Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
ACL 2023, Toronto, Canada, July 9-14, 2023, pages 6252‚Äì6272. Association for Computational
Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.344. URL https://doi.org/10.18653/
v1/2023.acl-long.344.
F. Schneider, O. Kamal, Z. Jin, and B. Sch√∂lkopf. Mo√ªsai: Text-to-music generation with long-context
latent diffusion, 2023.
N.Stiennon,L.Ouyang,J.Wu,D.Ziegler,R.Lowe,C.Voss,A.Radford,D.Amodei,andP.F.Christiano.
Learning to summarize with human feedback. Advances in Neural Information Processing Systems,
33:3008‚Äì3021, 2020.
R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.
G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut,J.Schalkwyk,A.M.Dai,A.Hauth,
et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,
2023.
M. Tervaniemi. The neuroscience of music‚Äìtowards ecological validity. Trends in Neurosciences, 2023.
J. C. Thomas and W. A. Kellogg. Minimizing ecological gaps in interface design. IEEE Software, 6(1):
78‚Äì86, 1989.
R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker,
Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239,
2022.
16MusicRL:AligningMusicGenerationtoHumanPreferences
S.E.Trehub,J.Becker,andI.Morley. Cross-culturalperspectivesonmusicandmusicality. Philosophical
Transactions of the Royal Society B: Biological Sciences, 370(1664):20140096, 2015.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin.
Attention is all you need. Advances in neural information processing systems (NeurIPS), 2017.
B. Wallace, M. Dang, R. Rafailov, L. Zhou, A. Lou, S. Purushwalkam, S. Ermon, C. Xiong, S. Joty,
and N. Naik. Diffusion model alignment using direct preference optimization. arXiv preprint
arXiv:2311.12908, 2023.
C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao,
and F. Wei. Neural codec language models are zero-shot text to speech synthesizers. CoRR,
abs/2301.02111,2023. doi: 10.48550/ARXIV.2301.02111. URLhttps://doi.org/10.48550/
arXiv.2301.02111.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229‚Äì256, 1992.
Y. Wu and B. Hu. Learning to extract coherent summary via deep reinforcement learning. In S. A.
McIlraithandK.Q.Weinberger,editors,ProceedingsoftheThirty-SecondAAAIConferenceonArtificial
Intelligence,(AAAI-18),the30thinnovativeApplicationsofArtificialIntelligence(IAAI-18),andthe8th
AAAISymposiumonEducationalAdvancesinArtificialIntelligence(EAAI-18),NewOrleans,Louisiana,
USA, February 2-7, 2018, pages 5602‚Äì5609. AAAI Press, 2018. doi: 10.1609/AAAI.V32I1.11987.
URL https://doi.org/10.1609/aaai.v32i1.11987.
Y.Wu,M.Schuster,Z.Chen,Q.V.Le,M.Norouzi,W.Macherey,M.Krikun,Y.Cao,Q.Gao,K.Macherey,
J.Klingner,A.Shah,M.Johnson,X.Liu,L.Kaiser,S.Gouws,Y.Kato,T.Kudo,H.Kazawa,K.Stevens,
G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado,
M. Hughes, and J. Dean. Google‚Äôs neural machine translation system: Bridging the gap between
human and machine translation. CoRR, abs/1609.08144, 2016. URL http://arxiv.org/abs/
1609.08144.
D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu. Diffsound: Discrete diffusion model
for text-to-sound generation. arXiv:2207.09983, 2022.
D.Yang,J.Tian,X.Tan,R.Huang,S.Liu,X.Chang,J.Shi,S.Zhao,J.Bian,X.Wu,Z.Zhao,S.Watanabe,
and H. Meng. Uniaudio: An audio foundation model toward universal audio generation, 2023.
L. Yu, D. Simig, C. Flaherty, A. Aghajanyan, L. Zettlemoyer, and M. Lewis. Megabyte: Predicting
million-byte sequences with multiscale transformers. arXiv preprint arXiv:2305.07185, 2023.
N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi. Soundstream: An end-to-end
neural audio codec. IEEE ACM Trans. Audio Speech Lang. Process., 30, 2022.
D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving.
Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.
17MusicRL:AligningMusicGenerationtoHumanPreferences
A. Qualitative Evaluation.
For the qualitative evaluation, the list of the 101 diverse prompts is the following:
‚ÄôAmid-tempocountrychoruswithachoircountermelody‚Äô,
‚Äôgrungewithadrumnbassbeat‚Äô,
‚ÄôAneerie,powerfulslowmetalguitarriffwithdrumsbackingthatbuildstensionandanticipation.‚Äô,
‚ÄôAwistful,nostalgicindiefolk-popsongwithastrongbassandadeepmalevoice‚Äô,
‚ÄôReggeatonwithdeepbassandarappingvoice‚Äô,
‚ÄôAmodern,romanticslowwaltzplayedbyajazztrio‚Äô,
‚ÄôArock-steadyintrowithtrumpetsprovidingthebackingtoagentleguitar‚Äô,
‚Äôafunkydiscosongwithabassplayer‚Äô,
‚ÄôAslowrumbacompositionwithafemalevoicesupportedbyapianoapercussions‚Äô,
‚ÄôAsadpopmelodywithpianoandstringsaccompaniment‚Äô,
‚ÄôChinesemusicinstrumentsinfuturistictheme,fastpace‚Äô,
‚ÄôAfranticdrummachinebeatandpew-pewlasernoisesfillthecavernouswarehouserave‚Äô,
‚Äôfast,classicalmusicwithachurchorganwithaneeriefeeling,foradarkthrillersoundtrack‚Äô,
‚ÄôAfast,energetictangoplayedbyanaccordionandaviolin‚Äô,
‚ÄôAmellowbritish-rockacousticchorus‚Äô,
‚ÄôRepetitivehousemusicwithstrongpercussiveline‚Äô,
"Thesitar‚Äôsslow,meanderingmelodywasaccompaniedbythetabla‚Äôssteadybeat,creatingasound
thatwasbothcalmingandenchanting.",
‚ÄôEnergeticpunkrockwithafemalevoicesinging‚Äô,
‚Äôacheerfulchildrensongwithasimplexylophonebacking‚Äô,
‚ÄôAnenergeticgospelchoirperformance‚Äô,
‚Äôslow,mellow,andinstrumentalnewagemusicformeditation.‚Äô,
‚ÄôFlamencoperformancefullofenergy‚Äô,
‚ÄôMelodicdanceablebrazilianmusicwithpercussions.‚Äô,
‚ÄôAnindie-rockchorusisplayedbyamalesingerwithasmallbandbacking.‚Äô,
‚Äôepicmoviesoundtrack‚Äô,
"The K-pop group‚Äôs powerful vocals were accompanied by a lush string arrangement, creating a
trulyepicsoundscape.",
‚ÄôAfunkbassintrowithaguitarplayingshortchordsandadrumsbacking‚Äô,
‚ÄôSalsamusicplayedbyanorchestra‚Äô,
‚ÄôAsmallbandplaysalatindanceablesong‚Äô,
‚ÄôAwhistlingtuneforawesternduelsoundtrack‚Äô,
‚ÄôAsambabeatandalivelychoruscombinetocreateafestiveatmosphere.‚Äô,
‚ÄôAjazzypopsongplayedbyabigband‚Äô,
‚Äôaska-punktrumpetriffsupportedbyanup-beatguitar‚Äô,
‚Äômalebasslowgravevoicemale-singingamedievalsongwithamandolin‚Äô,
‚Äôafastsymphonicmetalguitarsolowithachoirbacking‚Äô,
‚Äôchorusofasweetacousticrockballad‚Äô,
‚ÄôAbluesypianoriffdrivesthebandastheybeltoutasoulfultune.‚Äô,
‚ÄôAslow,swingpopsongwithpianoanddrumsbacking‚Äô,
‚ÄôAfusionofreggaetonandelectronicdancemusic,withaspacey,otherworldlysound.‚Äô,
‚ÄôAmarchingbandplaysacatchytune‚Äô,
‚ÄôAclassicalorchestralwaltzforacostumedance‚Äô,
‚ÄôIrishfolkchoruswithamandolinandteamwhistle‚Äô,
‚ÄôAmalevoicesingsapopanthemaccompaniedbyhispiano‚Äô,
‚ÄôAcatchypoptuneissungontopadancedrumbeat‚Äô,
"Thesoprano‚Äôsvoicesoaredoverthedelicateaccompanimentofthepiano,fillingtheoperahouse
withbeautyandemotion.",
‚ÄôRapsongwithafemalemelodicline‚Äô,
‚Äôareggaesongwithguitarandsinging‚Äô,
‚ÄôAcornypopchorussungbyafemalevoicewithalotofautotune‚Äô,
18MusicRL:AligningMusicGenerationtoHumanPreferences
"Themarimba‚Äôssoulfulmelodywasaccompaniedbythesteadybeatofthedrums,creatingabluesy
soundthatwasbothmelancholyanduplifting.",
‚ÄôAgospelchoirsingsontopametalguitarbacking‚Äô,
‚ÄôApowerfulfemalevoicesingswithsoulandenergyoveradrivingdrumbeat.‚Äô,
‚ÄôArepetitivelullabysungbyafemalevoicewithacarillonbacking‚Äô,
‚ÄôTraditionalfastsongplayedbyamalevoicewithanaccordionbacking‚Äô,
‚ÄôAnup-beatreggaewithadeepmalevoiceandapianostrikingthechords‚Äô,
‚ÄôSlow,melodicmusicbackedbyasitarandstrings.‚Äô,
‚ÄôFunkypiecewithastrong,danceablebeat,aprominentbasslineandakeyboardmelody.‚Äô,
"Adanceable,fastandcheerfulswingtunefromthe50‚Äôs",
‚Äôaprofessionalsolocellistplayingasadmelodyforsolocelloonthecello,highqualityrecording‚Äô,
‚ÄôArockguitarriff,aslideguitarsoloandaflutemelodycreatealively,upbeatsound.‚Äô,
‚Äôanacappellachorussingingachristmassong‚Äô,
‚Äôniceragtimeguitarchordprogression‚Äô,
"AcheerfulR‚Äôn‚ÄôBsongisplayedbytwosingerswithatrumpetmelody",
‚ÄôAdancesongwithafastmelodytakenfromsampledvoice,givingtheimpressionofpercussions‚Äô,
‚Äôagospelsongwithafemaleleadsinger‚Äô,
‚Äôanostalgictuneplayedbyaccordionband‚Äô,
‚ÄôAmariachisongwithanepictwistandsymphonicorchestrabacking‚Äô,
‚ÄôAmiddle-eastertunewithpercussionsandflutes‚Äô,
‚ÄôJazzcompositionforpianoandtrumpet‚Äô,
‚ÄôAslowbluesintrowithaharmonicaandminimalbacking.‚Äô,
‚ÄôTheexperimentalmodularsynthesizercreatedauniquesoundscapebycombiningthesoundsof
waterwithelectronicmusic.‚Äô,
‚Äôacheerfulragtimewithguitar‚Äô,
‚ÄôIndustrialtechnosounds,withhypnoticrhythms. Stringsplayingarepetitivemelodycreatesan
unsettlingatmosphere.‚Äô,
‚ÄôThemicrophonepickedupthesoulful,funkyscreamoftheleadsingerashereachedtheclimaxof
thesong.‚Äô,
‚ÄôThesnaredrumandluteplayedalivelyduet,withthesnaredrumprovidingasteadybeatandthe
luteplayingamelodyontop.‚Äô,
‚ÄôThetworapperstradedversesoverapulsatingsynthbeat,creatingasoundthatwasbothenergetic
andinfectious.‚Äô,
‚ÄôAbagpipeisplayinganaggressivetunewithapunkbacking‚Äô,
‚ÄôAstringquartetplaysalivelytune.‚Äô,
‚ÄôAveryfastpianocadenzathatishardtoplay.‚Äô,
‚ÄôAloneharmonicaplaysahauntingmelodyoverthesoundofthewindblowingthroughthedesert.‚Äô,
‚ÄôAnaggressive,butsadpunkverse,withaprominentslowguitarmelodyanddarkbassline.‚Äô,
‚Äôabandplayingcumbiainaboatalongthemagdalenariverincolombia‚Äô,
‚ÄôAslowjamaicanskasongwithanorganbacking‚Äô,
‚ÄôThegramophoneneedlecrackledandhissedasitspunacrossthevinylrecord,fillingtheroomwith
awarm,nostalgicsound.‚Äô,
‚Äôfastpianotoccata‚Äô,
"RomanticR‚Äôn‚ÄôBsongwithawarmfemalevoice",
‚ÄôAcheerfulbollywood-stylegroupdance‚Äô,
‚ÄôDancemusicwithamelodicsynthlineandarpeggiation‚Äô,
‚ÄôThe wooden bongo drums beat a deep, resonating bass as the dancers move their bodies to the
music.‚Äô,
‚Äôatenorsingingwithabackingguitar‚Äô,
‚ÄôSlowtrapsongwithalotofreverbandautotune‚Äô,
‚ÄôAsyncopatedprogressiverocktunewithasaxophone‚Äô,
‚ÄôAsyncopateddrumbeatbacksahardrockguitarriff‚Äô,
‚Äôagregorianchant‚Äô,
‚ÄôAdanceablefolkwaltzisplayedbyanaccordion‚Äô,
‚ÄôAbagpipeisplayingafastcatchytuneinadance-popsong‚Äô,
19MusicRL:AligningMusicGenerationtoHumanPreferences
‚ÄôAfullorchestraplayingaviolinconcertofromthe1800s‚Äô,
‚ÄôThetrapbeatwaslayeredwithacousticstringsoundscreatingacatchychorus.‚Äô,
‚ÄôAchurchchoirsingsahigh-pitched,soothingmelody.‚Äô,
‚ÄôAnenergeticdance-popsong,sangbyapowerfulfemalevoice‚Äô,
‚ÄôAnharmonicaplaysamelancholicsolooveranacousticguitar‚Äô,
‚ÄôAfastrapchorusissungontopofasimplecatchytune‚Äô
B. Additional Quantitative Evaluation Plots
Figure 7 and Figure 8 show the progress of the RL-finetuned models along training as measured by
the three reward signals.
0.775
4.6
0.750
4.4
0.725
4.2 0.700
0.675
4.0
0.650
3.8 0.625
MusicRL-MuLan MusicRL-U 0.600 MusicRL-MuLan MusicRL-U
3.6 MusicRL-Quality MusicRL-RU MusicRL-Quality MusicRL-RU
MusicRL-R start RU 0.575 MusicRL-R start RU
0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000
step step
Figure 7 | Quality (left) or MuLan score (right) vs step for the RL-finetuned models. The black cross
corresponds to the checkpoint used to start the training of MusicRL-RU. RL-finetuning successfully
optimisesthequalityandtheMuLanscores(MusicRL-R).Additionally,optimizingtheuserpreference
reward (MusicRL-RU, MusicRL-RU) improves the quality score while the MuLan score starts to be
significantly impacted when the model over optimise the user preference reward.
2.5
2.0
1.5
MusicRL-MuLan 1.0
MusicRL-Quality
MusicRL-R
0.5
MusicRL-U
MusicRL-RU
0.0
start RU
0.5
1.0
1.5
0 2500 5000 7500 10000 12500 15000 17500 20000
step
Figure 8 | User Preference Reward Model Score for the different RL-finetuned models. The black
cross corresponds to the checkpoint used to start the training of MusicRL-RU. RL-finetuning suc-
cessfully improves the user preference reward model score of the generations (see MusicRL-U and
MusicRL-RU curves). When trained on other rewards (MuLan and/or quality) the user preference
reward model score slightly improves.
20
erocS
ytilauQ
erocS
ledoM
draweR
ecnereferP
resU
erocS
naLuMMusicRL:AligningMusicGenerationtoHumanPreferences
C. Advantages of User Data
In Section 5, we show that we could leverage a model that was trained with human rater data to
improve a music generation model with RL. However, rater data have some limitations and biases.
In behavioural sciences, the ecological validity5 of a lab study refers its potential of generalization to
the real world (Lewkowicz, 2001). In the context of music, it is crucial to experiment on real-world
settings(Tervaniemi,2023). ThomasandKellogg(1989)exploretheecologicalvalidityconceptinthe
context of interface design and say that "User-related ecological gaps are caused by characteristics
of users - such as what motivates them, their cognitive abilities, preferences, and habits - that may
vary between the lab and the target environment." The concept of user-related ecological gaps is
particularly relevant for the finetuning and the evaluation of large language models as the raters and
users are often dissimilar.
Population Gap. Raters are often not representative of the user population especially as the rating
task is often outsourced to crowdsourcing services which employ people in different countries than
the ones the model is deployed in e.g. Amazon Mechanical Turk6 proposes a global workforce for
rating tasks. This population difference creates biases such as cultural biases which can impact the
music preferences (Trehub et al., 2015).
MotivationGap. AsmentionedinThomasandKellogg(1989),themotivationgapwhichcorresponds
tothedifferenceofmotivationsbetweenthedifferentuserscanhaveasignificanteffectontheresults.
Inourcontext,whiletheusersofmusicgenerationmodelshaveagenuineinterestinplayingwiththe
model,theincentiveoftheratersareverydifferent. Hence,forratingtasks,itiscrucialtogivespecific
set of instructions to make sure the raters make their decisions aligned with what the creator of the
ratingtaskwouldexpectwhichalsocanbeasourceofbiases. Whereasforusers,weareinterestedin
general interactions where no instructions are given.
Dataset Size. Due to the cost of rater data, the number of collected human preference is often below
100,000(Leeetal.,2023;Stiennonetal.,2020;Ziegleretal.,2019). Ontheotherhand,thenumber
of user interactions can be orders of magnitude higher once a model is deployed.
5https://en.wikipedia.org/wiki/Ecological_validity
6https://www.mturk.com/
21