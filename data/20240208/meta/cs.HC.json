[
    {
        "title": "Human Emotions Analysis and Recognition Using EEG Signals in Response to 360$^\\circ$ Videos",
        "authors": "Haseeb ur Rahman AbbasiZeeshan RashidMuhammad MajidSyed Muhammad Anwar",
        "links": "http://arxiv.org/abs/2402.04142v1",
        "entry_id": "http://arxiv.org/abs/2402.04142v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04142v1",
        "summary": "Emotion recognition (ER) technology is an integral part for developing\ninnovative applications such as drowsiness detection and health monitoring that\nplays a pivotal role in contemporary society. This study delves into ER using\nelectroencephalography (EEG), within immersive virtual reality (VR)\nenvironments. There are four main stages in our proposed methodology including\ndata acquisition, pre-processing, feature extraction, and emotion\nclassification. Acknowledging the limitations of existing 2D datasets, we\nintroduce a groundbreaking 3D VR dataset to elevate the precision of emotion\nelicitation. Leveraging the Interaxon Muse headband for EEG recording and\nOculus Quest 2 for VR stimuli, we meticulously recorded data from 40\nparticipants, prioritizing subjects without reported mental illnesses.\nPre-processing entails rigorous cleaning, uniform truncation, and the\napplication of a Savitzky-Golay filter to the EEG data. Feature extraction\nencompasses a comprehensive analysis of metrics such as power spectral density,\ncorrelation, rational and divisional asymmetry, and power spectrum. To ensure\nthe robustness of our model, we employed a 10-fold cross-validation, revealing\nan average validation accuracy of 85.54\\%, with a noteworthy maximum accuracy\nof 90.20\\% in the best fold. Subsequently, the trained model demonstrated a\ncommendable test accuracy of 82.03\\%, promising favorable outcomes.",
        "updated": "2024-02-06 16:48:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04142v1"
    },
    {
        "title": "Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)",
        "authors": "Michael De'Shazer",
        "links": "http://arxiv.org/abs/2402.04140v2",
        "entry_id": "http://arxiv.org/abs/2402.04140v2",
        "pdf_url": "http://arxiv.org/pdf/2402.04140v2",
        "summary": "This study consists of a novel approach toward the analysis of court\njudgments spanning five countries, including the United States, the United\nKingdom, Rwanda, Sweden and Hong Kong. This study also explores the\nintersection of the latest advancements in artificial intelligence (AI) and\nlegal analysis, emphasizing the role of AI (specifically generative AI) in\nidentifying human biases and facilitating automated, valid, and coherent\nmultisided argumentation of court judgments with the goal of ensuring\nconsistent application of laws in and across various jurisdictions. By\nincorporating Advanced Language Models (ALMs) and a newly introduced human-AI\ncollaborative framework, this paper seeks to analyze Grounded Theory-based\nresearch design with Advanced Language Models (ALMs) in the practice of law.\nSHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT\ntechnology), focusing on detecting logical inconsistencies and biases across\nvarious legal decisions. SHIRLEY analysis is aggregated and is accompanied by a\ncomparison-oriented AI-based application called SAM (also an ALM) to identify\nrelative deviations in SHIRLEY bias detections. Further, a CRITIC is generated\nwithin semi-autonomous arbitration process via the ALM, SARA. A novel approach\nis introduced in the utilization of an AI arbitrator to critically evaluate\nbiases and qualitative-in-nature nuances identified by the aforementioned AI\napplications (SAM in concert with SHIRLEY), based on the Hague Rules on\nBusiness and Human Rights Arbitration. This Semi-Automated Arbitration Process\n(SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring\na nuanced debate-resultant \"understanding\" through a hybrid system of AI and\nhuman-based collaborative analysis.",
        "updated": "2024-02-07 14:48:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04140v2"
    },
    {
        "title": "Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy",
        "authors": "Efe BozkirSüleyman ÖzdelKa Hei Carrie LauMengdi WangHong GaoEnkelejda Kasneci",
        "links": "http://arxiv.org/abs/2402.03907v1",
        "entry_id": "http://arxiv.org/abs/2402.03907v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03907v1",
        "summary": "Recent developments in computer graphics, hardware, artificial intelligence\n(AI), and human-computer interaction likely lead to extended reality (XR)\ndevices and setups being more pervasive. While these devices and setups provide\nusers with interactive, engaging, and immersive experiences with different\nsensing modalities, such as eye and hand trackers, many non-player characters\nare utilized in a pre-scripted way or by conventional AI techniques. In this\npaper, we argue for using large language models (LLMs) in XR by embedding them\nin virtual avatars or as narratives to facilitate more inclusive experiences\nthrough prompt engineering according to user profiles and fine-tuning the LLMs\nfor particular purposes. We argue that such inclusion will facilitate diversity\nfor XR use. In addition, we believe that with the versatile conversational\ncapabilities of LLMs, users will engage more with XR environments, which might\nhelp XR be more used in everyday life. Lastly, we speculate that combining the\ninformation provided to LLM-powered environments by the users and the biometric\ndata obtained through the sensors might lead to novel privacy invasions. While\nstudying such possible privacy invasions, user privacy concerns and preferences\nshould also be investigated. In summary, despite some challenges, embedding\nLLMs into XR is a promising and novel research area with several opportunities.",
        "updated": "2024-02-06 11:19:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03907v1"
    },
    {
        "title": "Robot voice a voice controlled robot using arduino",
        "authors": "Vineeth TeedaK SujathaRakesh Mutukuru",
        "links": "http://arxiv.org/abs/2402.03803v1",
        "entry_id": "http://arxiv.org/abs/2402.03803v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03803v1",
        "summary": "Robotic assistants reduce the manual efforts being put in by humans in their\nday-to-day tasks. In this paper, we develop a voice-controlled personal\nassistant robot. The robot takes the human voice commands by its own built-in\nmicrophone. This robot not only takes the commands and executes them but also\nacknowledges them through speech output. This robot can perform different\nmovements, turns, wakeup/shutdown operations, relocate an object from one place\nto another, and can also develop a conversation with humans. The voice commands\nare processed in real time using an offline server. The speech signal commands\nare directly communicated to the server using a USB cable. The personal\nassistant robot is developed on a microcontroller-based platform. Performance\nevaluation is carried out with encouraging results of the initial experiments.\nPossible improvements for applications in homes, hospitals, car systems, and\nindustries are also discussed.",
        "updated": "2024-02-06 08:44:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03803v1"
    },
    {
        "title": "Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach",
        "authors": "Xin ChenMingliang HouTao TangAchhardeep KaurFeng Xia",
        "links": "http://dx.doi.org/10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00182",
        "entry_id": "http://arxiv.org/abs/2402.03750v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03750v1",
        "summary": "With the arrival of the big data era, mobility profiling has become a viable\nmethod of utilizing enormous amounts of mobility data to create an intelligent\ntransportation system. Mobility profiling can extract potential patterns in\nurban traffic from mobility data and is critical for a variety of\ntraffic-related applications. However, due to the high level of complexity and\nthe huge amount of data, mobility profiling faces huge challenges. Digital Twin\n(DT) technology paves the way for cost-effective and performance-optimised\nmanagement by digitally creating a virtual representation of the network to\nsimulate its behaviour. In order to capture the complex spatio-temporal\nfeatures in traffic scenario, we construct alignment diagrams to assist in\ncompleting the spatio-temporal correlation representation and design dilated\nalignment convolution network (DACN) to learn the fine-grained correlations,\ni.e., spatio-temporal interactions. We propose a digital twin mobility\nprofiling (DTMP) framework to learn node profiles on a mobility network DT\nmodel. Extensive experiments have been conducted upon three real-world\ndatasets. Experimental results demonstrate the effectiveness of DTMP.",
        "updated": "2024-02-06 06:37:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03750v1"
    }
]