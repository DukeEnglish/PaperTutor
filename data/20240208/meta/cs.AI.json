[
    {
        "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
        "authors": "Mantas MazeikaLong PhanXuwang YinAndy ZouZifan WangNorman MuElham SakhaeeNathaniel LiSteven BasartBo LiDavid ForsythDan Hendrycks",
        "links": "http://arxiv.org/abs/2402.04249v1",
        "entry_id": "http://arxiv.org/abs/2402.04249v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04249v1",
        "summary": "Automated red teaming holds substantial promise for uncovering and mitigating\nthe risks associated with the malicious use of large language models (LLMs),\nyet the field lacks a standardized evaluation framework to rigorously assess\nnew methods. To address this issue, we introduce HarmBench, a standardized\nevaluation framework for automated red teaming. We identify several desirable\nproperties previously unaccounted for in red teaming evaluations and\nsystematically design HarmBench to meet these criteria. Using HarmBench, we\nconduct a large-scale comparison of 18 red teaming methods and 33 target LLMs\nand defenses, yielding novel insights. We also introduce a highly efficient\nadversarial training method that greatly enhances LLM robustness across a wide\nrange of attacks, demonstrating how HarmBench enables codevelopment of attacks\nand defenses. We open source HarmBench at\nhttps://github.com/centerforaisafety/HarmBench.",
        "updated": "2024-02-06 18:59:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04249v1"
    },
    {
        "title": "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science",
        "authors": "Xiangru TangQiao JinKunlun ZhuTongxin YuanYichi ZhangWangchunshu ZhouMeng QuYilun ZhaoJian TangZhuosheng ZhangArman CohanZhiyong LuMark Gerstein",
        "links": "http://arxiv.org/abs/2402.04247v2",
        "entry_id": "http://arxiv.org/abs/2402.04247v2",
        "pdf_url": "http://arxiv.org/pdf/2402.04247v2",
        "summary": "Intelligent agents powered by large language models (LLMs) have demonstrated\nsubstantial promise in autonomously conducting experiments and facilitating\nscientific discoveries across various disciplines. While their capabilities are\npromising, they also introduce novel vulnerabilities that demand careful\nconsideration for safety. However, there exists a notable gap in the\nliterature, as there has been no comprehensive exploration of these\nvulnerabilities. This position paper fills this gap by conducting a thorough\nexamination of vulnerabilities in LLM-based agents within scientific domains,\nshedding light on potential risks associated with their misuse and emphasizing\nthe need for safety measures. We begin by providing a comprehensive overview of\nthe potential risks inherent to scientific LLM agents, taking into account user\nintent, the specific scientific domain, and their potential impact on the\nexternal environment. Then, we delve into the origins of these vulnerabilities\nand provide a scoping review of the limited existing works. Based on our\nanalysis, we propose a triadic framework involving human regulation, agent\nalignment, and an understanding of environmental feedback (agent regulation) to\nmitigate these identified risks. Furthermore, we highlight the limitations and\nchallenges associated with safeguarding scientific agents and advocate for the\ndevelopment of improved models, robust benchmarks, and comprehensive\nregulations to address these issues effectively.",
        "updated": "2024-02-07 14:26:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04247v2"
    },
    {
        "title": "Can Generative Agents Predict Emotion?",
        "authors": "Ciaran ReganNanami IwahashiShogo TanakaMizuki Oka",
        "links": "http://arxiv.org/abs/2402.04232v2",
        "entry_id": "http://arxiv.org/abs/2402.04232v2",
        "pdf_url": "http://arxiv.org/pdf/2402.04232v2",
        "summary": "Large Language Models (LLMs) have demonstrated a number of human-like\nabilities, however the empathic understanding and emotional state of LLMs is\nyet to be aligned to that of humans. In this work, we investigate how the\nemotional state of generative LLM agents evolves as they perceive new events,\nintroducing a novel architecture in which new experiences are compared to past\nmemories. Through this comparison, the agent gains the ability to understand\nnew experiences in context, which according to the appraisal theory of emotion\nis vital in emotion creation. First, the agent perceives new experiences as\ntime series text data. After perceiving each new input, the agent generates a\nsummary of past relevant memories, referred to as the norm, and compares the\nnew experience to this norm. Through this comparison we can analyse how the\nagent reacts to the new experience in context. The PANAS, a test of affect, is\nadministered to the agent, capturing the emotional state of the agent after the\nperception of the new event. Finally, the new experience is then added to the\nagents memory to be used in the creation of future norms. By creating multiple\nexperiences in natural language from emotionally charged situations, we test\nthe proposed architecture on a wide range of scenarios. The mixed results\nsuggests that introducing context can occasionally improve the emotional\nalignment of the agent, but further study and comparison with human evaluators\nis necessary. We hope that this paper is another step towards the alignment of\ngenerative agents.",
        "updated": "2024-02-07 17:27:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04232v2"
    },
    {
        "title": "Intelligent Collective Escape of Swarm Robots Based on a Novel Fish-inspired Self-adaptive Approach with Neurodynamic Models",
        "authors": "Junfei LiSimon X. Yang",
        "links": "http://dx.doi.org/10.1109/TIE.2024.3363723",
        "entry_id": "http://arxiv.org/abs/2402.04228v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04228v1",
        "summary": "Fish schools present high-efficiency group behaviors through simple\nindividual interactions to collective migration and dynamic escape from the\npredator. The school behavior of fish is usually a good inspiration to design\ncontrol architecture for swarm robots. In this paper, a novel fish-inspired\nself-adaptive approach is proposed for collective escape for the swarm robots.\nIn addition, a bio-inspired neural network (BINN) is introduced to generate\ncollision-free escape robot trajectories through the combination of attractive\nand repulsive forces. Furthermore, to cope with dynamic environments, a\nneurodynamics-based self-adaptive mechanism is proposed to improve the\nself-adaptive performance of the swarm robots in the changing environment.\nSimilar to fish escape maneuvers, simulation and experimental results show that\nthe swarm robots are capable of collectively leaving away from the threats.\nSeveral comparison studies demonstrated that the proposed approach can\nsignificantly improve the effectiveness and efficiency of system performance,\nand the flexibility and robustness in complex environments.",
        "updated": "2024-02-06 18:36:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04228v1"
    },
    {
        "title": "\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors",
        "authors": "Lin GuanYifan ZhouDenis LiuYantian ZhaHeni Ben AmorSubbarao Kambhampati",
        "links": "http://arxiv.org/abs/2402.04210v1",
        "entry_id": "http://arxiv.org/abs/2402.04210v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04210v1",
        "summary": "Large-scale generative models are shown to be useful for sampling meaningful\ncandidate solutions, yet they often overlook task constraints and user\npreferences. Their full power is better harnessed when the models are coupled\nwith external verifiers and the final solutions are derived iteratively or\nprogressively according to the verification feedback. In the context of\nembodied AI, verification often solely involves assessing whether goal\nconditions specified in the instructions have been met. Nonetheless, for these\nagents to be seamlessly integrated into daily life, it is crucial to account\nfor a broader range of constraints and preferences beyond bare task success\n(e.g., a robot should grasp bread with care to avoid significant deformations).\nHowever, given the unbounded scope of robot tasks, it is infeasible to\nconstruct scripted verifiers akin to those used for explicit-knowledge tasks\nlike the game of Go and theorem proving. This begs the question: when no sound\nverifier is available, can we use large vision and language models (VLMs),\nwhich are approximately omniscient, as scalable Behavior Critics to catch\nundesirable robot behaviors in videos? To answer this, we first construct a\nbenchmark that contains diverse cases of goal-reaching yet undesirable robot\npolicies. Then, we comprehensively evaluate VLM critics to gain a deeper\nunderstanding of their strengths and failure modes. Based on the evaluation, we\nprovide guidelines on how to effectively utilize VLM critiques and showcase a\npractical way to integrate the feedback into an iterative process of policy\nrefinement. The dataset and codebase are released at:\nhttps://guansuns.github.io/pages/vlm-critic.",
        "updated": "2024-02-06 18:07:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04210v1"
    }
]