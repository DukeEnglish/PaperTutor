[
    {
        "title": "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls",
        "authors": "Yu DuFangyun WeiHongyang Zhang",
        "links": "http://arxiv.org/abs/2402.04253v1",
        "entry_id": "http://arxiv.org/abs/2402.04253v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04253v1",
        "summary": "We introduce AnyTool, a large language model agent designed to revolutionize\nthe utilization of a vast array of tools in addressing user queries. We utilize\nover 16,000 APIs from Rapid API, operating under the assumption that a subset\nof these APIs could potentially resolve the queries. AnyTool primarily\nincorporates three elements: an API retriever with a hierarchical structure, a\nsolver aimed at resolving user queries using a selected set of API candidates,\nand a self-reflection mechanism, which re-activates AnyTool if the initial\nsolution proves impracticable. AnyTool is powered by the function calling\nfeature of GPT-4, eliminating the need for training external modules. We also\nrevisit the evaluation protocol introduced by previous works and identify a\nlimitation in this protocol that leads to an artificially high pass rate. By\nrevising the evaluation protocol to better reflect practical application\nscenarios, we introduce an additional benchmark, termed AnyToolBench.\nExperiments across various datasets demonstrate the superiority of our AnyTool\nover strong baselines such as ToolLLM and a GPT-4 variant tailored for tool\nutilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of\naverage pass rate on ToolBench. Code will be available at\nhttps://github.com/dyabel/AnyTool.",
        "updated": "2024-02-06 18:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04253v1"
    },
    {
        "title": "Linear-time Minimum Bayes Risk Decoding with Reference Aggregation",
        "authors": "Jannis VamvasRico Sennrich",
        "links": "http://arxiv.org/abs/2402.04251v1",
        "entry_id": "http://arxiv.org/abs/2402.04251v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04251v1",
        "summary": "Minimum Bayes Risk (MBR) decoding is a text generation technique that has\nbeen shown to improve the quality of machine translations, but is expensive,\neven if a sampling-based approximation is used. Besides requiring a large\nnumber of sampled sequences, it requires the pairwise calculation of a utility\nmetric, which has quadratic complexity. In this paper, we propose to\napproximate pairwise metric scores with scores calculated against aggregated\nreference representations. This changes the complexity of utility estimation\nfrom $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains\nof MBR decoding. We release our source code at https://github.com/ZurichNLP/mbr",
        "updated": "2024-02-06 18:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04251v1"
    },
    {
        "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
        "authors": "Mantas MazeikaLong PhanXuwang YinAndy ZouZifan WangNorman MuElham SakhaeeNathaniel LiSteven BasartBo LiDavid ForsythDan Hendrycks",
        "links": "http://arxiv.org/abs/2402.04249v1",
        "entry_id": "http://arxiv.org/abs/2402.04249v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04249v1",
        "summary": "Automated red teaming holds substantial promise for uncovering and mitigating\nthe risks associated with the malicious use of large language models (LLMs),\nyet the field lacks a standardized evaluation framework to rigorously assess\nnew methods. To address this issue, we introduce HarmBench, a standardized\nevaluation framework for automated red teaming. We identify several desirable\nproperties previously unaccounted for in red teaming evaluations and\nsystematically design HarmBench to meet these criteria. Using HarmBench, we\nconduct a large-scale comparison of 18 red teaming methods and 33 target LLMs\nand defenses, yielding novel insights. We also introduce a highly efficient\nadversarial training method that greatly enhances LLM robustness across a wide\nrange of attacks, demonstrating how HarmBench enables codevelopment of attacks\nand defenses. We open source HarmBench at\nhttps://github.com/centerforaisafety/HarmBench.",
        "updated": "2024-02-06 18:59:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04249v1"
    },
    {
        "title": "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science",
        "authors": "Xiangru TangQiao JinKunlun ZhuTongxin YuanYichi ZhangWangchunshu ZhouMeng QuYilun ZhaoJian TangZhuosheng ZhangArman CohanZhiyong LuMark Gerstein",
        "links": "http://arxiv.org/abs/2402.04247v2",
        "entry_id": "http://arxiv.org/abs/2402.04247v2",
        "pdf_url": "http://arxiv.org/pdf/2402.04247v2",
        "summary": "Intelligent agents powered by large language models (LLMs) have demonstrated\nsubstantial promise in autonomously conducting experiments and facilitating\nscientific discoveries across various disciplines. While their capabilities are\npromising, they also introduce novel vulnerabilities that demand careful\nconsideration for safety. However, there exists a notable gap in the\nliterature, as there has been no comprehensive exploration of these\nvulnerabilities. This position paper fills this gap by conducting a thorough\nexamination of vulnerabilities in LLM-based agents within scientific domains,\nshedding light on potential risks associated with their misuse and emphasizing\nthe need for safety measures. We begin by providing a comprehensive overview of\nthe potential risks inherent to scientific LLM agents, taking into account user\nintent, the specific scientific domain, and their potential impact on the\nexternal environment. Then, we delve into the origins of these vulnerabilities\nand provide a scoping review of the limited existing works. Based on our\nanalysis, we propose a triadic framework involving human regulation, agent\nalignment, and an understanding of environmental feedback (agent regulation) to\nmitigate these identified risks. Furthermore, we highlight the limitations and\nchallenges associated with safeguarding scientific agents and advocate for the\ndevelopment of improved models, robust benchmarks, and comprehensive\nregulations to address these issues effectively.",
        "updated": "2024-02-07 14:26:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04247v2"
    },
    {
        "title": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations",
        "authors": "Ji QiMing DingWeihan WangYushi BaiQingsong LvWenyi HongBin XuLei HouJuanzi LiYuxiao DongJie Tang",
        "links": "http://arxiv.org/abs/2402.04236v1",
        "entry_id": "http://arxiv.org/abs/2402.04236v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04236v1",
        "summary": "Vision-Language Models (VLMs) have demonstrated their widespread viability\nthanks to extensive training in aligning visual instructions to answers.\nHowever, this conclusive alignment leads models to ignore critical visual\nreasoning, and further result in failures on meticulous visual problems and\nunfaithful responses. In this paper, we propose Chain of Manipulations, a\nmechanism that enables VLMs to solve problems with a series of manipulations,\nwhere each manipulation refers to an operation on the visual input, either from\nintrinsic abilities (e.g., grounding) acquired through prior training or from\nimitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs\nto generate faithful responses with evidential visual reasoning, and permits\nusers to trace error causes in the interpretable paths. We thus train CogCoM, a\ngeneral 17B VLM with a memory-based compatible architecture endowed this\nreasoning mechanism. Experiments show that our model achieves the\nstate-of-the-art performance across 8 benchmarks from 3 categories, and a\nlimited number of training steps with the data swiftly gains a competitive\nperformance. The code and data are publicly available at\nhttps://github.com/THUDM/CogCoM.",
        "updated": "2024-02-06 18:43:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04236v1"
    }
]