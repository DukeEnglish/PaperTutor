[
    {
        "title": "Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning",
        "authors": "Maxime ToquebiauNicolas BredecheFaïz BenamarJae-Yun Jun",
        "links": "http://arxiv.org/abs/2402.03972v1",
        "entry_id": "http://arxiv.org/abs/2402.03972v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03972v1",
        "summary": "Multi-agent deep reinforcement learning (MADRL) problems often encounter the\nchallenge of sparse rewards. This challenge becomes even more pronounced when\ncoordination among agents is necessary. As performance depends not only on one\nagent's behavior but rather on the joint behavior of multiple agents, finding\nan adequate solution becomes significantly harder. In this context, a group of\nagents can benefit from actively exploring different joint strategies in order\nto determine the most efficient one. In this paper, we propose an approach for\nrewarding strategies where agents collectively exhibit novel behaviors. We\npresent JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation\nmethod that follows the centralized learning with decentralized execution\nparadigm. JIM rewards joint trajectories based on a centralized measure of\nnovelty designed to function in continuous environments. We demonstrate the\nstrengths of this approach both in a synthetic environment designed to reveal\nshortcomings of state-of-the-art MADRL methods, and in simulated robotic tasks.\nResults show that joint exploration is crucial for solving tasks where the\noptimal strategy requires a high level of coordination.",
        "updated": "2024-02-06 13:02:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03972v1"
    },
    {
        "title": "Approximating the Core via Iterative Coalition Sampling",
        "authors": "Ian GempMarc LanctotLuke MarrisYiran MaoEdgar Duéñez-GuzmánSarah PerrinAndras GyorgyRomuald ElieGeorgios PiliourasMichael KaisersDaniel HennesKalesha BullardKate LarsonYoram Bachrach",
        "links": "http://arxiv.org/abs/2402.03928v1",
        "entry_id": "http://arxiv.org/abs/2402.03928v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03928v1",
        "summary": "The core is a central solution concept in cooperative game theory, defined as\nthe set of feasible allocations or payments such that no subset of agents has\nincentive to break away and form their own subgroup or coalition. However, it\nhas long been known that the core (and approximations, such as the least-core)\nare hard to compute. This limits our ability to analyze cooperative games in\ngeneral, and to fully embrace cooperative game theory contributions in domains\nsuch as explainable AI (XAI), where the core can complement the Shapley values\nto identify influential features or instances supporting predictions by\nblack-box models. We propose novel iterative algorithms for computing variants\nof the core, which avoid the computational bottleneck of many other approaches;\nnamely solving large linear programs. As such, they scale better to very large\nproblems as we demonstrate across different classes of cooperative games,\nincluding weighted voting games, induced subgraph games, and marginal\ncontribution networks. We also explore our algorithms in the context of XAI,\nproviding further evidence of the power of the core for such applications.",
        "updated": "2024-02-06 11:54:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03928v1"
    },
    {
        "title": "Convergence Analysis of Distributed Generalized Nash Equilibria Seeking Algorithm with Asynchrony and Delays",
        "authors": "Huaqing LiLiang RanLifeng ZhengZhe LiJinhui HuJun LiTingwen Huang",
        "links": "http://arxiv.org/abs/2402.03669v1",
        "entry_id": "http://arxiv.org/abs/2402.03669v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03669v1",
        "summary": "This paper considers a class of noncooperative games in which the feasible\ndecision sets of all players are coupled together by a coupled inequality\nconstraint. Adopting the variational inequality formulation of the game, we\nfirst introduce a new local edge-based equilibrium condition and develop a\ndistributed primal-dual proximal algorithm with full information. Considering\nchallenges when communication delays occur, we devise an asynchronous\ndistributed algorithm to seek a generalized Nash equilibrium. This asynchronous\nscheme arbitrarily activates one player to start new computations independently\nat different iteration instants, which means that the picked player can use the\ninvolved out-dated information from itself and its neighbors to perform new\nupdates. A distinctive attribute is that the proposed algorithms enable the\nderivation of new distributed forward-backward-like extensions. In theoretical\naspect, we provide explicit conditions on algorithm parameters, for instance,\nthe step-sizes to establish a sublinear convergence rate for the proposed\nsynchronous algorithm. Moreover, the asynchronous algorithm guarantees almost\nsure convergence in expectation under the same step-size conditions and some\nstandard assumptions. An interesting observation is that our analysis approach\nimproves the convergence rate of prior synchronous distributed\nforward-backward-based algorithms. Finally, the viability and performance of\nthe proposed algorithms are demonstrated by numerical studies on the networked\nCournot competition.",
        "updated": "2024-02-06 03:46:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03669v1"
    },
    {
        "title": "Agent-Based Triangle Counting and its Applications in Anonymous Graphs",
        "authors": "Prabhat Kumar ChandApurba DasAnisur Rahaman Molla",
        "links": "http://arxiv.org/abs/2402.03653v1",
        "entry_id": "http://arxiv.org/abs/2402.03653v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03653v1",
        "summary": "Triangle counting in a graph is a fundamental problem and has a wide range of\napplications in various domains. It is crucial in understanding the structural\nproperties of a graph and is often used as a building block for more complex\ngraph analytics. In this paper, we solve the triangle counting problem in an\nanonymous graph in a distributed setting using mobile agents and subsequently\nuse this as a subroutine to tackle the truss decomposition and triangle\ncentrality problem. The paper employs mobile agents, placed on the nodes of the\ngraph to coordinate among themselves to solve the triangle enumeration problem\nfor the graph. Following the literature, we consider the synchronous systems\nwhere each robot executes its tasks concurrently with all others and hence time\ncomplexity can be measured as the number of rounds needed to complete the task.\nThe graph is anonymous, i.e., without any node labels or IDs, but the agents\nare autonomous with distinct IDs and have limited memory. Agents can only\ncommunicate with other agents locally i.e., if and only if they are at the same\nnode. The goal is to devise algorithms that minimise both the time required for\ntriangle counting and the memory usage at each agent. We further demonstrate\nhow the triangle count obtained through the mobile agent approach can be\nleveraged to address the truss decomposition, triangle centrality and local\nclustering coefficient problems, which involves finding maximal sub-graphs with\nstrong interconnections. Truss decomposition helps in identifying maximal,\nhighly interconnected sub-graphs, or trusses, within a network, thus, revealing\nthe structural cohesion and tight-knit communities in complex graphs,\nfacilitating the analysis of relationships and information flow in various\nfields, such as social networks, biology, and recommendation systems.",
        "updated": "2024-02-06 03:00:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03653v1"
    },
    {
        "title": "Assessing the Impact of Distribution Shift on Reinforcement Learning Performance",
        "authors": "Ted FujimotoJoshua SuetterleinSamrat ChatterjeeAuroop Ganguly",
        "links": "http://arxiv.org/abs/2402.03590v1",
        "entry_id": "http://arxiv.org/abs/2402.03590v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03590v1",
        "summary": "Research in machine learning is making progress in fixing its own\nreproducibility crisis. Reinforcement learning (RL), in particular, faces its\nown set of unique challenges. Comparison of point estimates, and plots that\nshow successful convergence to the optimal policy during training, may\nobfuscate overfitting or dependence on the experimental setup. Although\nresearchers in RL have proposed reliability metrics that account for\nuncertainty to better understand each algorithm's strengths and weaknesses, the\nrecommendations of past work do not assume the presence of out-of-distribution\nobservations. We propose a set of evaluation methods that measure the\nrobustness of RL algorithms under distribution shifts. The tools presented here\nargue for the need to account for performance over time while the agent is\nacting in its environment. In particular, we recommend time series analysis as\na method of observational RL evaluation. We also show that the unique\nproperties of RL and simulated dynamic environments allow us to make stronger\nassumptions to justify the measurement of causal impact in our evaluations. We\nthen apply these tools to single-agent and multi-agent environments to show the\nimpact of introducing distribution shifts during test time. We present this\nmethodology as a first step toward rigorous RL evaluation in the presence of\ndistribution shifts.",
        "updated": "2024-02-05 23:50:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03590v1"
    }
]