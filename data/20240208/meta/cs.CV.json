[
    {
        "title": "EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters",
        "authors": "Quan SunJinsheng WangQiying YuYufeng CuiFan ZhangXiaosong ZhangXinlong Wang",
        "links": "http://arxiv.org/abs/2402.04252v1",
        "entry_id": "http://arxiv.org/abs/2402.04252v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04252v1",
        "summary": "Scaling up contrastive language-image pretraining (CLIP) is critical for\nempowering both vision and multimodal models. We present EVA-CLIP-18B, the\nlargest and most powerful open-source CLIP model to date, with 18-billion\nparameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an\nexceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized\nimage classification benchmarks, outperforming its forerunner EVA-CLIP\n(5-billion parameters) and other open-source CLIP models by a large margin.\nRemarkably, we observe a consistent performance improvement with the model size\nscaling of EVA-CLIP, despite maintaining a constant training dataset of\n2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly\navailable and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B)\nemployed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the\npotential of EVA-style weak-to-strong visual model scaling. With our model\nweights made publicly available, we hope to facilitate future research in\nvision and multimodal foundation models.",
        "updated": "2024-02-06 18:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04252v1"
    },
    {
        "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
        "authors": "Mantas MazeikaLong PhanXuwang YinAndy ZouZifan WangNorman MuElham SakhaeeNathaniel LiSteven BasartBo LiDavid ForsythDan Hendrycks",
        "links": "http://arxiv.org/abs/2402.04249v1",
        "entry_id": "http://arxiv.org/abs/2402.04249v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04249v1",
        "summary": "Automated red teaming holds substantial promise for uncovering and mitigating\nthe risks associated with the malicious use of large language models (LLMs),\nyet the field lacks a standardized evaluation framework to rigorously assess\nnew methods. To address this issue, we introduce HarmBench, a standardized\nevaluation framework for automated red teaming. We identify several desirable\nproperties previously unaccounted for in red teaming evaluations and\nsystematically design HarmBench to meet these criteria. Using HarmBench, we\nconduct a large-scale comparison of 18 red teaming methods and 33 target LLMs\nand defenses, yielding novel insights. We also introduce a highly efficient\nadversarial training method that greatly enhances LLM robustness across a wide\nrange of attacks, demonstrating how HarmBench enables codevelopment of attacks\nand defenses. We open source HarmBench at\nhttps://github.com/centerforaisafety/HarmBench.",
        "updated": "2024-02-06 18:59:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04249v1"
    },
    {
        "title": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations",
        "authors": "Ji QiMing DingWeihan WangYushi BaiQingsong LvWenyi HongBin XuLei HouJuanzi LiYuxiao DongJie Tang",
        "links": "http://arxiv.org/abs/2402.04236v1",
        "entry_id": "http://arxiv.org/abs/2402.04236v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04236v1",
        "summary": "Vision-Language Models (VLMs) have demonstrated their widespread viability\nthanks to extensive training in aligning visual instructions to answers.\nHowever, this conclusive alignment leads models to ignore critical visual\nreasoning, and further result in failures on meticulous visual problems and\nunfaithful responses. In this paper, we propose Chain of Manipulations, a\nmechanism that enables VLMs to solve problems with a series of manipulations,\nwhere each manipulation refers to an operation on the visual input, either from\nintrinsic abilities (e.g., grounding) acquired through prior training or from\nimitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs\nto generate faithful responses with evidential visual reasoning, and permits\nusers to trace error causes in the interpretable paths. We thus train CogCoM, a\ngeneral 17B VLM with a memory-based compatible architecture endowed this\nreasoning mechanism. Experiments show that our model achieves the\nstate-of-the-art performance across 8 benchmarks from 3 categories, and a\nlimited number of training steps with the data swiftly gains a competitive\nperformance. The code and data are publicly available at\nhttps://github.com/THUDM/CogCoM.",
        "updated": "2024-02-06 18:43:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04236v1"
    },
    {
        "title": "Instance by Instance: An Iterative Framework for Multi-instance 3D Registration",
        "authors": "Xinyue CaoXiyu ZhangYuxin ChengZhaoshuai QiYanning ZhangJiaqi Yang",
        "links": "http://arxiv.org/abs/2402.04195v1",
        "entry_id": "http://arxiv.org/abs/2402.04195v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04195v1",
        "summary": "Multi-instance registration is a challenging problem in computer vision and\nrobotics, where multiple instances of an object need to be registered in a\nstandard coordinate system. In this work, we propose the first iterative\nframework called instance-by-instance (IBI) for multi-instance 3D registration\n(MI-3DReg). It successively registers all instances in a given scenario,\nstarting from the easiest and progressing to more challenging ones. Throughout\nthe iterative process, outliers are eliminated continuously, leading to an\nincreasing inlier rate for the remaining and more challenging instances. Under\nthe IBI framework, we further propose a sparse-to-dense-correspondence-based\nmulti-instance registration method (IBI-S2DC) to achieve robust MI-3DReg.\nExperiments on the synthetic and real datasets have demonstrated the\neffectiveness of IBI and suggested the new state-of-the-art performance of\nIBI-S2DC, e.g., our MHF1 is 12.02%/12.35% higher than the existing\nstate-of-the-art method ECC on the synthetic/real datasets.",
        "updated": "2024-02-06 17:50:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04195v1"
    },
    {
        "title": "SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models",
        "authors": "Yichen ShiYuhao GaoYingxin LaiHongyang WangJun FengLei HeJun WanChangsheng ChenZitong YuXiaochun Cao",
        "links": "http://arxiv.org/abs/2402.04178v1",
        "entry_id": "http://arxiv.org/abs/2402.04178v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04178v1",
        "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\nproblem-solving capabilities in various vision fields (e.g., generic object\nrecognition and grounding) based on strong visual semantic representation and\nlanguage reasoning ability. However, whether MLLMs are sensitive to subtle\nvisual spoof/forged clues and how they perform in the domain of face attack\ndetection (e.g., face spoofing and forgery detection) is still unexplored. In\nthis paper, we introduce a new benchmark, namely SHIELD, to evaluate the\nability of MLLMs on face spoofing and forgery detection. Specifically, we\ndesign true/false and multiple-choice questions to evaluate multimodal face\ndata in these two face security tasks. For the face anti-spoofing task, we\nevaluate three different modalities (i.e., RGB, infrared, depth) under four\ntypes of presentation attacks (i.e., print attack, replay attack, rigid mask,\npaper mask). For the face forgery detection task, we evaluate GAN-based and\ndiffusion-based data with both visual and acoustic modalities. Each question is\nsubjected to both zero-shot and few-shot tests under standard and chain of\nthought (COT) settings. The results indicate that MLLMs hold substantial\npotential in the face security domain, offering advantages over traditional\nspecific models in terms of interpretability, multimodal flexible reasoning,\nand joint face spoof and forgery detection. Additionally, we develop a novel\nMulti-Attribute Chain of Thought (MA-COT) paradigm for describing and judging\nvarious task-specific and task-irrelevant attributes of face images, which\nprovides rich task-related knowledge for subtle spoof/forged clue mining.\nExtensive experiments in separate face anti-spoofing, separate face forgery\ndetection, and joint detection tasks demonstrate the effectiveness of the\nproposed MA-COT. The project is available at\nhttps$:$//github.com/laiyingxin2/SHIELD",
        "updated": "2024-02-06 17:31:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04178v1"
    }
]