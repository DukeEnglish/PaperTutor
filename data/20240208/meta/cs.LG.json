[
    {
        "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
        "authors": "Mantas MazeikaLong PhanXuwang YinAndy ZouZifan WangNorman MuElham SakhaeeNathaniel LiSteven BasartBo LiDavid ForsythDan Hendrycks",
        "links": "http://arxiv.org/abs/2402.04249v1",
        "entry_id": "http://arxiv.org/abs/2402.04249v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04249v1",
        "summary": "Automated red teaming holds substantial promise for uncovering and mitigating\nthe risks associated with the malicious use of large language models (LLMs),\nyet the field lacks a standardized evaluation framework to rigorously assess\nnew methods. To address this issue, we introduce HarmBench, a standardized\nevaluation framework for automated red teaming. We identify several desirable\nproperties previously unaccounted for in red teaming evaluations and\nsystematically design HarmBench to meet these criteria. Using HarmBench, we\nconduct a large-scale comparison of 18 red teaming methods and 33 target LLMs\nand defenses, yielding novel insights. We also introduce a highly efficient\nadversarial training method that greatly enhances LLM robustness across a wide\nrange of attacks, demonstrating how HarmBench enables codevelopment of attacks\nand defenses. We open source HarmBench at\nhttps://github.com/centerforaisafety/HarmBench.",
        "updated": "2024-02-06 18:59:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04249v1"
    },
    {
        "title": "Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks",
        "authors": "Jongho ParkJaeseung ParkZheyang XiongNayoung LeeJaewoong ChoSamet OymakKangwook LeeDimitris Papailiopoulos",
        "links": "http://arxiv.org/abs/2402.04248v1",
        "entry_id": "http://arxiv.org/abs/2402.04248v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04248v1",
        "summary": "State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed\nas alternatives to Transformer networks in language modeling, by incorporating\ngating, convolutions, and input-dependent token selection to mitigate the\nquadratic cost of multi-head attention. Although SSMs exhibit competitive\nperformance, their in-context learning (ICL) capabilities, a remarkable\nemergent property of modern language models that enables task execution without\nparameter optimization, remain underexplored compared to Transformers. In this\nstudy, we evaluate the ICL performance of SSMs, focusing on Mamba, against\nTransformer models across various tasks. Our results show that SSMs perform\ncomparably to Transformers in standard regression ICL tasks, while\noutperforming them in tasks like sparse parity learning. However, SSMs fall\nshort in tasks involving non-standard retrieval functionality. To address these\nlimitations, we introduce a hybrid model, \\variant, that combines Mamba with\nattention blocks, surpassing individual models in tasks where they struggle\nindependently. Our findings suggest that hybrid architectures offer promising\navenues for enhancing ICL in language models.",
        "updated": "2024-02-06 18:56:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04248v1"
    },
    {
        "title": "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science",
        "authors": "Xiangru TangQiao JinKunlun ZhuTongxin YuanYichi ZhangWangchunshu ZhouMeng QuYilun ZhaoJian TangZhuosheng ZhangArman CohanZhiyong LuMark Gerstein",
        "links": "http://arxiv.org/abs/2402.04247v2",
        "entry_id": "http://arxiv.org/abs/2402.04247v2",
        "pdf_url": "http://arxiv.org/pdf/2402.04247v2",
        "summary": "Intelligent agents powered by large language models (LLMs) have demonstrated\nsubstantial promise in autonomously conducting experiments and facilitating\nscientific discoveries across various disciplines. While their capabilities are\npromising, they also introduce novel vulnerabilities that demand careful\nconsideration for safety. However, there exists a notable gap in the\nliterature, as there has been no comprehensive exploration of these\nvulnerabilities. This position paper fills this gap by conducting a thorough\nexamination of vulnerabilities in LLM-based agents within scientific domains,\nshedding light on potential risks associated with their misuse and emphasizing\nthe need for safety measures. We begin by providing a comprehensive overview of\nthe potential risks inherent to scientific LLM agents, taking into account user\nintent, the specific scientific domain, and their potential impact on the\nexternal environment. Then, we delve into the origins of these vulnerabilities\nand provide a scoping review of the limited existing works. Based on our\nanalysis, we propose a triadic framework involving human regulation, agent\nalignment, and an understanding of environmental feedback (agent regulation) to\nmitigate these identified risks. Furthermore, we highlight the limitations and\nchallenges associated with safeguarding scientific agents and advocate for the\ndevelopment of improved models, robust benchmarks, and comprehensive\nregulations to address these issues effectively.",
        "updated": "2024-02-07 14:26:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04247v2"
    },
    {
        "title": "CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers",
        "authors": "Adjorn van EngelenhovenNicola StrisciuglioEstefanía Talavera",
        "links": "http://arxiv.org/abs/2402.04239v1",
        "entry_id": "http://arxiv.org/abs/2402.04239v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04239v1",
        "summary": "The Transformer architecture has shown to be a powerful tool for a wide range\nof tasks. It is based on the self-attention mechanism, which is an inherently\ncomputationally expensive operation with quadratic computational complexity:\nmemory usage and compute time increase quadratically with the length of the\ninput sequences, thus limiting the application of Transformers. In this work,\nwe propose a novel Clustering self-Attention mechanism using Surrogate Tokens\n(CAST), to optimize the attention computation and achieve efficient\ntransformers. CAST utilizes learnable surrogate tokens to construct a cluster\naffinity matrix, used to cluster the input sequence and generate novel cluster\nsummaries. The self-attention from within each cluster is then combined with\nthe cluster summaries of other clusters, enabling information flow across the\nentire input sequence. CAST improves efficiency by reducing the complexity from\n$O(N^2)$ to $O(\\alpha N)$ where N is the sequence length, and {\\alpha} is\nconstant according to the number of clusters and samples per cluster. We show\nthat CAST performs better than or comparable to the baseline Transformers on\nlong-range sequence modeling tasks, while also achieving higher results on time\nand memory efficiency than other efficient transformers.",
        "updated": "2024-02-06 18:47:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04239v1"
    },
    {
        "title": "MusicRL: Aligning Music Generation to Human Preferences",
        "authors": "Geoffrey CideronSertan GirginMauro VerzettiDamien VincentMatej KastelicZalán BorsosBrian McWilliamsVictor UngureanuOlivier BachemOlivier PietquinMatthieu GeistLéonard HussenotNeil ZeghidourAndrea Agostinelli",
        "links": "http://arxiv.org/abs/2402.04229v1",
        "entry_id": "http://arxiv.org/abs/2402.04229v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04229v1",
        "summary": "We propose MusicRL, the first music generation system finetuned from human\nfeedback. Appreciation of text-to-music models is particularly subjective since\nthe concept of musicality as well as the specific intention behind a caption\nare user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a\nretro guitar solo or a techno pop beat). Not only this makes supervised\ntraining of such models challenging, but it also calls for integrating\ncontinuous human feedback in their post-deployment finetuning. MusicRL is a\npretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete\naudio tokens finetuned with reinforcement learning to maximise sequence-level\nrewards. We design reward functions related specifically to text-adherence and\naudio quality with the help from selected raters, and use those to finetune\nMusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial\ndataset comprising 300,000 pairwise preferences. Using Reinforcement Learning\nfrom Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model\nthat incorporates human feedback at scale. Human evaluations show that both\nMusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU\ncombines the two approaches and results in the best model according to human\nraters. Ablation studies shed light on the musical attributes influencing human\npreferences, indicating that text adherence and quality only account for a part\nof it. This underscores the prevalence of subjectivity in musical appreciation\nand calls for further involvement of human listeners in the finetuning of music\ngeneration models.",
        "updated": "2024-02-06 18:36:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04229v1"
    }
]