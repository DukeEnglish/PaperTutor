[
    {
        "title": "Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People",
        "authors": "Dun-Ming HuangPol Van RijnIlia SucholutskyRaja MarjiehNori Jacoby",
        "links": "http://arxiv.org/abs/2406.04278v1",
        "entry_id": "http://arxiv.org/abs/2406.04278v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04278v1",
        "summary": "Conversational tones -- the manners and attitudes in which speakers\ncommunicate -- are essential to effective communication. Amidst the increasing\npopularization of Large Language Models (LLMs) over recent years, it becomes\nnecessary to characterize the divergences in their conversational tones\nrelative to humans. However, existing investigations of conversational\nmodalities rely on pre-existing taxonomies or text corpora, which suffer from\nexperimenter bias and may not be representative of real-world distributions for\nthe studies' psycholinguistic domains. Inspired by methods from cognitive\nscience, we propose an iterative method for simultaneously eliciting\nconversational tones and sentences, where participants alternate between two\ntasks: (1) one participant identifies the tone of a given sentence and (2) a\ndifferent participant generates a sentence based on that tone. We run 100\niterations of this process with human participants and GPT-4, then obtain a\ndataset of sentences and frequent conversational tones. In an additional\nexperiment, humans and GPT-4 annotated all sentences with all tones. With data\nfrom 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4\nqueries, we show how our approach can be used to create an interpretable\ngeometric representation of relations between conversational tones in humans\nand GPT-4. This work demonstrates how combining ideas from machine learning and\ncognitive science can address challenges in human-computer interactions.",
        "updated": "2024-06-06 17:26:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04278v1"
    },
    {
        "title": "The 3D-PC: a benchmark for visual perspective taking in humans and machines",
        "authors": "Drew LinsleyPeisen ZhouAlekh Karkada AshokAkash NagarajGaurav GaonkarFrancis E LewisZygmunt PizloThomas Serre",
        "links": "http://arxiv.org/abs/2406.04138v1",
        "entry_id": "http://arxiv.org/abs/2406.04138v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04138v1",
        "summary": "Visual perspective taking (VPT) is the ability to perceive and reason about\nthe perspectives of others. It is an essential feature of human intelligence,\nwhich develops over the first decade of life and requires an ability to process\nthe 3D structure of visual scenes. A growing number of reports have indicated\nthat deep neural networks (DNNs) become capable of analyzing 3D scenes after\ntraining on large image datasets. We investigated if this emergent ability for\n3D analysis in DNNs is sufficient for VPT with the 3D perception challenge\n(3D-PC): a novel benchmark for 3D perception in humans and DNNs. The 3D-PC is\ncomprised of three 3D-analysis tasks posed within natural scene images: 1. a\nsimple test of object depth order, 2. a basic VPT task (VPT-basic), and 3.\nanother version of VPT (VPT-Strategy) designed to limit the effectiveness of\n\"shortcut\" visual strategies. We tested human participants (N=33) and linearly\nprobed or text-prompted over 300 DNNs on the challenge and found that nearly\nall of the DNNs approached or exceeded human accuracy in analyzing object depth\norder. Surprisingly, DNN accuracy on this task correlated with their object\nrecognition performance. In contrast, there was an extraordinary gap between\nDNNs and humans on VPT-basic. Humans were nearly perfect, whereas most DNNs\nwere near chance. Fine-tuning DNNs on VPT-basic brought them close to human\nperformance, but they, unlike humans, dropped back to chance when tested on\nVPT-perturb. Our challenge demonstrates that the training routines and\narchitectures of today's DNNs are well-suited for learning basic 3D properties\nof scenes and objects but are ill-suited for reasoning about these properties\nlike humans do. We release our 3D-PC datasets and code to help bridge this gap\nin 3D perception between humans and machines.",
        "updated": "2024-06-06 14:59:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04138v1"
    },
    {
        "title": "Watching Popular Musicians Learn by Ear: A Hypothesis-Generating Study of Human-Recording Interactions in YouTube Videos",
        "authors": "Christopher LiscioDaniel G. Brown",
        "links": "http://arxiv.org/abs/2406.04058v1",
        "entry_id": "http://arxiv.org/abs/2406.04058v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04058v1",
        "summary": "Popular musicians often learn music by ear. It is unclear what role\ntechnology plays for those with experience at this task. In search of\nopportunities for the development of novel human-recording interactions, we\nanalyze 18 YouTube videos depicting real-world examples of by-ear learning, and\ndiscuss why, during this preliminary phase of research, online videos are\nappropriate data. From our observations we generate hypotheses that can inform\nfuture work. For example, a musician's scope of learning may influence what\ntechnological interactions would help them, they could benefit from tools that\naccommodate their working memory, and transcription does not appear to play a\nkey role in ear learning. Based on these findings, we pose a number of research\nquestions, and discuss their methodological considerations to guide future\nstudy.",
        "updated": "2024-06-06 13:25:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04058v1"
    },
    {
        "title": "Exploring Topic Modelling of User Reviews as a Monitoring Mechanism for Emergent Issues Within Social VR Communities",
        "authors": "Angelo SinghJoseph O'Hagan",
        "links": "http://arxiv.org/abs/2406.03994v1",
        "entry_id": "http://arxiv.org/abs/2406.03994v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03994v1",
        "summary": "Users of social virtual reality (VR) platforms often use user reviews to\ndocument incidents of witnessed and/or experienced user harassment. However, at\npresent, research has yet to be explore utilising this data as a monitoring\nmechanism to identify emergent issues within social VR communities. Such a\nsystem would be of much benefit to developers and researchers as it would\nenable the automatic identification of emergent issues as they occur, provide a\nmeans of longitudinally analysing harassment, and reduce the reliance on\nalternative, high cost, monitoring methodologies, e.g. observation or interview\nstudies. To contribute towards the development of such a system, we collected\napproximately 40,000 Rec Room user reviews from the Steam storefront. We then\nanalysed our dataset's sentiment, word/term frequencies, and conducted a topic\nmodelling analysis of the negative reviews detected in our dataset. We report\nour approach was capable of longitudinally monitoring changes in review\nsentiment and identifying high level themes related to types of harassment\nknown to occur in social VR platforms.",
        "updated": "2024-06-06 12:15:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03994v1"
    },
    {
        "title": "POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models",
        "authors": "Jianben HeXingbo WangShiyi LiuGuande WuClaudio SilvaHuamin Qu",
        "links": "http://arxiv.org/abs/2406.03843v1",
        "entry_id": "http://arxiv.org/abs/2406.03843v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03843v1",
        "summary": "Large language models (LLMs) have exhibited impressive abilities for\nmultimodal content comprehension and reasoning with proper prompting in zero-\nor few-shot settings. Despite the proliferation of interactive systems\ndeveloped to support prompt engineering for LLMs across various tasks, most\nhave primarily focused on textual or visual inputs, thus neglecting the complex\ninterplay between modalities within multimodal inputs. This oversight hinders\nthe development of effective prompts that guide model multimodal reasoning\nprocesses by fully exploiting the rich context provided by multiple modalities.\nIn this paper, we present POEM, a visual analytics system to facilitate\nefficient prompt engineering for enhancing the multimodal reasoning performance\nof LLMs. The system enables users to explore the interaction patterns across\nmodalities at varying levels of detail for a comprehensive understanding of the\nmultimodal knowledge elicited by various prompts. Through diverse\nrecommendations of demonstration examples and instructional principles, POEM\nsupports users in iteratively crafting and refining prompts to better align and\nenhance model knowledge with human insights. The effectiveness and efficiency\nof our system are validated through two case studies and interviews with\nexperts.",
        "updated": "2024-06-06 08:21:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03843v1"
    }
]