[
    {
        "title": "Verbalized Machine Learning: Revisiting Machine Learning with Language Models",
        "authors": "Tim Z. XiaoRobert BamlerBernhard SchölkopfWeiyang Liu",
        "links": "http://arxiv.org/abs/2406.04344v1",
        "entry_id": "http://arxiv.org/abs/2406.04344v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04344v1",
        "summary": "Motivated by the large progress made by large language models (LLMs), we\nintroduce the framework of verbalized machine learning (VML). In contrast to\nconventional machine learning models that are typically optimized over a\ncontinuous parameter space, VML constrains the parameter space to be\nhuman-interpretable natural language. Such a constraint leads to a new\nperspective of function approximation, where an LLM with a text prompt can be\nviewed as a function parameterized by the text prompt. Guided by this\nperspective, we revisit classical machine learning problems, such as regression\nand classification, and find that these problems can be solved by an\nLLM-parameterized learner and optimizer. The major advantages of VML include\n(1) easy encoding of inductive bias: prior knowledge about the problem and\nhypothesis class can be encoded in natural language and fed into the\nLLM-parameterized learner; (2) automatic model class selection: the optimizer\ncan automatically select a concrete model class based on data and verbalized\nprior knowledge, and it can update the model class during training; and (3)\ninterpretable learner updates: the LLM-parameterized optimizer can provide\nexplanations for why each learner update is performed. We conduct several\nstudies to empirically evaluate the effectiveness of VML, and hope that VML can\nserve as a stepping stone to stronger interpretability and trustworthiness in\nML.",
        "updated": "2024-06-06 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04344v1"
    },
    {
        "title": "On the Expressive Power of Spectral Invariant Graph Neural Networks",
        "authors": "Bohang ZhangLingxiao ZhaoHaggai Maron",
        "links": "http://arxiv.org/abs/2406.04336v1",
        "entry_id": "http://arxiv.org/abs/2406.04336v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04336v1",
        "summary": "Incorporating spectral information to enhance Graph Neural Networks (GNNs)\nhas shown promising results but raises a fundamental challenge due to the\ninherent ambiguity of eigenvectors. Various architectures have been proposed to\naddress this ambiguity, referred to as spectral invariant architectures.\nNotable examples include GNNs and Graph Transformers that use spectral\ndistances, spectral projection matrices, or other invariant spectral features.\nHowever, the potential expressive power of these spectral invariant\narchitectures remains largely unclear. The goal of this work is to gain a deep\ntheoretical understanding of the expressive power obtainable when using\nspectral features. We first introduce a unified message-passing framework for\ndesigning spectral invariant GNNs, called Eigenspace Projection GNN (EPNN). A\ncomprehensive analysis shows that EPNN essentially unifies all prior spectral\ninvariant architectures, in that they are either strictly less expressive or\nequivalent to EPNN. A fine-grained expressiveness hierarchy among different\narchitectures is also established. On the other hand, we prove that EPNN itself\nis bounded by a recently proposed class of Subgraph GNNs, implying that all\nthese spectral invariant architectures are strictly less expressive than 3-WL.\nFinally, we discuss whether using spectral features can gain additional\nexpressiveness when combined with more expressive GNNs.",
        "updated": "2024-06-06 17:59:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04336v1"
    },
    {
        "title": "Coarse-To-Fine Tensor Trains for Compact Visual Representations",
        "authors": "Sebastian LoeschckeDan WangChristian Leth-EspensenSerge BelongieMichael J. KastoryanoSagie Benaim",
        "links": "http://arxiv.org/abs/2406.04332v1",
        "entry_id": "http://arxiv.org/abs/2406.04332v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04332v1",
        "summary": "The ability to learn compact, high-quality, and easy-to-optimize\nrepresentations for visual data is paramount to many applications such as novel\nview synthesis and 3D reconstruction. Recent work has shown substantial success\nin using tensor networks to design such compact and high-quality\nrepresentations. However, the ability to optimize tensor-based representations,\nand in particular, the highly compact tensor train representation, is still\nlacking. This has prevented practitioners from deploying the full potential of\ntensor networks for visual data. To this end, we propose 'Prolongation\nUpsampling Tensor Train (PuTT)', a novel method for learning tensor train\nrepresentations in a coarse-to-fine manner. Our method involves the prolonging\nor `upsampling' of a learned tensor train representation, creating a sequence\nof 'coarse-to-fine' tensor trains that are incrementally refined. We evaluate\nour representation along three axes: (1). compression, (2). denoising\ncapability, and (3). image completion capability. To assess these axes, we\nconsider the tasks of image fitting, 3D fitting, and novel view synthesis,\nwhere our method shows an improved performance compared to state-of-the-art\ntensor-based methods. For full results see our project webpage:\nhttps://sebulo.github.io/PuTT_website/",
        "updated": "2024-06-06 17:59:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04332v1"
    },
    {
        "title": "Simplified and Generalized Masked Diffusion for Discrete Data",
        "authors": "Jiaxin ShiKehang HanZhe WangArnaud DoucetMichalis K. Titsias",
        "links": "http://arxiv.org/abs/2406.04329v1",
        "entry_id": "http://arxiv.org/abs/2406.04329v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04329v1",
        "summary": "Masked (or absorbing) diffusion is actively explored as an alternative to\nautoregressive models for generative modeling of discrete data. However,\nexisting work in this area has been hindered by unnecessarily complex model\nformulations and unclear relationships between different perspectives, leading\nto suboptimal parameterization, training objectives, and ad hoc adjustments to\ncounteract these issues. In this work, we aim to provide a simple and general\nframework that unlocks the full potential of masked diffusion models. We show\nthat the continuous-time variational objective of masked diffusion models is a\nsimple weighted integral of cross-entropy losses. Our framework also enables\ntraining generalized masked diffusion models with state-dependent masking\nschedules. When evaluated by perplexity, our models trained on OpenWebText\nsurpass prior diffusion language models at GPT-2 scale and demonstrate superior\nperformance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our\nmodels vastly outperform previous discrete diffusion models on pixel-level\nimage modeling, achieving 2.78~(CIFAR-10) and 3.42 (ImageNet 64$\\times$64) bits\nper dimension that are comparable or better than autoregressive models of\nsimilar sizes.",
        "updated": "2024-06-06 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04329v1"
    },
    {
        "title": "PaCE: Parsimonious Concept Engineering for Large Language Models",
        "authors": "Jinqi LuoTianjiao DingKwan Ho Ryan ChanDarshan ThakerAditya ChattopadhyayChris Callison-BurchRené Vidal",
        "links": "http://arxiv.org/abs/2406.04331v1",
        "entry_id": "http://arxiv.org/abs/2406.04331v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04331v1",
        "summary": "Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable output, via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Then,\ngiven any alignment task, we instruct a concept partitioner to efficiently\nannotate the concepts as benign or undesirable. Finally, at inference time, we\ndecompose the LLM activations along the concept dictionary via sparse coding,\nto accurately represent the activation as a linear combination of the benign\nand undesirable components. By removing the latter ones from the activation, we\nreorient the behavior of LLMs towards alignment goals. We conduct experiments\non tasks such as response detoxification, faithfulness enhancement, and\nsentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities.",
        "updated": "2024-06-06 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04331v1"
    }
]