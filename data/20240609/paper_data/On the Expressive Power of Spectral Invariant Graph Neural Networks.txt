On the Expressive Power of Spectral Invariant Graph Neural Networks
BohangZhang1 LingxiaoZhao2 HaggaiMaron34
Abstract tionalandstructuralencodingsinGraphNeuralNetworks
(GNNs) and Graph Transformers (GTs) (Dwivedi et al.,
Incorporating spectral information to enhance
2020;Dwivedi&Bresson,2021;Kreuzeretal.,2021;Ram-
Graph Neural Networks (GNNs) has shown
paseketal.,2022;Kimetal.,2022). Thesespectralfeatures
promisingresultsbutraisesafundamentalchal-
encapsulatevaluableinformationaboutgraphconnectivity,
lengeduetotheinherentambiguityofeigenvec-
inter-node distances, node clustering patterns, and more.
tors. Various architectures have been proposed
When using eigenvectors as inputs for machine learning
toaddressthisambiguity,referredtoasspectral
models,amajorchallengearisesduetotheinherenteigenes-
invariantarchitectures. Notableexamplesinclude
pacesymmetry(Limetal.,2023)—eigenvectorsarenot
GNNsandGraphTransformersthatusespectral
unique. Specifically, for any eigenvector v, −v is also a
distances, spectral projection matrices, or other
valideigenvector. Theambiguitybecomesworseinthecase
invariant spectral features. However, the poten-
ofrepeatedeigenvalues;here,anyorthogonaltransforma-
tialexpressivepowerofthesespectralinvariant
tion of the basis vectors in a particular eigenspace yields
architectures remains largely unclear. The goal
alternativebutequivalentinputrepresentations.
ofthisworkistogainadeeptheoreticalunder-
standingoftheexpressivepowerobtainablewhen Toaddresstheambiguityproblem, amajorlineofrecent
usingspectralfeatures. Wefirstintroduceauni- worksleveragesinvariantfeaturesderivedfromeigenvec-
fied message-passing framework for designing tors and eigenvalues to design spectral invariant architec-
spectralinvariantGNNs,calledEigenspacePro- tures. Popularchoicesforsuchfeaturesincludeeigenspace
jectionGNN(EPNN).Acomprehensiveanalysis projectionmatrices1(Limetal.,2023;Huangetal.,2024),
showsthatEPNNessentiallyunifiesallpriorspec- spectralnodedistances(e.g.,thoseassociatedwithrandom
tralinvariantarchitectures,inthattheyareeither walks or graph diffusion) (Li et al., 2020; Zhang et al.,
strictlylessexpressiveorequivalenttoEPNN.A 2023b; Feldman et al., 2023), or other invariant spectral
fine-grainedexpressivenesshierarchyamongdif- characteristics(Wangetal.,2022). Allofthesefeaturescan
ferent architectures is also established. On the beeasilyintegratedintoGNNstoenhanceedgefeaturesor
otherhand,weprovethatEPNNitselfisbounded functionasrelativepositionalencodingofGTs. However,
byarecentlyproposedclassofSubgraphGNNs, onthetheoreticalside,whiletheexpressivepowerofGNNs
implying that all these spectral invariant archi- hasbeenstudiedextensively(Xuetal.,2019;Maronetal.,
tectures are strictly less expressive than 3-WL. 2019a;Morrisetal.,2021;Geerts&Reutter,2022;Zhang
Finally, we discuss whether using spectral fea- etal.,2024),thereremainslittleunderstandingoftheimpor-
tures can gain additional expressiveness when tantcategoryrepresentedbyspectralinvariantGNNs/GTs.
combinedwithmoreexpressiveGNNs.
Currentwork.Thegoalofthisworkistogaindeepinsights
intotheexpressivepowerofspectralinvariantarchitectures
andestablishacompleteexpressivenesshierarchy. Webe-
1.Introduction gin by presenting Eigenspace Projection GNN (EPNN),
a novel GNN framework that unifies the study of all the
Recentworkshavedemonstratedthepromiseofusingspec-
aforementionedspectralinvariantmethods. EPNNisvery
tralgraphfeatures,particularlytheeigenvaluesandeigen-
simple: itencodesallspectralinformationforanodepair
vectorsofthegraphLaplacianorfunctionsthereof,asposi-
(u,v)asasetcontainingthevaluesofallprojectionmatri-
cesonthatnodepair,alongwiththeassociatedeigenvalues.
1Peking University 2Carnegie Mellon University 3Technion
4NVIDIAResearch.Correspondenceto:BohangZhang<zhang- It then computes and refines node representations using
bohang@pku.edu.cn>,HaggaiMaron<hmaron@nvidia.com>. thespectralinformationasedgefeatureswithinastandard
message-passingframeworkonafullyconnectedgraph.
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by 1SeeSection3foraformaldefinitionofprojectionmatrices.
theauthor(s).
1
4202
nuJ
6
]GL.sc[
1v63340.6042:viXraOntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
Graphormer
Distance GNN Graphormer-GD
1-WL (GD-WL) GraphiT
⊐ GRIT
PEG
BasisNet
EPNN
⊒ (EPWL) SPE
⊐ ⊑∕
Weak Spectral 3-IGN
⊐ Siamese IGN Spectral IGN
Sprectral IGN ≡ Spectral PPGN
incomparable ⊐ ⊐ ⊐
incomparable
SWL ⊐PSWL ≡3-WL
⊐ ⊐
more expressive
Figure1.ExpressivehierarchyforallGNNarchitecturesstudiedinthispaper.Here,thesymbol“≡”meansthatthetwoGNNsbeing
comparedhavethesameexpressivepower;“⊐”meansthatthelatterGNNisstrictlymoreexpressivethantheformerone;“⊒”means
thatthelatterGNNiseitherstrictmoreexpressivethanorasexpressiveastheformerone;“̸⊑”meansthatthelatterGNNis(strictly)not
lessexpressivethantheformerone.Finally,“incomparable”meansthateitherGNNis(strictly)notmoreexpressivethantheother.The
dialogbubbleslistliteraturearchitecturesthatcanbeseenasinstantiationsofthecorrespondingGNNclass.
Ourfirsttheoreticalresultestablishesatightexpressiveness Finally,wediscussthepotentialofusingspectralfeaturesto
upperboundforEPNN,showingthatitisstrictlylessexpres- boosttheexpressivepowerofhigher-orderGNNs. Weshow
sivethananimportantclassofSubgraphGNNsproposed using the projection matrices alone does not provide any
inZhangetal.(2023a),calledPSWL.Thisobservationis additionalexpressivepoweradvantagewhencombinedwith
intriguingfortworeasons. First,itconnectsspectralinvari- highlyexpressiveGNNssuchasPPGNandk-IGN(Maron
antGNNsandGTswiththeseeminglyunrelatedresearch etal.,2019b;a). Nevertheless,weproposeapossiblesolu-
directionofSubgraphGNNs(Cottaetal.,2021;Bevilacqua tiontowardsfurtherexpressivenessgains: wehypothesize
etal.,2022;Frascaetal.,2022;Qianetal.,2022;Zhaoetal., thatstrongerexpressivitycouldbeachievedthroughhigher-
2022)—alineofresearchstudyingexpressiveGNNsfrom orderextensionsofgraphspectra,suchasprojectiontensors.
astructuralandpermutationsymmetryperspective. Second, Overall,ourtheoreticalresultscharacterizeanexpressive-
combined with recent results (Frasca et al., 2022; Zhang nesshierarchyacrossbasisinvariantGNNs,distance-based
etal.,2023a),itimpliesthatEPNNisstrictlyboundedby GNNs,GTswithspectralencoding,subgraph-basedGNNs,
3-WL. As an implication, bounding previously proposed and higher-order GNNs. The resulting hierarchy is illus-
spectralinvariantmethodsbyEPNNwouldreadilyindicate tratedinFigure1.
thattheyareallstrictlylessexpressivethan3-WL.
We then explore how EPNNs are related to GNNs/GTs 2.RelatedGNNModels
thatemployspectraldistancesaspositionalencoding(Ying
2.1.Spectrally-enhancedGNNs
etal.,2021;Mialonetal.,2021;Zhangetal.,2023b;Ma
etal.,2023b;Wangetal.,2022;Lietal.,2020). Weprove In recent years, a multitude of research has emerged to
thatunderthegeneralframeworkproposedinZhangetal. developspectrally-enhancedGNNs/GTs,integratinggraph
(2023b),allcommonlyusedspectraldistancesgiveriseto spectralinformationintoeitherGNNnodefeaturesorthe
modelswithanexpressivepowerboundedbyEPNNs. This subsequentmessage-passingprocess. Theseendeavorscan
highlightsaninherentexpressivenesslimitationofdistance- becategorizedintothefollowingthreegroups.
basedapproachesintheliterature. Moreover,ouranalysis
Laplacianeigenvectorsasabsolutepositionalencoding.
underscoresthecrucialroleofmessage-passinginenhanc-
Onewaytodesignspectrally-enhancedGNNsinvolvesen-
ingtheexpressivepowerofspectralfeatures.
codingLaplacianeigenvectors. Thisapproachtreatseach
Our next step aims to draw connections between EPNNs eigenvectorasa1-dimensionalnodefeatureandincorpo-
andtwoimportantspectralinvariantarchitecturesthatutilize ratesthetopkeigenvectorsasatypeofabsolutepositional
projectionmatrices,knownasBasisnet(Limetal.,2023) encoding, which can be used to enhance any message-
andSPE(Huangetal.,2024). Thisisachievedbyanovel passing GNNs and GTs (Dwivedi et al., 2020; Dwivedi
symmetryanalysisforeigenspaceprojections,whichyields & Bresson, 2021; Kreuzer et al., 2021; Rampasek et al.,
a theoretically-inspired architecture called Spectral IGN. 2022;Maskeyetal.,2022;Dwivedietal.,2022;Kimetal.,
Surprisingly,weprovethatSpectralIGNisasexpressiveas 2022). However, one main drawback of using Laplacian
EPNN.Ontheotherhand,SPEandBasisNetcanbeeasily eigenvectorsarisesfromtheambiguityproblem. Sucham-
upperboundedbyeitherSpectralIGNoritsweakervariant. biguitycreatessevereissuesregardingtraininginstability
2
more
expressiveOntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
and poor generalization (Wang et al., 2022). While this tancebetweennodes. Distancescanbeeasilyencodedin
problemcanbepartiallymitigatedthroughtechniqueslike GNNmodelsbyeitherservingasedgefeaturesinmessage-
randomlyflippingeigenvectorsignsoremployingacanon- passing aggregations (Wang et al., 2022; Velingker et al.,
izationmethod(Maetal.,2023a),itbecomesmuchmore 2023)orasrelativepositionalencodinginGraphTransform-
complicatedwheneigenvalueshavehighermultiplicities. ers(Yingetal.,2021;Zhangetal.,2023a;Maetal.,2023b).
Spectralinvariantarchitectures. Abetterapproachwould
2.2.ExpressiveGNNs
be to design GNNs that are invariant w.r.t. the choice of
eigenvectors. Forexample,SignNet(Limetal.,2023)trans- TheexpressivepowerofGNNshasbeenstudiedindepth
formseacheigenvectorvtoϕ(v)+ϕ(−v)forsomepermu- intherecentfewyears. Earlyworks(Xuetal.,2019;Mor-
tationequivariantfunctionϕ,whichguaranteestheinvari- risetal.,2019)havepointedoutafundamentallimitation
ancewhenalleigenvalueshaveamultiplicityof1. Incase ofGNNsbyestablishinganequivalencebetweenmessage-
ofhighermultiplicity,BasisNet(Limetal.,2023)achieves passingneuralnetworksandthe1-WLgraphisomorphism
spectralinvarianceforthefirsttimebyutilizingtheprojec- test (Weisfeiler & Lehman, 1968). To develop more ex-
tionmatrix. Specifically,givenaneigenvalueλwithmul- pressivemodels,severalstudiesleveragedhigh-dimensional
tiplicityk,theprojectionmatrixdefinedas(cid:80)k i=1v iv i⊤ is variantsoftheWLtest(Caietal.,1992;Grohe,2017). Rep-
invariantw.r.t. thechoiceof(unit)eigenvectorsv 1,··· ,v k resentative models include k-IGN (Maron et al., 2019b),
aslongastheyformanorthogonalbasisoftheeigenspace PPGN (Maron et al., 2019a), and k-GNN (Morris et al.,
associated with λ. Therefore, BasisNet simply feeds the 2019; 2020). However, these models suffer from severe
projection matrix into a permutation equivariant model computationalcostsandaregenerallynotsuitableinprac-
ρ : Rn×n → Rn (e.g., 2-IGN (Maron et al., 2019b)) to tice. Currently,onemainstreamapproachtodesigningsim-
generate spectral invariant node features ρ((cid:80)k v v⊤). ple,efficient,practical,andexpressivearchitecturesisthe
i=1 i i
Thenodefeaturesgeneratedfordifferenteigenspacesare SubgraphGNNs(Cottaetal.,2021;Bevilacquaetal.,2022;
concatenatedtogether. WhiletheauthorsprovedthatBasis- 2023;Youetal.,2021;Zhang&Li,2021;Zhaoetal.,2022;
Netcanuniversallyrepresentanygraphfunctionswhenρis Kongetal.,2023). Inparticular, theexpressivepowerof
universal(e.g.,usingn-IGN),theempiricalperformanceis SubgraphGNNsaswellastheirrelationtotheWLtestsare
generallyunsatisfactorywhenemployingapracticalmodel well-understoodinrecentstudies(Frascaetal.,2022;Qian
ρ(i.e.,2-IGN).Recently,Huangetal.(2024)furthergen- etal.,2022;Zhangetal.,2023a;2024). Theseresultswill
eralized BasisNet by proposing SPE, which performs a beusedtoanalyzespectrally-enhancedGNNsinthispaper.
soft aggregation across different eigenspaces rather than
a hard separation implemented in BasisNet. Specifically, 2.3.ExpressivepowerofspectralinvariantGNNs
letv ,··· ,v beanorthogonalbasisof(unit)eigenvectors
1 n
While spectrally-enhanced GNNs have been extensively
associatedwitheigenvaluesλ ,··· ,λ ,respectively;then,
1 n
studied in the literature, much less is known about their
each1-dimensionalnodefeaturegeneratedbySPEhasthe
form ρ((cid:80)n ψ (λ )v v⊤), where ψ : R → R is a pa- expressive power. Balcilar et al. (2021); Wang & Zhang
i=1 j i i i j (2022) delved into the expressive power of specific spec-
rameterizedfunctionassociatedwithfeaturedimensionj.
tralfilteringGNNs,buttheirexpressivepowerisinherently
TheauthorsdemonstratedthatSPEcanenhancethestability
limitedby1-WL.Anotherlineofworksstudiedtheexpres-
andgeneralizationofGNNs,yieldingmuchbetterempirical
sivepoweroftherawspectralinvariants(e.g., projection
performancecomparedwithBasisNet.
matrices)inrelationtotheWeisfeiler-Lehmanalgorithms
Spectraldistancesasinvariantrelativepositionalencod- (Fu¨rer,1995;2010;Rattan&Seppelt,2023).However,their
ing. IncontrasttoencodingLaplacianeigenvectors,anal- analysisdoesnotconsideranyaggregationorrefinement
ternativeapproachtoachievingspectralinvarianceinvolves procedures over spectral invariants, and thus, it does not
utilizing(spectral)distances. Previousstudieshaveidenti- provide explicit insights into the expressive power of the
fiedvariousdistances,spanningfromthebasicshortestpath correspondingGNNs. Limetal.(2023)proposedaconcrete
distance(Fengetal.,2022;Abboudetal.,2022)tomoread- spectral invariant GNN called BasisNet, but their expres-
vancedonessuchasPageRankdistance,resistancedistance, sivenessanalysisstilllargelyfocusesonraweigenvectors
anddistancesassociatedwithrandomwalksandgraphdif- and projection matrices. To our knowledge, none of the
fusion(Lietal.,2020;Zhangetal.,2023b;Mialonetal., priorworksaddressesthecrucialproblemofwhether/how
2021;Feldmanetal.,2023). Notably,allofthesedistances the design of GNN layers contributes to the model’s ex-
have a deep relation to the graph Laplacian while being pressiveness. In this paper, we will answer this question
moreinterpretablethaneigenvectorsandnotsufferingfrom by showing that (i) a suitable aggregation procedure can
ambiguityproblems. TheworkofPEG(Wangetal.,2022) strictlyimprovetheexpressivepowerbeyondrawspectral
designedaninvariantrelativepositionalencodingbasedon features,and(ii)differentaggregationschemescanleadto
Laplacianeigenvectors,whichcanalsobetreatedasadis- considerablevariationsinthemodels’expressiveness.
3OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
3.Preliminaries cus on the eigenspaces characterized by their projection
matrices. Concretely, there is a unique projection matrix
We use { } and {{ }} to denote sets and multisets, respec-
P foreacheigenvalueλ , whichcanbeobtainedviathe
i i
tively. Givena(multi)setS,itscardinalityisdenotedas|S|. (cid:80)
eigen-decompositionM = λ P ,wheremisthe
Inthispaper,weconsiderfinite,undirected,simplegraphs i∈[m] i i
numberofdifferenteigenvalues. Itfollowsthatthesepro-
with no isolated vertices. Let G = (V ,E ) be a graph
G G jection matrices are symmetric, idempotent (P2 = P ),
with vertex set V and edge set E , where each edge in i i
G G “orthogonal”(P P =Oforalli̸=j),andsumtoidentity
i j
E G isrepresentedasaset{u,v}⊂V G ofcardinalitytwo. ((cid:80) P =I). Thereisacloserelationbetweenprojec-
Theneighborsofavertexu∈V isdenotedasN (u)= i∈[m] i
G G tionmatrixP andanyorthogonalbasisofuniteigenvectors
i
{v ∈ V : {u,v} ∈ E },andthedegreeofuisdenoted
G G {z ,··· ,z }thatspanstheeigenspaceassociatedwith
asdeg G(u)=|N G(u)|. Givenvertexpair(u,v)∈V G2,de- λ :i, s1 pecificai l, lJ yi ,P =(cid:80)Ji z z⊤. Theprojectionmatri-
notebyatp (u,v)itsatomictype,whichencodeswhether i i j=1 i,j i,j
G cesnaturallydefineagraphinvariantPM overG :
u = v, {u,v} ∈ E , or u and v are not adjacent. Given 2
G
vertex tuple u ∈ V Gk, the rooted graph Gu is a graph ob- P GM(u,v):={{(λ 1,P 1(u,v)),··· ,(λ m,P m(u,v))}}.
tainedfromGbymarkingverticesu ,··· ,u sequentially.
1 k
WedenotebyG thesetofallgraphsandbyG thesetofall WecallPM theeigenspaceprojectioninvariant(associated
k
rootedgraphsmarkingkvertices. ItfollowsthatG =G. withgraphmatrixM).
0
Graphinvariant. Two(rooted)graphsGu,Hv ∈ G are
k
4.EigenspaceProjectionNetwork
calledisomorphic(denotedbyGu ≃Hv)ifthereisabijec-
tion f : V → V such that f(u ) = v for all i ∈ [k],
G H i i This section introduces a simple GNN design paradigm
and for all vertices w 1,w 2 ∈ V G, {w 1,w 2} ∈ E G iff basedontheinvariantPM definedabove,calledEigenspace
{f(w ),f(w )} ∈ E . A function f defined on graphs
1 2 H ProjectionGNN(EPNN).TheideaofEPNNisverysimple:
G k iscalledagraphinvariantifitisinvariantunderisomor- PM(u,v)essentiallyencodestherelationbetweenvertices
phism,i.e.,f(Gu)=f(Hv)ifGu ≃Hv.Inthecontextof G
u and v in graph G and can thus be treated as a form of
graphlearning,anyGNNthatoutputsagraphrepresentation
“edgefeature”. Inlightofthis,onecannaturallydefinea
shouldbeagraphinvariantoverG ;similarly,anyGNNthat
0 message-passingGNNthatupdatesthenoderepresentation
outputsarepresentationforeachnode/eachpairofnodes
ofeachvertexubyiterativelyaggregatingtherepresenta-
shouldbeagraphinvariantoverG /G ,respectively.
1 2 tionsofothernodesvalongwithedgefeaturesassociated
Graphvectorsandmatrices. Anyreal-valuedgraphin- with(u,v). Formally,consideraK-layerEPNNanddenote
variant x defined over G corresponds to a graph vector byh(l)(u)thenoderepresentationofu∈V computedby
1 G G
x :V →RwhenrestrictingonaspecificgraphG∈G. anEPNNafterthel-thlayer. Then,wecanwritetheupdate
G G
Withoutambiguity,wedenotetheelementsinx asx (u) ruleofeachEPNNlayerasfollows:
G G
foreachu ∈ V ,whichisequaltox(Gu). Similarly,any
real-valuedgrapG
hinvariantM definedoverG 2corresponds
h( Gl+1)(u)=g(l+1)(h( Gl)(u),
(1)
t Go ∈a g Gr ,a wph hem reat er li ex mM enG tM: V (G2 u,→ v)eR quw alh se tn oMres (t Gric ut vin ).g Fo on
r
{{(h( Gl)(v),P GM(u,v)):v ∈V G}}),
G
easeofreading,wewilldropthesubscriptGwhenthereis whereallnoderepresentationsh(0)(u)arethesameatini-
G
noambiguityofthegraphusedincontext. Onecangeneral- tialization. Here,g(l+1)canbeanyparameterizedfunction
izeallbasiclinearalgebrasfromclassicvectors/matricesto representingthe(l+1)-thlayer. Inpractice,itcanbeimple-
thosedefinedongraphs. Forexample,thematrixproductis mentedinvariouswayssuchasGIN-basedaggregation(Xu
(cid:80)
definedas(M 1M 2)(u,v)= w∈VGM 1(u,w)M 2(w,v). etal.,2019)orGraphTransformers(Yingetal.,2021),and
Severalbasicgraphmatricesincludetheadjacencymatrix wehighlightthatitisparticularlysuitedforGraphTrans-
A,degreematrixD,LaplacianmatrixL := D−A,and formersasthegraphbecomesfullyconnectedwiththisedge
normalizedLaplacianmatrixLˆ :=D−1/2LD−1/2. Note feature. Finally, after the K-th layer, a global pooling is
thatallthesematricesaresymmetric. performedoverallverticesinthegraphtoobtainthegraph
Graphspectraandprojection. LetM beanysymmetric
representationPOOL({{h( GK)(u):u∈V G}}).
graph matrix (e.g., A, L, or Lˆ). The graph spectrum is EPNNiswell-defined. First,thegraphrepresentationcom-
thesetofalleigenvaluesofM,whichisagraphinvariant
putedbyanEPNNispermutationinvariantw.r.t. vertices,
over G 0. In addition to eigenvalues, the spectral informa- asPM isagraphinvariantoverG 2. Second,EPNNdoes
tionofagraphalsoincludeseigenvectorsoreigenspaces, notsufferfromtheeigenvectorambiguityproblem,asPM
G
whichcontainmuchmorefine-grainedinformation. Unfor- is uniquely determined by graph G. Later, we will show
tunately,eigenvectorshaveinherentambiguityandcannot thatEPNNcanserveasasimpleyetunifiedframeworkfor
serve as a valid graph invariant over G 1. Instead, we fo- studyingtheexpressivepowerofspectralinvariantGNNs.
4OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
4.1.ExpressivepowerofEPNN rootedgraphs{{Gu : u ∈ V }}(knownasnodemarking),
G
independentlyruns1-WLforeachGu,andfinallymerges
Thecentralquestionwewouldliketostudyis: whatisthe
their graph representations. We call the above algorithm
expressive power of EPNN? To formally study this ques-
SWL,andtherefinementrulecanbeformallywrittenas
tion, this subsection introduces EPWL (Eigenspace Pro-
jection Weisfeiler-Lehman), an abstract color refinement χS,(l+1)(u,v)=hash(χS,(l)(u,v),
algorithmtailoredspecificallyforgraphisomorphismtest. G G (4)
ComparedwithEquation(1),inEPWLthenoderepresen- {{(χS G,(l)(u,w),atp G(v,w)):w ∈V G}}).
tationh(l+1)(u)isreplacedbyacolorχ(l+1)(u),andthe
aggregatG
ion function g(l+1) is replaced
bG
y a perfect hash
whereχ GS,(l)(u,v)istheSWLcolorofvertexvingraphGu
functionhash,aspresentedbelow:
afterliterations,andtheinitialcolorχS,(0)(u,v)=I[u=
G
v]distinguishesthemarkedvertexuinGu.Recently,Zhang
χ(l+1)(u)=hash(χ(l)(u),{{(χ(l)(v),PM(u,v)):v∈V }}). etal.(2023a);Frascaetal.(2022)significantlygeneralized
G G G G G
(2) SubgraphGNNsbyenablinginteractionsamongdifferent
Initially, all node colors χ(0)(u) are the same. For each subgraphsandbuiltacompletedesignspace. Amongthem,
G
iterationl,thecolormappingχ(l) inducesanequivalence Zhangetal.(2023a)proposedthePSWLalgorithm,which
G addsacross-graphaggregationtoSWLasshownbelow:
relationoververtexsetV ,andtherelationgetsrefinedwith
G
theincreaseofl. Therefore,withasufficientlylargenumber χPS,(l+1)(u,v)=hash(χPS,(l)(u,v),χPS,(l)(v,v),
ofiterationsl ≤ |V G|,therelationsgetstable. Thegraph G G G (5)
representation is then defined to be the multiset of stable {{(χP GS,(l)(u,w),atp G(v,w)):w ∈V G}}),
colors. EPWLdistinguishestwonon-isomorphicgraphsiff
thecomputedgraphrepresentationsaredifferent. Wehave
whereχPS,(l)(u,v)isthePSWLcolorof(u,v)∈V2after
G G
thefollowingresult(whichcanbeeasilyprovedfollowing literations. Wenowpresentourmainresult,whichreveals
standardtechniques,seee.g.,Zhangetal.(2023a)): afundamentalconnectionbetweenPSWLandEPWL:
Proposition4.1. TheexpressivepowerofEPNNisbounded Theorem4.3. ForanygraphmatrixM ∈{A,L,Lˆ},the
by EPWL in terms of graph isomorphism test. Moreover, expressivepowerofEPWLisstrictlyboundedbyPSWLin
withsufficientlayersandproperfunctionsg(l),EPNNcan distinguishingnon-isomorphicgraphs.
beasexpressiveasEPWL.
The proof of Theorem 4.3 is deferred to Appendix A.3,
Insubsequentanalysis,wewillboundtheexpressivepower whichisbasedontherecentgraphtheoryresultestablished
ofEPWLbybuildingrelationstothestandardWeisfeiler- byRattan&Seppelt(2023). Specifically,givenanygraph
Lehman hierarchy. First, it is easy to see that EPWL is Gandverticesu,v ∈V G,eachprojectionelementP i(u,v)
lowerboundedbytheclassic1-WLdefinedbelow: inPM isdeterminedbytheSWLstablecolorχS G(u,v)for
anysymmetric“equitable”matrixM definedinRattan&
χ˜(l+1)(u)=hash(χ˜(l)(u),{{(χ˜(l)(v),atp (u,v)):v∈V }}), Seppelt (2023), and the eigenvalues are also determined
G G G G G
(3) by the SWL graph representation. Notably, all matrices
whereχ˜(l)(u)isthe1-WLcolorofuingraphGafterlitera- studied in this paper (e.g., A,L,Lˆ) are equitable. Based
G
onthisresult,onemayguessthatEPWLcanbebounded
tions. ThisresultfollowsfromthefactthattheEPWLcolor
mappingχ(l+1) alwaysinducesafinerrelationthanthe1- by SWL. However, we show this is actually not the case
WLcolormappingχ˜(l+1),asatp (u,v)isfullyencoded whenfurthertakingthemessage-passingaggregationinto
G
inPM(u,v)(seeLemmaA.5). Besides,itiseasytogive account. Thekeytechnicalcontributioninourproofisto
G
relatetherefinementprocedureinEPWLtotheadditional
1-WLindistinguishablegraphsthatcanbedistinguishedvia
spectralinformation(seeFigure2). Puttingthesetogether,
cross-graphaggregationχPS,(l)(v,v)inPSWL.Tothisend,
G
wearriveatthefollowingconclusion: weshowthestablecolorχPS(u,u)isstrictlyfinerthanthe
G
Proposition4.2. ForanygraphmatrixM ∈ {A,L,Lˆ}, stablecolorχ G(u),thusconcludingtheproof.
thecorrespondingEPWLisstrictlymoreexpressivethan Remark4.4. BasedonTheorem4.3,onecanalsobound
1-WLindistinguishingnon-isomorphicgraphs. the expressiveness of EPWL by other popular GNNs in
literature, such as SSWL (Zhang et al., 2023a), Local 2-
Ontheotherhand,thequestionbecomesmoreintriguing GNN (Morris etal., 2020; Zhang et al.,2024), ReIGN(2)
whenstudyingtheupperboundofEPWL.Below,wewill (Frascaetal.,2022),ESAN(Bevilacquaetal.,2022),and
approachtheproblembybuildingfundamentalconnections GNN-AK(Zhaoetal.,2022),asallthesearchitecturesare
betweenEPWLandanimportantclassofexpressiveGNNs moreexpressivethanPSWL(Zhangetal.,2023a).However,
known as Subgraph GNNs. The basic form of Subgraph EPWLisincomparabletothevanillaSWL,wherewegive
GNNisverysimple: givenagraphG,ittreatsGasasetof counterexamplesinAppendixA.8.
5OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
The significance of Theorem 4.3 is twofold. First, it re- • PageRankdistance(PRD).Givenweightsequence
vealsasurprisingrelationbetweenGNNsaugmentedwith γ ,γ ,···,thePageRankdistancebetweenverticesu
0 1
spectral information and the ones grounded in structural andv isdefinedas(cid:80)∞ γ Wk(u,v),whereW =
k=0 k
message-passing,whichrepresentstwoseeminglyunrelated D−1A is the random walk probability matrix. It is
researchdirections. Ourresultthusoffersinsightsintohow ageneralizationofthep-steplandingprobabilitydis-
previouslyproposedexpressiveGNNscanencodespectral tance,whichcorrespondstosettingγ =1andγ =0
p k
information. Second,Theorem4.3pointsoutafundamental for all k ̸= p. Li et al. (2020) first proposed to use
limitationofEPNN.Indeed,combinedwiththeresultthat PRD-WLtoboosttheexpressivepowerofGNNs.
PSWLisstrictlyboundedby3-WL(Zhangetal.,2023a;
• Otherdistances. Wealsostudythe(normalized)dif-
2024),weobtaintheconcludingcorollary:
fusiondistance(Coifman&Lafon,2006)andthebi-
Corollary4.5. ForanygraphmatrixM ∈{A,L,Lˆ},the harmonicdistance(Lipmanetal.,2010). Duetospace
expressivepowerofEPWLisstrictlyboundedby3-WL. limit,pleaserefertoAppendixA.4formoredetails.
Ourmainresultisstatedasfollows:
5.DistanceGNNsandGraphTransformers
Theorem 5.1. For any distance listed above, the expres-
Inthissection,wewillshowthatEPNNunifiesalldistance- sivepowerofGD-WLisupperboundedbyEPWLwiththe
basedspectralinvariantGNNs. Here,weadopttheframe-
normalizedgraphLaplacianmatrixLˆ.
workproposedinZhangetal.(2023b),knownasGeneral-
The proof of Theorem 4.3 is highly technical and is de-
izedDistance(GD)GNNs. Theaggregationformulaofthe
ferred to Appendix A.4, with several important remarks
correspondingGD-WLcanbewrittenasfollows:
madeasfollows. ForthecaseofSPD,theproofisbased
χD,(l+1)(u)=hash(χD,(l)(u),
on the key finding that
PLˆ(u,v)
determines the shortest
G G (6) G
{{(χD,(l)(v),d (u,v)):v ∈V }}). pathdistancebetweenuandvforanygraphGandvertices
G G G u,v ∈ V . Unfortunately, this property does not trans-
G
whereχD,(l)(u)istheGD-WLcolorofvertexu∈V after fer to other distances listed above. For general distances,
G G
l iterations, and d can be any valid distance metric. By the reason why EPWL is still more expressive lies in the
choosingdifferentdistances,GD-WLincorporatesvarious entiremessage-passingprocess(orcolorrefinementproce-
priorworkslistedbelow. dure). Concretely,therefinementcontinuouslyenrichesthe
informationembeddedinnodecolorsχ(l)(v), sothatthe
• Shorest-pathdistance(SPD).Thisisthemostbasic G
tuple(χ(l)(u),χ(l)(v),PLˆ(u,v))eventuallyencompasses
distancemetricandhasbeenextensivelyusedinde- G G G
sufficient information to determine any distance d (u,v)
signingexpressiveGNNs(e.g., Lietal.,2020;Ying G
etal.,2021;Abboudetal.,2022;Fengetal.,2022). (althoughP GLˆ(u,v)alonemaynotdetermineit). Ourproof
thus emphasizes the critical role of message-passing ag-
• Resistancedistance(RD).Itisdefinedtobetheef-
gregation in enhancing the expressiveness of spectral in-
fectiveresistancebetweentwonodeswhentreatingthe
formation. Note that this is also justified in the proof of
graphasanelectricalnetworkwhereeachedgehasare-
Theorem4.3,wherethemessage-passingprocessbooststhe
sistanceof1Ω. Recently,Zhangetal.(2023b)showed
expressive power of EPWL beyond SWL. Moreover, we
that incorporating RD can significantly improve the
emphasizethatthesedistanceslistedabovecannotbewell-
expressive power of GNNs for biconnectivity prob-
encodedwhenusingweakermessage-passingaggregations,
lemssuchasidentifyingcutvertices/edges. Besides,
aswillbeelucidatedinSection6.2.
RDhasbeenextensivelystudiedinotherareasinthe
GNN community, such as alleviating oversquashing Implications. Theorem5.1hasaseriesofconsequences.
problems(Arnaiz-Rodr´ıguezetal.,2022). First,itimpliesthatallthepowerofdistanceinformation
is possessed by EPWL. As an example, we immediately
• Distancesbasedonrandomwalk. Thehitting-time
havethefollowingcorollarybasedontherelationbetween
distance(HTD)betweentwoverticesuandvisdefined
distanceandbiconnectivityofagraphestablishedinZhang
astheexpectednumberofstepsinarandomwalkstart-
et al. (2023b), which significantly extends the classic re-
ingfromuandreachingvforthefirsttime,whichisan
sult that Laplacian spectrum encodes graph connectivity
asymmetricdistance. Instead,thecommute-timedis-
(Brouwer&Haemers,2011).
tance(CTD)isdefinedastheexpectednumberofsteps
foraround-tripstartingatutoreachvandthenreturn Corollary5.2. EPWLisfullyexpressiveforencodinggraph
tou,whichisasymmetrizedversionofHTD.These biconnectivityproperties, suchasidentifyingcutvertices
distances are fundamental in graph theory and have andcutedges,determiningthenumberofbiconnectedcom-
been used to develop/understand expressive GNNs ponents, and distinguishing graphs with non-isomorphic
(Velingkeretal.,2023;Zhangetal.,2023a). blockcut-vertextreesandblockcut-edgetrees.
6OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
Remark 5.3. Corollary 5.2 offers a novel understanding (σ,τ)∈S ×S . Interestingly,thissettingpreciselycor-
m n
of the work by Zhang et al. (2023b) on why ESAN can respondstoaspecificcaseoftheDSSframeworkproposed
encodegraphbiconnectivity,therebythoroughlyunifying inMaronetal.(2020);Bevilacquaetal.(2022). Thisof-
their analysis. Essentially, this is just because ESAN is fersatheoreticallyinspiredapproachtodesigningpowerful
morepowerfulthanEPWL(Remark4.4),andEPWLitself invariantarchitectures,aswewilldetailbelow.
isalreadycapableofencodingbothSPDandRD.
Remark5.4. CombiningTheorems4.3and5.1resolvesan 6.1.SpectralIGN
openquestionposedbyZhangetal.(2023a), confirming
IntheDSSframework,theneuralnetworkf isformedby
thatPSWLcanencoderesistancedistance.
stackingaseriesofequivariantlinearlayersL(i)interleaved
with elementwise non-linear activation ϕ, culminating in
Asasecondimplication,combinedwithCorollary4.5,The-
thefinalpoolinglayer:
orem5.1highlightsafundamentallimitationofalldistance-
basedGNNsasstatedbelow: f =M ◦ϕ◦g◦ϕ◦L(K)◦ϕ◦···◦ϕ◦L(1), (8)
Corollary5.5. Foranydistancedefinedabove,theexpres- whereeachL(l) :Rm×n×n×dl−1 →Rm×n×n×dl isequiv-
sivepowerofGD-WLisstrictlyboundedby3-WL.
ariantw.r.t. S ×S ,i.e.,forall(σ,τ)∈S ×S ,
m n m n
Distances as positional encoding. Distances have also L(l)((σ,τ)·X)=(σ,τ)·L(l)(X) ∀X∈Rm×n×n×dl−1;
foundextensiveapplicationaspositionalencodinginGNNs
and GTs. For instance, Graphormer (Ying et al., 2021),
g :Rm×n×n×dK →RdK isaninvariantpoolinglayer(e.g.,
Graphormer-GD(Zhangetal.,2023b),andGraphiT(Mi-
averagepoolingormaxpooling),andM :RdK →RdK+1
isamulti-layerperceptron. Here,thekeyquestionliesin
alonetal.,2021)employvariousdistancesasrelativeposi-
designinglinearlayersL(l)equivarianttotheproductgroup
tionalencodinginTransformer’sattentionlayers. Similarly,
S ×S . Maronetal.(2020)theoreticallyshowedthatL(l)
GRIT(Maetal.,2023b)employsmulti-dimensionalPRD m n
canbedecomposedinthefollowingway:
andanovelattentionmechanismtofurtherboosttheper-
formanceofGTs. ThepositionalencodingdevisedinPEG  
(Wangetal.,2022)canalsobeviewedasafunctionofadis- [L(l)(X)]
i
=L˜( 1l)(X i)+L˜( 2l) (cid:88) X i, (9)
tance,withthearchitecturerepresentinganinstantiationof
i∈[m]
GD-WL.ThesearchitecturesareanalyzedinAppendixA.7,
wherewehavethefollowingconcludingcorollary: where L˜( 1l),L˜( 2l) : Rn×n×dl−1 → Rn×n×dl aretwolinear
functions equivariant to the graph symmetry modeled by
Corollary 5.6. The expressive power of Graphormer,
S . Thisdecompositionissignificantasthedesignspaceof
Graphormer-GD,GraphiT,GRIT,andPEGareallbounded n
equivariantlinearlayersforgraphshasbeenfullycharacter-
byEPWLandstrictlylessexpressivethan3-WL.
izedinMaronetal.(2019b),knownas2-IGN.Wethuscall
ourmodel(Equations(8)and(9))SpectralIGN.
6.SpectralInvariantGraphNetwork
Thecentralquestionwewouldliketostudyis: whatisthe
Togainanin-depthunderstandingoftheexpressivepowerof expressivepowerofSpectralIGN?Surprisingly,wehave
EPNN,inthissectionwewillswitchourattentiontoamore thefollowingmainresult:
principledperspectivebystudyinghowtomodelspectralin-
Theorem 6.1. The expressive power of Spectral IGN is
variantGNNsbasedonthesymmetryofprojectionmatrices.
bounded by EPWL. Moreover, with sufficient layers and
ConsideragraphGwithvertexsetV ={1,··· ,n}. We
G propernetworkparameters,SpectralIGNisasexpressive
cangroupallspectralinformationofGintoa4-dimensional
asEPWLindistinguishingnon-isomorphicgraphs.
tensorP ∈ Rm×n×n×2, whereP = λ encodesthe
i,u,v,1 i
i-theigenvalueandP =P (u,v)encodesthe(u,v)- The proof of Theorem 6.1 is given in Appendix A.5. It
i,u,v,2 i
thelementoftheprojectionmatrixP associatedwithλ . offersaninterestingandalternativeviewfortheoretically
i i
Fromthisrepresentation,onecaneasilyfigureoutthesym- understandingtheEPNNdesigningframeworkandjustify-
metrygroupassociatedwithP,whichistheproductgroup ingitsexpressiveness. Moreover,theconnectiontoSpectral
S ×S oftwopermutationgroupsS andS represent- IGNallowsustobridgeEPNNwithimportantarchitectural
m n m n
ingtwoindependentsymmetries—eigenspacesymmetry variantsaswewilldiscussinthenextsubsection.
andgraphsymmetry. Foranyelement(σ,τ)∈S m×S n,it Remark6.2. WeremarkthatthereisavariantofSpectral
actsonPinthefollowingway: IGN,wherethepoolinglayergisdecomposedintotwopool-
inglayersviag =g(2)◦ϕ◦g(1)withg(1) :Rm×n×n×dK →
[(σ,τ)·P] =P . (7)
i,u,v,j σ−1(i),τ−1(u),τ−1(v),j Rn×n×dK andg(2) :Rn×n×dK →RdK poolinglayersfor
We would like to design a GNN model f that is invari- symmetrygroupsS andS ,respectively. Thisvarianthas
m n
ant under S × S , i.e., f((σ,τ) · P) = f(P) for all thesameexpressivepower,andTheorem6.1stillholds.
m n
7OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
6.2.SiameseIGN Proposition 6.5 theoretically justifies the design of SPE.
Combinedwithpreviousresults,weconcludethatSPEis
Let us consider an interesting variant of Spectral IGN,
strictlymoreexpressivethanBasisNetwhenusingthesame
dubbedSiameseIGN(Maronetal.,2020),wherethenet-
2-IGNbackbone,whilebeingstrictlyboundedby3-WL.
workprocesseseacheigenspaceP (i∈[m])independently
i
usinga“Siamese”2-IGNwithoutaggregatingoverdifferent Delvingmoreintothegap.Weremarkthatthegapbetween
eigenspaces. ThiscorrespondstoreplacingEquation(9)by Siamese IGN and Spectral IGN is not just theoretical; it
[L(l)(X)]
i
= L˜(l)(X i)withL˜(l) : Rn×n×dl−1 → Rn×n×dl alsorevealssignificantlimitationsofthesiamesedesignin
a2-IGNlayer. SiameseIGNisinterestingbecauseitisa practicalaspects.Specifically,weidentifythatbothSiamese
special type of Spectral IGN that is invariant to a strictly IGNandWeakSpectralIGNcannotfullyencodeanygraph
larger group formed by the wreath product S ≀S (see distancelistedinSection5(eventhebasicSPD),asstated
m n
Maron et al., 2020; Wang et al., 2020). Moreover, it is inthefollowingtheorem:
closelyrelatedtoapriorarchitecturecalledBasisNet(Lim
Theorem6.6. ForanydistancelistedinSection5, there
etal.,2023,seeAppendixA.6foradetaileddescription),
existtwonon-isomorphicgraphswhichGD-WLcandistin-
asBasisNetalsoprocesseseacheigenspaceindependently guishbutWeakSpectralIGN(appliedtomatrixLˆ)cannot.
withoutinteraction. Toelaborateonthisconnection,con-
sideravariantofSiameseIGN,dubbedWeakSpectralIGN, Ontheotherhand,wehaveprovedthatEPNNappliedtoma-
whichisobtainedfromSiameseIGNbydecomposingthe trixLˆ ismorepowerfulthanGD-WL.Thiscontrastreveals
pooling layer according to Remark 6.2. Note that unlike thecrucialroleofallowinginteractionbetweeneigenspaces
SiameseIGN,WeakSpectralIGNisnolongerinvariantto forenhancingmodel’sexpressiveness.
groupS ≀S . Wehavethefollowingresult:
m n
Proposition6.3. LetthetopgraphencoderusedinBasisNet 6.3.Extendingtohigher-orderspectralinvariantGNNs
beany1-layermessage-passingGNN.Then,theexpressive
TheDSSframeworkpresentedinSection6.1isquitegen-
powerofBasisNetisboundedbyWeakSpectralIGN.
eral. Inprinciple,anyS -equivariantgraphlayerE canbe
n
usedtobuildaGNNmodelf invarianttoS ×S .Thiscan
Ontheotherhand,amorefundamentalquestionliesinthe m n
relation between Siamese IGN, Weak Spectral IGN, and
beachievedbymakingL˜( 1l),L˜( 2l)inEquation(9)twoinstan-
Spectral IGN. Our main result states that there are strict tiationsofE. Inthissubsection,wewillstudyhigher-order
expressivitygapsbetweenthem: spectralinvariantGNNswheretheusedgraphencodersare
beyond2-IGN.Weconsidertwostandardsettingsforchoos-
Theorem6.4. SiameseIGNisstrictlylessexpressivethan
inghighlyexpressivegraphencoders: thek-IGNandthe
WeakSpectralIGN.Moreover,WeakSpectralIGNisstrictly
k-orderFolkloreGNN(Maronetal.,2019b;a;Azizianetal.,
lessexpressivethanSpectralIGN.
2021). We call the resulting models Spectral k-IGN and
Spectralk-FGNN,respectively. Unfortunately,ourresults
DiscussionswithBasisNet. (i)Combinedwithprevious
arenegativeforallofthesehigher-orderspectralinvariant
results,onecanprovethatEPNNisstrictlymoreexpressive
GNNs:
thanBasisNet2. Thisresultisparticularlystriking,asEPNN
only stores node representations while BasisNet stores a Proposition6.7. Forallk >2,Spectralk-IGNisasexpres-
representationforeach3-tuple(i,u,v)∈[m]×V ×V , siveask-WL.Similarly,forallk ≥2,Spectralk-FGNNis
G G
whosememorycomplexityscaleslikeO(|V |3). (ii)Com- asexpressiveask-FWL.
G
binedwithCorollary4.5,weconcludethattheexpressive
powerofBasisNetisalsostrictlyboundedby3-WL. We give a proof in Appendix A.7. Combined with the
results that k-IGN is already as expressive as k-WL and
DiscussionswithSPE.WenextturntotheSPEarchitecture
k-FGNNisalreadyasexpressiveask-FWL(Maronetal.,
(Huang et al., 2024). Surprisingly, while SPE was origi-
2019a;Azizianetal.,2021;Geerts&Reutter,2022), we
nallydesignedtoimprovethestabilityandgeneralization
concludethatcommonly-usedspectralinformationdoesnot
ofspectralinvariantGNNs(seeSection2.1),wefoundthe
helpwhencombinedwithhighlypowerfulGNNdesigns.
softaggregationacrossdifferenteigenspacessimultaneously
enhancesthenetwork’sexpressivepower. Indeed,wehave: Discussionsonhigher-orderspectralfeatures. Theabove
negativeresultfurtherinspiresustothinkaboutthefollow-
Proposition6.5. When2-IGNisusedtogeneratenodefea-
ingquestion:isitstillpossibletousespectralinformationto
turesinSPEandthetopgraphencoderisamessage-passing
enhancetheexpressivepowerofhigher-orderGNNs? Here,
GNN,theexpressivepowerofthewholeSPEarchitectureis
we offer some possible directions towards this goal. The
asexpressiveasSpectralIGN.
cruxhereistousehigher-order spectralfeatures. Specif-
2Thisholdsforanymessage-passing-basedtopgraphencoder ically, allthespectralinformationconsideredinprevious
witharbitrarylayersbyfollowingtheconstructioninTheorem6.4. sections (e.g., distance or projection matrices) is at most
8OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
2-dimensional. Canwegeneralizethesespectralfeatures
Table1.EmpiricalperformanceofdifferentGNNsonBREC.
intomulti-dimensionaltensors? Thisisindeedpossible: for
Model WLclass Basic Reg Ext CFI Total
example, a simple approach is to use symmetric powers
Graphormer SPD-WL 26.7 10.0 41.0 10.0 19.8
of a graph (also called the token graph), which has been NGNN SWL 98.3 34.3 59.0 0 41.5
widelystudiedinliterature(Audenaertetal.,2007;Alzaga ESAN GSWL 96.7 34.3 100.0 15.0 55.2
etal.,2010;Barghi&Ponomarenko,2009;Fabila-Monroy PPGN 3-WL 100.0 35.7 100.0 23.0 58.2
EPNN EPWL 100.0 35.7 100.0 5.0 53.8
et al., 2012). The k-th symmetric power of graph G, de-
notedbyG{k},isthegraphformedbyvertexsetV :=
G{k}
{S ⊂ V : |S| = k} and edge set E := {{S ,S } : TheresultsarepresentedinTable1. Fromtheseresults,one
G G{k} 1 2
S ,S ∈ V ,S △S ∈ E }. Here, each element in canseethattheempiricalperformanceofEPNNmatchesits
1 2 G{k} 1 2 G
V isamultisetofcardinalityk,andtwomultisetsare theoreticalexpressivityinourestablishedhierarchy. Con-
G{k}
connectediftheirsymmetricdifferenceisanedge. Inthis cretely,EPNNperformsmuchbetterthanGraphormer(SPD-
way,onecaneasilydefinehigher-orderspectralinformation WL) and NGNN (SWL), while underperforming ESAN
(λ ,P )∈R×Rn2k basedonPM ={(λ ,P )}m (the (GS-WL)andPPGN(3-WL).
i i G{k} i i i=1
eigenspaceprojectioninvariantassociatedwiththek-thto-
kengraph), e.g., bysettingP (u ,··· ,u ,v ,··· ,v ) = 8.Conclusion
i 1 k 1 k
P ({{u ,··· ,u }},{{v ,··· ,v }}). Thehigher-orderspec-
i 1 k 1 k
tral information can then serve as initial features of any This paper investigates the expressive power of spectral
higher-orderGNNthatcomputesrepresentationsforeach invariantGNNsandrelatedmodels. Itestablishesanexpres-
vertextuple,suchas2k-IGN. sivenesshierarchybetweencurrentmodelsusingaunifying
framework we propose. We also draw a surprising con-
Severalworkshavepointedoutthestrongexpressivepower
nection to a recently proposed class of highly expressive
of higher-order spectral features. Audenaert et al. (2007)
GNNs,demonstratingthatspecificinstancesfromthisclass
verifiedthatthespectraofthe3rdsymmetricpowerareal-
(e.g.,PSWL)upperboundallspectralinvariantGNNs. This
readynotlessexpressivethan3-WL.Moreover,Alzagaetal.
impliesspectralinvariantGNNsarestrictlylessexpressive
(2010);Barghi&Ponomarenko(2009)upperboundsthe
thanthe3-WLtest. Furthermore,weshowspectralprojec-
expressivepowerofthespectraofthek-thsymmetricpower
tionfeaturesandspectraldistancesdonotprovideadditional
by 2k-FWL. These results imply that using higher-order
expressivitybenefitswhencombinedwithmorepowerful
projectiontensorsisapromisingapproachtofurtherboost-
high-orderarchitectures. Wegiveagraphicalillustrationof
ingtheexpressivepowerofhigher-orderGNNs. Weleave
alloftheseresultsinFigure1.
thecorrespondingarchitecturaldesignandexpressiveness
analysisasanopendirectionforfuturestudy. Open questions. There are still several promising direc-
tions that are not fully explored in this paper. First, an
interestingquestionliesinhowthechoiceofgraphmatrix
7.Experiments
M affectstheexpressivepowerofspectralinvariantGNNs.
Wesuspectthatusing(normalized)Laplacianmatrixcan
Inthissection,weempiricallyevaluatetheexpressivepower
bemorebeneficialthanusingtheadjacencymatrix,andthe
ofvariousGNNarchitecturesstudiedinthispaper.Weadopt
formerisstrictlymoreexpressive. Wehavegivenimplicit
theBRECbenchmark(Wang&Zhang,2023),acomprehen-
evidenceshowingthatEPNNwithmatrixLˆ canencodeall
sivedatasetforcomparingtheexpressivepowerofGNNs.
distancesstudiedinthispaper; however, wewereunable
WefocusonthefollowingGNNsthatarecloselyrelatedto
thispaper: (i)Graphormer(Yingetal.,2021)(adistance- to demonstrate the same result for other graph matrices.
based GNN that uses SPD, see Section 5); (ii) NGNN Besides,anotherimportantopenquestionistheexpressive
power of higher-order spectral features, such as the one
(Zhang&Li,2021)(avariantofsubgraphGNN,seeSec-
tion4.1);(ii)ESAN(Bevilacquaetal.,2022)(anadvanced obtainedbyusingtokengraphs. Itisstillunknownabout
atightlower/upperboundoftheirexpressivepowerinre-
subgraphGNNthataddscross-graphaggregations,seeSec-
tion4.1);(iv)PPGN(Maronetal.,2019a)(ahigher-order lation to higher-order WL tests. Moreover, investigating
GNN,seeSection6.3);(v)EPNN(thispaper). Wefollow
therefinementsoverhigher-orderspectralfeaturesandthe
correspondingGNNscouldbeafantasticopendirection.
thesamesetupasinWang&Zhang(2023)inbothtraining
andevaluation. ForallbaselineGNNs,thereportednum-
bersaredirectlyborrowedfromWang&Zhang(2023);For ImpactStatement
EPNN,werunthemodel10timeswithdifferentseedsand
reporttheaverageperformance3. Thispaperpresentsworkwhosegoalistoadvancethefield
of Machine Learning. There are many potential societal
3Ourcodecanbefoundinthefollowinggithubrepo: consequencesofourwork,noneofwhichwefeelmustbe
https://github.com/LingxiaoShawn/EPNN-Experiments
specificallyhighlightedhere.
9OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
Acknowledgements Cotta, L., Morris, C., andRibeiro, B. Reconstructionfor
powerfulgraphrepresentations. AdvancesinNeuralIn-
HMistheRobertJ.ShillmanFellowandissupportedby
formationProcessingSystems,34:1713–1726,2021.
theIsraelScienceFoundationthroughapersonalgrant(ISF
264/23)andanequipmentgrant(ISF532/23). BZwould Dwivedi,V.P.andBresson,X. Ageneralizationoftrans-
liketothankJingchuGaiforhelpfuldiscussions. former networks to graphs. AAAI Workshop on Deep
LearningonGraphs: MethodsandApplications,2021.
References
Dwivedi,V.P.,Joshi,C.K.,Luu,A.T.,Laurent,T.,Bengio,
Abboud,R.,Dimitrov,R.,andCeylan,I.I. Shortestpath Y.,andBresson,X.Benchmarkinggraphneuralnetworks.
networks for graph property prediction. In The First arXivpreprintarXiv:2003.00982,2020.
LearningonGraphsConference,2022.
Dwivedi,V.P.,Luu,A.T.,Laurent,T.,Bengio,Y.,andBres-
Alzaga,A.,Iglesias,R.,andPignol,R.Spectraofsymmetric son,X. Graphneuralnetworkswithlearnablestructural
powersofgraphsandtheweisfeiler–lehmanrefinements. andpositionalrepresentations. InInternationalConfer-
JournalofCombinatorialTheory,SeriesB,100(6):671– enceonLearningRepresentations,2022.
682,2010.
Fabila-Monroy,R.,Flores-Pen˜aloza,D.,Huemer,C.,Hur-
Arnaiz-Rodr´ıguez,A.,Begga,A.,Escolano,F.,andOliver,
tado, F., Urrutia, J., and Wood, D. R. Token graphs.
N.M. Diffwire: Inductivegraphrewiringviathelova´sz
GraphsandCombinatorics,2012.
bound. In The First Learning on Graphs Conference,
2022. Feldman,O.,Boyarski,A.,Feldman,S.,Kogan,D.,Mendel-
son,A.,andBaskin,C. Weisfeilerandlemangoinfinite:
Audenaert,K.,Godsil,C.,Royle,G.,andRudolph,T. Sym-
Spectralandcombinatorialpre-colorings. Transactions
metricsquaresofgraphs. JournalofCombinatorialThe-
onMachineLearningResearch,2023. ISSN2835-8856.
ory,SeriesB,97(1):74–90,2007. ISSN0095-8956.
Feng,J.,Chen,Y.,Li,F.,Sarkar,A.,andZhang,M. How
Azizian,W.etal. Expressivepowerofinvariantandequiv-
powerful are k-hop message passing graph neural net-
ariantgraphneuralnetworks.InInternationalConference
works. In Advances in Neural Information Processing
onLearningRepresentations,2021.
Systems,volume35,pp.4776–4790,2022.
Balcilar,M.,Renton,G.,He´roux,P.,Gau¨ze`re,B.,Adam,S.,
andHoneine,P. Analyzingtheexpressivepowerofgraph Frasca,F.,Bevilacqua,B.,Bronstein,M.M.,andMaron,H.
neuralnetworksinaspectralperspective.InInternational Understandingandextendingsubgraphgnnsbyrethink-
ConferenceonLearningRepresentations,2021. ingtheirsymmetries. InAdvancesinNeuralInformation
ProcessingSystems,2022.
Barghi,A.R.andPonomarenko,I. Non-isomorphicgraphs
withcospectralsymmetricpowers. theelectronicjournal Fu¨rer, M. Graph isomorphism testing without numerics
ofcombinatorics,pp.R120–R120,2009. for graphs of bounded eigenvalue multiplicity. In Pro-
ceedingsofthesixthannualACM-SIAMsymposiumon
Bevilacqua, B., Frasca, F., Lim, D., Srinivasan, B., Cai,
Discretealgorithms,pp.624–631,1995.
C.,Balamurugan,G.,Bronstein,M.M.,andMaron,H.
Equivariantsubgraphaggregationnetworks. InInterna- Fu¨rer,M. Weisfeiler-lehmanrefinementrequiresatleasta
tionalConferenceonLearningRepresentations,2022. linearnumberofiterations. InInternationalColloquium
on Automata, Languages, and Programming, pp. 322–
Bevilacqua,B.,Eliasof,M.,Meirom,E.,Ribeiro,B.,and
333.Springer,2001.
Maron, H. Efficient subgraph gnns by learning effec-
tiveselectionpolicies. arXivpreprintarXiv:2310.20082,
Fu¨rer,M. Onthepowerofcombinatorialandspectralin-
2023.
variants. Linear algebra and its applications, 432(9):
2373–2380,2010.
Brouwer, A. E. and Haemers, W. H. Spectra of graphs.
SpringerScience&BusinessMedia,2011.
Geerts,F.andReutter,J.L. Expressivenessandapproxima-
Cai,J.-Y.,Fu¨rer,M.,andImmerman,N. Anoptimallower tionpropertiesofgraphneuralnetworks. InInternational
boundonthenumberofvariablesforgraphidentification. ConferenceonLearningRepresentations,2022.
Combinatorica,12(4):389–410,1992.
Grohe, M. Descriptivecomplexity, canonisation, andde-
Coifman,R.R.andLafon,S. Diffusionmaps. Appliedand finablegraphstructuretheory,volume47. Cambridge
computationalharmonicanalysis,21(1):5–30,2006. UniversityPress,2017.
10OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
Huang, Y., Lu, W., Robinson, J., Yang, Y., Zhang, M., Maron, H., Litany, O., Chechik, G., and Fetaya, E. On
Jegelka, S., and Li, P. On the stability of expressive learning sets of symmetric elements. In International
positionalencodingsforgraphneuralnetworks. InInter- conferenceonmachinelearning,pp.6734–6744.PMLR,
nationalConferenceonLearningRepresentations,2024. 2020.
Kim,J.,Nguyen,D.,Min,S.,Cho,S.,Lee,M.,Lee,H.,and Maskey, S., Parviz, A., Thiessen, M., Sta¨rk, H., Sadikaj,
Hong,S. Puretransformersarepowerfulgraphlearners. Y.,andMaron,H. Generalizedlaplacianpositionalen-
InAdvancesinNeuralInformationProcessingSystems, coding for graph representation learning. In NeurIPS
volume35,pp.14582–14595,2022. 2022 Workshop on Symmetry and Geometry in Neural
Representations,2022.
Klein,D.J.andRandic,M. Resistancedistance. Journalof
MathematicalChemistry,12:81–95,1993. Mialon,G.,Chen,D.,Selosse,M.,andMairal,J. Graphit:
Encodinggraphstructureintransformers. arXivpreprint
Kong,L.,Feng,J.,Liu,H.,Tao,D.,Chen,Y.,andZhang,M. arXiv:2106.05667,2021.
Mag-gnn: Reinforcementlearningboostedgraphneural
Morris,C.,Ritzert,M.,Fey,M.,Hamilton,W.L.,Lenssen,
network. arXivpreprintarXiv:2310.19142,2023.
J.E., Rattan, G., andGrohe, M. Weisfeilerandleman
Kreuzer,D.,Beaini,D.,Hamilton,W.,Le´tourneau,V.,and goneural: Higher-ordergraphneuralnetworks. InPro-
Tossou,P. Rethinkinggraphtransformerswithspectral ceedingsoftheAAAIconferenceonartificialintelligence,
attention. Advances in Neural Information Processing volume33,pp.4602–4609,2019.
Systems,34:21618–21629,2021.
Morris,C.,Rattan,G.,andMutzel,P. Weisfeilerandleman
Li,P.,Wang,Y.,Wang,H.,andLeskovec,J.Distanceencod- gosparse: Towardsscalablehigher-ordergraphembed-
ing: designprovablymorepowerfulneuralnetworksfor dings. In Advances in Neural Information Processing
graphrepresentationlearning. InProceedingsofthe34th Systems,volume33,pp.21824–21840,2020.
InternationalConferenceonNeuralInformationProcess-
Morris,C.,Lipman,Y.,Maron,H.,Rieck,B.,Kriege,N.M.,
ingSystems,pp.4465–4478,2020.
Grohe,M.,Fey,M.,andBorgwardt,K. Weisfeilerand
Lim,D.,Robinson,J.D.,Zhao,L.,Smidt,T.,Sra,S.,Maron, leman go machine learning: The story so far. arXiv
H.,andJegelka,S. Signandbasisinvariantnetworksfor preprintarXiv:2112.09992,2021.
spectralgraphrepresentationlearning. InTheEleventh
Qian,C.,Rattan,G.,Geerts,F.,Niepert,M.,andMorris,C.
InternationalConferenceonLearningRepresentations,
Orderedsubgraphaggregationnetworks. InAdvancesin
2023.
NeuralInformationProcessingSystems,volume35,pp.
21030–21045,2022.
Lipman,Y.,Rustamov,R.M.,andFunkhouser,T.A. Bihar-
monicdistance. ACMTransactionsonGraphics(TOG),
Rampasek,L.,Galkin,M.,Dwivedi,V.P.,Luu,A.T.,Wolf,
29(3):1–11,2010.
G.,andBeaini,D. Recipeforageneral,powerful,scal-
ablegraphtransformer. InAdvancesinNeuralInforma-
Ma,G.,Wang,Y.,andWang,Y. Laplaciancanonization:
tionProcessingSystems,2022.
Aminimalistapproachtosignandbasisinvariantspec-
tralembedding. InThirty-seventhConferenceonNeural
Rattan, G. and Seppelt, T. Weisfeiler-leman and graph
InformationProcessingSystems,2023a.
spectra. InProceedingsofthe2023AnnualACM-SIAM
SymposiumonDiscreteAlgorithms(SODA),pp.2268–
Ma, L., Lin, C., Lim, D., Romero-Soriano, A., Dokania,
2285.SIAM,2023.
P.K.,Coates,M.,Torr,P.,andLim,S.-N. Graphinduc-
tivebiasesintransformerswithoutmessagepassing. In Velingker,A.,Sinop,A.K.,Ktena,I.,Velicˇkovic´,P.,and
Proceedingsofthe40thInternationalConferenceonMa- Gollapudi, S. Affinity-aware graph networks. In Ad-
chineLearning,volume202,pp.23321–23337.PMLR, vancesinNeuralInformationProcessingSystems, vol-
2023b. ume36,2023.
Maron,H.,Ben-Hamu,H.,Serviansky,H.,andLipman,Y. Wang,H.,Yin,H.,Zhang,M.,andLi,P. Equivariantand
Provablypowerfulgraphnetworks.InAdvancesinneural stablepositionalencodingformorepowerfulgraphneu-
informationprocessingsystems,volume32,2019a. ralnetworks. InInternationalConferenceonLearning
Representations,2022.
Maron, H., Ben-Hamu, H., Shamir, N., and Lipman, Y.
Invariantandequivariantgraphnetworks.InInternational Wang,R.,Albooyeh,M.,andRavanbakhsh,S. Equivariant
ConferenceonLearningRepresentations,2019b. mapsforhierarchicalstructures. InProceedingsofthe
11OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
34th International Conference on Neural Information
ProcessingSystems,pp.13806–13817,2020.
Wang,X.andZhang,M. Howpowerfularespectralgraph
neuralnetworks.InInternationalConferenceonMachine
Learning,pp.23341–23362.PMLR,2022.
Wang, Y. and Zhang, M. Towards better evaluation of
gnn expressiveness with brec dataset. arXiv preprint
arXiv:2304.07702,2023.
Wei, Y., Li, R.-h., and Yang, W. Biharmonic distance of
graphs. arXivpreprintarXiv:2110.02656,2021.
Weisfeiler, B. and Lehman, A. The reduction of a graph
tocanonicalformandthealgebrawhichappearstherein.
NTI,Series,2(9):12–16,1968.
Xu,K.,Hu,W.,Leskovec,J.,andJegelka,S. Howpowerful
aregraphneuralnetworks? InInternationalConference
onLearningRepresentations,2019.
Ying,C.,Cai,T.,Luo,S.,Zheng,S.,Ke,G.,He,D.,Shen,Y.,
andLiu,T.-Y. Dotransformersreallyperformbadlyfor
graphrepresentation? AdvancesinNeuralInformation
ProcessingSystems,34,2021.
You,J.,Gomes-Selman,J.M.,Ying,R.,andLeskovec,J.
Identity-awaregraphneuralnetworks. InProceedingsof
theAAAIconferenceonartificialintelligence,volume35,
pp.10737–10745,2021.
Zhang, B., Feng, G., Du, Y., He, D., and Wang, L. A
complete expressiveness hierarchy for subgraph gnns
via subgraph weisfeiler-lehman tests. arXiv preprint
arXiv:2302.07090,2023a.
Zhang, B., Luo, S., Wang, L., and He, D. Rethinking
the expressive power of gnns via graph biconnectivity.
InTheEleventhInternationalConferenceonLearning
Representations,2023b.
Zhang, B., Gai, J., Du, Y., Ye, Q., He, D., and Wang, L.
Beyond weisfeiler-lehman: A quantitative framework
for GNN expressiveness. In The Twelfth International
ConferenceonLearningRepresentations,2024.
Zhang, M.andLi, P. Nestedgraphneuralnetworks. Ad-
vances in Neural Information Processing Systems, 34:
15734–15747,2021.
Zhao,L.,Jin,W.,Akoglu,L.,andShah,N. Fromstarsto
subgraphs: Upliftinganygnnwithlocalstructureaware-
ness. InInternationalConferenceonLearningRepresen-
tations,2022.
12OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
A.Proofs
Thissectionpresentsallthemissingproofsinthispaper. First,AppendixA.1definesbasicconceptsandintroducesour
prooftechniquethatwillbefrequentlyusedinsubsequentanalysis. Then,AppendixA.2givesseveralbasicresultsand
provesProposition4.2. Theformalproofsofourmaintheorems,includingTheorems4.3,5.1and6.1andProposition6.3,
arepresentedinAppendicesA.3toA.6,respectively. Discussionwithotherarchitectures,suchasGRIT,PEG,Spectral
PPGN,andSpectralk-IGNarepresentedinAppendixA.7. Finally,AppendixA.8revealsthegapsbetweeneachpairof
architecturesinourpaper,leadingtotheproofsofTheorems6.4and6.6.
A.1.Preliminary
Wefirstintroducesomebasicterminologiesandconceptsforgeneralcolorrefinementalgorithms.
Colormapping. AnygraphinvariantoverG iscalledak-dimensionalcolormapping. WeuseM todenotethefamily
k k
of all k-dimensional color mappings. For a color mapping χ ∈ M , our interest lies not in the specific values of the
k
function(sayχ (u)forsomeu ∈ Vk), butintheequivalencerelationsamongdifferentvalues. Formally, eachcolor
G G
mappingχ∈M definesanequivalencerelation∼χ betweenrootedgraphsGu,Hv markingkvertices,whereGu ∼χ Hv
k
iffχ (u)=χ (v). ForanygraphG∈G,theequivalencerelation∼χ inducesapartitionQ (χ)overthesetVk.
G H G G
Giventwocolormappingsχ ,χ ∈M ,wesayχ isequivalenttoχ ,denotedasχ ≡χ ,ifGu χ ∼1 Hv ⇐⇒ Gu χ ∼2 Hv
1 2 k 1 2 1 2
forallgraphsG,H ∈G andverticesu∈Vk,v ∈Vk. Onecanseethat“≡”formsanequivalencerelationoverM . We
G H k
sayχ isfiner thanχ ,denotedasχ ⪯ χ ,ifGu χ ∼1 Hv =⇒ Gu χ ∼2 Hv forallgraphsG,H andverticesu ∈ Vk,
1 2 1 2 G
v ∈Vk. Onecanseethat“⪯”formsapartialrelationonM . Wesayχ isstrictlyfinerthanχ ,denotedasχ ≺χ ,if
H k 1 2 1 2
χ ⪯χ andχ ̸≡χ .
1 2 1 2
Colorrefinement.AfunctionT :M →M thatmapsfromonecolormappingtoanotheriscalledacolortransformation.
k k′
Throughoutthispaper,weassumethatallcolortransformationsareorder-preserving,i.e.,forallχ ,χ ∈M ,T(χ )⪯
1 2 k 1
T(χ ) if χ ⪯ χ . An order-preserving color transformation T : M → M is further called a color refinement if
2 1 2 k k
T(χ) ⪯ χforallχ ∈ M . ForanycolorrefinementT,wedenotebyTt thet-thfunctionpowerofT,i.e.,thefunction
k
compositionT ◦···◦T withtoccurrencesofT. NotethatifT isacolorrefinement,soisTtforallt≥0.
Given two color transformations T ,T : M → M , we say T is as expressive as T , denoted by T ≡ T , if
1 2 k k′ 1 2 1 2
T (χ) ≡ T (χ)forallχ ∈ M . WesayT ismoreexpressivethanT ,denotedbyT ⪯ T ,ifT (χ) ⪯ T (χ)forall
1 2 k 1 2 1 2 1 2
χ ∈ M . WesayT isstrictlymoreexpressivethanT ,denotedbyT ≺ T ,ifT ismoreexpressivethanT andnot
k 1 2 1 2 1 2
asexpressiveasT . Aswillbeclearinoursubsequentproofs, theexpressivepowerofGNNsiscomparedthroughan
2
examinationoftheircolortransformations.
ForanycolorrefinementT : M → M ,wedefinethecorrespondingstablerefinementT∞ : M → M asfollows.
k k k k
For any χ ∈ M , define the color mapping T∞(χ) such that Gu T∞ ∼(χ) Hv iff Gu Tt ∼(χ) Hv where t ≥ 0 is the
k
minimumintegersatisfyingQ (Tt(χ))=Q (Tt+1(χ))andQ (Tt(χ))=Q (Tt+1(χ)). NotethatT∞iswell-defined
G G H H
since t always exists (one can see that Q (Tt(χ)) = Q (Tt+1(χ)) and Q (Tt(χ)) = Q (Tt+1(χ)) holds for all
G G H H
t ≥ max(|V |k,|V |k)whenT isacolorrefinement),anditiseasytoseethatT∞ isacolorrefinement. WecallT∞
G H
stablebecause(T ◦T∞)(χ)≡T∞(χ)holdsforallχ∈M ,i.e.,T ◦T∞ ≡T∞.
k
A color refinement algorithm A is formed by the composition of a stable refinement T∞ : M → M with a color
k k
transformationU :M →M ,calledthepoolingtransformation. ItcanbeformallywrittenasA:=U ◦T∞. Below,we
k 0
willderiveseveralusefulpropertiesforgeneralcolorrefinementalgorithms. Thesepropertieswillbefrequentlyusedto
comparetheexpressivepowerofdifferentalgorithms.
PropositionA.1. LetT ,T :M →M ,U ,U :M →M becolortransformations. IfT ⪯T andU ⪯U ,
1 2 k1 k2 1 2 k2 k3 1 2 1 2
thenU ◦T ⪯U ◦T .
1 1 2 2
Proof. SinceT ⪯ T , T (χ) ⪯ T (χ)holdsforallχ ∈ M . SinceU isorder-preserving, U (T (χ)) ⪯ U (T (χ))
1 2 1 2 k1 2 2 1 2 2
holdsforallχ ∈ M . Finally,sinceU ⪯ U ,U (T (χ)) ⪯ U (T (χ))holdsforallχ ∈ M . Combiningtheabove
k1 1 2 1 1 2 1 k1
inequalitiesyieldsthedesiredresult.
PropositionA.2. LetT : M → M andT : M → M becolorrefinements,andletU : M → M and
1 k1 k1 2 k2 k2 1 k0 k1
U :M →M becolortransformations. IfT ◦U ◦T∞◦U ≡U ◦T∞◦U ,thenU ◦T∞◦U ⪯T∞◦U ◦U .
2 k1 k2 2 2 1 1 2 1 1 2 1 1 2 2 1
13OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
Proof. IfT ◦U ◦T∞◦U ≡U ◦T∞◦U ,thenbydefinitionofstablerefinement,T∞◦U ◦T∞◦U ≡U ◦T∞◦U .
2 2 1 1 2 1 1 2 2 1 1 2 1 1
Since T∞ is a refinement, U ◦T∞ ◦U ⪯ T∞ ◦U ◦U according to Proposition A.1. We thus obtain the desired
1 2 1 1 2 2 1
result.
CorollaryA.3. LetT ,T :M →M becolorrefinements. Then,T ⪯T impliesthatT ◦T∞ ≡T∞.
1 2 k k 1 2 2 1 1
Proof. SinceT ⪯T ,T ◦T∞ ⪯T ◦T∞byPropositionA.1.Namely,T∞ ⪯T ◦T∞.Ontheotherhand,T ◦T∞ ⪯T∞
1 2 1 1 2 1 1 2 1 2 1 1
sinceT isacolorrefinement. Combinedthetwodirectionsyieldsthedesiredresult.
2
The above two propositions will play a crucial role in our subsequent proofs. Below, we give a simple example to
illustrate how these results can be used to give a proof that a GNN model M is more expressive than another model
1
M . SupposeT ,T : M → M arecolorrefinementscorrespondingtooneGNNlayerofM andM ,respectively,
2 1 2 k k 1 2
and let U : M → M be the color transformation corresponding to the final pooling layer in M and M . Then,
k 0 1 2
the color refinement algorithms associated with M and M can be represented by U ◦T∞ and U ◦T∞, respectively.
1 2 1 2
Concretely, denote by χ0 the initial color mapping in the two algorithms, e.g., the constant mapping where χ0(u) is
G
the same for all Gu ∈ G . It follows that the graph representation of a graph G computed by the two algorithms is
k
[U(T (χ0))](G)and[U(T (χ0))](G), respectively. Then, Thestatement“M ismoreexpressivethanM ”meansthat
1 2 1 2
[U(T (χ0))](G)=[U(T (χ0))](H) =⇒ [U(T (χ0))](G)=[U(T (χ0))](H)forallgraphsG,H ∈G.
1 1 2 2
ToprovethatM ismoreexpressivethanM ,itsufficestoprovethatT ◦T∞ isasexpressiveasT∞. Indeed,thisis
1 2 2 1 1
actuallyasimpleconsequenceofPropositionsA.1andA.2. IfT ◦T∞isasexpressiveasT∞,thenT∞ismoreexpressive
2 1 1 1
thanT∞(PropositionA.2),andthusU ◦T∞ismoreexpressivethanU ◦T∞(PropositionA.1),yieldingthedesiredresult.
2 1 2
A.2.Basicresults
ThissubsectionprovesseveralbasicresultsforEPWL.WebeginbyprovingthatEPWLisstrictlymoreexpressivethan1-WL
(Proposition4.2). WewillfirstrestateProposition4.2usingthecolorrefinementterminologiesdefinedinAppendixA.1.
Weneedthefollowingcolortransformations:
• EPWLcolorrefinement. DefineT :M →M suchthatforanycolormappingχ∈M androotedgraphGu,
EP,M 1 1 1
[T (χ)] (u)=hash(χ (u),{{(χ (v),PM(u,v)):v ∈V }}). (10)
EP,M G G G G G
• 1-WLcolorrefinement. DefineT :M →M suchthatforanycolormappingχ∈M androotedgraphGu,
WL 1 1 1
[T (χ)] (u)=hash(χ (u),{{(χ (v),atp (u,v)):v ∈V }}). (11)
WL G G G G G
• Globalpooling. DefineT :M →M suchthatforanycolormappingχ∈M andgraphG,
GP 1 0 1
[T (χ)](G)=hash({{χ (u):u∈V }}). (12)
GP G G
Equippedwiththeabovecolortransformations,Proposition4.2isequivalenttothefollowing:
PropositionA.4. ForanygraphmatrixM ∈{A,L,Lˆ},T ◦T∞ ⪯T ◦T∞.
GP EP,M GP WL
BasedonPropositionsA.1andA.2,itsufficestoprovethatT ⪯T .
EP,M WL
LemmaA.5. ForanygraphmatrixM ∈{A,L,Lˆ},PM ⪯atp. Here,theatomictypeoperatoratp∈M isregardedas
2
a2-dimensionalcolormapping. ThisreadilyimpliesthatT ⪯T .
EP,M WL
Proof. Itsufficestoprovethat,foranytwographsG,H ∈Gandverticesu,v ∈V ,x,y ∈V ,ifPM(u,v)=PM(x,y),
G H G H
then(a)u=v ⇐⇒ x=y;(b){u,v}∈E ⇐⇒ {x,y}∈E .
G H
(cid:80)
Item (a) simply follows from the fact that u = v iff P(u,v) = 1 (by definition of eigen-
(λ,P(u,v))∈PM(u,v)
G (cid:80)
decomposition). Item (b) simply follows from the fact that {u,v} ∈ E iff λP(u,v) ̸= 0 and
G (λ,P(u,v))∈PM(u,v)
G
u̸=v(whichholdsforallmatricesM ∈{A,L,Lˆ}).
14OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
Strict sepatation. It is easy to give a pair of counterexample graphs G,H
that are indistinguishable by 1-WL but can be distinguished via EPWL. We
givesuchapairofgraphsinFigure2. Onecancheckthatthetwographshas
differencesetofeigenvaluesnomatterwhatgraphmatrixM ∈{A,L,Lˆ}is
usedinEPWL.
Figure2.Apairofcounterexamplegraphs
WenextshowthatEPWLismoreexpressivethanspectralpositionalencoding thatareindistinguishableby1-WLbutcan
usingLaplacianeigenvectors. Wewillprovethefollowingresult: bedistinguishedviaEPWL.
PropositionA.6. GivenanygraphmatrixM ∈{A,L,Lˆ}andanyinitialcolormappingχ0 ∈M,letχ1 =T (χ0)
EP,M
betheEPWLcolormappingafterthefirstiteration. Then, foranygraphsG,H ∈ G andverticesu ∈ V ,x ∈ V , if
G H
χ1(u)=χ1 (v),thenPM(u,u)=PM(x,x).
G H G H
Proof. LetG,H ∈G andverticesu∈V ,x∈V satisfythatχ1(u)=χ1 (v). Then,
G H G H
{{(χ0(v),PM(u,v)):v ∈V }}={{(χ0 (y),PM(x,y)):y ∈V }}. (13)
G G G H H H
Notethatforanyu,v ∈V andx,y ∈V ,ifPM(u,v)=PM(x,y),thenu=v ⇐⇒ x=y(LemmaA.5). Therefore,
G H G H
Equation(13)impliesthatPM(u,u)=PM(x,x).
G H
A.3.ProofofTheorem4.3
ThissubsectionaimstoproveTheorem4.3. WewillfirstrestateTheorem4.3usingthecolorrefinementterminologies
definedinAppendixA.1. Weneedthefollowingcolortransformations:
• EPWLcolorrefinement. DefineT :M →M suchthatforanycolormappingχ∈M androotedgraphGu,
EP,M 1 1 1
[T (χ)] (u)=hash(χ (u),{{(χ (v),PM(u,v)):v ∈V }}). (14)
EP,M G G G G G
• SWLcolorrefinement. DefineT :M →M suchthatforanycolormappingχ∈M androotedgraphGuv,
S 2 2 2
[T (χ)] (u,v)=hash(χ (u,v),{{(χ (u,w),atp (v,w)):w ∈V }}). (15)
S G G G G G
• PSWLcolorrefinement. DefineT :M →M suchthatforanycolormappingχ∈M androotedgraphGuv,
PS 2 2 2
[T (χ)] (u,v)=hash(χ (u,v),χ (v,v),{{(χ (u,w),atp (v,w)):w ∈V }}). (16)
PS G G G G G G
• Globalrefinement. DefineT ,T :M →M suchthatforanycolormappingχ∈M androotedgraphGuv,
Gu Gv 2 2 2
[T (χ)] (u,v)=hash(χ (u,v),{{χ (u,w):w ∈V }}), (17)
Gu G G G G
[T (χ)] (u,v)=hash(χ (u,v),{{χ (w,v):w ∈V }}). (18)
Gv G G G G
• Diagonalrefinement. DefineT ,T :M →M suchthatforanycolormappingχ∈M androotedgraphGuv,
Du Dv 2 2 2
[T (χ)] (u,v)=hash(χ (u,v),χ (u,u)), (19)
Du G G G
[T (χ)] (u,v)=hash(χ (u,v),χ (v,v)). (20)
Dv G G G
• Nodemarkingrefinement. DefineT : M → M suchthatforanycolormappingχ ∈ M androotedgraph
NM 2 2 2
Guv,
[T (χ)] (u,v)=hash(χ (u,v),I[u=v]). (21)
NM G G
• Lifttransformation. DefineT :M →M suchthatforanycolormappingχ∈M androotedgraphGuv,
↑ 1 2 1
[T (χ)] (u,v)=χ (v). (22)
↑ G G
• Subgraphpooling. DefineT :M →M suchthatforanycolormappingχ∈M androotedgraphGu,
SP 2 1 2
[T (χ)] (u)=hash({{χ (u,v):v ∈V }}). (23)
SP G G G
15OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
• Globalpooling. DefineT :M →M suchthatforanycolormappingχ∈M andgraphG,
GP 1 0 1
[T (χ)](G)=hash({{χ (u):u∈V }}). (24)
GP G G
NotethatT ⪯T ,T ⪯T ,andT ⪯T . Equippedwiththeabovecolortransformations,Theorem4.3isequivalent
PS S PS Gu PS Dv
tothefollowing:
TheoremA.7. ForanygraphmatrixM ∈{A,L,Lˆ},T ◦T ◦T∞◦T ◦T ⪯T ◦T∞ .
GP SP PS NM ↑ GP EP,M
Wewilldecomposetheproofintoasetoflemmas. First,weleveragearecentbreakthroughingraphtheoryestablishedby
Rattan&Seppelt(2023). Werestatetheirresultinourcontextasfollows.
DefinitionA.8. DefineafamilyofgraphmatricesEasfollows,calledequitablematrices:
a) Basematrices: theidentifymatrixI,all-onematrixJ,adjacencymatrixA,degreematrixDareallequitable;
b) Algebraicproperty: foranyequitablematricesM ,M ∈E,M +M ,cM ,M M ∈E,wherec∈Ccanbeany
1 2 1 2 1 1 2
constant.
c) Spectral property: for any M ∈ E and λ ∈ C, let PM be the projection onto the eigenspace spanned by the
λ
eigenvectorsofM witheigenvalueλ(PM =OifλisnotaneigenvalueofM). Then,PM ∈E.
λ λ
BasedonDefinitionA.8,wereadilyhavethefollowingproposition:
PropositionA.9. TheLaplacianmatrixLandthenormalizedLaplacianmatrixLˆ areequitable.
Rattan&Seppelt(2023)provedthefollowingmainresult:
TheoremA.10. ForanyM ∈E,T∞(T (χC))⪯M,whereχC ∈M istheconstantcolormapping.
S NM 2
CorollaryA.11. ForanysymmetricequitablematrixM ∈Eandinitialcolormappingχ0 ∈M ,(T∞◦T ◦T )(χ0)⪯
1 PS NM ↑
PM.
Proof. For any symmetric equitable matrix M ∈ E and any λ ∈ R, PM ∈ E holds by Definition A.8(c). Therefore,
λ
Theorem A.10 implies that T∞(T (χC)) ⪯ PM, i.e., T∞(T (χC)) ⪯ PM. Next, note that PM ̸= O iff λ is an
S NM λ PS NM λ λ
eigenvalueofM.Therefore,thegraphinvariant∆overG definedby∆ (u,v)=I[λisaneigenvalueofM]satisfiesthat
2 G
(T ◦T ◦T ◦T∞◦T )(χC))⪯∆. SinceT ◦T ◦T ◦T∞ ≡T∞(basedonthefactsT ⪯T andT ⪯T
Gu Dv Gu PS NM Gu Dv Gu PS PS PS Gu PS Dv
andCorollaryA.3),wehaveT∞(T (χC))⪯∆.Byconsideringallλ∈R,weobtainthatT∞(T (χC))⪯PM.Finally,
PS NM PS NM
notingthatT∞◦T isorder-preservingandT (χ0)⪯χC,wehave(T∞◦T ◦T )(χ0)⪯PM forallχ0 ∈M .
PS NM ↑ PS NM ↑ 1
WearenowreadytoproveTheoremA.7.
ProofofTheoremA.7. NotethatT ◦T ◦T isacolorrefinement,andthusT∞ ◦T ◦T ◦T ⪯T∞ . Toprove
SP NM ↑ EP,M SP NM ↑ EP,M
thatT ◦T ◦T∞◦T ◦T ⪯T ◦T∞ ,itsufficestoprovethatT ◦T∞◦T ⪯T∞ ◦T ◦T accordingto
GP SP PS NM ↑ GP EP,M SP PS NM EP,M SP NM
PropositionA.1. Moreover,basedonPropositionA.2,itsufficestoprovethatT ◦T ◦T∞◦T ≡T ◦T∞◦T .
EP,M SP PS NM SP PS NM
Letχ0 ∈M beanyinitialcolormappingandletχ=T∞(T (χ )). PickanygraphsG,H andverticesu∈V ,x∈V
2 PS NM 0 G H
suchthat[T (χ)] (u)=[T (χ)] (x),i.e.,
SP G SP H
{{χ (u,v):v ∈V }}={{χ (x,y):y ∈V }}. (25)
G G H H
InvokingCorollaryA.11obtainsthat
{{(χ (u,v),PM(u,v)):v ∈V }}={{(χ (x,y),PM(x,y)):y ∈V }}. (26)
G G G H H H
SinceT ⪯T ,basedonCorollaryA.3wehaveT ◦T∞ ≡T∞,i.e.,χ≡T (χ). Therefore,
PS Dv Dv PS PS Dv
{{(χ (v,v),PM(u,v)):v ∈V }}={{(χ (y,y),PM(x,y)):y ∈V }}. (27)
G G G H H H
SinceT ⪯T ,wesimilarlyhaveχ≡T (χ). Therefore,
PS Gu Gu
{{({{χ (v,w):w ∈V }},PM(u,v)):v ∈V }}={{({{χ (y,z):z ∈V }},PM(x,y)):y ∈V }}. (28)
G G G G H H H H
Combining with Equations (25) and (28), we have T (T (χ)) ≡ T (χ). Finally, noting that χ0 is arbitrary, we
EP,M SP SP
concludethatT ◦T ◦T∞◦T ≡T ◦T∞◦T .
EP,M SP PS NM SP PS NM
16OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
A.4.ProofofTheorem5.1
ThissubsectionaimstoproveTheorem5.1. WewillfirstrestateTheorem5.1usingthecolorrefinementterminologies
definedinAppendixA.1. SimilarlytoAppendixA.3,wedefinethefollowingcolortransformations:
• EPWLcolorrefinement. DefineT :M →M suchthatforanycolormappingχ∈M androotedgraphGu,
EP,M 1 1 1
[T (χ)] (u)=hash(χ (u),{{(χ (v),PM(u,v)):v ∈V }}). (29)
EP,M G G G G G
• GD-WLcolorrefinement. DefineT :M →M suchthatforanycolormappingχ∈M androotedgraph
GD,M 1 1 1
Gu,
[T (χ)] (u)=hash(χ (u),{{(χ (v),M (u,v)):v ∈V }}). (30)
GD,M G G G G G
• Globalpooling. DefineT :M →M suchthatforanycolormappingχ∈M andgraphG,
GP 1 0 1
[T (χ)](G)=hash({{χ (u):u∈V }}). (31)
GP G G
Theorem5.1isequivalenttothefollowing:
TheoremA.12. ForanygraphdistancematrixM listedinSection5,T ◦T∞ ⪯T ◦T∞ .
GP EP,Lˆ GP GD,M
Inthissubsection,letχC ∈M betheconstantcolormappingandletχ=T∞ (χC). Defineacolormappingχ¯∈M
1 EP,Lˆ 2
(cid:16) (cid:17)
suchthatχ¯ (u,v)= χ (u),χ (v),PLˆ(u,v) forallgraphGandverticesu,v ∈V . Thefollowingpropositionwill
G G G G G
playacentralroleinoursubsequentproofs:
PropositionA.13. ForanygraphmatrixM,ifχ¯⪯M,thenT ◦T∞ ⪯T ◦T∞ .
GP EP,Lˆ GP GD,M
Proof. BasedonPropositionsA.1andA.2,itsufficestoprovethatT ◦T∞ ≡T∞ . Pickanyχ0 ∈M andlet
GD,M EP,Lˆ EP,Lˆ 1
χ′ =T∞ (χ0). Sinceχ′ ≡T (χ′),wehave
EP,Lˆ EP,Lˆ
χ′ (u)=χ′ (y) =⇒ χ′ (u)=χ′ (y)∧{{(χ′ (v),PLˆ (u,v)):v ∈V }}={{(χ′ (y),PLˆ (x,y)):y ∈V }}
G H G H G G G H H H
=⇒ {{(χ′ (u),χ′ (v),PLˆ (u,v)):v ∈V }}={{(χ′ (x),χ′ (y),PLˆ (x,y)):y ∈V }}
G G G G H H H H
forallgraphsG,H ∈G andverticesu∈V ,x∈V . Moreover,sinceχ′ ⪯χandχ¯⪯M,wehave
G H
χ′ (u)=χ′ (y) =⇒ {{(χ′ (v),M (u,v)):v ∈V }}={{(χ′ (y),M (x,y)):y ∈V }}
G H G G G H H H
forallgraphsG,H ∈G andverticesu∈V ,x∈V . Therefore,χ′ ≡T (χ′). Finally,bynotingthatχ′ =T∞ (χ0)
G H GD,M EP,Lˆ
andχ0isarbitrary,weobtainthatT ◦T∞ ≡T∞ ,asdesired.
GD,M EP,Lˆ EP,Lˆ
WenextpresentseveralbasicfactsaboutEP-WLstablecolorsχ.
PropositionA.14. ForanygraphsG,H ∈G andverticesu∈V ,x∈V ,ifχ (u)=χ (x),then
G H G H
a) deg (u)=deg (x);
G H
b) PLˆ(u,u)=PLˆ(x,x).
G H
Proof. SinceT ⪯ T (LemmaA.5),wehaveχ ⪯ T∞(χC). Item(a)followsfromthefactthatthe1-WLstable
EP,M WL WL
colorcanencodethevertexdegree, i.e., T∞(χC) ⪯ deg. Toproveitem(b), notethatχ ⪯ χ1, whereχ1 isdefinedin
WL
PropositionA.6. Therefore,item(b)readilyfollowsfromPropositionA.6.
Below,wewillseparatelyconsidereachofthedistanceslistedinSection5.
LemmaA.15. LetM betheshortestpathdistancematrix. Then,χ¯⪯M.
17OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
Proof. WewillproveastrongerresultthatPLˆ ⪯M. NotethatthereisarelationbetweenM andthenormalizedadjacency
matrixAˆ :=D−1/2AD−1/2 =I−Lˆ,whichcanbeexpressedasfollows:
M (u,v)=min{i∈N:Aˆi (u,v)>0} ∀G∈G,u,v ∈V . (32)
G G G
The above relation can be interpreted as follows: Aˆi (u,v) = (cid:80) ω(p (u,v)) where p (u,v) ranges over all
G pi(u,v) i i
walks of length i from u to v, and ω(p (u,v)) := (deg (u)deg (v))−1/2(cid:0) Πi−1deg (w )(cid:1)−1 for a walk p (u,v) =
i G G j=1 G i i
(u,w ,··· ,w ,v),whichisalwayspositive.
1 i−1
Basedonthepropertyofeigen-decomposition,wehave
Aˆi (u,v)= (cid:88) λiP (u,v)= (cid:88) (1−λ)iP (u,v). (33)
G G G
(λ,PG(u,v))∈P GAˆ (λ,PG(u,v))∈P GLˆ(u,v)
ThisimpliesthatM (u,v)canbepurelydeterminedbyPLˆ(u,v),i.e.,PLˆ ⪯M.
G G
LemmaA.16. LetM betheresistancedistancematrix. Then,χ¯⪯M.
Proof. Wefirstconsiderthecaseofconnectedgraphs. WeleverageacelebratedresultestablishedinKlein&Randic(1993),
whichbuildsconnectionsbetweenresistancedistanceandgraphLaplacian. Specifically,foranyconnectedgraphG∈G
andverticesu,v ∈V ,
G
M (u,v)=L†(u,u)+L†(v,v)−2L†(u,v), (34)
G
where†denotesthematrixMoore–Penroseinversion. SubstitutingL=D1/2LˆD1/2intoEquation(34),weobtain
M (u,v)=(deg (u))−1Lˆ†(u,u)+(deg (v))−1Lˆ†(v,v)−2(deg (u)deg (v))−1/2Lˆ†(u,v), (35)
G G G G G
Moreover,wehaveLˆ† =(cid:80) λ−1P where(cid:80) λ P istheeigen-decompositionofLˆ. Combiningalltheserelations,
i:λi̸=0 i i i i i
onecanseethatM (u,v)ispurelydeterminedbythetuple(deg (u),deg (v),PLˆ(u,u),PLˆ(v,v),PLˆ(u,v)). Basedon
G G G G G G
PropositionA.14,χ (u)determinesdeg (u)andPLˆ(u,u),andχ (v)determinesdeg (v)andPLˆ(v,v). Consequently,
G G G G G G
χ¯⪯M.
Wenextconsiderthe generalcasewherethegraph Gisdisconnected. In thiscase, Lisablock diagonalmatrix. Itis
straightforwardtoseethat
(cid:26) L†(u,u)+L†(v,v)−2L†(u,v) ifuandvareinthesameconnectedcomponent,
M (u,v)= (36)
G ∞ otherwise.
SincewehaveprovedthatPM canencodetheshortestpathdistance(LemmaA.15),PLˆ(u,v)canencodewhetheruandv
G
areinthesameconnectedcomponent. Sowestillhaveχ¯⪯M.
LemmaA.17. LetM bethehitting-timedistancematrix. Then,χ¯⪯M.
Proof. Withoutlossofgenerality,weonlyconsiderconnectedgraphsasinLemmaA.16. Accordingtothedefinitionof
hitting-timedistance,foranyconnectedgraphG∈G andverticesu,v ∈V ,wehavethefollowingrecursiverelation:
G
(cid:26)
0 ifu=v,
M G(u,v)= 1+ 1 (cid:80) M (u,v) otherwise. (37)
deg G(u) w∈NG(u) G
Nowdefineanewgraphmatrix
H˜ =11⊤+D−1A(H˜ −diag(H˜)) (38)
wherediag(H˜)isthediagonalmatrixobtainedfromH˜ byzeroingoutallnon-diagonalelementsofH˜. Itfollowsthat
M =H˜ −diag(H˜).
Wefirstcalculatediag(H˜). Left-multiplyingEquation(38)by1⊤Ayieldsthat
1⊤AH˜ =1⊤A11⊤+1⊤AD−1A(H˜ −diag(H˜))=1⊤A11⊤+1⊤A(H˜ −diag(H˜)),
18OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
where we use the fact that 1⊤AD−1 = 1⊤. Therefore, 1⊤Adiag(H˜) = 1⊤A11⊤, namely, H˜ (u,u) =
G
2|E |(deg (u))−1.
G G
WenextcomputethefullmatrixM. BasedonEquation(38),wehave
(I−D−1A)M =11⊤−diag(H˜). (39)
Left-multiplyingEquation(39)byDleadstothefundamentalequation
LM =D11⊤−2|E |I. (40)
G
WhenthegraphGisconnected,theeigenspaceofLassociatedwitheigenvalueλ=0onlyhasonedimension,andany
eigenvectorbsatisfyingLb=0hastheformb=c1forsomec. ThisimpliesthatallsolutionstoEquation(40)hasthe
form
M =L†DJ +JS−2|E |L† (41)
G
where S is a diagonal matrix and J = 11⊤. Noting that diag(M) = O, we have O = diag(L†DJ)+diag(JS)−
2|E |diag(L†). Sincediag(JS)=S foranydiagonalmatrixS andJdiag(L†DJ)=JDL†,wefinalobtain
G
M =L†DJ +J(cid:0) 2|E |diag(L†)−diag(L†DJ)(cid:1) −2|E |L†
G G
(42)
=L†DJ −JDL†+2|E |Jdiag(L†)−2|E |L†.
G G
Below,wewillprovethatχ¯⪯M. NotingthatL† =D−1/2Lˆ†D−1/2,wehave
M =D−1/2Lˆ†D1/2J −JD1/2Lˆ†D−1/2+2|E |JD−1/2diag(Lˆ†)D−1/2−2|E |D−1/2Lˆ†D−1/2. (43)
G G
Equivalently,foranygraphG∈G andverticesu,v ∈V ,
G
M (u,v)=D−1/2(u,u) (cid:88) Lˆ† (u,w)D1/2(w,w)−D−1/2(v,v) (cid:88) Lˆ† (w,v)D1/2(w,w)
G G G G G G G
w∈VG w∈VG (44)
+2|E |Lˆ† (v,v)D−1(v,v)−2|E |D−1/2(u,u)Lˆ† (u,v)D−1/2(v,v).
G G G G G G G
Fromtheaboveequation,onecanseethatM (u,v)isfullydeterminedbythefollowingtuple:
G
(cid:16) (cid:17)
|E |,deg (u),deg (v),Lˆ† (v,v),Lˆ† (u,v),{{(Lˆ† (u,w),deg (w)):w ∈V }},{{(Lˆ† (w,v),deg (w)):w ∈V }} .
G G G G G G G G G G G
Accordingtotheeigen-decomposition,Lˆ† =(cid:80) λ−1P where(cid:80) λ P istheeigen-decompositionofLˆ. Therefore,
i:λi̸=0 i i i i i
Lˆ† ⪯PLˆ . Besides,basedonPropositionA.14wehavethatdeg (u)isdeterminedbyχ (u),deg (v)isdeterminedby
G G G
χ (v),andPLˆ(v,v)isdeterminedbyχ (v). Itfollowsthat|E |isdeterminedbyχ (u),becausebyχ≡T (χ)we
G G G G G EP,Lˆ
haveforallgraphsG,H ∈G andverticesu∈V ,x∈V ,
G H
χ (u)=χ (x) =⇒ {{χ (v):v ∈V }}={{χ (y):y ∈V }}
G H G G H H
=⇒ {{deg (v):v ∈V }}={{deg (y):y ∈V }}
G G H H
=⇒ |E |=|E |.
G H
Wenextprovethat{{(Lˆ† (u,w),deg (w)):w ∈V }}isdeterminedbyχ (u). Againbyusingχ≡T (χ),wehave
G G G G EP,Lˆ
forallgraphsG,H ∈G andverticesu∈V ,x∈V ,
G H
χ (u)=χ (x) =⇒ {{(χ
(v),PLˆ
(u,v)):v ∈V }}={{(χ
(y),PLˆ
(x,y)):y ∈V }}
G H G G G H H H
=⇒ {{(deg (v),Lˆ† (u,v)):v ∈V }}={{(deg (y),Lˆ† (x,y)):y ∈V }}.
G G G H H H
Using the same analysis and noting that M†(w,v) = M†(v,w) for any symmetric matrix M, we can prove that
G G
{{(Lˆ† (w,v),deg (w)) : w ∈ V }}isdeterminedbyχ (v). Combiningalltheserelationsleadstotheconclusionthat
G G G G
χ¯⪯M.
19OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
CorollaryA.18. LetM bethecommute-timedistancematrix. Then,χ¯⪯M.
Proof. ThisisasimpleconsequenceofLemmaA.17. DenotingbyH thehitting-timedistancematrix,wehaveM =
H +H⊤. Sinceχ¯ ⪯H,χ¯ ⪯H⊤ (becausePLˆ(u,v)=PLˆ(v,u)foranygraphGandverticesu,v ∈V ). Therefore,
G G G
χ¯⪯M.
LemmaA.19. LetM bethePageRankdistancematrixassociatedwithweightsequenceγ ,γ ,···. Then,χ¯⪯M.
0 1
Proof. By definition of PageRank distance, M = (cid:80)∞ γ (D−1A)k. Let (cid:80) λ P be the eigen-decomposition of Lˆ.
k=0 k i i i
Then,Aˆ =D−1/2AD−1/2 =I−Lˆ =(cid:80) (1−λ )P . Therefore,
i i i
(D−1A)k =D−1/2AˆkD1/2 =(cid:88) (1−λ )kD−1/2P D1/2. (45)
i i
i
PluggingtheaboveequationintothedefinitionofPageRankdistance,wehaveforanygraphG∈G andverticesu,v ∈V ,
G
(cid:32) ∞ (cid:33)
(cid:88) (cid:88)
M (u,v)= γ (1−λ )k P (u,v)(deg (u))−1/2(deg (v))1/2. (46)
G k i i G G
i k=0
OnecanseethatM (u,v)ispurelydeterminedbythetuple(deg (u),deg (v),PLˆ(u,v)). BasedonPropositionA.14,
G G G G
χ (u)determinesdeg (u)andχ (v)determinesdeg (v). Consequently,χ¯⪯M.
G G G G
Wenextstudythe(normalized)diffusiondistance. Considerthecontinuousgraphdiffusionprocessdefinedasfollows.
Giventimet ≥ 0,letpt (u)betheprobability“mass”ofparticlesatpositionu. Theparticleswillmovefollowingthe
G
differentialequationgivenbelow:
d
pt =Tpt, (47)
dt
whereT isthetransitionmatrix. Forexample,whenT =AD−1−I,thedifferentialequationessentiallycharacterizes
a random walk diffusion process. In this paper, we consider the normalized diffusion distance, which corresponds to
T =Aˆ−I =−Lˆ. Givenhyperparameterτ ≥0,denoteby(p| )τ betheprobability“mass”vectorattimeτ withthe
Gu
initialconfiguration(p| )0(u)=1and(p| )0(v)=0forallv ̸=u. Then,thediffusiondistancematrixM isdefinedas
Gu Gu
M (u,v)=∥(p| )τ −(p| )τ∥ . (48)
G Gu Gv 2
LemmaA.20. LetM bethenormalizeddiffusiondistancedefinedabove. Then,χ¯⪯M.
Proof. Since Equation (47) is a linear differential equation, we can solve it and obtain (p| )τ = exp(τT)(p| )0.
Gu Gu
Therefore,
M G(u,v)=(cid:13) (cid:13)exp(τT)(cid:0) (p| Gu)0−(p| Gv)0(cid:1)(cid:13) (cid:13) 2, (49)
whereexp(T)isthematrixexponentialofT. Equivalently,
M2(u,v)=(cid:0) (p| )0−(p| )0(cid:1)⊤ exp(2τT)(cid:0) (p| )0−(p| )0(cid:1) . (50)
G Gu Gv Gu Gv
Let(cid:80) λ P betheeigen-decompositionofLˆ. Then,exp(2τT)=exp(−2τLˆ)=(cid:80) exp(−2τλ )P . Therefore,
i i i i i i
(cid:88)
M2(u,v)= exp(−2τλ )(P (u,u)+P (v,v)−2P (u,v)). (51)
G i i i i
i
ThisimpliesthatM (u,v)ispurelydeterminedbythetuple(PLˆ(u,u),PLˆ(v,v),PLˆ(u,v)). Weconcludethatχ¯⪯M
G G G G
byusingPropositionA.14.
Wefinallystudythebiharmonicdistance.
LemmaA.21. LetM bethebiharmonicdistancedefinedinLipmanetal.(2010). Then,χ¯⪯M.
20OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
Proof. Withoutlossofgenerality, weonlyconsiderconnectedgraphs. AccordingtoWeietal.(2021), thebiharmonic
distanceforaconnectedgraphcanbeequivalentlywrittenas
M (u,v)=(L2)†(u,u)+(L2)†(v,v)−2(L2)†(u,v). (52)
G
ThesubsequentproofisalmostthesameasinLemmaA.16andweomititforclarity.
A.5.ProofofTheorem6.1
ThissectionaimstoproveTheorem6.1. Below,wewilldecomposetheproofintothreeparts. First,wewilldescribethe
colorrefinementalgorithmcorrespondingtoSpectralIGN,whichisequivalenttoSpectralIGNintermsofdistinguishing
non-isomorphicgraphs. Then,wewillprovethatthiscolorrefinementalgorithmismoreexpressivethanEPWLusingthe
colorrefinementterminologiesdefinedinAppendixA.1. Finally,wewillprovetheotherdirection,i.e.,EPWLismore
expressivethanthecolorrefinementalgorithmofSpectralIGN,thusconcludingtheproofofTheorem6.1.
ColorrefinementalgorithmforSpectralIGN.Todefinethealgorithm,wefirstneedtoextendseveralconceptsdefined
inAppendixA.1toincorporateeigenvalues. Formally, letΛM bethegraphspectruminvariantrepresentingthesetof
eigenvaluesforgraphmatrixM,i.e.,ΛM(G):={λ:λisaneigenvalueofM }forG∈G. Define
G
GPM :={(Gu,λ):Gu ∈G ,λ∈ΛM(G)}. (53)
k k
WecanthendefinecolormappingsoverGPM. Formally,afunctionχdefinedoverdomainGPM iscalledacolormappingif
k k
χ(Gu,λ)=χ(Hv,µ)holdsforall(Gu,λ),(Hv,µ)∈GPM satisfyingGu ≃Hv andλ=µ. Withoutambiguity,wewill
k
usethenotationχ (λ,u)torefertoχ(Gu,λ)for(Gu,λ)∈GPM. DefineMPM tobethefamilyofallcolormappings
G k k
overGPM. Wecansimilarlydefineequivalencerelation“≡”andpartialrelation“⪯”betweencolormappingsinMPM. In
k k
addition,thecolortransformationcanalsobeextendedinasimilarmanner.
Throughoutthissection,weusethenotationχPM ∈MPM torepresenttheinitialcolormappinginSpectralIGN,whichis
2
definedasχPM(λ,u,v) = (λ,PM(u,v))forall(Guv,λ) ∈ GPM,wherePM istheprojectionmatrixassociatedwith
G λ 2 λ
eigenvalueλ. Wethendefinethefollowingcolortransformations:
• 2-IGNcolorrefinement. DefineT :M →M suchthatforanycolormappingχ∈M andGuv ∈G ,
IGN 2 2 2 2
[T (χ)] (u,v)=hash(χ (u,v),χ (u,u),χ (v,v),χ (v,u),δ (χ (u,u)),
IGN G G G G G uv G
{{χ (u,w):w ∈V }},{{χ (w,u):w ∈V }},
G G G G
{{χ (v,w):w ∈V }},{{χ (w,v):w ∈V }},
G G G G
(54)
{{χ (w,w):w ∈V }},{{χ (w,x):w,x∈V }},
G G G G
δ ({{χ (u,w):w ∈V }}),δ ({{χ (w,u):w ∈V }}),
uv G G uv G G
δ ({{χ (w,w):w ∈V }}),δ ({{χ (w,x):w,x∈V }})).
uv G G uv G G
Here,thefunctionδ satisfiesthatδ (c)=cifu=vandδ (c)=0otherwise(0isaspecialelementthatdiffers
uv uv uv
fromallχ (u,v)). OnecanseethatEquation(54)has15aggregationsinsidethehashfunction,whichmatchesthe
G
numberoforthogonalbasesfora2-IGNlayer(Maronetal.,2019b).
• Spectralpooling. DefineT :MPM →M suchthatforanycolormappingχ∈MPM andGuv ∈G ,
SP 2 2 2 2
[T (χ)] (u,v)=hash({{χ (λ,u,v):λ∈ΛM(G)}}). (55)
SP G G
• SpectralIGNcolorrefinement. DefineT :MPM →MPM suchthatforanycolormappingχ∈MPM and
SIGN 2 2 2
(Guv,λ)∈GPM,
2
[T (χ)] (λ,u,v)=hash([T (χ(λ,·,·))] (u,v),[T (T (χ))] (u,v)). (56)
SIGN G IGN G IGN SP G
• SpectralIGNfinalpooling. DefineT :MPM →M suchthatforanycolormappingχ∈MPM andG∈G,
FP 2 0 2
[T (χ)](G)=hash({{χ (λ,u,v):λ∈ΛM(G),u,v ∈V }}). (57)
FP G G
21OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
• Jointpooling. DefineT :M →M suchthatforanycolormappingχ∈M andG∈G,
JP 2 0 2
[T (χ)](G)=hash({{χ (u,v):u,v ∈V }}). (58)
JP G G
• 2-dimensionalPooling. DefineT :M →M suchthatforanycolormappingχ∈M andG∈G ,
P2 2 1 2 1
[T (χ)] (u)=hash({{χ (u,v):v ∈V }}). (59)
P2 G G G
• EPWLcolorrefinement. DefineT :M →M suchthatforanycolormappingχ∈M andGu ∈G ,
EP,M 1 1 1 1
[T (χ)] (u)=hash(χ (u),{{(χ (v),PM(u,v)):v ∈V }}). (60)
EP,M G G G G G
• Globalpooling. DefineT :M →M suchthatforanycolormappingχ∈M andgraphG,
GP 1 0 1
[T (χ)](G)=hash({{χ (u):u∈V }}). (61)
GP G G
We now related the expressive power of Spectral IGN to the corresponding color refinement algorithm. The proof is
straightforwardfollowingstandardtechniques,seee.g.,Zhangetal.(2023a).
Proposition A.22. The expressive power of Spectral IGN is bounded by the color mapping (T ◦ T∞ )(χPM) in
FP SIGN
distinguishingnon-isomorphicgraphs. Moreover,withsufficientlayersandpropernetworkparameters,SpectralIGNcan
beasexpressiveastheabovecolormappingindistinguishingnon-isomorphicgraphs.
Equivalently,thepoolingT canbedecomposedintothreepoolingtransformationsT ◦T ◦T ,asstatedbelow:
FP GP P2 SP
LemmaA.23. T ◦T ◦T ◦T∞ ≡T ◦T∞ .
GP P2 SP SIGN FP SIGN
Proof. First, it is clear that T ◦ T ◦ T ◦ T∞ ⪯ T ◦ T∞ . Thus, it suffices to prove that T ◦ T∞ ⪯
GP P2 SP SIGN FP SIGN FP SIGN
T ◦T ◦T ◦T∞ . Pickanyinitialcolormappingχ0 ∈MPM andletχ=T∞ (χ0). Notethatχ≡T (χ). We
GP P2 SP SIGN 2 SIGN SIGN
willprovethat(T )(χ)⪯(T ◦T ◦T )(χ). PickanygraphsG,H ∈G. Wehave
FP GP P2 SP
[T (χ)](G)=[T (χ)](H)
FP FP
=⇒ {{χ (λ,u,v):λ∈ΛM(G),u,v ∈V }}={{χ (µ,x,y):µ∈ΛM(H),x,y ∈V }}
G G H H
=⇒ {{χ (λ,u,u):λ∈ΛM(G),u∈V }}={{χ (µ,x,x):µ∈ΛM(H),x∈V }}
G G H H
=⇒ {{{{χ (λ,u,v):v ∈V }}:λ∈ΛM(G),u∈V }}={{{{χ (µ,x,y):y ∈V }}:µ∈ΛM(H),x∈V }}
G G G H H H
=⇒ {{{{{{χ (λ′,u,v):λ′ ∈ΛM(G)}}:v ∈V }}:λ∈ΛM(G),u∈V }}
G G G
={{{{{{χ (µ′,x,y):µ′ ∈ΛM(H)}}:y ∈V }}:µ∈ΛM(H),x∈V }}
H H H
=⇒ {{{{{{χ (λ′,u,v):λ′ ∈ΛM(G)}}:v ∈V }}:u∈V }}
G G G
={{{{{{χ (µ′,x,y):µ′ ∈ΛM(H)}}:y ∈V }}:x∈V }}
H H H
=⇒ [(T ◦T ◦T )(χ)](G)=[(T ◦T ◦T )(χ)](H),
GP P2 SP GP P2 SP
wherethesecond,third,andfourthstepsintheabovederivationarebasedonEquations(54)to(56). Wehaveobtainedthe
desiredresult.
Inthesubsequentproof,wewillshowthat(T ◦T ◦T ◦T∞ )(χPM)≡(T ◦T )(χ0),whereχ0 ∈M isthe
GP P2 SP SIGN GP EP,M 1
constantmapping.
LemmaA.24. Letχ0 ∈M betheconstantmapping. Then,(T ◦T ◦T ◦T∞ )(χPM)⪯(T ◦T∞ )(χ0).
1 GP P2 SP SIGN GP EP,M
Proof. DefineanauxiliarycolortransformationT : M → M suchthatforanycolormappingχ ∈ M androoted
× 1 2 1
graphGuv ∈G ,
2
[T (χ)] (u,v)=hash(χ (u),χ (v)). (62)
× G G G
22OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
WefirstprovethatT ≡T ◦T ◦T . Pickanycolormappingχ∈M andgraphsG,H ∈G,
GP GP P2 × 1
[T (χ)](G)=[T (χ)](H)
GP GP
⇐⇒ {{χ (u):u∈V }}={{χ (x):x∈V }}
G G H H
⇐⇒ {{(χ (u),{{χ (v):v ∈V }}):u∈V }}={{(χ (x),{{χ (y):y ∈V }}):x∈V }}
G G G G H H H H
⇐⇒ {{{{(χ (u),χ (v)):v ∈V }}:u∈V }}={{{{(χ (x),χ (y)):y ∈V }}:x∈V }}
G G G G H H H H
⇐⇒ [(T ◦T ◦T )(χ)](G)=[(T ◦T ◦T )(χ)](H).
GP P2 × GP P2 ×
Basedontheequivalence,itsufficestoprovethat(T ◦T ◦T ◦T∞ )(χPM)⪯(T ◦T ◦T ◦T∞ )(χ0). Since
GP P2 SP SIGN GP P2 × EP,M
T ◦T isorder-preservingbydefinition,itsufficestoprovethat(T ◦T∞ )(χPM)⪯(T ◦T∞ )(χ0).
GP P2 SP SIGN × EP,M
Wewillprovethefollowingstrongerresult: forallt∈N,(T ◦Tt )(χPM)⪯(T ◦Tt )(χ0). Theproofisbased
SP SIGN × EP,M
oninduction. Forthebasecaseoft = 0,since(T ◦Tt )(χ0)isaconstantmapping,theresultclearlyholds. Now
× EP,M
assumethattheresultholdsfort = t′ andconsiderthecaseoft = t′+1. Denoteχ = Tt′ (χPM),χˆ = Tt (χ0),
SIGN EP,M
andnotethatT (χ)⪯T (χPM)≡PM. PickanygraphsG,H ∈G andverticesu,v ∈V ,x,y ∈V . Basedonthe
SP SP G H
inductionhypothesis,[T (χ)] (u,v)=[T (χ)] (x,y)impliesthatχˆ (u)=χˆ (x)andχˆ (v)=χˆ (y). Wehave
SP G SP H G H G H
[T (T (χ))] (u,v)=[T (T (χ))] (x,y)
SP SIGN G SP SIGN H
=⇒ {{[T (T (χ))] (u,v):λ∈ΛM(G)}}={{[T (T (χ))] (x,y):µ∈ΛM(H)}}
IGN SP G IGN SP H
=⇒ [T (T (χ))] (u,v)=[T (T (χ))] (x,y)
IGN SP G IGN SP H
=⇒ [T (χ)] (u,v)=[T (χ)] (x,y)∧{{[T (χ)] (u,w):w ∈V }}={{[T (χ)] (x,z):z ∈V }}
SP G SP H SP G G SP H H
∧{{[T (χ)] (v,w):w ∈V }}={{[T (χ)] (y,z):z ∈V }}
SP G G SP H H
=⇒ χˆ (u)=χˆ (x)∧{{(χˆ (w),PM(u,w)):w ∈V }}={{(χˆ (z),PM(x,z)):z ∈V }}
G H G G G H H H
∧χˆ (v)=χˆ (y)∧{{(χˆ (w),PM(v,w)):w ∈V }}={{(χˆ (z),PM(y,z)):z ∈V }}
G H G G G H H H
=⇒ [T (χˆ)] (u)=[T (χˆ)] (x)∧[T (χˆ)] (v)=[T (χˆ)] (y)
EP,M G EP,M H EP,M G EP,M H
=⇒ [(T ◦T )(χˆ)] (u,v)=[(T ◦T )(χˆ)] (x,y)
× EP,M G × EP,M H
whereinthefirststepweusethedefinitionofSpectralIGN(Equation(56))andspectralpooling(Equation(55));inthe
thirdstepweusethedefinitionof2-IGN(Equation(54));inthefourthstepweusetheinductionhypothesisandthefactthat
T (χ)⪯PM. Thisconcludestheinductionstep.
SP
LemmaA.25. DefineanauxiliarycolortransformationT :M →M suchthatforanycolormappingχ∈M
×,PM 1 2 1
androotedgraphGuv ∈G ,
2
[T (χ)] (u,v)=hash(χ (u),χ (v),PM(u,v)). (63)
×,PM G G G G
Then,(T ◦T∞ )(χ0)⪯(T ◦T∞ )(χPM).
×,PM EP,M SP SIGN
Proof. Wewillprovethefollowingstrongerresult: foranyt ≥ 0,(T ◦T2t )(χ0) ⪯ (T ◦Tt )(χPM). The
×,PM EP,M SP SIGN
proofisbasedoninduction. Forthebasecaseoft = 0,since(T ◦T0 )(χ0) = T (χ0) ≡ T ◦χPM,the
×,PM EP,M ×,PM SP
resultclearlyholds. Nowassumethattheresultholdsfort=t′andconsiderthecaseoft=t′+1. Denoteχ=T2t′ (χ0)
EP,M
andχˆ=Tt′ (χPM). PickanygraphsG,H ∈G andverticesu,v ∈V ,x,y ∈V . Basedontheinductionhypothesis,
SIGN G H
[T (χ)] (u,v)=[T (χ)] (x,y)impliesthat[T (χˆ)] (u,v)=[T (χˆ)] (x,y). Wehave
×,PM G ×,PM H SP G SP H
[(T ◦T2 )(χ)] (u,v)=[(T ◦T2 )(χ)] (x,y)
×,PM EP,M G ×,PM EP,M H
=⇒ [T2 (χ)] (u)=[T2 (χ)] (x)∧[T2 (χ)] (v)=[T2 (χ)] (y)∧PM(u,v)=PM(x,y)
EP,M G EP,M H EP,M G EP,M H G H
=⇒ χ (u)=χ (x)∧χ (v)=χ (y)∧PM (u,v)=PM (x,y)
G H G H G H
∧PM(u,u)=PM(x,x)∧PM(v,v)=PM(y,y)∧I[u=v]=I[x=y]
G H G H
∧{{([T (χ)] (w),PM(u,w)):w ∈V }}={{([T (χ)] (z),PM(x,z)):z ∈V }}
EP,M G G G EP,M H H G
23OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
∧{{([T (χ)] (w),PM(v,w)):w ∈V }}={{([T (χ)] (z),PM(y,z)):z ∈V }}
EP,M G G G EP,M H H G
=⇒ χ (u)=χ (x)∧χ (v)=χ (y)∧PM(u,v)=PM(x,y)
G H G H G H
∧PM(u,u)=PM(x,x)∧PM(v,v)=PM(y,y)∧I[u=v]=I[x=y]
G H G H
∧{{(χ (u),χ (w),PM(u,w)):w ∈V }}={{(χ (x),χ (z),PM(x,z)):z ∈V }}
G G G G H H H G
∧{{(χ (v),χ (w),PM(v,w)):w ∈V }}={{(χ (y),χ (z),PM(y,z)):z ∈V }}
G G G G H H H G
∧{{(χ (w),PM(w,w)):w ∈V }}={{(χ (z),PM(z,z)):z ∈V }}
G G G H H G
∧{{(χ (w),χ (w′),PM(w,w′)):w,w′ ∈V }}={{(χ (z),χ (z′),PM(z,z′)):z,z′ ∈V }}
G G G G H H H G
=⇒ [T (χˆ)] (u,v)=[T (χˆ)] (x,y)∧[T (χˆ)] (u,u)=[T (χˆ)] (x,x)
SP G SP H SP G SP H
∧[T (χˆ)] (v,v)=[T (χˆ)] (y,y)∧[T (χˆ)] (v,u)=[T (χˆ)] (y,x)∧I[u=v]=I[x=y]
SP G SP H SP G SP H
∧{{[T (χˆ)] (u,w):w ∈V }}={{[T (χˆ)] (x,z):z ∈V }}
SP G G SP H H
∧{{[T (χˆ)] (v,w):w ∈V }}={{[T (χˆ)] (y,z):z ∈V }}
SP G G SP H H
∧{{[T (χˆ)] (w,u):w ∈V }}={{[T (χˆ)] (z,x):z ∈V }}
SP G G SP H H
∧{{[T (χˆ)] (w,v):w ∈V }}={{[T (χˆ)] (z,y):z ∈V }}
SP G G SP H H
∧{{[T (χˆ)] (w,w):w ∈V }}={{[T (χˆ)] (z,z):z ∈V }}
SP G G SP H H
∧{{[T (χˆ)] (w,w′):w,w′ ∈V }}={{[T (χˆ)] (z,z′):z,z′ ∈V }}
SP G G SP H H
=⇒ [(T ◦T )(χˆ)] (u,v)=[(T ◦T )(χˆ)] (x,y)
IGN SP G IGN SP H
whereinthesecondstepweusethedefinitionofT andalsoLemmaA.5andPropositionA.6;inthethirdstepweuse
EP,M
thedefinitionofT again;inthefourthstepweusetheinductionhypothesisandthefactthatPM(u,v)=PM(v,u)
EP,M G G
forallG∈G andu,v ∈V ;inthelaststepweusethedefinitionof2-IGN(Equation(54)).
G
We next prove that [(T ◦T )(χˆ)] (u,v) = [(T ◦T )(χˆ)] (x,y) =⇒ [(T ◦T )(χˆ)] (u,v) = [(T ◦
IGN SP G IGN SP H SP SIGN G SP
T )(χˆ)] (x,y). Thisisbecause
SIGN H
[(T ◦T )(χˆ)] (u,v)=[(T ◦T )(χˆ)] (x,y)
IGN SP G IGN SP H
=⇒ ΛM(G)=ΛM(H)∧[T (χˆ(λ,·,·))] (u,v)=[T (χˆ(λ,·,·))] (x,y) ∀λ∈ΛM(G)
IGN G IGN H
=⇒ [(T ◦T )(χˆ)] (u,v)=[(T ◦T )(χˆ)] (x,y),
SP SIGN G SP SIGN H
whereinthefirststepweusethefollowingobservations: (i)χˆ⪯χPM,whichimpliesthatχˆ (λ,u,v)=χˆ (µ,x,y) =⇒
G H
λ=µ;(ii)(T ◦T )(χˆ)⪯T ◦χPM ≡PM andthusΛM(G)=ΛM(H). Wethusconcludetheinductionstep.
IGN SP SP
LemmaA.26. Letχ0 ∈M betheconstantmapping. Then,(T ◦T∞ )(χ0)⪯(T ◦T ◦T ◦T∞ )(χPM).
1 GP EP,M GP P2 SP SIGN
Proof. BasedonLemmaA.25,itsufficestoprovethat(T ◦T∞ )(χ0)≡(T ◦T ◦T ◦T∞ )(χ0),where
GP EP,M GP P2 ×,PM EP,M
T is defined in Lemma A.25. Denote χ = T∞ (χ0). Let G,H ∈ G be any graphs such that [T (χ)](G) =
×,PM EP,M GP
[T (χ)](H). BydefinitionofT ,
GP GP
{{χ (u):u∈V }}={{χ (x):x∈V }}. (64)
G G H H
Sinceχ≡T ◦χ,
EP,M
{{(χ (u),{{(χ (v),PM(u,v)):v ∈V }}):u∈V }}={{(χ (x),{{(χ (y),PM(x,y)):y ∈V }}):x∈V }}.
G G G G G H H H H H
(65)
Equivalently,
{{{{(χ (u),χ (v),PM(u,v)):v ∈V }}:u∈V }}={{{{(χ (x),χ (y),PM(x,y)):y ∈V }}:x∈V }}. (66)
G G G G G H H H H H
Thisimpliesthat[(T ◦T ◦T )(χ)](G)=[(T ◦T ◦T )(χ)](H),concludingtheproof.
GP P2 ×,PM GP P2 ×,PM
CombiningPropositionA.22andLemmasA.23,A.24andA.26,weconcludetheproofofTheorem6.1.
24OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
A.6.ProofofPropositions6.3and6.5
ThissectionsaimstoprovePropositions6.3and6.5. WewillfirstgiveabriefintroductionofBasisNet. TheBasisNet
architectureiscomposedoftwoparts: aneigenspaceencoderΦ:Rm×n×n →Rn×dandatopgraphencoderρ:Rn×d →
Rd′. Here,weconsiderthestandardsettingwhereρisamessgage-passingGNNthattakesnodefeaturesasinputsand
outputsagraphrepresentationinvarianttonodepermutation. Weassumethattheexpressivepowerofρisboundedbythe
classic1-WLtest.
WenextdescribethedesignoftheeigenspaceencoderΦ. GivengraphmatrixM andgraphG,letM
=(cid:80)m
λ P be
G i=1 i i
theeigen-decompositionofM ,whereλ <···<λ areeigenvaluesofM . Foreacheigenspace,BasisNetprocesses
G 1 m G
theprojectionmatrixP usinga2-IGNIGN(di) :Rn×n →Rn,whered isthemultiplicityofeigenvalueλ . Theoutputof
i i i
Φisthendefinedas
Φ(G)=[IGN(d1)(P ),··· ,IGN(dm)(P ),0,··· ,0]∈Rn×d, (67)
1 m
where[]denotestheconcatenation. Whenthenumberofeigenspacesislessthantheoutputdimensiond,zero-paddingis
applied. NotethatBasisNetprocessesdifferentprojectionsusingdifferentIGNsiftheirmultiplicitiesdiffer.
ColorrefinementalgorithmsforSiameseIGNandBasisNet. SimilartoSpectralIGN,wecanwritethecorresponding
colorrefinementalgorithmsforthetwoGNNarchitectures. WefirstdefinetheinitialcolormappingχBasis,PM ∈MPM
2
asfollows: χBasis,PM (λ,u,v)=(d ,PM(u,v))forall(Guv,λ)∈GPM,wherePM istheprojectionmatrixassociated
G λ λ 2 λ
witheigenvalueλandd isthemultiplicityofeigenvalueλ. Here,weencodethemultiplicityinχBasis,PM becauseBasisNet
λ
usesdifferentIGNsfordifferenteigenvaluemultiplicities. Wethendefineseveralcolortransformations:
• SiameseIGNcolorrefinement. DefineT : MPM → MPM suchthatforanycolormappingχ ∈ MPM and
Siam 2 2 2
(Guv,λ)∈GPM,
2
[T (χ)] (λ,u,v)=[T (χ(λ,·,·))] (u,v), (68)
Siam G IGN G
whereT isdefinedinEquation(54).
IGN
• BasisNetpooling. DefineT :MPM →MPM suchthatforanycolormappingχ∈MPM and(Gu,λ)∈GPM,
BP 2 1 2 1
[T (χ)] (λ,u)=hash(χ (λ,u,u),{{χ (λ,u,v):v ∈V }},{{χ (λ,v,u):v ∈V }},
BP G G G G G G
(69)
{{χ (λ,v,v):v ∈V }},{{χ (λ,v,w):v,w ∈V }}).
G G G G
OnecanseethatEquation(69)has5aggregationsinsidethehashfunction,whichmatchesthenumberoforthogonal
basesinMaronetal.(2019b).
• Spectralpooling. DefineT :MPM →M suchthatforanycolormappingχ∈MPM andGu ∈G ,
SP1 1 1 1 1
[T (χ)] (u)=hash({{χ (λ,u):λ∈ΛM(G)}}). (70)
SP1 G G
Similarly,defineT :MPM →M suchthatforanycolormappingχ∈MPM andGuv ∈G ,
SP2 2 2 2 2
[T (χ)] (u,v)=hash({{χ (λ,u,v):λ∈ΛM(G)}}). (71)
SP2 G G
• Jointpooling. ThishasbeendefinedinEquation(58).
• Diagonalpooling. DefineT :M →M suchthatforanycolormappingχ∈M androotedgraphGu,
D 2 1 2
[T (χ)] (u)=χ (u,u). (72)
D G G
• 1-WLrefinement. ThishasbeendefinedinEquation(11).
• Globalpooling. ThishasbeendefinedinEquation(61).
Wearereadytodefinethecolormappingscorrespondingtothewholealgorithms:
• WeakSpectralIGN:thecolormappingisdefinedas(T ◦T ◦T∞ )(χPM).
JP SP2 Siam
• BasisNet: thecolormappingisdefinedas(T ◦T ◦T ◦T ◦T∞ )(χBasis,PM).
GP WL SP1 BP Siam
25OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
Similartothepreviousanalysis,wecanprovethattheabovetwocolormappingsupperboundtheexpressivepowerofthe
correspondingGNNmodels. Below,itsufficestoprovethefollowingkeylemma:
LemmaA.27. ForanygraphmatrixM,(T ◦T ◦T∞ )(χPM)⪯(T ◦T ◦T ◦T ◦T∞ )(χBasis,PM).
JP SP2 Siam GP WL SP1 BP Siam
Proof. Theproofwillbedecomposedintoaseriesofsteps. WefirstprovethatT∞ (χPM)⪯T∞ (χBasis,PM). Itsuffices
Siam Siam
toprovethatT (χPM)⪯χBasis,PM. PickanygraphsG,H ∈G,eigenvaluesλ∈ΛM(G),µ∈ΛM(H),andvertices
Siam
u,v ∈V ,x,y ∈V . Then,bydefinitionofT ,[T (χPM)] (λ,u,v)=[T (χPM)] (µ,x,y)impliesthat
G H Siam Siam G Siam H
χPM (λ,u,v)=χPM (µ,x,y)∧{{χPM
(λ,w,w):w ∈V
}}={{χPM
(µ,z,z):z ∈V }}. (73)
G H G G H H
Therefore,tr([PM] )=tr([PM] ),wheretr(·)denotesthematrixtrace. Notingthattr([PM] )isexactlythemultiplic-
λ G µ H λ G
ityofeigenvalueλforgraphmatrixM
,wehaveχBasis,PM (λ,u,v)=χBasis,PM
(µ,x,y).
G G H
WethenprovethatT ◦T ◦T∞ ⪯T ◦T ◦T ◦T ◦T∞ . Pickanyinitialcolormappingχ0 ∈MPM and
JP SP2 Siam GP WL D SP2 Siam 2
letχ=T∞ (χ0). Notethatχ≡T (χ). Wewillprovethat(T ◦T )(χ)⪯(T ◦T ◦T ◦T )(χ). Pickany
Siam Siam JP SP2 GP WL D SP2
graphsG,H ∈G. Wehave
[(T ◦T )(χ)](G)=[(T ◦T )(χ)](H)
JP SP2 JP SP2
=⇒ {{{{χ (λ,u,v):λ∈ΛM(G)}}:u,v ∈V }}={{{{χ (µ,x,y):µ∈ΛM(H)}}:x,y ∈V }}
G G H H
=⇒ {{({{χ (λ,u,v):λ∈ΛM(G)}},atp (u,v)):u,v ∈V }}
G G G
={{({{χ (µ,x,y):µ∈ΛM(H)}},atp (x,y)):x,y ∈V }}
H H H
=⇒ {{({{χ (λ,v,v):λ∈ΛM(G)}},atp (u,v)):u,v ∈V }}
G G G
={{({{χ (µ,y,y):µ∈ΛM(H)}},atp (x,y)):x,y ∈V }}
H H H
=⇒ [(T ◦T ◦T ◦T )(χ)](G)=[(T ◦T ◦T ◦T )(χ)](H),
GP WL D SP2 GP WL D SP2
where the second step is based on Lemma A.5, and the third step is based on the definition of T . This proves that
Siam
T ◦T ◦T∞ ⪯T ◦T ◦T ◦T ◦T∞ .
JP SP2 Siam GP WL D SP2 Siam
WenextprovethatT ◦T ◦T∞ ⪯T ◦T ◦T∞ . PickanygraphsG,H ∈G andverticesu∈V ,x∈V . We
D SP2 Siam SP1 BP Siam G H
have
[(T ◦T )(χ)](G)=[(T ◦T )(χ)](H)
D SP2 D SP2
=⇒ {{χ (λ,u,u):λ∈ΛM(G)}}={{χ (µ,x,x):µ∈ΛM(H)}}
G H
=⇒ {{{{χ (λ,u,v):v ∈V }}:λ∈ΛM(G)}}={{{{χ (µ,x,y):y ∈V }}:µ∈ΛM(H)}}
G G H H
=⇒ [(T ◦T )(χ)](G)=[(T ◦T )(χ)](H),
SP1 BP SP1 BP
wherethesecondstepisbasedonthedefinitionofT . ThisprovesthatT ◦T ◦T∞ ⪯T ◦T ◦T∞ .
Siam D SP2 Siam SP1 BP Siam
WeconcludetheproofbycombiningtheaboverelationswithPropositionsA.1andA.2.
WenextturntotheproofofProposition6.5, whichisalmostthesameasthecaseofBasisNet. Below, wewilldefine
theequivalentcolorrefinementalgorithmforSPE.TheinitialcolormappingassociatedwithSPEissimplyPM. Then,
the architecture refines PM by using color transformation T defined in Equation (54). The remaining procedure is
IGN
thesameasBasisNet. Combinedthesetogether,thecolormappingcorrespondingtothewholealgorithmcanbewritten
as(T ◦T∞ ◦T ◦T∞ )(PM). Then,itsufficestoprovethefollowingtwoequivalencerelations: (i)T∞ (PM) ≡
GP WL P2 IGN IGN
(T ◦T∞ )(χPM);(ii)T ◦T ◦T ◦T∞ ≡T ◦T ◦T∞ . Theproofprocedureisalomstthesameasin
SP2 SIGN WL P2 SP2 SIGN P2 SP2 SIGN
AppendicesA.5andA.6andweomitithere.
A.7.Discussionswithotherarchitectures
Graphormer(Yingetal.,2021),Graphormer-GD(Zhangetal.,2023b),andGraphiT(Mialonetal.,2021). Zhang
etal.(2023b)hasshownthattheexpressivepowerofthesearchitecturesisinherentlyboundedbyGD-WLwithdifferent
26OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
distances. Here,GraphormerusesSPD,Graphormer-GDusesbothSPDandRD,whilethedistanceusedinGraphiThasthe
formd (u,v)=(cid:80)m ϕ(λ )P (u,v),whereP istheprojectionmatrixassociatedwitheigenvalueλforgraphmatrixLˆ,
G i=1 i i i
andϕ:R→Risageneralfunction. Therefore,basedonTheorem5.1andtheproof,itisstraightforwardtoseethatall
thesearchitecturesareboundedbyEPWL.
PEG(Wangetal.,2022). GivengraphG,PEGmaintainsafeaturevectorh(l)(u) ∈ Rd foreachnodeu ∈ V ineach
G
layerl,andthefeatureisupdatedbythefollowingformula:
 
(cid:88)
h(l+1)(u)=ψ ϕ(∥z G(u)−z G(v)∥)Wh(l)(v), (74)
v∈NG(u)
whereϕ:R→Randψ :Rd →Rdarearbitraryfunctions,W ∈Rd×disaparameterizedweightmatrix,andz (u)∈Rk
G
isthepositionalencodingcorrespondingtothetopkeigenvectorsatnodeu. Here,weassumethatthenumberkischosen
suchthat∥z (u)−z (v)∥isuniqueforallgraphsGofinterest(i.e.,noambiguityproblem).
G G
WewillshowthattheexpressivepowerofPEGisboundedbyEPWL.Toobtainthisresult,notethat∥z (u)−z (v)∥2 =
G G
(cid:80)k (z (u))2+(z (v))2−2z (u)z (v). Since∥z (u)−z (v)∥isunique,thespanoftopkeigenvectorsmust
i=1 G,i G,i G,i G,i G G
beequivalenttothedirectsumoftheeigenspacescorrespondingtotopk′ eigenvaluesλ > λ > ··· > λ forsome
1 2 k′
k′ ≤ k. Itfollowsthat∥z (u)−z (v)∥2 = (cid:80)k′ P (u,u)+P (v,v)−2P (u,v)whereP istheprojectionontothe
G G i=1 i i i i
eigenspacecorrespondingtoeigenvalueλ . Therefore,theexpressivepowerofPEGisboundedbythecolorrefinement
i
algorithm(T ◦T∞ )(χ0)withχ0theinitialcolormappingandT thecolortransformationdefinedbelow:
GP PEG PEG
[T (χ)] (u)=hash(cid:0) {{(χ (v),PM(u,u),PM(v,v),PM(u,v)):v ∈V }}(cid:1) . (75)
PEG G G G G G G
Wewillprovethat(T ◦T∞ )(χ0)⪯(T ◦T∞ )(χ0). BasedonPropositionsA.1andA.2,itsufficestoprovethat
GP EP,M GP PEG
T∞ ⪯T ◦T∞ . Denoteχ=T∞ (χ0)andnotethatχ≡T∞ (χ). PickanygraphsG,H ∈G andvertices
EP,M PEG EP,M EP,M EP,M
u∈V ,x∈V . Wehave
G H
χ (u)=χ (x)
G H
=⇒ PM(u,u)=PM(x,x)∧{{(χ (v),PM(u,v)):v ∈V }}={{(χ (y),PM(x,y)):y ∈V }}
G H G G G H H H
=⇒ {{(χ (v),PM(u,u),PM(v,v),PM(u,v)):v ∈V }}={{(χ (y),PM(x,x),PM(y,y),PM(x,y)):y ∈V }}
G G G G G H H H H H
=⇒ [T (χ)] (u)=[T (χ)] (x),
PEG G PEG H
whereinthefirstandsecondstepsweusePropositionA.14. ThisconcludestheproofthattheexpressivepowerofPEGis
boundedbyEPWL.
GIRT(Maetal.,2023b). GivengraphG,GIRTmaintainsafeaturevectorforbothverticesandvertexpairs. Denoteby
h(l)(u)∈Rdthefeatureofnodeu∈V inlayerl,anddenotebyh(l)(u,v)∈Rd′ thefeatureofnodepair(u,v)∈V2 in
G G
layerl. Thefeaturesareupdatedbythefollowingformula:
(cid:16) (cid:16) (cid:17) (cid:17)
h(l+1)(u,v)=σ ρ (W h (u)+W h (v))⊙W h(l)(u,v) +W h(l)(u,v) , (76)
G Q G K G Ew G Eb G
α(l+1)(u,v)=Softmax (W h(l+1)(u,v)), (77)
G j∈VG A G
h(l+1)(u)= (cid:88) α(l+1)(u,v)·(W h(l)(v)+W h(l+1)(u,v)), (78)
G G V G Ev G
v∈VG
wheretheinitialfeatureisdefinedas
h(0)(u,v)=[(D−1A)0(u,v),(D−1A)1(u,v),··· ,(D−1A)K(u,v)],
G G G G
h(0)(u)=h(0)(u,u).
G G
OnecaneasilywritethecorrespondingcolorrefinementalgorithmthatupperboundsoftheexpressivepowerofGIRT.
Formally,itcanbeexpressedas(T ◦T ◦T∞ )(χGIRT),whereT isdefinedinEquation(72),theinitialcolormapping
GP D GIRT D
χGIRTissimplythemulti-dimensionalPageRankdistance,andT :M →M isthecolorrefinementdefinedbelow:
GIRT 2 2
(cid:26)
hash(χ (u,v),χ (u,u),χ (v,v)) ifu̸=v,
[T (χ)] (u,v)= G G G (79)
GIRT G hash(χ (u,u),{{(χ (u,v),χ (v,v)):v ∈V }} ifu=v.
G G G G
27OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
Wewillprovethat(T ◦T∞ )(χ0)⪯(T ◦T ◦T∞ )(χGIRT). DefinecolortransformationT :M →M
GP EP,M GP D GIRT ×,PM 1 2
suchthatforanycolormappingχ∈M androotedgraphGuv ∈G ,
1 2
[T (χ)] (u,v)=hash(χ (u),χ (v),PM(u,v),PM(u,u),PM(v,v)). (80)
×,PM G G G G G G
NotethatT ◦T∞ ≡T ◦T ◦T ◦T∞ byPropositionA.14. Also,T (χ0)⪯χGIRTduetoLemmaA.16.
GP EP,M GP D ×,PM EP,M ×,PM
Therefore,itsufficestoprovethatT ◦T∞ ⪯T∞ ◦T . BasedonPropositionA.2,itsufficestoprovethat
×,PM EP,M GIRT ×,PM
T ◦T ◦T∞ ≡T ◦T∞ . Denoteχ=T∞ (χ0),whereχ0 ∈M isanyinitialcolormapping. Pick
GIRT ×,PM EP,M ×,PM EP,M EP,M 1
anygraphsG,H ∈G andverticesu,v ∈V ,x,y ∈V suchthat[T (χ)] (u,v)=[T (χ)] (x,y). Wehave:
G H ×,PM G ×,PM H
• Ifu=v,thenx=y.
[T (χ)] (u,u)=[T (χ)] (x,x)
×,PM G ×,PM H
=⇒ χ (u)=χ (x),PM(u,u)=PM(x,x)
G H G H
=⇒ {{(χ (u),χ (v),PM(u,v)):v ∈V }}={{(χ (x),χ (y),PM(x,y)):y ∈V }}
G G G G H H H H
=⇒ {{(χ (u),PM(u,u),χ (v),PM(v,v),PM(u,v)):v ∈V }}
G G G G G G
={{(χ (x),PM(x,x),χ (y),PM(y,y),PM(x,y)):y ∈V }}
H H H H H H
=⇒ {{([T (χ)] (u,v),[T (χ)] (v,v)):v ∈V }}
×,PM G ×,PM G G
={{([T (χ)] (x,y),[T (χ)] (y,y)):y ∈V }}
×,PM H ×,PM H H
=⇒ [(T ◦T )(χ)] (u,u)=[(T ◦T )(χ)] (x,x).
GIRT ×,PM G GIRT ×,PM G
• Ifu̸=v,thenx̸=y.
[T (χ)] (u,v)=[T (χ)] (x,y)
×,PM G ×,PM H
=⇒ χ (u)=χ (x),χ (v)=χ (y),PM(u,v)=PM(x,y),PM(u,u)=PM(x,x),PM(v,v)=PM(y,y)
G H G H G H G H G H
=⇒ [(T ◦T )(χ)] (u,v)=[(T ◦T )(χ)] (x,y).
GIRT ×,PM G GIRT ×,PM G
Here,intheabovederivationsweusePropositionA.14. WehaveconcludedtheproofthattheexpressivepowerofGIRTis
boundedbyEPWL.
SpectralPPGNandSpectralk-IGN.BasedonMaronetal.(2019a),PPGNcanmimicthe2-FWLtest(Caietal.,1992),
andSpectralk-IGNcanmimicthek-WLtest(Grohe,2017). LetT : M → M andT : M → M be
WL(k) k k FWL(k) k k
the color refinements associated with k-WL and k-FWL, respectively, and let χ0 ∈ M be the initial color mapping
k k
in k-WL and k-FWL. The color refinement algorithms corresponding to k-WL and k-FWL can then be described as
(T ◦T∞ )(χ0) and (T ◦T∞ )(χ0), respectively, where T is defined in Equation (84). Maron et al.
JP(k) WL(k) k JP(k) FWL(k) k JP(k)
(2019a)provedthatwithsufficientlylayers,thefeaturesofvertexk-tuplescomputedbyk-IGNisfinerthanT∞ (χ0),
WL(k) k
andthefeaturesofvertexpairscomputedbyPPGNisfinerthanT∞ (χ0). Later,Azizianetal.(2021)provedthatthe
FWL(2) 2
featuresofvertexpairscomputedbyPPGNisalsoboundedby(andthusasfineas)T∞ (χ0)(seeLemma12intheir
FWL(2) 2
paper). Finally,Geerts&Reutter(2022)provedthatthefeaturesofvertexk-tuplescomputedbyk-IGNisboundedby(and
thusasfineas)T∞ (χ0)(seeLemmaE.1intheirpaper).
WL(k) k
WenowdefinethecolorrefinementalgorithmsforSpectralPPGNandSpectralk-IGN,whichareasexpressiveasthe
correspondingGNNarchitecturesbasedontheresultsofMaronetal.(2019a);Azizianetal.(2021);Geerts&Reutter
(2022). Firstdefinethefollowingcolortransformations:
• Spectralk-IGNcolorrefinement. DefineT :MPM →MPM suchthatforanycolormappingχ∈MPM
SIGN(k) k k k
and(Gu,λ)∈GPM,
k
[T (χ)] (λ,u)=hash([T (χ(λ,···))] (u),[T (T (χ))] (u)). (81)
SIGN(k) G WL(k) G WL(k) SP(k) G
• SpectralPPGNcolorrefinement. DefineT :MPM →MPM suchthatforanycolormappingχ∈MPM and
PPGN 2 2 2
(Gu,λ)∈GPM,
2
[T (χ)] (λ,u)=hash([T (χ(λ,···))] (u),[T (T (χ))] (u)). (82)
SPPGN G FWL(2) G FWL(2) SP(2) G
28OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
• Spectralpooling. DefineT :MPM →M suchthatforanycolormappingχ∈MPM andGu ∈G ,
SP(k) k k k k
[T (χ)] (u)=hash({{χ (λ,u):λ∈ΛM(G)}}). (83)
SP(k) G G
• Jointpooling. DefineT :M →M suchthatforanycolormappingχ∈M andG∈G,
JP(k) k 0 k
[T (χ)](G)=hash({{χ (u):u∈Vk}}). (84)
JP(k) G G
DefinetheinitialcolormappingχPM ∈MPM suchthatforanygraphG,verticesu∈Vk,andλ∈ΛM(G),
k k G
[χPM ] (λ,u)=hash(λ,[PM] (u ,u ),··· ,[PM] (u ,u ),··· ,[PM] (u ,u ),··· ,[PM] (u ,u )), (85)
k G λ G 1 1 λ G 1 k λ G k 1 λ G k k
where PM is the projection onto eigenspace associated with eigenvalue λ for graph matrix M. The color refinement
λ
algorithmforSpectralPPGNisthendefinedas(T ◦T ◦T∞ )(χPM). Similarly,thecolorrefinementalgorithm
JP(2) SP(2) SPPGN 2
forSpectralk-IGNisthendefinedas(T ◦T ◦T∞ )(χPM). Weaimtoprovethatfollowingtworesults:
JP(k) SP(k) SIGN(k) 2
PropositionA.28. (T ◦T ◦T∞ )(χPM)≡(T ◦T∞ )(χ0).
JP(2) SP(2) SPPGN 2 JP(2) FWL(2) 2
PropositionA.29. (T ◦T ◦T∞ )(χPM)≡(T ◦T∞ )(χ0).
JP(k) SP(k) SIGN(k) k JP(k) WL(k) k
WewillonlyprovePropositionA.28,astheproofofPropositionA.29isalmostthesame.
ProofofPropositionA.28. We first prove that (T ◦ T ◦ T∞ )(χPM) ⪯ (T ◦ T∞ )(χ0). Since
JP(2) SP(2) SPPGN 2 JP(2) FWL(2) 2
T (χPM) ⪯ χ0 (Lemma A.5), it suffices to prove that T ◦ T∞ ⪯ T∞ ◦ T . Based on Proposi-
SP(2) 2 2 SP(2) SPPGN FWL(2) SP(2)
tion A.2, it suffices to prove that T ◦ T ◦ T∞ ≡ T ◦ T∞ . Denote χ = T∞ (χˆ0), where
FWL(2) SP(2) SPPGN SP(2) SPPGN SPPGN
χˆ0 ∈ MPM is any initial color mapping. Pick any graphs G,H ∈ G and vertices u,v ∈ V , x,y ∈ V such that
2 G H
[T (χ)] (u,v)=[T (χ)] (x,y). Wehave:
SP(2) G SP(2) H
[T (χ)] (u,v)=[T (χ)] (x,y)
SP(2) G SP(2) H
=⇒ {{χ (λ,u,v):λ∈ΛM(G)}}={{χ (µ,x,y):µ∈ΛM(H)}}
G H
=⇒ {{[(T ◦T )(χ)] (u,v):λ∈ΛM(G)}}={{[(T ◦T )(χ)] (x,y):µ∈ΛM(H)}}
FWL(2) SP(2) G FWL(2) SP(2) H
=⇒ [(T ◦T )(χ)] (u,v)=[(T ◦T )(χ)] (x,y).
FWL(2) SP(2) G FWL(2) SP(2) H
whereinthesecondstepweusethedefinitionofT andthefactthatχ≡T (χ).
SPPGN SPPGN
Wenextprovethat(T ◦T∞ )(χ0)⪯(T ◦T ◦T∞ )(χPM). BasedonZhangetal.(2023a),wehave
JP(2) FWL(2) 2 JP(2) SP(2) SPPGN 2
T∞ ⪯T∞◦T andT∞ ≡T ◦T ◦T∞,whereT ,T ,T ,T aredefinedinAppendixA.3. Ontheother
FWL(2) PS NM PS Du Dv PS PS NM Du Dv
hand,wehaveprovedinCorollaryA.11that(T∞◦T )(χ0)⪯PM. Therefore,(T∞◦T )(χ0)⪯(T ◦T )(PM)⪯
PS NM 2 PS NM 2 Du Dv
T (χPM). ThisfinallyimpliesthatT∞ (χ0)⪯T (χPM).
SP(2) 2 FWL(2) 2 SP(2) 2
Itthussufficestoprovethat(T∞ ◦T )(χPM)⪯(T ◦T∞ )(χPM). Wewillprovethefollowingstronger
FWL(2) SP(2) 2 SP(2) SPPGN 2
result: foranyt ≥ 0,(T∞ ◦T )(χPM)) ⪯ (T ◦Tt )(χPM). Theproofisbasedoninduction. Forthe
FWL(2) SP(2) 2 SP(2) SPPGN 2
basecaseoft=0,theresultclearlyholds. Nowassumethattheresultholdsfort=t′andconsiderthecaseoft=t′+1.
Denoteχ=Tt (χPM). PickanygraphsG,H ∈G andverticesu,v ∈V ,x,y ∈V . Wehave
SPPGN 2 G H
[(T∞ ◦T )(χPM ))] (u,v)=[(T∞ ◦T )(χPM ))] (x,y)
FWL(2) SP(2) 2 G FWL(2) SP(2) 2 H
=⇒ [(T ◦T∞ ◦T )(χPM ))] (u,v)=[(T ◦T∞ ◦T )(χPM ))] (x,y)
FWL(2) FWL(2) SP(2) 2 G FWL(2) FWL(2) SP(2) 2 H
=⇒ [(T ◦T )(χ)] (u,v)=[(T ◦T )(χ)] (x,y)
FWL(2) SP(2) G FWL(2) SP(2) H
=⇒ ΛM(G)=ΛM(H)∧[(T (χ(λ,···))] (u,v)=[(T (χ(λ,···))] (x,y) ∀λ∈ΛM(G).
FWL(2) G FWL(2) H
Combining the last two steps implies that [(T ◦ T )(χ)] (u,v) = [(T ◦ T )(χ)] (x,y). We thus
SP(2) SPPGN G SP(2) SPPGN H
concludetheinductionstep.
29OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
A.8.Counterexamples
Inthissubsection,weaimtorevealtheexpressivitygapsbetweendifferentGNNmodels. Thisisachievedbyconstructinga
pairofcounterexamplegraphsG,H ∈G suchthatoneGNNcandistinguishwhiletheothercannot. Here,wewillleverage
animportanttheoreticaltooltoconstructcounterexamples,knownasFu¨rergraphsFu¨rer(2001). Anin-depthintroductionof
Fu¨rergraphscanbefoundinZhangetal.(2023a).
DefinitionA.30(Fu¨rergraphs). GivenanyconnectedgraphF =(V ,E ),theFu¨rergraphG(F)=(V ,E )is
F F G(F) G(F)
constructedasfollows:
V ={(x,X):x∈V ,X ⊂N (x),|X|mod2=0},
G(F) F F
E ={{(x,X),(y,Y)}⊂V :{x,y}∈E ,(x∈Y ↔y ∈X)}.
G(F) G F
Here,x∈Y ↔y ∈X holdswheneither(x∈Y andy ∈X)or(x∈/ Y andy ∈/ X)holds. Foreachx∈V ,denotethe
F
set
Meta (x):={(x,X):X ⊂N (x),|X|mod2=0}, (86)
F F
(cid:83)
whichiscalledthemetaverticesofG(F)associatedtox. NotethatV = Meta (x).
G(F) x∈VF F
Wenextdefineanoperationcalled“twist”:
DefinitionA.31(TwistedFu¨rergraphs). LetG(F)=(V ,E )betheFu¨rergraphofF =(V ,E ),andlet{x,y}∈
G(F) G(F) F F
E beanedgeofF. ThetwistedFu¨rergraphofG(F)foredge{x,y},isconstructedasfollows: twist(G(F),{x,y}):=
F
(V ,E ),where
G(F) twist(G(F),{x,y})
E :=E △{{ξ,η}:ξ ∈Meta (x),η ∈Meta (y)},
twist(G(F),{x,y}) G(F) F F
and△isthesymmetricdifferenceoperator,i.e.,A△B =(A\B)∪(B\A). ForanedgesetS ={e ,··· ,e }⊂E ,we
1 k F
furtherdefine
twist(G(F),S):=twist(···twist(G(F),e )··· ,e ). (87)
1 k
NotethatEquation(87)iswell-definedastheresultinggraphdoesnotdependontheorderofedgese ,··· ,e fortwisting.
1 k
Thefollowingresultiswell-known(seee.g., Zhangetal.,2023a,CorollaryI.5andLemmaI.7)):
Theorem A.32. For any graph F and any set S ,S ⊂ E , twist(G(F),S ) ≃ twist(G(F),S ) iff |S |mod2 =
1 2 F 1 2 1
|S |mod2.
2
Below,wewillprovePropositionA.33usingFu¨rergraphs. Notethatitcanbeeasilycheckedviaacomputerprogram
whetherapairofgraphscanbedistinguishedbyagivencolorrefinementalgorithm. However,anin-depthunderstandingof
whyagivencolorrefinementalgorithmcan/cannotdistinguishthese(twisted)Fu¨rergraphsisbeyondthescopeofthispaper
andisleftforfuturework.
PropositionA.33. Thefollowinghold:
a) ThereexistsapairofgraphsG,H suchthatSWLcannotdistinguishthembutSiameseIGN,SpectralIGN,andEPWL
withanygraphmatrixM ∈{A,L,Lˆ}candistinguishthem;
b) ForanyM ∈{A,L,Lˆ},thereexistsapairofgraphsG,H suchthatWeakSpectralIGNcannotdistinguishthembut
SpectralIGNcandistinguishthem;
c) ThereexistsapairofgraphsG,H suchthatWeakSpectralIGNcannotdistinguishthemwithanyM ∈{A,L,Lˆ},
butGD-WLwithanydistancelistedinSection5candistinguishthem;
d) ThereexistsapairofgraphsG,H suchthatSpectralIGNandEPWLwithanygraphmatrixM ∈{A,L,Lˆ}cannot
distinguishthem,butSWLcandistinguishthem;
e) ThereexistsapairofgraphsG,H suchthatSiameseIGNwithanygraphmatrixM ∈{A,L,Lˆ}cannotdistinguish
them,butWeakSpectralIGNwithanygraphmatrixM ∈{A,L,Lˆ}candistinguishthem.
Proof. ForPropositionA.33(a,b,c,d,e),thecounterexamplegraphsaretheFu¨rergraphandtwistedFu¨rergraphforbase
graphF definedinFigure3(a,b,c,d,e),respectively.
30OntheExpressivePowerofSpectralInvariantGraphNeuralNetworks
(a) (b) (c) (d) (e)
Figure3. IllustrationsofbasegraphsusedtoconstructFu¨rergraphandtwistedFu¨rergraphforprovingPropositionA.33.
31