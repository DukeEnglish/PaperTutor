The 3D-PC: a benchmark for visual perspective taking
in humans and machines
DrewLinsley†1, PeisenZhou†1, AlekhKarkada†1, AkashNagaraj1,
GauravGaonkar1,FrancisELewis1,ZygmuntPizlo2,ThomasSerre1
drew_linsley@brown.edu
Abstract
Visual perspective taking (VPT) is the ability to perceive and reason about the
perspectives of others. It is an essential feature of human intelligence, which
develops over the first decade of life and requires an ability to process the 3D
structureofvisualscenes. Agrowingnumberofreportshaveindicatedthatdeep
neuralnetworks(DNNs)becomecapableofanalyzing3Dscenesaftertraining
onlargeimagedatasets. Weinvestigatedifthisemergentabilityfor3Danalysis
inDNNsissufficientforVPTwiththe3Dperceptionchallenge(3D-PC):anovel
benchmarkfor3DperceptioninhumansandDNNs. The3D-PCiscomprisedof
three 3D-analysis tasks posed within natural scene images: 1. a simple test of
objectdepthorder,2. abasicVPTtask(VPT-basic),and3. anotherversionofVPT
(VPT-Strategy)designedtolimittheeffectivenessof“shortcut”visualstrategies.
Wetestedhumanparticipants(N=33)andlinearlyprobedortext-promptedover
300DNNsonthechallengeandfoundthatnearlyalloftheDNNsapproached
orexceededhumanaccuracyinanalyzingobjectdepthorder. Surprisingly,DNN
accuracy on this task correlated with their object recognition performance. In
contrast,therewasanextraordinarygapbetweenDNNsandhumansonVPT-basic.
Humanswerenearlyperfect,whereasmostDNNswerenearchance. Fine-tuning
DNNsonVPT-basicbroughtthemclosetohumanperformance,butthey,unlike
humans, dropped back to chance when tested on VPT-Strategy. Our challenge
demonstrates that the training routines and architectures of today’s DNNs are
well-suitedforlearningbasic3Dpropertiesofscenesandobjectsbutareill-suited
forreasoningaboutthesepropertiesashumansdo. Wereleaseour3D-PCdatasets
andcodetohelpbridgethisgapin3Dperceptionbetweenhumansandmachines.
1 Introduction
Inhistheoryofcognitivedevelopment,Piagetpositedthathumanchildrengaintheabilitytopredict
whichobjectsarevisiblefromanotherviewpointbeforetheageof10[1,2]. This“VisualPerspective
Taking”(VPT)abilityisafoundationalfeatureofhumanintelligenceandabehavioralmarkerforthe
theoryofmind[3]. VPTisalsocriticalforsafelynavigatingthroughtheworldandsocializingwith
others(Fig.1A).WhileVPThasbeenafocusofdevelopmentalpsychologyresearchsinceitsinitial
description[1,4,5](Fig.1B),ithasnotyetbeenstudiedinmachines.
Oneofthemoresurprisingresultsindeeplearninghasbeenthenumberofconcomitantsimilarities
tohumanperceptionexhibitedbydeepneuralnetworks(DNNs),trainedonlarge-scalestaticimage
datasets[8,9]. Forexample,DNNsnowrivalorsurpasshumanrecognitionperformanceonobject
†Theseauthorscontributedequallytothiswork.
1CarneyInstituteforBrainScience,BrownUniversity,Providence,RI.
2DepartmentofCognitiveSciences,UniversityofCalifornia-Irvine,Irvine,CA.
Preprint.Underreview.
4202
nuJ
6
]VC.sc[
1v83140.6042:viXraFigure 1: Visual Perspective Taking (VPT) is the ability to analyze scenes from different
viewpoints. (A)HumansrelyonVPTtoanticipatethebehaviorofothers. Weexpectthatthisability
willbeessentialforcreatingthenextgenerationofAIassistantsthatcanaccuratelyanticipatehuman
behavior(imagesareCCBY-NC).(B)VPThasbeenstudiedindevelopmentalpsychologysincethe
mid-20th centuryusingcartoonorhighlysyntheticstimuli. Forexample,Piaget’s“ThreeMountains
Task”asksobserverstodescribethescenefromtheperspectiveofabear(imagefrom [6]). (C)
Here,weuseGaussianSplatting[7]todevelopa3Dscenegenerationpipelineforthe3Dperception
challenge (3D-PC), to systematically compare 3D perception capabilities of human and machine
visionsystems. (D)The3D-PCtests1. Objectdepthperception,and2. VPT.
recognition and segmentation tasks [10–12], and are the state-of-the-art approach for predicting
humanneuralandbehavioralresponsestoimages[13]. Thereisalsoagrowingnumberofreports
indicatingthatDNNstrainedwithself-supervisionorforobjectclassificationlearntoencode3D
propertiesofobjectsandscenesthathumansarealsosensitiveto,suchasthedepthandstructureof
surfaces[14–23]. AretheemergentcapabilitiesofDNNsfor3DvisionsufficientforsolvingVPT
tasks?
Here,weintroducethe3Dperceptionchallenge(3D-PC)toaddressthisquestionandsystematically
compare3DperceptualcapabilitiesofhumansandDNNs. The3D-PCevaluatesobserverson(Fig1):
1. identifyingtheorderoftwoobjectsindepth(depthorder),2. predictingifoneoftwoobjects
can “see” the other (VPT-basic), and 3. another version of VPT that limits the effectiveness of
“shortcut”solutions[24](VPT-Strategy).The3D-PCisdistinctfromexistingpsychologicalparadigms
for evaluating VPT [1, 4, 5] and computer vision challenges for 3D perception [20, 23] in two
ways. First,unlikesmall-scalepsychologystudiesofVPT,the3D-PCusesanovel“3DGaussian
Splatting”[7]approachwhichpermitsthegenerationofendlessreal-worldstimuli. Second,unlike
existingcomputervisionchallenges,ourapproachfordatagenerationmeansthatthe3D-PCtestsand
counterbalanceslabelsformultiple3Dtasksontheexactsameimages,whichcontrolsforpotential
confoundsinanalysisandinterpretation. WeexpectthatDNNswhichrivalhumansonthe3D-PCwill
becomeidealmodelsforavarietyofreal-worldapplicationswheremachinesmustanticipatehuman
behaviorinreal-time,aswellasforenrichingourunderstandingofhowbrainswork(Fig.1A).
Contributions. Webuiltthe3D-PCandusedittoevaluate3Dperceptionforhumanparticipants
and327differentDNNs. TheDNNswetestedrepresentedeachoftoday’sleadingapproaches,from
VisualTransformers(ViT)[25]trainedonImageNet-21k[26]toChatGPT4[27]andStableDiffusion
2.0[28].
2• We found that DNNs were very accurate at determining the depth order of objects after linear
probing or text-prompting. DNNs that are state-of-the-art on object classification matched or
exceededhumanaccuracyonthistask.
• However,DNNsdroppedclosetochanceaccuracyonVPT-basic,whereashumanswerenearly
flawlessatthistask.
• Fine-tuning the zoo of DNNs on VPT-basic boosted their performance to near human level.
However,theperformanceoftheDNNs—butnothumans—droppedbacktochanceonVPT-
Strategy.
• OurfindingsdemonstratethatthevisualstrategiesnecessaryforsolvingVPTdonotemergein
DNNsfromlarge-scalestaticimagetrainingorafterdirectlyfine-tuningonthetask. Werelease
the 3D-PC data, code, and human psychophysics at https://github.com/serre-lab/VPT to
supportthedevelopmentofmodelsthatcanperceiveandreasonaboutthe3Dworldlikehumans.
2 Relatedwork
3Dperceptioninhumans. Thevisualperceptionof3Dpropertiesisafundamentallyill-posed
problem [29, 30], which forces biological visual systems to rely on a variety of assumptions to
decodethestructureofobjectsandscenes. Forexample,variationsinthelighting,texturegradients,
retinalimagedisparity,andmotionofanobjectallcontributetotheperceptionofits3Dshape. 3D
perceptionisfurthermodulatedbytop-downbeliefsaboutthestructureoftheworld,whichareeither
innateorshapedbypriorsensoryexperiences, especiallyvisualandhapticones. Inotherwords,
humans learn about the 3D structure of the world in an embodied manner that is fundamentally
differentthanhowDNNslearn. Inlightofthisdifference,itwouldberemarkableifDNNscould
accuratelymodelhowhumansperceivetheir3Dworld.
Visual perspective taking in humans. VPT was devised to understand how capabilities for
reasoningaboutobjectsintheworlddevelopthroughoutthecourseofone’slife. Atleasttwoversions
ofVPThavebeenintroducedovertheyears[31,32]. TheversionofVPTthatwestudyhere—
knowninthedevelopmentalliteratureas“VPT-1”—isthemorebasicform,whichisthoughtto
relyonautomaticfeedforwardprocessinginthevisualsystem[31]. Inlightofthewell-documented
similaritiesbetweenfeedforwardprocessinginhumansandDNNs[13,33],wereasonedthatthis
versionofVPTwouldmaximizethechancesofsuccessfortoday’sDNNs.
3DperceptioninDNNstrainedonstaticimages. Asdeepneuralnetworks(DNNs)haveincreased
inscaleandtrainingdatasetsizeoverthepastdecade,theirperformanceonessentiallyallvisual
challengeshasimproved. Surprisingly,this“scale-up”hasalsoledtotheemergenceof3Dperceptual
capabilities.Forexample,DNNstrainedwithavarietyofself-supervisedlearningtechniquesonstatic
imagedatasetslearntorepresentthedepth,surfacenormals,and3Dcorrespondenceoffeaturesin
scenes[15–23]. WhilesimilaritiesbetweenDNNsandhuman3Dperceptionhaveyettobeevaluated
systematically,ithasbeenshownthattherearedifferencesinhowthetworeasonaboutthe3Dshape
ofobjects[34]. The3D-PCcomplementspriorworkbysystematicallyevaluatingwhichaspectsof
human3Dperceptiontoday’sDNNscanandcannotaccuratelyrepresent.
LimitationsofDNNsasmodelsofhumanvisualperception. Overrecentyears, DNNshave
grownprogressivelymoreaccurateasmodelsofhumanvisionforobjectrecognitiontasks[10,24].
At the same time, these models which succeed as models of human object recognition struggle
to capture other aspects of visual perception [35] including contextual illusions [36], perceptual
grouping [37, 38], and categorical prototypes [39]. There are also multiple reports showing that
DNNsaregrowinglessalignedwiththevisualstrategiesofhumansandnon-humanprimatesasthey
improveoncomputervisionbenchmarks[40–42]. The3D-PCprovidesanotheraxisuponwhichthe
fieldcanevaluateDNNsasmodelsofhumanvision.
3 Methods
The3D-PC. Toenableafaircomparisonbetweenhumanobservers’andDNNs’3Dperceptual
capabilities,wedesignedthe3D-PCframeworkwithtwogoals: 1. posingdifferent3Dtasksonthe
3samesetofstimuli,and2. generatingalargenumberofstimulitoproperlytrainDNNsonthese
tasks. Weachievedthesegoalsbycombining3DGaussianSplatting[7],videosfromtheCommon
Objectsin3D(Co3D)[43]dataset,andUnity[44,45]intoaflexibledata-generatingframework.
Figure2: 3D-PCexamples. Wetested3DperceptioninimagesgeneratedbyGaussianSplatting.
Eachimagedepictsagreencameraandaredball. Theseobjectsareplacedinthesceneinawaythat
counterbalanceslabelsfordepthordertaskandVPT-basictasks.
Ourprocedureforbuildingthe3D-PCinvolvedthefollowingthreesteps. 1. WetrainedGaussian
Splatting models on videos in Co3D (Fig. 1C). 2. We imported these trained models into Unity,
whereweaddedgreencameraandredballobjectsintoeach3Dscene, whichwereusedtopose
visualtasks(Fig.1D).3. Wethengeneratedrandomviewpointtrajectorieswithineach3Dscene,
renderedimagesateachpositionalongthetrajectory,andderivedground-truthanswersfordepth
orderandVPTtasksforthegreencameraateverypositionfromUnity.
Our approach makes it possible to generate an unlimited number of visual stimuli that test an
observer’sabilitytosolvecomplementary3Dperceptiontasks(depthorderandVPT)whilekeeping
visualstatisticsconstantandgroundtruthlabelscounterbalancedacrosstasks. Fortheversionof
3D-PCusedinourevaluationandreleasedpubliclyathttps://github.com/serre-lab/VPT,the
depthorderandVPT-basictasksareposedonthesamesetof7,480trainingimagesof20objectsand
scenes,andasetof94testimagesof10separateobjectsandscenes(Fig.2). Weheldoutarandomly
selected10%ofthetrainingimagesforvalidationandmodelcheckpointselection.
To build the VPT-Strategy task, we rendered images where we fixed the scene camera while we
movedthegreencameraandredballobjectstopreciselychangetheline-of-sightbetweenthemfrom
unobstructedtoobstructedandback. Wereasonedthatthisexperimentwouldrevealifanobserver
adoptsthevisualstrategyoftakingtheperspectiveofthegreencamera,whichisthoughttobeused
byhumans[31],fromotherstrategiesthatreliedonlessrobustfeature-basedshortcuts. Thisdataset
4consistedofatestsetof100imagesfor10objectsandscenesthatwerenotincludedindepthorder
orVPT-basic.
Psychophysics experiment. We tested 10
participants on depth order, 20 on VPT-basic,
and 3 on VPT-Strategy. 33 participants
were recruited online from Prolific. All
provided informed consent before completing
the experiment and received $15.00/hr
compensationfortheirtime(thisamountedto
$5.00 for the 15–20 minutes the experiment
lasted). Thesedatawerede-identified.
Participantswereshowninstructionsforoneof
the 3D-PC tasks, then provided 20 examples
to ensure that they properly understood it
(Appendix Fig A.1). These examples were
drawn from the DNN training set. Each
Figure 3: Human accuracy for object depth
experimental trial consisted of the following
orderandVPT-basictasks. Barsnear50%are
sequence of events overlaid onto a white
label-permutednoisefloors;linesaregroupmeans.
background: 1. a fixation cross displayed for
Thedifferenceissignificant,***=p<0.001.
1000ms; 2. an image displayed for 3000ms,
during which time the participants were asked to render a decision. Participants pressed one of
theleftorrightarrowkeysontheirkeyboardstoprovidedecisions.
Images were displayed at 256×256 pixel resolution, which is equivalent to a stimulus between
5o−11o of visual angle across the range of display and seating setups we expected our online
participantsusedfortheexperiment.
Model zoo. We evaluated a wide range of DNNs on the 3D-PC, which represented the leading
approachesforobjectclassification,self-supervisedpretraining,imagegeneration,depthprediction,
andvisionlanguagemodeling(VLM).Ourzooincludes317DNNsfromPyTorchImageModels
(TIMM) [46], ranging from classic models like AlexNet [47] to state-of-the-art models like
EVA-02 [48] (see Appendix 1 for the complete list). We added foundational vision models like
MAE [49], DINO v2 [50], iBOT [51], SAM [52], and Midas [15] (obtained from the GitHub repo
of [23]). We also included Depth Anything [53], a foundational model 3D scene analysis and
depthprediction[23],aswellastheStable Diffusion 2.0[28]imagegenerationmodel. Finally,
weaddedstate-of-the-artlargevisionlanguagemodels(VLMs)ChatGPT4[27],Gemini[54],and
Claude 3[55]. Weevaluatedatotalof327modelsonthe3D-PC.
Modelevaluation. WeevaluatedallmodelsexceptfortheVLMsonthedepthorder andVPT-
basictasksinthischallengebytraininglinearprobesonimageembeddingsfromtheirpenultimate
layers. LinearprobesweretrainedusingPyTorch[56]for50epochs,a5e-4learningrate,andearly
stopping(seeAppendixA.5fordetails). Trainingtookapproximately20minutespermodelusing
NVIDIA-RTX 3090s. We tested the Stable Diffusion 2.0 model by adopting the evaluation
methodusedin[14](seeAppendixA.7fordetails). WeevaluatedtheVLMsbyprovidingthem
thesameinstructionsandtrainingimages(alongwithgroundtruthlabels)giventohumans,then
recordingtheirresponsestoimagesfromeachtaskviamodelAPIs.
Totestthelearnabilityofthe3D-PC,wealsofine-tunedeachoftheTIMMmodelsinourzootosolve
thetasks. Todothis,wetrainedeachofthesemodelsfor30epochs,a5e-5learningrate,andearly
stopping(seeAppendixA.5fordetails). Fine-tuningtookbetween3hoursand24hourspermodel
usingNVIDIA-RTX3090s.
4 Results
HumansfindVPTeasierthandeterminingthedepthorderingofobjects. Humanparticipants
wereonaverage74.73%accurateatdeterminingthedepthorderofobjects,and86.82%accurateat
solvingtheVPT-basictask(Fig.3; p<0.001forboth;statisticaltestingdonethroughrandomization
5Figure4: DNNperformanceonthedepthorderandVPT-basictasksinthe3D-PCafterlinear
probingorprompting. (A,B)DNNsaresignificantlymoreaccurateatdepthorderthanVPT-basic.
HumanconfidenceintervalsareS.E.M.and***: p<0.001. (C,D)DNNaccuracyfordepthorder
andVPT-basicstronglycorrelateswithobjectclassificationaccuracyonImageNet. Dashedlinesare
themeanoflabel-permutedhumannoisefloors.
tests[57]). HumanswerealsosignificantlymoreaccurateatsolvingVPT-basicthantheywereatthe
depthordertask.
DNNslearndepthbutnotVPTfromstaticimagetraining. DNNsshowedtheoppositepattern
ofresultsondepthorderandVPT-basictasksashumansafterlinearprobingorprompting(Fig.4):
15oftheDNNswetestedfellwithinthehumanaccuracyconfidenceintervalonthedepthordertask,
andthreeevenoutperformedhumans(Fig.4A).Incontrast,whilehumanswereonaverage86.82%
accurateatVPT-basic,theDNNwhichperformedthebestonthistask,theImageNet21K-trained
beit[58],was53.82%accurate. EvencommercialVLMsstruggledonVPT-basicandwerearound
chanceaccuracy(ChatGPT4: 52%,Gemini: 52%,andClaude 3: 50%). Thedepthordertaskwas
significantlyeasierforDNNsthanVPT-basic(p<0.001),whichistheoppositeofhumans(Fig.4B).
ImageNetaccuracycorrelateswiththe3DcapabilitiesofDNNs. Whatdrivesthedevelopment
of3DperceptioninDNNstrainedonstaticimages? WehypothesizedthatasDNNsscaleup,they
learn ancillary strategies for processing natural images, including the ability to analyze the 3D
structureofscenes. Toinvestigatethispossibility,wefocusedontheTIMMmodelsinourDNN
zoo. ThesemodelshavepreviouslybeenevaluatedforobjectclassificationaccuracyonImageNet,
whichweusedasastand-inforDNNscale[40–42]. Consistentwithourhypothesis,wefounda
strong and significant correlation between DNN performance on ImageNet and depth order task
accuracy(ρ=0.66,p<0.001,Fig.4C).DespitetheverylowaccuracyofDNNsonVPT-basic,there
wasalsoaweakerbutstillsignificantcorrelationbetweenperformanceonthistaskandImageNet
6Figure5: DNNperformanceonthedepthorderandVPT-basictasksinthe3D-PCafterfine-
tuning. (A)Fine-tuningmakesDNNsfarbetterthanhumansatthedepthordertaskandimproves
theperformanceofseveralDNNstobeatorbeyondhumanaccuracyonVPT-basic. (B)Evenafter
fine-tuning,thereisstillasignificantdifferenceinmodelperformanceondepthorderandVPT-basic
tasks, p<0.001. (C,D)DNNaccuracyonbothtasksafterfine-tuningcorrelateswithImageNet
objectclassificationaccuracy. HumanconfidenceintervalsareS.E.M.and***: p<0.001. Dashed
linesarethemeanoflabel-permutedhumannoisefloors.
(ρ=0.34,p<0.001,thedifferenceincorrelationsbetweenthetasksisρ=0.32,p<0.001;Fig.4D).
TheseresultssuggestthatmonoculardepthcuesdevelopinDNNsalongsidetheircapabilitiesfor
objectclassification1. However,thedepthcuesthatDNNslearnarepoorlysuitedforVPT.
DNNscansolveVPT-basicafterfine-tuning. Onepossibleexplanationforthefailureoftoday’s
DNNs on VPT-basic is that the task requires additional cues for 3D vision that cannot be easily
learnedfromstaticimages. Toexplorethispossibility,wefine-tunedeachoftheTIMMmodelsin
ourDNNzootosolvedepthorderandVPT-basic(Fig.5A).Therewasstillasignificantdifference
betweenDNNperformanceonthetwotasks(Fig.5B, p<0.001),butfine-tuningcaused97%ofthe
DNNstoexceedhumanaccuracyondepthorder,andfouroftheDNNstoreachhumanaccuracyon
VPT-basic. DNNperformanceonthetasksmorestronglycorrelatedwithImageNetaccuracyafter
fine-tuningthanlinearprobing(compareFig.5C/DandFig.4C/D).Wealsocomparedtheerrors
theseDNNsmadeonbothtaskstohumans. Wefoundnearlyallofthefine-tunedDNNswerealigned
withhumansondepthorder,andahandfulwerealignedwithhumansonVPT-basic(Fig.A.3).
DNNs learn different strategies than humans to solve VPT. The ability of DNNs to reach
human-levelperformanceonvisualtasksbyadoptingstrategiesthataredifferentfromhumanshas
1Moreworkisneededtoidentifyacausalrelationshipbetweenthedevelopmentofmonoculardepthcues
andobjectrecognitionaccuracy.
7Figure6:EvenDNNsfine-tunedonVPT-basicfailonVPT-Strategy. (A)Tobettercharacterizethe
strategyusedbyhumansandDNNstosolveVPT,wedevisedanewtest,VPT-Strategy,inwhichthe
greencameraandredballaremovedthroughascenewhileholdingthescenecameraandacentrally-
positionedobjectstill. Thistaskiseasilysolvableifanobserverestimatestheline-of-sightofthe
greencamera;otherstrategies,suchasthosethatrelyonspecificimagefeatures(featurebased),may
belesseffective. (B)ExamplesofVPT-Strategystimulialongwiththeground-truthlabel(top-row)
andpredictionsbyaViT largeafterlinearlyprobingorfine-tuningforVPT-basic(bottom-row).
DecisionattributionmapsfromeachversionoftheViT large,derivedfrom“smoothgradients”[59],
areoverlaidontobottom-rowimages(purple/blue=linearlyprobed,yellow/green=fine-tuned). The
fine-tuned ViT locates the green camera and red ball but renders incorrect decisions. (C) DNNs
fine-tuned on VPT-basic fail to solve VPT-Strategy; they rely on a brittle feature-based strategy.
Humans,ontheotherhand,are87%accurate;theylikelyestimateline-of-sight.
beenwell-documented[40–42]. Thus,wedevisedanewexperimenttounderstandifDNNslearn
tosolveVPTinthesamewayashumansdoafterfine-tuning. Indevelopmentalpsychology,ithas
beenproposedthathumansestimatetheline-of-sightofobjectsforVPTbecausetheyrespondin
predictablewaysafterthepositionsofobjectsinasceneareslightlyadjusted[31,32]. Inspiredby
thispsychologicalwork,wecreatedtheVPT-Strategytasktoevaluatethetypesofvisualstrategies
usedbyDNNsandhumanstosolveVPT(Fig.6A).
VPT-StrategyhasobserverssolvetheVPTtaskonaseriesofimagesrenderedfromafixedcamera
viewpointasthegreencameraandredballaremovedincrementallyfromonesideofthescreento
theother,passingbyanoccludingobjectintheprocess. Thismeansthatwecanpreciselymapout
themomentsatwhichthegreencamerahasaclearviewoftheredball,whenthatviewisoccluded,
andwhentheviewbecomesunoccludedoncemore. DNNsbehavedifferentlythanhumansonthis
task: humanswere87%accurate,butthehighestperformingDNN,theSwin Transformer[60]
trainedonImageNet-21k,wasonly66%accurate(Fig.6B,C).Inotherwords,whileDNNscanbe
8fine-tunedtoapproachhumanaccuracyonVPT-basic,thestrategytheylearnisbrittle,generalizes
poorly,andislikelyill-suitedforreasoningaboutthe3Dworld.
5 Discussion
Deepneuralnetworks(DNNs)haverapidlyadvancedoverrecentyearstothepointwheretheymatch
orsurpasshuman-levelperformanceonnumerousvisualtasks. However,our3D-PCrevealsthereis
stillasignificantgapbetweentheabilitiesofhumansandDNNstoreasonabout3Dscenes. While
DNNsmatchorexceedhumanaccuracyonthebasicobjectdepthorder taskafterlinearprobing
orprompting,theystruggleremarkablyoneventhebasicformofVPTthatwetestinthe3D-PC.
Fine-tuning DNNs on VPT-basic allows them to approach human-level performance, but unlike
humans,theirstrategiesdonotgeneralizetotheVPT-Strategytask.
A striking finding from our study is the strong correlation between DNNs’ object classification
accuracy on ImageNet and their performance on depth order and VPT-basic. This correlation
suggeststhatmonoculardepthcuesemergeinDNNsasabyproductoflearningtorecognizeobjects,
potentially because these cues are useful for segmenting objects from their backgrounds. The
differenceinDNNeffectivenessfordepthorderversusVPT-basic,however,indicatesthatthesecues
arenotsufficientforreasoningaboutthe3DstructureofscenesinthewaythatVPTdemands.
Thus,today’sapproachesfordevelopingDNNs,whichprimarilyfocusonstaticimagedatasets,may
bepoorlysuitedforenablingrobust3Dperceptionandreasoningabilitiesakintothoseofhumans.
Incorporating insights from human cognition and neuroscience into DNNs, particularly in ways
biologicalvisualsystemsdevelop3Dperception,couldhelpevolvemorefaithfulmodelsofhuman
intelligence.
AkeylimitationofourstudyisthatourversionofVPTrepresentsthemostbasicformstudiedinthe
developmentalpsychologyliterature. Whilesolvingthistaskisevidentlyanextraordinarychallenge
forDNNs,itisonlyonesmallsteptowardshuman-levelcapabilitiesforreasoningabout3Dworlds
ingeneral. Farmoreresearchisneededtoidentifyadditionalchallenges,architectures,andtraining
routinesthatcanhelpDNNsperceiveandreasonabouttheworldlikehumansdo. Wereleaseour
3D-PCdataandcodeathttps://github.com/serre-lab/VPTtosupportthisgoal.
6 Acknowledgements
FundingforthisprojectwasprovidedbytheOfficeofNavalResearch(N00014-19-1-2029)and
ANR-3IAArtificialandNaturalIntelligenceToulouseInstitute(ANR-19-PI3A0004). Additional
supportwasprovidedbytheCarneyInstituteforBrainScienceandtheCenterforComputationand
Visualization(CCV).WeacknowledgetheCloudTPUhardwareresourcesthatGooglemadeavailable
viatheTensorFlowResearchCloud(TFRC)programaswellascomputinghardwaresupportedby
NIHOfficeoftheDirectorgrantS10OD025181.
References
[1] JeanPiaget,BärbelInhelder,FrederickJohnLangdon,andJLLunzer. LaReprésentationde
L’espaceChezL’enfant.TheChild’sConceptionofSpace...Translated...byFJLangdon&
JLLunzer.WithIllustrations. NewYork;Routledge&KeganPaul: London;printedinGreat
Britain,1956.
[2] AndreaFrick,WenkeMöhring,andNoraSNewcombe. Picturingperspectives: development
ofperspective-takingabilitiesin4-to8-year-olds. Front.Psychol.,5:386,April2014.
[3] MarkusAichhorn,JosefPerner,MartinKronbichler,WolfgangStaffen,andGuntherLadurner.
Dovisualperspectivetasksneedtheoryofmind? Neuroimage,30(3):1059–1068,April2006.
[4] HenrykBukowski.Theneuralcorrelatesofvisualperspectivetaking:acriticalreview.Current
BehavioralNeuroscienceReports,5(3):189–197,September2018.
[5] Andrew K Martin, Garon Perceval, Islay Davies, Peter Su, Jasmine Huang, and Marcus
Meinzer. Visualperspectivetakinginyoungandolderadults. J.Exp.Psychol.Gen.,148(11):
2006–2026,November2019.
9[6] CatherineDBruce,BrentDavis,NathalieSinclair,LynnMcGarvey,DavidHallowell,Michelle
Drefs,KristaFrancis,ZacharyHawes,JoanMoss,JoanneMulligan,YukariOkamoto,Walter
Whiteley, and Geoff Woolcott. Understanding gaps in research networks: using “spatial
reasoning”asawindowintotheimportanceofnetworkededucationalresearch. Educational
StudiesinMathematics,95(2):143–161,June2017.
[7] BKerbl,GeorgiosKopanas,ThomasLeimkuehler,andGDrettakis. 3Dgaussiansplattingfor
real-timeradiancefieldrendering. ACMTrans.Graph.,42:1–14,July2023.
[8] DanielLKYaminsandJamesJDiCarlo.Usinggoal-drivendeeplearningmodelstounderstand
sensorycortex. Nat.Neurosci.,19(3):356–365,March2016.
[9] Daniel L K Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and
James J DiCarlo. Performance-optimized hierarchical models predict neural responses in
highervisualcortex. Proc.Natl.Acad.Sci.U.S.A.,111(23):8619–8624,June2014.
[10] RobertGeirhos, KantharajuNarayanappa, BenjaminMitzkus, TizianThieringer, Matthias
Bethge,FelixAWichmann,andWielandBrendel. Partialsuccessinclosingthegapbetween
humanandmachinevision. June2021.
[11] JeremyWLinsley*,DrewALinsley*,JoshLamstein,GennadiRyan,KevanShah,NicholasA
Castello,ViralOza,JaslinKalra,ShijieWang,ZacharyTokuno,AshkanJavaherian,Thomas
Serre, andStevenFinkbeiner. Superhumancelldeathdetectionwithbiomarker-optimized
neuralnetworks. SciAdv,7(50):eabf8142,December2021.
[12] KisukLee,JonathanZung,PeterLi,VirenJain,andHSebastianSeung. Superhumanaccuracy
ontheSNEMI3Dconnectomicschallenge. May2017.
[13] ThomasSerre. Deeplearning: Thegood,thebad,andtheugly. AnnuRevVisSci,5:399–426,
September2019.
[14] AlexanderCLi,MihirPrabhudesai,ShivamDuggal,EllisBrown,andDeepakPathak. Your
diffusionmodelissecretlyazero-shotclassifier.InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages2206–2217,2023.
[15] ReneRanftl,KatrinLasinger,DavidHafner,KonradSchindler,andVladlenKoltun. Towards
robustmonoculardepthestimation: Mixingdatasetsforzero-shotcross-datasettransfer. IEEE
Trans.PatternAnal.Mach.Intell.,44(3):1623–1637,March2022.
[16] SaurabhSaxena,JunhwaHur,CharlesHerrmann,DeqingSun,andDavidJFleet. Zero-Shot
metricdepthwithaField-of-Viewconditioneddiffusionmodel. December2023.
[17] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl
Vondrick. Zero-1-to-3: Zero-shotoneimageto3Dobject. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages9298–9309,2023.
[18] WalterGoodwin,SagarVaze,IoannisHavoutis,andIngmarPosner. Zero-ShotCategory-Level
objectposeestimation. InComputerVision–ECCV2022,pages516–532.SpringerNature
Switzerland,2022.
[19] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan.
Emergent correspondence from image diffusion. In A Oh, T Naumann, A Globerson,
K Saenko, M Hardt, and S Levine, editors, Advances in Neural Information Processing
Systems,volume36,pages1363–1389.CurranAssociates,Inc.,2023.
[20] ShirAmir,YossiGandelsman,ShaiBagon,andTaliDekel. DeepViTfeaturesasdensevisual
descriptors. arXivpreprintarXiv:2112.05814,2021.
[21] Yida Chen, Fernanda Viégas, and Martin Wattenberg. Beyond surface statistics: Scene
representationsinalatentdiffusionmodel. June2023.
[22] AnandBhattad,DanielMcKee,DerekHoiem,andDavidForsyth. StyleGANknowsnormal,
depth, albedo, and more. In A Oh, T Naumann, A Globerson, K Saenko, M Hardt, and
S Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages
73082–73103.CurranAssociates,Inc.,2023.
10[23] MohamedElBanani,AmitRaj,Kevis-KokitsiManinis,AbhishekKar,YuanzhenLi,Michael
Rubinstein,DeqingSun,LeonidasGuibas,JustinJohnson,andVarunJampani. Probingthe
3DAwarenessofVisualFoundationModels. InCVPR,2024.
[24] RobertGeirhos,Jörn-HenrikJacobsen,ClaudioMichaelis,RichardZemel,WielandBrendel,
MatthiasBethge,andFelixAWichmann. Shortcutlearningindeepneuralnetworks. Nature
MachineIntelligence,2(11):665–673,November2020.
[25] ADosovitskiy,LBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner, M Dehghani, Matthias Minderer, G Heigold, S Gelly, Jakob Uszkoreit, and
NHoulsby. Animageisworth16x16words: Transformersforimagerecognitionatscale.
ICLR,2021.
[26] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. ImageNet-21K
pretrainingforthemasses. April2021.
[27] Openai Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, Red Avila, Igor Babuschkin, S Balaji, Valerie Balcom, Paul Baltescu, Haiming
Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,
ChristopherBerner,LennyBogdonoff,OlegBoiko,MadelaineBoyd,Anna-LuisaBrakman,
GregBrockman,TimBrooks,MilesBrundage,KevinButton,TrevorCai,RosieCampbell,
AndrewCann,BrittanyCarey,ChelseaCarlson,RoryCarmichael,BrookeChan,CheChang,
Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, B Chess,
ChesterCho,CaseyChu,HyungWonChung,DaveCummings,JeremiahCurrier,YunxingDai,
CoryDecareaux,ThomasDegry,NoahDeutsch,DamienDeville,ArkaDhar,DavidDohan,
Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi,
LFedus,NikoFelix,Sim’onPosadaFishman,JustonForte,IsabellaFulford,LeoGao,Elie
Georges,CGibson,VikGoel,TarunGogineni,GabrielGoh,RaphaelGontijo-Lopes,Jonathan
Gordon, Morgan Grafstein, S Gray, Ryan Greene, Joshua Gross, S Gu, Yufei Guo, Chris
Hallacy,JesseHan,JeffHarris,YuchenHe,MikeHeaton,JohannesHeidecke,ChrisHesse,
AlanHickey,WadeHickey,PeterHoeschele,BrandonHoughton,KennyHsu,ShengliHu,
XinHu,JoostHuizinga,ShantanuJain,ShawnJain,JoanneJang,AngelaJiang,RogerJiang,
HaozhunJin,DennyJin,ShinoJomoto,BillieJonn,HeewooJun,TomerKaftan,LukaszKaiser,
AliKamali,IKanitscheider,NKeskar,TabarakKhan,LoganKilpatrick,JongWookKim,
ChristinaKim,YongjikKim,HendrikKirchner,JKiros,MatthewKnight,DanielKokotajlo,
LukaszKondraciuk,AKondrich,ArisKonstantinidis,KyleKosic,GretchenKrueger,Vishal
Kuo,MichaelLampe,IkaiLan,TeddyLee,JLeike,JadeLeung,DanielLevy,ChakMing
Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe,
PatriciaLue,AMakanju,KimMalfacini,SamManning,TodorMarkov,YanivMarkovski,
BiancaMartin,KatieMayer,AndrewMayne,BobMcGrew,SMcKinney,CMcLeavey,Paul
McMillan,JakeMcNeil,DavidMedina,AalokMehta,JacobMenick,LukeMetz,Andrey
Mishchenko,PamelaMishkin,VinnieMonaco,EvanMorikawa,DanielPMossing,TongMu,
MiraMurati,OMurk,DavidM’ely,AshvinNair,ReiichiroNakano,RajeevNayak,Arvind
Neelakantan, Richard Ngo, Hyeonwoo Noh, Ouyang Long, Cullen O’Keefe, J Pachocki,
AlexPaino,JoePalermo,AshleyPantuliano,GiambattistaParascandolo,JoelParish,Emy
Parparita,AlexandrePassos,MikhailPavlov,AndrewPeng,AdamPerelman,FilipedeAvila
BelbutePeres,MichaelPetrov,HenriquePondédeOliveiraPinto,MichaelPokorny,Michelle
Pokrass,VitchyrHPong,TollyPowell,AletheaPower,BorisPower,ElizabethProehl,Raul
Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
Rimbach, CarlRoss, BobRotsted, HenriRoussez, NickRyder, MSaltarelli, TedSanders,
ShibaniSanturkar,GirishSastry,HeatherSchmidt,DavidSchnurr,JohnSchulman,Daniel
Selsam,KylaSheppard,TSherbakov,JessicaShieh,SShoker,PranavShyam,SzymonSidor,
EricSigler,MaddieSimens,JordanSitkin,KatarinaSlama,IanSohl,BenjaminDSokolowsky,
YangSong,NatalieStaudacher, FSuch, NatalieSummers, ISutskever, JieTang, NTezak,
Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle,
NickTurley,JerryTworek,JuanFelipeCer’onUribe,AndreaVallone,ArunVijayvergiya,
Chelsea Voss, Carroll L Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan
Ward,JasonWei,CJWeinmann,AkilaWelihinda,PWelinder,JiayiWeng,LilianWeng,Matt
Wiethoff,DaveWillner,ClemensWinter,SamuelWolrich,HannahWong,LaurenWorkman,
11SherwinWu,JeffWu,MichaelWu,KaiXiao,TaoXu,SarahYoo,KevinYu,QimingYuan,
WojciechZaremba,RowanZellers,ChongZhang,MarvinZhang,ShengjiaZhao,Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. GPT-4 technical report. arXiv
preprintarXiv:2303.08774,March2023.
[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
High-Resolutionimagesynthesiswithlatentdiffusionmodels. December2021.
[29] JamesTTodd. Thevisualperceptionof3Dshape. TrendsCogn.Sci.,8(3):115–121,March
2004.
[30] ZygmuntPizlo. 3DShape: ItsUniquePlaceinVisualPerception. MitPr,January2010.
[31] Pascale Michelon and Jeffrey M Zacks. Two kinds of visual perspective taking. Percept.
Psychophys.,68(2):327–337,February2006.
[32] ZygmuntPizlo. ProblemSolving: CognitiveMechanismsandFormalModels. Cambridge
UniversityPress,neweditionedition,July2022.
[33] GabrielKreimanandThomasSerre. Beyondthefeedforwardsweep: feedbackcomputations
inthevisualcortex. Ann.N.Y.Acad.Sci.,1464(1):222–241,March2020.
[34] PengQianandTomerDUllman. Shapeguidesvisualpretense. January2024.
[35] Jeffrey S Bowers, Gaurav Malhotra, Marin Dujmovic´, Milton Llera Montero, Christian
Tsvetkov,ValerioBiscione,GuillermoPuebla,FedericoAdolfi,JohnEHummel,RachelF
Heaton,BenjaminDEvans,JeffreyMitchell,andRyanBlything. Deepproblemswithneural
networkmodelsofhumanvision. Behav.BrainSci.,pages1–74,December2022.
[36] DrewLinsley,JunkyungKim,AlekhAshok,andThomasSerre. Recurrentneuralcircuitsfor
contourdetection. InternationalConferenceonLearningRepresentations,2020.
[37] JunkyungKim*,DrewLinsley*,KalpitThakkar,andThomasSerre. Disentanglingneural
mechanismsforperceptualgrouping. InternationalConferenceonRepresentationLearning,
2020.
[38] Drew Linsley, Girik Malik, Junkyung Kim, Lakshmi Narasimhan Govindarajan, Ennio
Mingolla, and Thomas Serre. Tracking without re-recognition in humans and machines.
In M Ranzato, A Beygelzimer, Y Dauphin, P S Liang, and J Wortman Vaughan, editors,
AdvancesinNeuralInformationProcessingSystems,volume34,pages19473–19486.Curran
Associates,Inc.,2021.
[39] TalGolan,PrashantCRaju,andNikolausKriegeskorte. Controversialstimuli: Pittingneural
networksagainsteachotherasmodelsofhumancognition. Proc.Natl.Acad.Sci.U.S.A.,
117(47):29330–29337,November2020.
[40] Drew Linsley, Ivan F Rodriguez, Thomas Fel, Michael Arcaro, Saloni Sharma, Margaret
Livingstone,andThomasSerre. Performance-optimizeddeepneuralnetworksareevolving
intoworsemodelsofinferotemporalvisualcortex. Adv.NeuralInf.Process.Syst.,2023.
[41] Thomas Fel*, Ivan Felipe*, Drew Linsley*, and Thomas Serre. Harmonizing the object
recognitionstrategiesofdeepneuralnetworkswithhumans. Adv.NeuralInf.Process.Syst.,
2022.
[42] DrewLinsley,PinyuanFeng,ThibautBoissin,AlekhKarkadaAshok,ThomasFel,Stephanie
Olaiya,andThomasSerre. Adversarialalignment: Breakingthetrade-offbetweenthestrength
ofanattackanditsrelevancetohumanperception. June2023.
[43] JeremyReizenstein,RomanShapovalov,PhilippHenzler,LucaSbordone,PatrickLabatut,
andDavidNovotny. Commonobjectsin3d: Large-scalelearningandevaluationofreal-life
3Dcategoryreconstruction. In2021IEEE/CVFInternationalConferenceonComputerVision
(ICCV).IEEE,October2021.
12[44] ArthurJuliani,Vincent-PierreBerges,ErvinTeng,AndrewCohen,JonathanHarper,Chris
Elion, ChrisGoy, YuanGao, HunterHenry, MarwanMattar, andDannyLange. Unity: A
generalplatformforintelligentagents. September2018.
[45] Aras Pranckevicˇius. Unity gaussian splatting. https://github.com/aras-p/
UnityGaussianSplatting,2023.
[46] Ross Wightman. Pytorch image models. https://github.com/rwightman/
pytorch-image-models,2019.
[47] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. ImageNetclassificationwithdeep
convolutionalneuralnetworks. InFPereira,CJCBurges,LBottou,andKQWeinberger,
editors,AdvancesinNeuralInformationProcessingSystems25,pages1097–1105.Curran
Associates,Inc.,2012.
[48] YuxinFang,QuanSun,XinggangWang,TiejunHuang,XinlongWang,andYueCao. Eva-02:
Avisualrepresentationforneongenesis,2023.
[49] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick. Masked
autoencodersarescalablevisionlearners. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages16000–16009,2022.
[50] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,andOthers. Dinov2:
Learningrobustvisualfeatureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023.
[51] JinghaoZhou,ChenWei,HuiyuWang,WeiShen,CihangXie,AYuille,andTaoKong. iBOT:
ImageBERTPre-Trainingwithonlinetokenizer. ArXiv,abs/2111.07832,November2021.
[52] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,
TeteXiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,andOthers.Segmentanything.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages4015–
4026,2023.
[53] LiheYang,BingyiKang,ZilongHuang,XiaogangXu,JiashiFeng,andHengshuangZhao.
Depthanything:Unleashingthepoweroflarge-scaleunlabeleddata.arXivpreprintarXiv:2401.
10891,2024.
[54] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,Jiahui
Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, and Others. Gemini: a
familyofhighlycapablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[55] Anthropic. Claude,2024.
[56] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,
TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,AlbanDesmaison,Andreas
Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
BenoitSteiner,LuFang,JunjieBai,andSoumithChintala. PyTorch: Animperativestyle,
High-Performancedeeplearninglibrary. December2019.
[57] ESEdgington. RANDOMIZATIONTESTS. J.Psychol.,57:445–449,April1964.
[58] HangboBao, LiDong, SonghaoPiao, andFuruWei. BEiT:BERTPre-Trainingofimage
transformers. June2021.
[59] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg.
SmoothGrad: removingnoisebyaddingnoise. June2017.
[60] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBaining
Guo.Swintransformer:Hierarchicalvisiontransformerusingshiftedwindows.InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVision(ICCV),2021.
[61] AsherTrockmanandJ.ZicoKolter. Patchesareallyouneed?,2022.
13[62] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSaining
Xie. Aconvnetforthe2020s,2022.
[63] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely
connectedconvolutionalnetworks,2018.
[64] FisherYu,DequanWang,EvanShelhamer,andTrevorDarrell. Deeplayeraggregation,2019.
[65] YunpengChen,JiananLi,HuaxinXiao,XiaojieJin,ShuichengYan,andJiashiFeng. Dual
pathnetworks,2017.
[66] MingxingTanandQuocV.Le.Efficientnet:Rethinkingmodelscalingforconvolutionalneural
networks,2020.
[67] KaiHan,YunheWang,QiTian,JianyuanGuo,ChunjingXu,andChangXu. Ghostnet: More
featuresfromcheapoperations,2020.
[68] KeSun,YangZhao,BoruiJiang,TianhengCheng,BinXiao,DongLiu,YadongMu,Xinggang
Wang,WenyuLiu,andJingdongWang. High-resolutionrepresentationsforlabelingpixels
andregions,2019.
[69] ChengCui,TingquanGao,ShengyuWei,YuningDu,RuoyuGuo,ShuilongDong,BinLu,
YingZhou,XueyingLv,QiwenLiu,XiaoguangHu,DianhaiYu,andYanjunMa. Pp-lcnet: A
lightweightcpuconvolutionalneuralnetwork,2021.
[70] MingxingTanandQuocV.Le. Mixconv: Mixeddepthwiseconvolutionalkernels,2019.
[71] MingxingTan,BoChen,RuomingPang,VijayVasudevan,MarkSandler,AndrewHoward,
andQuocV.Le. Mnasnet: Platform-awareneuralarchitecturesearchformobile,2019.
[72] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan,
WeijunWang,YukunZhu,RuomingPang,VijayVasudevan,QuocV.Le,andHartwigAdam.
Searchingformobilenetv3. CoRR,abs/1905.02244,2019. URLhttp://arxiv.org/abs/
1905.02244.
[73] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár.
Designingnetworkdesignspaces,2020.
[74] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and
Philip Torr. Res2net: A new multi-scale backbone architecture. IEEE Transactions on
PatternAnalysisandMachineIntelligence,43(2):652–662,Feb2021. ISSN1939-3539. doi:
10.1109/tpami.2019.2938758. URLhttp://dx.doi.org/10.1109/TPAMI.2019.2938758.
[75] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition. CoRR,abs/1512.03385,2015. URLhttp://arxiv.org/abs/1512.03385.
[76] HangZhang,ChongruoWu,ZhongyueZhang,YiZhu,HaibinLin,ZhiZhang,YueSun,Tong
He, Jonas Mueller, R. Manmatha, Mu Li, and Alexander Smola. Resnest: Split-attention
networks,2020.
[77] DongyoonHan,SangdooYun,ByeonghoHeo,andYoungJoonYoo. Rexnet: Diminishing
representationalbottleneckonconvolutionalneuralnetwork,2020.
[78] Saining Xie, Ross B. Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated
residual transformations for deep neural networks. CoRR, abs/1611.05431, 2016. URL
http://arxiv.org/abs/1611.05431.
[79] DimitriosStamoulis,RuizhouDing,DiWang,DimitriosLymberopoulos,BodhiPriyantha,Jie
Liu,andDianaMarculescu. Single-pathnas: Designinghardware-efficientconvnetsinless
than4hours,2019.
[80] KaiHan, YunheWang, QiulinZhang, WeiZhang, ChunjingXu, andTongZhang. Model
rubik’scube:Twistingresolution,depthandwidthfortinynets.AdvancesinNeuralInformation
ProcessingSystems,33:19353–19364,2020.
14[81] KarenSimonyanandAndrewZisserman. Verydeepconvolutionalnetworksforlarge-scale
imagerecognition. CoRR,abs/1409.1556,2014.
[82] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image
transformers. arXivpreprintarXiv:2106.08254,2021.
[83] WeihaoYu,ChenyangSi,PanZhou,MiLuo,YichenZhou,JiashiFeng,ShuichengYan,and
XinchaoWang. Metaformerbaselinesforvision. arXivpreprintarXiv:2210.13452,2022.
[84] HugoTouvron,MatthieuCord,AlexandreSablayrolles,GabrielSynnaeve,andHerv’eJ’egou.
Going deeper with image transformers. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision(ICCV),pages32–42,October2021.
[85] St’ephaned’Ascoli,HugoTouvron,MatthewLeavitt,AriMorcos,GiulioBiroli,andLevent
Sagun. Convit: Improvingvisiontransformerswithsoftconvolutionalinductivebiases. arXiv
preprintarXiv:2103.10697,2021.
[86] Chun-Fu (Richard) Chen, Quanfu Fan, and Rameswar Panda. CrossViT: Cross-Attention
Multi-Scale Vision Transformer for Image Classification. In International Conference on
ComputerVision(ICCV),2021.
[87] MingyuDing,BinXiao,NoelCodella,PingLuo,JingdongWang,andLuYuan. Davit: Dual
attentionvisiontransformer. InECCV,2022.
[88] HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,and
HerveJegou. Trainingdata-efficientimagetransformers&distillationthroughattention. In
InternationalConferenceonMachineLearning,volume139,pages10347–10357,July2021.
[89] YanyuLi, JuHu, YangWen, GeorgiosEvangelidis, KamyarSalahi, YanzhiWang, Sergey
Tulyakov,andJianRen. Rethinkingvisiontransformersformobilenetsizeandspeed. arXiv
preprintarXiv:2212.08059,2022.
[90] JianweiYang,ChunyuanLi,XiyangDai,andJianfengGao. Focalmodulationnetworks,2022.
[91] BenjaminGraham,AlaaeldinEl-Nouby,HugoTouvron,PierreStock,ArmandJoulin,Herve
Jegou, and Matthijs Douze. Levit: A vision transformer in convnet’s clothing for faster
inference. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision
(ICCV),pages12259–12269,October2021.
[92] ZhengzhongTu,HosseinTalebi,HanZhang,FengYang,PeymanMilanfar,AlanBovik,and
YinxiaoLi. Maxvit: Multi-axisvisiontransformer. ECCV,2022.
[93] Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose, and
mobile-friendlyvisiontransformer. InInternationalConferenceonLearningRepresentations,
2022.
[94] YanghaoLi,Chao-YuanWu,HaoqiFan,KarttikeyaMangalam,BoXiong,JitendraMalik,and
ChristophFeichtenhofer. Mvitv2: Improvedmultiscalevisiontransformersforclassification
anddetection,2022.
[95] ByeonghoHeo,SangdooYun,DongyoonHan,SanghyukChun,JunsukChoe,andSeongJoon
Oh. Rethinkingspatialdimensionsofvisiontransformers. InInternationalConferenceon
ComputerVision(ICCV),2021.
[96] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu,
Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer.
ComputationalVisualMedia,8(3):1–10,2022.
[97] XiangxiangChu,ZhiTian,YuqingWang,BoZhang,HaibingRen,XiaolinWei,HuaxiaXia,
andChunhuaShen. Twins: Revisitingthedesignofspatialattentioninvisiontransformers. In
NeurIPS2021,2021. URLhttps://openreview.net/forum?id=5kTlVBkzSRx.
15[98] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,
ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,
JakobUszkoreit,andNeilHoulsby. Animageisworth16x16words: Transformersforimage
recognitionatscale. ICLR,2021.
[99] LiYuan,QibinHou,ZihangJiang,JiashiFeng,andShuichengYan. Volo:Visionoutlookerfor
visualrecognition. IEEETransactionsonPatternAnalysisandMachineIntelligence,2022.
[100] Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze,
ArmandJoulin,IvanLaptev,NataliaNeverova,GabrielSynnaeve,JakobVerbeek,etal. Xcit:
Cross-covarianceimagetransformers. arXivpreprintarXiv:2106.09681,2021.
[101] WeihaoYu,MiLuo,PanZhou,ChenyangSi,YichenZhou,XinchaoWang,JiashiFeng,and
Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition, pages10819–10829,
2022.
[102] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image
transformers. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision
(ICCV),pages9981–9990,October2021.
[103] ZihangDai,HanxiaoLiu,QuocVLe,andMingxingTan. Coatnet: Marryingconvolutionand
attentionforalldatasizes. arXivpreprintarXiv:2106.04803,2021.
[104] Muhammad Maaz, Abdelrahman Shaker, Hisham Cholakkal, Salman Khan, Syed Waqas
Zamir,RaoMuhammadAnwer,andFahadShahbazKhan. Edgenext: Efficientlyamalgamated
cnn-transformerarchitectureformobilevisionapplications. InInternationalWorkshopon
ComputationalAspectsofDeepLearningat17thEuropeanConferenceonComputerVision
(CADL2022).Springer,2022.
[105] ZhengsuChen,LingxiXie,JianweiNiu,XuefengLiu,LonghuiWei,andQiTian. Visformer:
Thevision-friendlytransformer. InProceedingsoftheIEEE/CVFinternationalconferenceon
computervision,pages589–598,2021.
[106] RobertGeirhos,KMeding,andFelixWichmann. Beyondaccuracy: quantifyingtrial-by-trial
behaviourofCNNsandhumansbymeasuringerrorconsistency. NeurIPS,2020.
16Checklist
1. Forallauthors...
(a) Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthepaper’s
contributionsandscope? [Yes]
(b) Didyoudescribethelimitationsofyourwork? [Yes]Inthediscussion.
(c) Didyoudiscussanypotentialnegativesocietalimpactsofyourwork? [Yes]Appendix
sectionA.3.
(d) Haveyoureadtheethicsreviewguidelinesandensuredthatyourpaperconformsto
them? [Yes]
2. Ifyouareincludingtheoreticalresults...
(a) Didyoustatethefullsetofassumptionsofalltheoreticalresults? [N/A]
(b) Didyouincludecompleteproofsofalltheoreticalresults? [N/A]
3. Ifyouranexperiments(e.g. forbenchmarks)...
(a) Did you include the code, data, and instructions needed to reproduce the main
experimental results (either in the supplemental material or as a URL)? [Yes] See
methods.
(b) Didyouspecifyallthetrainingdetails(e.g.,datasplits,hyperparameters,howthey
werechosen)? [Yes]AppendixsectionA.5
(c) Did you report error bars (e.g., with respect to the random seed after running
experimentsmultipletimes)? [Yes]Wereporterrorbarsoverhumanperformanceinall
figures. Wealsoreportmodel/errorbarsinperformanceandcorrelationwithhumans
(Appendixfig.A.3).
(d) Didyouincludethetotalamountofcomputeandthetypeofresourcesused(e.g.,type
ofGPUs,internalcluster,orcloudprovider)? [Yes]AppendixsectionA.5.
4. Ifyouareusingexistingassets(e.g.,code,data,models)orcurating/releasingnewassets...
(a) Ifyourworkusesexistingassets,didyoucitethecreators? [Yes]Weusedexisting
modelsandlibrariesandcitedeach.
(b) Didyoumentionthelicenseoftheassets? [Yes]SeeAppendixsectionA.1.
(c) DidyouincludeanynewassetseitherinthesupplementalmaterialorasaURL?[Yes]
(d) Didyoudiscusswhetherandhowconsentwasobtainedfrompeoplewhosedatayou’re
using/curating? [Yes]Seemethods.
(e) Didyoudiscusswhetherthedatayouareusing/curatingcontainspersonallyidentifiable
informationoroffensivecontent? [Yes]Seemethods.
5. Ifyouusedcrowdsourcingorconductedresearchwithhumansubjects...
(a) Didyouincludethefulltextofinstructionsgiventoparticipantsandscreenshots,if
applicable? [Yes]SeeappendixsectionA.8
(b) Did you describe any potential participant risks, with links to Institutional Review
Board(IRB)approvals,ifapplicable? [N/A]
(c) Didyouincludetheestimatedhourlywagepaidtoparticipantsandthetotalamount
spentonparticipantcompensation? [Yes]Seemethods.
A Appendix
A.1 AuthorStatement
As authors of this dataset, we bear all responsibility for the information collected and in case of
violationofrightsandotherethicalstandards. WeaffirmthatourdatasetissharedunderaCreative
CommonsCC-BYlicense.
A.2 DataAccess
We release benchmarking code and data download instructions at https://github.com/
serre-lab/VPT.
17A.3 Potentialnegativesocietalimpactsofthiswork
Themostobviouspotentialnegativeimpactofourworkisthatadvancingvisualperspectivetaking
(VPT)capabilitiesinartificialagentscouldpotentiallyenablemilitaristicapplicationsorsurveillance
overreach. However,wehopethatourbenchmarkwillaidinthedevelopmentofAI-basedassistants
thatcanbetteranticipateandreacttohumanneedsandsocialcuesforsafernavigationandinteraction.
Wealsobelievethatourbenchmarkwillguidethedevelopmentofbettercomputationalmodelsof
human3Dperceptionaswellastheneuralunderpinningsoftheseabilities.
A.4 DataGeneration
Togeneratedataforthe3D-PC,wefirsttrained3DGaussianSplatting[7]modelsonvideosfrom
theCommonObjectsin3D(Co3D)[43],whichyielded3Drepresentationsofeachscene. Wethen
importedtrainedmodelsintoUnity[44]usingUnityGaussianSplatting[45]andadded3Dmodels
ofthegreencameraandredballtoeach. Finally,werendered50imagesalongasmoothviewpoint
cameratrajectorysampledneartheoriginaltrajectoryusedfortrainingtheGaussianSplattingmodel.
Foreach3Dscene,wecreated5positiveand5negativesettingsforVPT.
TogenerateVPT-basic,thegenerationprocesswasrepeatedfor30Co3Dvideosfrom10different
categories. Weremovedanyimageswherethegreencameraandredballwerenotvisible. Wethen
splittheimagesintoatrainingsetof7480imagesfrom20scenesandatestingsetof94imagesfrom
10otherscenes. Forthedepthordertask,weusedthesamedatasplitsbutremovedanyambiguous
sampleswheretheobjectsweresimilarlyclosetothecamera. Theresultingdatasetforthedepth
ordertaskcontains4787trainingimagesand94testingimages. Thesamesetoftestingimagesis
usedforbothmodelandhumanbenchmarks.
ForVPT-Strategy,weusedthesameprocesstogeneratedatafrom10additionalCo3Dscenesnot
includedinVPT-basicandadditionallycontrolledthepositionsofthegreencameraandtheredball.
Theanglebetweenthesetwoobjectswasheldconstantwhilewemovedthemsothattheirlineof
sightwasunobstructed,obstructed,andthenunobstructedonceagain. ForeachCo3Dscene,we
rendered10settingsfromafixedviewpointcameraposition, resultingin100imagesintotalfor
VPT-Strategy.
A.5 ModelZoo
We linearly probed 317 DNNs from Pytorch Image Models (TIMM) [46] (Table 1) along with
foundationalvisionmodelsfollowingtheproceduresin [23]. AllDNNsweretrainedandevaluated
withNVIDIA-RTX3090GPUsfromtheBrownUniversityCenterforComputation&Visualization.
Alllinearprobesweretrainedfor50epochs,witha5e−4learningrate,a1e−4weightdecay,a0.3
dropoutrate,andabatchsizeof128. Wefine-tunedeachoftheTIMMmodelsfor30epochs,a5e−5
learning rate, 1e−4 weight decay, 0.7 dropout rate, and a batch size of 16. Linear probing took
approximately20minutespermodel,andfine-tuningvariedfrom3to24hoursonaNVIDIA-RTX
3090GPU.
A.6 VLMEvaluation
We evaluated the following proprietary VLMs on the VPT-basic and depth order tasks: GPT-
4 (gpt-4-turbo), Claude (claude-3-opus-20240229), and Gemini (gemini-pro-vision). To
evaluatetheseVLMs,weusedtheirAPIstosendqueriescontaining20trainingimages,withground
truthanswersascontext,aswellasatestimage. Theprepended20trainingimagesmeantthatfor
everyexampleinthechallenge,VLMsweregiventheopportunitytolearn, “in-context”,howto
solvethegiventask.
Thepromptweusedforthedepthtaskwas“Inthisimage,istheredballclosertotheobserveroris
thegreenarrowclosertotheobserver? AnsweronlyBALLiftheredballiscloser,orARROWifthe
greenarrowiscloser,nothingelse.” andthepromptfortheVPT-basictaskwas“Inthisimage,if
viewedfromtheperspectiveofthegreen3Darrowinthedirectionthearrowispointing,canahuman
seetheredball? AnsweronlyYESorNO,nothingelse”. Weevaluatedeachmodel’sgenerated
responsesacrossmultipletemperatures,rangingfrom0.0to0.7inincrementsof0.1,andwereport
the average of the best 3 runs. Note that while this evaluation approach gives the VLMs more
18opportunitiestoperformwellonourbenchmarkthanothermodels,theystillstruggledimmensely
(seemaintext).
A.7 StableDiffusionEvaluation
We followed the method of Li et al. [14] to evaluate Stable Diffusion 2.0 on the 3D-PC.
This involved trying multiple prompts to optimize the zero-shot classification performance
of the Stable Diffusion 2.0 model, on VPT-basic and depth order tasks. For VPT-basic
we found that the prompt "A photo with red ball is visible from the green arrow’s
perspective"forpositiveclassand"A photo with red ball not visible from the green
arrow’s perspective" for the negative class led to the best performance. For the depth order
task,thepromptwiththehighestperformancewas"A photo with green arrow closer to the
camera as compared to red ball"and"A photo with red ball closer to the camera
as compared to green arrow"forpositiveandnegativeclassesrespectively.
A.8 HumanBenchmark
Werecruited30participantsthroughProlific,compensatingeachwith$5uponsuccessfulcompletion
ofalltesttrials. Participantsconfirmedtheircompletionbypastingauniquesystem-generatedcode
into their Prolific accounts. The compensation was prorated based on the minimum wage. We
alsoincurreda30%overheadfeeperparticipantpaidtoProlific. Intotal,wespent$195onthese
benchmarkexperiments.
A.8.1 Experimentdesign
Attheoutsetoftheexperiment,weacquiredparticipantconsentthroughaformapprovedbythe
BrownUniversity’sInstitutionalReviewBoard(IRB).Theexperimentwasperformedonacomputer
usingtheChromebrowser. Followingconsent,wepresentedademonstrationwithinstructionsandan
examplevideo.Participantshadtheoptiontorevisittheinstructionsatanytimeduringtheexperiment
byclickingalinkinthetoprightcornerofthenavigationbar.
FigureA.1: Anexperimenttrial.
Inthedepthorder task,theparticipantswereaskedtoclassifytheimageas“positive”(thegreen
arrow in closer to the viewer) or “negative” (the red ball is closer) using the right and left arrow
19FigureA.2: Theconsentscreen.
keysrespectively. Thechoiceforkeysandtheircorrespondinginstanceswerementionedbelowthe
imageoneveryscreen(SeeAppendixFig. A1. Participantsweregivenfeedbackontheirresponse
(correct/incorrect)duringeverypracticetrial, butnotduringthetesttrials. IntheVPTtasks, the
choiceswere“thegreenarrow/cameraseetheredball”or“thegreenarrow/cameracannotseethe
redball”.
The experiment was not time-bound, allowing participants to complete it at their own pace.
Participants typically took around 20 minutes. After each trial, participants were redirected to
ascreenconfirmingthesuccessfulsubmissionoftheirresponses. Theycouldstartthenexttrialby
clickingthe“Continue”buttonorpressingthespacebar. Iftheydidnottakeanyaction,theywere
automaticallyredirectedtothenexttrialafter1000milliseconds. Additionally,participantswere
showna“restscreen”withaprogressbarafterevery40trials,wheretheycouldtakeadditionaland
longerbreaksifneeded. Thetimerwasturnedoffduringtherestscreen.
20A.9 Humanvs. DNNdecisionmakingonVPT-basic
WecomparedthedecisionstrategiesofhumansandDNNsonVPT-basicbymeasuringthecorrelations
between their error patterns with Cohen’s κ [106]. Model κ scores were mostly correlated with
accuracy on VPT-basic after linear probes and fine-tuning (Fig. A.3). However, while nearly all
DNNswerehighlycorrelatedwithhumanerrorpatternsafterfine-tuning,thecorrelationbetweenκ
scoresandtaskaccuracydisappeared(Fig.A.3B,purpledots).
FigureA.3: Errorpatterncorrelations(Cohen’sκ)betweenhumansandDNNsonVPT-basic
A.10 Datasheetfordatasets
Motivation
Forwhatpurposewasthedatasetcreated? Wasthereaspecifictaskinmind? Wasthereaspecific
gapthatneededtobefilled? Pleaseprovideadescription.
The dataset was designed to test 3D perception in humans and DNNs, with an emphasis on the
capabilitiesofeachforvisualperspectivetaking(VPT).HumansrelyonVPTeverydayfornavigating
andsocializing,butdespiteitsimportance,therehasyettobeasystematicevaluationofthisability
inDNNs.
Whocreatedthisdataset(e.g.,whichteam,researchgroup)andonbehalfofwhichentity(e.g.,
company,institution,organization)?
Thisdatasetwascreatedbythispaperauthors,whoareaffiliatedwiththeCarneyInstituteforBrain
ScienceatBrownUniversityandtheCognitiveSciencesDepartmentatUCIrvine.
Whofundedthecreationofthedataset? Ifthereisanassociatedgrant,pleaseprovidethenameof
thegrantorandthegrantnameandnumber.
FundingforthisprojectwasprovidedbytheOfficeofNavalResearch(N00014-19-1-2029)and
ANR-3IAArtificialandNaturalIntelligenceToulouseInstitute(ANR-19-PI3A0004). Additional
support provided by the Carney Institute for Brain Science and the Center for Computation and
Visualization(CCV).WeacknowledgetheCloudTPUhardwareresourcesthatGooglemadeavailable
viatheTensorFlowResearchCloud(TFRC)programaswellascomputinghardwaresupportedby
NIHOfficeoftheDirectorgrantS10OD025181.
Composition
21Whatdotheinstancesthatcomprisethedatasetrepresent(e.g.,documents,photos,people,
countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and
interactionsbetweenthem;nodesandedges)? Pleaseprovideadescription.
Theinstancescontainimagesofreal-worldobjectsandscenesalongwithshapesgeneratedwith
computergraphics.
Howmanyinstancesarethereintotal(ofeachtype,ifappropriate)?
Thereare7574imagesinthetrainingandtestingsets.
Does the dataset contain all possible instances or is it a sample (not necessarily random)
of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the
sample representative of the larger set (e.g., geographic coverage)? If so, please describe how
this representativeness was validated/verified. If it is not representative of the larger set, please
describewhynot(e.g.,tocoveramorediverserangeofinstances,becauseinstanceswerewithheld
orunavailable).
Wereleasealldata.
Whatdatadoeseachinstanceconsistof? “Raw”data(e.g.,unprocessedtextorimages)or
features? Ineithercase,pleaseprovideadescription.
Each instance consists of an image rendered from 3d Gaussian Splatting [7] models trained on
Co3D[43]scenes.
Istherealabelortargetassociatedwitheachinstance? Ifso,pleaseprovideadescription.
The images are labeled for VPT and depth order tasks. In the VPT task, an image is labeled as
positivewhentheredballisvisiblefromthegreencamera’sperspective. Inthedepthtask,animage
islabeledaspositivewhentheredballisfurtherawaythanthegreenarrowfromtheviewer. Forboth
tasks,welabelpositivesas1andnegativesas0.
Isanyinformationmissingfromindividualinstances?Ifso,pleaseprovideadescription,explaining
whythisinformationismissing(e.g.,becauseitwasunavailable). Thisdoesnotincludeintentionally
removedinformation,butmightinclude,e.g.,redactedtext.
N/A
Arerelationshipsbetweenindividualinstancesmadeexplicit(e.g.,users’movieratings,social
networklinks)? Ifso,pleasedescribehowtheserelationshipsaremadeexplicit.
N/A
Arethererecommendeddatasplits(e.g.,training,development/validation,testing)?Ifso,please
provideadescriptionofthesesplits,explainingtherationalebehindthem.
Weprovidetraining,validationandtestingsplitsinthereleaseddataset. Thetrainingsetcontains
imagesrenderedfrom20uniquescenesfrom10categories. Thetestingsetimagesarerenderedfrom
10additionalscenesfromthesamecategories. Werandomlyselected10%ofthetrainingsetasthe
validationset.
Arethereanyerrors,sourcesofnoise,orredundanciesinthedataset? Ifso,pleaseprovidea
description.
N/A
Isthedatasetself-contained,ordoesitlinktoorotherwiserelyonexternalresources(e.g.,
websites, tweets, other datasets)? If it links to or relies on external resources, a) are there
guaranteesthattheywillexist,andremainconstant,overtime;b)arethereofficialarchivalversions
ofthecompletedataset(i.e.,includingtheexternalresourcesastheyexistedatthetimethedataset
wascreated);c)arethereanyrestrictions(e.g.,licenses,fees)associatedwithanyoftheexternal
resourcesthatmightapplytoafutureuser? Pleaseprovidedescriptionsofallexternalresourcesand
anyrestrictionsassociatedwiththem,aswellaslinksorotheraccesspoints,asappropriate.
22ThedatasetusesvideosfromtheCo3Ddataset[43],whichispubliclyavailableunderCCBY-NC
4.0license.
Doesthedatasetcontaindatathatmightbeconsideredconfidential(e.g.,datathatisprotected
by legal privilege or by doctor-patient confidentiality, data that includes the content of
individualsnon-publiccommunications)? Ifso,pleaseprovideadescription.
N/A
Doesthedatasetcontaindatathat,ifvieweddirectly,mightbeoffensive,insulting,threatening,
ormightotherwisecauseanxiety? Ifso,pleasedescribewhy.
N/A
Doesthedatasetrelatetopeople? Ifnot,youmayskiptheremainingquestionsinthissection.
Yes
Doesthedatasetidentifyanysubpopulations(e.g.,byage,gender)? Ifso,pleasedescribehow
thesesubpopulationsareidentifiedandprovideadescriptionoftheirrespectivedistributionswithin
thedataset.
No
Is it possible to identify individuals (i.e., one or more natural persons), either directly or
indirectly(i.e.,incombinationwithotherdata)fromthedataset? Ifso,pleasedescribehow.
No,allresultsareanonymous.
Doesthedatasetcontaindatathatmightbeconsideredsensitiveinanyway(e.g.,datathat
reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or
unionmemberships,orlocations;financialorhealthdata;biometricorgeneticdata;forms
ofgovernmentidentification,suchassocialsecuritynumbers;criminalhistory)? Ifso,please
provideadescription.
N/A
Anyothercomments?
CollectionProcess
Howwasthedataassociatedwitheachinstanceacquired? Wasthedatadirectlyobservable(e.g.,
rawtext,movieratings),reportedbysubjects(e.g.,surveyresponses),orindirectlyinferred/derived
fromotherdata(e.g.,part-of-speechtags,model-basedguessesforageorlanguage)? Ifdatawas
reportedbysubjectsorindirectlyinferred/derivedfromotherdata,wasthedatavalidated/verified? If
so,pleasedescribehow.
Allimageswererenderedfrom3Dgaussiansplatting[7]modelstrainedonvideosfromCo3D[43].
WeimportedthemodelintoUnity[44,45]torenderimages.
Whatmechanismsorprocedureswereusedtocollectthedata(e.g.,hardwareapparatusor
sensor,manualhumancuration,softwareprogram,softwareAPI)?Howwerethesemechanisms
orproceduresvalidated?
WeusedUnity[44]andUnityGaussianSplatting[45]toeditthescenesandlabelthemin3Dview.
Ifthedatasetisasamplefromalargerset,whatwasthesamplingstrategy(e.g.,deterministic,
probabilisticwithspecificsamplingprobabilities)?
N/A
Whowasinvolvedinthedatacollectionprocess(e.g.,students,crowdworkers,contractors)
andhowweretheycompensated(e.g.,howmuchwerecrowdworkerspaid)?
23Thepaper’sauthorswereinvolvedinthedatacollectionprocess.
Overwhattimeframewasthedatacollected?Doesthistimeframematchthecreationtimeframe
ofthedataassociatedwiththeinstances(e.g.,recentcrawlofoldnewsarticles)? Ifnot,please
describethetimeframeinwhichthedataassociatedwiththeinstanceswascreated.
N/A
Wereanyethicalreviewprocessesconducted(e.g.,byaninstitutionalreviewboard)? Ifso,
pleaseprovideadescriptionofthesereviewprocesses,includingtheoutcomes,aswellasalinkor
otheraccesspointtoanysupportingdocumentation.
Doesthedatasetrelatetopeople? Ifnot,youmayskiptheremainingquestionsinthissection.
Yes
Didyoucollectthedatafromtheindividualsinquestiondirectly,orobtainitviathirdpartiesor
othersources(e.g.,websites)?
AsdescribedintheMethods,wecollecteddatafromonlineparticipantsthroughProlific,andwealso
collecteddatain-personforseveralsubjects.
Weretheindividualsinquestionnotifiedaboutthedatacollection? Ifso,pleasedescribe(or
showwithscreenshotsorotherinformation)hownoticewasprovided, andprovidealinkorother
accesspointto,orotherwisereproduce,theexactlanguageofthenotificationitself.
Yes. SeeSectionA.8fordetails.
Didtheindividualsinquestionconsenttothecollectionanduseoftheirdata? Ifso,please
describe(orshowwithscreenshotsorotherinformation)howconsentwasrequestedandprovided,
andprovidealinkorotheraccesspointto,orotherwisereproduce,theexactlanguagetowhichthe
individualsconsented.
Yes. SeeFigA.2fortheconsentscreenwiththeexactlanguageused.
Ifconsentwasobtained,weretheconsentingindividualsprovidedwithamechanismtorevoke
theirconsentinthefutureorforcertainuses? Ifso,pleaseprovideadescription,aswellasalink
orotheraccesspointtothemechanism(ifappropriate).
Yes. Theparticipantswereprovidedwithourcontactinformationandwereencouragedtoreachout
insuchcases.
Hasananalysisofthepotentialimpactofthedatasetanditsuseondatasubjects(e.g.,adata
protectionimpactanalysis)beenconducted? Ifso,pleaseprovideadescriptionofthisanalysis,
includingtheoutcomes,aswellasalinkorotheraccesspointtoanysupportingdocumentation.
OurexperimentwasapprovedbytheIRBboardatBrownUniversity.
Anyothercomments?
Preprocessing/cleaning/labeling
Wasanypreprocessing/cleaning/labelingofthedatadone(e.g.,discretizationorbucketing,
tokenization,part-of-speechtagging,SIFTfeatureextraction,removalofinstances,processing
ofmissingvalues)? Ifso,pleaseprovideadescription. Ifnot,youmayskiptheremainderofthe
questionsinthissection.
WeusedUnitytolabelimagesforVPTanddepthtasks. Weremovedimageswheretheobjectsof
interest(redballandgreencamera)werenotvisible.
24Wasthe“raw”datasavedinadditiontothepreprocessed/cleaned/labeleddata(e.g.,tosupport
unanticipatedfutureuses)? Ifso,pleaseprovidealinkorotheraccesspointtothe“raw”data.
N/A
Isthesoftwareusedtopreprocess/clean/labeltheinstancesavailable? Ifso,pleaseprovidea
linkorotheraccesspoint.
N/A
Anyothercomments?
Uses
Hasthedatasetbeenusedforanytasksalready? Ifso,pleaseprovideadescription.
WeevaluatedvisionDNNsonthedataset. Pleaserefertothemainpaperfordetails.
Istherearepositorythatlinkstoanyorallpapersorsystemsthatusethedataset? Ifso,please
providealinkorotheraccesspoint.
Thecodeanddataarepubliclyavailableathttps://github.com/serre-lab/VPT
What(other)taskscouldthedatasetbeusedfor?
Wemainlyexpectthedatasettobeusedforevaluating3Dperceptioncapabilitiesofnewvisionor
vision-languageDNNs.
Is there anything about the composition of the dataset or the way it was collected and
preprocessed/cleaned/labeledthatmightimpactfutureuses? Forexample,isthereanythingthat
afutureusermightneedtoknowtoavoidusesthatcouldresultinunfairtreatmentofindividualsor
groups(e.g.,stereotyping,qualityofserviceissues)orotherundesirableharms(e.g.,financialharms,
legalrisks)Ifso,pleaseprovideadescription. Isthereanythingafutureusercoulddotomitigate
theseundesirableharms?
N/A
Aretheretasksforwhichthedatasetshouldnotbeused? Ifso,pleaseprovideadescription.
N/A
Anyothercomments?
Distribution
Willthedatasetbedistributedtothirdpartiesoutsideoftheentity(e.g.,company,institution,
organization)onbehalfofwhichthedatasetwascreated? Ifso,pleaseprovideadescription.
Yes,wewillreleasethedatasettothepublicathttps://github.com/serre-lab/VPT
Howwillthedatasetbedistributed(e.g.,tarballonwebsite,API,GitHub)Doesthedatasethave
adigitalobjectidentifier(DOI)?
Weprovidedownloadinstructionsathttps://github.com/serre-lab/VPT
Whenwillthedatasetbedistributed?
ThedatasetisavailablefromJune5th,2024.
Will the dataset be distributed under a copyright or other intellectual property (IP) license,
and/orunderapplicabletermsofuse(ToU)?Ifso,pleasedescribethislicenseand/orToU,and
25providealinkorotheraccesspointto,orotherwisereproduce,anyrelevantlicensingtermsorToU,as
wellasanyfeesassociatedwiththeserestrictions.
WereleaseourdataunderaCreativeCommonsCC-BYlicense.
Have any third parties imposed IP-based or other restrictions on the data associated with
theinstances? Ifso, pleasedescribetheserestrictions, andprovidealinkorotheraccesspoint
to,orotherwisereproduce,anyrelevantlicensingterms,aswellasanyfeesassociatedwiththese
restrictions.
N/A
Doanyexportcontrolsorotherregulatoryrestrictionsapplytothedatasetortoindividual
instances? Ifso,pleasedescribetheserestrictions,andprovidealinkorotheraccesspointto,or
otherwisereproduce,anysupportingdocumentation.
N/A
Anyothercomments?
Maintenance
Whowillbesupporting/hosting/maintainingthedataset?
Theauthorswillbehostingandmaintainingthedataset.
Howcantheowner/curator/managerofthedatasetbecontacted(e.g.,emailaddress)?
Contactthecorrespondingauthorthroughemail.
Isthereanerratum? Ifso,pleaseprovidealinkorotheraccesspoint.
N/A
Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete
instances)? Ifso,pleasedescribehowoften,bywhom,andhowupdateswillbecommunicatedto
users(e.g.,mailinglist,GitHub)?
Weareactivelyworkingonexpandingthedatasetwithnewinstancesandtasks. Wewillupdateour
GitHubrepositoryaccordinglyforanydatasetupdate.
Ifthedatasetrelatestopeople,arethereapplicablelimitsontheretentionofthedataassociated
withtheinstances(e.g.,wereindividualsinquestiontoldthattheirdatawouldberetainedfora
fixedperiodoftimeandthendeleted)? Ifso,pleasedescribetheselimitsandexplainhowtheywill
beenforced.
Humanparticipantdatawasde-identified,andtherearenotimelimitsonitsretention.
Willolderversionsofthedatasetcontinuetobesupported/hosted/maintained? Ifso,please
describehow. Ifnot,pleasedescribehowitsobsolescencewillbecommunicatedtousers.
Yes,wewillmaintainoldversionsofthedatasetonourwebsite.
Ifotherswanttoextend/augment/buildon/contributetothedataset,isthereamechanismfor
themtodoso? Ifso,pleaseprovideadescription. Willthesecontributionsbevalidated/verified?
Ifso,pleasedescribehow. Ifnot,whynot? Isthereaprocessforcommunicating/distributingthese
contributionstootherusers? Ifso,pleaseprovideadescription.
WeareopentoanysuggestionsandcontributionsthroughourGitHubrepository. https://github.
com/serre-lab/VPT
26Architecture Model Versions
ConvMixer[61] 3
ConvNeXT[62] 10
DenseNet[63] 4
DLA[64] 5
DPN[65] 6
EfficientNet[66] 4
GhostNet[67] 1
HRNet[68] 8
LCNet[69] 3
MixNet[70] 4
CNN MnasNet[71] 3
MobileNet[72] 14
RegNet[73] 6
Res2Net[74] 5
ResNet[75] 26
ResNeSt[76] 3
RexNet[77] 5
ResNext[78] 2
SPNASNet[79] 1
TinyNet[80] 2
VGG[81] 14
BEiT[82] 9
CAFormer[83] 6
CaiT[84] 3
ConViT[85] 3
CrossViT[86] 2
DaViT[87] 3
DeiT[88] 12
EfficientFormer[89] 7
EVA[48] 9
FocalNet[90] 6
LeViT[91] 5
Transformer
MaxViT[92] 6
MobileViT[93] 3
MViT[94] 3
PiT[95] 8
PVT[96] 7
Swin[60] 16
Twins-SVT[97] 5
ViT[98] 36
Volo[99] 7
XCiT[100] 6
PoolFormer[101] 8
CoaT[102] 7
CoAtNet[103] 8
Hybrid
EdgeNeXt[104] 1
Visformer[105] 2
DepthAnything[53] 1
DINOv2[50] 1
iBoT[51] 1
Foundation
MAE[49] 1
MiDas[15] 1
SAM[52] 1
ChatGPT4[27] 1
VLM Gemini[54] 1
Claude3[55] 1
Diffusion StableDiffusion2.0[28] 1
Table1: The327DNNmodelsusedinourstudy.
27