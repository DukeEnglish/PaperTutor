Towards Principled Superhuman AI for Multiplayer
Symmetric Games
JiaweiGe*† YuanhaoWang*‡ WenzheLi§ ChiJin¶
Abstract
Multiplayergames, whenthenumber ofplayersexceeds two, present uniquechallengesthatfunda-
mentally distinguish them from the extensively studied two-player zero-sum games. These challenges
arisefromthenon-uniquenessofequilibriaandtheriskofagentsperforminghighlysuboptimallywhen
adopting equilibrium strategies. While a line of recent works developed learning systems successfully
achievinghuman-levelorevensuperhumanperformanceinpopularmultiplayergamessuchasMahjong,
Poker,andDiplomacy,twocriticalquestionsremainunaddressed:(1)Whatisthecorrectsolutionconcept
thatAIagentsshouldfind? and(2)Whatisthegeneral algorithmicframeworkthatprovably solvesall
gameswithinthisclass? Thispapertakesthefirststeptowardssolvingtheseuniquechallengesofmul-
tiplayergamesbyprovablyaddressingbothquestionsinmultiplayersymmetricnormal-formgames. We
alsodemonstratethatmanymeta-algorithmsdeveloped inpriorpracticalsystemsformultiplayergames
canfailtoachieveeventhebasicgoalofobtainingagent’sequalshareofthetotalreward.
1 Introduction
In recent years, AI has achieved remarkable success in multi-agent decision-making problems, particu-
larly in a wide range of strategic games. These include, but are not limited to, Go (Silveretal., 2016),
Mahjong(Lietal.,2020),Poker(Moravcˇ´ıketal.,2017;BrownandSandholm,2018,2019),Starcraft2(Vinyalsetal.,
2019),DOTA2(Berneretal.,2019),LeagueofLegends(Yeetal.,2020),andDiplomacy(Grayetal.,2020;
Bakhtinetal., 2022; , FAIR). The majority of these games are two-player zero-sum games1, where Nash
equilibriaalwaysexistandcanbecomputedefficiently. Nashequilibriaintwo-playerzero-sumgamesare
also non-exploitable—an agent employing a Nash equilibrium policy will not lose even when facing an
adversarial opponent who seeks to exploit the agent’s weaknesses. Although such equilibrium strategies
donotnecessarilycapitalizeonopponents’weaknessesorguaranteelarge-marginvictories,humanplayers
oftenadoptsuboptimalpoliciesthatdeviatesignificantlyfromequilibriaincomplexgameswithlargestate
spaces. Consequently,AIagentswhoadoptequilibriumstrategiesoftenoutperformhumansinpracticefor
two-playerzero-sumgames.
Incontrast,multiplayergames—definedhereasthosewithmorethantwoplayers—exhibitfundamen-
tallydifferentgamestructurescomparedtotwo-playerzero-sumgames. Thisdistinctionintroducesseveral
*equalcontribution
†DepartmentofOperationsResearchandFinancialEngineering,PrincetonUniversity;jg5300@princeton.edu
‡DepartmentofComputerScience,PrincetonUniversity;yuanhao@princeton.edu
§DepartmentofElectricalandComputerEngineering,PrincetonUniversity;wenzhe.li@princeton.edu
¶DepartmentofElectricalandComputerEngineering,PrincetonUniversity;chij@princeton.edu
1GamessuchasDOTAandLeagueofLegends,despiteinvolvingtwoteams,canbemostlyconsideredsimilartotwo-playerzero-
sumgamesintermsoftheirgamestructuresandsolutions.
1
4202
nuJ
6
]GL.sc[
1v10240.6042:viXraunique challenges. Firstly, Nash equilibria are no longer efficiently computable (Daskalakisetal., 2009;
ChenandDeng,2005). Moreover,theremayexistmultipleNashequilibriawithdistinctvalues. Suchnon-
uniquenessinequilibriaraisesacriticalconcernabouttheadoptionofequilibriumstrategiesinmultiplayer
settings: if an AI agent adopts an equilibrium that is different from other players, collectively, they are
notplayinganysingleequilibrium,whichunderminestheequilibriumpropertythatdissuadesagentsfrom
changingstrategiesaslongasothersmaintaintheirs. Finally, inmultiplayergames,equilibriumstrategies
are no longer non-exploitable. Although the introduction of alternative equilibrium notions such as Cor-
relatedEquilibria(CE)orCoarseCorrelatedEquilibria(CCE)alleviatescomputationalhardness,issuesof
non-uniquenessandpotentialexploitabilityremain.Thisleadstothefirstcriticalquestion:
WhatisthecorrectsolutionconceptforanAIagenttolearninmultiplayergames?
Whilenotansweringthisquestiondirectly,severalpracticalsystemsformultiplayergameshavealready
achievedhuman-levelorevensuperhumanperformanceinseveralpopulargamesincludingMahjong(Lietal.,
2020) (4 players), Poker (BrownandSandholm, 2019) (6 players), and Diplomacy(Bakhtinetal., 2022; ,
FAIR) (7 players). These systems focuson developingalgorithmicframeworkscapableof learningeffec-
tivestrategiesthatexcelin competitivesettings, suchasladders, onlinegamingplatforms,or tournaments
againsthumanplayers. Generally,mostofthesesystemsrelyonabasicself-playframework,startingfrom
scratch or fromhumanpoliciesacquiredthroughbehaviorcloning, with or withoutregularizations. Since
allthesesystemsdemandsubstantialcomputationalpower,significanthumanexpertise,andextensiveengi-
neeringefforts,thegeneralapplicabilityofthealgorithmicframeworksdevelopedinthesestudiesremains
tobetestedinawiderangeofmultiplayergamesbeyondMahjong,Poker,orDiplomacy. Thisleadstothe
secondcriticalquestion:
Whatisthegeneralalgorithmicframeworkthatcanprovablysolveallmultiplayergameswithin
certainrichclasses?
Inthispaper,weconsideranimportantsubclassofmultiplayergames: symmetricconstant-sumgames,
which are prevalent in games involving more than two players. Examples include previously discussed
multiplayer games like Mahjong, Poker, Diplomacy, as well as a variety of board games such as Avalon
(Lightetal., 2023), Mafia and Catan.2 Symmetry brings fairness among players, and a natural baseline
forsymmetricconstant-sumgamesisthattheAIagentshould,ataminimum,secureanequalshareofthe
totalreward. For instance, in a four-playergamewith onlyone winner, the AI shouldaim to win at least
one-fourthofthegames. Thispaperalsofocusesontheformulationofnormal-formgames,whicharenot
onlyfoundationalbutalsoexpressiveenoughtoencapsulatesequentialgameslikeextensive-formgamesor
Markovgamesasspecialcases.3
Thispapertakesaninitialsteptowardsaddressingthegrandchallengesofgeneralmultiplayergamesby
effectivelytacklingthetwohighlightedquestionswithinthecontextofmultiplayersymmetricnormal-form
games.Regardingthefirstquestionofsolutionconcepts,wediscusstheinadequacyofclassicalequilibrium
solutionsinmeetingthebaselinerequirementofachievinganequalshareinsymmetricgames. Wefurther
establishimportantclaimsthat(1)aslongasopponentscandeploydifferentindividualpolicies,evenplaying
theirbestresponsecanfailtoachieveanequalshare;(2)theremaynotexistanon-exploitablestrategyeven
whenall opponentsare restrictedto playthe same strategy(See Section4 fordetails). Thisunfortunately
leavesuswiththeonlyviableoptionthattheagentmustlearnstrategiesthatadapttotheidenticalstrategy
2Alltheseexamplesaresymmetricgamesuptorandomizationoftheseating.
3Extensive-FormGames(EFGs)orMarkovGames(MGs)canbeviewedasspecialcasesofnormal-formgames,whereeachaction
innormal-formgamescorrespondstoapolicyinEFGsorMGs,althoughsuchrepresentationsmaynotalwaysbeefficient.
2played by the opponents. Solution concepts that fall into this category, such as the best response to the
identicalstrategy of the opponents, always meet the baseline requirement. While assuming all opponents
playingidenticalstrategyappearsrestrictive,we arguethatithasalreadybeenimplicitlyadoptedin many
modernAIsystemsformultiplayergames. Wealsoprovethatsuchanassumptionapproximatelyholdsin
multiplayergamingplatformswithalargeplayerbase.
Forthesecondquestionconcerninggeneralalgorithms,thispaperillustrateshowwecanleverageexist-
ingno-regretandno-dynamicregretlearningtoolsappropriatelytomeetthebaselinerequirementofequal
sharewithregretguarantees,particularlywhenhumanpoliciesareeitherstationaryorevolvingslowly. We
also presentmatchinglower boundsthatshowthese regretguaranteescannotbe significantlyimprovedin
theworstcase. Ourexperimentalresultsdemonstratethatwhileouralgorithmicsolutionsconsistentlyout-
performstatichumanpolicies,theself-playmeta-algorithmsdevelopedinpreviousstate-of-the-artsystems
formultiplayergameslikeMahjong,Poker,andDiplomacymayconvergetosuboptimalsolutions,resulting
intheAIagentconsistentlylosingthegameevenagainstnon-adaptivehumanplayers.
1.1 Related work
Human-level or superhuman AI in practice Building superhuman AI has long been a goal in vari-
ous games. A large body of works in this line focus on two-player or two-team zero-sum games like
Chess (Campbelletal., 2002), Go (Silveretal., 2016), Heads-Up Texas Hold’em (Moravcˇ´ıketal., 2017;
BrownandSandholm, 2018), Starcraft 2 (Vinyalsetal., 2019), DOTA 2 (Berneretal., 2019) and League
of Legends (Yeetal., 2020). Most of them are based on finding equilibria via self-play, fictitious play,
league training, etc. There is comparatively much less amount of work on games with more than two
players, whose game structures are fundamentally different from two-player zero-sum games. Several
remarkable multiplayer successes include Poker (BrownandSandholm, 2019), Mahjong (Lietal., 2020),
Doudizhu(Zhaetal.,2021)andDiplomacy(Bakhtinetal.,2022;,FAIR).Despitelackingaclearlyformu-
lated learningobjective,these workstypicallydesignmeta-algorithms,whichincludeinitiallytrainingthe
modelusingbehaviorcloningfromhumanplayers,thenenhancingitthroughself-play,andfinallyapplying
adaptationsbased on the game’s specific structure or human expertise. It remains elusive whether such a
recipeisgenerallyeffectiveforawiderangeofmultiplayergames.
Existingresultsonsymmetricgames VonNeumann&Morgenstern,1947(VonNeumannandMorgenstern,
1947) gave the first definition of symmetric games and used the three-player majority-vote example to
showcasethe stark differencebetweensymmetricthree-playerzero-sumgamesandsymmetrictwo-player
zero-sum games. In his seminal paper that introduced Nash equilibrium, Nash proved that a symmet-
ric finite multi-player game must have a symmetric Nash equilibrium (Nash, 1951). However, this ex-
istence result might mean little from an individual standpoint, as there is no reason a priori to assume
that other players are indeed playing according to this symmetric equilibrium. Papadimitriou & Rough-
garden, 2005(PapadimitriouandRoughgarden, 2005) studied the computationalcomplexityof finding the
Nash equilibriumin symmetricmulti-playergameswhenthe numberofactionsavailableis muchsmaller
thanthenumberofplayers,andgaveapolynomial-timealgorithmfortheproblem. Inthiscase,symmetry
greatlyreducedthecomputationalcomplexity(ascomputingNashingeneralisPPAD-hard). Daskalakiset
al.,2009(Daskalakis,2009)proposedanonymousgames,ageneralizationofsymmetricgames.
No-regretlearningingames Thereisarichliteratureonapplyingno-regretlearningalgorithmstolearn-
ing equilibria in games. It is well-knownthat if all agentshave no-regret,the resulting empiricalaverage
wouldbeanapproximateCoarseCorrelatedEquilibrium(CCE)(Young,2004),whileifallagentshaveno
3swap-regret,theresultingempiricalaveragewouldbeanǫ-CorrelatedEquilibrium(CE)(HartandMas-Colell,
2000; Cesa-BianchiandLugosi, 2006). Later work continuing this line of research includes those with
fasterconvergencerates(Syrgkanisetal.,2015;ChenandPeng,2020;Daskalakisetal., 2021), last-iterate
convergenceguarantees(DaskalakisandPanageas,2018;Weietal.,2020),andextensiontoextensive-form
games(Cellietal.,2020;Baietal.,2022b,a;Songetal.,2022)andMarkovgames(Songetal.,2021;Jinetal.,
2021).
2 Preliminaries
Notation. Foranyset ,itscardinalityisrepresentedby ,and∆( )denotesaprobabilitydistribution
A |A| A
over . Weemploy ⊗n todenotetheCartesianproductofninstancesof . Givenadistributionxover
A A A
,x⊗n representsthejointdistributionofnindependentcopiesofx,formingadistributionover ⊗n. For
A afunctionf : R,wedenote f :=max f(a). Weuse[n]todenotetheset 1,...,nA . Inthis
∞ a∈A
A→ k k | | { }
paper,weuseC todenoteuniversalconstants,whichmayvaryfromlinetoline.
Normal-form game An n-player normal-form game consists of a finite set of n players, where each
player has an action space and a corresponding payoff function U : [ 1,1]
i i 1 n
A A × ··· × A → −
with U (a ,...,a ) denotes the payoff received by the i-th player if n players are taking joint actions
i 1 n
(a ,...,a ). Thispaperfocusesonsymmetricandzero-sumnormal-formgames.4
1 n
Definition2.1(Symmetriczero-sumnormal-formgame). Forann-playernormal-formgamewithanaction
space and a payoff U for player i, we say the game is symmetric if (1) = , for all i [n];
i i i
A A A ∈
(2)foranypermutationπ: U i(a 1, ,a n) = U π−1(i)(a π(1), ,a π(n)). We say thegameis zero-sumif
n U (a)=0foralla ··· . ···
i=1 i ∈A1 ×···×An
P Brieflyspeaking,inasymmetricgame,thepayoffsforemployingaspecificactionaredeterminedsolely
bytheactionsusedbyothers,agnosticoftheidentitiesoftheplayersusingthem. Thus,thepayofffunction
of the first player, denoted as U , is sufficient to encapsulate the entire game. For i [n], x ∆( )
1 i
∈ ∈ A
denotes a mixed strategy of the i-th player and x ∆( ⊗n−1) denotes a mixed strategy of the other
−i
∈ A
players.Correspondingly,a denotestheactionofthei-thplayeranda ⊗n−1denotestheaction
i −i
oftheotherplayers. Foranya∈A ,wedenoteU (a ,x ):=E [U (a∈ ,A a )].
i
∈A
i i −i a−i∼x−i i i −i
Bestresponse Givenamixedstrategyx oftheothern 1players,thebestresponsesetBR (x )of
−i i −i
−
thei-thplayerisdefinedasBR (x ):=argmax U (a ,x ).
i −i ai∈Ai i i −i
Equilibrium Nashequilibriumisthemostcommonly-usedsolutionconceptforgames: amixedstrategy
x ∆( )ofallplayersissaidtobeNashEquilibrium(NE)ifxisaproductdistribution,and
1 n
∈ A ×···×A
noplayercouldgainbydeviatingherownstrategywhileholdingallotherplayers’strategiesfixed. Thatis,
foralli [n]anda′ ,E [U (a ,a )] E [U (a′,a )].
∈ i ∈Ai a∼x i i −i ≥ a∼x i i −i
TherearealsotwoequilibriumnotionsrelaxingthenotionofNashequilibriumbynolongerrequiring
x to be a product distribution. It allows general joint distribution x which describes correlated policies
among players. In particular, (1) x is a Correlated Equilibrium (CE) if for all i [n] and a′ ,
E [U (a ,a ) a ] E [U (a′,a ) a ], and (2) x is Coarse Correlated E∈ quilibrium (i C∈ CEA ) ii f
a∼x i i −i | i ≥ a∼x i i −i | i
4Ourworktrivially extends to symmetric andconstant-sum normal-form games, since one can convert constant-sum games to
zero-sumgamesbyoffsettingallplayers’payofffunctionsbyaconstant.
4foralli [n]anda′ : E [U (a ,a )] E [U (a′,a )]. Themajordifferencebetweenthose
∈ i ∈ Ai a∼x i i −i ≥ a∼x i i −i
two notionsis in the cases when the agentdeviatesfrom her currentstrategy, whether she is still allowed
to observethe randomnessin drawingactionsfromthe correlatedpolicy. Therelationshipamongvarious
equilibriumconceptsisencapsulatedbyNE CE CCE.
⊂ ⊂
We notein two-playerzero-sumgames, a Nash equilibriumis non-exploitable. In math, if (µ⋆,ν⋆) is
theNashequilibrium,wehavemin U (µ⋆,ν)=max min U (µ,ν)=0.
ν 1 µ ν 1
No-regretlearning No-regretlearningisacommonlyadoptedstrategyingametheorytofindequilibrium
solutions. We consider a T-step learning procedure,where for each round t [T]: (1) the agentpicks a
∈
mixedstrategyµt over ,(2)theenvironmentpicksanadversariallossℓ [0,1]|A|. Theexpectedutility
t
A ∈
for t-th round is defined as µt,ℓ . To measure the performance of a particular algorithm, a common
t
−h i
approachis to consider the regret, where the algorithm’sperformanceis comparedagainstthe single best
actioninhindsight.Specifically,forpolicysequence(µ1,...,µT)takenbyanalgorithm,wedefine
T T
Reg(T):= µt,ℓ min ℓ (a),
t t
h i−a∈A
t=1 t=1
X X
We say that the algorithm is a no-regret algorithm if Reg(T) = o(T). One of such no-regret learning
algorithmsisHedgealgorithm,whichperformsthefollowingexponentialweightupdates:
µt+1(a) µt(a)e−ηtℓt(a), for a . (1)
∝ ∀ ∈A
whereη isthelearningrate.
t
Relation between no-regret learning and equilibria It is known that in the normal-form game setup,
if each player independently plays a no-regret learning algorithm treating all other players as adversarial
environments,andwedenotext =xt xt asthejointstrategybyallplayersatroundt,thenwehave
1×···× n
theaveragepolicy(1/T) T xt convergestoCCEswhenT islarge.
t=1
P
3 Equilibria and Self-play are Insufficient for Multiplayer Games
In a multiplayer symmetric zero-sum game, every player is created equal, and the equal share for each
player is zero. Intuitively, one might expect a good player to receive a non-negative expected payoff,
makingthisaminimalbaselineforawell-designedAIagent. However,inthissection,wedemonstrate
that both the existing equilibria notionsand the algorithmicframeworkof self-play from scratch—widely
used in achieving superhuman performance in practical games—are insufficient to secure this seemingly
straightforwardgoal.
Toillustratethis,weconsiderthefollowing3-playermajorityvotegameinvolvinganAIagentandtwo
otherplayers.
Example1(Three-playermajorityvotegame). Ineachround,everyplayerchooseseither0or1,withthe
rulethatbeinginthemajorityyieldsapositivepayoffof1,whilebeingintheminorityresultsinanegative
payoffof 2.
−
5Limitationofequilibria Inthis setup, bothstrategies(0,0,0)and(1,1,1)constituteNE. However,the
existence of multiple NEs creates a predicament for the AI agent. It must choose which equilibrium to
follow, yet there’salways the risk that the other two playersmay coordinateon the opposingNE, leading
to a negativepayofffor theAI agent. Inotherwords, adheringto a single NE doesnotreliablyensurean
averagepayoffwhenmultipleequilibriaexist. SinceNE CE CCE,weknowthesamelimitationalso
⊂ ⊂
holdsforCEandCCE.
Limitation of self-play from scratch Self-play is a training method in which an AI agent improvesits
performance by repeatedly playing against copies of itself without human supervision. Typical self-play
is carried out by optimizing one agent’s policy (denoted as the main agent) while fixing all other agents’
policies to be the currentpolicy or the earlier versions of the main agent. When no-regretlearning algo-
rithms such as Hedge are adoptedas the optimizer, the self-play algorithmis guaranteedto convergeto a
CCE.Moreover,itmayconvergetoacertain“good”subclassofCCEsdependingonthechoiceofspecific
optimizerandinitialization.
Here, we argue that self-play from scratch (BrownandSandholm, 2019) fails to always achieve non-
negativeexpectedpayoffinmultiplayersymmetriczero-sumgames. Theexampleisagainthethree-player
majorityvotegame.Imaginetwoparallelworlds,withtwootherplayersalwaysplaying1inoneworld,and
0intheother. Thelearnermustplaytheexactsameactionastheothertwoplayerstoreceivenon-negative
expectedpayoff. Notingthatself-playfromscratchdoesnotcheckotherplayers’actualpolicy,thelearner
hasnocluewhichworlditis,andthuslosesinatleastoneofthetwoworlds.
We note that severalrecentsystems (Lietal., 2020; Jacobetal., 2022) combineself-play with human
policymodelingwhich avoidsthe drawbackof self-playfromscratch. However,ourexperimentsdemon-
stratethatmanyofthesesolutionsremainineffectiveeveninsomesimplenormal-formgames(SeeSection
6).
4 New Solution Concepts
The limitation of standard equilibria leads us to the following question: What solution conceptshould be
adoptedtosecure anequalshare (non-negativeexpectedpayoff)in multiplayersymmetric games? Inthis
section,weprovideaclearanswertothisquestion.
Diversestrategiesbyopponents Westartbyconsideringthegeneralcaseswhereopponentscanpossibly
deploydifferentstrategies.
Claim4.1. Thereexistsymmetriczero-sumgamessuchthatnopolicyachievesnon-negativeexpectedpayoff
iftheotherplayers,evenwithoutcollusion,arepermittedtoadoptdifferentstrategies.
Here,wedefine“nocollusion”byrequiringthepoliciesofotherplayerstobestatisticallyindependent
withoutanysharedrandomness.
Proof. We consider a 3-player minority game involving an AI agent and two other players. The rule of
thegameisthatbeingintheminorityyieldsapositivepayoffof2,whilebeinginthemajorityresultsina
negativepayoffof 1. Iftheothertwoplayersact0and1,respectively,thentheAIagentcanonlyreceive
−
1payoff,whichisstrictlylessthan0.Sincetheothertwoplayersaresimplyplayingdeterministicpolicies,
−
theyareindependent,i.e.,withoutcollusion.
6Thisobservationunderscoresacrucialaspectofthemultiplayersymmetricgamedynamicwherediverse
strategiesbyopponentscandrasticallyaffectthepossibilityofachievinganequalshare.
Identicalstrategybyopponents Claim4.1leavesoutnochoicebuttoconsiderthesettingwhereoppo-
nentsmustdeployidenticalstrategy.Whileitmayseemlimitingtoassumethatopponentsemployidentical
strategies, we argue that this assumption is actually already implicit in the majority of practical systems
(Lietal.,2020;BrownandSandholm,2019;Bakhtinetal.,2022;,FAIR).Thesesystemsutilizeaself-play
framework,equatingthestrategiesofallopponentswiththoseofthelearner. Identicalstrategyassumption
isalsonaturallysatisfiedinmodernmultiplayergamingplatformswithalargeplayerbase(SeeSection4.1).
After making the strategies of opponents equal, one may wonder whether we can find a fixed non-
exploitablepolicysimilartothecaseoftwo-playerzero-sumgames.
Claim 4.2. There exist symmetric zero-sum games such that no fixed strategy guaranteesa non-negative
expectedpayoffagainstadversarialstrategiesby opponents,evenundertheconstraintthattheyadhereto
identicalstrategies.
Proof. ImagineanAIagentinvolvesina3-playermajoritygameandplaysamixedstrategy(β,1 β)(i.e.
−
play0w.p.β;play1w.p.1 β).Andtheothertwoplayersadoptanidenticalmixedstrategy(p,1 p)(i.e.
− −
play0w.p.p;play1w.p.1 p).Then,wecancalculatethepayoffoftheAIagentasU (β,p,p)=β( 2(1
1
− − −
p)2+2p(1 p))+(1 β)( 2p2+2p(1 p)). Itthenfollowsthatmax min U (β,p,p)
β∈[0,1] p∈[0,1] 1
− − − − ≤
max min U (β,p,p)= 1,whichisstrictlylessthan0.
β∈[0,1] p∈{0,1} 1
−
Thisobservationagainmakesmultiplayergamessignificantlydifferentfromtwo-playerzero-sumgames.
Thisleadstothefollowingconclusion:
Toreliablysecureanequalshareinsymmetricgames,anagentmustbeinascenariowhere: (a)opponents
employtheidenticalstrategy,and(b)theAIagentadaptsitsstrategytotheopponents’strategies,which
involvesopponentmodeling.
Itisnotdifficulttofindsolutionconcepts—suchas(1)behaviorcloningforthestrategybyopponents;and
(2) best response to the identical strategy by opponents—thatsatisfy both criteria, and are guaranteed to
secureanequalshare.
Finally, we rigorously summarize our discussion using mathematical language, detailing the relation-
shipsbetweenvariousminimaxquantities.
Proposition4.3. Forsymmetriczero-sumgames,wehave
max min U (x , ,x ) min maxU (x , ,x ) minmaxU (x ,x⊗n−1)=0
1 1 n 1 1 n 1 1
x1 x2,···,xn ··· ≤x2,···,xn x1 ··· ≤ x x1
maxminU (x ,x⊗n−1) minmaxU (x ,x⊗n−1)=0,
1 1 1 1
x1 x ≤ x x1
wherealltheinequalitiescanbemadestrictincertaingames.
4.1 Multiplayergames witha largeplayerbase
Wearguethatassumptionofallopponentsplayingidenticalstrategyiswell-justifiedinmodernmultiplayer
gamingplatformswith a largeplayerbase. Imaginea casino hostingN playerswhorandomlyjoin poker
tables,oranonlineMahjongmatch-makingplatformwithN users.Let x N bethestrategysetforthese
{ i }i=1
N players. We canthendefinethemeta-strategyofthepopulationasx¯ = (1/N) N x . Thefollowing
i=1 i
P
7proposition claims that, for n-player symmetric zero-sum games, as long as N (n 2)2, the payoff
≫ −
achievedbyplayingagainstn 1playersthatuniformlydrawfromtheplayerpoolisalmostthesameas
−
playingagainstn 1playerswhoalladoptthesamemeta-strategyx¯.
−
Proposition4.4. LetE betheexpectationovertherandomnessonsamplingn 1strategiesuniformly
from the set x N wix t− h1 outreplacement. Then for any policy z ∆( ), we ha− ve E [U (z,x )]
{ i }i=1 ∈ A | x−1 1 −1 −
U (z,x¯⊗n−1) 2(n 2)2/N.
1
|≤ −
5 Efficient Algorithms
Inthissection,weexploreefficientalgorithmsthatareabletosecureanaveragepayoffwithinaframework
where all opponents adopt a common strategy yt for each round t [T]. Correspondingly, the payoff
∈
functionoftheAIagentisgivenbyut():=U (,(yt)⊗n−1). Giventhattherulesofthegameareknownto
1
· ·
theplayer,theplayerisactuallyawareofU . Weapproachthisexplorationthroughtwodistinctscenarios:
1
(i) Section 5.1 examines scenarios with a stationary opponent, wherein yt remains the same over time;
(ii) Section 5.2 transitions to considering slowly changing adversarial opponents, limiting their power by
imposingavariationbudget.
5.1 Fixedopponents
Webeginbyexploringthesimplestationaryscenario,wherethecommonstrategyadoptedbytheopponent
remainsconstantovertime,denotedasyt =yforallt [T].
∈
Notably,inthisparticularscenario,thepayofffunctionut()remainsconstantovertime. Additionally,
·
bysymmetry,itisobservablethatmax U (a,y⊗n−1) 0,indicatingtheexistenceofatleastoneaction
a∈A 1
≥
that can consistently yield an average payoff in each round. Thus, any no-regret algorithm is poised to
achievezero payoffunderthissetting. For instance, we willdeploythe Hedgealgorithm, whichprovides
thefollowingguarantees:
Theorem 5.1 (Stationary opponents). Let xt T be the strategy sequence implemented by the Hedge
{ }t=1
algorithmagainststationaryopponents.Then,withprobabilityatleast1 δ,wehave
−
(1/T) T ut(xt) u⋆ C log(A/δ)/T,
t=1 ≥ −
forsomeabsoluteconstantC >,whePreu⋆ :=max U (a,p y⊗n−1) 0.
a∈A 1
≥
Theorem5.1claimsthatwithstationaryopponents,theHedgealgorithmiscapableofachievingapprox-
imatelyanequalsharewhenT islarge,demonstratingitseffectivenessinthelongterm.
5.2 Adaptive opponents
In practicalscenarios, encounteringa fixed opponentstrategy is relativelyuncommon. More often, oppo-
nents adapt and modify their strategies over time, responding to the game’s dynamics and the actions of
other players. Thus, in this section, we shift our focusto the non-stationaryscenario, where the common
strategyytadoptedbytheopponentvariesovertime.
8Challengeswhenfacingfastadaptingopponents Inmultiplayergames,asignificantchallengearisesif
opponentscanchangetheirstrategiesarbitrarilyfastasdemonstratedbythefollowingfact.
Fact 5.2. For a 3-player majority game, there exists a stochastic evolution such that the maximum total
payoffachievablebyanyalgorithmoverthedurationofthegameisatmost T.
−
In light of this negative result, it is clear that attaining an average payoff is impractical if arbitrary
evolution is allowed. Thuswe introducea constraint on this evolutionby positing a variation budgetV ,
T
whichboundsthetotalvariationofthepayofffunctionacrossthetimehorizon.Specifically,weassumethe
payofffunctionbelongsto ,whichisdefinedas
U
:= ut T T−1 ut+1 ut V . (2)
U ({ }t=1 (cid:12) t=1 k − k∞ ≤ T )
(cid:12)
(cid:12) P
(cid:12)
Furthermore,wedenote (n,A,V T)astheset(cid:12)ofsymmetriczero-sumgamesinvolvingnplayers,Aactions,
G
and the payoff function ut T . This constraint effectively moderates the power of the opponents
{ }t=1 ∈ U
comparedtoafullyadversarialsetup.
Slowly adapting opponents Minimizing standard regret is only effective in the stationary environment
when a fixed actionconsistently yieldsa satisfactorypayoff. In non-stationaryenvironments,we turnour
attentiontodynamicregret,definedas:
D-Reg(T):= T max ut(a) T ut(xt).
t=1 a∈A − t=1
Thismeasuresastrategy’sperformanceagPainstthebestactionateaPchtimestep(dynamicoracle),providing
amorerelevantbenchmarkinchangingenvironments.
Note that in the setting of symmetric games, the dynamic oracle is always assured to secure an equal
share,i.e., T max ut(a) 0. Thus,ifanalgorithmcanachieveno-dynamic-regret,thenit’sguar-
t=1 a∈A ≥
anteed to achieve zero expected payoff even as the scenario evolves over time. To this ends, we adapt a
P
no-dynamic-regretalgorithm—StronglyAdaptive Online Learner with Hedge as a blackbox algorithm
(SAOLH),asproposedby(Danielyetal.,2015)tooursettingandachievefolloH
wingguarantees:
Theorem 5.3. Suppose that n 3, A 2, and V [1,T]. Then there exists some absolute constant
T
C > 0, for any fixed game in t≥ he set ≥ (n,A,V ), wi∈ th probability at least 1 δ, SAOLH satisfies the
T
G −
followingD-Reg(T) CV1/3T2/3 log(A/δ)+logT .
≤ T
(cid:16)p (cid:17)
Theorem 5.3 implies that the payoff of SAOLH achieves an average payoff (1/T) T ut(xt)
t=1 ≥
T max ut(a) O˜(V1/3T−1/3). Therefore,ifV issublinearinT,SAOLH iscapableofapproxi-
t=1 a∈A − T T P
matelyachievingzeroexpectedpayoffoveranextendedduration.
P
Middleregime Interesting,thereisaregimeinthemiddlewhereopponentsarechangingreasonablyfast
whilethevalueofdynamicoracle T max ut(a)isnothigh.Inthiscase,thefavorablepolicyforthe
t=1 a∈A
learnermightbesimplybehaviorcloning—simplymimicopponents’strategies.
P
Formally,we define the behaviorcloningalgorithmby the learnermakingher actionin t-throundthe
sameastheactiontakenbythe2ndplayerin(t 1)-thround(SeeAlgorithm1). Behaviorcloningachieves
−
thefollowing:
9Theorem 5.4. Suppose that n 3, A 2, and V [1,T], then behavior cloning guarantees that
T
≥ ≥ ∈
inf E T ut(xt) V 1,wheretheexpectationistakenoverthenoisyactions.
G(n,A,VT) t=1 ≥− T −
h i
Theorem 5.4Pshows that the expected average total payoff achieved by behavior cloning is at least
O( V /T). We note that this can be better than the guarantees of SAOLH when the measurement for
T
−
changeV islargeanddynamicoracle T max ut(a)issmall.
T t=1 a∈A
P
Fundamentallimit Finally, we also complementourupper boundsby matchinglower boundsshowing
thatSAOLHandbehaviorcloningarealreadythenear-optimalalgorithmsintermsofrateswhencompeting
with dynamic oracles and zero payoff respectively. The techniques are based on adapting existing hard
instancesforamoregeneralsetuptothesymmetriczero-sumgamesetting. Pleaseseemorediscussionin
AppendixD.
Theorem5.5. ThereexistssomeabsoluteconstantC >0suchthatforanyn 3,A 2,andV [1,T],
T
≥ ≥ ∈
itholdsthatinf sup E [D-Reg(T)] CV1/3T2/3,wheretheexpectationE []istakenover
Alg G(n,A,VT) Alg ≥ T Alg ·
thenoisyactionsandtheintrinsicrandomnessinthealgorithm.
Theorem 5.6. There exists some absolute constant C > 0 such that for any n 3, A 2, and V
T
≥ ≥ ∈
[1,T],itholdsthatsup inf E T ut(xt) CV ,where xt T isthepolicysequence
Alg G(n,A,VT) Alg t=1 ≤− T { }t=1
implementedby thealgorithm,andtheexpehc PtationE Alg[ ·i] istakenoverthenoisyactionsandtheintrinsic
randomnessinthealgorithm.
6 Experiments
In this section, we focuson the scenariowhere one AI agentcompetesagainstn 1 humanplayerswho
−
playtheidenticalmeta-strategy. Weaimtoanswer: (Q1)Canexistingalgorithmicframeworksinprevious
superhumanAIsystemsconsistentlysecureanequalsharewhencompetingagainsthumanplayersingames?
If not, whatare the patternsofthe failure cases? (Q2)Are these trainedagentsexploitableby adversarial
opponents?Wedesignthefollowingtwogamestocompareourproposedmethodsandotherbaselinesbased
onself-play.
MajorityVote(MV). We first considerthe standardmajorityvotegame(Example1) with 3 players. A
payoffof0.5isevenlydistributedamongthemajority,andapayoffof-0.5isevenlydistributedamongthe
minority. Notethateveryonereceives0ifchoosingthesameaction. Herethemixedstrategyofthehuman
populationisassumedtobeπ =[0.49,0.51].Itcanbeseenthat[1,0],[0,1],and[1/2,1/2]areallNE.
human
SwitchDominanceGame(SDG). Ineachround,playerssimultaneouslychoosean actionfromactions
A,B, orC. LetN bethetotalnumberofplayersandn bethenumberofagentschoosingactionA,We
A
definethegameruleas:
B A C ifn >0.2N,
A
≻ ≻
(C B A otherwise,
≻ ≻
wheretherulei j kintuitivelymeansthatactionidominatesbothj andk,andactionj dominatesk.
≻ ≻
SDGisdesignedsothatC isadominatedactionwhenthereisareasonablenumberofplayerstakingaction
10A,butadominatingactionotherwise. Concretely,fori j k,weusethepayoffdefinedas:
≻ ≻
r =I[n +n >0],
i j k
n
r =I[n >0] I[n +n >0] i ,
j k j k
− n +n
j k
n n
r = I[n +n >0] i I[n >0] j .
k j k k
− n +n − n
j k k
This payoff design guarantees that SDG is a zero-sum game. Throughout our experiments, we choose
N = 30andthehumanmeta-strategyπ = [0.399,0.6,0.001](intheorderofA,B,andC). Notethat
human
whilethisgamehasanNEpolicy[0,0,1],itsutilityisnegativeagainstallotherplayersplayingπ .
human
6.1 Learning algorithms
Tobetterfocusonthegame-theoreticalpartofthealgorithms,weidealizetheprocessofimitationlearning
byassumingthattheagenthasaccesstothehumanpopulationpolicyπ .Weaimtocompareoursolution
human
concepts—bestresponsetobehaviorcloning(BR BC),withthreemeta-algorithmsadoptedpreviouslyto
build superhuman AI agents: self-play from scratch (SP scratch) BrownandSandholm (2019), self-play
from behavior cloning (SP BC) Lietal. (2020), and self-play from behavior cloning with regularization
towardshumanbehavior(SP BC reg)Jacobetal.(2022). WhiletheseAIsystemsfurtherimplementmulti-
steplookaheadwithafewadditionaltechniques,manyofthemonlyapplytosequentialgames,ratherthan
basicnormal-formgames. Here,wefocusonthecomparisonofthehigh-levelmeta-algorithms.
Algorithmdetails. WeusetheHedgealgorithmasthebackboneforself-play.Comparedwiththevanilla
SP scratch which is initialized with the uniform action distribution, SP BC starts from the human policy
π , while SP BC reg (Jacobetal., 2022) further regularizes the KL divergence between the learned
human
policyandπ duringtraining.TheHedgealgorithmalsoprovidesanaturalwaytolearnthebestresponse
human
inBR BC. We choosethe learningratefortheHedgealgorithmbasedontheoreticallyoptimalvalue,and
choose the regularizationparameter accordingto Jacobetal. (2022). We refer readersto AppendixE for
moredetails.
6.2 Results
To answer Q1 and Q2, we evaluate the utility of the learned strategy against the human population, i.e.,
U (π ,π⊗(n−1)), as well as the exploitabilityof π , i.e., min U (π ,π⊗(n−1)). To measure the
1 agent human agent π 1 agent
utility,weevaluatethepayoffoftheagent’sconvergedpolicybyMonteCarlomethodswith3 105games,
×
andreportthemeanandstandarddeviationof10runs. Asfortheexploitability,wepickthebestexploiter
policywithin100runsandreportthecorrespondingutility.
Convergence analysis. Before comparing the utility and exploitability, we first check the convergence
policyforeachalgorithm:
(1)MV:WenoticethatBR BCconsistentlyconvergestothepolicy[0,1],whileself-playvariantsapproach
different equilibria across different runs, as summarized in Table 1. Therefore, all self-play variants can
convergetoasuboptimalstrategy[1,0]withanon-negligibleprobabilityandsufferanegativepayoffagainst
thehumanpopulationpolicy[0.49,0.51]. Basedonthisanalysis,wereporttheutilityandexploitabilityof
11MV SP scratch SP BC SP BC reg(λ=10−5/10−4/10−3/10−2)
[1,0] 52 48 48
[0,1] 48 52 52
Table 1: The distribution of convergencepolicy for self-play algorithmsin 100 runs. For SP BC reg, we
reportresultswithdifferentregularizationcoefficientsλ.
MV SP scratch/SP BC/SP BC reg BR BC
Utility( 10−2) -0.50 0.05 0.52 0.05
× ± ±
Exploitability -0.50 0.00 -0.50 0.00
± ±
SDG SP scratch/SP BC/SP BC reg BR BC
Utility -12.67 0.01 1.00 0.00
± ±
Exploitability -29.00 0.00 -29.00 0.00
± ±
Table 2: The utility and exploitability of each algorithm. Particularly, for MV, as self-play algorithms
convergeto differentsolutions, we evaluate the suboptimal one with the negative utility, i.e., [1, 0]. The
results show that in two well-designed games, all algorithms except for BR BC achieve negative payoffs
evenagainstnon-adaptivehumans,andallthelearnedpoliciescanbeexploited.
theconvergencepolicy[0,1]forBR BC,andthesuboptimalpolicy[1,0]foralgorithmsbasedonself-play.
(2) SDG: We notice that self-play algorithms consistently convergeto the policy [0, 0, 1], while BR BC
converges to the policy [0, 1, 0]. Based on this analysis, we report the utility and exploitability of the
convergencepolicy[0,0,1]foralgorithmsbasedonself-play,andthepolicy[0,1,0]forBR BC.
UtilityandExploitability. WesummarizetheresultsinTable2,whichshowthateveninthesetwosimple
cases, none of the self-play algorithmscan consistently secure an equal share in symmetric constant-sum
games. Thisundesirablebehaviorpersistsevenwithouthumansmakinganyadaptations! Moreover,based
onthesetwogames,wefurtherconcludetwopotentialfailuremodesofself-playalgorithms:(1)Forgames
withmultipleNE,self-playmethodsbasedonno-regretalgorithmsmayconvergetodifferentNEsaccording
todifferentinitialpolicies.Whenhumanpolicy(theinitialpolicyforSP BC)liesclosetotheboundaryofthe
convergencebasinsoftwodifferentNEs,self-playalgorithmswillhaveanon-zeroprobabilitytoconverge
to both of them due to the statistical randomness in the game. It is likely one of the two NEs is highly
suboptimal against π . Admittedly, this statistical randomness can be, to some extent, mitigated by
human
pickingasufficientlysmalllearningrate. However,theclosertheinitialpolicytotheboundary,thesmaller
thelearningrateweneed,whichquicklymakesthelearningratetoosmalltoconvergewithinareasonable
numberofsamplesinpractice.(2)EvenforagamewithasingleNE,acarefullydesignedgamestructurecan
resultinthisNEyieldinganegativeutilitywhencomparedtoaspecifichumanpolicy,andhencejailbreakall
self-playvariants. Theaforementionedfailuremodeshighlightasignificantlimitationinself-playvariants’
capabilitytogeneralizeeffectivelytodiverseandcomplexgamescenarios. Incontrast,thesolutionBR BC
indicatedbyour theoryconsistentlyoutperformshumanpoliciesandreceivesmuchhigherutility. On the
exploitabilityside,alllearnedpoliciescanbeexploitedeasilybyadversarialopponents.
127 Conclusion
Thispapertakestheinitialsteptowardsaddressinguniquechallengesinmultiplayergamesbyinvestigating
symmetriczero-sumnormal-formgames,whichcommonlyappearinourdailylives. Weclarifyanumber
ofimportantsolutionconceptsformultiplayergames,andinvestigatethebehaviorsandlimitationsofmany
existingmeta-algorithmsthatweredeployedbypreviousstate-of-the-artAIsystemsforgames.Wehopeour
resultspromotefurtherresearchinprincipledmethodologiesandalgorithmsthatleadtogeneralsuperhuman
AIforawiderangeofmultiplayergames.
Acknowledgement
The authorswould like to thank HaifengXu for helpfuldiscussions. Thiswork is supportedby Office of
NavalResearchN00014-22-1-2253.
References
Bai, Y., Jin, C., Mei, S., Song, Z., and Yu, T. (2022a). EfficientΦ-regretminimizationin extensive-form
gamesviaonlinemirrordescent. arXivpreprintarXiv:2205.15294.
Bai,Y.,Jin,C.,Mei,S.,andYu,T.(2022b). Near-optimallearningofextensive-formgameswithimperfect
information. arXivpreprintarXiv:2202.01752.
Bakhtin, A., Wu, D. J., Lerer, A., Gray, J., Jacob, A. P., Farina, G., Miller, A. H., and Brown, N. (2022).
Masteringthegameofno-pressdiplomacyvia human-regularizedreinforcementlearningandplanning.
arXivpreprintarXiv:2210.05492.
Berner,C.,Brockman,G.,Chan,B.,Cheung,V.,Debiak,P.,Dennison,C.,Farhi,D.,Fischer,Q.,Hashme,
S., Hesse, C., et al. (2019). Dota 2 with large scale deep reinforcement learning. arXiv preprint
arXiv:1912.06680.
Besbes, O., Gur, Y., and Zeevi, A. (2014). Stochastic multi-armed-bandit problem with non-stationary
rewards. Advancesinneuralinformationprocessingsystems,27.
Brown,N.andSandholm,T.(2018). Superhumanaiforheads-upno-limitpoker:Libratusbeatstopprofes-
sionals. Science,359(6374):418–424.
Brown,N.andSandholm,T.(2019). Superhumanaiformultiplayerpoker. Science,365(6456):885–890.
Campbell,M.,HoaneJr,A.J.,andHsu,F.-h.(2002). Deepblue. Artificialintelligence,134(1-2):57–83.
Celli, A., Marchesi, A., Farina, G., andGatti, N. (2020). No-regretlearningdynamicsforextensive-form
correlatedequilibrium. AdvancesinNeuralInformationProcessingSystems,33:7722–7732.
Cesa-Bianchi,N.andLugosi,G.(2006). Prediction,learning,andgames. Cambridgeuniversitypress.
Chen, X. and Deng, X. (2005). 3-nash is ppad-complete. In Electronic Colloquium on Computational
Complexity,volume134,pages2–29.Citeseer.
13Chen,X.andPeng,B.(2020).Hedgingingames:Fasterconvergenceofexternalandswapregrets.Advances
inNeuralInformationProcessingSystems,33:18990–18999.
Daniely,A.,Gonen,A.,andShalev-Shwartz,S.(2015). Stronglyadaptiveonlinelearning. InInternational
ConferenceonMachineLearning,pages1405–1411.PMLR.
Daskalakis, C. (2009). Nash equilibria: Complexity, symmetries, and approximation. Computer Science
Review,3(2):87–100.
Daskalakis,C.,Fishelson,M.,andGolowich,N.(2021). Near-optimalno-regretlearningingeneralgames.
AdvancesinNeuralInformationProcessingSystems,34:27604–27616.
Daskalakis, C., Goldberg, P. W., and Papadimitriou, C. H. (2009). The complexity of computing a nash
equilibrium. SIAMJournalonComputing,39(1):195–259.
Daskalakis,C.andPanageas,I.(2018).Last-iterateconvergence:Zero-sumgamesandconstrainedmin-max
optimization. arXivpreprintarXiv:1807.04252.
(FAIR)†, M. F. A. R. D. T., Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff,
A., Gray,J., Hu, H., etal.(2022). Human-levelplayin thegameofdiplomacybycombininglanguage
modelswithstrategicreasoning. Science,378(6624):1067–1074.
Gray,J.,Lerer,A.,Bakhtin,A.,andBrown,N.(2020).Human-levelperformanceinno-pressdiplomacyvia
equilibriumsearch. arXivpreprintarXiv:2010.02923.
Hart,S.andMas-Colell,A.(2000). Asimpleadaptiveprocedureleadingtocorrelatedequilibrium. Econo-
metrica,68(5):1127–1150.
Jacob, A. P., Wu, D. J., Farina, G., Lerer, A., Hu, H., Bakhtin, A., Andreas, J., and Brown, N. (2022).
Modeling strong and human-likegameplay with kl-regularizedsearch. In InternationalConference on
MachineLearning,pages9695–9728.PMLR.
Jin, C., Liu, Q., Wang, Y., and Yu, T. (2021). V-learning–asimple, efficient, decentralized algorithm for
multiagentrl. arXivpreprintarXiv:2110.14555.
Li, J., Koyamada, S., Ye, Q., Liu, G., Wang, C., Yang, R., Zhao, L., Qin, T., Liu, T.-Y., and Hon, H.-W.
(2020). Suphx:Masteringmahjongwithdeepreinforcementlearning. arXivpreprintarXiv:2003.13590.
Light,J.,Cai,M.,Shen,S.,andHu,Z.(2023). Avalonbench: Evaluatingllmsplayingthegameofavalon.
InNeurIPS2023FoundationModelsforDecisionMakingWorkshop.
Moravcˇ´ık, M., Schmid, M., Burch, N., Lisy`, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Johanson,
M., andBowling, M. (2017). Deepstack: Expert-levelartificialintelligencein heads-upno-limitpoker.
Science,356(6337):508–513.
Nash,J.(1951). Non-cooperativegames. Annalsofmathematics,pages286–295.
Papadimitriou,C.H.andRoughgarden,T.(2005). Computingequilibriainmulti-playergames. InSODA,
volume5,pages82–91.Citeseer.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J.,
Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with deep
neuralnetworksandtreesearch. nature,529(7587):484–489.
14Song,Z.,Mei,S.,andBai,Y.(2021). Whencanwelearngeneral-summarkovgameswithalargenumber
ofplayerssample-efficiently? arXivpreprintarXiv:2110.04184.
Song, Z., Mei, S., andBai, Y. (2022). Sample-efficientlearningofcorrelatedequilibriain extensive-form
games. arXivpreprintarXiv:2205.07223.
Syrgkanis,V., Agarwal,A.,Luo,H.,andSchapire,R.E.(2015). Fastconvergenceofregularizedlearning
ingames. AdvancesinNeuralInformationProcessingSystems,28.
Vinyals,O.,Babuschkin,I.,Czarnecki,W.M.,Mathieu,M.,Dudzik,A.,Chung,J.,Choi,D.H.,Powell,R.,
Ewalds,T.,Georgiev,P.,etal.(2019). Grandmasterlevelinstarcraftiiusingmulti-agentreinforcement
learning. Nature,575(7782):350–354.
VonNeumann,J.andMorgenstern,O.(1947). Theoryofgamesandeconomicbehavior,2ndrev.
Wei, C.-Y., Lee, C.-W., Zhang, M., and Luo, H. (2020). Linear last-iterate convergence in constrained
saddle-pointoptimization. arXivpreprintarXiv:2006.09517.
Ye, D., Chen, G., Zhang, W., Chen, S., Yuan, B., Liu, B., Chen, J., Liu, Z., Qiu, F., Yu, H., et al. (2020).
Towards playing full moba games with deep reinforcement learning. Advances in Neural Information
ProcessingSystems,33:621–632.
Young,H.P.(2004). StrategicLearninganditsLimits. OxfordUniversityPress.
Zha,D.,Xie,J.,Ma,W.,Zhang,S.,Lian,X.,Hu,X.,andLiu,J.(2021).Douzero:Masteringdoudizhuwith
self-play deep reinforcementlearning. In internationalconferenceon machinelearning, pages 12333–
12344.PMLR.
15A Proofs for Section 4
A.1 3-playermajority andminority game
Inthispaper,wedefinethe3-playermajoritygameasasymmetriczero-sumgamewithactionspace :=
A
0,1 andthepayofffunctiongivenby:
{ }
U (0,0,0)=U (1,1,1)=0
1 1
U (0,1,0)=U (0,0,1)=U (1,1,0)=U (1,0,1)=1
1 1 1 1
U (0,1,1)=U (1,0,0)= 2.
1 1
−
Inotherwords,playersreceiveapositivepayoffiftheyarepartofthemajorityandanegativepayoffifthey
areintheminority. Correspondingly,wedefinethe3-playerminoritygameasasymmetriczero-sumgame
withactionspace := 0,1 andthepayofffunctiongivenby:
A { }
U (0,0,0)=U (1,1,1)=0
1 1
U (0,1,0)=U (0,0,1)=U (1,1,0)=U (1,0,1)= 1
1 1 1 1
−
U (0,1,1)=U (1,0,0)=2.
1 1
Inotherwords,playersreceiveapositivepayoffiftheyarepartoftheminorityandanegativepayoffifthey
areinthemajority.
A.2 ProofofProposition4.3
ProofofProposition4.3. Firstofall,weshowthat
max min U (x , ,x ) min maxU (x , ,x ) minmaxU (x ,x⊗n−1)=0,
1 1 n 1 1 n 1 1
x1 x2,···,xn ··· ≤x2,···,xn x1 ··· ≤ x x1
wherealltheinequalitiescanbestrict.
Forthefirstinequality,notethatforany(x ,...,x ):
1 n
U (x , ,x ) maxU (x , ,x ),
1 1 n 1 1 n
··· ≤ x1 ···
whichimplies
min U (x , ,x ) min maxU (x , ,x ).
1 1 n 1 1 n
x2,···,xn ··· ≤x2,···,xn x1 ···
Byfurthertakingmaximumoverx ∆( ),weprovethat
1
∈ A
max min U (x , ,x ) min maxU (x , ,x ).
1 1 n 1 1 n
x1 x2,···,xn ··· ≤x2,···,xn x1 ···
Thesecondinequalityisstraightforwardduetoarestrictionontheminimizationconstraints. Inthesequel,
weprovetheequationviacontradiction.Notethatbychoosingx =x,wecanshowthat
1
min maxU (x ,x, ,x) 0.
1 1
x,···,x x1 ··· ≥
Supposeforsomegameinequalityholds,thenbydefinition
x ∆( ), x′ ∆( ),s.t.U (x′,x, ,x)>0.
1
∀ ∈ A ∃ ∈ A ···
16Definetheset-valuedargmaxfunctionφ:∆( ) 2∆(A):
A →
φ(x):= x′ ∆( ) U (x′,x, ,x)=maxU (x′′,x, ,x) .
1 1
{ ∈ A | ··· x′′ ··· }
Weclaimthatargmaxfunctionφ(x)is:
• Alwaysnon-emptyandconvex;
• Hasaclosedgraph.
Thefirstpropertyisobvious,sowefocusonthesecondone.Supposethatsequences x , y satisfyx
i i i
{ } { } →
x,y
i
yandy
i
φ(x i). Sincethepayofffunctionis(Lipschitz)continuous,max x′′U 1(x′′, )iscontinu-
→ ∈ ·
ousbyBerge’smaximumtheorem.Thusmax x′′U 1(x′′,x i, ,x i)convergestomax x′′U 1(x′′,x, ,x).
··· ···
MeanwhileU (y ,x , ,x )convergestoU (y,x, ,x). Thus
1 i i i 1
··· ···
U (y,x ,x)= lim U (y ,x , ,x )= lim maxU (x′′,x , ,x )=maxU (x′′,x, ,x).
1 1 i i i 1 i i 1
··· i→∞ ··· i→∞ x′′ ··· x′′ ···
Thisimpliesy φ(x),andthatφhasaclosedgraph. ThusbyKakutani’sfixedpointtheorem, x∗ : x∗
∈ ∃ ∈
φ(x∗). Nowwehave
U (x∗, ,x∗)=maxU (x′′,x∗, ,x∗)>0,
1 1
··· x′′ ···
whichcontradictswiththeassumptionthatthegameiszero-sumandsymmetric. Consequently,weprove
theequation.
InClaim4.1,wehaveprovedthatthesecondinequalitycanbestrict. Toshowthefirstinequalitycanbe
strict, we considerthe3-playermajorityvote. Suppose3 playersadoptthemixedstrategies(α ,1 α ),
1 1
−
(α ,1 α )and(α ,1 α ),respectively.Itthenholdsthat
2 2 3 3
− −
U (x ,x ,x )
1 1 2 3
=U (α ,α ,α )
1 1 2 3
=α ( 2(1 α )(1 α )+α (1 α )+α (1 α ))
1 2 3 2 3 3 2
− − − − −
+(1 α )( 2α α +α (1 α )+α (1 α )).
1 2 3 2 3 3 2
− − − −
Bychoosingα =α =0whenα >1/2andα =α =1whenα 1/2,itcanbeseenthat
2 3 1 2 3 1
≤
max min U (α ,α ,α ) maxmin 2α , 2(1 α ) = 1.
1 1 2 3 1 1
α1 α2,α3 ≤ α1 {− − − } −
Notethat
min maxU (α ,α ,α )
1 1 2 3
α2,α3 α1
= min max 2(1 α )(1 α )+α (1 α )+α (1 α ), 2α α +α (1 α )+α (1 α )
2 3 2 3 3 2 2 3 2 3 3 2
α2,α3 {− − − − − − − − }
= min max 3(α +α ) 4α α 2,α +α 4α α
2 3 2 3 2 3 2 3
α2,α3 { − − − }
=0.
Thus,weshowthatmax min U (α ,α ,α )<min max U (α ,α ,α ).
α1 α2,α3 1 1 2 3 α2,α3 α1 1 1 2 3
17Wethenshowthat
max min U (x ,x⊗n−1) min max U (x ,x⊗n−1)=0,
1 1 1 1
x1∈∆(A)x∈∆(A) ≤x∈∆(A)x1∈∆(A)
wheretheinequalitycanbestrict.
Notethatforanyx ,x ∆( ),wehave
1
∈ A
U (x ,x⊗n−1) max U (x ,x⊗n−1),
1 1 1 1
≤x1∈∆(A)
whichimpliesforanyx ∆( )
1
∈ A
min U (x ,x⊗n−1) min max U (x ,x⊗n−1).
1 1 1 1
x∈∆(A) ≤x∈∆(A)x1∈∆(A)
Byfurthertakingmaximumoverx ∆( ),weshowthat
1
∈ A
max min U (x ,x⊗n−1) min max U (x ,x⊗n−1).
1 1 1 1
x1∈∆(A)x∈∆(A) ≤x∈∆(A)x1∈∆(A)
InClaim4.2,wehaveprovedthattheinequalitycanbestrict.
A.3 ProofofProposition4.4
ProofofProposition4.4. Let Pw/o(i ,...,i ) denote the probabilityof observing(i ,...,i ) when
1 n−1 1 n−1
sampling n 1 points from N without replacement, and let Pw(i ,...,i ) denote the probability of
1 n−1
−
observing(i ,...,i )whensamplingn 1pointsfromN withreplacement.Foranya,wethenhave
1 n−1
−
E [U (a,x )]= Pw/o(i ,...,i )U (a,x ,...,x )
x−1 1 −1 1 n−1 1 i1 in−1
(i1,.X..,in−1)
U (a,x¯⊗n−1)= Pw(i ,...,i )U (a,x ,...,x ).
1 1 n−1 1 i1 in−1
(i1,.X..,in−1)
18Notethat U 1. Thus,wehave
1 ∞
k k ≤
E [U (a,x )] U (a,x¯⊗n−1)
x−1 1 −1
−
1
(cid:12) Pw/o(i ,...,i ) (cid:12)Pw(i ,...,i )
(cid:12)≤ 1 n−1 −(cid:12) 1 n−1
(i1,.X..,in−1)(cid:12) (cid:12)
(cid:12) (cid:12)
= (cid:12) Pw(i ,...,i ) Pw/o(i(cid:12),...,i )
1 n−1 1 n−1
−
(i1,...,in−1)Xhasrepeatedvalue
+ Pw/o(i ,...,i ) Pw(i ,...,i )
1 n−1 1 n−1
−
(i1,...,in−1)Xnorepeatedvalue
=2 Pw(i ,...,i ) Pw/o(i ,...,i )
1 n−1 1 n−1
−
(i1,...,in−1)Xhasrepeatedvalue
N(N 1)...(N n+2)
=2 1 − −
− Nn−1
(cid:18) (cid:19)
1 2 n 2
=2 1 1 1 ... 1 −
− − N − N − N
(cid:18) (cid:18) (cid:19)(cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
n−2
n 2
2 1 1 −
≤ − (cid:18) − N (cid:19) !
2(n 2)2
− .
≤ N
B Proofs for Section 5.1
B.1 ProofofTheorem 5.1
ProofofTheorem5.1. Leta⋆ argmax U (,y⊗n−1). Wethenhave
∈ a∈A 1 ·
T
1
u⋆ ut(xt)
− T
t=1
X
T
1
=U (a⋆,y⊗n−1) ut(xt)
1
− T
t=1
X
T T T
1 1 1
=U (a⋆,y⊗n−1) U (a⋆,at )+ U (a⋆,at ) U (xt,at )
1 − T 1 −1 T 1 −1 − T 1 −1
t=1 t=1 t=1
X X X
(i) (ii)
| 1 T {z 1 T } | {z }
+ U (xt,at ) ut(xt)
T 1 −1 − T
t=1 t=1
X X
(iii)
| {z }
19For(i),byHoeffding’sinequalityandunionbound,wehavewithprobabilityatleast1 δthat
−
log(A/δ)
(i) O( )
≤ T
r
For(ii),byHedgealgorithm,wehave
log(A)
(ii) O( )
≤ T
r
For(iii),notethat U (xt,at ) ut(xt) T isamartingaledifferencesequence,thusbyAzuma–Hoeffding
{ 1 −1 − }t=1
inequality,wehavewithprobabilityatleast1 δthat
−
log(1/δ)
(iii) O( ).
≤ T
r
Combiningtheaboveresults,wehave
T
1 log(A/δ)
u⋆ ut(xt) C
− T ≤ T
r
t=1
X
forsomeabsoluteconstantC >0. Thuswefinishtheproof.
C Proofs for Section 5.2
C.1 ProofofFact5.2
ProofofFact5.2. TheproofofFact5.2followsdirectlyfromtheconstructioninTheorem5.6withn = 3,
= 0,1 and∆ =1.
T
A { }
C.2 ProofofTheorem 5.3
ProofofTheorem5.3. ThebasicideabehindSAOLHistoexecute inparallelovereachintervalwithina
H
carefullyselectedset. Thisalgorithmdynamicallyadjuststheweightofeachintervalbasedonthepreviously
observedregret. Ineachround,SAOLH selectsanintervalinproportiontoitsassignedweight,applies
toeachtimeslotwithinthisinterval,andfollowsitsadvice. Throughthismechanism,SAOLH achievesH a
near-optimalperformanceoneverytimeinterval. WewillleveragethestrongadaptivityofSAOLH inour
proofs.
Let be anyfixed intervalin [0,T], a argmax ut(a) and ut,⋆ := max ut(a). It
I 0 ∈ a∈A t∈I a∈A
holdsthat
(cid:8)P (cid:9)
ut,⋆ ut(xt)
−
t∈I
X(cid:0) (cid:1)
= ut,⋆ ut(a ) + ut(a ) U (a ,at )
− 0 0 − 1 0 −1
t∈I t∈I
X(cid:0) (cid:1) X(cid:0) (cid:1)
(i)
+| U{z(a ,at )} U (xt,at ) + U (xt,at ) ut(xt)
1 0 −1 − 1 −1 1 −1 −
t∈I t∈I
X(cid:0) (cid:1) X(cid:0) (cid:1)
(ii)
| {z }
20For(i),itcanbeseenthat
(i)= ut,⋆ ut(a ) max ut,⋆ ut(a ) 2V .
0 0 I
− ≤|I| t∈I − ≤ |I|
t∈I
X(cid:0) (cid:1) (cid:8) (cid:9)
Here the last inequality follows from the following argument: otherwise there exists t such that
0
∈ I
ut0,⋆ ut0(a ) > 2V . Leta argmax ut0(a). Forallt ,itthenholdsthatut(a ) ut0(a )
− 0 I 1 ∈ a∈A ∈ I 1 ≥ 1 −
V =ut0,⋆ V >ut0(a )+V ut(a ). Contradicttothedefinitionofa !
I I 0 I 0 0
− ≥
For(ii),wehave
(ii) max U (a,at ) U (xt,at ) C( logA+logT) ,
≤ a∈A 1 −1 − 1 −1 ≤ |I|
t∈I
X(cid:0) (cid:1) p p
wherethelastinequalityfollowsfromTheorem1in(Danielyetal.,2015).
Combiningtheupperboundof(i)and(ii),wehaveforanyfixedinterval [0,T],
I ⊂
ut,⋆ ut(xt)
−
t∈I
X(cid:0) (cid:1)
2V + ut(a ) U (a ,at ) +C( logA+logT) + U (xt,at ) ut(xt) .
≤ I |I| 0 − 1 0 −1 |I| 1 −1 −
t∈I t∈I
X(cid:0) (cid:1) p p X(cid:0) (cid:1)
WesegmentthetimehorizonT intoT/ batches witheachlength . Itthenholdsforalljthat
j
|I| {I } |I|
ut,⋆ ut(xt)
−
t X∈Ij(cid:0)
(cid:1)
2V + ut(a ) U (a ,at ) +C( logA+logT) + U (xt,at ) ut(xt) .
≤ Ij|I| 0 − 1 0 −1 |I| 1 −1 −
t X∈Ij(cid:0)
(cid:1) p p
t X∈Ij(cid:0)
(cid:1)
Sumoverj gives
D-Reg(T)
T T
2V + ut(a ) U (a ,at ) +C(T/ ) ( logA+logT)+ U (xt,at ) ut(xt) .
≤ T |I| 0 − 1 0 −1 |I| · 1 −1 −
t=1 t=1
X(cid:0) (cid:1) p p X(cid:0) (cid:1)
(iii) (iv)
| {z } | {z }
For(iii),notethat ut(a) U (a,at ) T isamartingaledifferencesequence,wehavewithprobability
{ − 1 −1 }t=1
atleast1 δthat
−
T
(iii) max ut(a) U (a,at ) O T log(A/δ) ,
≤ a∈A − 1 −1 ≤
Xt=1 (cid:0) (cid:1) (cid:16)p (cid:17)
wherethelastinequalityfollowsfromAzuma–Hoeffdinginequalityandunionbound.
For(iv),notethat U (xt,at ) ut(xt) T isamartingaledifferencesequence,thusbyAzuma–Hoeffding
{ 1 −1 − }t=1
inequality,wehavewithprobabilityatleast1 δthat
−
(iv) O T log(1/δ) .
≤
(cid:16)p (cid:17)
21Algorithm1BehaviorCloning
1: Inthefirstround,playa Uniform( ).
∼ A
2: fort=2,...,T do
3: Playat−1,i.e. theactionplayedbyPlayer2inthelastround.
2
4: endfor
Consequentlywehavewithprobabilityatleast1 δthat
−
D-Reg(T) 2V +C(T/ ) ( logA+logT)+O Tlog(A/δ) .
T
≤ |I| |I| ·
p p (cid:16)p (cid:17)
Choosing =(T/V )2/3,wehavewithprobabilityatleast1 δthat
T
|I| −
D-Reg(T) O V1/3T2/3( log(A/δ)+logT) .
≤ T
(cid:16) p (cid:17)
C.3 ProofofTheorem 5.4
ProofofTheorem5.4. Notethat
T T
E ut(xt) 1+E ut(xt)
" #≥− " #
t=1 t=2
X X
T
= 1+E U (at−1,(yt)⊗n−1)
− " 1 2 #
t=2
X
T
1 V E U (at−1,(yt−1)⊗n−1) (bythedefitionofV )
≥− − T − " 1 2 # T
t=2
X
T
= 1 V E U (yt−1,(yt−1)⊗n−1) (sinceat−1 yt−1)
− − T − " 1 # 2 ∼
t=2
X
= 1 V (sincethegameissymmetricandzero-sum)
T
− −
Thuswefinishtheproof.
D Fundamental limits
UponexaminingTheorem5.3 alongsideTheorem5.4, it becomesapparentthatTheorem5.3 benchmarks
against a more stringent standard (i.e., the dynamicoracle) and incursa larger regret of V1/3T2/3, while
T
Theorem 5.4 sets its comparison against a baseline metric (i.e., the average payoff) and attains a smaller
regretof V . Regardingthis observation, one mightaspire to devise an algorithmwhose payoffsatisfies:
T
T ut(xt) T max ut(a) O˜(V ).However,Theorem5.5andTheorem5.6demonstratethat
t=1 ≥ t=1 a∈A − T
suchagoalisunattainable,byexploringthefundamentallimitsfacedwhencompetingagainstnon-stationary
P P
opponents.
22Theorem5.5shows, whencontendingwith non-stationaryopponent,theoptimalalgorithmmustincur
a dynamic regret at least order of V1/3T2/3, closing off the possibility of attaining a better V rate. It’s
T T
noteworthythatasimilarlowerboundfordynamicregrethasalreadybeenestablishedunderbroadercon-
ditionsBesbesetal.(2014). ThedistinctionofTheorem5.5liesinfurtherrestrictingthehardproblemsto
besymmetricgames,implyingthatthestructureofsymmetricgamedoesnotofferanadvantageinimprov-
ingdynamicregretinthe worstcase. By comparingthislowerboundwith Theorem5.3, it isevidentthat
SAOLHisdemonstratedtobeminimaxoptimal,albeitwiththeinclusionofsomelogarithmicfactors.
Theorem 5.6 establishes the fundamentallimit when comparing to average payoff 0. The guarantees
achievedbyTheorem5.4cannotbeimprovedintheworstcase,showingbehaviorcloningisdemonstrated
tobeoptimaluptosomeconstant.
D.1 ProofofTheorem 5.5
ProofofTheorem5.5. Wedefine
payofffor3-playermajoritygame ifa,b,c 0,1
∈{ }
U(3)(a,b,c):= 2 ifa / 0,1 ,b,c 0,1
1  − ∈{ } ∈{ }
definedbysymmetric o.w.
whichisbasicallythepayofffunctionfor3-playermajoritygamewithextradummyactions.Wethendefine
1
U(n)(a,a ,...,a ):= U(3)(a,a ,a ).
1 2 n (n 1)(n 2) 1 i j
− − 2≤i6=j≤n
X
Weconsideragamethatevolvesstochastically,withnplayers,actionspace = 0,1,...,A 1 ,andthe
A { − }
payofffunctionof the first playergivenbyU(n). We segmentthe decisionhorizonT into T/∆ batches
1 T
,witheachbatchcomprising∆ episodes. Weconsidertwodistinctscenarios:
j T
{T }
• Case1:Alltheotherplayersemployamixturestrategy(1/2 ǫ,1/2+ǫ)(i.e.,playing0withproba-
−
bility1/2 ǫ,playing1withprobability1/2+ǫ);
−
• Case 2: All the other players employ a mixture strategy (1/2 + ǫ,1/2 ǫ) (i.e., playing 0 with
−
probability1/2+ǫ,playing1withprobability1/2 ǫ);
−
At the beginning of each batch, one of these scenarios is randomly selected (with equal probability) and
remainsconstantthroughoutthatbatch.
Letm=T/∆ representtotalnumberofbatches. Wefixsomealgorithmandabatchj 1,...,m .
T
Letδ 1,2 indicatebatchj belongstoCase1orCase2. We denotebyPj theprobabili∈ ty{ distributio} n
condij tio∈ ne{ don} batchj belongstoCaseδ ,andbyP theprobabilitydistributiδ oj nwhenalltheotherplayers
j 0
employamixturestrategy(1/2,1/2).WefurtherdenotebyEj []andE []thecorrespondingexpectations.
δj · 0 ·
WedenotebyNj thenumberoftimesactionawasplayedinbatchj. IfthebatchjbelongstoCaseδ ,then
a j
theoptimalactioninthebatchis δ +2. Wefirstpresentausefullemma.
j
−
LemmaD.1. Letf : 2,0,1 |Tj|×A [0,M]beanyboundedrealfunctiondefinedonthepayoffmatrices
{− } →
R. Then,foranyδ 1,2 ,ǫ 1/4:
j
∈{ } ≤
M
Ej [f(R)] E [f(R)] 2 ln(1 4ǫ2) 2Mǫ ∆ .
δj − 0 ≤ 2 − |Tj | − ≤ T
q p
23ByLemmaD.1withf =Nj ,wehave
−δj+2
Ej [Nj ] E [Nj ] 2ǫ ∆ . (3)
δj −δj+2 − 0 −δj+2 ≤ |Tj | T
Notethat p
Ej [ut(xt)]= 2Pj (xt / 0,1 )+( 2ǫ 4ǫ2)Pj (xt =δ 1)+(2ǫ 4ǫ2)Pj (xt = δ +2)
δj − δj ∈{ } − − δj j − − δj − j
( 2ǫ 4ǫ2)Pj (xt = δ +2)+(2ǫ 4ǫ2)Pj (xt = δ +2)
≤ − − δj 6 − j − δj − j
= 2ǫ 4ǫ2+4ǫ Pj (xt = δ +2),
− − · δj − j
therefore,
Ej ut(xt) ( 2ǫ 4ǫ2) +4ǫ Ej [Nj ]
δj  ≤ − − |Tj | · δj −δj+2
t X∈Tj
  ( 2ǫ 4ǫ2) +4ǫ Ej[Nj ]+8ǫ2 ∆ . (by(3))
≤ − − |Tj | · 0 −δj+2 |Tj | T
Consequently,wehave p
1 1
Ej ut(xt) + Ej ut(xt) ( 2ǫ 4ǫ2) +2ǫ +8ǫ2 ∆ (4)
2 1  2 2 ≤ − − |Tj | |Tj | |Tj | T
t X∈Tj t X∈Tj
p
   
Itthenholdsthat
m m
E ut(xt) = E ut(xt)
Alg Alg
   
Xj=1t X∈Tj Xj=1 t X∈Tj
   
m
1 1
= E Ej ut(xt) + Ej ut(xt)
Alg 2 1  2 2 
Xj=1 t X∈Tj t X∈Tj
m     
(( 2ǫ 4ǫ2) +2ǫ +8ǫ2 ∆ )
j j j T
≤ − − |T | |T | |T |
j=1
X p
= 4ǫ2T +8ǫ2T ∆ .
T
−
Setǫ=min 1/(8√∆ ),V ∆ /T .Wethenhave p
T T T
{ }
T
E [D-Reg(T)]=(2ǫ 4ǫ2)T E ut(xt)
Alg Alg
− − " #
t=1
X
(2ǫ 4ǫ2)T ( 4ǫ2T +8ǫ2T ∆ )
T
≥ − − −
=2ǫT 8ǫ2T ∆ p
T
−
=2ǫT(1 4ǫp∆ )
T
−
ǫT p
≥
1 V ∆
T T
=min , T.
8√∆ T
(cid:26) T (cid:27)
24Choosing∆ =(T/V )2/3,wethenhave
T T
E [D-Reg(T)] CV1/3T2/3,
Alg ≥ T
whichcompletestheproof.
WeproveLemmaD.1inthefollowing.
ProofofLemmaD.1. Wehavethat
Ej [f(R)] E [f(R)]= f(R) Pj (R) P (R)
δj − 0 δj − 0
XR (cid:16) (cid:17)
f(R) Pj (R) P (R)
≤ δj − 0
R:Pj δj(XR)≥P 0(R) (cid:16) (cid:17)
M Pj (R) P (R)
≤ δj − 0
R:Pj δj(XR)≥P 0(R)(cid:16) (cid:17)
M
= Pj P
2 k δj − 0 kTV
M
2KL(P Pj ), (5)
≤ 2 0 k δj
q
where the last ineqaulity follows from Pinsker’s inequality. Let R RA be a random vector denoting
t
the payoff for each action at time t, and let Rt Rt×A denote the ∈ payoff matrix received upon time t:
∈
Rt =[R ,...,R ]T. Bythechainrulefortherelativeentropy,wehave
1 t
|Tj|
KL(P 0 kPj δj)= E Rt−1 KL P 0(R t |Rt−1) kPj δj(R t |Rt−1) . (6)
Xt=1 h (cid:16) (cid:17)i
Notethat
P (R =[ 2,0, 2,..., 2] Rt−1)=P (R =[0, 2, 2,..., 2] Rt−1)=1/4
0 t 0 t
− − − | − − − |
P (R =[1,1, 2,..., 2] Rt−1)=1/2.
0 t
− − |
Inthecaseδ =1,wehave
j
Pj (R =[ 2,0, 2,..., 2] Rt−1)=(1/2+ǫ)2
δj t − − − |
Pj (R =[0, 2, 2,..., 2] Rt−1)=(1/2 ǫ)2
δj t − − − | −
Pj (R =[1,1, 2,..., 2] Rt−1)=2(1/2+ǫ)(1/2 ǫ).
δj t − − | −
Inthecaseδ =2,wehave
j
Pj (R =[ 2,0, 2,..., 2] Rt−1)=(1/2 ǫ)2
δj t − − − | −
Pj (R =[0, 2, 2,..., 2] Rt−1)=(1/2+ǫ)2
δj t − − − |
Pj (R =[1,1, 2,..., 2] Rt−1)=2(1/2+ǫ)(1/2 ǫ).
δj t − − | −
25Thus,wehave
KL P (R Rt−1) Pj (R Rt−1) (7)
0 t | k δj t |
1 (cid:16) 1/4 1 1/4 (cid:17)1 1/2
= ln + ln + ln
4 (1/2+ǫ)2 4 (1/2 ǫ)2 2 2(1/2+ǫ)(1/2 ǫ)
− −
= ln 1 4ǫ2 . (8)
− −
Combining(5),(6)and(7),(cid:0)wehave(cid:1)
M
Ej [f(R)] E [f(R)] 2 ln(1 4ǫ2).
δj − 0 ≤ 2 − |Tj | −
q
Ifwefurtherhaveǫ 1/4,itthenholdsthat ln 1 4ǫ2 16ln(4/3)ǫ2andconsequently
≤ − − ≤
M (cid:0) (cid:1)
Ej [f(R)] E [f(R)] 2 ln(1 4ǫ2) 2Mǫ 2Mǫ ∆ .
δj − 0 ≤ 2 − |Tj | − ≤ |Tj |≤ T
q q p
D.2 ProofofTheorem 5.6
ProofofTheorem5.6. We consider a game that evolves stochastically, with n players, action space =
A
0,1,...,A 1 ,andthesamepayofffunctionU(n) asoutlinedinTheorem5.5. Wesegmentthedecision
{ − } 1
horizon T into T/∆ batches , with each batch comprising ∆ episodes. We consider two distinct
T j T
{T }
scenarios:
• Case1:Alltheotherplayersplay0;
• Case2:Alltheotherplayersplay1.
InCase1,wehaveut(0)=0andut(a)= 2foralla=0. InCase2,wehaveut(1)=0andut(a)= 2
− 6 −
for all a = 1. At the beginning of each batch, one of these scenarios is randomly selected (with equal
6
probability)andremainsconstantthroughoutthatbatch.
Let m = T/∆ representtotal numberof batches. We fix some algorithm. Let δ 1,2 indicate
T j
∈ { }
batch j belongsto Case1 or Case2. We denote by Pj the probabilitydistributionconditionedon batch j
δj
belongstoCaseδ ,andbyEj []thecorrespondingexpectation.Itthenholdsthat
j δj ·
m m
E ut(xt) = E ut(xt)
Alg Alg
   
Xj=1t X∈Tj Xj=1 t X∈Tj
   
m
1 1
= E Ej ut(xt) + Ej ut(xt)
Alg 2 1  2 2 
Xj=1 t X∈Tj t X∈Tj
m     
E 1 Ej utj,1 (xtj,1 ) + 1 Ej utj,1 (xtj,1 ) ,
≤ Alg 2 1 2 2
Xj=1 (cid:20) h i h i(cid:21)
26wheretj,1 representsthefirstepisodeofbatchj andtheinequalityfollowsfromthefactthatut 0. Note
≤
that
1 Ej utj,1 (xtj,1 ) + 1 Ej utj,1 (xtj,1 ) = Pj(xtj,1 =0)+Pj(xtj,1 =1)
2 1 2 2 − 1 6 2 6
h i h i
=
(cid:16) P(xtj,1 =0)+P(xtj,1
=1)
(cid:17)
1,
− 6 6 ≤−
(cid:16) (cid:17)
wherethesecondequationfollowsfromthefactthatxtj,1
isindependentofδ . Thus,wehave
j
m
E ut(xt) m= T/∆ .
Alg T
 ≤− −
Xj=1t X∈Tj
 
Choosing∆ =2T/V ,wehave
T T
m
E ut(xt) V /2.
Alg T
 ≤−
Xj=1t X∈Tj
 
E Experiments Details
E.1 Algorithms
WereferreaderstoAlgorithm2-5fordetailedimplementationofalgorithmsintheexperiment.ForMV,we
chooseη =1. ForSDG,wechooseη =2.
Algorithm2Self-Play
Require: NumberofiterationsT,actionspace ,learningrateη = η log|A|,numberofplayersn,and
A t t
initializepolicyπ 0. q
1: fort=1toTdo
2: Sampleactionsat i−1 ∼π t−1fori=2,...,n. Denoteat −− 11 :=(at 2−1,...,at n−1).
3: Update
π (a) π (a)exp η U (a,at−1) , a .
t ∝ t−1 { t 1 −1 } ∀ ∈A
4: endfor
27Algorithm3Self-PlaywithRegularization
Require: Number of iterations T, action space , learning rate η = η log|A|, number of players n,
A t t
initializepolicyπ 0,humanpolicyπ
human
andregularizationparameterλq.
1: fort=1toTdo
2: Sampleactionsat i−1 ∼π t−1fori=2,...,n. Denoteat −− 11 :=(at 2−1,...,at n−1).
3: Update
logπ (a)+ η U (a,aτ )+λ η logπ (a)
π (a) exp 0 τ<t τ 1 −1 τ<t τ human , a .
t
∝ 1+λ η ∀ ∈A
(cid:26) P τ<t τP (cid:27)
4: endfor P
Algorithm4BestResponsetoπ
human
Require: NumberofiterationsT,actionspace ,learningrateη = η log|A|,numberofplayersn,and
A t t
initializepolicyπ 0. q
1: fort=1toTdo
2: Sampleactionsat i−1 ∼π humanfori=2,...,n. Denoteat −− 11 :=(at 2−1,...,at n−1).
3: Update
π (a) π (a)exp η U (a,at−1) , a .
t ∝ t−1 { t 1 −1 } ∀ ∈A
4: endfor
Algorithm5Exploiterforπ
Require: NumberofiterationsT,actionspace ,learningrateη = η log|A|,numberofplayersn,and
A t t
initializepolicyπ 0. q
1: fort=1toTdo
2: Sampleactionsat−1 π.
3: Sampleactionsat i1 −1 ∼∼ π t−1fori=2,...,n. Denoteat −− 11 :=(at 2−1,...,at n−1).
4: Update
n
η
π (a) π (a)exp t U at−1,at−1[:i 1],a,at−1[i+1:] , a .
t ∝ t−1 (−n 1 1 1 −1 − −1 ) ∀ ∈A
− i=2
X (cid:0) (cid:1)
5: endfor
E.2 ComputationResources
The experimentsare conducted on a server with 256 CPUs. Each experimentcan be completed in a few
minutes.
28