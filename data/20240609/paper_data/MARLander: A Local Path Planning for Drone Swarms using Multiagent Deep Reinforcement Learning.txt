MARLander: A Local Path Planning for Drone Swarms using
Multiagent Deep Reinforcement Learning
Demetros Aschu, Robinroy Peter, Sausar Karaf, Aleksey Fedoseev, and Dzmitry Tsetserukou
Abstract—Achieving safe and precise landings for a swarm
of drones poses a significant challenge, primarily attributed to
conventionalcontrolandplanningmethods.Thispaperpresents
theimplementationofmulti-agentdeepreinforcementlearning
(MADRL)techniquesforthepreciselandingofadroneswarm
atrelocatedtargetlocations.Thesystemistrainedinarealistic
simulated environment with a maximum velocity of 3 m/s in
training spaces of 4 x 4 x 4 m and deployed utilizing Crazyflie
drones with a Vicon indoor localization system.
The experimental results revealed that the proposed ap-
proach achieved a landing accuracy of 2.26 cm on stationary
and3.93cmonmovingplatformssurpassingabaselinemethod
used with a Proportional–integral–derivative (PID) controller
with an Artificial Potential Field (APF). This research high-
lights drone landing technologies that eliminate the need for
analytical centralized systems, potentially offering scalability
and revolutionizing applications in logistics, safety, and rescue
missions.
Keywords: Swarm of Drones, Multi-agent system, Deep
Reinforcement Learning, Collision Avoidances, Planner,
Controller
I. INTRODUCTION
Swarm drones, characterized by their collaborative be-
havior, are driving research due to their disruptive potential
acrossindustrieslikeagriculture,construction,entertainment,
andlogistics[1],[2].Challengespersistinachievingaccurate
landings on specified targets, highlighting the importance of
safe descent to prevent damage and ensure mission success
soaddressingthischallengenecessitateseffectivecontroland
planning strategies.
A conventional approach of control and planning is sus-
ceptibletosinglepointsoffailure,scalabilityissues,andhigh
Fig.1:MARlandertwodroneslandingonthetargetplatform
communication overhead due to routing decisions through a
placed on the robot manipulator
central hub, leading to delays and congestion [6]. Classical
planners relying on full-state information face challenges
with high-dimensional problems, demanding heavy compu-
tationalresources[3].Kinodynamicplannersmustaccurately In light of these challenges, deep reinforcement learning
modeldronedynamicsnearphysicallimits,impactingpracti- (DRL) has emerged as a critical tool in robotic applications,
cal reliability [3], [26]. Conversely, conventional controllers particularly for drones, enabling them to learn policies di-
like PID controllers are tuning-dependent and find it chal- rectly from raw data for informed decision-making based
lenging to handle constraints. Moreover, model predictive on their observation [7]. Extensive research highlights the
controller(MPC)encountersissues,e.g.,computationalcom- effectiveness of DRL in single drone control, setting the
plexityandmodeldependency,whichmakethemlessrobust stage for its expansion to multiple drones through a means
in environments with complex dynamics and uncertainties of analytical centralization [4], [23]. However, the approach
[4], [5]. still inherits some challenges when working with multiple
agents.
TheauthorsarewiththeIntelligentSpaceRoboticsLaboratory,Skolkovo Significant advancements in multi-agent reinforcement
InstituteofScienceandTechnology,BolshoyBoulevard30,bld.1,121205,
learning (MARL) have driven the development of au-
Moscow, Russia {Demetros.Aschu,Robinroy.Peter,Sausar
Karaf,Aleksey.Fedoseev, D.Tsetserukou}@skoltech.ru tonomous drone swarms capable of coordinated formations
4202
nuJ
6
]OR.sc[
1v95140.6042:viXra[24], cooperative navigation [25], and path finding [8], [9] posed by authors [18] with a deep-learning-based robust
thathasbeennotablyinfluentialinmanyfields[7].However, nonlinear controller called the Neural-Lander, Furthermore,
implementing a multiagent reinforcement learning (MARL) theydemonstratedNeural-Swarm2forcontrollingmulti-rotor
approach,especiallyinmaintainingindividualagentbehavior drone swarms [19].
while fostering cooperative behavior, poses a significant However, due to a lack of robustness approach a single-
challenge. agent autonomous landing has been enhanced using RL
A centralized training approach, particularly which uti- techniques the research work [20] introduces an RL-based
lizing a single policy to govern interactions and decisions controller employing Least Square Policy Iteration (LSPI)
among multiple agents, has remained a key focus in current to learn optimal control policies for generating landing
research [10], while another approach in recent research has trajectories.[20]showslandingofaquadrotoronaplatform,
pivoted towards decentralized multiagent systems where the Additionally, [19] suggested the autonomous landing of
individual agents can execute their control policy indepen- quadrotors on inclined surfaces, while [20], [21] showcased
dentlyaftertrainingwithsupervisedcentralizedsystem[26], Autonomous Landing on a platform with different vehicles
[27]. and velocities.
This study introduces a multi-agent deep reinforcement The proposed solution from [23] introduced a method
learning (MADRL) approach for landing drone swarms involving a single-agent RL-based control policy integrated
in three-dimensional environments. The proposed solution with analytical planners such as APF to address collision
utilizes a trained RL agent that receives local observations avoidancewithinadroneswarm,withatrainedRLcontroller
from each drone, enabling it to learn a control policy for the directing drones toward a moving target for landing.
autonomous execution of landing trajectories. Preliminary Despite improving upon the limitations of conventional
results demonstrate the efficacy of the proposed approach controllers, this method encounters challenges when scaling
in addressing the challenge of landing due to conventional to a large number of drones and managing complexity due
approaches. to centralization.
In our paper, we demonstrated the capability of our agent In recent research, the MARL approach has been in-
to land on the target platform with an acceptable accuracy creasingly applied in drone swarm studies. For instance,
of mean landing error of 2.26 cm, while also ensuring the authors [26] developed a decentralized control policy allow-
avoidance of internal collisions during landing. This will be ing drones to map observations directly to motor thrusts,
a base for the advancement in the landing of large fleets of aiding in formation adjustments in 3D space. Additionally,
dronesinadecentralizedmanner,effectivelyovercomingthe in a study by [24], cooperative agents maintained a desired
current challenges associated with it. formation relative to two tracked agents while progressing
towards a common objective. Furthermore, research by [25]
II. RELATEDWORKS illustrated a MARL method for the collaborative navigation
Researchers have been addressing the challenge of devel- of Unmanned Aerial Vehicles (UAVs) using Centralized
opinganautonomousquadrotorcapableoflandingonstatics training Decentralized execution (CTDE).
platform[11]andlandingondynamicplatformsinturbulent
III. METHODOLOGY
wind conditions [12] by developing a fully autonomous
This section details the methodology used to develop and
vision-based system. This system integrates localization,
evaluate Lander, a multi-agent system enabling autonomous
planning, and control components effectively.
drones to land on target platforms. We employ the MADRL
Moreover, multiple drone landing scenarios have been
framework in a decentralized approach, which is described
explored in truncation single agent drone landing showcas-
in detail below.
ing advancements in control and planning methods such
researchwork[13]and[14]showcooperativemultipledrone A. System Overview
landing and grasping task, Furthermore, the research [15] The MARLander system depicted in Fig. 2 comprises a
shows Manipulator assisted landing where uses a tether and swarm of two Bitcraze Crazyflie drones, with a Universal
robotmanipulationtoenablemultipleUAVlandingswithout Robot(UR10)affixedtothelandingpad.Twoseparatetarget
needing a large platform. The system incorporates MPC for platforms are positioned on the landing pad, each spaced
stability and tracking, along with an adaptive estimator to 0.5cm apart.
handle motion disturbances. A VICON Vantage V5 motion capture system is utilized
The authors [16] proposed a novel scientific method to for micro drone localization, transmitting updated position
achieve the dynamic landing of a heterogeneous swarm and orientation data via the Robot Operating System (ROS)
of drones on a moving platform. This method involves a to the control station. The control station processes this in-
leaderdroneequippedwithacameraguidingfollowerdrones formationtoestimatelinearandangularvelocity,forwarding
throughcommandswhileensuringcollisionavoidanceviaan it to a DRL model for policy generation.
APF, they also propose a system for landing on a moving The policy generates control outputs at each time step,
platform using multiple agents under supervision [27]. subsequently relayed to the Crazyflie onboard controller for
A new approach to landing drones by incorporating motor actuation which generates trajectories for each drone,
neural networks into control and planning processes pro- guiding them to land on their respective targets successfully.r i =α(∥p ∥−∥p ∥)+β∥v ∥+c (5)
t t−1 t t
where α is the position-shaping factor determining whether
the drone is approaching or moving away from its target, a
positive reward is given for proximity to the target, while
a negative reward is assigned for moving farther away. β
scales the velocity penalty, penalizing high speeds to ensure
a safe landing and c is the constant reward for encouraging
a successful landing. rc accounts for collision penalties,
calculated as:
(cid:40)
−α if collision,
Fig. 2: General system overview of MARLander. rc = c (6)
0 else
If drones approach each other within minimal collision
B. Problem Formulation and Preliminaries
distances, a negative penalty α is applied to discourage
c
The landing problem is addressed using state-of-the-art collisions; otherwise, no penalty is imposed. K is the final
DRL techniques. In a 3D environment with N quadrotor reward granted when all agents successfully land on the
drones, we utilize a Markov Decision Process (MDP) model target simultaneously.
denoted as:
MDP(O ,A ,R ) (1) D. Model Architecture
t t t
We employed a Proximal Policy Optimization (PPO) al-
whereO istheobservationoftheenvironment,consistingof
t
gorithm with a neural network architecture depicted in Fig.
s whichisthedrone’sstateincludingkinematicinformation,
t
3. The input layer of the neural network receives an array
and g is the state of the target platform. A is the actions,
t t
of data from two drone observations, which is subsequently
and R is the reward signal guiding our objective.
t
flattened into a 1D array with the assistance of a vector
The aim is to develop an optimal policy to map obser-
environment from Gymnasium. This flattened array is then
vations from each drone to control actions that drive it to
fed as input to the policy network, where the features are
landatthetargetposition whilepreventingcollisionsamong
processed through a series of fully connected layers. These
them. The optimal policy is formulated by maximizing the
layers incorporate Rectified Linear Unit (ReLU) activation
sum of discounted rewards.
functions and are structured with dimensions of 512 x 2,
C. Environment Setup
256, and 128. The output is fed into six neurons, where the
Ourenvironmentfortrainingandassessingswarmlanding first neuron determines the action for the first drone, and
in static and dynamic conditions is crafted to represent the the remaining neurons correspond to the second drone. The
MDP framework. output is utilized to calculate the next target position of the
1) Observation and Actions: The observation vector of drone.
the ith drone in the swarm is denoted by:
O i =[p i,q i,v i,ω i] (2)
t t t t t
where p i represents the drone’s position relative to the goal
t
point,q i denotesorientation,v i signifiesarelativevelocity
t t
and ω i indicates angular velocity. Similarly, the action for
t
the ith drone in the swarm is denoted by:
A i =[u i,u i,u i,] (3)
t x,t y,t z,t
where ui denotes the velocity of the ith drone within the
,t
swarmineach3Daxis.Thisvelocityissubsequentlyutilized
tocomputethedrone’snextpositionasitprogressestowards
the target.
2) Reward Function: The reward signal denoted R ti is Fig. 3: A neural network architecture for PPO algorithm
defined as follows:
n
(cid:88)
R = r i+rc+K (4) E. Simulation Setup
t t
i=1 Gym PyBullet environment [28] is employed to simulate
where r i is the individual agent’s reward at time t for ith and train a customized drone swarm along with individ-
t
drone. It is computed as: ual target platforms depicted in Fig. 4. The environmentis vectorized and adapted for multi-agent requirements by
integrating with the Stable Baselines3 RL framework.
This environment offers a versatile platform for modeling
complex dynamics and interactions within the simulated
world. Leveraging PyBullet’s physics engine, it accurately
captures the intricate behaviors of drones and their interac-
tions with the environment.
Fig. 6: Episode length mean during training
IV. EXPERIMENTS
We experimented to validate our proposed solution de-
ployed via ROS, as discussed in the following section. Our
experiments aim to investigate drone landing accuracy and
compare it to other approaches used for drone landing.
A. Experimental Setup
Wedevelopedapracticaltestingframeworktovalidateour
Fig.4:GymPyBulletenvironmentforsimulatingaMADRL- proposedsolutionacrossaspectrumoftestcases,employing
driven swarm of drones various compression approaches tailored to our solution.
Initially,wecreateda0.5x0.5acryliclandingpadfeaturing
two cylindrical pads, each with a radius of 0.2 m and a
F. Training Configuration thickness of 2.5 mm, tailored for the two Bitcraze nano
We employed the Proximal Policy Optimization (PPO) quadrotors. This landing pad was affixed to the Tool Center
algorithm due to its high performance and sample efficiency Point (TCP) of a UR10 robotic arm, as depicted in Fig. 7.
where the training is carried out by randomly placing the
drones and the target platform within 3D spaces of 4 x 4 x
4 m. During each time step, both the drones and the target
platform were repositioned randomly to new locations.
Fig.7:MARLanderexperimentalsetupformultiagentdrone
landing in different conditions of the landing pads.
Fig. 5: Episode reward mean during training
B. MARLander experiment with stationary platform
The training process has been carried out using the super-
computer with resources equipped with 36 CPU cores and 1) Description: In this experiment, a drone is randomly
1 GPU core for 20 million time steps. The results reveal initialized, and its actions are predicted by a trained policy
that the mean reward converges over time shown in Fig. 5 that processes incoming data from Vicon and broadcasts the
while the average episode length reduces shown in Fig. 6 corresponding control outputs to each drone. This process is
indicating the effectiveness of the training process. repeated12timeswherehalfoftheexperimentisconducted(a) (b) (c)
Fig. 8: Trajectories of the first drone (light blue), second drone (orange), target platform for the first drone (red), and
the second drone (blue) landing on static platforms: (a) on the floor, (b) on a UR10 robotic arm, and (c) on a UR10
arm performing linear movement with constant velocity where the first platform (magenta) and the second platform (cyan)
represent trajectories while moving.
when the landing pad is placed on the floor and the rest is toward their designated target locations.
equipped on TCP of UR10 manipulator, with the collected 2) Result and Discussion: The results of the experiment
data analyzed to evaluate the trained model’s effectiveness are summarized as follows: the mean landing precision was
basedonthedronelandingsuccessrateandaccuracymetrics. assessedacross10experiments,revealinganaveragelanding
2) Result and Discussion: The landing precision was error of 4.8 cm on the statics platform with a success rate
assessed over 12 experiments, showcasing mean deviations of 80% 7.4 cm with a success rate of 60%.
of 2.67 cm and 3.26 cm for the drones concerning their
E. Additional Baselines Experiment
intended targets. The average landing error was calculated
tobe2.965cm,withasuccessrateof91.67%.Theseresults 1) Description: In this experiment, a trained single-agent
indicate that the trained policy effectively guided the swarm RL (Reinforcement Learning) agent, adept at navigating to
of dronesto land onthe platform withremarkable precision. agoalpoint,withanAPF(ArtificialPotentialField)planner
proposed by the authors [23] was used as a comparison to
C. MARLander experiment with moving platform
our developed solution.
1) Description: In this experiment, a UR10 robot manip-
ulator is directed to move from its base within the angular F. Single-Agent and Multi-Agent DRL Landing Comparison
range of −π/2 to π/2, with a linear velocity spanning from Our experimentation shows that, proposed multi-agent re-
0.2 to 0.5 m/s. The drone is randomly initialized, and its inforcement learning demonstrates acceptable accuracy with
actions are anticipated through trained policy to determine ahighsuccessrate.Notably,single-agent-basedlandingalso
the control outputs of each drone. This entire process is achievedpreciselandings.However,oursystemoutperforms
iterated 8 times across various velocities. The gathered data in terms of landing time and collision avoidance compared
isthenscrutinizedtoassesstheefficacyofthetrainedmodel, to the other approach, although this aspect is not explicitly
relying on predefined metrics. mentioned in [23].
2) Result and Discussion: The experiment involved as-
sessing landing precision through 8 trials and presenting TABLE I: Comparison of the experiments for the static
mean deviations of 4.38 cm and 3.47 cm for the drones platform.
in relation to their designated targets. The average landing
error was found to be 3.93 cm, with an average success Successrate(%) Precision(cm) time(s)
rate of 75 %. Interestingly, it was observed that as the Baseline 80.0 4.8 24
speed of the manipulator increased, the success rate of Morpholander − 2.35 −
MARLander 91.67 2.26 12
the experiment decreased, and the mean average landing
accuracy was reduced. This suggests that while the trained
policy was able to successfully guide a swarm of drones to
V. CONCLUSIONANDFUTUREWORK
land on the platform, it faced challenges when dealing with
In our research, we developed a swarm of drone landing
high-speed manipulator movements.
systems using a novel MADRL approach. Through a series
D. Baselines Experiment
ofexperiments,wesuccessfullydemonstratedthatourdevel-
1) Description: In this experiment, we utilized a PID oped solution can effectively navigate a swarm of drones to
controller alongside an APF planner to direct the drones their target landing position. Our results show a remarkableTABLE II: Comparison of the experiments for the moving
[11] Falanga, Davide, and Zanchettin, Alessio and Simovic, Alessandro
platform. and Delmerico, Jeffrey and Scaramuzza, Davide, “Vision-based au-
tonomous quadrotor landing on a moving platform,” 2017 IEEE
Successrate(%) Precision(cm) time(s) International Symposium on Safety, Security and Rescue Robotics
(SSRR),Pages200-207,2017,
Baseline 60.0 7.4 28 [12] Paris, Aleix and Lopez, Brett T. and How, Jonathan P., “Dynamic
Morpholander − 3.5 − Landing of an Autonomous Quadrotor on a Moving Platform in
MARLander 75 3.93 17 TurbulentWindConditions,”2020,
[13] Mellinger, Daniel and Shomin, Michael and Michael, Nathan, “Co-
operative Grasping and Transport Using Multiple Quadrotors,” Dis-
tributedAutonomousRoboticSystems,Pages545-558,2013.
successrateofover90%onstatictargetsand75%onmoving
[14] Loianno,GiuseppeandKumar,Vijay,”CooperativeTransportationUs-
targets, with acceptable landing accuracy. ingSmallQuadrotorsUsingMonocularVisionandInertialSensing,”
Furthermore, we conducted comparative analyses with a IEEERoboticsandAutomationLetters,VolumePP,Pages1-1,2017,
baseline experiment employing PID controllers and APF [15] Xu, Ruoyu and Liu, Chongfeng and Cao, Zhongzhong and Wang,
Yuquan and Qian, Huihuan, “A Manipulator-Assisted Multiple UAV
planners. The results indicate the superior performance of
Landing System for USV Subject to Disturbance,” arXiv preprint
our MARLander. arXiv:2212.12196,2023.
Inthefuture,ourfocuswillbeonenhancingthescalability [16] Gupta,AyushandBaza,AhmedandDorzhieva,EkaterinaandAlper,
MertandMakarova,MariiaandPerminov,StepanandFedoseev,Alek-
oftheMARLanderapproach.Additionally,weplantoextend
sey and Tsetserukou, Dzmitry, “SwarmHive: Heterogeneous Swarm
the experiments on dynamic platforms to showcase the ofDronesforRobustAutonomousLandingonMovingRobot,”arXiv
system’s capability to land on moving and inclined surfaces preprintarXiv:2206.08856,2022.
[17] Gupta,AyushandDorzhieva,EkaterinaandBaza,AhmedandAlper,
with a high-speed manipulation operation.
MertandFedoseev,AlekseyandTsetserukou,Dzmitry,“SwarmHawk:
Self-Sustaining Multi-Agent System for Landing on a Moving Plat-
REFERENCES
formthroughanAgentSupervision,
[1] Abdelkader, Mohamed and Guler, Samet and Jaleel, Hassan and [18] Shi,GuanyaandHo¨nig,WolfgangandYue,YisongandChung,Soon-
Shamma,Jeff,“AerialSwarms:RecentApplicationsandChallenges,” Jo,“Neural-Swarm:DecentralizedClose-ProximityMultirotorControl
CurrentRoboticsReports,Volume2,Pages1-12,2021. UsingLearnedInteractions,”presentedatthe2020IEEEInternational
[2] Zhou, Yongkun and Rao, Bin and Wang, Wei, “UAV Swarm Intelli- Conference on Robotics and Automation (ICRA), Pages 3241-3247,
gence: Recent Advances and Future Trends,” IEEE Access, Volume 2020,.
8,Pages183856-183878,2020. [19] Shi,GuanyaandHo¨nig,WolfgangandShi,XichenandYue,Yisong
[3] Danancier, Ke´vin and Ruvio, Delphine and Sung, Inkyung and andChung,Soon-Jo,”Neural-Swarm2:PlanningandControlofHet-
Nielsen, Peter, “Comparison of Path Planning Algorithms for erogeneousMultirotorSwarmsusingLearnedInteractions,”2021,
an Unmanned Aerial Vehicle Deployment Under Threats,” [20] Vankadari, Madhu Babu and Das, Kaushik and Shinde, Chinmay
IFAC-PapersOnLine, Volume 52, Pages 1978-1983, 2019, and Kumar, Swagat, “A Reinforcement Learning Approach for Au-
doi:10.1016/j.ifacol.2019.11.493. tonomous Control and Landing of a Quadrotor,” 2018 International
[4] Hwangbo, Jemin and Sa, Inkyu and Siegwart, Roland and Hutter, ConferenceonUnmannedAircraftSystems(ICUAS),2018.
Marco,“ControlofaQuadrotorWithReinforcementLearning,”IEEE [21] Kooi, Jacob E. and Babuska, Robert, “Inclined Quadrotor Landing
RoboticsandAutomationLetters,Volume2,Number4,Pages2096- using Deep Reinforcement Learning,” 2021 IEEE/RSJ International
2103,2017. ConferenceonIntelligentRobotsandSystems(IROS),2021.
[5] Chatterjee, Debasish and Roy, Rajarshi and Sengupta, Apara- [22] Jiang, Z. and Song, G., “A Deep Reinforcement Learning Strat-
jita, “Comparison of Reinforcement Learning controller with a egy for UAV Autonomous Landing on a Platform,” arXiv preprint
classical controller for an UAV,” presented at the 2023 Sec- arXiv:2209.02954,2022.
ond International Conference on Electrical, Electronics, Information [23] Karaf, Sausar and Fedoseev, Aleksey and Martynov, Mikhail and
and Communication Technologies (ICEEICT), Pages 01-05, 2023, Darush,ZhanibekandShcherbak,AlekseiandTsetserukou,Dzmitry,
doi:10.1109/ICEEICT56924.2023.10156999. “MorphoLander:ReinforcementLearningBasedLandingofaGroup
[6] Liu,ShiyuandErskine,JulianandChriette,AbdelhamidandFantoni, ofDronesontheAdaptiveMorphogeneticUAV,”2023,
Isabelle, “Decentralized Control and Teleoperation of a Multi-UAV [24] Rawat, Abhay and Karlapalem, Kamalakar, “Multi-Robot Formation
Parallel Robot Based on Intrinsic Measurements,” 2021 IEEE/RSJ ControlUsingReinforcementLearning,”2020,
International Conference on Intelligent Robots and Systems (IROS), [25] Azzam,RanaandBoiko,IgorandZweiri,Yahya,“SwarmCooperative
Pages6329-6335,2021 NavigationUsingCentralizedTrainingandDecentralizedExecution,”
[7] Zhang, Kaiqing and Yang, Zhuoran and Basar, Tamer, “Multi-Agent Drones,Volume7,Number3,ArticleNumber193,2023,
Reinforcement Learning: A Selective Overview of Theories and Al- [26] Batra,SumeetandHuang,ZhehuiandPetrenko,Aleksei,andKumar,
gorithms,”CoRR,Volumeabs/1911.10635,2019, Tushar,andMolchanov,ArtemandSukhatme,GauravS.,“Decentral-
[8] Wu, Qizhen and Chen, Lei and Liu, Kexin and Lv, Jinhu, “UAV izedControlofQuadrotorSwarmswithEnd-to-endDeepReinforce-
Pathfinding in Dynamic Obstacle Avoidance with Multi-agent Rein- mentLearning,”2021,
forcementLearning,”2023 [27] Thumiger, Nicholas and Deghat, Mohammad, “A Multi-Agent Deep
[9] Chronis, Christos, and Anagnostopoulos, Georgios, and Politi, Elena Reinforcement Learning Approach for Practical Decentralized UAV
andGaryfallou,AntoniosandVarlamis,Iraklis,andDimitrakopoulos, CollisionAvoidance,”IEEEControlSystemsLetters,Volume6,Pages
George, “Path planning of autonomous UAVs using reinforcement 2174-2179,2022,
learning,”JournalofPhysics:ConferenceSeries,Volume2526,Num- [28] Panerati, Jacopo and Zheng, Hehui and Zhou, SiQi and Xu, James
ber1,Pages012088,2023, and Prorok, Amanda and Schoellig, Angela P., “Learning to Fly – a
[10] Park, Chanyoung and Kim, Gyu Seon and Park, Soohyun and Jung, GymEnvironmentwithPyBulletPhysicsforReinforcementLearning
SoyiandKim,Joongheon,“Multi-AgentReinforcementLearningfor ofMulti-agentQuadcopterControl,”2021,
Cooperative Air Transportation Services in City-Wide Autonomous
UrbanAirMobility,”IEEETransactionsonIntelligentVehicles,Vol-
ume8,Number8,Pages4016-4030,2023.