POEM: Interactive Prompt Optimization for Enhancing Multimodal
Reasoning of Large Language Models
JianbenHe,XingboWang,ShiyiLiu,GuandeWu,ClaudioSilva,andHuaminQu
A B C
A1 B1 B2 C1
A2
C2
B3
C3
A3
Fig.1:ThePOEMinterfaceconsistsofthreemajorpanels.ThePromptPanel(A)offersversatileoperationsforuserstoefficientlycraft
andeditpromptcontent,suchasimportingvariousprinciplesanddemonstrationexamples,tosupportaneffortlesspromptengineering
experience. The Reasoning Panel (B) facilitates a comprehensive multi-level investigation of the model’s multimodal reasoning
performance,rangingfromtheglobalmodalityinteractionleveltothelocalinstancelevel. TheEvaluationPanel(C)supportsboth
globalandlocalevaluationofprompts,coupledwithdetaileddocumentationofmodificationsduringpromptiterationsforcontinuous
monitoringandcomparison.
Abstract—Largelanguagemodels(LLMs)haveexhibitedimpressiveabilitiesformultimodalcontentcomprehensionandreasoning
withproperpromptinginzero-orfew-shotsettings. Despitetheproliferationofinteractivesystemsdevelopedtosupportprompt
engineeringforLLMsacrossvarioustasks,mosthaveprimarilyfocusedontextualorvisualinputs,thusneglectingthecomplexinterplay
betweenmodalitieswithinmultimodalinputs.Thisoversighthindersthedevelopmentofeffectivepromptsthatguidemodels’multimodal
reasoningprocessesbyfullyexploitingtherichcontextprovidedbymultiplemodalities. Inthispaper,wepresentPOEM,avisual
analyticssystemtofacilitateefficientpromptengineeringforenhancingthemultimodalreasoningperformanceofLLMs.Thesystem
enablesuserstoexploretheinteractionpatternsacrossmodalitiesatvaryinglevelsofdetailforacomprehensiveunderstandingofthe
multimodalknowledgeelicitedbyvariousprompts.Throughdiverserecommendationsofdemonstrationexamplesandinstructional
principles,POEMsupportsusersiniterativelycraftingandrefiningpromptstobetteralignandenhancemodelknowledgewithhuman
insights.Theeffectivenessandefficiencyofoursystemarevalidatedthroughtwocasestudiesandinterviewswithexperts.
IndexTerms—promptengineering,multimodalreasoning,multimodallargelanguagemodels
1 INTRODUCTION
Large Language Models (LLMs), pre-trained on massive data with spectrumofmultimodaltasks(e.g.,multimodalsceneunderstanding
billionsofparameters,havebecomeacornerstonefornaturallanguage andquestionanswering).Byusingtextastheuniversalrepresentation,
processing.Theyencodeextensiveknowledgeabouttheworldintheir theseworksaimtoleverageLLMstointegrateandanalyzeknowledge
parameterspace,exhibitingimpressivecapabilitiesintextunderstand- distilledfromdiversemodalities(e.g.,audioandimages)intextformat
ing,reasoning,andgenerationacrossvariousdownstreamtasks[9,64]. andprovideaholisticunderstandingofmultimodalcontent. These
BuildingonthestrengthofLLMs,thereareanincreasingnumberof modelsarealsoknownasmultimodalLLMs[51,67]. Comprehend-
works[2,6,16,27,57,57,72]exploringtheirapplicationsinawide ingmultimodalcontentnecessitatesextensivemultimodalknowledge,
wheremodelsnotonlyneedtounderstandtheinformationpresented
ineachindividualmodalitybutalsohavetocorrectlyinferhowthe
informationcombinestoinformaccuratereasoning[65].
Manuscriptreceivedxxxxx.201x;acceptedxxxxx.201x.DateofPublication Recently,promptinghasemergedasadata-efficientanduser-friendly
xxxxx.201x;dateofcurrentversionxxxxx.201x.Forinformationon paradigmforsteeringandenhancingLLM’sperformanceofcomplex
obtainingreprintsofthisarticle,pleasesende-mailto:reprints@ieee.org. reasoning tasks. Relying on extensive knowledge acquired during
DigitalObjectIdentifier:xx.xxxx/TVCG.201x.xxxxxxx pre-training,themodelscaninstantlyadapttonewdownstreamtasks
inthefew-shotorevenzero-shotsettingwithouttheneedformodel
4202
nuJ
6
]CH.sc[
1v34830.6042:viXraretraining [35]. Moreover, the LLMs can be prompted to generate fullydesignedvisualizationsandinteractionstosupportefficient
rationales(i.e.,free-textexplanations)emulatinghumanthoughtpro- promptengineeringformultimodalreasoningtasks.
cessesinthechain-of-thought(CoT)manner[6]. Forexample,they • Weconducttwocasestudiesandexpertinterviewstodemonstrate
canprovidestep-by-stepderivationsthatleadtothefinalanswerof theusefulnessandefficiencyofPOEM.
amathproblemorsubstantiatetheiranalysisprocesswithsupported
evidencesuchasemotivewordsforsentimentprediction[76].These 2 RELATEDWORKS
human-readablerationalesenhancetheaccuracyandtransparencyof TheresearchstudiesrelatedtothedesignofPOEM includeprompt
themodel’sreasoningprocess[41,44]. engineering, multimodal reasoning, and visual analytics for model
WhilemultimodalLLMsexhibitremarkableperformanceinvar- understandingandsteering.
ious tasks with prompting, their reasoning performance is notably
sensitivetopromptvariations. Moreover,inadequateorill-designed 2.1 PromptEngineering
promptsmayeliciterroneousknowledge,resultinginbiasedandunre- Equipped with extensive knowledge acquired during pre-training,
liablereasoning.Devisingwell-performingpromptsthatcanguideand LLMs (e.g., GPT- [9] and LLaMA [53] based models) exhibit re-
enhancethemultimodalreasoningperformanceofLLMsremainsa markableadaptabilitytospecializeddownstreamtaskssuchasquestion
persistentchallenge.Promptingisinherentlyaprocessrequiringexper- answering, content retrieval, and complex reasoning, given precise
tiseandtrial-and-error,whereusersneedtometiculouslycraftprompts, instructionsandproperillustrativeexamples.Thisemergingparadigm,
scrutinize the outputs to identify flaws requiring improvement, and knownaspromptingorpromptengineering,offersauser-friendlyand
iterativelyrefinethepromptstoreachtheintendedoutcomes[13,50]. data-efficientwayfornon-expertuserstointeractandsteerlargemod-
Duringtheprocess,usersfirstfacethechallengeofsystematicallyun- els.Promptinggenerallyincludesinstruction-basedandexample-based
derstandingandexaminingthemultimodalreasoningperformanceof prompts[5]. Instruction-basedpromptsincludesystempromptsthat
differentprompts. Manuallyinspectingeachinstanceisnotonlytime- provide general guidelines and task prompts that deliver direct and
consumingbutalsofailstoprovideaholisticunderstanding.Therefore, task-specificinstructions.Example-basedpromptsutilizeasmallset
summarizingandpresentinggeneratedrationalesatvaryingdetaillevels ofexamplestoshowcasethedesiredinput-outputpatternsformodels
isnon-trivialforuserstofullyverifyoutputsandpinpointproblematic tofollow. Numerousstudies[24,61,71]havehighlightedtwomajor
aspects. However,inthemultimodalcontext,thecomplexinterplay challengesinprompting:theformulationofeffectiveprompts,andthe
amongdifferentmodalities,coupledwiththeunstructuredandgener- assessmentofpromptefficacyalongsidestrategiesforenhancement.
ativenatureoffree-textexplanations,makesinterpretingmultimodal Manystudieshavebeenconductedtoaddressthepromptingchal-
LLMs’reasoningprocessparticularlychallenging.Furthermore,users lenges.Strobelt etal.introducedPromptIDE[50]asatoolforrapid
alsostruggletorevisetheirpromptsinawaythateffectivelyincorpo- explorationandassessmentofvariationsinprompttemplates.Knowl-
ratesandelicitsthedesiredmultimodalreasoningknowledgefromthe edgeVis[11]comparedmultiplefill-in-the-blankpromptstoprobethe
model[24].Well-clarifiedtaskinstructions(e.g.,format,phrasing,and input-outputassociationsinBERT-basedmodels.Beyondexpediting
content)andinformativedemonstrationexamplesarebothimperative thewordingandphrasestructurerefinement,ScatterShot[61]proposed
forenablingLLMstograsptheintendedinput-outputrelationshipsand aslice-basedsamplingstrategytoidentifythemostinformativedatapat-
generateconsistentoutputswithcorrectrationales[45].Considering ternsforhumanannotation.PromptAid[45]combinedmultipleprompt
thehugespaceofpossibletaskinstructionsandthedifficultyofselect- perturbationstrategiestofindsatisfactorypromptsfortextclassification
ingandannotatingdemonstrationexamplesfromhigh-dimensionaland tasks.Inaddition,PromptChainer[62]andAIChains[63]havebeen
information-complexmultimodaldata,itisimportantyetchallenging developedtosupportmoresophisticatedtasksbydecomposingthem
tofacilitateuserstocraftandrefinepromptsinanefficientmanner. intomanageablesub-tasks,andsupportedpromptchainprototypingand
Totackletheabovechallenges,wepresentPOEM,avisualanalytics authoringtoenhancecontrollability.Kimetal.proposedEvaLM[25]
approachdesignedtostreamlinetheprocessofpromptengineering foriterativepromptevaluationaccordingtouser-definedcriteria,while
formodelpractitioners,includingmodeldevelopersandnon-expert ContitutionMaker [46] converted users’ natural language feedback
modelusers,tosystematicallyprobeandsteerthemultimodalreason- intoaprincipleforchatbotbehaviorcustomization. Besidestext-to-
ingperformanceofLLMsfortargeteddownstreamtasks. Tobuilda textgenerativetasks,severalworksfacilitatetextpromptrefinement
comprehensiveunderstandingofLLMs’knowledgeandreasoningon fortext-to-imagegenerationbykeywords[14]andstyledescription
multimodaltasks,wedevelopcomputationalmethodstodecompose recommendation[8], structuredsearchofvisualconcepts[37], and
andsummarizecross-modalinteractionscapturedbyLLMsinvarious rubric-basedadjustmentforpreciseemotionexpression[56].
levelsofdetail.Atthemodalitylevel,weadoptathree-layeraugmented However,existinginteractivepromptengineeringsystemsarelim-
Sankeydiagramtocontextualizemodelperformancewithcomplement itedtotext-to-textortext-to-imagegenerationtasks,failingtodealwith
andconflictinteractionsbetweenmodalities.Then,drillingdowninto thecomplexityofmultimodalinputsformoresophisticatedreasoning
specificinteractions,wedistillandsummarizethelinguisticandvi- tasks. Inthispaper,wedevelopPOEMtooptimizethepromptengi-
sual evidence from individual instances to reflect model reasoning neeringprocessforadaptingandsteeringthemultimodalreasoning
patterns. Thesevisualizationshelpalignthemodel’sknowledgeand performanceofLLMs. POEMfacilitatesacomprehensiveinvestiga-
reasoningprocesseswiththehumanunderstandingatscale.Basedon tionofprompteffectsandprovidesdiversesupportforuserstoiterate
themulti-levelmodelunderstanding,POEMallowsuserstoconduct promptswithreducedcognitiveburdenandincreasedefficiency.
bothtop-downandbottom-upapproachestobuildandrefineprompts
thatguideLLM’smultimodalreasoning. Specifically,weemployan 2.2 MultimodalReasoning
effectivesamplingstrategyfordemonstrationexamples,ensuringabal- Reasoninggenerallyreferstotheprocessofdrawingonevidenceto
ancebetweenrelevanceanddiversitytoprovidevariedandinformative makelogicalinferencesbasedonexistingknowledgeforprediction
input-outputmappingsforinductivemodellearning.Ontheotherside, anddecision-making[51].Inthemultimodalcontext,itisimperative
drawingonhumaninnatecapabilitiesforsummarizationandgeneral- formodelstonotonlygraspevidencederivedfromsinglemodalities
ization,weincorporateanLLM-assistedmodulethatdistillsprinciples butalsotocomprehendhowevidencefromdifferentmodalitiesrelates
atbothinstance-specificandagnosticlevels.Thisapproachfacilitates toeachother.Thiscomprehensioncouldleadtothegenerationofnew
userstopreciselyarticulateandapplytheirdomain-specificknowledge insightsthatthemodelsmustcapturetoachieveaccuratereasoning.
andexpertisetoguidethemodeldeductively. Ourcontributionsare Recently,LLMshavedemonstratedthecapabilitytogeneratecoherent
summarizedasfollows: self-explanations through Chain of Thought (CoT) prompting [59],
• Weproposeaneffectivehuman-in-the-loopworkflowthatfacil- where LLMs not only output the final answer but also provide the
itatessystematicinvestigationandguidanceofthemultimodal intermediatereasoningstepsinnaturallanguage[76].Thesegenerated
reasoningperformanceofLLMs. rationales(i.e.,free-textexplanations)havebeenincreasinglyexplored
• WedevelopavisualanalyticssystemPOEM,equippedwithcare- formodelexplainability,astheyprovideanexplicitandtransparentwaytocommunicatethedecision-makingprocessofmodelstoend- canassesstheirreliabilityandenhanceknowledgeofunderperform-
usersinahuman-likemanner[44]. Huangetal.[22]quantitatively ingareasthroughproperpromptdesignwithdomainknowledgeand
demonstratedthemodel’sself-generatedexplanationsachievedcompa- expertise. However,summarizingtheknowledgeutilizedbymodels
rableperformancewithtraditionalexplanationmethods[47]interms forreasoningisdifficult,owingtotherichinformationandintricate
offaithfulness. interplayacrossdifferentmodalities.Moreover,craftingandrefining
Agrowingnumberofbenchmarkdatasets[15,38,70]havebeen promptstoeffectivelyelicitdesiredknowledgeofmodelsforspecific
proposedtoevaluatethecapabilitiesofLLMsinmultimodalreasoning tasksalsopresentssignificantchallenges.
tasks,withaprimaryfocusonvisualcontentunderstandinglikeVisual To better understand users’ requirements for system design, we
Question Answering (combining visual content and text questions). workedcloselywithfourNLPandmultimodalmachinelearningexperts
However,thecomprehensionofhowLLMsintegrateandcoordinate (E1-E4, E1 is the coauthor). E1 is a researcher with expertise in
informationfromvariousmodalities(visual+language,oradditional developinginteractivesystemsforNLPandmultimodalmodelanalysis.
modalities)inthegivencontextforquestionanswering(textquestions) E2isaresearcherwithindustryexperienceinapplyinganddeveloping
andreasoningremainsunder-explored. Thisincludestasksthatne- multimodalmodelsforreal-worldapplications. E3andE4arePh.D.
cessitate a nuanced understanding of multimodal contexts, such as candidateswithmultipletopconferencepublicationsintheareasof
multimodal scene comprehension and multimodal sentiment analy- multimodalmachinelearningandmultimodalLLMs.
sis[65].Moreover,themetricsonthesebenchmarksfailtocapturethe All experts concurred that there is a lack of tools for systemati-
detailedreasoningprocessofmodelsforin-depthmodelunderstanding callyanalyzingthemultimodalreasoningperformanceofLLMs.The
anddiagnosis. Besidesevaluation,manyworks[39,48,57,72]have currentpracticeusuallystartswithobservingthemodel’soverallperfor-
triedtosteermultimodalLLMs’reasoningabilities. Comparedwith mancestatistics(i.e.,performancematrix)andthenrandomlysampling
the labor-intensive fine-tuning approach involving curating specific several instances to examine the reasoning correctness. While few
datasetswithadditionalreasoningchainannotation,thetraining-free datasets[29,38]provideexpert-writtenrationalesasgroundtruth,the
prompting-basedmethods[73,77]havebecomeprevalent.However, vastdiversityinfree-textexpressionsstillposesdifficultiesinsystemat-
theseautomatictechniquesfallshortofprovidingfine-grainedprompt icallysummarizingtheknowledgeutilizedbymodelsandidentifying
evaluationandflexiblepromptrefinement.Instead,wepresentahuman- theirdeficiencies. Moreover,steeringexistingmultimodalLLMsfor
in-the-loopapproachwhereuserscaninteractivelyexamine,evaluate, specificapplicationsthroughpromptingoftenrequiresnumerousitera-
andrefinepromptstoguideandenhancemodelperformanceinamore tionsofpromptadjustments,aprocessthatisbothlabor-intensiveand
interpretableandcontrollablemanner. tedious.Consequently,anintegratedvisualanalyticstoolisdesiredto
facilitatesystematicinvestigationofmodelbehaviorsatvariouslevels
2.3 VisualAnalyticsformodelunderstandingandsteering andsupport efficientandwell-informed promptiterationswith less
cognitiveeffort.Thedesignrequirementsaresummarizedasfollows:
VisualAnalyticshasprovedtobeaneffectiveapproachtohelpusersun-
derstandandsteermachinelearningmodels[64,69].Priorworksaimed R1 Summarizemultimodalreasoningperformanceacrossvary-
todisclosethefunctionalitiesofneuronsandlayersofdiverseneural inglevelsofdetailsWhenevaluatingthereasoningperformance
networkmodelslikeRNNs[18,43,49].Inrecentyears,thetransformer- ofdifferentprompts,usersnotonlycareabouttheoverallperfor-
basedmodelsexhibitedremarkableperformancewiththeself-attention mancebutalsothevalidityoftheunderlyingrationales. Users
mechanism. Many works have sought to interpret the dynamics of needtounderstandhowwellthemodel’sreasoningalignswith
attentionflow[12],theweightsofattentionheads[19,23,28,58]and establishedhumanknowledgetoidentifyandaddressknowledge
attentionpatternsacrossmultipleinstances[36,66]. Beyondvisual- gaps in areas where the model falls short. Therefore, it is es-
izingmodelinternals,numerousstudies[10,33,52,54,60,74]have sentialforthesystemtosupportmulti-levelandmulti-faceted
tried to probe model knowledge through analyzing post-hoc model investigationofthemodel’smultimodalreasoningperformance.
behaviorswithinputvariations. Forexample,theWhat-IfTool[60] Initially,thesystemneedstopresentaglobaloverviewofmodel
andSliceTeller[74]identifiedspecificdataslicestounderstandmodel performance.Inthecontextofmultimodalreasoning,understand-
failures. M2Lens[54]leveragedtheSHAPtechnique[40]tocharac- inghowdifferentmodalitiesinteractiscrucialforinterpreting
terizeintra-andinter-modalinteractionsinmultimodalmodels,and the model’s behavior. Users need to recognize the modalities
MultiViz[33]extendedtoevaluatemodelsfordifferenttasks.Tenney themodelreliesonforitsdecisionsandhowthemodelbehaves
etal.[52]andCaberaetal.[10]proposedintegratedtoolsforunified whendifferentmodalitiespresentcomplementaryorcontradic-
modelevaluation. toryinformation.Aftergettingaglobalunderstanding,usersalso
Severalworks[7,17,20,55]haveprogressedbeyondmereunder- needtoknowhowevidencefromdistinctmodalitiesandtheir
standingtoalignmodelbehaviorwithhumanknowledge,therebyadapt- combinationsinfluencethemodel.Forexample,usersmaywant
ingandsteeringmodelstogeneratedesiredoutcomesforspecifictasks. toknowwhichtypeofvisualcuesorspokenwordsthemodel
SharedInterest[7]designedquantitativemetricsusingsaliencymethods interpretsaskeyindicatorsduringreasoning.Usersalsoneedto
tocomparehumanandmodelreasoningforidentifyingrecurringmodel inspectthemodels’reasoningandpredictionattheinstancelevel
behaviorpatterns.Hoqueetal.[20]andHeetal.[17]employeddata togetanintuitiveunderstandingandvalidatethealignmentof
programmingconceptstoinjecthumanknowledgeatscaleformodel rationaleswiththeoriginaldata.
improvement.CommonsenseVis[55]constructedknowledgegraphs R2 Provide comparative analysis of different prompt perfor-
withexternalknowledgebasestocontextualizemodelbehaviorsandal- manceMultipleaspectsofpromptsinfluencemodelreasoning
lowinteractivemodeleditingtoenhancespecificknowledgeforpoorly performance,includingthestructureandcontentofinstructions,
behavedareas.Asmentioned,promptingtechniques[11,45,50]offered aswellasthechoiceandorderofdemonstrationexamples.Nav-
anewinteractiveparadigmforhumanstoprovidefeedbackandinject igatingandexploringtheevolvingdynamicsofpromptsisnec-
knowledgeformodelvalidationandsteering. Ourworkexpandson essaryforuserstoidentifyinfluentialfactorsforimprovement.
theseideasbyallowingmodelpractitionerstoalignandenhancemodel Therefore,itisimperativeforthesystemtodocumentalterations
performancewiththeirknowledgeandexpertisethroughprompting. inprompts, supportstreamlinedtestingofprompts, andassist
usersintrackingandcomparingtheeffectofdiversepromptal-
terationsthroughoutthepromptrefinementprocess.Thisprocess
3 DESIGNREQUIREMENTS
facilitatesanunderstandingofhowdifferentmodificationsimpact
Ourgoalistodevelopavisualanalyticsapproachthatfacilitatesef- modelreasoningperformance,therebyofferingvaluableinsights
ficientpromptengineering,empoweringmodelpractitionerstoeffec- for users to provide appropriate feedback and make informed
tivelyadaptandsteerthemultimodalreasoningperformanceofLLMs decisionsregardingsubsequentiterations.
fortargeteddownstreamtasks. Throughasystematicunderstanding R3 Facilitateeffectivepromptrefinementindiverseandefficient
ofhowmodelsintegratemultimodalinformationforreasoning,users mannerAfterpinpointingareasofunderperformance,userscanFig.2:ThePOEMsystemframeworkcomprisesfourprimarymodules.(A)Thevisualandlanguagemodalityinformationfromthemultimodalvideo
datasetisprocessedbyexpertmodels,whicharethenfusedandfedintomultimodalLLMs.(B)Themultimodalreasoningunderstandingmodule
summarizedthenuancedmodalityinteractionsandpatternsatglobalandgrouplevels.(C)Thepromptiterationstrategyrecommendationpanel
providesdiversesupportforpromptrefinementwithsemi-automatick-shotexampleconstructionandinstructionalprinciplegeneration.(D)The
POEMinterfacefacilitatesefficientpromptperformanceexamination,promptrefinementassistance,andpromptmonitoringandcomparison.
alignandelicitmodelknowledgethroughrefiningprompts,This Panel.Subsequently,theycaninspectthemodel’smultimodalreason-
refinement involves more precise descriptions of the task and ingperformancefromdifferentlevelsofdetailintheReasoningPanel.
scenario,cleardelineationofprinciplesforthemodeltoadhere Specifically, atagloballevel, userscaninspectthemodel’soverall
to,andthesupplementationofillustrativeexamplesthathelpthe performanceandtheinteractionbetweenandwithinmodalities.Atthe
modeltograsptheintendedrelationships. However,giventhe grouplevel,usersareabletoscrutinizethemodel’sreasoningpatterns
vastnessofpotentialfeedbackoptions,itimposesahugecognitive concerningdifferentconceptsspanningacrossmodalities. Atthein-
burdenonuserstomanuallyrevisetaskarticulation,formulate stancelevel,userscanexamineindividualinstancesindetail.Userscan
principlesfromscratch,andsourcethemostinformativeexamples thenreviseandincorporateprinciplesandk-shotexamplesintoprompts
forlearning.Moreover,asthefeedbackusersintendtoprovide basedonautomaticrecommendationsandinsightsobtainedfromcur-
onthemodel’soutputoftenstemsfromtheirintuitionandexper- rentmodelandpromptperformanceexaminations.Therefinedprompt
tise,encompassingbothinductiveanddeductivereasoning[46], isthensenttothemodelforevaluation.IntheEvaluationPanel,users
thesystemoughttofacilitatetheconversionoftheseintuitive canevaluateandcomparetheeffectofeachpromptiterationonboth
insightsintospecificandeffectivepromptsinanefficientand globalmodelperformanceandindividualinstances.Additionally,they
user-friendlymannerformodelsteering. Specifically, thesys- canmonitorandtrackdetailedchangesacrossvariouspromptversions
temcanprovidefunctionalitiessuchassuggestingpromptswith anditerativelyrefinetheprompttoachievesatisfactorymultimodal
linguisticmodificationstosimplifycontentphrasing,inducing reasoningperformancewithalignedknowledge.
andtranslatingusers’fragmentedfeedbackintosystematicprinci-
plesthemodelcanfollow,andsourcinginformativeexamplesfor 4.2 DatasetandModel
userstoprovidehigh-qualityrationalesforknowledgealignment.
Wedemonstratetheeffectivenessofoursystemontwodifferentdatasets
4 SYSTEM&METHODS formultimodalcontentcomprehensiontasks: CMU-MOSEI[3]for
multimodalsentimentanalysis,andWTaG[4]foruserintentunder-
We designed POEM based on the distilled design requirements in
standing.TheCMU-MOSEI[3]datasetconsistsofmonologuevideo
Sec.3. Inthissection, wefirstintroducetheoverallsystemframe-
clipsinwhichspeakersexpresstheirsentimentsaboutaspecifictopic.
work.Thenweillustratethemethodsfordataprocessing,multimodal
TheWTaGdataset[4]comprisesegocentricvideoclipsofusersper-
rationaleunderstanding,andpromptiterationstrategyiteration.
formingcookingtasksundertheguidanceofaninstructorwithinan
augmented reality setting. The videos within both datasets contain
4.1 SystemFramework
informationfromtwoprimarymodalities:thelanguagemodality,rep-
Figure2demonstratestheoverarchingworkflowofthesystem. The resented by spoken content, and the visual modality, characterized
inputistherawmultimodalvideodataset,wherethevisual(e.g.,scene bythescenesanduserbehaviorsdepictedinthevideos. Following
andhumanbehaviors)andlanguage(e.g.,spokennarratives)compo- thepracticeinpriorworks[45,61],wespliteachdatasetintothree
nentscontainedwithinthevideosareseparatelyprocessedbyexpert subsets: avalidationset,ademonstrationexampleset,andatestset.
modelsandfusedastheinputofLLM.TheLLMthengeneratespre- Inthesplittingprocess,weensurethatthelabeldistributionremains
dictionoutcomesaccompaniedbyfree-textrationales(Fig.2A).Subse- consistentacrossthesesubsets.Thevalidationsetservesthepurposeof
quently,Thetopmultimodalrationaleunderstandingmodule(Fig.2B) promptiterationevaluation.Thedemonstrationexamplesetfacilitates
providesamulti-levelanalysisofthegeneratedmultimodalrationales. the construction of k-shot examples, and the test set provides addi-
Initially,itcharacterizesdifferentinteractiontypesbetweenmodalities tionalinstancesbeyondthevalidationsetforamorecomprehensive
(Fig.2B-1). Then,amultimodalreasoningpatternminingalgorithm assessmentofpromptefficacy.Thesizeofthevalidationsetneedsto
isemployedtoidentifyintricateandfine-grainedreasoningpatterns bemoderatesothatuserscangettimelyfeedbackduringtheprompt
(Fig.2B-2).Concurrently,thebelowpromptiterationstrategyrecom- iterationwhilealsocoveringdiversedatapatternsforcomprehensive
mendationmodule(Fig.2C)offersvariedsupportforbottom-upk-shot modelreasoningperformancediagnosis. Basedonourpreliminary
examplerecommendation(Fig.2C-1)andtop-downprinciplesumma- experiment,wemaintainadistributionratioof1:2:1forthevalidation,
rization(Fig.2C-2),aimingtoelicitandenhancespecificknowledgeto demonstration,andtestsets,respectively.Wealsoimplementedbatch
guideandimprovemodelperformance. processingtoimprovethesystem’sresponsespeed.
InthePOEMinterface(Fig.2D),usershavetheoptiontoeitherin- Tobridgethegapbetweennaturallanguageandothermodalitiesto
puttheirownpromptsorchoosefromavailabletemplatesinthePrompt extendLLM’sabilityformultimodaltasks,twoprimaryapproachesarecommonlyadopted[76].Thefirstapproach[21,27]leveragesadditional • Conflict-Distinct: when f M distinct from f 1 or f 2, where
trainingdatatomapmultimodalfeaturesintofeaturespacealigned d(f 1,f M)<θ,andd(f 2,f M)<θ
withlanguage.Thesecondapproach[6,57,72]exploitsthescheduling Inthispaper,weprimarilyfocusonthevisualandlanguagemodalities,
abilityofLLMstointegratecutting-edgeexpertmodelstoexplicitly which are the main subjects of investigation in current multimodal
translatedifferentmodalitiesintolanguagewithouttraining[77].We LLMresearch. Formoremodalities,theinteractioncharacterization
adoptedthesecondapproachduetoits“plug-and-play”characteristic frameworkcanbeextendedbypairwisecomparison.
andtheflexibilityitofferstochainvariousexpertmodelsforquicktask-
specificadaptation,asdifferenttasksmaynecessitatetheextraction
4.3.2 MultimodalReasoningPatternMining
andconsiderationofvaryingtypesofrelevantinformationinthesame
modality.Specifically,Forthetwodatasetsusedintheevaluation,we Upongaininginsightintothemodel’sreasoningprocessatthemodality
followedthepractice[4,72]andleveragedmodelssuchasBLIP2[27] interactionlevel,itbecomescrucialtoidentifythespecificconceptsor
and LLaVA [34] as expert models for information extraction from theircombinationswithinandacrossindividualmodalitiesthemodel
visualmodality. utilizesforreasoning.Weparsethegeneratedexplanationsintoalistof
Regardingthemodelsetting,weemployedthegpt-3.5-turbo-0125 intermediateevidencealongwiththeirassociatedinferencesthatcon-
tributetothefinalpredictionresult.Forexample,withinastep-by-step
model for the reasoning module due to its strong reasoning and
explanationofferedbytheLLM,thevisualevidenceofa“smile”isin-
instruction-followingabilities.It’simportanttonotethatourapproach
terpretedtoimplya“positive”sentiment.GiventheLLM’sgenerative
isdesignedtobemodel-agnostic.Itcanbeeasilyadaptedtoothermulti-
characteristics,whichresultinthevariabilityofevidenceacrossdif-
modalcontentcomprehensiontasksandLLMs.Wesetthetemperature
ferentexplanations,weemployedthetext-embedding-ada-002model
to0tocontroltherandomnessandensurereproducibility.Giveneach
testinputdatapointxt,wefollowthecommonlyadoptedpractice[75] tocalculateembeddingsfortheseextractedconcepts. Subsequently,
toorganizetheinputpromptintheformatof(I,{x i,y i}k i=1,xt).Here,I Wwe eu idti eli nz te ifid et dhe thH eD eB viS dC enA cN
e
a lolg co atr eit dhm clo[ s4 e2 s] tt to oc tl hu este cr lut sh te es re ’se cv eid ne trn oc ie d.
isthetask-specificinstructionselaboratingonthetargetedscenarios,
astherepresentativeconceptforeachcluster. Subsequently,weuti-
taskstobefinished,andexpectedoutputstructure(e.g.,returnyour
lizedtheApriori[1]algorithmtoidentifyfrequentpatternsofconcept
answerinaJSONobject). TopropeltheLLMtoperformCoTrea-
co-occurrencewithinandacrossdifferentmodalitiesinthegenerated
soning,Thepromptcouldincludeinstructionslike“Pleaseprovidea
rationales.Thisapproachenablesuserstoconductamoresystematic
step-by-stepanalysis”. {x i,y i}k
i=1
isthedemonstrationexampleset.
andhigh-levelanalysisofthepatternswithinthegeneratedrationales.
Eachdemonstrationexampleincludesinputx iandoutputy i.Wealso
explorethezero-shotsettingwheredemonstrationexamplesarenot
4.4 PromptIterationStrategyRecommendation
provided.Finally,thetestinputxtisprovided,andtheLLMisexpected
togeneratetheoutputy,containingapredictionresultandfree-text Asmentionedin Sec.2,thecontentandphrasingofinstructions,along
explanationforeachinput. withthechoiceofdemonstrationexamples,caninfluencethemodel’s
performance.Ourpreliminaryexperimentandexpertinterviewresults
suggestedthattheinstructioncontent(e.g., taskspecifications)and
4.3 MultimodalRationaleUnderstanding
thechoiceofdemonstrationexamplesexertamorepronouncedeffect
4.3.1 ModalityInteractionCharacterization onmodelperformancethantheprecisewordingusedforourtargeted
multimodalreasoningtasks.Therefore,inthispaper,wemainlyfocus
Comprehendingtheinformationutilizedbythemodelwithindistinct
onfacilitatingusersininstructioncontentrefinementanddemonstration
modalitiesandhowitcombinesthiscross-modalinformationtomake
exampleconstruction.
inferencesisfundamentaltogaininginsightintothemodel’sreasoning
process.Severalworks[33,54]havetriedtocharacterizetheinteraction
4.4.1 K-shotExampleRecommendation
betweendifferentmodalitiesbasedonaggregatedfeatureattribution
values[40,47].Therearealsoworks[30–32,68]tryingtoquantifythe Few-shotpromptinghasbeenadata-efficientstrategytoadaptLLMs
degreeofinteractionsbetweenmodalitieswithapartialinformation forspecificdownstreamtasksusingmerelyahandfulofillustrative
decompositionframework.Buildingonthefoundationoftheseworks, input-outputpairs.However,theeffectivenessoftheadaptationheavily
wecharacterizethemodalityinteractioninthecontextofourtargeted reliesonthechoiceofexamplestoinformthemodelaboutthedesired
multimodalpredictiontasksasfollows: mapping[24,61].Identifyinginformativeexamplesforeffectivelyguid-
LetX iandYdenotethesamplespaceofindividualmodalityfeatures ingthemodelcanbechallengingforusers.Moreover,beyondsimply
andlabels,while∆Ydenotestheprobabilitysimplex,andδ denotes pairinginputswithfinalanswers,reasoningnecessitatesprovidinga
probability.Theunimodalclassifierusingasinglemodalityforpredic- rationaleforeachexample,whichisequallydifficultforuserstocraft
tionis f i:X i→∆Yandthemultimodalclassifieris f M:X 1×X 2→∆Y. ontheirown.
Here,consideringtheresultsoftwoclassifiers f iand f j,thedistance Toenhancetheefficiencyofsourcingthedemonstrationexample
(cid:0) (cid:1) (cid:13) (cid:13)
functioncanbedefinedasd f i,f
j
=(cid:13)δi−δj(cid:13)tomeasurethedis- set,wefirstemployedthek-nearestneighborsalgorithmtosamplethe
tancebetweenthepredictionresultsofthetwoclassifiers. Basedon candidatek-shotexamplesetconsideringbothrelevancyanddiversity.
thedistancefunction, wecandefinetwobasicinteractiontypesbe- Specifically,weleveragedthetext-embedding-3-smallmodeltoextract
tweentwomodalities.Whend(f 1,f 2)<θ,whereθ isapre-defined embeddingsforbothvalidationinstancesandthosewithinthedemon-
threshold,theinteractiontypeiscomplement,indicatingthesetwo strationexamplepool. Foreachvalidationinstance,weidentifiedits
modalitiescontributetothefinalpredictionresultinthesamedirection. k-nearestneighborsaspotentialcandidates. Thesecandidateswere
Conversely,whend(f 1,f 2)>θ,theinteractiontypeisconflict,indicat- thenrankedbytheircosinesimilarity. Toselectthefinalk-shotex-
ingthesetwomodalitiesprovidediscrepantinformationforprediction. amples,weprioritizedbothrankingandlabeldiversity,ensuringthe
Byfurtherconsideringhowpredictionswillchangewhenanalyzing inclusionofallpossiblelabelsinthefinalsettopreventmodelbias.To
informationfromeachmodalityindependently,andcombininginfor- streamlinetheprocessofcraftingrationalesforusers,weintegrated
mationfromtwomodalitiesjointly,i.e.,thedistancefunctiond(f 1,f M) thegpt-4-turbomodeltoautomaticallygeneratestructuredrationales
andd(f 2,f M),wecandefinesubdividedinteractiontypeas[30,32]: foreachdemonstrationexamplebasedonitsgroundtruthanswer.This
• Complement-Redundant: when f M aligns with f 1 and f 2, approachoffersusersapreliminarybasisforrefinement,sparingthem
whered(f 1,f M)<θ,andd(f 2,f M)<θ theneedtobeginfromscratch.Furthermore,weutilizedthefeedback
• Complement-Distinct:when f M distinctfrom f 1and f 2,where providedbyuserstoiterativelyenhancethequalityofthegenerated
d(f 1,f M)>θ,andd(f 2,f M)>θ rationales.Thesedemonstrationexamplesarethencombinedintothe
• Conflict-Dominant: when f M aligns with f 1 or f 2, where sequence{x i,y i}k i=1forinclusionintheprompt,wherey iistherationale
d(f 1,f M)<θ andd(f 2,f M)>θ orswitchthe f 1and f
2
andfinalanswerprovidedbyusersforx i.4.4.2 InstructionalPrincipleGeneration 5.2 ReasoningPanel
While k-shot examples aim to inductively teach the model the cor- TheReasoningPanel(Fig.1B)facilitatesathoroughinvestigationofthe
rectmappingbetweeninput-outputpairs,providingexplicitprinciples model’smultimodalreasoningbehaviors,fromglobalandsub-group
regardingproperpracticesorclarifyingtherationalesbehinderrors patternsdowntospecificindividualinstances(R1).
has also proven to be an effective strategy for drawing out desired Athree-layerSankeydiagram-baseddesign(Fig.1B-1)isadopted
knowledgeandguidingmodelperformance[46,73].Humansgenerally toportrayinteractionsamongmodalitiesatthegloballevel.Thefirst
formulateprinciplesintwoways. Oneinvolvesdirectlyleveraging layerdemonstratestheoveralldistributionofpredictionclassesand
theirexistingknowledge. Forexample,theprincipleforidentifying errorswithtwoverticallystackedbarcodecharts.Thehorizontallength
sarcasmcouldbe“payattentiontotheinconsistencybetweenaword’s encodesthenumberofinstances,andthecolorencodesthecorrespond-
literal interpretation and its contextual meaning.” The other is that ingclassanderror.Instancesbelongingtothesameclassarepositioned
individuals derive lessons from specific instances and subsequently close together to enable easy exploration both within and between
aggregatetheseinstance-levelinsightsintohigher-levelprinciplesin classes. Thesecondintermediatelayersummarizestheconflictand
abottom-upmanner. However, usersmayfinditdifficulttoimme- complementrelationshipbetweenvisualandlanguagemodalitiesand
diatelygenerateprinciplesfromscratch,deriveinsightsbymanually adoptsthesameencodingasthefirstlayer.Thethirdlayerdelvesinto
examininginstancesoneatatime,andfullyarticulatetheirprinciples thefine-grainedfourtypesofmodalityinteractions. Whileretaining
consideringthecomplexitiesinvolvedinmultimodalreasoning.There- thesamevisualencodingforthepredictionclassanderrordistribution
fore,weemployedanauxiliaryLLMtofacilitatethesummarization ineachinteractiontype,thislayerintroducestwoadditionalbarcode
andrecommendationofprinciples.Forthispurpose,weharnessedthe chartstodelineatethepredictionresultofthesinglevisualandlanguage
capabilitiesofthegpt-4-turbomodel,whichisrecognizedforbeing modality,thusillustratingthedetaileddistributionsofthetwomodali-
amongthemostpowerfulLLMswithextensiveknowledge. tiesacrossvarioustypesofinteractions.Besides,twoadjacentlayers
Specifically,weinstructedtheauxiliaryLLMtoproduceprinciples areinterconnectedthroughflows,thewidthofwhichisproportionalto
atbothinstance-specificandinstance-agnosticlevels.Attheinstance- thenumberofinstancestheyencompass.Hoveringovertheflowswill
specificlevel,theauxiliaryLLMistaskedwithanalyzingdiscrepancies highlighttherelatedinstancesacrossallthreelayers. Userscanalso
betweengeneratedreasoningandgroundtruthanswersforeachindivid- brushthebarcodechartineachlayertoselectaninterestedgroupof
ualinstance,summarizingpotentialerrorcauses,andfurtherderiving instancesforfurtherinvestigation.
principlestoavoidsimilarmistakes.Attheinstance-agnosticlevel,We Afterselectingtheinterestedgroupofinstances, themultimodal
instructtheauxiliaryLLMtocondensethegeneratedinstance-specific reasoningpatternminingalgorithmisapplied,withtheextractedpat-
principlesintomoregenericprinciplestailoredtothespecifictargeted ternsdisplayedintherighttable(Fig.1B-2). Eachrowexhibitsone
task.Itisimportanttoacknowledgethatthegeneratedprinciplesmay distinctpatternwithitsrepresentativevisualandlanguageconcepts,
notalwaysbeaccurateandshouldnotbetreatedasgoldenrules.Their support(i.e.,containedinstancenumbers),anderrorstatistics. They
primarypurposeistoprovokethoughtandinspireuserstoconceive cansortandfilterthepatternsbasedonthesestatisticsbyclickingon
newideasorenhanceexistingonesratherthaninitiatefromzero.Thus, the corresponding column. The representative language and visual
usersareempoweredtoeitherinputandcreatetheirownprinciplesor conceptsareshownforintuitivepatternunderstandingbyusers.Adja-
choosetoamendandreviseprinciplesthathavealreadybeengenerated centtoeachconcept,astackedbarchartpresentsthedistributionofits
accordingtotheirpreferences. associatedclass.Userscanexpandeachrowtoviewthedetaileddistri-
butionofevidenceinawordcloud,whereeachphrase’ssizerepresents
itsfrequencyofoccurrenceanditscolordenotestheassociatedclass.
5 INTERFACEDESIGN
Userscanselectpatternsorevidenceofinterestbyclicking,andthe
ThePOEM interface(Fig.1)consistsofthreecoordinatedviewsto correspondinginstanceswillbedisplayedintheinstanceviewbelow.
assist users in seamlessly evaluating the impact of prompts on the Theinstanceviewbelow(Fig.1B-3)isdesignedtoexpeditethe
multimodal reasoning performance, refining prompts through semi- examinationandverificationofindividualinstancesbyshowcasingthe
automaticsuggestions,andconductingiterativetestingofprompts.In originalmultimodalvideocontentalongwithitsdetailedreasoning.
thissection,weintroducethedesignofeachviewandtheinteractions Therawdatacolumnexhibitsthekeyframesequenceandthespoken
thatconnectthemindetail. narrativeinthevideotoenablequickvideocontentdigestion. Users
canhoverovertheseframesforanenlargedviewandplaybackthe
5.1 PromptPanel originalvideoforrapidverification.Subsequentcolumnspresentthe
groundtruthlabels,themodel’spredictions,andthegeneratedfree-text
ThePromptPanel(Fig.1A)providesflexiblepromptoperationsto
rationales.Toenhancereadabilityandquicktextcomprehension,evi-
supportsmoothpromptengineeringexperience(R3).Uponselecting
denceishighlightedwiththecorrespondingcolorofitsassociatedclass.
the dataset and model, users can craft the prompt on their own or
The ground truth and prediction columns are also colored for easy
initiate by selecting from a list of prompt templates collected from
comparison.Inadditiontodisplayingselectedvalidationinstancesfor
state-of-the-artbenchmarks[65]inthePromptEditor(Fig.1A-1).The
review,userscantoggletotheK-shotExampleMode.Withinthismode,
promptisorganizedintodistinctsections,asintroducedin Sec.4.2,to
theinterfacepresentstherankedlistofk-shotexamplesrecommended
facilitateaclearandstraightforwardeditingexperience.Userscanalso
bytheproposedsamplingstrategy. Foreachexample,theinterface
switchtotheplaintexteditingmodeforeditingandformatchecking
detailsitsrawdata(i.e.,keyframesequenceandspokennarrativesin
beforesubmission.ThePrincipleRecommendationview(Fig.1A-2)
therawvideo),itsgroundtruth,andtherationalesgeneratedbytheaux-
displaysanorganizedsummaryofprinciplesforuservalidation.The
iliaryLLM.Usersareabletomodifythecontentinthecorresponding
generatedinstance-specificandagnosticprinciplesaredifferentiatedby
columndirectlytoprovidehigh-qualityrationales.Moreover,userscan
backgroundcolors:grayforinstance-specificprinciplesandgreenfor
sourcemorek-shotexamplesforselectionandannotationbyclicking
instance-agnosticprinciples.Newlygeneratedprinciplesaremarked
the“Retrieve ”button.
withreddotsatthetoprightforhighlighting. Userscanmodifyany
existingprinciplebyutilizingtheeditingfunctionorarticulatetheirown
5.3 EvaluationPanel
principlesviatheprincipleinputbox.Furthermore,usersareallowedto
deleteanyprinciplesdeemedinappropriateorredundant.Subsequently, The Evaluation Panel (Fig. 1C) offers comprehensive insights into
uponselectingthedesiredprinciples,userscanintegratethemintothe bothglobalandlocalperformanceofprompts,alongwiththeprompt
currentpromptwithintheprompteditorbyclickingthe“Import ” iterationhistoryforefficientmonitoringandcomparisonofprompt
button.TheK-shotExampleList(Fig.1A-3)belowprovidesaconcise performance(R2).
summaryofK-shotexampleswithuser-annotatedrationales,waiting ThePromptHistoryview(Fig.1C-1)archivespreviousprompts
forfurthereditingorinclusionintotheprompt. regardingtheircontentandperformance.Eachrowrepresentsapromptversionwithitsaccuracyandmodificationsaresummarizedusingintu- positiveinfluence,whilethelanguagemodalitysuggestedanegative
itiveicons.Thisdesignenablesuserstoeasilycompareperformance one. Theultimatecombinedeffectwaspositive,indicatingthatthe
andtracealterationsindifferentsectionsoftheprompts. Userscan visualmodalityoutweighedthereasoningprocess.
expandandcollapseeachrowforahierarchicalexaminationofmodifi- Followingthis,E5brushedthisgroupofinstancestofurtherinspect
cationswithineachpromptsection.Detailedadditionsanddeletions their contained reasoning patterns in the table on the right. When
inthecontentaredistinctlymarkedandhighlightedthroughvaried goingthroughthepatternsindescendingorderoferrorrate,E5dis-
colorsandlinestyles.TheOverallModelPerformanceview(Fig.1C- coveredthecombinationofthelanguageconcept“didn’tlike”with
2)recordstheglobalperformancestatisticsofeachpromptiteration. thevisualconcept“smile”yieldedhigherrorrates(Fig.3B).Theadja-
Userscanexpandeachrowtoinspectthedetailedconfusionmatrix. centbarcharts,predominantlyshadedinbluefor“didn’tlike”andred
TheInstanceTestview(Fig.1C-3)exhibitstheperformanceofprompts for“smile,”,indicatedthattheLLMconsistentlyinterpretedlanguage
onindividualinstancesthatareofparticularinteresttousers.Userscan evidenceconcludedwith“didn’tlike”asanegativesignalandvisual
selectinstancesfromReasoningPanelandsavethemtoobservetheir evidencesummarizedwith“smile”aspositiveduringthereasoning
performancechangeduringpromptiterations. Theycanalsosource process.Shefurtherexploredthispattern,whereevidenceunderthelan-
additionalunseentestinstanceswiththe“Retrieve ”function.The guageconcept“didn’tlike”includedphraseslike“arduous”,“boring”,
viewalsoallowsuserstoconvenientlyaccesstherawdataandgenerate and“hate”highlightedinblue(Fig.3B-1),whilethevisualconcept
rationalesbyexpandingtheinstancerow. “smile”comprisedinstancessuchas“smallsmile”,“slightsmile”,and
“smiling”markedinred(Fig.3B-2).E5thoughttheseinferencesfor
5.4 Cross-ViewInteractions eachindividualmodalityreasonablebutwonderedhowthecorrectly
ThePOEM providesampleinteractionswithinandacrossviewsto deducedevidenceledtothefinalerror. Therefore,sheproceededto
provideaseamless,promptengineeringexperience. inspectthedetailedreasoningsofindividualinstancesexhibitingthis
Brushing: Userscanbrushthebarcodechartineachlayerinthe patternintheinstanceviewbelow(Fig.3C).Uponexaminingtheraw
ReasoningPanel,andthenthecorrespondingminedpatternsandin- dataandtherationalesgeneratedbytheLLM,E5figuredoutthatin
stancedetailswillbedisplayedontherightandbelow,respectively. thesecases,thespeakershadexplicitlystatedtheirnegativeopinions
Clicking: Users can select instances in the Reasoning Panel for verballywhiledisplayingmildpositivefacialexpressionslikegentle
principlegenerationandtesting. Clickingthe“Generate ”button smiles. However, the LLM was biased by the positive visual cues,
listsgeneratedprinciplesinthePrincipleRecommendationview.The allowingthemtoovershadowanddominateitsreasoning,despitethe
“Save ”buttonallowsuserstosaveinstancesintheInstanceTestview explicitandapparentnegativesentimentconveyedthroughlanguage.
fortestingandtheK-shotExampleListforimporting.The“Retrieve
”buttonretrievessimilartestinstancesormorek-shotExamples.
6 EVALUATION
Inthissection,weshowcasetheefficacyandefficiencyofPOEMvia
twocasestudiesandfeedbackgatheredfromexpertinterviews. The
primaryobjectiveofthetwocasestudiesistohelpusersobtainwell-
performingpromptsutilizingtheirdomainexpertiseandknowledgeto
guideLLM’smultimodalreasoningperformancewithminimaleffort.
6.1 CaseOne: Improvingmultimodalsentimentreasoning
withCMU-MOSEIdataset
E5,asentimentanalysisexpert,seekstogenerateeffectiveprompts
forsteeringLLM’smultimodalsentimentreasoningperformancewith
theCMU-MOSEIdataset. TheLLMistaskedwithinterpretingthe
speakers’ verbal and visual signals to determine their sentiment as
“positive”,“negative”,or“neutral”.
Afterloadingthedatasetandmodel,E5initiallyselectedtwopro-
videdprompttemplatesinthePromptPanelandsubmittedthemto
evaluatetheirperformanceinsequence(R2).IntheEvaluationPanel,
shenoticedthatversiontwoexhibitedhigheraccuracythanversion
one(Fig.1C-1),wheretheiconindicatedthatversiontwoonlyhad
modificationsinthetaskpromptsectioncomparedwithversionone.
E5expandedversiontwotoinspectthemodificationdetailsandfound
thatthesoledifferencewasversiononedirectedtheLLMtopredictand
thenexplain,whereasversiontwowasfirsttoexplainandthenpredict.
This finding suggested that this LLM may exhibit better reasoning Fig.3: (A)Identifieddenseerrorareasinconflict-dominant modality
performanceadoptingtheexplainandpredictschema.Subsequently, interaction. (B) The multimodal pattern “didn’t like” and “smile” and
E5decidedtorefinetheprompttakingversiontwoasthebasis. their associated evidence group. (c) The error cases where “smile”
E5beganbyexaminingtheSankeydiagramintheReasoningPanel predominatedandbiasedthereasoningprocess.
togainanoverviewoftheinteractionsbetweenvisualandlanguage
modalityintheLLM’sreasoningprocess(R1). Throughobserving Followingthisdiscovery,E5decidedtoderiveprinciplesfromthese
thelengthanderrordistributionofthebarcodechartsinthesecond erroneous cases to guide the LLM toward correct reasoning in this
layer,E5noticedthatforalargeproportionofinstances,thevisualand situation(R3).Therefore,E5selectedtheseinstancesandclickedthe
languagemodalitiesprovidedcomplementaryinformation,whilefor “Generate ”togenerateprinciples. Shealsosavedtheseinstances
others,theypresentedconflictinginformationwithincreasederrors.E5 ofinteresttotherighttestpanelforfurthervalidation. InthePrinci-
wasparticularlyinterestedinunderstandinghowtheLLMreasoned pleRecommendationView,E5reviewedthegeneratedprinciplesand
inscenarioswherethetwomodalitiesshowedconflictinginformation identifiedwell-articulatedgeneralprinciplesthatunderscoredtheim-
andhowtheerrorsoccurred.So,shedelvedintothethirdlayerfora portanceofinterpretingvisualcuesalongsidethecorrespondingverbal
morefine-grainedinteractionrelationshipexamination. Atthethird contentwithcarefulconsiderationofspecificcontext(Fig.4A).To
layer (Fig. 3A-1), she pinpointed a dense area of errors within the ensuregeneralizabilityandavoidintroducingnewbias,E5addedthe
conflict-dominant relationship, wherethevisualmodalityimplieda lastsentenceas“Itiscrucialtoavoidoveremphasizingonemodalityoveranotherwhenthelattercarriesclearindicationsofopinionsor modeltofollow(R3). Thus,herevisedtheprompttoaddtheclari-
explicitexpressionsofsentiment.” Then,E5importedthisprinciple ficationsuchas“SelfDescriptionreferstoscenarioswheretheuser
intotheprompteditorandsubmitteditfortesting.IntheModelPerfor- narratesorexplainswhattheyaredoing,intendtodo,ortheirthought
manceView,shefoundaslightimprovementintheoverallaccuracy. processregardingthetaskathand.” alongwithclarificationforthe
Meanwhile,intheTestPanelView,shecheckedtheperformanceofthe remainingclasses. Whilesubmittingthispromptfortesting,E6also
newpromptonpreviouslysavedinstances,themajorityofwhichhad thoughtthatbesidesgivingexplicitexplanations,shecouldalsoadd
beencorrectlyreasoned.Thisindicatedthattheincorporatedprinciple someconcretek-shotexamplesforthemodeltolearn(R3).Therefore,
hadsuccessfullyguidedtheLLMtoapplythecorrectknowledgefor henavigatedtotheK-shotExampleModeintheReasoningPaneland
reasoninginthisscenario. selectedfiveK-shotexamples,eachrepresentingadistinctclassfrom
Subsequently,E5soughttoenhancethemodel’sreasoningstability thetoprecommendedones(Fig.5A).E6alsonoticedthattherationales
anditsabilitytorecognizevariedpatternsinsentimentanalysisby generatedbythemoreadvancedauxiliaryLLMalsocontainederrors
incorporatingsomek-shotexamples(R3).Thus,sheswitchedtoK-shot forthe“SelfDescription”class.Thisfindingsuggestedthattheconcept
ExampleMode,whererecommendedK-shotexampleswithreasonings mightbechallengingforLLMstograspandreason,underscoringthe
craftedbytheauxiliaryLLMwerelisted. E5selectedthetopthree needforprovidingadditionalguidanceintheprompts.Followingthe
instancesspanningdistinctclassesandrefinedtheprovidedreasoning refinementofrationalesforthek-shotexamples,E6importedthese
leveraginghisknowledgeandexpertise.Uponcompletingtherationale annotatedexamplesandsubmittedthispromptversionfortesting.E6
annotations,E5appendedtheseexamplestotheK-shotexampleliston thenexaminedtheupdatedtestoutcomesintheEvaluationPanel(R2).
theleftsideandimportedthemintotheprompt.Afterrunningthetest, Theincreasedaccuracyanddarkercoloralongtheconfusionmatrix’s
theoverallaccuracyincreasedto82%,withbothprecisionandrecall diagonalprovedthatprovidingeitherexplicitexplanationsork-shot
ofthetemplateshowingimprovements(Fig.4B). examplescanhelpimprovetheLLM’sreasoningperformance,while
k-shotexamplesexhibitedamorepronouncedinfluenceinthiscase.
Fig.4:(A)Therecommendedprinciplesforalleviatingerrorsincaseone.
(B)Therecordedpromptiterationhistoryincaseone.
6.2 CaseTwo: EnhancingMultimodalUserIntentionUn-
derstandingwithWTaGdataset
E6isanengineertaskedwithbuildinganintelligentvirtualassistant Fig.5: (A)Theselectedandannotatedk-shotexamplesfromdistinct
tohelpusersperformcomplextaskswithinaugmentedrealityenvi- classes.(B)The“uh”patterninfluencedthe“Hesitation”classreasoning.
ronments.Buildingsuchanassistantnecessitatescomprehendinguser (C)Therecommendedprinciplestoguide“Hesitation”classreasoning.
intentions.E6thuswantedtosteerthemultimodalLLMusingPOEM (D)Thetestresultsofaddedoutofdistributioninstances.
tofinish thistask. E6 experimented ontheWTaGdataset[4], sup-
plyingthemultimodalLLMwithvideoclipscapturedfromtheuser’s
egocentricperspective.Theseclipsincludeduser-instructordialogues E6furtherexploredtheperformancespecificsofthelatestprompt
recordedwithmicrophonesandvisualcontextthatencompassesthe version(enhancedwithk-shotexamples)intheReasoningPanel(R1).
scene and user behaviors captured by head-mounted cameras. The Here, henoticedaclusteroferrorsinthethirdlayeroftheSankey
LLMneedstodeducetheuser’sintentionbasedonmultimodalcon- diagram,associatedwiththeconflict-dominantinteractiontype. The
textandcategorizeitintooneoffiveclasses: “Question”,“Answer”, consistentyellowcolorofthelanguagemodalityandtheoverallpre-
“Confirmation”,“Hesitation”and“SelfDescription”. dictionsuggestedthatlanguagemodalitypredominatedthereasoning
Afterinitializingthedatasetandmodel,E6firstchosetousethe process,andalltheseinstancesweremisclassifiedasthe“Hesitation”
prompt available in the dataset repository for validation (R2). The class. Inthepatterntable,heidentifiedafrequentlanguagepattern,
EvaluationPanelrevealedthatthispromptonlyattained40%accuracy “uh”,associatedwithahigherrorrate.Itsbarchartwasentirelyyellow
inazero-shotsetting,consistentwiththeresultsreportedintheresearch (Fig.5B).Heexpandedtherowandfounditincludedevidencesuchas
paper[4].E6nextexaminedtheconfusionmatrix,observingthatthe “uh”and“oh”thatindicate“Hesitation”.Therefore,E6clickedtherow
model’spredictionswereseverelybiasedtowardsthe“Confirmation” toexaminethespecificinstancesitincluded.Hefoundthatwhenever
and”Answer”classes.Furthermore,E6brushedinstancesincorrectly the spoken content contained modal words like “uh” and “oh”, the
classifiedunderthesetwoclassesintheReasoningPaneltoclosely modelwouldinterprettheseasindicatorsofunwillingnesstocontinue,
analyzethemodel’sreasoningdetailsandidentifythereasonsforer- therebypredictinguserintentionas“Hesitation”withoutconsidering
rors(R1).Uponrandomlyinspectingthemodel-generatedrationales anyotherfactors.Consequently,E6selectedtheseinstancestoletauxil-
alongsidetherawdata, E6observedthatthemodelwascapableof iaryLLMsummarizeprinciplesforavoidingsucherror(R3).ThenE6
adequatelydescribingandanalyzingboththevisualandspokencontent. refinedandincorporatedtheseprinciples(Fig.5C)intothepromptand
However,itstruggledtocomprehendthemeaningofdesignatedpredic- savedtheseinstancesintheInstanceTestview.Additionally,healso
tionclasses,especially“SelfDescription,”leadingtoscarcepredictions addedmultipleinstancesfromherownprojectintotheInstanceTest
forthisclassandinsteadbiasedtomorefamiliarandrecognizedclasses viewtoevaluatethepromptrobustness(R2).Thetestresultsshowed
suchas“Confirmation”and“Answer.” thattheaccuracyreached75%,withtheaddedtestinstancescorrectly
Toaddressthisproblem,E6decidedtoincludemoreexplicitexpla- predicted(Fig.5D).E6wassatisfiedwiththisresultandintendedto
nationsofeachpredictionclasswithinthepromptinstructionsforthe usethepromptforhisproject.6.3 ExpertInterviews applyprinciplesmoreeffectivelyacrossvariedtasksandcontextsre-
mainsafertileareaforresearch.Aspointedoutbypriorworks[46,73],
Wefurtherconductedsemi-structuredinterviewswithtwoacademic
thereisnoone-size-fits-allprinciplegranularity,astheeffectiveness
researchersandoneindustryresearchscientist(P1-P3)toverifythe
varieswithtaskcomplexity,datasetdiversity,andprinciplequality.In
effectivenessandusabilityofPOEM.Allparticipantshadexperiencein
ourwork, weprovidebothspecificanduniversalprinciplesforbal-
promptengineeringandthetrainingoradaptionofmultimodalLLMs
ancingbothuniquenessandgenerability. Identifyingandcraftingan
fordownstreamtasks,whilenonehadpreviouslytriedthePOEMbe-
effectivesetofprincipleswithsuitablegranularityfordifferenttasks
foretheinterviews.Eachinterviewbeganwiththeresearchbackground
remainsanopenquestion.Moreover,currentuserscanonlyarticulate
introduction,followedbythesystemworkflowandfunctiondemon-
principlesinnaturallanguagewheremorediverseinteractions(e.g.,
strationwithexamples. Expertsweretheninvitedtofreelyexplore
clicking in SAM [26]) can be integrated to enable users to provide
thesystemusingrealdatasets,voicingtheirthoughtsinathink-aloud
morenuancedandprecisefeedback.Meanwhile,managingtheaccu-
manner.Thegatheredfeedbackissummarizedbelow:
mulatedprinciplesisnon-trivialduetoconflictandforgettingissues.
SystemworkflowAllexpertsconcurredthattheworkflowofPOEM
Usersmayalsostruggletograsptheinfluenceofvaryingprincipleson
isthoughtfullydesigned,enhancingtheefficiencyofpromptiteration
modelperformance.UtilizingLLMstocondenseanddifferentiatethe
comparedtotheircurrentpractices,wheretheyreliedsolelyonper-
patternsandimpactsofprinciplescouldserveasapotentialsolution.
formancestatisticsforevaluatingprompteffectsandlaboriousmanual
Ontheotherside,whileprinciplesaremosteffectiveforlargemodels
experimentstosearchforbetter-performingones.AsP2noted,“Ithink
possessingrobustinstruction-followingcapacities,theycanalsobenefit
POEM offersamoresystematicandcomprehensivewaytoanalyze
smallermodelsbyguidingdatasetretrievalandgenerationformodel
the model multimodal reasoning performance.” P1 also mentioned
fine-tuning. Furthermore,asdemonstrationexamplesandprinciples
thevariedstrategiesandstreamlinedprocessprovidedbyPOEMno-
representtwodistinctapproachesofinjectingandelicitingknowledge
tably“reducethepainforpromptwritingandtesting”asdeveloping
forreasoninginbottom-upandtop-downmannerrespectively,how
principlesandchoosingk-shotexamplesarechallengingtasksontheir
tocollocatek-shotexampleswithprinciplestomaximizeinformation
own.TherecommendedprinciplesandK-shotexamples“serveasgood
gaininpromptengineeringremainsacompellingquestion.
startingpointstobringnewperspectivesandinspirethoughts”.
SystemdesignsandinteractionsAllexpertscommentedthatthe
SystemgeneralizabilityandscalabilityInthispaper,wemainlyfo-
visualandinteractiondesignofPOEMisintuitiveandeasytolearn
cusontheinteractionbetweenthetwomost-studiedvisualandlanguage
anduse.P3expressedaparticularfavorforthePromptHistorydesign,
modalities. However,oursystemcanbeextendedtoinvestigatethe
whichmakesiteffortlesstotrackeverydetailofchanges,“asIusually
interactionsbetweenmultiplemodalitiesbypair-wisecomparison.Be-
getlostafterseveralroundsofpromptiteration.NowIcanstartwith
sidestheanalysistasksevaluatedinthispaper,theproposedframework
anyversionatease. ” P1valuedtheconvenientgenerateandimport
isreadilytobeutilizedforothermultimodalcontentcomprehension
at one-click function, saving tons of time in manually editing and
andreasoningtaskssuchasmultimodalhateorsarcasmrecognition,
formattingtheprompt.P2appreciatedthecapabilitytoexamineand
andmultimodalcontextquestionanswering[65],wherethemodality
evaluateattheinstancelevelwithreferencetorawdata.Hehighlighted,
interactionrelationshippersistentlyexist. Thisprompting-basedsys-
“Sincehallucinationscanhappeninevitablyandglobalperformance
temcanalsoserveasatestingtooltouncoverweaknessesinmodel
metricsarenotenough,havingaccesstoinstance-specificdetailsfor
multimodal reasoning performance and identify example types and
validationsignificantlyincreasedmytrustforthesystemandconfidence
principlestoinformlarger-scaledatacollectionformodelfine-tuning.
inthepromptsIdeveloped”.Meanwhile,expertsmentionedthatittook
Moreover,thedesignofthesystemcanbeextendedforotherapplica-
themsometimetounderstandandproficientlyusetheSankeydiagram,
tions.Forexample,theReasoningPaneldesigncanbeusedforother
yettheyacknowledgedthatthecomplexityofmultimodalreasoning
tasksthatnecessitatesummarizingrelationshipsacrossvariousinforma-
performancenecessitatessuchadesign.
tionchannelsatmultiplelevels.Thehighlighteddifferencedesignin
Suggestions for improvement P1 proposed that the generated
PromptHistoryviewcanalsohelptextsummarizationandcomparison
instance-specific principles can be visually linked to their originat-
tasksinastructuredandintuitiveway.Thesystemscalabilityisrooted
inginstancestoofferamoreintuitiveandcomprehensiblereference.
inthealgorithmandvisualdesign.Thebottleneckofthealgorithmpart
P2 wished to have the function that the system can recommend in-
isthetimecostofprocessingthevideodatasetandLLM’sgeneration
stancesbasedonusers’inputhigh-levelcriteriaforfurtherevaluation
speed.Currently,wehaveimplementedbatchprocessingtoexpedite
ordemonstration.Healsothoughtitwouldbegreatifthesystemcould
thedataprocessingandgenerationprocessforasmoothprompting
helpsummarizetheusers’annotatedrationalestoidentifypotential
experience. However,thisapproachmaynotsufficeforhandlingthe
ambiguityandconflict.P3thoughtitwouldbeinterestinganduseful
datascaleofthousandsofinstances,necessitatingtheexplorationof
toenablemultipleLLMcomparisons.Besides,step-by-stepguidesare
strategieslikeparallelcomputinganddatasamplingtoensureinstant
wantedduringtheirreal-timeexplorationtoreducethelearningcurve.
feedback. Forthevisualdesign,TheSankeydiagramdesigninRea-
soningPanelmaybecomevisuallyclutteredwhendealingwithalarge
7 DISCUSSION
numberofpredictionclassesorcomplexmodalities.Forthissituation,
Inthissection,wediscussthePOEMregardingknowledgealignment wecanconsideradoptingahierarchicalvisualizationdesigncoupled
withprinciple,systemgeneralizability,andscalability.Wealsopointed withinteractiontechniquestoenhancevisualscalability.
outcurrentlimitationsandpotentialdirectionsforfuturework.
Human-AIknowledgealignmentthroughprincipleGiventhe Limitations&FutureWorkCurrentLLMsexhibitdeficiencies
emergingpromptingparadigmthatallowsuserstointeractwithLLMs inproducinghallucinatedandinconsistentresponses.Oursystemhas
throughnaturallanguage,thereisagrowinginterestinharnessingex- triedtomitigatethisissuebyfixinghyperparametersandprovidinga
plicitlystatedprinciplesforevaluatingandguidingmodelperformance multi-levelsystematicanalysisofoutputsregardingdifferentprompts,
indownstreamapplications.Whilepreviousstudieshaveexploredthe allowinguserstoeasilyexamineandidentifyoutlyingresponses.Fu-
assessmentofmodelsusinghuman-inputcriteria[25]andthealignment tureeffortscanbedirectedtowardsdevelopingtechniquesforreducing
ofchatbotbehaviorswithuserpreferencesthroughconvertingfeedback hallucinationoccurrenceinmodeloutputs.Moreover,consideringthe
intoprinciples[46],ourresearchpioneerstheuseofdata-derivedprin- potentialinformationlossorinaccuraciesintroducedbyexpertmodels
ciplestodirectandenhancemodelmultimodalreasoningperformance. acrossdifferentmodalities,weplantointegratemoreadvancedexpert
Drawingontheinnatehumancapacityforbothinductiveanddeduc- modelsandvisualizepotentialuncertaintiestoincreaseusertrust.In
tivereasoning,POEMproposedanLLM-assistedmodulecondensing thefuture,weconsiderenablingcomparisonacrossmultipleLLMsto
bothinstance-specificandagnosticprinciplestoencourageusersto furtherinvestigateeffectivepromptengineeringstrategiesfordifferent
efficientlyexpressandexternalizetheirdomain-specificknowledgeand modelsandtasks.Additionally,weplantoextendourworktostudyin-
expertiseformodelsteering.Despitetheexhibitedgreatpotentialfor teractioninvolvingmoremodalitiesinincreasinglycomplexscenarios
elicitingdesiredknowledge, exploringhowtodesign, manage, and andapplications.8 CONCLUSION [15] C.Fu,P.Chen,Y.Shen,Y.Qin,M.Zhang,X.Lin,J.Yang,X.Zheng,
K.Li,X.Sun,etal. Mme:Acomprehensiveevaluationbenchmarkfor
Inthispaper,weintroducePOEM,anovelvisualanalyticstoolde-
multimodallargelanguagemodels. arXiv,2023.doi: 10.48550/arXiv.
signedtofacilitatepromptengineeringforenhancingmultimodalrea-
2306.133943
soning of LLMs with human insight and expertise. The system al- [16] L.Hanu,A.L.Vero˝,andJ.Thewlis.Languageasthemedium:Multimodal
lows users to thoroughly assess prompt effectiveness through well- videoclassificationthroughtextonly.arXiv,2023.doi:10.48550/arXiv.
summarizedmultimodalreasoningpatternsandoffersvariedstrate- 2309.107831
giesforpromptrevision,enablinguserstoapplytheirknowledgefor [17] J.He,X.Wang,K.K.Wong,X.Huang,C.Chen,Z.Chen,F.Wang,
efficient prompt iteration. The system’s efficacy and efficiency are M.Zhu,andH.Qu.Videopro:Avisualanalyticsapproachforinteractive
validatedthroughtwocasestudiesandpositivefeedbackfromexperts. videoprogramming.IEEETransactionsonVisualizationandComputer
Graphics,30(1):87–97,2024.doi:10.1109/TVCG.2023.33265863
[18] F.Hohman,H.Park,C.Robinson,andD.H.PoloChau.Summit:Scaling
REFERENCES
deeplearninginterpretabilitybyvisualizingactivationandattributionsum-
[1] R.Agrawal,R.Srikant,etal.Fastalgorithmsforminingassociationrules. marizations.IEEETransactionsonVisualizationandComputerGraphics,
InProc.VLDB,vol.1215,pp.487–499.Santiago,1994.5 26(1):1096–1106,2020.doi:10.1109/TVCG.2019.29346593
[2] J.-B.Alayrac,J.Donahue,P.Luc,A.Miech,I.Barr,Y.Hasson,K.Lenc, [19] B.Hoover,H.Strobelt,andS.Gehrmann. exBERT:AVisualAnalysis
A.Mensch,K.Millican,M.Reynolds,R.Ring,E.Rutherford,S.Cabi, TooltoExploreLearnedRepresentationsinTransformerModels.InProc.
T.Han,Z.Gong,S.Samangooei,M.Monteiro,J.L.Menick,S.Borgeaud, ACL:SystemDemonstrations,pp.187–196.ACL,Online,2020.doi:10.
A.Brock,A.Nematzadeh,S.Sharifzadeh,M.a.Bin´kowski,R.Barreira, 18653/v1/2020.acl-demos.223
O.Vinyals,A.Zisserman,andK.Simonyan.Flamingo:avisuallanguage [20] M.N.Hoque,W.He,A.K.Shekar,L.Gou,andL.Ren.Visualconcept
modelforfew-shotlearning.InProc.NeurIPS,vol.35,pp.23716–23736, programming:Avisualanalyticsapproachtoinjectinghumanintelligence
2022.1 atscale. IEEETransactionsonVisualizationandComputerGraphics,
[3] A.BagherZadeh,P.P.Liang,S.Poria,E.Cambria,andL.-P.Morency. 29(1):74–83,2023.doi:10.1109/TVCG.2022.32094663
Multimodallanguageanalysisinthewild: CMU-MOSEIdatasetand [21] S.Huang,L.Dong,W.Wang,Y.Hao,S.Singhal,S.Ma,T.Lv,L.Cui,
interpretable dynamic fusion graph. In Proc. ACL (Volume 1: Long O.K.Mohammed,B.Patra,Q.Liu,K.Aggarwal,Z.Chi,N.Bjorck,
Papers),pp.2236–2246.ACL,Melbourne,Australia,2018.doi:10.18653/ V.Chaudhary,S.Som,X.SONG,andF.Wei. Languageisnotallyou
v1/P18-12084 need: Aligning perception with language models. In Proc. NeurIPS,
[4] Y.Bao,K.Yu,Y.Zhang,S.Storks,I.Bar-Yossef,A.delaIglesia,M.Su, vol.36,pp.72096–72109,2023.5
X.Zheng,andJ.Chai. Canfoundationmodelswatch,talkandguide [22] S.Huang,S.Mamidanna,S.Jangam,Y.Zhou,andL.H.Gilpin. Can
youstepbysteptomakeacake? InFindingsofACL:EMNLP,pp. large language models explain themselves? a study of llm-generated
12325–12341.ACL,2023.doi:10.18653/v1/2023.findings-emnlp.8244, self-explanations.arXiv,2023.doi:10.48550/arXiv.2310.112073
5,8 [23] T.Jaunet,C.Kervadec,R.Vuillemot,G.Antipov,M.Baccouche,and
[5] D.Bhattacharjya,J.Lee,D.J.Agravante,B.Ganesan,andR.Marinescu. C.Wolf.Visqa:X-rayingvisionandlanguagereasoningintransformers.
Foundationmodelsherpas:Guidingfoundationmodelsthroughknowl- IEEETransactionsonVisualizationandComputerGraphics,28(1):976–
edgeandreasoning,2024.doi:10.48550/arXiv.2402.016022 986,2022.doi:10.1109/TVCG.2021.31146833
[6] A. Bhattacharyya, Y. K. Singla, B. Krishnamurthy, R. R. Shah, and [24] E.Jiang,K.Olson,E.Toh,A.Molina,A.Donsbach,M.Terry,andC.J.
C.Chen.Avideoisworth4096tokens:Verbalizevideostounderstand Cai.Promptmaker:Prompt-basedprototypingwithlargelanguagemodels.
theminzeroshot. InProc.EMNLP,pp.9822–9839.ACL,Singapore, InProc.CHI:ExtendedAbstracts,articleno.35,8pages.ACM,New
2023.doi:10.18653/v1/2023.emnlp-main.6081,2,5 York,2022.doi:10.1145/3491101.35035642,5
[7] A.Boggust,B.Hoover,A.Satyanarayan,andH.Strobelt.Sharedinterest: [25] T.S.Kim,Y.Lee,J.Shin,Y.-H.Kim,andJ.Kim. Evallm: Interactive
Measuringhuman-aialignmenttoidentifyrecurringpatternsinmodel evaluationoflargelanguagemodelpromptsonuser-definedcriteria.arXiv,
behavior.InProc.CHI,articleno.10,17pages.ACM,NewYork,2022. 2023.doi:10.48550/arXiv.2309.136332,9
doi:10.1145/3491102.35019653 [26] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,T.Xiao,
[8] S.Brade,B.Wang,M.Sousa,S.Oore,andT.Grossman.Promptify:Text- S.Whitehead,A.C.Berg,W.-Y.Lo,P.Dollar,andR.Girshick.Segment
to-imagegenerationthroughinteractivepromptexplorationwithlarge anything. InProc.ICCV,pp.4015–4026.IEEEComputerSociety,Los
languagemodels. InProc.UIST,articleno.96,14pages.ACM,New Alamitos,2023.9
York,2023.doi:10.1145/3586183.36067252 [27] J.Li,D.Li,S.Savarese,andS.Hoi. BLIP-2: Bootstrappinglanguage-
[9] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal, imagepre-trainingwithfrozenimageencodersandlargelanguagemodels.
A.Neelakantan,P.Shyam,G.Sastry,A.Askell,S.Agarwal,A.Herbert- InProc.ICML,vol.202,pp.19730–19742.PMLR,2023.1,5
Voss,G.Krueger,T.Henighan,R.Child,A.Ramesh,D.Ziegler,J.Wu, [28] R.Li,W.Xiao,L.Wang,H.Jang,andG.Carenini.T3-vis:visualanalytic
C.Winter,C.Hesse,M.Chen,E.Sigler,M.Litwin,S.Gray,B.Chess, fortrainingandfine-tuningtransformersinNLP.InProc.EMNLP:System
J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and Demonstrations,pp.220–230.ACL,Singapore,2021.doi:10.18653/v1/
D.Amodei. Languagemodelsarefew-shotlearners. InProc.NeurIPS, 2021.emnlp-demo.263
vol.33,pp.1877–1901,2020.1,2 [29] Z.Lian,L.Sun,M.Xu,H.Sun,K.Xu,Z.Wen,S.Chen,B.Liu,and
[10] A.A.Cabrera,E.Fu,D.Bertucci,K.Holstein,A.Talwalkar,J.I.Hong, J.Tao.Explainablemultimodalemotionreasoning.arXiv,2023.doi:10.
andA.Perer.Zeno:Aninteractiveframeworkforbehavioralevaluation 48550/arXiv.2306.154013
ofmachinelearning.InProc.CHI,articleno.419,14pages.ACM,New [30] P.P.Liang,Y.Cheng,X.Fan,C.K.Ling,S.Nie,R.Chen,Z.Deng,
York,2023.doi:10.1145/3544548.35812683 N. Allen, R. Auerbach, F. Mahmood, R. R. Salakhutdinov, and L.-P.
[11] A.CosciaandA.Endert. Knowledgevis:Interpretinglanguagemodels Morency.Quantifying&modelingmultimodalinteractions:Aninforma-
bycomparingfill-in-the-blankprompts.IEEETransactionsonVisualiza- tiondecompositionframework. InNeurIPS,vol.36,pp.27351–27393,
tionandComputerGraphics,pp.1–13,2023.doi:10.1109/TVCG.2023. 2023.5
33467132,3 [31] P.P.Liang,Y.Cheng,R.Salakhutdinov,andL.-P.Morency.Multimodal
[12] J.F.DeRose,J.Wang,andM.Berger. Attentionflows:Analyzingand fusioninteractions:Astudyofhumanandautomaticquantification. In
comparingattentionmechanismsinlanguagemodels.IEEETransactions Proc.ICMI,11pages,pp.425—-435.ACM,NewYork,2023.doi: 10.
onVisualizationandComputerGraphics,27(2):1160–1170,2021.doi:10 1145/3577190.36141515
.1109/TVCG.2020.30289763 [32] P.P.Liang, C.K.Ling, Y.Cheng, A.Obolenskiy, Y.Liu, R.Pandey,
[13] Q.Dong,L.Li,D.Dai,C.Zheng,Z.Wu,B.Chang,X.Sun,J.Xu,and A.Wilf,L.-P.Morency,andR.Salakhutdinov.Quantifyinginteractions
Z.Sui.Asurveyforin-contextlearning.arXiv,2022.doi:10.48550/arXiv insemi-supervisedmultimodallearning:Guaranteesandapplications.In
.2301.002342 ICLR,2024.5
[14] Y. Feng, X. Wang, K. K. Wong, S. Wang, Y. Lu, M. Zhu, B. Wang, [33] P. P. Liang, Y. Lyu, G. Chhablani, N. Jain, Z. Deng, X. Wang, L.-P.
andW.Chen.Promptmagician:Interactivepromptengineeringfortext- Morency,andR.Salakhutdinov.Multiviz:Towardsvisualizingandunder-
to-imagecreation. IEEETransactionsonVisualizationandComputer standingmultimodalmodels.InICLR,2023.3,5
Graphics,30(1):295–305,2024.doi:10.1109/TVCG.2023.33271682 [34] H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning. InProc.NeurIPS,vol.36,pp.34892–34916,2023.5 [55] X.Wang,R.Huang,Z.Jin,T.Fang,andH.Qu.Commonsensevis:Visual-
[35] P.Liu,W.Yuan,J.Fu,Z.Jiang,H.Hayashi,andG.Neubig. Pre-train, izingandunderstandingcommonsensereasoningcapabilitiesofnatural
prompt,andpredict:Asystematicsurveyofpromptingmethodsinnatural languagemodels. IEEETransactionsonVisualizationandComputer
languageprocessing.ACMComputingSurveys,55(9),articleno.195,35 Graphics,30(01):273–283,2024.doi:10.1109/TVCG.2023.33271533
pages,2023.doi:10.1145/35608152 [56] Y.Wang,S.Shen,andB.Y.Lim.Reprompt:Automaticprompteditingto
[36] S.Liu,Z.Li,T.Li,V.Srikumar,V.Pascucci,andP.-T.Bremer.Nlize:A refineai-generativearttowardspreciseexpressions.InProc.CHI,article
perturbation-drivenvisualinterrogationtoolforanalyzingandinterpreting no.22,p.29.ACM,NewYork,2023.doi:10.1145/3544548.35814022
naturallanguageinferencemodels.IEEETransactionsonVisualization [57] Z.Wang,M.Li,R.Xu,L.Zhou,J.Lei,X.Lin,S.Wang,Z.Yang,C.Zhu,
andComputerGraphics,25(1):651–660,2019.doi:10.1109/TVCG.2018. D.Hoiem,S.-F.Chang,M.Bansal,andH.Ji. Languagemodelswith
28652303 imagedescriptorsarestrongfew-shotvideo-languagelearners. InProc.
[37] V.Liu,H.Qiao,andL.Chilton.Opal:Multimodalimagegenerationfor NeurIPS,vol.35,pp.8483–8497,2022.1,3,5
newsillustration.InProc.UIST,articleno.73,17pages.ACM,NewYork, [58] Z.J.Wang,R.Turko,andD.H.Chau. Dodrio: Exploringtransformer
2022.doi:10.1145/3526113.35456212 modelswithinteractivevisualization.InProc.ACL:SystemDemonstra-
[38] P.Lu,S.Mishra,T.Xia,L.Qiu,K.-W.Chang,S.-C.Zhu,O.Tafjord, tions,pp.132–141.ACL,Online,2021.doi:10.18653/v1/2021.acl-demo.
P.Clark,andA.Kalyan. Learntoexplain: Multimodalreasoningvia 163
thoughtchainsforsciencequestionanswering.InProc.NeurIPS,vol.35, [59] J.Wei,X.Wang,D.Schuurmans,M.Bosma,b.ichter,F.Xia,E.Chi,Q.V.
pp.2507–2521,2022.3 Le,andD.Zhou.Chain-of-thoughtpromptingelicitsreasoninginlarge
[39] P.Lu,B.Peng,H.Cheng,M.Galley,K.-W.Chang,Y.N.Wu,S.-C.Zhu, languagemodels.InProc.NeurIPS,vol.35,pp.24824–24837,2022.2
andJ.Gao.Chameleon:Plug-and-playcompositionalreasoningwithlarge [60] J.Wexler,M.Pushkarna,T.Bolukbasi,M.Wattenberg,F.Viégas,and
languagemodels.InProc.NeurIPS,vol.36,pp.43447–43478,2023.3 J.Wilson. Thewhat-iftool: Interactiveprobingofmachinelearning
[40] S.M.LundbergandS.-I.Lee.Aunifiedapproachtointerpretingmodel models. IEEETransactionsonVisualizationandComputerGraphics,
predictions.InProc.NeurIPS,10pages,p.4768–4777,2017.3,5 26(1):56–65,2020.doi:10.1109/TVCG.2019.29346193
[41] A.Madsen,S.Reddy,andS.Chandar.Post-hocinterpretabilityforneural [61] S.Wu,H.Shen,D.S.Weld,J.Heer,andM.T.Ribeiro. Scattershot:
nlp:Asurvey.ACMComputingSurveys,55(8),articleno.155,42pages, Interactivein-contextexamplecurationfortexttransformation.InProc.
2022.doi:10.1145/35465772 UIST,pp.353—-367.ACM,NewYork,2023.doi: 10.1145/3581641.
[42] L.McInnes,J.Healy,S.Astels,etal.hdbscan:Hierarchicaldensitybased 35840592,4,5
clustering.J.OpenSourceSoftw.,2(11):205,2017.5 [62] T.Wu,E.Jiang,A.Donsbach,J.Gray,A.Molina,M.Terry,andC.J.Cai.
[43] Y.Ming,S.Cao,R.Zhang,Z.Li,Y.Chen,Y.Song,andH.Qu.Understand- Promptchainer:Chaininglargelanguagemodelpromptsthroughvisual
inghiddenmemoriesofrecurrentneuralnetworks.InIEEEConference programming.InProc.CHI:ExtendedAbstracts,articleno.359,10pages.
onVisualAnalyticsScienceandTechnology(VAST),pp.13–24,2017.doi: ACM,NewYork,2022.2
10.1109/VAST.2017.85857213 [63] T.Wu,M.Terry,andC.J.Cai. Aichains:Transparentandcontrollable
[44] A.Mishra,S.Rahman,H.Kim,K.Mitra,andE.Hruschka.Characterizing human-aiinteractionbychaininglargelanguagemodelprompts.InProc.
largelanguagemodelsasrationalizersofknowledge-intensivetasks.arXiv, CHI,p.385.ACM,NewYork,2022.doi:10.1145/3491102.35175822
2023.doi:10.48550/arXiv.2311.050852,3 [64] W.Yang,M.Liu,Z.Wang,andS.Liu.Foundationmodelsmeetvisualiza-
[45] A.Mishra,U.Soni,A.Arunkumar,J.Huang,B.C.Kwon,andC.Bryan. tions:Challengesandopportunities. arXiv,2023.doi:10.48550/arXiv.
Promptaid:Promptexploration,perturbation,testinganditerationusing 2310.057711,3
visualanalyticsforlargelanguagemodels.arXiv,2023.doi:10.48550/ [65] X.Yang,W.Wu,S.Feng,M.Wang,D.Wang,Y.Li,Q.Sun,Y.Zhang,
arXiv.2304.019642,3,4 X.Fu,andS.Poria. Mm-bigbench: Evaluatingmultimodalmodelson
[46] S.Petridis,B.Wedin,J.Wexler,A.Donsbach,M.Pushkarna,N.Goyal, multimodalcontentcomprehensiontasks. arXiv,2023.doi: 10.48550/
C.J.Cai,andM.Terry.Constitutionmaker:Interactivelycritiquinglarge arXiv.2310.090361,3,6,9
languagemodelsbyconvertingfeedbackintoprinciples.arXiv,2023.doi: [66] C.Yeh,Y.Chen,A.Wu,C.Chen,F.Viégas,andM.Wattenberg. At-
10.48550/arXiv.2310.154282,4,6,9 tentionviz:Aglobalviewoftransformerattention.arXiv,2023.doi:10.
[47] M.Ribeiro,S.Singh,andC.Guestrin.“whyshouldItrustyou?”:Explain- 48550/arXiv.2305.032103
ingthepredictionsofanyclassifier.InProc.ACMSIGKDD,p.1135–1144. [67] S.Yin,C.Fu,S.Zhao,K.Li,X.Sun,T.Xu,andE.Chen. Asurveyon
ACM,NewYork,2016.doi:10.1145/2939672.29397783,5 multimodallargelanguagemodels. arXiv,2023.doi: 10.48550/arXiv.
[48] Z.Shao,Z.Yu,M.Wang,andJ.Yu. Promptinglargelanguagemodels 2306.135491
withanswerheuristicsforknowledge-basedvisualquestionanswering.In [68] H.Yu,P.P.Liang,R.Salakhutdinov,andL.-P.Morency.Mixtureofmulti-
Proc.CVPR,pp.14974–14983,2023.3 modalinteractionexperts. InUniReps:theFirstWorkshoponUnifying
[49] H.Strobelt,S.Gehrmann,H.Pfister,andA.M.Rush. Lstmvis:Atool RepresentationsinNeuralModels,2023.5
forvisualanalysisofhiddenstatedynamicsinrecurrentneuralnetworks. [69] J.Yuan,C.Chen,W.Yang,M.Liu,J.Xia,andS.Liu.Asurveyofvisual
IEEETransactionsonVisualizationandComputerGraphics,24(1):667– analyticstechniquesformachinelearning.ComputationalVisualMedia,
676,2018.doi:10.1109/TVCG.2017.27441583 7(1):2,2021.doi:10.1007/s41095-020-0191-73
[50] H.Strobelt,A.Webson,V.Sanh,B.Hoover,J.Beyer,H.Pfister,andA.M. [70] X.Yue,Y.Ni,K.Zhang,T.Zheng,R.Liu,G.Zhang,S.Stevens,D.Jiang,
Rush.Interactiveandvisualpromptengineeringforad-hoctaskadaptation W.Ren,Y.Sun,C.Wei,B.Yu,R.Yuan,R.Sun,M.Yin,B.Zheng,Z.Yang,
withlargelanguagemodels. IEEETransactionsonVisualizationand Y.Liu,W.Huang,H.Sun,Y.Su,andW.Chen.Mmmu:Amassivemulti-
ComputerGraphics,29(1):1146–1156,2023.doi:10.1109/TVCG.2022. disciplinemultimodalunderstandingandreasoningbenchmarkforexpert
32094792,3 agi.arXiv,2023.doi:10.48550/arXiv.2311.165023
[51] J.Sun,C.Zheng,E.Xie,Z.Liu,R.Chu,J.Qiu,J.Xu,M.Ding,H.Li, [71] J.Zamfirescu-Pereira,R.Y.Wong,B.Hartmann,andQ.Yang. Why
M.Geng,etal. Asurveyofreasoningwithfoundationmodels. arXiv, johnnycan’tprompt: Hownon-aiexpertstry(andfail)todesignllm
2023.doi:10.48550/arXiv.2312.115621,2 prompts.InProc.CHI,articleno.437,21pages.ACM,NewYork,2023.
[52] I.Tenney,J.Wexler,J.Bastings,T.Bolukbasi,A.Coenen,S.Gehrmann, doi:10.1145/3544548.35813882
E.Jiang,M.Pushkarna,C.Radebaugh,E.Reif,andA.Yuan.Thelanguage [72] A. Zeng, M. Attarian, brian ichter, K. M. Choromanski, A. Wong,
interpretabilitytool: Extensible,interactivevisualizationsandanalysis S.Welker,F.Tombari,A.Purohit,M.S.Ryoo,V.Sindhwani,J.Lee,
forNLPmodels.InProc.EMNLP:SystemDemonstrations,pp.107–118. V.Vanhoucke,andP.Florence. Socraticmodels:Composingzero-shot
ACL,Online,2020.doi:10.18653/v1/2020.emnlp-demos.153 multimodalreasoningwithlanguage.InICLR,2023.1,3,5
[53] H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix, [73] T.Zhang,A.Madaan,L.Gao,S.Zheng,S.Mishra,Y.Yang,N.Tandon,
B.Rozière,N.Goyal,E.Hambro,F.Azhar,etal. Llama: Openand andU.Alon. In-contextprinciplelearningfrommistakes. arXiv,2024.
efficientfoundationlanguagemodels.arXiv,2023.doi:10.48550/arXiv. doi:10.48550/arXiv.2402.054033,6,9
2302.139712 [74] X.Zhang,J.P.Ono,H.Song,L.Gou,K.-L.Ma,andL.Ren.Sliceteller:
[54] X.Wang,J.He,Z.Jin,M.Yang,Y.Wang,andH.Qu. M2lens: Visu- Adataslice-drivenapproachformachinelearningmodelvalidation.IEEE
alizingandexplainingmultimodalmodelsforsentimentanalysis.IEEE TransactionsonVisualizationandComputerGraphics,29(1):842–852,
TransactionsonVisualizationandComputerGraphics,28(1):802–812, 2023.doi:10.1109/TVCG.2022.32094653
2022.doi:10.1109/TVCG.2021.31147943,5 [75] Z.Zhang,S.Wang,W.Yu,Y.Xu,D.Iter,Q.Zeng,Y.Liu,C.Zhu,andM.Jiang.Auto-instruct:Automaticinstructiongenerationandrankingfor [15] C.Fu,P.Chen,Y.Shen,Y.Qin,M.Zhang,X.Lin,J.Yang,X.Zheng,
black-boxlanguagemodels.InFindingsofACL:EMNLP,pp.9850–9867. K.Li,X.Sun,etal. Mme:Acomprehensiveevaluationbenchmarkfor
ACL,2023.doi:10.18653/v1/2023.findings-emnlp.6595 multimodallargelanguagemodels. arXiv,2023.doi: 10.48550/arXiv.
[76] H.Zhao,H.Chen,F.Yang,N.Liu,H.Deng,H.Cai,S.Wang,D.Yin, 2306.133943
andM.Du. Explainabilityforlargelanguagemodels: Asurvey. ACM [16] L.Hanu,A.L.Vero˝,andJ.Thewlis.Languageasthemedium:Multimodal
TransactionsonIntelligentSystemsandTechnology,15(2):1–38,2024. videoclassificationthroughtextonly.arXiv,2023.doi:10.48550/arXiv.
doi:10.1145/36393722,5 2309.107831
[77] G.Zheng,B.Yang,J.Tang,H.-Y.Zhou,andS.Yang.Ddcot:Duty-distinct [17] J.He,X.Wang,K.K.Wong,X.Huang,C.Chen,Z.Chen,F.Wang,
chain-of-thoughtpromptingformultimodalreasoninginlanguagemodels. M.Zhu,andH.Qu.Videopro:Avisualanalyticsapproachforinteractive
InProc.NeurIPS,vol.36,pp.5168–5191,2023.3,5 videoprogramming.IEEETransactionsonVisualizationandComputer
Graphics,30(1):87–97,2024.doi:10.1109/TVCG.2023.33265863
[18] F.Hohman,H.Park,C.Robinson,andD.H.PoloChau.Summit:Scaling
REFERENCES
deeplearninginterpretabilitybyvisualizingactivationandattributionsum-
[1] R.Agrawal,R.Srikant,etal.Fastalgorithmsforminingassociationrules. marizations.IEEETransactionsonVisualizationandComputerGraphics,
InProc.VLDB,vol.1215,pp.487–499.Santiago,1994.5 26(1):1096–1106,2020.doi:10.1109/TVCG.2019.29346593
[2] J.-B.Alayrac,J.Donahue,P.Luc,A.Miech,I.Barr,Y.Hasson,K.Lenc, [19] B.Hoover,H.Strobelt,andS.Gehrmann. exBERT:AVisualAnalysis
A.Mensch,K.Millican,M.Reynolds,R.Ring,E.Rutherford,S.Cabi, TooltoExploreLearnedRepresentationsinTransformerModels.InProc.
T.Han,Z.Gong,S.Samangooei,M.Monteiro,J.L.Menick,S.Borgeaud, ACL:SystemDemonstrations,pp.187–196.ACL,Online,2020.doi:10.
A.Brock,A.Nematzadeh,S.Sharifzadeh,M.a.Bin´kowski,R.Barreira, 18653/v1/2020.acl-demos.223
O.Vinyals,A.Zisserman,andK.Simonyan.Flamingo:avisuallanguage [20] M.N.Hoque,W.He,A.K.Shekar,L.Gou,andL.Ren.Visualconcept
modelforfew-shotlearning.InProc.NeurIPS,vol.35,pp.23716–23736, programming:Avisualanalyticsapproachtoinjectinghumanintelligence
2022.1 atscale. IEEETransactionsonVisualizationandComputerGraphics,
[3] A.BagherZadeh,P.P.Liang,S.Poria,E.Cambria,andL.-P.Morency. 29(1):74–83,2023.doi:10.1109/TVCG.2022.32094663
Multimodallanguageanalysisinthewild: CMU-MOSEIdatasetand [21] S.Huang,L.Dong,W.Wang,Y.Hao,S.Singhal,S.Ma,T.Lv,L.Cui,
interpretable dynamic fusion graph. In Proc. ACL (Volume 1: Long O.K.Mohammed,B.Patra,Q.Liu,K.Aggarwal,Z.Chi,N.Bjorck,
Papers),pp.2236–2246.ACL,Melbourne,Australia,2018.doi:10.18653/ V.Chaudhary,S.Som,X.SONG,andF.Wei. Languageisnotallyou
v1/P18-12084 need: Aligning perception with language models. In Proc. NeurIPS,
[4] Y.Bao,K.Yu,Y.Zhang,S.Storks,I.Bar-Yossef,A.delaIglesia,M.Su, vol.36,pp.72096–72109,2023.5
X.Zheng,andJ.Chai. Canfoundationmodelswatch,talkandguide [22] S.Huang,S.Mamidanna,S.Jangam,Y.Zhou,andL.H.Gilpin. Can
youstepbysteptomakeacake? InFindingsofACL:EMNLP,pp. large language models explain themselves? a study of llm-generated
12325–12341.ACL,2023.doi:10.18653/v1/2023.findings-emnlp.8244, self-explanations.arXiv,2023.doi:10.48550/arXiv.2310.112073
5,8 [23] T.Jaunet,C.Kervadec,R.Vuillemot,G.Antipov,M.Baccouche,and
[5] D.Bhattacharjya,J.Lee,D.J.Agravante,B.Ganesan,andR.Marinescu. C.Wolf.Visqa:X-rayingvisionandlanguagereasoningintransformers.
Foundationmodelsherpas:Guidingfoundationmodelsthroughknowl- IEEETransactionsonVisualizationandComputerGraphics,28(1):976–
edgeandreasoning,2024.doi:10.48550/arXiv.2402.016022 986,2022.doi:10.1109/TVCG.2021.31146833
[6] A. Bhattacharyya, Y. K. Singla, B. Krishnamurthy, R. R. Shah, and [24] E.Jiang,K.Olson,E.Toh,A.Molina,A.Donsbach,M.Terry,andC.J.
C.Chen.Avideoisworth4096tokens:Verbalizevideostounderstand Cai.Promptmaker:Prompt-basedprototypingwithlargelanguagemodels.
theminzeroshot. InProc.EMNLP,pp.9822–9839.ACL,Singapore, InProc.CHI:ExtendedAbstracts,articleno.35,8pages.ACM,New
2023.doi:10.18653/v1/2023.emnlp-main.6081,2,5 York,2022.doi:10.1145/3491101.35035642,5
[7] A.Boggust,B.Hoover,A.Satyanarayan,andH.Strobelt.Sharedinterest: [25] T.S.Kim,Y.Lee,J.Shin,Y.-H.Kim,andJ.Kim. Evallm: Interactive
Measuringhuman-aialignmenttoidentifyrecurringpatternsinmodel evaluationoflargelanguagemodelpromptsonuser-definedcriteria.arXiv,
behavior.InProc.CHI,articleno.10,17pages.ACM,NewYork,2022. 2023.doi:10.48550/arXiv.2309.136332,9
doi:10.1145/3491102.35019653 [26] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,T.Xiao,
[8] S.Brade,B.Wang,M.Sousa,S.Oore,andT.Grossman.Promptify:Text- S.Whitehead,A.C.Berg,W.-Y.Lo,P.Dollar,andR.Girshick.Segment
to-imagegenerationthroughinteractivepromptexplorationwithlarge anything. InProc.ICCV,pp.4015–4026.IEEEComputerSociety,Los
languagemodels. InProc.UIST,articleno.96,14pages.ACM,New Alamitos,2023.9
York,2023.doi:10.1145/3586183.36067252 [27] J.Li,D.Li,S.Savarese,andS.Hoi. BLIP-2: Bootstrappinglanguage-
[9] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal, imagepre-trainingwithfrozenimageencodersandlargelanguagemodels.
A.Neelakantan,P.Shyam,G.Sastry,A.Askell,S.Agarwal,A.Herbert- InProc.ICML,vol.202,pp.19730–19742.PMLR,2023.1,5
Voss,G.Krueger,T.Henighan,R.Child,A.Ramesh,D.Ziegler,J.Wu, [28] R.Li,W.Xiao,L.Wang,H.Jang,andG.Carenini.T3-vis:visualanalytic
C.Winter,C.Hesse,M.Chen,E.Sigler,M.Litwin,S.Gray,B.Chess, fortrainingandfine-tuningtransformersinNLP.InProc.EMNLP:System
J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and Demonstrations,pp.220–230.ACL,Singapore,2021.doi:10.18653/v1/
D.Amodei. Languagemodelsarefew-shotlearners. InProc.NeurIPS, 2021.emnlp-demo.263
vol.33,pp.1877–1901,2020.1,2 [29] Z.Lian,L.Sun,M.Xu,H.Sun,K.Xu,Z.Wen,S.Chen,B.Liu,and
[10] A.A.Cabrera,E.Fu,D.Bertucci,K.Holstein,A.Talwalkar,J.I.Hong, J.Tao.Explainablemultimodalemotionreasoning.arXiv,2023.doi:10.
andA.Perer.Zeno:Aninteractiveframeworkforbehavioralevaluation 48550/arXiv.2306.154013
ofmachinelearning.InProc.CHI,articleno.419,14pages.ACM,New [30] P.P.Liang,Y.Cheng,X.Fan,C.K.Ling,S.Nie,R.Chen,Z.Deng,
York,2023.doi:10.1145/3544548.35812683 N. Allen, R. Auerbach, F. Mahmood, R. R. Salakhutdinov, and L.-P.
[11] A.CosciaandA.Endert. Knowledgevis:Interpretinglanguagemodels Morency.Quantifying&modelingmultimodalinteractions:Aninforma-
bycomparingfill-in-the-blankprompts.IEEETransactionsonVisualiza- tiondecompositionframework. InNeurIPS,vol.36,pp.27351–27393,
tionandComputerGraphics,pp.1–13,2023.doi:10.1109/TVCG.2023. 2023.5
33467132,3 [31] P.P.Liang,Y.Cheng,R.Salakhutdinov,andL.-P.Morency.Multimodal
[12] J.F.DeRose,J.Wang,andM.Berger. Attentionflows:Analyzingand fusioninteractions:Astudyofhumanandautomaticquantification. In
comparingattentionmechanismsinlanguagemodels.IEEETransactions Proc.ICMI,11pages,pp.425—-435.ACM,NewYork,2023.doi: 10.
onVisualizationandComputerGraphics,27(2):1160–1170,2021.doi:10 1145/3577190.36141515
.1109/TVCG.2020.30289763 [32] P.P.Liang, C.K.Ling, Y.Cheng, A.Obolenskiy, Y.Liu, R.Pandey,
[13] Q.Dong,L.Li,D.Dai,C.Zheng,Z.Wu,B.Chang,X.Sun,J.Xu,and A.Wilf,L.-P.Morency,andR.Salakhutdinov.Quantifyinginteractions
Z.Sui.Asurveyforin-contextlearning.arXiv,2022.doi:10.48550/arXiv insemi-supervisedmultimodallearning:Guaranteesandapplications.In
.2301.002342 ICLR,2024.5
[14] Y. Feng, X. Wang, K. K. Wong, S. Wang, Y. Lu, M. Zhu, B. Wang, [33] P. P. Liang, Y. Lyu, G. Chhablani, N. Jain, Z. Deng, X. Wang, L.-P.
andW.Chen.Promptmagician:Interactivepromptengineeringfortext- Morency,andR.Salakhutdinov.Multiviz:Towardsvisualizingandunder-
to-imagecreation. IEEETransactionsonVisualizationandComputer standingmultimodalmodels.InICLR,2023.3,5
Graphics,30(1):295–305,2024.doi:10.1109/TVCG.2023.33271682 [34] H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning. InProc.NeurIPS,vol.36,pp.34892–34916,2023.5 [55] X.Wang,R.Huang,Z.Jin,T.Fang,andH.Qu.Commonsensevis:Visual-
[35] P.Liu,W.Yuan,J.Fu,Z.Jiang,H.Hayashi,andG.Neubig. Pre-train, izingandunderstandingcommonsensereasoningcapabilitiesofnatural
prompt,andpredict:Asystematicsurveyofpromptingmethodsinnatural languagemodels. IEEETransactionsonVisualizationandComputer
languageprocessing.ACMComputingSurveys,55(9),articleno.195,35 Graphics,30(01):273–283,2024.doi:10.1109/TVCG.2023.33271533
pages,2023.doi:10.1145/35608152 [56] Y.Wang,S.Shen,andB.Y.Lim.Reprompt:Automaticprompteditingto
[36] S.Liu,Z.Li,T.Li,V.Srikumar,V.Pascucci,andP.-T.Bremer.Nlize:A refineai-generativearttowardspreciseexpressions.InProc.CHI,article
perturbation-drivenvisualinterrogationtoolforanalyzingandinterpreting no.22,p.29.ACM,NewYork,2023.doi:10.1145/3544548.35814022
naturallanguageinferencemodels.IEEETransactionsonVisualization [57] Z.Wang,M.Li,R.Xu,L.Zhou,J.Lei,X.Lin,S.Wang,Z.Yang,C.Zhu,
andComputerGraphics,25(1):651–660,2019.doi:10.1109/TVCG.2018. D.Hoiem,S.-F.Chang,M.Bansal,andH.Ji. Languagemodelswith
28652303 imagedescriptorsarestrongfew-shotvideo-languagelearners. InProc.
[37] V.Liu,H.Qiao,andL.Chilton.Opal:Multimodalimagegenerationfor NeurIPS,vol.35,pp.8483–8497,2022.1,3,5
newsillustration.InProc.UIST,articleno.73,17pages.ACM,NewYork, [58] Z.J.Wang,R.Turko,andD.H.Chau. Dodrio: Exploringtransformer
2022.doi:10.1145/3526113.35456212 modelswithinteractivevisualization.InProc.ACL:SystemDemonstra-
[38] P.Lu,S.Mishra,T.Xia,L.Qiu,K.-W.Chang,S.-C.Zhu,O.Tafjord, tions,pp.132–141.ACL,Online,2021.doi:10.18653/v1/2021.acl-demo.
P.Clark,andA.Kalyan. Learntoexplain: Multimodalreasoningvia 163
thoughtchainsforsciencequestionanswering.InProc.NeurIPS,vol.35, [59] J.Wei,X.Wang,D.Schuurmans,M.Bosma,b.ichter,F.Xia,E.Chi,Q.V.
pp.2507–2521,2022.3 Le,andD.Zhou.Chain-of-thoughtpromptingelicitsreasoninginlarge
[39] P.Lu,B.Peng,H.Cheng,M.Galley,K.-W.Chang,Y.N.Wu,S.-C.Zhu, languagemodels.InProc.NeurIPS,vol.35,pp.24824–24837,2022.2
andJ.Gao.Chameleon:Plug-and-playcompositionalreasoningwithlarge [60] J.Wexler,M.Pushkarna,T.Bolukbasi,M.Wattenberg,F.Viégas,and
languagemodels.InProc.NeurIPS,vol.36,pp.43447–43478,2023.3 J.Wilson. Thewhat-iftool: Interactiveprobingofmachinelearning
[40] S.M.LundbergandS.-I.Lee.Aunifiedapproachtointerpretingmodel models. IEEETransactionsonVisualizationandComputerGraphics,
predictions.InProc.NeurIPS,10pages,p.4768–4777,2017.3,5 26(1):56–65,2020.doi:10.1109/TVCG.2019.29346193
[41] A.Madsen,S.Reddy,andS.Chandar.Post-hocinterpretabilityforneural [61] S.Wu,H.Shen,D.S.Weld,J.Heer,andM.T.Ribeiro. Scattershot:
nlp:Asurvey.ACMComputingSurveys,55(8),articleno.155,42pages, Interactivein-contextexamplecurationfortexttransformation.InProc.
2022.doi:10.1145/35465772 UIST,pp.353—-367.ACM,NewYork,2023.doi: 10.1145/3581641.
[42] L.McInnes,J.Healy,S.Astels,etal.hdbscan:Hierarchicaldensitybased 35840592,4,5
clustering.J.OpenSourceSoftw.,2(11):205,2017.5 [62] T.Wu,E.Jiang,A.Donsbach,J.Gray,A.Molina,M.Terry,andC.J.Cai.
[43] Y.Ming,S.Cao,R.Zhang,Z.Li,Y.Chen,Y.Song,andH.Qu.Understand- Promptchainer:Chaininglargelanguagemodelpromptsthroughvisual
inghiddenmemoriesofrecurrentneuralnetworks.InIEEEConference programming.InProc.CHI:ExtendedAbstracts,articleno.359,10pages.
onVisualAnalyticsScienceandTechnology(VAST),pp.13–24,2017.doi: ACM,NewYork,2022.2
10.1109/VAST.2017.85857213 [63] T.Wu,M.Terry,andC.J.Cai. Aichains:Transparentandcontrollable
[44] A.Mishra,S.Rahman,H.Kim,K.Mitra,andE.Hruschka.Characterizing human-aiinteractionbychaininglargelanguagemodelprompts.InProc.
largelanguagemodelsasrationalizersofknowledge-intensivetasks.arXiv, CHI,p.385.ACM,NewYork,2022.doi:10.1145/3491102.35175822
2023.doi:10.48550/arXiv.2311.050852,3 [64] W.Yang,M.Liu,Z.Wang,andS.Liu.Foundationmodelsmeetvisualiza-
[45] A.Mishra,U.Soni,A.Arunkumar,J.Huang,B.C.Kwon,andC.Bryan. tions:Challengesandopportunities. arXiv,2023.doi:10.48550/arXiv.
Promptaid:Promptexploration,perturbation,testinganditerationusing 2310.057711,3
visualanalyticsforlargelanguagemodels.arXiv,2023.doi:10.48550/ [65] X.Yang,W.Wu,S.Feng,M.Wang,D.Wang,Y.Li,Q.Sun,Y.Zhang,
arXiv.2304.019642,3,4 X.Fu,andS.Poria. Mm-bigbench: Evaluatingmultimodalmodelson
[46] S.Petridis,B.Wedin,J.Wexler,A.Donsbach,M.Pushkarna,N.Goyal, multimodalcontentcomprehensiontasks. arXiv,2023.doi: 10.48550/
C.J.Cai,andM.Terry.Constitutionmaker:Interactivelycritiquinglarge arXiv.2310.090361,3,6,9
languagemodelsbyconvertingfeedbackintoprinciples.arXiv,2023.doi: [66] C.Yeh,Y.Chen,A.Wu,C.Chen,F.Viégas,andM.Wattenberg. At-
10.48550/arXiv.2310.154282,4,6,9 tentionviz:Aglobalviewoftransformerattention.arXiv,2023.doi:10.
[47] M.Ribeiro,S.Singh,andC.Guestrin.“whyshouldItrustyou?”:Explain- 48550/arXiv.2305.032103
ingthepredictionsofanyclassifier.InProc.ACMSIGKDD,p.1135–1144. [67] S.Yin,C.Fu,S.Zhao,K.Li,X.Sun,T.Xu,andE.Chen. Asurveyon
ACM,NewYork,2016.doi:10.1145/2939672.29397783,5 multimodallargelanguagemodels. arXiv,2023.doi: 10.48550/arXiv.
[48] Z.Shao,Z.Yu,M.Wang,andJ.Yu. Promptinglargelanguagemodels 2306.135491
withanswerheuristicsforknowledge-basedvisualquestionanswering.In [68] H.Yu,P.P.Liang,R.Salakhutdinov,andL.-P.Morency.Mixtureofmulti-
Proc.CVPR,pp.14974–14983,2023.3 modalinteractionexperts. InUniReps:theFirstWorkshoponUnifying
[49] H.Strobelt,S.Gehrmann,H.Pfister,andA.M.Rush. Lstmvis:Atool RepresentationsinNeuralModels,2023.5
forvisualanalysisofhiddenstatedynamicsinrecurrentneuralnetworks. [69] J.Yuan,C.Chen,W.Yang,M.Liu,J.Xia,andS.Liu.Asurveyofvisual
IEEETransactionsonVisualizationandComputerGraphics,24(1):667– analyticstechniquesformachinelearning.ComputationalVisualMedia,
676,2018.doi:10.1109/TVCG.2017.27441583 7(1):2,2021.doi:10.1007/s41095-020-0191-73
[50] H.Strobelt,A.Webson,V.Sanh,B.Hoover,J.Beyer,H.Pfister,andA.M. [70] X.Yue,Y.Ni,K.Zhang,T.Zheng,R.Liu,G.Zhang,S.Stevens,D.Jiang,
Rush.Interactiveandvisualpromptengineeringforad-hoctaskadaptation W.Ren,Y.Sun,C.Wei,B.Yu,R.Yuan,R.Sun,M.Yin,B.Zheng,Z.Yang,
withlargelanguagemodels. IEEETransactionsonVisualizationand Y.Liu,W.Huang,H.Sun,Y.Su,andW.Chen.Mmmu:Amassivemulti-
ComputerGraphics,29(1):1146–1156,2023.doi:10.1109/TVCG.2022. disciplinemultimodalunderstandingandreasoningbenchmarkforexpert
32094792,3 agi.arXiv,2023.doi:10.48550/arXiv.2311.165023
[51] J.Sun,C.Zheng,E.Xie,Z.Liu,R.Chu,J.Qiu,J.Xu,M.Ding,H.Li, [71] J.Zamfirescu-Pereira,R.Y.Wong,B.Hartmann,andQ.Yang. Why
M.Geng,etal. Asurveyofreasoningwithfoundationmodels. arXiv, johnnycan’tprompt: Hownon-aiexpertstry(andfail)todesignllm
2023.doi:10.48550/arXiv.2312.115621,2 prompts.InProc.CHI,articleno.437,21pages.ACM,NewYork,2023.
[52] I.Tenney,J.Wexler,J.Bastings,T.Bolukbasi,A.Coenen,S.Gehrmann, doi:10.1145/3544548.35813882
E.Jiang,M.Pushkarna,C.Radebaugh,E.Reif,andA.Yuan.Thelanguage [72] A. Zeng, M. Attarian, brian ichter, K. M. Choromanski, A. Wong,
interpretabilitytool: Extensible,interactivevisualizationsandanalysis S.Welker,F.Tombari,A.Purohit,M.S.Ryoo,V.Sindhwani,J.Lee,
forNLPmodels.InProc.EMNLP:SystemDemonstrations,pp.107–118. V.Vanhoucke,andP.Florence. Socraticmodels:Composingzero-shot
ACL,Online,2020.doi:10.18653/v1/2020.emnlp-demos.153 multimodalreasoningwithlanguage.InICLR,2023.1,3,5
[53] H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix, [73] T.Zhang,A.Madaan,L.Gao,S.Zheng,S.Mishra,Y.Yang,N.Tandon,
B.Rozière,N.Goyal,E.Hambro,F.Azhar,etal. Llama: Openand andU.Alon. In-contextprinciplelearningfrommistakes. arXiv,2024.
efficientfoundationlanguagemodels.arXiv,2023.doi:10.48550/arXiv. doi:10.48550/arXiv.2402.054033,6,9
2302.139712 [74] X.Zhang,J.P.Ono,H.Song,L.Gou,K.-L.Ma,andL.Ren.Sliceteller:
[54] X.Wang,J.He,Z.Jin,M.Yang,Y.Wang,andH.Qu. M2lens: Visu- Adataslice-drivenapproachformachinelearningmodelvalidation.IEEE
alizingandexplainingmultimodalmodelsforsentimentanalysis.IEEE TransactionsonVisualizationandComputerGraphics,29(1):842–852,
TransactionsonVisualizationandComputerGraphics,28(1):802–812, 2023.doi:10.1109/TVCG.2022.32094653
2022.doi:10.1109/TVCG.2021.31147943,5 [75] Z.Zhang,S.Wang,W.Yu,Y.Xu,D.Iter,Q.Zeng,Y.Liu,C.Zhu,andM.Jiang.Auto-instruct:Automaticinstructiongenerationandrankingfor
black-boxlanguagemodels.InFindingsofACL:EMNLP,pp.9850–9867.
ACL,2023.doi:10.18653/v1/2023.findings-emnlp.6595
[76] H.Zhao,H.Chen,F.Yang,N.Liu,H.Deng,H.Cai,S.Wang,D.Yin,
andM.Du. Explainabilityforlargelanguagemodels: Asurvey. ACM
TransactionsonIntelligentSystemsandTechnology,15(2):1–38,2024.
doi:10.1145/36393722,5
[77] G.Zheng,B.Yang,J.Tang,H.-Y.Zhou,andS.Yang.Ddcot:Duty-distinct
chain-of-thoughtpromptingformultimodalreasoninginlanguagemodels.
InProc.NeurIPS,vol.36,pp.5168–5191,2023.3,5