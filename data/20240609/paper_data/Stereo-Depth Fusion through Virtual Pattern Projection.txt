1
Stereo-Depth Fusion through
Virtual Pattern Projection
Luca Bartolomei, Student Member, IEEE, Matteo Poggi, Member, IEEE, Fabio Tosi, Member, IEEE,
Andrea Conti, Student Member, IEEE, and Stefano Mattoccia, Senior Member, IEEE
Abstract—Thispaperpresentsanovelgeneral-purposestereoanddepthdatafusionparadigmthatmimicstheactivestereoprinciple
byreplacingtheunreliablephysicalpatternprojectorwithadepthsensor.Itworksbyprojectingvirtualpatternsconsistentwiththe
scenegeometryontotheleftandrightimagesacquiredbyaconventionalstereocamera,usingthesparsehintsobtainedfromadepth
sensor,tofacilitatethevisualcorrespondence.Purposely,anydepthsensingdevicecanbeseamlesslypluggedintoourframework,
enablingthedeploymentofavirtualactivestereosetupinanypossibleenvironmentandovercomingtheseverelimitationsofphysical
patternprojection,suchasthelimitedworkingrangeandenvironmentalconditions.Exhaustiveexperimentsonindoorandoutdoor
datasetsfeaturingbothlongandcloserange,includingthoseprovidingraw,unfiltereddepthhintsfromoff-the-shelfdepthsensors,
highlighttheeffectivenessofourapproachinnotablyboostingtherobustnessandaccuracyofalgorithmsanddeepstereowithoutany
codemodificationandevenwithoutre-training.Additionally,weassesstheperformanceofourstrategyonactivestereoevaluation
datasetswithconventionalpatternprojection.Indeed,inallthesescenarios,ourvirtualpatternprojectionparadigmachieves
state-of-the-artperformance.Thesourcecodeisavailableat:https://github.com/bartn8/vppstereo.
IndexTerms—Depthperception,Sensorfusion,Stereovision,Depthsensor,Activestereovision
✦
1 INTRODUCTION inthetargetdomainwithamuchsmalleramountofrealdata,with
thepreviouslyoutlinedchallengetocopewiththesynthetic-to-real
D EPTH perceptionispivotalinseveraldownstreamcomputer
domainissue.
visiontasks,suchasautonomousdriving,robotics,andaug-
An alternative strategy to infer depth relies on active sensing
mentedreality.Forthisreason,thistopichasreceivedlongstand-
technologies, such as LiDAR (Light Detection and Ranging),
ingattentionintheliteratureandindustrythroughthedeployment
ToF (Time of Flight), Radar (Radio Detection and Ranging), and
of different technologies. Nonetheless, achieving this goal with
structured or unstructured light devices. However, each of these
camerashasmanyadvantagesduetothelowcostandpotentially
technologies has specific limitations. LiDAR technology [5] is
unbounded image resolution. Consequently, different setups exist
quite reliable but features a much lower density than cameras,
toinferdepthuniquelyfromstandardimagingdevices.Atthecore
and increasing density is extremely expensive and challenging
of them, either using multiple cameras or a moving one, there
due to technological issues. ToF [5] has similar limitations and
is the problem of determining the visual correspondence across
isalsoineffectiveinsunlightandatlongerdistances.Radar[6]is
images. However, this task turns intrinsically ambiguous, espe-
apopularactivetechnologymassivelydeployedintheautomotive
cially when dealing with textureless regions, repetitive patterns,
field to infer depth by measuring the time it takes to send and
and non-Lambertian materials. Moreover, the difficulty increases
receive electromagnetic waves. Compared to LiDAR, it allows
significantly when the task is performed for each image pixel.
for similar and even more extensive range sensing. However, it
Despite these facts, as reported in the literature, learning-based
providessparserandnoisierdepthdataandhasasmallervertical
techniques achieve compelling performance [1], [2]. However,
fieldofview.Finally,wementionactivesystemsestimatingdepth
a severe downside is that these methods are prone to domain
from images; they rely on a structured [7] or unstructured [8]
shift issues that are not present in traditional yet less accurate
patternprojectiontoincreasetherobustnessofpurepassive-based
hand-crafted methods. Specifically, their dependency on training
sensing systems. Similarly to the latter approaches, they enable
data yields significant performance deterioration when dealing
higher resolution than active sensors. However, their strength is
with different data distributions, such as deploying a network
also their main weakness, since they require a projector to ease
trained with outdoor data in indoor environments or vice versa.
visualcorrespondencewithappropriatepatternsprojectedintothe
Hence, fine-tuning the network in the target domain is crucial to
sensedscene.Consequently,theystrugglewhentheartificiallyin-
adapt it to the new environment. Unfortunately, collecting new
jectedvisualcuesarenotvisible,suchaswhenthetargetdistance
data in the target domain requires a significant effort. Moreover,
falls just a few meters away or under sunlight. Moreover, since
since collecting training data is expensive, unpractical and time-
patternprojectiontypicallyoccursintheIRspectrum,theyrequire
consuming, the massive amount of labeled training data needed
at least one additional RGB camera. Finally, multiple projectors
by deep networks is typically obtained by exploiting computer
might interfere with each other, especially using structured light
graphic frameworks to gather synthetic data and then fine-tuning
projectionsystems.
The complementary strengths and weaknesses of active and
• L. Bartolomei, M. Poggi, F. Tosi, A. Conti and S. Mattoccia are with passive technologies mentioned above led to their joint deploy-
UniversityofBologna,Italy,40136.
ment in many fields to improve depth perception. For instance,
4202
nuJ
6
]VC.sc[
1v54340.6042:viXra2
Error Rate > 3: 47.26% Error Rate > 3: 1.93% Error Rate > 3: 1.44%
Error Rate > 2: 75.16% Error Rate > 2: 7.91% Error Rate > 3: 6.36%
(a) (b) (c) (d) (e)
Fig.1:VirtualPatternProjectionfordeepstereo.Eitherinchallengingoutdoor(top)orindoor(bottom)environments(a),stereonetworks
likeRAFT-Stereo[3](top)orPSMNet[4](bottom)struggle(b).Byprojectingavirtualpatternonimages(c),thesamenetworksdramatically
improvetheiraccuracywithoutretraining(d).Trainingthemodelstodealwiththeaugmentedimages(e)furtherreduceserrors.
almostallautonomousdrivingprototypesleverageheterogeneous • Actingbeforeanyprocessingoccurs,itcanbeseamlessly
sensor suites, including cameras and LiDARs. Similarly, most deployedwithanystereoalgorithmordeepnetworkwith-
smartphones/tabletsandARdevices,suchasAppleVisionProand out modifications and benefit from future progress in the
Meta Quest Pro, have cameras coupled with ToF depth sensors. field.
For these reasons, researchers have proposed strategies to exploit
Moreover, in contrast to active stereo systems, using a depth
the synergy between active and passive depth sensing [9], [10],
sensorinplaceofapatternprojector:
[11].Thesharedmaintraitofthesesensor-fusionmethodsconsists
of modifying their internal behaviour to account for the sparse • As depicted in Fig. 2 (a), it can work under sunlight,
depthseedsandtoguidethemtoabettersolution. indoorsandoutdoors,atlongandcloseranges,andwith-
In this paper, we follow a completely different path, initially out any additional processing cost for the selected stereo
proposedin[12],toexploitthesynergybetweenpassiveandactive matcher;
sensingbyactingdirectlyonthevanillainputimagesacquiredby • Itismoreeffectiveeveninthespecificapplicationdomain
thecamerasbeforeanyprocessingoccurs.Specifically,toeasethe ofprojector-basedsystemsandpotentiallylessexpensive;
visualcorrespondenceproblemexploitingtheavailabilityofsparse • It does not require additional hardware (e.g., additional
depthseeds,wecoherentlyhallucinatethestereopairacquiredby RGBorIRcameras),asdepthestimationisperformedin
aconventionalcameratofacilitatethevisualcorrespondencetask thesametargetvisualspectrum;
performedbyanystereonetwork/algorithm,asifavirtualpattern • Thevirtualprojectionparadigmcanbetailoredonthefly
projectorwerepresentinthescene.Fig.1showcasestheimprove- to adapt to the image content and is agnostic to dynamic
mentsyieldedbyourproposal.Asforsensorfusionmethodsinthe objectsandego-motion.
literature [9], [10], the only requirements are a calibrated stereo
For the reasons mentioned above, we believe that our Virtual
rigandthesparseyetaccuratedepthmeasurementsregisteredwith
Pattern Projection (VPP) paradigm has the potential to become
thereferencecamera.However,incontrasttoothersensorfusion
a standard component for depth perception and pave the way to
approaches,themotivationbehindourproposalsharessimilarities
fosterexcitingprogressinthefield.
with active methods based on unstructured pattern projections
[13], [8], though implemented through a completely different
2 RELATED WORK
setup and paradigm. Neglecting a physical pattern projector in
favour of a depth sensor eliminates all the severe limitations This section summarizes the literature relevant to our work con-
mentioned above by simply selecting the device best suited for cerningpassiveandactivestereoandmethodstoinferdepthjointly
thespecifictargetapplicationdomain. usingcamerasandsparsedepthseeds.
An exhaustive experimental evaluation with various stereo Stereo Matching. Conventional stereo algorithms, deeply in-
datasets, including those featuring the availability of raw data vestigatedinthepastdecadesandthoroughlysurveyedin[14],in-
as gathered by off-the shelf depth sensors without any filtering, ferdensedisparitymapsfromstereopairsmainlyusingpriorsand
supportsthefollowingclaims: handcrafted features [15], [16], [17], [18], [19], [20], [21], [22],
[23]. Nonetheless, learning-based approaches have profoundly
• Even with a meager number of sparse depth seeds (e.g., modified the traditional paradigm, enabling them to outperform
lessthan1%ofthewholeimagecontent),ourvirtualpat- previousmethodsbyalargemarginstartingfrompioneeringwork
tern paradigm outperforms state-of-the-art sensor fusion [24] that initially acted on a single phase of conventional stereo
methodsbyalargemargin; pipelines. Eventually, more sophisticated end-to-end stereo net-
• As shown in Fig. 1, it has a compelling ability to tackle worksbecamethemosteffectivesolutionfordisparityestimation.
domainshiftissues,evenwithnetworkstrainedonlywith Thesenetworks,thoroughlyreviewedin[1],[2],fallinto2Dand
synthetic data and without additional training or fine- 3Darchitectures.Theformerreliesonanencoder-decoderdesign
tuning; [25],[26],[27],[28],[29],[30],[31],[32]inspiredbyU-Net[33].3
The latter creates a cost volume from features extracted by the [96], [97], [98]. However, despite the compelling results, these
imagepairandestimatesdisparitythrough3Dconvolutions[34], approachessufferinareaswithoutdepthhints,aresensitivetothe
[4],[35],[36],[37],[38],[39],[40],[41],[42],[43],atthecostofa number of depth seeds used for training, and generalize poorly
muchhighermemoryandruntimerequirements.Morerecentdeep to out-of-domain data distributions. Purposely, [99] proposed a
stereo networks [3], [44], [45], [46], [47], [48] exploit the RAFT methodtodealwiththefirsttwomentionedissuesbyrelyingmore
[49] iterative refinement paradigm initially proposed for optical heavily on the image content. In contrast, [100] faced domain-
flow,orVisionTransformers[50],[51]tocatchbroadercontextual shift issues by casting depth completion as a fictitious stereo
information from images. Unfortunately, to achieve compelling correspondenceproblemdeployingthevirtualprojectionparadigm
performance,learning-basedmethodsneedmassiveannotateddata proposedinthispaper.
fortrainingandyieldlimitedgeneralizationcapabilitieswithout- GuidedStereoMatching.Sharingthemotivationoftheprevi-
of-domaindistributions[52],[53],[54],[55],[56],[57],[58],[59]. ousstrategy,theyaimtoinferdepthbyfusingsparsedepthseeds
Tomitigatetheseissues,self-supervisedtechniquesallowtotrain gathered by active sensors with registered stereo pairs exploiting
deep stereo models without ground-truth annotations leveraging the complementary strengths of the two sensing modalities. In
photometriclossesonstereopairsorvideos[60],[61],[62],[63], particular, sparse depth hints can provide additional cues that
[64], [65], traditional algorithms and confidence [66] measures complementstereoandimproverobustnessinchallengingregions,
[67], [68]. Other methods perform continuous self-supervised especiallythosefeaturingpoortexture.Inearlyworks,Badinoet
adaptation from input images to cope with domain-shift issues al.[101]usedynamicprogrammingtointegrateLiDARdatainto
[61],[69],[70]. aconventionalstereoalgorithm,whileGandhietal.[102]propose
Active Stereo Matching. Active stereo systems rely on a fusing ToF data and stereo pairs with an efficient seed-growing
projector, typically operating in the IR spectrum, to infer depth algorithm. In contrast, more recent works exploit sparse depth
byinjectingpatternsontothesensedsceneemployingtwoprinci- seedseitherbyconcatenatingthemasinputtoCNN-basedarchi-
ples: structured [7] and unstructured [8], [13] light. According tectures [10], [103], [104], [105] or by guiding cost aggregation
to different strategies, former methods work by analyzing the from existing cost volumes [11], [9], [106], [107] which require
behaviour of known patterns encoding depth in a single shot modificationstotheinternalstructureofthenetworks.Incontrast,
[71],[72],[73],[74]orbytemporalprojectionofknownpatterns our proposal follows an entirely different direction by acting
[75]. However, temporal projection is problematic when dealing outside the network before processing images. We increase the
with moving objects or camera ego-motion; thus, a much higher match distinctiveness, thus facilitating the visual correspondence
frame rate is sometimes used to soften this constraint [75]. In ofanystereomethodbycoherentlyhallucinatingtheinputvanilla
active systems that leverage structured light, depth triangulation stereopairwithvirtualpatternsgeneratedaccordingtothesparse
typically relies on the pattern projector serving as one of the two depthseedsprovidedbyadepthsensorbeforefeedingthemtoan
cameraviewpoints,althoughsomeexceptionsexist[76],[77].Due unmodifiedstereomatcher.
to its relevance, methods to optimize the projected patterns were
proposedin[78],[79],[80].Conversely,unstructuredlightmeth- 3 VIRTUAL PATTERN PROJECTION (VPP)
ods rely on a stereo camera and take advantage of the additional
Densely matching points across images is an inherently chal-
textureinjectedintothescenethrougharandompatternprojection
lenging task at the core of many vision problems. The strategy
[81], sometimes conceived beforehand to increase local distinc-
adopted by active stereo is potentially disruptive, but its depen-
tiveness [13], to enrich the scene and thus ease stereo matching,
dence on a physical pattern projector severely limits its practical
especially in challenging regions with ambiguous/repetitive fea-
effectiveness.Inlightofthisevidence,givenasetupwithastereo
tures or lacking distinctive details [82]. On the other hand, most
camera and the availability of sparse depth seeds registered with
of unstructured light proposals infer depth in a single shot [83],
the reference frame of the rig, our proposal aims to facilitate
[8], [13], [74], but, assuming static scenes, some methods can
visual correspondence by virtually hallucinating the vanilla input
increase accuracy when exposed to a temporal pattern [84], [85],
imagescoherentlywiththesparsedepthinformation.Incontrastto
[86]. Regardless of the working principle, pattern projection is
projector-based systems, it removes all the mentioned limitations
constrained to shorter distances up to a few meters away and
and works regardless of the technology used to infer such depth
unfeasibleundersunlight.Moreover,methodsbasedonstructured
points.Nonetheless,inthemosttypicalcurrentembodiment,these
projectionrequirecarefully designedprojectorssufferingthermal
points come from an active depth sensor – including cheaper
drifts [87] among other issues [88], and multiple projectors in
devices providing a modest yet accurate number of depth seeds
thesameareainterfere.Despitetheseseverelimitations,enriching
suffices(e.g.,evenlessthan1%ofthewholeimagecontent).
the original visual content with artificial texture yields dramatic
improvementoverpassivestereo.Consequently,activestereoisa
livelyresearchfield[74],[83],[86],[77]. 3.1 VirtualProjectionPrinciple
Image-Guided Methods. An alternative strategy for depth Despitehavingthesamegoalandprocessingpatternedimages,our
estimation aims to integrate sparse depth seeds gathered from an methodinitiallyreversesthebasicunstructuredprojectionprinci-
activesensorwithvisualinformation.Accordingtothisprinciple, pleofactivestereotocompensateforthemissingphysicalprojec-
twomainstrategiesexistintheliterature. tionusinginsteadtheinputsparsedepthpointstohallucinatethe
DepthCompletion.Itaimstoestimatedensedepthmapsatthe vanillastereopaircoherently.Specifically,ourcrucialobservation
resolution of the single RGB camera in the setup, exploiting the is that, given a rectified stereo rig and a pool of sparse depth
visualcontentandsparsedepthseedsgatheredbyadepthsensor. points registered with the reference image of the stereo rig, each
Manymethodshavebeenproposedforthispurpose,rangingfrom of these depth points implicitly locates two corresponding pixels
traditional methods based on interpolation and optimization [89], in the reference and target image of the stereo camera. Relying
[90], [91], to learning-based approaches [92], [93], [94], [95], onthisevidence,wecoherentlyaugmentthevisualappearanceof4
(a) (b)
RLaon ngge RLaon ngge
A B
Reference Sparse depth Target
Stereo
VPP
Matcher
Traditional Active Stereo Virtual Pattern Projection (ours) Reference (VPP) Target (VPP) Disparity map
(Stereo + Projector) (Stereo + Depth sensor)
Fig.2:Overviewof theVirtual PatternProjection framework.(a)Unlikeconventionalactivestereosystems,wherethephysically
projectedpatternsarevisibleonlyincertainlightconditionsandwithinalimiteddistance,oursetupovercomestheseconstraintsusing
a standard stereo camera and a depth sensor. (b) The Virtual Pattern Projection (VPP) paradigm coherently hallucinates the vanilla
stereopairacquiredbyastandardcameraexploitingthesparsedepthseedsgatheredbyadepthsensor.Thefictitiouslypatternedstereo
pairinoutputcanbeseamlesslyprocessedbyanystereomatcher,eitherhandcraftedorlearning-based.
both pixels in the two images, hallucinating them identically and augmenting strategies named random and histogram-based pat-
makingthemasdistinctiveaspossiblefromtheirneighbours.This tern projection. Both operate on corresponding points (x,y) and
novel paradigm allows us to conceive an imaginary setup with a (x′,y)locatedalongtheepipolarlineasoutlinedbefore,applying
realstereocameraandafictitiousprojectorcapableofprojecting thesamepatternoperatorP(x,x′,y)ontothetwoimages:
consistent patterns onto the input vanilla stereo pair where depth
seedsareavailable.Ourkeyinsightisthat,evenwhenappliedvery I (x,y)←P(x,x′,y)
L
(1)
sparsely, our peculiar virtual image patterning can significantly I (x′,y)←P(x,x′,y)
R
enhance matching by enabling more meaningful local and global
Thedifferencebetweenthetwostrategiesishowtheoperator
reasoning over the entire image. Besides, by selecting a depth
P(x,x′,y) generates the virtual pattern superimposed to input
sensor or any other means to infer reliable sparse seeds suited
images. Fig. 3 outlines the possible patterns we will discuss in
forthetargetsensingenvironment,weeliminatetheshortcomings
the remainder. Moreover, since x′ is unlikely to be an integer,
of active stereo based on unstructured light while retaining its
we apply P(x,x′,y) to I (⌊x′⌋,y) and I (⌈x′⌉,y) using a
advantages. Additionally, our proposal enables processing in the R R
weightedsplattingaccordingtoβ =x′−⌊x′⌋.
samevisualtargetdomain,hencenotrequiringadditionalcameras.
Fig.2(b)outlinesthecentralideabehindourvirtualprojection
I (⌊x′⌋,y)←βI (⌊x′⌋,y)+(1−β)P(x,x′,y)
principle. The envisioned setup consists of a calibrated stereo R R
(2)
camera and a depth sensor providing sparse yet accurate depth I R(⌈x′⌉,y)←(1−β)I R(⌈x′⌉,y)+βP(x,x′,y)
points registered with the stereo reference camera through an
3.2.1 RandomPatterning
initial calibration. We observe that through the known stereo
Tobetterfacevisualcorrespondence,thepatternsshouldincrease
camera geometry, the depth z(x,y) of each input seed, turned
similarity across corresponding points in the two images and
in a disparity value, implicitly locates two corresponding points
improve local distinctiveness [109], [110], at least along epipolar
I (x,y) and I (x′,y) in the input images. Specifically, by
L R lines. Consequently, a pattern like the one in Fig. 3 (i) would be
knowing the focal length f and the baseline b of the stereo
suboptimal.Hence,tobetterfacethisissue,afirstmethodusesan
camera,thedisparitycorrespondingtoz(x,y)canbeobtainedas
operator P(x,x′,y) that projects a random value picked from a
d(x,y) = b·f [108]. Given a pixel I (x,y), such a disparity
z(x,y) L uniformdistributionU asfollows:
value represents the offset to obtain the position along the same
epipolar line of the corresponding point I R(x′,y) in the target P(x,x′,y)∼U(0,255) (3)
imagewithx′ = x−d(x,y).Fig.2(b)illustratesthisevidence,
showing how two corresponding pixels A e B in the two input Fig. 3 (ii) reports a possible outcome of this strategy whose
images can be located by knowing the depth of A. Relying on extensiontomulti-channelimages,asforanymethodproposed,is
this, we can coherently hallucinate the two vanilla input images straightforward by applying the same patterning strategy on each
wheredepthseedsareavailabletofacilitatestereocorrespondence. channel. This strategy might generate more meaningful patterns
with an almost negligible computing overhead. However, inher-
ently, it does not grant a complete fulfilment of distinctiveness
3.2 VirtualPatterning
requirements, as depicted on the head and a portion of the back-
For our purposes, a proper image hallucination should render ground Fig. 3 (ii). In contrast, a patterning operator that analyses
corresponding points identical. Accordingly, we propose two theimagecontentcouldconstantlyimprovedistinctiveness.5
(i)
(ii)
3 (iii)
(iv)
3 (v)
(vi)
5 (vii)
L L
Fig. 3: Virtual patterns. From top to bottom: (i) indistinctive, Fig. 4: Handling occlusions. I) P is framed by the reference
(ii) randomly generated, (iii) distinctive, (iv) randomly generated cameraanddepthsensorbutoccludedinthetargetcamera(“NO”
patch-based uniform, (v) distinctive patch-based uniform, (vi) projection), II) Projection of the same pattern (yellow) onto the
randomlygeneratedpatch-based,and(vii)distinctivepatch-based twoinputimagesaccordingtodepthinthebackground(“BKGD”
patterns.Inallexperiments,weuseasearchareaoflengthL,set projection), III) Projection of the foreground image content (Q)
to64andheight(2+N)whereN representstheverticalsizeof from the target image to the background in the reference image
thevirtualpatches. (“FGD”projection).
3.2.2 Histogram-basedPatterning 5×5), implicitly assuming that the disparity within these areas
In light of the previous discussion, patterns superimposed onto doesnotchange.Fig.3,in(iv)and(v),depictsapossibleoutcome
imagesshouldstandoutfromthebackgroundandbeunambiguous of the operator P(x,x′,y) acting on a region of length L, that
withinnearbypixelsalongthesamehorizontalscanline,asshown picks a uniform color within patches according to, respectively,
inFig.3(iii). random sampling or a histogram based selection. Nonetheless,
Purposely, for each new pixel needing a virtual pattern, we we can generate additional and more distinctive patterns through
use a histogram-based operator P(x,x′,y), to select the optimal (vi) random sampling or (vii) and a histogram-base selection
patternbyanalyzingthescanlinecontentinthetwoimagessubject being performed for every pixel in the patch independently, as
to hallucination. Specifically, for a pixel (x,y) in the reference depicted in Fig. 3. However, since naively setting a constant size
image,weconsidertwowindowsofheight3andlengthLcentred squaredwindowmightbesub-optimal,weproposetwoadditional
on it and on (x′,y) in the target image. Then, the histograms strategiesintheremainder.
computed over the two areas are merged into a single one and Distance-based patterning Since things appear smaller in
the operator P(x,x′,y) picks from it the value that maximizes the images at farther distances, a large virtual patch might cover
the distance from any other entry in the histogram hdist(i), with mostofasmallobjectwhenitisfarfromthecamera,preventing
hdist(i) returning the minimum distance from a filled bin in the thestereoalgorithmfrominferringitspossiblydifferentdisparity
sumhistogramH values. To deal with this issue, we propose a dynamic patch size
thatexploitstheavailableinputsparsedepthseeds.Consideringa
hdist(i)=(cid:8)
min{|i−i |, |i−i |},
disparity hint d(x,y), a disparity search range [D min,D max], and
l r amaximumpatchsizeN ,weadjustthepatchsizeN(x,y)as
max
i ∈[0,i[:H(i )>0, (4)
l l follows:
(cid:9)
i ∈]i,255]:H(i )>0
r r
When every bin in H is not empty, we select the value with the N(x,y)=round(cid:32)(cid:18)d(x,y)−D min(cid:19) ϕ1
·(N
−1)+1(cid:33)
minimumoccurrences. D −D max
max min
(6)
3.3 AdvancedVirtualPatterns where ϕ models the mapping curve and D ,D are given
min max
Wewillnowextendthestrategiesdescribedsofartoaccountfor respectivelybythenearestandthefarthesthintintheframe.
theimagecontent,spatiallocality,distance,andocclusions. Adaptivepatch.Regardlessofthedistancefromthecamera,a
fixedpatchshapebecomesproblematicneardepthdiscontinuities
3.3.1 Blendingimagecontentandvirtualpatterns and with thin objects, although widely used in stereo and other
tasks [108]. As detailed next, following [100], we adapt the
Although a virtual pattern facilitates matching for traditional
patch shape according to the reference image content to address
algorithms like [22], it might hamper a deep stereo network not
this challenge. Given a local window N(x,y) centered around
usedtodealwithsuchdata.Hence,tosoftenthisissue,wemerge
the disparity hint d(x,y), for each pixel (u,v) ∈ N(x,y) we
the image content with virtual patterns using an alpha-blending
estimatethespatialS(x,y,u,v)=(x−u)2+(y−v)2andcolor
[108]tunedaccordingtoahyperparameterα∈[0,1]:
C(x,y,u,v) = |I (u,v)−I (x,y)| agreement with the refer-
L L
encehintpoint(x,y)andfeedthemintomodelW (x,y,u,v):
I (x,y)←(1−α)I (x,y)+αP(x,x′,y) c
L L
(5)
I R(x′,y)←(1−α)I R(x′,y)+αP(x,x′,y) (cid:18)S(x,y,u,v) C(x,y,u,v)(cid:19)
W (x,y,u,v)=exp + (7)
c −2σ2 −2σ2
3.3.2 SpatialLocality s c
The pointwise patterning strategy proposed can be extended to where σ and σ control the impact of the spatial and color
s c
nearby points to further enrich the overall visual appearance. To contribution as in a bilateral filter. For each pixel (u,v) within
this aim, we apply previous methods to patches (e.g., 3 × 3, a patch centred in (x,y), we apply the virtual projection only6
ErrorRate(%)>2
Patch Setting
RAFT-Stereo[3] PSMNet[4] rSGM
(A) ✗ No-VPP 11.7 29.6 26.4
(B) 3×3 Fixedpatchsize(baseline[12]) 5.2 15.2 11.9
(a) (b) (C) 7×7 Distance-basedpatch 5.2 15.2 12.6
(D) 7×7 Bilateralbasedpatch 4.9 15.2 11.0
(E) 7×7 (C)+(D) 5.2 15.2 12.4
TABLE1:Ablationstudyonadaptivepatches.ResultsonMidd-
A.Networkstrainedonsyntheticdata[25].
(c)
(d)
Fig.5:VPPinaction.Hallucinatedstereopairusingpattern(vi);
zoomed-inviewwithFGD-Projection(a),correspondingarea(b) deep stereo networks. Fig. 5 shows an example of a stereo pair
with sub-pixel splatting and left border occlusion projection (c). hallucinatedaccordingtothe‘FGD”strategy.
Adaptive patches guarantee the preservation of depth discontinu-
itiesandthindetails(d). 4 EXPERIMENTAL RESULTS
This section describes our exhaustive experimental evaluation,
if W(x,y,u,v) exceeds a threshold t . Additionally, we store includingimplementationdetails,datasets,andresultsanalysis.
w
W(x,y,u,v)valuesinaproperdatastructuretohandleoverlap-
pingpixelsbetweentwoormorepatches–i.e,weperformvirtual 4.1 ImplementationandExperimentalSettings
projectiononlyforthepixelwiththehighestscore.
All virtual pattern variants depicted in Fig. 3 from (ii) to (vii)
areimplementedinPythonandNumbawithsub-pixeldisparities
3.3.3 Occlusions
splatted on adjacent pixels in the right view. Concerning hyper-
Evenassumingadepthsensorperfectlyalignedwiththereference
parameters, we set λ = 2,γ = 0.4375,t = 1 r ×r = 9×7
camera – although not feasible [111] – a stereo setup generates x y
for the occlusion detection heuristic, following our previous in-
occlusions by construction, given the different positions of the
vestigation [12], σ = 2,σ = 1,t = 0.001 for adaptive
two cameras. Consequently, we cannot consistently project the s c w
patch and ϕ = 0.3 for distance-based pattering. To assess the
pattern between the two views for these areas. Considering this
effectiveness of VPP, we run several experiments to compare
evidence, detecting depth seeds in occluded regions is crucial to
it with existing approaches that combine sparse depth points
avoid projecting the same pattern on the occluded and occluder
with stereo algorithms and networks. In particular, we select the
pixelsonthereferenceandtargetimage,asdepictedinFig.4I).To
Guided Stereo matching framework [9] and LidarStereoNet [10]
tacklethisproblem,weproposeasimpleyeteffectiveheuristicto
ascompetitorsbeingsourcecodeavailable.
classifysparseinputdisparityhintswarpedontothetargetimage
SincebothareimplementedoverthePSMNet[4]architecture,
according to the difference in disparity and spatial distance from
weapplyVPPtothesamemodelforadirectandfaircomparison.
otherinputseeds.
However, the original PSMNet weights used in [9] yield poor
Specifically,wewarpdisparityd(x,y)intoanimage-likegrid
generalization results. Hence, we retrain it following the original
W atcoordinates(x′,y);whenmultiplewarpeddisparitiescollide
protocol[4],i.e.,for10epochsonSceneFlow(specifically,using
at the exact location (x′,y), we keep the largest one. Then, each
FlyingThings [25]) with a constant learning rate equal to 1e−3.
(x ,y )inW isclassifiedasoccludedifthefollowinginequality
o o For fairness, we retrain LidarStereoNet using the same protocol.
holdsforatleastoneneighborW(x,y)withinanr ×r patch:
x y Unlessotherwisespecified,wetrainallthenetworksaccordingto
W(x,y)−W(x ,y )−λ(γ|x−x |+(1−γ)|y−y |)>t (8) this protocol. Moreover, since Guided Stereo [9] represents our
o o o o
closest competitor – i.e., it does not require any architectural
with λ,γ,r ,r ,t hyper-parameters. Finally, we warp back oc-
x y change to the stereo model – we implement it over a more
cludedpointstoupdatethebinaryocclusionmaskΩ.
modern network, RAFT-Stereo [3], and compare it against our
When a disparity hint (x,y) turns occluded (Ω(x,y) = 1),
VPPproposalusingthesamenetwork.Toconclude,weextendthis
we could neglect projection on both images of the stereo pair
comparisonwithanimplementationoftheSemi-Global-Matching
(“NO”projectionstrategy).Hence,intheproximityofocclusions,
method [22], i.e., rSGM [112], as a representative of traditional
this strategy can refrain from projecting the same pattern on the
stereo algorithms. We run it by setting the maximum disparity to
foreground(inthetargetimage)andbackground(inthereference)
192 disparity P1 = 11, adaptive P2 [113] (with P2 = 17,
min
– as would occur with the strategy named “BKGD” projection,
P2 =0.5,P2 =35),applyingsub-pixelrefinement,left-right
α γ
Fig. 4 II) – to reduce ambiguity. Nonetheless, to better exploit
check and a speckle filter to remove outliers, then filling holes
the few sparse seeds available at best, we follow a third strategy:
withbackgroundinterpolation[114].
we do not project when Ω(x,y) = 1 and instead replace the
originalcontentin(x,y)onthereferenceimagewiththecontent
at (x′,y) in the target image. This approach – named “FGD” 4.2 EvaluationDatasets&Protocol
projection,Fig.4III)–doesnotaltertheappearanceofthecorrect Weconductexperimentsonsevenindoor/outdoordatasets,includ-
foregroundmatch(raysoriginatingfromQ),yettriggersthestereo ingtwofeaturingpassive/activeimages.Werefertoourcodebase
matchertosearchforasecondcorrespondencewiththesamepoint fortheimagesselectedfromeachdataset.
(x′,y)inthetargetimage,i.e.,withpixel(x,y)originatingfrom Middlebury.TheMiddleburydataset[76]isahigh-resolution
P. Additionally, points close to the left border of the reference stereo dataset featuring indoor scenes, captured under controlled
image would project patterns outside the target image. Although lighting conditions with accurate ground-truth obtained using
this fact would be irrelevant for traditional algorithms, we still structuredlight.Weevaluateourapproachonthreedifferentsplits:
project there (‘FGD”) to avoid artefacts in the predictions by the Additional 13 scenes in Middlebury 2014 (Midd-A), the 157
Midd-14 Midd-21 ETH3D
Model TrD aie npthPoin Tts est >1 >Err 2orRate(% >) 3 >4 a (pv xg ). >1 >Err 2orRate(% >) 3 >4 a (pv xg ). >1 E >rr 2orRate(% >) 3 >4 a (pv xg ).
rSGM[112] ✗ ✗ 44.82 32.00 27.21 24.48 13.19 49.62 33.10 26.12 22.16 7.69 16.58 8.02 5.35 4.12 0.93
rSGM-gd[9] ✗ ✓ 32.47 18.80 14.64 12.77 9.44 34.94 17.27 11.7 9.34 4.05 7.26 2.40 1.53 1.19 0.47
rSGM-vpp ✗ ✓ 14.35 10.31 9.05 8.37 8.18 10.98 6.61 5.51 4.95 2.47 0.62 0.43 0.38 0.34 0.20
PSMNet[4] ✗ ✗ 48.52 31.11 24.29 20.58 10.05 47.25 28.03 20.19 15.88 4.52 19.73 6.48 4.09 3.11 0.88
PSMNet-gd[9] ✗ ✓ 48.24 30.66 24.03 20.47 10.50 46.61 27.18 19.60 15.59 4.49 18.96 5.91 3.56 2.85 0.84
PSMNet-vpp ✗ ✓ 25.75 15.75 12.83 11.50 6.73 21.32 10.21 6.81 5.29 1.58 11.58 2.43 1.65 1.53 0.62
PSMNet-gd-ft[9] ✓ ✓ 33.82 15.61 10.74 8.77 4.52 35.76 13.77 7.55 5.21 1.71 12.28 2.43 0.86 0.61 0.54
PSMNet-vpp-ft ✓ ✓ 23.68 13.88 11.09 9.87 6.29 20.44 9.90 6.69 5.26 1.72 2.92 1.43 1.23 1.14 0.39
PSMNet-gd-tr[9] ✓ ✓ 25.17 12.61 9.13 7.59 3.84 23.67 9.63 5.75 4.15 1.33 4.79 0.85 0.54 0.42 0.33
PSMNet-vpp-tr ✓ ✓ 20.79 12.07 9.59 8.44 4.40 18.03 8.22 5.35 4.14 1.49 1.77 1.12 0.98 0.91 0.28
LidarStereoNet[10] ✓ ✓ 32.72 16.30 12.10 10.34 4.48 27.32 11.67 7.58 5.88 1.80 10.39 1.05 0.47 0.30 0.45
CCVNorm[11] ✓ ✓ 30.22 12.49 7.54 5.58 2.27 20.88 7.63 4.47 3.28 1.14 17.63 4.69 2.16 1.28 0.66
RAFT-Stereo[3] ✗ ✗ 24.24 15.66 12.47 10.63 3.85 20.06 10.27 7.19 5.52 1.31 2.62 1.22 0.88 0.70 0.27
RAFT-Stereo-gd[9] ✗ ✓ 24.14 11.58 7.85 6.08 2.87 20.38 8.31 5.09 3.65 1.11 5.32 1.68 1.06 0.76 0.41
RAFT-Stereo-vpp ✗ ✓ 7.54 5.09 4.20 3.68 2.18 6.58 4.00 3.09 2.55 0.67 1.19 0.75 0.57 0.47 0.14
RAFT-Stereo-gd-ft[9] ✓ ✓ 15.22 8.02 5.97 5.05 2.67 15.51 6.76 4.63 3.63 1.21 2.52 1.28 1.00 0.77 0.29
RAFT-Stereo-vpp-ft ✓ ✓ 6.46 4.16 3.42 3.04 1.85 5.93 3.59 2.82 2.40 0.76 0.94 0.68 0.58 0.54 0.13
RAFT-Stereo-gd-tr[9] ✓ ✓ 6.39 3.29 2.35 1.93 0.91 6.45 3.14 2.25 1.82 0.64 0.94 0.59 0.46 0.38 0.14
RAFT-Stereo-vpp-tr ✓ ✓ 5.12 3.71 3.19 2.90 1.26 4.62 2.82 2.20 1.85 0.62 0.78 0.61 0.54 0.50 0.12
TABLE2:Comparisonwithexistingmethods.ResultsonMidd-14,Midd-21,ETH3D.Networkstrainedonsyntheticdata[25].
scenes from Middlebury 2014 training set (Midd-14), and the 24 depthproportionallytothesquareofthedistancefromtheoptical
scenesfromMiddlebury2021(Midd-21).Resultsareevaluatedat centre. To select a percentage of disparity hints comparable to
fullresolution,withPSMNetrunningathalfresolution. the number of IR dots of the original dataset, we utilize the
KITTI 2015 [114]. This real-world stereo dataset depicts SIMSTEREOSDKtorenderthepatternonaplane;wethencount
autonomousdrivingscenarios,capturedataresolutionofapprox- theblobsand,finally,werandomlysample2%ofpointsfromthe
imately 1280×384 pixels with sparse ground-truth depth maps ground-truthtoget∼6000noisydisparityhints–slightlysmaller
collected using a LiDAR sensor. It provides 200 stereo pairs thanthenumberofIRblobs.
annotatedwithground-truth,includingindependentlymovingob- M3ED-active. After manual inspection of M3ED, we found
jects such as cars. Among the 200 samples, we select 142 stereo thattheindoorsequencescollectedbythequadrupedrobotexpose
pairsforwhichrawLiDARmeasurementsareprovided[10].This an active pattern projected in the scene, appearing once every
allows for evaluating the effectiveness of VPP and competitors twoframes.Accordingly,weselectedtwosplits:(PassiveIndoor,
withsparsepointsobtainedbyarealsensor,ratherthansimulated. counting200frameswheretheactivepatternisabsent;andActive
ETH3D [115]. This dataset collects indoor and outdoor Indoor), made of the 200 frames with visible pattern closest in
scenes, with a total of 27 grayscale low-resolution stereo pairs time to those in the passive split. Both splits also provide raw
andcorrespondingground-truthdisparitymaps. LiDARscansandground-truthmaps.Althoughtheimagesinthe
DSEC [116]. An outdoor stereo dataset focused on au- two splits do not coincide, the tiny timeshift between adjacent
tonomous driving with RGB and event cameras, captured using passive and active frames – < 0.1s – introduces negligible
awidebaseline(50cm)at1440×1080resolution.Ground-truth changes in the geometry of the scene, allowing for a fair enough
disparity,consistingof26384mapsorganizedinto41sequences,is comparison between methods running on the two different splits
obtained using a LiDAR-Inertial-Odometry (LIO) algorithm that distinctly.Forfairness,wemanuallymaskpartoftheground-truth
accumulates the 16-line LiDAR scans. We split them into three andrawLiDARscanstomatchregionswithprojectedpatterns.
different test sets with increasing challenging illumination condi- Evaluation Protocol. We compute the percentage of pixels
tions–i.e,Day,AfternoonandNight splits.Wealsodevelopeda with a disparity error higher than a certain threshold τ compared
pipeline to extract de-skewed raw LiDAR scans aligned with the totheground-truth.Wereporterrorratesbyvaryingτ to1,2,3,
reference image and to subsample our splits with a final number and4,togetherwiththeaveragedisparityerror(avg).Weevaluate
of200stereoframesforeachsplit. our computed disparity maps across occluded and non-occluded
M3ED [117]. This dataset collects 57 indoor/outdoor scenes regionswithvalidgroundtruthdisparityunlessotherwisenoted.
acquired using a portable multi-sensor device mounted on three
different vehicles – i.e, a drone, a quadruped robot, and a car. 4.3 AblationStudy
In contrast to KITTI and DSEC, the 64-line LiDAR provides We start by studying the impact of VPP under different settings
bothdenserrawLiDARscansandLIO-accumulatedground-truth in Tab. 1. These experiments are carried out on Midd-A at full
maps. At the same time, the grayscale stereo rig has a shorter resolution, involving PSMNet [4], RAFT-Stereo [3], and rSGM
baseline (12 cm) and a slightly smaller resolution (1280×800), [112]. The first row (A) reports the error rates achieved by the
cropped to 950 × 525 after our pre-processing pipeline that original networks/algorithm without applying any virtual pattern
rectifiesstereoframesandground-truth,projectsrawLiDARscans projection.In(B),weenableVPPbysettingthehyper-parameters
ontothereferenceimage,andgeneratesasubsetof200framesfor accordingtoourpreviousstudiesin[12]–i.e,a3×3patch-based
the three splits we define –i.e, Outdoor Day, Outdoor Night, and random pattern (vi) with alpha-blending α = 0.4 and “FGD”
Indoor. occlusionsetting–appreciatingthenotabledecreaseinerrorrates.
SIMSTEREO [118]. A synthetic indoor/outdoor pas- Starting from these settings, we evaluated different patches with
sive/active dataset with dense ground-truth; RGB and IR stereo a grid search, studying the impact given by using (C) distance-
pairshave640×480resolution,withthelattermodalityexposing based patches or (D) adaptive patches, observing that only the
the active pattern. The authors utilized a photo-realistic renderer latter yields a further improvement, optimal with a 7×7 patch.
togenerate515frameswithoverlappingRGBandIRmodalities: Finally, we combine the two in (E), finding that the best choice
we utilize the 103 frames from the predefined test subset. We is (D) alone. Therefore, as hyper-parameters for the remaining
simulatetheavailabilityofsparsedepthseedsusingground-truth experiments, we will assume those found in [12] using 7 × 7
sampling and add a Kinect noise model [119] perturbating the adaptivepatches(D).8
Midd-21
DepthPoints ErrorRate(%) avg.
Model
Train Test >1 >2 >3 >4 (px)
PSMNet[4] ✗ ✗ 44.75 24.98 17.31 13.37 3.42
PSMNet-gd ✗ ✓ 44.40 24.58 16.98 13.09 3.40
PSMNet-vpp ✗ ✓ 21.19 10.56 7.13 5.52 1.55
PSMNet-gd-ft ✓ ✓ 40.47 16.06 8.56 5.83 1.78
PSMNet-vpp-ft ✓ ✓ 20.65 10.08 6.78 5.29 1.61
PSMNet-gd-tr ✓ ✓ 23.15 9.44 5.60 4.04 1.29
PSMNet-vpp-tr ✓ ✓ 18.08 8.40 5.52 4.29 1.52
LidarStereoNet[10] ✓ ✓ 25.08 10.09 6.47 4.94 1.86
CCVNorm[11] ✓ ✓ 20.54 7.45 4.31 3.12 1.06
RAFT-Stereo[3] ✗ ✗ 19.22 9.38 6.28 4.68 1.26
RAFT-Stereo-gd ✗ ✓ 18.82 6.95 4.06 2.88 1.05
RAFT-Stereo-vpp ✗ ✓ 6.13 3.52 2.67 2.20 0.67
RAFT-Stereo-gd-ft ✓ ✓ 14.87 6.09 3.99 3.09 1.02
RAFT-Stereo-vpp-ft ✓ ✓ 6.18 3.74 2.95 2.51 0.70
RAFT-Stereo-gd-tr ✓ ✓ 6.04 2.91 2.08 1.70 0.58
RAFT-Stereo-vpp-tr ✓ ✓ 4.36 2.66 2.09 1.77 0.52
TABLE4:Fine-tunedmodels.ResultsonMidd-21,aftertraining
PSMNet[4],LidarStereoNet[10]andCCVNorm[11]onsynthetic
Fig. 6: Depth sparsity vs accuracy. Results on Midd-14 [76] data [25] and fine-tuning on Midd-14 [76]. We deployed the
by VPP and competitors with different amounts of depth points. officialMiddleburyweightsforRAFT-Stereo[3].
Networkstrainedonsyntheticdata[25].
KITTI142
Midd-14 Model DepthPoints ErrorRate(%) avg.
DepthPoints ErrorRate(%) avg. Train Test >1 >2 >3 >4 (px)
Model
Train Test >1 >2 >3 >4 (px) rSGM[112] ✗ ✗ 30.27 12.24 7.30 5.27 1.35
rSGM[112] ✗ ✗ 44.82 32.00 27.21 24.48 13.19 rSGM-gd ✗ ✓ 20.87 7.72 4.90 3.73 1.05
rSGM-gd ✗ ✓ 32.47 18.80 14.64 12.77 9.44 rSGM-vpp ✗ ✓ 14.20 5.59 4.18 3.48 1.06
rSGM-vpp ✗ ✓ 14.35 10.31 9.05 8.37 8.18 PSMNet[4] ✗ ✗ 32.50 11.70 6.40 4.51 1.32
rSGM-gd-vpp ✗ ✓ 13.96 9.98 8.74 8.06 7.91 PSMNet-gd ✗ ✓ 32.59 11.94 6.71 4.81 1.35
PSMNet[4] ✗ ✗ 48.52 31.11 24.29 20.58 10.05 PSMNet-vpp ✗ ✓ 19.92 6.75 4.43 3.50 1.09
PSMNet-gd ✗ ✓ 48.24 30.66 24.03 20.47 10.50 PSMNet-gd-ft ✓ ✓ 30.39 9.79 5.18 3.69 1.27
PSMNet-vpp ✗ ✓ 25.75 15.75 12.83 11.50 6.73 PSMNet-vpp-ft ✓ ✓ 20.77 6.55 4.11 3.21 1.10
PSMNet-gd-vpp ✗ ✓ 26.92 16.14 13.21 11.87 6.91 PSMNet-gd-tr ✓ ✓ 25.84 8.30 4.73 3.47 1.17
LidarStereoNet[10] ✓ ✓ 32.72 16.30 12.10 10.34 4.48 PSMNet-vpp-tr ✓ ✓ 16.44 5.94 4.09 3.29 1.07
LidarStereoNet-vpp ✓ ✓ 30.00 15.63 11.91 10.32 4.62 LidarStereoNet[10] ✓ ✓ 33.01 10.16 5.13 3.57 1.33
CCVNorm[11] ✓ ✓ 30.22 12.49 7.54 5.58 2.27 CCVNorm[11] ✓ ✓ 18.98 6.78 4.50 3.52 1.17
CCVNorm-vpp ✓ ✓ 29.29 12.66 7.94 5.98 2.34 RAFT-Stereo[3] ✗ ✗ 24.57 8.74 5.09 3.66 1.10
RAFT-Stereo[3] ✗ ✗ 24.24 15.65 12.48 10.62 3.87 RAFT-Stereo-gd ✗ ✓ 32.50 12.61 7.10 4.95 1.33
RAFT-Stereo-gd ✗ ✓ 24.14 11.58 7.85 6.08 2.87 RAFT-Stereo-vpp ✗ ✓ 14.27 6.35 4.55 3.63 0.94
RAFT-Stereo-vpp ✗ ✓ 7.54 5.09 4.20 3.68 2.18 RAFT-Stereo-gd-ft ✓ ✓ 23.90 8.37 5.01 3.72 1.13
RAFT-Stereo-gd-vpp ✗ ✓ 12.08 6.12 4.52 3.71 2.20 RAFT-Stereo-vpp-ft ✓ ✓ 13.03 5.62 4.04 3.27 0.91
RAFT-Stereo-gd-tr ✓ ✓ 15.51 6.30 4.22 3.27 0.95
RAFT-Stereo-vpp-tr ✓ ✓ 11.35 4.92 3.65 3.00 0.89
TABLE 3: Combining VPP with existing methods. Results on
Midd-14[76]withnetworkstrainedonsyntheticdata[25]. TABLE5:ExperimentsonKITTI.Resultsonthe142split[10]
withrawLiDAR.Networkstrainedonsyntheticdata[25].
4.4 ComparisonwithExistingApproaches
WenowassesstheperformanceofVPPanditstwomaincompeti- by the developer, either through a short fine-tuning on the pre-
tors. If not differently specified, we randomly sample 5% depth existingmodelor,withmajorefforts,byretrainingitfromscratch.
pointsfromGT,asin[9]. Comparison on Middlebury/ETH3D. Tab. 2 reports the
Guided/VPP variants. Given the flexibility of VPP and outcome on Midd-14, Midd-21 and ETH3D datasets using the
Guided Stereo, we evaluate their application to stereo networks rSGMalgorithmandmodelstrainedonsyntheticdataonly.From
underthreedifferentsettings,respectively: it, we can highlight one of the most evident advantages of our
proposal, i.e., its remarkable capability to boost cross-domain
• Withoutretrainingthestereonetwork(-gd/-vpp)
generalization on all datasets without any retraining (-vpp). In
• After a brief fine-tuning of the pre-trained model, en-
contrast,GuidedStereoyieldsonlymarginalimprovementsunder
hanced by Guided Stereo or VPP frameworks (-gd-ft/-
thissetting(-gd),andLidarStereoNetneedstrainingfromscratch
vpp-ft)
to process depth points since they are concatenated to the RGB
• By training from scratch a new model, enhanced by
images.Inlightofthisevidence,theVPPparadigmisundoubtedly
GuidedStereoorVPP(-gd-tr/-vpp-tr)
thebestchoicetoboosttheaccuracyofoff-the-shelf networks.
For -ft variants, two epochs of finetuning are carried out on Fine-tuningthenetworksshortlytogetacquaintedbetterwith
FlyingThings [25], with learning rate 1e-4 and 1e-5 for PSMNet the sparse depth cues is beneficial for Guided Stereo (-gd-ft)
and RAFT-Stereo, respectively. For -tr variants, PSMNet and and VPP (-vpp-ft) despite the latter frequently – always with
RAFT-Stereoaretrainedfor10and20epochs,withlearningrates RAFTStereo – outperforming the former even without any fine-
1e−3 and 1e−4, respectively. In any case, PSMNet and RAFT- tuning (-vpp). When training from scratch the networks, Guided
Stereoprocess384×512and360×720crops,respectively,with Stereo(-gd-tr)is,sometimes,marginallymoreeffectivethanVPP
batchsize2onasingle3090GPU. (-vpp-tr). This fact again emphasizes how VPP is much more
ThethreevariantsallowforevaluatingGuidedStereoandVPP trainingindependent,aremarkablefactorofmeritofourproposal.
whendeployedwiththeleasteffort–i.e.,bysimplytakingapre- Indeed, acting at the image level turns out more robust than
trained stereo network off the shelf – or with deeper intervention concatenating depth to RGB (LidarStereoNet) or acting on cost9
Midd-14 Midd-21 ETH3D KITTI142
Model Modelname TD rae ip nthPoi Tn et ss t >1 E >rro 2rRate( >%) 3 >4 a (pv xg ). >1 E >rro 2rRate( >%) 3 >4 a (pv xg ). >1 Er >ror 2Rate( >%) 3 >4 a (pv xg ). >1 E >rro 2rRate(% >) 3 >4 a (pv xg ).
RAFT-Stereo[3] Middlebury ✗ ✗ 17.77 9.74 7.17 5.85 1.69 19.22 9.38 6.28 4.70 1.26 2.75 1.41 1.01 0.86 0.29 24.95 7.81 4.39 3.19 1.05
RAFT-Stereo-vpp Middlebury ✗ ✓ 6.36 3.88 3.09 2.65 1.01 6.13 3.52 2.67 2.20 0.67 1.10 0.75 0.63 0.56 0.14 14.67 5.42 3.78 2.98 0.88
RAFT-Stereo[3] ETH3D ✗ ✗ 24.70 16.23 13.09 11.36 4.83 20.26 10.19 7.09 5.48 1.39 2.61 1.26 0.94 0.76 0.27 24.42 8.63 4.99 3.64 1.09
RAFT-Stereo-vpp ETH3D ✗ ✓ 7.55 5.04 4.13 3.60 2.42 6.44 3.83 2.96 2.51 0.67 1.06 0.66 0.51 0.42 0.12 14.04 6.19 4.43 3.53 0.93
GMStereo[120] Sceneflow ✗ ✗ 49.95 28.71 20.19 15.66 3.77 41.10 21.56 14.32 10.68 2.39 6.24 2.68 1.82 1.18 0.42 29.98 10.25 5.42 3.77 1.20
GMStereo-vpp∗ Sceneflow ✗ ✓ 28.17 13.59 9.90 8.25 3.36 17.67 8.31 5.71 4.46 1.33 2.77 1.74 1.37 1.13 0.30 19.02 7.10 4.60 3.49 1.03
GMStereo[120] Mixdata ✗ ✗ 31.36 14.11 8.98 6.63 1.80 30.44 11.07 6.20 4.29 1.32 1.76 0.50 0.36 0.31 0.30 16.28 3.82 2.10 1.40 0.71
GMStereo-vpp∗ Mixdata ✗ ✓ 20.76 9.67 7.05 5.85 1.92 17.86 7.13 4.53 3.40 1.07 1.83 1.19 1.01 0.90 0.33 13.00 3.71 2.24 1.59 0.68
CFNet[43] Sceneflow ✗ ✗ 43.78 29.85 24.32 21.13 11.90 39.75 22.44 16.08 12.81 11.24 7.03 4.06 3.12 2.65 1.14 25.49 9.25 5.25 3.83 1.11
CFNet-vpp∗ Sceneflow ✗ ✓ 23.54 16.58 13.86 12.29 12.50 14.70 8.23 6.18 5.09 1.36 2.46 1.93 1.74 1.61 0.71 14.22 6.27 4.44 3.49 0.95
CFNet[43] Middlebury ✗ ✗ 24.46 13.26 9.46 7.48 2.21 30.31 15.03 10.32 7.85 1.90 1.32 0.53 0.39 0.33 0.22 11.71 3.20 1.83 1.23 0.60
CFNet-vpp∗ Middlebury ✗ ✓ 14.41 9.59 7.93 7.04 2.85 12.54 7.12 5.38 4.44 1.10 1.84 1.23 1.11 1.06 0.30 9.24 3.02 1.88 1.35 0.55
HSMNet[31] Middlebury ✗ ✗ 32.73 17.50 12.25 9.57 2.55 36.77 18.12 11.97 8.97 2.27 10.26 3.05 1.78 1.24 0.58 31.69 12.34 6.74 4.55 1.27
HSMNet-vpp Middlebury ✗ ✓ 18.90 9.78 7.17 5.94 2.62 17.84 8.61 5.83 4.48 1.25 6.06 2.19 1.43 1.14 0.55 26.56 10.54 6.60 4.90 1.32
CREStereo[44] ETH3D ✗ ✗ 16.59 8.97 6.19 4.77 1.47 19.07 9.08 6.33 4.95 1.24 1.33 0.60 0.45 0.34 0.19 22.94 7.94 4.80 3.64 1.09
CREStereo-vpp∗ ETH3D ✗ ✓ 7.20 4.84 3.95 3.44 1.38 6.41 3.82 3.00 2.53 0.68 1.21 0.81 0.66 0.56 0.18 13.93 6.15 4.43 3.54 0.96
LEAStereo[38] Sceneflow ✗ ✗ 59.66 41.39 32.95 28.05 14.42 52.03 32.31 23.22 18.18 3.73 14.38 6.89 4.58 3.57 1.95 46.43 22.28 13.57 9.70 1.92
LEAStereo-vpp Sceneflow ✗ ✓ 24.47 14.83 11.74 10.26 7.96 20.21 10.17 7.02 5.56 1.77 2.03 1.06 0.83 0.70 0.24 17.48 7.21 5.03 4.02 1.12
LEAStereo[38] KITTI12 ✗ ✗ 62.59 47.10 39.79 35.63 18.74 56.79 35.90 26.62 21.46 5.45 21.54 7.97 4.79 3.79 2.98 22.49 6.07 3.00 2.00 0.87
LEAStereo-vpp KITTI12 ✗ ✓ 28.51 16.30 12.98 11.56 8.57 24.46 12.15 8.70 7.17 2.21 8.60 1.82 1.18 0.93 0.43 12.12 3.66 2.28 1.64 0.68
HITNet[32] Sceneflow ✗ ✗ 42.67 26.74 20.49 17.07 5.88 39.21 22.34 16.02 12.52 3.53 18.22 7.95 4.08 3.25 0.74 29.92 10.76 5.94 4.22 1.23
HITNet-vpp∗ Sceneflow ✗ ✓ 24.03 13.62 10.20 8.52 3.42 19.35 9.96 7.07 5.67 1.59 11.82 3.25 1.32 1.15 0.47 19.56 7.00 4.47 3.43 1.02
CoEx[121] Sceneflow ✗ ✗ 50.56 40.47 36.52 34.11 22.19 41.36 27.48 22.60 20.01 9.26 16.01 4.67 2.76 2.08 0.68 35.52 12.40 6.38 4.41 1.35
CoEx-vpp∗ Sceneflow ✗ ✓ 32.47 27.31 25.49 24.38 17.92 19.30 14.01 12.48 11.71 6.75 12.20 2.91 1.50 1.24 0.57 31.17 10.14 5.47 4.02 1.30
ELFNet[122] Sceneflow ✗ ✗ 61.96 44.79 36.02 30.83 15.51 41.16 25.81 19.78 16.32 5.80 24.91 11.90 7.69 6.11 3.53 32.36 13.71 8.19 5.93 1.65
ELFNet-vpp∗ Sceneflow ✗ ✓ 40.86 25.14 18.89 15.71 7.79 18.80 11.32 9.04 7.90 3.42 3.94 1.54 1.30 1.15 0.38 16.44 6.93 4.97 4.11 1.25
PCWNet[123] Sceneflow ✗ ✗ 50.72 31.59 24.19 20.28 8.59 43.15 22.96 16.02 12.45 3.25 6.13 2.58 1.78 1.49 0.48 28.83 9.63 5.19 3.73 1.18
PCWNet-vpp∗ Sceneflow ✗ ✓ 27.09 13.46 9.87 8.37 5.72 22.04 9.50 6.06 4.57 1.41 2.25 1.33 1.06 0.93 0.34 16.90 6.29 4.18 3.29 0.99
PCWNet[123] KITTI ✗ ✗ 55.80 38.31 31.52 27.85 12.65 45.76 27.13 20.03 16.35 4.30 15.47 2.78 1.36 1.01 0.58 20.70 5.44 2.65 1.77 0.83
PCWNet-vpp∗ KITTI ✗ ✓ 28.43 15.97 12.73 11.30 7.12 24.46 12.62 9.10 7.60 2.31 13.58 1.50 0.79 0.70 0.45 12.04 3.60 2.24 1.66 0.68
PCVNet[46] Sceneflow ✗ ✗ 29.16 20.57 17.07 14.81 7.01 23.59 13.00 9.63 7.77 2.20 4.21 2.07 1.40 1.05 0.41 24.46 7.99 4.61 3.35 1.06
PCVNet-vpp Sceneflow ✗ ✓ 10.75 7.35 6.16 5.47 2.76 7.64 4.31 3.24 2.64 0.72 1.38 0.86 0.68 0.56 0.21 15.86 6.16 4.24 3.32 0.94
DLNR[45] Middlebury ✗ ✗ 12.54 6.43 4.54 3.55 1.11 15.40 6.77 4.51 3.43 1.00 13.32 10.95 9.77 8.76 3.32 24.58 8.15 4.78 3.50 1.11
DLNR-vpp Middlebury ✗ ✓ 5.08 3.14 2.54 2.23 0.80 5.35 2.99 2.24 1.84 0.59 6.99 5.99 5.53 5.22 2.37 14.32 5.85 4.20 3.37 0.94
NMRF[124] Sceneflow ✗ ✗ 33.85 21.26 16.65 14.20 5.88 30.68 16.23 11.28 8.83 2.53 4.35 2.42 1.87 1.54 0.42 25.12 8.98 5.01 3.71 1.15
NMRF-vpp∗ Sceneflow ✗ ✓ 16.90 11.03 9.13 8.19 4.86 12.46 6.65 4.87 3.95 1.16 2.05 1.47 1.25 1.13 0.34 15.25 6.41 4.48 3.59 1.01
NMRF[124] KITTI ✗ ✗ 45.55 27.97 22.01 18.93 7.64 39.02 20.19 14.27 11.32 3.29 20.47 11.18 6.75 5.29 1.96 4.48 1.05 0.60 0.42 0.37
NMRF-vpp∗ KITTI ✗ ✓ 24.86 14.42 11.78 10.51 5.32 19.16 9.80 7.30 6.20 1.95 16.65 10.64 7.28 5.36 1.83 8.21 2.03 1.19 0.87 0.50
TABLE6:VPPwithoff-the-shelfnetworks.ResultsonMidd-14,Midd-21,ETH3DandKITTI.∗ usesα=0.2forblending.
Fig. 8: Analysis of outdoor datasets. We compare the density
of raw depth hints (a), the noisiness w.r.t ground-truth (b), and
disparity distribution (c) of three different datasets: KITTI [114],
DSEC[116],andM3ED[117]outdoorsequences.
Fig.7:StereonetworksDLNRandPCVNetwithandwithout
VPP.Inbothcases,VPPpreservesmostfinedetails,andthereisa
useVPPwithotherimage-guidedmethods,Tab.3reportsthere-
significant improvement in the presence of uniform regions (e.g.,
sultsofsuchjointdeploymentontheMidd-14.ExceptwithrSGM,
asevidentwiththeexteriorwhitewalls).
benefitingthejointdeploymentwithGuidedStereo,VPPaloneis
always more effective. Deploying VPP with LidarStereoNet also
yieldsslightimprovements.
volumes (Guided Stereo) and, not of little importance, allows for Results after Fine-Tuning on Real Data. Similarly to Tab.
aseamlessintegrationevenwithclosedsourcecode. 2,Tab.4reportsresultsafterfine-tuningPSMNetandLidarStere-
Depth Sparsity vs Accuracy. Fig. 6 plots the error rate (> oNet on Midd-14 for ∼ 4000 steps [49]. For RAFT-Stereo, we
2) on Midd-14 varying the density of sparse depth points from use the official Middlebury weights provided by the authors. We
0% to 5%. From it, we observe that VPP generally yields almost can observe a similar trend, with most VPP variants consistently
optimalperformancenearlywith1%orlessdensityand,withfew outperformingthosebasedonGuidedStereoandLidarStereoNet.
exceptions–the-trconfigurationswithhigherdensity–achieves This behaviour underlines how VPP improves cross-domain gen-
muchlowererrorrates. eralizationaswellasdomainspecialization.
VPP with other Guided Methods. Since we can seamlessly Comparison on KITTI. To prove the robustness of VPP
RNLD
ppv-RNLD
teNVCP
ppv-teNVCP10
DSECDay DSECAfternoon DSECNight
Model Modelname TrD aie npthPoin Tts est >1 E >rr 2orRate(% >) 3 >4 a (pv xg ). >1 >Err 2orRate(% >) 3 >4 a (pv xg ). >1 >Err 2orRate(% >) 3 >4 a (pv xg ).
rSGM[112] - ✗ ✗ 30.70 8.57 3.46 2.07 1.00 38.39 13.55 6.37 3.96 1.30 43.08 17.45 8.91 5.65 1.58
rSGM-vpp - ✗ ✓ 29.83 8.57 3.33 1.89 0.98 35.82 12.43 5.68 3.44 1.21 38.45 14.47 6.94 4.27 1.38
RAFT-Stereo[3] Sceneflow ✗ ✗ 25.68 6.90 3.25 2.26 0.93 27.83 9.46 4.64 3.06 1.06 37.96 16.21 9.24 6.34 1.53
RAFT-Stereo-vpp Sceneflow ✗ ✓ 22.86 6.00 2.99 2.13 0.86 24.28 8.08 4.06 2.73 0.96 30.31 12.46 7.00 4.80 1.25
RAFT-Stereo[3] Middlebury ✗ ✗ 25.51 6.20 2.79 2.06 0.93 23.65 7.66 4.10 3.00 1.03 30.32 12.42 7.71 5.97 1.57
RAFT-Stereo-vpp Middlebury ✗ ✓ 23.43 5.59 2.67 1.99 0.88 21.48 7.00 3.85 2.86 0.97 24.20 9.98 6.18 4.75 1.27
RAFT-Stereo[3] ETH3D ✗ ✗ 24.99 6.57 3.08 2.15 0.91 27.10 8.77 4.24 2.80 1.03 35.30 14.25 7.92 5.46 1.44
RAFT-Stereo-vpp ETH3D ✗ ✓ 22.31 5.76 2.86 2.05 0.85 23.51 7.55 3.77 2.55 0.94 27.79 11.01 6.18 4.30 1.18
PSMNet[4] Sceneflow ✗ ✗ 40.51 15.25 6.70 3.82 1.32 44.53 19.39 9.76 5.97 1.58 48.12 21.91 11.61 7.42 1.88
PSMNet-vpp Sceneflow ✗ ✓ 39.78 15.66 7.03 3.93 1.30 42.78 18.66 9.39 5.66 1.50 43.78 19.63 10.31 6.44 1.66
PSMNet[4] Middlebury ✗ ✗ 38.38 13.59 5.40 2.84 1.19 42.13 16.80 7.78 4.52 1.41 44.68 18.56 8.98 5.36 1.61
PSMNet-vpp Middlebury ✗ ✓ 37.22 13.61 5.54 2.89 1.18 40.22 16.16 7.52 4.35 1.36 41.35 17.29 8.52 5.10 1.49
GMStereo[120] Sceneflow ✗ ✗ 32.91 10.58 4.79 3.01 1.13 43.34 17.56 8.16 4.76 1.42 50.94 24.86 13.76 8.87 1.93
GMStereo-vpp∗ Sceneflow ✗ ✓ 30.15 9.51 4.43 2.86 1.07 39.70 15.52 7.34 4.43 1.34 45.76 21.12 11.69 7.61 1.75
GMStereo[120] Mixdata ✗ ✗ 35.00 10.44 4.62 3.13 1.18 33.22 12.45 6.69 4.67 1.30 37.56 14.26 7.27 4.77 1.40
GMStereo-vpp∗ Mixdata ✗ ✓ 31.59 9.31 4.42 3.06 1.13 32.03 11.97 6.50 4.58 1.29 34.92 13.43 7.08 4.77 1.40
CFNet[43] Sceneflow ✗ ✗ 29.37 8.42 3.77 2.48 1.02 41.49 16.24 7.80 4.83 1.44 55.00 31.54 22.22 18.23 26.79
CFNet-vpp∗ Sceneflow ✗ ✓ 27.95 8.27 3.71 2.43 0.99 38.24 14.96 7.26 4.51 1.36 45.46 23.41 15.45 12.22 13.05
CFNet[43] Middlebury ✗ ✗ 25.66 5.85 2.48 1.71 0.90 28.68 8.87 3.92 2.55 1.06 31.41 10.92 5.13 3.24 1.18
CFNet-vpp∗ Middlebury ✗ ✓ 24.53 5.87 2.56 1.77 0.89 26.88 8.55 4.00 2.68 1.04 28.95 10.46 5.25 3.48 1.19
HSMNet[31] Middlebury ✗ ✗ 32.13 9.40 4.07 2.52 1.04 36.08 12.80 5.93 3.56 1.20 40.44 15.84 7.98 4.98 1.43
HSMNet-vpp Middlebury ✗ ✓ 30.08 8.73 3.83 2.38 1.00 33.85 11.61 5.34 3.19 1.13 36.38 13.49 6.71 4.22 1.30
CREStereo[44] ETH3D ✗ ✗ 23.72 6.21 3.28 2.51 0.94 24.26 8.10 4.43 3.20 1.03 28.76 11.14 6.45 4.70 1.30
CREStereo-vpp∗ ETH3D ✗ ✓ 22.10 5.86 3.24 2.49 0.90 22.51 7.64 4.25 3.11 0.98 25.05 10.16 6.06 4.46 1.20
LEAStereo[38] Sceneflow ✗ ✗ 46.70 21.98 12.28 8.22 1.88 53.54 28.17 16.36 10.75 2.11 64.89 41.42 28.96 22.16 3.98
LEAStereo-vpp Sceneflow ✗ ✓ 39.99 16.55 8.13 5.05 1.42 46.65 22.53 12.26 7.85 1.76 51.04 27.93 17.62 12.81 2.59
LEAStereo[38] KITTI12 ✗ ✗ 40.33 14.56 6.92 4.30 1.44 43.61 17.07 7.74 4.43 1.50 49.29 23.26 13.16 8.94 2.25
LEAStereo-vpp KITTI12 ✗ ✓ 33.37 10.29 4.72 3.01 1.18 36.67 13.00 5.88 3.58 1.33 38.82 15.78 8.50 5.80 1.64
HITNet[32] Sceneflow ✗ ✗ 36.53 13.24 6.41 4.16 1.30 44.52 19.27 10.38 6.94 1.68 46.78 21.46 12.11 8.27 1.91
HITNet-vpp∗ Sceneflow ✗ ✓ 34.83 12.93 6.35 4.09 1.25 41.87 18.22 9.85 6.56 1.60 41.33 18.68 10.51 7.22 1.73
CoEx[121] Sceneflow ✗ ✗ 30.59 9.28 4.15 2.75 1.06 35.12 13.09 6.64 4.40 1.30 39.20 15.78 8.54 5.87 1.57
CoEx-vpp∗ Sceneflow ✗ ✓ 30.99 10.38 4.67 2.83 1.06 33.62 12.78 6.46 4.16 1.24 35.01 13.94 7.43 4.95 1.38
ELFNet[122] Sceneflow ✗ ✗ 45.42 23.60 14.33 9.93 2.14 48.35 25.96 16.00 11.11 2.07 58.49 36.09 25.11 19.24 3.56
ELFNet-vpp∗ Sceneflow ✗ ✓ 37.46 15.88 8.35 5.40 1.40 41.72 19.69 11.15 7.48 1.63 45.50 23.75 14.67 10.50 2.21
PCWNet[123] Sceneflow ✗ ✗ 42.95 16.33 7.96 4.99 1.45 50.21 22.83 11.80 7.25 1.76 55.93 30.04 18.43 13.08 2.72
PCWNet-vpp∗ Sceneflow ✗ ✓ 39.08 13.78 6.61 4.21 1.33 45.24 19.19 9.75 6.15 1.59 48.09 23.43 13.80 9.81 2.21
PCWNet[123] KITTI ✗ ✗ 32.73 10.01 4.46 2.83 1.16 38.26 13.33 5.67 3.40 1.34 42.00 16.00 7.43 4.46 1.49
PCWNet-vpp∗ KITTI ✗ ✓ 29.24 8.70 4.01 2.65 1.10 32.89 10.97 5.06 3.28 1.24 34.07 12.52 6.55 4.48 1.43
PCVNet[46] Sceneflow ✗ ✗ 26.42 7.07 3.20 2.19 0.93 27.65 9.07 4.34 2.84 1.03 35.93 15.20 8.72 6.11 1.69
PCVNet-vpp Sceneflow ✗ ✓ 23.98 6.07 2.82 1.97 0.87 24.36 7.72 3.77 2.54 0.95 28.51 11.23 6.24 4.35 1.23
DLNR[45] Middlebury ✗ ✗ 27.93 10.43 7.13 6.09 1.55 44.38 31.10 26.65 24.11 4.44 61.11 50.30 45.85 43.00 9.01
DLNR-vpp Middlebury ✗ ✓ 27.09 10.29 7.22 6.18 1.53 42.75 30.73 26.51 24.01 4.30 58.51 48.66 44.53 41.86 8.72
NMRF[124] Sceneflow ✗ ✗ 28.96 8.51 3.85 2.59 1.05 37.21 14.11 6.79 4.34 1.35 40.88 16.81 8.76 5.78 1.57
NMRF-vpp∗ Sceneflow ✗ ✓ 27.78 8.37 3.84 2.60 1.03 35.39 13.40 6.56 4.25 1.32 36.57 15.03 8.01 5.41 1.48
NMRF[124] KITTI ✗ ✗ 29.27 6.70 2.77 1.92 1.03 30.18 9.18 4.24 2.80 1.17 32.41 11.44 5.87 4.02 1.34
NMRF-vpp∗ KITTI ✗ ✓ 27.45 6.48 2.83 2.01 1.01 29.59 9.17 4.40 2.98 1.19 29.97 10.82 5.90 4.23 1.34
TABLE7:VPPwithmoreoff-the-shelfnetworks.ResultsonDSEC[116]using16-lineLiDAR.∗ usesα=0.2forblending.
RGB&VPP LEAStereo LEAStereo-vpp PCWNet PCWNet-vpp
ErrorRate>3:11.73% ErrorRate>3:2.55% ErrorRate>3:5.03% ErrorRate>3:1.46%
Fig. 9: Qualitative result on DSEC [116] night split. Passive stereo networks (e.g, LEAStereo [38] and PCWNet [123]) often fail
whendealingwithlowambientlight,whileVPPallowsperceivingevenchallengingobjects,suchasthefarbackgroundwalls.
with noisy depth data inferred with a real sensor outdoor and the same synthetic data: VPP guarantees a correct reconstruction
at long-range, where a physical pattern would be useless, Tab. ofuniformareaswhilepreservingfinedetails.
5 reports experimental results on the KITTI 2015 dataset using
the raw output of a Velodyne LiDAR. The results confirm the
4.6 AdditionalEvaluationwithRawDepthData
previoustrends:VPPconstantlyoutperformsGuidedStereo,with
In addition to the previous evaluation with raw data on KITTI
anynetworkandconfiguration,andLidarStereoNet.
2015,weextendourexperimentswithotherunfiltereddepthhints
sourced from off-the-shelf LiDAR sensors using two additional
4.5 VPPwithMoreOff-the-shelfNetworks
datasets:DSEC[116]andM3ED[117].
Tab.6collectsresultsobtaineddeployingVPPwithstate-of-the-art Analysis of Depth Hints. Fig. 8 plots various indicators of
stereomodels[3],[31],[32],[38],[43],[44],[120],[121],[122], thequalityofthesparsedepthseedsandthedatasetsthemselves:
[123], [46], [45], [124] taken off-the-shelf, running the weights we measure a) the density of sparse depth hints, b) the mean
providedbytheauthors.Again,VPPsensiblybooststheaccuracy absoluteerror ofsparsehintscompared toground-truthdata,and
of any model with rare exceptions, either trained on synthetic or c)thedisparitydistributionofstereoframes.Ina),asexpected,we
realdata.Fig.7showstheoutcomewithtwonetworkstrainedon appreciatehowdepthhintsprovidedinDSECareverysparsewith11
M3EDOutdoorDay M3EDOutdoorNight M3EDIndoor
Model Modelname TrD aie npthPoin Tts est >1 >Err 2orRate(% >) 3 >4 a (pv xg ). >1 >Err 2orRate(% >) 3 >4 a (pv xg ). >1 >Err 2orRate(% >) 3 >4 a (pv xg ).
rSGM[112] - ✗ ✗ 44.51 13.50 6.63 3.75 1.23 79.04 45.38 21.22 11.27 2.36 62.51 40.57 30.40 24.69 6.46
rSGM-vpp - ✗ ✓ 5.88 1.74 0.99 0.66 0.33 6.39 2.76 1.51 0.98 0.35 24.55 13.79 9.30 7.03 1.58
RAFT-Stereo[3] Sceneflow ✗ ✗ 43.04 8.87 4.94 2.69 1.14 89.44 39.75 14.42 11.29 13.36 52.74 29.96 21.70 16.92 3.06
RAFT-Stereo-vpp Sceneflow ✗ ✓ 6.97 1.95 1.15 0.78 0.36 5.90 1.76 0.75 0.41 0.28 26.27 15.24 10.78 8.38 1.99
RAFT-Stereo[3] Middlebury ✗ ✗ 41.08 9.12 5.28 3.18 1.65 88.90 35.94 10.70 7.24 4.02 50.87 28.04 19.99 15.53 3.11
RAFT-Stereo-vpp Middlebury ✗ ✓ 6.38 1.89 1.21 0.87 0.36 5.16 1.51 0.73 0.43 0.27 24.98 14.17 9.76 7.45 1.78
RAFT-Stereo[3] ETH3D ✗ ✗ 43.81 8.90 4.97 2.72 1.14 87.66 35.72 10.57 7.52 6.04 51.95 29.19 21.01 16.39 3.00
RAFT-Stereo-vpp ETH3D ✗ ✓ 6.94 1.97 1.15 0.78 0.36 5.75 1.69 0.71 0.39 0.27 26.12 15.20 10.74 8.35 2.37
PSMNet[4] Sceneflow ✗ ✗ 59.54 22.68 9.34 4.53 1.80 91.78 61.08 24.83 9.25 3.64 59.77 36.86 26.86 21.55 4.24
PSMNet-vpp Sceneflow ✗ ✓ 25.34 8.41 2.12 1.06 0.82 27.16 10.13 2.15 1.01 0.90 27.63 16.01 11.24 8.78 2.21
PSMNet[4] Middlebury ✗ ✗ 54.69 19.26 7.94 3.83 1.74 83.05 42.72 14.82 6.01 4.03 56.44 34.06 24.95 19.88 3.87
PSMNet-vpp Middlebury ✗ ✓ 23.97 8.89 2.03 1.04 0.86 25.50 10.22 1.87 0.97 1.06 27.80 16.17 11.33 8.83 2.22
GMStereo[120] Sceneflow ✗ ✗ 42.37 10.57 5.01 2.61 1.12 65.72 29.31 9.67 4.77 1.76 55.80 33.24 24.60 19.73 3.56
GMStereo-vpp∗ Sceneflow ✗ ✓ 8.70 2.42 1.26 0.77 0.47 10.04 2.81 1.25 0.65 0.43 29.67 17.26 11.95 9.01 1.80
GMStereo[120] Mixdata ✗ ✗ 33.55 7.99 4.50 2.33 1.01 81.00 21.99 4.87 2.13 1.67 48.34 27.69 19.92 15.30 2.78
GMStereo-vpp∗ Mixdata ✗ ✓ 8.72 2.61 1.39 0.85 0.44 10.04 3.16 1.40 0.69 0.41 26.67 15.10 10.26 7.71 1.62
CFNet[43] Sceneflow ✗ ✗ 46.04 10.84 6.21 3.91 2.26 91.02 63.93 47.12 44.85 76.28 58.71 36.75 28.06 23.41 11.56
CFNet-vpp∗ Sceneflow ✗ ✓ 7.85 2.21 1.35 0.97 0.46 12.09 7.77 6.87 6.50 5.03 26.91 15.62 10.81 8.33 2.90
CFNet[43] Middlebury ✗ ✗ 40.76 8.14 4.48 2.57 1.32 82.64 27.44 6.37 3.53 2.01 49.97 28.23 20.32 15.82 3.20
CFNet-vpp∗ Middlebury ✗ ✓ 6.03 1.56 0.98 0.72 0.43 5.12 1.74 1.12 0.86 0.35 23.93 13.45 9.09 6.87 1.49
HSMNet[31] Middlebury ✗ ✗ 39.38 11.28 5.26 2.64 1.11 74.93 32.11 9.67 3.06 1.73 57.39 34.30 23.72 18.22 3.41
HSMNet-vpp Middlebury ✗ ✓ 12.19 2.62 1.17 0.72 0.56 9.92 2.09 0.78 0.44 0.49 33.06 16.79 10.68 7.79 1.93
CREStereo[44] ETH3D ✗ ✗ 40.29 8.99 5.27 2.92 1.13 90.53 31.79 6.31 2.97 1.91 51.60 29.23 20.96 16.25 2.93
CREStereo-vpp∗ ETH3D ✗ ✓ 10.34 3.28 1.87 1.18 0.48 10.38 3.19 1.23 0.62 0.35 28.69 16.39 11.22 8.46 1.72
LEAStereo[38] Sceneflow ✗ ✗ 53.18 20.40 10.68 6.04 1.71 88.43 62.78 38.88 28.29 11.87 62.06 38.04 28.06 22.41 5.14
LEAStereo-vpp Sceneflow ✗ ✓ 6.05 1.78 1.09 0.77 0.38 5.53 2.44 1.42 0.90 0.45 25.16 13.81 9.15 6.86 1.47
LEAStereo[38] KITTI12 ✗ ✗ 40.57 12.84 7.34 4.56 2.13 75.45 49.43 32.11 25.00 15.88 56.51 34.05 24.28 18.87 4.04
LEAStereo-vpp KITTI12 ✗ ✓ 6.77 1.27 0.76 0.55 0.44 7.61 2.31 1.60 1.26 0.60 25.20 13.55 8.94 6.56 1.61
HITNet[32] Sceneflow ✗ ✗ 57.81 18.77 9.39 4.76 1.47 89.86 56.53 22.08 10.06 3.27 56.88 34.75 26.02 21.14 4.50
HITNet-vpp∗ Sceneflow ✗ ✓ 21.92 7.83 3.34 1.46 0.74 21.39 7.64 2.76 1.10 0.67 29.73 17.11 12.04 9.30 2.14
CoEx[121] Sceneflow ✗ ✗ 50.50 17.44 7.27 3.58 1.37 87.22 53.00 22.21 8.86 4.29 56.64 33.90 24.81 19.76 4.88
CoEx-vpp∗ Sceneflow ✗ ✓ 22.39 6.46 2.31 1.26 0.76 20.93 5.95 1.95 0.81 0.74 31.08 17.14 11.85 9.16 2.30
ELFNet[122] Sceneflow ✗ ✗ 60.09 23.93 11.59 5.83 1.93 90.65 69.69 43.46 28.40 15.69 57.11 36.44 28.74 24.12 6.96
ELFNet-vpp∗ Sceneflow ✗ ✓ 16.25 3.79 1.85 1.33 0.65 14.75 3.93 2.10 1.68 1.66 27.72 16.00 11.66 9.41 2.87
PCWNet[123] Sceneflow ✗ ✗ 49.14 11.03 5.63 3.28 1.31 88.83 48.30 19.37 10.17 2.43 55.94 33.07 23.96 18.82 3.70
PCWNet-vpp∗ Sceneflow ✗ ✓ 10.48 2.47 1.43 1.02 0.56 8.99 1.90 0.82 0.49 0.38 27.86 14.93 9.90 7.35 1.60
PCWNet[123] KITTI ✗ ✗ 33.36 8.27 5.37 3.39 1.76 63.67 20.20 7.20 4.83 2.18 52.86 29.49 20.45 15.61 3.23
PCWNet-vpp∗ KITTI ✗ ✓ 14.03 2.51 1.65 1.32 0.79 14.80 1.80 0.81 0.56 0.51 24.69 12.87 8.53 6.29 1.54
PCVNet[46] Sceneflow ✗ ✗ 42.84 10.38 5.74 3.32 1.25 86.96 45.19 24.00 19.15 13.25 55.67 34.29 25.84 20.91 6.19
PCVNet-vpp Sceneflow ✗ ✓ 8.55 2.33 1.31 0.88 0.44 8.57 2.53 1.53 1.17 0.73 28.20 16.48 11.55 8.90 2.76
DLNR[45] Middlebury ✗ ✗ 54.04 26.19 21.82 18.72 6.63 90.96 76.47 68.15 65.46 38.39 53.64 32.57 24.26 19.43 3.99
DLNR-vpp Middlebury ✗ ✓ 23.03 17.81 15.82 14.35 4.75 48.85 44.29 41.70 39.75 19.73 28.06 17.12 12.34 9.80 2.26
NMRF[124] Sceneflow ✗ ✗ 42.58 9.23 4.96 2.69 1.15 84.26 37.34 11.22 6.16 2.12 53.01 30.77 22.48 17.92 3.28
NMRF-vpp∗ Sceneflow ✗ ✓ 10.17 2.70 1.53 1.01 0.50 10.71 2.69 1.18 0.72 0.49 28.98 16.67 11.61 8.90 1.85
NMRF[124] KITTI ✗ ✗ 40.19 20.30 14.90 11.63 5.36 73.95 34.72 22.21 17.83 7.22 48.20 27.07 19.69 15.36 3.12
NMRF-vpp∗ KITTI ✗ ✓ 22.89 13.95 10.10 7.94 3.37 23.56 14.02 10.10 7.10 2.38 26.18 14.43 9.83 7.39 1.57
TABLE8:VPPwithmoreoff-the-shelfnetworks.ResultsonM3ED[117]using64-lineLiDAR.∗ usesα=0.2forblending.
Grayscale&VPP ELFNet ELFNet-vpp HITNet HITNet-vpp
ErrorRate>3:30.47% ErrorRate>3:7.40% ErrorRate>3:30.84% ErrorRate>3:8.54%
Fig. 10: Qualitative result on M3ED [117] day split. Both ELFNet [122] and HITNet [32] struggle with large texture-less areas,
forexample,createdbysceneswithhighdynamicrange.Ourframeworkcanleveragesparsedepthpointstosolvethisissue,whereas
projectedpatternswouldbeineffective.
–amediandensityof0.16%–whileotherdatasetscollectedwith – Day, Afternoon, Night –, from easier (i.e, Day) to harder (i.e,
64-line LiDAR achieve a median of 1.80 and 4.00% respectively Night)lightconditions.Evenifthedepthhintsdensityisextremely
(becauseofthedifferentimageresolution)beneficialtotheeffec- low, VPP improves the accuracy of stereo networks in nearly
tivenessofVPPandotherfusionframeworksashighlightedinFig. all cases: the gain in accuracy is less appreciable in Day split
6.Interestingly,inb),wefoundanotabledifferenceinthequality –nonetheless,>1errorrateisdecreasedover2%inalmostcases
of raw depth hints across the datasets, ranging from a median –incontrast,aslightconditionsaregettingworse(i.e,Afternoon,
errorof0.1mforM3EDtoamedianof0.3mforKITTI2015.In Night), our framework can reduce the very same error rate over
theremainder,ourexperimentswillconfirmthatahigherdensity 5%,withsomecasesexciding10%.Fig.9exhibitsachallenging
and depth quality translates into a higher performance boost. frame from Night split and predictions from LEAStereo [38]
Finally,c)exposesthesignificantlydifferentdisparitydistributions and PCWNet [123] stereo networks: even with very few virtual
characterizing M3ED with respect to KITTI and DSEC, mainly patches, our solution allows to detect challenging obstacles such
duetoitsshorterbaseline. asthebarelyvisiblewallinthemiddle,thatotherwisewouldhave
beenmissedprocessingthevanillastereopair.
Results on DSEC.Tab.7reportstheperformancebyoff-the-
shelfnetworksandrSGMalgorithmwithinthethreeDSECsplits ResultsonM3ED.Tab.8reportstheaccuracybyoff-the-shelf12
VanillaImages ActiveStereo VPPStereo
ErrorRate(%) avg. ErrorRate(%) avg. ErrorRate(%) avg.
Model
Modelname >1 >2 >3 >4 (px) >1 >2 >3 >4 (px) >1 >2 >3 >4 (px)
rSGM[112] - 39.88 26.79 20.04 15.87 2.88 11.32 6.51 5.14 4.45 1.11 7.65 4.44 3.48 3.00 0.83
RAFT-Stereo[3] Sceneflow 29.18 19.82 14.13 10.60 1.61 7.83 3.96 2.76 2.18 0.51 4.12 2.34 1.70 1.37 0.44
RAFT-Stereo[3] Middlebury 25.82 16.57 11.30 8.31 1.51 6.27 3.82 2.95 2.49 0.65 4.20 2.34 1.67 1.35 0.48
RAFT-Stereo[3] ETH3D 28.16 18.78 13.73 10.83 1.68 7.64 3.82 2.68 2.13 0.50 4.08 2.33 1.70 1.37 0.45
PSMNet[4] Sceneflow 30.46 19.53 14.69 12.06 2.53 8.15 3.62 2.47 1.94 0.67 7.19 2.92 1.96 1.55 0.60
PSMNet[4] Middlebury 29.41 17.74 12.49 9.61 2.03 8.23 3.70 2.59 2.07 0.73 7.76 3.15 2.06 1.62 0.58
GMStereo∗[120] Sceneflow 33.34 21.24 14.83 10.82 1.67 7.22 3.24 2.21 1.71 0.55 4.43 2.18 1.48 1.15 0.45
GMStereo∗[120] Mixdata 22.29 11.98 7.48 5.11 1.08 5.53 2.46 1.67 1.28 0.54 6.43 1.88 1.21 0.90 0.55
CFNet∗[43] Sceneflow 30.20 20.04 14.53 11.35 1.84 10.88 8.02 6.95 6.36 2.78 5.37 3.16 2.37 1.93 0.60
CFNet∗[43] Middlebury 27.57 16.76 11.39 8.35 1.42 6.34 3.87 2.94 2.44 0.58 5.42 3.29 2.49 2.04 0.59
HSMNet[31] Middlebury 29.90 15.73 10.61 7.80 1.44 17.82 7.08 4.27 3.08 0.92 13.79 5.46 3.33 2.41 0.77
CREStereo∗[44] ETH3D 23.16 14.40 9.31 6.20 1.07 4.14 2.38 1.74 1.41 0.35 3.34 1.88 1.35 1.07 0.36
LEAStereo[38] Sceneflow 33.40 21.44 16.20 13.20 2.90 11.82 7.16 5.70 5.03 1.32 8.48 5.54 4.70 4.29 1.21
LEAStereo[38] KITTI12 37.13 25.26 20.09 17.40 3.75 10.79 6.64 5.40 4.72 0.99 9.60 5.63 4.50 3.93 0.97
HITNet∗[32] Sceneflow 27.20 18.16 14.03 11.34 1.82 8.55 4.38 3.21 2.62 0.67 5.85 3.00 2.14 1.74 0.49
CoEx∗[121] Sceneflow 29.17 17.43 12.66 10.03 1.98 15.76 7.82 5.55 4.49 1.15 8.80 3.39 2.19 1.67 0.60
ELFNet∗[122] Sceneflow 36.38 26.42 21.92 19.25 4.80 14.65 9.97 8.15 7.20 1.78 9.46 6.64 5.67 5.13 1.34
PCWNet∗[123] Sceneflow 26.51 16.95 12.94 10.29 1.97 11.12 6.31 4.89 4.18 1.07 5.37 2.56 1.79 1.44 0.50
PCWNet∗[123] KITTI 31.57 21.88 17.99 15.69 3.04 9.62 6.38 5.27 4.64 0.95 8.73 5.49 4.08 3.42 0.84
PCVNet[46] Sceneflow 34.09 24.32 18.48 14.72 2.04 8.00 4.14 2.87 2.20 0.55 4.99 2.62 1.83 1.44 0.47
DLNR[45] Middlebury 28.19 19.37 14.58 11.83 2.57 27.44 24.58 23.07 21.93 8.06 5.10 2.99 2.13 1.74 0.55
NMRF∗[124] Sceneflow 29.84 20.31 14.99 11.34 1.53 5.73 2.88 2.08 1.71 0.43 4.03 2.27 1.64 1.34 0.42
NMRF∗[124] KITTI 28.58 18.85 14.21 11.58 2.28 8.07 5.34 4.45 4.00 0.97 6.76 4.59 3.85 3.45 1.09
TABLE 9: VPP versus Active Stereo. Results on SIMSTEREO [118] dataset sampling 2% of depth points from simulated noisy
ground-truth[119].∗ usesα=0.2forblending.
Active VPP DLNR DLNR-active DLNR-vpp
ErrorRate>1:14.90% ErrorRate>1:10.80% ErrorRate>1:5.06%
Fig.11:QualitativeresultonSIMSTEREO[118].Somenetworks,suchasDLNR[45],oftenproduceartefactswhenfedwithstereo
pairsacquiredwithpatternprojectionactive.Incontrast,thesamenetworkscanseamlesslytakeadvantageofourvirtualpatterns.
networksandrSGMalgorithmwithintwooutdoorsplitsfeaturing confirmtheprevioustrend,decreasingthe>1errorratebymore
differentlightconditions(i.e,Day,Night)andtheIndoor splitin than 20% in all cases. Fig. 10 plots a frame from Outdoor Day
M3ED.Ontheonehand,M3EDisslightlymorechallengingthan with the predictions by ELFNet [122] and HITNet [32] stereo
DSEC due to a smaller baseline and open scenes without street networks alone or when assisted by VPP, clearly confirming the
lights. On the other hand, the depth hints are less noisy and 10 benefitsyieldedbyourmethodinlow-texturedregions.
times denser than in the DSEC dataset; as a result, the overall
improvement given by our proposal is an order of magnitude 4.7 ComparisonwithActiveStereo
higher than in DSEC. Specifically, although M3ED is a very
Toconclude,weembarkonaquantitativecomparisonbetweenthe
challenging dataset – i.e, > 1 error rate is, in most cases, over
effectiveness of VPP against the traditional active stereo setup –
40% for Outdoor Day split and over 70% for Outdoor Night
typicallycomposedofanIRstereocameraandanIRprojector–
split – when stereo networks and rSGM algorithm are used in
usingonesyntheticdataset[118]andtheM3ED-active[117]split.
combination with VPP we observe an outstanding improvement
Results on SIMSTEREO. Tab. 9 collects our findings in an
inanymetrics:allmodelsbenefitfromourframework,with> 1
idealscenariowhereIRandRGBcamerasareperfectlyaligned–
error rate reduction respectively over 30% for Outdoor Day split
i.e, RGB and IR images frame the very same scene, for which
and 50% for Outdoor Night split. Also, the Indoor split results
a dense, synthetic ground-truth disparity map is provided: as13
VanillaImages ActiveStereo VPPStereo
ErrorRate(%) avg. ErrorRate(%) avg. ErrorRate(%) avg.
Model
Modelname >1 >2 >3 >4 (px) >1 >2 >3 >4 (px) >1 >2 >3 >4 (px)
rSGM[112] - 73.13 57.74 49.52 44.00 15.36 64.10 44.68 34.27 27.80 7.77 33.69 18.82 12.06 9.09 2.38
RAFT-Stereo[3] Sceneflow 63.31 44.48 34.25 27.21 4.69 54.89 36.34 26.18 19.63 3.50 34.92 20.54 13.53 10.36 2.18
RAFT-Stereo[3] Middlebury 59.29 39.70 29.54 23.06 4.49 52.26 34.21 24.16 17.70 3.30 33.76 19.23 12.35 9.38 2.10
RAFT-Stereo[3] ETH3D 59.78 41.32 32.11 25.79 4.65 53.90 35.82 25.85 19.35 3.47 34.65 20.29 13.34 10.21 2.18
PSMNet[4] Sceneflow 69.28 52.23 43.09 36.70 8.33 57.58 39.19 28.98 22.37 4.37 35.87 20.92 13.82 10.45 2.61
PSMNet[4] Middlebury 67.99 50.97 41.78 35.47 7.38 55.58 37.49 27.31 20.82 4.24 36.53 21.78 14.47 10.87 2.65
GMStereo∗[120] Sceneflow 66.81 50.59 41.90 35.69 7.10 55.50 37.46 27.63 21.75 4.05 36.96 21.77 14.44 11.09 2.37
GMStereo∗[120] Mixdata 58.58 40.71 30.50 23.73 4.21 50.18 33.45 23.72 17.56 3.28 34.65 19.45 12.57 9.62 2.12
CFNet∗[43] Sceneflow 68.13 51.38 43.01 37.45 15.91 60.51 42.21 32.69 26.64 8.14 35.26 20.22 13.09 9.79 2.20
CFNet∗[43] Middlebury 63.30 46.35 36.69 29.93 5.62 51.72 33.73 23.55 17.06 3.19 33.10 18.55 11.90 9.00 2.03
HSMNet[31] Middlebury 67.60 49.39 38.60 31.35 5.48 57.69 38.15 27.00 20.10 4.02 40.83 22.73 14.23 10.31 2.55
CREStereo∗[44] ETH3D 63.18 44.57 34.00 26.92 4.38 52.78 34.65 24.54 18.06 3.37 35.76 20.63 13.10 9.68 2.14
LEAStereo[38] Sceneflow 70.74 55.10 46.96 41.29 12.60 64.25 43.72 32.52 25.77 5.50 33.24 17.92 11.10 8.26 1.99
LEAStereo[38] KITTI12 67.58 51.18 42.46 36.68 10.16 56.76 37.77 27.21 20.51 3.99 34.57 19.45 12.20 8.84 2.14
HITNet∗[32] Sceneflow 68.03 51.94 43.60 37.62 10.72 56.38 38.39 28.35 21.65 3.89 37.04 21.44 14.04 10.65 2.41
CoEx∗[121] Sceneflow 66.40 49.61 40.94 34.93 12.36 55.83 37.78 27.23 20.50 3.78 37.91 21.34 13.99 10.75 2.46
ELFNet∗[122] Sceneflow 68.21 53.20 45.48 39.95 13.65 59.25 42.26 33.12 27.48 8.15 36.37 21.37 15.07 12.36 4.41
PCWNet∗[123] Sceneflow 66.75 50.53 42.01 35.51 7.49 56.45 37.86 27.89 21.18 3.69 35.69 19.58 12.23 9.22 2.11
PCWNet∗[123] KITTI 65.56 47.77 38.33 31.99 7.23 54.20 34.65 23.67 16.61 3.12 33.23 16.97 10.50 7.84 1.90
PCVNet[46] Sceneflow 66.36 50.55 41.60 35.61 17.40 56.58 39.00 28.88 22.35 4.13 36.73 21.66 14.20 10.74 2.34
DLNR[45] Middlebury 57.44 39.14 28.87 22.12 4.05 53.09 35.83 25.80 19.13 3.53 35.14 20.15 12.73 9.52 2.17
NMRF∗[124] Sceneflow 63.03 45.08 35.59 28.87 5.16 54.34 36.31 26.36 20.23 3.91 36.04 21.09 14.00 10.76 2.30
NMRF∗[124] KITTI 61.86 45.93 37.32 31.46 6.99 51.14 33.21 23.45 17.29 3.28 34.99 19.24 12.43 9.40 2.11
TABLE10:VPPversusActiveStereo.ResultsonM3ED-active[117]splitusingfrom64-lineLiDAR.∗ usesα=0.2forblending.
Active VPP RAFT-Stereo RAFT-Stereo-active RAFT-Stereo-vpp
ErrorRate>3:47.68% ErrorRate>3:21.61% ErrorRate>3:13.93%
Fig.12:Qualitative result on M3ED-active [117] .Virtualandphysicalprojectedpatternshelptoimprovetheaccuracyofarecent
stereonetwork(i.e,RAFT-Stereo[3])whendealingwithlargeuniformareas.
expected, the IR active pattern is beneficial in reducing error by predictions using DLNR [45] stereo network. Feeding the
metrics,withonlyafewexceptions–CFNet[43]andDLNR[45]; networkwithtraditionallypatternedimagesreducesthe>1error
thisfindingisconsistentwiththosebytheauthorsofSIMSTEREO compared to passive stereo yet introduces some notable artifacts.
paper [118], where they propose to adapt networks to the active In contrast, feeding the same network with virtually patterned
pattern via fine-tuning. In contrast, our framework can be tuned frames,mostartifactsdisappearandtheerrormetricismorethan
(e.g,usingadifferentamountofalpha-blending)tothesenetworks halvedcomparedtoactivestereo.
withoutanyadditionaltraining.
ResultsonM3ED-active.Tofurthersupportourfindingseven
Furthermore,ourproposaladdressesdifferentissuesregarding on real data, we extend the previous experiment to the Passive
active stereo [81]: firstly, traditional active stereo systems use IR and Active splits from M3ED [117]. We recall that, although
spectrumtonotinterferewithhumanvisionandconsequentlyre- the images on the two splits do not coincide exactly – i.e., the
quireanadditionalRGBcameratocapturevisiblespectrum,while activepatternisvisibleonlyonceeverytwoframes,andtherobot
VPP can directly exploit RGB spectrum to create more complex is moving during acquisition – we assume a negligible motion
patterns; secondly, the IR projected pattern fades proportionally occurs between adjacent passive/active frames, because of the
to the square of the distance and it suffers from strong ambient very short time delay between the two: accordingly, we feel a
lights,whereasourproposalcompletelybypassestheseissues.In direct comparison between methods running on the two distinct
addition to these advantages, our framework generally performs splits is fair enough, although not perfect. Tab. 10 highlights that
betterthanactivestereo,sometimeshalvingtheerrormetrics.Fig. our framework surpasses traditional active pattering in all tested
11visuallyshowsthedifferencebetweentraditionalIRprojected cases, halving all error metrics in most experiments. Although
patterns and the virtual patterns on the same frame, followed activestereoalreadyhalvesmostoftheerrormetricswithrespect14
to passive stereo, VPP confirms its undisputed superiority with [7] J. Geng, “Structured-light 3d surface imaging: a tutorial,” Adv. Opt.
dramatically larger improvements. Fig 12 shows a qualitative Photon., vol. 3, no. 2, pp. 128–160, Jun 2011. [Online]. Available:
https://opg.optica.org/aop/abstract.cfm?URI=aop-3-2-128 1,3
example with a large uniform area, texturized both by physically
[8] L.Keselman,J.IselinWoodfill,A.Grunnet-Jepsen,andA.Bhowmik,
projected patterns (fisrt column) or VPP (second column): not
“Intel realsense stereoscopic depth cameras,” in IEEE Conference on
surprisingly,passivestereoalmostfailsevenwithRAFT-Stereo[3], Computer Vision and Pattern Recognition (CVPR) Workshops, July
whereasbothpatternshelptoreconstructthewallscorrectly,with 2017. 1,2,3
[9] M. Poggi, D. Pallotti, F. Tosi, and S. Mattoccia, “Guided stereo
VPPresultingtheabsolutewinnerbyamassivemargin.
matching,”inIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),2019,pp.979–988. 2,3,6,7,8
[10] X.Cheng,Y.Zhong,Y.Dai,P.Ji,andH.Li,“Noise-awareunsupervised
5 LIMITATIONS AND FAILURE CASES deep lidar-stereo fusion,” in IEEE Conference on Computer Vision
PatternRecognition(CVPR),2019. 2,3,6,7,8
Our evaluation does not reveal noticeable failure cases or lim-
[11] T.-H. Wang, H.-N. Hu, C. H. Lin, Y.-H. Tsai, W.-C. Chiu, and
itations. However, selecting the correct device to infer robust M. Sun, “3d lidar and stereo fusion using stereo matching network
depth hints in the desired target domains is crucial, especially withconditionalcostvolumenormalization,”inIEEE/RSJInternational
for safety-critical applications. In the worst-case scenario, if the ConferenceonIntelligentRobotsandSystems(IROS),2019,pp.5895–
5902. 2,3,7,8
depthsensorfails–e.g.,duetoextremeenvironmentalconditions,
[12] L.Bartolomei,M.Poggi,F.Tosi,A.Conti,andS.Mattoccia,“Active
challengingobject/material,oranyotherreasonyieldingsporadic stereo without pattern projector,” in IEEE/CVF International Confer-
or continuous breakdown of the deployed sensing technology – enceonComputerVision(ICCV),October2023,pp.18470–18482. 2,
6,7
theperceptionmodulewouldperformidenticallyastheunderlying
[13] K.Konolige,“Projectedtexturestereo,”inIEEEInternationalConfer-
stereomatcher,likeapassivesystemwithoutbenefitingfromvir-
enceonRoboticsandAutomation(ICRA),2010,pp.148–155. 2,3
tualpatterns.Finally,thesparsedepthhintsshouldbespreadover [14] D. Scharstein and R. Szeliski, “A taxonomy and evaluation of dense
thewholeimageforoptimalperformance,asforanyotherfusion two-frame stereo correspondence algorithms,” Internation Journal of
method.Nonetheless,astheexperimentalanalysishighlights,our ComputerVision,vol.47,no.1-3,pp.7–42,2002. 2
[15] R. Zabih and J. Woodfill, “Non-parametric local transforms for com-
proposal can seamlessly deal with very sparse, fluctuating, and
puting visual correspondence,” in European Conference on Computer
noisydepthhints. Vision(ECCV),1994,pp.151–158. 2
[16] O. Veksler, “Stereo correspondence by dynamic programming on a
tree,”inIEEEConferenceonComputerVisionandPatternRecognition
6 CONCLUSION (CVPR),vol.2,2005,pp.384–390. 2
[17] Q. Yang, L. Wang, R. Yang, H. Stewe´nius, and D. Niste´r, “Stereo
This paper proposes a novel paradigm for combining stereo with matchingwithcolor-weightedcorrelation,hierarchicalbeliefpropaga-
sparse depth hints. Inspired by active stereo but unaffected by its tion,andocclusionhandling,”IEEETransactionsonPatternAnalysis
andMachineIntelligence,vol.31,no.3,pp.492–504,2008. 2
inherent severe limitations, our Virtual Pattern Projection (VPP)
[18] Q.Yang,L.Wang,andN.Ahuja,“Aconstant-spacebeliefpropagation
exploitssuchdepthhintstogeneratecoherentlyhallucinatedstereo
algorithmforstereomatching,”inIEEEConferenceonComputerVision
pairs to facilitate visual correspondence. Extensive experiments andPatternRecognition(CVPR),2010,pp.1458–1465. 2
on several standard stereo datasets prove that VPP outperforms [19] C.-K. Liang, C.-C. Cheng, Y.-C. Lai, L.-G. Chen, and H. H. Chen,
“Hardware-efficientbeliefpropagation,”IEEETransactionsonCircuits
existing stereo-depth fusion techniques, works seamlessly with
andSystemsforVideoTechnology,vol.21,no.5,pp.525–537,2011.2
any state-of-the-art deep stereo networks and algorithm, and is
[20] T.Taniai,Y.Matsushita,andT.Naemura,“Graphcutbasedcontinuous
more robust and effective than conventional active stereo tech- stereo matching using locally shared labels,” in IEEE Conference on
nologies. Additionally, it remarkably increases the cross-domain Computer Vision and Pattern Recognition (CVPR), 2014, pp. 1613–
1620. 2
generalization ability of deep stereo networks. In light of this
[21] V.KolmogorovandR.Zabin,“Whatenergyfunctionscanbeminimized
evidence, our paradigm has the potential to become a standard
viagraphcuts?”IEEETransactionsonPatternAnalysisandMachine
component in the perception pipelines of several applications, Intelligence,vol.26,no.2,pp.147–159,2004. 2
rangingfromautonomousdrivingtorobotics. [22] H.Hirschmuller,“Stereoprocessingbysemiglobalmatchingandmu-
tualinformation,”IEEETransactionsonPatternAnalysisandMachine
Intelligence,vol.30,no.2,pp.328–341,2007. 2,5,6
REFERENCES [23] Y. Boykov, O. Veksler, and R. Zabih, “Fast approximate energy min-
imizationviagraphcuts,”IEEETransactionsonPatternAnalysisand
MachineIntelligence,vol.23,no.11,pp.1222–1239,2001. 2
[1] M.Poggi,F.Tosi,K.Batsos,P.Mordohai,andS.Mattoccia,“Onthe
synergies between machine learning and binocular stereo for depth [24] J.Zbontar,Y.LeCunetal.,“Stereomatchingbytrainingaconvolutional
estimation from images: A survey,” IEEE Transactions on Pattern neural network to compare image patches.” J. Mach. Learn. Res.,
AnalysisandMachineIntelligence,vol.44,no.9,pp.5314–5334,2022. vol.17,no.1,pp.2287–2318,2016. 2
1,2 [25] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy,
[2] H. Laga, L. V. Jospin, F. Boussaid, and M. Bennamoun, “A survey and T. Brox, “A large dataset to train convolutional networks for
ondeeplearningtechniquesforstereo-baseddepthestimation,”IEEE disparity,opticalflow,andsceneflowestimation,”inIEEEConference
Transactions on Pattern Analysis and Machine Intelligence, vol. 44, onComputerVisionandPatternRecognition(CVPR),June2016. 2,6,
no.4,pp.1738–1764,2022. 1,2 7,8
[3] L.Lipson,Z.Teed,andJ.Deng,“Raft-stereo:Multilevelrecurrentfield [26] J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan, “Cascade residual
transforms for stereo matching,” in International Conference on 3D learning: A two-stage convolutional neural network for stereo match-
Vision(3DV),2021. 2,3,6,7,8,9,10,11,12,13,14 ing,” in IEEE International Conference on Computer Vision (ICCV),
[4] J.-R. Chang and Y.-S. Chen, “Pyramid stereo matching network,” in Oct2017. 2
IEEE/CVF Conference on Computer Vision and Pattern Recognition [27] Z. Liang, Y. Feng, Y. Guo, H. Liu, W. Chen, L. Qiao, L. Zhou, and
(CVPR),2018,pp.5410–5418. 2,3,6,7,8,10,11,12,13 J.Zhang,“Learningfordisparityestimationthroughfeatureconstancy,”
[5] X. Qiao, M. Poggi, P. Deng, H. Wei, C. Ge, and S. Mattoccia, “Rgb in IEEE Conference on Computer Vision and Pattern Recognition
guidedtofimagingsystem:Asurveyofdeeplearning-basedmethods,” (CVPR),June2018. 2
InternationalJournalonComputerVision,2024. 1 [28] T.Saikia,Y.Marrakchi,A.Zela,F.Hutter,andT.Brox,“Autodispnet:
[6] J.-T.Lin,D.Dai,andL.VanGool,“Depthestimationfrommonocular Improving disparity estimation with automl,” in IEEE/CVF Interna-
imagesandsparseradardata,”inInternationalConferenceonIntelligent tionalConferenceonComputerVision(ICCV),2019,pp.1812–1823.
RobotsandSystems(IROS),2020. 1 215
[29] X. Song, X. Zhao, H. Hu, and L. Fang, “Edgestereo: A context [49] Z. Teed and J. Deng, “Raft: Recurrent all-pairs field transforms for
integrated residual pyramid network for stereo matching,” in Asian optical flow,” in European Conference on Computer Vision (ECCV),
ConferenceonComputerVision(ACCV),2018. 2 2020,pp.402–419. 3,9
[30] G.Yang,H.Zhao,J.Shi,Z.Deng,andJ.Jia,“Segstereo:Exploiting [50] Z. Li, X. Liu, N. Drenkow, A. Ding, F. X. Creighton, R. H. Taylor,
semanticinformationfordisparityestimation,”inEuropeanConference andM.Unberath,“Revisitingstereodepthestimationfromasequence-
onComputerVision(ECCV),2018,pp.636–651. 2 to-sequenceperspectivewithtransformers,”inIEEE/CVFInternational
[31] Z. Yin, T. Darrell, and F. Yu, “Hierarchical discrete distribution de- ConferenceonComputerVision(ICCV),2021,pp.6197–6206. 3
composition for match density estimation,” in IEEE/CVF Conference [51] W.Guo,Z.Li,Y.Yang,Z.Wang,R.H.Taylor,M.Unberath,A.Yuille,
onComputerVisionandPatternRecognition(CVPR),2019,pp.6044– andY.Li,“Context-enhancedstereotransformer,”inEuropeanConfer-
6053. 2,9,10,11,12,13 enceonComputerVision(ECCV),2022. 3
[32] V. Tankovich, C. Hane, Y. Zhang, A. Kowdle, S. Fanello, and [52] F.Zhang,X.Qi,R.Yang,V.Prisacariu,B.Wah,andP.Torr,“Domain-
S. Bouaziz, “Hitnet: Hierarchical iterative tile refinement network for invariantstereomatchingnetworks,”inEuropeanConferenceonCom-
real-time stereo matching,” in IEEE/CVF Conference on Computer puterVision(ECCV),2020. 3
VisionandPatternRecognition(CVPR),June2021,pp.14362–14372. [53] C. Cai, M. Poggi, S. Mattoccia, and P. Mordohai, “Matching-space
2,9,10,11,12,13 stereonetworksforcross-domaingeneralization,”in2020International
[33] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional net- Conferenceon3DVision(3DV),2020,pp.364–373. 3
worksforbiomedicalimagesegmentation,”inMedicalImageComput- [54] F.Aleotti,F.Tosi,P.ZamaRamirez,M.Poggi,S.Salti,L.DiStefano,
ingandComputer-AssistedIntervention(MICCAI),2015,pp.234–241. andS.Mattoccia,“Neuraldisparityrefinementforarbitraryresolution
2 stereo,”inInternationalConferenceon3DVision(3DV),2021. 3
[34] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, [55] J. Zhang, X. Wang, X. Bai, C. Wang, L. Huang, Y. Chen, L. Gu,
A. Bachrach, and A. Bry, “End-to-end learning of geometry and J.Zhou,T.Harada,andE.R.Hancock,“Revisitingdomaingeneralized
contextfordeepstereoregression,”inIEEEInternationalConference stereo matching networks from a feature consistency perspective,” in
onComputerVision(ICCV),Oct2017. 3 IEEE/CVF Conference on Computer Vision and Pattern Recognition
[35] S. Khamis, S. Fanello, C. Rhemann, A. Kowdle, J. Valentin, and (CVPR),2022,pp.13001–13011. 3
S.Izadi,“Stereonet:Guidedhierarchicalrefinementforreal-timeedge- [56] B. Liu, H. Yu, and G. Qi, “Graftnet: Towards domain generalized
awaredepthprediction,”inEuropeanConferenceonComputerVision stereo matching with a broad-spectrum and task-oriented feature,” in
(ECCV),2018,pp.573–590. 3 IEEE/CVF Conference on Computer Vision and Pattern Recognition
[36] F.Zhang,V.Prisacariu,R.Yang,andP.H.Torr,“GA-Net:Guidedag- (CVPR),2022,pp.13012–13021. 3
gregationnetforend-to-endstereomatching,”inIEEE/CVFConference [57] W. Chuah, R. Tennakoon, R. Hoseinnezhad, A. Bab-Hadiashar, and
onComputerVisionandPatternRecognition(CVPR),2019. 3 D.Suter,“Itsa:Aninformation-theoreticapproachtoautomaticshortcut
[37] X.Cheng,P.Wang,andR.Yang,“Learningdepthwithconvolutional avoidanceanddomaingeneralizationinstereomatchingnetworks,”in
spatial propagation network,” IEEE Transactions on Pattern Analysis IEEE/CVF Conference on Computer Vision and Pattern Recognition
andMachineIntelligence,vol.42,no.10,pp.2361–2379,2019. 3 (CVPR),2022,pp.13022–13032. 3
[38] X.Cheng,Y.Zhong,M.Harandi,Y.Dai,X.Chang,H.Li,T.Drum- [58] J. Watson, O. M. Aodha, D. Turmukhambetov, G. J. Brostow, and
mond, and Z. Ge, “Hierarchical neural architecture search for deep M. Firman, “Learning stereo from single images,” in European Con-
stereomatching,”AdvancesinNeuralInformationProcessingSystems, ferenceonComputerVision(ECCV),2020. 3
vol.33,2020. 3,9,10,11,12,13 [59] F.Tosi,A.Tonioni,D.DeGregorio,andM.Poggi,“Nerf-supervised
[39] S.Duggal,S.Wang,W.-C.Ma,R.Hu,andR.Urtasun,“Deeppruner: deepstereo,”inIEEE/CVFConferenceonComputerVisionandPattern
Learning efficient stereo matching via differentiable patchmatch,” in Recognition(CVPR),June2023,pp.855–866. 3
IEEE/CVFInternationalConferenceonComputerVision(ICCV),2019, [60] Y. Zhong, Y. Dai, and H. Li, “Self-supervised learning for stereo
pp.4384–4393. 3 matchingwithself-improvingability,”arXiv:1709.00930,2017. 3
[40] G.Yang,J.Manela,M.Happold,andD.Ramanan,“Hierarchicaldeep [61] A.Tonioni,F.Tosi,M.Poggi,S.Mattoccia,andL.D.Stefano,“Real-
stereomatchingonhigh-resolutionimages,”inIEEE/CVFConference timeself-adaptivedeepstereo,”inIEEEConferenceonComputerVision
onComputerVisionandPatternRecognition(CVPR),2019,pp.5515– andPatternRecognition(CVPR),June2019. 3
5524. 3 [62] A.Tonioni,O.Rahnama,T.Joy,L.DiStefano,A.Thalaiyasingam,and
[41] Y. Wang, Z. Lai, G. Huang, B. H. Wang, L. Van Der Maaten, P.Torr,“Learningtoadaptforstereo,”inIEEEConferenceonComputer
M. Campbell, and K. Q. Weinberger, “Anytime stereo image depth VisionandPatternRecognition(CVPR),June2019. 3
estimationonmobiledevices,”inInternationalConferenceonRobotics [63] H.-Y. Lai, Y.-H. Tsai, and W.-C. Chiu, “Bridging stereo matching
andAutomation(ICRA),2019,pp.5893–5900. 3 and optical flow via spatiotemporal correspondence,” in IEEE/CVF
[42] X.Guo,K.Yang,W.Yang,X.Wang,andH.Li,“Group-wisecorrela- ConferenceonComputerVisionandPatternRecognition(CVPR),2019,
tionstereonetwork,”inIEEE/CVFConferenceonComputerVisionand pp.1890–1899. 3
PatternRecognition(CVPR),2019,pp.3273–3282. 3 [64] Y.Wang,P.Wang,Z.Yang,C.Luo,Y.Yang,andW.Xu,“Unos:Unified
[43] Z.Shen,Y.Dai,andZ.Rao,“Cfnet:Cascadeandfusedcostvolumefor unsupervised optical-flow and stereo-depth estimation by watching
robuststereomatching,”inIEEE/CVFConferenceonComputerVision videos,”inIEEEConferenceonComputerVisionandPatternRecogni-
andPatternRecognition(CVPR),June2021,pp.13906–13915. 3,9, tion(CVPR),2019,pp.8071–8081. 3
10,11,12,13 [65] C.Chi,Q.Wang,T.Hao,P.Guo,andX.Yang,“Feature-levelcollab-
[44] J.Li,P.Wang,P.Xiong,T.Cai,Z.Yan,L.Yang,J.Liu,H.Fan,and oration: Joint unsupervised learning of optical flow, stereo depth and
S.Liu,“Practicalstereomatchingviacascadedrecurrentnetworkwith camera motion,” in IEEE/CVF Conference on Computer Vision and
adaptivecorrelation,”inIEEE/CVFConferenceonComputerVisionand PatternRecognition(CVPR),2021,pp.2463–2473. 3
PatternRecognition(CVPR),2022,pp.16263–16272.3,9,10,11,12, [66] M. Poggi, S. Kim, F. Tosi, S. Kim, F. Aleotti, D. Min, K. Sohn, and
13 S.Mattoccia,“Ontheconfidenceofstereomatchinginadeep-learning
[45] H. Zhao, H. Zhou, Y. Zhang, J. Chen, Y. Yang, and Y. Zhao, “High- era:Aquantitativeevaluation,”IEEETransactionsonPatternAnalysis
frequency stereo matching network,” in IEEE/CVF Conference on andMachineIntelligence,vol.44,no.09,pp.5293–5313,sep2022. 3
Computer Vision and Pattern Recognition (CVPR), 2023, pp. 1327– [67] A.Tonioni,M.Poggi,S.Mattoccia,andL.DiStefano,“Unsupervised
1336. 3,9,10,11,12,13 adaptationfordeepstereo,”inInternationalConferenceonComputer
[46] J. Zeng, C. Yao, L. Yu, Y. Wu, and Y. Jia, “Parameterized cost Vision(ICCV),Oct2017. 3
volume for stereo matching,” in IEEE/CVF International Conference [68] F.Aleotti,F.Tosi,L.Zhang,M.Poggi,andS.Mattoccia,“Reversing
onComputerVision(ICCV),2023,pp.18347–18357. 3,9,10,11,12, the cycle: self-supervised deep stereo through enhanced monocular
13 distillation,” in European Conference on Computer Vision (ECCV),
[47] G.Xu,X.Wang,X.Ding,andX.Yang,“Iterativegeometryencoding 2020. 3
volume for stereo matching,” in IEEE/CVF Conference on Computer [69] M. Poggi, A. Tonioni, F. Tosi, S. Mattoccia, and L. Di Stefano,
VisionandPatternRecognition(CVPR),2023,pp.21919–21928. 3 “Continualadaptationfordeepstereo,”IEEETransactionsonPattern
[48] X. Wang, G. Xu, H. Jia, and X. Yang, “Selective-stereo: Adaptive AnalysisandMachineIntelligence(TPAMI),2021. 3
frequency information selection for stereo matching,” in IEEE/CVF [70] M. Poggi and F. Tosi, “Federated online adaptation for deep stereo,”
ConferenceonComputerVisionandPatternRecognition(CVPR),2024. inIEEE/CVFConferenceonComputerVisionandPatternRecognition
3 (CVPR),2024. 316
[71] J.Han,L.Shao,D.Xu,andJ.Shotton,“Enhancedcomputervisionwith [92] F. Ma, G. V. Cavalheiro, and S. Karaman, “Self-supervised sparse-
microsoftkinectsensor:Areview,”IEEETransactionsonCybernetics, to-dense: Self-supervised depth completion from lidar and monocular
vol.43,no.5,pp.1318–1334,2013. 3 camera,” in International Conference on Robotics and Automation
[72] G.Riegler,Y.Liao,S.Donne,V.Koltun,andA.Geiger,“Connectingthe (ICRA). IEEE,2019,pp.3288–3295. 3
dots:Learningrepresentationsforactivemonoculardepthestimation,” [93] X. Cheng, P. Wang, and R. Yang, “Depth estimation via affinity
inIEEE/CVFConferenceonComputerVisionandPatternRecognition learnedwith convolutionalspatial propagationnetwork,” in European
(CVPR),June2019. 3 ConferenceonComputerVision(ECCV),2018,pp.103–119. 3
[73] S. Schreiberhuber, J.-B. Weibel, T. Patten, and M. Vincze, “Gi- [94] J.Park,K.Joo,Z.Hu,C.-K.Liu,andI.SoKweon,“Non-localspatial
gadepth: Learning depth from structured light with branching neural propagation network for depth completion,” in European Conference
networks,”inEuropeanConferenceonComputerVision(ECCV),2022, ComputerVision(ECCV). Springer,2020,pp.120–136. 3
p.214–229. 3 [95] M. Hu, S. Wang, B. Li, S. Ning, L. Fan, and X. Gong, “Penet:
[74] S. R. Fanello, J. Valentin, C. Rhemann, A. Kowdle, V. Tankovich, Towardspreciseandefficientimageguideddepthcompletion,”inIEEE
P.Davidson,andS.Izadi,“Ultrastereo:Efficientlearning-basedmatch- InternationalConferenceonRoboticsandAutomation(ICRA). IEEE,
ingforactivestereosystems,”inIEEEConferenceonComputerVision 2021,pp.13656–13662. 3
andPatternRecognition(CVPR),2017,pp.6535–6544. 3 [96] Y. Chen, B. Yang, M. Liang, and R. Urtasun, “Learning joint 2d-
[75] A.Zabatani,V.Surazhsky,E.Sperling,S.B.Moshe,O.Menashe,D.H. 3d representations for depth completion,” in IEEE/CVF International
Silver, Z. Karni, A. M. Bronstein, M. M. Bronstein, and R. Kimmel, ConferenceonComputerVision(ICCV),2019,pp.10023–10032. 3
“Intel® realsense™ sr300 coded light depth camera,” IEEE Transac- [97] Y.Lin,T.Cheng,Q.Zhong,W.Zhou,andH.Yang,“Dynamicspatial
tionsonPatternAnalysisandMachineIntelligence,vol.42,no.10,pp. propagation network for depth completion,” in AAAI Conference on
2333–2345,2020. 3 ArtificialIntelligence,vol.36,no.2,2022,pp.1638–1646. 3
[76] D.Scharstein,H.Hirschmu¨ller,Y.Kitajima,G.Krathwohl,N.Nesˇic´,
[98] Y. Zhang, X. Guo, M. Poggi, Z. Zhu, G. Huang, and S. Mattoccia,
X. Wang, and P. Westling, “High-resolution stereo datasets with
“Completionformer: Depth completion with convolutions and vision
subpixel-accurate ground truth,” in German Conference on Pattern
transformers,”inIEEE/CVFConferenceonComputerVisionandPat-
Recognition(GCPR). Springer,2014,pp.31–42. 3,6,8
ternRecognition(CVPR),2023,pp.18527–18536. 3
[77] Y.Liu,T.Jia,X.Yang,X.Li,W.Li,H.Wang,andD.Chen,“Joastereo:
[99] A.Conti,M.Poggi,andS.Mattoccia,“Sparsityagnosticdepthcom-
Joint learning structured light and reconstruction for generalizable
pletion,”inIEEE/CVFWinterConferenceonApplicationsofComputer
activestereo,”IEEETransactionsonInstrumentationandMeasurement,
Vision(WACV),January2023,pp.5871–5880. 3
2023. 3
[100] L.Bartolomei,M.Poggi,A.Conti,F.Tosi,andS.Mattoccia,“Revis-
[78] P.Mirdehghan,W.Chen,andK.N.Kutulakos,“Optimalstructuredlight
iting depth completion from a stereo matching perspective for cross-
alacarte,”inIEEE/CVFConferenceonComputerVisionandPattern
domain generalization,” in International Conference on 3D Vision
Recognition(CVPR),2018,pp.6248–6257. 3
(3DV). IEEE,2024. 3,5
[79] M.GuptaandN.Nakhate,“Ageometricperspectiveonstructuredlight
[101] H.Badino,D.F.Huber,andT.Kanade,“Integratinglidarintostereo
coding,”inEuropenaConferenceonComputerVision(ECCV),2018,
forfastandimproveddisparitycomputation,”InternationalConference
pp.90–107. 3
on3DImaging,Modeling,Processing,VisualizationandTransmission
[80] W. Chen, P. Mirdehghan, S. Fidler, and K. N. Kutulakos, “Auto-
(3DPVT),pp.405–412,2011. 3
tuningstructuredlightbyopticalstochasticgradientdescent,”inIEEE
[102] V.Gandhi,J.Cˇech,andR.Horaud,“High-resolutiondepthmapsbased
ConferenceonComputerVisionandPatternRecognition(CVPR),June
on tof-stereo fusion,” in IEEE International Conference on Robotics
2020. 3
andAutomation. IEEE,2012,pp.4742–4749. 3
[81] A.Grunnet-Jepsen,J.N.Sweetser,P.Winer,A.Takagi,andJ.Woodfill,
[103] J.Choe,K.Joo,T.Imtiaz,andI.S.Kweon,“Volumetricpropagation
“Projectors for intel® realsense™ depth cameras d4xx,” Intel Corp.,
network: Stereo-lidar fusion for long-range depth estimation,” IEEE
SantaClara,CA,USA,2018. 3,13
RoboticsandAutomationLetters,vol.6,no.3,pp.4672–4679,2021.3
[82] R.Chen,J.Xu,andS.Zhang,“Comparativestudyon3dopticalsensors
[104] K.Park,S.Kim,andK.Sohn,“High-precisiondepthestimationwith
for short range applications,” Optics and Lasers in Engineering, vol.
the 3d lidar and stereo fusion,” in IEEE International Conference on
149,p.106763,2022. 3
RoboticsandAutomation(ICRA). IEEE,2018,pp.2156–2163. 3
[83] Y. Zhang, S. Khamis, C. Rhemann, J. Valentin, A. Kowdle,
[105] J. Zhang, M. S. Ramanagopal, R. Vasudevan, and M. Johnson-
V.Tankovich,M.Schoenberg,S.Izadi,T.Funkhouser,andS.Fanello,
Roberson,“Listereo:Generatedensedepthmapsfromlidarandstereo
“Activestereonet:End-to-endself-supervisedlearningforactivestereo
systems,”inEuropeanConferenceonComputerVision(ECCV),2018,
imagery,”inIEEEInternationalConferenceonRoboticsandAutoma-
tion(ICRA). IEEE,2020,pp.7829–7836. 3
pp.784–801. 3
[84] L. Zhang, B. Curless, and S. M. Seitz, “Spacetime stereo: Shape [106] Y.-K.Huang,Y.-C.Liu,T.-H.Wu,H.-T.Su,Y.-C.Chang,T.-L.Tsou,
recoveryfordynamicscenes,”inIEEEConferenceonComputerVision Y.-A.Wang,andW.H.Hsu,“S3:Learnablesparsesignalsuperdensity
andPatternRecognition(CVPR),vol.2. IEEE,2003,pp.II–367. 3 for guided depth estimation,” in IEEE/CVF Conference on Computer
VisionandPatternRecognition(CVPR),2021,pp.16706–16716. 3
[85] P.Ramirez,A.Costanzino,F.Tosi,M.Poggi,S.Salti,S.Mattoccia,and
L.Stefano,“Booster:Abenchmarkfordepthfromimagesofspecular [107] Y.Zhang,S.Zou,X.Liu,X.Huang,Y.Wan,andY.Yao,“Lidar-guided
andtransparentsurfaces,”IEEETransactionsonPatternAnalysisand stereomatchingwithaspatialconsistencyconstraint,”ISPRSJournal
MachineIntelligence,vol.46,no.01,pp.85–102,jan2024. 3 ofPhotogrammetryandRemoteSensing,vol.183,pp.164–177,2022.
3
[86] I.Liu,E.Yang,J.Tao,R.Chen,X.Zhang,Q.Ran,Z.Liu,andH.Su,
“Activezero: Mixed domain learning for active stereovision with zero [108] R. Szeliski, Computer Vision - Algorithms and Applications, Second
annotation,”inIEEE/CVFConferenceonComputerVisionandPattern Edition,ser.TextsinComputerScience. Springer,2022. 4,5
Recognition(CVPR),June2022,pp.13033–13042. 3 [109] K.YoonandI.Kweon,“Stereomatchingwiththedistinctivesimilarity
[87] D. Fiedler and H. Mu¨ller, “Impact of thermal and environmental measure,” in International Conference on Computer Vision (ICCV).
conditionsonthekinectsensor,”inAdvancesinDepthImageAnalysis IEEEComputerSociety,2007,pp.1–7. 4
andApplications,X.Jiang,O.R.P.Bellon,D.Goldgof,andT.Oishi, [110] R.ManduchiandC.Tomasi,“Distinctivenessmapsforimagematch-
Eds. SpringerBerlinHeidelberg,2013,pp.21–31. 3 ing,”in10thInternationalConferenceonImageAnalysisandProcess-
[88] H. Sarbolandi, D. Lefloch, and A. Kolb, “Kinect range sensing: ing(ICIAP),1999,pp.26–31. 4
Structured-light versus time-of-flight kinect,” Computer Vision and [111] A.Conti,M.Poggi,F.Aleotti,andS.Mattoccia,“Unsupervisedconfi-
ImageUnderstanding,vol.139,pp.1–20,2015. 3 denceforlidardepthmapsandapplications,”inIEEE/RSJInternational
[89] M. Camplani and L. Salgado, “Efficient spatio-temporal hole filling ConferenceonIntelligentRobotsandSystems,2022,iROS. 6
strategyforkinectdepthmaps,”inThree-dimensionalimageprocessing [112] R. Spangenberg, T. Langner, S. Adfeldt, and R. Rojas, “Large scale
(3DIP)andapplicationsIi,vol.8290. SPIE,2012,pp.127–136. 3 semi-global matching on the cpu,” in IEEE Intelligent Vehicles Sym-
[90] J.ShenandS.-C.S.Cheung,“Layerdepthdenoisingandcompletion posiumProceedings. IEEE,2014,pp.195–201. 6,7,8,10,11,12,
for structured-light rgb-d cameras,” in IEEE conference on Computer 13
VisionandPatternRecognition(CVPR),2013,pp.1187–1194. 3 [113] C.Banz,P.Pirsch,andH.Blume,“Evaluationofpenaltyfunctionsfor
[91] S. Lu, X. Ren, and F. Liu, “Depth enhancement via low-rank matrix semi-globalmatchingcostaggregation,”TheInternationalArchivesof
completion,” in IEEE conference on Computer Vision and Pattern thePhotogrammetry,RemoteSensingandSpatialInformationSciences;
Recognition(CVPR),2014,pp.3390–3397. 3 39-B3,vol.39,no.B3,pp.1–6,2012. 617
[114] M.MenzeandA.Geiger,“Objectsceneflowforautonomousvehicles,”
in Conference on Computer Vision and Pattern Recognition (CVPR),
2015. 6,7,9
[115] T. Scho¨ps, J. L. Scho¨nberger, S. Galliani, T. Sattler, K. Schindler,
M. Pollefeys, and A. Geiger, “A multi-view stereo benchmark with
high-resolution images and multi-camera videos,” in Conference on
ComputerVisionandPatternRecognition(CVPR),2017. 7
[116] M. Gehrig, W. Aarents, D. Gehrig, and D. Scaramuzza, “Dsec: A
stereoeventcameradatasetfordrivingscenarios,”IEEERoboticsand
AutomationLetters,2021. 7,9,10
[117] K.Chaney,F.Cladera,Z.Wang,A.Bisulco,M.A.Hsieh,C.Korpela,
V.Kumar,C.J.Taylor,andK.Daniilidis,“M3ed:Multi-robot,multi-
sensor,multi-environmenteventdataset,”inIEEE/CVFConferenceon
Computer Vision and Pattern Recognition (CVPR) Workshops, June
2023,pp.4015–4022. 7,9,10,11,12,13
[118] L.Jospin,A.Antony,L.Xu,H.Laga,F.Boussaid,andM.Bennamoun,
“Active-passivesimstereo-benchmarkingthecross-generalizationcapa-
bilities of deep learning-based stereo methods,” Advances in Neural
InformationProcessingSystems(NeurIPS),vol.35,pp.29235–29247,
2022. 7,12,13
[119] A.Handa,T.Whelan,J.McDonald,andA.J.Davison,“Abenchmark
for rgb-d visual odometry, 3d reconstruction and slam,” in IEEE
International Conference on Robotics and Automation (ICRA), 2014,
pp.1524–1531. 7,12
[120] H. Xu, J. Zhang, J. Cai, H. Rezatofighi, F. Yu, D. Tao, and
A.Geiger,“Unifyingflow,stereoanddepthestimation,”arXivpreprint
arXiv:2211.05783,2022. 9,10,11,12,13
[121] A. Bangunharcana, J. W. Cho, S. Lee, I. S. Kweon, K.-S. Kim, and
S. Kim, “Correlate-and-excite: Real-time stereo matching via guided
cost volume excitation,” in IEEE/RSJ International Conference on
IntelligentRobotsandSystems(IROS). IEEE,2021,pp.3542–3548.
9,10,11,12,13
[122] J. Lou, W. Liu, Z. Chen, F. Liu, and J. Cheng, “Elfnet: Evidential
local-global fusion for stereo matching,” in IEEE/CVF International
ConferenceonComputerVision(ICCV),2023,pp.17784–17793. 9,
10,11,12,13
[123] Z.Shen,Y.Dai,X.Song,Z.Rao,D.Zhou,andL.Zhang,“Pcw-net:
Pyramidcombinationandwarpingcostvolumeforstereomatching,”in
Europeanconferenceoncomputervision(ECCV). Springer,2022,pp.
280–297. 9,10,11,12,13
[124] T. Guan, C. Wang, and Y.-H. Liu, “Neural markov random field for
stereo matching,” in IEEE/CVF Conference on Computer Vision and
PatternRecognition(CVPR),2024. 9,10,11,12,13