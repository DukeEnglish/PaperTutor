MINI HONOR OF KINGS: A LIGHTWEIGHT ENVIRONMENT FOR
MULTI-AGENT REINFORCEMENT LEARNING
LinLiu1,JianZhao2,ChengHu4,ZhengtaoCao2,YoupengZhao3,ZhenbinYe1,
MengMeng1,WenjunWang1,ZhaofengHe4,HouqiangLi3,XiaLin1,LanxiaoHuang1
1TencentTimiStudio
2Polixir
3UniversityofScienceandTechnologyofChina
4BeijingUniversityofPostsandTelecommunications
{lincliu,zhenbinye,promengmeng}@tencent.com
{jamesonwang,hugolin,jackiehuang}@tencent.com
{jian.zhao,zhengtao.cao}@polixir.ai
zyp123@mail.ustc.edu.cn,lihq@ustc.edu.cn
hucheng@bupt.edu.cn,zhaofenghe@bupt.edu.cn
June7,2024
ABSTRACT
Gamesarewidelyusedasresearchenvironmentsformulti-agentreinforcementlearning(MARL),
buttheyposethreesignificantchallenges: limitedcustomization,highcomputationaldemands,and
oversimplification. Toaddresstheseissues,weintroducethefirstpubliclyavailablemapeditorfor
the popular mobile game Honor of Kings and design a lightweight environment, Mini Honor of
Kings(MiniHoK),forresearcherstoconductexperiments. MiniHoKishighlyefficient,allowing
experiments to be run on personal PCs or laptops while still presenting sufficient challenges for
existingMARLalgorithms. WehavetestedourenvironmentoncommonMARLalgorithmsand
demonstratedthatthesealgorithmshaveyettofindoptimalsolutionswithinthisenvironment. This
facilitatesthedisseminationandadvancementofMARLmethodswithintheresearchcommunity.
Additionally, we hope that more researchers will leverage the Honor of Kings map editor to de-
velopinnovativeandscientificallyvaluablenewmaps. Ourcodeandusermanualareavailableat:
https://github.com/tencent-ailab/mini-hok.
1 Introduction
Cooperativemulti-agentsystemsareessentialinnumerousreal-worldscenarioswhereagroupofagentscollaborateto
accomplishataskandmaximizecumulativerewardsfortheteam. [1,2]. Theemergenceofreinforcementlearning
techniques [3, 4, 5] has propelled the progress of multi-agent reinforcement learning (MARL) across an array of
domains,includingautonomousvehicles[6],trafficlightcontrol[7],robotics[8]andthesmartgrid[9]. However,for
algorithmresearch,gamesarestillthemostcommonlyutilizedexperimentalenvironmentforMARL.
Asanexperimentalenvironment,thegameenvironmentoffersseveralinherentadvantages. Firstly,gamesareinherently
engagingandcaneasilycapturetheinterestofusers. Secondly,populargamestypicallyhavelargeuserbasesandhave
undergoneextensivetesting,resultinginrobustenvironmentsthatarelesspronetobugs. Lastly,beingdesignedfor
generalconsumers,gamesareintuitiveanddonotrequireextensivedomainknowledgetounderstand. TheMARL
researchencompassesavarietyofgameenvironments,includingHanabiLearningEnvironment[10],PettingZoo[11],
StarCraftMulti-AgentChallenge(SMAC)[12]andGoogleResearchFootball(GRF)[13].
AsthenumberofMARLenvironmentsgrows,theevaluationsystemfortheseenvironmentsmustbecorrespondingly
enhancedandrefined. Weproposethatapurelyresearch-orientedgameenvironmentshouldembodythefollowing
characteristics:
4202
nuJ
6
]AM.sc[
1v87930.6042:viXraAPREPRINT-JUNE7,2024
• Lightweight: Given that reinforcement learning necessitates the collection of vast amounts of data, the
interactionbetweentheagentandtheenvironmentmustbeefficient.
• Potential: While maintaining a lightweight design, the environment should not be overly simplistic. An
environmentthatallowsexistingMARLalgorithmstoquicklyconvergetoaglobaloptimalsolutionfailsto
effectivelyevaluatethesealgorithms.
• Customizable: Different MARL algorithms often address various challenges within multi-agent systems.
Therefore,theenvironmentshouldincludeaneditorthatallowsforthemodificationandcustomizationofits
elements.
Unfortunately,existinggameenvironmentsoftenstruggletosimultaneouslymeetallthesecriteria.
Inthispaper,weintroducethelatestHonorofKingsmapeditor. Leveragingthispowerfultool,researcherscandesign
avarietyofengagingtasks,suchasquicksearch,towerdefenseandbossbattle. Thesegeneratedenvironmentssupport
fastexecutionwithouttheneedforrenderinganinterfaceonLinuxorWindowsoperatingsystem. Uponcompletion,
therunsproducevideofilesthatcanbeplayedbackusingspecificsoftwareforvisualreview. Anotableadvantageof
thismapeditoroverothersisindependenceonaUIinterfaceformodifications,whichcanbeinconvenientandrestrict
flexibilityinparameteradjustments. Instead,userscansimplyadjusttheconfigurationfilestochangethenumber,type,
andpropertiesofagentsintheenvironment. Thisfeaturefacilitatesconvenienttrainingandlearningfortaskssuchas
curriculumlearningandtransferlearning.
(a) (b)
Figure1: Examplestoshowcharacteristicsofourenvironment. (a)Anexampletoshowthebattlingscenarioofour
MiniHoKenvironment,whereheroesarecooperatingtofightagainsttheDarkDragon. (b)Anexampletopresentthe
startpositionsofheroesintheenvironment. Thedefaultstartpositionsaredesignatedaspoints1-5. Themiddlepointis
initialpositionoftheDarkDragon.
Withthistool,wehavedesignedanewMARLenvironment,MiniHonorofKings(MiniHoK),basedonthepopular
gameHonorofKings. Priortothis,HonorofKingshadintroducedtwoenvironments: HokArena(1v1)[14]andHok
3v3[15].However,theseenvironmentsareaimedatresearcherswithasolidfoundationinMARLandthecomputational
resourcesforin-depthscientificresearch. MiniHoKissetintheclassic“FightingtheDarkDragon"scenariofrom
HonorofKings. Userscanchooseanynumberandtypeofheroestoparticipate,withtheobjectivebeingtocauseas
muchdamageaspossibleontheDarkDragonwithinaspecifiedtimeframe. Inthispaper,wehavedesigned7different
modes,eachcorrespondingtoaspecificsetting,tofacilitateusersintestingtheperformanceoftheiralgorithms.
WehaveevaluatedcommonMARLalgorithmsinthisenvironmentandobservedefficientinteractionbetweenthe
environmentandtheagents. UsingaNvidiaGeForceRTX4090GPUanda4-coreCPUfortraining,oneepisodecan
becompletedeverysecond. Thisefficiencymakestheenvironmenthighlysuitableforeducationalpurposesandfor
beginners. Moreover,thisenvironmenthasdemonstratedsignificantpotential. TheperformanceofcommonMARL
algorithmsintheMiniHoKenvironmentstilllagsbehindthatofarule-basedmethod. Thisgappresentsanopportunity
forresearcherstoexploreanddevelopmoreeffectiveMARLalgorithmswithinthisenvironment.
InadditiontosupportingthestudyofgeneralMARLalgorithms,thisenvironmentsimplifiesresearchoncurriculum
reinforcementlearningandtransferreinforcementlearning. Wehaveopen-sourcedtheentireenvironmentandprovided
usermanualathttps://github.com/tencent-ailab/mini-hok. Lookingahead,wehopecontributionsfromthecommunity
toenhancevariousalgorithmswithintheMiniHonorofKingsenvironment. Wealsoencouragetheuseofthemap
editortodesignmoreinterestingandscientificallymeaningfulgameenvironments.
2APREPRINT-JUNE7,2024
2 RelatedWork
Avarietyofreinforcementlearningenvironmentshaveplayedapivotalroleinadvancingthestudyofmulti-agent
reinforcementlearning(MARL).However,existingbenchmarksownsomelimitations. Toaddressthesedrawbacks,we
proposethenovelMiniHonorofKingsEnvironmentforfurtherMARLresearch.
Firstly, with the development of MARL algorithms demonstrating increasingly superior performance, quite a few
benchmarkenvironmentshavebecomeeasytosolve. Forinstance,thecooperativescenariosprovidedbyOpenSpiel
[16] and PettingZoo [11] are set in grid-worlds and board games, which are characterized by simplistic dynamics
oralimitednumberofagents,renderingthemsusceptibletofacilesolutions. Moreover,inthemainstreamMARL
benchmark,SMAC[12],recentworkhasreportedremarkableachievements,evennear-perfectwinrates,acrossmost
scenarios[17,18,19]. Inlightofthesedevelopments,therearisesapressingneedforthecreationofanovelMARL
benchmarkenvironmentcapableofpresentingsubstantialchallenges,therebyfacilitatingthediscernmentofalgorithmic
performance.
What’smore,manyprevalentMARLbenchmarkenvironmentsarebasedonvideogamesorintricatesimulators. For
example,SMACisbuiltonthegameStarcraftII,GRFisrootedintheGameplayFootballsimulator[13]andMulti-Agent
Mujoco[20]representsamulti-agentadaptationoftheMuJoCoenvironment[21]tailoredforroboticcontrol. These
benchmarksarecharacterizedbythecomplexenginefeaturingmultipleagents,intricaterules,anddynamicinteractions.
Consequently,trainingMARLalgorithmswithintheseenvironmentsarecomputationallyintensiveandtime-consuming,
therebycurtailingaccessibilityforresearcherswithlimitedresources. Furthermore,thisinherentcomplexityposes
challengesinscalingupthenumberofagentsduetocomputationalconstraints. Conversely,ourMiniHonorofKings
Environmentpossessesexceptionalcomputationalefficiency,enablingresearcherstoconductexperimentsevenwith
limitedcomputationalresources.
Lastbutnotleast,certainadvancedsimulatorsimposerestrictiveusetermsornecessitateaccesstoclosed-sourcebinaries,
therebylimitingtheabilitytomodifytheenvironments[21,22]. What’sworse,someenvironmentshavewitnessed
maintenancecessation,exemplifiedbythediscontinuationofupdatesforSMACduetothecessationofupdatesforthe
StarCraftIIgame. Thesefactorscollectivelyposeschallengestoeditingtheenvironmentsforresearchers,renderingthe
scenariosoutdatedandunabletocatertotheevolvinganddiverserequirementsoffutureresearch. Conversely,our
environmentisbuiltonthegameHonorofKings,knownasoneoftheworld’smostpopularandhigh-grossinggames
ofalltime. Thecontinuousupdatesoftheoriginalgameensureongoingsupport, enablingdeveloperstofacilitate
thedevelopmentofthisbenchmark. Moreover,theendeavorofcontinuousbenchmarkenvironmentupdatingincurs
significantcosts,underscoringtheneedforabenchmarkenvironmentthatsupportsusercustomization. Inthisregard,
ourworkoffersatoolthatenablesuserstomodifyenvironments,thusfacilitatingthegenerationofdiversescenarios
alignedwithresearchrequirements.
Apart from our proposed environment, another reinforcement learning benchmark environment, named Honor of
KingsArena[14],isalsoderivedfromthegameHonorofKings. Thisenvironmentcloselyresemblestheoriginal
gamedynamics,whereplayerscontrolheroestogaingoldandexperiencebyeliminatingotherin-gameunits. The
primaryobjectiveistodestroyopponentturretsandthebasecrystalwhilesafeguardingone’sownstructures. However,
HonorofKingsArenaisa1v1environment, predominantlycateringtosingle-agentreinforcementlearningrather
than cooperative multi-agent systems. Although the developers have recently updated the Hok3v3 environment,
its complexity is heightened due to the diverse array of units and intricate map configurations. Consequently, the
computationaldemandsfortrainingaresubstantial,potentiallyexceedingtheresourcesavailabletocommonresearchers.
Incontrast,ourMiniHonorofKingsEnvironmentiscenteredonthetaskofcombatingtheDarkDragon,offeringa
streamlinedandefficienttrainingprocessconducivetoconvenientexperiments.
3 HonorofKingsMapEditor
AsthefoundationofourMiniHoKenvironment,theHonorofKingsMapEditorallowsuserstofreelycustomize
scenarios. AsdepictedinFigure1b,therearetwentyinitialpositionswhereheroescanbeplaced,providingusers
withtheflexibilitytodeterminestartpositionsusingthemapeditor. Moreover,userscanconfigurethebehaviorof
agents,therebyalteringtheobjectivesofthescenario. Forexample,settingthedragontostaystillcantransformthe
scenariointoanavigationtask,whereagentshavetoreachtargetedpositions. Althoughweprovidetaskswhereagents
cooperatetofightthedragon,theeditorenablesuserstocreateawiderangeoftasksforMARLresearch.
The map editor endows our environment with high expansibility, allowing users to modify various aspects of the
settings. Figure2demonstratesthistraitclearly,showcasingtheabilitytoadjustparameterssuchasthenumberand
identityofheroes,theirrespectivelevels,andthehealthpointsofthedragon. Furthermore,althoughnotdepictedinthe
figure,usersretaintheoptiontodeterminewhetherheroesareequippedorotherparameters. Incomparisontoexisting
3APREPRINT-JUNE7,2024
benchmarkslikeSMACwherescenarioeditingisfeasible,ourmapeditoroffersmoreconvenienceandautonomy,as
SMAConlyallowsuserstomanipulatethenumberandtypesofunitsratherthanchangingspecificattributes.
Figure2: ExamplestoshowtheexpansibilityofourMiniHonorofKingsEnvironment. Infigure(a)and(b),five
heroesengageinbattleagainstdragon,featuringZhuangZhouintheformerandZhangFeiinthelatter. Figure(c)
showcasesascenariowiththreeheroesparticipatingincombat. Also,thelevelsoftheheroesandthedragon’shealth
pointscanbeadjusted. Additionally,userscantogglethepresenceofequipmentfortheheroesorotherparameters
withinourenvironment,therebyofferingacomprehensivearrayofcustomizationoptions.
4 MiniHonorofKingsEnvironment
Setting Herolevel Skill1 Skill2 Skill3 Equipment Herocomposition
(A) 1 0 0 0 (cid:35) basic
(B) 4 0 0 0 (cid:35) basic
(C) 15 0 0 0 (cid:35) basic
(D) 15 6 6 3 (cid:35) basic
(E) 15 6 6 3 (cid:35) homo(SunWukong)
(F) 4 2 1 1 (cid:35) basic
(G) 4 2 1 1 (cid:33) basic
Table1: Differentconfigurationsoftheexperiments. Tableentriesdenotethelevelsofheroesandtheirrespectivethree
skills. Thesymbol(cid:35)indicatesabsenceofequipmentforallheroes,while(cid:33)signifiesallheroespossesssixpre-set
equipment,asdeterminedbythegameengine. Whenallskilllevelsaresettozero,itimpliesthattheheroesareonly
capableofexecutingnormalattacks. Thebasicheroconfigurationincludesthefiveaforementionedheroes,whilethe
homogeneousherocompositionconsistsoffiveSunWukongforsimplicity.
OurMiniHonorofKingsEnvironmentisbasedontheengineofferedbyHonorofKingsandaccessibletoallindividuals
fornon-commercialactivities. Thisenvironmentconsistsoftwocomponentstointeractwiththegamecore,namely
clientandserver. Theserverhoststhemapeditorandgameengine,allowinguserstoinputstartingcommandorchange
environmentconfigurationswhensimulatingbattlesbetweenheroesandtheDarkDragon. Ontheotherhand,users
can develop their programs within the client. To aid users in this process, we provide example code based on the
Pymarl2algorithmlibrary[23],demonstratingtheapplicationofabovementionedclassicMARLalgorithmswithinour
benchmarkenvironment.
4.1 EnvironmentSettings
OverviewThesimulatedenvironmentportraysascenariowhereinheroes,underthecontrolofamulti-agentsystem,
collaboratetocombattheDarkDragon. Controlledbyheuristicrulescharacterizedbysignificantstochasticity,the
DarkDragonpossessesthreeskillswhosereleaseoccursrandomly. Bydefault,thecombatinvolvesfiveheroes,each
representingdifferentherotypesandpossessingdistinctattributes. Thebasicattributesofeachherocontainshealth
pointvolume,magicalpointvolume,attack,defence,resistanceandsoon. Additionally,thegame’sdifficultylevel
canbemodifiedbyadjustingparameterssuchasthehealthpointsofthedragonorspecifyingwhethertheheroeshave
equipment.
4APREPRINT-JUNE7,2024
State&ObservationsWedelineatetheconceptofstateasthecomprehensivesetofdataprovidedbytheenvironment
subsequenttoactionexecution. Meanwhile,observations,definedasinputsforthesystem,consistofindividualobser-
vationsandjointobservations,wheretheformeristheinputforeachagentwhilethelatterinvolvestheamalgamation
ofindividualobservations. Notably,thejointobservationfeatureisonlyaccessibletoagentsduringcentralisedtraining
forvalueestimation. Leveragingthestateinformation,userscancustomizeandredefinetheirobservationsaccording
totheirspecificneeds. Inthiswork,wefurnishrudimentaryobservationfeaturesforeachagent,encompassingthe
agent’sownlocationandhealthpointvolume,aswellasthoseoftheDarkDragon. Thissimplisticdesignaimsto
accommodatediversesettingsirrespectiveoftheheroselectionsandshowcasetheextensibilityofourenvironment.
Actions The available actions for an individual agent encompass movement in eight directions and various attack
maneuvers,includingstandardattack,threehero-specificskillsandasummonerskill,whichisselectedaccordingto
specificrulesforeachhero. Heroskillscanbecategorizedbasedontheirreleasemechanisms,includingtargeting
specificentities,directions,orpositions. Tostreamlinetheactionspaceofagents,userscanmodifyfunctionsthat
transformactionsintocommandsrequiredbythegamecore,implementingrulestoaccountfortheuniqueskillsetsof
eachhero. Giventheinherentcomplexityarisingfromthedistinctskillsetspossessedbyeachhero,suchadjustments
arehelpfulformanagingtheintricatenatureoftheproblem. Inourcurrentconfiguration,skillsarebydefaultdirected
towardsthedragonforsimplicity. Toenhancethecomplexityoftheproblem,considerationssuchastheexistenceof
additionaladversariesorsupportiveskillsforteammatescanbeintegratedintotheactionspace,therebyexpandingthe
rangeofpossibleactions. Furthermore,bothheroskillsandsummonerskillshavecooldowntime,whichmayrender
certainactionstemporarilyunavailable. Utilizingthestateinformationprovidedbytheenvironment, legalactions
featurescanbederivedtojudgewhichactionsarepermissibleforselection.
RewardsInoursetup,weprovideabasicconfigurationfordensereward. Tobespecific,therewardcomputationat
eachtimestepreliesonthedifferenceinthehealthpointsofthedragon,adjustedbyafactorof-0.01toreflectthe
decrementincurrenthealthpoints. Thisconfigurationgivesdensefeedbacktothesystem,facilitatingthemodelsin
discerningactionsconducivetoinflictingdamageuponthedragon. Moreover,userscanusethereductionintheagents’
healthpointsasasignaltoencouragetheagentstoevadeenemyattacks. Insituationsthatrequiresparserewards,the
outcomeofdefeatingthedragoncanserveasaviablerewardsignal. Leveragingthestateinformation,rewardscanbe
tailoredbyuserstoaccommodateaspectrumofresearchrequirements.
StochasticityThestochasticnatureoftheenvironmentinreinforcementlearningplaysapivotalrolebyintroducing
variabilityanduncertaintyintotheinteractionsofagents,therebyincentivizingexplorationandaidinginthegener-
alizationandadaptationofagents. Ourenvironmentpossessesenoughstochasticity,providinganidealplatformfor
researchendeavors. Forexample,beingbuiltonthegameHonorofKings,theheroes’skillsmayhavesomerandom
characteristicsandcriticalstrikesmayoccur. Similarly,theDarkDragon’sskillreleasealsoexhibitrandomness,further
contributingtotheunpredictabilityoftheenvironment. Additionally,usersretaintheoptiontogeneraterandomized
startingpositionstailoredtotheirspecificexperimentalneeds.
4.2 APIsandImplementation
Composedofbothclientandservercomponents,theenvironmenttransmitsdatathroughJSONstringsforcommunica-
tionpurposes. Toenhanceusability,weencapsulatetheinteractionwiththeenvironmentusingPython. Anillustrative
exampleoftheinterfaceandconfigisprovidedinFigure3,accompaniedbydescriptionsofkeyfunctionsoutlined
below:
• reset(): Thisfunctionstartsanewepisode,initializingandstoringinformationforthegame.
• get_state();get_obs();get_avail_actions(): Thesefunctionsretrievejointobservations,individualobserva-
tions,andavailableactionsforthemulti-agentsystem,aselucidatedinSection4. Notably,theobservations
andavailableactionsarevectorizedtoencompassinformationforallagents.
• select_actions(): Thisfunctionobtainsthedecisionofeachagent,typicallyderivedfrommodelstrainedby
MARLalgorithms.
• step(): Thisfunctionisusedtotransmiteachagent’sdecisiontothesimulatorandupdatethestateinformation
ofenvironment. Italsoreturnsthereward,episodeterminationflags,anddebugginginformation. Additionally,
itencompassesasubordinatefunctiontoconvertactionsintoexecutablecommandsfortheenvironment.
Tobenoted,theencapsulationprovidesaclearandaccessibleinterfaceconducivetotheswiftdevelopmentofnovel
algorithms. Usersretainconsiderableflexibilitytotailorthesefunctionsaccordingtotheirspecificrequirements.
Moreover,theunderlyingengineofourbenchmarkenvironmentiscraftedwithhighlyoptimizedcode,allowingit
to operateefficiently even on standardlaptop configurations. This efficiencysurpasses thatof comparable MARL
5APREPRINT-JUNE7,2024
Figure3: Pythonexampleandconfigexample.
benchmarks,whichoftenrequireserver-levelresourcestorun. Consequently,ourenvironmentisnotablylightweight
andaccessible. LeveragingasolitaryCPUcore,theenvironmentexecutesafullepisodewithinameresecond,thereby
achievingaperformanceofapproximately0.5millionsamplesperhour. Suchexpeditedprocessingcapabilitiesafford
researcherswithlimitedcomputationalresourcestheabilitytoswiftlyassesstheefficacyofMARLalgorithms.
4.3 EfficiencyAnalysis
TofurtherillustratetheefficiencyofourMiniHoKenvironment,weconductanalysisofthetrainingresourcesrequired
forourenvironmentversusotherenvironmentsbasedonthegameHonorofKings. Giventhesignificantdifferencesin
tasksacrossenvironments,wefocusonthecomputingresourcesutilizedwhenrunningexperiments,asdetailedinthe
accompanyingTable2. ItcanbeobservedthatourMiniHoKoperateswithsubstantiallyhigherefficiencycomparedto
itscounterparts. Notably,whileHokArenaandHok3v3necessitatedistributedreinforcementlearningtechniquesto
enhancetrainingefficiency,ourenvironmentdoesnotrequiresuchtechniques,underscoringitsinherentefficiency.
Moreover,ourexperimentsutilizedsimpleandgeneralfeaturedesigns,facilitatingtheuseofstraightforwardnetwork
architectures. In contrast, other environments, due to their complexity, necessitate elaborately designed features
withextensiveinformation,resultinginhigher-dimensionalinputvectorsandmoreintricatenetworkarchitectures.
In summary, the efficiency of our Mini HoK environment is exceptional. Despite its simplicity and lightweight
characteristic,theenvironmentstilloffersampleimprovementspaceforMARLalgorithms,invitingfurtherresearch
andexploration.
ObservationSpace ActionSpace CPUCores GPU TrainingHours FinalResult
HokArena[14] 491 83 128 NVIDIATeslaV100 6.16 BeatBTAI
Hok3v3[15] 4586 161 64 NVIDIATeslaT4 54.30 BeatBTAI
MiniHoK 6 13 4 NvidiaGeForceRTX4090 13.21 Converge
MiniHoK 6 13 40 / 15.41 Converge
Table2: EfficiencyanalysisofenvironmentsbasedongameHonorofKings,includingthetrainingresourcesrequired
toruntheenvironmentsandtheobservationandactionspaces,whichinfluencethecomplexityofneuralnetworks
adopted. Wealsoincludethetrainingoutcomesintheseenvironments,whereHokArenaandHok3v3reportsthe
training duration until the system defeats behaviour-tree (BT) opponents while our experiments focus on the time
requiredtoachieveconvergence.
6APREPRINT-JUNE7,2024
4.4 ExperimentResults
Asareference,weevaluatetheperformanceofVDN[24],QMIX[25],QATTEN[26],QPLEX[17],MAPPO[18]
andHAPPO[19]intheenvironment,allofwhichareintroducedinsupplementmaterials. Inordertofacilitatethese
evaluations,weprovideexamplecodebasedonthePymarl2algorithmlibrary[23]. Giventhesignificantvariability
among heroes in the Honor of Kings game, the basic hero configuration we adopt consists of five heroes, namely
ZhuangZhou,DiRenjie,DiaoChan,SunWukongandCaoCao,comprisingastandardfive-heroteamofthegame.
Performanceisevaluatedbasedonthesystem’sabilitytoengageincombat,measuredbythetotalnumberofdamage
thesystemcancause. Thismetriceffectivelycircumventsthelimitationinmeasuringtheupperboundofalgorithm
performance,asthemaximumhealthpointsoftheDragoncanbeadjusted.
Asourbenchmarkisalightweightenvironment,themulti-agentsystemcanswiftlyacquirerelevantknowledgeand
weruneveryalgorithmfor10000episodesacross5randomseeds. Theobservationfeatureadoptedinthealgorithms
followsthedescriptionsinSection4. Toensurefaircomparisonsamongdifferentalgorithms,eachagent’snetwork
utilizedthesamearchitecture. Furthertechnicaldetailsoftheexperimentscanbefoundintheappendix.
Inordertoassessthesystem’sperformanceundervaryingconfigurations,wedeviseseveralexperimentalsetups. To
benoted,althoughquiteafewparametersoftheenvironmentcanbeadjusted,ourbenchmarkisbuiltontheHonor
ofKingsgameengine,necessitatingadherencetocertainfoundationalprinciples. Forinstance,inthegameheroes
canupgradetheirskillsonceeverytimetheylevelup,whichcanresultinincreaseddamageandreducethecooldown
time. Skill1andskill2canundergosixupgrades,whereasskill3canonlybeupgradedwhenthehero’slevelisa
multipleof4. Inourexperiments,wehavetheflexibilitytoconfigureheroestoabstainfromlearningskillsandonly
utilizenormalattacks,evenwhentheirlevelsaresetto15butitshouldbenotedthatskill3cannotbeacquireduntilthe
heroattainslevel4. Inadditiontoparameteradjustments,ourenvironmentallowsfortheconfigurationofdifferent
herocompositionstoexaminetheimpactofheterogeneityinMARL.Specifically,weconductexperimentsusingfive
identicalheroestoinvestigatetheeffectsofhomogeneousteamconfigurations.
Basedontheseconsiderations,thesettingsoftheconductedexperimentsareshownintable1,illustratingthelevels
of heroes, their respective skill levels, hero compositions and whether they possess equipment. To be noted, each
experimentalconfigurationmaintainsconsistentsettingsforallheroestoensuresimplicityandcoherence,namely,the
skilllevelsandwhethertohaveequipmentareuniformforeachherowithinagivensetting. Moreover,undercurrent
setting,aheroequippedwithequipmentpossessessixpre-setequipment,whichisdeterminedbythegameengine.
(a) Setting A (b) Setting B (c) Setting C (d) Setting D
HAPPO MAPPO
QATTEN QMIX
QPLEX VDN
(e) Setting E (f) Setting F (g) Setting G
Figure4: Experimentresultsfordifferentsettingsthatreflectsthedamagethatthesystemcancauseduringthetraining.
Thesolidlinesshowtheaverageperformanceacross5randomseedsandtheshadedareasreflectthestandarderrorof
theperformanceoutcomes.
4.5 ExperimentAnalysis
The experimental results for each setting are presented in figure 4 and table 3. In addition to the classic MARL
algorithms,weincludeamethodbasedonheuristicrulesforbettercomparison. Theresultsindicatethatagentswith
higherherolevels,enhancedskilllevels,andequipmentinflictmoredamage. Furthermore,thebasicherocomposition,
whichincludesavarietyofherotypes, outperformshomogeneousteamconfigurations, underscoringthepotential
7APREPRINT-JUNE7,2024
Method
VDN[24] QMIX[25] QATTEN[26] QPLEX[17] MAPPO[18] HAPPO[19] Rule
Setting
A 808±391 1099±343 1280±661 964±358 664±434 349±294 3806
B 1830±473 1726±360 1810±450 1309±208 1115±315 1088±649 5101
C 11399±385 11562±59 10753±426 10548±922 9187±879 8344±978 12313
D 24465±960 25571±835 24973±1343 25661±1064 19545±562 19906±1224 28133
E 15614±123 15606±685 15750±426 16427±670 13769±858 13698±733 18131
F 9458±674 8709±1268 8502±1011 10814±1481 7261±830 7126±825 15556
G 198927±26453 408239±49792 303081±34428 401821±67922 221668±37440 348075±34815 500000
Table3: Thecauseddamageofthemulti-agentsystemunderdifferentsettingsusingdifferentmethods. Tobenoted,
wealsoprovideamethodbasedonheuristicrulesforbettercomparison. Wereporttheaverageperformanceacross5
randomseedsandthestandarderrors.
Figure5: Thelazyagentscenario.
ofheterogeneousagentsinMARLproblems,asdiverseagentscanassumedifferentrolesandenhancecooperation.
Additionally,valuedecompositionalgorithmstendtooutperformpolicygradientMARLalgorithmsinourenvironment,
yettheystillfallshortofsurpassingtheheuristic-basedmethod. Despiteemployingstraightforwardanduniversal
featuredesignsinourexperiments,whichcouldpotentiallyconstraintheefficacyofMARLalgorithms,theresultsstill
indicatethatcurrentMARLalgorithmsarenotfullycapabletomastertheenvironment. Thisobservationhighlights
significantopportunitiesforimprovementinfutureMARLresearchendeavors.
Inthisexperiment,weobservedthatZhuangZhouconsistentlymaintainedacertaindistancefromthemonsteranddid
notparticipateintheactualattack,whiletheotherfouragentswereactivelyengagingthemonster,asshowninFigure
5. ByreviewingthebattlereplayvideoandanalyzingZhuangZhou’spositiondataandbehaviortrajectory,wefound
thatZhuangZhoudidnotinflictanydamageonthemonsterthroughouttheentireattack,exhibitingclearsignsoflazy
behavior.
The reason behind Zhuang Zhou’s inaction may be attributed to his role as a support character, which inherently
limitshisabilitytocauseobviousdamagetothemonster. Thisobservationhighlightsacriticalissueinmulti-agent
collaborativetasks: theemergenceoflazybehavioramongagents. Addressingthischallengeiscrucialforadvancing
MARLresearch.
5 Conclusion
Inthisstudy,weintroduceamapeditorforHonorofKingsanddevelopalightweightMARLenvironment,MiniHonor
ofKings,utilizingthistool.ConstructedonthefoundationofthegameHonorofKingsandahighlyoptimizedsimulator,
our environment operates with exceptional efficiency, making it accessible to researchers with limited computing
resources. WeprovideexamplecodebasedonthepopularPymarl2algorithmlibraryandconductevaluationsusing
severalclassicMARLalgorithmstovalidatetheutilityofourbenchmarkandenableuserstoswiftlyengagewiththe
environment. TheresultsrevealampleroomforimprovementforcommonMARLalgorithmswithintheenvironment,
awaitingfurtherinvestigation. Withtheflexiblemapeditor,userscanfreelycustomizetheenvironmentanddesign
diversescenariosaccordingtotheirrequirements. Weareenthusiasticaboutsharingthemapeditorandenvironemnt
withthebroadercommunityandanticipatepositivecontributiontoMARLresearchendeavors.
8APREPRINT-JUNE7,2024
References
[1] LucianBusoniu,RobertBabuska,andBartDeSchutter. Acomprehensivesurveyofmultiagentreinforcement
learning. IEEETransactionsonSystems,Man,andCybernetics,38(2):156–172,2008.
[2] KarlTuylsandGerhardWeiss. Multiagentlearning: Basics,challenges,andprospects. AIMagazine,33(3):41–41,
2012.
[3] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,etal. Masteringthegameofgowith
deepneuralnetworksandtreesearch. Nature,529(7587):484–489,2016.
[4] OriolVinyals,IgorBabuschkin,WojciechMCzarnecki,MichaëlMathieu,AndrewDudzik,JunyoungChung,
David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft II using
multi-agentreinforcementlearning. Nature,575(7782):350–354,2019.
[5] ChristopherBerner,GregBrockman,BrookeChan,VickiCheung,PrzemysławDebiak,ChristyDennison,David
Farhi,QuirinFischer,ShariqHashme,ChrisHesse,etal. Dota2withlargescaledeepreinforcementlearning.
arXivpreprintarXiv:1912.06680,2019.
[6] YongcanCao,WenwuYu,WeiRen,andGuanrongChen.Anoverviewofrecentprogressinthestudyofdistributed
multi-agentcoordination. IEEETransactionsonIndustrialInformatics,9(1):427–438,2012.
[7] TongWu,PanZhou,KaiLiu,YaliYuan,XiuminWang,HuaweiHuang,andDapengOliverWu. Multi-agent
deepreinforcementlearningforurbantrafficlightcontrolinvehicularnetworks. IEEETransactionsonVehicular
Technology,69(8):8243–8256,2020.
[8] Abubakr O Al-Abbasi, Arnob Ghosh, and Vaneet Aggarwal. Deeppool: Distributed model-free algorithm
forride-sharingusingdeepreinforcementlearning. IEEETransactionsonIntelligentTransportationSystems,
20(12):4714–4727,2019.
[9] KaiqingZhang,ZhuoranYang,andTamerBas¸ar.Decentralizedmulti-agentreinforcementlearningwithnetworked
agents: recentadvances. FrontiersofInformationTechnology&ElectronicEngineering,22(6):802–814,2021.
[10] NolanBard, JakobNFoerster, SarathChandar, NeilBurch, MarcLanctot, HFrancisSong, EmilioParisotto,
VincentDumoulin,SubhodeepMoitra,EdwardHughes,etal. Thehanabichallenge:Anewfrontierforairesearch.
ArtificialIntelligence,280:103216,2020.
[11] Jordan Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari, Ryan Sullivan, Luis S
Santos,ClemensDieffendahl,CarolineHorsch,RodrigoPerez-Vicente,etal. Pettingzoo: Gymformulti-agent
reinforcementlearning. AdvancesinNeuralInformationProcessingSystems(NeurIPS),34:15032–15043,2021.
[12] MikayelSamvelyan,TabishRashid,ChristianSchroederDeWitt,GregoryFarquhar,NantasNardelli,TimGJ
Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent
challenge. CoRR,abs/1902.04043,2019.
[13] KarolKurach,AntonRaichuk,PiotrStan´czyk,MichałZaja˛c,OlivierBachem,LasseEspeholt,CarlosRiquelme,
DamienVincent,MarcinMichalski,OlivierBousquet,etal. Googleresearchfootball: Anovelreinforcement
learningenvironment. InAAAIconferenceonartificialintelligence(AAAI),volume34,pages4501–4510,2020.
[14] HuaWei,JingxiaoChen,XiyangJi,HongyangQin,MinwenDeng,SiqinLi,LiangWang,WeinanZhang,Yong
Yu,LiuLinc,etal. Honorofkingsarena: anenvironmentforgeneralizationincompetitivereinforcementlearning.
AdvancesinNeuralInformationProcessingSystems(NeurIPS),35:11881–11892,2022.
[15] LinLiu,JianzhunShao,XinkaiChen,YunQu,BoyuanWang,ZhenbinYe,YuexuanTu,HongyangQin,YangJun
Feng,LinLai,YuanqinWang,MengMeng,WenjunWang,XiyangJi,QIANGFU,LanxiaoHuang,Minwen
Deng,YangWei,HouqiangLi,WengangZhou,NingXie,XiangyangJi,LvfangTao,LinYuan,JuchaoZhuo,
YANG GUANG, and Deheng Ye. Hok3v3: an environment for generalization in heterogeneous multi-agent
reinforcementlearning,2023.
[16] MarcLanctot,EdwardLockhart,Jean-BaptisteLespiau,ViniciusZambaldi,SatyakiUpadhyay,JulienPérolat,
Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, et al. Openspiel: A framework for
reinforcementlearningingames. arXivpreprintarXiv:1908.09453,2019.
[17] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent
q-learning. InInternationalConferenceonLearningRepresentations(ICLR),pages1–16,2020.
[18] ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,andYiWu. Thesurprising
effectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems
(NeurIPS),35:24611–24624,2022.
9APREPRINT-JUNE7,2024
[19] Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang.
Trustregionpolicyoptimisationinmulti-agentreinforcementlearning. InInternationalConferenceonLearning
Representations(ICLR),2021.
[20] ChristianSchroederdeWitt,BeiPeng,Pierre-AlexandreKamienny,PhilipTorr,WendelinBöhmer,andShimon
Whiteson. Deep multi-agent reinforcement learning for decentralized continuous cooperative control. arXiv
preprintarXiv:2003.06709,19,2020.
[21] EmanuelTodorov,TomErez,andYuvalTassa. Mujoco: Aphysicsengineformodel-basedcontrol. InIEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems,pages5026–5033.IEEE,2012.
[22] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo,
AlirezaMakhzani,HeinrichKüttler,JohnAgapiou,JulianSchrittwieser,etal. Starcraftii: Anewchallengefor
reinforcementlearning. arXivpreprintarXiv:1708.04782,2017.
[23] JianHu,SiyangJiang,SethAustinHarding,HaibinWu,andShihweiLiao. Rethinkingtheimplementationtricks
andmonotonicityconstraintincooperativemulti-agentreinforcementlearning. 2021.
[24] PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,MaxJaderberg,
MarcLanctot,NicolasSonnerat,JoelZLeibo,KarlTuyls,etal. Value-decompositionnetworksforcooperative
multi-agentlearningbasedonteamreward. InInternationalConferenceonAutonomousAgentsandMultiAgent
Systems(AAMAS),pages2085–2087,2018.
[25] TabishRashid,MikayelSamvelyan,ChristianSchroeder,GregoryFarquhar,JakobFoerster,andShimonWhiteson.
Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International
ConferenceonMachineLearning(ICML),pages4295–4304.PMLR,2018.
[26] YaodongYang,JianyeHao,BenLiao,KunShao,GuangyongChen,WulongLiu,andHongyaoTang. Qatten: A
generalframeworkforcooperativemultiagentreinforcementlearning. InarXivpreprintarXiv:2002.03939,pages
1–14,2020.
10