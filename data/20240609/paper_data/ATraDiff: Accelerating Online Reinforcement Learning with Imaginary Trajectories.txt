ATraDiff: Accelerating Online Reinforcement Learning with Imaginary
Trajectories
QianlanYang1 Yu-XiongWang1
Abstract Action
State
ATraDiff
Trainingautonomousagentswithsparserewards
is a long-standing problem in online reinforce-
Trajectory
ment learning (RL), due to low data efficiency.
Priorworkovercomesthischallengebyextracting Replay Buffer
usefulknowledgefromofflinedata,oftenaccom- Transition
Reward and
plishedthroughthelearningofactiondistribution New State
from offline data and utilizing the learned dis- 100
w/o ATraDiff
tributiontofacilitateonlineRL.However,since w/ ATraDiff
80
theofflinedataaregivenandfixed,theextracted
knowledge is inherently limited, making it dif- 60
ficult to generalize to new tasks. We propose
a novel approach that leverages offline data to 40
learn a generative diffusion model, coined as
20
Adaptive Trajectory Diffuser (ATraDiff). This
model generates synthetic trajectories, serving 0 SAC RLPD SPiRe PARROT
asaformofdataaugmentationandconsequently Algorithms
enhancing the performance of online RL meth-
Figure1.IllustrationandperformanceshowcaseofourATraDiff.
ods. The key strength of our diffuser lies in ATraDiffcanseamlesslyintegratewithawiderangeofRLmethods
itsadaptability,allowingittoeffectivelyhandle andconsistentlyimprovetheirperformance,byaugmentingthe
varyingtrajectorylengthsandmitigatedistribu- replay buffer with synthesized trajectories. Top: Overview of
tion shifts between online and offline data. Be- onlineRLwithATraDiff. Bottom: Performancecomparisonof
cause of its simplicity, ATraDiff seamlessly in- RLmethodswithandwithoutATraDiffinD4RLKitchen.
tegrates with a wide spectrum of RL methods.
Empirical evaluation shows that ATraDiff con-
sistently achieves state-of-the-art performance energyoptimization(Specht&Madlener,2023). Despite
across a variety of environments, with particu- its impressive performance, RL often requires extensive
larly pronounced improvements in complicated onlineinteractionswiththeenvironment,whichcanbepro-
settings. Ourcodeanddemovideoareavailable hibitivelycostlyinpractice. SuchdownsideofRLisaggra-
athttps://atradiff.github.io. vatedbythefactthatinreal-worldscenarios,environments
are often characterized by sparse rewards, which further
necessitatesanexceptionallylargenumberofsamplesfor
1.Introduction effectiveexploration. Forexample, whenmanipulatinga
robotic arm to move an item, oftentimes the only reward
Deepreinforcementlearning(RL)hasshowngreatpromise feedbackgivenisatthesuccessmomentofthetask,which
invariousapplications,suchasautonomousdriving(Wang maytakehundredsofstepstoobtain. Consequently,aper-
et al., 2019), chip design (Mirhoseini et al., 2021), and sistentchallengeinRLisaddressingthehighsamplecosts,
particularlyincontextswithsparserewards.
1Department of Computer Science, University of Illinois
Urbana-Champaign,Urbana,Illinois,USA.Correspondenceto: One prevalent solution to this challenge is leveraging
QianlanYang<qianlan2@illinois.edu>.
offline data, by directly learning policies from offline
data (Kostrikov et al., 2022; Ball et al., 2023) or extract-
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by ingexperiencesthatcanenhanceonlinetraining(Pertsch
theauthor(s). etal.,2020),especiallyinthecontextofexploration. How-
1
4202
nuJ
6
]GL.sc[
1v32340.6042:viXra
nruteR
dezilamroNATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
ever,suchsolutionscanonlyextractlimitedknowledgeas toselectmoreinformativesamplesthroughtheuseofan
theofflinedataaregivenandfixed,andthusaredifficultto importance indicator, while also mitigating catastrophic
generalizetonewtasks. Incontrasttopriorwork,thispaper forgettingduringadaptation.
takesadifferentperspectiveinspiredbytherecentadvances
Ourcontributionsarethree-fold. (i)WeproposeATraD-
ingenerativemodeling–canweharnessmoderngenerative
iff,anoveldiffusion-basedapproachthatleveragesoffline
models, such as diffusion models, trained on offline data
datatogeneratefullsynthetictrajectoriesandenhancethe
andsynthesizeusefuldatathatfacilitateonlineRL?
performance of online RL methods. ATraDiff is general
Indeed,diffusionmodelshaveemergedaspowerfuldeep andcanbeseamlesslyappliedtoaccelerateanyonlineRL
generativemodels,demonstratingimpressivecapabilitiesin algorithmwithareplaybuffer,aswellasbeingeffectivein
datasynthesisacrossvisionandlanguageapplications(Ho offlineenvironments.(ii)Weintroduceasimpleyeteffective
etal.,2020;Gongetal.,2023;Lietal.,2022). Nevertheless, coarse-to-precisestrategythatensuresgeneratedtrajectories
theirinvestigationinRLhasbeenrelativelylimited. Stud- preciselyalignwiththelengthrequiredforevaluationtasks.
iesinofflineRLmainlytapintothegenerativecapabilities (iii)Wedeviseanonlineadaptationmechanismthatsuccess-
ofdiffusionmodelstoenhancelong-termplanning(Janner fullyaddresseschallengesstemmingfromdatadistribution
etal.,2022;Ajayetal.,2023)andamplifypolicyexpres- shifts. Empirical evaluation shows that ATraDiff consis-
siveness(Wangetal.,2023;Chenetal.,2023). Recently, tentlyachievesstate-of-the-artperformanceacrossavariety
SynthER(Luetal.,2023)isintroducedtoutilizediffusion ofonline,offline-to-online,andofflineenvironments,with
modelsforRLdataaugmentation,byupsamplingthedata particularlylargeimprovementsincomplicatedsettings.
withadiffusionmodeltrainedonanofflinedataset. How-
ever,suchanapproachprimarilyfocusesongeneratingtran-
2.Relatedwork
sitionsratherthancompletetrajectories. Thisunder-utilizes
thegenerativepotentialofdiffusionmodelsandlimitsthe Offline pretraining for online RL. Leveraging prior ex-
benefitsofaugmenteddataforRL.Incontrast,fulltrajec- periencestoexpediteonlinelearningforsubsequenttasks
tories offer a more comprehensive source of information, hasbeenapersistentchallenge(Nairetal.,2021). Pastre-
enablingRLagentstobetterlearnfrompastexperiences. searchhasproposednumeroussolutionstotacklethisissue.
Somestudiessuggesttreatingofflinedatasimilarlytodata
Toovercometheseissues,weproposeAdaptiveTrajectory
collectedonline. Representativeapproachesemployoffline
Diffuser(ATraDiff),anovelmethoddesignedtosynthesize
datatoinitializeareplaybuffer(Vecˇer´ıketal.,2017;Hester
full trajectories for online RL. As depicted in Figure 1,
et al., 2018). Meanwhile, others advocate for a balanced
our approach trains a diffusion model using offline data,
sampling strategy, drawing from both offline and online
which then synthesizes complete trajectories conditioned
sources(Nairetal.,2018;Kalashnikovetal.,2018;Hansen
on the current state. By employing this diffusion model
etal.,2023;Zhangetal.,2023;Balletal.,2023).
toproduceadditionaltrajectories,weaimtosignificantly
acceleratetheonlineRLprocess. Notably,becauseofits Oneprevalentstrategyistoestablishabehaviorprior,which
simplicityinaugmentingthereplaybufferbyaddinguseful capturestheactiondistributioninpriorexperiencestomit-
data,ATraDiffseamlesslyintegrateswithawiderangeof igate overestimation for actions outside the training data
RLmethodsandconsistentlyelevatestheirperformance. distribution(Singhetal.,2021;Siegeletal.,2020). Anal-
ternativestrategyinvolvesextractingskillsfromofflinedata
Thekeypropertyofourdiffuserliesinitsadaptabilityto
andadaptingthemtonewtasks(Guptaetal.,2019;Merel
effectively handle varying trajectory lengths and address
etal.,2019;Kipfetal.,2019;Whitneyetal.,2020;Pertsch
thedistributionshiftsbetweenonlineandofflinedata. Un-
etal.,2020). Thesestudiestypicallyrepresenttheacquired
likegeneratingtransitions,managingtheuncertaintyintask
skillswithinanembeddingspaceandthentrainapolicyto
lengths presents a significant new challenge in trajectory
selectthemostappropriateskillsbasedonthecurrentstate.
generation. Whilelongertrajectoriescanpotentiallylead
In contrast, ATraDiff synthesizes the complete trajectory
toimprovedperformance,excessiveorredundantsegments
basedonthecurrentstateandaugmentsthereplaybuffer
maybedetrimental. Ideally,weaimforagenerationwitha
with these additional data. Our approach offers broader
precisetrajectorylength. Tothisend,weintroduceasimple
applicabilityacrossadiversesetofRLmethodologies.
yeteffectivecoarse-to-precisestrategy: initially,wetrain
multiplediffusionmodelswithvaryinggenerationlengths. DiffusionmodelsinRL.Thefocusofemployingdiffusion
Priortoactualgeneration,weassesstherequiredlengthand models in RL has primarily centered on enhancing long-
subsequentlypruneanyredundantsegments.Indealingwith termplanningandamplifyingpolicyexpressiveness. For
thedistributionshiftbetweenofflinedataandonlineevalua- instance,Diffuser(Janneretal.,2022)constructsafulltra-
tiontasks,wedesignourdiffusertobeadaptablethroughout jectoryoftransitions,throughconditionedsamplingguided
the RL process. This adaptability includes the capability byhigherrewardsandgoal-orientednavigation. Thislever-
2ATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
agesthediffusionmodel’scapabilityingeneratingextensive thedenoisingprocess. Thedatadistributioninducedbythe
trajectories,addressingchallengessuchaslonghorizonsand modelisgivenby:
sparserewardsinRLplanning. Otherstudies(Ajayetal.,
(cid:90) N
2023; Du et al., 2023; He et al., 2023) have adopted this (cid:89)
p (τ0)= p(τN) p (τi−1|τi)dτ1:N,
paradigm, particularly in the context of visual data. An- θ θ
i=1
othernotablework(Pearceetal.,2023)suggestsadiffusion-
basedmethodforgeneratingfulltrajectoriesbyimitating wherep(τN)isastandardGaussianpriorandp(τ0)denotes
humanbehavior. Differentfrompriorwork,ourapproach noiselessdata. Parametersθareoptimizedbyminimizing
isorientedtowardssynthesizingtrajectoriesinevaluation a variational bound on the negative log-likelihood of the
scenariosthatmaydifferfromthetrainingscenarios. reverse process: θ∗ = argmin −E [logp (τ0)]. The
θ τ0 θ
reverse process is often parameterized as Gaussian with
DataaugmentationinRL.Dataaugmentationisacom-
fixedtimestep-dependentcovariances:
montechniquethathasdemonstratedeffectivenessinRL.
Previousmethods(Yaratsetal.,2021;Laskinetal.,2020;
p (τi−1|τi)=N(τi−1|µ (τi,i),Σi).
θ θ
Sinhaetal.,2021)typicallyfocusonperturbingoriginalob-
servationdatainvisual-basedRL,suchasaddingnoiseand Replay buffers. Many algorithms in RL employ replay
applyingrandomtranslation. Thisenablesagentstolearn buffers, denoted as D, to retain trajectories derived from
frommultipleviewsofthesameobservationandincrease executing a sample policy within an environment param-
theirrobustness. Recenteffortshavefocusedonupsampling eterized by MDP. Throughout the training process, these
thereplaybufferwiththediffusionmodel. Acloselyrelated replaybuffersareaccessedtoextractsamples(transitions
study, SynthER(Luetal.,2023), generatestransitionsto ortrajectories)forupdatingthelearnedexecutionpolicy.
augmentthereplaybufferviaadiffusionmodel. However,
ourATraDiffoperatesatthetrajectorylevel,canemploy
4.Method
a visual-based diffusion model, and has the capability to
synthesizetrainingdatathroughstate-andtask-conditioned Wenowpresentourapproachtoacceleratingonlinerein-
generation. Furthermore, our work primarily focuses on forcementlearningbytrainingourgenerativemodelATraD-
leveragingofflinedatatoenhanceonlineRLperformance. iffontheofflinedatatosynthesizetrajectories. Webeginby
Incontrast,SynthERaimstoupsampletheofflinedataset introducinghowwedesignandtrainATraDiff(Section4.1)
usingitsdatasynthesizer. Oursettingthuspresentsmore andthenexplainhowweapplyATraDifftoaccelerateonline
complexchallenges,involvingdynamicgenerationandman- reinforcementlearning(Section4.2). Finally,weintroduce
agingthedistributionshiftbetweenofflineandonlinedata. howthisgeneratorcouldbedynamicallyadaptedduringthe
Anotherwork(Heetal.,2023)investigatestrainingadif- onlinetrainingprocess(Section4.3). Figure2illustratesthe
fusionmodeltoenhancemulti-tasklearning,whereaswe overallframeworkofourapproach.
developageneralmethodtoimproveanyRLmethod.
4.1.AdaptiveTrajectoryDiffuser
3.Background
Ourprimaryobjectivewiththegeneratoristotrainadiffu-
sionmodelp(·)thatcapturesthetrajectorydatadistribution
MDP.Inthispaper,weconsidersequentialdecision-making
present within the offline data. By doing so, we can syn-
tasks that can be modeled as a Markov Decision Process
thesizenewtrajectories(s ,a ,r ,s ,a ,r ,...,s ,a ,r )
(MDP) defined as M = ⟨S,A,T,R,γ⟩, where S is the 1 1 1 2 2 2 t t t
with the learned diffusion model. Our work introduces a
set of states, A is the set of actions, and γ ∈ [0,1) is the
generaldatagenerationframeworkthatisagnostictothe
discount factor. T(s′|s,a) and R(s,a) represent the dy-
specificformofgeneration: weinvestigatebothstate-level
namicsandrewardfunctions, respectively. Ateachstage
andimage-levelgeneration. Thestate-levelgenerationisto
t, the agent takes an action a ∈ A, which leads to a next
directlygeneratethestates,actions,andrewardsfromthe
states′ accordingtothetransitionfunctionT(s′|s,a)and
diffusionmodelwhichistrainedfromscratch. Incontrast,
animmediaterewardR(s,a). Atrajectoryofsuchataskis
image-levelgenerationallowsustobenefitfrompowerful
definedasasequencecomposedofstatesandactionsgiven
pretrainedmodels,whichpotentiallyleadstoimprovedper-
by (s ,a ,s ,a ,...,s ,a ), where s and a denote the
1 1 2 2 t t t t formance. To harness these models more effectively, we
stateandactionattime-stept,respectively.
advocateforthesynthesisoftrajectoryimagesratherthan
Diffusionmodels. Diffusionprobabilisticmodelsposethe direct generation of states. Specifically, our strategy en-
data-generatingprocessasaniterativedenoisingprocedure tailsinitiallygeneratingimagesI ,I ,I ,...,I spanning
1 2 3 k
p (τi−1|τi). Thisdenoisingisthereverseofaforwarddif- kcontinuousframes,wherekisafixedgenerationlength
θ
fusionprocessq(τi|τi−1)thatslowlycorruptsthestructure presetforagivengenerator. Subsequently,wecangetthe
indatabyaddingnoiseinN steps,whereτiisthei-stepof state,action,andrewards ,a ,r fromtheresultantimages
i i i
3ATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
'=0 '=5
ATraDiff
Diffusion
Model1
ATraDiff
'=0 '=5 '=10 '=0 '=5 '=8
Diffusion Trajectory
Model2 Pruner … Selected
Data
'=0 '=5 '=10 '=25
Diffusion Importance
ModelM … Sampler
Length
Estimator
Collected
Real Data
RealReplay Buffer Modified Replay Buffer Synthesized Replay Agent
Buffer
Figure2.IllustrativeoverviewofourATraDiffframework.Left:Adiffusercontainingmultiplediffusionmodels,alengthestimator,and
atrajectorypruner.Right:Workflowoftheonlineadaptation.
using a trained decoder that converts images into states. tothepropertyofdiffusionmodels,ourdiffuserdescribed
Conversely,toprovideimagesforourgenerator,weusean- above can only obtain trajectories with a fixed length of
otherencodertoconvertstatesintoimages. Weimplement k; however, to generate a complete trajectory where the
the encoder by simple ray tracing (Shirley, 2000). More horizonisindefinite,wewouldrequirethediffusertooutput
detailsareprovidedinAppendixA. trajectorieswithflexiblelengths,whichbecomesthecrucial
problem. To solve this, we introduce a simple coarse-to-
Imagediffusionmodels. Forthearchitecturedesignofour
precisestrategy. Weinitiallytrainmultiple(M)diffusion
image diffuser, we use Stable Diffusion (Rombach et al.,
models with varying lengths, such as k = 5,10,15,....
2022)asthepretrainedmodelandfine-tuneitonthespe-
Beforegeneration,wefirstestimatetherequiredlengthfor
cificdataset. Togenerateimagesofk continuousframes,
the current state with a pretrained network and round it
weconcatenatetheksingleimagesI ,I ,...,I intoone
1 2 k uptothenearestpresetlength,minimizingtheoccurrence
larger 2D image I by aligning them in order and use the
ofredundanttransitions. Toprunetheredundantsegment
diffusionmodeltogeneratetheconcatenatedimageI. Fur-
aftergeneration,weintroduceanon-parametricalgorithm
thermore,weusetheinformationofthefirstframeasthe
toidentifythebestendingposition.
generationcondition,includingthecurrentstate,task,etc.,
sothatthegeneratedtrajectorywillbecloselyrelatedtoour Moreconcretely,forageneratedtrajectory(s ,s ,...,s ),
1 2 k
learningprocess,whereonlinelearningcouldbenefitmore we first calculate the similarity between each of two
fromthesesynthesizeddata. adjacentstates,sim(s ,s ),sim(s ,s ),...,sim(s ,s ).
1 2 2 3 k−1 k
Then we compute the prefix average pre and suffix av-
End-to-enddecoder. Wedesignadecodertodecodestates, i
erage suf of this similarity sequence, where pre =
actions, and rewards from the generated images. The de- i i
coderisaneuralnetworkconsistingoftwocomponents.The
(cid:80)i j=1sim i(sj,sj+1)
,suf i =
(cid:80)k
j=i
ksi −m i( +sj 1−1,sj)
. Then we get
firstcomponentincorporatesthefeaturedistilledfromthe thedifferencebetweentheprefixaverageandsuffixaverage
diffusionlayer(Zhaoetal.,2023).Inthesecondcomponent, |pre i−suf i|ofeachpositioni(1<i<k),andfindtheone
separatemultilayerperception(MLP)networksareusedto withthelargestdifferencetobetheendingposition.
determinethe3Dpositionsofspecifickeypoints,whichare
This trajectory pruning algorithm operates under the as-
usedtogeneratetherealstates. Thistwo-componentdesign
sumptionthattheaveragesimilaritybeforetheendingpoint
ofthedecoderallowsforefficientprocessingoftheimages,
shouldbesignificantlylowerthanthataftertheendingpoint.
byfirstidentifyingrelevantfeaturesthroughthediffusion
Thisisachievedbytrainingthediffusionmodelinaway
modelandthenpinpointingkeypoints’positionsusingMLP
thatproducesnearlyduplicatedframeswithinaredundant
networks,sothatwecancalculatetheproprioceptivestates.
segment. Tothisend,wepreprocessthetrainingdatasetto
Therewardsandactionsarepredictedfromthefeaturesand
explicitlyintroduceredundantsegments. Specifically,when
thepredictedstateswithseparateMLPnetworks.
training the diffusion model with a generation length of
GenerationofTrajectorieswithFlexibleLengths. Due k,givenafulltrajectory(I 1,I 2,...,I t),wefirstappendk
4
……ATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
Algorithm1ModifiedReplayBufferforRL theRLalgorithm,thefollowingthreestepsareperformed:
Require: D =(D ,D ,ρ,L),ATraDiff.
s o
1: functionStoreD,z =(s,a′,s′,r) • WefirststoreitintoD o;
2: ReplayBufferStore(D o,z) • With probability ρ , a full trajectory
3: ifwithprobability ρ then (1−ρ)L
(1−ρ)L (s ,a ,r s ,a ,r ...,s ,a ,r ) is synthesized
1 1 1 2 2 2 l l l
4: (s 1,a 1,r 1,...,s l,a l,r l)←ATraDiff(s′) withthediffuser,withstates′ astheinitialstate(i.e.,
5: for∀ido s = s′). Notetheprobability ρ isdesignedto
6: z i =(s i,a i,s i+1,r i) k1 eeptheratiobetweenthetotal( s1 i− zeρ) oL fD andD to
s o
7: ReplayBufferStore(D s,z i) be ρ ;
8: endfor 1−ρ
9: endif • All synthesized transitions (s ,a ,s ,r ) will be
i i i+1 i
10: endfunction storedtothereplaybufferD .
s
11:
12: functionSampleD WhenwesamplefromD,wesamplefromD swithproba-
13: ifwithprobabilityρthen bilityρandfromD owithprobability1−ρ. SeeAlg.1for
14: z ←ReplayBufferSample(D s) pseudo-code. Notethatthesamplingprocesscanbearbi-
15: else trary,i.e.,ourmethodisalsoorthogonaltoothersampling
16: z ←ReplayBufferSample(D o) techniquessuchasprioritizedbuffer(Schauletal.,2016).
17: endif
Inouralgorithm,theexpectedlength,denotedasL,andthe
18: returnz
actuallength,denotedasl,servedistinctpurposesandare
19: endfunction
derivedthroughdifferentmethods. TheexpectedlengthL
isdefinedasthemeanlengthofalltrajectoriesgenerated
duringthetrainingphase. Itisempiricallydeterminedby
additionalframesidenticaltothelastframeI aftertheend averagingthelengthsofalltrajectoriesgeneratedinprelim-
t
ofthetrajectory,resultingin(I ,I ,...,I ,I ,I ,...,I ). inary empirical experiments, conducted before the actual
1 2 t t t t
Subsequently,wesampletdifferentsub-trajectoriesfrom trainingphase. Asacriticalhypeparameter,Lguidesthe
the padded trajectory, each with a length of k, where the totalnumberoftransitionsgeneratedbyATraDiff,ensuring
starting position of the sub-trajectory ranges from 1 to t. acontrolledandpredictablesizeofthegenerateddataset.
Hence,forthesub-trajectoriesstartingaftertime-stept−k, Conversely,theactuallengthlisdeterminedsubsequentto
theirfinalseveralframeswouldalwaysbethesame. thegenerationprocess. Withinthisphase,theappropriate
generatorisselectedviathelengthestimator,followedby
4.2.DiffuserDeployment trajectorypruningtoachievethedesiredlength. Thevalue
of l is inherently variable, adjusting across different gen-
WiththediffuserofATraDiffexplained,wenowdelveinto
eration instances to align with the dynamic contexts and
howthediffusercanbeseamlesslyintegratedwithanyon-
specificrequirementsofeach. Overthecourseofthetrain-
lineRLmethodwithareplaybuffer. Intuitively,ATraDiff
ingphase,weexpectthatthemeanofallactuallengthslto
augments the replay buffer with the data synthesized by
convergetowardsthepredefinedexpectedlengthL.
itsdiffuser,andleavestheRLalgorithmitselfuntouched;
thus,ourapproachisorthogonaltoanyonlineRLmethod
4.3.OnlineAdaptation
equippedwithareplaybuffer.
AlthoughthefixedATraDiffcanimprovetheperformanceof
Morespecifically,consideranyRLalgorithmwithareplay
RLmethodsinsomesimpleenvironments,wemaystillface
bufferdenotedasD . Typically,RLmethodsengagewith
o theproblemofdistributionshiftbetweentheevaluationtask
thereplaybufferthroughtwoprimaryactions: store,which
andtheofflinedataincomplicatedenvironments. Toover-
archives a new transition into the replay buffer, and sam-
comethisissueandfurtherimprovethegenerationquality,
ple, whichextractsatransitionrandomlyfromthereplay
weproposeanonlineadaptationtechniquebycontinually
buffer. Withthisinmind,wesubstitutetheoriginalbuffer
trainingthediffusionmodelonnewexperiences.
D withanewreplaybufferD =D ∪D ,whereD isthe
o o s s
augmentingbuffersynthesizedbyATraDiff. Thismodified Concretely, ATraDiff is periodically updated on the real
replaybufferD ischaracterizedbytwohyperparameters: transitionsstoredinD andthenusedtogeneratenewtra-
o
ρ ∈ [0,1],denotingtheprobabilityofsamplingfromsyn- jectories.Meanwhile,wekeepacopyoftheoriginalversion
thesizeddataD inRL,andL∈N,indicatingtheexpected ofthediffusertomitigatepotentialcatastrophicforgetting.
s
lengthofsynthesizedtrajectories.
Furthermore, we use more valuable samplesto adapt our
Whenever we store a transition (s,a,s′,r) into D during ATraDiffduringonlinetraining. Specifically,wedesignan
5ATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
indicatortomeasuretheimportanceofeachsampleforour SAC_w/ SAC_w/o SAC_SynthER REDQ_w/ REDQ_w/o REDQ_SynthER
walker2d hopper halfcheetah
onlinelearning,andapick-upstrategytochoosesamples 120 120 120
100 100 100
from D o. By default, we are introducing two indicators: 80 80 80
60 60 60
theTD-errorindicator,andtheRewardindicator. Forthe
40 40 40
TD-errorindicator,theimportanceofatransition(s,a,s′,r) 20 20 20
0 0 0
is defined to be |r+γmax a′Q(s′,a′)−Q(s,a)|, where 0 50 10 S0 teps(K1 )50 200 250 0 50 10 S0 teps(K1 )50 200 250 0 50 10 S0 teps(K1 )50 200 250
Q is the learned value function. The TD-error indicator Figure3.LearningcurvesofonlineRLontheD4RLLocomotion
benchmark. ATraDiff(denotedas‘w/’)consistentlyandsignif-
performs better in most cases, despite that it could only
icantlyimprovestheperformanceofthetworepresentativeRL
be used in some value-based RL methods. The Reward
methods across all three environments, irrespective of whether
indicatorwouldbemoregeneraltoallRLmethods,asthe
basicoradvancedalgorithmsareemployed. ATraDiffalsoout-
importanceofatransitionisdefinedtobethetotalreward
performsSynthERwhichsynthesizestransitions. Theseresults
collectedinthefulltrajectory. Theprimarypick-upstrategy
validatetheeffectivenessandgeneralizabilityofourdiffuser.
istomaintainasubsetofsampleswithhigherimportance,
butpartofthesamplesarealwaysusedtoupdatethedif-
fuser. Hence, we randomly drop some samples from the ofthetwobaselinesisbothsignificantlyimprovedbyour
maintainedsubsetregardlessoftheirimportance. Wefur- ATraDiff, especially on the halfcheetah and walker2d en-
therconductexperimentstoanalyzetheeffectivenessofour vironments. ThisvalidatesthestrengthofATraDiff. Ifwe
indicatorandpick-upstrategyandtheimpactofdifferent runtheseonlineRLmethodsforenoughtime,theycanalso
designchoicesintheablationstudy. achievecomparableresults, whileATraDiffimprovesthe
sampleefficiency. Here,weonlyutilizethefixeddiffuser
5.Experiments insteadofusingonlineadaptation,whichindicatesthatour
fixed diffuser can already accelerate online RL in some
Weconductcomprehensiveexperimentstoevaluatetheef- simpleenvironments.
fectivenessofourdatageneratorATraDiff.First,wevalidate
thatourapproachisabletoimprovetheperformanceofboth 5.2.ATraDiffImprovesOffline-to-OnlineRLin
basicandstate-of-the-artonlineRLmethodsbycombining ComplicatedEnvironments
themwithourATraDiff(Section5.1). Next,weshowthat
Inthissection,weshowthatATraDiffwithonlineadaptation
ourmethodcanfurtherimprovetheperformanceofavariety
canimprovetheperformanceofthestate-of-the-artoffline-
ofstate-of-the-artoffline-to-onlineRLalgorithmsincom-
to-onlineRLmethods. Wefurtherconsiderthefollowing
plicatedenvironmentsusingonlineadaptation(Section5.2).
environments:
Inaddition, weevaluateourmethodintheofflinesetting
toshowthatATraDiffachievespromisingresultsinoffline D4RLAntMaze(Fuetal.,2020).Thereare6sparsereward
dataaugmentationaswell(Section5.3). Finally,wecon- tasksthatrequiretheagenttolearntowalkwithcontrolling
ductsomeablationstudiestovalidatetheeffectivenessof an8-DoFAntrobotandnavigatethroughamazetoreach
different components in our approach (Section 5.4). For thegoal.
evaluation, all results in this section are presented as the
D4RLKitchen(Fuetal.,2020). Asimulatedkitchenenvi-
medianperformanceover5randomseedsalongwiththe
ronmentisproposedbyGuptaetal.(2019),whichinvolves
25%-75%percentiles.
controllinga9-DoFrobotthatmanipulatesdifferentobjects
inakitchen environment (e.g., slide cabinetdoor, switch
5.1.ATraDiffImprovesOnlineRL
overheadlight,andopenmicrowave). Thedownstreamtask
Wefirstshowthattheperformanceofstate-of-the-artonline istocompleteasequenceofmultiplesubtasksinorderwith
RLmethodscanbeimprovedbyourATraDifflearnedwith a sparse, binary reward for each successfully completed
theofflinedata. Weconsider3environmentsfromD4RL subtask. Theofflinedatasetonlycontainspartofthefull
Locomotion(Fuetal.,2020),including12differentoffline sequence,meaningthattheagentneedstolearntocompose
datawithvaryinglevelsofexpertise. Forcomparison,we sub-trajectories.
chooseSAC(Haarnojaetal.,2018)asthebasiconlineRL
Meta-World(Yuetal.,2019). Bycombiningthemodified
algorithmandREDQ(Chenetal.,2021)asthestate-of-the-
tasksusingasinglecameraviewpointconsistentlyoverall
artsample-efficientalgorithm. Werunbothbaselinesand
the15subtasksgeneratedbySeoetal.(2022),wecreatetwo
theirvariantscombinedwithourATraDifffor250Ksteps.
challengingtasks.Thefirstoneisamulti-tasksetting,where
TheoverallresultissummarizedinFigure3,showcasing theofflinedatacontainthetrajectoriesof14subtasksand
the average performance of each environment with 4 dif- 1differentevaluationsubtask. Thesecondoneisaharder
ferentofflinedatasets. Detailedperformancebreakdowns versionoftheD4RLKitchentask,wheretheagentneedsto
areprovidedinAppendixC.1. Weseethattheperformance completeasequenceof8subtaskswithmediumdifficulty
6
nruteR
dezilamroN
nruteR
dezilamroN
nruteR
dezilamroNATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
SPiRL_w/ SPiRL_w/o RLPD_w/ RLPD_w/o PARROT_w/ PARROT_w/o SynthER
multitask sequencetask
SPiRL_w/ SPiRL_w/o RLPD_w/ RLPD_w/o PARROT_w/ PARROT_w/o SynthER
Locomotion AntMaze Kitchen 1.0 1.0
120 120 120
100 100 100 0.8 0.8
80 80 80
0.6 0.6 60 60 60
40 40 40 0.4 0.4
20 20 20
0 0 0 0.2 0.2
0 50 100 150 200 250 0 50 100 150 200 250 300 0 50 100 150 200 250 300
Steps(K) Steps(K) Steps(K)
0.0 0.0
Figure4.Learning curves of offline-to-online RL on the D4RL
0 200 400 600 800 1000 0 200 400 600 800 1000
Steps(K) Steps(K)
benchmark. ATraDiff (denoted as ‘w/’) further boosts the per-
Figure5.Learningcurvesofoffline-to-onlineontheMeta-World
formanceofadvancedandrecentoffline-to-onlineRLbaselines
benchmark. While the two tasks within the Meta-World envi-
acrossallthreeenvironments,leadingtostate-of-the-artresults
ronment are designed purposefully to be very changeling with
especiallyincomplexsettings,wheretheimprovementsarepar-
considerabledistributionshifts,ATraDiff(denotedas‘w/’)isstill
ticularly noteworthy. This shows the importance of our online
effectiveandsignificantlyimprovestheperformanceofadvanced
adapted diffuser. The curve named “SynthER” shows the best
andrecentoffline-to-onlineRLbaselines.Thisfurthervalidatesthe
performanceofSynthERcombinedwithanyofthebaselines.
strengthofATraDiffintackingdistributionshiftsbetweenoffline
dataandonlinetasks.Thecurvenamed“SynthER”showsthebest
performanceofSynthERcombinedwithanyofthebaselines.
in the correct order, while the offline data only contain
trajectoriesforsinglesubtasks. Theenvironmentwillbeset
in offline data augmentation. This section details experi-
to the initial state of the next subtask, when the previous
ments conducted to assess ATraDiff’s effectiveness in an
onewascompletedbytheagent. Thisdesignensuresthat
exclusivelyofflineenvironment.
theagentprogressesthroughthetasksinthepredetermined
order, with objects from different tasks kept isolated to OurobjectiveistoverifywhetherATraDiffcanmatchor
preventinterference. Forbothtasks,thetrajectoriesofany surpass the results achieved in previous research, specifi-
single subtask consist of the provided data and collected callyreferencingtheworkSynthER(Luetal.,2023)which
datafromtheonlinetraining. synthesizestransitions. Toensureafairandaccuratecom-
parison, we meticulously replicated the experimental set-
We consider a set of strong baselines from prior work
tingsusedinLuetal.(2023). Wealsocomparewiththe
onoffline-to-onlineRL,includingtheskill-basedmethod
previousstate-of-the-artaugmentationmethodS4RL(Sinha
(SPiRL(Pertschetal.,2020)),behavior-prior-basedmethod
etal.,2021). OurtestinggroundistheD4RLLocomotion
PARROT (Kumar et al., 2022), and balanced sampling
benchmark(Fuetal.,2020). Weextendtheoriginaldataset
methodRLPD(Balletal.,2023). Werunthesebaselines
to5Msamples,followingthesettingsinLuetal.(2023).
andtheirvariantscombinedwithourdiffuser. Toshowthe
effectivenessofourmethod,wealsocompareitwithSyn- The findings, as outlined in Table 1, are quite revealing.
thER(Luetal.,2023). Forthefollowingresults,thecurve TheyindicatethatourATraDiffmethodgenerallyoutper-
named“SynthER”showsthebestperformanceofSynthER forms other baselines across various datasets. This per-
combinedwithanyofthebaselines. formanceimprovementisparticularlypronouncedinhigh-
qualitydatasets,underscoringtheeffectivenessofourap-
AsshowninFigure4,ourdiffusersignificantlyimproves
proachingeneratinghelpfultrajectoriesandthebenefitsof
theperformanceofsomemethods,andobtainsatleastcom-
trajectory-levelgenerationovertransition-levelgeneration.
parableresultsforallmethods. Foralltheseenvironments,
thebestresultisalwaysachievedbysomemethodcombined
5.4.AblationStudyandAnalysis
withourdiffuser,whichshowsthatouronlineadapteddif-
fusercanfurtherimprovetheperformanceofcurrentoffline- Inthissection,wecarryoutablationstudiestojustifythe
to-online methods. Notably, we introduce two advanced effectofdifferentcomponentsofourATraDiff. Additional
taskswithintheMeta-Worldenvironmenttoemphasizethe ablationstudiesareprovidedinAppendixC.3andthegen-
challengesposedbydistributionshifts. Ourmotivationis eratedtrajectoriesarevisualizedinAppendixE.
tounderstandhowourmethodrespondsunderconditions
Image-level generation vs. state-level generation. Our
wheredistributionshiftproblemsaremorepronounced. As
work introduces a general data generation method that is
demonstrated in Figure 5, our approach exhibits superior
agnostictothespecificformofgeneration.AsshowninFig-
performance,suggestingthatourmethodholdssignificant
ure6,bothimageandstategenerationvariantssignificantly
promiseineffectivelyaddressingsuchshifts.
outperform the baselines, while the image-level diffuser
generallyoutperformsthestate-leveldiffuserontheD4RL
5.3.ATraDiffImprovesOfflineRL
Locomotionenvironment. Weusuallyneedtosynthesize
Although ATraDiff is primarily intended for offline-to- longerandmoreflexibletrajectoriesinsuchcomplicateden-
onlinescenarios,italsodemonstratesstrongperformance vironments,wheretheimage-leveldiffuserperformsbetter
7
nruteR
dezilamroN
nruteR
dezilamroN
nruteR
dezilamroN nruteR nruteRATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
TaskName TD3+BC TD3+BC TD3+BC TD3+BC IQL IQL+ IQL+ IQL+
+SynthER +S4RL +ATraDiff SynthER S4RL ATraDiff
halfcheetah-random 11.3 12.2 11.5 12.5 15.2 17.2 15.8 17.1
halfcheetah-medium 48.1 49.9 48.5 52.3 48.3 49.6 48.8 53.1
halfcheetah-replay 44.8 45.9 45.9 46.5 43.5 46.7 46.3 49.2
halfcheetah-expert 90.8 87.2 91.2 93.6 94.6 93.3 94.3 95.2
hopper-random 8.6 14.6 9.4 15.2 7.2 7.7 7.4 8.1
hopper-medium 60.4 62.5 63.4 65.7 62.8 72.0 70.3 72.4
hopper-replay 64.4 63.4 62.3 64.7 84.6 103.2 95.6 103.6
hopper-expert 101.1 105.4 103.5 111.2 106.2 110.8 108.1 113.6
walker-random 0.6 2.3 3.2 2.1 4.1 4.2 4.1 4.3
walker-medium 82.7 84.8 83.7 87.5 84.0 86.7 84.5 89.1
walker-replay 85.6 90.5 88.3 86.3 82.6 83.3 83.1 85.4
walker-expert 110.0 110.2 106.3 111.2 111.7 111.4 111.3 111.7
Table1.NormalizedreturnofofflineRLontheD4RLLocomotionbenchmark.OurATraDiffoutperformsSynthERandS4RLonalmost
allthetasksanddatasets.Whileprimarilyintendedforoffline-to-onlinescenarios,ATraDiffalsodemonstratesstrongperformancein
offlinedataaugmentation.
Isonlineadaptationbeneficialforourdiffuser? Wenow
SAC_w/_pixel SAC_w/_state SAC
Locomotion examinetheeffectofonlineadaptationonperformance. We
120 revisittheexperimentsinSection5.2andreplacetheonline
100 adapteddiffuserwithafixedone. AsshowninFigure7,we
80 find that the online diffuser significantly outperforms the
fixeddiffuserincomplicatedtasks. IntasksofD4RLLo-
60
comotionandD4RLAntMaze,theonlinediffuserachieves
40
comparable results to the fixed diffuser. However, in the
20
task of D4RL Kitchen and two tasks in Meta-World, the
0 onlinediffuserhasfullydemonstrateditssuperiority,which
0 50 100 150 200 250
Steps(K) validatesthattheonlineadaptationcanindeedmitigatethe
problem of data distribution shift and thus focus on the
Figure6.Ablationstudyonimage-levelgenerationandstate-level
evaluationtask.
generation. Theimage-leveldiffuseroutperformsthestate-level
diffuserincomplicatedtasks,withnoticeableperformancegains.
RLPD_online RLPD_offline
comparedwithsimplerdiffusionmodels. Ourhypothesisis
multitask sequencetask
thatimagegenerationcanbenefitfromapretrainedgenera- 1.0 1.0
tionmodel(e.g.,StableDiffusion)whilestategenerationis 0.8 0.8
trainedfromscratch,thoughimagegenerationneedsaddi- 0.6 0.6
tionalmodulestoconvertbetweenimagesandstates. 0.4 0.4
0.2 0.2
Meanwhile,weconductananalysistodemonstratetheef-
0.0 0.0
fectivenessandgeneralizabilityofourATraDiffonpixel- 0 200 400 600 800 1000 0 200 400 600 800 1000
Steps(K) Steps(K)
based reinforcement learning methods. To this end, we Figure7.Ablation study on online adaptation of ATraDiff. In
employATraDifftoupsamplethedatasetfromtheV-D4RL simpletasks,weobservethattheonlinediffuserachievesresults
benchmark (Lu et al., 2022), and use Behavior Cloning comparabletothefixeddiffuser.However,onlineadaptationmiti-
(BC)(Torabietal.,2018)asthebaselinetotestonthebench- gatestheproblemofdatadistributionshiftandbecomescriticalin
complicatedenvironments.
mark. TheresultsshowninTable2validatethatouralgo-
rithmisalsoapplicabletothevisualreinforcementlearning
methods,consistentlyimprovingtheirperformance. Effect of different importance indicators over online
adaptation. Weshowthatdifferentonlineadaptationstrate-
gieshavenoticeableimpactonperformance. Ourexperi-
TaskName walker cheetah
mentsfocusontheindicatorusedtomeasuretheimportance
Dataset mixed mediummedexpexpert mixed mediummedexpexpert
of collected samples together with its associated pick-up
BC 16.5 40.9 47.7 91.5 25.0 51.6 57.5 67.4
strategyforselectingsamplesaccordingtothisimportance.
BC+ATraDiff 17.7 42.2 50.2 93.1 25.2 55.3 68.1 85.3
WeincludetheRewardindicatoranditscorrespondingpick-
upstrategy. TheresultshowninFigure8indicatesthatthe
Table2.NormalizedreturnofofflineRLontheV-D4RLbench-
TD-errorindicatoroutperformstheRewardindicator.
mark. Our ATraDiff consistently improves the performance of
pixel-basedreinforcementlearningmethods. Effectofdifferenttaskprompts. Weshowthatdifferent
8
nruteR
dezilamroN
nruteR nruteRATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
RLPD_TDerror RLPD_Reward
multitask sequencetask
1.0 1.0 ATraDiff ATraDiff_w/o_dropping
Locomotion
0.8 0.8
120
0.6 0.6 100
0.4 0.4
80
0.2 0.2
60
0.0 0.0
0 200 400 Steps(K)600 800 1000 0 200 400 Steps(K)600 800 1000 40
Figure8.Ablationstudyontheimportanceindicatortogetherwith 20
itsassociatedpick-upstrategy.Differenttypesofindicatorslead
0
tovariedbehaviorandperformance.WhiletheRewardindicator 0 50 100 150 200 250
Steps(K)
isinprinciplemoregeneraltoRLmethods,theindicatorbased
onTD-errorachievesbetterperformanceinthecomplicatedMeta- Figure10.Ablationstudyontherandomdroppingstrategy. We
Worldbenchmark. observethattherandomdroppingstrategycansignificantlyim-
provetheperformance,whichvalidatesthatdifferentstrategies
indeedinfluencetheperformance.
task prompts noticeably affect performance. To evaluate
this,weconductexperimentsonMeta-World,testingthree
strategies: languagetaskprompt,one-hottaskprompt,and RLPD_ATraDiff_multi RLPD_ATraDiff_single
multitask sequencetask
no prompt, where the one-hot prompt simply uses a one-
1.0 1.0
hot vector to represent different tasks. The result shown
0.8 0.8
inFigure9demonstratesthatthelanguagepromptoutper-
0.6 0.6
formstheone-hotprompt,andbothsignificantlyexceedthe
0.4 0.4
performanceofthebaselineswithoutanyprompt.
0.2 0.2
0.0 0.0
0 200 400 600 800 1000 0 200 400 600 800 1000
language_prompt vector_prompt no_prompt Steps(K) Steps(K)
multitask sequencetask Figure11.Ablationstudyonmultiplediffusionmodels. Weob-
1.0 1.0 servethattheperformanceofATraDiffwithmultiplediffusion
0.8 0.8 modelssignificantlyoutperformsthefixedsinglediffusionmodel.
0.6 0.6
0.4 0.4
0.2 0.2
theperformanceofourmethodsignificantlyoutperforms
0.0 0.0
thefixedsinglediffusionmodel.
0 200 400 600 800 1000 0 200 400 600 800 1000
Steps(K) Steps(K)
Figure9.Ablationstudyondesignchoicesoftaskprompts.Differ-
enttypesoftaskpromptsleadtovariedbehaviorandperformance.
6.Conclusion
Effectofrandomdroppingstrategy. Hereweshowthat WeintroduceATraDiff,anoveldiffusion-basedapproach
therandomdroppingstrategyisveryimportantintheonline thatsynthesizesfulltrajectories. Bytrainingasetofdiffu-
adaptationprocesswiththeRewardindicator. Inouronline sionmodelsthatgeneratetrajectoriesofdifferentlengths
adaptationphase,whenusingthetotalrewardimportance andselectingthemostsuitablemodeltogetherwithanon-
indicator,theimportanceofanytrajectorywillneverchange parametric pruning algorithm, we obtain a generator that
duringthewholetrainingprocess. Hence,trajectorieswith producestrajectorieswithvariedlengths. Byapplyingsuch
highimportancemightalwaysbeusedtoupdatethegen- a generator to transitions stored in the replay buffer and
erator, whichinspiresus tointroduce arandomdropping augmenting the buffer with generated data, ATraDiff can
strategy. The ablation study conducted on the D4RL Lo- accelerate any online RL algorithm with a replay buffer.
comotionenvironmentshowninFigure10illustratesthat Inmultipleenvironments,ATraDiffsignificantlyimproves
therandomdroppingstrategycansignificantlyimprovethe existingonline,offline,andoffline-to-onlineRLalgorithms.
performance. WehopethatourworkisbeneficialtotheonlineRLcommu-
nityinaddressingthelong-standingdataefficiencyproblem.
Effect of multi-generator. Now we show that the multi-
generatorarchitectureiscrucial. Weconductexperiments Limitationsandfuturework. OnelimitationofATraD-
onMeta-Worldtotesttheeffectofthemultiplediffusion iffisthatthetrainingofmultiplediffusionmodelscanbe
modeldesign.Wereplacetheflexibletask-lengthgeneration computationallyintensive(asevaluatedinAppendixC.2),
controlschemewithafixedlengthgeneratorof25,which andtheproblemescalateswithlongergeneratedtrajectories.
isthelargestlengthinoursetting, anddirectlyapplyour Thus,aninterestingfuturedirectionistolearnasingledif-
pruningstrategyafterthegeneration. Theresultshownin fuserthatdirectlygeneratestrajectoriesofvariedlengths
Figure11demonstratesthatwithmultiplediffusionmodels, insteadofselectingfromacollectionofmodels.
9
nruteR
nruteR
nruteR
nruteR
nruteR
nruteR
dezilamroN
nruteRATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
Acknowledgements Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine,
S. D4RL:Datasetsfordeepdata-drivenreinforcement
This work was supported in part by NSF Grant 2106825,
learning. arXiv: 2004.07219,2020.
NIFAAward2020-67021-32799, theJumpARCHESen-
dowmentthroughtheHealthCareEngineeringSystemsCen- Gong,S.,Li,M.,Feng,J.,Wu,Z.,andKong,L. DiffuSeq:
teratIllinoisandtheOSFFoundation,andtheIBM-Illinois Sequencetosequencetextgenerationwithdiffusionmod-
DiscoveryAcceleratorInstitute. ThisworkusedNVIDIA els. InICLR,2023.
GPUsatNCSADeltathroughallocationsCIS220014and
CIS230012fromtheACCESSprogram. Gupta,A.,Kumar,V.,Lynch,C.,Levine,S.,andHausman,
K. Relaypolicylearning: Solvinglong-horizontasksvia
imitationandreinforcementlearning. InCoRL,2019.
ImpactStatement
Haarnoja,T.,Zhou,A.,Abbeel,P.,andLevine,S.Softactor-
Ourresearchcontributestodecision-makingprocessesby
critic: Off-policymaximumentropydeepreinforcement
leveragingofflinedata,aimingtoenhancetheefficiencyof
learningwithastochasticactor. ICML,2018.
reinforcementlearning. Whileouradvancementsoffersig-
nificantbenefitsintermsoftask-solvingcapabilities,they
Hafner,D.,Lillicrap,T.P.,Norouzi,M.,andBa,J. Master-
alsoraiseconcernsaboutpotentialnegativesocietalimpacts. ingAtariwithdiscreteworldmodels. InICLR,2021.
Forinstance,thedeploymentofhighlyefficientautonomous
systems in the workforce could lead to job displacement Hansen, N., Lin, Y., Su, H., Wang, X., Kumar, V., and
and increased economic inequality. Furthermore, our ap- Rajeswaran, A. Modem: Accelerating visual model-
proachreliesonagenerativemodelforsynthesizingnew based reinforcement learning with demonstrations. In
data,whichintroducesconcernsregardingtheauthenticity ICLR,2023.
andreliabilityofthegeneratedoutcomes. Thepossibility
He, H., Bai, C., Xu, K., Yang, Z., Zhang, W., Wang, D.,
ofgeneratingfictitiousormisleadingdataunderscoresthe
Zhao, B., and Li, X. Diffusion model is an effective
importanceofcriticallyassessingandmitigatingpotential
planneranddatasynthesizerformulti-taskreinforcement
ethical and societal implications, ensuring that advance-
learning. InNeurIPS,2023.
mentsinautonomousdecision-makingcontributepositively
tosocietywhileminimizingadverseeffects.
Hester,T.,Vecerik,M.,Pietquin,O.,Lanctot,M.,Schaul,
T.,Piot,B.,Horgan,D.,Quan,J.,Sendonaris,A.,Osband,
References I.,Dulac-Arnold,G.,Agapiou,J.,Leibo,J.,andGruslys,
A. DeepQ-learningfromdemonstrations. InAAAI,2018.
Ajay, A., Du, Y., Gupta, A., Tenenbaum, J. B., Jaakkola,
T.S.,andAgrawal,P. Isconditionalgenerativemodeling Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
allyouneedfordecisionmaking? InICLR,2023. bilisticmodels. InNeurIPS,2020.
Ball,P.J.,Smith,L.,Kostrikov,I.,andLevine,S. Efficient Janner,M.,Du,Y.,Tenenbaum,J.B.,andLevine,S. Plan-
onlinereinforcementlearningwithofflinedata. InICML, ning with diffusion for flexible behavior synthesis. In
2023. ICML,2022.
Black,K.,Nakamoto,M.,Atreya,P.,Walke,H.,Finn,C., Kalashnikov,D.,Irpan,A.,Pastor,P.,Ibarz,J.,Herzog,A.,
Kumar,A.,andLevine,S.Zero-shotroboticmanipulation Jang,E.,Quillen,D.,Holly,E.,Kalakrishnan,M.,Van-
withpretrainedimage-editingdiffusionmodels. InICLR, houcke,V.,andLevine,S. Scalabledeepreinforcement
2024. learningforvision-basedroboticmanipulation. InCoRL,
2018.
Chen,H.,Lu,C.,Ying,C.,Su,H.,andZhu,J. Offlinerein-
forcementlearningviahigh-fidelitygenerativebehavior Kipf,T.,Li,Y.,Dai,H.,Zambaldi,V.F.,Sanchez-Gonzalez,
modeling. InICLR,2023. A.,Grefenstette,E.,Kohli,P.,andBattaglia,P.W. Com-
pile: Compositionalimitationlearningandexecution. In
Chen,X.,Wang,C.,Zhou,Z.,andRoss,K.W. Randomized
ICML,2019.
ensembled double Q-learning: Learning fast without a
model. InICLR,2021. Kostrikov,I.,Nair,A.,andLevine,S. Offlinereinforcement
learningwithimplicitQ-learning. InICLR,2022.
Du,Y.,Yang,M.,Dai,B.,Dai,H.,Nachum,O.,Tenenbaum,
J.B.,Schuurmans,D.,andAbbeel,P. Learninguniversal Kumar,A.,Yazdanbakhsh,A.,Hashemi,M.,Swersky,K.,
policies via text-guided video generation. In NeurIPS, andLevine,S. Data-drivenofflineoptimizationforarchi-
2023. tectinghardwareaccelerators. InICLR,2022.
10ATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
Laskin,M.,Lee,K.,Stooke,A.,Pinto,L.,Abbeel,P.,and Siegel,N.Y.,Springenberg,J.T.,Berkenkamp,F.,Abdol-
Srinivas,A.Reinforcementlearningwithaugmenteddata. maleki,A.,Neunert,M.,Lampe,T.,Hafner,R.,Heess,
InNeurIPS,2020. N.,andRiedmiller,M. Keepdoingwhatworked: Behav-
ioralmodellingpriorsforofflinereinforcementlearning.
Li,X.,Thickstun,J.,Gulrajani,I.,Liang,P.,andHashimoto, InICLR,2020.
T.B.Diffusion-LMimprovescontrollabletextgeneration.
InNeurIPS,2022. Singh, A., Liu, H., Zhou, G., Yu, A., Rhinehart, N., and
Levine,S. Parrot: Data-drivenbehavioralpriorsforrein-
Lu,C.,Ball,P.J.,Rudner,T.G.J.,Parker-Holder,J.,Os- forcementlearning. InICLR,2021.
borne,M.A.,andTeh,Y.W. Challengesandopportuni-
Sinha,S.,Mandlekar,A.,andGarg,A. S4RL:surprisingly
tiesinofflinereinforcementlearningfromvisualobserva-
simpleself-supervisionforofflinereinforcementlearning
tions. arXiv:2206.04779,2022.
inrobotics. InCoRL,2021.
Lu,C.,Ball,P.J.,Teh,Y.W.,andParker-Holder,J.Synthetic
Specht,J.M.andMadlener,R. Deepreinforcementlearn-
experiencereplay. InNeurIPS,2023.
ingfortheoptimizedoperationoflargeamountsofdis-
tributed renewable energy assets. Energy and AI, 11:
Merel,J.,Hasenclever,L.,Galashov,A.,Ahuja,A.,Pham,
100215,2023.
V.,Wayne,G.,Teh,Y.W.,andHeess,N. Neuralproba-
bilisticmotorprimitivesforhumanoidcontrol. InICLR, Torabi, F., Warnell, G., andStone, P. Behavioralcloning
2019. fromobservation. InIJCAI,2018.
Mirhoseini, A., Goldie, A., Yazgan, M., Jiang, J. W., Vecˇer´ık,M.,Hester,T.,Scholz,J.,Wang,F.,Pietquin,O.,
Songhori,E.M.,Wang,S.,Lee,Y.-J.,Johnson,E.,Pathak, Piot,B.,Heess,N.,Rotho¨rl,T.,Lampe,T.,andRiedmiller,
O.,Nazi,A.,Pak,J.,Tong,A.,Srinivasa,K.,Hang,W., M. Leveragingdemonstrationsfordeepreinforcement
Tuncer,E.,Le,Q.V.,Laudon,J.,Ho,R.,Carpenter,R., learningonroboticsproblemswithsparserewards. arXiv:
and Dean, J. A graph placement methodology for fast 1707.08817,2017.
chipdesign. Nature,594(7862):207–212,2021.
Wang,S.,Jia,D.,andWeng,X. Deepreinforcementlearn-
Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., ingforautonomousdriving. arXiv:1811.11329,2019.
andAbbeel,P. Overcomingexplorationinreinforcement
Wang,Z.,Hunt,J.J.,andZhou,M. Diffusionpoliciesasan
learningwithdemonstrations. InICRA,2018.
expressivepolicyclassforofflinereinforcementlearning.
InICLR,2023.
Nair, A., Gupta, A., Dalal, M., and Levine, S. AWAC:
Acceleratingonlinereinforcementlearningwithoffline Whitney, W. F., Agarwal, R., Cho, K., and Gupta, A.
datasets. arXiv: 2006.09359,2021. Dynamics-awareembeddings. InICLR,2020.
Pearce,T.,Rashid,T.,Kanervisto,A.,Bignell,D.,Sun,M., Yarats,D.,Kostrikov,I.,andFergus,R.Imageaugmentation
Georgescu,R.,Macua,S.V.,Tan,S.Z.,Momennejad,I., isallyouneed:Regularizingdeepreinforcementlearning
Hofmann,K.,andDevlin,S. Imitatinghumanbehaviour frompixels. InICLR,2021.
withdiffusionmodels. InICLR,2023.
Yu,T.,Quillen,D.,He,Z.,Julian,R.,Hausman,K.,Finn,C.,
Pertsch,K.,Lee,Y.,andLim,J.J. Acceleratingreinforce- andLevine,S.Meta-World:Abenchmarkandevaluation
mentlearningwithlearnedskillpriors. InCoRL,2020. formulti-taskandmetareinforcementlearning. InCoRL,
2019.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Zhang,H.,Xu,W.,andYu,H.Policyexpansionforbridging
Ommer,B. High-resolutionimagesynthesiswithlatent
offline-to-onlinereinforcementlearning. InICLR,2023.
diffusionmodels. InCVPR,2022.
Zhao, W., Rao, Y., Liu, Z., Liu, B., Zhou, J., and Lu, J.
Schaul,T.,Quan,J.,Antonoglou,I.,andSilver,D. Priori-
Unleashingtext-to-imagediffusionmodelsforvisualper-
tizedexperiencereplay. InICLR,2016.
ception. InICCV,2023.
Seo,Y.,Hafner,D.,Liu,H.,Liu,F.,James,S.,Lee,K.,and
Abbeel,P. Maskedworldmodelsforvisualcontrol. In
CoRL,2022.
Shirley,P. Realisticraytracing. AKPeters,2000.
11ATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
A.AlgorithmDetails
A.1.Pick-upStrategiesforOnlineAdaptation
Inourtrainingmethodology,wehavedesigneddifferentpick-upstrategiesforhandlingdifferentimportanceindicators,
recognizingtheiruniquepropertiesandimplicationsforthelearningprocess.
FortheTD-errorindicator,adynamicapproachisemployedduetothevariablenatureofitsimportance,whichcanchange
asthecriticfunctionupdatesduringtraining. Tomanagethis,aheapstructureisusedtomaintainabufferof50,000samples.
Thisbufferiscontinuouslyupdatedtoreflectthecurrentimportanceofeachsample,ensuringthateventhosewithinitially
lowimportanceareretainedforpotentialfuturerelevance. Whenitcomestimetoupdatethediffuser,aselectionof5,000
high-importancesamplesisdrawnfromthisbuffer,aligningtheupdateprocesswiththemostpertinentdataatthatmoment.
Incontrast,thestrategyforrewardimportancetakesintoaccountitsstaticcharacteristic–thesampleimportanceofsuchan
indicatordoesnotchangethroughoutthetraining. Here,asmallerbufferof5,000samplesisemployed,updatedsimilarly
basedonimportance. However,tocountertheriskofhigh-importancesamplesperpetuallydominatingthebuffer,arotation
schemeisimplemented. Aftereachupdate,somesamples,particularlytheolderones,aredropped. Thisschemeensuresthat
eachsamplecontributestothediffuserupdateforalimitednumberoftimes,thusmaintainingafreshandcurrentdatasetfor
training,reflectiveofthelatestenvironmentalinteractions.
These tailored strategies highlight a nuanced understanding of how different indicators behave and affect the learning
process,ensuringthatbothdynamicandstaticaspectsofthetrainingdataareoptimallyutilizedforupdatingthediffuser.
A.2.State-levelTrajectoryGeneration
In the state-level generation, we directly generate trajectories consisting of states, actions and rewards,
{s ,a ,r ,s ,a ,r ...}. For the architecture of the diffusion model used in the state-level generation, we di-
t t t t+1 t+1 t+1
rectlyrefertothearchitectureusedinLuetal.(2023). Meanwhile,weextendthesizeofthenetworkusedfrom24to128to
supporttheincreasedlengthofgeneratedtrajectories.
A.3.ImageEncoder
Intheimage-levelgeneration,anencoderandadecoderareemployedforimagerenderingandkeypointdetectiontasks,
respectively. The encoder, based on previous work (Lu et al., 2022; Hansen et al., 2023; Seo et al., 2022), captures
observationsinanenvironmentwhereanagentoperates. Therenderedimages,setataresolutionof84×84,serveasinput
ofthegenerationmodel.
B.DetailsofExperimentSetup
B.1.D4RL
WeconsiderthreedifferentenvironmentsfromD4RL(Fuetal.,2020). WeusetheoriginalofflinedatasetfromD4RL(Fu
etal.,2020).
AntMaze. Thisdomainstepsupthecomplexitybyreplacingthe2DballinMaze2Dwithan8-DoF“Ant”quadrupedrobot,
addingalayerofmorphologicalcomplexity. Itisanavigationdomainthatcloselyresemblesreal-worldroboticnavigation
tasks. Wefollowthedesignofasparse0-1rewardstrategyinthisenvironment,activatedonlyuponreachingthegoal,to
testthestitchingchallengeundermorecomplexconditions.
Locomotion. ComprisingtaskslikeHopper,HalfCheetah,andWalker2d,theLocomotiondomainisastapleinoffline
deepRLbenchmarks. WeusethesamedatasetsinD4RL(Fuetal.,2020)forconsistencywithpreviousstudies,andalso
experimentwithavarietyofdatasetstoobservetheeffectsofdifferentdataqualities.
Kitchen. This environment involves controlling a 9-DoF Franka robot in a kitchen setting, interacting with everyday
householditemslikeamicrowave,kettle,cabinets,anoverheadlight,andanoven. Eachtaskaimstoachieveaspecificgoal
configuration,likeopeningthemicrowaveandslidingcabinetdoor,placingthekettleontheburner,andturningonthe
overheadlight. TheKitchendomainservesasabenchmarkformulti-taskbehaviorinarealistic,non-navigationsetting.
Here, the stitching challenge is amplified due to the complexity of the trajectories through the state space, compelling
algorithmstogeneralizetounseenstatesratherthanrelysolelyontrainingtrajectories.
12ATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
TaskName Subtasks
Multitask evaluation task: Push
Sequencetask Sweep,SweepInto,CoffeePush,BoxClose,PushWall,PegInsertSide,Basketball,Soccer
Table3.Meta-Worldtasksettings. Weselectthepushtaskastheevaluationtaskforthemulti-taskenvironment. Wearrangeallthe8
mediumtaskstoformthesequence-taskenvironment.
B.2.Meta-World
TheMeta-Worldbenchmark(Yuetal.,2019)isacomprehensivesuitedesignedforevaluatingandadvancingreinforcement
learning and multi-task learning algorithms. It features 50 distinct robotic manipulation tasks, offering a diverse and
challenging environment for testing the ability of algorithms to generalize and quickly acquire new skills. Following
previouswork(Hansenetal.,2023),weselectatotalof15tasksfromMeta-WorldbasedontheirdifficultyaccordingtoSeo
etal.(2022),whichcategorizestasksintoeasy,medium,hard,andveryhardcategories. SameasHansenetal.(2023),we
discardeasytasksandselectalltasksfromtheremaining3categories. Ourapproachdiffersfrompreviousstudies(Hansen
etal.,2023;Seoetal.,2022)inthatwesolelyutilizeproprioceptivestateinformationasinputofRLagents,ratherthan
RGBframesorcombinedstateinformation. Thischoiceallowsforeasierapplicationtogeneralreinforcementlearning
methods. Followingtheprevioussettings,weuseasparserewardsignalthatonlyprovidesarewardof1whenthecurrent
taskissolvedand0otherwise. Forthesuccesscriteria,wefollowtheoriginalsettinginMeta-World. Table3detailsour
multi-taskandsequence-tasksettings.
TrainingDataset. ThedatasetisacquiredbytraininganRLagentwithSAC(Haarnojaetal.,2018)oneachsingletask.
Forthemulti-tasksetting,theentiredatasetiscombinedbyall14singletasks. Forthesequencetask,theentiredatasetis
combinedbyall8singletasks,withoutanytrajectorychange.
C.AdditionalExperimentalResults
This section provides supplementary experimental findings, offering detailed insights and further analysis to deepen
understandingofthepropertiesofATraDiffanditscomponents.
C.1.DetailedOnlineRLResults
HerewepresentthedetailedonlineRLresultsofeachenvironmentonD4RLLocomotionwithall4differentofflinedatasets.
Figure12showsthatourATraDiffiscomparabletoorbetterthantheoriginalRLbaselinemethodswithlow-qualitydata
(suchasrandomdata),andtheperformancegapbetweenourATraDiffandthebaselinesbecomesmuchmorepronounced
withmedium/high-qualitydata(suchasmedium-expertdata).
C.2.TrainingTime
TheexperimentsareconductedonasingleNVIDIARTX4090TIGPU.ThetrainingtimeofATraDiffislistedinTable4.
Walker2D Hopper Halfcheetah
SAC(Haarnojaetal.,2018) 6.5h 7h 7h
SACwithATraDiff 15h 17h 16.5h
RDEQ(Chenetal.,2021) 6h 6.5h 6.5h
RDEQwithATraDiff 14.5h 16.5h 16h
Table4. Timecost(hours)oftrainingSACandRDEQwithandwithoutATraDiff.
C.3.ComprehensiveAnalysisandFurtherComparisonofATraDiffandSynthER
ATraDiffdifferssignificantlyfromSynthER(Luetal.,2023)invariouscrucialaspects,particularlyinthecontextofthe
onlinelearningenvironment. TheseaspectscollectivelycontributetooursubstantialimprovementsoverSynthER.Below,
wesummarizeseveralkeydifferences.
13ATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
SAC_w/ SAC_w/o REDQ_w/ REDQ_w/o
walker2d-random walker2d-medium-replay walker2d-medium walker2d-medium-expert
100 100 100 100
50 50 50 50
0 0 0 0
0 100 200 0 100 200 0 100 200 0 100 200
Steps(K) Steps(K) Steps(K) Steps(K)
hopper-random hopper-medium-replay hopper-medium hopper-medium-expert
100 100 100 100
50 50 50 50
0 0 0 0
0 100 200 0 100 200 0 100 200 0 100 200
Steps(K) Steps(K) Steps(K) Steps(K)
halfcheetah-random halfcheetah-medium-replay halfcheetah-medium halfcheetah-medium-expert
100 100 100 100
50 50 50 50
0 0 0 0
0 100 200 0 100 200 0 100 200 0 100 200
Steps(K) Steps(K) Steps(K) Steps(K)
Figure12.LearningcurvesofonlineRLontheD4RLLocomotionbenchmark.ATraDiff(denotedas‘w/’)iscomparabletoorbetterthan
theoriginalRLbaselinemethodswithlow-qualitydata,andtheperformancegapbetweenATraDiffandthebaselinesbecomesmuch
morepronouncedwithmedium/high-qualitydata.
• ATraDiffcanperformbothimageandstategeneration,withourfindingsindicatingasuperiorperformanceofimage
generation within our framework. This enhancement is likely attributable to the more effective utilization of the
pretrainedgenerativemodelinimagegeneration,asopposedtoSynthER’sstate-onlygenerationapproach.
• Moreover, ATraDiff is designed to synthesize entire trajectories rather than generating individual transitions as in
SynthER.Wepositthatthiscapabilitytoproducefulltrajectoriesyieldsaricher,moreinformativedatasetforthe
learningalgorithmthanisolatedtransitions.
• Intherealmofonlinetraining,ATraDiffuniquelygeneratesthesetrajectoriesbasedonthespecificcurrentstateand
task,therebyoptimizingtherelevanceandqualityofthegenerateddataforimmediateonlinelearningapplications.
• Inadditiontothesefeatures,duringtheonlineadaptationphase,ATraDiffemploysimportancesamplingstrategiesto
selectivelyleveragedatacollectedonline,prioritizinginformationthatismostbeneficialfortheonlineadaptation.
To substantiate the advantages of ATraDiff, we conduct an addtional ablation study that systematically evaluates the
incremental contributions of its components mentioned above. The study is structured as follows: initially, compared
with SynthER, ATraDiff is applied to generate pixel-level single transitions without auxiliary mechanisms, denoted as
“ transition.” Subsequently,weevaluatethegenerationofpixel-leveltrajectories,stilldevoidofsupplementarytechniques,
denotedas“ trajectory.” Next,weassessedATraDiff’scapabilitytoproducepixel-leveltrajectoriesinformedbythecurrent
state,albeitwithoutimplementingimportancesampling,denotedas“ condition.” Lastly,weevaluatethefullATraDiff,
denotedas“ ATraDiff.” Theresultsfromthisexperiment,aspresentedinTable5,demonstrateATraDiff’sprogressive
improvement,affirmingthateachtechnique’sintegrationsignificantlybooststhemodel’seffectiveness,particularlyunder
onlinetrainingconditions. ThesefindingsunderscoreATraDiff’snovel,sophisticatedmethodologyinenhancinglearning
andadaptationindynamicenvironments,andexplainwhyATraDiffnotablyoutperformsSynthER.
14
nruteR
dezilamroN
nruteR
dezilamroN
nruteR
dezilamroN
nruteR
dezilamroN
nruteR
dezilamroN
nruteR
dezilamroN
nruteR
dezilamroN
nruteR
dezilamroN
nruteR
dezilamroN
nruteR
dezilamroN
nruteR
dezilamroN
nruteR
dezilamroNATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
TaskName SynthER RLPD transition RLPD trajectory RLPD condition RLPD ATraDiff
Locomotion 94.6 93.5 97.1 101.3 102.3
AntMaze 97.3 97.2 97.6 98.2 98.7
Multitask 0.57 0.60 0.72 0.75 0.77
Sequencetask 0.62 0.64 0.71 0.79 0.81
Table5.FullcomparisonbetweenATraDiffandSynthERonbothD4RL(normalizedreturn)andMeta-World(return)benchmarks.
ATraDiffnotablyoutperformsSynthER,witheachofourproposedcomponentscontributingtotheimprovement.
D.AdditionalRelatedWork: DiscussionsonWorldModelsinRL
Asanemergingresearchdirection,werecognizethesignificantpotentialofworldmodelsindecision-makingdomains.
However,wewouldliketoclarifythatourworkaimstodemonstrateaspecificapplicationofworldmodelswithintheRL
framework,whichisnovel,under-explored,andeffective,butwedonotclaimthatourmethodrepresentsthebestwayof
exploitingworldmodelsinRL.Anevaluationoftheoptimalstrategiesisbeyondthescopeofthispaper,andweleaveitas
interestingfuturework. Wealsobelievethattheoptimalapproachwilllikelyvarydependingontheapplicationandtaskat
hand.
Distinctfromstudiesthatcreatecomprehensivedecision-makingmodelscapableofautonomouslycompletingtasks(e.g.,
SUSIE(Blacketal.,2024)andDreamerv2(Hafneretal.,2021)),ourworkfocusesondevelopingaversatileauxiliary
methoddesignedtoenhanceexistingRLalgorithms. Ourcontributionisnotastandalonetask-solvingmechanism;rather,it
isaplug-inapproachintendedtointegrateseamlesslywithanyRLalgorithmthatemploysareplaybuffer. Thisdistinction
underscorestheinnovativeaspectofourwork: offeringamethodthatbroadenstheapplicabilityandefficiencyofworld
modelsassupportivetoolsintheRLlandscape,therebyfacilitatingtheiradoptionandutilityindiverseRLcontextsand
applications.
E.Visualizations
InFigure13,weshowsomerepresentativevisualizationsofourgeneratedimagetrajectories. Wepresentavarietyofimages
generatedunderdifferentconditions,includingvaryinginitialstatesandarangeoftasks. Thelabelsintheleftofthefigures
representthetask. Theresultsdemonstratetherobustnessandeffectivenessofourimagegenerationmethod,consistently
performingwellacrossdiversescenariosandproducinghigh-qualityandtemporallycoherentimagetrajectories.
15ATraDiff:AcceleratingOnlineReinforcementLearningwithImaginaryTrajectories
initial t=0 t=5 t=10 t=15 t=20
Figure13.Representativevisualizationsofourgeneratedimagetrajectories.Eachrowcontainsonegeneration,theinitialimagerepresents
thestatereceivedbyATraDiff,andthefollowingimagesrepresentthegeneratedresultsindifferenttime-steps.
16
peewS
peewS
llaWhsuP
reccoS