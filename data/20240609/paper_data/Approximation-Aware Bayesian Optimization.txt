Approximation-Aware Bayesian Optimization
NatalieMaus KyuraeKim GeoffPleiss
UniversityofPennsylvania UniversityofPennsylvania UniversityofBritishColumbia
nmaus@seas.upenn.edu
DavidEriksson JohnP.Cunningham JacobR.Gardner
Meta ColumbiaUniversity UniversityofPennsylvania
Abstract
High-dimensionalBayesianoptimization(BO)taskssuchasmoleculardesignoften
require>10,000functionevaluationsbeforeobtainingmeaningfulresults. While
methodslikesparsevariationalGaussianprocesses(SVGPs)reducecomputational
requirementsinthesesettings,theunderlyingapproximationsresultinsuboptimal
dataacquisitionsthatslowtheprogressofoptimization. Inthispaperwemodify
SVGPstobetteralignwiththegoalsofBO:targetinginformeddataacquisition
rather than global posterior fidelity. Using the framework of utility-calibrated
variational inference, we unify GP approximation and data acquisition into a
jointoptimization problem, thereby ensuring optimaldecisionsunderalimited
computational budget. Our approach can be used with any decision-theoretic
acquisition function and is compatible with trust region methods like TuRBO.
Wederiveefficientjointobjectivesfortheexpectedimprovementandknowledge
gradient acquisition functions in both the standard and batch BO settings. Our
approachoutperformsstandardSVGPsonhigh-dimensionalbenchmarktasksin
controlandmoleculardesign.
1 Introduction
Bayesianoptimization(BO;Frazier,2018;Garnett,2023;Jonesetal.,1998;Mockus,1982;Shahriari
etal.,2015)castsoptimizationasasequentialdecision-makingproblem. ManyrecentsuccessesofBO
haveinvolvedcomplexandhigh-dimensionalproblems. Incontrastto“classic”low-dimensionalBO
problems—whereexpensiveblack-boxfunctionevaluationsfarexceededcomputationalcosts—these
modernproblemsnecessitatetensofthousandsoffunctionevaluations,anditisoftenthecomplexity
anddimensionalityofthesearchspacethatmakesoptimizationchallenging,ratherthanalimited
evaluationbudget(Erikssonetal.,2019;GriffithsandHernández-Lobato,2020;Mausetal.,2022,
2023;Stantonetal.,2022). Becauseofthesescenarios,BOisenteringaregimewherecomputational
costsarebecomingaprimarybottleneck(Maddoxetal.,2021;Mausetal.,2023;Mossetal.,2023;
Vakilietal.,2021),astheGaussianprocess(GP;RasmussenandWilliams,2005)surrogatemodels
thatunderpinmostofBayesianoptimizationscalecubicallywiththenumberofobservations.
In this new regime, we require scalable GP approximations, an area that has made tremendous
progressoverthelastdecade. Inparticular,sparsevariationalGaussianprocesses(SVGP;Hensman
et al., 2013; Quiñonero-Candela and Rasmussen, 2005; Titsias, 2009) have seen an increase in
use(GriffithsandHernández-Lobato,2020;Maddoxetal.,2021;Mausetal.,2022,2023;Stanton
etal.,2022;Trippetal.,2020;Vakilietal.,2021),butmanychallengesremaintoeffectivelydeploy
SVGPs for large-budget BO. In particular, the standard SVGP training objective is not aligned
withthegoalsofblack-boxoptimization. SVGPsconstructaninducingpointapproximationthat
maximizesthestandardvariationalevidencelowerbound(ELBO;Jordanetal.,1999),yieldinga
posteriorapproximation𝑞∗(𝑓)thatmodelsallobserveddata(Matthewsetal.,2016;Mossetal.,2023).
Preprint. Underreview.
4202
nuJ
6
]GL.sc[
1v80340.6042:viXraHowever,theoptimalposteriorapproximation𝑞∗issuboptimalforthedecision-makingtasksinvolved
inBO(Lacoste–Julienetal.,2011). InBO,wedonotcareaboutposteriorfidelityatthemajority
ofpriorobservations;rather,weonlycareaboutthefidelityofdownstreamfunctionsinvolvingthe
posterior,suchastheexpectedutility. Toillustratethispointintuitively,considerusingthecommon
expectedimprovement(EI;Jonesetal.,1998)acquisitionfunctionforselectingnewobservations.
MaximizingtheELBOmightresultinaposteriorapproximationthatmaintainsfidelityfortraining
examplesinregionsofvirtuallyzeroEI,thuswasting“approximationbudget.”
Tosolvethisproblem,wefocusonthedeepconnectionsbetweenstatisticaldecisiontheory(Robert,
2001;Wasserman,2013,§12)andBayesianoptimization(Garnett,2023,§6-7),whereacquisition
maximizationcanbeviewedasmaximizingposterior-expectedutility. Followingthisperspective,
we leverage the utility-calibrated approximate inference framework (Jaiswal et al., 2020, 2023;
Lacoste–Julienetal.,2011),andsolvetheaforementionedproblemthroughavariationalbound(Blei
etal.,2017;Jordanetal.,1999)–the(log)expectedutilitylowerbound(EULBO)—ajointfunctionof
thedecision(theBOquery)andtheposteriorapproximation(theSVGP).Whenoptimizedjointly,
theEULBOautomaticallyyieldstheapproximatelyoptimaldecisionthroughtheminorize-maximize
principle (Lange, 2016). The EULBO is reminiscent of the standard variational ELBO (Jordan
etal.,1999),andcanindeedbeviewedasastandardELBOforageneralizedBayesianinference
problem(Bissirietal.,2016;Knoblauchetal.,2022)whereweseektoapproximatetheutility-weighted
posterior. Thisworkrepresentsthefirstapplicationofutility-calibratedapproximateinferencetowards
BOdespiteitsinherentconnectionwithutilitymaximization.
The benefits of our proposed approach are visualized in Fig. 1. Furthermore, it can be applied
toacquisitionfunctionthatadmitsadecision-theoreticinterpretation,whichincludesthepopular
expectedimprovement(EI;Jonesetal.,1998)andknowledgegradient(KG;Wuetal.,2017)acquisition
functions,andistriviallycompatiblewithlocaloptimizationtechniqueslikeTuRBO(Erikssonetal.,
2019)forhigh-dimensionalproblems. WedemonstratethatourjointSVGP/acquisitionoptimization
approachyieldssignificantimprovementsacrossnumerousBayesianoptimizationbenchmarks. Asan
addedbenefit,ourapproachcansimplifytheimplementationandreducethecomputationalburdenof
complex(decision-theoretic)acquisitionfunctionslikeKG.Wedemonstrateanovelalgorithmderived
fromourjointoptimizationapproachforcomputingandoptimizingtheKGthatexpandsrecentwork
onone-shotKG(Balandatetal.,2020)andvariationalGPposteriorrefinement(Maddoxetal.,2021).
Overall,ourcontributionsaresummarizedasfollows:
∙
Weproposeutility-calibratedvariationalinferenceofSVGPsinthecontextoflarge-budgetBO.
∙
Westudythisframeworkintwospecialcasesusingtheutilityfunctionsoftwocommonacquisition
functions: EIandKG.Foreach,wederivetractableEULBOexpressionsthatcanbeoptimized.
∙ For KG, we demonstrate that the computation of the EULBO takes only negligible additional
workovercomputingthestandardELBObyleveraginganonlinevariationalupdate. Thus,asa
byproductofoptimizingtheEULBO,optimizingKGbecomescomparabletothecostoftheEI.
∙ We extend this framework to be capable of running in batch mode, by introducing q-EULBO
analogsofq-KGandq-EIascommonlyusedinpractice(Wilsonetal.,2018).
∙
WedemonstratetheeffectivenessofourproposedmethodagainststandardSVGPstrainedwith
ELBOmaximizationonhigh-dimensionalbenchmarktasksincontrolandmoleculardesign,
wherethedimensionalityandevaluationbudgetgoupto256and80k,respectively.
2 Background
Black-Boxoptimization referstoproblemsoftheform: maximize 𝐹(𝒙),where𝒳 ⊂ℝ𝑑 is
𝒙∈𝒳
somecompactdomain,andweassumethatonlyzeroth-orderinformationisavailable. Generally,we
assumethatobservationsoftheobjectivefunction(𝒙,𝑦 =𝐹ˆ(𝒙))havebeencorruptedbyGaussian
𝑖 𝑖 𝑖
noise𝐹ˆ(𝒙)≜𝐹(𝒙)+𝜖,where𝜖 ∼𝒩(0,𝜎2). Thenoisevariance𝜎2 iscommonlyunknownand
𝑖 𝑖 𝑛 𝑛
inferredfromthetrainingdata.
BayesianOptimization (BO)isanditerativeapproachtoblack-boxoptimizationthatutilizesthe
followingsteps: ❶Ateachstep𝑡 ≥ 0,weuseasetofobservations𝒟 =
{(
𝒙,𝑦
=𝐹ˆ(𝒙)) }𝑛𝑡
of
𝑡 𝑖 𝑖 𝑖 𝑖=1
𝐹ˆto fit a surrogate supervised model 𝑓. Typically 𝑓 is taken to be a Gaussian process, with the
function-valuedposteriordistribution𝜋(𝑓 ∣𝒟)asoursurrogatemodelatstep𝑡. ❷Thesurrogate
2ELBO fit (m=4) ELBO EULBO (Ours)
Data True function Inducing points GP mean EI (approx) EI (exact)
Figure1: (Left.) FittinganSVGPmodelwithonly𝑚=4inducingpointssacrificesmodelingareas
ofhighEI(fewdatapointsatright)becausetheELBOfocusesonlyonglobaldataapproximation
(left data) and is ignorant of the downstream decision making task. (Middle.) Because of this,
(normalized)EIwiththeSVGPmodelpeaksinanincorrectlocationrelativetotheexactposterior.
(Right.) UpdatingtheGPfitandselectingacandidatejointlyusingtheEULBO(ourmethod)resultsin
candidateselectionmuchclosertotheexactmodel.
is then used to form a decision problem where we choose which point we should evaluate next,
𝒙 =𝛿 (𝒟 ),bymaximizinganacquisitionfunction𝛼 ∶𝒳 →ℝas
𝑡+1 𝛼 𝑡
𝛿 (𝒟 )≜argmax 𝛼(𝒙;𝒟 ). (1)
𝛼 𝑡 𝑡
𝒙∈𝒳
❸Afterselecting𝒙 ,𝐹ˆisevaluatedtoobtainthenewdatapoint(𝒙 ,𝑦 =𝐹ˆ(𝒙 )). Thisis
𝑡+1 𝑡+1 𝑡+1 𝑡+1
thenaddedtothedataset,forming𝒟 =𝒟 ∪(𝒙 ,𝑦 )tobeusedinthenextiteration.
𝑡+1 𝑡 𝑡+1 𝑡+1
UtilityPerspectiveofAcquisitionFunctions. Manycommonlyusedacquisitionfunctions,includ-
ingEIandKG,canbeexpressedasposterior-expectedutilityfunctions
𝛼(𝒙;𝒟)≜∫ 𝑢(𝒙,𝑓;𝒟 )𝜋(𝑓 ∣𝒟)d𝑓, (2)
𝑡
where𝑢(𝒙,𝑓;𝒟 )∶𝒳×ℱ →ℝissomeutilityfunctionassociatedwith𝛼(Garnett,2023,§6-7).
𝑡
Indecisiontheory,posterior-expectedutilitymaximizationpoliciessuchas𝛿 areknownasBayes
𝛼
policies. These are important because, for a given utility function, they attain certain notions of
statisticaloptimalitysuchasBayesoptimalityandadmissibility(Robert,2001,§2.4;Wasserman,
2013,§12). However,thisonlyholdstrueifwecanexactlycomputeEq.(2)overtheposterior. Once
approximateinferenceisinvolved,makingoptimalBayesdecisionsbecomeschallenging.
SparseVariationalGaussianProcesses Whilethe𝒪(𝑛3)complexityofexactGaussianprocess
model selection and inference is not necessarily a roadblock in the traditional regression setting
with 10,000-50,000 training examples, BO amplifies the scalability challenge by requiring us to
sequentiallytrainorupdatemanylargescaleGPsasweiterativelyacquiremoredata.
Toaddressthis,sparsevariationalGPs(SVGP;Hensmanetal.,2013;Titsias,2009)havebecome
commonlyusedinhigh-throughputBayesianoptimization. SVGPsmodifytheoriginalGPprior
from𝑝(𝑓)to𝑝(𝑓 ∣ 𝒖)𝑝(𝒖),whereweassumethelatentfunction𝑓 is“induced”byafinitesetof
inducingvalues𝒖=(𝑢 ,…,𝑢 )∈ℝ𝑚 locatedatinducingpoints𝒛 ∈𝒳 for𝑖 =1,…,𝑚. Inference
1 𝑚 𝑖
is done through variational inference (Blei et al., 2017; Jordan et al., 1999) the posterior of the
inducingpointsisapproximatedusing𝑞 (𝒖)=𝒩(𝒖;𝝀=(𝒎,𝑺))andthatofthelatentfunctions
𝝀
with𝑞(𝑓 ∣𝒖)=𝑝(𝑓 ∣𝒖). TheresultingELBOobjective,whichcanbecomputedinclosedform
(Hensmanetal.,2013),isthen
ℒ (𝝀;𝒟 )≜𝔼
[∑𝑛𝑡
log𝓁(𝑦
∣𝑓(𝒙))]
−D (𝑞 (𝒖),𝑝(𝒖)), (3)
ELBO 𝑡 𝑞𝜆(𝑓) 𝑖=1 𝑖 𝑖 KL 𝜆
where 𝓁(𝑦 ∣ 𝑓(𝒙)) = 𝒩(𝑦 ∣𝑓(𝒙),𝜎 ) is a Gaussian likelihood. The marginal variational
𝑖 𝑖 𝑖 𝑖 𝜖
approximationcanbecomputedas
𝑞 (𝑓)=∫ 𝑞 (𝑓,𝒖)d𝒖=∫ 𝑝(𝑓 ∣𝒖)𝑞 (𝒖)d𝒖
𝜆 𝝀 𝜆
suchthat
( )
𝑞 (𝑓(𝒙))=𝒩 𝑓(𝒙); 𝜇 (𝒙)≜𝑲 𝑲−1𝒎, 𝜎2(𝒙)≜𝑘˜ +𝒌⊤ 𝑲−1𝑺𝑲−1𝒌 , (4)
𝜆 𝑓 𝒙𝒁 𝒁𝒁 𝑓 𝒙𝒙 𝒙𝒁 𝒁𝒁 𝒁𝒁 𝒁𝒙
with𝑘˜ ≜𝑘(𝒙,𝒙)−𝒌 𝑲−1𝒌⊤ ,thevector𝒌 ∈ℝ𝑚 isformedas[𝒌 ] =𝑘(𝒛,𝒙),andthe
𝒙𝒙 𝒙𝒁 𝒁𝒁 𝒁𝒙 𝒁𝒙 𝒁𝒙 𝑖 𝑖
matrix𝑲 ∈ℝ𝑚×𝑚 isformedas[𝑲 ] =𝑘(𝒛,𝒛 ). Additionally,theGPlikelihoodandkernel
𝒁𝒁 𝒁𝒁 𝑖𝑗 𝑖 𝑗
3containhyperparameters,whichwedenoteas𝜽 ∈Θ,andwecollectivelydenotethesetofinducing
pointlocationsas𝒁 ={𝒛 ,…,𝒛 }∈ℝ𝑑𝑚. WethereforedenotetheELBOasℒ (𝝀,𝒁,𝜽;𝒟 ).
1 𝑚 ELBO 𝑡
3 Approximation-AwareBayesianOptimization
WhenSVGPsareusedinconjunctionwithBO(Maddoxetal.,2021;Mossetal.,2023),acquisition
functionsoftheformofEq.(2)arenaïvelyapproximatedas
𝛼(𝒙;𝒟)≈∫ 𝑢(𝒙,𝑓;𝒟 )𝑞 (𝑓)d𝑓,
𝑡 𝝀
where𝑞 (𝑓)istheapproximateSVGPposteriorgivenbyEq.(4). Theacquisitionpolicyimpliedby
𝝀
thisapproximationcontainstwoseparateoptimizationproblems:
𝒙 𝑡+1 =arg 𝒙∈m 𝒳ax∫ 𝑢(𝒙,𝑓;𝒟 𝑡)𝑞 𝝀∗ ELBO(𝑓)d𝑓 and 𝝀∗ ELBO =arg 𝝀∈m Λaxℒ ELBO(𝝀;𝒟 𝑡). (5)
Treating these optimization problems separately creates an artificial bottleneck that results in
suboptimaldataacquisitiondecisions. Intuitively,𝝀∗ ischosentofaithfullymodelallobserved
ELBO
data(Matthewsetal.,2016;Mossetal.,2023),withoutregardforhowtheresultingmodelperformsat
selectingthenextfunctionevaluationintheBOloop. Foranillustrationofthis,seeFigure1. Instead,
weproposeamodificationtoSVGPsthatcouplestheposteriorapproximationanddataacquisition
throughajointproblemoftheform:
(𝒙 , 𝝀∗)=argmaxℒ (𝝀,𝒙;𝒟 ). (6)
𝑡+1 EULBO 𝑡
𝝀∈Λ,𝒙∈𝒳
Thisresultsin𝒙 directlyapproximatingasolutiontoEq.(2),wheretheexpectedutilitylower-
𝑡+1
bound(EULBO)isanELBO-likeobjectivefunctionderivedbelow.
3.1 ExpectedUtilityLower-Bound
ConsideranacquisitionfunctionoftheformofEq.(2)wheretheutility𝑢∶𝒳×ℱ →ℝ isstrictly
>0
positive. Wecanderiveasimilarvariationalformulationoftheacquisitionfunctionmaximization
problemfollowingLacoste–Julienetal.(2011). Thatis,givenanydistribution𝑞 indexedby𝝀∈Λ
𝝀
andconsideringtheSVGPprioraugmentation𝑝(𝑓)→𝑝(𝑓 ∣𝒖)𝑝(𝒖),theacquisitionfunctioncanbe
lower-boundedthroughJensen’sinequalityas
log𝛼(𝒙;𝒟 )=log∫ 𝑢(𝒙,𝑓;𝒟 )𝜋(𝑓 ∣𝒟 )d𝑓
𝑡 𝑡 𝑡
𝑞 (𝑓,𝒖)
𝝀
=log∫ 𝑢(𝒙,𝑓;𝒟 )𝜋(𝑓,𝒖∣𝒟 ) d𝑓d𝒖
𝑡 𝑡 𝑞 (𝑓,𝒖)
𝝀
𝑞 (𝒖)𝑝(𝑓 ∣𝒖)
𝝀
=log∫ 𝑢(𝒙,𝑓;𝒟 ) 𝓁(𝒟 ∣𝑓)𝑝(𝑓 ∣𝒖)𝑝(𝒖) d𝑓d𝒖−log𝑍
𝑡 𝑡 𝑞 (𝒖)𝑝(𝑓 ∣𝒖)
𝝀
𝑢(𝒙,𝑓;𝒟 )𝓁(𝒟 ∣𝑓)𝑝(𝒖)
𝑡 𝑡
≥∫ log( )𝑝(𝑓 ∣𝒖)𝑞 (𝒖) d𝑓d𝒖−log𝑍, (7)
𝑞 (𝒖) 𝝀
𝝀
where𝑍isanormalizingconstant. Notethatthisderivationissimilartothederivationofexpectation-
maximization(Dempsteretal.,1977)andvariationallowerbounds(Jordanetal.,1999). Thus,this
lowerboundimpliesthat,bytheminorize-maximizeprinciple(Lange,2016),maximizingthelower
boundwithrespectto𝒙and𝝀approximatelysolvestheoriginalproblemofmaximizingthe(exact)
posterior-expectedutility.
ExpectedUtilityLower-Bound Uptoaconstantandrearrangingterms,maximizingEq.(7)is
equivalenttomaximizing
ℒ (𝝀,𝒙;𝒟 )≜𝔼 [log𝓁(𝒟 ∣𝑓)+log𝑝(𝒖)−log𝑞 (𝒖)+log𝑢(𝒙,𝑓;𝒟 )]
EULBO 𝑡 =𝔼𝑝(𝑓∣𝒖) [𝑞 ∑𝜆(𝒖 𝑛)
𝑡
log𝓁(𝑦𝑡
∣𝑓)]
−D (𝑞
(𝒖),𝑝(𝒖𝜆
))+𝔼
log𝑢(𝒙,𝑓𝑡
;𝒟 )
𝑞𝜆(𝑓) 𝑖=1 𝑖 KL 𝝀 𝑞𝜆(𝑓) 𝑡
=ℒ (𝝀;𝒟 )+𝔼 log𝑢(𝒙,𝑓;𝒟 ), (8)
ELBO 𝑡 𝑞𝝀(𝑓) 𝑡
whichisthejointobjectivefunctionalludedtoinEq.(6). WemaximizeEULBOtoobtain(𝒙 ,𝝀∗)=
𝑡+1
argmax ℒ (𝒙,𝝀),where𝒙 correspondsournextBO“query”.
𝒙∈𝒳,𝝀∈Λ EULBO 𝑡+1
FromEq.(8),theconnectionbetweentheEULBOandELBOisobvious: theEULBOisnow“nudging”
theELBOsolutiontowardhighutilityregions. Analternativeperspectiveisthatweareapproximating
4ageneralizedposterior weightedbytheutility(Table. 1byKnoblauchetal.,2022; Bissirietal.,
2016). Furthermore,Jaiswaletal.(2020,2023)provethattheresultingactionssatisfyconsistency
guaranteesunderassumptionstypicalinsuchresultsforvariationalinference(WangandBlei,2019).
HyperparametersandInducingPointLocations Forthehyperparameters𝜽 andinducingpoint
locations𝒁,weusethemarginallikelihoodtoperformmodelselection,whichiscommonpracticein
BO(Shahriarietal.,2015,§V.A).(Optimizingover𝒁 waspopularizedbySnelsonandGhahramani,
2005.) Followingsuit,wealsooptimizetheEULBOasafunctionof𝜽 and𝒁 as
{ }
maximize ℒ (𝝀,𝒙,𝜽,𝒁;𝒟 )≜ℒ (𝝀,𝒁,𝜽;𝒟 )+𝔼 log𝑢(𝒙,𝑓;𝒟 ) .
𝝀,𝒙,𝜽,𝒁
EULBO 𝑡 ELBO 𝑡 𝑞𝝀(𝑓) 𝑡
WeemphasizeherethattheSVGP-associatedparameters𝝀,𝜽,𝒁 havegradientsthataredetermined
bybothtermsabove. Thus,theexpectedlog-utilityterm𝔼 log𝑢(𝒙,𝑓;𝒟 )simultaneously
𝑓∼𝑞𝝀(𝑓) 𝑡
resultsinacquisitionof𝒙 anddirectlyinfluencestheunderlyingSVGPregressionmodel.
𝑡+1
3.2 EULBOforExpectedImprovement(EI)
TheEIacquisitionfunctioncanbeexpressedasaposterior-expectedutility,wheretheunderlying
“improvement”utilityfunctionisgivenbythedifferencebetweentheobjectivevalueofthequery,
𝑓(𝒙),andthecurrentbestobjectivevalue𝑦∗ =max {𝑦 ∣𝑦 ∈𝒟 }:
𝑡 𝑖=1,…,𝑡 𝑖 𝑖 𝑡
( )
𝑢 (𝒙,𝑓;𝒟 )≜ReLU 𝑓(𝒙)−𝑦∗ , (EI;Jonesetal.,1998) (9)
EI 𝑡 𝑡
whereReLU(𝑥)≜max(𝑥,0). Unfortunately,thisutilityisnotstrictlypositivewhenever𝑓(𝒙)≤𝑦∗.
Thus,wecannotimmediatelyplug𝑢 intotheEULBO.Whileitispossibletoaddasmallpositive
EI
constantto𝑢 andmakeitstrictlypositiveasdonebyKuśmierczyketal.(2019),thisresultsina
EI
looserJensengapinEq.(7),whichcouldbedetrimental. Thisalsointroducestheneedfortuningthe
constant,whichisnotstraightforward. Instead,definethefollowing“softimprovement”utility:
( )
𝑢 (𝒙,𝑓;𝒟 )≜softplus 𝑓(𝒙)−𝑦∗ ,
SEI 𝑡 𝑡
wherewereplacetheReLUinEq.(9)withsoftplus(𝑥)≜log(1+exp(𝑥)). softplus(𝑥)converges
totheReLUinbothextremesof𝑥 →−∞and𝑥 →∞. Thus,𝑢 behavessimilarlyto𝑢 ,butwill
SEI EI
beslightlymoreexplorativeduetopositivity.
ComputingtheEULBOanditsderivativesnowrequiresthecomputationof𝔼 log𝑢 (𝒙,𝑓;𝒟 ),
𝑓∼𝑞𝝀(𝑓) SEI 𝑡
which, unlike EI, does not have a closed-form. However, since the utility function only depends
onthefunctionvaluesof𝑓,theexpectationcanbeefficientlycomputedtohighprecisionthrough
one-dimensionalGauss-Hermitequadrature.
3.3 EULBOforKnowledgeGradient(KG)
Althoughnon-trivial,theKGacquisitionisalsoaposterior-expectedutility,wheretheunderlying
utilityfunctionisgivenbythemaximumpredictivemeanvalueanywhereintheinputdomainafter
conditioningonanewobservation(𝒙,𝑦):
[ ]
𝑢 (𝒙,𝑦;𝒟 )≜max 𝔼 𝑓(𝒙′)∣𝒟 ∪{(𝒙,𝑦)} . (KG;Frazier,2009;Garnett,2023)
KG 𝑡 𝑡
𝒙′∈𝒳
Notethattheutilityfunctionasdefinedaboveisnotnon-negative: themaximumpredictivemeanofa
Gaussianprocesscanbenegative. Forthisreason,theutilityfunctioniscommonly(andoriginally,
e.g. Frazier,2009,Eq. 4.11)writtenintheliteratureasthedifferencebetweenthenewmaximum
meanafterconditioningon(𝒙,𝑦)andthemaximummeanbeforehand:
[ ]
𝑢 (𝒙,𝑦;𝒟 )≜max 𝔼 𝑓(𝒙′)∣𝒟 ∪{(𝒙,𝑦)} −𝜇+,
KG 𝑡 𝒙′∈𝒳 𝑡 𝑡
[ ]
where𝜇+ ≜max 𝐸 𝑓(𝒙′′)∣𝒟 . Note𝜇+playstheroleofasimpleconstantasitdependson
𝑡 𝒙′′∈𝒳 𝑡 𝑡
neither𝒙nor𝑦. SimilarlytotheEIacquisition,thisutilityisnotstrictlypositive,andwethusdefine
its“soft”variant:
( )
𝑢 (𝒙,𝑦;𝒟 )≜softplus 𝑢 (𝒙,𝑦;𝒟 )−𝑐+ .
SKG 𝑡 KG 𝑡
Here,𝑐+actsas𝜇+bymaking𝑢 positiveasoftenaspossible. Thisisparticularlyimportantwhen
𝑡 KG
theGPpredictivemeanisnegativeasaconsequenceoftheobjectivevaluesbeingnegative. One
naturalchoiceofconstantisusing𝜇+;however,wefindthatsimplychoosing𝑐+ =𝑦+workswell
𝑡 𝑡
andismorecomputationallyefficient.
5One-shotKGEULBO. TheEULBOusing𝑢 resultsinanexpensivenestedoptimizationproblem.
SKG
Toaddressthis,weuseanapproachsimilartotheone-shotknowledgegradientmethodofBalandat
etal.(2020). Forclarity,wewilldefinethereparameterizationfunction
𝑦 (𝒙;𝜖)≜𝜇 (𝒙)+𝜎 (𝒙)𝜖,
𝝀 𝑖 𝑞𝜆 𝑞𝜆 𝑖
where, f (or an i.i.d. sam ) ple 𝜖 𝑖 ∼ 𝒩(0,1), computing 𝑦 𝑖 = 𝑦 𝝀(𝒙,𝜖 𝑖) is equivalent to sampling
𝑦 ∼𝒩 𝜇 (𝒙),𝜎 (𝒙) . Thisenablestheuseofthereparameterizationgradientestimator(Kingma
𝑖 𝑞𝜆 𝑞𝜆
andWelling,2014;Rezendeetal.,2014;TitsiasandLázaro-Gredilla,2014). Now,noticethattheKG
acquisitionfunctioncanbeapproximatedthroughMonteCarloas
1 ∑𝑆 1 ∑𝑆 [ ]
𝛼 (𝒙;𝒟)≈ 𝑢 (𝒙,𝑦 (𝒙;𝜖);𝒟 )= max𝔼 𝑓(𝒙′)∣𝒟 ∪{𝒙,𝑦 (𝒙;𝜖)} ,
KG 𝑆 KG 𝝀 𝑖 𝑡 𝑆 𝒙′ 𝑡 𝝀 𝑖
𝑖=1 𝑖=1
where 𝜖 ∼ 𝒩(0,1) are i.i.d. for 𝑖 = 1,…,𝑆. The one-shot KG approach absorbs the nested
𝑖
optimizationover𝒙′intoasimultaneousjointoptimizationover𝒙andameanmaximizerforeachof
theSsamples,𝒙′ 1,...,𝒙′
𝑆
suchthatmax 𝒙𝛼 KG(𝒙;𝒟 𝑡)≈max 𝒙,𝒙′ 1,...,𝒙′ 𝑆𝛼 1-KG(𝒙;𝒟),where
1 ∑𝑆 1 ∑𝑆 [ ]
𝛼 (𝒙;𝒟 )≜ 𝑢 (𝒙,𝒙′,𝑦 (𝒙;𝜖);𝒟 )= 𝔼 𝑓(𝒙′)∣𝒟 ∪{𝒙,𝑦 (𝒙;𝜖)} ,
1-KG 𝑡 𝑆 1-KG 𝑖 𝝀 𝑖 𝑡 𝑆 𝑖 𝑡 𝝀 𝑖
𝑖=1 𝑖=1
Evidently,thereisnolongeraninneroptimizationproblemover𝒙′. Toestimatethe𝑖thtermofthis
sum,wedrawasampleoftheobjectivevalueof𝒙,𝑦 (𝒙;𝜖),andconditionthemodelonthissample.
𝝀 𝑖
Wethencomputethenewposteriorpredictivemeanat𝒙′. Aftersumming,wecomputegradients
𝑖
withrespecttoboththecandidate𝒙andthemeanmaximizers𝒙′,...,𝒙′. Again,weusethe“soft”
1 𝑆
versionofone-shotKGinourEULBOoptimizationproblem:
( ) ( [ ] )
𝑢 𝒙,𝒙′,𝑦;𝒟 =softplus 𝔼 𝑓(𝒙′)∣𝒟 ∪{(𝒙,𝑦)} −𝑐+ ,
1-SKG 𝑡 𝑡
wherethisutilityfunctioniscruciallyafunctionofboth𝒙andafreeparameter𝒙′. Aswith𝛼 ,
1-KG
maximizingtheEULBOcanbesetupasajointoptimizationproblem:
1 ∑𝑆 ( )
maximize ℒ (𝝀,𝒁,𝜽)+ log𝑢 𝒙,𝒙′,𝑦 (𝒙;𝜖);𝒟 (10)
𝒙,𝒙′,...,𝒙′,𝝀,𝒁,𝜽 ELBO 𝑆 1-SKG 𝑖 𝝀 𝑖 𝑡
1 𝑆 𝑖=1
EfficientKG-EULBOComputation. Computingthenon-ELBOterminEquation10isdominated
[ ]
byhavingtocompute𝔼 𝑓(𝒙′)∣𝒟 ∪{(𝒙,𝑦 (𝒙;𝜖))} 𝑆-times. Noticethatweonlyneedtocompute
𝑖 𝑡 𝝀 𝑖
anupdatedposteriorpredictivemean,andcanignorepredictivevariances. Forthis,wecanleverage
onlineupdating(Maddoxetal.,2021). Inparticular,thepredictivemeancanbeupdatedin𝒪(𝑚2)
time using a simple Cholesky update. The additional 𝒪(𝑆𝑚2) cost of computing the EULBO is
thereforeamortizedbytheoriginal𝒪(𝑚3)costofcomputingtheELBO.
3.4 Extensiontoq-EULBOforBatchBayesianOptimization
TheEULBOcanbeextendedtosupportbatchBayesianoptimizationbyusingtheMonteCarlobatch
modeanalogsofutilityfunctionsasdiscussede.g. byBalandatetal.(2020);Wilsonetal.(2018).
[ ]
Givenasetofcandidates𝑿 = 𝒙 ,...,𝒙 ,the𝑞-improvementutilityfunctionisgivenby:
1 𝑞
( ( ) )
𝑢 (𝑿,𝒇;𝒟 )≜ max ReLU 𝑓 𝒙 −𝑦∗ (q-EI;Balandatetal.,2020;Wilsonetal.,2018)
q-I 𝑡 𝑗 𝑡
𝑗=1...𝑞
Thisutilitycanagainbesoftenedas:
( ( ) )
𝑢 (𝑿,𝒇;𝒟 )≜ max softplus 𝑓 𝒙 −𝑦∗
q-SI 𝑡 𝑗 𝑡
𝑗=1…𝑞
Because this is now a 𝑞-dimensional integral, Gauss-Hermite quadrature is no longer applicable.
However,wecanapplyMonteCarloas
1 ∑𝑆 ( )
𝔼 log𝑢 (𝑿,𝒇;𝒟 )≈ max softplus 𝑦 (𝒙;𝜖)−𝑦∗ .
𝑞𝝀(𝑓) 𝑞-SI 𝑡 𝑆 𝑗=1...𝑞 𝝀 𝑖 𝑡
𝑖=1
AsdoneintheBoTorchsoftwarepackage(Balandatetal.,2020),weobservethatfixingthesetof
basesamples𝜖 ,...,𝜖 duringeachBOiterationresultsinbetteroptimizationperformanceatthecost
1 𝑆
ofnegligibleq-EULBObias. Now,optimizingtheq-EULBOisdoneoverthefullsetof𝑞candidates
𝒙 ,...,𝒙 jointly,aswellastheGPhyperparameters,inducingpoints,andvariationalparameters.
1 𝑞
6KnowledgeGradient. TheKGversionoftheEULBOcanbesimilarlyextended. Theexpectedlog
utilityterminthemaximizationproblemEq.(10)becomes:
1
∑𝑆
maximize ℒ (𝝀,𝒁,𝜽)+ max log𝑢 (𝒙 ,𝒙′,𝑦 (𝒙;𝜖);𝒟 ),
𝒙1,...,𝒙𝑞,𝒙′ 1,...,𝒙′ 𝑆,𝝀,𝒁,𝜽 ELBO 𝑆 𝑖=1𝑗=1..𝑞 1-SKG 𝑗 𝑖 𝝀 𝑖 𝑡
resultinginasimilaranalogtoq-KGasdescribedbyBalandatetal.(2020).
3.5 OptimizingtheEULBO
OptimizingtheELBOforSVGPsisknowntobechallenging(Galy-FajouandOpper,2021;Terenin
etal.,2024)astheoptimizationlandscapefortheinducingpointsisnon-convex,multi-modal,and
non-smooth. Naturally,thesearealsochallengesforEULBO.Inpractice,wefoundthatcaremustbe
takenwhenimplementingandinitializingtheEULBOmaximizationproblem. Inthissubsection,we
outlinesomekeyideas,whileadetaileddescriptionwithpseudocodeispresentedinAppendixA.
InitializationandWarm-Starting. Wewarm-starttheEULBOmaximizationprocedurebysolving
theconventionaltwo-stepschemeinEq.(5): AteachBOiteration, weobtainthe“warm”initial
valuesfor(𝝀,𝒁,𝜽)byoptimizingthestandardELBO.Then,weusethistomaximizetheconventional
acquisitionfunctioncorrespondingtothechosenutilityfunction𝑢(theexpectationof𝑢over𝑞 (𝑓)),
𝝀
whichprovidesthewarm-startinitializationfor𝒙.
AlternatingMaximizationScheme. Tooptimizeℒ (𝒙,𝝀,𝒁,𝜽),wealternatebetweenopti-
EULBO
mizingoverthequery𝒙andtheSVGPparameters𝝀,𝒁,𝜽. Wefindblock-coordinatedescenttobe
morestableandrobustthanjointlyupdatingallparameters,thoughthereasonwhythisismorestable
thanjointlyoptimizingallparametersrequiresfurtherinvestigation.
4 Experiments
We evaluate EULBO-based SVGPs on a number of benchmark BO tasks, described in detail in
Section4.1. Thesetasksincludestandardlow-dimensionalBOproblems,e.g.,the6DHartmann
function,aswellas7high-dimensionalandhigh-throughputoptimizationtasks.
Baselines. WecompareEULBOtoseveralbaselineswiththemaingoalofachievingahighreward
usingasfewfunctionevaluationsaspossible. OurprimarypointofcomparisonisELBO-basedSVGPs.
Weconsidertwoapproachesforinducingpointlocations: 1.optimizinginducingpointlocationsvia
theELBO(denotedasELBO),2.placingtheinducingpointsusingthestrategyproposedbyMoss
etal.(2023)ateachstageofELBOoptimization(denotedasMossetal.). Thelatteroffersimproved
BOperformanceoverstandardELBO-SVGPinBOsettings,yet—unlikeourmethod—itexclusively
targetsinducingpointplacementanddoesnotaffectvariationalparametersorhyperparametersofthe
model. Inaddition,wecomparetoBOusingexactGPsusing2,000functionevaluationsastheuse
ofexactGPisintractablebeyondthispointduetotheneedtorepeatedlyfitmodels.
AcquisitionfunctionsandBOalgorithms. ForEULBO,wetesttheversionsbasedonboththe
Expected Improvement (EI) and Knowledge Gradient (KG) acquisition functions as well as the
batchvariant. WetestthebaselinemethodsusingEIonly. Onhigh-dimensionaltasks(taskswith
dimensionalityabove10),werunEULBOandbaselinemethodswithstandardBOandwithtrustregion
Bayesianoptimization(TuRBO)(Erikssonetal.,2019). Forthelargesttasks(Lasso,Molecules)we
useacquisitionbatchsizeof20(𝑞 =20),andbatchsize1(𝑞 =1)forallothers.
ImplementationDetailsandHyperparameters. Wewillprovidecodetoreproduceallresults
inthepaperinapublicGitHubrepository. WeimplementEULBOandbaselinemethodsusingthe
GPyTorch(Gardneretal.,2018)andBoTorch(Balandatetal.,2020)packages. Forallmethods,we
initializeusingasetof100datapointssampleduniformlyatrandominthesearchspace. Weusethe
sametrustregionhyperparametersasin(Erikssonetal.,2019). InAppendixB.1,wealsoevaluatean
additionalinitializationstrategyforthemoleculardesigntasks. Thisalternativeinitializationmatches
priorworkinusing10,000moleculesfromtheGuacaMoldatasetBrownetal.(2019)ratherthanthe
detailsweusedaboveforconsistencyacrosstasks,butdoesachievehigheroverallperformance.
4.1 Tasks
Hartmann6D. ThewidelyusedHartmannbenchmarkfunction(SurjanovicandBingham,2013).
7Figure 2: Optimization results on the 8 considered tasks. We compare all methods for both
standardBOandTuRBO-basedBO(onalltasksexceptHartmann). Eachline/shadedregionrepresents
themean/standarderrorover20runsSeesubsectionB.1foradditionalmoleculeresults.
Figure 3: Ablation study measuring the impact of EULBO optimization on various SVGP
parameters. At each BO iteration, we use the standard ELBO objective to optimize the SVGP
hyperparameters,variationalparameters,andinducingpointlocations. Wethenrefinesomesubsetof
theseparametersbyfurtheroptimizingthemwithrespecttotheEULBOobjective.
LunarLander. Thegoalofthistaskistofindanoptimal12-dimensionalcontrolpolicythatallows
an autonomous lunar lander to consistently land without crashing. The final objective value we
optimizeistherewardobtainedbythepolicyaveragedoverasetof50randomlandingterrains. For
thistask,weusethesamecontrollersetupusedbyErikssonetal.(2019).
Rover. TherovertrajectoryoptimizationtaskintroducedbyWangetal.(2018)consistsoffinding
a60-dimensionalpolicythatallowsarovertomovealongsometrajectorywhileavoidingasetof
obstacles. WeusethesameobstaclesetupasinMausetal.(2023).
LassoDNA. Weoptimizethe180−dimensionalDNAtaskfromtheLassoBenchlibrary(Šehić
etal.,2022)ofbenchmarksbasedonweightedLASSOregression(Gassoetal.,2009).
Moleculardesigntasks(x4). WeselectfourchallengingtasksfromtheGuacamolbenchmark
suiteofmoleculardesigntasks(Brownetal.,2019): OsimertinibMPO,FexofenadineMPO,Median
Molecules1,andMedianMolecules2. WeusetheSELFIES-VAEintroducedbyMausetal.(2022)
toenablecontinuous256dimensionaloptimization.
84.2 Optimizationresults
InFigure2,weplottherewardofthebestpointfoundbytheoptimizerafteragivennumberoffunction
evaluations. Errorbarsshowthestandarderrorofthemeanover20replicateruns. EULBOwithTuRBO
outperformstheotherbaselineswithTuRBO.Similarly, EULBOwithstandardBOoutperformsthe
otherstandardBObaselines. Onenoteworthyobservationisthatneitheracquisitionfunctionappears
toconsistentlyoutperformtheother. However,EULBO-SVGPalmostalwaysdominatesELBO-SVGP
andoftenrequiresasmallfractionofthenumberoforaclecallstoachievecomparableperformance.
These results suggest that coupling data acquisition with approximate inference/model selection
resultsinsignificantlymoresample-efficientoptimization.
4.3 AblationStudy
WhiletheresultsinFig.2demonstratethatEULBO-SVGPimprovestheBOperformanceitisnot
immediatelycleartowhatextentjointoptimizationmodifiestheposteriorapproximationbeyondwhat
isobtainedbystandardELBOoptimization. Tothatend,inFig.3werefineanELBO-SVGPmodel
withvaryingdegreesofadditionalEULBOoptimization. AteveryBOiterationwebeginbyobtaininga
SVGPmodel(wherethevariationalparameters,inducingpointlocations,andGPhyperparametersare
allobtainedbyoptimizingthestandardELBOobjective). Wethenrefinesomesubsetofparameters
(eithertheinducingpoints,thevariationalparameters,theGPhyperparameters,oralloftheabove)
throughadditionaloptimizationwithrespecttotheEULBOobjective. Interestingly,wefindthattasks
responddifferentlytothevaryinglevelsofEULBOrefinement. InthecaseofLassoDNA,thereisnot
muchofadifferencebetweenEULBOrefinementonallparametersversusrefinementonthevariational
parametersalone. Ontheotherhand,theperformanceonMedianMolecules2isclearlydominatedby
refinementonallparameters. Nevertheless,weseethatEULBOisalwaysbeneficial,whetherapplied
toallparametersorsomesubset.
5 RelatedWork
ScalingBayesianoptimizationtotheLarge-BudgetRegime. BOhastraditionallybeenconfined
tothesmall-budgetoptimizationregime. However,recentinterestinhigh-dimensionaloptimization
problemshasdemonstratedtheneedtoscaleBOtolargedataacquisitionbudgets. Forproblems
with𝑂(103)dataacquisitions, Hernández-Lobatoetal.(2017);Snoeketal.(2015);Springenberg
et al. (2016) consider Bayesian neural networks (BNN; Neal, 1996), McIntire et al. (2016) use
SVGP,andWangetal.(2018)turntoensemblesofsubsampledGPs. Forproblemswith≫ 1000
acquisitions,SVGPhasbecomethedefactoapproachtoalleviatecomputationalcomplexity(Griffiths
andHernández-Lobato,2020;Mausetal.,2022,2023;Stantonetal.,2022;Trippetal.,2020;Vakili
etal.,2021). Asinthispaper, manyworkshaveproposedmodificationstoSVGPtoimproveits
performanceinBOapplications. Mossetal.(2023)proposedaninducingpointplacementbasedona
heuristicmodificationofdeterminantalpointprocesses(KuleszaandTaskar,2012),whichweused
forinitialization,whileMaddoxetal.(2021)proposedamethodforafastonlineupdatestrategyfor
SVGPs,whichweutilizefortheKGacquisitionstrategy.
Utility-CalibratedApproximateInference. ThiswasfirstproposedbyLacoste–Julienetal.(2011),
wheretheauthorsuseacoordinateascentalgorithmtoperformloss-calibratedvariationalinference.
Sincethen,variousextensionshavebeenproposed: Kuśmierczyketal.(2019)leverageblack-box
variationalinference(Ranganathetal.,2014;TitsiasandLázaro-Gredilla,2014); MoraisandPillow
(2022)useexpectation-propagation(EP;Minka,2001); Abbasnejadetal.(2015)employimportance
sampling; Cobbetal.(2018)andLiandZhang(2023)deriveaspecificvariantforBNNs;and (Wei
et al., 2021) derive a specific variant for GP classification. Closest to our work is the GP-based
recommendationmodellearningalgorithmbyAbbasnejadetal.(2013),whichsparsifiesanEP-based
GPapproximationbymaximizingautilitysimilartothoseusedinBO.
6 LimitationsandDiscussion
Themainlimitationofourproposedapproachisincreasedcomputationalcost. WhileEULBO-SVGP
stillretainsthe𝑂(𝑚3)computationalcomplexityofstandardSVGP,ourpracticalimplementation
requiresawarm-start: firstfittingSVGPwiththeELBOlossandthenmaximizingtheacquisition
functionbeforejointlyoptimizingwiththeEULBOloss. Furthermore,EULBOoptimizationcurrently
requires multiple tricks such as clipping and block-coordinate updates. In future work, we aim
9to develop a better understanding of the EULBO geometry in order to develop developing more
stable,efficient,andeasy-to-useEULBOoptimizationschemes. Nevertheless,ourresultsinSection4
demonstratethattheadditionalcomputationofEULBOyieldssubstantialimprovementsinBOdata-
efficiency, a desirable trade-off in many applications. Moreover, EULBO-SVGP is modular, and
ourexperimentscaptureafractionofitspotentialuse. Itcanbeappliedtoanydecision-theoretic
acquisitionfunction,anditislikelycompatiblewithnon-standardBayesianoptimizationproblems
suchascost-constrainedBO(Snoeketal.,2012),causalBO(Agliettietal.,2020),andmanymore.
Moreimportantly,ourpaperhighlightsanewavenueforresearchinBO,whereourpaperisthefirst
tojointlyconsidersurrogatemodeling,approximateinference,anddataselection. Extendingthis
ideatoGPapproximationsbeyondSVGPandacquisitionfunctionsbeyondEI/KGmayyieldfurther
improvements,especiallyintheincreasinglypopularhigh-throughputBOsetting.
10References
EhsanAbbasnejad,JustinDomke,andScottSanner. Loss-calibratedMonteCarloactionselection. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume29ofAAAI.AAAIPress,
March2015. (page9)
M.EhsanAbbasnejad, EdwinV.Bonilla, andScottSanner. Decision-theoreticsparsificationfor
Gaussianprocesspreferencelearning.InMachineLearningandKnowledgeDiscoveryinDatabases,
volume13717ofLNCS,pages515–530,Berlin,Heidelberg,2013.Springer. (page9)
VirginiaAglietti,XiaoyuLu,AndreiPaleyes,andJavierGonzález. CausalBayesianoptimization. In
ProceedingsoftheInternationalConferenceonArtificialIntelligenceandStatistics,volume108of
PMLR,pages3155–3164.JMLR,June2020. (page10)
Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, An-
drew Gordon Wilson, and Eytan Bakshy. BoTorch: A framework for efficient Monte-Carlo
Bayesianoptimization. InAdvancesinNeuralInformationProcessingSystems,volume33,pages
21524–21538.CurranAssociates,Inc.,2020. (pages2,6,7,16)
P.G.Bissiri,C.C.Holmes,andS.G.Walker. Ageneralframeworkforupdatingbeliefdistributions.
JournaloftheRoyalStatisticalSocietySeriesB:StatisticalMethodology,78(5):1103–1130,2016.
(pages2,5)
DavidM.Blei,AlpKucukelbir,andJonD.McAuliffe.Variationalinference: Areviewforstatisticians.
JournaloftheAmericanStatisticalAssociation,112(518):859–877,April2017. (pages2,3)
NathanBrown,MarcoFiscato,MarwinH.S.Segler,andAlainC.Vaucher. Guacamol: Benchmarking
models for de novo molecular design. Journal of Chemical Information and Modeling, 59(3):
1096–1108,Mar2019. (pages7,8)
Adam D. Cobb, Stephen J. Roberts, and Yarin Gal. Loss-Calibrated Approximate Inference in
BayesianNeuralNetworks. arXivPreprintarXiv:1805.03901,arXiv,May2018. (page9)
A.P.Dempster,N.M.Laird,andD.B.Rubin. Maximumlikelihoodfromincompletedataviathe
EMalgorithm. JournaloftheRoyalStatisticalSociety: SeriesB(Methodological),39(1):1–22,
September1977. (page4)
DavidEriksson,MichaelPearce,JacobGardner,RyanDTurner,andMatthiasPoloczek. Scalable
globaloptimizationvialocalBayesianoptimization. InAdvancesinNeuralInformationProcessing
Systems,volume32,pages5496–5507.CurranAssociates,Inc.,2019. (pages1,2,7,8)
PeterIFrazier. Knowledge-gradientmethodsforstatisticallearning. PhDthesis,PrincetonUniversity
Princeton,2009. (page5)
PeterIFrazier. AtutorialonBayesianoptimization. arXivPreprintarXiv:1807.02811,ArXiv,2018.
(page1)
ThéoGaly-FajouandManfredOpper. AdaptiveinducingpointsselectionforGaussianprocesses.
arXivPreprintarXiv:2107.10066,arXiv,2021. (page7)
JacobGardner,GeoffPleiss,KilianQ.Weinberger,DavidBindel,andAndrewG.Wilson. GPyTorch:
Blackboxmatrix-matrixGaussianprocessinferencewithGPUacceleration. InAdvancesinNeural
Information Processing Systems, volume 31, pages 7576–7586. Curran Associates, Inc., 2018.
(pages7,16)
RomanGarnett. BayesianOptimization. CambridgeUniversityPress,Cambridge,UnitedKingdom;
NewYork,NY,2023. (pages1,2,3,5)
GillesGasso,AlainRakotomamonjy,andStéphaneCanu. Recoveringsparsesignalswithacertain
familyofnonconvexpenaltiesandDCprogramming. IEEETransactionsonSignalProcessing,57
(12):4686–4698,2009. (page8)
Ryan-RhysGriffithsandJoséMiguelHernández-Lobato. ConstrainedBayesianoptimizationfor
automatic chemical design using variational autoencoders. Chemical Science, 11(2):577–586,
2020. (pages1,9)
JamesHensman,NicoloFusi,andNeilD.Lawrence. Gaussianprocessesforbigdata. InProceedings
of the Conference on Uncertainty in Artificial Intelligence, pages 282–290. AUAI Press, 2013.
(pages1,3)
11JoséMiguelHernández-Lobato,JamesRequeima,EdwardO.Pyzer-Knapp,andAlánAspuru-Guzik.
ParallelanddistributedThompsonsamplingforlarge-scaleacceleratedexplorationofchemical
space. InProceedingsoftheInternationalConferenceonMachineLearning,volume70ofPMLR,
pages1470–1479.JMLR,July2017. (page9)
PrateekJaiswal,HarshaHonnappa,andVinayakA.Rao. Asymptoticconsistencyofloss-calibrated
variationalBayes. Stat,9(1):e258,2020. (pages2,5)
PrateekJaiswal,HarshaHonnappa,andVinayakRao. Onthestatisticalconsistencyofrisk-sensitive
bayesiandecision-making. InAdvancesinNeuralInformationProcessingSystems,volume36,
pages53158–53200.CurranAssociates,Inc.,December2023. (pages2,5)
DonaldR.Jones,MatthiasSchonlau,andWilliamJ.Welch.Efficientglobaloptimizationofexpensive
black-boxfunctions. JournalofGlobalOptimization,13(4):455–492,1998. (pages1,2,5)
MichaelI.Jordan,ZoubinGhahramani,TommiS.Jaakkola,andLawrenceK.Saul. Anintroduction
tovariationalmethodsforgraphicalmodels. MachineLearning,37(2):183–233,1999. (pages1,2,
3,4)
DiederikP.KingmaandJimmyBa. Adam: AMethodforStochasticOptimization. InProceedings
oftheInternationalConferenceonLearningRepresentations,SanDiego,California,USA,2015.
(pages15,16)
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of the
InternationalConferenceonLearningRepresentations,Banff,AB,Canada,April2014. (page6)
JeremiasKnoblauch,JackJewson,andTheodorosDamoulas. Anoptimization-centricviewonBayes’
rule: Reviewingandgeneralizingvariationalinference. JournalofMachineLearningResearch,23
(132):1–109,2022. (pages2,5)
AlexKuleszaandBenTaskar. Determinantalpointprocessesformachinelearning. Foundationsand
Trends®inMachineLearning,5(2–3):123–286,2012. (page9)
TomaszKuśmierczyk,JosephSakaya,andArtoKlami. VariationalBayesiandecision-makingfor
continuousutilities. InAdvancesinNeuralInformationProcessingSystems,volume32,pages
6395–6405.CurranAssociates,Inc.,2019. (pages5,9)
Simon Lacoste–Julien, Ferenc Huszár, and Zoubin Ghahramani. Approximate inference for the
loss-calibratedBayesian. InProceedingsoftheInternationalConferenceonArtificialIntelligence
andStatistics,volume15ofPMLR,pages416–424.JMLR,June2011. (pages2,4,9)
Kenneth Lange. MM Optimization Algorithms. Society for Industrial and Applied Mathematics,
Philadelphia,2016. (pages2,4)
BolianLiandRuqiZhang. Long-tailedClassificationfromaBayesian-decision-theoryPerspective.
arXivPreprintarXiv:2303.06075,arXiv,2023. (page9)
WesleyJMaddox,SamuelStanton,andAndrewGWilson. ConditioningsparsevariationalGaussian
processesforonlinedecision-making. InAdvancesinNeuralInformationProcessingSystems,
volume34,pages6365–6379.CurranAssociates,Inc.,2021. (pages1,2,4,6,9)
Alexander G. de G. Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani. On
sparsevariationalmethodsandtheKullback-Leiblerdivergencebetweenstochasticprocesses. In
ProceedingsoftheInternationalConferenceonArtificialIntelligenceandStatistics,volume51of
PMLR,pages231–239.JMLR,May2016. (pages1,4)
NatalieMaus,HaydnJones,JustonMoore,MattJ.Kusner,JohnBradshaw,andJacobGardner. Local
latent space Bayesian optimization over structured inputs. In Advances in Neural Information
ProcessingSystems,volume35,pages34505–34518,December2022. (pages1,8,9)
NatalieMaus,KaiwenWu,DavidEriksson,andJacobGardner. Discoveringmanydiversesolutions
withBayesianoptimization.InProceedingsoftheInternationalConferenceonArtificialIntelligence
andStatistics,volume206,pages1779–1798.PMLR,April2023. (pages1,8,9)
Mitchell McIntire, Daniel Ratner, and Stefano Ermon. Sparse Gaussian Processes for Bayesian
Optimization. InProceedingsoftheConferenceonUncertaintyinArtificialIntelligence,Jersey
City,NewJersey,USA,2016.AUAIPress. (page9)
ThomasP.Minka. Expectationpropagationforapproximatebayesianinference. InProceedingsof
theConferenceonUncertaintyinArtificialIntelligence,pages362–369,SanFrancisco,CA,USA,
2001.MorganKaufmannPublishersInc. (page9)
12JonasMockus. TheBayesianapproachtoglobaloptimization. InSystemModelingandOptimization,
pages473–481.Springer,1982. (page1)
MichaelJ.MoraisandJonathanW.Pillow. Loss-calibratedexpectationpropagationforapproximate
Bayesiandecision-making. TechnicalReportarXiv:2201.03128,arXiv,January2022. (page9)
HenryB.Moss,SebastianW.Ober,andVictorPicheny. InducingpointallocationforsparseGaussian
processesinhigh-throughputBayesianoptimisation.InProceedingsoftheInternationalConference
onArtificialIntelligenceandStatistics,volume206ofPMLR,pages5213–5230.JMLR,April
2023. (pages1,4,7,9,16,17,18)
RadfordM.Neal. BayesianLearningforNeuralNetworks,volume118ofLectureNotesinStatistics.
SpringerNewYork,NewYork,NY,1996. (page9)
JoaquinQuiñonero-CandelaandCarlEdwardRasmussen. Aunifyingviewofsparseapproximate
Gaussianprocessregression. JournalofMachineLearningResearch,6(65):1939–1959,2005.
(page1)
RajeshRanganath,SeanGerrish,andDavidBlei. Blackboxvariationalinference. InProceedingsof
theInternationalConferenceonArtificialIntelligenceandStatistics,volume33ofPMLR,pages
814–822.JMLR,April2014. (page9)
CarlEdwardRasmussenandChristopherK.I.Williams. GaussianProcessesforMachineLearning.
TheMITPress,November2005. (page1)
DaniloJimenezRezende,ShakirMohamed,andDaanWierstra. Stochasticbackpropagationand
approximateinferenceindeepgenerativemodels. InProceedingsoftheInternationalConference
onMachineLearning,volume32ofPMLR,pages1278–1286.JMLR,June2014. (page6)
ChristianP.Robert. TheBayesianChoice: FromDecision-TheoreticFoundationstoComputational
Implementation. SpringerTextsinStatistics.Springer,NewYorkBerlinHeidelberg,2.ededition,
2001. (pages2,3)
BobakShahriari,KevinSwersky,ZiyuWang,RyanPAdams,andNandoDeFreitas. Takingthe
human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1):
148–175,2015. (pages1,5)
Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In
AdvancesinNeuralInformationProcessingSystems,volume18,pages1257–1264.MITPress,
2005. (page5)
JasperSnoek,HugoLarochelle,andRyanPAdams. PracticalBayesianoptimizationofmachine
learningalgorithms. Advancesinneuralinformationprocessingsystems,25:2951–2959,2012.
(page10)
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
MostofaPatwary,MrPrabhat,andRyanAdams. ScalableBayesianoptimizationusingdeepneural
networks. InProceedingsoftheInternationalConferenceonMachineLearning,volume37of
PMLR,pages2171–2180.JMLR,June2015. (page9)
JostTobiasSpringenberg,AaronKlein,StefanFalkner,andFrankHutter. BayesianOptimization
withRobustBayesianNeuralNetworks. InAdvancesinNeuralInformationProcessingSystems,
volume29,pages4134–4142.CurranAssociates,Inc.,2016. (page9)
SamuelStanton,WesleyMaddox,NateGruver,PhillipMaffettone,EmilyDelaney,PeytonGreenside,
andAndrewGordonWilson. AcceleratingBayesianoptimizationforbiologicalsequencedesign
withdenoisingautoencoders.InProceedingsoftheInternationalConferenceonMachineLearning,
volume162ofPMLR,pages20459–20478.JMLR,June2022. (pages1,9)
SonjaSurjanovicandDerekBingham. Virtuallibraryofsimulationexperiments: Testfunctionsand
datasets,2013. (page7)
AlexanderTerenin,DavidR.Burt,ArtemArtemev,SethFlaxman,MarkvanderWilk,CarlEdward
Rasmussen,andHongGe. NumericallystablesparseGaussianprocessesviaminimumseparation
usingcovertrees. JournalofMachineLearningResearch,25(26):1–36,2024. (page7)
Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In
ProceedingsoftheInternationalConferenceonArtificialIntelligenceandStatistics,volume5of
PMLR,pages567–574.JMLR,April2009. (pages1,3)
13MichalisTitsiasandMiguelLázaro-Gredilla. DoublystochasticvariationalBayesfornon-conjugate
inference. InProceedingsoftheInternationalConferenceonMachineLearning,volume32of
PMLR,pages1971–1979.JMLR,June2014. (pages6,9)
AustinTripp,ErikDaxberger,andJoséMiguelHernández-Lobato. Sample-efficientoptimization
in the latent space of deep generative models via weighted retraining. In Advances in Neural
InformationProcessingSystems,volume33,pages11259–11272.CurranAssociates,Inc.,2020.
(pages1,9)
Sattar Vakili, Henry Moss, Artem Artemev, Vincent Dutordoir, and Victor Picheny. Scalable
ThompsonsamplingusingsparseGaussianprocessmodels. InAdvancesinNeuralInformation
ProcessingSystems,volume34,pages5631–5643,2021. (pages1,9)
KenanŠehić,AlexandreGramfort,JosephSalmon,andLuigiNardi.Lassobench: Ahigh-dimensional
hyperparameteroptimizationbenchmarksuiteforLASSO. InProceedingsoftheInternational
ConferenceonAutomatedMachineLearning,volume188ofPMLR,pages2/1–24.JMLR,25–27
Jul2022. (page8)
YixinWangandDavidM.Blei.FrequentistconsistencyofvariationalBayes.JournaloftheAmerican
StatisticalAssociation,114(527):1147–1161,July2019. (page5)
ZiWang,ClementGehring,PushmeetKohli,andStefanieJegelka. Batchedlarge-scalebayesian
optimization in high-dimensional spaces. In Proceedings of the International Conference on
ArtificialIntelligenceandStatistics,volume84ofPMLR,pages745–754.JMLR,March2018.
(pages8,9)
LarryWasserman. Allofstatistics: aconcisecourseinstatisticalinference. SpringerScience&
BusinessMedia,2013. (pages2,3)
YadiWei,RishitSheth,andRoniKhardon. DirectlossminimizationforsparseGaussianprocesses.
InProceedingsoftheInternationalConferenceonArtificialIntelligenceandStatistics,volume130
ofPMLR,pages2566–2574.JMLR,March2021. (page9)
JamesWilson,FrankHutter,andMarcDeisenroth. MaximizingacquisitionfunctionsforBayesian
optimization. InAdvancesinNeuralInformationProcessingSystems,pages9884–9895.Curran
Associates,Inc.,2018. (pages2,6)
Jian Wu, Matthias Poloczek, Andrew G Wilson, and Peter Frazier. Bayesian optimization with
gradients. InAdvancesinNeuralInformationProcessingSystems,volume30,pages5267–5278.
CurranAssociates,Inc.,2017. (page2)
14A ImplementationDetails
Wewillnowprovideadditionaldetailsontheimplementation. Fortheimplementation, wetreat
the SVGP parameters, such as the variational parameters 𝝀, inducing point locations 𝒁, and
hyperparameters𝜽,equally. Therefore,forclarity,wewillcollectivelydenotethemas𝒘=(𝝀,𝒁,𝜽)
suchthat𝒘∈𝒲 =Λ×𝒳𝑚×Θ,andtheresultingSVGPvariationalapproximationas𝑞 . Then,
𝒘
theELBOandEULBOareequivalentlydenotedasfollows:
ℒ (𝒘;𝒟)≜ℒ (𝝀,𝒁,𝜽;𝒟)
ELBO ELBO
ℒ (𝒙,𝒘;𝒟 ,𝒟 )≜𝔼 log𝑢(𝒙,𝑓;𝒟 )+ℒ (𝒘;𝒟 ).
EULBO 𝒙 𝒘 𝑓∼𝑞𝒘(𝑓) 𝒙 ELBO 𝒘
Also,noticethattheℒ separatelydenotethedatasettobepassedtotheutilityandtheELBO.
EULBO
(Setting𝒟 =𝒟 =𝒟 retrievestheoriginalformulationinEq.(8).)
𝑡 𝒘 𝒙
AlternatingUpdates Weperformblock-coordinateascentontheEULBObyalternatingbetween
maximizingover𝒙as𝒘. Usingvanillagradientdescent,the𝒙-updateisequivalentto
𝒙←𝒙+𝛾 ∇ ℒ (𝒙,𝒘;𝒟)=𝒙+𝛾 ∇ 𝔼 log𝑢(𝒙,𝑓;𝒟),
𝒙 𝒙 EULBO 𝒙 𝒙 𝑓∼𝑞𝒘(𝑓)
where𝛾 isthestepsize. Ontheotherhand,forthe𝒘-update,wesubsamplethedatasuchthatwe
𝒙
optimizetheELBOoveraminibatch𝑆 ⊂𝒟ofsize𝐵 =|𝑆|as
( )
𝒘←𝒘+𝛾 ∇ ℒ (𝒙,𝒘;𝑆,𝒟)=𝒘+𝛾 ∇ 𝔼 log𝑢(𝒙,𝑓;𝒟)+ℒ (𝒘;𝑆) ,
𝒘 𝒘 EULBO 𝒘 𝒘 𝑓∼𝑞𝒘(𝑓) ELBO
where𝛾 isthestepsize. Naturally,the𝒘-updateisstochasticduetominibatching,whilethe𝒙-update
𝒘
isdeterministic. Inpractice,weleveragetheAdamupdaterule(KingmaandBa,2015)insteadof
simplegradientdescent. Togetherwithgradientclipping,thisalternatingupdateschemeismuch
morerobustthanjointlyupdating(𝒙,𝒘).
Algorithm1:EULBOMaximizationPolicy
Input: SVGPparameters𝒘 =(𝝀 ,𝒁 ,𝜽 ),Dataset𝒟 ,BOutilityfunction𝑢,
0 0 0 0 𝑡
Output: BOquery𝒙
𝑡+1
1
⊳ Compute Warm-Start Initializations
2 𝒘←argmax 𝒘∈𝒲ℒ ELBO(𝒘;𝒟 𝑡)with𝒘 0asinitialization.
3 𝒙←argmax 𝒙∈𝒳∫ 𝑢(𝒙,𝑓;𝒟 𝑡)𝑞 𝒘(𝑓)d𝑓
4
⊳ Maximize EULBO
5 repeat
⊳ Update posterior approximation 𝑞
𝒘
6 Fetchminibatch𝑆from𝒟 𝑡
7 Compute𝒈 𝒘 ←∇ 𝒘ℒ EULBO(𝒙,𝒘;𝑆,𝒟 𝑡)
8 Clip𝒈 𝒘withthreshold𝐺 clip
9 𝒘←AdamStep 𝛾𝒘(𝒙,𝒈 𝒘)
10
⊳ Update BO query 𝒙
11 Compute𝒈 𝒙 ←∇ 𝒙ℒ EULBO(𝒙,𝒘;𝑆,𝒟 𝑡)
12 Clip𝒈 𝒙withthreshold𝐺 clip
13 𝒙←AdamStep 𝛾𝒙(𝒙,𝒈 𝒙)
14 𝒙←proj (𝒙)
𝒳
15 untiluntilconverged
16 𝒙 𝑡+1 ←𝒙
17
OverviewofPseudocode. Thecompletehigh-levelviewofthealgorithmispresentedinAlgorithm1,
exceptfortheacquisition-specificdetails. AdamStep (𝒙,𝒈)appliestheAdamstepsizerule(Kingma
𝛾
andBa,2015)tothecurrentlocation𝒙withthegradientestimate𝒈andthestepsize𝛾. Inpractice,
Adamisa“stateful”optimizer,whichmaintainstwoscalar-valuedstatesforeachscalarparameter.
Forthis,were-initializetheAdamstatesatthebeginningofeachBOstep.
15Initialization. IntheinitialBOstep𝑡 = 0, weinitialize𝒁 withtheDPP-basedinducingpoint
0
selectionstrategyofMossetal.(2023). FortheremainingSVGPparameters𝝀 and𝜽 ,weusedthe
0 0
defaultinitializationofGPyTorch(Gardneretal.,2018). FortheremainingBOsteps𝑡 >0,weuse𝒘
fromthepreviousBOstepastheinitialization𝒘 ofthecurrentBOstep.
0
Warm-Starting. Duetothenon-convexityandmulti-modalityofboththeELBOandtheacquisition
function,itiscriticaltoappropriatelyinitializetheEULBOmaximizationprocedure. Asmentioned
inSection3.5,towarm-starttheEULBOmaximizationprocedure,weusetheconventional2-step
schemeEq.(5), wherewemaximizetheELBOandthenmaximizetheacquisitionfunction. For
ELBOmaximization,weapplyAdam(KingmaandBa,2015)withthestepsizesetas𝛾 untilthe
𝑤
convergencecriteria(describedbelow)aremet. Foracquisitionfunctionmaximization,weinvokethe
highlyoptimizedBoTorch.optimize.optimize_acqffunction(Balandatetal.,2020).
MinibatchSubsamplingStrategy. Ascommonlydone,weusethereshufflingsubsamplingstrategy
wherethedataset𝒟 isshuffledandpartitionedintominibatchesofsize𝐵. Thenumberofminibatches
𝑡
constitutesan“epoch.” Thedatasetisreshuffled/repartitionedaftergoingthroughafullepoch.
ConvergenceDetermination. ForbothmaximizingtheELBOduringwarm-startingandmaximizing
theEULBO,wecontinueoptimizationuntilwestopmakingprogressorexceed𝑘 numberof
epochs
epochs. ThatisiftheELBO/EULBOfunctionvaluefailstomakeprogressfor𝑛 numberofsteps.
fail
Table1: ConfigurationsofHyperparametersusedfortheExperiments
Hyperparameter Value Description
𝛾 0.001 ADAMstepsizeforthequery𝒙
𝒙
𝛾 0.01 ADAMstepsizefortheSVGPparameters𝒘
𝒘
𝐵 32 Minibatchsize
𝐺 2.0 Gradientclippingthreshold
clip
𝑘 30 Maximumnumberofepochs
epochs
𝑛 3 Maximumnumberoffailuretoimprove
fail
𝑚 100 Numberofinducingpoints
𝑛 =|𝒟 | 100 NumberofobservationsforinitializingBO
0 0
#quad. 20 NumberofGauss-Hermitequadraturepoints
optimize_acqf: restarts 10
optimize_acqf: raw_samples 256
optimize_acqf: batch_size 1∕20 Dependsontask,seedetailsinSection4
Hyperparameters. ThehyperparametersusedinourexperimentsareorganizedinTable1. For
the full-extent of the implementation details and experimental configuration, please refer to the
supplementarycode.
16B AdditionalPlots
Weprovideadditionalresultsandplotsthatwereomittedfromthemaintext.
B.1 AdditionalResultsonMoleculeTasks
InFig.4,weprovideplotsonadditionalresultsthataresimilartothoseinFig.2. Onthreeofthe
moleculetasks,weuse10,000randommoleculesfromtheGuacaMoldatasetasinitialization. Thisis
moreconsistentwithwhathasbeendoneinpreviousworksandachievesbetteroveralloptimization
performance.
Figure4: Additionaloptimizationresultsonthreemoleculetasksusing10,000randommolecules
fromtheGuacaMoldatasetasinitialization. Eachline/shadedregionrepresentsthemean/standard
errorover20runs. Wecountoraclecallsstartingaftertheseinitializationevaluationsforallmethods.
B.2 SeparatePlotsforBOandTuRBOResults
Inthissection,weprovideadditionalplotsseparatingoutBOandTuRBOresultstomakevisualization
easier.
Figure 5: BO-only optimization results of Fig. 2. We compare EULBO-SVGP, ELBO-SVGP,
ELBO-SVGPwithDPPinducingpointplacement(Mossetal.,2023),andexactGPs. Thesearea
subsetofthesameresultsshowninFig.2. Eachline/shadedregionrepresentsthemean/standard
errorover20runs.
17Figure6: TuRBO-onlyoptimizationresultsofFig.2. WecompareEULBO-SVGP,ELBO-SVGP,
ELBO-SVGPwithDPPinducingpointplacement(Mossetal.,2023),andexactGPs. Thesearea
subsetofthesameresultsshowninFig.2. Eachline/shadedregionrepresentsthemean/standard
errorover20runs.
B.3 EffectofNumberofInducingPoints
Fortheresultswithapproximate-GPsinSection4,weused𝑚=100inducingpoints. InFig.7,we
evaluatetheeffectofusingalargernumberofinducingpoints(𝑚 = 1024)forEULBO-SVGPand
ELBO-SVGP.
Lasso DNA
TuRBO (EULBO EI) w/ 1024 inducing points
TuRBO (EULBO EI)
0.29 TuRBO (ELBO EI) w/ 1024 inducing points
TuRBO (ELBO EI)
0.30
0.31
0.32
0.33
0.34
0 5000 10000 15000 20000
Number of Oracle Calls
Figure7: AblatingthenumberofinducingpointsusedbyEULBO-SVGPandELBO-SVGP.
AsinFig.2,wecomparerunningTuRBOwithEULBO-SVGPandwithELBO-SVGPusing𝑚=100
inducingpointsusedforbothmethods. WeaddtwoadditionalcurvesforTuRBOwithEULBO-SVGP
andTuRBOwithELBO-SVGPusing𝑚=1024inducingpoints. Eachline/shadedregionrepresents
themean/standarderrorover20runs.
Fig.7showsthatthenumberofinducingpointshaslimitedimpactontheoverallperformanceof
TuRBO,andEULBO-SVGPoutperformsELBO-SVGPregardlessofwhichthenumberofinducing
pointsused.
18
draweR
naeMC ComputeResources
Table2: InternalClusterSetup
Type ModelandSpecifications
SystemTopology 20nodeswith2socketseachwith24logicalthreads(total48threads)
Processor 1IntelXeonSilver4310,2.1GHz(maximum3.3GHz)persocket
Cache 1.1MiBL1,30MiBL2,and36MiBL3
Memory 250GiBRAM
Accelerator 1NVIDIARTXA5000pernode,2GHZ,24GBRAM
TypeofComputeandMemory. AllresultsinthepaperrequiredtheuseofGPUworkers(one
GPU per run of each method on each task). The majority of runs were executed on an internal
cluster,wheredetailsareshowninTable2,whereeachnodewasequippedwithanNVIDIARTX
A5000 GPU. In addition, we used cloud compute resources for a short period leading up to the
subsmissionofthepaper. Weused40RTX4090GPUworkersfromrunpod.io,whereeachGPU
hadapproximately24GBofGPUmemory. Whileweused24GBGPUsforourexperiments,each
runofourexperimentsonlyrequiresapproximately15GBofGPUmemory.
ExecutionTime. Eachoptimizationrunfornon-moleculetaskstakesapproximatelyonedayto
finish. Sincewerunthemoleculetasksouttoamuchlargernumberoffunctionevaluationsthan
othertasks(80000totalfunctionevaluationsforeachmoleculeoptimizationtask),eachmolecule
optimizationtaskruntakesapproximately2daysofexecutiontime. Withalleighttasks,tenmethods
run,and20runscompletedpermethod,resultsinFig.2include1600totaloptimizationruns(800
for molecule tasks and 800 for non-molecule tasks). Additionally, the two added curves in each
plotinFig.3required160additionalruns(120formoleculetasksand40fornon-moleculetask).
Completingalloftherunsneededtoproducealloftheresultsinthispaperthereforerequiredroughly
2680totalGPUhours.
ComputeResourcesusedDuringPreliminaryInvestigations. Inadditiontothecomputational
resourcesrequiredtoproduceexperimentalresultsinthepaperdiscussedabove,wespentapproximately
500hoursofGPUtimeonpreliminaryinvestigations. Thiswasdoneontheaforementionedinternal
clustershowninTable2.
19