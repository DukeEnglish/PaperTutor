Simplified and Generalized
Masked Diffusion for Discrete Data
JiaxinShi∗,KehangHan∗,ZheWang,ArnaudDoucet,MichalisK.Titsias
GoogleDeepMind
Abstract
Masked(orabsorbing)diffusionisactivelyexploredasanalternativetoautore-
gressivemodelsforgenerativemodelingofdiscretedata. However,existingwork
inthisareahasbeenhinderedbyunnecessarilycomplexmodelformulationsand
unclearrelationshipsbetweendifferentperspectives,leadingtosuboptimalparame-
terization,trainingobjectives,andadhocadjustmentstocounteracttheseissues. In
thiswork,weaimtoprovideasimpleandgeneralframeworkthatunlocksthefull
potentialofmaskeddiffusionmodels.Weshowthatthecontinuous-timevariational
objectiveofmaskeddiffusionmodelsisasimpleweightedintegralofcross-entropy
losses. Ourframeworkalsoenablestraininggeneralizedmaskeddiffusionmodels
withstate-dependentmaskingschedules.Whenevaluatedbyperplexity,ourmodels
trainedonOpenWebTextsurpasspriordiffusionlanguagemodelsatGPT-2scale
anddemonstratesuperiorperformanceon4outof5zero-shotlanguagemodeling
tasks. Furthermore,ourmodelsvastlyoutperformpreviousdiscretediffusionmod-
elsonpixel-levelimagemodeling,achieving2.78(CIFAR-10)and3.42(ImageNet
64×64)bitsperdimensionthatarecomparableorbetterthanautoregressivemodels
ofsimilarsizes.
1 Introduction
Sincetheirinception[1–3],diffusionmodelshaveemergedastheworkhorseforgenerativemedia,
achievingstate-of-the-artintaskssuchasimagesynthesis[4–6],audio[7,8]andvideogeneration[9–
13]. Themajorityofexistingsuccessesareforcontinuousstatespacediffusions. Whilediffusion
modelshavebeenextendedtodiscretestatespaces[1,14,15]andhavebeensuccessfullyapplied
toapplicationsrangingfromgraphgeneration[16],text-to-soundgeneration[17]orproteindesign
[18],theyremainnotaswidelyusedastheircontinuouscounterpartsastheyarenotcompetitive
with autoregressive models in important domains such as text modeling. This has motivated the
development of continuous space diffusion models where the discrete data are embedded in the
Euclideanspace[19–23]orthesimplex[24–28]. Webelievethatoneofthereasonsforthelimited
successofdiscretediffusionsisthattheyhavebeenhinderedbyfairlycomplexformulationsand
trainingobjectives. Thispaperisasteptowardsclosingthisgap.
Inthiswork,wefocuson“masked”(or“absorbing”)diffusions,adiscretediffusionformulation
firstpresentedbyAustinetal.[14],andlaterexploredbytheliteraturefromvariousperspectives
[29–32]. Wefollowhereacontinuous-timeframeworkwhichhasprovenveryusefultoimprovethe
trainingandunderstandingofcontinuousstatespacediffusions[seee.g.,3,33,34]. Wemakeseveral
technicalcontributionswhichsimplifythetrainingofthesemodelsandimprovesignificantlytheir
performance. Ourcontributionsareasfollows:
• Byusingelementaryarguments,weestablishseveralpropertiesfortheforwardprocessinduced
bythismodelanditscorrespondingtimereversal,improvingourunderstandingofthismodelclass.
∗Equalcontribution.Correspondenceto:jiaxins@google.com.
Preprint.Underreview.
4202
nuJ
6
]GL.sc[
1v92340.6042:viXra• WeprovidearemarkablysimpleexpressionoftheEvidenceLowerBound(ELBO)formasked
diffusion models, showing that it corresponds to a weighted integral over time of cross-entropy
losses. Similarly to continuous space diffusions [33], this objective can be rewritten in terms of
signal-to-noiseratioandexhibitsinvarianceproperties.
• Wedevelopaunifyingunderstandingofpreviouslyproposedcontinuous-timediscretediffusion
models[29,32,35],revealingthechangestheymadetoourELBOobjectiveand/ormodelparame-
terization. Weshowthatthesechangeseitherleadtoexpensivemodelevaluations,orlargevariance
intraining,orbreakingtheconsistencybetweenforwardandreverseprocesses.
• OnGPT-2scaletextmodelingandpixel-levelimagemodelingtasks,maskeddiffusionstrained
usingoursimpleELBOobjectiveoutperformpreviousproposals,leadingtothebestlikelihoodand
zero-shottransferperformanceamongdiscretediffusionmodels.
• Finally,basedonoursimplifiedmaskeddiffusionformulation,weproposeageneralizedmasked
diffusionmodelthatallowsstate-dependentmaskingschedules. Thisgeneralizedmaskeddiffusion
modelfurtherimprovespredictiveperformanceontestlikelihoods.
2 MaskedDiffusion
Assumeourdatalivesinafinitediscretestatespaceofsizem.Weusetheintegers0,1,...,m−1and
theircorrespondingone-hotvectorse ,e ,...,e torepresentthestates.2 Inmaskeddiffusion,
0 1 m−1
we augment the space with an additional mask state, which is assigned the index m. A forward
“masking”processisdefinedsuchthatadatapointisevolvedintothemaskstateatsomerandom
time. Reversingthisprocessthengivesagenerativemodelofthediscretedata.
Discrete-time forward process. The forward process is defined as a Markovian sequence of
discreterandomvariablesx indexedbytimet,wheretrunsfrom0to1. Throughoutthework,we
t
abusethenotationsuchthatx canbeeitheranintegeroritscorrespondingone-hotvector,whenever
t
itisclearfromthecontext. Wedivide[0,1]intoT intervals,andlets(i)=(i−1)/T,t(i)=i/T.
FollowingAustinetal.[14], thestatetransitionbetween[s(i),t(i)]isdeterminedbyatransition
matrixofsizem×m: Q = (1−β )I +β 1e⊤,where1isanall-onevectorofsizem+1,e
i i i m m
representsaone-hotvectorwiththem-thelementbeing1. Eachentry[Q ] denotestheprobability
i jk
oftransitionfromthestatej tothestatek:
[Q ] =q(x =k|x =j)=(1−β )δ +β δ .
i jk t(i) s(i) i jk i km
Thismeans,withprobability1−β ,x =x ,otherwiseitjumpstothemaskstate. Giventhe
i t(i) s(i)
abovetransitionmatrix,themarginaldistributionattimet(i)givenx is
0
q(x |x )=Cat(x ;Q¯⊤x )=x⊤Q¯ x .
t(i) 0 t(i) i 0 0 i t(i)
Here,weuseCat(x;p)todenoteaCategoricaldistributionwherepisthevectorofprobabilities
ofbeingineachcategory,andQ¯ ≜(cid:81)i Q =α I+(cid:0) 1−α (cid:1) 1e⊤ forα =(cid:81)i (1−β ). We
i j=1 j i i m i j=1 j
expectα tobecomeverysmallorzeroforasufficientlylargeT suchthatq(x |x )foranyx will
T 1 0 0
becomeadeltamassatthemaskstate.
Continuous-timelimit. Wecandefineacontinuous-timeforwardprocessbytakingalimitofthe
abovediscrete-timeprocess. Wefirstspecifyacontinuousfunctionβ(t)suchthatβ =β(t(i))/T.
i
WethenletT →∞inthediscrete-timeprocessandcomputethelimitofQ¯ (provedinAustinetal.
i
14,AppendixA.6,seealsoAppendixAandAppendixB)as
(cid:16) (cid:90) t (cid:17)
Q¯(t)≜ lim Q¯ =α I+(1−α )1e⊤, whereα ≜exp − β(s)ds , (1)
i t t m t
T→∞ 0
so that q(x |x ) = Cat(x ;Q¯(t)⊤x ). For two arbitrary times, 0 ≤ s < t ≤ 1, the transition
t 0 t 0
(cid:80)
distributionthatiscompatiblewiththeabovemarginal(i.e.,q(x |x )= q(x |x )q(x |x ))is
t 0 xs t s s 0
q(x |x )=Cat(x ;Q¯(s,t)⊤x ), whereQ¯(s,t)≜Q¯(s)−1Q¯(t)= α tI+(cid:0) 1− α t(cid:1) 1e⊤.
t s t s α α m
s s
NotethatAustinetal.[14]didnotderivethisexplicitformoftransitionmatrixbetweentwoarbitrary
timesandt,whichappearedlaterinZhaoetal.[36]concurrentlywithourwork.
2Wemakenoassumptionontheorderingofstates—anypermutationoftheintegerswillgivethesame
result.
2t t t/(1 t)
0
1.0
linear
5
geometric
0.5 10 cosine
poly2
15
poly0.5
0.0
20
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
t t
Figure1: Maskingschedulesintheliterature: (Left)α ;(Right)weightofthecross-entropylossw.r.t.
t
t;EquationsfortheseschedulesaregiveninTable4inAppendix.
Maskingschedules. Fromthedefinitionofα ,wehavethatα =1. Andsimilartothediscrete-
t 0
timeformulation,wewouldlikeα bezeroorveryclosetozero. Weprovideasummaryofmasking
1
schedulesfromliteraturethatsatisfythesepropertiesinFigure1. Thelinearschedulewasproposed
inSohl-Dicksteinetal.[1]forbinaryvariablesandthenre-derivedbyAustinetal.[14]frommutual
informationfordiscrete-timemodels. Thegeometricscheduleα isplottedforβ¯ = 10−5 and
t min
β¯ =20. Itwasfirstusedforcontinuousdiffusions[3]andthenfordiscretebyLouetal.[32]. The
max
cosineschedulewasoriginallyproposedinMaskGIT[30],aniterativeunmaskinggenerativemodel
inspiredbydiffusion. Thisschedulehasthepropertyofslowingdowntheunmaskingprocessatthe
beginningofthereversegeneration. Aligningwiththeirobservation,wefindthatthisresultsina
lowerchanceofconflictingtokensbeingunmaskedsimultaneouslyatthestartofgeneration,thereby
enhancingtheoverallgenerationquality.
Timereversaloftheforwardprocessgivenx . Theanalyticpropertyofourforwardprocess
0
allowstocomputemanyquantitiesofinterestinclosedform. Onesuchquantityfrequentlyusedin
diffusionmodelsisthetimereversaloftheforwardprocessgivenx : q(x |x ,x )fors ≤ t. We
0 s t 0
deriveitinAppendixCas
α −α
q(x |x ,x )=Cat(x ;R¯x0(t,s)⊤x ), whereR¯x0(t,s)=I+ s te (x −e )⊤.
s t 0 s t 1−α m 0 m
t
FromthetransitionmatrixR¯x0(t,s)∈R(m+1)×(m+1) wecanseethereverseprocessconditioned
onx 0hasaverysimplelogic—ifx tisamask,withprobability α 1s −− αα tt,itwilljumptothestatex 0at
times,otherwiseitwillstaymasked. Oncex isunmasked,itremainsinthesamestateuntiltheend.
t
3 ModelandObjective
For a discrete-time masked diffusion process, we define our generative model by approximately
reversingtheforwardtransitionsusingabackwardmodelp (x |x ). Onewaytodefinethismodelis
θ s t
p (x |x )≜q(x |x ,µ (x ,t)), (2)
θ s t s t θ t
whereµ (x ,t)∈∆m+1isaprobabilityvectorparametrizedbyaneuralnetworkf withasoftmax
θ t θ
appliedtotheoutputlogits(notethem-thoutputisforcedto0sincethecleandatacannotbemasks):
(cid:26)
softmax(f (x ,t)) x =m,
µ (x ,t)= θ t t (3)
θ t x x ̸=m.
t t
Thisisknownasmean-parameterizationsinceitleveragesapredictionmodelforthemeanofx .
0
Besidesp (x |x ),wealsoneedtospecifyp(x |x )andthepriordistributionp(x )=p(x ).
θ s t 0 t(1) t(T) 1
Followingthepracticeincontinuousdiffusionmodels[33],wechoosep(x |x ) ∝ q(x |x ).
0 t(1) t(1) 0
Andsinceq(x |x )≈δ foranyx asα ≈0,wesetp(x )≈δ ,seeAppendixE.
1 0 x1,m 0 1 1 x1,m
Wethenwriteoutthediscrete-timediffusionmodelobjective[1,2],whichisalowerboundofthelog
marginallikelihoodofdatax underthemodelp(knownastheEvidenceLowerBound,orELBO):
0
logp(x )≥E [logp(x |x )]−KL(q(x |x )∥p(x ))−L ,
0 q(xt(1)|x0) 0 t(1) 1 0 1 T
3whereL =(cid:80)T E [KL(q(x |x ,x )∥p (x |x ))]. Fortheabovechoicesofthe
T i=2 q(xt(i)|x0) s(i) t(i) 0 θ s(i) t(i)
priordistribution,thetermKL(q(x |x )∥p(x ))becomeszero. Underthebackwardmodel(2),the
1 0 1
KLdivergencetermsinL becomes(proofinAppendixD)
T
α −α
KL(q(x |x ,x )∥p (x |x ))=− s tδ ·x⊤logµ (x ,t),
s t 0 θ s t 1−α xt,m 0 θ t
t
whichisasimplecross-entropylossbetweenthepredictedlogitsandthecleandata. InAppendixD,
weshowthatL isaRiemannsumandislowerboundedbythecorrespondingcontinuousintegral:
T
L ≜ lim L =(cid:90) 1 α t′ E (cid:2) δ ·x⊤logµ (x ,t)(cid:3) dt, (4)
∞ T→∞ T t(1) 1−α t q(xt|x0) xt,m 0 θ t
whereα′ denotesthederivativeofα withrespecttot. Therefore,wecanobtainanELBOthatis
t t
tighterthanthatofanyfiniteT bypushingT →∞. ThisELBOcanbefurthersimplifiedbyletting
t(1)→0. Asaresult,E [logp(x |x )]goesto0andtheELBObecomes−L .
q(xt(1)|x0) 0 t(1) ∞
Forcontinuousstate-spacediffusions,itwasshownthattheELBOisinvarianttothenoiseschedule,
exceptforthesignal-to-noiseratio(SNR)atitsendpoints[33]. Weestablishhereasimilarresult
fordiscretediffusions. Considerchoosingα = σ(λ ), whereσ representsthesigmoidfunction
t t
σ(x)= 1+1 e−x. Inthiscontext,thelog-SNRisdefinedbyλ t =log 1−α αt
t
=log-SNR(t). Bymaking
achangeofvariablesin(4)tomakeeverythingafunctionofthelog-SNR,weobtain
(cid:90) λ1
L = σ(λ)x⊤E [δ logµ˜ (x ,λ)]dλ.
∞ 0 q˜(xλ|x0) xλ,m θ λ
λt(1)
whereµ˜ (x,λ):=µ (x,t)andq˜(x |x ):=q(x |x )fort=log-SNR−1(λ). Thisshowsthatthe
θ θ λ 0 t 0
onlyeffectα hasonthelossisthroughthevaluesoftheSNRattheendpoints.Still,becausewedraw
t
uniformsamplesofttoestimatetheintegral,thechoiceofmaskingscheduleaffectsthevariance.
Multidimensionaldata. Intheprevioussections,x wasassumedtobeasinglediscretetoken.
t
To extend the method to multidimensional data, let x be now a sequence (x(1),x(2),...,x(N)),
t t t t
whereeachelementx(n)representsadiscretetoken. Weselectaforwardprocesswhichfactorizes
t
acrossallN tokens: q(x |x ) = (cid:81)N q(x(n)|x(n)). Asaresult, theforwardmarginalsq(x |x )
t s n=1 t s t 0
andreversalq(x |x ,x )alsofactorize. Inthiscase,wedefinethebackwardmodelasp (x |x )≜
s t 0 θ s t
(cid:81)N q(x(n)|x(n),µ(n)(x ,t)), where µ (x ,t) is a neural network that takes the full N tokens
n=1 s t θ t θ t
asinputandoutputsN probabilityvectors. Then-thoutputµ(n)(x ,t)isapredictionmodelfor
θ t
E[x(n)|x ],themeanvalueofthen-thtoken. Repeatingabovederivationsgives
0 t
(cid:90) 1 α′ (cid:104) (cid:105)
L(N) ≜ t E (cid:80) (x(n))⊤logµ(n)(x ,t) dt. (5)
∞ 1−α q(xt|x0) n:x(n)=m 0 θ t
0 t t
4 RelationtoExistingWork
Wediscusshowtounifyseveralexistingmaskeddiffusionmodelsusingourframework.
Continuous-Time Markov Chains (CTMC). To show the connection with the CTMC view
presented in Austin et al. [14], Campbell et al. [29], we can write out the forward and backward
maskeddiffusionusingCTMCmachinery. Toseethis, forashorttime∆t, givenx , theTaylor
0
expansionsofourforwardandreversetransitionmatricesattare
Q¯(t,t+∆t)=I+Q(t)∆t+o(∆t) for Q(t)≜β(t)(1e⊤ −I), (6)
m
α′
R¯x0(t,t−∆t)=I+Rx0(t)∆t+o(∆t) for Rx0(t)≜− t e (x −e )⊤. (7)
1−α m 0 m
t
where Q(t) and Rx0(t) are known as the transition rate matrices. Austin et al. [14] derived the
sameQ(t)inAppendixA.6oftheirpaper. However,theydidnotexplorethereverseprocessora
4continuous-timeobjective. Campbelletal.[29]obtainedthecontinuous-timeELBOuptoaconstant
fromtheCTMCview. TheyestablishedthefollowingalternativeexpressionfortheELBO:
(cid:90) 1 (cid:104) (cid:88) (cid:105)
L =− E R (t) + Q(t) logR (t) dt+C, (8)
∞ qt|0(k|x0) θ kk kj θ jk
t(1) j̸=k
where C is a constant independent of θ and q (k|x ) := q(x = k|x ) for any k. We use the
t|0 0 t 0
shorthandR (t) todenotetheapproximatebackwardtransitionratefromthestatektoj obtained
θ kj
bysubstitutingourpredictionmodelµ θ(k,t)forx 0inRx0(t) kj. InAppendixF.1,weshowhowto
recoverthisexpressionfrombyseparatingoutaconstantfromourELBOexpression(4)andapplying
adiscrete“integration-by-part”. Campbelletal.[29]used(8)asthetrainingloss. Akeylimitation
ofthislossisthatitrequiresmevaluationsofthepredictionmodelµ (·,t)tocomputetheinner
θ
summationterm. Tocircumventthiscomputationalburden,theyproposedusingadoublystochastic
estimate. However,thisleadstosignificantlyhighervariancecomparedtotheanalyticcross-entropy
(4)whichonlyrequiresonepassofµ (·,t). PleaserefertoAppendixF.2formoredetails.
θ
Score parameterization. While so far we used a prediction model µ (x ,t) for the mean of
θ t
clean data given x (i.e., mean parameterization), one can choose other ways of parameterizing
t
thereversemodel. Louetal.[32],Bentonetal.[35]proposedtoparameterizethediscrete“score”
s(x ,t) ≜ qt(j) andintroducedthefollowingscore-basedlossfordiscretediffusions:
t j qt(xt)
L
=(cid:90) 1
E
(cid:104)(cid:88)
Q(t)
(cid:16)
s (k,t)
−q t|0(j|x 0)
logs (k,t)
+ψ(cid:16)q t|0(j|x 0)(cid:17)(cid:17)(cid:105)
dt, (9)
sθ qt|0(k|x0) jk θ j q (k|x ) θ j q (k|x )
0 j̸=k t|0 0 t|0 0
where ψ(y) ≜ ylogy −y. In Appendix F.3, we provide an alternative derivation of (9) which
is simpler. We show the link between score and mean parameterizations through the following
proposition.
Proposition1. Letq bethemarginaldistributionofthemaskeddiffusiondefinedinSection2at
t
timet. Thediscretescores(x ,t) = qt(j) foramaskstatex =mandj ̸=mcanbeexpressedas
t j qt(xt) t
s(m,t) = α t E[x |x =m]⊤e ,whichsatisfies (cid:88) s(m,t) = α t . (10)
j 1−α 0 t j j 1−α
t t
j̸=m
Proposition1(provedinAppendixF.3)impliesthatareasonablescoremodelforamaskstateis
α
s (m,t) = t µ (m,t). (11)
θ j 1−α θ
t
Indeed, plugging (11) into the score-based loss (9) recovers our objective (4). In Lou et al. [32]
the score is parameterized as a neural network without enforcing the constraint in (10). This
meansthelearnedbackwardmodelcanbeincompatiblewiththeforwardprocess. Wefindthatour
parameterization,whichenforcestheconstraint,leadstomorestabletrainingandbetterresults.
Other related work. Training of MaskGIT [30] follows the steps: (a) Sample t ∈ [0,1]. (b)
Given a mask scheduling function γ(t), sample γ(t)N tokens to place masks. (c) For the data
x ∈R(m+1)×N andthemaskedtokenstatex ∈R(m+1)×N,minimizethenegativelog-likelihood
0 t
(cid:90) 1 (cid:104) (cid:105)
L =− E (cid:80) (x(n))⊤logµ(n)(x ,t) dt. (12)
MaskGIT xt n:x(n)=m 0 θ t
0 t
From(10)weknowthat (cid:80) j q̸= tm |0(q mt|0 |x( 0j| )x0) = 1−α αt
t
thusq t|0(m|x 0) = 1−α t. Therefore,whenwe
setthemaskschedulingfunctionasγ(t)=1−α twecanrecoverthelossin(5)withoutthe 1−α α′ t
t
weighting. Onemightbeinterestedinwhethertheuniformweightingcanberecoveredbyselecting
anappropriatescheduleα . However,solvingα suchthatα′ = α −1yieldsα = cet+1and
t t t t t
thereisnocthatsatisfiesbothα =1andα =0. ThisshowsthattheMaskGITloss(12)maynot
0 1
beaupperboundonthenegativelog-likelihood. Forthelinearscheduleα =1−t,thebackward
t
transitionratematrixconditionedonx is:
0
α′ 1
Rx0(t)=− t e (x −e )⊤ = e (x −e )⊤.
1−α m 0 m t m 0 m
t
5ThisisthesameastheconditionalbackwardtransitionrateusedinCampbelletal.[37,Eq. (22)]—
notethattheirtimetisreversed,andtheratematrixwasthereforeintheformRx0(t)= 1−1 te m(x 0−
e )⊤. PleaserefertoAppendixFformorerelatedwork.
m
5 GeneralizationtoState-dependentMaskingSchedules
Weconsiderherestate-dependentmaskingschedulesbymakingthetime-dependentprobabilityof
maskingatokendependentalsoonthetokenvalue. Thus,sometokenswillhavehigherprobability
tobemaskedearlierintimecomparedtoothers.
We first define the forward process for a single token x . Let α be a m+1 dimensional vector
t t
function,i.e.,thereisadifferentfunctionα foreachpossiblevalueiofthetokenx . Also,by
t,i t
vector αt wedenotetheelement-wisedivisionofthetwovectors. Wedefinetheforwardtransition
asq(x α |xs )=Cat(x ;Q¯(s,t)⊤x )where
t s t s
(cid:16)α (cid:17) (cid:16) (cid:16)α (cid:17)(cid:17)
Q¯(s,t)=diag t + I−diag t 1e⊤
α α m
s s
anddiag(cid:0)αt(cid:1) isadiagonalmatrixwiththevector αt initsdiagonal. Theprobabilityofmovingfrom
αs αs
currentstatex toafuturestatex (eitherthesameasx ormask)isdeterminedbyastate-dependent
s t s
rate(cid:0) αα st(cid:1)⊤ x s,whilethemarginalattimesgivenx 0is
q(x |x )=Cat(x ;Q¯(s)⊤x ) for Q¯(s)=diag(α )+(I−diag(α ))1e⊤.
s 0 s 0 s s m
(cid:80)
Further,foranytime0≤s<t≤1itholdsthatq(x |x )= q(x |x )q(x |x )sotheaboveis
t 0 xs t s s 0
avalidcontinuous-timeMarkovchain.
Giventheforwardconditionalsandmarginals,wecannowcomputethetimereversalconditionedon
x . Thefullformofq(x |x ,x )isderivedinAppendixG.1. Forx =m,wehave
0 s t 0 t
(cid:16) (cid:17)⊤ (cid:16) (cid:17)⊤
q(x s|x t =m,x 0)=q(x s|x t =m,x 0,x 0x⊤ 0)= 1 1− −α αs
t
x 0e⊤ mx s+ α 1s −− αα tt x 0x⊤ 0x s. (13)
Thissuggeststhatthebackwardmodelgivenx =mcanbechosenasp (x |x =m)≜q(x |x =
t θ s t s t
m,µ (x ,t),diag(µ (x ,t))) where µ (x ,t) is a probability vector defined as in Section 3 that
θ t θ t θ t
approximatesE[x |x ]whilediag(µ (x ,t))approximatesE[x x⊤|x ]=diag(E[x |x ]). Weshow
0 t θ t 0 0 t 0 t
inAppendixG.1thatthenegativecontinuous-timeELBOforthestate-dependentratecaseis
L =(cid:90) 1(cid:16) α t′ (cid:17)⊤ E (cid:2) δ ·(x −µ (x ,t)+x x⊤logµ (x ,t))(cid:3) dt, (14)
∞ 1−α q(xt|x0) xt,m 0 θ t 0 0 θ t
0 t
whereα′ istheelementwisederivativeofα w.r.t.t. Thisgeneralizesthelossfrom(4),whichcanbe
t t
recoveredbychoosingα asascalarscheduletimesanall-onevector. ForN tokensthebackward
t
modelandthelossfurthergeneralizesimilarlytoSection3;seeAppendixG.2.
TolearnthetokendependentmaskingscheduleusingELBOoptimization,weparametrizethem+1
dimensionalfunctionα tusingthepolynomialschedule(seeFigure1)asα
t,i
=1−twi andoptimize
eachparameterw > 0.3 Thevalueofw , throughthemaskingprobability1−α , determines
i i t,i
how fast the token with value i jumps to the mask state. Since in the loss (14) the distribution
q(x |x )dependsonα andthusthevectorw,optimizingw posesadiscretegradientestimation
t 0 t
problem[see,e.g., 38]. Naiveautodiffleadstobiasedgradientsandpusheswtowardszerobecause
thegradientscannotpropagatethroughthe(discrete)samplesdrawnfromq(x |x ). Tofixthis,we
t 0
usedtheREINFORCEleave-one-outestimator[39,40]tocomputelow-varianceunbiasedgradients
foroptimizingw. DetailsaregiveninAppendixG.2.
6 Experiments
6.1 Text
Text is natural discrete data with rich structures. We experiment with two datasets: text8 [41], a
character-leveltextmodelingbenchmarkextractedfromEnglishWikipedia,andOpenWebText[42],
3Weonlyneedmlearnableparametersw ,fori=0,...,m−1,sincex canneverbethemasktoken.For
i 0
thefinalmaskdimensionwecanchooseanarbitraryfixedvaluesuchasw =0.
m
6Figure 2: Iterative unmasking process for a unconditionally generated sample by MD4. This
visualizationonlyincludesasubsequencefroma1024-tokensequencegeneratedbyMD4. The"?"
symbolrepresentsmaskedtokens. Greenmaskedtokensarerevealedinsteps500-700,followedby
yellowmaskedtokensinsteps700-850,andfinally,redmaskedtokensinsteps850-1000. Additional
unconditionalgenerationfromourMD4canbefoundinAppendixI.
anopencloneoftheunreleasedWebTextdatasetusedtotrainGPT-2[43]. Wechoosethelinear
noisescheduledescribedinSection2foralltextexperimentssincewefinditworksbest. Wereferto
oursimplemaskeddiffusionmodelasMD4(MaskedDiscreteDiffusionforDiscreteData)andour
generalizedstate-dependentmodelasGenMD4.
OpenWebText. We follow Lou et al. [32],
Radford et al. [43] to test the language mod-
eling capabilities of our model. Specifically,
 ȱ Ȱ Ȳ
wetrainaGPT2-smallandGPT2-mediumsize
of MD4 on OpenWebText (98% training, 2%  ; ´ ľ Ĳ Ĳ ô ´ Č  $ ô ë ë ľ Ĳ ô Ē Č ʲ 
 ȶ ˻ ȱ Ȱ ȱ   ( $ $ ʲ 
validation)andevaluatezero-shotperplexityun-  Y $ ȴ ʲ 
conditionally on the test splits of five bench-  ; Ø Č Y $ ȴ ʲ 
 ȴ ˻ ȱ Ȱ ȱ  Y $ ȴ ʲ Y
markdatasetsusedinRadfordetal.[43]: LAM-
BADA [44], WikiText2, Penn Treebank [45],  ȳ ˻ ȱ Ȱ ȱ
WikiText103[46],andOneBillionWords[47].
 Ȳ ˻ ȱ Ȱ ȱ
WekeepourevaluationsetupthesameasLou
etal.[32]. Toensurethecomparisonisfair,we  Ȱ  Ȳ Ȱ Ȱ  ȴ Ȱ Ȱ  ȶ Ȱ Ȱ  ȸ Ȱ Ȱ  ȱ Ȱ Ȱ Ȱ
reproducedtheirresultinourimplementation.   Į ´ ô Č ô Č ì  Ĳ Ĺ Ø ī Ĳ  ʬ ȱ  ľ Č ô Ĺ  ˽  ȱ Ȱ Ȱ Ȱ  Ĳ Ĺ Ø ī Ĳ ʭ
The results are shown in Table 1. We can see Figure3: PerplexityonOpenWebText(OWT)val-
thatoursmallmodeloutperformspreviousbest idationsetduringtraining.
discretediffusionmodelsonallfivetasks. We
also confirmed that our re-implementation of
SEDDAbsorbleadstosimilarandslightlybetterresultsthanthosereportedintheirpaper.Webelieve
thisisbecausetheywereusingmixed-precisiontraining(thuslargervariances)whileweareusing
fullprecision(float32)training. WearealsobetterthanGPT-2onalltasksexceptLAMBADAwhere
wearethesecondbestamongallmethods. Whenwescaleupthemodelsizetomedium,weobserve
thatMD4similarlybeatsSEDDAbsorbandGPT-2onalltasksexceptLAMBADA.
To validate that the strong zero-shot performance is a result of a better-trained model, we plot
perplexityontheOpenWebTextvalidationsetinFigure3,itdemonstratesthatourmodelconverges
fasterandhasbetterfinallikelihoodsthanpriormethods. WealsoobservedthatSEDD[32]has
training instabilities. We believe this is due to the score parameterization breaking consistency
betweenforwardandreverseprocesses,asexplainedinSection4. AlthoughGenMD4achieveslower
perplexitythanMD4,weobservedthatthelearnedwscaneasilyoverfittodatasetstatistics,making
itlesseffectiveonzero-shottransfertasks. Beyondlikelihoodevaluation,wealsoexaminegeneration
7
 Ř Ĺ ô ŗ Ø Ć ī Į Ø y  Ć ´ ő (  Ĺ ŗ Ø  Í Ø  Č Ø ī `Table1: Zero-shotunconditionalperplexityonfivebenchmarkdatasetsfromRadfordetal.[43]. The
numbersforothermethodsarefromLouetal.[32]exceptourreimplementationofSEDDAbsorb.
OurMD4modelachievesthebestresultonallbenchmarksexceptLAMBADAwhereitisthesecond
best. ∗TheGPT-2numbersarereportedfortheGPT-2checkpointpretrainedonWebTextinsteadof
OWTthusisnotadirectcomparison.
Size Method LAMBADA WikiText2 PTB WikiText103 IBW
Small GPT-2(WebText)∗ 45.04 42.43 138.43 41.60 75.20
D3PM ≤93.47 ≤77.28 ≤200.82 ≤75.16 ≤138.92
Plaid ≤57.28 ≤51.80 ≤142.60 ≤50.86 ≤91.12
SEDDAbsorb ≤50.92 ≤41.84 ≤114.24 ≤40.62 ≤79.29
SEDDAbsorb(reimpl.) ≤49.73 ≤38.94 ≤107.54 ≤39.15 ≤72.96
MD4(Ours) ≤48.43 ≤34.94 ≤102.26 ≤35.90 ≤68.10
Medium GPT-2(WebText)∗ 35.66 31.80 123.14 31.39 55.72
SEDDAbsorb ≤42.77 ≤31.04 ≤87.12 ≤29.98 ≤61.19
MD4(Ours) ≤44.12 ≤25.84 ≤66.07 ≤25.84 ≤51.45
qualityofourmodels. Figure2showcasesarandomlyselected,notablycoherentsampleproduced
byMD4-medium,alongsideitsiterativedenoisingprocess. Quantitatively,weemployGPT-2Large
modeltoevaluateperplexityofthegeneratedsamplesasshowninFigure6. WecanseeMD4is
abletooutperformitsauto-regressivecounterpartGPT-2byalargemargin. Samplequalitycanbe
improvedbytwoorthogonalstrategies: scalingupmodelsize(morecomputeduringtraining)or
increasingdecodingsteps(morecomputeduringinference). ComparedwithSEDD,MD4reachesa
newstateoftheart. Notably,after64decodingsteps,ourMD4ofsmallsizematchestheperformance
ofSEDD-medium.
text8. Following the convention in Austin et al. [14], Lou et al. [32], we experimented with
trainingmaskeddiffusionmodelsontextchunksoflength256. Weusedthesamedatasetsplitsand
transformersofthesamesizetoparameterizethebackwardpredictionmodel. Detailscanbefoundin
AppendixH.1. ResultsaresummarizedinTable2. Weusedthestandardbits-per-charactermetric(a
normalizednegativeloglikelihood)forthisdataset. Wecanseethatourmodelsoutperformprevious
diffusionmodels,whetherdiscreteorcontinuousinnature. Wealsooutperformthebestany-order
autoregressivemodels,whichdonotassumeafixedgenerationorderandhasastrongconnectionto
discretediffusion[48]. OurmodelisonlybeatenbyanautoregressivetransformerandtheDiscrete
Flow(whichalsousedanautoregressivebackbone). Webelievethisisbecauseautoregressivemodels
onlyrequirelearningafixedorderofgenerationthusbetterutilizemodelcapacity.
Thetext8datasethasasmallvocabularythatconsistsof26lettersa-zandaspacetoken. Therefore,
wedidnotexpectmuchflexibilityintroducedbyourstate-dependentformulation. Still,withthe
generalizedobjectivein(14),GenMD4isabletoachievesignificantlybetterBPCthanMD4,showing
thepromiseofastate-dependentdiffusionondiscretedatatasks.
6.2 Pixel-levelimagemodeling
TodemonstrateMD4beyondtextdomains,wefollowAustinetal.[14],Campbelletal.[29]and
trainMD4onorder-agnosticimagedata. Specifically,wetaketheCIFAR-10andtheDownsampled
ImageNet64×64[52]datasetsandtreateachimageasasetofdiscretetokensfromavocabularyof
size256suchthatthemodelisunawareoftherelativeproximitybetweendifferentpixelvalues. We
comparetootherdiscretediffusionandautoregressivemodelsthathavereportedlikelihoodresults
onthesamedataset,althoughtoourknowledgetherearenopublishedresultondiscretediffusion
methodsforImageNet64×64thatdirectlymodelrawpixelspace.
ResultsaresummarizedinTable3. Weestablishanewstate-of-the-artfordiscretediffusionmodels,
outperformingpreviousdiscrete-time[14]andcontinuous-time[29]modelsbyasignificantmargin.
4OurpersonalcommunicationwiththeauthorsofLouetal.[32]concludedthatthepaper’sreportedresult
(1.32)isaccidentallycomputedonthetrainingset.Herewereportourreproducedresultaftercorrectingthe
mistake.
8Table2:BitsPerCharacter(BPC)onText8 Table3: BitsPerDimension(BPD)onCIFAR-10
testset. Allmodelsusestandard12-layer testsetandDownsampledImageNet64×64[52]
transformers similar to GPT2-small [43] validationset. Allmodelsinthetablearetrained
except Discrete Flow which uses 8 × 3 withoutdataaugmentation.
layers.
Method #Params BPD(↓)
Method BPC(↓)
Autoregressive
ContinuousDiffusion PixelRNN[52] 3.00
Plaid[22](Ourimpl.) ≤1.48 GatedPixelCNN[53] 3.03
BFN[26] ≤1.41 PixelCNN++[54] 53M 2.92
PixelSNAIL[55] 46M 2.85
Any-orderAutoregressive ImageTransformer[56] 2.90
ARDM[48] ≤1.43 SparseTransformer[57] 59M 2.80
MAC[49] ≤1.40
DiscreteDiffusion
Autoregressive D3PMAbsorb[14] 37M ≤4.40
IAF/SCF[50] 1.88 D3PMGauss[14] 36M ≤3.44
ARArgmaxFlow[15] 1.39 τLDR[29] 36M ≤3.59
DiscreteFlow[51] 1.23 MD4(Ours) 28M ≤2.78
TransformerAR[14] 1.23
Autoregressive
DiscreteDiffusion PixelRNN[52] 3.63
Mult.Diffusion[15] ≤1.72 GatedPixelCNN[53] 3.57
D3PMUniform[14] ≤1.61 SparseTransformer[57] 152M 3.44
D3PMAbsorb[14] ≤1.45 RoutingTransformer[58] 3.43
SEDDAbsorb[32] ≤1.414 PerceiverAR[57] 770M 3.40
MD4(Ours) ≤1.37 DiscreteDiffusion
GenMD4(Ours) ≤1.34 MD4(Ours) 198M ≤3.42
Figure4: Noncherry-pickedunconditionalsamplesfromMD4trainedonImageNet64x64,treating
pixels as discrete tokens. More samples can be found in Figure 5 in Appendix. The model is
optimizedforlikelihoodinsteadofvisualquality—seee.g.,Kingmaetal.[33]forsamplesfroma
continuousdiffusionmodeloptimizedsimilarlyforlikelihood.
Notably,despitelackingknowledgeoftheordinalstructureofpixelvalues,MD4stilloutperforms
models trained with this inductive bias, including D3PM Gauss and τLDR where the noising
distribution is a discrete Gaussian distribution that gives larger probabilities to near pixel values.
OurCIFAR-10modelwithoutdataaugmentationalsoachievesbetterlikelihoodthantheprevious
bestautoregressivemodelsonthistask,whileonImageNet64×64ourresultiscompetitivewith
TransformerARmodelsofsimilarsizes,onlybeatenbyPerceiverARwhichisapproximately4×
largerinmodelsize. WeprovidearandomsamplefromourImageNet64×64modelinFigure4.
MoreresultscanbefoundininAppendixH.3.
7 Conclusion
Inthiswork,werevisitmaskeddiffusionmodels,focusingonaflexiblecontinuous-timeformulation.
Existingworksinthisareaarenoteasilyaccessibletonon-specialistsandpresentELBOsthatare
difficulttooptimize,oftenresultinginperformancethatisnotcompetitivewithcontinuousdiffusions
andautoregressivemodels. Theframeworkweproposeprovidesaverysimpleexpressionofthe
ELBOasaweightedintegralofcross-entropylosses. Additionally,weproposeageneralizedmasked
diffusionformulation(GenMD4),wherethemaskingscheduledependsonthecurrentstateofthe
process,andderiveitscorrespondingELBO.Experimentally,ontextandimagedata,theresulting
maskeddiffusionsoutperformexistingdiscreteandcontinuousdiffusionmodelsandfareverywell
9
01-RAFIC
46×46teNegamIcomparedtoautoregressivemodelsformosttasks. GenMD4providesfurtherimprovementsinterms
oflikelihoodsoverthestate-independentcase.
Althoughwehaveimprovedmaskeddiffusionmodels,theystillsufferfromlimitations. First,in
some tasks such as text8, masked diffusions are not yet competitive with autoregressive models.
Weconjecturethatthisisbecauseautoregressivemodelscanbetterleveragemodelcapacitysince
they only require learning one order. It would be interesting to develop better architectures for
discretediffusions. Moreover,GenMD4ispromising,butitcaneasilyoverfittothedataset,making
itlesseffectiveforzero-shottransfercomparedtosimplerversions. Additionally,inferencewitha
state-dependentscheduleismorechallenging.
References
[1] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli.Deepunsupervisedlearning
usingnonequilibriumthermodynamics. InInternationalConferenceonMachineLearning,2015.
[2] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InAdvancesin
NeuralInformationProcessingSystems,2020.
[3] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBenPoole.
Score-basedgenerativemodelingthroughstochasticdifferentialequations. InInternationalConferenceon
LearningRepresentations,2020.
[4] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages10684–10695,2022.
[5] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.Hierarchicaltext-conditional
imagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,2022.
[6] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistictext-to-
imagediffusionmodelswithdeeplanguageunderstanding. InAdvancesinNeuralInformationProcessing
Systems,2022.
[7] NanxinChen,YuZhang,HeigaZen,RonJWeiss,MohammadNorouzi,andWilliamChan. Wavegrad:
Estimatinggradientsforwaveformgeneration. InInternationalConferenceonLearningRepresentations,
2021.
[8] ZhifengKong,WeiPing,JiajiHuang,KexinZhao,andBryanCatanzaro. Diffwave:Aversatilediffusion
modelforaudiosynthesis. InInternationalConferenceonLearningRepresentations,2021.
[9] JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,MohammadNorouzi,andDavidJFleet.
Videodiffusionmodels. InAdvancesinNeuralInformationProcessingSystems,2022.
[10] RubenVillegas,MohammadBabaeizadeh,Pieter-JanKindermans,HernanMoraldo,HanZhang,Moham-
madTaghiSaffar,SantiagoCastro,JuliusKunze,andDumitruErhan. Phenaki: Variablelengthvideo
generationfromopendomaintextualdescriptions. InInternationalConferenceonLearningRepresenta-
tions,2023.
[11] OmerBar-Tal,HilaChefer,OmerTov,CharlesHerrmann,RoniPaiss,ShiranZada,ArielEphrat,Junhwa
Hur,YuanzhenLi,TomerMichaeli,etal. Lumiere:Aspace-timediffusionmodelforvideogeneration.
arXivpreprintarXiv:2401.12945,2024.
[12] OpenAI. Sora. https://openai.com/index/sora/,2024.
[13] FanBao,ChendongXiang,GangYue,GuandeHe,HongzhouZhu,KaiwenZheng,MinZhao,Shilong
Liu,YaoleWang,andJunZhu. Vidu:ahighlyconsistent,dynamicandskilledtext-to-videogeneratorwith
diffusionmodels. arXivpreprintarXiv:2405.04233,2024.
[14] JacobAustin,DanielDJohnson,JonathanHo,DanielTarlow,andRianneVanDenBerg. Structured
denoisingdiffusionmodelsindiscretestate-spaces.InAdvancesinNeuralInformationProcessingSystems,
2021.
[15] EmielHoogeboom,DidrikNielsen,PriyankJaini,PatrickForré,andMaxWelling. Argmaxflowsand
multinomialdiffusion:Learningcategoricaldistributions. InAdvancesinNeuralInformationProcessing
Systems,2021.
10[16] ClémentVignac,IgorKrawczuk,AntoineSiraudin,BohanWang,VolkanCevher,andPascalFrossard.
DiGress: Discretedenoisingdiffusionforgraphgeneration. InInternationalConferenceonLearning
Representations,2023.
[17] DongchaoYang,JianweiYu,HelinWang,WenWang,ChaoWeng,YuexianZou,andDongYu. Diffsound:
Discretediffusionmodelfortext-to-soundgeneration. IEEE/ACMTransactionsonAudio,Speech,and
LanguageProcessing,2023.
[18] NateGruver,SamuelStanton,NathanFrey,TimGJRudner,IsidroHotzel,JulienLafrance-Vanasse,Arvind
Rajpal,KyunghyunCho,andAndrewGWilson.Proteindesignwithguideddiscretediffusion.InAdvances
inNeuralInformationProcessingSystems,2023.
[19] Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H
Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion
forcategoricaldata. arXivpreprintarXiv:2211.15089,2022.
[20] TingChen,RuixiangZHANG,andGeoffreyHinton. Analogbits:Generatingdiscretedatausingdiffusion
modelswithself-conditioning. InInternationalConferenceonLearningRepresentations,2022.
[21] XiangLi,JohnThickstun,IshaanGulrajani,PercySLiang,andTatsunoriBHashimoto. Diffusion-LM
improvescontrollabletextgeneration. InAdvancesinNeuralInformationProcessingSystems,2022.
[22] IshaanGulrajaniandTatsunoriBHashimoto. Likelihood-baseddiffusionlanguagemodels. InAdvancesin
NeuralInformationProcessingSystems,2023.
[23] JustinLovelace,VarshaKishore,ChaoWan,EliotShekhtman,andKilianQWeinberger. Latentdiffusion
forlanguagegeneration. InAdvancesinNeuralInformationProcessingSystems,2024.
[24] PierreHRichemond,SanderDieleman,andArnaudDoucet. CategoricalSDEswithsimplexdiffusion.
arXivpreprintarXiv:2210.14784,2022.
[25] PavelAvdeyev,ChenlaiShi,YuhaoTan,KseniiaDudnyk,andJianZhou. Dirichletdiffusionscoremodel
forbiologicalsequencegeneration. InInternationalConferenceonMachineLearning,2023.
[26] AlexGraves,RupeshKumarSrivastava,TimothyAtkinson,andFaustinoGomez. Bayesianflownetworks.
arXivpreprintarXiv:2308.07037,2023.
[27] KaiwenXue,YuhaoZhou,ShenNie,XuMin,XiaoluZhang,JunZhou,andChongxuanLi. Unifying
Bayesianflownetworksanddiffusionmodelsthroughstochasticdifferentialequations. arXivpreprint
arXiv:2404.15766,2024.
[28] Guan-HorngLiu,TianrongChen,EvangelosTheodorou,andMoleiTao. Mirrordiffusionmodelsfor
constrainedandwatermarkedgeneration. InAdvancesinNeuralInformationProcessingSystems,2024.
[29] AndrewCampbell,JoeBenton,ValentinDeBortoli,ThomasRainforth,GeorgeDeligiannidis,andArnaud
Doucet. Acontinuoustimeframeworkfordiscretedenoisingmodels. InAdvancesinNeuralInformation
ProcessingSystems,2022.
[30] HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamTFreeman.Maskgit:Maskedgenerativeimage
transformer. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
2022.
[31] LinZheng,JianboYuan,LeiYu,andLingpengKong. Areparameterizeddiscretediffusionmodelfortext
generation. arXivpreprintarXiv:2302.05737,2023.
[32] AaronLou,ChenlinMeng,andStefanoErmon. Discretediffusionlanguagemodelingbyestimatingthe
ratiosofthedatadistribution. InInternationalConferenceonMachineLearning,2024.
[33] DiederikKingma,TimSalimans,BenPoole,andJonathanHo. Variationaldiffusionmodels. InAdvances
inNeuralInformationProcessingSystems,2021.
[34] TeroKarras,MiikaAittala,TimoAila,andSamuliLaine. Elucidatingthedesignspaceofdiffusion-based
generativemodels. InAdvancesinNeuralInformationProcessingSystems,2022.
[35] JoeBenton,YuyangShi,ValentinDeBortoli,GeorgeDeligiannidis,andArnaudDoucet. Fromdenoising
diffusionstodenoisingMarkovmodels. JournaloftheRoyalStatisticalSocietySeriesB:Statistical
Methodology,86(2):286–301,2024.
[36] Lingxiao Zhao, Xueying Ding, Lijun Yu, and Leman Akoglu. Improving and unifying discrete and
continuous-timediscretedenoisingdiffusion. arXivpreprintarXiv:2402.03701,2024.
11[37] AndrewCampbell,JasonYim,ReginaBarzilay,TomRainforth,andTommiJaakkola. Generativeflowson
discretestate-spaces:Enablingmultimodalflowswithapplicationstoproteinco-design. InInternational
ConferenceonMachineLearning,2024.
[38] JiaxinShi,YuhaoZhou,JessicaHwang,MichalisTitsias,andLesterMackey. Gradientestimationwith
discreteSteinoperators. InAdvancesinNeuralInformationProcessingSystems,2022.
[39] TimSalimansandDavidAKnowles.Onusingcontrolvariateswithstochasticapproximationforvariational
bayesanditsconnectiontostochasticlinearregression. arXivpreprintarXiv:1401.1022,2014.
[40] W. Kool, H. V. Hoof, and M. Welling. Buy 4 REINFORCE samples, get a baseline for free! In
DeepRLStructPred@ICLR,2019.
[41] MattMahoney. Text8. https://mattmahoney.net/dc/textdata.html. Accessed:2024-05-14.
[42] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/
OpenWebTextCorpus,2019.
[43] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever.Languagemodels
areunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[44] DenisPaperno,GermánKruszewski,AngelikiLazaridou,Ngoc-QuanPham,RaffaellaBernardi,Sandro
Pezzelle,MarcoBaroni,GemmaBoleda,andRaquelFernández. Thelambadadataset:Wordprediction
requiringabroaddiscoursecontext. InProceedingsofthe54thAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1:LongPapers),pages1525–1534,2016.
[45] MitchMarcus,BeatriceSantorini,andMaryAnnMarcinkiewicz. Buildingalargeannotatedcorpusof
english:Thepenntreebank. ComputationalLinguistics,19(2):313–330,1993.
[46] StephenMerity,CaimingXiong,JamesBradbury,andRichardSocher. Pointersentinelmixturemodels. In
InternationalConferenceonLearningRepresentations,2016.
[47] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
Robinson. Onebillion wordbenchmark formeasuring progressinstatistical languagemodeling. In
Interspeech,2014.
[48] EmielHoogeboom,AlexeyAGritsenko,JasmijnBastings,BenPoole,RiannevandenBerg,andTim
Salimans. Autoregressivediffusionmodels. InInternationalConferenceonLearningRepresentations,
2021.
[49] AndyShih,DorsaSadigh,andStefanoErmon. Trainingandinferenceonany-orderautoregressivemodels
therightway. InAdvancesinNeuralInformationProcessingSystems,2022.
[50] ZacharyZieglerandAlexanderRush. Latentnormalizingflowsfordiscretesequences. InInternational
ConferenceonMachineLearning,2019.
[51] DustinTran, KeyonVafa, KumarAgrawal, LaurentDinh, andBenPoole. Discreteflows: Invertible
generativemodelsofdiscretedata. InAdvancesinNeuralInformationProcessingSystems,2019.
[52] AäronVanDenOord,NalKalchbrenner,andKorayKavukcuoglu. Pixelrecurrentneuralnetworks. In
InternationalConferenceonMachineLearning,2016.
[53] AaronVandenOord,NalKalchbrenner,LasseEspeholt,OriolVinyals,andAlexGraves. Conditional
imagegenerationwithpixelcnndecoders. InAdvancesinNeuralInformationProcessingsystems,2016.
[54] TimSalimans,AndrejKarpathy,XiChen,andDiederikPKingma. Pixelcnn++:Improvingthepixelcnn
with discretized logistic mixture likelihood and other modifications. In International Conference on
LearningRepresentations,2016.
[55] XiChen,NikhilMishra,MostafaRohaninejad,andPieterAbbeel. Pixelsnail:Animprovedautoregressive
generativemodel. InInternationalConferenceonMachineLearning,2018.
[56] NikiParmar,AshishVaswani,JakobUszkoreit,LukaszKaiser,NoamShazeer,AlexanderKu,andDustin
Tran. Imagetransformer. InInternationalConferenceonMachineLearning,2018.
[57] RewonChild, ScottGray, AlecRadford, andIlyaSutskever. Generatinglongsequenceswithsparse
transformers. arXivpreprintarXiv:1904.10509,2019.
12[58] AurkoRoy, MohammadSaffar, AshishVaswani, andDavidGrangier. Efficientcontent-basedsparse
attentionwithroutingtransformers. TransactionsoftheAssociationforComputationalLinguistics,9:
53–68,2021.
[59] NikolaySavinov,JunyoungChung,MikolajBinkowski,ErichElsen,andAaronvandenOord. Step-
unrolleddenoisingautoencodersfortextgeneration. InInternationalConferenceonLearningRepresenta-
tions,2022.
[60] KehangHan,KathleenKenealy,AdityaBarua,NoahFiedel,andNoahConstant. Transferlearningfortext
diffusionmodels. arXivpreprintarXiv:2401.17181,2024.
[61] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
predictionwithrecurrentneuralnetworks. InAdvancesinNeuralInformationProcessingSystems,2015.
[62] PeterW.Glynn. Likelihoodratiogradientestimationforstochasticsystems. CommunicationsoftheACM,
33(10):75–84,1990.
[63] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. MachineLearning,8(3-4):229–256,1992.
[64] WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages4195–4205,2023.
13Table4: Maskingscheduleformulas.
Maskingschedules α Cross-entropylossweight
α′
t
t 1−αt
Linear 1−t −1
t
Polynomial 1−tw −w
t
Geometric exp(cid:0) −β¯1−tβ¯t (cid:1) − exp(−β¯ m1− intβ¯ mt ax) β¯1−tβ¯t log σmin
min max 1−exp(−β¯ m1− intβ¯ mt ax) min max σmax
Cosine 1−cos(π(1−t)) −πtan(π(1−t))
2 2 2
A Discrete-timederivation
Wedividetimefrom0to1intoT intervals, andlets(i) = (i−1)/T, t(i) = i/T. Theforward
transitionmatrixQ ∈R(m+1)×(m+1)(misvocabularysize)attimet(i)is
i
1 j =k =m
1−β
j =k ̸=m
[Q ] = i
i jk β k =m,j ̸=m
 i
0 otherwise
ormorecompactlywrittenas
Q =(1−β )I+β 1e⊤,
i i i m
where1denotesanall-onevectorofsizem+1,ande isanone-hotvectorofsizem+1withthe
m
m-thelement(recallthatcountingstartsfrom0)beingone. Weuseanone-hotvectorx oflength
t
m+1todenotethediscretestate. Theforwardconditionalsaredefinedas
q(x |x )=Cat(x ;Q⊤x )=x⊤ Q x , (15)
t(i) s(i) t(i) i s(i) s(i) i t(i)
whereQ⊤x istheprobabilitiesforeachofthem+1categoriesthatx cantake. Themarginal
i s(i) t(i)
forwarddistributionattimet(i)givenx is
0
q(x |x )=Cat(x ;Q¯⊤x )=x⊤Q¯ x ,
t(i) 0 t(i) i 0 0 i t(i)
whereQ¯ =(cid:81)i Q =(cid:81)i (1−β )I +(cid:0) 1−(cid:81)i (1−β )(cid:1) 1e⊤. Toseewhatthisleadstoin
i j=1 j j=1 j j=1 j m
continuoustime,weletβ = β(t(i)) andT →∞:
i T
i i
(cid:89) (cid:16)(cid:88) (cid:17)
(1−β )=exp log(1−β )
j j
j=1 j=1
i
(cid:16)(cid:88) β(t(j)) (cid:17)
=exp − +o(1/T)
T
j=1
(cid:16) (cid:90) t(i) (cid:17)
T→∞
→ exp − β(s)ds .
0
WeletQ¯(t)denotethelimitofQ¯ inthiscase:
i
Q¯(t)=exp(cid:0) −(cid:90) t β(s)ds(cid:1) I+(cid:16) 1−exp(cid:0) −(cid:90) t β(s)ds(cid:1)(cid:17) 1e⊤
m
0 0
≜α I+(1−α )1e⊤.
t t m
Herewedefineα
≜exp(−(cid:82)t
β(s)ds). Andthemarginalforwardtransitionis
t 0
q(x |x )=Cat(x ;Q¯(t)⊤x )=x⊤Q¯(t)x =α x⊤x +(1−α )e⊤x . (16)
t 0 t 0 0 t t 0 t t m t
14B Continuous-timederivation
Weconsideracontinuous-timeMarkovchainwithtransitionrates
Q(t)=(Q −I)/(1/T)=β(t)(1e⊤ −I). (17)
i m
For simplicity, we let Q = 1e⊤ − I. The marginal forward distribution at time t given x is
m 0
q(x |x )=Cat(x ;Q¯(t)⊤x ),where
t 0 t 0
(cid:16)(cid:90) t (cid:17) (cid:16) (cid:90) t (cid:17)
Q¯(t)=exp Q(s)ds =exp Q β(s)ds =exp(β¯(t)Q).
0 0
Herewedefineβ¯(t)≜(cid:82)t
β(s)ds. Thematrixexponentialcanbecomputedviaeigendecomposition:
0
β¯(t)Q=UΛU−1,
where
1
U =I−e e⊤ + √ 1e⊤,
m m n+1 m
√
U−1 =I+ n+1e e⊤ −1e⊤,
m m m
Λ=β¯(t)(e e⊤ −I),
m m
andthusexp(Λ)=α I+(1−α )e e⊤,
t t m m
Q¯(t)=Uexp(Λ)U−1 =α I+(1−α )1e⊤.
t t m
Asimplerderivationusesthefollowingproperty:
Q2 =−Q.
Therefore,
Q¯(t)=exp(β¯(t)Q)
1 1
=I+β¯(t)Q+ β¯(t)2Q2+ β¯(t)3Q3+...
2 3
1 1
=I+Q−(1−β¯(t)+ β¯(t)2− β¯(t)3+...)Q
2 3
=I+Q−exp(−β¯(t))Q
=α I+(1−α )1e⊤.
t t m
Thismarginalforwardtransitionmatrixattimetcoincideswiththeresult(1)wegetbytakingthe
limitofdiscrete-timederivation.
Arbitrarydiscretizationofthecontinuous-timeforwardprocess. Forthediscrete-timeprocess
wehavedefinedtheper-steptransitionin(15). Forthecontinuous-timeprocess,wecanderivethe
transitionmatrixQ¯(s,t) ≜q(x =j|x =i)betweentwoarbitrarytimesandtasthesolutionto
ij t s
thefollowingdifferentialequation(knownasKolmogorovforwardequation)
d
Q¯(s,t)=Q¯(s,t)Q(t)whereQ(t)=β(t)Q
dt
withinitialconditionQ¯(s,s)=I. Thesolutionisgivenby
Q¯(s,t)=exp(cid:0) (β¯(t)−β¯(s))Q(cid:1) =Q¯(s)−1Q¯(t).
Routinework(usingtheWoodburymatrixinversionlemma)showsthat
Q¯(t)−1 =α−1I+(1−α−1)1e⊤.
t t m
Pluggingtheresultback,wegettheforwardtransitiondistributionfromstot:
q(x |x )=Cat(x ;Q¯(s,t)⊤x )=x⊤Q¯(s,t)x , (18)
t s t s s t
whereQ¯(s,t)≜Q¯(s)−1Q¯(t)= α tI+(cid:0) 1− α t(cid:1) 1e⊤.
α α m
s s
15C Timereversaloftheforwardprocessgivenx
0
Theanalyticpropertyofourforwardprocessallowstocomputemanyquantitiesofinterestinclosed
form. One such quantity frequently used in diffusion models is the time reversal of the forward
processgivenx : q(x |x ,x ). Wecancomputeitusing(16)and(18)as
0 s t 0
q(x |x )q(x |x )
q(x |x ,x )= t s s 0
s t 0 q(x |x )
t 0

α 1s −− αα ttx⊤ sx 0 x s ̸=m,x t =m
= x1 1− − ⊤α α xs t x xs ̸== mm .,x t =m (19)
s t t
WecanrewritetheaboveusingreversetransitionmatrixR¯x0(t,s)∈R(m+1)×(m+1)as
α −α
q(x |x ,x )=Cat(x ;R¯x0(t,s)⊤x ), whereR¯x0(t,s)=I+ s te (x −e )⊤.
s t 0 s t 1−α m 0 m
t
Wearealsointerestedinwhatwouldhappenintheinfinitesimaltimelimit,i.e.,whens=t−∆t
and∆t→0. Notethat d dα tt =−β(t)α t,
α −α =α′∆t+o(∆t).
t−∆t t t
Pluggingitintotheoriginalformula,weget
α′
R¯x0(t,t−∆t)=I+ t e (x −e )⊤∆t+o(∆t).
1−α m 0 m
t
ComparingtheabovewiththetransitionratematrixRx0(t)definition
R¯x0(t,t−∆t)=I+Rx0(t)∆t+o(∆t),
wehavedeterminedthetransitionratematrixforthereverseprocessconditionedonx :
0
α′
Rx0(t)= t e (x −e )⊤. (20)
1−α m 0 m
t
D DetailsoftheELBO
Using(19)and(3),wecomputetheKLdivergencesbetweenforwardandbackwardtransitions
KL(q(x |x ,x )∥p (x |x ))=KL(q(x |x ,x )∥q(x |x ,µ (x ,t))) (21)
s t 0 θ s t s t 0 s t θ t
(cid:40)(cid:80)m
q(x |x ,x )log q(xs|xt,x0) x =m
= xs=0 s t 0 q(xs|xt,µθ(xt,t)) t
0 x ̸=m
t
=δ (cid:88) α s−α tx⊤e log x⊤ 0e k
xt=m 1−α 0 k µ (x ,t)⊤e
t θ t k
k̸=m
α −α
=−δ s tx⊤logµ (x ,t).
xt=m 1−α 0 θ t
t
Notethat0log0=0. Inthiscase,thereconstructiontermbecomes
E [logp(x |x
)]=(cid:88)m
q (k|x )log
q t(1)|0(k|x 0)
q(xt(1)|x0) 0 t(1) t(1)|0 0 (cid:80) q (k|j)
k=0 j̸=m t(1)|0
α 1
t(1)
=α ·log +(1−α )log
t(1) α t(1) m
t(1)
=−(1−α )logm.
t(1)
ThepriorKLtermcanbecomputedas
KL(q(x |x )∥p(x ))=KL(δ ∥δ )=0.
1 0 1 x1,m x1,m
16Asusual,wetakethecontinuous-timelimitbylettingT →∞:
L ≜ lim L
∞ T
T→∞
= lim (cid:88)T −α s(i)−α t(i)s(i)−t(i) x⊤E (cid:2) δ logµ (x ,t(i))(cid:3)
T→∞ s(i)−t(i) 1−α t(i) 0 q(xt(i)|x0) xt(i),m θ t(i)
i=2
(cid:90) 1 α′
= t x⊤E [δ logµ (x ,t)]dt.
1−α 0 q(xt|x0) xt,m θ t
t(1) t
E AvoidingundefinedKLdivergence
Whendefiningtheforwardprocess,weoftendonotwantα tobeexactly0,orequivalently,λ to
1 1
be∞fornumericalstabilityreasons. Instead,wesetλ tobeafinitevalue,andtherebyα hasa
1 1
smallpositivevalue. Thishasaproblemthatthesupportofq(x |x )isnolonger{m}andinstead
1 0
becomes{m,x }. Asaresult,theKLdivergencebetweenq(x |x )andp(x )isundefinedbecause
0 1 0 1
q(x |x )isnotabsolutelycontinuouswithrespecttop(x )=δ . Toresolvetheissue,wemodify
1 0 1 x1,m
thepriordistributionp(x )suchthatithassupportoverallm+1values. Onesuchchoiceisletting
1
p(x )= α 1 (cid:88) δ +(1−α )δ .
1 m x1,j 1 x1,m
j̸=m
Then,thepriorKLdivergencetermbecomes
m
KL(q(x |x )∥p(x ))=
(cid:88)
q(x |x
)logq(x 1|x 0)
1 0 1 1 0 p(x )
1
x1=0
m
= (cid:88) (α δ +(1−α )δ )logα 1δ x1,x0 +(1−α 1)δ x1=m
1 x1,x0 1 x1,m p(x )
1
x1=0
α 1−α
=α log 1 +(1−α )log 1
1 α /m 1 1−α
1 1
=α logm.
1
F UnifyingExistingMaskedDiffusionModels
F.1 TheCTMCpointofview
Wefirstprovealemmathatconnectstheforwardandreversetransitionratematrices. Thisfollows
fromtheresultsin[29]butwegiveaproofforcompleteness.
Lemma2. TheforwardtransitionratematrixQ(t)andthereversetransitionratematrix(givenx )
0
Rx0(t)satisfy:
q (j|x )
Rx0(t) =Q(t) t|0 0 forj ̸=k. (22)
kj jkq (k|x )
t|0 0
Proof Considerthebackwardtransitionfromtimet+τ tot. Forj ̸=k,Bayes’ruleyields
q(x =j|x )q(x =k|x =j)
q(x =j|x =k,x )= t 0 t+τ t
t t+τ 0 q(x =k|x )
t+τ 0
q(x =j|x )(δ +Q(t) τ +o(τ))
= t 0 jk jk
q(x =k|x )
t+τ 0
q(x =j|x )
τ =→0 δ + t 0 Q(t) τ +o(τ).
kj q(x =k|x ) jk
t 0
Then,itfollowsfromthedefinitionofthetransitionratematrixthatRx0(t)
kj
=Q(t) jkqq tt || 00 (( kj| |x x0 0) ).
17Proposition3. WeusetheshorthandR (t) todenotetheapproximatebackwardtransitionrate
θ kj
fromthestatektoj obtainedbysubstitutingourpredictionmodelµ θ(k)forx 0inRx0(t) kj. Then,
thecontinuous-timeobjective(4)canbeequivalentlyexpressedas
(cid:90) 1 (cid:104) (cid:88) (cid:105)
L =− E R (t) + Q(t) logR (t) dt+C, (23)
∞ qt|0(k|x0) θ kk kj θ jk
t(1) j̸=k
whereC isaconstantindependentofθ.
Proof To rewrite our objective L with the transition rate matrices, we first go back to (21).
∞
There,insteadofpluggingintheexplicitformofR¯x0(t,s),wesubstituteitwith(7)whichleverages
the transition rate Rx0(t). To simplify the notation, we assume x
t
= k and use the shorthand
R θ(t)
kj
≜Rµθ(k)(t) kj. Wethenhave
KL(q(x |x ,x )∥p (x |x ))
t−∆t t 0 θ t−∆t t
=KL(Cat(x s;R¯x0(t,t−∆t)⊤e k)∥Cat(x s;R¯µθ(k)(t,t−∆t)⊤e k))
=(cid:88)m
e⊤(I+Rx0(t)∆t+o(∆t))e
loge⊤ k(I+Rx0(t)∆t+o(∆t))e
j
k j e⊤(I+R (t)∆t+o(∆t))e
j=0 k θ j
=(1+Rx0(t)
∆t)log1+Rx0(t) kk∆t+o(∆t)
kk 1+R (t) ∆t+o(∆t)
θ kk
+(cid:88)
(Rx0(t)
∆t)logRx0(t) kj∆t+o(∆t)
+o(∆t)
kj R (t) ∆t+o(∆t)
θ kj
j̸=k
=(Rx0(t) −R (t)
)∆t+(cid:88)
(Rx0(t)
∆t)logRx0(t) kj∆t+o(∆t)
+o(∆t).
kk θ kk kj R (t) ∆t+o(∆t)
θ kj
j̸=k
Forthelastidentity,wehaveusedthefactthatlog(1+x)=x+o(x). ToobtainL ,wetakethe
∞
limitofL asT →∞,whichisequivalenttoletting∆t=1/T →0. Weobtain
T
T
(cid:88)
L = lim E [KL(q(x |x ,x )∥p (x |x ))]
∞
T→∞
q(xt(i)|x0) s(i) t(i) 0 θ s(i) t(i)
i=2
T
(cid:88) (cid:104)(cid:16)
= lim E Rx0(t(i)) −R (t(i))
T→∞
q(xt(i)|x0) kk θ kk
i=2
+(cid:88)
Rx0(t(i))
logRx0(t(i)) kj∆t+o(∆t)(cid:17) ∆t+o(∆t)(cid:105)
kj R (t(i)) ∆t+o(∆t)
θ kj
j̸=k
=(cid:90) 1
E
(cid:104)
Rx0(t) −R (t)
+(cid:88)
Rx0(t)
logRx0(t) kj(cid:105)
dt.
qt|0(k|x0) kk θ kk kj R (t)
t(1) j̸=k θ kj
NotethatRx0(t)isaconstantmatrixindependentofθ. AbsorbingallconstanttermsintoC,wehave
(cid:90) 1 (cid:104) (cid:88) (cid:105)
L =− E R (t) + Rx0(t) logR (t) dt+C.
∞ qt|0(k|x0) θ kk kj θ kj
t(1) j̸=k
Next,wesubtituteRx0(t)withtheforwardtransitionrateusingLemma2:
L
=−(cid:90) 1
E
(cid:104)
R (t)
+(cid:88)
Q(t)
q t|0(j|x 0)
logR (t)
(cid:105)
dt+C
∞ qt|0(k|x0) θ kk jkq (k|x ) θ kj
t(1) j̸=k t|0 0
(cid:90) 1 (cid:104)(cid:88)m (cid:88)m (cid:88) (cid:105)
=− q (k|x )R (t) + Q(t) q (j|x )logR (t) dt+C
t|0 0 θ kk jk t|0 0 θ kj
t(1) k=0 k=0j̸=k
(cid:90) 1 (cid:104)(cid:88)m (cid:88)m (cid:88) (cid:105)
=− q (k|x )R (t) + Q(t) q (k|x )logR (t) dt+C,
t|0 0 θ kk kj t|0 0 θ jk
t(1) k=0 k=0j̸=k
18where the last identity used the discrete analog to integration-by-part (or summation-by-part):
(cid:80) (cid:80) (cid:80) (cid:80)
f(j,k)= f(k,j). Rearrangingthetermsthengives(23).
k=0 j̸=k k=0 j̸=k
F.2 DifferencesfromCampbelletal.[29]
Campbelletal.[29]usedthefirsttermof(8)asthetrainingloss. Akeylimitationofthislossfunction
isfromtheinnersummationterm
(cid:88)
Q(t) logR (t) .
kj θ jk
j̸=k
RecallthatR (t) iscomputedassubstitutingourneuralnetworkpredictionmodelµ (j)forx in
θ jk θ 0
Rx0(t) jk. Therefore,thesummationtogetherwithR θ(t)
kk
requiresmevaluationsofµ θ(·). Thisis
prohibitivesincetheneuralnetworkmodelisusuallyexpensive. Toresolvethisissue,Campbelletal.
[29]proposedtorewritethesumas
E [Z logR (t) ] where q˜(j|k)= Q(t) kj,Z ≜ (cid:88) Q(t)
j∼q˜(·|k) k θ jk Z k kj′
k
j′̸=k
andestimateitthroughMonteCarlo. Takingintoaccounttheouterexpectationunderq (k|x ),
t|0 0
thecomputationofthelossthenbecomesadoublystochasticestimate(usingk ∼ q (k|x )and
t|0 0
j ∼ q˜(j|k))whichsuffersfromlargevariance. Incontrast,theformofourloss(4)onlyrequires
evaluatingµ onceforasinglestochasticestimationoftheexpectationw.r.t. q(x |x ).
θ t 0
F.3 Scoreparameterization
Weprovideasimplerderivationofthescore-basedloss[32,35]below. Westartfromtheformofthe
ELBOin(23)andrewriteitas
L
∞
=(cid:90) t(1
1)E
qt|0(k|x0)(cid:104)(cid:88) j̸=k(cid:16)
Rµθ(t)
kj
−Rx0(t)
kj
+Rx0(t) kjlog
RR µx0 θ( (t t)
)k kj
j(cid:17)(cid:105)
dt. (24)
Forthelastidentityweusedthezero-row-sumpropertyoftransitionratematrix:
(cid:88)
Rx0(t) =− Rx0(t) .
kk kj
j̸=k
Ifweplug(22)into(24)andreparameterizewithascoremodel
q (j|µ (x ))
s (x ) ≜ t|0 θ t , (25)
θ t j q(x |µ (x ))
t θ t
werecoverthescoreentropylossfunctionfromLouetal.[32],Bentonetal.[35]:
L
=(cid:90) 1
E
(cid:104)(cid:88)
Q(t)
(cid:16)
s (k) −
q t|0(j|x 0)
logs (k)
+ψ(cid:16)q t|0(j|x 0)(cid:17)(cid:17)(cid:105)
dt,
∞ qt|0(k|x0) jk θ j q (k|x ) θ j q (k|x )
t(1) j̸=k t|0 0 t|0 0
where ψ(y) ≜ ylogy −y. Note that our derivation above is different and simpler than that of
Campbelletal.[29](whichLouetal.[32]isbasedon)sinceweleveragetheconditionalreverse
transitionrategivenx insteadofthetransitionratematrixofthereverseprocess. Wecanfurther
0
simplifythelosswiththefollowingrelationshipbetweentheconditionalscoreandx :
0
q (j|x ) x⊤Q¯(t)e α
t|0 0 = 0 j = t x⊤e fork =m,j ̸=k. (26)
q (k|x ) x⊤Q¯(t)e 1−α 0 j
t|0 0 0 k t
Note that only the result under the case k = m is needed. This is because when x is un-
t
masked, at any time between 0 and t, the state must stay unchanged and remain x . As a
0
19result, KL(q(x |x ,x )∥p (x |x )) = 0 for x ̸= m. From (17), we know Q(t) =
t−∆t t 0 θ t−∆t t t jk
β(t)(δ −δ ). Combining(26)and(9),weget
mk jk
L =(cid:90) 1 β(t)(cid:16) E (cid:2) δ (cid:0)(cid:88) s (k) − α t x⊤logs (k)(cid:1)(cid:3) +ψ(cid:0) α t (cid:1)(cid:17) dt. (27)
∞ qt|0(k|x0) mk θ j 1−α 0 θ 1−α
t(1) j̸=k t t
Further,wecanshowtheconnectionbetween(27)and(4)byrevertingthescoreparameterization
toameanparameterizationusing(25),orequivalentlys θ(x t) j = 1−α αt tµ θ(x t)⊤e j. Bydoingso,we
obtain
L =(cid:90) 1 β(t)(cid:16) E (cid:2) δ (cid:0)(cid:88) s (k) − α t x⊤logµ (k)(cid:3) + α t (cid:1) dt.
∞ qt|0(k|x0) mk θ j 1−α 0 θ 1−α
t(1) j̸=k t t
Observingthat
(cid:88) s (m) = α t , (28)
θ j 1−α
t
j̸=m
we conclude that this recovers the objective in (4). Interestingly, in Lou et al. [32] the score
parameterizationisnotconstrainedtosatisfy(28). Thatmeansthelearnedbackwardmodelmightbe
incompatiblewiththeforwardprocess.
ItiseasytoproveProposition1fromEquation(26).
ProofofProposition1
q t(j)
=
(cid:80) x0q t|0(j|x 0)q(x 0)
=
(cid:80) x0q t|0(j|x 0)q 0|t(x 0|m)
=E
(cid:20) q t|0(j|x 0) (cid:21)
q (m) q (m) q (m|x ) x0|xt=m q (m|x )
t t t|0 0 t|0 0
(cid:20) (cid:21)
α α
=E t x⊤e = t E[x |x =m]⊤e .
x0|xt=m 1−α 0 j 1−α 0 t j
t t
F.4 DifferencesfromSundae
Sundae [59, 60] uses a linear noise schedule and uniformly corrupts data with random tokens in
the vocab (Uniform style). Additionally it uses a second loss term from cross entropy between
un-corrupteddataand1-stepunrolledmodelprediction. Similarideashavebeenproposedin[61].
G Detailsforstate-dependentrates
G.1 Derivationsandtimecontinuouslimit
All derivations in this section assume that x is a single token, while for N tokens the masked
t
diffusionwithstate-dependentratesfactorisesacrosstheN tokens. LearningfromdataofN tokens
usingvariationalinferenceisdiscussedinSectionG.2.
Giventheforwardtransitionq(x |x )andmarginalq(x |x )derivedinmaintext(Section5)The
t s s 0
reversalgivenx 0isq(x s|x t,x 0)=Cat(x s;R¯x0(t,s)⊤x t)for
R¯x0(t,s) jk
=  δ(cid:0) (cid:0)α
1
11
−
−s −−
α
ααα
s
ttt (cid:1)(cid:1) ⊤⊤ xx 00x⊤ 0e k
j
jj
=
̸==
m
mm
,
., kk ≠= mm
jk
oralternativelycanbewrittenas
q(x |x )q(x |x )
q(x |x ,x )= t s s 0
s t 0 q(x |x )
t 0
=
(cid:104) α α⊤ t
⊤
sx xs sx⊤ sx t+(1− α α⊤ t
⊤
sx xs s)e⊤ mx t(cid:105)(cid:2) α s⊤x 0x⊤ 0x s+(1−α s⊤x 0)e⊤ mx s(cid:3)
. (29)
(cid:2) (cid:3)
α⊤x x⊤x +(1−α⊤x )e⊤x
t 0 0 t t 0 m t
20Tosimplifythisexpressionweconsiderthetwocases: eitherx = m(i.e.x ismask)orx ̸= m
t t t
whereinthesecondcasex =x . Forthecasex =m,thedenominatorin(29)simplifiesas
t 0 t
q(x =m|x )=1−α⊤x
t 0 t 0
duetox⊤x =0sincex ̸=m,i.e.theobservedtokenx cannotbeamask. Thengiventhatx =m
0 t 0 0 t
theprobabilitythatx =x =mis
s t
1−α⊤x (1−α )⊤x (cid:18) 1−α (cid:19)⊤
s 0 = s 0 = s x (30)
1−α⊤x (1−α )⊤x 1−α 0
t 0 t 0 t
whiletheremainingprobabilityforx =x ̸=mis
s 0
(α −α )⊤x (α −α )⊤x (cid:18) α −α (cid:19)⊤
s t 0 = s t 0 = s t x . (31)
1−α⊤x (1−α )⊤x 1−α 0
t 0 t 0 t
Then,combining(30)and(31)towriteq(x |x = m,x )inanunifiedwayyieldstheexpression
s t 0
(13)inthemainSection5. Inthesecondcase,whenx = x ̸= m,q(x |x ̸= m,x )from(29)
t 0 s t 0
simplifiesdramaticallyanditbecomesq(x |x ̸= m,x ) = x⊤x whichisapointmassthatsets
s t 0 t s
x =x .
s t
Derivation of the continuous-time limit of the loss in (14). To simplify the notation, we let
ξ s,t ≜ α 1s −− αα tt. WefirstcomputetheKLdivergencetermsinthediscrete-timeELBOas
KL(q(x |x ,x )∥p (x |x ))
s t 0 θ s t
(cid:40)(cid:80)m
q(x |x ,x )logq(xs|xt,x0) x =m
= xs=0 s t 0 pθ(xs|xt) t
0 x ̸=m
t
=δ (cid:104) (cid:88) ξ⊤x x⊤e log ξ s⊤ ,tx 0x⊤ 0e k +(1−ξ )⊤x log (1−ξ s,t)⊤x 0 (cid:105)
xt,m s,t 0 0 k ξ⊤diag(µ (x ,t))e s,t 0 (1−ξ )⊤µ (x ,t)
k̸=m s,t θ t k s,t θ t
(cid:104) (1−ξ )⊤x (cid:105)
=δ −ξ⊤x x⊤logµ (x ,t)+(1−ξ )⊤x log s,t 0 .
xt,m s,t 0 0 θ t s,t 0 (1−ξ )⊤µ (x ,t)
s,t θ t
Let∆ ≜ 1 =t(i)−s(i)foralli. Pluggingα =α −α′∆t+o(∆t)intotheaboveformula
t T t−∆t t t
andlettingγ t = 1−α α′ t t,weget
KL(q(x |x ,x )∥p (x |x ))
s t 0 θ s t
=δ (cid:20) γ⊤x x⊤logµ (x ,t)∆t+(cid:0) 1+γ⊤x ∆t(cid:1) ·log 1+γ t⊤x 0∆t+o(∆t) +o(∆t)(cid:21)
xt,m t 0 0 θ t t 0 1+γ⊤µ (x ,t)∆t+o(∆t)
t θ t
=δ (cid:2) γ⊤x x⊤logµ (x ,t)∆t+(cid:0) 1+γ⊤x ∆t(cid:1)(cid:0) γ⊤x ∆t−γ⊤µ (x ,t)∆t+o(∆t)(cid:1) +o(∆t)(cid:3)
xt,m t 0 0 θ t t 0 t 0 t θ t
=δ (cid:2) γ⊤x x⊤logµ (x ,t)∆t+γ⊤x ∆t−γ⊤µ (x ,t)∆t+o(∆t)(cid:3)
xt,m t 0 0 θ t t 0 t θ t
=δ ·γ⊤(x x⊤logµ (x ,t)+x −µ (x ,t))∆t+o(∆t).
xt,m t 0 0 θ t 0 θ t
Therefore,
T
(cid:88)
lim E [KL(q(x |x ,x )∥p (x |x ))]
T→∞
q(xt(i)|x0) s(i) t(i) 0 θ s(i) t(i)
i=2
T
(cid:88)
= lim E [δ ·γ⊤(x x⊤logµ (x ,t(i))+x −µ (x ,t(i)))∆t+o(∆t)]
T→∞
q(xt(i)|x0) xt(i),m t 0 0 θ t(i) 0 θ t(i)
i=2
(cid:90) 1
= γ⊤E [δ ·(x x⊤logµ (x ,t)+x −µ (x ,t))]dt.
t q(xt(i)|x0) xt,m 0 0 θ t 0 θ t
t(1)
Lettingt(1)→0provestheresult.
21G.2 Trainingandgradientestimation
ThemodelisappliedtodataconsistedofN tokenswherex =(x1,...,x(N))andwhereeachstate
0 0 0
inthemaskeddiffusionisx = (x1,...,x(N)). Thebackwardgeneratedmodelhasafactorizing
t t t
transitionconditionaloftheform(cid:81)N p (x(n)|x )wherep (x(n)|x )=q(x(n)|x(n),µ(n)(x ,t))
n=1 θ s t θ s t s t θ t
hasaformthatdependsonwhetherx(n) =morx(n) ̸=m. Forthefirstcase:
t t
(cid:16)1−α (cid:17)⊤ (cid:16)α −α (cid:17)⊤
p (x(n)|x(n) =m,{x(k)} )= s µ(n)(x ,t)e⊤x(n)+ s t diag(µ(n)(x ,t))x(n),
θ s t t k̸=n 1−α θ t m s 1−α θ t s
t t
whereµ(n)(x ,t)=softmax(f (x ))isam+1dimensionalprobabilityvectormodelledbyaNN
θ t θ t
(wherethefinalvalueisconstrainedtobezerosinceµ(n)(x ,t)isareconstructionofx(n) which
θ t 0
cannotbemask, soinpracticetheNNclassifierneedstohaveasoftmaxoutputonlyoverthem
actualtokenclasses). Crucially,notethattheNNclassifierreceivesasinputthefullstatex ofall
t
tokens,whileadditionaltimefeaturestoencodetarealsoincluded. Whenx(n) ̸=mthebackward
t
transitionmodelissettobep (x |x(n) ̸=m,{x(k)} )=(x(n))⊤x(n) whichmatchesprecisely
θ s t t k̸=n t s
q(x(n)|x(n) =m,x(n))=(x(n))⊤x(n)fromtheforwardprocess.
s t 0 t s
Thefullnegativelowerboundforstate-dependentratesandassumingN tokensisgivenby
 
L =(cid:90) 1(cid:16) α t′ (cid:17)⊤ E  (cid:88) (x(n)−µ(n)(x ,t)+x(n)(x(n))⊤logµ(n)(x ,t))dt.
∞ 1−α q(xt|x0) 0 θ t 0 0 θ t 
0 t
n:x(n)=m
t
Giventhateachα
t,i
=1−twi,thebackwardmodelbecomes
p θ(x( sn)|x( tn) ̸=m,{x( tk)} k̸=n)=(cid:0) ewlogs t(cid:1)⊤ µ( θn)(x t,t)e⊤ mx( sn)+(cid:0) 1−ewlogs t(cid:1)⊤ diag(µ( θn)(x t,t))x( sn),
where w is them+1 dimensional vectorof all w s. Note that theprobability of x(n) staying in
i s
themaskstate, i.e., x( sn) = mdependsonthefullx
t
anditisgivenby(cid:0) ewlogs t(cid:1)⊤ µ( θn)(x t,t) =
(cid:80)m i=− 01ewilogs tµ( θn)(x t,t)
i
whiletheprobabilityforx( sn) totakeacertainnon-masktokenvaluei
is(cid:0) 1−ewilogs t(cid:1) µ( θn)(x t,t) i.Thegradientwrttisα t′
,i
=−w itwi−1and 1−α α′ t,i
t,i
=−w ti theabove
lossiswrittenas
 
L =−(cid:90) 1 1 w⊤E  (cid:88) (x(n)−µ(n)(x ,t)+x(n)(x(n))⊤logµ(n)(x ,t))dt,
∞ t q(xt|x0) 0 θ t 0 0 θ t 
0
n:x(n)=m
t
wherewisthevectorofallw ’s. AnunbiasedgradientovertheNNparametersθisstraightforward
i
toobtainsincewejustneedtosampleonetimepointtandanx ∼ q(x |x )toapproximatethe
t t 0
integralandexpectationandthenusethegradient:
−∇ (cid:88) 1 w⊤(cid:16) x(n)−µ(n)(x ,t)+x(n)(x(n))⊤logµ(n)(x ,t)(cid:17) .
θ t 0 θ t 0 0 θ t
n:x(n)=m
t
The gradient wrt the w parameters is more complex since these parameters appear also in the
discretedistributionq(x |x )whichisnotreparametrizable. TodealwiththisweneedREINFORCE
t 0
unbiasedgradients [62,63],andinourimplementationweconsiderREINFORCEleave-one-out
(RLOO)[39,40]withtwosamples. Firstly,theexactgradientwrtwoftheexactlossiswrittenas
(cid:90) 1 1 (cid:90) 1 1
− E [g(x ,x )]dt− E [f(x ,x )∇ logq(x |x )]dt. (32)
t q(xt|x0) t 0 t q(xt|x0) t 0 w t 0
0 0
where
g(x ,x )= (cid:88) (x(n)−µ(n)(x ,t)+x(n)(x(n))⊤logµ(n)(x ,t)), f(x ,x )=w⊤g(x ,x ).
t 0 0 θ t 0 0 θ t t 0 t 0
n:x(n)=m
t
22Notethatg(x ,x )isavectorwhilef(x ,x )isascalar. Thelefttermin(32)iseasysinceitjust
t 0 t 0
requiressamplingtandx ∼q(x |x ),whiletherighttermistheREINFORCEtermwhichcould
t t 0
havehighvariance. ForthissecondtermweuseRLOOwithtwosamplesx1,x2andconstructthe
t t
unbiasedestimate
−1 (cid:0) ∇ logq(x1|x )−∇ logq(x1|x )(cid:1)(cid:2) f(x1,x )−f(x2,x )(cid:3) .
2t w t 0 w t 0 t 0 t 0
Thus,theoverallunbiasedgradientforwweuseis
−1 (cid:8) g(x1,x )+g(x2,x )+(cid:0) ∇ logq(x1|x )−∇ logq(x2|x )(cid:1)(cid:2) f(x1,x )−f(x2,x )(cid:3)(cid:9)
2t t 0 t 0 w t 0 w t 0 t 0 t 0
H AdditionalResultsandExperimentalDetails
Inallexperiments,themodelistrainedwithacontinuous-timelosswhilesamplesaredrawnfrom
thediscrete-timereversemodelof1000timestepsunlessotherwisenoted. Weusedanexponential
movingaveragefactor0.9999forallevaluationincludingsamplegeneration.
H.1 text8
WefollowedthestandarddatasetsplitasinAustinetal.[14],Louetal.[32]andtrainedourmodels
ontextchunksoflength256for1millionstepswithbatchsize512. Allmodelsinthetableuseda
standard12-layertransformerarchitectureunlessotherwisenoted. Ourtransformerhasalsothesame
numberofheads(12)andhiddendimension(784)asinAustinetal.[14],Louetal.[32].
Weusedthecontinuous-timeELBOanddrewonesampleoftforeachdatatoestimatetheintegral.
Toreducethevarianceoftraining,weusedthesameantitheticsamplingtrickdescribedinKingma
etal.[33]forcontinuousdiffusionmodels. Weusedthelinearmaskingscheduleα = 1−tand
t
addedasmallshiftϵ = 10−4 whentiscloseto0and1toensurenumericalstability. Theshifted
scheduleisα =(1−2ϵ)(1−t)+ϵ. Theshiftleadstoasupportmismatchbetweenq(x |x )andthe
t 1 0
priorp(x ),leadingtoanundefinedKLdivergenceterm. WeexplaininappendixEhowtomodify
1
thepriordistributiontoallowsmalluniformprobabilitiesinnon-maskstatestomitigatethisproblem.
Theshiftleadstoanon-zeroreconstructiontermandKLdivergencetermforthepriordistribution
butbothareofnegligiblescalesowecansafelyignorethemwhenreportingtheELBO.
Weusedacosinelearningrateschedulewithalinearwarmupof2000steps.Weappliedchannel-wise
dropoutofrate0.05andusedAdamWoptimizerwithlearningrate0.0003andaweightdecayfactor
of0.03. Ourmodelistrainedon16TPU-v5liteforlessthanaday.
H.2 OpenWebText
AdditionalunconditionalgenerationfromourMD4MediummodelisshowninAppendixI.
Wekept2%oftheoriginaltrainingsetforvalidation. Oursmallandmediumtransformermodelhave
thesamenumberoflayers,heads,andhiddendimensionsasinLouetal.[32]andourtokenizerwas
alsokeptthesamewithavocabularysizeofaround50K.Thetrainingobjective,maskingschedule
andotherarchitecturalchoiceswerekeptthesamewiththetext8experiment. Wekeptthetraining
hyperparametersthesameastext8experimentexceptthatwereducedthedropoutrateto0.02.
H.3 Images
AdditionalunconditionalgenerationfromourMD4modelispresentedinFigure5.
Weusedthesamelinearmaskingscheduleasinpreviousexperimentsinallreportedresults. Wehave
observedthatthecosinescheduleleadstobettersamplequalitysoweuseditinsteadforvisualizing
samplesfromthemodel.
We used the same U-Net plus self-attention architectures from the continuous diffusion model
describedinKingmaetal.[33]forCIFAR-10,exceptthatwedidnotuseFourierfeatureinputsand
addedanadditionalinputembeddinglayerwithembeddingsizethesameasthehiddendimension
ofthemodel. ForImageNet64×64,wereducedthenumberofresidualblocksfrom64to48and
23Figure5: MoreunconditionalsamplesfromMD4trainedonImageNet64x64.
add12additionaldiffusiontransformer[64]blockswith768hiddendimensionand12headsinthe
middle.
WeusedAdamWoptimizerwithlearningrate0.0002andweightdecayfactor0.01forbothdatasets.
The learning rate follows a cosine annealing after 100 warm up steps. The batch size is 128 for
CIFAR-10and512forImageNet64×64. OurCIFAR-10modelistrainedon16TPU-v5litefor24
hours. OurImageNet-64×64modelistrainedon256TPU-v4for2days.
24Figure6: SamplequalityevaluatedbyGPT-2LargefollowingLouetal.[32]. WecompareMD4
against GPT-2 (auto-regressive baseline) and SEDD (state of the art discrete diffusion model) in
generatingcoherentsequences(1024tokens). Weinvestigatetheeffectsoftwoorthogonalfactorson
samplequality: modelsizeanddecodingsteps. ThenumbersforGPT-2andSEDDarefromLou
etal.[32].
I AdditionalunconditionalgenerationfromMD4-M
I.1 MD4-Msample1: 1024tokens
like, I don’t have to be alive? Sometimes there are things that are too real
and you’re really supposed to experience them. So that’s a good feeling.
That is the scary thing. Not actually, being able to experience things, being
able to do these things, when you’re doing them, which, for most people
having to wake in a dream is something that seems the most significant, and then
you think about it the next day. It’s like the hope of the future,
and you wake up right now thinking about it. What happens is,, then you
have to stop and think about it and then all of a sudden, somebody always
says, "You’re dreaming."
And sometimes I wonder if this is a good time to teach your gut instincts to
your actors when you’re doing a show like this. Because even on this particular
show, it feels like everyone’s been through this all the time before, if even
a few years ago. I mean, if you’re doing a show together, at least not on
continuous development, you you’re a vet. I mean, you should really be along.
If you’re not sure, well --
VS: I’m working on that one.
Did any of you guys feel that an instinct could work? I thought, "Well, because
you didn’t do ’Deadwood’ you should stop doing this." But when I read the story
for the first time, I thought, "I think this is going to work." What I can’t
picture is a way to hold this apart.
VS: That’s me. It’s what we have to do. So do we. When we wrote the first episode,
we wrote a script that we felt like me and myself would want to see. I knew that I
wanted to be able to be in something -- and I wanted to be able to take refuge in
something that was real, that you could see and just really step out of yourself.
And then I saw it. Then, you get rehearsing it and doing it. And then I actually
started shooting. I think I knew I didn’t think it was going to be good. But,
25I know it was good. And now people are talked about because it’s not good enough.
Growing up, you say that you just completely hated the show, "Lost." Isn’t that
what you wish for at the end of the day?
VS: I don’t like the concept.
And so there’s a lot that you don’t know about that, so I think for me to have had
these ideas, if you didn’t understand even that it was coming out of this world
that doesn’t exist, we might never get together.
It’s so weird. This happened to happen at the same time?
VS: Yes. It happened to happen at basically the same time.
Nobody’s even had a show or had a movie/come out of the movie, but ...
VS: If I’m going to pretend I’m definitely not you and have to live through that
stuff, I don’t think I’m going to swallow that. I didn’t expect it to do quite
that long.
There are always things now that happen with ’Deadwood’ where you don’t know where
it’s going to end up next time, but I think there are occasions now where we have
to keep the fight, even if ’Lost’ was pretty consistent in the mindset and the form.
VS: I’m glad that we did fight the odds, because we should have understood that
there was a direct link. But there was almost a sense of not that we had showed up
on the same day, we know we work in the same pieces, but a lot of stuff we don’t
know about. Some of it, we need to deal with. We also just have to accept the
language, and there are a lot of things where we take from them and we do this
what they did because we want to
I.2 MD4-Msample2: 1024tokens
the groups let recreational vehicles use the three roads that will stay open in
the meantime of fighting off the permit. "The purpose of the permit is to make
sure that we work with the NPS and made roadways and rest areas. We’re not just
scaring guys kind of messing around." Community plans to build an urban bike
facility marched forward at the ongoing staff meeting of the King County
Commission.
Trail will be finished just south of the Greenview 5.
Instead of continuing with a pedestrian and bike trail to the MBTA’s campus, these
two trails could bridle the areas from Market to 14 and carry communities closer.
"This project will provide a car-free path to King County," said Andrew Weed. It’s
been put the brakes on in the past several months, but there are those residents
still skeptical.
"I’ve addressed some of the community concerns that’ve been raised. They’ve
expressed some of their concerns. I don’t think it’s terribly reasonable from a
transportation standpoint."
The trail had been set up to meet on for more than a year when the council
approved funding for a different proposal.
Mayor Muriel Bowser said after meetings with Commissioner Bushell on Thursday that
the new plan will be on board in December.
26"There’s enough of a finish for this project to roll out on time, and we’re going
to get it done," Bowser said.
For the public, the campaign appears over.
“There was one meeting that I feel like I lost at last night’s meeting," said
Shelley Potts, a local resident.
Local resident Joel Grimy, who lives on Uman Road, met residents there as well.
And in other groups that rode through Mayor assistant Stacey Land and even her son
held fliers saying to look for light sign, and also met with Bowser’s son, Deion
Bowser, about a future plan to also have a dog park on the transit corridor.
Advocates at Brickley’s event, many one waited at least 11 minutes in during the
start of the public meeting, said they expect at least another month from the
Board of Commissioners, even after a public hearing on Nov. 13.
"We’ve been trying to be a talkative board where we are meeting in advance, being
respectful of folks," Bowser said.
He considered that the proposal for the section of trail between the Greenview 5
and 3 “has to move on a schedule. We have other historic preservation projects
that would take over that.”
But Chad Routledge, a local advocate of the project, spoke out against the mayor’s
plan.
“The mayor has sent a new meeting to the public using the same route that resulted
from the loud criticism and onslaught of complaints from the community committee
back during the public hearing,” Routledge said.
The BDC doesn’t have a particular plan-turns around for the end of the planned
path, and says “nothing practical can happen right now.” But, she said the agency
still "looking to make investments in facilities along the route."
And still there is another part of the trail that might be just as much a wish for
the dogs, as cars: the district wants to go west foot a couple blocks south, to
make the trail safer for dogs.
“I feel that the accessibility of the trail is pretty important. I think the
education of the trail, and the uses along different routes are very important
pieces of a balanced outcome,” said Bushell.
Trams coming off Route 1
27