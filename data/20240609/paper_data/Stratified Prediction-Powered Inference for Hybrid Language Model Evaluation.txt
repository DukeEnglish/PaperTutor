Stratified Prediction-Powered Inference
for Hybrid Language Model Evaluation
AdamFisch†,∗ JoshuaMaynez†,∗ R.AlexHofer†
BhuwanDhingra† AmirGloberson‡ WilliamW.Cohen†
†GoogleDeepMind ‡GoogleResearch
{fisch,joshuahm,rofer,bdhingra,amirg,wcohen}@google.com
Abstract
Prediction-poweredinference(PPI)isamethodthatimprovesstatisticalestimates
based on limited human-labeled data. PPI achieves this by combining small
amountsofhuman-labeleddatawithlargeramountsofdatalabeledbyareasonably
accurate—butpotentiallybiased—automaticsystem,inawaythatresultsintighter
confidenceintervalsforcertainparametersofinterest(e.g.,themeanperformance
of a language model). In this paper, we propose a method called Stratified
Prediction-PoweredInference(StratPPI),inwhichweshowthatthebasicPPI
estimatescanbeconsiderablyimprovedbyemployingsimpledatastratification
strategies. Withoutmakinganyassumptionsontheunderlyingautomaticlabeling
systemordatadistribution,wederiveanalgorithmforcomputingprovablyvalid
confidence intervals for population parameters (such as averages) that is based
onstratifiedsampling. Inparticular,weshowboththeoreticallyandempirically
that,withappropriatechoicesofstratificationandsampleallocation,ourapproach
canprovidesubstantiallytighterconfidenceintervalsthanunstratifiedapproaches.
Specifically, StratPPI is expected to improve in cases where the performance
oftheautoratervariesacrossdifferentconditionaldistributionsofthetargetdata.
1 Introduction
Evaluatingmachinelearningmodelsrequiresevaluationdata. Inparticular,toiterativelyimprove
onamethodduringdevelopment,ortoreliablyreportanimprovement,oneneedstoconfidentlyand
quantitativelyassesstheperformanceofthemethod. Thisisespeciallychallengingforlargelanguage
models(LLMs),wheregatheringhigh-qualityannotationsforgenerationscanbedifficultandtime-
consuming—andcanultimatelybecomequitecostlyifgatheringmorethanafewhundredexamples.
Oneoften-proposedapproachtoavoidingtheevaluationbottleneckistouseasecondaryLLM-based
systemtojudgetheoutputoftheprimaryone. Forinstance,iftheprimarytaskisdevelopingan
LLM-basedquestion-answering(QA)system,onecanuseasecondLLM-basedsystemthatrates
question/answerpairsasacceptableornot[6;8;15].However,automatedraters(i.e.,autoraters)may
bebiasedrelativetothehumanraterstheyareintendedtomodel,asothershavenoted[1;9;16;21].
Thiscanbecomesubstantiallyworsewhenmodelsaretailoredtohillclimbontheautoratermetrics,
andeventuallyceasetobecomeagoodmetricatall—aphenomenoncommonlyreferredtoasGood-
hart’slaw,orrewardhackinginthecontextofreinforcementlearningfromhumanfeedback[12;22].
*Equalcontribution.
Preprint.Underreview.
4202
nuJ
6
]GL.sc[
1v19240.6042:viXraWethushavetwosignalsforassessingmodelperformance. Thefirstishumanlabels,whicharetyp-
icallyaccurate,butexpensivetocollect. Asaresult,usuallyonlyasmallsamplesizeisavailable,and
anestimatebasedonthesesamplesalonewillhavehighvariance. Thesecondisautoraterpredictions,
whichareeasytocollectforlargesamplesizes,butmayalsobesystematicallybiased.Theabovemay
suggestthatonemustmakeachoicebetweeneither(i)ahigh-variance,butunbiased,estimatefroma
smallhumansample,or(ii)alower-variance,butbiased,autorater-basedestimate. However,itturns
outtherearealsostatisticallyvalidwaysofcombiningtheauto-raterandhumandataforhybrideval-
uations. Following[1;3;7]wecallsuchmethodsprediction-poweredinference (PPI)methods. Ata
highlevel,PPI-basedmethodsoperatebyusingasmallsampleofexampleslabeledbybothhumans
andautoraterstoestimatethebiasoftheautorater. Thisbiasisthenusedasarectifierfortheautorater
estimate. Theresultingestimatecanthenbeshowntoprovideimproved(i.e.,tighter)confidence
intervalsforpropertiesofinterestforthesystembeingevaluated,suchasitstruemeanaccuracy.
AweaknessofstandardPPI,however,isthatitdoesnottakeheterogeneityintoaccount.Forexample,
inourQAsetting,anautoratermayhaveoneaccuracywhenpredictingifamodelansweriscorrect,
andadifferentaccuracywhenpredictingifitisincorrect. Thisisespeciallytrueincaseswherea
correctansweriseasytoverify(e.g.,itisalsopresentinWikipedia),buthardertorefute(e.g.,no
relevantexternalsearchresultscanberetrievedtoeithersupportorcontradictit). Inthesesettings,
itmaymakesensetoapplyadifferentPPIstrategywithineachsubdomain,dependingonthelocal
qualityoftheautorater.Moreover,weclaimthatsuchheterogenoussettingsaretobequiteexpectedin
practicalapplications.Inspiredbytherichpriorliteratureonstratifiedsamplingandsurveydesign[11;
17;19],wethereforeproposeastratifiedapproachtoPPI,andshowthatitcanbeveryadvantageous
whenperformancevariesacrosssubdomains,foreithertheautorater,orthemodelbeingevaluated.
Onatechnicallevel,itisnotimmediatelyclearhowtoapplystratificationtoPPI,asitinvolvestwo
typesofsamples: onesamplethatislabeledbybothhumansandanautorater,andanother(typically
muchlarger)samplethatisonlylabeledbytheautorater. Extendingtheanalysisof[3],inthiswork
weshowhowconfidenceintervalsbasedontheasymptoticnormalityofweightedM-estimators[30]
can in fact be derived for the stratified PPI setting. The next challenge we address is how to
determine the sample sizes used for stratification (i.e., the sample size of each stratum). Similar
torecentworkforactivestatisticalinference[34],wefurtherderiveoptimalratesthatdependon
certainmomentsoftheunderlyingdistributionthataregenerallyunknown—andprovideanapproach
foreffectivelyapproximatingthese. Finally,weprovideextensiveempiricalevidenceshowingthat
ourstratifiedapproach(StratPPI)leadstoconsiderablyimprovedconfidenceintervalsoverboth
classicalinferencemethodsthatuseonlyhumanlabels,andthebaseline(unstratified)PPIapproach.
2 RelatedWork
PredictionPoweredInference(PPI)wasintroducedin[1]asamethodforobtainingtighterconfidence
intervals for parameters learned in supervised machine learning (e.g., coefficients in logistic
regression)byalsoleveragingothermodel-basedpredictionsonadditional,unlabeleddata. Related
ideaswereexploredby[28],butwithafocusonbootstrappingasawayforobtainingconfidence
intervals. PPIwasthenextendedinseveraldirections. Forexample[33]showedhowthelabeled
datacanbeusedforbothestimatingtheparametersandtheautoratermodel. PPI++[3]showedhow
toobtainconfidenceintervalsthatareeasytocomputeefficiently,andintroducedaparameterfor
weightingthepredictionsoftheautoratersuchthattheoverallstatisticalefficiencycanbeimproved.
AsnotedinpreviousworksonPPI,theseapproachesarecloselyrelatedtootherstatisticalmethods
forintroducingcontrolvariatesbasedonautoevaluators[9],aswellasaugmentedinversepropensity
weighting [25] (see discussion in [1; 3]). Like this paper, prior work has also focused on using
PPI/PPI++forevaluatingmachinelearningsystemswithautoraters,includingforrankingchatbots
intheChatbotArena[7]andevaluatingretrieval-augmentedgenerationsystems[26].
Mostrelevanttooursetting,therecentworkof[34]focusesonactivesamplingofexamplestolabel
duringPPI,wherethetotalnumberofqueriedlabelsisrandom,butlessthanthesamplingbudgetn
inexpectation. Specifically,itproposestolabelexamplesforwhichtheautoraterislessconfidentin
itspredictions,andcorrectsforthesamplingbiaswithavariantofinversepropensityweighting. In
contrast,oursomewhatsimplifiedapproachtakesinspirationfromstratifiedsamplingwhereacoarse
stratificationisdefinedinadvance,andthetotalnumberoflabeledsamplesisconstant. Like[34],
wederiveananalogousoptimalallocationstrategyforagivenbudget,thoughweonlyapplyitatthe
stratumlevel,andnotforindividualexamples.Furthermore,whilethevariableallocationhelpsreduce
2varianceinheterogenoussettings,ourstratifiedtreatmentalsoallowsforastratum-specificextension
of[3]’sestimatedtuningparametersthatfurtherimprovesperformanceincomplementaryways.
Intermsofstratificationspecifically,concurrentwork[13]alsoproposedBayesianvariantsofseveral
PPImethods,includingstratifiedapproaches.Incontrast,themethodsheredonotrequireintroducing
priors,donotrequirerunningexpensiveBayesianinference,andgivemoreconventional,frequentist,
guaranteesofperformance. ThecredibleintervalsproducedbyBayesianmethodsarerelatedto,but
differentfrom,theconfidenceintervalsproducedhere,andbyotherpriorPPIapproaches[1;3;7].
Finally,thereisaconceptualsimilaritybetweenthePPIrectifiersandpost-hoccalibrationofaclassi-
fier. Thereisarichliteratureoncalibratingclassifiers(e.g.,[20;23]),butthereisacleardifferencein
goalsbetweencalibrationandPPI:theformerisaimedatmodifyingalearnedautoratertomakebetter
probabilisticpredictions,andthelatterisaimedatobtainingbetterconfidenceintervalsbymakinguse
ofanexistingautorater. Thatsaid,clearlyoneapproachtoimprovingPPIistouseabetter-calibrated
classifier,perhapsbytakingsomeofthelabeleddataandusingitforre-calibration. Thisissimilar
inmotivationtothecross-PPIapproachof[33]. Weleavesuchapproachesasfuturework,butwedo
experimentallyexplorePPIapproachesonbothwell-calibratedautoratersandpoorlycalibratedones.
3 Preliminaries
WebeginbybrieflyreviewingPPI[1],andspecificallytheefficientPPI++variantof[3]. Here,and
intherestofthepaper,upper-caseletters(X)denoterandomvariables;lower-caseletters(x)denote
constants,andscriptletters(X)denotesets,unlessspecified. AllproofsaredeferredtoAppendixA.
Following [1; 3; 7], we assume an empirical sample S = {(X ,Y ),...,(X ,Y )} of n i.i.d.
n 1 1 n n
examplesdrawnfromsomeunknown,butfixed,distributionP,whereX ∈X isaninput,andY ∈Y
i i
isthetargetoutput.WealsoassumealargersampleS˜ ={(X˜ ,f(X˜ )),...,(X˜ ,f(X˜ ))}where
N 1 1 N N
N ≫n,forwhichthetargetoutputsarenotavailable,butwehaveaccesstoanautoraterfunction
f: X →Y′whichprovidesanapproximationofY giventheobservedinput(weuseY′todenote
theoutputspaceoftheautoraterasitispossiblethatY ≠ Y′,e.g.,iff(X)=E[Y |X]).
Asanexample,assumeX isaquestion/answerpairwheretheanswerisproducedbyanLLM-based
QAsystemwewishtoevaluate,Y isa0/1goldratingofcorrectnessoftheanswer,andf(X)imer-
fectlypredictsthehumanratingofthequestion/answerpairX. Onevalueofinterestisthentheex-
pectedaccuracyoftheQAsystemonthedistributionfromwhichquestionsaredrawn.Inournotation,
thisissimplyE[Y].Moregenerally,weareinterestedinsomepropertyofthedistributionover(X,Y).
Specifically,givenanyconvexlossfunctionℓ (x,y)satisfyingcertainregularityconditions(seeDef-
θ
initionA.1intheAppendix),weareinterestedinestimatingthed-dimensionalparameterθ∗,where
θ∗ =argminE[ℓ (X,Y)]. (1)
θ
θ∈Θ
Inthestatisticsliterature,thisisbroadlyreferredtoasM-estimation(e.g.,see[27]). Inthispaper,we
areprimarilyinterestedinmeanestimation(asintheexamplementionedabove),forwhichtheloss
functionℓ (x,y)= 1∥y−θ∥2hastheoptimumθ∗ =E[Y]. However,ourmethodalsogeneralizes
θ 2
toothertypicallossfunctionssuchasthesquaredlossforlinearregression,wheretheparameters
θ∗aretheregressioncoefficients,or,similarly,theloglossforothergeneralizedlinearmodels.
Ofcourse,thetrueθ∗isgenerallynotknown,becausewecannotexactlycalculatetheexpectationin
(1). ThegoalofPPI,whichweshareinthiswork,istouseboththelabeledandunlabeleddataS
n
andS˜ toobtainanasymptoticallyvalidconfidenceintervalC forθ∗thatforanyj ∈[d]satisfies1
N α,j j
lim P(θ∗ ∈C )≥1−α. (2)
j α,j
n,N→∞
Thesimplestconfidenceintervalcanbeobtainedusingstandardtechniquesbyusingonlythelabeled
sampleS ,andignoringtheautoraterdataS˜ . However,PPIemploysaclevertrickwhichallows
n n
foralsousingtheunlabeledsampleS˜ togettighterconfidenceintervals,asdescribednext.
N
1Ind-dimensions,weobtainaconfidenceintervalforeachcoordinate.See[3]forotherpossiblechoices.
33.1 Arectifiedprediction-poweredloss
The key idea of PPI-based methods is to use autorater predictions to derive a low variance, but
unbiased,estimateoftheobjectivein(1). Considerthefollowing“rectified”prediction-poweredloss:
N n
LPP(θ)= 1 (cid:88) ℓ (X˜ ,f(X˜ )) + 1 (cid:88) ℓ (X ,Y )−ℓ (X ,f(X )). (3)
N θ i i n θ i i θ i i
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
autoraterdataloss lossrectifier
Therectifiertermontherighthandsideof(3)removesthebiasoftheautoraterdatasothatLPP(θ)
satisfiesE[LPP(θ)]=E[ℓ (X,Y)]. However,whenf(X)iscorrelatedwithY,thelossLPP(θ)will
θ
havelowervariance. Unfortunately,f(X)maynotbeagoodpredictorofY inallcases. Infact,itis
evenpossibleforLPPtobehighervariancethantheclassicalestimate(e.g.,iff(X)isanti-correlated
withY). Thus,toadapttothequalityoff(X),PPI++alsointroducesatuningparameterλ∈R:2
N n
LPP(θ)= λ (cid:88) ℓ (X˜ ,f(X˜ ))+ 1 (cid:88) ℓ (X ,Y )−λℓ (X ,f(X )). (4)
λ N θ i i n θ i i θ i i
i=1 i=1
Clearly,LPPstillremainsunbiasedforanyvalueofλ. Forλ=0,theframeworkreducestoclassical
λ
inference. Inmostcases,however,aproperchoiceofλ̸=0willresultinlowervariance. PPI++also
suggestshowtoautomaticallychooseadata-dependentλˆthatconvergestoanoptimalvalue.
3.2 Aprediction-poweredconfidenceinterval
WenextdescribethePPI++methodforderivingconfidenceintervalsbasedontherectifiedloss. Let
θˆPP =argminLPP(θ). (5)
λˆ
θ∈Θ
λˆ
betheprediction-poweredestimate. StandardanalysisforM-estimatorscanthenbeextendedtoshow
thatthescaleddifferenceoftheestimateθˆPPandthetrueθ∗isasymptoticallynormallydistributed.
λˆ
PPI++thenleveragesthisresulttocomputeaconfidenceintervalforθ∗thatisasymptoticallyvalid.
Theorem1(PPI++,[3]). Assumethatλˆ →p λand n →r ≥0. LetH :=E[∇2ℓ ],and
N θ∗ θ∗
Vλ :=λ2Cov(∇ℓ (X˜,f(X˜)), Vλ :=Cov(∇ℓ (X,Y)−λ∇ℓ (X,f(X))), (6)
f,θ∗ θ∗ ∆,θ∗ θ∗ θ∗
whereλ∈Risahyper-parameter. ThenundertheregularityconditionsofDefinitionA.1,wehave
√
that n(θˆPP−θ∗)→d N(0,Σλ),whereΣλ :=H−1(r·Vλ +Vλ )H−1.
λˆ θ∗ f,θ∗ ∆,θ∗ θ∗
Corollary1(PPI++ CI,[3]). LetΣ(cid:98)λˆ betheplug-inestimateforΣλusingS nandS˜ N. Define
(cid:18) (cid:113) (cid:19)
C αP ,P j := θˆ λP ˆ,P j ±z 1−α 2 n−1Σ(cid:98)λ jj , (7)
wherez denotestheβ-quantileofthestandardnormaldistribution. Thenforanyj ∈[d]itholds
β
thatlim P(cid:0) θ∗ ∈CPP(cid:1) ≥1−α.
n,N→∞ j α,j
Asmentionedabove,[3]furthershowedthatforanappropriatechoiceofλ(thatcanbeanalytically
derivedandestimatedviaaλˆ),thetraceofthecovariancematrix,Tr(Σλ),isatmostthatofthecovari-
ancematrixderivedwithoutusingautoraterdata,implyingthatthePPI++-basedconfidencesetCPP
α
canalwaysbeatleastastightasthatoftheclassicalM-estimator(andoftenmuchtighterinpractice).
4 Stratifiedprediction-poweredinference
WenowpresentourapproachforimprovingPPI++estimatesviastratification. Inparticular,weshow
howoptimizingarectifiedlosscomputedviastratifiedsamplingcanleadtoaconsistent,butlower
variance,estimateoftheoptimalparameterθ∗—andcorrespondinglytighterconfidenceintervals.
2Asnotedin[3],forsomelossesℓ wemusthaveλ∈[0,1]toguaranteeconvexityofLPP.Themainfocus
θ λ
ofthispaper,however,isonmeanestimationwithℓ (x,y)= 1∥y−θ∥2,whichisconvexforanyλ∈R.
θ 2
4Togainintuition,considertheQAexamplefromtheprevioussection. Formosttypesofautoraters,
itisreasonabletoassumethatthestrengthoftheirperformancecanvary,dependingonthetypeof
inputbeingpresented. Forinstance,anautoratermightbeaccurateatpredictingwhetherananswer
toanunambiguous,factoidquestion(e.g.,“WhatisthecapitalofFrance?”) iscorrect,butrelatively
pooratinferringifalong-formanswertoanopen-endedquestion(e.g.,“Whatisthebestwaytocook
ahamburger?”) isacceptableornot. Splittingthespaceofinputsandtheirautoraterpredictionsinto
differentdomainsallowsustoderiveamorespecializedformofthepredictionpoweredlossthatcan
betteradapttothisautoraterheterogeneityviastratifiedsampling.
Formally, assume that the input space X is partitioned in advance into K non-empty, mutually
exclusive,andexhaustivestrataA=(A ,...,A ),whereK isafiniteinteger. Foreachstratum,
1 K
wefurtherassumethatwecanestimatew = P(X ∈ A )arbitrarilywellusinglargeamountsof
k k
unlabeleddataorpriorknowledge;wetreatthemasknownconstantshereforsimplicity. Tocollect
thelabeledandunlabeleddatasetsS andS˜ ,wethenfollowastandardstratifiedsamplingprocedure
n N
inwhichwedrawtwoi.i.d. setsofsamplesoffixedsizen andN ,respectively,fromeachstratum
k k
k,where(cid:80)K
n
=nand(cid:80)K
N =N. Therelativesizesn /nandN /N arefreeparameters;
k=1 k k=1 k k k
theycansimplybethenaturalrates, n /n ≈ N /N ≈ w , orsystematicallydecided(see§4.3).
k k k
NotethatthisisafundamentallydifferentsamplingmodelfromstandardPPI:hereexamplesarei.i.d.
withineachstrata,andindependent(butnotnecessarilyidenticallydistributed)acrosseachstrata.
Next,wedefinethestratifiedprediction-poweredlossviatheweightedsum,
K
(cid:88)
LSPP(θ)= w LPP (θ), (8)
λ k k,λk
k=1
wherew isthestratumweight,λ=(λ ,...,λ )∈Rk arenowstratum-specifictuningparameters,
k 1 k
andLPP(θ)istheconditionalprediction-poweredlosscomputedwithineachstratum,i.e.,
k
LPP (θ)=
λ
k
(cid:88)N
ℓ (X˜ ,f(X˜ ))+
1
(cid:88)nk
ℓ (X ,Y )−λ ℓ (X ,f(X )). (9)
k,λk N θ ik ik n θ ik ik k θ ik ik
k
i=1 i=1
Asbefore,eachLPP isanunbiasedestimateoftheconditionalloss. LikePPI++,wealsoallowfor
k,λk
data-dependentparametersλˆ thatweshowhowtoautomaticallytuneforbestperformancein§4.2.
k
4.1 Astratifiedprediction-poweredconfidenceinterval
Wenowpresentourmainresult,whichisaconfidenceintervalforθ∗ basedonthestratifiedloss.
Moreprecisely,theresultstatesthat,asinPPI++,theminimizerofthestratifiedloss,
θˆSPP =argminLSPP(θ), (10)
λˆ
θ
λˆ
hasanasymptoticallynormaldistributionwithmeanθ∗. SeeAlgorithm1forpseudocode.
Theorem2. Assumethatλˆ k →p λ k, Nn →rforanyr ≥0, n nk →ρ k foranyρ k >0,and N Nk →ρ˜ k
foranyρ˜ >0. LetH :=E[∇2ℓ (X,Y)|X ∈A ],and
k k,θ∗ θ∗ k
V kλ ,fk
,θ∗
:=λ2 kCov(∇ℓ θ∗(X˜,f(X˜))|X˜ ∈A k), (11)
V kλ ,∆k
,θ∗
:=Cov(∇ℓ θ∗(X,Y)−λ k∇ℓ θ∗(X,f(X))|X ∈A k), (12)
whereλ ∈Risahyper-parameter. Then,undertheregularityconditionsofDefinitionA.1,
k
√
n(θˆSPP−θ∗)→d N(0,Σλ), (13)
λˆ w
whereΣλ :=A−1BλA−1and:
w w w w
K K (cid:18) (cid:19)
(cid:88) (cid:88) r 1
A w := w kH k,θ∗ B wλ := w k2 ρ˜ ·V kλ ,fk ,θ∗ + ρ ·V kλ ,∆k ,θ∗ . (14)
k k
k=1 k=1
Toobtaintheresult,wecombineunstratifiedPPI++withtheasymptoticpropertiesofweightedM-
estimators[30]. Theresultingconfidenceintervalforθ∗isthenderivedanalogouslytoCorollary1.
5Algorithm1Stratifiedprediction-poweredinferenceforgeneralM-estimators(StratPPI)
Definitions: f is the autorater. Inputs {(X˜ ,f(X˜ ))}Nk include the autorater predictions on
ik ik i=1
sampledunlabeleddataforeachpartitionA ,k ∈[K]. Inputs{(X ,Y ,f(X ))}nk includethe
k ik ik ik i=1
autoraterpredictionsonsampledlabeleddataforeachpartitionA ,k ∈ [K]. w isthepartition
k k
weightforA ,k ∈[K]. αistheconfidenceintervalcoverageerrortolerance.
k
1: #Pickweightingparameters,see§4.2.
2: Selectλˆ =(λˆ 1,...,λˆ K)
3: #Solvefortheminimizerofthestratified,prediction-poweredempiricalloss.
4: θˆ λS ˆPP =argmin θLS λˆPP(θ)
5: #EstimatetheHessianofthetrueexpectedlossatθ∗.
6: Aˆ w =(cid:80)K k=1 w nkk (cid:80)n i=k 1∇2ℓ θˆ λS ˆPP(X ik,Y ik)
7: #Buildtheestimatedcovariancematrixfromeachstratifiedcomponentoftheloss.
8: Σ(cid:98)λ wˆ =0 d×d
9: fork =1,2,...,K do
(cid:16) (cid:17)
10: V(cid:98)f =λˆ2 kC(cid:100)ov Nk ∇ℓ θˆSPP(X˜ ik,f(X˜ ik))
(cid:16) λˆ (cid:17)
11: V(cid:98)∆ =C(cid:100)ov nk ∇ℓ θˆSPP(X ik,Y ik)+λˆ k∇ℓ θˆSPP(X ik,f(X ik))
λˆ
(cid:16) (cid:17)
λˆ
12: Σ(cid:98)λˆ =Σ(cid:98)λˆ +w2Aˆ−1 V(cid:98)f + V(cid:98)∆ Aˆ−1
w w k w Nk nk w
13: #Returncoordinate-wiseconfidenceintervalsforθ∗.
(cid:110) (cid:113) (cid:111)
14: C αSPP = θˆ jSPP±z 1−α 2 Σ(cid:98)λ wˆ ,jj : j ∈[d]
Corollary2. LetΣ(cid:98)λ wˆ betheplug-inestimateforΣλ
w
usingS nandS˜ N. Define
(cid:18) (cid:113) (cid:19)
C αSP ,jP := θˆ λS ˆP ,jP±z 1−α 2 n−1Σ(cid:98)λ wˆ ,jj , (15)
wherez denotestheβ-quantileofthestandardnormaldistribution. Thenforanyj ∈[d],
β
lim P(cid:0) θ∗ ∈CSPP(cid:1) ≥1−α. (16)
j α,j
n,N→∞
Theformofthestratifiedprediction-poweredconfidenceintervalissimilartothatofPPI++,except
thatitisbasedoffoftheweightedstratum-conditionalcovariancematrices. Theeffectofthischange,
however,issignificant. Infact,inthecaseofmeanestimation,weshowthattheasymptoticvariance
ofStratPPIisatmostthatofPPI++(evenwithoutanyadditionaltuningofλ andρ ).
k k
Proposition1. Letλ ∈Rbeanyconstantforallstrata,andfixρ andρ˜ totheirnaturalratesw .
k k k k
Thenforℓ (x,y) = 1∥y−θ∥2 andanystratification(A ,...,A ),wehaveTr(Σλ) ≤ Tr(Σλ),
θ 2 1 K w
whereΣλistheasymptoticcovariancematrixofPPI++. Furthermore,wehavethatequalityholds
ifandonlyifbothE[Y |X ∈A ]andE[f(X)|X ∈A ]arethesameforallstrata.
k k
Moregenerally,althoughourresultswillholdforarbitrarystratifications,itisbestiftheyareheteroge-
neous,andchosensuchthattheindividualstratum-conditionalvariancesareminimized. Forexample,
ifanautoratersystematicallyover-estimatesthemodel’sperformanceononesubdomain,butsys-
tematicallyunder-estimatesthemodel’sperformanceonanother,thensplittingthesesubdomainsinto
differentstratacanresultinalowervarianceLSPP. Similarly,ifanautoraterhasmuchhighernoise
λ
onsomesubdomainsthanothers,itcanbebeneficialtostratifyonthosesubdomains—andtheneither
lowerλ ,allocateahigherproportionofsamplesn /nandN /N,orbothforthenoisierstratas.
k k k
In§5weempiricallydemonstratethatthestratifiedestimatorcanindeedleadtoconsiderablytighter
confidenceintervalsinpractice,especiallywithadditionaltuningofλ andρ ,asdiscussednext.
k k
4.2 Optimalweightingoftheautoraterpredictions
In[3]itwasshownthattheoptimalvalueofλforPPI++(i.e.,theoneminimizingthevariancesof
theestimatorandthecorrespondingconfidenceinterval)couldbefoundinclosedform. Wenow
6presentasimpleextensionofthisresulttothestratifiedcase. Fornotationalconvenience,weusethe
shorthand∇ℓ :=∇ℓ (X,Y)|X ∈A and∇ℓf :=∇ℓ (X,f(X))|X ∈A .
k,θ θ k k,θ θ k
Proposition2. Assumethatρ˜ andρ arefixed. Thenthetuningparameters(λ∗,...,λ∗),where
k k 1 k
(cid:16) (cid:17)
Tr A−1(Cov(∇ℓ ,∇ℓf )+Cov(∇ℓf ,∇ℓ ))A−1
w k,θ∗ k,θ∗ k,θ∗ k,θ∗ w
λ∗ = , (17)
k (cid:16) (cid:17) (cid:16) (cid:17)
2 1+ Nnk
k
Tr A− w1Cov(∇ℓf k,θ∗)A− w1
minimizethecumulativeasymptoticvariance,Tr(Σλ).
w
NotethatTr(Σλ)isproportionaltothetotalsizeoftheconfidenceintervalCSPPin(15).Furthermore,
w α
asin[3],wecanuseplug-inestimatesforthetermsin(17)tocomputeaλˆ ,whereλˆ →p λ∗. From
k k k
(17), we can see that λ∗ is closely related to the correlation coefficient of the (curvature-scaled)
k
gradients for minimizing the loss on an autorater label versus a true label. Intuitively, the more
correlatedthesetermsare,themorewecanrelyontheautoraterlabelsforfindingthetrueminimizer
ofE[ℓ (X,Y)]. For1-dmeanestimationinparticular,wecanseethatλ∗ takesonasimpleform:
θ k
Example1(λ∗ formeanestimation). Considerthe1-dmeanloss: ℓ (x,y)= 1(y−θ)2. Then:
k θ 2
Cov(Y,f(X)) Cov(Y,f(X))
λ∗ = ≈ forlargeN , (18)
k (1+ nk)Var(f(X)) Var(f(X)) k
Nk
whichisequivalenttotheoptimallinearregressioncoefficient,min E[|Y −λ f(X)|2 |X ∈A ].
λk k k
4.3 Optimalallocationofthesamplingbudget
Our stratification approach has an additional hyperparameter ρ that can also be tuned to reduce
variance. Recallthatρ determinestheratiobetweenthelabeleddatasizen forstratumkandthe
k k
(cid:80)
overalldatasizen(i.e., n = n). ρ˜ issimilarlydefinedfortheunlabeleddata. Anystrictly
k k k
positivevaluesofρ andρ˜ arevalidtobeused,thoughnotallvalueswillimproveperformance. It
k k
turnsoutthattheoptimalρ valuescanbeexactlycalculated,asthefollowingpropositionshows.
k
Proposition3. Assumethatλ isfixed.Thenthesamplingrates(ρ∗,...,ρ∗)and(ρ˜∗,...,ρ˜∗),where
k 1 k 1 k
(cid:113) (cid:113)
ρ∗ =
w
k
Tr(A− w1V kλ ,∆k ,θ∗A w−1)
and ρ˜∗ =
w
k
Tr(A− w1V kλ ,fk ,θ∗A− w1)
(19)
k (cid:113) k (cid:113)
(cid:80)K k′=1w k′ Tr(A− w1V kλ ′k ,∆′ ,θ∗A− w1) (cid:80)K k′=1w k′ Tr(A− w1V kλ ′k ,f′ ,θ∗A− w1)
minimizethecumulativeasymptoticvariance,Tr(Σλ).
w
Althoughthesolutionforρ∗ isinformative,itisnotnecessarilypractical,asitdependsonknowing
k
A−1Vλk A−1;thisinturndependsonθ∗andP(X,Y),whicharebothunknown.3 Inthespecial
w k,∆,θ∗ w
caseofmeanestimation,however,itturnsoutthereisnodependenceonθ∗. Toaddresstheremaining
dependenceonP(X,Y),weproposetouseautoraterconfidencescores,assumingtheyareavailable.
Specifically, assume Y is discrete, and let c(y | x) be the confidence of the autorater in label y
giveninputx,wherec(y |x)approximatesP(Y =y |X =x). Thiswillresultintheestimatefor
Tr(A−1Vλk A−1)below,whichcanthenbepluggedintotheexpressionforρ∗ inProposition3.
w k,∆,θ∗ w k
Example2(ρ∗ formeanestimation). Considerthe1-dmeanloss: ℓ (x,y)= 1(y−θ)2. Then:
k θ 2
Tr(A−1Vλk A−1)=Var(Y −λ f(X)|X ∈A ). (20)
w k,∆,θ∗ w k k
(20)canthenbeestimatedusingtheobserved,butunlabeled,samplesscoredbytheautoraterfor
eachstratumk,X˜ =x˜ ,i=1,...,N ,asVar(Y −λ f(X)|X ∈A )≈σˆ2,where
ik ik k k k k
σˆ2 = 1
(cid:88)Nk
(cid:88) c(y |x˜ )(y−λ f(x˜ )−µˆ )2 and (21)
k N ik k ik k
k
i=1y∈Y
1
(cid:88)Nk
(cid:88)
µˆ = c(y |x˜ )(y−λ f(x˜ )). (22)
k N ik k ik
k
i=1y∈Y
3Similarly,theoptimalsolutiontoρ˜ alsodependsontheunknownθ∗ingeneral,thoughthistermisless
k
importanttooptimizeifweassumeN tobelarge.Inpractice,wealwayskeepρ˜ fixedtothenaturalrate,w .
k k
7Figure1: MeanestimationsimulationstudywithK =2andα=0.1. Thetoprowplotscoverage
(i.e.,thefractionofthecaseswheretheCIcontainedthetrueparametervalueθ∗). Thebottomrow
plotsthemeanCIwidthacrossmultipletrialsateachsamplesizen(↓isbetter). Shadedareasplotthe
16/84quantilesacross1000trials.Theleftcolumnshowsasettingwherestrataarehomogeneous,and
thestratifiedestimateprovidesthenobenefitsoverstandardPPI++(butisnotworse).Themiddleand
rightcolumnsshowheterogeneoussettingswheretheautoraterhaseitheradifferentbias(µ)orvari-
ance(σ)perstratum,inwhichcasestratificationhelpssubstantially. Asstratavariancesareknownin
thissyntheticsetup,weonlyreportproportionalandoptimalsampleallocationresultsforStratPPI.
Ford-dimensionaldata,theresultissimilar,butwithasumofdvariances(oneforeachdimension).
Wealsoprovideasimplifiedexpressionforσˆ2 inAppendixBwhenf(x˜) := (cid:80) c(y | x˜)·y.
k y∈Y
Importantly, as we are free to use any ρ > 0, using this estimate still preserves the asymptotic
k
coverageguaranteesin(15), regardlessofiftheconfidenceestimatec(y | x)iscalibratedornot.
Empirically,weshowthatusingthisheuristiccanindeedleadtosubstantialimprovements.
5 Experimentalresults
Wecompareourstratifiedestimator,StratPPI,onbothsimulatedandrealdatatotwobaselines:
(i)theclassicalestimate,whichusesonlythelabeleddata,S ;and(ii)PPI++,whichusesbothS
n n
andS˜ . Allofourexperimentsfocuson1-dmeanestimation,i.e.,E[Y]. Weexplorethreedifferent
n
allocationstrategiesforStratPPI:thefirstistosetρ = w tobedataproportional(StratPPI
k k
Prop.),thesecondistosetρ optimallyviatheoracleρ =ρ∗ (StratPPI Opt.),andthethirdis
k k k
tousetheheuristicapproximation,ρ ∝w σˆ ,inExample2forλ =1whenconfidencescoresare
k k k k
available(StratPPI Heur.). Weuseλ-tuningforbothPPI++andStratPPI,asoutlinedin§4.2.
5.1 Simulationstudies
Westartwithasimplesyntheticexperimentthatisananalogueof§7.7.1in[3].Ourgoalistoestimate
themeanoutcomeE[Y],whereY ∼N(0,1). WeassumethattheinputspaceX ispartitionedinto
K =2strata,(A ,A ),ofequalmassP(X ∈A )=P(X ∈A )=0.5. Wethenassumethatpre-
1 2 1 2
dictionsareformedasf(X )=Y +µ +σ ϵ ,whereϵ ∼N(0,1). Inotherwords,thepredic-
ik ik k k ik ik
tionsdonotdependonthecovariatesX ,otherthantoreflectastratum-specificnoiseσ andbiasµ .
ik k k
Wetestthreedifferentscenarios:(i)wherethetwostrataarehomogeneouswithµ =µ andσ =σ ;
1 2 1 2
(ii)wherethetwostratahavedifferentpredictionbiases,µ ̸=µ ;and(iii)wherethetwostratahave
1 2
differentpredictionnoiselevels,σ ̸=σ . Foreachexperiment,wesampleN =10,000totalpredic-
1 2
tionsf(X˜)usingρ˜ =ρ˜ =0.5,i.e.,proportionaltomassesofthetwohypothetical,equal-weight
1 2
strata. WethenvarythetotalnumbernoflabeledexamplesY,wheretheallocationischosenaccord-
ingtoρ(whichdiffersdependingonifweareusingStratPPI Prop. orStratPPI Opt.). We
8Figure2: MeanestimationonrealdatawithK =10andα=0.05. Thexaxisplotsthenumberof
human-labeledexamplesn;theyaxisplotsCIwidth(↓isbetter),percentreductioninCIwidthagainst
theclassicalestimate(↑isbetter),andtheeffectivesamplesize(theamountofhumanlabelsnecessary
tomatchthesameconfidenceintervalviaclassicalinference;↑isbetter). Shadedareasplotthe16/84
quantilesacross1000trials. AllStratPPImethodsimproveoverclassicalinferenceandPPI++.
Mostofthedatasetsexhibitheterogeneousstratas(determinedfrombinnedautoraterconfidence
scores),whereheuristicallocationshowsevenmoreimprovementsoverproportionalallocation.
showresultsinFigure1forthemeanconfidenceinterval(CI)sizeandcoverage(i.e.,thefractionofthe
caseswheretheCIcontainedthetrueparametervalueθ∗)ofeachmethod,averagedover1000trials.
As the plots in Figure 1 illustrate, when the underlying strata are homogeneous (left column),
StratPPIbehavessimilartoPPI++. However,whenthestrataareheterogeneous(middleandright
columns), StratPPI outperforms both baselines significantly—whereas PPI++ becomes barely
morepowerfulthantheclassicalestimator. Additionally,weseethatwhenthevariancediffersper
stratum(rightcolumn),optimalallocationofthesamplingbudgetindeedprovidesadditionalbenefit.
5.2 Realdatastudies
Wenowdemonstratehowourmethodperformsonrealdatasets,wheretheunderlyingstructureof
theautoraterisunknown. TopartitionX, wechoosetofocusonstratificationsthatarebasedon
theautorater’spredictions,f(X),basedontheintuitionthatautoraterperformancecanoftendiffer
acrossthetypeofpredictionsthatitmakes. Concretely,iftheoutputspaceY′off isdiscrete,then
wedefineA = Y′;otherwisewedefineAbasedontheequal-massquantilesofY′ (whichcanbe
estimatedbysamplingalargesetofunlabeledX andapplyingtheautorater). WesetK =10. For
allexperiments,weplotperformanceasafunctionofn,wherenisthenumberofhumanratingsour
systemisallowedtoobservefromeachdataset. Theremainderofthedataset(includinganydata
pointsthatareunlabeled,orlabeledbutwiththelabelsremoved)isusedfortheautoratersampleS˜ .
N
Seahorse. The Seahorse dataset [10] focuses on multilingual summarization. The authors
consideredgenerativemodelsthatoutputsummariesforadocument,andcollectedlabelsformany
9systemsthatcoverserveraldimensionsofsummaryquality. Wefocusononequalitydimension—
whether the summary is fully attributable to the source document—and on one summarization
system—afinetuned13BparametermT5model[31]. Theautoratermodelsforeachdimensionare
alsomT5-XXLfinetunedmodels,whichoutputprobabilityscores. Thedatacontains2727examples
forthesetwotasks,allofwhichhavebothhumanratingsaswellasautoraterscores.
AttributedQA. Inattributedquestionanswering[6],thegoaloftheQAsystemistooutputboth
an answer, and a retrieved document that provides support for that answer. The system is only
consideredtobecorrectiftheanswerisbothcorrect,andindeedsupportedbythelinkeddocument.
Weevaluatethehighest-scoring“retrieve-and-read”QAsystemfromthisdataset,4 anddefineour
autorater to be an 11B parameter T5 model [24] fine-tuned on a collection of natural language
entailmenttasks[14]. LikeSeahorse,themodelpredictsaprobabilityforwhetherornottheQA
systemgaveanattributableanswer. Thisdatasethas1000humanlabelsand3000autoraterlabels.
Galaxy. TodemonstratethegeneralityofStratPPIbeyondLLM-basedsettings,wealsoconsider
the Galaxy dataset [29], where the task is to estimate the fraction of spiral galaxies in the local
universe. TheautoraterusedhereisaResNetclassifierappliedtoimagesfromtheSloanDigital
SkySurvey(SDSS)[32]. Theclassifierestimatestheprobabilitythatthegalaxyinquestionisspiral.
Weuse16,743observationsfromthedatasetwhichcontainboththehumanandautoraterlabels.
Methodology. We follow a procedure similar to [1; 3] to study CI estimates as a function of
n. We report results on the following estimates: Classical inference, PPI++, StratPPI Prop.
and StratPPI Heur. We do not report StratPPI Opt. since it is unknown for real data. For
eachvalueofn,wesamplenofthecaseswithhumanlabelsatanallocationrateρ (thisrateis
k
determineddifferentlyinStratPPI Prop. andStratPPI Heur.). Asnotedabove,weconstruct
theunlabeleddatasetS˜ byjoiningtheremaininglabeleddata(withoututilizingthetruelabels)
N
with any unlabeled data available for that dataset. We repeat this over 1000 trials, and for each
trialweobtaintheCIwidth. WereportthemeanofthesewidthsinFigure2,aswellasthepercent
reductioninwidthovertheclassicalinferencebaseline, |C αclassical|−|C αmethod| ×100%. Wealsoreport
|Cclassical|
α
the“effectivesamplesize”,whichwedefineasthenumberofsamplesrequiredtoobtainaCIof
thesamewidthasthemethodatsamplesizenwhenusingtheclassicalinferencebaselineinstead.
Results. Figure2showsalargeimprovementforStratPPImethodsoverbothPPI++andclas-
sicalinferenceformostdatasets. Specifically,thoughallCIsdecreasesubstantiallyinabsolutesize
withnasexpected,weseethatimprovementsinthepercentreductioninCIwidthoverPPI++canbe
aslargeas0.10→0.20,0.20→0.30,and0.25→0.35pointsinSeahorse,AttributedQA,and
Galaxy,respectively. Furthermore,wecanobservethatmanyofthedatasetsexhibitheterogeneous
characteristics,forwhichheuristicallocationhelpsconsiderably. InGalaxy,thisaccountsfora+10
percentreductioninCIwidth. InSeahorseandAttributedQA,theimprovementsarelessstrong
butstillclearlyapparent. Thepracticalimplicationoftheseresultsisthatwhenlimitedbyahuman
ratingbudget,StratPPIisabletoproduceanestimateofthemeanwithfewerhumanratingsvia
stratificationandsamplingallocation. Forexample,fortheSeahorsedataset,wecanseefromthe
rightcolumninFigure2thatStratPPI Heur. withonly300humanratingswillbeapproximately
asconfidentaboutthemeanastheclassicalestimatethatutilizes600humanratings,afactorof2×.
6 Conclusion
Assystemsbuildontopoflargelanguagemodelscontinuetobecomemoreandmoreadvanced,
itbecomesincreasinglychallengingtoevaluatetheirperformanceusingautomatictools. Manual
labeling, on the other hand, is slow and expensive. Methods which therefore save on annotation
costarecriticalforreliablyevaluatingmodels,andknowingwhentheyareimproving—ordegrading.
Prediction-poweredinference(PPI)isapromisingclassofsuchhybridevaluationmethods,since
itcanleveragedtoprovablyproducestatisticallyvalidconfidenceintervals,whilealsoeffectively
reducing the number of human labels needed to obtain intervals of certain width. Our results
demonstratethatwecanpushPPIevenfurtherbyintroducingamethodforperformingevenlower
4WeuseRTR-10.Retrieval-augmentedgeneration(RAG)modelsuseadenseretrievertofinddocuments
d fromacorpusforaqueryx,andthengenerateananswera withafine-tunedLLMthattakesd ascontext.
x x x
10varianceM-estimationbyemployingstratifiedsampling. Inparticular,wefindthatstratifications
basedonthepredictionsoftheautoratersthemselvesprovestobeanpowerfulstratificationtechnique.
Limitations. WhiletheconfidenceintervalsproducedbyStratPPI(andPPI)enjoycoverageguar-
antees,theseguaranteesareasymptotic. Whenfinite-sampleperformanceisofparticularimportance,
techniquesthataffordstrongerguaranteesmightbepreferred[2;4;5]. Wealsonotesomenon-trivial
aspectsofthestratifiedsamplingsetup: (i)thenumberofstratahastobefixed;ifK scaleswithna
morecarefultreatmentisrequired;and(ii)theassumedobservationmodelisdifferentfromtraditional
i.i.d. settings—wemustbeabletosamplefixedsizedsamplesfromeachpartition;and(iii)perfor-
mancemaynotbeimprovediftheselectedstratificationisnotstatisticallyuseful.Furthermore,inprac-
tice,thehumandatamighthavealreadybeencollected,inwhichcasethestudiedstratifiedsampling
setupdoesnotdirectlyapply. Weleavethestudyofsuchpost-stratifiedestimatorstofuturework.5
Broaderimpacts. ThispaperintroducesamorepowerfulstatisticalmethodforevaluatingLLMs,by
merginghumanevaluationswithautoratersinawaythatisawareofsubdomaindifferences. Ourgoal
istohelppowermorereliableevaluationswithlowerannotationeffort,bothintermsofcostandtime.
Acknowledgements. We thank Jacob Eisenstein, Jonathan Berant, Anastasios Angelopoulos,
TaylanCemgil,ArnoudDoucet,andChrisDyerforhelpfulcommentsanddiscussions.
References
[1] AnastasiosN.Angelopoulos,StephenBates,ClaraFannjiang,MichaelI.Jordan,andTijana
Zrnic. Prediction-poweredinference. Science,382(6671):669–674,2023. doi: 10.1126/science.
adi6000. URLhttps://www.science.org/doi/abs/10.1126/science.adi6000.
[2] AnastasiosN.Angelopoulos,StephenBates,AdamFisch,LihuaLei,andTalSchuster. Confor-
malriskcontrol. arXivpreprint: 2208.02814,2023.
[3] AnastasiosN.Angelopoulos,JohnC.Duchi,andTijanaZrnic. PPI++: Efficientprediction-
poweredinference. arXivpreprint: 2311.01453,2023.
[4] AnastasiosNikolasAngelopoulos,StephenBates,EmmanuelJ.Candès,MichaelI.Jordan,and
LihuaLei. Learnthentest: Calibratingpredictivealgorithmstoachieveriskcontrol. arXiv
preprint: 2110.01052,2021.
[5] StephenBates,AnastasiosNikolasAngelopoulos,LihuaLei,JitendraMalik,andMichaelI.
Jordan. Distributionfree,riskcontrollingpredictionsets. arXivpreprint: 2101.02703,2020.
[6] BerndBohnet,VinhQ.Tran,PatVerga,RoeeAharoni,DanielAndor,LivioBaldiniSoares,
MassimilianoCiaramita,JacobEisenstein,KuzmanGanchev,JonathanHerzig,KaiHui,Tom
Kwiatkowski,JiMa,JianmoNi,LierniSestorainSaralegui,TalSchuster,WilliamW.Cohen,
MichaelCollins,DipanjanDas,DonaldMetzler,SlavPetrov,andKellieWebster. Attributed
question answering: Evaluation and modeling for attributed large language models. arXiv
preprint: 2212.08037,2023.
[7] PierreBoyeau,AnastasiosN.Angelopoulos,NirYosef,JitendraMalik,andMichaelI.Jordan.
Autoevaldoneright: Usingsyntheticdataformodelevaluation. arXivpreprint: 2403.07008,
2024.
[8] JannisBulian,ChristianBuck,WojciechGajewski,BenjaminBörschinger,andTalSchuster.
Tomayto,tomahto.beyondtoken-levelanswerequivalenceforquestionansweringevaluation.In
YoavGoldberg,ZornitsaKozareva,andYueZhang,editors,Proceedingsofthe2022Conference
onEmpiricalMethodsinNaturalLanguageProcessing,pages291–305,AbuDhabi,United
ArabEmirates,December2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/
2022.emnlp-main.20. URLhttps://aclanthology.org/2022.emnlp-main.20.
5Wealsonotethatthebatchactiveinferencemethodof[34]wouldbeappropriateinthissetting.
11[9] ArunChaganty,StephenMussmann,andPercyLiang. Thepriceofdebiasingautomaticmetrics
innaturallanguageevalaution. InIrynaGurevychandYusukeMiyao,editors,Proceedingsof
the56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long
Papers), pages 643–653, Melbourne, Australia, July 2018. Association for Computational
Linguistics. doi: 10.18653/v1/P18-1060. URLhttps://aclanthology.org/P18-1060.
[10] ElizabethClark,ShrutiRijhwani,SebastianGehrmann,JoshuaMaynez,RoeeAharoni,Vitaly
Nikolaev,ThibaultSellam,AdityaSiddhant,DipanjanDas,andAnkurParikh. SEAHORSE:
Amultilingual,multifaceteddatasetforsummarizationevaluation. InHoudaBouamor,Juan
Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods
inNaturalLanguageProcessing,pages9397–9413,Singapore,December2023.Association
for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.584. URL https://
aclanthology.org/2023.emnlp-main.584.
[11] WilliamG.Cochran. SamplingTechniques,3rdEdition. JohnWiley,1977. ISBN0-471-16240-
X.
[12] LeoGao,JohnSchulman,andJacobHilton. Scalinglawsforrewardmodeloveroptimization.
InInternationalConferenceonMachineLearning,pages10835–10866.PMLR,2023.
[13] R.AlexHofer,JoshuaMaynez,BhuwanDhingra,AdamFisch,AmirGloberson,andWilliamW.
Cohen. Bayesianprediction-poweredinference. arXivepreprint: 2405.06034,2024.
[14] Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered
Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. True: Re-
evaluatingfactualconsistencyevaluation. arXivpreprint: 2204.04991,2022.
[15] EhsanKamalloo,NouhaDziri,CharlesClarke,andDavoodRafiei. Evaluatingopen-domain
questionansweringintheeraoflargelanguagemodels. InAnnaRogers,JordanBoyd-Graber,
andNaoakiOkazaki,editors,Proceedingsofthe61stAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1: LongPapers),pages5591–5606,Toronto,Canada,July
2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.acl-long.307. URL
https://aclanthology.org/2023.acl-long.307.
[16] Chia-WeiLiu,RyanLowe,IulianSerban,MikeNoseworthy,LaurentCharlin,andJoellePineau.
HowNOTtoevaluateyourdialoguesystem: Anempiricalstudyofunsupervisedevaluation
metricsfordialogueresponsegeneration. InJianSu,KevinDuh,andXavierCarreras,editors,
Proceedingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
pages2122–2132,Austin,Texas,November2016.AssociationforComputationalLinguistics.
doi: 10.18653/v1/D16-1230. URLhttps://aclanthology.org/D16-1230.
[17] S.L.Lohr. Sampling: DesignandAnalysis. Chapman&Hall/CRCTextsinStatisticalScience.
CRCPress,2021. ISBN9781000478235. URLhttps://books.google.com/books?id=
DahGEAAAQBAJ.
[18] WhitneyK.NeweyandDanielMcFadden. Chapter36largesampleestimationandhypothesis
testing. volume 4 of Handbook of Econometrics, pages 2111–2245. Elsevier, 1994. doi:
https://doi.org/10.1016/S1573-4412(05)80005-4. URLhttps://www.sciencedirect.com/
science/article/pii/S1573441205800054.
[19] Jerzy Neyman. On the two different aspects of the representative method: The method of
stratified sampling and the method of purposive selection. Journal of the Royal Statistical
Society, 97(4):558–625, 1934. ISSN 09528385. URL http://www.jstor.org/stable/
2342192.
[20] AlexandruNiculescu-MizilandRichCaruana. Obtainingcalibratedprobabilitiesfromboosting.
InUAI,volume5,pages413–20,2005.
[21] JekaterinaNovikova,OndˇrejDušek,AmandaCercasCurry,andVerenaRieser. Whyweneed
newevaluationmetricsforNLG.InMarthaPalmer,RebeccaHwa,andSebastianRiedel,editors,
Proceedingsofthe2017ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
pages2241–2252,Copenhagen,Denmark,September2017.AssociationforComputational
Linguistics. doi: 10.18653/v1/D17-1238. URLhttps://aclanthology.org/D17-1238.
12[22] Richard Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur Parikh, and He He.
Rewardgaminginconditionaltextgeneration. InProceedingsofthe61stAnnualMeetingof
theAssociationforComputationalLinguistics(Vlume1: LongPapers),2023.
[23] JohnPlattetal.Probabilisticoutputsforsupportvectormachinesandcomparisonstoregularized
likelihoodmethods. Advancesinlargemarginclassifiers,10(3):61–74,1999.
[24] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
YanqiZhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunified
text-to-texttransformer. JournalofMachineLearningResearch,21(140):1–67,2020. URL
http://jmlr.org/papers/v21/20-074.html.
[25] JamesM.RobinsandAndreaRotnitzky. Semiparametricefficiencyinmultivariateregression
modelswithmissingdata. JournaloftheAmericanStatisticalAssociation,90(429):122–129,
1995. ISSN01621459. URLhttp://www.jstor.org/stable/2291135.
[26] JonSaad-Falcon,OmarKhattab,ChristopherPotts,andMateiZaharia. Ares: Anautomated
evaluationframeworkforretrieval-augmentedgenerationsystems. arXivpreprint: 2311.09476,
2024.
[27] A.W.vanderVaart. AsymptoticStatistics. CambridgeSeriesinStatisticalandProbabilistic
Mathematics.CambridgeUniversityPress,1998.
[28] SiruoWang,TylerHMcCormick,andJeffreyTLeek. Methodsforcorrectinginferencebased
onoutcomespredictedbymachinelearning. ProceedingsoftheNationalAcademyofSciences,
117(48):30266–30275,2020.
[29] KyleW.Willett,ChrisJ.Lintott,StevenP.Bamford,KarenL.Masters,BrookeD.Simmons,
KevinR.V.Casteels, EdwardM.Edmondson, LucyF.Fortson, SugataKaviraj, WilliamC.
Keel, Thomas Melvin, Robert C. Nichol, M. Jordan Raddick, Kevin Schawinski, Robert J.
Simpson, Ramin A. Skibba, Arfon M. Smith, and Daniel Thomas. Galaxy Zoo 2: detailed
morphologicalclassificationsfor304122galaxiesfromtheSloanDigitalSkySurvey. Monthly
NoticesoftheRoyalAstronomicalSociety,435(4):2835–2860,092013. ISSN0035-8711. doi:
10.1093/mnras/stt1458. URLhttps://doi.org/10.1093/mnras/stt1458.
[30] JeffreyM.Wooldridge. Asymptoticpropertiesofweightedm-estimatorsforstandardstratified
samples. EconometricTheory,17:451–470,2001. URLhttps://api.semanticscholar.
org/CorpusID:123148497.
[31] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,
AdityaBarua,andColinRaffel. mt5: Amassivelymultilingualpre-trainedtext-to-texttrans-
former. arXivpreprintarXiv:2010.11934,2020.
[32] DonaldG.York,J.Adelman,Jr.JohnE.Anderson,ScottF.Anderson,JamesAnnis,NetaA.
Bahcall,J.A.Bakken,RobertBarkhouser,StevenBastian,EileenBerman,WilliamN.Boroski,
SteveBracker,CharlieBriegel,JohnW.Briggs,J.Brinkmann,RobertBrunner,ScottBurles,
LarryCarey,MichaelA.Carr,FranciscoJ.Castander,BingChen,PatrickL.Colestock,A.J.
Connolly,J.H.Crocker,IstvánCsabai,PaulC.Czarapata,JohnEricDavis,MamoruDoi,Tom
Dombeck,DanielEisenstein,NancyEllman,BrianR.Elms,MichaelL.Evans,XiaohuiFan,
GlennR.Federwitz,LarryFiscelli,ScottFriedman,JoshuaA.Frieman,MasatakaFukugita,
BruceGillespie,JamesE.Gunn,VijayK.Gurbani,ErnstdeHaas,MerleHaldeman,FrederickH.
Harris, J. Hayes, Timothy M. Heckman, G. S. Hennessy, Robert B. Hindsley, Scott Holm,
DonaldJ.Holmgren,ChihaoHuang,CharlesHull,DonHusby,Shin-IchiIchikawa,Takashi
Ichikawa,ŽeljkoIvezic´,StephenKent,RitaS.J.Kim,E.Kinney,MarkKlaene,A.N.Kleinman,
S.Kleinman, G.R.Knapp, JohnKorienek, RichardG.Kron, PeterZ.Kunszt, D.Q.Lamb,
B. Lee, R. French Leger, Siriluk Limmongkol, Carl Lindenmeyer, Daniel C. Long, Craig
Loomis,JonLoveday,RichLucinio,RobertH.Lupton,BryanMacKinnon,EdwardJ.Mannery,
P.M.Mantsch,BruceMargon,PeregrineMcGehee,TimothyA.McKay,AveryMeiksin,Aronne
Merelli,DavidG.Monet,JeffreyA.Munn,VijayK.Narayanan,ThomasNash,EricNeilsen,
RichNeswold,HeidiJoNewberg,R.C.Nichol,TomNicinski,MarioNonino,NorioOkada,
SadanoriOkamura,JeremiahP.Ostriker,RussellOwen,A.GeorgePauls,JohnPeoples,R.L.
Peterson,DonaldPetravick,JeffreyR.Pier,AdrianPope,RuthPordes,AngelaProsapio,Ron
13Rechenmacher, Thomas R. Quinn, Gordon T. Richards, Michael W. Richmond, Claudio H.
Rivetta, Constance M. Rockosi, Kurt Ruthmansdorfer, Dale Sandford, David J. Schlegel,
DonaldP.Schneider,MakiSekiguchi,GarySergey,KazuhiroShimasaku,WalterA.Siegmund,
StephenSmee, J.AllynSmith, S.Snedden, R.Stone, ChrisStoughton, MichaelA.Strauss,
ChristopherStubbs,MarkSubbaRao,AlexanderS.Szalay,IstvanSzapudi,GyulaP.Szokoly,
AniruddaR.Thakar,ChristyTremonti,DouglasL.Tucker,AlanUomoto,DanVandenBerk,
Michael S. Vogeley, Patrick Waddell, Shu i Wang, Masaru Watanabe, David H. Weinberg,
Brian Yanny, and Naoki Yasuda. The sloan digital sky survey: Technical summary. The
AstronomicalJournal,120(3):1579,sep2000. doi: 10.1086/301513. URLhttps://dx.doi.
org/10.1086/301513.
[33] TijanaZrnicandEmmanuelJ.Candès. Cross-prediction-poweredinference. Proceedingsofthe
NationalAcademyofSciences,121(15):e2322083121,2024. doi: 10.1073/pnas.2322083121.
URLhttps://www.pnas.org/doi/abs/10.1073/pnas.2322083121.
[34] TijanaZrnicandEmmanuelJ.Candès. Activestatisticalinference. arXivpreprint: 2403.03208,
2024.
A Proofs
Webeginbydefiningthebasicregularityconditionsthatweassumeℓ satisfies:
θ
DefinitionA.1(Regularityconditionsofℓ ). Assumethat
θ
(i) ΘisacompactsubsetofRd;
(ii) Theminimizerθ∗ ∈int(Θ)isuniquewithE[∇ℓ (X,Y)]=0;
θ∗
(iii) Forall(x,y)∈X ×Y,ℓ (x,y)istwicecontinuouslydifferentiableonint(Θ);
θ
(iv) Forallθ ∈Θ,ℓ (X,Y), ∂ℓθ(X,Y),and ∂2ℓθ(X,Y) havefiniteexpectations;
θ ∂θi ∂θi∂θj
(v) E[∇2ℓ (X,Y)]isnon-singular.
θ
Theseregularityconditionsarefairlymild—forexample,itisstraightforwardtoverifythatthelossof
themeanestimator, 1∥y−θ∥2,satisfiesDefinitionA.1. Wenote,however,thatPPI++canbeapplied
2
tomerely“smoothenough”losses, whichalsoincludenon-continuouslydifferentiablefunctions
losseslikethequantileloss. Asweprimarilyfocusonmeanestimationinthiswork,forsimplicityof
analysisweonlyconsider(thestillbroadclass)oflossessatisfyingDefinitionA.1,thoughourresults
canbeexpectedtoreadilyextendtothemoregeneralcasefollowingasimilartreatmentasin[3;27].
Tosupportourtheoreticalresults,wealsoprovidethefollowingtwolemmas.
d p
LemmaA.1(Slutsky’sTheorem,generalform). LetX →XandY →c,whereX ,X,andY are
n n n n
d
randomvectors,andcisaconstantvector.Thenforanycontinuousfunctiong,g(X ,Y )→g(X,c).
n n
d
Proof. ByTheorem2.7of[27]wehave(X ,Y )→(X,c). Thecontinuousmappingtheoremthen
n n
d
impliesthatg(X ,Y )→g(X,c).
n n
LemmaA.2. UndertheregularityconditionsofDefinitionA.1,wehavethatθˆSPP →p θ∗.
λˆ
Proof. Under the regularity conditions of Definition A.1 and the fact that λˆ →p λ where λ is
k k k
constant,theuniformweaklawoflargenumberscanbeappliedtoeachterminLSPP(θ)sothat
λˆ
(cid:13) (cid:13)
sup(cid:13)LSPP(θ)−E[ℓ (X,Y)](cid:13)→p 0.
(cid:13) λˆ θ (cid:13)
θ∈Θ
Asθ∗isunique,Θiscompact,andℓ iscontinuousitfollowsfromTheorem2.1of[18]that
θ
θˆSPP →p θ∗.
λˆ
14A.1 ProofofTheorem2
Proof. Foreaseofnotation,wewilldefine
L˜f (θ)=
1
(cid:88)Nk
ℓ (X˜ ,f(X˜ )
Nk N
k
θ ik ik
i=1
1
(cid:88)nk
L (θ)= ℓ (X ,Y )
nk n θ ik ik
k
i=1
1
(cid:88)nk
Lf (θ)= ℓ (X ,f(X )).
nk n θ ik ik
k
i=1
Wewillalsousethefollowingshorthandfortheconditionalgradients:
∇ℓ :=∇ℓ (X,Y)|X ∈A and
k,θ θ k
∇ℓf :=∇ℓ (X,f(X))|X ∈A .
k,θ θ k
Assamplesarei.i.d. withineachstratum,theCLTgives
(cid:112) N (cid:16) ∇L˜f (θ∗)−E[∇ℓf ](cid:17) −→d N (cid:16) 0,Cov(cid:16) ∇ℓf (cid:17)(cid:17) , (23)
k Nk k,θ∗ k,θ∗
and
√
n
(cid:20)∇L nk(θ∗)−E[∇ℓ k,θ∗](cid:21)
−→d
N
(cid:18)(cid:20) 0(cid:21) ,Cov(cid:18)(cid:20)∇ℓ k,θ∗(cid:21)(cid:19)(cid:19)
. (24)
k ∇Lf (θ∗)−E[∇ℓf ] 0 ∇ℓf
nk k,θ∗ k,θ∗
SincesamplesS andS˜ areindependentfromeachother,wealsohavethat(23)and(24)converge
n N
jointly. ApplyingLemmaA.1forthefollowingcontinuousfunctionofλˆ −→p λ thengives
k k
√ (cid:32) (cid:16) (cid:17) (cid:20) 1 (cid:21)⊤(cid:20) ∇L˜ (θ∗)−E[∇ℓ ](cid:21)(cid:33)
n λˆ ·N ∇L˜f (θ∗)−E[∇ℓf ] + n k,θ∗
k Nk k,θ∗ −λˆ
k
∇L˜f nk(θ∗)−E[∇ℓf k,θ∗]
(cid:18) (cid:19) (cid:18) (cid:19)
n n
−→d N 0,λ2 Cov(∇ℓf ) +N 0, Cov(∇ℓ −λ ∇ℓf ) (25)
kN k,θ∗ n k,θ∗ k k,θ∗
k k
(cid:18) (cid:19)
r 1
=N 0, ·Vλk + Vλk . (26)
ρ˜ k,f,θ∗ ρ k,∆,θ∗
k k
Sinceθ∗satisfiesE[∇ℓ ]=0,bythelawoftotalexpectation
θ∗
K
(cid:88)
w E[∇ℓ ]=E[∇ℓ ]=0. (27)
k k,θ∗ θ∗
k=1
Usingthisfact,wecanwrite
K
∇LSPP(θ)=(cid:88) w (cid:16) ∇(cid:16) λˆ L˜f (θ)+L (θ)−λˆ Lf (θ)(cid:17) −E[∇ℓ ](cid:17) . (28)
λˆ k k Nk nk k nk k,θ∗
k=1
SincesamplesacrosstheK (wherehereK isaconstant)stratasareindependent, combiningthe
resultsof(26)with(28)yields
√
n∇LSPP(θ∗)−→d N(0,Bλ). (29)
λˆ w
Applyingthemeanvaluetheorem,wehave
∇LSPP(θˆSPP)=∇LSPP(θ∗)+∇2LSPP(θ˜)(θˆSPP−θ∗) (30)
λˆ λˆ λˆ λˆ λˆ
15forsomeθ˜betweenθˆSPPandθ∗. LetA†representthepseudoinverseofA. Then
λˆ
(cid:16) (cid:17)
θˆSPP−θ∗ =∇2LSPP(θ˜)† ∇LSPP(θˆSPP)−∇LSPP(θ∗) . (31)
λˆ λˆ λˆ λˆ λˆ
AsθˆSPP →p θ∗perLemmaA.2,wehaveθ˜→p θ∗. UndertheregularityconditionsofDefinitionA.1
λˆ
and the fact that λˆ −→p λ , an application of the uniform weak law of large numbers and the
k k
continuousmappingtheoremthengives
∇2LSPP(θ˜)† →p ∇2E[ℓ ]−1, (32)
λˆ θ∗
whichisaconstantterm. Furthermore,bythelawoftotalexpectation,
(cid:32) K (cid:33)−1
(cid:88)
E[∇2ℓ ]−1 = w H =A−1. (33)
θ∗ k k,θ∗ w
k=1
Finally,thefactthatθˆSPP →p θ∗andθ∗ ∈int(Θ),togetherwiththefactthatθˆSPPisaminimumof
λˆ λˆ
LSPP,impliesthat∇LSPP(θˆSPP)→p
0. Combiningtheseresultswith(29)viaLemmaA.1andthe
λˆ λˆ λˆ
factthatA−1issymmetricfortwicecontinuouslydifferentiableℓ gives
w θ
√
n(θˆ−θ∗)−→d N(0,A−1Bλ(A−1)⊤)=N(0,A−1BλA−1). (34)
w w w w w w
A.2 ProofofCorollary2
Proof. TheregularityconditionsofDefinitionA.1allowustoapplyLemma4.3of[18]toshowthat
eachoftheplug-inestimatesforeachstratumtermsatisfies
H(cid:98) k,θˆSPP →p H k,θ∗ and V(cid:98) kλ ,fk ,θˆSPP →p V(cid:98) kλ ,fk ,θ∗ and V(cid:98) kλ ,∆k ,θˆSPP →p V(cid:98) kλ ,∆k ,θ∗
whichimpliesthattheweightedplug-inestimateforΣˆλˆ satisfiesΣˆλˆ →p Σλ. (15)isthusequivalent
w w w
totakingthe(cid:0)α,1− α(cid:1) quantilesoftheasymptoticnormaldistributionofθˆSPP,implying(16).
2 2
A.3 ProofofProposition1
Proof. Inthespecialcaseofthesquareloss,theHessianistheidentitymatrix.Therefore,simplifying
andapplyingthelinearityofthetrace,wehave
K (cid:18) (cid:19)
(cid:88) r 1
Tr(Σλ)= w2Tr ·Vλk + Vλk (35)
w k ρ˜ k,f,θ∗ ρ k,∆,θ∗
k k
k=1
K
(cid:88) (cid:16) (cid:17)
= w Tr r·Vλk +Vλk (36)
k k,f,θ∗ k,∆,θ∗
k=1
K K
(cid:88) (cid:16) (cid:17) (cid:88) (cid:16) (cid:17)
=r w Tr Vλk + w Tr Vλk (37)
k k,f,θ∗ k k,∆,θ∗
k=1 k=1
d K d K
(cid:88)(cid:88) (cid:88)(cid:88)
=r w Vλk + w Vλk (38)
k k,f,θ∗,jj k k,∆,θ∗,jj
j=1k=1 j=1k=1
whereweincludethesubscriptjj toindexthediagonalvariancetermsofbothcovariancematrices.
Foreaseofnotation,denotetheconditionalvariancesas
V
f,j
|Z =k :=λ2∇ℓ θ∗(X,f(X))|X ∈A
k
and
j
V |Z =k :=∇ℓ (X,Y)−λ∇ℓ (X,f(X))|X ∈A .
∆,j θ∗ θ∗ k
j j
16Thenwecanwrite(38)as
d K d K d
(cid:88)(cid:88) (cid:88)(cid:88) (cid:88)
r w Vλk + w Vλk = rE[Var(V |Z)]+E[Var(V |Z)],
k k,f,θ∗,jj k k,∆,θ∗,jj f,j ∆,j
j=1k=1 j=1k=1 j=1
(39)
andapplythelawoftotalvariancetoget
d
(cid:88)
Tr(Σλ)= rE[Var(V |Z)]+E[Var(V |Z)] (40)
w f,j ∆,j
j=1
d
(cid:88)
= r(Var(V )−Var(E[V |Z]))+(Var(V )−Var(E[V |Z])) (41)
f,j f,j ∆,j ∆,j
j=1
d
(cid:88)
≤ rVar(V )+Var(V ) (42)
f,j ∆,j
j=1
=Tr(Σλ). (43)
Finally,(42)holdswithequalityiffbothVar(E[V | Z]) = 0andVar(E[V | Z]) = 0,which,
f,j ∆,j
combined with the assumption that P(Z = k) = w > 0 for all k, is only satisfied when both
k
E[V |Z]andE[V |Z]areconstants.
f,j ∆,j
A.4 ProofofProposition2
Proof. BylinearityofBλ,wecanrewriteΣλ =A−1BλA−1as
w w w w w
K (cid:18) (cid:19)
(cid:88) r 1
Σλ = w2A−1 ·Vλk + Vλk A−1 (44)
w k w ρ˜ k,f,θ∗ ρ k,∆,θ∗ w
k k
k=1
=(cid:88)K w k2 A−1(cid:18) rρ
k ·Vλk +Vλk
(cid:19)
A−1 (45)
ρ w ρ˜ k,f,θ∗ k,∆,θ∗ w
k k
k=1
=(cid:88)K w k2 A−1(cid:18) n
k ·Vλk +Vλk
(cid:19)
A−1. (46)
ρ w N k,f,θ∗ k,∆,θ∗ w
k k
k=1
Bylinearityofthetrace,wethenalsohave
Tr(Σλ)=(cid:88)K w k2 Tr(cid:18) A−1(cid:18) n
k ·Vλk +Vλk
(cid:19) A−1(cid:19)
. (47)
w ρ w N k,f,θ∗ k,∆,θ∗ w
k k
k=1
Aseachterminthesumisindependent,wecanminimizethesumbyminimizingeachindividual
term,i.e.,
(cid:18) (cid:18) (cid:19) (cid:19)
n
λ∗ =argminTr A−1 k ·Vλk +Vλk A−1 (48)
k λk w N k k,f,θ∗ k,∆,θ∗ w
fork =1,...,K. WecanthendirectlyapplyProposition2of[3]toget
(cid:16) (cid:17)
Tr A−1(Cov(∇ℓ ,∇ℓf )+Cov(∇ℓf ,∇ℓ ))A−1
w k,θ∗ k,θ∗ k,θ∗ k,θ∗ w
λ∗ = , (49)
k (cid:16) (cid:17)
2(1+r )Tr A−1Cov(∇ℓf )A−1
k w k,θ∗ w
wherer =n /N .
k k k
17A.5 ProofofProposition3
Proof. FollowingthesamederivationasProposition2,wecanrewrite
K (cid:18) (cid:18) (cid:19) (cid:19)
(cid:88) r 1
Tr(Σλ)= w2Tr A−1 ·Vλk + Vλk A−1 (50)
w k w ρ˜ k,f,θ∗ ρ k,∆,θ∗ w
k k
k=1
(cid:88)K w2 (cid:16) (cid:17) (cid:88)K w2 (cid:16) (cid:17)
=r kTr A−1Vλk A−1 + kTr A−1Vλk A−1 , (51)
ρ˜ w k,f,θ∗ w ρ w k,∆,θ∗ w
k k
k=1 k=1
andthereforecanoptimizeρ˜ andρ independently.
k k
(cid:16) (cid:17)
We start with ρ . Let z = Tr A−1Vλk A−1 . We then solve the constrained optimization
k k w k,∆,θ∗ w
problem
(cid:88)K w2 (cid:88)K
minimize kz s.t. ρ =1,ρ ≥0. (52)
ρ k k k
k
k=1 k=1
TurningthisintoaLagrangianwithadditionalslackvariabless ≥0,wehave
k
(cid:88)K w2 (cid:32) (cid:88)K (cid:33) (cid:88)K
J(ρ,µ,η,s)= kz +µ ρ −1 + η (ρ −s2) (53)
ρ k k k k k
k
k=1 i=1 k=1
Setting∇J(ρ,µ,η)=0andsolvingforρ thengives:
k
∂J w2
=− kz +µ+η =0 (54)
∂ρ ρ2 k k
k k
∂J (cid:88)
=1− ρ =0 (55)
∂µ k
∂J
=ρ −s2 =0 (56)
∂η k k
k
∂J
=−2η s =0 (57)
∂s k k
k
Assumethattheinequalityconstraintisinactive,andη =0. Solvingforρ∗,
k k
(cid:115) (cid:115)
ρ∗ =s2 =
w k2z
k
=⇒(cid:88)K w k2z
k =1 (58)
k k µ µ
k=1
√ (cid:88)K (cid:113)
=⇒ µ= w2z (59)
k k
k=1
√
w z
=⇒ρ∗ k = (cid:80)K k
w
k √
z
. (60)
k′=1 k′ k′
We can now verify that the constraint is inactive, with ρ∗ ≥ 0, since (cid:80)K w = 1,w ≥ 0 by
k k=1 k k
definitionofavalidprobabilitydistribution,andwehavez ≥0sinceitisasumofnon-negative
k
varianceterms.
(cid:16) (cid:17)
Thesameanalysiscanthenbeappliedtoρ˜,butforz =Tr A−1Vλk A−1 .
k k w k,f,θ∗ w
B Asimplifiedestimateofthesampleallocationformeanestimation
InthesettingofExample2(i.e.,1-dmeanestimation),assumethatc(y |x)≈P(Y =y |X =x)is
aconfidenceestimateforlabely ∈Y,whereY isdiscrete. Then,ifwedefinetheautorateras
(cid:88)
f(x)= c(y |x)·y ≈E[Y |X =x], (61)
y∈Y
18thetermVar(Y −λ f(X) | X ∈ A )furthersimplifiesto≈ Var(Y | X ∈ A )foranyvalueof
k k k
λ . Applyingthelawoftotalvariance,wecanconvenientlywriteVar(Y |X ∈A )as
k k
Var(Y |X ∈A )=E[Var(Y |X)|X ∈A ]+Var(E[Y |X]|X ∈A ), (62)
k k k
whichcanbeempiricallyestimatedasσˆ onunlabeledautoraterdata,X˜ ,i=1,...,N ,via
k ik k
 
σˆ k2 =E (cid:98)Nk(cid:88) c(y |X˜ ik)·y−f(X˜ ik))+V(cid:100)ar Nk(f(X˜ ik)), (63)
y∈Y
whereE(cid:98)Nk andV(cid:100)ar
Nk
denotetheempiricalmeanandvarianceoverallX˜ ik,respectively. Lastly,
whenY isbinary(whichisthecaseinallofourexperimentsin§5.2),thisbecomes
(cid:104) (cid:105)
σˆ k2 =E (cid:98)Nk f(X˜ ik)(1−f(X˜ ik)) +V(cid:100)ar Nk(f(X˜ ik)), (64)
whichcanbeeasilycalculatedinPythonasnp.mean(yhat * (1 - yhat)) + np.var(yhat).
19