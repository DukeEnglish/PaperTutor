Coarse-To-Fine Tensor Trains for Compact Visual Representations
SebastianLoeschcke12 DanWang1 ChristianLeth-Espensen3
SergeBelongie1 MichaelJ.Kastoryano1 SagieBenaim4
Abstract searchwithinneuralfieldrepresentations(Chenetal.,2022;
Obukhovetal.,2022;Mu¨lleretal.,2022)hasdirectedits
The ability to learn compact, high-quality, and
focustowardsthesparsityandefficiencyoftensornetworks,
easy-to-optimizerepresentationsforvisualdata
hintingatthepotentialgainsinthecompactnessandqual-
isparamounttomanyapplicationssuchasnovel
ityofsuchrepresentations. Inparticular,TensoRF(Chen
view synthesis and 3D reconstruction. Recent
etal.,2022)appliedlow-ranktensordecompositionofra-
workhasshownsubstantialsuccessinusingten-
diancefieldstoachievesubstantialefficiencygains. This sor networks to design such compact and high-
approachwasfurtherextendedbyTT-NF(Obukhovetal.,
quality representations. However, the ability to
2022), which utilized a tensor train representation to im-
optimizetensor-basedrepresentations,andinpar-
provecompression. Indeed,tensortrains,aswellasmore
ticular,thehighlycompacttensortrainrepresen-
complex tensor networks (Oru´s, 2014; Vidal, 2007), are
tation,isstilllacking. Thishaspreventedpracti-
knowntoallowsubstantialimprovementsincompression.
tioners from deploying the full potential of ten-
sor networks for visual data. To this end, we However,despiteencouragingprogress,thefullrealization
propose‘ProlongationUpsamplingTensorTrain of tensor networks for compact and efficient representa-
(PuTT)’,anovelmethodforlearningtensortrain tionsisstilllacking. Themainobstaclesofcurrenttensor-
representationsinacoarse-to-finemanner. Our basedrepresentationsare: (1). Optimizationusingcurrent
methodinvolvestheprolongingor‘upsampling’ gradient-basedschemesoftengetsstuckinlocalminimaand
ofalearnedtensortrainrepresentation,creating fallsshortoftheultimatecompressionlimitofthetensor
a sequence of ‘coarse-to-fine’ tensor trains that trainformat. Inparticular,TensoRF(Chenetal.,2022)can
are incrementally refined. We evaluate our rep- only handle small tensors and does not enjoy the signifi-
resentation along three axes: (1). compression, cantparameterefficiencyoftensortrains. TT-NF(Obukhov
(2). denoisingcapability,and(3). imagecomple- etal.,2022)highlightsthecapacityoftensortrainstomodel
tioncapability. Toassesstheseaxes,weconsider larger tensors using significantly fewer parameters com-
the tasks of image fitting, 3D fitting, and novel pared to TensoRF, offering, in principle, a more scalable
viewsynthesis,whereourmethodshowsanim- approach. However, TT-NF lacks efficient optimization
provedperformancecomparedtostate-of-the-art strategiesalignedtoTensoRF.TT-NFoftenreachesalocal
tensor-basedmethods. minimuminoptimizationandcannotmodelthefullpoten-
tial of tensor trains, resulting in poorer performance. (2).
Theyoftenstrugglewithnoisyandincompletedata.
https://github.com/sebulo/PuTT
Toward resolving these obstacles, we propose a novel,
coarse-to-fine,tensor-train-basedrepresentationcalled‘Pro-
1.Introduction
longationUpsamplingTensorTrain(PuTT)’andanasso-
ciated gradient-based optimization strategy. Our strategy
Buildingacompact,easy-to-fit,andhigh-qualityvisualdata
involvestheprolongingor‘upsampling’ofalearnedtensor
representationisparamountformanycomputervisionand
trainrepresentationinacoarse-to-finemanner,creatinga
graphicsapplications,suchasnovelviewsynthesis,3Dfit-
sequenceof‘coarse-to-fine’tensortrainsthatareincremen-
ting,andgeneration(Xieetal.,2022). Tothisend,recentre-
tallyrefined. Thecoarse-to-fineoperationsareperformed
1UniversityofCopenhagen2ITUniversityofCopenhagen3Aarhus directlyinthetensortrainformat,circumventingtheneedto
University4HebrewUniversityofJerusalem.Correspondenceto:
processindividualdatapoints,asiscustomaryinlinearinter-
SebastianLoeschcke<sbl@di.ku.dk>.
polation. ThisallowsforefficientsamplingandforPuTTto
beappliedtoextremelyfinegridsandtofithigh-resolution
Proceedings of the 41st International Conference on Machine
dataatlimitedadditionalmemoryandcomputationalcost.
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
theauthor(s).
1
4202
nuJ
6
]VC.sc[
1v23340.6042:viXraCoarse-To-FineTensorTrainsforCompactVisualRepresentations
CentraltoPuTTisitsuniquecapabilitytolearnQuantized moregeneralandcompactlearnedmulti-scalerepresenta-
Tensor Trains (QTT) (Oseledets, 2009), a tensor format tion,wherebythehierarchylevelsaretensortrains.
designed for efficient representation through hierarchical
CompactVisualRepresentations Visualrepresentations
structuring. It does so through a hierarchical refinement
canbedividedbetweenexplicit,implicit,andhybridrep-
process, reminiscent of the multigrid method for solving
resentations. Explicitrepresentations,suchasimagegrids,
partialdifferentialequations(Bramble,2019). Ouriterative
3Dmeshes,pointcloudsandvoxelgridsareinterpretable
coarse-to-finerepresentationandoptimizationstrategyen-
and expressive but expensive to store and scale. Implicit
ablessignificantparameterefficiencyoftensortrainsonthe
representations (Sitzmann et al., 2019; Lombardi et al.,
onehandandtheirefficientoptimizationontheother. The
2019)wereusedforapplicationssuchasnovelviewsynthe-
empiricalfindingsindicatethatourstrategyeffectivelymit-
sis(Mu¨lleretal.,2022),generativemodeling(Niemeyer&
igatestheimpactoflocalminimaduringtraining,thereby
Geiger,2021;Chanetal.,2021;2022),andsurfacerecon-
enabling a parametrized learning model to approach the
struction(Oechsleetal.,2021). Whilecompact, theyare
theoreticallimitallowedwithintheTTformat;asobtained
knowntobeslow. Tothisend,hybridrepresentationswere
bytheTT-SVDdecomposition(Oseledets,2011). Unlike
developed. Forinstance,DVGO(Sunetal.,2022)optimize
TT-SVD,ourstrategyemployslearningfromsamples,and
voxel grids of features. This leads to significantly lower
benefitsfromtheassociatedversatility.
memoryandcapacityrequirements. Instant-NGP(Mu¨ller
Visualdataisknowntohaveanaturalhierarchicalstructure, etal.,2022)combinesanexplicitmulti-resolutionhashable
asevidencedbythesuccessofwavelet-basedmethodsin featuregridwithsmallMLPmappingfeaturestocolorand
imageandvideocompression(Antoninietal.,1990). To opacityvalues. Whileourapproachalsoemploysamultires-
thisend,theadoptionofQTTinPuTTismotivatedbyits olutionstructure,itiscenteredaroundafactorizationofa
suitabilityforhandlingthemulti-resolutionnatureandde- quantizedhierarchicalstructureineachlayer. Hashingisan
pendencieswithinsubregionsofdata,akintothoseinvisual orthogonaltechnique;combiningitwithourmethodisleft
data. ComparedtoothertensordecompositionslikeCPand forfuturework.
Tucker, which scale linearly with dimension (d) and side
Tensor network representations Large-scale tensor
length(L),QTToffersamoremodestscalinginsidelength
networks have seen extensive applications in quantum
O(dlog(L)R2),whereRistherankandcapturesthecor-
physics (White, 1992b), including quantum computa-
relationinthedataacrossdimensionsandresolutionscale.
tion (Pan & Zhang, 2022), and can be used to simulate
This scalability positions QTT as an effective choice for
stronglycorrelatedsystems(Oru´s,2019). Tolearnvisual
large-scaletensors,surpassingCP,VM,andTuckerinspace
representations,TensoRF(Chenetal.,2022)significantly
efficiency,whichisespeciallybeneficialasdimensionsand
improvedcompressionbyusingCPandVMdecomposition
resolutionsincrease. Weevaluateourrepresentationalong
forNeRFs. CPdecompositionwaslaterextendedby(Liu
threeaxes: (1). compression,(2). denoisingcapability,and
etal.,2020b). Instead,ourmethodcanhandleamuchmore
(3). ability to learn from incomplete or missing data. To
expressiverepresentationoftensortrains. Ourmethodex-
value these axes, we consider the tasks of image fitting,
tends the potential of tensor trains for larger tensors, as
3D fitting, and novel view synthesis, where our method
notedinTT-NF(Obukhovetal.,2022),byincorporatinga
showsimprovedqualitativeandquantitativeperformance
coarse-to-fineapproachthatensuresefficientoptimization
comparedtostate-of-the-arttensor-basedbaselines.
similartoTensoRFandenhancesdenoisingandimagecom-
pletioncapabilities. Thisstrategyhelpstofullyutilizethe
2.RelatedWork expressivestrengthoftensor-trainrepresentations.
Coarse-to-Fine Representations Coarse-to-fine, or
3.Notation
multi-scale,visualrepresentationsweredevelopedforsev-
eralpurposes,suchascompressionorreducingtransmission
Graphical notation Tensor Network Notation (TNN)
time(Szeliski,2022). Earlyworksincludemulti-resolution
providesapowerfultoolforvisualizingtheinteractionsbe-
pyramidssuchasLaplacianandGaussianpyramids(Adel-
tweentensorsinatensornetwork. Inthesevisualizations
sonetal.,1984),half-octavepyramids(Crowley&Stern,
(diagrams),eachtensorisrepresentedasanode,withanum-
1984)andwavelets(Mallat,1989). Recently,coarse-to-fine
beroflegscorrespondingtoitsdimensions. Forexample:
representationswereusedtobuildneuralfields(Yangetal.,
(i). AmatrixW ∈Rm×nisasanodewithtwolegs: W .
2022;Lindelletal.,2022). Similarly,Instant-NGP(Mu¨ller
(ii). Avectorx∈Rnisanodewithasingleleg: x. (iii).
et al., 2022) uses a learned mutual-resolution grid repre-
Vector-matrixmultiplicationWxisdepictedbycontracting
sentation as a fast and compact representation for novel
(summingover)legsoftheconnectedtensors: W x. (iv).
viewsynthesis. Similarlytoourmethod,thisrepresentation
islearnedusingSGD.Ourmethod, however, considersa LargertensorsT ∈Rm×n×r havemorelegs: T .
2Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Figure1.Trainingonelevelofhierarchy.Initially,abatchBserves
twopurposes:(1).sampledatapointsY fromthetargetinput,(2).
B
itistransformedintoQTTindicesBˆ.Subsequently,corresponding
values Yˆ are sampled from the QTT. The reconstruction loss
Bˆ
betweenY andYˆ isusedandbackpropagated.
B Bˆ
Figure3.Fullcoarse-to-finelearning,integrating‘Train’and‘Up-
sample’phasesofFig.1andFig.2.WestartwithinputI ,whichis
D
downscaledtoI (resolution2d−l×2d−l).AQTTisthenran-
D−l
domlyinitializedforI .AftertrainingthisQTT(asperFig.1)
D−l
uptoiterationi ,weproceedwithupsampling(asillustratedin
1
Fig.2),producingT oflengthD−l+1.Thisrepresentsa
D−l+1
gridatresolution2d−l+1×2d−l+1andrequiressamplingfroma
newlydownsampledtargetI ofthesameresolution.
D−l+1
distinguishthephysicalindicesj fromthevirtualindices
k
r . ThebonddimensionsR canbeunderstoodasarank
k k
truncation along the cut n ,...,n and n ,...,n , only
1 k−1 k d
keepingtheRlargestsingularvalues.
Inthecontextoftensortrains,therankmeasuresthecom-
Figure2.IllustrationofQTTupsamplingprocess.Beginningwith plexityorinformationcontentofthetensor.Thelargestrank
aQTTT DoflengthD,upsamplingisachievedthroughthepro- amongallvirtualindicesisconsideredtherankofthetensor
longationMPOP. ThisinvolvesconnectingtheD+1coresof
train. This rank dictates the ultimate expressivity of the
P to the corresponding cores of T and then contracting their
D tensortrainbydeterminingthenumberofvariationalparam-
sharedindices.TheresultisanewQTT,T ,oflengthD+1,
D+1 eters. AtensorT canberepresentedexactlywithamaximal
whereranksareincreasedtoR Rˆ .Tomanagethisrankgrowth,
i i bonddimensionR = max min{n ···n ,n ···n },
TT-SVD(Oseledets,2011)isemployedforranktruncation.This k 1 k−1 k D
processyieldsT withcontrolledranksR˜ ≤R . whichscalesexponentiallyinD. Tensortrainsareusedasa
D+1 i max
parameterizedvariationalclass,whereweupperboundthe
Suchdiagrams,composedofnodesandtheirinterconnec- bonddimensionatsomemaximalvalueR max. Themaxi-
tions,effectivelydepictwhatisknownasa“tensornetwork”. mumbonddimensiongovernsthemodel’sexpressivity.
Wefocusonfourclassesoftensornetworks: theCP,Tucker,
QTT Compressing visual data along physical dimen-
andtensortraindecompositions. In3D,wealsoconsider
sions(e.g.,imageheightorwidth)isnatural,asintheCP
theVMconstruction(Chenetal.,2022). Fordiscussionand
andTuckerdecompositions(Chenetal.,2022). However,
detailsonthesetensornetworks,seetheappendix.
significantly more compact representations are possible
TensorTrain(Oseledets,2011)oraMatrixProductState by compression along the ‘scaling dimension’. For that,
(MPS)(Klu¨mperetal.,1993;Perez-Garciaetal.,2007),is we consider the quantized tensor train (QTT) construc-
a decomposition of a tensor with N indices into a chain- tion (Khoromskij, 2011). The QTT format builds upon
like network of N order-3 tensors: This structure can modequantization,whichdecomposesthescalingdimen-
be graphically represented in Tensor Network Notation: sioninpowersoftwo. Asanillustrativeexample(Obukhov
A A A A A. Given a tensor T ∈ Rn1×n2×···nd, etal.,2022),considera3Dtensorrepresentinga(regular)
functionona16×16×16lattice. Throughmodequantiza-
theentriesofthetensorare:
tion,thistensorcanberecastasa12-dimensionalhypercube:
T
j1···jd
=Aj1Aj2···Ajd, (1) (2 1×2 2×2 3×2 4)×(2 1×2 2×2 3×2 4)×(2 1×2 2×2 3×2 4):
Q Q Q Q Q Q Q Q Q Q Q Q .
where {Ajk}n jkk =1 are R jk ×R jk+1 matrices for each j k; In this notation, the subscript indicates levels of
i.e. areR ×n ×R tensors. TheranksR are hierarchy, and parentheses are used for visual
A k k k+1 k
referredtoasthebonddimensionsofthetensortrain. We convenience. QTT assumes a particular reorder-
3Coarse-To-FineTensorTrainsforCompactVisualRepresentations
ing, where each hierarchical level is grouped as: QTTT oflengthDtoaprolongedQTT,T ,oflength
D D+1
(2 ×2 ×2 )·(2 ×2 ×2 )·(2 ×2 ×2 )·(2 ×2 ×2 ): D+1. ThisisachievedbylinkingDinputcoresofP tothe
1 1 1 2 2 2 3 3 3 4 4 4
Q Q Q Q .Thisorderingisfarmoreexpressivefor correspondingcoresofT andcontractingthesharedindices.
hierarchicaldata,asitavoidsbottlenecksconnectinglow- Asaresult,theranksofPT D increase,necessitatingarank
frequencydatainonedimension(sayx)tohigh-frequency reductionthroughTT-SVD(Oseledets,2011). Thisreduc-
datainanotherdimension(sayy). tioniscriticalsincetheranksexpandexponentiallyinthe
numberofupsamplingsteps. Theoutcomeisacompressed
Matrix Product operators A matrix product operator QTT,T ,withcontrolledranksR˜ ≤R ,ensuringan
D+1 i max
(MPO)isatensornetworkthatdescribesafactorizationof
efficientrepresentationoftheupsampledobject.
atensorwithdinputanddoutputindicesintoasequence
ofsmallertensors(Hubigetal.,2017;McCulloch,2008),
4.1.Learningtensortrains(PuTT)
witheachcorehavingtwophysicalandtwovirtualindices:
M M M M M. TensortrainsandMPOsdifferfun- WenowoutlineourProlongationUpsamplingTensorTrain
(PuTT)methodology. Trainingstartsatachosen(coarse)
damentallyintheirapplications: Tensortrainsserveascom-
resolution and progressively learns a QTT with finer and
pressedrepresentationsoflargevectorsinhigh-dimensional
finerresolution. ConsideratargetvectorI representing
spaces,whileMPOs,denotingmatrix-vectoroperations,act D
a visual object of size (2d)D, where d is the dimension
on tensor trains. This distinction enables MPOs to effec-
(i.e. d = 2 for images). We want to learn the best QTT
tivelyhandlelargeoperatorswithinthesehigh-dimensional
approximationofI ,T withDlegsofdimension2deach.
spacesinacompressedform(Stoudenmire,2021). D D
Westartwithadown-sampledobjectI andlearnT ,
D−l D−l
Intheappendix,weprovideadditionaldetailsandvisual- wherelisthenumberofupsamplingsteps. Forinstance,for
izationsoftensornetworks. InSec.D.1,Fig.16visualizes animageofresolution5122andthreeupsamplingsteps,we
the multi-resolution hierarchy of the QTT, illustrating its startlearningI andupsamplethreetimestoreachI .
6 9
coarse-to-finestructure. InSec.D.2,wedetailthestructure
Inourexperiments,welearntheQTTsateachresolution
ofthespecificMPOusedforupsamplingandthecomplexity
viaback-propagation,byoptimizingthemeansquareder-
oftheprolongationoperations.
ror. AfterobtainingasatisfactoryapproximationofI in
D−l
QTTformat,whichwecallT ,weapplytheupsampling
D−l
4.Method
MPO P to T , yielding a QTT with D −l +1
D−l+1 D−l
‘physical’legsP T . Ifthemaximalbonddimen-
Prolongationoperators WhileQTTallowsforsubstan- D−l+1 D−l
sionexceedsR ,thenweperformaTT-SVDcontraction
tial memory savings for equal representational power, it max
tokeepallranksbelowthemaxR . Wethenproceedto
suffersfrominstabilitiesintraining. Indeed, asobserved max
learnI startingfromP T . Initiatinglearn-
in (Obukhov et al., 2022; Novikov et al., 2021), without D−l+1 D−l+1 D−l
ingwithanupsampledpriorstabilizesconvergencetothe
explicitregularization,theQTTrepresentationoftenfailsto
correctlocalorglobalminimum. Thelearningproceedsit-
improveupontheCPandTuckerrepresentationsinpractice,
erativelyuntilwereachthefullscaleI andaQTTapprox-
gettingeasilystuckinlocalminima. Tothisend,weintro- D
imationT . Throughoutthelearning,weadoptatrapezoid
duceaglobalupsamplingstrategyinspiredbythemultigrid D
structure(Oseledets,2011)forthetensortrainranks,where
method for partial differential equations (Bramble, 2019;
ranksincreasetoamaximum(formingthetrapezoid’sas-
Lubaschetal.,2018),illustratedinFigs.1-3andbelow.
cendingedge),remainconstant(trapezoid’stop),andthen
We use a specific Matrix Product Operator (MPO) – the decrease(itsdescendingedge). Theupsamplingstepofthe
prolongationoperatorP –whichperformslinearinterpo- algorithmisillustratedinFig.2,andthefullcoarse-to-fine
lation globally in QTT. Given a vector v n ∈ R2n (e.g., a learningschemeisillustratedinFig.3.
waveform),P isamatrixthatmapsv tov ∈R2n+1,
n n n+1 Fig. 1 outlines our training process in a single hierarchy
wherev isalinearinterpolantofv toalatticeoftwice
n+1 n level. Initially, we sample a batch of indices B, which
theresolution. P issimilartotheHaarwavelettransform.
n serves two purposes: 1. Conversion to a batch of QTT
Crucially,thematrixP n isanMPOwithafixedbonddi- indicesB˜ ∈RD,mappingcartesianpointstoQTTpoints
mension of 3. Therefore, it can be applied to a QTT via forretrievingvaluesYˆ fromtheQTT,2.UsingBtosample
localtensoroperations. TheresultingQTTwillhave(three Bˆ
targetinputvaluesfromI , obtainingY . Thesevalues,
times)largerbonddimension. Ifthemaximalbonddimen- Y andYˆ ,areusedtocalD culateameansB quarederrorloss,
sionexceedsthedesiredthreshold(R ), thentheQTT B Bˆ
max whichisthenbackpropagatedtoupdatetheQTTweights.
canbecompresseddowntoR viaalocalsingularvalue
max
decomposition. Fortwoorthree-dimensionalvisualdata,
weinterpolateineachdimensionbyapplyingP ⊗P ⊗P .
n n n
Fig.2demonstratestheapplicationofP inupsamplinga
4Coarse-To-FineTensorTrainsforCompactVisualRepresentations
4.2.NovelViewSynthesis TT-SVD and can only be applied when the direct signal
(e.g., 2Dimageor3Dvoxelgrid)isprovided, asitisap-
ApplyingPuTTtonovelviewsynthesisinvolvesmodeling
pliedanalytically. Itdoesnotapplywhenoptimizationis
afunctionthatmapsany3Dlocationxandviewingdirec-
required, such as in learning from missing data, or novel
tion d to a volume density σ and a view-dependent color
view synthesis, and does not perform well on noisy data.
c,supportingdifferentialraymarchingforvolumerender-
Whenreferringtoabaselineusingupsampling,wetrainina
ing(Mildenhalletal.,2020). Weusetwovoxelgrids: G
σ similarcoarse-to-fineapproachwherethetensorfactorsare
forvolumedensityandG forview-dependentcolor,simi-
c linearlyinterpolatedtoafinergridusingtheapproachde-
lartoTensoRF(Chenetal.,2022). G isasingle-channel
σ velopedbyTensoRF(Chenetal.,2022). Theterm“TT”in
gridencodingσvalues,whileG isamulti-dimensionalgrid
c ourcomparisonsreferstotrainingaQTTwithoutadopting
holdingappearancefeatures.Thesefeaturesaretransformed
acoarse-to-finelearningapproach.
intocolorvaluesusingashadingfunctionS,suchasasmall
MLPorSphericalHarmonics. Thecontinuousgrid-based Datasets For2D,weutilizetwohigh-resolutionimages:
RadianceFieldisexpressedas: “Girl With a Pearl Earring” (Vermeer, 1665) photograph,
and“Tokyogigapixel”(Dobson,2018),whicharecenter-
σ,c=G (x),S(G (x),d)
σ c croppedtoa16kresolution. Wealsoincludethree4kim-
ages: “Marseille” (Studio, 2023), “Pluto” (NASA/Johns
whereG (x)andG (x)areinterpolatedusingtrilinearin-
σ c
HopkinsUniversity,2023),and“Westerlund”(NASAand
terpolationwithinthe3Dvoxelgrid.
ESA,2023)fornoiseandmissingdataexperiments. For3D,
Thetensorrepresentation,G σ ∈RX×Y×Z,isa3-orderten- we utilize the “Flower” data (of Zurich, 2023), and John
sorforvolumedensity,andG c ∈RX×Y×Z×P isa4-order HopkinsTurbulencedataset(Lietal.,2008),whichconsists
tensorforcolorwithanadditionalfeaturevectordimension of a set of 3D voxel grids at 10243 resolution, providing
P. WefactorizethesegridsintoQTT-formattensorsT σ and adiverserangeofhigh-resolutionstructuresbydownsam-
T c,eachwithDcoresandeightphysicalindicespercore, plingtodifferentresolutions. Fornovelviewsynthesis,we
usingtheQTT-Blockstructure(Obukhovetal.,2022)for employtheBlender(Mildenhalletal.,2020)andNSVF(Liu
G ctohandletheadditionalfeaturedimensionP. et al., 2020a) datasets, comprising eight synthetic 3D
scenes at 800×800 resolution, alongside the TanksTem-
Fig. 4 outlines our reconstruction and rendering pipeline.
ples(Knapitschetal.,2017)dataset(1920×1080).
PointssampledalongarayaretransformedintoQTTindices
andusedtosamplefromT andT . Theinterpolatedvalues
σ c
5.1.Compression
arethenusedfordifferentiablevolumerendering,enabling
trainingviabackpropagation.
2DCompression Fig.5providesavisualcomparisonto
baselineswhileFig.6providesanumericalone,showcasing
5.Results PSNRresultsasafunctionofthecompressionratio,defined
astheratioofuncompressedparameterstocompressedpa-
WedemonstratetheapplicabilityofPuTTalongthreeaxes:
rameters (Appendix shows corresponding SSIM results).
(1). Compression,showingthatPuTTcanbeoptimizedto
Theoretically,QTTshavealogarithmicdependencyonthe
achieve the expressive power and compactness of tensor
imagesidelength,whileCPandTuckerhavelineardepen-
trains,(2). Learningfrommissingdata,showingthatPuTT
dence. However,thetensorrankgovernsexpressivity,and
canlearntointerpolatemissingdataeffectively,(3). Denois-
the tensor ranks of CP, Tucker, and QTT are not directly
ing,showingthatPuTTcanbetterrecovertheoriginalsignal
comparable;i.e. aQTTwithmaxrank50mightbefarmore
givennoisyinputs. Weevaluatetheseaxeswithrespectto
expressivethanaCPwithrank50. However,weclearlysee
the applications of 2D fitting, 3D fitting, and novel view
theimprovedefficiencyofQTTacrossdifferentresolutions,
synthesis. Lastly, we conduct an ablation study to under-
whichbecomesmorepronouncedastheresolutionincreases
scoretheimpactofindividualcomponents. Intheappendix
acrossallcompressionratios. At16kresolution,PuTTsig-
andsupplementarywebpage,weprovideadditionalresults
nificantlyoutperformsbaselines,showinganadvantageof
accompanyingexistingfiguresanddiscussthelimitations.
over2.5inPSNRand0.1inSSIM.PuTTperformsbetter
Baseline Methods We compare our method to state- than the analytical TT-SVD in all 2D scenarios in terms
of-the-arttensor-basedmethods: CP,Tucker,andtheVM of PSNR. For SSIM, PuTT consistently outperforms TT-
decomposition (Chen et al., 2022) (see details in Sec. 3 SVDupto8kresolution,withaminimumimprovementof
andappendix). Furthermore,weincludeacomparisonwith 0.05SSIMacrossallcompressionratios. At16kresolution,
TT-SVD(Oseledets,2011)(adaptedtoQTT,seeappendix TT-SVDexhibitsbetterSSIM,indicatingthechallengesin
fordetails). TT-SVDisadeterministicapproach,thatstarts capturingstructuralimagepropertiesastheresolutionand
fromthefulluncompressedtensorandcompressestheten- theratiobetweenbatchsizeandimagesizeincrease. The
sorintotensortrainformwithasinglepassovertheinput. CP,Tucker,andQTTrankswereadaptedtomatchthetarget
5Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Figure4.NovelviewsynthesiswithPuTT.ThisillustrationshowstheprocessusingtwoQTTs,T andT ,forcomputingvolumedensity
σ c
(σ)andview-dependentcolor(c)throughdifferentialraymarching. Foreach3Dlocationx = (x,y,z)andviewingdirectiond,x
isconvertedtoQTTindices. TheseindicesquerythedensityvoxelgridT andthecolorvoxelgridT . Theprocessusestrilinear
σ c
interpolationtoobtaincontinuousσvaluesandappearancefeaturevectorscˆ. Thesevectorsareprocessedthroughashadingmodule
togeneraterawcolorvaluesc,which,alongwithσ,areusedindifferentialvolumetricrendering. Therenderinglossiscomputedby
comparingthegeneratedcolorvaluesagainstgroundtruth(g.t.)values.
evaluatemodelsinthreesizes: Large(>60MB),Medium
PuTT TTNoUpsampling TuckerUpsampling CPUpsampling
PSNR26.3,SSIM0.72PSNR25.7,SSIM0.70PSNR21.8,SSIM0.54PSNR21.6,SSIM0.54 (12MB),andSmall(<8MB).Our12MBPuTTmodel,de-
spitehavingsixtimesfewerparameters,matchestheperfor-
manceoftheLargebaselinesontheNSVFandTanksTem-
plesdatasets. Moreover,our7MBPuTTmodeloutperforms
Small baselines by over one PSNR on the NSVF dataset.
Foradetailedoverviewoftheimplementation,pleasecon-
Figure5.Qualitativecomparisontobaselineson16kimages. sulttheappendix. InTab.2,forthe7MBsize,weconsidera
moredetailedcomparison,distinguishingbetweennearand
compressionratiosshowninthefigure. farviewsattesttime.Ourimprovementismorepronounced
atnearbyviews,indicatingourabilitytocapturefinedetails
3DCompression In3D,weobservesimilartrends, as
betterthanTensoRF.Fig.14demonstratesvisuallyourabil-
shown in the Fig. 7 (PSNR) and appendix (SSIM). How-
itytobettercapturefine-graineddetails. InAppendixGwe
ever, the dynamics differ notably among the PuTT, CP,
provideper-sceneresults.
and Tucker at various resolutions. Up to 2563, CP and
TuckerperformcloselytoPuTT.Yet,for10243,PuTTsig-
5.2.LearningfromIncompleteData
nificantly outshines the other methods, leading to more
than1PSNRimprovementandnearly0.005improvement WeevaluatethecapabilityofPuTTtolearn2Dor3Drepre-
in SSIM. VM’s performance is less substantial due to its sentationsbytrainingonlyonasubsetoftheavailabledata,
quadraticdependencyonthesidelength,unlikethelinear thus assessing its generalizability. We randomly select a
dependencyofCPandTuckerandthelogarithmicdepen- percentagep%ofthefullinputdataI .Thisyieldsasubset
D
dencyofQTT.VMshowsrelativelypoorperformance.This ofindices,I ,whichweuseexclusivelyfortraining. We
D,p
isattributabletothehighcompressionsettingofourexper- evaluatetheperformanceofthefullinputdata,including
iment, coupled with VM’s quadratic dependency on side untrainedindices. Totrainwithadown-sampledtargetat
length. PuTTiscomparabletodeterministicTT-SVD,but lowerresolutions,wecreateamodifiedtargetimageI .
D−l,p
achieving high SSIM is exacerbated at larger resolutions, WebuildI fromI ,weuseacustommaskedaver-
D−1,p D,p
underscoringtheincreaseddifficultyinmaintainingimage agepoolingmethod,averagingonlythenon-zerovaluesin
qualityathighercompressionsettings. Onemightspeculate eachwindow,sizedaccordingtothedownsamplingfactor.
thatthecrossoverinefficiency,whereitpaysofftousethe The resulting downsampled image, I , thus contains
D−l,p
QTTrepresentation,occursaroundsidelengthL=512. aggregated information within each patch of size 2l ×2l
of I . Any non-zero value in I represents aggre-
NovelViewSynthesis AsseeninTab.1,PuTToutper- D,p D−l,p
gateddatafromtheoriginaldatawithinthecorresponding
formsTensoRFwhenusinghighcompressions(7MBand
patch. ThisformsanewsetofindicesI ,fromwhichwe
12MB)forBlender,NSVF,andTanksTemplesdatasetsand l,p
sampleduringtrainingonthedownsampledinputI .
hascomparableperformanceonthelargemodelsize. We D−l,p
6Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Figure6. PSNR(y-axis)vscompressionratio(x-axis)for2Dfitting.“up=upsampling”.
Figure7. (a)PSNR(y-axis)incomparisontocompressionratio(x-axis)for3Dfitting.
(a) (b)
Figure8.PSNRandSSIMvsnumberofupsamplingstepsforfivedifferentpercentagesoftrainingdataavailable(a)orvsdifferent
amountsofavailabletrainingdataavailableforthreedifferentmodelswithorwithoutupsampling(b).”Up”referstoupsampling.
Input PuTT(Ours) CP Tucker TTw/oup. CPw/oup. Tuckerw/oup.
Figure9.Visualcomparisonoftrainingwithnoiselevels0.1(Top),0.5(Middle),,and1.0(Bottom).“W/oup=withoutupsampling”.
(a) (b)
Figure10.PSNR(a)andSSIM(b)whenvaryingtheamountofGaussianorLaplaciannoiseforPuTTandbaselines.“up=upsampling”.
7
esioN1.0
esioN5.0
esioN0.1Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Synthetic-NeRF NSVF TanksTemples
Method Steps Size(MB)↓ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑
PlenOctrees*(Yuetal.,2021b)-L 200k 1976.3 31.71 0.958 - - 27.99 0.917
Plenoxels*(Yuetal.,2021a)-L 128k 778.1 31.71 0.958 - - 27.43 0.906
DVGO*(Sunetal.,2022)-L 30k 612.1 31.95 0.957 35.08 0.975 28.41 0.911
K-planes*(Fridovich-Keiletal.,2023)-L 30k 122.1 32.36 0.962 - - - -
Instant-NGP*(Mu¨lleretal.,2022)-L 35k 45.5 33.18 0.963 - - -
TensoRF(Chenetal.,2022)VM-192*-L 30k 71.8 33.14 0.963 36.52 0.982 28.56 0.920
OurPuTT-600-L 80k 60.3 32.79 0.958 36.57 0.982 28.37 0.917
NeRF*(Mildenhalletal.,2020)-S 300k 5.0 31.01 0.947 30.81 0.952 25.78 0.864
TT-NF*(Obukhovetal.,2022)-S 80k 7.9 31.09 0.945 - - - -
TensoRFCP-384*-S 30k 3.9 31.56 0.949 34.48 0.971 27.59 0.897
TensoRFVM-30(noshrinking)-S 80k 6.9 31.10 0.948 34.57 0.974 27.67 0.898
OurPuTT-200-S 80k 6.7 31.66 0.956 35.58 0.976 27.99 0.901
TensoRFVM-48(noshrinking)-M 80k 12.0 31.62 0.952 35.44 0.976 28.03 0.901
OurPuTT-280-M 80k 12.0 31.95 0.957 36.04 0.977 28.15 0.905
Table1.ComparisontobaselinesforthetaskofnovelviewsynthesisfortheSynthetic-NeRF(Mildenhalletal.,2020),NSVF(Liuetal.,
2020a),andTanksTemples(Knapitschetal.,2017)datasets.Scoresofthebaselinemethodswitha∗aretakendirectlyfromcorresponding
paperswheneverpossible.L,MandSindicateaLarge(>60MB),Medium(12MB)orsmall(<8MB)modelsize.Wecompareour
methodonMediumandSmall-sizedmodelsbutalsoincludeLarge-sizedmodelsforreference.
(a) (b)
Figure11. PSNRandSSIMforPuTT,undervaryingiterationcounts(a)andvaryinglevelsofGaussiannoise(σ)(b).
Chair Ficus AllScenes
TensoRF PuTT TensoRF PuTT TensoRF PuTT
Near 35.59 37.12 32.53 33.95 31.93 32.93
Far 31.74 32.71 30.16 31.06 28.97 29.41
All 32.22 33.50 30.71 31.69 30.43 31.66
Table2.NovelviewsynthesiscomparisontoTensoRF(PSNR)on
theChair,Ficus,andallscenes(Blenderdataset(Mildenhalletal.,
Figure12.Varyingthenumberofupsamplingstepswhentraining
2020)). Wetestonviewsneartraining(Near),farfromtraining
99% missing data. The first image displays the training data,
(Far),andalltestviews(All).Seetheappendixforalldetails.
followedbyPuTTresultswith0,1,4,and7upsamplingsteps.
Fig.8(a)considerstheeffectofvaryingthenumberofup-
samplingsteps,whentrainingwithdifferenttrainingdata
percentages(seevisualexamplesinFig.12). Itillustrates
thesignificanceofupsamplingwhenlearningfromlimited
datasamples,highlightingitsgrowingimportanceastrain-
ingdatasizedecreases. Withoutupsampling, trainingon
just1%oftheinput,PuTTgetsaPSNRof1.87andSSIM
of0.0018. However,applyingsevenupsamplingstepsen-
hancestheresultstoaPSNRof28.73andSSIMof0.7349.
Figure13.VisualcomparisonofPuTTtobaselineswhentraining
with90%(Top)and99%(Bottom)missingdata. Similarimprovementsareobservedwith10%trainingdata.
8Coarse-To-FineTensorTrainsforCompactVisualRepresentations
stepsnotablyenhanceSSIMbecauseSSIMissensitiveto
globalfeatureslikeoverallmean,standarddeviation,and
luminance. Trainingondownsampledinputsusingtensor
trains, which involves gradient updates based on batches
covering larger portions of the input, seems to facilitate
learningtheseglobalcharacteristicsmoreeffectively.
Fig. 11(b) presents an ablation study on the ”Girl with a
PearlEarring”imageat4kresolutionandrank200, with
a fixed iteration count of 8192. We examine the impact
ofvaryingthenumberofupsamplingstepsonthemodel’s
performance,forvaryingdegreesofGaussiannoiseapplied
ontheinput. TheLHSofFig.11(b)illustratestheinfluence
Figure14.VisualcomparisonofasingleNeRFfor“drums”. ofthefirstupsamplingsteponPSNR,showinganotable
improvement. Subsequent upsampling steps result in in-
Fig.8(b)showsPuTTconsistentlysurpassesCPandTucker, cremental gains. The SSIM plot, on the RHS, reveals a
especiallyatlowerresolutions. Fig.13givesavisualexam- morepronouncedimprovementwitheachupsamplingstep,
plewhen90%and99%ofthedataismissingforPuTTand indicatingtheeffectivenessofupsamplinginenhancingthe
baselines,showingabetterrecoveryoftheinputimage. model’sabilitytolearnglobalfeatures.
Thesefindingsaresupportedbyadditionalexperimentsand
5.3.NoiseRemoval
analysesintheappendix. Sec.Chighlightsthestatistical
Weaimtolearnatensorrepresentationofatarget,denoted robustnessofourexperiments,showingminimalvariation
as X , using samples from its noisy counterpart, Xˆ , ob- withupsamplingcomparedtowithout. Sec.Eexaminesthe
I I
tainedbyaddingnoiseZ. Inourexperiments,Z issampled influenceofinitialization,demonstratingPuTT’sconsistent
fromeitheraNormalorLaplaciandistribution(seedetailsin performanceacrossdifferentinitializationvalues. Without
appendix). Attraining,weminimize||Xˆ −R ||2. Atinfer- upsampling,QTTtrainingissensitivetoinitializationstan-
I T 2
ence,wecomparethelearnedR withtheoriginalcleantar- darddeviation,withPSNRfluctuatingbetween34.958(std
T
getX . AsshowninFig.10(a),PuTTconsistentlyachieves 0.05)and12.320(std0.5)fora512×512RGBimage. In
I
higher PSNR than other tensor network methods across contrast,PuTT’sPSNRremainsstablebetween36.100and
variouslevelsofnoisesigma. Notably,theapplicationof 36.116forstdvaluesfrom0.001to0.5. Sec.Jshowshow
upsamplingnotonlyavoidsoverfittingtonoisysamplesbut rankincrementationstrategyinspiredbySlimmeRF(Yuan
alsoconsistentlyoutperformsthenon-upsamplingapproach. &Zhao,2023)canfurtherimproveperformance.
Fig.10(b)underscorestheclearbenefitsofemployingup-
sampling strategies. This is particularly evident in PuTT, 6.Conclusion
whereupsamplingyieldsanSSIMscoreimprovementof
morethan0.1fornoiselevelsexceeding0.2σ. Weproposedanovelcoarse-to-fine,Tensor-Train-basedrep-
resentationcalled‘ProlongationUpsamplingTensorTrain
Fig.9showsrecoveredimages,bothwithandwithoutup-
(PuTT)’andanassociatedoptimizationstrategy. Thisap-
sampling at Gaussian noise levels of σ = 0.05,0.5,1.0.
proachinvolvesprolongingor“upsampling”alearnedTen-
Theseexampleshighlighttheefficacyofupsamplinginmit-
sorTrainrepresentation,creatingasequenceoftensortrains
igatingnoise,demonstratingvisiblysuperiorresultscom-
that are incrementally refined in a coarse-to-fine manner.
paredtomodelsnotutilizingupsampling. Moreover,PuTT
Wedemonstratedtheapplicabilityofourmethodalongthe
showcases enhanced details and a more effective noise-
axesofcompression, learningfrommissingdata, andde-
filteringcapabilitybecauseofitshierarchicalstructure.
noising. Weevaluatedtheseaxeswithrespecttotheappli-
cationsof2Dfitting,3Dfitting,andnovelviewsynthesis
5.4.AblationStudy anddemonstratedstate-of-the-artperformancecomparedto
othertensor-basedmethods,especiallyinhigh-compression
Fig.11(a)illustratestheimpactofvaryingthenumberof
settings. Ourlearningschemeleveragestheimprovedcom-
upsamplingsandthenumberofiterationsonefficiency. The
pressionabilityandmulti-resolutionnatureoftheQuantized
results highlight the efficiency of our method: using just
TensorTrain(QTT)forimproveddenoisingandimagecom-
1024iterationswithfourupsamplingsteps,weachievecom-
pletioncapabilities. Infuturework,wehopetoapplyPuTT
parablePSNRandSSIMtotrainingwithoutupsamplingfor
tolarge-scaleNeuralRadianceFields(NeRFs)anddynamic
16kiterations. Beyond4kiterations,thereisnosignificant
neuralfields,utilizingthelogarithmicdimensionalityadvan-
improvementinquality. Withoutupsamplingthisplateau
tagesofQTTstorepresentlargeandfinelydetailedscenes.
isnotreachedevenafter32kiterations. Moreupsampling
9Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Acknowledgements SebastianLoeschckeissupportedby Dobson, T. 1.2 gigapixel panorama of shibuya in tokyo,
theDanishDataScienceAcademy,whichisfundedbythe japan. https://www.flickr.com/photos/
NovoNordiskFoundation(NNF21SA0069429)andVIL- trevor_dobson_inefekt69/29314390837,
LUMFONDEN(40516). SergeBelongieandDanWang 2018. Accessed: 17-11-2023.
are supported by the Pioneer Centre for AI, DNRF grant
Fridovich-Keil,S.,Meanti,G.,Warburg,F.,Recht,B.,and
numberP1. MJKacknowledgessupportfromtheCarlsberg
Kanazawa,A. K-planes: Explicitradiancefieldsinspace,
FoundationandtheNovoNordiskFoundation.
time,andappearance,2023.
ImpactStatement ThispapercontributestotheMachine
Harshman,R.A. FoundationsofthePARAFACprocedure:
Learningfieldbyenhancingvisualrepresentationlearning
Modelsandconditionsforan”explanatory”multi-modal
usingtensortrainoptimization. Giventhecurrentstageof
factoranalysis. UCLAWorkingPapersinPhonetics,16:
ourresearch,weidentifynospecificethicalissueswarrant-
1–84,1970.
ingexploration. Nonetheless,weacknowledgethedynamic
natureoftechnologicalimpactsandpledgetocontinually Holtz,S.,Rohwedder,T.,andSchneider,R. Thealternating
assesstheethicalimplicationsasourresearchadvances. linearschemefortensoroptimizationinthetensortrain
format. SIAM Journal on Scientific Computing, 34(2):
References A683–A713,2012.
Adelson,E.H.,Anderson,C.H.,Bergen,J.R.,Burt,P.J., Hubig,C.,McCulloch,I.P.,andSchollwoeck,U. Generic
andOgden,J.M. Pyramidmethodsinimageprocessing. constructionofefficientmatrixproductoperators. Phys.
RCAengineer,29(6):33–41,1984. Rev.B,95:035129,Jan2017. doi: 10.1103/PhysRevB.95.
035129. URLhttps://link.aps.org/doi/10.
Antonini,M.,Barlaud,M.,Mathieu,P.,andDaubechies,I.
1103/PhysRevB.95.035129.
Image coding using vector quantization in the wavelet
transformdomain. InInternationalConferenceonAcous- Khoromskij,B.N.o(dlogn)-quanticsapproximationofn-d
tics,Speech,andSignalProcessing,pp.2297–2300vol.4, tensorsinhigh-dimensionalnumericalmodeling. Con-
1990. doi: 10.1109/ICASSP.1990.116036. structiveApproximation,34:257–280,2011.
Bramble,J.H. Multigridmethods. ChapmanandHall/CRC, Kingma,D.P.andBa,J. Adam: Amethodforstochastic
2019. optimization. arXivpreprintarXiv:1412.6980,2014.
Carroll, J. D. and Chang, J. J. Analysis of individual Klu¨mper, A., Schadschneider, A., and Zittartz, J. Matrix
differences in multidimensional scaling via an n-way productgroundstatesforone-dimensionalspin-1quan-
generalization of “eckart-young” decomposition. Psy- tum antiferromagnets. Europhysics Letters, 24(4):293,
chometrika, 35:283–319, 1970. URLhttps://api. 1993.
semanticscholar.org/CorpusID:50364581.
Knapitsch,A.,Park,J.,Zhou,Q.-Y.,andKoltun,V. Tanks
Chan,E.R.,Monteiro,M.,Kellnhofer,P.,Wu,J.,andWet- andtemples: benchmarkinglarge-scalescenereconstruc-
zstein,G. pi-gan: Periodicimplicitgenerativeadversarial tion. ACMTrans.Graph.,36(4),jul2017. ISSN0730-
networksfor3d-awareimagesynthesis.InProceedingsof 0301. doi: 10.1145/3072959.3073599. URL https:
theIEEE/CVFconferenceoncomputervisionandpattern //doi.org/10.1145/3072959.3073599.
recognition,pp.5799–5809,2021.
Kolda,T.G.andBader,B.W. Tensordecompositionsand
Chan, E. R., Lin, C. Z., Chan, M. A., Nagano, K., Pan, applications. SIAMreview,51(3):455–500,2009.
B.,DeMello,S.,Gallo,O.,Guibas,L.J.,Tremblay,J.,
Li, Y., Perlman, E., Wan, M., Yang, Y., Meneveau, C.,
Khamis,S.,etal. Efficientgeometry-aware3dgenerative
Burns, R., Chen, S., Szalay, A., and Eyink, G. A pub-
adversarialnetworks. InProceedingsoftheIEEE/CVF
licturbulencedatabaseclusterandapplicationstostudy
ConferenceonComputerVisionandPatternRecognition,
lagrangian evolution of velocity increments in turbu-
pp.16123–16133,2022.
lence. Journal of Turbulence, 9:N31, jan 2008. doi:
Chen, A., Xu, Z., Geiger, A., Yu, J., andSu, H. Tensorf: 10.1080/14685240802376389. URL https://doi.
Tensorialradiancefields,2022.URLhttps://arxiv. org/10.1080%2F14685240802376389.
org/abs/2203.09517.
Lindell,D.B.,VanVeen,D.,Park,J.J.,andWetzstein,G.
Crowley, J. L. and Stern, R. M. Fast computation of the Bacon: Band-limitedcoordinatenetworksformultiscale
differenceoflow-passtransform. IEEEtransactionson scenerepresentation. InProceedingsoftheIEEE/CVF
patternanalysisandmachineintelligence,1(2):212–222, conferenceoncomputervisionandpatternrecognition,
1984. pp.16252–16262,2022.
10Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Liu, L., Gu, J., Lin, K. Z., Chua, T.-S., and Theobalt, C. Obukhov,A.,Usvyatsov,M.,Sakaridis,C.,Schindler,K.,
Neuralsparsevoxelfields. NeurIPS,2020a. andVanGool,L. Tt-nf: Tensortrainneuralfields,2022.
URLhttps://arxiv.org/abs/2209.15529.
Liu, L., Gu, J., Lin, K. Z., Chua, T.-S., and Theobalt, C.
Neuralsparsevoxelfields. NeurIPS,2020b. Oechsle,M.,Peng,S.,andGeiger,A. Unisurf: Unifying
neural implicit surfaces and radiance fields for multi-
Lombardi, S., Simon, T., Saragih, J., Schwartz, G.,
view reconstruction. In Proceedings of the IEEE/CVF
Lehrmann,A.,andSheikh,Y. Neuralvolumes: Learning
InternationalConferenceonComputerVision,pp.5589–
dynamicrenderablevolumesfromimages.arXivpreprint
5599,2021.
arXiv:1906.07751,2019.
ofZurich,U. Computer-assistedpaleoanthropologygroup
Lubasch, M., Moinier, P., and Jaksch, D. Multigrid
and visualization and multimedia lab. University of
renormalization. Journal of Computational Physics,
Zurich, 2023. URL https://www.ifi.uzh.ch/
372:587–602, nov 2018. doi: 10.1016/j.jcp.2018.
en/vmml/research/datasets.html. We ac-
06.065. URL https://doi.org/10.1016%2Fj.
knowledge the Computer-Assisted Paleoanthropology
jcp.2018.06.065.
groupandtheVisualizationandMultiMediaLabatUni-
Mallat, S. G. A theory for multiresolution signal decom- versity of Zurich (UZH) for the acquisition of the CT
position: thewaveletrepresentation. IEEEtransactions datasets.
onpatternanalysisandmachineintelligence,11(7):674–
693,1989. Oru´s, R. Tensornetworksforcomplexquantumsystems.
NatureReviewsPhysics,1(9):538–550,2019.
McCulloch,I. Infinitesizedensitymatrixrenormalization
group,revisited. arxiv:0804.2509,2008. URLhttps: Oru´s, R. A practical introduction to tensor networks:
//arxiv.org/abs/0804.2509. Matrix product states and projected entangled pair
states. Annals of Physics, 349:117–158, 2014. ISSN
Mildenhall,B.,Srinivasan,P.P.,Tancik,M.,Barron,J.T.,
0003-4916. doi: https://doi.org/10.1016/j.aop.2014.06.
Ramamoorthi,R.,andNg,R. Nerf: Representingscenes
013. URLhttps://www.sciencedirect.com/
as neural radiance fields for view synthesis. CoRR,
science/article/pii/S0003491614001596.
abs/2003.08934,2020. URLhttps://arxiv.org/
abs/2003.08934. Oseledets,I. Approximationofmatriceswithlogarithmic
numberofparameters. DokladyMathematics,80:653–
Mu¨ller, T., Evans, A., Schied, C., and Keller, A. Instant
654,102009. doi: 10.1134/S1064562409050056.
neural graphics primitives with a multiresolution hash
encoding. ACMTrans.Graph.,41(4):102:1–102:15,July Oseledets, I. and Tyrtyshnikov, E. Tt-cross approx-
2022. doi: 10.1145/3528223.3530127. URL https: imation for multidimensional arrays. Linear Alge-
//doi.org/10.1145/3528223.3530127. bra and its Applications, 432(1):70–88, 2010. ISSN
0024-3795. doi: https://doi.org/10.1016/j.laa.2009.07.
NASA and ESA. Westerlund 2 - nasa and esa,
024. URLhttps://www.sciencedirect.com/
a. nota (esa/stsci), and the westerlund 2 science
science/article/pii/S0024379509003747.
team. https://hubblesite.org/contents/
news-releases/2020/news-2020-15, 2023.
Oseledets,I.V. Tensor-traindecomposition. SIAMJournal
Accessed: 17-11-2023.
on Scientific Computing, 33(5):2295–2317, 2011. doi:
10.1137/090752286. URL https://doi.org/10.
NASA/Johns Hopkins University. True colors of pluto
1137/090752286.
- from applied physics laboratory/southwest research
institute/alex parker. https://science.nasa.
Pan,F.andZhang,P. Simulationofquantumcircuitsusing
gov/resource/true-colors-of-pluto/
thebig-batchtensornetworkmethod. PhysicalReview
?category=planets/dwarf-planets_pluto,
Letters,128(3):030501,2022.
2023. Accessed: 17-11-2023.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Niemeyer,M.andGeiger,A. Giraffe: Representingscenes
Chanan,G.,Killeen,T.,Lin,Z.,Gimelshein,N.,Antiga,
ascompositionalgenerativeneuralfeaturefields. InPro-
L.,Desmaison,A.,Kopf,A.,Yang,E.,DeVito,Z.,Raison,
ceedingsoftheIEEE/CVFConferenceonComputerVi-
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
sionandPatternRecognition,pp.11453–11464,2021.
Bai, J., and Chintala, S. Pytorch: An imperative style,
Novikov,G.S.,Panov,M.E.,andOseledets,I.V. Tensor- high-performance deep learning library. In Advances
traindensityestimation. InUncertaintyinartificialintel- inNeuralInformationProcessingSystems32,pp.8024–
ligence,pp.1321–1331.PMLR,2021. 8035.CurranAssociates,Inc.,2019.
11Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Perez-Garcia,D.,Verstraete,F.,Wolf,M.M.,andCirac,J.I. Yang,G.,Benaim,S.,Jampani,V.,Genova,K.,Barron,J.,
Matrixproductstaterepresentations,2007. Funkhouser, T., Hariharan, B., and Belongie, S. Poly-
nomialneuralfieldsforsubbanddecompositionandma-
Sitzmann,V.,Thies,J.,Heide,F.,Nießner,M.,Wetzstein,
nipulation. AdvancesinNeuralInformationProcessing
G.,andZollhofer,M. Deepvoxels: Learningpersistent
Systems,35:4401–4415,2022.
3dfeatureembeddings. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition, Yu,A.,Fridovich-Keil,S.,Tancik,M.,Chen,Q.,Recht,B.,
pp.2437–2446,2019. and Kanazawa, A. Plenoxels: Radiance fields without
neuralnetworks. CoRR,abs/2112.05131,2021a. URL
Stoudenmire, E. M. The tensor network, 2021. URL https://arxiv.org/abs/2112.05131.
https://tensornetwork.org/mpo. Supportfor
theTensorNetworkisprovidedbytheFlatironInstitute, Yu,A.,Li,R.,Tancik,M.,Li,H.,Ng,R.,andKanazawa,
partoftheSimonsFoundation. A. Plenoctreesforreal-timerenderingofneuralradiance
fields. CoRR, abs/2103.14024, 2021b. URL https:
Studio, O. . Aerial drone view of urban build- //arxiv.org/abs/2103.14024.
ings from top. https://www.pexels.com/
@orbital101studio/, 2023. Accessed: 17-11- Yuan,S.andZhao,H. Slimmerf:Slimmableradiancefields,
2023. 2023.
Sun,C.,Sun,M.,andChen,H. Directvoxelgridoptimiza-
tion: Super-fast convergence for radiance fields recon-
struction. InCVPR,2022.
Szeliski,R. Computervision: algorithmsandapplications.
SpringerNature,2022.
Tucker,L.R. Somemathematicalnotesonthree-modefac-
toranalysis. Psychometrika,31(3):279–311,Sep1966.
ISSN 1860-0980. doi: 10.1007/BF02289464. URL
https://doi.org/10.1007/BF02289464.
Usvyatsov, M., Ballester-Ripoll, R., and Schindler, K.
tntorch: Tensornetworklearningwithpytorch,2022.
Vermeer, J. Girl with a Pearl Earring. https:
//commons.wikimedia.org/wiki/Category:
Girl_with_a_Pearl_Earring_by_
Johannes_Vermeer, 1665. Accessed: 17-11-
2023.
Vidal, G. Entanglement renormalization. Physical Re-
viewLetters,99(22),nov2007. doi: 10.1103/physrevlett.
99.220405. URL https://doi.org/10.1103%
2Fphysrevlett.99.220405.
White,S.R. Densitymatrixformulationforquantumrenor-
malizationgroups. Physicalreviewletters,69(19):2863,
1992a.
White,S.R. Densitymatrixformulationforquantumrenor-
malizationgroups. Physicalreviewletters,69(19):2863,
1992b.
Xie,Y.,Takikawa,T.,Saito,S.,Litany,O.,Yan,S.,Khan,
N.,Tombari,F.,Tompkin,J.,Sitzmann,V.,andSridhar,
S. Neuralfieldsinvisualcomputingandbeyond. InCom-
puterGraphicsForum,volume41,pp.641–676.Wiley
OnlineLibrary,2022.
12Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Thisappendixmaterialcomplementsthemainpaperbyprovidingdetailedexplanationsandextendedanalyses.
Sec.Adiscussesthesupplementarywebpage. InSec.B,wediscussthelimitationsofourapproachanditsconstraintsand
boundaries. InSec.C,wediscussthestatisticalrobustnessofourexperiments. InSec.D,wegivedetaileddescriptionsof
varioustensornetworkmethods,theircomplexities,andspecificadaptationsforourstudy. Sec.E,wediscusstheinfluence
ofinitializationonmodelperformance. InSec.F,weprovideinsightsintoourexperimentalframework,includingbatch
sizes, learningratestrategies, andhandlingofnoiseandincompletedata. InSec.G,weprovideadditionalnovelview
synthesisresults. InSec.H,weexplainthecompatibilityofTensoRF’s”shrinkage”withQTT.InSec.I,weexplorethe
inpaintingcapabilitieswithPuTT.InSec.J,weexplorerankincrementationwithSlimmeRF(Yuan&Zhao,2023).
A.SupplementaryWebpage
Additionalsupplementarymaterialsandtheassociatedcodecanbeaccessedathttps://sebulo.github.io/PuTT_
website/.
B.Limitations
InourcompressionresultsinSec.5.1ofthemaintext,weobservecertainconstraintsassociatedwithourPuTTmethod.
WhilelearningQTTrepresentationsusingPuTTshowamarkedadvantageinscenariosrequiringhighcompression,as
evident in our results for 2D and 3D data (Figures 6, 7) and in Novel View Synthesis (Tab. 1 of the main text), their
effectivenesstendstoconvergewithothermethodslikeCP,Tucker,andVMinlesscompressedsettingswithsmallertensor
sizes. Forexample,inthecaseof1kresolutionimages(referencedinSec.5.1,Fig.6ofthemaintext),wenotedthatthe
performancebenefitsofusingQTTswerelesspronounced. ThissuggeststhatwhileQTTsarehighlyeffectiveinhandling
large-scale,high-compressiontasks,theiradvantagesarelesssignificantinsimplerscenarioswithlowercompression.
C.SensitivityandStatisticalAnalysis
In our experimental framework, we tested each configuration using three distinct seeds to ensure the reliability and
consistencyofourresults. Weuseerrorbarsrepresenting±1standarddeviationfromthemeaninallplots. Inthecontextof
our2Dand3Dexperiments,it’snoteworthythattheerrorbarsareoftensominimal(seeSec.5.1ofthemaintext,Fig.6)
thattheymightbechallengingtodiscernintheplots. Thisobservationpointstoalowdegreeofvariation,suggestingahigh
levelofreproducibilityinourresults. Suchconsistencyisparticularlyevidentinexperimentsinvolvingupsampling,wherea
consistenttrendtowardssimilarendpointsintrainingwasobserved. Thispatterncanbeattributedtotheprogressivenature
ofourapproach,whichsystematicallybuildstherepresentationonincreasinglyfinergrids. Furthermore,whencomparing
theperformanceofourPuTTmethodwithotherestablishedmethodslikeTucker,CP,andVM,weobservedlessvariationin
PuTT’sperformanceacrossdifferentexperimentalsetups. Thisreducedvariability,especiallyvisibleinthecompression
plots,indicatesthatPuTTnotonlyensuresamorestableandpredictableperformancebutalsounderscoresitseffectiveness
indifferentexperimentalconditionsandsettings.
C.1.EnhancingTrainingStabilitythroughUpsampling
Figure15showsanexperimentfurtherillustratingthestabilizingeffectofourupsamplingstrategywithinthecontextof
2Dcompressiontasks. Theexperimentanalysesthe”GirlwithPearlEarring”and”Tokyo”images,bothat4Kresolution,
todrawcomparisonsbetweenourupsamplingapproachandabaselineQTTmodelwithnoupsampling. Thefindings,
visualizedinourplot,revealtheaveragePSNRagainstthenumberofiterations,withtheshadedareadepictingthevariance
within±1standarddeviationacrossthreeseparateruns. Thiscomparisonunderscorestheenhancedstabilityoftrainingwith
ourupsamplingmethod,evidencedbyreducedperformancefluctuationsandconsistentlyhigherPSNRlevelscomparedto
modelswithoutupsampling.
D.TensorNetworks
Wedescribethethreemainfamiliesoftensordecompositionsthatareconsideredinthiswork: CP/VM,andTucker. Tensor
TrainsandthespecificQTTformataredefinedinthemainarticle. WeprovideadditionalinformationonMPOsandthe
generalcomplexityofmanipulatingtensornetworks.
13Coarse-To-FineTensorTrainsforCompactVisualRepresentations
(a)Girl4K (b)Tokyo4K
Figure15.Thetwofigurespresentplotscorrespondingtothe”GirlwithPearlEarring”(15a)and”Tokyo”4Kimages(15b),showcasing
theperformanceofourPuTTmethodwithupsamplingversusabaselineQTTmodelwithoutupsampling.Forbothimages,weplotthe
averagePSNRover4000iterations,averagedacrossthreeseeds.Theshadedregionsaroundeachcurverepresentthevariabilitywithin
±1standarddeviationfromthemeanPSNR,servingasanindicatoroftrainingstability.Inbothplots,itisevidentthattheupsampling
modelexhibitslowerfluctuationinstandarddeviationandachieveshigherPSNRlevelsbytheendofthetrainingperiod.Thishighlights
theimprovedstabilityandeffectivenessofourupsamplingapproachin2Dcompressiontasks.
Thecanonicalpolyadic(CP)decompositions(Carroll&Chang,1970;Harshman,1970) factorizesatensorT ∈
Rn1×n2×···nd intoasumofrank-1components:
R
(cid:88)
T = v1⊗v2⊗···⊗vd (2)
r r r
r=1
wherev1,v2,··· ,vdarerank-1vectorsinRn1,··· ,Rnd. Eachtensorelementisthenasumofscalarproducts:
r r r
R d
(cid:88)(cid:88)
T = vk , (3)
j1···jd r,jk
r=1k=1
wherej =1,...,n foreachk =1,...,d. Wewillmostlyconsiderd=2,3inthiswork,aswecareaboutvisualdata.
k k
TheVectorMatrix(VM)decomposition(Chenetal.,2022) isaspecific3Dtensordecomposition,introducedtoremedy
a”toohigh”compactnessoftheCPdecomposition. Insteadofpurelyusingvectorfactors,theVMdecompositionfactorizes
atensorintovectorsandmatrices. ForT ∈Rn1×n2×n3,
(cid:88)R1 (cid:88)R2 (cid:88)R3
T = v1⊗M2,3+ v2⊗M1,3+ v3⊗M1,2 (4)
r r r r r r
r=1 r r=1
wherethesuperscriptonthevectorsvandmatricesM indicatethespaceonwhichtheyact. TheVMdecompositioncanbe
understoodasahalf-waybetweenthedense(bare)representationofT andtheCPdecomposition. Assuch,VMhasreduced
compressionascomparedwithCP.TheoverallrankoftheVMdecompositionisequaltothetotalnumberofcomponents
(R +R +R ).
1 2 3
The Tucker decomposition (Tucker, 1966) provides only a partial compression of a tensor. Given a tensor T ∈
Rn1×n2×···nd,wedefineareducedtensorK∈Rm1×m2×···md andmatricesUk ∈Rmk×nk,suchthat
(cid:88)m1 (cid:88)md
T = ··· K U1 ···Ud . (5)
j1,...,jd i1,...,id i1,j1 id,jd
i1=1 id
TheTuckerdecompositionismostlyusefulinsettingswheredissmall,andn arelarge.
k
14Coarse-To-FineTensorTrainsforCompactVisualRepresentations
MatrixProductOperators Amatrixproductoperator(MPO)isatensornetworkthatdescribesafactorizationofatensor
withdinputanddoutputindicesintoalinearnetworkofsmallertensors(Hubigetal.,2017;McCulloch,2008). Eachcore
(exceptforthefirstandlast)hastwophysicalindicesandtwovirtualindices.
ThedifferencebetweenTTsandMPOsisthenatureoftheobjectstheyaremeanttorepresent. ATTcanbeseenasa
parameterizationofalargevectorinahigh-dimensionalspace,providingacompressedyetexpressiverepresentationof
suchvectors. The”operator”partoftheterm”MatrixProductOperator”referstothemathematicaloperationofamatrix
actingonavector(Stoudenmire,2021),i.e.,theMPOoperatesonaTT.Thisessentiallyallowsonetoefficientlyrepresent
andworkwithlargeoperatorsinhigh-dimensionalspaces,workingdirectlyinthecompressedTTrepresentation.
UsingthesameindexnotationasforTTs,anMPOisatensornetworkoftheform:
Mj1,...,jd =Ai1,j1Ai2,j2...Aid,jd, (6)
i1...id
whereAik,jk areR k×R k+1matricesforeachpairi k,j k. MPOshaveimportantapplicationsinrepresentinglarge,sparse
matricesinaformthatisconvenientforTTalgorithms(White,1992a;Holtzetal.,2012;Lubaschetal.,2018). Theyplaya
centralroleinTTalgorithmsinPhysics,yethavebeenunder-exploitedinthedatasciencesetting. Hereweshowhowto
leverageMPOsinthevisualdatasettinginconnectionwithmulti-resolutionanalysis.
ComplexityoftheCP,VM,Tucker,andTTdecompositions Givenatensorwithdindicesofmaximaldimensionn,CP
decompositionstoresdR rank-1tensorsofsizen,resultinginaspacecomplexityofO(dnR ). Here,R istheCP
CP CP CP
rank. In3D,theVMdecompositionstoresR timesasizenvectorandan×nmatrix,resultinginaspacecomplexity
VM
O(R (n+n2)). TheTuckerdecompositionstoresatensorKwithmaximalindexrangemanddmatricesofsizen×m
VM
resultinginaspacecomplexityO(dmd+1n). Finally,thetensortrainstoresndmatricesofsizeR2 ,resultinginaspace
TT
complexityO(ndR2 ).Inpractice, thecomplexityislower, asonecanfurtherexploitthetensornetworkstructuresto
TT
reducetheindividualranksinthenetwork.
Ifdislarge-typicallyd>3-thenweexpectthememorycosttoscaleasTucker,CP≫TT.Whiletheseparationbetween
TuckerandTTisobvious,sinceTuckerscalesexponentiallyind,theseparationbetweenCPandTTiscontainedintherank
dependence. Forfixedaccuracy,weexpectTTtorequireaconstantrankR ,independentofd. WhiletheCPrankR
TT CP
willgrowuptoexponentiallyind(Kolda&Bader,2009).
However,withvisualdata,wehaved≤4,hencetherelationshipsaremoresubtleanddependcruciallyontherepresen-
tationalpowerofeachdecomposition;i.e. whatranksarenecessarytorepresentvisualdatauptoagivenprecision,and
whethertheserepresentationscanbelearnedinanefficientandreliablemanner. IntheQTTrepresentation,thecomplexity
isO(2dlog(n)R2).
Insummary,oneshouldexpectthatford=2,3,4andlargen(i.e. highresolution),theQTTformatshouldoutperformCP,
VM,andTucker,withthegapgrowinguptoexponentiallyasdandngrow.
ComputationalComplexityofProlongation: ForatensorT ofdimensionsDN,whereN =2typicallyrepresentsthe
dimensionsofanimage(heightandwidth),theupperboundforthecomputationalcomplexityinvolvedinmultiplyinga
MatrixProductOperator(MPO)withaQuantizedTensorTrain(QTT)isdenotedbyO(log(D)·max(R,S)3·ND). In
thisexpression,(log(D))denotesthenumberofcoresintheQTT,whichcorrespondstothelevelofquantization. (R)and
(S)arethemaximumrankswithintheQTTandMPO,respectively. Thisoperationisefficientlyexecutedanddoesnot
significantlyinfluencetheoverallruntime.
Implementation When learning tensor networks such as Tucker, CP, TT, QTT, and VM, especially in challenging
scenarioswithnoisy,incompletedataorinapplicationslikeNeRFwherethefullscenetensorisnotaccessible,itbecomes
essentialtoadoptasample-basedtrainingapproach. UnliketraditionalanalytictechniquessuchasHigher-OrderSingular
ValueDecomposition(HOSVD)(Tucker,1966)forTuckerdecomposition,orSingularValueDecomposition(SVD)and
QRdecompositionsforTT(Oseledets,2011)andQTT(Khoromskij,2011),ourmethodologydoesnotrelyonthesedirect
decompositionmethods. Instead,wecommencewithaninitialskeletontensorthatmirrorsthestructurerequiredbythe
chosendecompositionscheme. Thisskeletonservesasastartingpointforaniterativeoptimizationprocess,wherethetensor
valuesarerefinedusingstochasticgradientdescent. Thisapproachallowsforaflexibleandefficientadaptationofthetensor
factorizationmethodstotheintricaciesandlimitationsofthedataandapplicationcontext,suchasthoseencounteredin
NeRFscenarios.
15Coarse-To-FineTensorTrainsforCompactVisualRepresentations
D.1.MultiresolutionFormatofQuantizedTensorTrain
Themultiresolutionformat,asexplainedinSec.3,isillustratedinFig.16.
Figure16.This figure illustrates the transformation of a 16x16 matrix into QTT format, showcasing a transition from a standard
gridrepresentationtoamixedfactororder. Initiallypresentedasasimple16 by16 matrix,itisrestructuredintofactorsoftwo:
x y
(2 × 2 ×2 ×2 )combinedwith(2 ×2 ×2 ×2 ). Subsequently,thefactorsarepermutedintoamixedfactororder
1x 2x 3x 4x 1y 2y 3y 4y
(2 ×2 )× (2 ×2 )×(2 ×2 )×(2 ×2 ). This results in a multiresolution format where the initial core’s indices
1x 1y 2x 2y 3x 3y 4x 4y
correspondtoindexingintofourquadrantsoftheinputmatrix.Specifically,index0inthefirstcoreisassociatedwiththetop-leftquadrant
ofthegrid.Subsequentcoresprogressivelyexamineincreasinglydetailedsegmentsofthematrix.Thishierarchicalbreakdownallowsthe
QTTtocaptureandrepresentdataatvariouslevelsofgranularity,startingfrombroad,overarchingstructuresdowntomoreintricate
detailswithinthegrid,whichisreminiscentofawavelet.
D.2.Theprolongationoperators
WeemployaMatrixProductOperator(MPO)knownasaprolongationoperatorP,suggestedbyLubachetal.(Lubasch
etal.,2018),toupsamplecoarsesolutionsintheQuantizedTensorTrain(QTT)format.
Consideraone-dimensionalvectorv ∈R2d representingdatalikeacontinuousfunction. Toachieveafinerrepresentation
d
v ∈R2d+1,welinearlyinterpolatebetweenadjacentpointsinv usingthematrixP . Thismatrixresemblesthe
d+1 d 2d→2d+1
Haartransformmatrix. Forinstanceford=2,wecanexpressP as:
2d→2d+1
 
.5 0 0 0
1 0 0 0
 
.5 .5 0 0
 
0 1 0 0
P 4→8 = 0 .5 .5 0 , (7)
 
0 0 1 0
 
0 0 .5 .5
0 0 0 1
16Coarse-To-FineTensorTrainsforCompactVisualRepresentations
ThismatrixiscloselyrelatedtotheHaartransformmatrix. ThematrixP canbewrittenasanMPOP withbond
2d→2d+1
dimensiontwoas
Pi1,...,in,in+1 =P[1]j1,i1···P[d]jd,idP[d+1]in+1. (8)
j1,...,jn
Themiddled−1tensorsintheMPO,P[k]j αk k, ,i αk
k+1
havetwophysicalindicesj k,i k andtwovirtualindicesα k,α k+1. Each
indexi k,j k,α k ∈[0,1]. ThefirsttensorP[1]j α1 1,i1 hasonevirtualindexα 1 andtwophysicalindicesj 1,i 1,whilethelast
tensorP[d+1]id+1 hasonevirtualindexα andonephysicalindexi . Theentriesaregivenexplicitlyas:
αd d d+1
P[1]0,0 = P[1]1,1 =P[1]1,0 =1
0 0 1
P[k]0,0 = P[k]0,1 =P[k]1,1 =P[i]1,0 =1,
0,0 1,0 0,1 1,1
for1<k <d+1
1
P[d+1]0 = 1, andP[d+1]1 =P[d+1]1 = ,
0 0 1 2
withallotherentriesbeingzero(Lubaschetal.,2018)1.
ThepreviouslydescribedprolongationoperatorP appliestotensortrainrepresentationsofone-dimensionaltensorsinQTT
format. Inhigherdimensions,theprolongationoperatorissimplythetensorproductoftheone-dimensionaloperatorson
(cid:78) (cid:78) (cid:78)
eachdimension: P P for2-dimensionsandP P P for3-dimensions.
D.3.QuantizedTensorTrainInitialization
WeinitializetheTT-ranksbetweeneachcoretofollowatrapezoidstructurelikefirstintroducedbyOseledetsetal.(Oseledets,
2011). Inatrapezoidstructure,theTT-ranksmayincreaseuptoacertainpoint(creatingtherisingedgeofthetrapezoid),
then stay constant, and then decrease (creating the falling edge of the trapezoid). The top (flat part) of the trapezoid
correspondstothemaximumrankamongallcores,whichiswhatwecalltheTT-rank.
Thisstructurecanbebeneficialbecauseitallowsrepresentingahigh-dimensionaltensorinacompressedformwhichstill
maintainsagoodapproximationoftheoriginaltensor(Oseledets,2011).
TocomputetheTT-ranksinthecontextofthetrapezoidstructure,consideranarraycontainingtheDphysicalindicesofa
TTn=[n ,n ,...,n ]. TheTT-ranksarederivedasfollows:
1 2 D
1. Computethecumulativeproductofthetensorshapefromlefttorightandfromrighttoleft,denotedasranks left
andranks rightrespectively:
i
(cid:89)
ranks left[i]= n , ∀i=1,...,D
j
j=1
D
(cid:89)
ranks right[i]= n , ∀i=1,...,D
j
j=i
2. Then,calculatetheTT-ranksbytakingtheminimumoftheranks leftandranks rightateachposition:
ranks tt[i]=min(ranks left[i],ranks right[i]),
∀i=1,...,D
3. Finally,ifarank,max rank,isspecified,ensurethatnoindexrankexceedsthisvalue:
ranks tt[i]=min(ranks tt[i],max rank),
∀i=1,...,D
1Notethedifferenceinconventioninindexnotationascomparedto(Lubaschetal.,2018)
17Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Fromthis,itfollowsthatifthepayloaddimensionisgreaterthan1,thenwehavetoscalethedimensionsofeachcorefrom
thelefttotherightuntilwereachtheranktopofthetrapezoidstructureoftheTT.
(cid:18) (cid:19)
σˆ =exp 1 (cid:0) 2logσ−ΣD logR (cid:1) (9)
2D i=1 i
D.4.TT-SVDBaseline
TheTensorTrain-SingularValueDecomposition(TT-SVD)algorithm,asdetailedbyOseledetsetal.(Oseledets,2011),
offersamethodfordecomposinghigh-dimensionaltensorsintotensortrains. Itsequentiallyreshapesandappliesmatrix
SVDtoeachtensormode,transformingahigh-dimensionaltensorintoaseriesofinterconnectedlower-dimensionaltensors,
effectivelyformingatensortrain. Foranin-depthunderstandingofTT-SVD,Oseledetsetal.’sworkprovidescomprehensive
details.
AsalsopointedoutinSec.5ofthemaintext,TT-SVDhaslimitations,particularlyinitsapplicabilityonlywhenthedirect
signal(e.g.,a2Dimageor3Dvoxelgrid)isprovidedforananalyticalapplication. Itfallsshortinscenariosrequiring
optimization,suchaslearningfromincompletedata,novelviewsynthesis,orhandlingnoisydata. Anotherlimitationof
TT-SVDisitsinfeasibilitywhentheinputtensoristoolargeformemory.
Inourwork,TT-SVD,implementedviatheTNTorchframework(Usvyatsovetal.,2022),servesasabaselinetodetermine
whetherourProlongationUpsamplingTensorTrain(PuTT)methodcanescapelocalminimawithoutspecifichyperparame-
ters. ThisbenchmarkingaidsinassessingPuTT’seffectivenessinhandlinglarge-scaletensordataanditsabilitytonavigate
thechallengesofoptimization. Inourimplementation,weadaptTT-SVDfortheQTTformat. Thisadaptationinvolves
initiallyreshapingtheinputtensorintolog(N)factorsof2,rearrangingthesefactorstoenforceZ-ordercorrelationbetween
spatialdimensions,andthenapplyingTT-SVD.ThisprocessalignswithhowwetypicallyconstructthespecializedQTT
structure.
D.5.DifferencesBetweenVectorQuantizationandQTT
VQ compresses data by mapping input vectors to a set of discrete symbols from a codebook. In VQ, the codebook is
typicallygeneratedbyclusteringalargesetofinputvectors. Eachinputvectoristhenreplacedbytheindexoftheclosest
codebookentry. Duringdecompression,theoriginalvectorsarereconstructedusingthecodebookentries. representation
throughalimitedsetofprototypevectors.
QTT,ontheotherhand,pertainstoatensordecompositionapproachwhereahigh-dimensionaltensorisfactorizedintoa
sequenceoflower-dimensionaltensors,referredtoas”cores”. Thesecoresareconnectedinatrain-likestructure,which
givesthemethoditsname. Theterm”Quantized”inthiscontextreferstothedecompositionofthetensorintothesesmaller,
discreteunitsor”quants”,enablingacompactandefficientrepresentationofthetensor’smultidimensionalstructure. Such
distinctionwasalsodiscussedinthepreviousworkofTT-NFbyObukhovetal(Obukhovetal.,2022). (Discussionin
Appendixpage19),whichmentions: “Theterm“quantization”inmachinelearningoftenreferstothereductionofthe
dynamicrangeofvaluesoflearnedparametersinaparametricmodel”.
ItisalsonoteworthythatVQandourQTTfactorizationapproachareorthogonaltechniques. Thereexistsapotentialto
mergethesemethodologies,whereeachtensorcoreinaQTTmightbequantizedusingalearneddictionary,presentingan
intriguingavenueforfutureresearch.
E.InfluenceofInitializationonQTTPerformance
Toexaminetheinfluenceofinitializationstandarddeviationsonmodelperformance,weconductedanexperimentutilizing
theLena512×512×3imageacross3000iterations.Thisexperimentaimstoanalyzehowvaryingtheinitializationstandard
deviation(σ)oftensorcoresimpactstheefficacyofourmodels,particularlycomparingourPuTTmethodagainstaQTT
modelwithoutupsampling. Theexperimentspannedarangeofσvaluesfrom0.001to0.5,withtheTT-NFinitialization
approachalsoevaluatedforcomparisonandisshowninTable3. Notably,PuTTexhibitedremarkableconsistencyinPSNR
values,averagingaround36.10regardlessoftheselectedσ value. ThisstabilityhighlightstheeffectivenessofPuTT’s
progressivelearningstrategyinmitigatingtheimpactsofinitializationvariability. Incontrast, theQTTmodelwithout
18Coarse-To-FineTensorTrainsforCompactVisualRepresentations
upsamplingdisplayedconsiderablesensitivitytoσ variations,withpronouncedfluctuationsinPSNRvaluesacrossthe
testedspectrum. Atahigherσof0.5,theQTTmodel’sperformancesignificantlydeteriorated,underscoringthemodel’s
challengesinlearningeffectivelyundersuchinitializationconditions. ThehighestPSNRobservedfortheQTTmodel
withoutupsamplingwas34.96,attainedatanintermediateσvalue,illustratingthenuancedroleofinitializationinmodel
learning.
σ PuTT QTTw/oUpsampling
0.001 36.103±0.001 32.676±0.082
0.005 36.101±0.001 32.873±0.107
0.01 36.116±0.013 34.845±0.102
0.05 36.100±0.002 34.958±0.083
0.1 36.116±0.001 34.946±0.092
0.5 36.103±0.006 12.320±1.610
TT-NFinit. 36.116±0.001 34.941±0.102
Table3.ComparisonofaveragePSNR(±1std)valuesforLena512x512x3imageover3000iterationswithvariousinitializationstandard
deviations(σ)percore,meanofzero,forPuTTandQTTwithoutupsampling. TT-NFinit. istheTT-NFinitializationasdetailedin
AppendixD.2Line774.PuTTemploysfourupsamplingstepsatiterations[50,100,200,400],withalearningrateof5e-3andbatchsize
of1282.Resultsaveragedoverthreeseeds.
F.ExperimentSetup
ThecoreofourexperimentsinvolvecomparingtheeffectivenessofPuTTagainstestablishedtensor-basedmethods(CP,
VM,andTucker)acrossboth2Dand3Dgrayscaledata,aswellasinthesettingofincompleteornoisydata. DuetoVM’s
quadraticdependencyonthesidelengthoftheinputtensor,itsapplicationto2Dimagesisimpractical,asitfailstoprovide
acompressedrepresentation. Thequantitativeexperimentsfor2D,Noise,andincompletedataareperformedongrayscale
imagestoreducetheamountofcomputationrequired. Theseresultsareconsistentwithcolorimageexamples.
Wetestthesemethodsatfivedistinctresolutionsfrom1kto16kindoublingstepsforimagesandfrom643to10243for3D,
examiningthreecompressionratiosforeachresolutiontoensureacomprehensiveanalysis. Foreachspecificcombination
ofresolutionandcompressionratio,wecomputetheaveragevaluesofthePSNRandSSIMmetricsaswellas±1standard
deviation.
ThePuTTimplementationstartsbyinitializingtheQTTinPyTorch(Paszkeetal.,2019),asoutlinedinSec.D.3. The
parameteroptimizationutilizestheAdamalgorithm(Kingma&Ba,2014). PuTT’supsamplingstepsanditerationcountsare
tailoredtothedesiredresolutionforeachtask. Forinstance,toattaina2Dresolutionof10242,themodelundergoesthree
upsamplingstepsfromaninitial1282resolutionatthe64th,128th,and256thiterations,culminatingin1024iterationsin
total. Aresolutionof20482involvesfourupsamplingstepsand2048iterations,startingfrom1282. Thisstrategyallows
substantialinitialtrainingatalowerresolution,enhancingefficiencyandmitigatingtheriskofoverfittingtolow-resolution
aspects,beforetransitioningtofullresolutionfortheremainingtrainingtocapturemorecomplexdetails. ForNoiseand
IncompleteDatascenarios,welimitourfocustomedianresolutions(4kand2563)anddoubletheiterationcountduetothe
increaseddifficultythesepresent.
Fordetailedconfigurationsofeachresolutionandtask,refertoTab.4.
F.1.BatchSizeandLearningRate
InourcompressionexperimentswithPuTT,CP,Tucker,andVM,wedidacomprehensiveanalysistodeterminetheideal
learningrateandbatchsizecombinations. Thisentailedevaluatingbatchsizesrangingfrom322to10242andlearningrates
from10−1to10−4. Ourfindingsindicatedthatlargerbatchsizesgenerallyledtoimprovedresultsbutwithatrade-offin
increasedtrainingtime. Strikingabalancebetweenefficiencyandaccuracy,weoptedforabatchsizeof5122withabase
learningrateof5·10−3. Forthehigh-resolutioncaseof10243,wehadtoadjustthebatchsizeto1282sinceTuckerand
CPmethodsduetocomputationallimits. Instead,wedoubledthenumberofiterations. WefoundthatPuTTwasmore
sensitivetothelearningrateandwasconfiguredat0.005. Incontrast,theCP,VM,andTuckermethodsutilizeaslightly
higherlearningrateof0.01,astheseconfigurationswerefoundtobeoptimalforeachrespectivemodel. Thesesettings
balancethecomputationaldemandswiththeneedforaccuracyandefficiencyinourexperiments.
19Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Experiment FullResolution InitialResolution IterationsforUpsampling TotalIterations
2D 10242 1282 [64,128,256] 1024
20482 1282 [64,128,256,512] 2048
40962 1282 [64,128,256,512,1024] 4096
81922 1282 [64,128,256,512,1024,2048] 8192
163842 1282 [64,128,256,512,1024,2048,4096] 16384
3D 643 83 [16,48,144] 512
1283 83 [16,48,144,432] 1536
2563 83 [16,48,144,432,1296] 4608
5123 83 [16,48,144,432,1296,3888] 13824
10243 83 [48,144,432,1296,3888,11664,34992] 69984
Noise2D 40962 1282 [64,128,256,512,1024,2048] 8192
Noise3D 2563 83 [16,48,144,432,1296,3888] 13824
IncompleteData2D 40962 1282 [64,128,256,512,1024,2048] 8192
PuTT-NeRF 2562 322 [500,1000,4000] 80000
Table4.Configurationsforupsamplingiterationsofallexperiments.Resolutiondetailsthespecificresolutioninpixels/voxels,withthe
initialstartingresolutions,iterationsforupsampling,anditerationcountsfordifferentresolutionsandexperiments.
Fig.17illustratestheserelationships,displayinghowdifferentbatchsizesinfluencedboththePSNRandthetrainingtimein
ourexperimentswitha4k”GirlwithaPearlEarring”imageusingaPuTT.
F.2.LearningRateStrategyinTrainingProcess
Inourtrainingsetup,weimplementanexponentiallearningratedecaystrategywithadecayfactorofα=0.1. Thisdecay
reducesthelearningrateexponentiallybyαbetweenupsamplingiterations. Forexample,ifupsamplingoccursatiteration
1000,thelearningratedeclinesfrom5·10−3toα·5·10−3 =5·10−4fromstep0tostep1000.
Weobserveacharacteristicpatterninthelossbehaviorpost-upsampling: aninitialspikeinlossduetotheintroduction
ofhigher-resolutioninterpolations. Asthenumberofparametersintherepresentationincreases,anewoptimizerwithan
adjustedlearningrateisnecessary. Simplymaintainingthepre-upsamplinglearningrateleadstominimallossreduction,
potentially trapping the model in local minima. Conversely, resetting to the base learning rate causes significant loss
fluctuations.
To address this, we employ a β upsampling learning rate decay, where β = 0.9. Post-upsampling, the learning rate is
recalibratedto0.9l·5·10−3(wherelisthenumberofupsamplingstepscompleted),ensuringabalancedapproachbetween
lossstabilityandeffectivelearning. Followingthis,weintroducea50-iterationwarm-upphaseforthelearningrateafter
eachupsampling,enhancingthestabilityofthetrainingprocess. Finally,postthelastupsamplingstep,thelearningrate
undergoesanotherexponentialdecayofαuntilthefinaliteration,ensuringasmoothconvergencetowardstheendofthe
training.
F.3.Compression
InFig. 18,19weseetheSSIMresultsforour2Dand3DcompressionexperimentsthatcomplementsFig. 6and7(PSNR
results)ofthemaintext. SimilarlytothePSNRresults,weseethatPuTToutperformbaselinesforboth2Dand3D.
F.4.NoiseRemoval
InSec.5.3ofthemaintext,weevaluatePuTT’sproficiencyinlearningfromnoisysamples,comparingitwithvarioustensor
networkmethodsandQTTswithoutupsampling. Weaimtolearnatensorrepresentationofatarget,denotedasX ,using
I
samplesfromitsnoisycounterpart,Xˆ ,obtainedbyThisnoisyversionhasanidenticalstructuretoX andisgenerated
I I
byaddingnoiseZ. Inourexperiments,Z issampledfromeitheraNormalorLaplaciandistribution,Z ∼N(0,σ),ora
Laplaciandistribution,Z ∼Laplace(0,b). Thus,thenoisytargetisformulatedasXˆ =X +Z.
I I
20Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Figure17.The graph showcases the impact of batch size on PSNR (LHS) and training time (RHS). Various learning rates from
[0.0001,0.1] were tested, and the optimal rate for each batch size was identified. The training spanned over 4096 iterations with
upsamplingstepsatintervals[64,128,256,512,1024]. Thebatchsizeof5122,highlightedinthefigure,wasselectedforitsoptimal
balancebetweenhighaccuracyandtimeefficiency,asevidencedinthetrainingofthe4k”GirlwithaPearlEarring”imagewitharankof
200.
Figure18. SSIM(y-axis)vscompressionratio(x-axis)for2Dfitting.“up=upsampling”.
Figure19. (a)SSIM(y-axis)incomparisontocompressionratio(x-axis)for3Dfitting.
21Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Figure20.PSNRwhenvaryingtheamountofGaussianorLaplaciannoiseforPuTTandbaselinesfor3Dexperiments.“up=upsampling”.
Figure21. SameasFig.20butforSSIM
InSec.5.3ofthemaintext,Figures10(a)and(b)presentquantitativeresultsfornoiseresiliencein2DfocusingonPSNR
andSSIMmetrics. Inthe3Dnoiseresilienceanalysis(Figures20and21),PuTTmirrorsits2Dsuccess,maintaininghigher
PSNRagainstothermethodsacrossvariousnoiselevels. Upsamplingsignificantlyboostsperformance,especiallyinSSIM
metricsfornoiseabove0.2σ,demonstratingPuTT’sstrongnoisehandlingcapabilitiesinboth2Dand3Dscenarios.
ForourNoiseRemovalexperiments,weadjustthelearningratetoaccommodatevaryingnoiselevels. Startingwithan
initiallearningrateoflr =0.005atzeronoise,wemodifythisrateforbothPuTTandbaselinemethodsaccordingto
init
theformulalr =0.005×0.1σ,where0.1isourchosenadaptationfactor. Forinstance,atanoiselevelofσ =0.5,the
σ
learningratebecomeslr =0.005×0.10.5 ≈0.00158. Thisapproachhelpsinachievingoptimalresultsacrossdifferent
σ
noiseintensities.
F.5.IncompleteDataExperiments
InSec.5.2ofthemaintext,wepresentedtheresultsfortrainingonlimiteddataforvariouspercentages. Fig.22presents
thetrainingdatausedintrainingatvariouspercentages. Thesequencebeginswiththeoriginaltargetimage, followed
byitsversionwithapercentageoftheoriginaldatasampled. Theseversionsarecreatedbyrandomlysettingaspecified
percentageofthepixelsintheoriginalimagetozero,representingscenarioswithlimiteddataavailability. Thesubsampling
percentages,rangingfrom1%to50%,arespecifiedaboveeachcorrespondingimage.
InSec.5.2ofthemaintext,Fig.12showshowvaryingthenumberofupsamplingstepswhentraining99%missingdata
affectsPSNRandSSIMfor”GirlwithPearlEarrings”4k. Thefirstimagedisplaysthetrainingdata,followedbyPuTT
resultswith0,1,4,and7upsamplingsteps. Fig.23showsthelargervisualexamples.
InourIncompleteDataexperiments, similartotheNoiseRemovalexperiments, wemodifythelearningratebasedon
theproportionofavailabletrainingdata. Weobservedthatoptimalresultswereachievedwithlowerlearningrateswhen
lesstrainingdatawasaccessible. Initially, wesetthelearningratetolr = 0.005whenalldatapointsareavailable.
init
22Coarse-To-FineTensorTrainsforCompactVisualRepresentations
(a)OriginalandIncompleteImages
Figure22.IncompletedatatraininginputsforvariousPercentages:Thesequencestartswiththeoriginaltargetimage(leftmost),followed
bythevaryingpercentagesofmissingdatafrom99%to50%.
Figure23.Varyingthenumberofupsamplingstepswhentraining99%missingdata.Thefirstimagedisplaysthetrainingdata,followed
byPuTTresultswith0,1,4,and7upsamplingsteps.
Astheamountoftrainingdatadecreases,weadjustthelearningrateforPuTTandbaselinemethodsusingtheformula
lr = 0.005×f1−p. Here, f represents the adaptation factor, and p denotes the proportion of available training data,
p
rangingbetween0and1. Thisstrategyhelpsustailorthelearningprocesseffectivelytodifferentdataavailabilityscenarios.
Downsampledimages Fig.24showsthedownsampledtargetsusedduringtrainingfor95%missingdataof”Girlwith
PearlEarring”. ThethreeRGBchannelsareaveragedtovisualizehowaggregatedvaluesformablurredversionofthe
actualtargetatlowerdownsampledresolutions.
Figure24.Illustrationoftheeffectsofdownsamplinga4kimagewhere95%ofthedataatthetargetresolutionismissing.Theprogression
fromlefttorightshowsimagesatlowerresolutionscontainingaggregatedinformationaboutthepixelsavailableathigherresolutions.
Thethreechannelshavebeenaggregatedtomoreclearlyshowthispattern
Maskedaveragepooling AsexplainedinSec.5.2ofthemaintext,totrainwithadownsampledtargetatlowerresolutions,
wecreateamodifiedtargetimageI . WebuildI fromI ,weuseacustommaskedaveragepoolingmethod,
D−l,p D−1,p D,p
averagingonlythenon-zerovaluesineachwindow, sizedaccordingtothedownsamplingfactor. Themaskedaverage
poolingcanbeillustratedthroughasimpleexample. Considera2×2windowcontainingvalues[2,2,0,0]. Instandard
23Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Figure25.VisualcomparisonofTensoRFandPuTTonNearandFarviewsfor“Materials”,“Mic”,“Chair”,“Ficus”scenes.
averagepooling,theoutputwouldbetheaverageofallvalues,(2+2+0+0)/4=1. However,inourmaskedaverage
pooling,wherezerovaluesareexcludedfromthecalculation,theoutputistheaverageofthenon-zerovalues,(2+2)/2=2.
Thismethodensuresthatonlyrelevantdatacontributestothepoolingprocess. Fig.24providesavisualrepresentation
of this process. It showcases how a 4k image, with 95% of its data missing at the target resolution, is progressively
downsampled. Eachimageinthesequence,movingfromlefttoright,representsalower-resolutionversion,consolidating
aggregatedinformationfromtheavailablepixelsathigherresolutions. Foreasiervisualization,thechanneldimensionhas
beenaveraged.
G.NovelViewSynthesis
Model(Size) PSNR SSIM LPIPS
PuTT(12MB) 31.953 0.957 0.0597
PuTT(12MB)NoPE 31.880 0.9537 0.0612
PuTT(12MB)NoUp 31.466 0.9502 0.0654
TensoRF(12MB) 31.623 0.9523 0.0626
PuTT(7MB) 31.665 0.9560 0.0602
PuTT(7MB)NoPE 31.633 0.9516 0.0650
PuTT(7MB)NoUP 31.295 0.9480 0.0710
TensoRF(7MB) 31.103 0.9476 0.0697
Table5.Comparisonofnovelviewsynthesis(NeRF)underdifferentmodelsizes(7MBand12MB)incomparisontoTensoRF.”NoUp”:
Noupsampling,”NoPE”:NopositionalencodingusedforinputstotheShader(MLP)function.
Wecompareourapproachwithothergrid-basednovelviewsynthesismethods,includingpreviousworksNSVF(Liuetal.,
24Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Chair Drums Ficus Hotdog Lego Materials Mic Ship Average
Metric Method
TensoRF PuTT TensoRF PuTT TensoRF PuTT TensoRF PuTT TensoRF PuTT TensoRF PuTT TensoRF PuTT TensoRF PuTT TensoRF PuTT
Near 35.953 37.450 27.024 27.672 32.653 34.254 37.176 38.276 34.396 35.579 28.915 29.309 33.510 34.886 30.463 30.990 32.511 33.552
PSNR Far 31.798 33.262 24.444 24.951 30.457 31.822 31.579 31.331 31.434 31.523 25.986 26.177 33.515 34.382 27.287 27.264 29.563 30.089
All 32.616 34.144 25.004 25.444 30.986 32.253 36.034 36.590 33.264 34.009 28.953 29.331 32.520 33.596 29.128 29.559 31.063 31.866
Table6.NovelViewSynthesiscomparisontoTensoRF(PSNR)forallscenesintheSynthetic-NeRF(Mildenhalletal.,2020)dataset.We
testonviewsthatareneartrainingviews(Near),farfromtrainingviews(Far),andalltestviews(All).Themodelsbeingcomparedhavea
sizeog7MB.
2020a)PlenOctrees(Yuetal.,2021b),Plenoxels(Yuetal.,2021a),DVGO(Sunetal.,2022),K-planes(Fridovich-Keiletal.,
2023),andTensoRF(Chenetal.,2022). Mostoftheseresultsweretakendirectlyfromtherespectivepapersifpossible. We
directlycomparethetensormethodsusedbyTensoRFandtheirefficiencyvsourPuTTmethod’seffeciency,byaligning
themodelsizesofthetwo. InourcomparisonwithTensoRF,weomittedits”shrinkage”feature,whichinvolvesusingan
alphamasktodynamicallyreshapeandresampletensorfactorsduringtraining. However,duetoitscurrentincompatibility
withourQTTstructure,integratingthisfeatureintoPuTTrequiresadditionaldevelopment. Instead,inourexperiments,we
employedaprogressivelyupdatedalphamask,utilizedexclusivelyforray-filteringpurposes. Thisadjustmentwascriticalin
maintainingafairandrelevantcomparisonbetweenthemethodsundertest.
Inournovelviewsynthesisexperiments,modelsweretrainedforatotalof80,000steps. Thelearningrate(LR)schedule
followedthesamestructureasoutlinedinSec.5.1ofthemaintext,butforPuTT,weusedabaseLRof1e−3. Inthe
caseofTensoRF,weadheredtoitsstandardconfigurationbutadjustedthefactorcountsintheVMdecompositiontoalign
withour7MBand12MBmodelsizes. Forthe7MBTensoRFmodels,therankfactorsweresetto[4,4,4]fordensityand
[5,5,5]forappearance. Incontrast,forthelarger12MBmodels,weincreasedtheseranksto[8,8,8]forbothdensityand
appearancefactors,ensuringaconsistentcomparisonintermsofmemoryfootprint. ForPuTT7MBisequaltoarankof140
forthedensitygridand200fortheappearancegrid,andforthe12MBmodel,thisis200and280respectively.
Regardingbatchsizes,ourexperimentsutilized4096raysperbatch. Thenumberofuniformsamplesperraywascappedat
884. Bothmodels,PuTTandTensoRF,demonstratedsimilarrunningtimes,rangingbetween4and5hoursforboth7MB
and12MBonanNVIDIATeslaV100GPU.TheincreasedrunningtimeobservedwithTensoRFcomparedtothosereported
intheirpapercanbeattributedtonotemployingitsshrinkagefeature. Whilethisfeaturetypicallyenhancesefficiency
in TensoRF’s processing, its exclusion was necessary to ensure a more direct comparison with the PuTT model under
equivalentconditions.
Weassessedourmethodunderdifferentconditions: withandwithoutPositionalEncoding(PE),andwithandwithout
Upsampling,appliedtoboththe7MBand12MBmodelsizes. Additionally,theseconfigurationswerecontrastedwith
TensoRF, both with and without its Upsampling feature. Each experiment was replicated over three seeds to ensure
robustness,withtheoutcomesdetailedinTab.5.
G.1.OmittingPositionalEncoding
One of the key innovations in the original NeRF (Mildenhall et al., 2020) was the use of Positional Encoding (PE) to
transform5Dinputcoordinatesintoahigher-dimensionalspace,enhancingtherepresentationofhigh-frequencyscene
content. ThistechniqueresultedinasignificantPSNRimprovementforNeRF(Mildenhalletal.,2020). Ourfindings,as
showninTab.5,suggestthatthequantizedTTstructureeffectivelyencodesinputsfortheMLPresponsibleforcolorvalue
computationsbasedonviewingdirections. WiththequantizedTTstructure,weobservedonlyaminordropinperformance
whenPEwasomitted. Forthe12MBmodel,weonlyseeadropof0.07PSNRwhennotusingPositionalEncodingas
opposedtothedropreportedbyMildenhalletal. inNeRF(Mildenhalletal.,2020),whichwasadifferenceof2.24PSNR.
Thishighlightstheeffectivenessofthequantizedtensortrainencoding.
G.2.TestingfromNeartoFarViews
Tostudythefine-grainedrepresentationabilityofPuTT,weexperimentwithtestingviewscapturedfromneartofarcamera
positionscomparedwithsourceviews. Specifically,wecomputetheaveragevarianceinviewanglesbetweeneachtesting
viewanditsnearesttwentysourceviews. Aftersortingtheaverageviewangledifferencesforalltestingviews,weidentify
thetwentytestingviewswiththesmallestdifferencesasnearviewsandthetwentytestingviewswiththelargestdifferences
asfarviews.
InadditiontoSec.5.1ofthemaintext,Tab.6showsthatPuTToutperformsTensorRFfromneartofartestingviewson
25Coarse-To-FineTensorTrainsforCompactVisualRepresentations
bothSSIMandLPIPSforallscenesinthedataset. ItshouldbenoticedthattheadvantageofPuTToverTesorRFonnear
viewsismoresignificantthanthatonfarviews. ItindicatesthatPuTT,benefitingfromthecoarse-to-finelearningmethod,
learnsamorecompactandexpressiverepresentationofthefine-graineddetails,asshowninFig.25.
G.3.Per-sceneresultsnovelviewsynthesis
26Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Methods Avg. Chair Drums Ficus Hotdog Lego Materials Mic Ship
PSNR↑
PlenOctrees-L*(Yuetal.,2021b) 31.71 34.66 25.31 30.79 36.79 32.95 29.76 33.97 29.42
Plenoxels-L*(Yuetal.,2021a) 31.71 33.98 25.35 31.83 36.43 34.10 29.14 33.26 29.62
DVGO-L*(Sunetal.,2022) 31.95 34.09 25.44 32.78 36.74 34.64 29.57 33.20 29.13
kplanes-L*(Fridovich-Keiletal.,2023) 32.36 34.99 25.66 31.41 36.78 35.75 29.48 34.05 30.74
Instant-NGP*(Mu¨lleretal.,2022)-L 33.18 35.00 26.02 33.51 37.40 36.39 29.78 36.22 31.10
TensoRFVM-192-L*(Chenetal.,2022) 33.14 35.76 26.01 33.99 37.41 36.46 30.12 34.61 30.77
OurPuTT-200-L 32.79 35.29 25.82 33.39 37.29 35.93 29.93 34.21 30.39
NeRF-S*(Mildenhalletal.,2020) 31.01 33.00 25.01 30.13 36.18 32.54 29.62 32.91 28.65
TensoRFVM-30-S(noshrinking) 31.10 32.62 25.00 30.99 36.03 33.26 28.95 32.52 29.13
OurPuTT-200-S 31.87 34.14 25.44 32.25 36.59 34.01 29.33 33.60 29.56
Instant-NGP-M*(Mu¨lleretal.,2022) 33.17 35.00 26.02 33.51 37.40 36.39 29.78 36.22 31.10
TensoRFVM-48-M(noshrinking) 31.51 33.29 25.22 31.62 36.24 33.79 29.34 32.97 29.59
OurPuTT-200-M 31.98 34.23 25.67 32.32 37.07 34.02 29.58 33.56 29.44
SSIM↑
PlenOctrees-L*(Yuetal.,2021b) 0.958 0.981 0.933 0.970 0.982 0.971 0.955 0.987 0.884
Plenoxels-L*(Yuetal.,2021a) 0.958 0.977 0.933 0.976 0.980 0.976 0.949 0.985 0.890
DVGO-L*(Sunetal.,2022) 0.957 0.977 0.930 0.978 0.980 0.976 0.951 0.983 0.879
kplanes-L*(Fridovich-Keiletal.,2023) 0.962 0.983 0.938 0.975 0.982 0.982 0.950 0.988 0.897
TensoRFVM-192-L*(Chenetal.,2022) 0.963 0.985 0.937 0.982 0.982 0.983 0.952 0.988 0.895
OurPuTT-600-L 0.958 0.977 0.933 0.979 0.981 0.972 0.947 0.986 0.888
NeRF-S*(Mildenhalletal.,2020) 0.947 0.967 0.925 0.964 0.974 0.961 0.949 0.980 0.856
TensoRFVM-30-S(noshrinking) 0.948 0.965 0.920 0.966 0.974 0.966 0.938 0.979 0.873
OurPuTT-200-S 0.956 0.976 0.928 0.975 0.979 0.974 0.945 0.987 0.876
TensoRFVM-48-M(noshrinking) 0.952 0.971 0.923 0.969 0.975 0.974 0.944 0.982 0.879
OurPuTT-280-M 0.957 0.976 0.929 0.975 0.979 0.973 0.946 0.989 0.878
LPIPS ↓
Vgg
DVGO-L*(Sunetal.,2022) 0.035 0.016 0.061 0.015 0.017 0.014 0.026 0.014 0.118
TensoRFVM-192-L*(Chenetal.,2022) 0.027 0.010 0.051 0.012 0.013 0.007 0.026 0.009 0.085
OurPuTT-600-L 0.045 0.021 0.081 0.025 0.019 0.027 0.045 0.018 0.124
NeRF-S*(Mildenhalletal.,2020) 0.081 0.046 0.091 0.044 0.121 0.050 0.063 0.028 0.206
TensoRFVM-30-S(noshrinking) 0.067 0.042 0.104 0.034 0.042 0.039 0.074 0.025 0.177
OurPuTT-200-S 0.061 0.037 0.085 0.029 0.038 0.039 0.069 0.024 0.161
TensoRFVM-48-M(noshrinking) 0.062 0.041 0.100 0.032 0.0412 0.037 0.072 0.022 0.155
OurPuTT-280-M 0.059 0.034 0.084 0.029 0.037 0.039 0.069 0.022 0.158
Table7.QuantitativeresultsoneachscenefromtheSynthetic-NeRF(Mildenhalletal.,2020)dataset.(Resultsdenotedwithanasterisk
(*)weresourcedfromthedatapresentedinthereferencedpaper.
27Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Methods Avg. Wineholder Steamtrain Toad Robot Bike Palace Spaceship Lifestyle
PSNR↑
DVGO-L*(Sunetal.,2022) 35.08 30.26 36.56 33.10 36.36 38.33 34.49 37.71 33.79
TensoRFVM-192-L*(Chenetal.,2022) 36.52 31.32 37.87 34.85 38.26 39.23 37.56 38.60 34.51
OurPuTT-600-L 36.57 31.33 37.21 34.87 38.22 39.32 37.47 39.07 35.05
NeRF-S*(Mildenhalletal.,2020) 30.81 28.23 30.84 29.42 28.69 31.77 31.76 34.66 31.08
TensoRFVM-30-S(noshrinking) 34.58 29.62 35.51 32.21 35.42 37.85 35.49 37.05 33.48
OurPuTT-200-S 35.59 30.55 36.22 33.73 36.44 38.59 36.02 38.72 34.43
TensoRFVM-48-M(noshrinking) 35.41 30.35 36.54 32.84 36.85 38.33 36.54 37.51 34.34
OurPuTT-280-M 36.04 30.89 36.41 34.48 37.02 38.89 36.78 38.95 34.86
SSIM↑
DVGO-L*(Sunetal.,2022) 0.975 0.949 0.989 0.966 0.992 0.991 0.962 0.988 0.965
TensoRFVM-192-L*(Chenetal.,2022) 0.982 0.961 0.991 0.978 0.994 0.993 0.979 0.989 0.968
OurPuTT-600-L 0.982 0.960 0.989 0.979 0.994 0.993 0.977 0.990 0.970
NeRF-S*(Mildenhalletal.,2020) 0.952 0.920 0.966 0.920 0.960 0.970 0.950 0.980 0.946
TensoRFVM-30-S(noshrinking) 0.974 0.949 0.985 0.962 0.992 0.991 0.970 0.982 0.962
OurPuTT-200-S 0.976 0.954 0.985 0.971 0.990 0.989 0.968 0.989 0.965
TensoRFVM-48-M(noshrinking) 0.975 0.949 0.986 0.963 0.992 0.991 0.973 0.985 0.964
OurPuTT-280-M 0.977 0.956 0.983 0.974 0.990 0.990 0.975 0.984 0.967
LPIPS ↓
VGG
DVGO-L*(Sunetal.,2022) 0.019 0.038 0.010 0.030 0.005 0.004 0.027 0.009 0.027
TensoRF-VM-192-L*(Chenetal.,2022) 0.012 0.024 0.006 0.016 0.003 0.003 0.011 0.009 0.021
OurPuTT-600-L 0.017 0.032 0.017 0.023 0.008 0.004 0.021 0.011 0.023
NeRF-S*(Mildenhalletal.,2020) 0.043 0.096 0.031 0.069 0.038 0.019 0.031 0.016 0.047
TensoRFVM-30-S(noshrinking) 0.036 0.059 0.032 0.046 0.014 0.017 0.033 0.027 0.057
OurPuTT-200-S 0.032 0.059 0.029 0.039 0.013 0.014 0.032 0.018 0.055
TensoRFVM-48-M(noshrinking) 0.034 0.065 0.028 0.043 0.013 0.014 0.029 0.025 0.056
OurPuTT-280-M 0.027 0.053 0.022 0.031 0.011 0.011 0.024 0.017 0.046
Table8.QuantitativeresultsoneachscenefromtheSynthetic-NSVF(Liuetal.,2020a)dataset. Resultsdenotedwithanasterisk(*)
weresourcedfromthedatapresentedinthereferencedpaper.
H.CompatibilityofTensoRF’s”Shrinkage”withQTT
Shrinkage: TensoRF’s“shrinkage”processremovesparameterscorrespondingtoemptyregionsinthe3Dspace. For
instance,inTensoRF’sVM(Vector-Matrix)decomposition,parametersforindicesx,y,z <10areprunediftheregionis
empty,reallocatingthemtodenserregionstoenhanceresolution.
Incompatibility with QTT: Unlike TensoRF, QTT uses a hierarchical structure with mode quantization, compressing
dimensionsintopowersoftwo. Thisstructuremixesparameters,interlinkingthoserepresentingacoordinate(x,y,z)with
manyothers. Consequently, QTT’sparametersharingacrossthehierarchycomplicatesdirectpruningasinTensoRF’s
“shrinkage.”Figure16illustratesthisinterlinkedstructure. WhileQTT’sinterdependencycomplicates“shrinkage,”itallows
capturing dimensional correlations, ensures logarithmic parameter growth, and improves robustness against noise and
incomplete data. Updating parameters for a specific coordinate refines those across interconnected indices, leveraging
correlationsforresilientdatarepresentation. Thiscomplexitynecessitatesacoarse-to-finetrainingapproach,asproposedin
ourPuTTmethod,fosteringthehierarchicalstructuremodeledbyQTT.
I.ExploringInpaintingCapabilitieswithPuTT
Inpaintingtasksbenefitfromarichpriorofthetruedatadistribution,oftenusinggenerativemodels. Ourmethod,however,
focusesonasingleexampletolearnacompact,high-qualityrepresentationsuitablefortaskslikedenoisingorpredicting
missingvalues.
28Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Methods Avg. Ignatius Truck Barn Caterpillar Family
PSNR↑
PlenOctrees-L*(Yuetal.,2021b) 27.99 28.19 26.83 26.80 25.29 32.85
Plenoxels-L*(Yuetal.,2021a) 27.43 27.51 26.59 26.07 24.64 32.33
DVGO-L*(Sunetal.,2022) 28.41 28.16 27.15 27.01 26.00 33.75
TensoRFVM-192-L(Chenetal.,2022) 28.56 28.34 27.14 27.22 26.19 33.92
OurPuTT-600-L 28.37 28.08 27.09 27.22 25.93 33.56
NeRF-S*(Mildenhalletal.,2020) 25.78 25.43 25.36 24.05 23.75 30.29
TensoRFVM-30-S(noshrinking) 27.67 27.58 26.42 26.67 25.04 32.62
OurPuTT-200-S 27.99 27.98 26.47 27.08 25.47 32.96
TensoRFVM-48-M(noshrinking) 28.03 28.04 26.70 27.06 25.28 33.06
OurPuTT-280-M 28.15 28.01 26.91 27.01 25.61 33.23
SSIM↑
PlenOctrees-L*(Yuetal.,2021b) 0.917 0.948 0.914 0.856 0.907 0.962
Plenoxels-L*(Yuetal.,2021a) 0.906 0.943 0.901 0.829 0.902 0.956
DVGO-L*(Sunetal.,2022) 0.911 0.944 0.906 0.838 0.906 0.962
TensoRFVM-192-L*(Chenetal.,2022) 0.920 0.948 0.914 0.864 0.912 0.965
OurPuTT-600-L 0.917 0.943 0.908 0.865 0.907 0.960
NeRF-S*(Mildenhalletal.,2020) 0.864 0.920 0.860 0.750 0.860 0.932
TensoRFVM-30-S(noshrinking) 0.899 0.924 0.900 0.840 0.880 0.951
OurPuTT-200-S 0.901 0.931 0.901 0.824 0.901 0.950
TensoRFVM-48-M(noshrinking) 0.901 0.928 0.900 0.843 0.882 0.951
OurPuTT-280-M 0.907 0.934 0.909 0.835 0.903 0.953
LPIP ↓
VGG
DVGO-L*(Sunetal.,2022) 0.148 0.090 0.145 0.290 0.152 0.064
TensoRFVM-192-L*(Chenetal.,2022) 0.140 0.078 0.145 0.252 0.159 0.064
OurPuTT-600-L 0.142 0.082 0.153 0.241 0.163 0.070
NeRF-S*(Mildenhalletal.,2020) 0.198 0.111 0.192 0.395 0.196 0.098
TensoRFVM-30-S(noshrinking) 0.185 0.099 0.218 0.297 0.227 0.083
OurPuTT-200-S 0.197 0.094 0.326 0.272 0.220 0.072
TensoRFVM-48-M(noshrinking) 0.156 0.092 0.173 0.257 0.187 0.074
OurPuTT-280-M 0.150 0.086 0.162 0.259 0.170 0.073
Table9.QuantitativeresultsoneachscenefromtheTanks&Temples(Knapitschetal.,2017)dataset.Resultsdenotedwithanasterisk(*)
weresourcedfromthedatapresentedinthereferencedpaper.
29Coarse-To-FineTensorTrainsforCompactVisualRepresentations
Weconductedanexperimentwithtwoimages: abrickpatternandtheLenaimage. Eachimagehadacontinuousmask
covering10%ofthearea. UsingPuTT,weattemptedtoreconstructthemaskedregions. ResultsareshowninFig.26with
columnsfortheoriginalimage,themaskedimage,andthereconstructedimage.ThebrickimageshowsPuTT’seffectiveness
instructuredsettings,whiletheLenaimagehighlightsthechallengesofinpaintinglargeareaswithoutsophisticatedpriors.
Figure26.Inpaintingresultsontwo512×512×3images: abrickpattern(top)andtheLenaimage(bottom). Columnsdisplaythe
originalimage,themaskedimage(10%mask),andthePuTTreconstruction.Reconstructionswereachievedafter1024iterationsusing
onlyunmaskedregions.
J.ExploringRankIncrementationwithSlimmeRF
SlimmeRF (Yuan & Zhao, 2023) enhances the TensoRF VM framework by introducing an adaptive rank mechanism,
dynamicallyadjustingthemodel’slearningcapacity. Themodelstartswithalow-rankrepresentationandincrementally
increasestherankbasedonlearningprogress,capturingessentialfeaturesearlyandbuildingcomplexityasneeded. Our
approachsimilarlyincreasestensorrankprogressively,astrategyusedinphysicsmethodslikeDMRG(White,1992a)and
TT-cross(Oseledets&Tyrtyshnikov,2010),aligningwithourprogressivelearninginPuTT.
Weexploredthiswitha2Dcompressionexperimenton4Kimages,asshowninTable10. WecomparedastandardQTT
modelwithoutupsamplingtoonewithprogressiverankincrementation([2,4,8,16]ranksduringinitialtraining). These
experimentsindicatethatgradualrankincrementscanimprovePSNRscoreswithoutaddingcomplexity.
Theseresults,excludingdimensionalupsampling,highlightthepotentialofrankincrementation. However,moreextensive
experimentsareneededtofullyvalidatethesefindings.
30Coarse-To-FineTensorTrainsforCompactVisualRepresentations
RankIncrement PSNR(Avg) Std
0(Baseline) 37.444 0.821
2 38.168 0.391
4 38.273 0.444
8 38.331 0.464
16 38.131 0.513
Table10.ResultsofProgressiveRankIncrementation.Thistablesummarizestheeffectsofrankincrementationonlearningefficiencyfor
a1kresolutionimageofa’GirlwithaPearlEarring’.ItcontrastsabaselineQTTmodelwithoutupsampling,startingwitharankof200,
withQTTmodelsemployingprogressiverankincrementationfromabaserankof100to200.Rankincrementsof2,4,8,and16were
testedoverfourintervals:[100,1000],[100,1500],[250,1500],and[250,750],todeterminethestartandendpointsforrankincrements.
ThetabledisplaystheaveragePSNRvalues,alongsidestandarddeviations,acrossthreeiterationsforeachrankincrementstrategy.These
resultshighlighttheimprovedperformanceduetostrategicrankincrementation,withallstrategiessurpassingthebaseline.Forinstance,
anincrementstepof2withinthe[100,1000]intervalincreasedtherankevery(1000−100)/(200−100)=9iterations.Althougha
rankincrementof8yieldedthebestaveragePSNR,differencesbetweenincrementstepswereminimal,suggestingathoughtfulapproach
torankincrementationcansignificantlyenhancethemodel’slearningcapability.
31