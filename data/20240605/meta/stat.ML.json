[
    {
        "title": "Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks",
        "authors": "Tianyu HeDarshil DoshiAritra DasAndrey Gromov",
        "links": "http://arxiv.org/abs/2406.02550v1",
        "entry_id": "http://arxiv.org/abs/2406.02550v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02550v1",
        "summary": "Large language models can solve tasks that were not present in the training\nset. This capability is believed to be due to in-context learning and skill\ncomposition. In this work, we study the emergence of in-context learning and\nskill composition in a collection of modular arithmetic tasks. Specifically, we\nconsider a finite collection of linear modular functions $z = a \\, x + b \\, y\n\\;\\mathrm{mod}\\; p$ labeled by the vector $(a, b) \\in \\mathbb{Z}_p^2$. We use\nsome of these tasks for pre-training and the rest for out-of-distribution\ntesting. We empirically show that a GPT-style transformer exhibits a transition\nfrom in-distribution to out-of-distribution generalization as the number of\npre-training tasks increases. We find that the smallest model capable of\nout-of-distribution generalization requires two transformer blocks, while for\ndeeper models, the out-of-distribution generalization phase is\n\\emph{transient}, necessitating early stopping. Finally, we perform an\ninterpretability study of the pre-trained models, revealing the highly\nstructured representations in both phases; and discuss the learnt algorithm.",
        "updated": "2024-06-04 17:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02550v1"
    },
    {
        "title": "Guiding a Diffusion Model with a Bad Version of Itself",
        "authors": "Tero KarrasMiika AittalaTuomas KynkäänniemiJaakko LehtinenTimo AilaSamuli Laine",
        "links": "http://arxiv.org/abs/2406.02507v1",
        "entry_id": "http://arxiv.org/abs/2406.02507v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02507v1",
        "summary": "The primary axes of interest in image-generating diffusion models are image\nquality, the amount of variation in the results, and how well the results align\nwith a given condition, e.g., a class label or a text prompt. The popular\nclassifier-free guidance approach uses an unconditional model to guide a\nconditional model, leading to simultaneously better prompt alignment and\nhigher-quality images at the cost of reduced variation. These effects seem\ninherently entangled, and thus hard to control. We make the surprising\nobservation that it is possible to obtain disentangled control over image\nquality without compromising the amount of variation by guiding generation\nusing a smaller, less-trained version of the model itself rather than an\nunconditional model. This leads to significant improvements in ImageNet\ngeneration, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using\npublicly available networks. Furthermore, the method is also applicable to\nunconditional diffusion models, drastically improving their quality.",
        "updated": "2024-06-04 17:25:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02507v1"
    },
    {
        "title": "Ai-Sampler: Adversarial Learning of Markov kernels with involutive maps",
        "authors": "Evgenii EgorovRicardo ValpergaEfstratios Gavves",
        "links": "http://arxiv.org/abs/2406.02490v1",
        "entry_id": "http://arxiv.org/abs/2406.02490v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02490v1",
        "summary": "Markov chain Monte Carlo methods have become popular in statistics as\nversatile techniques to sample from complicated probability distributions. In\nthis work, we propose a method to parameterize and train transition kernels of\nMarkov chains to achieve efficient sampling and good mixing. This training\nprocedure minimizes the total variation distance between the stationary\ndistribution of the chain and the empirical distribution of the data. Our\napproach leverages involutive Metropolis-Hastings kernels constructed from\nreversible neural networks that ensure detailed balance by construction. We\nfind that reversibility also implies $C_2$-equivariance of the discriminator\nfunction which can be used to restrict its function space.",
        "updated": "2024-06-04 17:00:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02490v1"
    },
    {
        "title": "Meta-Learners for Partially-Identified Treatment Effects Across Multiple Environments",
        "authors": "Jonas SchweisthalDennis FrauenMihaela van der SchaarStefan Feuerriegel",
        "links": "http://arxiv.org/abs/2406.02464v1",
        "entry_id": "http://arxiv.org/abs/2406.02464v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02464v1",
        "summary": "Estimating the conditional average treatment effect (CATE) from observational\ndata is relevant for many applications such as personalized medicine. Here, we\nfocus on the widespread setting where the observational data come from multiple\nenvironments, such as different hospitals, physicians, or countries.\nFurthermore, we allow for violations of standard causal assumptions, namely,\noverlap within the environments and unconfoundedness. To this end, we move away\nfrom point identification and focus on partial identification. Specifically, we\nshow that current assumptions from the literature on multiple environments\nallow us to interpret the environment as an instrumental variable (IV). This\nallows us to adapt bounds from the IV literature for partial identification of\nCATE by leveraging treatment assignment mechanisms across environments. Then,\nwe propose different model-agnostic learners (so-called meta-learners) to\nestimate the bounds that can be used in combination with arbitrary machine\nlearning models. We further demonstrate the effectiveness of our meta-learners\nacross various experiments using both simulated and real-world data. Finally,\nwe discuss the applicability of our meta-learners to partial identification in\ninstrumental variable settings, such as randomized controlled trials with\nnon-compliance.",
        "updated": "2024-06-04 16:31:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02464v1"
    },
    {
        "title": "Coresets for Multiple $\\ell_p$ Regression",
        "authors": "David P. WoodruffTaisuke Yasuda",
        "links": "http://arxiv.org/abs/2406.02432v1",
        "entry_id": "http://arxiv.org/abs/2406.02432v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02432v1",
        "summary": "A coreset of a dataset with $n$ examples and $d$ features is a weighted\nsubset of examples that is sufficient for solving downstream data analytic\ntasks. Nearly optimal constructions of coresets for least squares and $\\ell_p$\nlinear regression with a single response are known in prior work. However, for\nmultiple $\\ell_p$ regression where there can be $m$ responses, there are no\nknown constructions with size sublinear in $m$. In this work, we construct\ncoresets of size $\\tilde O(\\varepsilon^{-2}d)$ for $p<2$ and $\\tilde\nO(\\varepsilon^{-p}d^{p/2})$ for $p>2$ independently of $m$ (i.e.,\ndimension-free) that approximate the multiple $\\ell_p$ regression objective at\nevery point in the domain up to $(1\\pm\\varepsilon)$ relative error. If we only\nneed to preserve the minimizer subject to a subspace constraint, we improve\nthese bounds by an $\\varepsilon$ factor for all $p>1$. All of our bounds are\nnearly tight.\n  We give two application of our results. First, we settle the number of\nuniform samples needed to approximate $\\ell_p$ Euclidean power means up to a\n$(1+\\varepsilon)$ factor, showing that $\\tilde\\Theta(\\varepsilon^{-2})$ samples\nfor $p = 1$, $\\tilde\\Theta(\\varepsilon^{-1})$ samples for $1 < p < 2$, and\n$\\tilde\\Theta(\\varepsilon^{1-p})$ samples for $p>2$ is tight, answering a\nquestion of Cohen-Addad, Saulpic, and Schwiegelshohn. Second, we show that for\n$1<p<2$, every matrix has a subset of $\\tilde O(\\varepsilon^{-1}k)$ rows which\nspans a $(1+\\varepsilon)$-approximately optimal $k$-dimensional subspace for\n$\\ell_p$ subspace approximation, which is also nearly optimal.",
        "updated": "2024-06-04 15:50:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02432v1"
    }
]