[
    {
        "title": "VHS: High-Resolution Iterative Stereo Matching with Visual Hull Priors",
        "authors": "Markus PlackHannah DrögeLeif Van HollandMatthias B. Hullin",
        "links": "http://arxiv.org/abs/2406.02552v1",
        "entry_id": "http://arxiv.org/abs/2406.02552v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02552v1",
        "summary": "We present a stereo-matching method for depth estimation from high-resolution\nimages using visual hulls as priors, and a memory-efficient technique for the\ncorrelation computation. Our method uses object masks extracted from\nsupplementary views of the scene to guide the disparity estimation, effectively\nreducing the search space for matches. This approach is specifically tailored\nto stereo rigs in volumetric capture systems, where an accurate depth plays a\nkey role in the downstream reconstruction task. To enable training and\nregression at high resolutions targeted by recent systems, our approach extends\na sparse correlation computation into a hybrid sparse-dense scheme suitable for\napplication in leading recurrent network architectures. We evaluate the\nperformance-efficiency trade-off of our method compared to state-of-the-art\nmethods, and demonstrate the efficacy of the visual hull guidance. In addition,\nwe propose a training scheme for a further reduction of memory requirements\nduring optimization, facilitating training on high-resolution data.",
        "updated": "2024-06-04 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02552v1"
    },
    {
        "title": "Dreamguider: Improved Training free Diffusion-based Conditional Generation",
        "authors": "Nithin Gopalakrishnan NairVishal M Patel",
        "links": "http://arxiv.org/abs/2406.02549v1",
        "entry_id": "http://arxiv.org/abs/2406.02549v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02549v1",
        "summary": "Diffusion models have emerged as a formidable tool for training-free\nconditional generation.However, a key hurdle in inference-time guidance\ntechniques is the need for compute-heavy backpropagation through the diffusion\nnetwork for estimating the guidance direction. Moreover, these techniques often\nrequire handcrafted parameter tuning on a case-by-case basis. Although some\nrecent works have introduced minimal compute methods for linear inverse\nproblems, a generic lightweight guidance solution to both linear and non-linear\nguidance problems is still missing. To this end, we propose Dreamguider, a\nmethod that enables inference-time guidance without compute-heavy\nbackpropagation through the diffusion network. The key idea is to regulate the\ngradient flow through a time-varying factor. Moreover, we propose an empirical\nguidance scale that works for a wide variety of tasks, hence removing the need\nfor handcrafted parameter tuning. We further introduce an effective lightweight\naugmentation strategy that significantly boosts the performance during\ninference-time guidance. We present experiments using Dreamguider on multiple\ntasks across multiple datasets and models to show the effectiveness of the\nproposed modules. To facilitate further research, we will make the code public\nafter the review process.",
        "updated": "2024-06-04 17:59:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02549v1"
    },
    {
        "title": "Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation",
        "authors": "Mohamed El Amine BoudjoghraAngela DaiJean LahoudHisham CholakkalRao Muhammad AnwerSalman KhanFahad Shahbaz Khan",
        "links": "http://arxiv.org/abs/2406.02548v1",
        "entry_id": "http://arxiv.org/abs/2406.02548v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02548v1",
        "summary": "Recent works on open-vocabulary 3D instance segmentation show strong promise,\nbut at the cost of slow inference speed and high computation requirements. This\nhigh computation cost is typically due to their heavy reliance on 3D clip\nfeatures, which require computationally expensive 2D foundation models like\nSegment Anything (SAM) and CLIP for multi-view aggregation into 3D. As a\nconsequence, this hampers their applicability in many real-world applications\nthat require both fast and accurate predictions. To this end, we propose a fast\nyet accurate open-vocabulary 3D instance segmentation approach, named Open-YOLO\n3D, that effectively leverages only 2D object detection from multi-view RGB\nimages for open-vocabulary 3D instance segmentation. We address this task by\ngenerating class-agnostic 3D masks for objects in the scene and associating\nthem with text prompts. We observe that the projection of class-agnostic 3D\npoint cloud instances already holds instance information; thus, using SAM might\nonly result in redundancy that unnecessarily increases the inference time. We\nempirically find that a better performance of matching text prompts to 3D masks\ncan be achieved in a faster fashion with a 2D object detector. We validate our\nOpen-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios:\n(i) with ground truth masks, where labels are required for given object\nproposals, and (ii) with class-agnostic 3D proposals generated from a 3D\nproposal network. Our Open-YOLO 3D achieves state-of-the-art performance on\nboth datasets while obtaining up to $\\sim$16$\\times$ speedup compared to the\nbest existing method in literature. On ScanNet200 val. set, our Open-YOLO 3D\nachieves mean average precision (mAP) of 24.7\\% while operating at 22 seconds\nper scene. Code and model are available at github.com/aminebdj/OpenYOLO3D.",
        "updated": "2024-06-04 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02548v1"
    },
    {
        "title": "Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning",
        "authors": "Alex Jinpeng WangLinjie LiYiqi LinMin LiLijuan WangMike Zheng Shou",
        "links": "http://arxiv.org/abs/2406.02547v1",
        "entry_id": "http://arxiv.org/abs/2406.02547v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02547v1",
        "summary": "Training models with longer in-context lengths is a significant challenge for\nmultimodal model due to substantial GPU memory and computational costs. This\nexploratory study does not present state-of-the-art models; rather, it\nintroduces an innovative method designed to increase in-context text length in\nmulti-modality large language models (MLLMs) efficiently. We present Visualized\nIn-Context Text Processing (VisInContext), which processes long in-context text\nusing visual tokens. This technique significantly reduces GPU memory usage and\nfloating point operations (FLOPs) for both training and inferenceing stage. For\ninstance, our method expands the pre-training in-context text length from 256\nto 2048 tokens with nearly same FLOPs for a 56 billion parameter MOE model.\nExperimental results demonstrate that model trained with VisInContext delivers\nsuperior performance on common downstream benchmarks for in-context few-shot\nevaluation. Additionally, VisInContext is complementary to existing methods for\nincreasing in-context text length and enhances document understanding\ncapabilities, showing great potential in document QA tasks and sequential\ndocument retrieval.",
        "updated": "2024-06-04 17:59:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02547v1"
    },
    {
        "title": "Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting",
        "authors": "Inkyu ShinQihang YuXiaohui ShenIn So KweonKuk-Jin YoonLiang-Chieh Chen",
        "links": "http://arxiv.org/abs/2406.02541v1",
        "entry_id": "http://arxiv.org/abs/2406.02541v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02541v1",
        "summary": "Recent advancements in zero-shot video diffusion models have shown promise\nfor text-driven video editing, but challenges remain in achieving high temporal\nconsistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting\n(3DGS)-based video refiner designed to enhance temporal consistency in\nzero-shot video editors. Our approach utilizes a two-stage 3D Gaussian\noptimizing process tailored for editing dynamic monocular videos. In the first\nstage, Video-3DGS employs an improved version of COLMAP, referred to as\nMC-COLMAP, which processes original videos using a Masked and Clipped approach.\nFor each video clip, MC-COLMAP generates the point clouds for dynamic\nforeground objects and complex backgrounds. These point clouds are utilized to\ninitialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent\nforeground and background views. Both foreground and background views are then\nmerged with a 2D learnable parameter map to reconstruct full views. In the\nsecond stage, we leverage the reconstruction ability developed in the first\nstage to impose the temporal constraints on the video diffusion model. To\ndemonstrate the efficacy of Video-3DGS on both stages, we conduct extensive\nexperiments across two related tasks: Video Reconstruction and Video Editing.\nVideo-3DGS trained with 3k iterations significantly improves video\nreconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency\n(x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods\non DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring\ntemporal consistency across 58 dynamic monocular videos.",
        "updated": "2024-06-04 17:57:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02541v1"
    }
]