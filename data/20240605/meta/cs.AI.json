[
    {
        "title": "To Believe or Not to Believe Your LLM",
        "authors": "Yasin Abbasi YadkoriIlja KuzborskijAndrás GyörgyCsaba Szepesvári",
        "links": "http://arxiv.org/abs/2406.02543v1",
        "entry_id": "http://arxiv.org/abs/2406.02543v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02543v1",
        "summary": "We explore uncertainty quantification in large language models (LLMs), with\nthe goal to identify when uncertainty in responses given a query is large. We\nsimultaneously consider both epistemic and aleatoric uncertainties, where the\nformer comes from the lack of knowledge about the ground truth (such as about\nfacts or the language), and the latter comes from irreducible randomness (such\nas multiple possible answers). In particular, we derive an\ninformation-theoretic metric that allows to reliably detect when only epistemic\nuncertainty is large, in which case the output of the model is unreliable. This\ncondition can be computed based solely on the output of the model obtained\nsimply by some special iterative prompting based on the previous responses.\nSuch quantification, for instance, allows to detect hallucinations (cases when\nepistemic uncertainty is high) in both single- and multi-answer responses. This\nis in contrast to many standard uncertainty quantification strategies (such as\nthresholding the log-likelihood of a response) where hallucinations in the\nmulti-answer case cannot be detected. We conduct a series of experiments which\ndemonstrate the advantage of our formulation. Further, our investigations shed\nsome light on how the probabilities assigned to a given output by an LLM can be\namplified by iterative prompting, which might be of independent interest.",
        "updated": "2024-06-04 17:58:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02543v1"
    },
    {
        "title": "Parrot: Multilingual Visual Instruction Tuning",
        "authors": "Hai-Long SunDa-Wei ZhouYang LiShiyin LuChao YiQing-Guo ChenZhao XuWeihua LuoKaifu ZhangDe-Chuan ZhanHan-Jia Ye",
        "links": "http://arxiv.org/abs/2406.02539v1",
        "entry_id": "http://arxiv.org/abs/2406.02539v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02539v1",
        "summary": "The rapid development of Multimodal Large Language Models (MLLMs) like GPT-4V\nhas marked a significant step towards artificial general intelligence. Existing\nmethods mainly focus on aligning vision encoders with LLMs through supervised\nfine-tuning (SFT) to endow LLMs with multimodal abilities, making MLLMs'\ninherent ability to react to multiple languages progressively deteriorate as\nthe training process evolves. We empirically find that the imbalanced SFT\ndatasets, primarily composed of English-centric image-text pairs, lead to\nsignificantly reduced performance in non-English languages. This is due to the\nfailure of aligning the vision encoder and LLM with multilingual tokens during\nthe SFT process. In this paper, we introduce Parrot, a novel method that\nutilizes textual guidance to drive visual token alignment at the language\nlevel. Parrot makes the visual tokens condition on diverse language inputs and\nuses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens.\nSpecifically, to enhance non-English visual tokens alignment, we compute the\ncross-attention using the initial visual features and textual embeddings, the\nresult of which is then fed into the MoE router to select the most relevant\nexperts. The selected experts subsequently convert the initial visual tokens\ninto language-specific visual tokens. Moreover, considering the current lack of\nbenchmarks for evaluating multilingual capabilities within the field, we\ncollect and make available a Massive Multilingual Multimodal Benchmark which\nincludes 6 languages, 15 categories, and 12,000 questions, named as MMMB. Our\nmethod not only demonstrates state-of-the-art performance on multilingual\nMMBench and MMMB, but also excels across a broad range of multimodal tasks.\nBoth the source code and the training dataset of Parrot will be made publicly\navailable.",
        "updated": "2024-06-04 17:56:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02539v1"
    },
    {
        "title": "Enhancing predictive imaging biomarker discovery through treatment effect analysis",
        "authors": "Shuhan XiaoLukas KleinJens PetersenPhilipp VollmuthPaul F. JaegerKlaus H. Maier-Hein",
        "links": "http://arxiv.org/abs/2406.02534v1",
        "entry_id": "http://arxiv.org/abs/2406.02534v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02534v1",
        "summary": "Identifying predictive biomarkers, which forecast individual treatment\neffectiveness, is crucial for personalized medicine and informs decision-making\nacross diverse disciplines. These biomarkers are extracted from pre-treatment\ndata, often within randomized controlled trials, and have to be distinguished\nfrom prognostic biomarkers, which are independent of treatment assignment. Our\nstudy focuses on the discovery of predictive imaging biomarkers, aiming to\nleverage pre-treatment images to unveil new causal relationships. Previous\napproaches relied on labor-intensive handcrafted or manually derived features,\nwhich may introduce biases. In response, we present a new task of discovering\npredictive imaging biomarkers directly from the pre-treatment images to learn\nrelevant image features. We propose an evaluation protocol for this task to\nassess a model's ability to identify predictive imaging biomarkers and\ndifferentiate them from prognostic ones. It employs statistical testing and a\ncomprehensive analysis of image feature attribution. We explore the suitability\nof deep learning models originally designed for estimating the conditional\naverage treatment effect (CATE) for this task, which previously have been\nprimarily assessed for the precision of CATE estimation, overlooking the\nevaluation of imaging biomarker discovery. Our proof-of-concept analysis\ndemonstrates promising results in discovering and validating predictive imaging\nbiomarkers from synthetic outcomes and real-world image datasets.",
        "updated": "2024-06-04 17:54:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02534v1"
    },
    {
        "title": "ReLUs Are Sufficient for Learning Implicit Neural Representations",
        "authors": "Joseph ShenoudaYamin ZhouRobert D. Nowak",
        "links": "http://arxiv.org/abs/2406.02529v1",
        "entry_id": "http://arxiv.org/abs/2406.02529v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02529v1",
        "summary": "Motivated by the growing theoretical understanding of neural networks that\nemploy the Rectified Linear Unit (ReLU) as their activation function, we\nrevisit the use of ReLU activation functions for learning implicit neural\nrepresentations (INRs). Inspired by second order B-spline wavelets, we\nincorporate a set of simple constraints to the ReLU neurons in each layer of a\ndeep neural network (DNN) to remedy the spectral bias. This in turn enables its\nuse for various INR tasks. Empirically, we demonstrate that, contrary to\npopular belief, one can learn state-of-the-art INRs based on a DNN composed of\nonly ReLU neurons. Next, by leveraging recent theoretical works which\ncharacterize the kinds of functions ReLU neural networks learn, we provide a\nway to quantify the regularity of the learned function. This offers a\nprincipled approach to selecting the hyperparameters in INR architectures. We\nsubstantiate our claims through experiments in signal representation, super\nresolution, and computed tomography, demonstrating the versatility and\neffectiveness of our method. The code for all experiments can be found at\nhttps://github.com/joeshenouda/relu-inrs.",
        "updated": "2024-06-04 17:51:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02529v1"
    },
    {
        "title": "RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots",
        "authors": "Soroush NasirianyAbhiram MaddukuriLance ZhangAdeet ParikhAaron LoAbhishek JoshiAjay MandlekarYuke Zhu",
        "links": "http://arxiv.org/abs/2406.02523v1",
        "entry_id": "http://arxiv.org/abs/2406.02523v1",
        "pdf_url": "http://arxiv.org/pdf/2406.02523v1",
        "summary": "Recent advancements in Artificial Intelligence (AI) have largely been\npropelled by scaling. In Robotics, scaling is hindered by the lack of access to\nmassive robot datasets. We advocate using realistic physical simulation as a\nmeans to scale environments, tasks, and datasets for robot learning methods. We\npresent RoboCasa, a large-scale simulation framework for training generalist\nrobots in everyday environments. RoboCasa features realistic and diverse scenes\nfocusing on kitchen environments. We provide thousands of 3D assets across over\n150 object categories and dozens of interactable furniture and appliances. We\nenrich the realism and diversity of our simulation with generative AI tools,\nsuch as object assets from text-to-3D models and environment textures from\ntext-to-image models. We design a set of 100 tasks for systematic evaluation,\nincluding composite tasks generated by the guidance of large language models.\nTo facilitate learning, we provide high-quality human demonstrations and\nintegrate automated trajectory generation methods to substantially enlarge our\ndatasets with minimal human burden. Our experiments show a clear scaling trend\nin using synthetically generated robot data for large-scale imitation learning\nand show great promise in harnessing simulation data in real-world tasks.\nVideos and open-source code are available at https://robocasa.ai/",
        "updated": "2024-06-04 17:41:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.02523v1"
    }
]