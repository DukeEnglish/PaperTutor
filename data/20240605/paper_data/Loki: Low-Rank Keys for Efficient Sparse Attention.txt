Loki: Low-Rank Keys for Efficient Sparse Attention
PrajwalSinghania SiddharthSingh ShwaiHe
prajwal@umd.edu ssingh37@umd.edu shwaihe@umd.edu
SoheilFeizi AbhinavBhatele
sfeizi@cs.umd.edu bhatele@cs.umd.edu
DepartmentofComputerScience
UniversityofMaryland
Abstract
Inference on large language models can be expensive in terms of the compute
andmemorycostsinvolved,especiallywhenlongsequencelengthsareused. In
particular,theself-attentionmechanismusedinsuchmodelscontributessignifi-
cantlytothesecosts,whichhasresultedinseveralrecentworksthatproposesparse
attentionapproximationsforinference. Inthiswork,weproposetoapproximate
theself-attentioncomputationbyfocusingonthedimensionalityofkeyvectors
computedintheattentionblock. Ouranalysisrevealsthatthekeyvectorsliein
asignificantlylower-dimensionalspace,consistentlyacrossseveraldatasetsand
models. Exploiting this observation, we propose Loki, a novel sparse attention
methodthatranksandselectstokensintheKV-cachebasedonattentionscores
computed in low-dimensional space. Our evaluations show that Loki is able to
maintaintheefficacyofthemodelsbetterthanotherpopularapproximationmeth-
ods,whilespeedinguptheattentioncomputationduetoreduceddatamovement
(load/store)andcomputecosts.
1 Introduction
As the sizes of large language models (LLMs) increase, deploying them for efficient inference
remainsasignificantchallenge,primarilyduetothecomputationandmemoryaccessbottlenecksin
theself-attentionblock[31]. Thesechallengesariseduetotheauto-regressivenatureofattention,
wheretheoutputisgeneratedtokenbytoken. Ateachgenerationstep,theentireprecedingstate,
representedbythekey-value(KV)cache[23],mustbefetchedfrommemory. Thisstatecan,attimes,
belargerthanthemodelparametersthemselves[17]. ThecostofreadingtheKVcachefromthe
GPUDRAMtotheregistersateachgenerationstepisprohibitivelyexpensive. Duringinference,this
KV-cacheread/writecostscalesquadraticallywiththesequencelength,asopposedtotrainingwhere
itscaleslinearly. Moreover,thecomputationalcostofmatrixmultiplicationsintheattentionlayers
alsoscalesquadraticallywiththesequencelength[36].
Severalstrategies[38,25,18]havebeenproposedtoaddressthischallengebyreducingthecom-
putationalcomplexityand/ormemorydemandsassociatedwiththeself-attentionmechanism. One
promisingcategoryofapproachesfocusesonapproximatingattention,employingtechniquessuchas
quantization(compressionalongthebitdimension)orusingasubsetofthethenumberoftokensin
theKVcache[9](sparseattention).
Whileothersparseattentionmethodseitherpermanentlythrowingawaytokens [38]orhaveafixed
sparsitypattern[33],wefocusonusingthekeys(inthefeaturedimension)todecidewhichtokens
Preprint.Underreview.
4202
nuJ
4
]GL.sc[
1v24520.6042:viXraDimensionality of Attention Keys
140
128
120 Ge an ne dra te PCA
save
100
C Dali ab tr aa st eio tn LLM PCA Transform
80
Keys
60
Query
40
20
X X Ap Ap tr teo nx tim ioa nt e
0 Keys Scores
Llama2- Ll7 aB ma2- Ll1 a3 mB a2-7 Ll0 aB ma3- Ll8 aB ma3-7 M0 isB tral M- i7 xB tral- Mi8 xx t7 rB al-8 Px y2 t2 hiB a-6.9B for ST co op r- iK ng Key
Figure1: (Left)Rankatwhich90%ofthevarianceisexplainedaveragedacrossalllayersandheads
fordifferentmodels. (Right)OverviewofLoki.
mightbeimportant. WehypothesizethatthekeysintheKVcachelieinalow-dimensionalspace.
WebeginwithexploringtheintrinsicdimensionalityofthekeyvectorsintheKVcache. Weperform
principal component analysis (PCA) [15] on the key vectors within each layer and head using a
calibrationdatasettostudytheirdimensionality. AsshowninFigure1(left),wefindthatthekey
vectorsindeedlieinalow-dimensionalspaceforseveralpopularandrecentmodelslikeLlama-3[2]
andMixtral[14].
Next,weexploitthelowdimensionalityofthekeyvectorstodevelopasparseattentionmethodthat
doesnotsignificantlysacrificemodelqualityandalsoreducesdatamovementandcomputecosts.
Wecallourmethod‘Loki’. WestorealltheprincipalcomponentsofthePCA-transformedkeys,but
usethefirstdtocomputeapproximateattentionscoresforpickingthetop-k tokens. Wethenuse
thedimensionalityonlyfortheselectedkeystocomputethefinalattentionscores. Figure1(right)
providesanoverviewofLoki.
OurtheoreticalcomplexityanalysisshowsthatLokicanprovidesignificantspeedupsintheattention
step. However,empirically,thisrequiresefficientimplementationofLokitominimizedatamovement
intheadditionaloperationsintroduced. Thus,weimplementoptimizedmatrixmultiplicationkernels
forLoki,leadingtoaspeedupofupto40%overbaseattentionfortheLlama2-13Bmodel. Forthis
speedupsetting,theaveragedegradationinmodelaccuracy(measuredacross6differentbenchmarks
and8differentmodels)isonly6.8%points.
Ourcontributionscanbesummarizedasfollows:
• Detailed analysis showing the intrinsic low-dimensionality of keys in self-attention, its
variationacrosslayersfordifferentmodels,andconsistencyacrossdifferentdatasets.
• Loki: asparseattentionmethodthatexploitstheaforementionedlowdimensionalityofkeys
tospeedupattentioncomputationwithoutsacrificingmodelquality.
• OptimizedkernelsforefficientimplementationofLokiinPyTorch.
• EvaluationofLokionmultipleLLMsanddownstreamtasks,showingthatitcanachieve
significantspeedupswithminimaldegradationinmodelquality.
2 BackgroundandRelatedWork
The attention mechanism [31] is at the core of the transformer architecture. Consider a single
attentionqueryheadwithheaddimensionD,processinganinputtokensequenceoflengthS. During
auto-regressivegeneration,theoutputoftheattentionheadiscalculatedas:
(cid:16)qK⊤(cid:17)
y =softmax √ ·V (1)
D
whereq∈R1×D isthequery,andK∈RS×D andV∈RS×D arethekeyandvaluecachesrespec-
tively. Additionally,newertransformermodelsaddRotationalPositionalEmbeddings(RoPE)[28]to
thekeysandquery,beforecomputingtheattentionscores. Sinceeveryqueryattendstoallpastkeys,
theattentionmechanismhasaquadraticcomplexityO(S2)inthenumberoftokens.
2
ecnairaV
denialpxE
%09
ta
knaR2.1 RelatedWork
Numerousstudieshaveexploredthelow-rankstructuresintransformersforvariouspurposes. Lin-
former[32]demonstratedthattheattentionscorematrixislow-rankandproposedalternativeattention
formulationsthroughlow-rankfactorizationsduringtrainingforlinearcomputationalcomplexity.
LoRA[11]showedthattheweightsoftransformersandtheupdatestotheseweightsduringfine-
tuningresideinalow-dimensionalsubspace. Tothebestofourknowledge, ourworkisthefirst
to study the intrinsic low dimensionality of the attention keys themselves and demonstrate the
generalizabilityofthislow-dimensionalstructureinapost-trainingsetting(fornaturallanguagedata).
Sparse-transformers[5]wasoneofthefirstworkstointroduceasparse-attentionmethodemploying
fixedorstridedsparsitypatternsintheattentionmechanism. Reformer[16]usedlocally-sensitive
hashing to compute attention scores in a sparse manner. Performer [6] used positive orthogonal
random features to approximate the attention mechanism. Unlike these methods, which require
trainingorfine-tuning,ourapproachoperatesentirelypost-trainingwithoutanyfine-tuning.
Anothercategoryofsparseattentionmethodsemploytokenevictionpoliciestopermanentlydelete
tokensfromtheKV-cachebasedonsomeheuristic. StreamingLLM[33]introducedtheconceptof
attentionsink,usinginitialtokensandarollingKV-cacheforprocessinginfinite-lengthsequences.
Zhangetal.[38]retainonly"HeavyHitters"tokensintheKV-cachebasedonaccumulatedattention
scores. Liu et al. [18] propose Scissorhands, which prioritizes important tokens based on the
"PersistenceofImportanceHypothesis". Geetal.[8]proposeanadaptiveevictionpolicyforeach
transformerlayer. Thesemethodsareeffectiveinreducingthememoryandcomputefootprintof
theattentionbutsufferfrompermanentlossofinformationthatcanleadtogenerationanon-trivial
degradationinmodelquality.Ourmethoddoesnotinvolveanypermanentlossofinformationwiththe
trade-offofnotreducingthememoryfootprint. Quantization-basedapproximateapproaches[12,21]
arecomplimentarytoourworkandcanbeappliedintandem.
SPAR-QAttention[25]isarecentworkthatinspiresourapproach.Theyusethemaximummagnitude
dimensionsofthequeries,andthecorrespondingdimensionsofthekeystocomputeapproximate
attentionscores,andthenusethefullattentionscoresforthetop-kkeys. However,themaindrawback
oftheirmethodisthatitrequiresexpensivenon-contiguousindexingofcolumnsofthekeysmatrix.
Further,theystoretwocopiesofthepastkeysleadingtoa50%increaseinmemoryusage. Lokidoes
notrequireadditionalmemory,andthenaturalorderingoftheprincipalcomponentsallowsoneof
thetwoindexingoperationstobereplacedwithamoreefficientslicingoperation.
3 DimensionalityAnalysisofAttentionKeys
RQ1: Doattentionkeysintransformermodelslieinasignificantlylowerdimensionalspace?
Inthissection,weanalyzewhethermostofthevarianceinthekeyscanbeexplainedusingdimensions
d<D,whereD ∈Z+isthefulldimensionalityofthekeys.
3.1 Setup
Toinvestigatethedimensionalityofattentionkeys,werun11transformer-basedmodels: Llama-2
7B/13B/70B [30], Llama-3 8B/70B [2], TinyLlama-1.1B [37], Pythia-6.9B [4], Mistral-7B [13],
Mixtral-8x7B/8x22B[14],andPhi3-Mini-4K[20]on3popularEnglishlanguagedatasetsdatasets:
WikiText-2[19](ValidationSplit),C4[24](CustomSplit),andBookCorpus[39](CustomSplit).
Customsplitsareused fordatasetswherethevalidationsplitisnotavailable. Werunperplexity
evaluationonthesedatasetsandsavethegeneratedattentionkeys,beforeandaftertheapplicationof
rotaryembeddings[28],referredtoaspre-rotaryandpost-rotarykeys,respectivelythroughoutthe
paper. WethenperformPCAonallthekeysgeneratedforeachlayerandheadindividually.
Themetricweuseinouranalysisistherankatwhichv%ofthevarianceisexplainedbytheprincipal
components. Wecalculatethismetricforeachlayerandheadofthemodelsasfollows:
 
 (cid:88)d 
Rank @v =min d∈Z+ : λj ≥v/100 (2)
l,h l,h
 
j=1
3where,λj isthejthnormalizedeigenvalueofthecovariancematrixofthekeysforthelthlayerand
l,h
hthhead. WeaveragethismetricranksacrossallheadsofthelthlayerandrefertoitasRank @v.
l
3.2 AnalysisandDiscussion
140Llama2-7B Attention Keys (Pre-Rotary) 140Llama2-7B Attention Keys (Post-Rotary) 140Llama3-70B Attention Keys (Pre-Rotary) 140Llama3-70B Attention Keys (Post-Rotary) 11 22 08 11 22 08 11 22 08 Wikitext 11 22 08 Wikitext
C4 C4
100 100 100 BookCorpus 100 BookCorpus
80 80 80 80
60 60 60 60
40 Wikitext 40 Wikitext 40 40
C4 C4 20 BookCorpus 20 BookCorpus 20 20
0 0 5 10 15 20 25 30 0 0 5 10 15 20 25 30 00 20 40 60 80 00 20 40 60 80
Layer Layer Layer Layer
140Mixtral-8x7B Attention Keys (Pre-Rotary) 140Mixtral-8x7B Attention Keys (Post-Rotary) Phi3-Mini-4K Attention Keys (Pre-Rotary) Phi3-Mini-4K Attention Keys (Post-Rotary)
11 22 08 W C4ikitext 11 22 08 W C4ikitext
100 BookCorpus 100 19060 BookCorpus 19060
80 80 80 80
60 60 60 60
40 40 Wikitext 40 40 Wikitext
C4 C4 20 20 BookCorpus 20 20 BookCorpus
00 5 10 15 20 25 30 00 5 10 15 20 25 30 00 5 10 15 20 25 30 00 5 10 15 20 25 30
Layer Layer Layer Layer
Figure2:Rankatwhich90%ofthevarianceisexplainedforpre-rotaryandpost-rotarykeysproduced
byeachlayeraveragedacrossallheads(Rank @90)fordifferentmodels. Weobservethatallmodels
l
exhibitsignificantlylowrank(Fulldimensionalityis128or96)consistentlyacrossalldatasets.
Figure 1 (left) shows the average Rank @90 averaged across all layers for models with full key
l
dimensionalityof128. Wecanseethattheaveragerankissignificantlylowerthanthefulldimen-
sionalityofthekeysforallmodels. Divingdeeper,wepresentalayer-wiseanalysisforafewmodels:
Llama2-7B,Llama3-70B,Mixtral-8x7B,andPhi3-Mini-4KinFigure2. Theresultsfortheother
modelsaresimilarandcanbefoundintheappendixA.1.
Weobservethatthedimensionalityofthekeys(bothpre-rotaryandpost-rotary)issignificantlylower
thanthefulldimensionalityofthekeysacrossallcalibrationdatasets. Furthermore,theRank @90
l
foraparticularlayerisconsistentacrossdatasets,forallcombinationsofmodelsanddatasets. This
indicatesthatthelower-dimensionalstructureofthekeysisconsistentwhencalculatedusingdifferent
calibrationdatasets. Anothertrendweobserveisthattheinitiallayersofmostmodelshaveavery
lowrank,ascomparedtothelaterlayers,andthistrendisparticularlyprominentforthepre-rotary
keys. Lastly,wealsoobservethatformostmodels,theaverageofRank @90acrossalllayersis
l
lowerforpre-rotarykeysascomparedtopost-rotarykeys, indicatingthattherotaryembeddings
increasethedimensionalityofthekeys. Furtheranalysisonthevariationoftherankacrossdifferent
headswithinalayerandacrossdifferentlayerswithinamodelcanbefoundintheappendix.
Theseresultsindicatetheexistenceofthefollowingproperties:(1)Thekeysproducedbytheattention
layersoftransformermodelslieinasignificantlylower-dimensionalspace.(2)Thelower-dimensional
structureofthekeysisconsistentacrossdifferentcalibrationdatasets.(3)Rotaryembeddingsincrease
thedimensionalityofthekeysformostmodels. WenowusethefirsttwopropertiestoproposeLoki,
anefficientsparse-attentionmethod.
4 Method: Loki
RQ2: Canthelowerintrinsicdimensionalityofthekeysbeusedtooptimizeattention?
Inthissection,wepresentLoki,anovelsparseattentionmethodleveragingPCAfortop-kattention
computation (i.e. computing attention with a k sized subset of the KV-cache tokens). We first
provesometheoreticalpropertiesofattentioninthePCA-transformedspace,andthenpresentthe
Lokialgorithm. Wealsopresentamemory-efficientkernelthetop-kattentioncomputation,crucial
forempiricalspeedupswithourmethod.
4
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR4.1 PropertiesofAttentioninthePCATransformedSpace
First,weintroducesomelemmasthatprovidetherationaleforourapproachtocomputingattention
inthePCA-transformedspace.
Lemma4.1. LetP∈RD×D bethePCAprojectionmatrixcalibratedofflineonadataset. Assuming
wearegeneratingtheSth tokeninthesequence,letq ∈ R1×D bethequeryvectorfortheSth
S
token,K ∈RS×D bethekeyvectors,includingthepast(S−1)keysandthecurrentkey. Then,the
:S
attentionscorescomputedusingthePCA-transformedqueryandkeysareequivalenttotheattention
scorescomputedusingtheoriginalqueryandkeys.
Proof. Letq′ =q PandK′ =K PbethePCAtransformedqueryandkeyvectors. Focusing
S S :S :S
onthedotproducttermintheattentioncomputation(Equation1),wehave:
q KT =q (K′ PT)T [invertingthePCAtransform]
S :S S :S
=q ((PT)TK′T)=(q P)K′T =q′ K′T
S :S S :S S :S
ItisimportanttonoteherethatLemma4.1holdsforanyorthogonalP.
Lemma 4.2. Let K′ ∈ RS×d (d < D) be the reduced dimension key vectors obtained by
:S,:d
projectingthekeyvectorsontothefirstdprincipalcomponentsofP. Then,theattentionscores
computedusingK′ areagoodapproximationofthetheactualattentionscores.
:S,:d
Proof. LetR ∈Rd×D beanorthogonaltransformationthattransformsthekeysintothereduced
:d
dimension space as L = K R . Our objective is to minimize the following reconstruction
:S,:d :S :d
error:
min||q KT −q (L RT)T||2 (3)
S :S S :S,:d :d 2
R:d
UsingCauchy-Schwarzinequality,wehave:
||q KT −q (L RT)T||2 ≤||q ||2||KT −(L RT)T||2 (4)
S :S S :S,:d :d 2 S 2 :S :S,:d :d 2
WechangeourobjectivetominimizetheupperboundontheRHSinsteadoftheoriginalobjective.
WeknowthatPCAminimizesthereconstructionerror(2ndtermintheRHS)amongalltheorthogonal
transformations. Thus,itfollowsthattheoptimalvalueofR∗ =P ,andL∗ =K′
:d :d :S,:d :S,:d
SinceweminimizeanupperboundwhenprovingLemma4.2,itispossiblethatsomeothertransfor-
mationmightgiveabetterapproximationtothedotproduct. Thus,inourexperiments,weusePCA
transformscomputedonboththepre-rotaryandpost-rotarykeysascandidatetransformations.
Using the above lemmas and our dimensionality analysis showing that the key vectors have low
intrinsicdimensionality,wearenowreadytopresentthealgorithmforLoki.
4.2 AlgorithmandComplexityAnalysis
In this section, we propose the algorithm for our PCA-based Top-K Attention approach (Loki).
Previous works [34, 29] have shown that attention scores for a query are highly concentrated on
a small subset of keys. This observation has led to several methods to compute attention using
onlythetop-kkeys. However,thesepreviousworkseithercomputetheexactattentionscoresand
thenselectthetop-kkeys[10]orcomputenon-exactscoresbuthavesignificantlyhighermemory
requirements [25]. Loki alleviates these issues by computing approximate attention scores (for
rankingthekeys)inthereducedlower-dimensionalspace,withoutanysignificantincreaseinmemory
requirements.Algorithm1showsourLokimethod.Line5ofthealgorithmcomputestheapproximate
attentionscoresusingdprincipaldimensionsofthequeryandkeyvectors. Lines6-7selectthetop-k
keysbasedontheapproximateattentionscores. Line8computestheexactattentionscoresusingthe
selectedtop-kkeys,directlyinthetransformedspace(Lemma4.1).
ComputeandMemoryAnalysis: Forvanillaattention, thecomplexityofcomputingq KT is
S :S
O(DS)andthecomplexityofmultiplyingthevalueswiththeattentionscoresisO(DS). ForLoki,
thecomplexityofcalculatingtheapproximateattentionscores(Line5)isO(dS). Thecomplexityof
selectingthetop-Kkeys(Lines6-7)isapproximatelyO(Slog(S)+k).Thecomplexityofcalculating
theexactattentionscoresandmultiplyingwiththevalues(Line8-9)isO(2Dk). Additionally,the
5Algorithm1Loki
Require: AttheSthstep-Input: x ∈R1×D,KV-Cache: K′ ,V ∈R(S−1)×D,Projection
S :S−1 :S−1
Matrix: P∈RD×D,d,k
1: functionLoki-ATTENTION(x S,K′ :S−1,V :S−1,P,d,k)
2: q S,k S,v S ←computeQKV(x S)
3: q′
S
←q SP,k′
S
←k SP
4: K′ :S ←concat(K′ :S−1,k′ S),V :S ←concat(V :S−1,v S)
5: A approx ←q′ S,:d(K′ :S,:d)T
6: indices←topk(A approx,k)
7: K′ :S′ ←K′ :S[indices],V :′ S′ ←V :S[indices]
8: A exact ←softmax(q′ S( √K′ :S′)T )
D
9: returnA exactV :′ S′
10: endfunction
complexity of projections into the PCA space (Line 3) is O(2D2). Assuming the complexity of
selecting the top-k keys is small compared to the other operations, the overall complexity of the
algorithmisO(dS+2Dk+2D2). Then,wehave:
2DS 1 1
speedup= = ≈ (givenD <<S) (5)
dS+2Dk+2D2 d/2D+k/S+D/S d /2+k
f f
where, d = d/D andk = k/S. ThememoryrequirementoftheKV-Cacheisthesameasthe
f f
originalattention,withasmalloverheadofstoringthePCAtransformationmatrix.
4.3 ImplementationinTriton
PerformingLokiefficientlyinvolvescomplexindexingoperationswithintheKV-Cache(lines5and7
ofAlgorithm1).StandardPyTorchoperationscreatetemporary,densecopiesoftheKV-Cachedatain
memory,leadingtoslowdownsduetoexpensivememoryaccess. Toalleviatethisissue,wedevelop
optimizedkernelsinTriton[1]forthethreematrixmultiplicationoperationsinLoki. Ourkernelscan
directlyaccessrelevantsubsetsoftheKV-Cache(bothfeatureandsequencedimensions)andperform
computationswithinGPUregisters. Thiseliminatestheneedforcreatingdensecopies,significantly
improvingperformance. OurapproachbuildsonSPAR-Q[25],whichintroducedsimilarkernels
fortop-kattentioncalculations. However,weidentifiedandaddressedinefficienciesintheSPAR-Q
kernels,whichresultedinspeedupsofnearly2−3×incertainscenarios. (seeAppendixC).
5 Evaluation
In this section, we present our experimental setup and evaluation of Loki via three studies: (1)
Comparing Loki with baselines on common ML benchmarks, (2) Examining its generalizability
acrossvariouscalibrationdatasets,and(3)Benchmarkingitscomputationalefficiencyagainstvanilla
attentioncomputation.
5.1 ExperimentalSetup
ForourMLbenchmarking,weevaluateourmethodusingtwoapproaches-perplexityevaluation
onWikiText-2[19]dataset(testsplit)anddownstreamtaskperformanceusingtheLLM-harness
benchmark[7]. Forthedownstreamtasks,wechoosethesametasksastheHuggingFaceOpenLLM
leaderboard [3]: Themetricsusedforthesetasksareidenticaltotheleaderboard
Wecompareourmethodagainst3baselines-fullattentionwithoutanyapproximations,theexact
TopKapproachwhichcomputestheexactattentionscoresandthenusesthetop-ktokenstocompute
thefinaloutput,andH2O[38]methodwhichisapopulartoken-evictionmethod. Forthesecompar-
isons,weshowtheresultswithabudgetsizeofk =0.25and0.125. Forourmethod,weadditionally
f
used =0.25and0.125. Thisconfigurationofourrepresentsa2.6xtheoreticalspeedup. Table1
f
providesanoverviewofthebaselinemethodsandtheassociatedbudgetterms. H2O’sbudgetwas
splitequallybetweentheheavyhitterandrecenttokens,aspertheoriginalpaper. ForH2O,wewere
6Table1: Explanationofkey-budgetanddimensionality(dim.) forbaselinesandourmethod,along
withthespeedupandmemorysavings.
Method Budget Dim. Description Speedup MemorySavings
ExactTop-K kf Full kffractionofkeysselectedusingexactattentionscores No No
H2O kf Full kffractionofkeys&valuesselectedusingH2Opolicy k1
f
k1
f
Loki kf df k scf orf er sac ct oio mn puo tf edke wy is th& dv fa flu rae cs tis oe nle oc fte fud llu ds ii mng ena st it oe nn at li io tn
y
(df/21 )+kf No
Loki vs Full Attention
12 100
Full Attention
10 Loki (pre, kf=0.25, df=0.25)
80
Loki (post, kf=0.25, df=0.25)
8 Loki (pre, kf=0.125, df=0.5)
60
Loki (post, kf=0.125, df=0.5)
6
40
4
20
2
0 0
Llama2-7B Llama2-13B Llama3-8B Llama3-70B Mistral-7B Mixtral-8x7B Llama2-7B Llama2-13B Llama3-8B Llama3-70B Mistral-7B Mixtral-8x7B
Figure3: EvaluationofLokionperplexity(left)anddownstreamtasks(right)fordifferentmodels.
Fordownstreamtasks,weshowtheaverageperformanceacrossalltasksmentionedin5.1.
unabletoruntheGSM8Ktaskasthetheauthor’sMLbenchmarkingcodewastoomemoryintensive
torunforthattask. Fortheaforementionedexperiments, wegeneratePCAtransformsusingthe
WikiText-103dataset. Forthegeneralizabilitystudy,wecomparetheresultsofourmethodwithPCA
transformsfromdifferentcalibrationdatasets: WikiText-103[19],C4[24],andBookCorpus[39].
Additionally,wealsobenchmarkourtritonbasedimplementationofPCAforrunningattentionina
Llama2-13B-likesetup(samehiddensizeandnumberofheads)forvariouspromptandgeneration
lengths,anddemonstratespeedupsovervanillaattention.
AllexperimentsarerunonNVIDIAA100GPUswith40and80GBofmemoryonthePerlmutter[22]
supercomputer. Forlargermodels,weuseAxoNN[26,27]toshardthemodelacrossmultipleGPUs.
5.2 ResultsandDiscussion
LetusbeginourdiscussionwithFigure3,showingtheperplexity(left)anddownstreamtask(right)
evaluation results for Loki on Llama2-7B/13B, Llama3-8B/70B, Mistral-7B, and Mixtral-8x22B
models. We’llfocusontheLlama2-7Bmodel,comparingpre-rotary(lightgreen/purple)andpost-
rotary(darkgreen/purple)PCAtransformsfordifferentk andd values. ForLlama2-7B,wesee
f f
thattheperformanceofbothcandidatetransformsissimilarandclosetothefullattentionmodel.
ThistrendisconsistentacrossallthemodelsexceptforLlama3-8B/70BandMistral-7B,wherethe
post-rotaryPCAtransformperformssignificantlyworsethanthepre-rotaryone. ForLlama3-8B,
perplexityjumpsfromabout5toover10,asignificantdeclinenotseenwiththepre-rotarytransform.
Mistral-7Bshowsasimilarpattern. Thisisasurprisingobservationasonemightexpectthepost-
rotaryPCAtransformtoperformbetter. Wedonothaveaclearexplanationforthisobservationand
furtherinvestigationintoRoPEembeddingsiswarrantedtounderstandthisbehavior. Nevertheless,
weseethatatleastoneofthePCAtransformsperformswellforanymodel,andthisisanimportant
hyperparametertotunewhenusingourmethod. Forsubsequentresults,weonlyshowresultsfrom
thebetter-performingtransformationforeachmodel. Comparingdifferent(k ,d )settings,wesee
f f
thatusingk =0.25andd =0.25(green),isbetterthanusingk =0.125andd =0.5(purple)for
f f f f
allmodels. Thesetwosettingsbalancespeedandperformancewell,withthefirstbeingsuperior.
Next,wecomparetheperformanceofLokiwithvariousbaselines,usingk =0.25forallmethods
f
andd =0.25forours. Table2showstheperplexityresultsforLlama2-7B,Llama2-13B,Llama3-8B,
f
andMistral-7B.Loki’sperplexitydropiswithin0.1offullattentionacrossallmodels,athreshold
consideredacceptableforattentionmechanismapproximations[35]. Incontrast,H2O’sperplexity
drop exceeds 0.1, nearing 0.2 for all models. Figure 4 confirms this trend in downstream task
7
ytixelpreP
ycaruccA
ksaT
egarevATable2: Perplexity(Lowerisbetter)evaluationofLokiandbaselinesfordifferentmodels
Method k d Speedup Llama2-7B Llama2-13B Llama3-8B Mistral-7B
f f
FullAttention - - No 5.1101 4.5680 5.5696 4.9140
Exact-TopK 0.25 - No 5.1809 4.5926 5.5716 4.9171
H2O 0.25 - Yes 5.2810 4.7009 5.7056 5.0805
Loki 0.25 0.25 Yes 5.2017 4.6102 5.6648 4.9233
Full Attention Exact-TopK (kf=0.25) Loki (kf=0.25, df=0.25) H2O (kf=0.25)
Loki vs Baselines
Llama2-7B Llama2-13B Llama3-8B Mistral-7B
100 100 100 100
75 75 75 75
50 50 50 50
25 25 25 25
0 0 0 0
Hellaswag TQA Winogrande ARC MML (U
w/A ov g
GSM8K) Hellaswag TQA Winogrande ARC MML (U
w/A ov g
GSM8K) Hellaswag TQA Winogrande ARC MML (U
w/A ov g
GSM8K) Hellaswag TQA Winogrande ARC MML (U
w/A ov g
GSM8K)
Figure4: DownstreamTaskPerformancehigherisbetterforLokiandbaselinesfordifferentmodels.
GSM8Kisexcluded,aswewereunabletorunH2Oforthistask.
evaluation. Lokiperformssimilarlytofullattentionforallmodels,exceptLlama3-8B,wherethe
performanceisnotablyworse,thoughstillbetterthanH2O.Importantly,onthechallengingMMLU
task,LokidegradeslessthanH2O.
ComparingLokiwithExact-TopK,wefindsimilarperformanceforLlama2-7B,Llama2-13B,and
Mistral-7B.Exact-TopKrepresentstheupperperformanceboundforLokiifitcouldperfectlyselect
thetop-ktokens. TounderstandwhyLokiworkswell,weexaminedthetop-kagreementbetween
attentionscoresfromlow-dimensionalkeysandexactattentionscores. Figure5showsaJaccard
similaritybetweenthetop-ktokensselectedbybothmethodsacrossalllayersandheadsforLlama2-
7B.Forthesettings: (k =0.25,d =0.25)and(k =0.125,d =0.5),evaluatedinFigure3,we
f f f f
seetheJaccardsimilarityisbetween0.85and0.95,validatingthattheLokiisabletoselectthetop-k
tokenswithhighaccuracy.
Top-k Agreement b/w Exact Loki Generalization over Calibration Datasets
1.0 and Reduced Dim. Attention Scores 100
BookCorpus
0.8 5 80 C W4 ikitext
4
0.6 60
3
40 0.4 2
0.2 kf 0.125 1 20
0.25
0.5 0 0
0.0
0.125 0 d.2 f5 0.5
Llama2-7B Llama2-13B Llama3-8B Mistral-7B Llama2-7B Llama2-13B Llama3-8B Mistral-7B
Figure5: (Left)Top-kagreementbetweenLokiandExact-TopKmethodsforLlama2-7B.(Right)
PerformanceofLokiusingtransformationsderivedfromdifferentcalibrationdatasets.
WenowturnourattentiontothegeneralizabilityofthePCAtransformationsusedinourmethod.
Figure5(right)showstheperformanceofLokiusingPCAtransformationsderivedfromdifferent
calibration datasets (k = 0.25,d = 0.25). For a given model, we consistently use the best-
f f
performingcandidatePCAtransformation(preorpost),basedontheperplexityevaluationinFigure
3,forallthecalibrationdatasets. WeseethattheperformanceofLokiisconsistentacrossdifferent
calibrationdatasets,indicatingthatthePCAtransformationsusedinourmethodaregeneralizable.
ThisisanimportantobservationasitshowsthatthePCAkeyscanbegeneratedusingavarietyof
calibrationdatasetsandstillachievegoodperformance.
8
ycaruccA
ksaT
ytiralimiS
draccaJ
ytixelpreP
ycaruccA
ksaT
egarevALlama2-13b - Attention w/ Llama2-13b - Attention w/o Attention Times for Llama2-13b Attention Times for Llama2-13b
KV-Cache Updates KV-Cache Updates with a Prompt of Length 2048 with a Prompt of Length 3072
0.8 5 1
3 K AV ct- uca ac l h ce o map pp ue tn ed 0.6 q q. .k kT T ( (t to op p- -d kff )) 12 V ka f=n 0ill .2a 5 A , t dte f=n 0ti .o 25n 0.5
2 s to. pv - k(top-kf) 0.5 kf=0.125, df=0.5 0.2
0.4 other 0.2
0.1
0.1
1 0.2 0.05 0.05
64 128 256 512 64 128 256 512
0 0.0
PCA-TopK Vanilla PCA-TopK Vanilla Number of Generated Tokens Number of Generated Tokens
Figure 6: Benchmarking vanilla attention and Loki attention in Llama2-13B using huggingface
transformers. Weuseapromptlengthof3072andgenerate1024tokensforthetwobreakdownplots
ontheleft. Forallfigures,weuseabatchsizeof16. Foreachdatapoint,wedo10runsandreportthe
averagetime. Acrossallruns,wefoundthestd.dev.intimestobelessthan0.05percentofthemean.
AnalyzingLlama2-13BwithHuggingFaceTransformersexposedaninterestingbottleneck(Figure6,
leftmost). Regardlessoftheattentiontype(vanillaorLoki),morethan80%oftheinferencetime
isconsumedwithintheHuggingFaceframeworkforappendingkey-valuepairsforthelatesttoken
totheKV-Cache. Thissharedbottleneckminimizestheoverallperformanceimprovementofour
optimizations. WehypothesizethatusingamoreadvancedinferencesystemlikevLLM[17]with
efficientKV-Cachemanagementcouldsignificantlyreducethisappendtime,butleavethatexploration
forfuturework. Toisolatetheimpactofouroptimizations,theremainingplotsinFigure6focus
solelyontheattentioncomputationtime,excludingtheKV-Cacheappendtime.
InthesecondfromleftplotofFigure6,weobservethatLokispeedsupthetotalcomputetimespent
inattention(sansKV-Cacheappends)bynearly40%! ThisisdespitethefactthatLokiincursan
extramatrixmultiplicationoperation(orange,line5ofAlgorithm1)tocomputeattentionscores.
Intheothertwofigures,wecomparetheperformanceofLokiandvanillaattentionfortwoprompt
lengths-2048and3072andvariousgenerationlengths. Fortheshorterpromptlengthof2048we
observeaspeedupofaround35%,whereasforthelongerpromptlengthof3072,weobservealarger
speedupof40%. ThistrendisexpectedaslargerpromptsresultinabiggerKV-Cache,amplifying
theimpactofouroptimizations.
LimitationsandFutureWork: Ourcomputebenchmarkingshowsthatthecostofselectingthetop-
kkeysimpactsoverallspeedup,andacustomkernelforthisoperationcouldenhanceperformance.
Additionally,thememorycostofupdatingtheKV-CacheisabottlenecksharedwithHuggingFace’s
implementationofvanillaattention. UsingamoreefficientKV-Cachemanagementsystemisleftfor
futurework. Ourmethoddoesnotfocusonreducingmemoryusage,andapotentialfuturedirection
couldinvolveutilizingCPUmemorytostoreallthekeys,andonlytransferringthetop-k keysto
the GPU. The surprising observation that the pre-rotary PCA transforms perform better than the
post-rotaryonesforsomemodelsalsowarrantsfurtherinvestigation. Ourkeyobservationofthe
lowintrinsicdimensionalityofthekeysopensupseveralavenuesforfutureresearch,andfurther
fine-tuningofLoki(likeusingperlayerd values)couldleadtobetterresults.
f
6 Conclusion
Inconclusion,thisworkaimstodeviseanefficientsparseattentionmethod,Loki,thatdoesnotcom-
promisethemodelqualitywhilereducingthecomputationalcomplexityofattentionintransformer
models. Wemakeakeyobservationthatthekeyvectorsinattentionlieinalow-dimensionalspace,
acrossdifferentmodelsanddatasets. Thisinsight,byitself,isinterestingandcanbeinvestigated
further in future work. Leveraging this insight, Loki uses attention scores computed in a lower
dimensionalspacetorankandtoselectthetop-ktokens. Itthenusesthefulldimensionalityonlyfor
theselectedtokenstocomputethefinalattention.OurtheoreticalanalysisshowsthatLokicanprovide
significantspeedupsintheattentionstep. Toimplementthisefficiently,wedevelopoptimizedkernels
thatreducedatamovementbetweentheGPUandregistersduringtheadditionaloperationsintroduced
by Loki. Our empirical evaluation shows that Loki performs better than popular approximation
methods on a variety of models and tasks, with respect to preserving model quality. Finally, we
showthatLoki canprovidespeedupsofupto40%overthebaseattentionempirically,makingita
promisingapproachtoaddressthecomputationalchallengesintransformerinference.
9
)s(
emiT
)s( emiTReferences
[1] Introducing triton: Open-source gpu programming for neural networks. https://openai.
com/index/triton/,2021.
[2] Introducingmetallama3: Themostcapableopenlyavailablellmtodate. https://ai.meta.
com/blog/meta-llama-3/,2024.
[3] EdwardBeeching,ClémentineFourrier,NathanHabib,SheonHan,NathanLambert,Nazneen
Rajani,OmarSanseviero,LewisTunstall,andThomasWolf. Openllmleaderboard. https:
//huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard,2023.
[4] StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleO’Brien,
EricHallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,Edward
Raff,etal. Pythia: Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling. In
InternationalConferenceonMachineLearning,pages2397–2430.PMLR,2023.
[5] RewonChild,ScottGray,AlecRadford,andIlyaSutskever. Generatinglongsequenceswith
sparsetransformers. arXivpreprintarXiv:1904.10509,2019.
[6] KrzysztofChoromanski,ValeriiLikhosherstov,DavidDohan,XingyouSong,AndreeaGane,
TamasSarlos,PeterHawkins,JaredDavis,AfrozMohiuddin,LukaszKaiser,DavidBelanger,
LucyColwell,andAdrianWeller. Rethinkingattentionwithperformers,2022.
[7] LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,Charles
Foster,LaurenceGolding,JeffreyHsu,AlainLeNoac’h,HaonanLi,KyleMcDonell,Niklas
Muennighoff,ChrisOciepa,JasonPhang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,
LintangSutawika,EricTang,AnishThite,BenWang,KevinWang,andAndyZou.Aframework
forfew-shotlanguagemodelevaluation,122023.
[8] SuyuGe,YunanZhang,LiyuanLiu,MinjiaZhang,JiaweiHan,andJianfengGao. Modeltells
youwhattodiscard:Adaptivekvcachecompressionforllms. arXivpreprintarXiv:2310.01801,
2023.
[9] SuyuGe,YunanZhang,LiyuanLiu,MinjiaZhang,JiaweiHan,andJianfengGao. Modeltells
youwhattodiscard: Adaptivekvcachecompressionforllms,2024.
[10] AnkitGupta,GuyDar,ShayaGoodman,DavidCiprut,andJonathanBerant. Memory-efficient
transformersviatop-kattention. CoRR,abs/2106.06899,2021.
[11] EdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,and
WeizhuChen. Lora: Low-rankadaptationoflargelanguagemodels. CoRR,abs/2106.09685,
2021.
[12] BenoitJacob,SkirmantasKligys,BoChen,MenglongZhu,MatthewTang,AndrewHoward,
HartwigAdam,andDmitryKalenichenko. Quantizationandtrainingofneuralnetworksfor
efficientinteger-arithmetic-onlyinference,2017.
[13] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.
[14] AlbertQ.Jiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,Chris
Bamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,
GiannaLengyel,GuillaumeBour,GuillaumeLample,LélioRenardLavaud,LucileSaulnier,
Marie-AnneLachaux,PierreStock,SandeepSubramanian,SophiaYang,SzymonAntoniak,
Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
WilliamElSayed. Mixtralofexperts,2024.
[15] Ian T Jolliffe and Jorge Cadima. Principal component analysis: a review and recent devel-
opments. Philosophical transactions of the royal society A: Mathematical, Physical and
EngineeringSciences,374(2065):20150202,2016.
10[16] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.
arXivpreprintarXiv:2001.04451,2020.
[17] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,
JosephE.Gonzalez,HaoZhang,andIonStoica. Efficientmemorymanagementforlargelan-
guagemodelservingwithpagedattention. InProceedingsoftheACMSIGOPS29thSymposium
onOperatingSystemsPrinciples,2023.
[18] ZichangLiu,AdityaDesai,FangshuoLiao,WeitaoWang,VictorXie,ZhaozhuoXu,Anastasios
Kyrillidis,andAnshumaliShrivastava. Scissorhands: Exploitingthepersistenceofimportance
hypothesisforllmkvcachecompressionattesttime. arXivpreprintarXiv:2305.17118,2023.
[19] StephenMerity,CaimingXiong,JamesBradbury,andRichardSocher. Pointersentinelmixture
models. CoRR,abs/1609.07843,2016.
[20] Microsoft. Introducing phi-3: Redefining what’s possi-
ble with slms. https://azure.microsoft.com/en-us/blog/
introducing-phi-3-redefining-whats-possible-with-slms/,2024.
[21] MarkusNagel,MariosFournarakis,RanaAliAmjad,YelyseiBondarenko,MartvanBaalen,
andTijmenBlankevoort. Awhitepaperonneuralnetworkquantization,2021.
[22] NERSC. Perlmutter system architecture. https://docs.nersc.gov/systems/
perlmutter/architecture/.
[23] ReinerPope,SholtoDouglas,AakankshaChowdhery,JacobDevlin,JamesBradbury,Anselm
Levskaya,JonathanHeek,KefanXiao,ShivaniAgrawal,andJeffDean. Efficientlyscaling
transformerinference,2022.
[24] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
YanqiZhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunified
text-to-texttransformer,2023.
[25] LukaRibar,IvanChelombiev,LukeHudlass-Galley,CharlieBlake,CarloLuschi,andDouglas
Orr. Sparqattention: Bandwidth-efficientllminference,2023.
[26] SiddharthSinghandAbhinavBhatele. AxoNN:Anasynchronous, message-drivenparallel
frameworkforextreme-scaledeeplearning. InProceedingsoftheIEEEInternationalParallel
&DistributedProcessingSymposium,IPDPS’22.IEEEComputerSociety,May2022.
[27] SiddharthSingh,PrajwalSinghania,AdityaK.Ranjan,ZackSating,andAbhinavBhatele. A
4dhybridalgorithmtoscaleparalleltrainingtothousandsofgpus,2024.
[28] JianlinSu,YuLu,ShengfengPan,AhmedMurtadha,BoWen,andYunfengLiu. Roformer:
Enhancedtransformerwithrotarypositionembedding,2023.
[29] MingjieSun,XinleiChen,J.ZicoKolter,andZhuangLiu.Massiveactivationsinlargelanguage
models,2024.
[30] HugoTouvronetal. Llama2: Openfoundationandfine-tunedchatmodels,2023.
[31] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,
LukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. CoRR,abs/1706.03762,2017.
[32] SinongWang,BelindaZ.Li,MadianKhabsa,HanFang,andHaoMa.Linformer:Self-attention
withlinearcomplexity,2020.
[33] GuangxuanXiao,YuandongTian,BeidiChen,SongHan,andMikeLewis. Efficientstreaming
languagemodelswithattentionsinks. arXivpreprintarXiv:2309.17453,2023.
[34] GuangxuanXiao,YuandongTian,BeidiChen,SongHan,andMikeLewis. Efficientstreaming
languagemodelswithattentionsinks,2024.
[35] ZheweiYao,XiaoxiaWu,ChengLi,StephenYoun,andYuxiongHe. Zeroquant-v2: Exploring
post-trainingquantizationinllmsfromcomprehensivestudytolowrankcompensation. 2023.
11[36] BiaoZhang,IvanTitov,andRicoSennrich. Sparseattentionwithlinearunits,2021.
[37] PeiyuanZhang,GuangtaoZeng,TianduoWang,andWeiLu. Tinyllama: Anopen-sourcesmall
languagemodel. arXivpreprintarXiv:2401.02385,2024.
[38] ZhenyuZhang,YingSheng,TianyiZhou,TianlongChen,LianminZheng,RuisiCai,Zhao
Song,YuandongTian,ChristopherRé,ClarkBarrett,etal. H_2o: Heavy-hitteroraclefor
efficientgenerativeinferenceoflargelanguagemodels. arXivpreprintarXiv:2306.14048,2023.
[39] Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio
Torralba,andSanjaFidler. Aligningbooksandmovies: Towardsstory-likevisualexplanations
bywatchingmoviesandreadingbooks,2015.
12A ComprehensiveDimensionalityAnalysis
A.1 RanksAnalysisforAllModels
140Llama2-7B Attention Keys (Pre-Rotary) 140Llama2-7B Attention Keys (Post-Rotary) 140Llama2-13B Attention Keys (Pre-Rotary) 140Llama2-13B Attention Keys (Post-Rotary) 11 22 08 11 22 08 11 22 08 11 22 08
100 100 100 100
80 80 80 80
60 60 60 60
40 Wikitext 40 Wikitext 40 Wikitext 40 Wikitext
C4 C4 C4 C4 20 BookCorpus 20 BookCorpus 20 BookCorpus 20 BookCorpus
0 0 5 10 15 20 25 30 0 0 5 10 15 20 25 30 00 10 20 30 40 00 10 20 30 40
Layer Layer Layer Layer
140Llama2-70B Attention Keys (Pre-Rotary) 140Llama2-70B Attention Keys (Post-Rotary) 140Llama3-8B Attention Keys (Pre-Rotary) 140Llama3-8B Attention Keys (Post-Rotary) 11 22 08 W C4ikitext 11 22 08 11 22 08 W C4ikitext 11 22 08
100 BookCorpus 100 100 BookCorpus 100
80 80 80 80
60 60 60 60
40 40 Wikitext 40 40 Wikitext
C4 C4 20 20 BookCorpus 20 20 BookCorpus
00 20 40 60 80 00 20 40 60 80 0 0 5 10 15 20 25 30 0 0 5 10 15 20 25 30
Layer Layer Layer Layer
140Llama3-70B Attention Keys (Pre-Rotary) 140Llama3-70B Attention Keys (Post-Rotary) 14T 0inyLlama-1.1B Attention Keys (Pre-Rotary) 14T 0inyLlama-1.1B Attention Keys (Post-Rotary) 11 22 08 W C4ikitext 11 22 08 W C4ikitext 11 22 08 W C4ikitext 11 22 08 W C4ikitext
100 BookCorpus 100 BookCorpus 100 BookCorpus 100 BookCorpus
80 80 80 80
60 60 60 60
40 40 40 40
20 20 20 20
00 20 40 60 80 00 20 40 60 80 00 5 10 15 20 00 5 10 15 20
Layer Layer Layer Layer
140Mistral-7B Attention Keys (Pre-Rotary) 140Mistral-7B Attention Keys (Post-Rotary) 140Mixtral-8x7B Attention Keys (Pre-Rotary) 140Mixtral-8x7B Attention Keys (Post-Rotary) 11 22 08 W C4ikitext 11 22 08 11 22 08 W C4ikitext 11 22 08
100 BookCorpus 100 100 BookCorpus 100
80 80 80 80
60 60 60 60
40 40 Wikitext 40 40 Wikitext
C4 C4 20 20 BookCorpus 20 20 BookCorpus
0 0 5 10 15 20 25 30 0 0 5 10 15 20 25 30 00 5 10 15 20 25 30 00 5 10 15 20 25 30
Layer Layer Layer Layer
111 224 080Mixtral-8x22B Attention Keys (Pre-Rotary) 111 224M 080ixtral-8x22B Attention Keys (Post-Rotary) 111 224 080Pythia-6.9B Attention Keys (Pre-Rotary) 111 224 080Pythia-6.9B Attention Keys W C(P 4io ks itt e- xR totary)
100 100 100 100 BookCorpus
80 80 80 80
60 60 60 60 40
Wikitext
40
Wikitext
40 W C4ikitext 40
20 BookCorpus 20 BookCorpus 20 BookCorpus 20
00 10 20 30 40 50 00 10 20 30 40 50 0 0 5 10 15 20 25 30 0 0 5 10 15 20 25 30
Layer Layer Layer Layer
Phi3-Mini-4K Attention Keys (Pre-Rotary) Phi3-Mini-4K Attention Keys (Post-Rotary)
Wikitext
C4
19060 BookCorpus 19060
80 80
60 60
40 40 Wikitext
C4 20 20 BookCorpus
00 5 10 15 20 25 30 00 5 10 15 20 25 30
Layer Layer
Figure7:Rankatwhich90%ofthevarianceisexplainedforpre-rotaryandpost-rotarykeysproduced
byeachlayeraveragedacrossallheads(Rank @90)fordifferentmodels. Weobservethatallmodels
l
exhibitsignificantlylowrankconsistentlyacrossalldatasets.
In this section, we present our dimensionality analysis results (from 3) for all the models we
experimentedwith. Figure7showstheRank @90forallthemodelsmentionedinSection3. We
l
observethatourfindingsaroundthelowdimensionalityofthekeysareconsistentacrossallmodels
anddatasets. TheresultsfortheothermodelsaresimilartotheonesshowninFigure2. Thesetof
modelsweexperimentedwithencompassesawiderangeofmodelsizes,architectureclasses(dense
vsMoEmodels),olderandnewermodels,andmodelstrainedondifferentdatasets. Evenwiththese
variations,ourkeyobservationholdstrue. AninterestingtrendweobserveisthattheRank @90
l
variesacrosslayersfordifferentmodels. Thisindicatesthattheintrinsicdimensionalityofthekeysis
13
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaR
ecnairaV
denialpxE
%09
ta knaRnotuniformacrosslayersofamodel. Apossiblefuturedirectioncouldbetoinvestigatethereasons
behindthisvariation,andwhetherhavingaperlayerd inLokicouldleadtobetterresults.
f
Figure8showsthenormalizedeigenvaluesofthecovariancematrixofthekeysforafewlayersand
headsofLlama2-7B,Mistral-7B,andPythia-6.9BontheWikiText-2datasetasanexample. The
resultsfortheothermodelsaresimilartotheonesshownhere.
Llama2-7B, Layer 1, Head 0 Mistral-7B, Layer 1, Head 0 Pythia-6.9B, Layer 1, Head 0
0.30 0.30 0.30
Pre-Rotary Pre-Rotary Pre-Rotary
0.25 Post-Rotary 0.25 Post-Rotary 0.25 Post-Rotary
0.20 0.20 0.20
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
0.00 0.00 0.00
0 20 40 60 80 100 120 0 20 40 60 80 100 120 0 20 40 60 80 100 120
Feature Dimension / Rank Feature Dimension / Rank Feature Dimension / Rank
Llama2-7B, Layer 28, Head 6 Mistral-7B, Layer 28, Head 6 Pythia-6.9B, Layer 28, Head 6
0.30 0.30 0.30
Pre-Rotary Pre-Rotary Pre-Rotary
0.25 Post-Rotary 0.25 Post-Rotary 0.25 Post-Rotary
0.20 0.20 0.20
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
0.00 0.00 0.00
0 20 40 60 80 100 120 0 20 40 60 80 100 120 0 20 40 60 80 100 120
Feature Dimension / Rank Feature Dimension / Rank Feature Dimension / Rank
Figure8: NormalizedeigenvaluesofthecovariancematrixofthekeysproducedbyLayer1,Head
1 (top row), and Layer 28, Head 6 (bottom row) of Llama2-7B (left), Mistral-7B (middle), and
Pythia-6.9B(right)ontheWikiText-2dataset. Weobservethattheexplainedvariancesignificantly
decreasesaftertheinitialprincipaldimensions. Thedashedlinesrepresenttherankatwhich90%of
thevarianceisexplained(Rank @90).
i,h
A.2 VariationofRankacrossAttentionHeads
In this section, we discuss the variation of the rank at which 90% of the variance is explained
(Rank @90)acrossdifferentheadswithinalayerfortwomodels:Llama2-7BandMistral-7B.Figure
l
9showstheheatmapoftheRank @90forthepre-rotary(top)andpost-rotary(bottom)keysacross
l
alllayersandheadsforMistral-7B.WeobservethattheRank @90isconsiderablylowerforpre-
l
rotarykeysvspost-rotarykeys. Focusingonthepre-rotarykeys,weseethattheinitiallayershavea
lowerrankcomparedtothelaterlayers. Ineachlayer,therearesomeheadsheadswithhigh-rank
valueseventhoughthemedianrankislow. Thismightindicatethatsomeheadinthatlayerismore
importantandusesmorecomplexinformationaboutthekeys. Interestinglyforpost-rotarykeys,we
seeapatternwhere4outofthe8headsineachlayerhavethesamerank. Thismighthavetodowith
howtherotaryembeddingsareappliedtoMistral-7BaswedonotseethispatterninLlama2-7B.
Figure10showstheheatmapoftheRank @90forthepre-rotary(left)andpost-rotary(right)keys
l
acrossalllayersandheadsforLlama2-7B.WeobserveasimilartrendasMistral-7Bwheretheinitial
layershavealowerrankcomparedtothelaterlayers. However,wedonotseethesamepatterninthe
post-rotarykeysaswesawinMistral-7B.Thismightindicatethattherotaryembeddingsareapplied
differentlyinLlama2-7BcomparedtoMistral-7B.
14
seulav-negiE
desilamroN
seulav-negiE
desilamroN
seulav-negiE
desilamroN
seulav-negiE
desilamroN
seulav-negiE
desilamroN
seulav-negiE
desilamroN0 5 10 15 20 25 30
0 3 13 50 31 41 57 47 59 59 43 46 38 61 63 50 43 61 44 33 74 54 38 44 71 76 83 84 83 63 47 37 67
80
1 1 12 84 53 54 50 58 43 62 46 63 56 28 47 41 43 48 61 51 43 69 54 48 65 58 83 43 77 61 96 47 43
2 2 30 70 39 41 47 61 53 53 50 50 61 37 58 53 39 59 67 65 32 45 50 51 59 77 53 62 96 68 48 43 68
60
3 1 11 40 45 44 59 57 43 37 51 71 52 60 60 57 59 42 69 43 47 55 75 87 35 52 73 60 68 50 50 49 63
4 2 16 55 30 62 73 47 30 46 53 53 56 47 61 49 44 54 66 60 37 46 52 66 70 73 53 47 67 46 60 55 35 40
5 1 15 30 40 43 56 40 49 52 58 62 42 45 57 40 42 45 78 44 77 46 64 75 79 56 78 74 82 62 55 78 68
6 3 25 70 55 34 57 42 54 55 42 61 57 45 53 45 41 46 41 64 79 57 68 64 75 68 53 62 60 43 73 54 57 20
7 5 10 58 49 60 41 66 52 62 48 56 45 67 68 49 43 48 60 51 48 48 81 61 78 88 55 61 55 70 69 60 50
Layers
0 5 10 15 20 25 30
0 51 47 97 81 91 86 87 88 96 93 87 88 88 89 86 80 84 74 51 92 92 63 70 84 93 91 88 90 88 90 63 90 90
1 51 47 97 81 91 86 87 88 96 93 87 88 88 89 86 80 84 74 51 92 92 63 70 84 93 91 88 90 88 90 63 90
80
2 51 47 97 81 91 86 87 88 96 93 87 88 88 89 86 80 84 74 51 92 92 63 70 84 93 91 88 90 88 90 63 90
3 51 47 97 81 91 86 87 88 96 93 87 88 88 89 86 80 84 74 51 92 92 63 70 84 93 91 88 90 88 90 63 90 70
4 37 60 79 79 80 95 85 93 95 86 91 84 43 88 88 78 79 90 87 82 88 88 75 90 89 90 70 90 92 70 82 79
60
5 37 60 79 79 80 95 85 93 95 86 91 84 43 88 88 78 79 90 87 82 88 88 75 90 89 90 70 90 92 70 82 79
6 37 60 79 79 80 95 85 93 95 86 91 84 43 88 88 78 79 90 87 82 88 88 75 90 89 90 70 90 92 70 82 79 50
7 37 60 79 79 80 95 85 93 95 86 91 84 43 88 88 78 79 90 87 82 88 88 75 90 89 90 70 90 92 70 82 79
Layers 40
Figure9: Heatmapshowingtherankat90%explainedvarianceforthepre-rotary(top)andpost-
rotary(bottom)keyvectorsacrossalllayersandheadsforMistral-7B.Wecanseethatineachlayer,
therearesomeheadswithhigh-rankvalues. Thiswouldindicatedwhenreducingthekeystoafixed
lowerdimensionality,someheadsmightperformbetterthanothers.
0 5 10 15 20 25 30 0 5 10 15 20 25 30
0 326483758646656646951617076895870468474447195659642919782718371 100 060545473674486747772806677788675817683696278967510171979889768564
237323353604779616578506370557749675279424186766888888687868272 6370502071796782737589747874748870877081636688828096969287898384 100
2124567473961596964513638545871575776768289939287948173101617868 52295669657380816870785655777975777781748388989593998577100697677
1 7166155486950526972616679457061876869558092834054448779939968 35256652727962747677627773937374748883837482978363696890839410060
3 5277068526143518056634156626148717849854273834088807887767868 6449606869467370798377856575797575737873797170807092767895818761
5 2 6436828516858476869406077584949878579594199936777968583647583 541386566656071796868736876887772749382837664100897978968591858195
218176759484231626763646180636844878156948983967691908274879072 80 606052776363705190787882758775756789667210199851037495877881888785
1 3423674605683506958627655497474898352628578438568947250867567 5455597177748486777680778970747576847576748575678492968677987590
2 2464647644170645266637065675566857452598659404083969988808267 534962676771568080787379787473737593846878857364659010010390849079 80
3 3465259624365536466525446527450545356935846668687738454477463 6361536463656074717571717462747372767376987164778880788862738685
10 217536364596627775558715966715872564642486673797894778485637479 104759576565667339837678878080798177777363777881778094778577757183
2 4386055536264797150545551547171495879628577934662999997838872 5852575766587366688175747972746769687279799471886977104103978810189
5 1 7 26 51 06 72 54 39 84 53 74 45 93 57 66 64 95 89 15 66 66 58 46 50 76 68 84 47 54 72 06 57 05 49 66 93 66 81 56 72 58 69 29 97 99 94 78 81 78 91 47 87 07 99 69 58 28 46 56 88 16 71 5 60 6 60 21 54 55 66 27 52 87 69 46 80 28 62 85 77 76 77 77 83 58 71 98 52 47 75 57 74 57 61 57 71 66 73 47 76 07 97 67 88 67 87 08 77 61 10 02 119 00 08 94 99 90 88 74 86 93 71 70 928 63 68 72 18 67 7
1 9326539576654536746376182734184909550548495966684647938825569 57486168586976817575666677847665879096737882941007895798171857776
15 130266668456175686344335372545373665439786882824599627185639745 153575466164768286736869487777767370667464788286866699806384839574 60 327626358642564768061724060537370485381954553733961787383718447 62695964746742776272767669747661737275701016775775576896587798968 2103942465954686749634157565361477087526310064683946898896978579 613057576174827478706172717374777170906975101717761678884901007986
439285658555366556039415556416157475262667894838599909396738890 58925467666774607778706077725874726973797684986971102858898809688
130366860597074656454577252525536807848889440799597938995438469 40 44696267716562747569787485737476547481729010064829498989499659188
20 2 3467140474558697168587563616368758491719473428390878874888279 206246665461677574678178736874757883908296759979678896849480897984
1 62051325952488165586957626392759079517792819585424510077529068 5520406850837671917478857577809390886969809772969061759884738682
1512561614865594465704473577056834190945377439894578010094897914 544744555763627166687462877078778560959872686510199748110498928434 40
214277255427156634562477554846067896377888453916981859978848473 67474364607171748178767180738675749279899386779470829310176919187
139417375637371466667528282395459719382927741944796418286929558 53726072566671797172847486896675697598831007963957395737494899476
25 2 31 882 735 58 55 66 05 69 65 75 16 63 73 34 45 71 27 54 67 53 25 56 66 51 97 41 66 93 25 76 15 41 97 81 36 75 06 94 37 57 49 72 69 97 89 89 68 76 64 44 29 94 77 80 98 43 87 69 34 67 7 20 253 63 55 54 93 20 77 80 96 57 17 72 47 71 75 79 85 57 07 75 86 72 97 88 08 72 86 78 18 79 37 96 18 70 87 66 67 85 38 70 417 07 08 72 09 63 49 97 41 80 548 86 16 68 418 09 07 74 78 68 78 70 37 74 0
238324250508157657058747081545351558068434141949591438180778671 5955485359777067766273696975717472797276676765969193698083859564
332404166666549486565524868677657748486494990909371579190759168 5562627859787873636875757673717674777290687292899176739691839485
30 3 1 5 66 57 35 54 56 50 85 58 76 33 76 73 46 70 05 51 14 69 15 58 47 50 88 53 74 65 95 56 95 48 69 52 09 40 28 89 54 42 55 49 48 40 98 87 04 51 01 90 807 66 717 01 119 06 07 69 99 93 57 73 3 305 55 25 34 43 50 26 62 56 60 97 63 26 60 38 77 66 67 96 79 77 78 97 75 87 73 37 74 77 72 07 79 37 64 59 77 88 68 69 88 96 76 17 63 78 77 69 85 46 72 21 10 05 18 82 48 91 919 05 38 60 69 98 89 81 5 20
2 6343460416047446257596753815854475165635365409889928783887973 62366666676573807078728167778773756974787274786110296948998899174
Layers Layers
Figure10: Heatmapshowingtherankat90%explainedvarianceforthepre-rotary(top)andpost-
rotary(bottom)keyvectorsacrossalllayersandheadsforMistral-7B.Wecanseethatineachlayer,
therearesomeheadswithhigh-rankvalues. Thiswouldindicatedwhenreducingthekeystoafixed
lowerdimensionality,someheadsmightperformbetterthanothers.
15
sdaeH
sdaeH
sdaeH
Rank
at 90% sdaeH
Rank
at
95%
Rank
at
95%
Rank
at 90%B ComprehensiveEvaluationResults
B.1 PerformanceofLokionPerplexityandDownstreamTasks
Loki vs Full Attention (prerotary)
100
20.0
17.5
80
15.0
12.5 60
10.0
40
7.5
5.0
20
2.5
0.0 0
Llama2-7B Llama2-13 LB lama2-70B Llama3-8B Llama3-7 T0 inB yLlama-1.1 MiB stral-7 MB ixtral-8x7B Llama2-7B Llama2-13 LB lama2-70B Llama3-8B Llama3-7 T0 inB yLlama-1.1 MiB stral-7 MB ixtral-8x7B
Model
Full Attention Loki (pre, kf=0.25, df=0.5) Loki (pre, kf=0.125, df=0.5)
Loki (pre, kf=0.5, df=0.5) Loki (pre, kf=0.25, df=0.25) Loki (pre, kf=0.125, df=0.25)
Loki (pre, kf=0.5, df=0.25) Loki (pre, kf=0.25, df=0.125) Loki (pre, kf=0.125, df=0.125)
Loki (pre, kf=0.5, df=0.125)
Loki vs Full Attention (postrotary)
60 100
50 80
40
60
30
40
20
20
10
0 0
Llama2-7B Llama2-13 LB lama2-70B Llama3-8B Llama3-7 T0 inB yLlama-1.1 MiB stral-7 MB ixtral-8x7B Llama2-7B Llama2-13 LB lama2-70B Llama3-8B Llama3-7 T0 inB yLlama-1.1 MiB stral-7 MB ixtral-8x7B
Model
Full Attention Loki (post, kf=0.25, df=0.5) Loki (post, kf=0.125, df=0.5)
Loki (post, kf=0.5, df=0.5) Loki (post, kf=0.25, df=0.25) Loki (post, kf=0.125, df=0.25)
Loki (post, kf=0.5, df=0.25) Loki (post, kf=0.25, df=0.125) Loki (post, kf=0.125, df=0.125)
Loki (post, kf=0.5, df=0.125)
Figure 11: Performance of Loki on Perplexity (left) and Downstream Tasks (right) for different
modelsusingpre-rotary(top)andpost-rotary(bottom)PCAtransformation. Foreachmodeland
eachtransformtype,werunLokiwithdifferentvaluesofkandd.
In this section, we present the detailed evaluation of our method on a wide range of models and
tasks. Figure11showstheperformanceofLokionperplexityanddownstreamtaskscomparedto
thefullattentionbaseline. Weshowtheresultsforbothpre-rotary(top)andpost-rotary(bottom)
PCAtransformation. ThemodelsevaluatedareLlama2-7B,Llama2-13B,Llama2-70B,Llama3-8B,
Llama3-70B,TinyLlama-1.1B,Mistral-7B,andMixtral-8x7B.Weevaluatethemodelswithdifferent
configurations of k and d for Loki. We can see that as k and d decrease, the performance of
f f
the model deteriorates. This is especially true when k and d are set to 0.125. We notice that
f f
the impact of k on performance is more significant than d . This is evident from the fact that
f f
k = 0.125,d = 0.5performssignificantlyworsethank = 0.5,d = 0.125foralmostallthe
f f f f
models. Thetwosettingswithk =0.25,d =0.25andk =0.125,d =0.5performrelatively
f f f f
wellacrossallmodels. Thesesettingsprovideagoodtrade-offbetweenperformanceandmodel
accuracy,withatheoreticalspeedupof2.6xforbothsettings. Allsettingswithk =0.5preserve
f
modelqualitymuchbetterbutdonotprovideasignificantspeedupempirically. Table3andTable4
showthesameresultsintabularform.
16
ytixelpreP
ytixelpreP
ycaruccA
ksaT
egarevA
ycaruccA
ksaT
egarevATable3: Performanceofdifferentmodelscomparedtohuggingfacebaselinewithdifferentconfigura-
tionsofkanddusingpre-rotaryPCAtransformation.
Model Method k d PPL↓ Hellaswag↑ TQA↑ Winogrande↑ ARC↑ GSM8K↑ MMLU↑ Avg↑
Llama2-7B FullAttention - - 5.1101 75.99 38.96 69.06 46.33 13.87 41.84 47.67
Loki 0.5 0.5 5.1195 75.96 38.85 69.22 46.16 13.19 41.34 47.45
Loki 0.5 0.25 5.1223 75.84 39.05 68.82 45.82 12.36 40.95 47.14
Loki 0.5 0.125 5.1250 75.09 38.51 69.53 44.28 10.77 39.07 46.21
Loki 0.25 0.5 5.1881 75.73 38.04 67.25 44.20 11.30 39.74 46.04
Llama2-7B Loki 0.25 0.25 5.2185 73.43 38.35 63.61 41.21 7.96 36.43 43.50
Loki 0.25 0.125 5.3044 53.23 40.08 59.35 36.09 2.81 30.99 37.09
Loki 0.125 0.5 5.4980 70.42 39.40 52.49 35.92 7.13 33.22 39.76
Loki 0.125 0.25 6.0729 56.04 42.76 49.57 31.91 2.27 27.15 34.95
Loki 0.125 0.125 8.0514 31.06 44.46 49.01 25.34 0.38 23.64 28.98
Llama2-13B FullAttention - - 4.5680 79.38 36.90 72.22 49.15 22.97 52.06 52.11
Loki 0.5 0.5 4.5701 79.34 37.06 73.09 48.81 23.20 52.19 52.28
Loki 0.5 0.25 4.5708 79.27 37.14 72.14 49.40 22.44 52.03 52.07
Loki 0.5 0.125 4.5737 78.45 37.39 70.09 47.95 19.86 50.98 50.79
Loki 0.25 0.5 4.5979 79.19 37.35 71.90 47.87 22.14 52.02 51.74
Llama2-13B Loki 0.25 0.25 4.6110 77.39 36.89 68.90 46.16 19.86 48.80 49.67
Loki 0.25 0.125 4.6829 71.17 37.21 58.17 36.26 7.88 41.30 42.00
Loki 0.125 0.5 4.8153 77.38 38.45 56.27 41.64 14.94 48.63 46.22
Loki 0.125 0.25 5.3912 61.85 36.79 52.09 32.08 2.96 36.40 37.03
Loki 0.125 0.125 7.6573 38.67 43.00 50.20 24.32 0.68 23.63 30.08
Llama2-70B FullAttention - - 3.1205 83.82 44.81 77.90 57.34 53.15 65.41 63.74
Loki 0.5 0.5 3.1319 - - - - - - -
Loki 0.5 0.25 3.1293 83.65 39.78 76.95 56.91 41.93 63.32 60.42
Loki 0.5 0.125 3.1316 82.38 39.33 72.85 54.61 37.45 60.85 57.91
Loki 0.25 0.5 3.2986 80.54 42.46 75.85 57.08 22.21 57.14 55.88
Llama2-70B Loki 0.25 0.25 3.2830 76.05 44.88 63.54 50.26 15.92 51.79 50.41
Loki 0.25 0.125 3.4571 52.25 44.73 50.36 25.09 2.35 29.37 34.02
Loki 0.125 0.5 3.8327 68.06 39.43 58.80 46.93 10.31 44.82 44.72
Loki 0.125 0.25 3.9259 46.59 45.88 46.96 28.67 2.35 28.90 33.22
Loki 0.125 0.125 6.4963 30.07 49.19 51.30 22.78 1.14 24.75 29.87
Llama3-8B FullAttention - - 5.5696 79.17 43.89 72.93 53.24 50.11 62.19 60.26
Loki 0.5 0.5 5.5703 78.84 44.21 73.64 54.01 48.90 61.47 60.18
Loki 0.5 0.25 5.5746 77.44 43.68 68.27 49.15 47.16 60.58 57.71
Loki 0.5 0.125 5.5876 74.83 44.23 65.43 43.94 40.41 56.97 54.30
Loki 0.25 0.5 5.5944 76.54 44.32 60.93 43.43 44.66 58.33 54.70
Llama3-8B Loki 0.25 0.25 5.6648 69.42 41.50 50.36 34.64 33.06 44.50 45.58
Loki 0.25 0.125 6.0558 56.11 42.14 50.36 27.13 9.17 30.46 35.90
Loki 0.125 0.5 5.7356 66.13 44.00 50.04 28.33 31.77 40.61 43.48
Loki 0.125 0.25 6.5780 45.14 41.00 49.33 23.89 3.18 26.05 31.43
Loki 0.125 0.125 11.1097 32.70 44.31 47.04 23.29 0.68 23.80 28.64
Llama3-70B FullAttention - - 2.5653 84.89 45.57 80.43 64.33 80.67 75.03 71.82
Loki 0.5 0.5 2.5656 85.17 45.66 79.95 63.99 79.91 74.90 71.60
Loki 0.5 0.25 2.5665 84.22 45.78 75.06 59.81 78.77 73.68 69.55
Loki 0.5 0.125 2.5712 82.21 45.53 69.61 54.78 74.98 70.28 66.23
Loki 0.25 0.5 2.5727 84.09 45.64 71.35 57.51 79.76 73.12 68.58
Llama3-70B Loki 0.25 0.25 2.5942 79.06 45.09 59.27 43.26 72.78 62.47 60.32
Loki 0.25 0.125 2.7577 67.59 45.46 50.67 31.48 45.56 42.21 47.16
Loki 0.125 0.5 2.6285 78.96 46.48 51.14 40.70 74.53 62.19 59.00
Loki 0.125 0.25 2.8796 63.93 41.69 46.33 27.65 50.19 36.08 44.31
Loki 0.125 0.125 4.1495 39.07 41.09 49.88 23.38 3.03 25.73 30.36
TinyLlama-1.1B FullAttention - - 7.9671 60.45 37.88 60.22 32.85 1.90 24.86 36.36
Loki 0.5 0.5 8.0040 60.39 38.19 59.98 32.08 1.90 24.62 36.19
Loki 0.5 0.25 8.0342 59.96 38.80 59.27 32.85 2.20 24.33 36.23
Loki 0.5 0.125 8.1057 57.93 39.10 57.14 31.91 1.52 24.98 35.43
Loki 0.25 0.5 8.3475 58.06 40.05 58.17 31.06 1.52 24.83 35.62
TinyLlama-1.1B Loki 0.25 0.25 8.6352 52.69 42.96 52.01 29.18 1.29 24.76 33.82
Loki 0.25 0.125 9.4947 44.43 44.21 50.75 23.89 1.44 24.34 31.51
Loki 0.125 0.5 9.3280 51.29 42.27 53.91 27.82 0.83 24.17 33.38
Loki 0.125 0.25 11.5887 37.32 47.04 47.51 25.00 1.52 23.49 30.31
Loki 0.125 0.125 19.9290 30.13 48.50 51.30 24.66 1.06 24.11 29.96
Mistral-7B FullAttention - - 4.9140 81.07 42.62 73.95 53.92 38.59 59.65 58.30
Loki 0.5 0.5 4.9147 80.84 42.99 74.27 53.58 38.06 59.83 58.26
Loki 0.5 0.25 4.9152 80.55 43.11 72.69 53.41 36.69 59.14 57.60
Loki 0.5 0.125 4.9193 79.38 42.29 70.40 51.28 33.59 57.29 55.71
Loki 0.25 0.5 4.9185 79.00 43.41 70.17 49.23 36.16 58.25 56.04
Mistral-7B Loki 0.25 0.25 4.9233 77.65 42.18 62.98 46.59 32.68 53.70 52.63
Loki 0.25 0.125 4.9986 66.95 39.58 52.64 36.35 14.86 38.20 41.43
Loki 0.125 0.5 4.9311 72.66 43.89 52.25 35.58 33.36 50.01 47.96
Loki 0.125 0.25 4.9636 65.93 41.12 51.78 29.18 18.42 38.14 40.76
Loki 0.125 0.125 5.7404 36.32 43.14 52.17 23.98 0.53 24.60 30.12
Mixtral-8x7B FullAttention - - 3.5967 84.01 48.53 76.32 59.73 58.38 67.90 65.81
Loki 0.5 0.5 3.5979 83.86 46.86 75.53 60.15 57.32 67.83 65.26
Loki 0.5 0.25 3.6047 83.70 46.70 76.24 59.73 57.01 67.21 65.10
Loki 0.5 0.125 3.6201 82.91 42.27 73.48 57.42 43.44 65.71 60.87
Loki 0.25 0.5 3.6076 82.58 48.16 71.43 58.28 56.18 66.72 63.89
Mixtral-8x7B Loki 0.25 0.25 3.6584 81.32 43.49 62.83 51.79 42.76 60.82 57.17
Loki 0.25 0.125 3.9252 73.16 39.49 56.04 44.80 4.85 45.55 43.98
Loki 0.125 0.5 3.6417 76.93 48.21 50.91 41.72 50.87 58.30 54.49
Loki 0.125 0.25 3.8467 70.07 37.88 49.17 32.68 11.52 39.23 40.09
Loki 0.125 0.125 6.9799 42.34 43.80 54.38 24.66 0.45 24.99 31.77
17Table4: Performanceofdifferentmodelscomparedtohuggingfacebaselinewithdifferentconfigura-
tionsofkanddusingpost-rotaryPCAtransformation.
Model Method k d PPL↓ Hellaswag↑ TQA↑ Winogrande↑ ARC↑ GSM8K↑ MMLU↑ Avg↑
Llama2-7B FullAttention - - 5.1101 75.99 38.96 69.06 46.33 13.87 41.84 47.67
Loki 0.5 0.5 5.1195 75.91 38.87 68.59 46.50 14.10 41.49 47.58
Loki 0.5 0.25 5.1206 75.84 39.05 68.82 45.82 12.36 40.95 47.14
Loki 0.5 0.125 5.1241 75.48 38.77 67.64 43.94 12.59 38.85 46.21
Loki 0.25 0.5 5.1838 75.19 38.16 62.12 41.21 10.69 40.42 44.63
Llama2-7B Loki 0.25 0.25 5.2017 72.59 39.16 56.59 37.37 10.24 37.74 42.28
Loki 0.25 0.125 5.4428 68.49 38.83 56.51 32.17 10.92 32.68 39.93
Loki 0.125 0.5 5.3601 70.42 39.40 52.49 35.92 7.13 33.22 39.76
Loki 0.125 0.25 5.5606 59.98 41.72 48.86 26.96 6.22 28.38 35.35
Loki 0.125 0.125 7.4062 40.14 43.84 49.64 25.43 5.99 24.13 31.53
Llama2-13B FullAttention - - 4.5680 79.38 36.90 72.22 49.15 22.97 52.06 52.78
Loki 0.5 0.5 4.5731 79.34 37.06 73.09 48.81 23.20 52.19 52.28
Loki 0.5 0.25 4.5737 79.05 37.46 72.69 48.29 23.58 51.94 52.17
Loki 0.5 0.125 4.5745 78.45 37.39 70.09 47.95 19.86 50.98 50.79
Loki 0.25 0.5 4.5937 79.19 37.35 71.90 47.87 22.14 52.02 51.74
Llama2-13B Loki 0.25 0.25 4.6102 77.39 36.89 68.90 46.16 19.86 48.80 49.67
Loki 0.25 0.125 4.8082 61.52 38.10 52.41 26.54 20.85 44.69 40.68
Loki 0.125 0.5 4.7029 77.38 38.45 56.27 41.64 14.94 48.63 46.22
Loki 0.125 0.25 4.9668 72.71 40.09 51.14 33.28 9.10 39.20 40.92
Loki 0.125 0.125 6.1436 34.81 46.07 52.09 24.15 8.79 26.50 32.07
Llama2-70B FullAttention - - 3.1205 83.82 44.81 77.90 57.34 53.15 65.41 63.74
Loki 0.5 0.5 3.1411 83.89 41.32 78.06 57.68 50.42 64.75 62.69
Loki 0.5 0.25 3.1453 83.69 43.42 76.80 56.31 52.99 64.73 62.99
Loki 0.5 0.125 3.1457 83.41 43.51 75.45 55.89 52.54 64.12 62.49
Loki 0.25 0.5 3.4619 82.36 41.91 76.87 56.48 42.61 60.11 60.06
Llama2-70B Loki 0.25 0.25 3.5701 81.42 45.26 71.11 49.74 44.28 59.56 58.56
Loki 0.25 0.125 3.5459 80.59 45.57 65.59 49.15 46.93 58.00 57.64
Loki 0.125 0.5 4.1427 71.90 44.59 58.09 41.98 34.34 50.30 50.20
Loki 0.125 0.25 4.7796 67.03 46.58 51.85 32.17 37.30 42.21 46.19
Loki 0.125 0.125 4.6898 64.84 44.89 50.51 29.95 38.74 39.08 44.67
Llama3-8B FullAttention - - 5.5696 79.17 43.89 72.93 53.24 50.11 62.19 60.26
Loki 0.5 0.5 5.5699 76.03 43.83 67.32 44.71 49.36 59.38 56.77
Loki 0.5 0.25 5.9343 72.55 42.67 61.64 39.93 41.09 57.88 52.63
Loki 0.5 0.125 5.7429 71.38 43.16 58.64 40.61 39.42 57.14 51.72
Loki 0.25 0.5 5.6783 68.02 42.07 48.78 31.31 43.59 48.06 46.97
Llama3-8B Loki 0.25 0.25 11.4459 57.39 42.13 48.70 27.90 28.28 38.69 40.52
Loki 0.25 0.125 13.2883 48.99 42.10 48.07 22.87 12.81 30.90 34.29
Loki 0.125 0.5 6.8023 49.68 41.14 49.25 25.51 31.39 30.86 37.97
Loki 0.125 0.25 16.3507 36.39 43.62 50.04 25.09 16.60 26.21 32.99
Loki 0.125 0.125 22.6596 31.60 46.38 49.25 23.12 1.14 23.61 29.18
Llama3-70B FullAttention - - 2.5653 84.89 45.57 80.43 64.33 80.67 75.03 71.82
Loki 0.5 0.5 2.5660 83.60 45.83 72.61 56.14 79.15 73.43 68.46
Loki 0.5 0.25 2.5697 79.92 46.22 62.90 48.46 78.39 71.11 64.50
Loki 0.5 0.125 2.7810 76.66 46.77 59.27 42.66 56.94 68.48 58.46
Loki 0.25 0.5 2.5742 74.91 47.67 51.54 38.05 77.71 64.14 59.00
Llama3-70B Loki 0.25 0.25 2.8593 61.40 47.86 48.38 27.73 67.32 41.18 48.98
Loki 0.25 0.125 5.6725 41.90 47.18 47.59 23.29 5.31 26.98 32.04
Loki 0.125 0.5 2.6231 56.24 43.91 50.51 24.66 72.48 38.52 47.72
Loki 0.125 0.25 4.2512 31.91 47.39 50.43 24.57 19.71 24.72 33.12
Loki 0.125 0.125 57.6788 27.01 49.28 50.67 24.06 0.68 24.76 29.41
TinyLlama-1.1B FullAttention - - 7.9671 60.45 37.88 60.22 32.85 1.90 24.86 36.36
Loki 0.5 0.5 7.9979 60.17 38.14 58.33 31.57 1.90 25.12 35.87
Loki 0.5 0.25 8.0135 58.78 39.95 54.38 30.55 1.29 24.58 34.92
Loki 0.5 0.125 8.0414 57.77 38.20 54.93 30.89 1.44 24.39 34.60
Loki 0.25 0.5 8.3190 57.35 37.87 53.83 29.69 1.67 25.13 34.26
TinyLlama-1.1B Loki 0.25 0.25 8.5687 52.40 40.86 49.33 26.96 2.20 23.34 32.51
Loki 0.25 0.125 8.8956 51.19 42.07 52.96 28.92 0.91 25.10 33.52
Loki 0.125 0.5 8.9679 51.32 38.24 50.20 24.23 1.29 24.92 31.70
Loki 0.125 0.25 10.2592 42.85 39.06 51.85 25.60 1.52 24.06 30.82
Loki 0.125 0.125 11.3508 39.27 41.55 50.67 22.78 0.45 24.50 29.87
Mistral-7B FullAttention - - 4.9140 81.07 42.62 73.95 53.92 38.59 59.65 58.30
Loki 0.5 0.5 4.9149 79.89 42.15 70.56 49.83 37.45 58.00 56.31
Loki 0.5 0.25 4.9221 78.99 40.84 63.06 45.48 33.43 55.15 52.82
Loki 0.5 0.125 4.9317 73.88 40.58 57.06 33.87 22.06 45.95 45.57
Loki 0.25 0.5 5.2052 71.86 40.74 56.04 38.91 24.18 45.56 46.22
Mistral-7B Loki 0.25 0.25 6.5445 62.62 38.93 48.62 25.17 1.82 30.80 34.66
Loki 0.25 0.125 7.7609 35.51 43.67 53.20 23.63 1.06 23.86 30.16
Loki 0.125 0.5 9.5167 51.73 45.44 51.62 25.77 3.03 27.99 34.26
Loki 0.125 0.25 13.5597 34.85 46.38 50.20 22.53 0.45 23.60 29.67
Loki 0.125 0.125 20.5289 28.52 51.98 50.91 26.96 0.45 23.64 30.41
Mixtral-8x7B FullAttention - - 3.5967 84.01 48.53 76.32 59.73 58.38 67.90 65.81
Loki 0.5 0.5 3.5970 83.24 47.32 74.27 58.53 56.48 67.23 64.51
Loki 0.5 0.25 3.6196 81.71 43.51 69.61 53.67 55.57 63.92 61.33
Loki 0.5 0.125 3.6635 76.18 41.63 61.72 47.78 49.28 58.94 55.92
Loki 0.25 0.5 3.6004 79.99 46.47 61.64 49.15 57.85 63.04 59.69
Mixtral-8x7B Loki 0.25 0.25 3.7906 71.58 37.77 53.28 37.54 37.38 46.66 47.37
Loki 0.25 0.125 4.2566 59.23 36.58 50.75 28.67 15.39 32.28 37.15
Loki 0.125 0.5 3.6358 72.29 45.28 50.67 33.70 55.50 47.15 50.76
Loki 0.125 0.25 4.5500 52.16 37.86 46.57 23.98 17.13 27.02 34.12
Loki 0.125 0.125 5.5250 46.93 40.33 49.72 23.55 0.91 24.78 31.04
18C Comparisonofourtop-kkernelswithSparQ
AsmentionedinSection4.3,wecreateoptimizedkernelsinTritontoefficientlycomputethethree
matrixmultiplicationsinLoki(lines5,8,and9ofAlgorithm1)withoutcreatingtemporarydense
copiesofsubsetsoftheKV-cache. Initially,weplannedtousetheimplementationsdevelopedbythe
authorsofSparQ[25]. However,wediscoveredtwomajorissueswiththeirkernels. Let’ssayyouare
multiplyingtwomatricesofsizesm×kandk×n,thenSparQkernelsparallelizecomputealong
onlythemdimension. However,itiswellknownthatonecanparallelizematrixmultiplicationsalong
thendimensionaswellandgainmoreperformance. Thus,weaddthisextradimensionofparallelism
totheirtritonkernel. Second,theirkernelscannothandlenon-powersof2numberoftokensinthe
KV-cache,asettingwhichiscommonlyencounteredininferencesincewegeneratedkeysandvalues
oneatatime. Therefore,weextendtheirkernelstohandlenon-powersoftwonumberoftokensin
theKV-cachesuccessfully. InFigure12,wecomparetheperformanceofourkernelwithsparqand
vanillaPyTorchbasedattentionforanattentionlayerinLlama2-7BforvarioussizesoftheKV-cache
rangingfrom512to4096. Wedothisforthematmuloperationofqueryandkeyswithtop-kas0.25.
Llama2-7B - Time for computing Q.KT Llama2-7B - Time for computing Q.KT
with 512 keys and k=0.25 with 1024 keys and k=0.25
0.1 0.2
Vanilla Attention
0.05 SparQ 0.1
Our kernel
0.05
0.01 0.01
1 2 4 8 16 1 2 4 8 16
Batch Size Batch Size
Llama2-7B - Time for computing Q.KT Llama2-7B - Time for computing Q.KT
with 2048 keys and k=0.25 with 4096 keys and k=0.25
0.3 0.5
Vanilla Attention
SparQ 0.2
0.1
Our kernel 0.1
0.05
0.05
0.01 0.01
1 2 4 8 16 1 2 4 8 16
Batch Size Batch Size
Figure 12: Comparing the performance of our proposed kernel for computing Q.KT, with
SparQ’s[25]top-kkernelforvariousbatchsizesandnumberofkeysintheKV-cache.
We see very high speedups over SparQ for small batch sizes. For instance, for a batch size of 1
with4096keys(bottomright),ourkernelisfasterthanSparQbynearly2.8×! Infact,theSparQ
kernelbarelyobtainsanyspeedupovervanillaPyTorcheventhoughitisonlyusing25%ofthekeys
(1024outof4096). ThisisbecauseSparQonlyparallelizescomputeacrossthem-dimensionaswe
discussedbefore. Inattentionthem-dimensionisproportionaltothebatchsize,andthushavinga
smallbatchsizelimitstheamountofparallelismSparQkernelscanexploit. Whereas,givenour2D
parallelapproachtomatrixmultiplication,wecanexploitparallelismalongthesequencedimension
andthusstillobtainsignificantspeedupsovervanillaPyTorch. Notethatsmallerbatchsizesare
verycommonininferenceandhenceitisextremelyimportanttodesignakernelthatcanfunction
efficientlyevenatsmallerbatchsizes. OurspeedupsoverSparQdecreasewithlargerbatchsizes,but
overallwearealwaysfasterthanbothSparQandvanillaPyTorch.
19
)sm(
emiT
)sm(
emiT
)sm(
emiT
)sm(
emiTD GenerationExamples
Inthissection,welookatagenerationexamplefromtheLlama2-7B(Chat)modelgivenacustom
promptfordifferentsettingsofkandd.
Prompt
Inaworldwheretechnologyhasadvancedbeyondimagination,societygrappleswiththe
consequencesofitsowncreations. Theintegrationofartificialintelligenceintoeverydaylife
hasrevolutionizedhowwelive,work,andinteract. However,withgreatpowercomesgreat
responsibility,andethicaldilemmasabound. GovernmentsstruggletoregulatetheuseofAI,
whilecorporationspushtheboundariesofwhatispossibleinthepursuitofprofit. Meanwhile,
individualsnavigatealandscapewhereprivacyisincreasinglyscarce,andthelinebetween
human and machine blurs. Against this backdrop, a new generation of thinkers emerges,
questioningtheverynatureofconsciousnessandwhatitmeanstobehuman. Astensionsrise
andalliancesshift,thefateofhumanityhangsinthebalance,waitingtobewrittenbythose
boldenoughtoseizethepenofdestiny.
Llama2-7BChatFull
Willwe createafuture whereAI andhumanitycoexistin harmony, or willthepursuit of
powerandprofitleadtoourdownfall? Thechoiceisours,andtheclockisticking.
Llama2-7BChat-k=50%,d=25%
Willwechartapathtowardsutopiaorsuccumbtothedangersofuncheckedprogress? Can
we find a balance between technological advancement and preserving the essence of our
humanity? The future is ours to create, and the choice is ours to make. The clock ticks
on,waitingforustotakethenextstepforwardintoanuncertaintomorrow,wheretheonly
constantistheendlesspossibilitiesofanageofAI
Llama2-7BChat-k=25%,d=25%
WillyouanswerthecalltoshapethefutureofAI?Willyouplayapartinshapingthecourse
ofhumanhistory? Thepossibilitiesareendless,andtheoutcomeuncertain. Areyoureadyto
embarkonajourneythatcouldchangetheworldforever?
20E PCAAttn: Usingreduced-dimensionalattentionscoresdirectlyin
transformermodels
Algorithm2PCAAttn
Require: Attheith step-Input: x ∈ R1×D,KV-Cache: K′ ,V ∈ R(S−1)×d,Projection
i i−1,d i−1
Matrix: P ∈RD×d(firstdprincipalcomponents)
d
1: functionPCA-ATTENTION(x i,K′ i−1,d,V i−1,P d)
2: q i,k i,v i ←computeQKV(x i)
3: q′ i,d ←q iP d,k′ i,d ←k iP d
4: K′ ←concat(K′ ,k′)
i,d i−1,d i
5: V i ←concat(V i−1,v i)
q′ (K′ )T
6: A exact =softmax( i,d√ i,d )
D
7: endfunction
Oneotherapproachwetriedistodirectlyusetheformulationin4.1tocomputethefinalattention
scores. Morespecifically,wecomputethePCAtransformedqueryandkeyvectors,projectedonto
thefirstdprincipalcomponents,andthencomputetheattentionscores. Weonlystorethereduced
dimensionkeyvectorsintheKVcache. WecallthismethodPCAAttn(Algorithm2).
ComputeandMemoryAnalysis: Whencomputingattentionbetweenasinglequeryq ∈R1×D
i
and the key vectors K ∈ RS×D, the matrix multiplication q KT has a complexity of O(DS).
i i i
UsingPCAAttn,thekeyandqueryvectorsarereducedtoddimensionsandthecomplexityofthe
matrixmultiplicationisreducedtoO(dS). Thus,wecangetaspeedupofD/dintheattentiondot
productcomputation. ThePCAtransformationofthequeryandkeyvectorgeneratedateachstep
hasacomplexityofO(D2),whichissmallwhenS >>D. TheKV-Cachememoryrequirementis
reducedbyafactorof0.5∗D/dbecauseweonlyreducethekeyvectorstoddimensionsandnotthe
values. Additionally,thePCAaddsasignificantlysmallmemoryoverheadofO(Dd). Table5shows
theexplanationofkey-budgetanddimensionalityforPCAAttn,alongwiththespeedupandmemory
savings.
Table5: Explanationofkey-budgetanddimensionality(dim.) forPCAAttn,alongwiththespeedup
andmemorysavings.
Method Budget Dim. Description Speedup Memory
Savings
PCAAttn Full d d%offulldimensionalityusedtostorekeysand 100 100
d 2d
computeattentionoutput
ExperimentalResults:
Table6: PerformanceofPCAAttnwithvariouscacheconfigurations. ComparewithTable??for
baselinenumbers
Model Method k d Perplexity↓ Hellaswag↑ Winogrande↑ MathQA↑ OpenbookQA↑ RTE↑ COPA↑
Llama2-7B FullAttention - - 5.1102 57.2 69.1 28.4 31.4 62.8 87.0
ExactTopK 50% - 5.1191 57.2 68.9 28.3 31.2 63.9 86.0
Llama2-7B H2O 50% - 5.1456 55.5 61.8 24.4 27.4 62.8 77.0
PCAAttn - 50% 38.3997 33.3 53.2 21.7 14.2 50.5 73
ExactTopK 25% - 5.1799 56.9 68.6 29.4 29 66.4 76.0
Llama2-7B H2O 25% - 5.2809 50.1 51.6 21.1 17.8 55.2 55.0
PCAAttn - 25% 243.2631 26.9 48.5 20.5 11.4 49.1 65.0
Mistral-7B FullAttention - - 4.9140 61.2 73.9 35.7 32.2 66.8 91.0
ExactTopK 50% - 4.9143 61.1 73.8 35.6 32.6 65.3 92.0
Mistral-7B H2O 50% - 4.9560 59.4 58.6 26.4 23.0 62.4 71.0
PCAAttn - 50% 396.8967 31.4 50.4 22.5 15.6 53.4 72.0
ExactTopK 25% - 4.9170 60.4 73.0 35.4 30.0 65.3 85.0
Mistral-7B H2O - 5.0805 52.7 49.7 21.9 17.4 52.0 56.0
PCAAttn - 25% 933.6016 27.2 52.2 21.6 13.6 53.0 63.0
21Table6showstheperformanceofPCAAttnonLlama2-7BandMistral-7Bmodels. Wecanseethat
ourPCAAttnmethodperformspoorlycomparedtoallthebaselinesandtheH2Omethodforall
cacheconfigurations. Webelievethatthishappensbecausetheapplicationofrotaryembeddings
increasesthedimensionalityofthekeyvectorsandusingreduceddimensionalitytostorethekeys
resultsinlossofinformation. Tofurtherinvestigatethis,letuslookatFigure9whichshowstherank
at90%explainedvarianceforthekeyvectorsacrossalllayersandheads. Eventhough,theaverage
rankperlayerisaround50%ofthefulldimensionality,therankforsomelayersandespeciallysome
headswithineachlayerismuchhigher. DuetothepoorperformanceofPCAAttn,wedonotinclude
itinthefinalresultsanddecidetofocusonLokiinsteadinthemainpaper.
22F EstimateofComputeResourcesRequiredtoReplicateourExperiments
AsmentionedinSection5,weconductallofourexperimentsonPerlmutter,amulti-GPUcluster
with 4 A100 GPUs per node. Since we do not do any training/fine-tuning, our experiments can
bedoneonaverysmallnumberofGPUs. Forinstance,allofourrunsinvolvingmodelswith7B
and13BparametersweredoneonasingleA100GPU.Formodelslargerthanthis(likeLLama2-
70B,Llama3-70B),wehadtoresorttorunningonfourA100GPUs(orasinglenode)withtensor
parallelismusingtheAxoNNparalleldeeplearningframework. Allresultsfor7Band13Bsized
modelscanbecompiledwithin3hours. Forlargermodelslikethe70BLLaMA-2and3aswellas
Mixtralmodels,thetotaltimesforcomputingallresultsareintheballparkof10hours. Ourcompute
benchmarkingrunsofLlama-13Bareveryshortandcanbecompletedwithin5minutes.
23