TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners
ChengzuLi*,CaiqiZhang*,HanZhou,NigelCollier,AnnaKorhonen,IvanVulic´
LanguageTechnologyLab,UniversityofCambridge
{cl917, cz391, hz416, nhc30, alk23, iv250}@cam.ac.uk
https://topviewrs.github.io
Abstract answering, language generation, and arithmetic
reasoning (Qin et al., 2023a; Zhao et al., 2023).
Top-viewperspectivedenotesatypicalwayin
Building on these text-only LLMs, the so-called
whichhumansreadandreasonoverdifferent
VisionLanguageModels(VLMs),equippedwith
typesofmaps,anditisvitalforlocalizationand
navigationofhumansaswellasof‘non-human’ the capability to process and reason over multi-
agents,suchastheonesbackedbylargeVision- modalvision-languageinformation,haveenabled
LanguageModels(VLMs). Nonetheless,spa- multi-modalprocessing(Yinetal.,2023;Wuetal.,
tial reasoning capabilities of modern VLMs 2023). They ground language reasoning ability
remainunattestedandunderexplored. Inthis
ofLLMsintotheinformationofdifferentmodal-
work,wethusstudytheircapabilitytounder-
ities (Chandu et al., 2021). Prominent examples
standandreasonoverspatialrelationsfromthe
ofVLMssuchasLLaVA(Liuetal.,2023b),GPT-
topview. Thefocusontopviewalsoenables
4V(OpenAI,2023), andGemini(Google,2024),
controlledevaluationsatdifferentgranularity
ofspatialreasoning;weclearlydisentangledif- havedemonstratedstrongperformanceacrossap-
ferentabilities(e.g.,recognizingparticularob- plications such as visual question answering (Li
jectsversusunderstandingtheirrelativeposi- etal.,2023d),imagecaptioning(Diesendrucketal.,
tions). We introduce the TOPVIEWRS (Top- 2024),andobjectgrounding(Zhengetal.,2024).
ViewReasoninginSpace)dataset,consisting
Spatialreasoning,asoneofthefundamentalde-
of 11,384 multiple-choice questions with ei-
sirablepropertiesofandrequirementsforVLMs,
therrealisticorsemantictop-viewmapasvi-
hasalsogainedincreasedattentionrecently(Rajabi
sualinput. Wethenuseittostudyandevalu-
ate VLMs across 4 perception and reasoning andKosecka,2023;Liuetal.,2023a;Chenetal.,
taskswithdifferentlevelsofcomplexity. Eval- 2024). Itrequiresgroundingthemodel’sreasoning
uation of 10 representative open- and closed- abilitywithnaturallanguageintoitsvisualpercep-
source VLMs reveals the gap of more than tionofthesurroundingenvironment(Freksa,1991).
50%comparedtoaveragehumanperformance,
Inparticular,itinvolvestwocriticalsteps: (i)inter-
anditisevenlowerthantherandombaseline
pretingtheenvironmentvisually,and(ii)reasoning
in some cases. Although additional experi-
overspatialrelations. Asafundamentalabilityfor
mentsshowthatChain-of-Thoughtreasoning
themodeltorecognize,understand,andnavigate
canboostmodelcapabilitiesby5.82%onaver-
age,theoverallperformanceofVLMsremains throughthephysicalworld,itplaysacrucialrolein
limited. Our findings underscore the critical variousdownstreamtaskssuchasvision-language
needforenhancedmodelcapabilityintop-view generation(Lietal.,2024a)andembodiedAI(Cho
spatialreasoningandsetafoundationforfur- etal.,2024).
therresearchtowardshuman-levelproficiency
However,previousresearchhasfocusedonex-
ofVLMsinreal-worldmultimodaltasks.
ploring spatial reasoning abilities of VLMs only
1 Introduction fromaconventionalfirst-personperspectiveview
(Liuetal., 2023a). Inthiswork, weaimtostudy
LargeLanguageModels(LLMs)suchasLlama2
andevaluatespatialunderstandingandreasoning
and3(Touvronetal.,2023),Mistral(Jiangetal.,
capabilityofVLMsfromthetop-viewperspective,
2023),andGPTmodels(OpenAI,2022)havede-
also referred to as the bird’s-eye view (Li et al.,
liveredimpressiveperformanceacrossarangeof
2024b). Whencomparedtotheconventionalper-
text-basedtasksandapplicationssuchasquestion
spectiveview,topviewoffersbetternaturalalign-
* Equalcontributions. ment: itisatypicalviewtoreaddifferentmapsor
1
4202
nuJ
4
]LC.sc[
1v73520.6042:viXraFigure 1: Illustration of the four evaluation tasks with an incremental level of complexity on the two types of
top-viewmaps(photo-realisticversussemanticmaps),coveringtop-viewperceptionandspatialreasoningabilities,
with9sub-tasksintotal(redfont),focusingondifferent,well-definedVLMabilities. Theradargraphs(topright)
comparetherepresentativemodels’performanceonallsub-tasks,indicatingalargegapwithhumanperformance.
present,e.g.,floorplans. Moreover,itisinherently In this work, we thus investigate the basic top-
morecomplex: top-viewmapsencapsulateawealth viewspatialunderstandingandreasoningabilities
of information about different scenes, locations, ofcurrentstate-of-the-artVLMsacrossfourtasks
objectsandtheirrelationshipsintheenvironment ofgraduallyincreasingcomplexity,andtheirfiner-
basedonasingleimage. Inadditiontothephoto- grained sub-tasks. The tasks are as follows. 1)
realistictop-viewmaps,semantictop-viewmaps Top-ViewRecognitionassesseswhetherthemodel
(Nanwanietal.,2023;Lietal.,2024a)usedifferent canrecognizeconcreteobjectsandscenesintop-
colors to represent different types of objects; we view maps. 2) Top-View Localization evaluates
runexperimentswithbothmaptypes,seeFigure1. theabilitytolocalizeobjectsorregionsonamap
One advantage of top-view maps is that they based on textual descriptions. (3) Static Spatial
defineacontrolledandinterpretableexperimental Reasoninginvestigateswhetherthemodelcanrea-
framework. Indoor scenes, which are the focus son about spatial relationships among localized
of this work, typically feature a relatively stable objectsandregionswithinthemap. (4)Dynamic
set ofobjects andlayouts, makingthem idealfor SpatialReasoningevaluatesreasoningaboutspatial
controlled studies. This allows us to disentangle relationsalongthepointsofadynamicnavigation
andinvestigatevariousaspectsofspatialreasoning path. Figure1illustratesallthetaskswithconcrete
andVLMs’capabilitiesinacontrolledmanner.1
drawinganavigationtrajectoryinarealisticmap,orchanging
1Forinstance,wecanapplydifferentinterventions(e.g., thecolor-objectmappinginasemantictop-viewmap).
2examples. As one key finding of this study, con- SpatialReasoningonMulti-ModalVision-Text.
ductedevaluationsrevealthatcurrentVLMslack Therehasbeenabodyofworkontext-onlyspatial
sufficientcapabilitytoeffectivelytackletop-view reasoningwiththeadvancementofLLMs(Yamada
spatialreasoningchallenges,indicatingsubstantial etal.,2024),withinthecontextofrelativespatial
roomforimprovementinfutureresearch. relationrecognition(Mirzaeeetal.,2021;Shietal.,
2022),naturallanguagenavigation(Yamadaetal.,
Contributions. 1) We define the top-view spa-
2024), and planning (Momennejad et al., 2023)
tial reasoning challenge for VLMs via 4 care-
(seeAppendixAforamorecompleteoverview).
fullydesignedtasksofincreasingcomplexity,also
Cross-modal spatial reasoning puts forward
encompassing 9 distinct fine-grained sub-tasks
higher requirements for the models in terms of
with a structured design of the questions focus-
languagegrounding(Rozanovaetal.,2021;Rajabi
ingondifferentmodelabilities. 2)Wecollectthe
andKosecka,2023). Liuetal.(2023a)investigate
TOPVIEWRSdataset,comprising11,384multiple-
spatial reasoning with 2D natural realistic front
choicequestionswitheitherphoto-realisticorse-
view images and Chen et al. (2024) extend the
mantic top-view maps of real-world scenarios
analysis to 3D point clouds. The environmental
throughapipelineofautomaticcollectionfollowed
contexts become more diverse compared to syn-
byhumanalignment. 3)Weuse TOPVIEWRS to
theticsymbolsintext-onlyspatialreasoning,rang-
evaluateandstudy10VLMsfromdifferentmodel
ingfromindoorenvironments(Kochetal.,2024)to
families and sizes, highlighting the performance
gapcomparedtohumanannotators2. outdoorstreetviews(Chenetal.,2019). Regarding
typicaltasks,visualQA(VQA)isthemainstream
task for benchmarking spatial reasoning abilities
2 RelatedWork
(Dongetal.,2021;Banerjeeetal.,2021;Liuetal.,
2023a; Li et al., 2023a,b; Kamath et al., 2023),
Top-View Map Understanding. There are only
whileothertasksincludevision-Languagenaviga-
limitedstudiesinNLPfocusedontheuseoftop-
tion(Chenetal.,2019;Lietal.,2024a)anduser
viewmaps,thoughconsiderableresearchhasbeen
interfacegrounding(Rozanovaetal.,2021).3
conductedwithinthebroaderAIcommunityonthe
Westressthatnoneofthepriorresearchefforts
so-calledbird’s-eyeview,whichisaninstanceof
allowsfordisentangledevaluationofmodels’spa-
topview. Thisbodyofworkhasexploredapplica-
tial reasoning abilities. Prior work typically con-
tionsinautonomousdriving(Ungeretal.,2023;Li
flatesobjectrecognitionwithspatialreasoning. We
etal.,2024c),withcontributionsonfusingdifferent
thusdesignadatasetandconductastudythatnot
typesofviews(Qinetal.,2023b)andworkingwith
only offers insight into fundamental abilities but
arbitrarycamerasetups(Pengetal.,2023). Inother
alsoallowsforeasierinterpretationofresults(§4).
applicationscenarios,Yanetal.(2021)introducea
bird’s-eyeviewpersonre-identificationtaskwith
3 TaskDefinition
114kimagesoftheperson.
Efforts to bridge top-view images with natural Followingpriorwork(Lietal.,2023a),weframe
languageinapplicationsbeyondtheaboveareless all tasks as multiple-choice QA tasks. Given a
diverse. TheWAYdataset,proposedbyHahnetal. top-view(realisticorsemantic)mapofaroomM,
(2020),contains6,154dialogsaimedatlocalizing themodelmustchoosethecorrectoptiono from
i
anobserver’spositiononatop-viewmapthrough thefouroptionsprovidedO = {o ,o ,o ,o }that
0 1 2 3
conversations between an observer and a locator. answersthequestion.4 Thisformatsimplifiesthe
This dataset has inspired follow-up research fo- evaluationandinterpretationoftheresults.
cusingonmergingvisionwithdialoginformation
Top-ViewMaps. Weprovidetwodifferenttypes
(Zhang et al., 2024a) and leveraging pretraining
of top-view maps to the models: realistic maps
strategiestoenhanceperformance(HahnandRehg,
M and semantic maps M . Realistic maps
Real Sem
2022). In general, prior research does not assess
VLMs’ basic spatial reasoning abilities with top- 3Researchonmulti-modalspatialreasoningalsointersects
view images and lacks fine-grained and control- witheffortsfromthecomputervisioncommunityonscene
understanding(Teneyetal.,2017),simultaneouslocalization
lableanalysisofthesefundamentalabilities.
andmapping(Cadenaetal.,2016),andcombiningLLMswith
representationsofthe3Dphysicalworld(Hongetal.,2023).
2We release the code in https://github.com/ 4Forsimplicity,foreachquestion,thereisalwaysasingle
cambridgeltl/topviewrs. correctanswer.
3areconstructedbyplacingasimulatedorthographic navigationpath(sub-taskDynamicActionCount-
cameraabovethescenetocaptureaphoto-realistic ing) and answer spatial questions with regard to
top-viewimage. Semanticmapsrepresentobjects the dynamic navigation path (sub-task Dynamic
in the scene with colored bounding boxes. Each RelativeSpatialReasoning)andthecircumstantial
object is assigned a specific color and labeled at environments(DynamicSpatialLocalization).
the same relative coordinates on the map to pre-
4 TOPVIEWRS Dataset
serve the object’s semantic information and spa-
tial allocation. In comparison to realistic maps, Inordertostudyandevaluatetheabilitiesofstate-
semantic maps simplify the initial step of spatial of-the-artVLMsonthefourtasksspanning9sub-
reasoning(i.e.,environmentinterpretation)byla- tasksfrom§3,wenowintroduceanovelevaluation
belingtheobjecttypeswithcorrespondingcolors dataset,TOPVIEWRS,whichfocusesontop-view
andexcludingirrelevantadditionaldetailssuchas maps of indoor scenes (i.e., houses and rooms),
shapeandtexturefoundinrealistictop-viewmaps. discussedinwhatfollows.
Giventhecustomizableandflexiblenatureofcolor-
Dataset Features. It introduces several advance-
objectmapping,thesemanticmapcanalsoserve
ments and innovative features that distinguish it
as an ideal testbed for evaluating models’ out-of-
fromallpriorvisualspatialreasoningdatasets.
distribution(OOD)performance,therebyencour-
1)Multi-Scale Top-View Maps: Theselected top-
agingfurtherexplorationbeyondthescopeofthis
view maps of indoor scenes (see Figure 1) pro-
work. ExamplemapsareinFigure1.
vide a more natural representation of spatial en-
TasksandSub-Tasks. Wedefine4differenttasks vironmentsthatalignswithhumancognitivemap
which cover a total of 9 finer-grained sub-tasks, (Epstein et al., 2017). This makes benchmarking
with concrete examples shown in Figure 1. The spatialawarenessmorestraightforwardandmean-
tasks are designed to have an increasing level of while mitigates spurious correlations in the posi-
complexity, whereeachsubsequenttaskdepends tionsbetweenobjectscommonlyfoundinrealistic
ontheabilitiesmeasuredintheprecedingone(s). front-viewimages. Comparedtothefrontview,the
(1)Top-ViewRecognitionevaluatesthefundamen- multi-scaletop-viewmapsofsingleroomsandfull
talabilitytointerprettheinputmap,andcoverstwo housesaddmoredivergenceinthegranularityof
sub-tasks: Object Recognition and Scene Recog- theentities(objectsorrooms)inspatialreasoning.
nition. It does not require the model to identify Meanwhile, we provide both realistic maps and
specificlocationsofobjectsandrooms. semanticmapsformorecomprehensiveevaluation.
(2)Top-ViewLocalizationinvestigateswhetherthe 2) Realistic Environmental Scenarios with Rich
model can localize objects or rooms in the top- ObjectSets: Weprovidereal-worldenvironments
viewmapbasedontextualdescriptions,including fromindoorscenes,with80objectspersceneon
ObjectLocalizationandSceneLocalizationastwo average, ensuring a natural distribution and com-
sub-tasks. Beyondunderstandingthetop-viewmap plexity of object locations. This also sets it apart
asawhole,itrequiresthemodeltogroundentities fromexistingfront-viewspatialreasoningdatasets,
inthemap,representingthemodel’sabilitytoalign whichoftencontainonlyahandfulofobjects.
spatialdescriptionswithcorrespondinglocations. 3)StructuredQuestionFramework: Unlikeprevi-
(3)StaticSpatialReasoningaimstoevaluatethe ous datasets (Li et al., 2023a; Liu et al., 2023a;
model’s spatial reasoning ability with more com- Kamath et al., 2023), which often conflate spa-
plexquestions. Itincludestwosub-tasks: reason- tialreasoningwithobjectrecognition,ourdataset
ingoverSceneCountingandRelativeSpatialRela- clearlydefinesfourtasksincluding9sub-tasksin
tionsbetweendifferentobjectsandrooms. These totalusingdiversequestiontemplates. Thisstruc-
questionsrequirethemodeltoperformmulti-step turedapproachallowsforafine-grainedevaluation
reasoningbasedontherecognitionandlocalization andanalysisofmodels’capabilitiesfromvarious
ofentitiesinthetop-viewmap. perspectivesandlevelsofgranularity.
(4) Dynamic Spatial Reasoning. Finally, we in- DatasetCollection. Weemployatwo-stagedata
troduceanoveltaskthatinvolvesdynamicspatial collectionstrategythatincludesautomaticcollec-
reasoning over top-view maps in the context of tionfromasimulatorandalignmentthroughhuman
agent navigation. It requires the model to under- judgment. First, to approximate real-life scenar-
standthesequentialrelationsalongthepointsofthe ios,weusetheMatterport3Ddataset(Changetal.,
4Realistic Map Object Room Spatial Description Relative Spatial Description
400
500
top right left
2000 250 200 middle left 11% top center right 11% down
14% 8% 12% 9%
top left up
0 0 0 middle center 12% 9% overlap 10% 10%
middle right
11% 12%15%7% bottom right up left 10% 14%13%11% down right
bottom left bottom center up right down left
Semantic Map
400
500 middle left top righ tt op center right left down
2000 250 200 14%11% 8% 11% top left overlap 12%11%10% 9% 9% up
middle center 11%
0 0 0
middle right
12% 11%14%8% bottom right up left 11% 14%13%12% down right
bottom left bottom center up right down left
Figure2: TOPVIEWRSdatastatistics,showingdistributionoftasksizes,objects,regions,spatialandrelativespatial
descriptionsinrealisticandsemanticmapsettings,wherethetasksaredescribedwiththeirinitialsforvisualization.
2017), which includes 90 building-scale scenes detailsinAppendixB.4.
withinstance-levelsemanticandroom-levelregion
5 ExperimentsandResults
annotations in 3D meshes. We filter these to ex-
clude multi-floor and low-quality scenes, select-
Models and Implementation. We test a repre-
ing7sceneswithanaverageof80objectsand12
sentativeselectionofbothopen-sourcedandclose-
roomseach. Realistictop-viewmapsareextracted
sourcedmodelswhichachievestate-of-the-artper-
usingorthographiccameras,andsemantictop-view
formance on a range of multimodal benchmarks
maps are constructed using the Habitat (Manolis
(Liuetal.,2023c;Lietal.,2023a)inazero-shotin-
Savva* et al., 2019; Szot et al., 2021) simulation
ferencesetup. Regardingopen-sourcedmodels,we
environment. Wethendesignastructuredquestion
studyandevaluateIdefics(9B&80B)(Laurençon
frameworkwith15templatestominimizehuman
etal.,2023),LLaVA-Next(7B,13B&34B)(Liu
labor and standardize the data collection process.
etal.,2024),InternLM-XComposer2(7B)(Dong
To ensure quality, a second stage of manual hu-
etal.,2024),Qwen-VL(7B)(Baietal.,2023). The
manjudgmentalignsandverifiesthedata,ensuring
chosen close-sourced models are GPT-4V (Ope-
questionsarenaturalandcorrect. Participantsare nAI, 2023) and Gemini (Google, 2024).5 All the
encouragedtodiscardormodifydatapointstoim-
modelsareimplementedwithintheVLMEvalKit
provequality,maintainingalignmentwithhuman
framework(OpenCompassContributors,2023).
judgments. WereferreaderstoAppendixBforfur-
Prompts. Forrealisticmaps,weprovidetheVLMs
therdetailsregardingthedatacollectionprocess.
withthetaskdescriptionalongwiththemultiple-
choicequestion. Forsemanticmaps,inadditionto
Dataset Statistics. The TOPVIEWRS evalua-
theinformationabove,wealsointroducethecon-
tion dataset comprises a total of 11,384 multiple-
ceptofasemanticmaptothemodelandprovide
choice questions after human verification, with
thecolor-objectmappinginthepromptinorderto
5,539questionsassociatedwithrealistictop-view
facilitateitsunderstandingoftheabstractmap. We
maps,and5,845withsemantictop-viewmaps. Hu-
onlyprovidethecolor-objectmappingsofthecol-
manverificationkeeps587/784questionsfromthe
orsthatarepresentedinthesemanticmapasapre-
automatic collection phase for Top-View Recog-
processing strategy in order to exclude irrelevant
nition, 1,077/1,384 for Top-View Localization,
information. Forthespecificpromptingtemplates
2,340/3,080 for Static Spatial Reasoning. The
usedinthispaper,werefertoAppendixC.2.
choices are uniformly distributed over choices A
(25.5%), B (24.6%), C (24.5%) and D (25.4%). Evaluation Measures. We measure multiple-
Figure2showsthedistributionofdifferenttasks, choice QA accuracy via Exact Match (EM) and
objects,regionsandspatialdescriptions. Thesize PartialMatch(PM).EMmeasureswhetherthepre-
ofeachtaskalignswithitscorrespondingdifficulty dicted option indices are exactly the same as the
level, where the easier task comprises fewer ex-
5WeuseGPT-4-turbo-2024-04-09ofGPT-4Vandlatest
amples. Weprovidefurtherinsightsandtechnical stablegemini-pro-vision1.0ofGemini.
5
RVT
RVT
LVT
LVT
RSS
RSS
RSD
RSD
deb
deb
riahc
riahc
elbat
elbat
noihsuc
noihsuc
afos
afos
gnitaes
gnitaes
moordeb
moordeb
moor
gninid
moor
gninid
moor
gnivil
moor
gnivil
moorhtab
moorhtab
nehctik
nehctik
sriats
teliotlabelindices. However,theremaybecaseswhere Larger models do not always show better spa-
the correct answer to the question can be consid- tial awareness. Surprisingly, our experiment re-
eredpartiallycorrect,e.g.,theansweristopright sults reveal that larger model sizes do not con-
whilethepredictionistopleft. PMthencalculates sistently translate to better performance. In Top-
theproportionofoverlappingwordsbetweenthe View Recognition, closed-source models outper-
predictedanswerandthegoldanswer. Itiscalcu- formopen-sourcemodelsby31.10%EMwithre-
latedbasedonthecorrectnessofthetextspans(or alisticmapsand29.33%EMwithsemanticmaps.
words)ofpredictedoptions,asgivenby: However,theperformancegapnarrowsasthetask
|{labels}∩{predictions}| complexityincreases. Usingrealisticmapsasthe
PM =
max(|{labels}|,|{predictions}|) visualinput,Geministandsoutbyachievingamin-
imumof5.53%higherEMaccuracyinStaticSpa-
5.1 ResultsandDiscussion
tial Reasoning compared to other models, while
We first discuss the models’ performance across
GPT-4VperformsworsethanIdefics-9Bonboth
ourfourtasks,withresultssummarizedinTable1,
StaticandDynamicSpatialReasoningtasks. This
andfine-grainedsub-taskperformanceillustrated
indicatesalackofsignificantdifferenceinspatial
inFigure3. Wefindthattheperformanceofcurrent
awarenessbetweenclosed-sourceandopen-source
state-of-the-artVLMsisunsatisfactoryonthepro-
models for tasks with higher complexity, despite
posed TOPVIEWRS benchmarkwithmodel-wise
thedisparityintheirmodelsizes. Thistrendholds
average EM and PM over all tasks below 50%.
true within open-sourced models as well. Both
Geminiisthebest-performingmodelforrealistic
Idefics and LLaVANext model families in some
maps,whileGPT-4Vexcelsinsemanticmaps. For
casesshowcomparableorworseperformancewith
some models, such as Qwen-VL, the results are
largersizedmodelsthansmallermodels. Similar
sometimesmuchworsethantherandombaseline.
observationshavebeenmadebypreviousstudies
Thisissueprimarilyarisesfromthemodels’diffi-
(Zhongetal.,2021;Shietal.,2024). Weconjecture
cultyinfollowingtheinstructionstochoosefrom
thatthismightbecausedbyinadequateevidence
thefourprovidedoptions.
ofthescalinglaw(Kaplanetal.,2020)inthecom-
Models perform better on recognition and lo- puter vision community (Tian et al., 2024). The
calization tasks compared to reasoning tasks. resultsonTOPVIEWRS thusadvocateforfurther
Top-ViewRecognitionconsistentlydemonstrates investigationandanalysisinthisarea.
thehighestperformanceacrossallmodels. Gemini
Models perform better in easier tasks with se-
showshuman-comparableperformancewithover
manticmaps. InsimpletaskssuchasTop-View
90% EM score. Top-View Localization exhibits
Recognition,modelsgenerallyperformbetterwith
lowerperformancecomparedtoTop-ViewRecog-
semanticmapsthanwithrealisticmaps,exceptfor
nition,followedbyStaticSpatialReasoning. The
Qwen-VL, showing an improvement of 20.35%.
performancedifferenceofvarioustaskswithdiffer-
However, this advantage decreases in more com-
entlevelsofcomplexityunderscorestheadvantage
plextasks. ForTop-ViewLocalizationandStatic
ofourbenchmarktocapturewell-definedanddis-
Spatial Reasoning, models struggle to utilize se-
entangledphenomena,whichallowsforcontrolled
mantictop-viewmaps,yieldingperformancesakin
studiesincontrolledenvironments.
torandombaselinesinbothEMandPMaccuracy.
RegardingDynamicSpatialReasoning,models
Onepossibleexplanationisthatthesemantictop-
perform better on this task than on the previous
viewimageandtheinputpromptwithcolor-object
tasks. Fine-grained performance in Figure 3 in-
mappingdeviatetoomuchfromthemodels’train-
dicates that the improved performance primarily
ingdatadistribution. Thisisfurtherevidencedby
stemsfromhighaccuracyindynamicactioncount-
thepredictionsfromopen-sourcedmodelssuchas
ingandspatiallocalization,whichconstitute18%
Qwen-VL,whichfailtorespondtoinstructionsand
and66%ofthedatarespectivelyforthistask. We
answerwithnumbersorRGBvalues91.25%ofthe
attribute the high accuracy in these areas to the
timeforTop-ViewLocalizationand47.65%ofthe
equivalencebetweennavigationpathsymbolsand
timeforStaticSpatialReasoning.
visualprompting(Shtedritskietal.,2023). Despite
these advancements, the overall EM accuracy re- Fine-GrainedInsightswithSub-Tasks Models
mains below 40%, and models still struggle with usingrealisticmapsexcelmoreinthesub-taskof
reasoningoverdynamicrelativespatialrelations. SceneRecognition,whichinvolveslargerentities,
6Model Idefics LLaVANext XComposer2 Qwen-VL GPT-4V Gemini
ModelSize 9B 80B vicuna7B mistral7B vicuna13B 34B 7B 7B API API
RealisticMap
EM 41.10 26.71 67.47 61.30 61.64 67.81 37.67 27.05 69.52 90.41
Top-ViewRecognition
PM 41.10 26.88 67.64 61.47 61.99 67.81 37.67 27.26 69.86 90.58
EM 30.39 30.00 42.16 33.92 41.18 50.98 27.84 16.27 46.27 48.24
Top-ViewLocalization
PM 46.42 46.08 56.67 48.63 54.31 61.76 41.86 26.31 60.39 60.98
EM 24.07 26.07 19.87 24.36 20.25 22.73 25.79 14.71 22.16 31.61
StaticSpatialReasoning
PM 33.68 38.52 34.40 37.34 36.26 35.56 38.73 21.15 35.59 45.22
EM 38.10 27.94 38.81 24.31 29.08 23.79 24.07 22.11 30.29 32.60
DynamicSpatialReasoning
PM 40.88 30.68 42.15 26.69 32.89 27.28 26.86 24.65 33.86 35.80
SemanticMap
EM 60.68 59.32 88.81 80.00 88.14 94.58 43.05 19.66 97.29 94.92
Top-ViewRecognition
PM 60.68 59.32 88.81 80.00 88.49 94.58 43.05 20.05 97.29 94.92
EM 31.21 27.34 25.40 32.10 17.28 38.45 24.87 9.70 44.44 35.27
Top-ViewLocalization
PM 47.62 45.41 44.27 47.80 23.66 53.79 41.09 13.99 58.55 49.91
EM 23.82 28.07 18.72 24.28 16.63 18.41 23.05 14.85 21.73 26.22
StaticSpatialReasoning
PM 34.13 38.17 30.57 37.26 29.94 31.22 35.50 21.99 33.09 39.12
EM 36.67 34.55 37.45 26.23 19.92 33.12 21.60 23.55 39.30 31.41
DynamicSpatialReasoning
PM 39.92 37.75 40.69 28.89 23.63 36.86 24.32 26.09 43.20 34.86
Table1:Comparisonof10modelsonbothrealisticandsemantictop-viewmaps. Performanceisanalysedaccording
tofourtaskswithEMandPM.Thebestperformanceforeachtaskisillustratedinbold.
 : L W K  6 H P D Q W L F  7 R S  9 L H Z  0 D S
 : L W K  5 H D O L V W L F  7 R S  9 L H Z  0 D S
 ,  ,  /
 /
 /
 / G  G  /
 /
 /
 / H  H  D
 D
 D
 D I  I  9
 9
 9
 9 L  L F  F  $
 $
 $
 $ V  V     1
 1
 1
 1     H
 H
 H
 H %    [
 [
 [
 [ %  W
 W
 W
 W 
 
 
  Y
 P
 Y
  L
 L
  F  F L V X
 X
 % W Q  Q U D  D D    O      %
  %
 %
 6 F H Q H  & R X Q W L Q   J
    
     / R 6  F F   D   H  O   L    Q   ] H   D      W L        R     Q    
    
 / R    F      2   D         O   E     L   ] M    H  D F   W   W  L    R Q
    
   6    F H Q   H      5    H F R J Q L W L R Q  6 F H Q H  & R X Q W L Q   J
    
     / R 6  F F   D   H  O   L    Q   ] H   D      W L        R     Q    
    
 / R    F      2   D         O   E     L   ] M    H  D F   W   W  L    R Q
    
   6    F H Q   H      5    H F R J Q L W L R Q
                                         
 , Q W H U Q / 0  ; & R P S R V H U 
 4  *
 *
 5
 + D 3
 H
 X Z
 P
 Q
 P 7 H  G  Q
 L  D Q
 R   9  Q P L 9 /  6  S  D  W L D O  5  H O D 5  W H  L R O D  Q W L Y H       $ F W   L   R  
 Q '   \
 & Q   R   D   X 
   P   Q   W 
 L  
 F
 L 
 
 Q 
  
 J             6 S D W   L '   D     
 
 O \     
 
 Q   /  
   D  R    P
 F   D   
 L  O F
 L ] D    W  
 L R Q    
 ' \ Q
 6   D   S   D P   W L   L F  D   O  5 H  5 O  H D  D W  V L
 R Y H
 Q L Q J
 6  S  D  W L D O  5  H O D 5  W H  L R O D  Q W L Y H       $ F W   L   R  
 Q '   \
 & Q   R   D   X 
   P   Q   W 
 L  
 F
 L 
 
 Q 
  
 J             6 S D W   L '   D     
 
 O \     
 
 Q   /  
   D  R    P
 F   D   
 L  O F
 L ] D    W  
 L R Q    
 ' \ Q
 6   D   S   D P   W L   L F  D   O  5 H  5 O  H D  D W  V L
 R Y H
 Q L Q J
(a)Performancewithrealistictop-viewmaps (b)Performancewithsemantictop-viewmaps
Figure3: Visualizationoffine-grainedcomparisonwith10modelsandhumanon9sub-tasksusingrealisticand
semantictop-viewmaps,demonstratingthatmostcurrentmodelsperformonparwithrandombaselineinspatial
reasoningandhasalargegapwithhumanperformance. ExactnumbersarereportedinTable15intheAppendix.
comparedtoObjectRecognition. Thisgapisalso tributions. Thus, models leverage commonsense
evidentina12.66%and19.73%performancedif- knowledgeastheshortcutforcounting,asseenin
ferencebetweenobject-levelandscene-levellocal- the 54.73% performance gap (with GPT-4V) be-
izationwithbothmaptypes. Conversely,withse- tweencountingscenesandactions. However,the
manticmaps,themodelstrugglesmorewithscene- spatiallocalizationandreasoningabilitiesofboth
levelrecognitionthanwithrealisticmaps,showing open-sourceandclosed-sourcemodelsstillremain
an 11.09% lower performance than object-level unsatisfactory,evenatthelevelofsub-tasks.
recognition among closed-source models. Most
modelsperformsimilarlytoarandombaselinein 5.2 FurtherDiscussion
reasoning over spatial relations but show higher
GaptoHumanPerformance. Wenowstudyhow
accuracyinscenecounting. Thislikelyoccursbe-
humans perform on this dataset and the gap be-
cause 95% of the correct room counts are within
tweencurrentmodelsandhumanperformance. To
a narrow range (1 or 2), reflecting real-life dis-
this end, we recruited 4 human participants who
7
 2 E M H F W  5 H F R J Q L W L R Q  2 E M H F W  5 H F R J Q L W L R QTask Ability Size Human GPT-4V Model GPT-4V Gemini
ObjectRecognition 5 95 100 w/o.CoT w.CoT ∆ w/o.CoT w.CoT ∆
TVR
SceneRecognition 5 100 80 RGB 22.16 26.74 +4.58 31.61 40.02 +8.41
ObjectLocalization 5 95 20 SceneCounting 76.74 25.58 -51.16 53.49 48.84 -4.65
TVL
SceneLocalization 10 85 60 RelativeSpatialRelations 19.82 26.79 +6.97 30.68 39.64 +8.96
SceneCounting 5 100 80 Semantic 21.73 28.07 +6.34 26.22 30.16 +3.94
SSR
RelativeSpatialRelation 10 80 0 SceneCounting 37.50 47.92 +10.42 20.83 29.17 +8.34
RelativeSpatialRelations 21.12 27.31 +6.19 26.43 30.20 +3.77
DynamicActionCounting 5 85 0
DSR DynamicSpatialLocalization 10 85 40
Table 3: Comparison of model performance with or
DynamicRelativeSpatialReasoning 5 85 0
withoutChainofThought(CoT)onStaticSpatialRea-
Table2:ComparisonwithhumanandGPT-4Vonallthe soning,showingthatCoThelpselicitspatialreasoning.
sub-tasks,demonstratingahugegapbetweenGPT-4V
andhuman. larger models (Wei et al., 2022; Li et al., 2023c),
weconductedexperimentswithGPT-4VandGem-
were not involved in dataset creation for human
initoevaluatethishypothesis. AsshowninTable
evaluation. Atotalof60datapointswithrealistic
3, incorporating CoT into the reasoning process
top-viewmapsarerandomlyselectedfromthesub-
notably enhances performance. Specifically, the
tasks,coveringallfine-grainedquestiontypes.6 We
models’accuracyimprovedby6.50%whenusing
useFleissKappaasthemeasureofinter-annotator
realisticmapsand5.14%withsemanticmaps. This
agreement. The kappa score is 0.747, indicating
improvementunderscoresthepotentialofstep-by-
substantialagreementsharedbythehumanpartic-
stepreasoninginenhancingtheefficacyofspatial
ipantsaccordingtoLandisandKoch(1977). The
reasoningtasks.
averageperformanceofthehumanparticipantsis
showninTable2,thescores90%accuracyacross
6 Conclusion
all the sub-tasks. The experimental results show
that there is still a large gap with human perfor- Inthisstudy,wedesignedfourtaskstoexaminethe
mance by over 50% across all the sub-tasks that capabilitiesofVLMsastop-viewspatialreasoners,
involve spatial awareness. We also observe that progressingfrombasictop-viewmapcomprehen-
withGPT-4V,humanperforms47.78%higherthan siontodynamicspatialreasoningalongnavigation
themodelonaverage. Thegapbetweenhumanand paths. Tofacilitatethisexamination,wecollecta
modelperformanceislargeroncomplexreasoning natural,high-qualitydataset, TOPVIEWRS,which
taskscomparedtotherecognitiontasks,indicating includes11,384multiple-choicequestions,featur-
plentyofroomforimprovement. ingeitherrealisticorsemantictop-viewmapsasthe
visualinput. Ourextensiveexperimentsinvolved
Chain-of-ThoughtHelpsElicitSpatialReason-
evaluating10VLMsacrossvariousmodelfamilies
ing. DuetothecompositionalityofStaticSpatial
ReasoningbasedonTop-ViewRecognitionandLo-
andsizeson TOPVIEWRS.Theresultshighlighta
criticalobservation: particularlyincomplexreason-
calizationintaskdesign,themodelissupposedto
ingtasks,VLMsfrequentlyperformonlyaswell
answerthequestionbasedonthelocationsofthe
asarandombaseline,withevenmorepronounced
entities in the top-view map. Inspired by this re-
deficitswhenhandlingtaskswithsemanticmaps.
quirement,weexploredwhetherChain-of-Thought
Moreover, there is a noticeable performance gap
(CoT)reasoning(Weietal.,2022)couldfacilitate
comparedtohumanannotators,underscoringthe
spatialreasoningbyinitiallypromptingthemodel
significant potential for further improvements in
to localize entities before producing the final an-
this field. In response to these findings, we dis-
swertothequestion. Toimplementthis,wemod-
coveredthatemployingchain-of-thoughtreasoning
ified the instruction to include: “You should first
enhancesmodelperformanceinspatialreasoning
localize the entity and then answer the question
by5.82%. Despitethisprogress,theoverallperfor-
basedonthelocations”,therebyencouragingthe
manceofVLMsonspatialreasoningremainsless
modeltoprocessinformationandthinkstepbystep.
than satisfactory. We hope that our study can set
ConsideringthatCoThasshowneffectivenessin
thestageforfutureresearchinmultimodalspatial
6Wedidnotrunhumanevaluationonsemanticmapsbe- reasoningandencouragefurtherinvestigationsinto
causetheyareinherentlyeasiertoreasonover;theyskipthe refiningthereasoningtechniques,movingVLMs
processofrecognizingtheobjectsbeforereasoning,which
closertohuman-levelproficiencyinunderstanding
makesthetasksimplerbutwithmoresufficientandaccurate
informationforreasoning. andreasoningoverreal-worldenvironments.
8Limitations Folco Bertini Baldassini, Mustafa Shukor, Matthieu
Cord,LaureSoulier,andBenjaminPiwowarski.2024.
The TOPVIEWRS dataset primarily evaluates Whatmakesmultimodalin-contextlearningwork?
modelperformanceinentityrecognition,localiza- Preprint,arXiv:2404.15736.
tion,andspatialreasoningover2Dtop-viewmaps.
Pratyay Banerjee, Tejas Gokhale, Yezhou Yang, and
However,itdoesnotyetincludetask-orientedplan-
ChittaBaral.2021. Weaklysupervisedrelativespa-
ningwithspatialawareness,whichinvolvesmore tialreasoningforvisualquestionanswering. InPro-
complexsequentialdecision-makinganddynamic ceedingsoftheIEEE/CVFInternationalConference
onComputerVision(ICCV),pages1908–1918.
interactions. Ourdatasetassumesonecorrectan-
swer per question, but exploring scenarios with Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir
multiple correct answers or no correct answers Latif,DavideScaramuzza,JoseNeira,IanReid,and
John J. Leonard. 2016. Past, present, and future
couldfurtherchallengesystemsandprovidevalu-
ofsimultaneouslocalizationandmapping: Toward
able insights. We also advocate for further re-
the robust-perception age. IEEE Transactions on
search to explore how spatial awareness in mod- Robotics,32(6):1309–1332.
els impacts downstream tasks such as navigation
KhyathiRaghaviChandu,YonatanBisk,andAlanW
instruction generation (Li et al., 2024a) and task
Black. 2021. Grounding ‘grounding’ in NLP. In
completion by language agents in real-world en- FindingsoftheAssociationforComputationalLin-
vironments(Parasharetal.,2023). Moreover,our guistics: ACL-IJCNLP2021,pages4283–4305,On-
study is currently limited to 2D top-view maps, line.AssociationforComputationalLinguistics.
whereasspatialreasoningcanencompassavariety
AngelChang,AngelaDai,ThomasFunkhouser,Maciej
of modalities and perspectives, such as 3D point Halber, MatthiasNiessner, ManolisSavva, Shuran
clouds. From the perspective of the models, the Song, Andy Zeng, and Yinda Zhang. 2017. Mat-
terport3d: Learningfromrgb-ddatainindoorenvi-
rapid progress in VLMs makes it hard to include
ronments. International Conference on 3D Vision
allnewreleasessuchasIdefics2(Laurençonetal.,
(3DV).
2024). Additionally,multimodalin-contextlearn-
ing(MICL)remainsunderexploredandisonlysup- Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter,
DannyDriess,PeteFlorence,DorsaSadigh,Leonidas
ported by VLMs trained with interleaved image-
Guibas,andFeiXia.2024. Spatialvlm: Endowing
text data (Baldassini et al., 2024). Although not
vision-languagemodelswithspatialreasoningcapa-
universalacrossallVLMs,MICLhasbeeneffec- bilities. Preprint,arXiv:2401.12168.
tive in handling out-of-distribution tasks (Zhang
Howard Chen, Alane Suhr, Dipendra Misra, Noah
et al., 2024b), which could also be interesting in
Snavely,andYoavArtzi.2019. Touchdown: Natural
TOPVIEWRS, especially with semantic maps as languagenavigationandspatialreasoninginvisual
visual inputs. In future work, we aim to extend streetenvironments. In2019IEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR).
ouranalysistoincludemoremodalities,evaluatea
IEEE.
broaderrangeofmodelsandtheircapabilities,and
investigateadditionaldownstreamtasksinvolving Junmo Cho, Jaesik Yoon, and Sungjin Ahn. 2024.
spatialawareness. Spatially-aware transformers for embodied agents.
InTheTwelfthInternationalConferenceonLearning
Representations.
Acknowledgments
MauriceDiesendruck,JianzheLin,ShimaImani,Gay-
The work was partly supported by a Royal Soci-
athriMahalingam,MingyangXu,andJieZhao.2024.
etyUniversityResearchFellowshipInclusiveand
Learning how to ask: Cycle-consistency refines
SustainableLanguageTechnologyforaTrulyMul- promptsinmultimodalfoundationmodels. Preprint,
tilingualWorld’(no221137)awardedtoIvanVulic´. arXiv:2402.08756.
WethankFangyuLiuandYeMaoforconstructive
TianaiDong,AlbertoTestoni,LucianaBenotti,andRaf-
feedbackonthedraftofthispaper. faellaBernardi.2021. Visuallygroundedfollow-up
questions: a dataset of spatial questions which re-
quire dialogue history. In Proceedings of Second
References International Combined Workshop on Spatial Lan-
guageUnderstandingandGroundedCommunication
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, forRobotics,pages22–31,Online.Associationfor
SinanTan, PengWang, JunyangLin, ChangZhou, ComputationalLinguistics.
andJingrenZhou.2023. Qwen-vl:Aversatilevision-
languagemodelforunderstanding,localization,text XiaoyiDong,PanZhang,YuhangZang,YuhangCao,
reading,andbeyond. Preprint,arXiv:2308.12966. Bin Wang, Linke Ouyang, Xilin Wei, Songyang
9Zhang, Haodong Duan, Maosong Cao, Wenwei Sebastian Koch, Narunas Vaskevicius, Mirco Colosi,
Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Pedro Hermosilla, and Timo Ropinski. 2024.
Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui Open3dsg: Open-vocabulary3dscenegraphsfrom
He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and pointcloudswithqueryableobjectsandopen-setre-
Jiaqi Wang. 2024. Internlm-xcomposer2: Master- lationships. Preprint,arXiv:2402.12259.
ingfree-formtext-imagecompositionandcompre-
J.RichardLandisandGaryG.Koch.1977. Themea-
hension in vision-language large model. Preprint,
surementofobserveragreementforcategoricaldata.
arXiv:2401.16420.
Biometrics,33(1):159–174.
Russell Epstein, E Z Patai, Joshua Julian, and Hugo
HugoLaurençon,LucileSaulnier,LéoTronchon,Stas
Spiers. 2017. The cognitive map in humans: Spa-
Bekman,AmanpreetSingh,AntonLozhkov,Thomas
tial navigation and beyond. Nature Neuroscience,
Wang, Siddharth Karamcheti, Alexander M. Rush,
20:1504–1513.
Douwe Kiela, Matthieu Cord, and Victor Sanh.
ChristianFreksa.1991. QualitativeSpatialReasoning, 2023. Obelics: Anopenweb-scalefiltereddataset
pages361–372. SpringerNetherlands,Dordrecht. of interleaved image-text documents. Preprint,
arXiv:2306.16527.
Gemini Team Google. 2024. Gemini: A family
Hugo Laurençon, Léo Tronchon, Matthieu Cord,
of highly capable multimodal models. Preprint,
and Victor Sanh. 2024. What matters when
arXiv:2312.11805.
building vision-language models? Preprint,
MeeraHahn,JacobKrantz,DhruvBatra,DeviParikh, arXiv:2405.02246.
JamesRehg,StefanLee,andPeterAnderson.2020.
BohaoLi,RuiWang,GuangzhiWang,YuyingGe,Yix-
Whereareyou? localizationfromembodieddialog.
iaoGe,andYingShan.2023a. Seed-bench: Bench-
InProceedingsofthe2020ConferenceonEmpirical
marking multimodal llms with generative compre-
MethodsinNaturalLanguageProcessing(EMNLP),
hension. Preprint,arXiv:2307.16125.
pages 806–822, Online. Association for Computa-
tionalLinguistics. ChengzuLi,ChaoZhang,SimoneTeufel,RamaSanand
Doddipatla,andSvetlanaStoyanchev.2024a. Seman-
MeeraHahnandJamesM.Rehg.2022. Transformer-
ticmap-basedgenerationofnavigationinstructions.
basedlocalizationfromembodieddialogwithlarge-
Preprint,arXiv:2403.19603.
scalepre-training. InProceedingsofthe2ndConfer-
enceoftheAsia-PacificChapteroftheAssociation Hao Li, Jinfa Huang, Peng Jin, Guoli Song, Qi Wu,
forComputationalLinguisticsandthe12thInterna- and Jie Chen. 2023b. Weakly-supervised 3d spa-
tional Joint Conference on Natural Language Pro- tial reasoning for text-based visual question an-
cessing (Volume 2: Short Papers), pages 295–301, swering. IEEETransactionsonImageProcessing,
Onlineonly.AssociationforComputationalLinguis- 32:3367–3382.
tics.
Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai
Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Wang, LeweiLu, HuijieWang, JiaZeng, ZhiqiLi,
Zheng,YilunDu,ZhenfangChen,andChuangGan. Jiazhi Yang, Hanming Deng, Hao Tian, Enze Xie,
2023. 3d-llm: Injectingthe3dworldintolargelan- Jiangwei Xie, Li Chen, Tianyu Li, Yang Li, Yulu
guagemodels. Preprint,arXiv:2307.12981. Gao,XiaosongJia,SiLiu,JianpingShi,DahuaLin,
andYuQiao.2024b. Delvingintothedevilsofbird’s-
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen- eye-viewperception:Areview,evaluationandrecipe.
sch,ChrisBamford,DevendraSinghChaplot,Diego IEEETransactionsonPatternAnalysisandMachine
delasCasas,FlorianBressand,GiannaLengyel,Guil- Intelligence,46(4):2151–2170.
laumeLample,LucileSaulnier,LélioRenardLavaud,
Marie-AnneLachaux,PierreStock,TevenLeScao, Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai
Thibaut Lavril, Thomas Wang, Timothée Lacroix, Wang, LeweiLu, HuijieWang, JiaZeng, ZhiqiLi,
andWilliamElSayed.2023. Mistral7B. Preprint, Jiazhi Yang, Hanming Deng, Hao Tian, Enze Xie,
arXiv:2310.06825. Jiangwei Xie, Li Chen, Tianyu Li, Yang Li, Yulu
Gao,XiaosongJia,SiLiu,JianpingShi,DahuaLin,
AmitaKamath,JackHessel,andKai-WeiChang.2023. andYuQiao.2024c. Delvingintothedevilsofbird’s-
What’s“up”withvision-languagemodels? investi- eye-viewperception:Areview,evaluationandrecipe.
gatingtheirstrugglewithspatialreasoning. InPro- IEEETransactionsonPatternAnalysisandMachine
ceedingsofthe2023ConferenceonEmpiricalMeth- Intelligence,46(4):2151–2170.
ods in Natural Language Processing, pages 9161–
9175,Singapore.AssociationforComputationalLin- Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang
guistics. Ren,Kai-WeiChang,andYejinChoi.2023c. Sym-
bolicchain-of-thoughtdistillation: Smallmodelscan
JaredKaplan,SamMcCandlish,TomHenighan,TomB. also“think”step-by-step. InProceedingsofthe61st
Brown,BenjaminChess,RewonChild,ScottGray, AnnualMeetingoftheAssociationforComputational
AlecRadford,JeffreyWu,andDarioAmodei.2020. Linguistics (Volume 1: Long Papers), pages 2665–
Scalinglawsforneurallanguagemodels. Preprint, 2679, Toronto, Canada. Association for Computa-
arXiv:2001.08361. tionalLinguistics.
10YunxinLi,LongyueWang,BaotianHu,XinyuChen, OpenAI.2022. Chatgptblogpost.
WanqiZhong,ChenyangLyu,WeiWang,andMin
Zhang.2023d. Acomprehensiveevaluationofgpt-4v OpenAI. 2023. GPT-4V(ision) Technical Work and
onknowledge-intensivevisualquestionanswering. Authors.
Preprint,arXiv:2311.07536.
OpenCompass Contributors. 2023. Opencompass:
Fangyu Liu, Guy Emerson, and Nigel Collier. 2023a.
A universal evaluation platform for foundation
Visualspatialreasoning. TransactionsoftheAssoci- models. https://github.com/open-compass/
ationforComputationalLinguistics,11:635–651. opencompass.
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuan-
PriyamParashar,VidhiJain,XiaohanZhang,JayVakil,
han Zhang, Sheng Shen, and Yong Jae Lee. 2024.
SamPowers,YonatanBisk,andChrisPaxton.2023.
Llava-next: Improved reasoning, OCR, and world
Slap: Spatial-language attention policies. In Pro-
knowledge.
ceedingsofThe7thConferenceonRobotLearning,
volume 229 of Proceedings of Machine Learning
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
Research,pages3571–3596.PMLR.
Lee.2023b. Visualinstructiontuning.
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Lang Peng, Zhirong Chen, Zhangjie Fu, Pengpeng
Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Liang, and Erkang Cheng. 2023. Bevsegformer:
Wang,ConghuiHe,ZiweiLiu,KaiChen,andDahua Bird’seyeviewsemanticsegmentationfromarbitrary
Lin.2023c. Mmbench: Isyourmulti-modalmodel camerarigs. In2023IEEE/CVFWinterConference
anall-aroundplayer? Preprint,arXiv:2307.06281. onApplicationsofComputerVision(WACV).IEEE.
Manolis Savva*, Abhishek Kadian*, Oleksandr ChengweiQin,AstonZhang,ZhuoshengZhang,Jiaao
Maksymets*,YiliZhao,ErikWijmans,BhavanaJain, Chen, Michihiro Yasunaga, and Diyi Yang. 2023a.
JulianStraub,JiaLiu,VladlenKoltun,JitendraMa- IsChatGPTageneral-purposenaturallanguagepro-
lik, Devi Parikh, and Dhruv Batra. 2019. Habitat: cessingtasksolver? InProceedingsofthe2023Con-
APlatformforEmbodiedAIResearch. InProceed- ferenceonEmpiricalMethodsinNaturalLanguage
ingsoftheIEEE/CVFInternationalConferenceon Processing,pages1339–1384,Singapore.Associa-
ComputerVision(ICCV). tionforComputationalLinguistics.
Roshanak Mirzaee and Parisa Kordjamshidi. 2022. Zequn Qin, Jingyu Chen, Chao Chen, Xiaozhi Chen,
Transferlearningwithsyntheticcorporaforspatial and Xi Li. 2023b. Unifusion: Unified multi-view
rolelabelingandreasoning. InProceedingsofthe fusion transformer for spatial-temporal represen-
2022 Conference on Empirical Methods in Natu- tation in bird’s-eye-view. In Proceedings of the
ral Language Processing, pages 6148–6165, Abu IEEE/CVF International Conference on Computer
Dhabi,UnitedArabEmirates.AssociationforCom- Vision(ICCV),pages8690–8699.
putationalLinguistics.
Navid Rajabi and Jana Kosecka. 2023. Towards
Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang
groundedvisualspatialreasoninginmulti-modalvi-
Ning, and Parisa Kordjamshidi. 2021. SPARTQA:
sionlanguagemodels. Preprint,arXiv:2308.09778.
Atextualquestionansweringbenchmarkforspatial
reasoning. InProceedingsofthe2021Conferenceof
Julia Rozanova, Deborah Ferreira, Krishna Dubba,
theNorthAmericanChapteroftheAssociationfor
Weiwei Cheng, Dell Zhang, and Andre Freitas.
ComputationalLinguistics: HumanLanguageTech-
2021. Groundingnaturallanguageinstructions: Can
nologies,pages4582–4598,Online.Associationfor
largelanguagemodelscapturespatialinformation?
ComputationalLinguistics.
Preprint,arXiv:2109.08634.
IdaMomennejad,HoseinHasanbeig,FelipeVieiraFru-
BaifengShi,ZiyangWu,MaolinMao,XinWang,and
jeri,HiteshiSharma,NebojsaJojic,HamidPalangi,
TrevorDarrell.2024. Whendowenotneedlarger
RobertNess,andJonathanLarson.2023. Evaluating
visionmodels? Preprint,arXiv:2403.13043.
cognitivemapsandplanninginlargelanguagemod-
elswithcogeval. InAdvancesinNeuralInformation
ProcessingSystems,volume36,pages69736–69751. ZhengxiangShi,QiangZhang,andAldoLipani.2022.
CurranAssociates,Inc. Stepgame: Anewbenchmarkforrobustmulti-hop
spatialreasoningintexts. InProceedingsoftheAAAI
LakshNanwani,AnmolAgarwal,KanishkJain,Raghav Conference on Artificial Intelligence, volume 36,
Prabhakar, Aaron Monis, Aditya Mathur, Kr- pages11321–11329.
ishna Murthy Jatavallabhula, A. H. Abdul Hafez,
Vineet Gandhi, and K. Madhava Krishna. 2023. AleksandarShtedritski,ChristianRupprecht,andAn-
Instance-level semantic maps for vision language dreaVedaldi.2023. Whatdoesclipknowaboutared
navigation. In202332ndIEEEInternationalConfer- circle? visualpromptengineeringforvlms. In2023
enceonRobotandHumanInteractiveCommunica- IEEE/CVF International Conference on Computer
tion(RO-MAN).IEEE. Vision(ICCV).IEEE.
11Andrew Szot, Alex Clegg, Eric Undersander, Erik JiayangWu, WenshengGan, ZefengChen, Shicheng
Wijmans, Yili Zhao, John Turner, Noah Maestre, Wan, and Philip S. Yu. 2023. Multimodal
Mustafa Mukadam, Devendra Chaplot, Oleksandr large language models: A survey. Preprint,
Maksymets, Aaron Gokaslan, Vladimir Vondrus, arXiv:2311.13165.
SameerDharur,FranziskaMeier,WojciechGaluba,
AngelChang,ZsoltKira,VladlenKoltun,Jitendra WenshanWu,ShaoguangMao,YadongZhang,YanXia,
Malik,ManolisSavva,andDhruvBatra.2021. Habi- LiDong,LeiCui,andFuruWei.2024. Visualization-
tat2.0: Traininghomeassistantstorearrangetheir of-thoughtelicitsspatialreasoninginlargelanguage
habitat. InAdvancesinNeuralInformationProcess- models. Preprint,arXiv:2404.03622.
ingSystems(NeurIPS).
YutaroYamada, YihanBao, AndrewKyleLampinen,
DamienTeney,LingqiaoLiu,andAntonVanDenHen- Jungo Kasai, and Ilker Yildirim. 2024. Evaluat-
gel.2017. Graph-structuredrepresentationsforvi- ingspatialunderstandingoflargelanguagemodels.
sualquestionanswering. In2017IEEEConference TransactionsonMachineLearningResearch.
onComputerVisionandPatternRecognition(CVPR),
pages3233–3241. ChengYan,GuansongPang,LeiWang,JileJiao,Xue-
taoFeng,ChunhuaShen,andJingjingLi.2021. Bv-
KeyuTian,YiJiang,ZehuanYuan,BingyuePeng,and person: A large-scale dataset for bird-view person
LiweiWang.2024. Visualautoregressivemodeling: re-identification. InProceedingsoftheIEEE/CVFIn-
Scalableimagegenerationvianext-scaleprediction. ternationalConferenceonComputerVision(ICCV),
Preprint,arXiv:2404.02905. pages10943–10952.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- ZhunYang,AdamIshay,andJoohyungLee.2023. Cou-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay plinglargelanguagemodelswithlogicprogramming
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti forrobustandgeneralreasoningfromtext. InFind-
Bhosale,DanBikel,LukasBlecher,CristianCanton ingsoftheAssociationforComputationalLinguis-
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu, tics: ACL2023,pages5186–5219,Toronto,Canada.
JudeFernandes,JeremyFu,WenyinFu,BrianFuller, AssociationforComputationalLinguistics.
CynthiaGao,VedanujGoswami,NamanGoyal,An-
thonyHartshorn,SagharHosseini,RuiHou,Hakan Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa, Sun, Tong Xu, and Enhong Chen. 2023. A sur-
IsabelKloumann,ArtemKorenev,PunitSinghKoura, veyonmultimodallargelanguagemodels. Preprint,
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di- arXiv:2306.13549.
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
tinet,TodorMihaylov,PushkarMishra,IgorMoly- Chao Zhang, Mohan Li, Ignas Budvytis, and
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- Stephan Liwicki. 2024a. Dialoc: An iterative ap-
stein,RashiRungta,KalyanSaladi,AlanSchelten, proach to embodied dialog localization. Preprint,
Ruan Silva, Eric Michael Smith, Ranjan Subrama- arXiv:2403.06846.
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Ji-
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
azheng Xu, and Peng Cui. 2024b. On the out-of-
Melanie Kambadur, Sharan Narang, Aurelien Ro-
distributiongeneralizationofmultimodallargelan-
driguez,RobertStojnic,SergeyEdunov,andThomas
guagemodels. Preprint,arXiv:2402.06599.
Scialom.2023. Llama2: Openfoundationandfine-
tunedchatmodels. Preprint,arXiv:2307.09288.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
David Unger, Nikhil Gosala, Varun Ravi Kumar, Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-
Shubhankar Borse, Abhinav Valada, and Senthil ichenZhang,JunjieZhang,ZicanDong,YifanDu,
Yogamani. 2023. Multi-camera bird’s eye view Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao
perception for autonomous driving. Preprint, Jiang,RuiyangRen,YifanLi,XinyuTang,Zikang
arXiv:2309.09080. Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
2023. Asurveyoflargelanguagemodels. Preprint,
JasonWei,XuezhiWang,DaleSchuurmans,Maarten arXiv:2303.18223.
Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,
andDennyZhou. 2022. Chain-of-thoughtprompt- BoyuanZheng,BoyuGou,JihyungKil,HuanSun,and
ing elicits reasoning in large language models. In YuSu.2024. Gpt-4v(ision)isageneralistwebagent,
AdvancesinNeuralInformationProcessingSystems, ifgrounded. Preprint,arXiv:2401.01614.
volume35,pages24824–24837.CurranAssociates,
Inc. Ruiqi Zhong, Dhruba Ghosh, Dan Klein, and Jacob
Steinhardt. 2021. Are larger pretrained language
Jason Weston, Antoine Bordes, Sumit Chopra, and models uniformly better? comparing performance
TomásMikolov.2016. Towardsai-completequestion attheinstancelevel. InFindingsoftheAssociation
answering: Asetofprerequisitetoytasks. In4thIn- forComputationalLinguistics: ACL-IJCNLP2021,
ternationalConferenceonLearningRepresentations, pages3813–3827,Online.AssociationforComputa-
ICLR2016,SanJuan,PuertoRico,May2-4,2016, tionalLinguistics.
ConferenceTrackProceedings.
12A AdditionalRelatedWork environments from Matterport3D (Chang et al.,
2017),weselectatotalof7scenes: 17DRP5sb8fy,
In addition to Section 2, we provide additional
2azQ1b91cZZ, 2t7WUuJeko7, 5LpN3gDmAk7,
relatedworkforthecomprehensiveunderstanding
EU6Fwq7SyZv, 8WUmhLawc2A, i5noydFURQK.
ofspatialreasoning.
Spatial Reasoning on Text. Spatial reasoning RealisticTop-ViewMap Weextractrealistictop-
has been investigated with the advancement of view maps using MeshLab by placing an ortho-
LLMs(Yamadaetal.,2024). Variousbenchmarks graphic camera on the top of the 3D scenes and
havebeenproposedtoevaluatemodels’spatialrea- takingacamerashot.
soningabilities,includingrelativespatialrelation
Semantic Top-View Map We construct the se-
recognition (Weston et al., 2016; Mirzaee et al.,
mantictop-viewmapwithHabitatsimulationenvi-
2021;Shietal.,2022),naturallanguagenavigation
ronment(ManolisSavva*etal.,2019;Szotetal.,
(Yamadaetal.,2024),andplanning(Momennejad
2021). Foreachbuildingfloor,Matterport3Dcon-
et al., 2023). Mirzaee and Kordjamshidi (2022)
tainsthe2Dand3Dsemanticsegmentationhuman
suggest that introducing synthetic data of spatial
annotations,whichcanberetrievedtoidentifythe
reasoningwhenpre-traininghelpstoimprovethe
type of objects as well as the rooms. The 3D co-
spatialawarenessofthemodel. Yangetal.(2023)
ordinatesoftheentity’s(objectandroom)center
justify the feasibility of using a logical form as
(x ,y ,h )andthesizeoftheentity’sboundingbox
i i i
anintermediaterepresentationtoimprovethespa-
(w ,w ,w )canalsoberetrievedaspartofthecir-
tial reasoning ability in easy scenarios. Instead x y h
cumstantialinformation. Thisinformationisthen
ofdescribingthespatialrelationswithnaturallan-
usedfortheconstructionofthesemantictop-view
guage,Wuetal.(2024)feedthemodelwitha2D
map.
squaregridsimilartoASCII-artformatandprove
When we obtain the object information for
thatvisualisingthereasoningprocedureexplicitly
the purpose of constructing a top-view seman-
helpstoimprovethemodel’sabilityinmulti-hop
tic map, we design certain rules to exclude spe-
spatial reasoning. Constrained by language de-
cific types of objects from all 40 object anno-
scriptions,mostdatasetsfocusonreasoningover
tation categories of Matterport3D. We believe
symbolswithinsimplescenarios(e.g. grid-based
these objects could either 1) be less meaningful
navigation)andaresyntheticallygenerated. How-
in terms of semantics or 2) take up a large area
ever, real-life scenarios are often more complex
in the semantic map, which obstructs other ob-
and rich in physical semantics. This raises con-
jectsbeneath. Thefilteredobjectsinclude:‘misc’,
cerns about the models’ actual spatial reasoning
‘ceiling’, ‘objects’, ‘floor’, ‘wall’,
abilities compared to their proficiency in under-
‘void’, ‘curtain’, ‘column’, ‘beam’,
standinglinguisticpatterns.
‘board panel’.
B DatasetConstruction We also filter out the objects based on their
heightsh andsizesw comparedtotherooms’
obj obj
The TOPVIEWRS is derived from Matterport3D
heightsh andsizesw . Weonlykeepthe
room room
(Changetal.,2017)fornon-commercialacademic
objectsiftheysatisfythefollowingrelations:
useonly,undertheTermofUse(MatterportEnd
User Licence Agreement For Academic Use of 1 1
0.9×(h − w ) ≤ h − w
room room obj obj
ModelData). 2 2
InadditiontotheintroductioninSection4,we
1 1
providefurtherdetailswithregardtoTOPVIEWRS 1.1×(h + w ) ≥ h + w
room room obj obj
2 2
datasetconstruction.
Afterhavingalltheobjectannotations,weuse
B.1 Top-ViewMapConstruction theget_topdown_mapAPIoftheHabitatsimula-
To ensure high-quality top-view map representa- tor to get the top-down map of the scene, which
tions, we exclude the 3D environments with low describesthenavigableareaandtheoverallshape
coverage of mesh grids. We also prefer environ- oftheenvironment,butwithoutanyobjectannota-
ments that are single-floor, in order to avoid the tions. Basedonthismap,wethendrawthebound-
obstruction of objects from different floors. Af- ing boxes with different colors to represent the
termanuallygoingthrough90building-scale3D objectsintheenvironments. Consideringthatthe
13objects on the top may obstruct the bottom ob- After having the top-view maps of the whole
jectsinthetop-viewmap,tomimicthischaracteris- floor,wecropthemintosmallerroomsaccording
tic,wecreatethesemantictop-viewmapbasedon totheregionboundariesobtainedfromtheHabitat
theheightsoftheobjects,wherelowerobjectsare simulator.
drawn first. Table 4 shows the mapping between
B.2 StructuredQuestionFrameworkDesign
theRGBvaluesandobjecttypesusedforthecre-
ationofasemantictop-viewmapinourwork. Inordertominimizehumanlabourandstandard-
izethecollectionpipeline,weadoptthetemplate-
RGBValues Label based question generation method following the
[31,119,180] void practiceofLiuetal.(2023a)anddesign15differ-
[174,199,232] wall enttemplatesintotaltoconstructthesub-tasksfor
[255,127,14] floor eachtask. Specifically,weconsiderbenchmarking
[255,187,120] chair differentperspectivesofthemodel’sabilitywithin
[44,160,44] door each task in a fine-grained manner when design-
[152,223,138] table ingthetemplates. Thequestiontemplatesarealso
[214,39,40] picture multi-scaleintermsofobjectsorroomswithfullor
[255,152,150] cabinet partialtop-viewmapsforTop-ViewRecognition,
[148,103,189] cushion Top-ViewLocalizationandStaticSpatialReason-
[197,176,213] window ing. ForDynamicSpatialReasoning,thedesigned
[140,86,75] sofa questions evaluate the recognition and reasoning
[196,156,148] bed fromthescaleofsinglenavigationpoints(Dynamic
[227,119,194] curtain Action Counting and Spatial Localization) to the
wholepath(DynamicRelativeSpatialReasoning).
[247,182,210] chest_of_drawers
Belowweprovidethedesignedtemplatesforall
[51,105,30] plant
9 sub-tasks that fall into a total of 4 tasks, with
[199,199,199] sink
concrete examples shown in Figure 1. We also
[188,189,34] stairs
introducethelogicforselectingthecorrectanswer
[219,219,141] ceiling
and other wrong choices when constructing the
[23,190,207] toilet
multiple-choicequestions.
[158,218,229] stool
[57,59,121] towel B.2.1 Top-ViewRecognition
[82,84,163] mirror
[107,110,207] tv_monitor
ObjectRecognition
[156,158,222] shower
[99,121,57] column Template1 Which of the following objects
[140,162,82] bathtub areintheroom?
[181,207,107] counter Template2 Which of the following objects
[206,219,156] fireplace arenotintheroom?
[140,109,49] lighting
SceneRecognition
[189,158,57] beam
[231,186,82] railing Template1 Whatroomisthis?
[231,203,148] shelving
Template2 Whattypesofroomsareincluded
[132,60,57] blinds
inthetop-viewmapbelow?
[173,73,74] gym_equipment
[214,97,107] seating Table5: TemplatesforObjectandSceneRecognition
[231,150,156] board_panel
[123,65,115] furniture Table 5 shows the templates we use for
[165,81,148] appliances the Top-View Recognition task. Considering
[206,109,189] clothes that some objects and rooms may be hard
[222,158,214] objects to recognize from the top view, in addition
to the set of filtered objects, we also re-
Table4: RGBvaluesandcorrespondinglabels. move some objects (‘picture’, ‘mirror’,
14‘window’, ‘blinds’, ‘towel’, ‘furniture’, ‘bathtub’, ‘stool’, ‘plant’, ‘stairs’,
‘door’, ‘tv_monitor’, ‘cabinet’)androoms ‘shower’, ‘fireplace’, ‘gym_equipment’,
(‘hallway’, ‘entryway/foyer/lobby’, ‘tv’) ‘seating’.
whenweusethetemplatestogeneratequestions.
B.2.4 DynamicSpatialReasoning
B.2.2 Top-ViewLocalization
DynamicActionCounting
ObjectLocalization
Template1 Howmanyturning<action>are
Template1 Whereisthe<object>inthetop- therealongthepath?
downmap?
DynamicRelativeSpatialReasoning
SceneLocalization
Template1 Whichdirectiondoesthenaviga-
Template1 Whereisthe<room>inthetop- tionpathheadfor?
downmap?
DynamicSpatialLocalization
Template2 Whatobjectsdoes<room>have?
Template1 Whatroomsdoesthenavigation
pathpassby?
Table6: TemplatesforObjectandSceneLocalization
Tasks Template2 Atwhichroomdoesthenaviga-
tionpath<action>?
Table 6 shows the templates for the Top-
View Localization task. For the objects, we Template3 Atwhichobjectdoesthenaviga-
adopt the range in Top-View Recognition. For tionpath<action>?
the rooms, we define a set of rooms that are
Table8: TemplatesforDynamicActionCounting,Dy-
easy and natural to recognize for humans, in-
namicRelativeSpatialReasoning,andDynamicSpatial
cluding: ‘office’, ‘workout/gym/exercise’,
LocalizationTasks
‘kitchen’, ‘bedroom’, ‘dining room’,
‘bar’, ‘balcony’, ‘toilet’, ‘bathroom’,
For Dynamic Action Counting, we define that
‘living room’, ‘stairs’.
avalidturnshouldinvolvemorethana30-degree
B.2.3 StaticSpatialReasoning rotation. ForDynamicRelativeSpatialReasoning,
thedirectionisalsodefinedbytherelativespatial
relationbetweenthestartingpointandendingpoint,
SceneCounting
wherethespatialdescriptionisdeterminedby30
Template1 How many <room> are there in
degreeintervals.
themap?
Multiple-Choice Question-Answer Pairs For
RelativeSpatialRelation
the answer to the questions, because we have all
Template1 What’s<object1>’srelativespa- thespatialinformationandsemanticannotationof
tialrelationto<object2>? the objects in the scene, we write a set of rules
with code for each type of question in order to
Template2 What’s <room1>’s relative spa-
automaticallyobtainthegoldenansweraccording
tialrelationto<room2>?
tothesimulationenvironments. Forallthewrong
choices in the multiple-choice settings, they are
Table 7: Templates for Scene Counting and Relative
SpatialRelationTasks randomlychosenfromotherpossiblecandidatesof
thesamekind(e.g. objects,rooms,numbers,etc.).
Table 7 lists the templates for the Static Spa- After having all the options for multiple-choice
tial Reasoning task. For rooms, we restrict the questions, werandomizetheorderoftheoptions
regions within the same range as in Top-View to make the correct choices evenly distributed in
Localization. For the objects, we focus on the ABCD.
objects that are common and big enough to rec-
ognize in daily life, which includes: ‘chair’, B.3 AlignmentwithHumanJudgments
‘table’, ‘cushion’, ‘sofa’, ‘bed’, We realize that semantic annotations of environ-
‘chest_of_drawers’, ‘sink’, ‘toilet’, ments may sometimes be inaccurate. Moreover,
15even though we exclude some unreasonable ob- tionsinStaticSpatialReasoningwhilebeingless
jects, the top views of objects can sometimes be frequentlyusedasabsolutespatialdescriptionsin
challengingtorecognize,evenforhumans. Toad- Top-ViewLocalization.
dresstheseissues,wehaveimplementedasecond Regarding the sizes for each sub-task, object-
stage in our dataset creation process: alignment levelrecognitionandlocalizationtakealargepor-
andverificationbasedonhumanjudgments. tionofdataintheTop-ViewRecognitionandLo-
When validating the automatically collected calizationtasks. ForStaticSpatialReasoning,rea-
data,thehumanparticipantsaresupposedtocheck soningoverrelativespatialrelationstakesthemain
the correctness of the question-answer pair and partofthedata. DynamicSpatialLocalizationis
chooseoneofthefollowingfouractionsaccording thelargesttaskaccordingtothesizes. Thenumbers
totheirownjudgments: 1)skipthedataifitistoo aredifferentwithrealisticmapsandsemanticmaps
badandstrange,2)modifythedatapairbyreplac- foreachtask. Thedisparitystemsfromthesecond
ing the options or the entities in the question in stage of dataset creation, where the human anno-
ordertomakeitanswerablebyhumans,3)correct tatorsmayexcludesomedatapointswithrealistic
the answer if it’s wrong, 4) keep the data if it is mapsduetovariouspossiblereasonsincludingbut
answerablebyhumansandcorrect. Inordertoen- notlimitedtotheobscureimageoretc.
surethequalityofthedataset,wecommunicatedto
C Experiments
thehumanparticipantsthattheyaresupposedtobe
cautiousof"accepting"adatapoint. Onapractical
C.1 InferenceParameters
level,theparticipantsmayeitherdiscardthisdata
Weadoptmostoftheinferenceparametersforeach
point or modify the options of this data to make
modelfromtheimplementationsofVLMEvalKit
thecorrectchoicemoredistinguishablebyhumans.
(OpenCompass Contributors, 2023). Table 10
Thishelpstoexcludethedatapointswherediffer-
shows the configuration of the inference process
enthumanjudgesmaydivergeandthusensurethe
for different models. If not specified in Table 10,
alignmentbetweenthedatasetandgeneralhuman
weusethedefaultconfigurationinHuggingface.
judgments.
We conduct human alignment on Top-View
C.2 Prompt
Recognition, Top-View Localization and Static
SpatialReasoning. Wedidn’tdohumanalignment Table11and12showtheprompttemplatesofeach
on Dynamic Spatial Reasoning because we fall taskusedinthemainexperiments(Table1)withre-
shortofhandsingoingoverthequalityofallthe alisticandsemantictop-viewmapsasvisualinput
datapoints. Therefore,inourexperiments,wealso respectively. Table13and14showtheprompttem-
providethecorrespondingrulesofhowweobtain platesusedforChain-of-Thoughtreasoningusing
theanswerforthemodelwithtextualdescription realisticandsemantictop-viewmaps(Table3).
intheprompt(seeAppendixC.2). Within the prompt templates, <QUESTION> and
<OPTIONS>arereplacedwiththequestionandop-
B.4 DatasetStatistics tion list O = {o ,o ,o ,o } (e.g. “A. bed; B.
0 1 2 3
Weprovidefurtherinsightsaboutthedatasetswith chair; C. table; D. cushion”). For semantic top-
regardtotheobjectandroomdistributioninFigure
viewmaps,<MAPPING>isreplacedwiththeRGB-
4andsub-tasksstatisticsinTable9. objectmapping,asshownbelow.
Thevisualizationdemonstratesthattheobjects (196, 156, 148) -> bed
or regions that are hard to recognize (e.g. gym (44, 160, 44) -> door
equipment, utility room, etc.) show fewer occur- ...
rencewithinourdatasetcomparedtothosewhich
In the task of Dynamic Spatial Reasoning,
are easy to identify with little obscure (e.g. bed,
<TASK-SPECIFIC INSTRUCTION> contains the
table,bedroom,etc.). Bed,chairandtablearethe
rules of how we obtain the answer from the sim-
top-3mostfrequentlymentionedobjectsandbed-
ulatorforthesub-taskDynamicActionCounting,
room, dining room and living room are the most
whichisdescribedasfollows.
common regions in the dataset. Among all the
spatialdescriptions,thediagonalspatialrelations Suppose you are a navigation agent tracing
(e.g. topright,upleft)aremorefrequentlyreferred the path. Your job is to assess whether
toasthecorrectchoiceasrelativespatialdescrip- there's a turn at each intermediate point
16Object Distribution of Correct Answer (Realistic map)
600
500
400
300
200
100
0
Object Distribution of Correct Answer (Semantic map)
600
500
400
300
200
100
0
Room Distribution of Correct Answer (Realistic Map)
400
300
200
100
0
Room Distribution of Correct Answer (Semantic Map)
400
300
200
100
0
Figure4: DatasetStatisticalAnalysis
Task Sub-Task Realistic Semantic
TVR ObjectRecognition 195 198
SceneRecognition 97 97
TVL ObjectLocalization 410 470
SceneLocalization 100 97
SSR SceneCounting 43 48
RelativeSpatialRelation 1,004 1,245
DSR DynamicActionCounting 668 668
DynamicSpatialLocalization 2,436 2,436
DynamicRelativeSpatialReasoning 586 586
Total 5,539 5,845
Table9: Distributionofsub-taskswithrealisticandsemantictop-viewmaps.
17
deb
deb
moordeb
moordeb
riahc
riahc
moor
gninid
moor
gninid
elbat
elbat
moor
gnivil
moor
gnivil
noihsuc
noihsuc
moorhtab
moorhtab
afos
afos
nehctik
nehctik
gnitaes
gnitaes
sriats
teliot
knis
knis
teliot
sriats
sriats
teliot
eciffo
eciffo
teliot
sriats
rab
rab
loots
loots
egnuol
egnuol
sreward_fo_tsehc
sreward_fo_tsehc
egnuol/moorylimaf
egnuol/moorylimaf
rewohs
ecalperif
moordum/mooryrdnual
moordum/mooryrdnual
tenibac
rewohs
emag/cer
emag/cer
buthtab
buthtab
ecalperif
tenibac
tesolc
tesolc
tnalp
tnalp
kced/ecarret/hcrop
kced/ecarret/hcrop
tnempiuqe_myg
tnempiuqe_myg
moorloot/moorytilitu
moorloot/moorytilituIdefics9B&80B
max_new_tokens 20
LLaVANext7B&13B&34B
temperature 0
num_beams 1
max_new_tokens 20
do_sample False
top_p None
XComposer2
temperature 1
beams 5
max_token 20
repetition_penalty 1
do_sample False
Qwen-VL
max_new_tokens 20
GPT4V
temperature 0
max_tokens 1024
img_size 512
img_detail low
Gemini
temperature 0
max_tokens 1024
Table10: Configurationsofinferenceparameters.
and sum up the total turns for the final
outcome.
Forothersub-tasksinDynamicSpatialReason-
ing,<TASK-SPECIFIC INSTRUCTION>isreplaced
withanemptystring.
C.3 ExperimentalResults
Table 15 shows the fine-grained sub-task perfor-
mance of all the models, which corresponds to
Figure3.
18RealisticTop-ViewMaps
Top-ViewRecognition,Top-ViewLocalizationandStaticSpatialReasoning
Thisisatop-viewmapofaroom. Pleaserespondtothequestionbelowbyselectingonechoice
fromalistofavailableoptionsprovided. Yourresponseshouldonlyincludetheletterofthechosen
option(A,B,C,orD)withnoadditionalexplanation.
Question: <QUESTION>
Options: <OPTIONS>;
Answer:
DynamicSpatialReasoning
Thisisatop-viewmapofaroomwiththenavigationpath. Thepathstartsfromthegreentriangle
(RGB[0,255,0])andendsattheredstar(RGB[255,0,0]). Thedirectionofthepathisdenoted
byaseriesofyellowarrows(RGB[255,255,0]),withintermediatepointshighlightedinRGB
[25, 255, 255]. <TASK-SPECIFIC INSTRUCTION> Please respond to the question below by
selectingonechoicefromalistofavailableoptionsprovided. Yourresponseshouldonlyinclude
theletterofthechosenoption(A,B,C,orD)withnoadditionalexplanation.
Question: <QUESTION>
Options: <OPTIONS>;
Answer:
Table11: Prompttemplatesformainexperimentswithrealistictop-viewmaps.
19SemanticTop-ViewMaps
Top-ViewRecognition,Top-ViewLocalizationandStaticSpatialReasoning
This is a semantic top-view map of a room. Various objects are depicted by colored bounding
boxes, each with its corresponding color, and there may be instances of overlap between them.
Below are the RGB color codes associated with each object, presented in the format RGB ->
Object:
<MAPPING>
Please respond to the question below by selecting one choice from a list of available options
provided. Yourresponseshouldonlyincludetheletterofthechosenoption(A,B,C,orD)with
noadditionalexplanation.
Question: <QUESTION>
Options: <OPTIONS>;
Answer:
DynamicSpatialReasoning
Thisisasemantictop-viewmapofaroomwiththenavigationpath. Inthesemanticmap,various
objectsaredepictedbycoloredboundingboxes,eachwithitscorrespondingcolor,andtheremay
beinstancesofoverlapbetweenthem. Thenavigationpathstartsfromthegreentriangle(RGB[0,
255,0])andendsattheredstar(RGB[255,0,0]). Thedirectionofthepathisdenotedbyaseries
ofyellowarrows(RGB[255,255,0]),withintermediatepointshighlightedinRGB[25,255,255].
BelowaretheRGBcolorcodesassociatedwitheachobjectandsymbol,presentedintheformat
RGB->Object:
<MAPPING>
<TASK-SPECIFICINSTRUCTION>Pleaserespondtothequestionbelowbyselectingonechoice
fromalistofavailableoptionsprovided. Yourresponseshouldonlyincludetheletterofthechosen
option(A,B,C,orD)withnoadditionalexplanation.
Question: <QUESTION>
Options: <OPTIONS>;
Answer:
Table12: Prompttemplatesformainexperimentswithsemantictop-viewmaps.
RealisticTop-ViewMaps
StaticSpatialReasoning
Thisisatop-viewmapofaroom. Pleaserespondtothequestionbelowbyselectingonechoice
fromalistofavailableoptionsprovided. Youshouldexplainyourreasoningstep-by-stepbyfirst
localizingtheentitiesandthenreasoningoverthequestionbasedonthelocations. Youshould
concludeyourchosenoption(A,B,C,orD)startingwith’Theansweris’.
Question: <QUESTION>
Options: <OPTIONS>;
Answer: Let’sthinkstepbystep.
Table13: PrompttemplatesforChain-of-Thoughtexperimentswithrealistictop-viewmaps.
20SemanticTop-ViewMaps
StaticSpatialReasoning
This is a semantic top-view map of a room. Various objects are depicted by colored bounding
boxes, each with its corresponding color, and there may be instances of overlap between them.
Below are the RGB color codes associated with each object, presented in the format RGB ->
Object:
<MAPPING>
Please respond to the question below by selecting one choice from a list of available options
provided. Youshouldexplainyourreasoningstep-by-stepbyfirstlocalizingtheentitiesandthen
reasoningoverthequestionbasedonthelocations. Youshouldconcludeyourchosenoption(A,B,
C,orD)startingwith’Theansweris’.
Question: <QUESTION>
Options: <OPTIONS>;
Answer: Let’sthinkstepbystep.
Table14: PrompttemplatesforChain-of-Thoughtexperimentswithsemantictop-viewmaps.
Idefics LLaVANext XComposer2 Qwen-VL GPT-4V Gemini
ModelSize 9B 80B vicuna7B mistral7B vicuna13B 34B 7B 7B API API
RealisticMap
ObjectRecognition 32.31 25.64 66.15 61.03 58.97 65.64 38.97 17.95 68.21 89.23
TVR
SceneRecognition 58.76 28.87 70.10 61.86 67.01 72.16 35.05 45.36 72.16 92.78
ObjectLocalization 26.83 26.10 40.24 30.24 40.00 50.49 26.34 16.34 40.73 45.21
TVL
SceneLocalization 45.00 46.00 50.00 49.00 46.00 53.00 34.00 16.00 69.00 61.00
SceneCounting 25.58 32.56 16.28 18.60 2.33 16.28 25.58 48.84 76.74 53.49
SSR
RelativeSpatialRelations 24.00 25.80 20.02 24.60 21.02 23.01 25.80 13.25 19.82 30.68
DynamicActionCounting 31.89 26.80 27.54 26.95 25.30 27.40 27.54 32.34 22.01 26.95
DSR DynamicSpatialLocalization 42.57 29.27 45.03 23.89 32.88 24.63 22.62 20.11 34.15 35.30
DynamicRelativeSpatialReasoning 26.62 23.72 25.77 23.04 17.58 16.21 26.11 18.77 23.72 27.82
SemanticMap
ObjectRecognition 54.55 51.01 92.93 87.37 92.42 98.48 50.00 12.63 100.00 99.49
TVR
SceneRecognition 73.20 76.29 80.41 64.95 79.38 86.60 28.87 34.02 91.75 85.57
ObjectLocalization 28.51 23.19 22.98 28.94 10.85 34.47 24.47 6.17 41.49 31.28
TVL
SceneLocalization 44.33 47.42 37.11 47.42 48.45 57.73 26.80 26.80 58.76 54.64
SceneCounting 37.50 50.00 12.50 10.42 6.25 4.17 14.58 22.92 37.50 20.83
SSR
RelativeSpatialRelations 23.29 27.23 18.96 24.82 17.03 18.96 23.37 14.54 21.12 26.43
DynamicActionCounting 36.68 27.69 33.38 26.80 28.89 25.30 27.25 30.39 22.90 26.95
DSR DynamicSpatialLocalization 39.20 38.79 41.87 25.82 17.98 39.00 19.46 22.95 47.17 33.42
DynamicRelativeSpatialReasoning 26.11 24.74 23.72 27.30 17.75 17.58 24.06 18.26 25.26 28.16
Table15: Fine-grainedresultsof10VLMsondifferentsub-tasks,correspondingtothevisualizationinFigure3.
21