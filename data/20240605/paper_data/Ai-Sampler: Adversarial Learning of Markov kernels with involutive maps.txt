Ai-Sampler: Adversarial Learning of Markov kernels with involutive maps
EvgeniiEgorov*1 RicardoValperga*2 EfstratiosGavves2
Abstract applicableingeneraland,mostimportantly,theyareasymp-
totically unbiased (Roberts & Rosenthal, 2004). In the
MarkovchainMonteCarlomethodshavebecome
recentyears,theevolutionofdeepneuralnetworkshasno-
popularinstatisticsasversatiletechniquestosam-
tablypropelledthefieldofVariationalInference(Rezende&
plefromcomplicatedprobabilitydistributions. In
Mohamed,2015;Kingma&Welling,2014;Rezendeetal.,
this work, we propose a method to parameter-
2014; Kingma et al., 2016) whilst MCMC methods have
izeandtraintransitionkernelsofMarkovchains
notbenefitedmuchfromtheseadvances. Usingneuralnet-
to achieve efficient sampling and good mixing.
worksinsideMCMCalgorithmswouldnotonlyresultin
Thistrainingprocedureminimizesthetotalvari-
morepowerfulandflexibleinferencemethods,capableof
ation distance between the stationary distribu-
handlingcomplex,high-dimensionalproblemsmoreeffec-
tion of the chain and the empirical distribution
tively,butalsoexploitthetremendousadvancementsineffi-
of the data. Our approach leverages involutive
ciencyofhardwarespecificallydesignedforneuralnetwork
Metropolis-Hastingskernelsconstructedfromre-
computations,suchasGPUsandTPUs. Wearguethatthis
versibleneuralnetworksthatensuredetailedbal-
stagnationindevelopingMCMCalgorithmsthatmakeuse
ance by construction. We find that reversibility
ofneuralnetworksispartlyduetothewell-knowndifficulty
alsoimpliesC -equivarianceofthediscriminator
2 ofmeasuringsamplequality(seeforexampleGorham&
functionwhichcanbeusedtorestrictitsfunction
Mackey(2015);Brooksetal.(2011)),meaningthatsystem-
space.
aticallyestablishingtheconvergenceofachaintoitstarget
distribution,ordefiningaqualitymeasureforsamplesfrom
aMarkovchainischallenging. Thismakesitintrinsically
1.Introduction
hardtodefineanobjectivefunctionthatcanbeoptimized
MarkovChainMonteCarlo(MCMC)isakeyapproachin withstochasticgradientdescenttoimprovetheperformance
statisticsandmachinelearningwhenitcomestosampling oftheMarkovchain. Ingeneral,thedesignofasuitableob-
fromcomplexunnormalizeddistributions. MCMCgener- jectivefunctionposesachallengeasonemustbalancetwo
atessamplesbysettingupaMarkovchainthathasthetarget competinggoals: encouragingboth high-qualitysamples
distributionasitsstationarydistribution. Onceconverged, andgoodexplorationofthewholespace(Levyetal.,2018).
samplescanbeobtainedbyrecordingstatesfromthechain. Inlightofthis,weattempttoanswerthesimplequestion:
NotonlyhaveMCMCmethodstransformedBayesianin- howcanwelearntosamplefromagivenunnormalizeddis-
ference,allowingtosamplefromtheuntractableposterior tribution,usingneuralnetwork-basedMCMCmethods? To
distributionsofcomplexprobabilisticmodels,buttheyare answer,weproposeanovelMCMCmethodthatmakesuse
alsowidelyusedforintegralestimation,time-seriesanaly- oftime-reversibleneuralnetworksforthetransitionkernel
sisandmanyotherproblemsinstatisticsandprobabilistic and derive an upper bound to the total variation distance
modelling(Robertetal.,2004). TogetherwithVariational betweenthestationarydistributionoftheresultingMarkov
Inference(Bleietal.,2017),theyarethetwoprimaryproba- chainandthetargetdistribution. Ourcontributioncanbe
bilisticinferencemethodsinmachinelearningandstatistics. summarizedasfollows: wefirstderiveanMCMCmethod
Compared to Variational Inference, MCMC methods are basedonreversibleneuralnetworksthatistrainedasanim-
oftenlessefficientbecauseoftheiriterativenature,butmore plicitgenerativemodelusinganoveladversarialobjective
fromagivenempiricaldatadistribution. Theproposedob-
*Equalcontribution 1AmsterdamMachineLearningLab,Uni-
jectivemakesuseofadiscriminator. Weprovetheoptimal
versityofAmsterdam, theNetherlands2RicardoValpergaVIS-
discriminator to be equivariant with respect to the cyclic
Lab,UniversityofAmsterdam,theNetherlands.Correspondence
to:EvgeniiEgorov<egorov.evgenyy@ya.ru>,RicardoValperga groupoforder2andproposeaclassofC 2-equivariantfunc-
<r.valperga@uva.nl>. tionsthatcanbeusedtoparameteriseit. Finally,weusea
bootstrapprocesstolearntosamplefromagivenanalytic
Proceedings of the 41st International Conference on Machine
densityandshow,onvarioussynthetic,andreal-worldex-
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
amples, thattheMarkovchainresultingfromthelearned
theauthor(s).
1
4202
nuJ
4
]GL.sc[
1v09420.6042:viXraAdversarialLearningofMarkovkernelswithinvolutivemaps
transitionkernelproducessamplesfromthetargetdensity 3.ParametrizingKernelswithInvolutions
much more effectively and efficiently than other existing
MCMCalgorithmsaredefinedbyatransitionkernelt(x′|x)
methods.
thatmapsprobabilitydensityfunctions1p (x)tootherprob-
t
ability density functions: p (x′) = (cid:82) t(x′|x)p (x)dx.
t+1 X t
2.RelatedWork Theprobabilitydensityp(x)isstationaryfortheMarkov
kernelt(x′|x)if
Wehighlightworksthatareclosetooursinthattheymake
(cid:90)
use of neural networks to define MCMC methods. For
t(x′|x)p(x)dx=p(x′). (1)
generalMCMCtechniques,thereaderisreferredtocom-
X
prehensive surveys such as Roberts & Rosenthal (2004);
Reversiblekernels,namelykernelsforwhichthedetailed
Brooksetal.(2011);Luengoetal.(2020). Toaddressthe
balanceconditionholds:
challenge in the design of an objective for MCMC meth-
ods,previousworks(Titsias&Dellaportas,2019;Hirtetal.,
t(x′|x)p(x)=t(x|x′)p(x′),∀x,x′ ∈X ×X, (2)
2021;Roberts&Rosenthal,2009)proposeaspecifictypeof
deterministicproposalthattargetsaparticularacceptancera-
have p as stationary probability distribution. Metropolis-
tiowhilepromotingmixingwithentropy-likeregularization.
Hastings kernels is a popular class of such kernels based
However,thisapproachimposesrestrictionsonthetypeof
on a proposal mechanism and a density ratio acceptance
proposalusedandintroducesahyperparameter,thatiseither
function. Throughouttherestofthepaperweconsiderthe
thetargetacceptancerateortheweightoftheregularization.
following transition kernel that satisfies detailed balance
Pasarica & Gelman (2010) develop an adaptive MCMC
withrespecttoagivenprobabilitydistributionp:
methodthatselectstheparametrickernelthatmaximizes
Definition3.1. (Neklyudovetal.,2020)
theexpectedsquaredjump. Anothersolutionproposedby
Givenadistributionp(x),x ∈ X andadeterministicmap
Levy et al. (2018) involves optimizing the difference be-
M : X → X, such that M ◦M = id , the involutive
tweentheaverageEuclideandistanceafteronestepandits X
Metropolis-Hastingskernelis
inverse. AninterestingmethodthatmixesVariationalInfer-
encewithHamiltonianMonteCarloistheworkofHoffman (cid:18) (cid:19)
p(Mx)
etal.(2019)whereautoregressiveflowsareusedtocorrect t(x′|x):=δ(x′−Mx)min 1, JM +
p(x) x
unfavorablegeometryofaposteriordistribution. Samsonov
(cid:18) (cid:18) (cid:19)(cid:19)
et al. (2022) propose a method that uses both local and +δ(x′−x) 1−min 1,p(Mx) JM ,
globalsamplersinwhichlocalstepsareenclosedbetween p(x) x
global updates from an independent proposal. Similarly, (3)
(Gabrie´etal.,2022)alsoproposeanadaptiveMCMCwhich whereJ xM istheabsolutevalueofthedeterminantofthe
augments sampling with non-local transition kernels pa- JacobianofM atpointx.
rameterizedwithnormalizingflows. Anotablemethodfor
Asspecifiedinthedefinition, forthistransitionkernelto
learningtosample,thatisalsotheclosesttoourapproach,
satisfy the fixed point equation (1) the deterministic map
istheworkofSongetal.(2017)whereamethodfortraining
M must be involutive. Given that the kernel is determin-
Markovkernelsparameterizedusingneuralnetworkswith
istic, this condition would obviously restrict our chain to
an autoencoder architecture is proposed. It consists of a
transitionbetweenxandx′ = Mx. Inordertocoverthe
GAN-likeobjectivejustifiedbytheassumptionthatGANs
whole support of p(x) we introduce auxiliary variables
attempttominimizetheJensen-Shannondivergence. De-
v ∈ V. Then, instead of sampling from p(x) we sample
spitetheempiricalsuccess,theoreticalguaranteesonGANs
fromp(x,v) = p(x)p(v|x), withp(v|x)beinganyproba-
arehardtoderiveandmostresultsassumeoptimalityofthe
bilitydistributionwecanefficientlysamplefrom.
discriminator which in practice never holds (Goodfellow
etal.,2020;Arjovskyetal.,2017). Furthermore,thereis Itcanbeshown(Neklyudovetal.,2020)thatthetransition
empiricalevidencethatGANsdonotactuallysamplefrom kerneloftheMarkovchainfromsuchalgorithmsatisfies
atargetdistribution(Arora&Zhang,2017). Similartoour detailedbalance(2)withrespecttop(x,v)andfixed-point
approach,theirmethodinvolvesabootstrapprocesswhere equation(1)withrespecttop(x).
thequalityoftheMarkovchainkernelincreasesovertime.
TheinvolutiveMCMCframeworkformulatesMCMCalgo-
Onekeydifferenceisthat,toachievereversibility,Songetal.
rithmsintermsoftwodegreesoffreedom: theinvolution
(2017)makeuseofanadditionalrandomvariable,whereas
M : X ×V −→ X ×V,andtheconditionaldistribution
ourparameterizeddeterministicproposalsarereversibleby
construction. 1Throughoutthepaperweusethewordsdensityanddistribu-
tioninterchangeably,sinceweassumeeverydistributionisabs.
continuousw.r.t.theLebesguemeasure.
2AdversarialLearningofMarkovkernelswithinvolutivemaps
p(v|x). Sincewewanttotrainasamplerthatsamplesopti- IfRisareversingsymmetryofafunctionwecallthefunc-
mally,wecandefineafamilyofparameterizedinvolutions tion R-reversible. Time-reversing symmetry is a typical
M forθ ∈ Θ,andwewouldliketooptimizeforθtoob- propertyofautonomousHamiltoniansystems. Itisparticu-
θ
tainaMarkovchainthatefficientlyandeffectivelysamples larlyimportantwhenconstructingMarkovchainsamplers
fromatargetdensity. Indeterminingwhatisefficientand out of Hamiltonian systems in Hybrid Monte Carlo algo-
effectivesampling,definingwhatisanaturalobjectiveis rithms (HMC), where the function L is the deterministic
harderthanitsounds. Ononehand,theacceptanceratioof flowofaHamiltoniansystem,definedusingthetargetden-
newproposalsquantifiesefficiency,ontheotherhand,the sity (Duane et al., 1987; Neal, 2011). In HMC the deter-
mixingofthechainthatquantifieseffectivenessisjustas ministicproposalisobtainedbynegatingthemomentaafter
importantyethardtodefine. Findingagoodcompromise applying the deterministic flow. From Eq. 4, it follows
betweenthetwosub-objectivesisdifficult. Toaddressthis, thattheresultingmapR◦Lisindeedaninvolution,since
wedesignanadversarialgamebetweenthemappingM (R◦L)◦(R◦L)=(L−1◦R−1)◦(R◦L)=id ,which
θ X×V
andadiscriminator. ensuresthatdetailedbalanceissatisfiedwithrespecttothe
targetdensity.
4.ParameterizingInvolutions
Decomposing and parameterizing reversibile maps.
Before delving into the details of training an involutive Valpergaetal.(2022)provideamethodfordefiningpara-
transitionkernel,inthissection,wediscusstheparameteri- metricfunctionsthat,byconstruction,arereversiblewith
zationofthedeterministicinvolutionM . Aspointedoutin respecttoanylinearreversingsymmetry. Inparticular,the
θ
Songetal.(2017)itisdifficulttoparameterizedeterminis- followingtheoremholds
ticinvolutions: iftheproposalisdeterministic,thenforall Theorem4.1. (Valpergaetal.,2022)LetL : RD → RD
(x,v) ∈ X ×V we shouldhave M (M (x,v)) = (x,v),
θ θ beanR-reversiblediffeomorphism2,withRbeingalinear
which is not straightforward to impose non trivially as a
involution. Then,thereexistsauniquediffeomorphismg :
constraintintheparameterization. Asasolutiontheyintro- RD →RD,suchthatL=R◦g−1◦R◦g.IfLissymplectic,
duce a third auxiliary random variable u sampled from a
thengcanbechosensymplectic.
uniformdistributionU(0,1)andeitherusetheforwardor
theinverseofM θ,dependingonthevalueofu,toobtain ThistheoremensuresthatanyR-reversible,oringeneral
theproposal. Recognizingthattime-reversibilityofdeter- R-reversibleandsymplecticmapL,canbedecomposedas
ministicdynamicalsystemsanddetailedbalanceofMarkov L = R◦g−1 ◦R◦g. This shifts the problem from that
chains are related, we propose as an alternative to use a ofapproximatingLtothatofapproximatingtheuniqueg
classofneuralnetworksknownastime-reversibleneural ofitsdecomposition. Atthispoint,gcanbeapproximated
networks. usinganyuniversalapproximatorwiththeonlyconstraint
thatweneedtheanalyticformoftheinverseg−1. Suitable
4.1.Time-reversibleneuralnetworks candidatesare,forexample,compositionsofrealNVPbi-
jective layers (Dinh et al., 2017), or, as done in Valperga
Inthecontextofphysics-informedlearningandforecasting
etal.(2022),compositionsofHe´nonmaps(seeAppendix
ofHamiltoniansystems,Valpergaetal.(2022)introduced
A).
time-reversible neural networks. These architectures are
goodcandidatesforparameterizinginvolutions.Tomotivate
InvolutiveMCMCkernelsbyconstruction. Usingthe
this,wefirstreviewhow(time)reversingsymmetriesrelate
aboveresulttogetherwithdefinition3.1fortheinvolutive
to Markov Chains and detailed balance, especially in the
Metropolis-Hastingskernels,wecanconstructtheparamet-
context of flow maps in Hamiltonian Monte Carlo. We
ricfamilyofdeterministicproposalsM :X×V →X×V
thendescribehowwecanfurtherdecomposeuniquelythe θ
time-reversible map and parameterize it with a universal
M =R◦L ,withL =R◦g−1◦R◦g , (6)
approximatorslikeneuralnetworks. θ θ θ θ θ
whereR isisthetime-reversingsymmetryofEq. (5), so
ReversingsymmetriesinHamiltonianMCkernels. We thatM (M (x,v))=(x,v)forall(x,v)∈X ×V.
θ θ
saythataninvertiblesmoothmapR:X×V →X×V isa
reversingsymmetryforaninvertiblefunctionL:X×V →
5.AdversarialTrainingforInvolutiveKernel
X ×V if
R◦L◦R=L−1. (4) Having defined a parameterization for an involutive map
M , we are now interested in how to train M using the
In particular, we call time-reversing symmetry the linear θ θ
map 2Itmustbesmoothlyisotopictotheidentity,amildcondition
R:(x,v)(cid:55)→(x,−v). (5) forsufficientlywell-behavedtargetfunctions.
3AdversarialLearningofMarkovkernelswithinvolutivemaps
p maximizingexpectedsquaredtransitiondistance. However,
balancingbetweenexpectedacceptancerateandexpected
distanceishard.
x
R◦L◦R◦L(x)
TotrainthetransitionkernelM ,ratherthanusingthetarget
θ
distributiondirectlytoacceptorrejectaproposedsample,
L(x)
wedelegatetheacceptanceandrejectionofproposedsam-
q plestoalearnablediscriminatornetworkthatistrainedto
optimallyacceptsamples. Inplaceofthetruedensityratio
R◦L(x)
p(x′)
weintroduceadiscriminatorD(x)asanapproxima-
p(x)
L◦R◦L(x) tion. Wethendefineanadversarialobjectivebetweentwo
terms. Thefirsttermisabouttrainingtheparametricpro-
posal R◦L that proposes novel x′. The second term is
θ
aboutoptimizingtheparametricdiscriminatorDtocorrectly
Figure1.Schematicrepresentationofaninvolutionconstructed tellaparttheproposednovelsamplesfromthosethathave
fromatime-reversiblediffeomorphismL. ForanR-reversible beendrawnfromp(x).
diffeomorphismL,withR : (q,p) (cid:55)→ (q,−p),thecomposition
Theoverallobjectiveisadversarial. Ononehand,wewant
R◦L◦R◦Listheidentity.
tooptimizetheproposalmappingtoreturnnovelproposals
thatfoolthediscriminatortobelievetheyaresamplesfrom
thetruedensity. Ontheotherhand,wewanttooptimizethe
transitionkernelinequation(3). Thecoreideaistotrainthe discriminatortonotbefooledandrecognizenovelproposals.
parametrictransitionkernelasagenerativemodelthatsam- By optimizing this adversarial objective successfully, we
plesfromthetargetdistribution. Therefore,theobjective obtainaMarkovtransitionkernelthateffectivelysamples
thatwederiveinthissectionallowsforunbiasedestimation fromthetargetdistribution.
withsamplesfromthetargetdistribution. Toretrievesam-
FromnowonweidentifyS =X×V withR2nandconsider
plesfromthetargetdistributionwemakeuseofabootstrap
theauxiliarynoisetobeincludedinx∈S.
process.
Definition 5.1. Let R◦L : S → S be a deterministic
θ
5.1.Bootstrap involutive map, and D : S → R + be a positive valued
deterministicfunction.Foragenerictestfunctionr :R →
+
Anunbiasedestimatoroftheobjectivederivedinthissec- [0,1],suchthatx·r(1)=r(x),andr′(x)≥0,wedefine
tioncanbecomputedasaMonteCarlosumoversamples x
theAdversarialMetropolis-Hastingstransitionkernelas:
drawnfromthetargetdistribution. SimilarlytoSongetal.
(2017),togetsamplesfromthetargetdistributionandtrain t (x′|x)=δ(x′(cid:57)RL (x))r[D(x)]+δ(x′(cid:57)x)(1(cid:57)r[D(x)]).
D θ
our transition kernel we propose a bootstrap process that (7)
graduallyincreasesthequalityofsamplesovertime. We
firstobtainsamplesfromp(x)usingapossiblyinefficient An example of a test function is r(x) = min(1,x). We
andslow-mixingkernelthatnonethelesshasp(x)asitssta- use the Barker test: r (x) = (cid:0) 1+ 1(cid:1)−1 during training
B x
tionarydistribution. Wethenusethesesamplestotrainour proposal and common min(1,x) during sampling from
kernelandgetnewsamplesofhigherquality. Byrepeating trainedproposal. Inordertosamplefromthetargetdistribu-
thisprocessthequalityofsamplesincreases,whichinturns tion,weneedtosatisfydetailedbalance,t (x′|x)p(x) =
D
leadstoabettertransitionkernel. Thebootstrapprocessis t (x|x′)p(x′), withrespecttothetargetdistribution. We
D
outlinedinAlgorithm1. proposetoensurethisbyminimisingthedistancebetween
the target distribution p(x) and the distribution obtained
5.2.TheadversarsialMHkernel afteronestepofthechainstartingfromthetarget,thatis
t ◦p(x). AsdoneinNeklyudovetal.(2019),weconsider
Let us assume that we are given samples from the target D
thetotalvariation(TV)distance:
distributionobtainedwiththebootstrapprocessasjustde-
scribed. Given samples and the target density ratio, sim- 1(cid:90)
TV[p,t ◦p(x)]:= |p(x)−t ◦p(x)|dx. (8)
plytrainingM θ tomaximizetheexpectedacceptancerate D 2 S D
wouldleadtopoorexploration. Forexample,M =id
θ X×V
wouldbeatriviallyoptimalsolutionforwhichthesampler For any kernel t D as in Def. 5.1 we have
maximizestheacceptanceratebysimplyremaininginthe TV [p(x′),t D◦p(x)] = 0 if logD(x) =
samelocation. AsdoneinLevyetal.(2018)wecouldalso logp(R p◦ (L xθ )(x))J xR◦Lθ:. Given that R ◦ L θ is an invo-
minimizethelag-oneautocorrelation,whichisequivalentto lution, we can show that the optimal log-discriminator
4AdversarialLearningofMarkovkernelswithinvolutivemaps
functionhasasimplesymmetryundertheactionofR◦L . Any equivariant function d : R2n ⊕ R2n → R can be
θ
We describe next the symmetry and how to include it to decomposedintoanequivariantandaninvariantpart4. We
deriveourfinalobjective. canthenwritetheequivariantpartasthedifferenceofany
function η : R2n → R computed at x and RL (x). The
θ
5.3.EquivarianceofthediscriminatorunderR◦L invariantpartcanbeanyfunctionψ :R2n →Rofthesum
θ
x+RL (x):
Let3R◦L beaninvolutivemapwithRvolumeanddensity- θ
θ
preserving,i.e.,suchthatp(RL θ(x))=p(x). It’simmedi- d ϕ(x)=ψ(x+RL θ(x))[η(RL θ(x))(cid:57)η(x)]. (13)
atetoverifythatthedensityratioλ(x) = p(RLθ(x))JRLθ
p(x) x
hasthefollowingsymmetry: Equivariancefollowsfromsymmetryoffunctionconstruc-
tion:
p(RL ◦RL (x))
λ(RL (x))= θ θ JRLθ =
θ p(I◦RL θ(x)) RLθ(x) d ϕ(RL θ(x))=ψ(RL θ(x)+x)[η(x)(cid:57)η(RL θ(x)]=
= p(x) (cid:0) JRLθ(cid:1)−1 =(λ(x))−1, (9) =−d ϕ(x).
p(RL (x)) x (14)
θ
Thisconstructionsisaspecialcaseofcompositionlifting
logλ(RL (x))=−logλ(x).
θ
layer with specific fixed linear layer. Below we consider
whereweused1 = J RR LL θθ (x)J xRLθ, and(RL θ)−1 = RL θ. generalcase.
Therefore,undertheactionofRL ,wewouldlikethepara-
θ
metricdiscriminatorsDtotransform,byconstruction,simi- C -equivariant composition of linear maps and non-
2
lartothedensityratioλ. linearactivatons. ThemoregeneralconstructionforaC -
2
equivariantdiscriminatoriswithfunctionsh:R2n⊕R2n →
WeenforcetheaboveconstraintusingC −equivariantfunc-
2 R⊕R,whichareequivariantunderρ andρ :
tions. LetusconsiderthecyclicgroupC = ⟨g,g2 = e⟩. 2n 1
2
Notethattheactionofthegenerallynon-linearinvolution
h(ρ x)=ρ h(x). (15)
RL islinearifweconsideritsactionontheliftedspace: 2n 1
θ
x⊕(cid:0) R◦L θ(x)(cid:1) ∼ = R2n⊕R2n. Inthisspace,RL θ isthe Let dˆ be a two-channel neural network that we use to
ϕ
representationρ oftheC group:
2n 2 approximatethelogarithmofthedensityratioatboththe
(cid:20) (cid:21)
0 I pre-imageandimageofRL :
ρ :C →GL(R2n⊕R2n), ρ (g)= 2n . θ
2n 2 2n I 0
2n
(10)
(cid:34) logp(RLθ(x))JRLθ (cid:35)
dˆ (x)≈ p(x) x . (16)
WealsoconsideranotherrepresentationofC 2givenbysign ϕ −logp(RLθ(x))JRLθ
p(x) x
flip:
ξ :C →GL(R), ξ (g)=−1. (11) (cid:20) (cid:21)
1 2 1 A B
Then,letusconsideralinearmap :R2n⊕R2n →
Next we describe parameterizations for the discriminator C D
usingC 2-equivariantfunctions. R2s ⊕R2s, forsomes ≤ n. Toobtainfunctionsthatare
equivariantwithrespecttoρ andρ fromcompositions
2n 2s
5.4.Discriminatorparametrization of such linear maps they must satisfy the following con-
straintforallx:
Toenforcethedesiredtransformationpropertyweparame-
(cid:20) (cid:21)(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21)(cid:20) (cid:21)
terizethelogarithmofthediscriminatorwithaneuralnet- A B 0 I RL(x) 0 I A B RL(x)
2n = 2s .
workd (x) = logD (x). Thegoalistoapproximatethe C D I 0 x I 0 C D x
ϕ ϕ 2n 2s
log-density ratio: d (x) ≈ λ(x), subject to equivariance (17)
ϕ
constraints. Weproposetwopossibleparameterizations: a whichisequivalenttosettingA=D,B =C. Wecanthen
simpleproductparameterization,andamoregeneralcom- composelinearlayersofthisformwithelement-wisenon-
positionoflinearmapsandnon-linearactivations. linearities. Thefunctiond (x),thatweusetoapproximate
ϕ
thelog-densityratiocanthenjustbethefirst,orthesecond,
Discriminatorwithproductparameterization. Aspe- coordinateofthetwo-dimensionaloutputofdˆ .
ϕ
cial construction of C -equivariant discriminator is with
2 Atthispointitisimportanttonoticethatinboththepro-
functionsf : R2n ⊕R2n → Rthatareequivariantunder
posedparameterizations,thediscriminatorDdependsnot
ρ andξ ,namelysuchthat
2n 1 onlyontheparametersϕ,butalsoonθthroughtheproposal
f(ρ x)=ξ f(x). (12) mapRL .
2n 1 θ
3fromnowon, whereneededforcompactness, weomitthe 4Thisistrivialsinceanyscalarequivariantfunctionremains
compositionsign’◦’andsimplyusejuxtaposition. equivariantifmultipliedbyaninvariantfunction.
5AdversarialLearningofMarkovkernelswithinvolutivemaps
5.5.Detailed-BalanceLoss Consideringthefinalbound,wehavethefollowingadver-
sarialoptimizationproblemovertheparametersθofRL
θ
Inordertosimplifyequations,andforreadability,wewill
andtheparametersϕofD :
consider the case of volume-preserving maps RL . For ϕ,RLθ
θ
thegeneralcase,thederivationsarethesame,butwiththe maxA =maxE (r[D (x)]),withfixedϕ
differencethattheJacobiansofRL areabsorbedbythe θ
θ
θ
p(x) ϕ,RLθ
θ
minE (r[D (x)]logr[D (x)]),withfixedθ.
densityratio.
ϕ
p(x) ϕ,RLθ ϕ,RLθ
(23)
Asanticipatedintheprevioussections,weneedtominimise
Asalreadymentioned,duetotheequivarianceconstraints,
TV[p,t ◦p]. Lett bethetransitionkernelofthetype
D D
D(x)isafunctionofbothθ,throughRL ,andϕ. Forany
(5.1) with discriminator D = exp(d). Given the target θ
fixedRL thesolutionoftheminimisationproblemisthe
densityp(x)thetotalvariationdistancebetweenpandt ◦p θ
D
log-densityratio:
is5
D∗(x)=
TV[p;t ◦p]=|p(RL (x))r[D(RL (x))](cid:57)p(x)r[D(x)]|
D θ θ 1 (cid:90)
(18) =argmin p(x)r[D (x)]logr[D (x)]dx
ϕ,RLθ ϕ,RLθ
Weconsideranupperboundonthisquantitywhichissuit-
p(RL )
ableforoptimisationandhasthesameoptimumpoints. =log θ JRLθ(x).
p(x) x
(24)
UpperBoundonTV withPinskerInequality Weuse
For such a discriminator TV2[p;t ◦ p] = 0 since de-
the famous Pinsker Inequality (Pinsker, 1964) to upper- D∗
tailed balance with respect to p is satisfied. However, in
bound the TV distance with a more malleable KL diver-
thebeginningthediscriminatorisnotoptimal. Instead,we
gence. Giventwodensitiespandqwehave
startoptimizingfromarandomlyinitialiseddiscriminator
TV[p;q]2 ≤KL[p;q]. (19) anditerativelyimproveD ϕandR◦L θ inalternatingsteps:
optimisingoverθmakestheproblemharderforthediscrim-
inatorandviceversa(seeSection6.1).
SincePinskerInequalityisdefinedforprobabilitydensities,
wetakeanintermediatestepandnormalizethefunctions: It is worth noting from Eq. (22) that the tightness of our
bounddependsonKL[p(x);p(R◦L (x))].However,ifR◦
θ
TV[p;t D◦p]=A·TV[p∞;p∞ R◦Lθ], (20) L
θ
isclosetobeingdensitypreserving,thistermvanishes.
where
5.6.Optimizationalgorithms
(cid:90)
A= p(x)r[D(x)]dx,A∈[0;1], The objectives (23) are defined as expectations over the
N
p∞
=AS
−1p(x)r[D(x)],
(21) given empirical distribution p(x) = N1 (cid:80) δ(x − x n).
n=1
Therefore,theyallowforunbiasedgradientestimates:
p∞ =A−1p(RL(x))r[D(RL(x))].
RLθ
∇ E (r[D (x)])=E ∇ (r[D (x)])
Theseequationslinkthedetailedbalanceconditiontorejec-
θ p(x) ϕ,R◦Lθ p(x) θ ϕ,R◦Lθ
B
tionsamplingsincep∞isthedensityinducedbyrejection 1 (cid:88)
≈ ∇ (r[D (x )]), {x }B , x ∼p(x),
sampling with proposal distribution p(x) and acceptance B θ ϕ,R◦Lθ b b b=1 b
b=1
functionr[D(x)]. NowwecanapplyPinskerinequality: (25)
andsimilarlyforthegradientwithrespecttoϕ. Withthe
TV2[p∞;p∞ RL]≤A2·KL[p∞;p∞ RL], unbiased gradient estimates, we use standard Adam opti-
A2KL[p∞;p∞ ]= mizerwithdefaulthyperparametersandconstantlearning
RL
(cid:18) (cid:18) (cid:19)(cid:19) rate. Withadversarialoptimizationitisoftenimportantto
p(x)
A·E r[D(x)] log +logr[D(x)] , appropriatelychoosethelearningscheduleforthenumber
p(x) p(RL(x))
ofgradientdescentstepstakenforϕandθ. Wefoundthat
TV2[p;t D◦p]≤ thediscriminatorrequiresmoregradientdescentsteps,e.g.,
(cid:18) (cid:18) p(x) (cid:19)(cid:19) a1:3or1:2ratio.
A·E r[D(x)] log +logr[D(x)] ≤
p(x) p(RL(x))
A·(cid:0) KL[p(x);p(RL(x))]+E (r[D(x)]logr[D(x)])(cid:1) . 6.Experiments
p(x)
(22)
Weprovidecodeforreproducingtheexperimentsathttps:
5Weuse|·| for(cid:82) |·|dx //github.com/ricvalp/aisampler. Following
1 S
6AdversarialLearningofMarkovkernelswithinvolutivemaps
Algorithm1Ai-sampler.
Input: target p(x), initial kernel and discriminator
T , d , initial X = {x }N , x ∼ p (x), DiscSteps,
θ ϕ j j=1 j 0
KernelSteps.
repeat
X = MH(T ,p,X,N) # MH is the Metropolis-
θ
Hastingsalgorithm.
forX∈X do
fori=1toKernelStepsdo
θ →θ+ϵ∇L (X,θ,ϕ)#Eq. (23),firstline
A
endfor Figure2.Synthetic2Ddensitiesusedintheexperiments. From
fori=1toDiscStepsdo lefttoright:mog2,mog6,ring,andring5.Toprow:truedensity.
ϕ → ϕ+ϵ∇L (X,θ,ϕ) # Eq. (23), second Bottomrow:KDEwithsamplesfromourAi-sampler.
adv
line
endfor
endfor
untilconvergence notthecase,fortheBayesianlogisticregressionexperiment
Return: T wespliteachdatasetintotrainandtestsubsetsandcompute
θ
theaveragelogposteriorusingsamplesobtainedwiththe
trainsubset(seeAppendixB).Furthermore, wecompare
Songetal.(2017),wetestourmethodwiththefollowing themeanestimateswiththegroundtruthones.
experiments. Thefirstexperimentiswithfour2Ddensities.
Theyhighlightspecificchallenges,suchasgoodmixingin 6.1.2-dimensionaldensities
the presence of multiple modes separated by low density
For a fair comparison with Song et al. (2017) we choose
regions. Thesecondexperimentiswithhigh-dimensional
tousethesamefour2Ddensitiestheyused. Twomixtures
complexdensitiesfromreal-worldscenarios. Inparticular,
ofGaussianswithtwoandsixmodes,mog2andmog6,a
wesamplefromtheposteriorofalogisticregressionmodel
ring-shaped distribution, ring, and one made of five con-
trainedwiththreedifferentdatasetswithvaryingnumber
centric rings, ring5. The densities are depicted in Fig. 2.
ofcovariates. Forbothexperimentswebenchmarkrunning
Weranasinglechainfor1000burn-instepsandcompute
timeandefficiencyofsamplingofourmethod.
ESSandESS/sforthefollowing1000steps. Despitebeing
2D,thesedensitiesposesomechallenges. Inparticular,the
Baselines. For all the experiments we compare with
mixtures of Gaussians, and the concentric rings are mul-
HamiltonianMonteCarlo(HMC)(Duaneetal.,1987;Neal,
timodal distributions with high-density regions separated
2011)andthemethodbySongetal.(2017),givenitscon-
byhigh-energy(low-density)barriers. Thischaracteristic
ceptualsimilaritieswithours. ForHMCwefollowSong
representsasignificanthurdleforHamiltonianMonteCarlo,
etal.(2017)andfixthenumberofleapfrogintegrationsteps
as Hamiltonian dynamics are unlikely to overcome these
to40andtunethestep-sizetoachievethebestperformance.
high-energybarriers,potentiallyleadingtoinefficientexplo-
Amoredetaileddescriptionoftheexperiments,including
rationofthestatespaceandconvergenceissues. Figure3
theanalyticexpressionofthedensitiescanbefoundinthe
showstheverydifferentbehaviourofourmethodcompared
AppendixAandB.
toHMChighlightingthefastmixingofourmethodcom-
paredinthepresenceofhighenergybarriers. Resultsare
Evaluation criteria. To compare the performances we
reportedinTable1.
usetheeffectivesamplesize(ESS).Practically,theESSis
anestimateofthenumberofsamplesrequiredtoachieve
thesamelevelofprecisionthatasetofuncorrelatedrandom
samples would achieve (for details see Appendix C). We Unbalancedmodes. Asasanadditionalqualitativeexper-
reportthelowestESSamongallcovariatesaveragedover imentswefollow(Samsonovetal.,2022)andinvestigate
several trials. Since MCMC methods can be very costly howourmethodperformsonmultimodaldistributionswith
torun,anotheroftenreportedperformancemeasureisthe unbalanced modes. In particular we use a 3-component
effectivesamplesizepersecond(ESS/s). Thisissimplythe mixtureof2-dimensionalisotropicGaussians,withmodes
ESSobtainedperunitoftime. Wereportthismeasureto centeredattheverticesofanequilateraltriangle. Wefind
evaluateefficiency. Despitebeingacommonperformance thatourmodelcaneffectivelysamplefromunbalancedmix-
metric,achaincanachievegoodESSwithoutgenerating tures and give quantitatively estimate the weights of the
goodsamplesfromtheposterior. Tomakesurethatthisis modes. SeeFig8intheAppendix.
7AdversarialLearningofMarkovkernelswithinvolutivemaps
Figure4.DiscriminatorasafunctionoftwoinputsasinEq.(26),
forthreedifferentvaluesofx:onefarfromthesixmodesandtwo
atthecenterofonemode.
Table2.Averagedpredictiveposteriorandl normofthediffer-
2
encebetweengroundtruthmeanposteriorandestimatesfromthe
chain.
Log-Predictiveposterior Meanmatching
Dataset
HMC Ours HMC Ours
Heart −0.479 −0.458 5.4×10−5 1.2×10−4
German −0.484 −0.484 1.3×10−5 8.2×10−6
Figure3.SingleMCMCtrajectorywiththelearnedkernel(top) Australian −0.375 −0.375 1.2×10−5 5.6×10−5
onthemog2andmog6synthetic2Ddensities,comparedtoHMC
(bottom). ThelowdensityregionsmakeitunlikelyforHMCto
getfromonemodetoanother. datapoints),australian(15covariates,690datapoints),and
german (25 covariates, 1000 data points). For all exper-
iments we ran a single chain for 1000 burn-in steps and
Table1.Effectivesamplesizeforsynthetic2Denergyfunctions.
computeESSandESSpersecond. forthefollowing5000
steps. Table3reportstheresultsandwereportaveragelog
ESS
Density HMC A-NICE-MC Ai-sampler(ours) posterioratTable2.
mog2 0.8 355.4 1000.0
mog6 2.4 320.0 1000.0
Table3.EffectivesamplesizeforBayesianlogisticregression.
ring 981.3 1000.0 378.0
ring5 256.6 155.57 396.5
ESS
Dataset
HMC A-NICE-MC Ai-sampler(ours)
Theroleofthediscriminator. Wecaninvestigatehow Heart 5000.0 1251.2 5000.0
thediscriminatoris”guiding”theparametricproposalby German 5000.0 926.49 5000.0
lookingthevalueofd (x)aftertraining. Inparticular,we Australian 1113.4 1015.8 1746.5
ϕ
artificiallyturndintoafunctionoftwopoints. Forexample,
fortheproductparameterizationwelookat
d (x,y)=ψ(x+y)[η(y)(cid:57)η(x)], (26) 6.3.Benchmarkingrunningtime
ϕ
Theadvantageoflearningtosamplewithatransitionkernel
for a fixed x and different values of y. Figure 4 shows a
parameterizedbyaneuralnetworkalsoliesinthehardware
discriminator,trainedwithmog6forthreedifferentvalues
beingoptimizedtoperformmultipleoperationsinparallel.
ofx. Notethat,forthediscriminatortobeeffectiveitonly
Ourmethodconsistsofadeterministicproposalparameter-
needstobeconsistentwiththegroundtruthdensityratio
izedbyaneuralnetworkand,asopposedtoHMC,doesnot
where the deterministic proposal is likely to propose the
requirethegradientofthetargetdensityfunction. InHMC,
newsamplefromthecurrentstateofthechain.
proposalsareobtainedbyintegratingHamiltoniandynam-
icswhichrequiresmultiplecallstothegradientfunctionof
6.2.Multi-dimensionaldensities
the density. For complex distributions, such as Bayesian
Bayesianlogisticregression TocomparewithSongetal. logisticregressionposteriorswithlargedatasets,callstothe
(2017) we use the same posterior distribution they used, gradientarecostly. WeimplementbothourAi-samplerand
resultingfromaBayesianprobabilisticlogisticregression HMCinJAX(Bradburyetal.,2018)andFlax(Heeketal.,
modelonthreefamousdatasets: heart(14covariates,532 2023),makinguseofthebuilt-inautodifferentiationtools,
8AdversarialLearningofMarkovkernelswithinvolutivemaps
vectorization,andjust-in-time(JIT)compilation. Wenote
that,inacertainrange,runningtimeremainsapproximately
constantasthenumberofparallelchainsincreases.Pastthat
range,runningtimeincreasesapproximatelylinearly. We
measurerunningtimewithintheconstantrange. Forboth
HMC and the Ai-sampler we run Gelman’s Rˆ diagnostic
(Brooks&Gelman,1998)andfindvaluesthatsuggestgood
convergence(≤1.004),andclose-to-zerocross-correlation
betweenchains. WecanthenassumethattheESSofpar-
allelchainsisthesumoftheESSofthesinglechains. For
thisreason,forabettercomparison,wedecidedtoreport
the ESS per second per chain. It is worth noting that, as
reportedinFig. 5,wefoundthattheAi-sampler,giventhe
Figure5.Timevs.numberofparallelchainsforasingleRTX3090
relativelysimplearchitecture,cansustainmanymorepar-
GPU,samplingfromtheBayesianlogisticregressionposterior
allelchainsthanHMC,whichwouldresultinmuchlarger
with German dataset. Every chain consists of 100 steps. For
overallESS.ForfurtherdetailsseetheAppendixD.
moremorethan104parallelchains,thejittingtimebecomespro-
hibitivelylongandthereforenotworthy.
Table4.Effective sample size per second per chain for the 2-
dimensionaldensities.
Bayesianinferencewithreal-wolddatasets. Inthefuture,
ESS/s weplantofurtherexplorethepotentialofourapproachas
Density
HMC Ai-sampler(ours) agenerativemodel,giventhattheadversarialobjectiveis
mog2 0.4 1052.6 computedfromanempiricaldistribution. Inthisregard,we
mog6 0.98 1041.7 wouldliketopointoutthesimilarityofourworkwithNor-
ring 2725.8 402.1 malizingFlows. InthegenerativemodelsettingNFshave
ring5 333.2 434.7
showngoodresultsandourapproachcanbeappliedthere
two. Weexpectpromisingresults,asinsteadoflearninga
globaltransformationfromasimpleprobabilitydistribution
Table5.EffectivesamplesizepersecondperchainforBayesian toamorecomplexone,e.g.,transformingGaussiannoise
logisticregression. intogood-lookingimages,ourapproachconsistsoflearning
local transition kernels that map points (e.g., images) to
ESS/s otherpointsinthetargetdistribution.
Density
HMC Ai-sampler(ours)
Heart 989.0 1736.0 ImpactStatement
German 672.0 1618.0
Australian 171.95 1724.0
Thispaperpresentsworkwhosegoalistoadvancethefield
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
7.Conclusions
specificallyhighlightedhere.
WeproposetheAi-sampler: anMCMCmethodwithinvo-
lutiveMetropolis-Hastingskernelsparameterizedbytime- References
reversibleneuralnetworkstoensuredetailedbalance. We
derive equivariance conditions for the discriminator and Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein
a novel simple objective to train the parameterized ker- generativeadversarialnetworks. InPrecup,D.andTeh,
nel. Theproposedobjectiveisanupper-boundonthetotal Y.W.(eds.),Proceedingsofthe34thInternationalCon-
variation distance between the target distribution and the ferenceonMachineLearning,ICML2017,Sydney,NSW,
stationary distribution of the Markov chain. We use the Australia,6-11August2017,volume70ofProceedings
C -equivarianceoftheoptimaldiscriminatortorestrictthe of Machine Learning Research, pp. 214–223. PMLR,
2
hypothesisis space of the parametric discriminators. We 2017. URL http://proceedings.mlr.press/
learn to sample with a bootstrap process that alternates v70/arjovsky17a.html.
between generating samples from the target density and
improving the quality of the kernel with the adversarial Arora, S. and Zhang, Y. Do gans actually learn the
objective. Wedemonstrategoodmixingpropertiesofthe distribution? an empirical study. arXiv preprint
resultingMarkovchainonsomesyntheticdistributionsand arXiv:1706.08224,2017.
9AdversarialLearningofMarkovkernelswithinvolutivemaps
Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Varia- Hirt, M., Titsias, M. K., and Dellaportas, P. Entropy-
tional inference: A review for statisticians. Journal of basedadaptivehamiltonianmontecarlo. InRanzato,M.,
theAmericanstatisticalAssociation,112(518):859–877, Beygelzimer,A.,Dauphin,Y.N.,Liang,P.,andVaughan,
2017. J.W.(eds.),AdvancesinNeuralInformationProcessing
Systems34: AnnualConferenceonNeuralInformation
Bradbury,J.,Frostig,R.,Hawkins,P.,Johnson,M.J.,Leary, ProcessingSystems2021,NeurIPS2021,December6-14,
C.,Maclaurin,D.,Necula,G.,Paszke,A.,VanderPlas,J., 2021,virtual,pp.28482–28495,2021.
Wanderman-Milne,S.,andZhang,Q. JAX:composable
transformationsofPython+NumPyprograms.2018.URL Hoffman,M.,Sountsov,P.,Dillon,J.V.,Langmore,I.,Tran,
http://github.com/google/jax. D., and Vasudevan, S. Neutra-lizing bad geometry in
hamiltonian monte carlo using neural transport. arXiv
Brooks,S.,Gelman,A.,Jones,G.,andMeng,X.-L. Hand- preprintarXiv:1903.03704,2019.
bookofmarkovchainmontecarlo. CRCpress,2011.
Kingma,D.P.andWelling,M. Auto-encodingvariational
Brooks,S.P.andGelman,A. Generalmethodsformoni- bayes. InBengio,Y.andLeCun,Y.(eds.),2ndInterna-
toringconvergenceofiterativesimulations. Journalof tional Conference on Learning Representations, ICLR
computational and graphical statistics, 7(4):434–455, 2014,Banff,AB,Canada,April14-16,2014,Conference
1998. TrackProceedings,2014. URLhttp://arxiv.org/
abs/1312.6114.
Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density es-
timation using real NVP. In 5th International Confer- Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X.,
enceonLearningRepresentations,ICLR2017,Toulon, Sutskever,I.,andWelling,M. Improvedvariationalinfer-
France,April24-26,2017,ConferenceTrackProceedings.
encewithinverseautoregressiveflow.Advancesinneural
OpenReview.net,2017.URLhttps://openreview. informationprocessingsystems,29,2016.
net/forum?id=HkpbnH9lx.
Levy,D.,Hoffman,M.D.,andSohl-Dickstein,J. Generaliz-
inghamiltonianmontecarlowithneuralnetworks. In6th
Duane,S.,Kennedy,A.D.,Pendleton,B.J.,andRoweth,D.
InternationalConferenceonLearningRepresentations,
Hybridmontecarlo. PhysicslettersB,195(2):216–222,
ICLR2018,Vancouver,BC,Canada,April30-May3,
1987.
2018,ConferenceTrackProceedings.OpenReview.net,
Gabrie´,M.,Rotskoff,G.M.,andVanden-Eijnden,E. Adap- 2018. URLhttps://openreview.net/forum?
tivemontecarloaugmentedwithnormalizingflows. Pro-
id=B1n8LexRZ.
ceedingsoftheNationalAcademyofSciences,119(10):
Luengo,D.,Martino,L.,Bugallo,M.,Elvira,V.,andSa¨rkka¨,
e2109420119,2022.
S. Asurveyofmontecarlomethodsforparameteresti-
mation. EURASIPJournalonAdvancesinSignalPro-
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B.,
cessing,2020(1):1–62,2020.
Warde-Farley,D.,Ozair,S.,Courville,A.C.,andBengio,
Y. Generative adversarial networks. Commun. ACM,
Neal,R.M. MCMCusingHamiltoniandynamics,volume2.
63(11):139–144, 2020. doi: 10.1145/3422622. URL ChapmanandHall/CRC,2011. URLhttps://www.
https://doi.org/10.1145/3422622.
mcmchandbook.net/index.html.
Gorham,J.andMackey,L.W. Measuringsamplequality Neklyudov, K., Egorov, E., and Vetrov, D. P. The im-
withstein’smethod. InCortes,C.,Lawrence,N.D.,Lee, plicitmetropolis-hastingsalgorithm. InWallach,H.M.,
D.D.,Sugiyama,M.,andGarnett,R.(eds.),Advances Larochelle,H.,Beygelzimer,A.,d’Alche´-Buc,F.,Fox,
in Neural Information Processing Systems 28: Annual E.B.,andGarnett,R.(eds.),AdvancesinNeuralInfor-
ConferenceonNeuralInformationProcessingSystems mation Processing Systems 32: Annual Conference on
2015,December7-12,2015,Montreal,Quebec,Canada, NeuralInformationProcessingSystems2019,NeurIPS
pp. 226–234, 2015. URL https://proceedings. 2019,December8-14,2019,Vancouver,BC,Canada,pp.
neurips.cc/paper/2015/hash/ 13932–13942,2019.
698d51a19d8a121ce581499d7b701668-Abstract.
html. Neklyudov, K., Welling, M., Egorov, E., and Vetrov,
D. P. Involutive MCMC: a unifying framework.
Heek,J.,Levskaya,A.,Oliver,A.,Ritter,M.,Rondepierre, In Proceedings of the 37th International Conference
B.,Steiner,A.,andvanZee,M. Flax: Aneuralnetwork on Machine Learning, ICML 2020, 13-18 July 2020,
libraryandecosystemforJAX. 2023. URLhttp:// Virtual Event, volume 119 of Proceedings of Ma-
github.com/google/flax. chineLearningResearch,pp.7273–7282.PMLR,2020.
10AdversarialLearningofMarkovkernelswithinvolutivemaps
URLhttp://proceedings.mlr.press/v119/ Titsias,M.K.andDellaportas,P. Gradient-basedadaptive
neklyudov20a.html. markovchainmontecarlo. InWallach,H.M.,Larochelle,
H.,Beygelzimer,A.,d’Alche´-Buc,F.,Fox,E.B.,andGar-
Pasarica,C.andGelman,A.Adaptivelyscalingthemetropo- nett,R.(eds.),AdvancesinNeuralInformationProcess-
lis algorithm using expected squared jumped distance. ingSystems32: AnnualConferenceonNeuralInforma-
StatisticaSinica,pp.343–364,2010. tionProcessingSystems2019,NeurIPS2019,December
8-14,2019,Vancouver,BC,Canada,pp.15704–15713,
Pinsker, M. S. Information and information stability of
2019.
randomvariablesandprocesses. Holden-Day,1964.
Valperga, R., Webster, K., Turaev, D., Klein, V., and
Rezende, D. J. and Mohamed, S. Variational inference Lamb, J. S. W. Learning reversible symplectic dy-
withnormalizingflows. InBach,F.R.andBlei,D.M. namics. In Firoozi, R., Mehr, N., Yel, E., Antonova,
(eds.), Proceedings of the 32nd International Confer- R., Bohg, J., Schwager, M., and Kochenderfer, M. J.
ence on Machine Learning, ICML 2015, Lille, France, (eds.), Learning for Dynamics and Control Confer-
6-11 July 2015, volume 37 of JMLR Workshop and ence, L4DC 2022, 23-24 June 2022, Stanford Univer-
Conference Proceedings, pp. 1530–1538. JMLR.org, sity, Stanford, CA, USA, volume 168 of Proceedings
2015. URL http://proceedings.mlr.press/ of Machine Learning Research, pp. 906–916. PMLR,
v37/rezende15.html. 2022.URLhttps://proceedings.mlr.press/
v168/valperga22a.html.
Rezende,D.J.,Mohamed,S.,andWierstra,D. Stochastic
backpropagationandapproximateinferenceindeepgen-
erativemodels. InInternationalconferenceonmachine
learning,pp.1278–1286.PMLR,2014.
Robert, C. P., Casella, G., and Casella, G. Monte Carlo
statisticalmethods,volume2. SpringerNewYork,New
York,NY,2004. ISBN978-1-4757-4145-2. doi:10.1007/
978-1-4757-4145-2 1. URLhttps://doi.org/10.
1007/978-1-4757-4145-2_1.
Roberts, G. O. and Rosenthal, J. S. General state space
Markov chains and MCMC algorithms. Probabil-
ity Surveys, 1(none):20 – 71, 2004. doi: 10.1214/
154957804100000024. URLhttps://doi.org/10.
1214/154957804100000024.
Roberts,G.O.andRosenthal,J.S. Examplesofadaptive
mcmc. Journalofcomputationalandgraphicalstatistics,
18(2):349–367,2009.
Samsonov,S.,Lagutin,E.,Gabrie´,M.,Durmus,A.,Nau-
mov,A.,andMoulines,E. Local-globalmcmckernels:
thebestofbothworlds. AdvancesinNeuralInformation
ProcessingSystems,35:5178–5193,2022.
Song,J.,Zhao,S.,andErmon,S. A-NICE-MC:adversarial
training for MCMC. In Guyon, I., von Luxburg, U.,
Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan,
S. V. N., and Garnett, R. (eds.), Advances in Neural
InformationProcessingSystems30: AnnualConference
on Neural Information Processing Systems 2017,
December4-9,2017,LongBeach,CA,USA,pp.5140–
5150, 2017. URL https://proceedings.
neurips.cc/paper/2017/hash/
2417dc8af8570f274e6775d4d60496da-Abstract.
html.
11AdversarialLearningofMarkovkernelswithinvolutivemaps
A.Architectures
AllexperimentsareperformedusingcompositionsofparametricHe´nonmaps. He´nonmapsaresymplectictransformations
onRn×Rn,(x,y)(cid:55)→(x¯,y¯),definedby
(cid:26)
x¯=y+η
(27)
y¯=−x+V(y),
withV :Rn →Rn,andηaconstant. ThereasonwhyweuseHe´nonmaps,otherthantheirapproximationproperties(see
Valpergaetal.(2022)),isbecausetheyareinvertibleanalytically: forgivenV andηtheinverseissimply
(cid:26)
x= −y¯+V(x¯−η)
(28)
y = x¯−η.
For experiments with 2D distributions we use He´non maps with the function V being a two-layer MLP with hidden
dimension32andcompose5ofsuchlayerstoconstructthefunctiong fromEq. (6). TosamplefromtheBayesianlogistic
θ
regressionposteriorwesetthehiddendimensionofthetwo-layerMLPis64.
Forthediscriminatorweusetheproductparameterizationusingtwothree-layerMLPwithhiddendimensions32forthe
experimentswiththe2Ddistribution,and128fortheBayesianposterior.
ThemodelshavebeentrainedononeNVIDIAA100. Trainingtimesarearound2to3minutesforthesimpledistributions
andabout5to10minutesfortheBayesianposterior.
B.Analyticformofthedensities
FollowingSongetal.(2017),forallexperimentsp(v|x)isaGaussiancenteredatzerowithidentitycovariance. Nowwith
f(x|µ,σ)denotingthelogdensityoftheGaussianN(µ,σ2),the2DlogdensitiesU(x)=logp(x)usedintheexperiments
are
mog2:
U(x)=f(x|µ ,σ )+f(x|µ ,σ )−log2
1 1 2 2
whereµ =[5,0],µ =[−5,0],σ =σ =[0.5,0.5].
1 2 1 2
mog6
6
(cid:88)
U(x)= f(x|µ ,σ )−log6
i i
i=1
(cid:20) 5sin(cid:0)iπ(cid:1)(cid:21)
whereµ i = 5cos(cid:0)i3 π(cid:1) andσ i =[0.5,0.5].
3
ring
(cid:32)(cid:112) (cid:33)2
x2+x2−2
U(x)= 1 2
0.32
ring5
U(x)=min(u ,u ,u ,u ,u )
1 2 3 4 5
(cid:16)(cid:112) (cid:17)2
whereu = x2+x2−i /0.04.
i 1 2
FortheBayesianlogisticregression,wedefinethelikelihoodandprioras
n
p(y|X,β)=(cid:89)(cid:2) σ(xTβ)(cid:3)yi(cid:2) 1−σ(xTβ)(cid:3)1−yi
(29)
i i
i=1
whereσ(z)= 1
1+e−z
12AdversarialLearningofMarkovkernelswithinvolutivemaps
ThentheunnormalizeddensityoftheposteriordistributionforadatasetD ={X,y}is
p(β|y,X,µ,Σ)∝p(y|X,β)·p(β|µ,Σ) (30)
wheretheGaussianpriorisp(β|µ,Σ)=N(β|µ,Σ)isaGaussianwithdiagonalcovariance.
Weusethreedatasets: german(25covariates,1000datapoints),heart(14covariates,532datapoints)andaustralian(15
covariates,690datapoints). Whencomputingtheaveragelogposteriorwecompute
1 (cid:88) 1 (cid:88)
log p(y|x,θ),
|D | |C|
test
x,y∈Dtest θ∈C
whereC isachainobtainedwithD .
train
C.Effectivesamplesize
FollowingSongetal.(2017)givenachain{x }N wecomputetheESSas:
i i=1
ESS(cid:0) {x }N (cid:1) = N (31)
i i=1 1+2(cid:80)N−1(cid:0)
1−
s(cid:1)
ρ
s=1 N s
whereρ istheautocorrelationofxatlags. Weusetheempiricalestimateρˆ ofρ :
s s s
N
1 (cid:88)
ρˆ = (x −µˆ)(x −µˆ) (32)
s σˆ2(N −s) n n−s
n=s+1
whereµˆandσˆ aretheempiricalmeanandvarianceobtainedbyanindependentsampler.
FollowingSongetal.(2017),wealsotruncatethesumovertheautocorrelationswhentheautocorrelationgoesbelow0.05
toduetonoiseforlargelagss.
D.Benchmarkingtime
Wereportinfigure6thetimeittakestoperformoneforwardpassoftheparametricproposalintheAi-samplercomparedto
asinglecalltothegradientfunctionoftheBayesianlogisticregressionposteriorobtainedwithJAXautodifferentiation. We
Figure6. Timetakenforasinglecallofthegradientfunctionandneuralnetworkproposalvs.batchsize.
donotcomparerunningtimewithSongetal.(2017)astheirimplementationusesTensorFlow1whichisnotasefficientas
JAX,whichXLAtocompileandruncodeonaccelerators. Westressthattimebenchmarkistohighlightthecostofmultiple
callstothedensitygradientfunctions,especiallyinthecaseofcomplexBayesianposteriordistributions.
13AdversarialLearningofMarkovkernelswithinvolutivemaps
Figure7. Adversarialobjectiveandacceptancerateduringtraining.Samplequalityincreasingduringtraining.
E.Additionalfigures
InFig. 7weshowanexampleoftrainingcurve,withtheacceptancerate,duringtraining.
F.UnbalancedMixtureofGaussians
ForunbalancedmixtureofGaussianswithweights(1/6,1/6,2/3)weprovidetotalvariationdistancebetweengroundtruth
andKDEestimationfromchainsamples. Inordertoassesmodecapturing,weestimatemodeweightsfromchainsamples
andprovideKLdifferencebetweenthemandgroundtruth.
Figure8. UnbalancedmixtureofGaussians.
14