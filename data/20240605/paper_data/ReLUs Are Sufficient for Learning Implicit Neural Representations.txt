ReLUs Are Sufficient for Learning Implicit Neural Representations
Joseph Shenouda∗ Yamin Zhou† Robert D. Nowak∗
June 2024
Abstract
Motivated by the growing theoretical understanding of neural networks that employ the Rectified
Linear Unit (ReLU) as their activation function, we revisit the use of ReLU activation functions for
learningimplicitneuralrepresentations(INRs). InspiredbysecondorderB-splinewavelets,weincorpo-
rate a set of simple constraints to the ReLU neurons in each layer of a deep neural network (DNN) to
remedythespectralbias. ThisinturnenablesitsuseforvariousINRtasks. Empirically,wedemonstrate
that, contrary to popular belief, one can learn state-of-the-art INRs based on a DNN composed of only
ReLU neurons. Next, by leveraging recent theoretical works which characterize the kinds of functions
ReLU neural networks learn, we provide a way to quantify the regularity of the learned function. This
offers a principled approach to selecting the hyperparameters in INR architectures. We substantiate
our claims through experiments in signal representation, super resolution, and computed tomography,
demonstratingtheversatilityandeffectivenessofourmethod. Thecodeforallexperimentscanbefound
at https://github.com/joeshenouda/relu-inrs.
1 Introduction
Recently, training deep neural networks (DNNs) to learn implicit neural representations (INRs) has led to
advancements in various vision-related tasks. These include, but are not limited to, computer graphics
(Mildenhall et al., 2021), image processing (Chen et al., 2021), and signal representation (Sitzmann et al.,
2020). They have also shown great promise in biomedical imaging, where they can be used for sparse-view
computedtomography(CT)(Sunetal.,2021;Wuetal.,2023). ManyINRtasksinvolvelearningcontinuous
representations of images, unlike image classification tasks which train DNNs on high-dimensional data.
INRs learn a continuous representation of an image by training a DNN on the low-dimensional coordinates
of the image. For such imaging tasks, the success of the INR hinges on the ability of the DNN to efficiently
approximate and learn high-frequency components of the image. This has, thus far, prohibited the use of
DNNswithReLUactivationsastheyhavebeenshowntoexhibitaspectral bias (Rahamanetal.,2019)—an
inherentbiasofReLUDNNswhichcausesthemtostruggleinapproximatinghigh-frequencyfunctionswhen
trained via gradient descent.
Therefore, in order to circumvent this problem while still utilizing the power of neural networks, practi-
tioners in the INR community have adopted preprocessing techniques (Mildenhall et al., 2021; Tancik et al.,
2020) and many non-standard activation functions. These include, but are not limited to, the sine func-
tion (Sitzmann et al., 2020), the Gaussian function (Ramasinghe and Lucey, 2022), or the complex Gabor
wavelet (Saragadam et al., 2023). However, using these highly unorthodox activation functions has raised
many questions about the theoretical properties of INRs (Yu¨ce et al., 2022). Therefore, motivated by the
fact that a majority of our theoretical understanding of neural networks is based on standard ReLU DNNs,
we revisit the use of ReLU activations for learning INRs.
∗DepartmentofElectricalandComputerEngineering,UniversityofWisconsin–Madison.
†DepartmentofComputerSciences,UniversityofWisconsin–Madison.
1
4202
nuJ
4
]VI.ssee[
1v92520.6042:viXra(a) (b)
Figure 1: (Left) A plot of the second order B-spline wavelet activation function. The red lines indicate the seven
non-localReLUfunctionsthatmakeupasinglesecondorderB-splinewavelet,showninblack. (Right)ABW-ReLU
neural network with two neurons represented as a constrained ReLU network with 14 neurons. Within each group
the orientation of each ReLU relative to the others is fixed. The input and output weights are learned and shared
across groups of neurons. Shared input/output weights are denoted by the same color.
Consider a shallow ReLU neural network of the form
K
(cid:88)
f (x)= v σ(wTx b ) (1)
θ k k − k
k=1
whereσ(z)=max 0,z . Theneuralnetworkfunctionisalinearcombinationofasetof“atomicfunctions”-in
{ }
thiscasetheReLUneurons σ(wTx b ) K . ThemonotonicallyincreasingnatureoftheReLUactivation
{ k − k }k=1
function implies that each ReLU neuron contributes globally to the function and is highly coherent with
the others. When attempting to fit this network to data, if the weights of one neuron changes, all the
other neurons in the layer must also adjust to correct for the influence of that one neuron. This makes the
function highly sensitive to even small changes in the parameters, and results in a severely ill-conditioned
optimization problem. This ill-conditioning mandates an inordinate number of iterations when training the
networkusinggradient-typeoptimizationmethods,evenwhenusingmodernoptimizerssuchasAdam. This
problem is exacerbated in INR tasks where the data is densely sampled, low dimensional and exhibits high
frequencies.
Guided by this simple insight, our investigation takes a different approach than previous works and
focuses solely on remedying the optimization problem. This is in contrast to wholly replacing the activation
functions of the neurons which generate our features. Specifically, we examine whether a localized ReLU-
based activation function can effectively address and overcome the ill-conditioning. To do this, we propose
incorporating constraints to groups of ReLU neurons within each of the hidden layers in a ReLU DNN.
For each group, all of the neurons share the same weights. Our constraints ensure that every seven ReLU
neurons in the hidden layer is effectively applying the activation function ψ(x):R R where,
→
1 16 8 23
ψ(x)= (σ(x)+σ(x 3)) σ(x 1.5) (σ(x 0.5)+σ(x 2.5))+ (σ(x 1)+σ(x 2)). (2)
6 − − 3 − − 6 − − 6 − −
ThisactivationfunctioncorrespondstoasecondorderB-splinewaveletintroducedinChuiandWang(1992);
Unser et al. (1993). A plot of the second order B-spline wavelet in the univariate case is shown in Figure 1a.
For ease of exposition we will refer to neural networks that use this activation function simply as BW-ReLU
neural networks.
2Thus, when training a BW-ReLU neural network of the form
K
(cid:88)
g (x)= v ψ(wTx b ), (3)
θ k k − k
k=1
wearestilllearningafunctionwhichcanbeexactlyrepresentedbyaReLUneuralnetworkwith7K neurons
as demonstrated in Figure 1b. At an intuitive level, the compact nature of these neurons allows for each
neuron to fit different parts of the function without affecting the contributions from other neurons. In
Section 3 we provide a more thorough discussion on the ill-conditioning and how the unique properties of
the BW-ReLU can remedy it. Our experiments in Section 5 demonstrate the effectiveness of this approach
on various INR tasks.
Next, having developed a method for learning ReLU-based INRs, we leverage the recently developed
theory characterizing the kinds of functions ReLU neural networks learn when fit to data (Savarese et al.,
2019;Ongieetal.,2020;ParhiandNowak,2021;Shenoudaetal.,2023). WeshowhowourBW-ReLUneural
networkfunctionsfitintothismathematicalframeworkandhowwecanmeasurethevariation norm ofthese
functions. This provides a measure of the regularity of the learned function. We also give new insights into
how this regularity is effected when one reparameterizes the INR with a scaling parameter c > 0 such
that the neurons are defined as v ψ(c (wTx b)). This heuristic is employed in all of the activation
· · −
functions introduced for INRs despite being poorly understood (Sitzmann et al., 2020; Ramasinghe and
Lucey, 2022; Saragadam et al., 2023). Moreover, we show how the variation norm of the BW-ReLU neural
network provides a good indication for how well the network can generalize to unseen data. This suggests
a principled way to tune INRs without the need for an additional validation dataset. In summary, our
contributions are:
1. A method for learning ReLU-based INRs: By incorporating a simple set of constraints on the
neurons of a ReLU neural network, we can overcome the ill-conditioning inherent to ReLU networks.
We demonstrate that this approach can be used in multiple INR tasks and performs comparably to
other INR architectures that use unconventional activation functions.
2. Insights on INR generalization: We present a way to measure the regularity of BW-ReLU neural
networksbyleveragingrecenttheoreticalresultsonthekindsoffunctionsReLUneuralnetworkslearn.
This regularity is measured in terms of the variation norm. We discuss how this perspective provides
insights into some of the heuristics employed in learning INRs and how BW-ReLU neural networks
with a lower variation norm tend to generalize better.
2 Related Works
The ReLU is perhaps the simplest activation function utilized in DNNs. Thus much of the recent DNN
theory has focused on this setting (Bach, 2017; Savarese et al., 2019; Arora et al., 2018; Ongie et al., 2020;
Parhi and Nowak, 2021). They are also useful in practice as they induce sparse activation which can be
exploitedforcompressionorspeedingupinference(Lietal.,2023;Kurtzetal.,2020;Mirzadehetal.,2024).
However, neural networks with ReLU activations are typically not utilized for INR tasks due to their
spectral bias (Rahamanetal.,2019). Toremedythis,pre-processingtechniques(Tanciketal.,2020;Milden-
hall et al., 2021) and unconventional activation functions (Sitzmann et al., 2020; Ramasinghe and Lucey,
2022; Saragadam et al., 2023) have been used alongside or instead of traditional ReLU DNNs. Our results
brings into question the necessity of these unorthodox approaches by showing that a ReLU-based DNN can
be trained for various INR tasks.
Our approach is inspired by B-spline wavelets which were first developed and studied in Chui and Wang
(1992); Unser et al. (1993, 1992). Moreover, our BW-ReLU neural networks are very related to the concept
of ridgelets which were originally developed and studied in Candes (1998); Cand`es and Donoho (1999).
3Figure2: ConditionnumberoffeatureembeddingmatrixgeneratedbyReLUvs. BW-ReLUneuralnetworksduring
training. We see that the ReLU produces a severely ill-conditioned feature matrix at initialization and throughout
training. In contrast, the BW-ReLU neural network enjoys a very well conditioned feature matrix all throughout
training. The rate of convergence is also correlated with how well conditioned the feature matrix in both cases.
3 Remedying the Spectral Bias of ReLU Neural Networks
Inthissection,weprovideamorethoroughdiscussiononwhyReLUneuralnetworkstendtoexhibitaspectral
bias when trained to fit low-dimensional datasets. We formalize the intuitions presented in the introduction
and explain how the strong coherence between ReLU neurons results in an ill-conditioned optimization
problem, which significantly slows down convergence. We also discuss key properties of the second-order B-
splinewavelet,whichremedythisill-conditioning(andconsequentlythespectralbias)makingthemsuitable
for INR tasks.
Considerapproximatingaunivariatefunctionu:D RbyaunivariateReLUneuralnetworkf :D R
→ →
of the form
K
(cid:88)
f(x)= v σ(w x b ). (4)
k k k
−
k=1
Where x D, w 1,1 and v ,b ,c R. Here D = [r ,r ] is a bounded domain, K denotes the
k k k 1 2
width of t∈ he network∈ an{ d− σ :R} R denotes∈ the ReLU activation function defined as σ()=max 0, . The
→ · { ·}
restriction of the input weights w to 1,1 follows from the homogeneity of the ReLU (i.e., for any α>0
k
{− }
we have that σ(αz)=ασ(z)). Now due to the fact that σ(z)=z σ( z) we can further restrict the input
− −
weights to be +1 by introducing a skip connection
K
(cid:88)
f(x)=c+ax+ v σ(x b ) (5)
k k
−
k=1
where x D and v ,b ,c,a R. By only considering the approximation on the domain D, setting
k k
∈ ∈
c=u(r ) and b [ 1,1] the network can be further reduced to
1 k
∈ −
K
(cid:88)
f (x)=c+ v σ(x b ) (6)
θ k k
−
k=1
4where θ =(v ,b )K denotes the parameters of the network.
k k k=1
Now suppose we train the neural network to approximate u by sampling the function and training the
networkonaunivariatedataset(x ,y )N tominimizingtheℓ loss. Thiscorrespondstosolvingthefollowing
i i i=1 2
optimization problem
min ΦTv y 2 =minvTΦΦTv 2(Φy)Tv, (7)
θ ∥ − ∥2 θ −
where y RN is a vector containing the labels for all N samples and v RK is a vector containing the
output w∈ eight of each neuron. Moreover, Φ RK×N is the feature embed∈ ding matrix, which depends on
∈
the input biases and is learned throughout training of the network,
(cid:2) (cid:3)
Φ = σ(x b ), ,σ(x b ) . (8)
i 1 i N i
− ··· −
Neural networks typically solve (7) via gradient descent. If we consider the gradient step update on just the
output weights v then it is clear from (7) that this is equivalent to taking a gradient descent step on the
least squares problem over a fixed set of features Φ. The effectiveness of each gradient step to minimize the
objectiveis dependent onthe condition numberof theHessian, ΦΦT(Boyd andVandenberghe, 2004). For a
real symmetric matrix A ∈Rm×m the condition number is defined as κ(A)= λλ m1 where λ 1 and λ m are the
largest and smallest eigenvalues respectively.
In the context of least squares, if the matrix ΦΦT has a very high condition number then the curvature
of the loss landscape (with respect to the output weights v) will vary dramatically in different directions.
This hinders the effectiveness of each gradient step and results in requiring many more gradient steps to
converge.
When using ReLU activations, the feature embedding matrix ΦΦT is typically severely ill-conditioned
which results in a slow converge. This ill-conditioning of the features is present at initialization and persists
throughout training, see for example Figure 2. To understand why, consider two ReLUs with biases that
are close to each other. In this case they are nearly colinear (causing ill-conditioning). On the other hand,
if all the neurons are orthogonal to each other (impossible with ReLUs), then ΦΦT would be perfectly well
conditioned.
To understand this quantitatively, consider approximating the univariate function u : [ 1,1] R with
− →
a ReLU neural network using a fixed set of neurons. Instead of minimizing the ℓ loss over a set of finite
2
samples we will instead consider minimizing the L loss between the neural network f and the function u
2 θ
1(cid:90) (cid:32) (cid:88)K (cid:33)2
min u(x) u( 1) v σ(x b ) dx. (9)
v 2 − − − k − k
D k=1
A simple expansion shows that solving this optimization problem is equivalent to solving
minvTG v rT v. (10)
v σ − u,σ
Where r RK is defined as
u,σ
∈
(cid:90)
(r ) = (u(x) u( 1))σ(x b ). (11)
u,σ i i
− − −
D
Moreover, G RK×K is the Gram matrix for the feature embedding matrix and is defined as,
σ
∈
(cid:90)
G := σ(x b )σ(x b )dx. (12)
i,j i j
− −
D
NotethatthisisanalogoustoΦΦT discussedabove. Therefore,theconditionnumberofG determineshow
σ
ill-conditioned our problem is and indicates how effective each gradient step will be. Theorem 4 in Zhang
et al. (2023) quantifies the condition number of G .
σ
5Theorem 3.1 (Zhang et al. (2023)). Suppose D = [ 1,1] and b K are quasi-evenly spaced on [ 1,1],
− { j }j=1 −
b = 1+ 2(j−1) +o(1). Let λ λ λ 0 be the eigenvalues of the Gram matrix G , then the
j − K K 1 ≥ 2 ≥···≥ K ≥ σ
condition number of G satisfies
σ
κ(G )=λ /λ =Ω(K3).
σ 1 K
The theorem shows us that even when the ReLU neurons are maximally separated, solving (7) for
the output weights is a severely ill-conditioned problem. Moreover, the condition number of the feature
matrix grows at a cubic rate. Thus, as we increase the number of neurons in the network, which increases
the approximation power of the model, we are simultaneously hindering the ability of gradient descent to
optimize the model.
We can potentially remedy this ill-conditioning by instead considering second order B-spline wavelets as
our activation function (2) and approximating u by the following BW-ReLU neural network,
K
(cid:88)
g (x)= v ψ(w x b ). (13)
θ k k k
−
k=1
As discussed earlier this is equivalent to incorporating constraints on a regular ReLU neural network such
that each group of ReLU neurons share weight and have a fixed orientation relative to the other neurons
in the group. We first present a simple proposition establishing that any ReLU neural network can be
represented by a BW-ReLU neural network over a bounded domain.
Proposition 3.2. Let f : [ 1,1] R denote a ReLU neural network with K neurons. The network is of
− →
the form
K
(cid:88)
f(x)=c+ v σ(w x b ) (14)
k k k
−
k=1
where v R, b [ 1,1], and w 1,+1 are the parameters of the model. Then there exists a
k k k
BW-ReLU∈ neural n∈ etw− ork g :R R w∈ ith{ t− he sam} e number of neurons of the form
→
K (cid:18) (cid:19)
(cid:88) 1
g(x)=c+ 24v ψ (w x b ) (15)
k 4 k − k
k=1
such that g represents f on the bounded domain [ 1,1].
−
The proof is in Appendix A. The proposition establishes the fact that on a bounded domain (the setting
relevanttoINRs)anyfunctionwhichwecanrepresentusingaReLUneuralnetworkcanalsoberepresented
byaBW-ReLUneuralnetworkwiththesamenumberofneurons. Therefore,norepresentationpowerislost
by incorporating these constraints into the ReLU network.
Now consider approximating the function u : [ 1,1] R by a univariate BW-ReLU neural network
− →
optimizing over the output weights with a fixed set of neurons. The L loss in this case is,
2
1(cid:90) 1(cid:32) (cid:88)K (cid:33)2
min u(x) v ψ(w x b ) dx. (16)
v 2 − k k − k
0 k=1
This is equivalent to solving
minvTG v rT v (17)
v ψ − u,ψ
where
(cid:90) 1
(G ) = ψ(w x b )ψ(w x b )dx. (18)
ψ i,j i i j j
− −
0
Our next theorem shows that the Gram matrix in this setting is far better conditioned than in the case of
ReLUs.
6Theorem 3.3. Suppose D =[ 1,1] and consider a BW-ReLU neural network with K =2J 1 neurons of
− −
the form,
(cid:18) (cid:19)
3
ψ (x)=2j/2ψ 2j (x+1) k j =0,...,J 1
j,k 2 − −
k =0,...,2j 1,
−
for any J N+. For each scale j = 0, ,J 1 we have k = 0,...,2j 1 shifted versions of the B-spline
∈ ··· − −
wavelets. Let λ λ λ 0 be the eigenvalues of G . Using these neurons the condition number of
1 2 K ψ
≥ ···≥ ≥
G satisfies
ψ
κ(G )=λ /λ = (1). (19)
ψ 1 K
O
TheproofisinAppendixB,wenotethatthenormalizationconstantisnotrequiredandmerelysimplifies
the analysis. The proof relies on key properties of the B-spline wavelets. In particular, the fact that they
are semiorthogonal, this ensures that wavelets of different scale are orthogonal to each other. For instance
in the dyadic wavelet system developed in the theorem ψ ,ψ =0 for any i=j and any k,ℓ Z.
j,k i,ℓ
⟨ ⟩ ̸ ∈
InFigure2wepresentanumericalexampleofhowtheconditionnumberofthefeatureembeddingmatrix
changes during training when using a ReLU or BW-ReLU neural network to fit a univariate function. This
illustrates how the Gram matrix of the feature embeddings remain well conditioned both at initialization
and throughout training for the BW-ReLU neural network while the ReLU neural network suffers from a
poorly conditioned feature embedding matrix all throughout training. We also see that the well conditioned
feature embedding matrix correlates with a faster rate of convergence.
Moreover, our experiments in Section 5 also demonstrate that deep BW-ReLU neural networks are not
susceptible to this ill-conditioning and can be utilized for real INR tasks.
TheGaussianRamasingheandLucey(2022)andGaborwaveletsSaragadametal.(2023)havealsobeen
utilized as activation functions for INR tasks due to their localized nature. However, there is little theory
characterizing the kinds of functions such networks learn and the effects their hyperparameters have on the
learned function. In the next section we leverage the fact that we are ultimately learning a ReLU neural
network. This allows us to utilize much of the recent theory characterizing of the function space associated
with ReLU neural networks giving insights into some of the heuristics used when training INRs.
4 Variation Norm and Scale
Having demonstrated how we can learn ReLU-based INRs by imposing constraints on groups of neurons,
we now explain the benefits of obtaining such a representation. Due to the prevalence and simplicity of the
ReLU activation function, much of our theoretical understanding of DNNs has been focused on those with
ReLUactivations. Inparticular,alineofworkBach(2017);Savareseetal.(2019);Ongieetal.(2020);Parhi
and Nowak (2021); Wojtowytsch (2020); Parhi and Nowak (2023a); Siegel and Xu (2023); Bartolucci et al.
(2023);Chen(2023);Shenoudaetal.(2023);Zenoetal.(2023)hasinvestigatedthekinds offunctionswhich
are learned when fitting ReLU neural networks to data. This mathematical framework has provided many
insights into the inner workings and success of neural networks.
Forinstance,itshedslightonwhyneuralnetworksseeminglybreakthecurseofdimensionality(Klusowski
and Barron, 2018) and how they differ from kernel methods (Parhi and Nowak, 2023b). Moreover, this
perspectiveoffersanexplanationfortheroleofvariousheuristicsthatarecommonlyemployedwhentraining
neuralnetworkssuchasweightdecay,bottlenecklinearlayersandskipconnections(ParhiandNowak,2022;
Shenouda et al., 2023). We will soon see how this perspective can also provide insights into one of the
hyperparameters uniquely used when learning INRs.
These efforts have revealed that for neural networks with ReLU activations the common regularization
technique of weight decay corresponds to regularizing a certain variation norm (Kurkov´a and Sanguineti,
2001). ThisnormisrelatedtototalvariationandistheappropriatenormforfunctionsrepresentedbyReLU
neuralnetworks. Thisnormprovidesameasureofsmoothnessforthelearnedfunctionand, remarkably, can
7∥ggµθ∥V==113311..3344 ∥gg θµ∥V==1177..7766 ∥ggµθ∥V==110033..1122
k k k k k k
g (c=0.5) g (c=2.0) g (c=6.0)
µ µ µ
f f f
§ § §
(a) (b) (c)
Figure 3: ThevariationnormofBW-ReLUneuralnetworks,g withvariousscalestrainedonunivariatedata. The
θ
red dots indicate our samples from the ground truth function f∗. We see that making c too low leads to a poor fit
to the data and a very high variation norm. On the other hand making c to large results in a very oscillatory fit to
the data. The interpolator which generalizes best corresponds to the one with the lowest variation norm.
beeasilycomputedintermsoftheweightsofthemodel(Neyshaburetal.,2015;Savareseetal.,2019;Parhi
and Nowak, 2021). In the univariate case, the variation norm is the total variation of the derivative of the
function f defined by the neural network. If f has a continuous derivative, then this is equivalent to the
θ θ
L norm of the second derivative. The variation norm is also well-defined for continuous functions having
1
discontinuous derivatives (in which case the second derivative will be a generalized function having Dirac
impulses). In the multivariate case, the variation norm is essentially the L norm of the Radon transform
1
of the Laplacian (second derivative operator) of the function. This is equivalent to considering the L norm
1
of the second derivative of the function along each direction of the multidimensional domain. For a ReLU
neuron, the directional second derivative is 0 in all but one direction determined by the orientation of the
neuron. And in that direction the second derivative is an impulse with magnitude equal to the slope of the
ReLU.
Thus, the variation norm has a clear connection to the smoothness of the function, as measured by the
size of the second derivatives. We refer the reader to the recent survey Parhi and Nowak (2023a) for more
details.
WenowpresentthisvariationnormandshowhowitcanbecomputedforourBW-ReLUneuralnetworks.
First, consider a function f represented by a ReLU neural network of the form,
θ
K
(cid:88)
f (x)= v σ(wTx b ), (20)
θ k k − k
k=1
where θ = (v ,w ,b )K and σ() : R R is the ReLU activation function applied elementwise. Each
k k k k=1 · →
neuron, η (x)=vσ(wTx b) contributes to the variation norm of the function. Explicitly, the variation
v,w
−
norm for each ReLU neuron is,
η = v w , (21)
v,w V 2 2
∥ ∥ ∥ ∥ ∥ ∥
where denotes the variation norm of a function. Note the biases are not regularized. The norm of the
V
∥·∥
entire function is computed by summing up the contribution from each neuron such that,
K
(cid:88)
f = v w . (22)
θ V k 2 k 2
∥ ∥ ∥ ∥ ∥ ∥
k=1
8Now consider a function represented by a BW-ReLU neural network of the form
K
(cid:88)
g (x)= v ψ(wTx b ). (23)
θ k k − k
k=1
This network can be exactly represented by a ReLU neural network with 7K neurons by the definition of
the B-spline wavelet activation function (2).
For each BW-ReLU neuron γ (x) = vψ(wTx b) its variation norm is found by summing up the
w,v
−
variation norm of each of the scaled ReLUs in the definition of the B-spline wavelet (2), from this we get
that
γ =16 v w . (24)
w,v V 2 2
∥ ∥ ∥ ∥ ∥ ∥
WecanseethatitissimplyamultipleofthetotalvariationofeachReLUneuron. Again,sincethevariation
norm of a neural network function consists of summing up the variation norm of each neuron we have that
for the BW-ReLU neural network its variation norm can be explicitly computed as,
K
(cid:88)
g =16 v w . (25)
θ V 2 2
∥ ∥ ∥ ∥ ∥ ∥
k=1
4.1 Understanding Scaling via Regularization
It is common to introduce an additional hyperparameter to the neurons used in INR applications, which
scales each activation function as follows. Consider an activation function ζ. Each neuron applies this
activationfunctiontowTx b,producingtheoutputζ(wTx b). MostINRnetworksemployanadditional
− −
activation scaling parameter c > 0, so that each neuron output is instead computed by ζ(c(wTx b)).
−
The scaling parameter can have a significant impact for certain activation functions, while being ineffectual
others.
To illustrate, first consider the ReLU activation. In this case, σ(c(wTx b)) = cσ(wTx b), so the
− −
the scaling affects the magnitude of output but has no other effect. This is due to the fact that the ReLU
activationishomogeneous(linearactivationsandleakyReLUsarealsohomogeneousinthisway). However,
the activations commonly used in INR applications are inhomogeneous. Take for example, the commonly
used sine activation function sin(wTx b) (Sitzmann et al., 2020). In this case if we scale by c, then
−
sin(c(wTx b)) has the same output range/magnitude but will have faster or slower oscillations relative to
−
sin(wTx b), if c>1 or c<1.
−
Including the scaling parameter with inhomogeneous activations functions, like the sine function, can
change the shape and variations of the activation function. Activation functions like the Gaussian (Ramas-
inghe and Lucey, 2022) the complex Gabor wavelets (Saragadam et al., 2023) and the B-spline wavelet used
here, are compressed or dilated depending on whether c > 1 or c < 1, which also makes the support of the
activation function smaller or larger, accordingly. These observations illustrate why scaling the activation
function may have significant effects on INR performance. Generally speaking, large values of c tend to
increase oscillations or variations in the activation function (and possibly decrease the support of the acti-
vation function). This fact has been used to explain why large values of c may help capture more detailed
structure in INR applications (Yu¨ce et al., 2022).
The story, however, is a bit subtle. Consider a standard training problem of the form
N K
min(cid:88) ℓ(cid:16) y ,(cid:88) v ζ(cid:0) c(wTx b )(cid:1)(cid:17) , (26)
θ i k k i − k
i=1 k=1
where the minimization is with respect to all the weights and biases. If we place no restrictions on the
allowable ranges of the input weights and biases, then the scaling factor c can simply be absorbed into the
weights and biases. So why does c play a crucial role? One answer is that the neural networks are trained
9via gradient descent methods, typically initialized with small random weights. This, coupled with the fact
that the training objective is nonconvex, tends to favor solutions that fit the data with weights of small
magnitudes (even though the data might also be fit using much larger weights) (Vardi and Shamir, 2021).
This is sometimes referred to as the implicit regularization of gradient descent.
To understand why the scaling factor c can play a significant role, it is enlightening to consider explicit
regularization in the form of the commonly used weight decay regularization term, which is proportional
to the sum of squared weights in the network. This is supported by established theoretical connections
between weight decay and implicit regularization (Chizat and Bach, 2020). The overall training objective is
to minimize the sum of losses plus the weight decay regularization term
N K K
(cid:88) ℓ(cid:16) y ,(cid:88) v ζ(cid:0) c(wTx b )(cid:1)(cid:17) +λ(cid:88) v 2+ w 2, (27)
i k k i − k ∥ k ∥2 ∥ k ∥2
i=1 k=1 k=1
where ℓ is a loss function and λ > 0 is the weight decay parameter. We can simply reparameterize the
problem by absorbing c into the weights to obtain an equivalent optimization
(cid:88)N ℓ(cid:16)
y
,(cid:88)K
v ζ(wTx b
)(cid:17) +λ(cid:88)K
v 2+
∥w
k
∥2
2. (28)
i k k i − k ∥ k ∥2 c2
i=1 k=1 k=1
Both objectives have the same global minima. The second objective clearly reveals the effect of the scaling
factor. Ifc>1,thenthereislessregularizationappliedtotheinputweightscomparedtotheoutputweights,
and vice-versa if c<1. So if c>1, then the regularizer encourages solutions with larger input weights and
hence increased oscillations or variations in inhomogeneous activation functions, like the sine, Gabor, or
B-spline wavelet activations. In the case of localized activations like the wavelets, larger values of c also
reducethesupport(spatialscale)oftheactivationfunctions. However,unlikeotheractivationfunctions, for
B-spline wavelets the scaling parameter’s effect on the regularity of the function can be readily understood
intermsofthevariationnorm. LetusreparameterizeourBW-ReLUneuralnetworkwithafixedscalec>0
such that,
K
(cid:88)
g (x)= v ψ(c (wTx b )). (29)
θ k · k − k
k=1
It follows from the previous discussion that the variation norm of the function represented by this neural
network can be computed as,
K
(cid:88)
g =16c w v . (30)
θ V k 2 k 2
∥ ∥ ∥ ∥ ∥ ∥
k=1
Thusaveryhighcresultinfunctionsthatismoreirregularwhileusinglowervaluesofccanleadtosmoother
functions.
In Figure 3 we present a simple univariate data fitting problem to illustrate the role of the scaling
parameter c and how it can effect regularity of the learned function. We see that the interpolator which
generalizes best is the one that minimizes the variation norm. Using a c value which is too large results
in a very oscillatory function with a high variation norm. However, if we instead make c too small then
the output weight must increase considerably to compensate for the wider B-spline wavelets. Moreover, in
Section 5.4 we illustrate how the variation norm can be a good indicator for how well our learned function
will perform when using INRs to solve inverse problems.
5 Experiments
Here we demonstrate how our BW-ReLU neural networks can be as effective as other INR architectures
for three INR tasks. We compared our method against SIREN (Sitzmann et al., 2020), WIRE (Saragadam
1031.5 dB (±0.311) 31.16dB(±0.196)
(a) Original (b) BW-ReLU (c) SIREN
30.47dB(±0.153) 27.5dB(±0.059)
(d) WIRE (e) ReLU+P.E. (f) ReLU
Figure 4: ExperimentsoncomputedtomographyreconstructionwithvariousINRarchitectures. Wereportaverage
PSNR and standard error across five random trials.
et al., 2023) and ReLUs + Positional Encoding (P.E.) introduced in Mildenhall et al. (2021). The positional
encoding is a pre-processing technique which maps the low dimensional coordinates to higher dimensional
Fourier features. The hyperparameters for each of the INR architectures were tuned to to give the best
results. For all of our experiments we utilized a three hidden layer DNN and trained with the Adam
optimizer. The full training details for all experiments can be found in Appendix D. Moreover, all the code
for reproducing the experiments can be found at https://github.com/joeshenouda/relu-inrs.
5.1 Computed Tomography(CT) Reconstruction
In this experiment we simulated CT reconstruction by taking 100 equally spaced CT measurements of
a 326 435 chest X-ray image (Clark et al., 2013). Figure 4 shows the results compared to other INR
×
architectures showing that our BW-ReLU neural networks perform just as well and perhaps slightly better
than conventional INR architectures.
5.2 Signal Representation
In this experiment we fit the four INR architectures to the standard 256 256 cameraman image. The
×
results are shown in Figure 5 where we have also included the case of using a traditional ReLU DNN for
comparison. The results show that our method can perform comparably to the newly introduced INR
architectures achieving a high PSNR as fast as the other methods.
5.3 Super resolution
Weimplementeda4 superresolutiononanimagefromtheDIV2Kimagedataset(AgustssonandTimofte,
×
2017). In this case our BW-ReLU neural networks performs slightly better than the rest. All networks were
11107.9dB(±1.077) 105.9dB(±0.836)
(a) Original (b) BW-ReLU (c) SIREN
87.2dB(±6.102) 23.82dB(±0.774)
(d) WIRE (e) ReLU+P.E. (f) ReLU
Figure 5: Experiments on signal representation for various INR architectures. We report average PSNR and
standard error across five random trials.
12trained for 2000 epochs. The results on all four INR architectures are shown in Figure 6.
27.1 dB (±0.013) 26.3dB(±0.212)
(a) Original (b) BW-ReLU (c) SIREN
27.05dB(±0.048) 26.2dB(±0.083)
(d) WIRE (e) ReLU+P.E. (f) ReLU
Figure 6: Experiments on the super resolution task for various INR architectures. We report average PSNR and
standard error across five random trials.
5.4 Low Norm Solutions and Inverse Problems
For our last experiment we trained the BW-ReLU neural network for the CT reconstruction task with three
different scaling parameters. In Figure 7 all three BW-ReLU neural networks that were used to reconstruct
the image achieved the same training loss on the CT measurement data. However we see that the model
with the smallest variation norm across all 3 layers corresponds to the best reconstruction. This suggests a
principled methodology for choosing the scaling parameter c in the case of inverse problems.
6 Conclusion
In this work we presented a simple way to utilize ReLU DNNs for INR tasks. Unlike previous works we
focusing solely on remedying the ill-conditioning of the optimization problem without sacrificing the ReLU
by drawing a connection to B-spline wavelets. We then related our methodology to the function space
associated with ReLU neural networks and showed how this framework can be useful in understanding and
quantifyingtheregularityofourINRs. ThisconnectionsuggestsamoreprincipledapproachtotuningINRs.
ForfutureworkitwouldinterestingtoapplythistechniquetomoreINRtasks. Inparticularneuralradiance
fields and physics informed neural networks.
Acknowledgement
The authors would like to thank Liu Yang and Rahul Parhi for helpful conversations. The authors would
also like to thank Ahmet Alacaoglu for his insights in the later stages of this project. Joseph Shenouda was
supported by the ONR MURI grant N00014-20-1-2787 and NSF grant DMS-2023239. Robert Nowak was
13(cid:80) ℓ∥g θℓ∥V =8650 (cid:80) ℓ∥g θℓ∥V =1978
(a) PSNR:29.9dB (b) PSNR:28.7dB
c=1 c=2
(cid:80) ℓ∥g θℓ∥V =1292 (cid:80) ℓ∥g θℓ∥V =1571
(c) PSNR:32.1 dB (d) PSNR:29.1dB
c=3 c=5
Figure 7: FourBW-ReLUDNNstrainedontheCTreconstructiontaskwithdifferentvaluesofc. Allnetworksare
trained to the same training loss. We see that the c which produces the highest PSNR corresponds to the one with
the lowest variation norm across all layers.
supported in part by the NSF grants DMS-2134140 and DMS-2023239, the ONR MURI grant N00014-20-1-
2787, and the AFOSR/AFRL grant FA9550-18-1-0166.
Broader Impact
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential
societal consequences of our work, none which we feel must be specifically highlighted here.
References
E. Agustsson and R. Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In
Proceedings of the IEEE conference on computer vision and pattern recognition workshops,pages126–135,
2017.
R. Arora, A. Basu, P. Mianjy, and A. Mukherjee. Understanding deep neural networks with rectified linear
14units. In International Conference on Learning Representations, 2018. URL https://openreview.net/
forum?id=B1J_rgWRW.
F.Bach.Breakingthecurseofdimensionalitywithconvexneuralnetworks.TheJournalofMachineLearning
Research, 18(1):629–681, 2017.
F. Bartolucci, E. De Vito, L. Rosasco, and S. Vigogna. Understanding neural networks with reproducing
kernel banach spaces. Applied and Computational Harmonic Analysis, 62:194–236, 2023.
S. P. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.
E. J. Candes. Ridgelets: theory and applications. Stanford University, 1998.
E. J. Cand`es and D. L. Donoho. Ridgelets: A key to higher-dimensional intermittency? Philosophical
Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences,
357(1760):2495–2509, 1999.
Y.Chen,S.Liu,andX.Wang. Learningcontinuousimagerepresentationwithlocalimplicitimagefunction.
InProceedings of the IEEE/CVF conference on computer vision and pattern recognition,pages8628–8638,
2021.
Z. Chen. Multi-layer neural networks as trainable ladders of hilbert spaces. In International Conference on
Machine Learning. PMLR, 2023.
L. Chizat and F. Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the
logistic loss. In Conference on Learning Theory, pages 1305–1338. PMLR, 2020.
C. K. Chui and J.-z. Wang. On compactly supported spline wavelets and a duality principle. Transactions
of the American Mathematical Society, 330(2):903–915, 1992.
K.Clark,B.Vendt,K.Smith,J.Freymann,J.Kirby,P.Koppel,S.Moore,S.Phillips,D.Maffitt,M.Pringle,
et al. The cancer imaging archive (tcia): maintaining and operating a public information repository.
Journal of digital imaging, 26:1045–1057, 2013.
R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge university press, 2012.
J. M. Klusowski and A. R. Barron. Approximation by combinations of relu and squared relu ridge functions
with ℓ1 and ℓ0 controls. IEEE Transactions on Information Theory, 64(12):7649–7656, 2018.
V. Kurkov´a and M. Sanguineti. Bounds on rates of variable-basis and neural-network approximation. IEEE
Transactions on Information Theory, 47(6):2659–2665, 2001.
M. Kurtz, J. Kopinsky, R. Gelashvili, A. Matveev, J. Carr, M. Goin, W. Leiserson, S. Moore, N. Shavit,
and D. Alistarh. Inducing and exploiting activation sparsity for fast inference on deep neural networks.
In International Conference on Machine Learning, pages 5533–5543. PMLR, 2020.
Z.Li,C.You,S.Bhojanapalli,D.Li,A.S.Rawat,S.Reddi,K.Ye,F.Chern,F.Yu,R.Guo,etal. Thelazy
neuron phenomenon: On emergence of activation sparsity in transformers. In Conference on Parsimony
and Learning (Recent Spotlight Track), 2023.
B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing
scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99–106, 2021.
I.Mirzadeh,K.Alizadeh,S.Mehta,C.C.DelMundo,O.Tuzel,G.Samei,M.Rastegari,andM.Farajtabar.
ReLU strikes back: Exploiting activation sparsity in large language models. In The Twelfth International
Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=osoWxY8q2E.
15B. Neyshabur, R. R. Salakhutdinov, and N. Srebro. Path-sgd: Path-normalized optimization in deep neural
networks. Advances in Neural Information Processing Systems, 28, 2015.
G. Ongie, R. Willett, D. Soudry, and N. Srebro. A function space view of bounded norm infinite width
relu nets: The multivariate case. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=H1lNPxHKDH.
R. Parhi and R. D. Nowak. Banach space representer theorems for neural networks and ridge splines. The
Journal of Machine Learning Research, 22(1):1960–1999, 2021.
R.ParhiandR.D.Nowak. Whatkindsoffunctionsdodeepneuralnetworkslearn? insightsfromvariational
spline theory. SIAM Journal on Mathematics of Data Science, 4(2):464–489, 2022.
R. Parhi and R. D. Nowak. Deep learning meets sparse regularization: A signal processing perspective.
IEEE Signal Processing Magazine, 40(6):63–74, 2023a. doi: 10.1109/MSP.2023.3286988.
R. Parhi and R. D. Nowak. Near-minimax optimal estimation with shallow relu neural networks. IEEE
Transactions on Information Theory, 69(2):1125–1140, 2023b.
N. Rahaman, A. Baratin, D. Arpit, F. Draxler, M. Lin, F. Hamprecht, Y. Bengio, and A. Courville. On
the spectral bias of neural networks. In International Conference on Machine Learning, pages 5301–5310.
PMLR, 2019.
S.RamasingheandS.Lucey.Beyondperiodicity: Towardsaunifyingframeworkforactivationsincoordinate-
mlps. In European Conference on Computer Vision, pages 142–158. Springer, 2022.
V.Saragadam,D.LeJeune,J.Tan,G.Balakrishnan,A.Veeraraghavan,andR.G.Baraniuk. Wire: Wavelet
implicit neural representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 18507–18516, 2023.
P. Savarese, I. Evron, D. Soudry, and N. Srebro. How do infinite width bounded norm networks look in
function space? In Conference on Learning Theory, pages 2667–2690. PMLR, 2019.
J.Shenouda,R.Parhi,K.Lee,andR.D.Nowak. Variationspacesformulti-outputneuralnetworks: Insights
on multi-task learning and network compression. arXiv preprint arXiv:2305.16534, 2023.
J. W. Siegel and J. Xu. Characterization of the variation spaces corresponding to shallow neural networks.
Constructive Approximation, pages 1–24, 2023.
V. Sitzmann, J. Martel, A. Bergman, D. Lindell, and G. Wetzstein. Implicit neural representations with
periodic activation functions. Advances in Neural Information Processing Systems, 33:7462–7473, 2020.
Y. Sun, J. Liu, M. Xie, B. Wohlberg, and U. S. Kamilov. Coil: Coordinate-based internal learning for
tomographic imaging. IEEE Transactions on Computational Imaging, 7:1400–1412, 2021. doi: 10.1109/
TCI.2021.3125564.
M. Tancik, P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi,
J. Barron, and R. Ng. Fourier features let networks learn high frequency functions in low dimensional
domains. Advances in Neural Information Processing Systems, 33:7537–7547, 2020.
M.Unser,A.Aldroubi,andM.Eden. Ontheasymptoticconvergenceofb-splinewaveletstogaborfunctions.
IEEE Transactions on Information Theory, 38(2):864–872, 1992.
M. Unser, A. Aldroubi, and M. Eden. A family of polynomial spline wavelet transforms. Signal processing,
30(2):141–162, 1993.
G. Vardi and O. Shamir. Implicit regularization in relu networks with the square loss. In Conference on
Learning Theory, pages 4224–4258. PMLR, 2021.
16S. Wojtowytsch. On the banach spaces associated with multi-layer relu networks: Function representation,
approximation theory and gradient descent dynamics. CSIAM Transactions on Applied Mathematics, 1:
387–440, 06 2020. doi: 10.4208/csiam-am.20-211.
Q.Wu,R.Feng,H.Wei,J.Yu,andY.Zhang. Self-supervisedcoordinateprojectionnetworkforsparse-view
computed tomography. IEEE Transactions on Computational Imaging, 2023.
G. Yu¨ce, G. Ortiz-Jim´enez, B. Besbinar, and P. Frossard. A structured dictionary perspective on implicit
neural representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 19228–19238, 2022.
C.Zeno,G.Ongie,Y.Blumenfeld,N.Weinberger,andD.Soudry. Howdominimum-normshallowdenoisers
look in function space? In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
URL https://openreview.net/forum?id=gdzxWGGxWE.
S.Zhang,H.Zhao,Y.Zhong,andH.Zhou. Whyshallownetworksstrugglewithapproximatingandlearning
high frequency: A numerical study. arXiv preprint arXiv:2306.17301, 2023.
17A Proof of Proposition 3.2
Proof. Let ψ(x) denote the second order B-spline wavelet. We can express this as a linear combination of
seven ReLU functions,
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
1 8 1 23 16 3 23 8 5 1
ψ(x)= σ(x) σ x + σ(x 1) σ x + σ(x 2) σ x + σ(x 3).
6 − 6 − 2 6 − − 3 − 2 6 − − 6 − 2 6 −
(31)
With this representation of the B-spline wavelet, it is clear that one ReLU on the interval [ 1,1] can be
−
represented in terms of a B-spline wavelet as
(cid:18) (cid:19)
1
σ(x)1 =24ψ x 1 , (32)
[−1,1] 4 [−1,1]
where 1 denotes an indicator function on [ 1,1]. Therefore if we define g :[ 1,1] R as
[−1,1]
− − →
K (cid:18) (cid:19)
(cid:88) 1
g(x)=c+ 24v ψ (w x b ) (33)
k 4 k − k
k=1
then clearly
f(x)=g(x) x [ 1,1]. (34)
∀ ∈ −
B Proof of Theorem 3.3
Our proof relies on Gershgorin’s circle theorem which we recall here.
Theorem B.1 (Gershgorin circle theorem Horn and Johnson (2012)). Let A Rn×n with entries a . For
ij
∈
i=1, ,n let R be the sum of the absolute values of off diagonals in each row of A
i
···
(cid:88)
R (A)= a .
i ij
| |
i̸=j
Consider the n Gershgorin discs defined as
z C: z a R (A) .
ii i
{ ∈ | − |≤ }
Then every eigenvalue of A lies within at least one of the Gershgorin discs.
We now proceed to the proof of Theorem 3.3.
Proof of Theorem 3.3. Our proof follows by bounding the sum of the absolute values of the off-diagonals of
each row of G and then employing Gershgorin’s circle theorem. Recall that, for each scale j =0,...,J 1
ψ
−
we have k =0,1,...,2j 1 shifted versions of the neurons at this scale. For ease of notation we define the
−
neurons as
ψ (x):=2j/2 ψ(2j(3/2)(x+1) k). (35)
j,k
· −
Note that the scaling factor 2j/2 ensures that the L norm of all the neurons are equal. The elements in
2
G RK×K consist of inner products between every pair of neurons.
ψ
∈
18The diagonal entries of G are simply the squared L norm of each neuron,
ψ 2
(cid:90)
ψ 2 = (2j/2ψ(2j(3/2)(x+1) k))2dx (36)
∥ j,k ∥2 −
D
(cid:90) 1
= (2j/2ψ(2j(3/2)(x+1) k))2dx. (37)
−
−1
Let t= 3(x+1), then by a change of variables we have
2
2(cid:90) 3 2(cid:18) 1(cid:19) 1
ψ 2 = (2j/2ψ(2jt k))2dt= = . (38)
∥ j,k ∥2 3 − 3 4 6
0
The 1 follows from the fact that ψ 2 = 1 and our normalization ensures that translations and dilation of
4 ∥ ∥2 4
the wavelet preserve its norm.
For the first row of G we have,
ψ
R (G )=0. (39)
1 ψ
This follows from the fact that each column in the first row consists of an inner product between ψ and
0,0
ψ for all resolution of j > 0 and all possible shifts at each resolution. Since the B-spline wavelets are
j,k
semiorthogonal (Chui and Wang, 1992) we must have that whenever i=j and for any k,ℓ Z,
̸ ∈
ψ ,ψ =0. (40)
⟨
j,k i,ℓ ⟩L2(D)
Where , denotes the L inner product of two functions. From this we can conclude that (G ) = 0
⟨·
·⟩L2 2 ψ 0,ℓ
for all ℓ=0, ,K 1.
··· −
Now for the rest of the rows each row is identified with a neuron ψ at a certain scale 2j and shift k.
j,k
The columns in this row are inner products of ψ with all the other neurons in the network. These are
j,k
the off-diagonals that we will bound. For each neuron ψ its inner product with all other neurons can be
j,k
broken up into two cases.
Case 1: The other neurons are at a different scale. Thefirstcaseconsistsofinnerproductsbetween
other neurons which are at a different scale than ψ i.e. ψ ,ψ for j = m but any k,p. Again,
j,k
⟨
j,k m,p ⟩L2
̸
thanks to the semiorthogonality of the B-spline wavelets (Chui and Wang, 1992) we have
ψ ,ψ =0. (41)
⟨
j,k m,p ⟩L2(D)
Case 2: The other neurons are at the same scale. Thesecondtypeofinnerproductsareoftheform,
ψ ,ψ , (42)
⟨
j,k j,p ⟩L2(D)
where k,p = 0,...,2j 1 and k = p. By the compactness of the B-spline wavelets, if k p 3 then the
− ̸ | − | ≥
inner product is zero. Therefore it remains to bound
ψ ,ψ (43)
⟨
j,k j,p ⟩L2(D)
for the case when k p = 1 or k p = 2. A direct computation, again applying a change of variables,
| − | | − |
reveals that for the case of k p =1 we have
| − |
2(cid:90) 3
2j/2ψ(2jt) 2j/2ψ(2jt 1)dt=0.030864=C . (44)
3 · − 1
0
Furthermore, when k ℓ =2 another computation reveals that
| − |
2(cid:90) 3
2j/2ψ(2jt) 2j/2ψ(2jt 2)dt= 0.0030864=C . (45)
3 · − − 2
0
19At each scale j we could have at most two neuron functions which are at the same scale j but shifted by
1andatmosttwoneuronfunctionswhichareatthesamescalebutshiftedby2. Therefore,byGershgorin’s
circle theorem we have that for all the eigenvalues of G they must satisfy,
ψ
(cid:26) (cid:12) (cid:12) (cid:27)
λ i
∈
z
∈R:(cid:12)
(cid:12) (cid:12)z
−
61(cid:12)
(cid:12) (cid:12)≤2( |C 1 |+ |C 2 |) . (46)
where 2(C + C )< 1. Therefore,
| 1 | | 2 | 6
κ(G )= (1). (47)
ψ
O
C Further Experiments on the Variation Norm
In this section we provide two more experiments demonstrating the correspondence between the optimal
c parameter and the network with the smallest variation norm across all layers. Our first experiment
demonstrates that the same observation holds for the superresolution experiment in Section 5. The results
areshowninFigure8. WealsorepeatedtheexperimentinFigure7onanalternativeCTimage. Theresults
are shown in Figure 9.
(cid:80) ℓ∥g θ(ℓ)∥V =2304 (cid:80) ℓ∥g θ(ℓ)∥V =1358 (cid:80) ℓ∥g θ(ℓ)∥V =1468
Original (a) PSNR:26.25dB (b) PSNR:27.01 dB (c) PSNR:25.29dB
FullResolutionImage c=2 c=3 c=5
Figure 8: Three BW-ReLU DNNs trained on the superresolution task with different values of c. The c which
produces the highest PSNR corresponds to the one with the lowest variation norm across all layers.
D Training Details for Experiments
For all experiments we applied an exponential learning rate scheduler of the form
η(t)=η (r)t/T,
0
where η is the initial learning rate, T is the total epochs we trained for and r is the decay rate.
0
D.1 Signal Representation
For the BW-ReLU DNN we used an initial learning rate of η =4e 3 a scaling parameter applied to each
0
−
neuronofc=9. ForSIRENwetrainedwithaninitiallearningrateofη =2e 3andsetthescaleω =50.
0 0
−
For WIRE we trained with a learning rate of η =1e 3 setting σ =10 and ω =20. Finally for ReLU +
0 0 0
−
positional encoding we used an initial learning rate of η =4e 3. All models were trained for 1000 epochs
0
−
and the decay rate for the learning rate was r =0.1. The architecture consisted of 3 hidden layers and 300
neurons per layer.
20(cid:80) ℓ∥g θ(ℓ)∥V =1147 (cid:80) ℓ∥g θ(ℓ)∥V =1191 (cid:80) ℓ∥g θ(ℓ)∥V =1601
Original (a) PSNR:33.9 dB (b) PSNR:32.5dB (c) PSNR:26.1dB
CT c=2 c=3 c=5
Figure 9: Three BW-ReLU DNNs trained on a different CT reconstruction task with different values of c. All
networksaretrainedtothesametrainingloss. AgainthecwhichproducesthehighestPSNRcorrespondstotheone
with the lowest variation norm across all layers.
D.2 Super Resolution
We performed super resolution by training the DNN to minimize the mean squared error between a low
resolutionimageoftheButterflyandadownsampledversionofthefullsampleimageproducedbytheDNN.
For the BW-ReLU DNN we trained with an initial learning rate of η =3e 3 and scaling parameter c=3.
0
−
For SIREN we used an initial learning rate of η = 2e 3 and set ω = 12. For WIRE we used an initial
0 0
−
learning rate of η = 3e 3 with ω = 8 and σ = 6. The ReLU + positional encoding architecture used
0 0 0
−
an initial learning rate of η = 4e 3. All models were trained for 2000 epochs and the decay rate for the
0
−
learningrateschedulerwasr =0.2. Inallcasesthearchitectureconsistedof3hiddenlayersand256neurons
per layer.
D.3 CT Reconstruction
Wecomputed100equallyspacedCTmeasurementsoftheX-rayimageintheRadondomain. TheDNNwas
trained by computing the mean squared error between the Radon transform of the image generated by the
DNN and the ground truth CT measurements. For the BW-ReLU neural network we used a learning rate
of η = 2e 3 with c = 3. For SIREN we trained with a learning rate of η = 1e 3 and set ω = 25. For
0 0 0
− −
WIREalearningrateofη =5e 3wasusedsettingω =7andσ =10. FinallyfortheReLU+positional
0 0 0
−
encoding we trained with a learning rate of η =3e 3. All methods were trained for 10000 epochs and the
0
−
decay rate for the learning rate scheduler was r = 0.1. In all cases the architecture consisted of 3 hidden
layers and 300 neurons per layer.
D.4 Low Norm Solutions and Inverse Problems
The learning rate we used in this experiment was varied for each value of c to ensure that all the networks
achieved equal training loss. We considered the DNN as a composition of three shallow BW-ReLU neural
networksandsummedupthevariationnormofeachlayeraccordingto(30). Inparticularforthearchitecture
defined as,
g(x)=W ψ(c W ψ(c W ψ(c W x))). (48)
3 2 1 0
· · ·
We can treat it as a composition of 3 shallow nets of the form
21g(1)(x)=W ψ(c W x) x R2 (49)
1 0
· ∈
g(2)(x)=W ψ(c Ix) x RK (50)
2
· ∈
g(3)(x)=W ψ(c Ix) x RK (51)
3
· ∈
and then the whole DNN is
g(x)=g g g . (52)
3 2 1
◦ ◦
The sum of the variation norm across all layers would be
(cid:32) K K K (cid:33)
(cid:88) g(ℓ) =16c (cid:88) w(1) w(0) +(cid:88) w(2) +(cid:88) w(3) . (53)
∥ ∥V ∥ k ∥2 ∥ k ∥2 ∥ k ∥2 ∥ k ∥2
ℓ k k=1 k=1
22