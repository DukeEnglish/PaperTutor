Meta-Learners for Partially-Identified Treatment Effects Across Multiple
Environments
JonasSchweisthal*12 DennisFrauen*12 MihaelavanderSchaar3 StefanFeuerriegel12
Abstract etal.,2024),economics(Angrist,1990;Kuzmanovicetal.,
2024),ormarketing(Varian,2016). Forexample,medical
Estimatingtheconditionalaveragetreatmentef-
professionalsareinterestedinleveragingelectronichealth fect (CATE) from observational data is rele-
recordstopersonalizecarebyunderstandingtheestimated
vantformanyapplicationssuchaspersonalized
CATEoftreatments.
medicine. Here, we focus on the widespread
settingwheretheobservationaldatacomefrom In this paper, we are interested in the CATE in a setting
multiple environments, such as different hospi- where we have access to observational data, that are col-
tals,physicians,orcountries. Furthermore,weal- lectedfrommultipleenvironments. Furthermore,weallow
lowforviolationsofstandardcausalassumptions, forviolationsofstandardcausalassumptions,namely,over-
namely,overlapwithintheenvironmentsandun- lap within the environments and unconfoundedness. Our
confoundedness.Tothisend,wemoveawayfrom settingisparticularlyrelevantformedicalapplicationsfor
pointidentificationandfocusonpartialidentifi- tworeasons.
cation. Specifically,weshowthatcurrentassump-
First,CATEestimationinpracticeofteninvolvessettings
tionsfromtheliteratureonmultipleenvironments
wheretheobservationaldatacomefrommultipleenviron-
allow us to interpret the environment as an in-
ments(Shietal.,2021). Commonexamplesinmedicineare
strumentalvariable(IV).Thisallowsustoadapt
settingswherepatientdatacomefrommultiplehospitals,
boundsfromtheIVliteratureforpartialidentifica-
multiplephysicians,ormultiplecountries(e.g.,Huangetal.,
tionofCATEbyleveragingtreatmentassignment
2022). As a result, each environment is characterized by
mechanismsacrossenvironments. Then,wepro-
unique patient demographics (e.g., a specialized hospital
posedifferentmodel-agnosticlearners(so-called
mayhavemoreseverecasesofdisease)and/oruniquetreat-
meta-learners)toestimatetheboundsthatcanbe
mentpolicies(e.g.,thedefaulttreatmentoptionmayvary
usedincombinationwitharbitrarymachinelearn-
acrosscountries).
ingmodels. Wefurtherdemonstratetheeffective-
nessofourmeta-learnersacrossvariousexperi- Second,identificationoftheCATEfromobservationaldata
mentsusingbothsimulatedandreal-worlddata. is challenging and typically requires strong assumptions
Finally,wediscusstheapplicabilityofourmeta- such as (i) overlap (within environments) and (ii) uncon-
learners to partial identification in instrumental foundedness(Wager&Athey,2018). (i)Overlapensures
variablesettings,suchasrandomizedcontrolled thateachindividualhasapositiveprobabilityofreceiving
trialswithnon-compliance. any treatment (D’Amour et al., 2021) within each envi-
ronment. (ii)Unconfoundednessimpliesthatallpotential
confoundersthatinfluenceboththetreatmentassignment
1.Introduction
and the outcome of interest are contained within the ob-
servational data. Under violations of these assumptions,
Estimatingconditionalaveragetreatmenteffects(CATEs)
the CATE is generally unidentifiable and can not be esti-
fromobservationaldataisrelevantformanyapplications.
mated consistently from observational data (Pearl, 2009).
Examplesincludemedicine(Glassetal.,2013;Feuerriegel
Notwithstanding,violationsarelikelytooccurinsettings
*Equal contribution 1LMU Munich, Germany 2Munich withmultipleenvironments. (i)Whenpatientcharacteristics
Center for Machine Learning (MCML), Germany and/ortreatmentpoliciesvaryacrossenvironments,thenit
3University of Cambridge, UK. Correspondence to: Jonas
is likely that certain patient characteristics and/or certain
Schweisthal <jonas.schweisthal@lmu.de>, Dennis Frauen
treatmentsarerarelyorevenneverobservedinsomeenvi-
<frauen@lmu.de>.
ronments(e.g.,certaintreatmentsareonlyadministeredby
Proceedings of the 41st International Conference on Machine specializedphysicians). Assuch,overlapisoftenviolated.
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by (ii) Confounders such as socioeconomic status are often
theauthor(s).
1
4202
nuJ
4
]GL.sc[
1v46420.6042:viXraMeta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
Unobserved Overlap Overlap Unobserved Overlap Overlap
confounding violations violations confounding violations violations
Figure1.Intuitionforourbounds. Left: Twopropensityscoresπe(x) = P(Ae = 1 | Xe = x)andπj(x) = P(Aj = 1 | Xj = x)
1 1
correspondingtodifferentenvironmentseandjareplottedoverobservedconfoundersX.Largevaluesofπe(x)andπj(x)correspond
1 1
toahighprobabilityofreceivingtreatmentandviceversa.Right:CATEτ(x)togetherwithboundsdependingonviolationsofoverlap
andunconfoundedness.InregionA ,nooverlapviolationsoccur,leadingtowideboundsfortheCATEτ duetopotentialunobserved
1
confounding.InregionA ,overlapviolationsoccuratoppositeendsacrossenvironments,leadingtotightboundsforτ.InregionA ,
2 3
overlapviolationsoccuronthesameendacrossenvironments,leadingtowideboundsforτ duetoalackofdatafortreatedindividuals.
notrecordedinmedicalstudies(Adler&Stead,2015). As agnosticandcanbeusedincombinationwithanymachine
such, unconfoundedness is often violated. Consequently, learningmodel. (3)Weprovidetheoreticalresultsforour
standard CATE estimators may be biased and thus unre- meta-learnersbyshowingconsistencyanddoublerobust-
liable, motivating the need for methods to relax standard ness properties. Finally, we confirm the effectiveness of
assumptions. ourmeta-learnersbyperformingvariousexperimentsusing
bothsimulatedandreal-worlddata.
To address the challenges of the setting above, we move
awayfrompointidentificationandfocusonpartialidentifi- Wenowgiveanintuitionbehindourbounds,and,thereby,
cation. Partialidentificationmeansthatonerelaxesassump- we explain that access to a discrete variable representing
tionsontheunderlyingdata-generatingprocesstoestimate theenvironmentcanindeedhelpwithpartialidentification
boundsforacausalqueryofinterest(Jessonetal.,2021). of the CATE, even under violations of overlap and unob-
Knowingthattheboundsareaboveorbelowzeroisoften served confounding. Fig. 1 shows two different environ-
sufficientforconsequentialdecision-making(Kallusetal., ments,namely,hospitaleandhospitalj.Bothhavedifferent
2019). Forexample,knowledgeaboutapositivetreatment treatmentassignmentmechanisms(i.e.,πeandπj,respec-
1 1
effectmaybesufficientforaphysiciantoprescribeatreat- tively),whichareplottedagainstsomepatientcharacteristic
ment. Tothebestofourknowledge,ourworkisuniquein X (e.g.,patientage). Letushighlightthreeregions: InA
1
twoways. First,whilethereisrichliteratureonpartialiden- andA ,thetreatmentassignmentsaresimilar,becauseof
3
tification(e.g.,Manski,1990;Kilbertusetal.,2020;Padh which,unfortunately,wehavewideboundsfortheCATE.
et al., 2023), no previous works allow for observational However,inA ,theoverlapassumptionisviolatedinboth
2
datasetsfrommultipleenvironments. Second,thederivation environments,butatoppositeends: inhospitale,middle-
of bounds for specific settings is a common theme in the agedpatientsarealmostalwaystreated,while,inhospitalj,
partial identification literature (e.g., Duarte et al., 2023), theyarerarelytreated. Hence,wecanovercomethelimited
yettheeffectiveestimationofsuchboundsisoftennotthe overlapbycombiningdataacrossbothenvironments. Fur-
focus. Here, anoveltyofoursisthatweprovideflexible thermore,wecandeduceclaimsregardingthe“strength”of
meta-learnersthatcanbecombinedwitharbitrarymachine unobservedconfoundingintheregionA ,astherecannot
2
learningmodelsforestimatingsuchboundsinoursetting. existanunobservedvariablewithastronginfluenceonthe
treatment assignment because this would imply more ob-
Inthispaper,wemakethreecontributions:1 (1)Weshow
servedvariationintheprescribedtreatments. Ourintuition
thatundersimilarassumptionsasinrelatedwork(Kallus
isthatviolationsofoverlapandunconfoundednessexclude
etal.,2018),thevariablerepresentingtheenvironmentcan
eachother,sothatweobtaintightboundsfortheCATE.
be interpreted as an instrument variable (IV). Then, we
generalizepreviousresultsfromthepartialidentificationlit-
eratureforbinaryIVs(Balke&Pearl,1997;Swansonetal., 2.Relatedwork
2018) to our setting to obtain bounds for CATE. (2) We
Ourworkdrawsfrommultiplestreamsofrelatedliterature,
proposenovelmeta-learnerstoestimatetheboundsfromob-
whichwelistinthefollowing. Additionalliteratureisin
servationaldata. Importantly,ourmeta-learnersaremodel-
AppendixA.
1Code is available at https://github.com/
Treatment effect estimation across different environ-
JSchweisthal/BoundMetaLearners.
ments: Severalworksfocusontreatmenteffectestimation
2Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
from different environments. (i) One stream focuses on boundedoutcomes. Balke&Pearl(1997)derivedtighter
randomizeddata,typicallytocombinerandomizedandob- boundsforaveragetreatmenteffectsinasimilarsettingthan
servationaldatasets(e.g.,Kallusetal.,2018;Atheyetal., ours, but with binary variables. Duarte et al. (2023) pro-
2020;Ghassamietal.,2022;Hattetal.,2022;Imbensetal., posedageneralprocedureforderivingboundsindiscrete
2022). (ii)Anotherstreamusespurelyobservationaldata structuralcausalmodels. Insettingswithcontinuousvari-
fromdifferentenvironments. Forexample,previousworks ables,recentworksleveragemachinelearningapproaches
offeratheoryforthetransportabilityofcausaleffectsacross tolearnbounds(Kilbertusetal.,2020;Huetal.,2021;Bal-
differentenvironments(Bareinboim&Pearl,2016)ormeth- azadeh et al., 2022; Chen et al., 2023; Padh et al., 2023).
odsfortransferlearning(Bica&vanderSchaar,2022)and Notethatthesemethodsfocusprimarilyonidentificationre-
fordetectingunobservedconfounding(Karlsson&Krijthe, sults(i.e.,derivingbounds),whilewedevelopmeta-learners
2023). However,noneoftheseworksestimatesboundsfor forestimatingbounds. Anexceptionistheestimatorfrom
theCATEacrossmultipleenvironmentsunderviolationsof (Levisetal.,2023),whichhowevertargetsboundsforaver-
assumptions. agetreatmenteffects.
Meta-learners for CATE estimation: Model-agnostic Finally,arelatedbutdistinctstreamofliteratureleverages
meta-learners, in particular two-stage learners, are com- machine learning for causal sensitivity analysis to obtain
monly used in the standard setting for CATE estimation boundsunderso-calledsensitivitymodels,whichmakeas-
withoutunobservedconfoundingandachievestate-of-the- sumptionsthatlimitthestrengthofunobservedconfounding
artperformanceboththeoreticallyandempirically(Foster (Jessonetal.,2021;Dorn&Guo,2022;Dornetal.,2022;
&Syrgkanis,2019;Curth&vanderSchaar,2021a). The Yinetal.,2024;Frauenetal.,2023;Jinetal.,2023;Frauen
basic idea of two-stage learners is to estimate the CATE et al., 2024). Again, we do not make an assumption that
directlyviaanadditionalsecond-stageregressionusinga limitsthestrengthofunobservedconfounding,becauseof
constructed pseudo-outcome. Several learners have been whichthetasks/boundsarenotcomparable. Ontopofthat,
developed,whichprimarilydifferintermsofthepseudo- tothebestofourknowledge,noneoftheseworksconsiders
outcomeused: (i)theregression-adjustmentlearner(called effectiveestimationofboundsforCATEviameta-learners.
RA- or X-learner) (Ku¨nzel et al., 2019; Curth & van der
Researchgap: WefocusonCATEestimationfromobser-
Schaar, 2021a); the inverse-propensity weighted learner
vationaldataacrossmultipleenvironmentsunderviolations
(IPW-learner)(Curth&vanderSchaar,2021a);thedoubly
ofstandardassumptions. Tothebestofourknowledge,we
robustlearner(DR-learner)(Kennedyetal.,2023);andthe
arethefirsttostudypartialidentificationinsettingswith
R-learner(Nie&Wager,2021). Therealsoexistsomeef-
multipleenvironments. Further,wearethefirsttodevelop
fortstogeneralizethestandardmeta-learnerstodifferent
effectivemeta-learnersinourpartialidentificationsetting
causalinferencesettings(e.g.,withinstrumentalvariables,
toestimatebounds.
Syrgkanisetal.,2019;Frauen&Feuerriegel,2023). How-
ever, these meta-learners are all designed for estimating
CATEunderpointidentificationbutnotpartialidentifica- 3.Problemsetup
tion.
Setting: Weconsiderasettingwithmultipleenvironments
Weareonlyawareofoneworkthatproposesameta-learner (e.g.,differenthospitals,differentphysicians,ordifferent
for partial identification of CATE, namely, the B-learner countries), which, forsimplicity, wedenotebyadiscrete
(Oprescu et al., 2023). However, the B-learner has been environment variable E ∈ {0,...,k}. We further have
proposed for causal sensitivity analysis, a setting with a accesstoanobservationaldatasetD = {e ,x ,a ,y }n
i i i i i=1
fundamentallydifferentsetofassumptions: theB-learner of size n. The data are sampled i.i.d. from a population
requirespriorknowledgethatlimitsthemaximumstrength (E,X,A,Y) ∼ P, withpatientcovariatesX ∈ X ⊆ Rp,
oftheconfounding. Incontrast,wedonot makeassump- discrete treatments A ∈ A ⊆ N, and bounded outcomes
tionsthatlimitthestrengthoftheconfoundingbutinstead Y ∈ Y ⊆ [s ,s ] ⊆ R. Thecausalstructureisshownin
1 2
assumethatobservationaldataiscollectedfrommultipleen- Fig.2. Inparticular,weassumethatE isaninstrumental
vironments. Becauseofthat,theB-learnerisnotapplicable variablethathasaneffectonthetreatmentAbutnodirect
inourwork. effectontheoutcomeY thatisnotmediatedviaA.
Partial identification of treatment effects: In the con- Oursettingisrelevantforavarietyofpracticalapplications
textofcausalinferencefromasingleobservationaldataset where observational data are collected from different en-
withunobservedconfounding,severalworkshaveproposed vironments. Consider electronic healthcare records from
proceduresforpartialidentificationoftreatmenteffects. patients,whereX includespatientcharacteristicssuchas
ageandgender,Aisanindicatorofwhetherapatienthas
Manski (1990) was the first to obtain bounds for (condi-
beenprescribedmedicaltreatment,andY isahealthout-
tional)averagetreatmenteffectsundertheassumptionof
3Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
comesuchasheartrateorbloodpressure. Further, letE Inparticular, theoutcome-generatingcausalmechanisms
correspondtoanenvironmentinwhichtheelectronichealth coincideinexpectationforeachenvironment. Similaras-
recordsarecollectedsuchasthehospital. Suchsettingsare sumptionsarecommonlymadeintheliteraturetoensure
commoninmedicalresearchtounderstandtheeffectiveness the transferability of treatment effect estimates. Finally,
oftreatments(e.g.,Huangetal.,2022). Assumption3.3ensuresthatthepopulationsofallenviron-
mentsaresupportedonacommondomain, i.e.,thereare
notypesofpatientsthatonlyexistincertainenvironments.
Thus,weareconsistentwithpriorliteratureoncausalinfer-
enceusingcombineddatasets(Kallusetal.,2018;Shietal.,
2021;Hattetal.,2022).
Targetestimand: UsingAssumption3.2,wecandefinethe
oracleresponsesurfaceasµ (x)=E[Ye(a)|X =x,E =
(cid:101)a
e], which is independent of the environment e. We are
Figure2.Causalgraphsfordifferentenvironmentseandj. We interestedinthe(environment-agnostic)conditionalaverage
assume that the causal structure between e and j remains un- treatmenteffect(CATE)
changedbutweallowfordifferenttreatmentassignmentmecha-
nisms(propensityscores)π ae(x)andπ aj(x). Thedottedarrows τ a1,a2(x)=µ (cid:101)a1(x)−µ (cid:101)a2(x), (1)
indicatepotentialunobservedconfounding.
which quantifies the difference in expected potential out-
comes for two treatments a ,a ∈ A. The CATE is of
Notation: Wedefinetheenvironment-specificpropensity 1 2
interestinvariousapplications,e.g.,personalizedmedicine
scoreasπe(x) = P(A = a | X = x,E = e), whichde-
a (Feuerriegeletal.,2024),becauseitcapturestreatmentef-
notesthetreatmentassignmentmechanismthatisobserved
fectheterogeneitybyconditioningonthecovariatesx.
inenvironmente. Furthermore,wedefinetheenvironment-
specificresponsesurfacesviaµe(x)=E[Y |X =x,A= Violationsofstandardcausalinferenceassumptions:The
a
a,E =e]. Importantly,weassumethatboththepropensity standardcausalinferenceliteratureimposesthefollowing
scores and the response surfaces may differ across envi- twoadditionalassumptionstoidentifytheCATEτ (x)
a1,a2
ronments, i.e., πe(x) ̸= πj(x) and µe(x) ̸= µj(x) for fromtheobservationaldatadistribution(Wager&Athey,
a a a a
environments e ̸= j. For example, the standard of care 2018;Curth&vanderSchaar,2021a):
mightvaryacrosscountries,leadingtodifferenttreatment (i) Overlapwithinenvionments: πe(x)>0;and
a
policiesandthuspropensityscores. Finally,wedefinethe (ii) Unconfoundedness: Ye(a)⊥⊥A|X =x
environmentprobabilityasδ (x)=P(E =e|X =x).
e forallx∈X anda∈A.
Assumptions: Weusethepotentialoutcomesframeworkto
Overlapimpliesthateachindividualwithcovariatesxmust
formalizeourcausalinferenceproblem(Rubin,1974). Let
have a non-zero probability of receiving every treatment
Ye(a) ∈ Y denote the potential outcome in environment
a. Unconfoundednessimpliesthattheobservedcovariates
E =eforatreatmentinterventionA=a. Inthispaper,we
X capture all confounding between the treatment A and
imposethefollowingassumptionsonthedata-generating
outcome Y. Under overlap and unconfoundedness, the
processacrossenvironments.
oracleresponsefunctionsareidentifiedviaµ (x)=µe(x)
(cid:101)a a
Assumption3.1(Consistencyacrossenvironments). For and,hence,coincidewiththeirobservedcounterpartsforall
eachenvironmente,observingA=aimpliesYe(a)=Y.
environmentse(Shalitetal.,2017). Inparticular,theCATE
Assumption3.2(Environment-agnosticoracleresponsesur- isidentifiedviaτ (x)=µe (x)−µe (x).
faces). We assume that E[Ye(a) | X = x,E = e] =
a1,a2 a1 a2
However,bothoverlapandunconfoundednessareviolated
E[Yj(a) | X = x,E = j] holds for all environments
in many practical settings. (i) The overlap violation is
0≤e,j ≤k,treatmentsa∈A,andcovariatesx∈X.
almostalwaysviolatedinsettingswithhigh-dimensional
Assumption 3.3 (Common support). We assume that
covariates X, which is common in modern settings with
δ (x) > 0 holds for all environments 0 ≤ e ≤ k and
e available text/image data, or other electronic healthcare
covariatesx∈X.
records (D’Amour et al., 2021; Schweisthal et al., 2023).
(ii)Theunconfoundednessassumptionishighlyunrealis-
Assumption 3.1 (consistency) excludes spillover effects
tic in medical settings, where confounders such as socio-
acrossindividualsandisastandardassumptioninthecausal
economic status are rarely recorded in electronic health
inference literature (Shalit et al., 2017; Wager & Athey,
2018). Assumption3.2impliesthatatreatmentresultsin Y(a) ⊥⊥ E | X (Pearl, 2009). Note that this assumption is
thesameexpectedpotentialoutcomeacrossenvironments.2 stronger than our Assumption 3.2. Hence, our results can be
applied to any setting where E is a discrete instrument in the
2Prior literature often defines instrumental variables E via classicalsense,e.g.,RCTswithnon-compliance.
4Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
records(Adler&Stead,2015). Ifeitheroverlaporuncon- ExtendingtheManskibounds: Theabovemotivatesusto
foundedness(orboth)areviolated,neitherresponsefunc- leveragethedifferentpropensityscoresacrossenvironments
tions µ (x) ̸= µe(x) nor the CATE are identified. Then, to obtain tighter bounds (see Fig. 1). By combining the
(cid:101)a a
unbiasedestimationisimpossible(Pearl,2009). boundsfromLemma4.1fortheoracleresponsefunctions
acrossenvironments,wederivethefollowingresult.
Inthefollowing,werelaxassumptionsrelatedtooverlap
andunconfoundednessandfocusonpartialidentification. Theorem4.2. UnderAssumptions3.1and3.2,theCATEis
boundedvia
4.PartialidentificationofCATE b−(x)≤τ (x)≤b+(x), (5)
a1,a2
WenowleveragetheideathattheenvironmentvariableE where
actsasaninstrumentalvariable(IV)andgeneralizeresults
b+(x)=minb+ (x) and b−(x)=maxb− (x) (6)
fromtheliteratureonpartialidentificationforIVs(Balke& e,j e,j
e,j e,j
Pearl,1997;Swansonetal.,2018)thatholdforbinaryvari-
with
ablesandaveragetreatmenteffects. Todoso,wefirstrecall
aknownresultfromtheliteratureonboundingtreatment b+ (x)=πe (x)µe (x)+(1−πe (x))s
effectswithoutmultipleenvironments. e,j a1 a1 a1 2
−πj (x)µj (x)−(1−πj (x))s , (7)
Lemma 4.1. (Manski, 1990) For any environment e, the a2 a2 a2 1
oracleresponsesurfacesareboundedunderAssumption3.1
via b− (x)=πe (x)µe (x)+(1−πe (x))s
e,j a1 a1 a1 1
µ (cid:101)a(x)≤π ae(x)µe a(x)+(1−π ae(x))s 2 (2) −πj (x)µj (x)−(1−πj (x))s . (8)
a2 a2 a2 2
and
µ (x)≥πe(x)µe(x)+(1−πe(x))s , (3) Proof. SeeAppendixB.
(cid:101)a a a a 1
where[s ,s ]denotesthesupportofY. TheboundsfromTheorem4.2areageneralizationofthe
1 2
so-callednaturalboundsfromtheliteratureonpartialiden-
tificationwithIVsanddiscretevariables(Swansonetal.,
Proof. SeeAppendixB.
2018). UnlikeotherboundssuchastheonesfromBalke
&Pearl(1997),thesecanbestraightforwardlyextendedto
TheintuitionbehindLemma4.1isasfollows:Wheneverwe
continuousoutcomesandthusourmultipleenvironments
haveoverlapviolationsandthepropensityscoreπe(x)is
a settingviaTheorem4.2. WerefertoSwansonetal.(2018)
largeforatreatmenta,mostoftherandomnessinthetreat-
foradetailedoverviewonboundsininstrumentalsettings
mentisremovedbyconditioningontheobservedcovariates
(withdiscretevariables). Finally,anotheradvantageofthe
x. Hence, there can not be unobserved confounders that
naturalboundsisthattheyareinclosed-formandthusallow
haveastronginfluenceonthetreatmentassignment. This
forestimationviameta-learners(seeSec.5).
isreflectedintheboundsfromLemma4.1,whichbecome
tighterwheneverπe(x)becomeslarger. Tightnessofthebounds: NotethatourboundsfromTheo-
a
rem4.2satisfy
Previously, Manski (1990) proposed to obtain an upper
boundfortheCATEτ a1,a2(x)=µ (cid:101)a1(x)−µ (cid:101)a2(x)bycom- b+(x)−b−(x)
biningtheupperandlowerboundfromLemma4.1forthe
≤min(cid:8) (s −s )(2−πe (x)−πj (x))(cid:9) , (9)
differenttreatmentsa 1anda 2. Thisresultsin
e,j
2 1 a1 a2
τ (x)≤πe (x)µe (x)+(1−πe (x))s whichshowsthattheyimproveontheManskiboundsfrom
a1,a2 a1 a1 a1 2 Eq.(4)intermsoftightnesswheneverπe (x)+πj (x)>1.
−πe (x)µe (x)−(1−πe (x))s (4) a1 a2
a2 a2 a2 1
Remark: Ourboundsbecometightwheneverthereexist
fortheupperbound. Ananalogousresultcanbeobtained twoenvironmentseandj sothatbothπe (x)andπj are
a1 a2
forthelowerboundbyswappingthesupportpointss and large. Inotherwords,wehavearathersurprisingimplica-
1
s . tion: violationsofoverlapcanbebeneficialtoincreasethe
2
tightnessofourboundsaslongastherespectivetreatment
OnedrawbackoftheManskiboundinEq.(4)isthatitisnot
assignmentsacrossenvironmentschangesufficiently.
particularlytight. Whensubtractingthelowerboundfrom
theupperbound,weobtainaconstanttightnessofs −s , Intuitively,thedegreetowhichthepropensityscoresacross
2 1
whichdoesnotdependonthecovariatesxandnotonthe environmentsdiffercanalsobeviewedasthestrengthofde-
propensityscoreπe(x). Inparticular,thetightnessequals pendencebetweentheenvironmentvariableE andthetreat-
a
thefullsupportoftheoutcomevariableY. mentA. IntheliteratureonIVregression,itiswell-known
5Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
that so-called weak instruments E that are only weakly tocancellationeffects),suchanapproachshouldperform
correlatedwithAmayleadtobiasedestimates(Angrist& betterthantheplug-inlearner. Wedothisbyconstructing
Pischke,2008). OurboundsfromTheorem4.2automati- pseudo-outcomes that are equal to the bound we want to
callyaccountforthisissueandbecomewiderwheneverE estimateinexpectation,whichthencanbeusedasasecond-
isaweakinstrumentandthepropensityscoresπe (x)and stage regression objective. As a result, we obtain novel
πj acrossenvironmentsbecomesimilar.
a1
two-stage learners for estimating b+ (x) and thus b+(x)
a2 e,j
(insteadofthenon-identifiableCATEτ (x)).
1,0
5.Meta-learnersforestimatingthebounds Typesoflearners: Wedistinguishbetweentwodifferent
cases: (1)within-environmentboundsb+ (x),whichcom-
Wenowdevelopmeta-learnersforestimatingourbounds e,e
binetheresponsefunctionboundsfromLemma4.1within
fromTheorem4.2,whichdenotequantitiesinpopulation.
thesameenvironmente,and(2)cross-environmentbounds
In the following, we thus propose model-agnostic meta-
b+ (x),whichcombinetheresponsefunctionboundsacross
learnersthatcanbeusedincombinationwithanymachine- e,j
different environments e ̸= j. Consequently, we yield
learningmethod. Inthissection,wedescribetheestimation
two different approaches for the second-stage learners,
of our bounds for binary treatments A ∈ {0,1}, so that
whichwecall(1)WB-learnerand(2)CB-learner(where
a =1anda =0. Wealsoprovideatheoreticalanalysis
1 2 the latter comes in different variants called CB-PI, CB-
(Sec. 5.3) and an implementation using neural networks
RA-, CB-IPW-, and CB-DR-learner). Once all second-
(Sec.5.4).
stage learners ˆb+ (x) are fitted, the within-environment
e,j
Forsimplicity,wewillfocusonobtainingestimatorsˆb+(x) boundsandthecross-environmentboundsarecombinedvia
fortheupperboundb+(x). Estimatorsforthelowerbound ˆb+(x)=min ˆb+ (x)toobtainthefinalboundestimator
e,j e,j
b−(x)canbeobtainedthesamewaybyinterchangings forCATE.ThefullprocedureisshowninAlgorithm1.
1
ands (seeTheorem4.2).
2
5.2.1.WB-LEARNER
5.1.Na¨ıveplug-inlearner
Whenever we consider a single environment e = j, we
Astraightforwardwaytoobtainanestimatorforb+(x)is definethepseudo-outcome
theso-calledna¨ıveplug-inapproach. Inthis,wefirstobtain
Bˆ+WB =1{E =e}(AY +(1−A)s −(1−A)Y −As ).
estimatorsµˆe(x)andπˆe(x)ofthenuisancefunctionsµe(x) e 2 1
a a a (11)
andπe(x)foralla,e∈{0,1}. Notethatestimatingµe(x)
a a Then, we use the pseudo-outcome Bˆ+WB to estimate
isaregressiontaskandestimatingπe(x)isaclassification e
a ˆb+ (x) = Eˆ[Bˆ+ | X = x,E = e] via a second-stage
task,whichmeansthatoff-the-shelfmachine-learningalgo- e,e e
rithmscanbeapplied. Then,wedirectlyplugtheestimated learnerEˆ conditionalonE = e. Thatis,weonlyusethe
nuisancefunctionsintoEq.(6),whichyields datafromenvironmentefortheestimatorˆb+ e,e(x). Wecall
thisthewithin-environmentbound-learner(WB-learner).
(cid:110)
ˆb+(x)=min πˆe(x)µˆe(x)+πˆe(x)s
1 1 0 2
e,j 5.2.2.CB-LEARNERS
(cid:111)
−πˆ 0j(x)µˆj 0(x)−πˆ 1j(x)s 1 . (10) Whenevere ̸= j,weneedtocombinedatafromdifferent
environments e and j to estimate the cross-environment
WecalltheestimatorfromEq.(10)thena¨ıveplugin-learner. boundsb+ (x). Forthispurpose,weshowthattheestima-
e,j
The general approach behind plug-in learners can suffer tionofb+ (x)canbecastintoastandardCATEestimation
e,j
fromaso-calledplug-inbias(Kennedy,2022)thatcanlimit problem with a transformed outcome and treatment vari-
estimation performance. As a remedy, we develop novel able Y(cid:101) and using the environment E as a (multi-valued)
two-stagelearnersforourtaskinthefollowing. treatment. Thatis,wedefine
5.2.Two-stagelearners Y(cid:101) e+
,j
=1{E =e}(AY +(1−A)s 2)
+1{E =j}((1−A)Y +As ), (12)
Wenowaimtoaddressthedrawbacksofplug-inlearners 1
byproposingso-calledtwo-stagelearnersthatdirectlyes-
and we denote the transformed response functions as
timate the bounds from Theorem 4.2. While the plug-in r+(x)=E[Y(cid:101)+ |X =x,E =ℓ]forℓ∈{e,j}.
learnerplugstheestimatednuisancefunctionintoEq.(10) ℓ e,j
toestimateb+ (x),wenowaimtoestimateb+ (x)directly High-levelapproach:Weproceedintwostages.Instage1,
viaasecond-e s, tj agelearnerforallenvironmente c,j ombinations weobtainestimatorsrˆ ℓ+andδˆ e(x)ofthenuisancefunctions
e,j ∈ {0,1}. Iftheboundsb+ e,j(x)areeasiertoestimate r ℓ+andδ ℓ(x). Instage2,wetreattheestimationofb+ e,j(x)
directlythanthecorrespondingnuisancefunctions(e.g.,due asastandardCATEestimationtaskusingrˆ+asestimated
ℓ
6Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
response functions and δˆ ℓ(x) as the estimated propensity Algorithm1:Two-stagelearnersforestimatingbounds
score. Then,weadopttheclassicalmeta-learners(plugin-,
Input :observationaldata(E,X,A,Y),methodm∈{RA,IPW,DR}
RA-,IPW-,andDR-learner(Curthetal.,2020;Kennedy, Output:estimatedupperboundˆb+(x)(forˆb−(x),interchanges1ands2)
2023)) for multi-valued treatments (Acharki et al., 2023) // Stage 1 (nuisance estimation)
(i.e.,ourenvironments). Forguaranteesonthisapproach,
rˆ ℓ+(x)←Eˆ[Y(cid:101)e+
,j
|X=x,E=ℓ]
werefertoSec.5.3.
δˆ ℓ(x)←Pˆ(E=ℓ|X=x)
// Stage 2
fore,j∈{0,...,k}do
(i)CB-PI-learner: Thecross-environment-boundplugin- ife=jthen
learnerisdefinedvia ˆb+ e,e(x)=Eˆ[Bˆ e+WB|X=x,E=e]
else
ˆb+,PI(x)=rˆ+(x)−rˆ+(x), (13) ˆb+ e,j(x)=Eˆ[Bˆ e+ ,m j |X=x]
e,j e j end
end
whichisanalogoustotheplugin-learnerforstandardCATE
// Final bound
estimation(Curthetal.,2020). ˆb+(x)=mine,jˆb+ e,j(x)
(ii) CB-RA-learner: The cross-environment-bound
regression-adjustment learner is defined via the pseudo
outcome
for e ∈ {0,1}. For the CB-m-learner with m ∈
Bˆ e+ ,R jA=1{E=e}(cid:16) Y(cid:101)e+ ,j−rˆ j+(x)(cid:17) +1{E=j}(cid:16) rˆe+(x)−Y(cid:101)e+ ,j(cid:17) {RA,IPW,DR},weobtain
+1{E̸=e}1{E̸=j}(cid:16) rˆe+(x)−rˆ j+(x)(cid:17) , (14) b+ e,j(x)=E[Bˆ e+ ,jm |X =x] (18)
andthesecond-stagepseudo-outcomeregressionˆb+ e,j(x)= fore,j ∈{0,1}whenever,foralle,a∈{0,1},oneofthe
Eˆ[Bˆ+RA |X =x]. following cases hold: (i) rˆ+(x) = r+(x) for m = RA;
e,j ℓ ℓ
(ii)δˆ(x) = δ (x)form = IPW; or(iii)eitherrˆ+(x) =
(iii) CB-IPW-learner: The cross-environment-bound in- ℓ ℓ ℓ
versepropensityweightedlearnerisdefinedvia r ℓ+(x)orδˆ ℓ(x)=δ ℓ(x)form=DR.
Bˆ e+ ,I jPW=(cid:32)1{ δˆE e(= x)e} − 1{ δˆE j(= x)j}(cid:33) Y(cid:101)e+ ,j, (15) Proof. SeeAppendixB.
andthesecond-stagepseudo-outcomeregressionˆb+ (x)= NotethattheCB-DR-learnersatisfiesadoublerobustness
e,j propertyanalogouslytothestandardDR-learnerforCATE
Eˆ[Bˆ+IPW |X =x].
e,j estimation(Kennedyetal.,2023). Inparticular,itallowsfor
(iv)CB-DR-learner: Thecross-environment-bounddoubly
amisspecificationofeitherrˆ+(x)orδˆℓ(x)aslongasthe
ℓ
robustlearnerisdefinedvia othernuisancefunctioniscorrectlyspecified. Furthermore,
(cid:32) 1{E=e}(cid:33) (cid:32) 1{E=j}(cid:33) theWB-learnerisalwaysconsistentbecausethecorrespond-
Bˆ e+ ,D jR=Bˆ e+ ,I jPW+ 1− δˆ e(x) rˆ e+(x)− 1− δˆ j(x) rˆ j+(x) ingpseudo-outcomefromEq.(11)doesnotdependonany
(16) nuisanceestimatorsbutjustontheobservationaldata. This
andthesecond-stagepseudo-outcomeregressionˆb+ (x)= is different for cross-environment bounds b+ (x), which
e,j e,j
Eˆ[Bˆ+DR |X =x]. wouldrequireobservingthesamedataintwodifferenten-
e,j
vironments. In that sense, the problem of estimating the
(v) Further meta-learners: We emphasize that, in prin- b+ (x) gives rise to the fundamental problem of causal
e,j
ciple,anyexistingmeta-learnerforCATEestimationcan inference: potential outcomes corresponding to different
be applied for estimating the cross-environment bounds. environmentsareneverobservedsimultaneously.
Furtherlearnersnotspecificallydiscussedhereincludethe
U-learner (Ku¨nzel et al., 2019) and the R-learner (Nie &
5.4.Implementationusingneuralnetworks
Wager,2021)(inthecaseofabinaryenvironmentalvariable
E). Ingeneral,ourproposedmeta-learnersaremodel-agnostic
inbothstagesandthusworkwitharbitrarymachinelearning
5.3.Theoreticalguarantees
models. Here,weprovideaflexibleimplementationusing
The following result analyzes the consistency of our pro- neuralnetworks.Specifically,weadapttheimplementations
posedsecond-stagelearnersdependingonwhetherthenui- used in previous works for evaluating meta-learners for
sanceparametersarecorrectlyspecified. point-identifiedCATEestimation(Curth&vanderSchaar,
2021a;b). Importantly,weusesimilarnetworkarchitectures
Theorem 5.1 (Consistency and double robustness). The
andsettingsforallmeta-learners. Thus,performancediffer-
meta-learnersareconsistentestimatorsofthewithinand
encesaredecoupledfrommodelchoice,sothatthesourceof
cross-environmentbounds. FortheWB-learner,itholdsthat
gainisgivenbythemeta-learners. Wereporttheimplemen-
b+ (x)=E[Bˆ+WB |X =x,E =e] (17) tationdetailsinAppendixD.Here,wewanttoemphasize
e,e e,e
7Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
method SyntheticData1 SyntheticData2 observe the following: (i) All of our meta-learners learn
validboundsreliably,asshownbylowaverageandvaria-
WBna¨ıve 0.073±0.031 0.075±0.045
tionoftheRMSEandcomparableperformancesbetween
WB 0.142±0.069 0.130±0.077
themeta-learners. (ii)Dependingonthesetting,different
CBna¨ıve 0.148±0.098 0.156±0.105 meta-learnersmayperformbetterorworse. Asexpected,in
CB-PI 0.125±0.059 0.127±0.063 dataset1withsimpleδ (x),theCB-IPWlearnerperforms
e
CB-RA 0.179±0.089 0.119±0.037 best, while, in the more complex dataset 2, the CB-DR-
CB-IPW 0.117±0.057 0.165±0.072 learner shows the best performance. This is in line with
CB-DR 0.132±0.061 0.111±0.069 previousworkaroundmeta-learnersforCATEestimation
(Curth&vanderSchaar,2021a),whileweshowthatthe
Table1.MeanandstandarddeviationoftheRMSEover5random sameholdstrueformeta-learnersaimedatpartialidentifica-
runsforsyntheticdatasets1&2. tion. (iii)Thecross-environmentboundscanbeespecially
helpfulforlearningtightandinformativebounds,asdemon-
stratedintheborderregionsofX inFigure3. Hence,reli-
ableestimationisparticularlyimportantintheseareas,and,
thatourexperimentsserveasa“proof-of-demonstration”to
intheconsideredsettings,ourCB-learnershelptoimprove
showthebehaviorofthedifferentmeta-learnersindiffer-
performanceoverthena¨ıveplug-inlearner.
entsettings,whiletheneuralnetworkarchitecturesarenot
optimizedfortherespectivetasks. Instead,inpractice,the Furthermore,inFigure3,wecangiveanintuitionintothe
exactmodelingchoicescanbeflexiblyadjustedtofitdata differencesinboundsestimationbetweenthena¨ıveplug-in
propertiesanddesiredinductivebiases. learner and the two-stage learners. For the latter, we use
theWB-learnerincombinationwiththeCB-DR-learneras
6.Experiments a representative example for illustration. While still per-
formingwell,wecanobservethatthena¨ıveplugin-learner
Weperformexperimentsonsyntheticandreal-worlddata yields slightly less stable estimates with higher stand de-
todemonstratethatourmeta-learnerscaneffectivelylearn viations,whichisexpectedduetoestimationerrorsinthe
boundsinoursettingwithmultipleenvironments. Synthetic nuisance estimation. The two-stage learners, in contrast,
dataarecommonlyusedtoevaluatecausalinferencemeth- are less prone to minor errors in the nuisance estimators
ods as it ensures that the causal ground-truth is available and,asexpected,yieldslightlysmootherandmorerobust
(Curth&vanderSchaar,2021a;Xuetal.,2021)andthus estimatesofthebounds,asshownbylowerstanddeviations.
allowsustoexaminetheperformance.Since,asinCATEes- ThisisalsoreflectedbylowerRMSEandlowervariationin
timation(Curth&vanderSchaar,2021a),theperformance Table1. However,notethatthiscanresultinoversmoothing
ofourdifferentindividualmeta-learnersdependsstrongly whentheground-truthboundshavearathercomplexform,
ontheexactdata-generatingprocess,werefrainfromheavy e.g.,asforthewithin-environmentbounds. Interestingly,as
benchmarking. Instead, we show that our meta-learners statedabove,alsohereasimilarbehaviorwasobservedfor
reliablylearnvalidboundsandgiveshortintuitiontotheir meta-learnersforpointidentification. Insum,ourresults
behavior. showthat,dependingonthedataproperties,differentlearn-
erscanbepreferableforestimatingbounds,andweprovide
Syntheticdata: Forsyntheticdata,unlikeforreal-world
aselectionofdifferentoptionswithournovelmeta-learners.
data, we have access to the ground-truth data-generating
process. Hence,wecancompareourmeta-learnersagainst Real-world data: We now demonstrate the applicability
theoracleCATEandtheoracleboundsfromTheorem4.2 ofourmeta-learnerstoreal-worlddata. Sincetheground-
calculatedwiththeground-truthnuisanceestimators. Here, truthCATEisunobservableforreal-worlddata,werefrain
we consider two settings where we vary the complexity from benchmarking, but, instead, our primary aim is to
of the environment probability δ e(x) from rather simple demonstratethepracticalvalueofourmeta-learners. Here,
(dataset 1) to more complex (dataset 2). Details on the we perform a case study using a dataset with COVID-19
data-generating mechanisms are in Appendix E. In both hospitalizations in Brazil across different regions (Baqui
cases,thetreatmentassignmenttendstobecomemoredif- etal.,2020). Weareinterestedinpredictingtheeffectof
ferentacrossenvironmentsattheborderofX,sothatthe comorbidityonthemortalityofCOVID-19patients. Forthe
cross-environmentboundsshouldbecometighterandmore environments,weusetheregionsofthehospitalsinBrazil,
informativeintheseareas. whicharesplitintoNorthandCentral-South. Asobserved
confounders,weincludeage,sex,andethnicity.
Wereporttheresultsintermsoftherootmeansquarederror
(RMSE)totheoracleboundsinTable1. Wefurtherdisplay We report the estimates of the CATE bounds wrt. to age
illustrative insights into the true and predicted bounds of and averaged over the other confounders (mean ± std),
different meta-learners for dataset 2 in Figure 3. We can
8Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
       W U X H  & $ 7 (
 : %   ( Q Y  
       :  & % %        ( (  Q Q  Y Y        [ 
 & %   ( Q Y   [ 
     
     
     
     
     
                                                                                                                                      
 ;  ;  ;
Figure3.Comparisonofestimationmethodsforboundsbasedonsyntheticdataset2(predictedboundsmean±3stdover5runs).Left:
Oracleboundsforwithin-(WB)andcross-(CB)environments.Center:Estimatedboundsbythena¨ıveplug-inlearner.Right:Estimated
boundsbyourtwo-stagemeta-learners(here:WB-learnerwithCB-DR-learner).
estimatedby(i)thena¨ıveplug-inlearnerand(ii)theCB-PI- thetreatmentdecision. Here,therandomizedtreatmentas-
learnerasanotherexampleforthetwo-stagelearners. The signmentautomaticallyrendersE independentofpotential
resultsaredisplayedinFig.4. Asexpected,forbothlearn- unobservedconfounders,thusfulfillingAssumption3.2.
ers,thebestlowerboundsareclosertozerothanthebest
InsettingswithdiscreteIVs,thereisagainrichliterature
upperboundsforallages,indicatingthatcomorbidityhas
onCATEestimationusingpointestimates(Syrgkanisetal.,
no(large)negativeeffectonmortality(i.e.,nolargepositive
2019;Frauen&Feuerriegel,2023). Therearealsodifferent
effectonsurvivalprobability). However,givenourdata,we
effortsaimedatthederivationofbounds(Swansonetal.,
cannotprovethatcomorbidityhasaneffectonmortalityat
2018),butmethodsforestimation,suchasmeta-learners,
all. Further,weobservethatthecross-environmentbounds
arescarce. Tothebestofourknowledge,ourworkisthe
helptotightenthebounds. Incomparison,ourtwo-stage
firsttoproposemeta-learnersforestimatingsuchbounds.
learnersyieldsimilarpredictionstothena¨ıveplug-inlearner,
indicatingrobustestimation.Insum,theresultsdemonstrate
theapplicabilityofourmeta-learnerstoreal-worlddata. 8.Discussion
Conclusion:Inthispaper,wederivetightboundsforpartial
        identificationoftheCATEwithdatafrommultipleenviron-
        mentsbyshowingtherelationofoursettingtoIVsettings.
        Further,weproposenovelmodel-agnosticmeta-learnersfor
 % R X Q G V
         & %   ( Q Y   [  estimatingtheseboundsfromobservationaldata. Wefind
 & %   ( Q Y   [ 
         : %   ( Q Y   thatthecross-environmentboundsareespeciallyhelpfulfor
 : %   ( Q Y  
        learningtightboundsinareaswherethetreatmentassign-
        mentpolicyvariesclearlybetweentheenvironments,and
        thattheperformancerankingoftherespectivemeta-learners
                                          depends on the data generating process. Hence, our pro-
 $ J H  $ J H
posedmeta-learnersprovideavaluabletoolfortailoringthe
Figure4.Insightsfromreal-worlddata:Effectofcomorbidityon estimationofboundsfortheCATEwithdatafrommulti-
mortalityinCOVID-19patientsinBrazil.Left:Estimatedbounds ple environments to the respective properties of different
byna¨ıveplug-inlearner. Right: Estimatedboundsbyourtwo- settings.
stagemeta-learners(here:WB-learnerandCB-PI-learner).
Future directions: We envision that our ideas could be
usedtodevelopmeta-learnersinotherpartialidentification
7.ApplicabilitytoIVsettings settings,forwhicheffectiveestimationproceduresareoften
lacking. Examplesaremeta-learnersforsettingswithcon-
Ourmeta-learnersapplytoanysettinginwhichadiscrete
tinuousinstruments,leakymediation,orsensitivityanalysis.
instrumentalvariable(IV),E,isavailable,whichdoesnot
necessarily need to correspond to an environment indica-
tor. A prominent example are randomized controlled tri-
als (RCTs) with non-compliance. Note that RCTs with
non-compliancedonotneedmultipleenvironmentsbutcan
have just a single environment. Then, E corresponds to
arandomizedtreatmentprescription(e.g.,byaphysician)
andAindicateswhetherthepatientcompliedandfollowed
9
 ( 7 $ &
 ( 7 $ &
 ( 7 $ &
 ( 7 $ &  ( 7 $ &Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
Impactstatement Balke,A.andPearl,J. Boundsontreatmenteffectsfrom
studieswithimperfectcompliance. JournaloftheAmeri-
Weacknowledgeourmeta-learnersbuildonformalassump-
canStatisticalAssociation,92(439):1171–1176,1997.
tions that are standard in the causal inference literature.
Hence, we recommend a cautious and responsible use of Baqui,P.,Bica,I.,Marra,V.,Ercole,A.,andvanDerSchaar,
ourmeta-learnerstoensurethattheassumptionsaremet,so M. Ethnicandregionalvariationsinhospitalmortality
thatreliableinferencescanbemade. Eventhoughweaim fromcovid-19inbrazil: across-sectionalobservational
torelaxstandardassumptionssuchasunconfoundedness, study. The Lancet Global Health, 8(8):e1018–e1026,
westillimplicitlyassumethattheenvironmentalvariableE 2020.
actsasaninstrument,i.e.,isnotcorrelatedwithanypoten-
tialunobservedconfounders. Thisassumptioncangenerally Bareinboim,E.andPearl,J. Causalinferenceandthedata-
notbetestedandmustbejustifiedbydomainknowledge. fusionproblem. ProceedingsoftheNationalAcademy
However, we note that there are many real-world scenar- of Sciences of the United States of America, 113(27):
ioswherethisassumptionholds,forexample,RCTswith 7345–7352,2016.
non-compliance(Finkelsteinetal.,2012).
Bica,I.andvanderSchaar,M. Transferlearningonhetero-
Notwithstanding,ourworkaimstorelaxthe(strict)assump- geneousfeaturespacesfortreatmenteffectsestimation.
tionsofpriorresearchbyallowingforpartialidentification InNeurIPS,2022.
(instead of point identification). This can help to make
Chen, K., Wang, B., and Small, D. S. A differential ef-
causalinferencemorerobust. Assuch,itcanevenhelpto
fectapproachtopartialidentificationoftreatmenteffects.
improvethereliabilityofinferencesformarginalizedgroups.
arXivpreprint,arXiv:2303.06332,2023.
Forexample,marginalizedgroupsaredifferentfromtherest
ofthepopulationintermsoftheirsocio-economicstatus,
Curth,A.andvanderSchaar, M. Nonparametricestima-
which is a known confounder in medical studies, so that
tionofheterogeneoustreatmenteffects: Fromtheoryto
causalinference thatignoressuch confoundermay bebi-
learningalgorithms. InAISTATS,2021a.
ased,whileourmeta-learnerscanhelptomakemorereliable
inferences. Curth,A.andvanderSchaar,M. Oninductivebiasesfor
heterogeneoustreatmenteffectestimation. InNeurIPS,
References 2021b.
Acharki,N.,Lugo,R.,Bertoncello,A.,andGarnier,J.Com- Curth,A.,Alaa,A.M.,andvanderSchaar,M. Estimating
parisonofmeta-learnersforestimatingmulti-valuedtreat- structural target functions using machine learning and
mentheterogeneouseffects. InInternationalConference influencefunctions. arXivpreprint,arXiv:2008.06461,
onMachineLearning,2023. 2020.
D’Amour,A.,Ding,P.,Feller,A.,Lei,L.,andSekhon,J.
Adler, N. E. and Stead, W. W. Patients in context: EHR
Overlapinobservationalstudieswithhigh-dimensional
captureofsocialandbehavioraldeterminantsofhealth.
covariates. Journal of Econometrics, 221(2):644–654,
TheNewEnglandJournalofMedicine,372(8):695–698,
2021.
2015.
Dorn,J.andGuo,K. Sharpsensitivityanalysisforinverse
Angrist,J.D.Lifetimeearningsandthevietnameradraftlot- propensityweightingviaquantilebalancing. Journalof
ter: Evidencefromsocialsecurityadministrativerecords. theAmericanStatisticalAssociation,2022.
TheAmericanEconomicReview,80(3):313–336,1990.
Dorn, J., Guo, K., and Kallus, N. Doubly-valid/ doubly-
Angrist,J.D.andPischke,J.-S. Mostharmlesseconomet- sharpsensitivityanalysisforcausalinferencewithunmea-
rics. PrincetonUniversityPress,Princeton,2008. sured confounding. arXiv preprint, arXiv:2112.11449,
2022.
Athey,S.,Chetty,R.,andImbens,G.Combiningexperimen-
Duarte, G., Finkelstein, N., Knox, D., Mummolo, J., and
talandobservationaldatatoestimatetreatmenteffectson
Shpitser,I. Anautomatedapproachtocausalinference
longtermoutcomes. arXivpreprint,arXiv:2006.09676,
indiscretesettings. JournaloftheAmericanStatistical
2020.
Association,2023.
Balazadeh,V.,Syrgkanis,V.,andKrishnan,R.G. Partial Feuerriegel, S., Frauen, D., Melnychuk, V., Schweisthal,
identificationoftreatmenteffectswithimplicitgenerative J.,Hess,K.,Curth,A.,Bauer,S.,Kilbertus,N.,Kohane,
models. InNeurIPS,2022. I.S.,andvanderSchaar,M. Causalmachinelearningfor
10Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
predictingtreatmentoutcomes. NatureMedicine,30(4): Imbens,G.,Kallus,N.,Mao,X.,andWang,Y. Long-term
958–968,2024. causal inference underpersistent confounding via data
combination. arXivpreprint,2022.
Finkelstein, A., Taubman, S., Wright, B., Bernstein, M.,
Gruber, J., Newhouse, J. P., Allen, H., and Baicker, K. Jesson,A.,Mindermann,S.,Gal,Y.,andShalit,U. Quanti-
Theoregonhealthinsuranceexperiment: Evidencefrom fyingignoranceinindividual-levelcausal-effectestimates
thefirstyear. TheQuarterlyJournalofEconomics,127 underhiddenconfounding. InICML,2021.
(3):1057–1106,2012.
Jin,Y.,Ren,Z.,andCande`s,E.J. Sensitivityanalysisof
Foster,D.J.andSyrgkanis,V. Orthogonalstatisticallearn- individual treatment effects: A robust conformal infer-
ing. arXivpreprint,arXiv:1901.09036,2019. enceapproach. ProceedingsoftheNationalAcademyof
Sciences(PNAS),120(6),2023.
Frauen,D.andFeuerriegel,S. Estimatingindividualtreat-
menteffectsunderunobservedconfoundingusingbinary
Johansson, F. D., Shalit, U., and Sonntag, D. Learning
instruments. InICLR,2023.
representations for counterfactual inference. In ICML,
2016.
Frauen, D., Melnychuk, V., and Feuerriegel, S. Sharp
bounds for generalized causal sensitivity analysis. In
Jung,Y.,Diaz,I.,Tian,J.,andBareinboim,E. Estimating
NeurIPS,2023.
causaleffectsidentifiablefromacombinationofobserva-
tionsandexperiments. InNeurIPS,2023.
Frauen, D., Imrie, F., Curth, A., Melnychuk, V., Feuer-
riegel,S.,andvanderSchaar,M. Aneuralframework
Kallus, N., Pulim Aahlad Manas, and Shalit, U. Remov-
forgeneralizedcausalsensitivityanalysis. InTheTwelfth
inghiddenconfoundingbyexperimentalgrounding. In
InternationalConferenceonLearningRepresentations,
NeurIPS,2018.
2024.
Kallus, N., Mao, X., and Zhou, A. Interval estimation
Ghassami,A.,Yang,A.,Richardson,D.,Shpitser,I.,and
ofindividual-levelcausaleffectsunderunobservedcon-
Tchetgen, E. T. Combining experimental and observa-
founding. InAISTATS,2019.
tionaldataforidentificationandestimationoflong-term
causaleffects. arXivpreprint,arXiv:2201.10743,2022.
Karlsson, R. K. A. and Krijthe, J. H. Combining obser-
vational datasets from multiple environments to detect
Glass,T.A.,Goodman,S.N.,Herna´n,M.A.,andSamet,
hiddenconfounding. InNeurIPS,2023.
J.M. Causalinferenceinpublichealth. AnnualReview
ofPublicHealth,34:61–75,2013.
Kennedy, E. H. Semiparametric doubly robust targeted
Hatt,T.,Berrevoets,J.,Curth,A.,Feuerriegel,S.,andvan doublemachinelearning: Areview. arXivpreprint,2022.
derSchaar,M. Combiningobservationalandrandomized
Kennedy,E.H. Towardsoptimaldoublyrobustestimation
dataforestimatingheterogeneoustreatmenteffects.arXiv
of heterogeneous causal effects. Electronic Journal of
preprint,2022.
Statistics,17(2):3008–3049,2023.
Heinze-Deml,C.,Peters,J.,andMeinshausen,N. Invariant
Kennedy,E.H.,Balakrishnan,S.,andWasserman,L. Semi-
causalpredictionfornonlinearmodels.JournalofCausal
parametriccounterfactualdensityestimation. Biometrika,
Inference,6(2),2018. ISSN2193-3677.
2023. ISSN0006-3444.
Hu,Y.,Wu,Y.,andWu,X. Agenerativeadversarialframe-
workforboundingconfoundedcausaleffects. InAAAI, Kilbertus, N., Kusner, M. J., and Silva, R. A class of al-
2021. gorithms for general instrumental variable models. In
NeurIPS,2020.
Huang,Y.,Talwar,A.,Lin,Y.,andAparasu,R.R. Machine
learningmethodstopredict30-dayhospitalreadmission Ku¨nzel,S.R.,Sekhon,J.S.,Bickel,P.J.,andYu,B. Met-
outcomeamongusadultswithpneumonia:analysisofthe alearnersforestimatingheterogeneoustreatmenteffects
nationalreadmissiondatabase.BMCMedicalInformatics using machine learning. Proceedings of the National
andDecisionMaking,22(1):288,2022. AcademyofSciences(PNAS),116(10):4156–4165,2019.
Ilse, M., Forre´, P., Welling, M., and Mooij, J. M. Com- Kuzmanovic,M.,Frauen,D.,Hatt,T.,andFeuerriegel,S.
bininginterventionalandobservationaldatausingcausal Causalmachinelearningforcost-effectiveallocationof
reductions. arXivpreprint,arXiv:2103.04786,2023. developmentaid. InKDD,2024.
11Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
Levis,A.W.,Bonvini,M.,Zeng,Z.,Keele,L.,andKennedy, Syrgkanis,V.,Lei,V.,Oprescu,M.,Hei,M.,Battocchi,K.,
E.H. Covariate-assistedboundsoncausaleffectswith and Lewis, G. Machine learning estimation of hetero-
instrumentalvariables. arXivpreprintarXiv:2301.12106, geneoustreatmenteffectswithinstruments. InNeurIPS,
2023. 2019.
Manski,C.F. Nonparametricboundsontreatmenteffects. Varian,H.R. Causalinferenceineconomicsandmarketing.
TheAmericanEconomicReview,80(2):319–323,1990. ProceedingsoftheNationalAcademyofSciences(PNAS),
113(27):7310–7315,2016.
Mooij,J.M.,Magliacane,S.,andClaassen,T. Jointcausal
inference from multiple contexts. Journal of Machine Wager,S.andAthey,S. Estimationandinferenceofhetero-
LearningResearch,21:1–108,2020. geneoustreatmenteffectsusingrandomforests. Journal
oftheAmericanStatisticalAssociation,113(523):1228–
Nie, X.andWager, S. Quasi-oracleestimationofhetero-
1242,2018.
geneoustreatmenteffects. Biometrika,108(2):299–319,
2021. Xu,L.,Chen,Y.,Srinivasan,S.,Freitas,N.d.,Doucet,A.,
andGretton,A. Learningdeepfeaturesininstrumental
Oprescu,M.,Dorn,J.,Ghoummaid,M.,Jesson,A.,Kallus,
variableregression. InICLR,2021.
N., and Shalit, U. B-learner: Quasi-oracle bounds on
heterogeneouscausaleffectsunderhiddenconfounding. Yin,M.,Shi,C.,Wang,Y.,andBlei,D.M. Conformalsen-
InICML,2023. sitivityanalysisforindividualtreatmenteffects. Journal
oftheAmericanStatisticalAssociation,119(545):1–14,
Padh, K., Zeitler, J., Watson, D., Kusner, M., Silva, R.,
2024.
and Kilbertus, N. Stochastic causal programming for
boundingtreatmenteffects. InCLeaR,2023. Zhang,J.,Tian,J.,andBareinboim,E.Partialcounterfactual
identificatitonfromobservationalandexperimentaldata.
Pearl,J. Causality. CambridgeUniversityPress,NewYork
InICML,2022.
City,2009. ISBN9780521895606.
Peters,J.,Bu¨hlmann,P.,andMeinshausen,N. Causalin-
ferencebyusinginvariantprediction: Identificationand
confidenceintervals. JournaloftheRoyalStatisticalSo-
ciety: SeriesB,78(5):947–1012,2016. ISSN1467-9868.
Rubin, D. B. Estimating causal effects of treatments in
randomizedandnonrandomizedstudies. JournalofEdu-
cationalPsychology,66(5):688–701,1974. ISSN0022-
0663.
Schweisthal,J.,Frauen,D.,Melnychuk,V.,andFeuerriegel,
S. Reliableoff-policylearningfordosagecombinations.
InNeurIPS,2023.
Shalit, U., Johansson, F. D., and Sontag, D. Estimating
individualtreatmenteffect: Generalizationboundsand
algorithms. InICML,2017.
Shi, C., Blei, D. M., and Veitch, V. Adapting neural net-
worksfortheestimationoftreatmenteffects. InNeurIPS,
2019.
Shi,C.,Veitch,V.,andBlei,D.M. Invariantrepresentation
learningfortreatmenteffectestimation. InUAI,2021.
Swanson,S.A.,Herna´n,M.A.,Miller,M.,Robins,J.M.,
andRichardson,T.S. Partialidentificationoftheaver-
agetreatmenteffectusinginstrumentalvariables: review
ofmethodsforbinaryinstruments,treatments,andout-
comes. JournaloftheAmericanStatisticalAssociation,
113(522):933–947,2018.
12Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
A.Extendedrelatedwork
Causalinferenceusingmultipledatasets: Arelatedstreamofliteraturefocusesoncausalinferencefrommultipledatasets
andcanberoughlyseparatedintoworksthatleverage(i)randomizedor(ii)purelyobservationaldata.
Worksfrom(i)oftenleveragesmallamountsofrandomizeddataincombinationwithobservationaldatatoobtainunbiased
causaleffectestimates. Forexample,Kallusetal.(2018)andHattetal.(2022)usetheobservationaldatatoobtainabiased
CATEestimator,whichisthensubsequentlydebiasedusingtherandomizeddata. Similarapproachesexistforlong-term
effects(Atheyetal.,2020;Ghassamietal.,2022;Imbensetal.,2022). Anestimationtheoryformoregeneralcausaleffects
hasbeenproposedbyJungetal.(2023). Finally,boundsforcausaleffectshavebeenderivedbyZhangetal.(2022)andIlse
etal.(2023). Notethatalloftheseworksrequirerandomizeddata,whileourpaperreliesonpurelyobservationaldata.
Worksfrom(ii)includeinvariantcausalpredictionmethods(Petersetal.,2016;Heinze-Demletal.,2018;Mooijetal.,
2020)thataimtodiscoverthecausalpredictorsofatargetvariable. ThisideahasbeenadaptedtoCATEestimationtofinda
validadjustmentsetofcovariates(Shietal.,2021). Bareinboim&Pearl(2016)proposedatheoryforidentifiabilityand
transportabilityofcausaleffectsacrossdifferentenvironmentsunderpotentialselectionbias. Otherworksthatleverage
observationaldatasetsfromdifferentenvironmentsincludetransferlearningforCATEunderunconfoundedness(Bica&van
derSchaar,2022)andmethodsfordetectingunobservedconfounding(Karlsson&Krijthe,2023). However,noneofthese
workslearnsboundsfortheCATEunderviolationsofassumptions.
Model-basedCATEestimation: Anotherstreamofliteraturefocusesonadaptingspecificmachinelearningmodelsfor
CATEestimation. Examplesincludeforest-basedmethods(Wager&Athey,2018)andneuralnetworks,whichleverage
sharedrepresentationsacrossresponsesurfacesandpropensityscorestoimprovefinite-sampleperformance(Shietal.,2019;
Curth&vanderSchaar,2021a). Acomplementaryapproachistolearnbalancedrepresentationsthatareunpredictableof
thetreatment(Johanssonetal.,2016;Shalitetal.,2017). Thesemethodsareincontrasttomodel-agnosticmeta-learners,
whichcanbeusedwithanyarbitrarymachinelearningmodel.
13Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
B.Proofs
B.1.ProofofLemma4.1
Proof. Theresultfollowsfromdecomposingtheoracleresponsefunctionintoafactualandcounterfactualpart,whichcan
bebounded. Thatis,
µ (x)=πe(x)E[Y(a)|X =x,A=a]+(1−πe(x))E[Y(a)|X =x,A̸=a] (19)
(cid:101)a a a
=πe(x)µe(x)+(1−πe(x))E[Y(a)|X =x,A̸=a] (20)
a a a
≤πe(x)µe(x)+(1−πe(x))s , (21)
a a a 2
whereweusedthedefinitionofs ands asthesupportboundaryofY.
1 2
B.2.ProofofTheorem4.2
Proof. Notethat
τ (x)=µ (x)−µ (x). (22)
a1,a2 (cid:101)a1 (cid:101)a2
Hence,Lemma4.1andAssumptions3.1and3.2implythat
b− (x)≤τ (x)≤b+ (x) (23)
e,j a1,a2 e,j
forallenvironmenteandj. Hence,takingtheminimumandmaximumovereandj yieldstheresult.
B.3.ProofofTheorem5.1
Proof. Withoutlossofgenerality,weshowtheresultfortheupperbounds. Forlowerbounds,theproofworksanalogously
byinterchangingthesupportpointss ands . WeproceedbycalculatingE[Bˆ+ |X =x]foreachpseudo-outcomeBˆ+ ,
1 2 e,j e,j
whichcorrespondstoanoraclesecondstageregression. WestartwiththeWB-learner,whichusespseudo-outcomesdefined
via
Bˆ+WB =1{E =e}(AY +(1−A)s −(1−A)Y −As ). (24)
e 2 1
Hence,weobtain
E[Bˆ+WB |X =x,E =e]=E[AY |X =x,E =e]+s E[1−A|X =x,E =e] (25)
e 2
−E[(1−A)Y |X =x,E =e]−s E[A|X =x,E =e] (26)
1
=πe(x)µe(x)+πe(x)s −πe(x)µe(x)−πe(x)s =b+ (x). (27)
1 1 0 2 0 0 1 1 e,e
asdesired. TheCB-RA-learnerusespseudo-outcomesdefinedvia
(cid:16) (cid:17) (cid:16) (cid:17)
Bˆ+RA =1{E =e} Y(cid:101)+ −rˆ+(x) +1{E =j} rˆ+(x)−Y(cid:101)+ (28)
e,j e,j j e e,j
+1{E ̸=e}1{E ̸=j}(cid:0) rˆ+(x)−rˆ+(x)(cid:1) . (29)
e j
TakingexpectationconditionalonX =xyields
E[Bˆ+RA |X =x]=δ (x)(cid:0)E[AY |X =x,E =e]+s E[1−A|X =x,E =e]−rˆ+(x)(cid:1) (30)
e,j e 2 j
+δ (x)(cid:0) rˆ+(x)−E[(1−A)Y |X =x,E =j]−s E[A|X =x,E =j](cid:1) (31)
j e 1
+(1−δ (x)−δ
(x))(cid:0) rˆ+(x)−rˆ+(x)(cid:1)
(32)
e j e j
=δ (x)(cid:0) πe(x)µe(x)+πe(x)s −rˆ+(x)(cid:1) (33)
e 1 1 0 2 j
(cid:16) (cid:17)
+δ (x) rˆ+(x)−πj(x)µj(x)−πj(x)s (34)
j e 0 0 1 1
+(1−δ (x)−δ
(x))(cid:0) rˆ+(x)−rˆ+(x)(cid:1)
. (35)
e j e j
Hence,wheneverrˆ+(x)=r+(x)=πe(x)µe(x)+πe(x)s andrˆ+(x)=r+(x)=πj(x)µj(x)−πj(x)s ,weobtain
e e 1 1 0 2 j j 0 0 1 1
E[Bˆ+RA |X =x]=δ (x)b+ (x)+δ (x)b+ (x)++(1−δ (x)−δ (x))b+ (x)=b+ (x). (36)
e,j e e,j j e,j e j e,j e,j
14Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
Thepseudo-outcomesoftheCB-IPW-learneraredefinedvia
1{E =e} 1{E =j}
Bˆ+IPW = (AY +(1−A)s )− ((1−A)Y +As ), (37)
e,j δˆ (x) 2 δˆ (x) 1
e j
whichyields
δ (x)
E[Bˆ+IPW |X =x]= e (E[AY |X =x,E =e]+s E[1−A|X =x,E =e]) (38)
e,j δˆ (x) 2
e
δ (x)
− j (E[(1−A)Y |X =x,E =j]+s E[A|X =x,E =j]) (39)
δˆ (x) 1
j
δ (x) δ (x)(cid:16) (cid:17)
= e (πe(x)µe(x)+πe(x)s )− j πj(x)µj(x)+πj(x)s . (40)
δˆ (x) 1 1 0 2 δˆ (x) 0 0 1 1
e j
Hence,δˆ(x)=δ (x)implies
ℓ ℓ
E[Bˆ+IPW |X =x]=b+ (x). (41)
e,j e,j
Finally,thepseudooutcomesoftheCB-DR-learneraredefinedvia
(cid:32) (cid:33) (cid:32) (cid:33)
1{E =e} 1{E =j}
Bˆ+DR =Bˆ+IPW+ 1− rˆ+(x)− 1− rˆ+(x). (42)
e,j e,j δˆ (x) e δˆ (x) j
e j
Again,bytakingexpectationconditionalonX =x,weobtain
δ (x) δ (x)(cid:16) (cid:17)
E[Bˆ+DR |X =x]= e (πe(x)µe(x)+πe(x)s )− j πj(x)µj(x)+πj(x)s (43)
e,j δˆ (x) 1 1 0 2 δˆ (x) 0 0 1 1
e j
(cid:32) (cid:33) (cid:32) (cid:33)
δ (x) δ (x)
+ 1− e rˆ+(x)− 1− j rˆ+(x). (44)
δˆ (x) e δˆ (x) j
e j
(45)
Underδˆ(x)=δ (x),thisreducesto
ℓ ℓ
(cid:16) (cid:17)
E[Bˆ+DR |X =x]=(πe(x)µe(x)+πe(x)s )− πj(x)µj(x)+πj(x)s +0rˆ+−0rˆ+ (46)
e,j 1 1 0 2 0 0 1 1 e j
=b+ (x). (47)
e,j
Underrˆ+(x)=r+(x)=πe(x)µe(x)+πe(x)s andrˆ+(x)=r+(x)=πj(x)µj(x)−πj(x)s ,theexpressionreducesto
e e 1 1 0 2 j j 0 0 1 1
(cid:32) (cid:33) (cid:32) (cid:33)
δ (x) δ (x) δ (x) δ (x) (cid:16) (cid:17)
E[Bˆ+DR |X =x]= 1+ e − e (πe(x)µe(x)+πe(x)s )− 1+ j − j πj(x)µj(x)+πj(x)s
e,j δˆ (x) δˆ (x) 1 1 0 2 δˆ (x) δˆ (x) 0 0 1 1
e e j j
(48)
=b+ (x), (49)
e,j
whichprovestheresult.
15Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
C.AdditionalInsights
InFigures3and4,weplotthelearnedboundsfor(i)bothwithin-environmentboundsand(ii)bothcross-environment
boundstodemonstratethegeneralperformanceofourlearners. However,inpractice,weareusuallyonlyinterestedinthe
tightestboundsasstatedinTheorem4.2. Hence,forcompleteness,weplotthelearnedtightestboundsfromFigures3and
4separatelyinFigures5and6. Weobservethesamepatternsforthetightestboundsasforallpossiblebounds: Forthe
syntheticdataset,theCB-DR-learnerresultsinvalidandslightlymorestableestimateswithlowervariationbetweenruns
comparedtothena¨ıvepluginlearner.
Forthereal-worlddataset,wherewecannotcomparewithground-truthbounds,bothselectedlearnersyieldsimilarestimates.
Also,inbothconsideredsettings,almostsolelyourproposedcross-environmentboundsresultinthetightestbounds,while
thewithin-environmentboundsarenothelpfulforyieldinginformativebounds. Here,thisimpliesthatoneshouldfocus
on proper estimation of the CB-bounds, which can be done by our different proposed two-stage CB-meta-learners. In
sum,thisdemonstratesthatbothcontributions,(i)theboundsmakinguseofdatafromdifferentenvironments,and(ii)the
meta-learners,areusefulforlearninginformativeboundsrobustly.
       7  7 L  L J  J K  K W  W H  H V  V W  W    /  8 R  S Z  S H  H U  U
     
     
     
     
 
   
 
   
                                                                                                                                      
 ;  ;  ;
Figure5.Comparisonofestimationmethodsfortightestpredictedboundsbasedonsyntheticdataset2(predictedboundsmean±3std
over5runs). Left: Oracleboundsforwithin-(WB)andcross-(CB)environments. Center: Estimatedboundsbythena¨ıveplug-in
learner.Right:Estimatedboundsbyourtwo-stagemeta-learners(here:WB-learnerwithCB-DR-learner).
 7 L J K W H V W  / R Z H U
 7 L J K W H V W  8 S S H U
       
       
       
       
   
   
   
   
                                         
 $ J H  $ J H
Figure6.Insightsfromreal-worlddata: EffectofcomorbidityonmortalityinCOVID-19patientsinBrazil. Left: Estimatedtightest
boundsbyna¨ıveplug-inlearner.Right:Estimatedtightestboundsbyourtwo-stagemeta-learners(here:WB-learnerandCB-PI-learner).
16
 ( 7 $ &
 ( 7 $ &
 ( 7 $ &
 ( 7 $ &
 ( 7 $ &Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
D.Implementationdetails
Modelarchitectureandparameters: Forourexperiments, weadapttheimplementationsusedinpreviousworksfor
evaluatingmeta-learnersforpoint-identifiedCATEestimation(Curth&vanderSchaar,2021a;b). Importantly,weuse
similarnetworkarchitecturesandparametersettingsforallmeta-learners. Thus,performancedifferencesaredecoupled
from model choice and can be explained by the meta-learners as the source of gain. In detail, we use the software
package https://github.com/AliciaCurth/CATENets and all of the default settings of the PyTorch CATE
meta-learnersprovidedinthispackage. Here,thenetworksforthefirst-andsecond-stagemodelsaresimpleMLPswith
2hiddenlayersandhiddenneuronsizeof100. Forthenuisancefunctionestimationofourna¨ıvepluginestimator, we
implementsimilararchitectures.
Implementation: Byusingourpseudo-outcomesforboundestimationasdescribedinSec.5.2,wecanlearntheboundsas
follows: FortheCB-learners,wedefinethepseudo-outcomesofEq.(12)foreachpossibleenvironmentcombinatione,j
andforupperandlowerbounds,respectively. Then,bysimplyreplacingthefactualoutcomeswiththepseudo-outcomes,
andthetreatmentassignmentwiththeenvironmentassignment,wecantraintherespectivetwo-stage-learnersbyusing
the above-mentioned implementation of the CATE meta-learners. We train an own meta-learner for each environment
combinationandeachupperandlowerbound. FortheWB-learner,wesimplydirectlylearnmodelstopredictthepseudo-
outcomesofEq.(11)foreachenvironmente,andupperandlowerbound,respectively. Weprovideawrappermodulefor
ourboundmeta-learnersinourcode.3
3https://github.com/JSchweisthal/BoundMetaLearners
17Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
E.Detailsregardingsimulateddata
Data-generatingprocess: Foroursyntheticdataset1,wesimulateanobservedconfounderX ∼Uniform[−1,1]andan
unobservedconfounderU ∼ Uniform[0,1]. Wedefineδ(x) = P(E = 1 | x) = σ(x),whereσ(·)denotesthesigmoid
functionσ(x)= 1 . Then,wesimulatetheenvironmentE via
1+exp(−x)
E =1|X =x∼Bernoulli(p=δ(x))). (50)
Wethensampleourtreatmentassignmentsfromtheenvironment-specificpropensityscoresas
A=1|X =x,U =u,E =1∼Bernoulli(p=σ(2.5x+u)), (51)
and
A=1|X =x,U =u,E =0∼Bernoulli(p=(1−σ(2.5x+u))). (52)
Wedefinethetreatmenteffectas
1
τ(x)= x. (53)
3
Finally,wesimulateacontinuousoutcome
1 1
Y =τ(X)A+ (sin(12X)+X)+ cos(2X)+U +0.3ε, (54)
3 60
whereε∼Laplace(0,1).
Foroursyntheticdataset2,wemodelamorecomplexenvironmentprobability. Here,wemodelδ(x)=0.15sin(5x)+0.5,
whilekeepingtheremainingprocessunchanged,suchthatwecanisolateandshowtheeffectoftheenvironmentprobability
ontheperformanceofthedifferentmeta-learners.
TocreatethesimulateddatausedinSec.6,forbothdatasets,wesamplen=10000fromthedata-generatingprocessabove.
Wethensplitthedataintotrain(70%),val(10%),andtest(20%)sets.
18Meta-LearnersforPartially-IdentifiedTreatmentEffectsAcrossMultipleEnvironments
F.Detailsregardingreal-worlddata
WeperformacasestudyusingadatasetwithCOVID-19hospitalizationsinBrazilacrossdifferentregions(Baquietal.,
2020).WeareinterestedinpredictingtheeffectofcomorbidityonthemortalityofCOVID-19patients.Fortheenvironments,
weusetheregionsofthehospitalsinBrazil,whicharesplitintoNorthandCentral-South. Asobservedconfounders,we
includeage,sex,andethnicity. Further,weexcludepatientsyoungerthan20orolderthan80years. Todefinecomorbidity
asabinaryvariable,wedefinecomorbidityas1ifatleastoneofthefollowingconditionswerediagnosedforthepatient:
cardiovasculardiseases,asthma,diabetes,pulmonarydisease,immunosuppression,obesity,liverdiseases,neurological
disorders,renaldisease. Wethensplitthedataintotrain(70%),val(10%),andtest(20%)sets. Fortraining,weusethe
samesettingsasreportedinAppendixD.
19