Coresets for Multiple ℓ Regression
𝑝
David P. Woodruff Taisuke Yasuda
Carnegie Mellon University Carnegie Mellon University
dwoodruf@cs.cmu.edu taisukey@cs.cmu.edu
June 5, 2024
Abstract
Acoreset ofadatasetwith𝑛examplesand𝑑featuresisaweightedsubsetofexamplesthatissufficient
forsolvingdownstreamdataanalytictasks. Nearlyoptimalconstructionsofcoresetsforleastsquaresand
ℓ linear regression with a single response are known in prior work. However, for multiple ℓ regression
𝑝 𝑝
where there can be 𝑚 responses, there are no known constructions with size sublinear in 𝑚. In this
work, we construct coresets of size 𝑂˜(𝜀−2𝑑) for 𝑝<2 and 𝑂˜(𝜀−𝑝𝑑𝑝/2) for 𝑝>2 independently of 𝑚 (i.e.,
dimension-free) that approximate the multiple ℓ regression objective at every point in the domain up 𝑝
to (1±𝜀) relative error. If we only need to preserve the minimizer subject to a subspace constraint, we
improve these bounds by an 𝜀 factor for all 𝑝>1. All of our bounds are nearly tight.
We give two application of our results. First, we settle the number of uniform samples needed to
approximate ℓ Euclidean power means up to a (1+𝜀) factor, showing that Θ˜(𝜀−2) samples for 𝑝=1,
𝑝
Θ˜(𝜀−1) samples for 1<𝑝<2, and Θ˜(𝜀1−𝑝) samples for 𝑝>2 is tight, answering a question of Cohen-
Addad, Saulpic, and Schwiegelshohn. Second, we show that for 1 < 𝑝< 2, every matrix has a subset
of 𝑂˜(𝜀−1𝑘) rows which spans a (1+𝜀)-approximately optimal 𝑘-dimensional subspace for ℓ subspace
𝑝
approximation, which is also nearly optimal.
4202
nuJ
4
]SD.sc[
1v23420.6042:viXra1 Introduction
Least squares linear regression and ℓ linear regression are some of the most fundamental and practically
𝑝
valuable computational problems in statistics and optimization. In this problem, our input is an 𝑛×𝑑 matrix
A∈R𝑛×𝑑 and a response vector b∈R𝑛, and our goal is to output an approximate minimizer xˆ ∈R𝑑 such
that
‖Axˆ−b‖𝑝 ≤(1+𝜀) min‖Ax−b‖𝑝. (1)
𝑝 𝑝
x∈R𝑑
Amongthevastliteratureonℓ regression, samplingalgorithmsandcoresets, whicharealgorithmsthatselect
𝑝
a weighted subset of the rows of A and b that suffice to solve ℓ regression, have played major roles in the
𝑝
development of efficient algorithms. That is, we seek a diagonal matrix S∈R𝑛×𝑛 with few non-zero entries,
i.e., nnz(S)≪𝑛, such that the weighted subset of rows SA and Sb are sufficient to compute a solution xˆ
satisfying (1). We will often refer to nnz(S) as the sample complexity. We focus on approaches that construct
S by i.i.d. sampling of each of the 𝑛 rows:
Definition 1.1 (ℓ sampling matrix). Let 𝑝 ≥ 1. A random diagonal matrix S ∈ R𝑛×𝑛 is a random ℓ
𝑝 𝑝
samplingmatrixwithsamplingprobabilities{𝑞 }𝑛 if for each 𝑖∈[𝑛], the 𝑖th diagonal entry is independently
𝑖 𝑖=1
set to be
{︃
1/𝑞1/𝑝 with probability 𝑞
S = 𝑖 𝑖
𝑖,𝑖
0 otherwise
Two well-studied guarantees for S are strong coresets and weak coresets. Strong coresets refer to coresets
that preserve the value of the objective function at every point in the domain, while weak coresets only
guarantee that the unconstrained minimizer is preserved. If we only care about solving the unconstrained ℓ
𝑝
regression problem, then weak coresets are sufficient to solve this problem, and it is known that weak coresets
can be substantially smaller than strong coresets in certain settings [MMWY22]. On the other hand, strong
coresets are necessary when the objective function must be evaluated at points away from the optimum, for
example for constrained optimization problems.
Definition 1.2 (Strong coreset). We say that S is a strong coreset if ‖S(Ax−b)‖𝑝 =(1±𝜀)‖Ax−b‖𝑝
𝑝 𝑝
simultaneously for every x∈R𝑑.
Definition 1.3 (Weak coreset). We say that S is a weak coreset if
‖Axˆ−b‖𝑝 ≤(1+𝜀) min‖Ax−b‖𝑝
𝑝 𝑝
x∈R𝑑
for xˆ =argmin ‖S(Ax−b)‖𝑝.
x∈R𝑑 𝑝
The efficient construction of coresets for ℓ regression has been studied in a long line of work [Cla05,
𝑝
DMM06a, DMM06b, DDH+09] culminating in the ℓ Lewis weight sampling algorithm [Lew78, BLM89,
𝑝
Tal90, LT91, Tal95, SZ01, CP15, WY23b], which gives an algorithm that constructs a strong coreset S with
{︃
𝑂˜(𝜀−2𝑑) 𝑝≤2
nnz(S)= .
𝑂˜(𝜀−2𝑑𝑝/2) 𝑝>2
A related line of work in the active ℓ regression setting shows that weak coresets for ℓ regression with
𝑝 𝑝
⎧ 𝑂˜(𝜀−2𝑑) 𝑝=1
⎪⎪⎪⎨𝑂˜(𝜀−1𝑑)
1<𝑝<2
nnz(S)=
⎪⎪⎪⎩𝑂𝑂 ˜(( 𝜀𝜀 −− (1 𝑝𝑑 −)
1)𝑑𝑝/2)
𝑝𝑝= >22
can be constructed even without knowing b [CP19, CD21, PPP21, MMWY22, WY23a]. Note that these
bounds strictly improve over the strong coreset guarantees of ℓ Lewis weight sampling for 1<𝑝<3.
𝑝
11.1 Multiple ℓ regression
𝑝
It is often the case that we are interested in more than just one target b to predict, and in general, we may
wish to simultaneously fit 𝑚 target vectors that are given by a matrix B∈R𝑛×𝑚 and solve the minimization
problem
𝑚
min ‖AX−B‖𝑝 = min ∑︁ ‖AXe −Be ‖𝑝
X∈R𝑑×𝑚 𝑝,𝑝 X∈R𝑑×𝑚 𝑗 𝑗 𝑝
𝑗=1
This is known as the multiple response ℓ regression problem, or simply the multiple ℓ regression problem,
𝑝 𝑝
and is the focus of the present work.
1.1.1 Coreset constructions for 𝑝=2
For 𝑝=2, the construction of strong coresets for the multiple response problem follows almost immediately
from strong coresets for the single response problem due to orthogonality and the Pythagorean theorem, and
we can construct S such that
‖S(AX−B)‖2 =(1±𝜀)‖AX−B‖2
𝐹 𝐹
with nnz(S)=𝑂˜(𝜀−2𝑑) samples. Indeed, assume without loss of generality that A has orthogonal columns,
and suppose that S satisfies
• ‖SAx‖2 =(1±𝜀)‖Ax‖2 for every x∈R𝑑 (i.e., S is a subspace embedding)
2 2
• ‖S(AX*−B)‖2 =(1±𝜀)‖AX*−B‖2 where X* is the optimal minimizer
𝐹 𝐹
• ‖A⊤S⊤S(AX*−B)‖2 ≤(𝜀2/𝑑)‖A‖2‖AX*−B‖2 =𝜀2‖AX*−B‖2
𝐹 𝐹 𝐹 𝐹
Then, the following argument of Section 7.5 of [CW13] shows that S is a strong coreset. Indeed,
‖S(AX−B)‖2 =‖SA(X−X*)‖2 +‖S(AX*−B)‖2 +2tr(︀ (X−X*)⊤A⊤S⊤S(AX*−B))︀
𝐹 𝐹 𝐹
by expanding the square, and the inner product term is bounded by
⃒ ⃒tr(︀ (X−X*)⊤A⊤S⊤S(AX*−B))︀⃒ ⃒≤‖X−X*‖ 𝐹‖A⊤S⊤S(AX*−B)‖
𝐹
≤𝜀‖A(X−X*)‖ ‖AX*−B‖
𝐹 𝐹
≤𝜀‖AX−B‖2
𝐹
and S also preserves the quantities ‖SA(X−X*)‖2 and ‖S(AX*−B)‖2 up to (1±𝜀) relative error. A
𝐹 𝐹
similar trick is available in the weak coreset setting (see, e.g., Section 3.1 of [CNW16]), which gives a bound
of nnz(S)=𝑂˜(𝜀−1𝑑) for this guarantee. Unfortunately, almost every step in the above argument uses special
properties of the ℓ norm that are not available for the ℓ norm, and thus we will need completely different
2 𝑝
arguments to handle 𝑝̸=2.
1.1.2 Challenges for 𝑝̸=2
If we desire only weak coresets, then prior results on active ℓ regression in fact almost immediately provide
𝑝
a solution. These results show that a weak coreset S for the single response ℓ regression problem can
𝑝
be constructed independently of b, and with the dependence of nnz(S) on the failure probability 𝛿 being
polylogarithmic. Thus by setting the failure rate to 𝛿 =1/10𝑚, we can simultaneously solve every column of
B independently with overall probability at least 9/10.
For strong coresets, however, such a column-wise strategy must be implemented carefully. If we consider
constructing a strong coreset for a single column 𝑗 ∈[𝑚], then the sampling probabilities now depend on the
target vector Be , so the sampling complexity would need to scale as 𝑚 rather than polylog(𝑚) as in the
𝑗
previous upper bound weak coresets. On the other hand, another natural strategy is to mimic the strategy
for the 𝑝=2 case and take the sampling probabilities to only guarantee an ℓ subspace embedding for the
𝑝
column space of A and that 𝑞 ≥ ‖e⊤B*‖𝑝/‖B*‖𝑝 for B* := AX* −B. This is a reasonable choice of
𝑖 𝑖 𝑝 𝑝,𝑝
sampling probabilities, and indeed it is not hard to see that
‖S(AX−B)‖𝑝 =(1±𝜀)‖AX−B‖𝑝
𝑝,𝑝 𝑝,𝑝
2for any fixed X∈R𝑑×𝑚 with only nnz(S)=𝑂˜(𝜀−2𝑑) samples for 𝑝<2 and nnz(S)=𝑂˜(𝜀−2𝑑𝑝/2) samples for
𝑝>2 via a Bernstein tail bound. However, it is unclear how to extend a guarantee for any single X∈R𝑑×𝑚
toaguaranteesimultaneouslyforall X∈R𝑑×𝑚. Althoughthedependenceonthefailurerate𝛿 islogarithmic,
a net argument, or even more sophisticated chaining arguments, over the possible choices of X∈R𝑑×𝑚 seem
to require a union bound over sets of size exp(𝑑𝑚), thus again introducing a linear dependence on 𝑚 in the
sample complexity nnz(S). As we show, a careful blend of these two ideas will be necessary to obtain our
strong coreset result.
1.2 Strong coresets for multiple ℓ regression
𝑝
Our first main result is the first construction of strong coresets for multiple ℓ regression that is independent
𝑝
of 𝑚.
Theorem 1.4 (Strong coresets for multiple ℓ regression). Let A∈R𝑛×𝑑, B∈R𝑛×𝑚, and 𝑝≥1. There is
𝑝
an algorithm which constructs S with
⎧ [︂ ]︂
𝑂(𝑑) 𝑑 1
⎪⎪⎪⎨
𝜀2
(log𝑑)2log
𝜀
+log
𝛿
1≤𝑝<2
nnz(S)=
⎪⎪⎪⎩𝑂(𝑑
𝜀𝑝𝑝/2)[︂
(log𝑑)2log𝑑
𝜀
+log1
𝛿]︂
𝑝>2
such that with probability at least 1−𝛿,
‖S(AX−B)‖𝑝 =(1±𝜀)‖AX−B‖𝑝
𝑝,𝑝 𝑝,𝑝
simultaneously for every X∈R𝑑×𝑚. Furthermore, S can be constructed in 𝑂˜(nnz(A)+nnz(B)+poly(𝑑))
time.
We achieve a nearly optimal dependence on 𝑑 and 𝜀, as we show that Ω(𝑑𝑝/2/𝜀𝑝) rows are necessary for
strong coresets in Theorem 5.1 for 𝑝>2, while it is known that Ω˜(𝑑/𝜀2) rows are necessary even for 𝑚=1
for 𝑝 < 2 [LWW21]. We note that our upper bound shows that multiple ℓ regression is as easy as single
𝑝
response ℓ regression for 𝑝<2, while our lower bound demonstrates an interesting separation between the
𝑝
two for 𝑝>2.
1.2.1 Initial log𝑚 bound
Our main technique is to generalize the “partition by sensitivity” technique introduced in the active ℓ
𝑝
regression work of [MMWY22] and show how this can be applied to the strong coreset setting. We describe
the idea for the case of 𝑝<2, as the case of 𝑝>2 is analogous.
In the active ℓ regression setting, we must show that we can design sampling algorithms that preserve
𝑝
the objective function value, even if we do not know the target vector b. In this setting, one of the main
observationsof[MMWY22]isthateventhoughwecannotpreserve‖Ax−b‖𝑝 itself, wecanactuallypreserve
𝑝
the difference ‖Ax−b‖𝑝−‖b‖𝑝, if ‖b‖𝑝 =𝑂(OPT𝑝) which is without loss of generality. To see this idea,
𝑝 𝑝 𝑝
assume (without loss of generality due to [DDH+09]) that we restrict our attention to ‖Ax‖𝑝 =𝑂(OPT𝑝).
𝑝
Then, the analysis of [MMWY22] proceeds by partitioning the coordinates of b into two sets, those such
that |b(𝑖)|𝑝 is larger than 𝜀−𝑝w OPT𝑝 and those that are smaller than this threshold, where w is the 𝑖-th ℓ
𝑖 𝑖 𝑝
Lewis weight of A. It is known that w bounds the sensitivities of A, that is, |[Ax](𝑖)|𝑝 ≤w ‖Ax‖𝑝 so it
𝑖 𝑖 𝑝
follows that for any |b(𝑖)|𝑝 ≥𝜀−𝑝w OPT𝑝, we have that
𝑖
||[Ax−b](𝑖)|𝑝−|b(𝑖)|𝑝|=𝑂(𝜀)|b(𝑖)|𝑝
for any x∈R𝑑 with ‖Ax‖𝑝 =𝑂(OPT𝑝). On the other hand, if |b(𝑖)|𝑝 ≤𝜀−𝑝w OPT𝑝, then we have by the
𝑝 𝑖
triangle inequality that
||[Ax−b](𝑖)|𝑝−|b(𝑖)|𝑝|≤𝑂(𝜀−𝑝)w OPT𝑝.
𝑖
Thus, up to an additive 𝑂(𝜀)(‖Sb‖𝑝 + ‖b‖𝑝) error, ||[Ax−b](𝑖)|𝑝−|b(𝑖)|𝑝| has sensitivities which are
𝑝 𝑝
controlled by the ℓ Lewis weights of A. This allows one to show that sampling by the ℓ Lewis weights of A
𝑝 𝑝
preserves ‖Ax−b‖𝑝−‖b‖𝑝 for all ‖Ax‖𝑝 =𝑂(OPT𝑝).
𝑝 𝑝 𝑝
3In order to apply this idea to the strong coreset setting, we generalize the above argument to multiple
scales. That is, we replace OPT𝑝 by an arbitrary scale 𝑅≥‖b‖𝑝, and show that for every ‖Ax‖𝑝 ≤𝑂(𝑅)
𝑝 𝑝
that
⃒ ⃒(︀ ‖S(Ax−b)‖𝑝−‖Sb‖𝑝)︀ −(︀ ‖Ax−b‖𝑝−‖b‖𝑝)︀⃒ ⃒≤𝜀(𝑅+‖Sb‖𝑝)
𝑝 𝑝 𝑝 𝑝 𝑝
Finally, we can generalize this to the following guarantee by union bounding over finitely many scales 𝑅,
which holds for every x∈R𝑑:
⃒ ⃒(︀ ‖S(Ax−b)‖𝑝−‖Sb‖𝑝)︀ −(︀ ‖Ax−b‖𝑝−‖b‖𝑝)︀⃒ ⃒≤𝜀(︀ ‖b‖𝑝+‖Sb‖𝑝+‖Ax‖𝑝)︀
(2)
𝑝 𝑝 𝑝 𝑝 𝑝 𝑝 𝑝
This guarantee is in a form that can be summed over the 𝑚 columns of B. Thus, if a log𝑚 dependence
is admissible, then we can apply the above result with failure probability 1/10𝑚, union bound over the 𝑚
columns, and sum the results to obtain
⃒ ⃒(‖S(AX−B)‖𝑝 −‖SB‖𝑝 )−(‖AX−B‖𝑝 −‖B‖𝑝 )⃒ ⃒≤𝜀(︀ ‖B‖𝑝 +‖SB‖𝑝 +‖AX‖𝑝 )︀ .
𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝
Now suppose that we additionally have
• ‖SB‖𝑝 =(1±𝜀)‖B‖𝑝
𝑝,𝑝 𝑝,𝑝
• ‖B‖𝑝 =𝑂(OPT𝑝) (which is without loss of generality by subtracting an 𝑂(1)-optimal solution)
𝑝,𝑝
Then, we have
‖S(AX−B)‖𝑝 =‖AX−B‖𝑝 −‖B‖𝑝 +‖SB‖𝑝 ±𝑂(𝜀)(︀ ‖B‖𝑝 +‖AX‖𝑝 )︀
𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝
=‖AX−B‖𝑝 ±𝜀‖B‖𝑝 ±𝑂(𝜀)(︀ ‖B‖𝑝 +‖AX‖𝑝 )︀
𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝
=‖AX−B‖𝑝 ±𝑂(𝜀)‖AX−B‖𝑝
𝑝,𝑝 𝑝,𝑝
so we indeed have a strong coreset as desired.
1.2.2 Removing the 𝑚 dependence
Next, we show how to completely remove the 𝑚 dependence, which requires additional ideas. When applying
(2) to each of the 𝑚 columns, suppose that we set the failure probability to poly(𝜀𝛿) instead of 𝑂(1/𝑚).
Then, this guarantee will hold for a 1−poly(𝜀𝛿) fraction of “good” columns, for which we can obtain
(1±𝜀) approximations. On the remaining poly(𝜀𝛿) fraction of “bad” columns, note that the mass of B
on these columns is at most poly(𝜀𝛿)‖B‖𝑝 with probability 1−𝛿 by Markov’s inequality. Then on these
𝑝,𝑝
columns, ‖S(AX−B)e ‖ is just ‖SAXe ‖ up to a small total additive error of poly(𝜀𝛿)‖B‖𝑝 . In
𝑗 𝑝 𝑗 𝑝 𝑝,𝑝
turn, we have that ‖SAXe ‖ = (1±𝜀)‖AXe ‖ by using that S is an ℓ subspace embedding. Thus,
𝑗 𝑝 𝑗 𝑝 𝑝
by combining with the (1±𝜀) approximation on the rest of the “good” columns, we can still ensure that
‖S(AX−B)‖ =(1±𝜀)‖AX−B‖ .
𝑝,𝑝 𝑝,𝑝
1.3 Weak coresets for multiple ℓ regression
𝑝
In the weak coreset setting, we consider a generalized multiple ℓ regression problem, where we are given
𝑝
a design matrix A ∈ R𝑛×𝑑, an “embedding” G ∈ R𝑡×𝑚, and a target matrix B ∈ R𝑛×𝑚, and we wish to
approximately minimize the objective function ‖AXG−B‖ .
𝑝,𝑝
As noted previously, for multiple ℓ regression without an embedding (i.e., G=I ) the construction of
𝑝 𝑡
weak coresets follows relatively straightforwardly by applying active ℓ regression results along each column.
𝑝
However, this strategy fails when we must additionally handle the embedding matrix G, as this constraint
couples the columns of AX together. Furthermore, we argue that handling the embedding G is substantially
moreinterestingthattheunconstrainedcase. Indeed,asweseelaterinSections1.4and1.5,theincorporation
of the embedding G will allow us to handle interesting extensions of our results to settings beyond the
entrywise ℓ norm via the use of a linear embedding into this norm. We will denote the optimal value as
𝑝
OPT:= min ‖AXG−B‖
𝑝,𝑝
X∈R𝑑×𝑡
and let X* denote the matrix achieving this optimum unless otherwise noted. We will prove the following
result:
4Theorem 1.5 (Weak coresets for multiple ℓ regression). Let A ∈ R𝑛×𝑑, G ∈ R𝑡×𝑚, B ∈ R𝑛×𝑚, and
𝑝
1≤𝑝<∞. There is an algorithm which constructs S independently of B with
𝑂(𝑑)[︂
𝑑
1]︂(︂ 1)︂2
nnz(S)= (log𝑑)2log +log loglog
𝜀2𝛿2 𝜀 𝛿 𝜀
for 𝑝=1,
𝑂(𝑑)[︂
𝑑
1]︂(︂ 1)︂2
nnz(S)= (log𝑑)2log +log loglog
𝜀𝛿2 𝜀 𝛿 𝜀
for 1<𝑝<2, and
𝑂(𝑑𝑝/2)[︂
𝑑
1]︂(︂ 1)︂𝑝
nnz(S)= (log𝑑)2log +log loglog
𝜀𝑝−1𝛿𝑝 𝜀 𝛿 𝜀
for 𝑝>2 such that with probability at least 1−𝛿, for any Xˆ ∈R𝑑×𝑡 such that
‖S(AXˆG−B)‖𝑝 ≤(1+𝜀) min ‖S(AXG−B)‖𝑝 ,
𝑝,𝑝 𝑝,𝑝
X∈R𝑑×𝑡
we have
‖AXˆG−B‖𝑝 ≤(1+𝑂(𝜀)) min ‖AXG−B‖𝑝 .
𝑝,𝑝 𝑝,𝑝
X∈R𝑑×𝑡
Conditioned on the event that ‖S(AX*G−B)‖𝑝 =𝑂(‖AX*G−B‖𝑝 ) for the global optimizer X*, the
𝑝,𝑝 𝑝,𝑝
dependence on 𝛿 can be replaced by a single log1 factor and the poly(loglog1) factor can be removed.
𝛿 𝜀
Furthermore, S can be constructed in 𝑂˜(nnz(A)+𝑑𝜔) time.
We achieve a nearly optimal dependence on 𝑑 and 𝜀, as we show that Ω(𝑑𝑝/2/𝜀𝑝−1) rows are necessary
for weak coresets in Theorem 5.2 for 𝑝>2. Our weak coreset upper bound result together with our strong
coreset lower bound of Theorem 5.1 shows a tight 𝜀 factor separation between the two coreset guarantees.
Note that in the statement of Theorem 1.5, the dependence on the failure rate 𝛿 is polynomial. This
is in fact necessary if we restrict our algorithm to be of the form of “sample-and-solve” algorithms whose
sampling matrices S do not depend on B, as demonstrated in a lower bound result of Theorem 12.8 of
[MMWY22]. The only reason why this dependence becomes necessary in the analysis of the upper bound
is that ‖S(AX*G−B)‖𝑝 may be as large as 𝑂(︀1)︀ ‖AX*G−B‖𝑝 with probability at least 𝛿, and this
𝑝,𝑝 𝛿 𝑝,𝑝
is the source of the hardness result of Theorem 12.8 of [MMWY22] as well. This is a mild problem and
can be easily circumvented in one of two ways. The first is to simply allow the algorithm to incorporate
the row norms of B into the sampling probabilities just as in Theorem 1.4. However, this would not give
an active regression algorithm that makes only polylogarithmic in 𝛿 many queries. If we wish for such an
active regression algorithm, then we can follow [MMWY22] and consider the following two-stage procedure.
First, we can obtain a constant factor solution Xˆ with a polylogarithmic dependence on 𝛿 by employing a
“median”-likeprocedure(seeSection3.1of[MMWY22]). Then, wecanrunlog1 copiesofthealgorithm, each
𝛿
of which succeeds with probability 1−𝛿. Then, we can sort the runs by their estimates ‖S(AXˆG−B)‖𝑝
𝑝,𝑝
anddiscardhalfoftherunswiththehighestvaluesof‖S(AXˆG−B)‖𝑝 . Thisguaranteesthattheremaining
𝑝,𝑝
runs have ‖S(AX*G−B)‖𝑝 =𝑂(1)‖AX*G−B‖𝑝 with probability at least 1−𝛿, which is enough for
𝑝,𝑝 𝑝,𝑝
the rest of the argument to go through with only a polylogarithmic dependence on 𝛿.
1.4 Applications: sublinear algorithms for Euclidean power means
Our first application of our results on coresets for multiple ℓ regression is on designing coresets for the
𝑝
Euclidean power means problem. In this problem, we are given as input a set of 𝑛 points {b }𝑛 ⊆R𝑡, and
𝑖 𝑖=1
we wish to find a center xˆ ∈R𝑡 that minimizes the sum of the Euclidean distances to xˆ, raised to the power
𝑝. That is, we seek to minimize the objective function given by
𝑛
∑︁
‖x−b ‖𝑝 =‖1x⊤−B‖𝑝
𝑖 2 𝑝,2
𝑖=1
5where 1 is the 𝑛×1 matrix of all ones, B ∈ R𝑛×𝑡 is the matrix with b in its 𝑛 rows, and ‖·‖ is the
𝑖 𝑝,2
(𝑝,2)-norm of a matrix given by the ℓ norm of the Euclidean norm of the rows. This is a fundamental
𝑝
problem which generalizes the well-studied problems of the mean (𝑝=2), geometric median (𝑝=1), and
minimum enclosing balls (𝑝=∞). Coresets and sampling algorithms for this problem were recently studied
by [CSS21], who showed that a uniform sample of 𝑂˜(𝜀−(𝑝+3)) points suffices to output a center xˆ ∈R𝑡 such
that
‖1xˆ⊤−B‖𝑝 ≤(1+𝜀)min‖1x⊤−B‖𝑝 =(1+𝜀)OPT𝑝.
𝑝,2 x∈R𝑡 𝑝,2
In comparison to the upper bounds, the lower bounds given by [CSS21] was Ω(𝜀−(𝑝−1)) which is off by an
𝜀4 factor compared to the upper bound, which was improved to Ω(𝜀−1) for 1<𝑝<2 by [MMWY22] and
Ω(𝜀−2) for 𝑝=1 by [CD21, PPP21].
One of the main open questions highlighted by the work of [CSS21] is to obtain tight bounds for this
problem: how many uniform samples are necessary and sufficient to output a (1+𝜀)-approximate solution to
the Euclidean power means problem. Our main contribution is a nearly optimal algorithm which matches the
lower bounds of [CD21, PPP21, CSS21, MMWY22].
Theorem 1.6. Let {b }𝑛 ⊆R𝑑. Then, there is a sublinear algorithm which uniformly samples at most
𝑖 𝑖=1
⎧ 𝑂(𝜀−2)(︀ log1 +log1)︀ log1 𝑝=1
⎪⎨ 𝜀 𝛿 𝛿
𝑠= 𝑂(𝜀−1)(︀ log1 +log1)︀ log1 1<𝑝≤2
𝜀 𝛿 𝛿
⎪⎩𝑂(𝜀1−𝑝)(︀ log1 +log1)︀ log1 2<𝑝<∞
𝜀 𝛿 𝛿
rows b and outputs a center xˆ such that
𝑖
𝑛 𝑛
∑︁ ∑︁
‖xˆ−b ‖𝑝 ≤(1+𝜀) min ‖x−b ‖𝑝
𝑖 2 𝑖 2
x∈R𝑑
𝑖=1 𝑖=1
with probability at least 1−𝛿.
To apply the techniques developed in this work to the Euclidean power means problem, we need to embed
the (𝑝,2)-norm into the entrywise ℓ norm. To make this reduction, we use a classic result of Dvoretzky and
𝑝
Milman [Dvo61, Mil71], which shows that a random subspace of a normed space is approximately Euclidean.
We will need the following version of this result for ℓ norms:
𝑝
Theorem 1.7 (Dvoretzky’s theorem for ℓ norms [FLM77, PVZ17]). Let 𝑝 ≥ 1 and 0 < 𝜀 < 1/𝑝. Let
𝑝
𝑛≥𝑂(max{𝜀−2𝑘,𝜀−1𝑘𝑝/2), and let G∈R𝑛×𝑘 be an i.i.d. random Gaussian matrix. Then, with probability
at least 2/3, ‖Gx‖𝑝 =(1±𝜀)𝑛‖x‖𝑝 for every x∈R𝑘.
𝑝 2
Note then that if G is an appropriately scaled random Gaussian matrix, then we have that
‖1x⊤−B‖𝑝 =(1±𝜀)‖1x⊤G−BG‖𝑝
𝑝,2 𝑝,𝑝
by the above result. We may now note that the latter optimization problem is exactly of the form of an
embedded ℓ regression problem, and thus our weak coreset results immediately apply to this problem. In
𝑝
fact, handling this Dvoretzky embedding is our main motivation for studying the ℓ regression problem with
𝑝
the embedding. We also note that similar reductions are possible by making use of other linear embeddings
between ℓ norms [WW19, LWY21, LLW23]. The full argument is given in Appendix D.1.
𝑝
In addition to sharpening the bound of [CSS21] to optimality, we note that our techniques, both
algorithmically and in the analysis, are simpler than the prior work of [CSS21]. The previous algorithm
required partitioning the dataset into “rings” of points with similar costs and preprocessing these rings.
Furthermore, the analysis uses a specially designed chaining argument with custom net constructions that
require terminal Johnson–Lindenstrauss embeddings. On the other hand, our algorithm simply runs multiple
instancesofa“sample-and-solve”algorithm,wheretherunwithlowestsampledmassiskept. Furthermore,the
analysis largely builds on existing net constructions for ℓ regression, and does not need terminal embeddings.
𝑝
In fact, our proof for the power means problem only need ℓ regression net constructions in 𝑑=1 dimensions
𝑝
due to our use of Dvoretzky’s theorem, which avoids the sophisticated constructions of [BLM89] for large 𝑑
6and only needs a standard volume argument (Remark B.15). Our partition of sensitivity can also be thought
of as a coarse notion of rings, where we only consider two classes of costs, “big” and “small”, whereas prior
work requires finer a classification of points into rings of points whose costs are related up to a constant
factor.
1.5 Applications: spanning coresets for ℓ subspace approximation
𝑝
As a second application of our results, we give the first construction of spanning coresets for ℓ subspace
𝑝
approximation with nearly optimal size. The ℓ subspace approximation is a popular generalization of the
𝑝
classic Frobenius norm low rank approximation problem, where the input is a set of 𝑛 points {a }𝑛 in 𝑑
𝑖 𝑖=1
dimensions, and we wish to compute a rank 𝑘 subspace 𝐹 ⊆R𝑑 that minimizes
𝑛
∑︁
‖a⊤(I −P )‖𝑝
𝑖 𝑑 𝐹 2
𝑖=1
where P denotes the orthogonal projection matrix onto 𝐹. Equivalently, we can write this as
𝐹
min ‖A(I −P )‖𝑝 .
𝑑 𝐹 𝑝,2
rank(𝐹)≤𝑘
While strong and weak coresets for this problem have attracted much attention [FL11, SV12, SW18,
HV20, FKW21, WY23a], our main contribution to this line of research is on a different coreset guarantee,
which we call spanning coresets. Spanning coresets are subsets of the points a which span a (1+𝜀)-optimal
𝑖
rank 𝑘 subspace, and is another popular guarantee in this literature [DV07, SV12, CW15]. In addition to
being an interesting object in its own right [SV12], the existence of small spanning coresets have found
applications to constructions for strong and weak coresets for ℓ subspace approximation [HV20].
𝑝
Definition 1.8 (Spanning coreset). Let {a }𝑛 ⊆R𝑑. A subset 𝑆 ⊆[𝑛] is a (1+𝜀)-spanning coreset if the
𝑖 𝑖=1
points {a } span a 𝑘-dimensional subspace 𝐹ˆ such that
𝑖 𝑖∈𝑆
‖A(I −P )‖𝑝 ≤(1+𝜀) min ‖A(I −P )‖𝑝 .
𝑑 𝐹^ 𝑝,2 𝑑 𝐹 𝑝,2
rank(𝐹)≤𝑘
Our main result is the following upper bound on the size of spanning coresets.
Theorem 1.9. Let {a }𝑛 ⊆R𝑑, 1≤𝑝<∞, 𝑘 ∈N, and 0<𝜀<1. Then, there exists a (1+𝜀)-spanning
𝑖 𝑖=1
coreset 𝑆 of size at most
⎧
𝑂(𝜀−2𝑘)(log(𝑘/𝜀))3 𝑝=1
⎪⎨
|𝑆|= 𝑂(𝜀−1𝑘)(log(𝑘/𝜀))3 1<𝑝≤2
⎪⎩𝑂(𝜀1−𝑝𝑘𝑝/2)(log(𝑘/𝜀))3
2<𝑝<∞
In particular, we improve the previous best result of 𝑂(𝜀−1𝑘2log(𝑘/𝜀)) due to Theorem 3.1 of [SV12] in
the 𝑘 dependence for all 1≤𝑝<4. The proof of this result is given in Section D.2. Furthermore, we give the
first lower bounds on the size of spanning coresets by generalizing an argument of [DV06] for 𝑝=2, showing
that spanning coresets must have size at least Ω(𝜀−1𝑘) in Theorem 5.3. Together, our results settle the size of
spanning coresets up to polylogarithmic factors for 1<𝑝<2. To obtain this result, we again use Dvoretzky’s
theorem to embed the problem to an embedded entrywise ℓ norm problem, and then apply our weak coreset
𝑝
results.
Finally, we note that our spanning coreset lower bound implies other interesting lower bounds for coresets.
First, we note that weak coresets for ℓ subspace approximation are automatically spanning coresets, so our
𝑝
lower bound for spanning coresets also gives the first nontrivial lower bound on the size of weak coresets for
ℓ subspace approximation. Secondly, we note that our proof of Theorem 1.9 in fact shows that any upper
𝑝
bound on weak coresets for ℓ regression with an embedding implies upper bounds for spanning coresets of
𝑝
the same size. Thus, our spanning coreset lower bound in fact implies an Ω(𝑑/𝜀) lower bound on the size of
weak coresets for ℓ regression with an embedding, which establishes that our weak coreset upper bound for
𝑝
ℓ regression (Theorem 1.5) is also nearly optimal for 1<𝑝<2 up to polylogarithmic factors.
𝑝
On the other hand, for 𝑝>2, our weak coreset lower bound of Theorem 5.2 shows that our technique of
reducing spanning coresets to weak coresets cannot prove a better upper bound than the result of Theorem
1.9, andthusnewideasarerequiredtoimproveuponthe𝑂˜(𝜀−1𝑘2)spanningcoresetupperboundofTheorem
3.1 of [SV12]. This is an interesting open problem.
71.6 Open directions
We conclude with several potential directions for future research. One interesting question is to improve our
understanding of upper bounds and lower bounds for coresets for single response ℓ regression.
𝑝
Question 1.10. How many rows are necessary and sufficient for strong and weak coresets for single response
ℓ regression?
𝑝
For strong coresets, this questions is already nearly optimally settled for 𝑝<2 with Θ˜(𝜀−2𝑑) rows known
to be necessary and sufficient [LWW21]. For 𝑝>2, however, there is still a gap in our understanding, with
the best known upper bound being Θ˜(𝜀−2𝑑𝑝/2) via ℓ Lewis weight sampling while the best known lower
𝑝
bound is only Ω(𝜀−1𝑑𝑝/2+𝜀−2𝑑). It is an interesting question to determine whether the lower bound can be
improved to match the ℓ Lewis weight sampling upper bound or not.
𝑝
For weak coresets, the deficiencies are much more glaring. There are currently no known nontrivial lower
bounds for weak coresets, while the best known algorithms are the better of the two upper bounds given by
active ℓ regression and strong coresets, both of which are substantially more restricted settings than weak
𝑝
coresets.
Finally, we highlight the question of obtaining a nearly optimal upper bound on spanning coresets for ℓ
𝑝
subspace approximation for 𝑝>2.
Question 1.11. How many rows are necessary and sufficient for spanning coresets for ℓ subspace approxi-
𝑝
mation?
We conjecture that our lower bound of Ω(𝑘/𝜀) is tight, while the best known upper bound is the better of
our Theorem 1.9 and 𝑂˜(𝜀−1𝑘2) [SV12].
2 Preliminaries
2.1 ℓ Lewis weights
𝑝
Definition 2.1 (One-sided ℓ Lewis weights [JLS22, WY22]). Let A∈R𝑛×𝑑 and 𝑝∈(0,∞). Let 𝛾 ∈(0,1].
𝑝
Then, weights w∈R𝑛 are 𝛾-one-sided ℓ Lewis weights if w ≥𝛾·𝜏 (W1/2−1/𝑝A), where W:=diag(w). If
𝑝 𝑖 𝑖
𝛾 =1, we just say that w are one-sided ℓ Lewis weights.
𝑝
The following theorem collects the results of [CP15, JLS22] on the fastest known algorithms for approxi-
mating one-sided ℓ Lewis weights:
𝑝
Theorem 2.2. Let A∈R𝑛×𝑑 and 𝑝>0. There is an algorithm which computes one-sided ℓ Lewis weights
𝑝
(Def. 2.1) w such that 𝑑≤‖w‖ ≤2𝑑 in 𝑂˜(nnz(A)+𝑑𝜔) time.
1
3 Strong coresets
Theorem 3.1 (Strong coresets for multiple ℓ regression). Let Xˆ ∈R𝑑×𝑚 satisfy
𝑝
‖AXˆ −B‖𝑝 ≤𝑂(1) min ‖AX−B‖𝑝
𝑝,𝑝 𝑝,𝑝
X∈R𝑑×𝑚
and let Bˆ := AXˆ − B. Let S be the ℓ sampling matrix (Definition 1.1) with sampling probabilities
𝑝
𝑞 ≥min{1,w /𝛼+v /𝛽} for 𝛾-one-sided ℓ Lewis weights w∈R𝑛, v =‖e⊤Bˆ‖𝑝/‖Bˆ‖𝑝 ,
𝑖 𝑖 𝑖 𝑝 𝑖 𝑖 𝑝 𝑝,𝑝
⎧ [︂ 1]︂−1
⎪⎪⎪⎨𝑂(𝛾)𝜀2 (log𝑑)2log𝑛+log
𝛿
𝑝<2
𝛼=
⎪⎪⎪⎩𝑂 ‖w(𝛾 ‖𝑝 𝑝/ /2 2) −𝜀 1𝑝[︂ (log𝑑)2log𝑛+log1 𝛿]︂−1
𝑝>2
1
8and 𝛽 =𝑂(𝜀−2log1). Then with probability at least 1−𝛿,
𝛿
‖S(AX−B)‖𝑝 =(1±𝜀)‖AX−B‖𝑝
𝑝,𝑝 𝑝,𝑝
simultaneously for every X∈R𝑑×𝑚.
Our main technical lemma is the following result which generalizes the sampling results of [MMWY22,
WY23a] on preserving differences. The proof can be found in Appendix A.
Theorem 3.2. LetSbetheℓ samplingmatrix(Definition1.1)withsamplingprobabilities 𝑞 ≥min{1,w /𝛼}
𝑝 𝑖 𝑖
for 𝛾-one-sided ℓ Lewis weights w∈R𝑛 and
𝑝
⎧ 𝑂(𝛾)𝜀2[︂ 1]︂−1
⎪⎪⎪⎨
𝜂2/𝑝
(log𝑑)2log𝑛+log
𝛿
𝑝<2
𝛼= .
⎪⎪⎪⎩ 𝜂𝑂 ‖( w𝛾 ‖𝑝/ 𝑝2 /) 2𝜀 −𝑝 1[︂ (log𝑑)2log𝑛+log1 𝛿]︂−1
𝑝>2
1
For each x* ∈R𝑑 and b* =Ax*−b, with probability at least 1−𝛿,
(︂ )︂
⃒ ⃒(︀ ‖S(Ax−b)‖𝑝−‖Sb*‖𝑝)︀ −(︀ ‖Ax−b‖𝑝−‖b*‖𝑝)︀⃒ ⃒≤𝜀 ‖b*‖𝑝+‖Sb*‖𝑝+ 1 ‖Ax−Ax*‖𝑝
𝑝 𝑝 𝑝 𝑝 𝑝 𝑝 𝜂 𝑝
simultaneously for every x∈R𝑑.
Given Theorem 3.2, the proof of Theorem 3.1 proceeds as described in the introduction.
Proof of Theorem 3.1. By replacing B by Bˆ −AXˆ, we assume that ‖B‖ =𝑂(OPT). We apply Theorem 3.2
𝑝
with failure probability at 𝜀𝑝𝛿2. Now let 𝑆 ⊆[𝑚] be the set of columns for which the guarantee of Theorem
3.2 fails. Note then that by Markov’s inequality,
∑︁
‖Be ‖𝑝 =𝑂(𝜀𝑝𝛿)‖B‖𝑝
𝑗 𝑝 𝑝,𝑝
𝑗∈𝑆
with probability at least 1−𝛿. We also have that
∑︁ 1∑︁
‖SBe ‖𝑝 ≤ ‖Be ‖𝑝 =𝑂(𝜀𝑝)‖B‖𝑝
𝑗 𝑝 𝛿 𝑗 𝑝 𝑝,𝑝
𝑗∈𝑆 𝑗∈𝑆
with probability at least 1−𝛿, again by Markov’s inequality. Then,
𝑂(1)
‖S(AX−B)e ‖𝑝 =(1±𝜀)‖SAXe ‖𝑝± ‖SBe ‖𝑝
𝑗 𝑝 𝑗 𝑝 𝜀𝑝−1 𝑗 𝑝
𝑂(1)
=(1±𝜀)2‖AXe ‖𝑝± ‖SBe ‖𝑝
𝑗 𝑝 𝜀𝑝−1 𝑗 𝑝
by using that S is a subspace embedding. Similarly, we have that
𝑂(1)
‖(AX−B)e ‖𝑝 =(1±𝜀)‖AXe ‖𝑝± ‖Be ‖𝑝.
𝑗 𝑝 𝑗 𝑝 𝜀𝑝−1 𝑗 𝑝
Then summing over 𝑗 ∈𝑆 gives that
∑︁ ∑︁
‖S(AX−B)e ‖𝑝 = ‖(AX−B)e ‖𝑝±𝑂(𝜀)‖B‖𝑝 .
𝑗 𝑝 𝑗 𝑝 𝑝,𝑝
𝑗∈𝑆 𝑗∈𝑆
On the other hand, for 𝑗 ∈/ 𝑆, Theorem 3.2 succeeds so we have
‖S(AX−B)e ‖𝑝 =‖(AX−B)e ‖𝑝−‖Be ‖𝑝+‖SBe ‖𝑝±𝜀(︀ ‖Be ‖𝑝+‖SBe ‖𝑝+‖AXe ‖𝑝)︀
𝑗 𝑝 𝑗 𝑝 𝑗 𝑝 𝑗 𝑝 𝑗 𝑝 𝑗 𝑝 𝑗 𝑝
Summing the guarantee over the 𝑚 columns 𝑗 gives
‖S(AX−B)‖𝑝 =‖AX−B‖𝑝 −‖B‖𝑝 +‖SB‖𝑝 ±𝑂(𝜀)(︀ ‖B‖𝑝 +‖AX‖𝑝 )︀
𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝
=‖AX−B‖𝑝 ±𝜀‖B‖𝑝 ±𝑂(𝜀)(︀ ‖B‖𝑝 +‖AX‖𝑝 )︀
𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝
=‖AX−B‖𝑝 ±𝑂(𝜀)‖AX−B‖𝑝 .
𝑝,𝑝 𝑝,𝑝
94 Weak coresets
We sketch the proof of the following result in this section. Full proofs can be found in Appendix C.
Theorem 4.1 (Weak coresets for multiple ℓ regression). Let S be the ℓ sampling matrix (Definition 1.1)
𝑝 𝑝
with sampling probabilities 𝑞 ≥min{1,w /𝛼} for 𝛾-one-sided ℓ Lewis weights w∈R𝑛 and
𝑖 𝑖 𝑝
[︂ 1]︂−1[︂ 1]︂−2
𝛼=𝑂(𝛾)𝜀𝛿2 (log𝑑)2log𝑛+log loglog
𝛿 𝜀
for 𝑝<2 and
𝑂(𝛾𝑝/2)𝜀𝑝−1𝛿𝑝[︂ 1]︂−1[︂ 1]︂−𝑝
𝛼= (log𝑑)2log𝑛+log loglog
‖w‖𝑝/2−1 𝛿 𝜀
1
for 𝑝>2. Then, for any Xˆ ∈R𝑑×𝑡 such that
‖S(AXˆG−B)‖𝑝 ≤(1+𝜀) min ‖S(AXG−B)‖𝑝 ,
𝑝,𝑝 𝑝,𝑝
X∈R𝑑×𝑡
we have
‖AXˆG−B‖𝑝 ≤(1+𝑂(𝜀)) min ‖AXG−B‖𝑝 .
𝑝,𝑝 𝑝,𝑝
X∈R𝑑×𝑡
We first establish lemmas that relate approximation quality to the closeness of solutions to the optimum
in Section 4.1, and we use this in an iterative argument in Section 4.2.
4.1 Closeness of nearly optimal solutions
The following lemma uses strong convexity for 𝑝<2 and a Bregman divergence bound for 𝑝>2 to quantify
the difference between the ℓ norms of two vectors.
𝑝
Lemma 4.2. For any y,y′ ∈R𝑛, we have
𝑝−1
‖y′‖2 ≥‖y‖2−2‖y‖2−𝑝⟨y∘(𝑝−1),y−y′⟩+ ‖y−y′‖2
𝑝 𝑝 𝑝 2 𝑝
if 1<𝑝<2 (Lemma 8.1 of [BMN01]) and
𝑝−1
‖y′‖𝑝 ≥‖y‖𝑝−𝑝⟨y∘(𝑝−1),y−y′⟩+ ‖y−y′‖𝑝
𝑝 𝑝 𝑝2𝑝 𝑝
if 2≤𝑝<∞ (Lemmas 3.2 and 4.6 of [AKPS19]).
We need the following elementary computation.
Lemma 4.3 (Gradients of multiple ℓ regression). The gradient ∇ ‖AXG−B‖𝑝 is given by the formula
𝑝 X 𝑝,𝑝
𝑛 𝑚
∑︁∑︁
𝑝[AXG−B](𝑖,𝑗)∘(𝑝−1)(A⊤e )(e⊤G⊤)
𝑖 𝑗
𝑖=1𝑗=1
The following lemma uses Lemmas 4.2 and 4.3 to show that if X achieves a nearly optimal value, then X
must be close to the optimal solution X*.
Lemma4.4(Closenessofnearlyoptimalsolutions). Let𝑝>1. ForanyX∈R𝑑×𝑡 suchthat‖AXG−B‖ ≤
𝑝,𝑝
(1+𝜂)OPT with 𝜂 ∈(0,1), we have that
{︃
𝑂(𝜂1/2)OPT 𝑝<2
‖AXG−AX*G‖ ≤
𝑝,𝑝 𝑂(𝜂1/𝑝)OPT 𝑝>2
where X* :=argmin ‖AXG−B‖ .
X∈R𝑑×𝑡 𝑝,𝑝
104.2 Iterative size reduction argument
We now sketch the proof of Theorem 4.1.
We will need the following initial result to seed our iterative argument. Note that the dependence on 𝜀 is
suboptimal by an 𝜀 factor for every 1<𝑝<∞.
Lemma 4.5. Let S be the ℓ sampling matrix (Definition 1.1) with sampling probabilities 𝑞 ≥min{1,w /𝛼}
𝑝 𝑖 𝑖
for 𝛾-one-sided ℓ Lewis weights w∈R𝑛 and
𝑝
[︂ 1]︂−1
𝛼=𝑂(𝛾)(𝜀𝛿)2 (log𝑑)2log𝑛+log
𝛿
for 1≤𝑝<2 and
𝑂(𝛾𝑝/2)(𝜀𝛿)𝑝[︂ 1]︂−1
𝛼= (log𝑑)2log𝑛+log
‖w‖𝑝/2−1 𝛿
1
for 2<𝑝<∞. Then, for any Xˆ ∈R𝑑×𝑡 such that
‖S(AXˆG−B)‖𝑝 ≤(1+𝜀) min ‖S(AXG−B)‖𝑝 ,
𝑝,𝑝 𝑝,𝑝
X∈R𝑑×𝑡
we have
‖AXˆG−B‖𝑝 ≤(1+𝑂(𝜀)) min ‖AXG−B‖𝑝 .
𝑝,𝑝 𝑝,𝑝
X∈R𝑑×𝑡
Starting from this initial solution bound of Lemma 4.5, we can proceed via an iterative argument similar
to those of [MMWY22, WY23a] which alternates between using a bound on the closeness of the solution to
the optimal solution to improve the approximation (Theorem 3.2), and using a bound on the approximation
to improve the closeness to the optimum (Lemma 4.4). More specifically, we can show that for 1<𝑝<2, a
bound of 𝐶/𝜀𝛽 on the sample complexity implies that a bound of 𝐶/𝜀2𝛽/(1+𝛽) is sufficient as well. Iterating
this argument starting from 𝛽 =2 due to Lemma 4.5 for 𝑂(loglog1) iterations yields the desired bound of
𝜀
𝐶/𝜀, as claimed. Similarly, for 𝑝>2, a bound of 𝐶/𝜀𝛽 implies a bound of 𝐶/𝜀𝑝𝛽/(1+𝛽), which results in a
final bound of 𝐶/𝜀𝑝−1, as claimed. The full details can be found in Appendix C.
5 Lower bounds
In this section, we complement our various upper bounds with matching lower bounds. In the interest of
space, the proofs are given in Appendix E.
Theorem 5.1. Let 2<𝑝<∞ be fixed. Let 𝜀∈(0,1) be less than some sufficiently small constant. Then, a
strong coreset S for multiple ℓ regression requires nnz(S)=Ω(𝜀−𝑝𝑑𝑝/2) non-zero rows.
𝑝
Theorem 5.2. Let 2<𝑝<∞ be fixed. Let 𝜀∈(0,1) be less than some sufficiently small constant. Then, a
weak coreset S for multiple ℓ regression requires nnz(S)=Ω(𝜀1−𝑝𝑑𝑝/2) non-zero rows.
𝑝
Theorem 5.3. Let 1≤𝑝<∞ and
{︃
1/6 𝑝≤2
𝑐 =
𝑝 1/(6·5𝑝/2−1) 𝑝>2
Let𝑘 ∈N. Then,thereisamatrixB∈R𝑛×(𝑛+1) suchthatforevery𝜀≥𝑘/𝑛andanysubsetof𝑠≤(𝑐 /4)𝜀−1𝑘
𝑝
rows, any rank 𝑘 subspace 𝐹′ spanned by the 𝑠 rows must have
‖BP −B‖𝑝 >(1+𝜀) min ‖BP −B‖𝑝 .
𝐹′ 𝑝,2 𝐹 𝑝,2
rank(𝐹)≤𝑘
Acknowledgements
We thank the anonymous reviewers for useful feedback on improving the presentation of this work. David P.
Woodruff and Taisuke Yasuda were supported by a Simons Investigator Award.
11References
[AKPS19] Deeksha Adil, Rasmus Kyng, Richard Peng, and Sushant Sachdeva. Iterative refinement for
ℓ -norm regression. In Timothy M. Chan, editor, Proceedings of the Thirtieth Annual ACM-
𝑝
SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January
6-9, 2019, pages 1405–1424. SIAM, 2019. 4.2
[BLM89] J. Bourgain, J. Lindenstrauss, and V. Milman. Approximation of zonoids by zonotopes. Acta
Math., 162(1-2):73–141, 1989. 1, 1.4, B.3.2, B.15
[BMN01] Aharon Ben-Tal, Tamar Margalit, and Arkadi Nemirovski. The ordered subsets mirror descent
optimization method with applications to tomography. SIAM J. Optim., 12(1):79–108, 2001. 4.2
[CD21] Xue Chen and Michal Derezinski. Query complexity of least absolute deviation regression via
robust uniform convergence. In Mikhail Belkin and Samory Kpotufe, editors, Conference on
Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA, volume 134 of
Proceedings of Machine Learning Research, pages 1144–1179. PMLR, 2021. 1, 1.4, A
[Cla05] Kenneth L. Clarkson. Subgradient and sampling algorithms for ℓ regression. In Proceedings of
1
the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’05, pages 257–266,
USA, 2005. Society for Industrial and Applied Mathematics. 1
[CNW16] Michael B. Cohen, Jelani Nelson, and David P. Woodruff. Optimal approximate matrix product
in terms of stable rank. In Ioannis Chatzigiannakis, Michael Mitzenmacher, Yuval Rabani,
and Davide Sangiorgi, editors, 43rd International Colloquium on Automata, Languages, and
Programming,ICALP2016,July11-15,2016,Rome,Italy,volume55ofLIPIcs,pages11:1–11:14.
Schloss Dagstuhl - Leibniz-Zentrum fu¨r Informatik, 2016. 1.1.1
[CP15] Michael B. Cohen and Richard Peng. Lp row sampling by lewis weights. In Rocco A. Servedio
and Ronitt Rubinfeld, editors, Proceedings of the Forty-Seventh Annual ACM on Symposium
on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015, pages 183–192.
ACM, 2015. 1, 2.1, A, A
[CP19] XueChenandEricPrice. Activeregressionvialinear-samplesparsification. InAlinaBeygelzimer
andDanielHsu,editors,ConferenceonLearningTheory, COLT2019, 25-28June2019, Phoenix,
AZ, USA, volume 99 of Proceedings of Machine Learning Research, pages 663–695. PMLR, 2019.
1
[CSS21] VincentCohen-Addad,DavidSaulpic,andChrisSchwiegelshohn.Improvedcoresetsandsublinear
algorithms for power means in euclidean spaces. In Marc’Aurelio Ranzato, Alina Beygelzimer,
Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural
Information Processing Systems 34: Annual Conference on Neural Information Processing
Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 21085–21098, Virtual, 2021.
1.4, 1.4
[CW13] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input
sparsity time. In Dan Boneh, Tim Roughgarden, and Joan Feigenbaum, editors, Symposium on
Theory of Computing Conference, STOC’13, Palo Alto, CA, USA, June 1-4, 2013, pages 81–90.
ACM, 2013. 1.1.1
[CW15] Kenneth L. Clarkson and David P. Woodruff. Input sparsity and hardness for robust subspace
approximation. InVenkatesanGuruswami,editor,IEEE56thAnnualSymposiumonFoundations
of Computer Science, FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015, pages 310–329.
IEEE Computer Society, 2015. 1.5
[DDH+09] Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, and Michael W. Mahoney.
Sampling algorithms and coresets for ℓ regression. SIAM J. Comput., 38(5):2060–2078, 2009. 1,
𝑝
1.2.1
12[DMM06a] Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Sampling algorithms for ℓ
2
regression and applications. In Proceedings of the Seventeenth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2006, Miami, Florida, USA, January 22-26, 2006, pages 1127–
1136. ACM Press, 2006. 1
[DMM06b] Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Subspace sampling and relative-
error matrix approximation: Column-row-based methods. In Yossi Azar and Thomas Erlebach,
editors,Algorithms-ESA2006, 14thAnnualEuropeanSymposium, Zurich, Switzerland, Septem-
ber 11-13, 2006, Proceedings, volume 4168 of Lecture Notes in Computer Science, pages 304–314.
Springer, 2006. 1
[DV06] Amit Deshpande and Santosh S. Vempala. Adaptive sampling and fast low-rank matrix
approximation. In Josep D´ıaz, Klaus Jansen, Jos´e D. P. Rolim, and Uri Zwick, editors,
Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques,
9th International Workshop on Approximation Algorithms for Combinatorial Optimization
Problems, APPROX 2006 and 10th International Workshop on Randomization and Computation,
RANDOM 2006, Barcelona, Spain, August 28-30 2006, Proceedings, volume 4110 of Lecture
Notes in Computer Science, pages 292–303. Springer, 2006. 1.5, E.3
[DV07] AmitDeshpandeandKasturiR.Varadarajan. Sampling-baseddimensionreductionforsubspace
approximation. In David S. Johnson and Uriel Feige, editors, Proceedings of the 39th Annual
ACM Symposium on Theory of Computing, San Diego, California, USA, June 11-13, 2007,
pages 641–650. ACM, 2007. 1.5
[Dvo61] Aryeh Dvoretzky. Some results on convex bodies and Banach spaces. In Proc. Internat.
Sympos. Linear Spaces (Jerusalem, 1960), pages 123–160. Jerusalem Academic Press, Jerusalem;
Pergamon, Oxford, 1961. 1.4
[FKW21] Zhili Feng, Praneeth Kacham, and David P. Woodruff. Dimensionality reduction for the sum-of-
distancesmetric. InMarinaMeilaandTongZhang,editors,Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of
Proceedings of Machine Learning Research, pages 3220–3229. PMLR, 2021. 1.5
[FL11] Dan Feldman and Michael Langberg. A unified framework for approximating and clustering
data. In Lance Fortnow and Salil P. Vadhan, editors, Proceedings of the 43rd ACM Symposium
on Theory of Computing, STOC 2011, San Jose, CA, USA, 6-8 June 2011, pages 569–578.
ACM, 2011. 1.5
[FLM77] T. Figiel, J. Lindenstrauss, and V. D. Milman. The dimension of almost spherical sections of
convex bodies. Acta Math., 139(1-2):53–94, 1977. 1.7
[HV20] LingxiaoHuangandNisheethK.Vishnoi. Coresetsforclusteringineuclideanspaces: importance
sampling is nearly optimal. In Konstantin Makarychev, Yury Makarychev, Madhur Tulsiani,
Gautam Kamath, and Julia Chuzhoy, editors, Proccedings of the 52nd Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2020, Chicago, IL, USA, June 22-26, 2020, pages
1416–1429. ACM, 2020. 1.5, D.2
[JLS22] Arun Jambulapati, Yang P. Liu, and Aaron Sidford. Improved iteration complexities for
overconstrained p-norm regression. In Stefano Leonardi and Anupam Gupta, editors, STOC
’22: 54th Annual ACM SIGACT Symposium on Theory of Computing, Rome, Italy, June 20 -
24, 2022, pages 529–542. ACM, 2022. 2.1
[Lew78] D. R. Lewis. Finite dimensional subspaces of 𝐿 . Studia Mathematica, 63(2):207–212, 1978. 1
𝑝
[LLW23] Yi Li, Honghao Lin, and David P. Woodruff. ℓ -regression in the arbitrary partition model
𝑝
of communication. In Gergely Neu and Lorenzo Rosasco, editors, The Thirty Sixth Annual
Conference on Learning Theory, COLT 2023, 12-15 July 2023, Bangalore, India, volume 195 of
Proceedings of Machine Learning Research, pages 4902–4928. PMLR, 2023. 1.4
13[LT91] Michel Ledoux and Michel Talagrand. Probability in Banach spaces. Classics in Mathematics.
Springer-Verlag, Berlin, 1991. Isoperimetry and processes, Reprint of the 1991 edition. 1, B.8
[LWW21] Yi Li, Ruosong Wang, and David P. Woodruff. Tight bounds for the subspace sketch problem
with applications. SIAM J. Comput., 50(4):1287–1335, 2021. 1.2, 1.6
[LWY21] YiLi,DavidP.Woodruff,andTaisukeYasuda. Exponentiallyimproveddimensionalityreduction
for ℓ : Subspace embeddings and independence testing. In Mikhail Belkin and Samory Kpotufe,
1
editors, Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado,
USA, volume 134 of Proceedings of Machine Learning Research, pages 3111–3195. PMLR, 2021.
1.4
[Mil71] V. D. Milman. A new proof of A. Dvoretzky’s theorem on cross-sections of convex bodies.
Funkcional. Anal. i Priloˇzen., 5(4):28–37, 1971. 1.4
[MMR19] Konstantin Makarychev, Yury Makarychev, and Ilya P. Razenshteyn. Performance of johnson-
lindenstrauss transform for k-means and k-medians clustering. In Moses Charikar and Edith
Cohen, editors, Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of
Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019, pages 1027–1038. ACM, 2019.
B.2
[MMWY22] Cameron Musco, Christopher Musco, David P. Woodruff, and Taisuke Yasuda. Active linear
regression for ℓ norms and beyond. In 63rd IEEE Annual Symposium on Foundations of
𝑝
Computer Science, FOCS 2022, Denver, CO, USA, October 31 - November 3, 2022, pages
744–753. IEEE, 2022. 1, 1, 1.2.1, 1.3, 1.4, 3, 4.2, D.1
[PPP21] AdityaParulekar,AdvaitParulekar,andEricPrice.L1regressionwithLewisweightssubsampling.
InMaryWoottersandLauraSanita`,editors,Approximation, Randomization, and Combinatorial
Optimization. Algorithms and Techniques, APPROX/RANDOM 2021, August 16-18, 2021,
University of Washington, Seattle, Washington, USA (Virtual Conference), volume 207 of
LIPIcs, pages 49:1–49:21. Schloss Dagstuhl - Leibniz-Zentrum fu¨r Informatik, 2021. 1, 1.4
[PTB13] Udaya Parampalli, Xiaohu Tang, and Serdar Boztas. On the construction of binary sequence
families with low correlation and large sizes. IEEE Trans. Inf. Theory, 59(2):1082–1089, 2013.
E.1
[PVZ17] Grigoris Paouris, Petros Valettas, and Joel Zinn. Random version of Dvoretzky’s theorem in ℓ𝑛.
𝑝
Stochastic Process. Appl., 127(10):3187–3227, 2017. 1.7
[SV12] Nariankadu D. Shyamalkumar and Kasturi R. Varadarajan. Efficient subspace approximation
algorithms. Discret. Comput. Geom., 47(1):44–63, 2012. 1.5, 1.5, 1.6
[SW18] Christian Sohler and David P. Woodruff. Strong coresets for k-median and subspace approx-
imation: Goodbye dimension. In Mikkel Thorup, editor, 59th IEEE Annual Symposium on
Foundations of Computer Science, FOCS 2018, Paris, France, October 7-9, 2018, pages 802–813.
IEEE Computer Society, 2018. 1.5
[SZ01] Gideon Schechtman and Artem Zvavitch. Embedding subspaces of 𝑙 into 𝑙𝑛, 0 < 𝑝 < 1.
𝑝 𝑝
Mathematische Nachrichten, 227(1):133–142, 2001. 1
[Tal90] Michel Talagrand. Embedding subspaces of 𝐿 into 𝑙𝑁. Proc. Amer. Math. Soc., 108(2):363–369,
1 1
1990. 1
[Tal95] MichelTalagrand. Embeddingsubspacesof𝐿 in𝑙𝑁. InGeometric aspects of functional analysis
𝑝 𝑝
(Israel, 1992–1994), volume 77 of Oper. Theory Adv. Appl., pages 311–325. Birkh¨auser, Basel,
1995. 1
[Ver18] Roman Vershynin. High-dimensional probability, volume 47 of Cambridge Series in Statistical
and Probabilistic Mathematics. Cambridge University Press, Cambridge, 2018. B.9
14[WW19] Ruosong Wang and David P. Woodruff. Tight bounds for ℓ oblivious subspace embeddings.
𝑝
In Timothy M. Chan, editor, Proceedings of the Thirtieth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019, pages
1825–1843. SIAM, 2019. 1.4
[WY22] David P. Woodruff and Taisuke Yasuda. High-dimensional geometric streaming in polynomial
space. In 63rd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2022,
Denver, CO, USA, October 31 - November 3, 2022, pages 732–743. IEEE, 2022. 2.1
[WY23a] David P. Woodruff and Taisuke Yasuda. New subset selection algorithms for low rank approxi-
mation: Offline and online. In Barna Saha and Rocco A. Servedio, editors, Proceedings of the
55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June
20-23, 2023, pages 1802–1813. ACM, 2023. 1, 1.5, 3, 4.2
[WY23b] David P. Woodruff and Taisuke Yasuda. Online Lewis weight sampling. In Nikhil Bansal and
Viswanath Nagarajan, editors, Proceedings of the 2023 ACM-SIAM Symposium on Discrete
Algorithms, SODA 2023, Florence, Italy, January 22-25, 2023, pages 4622–4666. SIAM, 2023. 1,
A
[WY23c] David P. Woodruff and Taisuke Yasuda. Sharper bounds for ℓ sensitivity sampling. In
𝑝
Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and
Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29
July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research,
pages 37238–37272. PMLR, 2023. B.1.2, B.3.2
A ℓ Lewis weight sampling for differences
𝑝
Throughout this section, we fix the following notation:
Definition A.1.
• Let 1≤𝑝<∞.
• Let 𝜀∈(0,1) be an accuracy parameter and let 𝛿 ∈(0,1) be a failure probability parameter.
• Let A∈R𝑛×𝑑 and b∈R𝑛.
• Let w∈R𝑛 be 𝛾-one-sided ℓ Lewis weights for A such that max𝑛 w ≤𝑤.
𝑝 𝑖=1 𝑖
• Let x* ∈ R𝑑 any center, let 𝜂 ∈ (0,1) be a proximity parameter, and let 𝑅 ≥ ‖Ax*−b‖𝑝 be a scale
𝑝
parameter.
• For each 𝑖∈[𝑛] and x∈R𝑑, let
∆ (x):=|[Ax−b](𝑖)|𝑝−|[Ax*−b](𝑖)|𝑝
𝑖
Our main result of the section is the following:
TheoremA.2. LetSbetheℓ samplingmatrix(Definition1.1)withsamplingprobabilities𝑞 ≥min{1,w /𝛼}
𝑝 𝑖 𝑖
for 𝛾-one-sided ℓ Lewis weights w∈R𝑛 and
𝑝
⎧ 𝜀2 [︂ 1]︂−1
⎪⎪⎪⎨𝑂(𝛾)
𝜂2/𝑝
(log𝑑)2log𝑛+log
𝛿
𝑝<2
𝛼= .
⎪⎪⎪⎩𝑂(𝛾𝑝/2) 𝜂‖w𝜀 ‖𝑝 𝑝/2−1[︂ (log𝑑)2log𝑛+log1 𝛿]︂−1
𝑝>2
1
Then for each x* ∈R𝑑 and 𝑅≥‖Ax*−b‖𝑝, with probability at least 1−𝛿,
𝑝
sup ⃒ ⃒(︀ ‖S(Ax−b)‖𝑝−‖S(Ax*−b)‖𝑝)︀ −(︀ ‖Ax−b‖𝑝−‖Ax*−b‖𝑝)︀⃒ ⃒≤𝜀(𝑅+‖S(Ax*−b)‖𝑝)
𝑝 𝑝 𝑝 𝑝 𝑝
‖Ax−Ax*‖𝑝 𝑝≤𝜂𝑅
15We will prove Theorem A.2 throughout this section. Before doing so, we state the following more
convenient form of the result:
Theorem 3.2. LetSbetheℓ samplingmatrix(Definition1.1)withsamplingprobabilities 𝑞 ≥min{1,w /𝛼}
𝑝 𝑖 𝑖
for 𝛾-one-sided ℓ Lewis weights w∈R𝑛 and
𝑝
⎧ 𝑂(𝛾)𝜀2[︂ 1]︂−1
⎪⎪⎪⎨
𝜂2/𝑝
(log𝑑)2log𝑛+log
𝛿
𝑝<2
𝛼= .
⎪⎪⎪⎩ 𝜂𝑂 ‖( w𝛾 ‖𝑝/ 𝑝2 /) 2𝜀 −𝑝 1[︂ (log𝑑)2log𝑛+log1 𝛿]︂−1
𝑝>2
1
For each x* ∈R𝑑 and b* =Ax*−b, with probability at least 1−𝛿,
(︂ )︂
⃒ ⃒(︀ ‖S(Ax−b)‖𝑝−‖Sb*‖𝑝)︀ −(︀ ‖Ax−b‖𝑝−‖b*‖𝑝)︀⃒ ⃒≤𝜀 ‖b*‖𝑝+‖Sb*‖𝑝+ 1 ‖Ax−Ax*‖𝑝
𝑝 𝑝 𝑝 𝑝 𝑝 𝑝 𝜂 𝑝
simultaneously for every x∈R𝑑.
Proof. We apply Theorem A.2 with 𝛿 set to 𝛿/𝐿 for 𝐿=𝑂(log(1/𝛿𝜀)) and 𝑅 set to 2𝑙‖Ax*−b‖𝑝 for 𝑙∈[𝐿].
𝑝
By a union bound, the conclusion holds simultaneously for every 𝑙 ∈ [𝐿] with probability at least 1−𝛿.
Furthermore, by Markov’s inequality, ‖S(Ax*−b)‖𝑝 =𝑂(1/𝛿)‖Ax*−b‖𝑝 with probability at least 1−𝛿.
𝑝 𝑝
If ‖Ax−Ax*‖𝑝 ≤ 2𝐿‖Ax*−b‖𝑝 = poly(1/𝛿𝜀)‖Ax*−b‖𝑝, then the result follows immediately from
𝑝 𝑝 𝑝
applying the conclusion of Theorem A.2 at the appropriate scale 𝑙 ∈[𝐿]. Otherwise, we have that ‖Ax−
Ax*‖𝑝 ≥poly(1/𝛿𝜀)‖Ax*−b‖𝑝, in which case
𝑝 𝑝
‖S(Ax−Ax*)‖𝑝 ≥Ω(1)‖Ax−Ax*‖𝑝 ≥poly(1/𝛿𝜀)‖Ax*−b‖𝑝
𝑝 𝑝 𝑝
so
(1+𝜀)𝑝−1
‖S(Ax−b)‖𝑝−‖S(Ax*−b)‖𝑝 =(1±𝜀)‖S(Ax−Ax*)‖𝑝± ‖S(Ax*−b)‖𝑝
𝑝 𝑝 𝑝 𝜀𝑝−1 𝑝
(1+𝜀)𝑝−1
=(1±𝜀)‖S(Ax−Ax*)‖𝑝± ‖Ax*−b‖𝑝
𝑝 𝛿𝜀𝑝−1 𝑝
=(1±𝑂(𝜀))‖S(Ax−Ax*)‖𝑝
𝑝
and similarly,
‖Ax−b‖𝑝−‖Ax*−b‖𝑝 =(1±𝑂(𝜀))‖Ax−Ax*‖𝑝.
𝑝 𝑝 𝑝
Thus it suffices to have that
⃒ ⃒ 𝜀
⃒‖S(Ax−Ax*)‖𝑝−‖Ax−Ax*‖𝑝⃒≤ ‖Ax−Ax*‖𝑝.
⃒ 𝑝 𝑝⃒ 𝜂 𝑝
In fact, standard ℓ Lewis weight sampling guarantees give
𝑝
⎧ 𝜀
⃒ ⃒
⎪⎪⎨𝜂1/𝑝‖Ax−Ax*‖𝑝
𝑝
𝑝<2
⃒‖S(Ax−Ax*)‖𝑝−‖Ax−Ax*‖𝑝⃒≤
⃒ 𝑝 𝑝⃒ 𝜀𝑝/2
⎪⎪⎩ 𝜂1/2‖Ax−Ax*‖𝑝
𝑝
𝑝>2
which is stronger.
Throughout our proof of Theorem A.2, we will assume without loss of generality that S𝑝 >1, that is we
𝑖,𝑖
only consider rows that are sampled with probability 𝑞 <1, since rows that are kept with probability 𝑞 =1
𝑖 𝑖
do not contribute towards the sampling error. Note first that we can write
⃒ ⃒
⃒ ⃒(︀ ‖S(Ax−b)‖𝑝 𝑝−‖S(Ax*−b)‖𝑝 𝑝)︀ −(︀ ‖Ax−b‖𝑝 𝑝−‖Ax*−b‖𝑝 𝑝)︀⃒ ⃒=⃒ ⃒ ⃒∑︁𝑛 (S𝑝 𝑖,𝑖−1)∆ 𝑖(x)⃒ ⃒ ⃒.
⃒ ⃒
𝑖=1
Thesupremumofthisquantity,normalizedby(𝑅+‖S(Ax*−b)‖𝑝)𝑙,over{︀ ‖Ax−Ax*‖𝑝 ≤𝜂𝑅}︀ isarandom
𝑝 𝑝
variable. We will bound the 𝑙-th moment of this random variable for 𝑙=𝑂(log1 +log𝑛).
𝛿
We start with a standard symmetrization procedure (see, e.g., [CP15, CD21]).
16Lemma A.3 (Symmetrization).
⎡
1
⃒
⃒∑︁𝑛
⃒ ⃒𝑙⎤
E S⎣ (𝑅+‖S(Ax*−b)‖𝑝 𝑝)𝑙 ‖Ax−Asu xp
*‖𝑝
𝑝≤𝜂𝑅⃒ ⃒
⃒
𝑖=1(S𝑝 𝑖,𝑖−1)∆ 𝑖(x)⃒ ⃒
⃒
⎦
⎡
1
⃒
⃒∑︁𝑛
⃒ ⃒𝑙⎤
≤ 2𝑙 𝜀∼{±E 1}𝑛,S⎣ (𝑅+‖S(Ax*−b)‖𝑝 𝑝)𝑙 ‖Ax−Asu xp
*‖𝑝
𝑝≤𝜂𝑅⃒ ⃒
⃒
𝑖=1𝜀 𝑖S𝑝 𝑖,𝑖∆ 𝑖(x)⃒ ⃒
⃒
⎦
Next, we replace the Rademacher process on the right hand side of Lemma A.3 by one which “removes”
S𝑝 , that is, one of the form
𝑖,𝑖
⎡ ⃒
⃒∑︁𝑛
⃒ ⃒𝑙⎤
E ⎣ sup ⃒ ⃒ 𝜀 𝑖∆ 𝑖(x)⃒ ⃒ ⎦. (3)
𝜀∼{±1}𝑛 ‖Ax−Ax*‖𝑝 𝑝≤𝜂𝑅⃒
𝑖=1
⃒
ThisisroughlydonebynotingthatifwetakeSAtobea“partof”A,thenthedomain{︀ ‖Ax−Ax*‖𝑝 ≤𝜂𝑅}︀
𝑝
only dilates by a constant factor as S preserves ℓ norms in the column space of A. More formally, we have
𝑝
the following lemma:
Lemma A.4. Let B∈R𝑚×𝑑 satisfy ‖Bx‖𝑝 ≤𝐶‖Ax‖𝑝 for every x∈R𝑑. For every fixing of S, let
𝑝 𝑝
(︃ )︃
SA
B :=
S
B
be the concatenation of SA and B, and let
𝐹
S
= sup ⃒ ⃒‖SAx‖𝑝 𝑝−‖Ax‖𝑝 𝑝⃒ ⃒.
‖Ax‖𝑝 𝑝≤1
Suppose that for every fixing of S and 𝑅′ ≥𝑅+‖S(Ax*−b)‖𝑝, we have that
𝑝
⃒ ⃒
⃒∑︁𝑛 ⃒
E sup ⃒ 𝜀 S𝑝 ∆ (x)⃒≤𝜀𝑙𝛿𝑅′𝑙
⃒ 𝑖 𝑖,𝑖 𝑖 ⃒
𝜀∼{±1}𝑛‖BSx−BSx*‖𝑝 𝑝≤𝜂𝑅′⃒
𝑖=1
⃒
Then,
1
⃒ ⃒∑︁𝑛 ⃒ ⃒𝑙
(︁ )︁
E E sup ⃒ 𝜀 S𝑝 ∆ (x)⃒ ≤(2𝜀)𝑙𝛿 (1+𝐶)𝑙+E[𝐹𝑙]
S (𝑅+‖S(Ax*−b)‖𝑝 𝑝)𝑙 𝜀∼{±1}𝑛‖Ax−Ax*‖𝑝 𝑝≤𝜂𝑅⃒ ⃒
𝑖=1
𝑖 𝑖,𝑖 𝑖 ⃒ ⃒ S S
Proof. Note that
‖B (x−x*)‖𝑝 =‖SA(x−x*)‖𝑝+‖B(x−x*)‖𝑝 ≤(1+𝐹 +𝐶)‖A(x−x*)‖𝑝
S 𝑝 𝑝 𝑝 S 𝑝
so
⃒ ⃒∑︁𝑛 ⃒ ⃒𝑙 ⃒ ⃒∑︁𝑛 ⃒ ⃒𝑙
E sup ⃒ 𝜀 S𝑝 ∆ (x)⃒ ≤ E sup ⃒ 𝜀 S𝑝 ∆ (x)⃒
⃒ 𝑖 𝑖,𝑖 𝑖 ⃒ ⃒ 𝑖 𝑖,𝑖 𝑖 ⃒
𝜀∼{±1}𝑛‖Ax−Ax*‖𝑝 𝑝≤𝜂𝑅⃒
𝑖=1
⃒ 𝜀∼{±1}𝑛‖BSx−BSx*‖𝑝 𝑝≤(1+𝐹S+𝐶)𝜂𝑅⃒
𝑖=1
⃒
≤𝜀𝑙𝛿(1+𝐹 +𝐶)𝑙(𝑅+‖S(Ax*−b)‖𝑝)𝑙
S 𝑝
≤𝜀𝑙𝛿2𝑙−1((1+𝐶)𝑙+𝐹𝑙)(𝑅+‖S(Ax*−b)‖𝑝)𝑙 Fact B.1
S 𝑝
Taking expectations on both sides proves the lemma.
NotethatifSistheℓ Lewisweightsamplingmatrix,thenE[|𝐹 |𝑙]inLemmaA.4isknowntobebounded
𝑝 S
by 𝑂(1)𝑙 (that is, S is an 𝑂(1)-approximate ℓ subspace embedding) by standard results on ℓ Lewis weight
𝑝 𝑝
sampling [CP15, WY23b].
17Furthermore, we can design B such that the ℓ Lewis weights of B are uniformly bounded by 𝛼, where
𝑝 S
𝛼 is the oversampling parameter such that S samples the 𝑖th row with probability min{1,w /𝛼}. For 𝑝<2,
𝑖
this simply follows by taking B to be a flattening of A where every row is duplicated 1/𝛼 times due to the
monotonicity of ℓ Lewis weights [CP15]. For 𝑝 > 2, monotonicity of ℓ Lewis weights does not hold, but
𝑝 𝑝
Theorem 5.2 of [WY23b] nonetheless shows that 𝛾-one-sided ℓ Lewis weights can be constructed for B
𝑝 S
with 𝛾 =Ω(1) that makes a similar argument go through.
Finally, it remains to bound the Rademacher process of the form of (3), where A has 𝛾-one-sided ℓ Lewis
𝑝
weights uniformly bounded by 𝑤 = 𝛼. We will prove the following in Section B. Assuming this theorem,
Theorem A.2 follows by setting 𝑤 =𝛼 as stated.
Theorem A.5. For all 𝑙∈N, we have
E sup
⃒ ⃒ ⃒∑︁𝑛
𝜀 ∆
(x)⃒ ⃒ ⃒𝑙
≤(𝜀𝑅)𝑙 (4)
⃒ 𝑖 𝑖 ⃒
𝜀∼{±1}𝑛‖Ax−Ax*‖𝑝 𝑝≤𝜂𝑅⃒
𝑖=1
⃒
where
⎧ ⎪⎨𝑂(𝑤𝜂2/𝑝)1/2𝛾−1/2[︁(︀ (log𝑑)2log𝑛)︀1+1/𝑙 +𝑙]︁1/2
𝑝<2
𝜀= .
⎪⎩𝑂(𝑤𝜂‖w‖𝑝/2−1)1/𝑝𝛾−1/2[︁(︀ (log𝑑)2log𝑛)︀1+1/𝑙 +𝑙]︁1/𝑝
𝑝>2
1
B Rademacher process bounds
We continue to fix our notation from Definition A.1. We will prove Theorem A.5 in this section.
We split the sum in (4) into two parts: the part that is bounded by the 𝛾-one-sided Lewis weights of A,
and the part that is not. To this end, define a threshold
⎧ 𝜂
𝑝<2
⎪⎪⎨𝛾𝑝/2𝜀𝑝
𝜏 :=
𝜂‖w‖𝑝/2−1
⎪⎪⎩ 1 𝑝>2
𝛾𝑝/2𝜀𝑝
where 𝜀 will be determined later, and define the set of “good” entries 𝐺⊆[𝑛] as
𝐺:={𝑖∈[𝑛]:|[Ax*−b](𝑖)|≤𝜏w 𝑅} (5)
𝑖
We then bound
⃒ ⃒∑︁𝑛 ⃒ ⃒𝑙 ⃒ ⃒∑︁ ⃒ ⃒𝑙
E sup ⃒ 𝜀 ∆ (x)⃒ ≤2𝑙−1 E sup ⃒ 𝜀 ∆ (x)⃒
⃒ 𝑖 𝑖 ⃒ ⃒ 𝑖 𝑖 ⃒
𝜀∼{±1}𝑛‖Ax−Ax*‖𝑝 𝑝≤𝜂𝑅⃒
𝑖=1
⃒ 𝜀∼{±1}𝑛‖Ax−Ax*‖𝑝 𝑝≤𝜂𝑅⃒
𝑖∈𝐺
⃒
⃒ ⃒𝑙
⃒ ⃒
+2𝑙−1 E sup ⃒ ⃒ ∑︁ 𝜀 𝑖∆ 𝑖(x)⃒ ⃒
𝜀∼{±1}𝑛‖Ax−Ax*‖𝑝 𝑝≤𝜂𝑅⃒
⃒𝑖∈[𝑛]∖𝐺
⃒
⃒
using the Fact B.1, and separately estimate each term. We can think of the first term as the “sensitivity”
term, where each term in the sum is bounded by the Lewis weights of A, and the latter term as the “outlier”
term, where each term in the sum is much larger than the corresponding Lewis weights.
B.1 Preliminaries
We repeatedly use the following inequalities.
Fact B.1. For any 𝑝≥1 and any 𝑎,𝑏∈R, |𝑎+𝑏|𝑝 ≤2𝑝−1(|𝑎|𝑝+|𝑏|𝑝)=𝑂(|𝑎|𝑝+|𝑏|𝑝).
Fact B.2 (Corollary A.2, [MMR19]). For any 𝑝 ≥ 1, 𝜀 > 0, and any 𝑎,𝑏 ∈ R, |𝑎+𝑏|𝑝 ≤ (1+𝜀)|𝑎|𝑝 +
(1+𝜀)𝑝−1 |𝑏|𝑝.
𝜀𝑝−1
18Fact B.3. For any 𝑝≥1 and any 𝑎,𝑏∈R, |𝑎|𝑝−|𝑏|𝑝 ≤𝑝|𝑎−𝑏|(|𝑎|𝑝−1+|𝑏|𝑝−1).
We will need the notion of weighted ℓ norms ‖·‖ :
𝑝 w,𝑝
Definition B.4. Let w∈R𝑛 be non-negative weights. Then for y∈R𝑛, we define
(︃ 𝑛 )︃1/𝑝
∑︁
‖y‖ := w |y(𝑖)|𝑝 .
w,𝑝 𝑖
𝑖=1
B.1.1 ℓ Lewis weights
𝑝
Lemma B.5 (One-sided Lewis weights bound sensitivities). Let A∈R𝑛×𝑑 and 0<𝑝<∞. Let w∈R𝑛 be
𝛾-one-sided ℓ Lewis weights. Then,
𝑝
{︃
|[Ax](𝑖)|𝑝 𝛾−𝑝/2‖w‖𝑝/2−1·w 𝑝>2
sup ≤ 1 𝑖
‖Ax‖𝑝 𝛾−1·w 𝑝<2
x∈rowspan(A)∖{0} 𝑝 𝑖
Lemma B.6. Let A∈R𝑛×𝑑 and let w be 𝛾-one-sided ℓ Lewis weights for A. Then,
𝑝
{︃
⃦ ⃦ ‖w‖1/2−1/𝑝‖Ax‖ 𝑝>2
⃦W1/2−1/𝑝Ax⃦ ≤ 1 𝑝
⃦ ⃦ 2 𝛾1/2−1/𝑝‖Ax‖ 𝑝 𝑝<2
Lemma B.7. Let A∈R𝑛×𝑑 and let 0<𝑝<∞. The following hold: Let w∈R𝑛 be 𝛾-one-sided ℓ Lewis
𝑝
weights, and let R be a change of basis matrix R such that W1/2−1/𝑝AR is an orthonormal matrix. Then,
for each 𝑖∈[𝑛],
w
𝑖
≥𝛾𝑝/2·⃦ ⃦e⊤
𝑖
AR⃦ ⃦𝑝 2.
B.1.2 Gaussian processes
Theorem B.8 (Gaussian comparison, Equation 4.8, [LT91]). Let 𝐹 :R →R be convex and let {x }𝑛 be
+ + 𝑖 𝑖=1
a finite sequence in a Banach space. Then,
(︃⃦ ⃦∑︁𝑛 ⃦ ⃦)︃ (︃ (︁𝜋)︁1/2⃦ ⃦∑︁𝑛 ⃦ ⃦)︃
E 𝐹 ⃦ 𝜀 x ⃦ ≤ E 𝐹 ⃦ g x ⃦ .
𝜀∼{±1}𝑛 ⃦ ⃦
𝑖=1
𝑖 𝑖⃦ ⃦ g∼𝒩(0,I𝑛) 2 ⃦ ⃦
𝑖=1
𝑖 𝑖⃦ ⃦
Theorem B.9 (Dudley’s entropy integral, Theorem 8.1.6, [Ver18]). Let (𝑋 ) be a Gaussian process with
𝑡 𝑡∈𝑇
pseudo-metric 𝑑 (𝑠,𝑡):=‖𝑋 −𝑋 ‖ . Let 𝐸(𝑇,𝑑 ,𝑢) denote the minimal number of 𝑑 -balls of radius 𝑢
𝑋 𝑠 𝑡 2 𝑋 𝑋
required to cover 𝑇. Then, for every 𝑧 ≥0, we have that
{︂ [︂∫︁ ∞ ]︂}︂
√︀
Pr sup|𝑋 −𝑋 |≥𝐶 log𝐸(𝑇,𝑑 ,𝑢) 𝑑𝑢+𝑧·diam(𝑇) ≤2exp(−𝑧2)
𝑠 𝑡 𝑋
𝑠,𝑡∈𝑇 0
Integrating the tail bound gives moment bounds. The following is taken from Lemma 6.8 of [WY23c].
Lemma B.10 (Moment bounds). Let Λ be a Gaussian process with domain 𝑇 and distance 𝑑 . Let
𝑋
ℰ :=∫︀∞√︀ log𝐸(𝑇,𝑑 ,𝑢) 𝑑𝑢 and 𝒟 =diam(𝑇). Then, for 𝑙∈N,
0 𝑋
√
E [|Λ|𝑙]≤(2ℰ)𝑙(ℰ/𝒟)+𝑂( 𝑙𝒟)𝑙
g∼𝒩(0,I𝑛)
B.2 Estimates on the outlier term
We first bound the outlier terms (𝑖∈/ 𝐺), which is much easier.
Lemma B.11. With probability 1, we have that
∑︁
sup |∆ (x)|≤𝑂(𝜀)𝑅.
𝑖
‖Ax−Ax*‖𝑝≤𝜂𝑅
𝑝 𝑖∈[𝑛]∖𝐺
19Proof. For each 𝑖∈[𝑛]∖𝐺, we have that
|[Ax−b](𝑖)|∈|[Ax*−b](𝑖)|±|[Ax*−Ax](𝑖)|
∈|[Ax*−b](𝑖)|±𝛾−1/2‖w‖1/2−1/𝑝w1/𝑝‖Ax*−Ax‖ Lemma B.5
1 𝑖 𝑝
∈|[Ax*−b](𝑖)|±𝛾−1/2𝜂1/𝑝‖w‖1/2−1/𝑝w1/𝑝𝑅1/𝑝
1 𝑖
∈|[Ax*−b](𝑖)|±𝜀|[Ax*−b](𝑖)| 𝑖∈[𝑛]∖𝐺
Thus,
|∆ (x)|≤𝑂(𝜀)|[Ax*−b](𝑖)|𝑝
𝑖
so
𝑛
∑︁ |∆ (x)|≤∑︁ 𝑂(𝜀)|[Ax*−b](𝑖)|𝑝 =𝑂(𝜀)‖Ax*−b‖𝑝 ≤𝑂(𝜀)𝑅.
𝑖 𝑝
𝑖∈[𝑛]∖𝐺 𝑖=1
B.3 Estimates on the sensitivity term
Next, we estimate the sensitivity term (𝑖∈𝐺),
⃒ ⃒𝑙
⃒∑︁ ⃒
E sup ⃒ 𝜀 ∆ (x)⃒ .
⃒ 𝑖 𝑖 ⃒
𝜀∼{±1}𝑛‖Ax−Ax*‖𝑝≤𝜂𝑅⃒ ⃒
𝑝 𝑖∈𝐺
To estimate this moment, we obtain a subgaussian tail bound via the tail form of Dudley’s entropy integral,
and then integrate it. We will crucially use that |∆ (x)| for 𝑖∈𝐺 is bounded over all ‖Ax−Ax*‖𝑝 ≤𝜂𝑅,
𝑖 𝑝
which gives the following sensitivity bound:
Lemma B.12. For all 𝑖∈𝐺, and x∈R𝑑 with ‖Ax−Ax*‖𝑝 ≤𝜂𝑅, we have |[Ax−b](𝑖)|𝑝 ≤𝑂(𝜏w 𝑅) and
𝑝 𝑖
|∆ (x)|≤𝑂(𝜏w 𝑅).
𝑖 𝑖
Proof. We have
|[Ax−b](𝑖)|𝑝 ≤2𝑝−1(|[Ax*−b](𝑖)|𝑝+|[Ax−Ax*](𝑖)|𝑝) Fact B.1
≤2𝑝−1𝜏w 𝑅+2𝑝−1𝛾−𝑝/2𝜂‖w‖0∨(𝑝/2−1)w 𝑅 𝑖∈𝐺 (see (5)) and Lemma B.5
𝑖 1 𝑖
≤𝑂(𝜏w 𝑅)
𝑖
The bound on ∆ (x) follows easily from the above calculation.
𝑖
B.3.1 Bounding low-sensitivity entries
We now separately handle entries 𝑖∈𝐺 with small Lewis weight. To do this end, define
{︁ 𝜀 }︁
𝐽 := 𝑖∈𝐺:w ≥ .
𝑖 𝜏𝑛
We then bound the mass on the complement of 𝐽:
Lemma B.13. For all ‖Ax−Ax*‖𝑝 ≤𝜂𝑅, we have that
𝑝
∑︁
|∆ (x)|≤𝑂(𝜀𝑅)
𝑖
𝑖∈[𝑛]∖𝐽
Proof. We have that for each 𝑖∈[𝑛]∖𝐽, w ≤𝜀/𝜏𝑛 so by Lemma B.12,
𝑖
∑︁ ∑︁ ∑︁ 𝑂(𝜀)
|∆ (x)|≤ 𝑂(𝜏w 𝑅)≤ 𝑅≤𝑂(𝜀𝑅)
𝑖 𝑖 𝑛
𝑖∈[𝑛]∖𝐽 𝑖∈[𝑛]∖𝐽 𝑖∈[𝑛]∖𝐽
20B.3.2 Bounding high-sensitivity entries: Gaussian processes
Finally, it remains to bound the Rademacher process only on the entries indexed by 𝑖∈𝐽. By a Gaussian
comparison theorem (Theorem B.8), we may bound the Rademacher process above by a Gaussian process
instead, that is,
⃒ ⃒𝑙 ⃒ ⃒𝑙
⃒∑︁ ⃒ (︁𝜋)︁𝑙/2 ⃒∑︁ ⃒
E sup ⃒ 𝜀 ∆ (x)⃒ ≤ E sup ⃒ g ∆ (x)⃒ . (6)
𝜀∼{±1}𝑛‖Ax−Ax*‖𝑝 𝑝≤𝜂𝑅⃒ ⃒
𝑖∈𝐽
𝑖 𝑖 ⃒ ⃒ 2 g∼𝒩(0,I𝑛)‖Ax−Ax*‖𝑝 𝑝≤𝜂𝑅⃒ ⃒
𝑖∈𝐽
𝑖 𝑖 ⃒ ⃒
We can now appeal to the theory of Gaussian processes to bound this quantity. Define a Gaussian process by
∑︁
𝑋 := g ∆ (x)
x 𝑖 𝑖
𝑖∈𝐽
with pseudo-metric
(︂ )︂1/2 (︃ )︃1/2
𝑑 (x,x′):= E|𝑋 −𝑋′|2 = ∑︁ (∆ (x)−∆ (x′))2
𝑋 x x 𝑖 𝑖
g
𝑖∈𝐽
We will use Dudley’s entropy integral (Theorem B.9) to bound the tail of this quantity, and then integrate to
obtain moment bounds.
Using the sensitivity bound of Lemma B.12, we obtain a bound on the pseudo-metric 𝑑 .
𝑋
Lemma B.14. Let 𝑞 =𝑂(log(𝜏𝑛/𝜀)). For x,x′ ∈𝑇 for 𝑇 ={‖Ax−Ax*‖𝑝 ≤𝜂𝑅}, we have that
𝑝
{︃
𝑂(𝑤1/2)𝜂1/𝑝−1/2‖W−1/𝑝A(x−x′)‖𝑝/2𝑅1/2 𝑝<2
𝑑 (x,x′)≤ w,𝑞
𝑋
𝑂(𝑤1/2)𝜏1/2−1/𝑝‖W−1/𝑝A(x−x′)‖ 𝑅1−1/𝑝 𝑝>2
w,𝑞
and
{︃
𝑂(𝑤1/2𝜂1/𝑝𝛾−1/2𝑅) 𝑝<2
diam(𝑇)= sup 𝑑 (x,x′)≤
𝑋
x,x′∈𝑇 𝑂(𝜀𝑤1/2𝜏1/2𝑅) 𝑝>2
Proof. Let y=Ax−b and y′ =Ax′−b. Note then that
∑︁ ∑︁
𝑑 (x,x′)2 = (∆ (x)−∆ (x′))2 = (|y(𝑖)|𝑝−|y′(𝑖)|𝑝)2
𝑋 𝑖 𝑖
𝑖∈𝐽 𝑖∈𝐽
∑︁
≤𝑝2 |y(𝑖)−y′(𝑖)|2(|y(𝑖)|𝑝−1+|y′(𝑖)|𝑝−1)2 Fact B.3
𝑖∈𝐽
For 𝑝<2, we have that
∑︁
𝑑 (x,x′)2 ≤𝑝2‖(y−y′)| ‖𝑝 (|y(𝑖)−y′(𝑖)|)2−𝑝(|y(𝑖)|𝑝−1+|y′(𝑖)|𝑝−1)2
𝑋 𝐽 ∞
𝑖∈𝐽
∑︁
≤2𝑝2‖(y−y′)| ‖𝑝 (|y(𝑖)−y′(𝑖)|)2−𝑝(|y(𝑖)|2𝑝−2+|y′(𝑖)|2𝑝−2)
𝐽 ∞
𝑖∈𝐽
≤2𝑝2‖(y−y′)| ‖𝑝 ‖y−y′‖2−𝑝(‖y‖2𝑝−2+‖y′‖2𝑝−2) H¨older’s inequality
𝐽 ∞ 𝑝 𝑝 𝑝
≤𝑂(𝜂2/𝑝−1)‖(y−y′)| ‖𝑝 𝑅.
𝐽 ∞
where H¨older’s inequality is applied with exponents 𝑝 >1 and 𝑝 >1. For 𝑝>2, we have that
2−𝑝 2𝑝−2
𝑛
𝑑 (x,x′)2 ≤2𝑝2‖(y−y′)| ‖2 ∑︁ |y(𝑖)|2𝑝−2+|y′(𝑖)|2𝑝−2
𝑋 𝐽 ∞
𝑖=1
𝑛
≤2𝑝2max{‖y| ‖ ,‖y′| ‖ }𝑝−2‖(y−y′)| ‖2 ∑︁ |y(𝑖)|𝑝+|y′(𝑖)|𝑝
𝐽 ∞ 𝐽 ∞ 𝐽 ∞
𝑖=1
21≤𝑂(1)(𝜏𝑤𝑅)1−2/𝑝‖(y−y′)| ‖2 𝑅 Lemma B.12
𝐽 ∞
Furthermore, we have that
‖(y−y′)| ‖ =‖(Ax−Ax′)| ‖
𝐽 ∞ 𝐽 ∞
=‖W1/𝑝(W−1/𝑝Ax−W−1/𝑝Ax′)| ‖
𝐽 ∞
≤𝑤1/𝑝‖(W−1/𝑝Ax−W−1/𝑝Ax′)| ‖
𝐽 ∞
≤2𝑤1/𝑝‖W−1/𝑝Ax−W−1/𝑝Ax′‖
w,𝑞
where the last step follows from the fact that w ≥𝜀/𝜏𝑛 for 𝑖∈𝐽 and 𝑞 =𝑂(log(𝜏𝑛/𝜀)). Combining these
𝑖
bounds gives the claimed bound on 𝑑 (x,x′).
𝑋
Finally, we have by Lemma B.5 that
‖W−1/𝑝A(x−x*)‖ =m𝑛
ax|[A(x−x*)](𝑖)| ≤{︃ 𝛾−1/𝑝‖A(x−x*)‖
𝑝
𝑝<2
∞ 𝑖=1 w 𝑖 𝛾−1/2‖w‖1 1/2−1/𝑝‖A(x−x*)‖ 𝑝 𝑝>2
so we have the claimed diameter bound for the set {‖A(x−x*)‖𝑝 ≤𝜂𝑅}.
𝑝
The following entropy bounds are obtained from [WY23c], which in turn largely follow [BLM89].
Remark B.15. The following entropy bounds are net necessary if we only need this result for 𝑑 = 1, for
example for applications to Euclidean power means. In this case, standard volume arguments suffice (see, e.g.,
Lemma 2.4 of [BLM89]).
Lemma B.16. Let 1 ≥ w ∈ R𝑛 be non-negative weights. Let 2 ≤ 𝑞 < ∞ and let A ∈ R𝑛×𝑑 be such that
W1/2A is orthonormal. Let 𝜏 ≥max𝑛 𝑖=1⃦ ⃦e⊤
𝑖
A⃦ ⃦2 2. Let 𝐵 w𝑝(A):={x:‖Ax‖
w,𝑝
≤1}. Then,
𝑛2/𝑞𝑞·𝜏
log𝐸(𝐵2(A),𝐵𝑞(A),𝑡)≤𝑂(1)
w w 𝑡2
and
(︂ )︂
1 log𝑑
log𝐸(𝐵𝑝(A),𝐵𝑞(A),𝑡)≤𝑂(1) +log𝑛+𝑛2/𝑞𝑞 𝜏.
w w 𝑡𝑝 2−𝑝
for 𝑝<2.
We may now evaluate Dudley’s entropy integral.
Lemma B.17 (Entropy integral bound for 𝑝<2). We have that
∫︁ ∞ √︀ (︁ 𝜏𝑛)︁1/2
log𝐸(𝐵𝑝(A),𝑑 ,𝑡) 𝑑𝑡≤𝑂(𝑤1/2𝛾−1/2𝜂1/2𝑅) log log𝑑
𝑋 𝜀
0
Proof. Note that it suffices to integrate the entropy integral to diam(𝑇), which is bounded in Lemma B.14.
Note also that 𝑇 is just a translation of (𝜂𝑅)1/𝑝·𝐵𝑝(A), so we have
log𝐸(𝑇,𝑑 ,𝑡)=log𝐸((𝜂𝑅)1/𝑝·𝐵𝑝(A),𝑑 ,𝑡)
𝑋 𝑋
=log𝐸((𝜂𝑅)1/𝑝·𝐵𝑝(A),𝐾‖W−1/𝑝A(·)‖𝑝/2,𝑡) Lemma B.14
w,𝑞
=log𝐸(𝐵𝑝(W−1/𝑝A),𝐵𝑞(W−1/𝑝A),𝑡2/𝑝/𝐾2/𝑝(𝜂𝑅)1/𝑝)
w w
where 𝐾 =𝑂(𝑤1/2𝜂1/𝑝−1/2𝑅1/2).
For small radii less than 𝜆 for a parameter 𝜆 to be chosen, we use a standard volume argument, which
shows that
𝑛
log𝐸(𝐵𝑝(W−1/𝑝A),𝐵𝑞(W−1/𝑝A),𝑡)≤𝑂(𝑑)log
w w 𝑡
so
∫︁ 𝜆 √︀ ∫︁ 𝜆√︂ 𝑛𝐾2/𝑝(𝜂𝑅)1/𝑝
log𝐸(𝑇,𝑑 ,𝑡) 𝑑𝑡≤ 𝑑log 𝑑𝑡
𝑋 𝑡2/𝑝
0 0
22√︁ √ ∫︁ 𝜆√︂ 𝑅2/𝑝
≤𝜆 𝑑log(𝑛(𝜂2/𝑝𝑤)1/𝑝)+ 𝑑 log 𝑑𝑡
𝑡2/𝑝
0
√︁ √ √︂ 𝑅
≤𝜆 𝑑log(𝑛(𝜂2/𝑝𝑤)1/𝑝)+ 𝑑·𝑂(𝜆) log
𝜆
√︂
𝑛(𝜂2/𝑝𝑤)1/𝑝𝑅
≤𝑂(𝜆) 𝑑log
𝜆
On the other hand, for large radii larger than 𝜆, we use the bounds of Lemma B.16. Note that the entropy
bounds do not change if we replace A by AR, where R is the change of basis matrix such that W1/2−1/𝑝AR
is orthonormal. Then by the properties of 𝛾-one-sided ℓ Lewis weights (Lemma B.7), we have
𝑝
‖e⊤W−1/𝑝AR‖2 =w−2/𝑝‖e⊤AR‖2 ≤𝛾−1.
𝑖 2 𝑖 𝑖 2
Then, Lemma B.16 gives
𝑂(𝑤𝜂2/𝑝𝑅2) 𝜏𝑛
log𝐸(𝐵𝑝(W−1/𝑝A),𝐵𝑞(W−1/𝑝A),𝑡2/𝑝/𝐾2/𝑝(𝜂𝑅)1/𝑝)= log
w w 𝛾𝑡2 𝜀
so the entropy integral gives a bound of
𝑂(𝑤1/2𝜂1/𝑝𝑅)(︁ 𝜏𝑛)︁1/2∫︁ diam(𝑇) 1 𝑂(𝑤1/2𝜂1/𝑝𝑅)(︁ 𝜏𝑛)︁1/2 diam(𝑇)
log 𝑑𝑡= log log .
𝛾1/2 𝜀 𝑡 𝛾1/2 𝜀 𝜆
𝜆
√
We choose 𝜆=diam(𝑇)/ 𝑑, which yields the claimed conclusion.
An analogous result and proof holds for 𝑝>2.
Lemma B.18 (Entropy integral bound for 𝑝>2). Let 2<𝑝<∞. Let A∈R𝑛×𝑑 and let 0≤w ∈R𝑛 be
𝛾-one-sided ℓ Lewis weights. Let 𝑤 =max w . Then,
𝑝 𝑖∈[𝑛] 𝑖
∫︁ ∞ √︀ (︁ 𝜏𝑛)︁1/2
log𝐸(𝐵𝑝(A),𝑑 ,𝑡) 𝑑𝑡≤𝑂(𝑤1/2𝜀𝜏1/2𝑅) log log𝑑
𝑋 𝜀
0
Proof. Note that it suffices to integrate the entropy integral to diam(𝑇), which is bounded in Lemma B.14.
Note also that 𝑇 is just a translation of (𝜂𝑅)1/𝑝·𝐵𝑝(A), so we have
log𝐸(𝑇,𝑑 ,𝑡)=log𝐸((𝜂𝑅)1/𝑝·𝐵𝑝(A),𝑑 ,𝑡)
𝑋 𝑋
=log𝐸((𝜂𝑅)1/𝑝·𝐵𝑝(A),𝐾‖W−1/𝑝A(·)‖ ,𝑡) Lemma B.14
w,𝑞
=log𝐸(𝐵𝑝(W−1/𝑝A),𝐵𝑞(W−1/𝑝A),𝑡/𝐾(𝜂𝑅)1/𝑝)
w w
where 𝐾 =𝑂(𝑤1/2𝜏1/2−1/𝑝𝑅1−1/𝑝).
For small radii less than 𝜆 for a parameter 𝜆 to be chosen, we use a standard volume argument, which
shows that
𝑛
log𝐸(𝐵𝑝(W−1/𝑝A),𝐵𝑞(W−1/𝑝A),𝑡)≤𝑂(𝑑)log
w w 𝑡
so
∫︁ 𝜆 √︀ ∫︁ 𝜆√︂ 𝑛𝐾(𝜂𝑅)1/𝑝
log𝐸(𝑇,𝑑 ,𝑡) 𝑑𝑡≤ 𝑑log 𝑑𝑡
𝑋 𝑡
0 0
√︁ √ ∫︁ 𝜆√︂ 𝑅
≤𝜆 𝑑log(𝑛𝑤1/2𝜂1/𝑝𝜏1/2−1/𝑝)+ 𝑑 log 𝑑𝑡
𝑡
0
√︁ √ √︂ 𝑅
≤𝜆 𝑑log(𝑛𝑤1/2𝜂1/𝑝𝜏1/2−1/𝑝)+ 𝑑·𝑂(𝜆) log
𝜆
√︂
𝑛𝑤1/2𝜂1/𝑝𝜏1/2−1/𝑝𝑅
≤𝑂(𝜆) 𝑑log
𝜆
23On the other hand, for large radii larger than 𝜆, we use the bounds of Lemma B.16. Note that the entropy
bounds do not change if we replace A by AR, where R is the change of basis matrix such that W1/2−1/𝑝AR
is orthonormal. Then by the properties of 𝛾-one-sided ℓ Lewis weights (Lemma B.7), we have
𝑝
‖e⊤W−1/𝑝AR‖2 =w−2/𝑝‖e⊤AR‖2 ≤𝛾−1.
𝑖 2 𝑖 𝑖 2
Then, Lemma B.6 and Lemma B.16 give
log𝐸(𝐵𝑝(W−1/𝑝A),𝐵𝑞(W−1/𝑝A),𝑡/𝐾(𝜂𝑅)1/𝑝)
w w
≤ log𝐸(𝐵2(W−1/𝑝A),𝐵𝑞(W−1/𝑝A),𝑡/𝐾(𝜂𝑅)1/𝑝‖w‖1/2−1/𝑝)
w w 1
𝐾2(𝜂𝑅)2/𝑝‖w‖1−2/𝑝 𝜏𝑛
≤ 1 log
𝛾𝑡2 𝜀
𝑂(𝑤)𝜀2𝜏𝑅2 𝜏𝑛
≤ log
𝑡2 𝜀
so the entropy integral gives a bound of
(︁ 𝜏𝑛)︁1/2∫︁ diam(𝑇) 1 (︁ 𝜏𝑛)︁1/2 diam(𝑇)
𝑂(𝑤1/2𝜀𝜏1/2𝑅) log 𝑑𝑡=𝑂(𝑤1/2𝜀𝜏1/2𝑅) log log .
𝜀 𝑡 𝜀 𝜆
𝜆
√
We choose 𝜆=diam(𝑇)/ 𝑑, which yields the claimed conclusion.
We are now ready to prove Theorem A.5.
Proof of Theorem A.5. We have by Lemma B.10 that the Gaussian process of (6) is bounded by
√
(2ℰ)𝑙(ℰ/𝒟)+𝑂( 𝑙𝒟)𝑙
where
⎧ (︁ 𝜏𝑛)︁1/2
⎪⎨𝑂(𝑤1/2𝛾−1/2𝜂1/𝑝𝑅) log
𝜀
log𝑑 𝑝<2
ℰ ≤
(︁ 𝜏𝑛)︁1/2
⎪⎩𝑂(𝜀𝑤1/2𝜏1/2𝑅)
log log𝑑 𝑝>2
𝜀
by Lemmas B.17 and B.18 and
{︃
𝑂(𝑤1/2𝜂1/𝑝𝛾−1/2𝑅) 𝑝<2
𝒟 ≤
𝑂(𝜀𝑤1/2𝜏1/2𝑅) 𝑝>2
by Lemma B.14. This gives a bound of (𝛼𝑅)𝑙 on (6), where
⎧ [︃(︂(︁ 𝜏𝑛)︁1/2 )︂1+1/𝑙 √ ]︃
⎪⎪⎪⎪⎨𝑂(𝑤1/2𝜂1/𝑝𝛾−1/2) log
𝜀
log𝑑 + 𝑙 𝑝<2
𝛼=
⎪⎪⎪⎪⎩𝑂(𝜀𝑤1/2𝜏1/2𝑅)[︃(︂(︁ log𝜏 𝜀𝑛)︁1/2 log𝑑)︂1+1/𝑙 +√ 𝑙]︃
𝑝>2
We now set 𝛼=𝜀 and solve for the 𝜀 that we can obtain. From this, we see that we can set
⎧ ⎪⎨𝑂(𝑤𝜂2/𝑝)1/2𝛾−1/2[︁(︀ (log𝑑)2log𝑛)︀1+1/𝑙 +𝑙]︁1/2
𝑝<2
𝜀= .
⎪⎩𝑂(𝑤𝜂‖w‖𝑝/2−1)1/𝑝𝛾−1/2[︁(︀ (log𝑑)2log𝑛)︀1+1/𝑙 +𝑙]︁1/𝑝
𝑝>2
1
24C Missing proofs for weak coresets
C.1 Proof of the closeness lemma
Proof of Lemma 4.4. First note that
𝑛 𝑚
⟨ ⟩ ∑︁∑︁
(AX*G−B)∘(𝑝−1),AX*G−AXG = [AX*G−B](𝑖,𝑗)∘(𝑝−1)[A(X*−X)G](𝑖,𝑗)
𝑖=1𝑗=1
𝑛 𝑚
=∑︁∑︁ [AX*G−B](𝑖,𝑗)∘(𝑝−1)⟨︀ (A⊤e )(e⊤G⊤),X*−X⟩︀
𝑖 𝑗
𝑖=1𝑗=1
⟨ 𝑛 𝑚 ⟩
∑︁∑︁
= [AX*G−B](𝑖,𝑗)∘(𝑝−1)(A⊤e )(e⊤G⊤),X*−X .
𝑖 𝑗
𝑖=1𝑗=1
The left term in the product is the gradient of the objective at the optimum by Lemma 4.3, so this is just 0
for any X. Then for 𝑝<2, we have by Lemma 4.2 that
𝑝−1
‖AX*G−B‖2 + ‖AXG−AX*G‖2 ≤‖AXG−B‖2 ≤(1+𝜂)2‖AX*G−B‖2
𝑝,𝑝 2 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝
which rearranges to
‖AXG−AX*G‖ ≤𝑂(𝜂1/2)OPT.
𝑝,𝑝
and for 𝑝>2, we have by Lemma 4.2 that
𝑝−1
‖AX*G−B‖𝑝 + ‖AXG−AX*G‖𝑝 ≤‖AXG−B‖𝑝 ≤(1+𝜂)𝑝‖AX*G−B‖𝑝
𝑝,𝑝 𝑝2𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝
which rearranges to
‖AXG−AX*G‖ ≤𝑂(𝜂1/𝑝)OPT.
𝑝,𝑝
C.2 Proof of the initial weak coreset bound
Proof of Lemma 4.5. We first show that
(︂ )︂
1
‖AXˆG−AX*G‖𝑝 ≤𝑂 OPT𝑝
𝑝,𝑝 𝛿
with probability at least 1−𝛿. By using the fact that S is an 𝑂(1)-approximate ℓ subspace embedding, we
𝑝
have that
‖AXˆG−AX*G‖𝑝 ≤‖S(AXˆG−AX*G)‖𝑝
𝑝,𝑝 𝑝,𝑝
(︁ )︁
≤2𝑝−1 ‖S(AXˆG−B)‖𝑝 +‖S(AX*G−B)‖𝑝 Fact B.1
𝑝,𝑝 𝑝,𝑝
≤2𝑝+1‖S(AX*G−B)‖𝑝 Approximate optimality of Xˆ
𝑝,𝑝
Thelatterquantityisatmost𝑂(1)OPT𝑝 withprobabilityatleast1−𝛿 byMarkov’sinequality. Thus,wemay
𝛿
replace the optimization of Xˆ over all X∈R𝑑×𝑡 with optimization over the ball {X:‖AXG−AX*G‖𝑝 =
𝑝,𝑝
𝑂(1)OPT𝑝}.
𝛿
The rest of the proof now mimics the proof of Theorem 3.1. We apply Theorem 3.2 with accuracy
parameter 𝜀 set to 𝜀𝛿, failure parameter set to (𝜀𝛿)𝑝𝛿2, and proximity parameter 𝜂 set to 1. Let 𝑆 ⊆[𝑚] be
the set of columns for which Theorem 3.2 fails. Then by applying Markov’s inequality twice as in the proof
of Theorem 3.1, we have that
∑︁
‖S(AX*G−B)e ‖𝑝 =𝑂((𝜀𝛿)𝑝)OPT𝑝
𝑗 𝑝
𝑗∈𝑆
25and
∑︁
‖(AX*G−B)e ‖𝑝 =𝑂((𝜀𝛿)𝑝)OPT𝑝
𝑗 𝑝
𝑗∈𝑆
and thus it follows that
∑︁ ‖S(AXG−B)e ‖𝑝 =∑︁ ‖(AXG−B)e ‖𝑝±𝑂(𝜀𝛿)(︀ ‖A(X−X*)G‖𝑝+OPT𝑝)︀ .
𝑗 𝑝 𝑗 𝑝 𝑝
𝑗∈𝑆 𝑗∈𝑆
Summing this result with the rest of the columns 𝑗 ∈/ 𝑆 gives that
⃒ ⃒(︀ ‖S(AXG−B)‖𝑝 −‖S(AX*G−B)‖𝑝 )︀ −(︀ ‖AXG−B‖𝑝 −‖AX*G−B‖𝑝 )︀⃒
⃒
𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝
≤ 𝜀𝛿(︀ ‖AX*G−B‖𝑝 +‖S(AX*G−B)‖𝑝 +‖AXG−AX*G‖𝑝 )︀ ≤𝑂(𝜀)OPT𝑝
𝑝,𝑝 𝑝,𝑝 𝑝,𝑝
Thus, in the ball {X:‖AXG−AX*G‖𝑝 =𝑂(1)OPT𝑝}, we have that
𝑝,𝑝 𝛿
‖S(AXG−B)‖𝑝 =‖AXG−B‖𝑝 +(‖S(AX*G−B)‖𝑝 −‖AX*G−B‖𝑝 )±𝑂(𝜀)OPT𝑝.
𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝
It follows that Xˆ must minimize ‖AXG−B‖𝑝 up to an additive 𝑂(𝜀)OPT𝑝.
𝑝,𝑝
C.3 Proof of the weak coreset construction
Proof of Theorem 4.1. Let
⎧ [︂ 1]︂
⎪⎪⎨𝑂(𝛾−1)𝛿−2‖w‖
1
(log𝑑)2log𝑛+log
𝛿
𝑝<2
𝐶 =
[︂ ]︂
1
⎪⎪⎩𝑂(𝛾−𝑝/2)𝛿−𝑝‖w‖𝑝 1/2 (log𝑑)2log𝑛+log
𝛿
𝑝>2
We will make use of the fact that ‖S(AX*G−B)‖𝑝 =𝑂(1)‖S(AX*G−B)‖𝑝 with probability at least
𝑝,𝑝 𝛿 𝑝,𝑝
1−𝛿 by Markov’s inequality.
We will first give the argument for 𝑝<2. Suppose that 𝐶/𝜀𝛽 rows are needed for a (1+𝜀)-approximate
weak coreset. Now choose 𝑎 such that 𝑎 − 2 = −𝑎𝛽, that is, 𝑎 = 2/(1 + 𝛽). Then for 𝜂2/𝑝 = 𝜀𝑎,
𝐶𝜂2/𝑝/(𝜀𝛿)2 =𝐶/𝜂(2/𝑝)𝛽 rows yields a (1+𝜂2/𝑝)-approximate weak coreset. Then, a (1+𝜂2/𝑝)-approximate
minimizer X satisfies
‖AXG−AX*G‖𝑝 ≤𝑂(𝜂)‖AX*G−B‖𝑝
𝑝,𝑝 𝑝,𝑝
by Lemma 4.4. For all such X, an argument as done in Theorem 3.1 and Lemma 4.5 shows that ‖S(AXG−
B)‖𝑝 −‖S(AX*G−B)‖𝑝 and ‖AXG−B‖𝑝 −‖AX*G−B‖𝑝 are close up to an additive error of
𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝
(︂ )︂
1
𝜀𝛿 ‖AX*G−B‖𝑝 +‖S(AX*G−B)‖𝑝 + ‖AXG−AX*G‖𝑝 =𝑂(𝜀)‖AX*G−B‖𝑝
𝑝,𝑝 𝑝,𝑝 𝜂 𝑝,𝑝 𝑝,𝑝
Thus, 𝐶/𝜂(2/𝑝)𝛽 rows in fact gives a (1+𝑂(𝜀))-approximate minimizer. That is, if 𝐶/𝜀𝛽 rows is sufficient for
(1+𝜀)-approximation, then 𝐶/𝜂(2/𝑝)𝛽 =𝐶/𝜀𝑎𝛽 =𝐶/𝜀2𝛽/(1+𝛽) rows is sufficient for (1+𝜀)-approximation as
well. We may now iterate this argument. Consider the sequence 𝛽 given by
𝑖
2𝛽
𝛽 =2, 𝛽 = 𝑖 .
0 𝑖+1 1+𝛽
𝑖
The solution to this recurrence is given by the following lemma, with 𝑝=2:
Lemma C.1. Let 𝑝>1 and let {𝛽 }∞ be defined by the recurrence relation 𝛽 =𝑝 and 𝛽 =𝑝𝛽 /(1+𝛽 ).
𝑖 𝑖=0 0 𝑖+1 𝑖 𝑖
Then,
1
𝛽 =
𝑖 𝑝−𝑖(𝑝−1−(𝑝−1)−1)+(𝑝−1)−1
26Proof. Note that 1 = 1 1 + 1 so the sequence {𝑎 }∞ given by 𝑎 =1/𝛽 satisfies the linear recurrence
𝛽𝑖+1 𝑝𝛽𝑖 𝑝 𝑖 𝑖=0 𝑖 𝑖
𝑎 = 1𝑎 + 1. Note that this recurrence has the fixed point 𝑎 = 1/(𝑝−1), so the sequence 𝑎′ = 𝑎 −𝑎
𝑖+1 𝑝 𝑖 𝑝 𝑖 𝑖
satisfies 𝑎′ = 1𝑎′, which gives, 𝑎′ =𝑝−𝑖𝑎′. Thus, 𝑎 −𝑎=𝑝−𝑖(𝑎 −𝑎) so
𝑖+1 𝑝 𝑖 𝑖 0 𝑖 0
1 1
𝛽 = =
𝑖 𝑎 𝑝−𝑖(𝑎 −𝑎)+𝑎
𝑖 0
1
= .
𝑝−𝑖(𝑝−1−(𝑝−1)−1)+(𝑝−1)−1
Thus, applying this argument 𝑂(loglog1) times yields that 𝛽 ≤ 1+𝑂(1/log(1)) which means that
𝜀 𝑖 𝜀
reading only 𝑂(1)𝐶/𝜀 entries suffices. Union bounding over the success of the 𝑂(loglog1) rounds completes
𝜀
the argument.
Next, let 𝑝 > 2. Suppose that 𝐶/𝜀𝛽 rows are needed for a (1+𝜀)-approximate weak coreset. Now
choose 𝑎 such that 𝑎−𝑝 = −𝑎𝛽, that is, 𝑎 = 𝑝/(1+𝛽). Then for 𝜂 = 𝜀𝑎, 𝐶𝜂/𝜀𝑝 = 𝐶/𝜂𝛽 rows yields a
(1+𝜂)-approximate weak coreset. Then, a (1+𝜂)-approximate minimizer X satisfies
‖AXG−AX*G‖𝑝 ≤𝑂(𝜂)‖AX*G−B‖𝑝
𝑝,𝑝 𝑝,𝑝
by Lemma 4.4. For all such X, an argument as done in Theorem 3.1 and Lemma 4.5 shows that ‖S(AXG−
B)‖𝑝 −‖S(AX*G−B)‖𝑝 and ‖AXG−B‖𝑝 −‖AX*G−B‖𝑝 are close up to an additive error of
𝑝,𝑝 𝑝,𝑝 𝑝,𝑝 𝑝,𝑝
(︂ )︂
1
𝜀 ‖AX*G−B‖𝑝 + ‖AXG−AX*G‖𝑝 =𝑂(𝜀)‖AX*G−B‖𝑝
𝑝,𝑝 𝜂 𝑝,𝑝 𝑝,𝑝
Thus, 𝐶/𝜂𝛽 rows in fact gives a (1+𝑂(𝜀))-approximate minimizer. That is, if 𝐶/𝜀𝛽 rows is sufficient for
(1+𝜀)-approximation, then 𝐶/𝜂𝛽 =𝐶/𝜀𝑎𝛽 =𝐶/𝜀𝑝𝛽/(1+𝛽) rows is sufficient for (1+𝜀)-approximation as well.
We may now iterate this argument. Consider the sequence 𝛽 given by
𝑖
𝑝𝛽
𝛽 =𝑝, 𝛽 = 𝑖 .
1 𝑖+1 1+𝛽
𝑖
Then by Lemma C.1, applying this argument 𝑂(loglog1) times yields that 𝛽 ≤(𝑝−1)+𝑂(1/log(1)) which
𝜀 𝑖 𝜀
means that reading only 𝑂(1)𝐶/𝜀𝑝−1 entries suffices. Union bounding over the success of the 𝑂(loglog1)
𝜀
rounds completes the argument.
D Missing proofs for applications
D.1 Sublinear algorithm for Euclidean power means
Theorem 1.6. Let {b }𝑛 ⊆R𝑑. Then, there is a sublinear algorithm which uniformly samples at most
𝑖 𝑖=1
⎧ 𝑂(𝜀−2)(︀ log1 +log1)︀ log1 𝑝=1
⎪⎨ 𝜀 𝛿 𝛿
𝑠= 𝑂(𝜀−1)(︀ log1 +log1)︀ log1 1<𝑝≤2
𝜀 𝛿 𝛿
⎪⎩𝑂(𝜀1−𝑝)(︀ log1 +log1)︀ log1 2<𝑝<∞
𝜀 𝛿 𝛿
rows b and outputs a center xˆ such that
𝑖
𝑛 𝑛
∑︁ ∑︁
‖xˆ−b ‖𝑝 ≤(1+𝜀) min ‖x−b ‖𝑝
𝑖 2 𝑖 2
x∈R𝑑
𝑖=1 𝑖=1
with probability at least 1−𝛿.
Proof. We will assume without loss of generality that by reading 𝑂(log1) rows of B, we can identify an
𝛿
𝑂(1)-approximate solution xˆ (see, e.g., Section 3.1 of [MMWY22]). Thus by subtracting off this solution, we
may assume that ‖B‖𝑝 =𝑂(OPT𝑝).
𝑝,2
27We then use Dvoretzky’s thoerem to embed this problem into the entrywise ℓ norm, so that
𝑝
‖1x⊤−B‖𝑝 =(1±𝜀)‖1x⊤G−BG‖𝑝
𝑝,2 𝑝,𝑝
for every center x∈R𝑑. This is now in a form where we may apply our weak coreset results for multiple ℓ
𝑝
regression of Theorem 1.5. Note that in this particular setting, the A matrix corresponds to the 𝑛×𝑑 all
ones matrix with 𝑑=1, and the ℓ Lewis weights can be taken to be uniform.
𝑝
Now consider running 𝐿=𝑂(log1) independent instances of the weak coreset algorithm, each which has
𝛿
the property that the algorithm makes at most
(︂ )︂
1 1
𝑂(𝜀−𝜌) log +log (7)
𝜀 𝛿
queries for 𝜌 = 2 for 𝑝 = 1, 𝜌 = 1 for 1 < 𝑝 < 2, and 𝜌 = 𝑝−1 for 2 < 𝑝 < ∞, and that if ‖S(1(x*)⊤G−
BG)‖𝑝 =𝑂(‖1(x*)⊤G−BG‖𝑝 ) for the optimal solution x*, then it succeeds with probability at least
𝑝,𝑝 𝑝,𝑝
1−𝛿/𝐿. By a union bound, this holds for all 𝐿 instances.
By Markov’s inequality, each instance satisfies ‖SBG‖𝑝 =𝑂(‖BG‖𝑝 ) with probability at least 9/10,
𝑝,𝑝 𝑝,𝑝
so at least 2/3 of the 𝐿 instances must satisfy this bound with probability at least 1−𝛿. By Dvoretzky’s
theorem, this means that ‖SB‖𝑝 =𝑂(‖B‖𝑝 ). Then, if we restrict our attention to the (2/3)𝐿 instances
𝑝,2 𝑝,2
with the smallest values of ‖SB‖𝑝 , then all of these instances must output a correct (1+𝜀)-approximately
𝑝,2
optimal solution, simultaneously with probability 1−𝛿. This gives a query bound of 𝐿 times (7).
D.2 Spanning coresets for ℓ subspace approximation
𝑝
We show that weak coreset construction imply spanning sets for ℓ subspace approximation.
𝑝
Theorem 1.9. Let {a }𝑛 ⊆R𝑑, 1≤𝑝<∞, 𝑘 ∈N, and 0<𝜀<1. Then, there exists a (1+𝜀)-spanning
𝑖 𝑖=1
coreset 𝑆 of size at most
⎧
𝑂(𝜀−2𝑘)(log(𝑘/𝜀))3 𝑝=1
⎪⎨
|𝑆|= 𝑂(𝜀−1𝑘)(log(𝑘/𝜀))3 1<𝑝≤2
⎪⎩𝑂(𝜀1−𝑝𝑘𝑝/2)(log(𝑘/𝜀))3
2<𝑝<∞
Proof. By first computing a strong coreset of size poly(𝑘/𝜀) [HV20], we can assume that 𝑛,𝑑=poly(𝑘/𝜀).
Let P=VV⊤ be the rank 𝑘 projection that minimizes ‖AP−A‖𝑝 . Note then that
𝑝,2
min ‖AVX−A‖𝑝 =‖AP−A‖𝑝 .
𝑝,2 𝑝,2
X∈R𝑘×𝑑
We then use Dvoretzky’s theorem to embed this problem into the entrywise ℓ norm, so that
𝑝
‖AVX−A‖𝑝 =(1±𝜀)‖AVXG−AG‖𝑝
𝑝,2 𝑝,𝑝
for every X ∈ R𝑘×𝑑, for some fixed G ∈ R𝑑×𝑚 with 𝑚 = poly(𝑑/𝜀). Then by our weak coreset result for
multiple ℓ regression (Theorem 4.1), there is a diagonal matrix S with
𝑝
⎧
𝑂(𝜀−2𝑘)(log(𝑘/𝜀))3 𝑝=1
⎪⎨
nnz(S)≤ 𝑂(𝜀−1𝑘)(log(𝑘/𝜀))3 1<𝑝≤2
⎪⎩𝑂(𝜀1−𝑝𝑘𝑝/2)(log(𝑘/𝜀))3
2<𝑝<∞
such that any (1+𝜀)-approximate minimizer Xˆ of ‖S(AVXG−AG)‖𝑝 satisfies
𝑝,𝑝
‖AVXˆG−AG‖𝑝 ≤(1+𝜀) min ‖AVXG−AG‖𝑝 .
𝑝,𝑝 𝑝,𝑝
X∈R𝑘×𝑑
We will take Xˆ to be
Xˆ =arg min ‖S(AVX−A)‖𝑝
𝑝,2
X∈R𝑘×𝑑
28which is indeed a (1+𝜀)-approximate minimizer of ‖S(AVXG−AG)‖𝑝 by Dvoretzky’s theorem. Then,
𝑝,𝑝
again by Dvoretzky’s theorem, we then have for this Xˆ that
‖AVXˆ −A‖𝑝 ≤(1+𝑂(𝜀)) min ‖AVX−A‖𝑝
𝑝,2 𝑝,2
X∈R𝑘×𝑑
=(1+𝑂(𝜀))‖AP−A‖𝑝 .
𝑝,2
Finally, note that Xˆ has row span contained in the row span of SA, since otherwise ‖S(AVX−A)‖𝑝
𝑝,2
can be reduced by projecting the rows of X onto rowspan(SA). Then, if P is the projection matrix onto
𝐹
𝐹 =rowspan(Xˆ), then for each row 𝑖∈[𝑛] of A,
‖P a −a ‖ =min‖x−a ‖ ≤‖Xˆ⊤V⊤a −a ‖
𝐹 𝑖 𝑖 2 𝑖 2 𝑖 𝑖 2
x∈𝐹
so
‖AP −A‖𝑝 ≤‖AVXˆ −A‖𝑝 .
𝐹 𝑝,2 𝑝,2
We thus conclude that there is a rank 𝑘 subspace in the row span of SA that is (1+𝜀)-approximately
optimal.
E Missing proofs for coreset lower bounds
We provide missing proofs from Section 5.
We will use the following lemma from coding theory.
Theorem E.1 ([PTB13]). For any 𝑝≥1 and 𝑑=2𝑘−1 for some integer 𝑘, there exists a set 𝑆 ⊆{−1,1}𝑑
and a constant 𝐶 depending only on 𝑝 which satisfy
𝑝
• |𝑆|=𝑑𝑝
√
• For any 𝑠,𝑡∈𝑆 such that 𝑠̸=𝑡, |⟨𝑠,𝑡⟩|≤𝐶 𝑑
𝑝
E.1 Strong coresets
Proof of Theorem 5.1. Let 𝑠 = 𝑑𝑝/2 and let 𝑆 ⊆ {±1}𝑑 be a set of |𝑆| = 𝑠 points given by Theorem E.1
√ √
such that ⟨a,a′⟩ ≤ 𝐶 𝑑 = 𝑂( 𝑑) for some 𝐶𝑝 ≥ 1, for every distinct a,a′ ∈ 𝑆. Let 𝑚 = 𝑠𝜀−𝑝, let
𝑝/2 𝑝/2
A∈{±1}𝑚×𝑑 be the matrix with 𝜀−𝑝 copies of a in its rows for each a∈𝑆, and let B=𝑑·I be the 𝑚×𝑚
𝑚
identity matrix scaled by 𝑑. For each row 𝑖∈[𝑚], we say that 𝑖′ ∈[𝑠] is its group number if e⊤A is the 𝑖′-th
𝑖
point in 𝑆.
Suppose for contradiction that S is a strong coreset with nnz(S)≤𝑚/16 such that
(︃ )︃
𝜀
‖S(AX−B)‖𝑝 = 1± ‖AX−B‖𝑝
𝑝,𝑝 12𝐶𝑝 𝑝,𝑝
𝑝/2
for every X∈R𝑑×𝑚. Then, there is a subset 𝑇 ⊆[𝑚] with |𝑇|=𝑚/16 such that S is supported on 𝑇. For
each𝑖′ ∈[𝑠],let𝑇 ⊆𝑇 denotetherowsof𝑇 whoserowsinAwithgroupnumber𝑖′ ∈[𝑠],so∑︀𝑠 |𝑇 |=|𝑇|.
𝑖′ 𝑖′=1 𝑖′
Then by averaging, there are at least (3/4)𝑠 groups 𝑖′ ∈[𝑠] such that |𝑇 |≤𝜀−𝑝/2. Thus, we may assume
𝑖′
without loss of generality that |𝑇 |=𝜀−𝑝 for the first (1/4)𝑠 groups, |𝑇 |=𝜀−𝑝/2 for the last (3/4)𝑠 groups,
𝑖′ 𝑖′
and |𝑇|=(5/8)𝑚.
Let 𝑊 := ∑︀𝑚 |S |𝑝 denote the total weight mass of S. Note then that by querying X = 0, we must
𝑖=1 𝑖,𝑖
have that
(︃ )︃
𝜀
‖SB‖𝑝 =𝑊 =(1±𝜀)‖B‖𝑝 = 1± 𝑚.
𝑝,𝑝 𝑝,𝑝 12𝐶𝑝
𝑝/2
Let 𝑊 denote the sum of |S |𝑝 on the first (1/4)𝑠 groups, and let 𝑊 denote the sum of |S |𝑝 on the last
1 𝑖,𝑖 2 𝑖,𝑖
(3/4)𝑠 groups. We will assume that 𝑊 ≤𝑚/4, since the case of 𝑊 ≥𝑚/4 is symmetric.
1 1
29We now construct a query X∈R𝑑×𝑚 with the 𝑗-th column given by
{︃
𝜀·e⊤A 𝑗 ∈𝑇
Xe = 𝑗
𝑗
0 𝑗 ∈/ 𝑇
Note then that for each 𝑖,𝑗 ∈[𝑚],
⎧
𝜀𝑑 e⊤A=e⊤A,𝑗 ∈𝑇
⎪⎨ √ 𝑖 𝑗
e⊤AXe = 𝜀𝐶 𝑑 e⊤A̸=e⊤A,𝑗 ∈𝑇
𝑖 𝑗 𝑝/2 𝑖 𝑗
⎪⎩0
𝑗 ∈/ 𝑇
Let 𝑖∈[𝑚] and let 𝑖′ ∈[𝑠] be its group number. Then the cost of row 𝑖 if 𝑖∈𝑇 is
𝑚
‖e⊤
𝑖
AX−e⊤
𝑖
B‖𝑝
𝑝
=∑︁ 𝑗=1⃒ ⃒e⊤
𝑖
AXe
𝑗
−B(𝑖,𝑗)⃒ ⃒𝑝 = ⏟(1−
𝑖
=𝜀
⏞
𝑗)𝑝𝑑𝑝 +(|𝑇 𝑖′|−1)·
e⊤ 𝑖
A⏟𝜀𝑝 =𝑑
⏞
e𝑝
⊤
𝑗A+(|𝑇|−|𝑇 𝑖′|)·𝜀
⏟
e𝑝 ⊤𝐶 A𝑝𝑝
̸=/ ⏞2
e𝑑 ⊤𝑝 A/ 2
𝑖 𝑗
=(1−𝑝𝜀+|𝑇 |𝜀𝑝+(5/8)𝐶𝑝 +𝑜(𝜀))𝑑𝑝
𝑖′ 𝑝/2
while the cost of row 𝑖∈[𝑚] if 𝑖∈/ 𝑇 is
𝑚
‖e⊤
𝑖
AX−e⊤
𝑖
B‖𝑝
𝑝
=∑︁⃒ ⃒e⊤
𝑖
AXe
𝑗
−B(𝑖,𝑗)⃒ ⃒𝑝 = ⏟𝑑 ⏞𝑝 +|𝑇 𝑖′|· 𝜀 ⏟𝑝 𝑑 ⏞𝑝 +(|𝑇|−|𝑇 𝑖′|)·𝜀𝑝𝐶 𝑝𝑝 /2𝑑𝑝/2
𝑗=1 𝑖=𝑗 e⊤A=e⊤A ⏟ ⏞
𝑖 𝑗 e⊤A̸=e⊤A
𝑖 𝑗
=(1+|𝑇′|𝜀𝑝+(5/8)𝐶𝑝 +𝑜(𝜀))𝑑𝑝.
𝑖 𝑝/2
Let
𝑐 =(1−𝑝𝜀+1+(5/8)𝐶𝑝 +𝑜(𝜀))𝑑𝑝
1 𝑝/2
𝑐 =(1−𝑝𝜀+(1/2)+(5/8)𝐶𝑝 +𝑜(𝜀))𝑑𝑝
2 𝑝/2
𝑐 =(1+(1/2)+(5/8)𝐶𝑝 +𝑜(𝜀))𝑑𝑝
3 𝑝/2
Then, the total true cost is at least
𝑚 3𝑚 3𝑚
‖AX−B‖𝑝 = 𝑐 + 𝑐 + 𝑐
𝑝,𝑝 4 1 8 2 8 3
𝑚 3𝑚 3𝑚
= 𝑐 + 𝑐 + (𝑐 −𝑐 )
4 1 4 2 8 3 2
𝑚 3𝑚 3𝑚
≥ 𝑐 + 𝑐 + ·(𝜀−𝑜(𝜀))𝑑𝑝
4 1 4 2 4
while the strong coreset estimate is at most
‖S(AX−B)‖𝑝 =𝑊 𝑐 +𝑊 𝑐
𝑝,𝑝 1 1 2 2
=𝑊 (𝑐 −𝑐 )+(𝑊 +𝑊 )𝑐
1 1 2 1 2 2
(︃ )︃
𝑚 𝜀
≤ (𝑐 −𝑐 )+ 1+ 𝑚𝑐
4 1 2 12𝐶𝑝 2
𝑝/2
𝑚 3𝑚 𝜀
≤ 𝑐 + 𝑐 + 𝑚𝑑𝑝.
4 1 4 2 4
Furthermore,
(︂ )︂
𝜀 𝑚 3𝑚 𝜀 𝜀
𝑐 + 𝑐 + 𝑚𝑑𝑝 ≤ 𝑚𝑑𝑝
12𝐶𝑝 4 1 4 2 4 4
𝑝/2
so (1+ 𝜀 )‖S(AX−B)‖𝑝 < ‖AX−B‖𝑝 and thus S fails to be a strong coreset. Rescaling 𝜀 by
12𝐶𝑝 𝑝,𝑝 𝑝,𝑝
𝑝/2
constant factors gives the desired result.
30E.2 Weak coresets
Proof of Theorem 5.2. Our hard instance is identical to the one of Theorem 5.1, except that each group has
𝜀1−𝑝/2𝐶𝑝 copies rather than 𝜀−𝑝 copies.
𝑝/2
Note that if S does not sample some row 𝑖∈[𝑚], then the 𝑖-th column of SB is all zeros, so the solution
obtained by the weak coreset is Xe =0, which has objective function value ‖Be ‖𝑝 =𝑑𝑝. On the other hand,
𝑖 𝑖 𝑝
the optimal value is at most (1−𝜀)𝑝𝑑𝑝 since we can set Xe =𝜀A⊤e so that
𝑖 𝑖
𝜀1−𝑝 𝜀1−𝑝
‖(AX−B)e ‖𝑝 ≤(1−𝜀)𝑝𝑑𝑝+ ·𝜀𝑝𝑑𝑝+𝑑𝑝/2 ·𝐶𝑝 𝜀𝑝𝑑𝑝/2
𝑖 𝑝 2𝐶𝑝 2𝐶𝑝 𝑝/2
𝑝/2 𝑝/2
𝜀 𝜀
≤(1−𝜀)𝑝𝑑𝑝+ ·𝑑𝑝+ ·𝑑𝑝
2 2
≤((1−𝜀)𝑝+𝜀)𝑑𝑝
which is a (1+𝜀) factor smaller for all 𝜀 sufficiently small. Thus, if nnz(S)≤𝑚/2, then the solution X that
minimizes ‖S(AX−B)‖𝑝 must be at least an additive 𝜀𝑑𝑝·𝑚/2 more expensive than the optimal solution,
𝑝,𝑝
and thus it fails to be a (1+𝜀/2)-optimal solution.
E.3 Spanning coresets
We generalize an argument of Section 4 of [DV06].
Lemma E.2. Let 1≤𝑝<∞ and
{︃
1/6 𝑝≤2
𝑐 =
𝑝 1/(6·5𝑝/2−1) 𝑝>2
Then, there is a matrix A ∈ R𝑛×(𝑛+1) such that for every 𝜀 ≥ 1/𝑛 and any subset of 𝑠 ≤ 𝑐 𝜀−1 rows, any
𝑝
rank 1 subspace 𝐹′ spanned by the 𝑠 rows must have
‖AP −A‖𝑝 >(1+𝜀) min ‖AP −A‖𝑝 .
𝐹′ 𝑝,2 𝐹 𝑝,2
rank(𝐹)≤1
Proof. Let 𝑛≤𝜀−1 and let A be the 𝑛×(𝑛+1) matrix given by [𝑅·1 ,I ] for some large enough 𝑅>0.
𝑛 𝑛
That is, A is 𝑅 along the first column and the 𝑛×𝑛 identity for the last 𝑛 columns. Note that the optimal
value is upper bounded by
𝑛((1−𝜀)2+𝜀2·(𝑛−1))𝑝/2 =𝑛(1−2𝜀+𝜀2𝑛)𝑝/2 =𝑛(1−𝜀)𝑝/2.
Letx∈R𝑠bethecoefficientsofalinearcombinationof𝑠rowsofA. Wemayassumethecoefficientsarenon-
negative, since making the coefficients negative can only increase the cost. Note first that 1/2≤‖x‖ ≤3/2
1
since otherwise
𝑛·|𝑅−𝑅‖x‖ |𝑝 ≥𝑛·𝑅/2
1
which cannot be (1+𝜀)-approximately optimal for 𝑅≥2.
The cost of the 𝑖-th row is
(︀
(1−x
)2+‖x‖2−x2)︀𝑝/2 =(︀
1−2x
+‖x‖2)︀𝑝/2
. If ‖x‖ ≥2, then
𝑖 2 𝑖 𝑖 2 2
(︀ 1−2x +‖x‖2)︀𝑝/2 ≥(1−2‖x‖ +‖x‖2)𝑝/2 =(‖x‖ −1)𝑝 ≥1
𝑖 2 2 2 2
so this cannot produce a (1+𝜀)-approximately optimal solution. Thus, assume ‖x‖ ≤2. Then,
2
(︀
1−2x
+‖x‖2)︀𝑝/2 =(︀
1+‖x‖2)︀𝑝/2(︂
1−
2
x
)︂𝑝/2
≥(︀
1+‖x‖2)︀𝑝/2(︂
1−
𝑝
x
)︂
𝑖 2 2 1+‖x‖2 𝑖 2 1+‖x‖2 𝑖
2 2
so summing over the rows gives a cost of
(︂ )︂
(︀ 1+‖x‖2)︀𝑝/2 𝑛− 𝑝 ‖x‖ =(︀ 1+‖x‖2)︀𝑝/2 𝑛−𝑝(1+‖x‖2)𝑝/2−1‖x‖
2 1+‖x‖2 1 2 2 1
2
31≥(︀ 1+‖x‖2/𝑠)︀𝑝/2 𝑛−𝑝(1+‖x‖2)𝑝/2−1‖x‖ since 1/2≤‖x‖ ≤3/2
1 2 1 1
≥(1+1/2𝑠)𝑝/2𝑛−(3/2)𝑝(1+‖x‖2)𝑝/2−1
2
≥(1+𝑝/4𝑠)𝑛−(3/2)𝑝(1+‖x‖2)𝑝/2−1
2
{︃
(1+𝑝/4𝑠)𝑛−(3/2)𝑝 𝑝≤2
≥
(1+𝑝/4𝑠)𝑛−(3/2)𝑝·5𝑝/2−1 𝑝>2
Thus, this fails to be a (1+𝜀)-approximately optimal solution for
{︃
(3/2)𝑝 𝑝≤2
(𝑝/4𝑠)𝑛≥
(3/2)𝑝·5𝑝/2−1 𝑝>2
that is,
{︃
𝑛/6 𝑝≤2
𝑠≤ .
𝑛/(6·5𝑝/2−1) 𝑝>2
We now extend Lemma E.2 to a general rank 𝑘 lower bound.
Proof of Theorem 5.3. Let 𝑛=𝜀−1 and let B be a 𝑘𝑛×𝑘(𝑛+1) block diagonal matrix with the 𝑛×(𝑛+1)
matrix construction A∈R𝑛×(𝑛+1) of Lemma E.2 on the block diagonal. Consider any set 𝑆 of 𝑠 rows of B,
and let 𝑆 denote the set of |𝑆 | = 𝑠 rows supported on the 𝑖-th block for each 𝑖 ∈ [𝑘]. Let 𝐹 denote the
𝑖 𝑖 𝑖 𝑖
optimal subspace spanned by the rows 𝑆 on the 𝑖th block.
𝑖
Let 𝑇 ⊆[𝑘] denote the set of 𝑖∈[𝑘] such that 𝑠 ≤𝑐 𝑛. If 𝑖∈𝑇, then we by Lemma E.2 that
𝑖 𝑝
(︂ )︂
𝑐
‖AP −A‖𝑝 > 1+ 𝑝 min ‖AP −A‖𝑝
𝐹𝑖 𝑝,2 𝑠 𝑖 rank(𝐹)≤𝑘 𝐹 𝑝,2
Then, the additive error from these rows is bounded below by
∑︁𝑐
𝑝 min ‖AP −A‖𝑝 ≥|𝑇|·
𝑐 𝑝|𝑇|
min ‖AP −A‖𝑝 AM-HM
𝑖∈𝑇
𝑠 𝑖 rank(𝐹)≤𝑘 𝐹 𝑝,2 ∑︀ 𝑖∈[𝑘]:𝑠𝑖≤𝑐𝑝𝑛𝑠 𝑖 rank(𝐹)≤𝑘 𝐹 𝑝,2
𝑐 |𝑇|
≥|𝑇|· 𝑝 min ‖AP −A‖𝑝
𝑠 rank(𝐹)≤𝑘 𝐹 𝑝,2
𝑐 |𝑇|2
≥ 𝑝 min ‖BP −B‖𝑝
𝑘𝑠 rank(𝐹)≤𝑘 𝐹 𝑝,2
Note that |𝑇|≥𝑘/2 by averaging, so
𝑐 |𝑇|2 𝑐 𝑘
𝑝 ≥ 𝑝 ≥𝜀
𝑘𝑠 4𝑠
which proves the theorem.
F Experimental evaluation
We show that empirically, we indeed see that the trade-off between the number of uniform samples and the
approximationqualityisindependentofthedimension𝑚inthesettingofEuclideanpowermeans. Wedothis
by plotting the sample size against the resulting relative error for 𝑚∈{100,500}, where an 𝑚-dimensional
dataset is constructed by sampling 𝑚 random features from the MNIST dataset. The results are shown in
Figure 1 and the experiment code is provided in Section F.1.
32Sample size vs relative error for 1-mean estimation
0.010
m=100
m=500
0.008
0.006
0.004
0.002
0.000
0 2000 4000 6000 8000 10000
Sample size
Figure 1: Sample size vs relative error for 1-mean estimation
F.1 Experiment code
We provide the code snippet for the experimental evaluation below.
from keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
np.random.seed(2024)
(train_X, train_y), (test_X, test_y) = mnist.load_data()
train_X = train_X.reshape(len(train_X), -1)
train_X = train_X / np.max(train_X)
n, d = train_X.shape
def power_mean_loss(train_ds, x, p=1):
x = np.expand_dims(x, axis=0)
x = np.repeat(x, repeats=n, axis=0)
e = train_ds - x
e = np.linalg.norm(e, axis=-1)
e = np.power(e, p)
return np.sum(e) / n
def run(train_ds, max_iter=200, p=1):
n, d = train_ds.shape
x0 = np.zeros(d)
x = tf.Variable(initial_value=x0)
opt = tf.keras.optimizers.Adam(learning_rate=0.5)
x.assign(x0)
def power_mean_loss_tf():
e = train_ds - x
e = tf.norm(e, axis=-1)
e = tf.math.pow(e, p)
33
rorre
evitaleRreturn tf.reduce_sum(e) / n
losses = []
while opt.iterations < max_iter:
opt.minimize(power_mean_loss_tf, var_list=[x])
loss = power_mean_loss_tf().numpy()
if np.isnan(loss):
print(x.numpy())
losses.append(loss)
return x.numpy(), losses
n, d = train_X.shape
sample_sizes = [100, 500, 1000, 5000, 10000]
for m in [100, 500]:
cols = np.random.choice(d, m)
train_m = train_X[:, cols]
x, losses = run(train_m)
OPT = losses[-1]
estimates = []
for sample_size in sample_sizes:
train_sample = np.random.choice(n, sample_size)
train_sample = train_m[train_sample, :]
x, losses = run(train_sample)
estimates.append(power_mean_loss(train_m, x))
relative_errors = [(e / OPT) - 1 for e in estimates]
print(’relative errors’, relative_errors)
34