Enhancing Human-Robot Collaborative Assembly in Manufacturing
Systems Using Large Language Models
Jonghan Lim1, Sujani Patel2, Alex Evans3, John Pimley1, Yifei Li2, and Ilya Kovalenko1,2
Abstract—The development of human-robot collaboration Further advancing the field of HRC in manufacturing
has the ability to improve manufacturing system performance systems by improving the interaction of human operators
byleveragingtheuniquestrengthsofbothhumansandrobots.
and robots presents significant challenges. Specifically, in-
Ontheshopfloor,humanoperatorscontributewiththeiradapt-
teraction with robots induces psychological stress and ten-
abilityandflexibilityindynamicsituations,whilerobotsprovide
precision and the ability to perform repetitive tasks. However, sion among operators due to language barrier [8]. In con-
the communication gap between human operators and robots temporary manufacturing systems, human operators require
limitsthecollaborationandcoordinationofhuman-robotteams extensivepre-servicetrainingandcomplexcodedevelopment
in manufacturing systems. Our research presents a human-
to ensure accurate and safe production with the robots [9].
robot collaborative assembly framework that utilizes a large
These difficulties highlight the need to develop human-robot
language model for enhancing communication in manufac-
turing environments. The framework facilitates human-robot communication systems that enable interaction between hu-
communicationbyintegratingvoicecommandsthroughnatural mans and robots without extensive robotics training (C1).
language for task management. A case study for an assembly Another challenge involves the need for greater flexibility
task demonstrates the framework’s ability to process natural
and adaptability in human-robot interaction. As the number
language inputs and address real-time assembly challenges,
of interactions between humans and robots on the shop
emphasizing adaptability to language variation and efficiency
in error resolution. The results suggest that large language floor increases, there is a higher chance of encountering
models have the potential to improve human-robot interaction unexpected changes and errors. Thus, HRC needs to be
for collaborative manufacturing assembly applications. more adaptable to real-time changes and errors during the
manufacturing assembly process (C2). Additionally, the de-
I. INTRODUCTION
mand for adaptability and flexibility extends to the need for
Advances in robotics technology have significantly en- human-robotcollaborativeassemblyapplicationstointegrate
hancedmanufacturingefficiency,resultingincostreductions advancedtechnologiesinahuman-centricdesigntoenhance
andincreasedproductivity[1],[2].Whilerobotscanexecute communication and usability (C3).
rapid, accurate, and repetitive tasks that demand heavy-duty
Large Language Models (LLMs) have recently been in-
efforts in manufacturing settings, they lack the capability
troduced in the AI community to enhance natural language
for adaptation and versatility of human operators [2]. There-
understanding and generation capabilities. These can be
fore, the importance of Human-Robot Collaboration (HRC)
extended to improve human-robot interaction in the man-
is growing as humans and robots complement each other
ufacturing facility. Models such as OpenAI’s GPT-3 [10]
skills and capabilities. HRC refers to the interaction and
and GPT-4 [11], have shown proficiency in processing,
cooperation between human operators and robotic systems
understanding, and communicating in natural language. The
within shared workspace [3]. Prior works have used HRC
integration of LLMs facilitates natural language communi-
frameworkstoimproveergonomicsinmanufacturingsettings
cation between humans and robots. Using voice interaction
andsafehuman-robotinteractionwithoutcompromisingpro-
for this communication enhances collaboration and operator
ductivity [4], [5]. Transitioning from tasks involving large
safety in dynamic workspaces. The main contributions of
components, such as loading, placing, and unloading, to the
this work are: (1) the use of LLMs for interpreting natural
complex assembly of smaller components such as printed
language to allow operators to coordinate with robotic arms,
circuit boards, the collaboration between human operators
(2) a framework for system integration of voice commands,
and robots significantly enhances both the efficiency and
robotic arms, and vision systems to facilitate HRC in an
safety of production lines [6], [7].
assembly task, enhancing operational flexibility, and (3) a
1JonghanLimandJohnPimleyarewiththeDepartmentofIndustrialand dynamic adaptation approach to task variations, errors, and
Manufacturing,PennsylvaniaStateUniversity,StateCollege,USA(e-mail: obstacles through human-robot communication.
{jxl567;jap6581@psu.edu).
2Sujani Patel and Yifei Li are with the Department of Mechanical The rest of the manuscript is organized as follows. Sec-
Engineering, Pennsylvania State University, State College, USA (e-mail: tion II reviews related work and identifies gaps in HRC for
{spp22;ybl5717@psu.edu).
3Alex Evans is with the Department of Computer Science and En- manufacturing assembly. Section III describes the proposed
gineering, Pennsylvania State University, State College, USA (e-mail: framework that integrates LLMs with voice commands,
ade5221@psu.edu). robotic arm, and sensors. Section IV showcases a case
1,3IlyaKovalenkoiswiththeDepartmentofIndustrialandManufacturing
study and evaluates the framework’s performance. Finally,
andDepartmentofMechanicalEngineering,PennsylvaniaStateUniversity,
StateCollege,USA(e-mail:iqk5135@psu.edu). Section V provides conclusions and future work.
4202
nuJ
4
]OR.sc[
1v51910.6042:viXraII. BACKGROUND to combine natural language input with visual information
based on a multi-camera system to enhance task completion
A. Human-Robot Collaboration in Manufacturing
ability.Zhangetal.[21]utilizesLLMtoaddressmulti-agent
Recently, a wide range of methods has been developed in cooperationchallenges,resolvingissuesrelatedtodecentral-
thefieldofHRCtoimprovethesafetyandefficiencyofinter- ized control, raw sensory data processing, and efficient task
actionsbetweenhumansandrobots.Forinstance,Mendeset execution in embodied environments. A framework called
al. [12] proposed an easy-to-perform HRC operating space KnowNo [22] enhances robot autonomy and efficiency in
that is more accessible to human operators. Mendes et al. ambiguous tasks using conformal prediction to guide robots
integrate task-based programming and various interaction in seeking human help when faced with uncertainty.
modes, including gestures and physical contact. Another TheseresearchstudiesdemonstratehowintegratingLLMs
workbyMezianeetal.[13]developedasystemforcollision- into robotic applications improves human-robot interaction
free trajectories for robots in shared workspaces, utilizing and robot capabilities. The methodologies involve convert-
a neural network for obstacle avoidance. Furthermore, the ing human instructions into executable robot actions and
research conducted by Zanchettin et al. [14] shows that an accounting for environmental conditions. However, existing
algorithmic approach to HRCs can predict human activity studies have not addressed common challenges found in the
patterns to control the robot more safely and effectively, manufacturing environment, such as adapting natural lan-
resultinginasignificantreductionintasktime.Thus,several guage communication for different operators (e.g., adapting
studies in the field of HRC have enhanced worker pro- commands based on skills, shift time, or other metrics),
ductivity and safety by combining the human experience matchingnaturallanguagetostructuredtasksequencesfound
with the automation capabilities of robots. Moreover, some in a manufacturing environment, and managing the timing
approaches have utilized environmental data to enhance of commands (e.g., identifying when to request operator
safety and efficiency. For example, Fernandez et al. [15] assistance in the process). We aim to bridge the gap by
developed a dual-arm robotic system with multisensor capa- improving the integration of natural language for human-
bilitiesforsafeandefficientcollaboration,integratinggesture robot interaction for an assembly process through an LLM-
recognition. Additionally, Wei et al. [16] gather information based framework.
on human operators and the environment using RGB-D
videosandpredicthumanintentionsbasedonadeep-learning III. FRAMEWORK
method. The proposed framework, presented in Figure 1, is devel-
Researcheffortshaveintroducedinnovativemethods,such oped for a human-robot collaborative assembly within the
as gesture and physical contact for interaction, and neural manufacturing environment. The framework is designed to
networksfordynamicobstacleavoidance.However,therehas facilitate interaction between a human operator and a robot
beenlimitedresearchfocusingonhuman-robotcollaborative for an assembly process.
assemblyinmanufacturingsystemsthateffectivelyintegrates Inthisframework,ahumancaninitializeandprovideaset
interactionbetweenhumansandrobotswithnaturallanguage oftasksT ={t ,t ,...,t }astheobjectivestobeachieved
1 2 n
capabilities. We aim to integrate an LLM-based approach to by the robot. The robot’s capabilities C = {c ,c ,...,c }
1 2 l
facilitate communication into the human-robot collaborative represent the specific actions that are programmed to per-
assembly framework leveraging the strengths of both human form within an assembly process. If the task matches the
flexibility and robot precision within the dynamic manufac- robot’s capabilities, the robot should proceed to execute
turing environment. the specified task t . That task, t , is broken down into
i i
subtasks(t ,t ,...,t )thatoutlinethesequentialstepsfor
i1 i2 ik
B. Large Language Models for Robots
completion. The framework has to track the last completed
Recent research has explored the integration of LLMs for subtask, denoted as t , where 1 ≤ c ≤ k indicates the
ic
robotic applications to improve human-robot interaction and completed process within t ,t ,...,t . When an error
i1 i2 ik
robot functionalities [17], [18], [19], [20], [21], [22]. This is resolved by a human operator, the robot resumes from
includestheconversionofhigh-levelhumaninstructionsinto t , ensuring continuous task progression. To enable this
ic+1
tasks that robots execute. The research by Huang et al. [17] human-robotcollaborativeassemblyprocess,thisframework
explored leveraging LLMs, such as GPT-3 and CODEX, to is structured into two primary layers: the physical layer and
converthigh-levelhumaninstructionsintoexecutableactions thevirtuallayer,bridgingthegapbetweenhumaninputsand
for robots. Similarly, Singh et al. [18] developed a method robot actions.
called ProgPrompt. This method generates Python code for
A. Physical Layer
taskplanningthataccountsforenvironmentalconditionsand
pre-programmed actions. Furthermore, Lin et al. [19] devel- The physical layer facilitates the interaction between the
oped a framework for taking natural language instructions humanoperatorandtherobotwithinasharedphysicalspace
and then developing task and motion plans. Other works based on the information and data gathered from the virtual
in robotics have utilized LLMs to maximize the robot’s layer.Thephysicalconsistsofthreeprimarycomponents:the
capability to comprehend and interact with its environment human commands, the robot actions, and the sensor data.
to resolve errors. Mees et al [20], proposed an approach A human command is where an operator provides voiceFig. 1: Human-Robot Collaborative Assembly Framework Using LLM
instructions to control robot actions in executing various serves as a bridge, transmitting commands and information
tasks.Therobotactioninvolvestherobotperformingactions to the robot.
based on a predefined set of T. The sensor data is utilized 2) Robot Agent: The robot agent interprets and executes
to monitor environmental conditions. This data ensures the the set of manufacturing tasks T based on the voice com-
action adapts to changes in the workspace (e.g. position, mands received from the human operator. This process is
orientation, or availability of parts). facilitated by various functional modules:
When a predefined event or error e ∈ E is detected, the
i • Initialization Module: The initialization of the robot
robotutilizesthecommunicationprotocoltoalertthehuman
agent, provides basic operational guidelines and task
operator. This is achieved through the LLM module, which
executionprotocols.Thisprocessinvolvesprogramming
converts e into a natural language message M (t ), com-
i ei i the robot with instructions to perform tasks T within
municated via text-to-speech technology. Different instances
its capabilities C and to communicate errors or request
of e can lead to varied message M (t ) depending on the
i ei i assistance from human operators when necessary. The
task t error. After receiving and understanding M (t ), the
i ei i following initial prompt is given to the robot agent:
humanoperatorresolvesthecorrespondinge andcommands
i
the robot to resume t . "You are a robot agent in a human-robot
i
collaborative assembly system designed to assist
in tasks and respond to commands. Upon receiving
B. Virtual Layer
a request within your capability range, execute
The virtual layer serves to facilitate communication in the service. In the event of encountering errors,
naturallanguage.Thislayerstoresthesystem’sfunctionality request assistance from a human operator for error
to enable interaction between human commands and robot correction, providing clear and understandable
actions.Thevirtuallayerofthesystemconsistsoftwomain explanations."
agents: a human agent and a robot agent. This initialization prompt defines the robot agent’s role
1) Human Agent: We have integrated a human agent into in responding to commands. It also establishes a pro-
our system to enhance the human-robot interaction based tocol for seeking assistance and communicating errors
on the approach by Zhang et al. [23]. The goal of the to human operators for task completion. Following the
human agent in our proposed framework is to process the initial setup of the robot agent, specific capabilities C
human commands in voice instructions into a text that the are defined, such as specific assembly tasks.
robot agent can understand. This agent ensures that the • LLM Module: A LLM is used to interpret human
humancommandintentionsaretransferredtotherobotagent commands into a corresponding task t ∈ T. Human
i
withouttheneedformanualprogramming.Thehumanagent commands and functional capabilities C including the
contains an Automatic Speech Recognition (ASR) module detailed functional information of C are put into the
responsible for processing the human command voice data, LLM.TheLLMthenanalyzesthelanguageandcontext
whichconvertsthisdataintotextthatcanbeprocessedbythe ofthehumancommand.Thisprocessallowsthesystem
robotagent.Thecommunicationmoduleinthehumanagent to match the command with the predefined functionalcapabilities C, ensuring the intended task is accurately
executed. The LLM module also communicates any
detected issues from the task control module into un-
derstandable natural language for the human operators,
facilitating HRC for error resolution.
• Sensor Module: The sensor module processes data
fromthephysicallayersuchasposition,orientation,and
part availability. This information is utilized for adjust-
ing robot actions required for executing manufacturing
tasks T. For example, the module provides component
positions and orientations for the task control module.
This enables accurate robotic adjustments for assembly
or initiates error notifications for missing components,
ensuringeffectivetaskexecutionanderrormanagement.
• TaskControlModule:Thetaskcontrolmoduledirects
robot actions to fulfill task t within the set T, based Fig. 2: Sequence Diagram for Human-Robot Collaborative
i
on the functional capabilities C. The task control mod- Assembly in Manufacturing Systems
ule adjusts robot actions to environmental conditions
(4) and an end cap. The exploded cable shark assembly and
obtained from the sensor module. Additionally, this
the final assembled product are shown in Figure 3. This
module plays a role in managing errors by verifying
assembly process task t include four sequential subtasks:
1
sensor data and communicating detected issues through
housing assembly t , wedge assembly t , spring assembly
11 12
the LLM module to facilitate resolution by the human
t , and end cap assembly t .
13 14
operator.
ThephysicalsetupisimplementedusingaSixDegreesof
C. Human-Robot Collaborative Assembly Workflow Freedom (6DOF) UFactory xArm with a UFactory 2-finger
gripper as the end effector [24]. The base of the robotic
The overall workflow is depicted in a sequence diagram,
arm remains fixed at the center, accompanied by a mat on
Figure 2, which visually represents the process flow within
one side, while the parts are assembled on the adjacent
a human-robot collaborative assembly framework. The se-
side. Two fixed cameras as visual sensors were placed in
quencediagramshowshowvoicecommandsfromthehuman
the surrounding area, one focused on the mat area for pre-
operator are processed and interpreted by the LLM Module
assemblycomponentplacement,andtheothercameraonthe
and guides the robot actions.
assembling process. The conversion between camera coordi-
The process begins with a human operator delivering
natesandrobotarmcoordinatesisaccomplishedthroughthe
a voice command, which is converted into a discrete set
coordinate transformation, as described in Section IV-C.
of tasks T by the LLM module that the robot agent is
programmed to understand and execute. The robot then
requestssensordatatoexecutet .Ifthedataprovidedbythe
i
sensor is valid, the robot proceeds to execute the assigned A. LLM and ASR Module
t .Thesensormoduledeterminesdatavaliditybycomparing
i
detected parameters to predefined criteria. A successful task This section outlines how LLM and the ASR module,
execution results in a completion message M (t ) to the were implemented within the system. The communication
c i
human operator via the LLM Module. aspect within the system is enabled by OpenAI’s speech-to-
Whensensordataisinvalid,ort hasanyerrors,therobot text and text-to-speech models. The OpenAI’s transcription
i
agentinitiatesthecommunicationprotocoltogenerateaner- model ‘whisper1’ [25] is implemented to transform speech-
ror message M (t ) via the LLM module, aiming to inform to-text,ensuringthathumanvoiceinstructionsareaccurately
ei i
the human operator of the specific error and its occurrence captured. To enable the communication protocol to provide
within subtask t for efficient resolution. Following the a verbal response from the robot, OpenAI’s text-to-speech
ic+1
error identification and correction by the human operator, a model ‘tts-1’ [26] is utilized to generate audio responses.
new command by the human operator is issued to the robot. This approach enhances and allows for a clear exchange of
Therobotthenresumestaskexecutionatt ,startingfromthe information between the human and the robot.
i
interruptedsubtaskt ,basedonthenewsensordata.This The LLM module, enabled by OpenAI’s GPT-4.0 [11], is
ic+1
procedure is repeated until t is successfully completed. the decision-making component of the proposed framework.
i
The LLM module converts instructions from the human
IV. CASESTUDY
operatorintotasksT executablebytherobot,utilizingitsca-
The proposed framework was tested in a manufacturing pabilities C through OpenAI’s function calling feature [27].
assembly manufacturing cell. The goal of the manufacturing This module ensures that the robot actions match the human
cell is to assemble a cable shark product. The cable shark operator commands, allowing for adaptive and responsive
assembly contains (1) a housing, (2) a wedge, (3) a spring, task management within the manufacturing environment.Fig. 3: Cable Shark Assembly
Fig. 4: Feature Extraction Method with the Vision System
B. Sensor Module: Vision System
We incorporated a vision system as our sensor module.
During the collaborative assembly process, the vision sys-
tem provides feedback on environmental data to the task
control module. This enables precise object detection and
identificationofobjectorientation,facilitatingtaskexecution
and error management. To enable object detection, Yolov5,
a computer vision model, is utilized [28]. Custom Yolov5
models are trained using a dataset assembled for object
Fig. 5: Cable Shark Assembly Process
detection. This dataset comprises images of individual parts
to the LLM module to convert the information into natural
(i.e.housing,wedge,spring,endcap),withmanualbounding
language.Thisprocessactivatesthecommunicationprotocol,
box annotations around each part in the images, and distinct
allowing the human operator to recognize and address the
classes for each part are created. The vision system utilizes
error.
this dataset with bounded boxes as a reference to identify
the parts, as shown in Figure 4. If a part is detected, the To enable the assembly process, the task control module
coordinates of the top-left and bottom-right corners of the integrates various functionalities, including coordinate trans-
bounding box are retrievable. Once these coordinates are formation and position and rotation control, based on the
obtained, the x and y coordinates of these points are utilized sensordataobtainedfromthevisionsystem.Thesefunctions
to determine the object’s midpoint. This midpoint serves as enable the execution of tasks interpreted by the LLM. The
the target point for the robotic arm to pick up the part. coordinate transformation is conducted to align the robotic
Following the successful object identification process, the arm (A) and camera (C) coordinate systems within the
valid sensor data is transferred to the task control module assembly process’s base frame (B) to ensure an accurate
for guiding robot action. For parts that are not accurately assembly task. The transformation between the robot’s base
detected, the sensor module transfers invalid data to the task (A Bp) and camera’s (C Bp) positions is defined by
control module to notify the human operator via LLM. Ap=A T ·C p, (1)
B C B
C. Task Control Module: Assembly Task where AT is the transformation matrix incorporating trans-
C
The objective of the Task Control Module is to execute lation and rotation. The matrix includes a 3x3 rotation
tasks as interpreted from human commands by LLM and to matrix AR and a translation vector t, translating coordinates
C
manage errors within a human-robot collaborative assembly from the camera to the robotic arm system. The effective
system. Error handling is also a critical function of this calculation of positions (x,y) relative to the base frame
module. The task control module verifies sensor data from relies on this transformation for the robotic arm’s precise
the vision system. For instance, this module checks if a movements and interactions within the assembly setup. The
component is properly positioned for a pickup. If the sensor positionandrotationcontrolfunctionisthenimplementedto
data is valid, the task control module continues with its fine-tuneboththepositionalalignmentandtheorientationof
operation. When the sensor data is invalid (e.g. missing componentsforassemblytasks.Totranslatecamera-detected
component), the module transfers detailed error information object coordinates (x ,y ) from pixel values to the world
p pTABLE I: Language Variations for Task Instructions
Scenario 1 Instruction Type Instruction
Specific ["Overlap resolved. Proceed with the task."], ["Problem is..."]
Scenario 1: Component Overlap Moderately Specific ["I’ve placed the components correctly."], ["I’ve sorted out the..."]
Least Specific ["Fixed."], ["Done."], ["Completed."], ["Handled."], ["Adjusted."]
Specific ["Correction is made. Resume the task."], ["The wedge is..."]
Scenario 2: Incorrectly Assembled Part Moderately Specific ["I’ve fixed the issue with the wedge."], ["I’ve placed the wedge..."]
Least Specific ["Fixed."], ["Done."], ["Addressed."], ["All set."], ["Under control."]
Specific ["I’ve placed the spring component. Please proceed."].["Spring..."]
Scenario 3: Missing Component Moderately Specific ["I’ve fixed the issue with the spring."], ["The spring component..."]
Least Specific ["Fixed."], ["Done."], ["Managed."], ["Handled"], ["Settled."]
TABLE II: Success Rates for Language Variations
Scenario Instruction Category Success Rate
Specific 100%
Scenario 1:
Moderately Specific 73%
ComponentOverlap
Least Specific 27%
Scenario 2: Specific 93%
Incorrectly Moderately Specific 87%
Assembled Part Least Specific 53%
Specific 100%
Scenario 3: Missing
Moderately Specific 67%
Component
Least Specific 27%
coordinatesystem,Weusedpredefinedoffsetvalues(x ,y ),
o o
calculated as
V =x ×x , H =y ×y , (2)
o p o p
where V and H adjust the arm’s trajectory based on the
Fig.6:CaseStudyCommunicationResultsforEachScenario
visual sensor input. Concurrently, the rotation control mech-
anismassignsspecifictrajectoryanglestotheroboticgripper, was classified into three distinct categories: specific, mod-
ensuring precise orientation alignment of parts for insertion. erately specific, and least specific. Five unique variations
The cable shark assembly process is shown in Figure 5. of each instruction type were provided across three distinct
tasks. To ensure the reliability of the evaluation, each lan-
D. Case Study Results
guage variation scenario presented in Table I is conducted
Theproposedframeworkwasintegratedintotheassembly
with three repetitions.
system to study the effect of integrating LLMs with a
The results highlight the effectiveness of integrating LLM
knowledgeable operator1. The operator knew the assembly
intothehuman-robotcollaborativeassemblyframework.Fig-
processandwasabletoconverse.Threescenarioswereused
ure 6 showcases the human-robot communication utilizing
to evaluate the proposed framework:
the vision system, outlined in Section IV-B, to ensure task
Scenario 1: The system detects an overlap of the housing
completion in each scenario. In scenario 1, the robot detects
components and requests human intervention.
overlapping components which signal to human operators
Scenario 2: If a wedge component is incorrectly assembled,
for help. Upon resolution, the human operator prompts task
the robot halts and requests human correction.
continuation. Similarly in scenario 2, the robot identifies a
Scenario 3: A missing spring component is detected and
misassembledwedge,notifyingthehumanoperator,andthen
the robot requests the human operator to place the spring
the human commands the robot to continue after manually
component.
assembling the wedge. For Scenario 3, the robot flags a
Basedonthesescenarios,theevaluationfocusedonassess-
missing spring component, requesting human intervention,
ing the system’s proficiency in interpreting and performing
and the human instructs the robot to resume the assembly
tasks derived from human commands, particularly consid-
once the issue is fixed.
ering the linguistic and human diversity. The variations in
Based on the evaluation setup, we assessed the system’s
phrasing and terminology across different commands were
capability to understand and execute commands with varied
cataloged, as presented in Table I, to validate the system’s
language expressions. For tasks with specific instructions,
adaptability to diverse linguistic inputs. The instruction type
success rates were high, averaging approximately 98%. The
rates decreased to 76% with moderately specific instructions
1See supplementary video file for a demonstration of the human-robot
collaborativeassembly. and dropped further to 36% with least specific instructions.The success rates for each category of instruction type are manufacturing environment (C2), and integrating advanced
detailed in Table II. This data suggests a positive correlation technologies into human-robot collaborative assembly pro-
between instruction type and task execution success. cess with a human-centric design to improve usability and
communication (C3). To validate the effectiveness of our
E. Case Study Discussion and Limitations
proposed framework, we implemented a practical setup in-
The case study evaluated the system’s ability to facilitate volving the cable shark device assembly process. This setup
collaboration between humans and robots and showcases showcased the framework’s capability to facilitate intuitive
howtheintegrationofLLMscanleadtoefficientandflexible human-robot communication through voice commands with
manufacturingprocesses.Theresultsdemonstratethatasthe languagevariations.Wedynamicallyadapttotaskvariations
instructions become less specific, the robot’s performance and errors by integrating LLM, sensors, and task control
decreases significantly, indicating that the robot agent needs mechanisms, demonstrating its ability to maintain produc-
well-defined human commands to operate effectively. These tivity and ensure a continuous workflow.
results highlight the limitations of the proposed framework Future work includes extending this LLM-based human-
andsuggestanareaforimprovement.Onesuchimprovement robot collaborative assembly framework by focusing on
involves introducing feedback mechanisms that allow the increasingitsadaptabilitybyfeedingLLMwithawiderange
system to ask for clarification when instructions are unclear, of data, such as various robotic tasks and sensor data. The
refiningtheinteractionbetweenhumansandrobots.Notably, aim is to increase the robot’s flexibility in performing tasks,
the robot’s success was higher with commands that included enhance safety, and enable it to address unexpected errors
keywords like ’fixed’ or ’addressed,’ which means that the andtaskvariations.Additionally,wealsoplantoincorporate
robotunderstandsitsinitialization.Theinitializationprotocol multiple modalities, such as haptic and gestures to improve
informstherobotthatthehuman’sroleistocorrecterrorsto human-robot interaction within the manufacturing environ-
resume the assembly task. Based on the case study, we have ment. Our framework plans to enhance this multimodal
found that the initialization process is crucial as it ensures strategy by integrating LLM to improve communication
that the robot can understand the roles within the system. efficiency,taskadaptability,andintuitiveinteractionbetween
There were also limitations in the case study and eval- humans and robots within the manufacturing environment.
uation of the proposed framework. Specifically, we only
evaluated a limited range of commands from the operator. ACKNOWLEDGMENT
These commands were specifically related to predefined
We thank Dana Smith from DMI Companies, Inc. for
assembly scenarios and did not include other interactions,
providing the case study and offering valuable feedback on
such as human interruptions and task-irrelevant questions
the project.
from the human. The case study also did not study the
variability in the operator’s knowledge, as the framework REFERENCES
assumedoperatorswouldproviderelevantinstructionstothe
assembly task. Furthermore, the operator was not allowed to [1] Paryanto,M.Brossog,M.Bornschlegl,andJ.Franke,“Reducingthe
EnergyConsumptionofIndustrialRobotsinManufacturingSystems,”
change the defined tasks, e.g., variations in positioning, or The International Journal of Advanced Manufacturing Technology,
taskorder.Futureworkwillfocusonanalyzingthedeveloped vol.78,pp.1315–1328,2015.
framework in a variety of manufacturing scenarios. We will [2] L.Wang,S.Liu,H.Liu,andX.V.Wang,“OverviewofHuman-Robot
CollaborationinManufacturing,”pp.15–58,2020.
look to test the proposed system with operators who do not
[3] C. S. Franklin, E. G. Dominguez, J. D. Fryman, and M. L.
have prior knowledge of the assembly process. We will also Lewandowski, “Collaborative Robotics: New Era of Human–Robot
test the proposed framework for other material handling and Cooperation in the Workplace,” Journal of Safety Research, vol. 74,
pp.153–160,2020.
manufacturing tasks.
[4] A.M.Zanchettin,N.M.Ceriani,P.Rocco,H.Ding,andB.Matthias,
“SafetyinHuman-RobotCollaborativeManufacturingEnvironments:
V. CONCLUSIONANDFUTUREWORK MetricsandControl,”IEEETransactionsonAutomationScienceand
Engineering,vol.13,no.2,pp.882–893,2015.
Advancements in LLM are applied to human-robot col-
[5] M.Pearce,B.Mutlu,J.Shah,andR.Radwin,“OptimizingMakespan
laborative assembly for executing robot actions and col- and Ergonomics in Integrating Collaborative Robots into Manufac-
laborating with humans based on environmental data. By turing Processes,” IEEE Transactions on Automation Science and
Engineering,vol.15,no.4,pp.1772–1784,2018.
incorporatingLLM,robotscanbetterunderstandandinteract
[6] M. Javaid, A. Haleem, R. P. Singh, and R. Suman, “Substantial
with human operators to resolve errors and improve execu- Capabilities of Robotics in Enhancing Industry 4.0 Implementation,”
tion based on environmental feedback. Drawing on insights CognitiveRobotics,vol.1,pp.58–75,2021.
[7] K. Bogner, U. Pferschy, R. Unterberger, and H. Zeiner, “Optimised
from these studies, we incorporated LLM into our human-
Scheduling in Human–Robot Collaboration–A Use Case in the As-
robot collaborative assembly framework, enabling dynamic semblyofPrintedCircuitBoards,”InternationalJournalofProduction
responses to task variations. Research,vol.56,no.16,pp.5522–5540,2018.
[8] U. Körner, K. Müller-Thur, T. Lunau, N. Dragano, P. Angerer, and
This research addresses key challenges in human-robot
A.Buchner,“PerceivedStressinHuman–MachineInteractioninMod-
collaborativeassembly,includingdevelopingcommunication ernManufacturingEnvironments—ResultsofaQualitativeInterview
systems that require minimal robotics training (C1), enhanc- Study,”StressandHealth,vol.35,no.2,pp.187–199,2019.
[9] E. Matheson, R. Minto, E. G. Zampieri, M. Faccio, and G. Rosati,
ing adaptability and flexibility to manage real-time changes
“Human–Robot Collaboration in Manufacturing Applications: A Re-
and errors in human-robot collaborative assembly within the view,”Robotics,vol.8,no.4,p.100,2019.[10] L. Floridi and M. Chiriatti, “GPT-3: Its Nature, Scope, Limits, and [18] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay,
Consequences,”MindsandMachines,vol.30,pp.681–694,2020. D.Fox,J.Thomason,andA.Garg,“ProgPrompt:GeneratingSituated
[11] J.Achiam,S.Adler,S.Agarwal,L.Ahmad,I.Akkaya,F.L.Aleman, RobotTaskPlansUsingLargeLanguageModels,”2022.
D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4 [19] K.Lin,C.Agia,T.Migimatsu,M.Pavone,andJ.Bohg,“Text2Motion:
TechnicalReport,”arXivpreprintarXiv:2303.08774,2023. From Natural Language Instructions to Feasible Plans,” Autonomous
[12] N. Mendes, M. Safeea, and P. Neto, “Flexible Programming and Robots,vol.47,no.8,p.1345–1365,Nov.2023.
Orchestration of Collaborative Robotic Manufacturing Systems,” in [20] O.Mees,J.Borja-Diaz,andW.Burgard,“GroundingLanguagewith
2018 IEEE 16th International Conference on Industrial Informatics VisualAffordancesOverUnstructuredData,”2023.
(INDIN). IEEE,2018,pp.913–918. [21] H.Zhang,W.Du,J.Shan,Q.Zhou,Y.Du,J.B.Tenenbaum,T.Shu,
[13] R. Meziane, M. J.-D. Otis, and H. Ezzaidi, “Human-Robot Collabo- andC.Gan,“BuildingCooperativeEmbodiedAgentsModularlywith
rationWhileSharingProductionActivitiesinDynamicEnvironment: LargeLanguageModels,”2024.
SPADERSystem,”RoboticsandComputer-IntegratedManufacturing, [22] A.Z.Ren,A.Dixit,A.Bodrova,S.Singh,S.Tu,N.Brown,P.Xu,
vol.48,pp.243–253,2017. L. Takayama, F. Xia, J. Varley, Z. Xu, D. Sadigh, A. Zeng, and
[14] A.M.Zanchettin,A.Casalino,L.Piroddi,andP.Rocco,“Prediction A. Majumdar, “Robots That Ask For Help: Uncertainty Alignment
ofHumanActivityPatternsforHuman–RobotCollaborativeAssembly forLargeLanguageModelPlanners,”2023.
Tasks,” IEEE Transactions on Industrial Informatics, vol. 15, no. 7, [23] G. Zheng, I. Kovalenko, K. Barton, and D. Tilbury, “Integrating
pp.3934–3942,2018. HumanOperatorsintoAgent-BasedManufacturingSystems:ATable-
[15] J.deGeaFernández,D.Mronga,M.Günther,T.Knobloch,M.Wirkus, Top Demonstration,” Procedia manufacturing, vol. 17, pp. 326–333,
M. Schröer, M. Trampler, S. Stiene, E. Kirchner, V. Bargsten et al., 2018.
“Multimodal Sensor-Based Whole-Body Control for Human–Robot [24] UFACTORY,Nov2023.[Online].Available:https://www.ufactory.cc/
Collaboration in Industrial Settings,” Robotics and Autonomous Sys- [25] OpenAI, “Speech to Text.” [Online]. Available: https://platform.
tems,vol.94,pp.102–119,2017. openai.com/docs/guides/speech-to-text
[16] D. Wei, L. Chen, L. Zhao, H. Zhou, and B. Huang, “A Vision- [26] ——, “Text to Speech.” [Online]. Available: https://platform.openai.
BasedMeasureofEnvironmentalEffectsonInferringHumanIntention com/docs/guides/text-to-speech
During Human Robot Interaction,” IEEE Sensors Journal, vol. 22, [27] ——,“FunctionCalling.”[Online].Available:https://platform.openai.
no.5,pp.4246–4256,2021. com/docs/guides/function-calling
[17] W.Huang,P.Abbeel,D.Pathak,andI.Mordatch,“LanguageModels [28] G.Jocher,“YOLOv5byUltralytics,”May2020.[Online].Available:
asZero-ShotPlanners:ExtractingActionableKnowledgeforEmbod- https://github.com/ultralytics/yolov5
iedAgents,”2022.