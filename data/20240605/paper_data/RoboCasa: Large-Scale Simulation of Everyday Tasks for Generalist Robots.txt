RoboCasa: Large-Scale Simulation of
Everyday Tasks for Generalist Robots
Soroush Nasiriany1, Abhiram Maddukuri1,∗, Lance Zhang1,∗, Adeet Parikh1,
Aaron Lo1, Abhishek Joshi1, Ajay Mandlekar2, Yuke Zhu1,2
1The University of Texas at Austin, 2NVIDIA Research; ∗Denotes equal contribution
robocasa.ai
Fig.1:OverviewofRoboCasa.RoboCasaisasimulationframeworkfortraininggeneralistrobotagents.FourpillarsunderlieRoboCasa:(1)Diverseassets,
including120kitchenscenesand2,500+3Dobjects,createdwiththeaidofgenerativeAItools;(2)Cross-embodimentsupportformobilemanipulatorsand
humanoidrobots;(3)Diversetaskscreatedwiththeguidanceoflargelanguagemodels;(4)Massivetrainingdatasetswithover100Ktrajectories.
Abstract—Recent advancements in Artificial Intelligence (AI) I. INTRODUCTION
have largely been propelled by scaling. In Robotics, scaling is
hindered by the lack of access to massive robot datasets. We Recent breakthroughs in Artificial Intelligence have been
advocate using realistic physical simulation as a means to scale
driven by training giant neural network models on Internet-
environments, tasks, and datasets for robot learning methods.
scale datasets. Unlike computer vision and natural language
We present RoboCasa, a large-scale simulation framework for
training generalist robots in everyday environments. RoboCasa processing domains, where massive visual and text data are
features realistic and diverse scenes focusing on kitchen en- abundantfromonlinesources,roboticdataisrelativelyscarce.
vironments. We provide thousands of 3D assets across over A key question in Robotics is how to acquire robotic training
150 object categories and dozens of interactable furniture and
datathatcapturesthevastdiversityandcomplexityofthereal
appliances.Weenrichtherealismanddiversityofoursimulation
world. Several prominent recent attempts have been made to
with generative AI tools, such as object assets from text-to-3D
modelsandenvironmenttexturesfromtext-to-imagemodels.We createlarge,diversedatasetsfortraininggeneralistrobotmod-
design a set of 100 tasks for systematic evaluation, including els [2, 9, 5, 20]. While these datasets have advanced robots’
composite tasks generated by the guidance of large language generalization abilities in narrow domains, there remains a
models. To facilitate learning, we provide high-quality human
considerable gap between the capabilities achieved thus far
demonstrations and integrate automated trajectory generation
and general-purpose robots that can be reliably deployed in
methods to substantially enlarge our datasets with minimal
human burden. Our experiments show a clear scaling trend in thewild.Itraisesthequestion—whatisaviablepathforward
usingsyntheticallygeneratedrobotdataforlarge-scaleimitation toward scaling in robot learning?
learning and show great promise in harnessing simulation data
As collecting ever-larger datasets in the real world would
in real-world tasks. Videos and open-source code are available
require unrealistic amounts of capital and labor, many turn
on the project website.
to simulation as a promising alternative to producing large
4202
nuJ
4
]OR.sc[
1v32520.6042:viXraquantities of synthetic data for model training. We expect skills. We design these composite tasks to capture naturalistic
simulation to play an integral role in scaling robot learning kitchenactivitiesbysolicitingsuggestionsfromlargelanguage
for the following reasons. First, once a feature-rich, high- models (LLMs). Our key intuition is that LLMs are trained
fidelity simulator is created, we can generate large amounts on human-centered Internet content, effectively capturing the
of robot data at low cost. This is exemplified by recent ecological statistics of human behaviors. We obtain a list of
automated data generation methods, such as MimicGen [34] activities from LLMs, such as washing dishes, frying, and
and Optimus [6], which exploit the privileged information restocking cabinets. Using these activities to ground our task
of simulation to generate data with minimal human labor. design,weprompttheLLMtosuggestconcretetasksforeach
Second, the creation of realistic simulations has been facil- activity. We look to expand the list of tasks in future releases.
itated by rapid advances in generative AI. Today’s generative We complement our tasks with high-quality human demon-
AI tools are capable of generating images, synthesizing 3D strations across all 100 tasks. To augment our datasets, we
assets, and writing source code [38, 35, 42]. These tools extendMimicGen[34]togenerate100Kadditionaltrajectories
can be employed to create millions of scenes procedurally, forouratomictasks.Wetrainpolicieswithbehavioralcloning
import novel categories of objects, and program natural tasks on human demonstrations and automatically generated data.
and reward functions. Finally, simulation democratizes and We find that generated data significantly improves general-
acceleratesrobotlearningresearch,enablingrapidprototyping ization, hinting at a promising path for scaling in robotics.
of new ideas and reproducible research. Furthermore, we show in a real-world kitchen environment
To unleash the potential of simulation, it must satisfy three thatco-trainingwithoursimulationdatasignificantlyincreases
core criteria. First, the simulator must guarantee realism in task success in real-robot deployment. We summarize our
physics, rendering, and underlying models to enable transfer contributions as follows:
totherealworld.Second,thesimulatormustsatisfydiversity • We develop the RoboCasa simulation framework fea-
inthescenes,assets,andtasksitoffers.GenerativeAIwillbe turing diverse, realistic kitchen scenes, thousands of
crucial in enabling this diversity at scale. Finally, a simulator high-quality object assets, and cross-embodiment mobile
aloneisnotsufficienttotrainahighlycapablegeneralistrobot manipulators. We employ generative AI tools to create
agent. The simulation must be accompanied by large robot environment textures and 3D objects.
datasetsthatcapturethediversityofscenesandbehaviorsthat • Weintroduceasetof100tasksforsystematicevaluation,
ithastooffer.Numerouspriorattemptsatcreatingsimulations including 25 atomic tasks representing foundational sen-
have partially satisfied some of these criteria, yet none have sorimotor skills and 75 composite tasks generated with
satisfied all. the guidance of large language models.
WepresentRoboCasa,alarge-scalesimulationframework • We provide a large multi-task dataset for model training,
centered around home environments for training generalist including large-scale synthetically generated trajectories.
robots. RoboCasa builds upon RoboSuite [51], a modular, Weshowaclearscalingtrendwhenusinggenerateddata
fast, and easy-to-use framework based in MuJoCo. RoboCasa andshowtheutilityofsimulationdatainreal-worldtasks.
inherits these features and goes far beyond by offering a large
array of scenes, objects, and hardware platforms suited for
II. RELATEDWORK
building a general-purpose home robot. In this initial release, Simulation Frameworks for Robotics. Many simulation
wefocusoureffortsonkitchenscenes.Tocapturerealisticand frameworkshavebeenbuiltforrobotics—weprovideathor-
diverse scenes, we consult numerous architecture and home ough comparison between RoboCasa and popular frameworks
design magazines and compile several kitchen layouts and in Table I. Some are limited to tabletop manipulation [33,
styles reflecting the diversity of kitchens in homes around the 15, 6, 28], but RoboCasa, along with others [21, 40, 26, 27,
world. We model these kitchens according to standard size 10, 34], support mobile manipulation. RoboCasa also sup-
and spatial specifications and fit them with a large repository ports room-scale scenes, unlike other frameworks that include
of interactable furniture and appliances spanning cabinets, mobile manipulation in smaller portions of a room [10, 34].
stoves, microwaves, coffee machines, and more. Furthermore, RoboCasa runs realistic physics for all interactions, including
we curate a repository of over 2,500 objects across over 150 object grasping and placement, unlike some other mobile
categories, the majority of which are generated by text-to- manipulationframeworkssuchasAI2-THOR[21]andHabitat
3Dtools.RoboCasahascross-embodimentsupportformobile 2.0 [40]. RoboCasa is one of the few frameworks to feature
manipulators of diverse forms, such as single-arm mobile photorealistic rendering [21, 40, 27, 10, 6] and multiple
platforms, humanoid robots, and quadruped robots with arms. robot embodiments [21, 26, 27, 6, 34]. RoboCasa also has
Theseassetsallowustosimulateawiderangeofbehaviors a large collection of tasks, room-scale scenes, and objects
in kitchen scenes. This release includes 100 tasks for sys- — only a small number of other works [27, 41] offer these
tematic evaluation. The first 25 are atomic tasks that feature at scale. Meanwhile, recent work in incorporating generative
foundationalrobotskills,suchaspickingandplacing,opening AI tools have explored AI-generated tasks [44] and scene
andclosingdoors,andtwistingknobs.Theyserveasthebasic configurations [45]. But critically, RoboCasa is the only one
building blocks to scaffold complex long-horizon tasks. The to support a large array of tasks, room-scale scenes, and
other 75 are composite tasks involving a sequence of robot objects while incorporating AI-generated tasks and assets,Feature R o b o C asa AI2-T H O R H a bitat 2.0 iGibso n 2.0 R L B e n c h B e h avior-1 K ro b o mimic M a niS kill 2 O P TIM U S LIB E R O Mimic G e n
Mobile Manipulation ✓ ✓ ✓ ✓ ✗ ✓ ✗ ✓ ✗ ✗ ✓
Room-Scale Scenes ✓ ✓ ✓ ✓ ✗ ✓ ✗ ✗ ✗ ✗ ✗
Realistic Object Physics ✓ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
AI-generated Tasks ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗
AI-generated Assets ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗
Photorealism ✓ ✓ ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✗ ✗
Cross-Embodiment ✓ ✓ ✗ ✓ ✗ ✓ ✗ ✗ ✓ ✗ ✓
Num Tasks 100 - 3 6 100 1000 8 20 10 130 12
Num Scenes 120 - 1 15 1 50 3 - 4 20 1
Num Object Categories 153 - 46 - 28 1265 - - - x -
Num Objects 2509 3578 169 1217 28 5215 15 2144 72 x 40
Human Data ✓ ✗ ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✓ ✓
Machine-Generated Data ✓ ✗ ✗ ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✓
Num Trajectories 100K+ - - - - 0 6K 30K 245K 5K 50K
TABLEI:ComparisontoPopularSimulationFrameworksusedintheRobotLearningLiterature.
ensuring potentially limitless diversity in scenes and tasks. This recently proposed data generation system synthesizes
Furthermore, unlike many other simulation platforms (includ- additionaldemonstrationsusingasetofhumandemonstrations
ing Behavior-1K [27]) we provide large-scale datasets of task to generate a much larger dataset.
demonstrationsthroughacombinationofhumanteleoperation Learning from Large Offline Datasets. Behavioral
and the MimicGen system [34] (more discussion below) and Cloning [37] is a popular method for learning policies offline
provide a thorough analysis of agents trained via imitation from a set of demonstrations. It trains a policy to imitate the
learning across our large collection of tasks. The convergence actions in the dataset. It has been used extensively in prior
of diverse scenes, tasks, and assets alongside the extensive works [49, 31, 9, 2, 16, 6, 17]. Offline Reinforcement Learn-
datasetprovidedbyRoboCasawillfulfillacrucialrequirement ing [25] is an alternative method that tries to prefer certain
notaddressedbyanyothersimulationframeworksintherobot dataset actions over others using a reward function. It has
learning community. also been used to learn from large offline robot manipulation
datasets [19, 3, 11, 23, 22]. In this work, we use Behavioral
Datasets and Benchmarks for Robotics.Recently,several
Cloning using a Transformer-based [43] visuomotor policy
large-scaledatacollectioneffortshavebeenmadeforrobotics.
similar to other works [6] to train agents on large offline
One approach is self-supervised learning, where trial-and-
datasets. We also consider other popular policy architectures,
error is used to collect data for tasks like grasping and
namely diffusion models [14, 4].
pushing[24,36,18,19,46,7].Thiscantakesignificanttimeto
generate high-quality data due to the trial-and-error process.
A popular alternative is to collect robot demonstrations via III. ROBOCASASIMULATION
humanteleoperation,whereahumancontrolsarobottoguide
We outline the core simulation components of RoboCasa.
itthroughdifferenttasks[49,29,30,32,9,16].Severalrecent
We highlight our efforts to create diverse and realistic kitchen
effortshavescaledthisparadigmupbyusingteamsofhuman
scenes, furniture and appliances, and objects.
operatorsoverextendedperiodsoftime[9,1,16,2].However,
most of these efforts focus on collected real-world datasets.
By contrast, we focus on collecting large-scale datasets in A. Core Simulation Platform
simulation,whereresultsareeasiertoreproduceandthorough
WeadoptRoboSuite[51]asthecoresimulationplatformon
evaluations are possible due to lower human burdens.
whichwedevelopRoboCasa.WechoseRoboSuitebecauseof
Another approach is to leverage algorithmic trajectory gen- itsfocusonphysicalrealism,highspeed,andmodulardesign,
eratorsinsimulation[15,48,17,10,6],buttheseeffortsoften which allows us to scale to large-scale scenes. We directly
makeuseofprivilegedinformationandhand-designedheuris- inherit several core components of RoboSuite, including the
tics, and can consequently be difficult to apply to arbitrary environmentmodelformatsandrobotcontrollers.Crucially,in
tasks without significant human effort. Some recent efforts order to support room-scale environments, we extend Robo-
have used Large Language Models to generate datasets in Suite to accommodate mobile manipulators, including robots
simulation[45,12],butitcanstillinvolvecarefullyengineered mounted on wheeled bases, humanoid robots, and quadrupeds
pipelines. To combine the quality and wide applicability of with arms. We obtain and adapt these models from various
human teleoperation with the scale of using pre-programmed roboticsrepositories[51,47,13].Wealsosupporthigh-quality
demonstratorsinsimulation,wecollectasetofhumandemon- rendering with NVIDIA Omniverse, allowing us to capture
strations in simulation and then leverage MimicGen [34]. photorealistic images (see Figure 1).One wall One wall w/ island L-shaped L-shaped w/ island Galley
U-shaped U-shaped w/ island G-shaped G-shaped (large) Wraparound
Fig.3:KitchenFloorPlans.Weconsulthomeplanningandarchitecturemagazinesandcompilealistofcommonkitchenfloorplans.Ourfloorplanstake
onavarietyofshapesandsizes,frombasicdesigns(e.g.,onewall)tohigh-endones(e.g.,U-shapedw/island).
B. Kitchen Scenes
In this initial release, we focus on household tasks centered
around kitchen activities. We created a large array of kitchen
sceneswithfullyinteractivecabinets,drawers,andappliances.
We consulted online home design and architecture magazines
to compile a diverse list of kitchen floor plans. We model
10 floor plans (Figure 3) ranging from basic designs found
in apartments to more elaborate designs found in high-end
homes. Each kitchen can be configured to take on a custom
architecturalstyle.Afterconsultingarchitecturemagazines,we Fig. 4: Examples of Interactable Appliances. Our simulation framework
compilethepopularkitchenstyles,includingIndustrial,Scan- comeswithdozensofappliances.Severaltypesofappliancesarearticulated.
For example, we can open and close doors on microwaves and twist knobs
dinavian,Coastal,Modern,Traditional,Mediterranean,Rustic,
onstoves.Someappliancescanundergostatechanges,e.g.,whenweturnthe
and more. Each style features a unique combination of design knobonthestove,thecorrespondingburnerturnson.
elements, including textures, appliance choices, and cabinet
panels and handles. For example, Scandinavian kitchens em-
ploy light, low-contrast textures and simple, sleek cabinet and the knobs on a stove. It allows us to represent rich
panelsandappliances.Incontrast,Mediterraneankitchensuse interactions, such as closing a microwave door or turning on
ornate appliances, glass panel cabinets, and colorful textures. a stove. Furthermore, these appliances undergo state changes,
Intotal,wehavemodeled12kitchenstyles,andweshowcase e.g., when we turn a stove knob on, the corresponding burner
these styles across different floor plans in Figure 1. Each turns on to simulate heat. See Figure 4 for an illustration of
floor plan can be configured to take on any style, resulting our appliances.
in 120 kitchen scenes. Each scene can be customized further Inadditiontoappliances,wecreatearichlibraryofobjects
by replacing textures from a large selection of high-quality commonly found in kitchens, spanning fruits and vegetables,
AI-generated textures. We have 100 textures for walls, 100 dairy, poultry, drinks, receptacles, tools, and more. We gather
for the floor, 100 for counters, and 100 for cabinet panels. object assets from two sources, the Objaverse [8] dataset and
Weusethepopulartext-to-imagetoolMidJourneytogenerate Luma.ai, an online text-to-3D service. We mine a large set of
these images. We use these textures as a form of domain candidate objects and filter out defective or low-quality ones.
randomization to significantly increase the visual diversity of Attheendofthisprocess,wecollect2,509high-qualityassets
our training datasets. spanning 153 unique object categories. The majority of these
assets (1,592) are sourced from Luma.ai. See Figure 5 for an
C. Assets
illustration of our objects.
We create a large repository of intractable 3D assets to ac-
commodate diverse kitchen activities. Our repository includes IV. ROBOCASAACTIVITYDATASET
cabinets, drawers, and various kitchen appliances. We source
these assets from 3D model repositories online and convert Our simulator supports a wide array of possible kitchen ac-
them to the MuJoCo MJCF model format. Our postprocess- tivities,andwerepresenttheseactivitieswithacomprehensive
ing operation involves segmenting appliances into articulated suite of 100 tasks. This section outlines these tasks and our
entities, for example, segmenting the door of a microwave large multi-task dataset accompanying them.code implementations for them. Except for a select number
of composite tasks designed to work in certain environments,
all tasks are simulatable in any of our kitchen scenes. We
describe in detail our prompts and tasks in the appendix.
C. RoboCasa Datasets
We have outlined a comprehensive set of 100 tasks consist-
ing of 25 atomic tasks (Sec. IV-A) and 75 composite tasks
created with LLMs (Sec. IV-B). This section explains how
we collect our large-scale demonstration dataset across these
tasks.Wefirstusehumanteleoperationtocollectabasesetof
demonstrations and then use automated trajectory generation
methodstoexpandthistoamuchlargersetofdemonstrations.
Fig. 5: Diverse High-Quality 3D Objects. RoboCasa offers 2,509 high-
quality3Dobjectsacross153diversecategoriesspanningvegetables,poultry, Collecting a base set of demonstrations through human
drinks,andmore.Hereweillustrateasmallsubsetoftheseobjects. teleoperation. A team of four human operators collect 50
high-quality demonstrations for each atomic task using a 3D
A. Atomic Tasks: Building Blocks of Behavior SpaceMouse [50, 51]. Each task demonstration is collected in
For a robot to perform complex tasks, it must master the a random kitchen scene (random kitchen floor plan, random
foundational skills needed to solve these tasks. We focus on kitchen style, and random AI-generated textures). This results
a set of eight sensorimotor skills that form the basis for the in large and diverse simulation datasets through human tele-
majorityofhouseholdactivities:1)Pickandplace,2)Opening operation (1,250 demonstrations). However, our experiments
andclosingdoors,3)Openingandclosingdrawers,4)Twisting showthateventhisscaleofhumandataisinsufficienttosolve
knobs,5)Turninglevers,6)Pressingbuttons,7)Insertion,and most of our tasks. This is likely due to the significant scope
8) Navigation. These skills do not constitute an exhaustive and diversity of the tasks and scenes. Consequently, we opt to
list, and including additional skills centered around behaviors use data generation tools to expand our data quantity.
such as deformable manipulation is left for future work. To Leveraging automated trajectory generation methods to
effectively learn the skills, we propose a set of 25 tasks that synthesizedemonstrations.Tofurtherscalethedataset’ssize
each involve one of these eight skills. We will refer to these with minimal human effort, we employ MimicGen [34], a
as atomic tasks. The full breakdown of these tasks is outlined recently developed trajectory generation method. MimicGen
in the appendix (Figure 11). can automatically synthesize rich datasets from a seed set
of human demonstrations by adapting them to new settings.
B. Creating Composite Tasks with Large Language Models
The core generation mechanism first decomposes each human
Our composite tasks involve sequencing skills to solve se- demonstration into a sequence of object-centric manipulation
manticallymeaningfulactivitiessuchascookingandcleaning. segments. Then, for a novel scene, it transforms each object-
Ourgoalincreatingthesetasksistocapturediversetasksthat centric segment according to the current pose of the relevant
reflect the ecological statistics of real-world household activ- object,stitchesthesegmentstogether,andhastherobotfollow
ities. We use the guidance of large language models (LLMs) the new trajectory to collect a new task demonstration.
to define our tasks. This approach offers several key benefits. MimicGen requires some basic assumptions on simulation.
First, LLMs encapsulate diverse sources of human knowledge We outline how they are easily satisfied in RoboCasa: Mim-
and can thus effectively communicate diverse ideas grounded icGen assumes that tasks consist of a known sequence of
intherealworld.Inaddition,theseLLMscanbeusedatscale object-centric subtasks — this sequence must be specified
to define thousands of unique tasks, significantly reducing the for each new task. Fortunately, the atomic tasks (Sec. IV-A)
human labor involved in task definition. We generate tasks consist of eight core skills. All tasks that correspond to a skill
across two steps (see Figure 6). First, we prompt ChatGPT have the same or similar sequences of object-centric subtasks,
(GPT-4 [35]) to list common high-level kitchen activities. We with the main differences coming from the identity of the
compile a list of 20 activities: brewing coffee or tea, washing reference object. For example, for pick-and-place tasks, the
dishes, restocking kitchen supplies, chopping food, making first stage is a pick subtask with one reference object, and
toast,defrostingfood,boilingwater,meatpreparation,setting the second stage is a place subtask with a second reference
the table, clearing the table, sanitizing, snack preparation, object.Consequently,specifyingsubtasksequencestakesmin-
tidying cabinets and drawers, washing fruits and vegetables, imal human effort. In addition, each human demonstration
frying, reheating food, mixing and blending, baking, serving provided to MimicGen must also be annotated with segments
food, and steaming vegetables. We then prompt GPT-4 and correspondingtoeachobject-centricsubtask.Thiscanbedone
Gemini 1.5 [42] to propose representative tasks for each with automated metrics that detect the end of each subtask —
activity label. The LLMs occasionally exhibit logical flaws, these functions only need to be implemented once for each of
so we filter or modify some of their outputs. We compile the eight core skills and can be re-used across the entire set
75 task blueprints in total from the LLM and proceed to of demonstrations.Activity Prompting GPT-4 List of activitie  Task Generation Process
Chopping Foo 
Can you give me 30 simple ...   Fryin  pick(vegetable) place(microwave)
everyday kitchen activities?   Serving Food ...
...
Task Prompting
Task: Prepare Microwave Steaming



Goal: Put a bowl of vegetables inside place(bowl) close_door(microwave)
Your goal is to come up with 15 ... the microwave to steam them there.


unique tasks that a robot can
Objects: bowl, vegetables


complete that all fall under
Fixtures: sink, microwave


{ACTIVITY FROM GPT}.

 ...
Skills (6): 
pick(vegetable 
Available objects and skills:
   put(bowl  place(bowl) press(microwave)
...
   pick(bowl 
Example tasks:
   place(microwave 
...   close_door(microwave 
press(microwave)




Fig.6:CreatingDiverseTaskswithLargeLanguageModels.WeemployLLMstogeneratediversetasks.First,wepromptGPT-4togivediversehigh-level
kitchenactivities.Subsequently,foreachactivity,wepromptGPT-4(orGemini1.5)tosuggestadiversesetofrepresentativetasks.Weillustrateonesuch
task“PreparingtheMicrowaveForSteaming”fortheactivitylabel“SteamingVegetables”.
MimicGen data generation attempts are not always suc- We take the 50 human demonstrations as input for
cessful. In practice, it employs a rejection sampling scheme each task and use them to generate 3,000 trajectories
only to keep generation attempts that lead to task success. autonomously.
Leveraging RoboCasa simulation, we parallelize MimicGen 3) Generated-300: A random 1/10 subset of our full gen-
data generation across multiple simulation processes to speed erated dataset, where we generate 300 demos per task.
up the data generation process. This results in a total of 7,200 trajectories.
V. EXPERIMENTS 4) Generated-100: A random 1/30 subset of our full gen-
We aim to explore the following research questions in our erated dataset, where we generate 100 demos per task.
experiments: This results in a total of 2,400 trajectories.
1) How effective are machine-generated trajectories from ImagesinthetrainingdatasetsarerenderedwithAI-generated
MimicGeninlearningmulti-taskpolicies,incomparison textures. We evaluate our policies in kitchen scenes with
to human demonstrations? human-curated textures. In these datasets, our focus is specif-
2) Howwillthegeneralizationperformanceoftheimitation ically on a Franka Panda robot with an Omron mobile base,
learning policy scale with increasing training dataset resembling the Omni-Frankie robot [13]. We train a visuo-
sizes? motor policy with behavioral cloning on each of these four
3) Can large-scale simulation datasets facilitate knowledge multi-task datasets. We specifically use the publicly available
transfer to downstream tasks within simulation and fa- BC-TransformerimplementationinRoboMimic[33].Referto
cilitate policy learning for real-world tasks? Section IX for additional details.
After training, we perform a comprehensive evaluation of
A. Imitation Learning for Atomic Tasks
the model. For each task, we evaluate the model performance
First, we perform a systematic study with the atomic tasks. across 50 trials across five fixed evaluation scenes, each with
One question we are interested in is how the performance of a distinct floor plan and style. In order to test generalization
imitation learning policies compare when trained on human capabilities,weonlyevaluatethepolicyonlyonunseenobject
data versus machine-generated data and how the scale of this instances. Additionally, two of the five scenes encompass
data plays a role in performance. We compare the following unseenstylesthatwereneverencounteredinthetrainingdata.
four multi-task data settings: We report results in Figure 7 where we group results together
1) Human-50: A dataset of 1250 human demonstrations with tasks belonging to the same skill. The overall perfor-
spanning all 25 atomic tasks, each with 50 human mance on human data is 28.8% success rate, and with the
demonstrations. fully generated dataset, we observe a significant improvement
at 47.6% success rate. Furthermore, we observe a scaling
2) Generated-3000: A dataset of 72,000 demonstrations
synthesized by MimicGen1 across 24 atomic tasks2 trend from using machine-generated data: as we increase the
quantity of generated data, the model performance increases
1TheseexperimentsfeatureObjaverseobjects.Wereleaseanadditional28K steadily.Thisoffersapromisingoutlook:datagenerationtools
trajectoriesfeaturingAI-generatedobjects,formingthefull100Ktrajectory enable us to learn significantly more performant agents at a
dataset.
relatively low cost. We observe several other notable findings
2Weexcludethekitchennavigationtask,asMimicGenisnotcurrentlyable
togeneratemobilemanipulationtrajectories.Weleavethistofuturework. intheresults.Someskillsaresignificantlyeasiertolearn(e.g.,80
70
60
50
40
30
20
10
0
Pick and Place Open/Close Doors Open/Close Drawers Twisting Knobs Turning Levers Pressing Buttons Insertion Overall
Human-50 Generated-100 Generated-300 Generated-3000
Fig.7:Comparisonbetweenhumandemonstrationsandmachine-generateddatasets.Wepresentlearningresultsacross24atomictasksspanningdiverse
robotskills.Wecomparetrainingonfourdifferentmulti-taskdatasets,includingahumandatasetwith50demonstrationspertask,amachinegenerateddataset
with 3000 demonstrations per task, and smaller variants with 300 or 100 demonstrations per task. We group task results according to their corresponding
sensorimotorskills(see Figure13forafullbreakdownofresultsbytask).Weseeaclearscalingtrend:increasingthesizeofthegenerateddatasetcanyield
consistentlyhigheroverallsuccessrates,eventuallysignificantlyoutperformingperformanceonhumandatasets.
openingandclosingdoorsanddrawers),whileothersarequite Scratch Fine-tuning
challenging (e.g., pick and place). We hypothesize a number ArrangeVegetables 2.0% 12.0%
of factors explaining these findings. First, tasks exhibiting MicrowaveThawing 0% 2.0%
high diversity are significantly more challenging to learn. RestockPantry 0% 6.0%
PreSoakPan 0% 4.0%
One example is the pick and place tasks involving dozens of
PrepareCoffee 0% 0%
differentobjectcategorieswithawiderangeofaffordances.In
comparison, opening and closing doors involves six different Fig.8:LearningResultsonCompositeTasks.Welearnsingle-taskpolicies
instances of doors and is thus significantly easier to learn. forfiverepresentativecompositetasks.Wecomparelearningthesetasksfrom
scratchwith50humandemonstrationsversusfine-tuningapolicytrainedon
Anotherfactorisdexterity,asweseethattasksinvolvinghigh
machine-generatedatomictaskdata.Thefine-tuningmethodperformsbetter
levels of dexterity, such as insertion, are challenging to learn. butstillstrugglestolearnrobustbehaviors.
B. Imitation Learning for Composite Tasks
Next we study learning on our composite tasks. These
tasks are more challenging as they require multiple skills, cabinet (either on the right or left side) and place the
which increases the horizon of the task and introduces new new cans right next to them;
subtleties. Due to the increased difficulty of these tasks and
• PreSoakPan:Thistaskbelongstothe“washingdishes”
thechallengesofmulti-tasklearning,weopttolearnasingle-
activity.Therobotmustpickandplaceapanandasponge
taskpolicyforeachtask.Foreachtask,wecollected50human
into the sink and turn on the water faucet to prepare the
demonstrations and compared the following settings:
pan for washing;
• Scratch: learning a policy from scratch on these 50 • PrepareCoffee: This task belongs to the “brewing
demonstrations; coffee of tea” activity. The robot must take a mug out of
• Fine-tuning: taking our pre-trained policy learned on the cabinet, place it under the coffee machine, and press
atomic tasks with the full MimicGen generated dataset the coffee machine button to serve it into the mug.
and fine-tuning on these 50 demonstrations.
We independently train models for the following five tasks: See Figure 8 for results. Learning on these composite tasks
isverychallenging,withtheScratchbaselinefailingtoachieve
• ArrangeVegetables:Thistaskbelongstothe“chop-
anynon-zerosuccessrateon4/5tasks.Thefine-tuningmethod
ping food” activity. The robot must place two vegetables
achieves non-zero success rates on 4/5 tasks. Some common
from the sink onto the cutting board on the counter;
failuremodesincludedifficultywithfine-grainedmanipulation
• MicrowaveThawing: This task belongs to the “de- and difficulty effectively transitioning to the next stage of
frosting food” activity. The robot must place a frozen the task. However, we generally observe that the fine-tuned
food item from the counter inside the microwave and models perform better qualitatively, with more robust picking
turn on the microwave; and placing strategies in particular. We attribute this to the
• RestockPantry: This task belongs to the “restocking large pretraining dataset of atomic behaviors. Our benchmark
kitchen supplies” activity. The robot must pick and place leaves room for significant improvement on these tasks. The
multiple cans from the counter to the cabinet. There are choice of policy architecture, learning algorithm, and fine-
a number of cans already in the cabinet, among other tuning strategy may play a critical role in performance, and
objects. The robot must locate the existing cans in the these factors warrant investigation in future work.
)%(
etaR
sseccuSSetting Task Real only Real+Sim (Ours)
Counter to sink 12.7 ± 2.5 22.0 ± 2.8
Seen Obj
Sink to counter 20.0 ± 5.9 29.3 ± 4.1
Counter to cabinet 8.0 ± 1.6 22.0 ± 5.8
Task average 13.6 24.4
Counter to sink 3.3 ± 4.7 8.9 ± 7.9
Unseen Obj
Sink to counter 1.1 ± 1.6 7.8 ± 4.2
Fig. 9: Real-World Experiment Setup. We conduct experiments in a real- Counter to cabinet 3.3 ± 4.7 11.1 ± 11.0
world kitchen environment with a Franka Emika Panda arm on a wheeled
mobileplatform.Thetwopicturesillustratetwoofthethreeevaluationtasks: Task average 2.6 9.3
(left) pick and place an object from the counter to the cabinet, and (right)
pickandplaceanobjectfromthecountertothesink. Fig. 10: Real Robot Evaluations. In a real-world kitchen domain with
only a handful of demonstrations, we explore co-training policies with our
simulationdata.Comparedtotrainingpoliciesexclusivelyonin-domainreal-
worlddemonstrations,co-trainingsubstantiallyimprovespolicyperformance.
C. Transfer to Real World Environments
VI. CONCLUSION
We show how large-scale data generated in simulation
can aid in learning tasks in RoboCasa and other domains, We have presented RoboCasa, a large-scale simulation
including in the real world. We conduct experiments in a framework for training generalist robots in everyday envi-
real-world kitchen environment with a Franka Emika Panda ronments. RoboCasa features 120 realistic scenes, dozens of
robot running on the DROID hardware infrastructure [20]. appliances, 2,500+ high-quality 3D objects spanning 150+
While both the real world and simulated Franka robot are categories, 100 diverse tasks, and a large multi-task dataset
controlled via workspace end effector control, our simulated of 100K+ trajectories. Generative AI tools play a central role
robotusesOperationalSpaceControlwhiletheDROID-based in our simulation, with object assets from text-to-3D models,
real robot does not. In addition, our robot controller runs at environment textures from text-to-image models, and LLMs
20Hzfrequencywhiletherealrobotcontrollerrunsat15Hz. to generate kitchen activities and corresponding tasks. In our
In addition to controller differences, there are differences in experiments, we show that synthetically generated data in
camera calibration, lighting conditions, and the placement of simulation can be useful in scaling robot policy learning.
the robot base with respect to the scene.
We now pinpoint limitations and discuss exciting avenues
Our experiments include three tasks in the real kitchen for future future. First, our experiments show that fine-tuning
environment: 1) pick and place an object from the counter on composite tasks yields relatively low performance, leaving
to the sink, 2) pick and place an object from the sink to the room for improvement. In the future, we will investigate
counter,and3)pickandplaceanobjectfromthecountertothe more powerful policy architectures and learning algorithms
cabinet. These tasks resemble single-stage tasks in RoboCasa. and improve the quality of our machine-generated datasets.
For each task, we collected 50 demonstrations, each over five While the generated trajectories are technically considered
distinct object categories. We train a policy for each task, and successful, many exhibited undesirable effects, such as jerky
we compare the following two settings: motions and collisions. Many of these behaviors can be
automatically detected by checking simulation states and tra-
• Realonly:trainingonthereal-worlddemosforthetarget jectories exhibiting such behaviors can be discarded. Next,
task only; whileweshowedhowtouseLLMstocreatetasks,thisprocess
• Real+Sim: co-training on real-world demos for the tar- still required human guidance to write the implementations
get task and all of our simulation MimicGen demonstra- for these tasks. One interesting research direction is to use
tions over all single-stage tasks. LLMs to propose thousands of new scenes and tasks and
write code to implement these scenes and tasks with minimal
In Figure 10, we report policy success rates (mean and stan- human guidance. We anticipate that this will be possible as
darddeviation,inpercentage)averagedover3seeds.Foreach LLMs become more performant. In addition, we would like
seed, we evaluate the model over five seen object categories to extend the scope beyond kitchen environments and tasks
and 3 unseen object categories (unseen with respect to the in future releases. Next, our dataset consists of critical coarse
real-world demonstrations). On seen objects, we see that co- manipulationbehaviorsbutdoesnotencapsulatehighlydexter-
training with simulated data yields a 24.4% average success ousskills,deformablemanipulationtasks,ortasksthatrequire
rate, compared to 13.6% with using real data only, a relative bimanualmanipulation.Finally,weareinterestedinexploring
improvement of 79%. While performance suffers on unseen trainingusingacombinationofdatafromoursimulation,other
objects,westillseeasignificantimprovementinincorporating simulators,anddiversesourcessuchasinternetvideosandreal
simulation data. We attribute this to the rich diversity and robot datasets. We envision a future in which these diverse
visual and physical realism of our simulator. forms of data complement each other to create a powerful
foundation model for robotics.ACKNOWLEDGMENTS domaindatasets.InRobotics:ScienceandSystems(RSS),
2022.
We would like to thank Yifeng Zhu for significant con-
[10] Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling,
tributions in the cross-embodiment efforts. We also thank
Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao,
YuqiXieforrenderingsupportandRobertoMart´ın-Mart´ınfor
Xinyue Wei, Yunchao Yao, et al. Maniskill2: A unified
discussionsonmobilemanipulationsupport.Finally,wethank
benchmark for generalizable manipulation skills. arXiv
all UT Austin Robot Perception and Learning Lab members
preprint arXiv:2302.04659, 2023.
fortheirinvaluablefeedbackonRoboCasa.Thisworkhasbeen
[11] Nico Gu¨rtler, Sebastian Blaes, Pavel Kolev, Felix
partiallysupportedbytheNationalScienceFoundation(FRR-
Widmaier, Manuel Wu¨thrich, Stefan Bauer, Bernhard
2145283, EFRI-2318065) and the Office of Naval Research
Scho¨lkopf, and Georg Martius. Benchmarking offline
(N00014-22-1-2204).
reinforcement learning on real-robot hardware. arXiv
preprint arXiv:2307.15690, 2023.
REFERENCES
[12] HuyHa,PeteFlorence,andShuranSong. Scalingupand
[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen distilling down: Language-guided robot skill acquisition.
Chebotar, Omar Cortes, Byron David, Chelsea Finn, In Conference on Robot Learning, pages 3766–3777.
Keerthana Gopalakrishnan, Karol Hausman, Alex Her- PMLR, 2023.
zog, et al. Do as i can, not as i say: Grounding language [13] Jesse Haviland, Niko Su¨nderhauf, and Peter Corke. A
inroboticaffordances. arXivpreprintarXiv:2204.01691, holistic approach to reactive mobile manipulation. IEEE
2022. RoboticsandAutomationLetters,7(2):3122–3129,2022.
[2] Anthony Brohan, Noah Brown, Justice Carbajal, Yev- [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising
gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana diffusionprobabilisticmodels. Advancesinneuralinfor-
Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine mation processing systems, 33:6840–6851, 2020.
Hsu, et al. RT-1: Robotics transformer for real-world [15] Stephen James, Zicong Ma, David Rovick Arrojo, and
control at scale. In arXiv preprint arXiv:2212.06817, Andrew J Davison. Rlbench: The robot learning bench-
2022. mark & learning environment. IEEE Robotics and
[3] Yevgen Chebotar, Quan Vuong, Karol Hausman, Fei Automation Letters, 5(2):3019–3026, 2020.
Xia, Yao Lu, Alex Irpan, Aviral Kumar, Tianhe Yu, [16] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler,
Alexander Herzog, Karl Pertsch, et al. Q-transformer: FrederikEbert,CoreyLynch,SergeyLevine,andChelsea
Scalableofflinereinforcementlearningviaautoregressive Finn. Bc-z: Zero-shot task generalization with robotic
q-functions. In Conference on Robot Learning, pages imitation learning. In Conference on Robot Learning,
3909–3928. PMLR, 2023. 2021.
[4] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric [17] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi
Cousineau, Benjamin Burchfiel, and Shuran Song. Dif- Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima
fusion policy: Visuomotor policy learning via action Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General
diffusion. arXiv preprint arXiv:2303.04137, 2023. robot manipulation with multimodal prompts. In Inter-
[5] Open X-Embodiment Collaboration et al. Open X- national Conference on Machine Learning, 2023.
Embodiment: Robotic learning datasets and RT-X mod- [18] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian
els. https://arxiv.org/abs/2310.08864, 2023. Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen,
[6] Murtaza Dalal, Ajay Mandlekar, Caelan Garrett, Ankur Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke,
Handa, Ruslan Salakhutdinov, and Dieter Fox. Imitating et al. Qt-opt: Scalable deep reinforcement learning
task and motion planning with visuomotor transformers. for vision-based robotic manipulation. arXiv preprint
arXiv preprint arXiv:2305.16309, 2023. arXiv:1806.10293, 2018.
[7] SudeepDasari,FrederikEbert,StephenTian,SurajNair, [19] Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar,
BernadetteBucher,KarlSchmeckpeper,SiddharthSingh, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn,
Sergey Levine, and Chelsea Finn. Robonet: Large-scale SergeyLevine,andKarolHausman. Mt-opt:Continuous
multi-robot learning. In Conference on Robot Learning, multi-task robotic reinforcement learning at scale. arXiv
2019. preprint arXiv:2104.08212, 2021.
[8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca [20] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ash-
Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, win Balakrishna, Sudeep Dasari, Siddharth Karam-
Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. cheti, Soroush Nasiriany, Mohan Kumar Srirama,
Objaverse: A universe of annotated 3d objects. arXiv Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: A
preprint arXiv:2212.08051, 2022. large-scale in-the-wild robot manipulation dataset, 2024.
[9] Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, [21] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-
Bernadette Bucher, Georgios Georgakis, Kostas Dani- derBilt,LucaWeihs,AlvaroHerrasti,MattDeitke,Kiana
ilidis, Chelsea Finn, and Sergey Levine. Bridge data: Ehsani,DanielGordon,YukeZhu,etal. AI2-THOR:An
Boosting generalization of robotic skills with cross- interactive 3d environment for visual ai. arXiv preprintarXiv:1712.05474, 2017. Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei,
[22] Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Silvio Savarese, Yuke Zhu, and Roberto Mart´ın-Mart´ın.
Finn, and Sergey Levine. A workflow for offline model- What matters in learning from offline human demonstra-
free robotic reinforcement learning. arXiv preprint tions for robot manipulation. In Conference on Robot
arXiv:2109.10813, 2021. Learning, 2021.
[23] Aviral Kumar, Anikait Singh, Frederik Ebert, Mitsuhiko [34] Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Ire-
Nakamoto, Yanlai Yang, Chelsea Finn, and Sergey tiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu,
Levine. Pre-training for robots: Offline rl enables learn- and Dieter Fox. Mimicgen: A data generation system
ing new tasks from a handful of trials. arXiv preprint for scalable robot learning using human demonstrations.
arXiv:2210.05178, 2022. arXiv preprint arXiv:2310.17596, 2023.
[24] Sergey Levine, Peter Pastor, Alex Krizhevsky, and [35] OpenAI et al. GPT-4 technical report, 2024.
Deirdre Quillen. Learning hand-eye coordination for [36] Lerrel Pinto and Abhinav Gupta. Supersizing self-
roboticgraspingwithlarge-scaledatacollection.InISER, supervision: Learning to grasp from 50k tries and 700
pages 173–184, 2016. robot hours. In Robotics and Automation (ICRA), 2016
[25] Sergey Levine, Aviral Kumar, George Tucker, and Justin IEEE Int’l Conference on. IEEE, 2016.
Fu. Offline reinforcement learning: Tutorial, review, [37] DeanAPomerleau. Alvinn:Anautonomouslandvehicle
and perspectives on open problems. arXiv preprint in a neural network. In Advances in neural information
arXiv:2005.01643, 2020. processing systems, pages 305–313, 1989.
[26] Chengshu Li, Fei Xia, Roberto Mart´ın-Mart´ın, Michael [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Lingelbach, Sanjana Srivastava, Bokui Shen, Kent Patrick Esser, and Bjo¨rn Ommer. High-resolution image
Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, synthesis with latent diffusion models, 2021.
et al. igibson 2.0: Object-centric simulation for robot [39] Jiaming Song, Chenlin Meng, and Stefano Ermon. De-
learning of everyday household tasks. arXiv preprint noising diffusion implicit models, 2022.
arXiv:2108.03272, 2021. [40] Andrew Szot, Alexander Clegg, Eric Undersander,
[27] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gok- Erik Wijmans, Yili Zhao, John Turner, Noah Maestre,
men, Sanjana Srivastava, Roberto Mart´ın-Mart´ın, Chen Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr
Wang,GabraelLevine,MichaelLingelbach,JiankaiSun, Maksymets, et al. Habitat 2.0: Training home assistants
et al. Behavior-1k: A benchmark for embodied ai with to rearrange their habitat. Advances in Neural Informa-
1,000 everyday activities and realistic simulation. In tion Processing Systems, 34:251–266, 2021.
Conference on Robot Learning, pages 80–93. PMLR, [41] Andrew Szot, Max Schwarzer, Harsh Agrawal, Bog-
2023. dan Mazoure, Walter Talbott, Katherine Metcalf, Natalie
[28] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Mackraz, Devon Hjelm, and Alexander Toshev. Large
Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking language models as generalizable policies for embodied
knowledge transfer for lifelong robot learning. arXiv tasks. arXiv preprint arXiv:2310.17722, 2023.
preprint arXiv:2306.03310, 2023. [42] Gemini Team. Gemini: A family of highly capable
[29] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan multimodal models, 2024.
Booher, Max Spero, Albert Tung, Julian Gao, John [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Emmons, Anchit Gupta, Emre Orbay, Silvio Savarese, Uszkoreit,LlionJones,AidanN.Gomez,LukaszKaiser,
and Li Fei-Fei. Roboturk: A crowdsourcing platform for and Illia Polosukhin. Attention is all you need. In
robotic skill learning through imitation. In Conference Advances in Neural Information Processing Systems,
on Robot Learning, 2018. 2017.
[30] Ajay Mandlekar, Jonathan Booher, Max Spero, Albert [44] Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shrid-
Tung, Anchit Gupta, Yuke Zhu, Animesh Garg, Silvio har,ChenBao,YuzheQin,BailinWang,HuazheXu,and
Savarese, and Li Fei-Fei. Scaling robot supervision to Xiaolong Wang. Gensim: Generating robotic simulation
hundreds of hours with roboturk: Robotic manipulation tasks via large language models. In Arxiv, 2023.
dataset through human reasoning and dexterity. arXiv [45] Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang,
preprint arXiv:1911.04052, 2019. Yian Wang, Katerina Fragkiadaki, Zackory Erickson,
[31] Ajay Mandlekar, Danfei Xu, Roberto Mart´ın-Mart´ın, David Held, and Chuang Gan. Robogen: Towards un-
Silvio Savarese, and Li Fei-Fei. Learning to generalize leashing infinite data for automated robot learning via
across long-horizon tasks from human demonstrations. generative simulation. arXiv preprint arXiv:2311.01455,
In Robotics: Science and Systems (RSS), 2020. 2023.
[32] Ajay Mandlekar, Danfei Xu, Roberto Mart´ın-Mart´ın, [46] Kuan-Ting Yu, Maria Bauza, Nima Fazeli, and Alberto
Yuke Zhu, Li Fei-Fei, and Silvio Savarese. Human-in- Rodriguez. More than a million ways to be pushed. a
the-loop imitation learning using remote teleoperation, high-fidelity experimental dataset of planar pushing. In
2020. URL https://arxiv.org/abs/2012.06733. Int’lConferenceonIntelligentRobotsandSystems,2016.
[33] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush [47] KevinZakka,YuvalTassa,andMuJoCoMenagerieCon-tributors. MuJoCo Menagerie: A collection of high-
quality simulation models for MuJoCo, 2022. URL
http://github.com/google-deepmind/mujoco menagerie.
[48] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan
Welker, Jonathan Chien, Maria Attarian, Travis Arm-
strong, Ivan Krasin, Dan Duong, Vikas Sindhwani, and
Johnny Lee. Transporter networks: Rearranging the
visual world for robotic manipulation. In Conference
on Robot Learning, 2020.
[49] Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee,
Xi Chen, Ken Goldberg, and Pieter Abbeel. Deep imita-
tionlearningforcomplexmanipulationtasksfromvirtual
reality teleoperation. In IEEE International Conference
on Robotics and Automation (ICRA), 2018.
[50] Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu,
Tom Erez, Serkan Cabi, Saran Tunyasuvunakool, Ja´nos
Krama´r, Raia Hadsell, Nando de Freitas, et al. Rein-
forcement and imitation learning for diverse visuomotor
skills. arXiv preprint arXiv:1802.09564, 2018.
[51] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto
Mart´ın-Mart´ın. robosuite: A modular simulation frame-
workandbenchmarkforrobotlearning.InarXivpreprint
arXiv:2009.12293, 2020.VII. SIMULATOR Skills (3): Pick_up(dairy), Place(counter),
We benchmark the speed of our simulator on the Push(dairy, blender)
PickPlaceCounterToCab task, running for 10 episodes, Reasoning: Preparing dairy products near a
with each episode spawned in a random scene. We use native blender is common when making mixtures for
MuJoCo rendering on an NVIDIA RTX A5000 GPU which baking.
addsoverheadtothesimulationspeed.Thesimulationphysics
runs on CPUs and we specifically used an AMD EPYC 7543 Improper use of skills:
32-Core Processor. The average time to reset the scene is Task: Wine Selection for Cooking
9.50 seconds and the average speed of stepping through the Goal: Retrieve a bottle of wine from the
simulator(callingenv.step)is25.2fps.Withoutrendering, cabinet for use in a recipe
theaverageresettimeis9.46secondsandthesimulatorspeed Objects: drink (wine)
is 31.9 fps. For reference, each timestep in our simulator Fixtures: cabinet Skills (5):
corresponds to 0.04 seconds in the world or 25 fps. Our Open(cabinet), Pick_up(drink),
observed simulation speed with the rendering of 25.2 fps Place(counter), Close(cabinet),
almostexactlymatchesthisspeed,meaningoursimulatorruns Press(button_on_coffee_machine)(simulating
roughly at real-time speed. uncorking)
Reasoning: Wine is occasionally used in baking
VIII. TASKSANDDATASETS
recipes and needs to be opened and ready to
A. Atomic Tasks use.
We design 25 atomic tasks representing eight foundational
robot skills: (1) Pick and place, (2) Opening and closing Picking up objects that should not be grasped (utensils):
doors, (3) Opening and closing drawers, (4) Twisting knobs, Task: Retrieve Baking Utensils
(5) Turning levers, (6) Pressing buttons, (7) Insertion, and (8) Goal: Gather all utensils needed for baking
Navigation. We outline details for all tasks in Figure 11. Each (spoons, ladle) and place them on the
task can come in the form of multiple task variants. These countertop.
task variants can disambiguate the task goal with language. Objects: utensils
For pick-and-place tasks, there is a task variant for each Fixtures: drawer, counter
object category being manipulated to disambiguate objects in Skills (4): Open(drawer), Pick_up(utensils),
clutter.Forturningonandoffthestove,thereisataskvariant Place(counter), Close(drawer)
for each burner being turned on or off, to disambiguate the Reasoning: Assembling the correct utensils is
relevantburner.Fornavigation,thereisataskvariantforeach essential before starting to bake.
appliancetherobotisnavigatingto,todisambiguatethetarget
C. Datasets
location.
In our atomic task experiments, we use human datasets and
B. Composite Tasks machine-generated datasets over 25 tasks. We render images
We obtain activity labels by asking ChatGPT the following forthesetasksusingrandomlysampledAI-generatedtextures,
simple prompt: “Can you give me 30 simple everyday high- which replace the native textures for each scene. Due to the
level kitchen activities? Each activity should be unique.” We high volume of these datasets and time constraints, we opt
obtain a set of candidate responses and manually select 20 to use the light-weight MuJoCo renderer to render images for
activities. We use a more elaborate prompt to obtain task thesedatasets.Forthepublicreleasewewillprovideusersthe
suggestions.Welisttherobotskills,relevantobjectcategories option to render all datasets with the Omniverse renderer.
and fixtures, specify constraints of the simulation (eg. small
objectsthatcannotbegrasped,limitedsupportfordeformable
IX. POLICYLEARNINGIMPLEMENTATION
manipulation tasks), and highlight example task blueprints as Our BC-Transformer policy takes as input the history of
a form of few-shot prompting. We list all 20 kitchen activities the past 10 observations in addition to the language goal
and representative tasks across all activities in Figure 12. for the text and outputs the next ten actions for the robot
In selecting composite tasks, we filter out LLM task sug- to execute. The agent replans after executing the first action.
gestions that have logical flaws. For a list of examples, see We modified the policy to support language conditioning by
below: encoding language goals using a CLIP sentence encoder. For
each observation in the input, the policy encodes proprio-
Using invalid object (no blender exists): ceptive information (end-effector pose and mobile base pose)
Task: Set Up Blending Station and images from three cameras: an eye-in-hand camera, a left
Goal: Place dairy ingredients next to the workspace camera, and a right workspace camera. It encodes
blender for making creamy fillings or batter. eachoftheseimageswithadedicatedResNet-18encoderstack
Objects: cheese, milk and fuses the visual representation using FiLM layers. The
Fixtures: counter, blender encodedobservationsarepassedtoa6-layerTransformerwith∼ 20M trainable parameters. We train the model for 500k
gradient steps at a learning rate of 1e−4 with a learning rate
warmup.
Wealsoexperimentwithdiffusionpolicy[4].Weimplement
thediffusionpolicyinRoboMimicforafaircomparisontoour
existing BC-Transformer method. The diffusion policy uses
the same observation encoder (ResNet, FiLM conditioning)
as the BC-Transformer. We use all recommended hyperpa-
rameters from the official implementation: 2 timesteps for
observation history, a prediction horizon of 16 steps, and an
action horizon of 8 steps. We use DDIM [39] with 100 train
timesteps and 10 inference timesteps, as recommended by
the Diffusion Policy authors. We found the Diffusion Policy
to underperform the BC-Transformer implementation signifi-
cantly.Onthesingle-stagePickPlaceCounterToSinktask,BC-
Transformer achieves a 56% success rate while Diffusion
Policyonlyachieves12%.Onepossibleexplanationforthisis
thatourBC-Transformerimplementationusesalongerhistory
lengthof10observations,whilediffusionpolicyusesahistory
length of 2 observations (this is the default choice used by
Chi et al. which we also opt to use). Incorporating a longer
observation history may be critical for our tasks.Task Skill Family Description
Pickanobjectfromthecounterandplaceitinsidethecabinet.
PickPlaceCounterToCabinet pick and place
The cabinet is already open.
Pick an object from the cabinet and place it on the counter.
PickPlaceCabinetToCounter pick and place
The cabinet is already open.
PickPlaceCounterToSink pick and place Pick an object from the counter and place it in the sink.
Pick an object from the sink and place it on the counter area
PickPlaceSinkToCounter pick and place
next to the sink.
Pick an object from the counter and place it inside the
PickPlaceCounterToMicrowave pick and place
microwave. The microwave door is already open.
Pick an object from inside the microwave and place it on the
PickPlaceMicrowaveToCounter pick and place
counter. The microwave door is already open.
Pick an object from the counter and place it in a pan or pot
PickPlaceCounterToStove pick and place
on the stove.
Pick an object from the stove (via a pot or pan) and place it
PickPlaceStoveToCounter pick and place
on (the plate on) the counter.
opening and closing
OpenSingleDoor Open a microwave door or a cabinet with a single door.
doors
opening and closing
CloseSingleDoor Close a microwave door or a cabinet with a single door.
doors
opening and closing
OpenDoubleDoor Open a cabinet with two opposite-facing doors.
doors
opening and closing
CloseDoubleDoor Close a cabinet with two opposite-facing doors.
doors
opening and closing
OpenDrawer Open a drawer.
drawers
opening and closing
CloseDrawer Close a drawer.
drawers
Turn on a specified stove burner by twisting the respective
TurnOnStove twisting knobs
stove knob.
Turn off a specified stove burner by twisting the respective
TurnOffStove twisting knobs
stove knob.
TurnOnSinkFacuet turning levers Turn on the sink faucet to begin the flow of water.
TurnOffSinkFaucet turning levers Turn off the sink faucet to begin the flow of water.
TurnSinkSpout turning levers Turn the sink spout.
Pressthebuttononthecoffeemachinetopourcoffeeintothe
CoffeePressButton pressing buttons
mug.
TurnOnMicrowave pressing buttons Turn on the microwave by pressing the start button.
TurnOffMicrowave pressing buttons Turn off the microwave by pressing the stop button.
Pick the mug from the counter and insert it onto the coffee
CoffeeSetupMug insertion
machine mug holder area.
Remove the mug from the coffee machine mug holder and
CoffeeServeMug insertion
place it on the counter.
NavigateKitchen navigation Navigate to a specified appliance in the kitchen.
Fig.11:AtomicTasks.Task Activity Description
Place a mug under the coffee machine nozzle and press the
PrepareCoffee Brewing coffee or tea
start button to make coffee.
Setbowlsandcupsonthecountertodry,aftertheyhavebeen
DryDishes Washing dishes
washed.
Restocking kitchen Groupallcannedfoodstogetheronashelfinthecabinet,right
RestockPantry
supplies next to existing canned foods.
Place vegetables from the sink onto the cutting board on the
ArrangeVegetables Chopping food
counter, to prepare for chopping them.
CheesyBread Making toast Pickupthewedgeofcheeseandplaceitonthesliceofbread.
Pick frozen food and place in microwave, then turn on mi-
MicrowaveThawing Defrosting food
crowave to thaw the food.
Place an empty kettle from the cabinet and place in the sink
FillKettle Boiling water
to be filled with water.
PrepMarinatingMeat Meat preparation Place steak and condiments on cutting board for marination.
Pick up candles and a bottle of wine and place them in the
DateNite Setting up the table
dining area for date night.
Anemptybowlandanemptycupareleftontheisland.Stack
BowlAndCup Clearing the table the cup inside the bowl and move the stacked items to the
counter.
PrepForSanitizing Sanitizing Organize cleaning supplies on the counter for easy access.
MakeFruitBowl Snack preparation Gather various fruits into a bowl for serving.
Tidying cabinets and Sort the canned and packaged foods into a drawer while
PantryMishap
drawers placing the vegetables on a nearby counter.
Washingfruitsandveg- Dump the veggies in the pot into the sink to drain. Turn off
DrainVeggies
etables the faucet, and place the pot back on the counter.
Retrieve a pan and transport it to the stove, then turn on the
SetupFrying Frying
stove.
HeatMug Reheating food Placeamuginthemicrowaveandthenturnonthemicrowave.
Createsalsabygatheringspecificvegetablesfromamixedpile
ColorfulSalsa Mixing and blending
on the counter and placing them onto the cutting board.
OrganizeBakingIngredients Baking Obtain dairy ingredients and place them together for baking.
Arrange bowls on the counter and place an assortment of
PlaceFoodInBowls Serving food
vegetable in each bowl.
Put a bowl of vegetables inside the microwave to steam them
SteamInMicrowave Steaming vegetables
there.
Fig.12:RepresentativeCompositeTasksAcross20KitchenActivites.Human-50 Generated-100 Generated-300 Generated-3000
PickPlaceCabToCounter 0.02 0.04 0.10 0.18
PickPlaceCounterToCab 0.06 0.08 0.16 0.28
PickPlaceCounterToMicrowave 0.02 0 0 0.18
PickPlaceCounterToSink 0.02 0.02 0.16 0.44
PickPlaceCounterToStove 0.02 0 0 0.06
PickPlaceMicrowaveToCounter 0.02 0 0.12 0.08
PickPlaceSinkToCounter 0.08 0.02 0.14 0.42
PickPlaceStoveToCounter 0.06 0 0.04 0.28
OpenSingleDoor 0.46 0.42 0.44 0.50
OpenDoubleDoor 0.28 0.12 0.22 0.48
CloseDoubleDoor 0.28 0.18 0.62 0.46
CloseSingleDoor 0.56 0.82 0.86 0.94
OpenDrawer 0.42 0.26 0.40 0.74
CloseDrawer 0.8 0.92 0.98 0.96
TurnOnStove 0.32 0.42 0.44 0.46
TurnOffStove 0.04 0.08 0.12 0.24
TurnOnSinkFaucet 0.38 0.26 0.48 0.34
TurnOffSinkFaucet 0.50 0.48 0.46 0.72
TurnSinkSpout 0.54 0.50 0.58 0.96
CoffeePressButton 0.48 0.48 0.42 0.74
TurnOnMicrowave 0.62 0.36 0.76 0.90
TurnOffMicrowave 0.70 0.70 0.62 0.60
CoffeeServeMug 0.22 0.12 0.24 0.34
CoffeeSetupMug 0 0.02 0.04 0.12
Average 0.288 0.263 0.350 0.476
Fig.13:Multi-taskLearningonAtomicTasks:FullResults