VHS: High-Resolution Iterative Stereo Matching with Visual Hull Priors
MarkusPlack HannahDro¨ge LeifVanHolland MatthiasB.Hullin
UniversityofBonn
Bonn,Germany
{mplack,droege,holland,hullin}@cs.uni-bonn.de
VisualHull Limits
Masksfrom
AuxiliaryViews
StereoPair
StrongPrior WeakPrior
k
h
w
SparseInit. DenseRefinement DisparityMap
Figure1. Weproposeatechniquetoinducearoughshapeestimatefromobjectmasks(top)aspriorinformationtoanovel,sparse-dense
stereo-matchingnetwork(bottom)fortheapplicationincapturestages(left)foraccurateandmemory-efficientdisparityestimation(right).
Abstract 1.Introduction
We present a stereo-matching method for depth estima- Stereo matching is a long-standing problem in the area
tion from high-resolution images using visual hulls as pri- of computer vision, driving core functionality in a wide
ors, and a memory-efficient technique for the correlation range of applications, for example in the automotive in-
computation. Ourmethodusesobjectmasksextractedfrom dustry,virtualandaugmentedrealitysystems,aswellasin
supplementaryviewsofthescenetoguidethedisparityes- medicalimaging,agriculture,remotesensing,androbotics
timation,effectivelyreducingthesearchspaceformatches. domains. Recently,interestsurgedintelepresenceandvir-
Thisapproachisspecificallytailoredtostereorigsinvolu- tualproductionscenariosthatusevolumetriccapturingsys-
metriccapturesystems,whereanaccuratedepthplaysakey tems [7,12,16,33], which rely on fast and accurate depth
roleinthedownstreamreconstructiontask.Toenabletrain- estimates for downstream reconstruction tasks. The dis-
ing and regression at high resolutions targeted by recent parity regression problem is typically solved by initially
systems,ourapproachextendsasparsecorrelationcompu- computing the matching cost between a stereo image pair
tationintoahybridsparse-denseschemesuitableforappli- or a suitable feature representation thereof and searching
cationinleadingrecurrentnetworkarchitectures. for the best correspondences along the epipolar lines re-
We evaluate the performance-efficiency trade-off of our sultinginahighlyirregularcostlandscape. Challengesin-
method compared to state-of-the-art methods, and demon- cludeocclusion,view-dependentreflectivity,repetitivepat-
strate the efficacy of the visual hull guidance. In addi- terns, and insufficient calibration accuracy. With the rise
tion, we propose a training scheme for a further reduction of deep learning in the domain of computer vision, classi-
of memory requirements during optimization, facilitating calmatchingmethods[3,15,31,36]aresurpassedbydata-
trainingonhigh-resolutiondata. driven approaches [14,18,29,49]. Recently, so-called all-
pairs-correlation networks based on the optical flow net-
1
4202
nuJ
4
]VC.sc[
1v25520.6042:viXrawork RAFT [40] have shown to perform remarkably well 2.RelatedWork
when applied in the stereo matching context [24]. Those
Learning-based methods using correlation volumes to
methods compute a dense correlation volume for all pos-
predictaccuratedisparitymapshaveshowngreatpotential
sible matches and perform stereo regression in an itera-
instereomatching. Webrieflyreviewapproachesforgen-
tive fashion akin to gradient descent methods. One dis-
erating cost volumes and discuss previous work on further
tinct drawback of such approaches is that the size of the
refinementofthedisparitiesbyiterativeupdatemethodsbe-
full correlation volume scales quadratically with the hori-
foregivinganoverviewofstereovisionapproachestarget-
zontalinputresolution,limitingtheirapplicabilityonhigh-
ingefficiencyaspects.
resolution inputs. One solution to reduce the prohibitive
memory requirement is to use sparse representations [45] 2.1.MatchingCostVolume
that only store the k most relevant entries of the correla-
Recentdevelopmentsinend-to-endlearningapproaches
tionvolume,similartok-nearest-neighbor(kNN)methods.
for cost volumes have successfully captured the similarity
While this still requires the computation of all correlation
of pixel pairs across varied degrees of disparity in stereo
values,whichdoesnotreducethecomputationalcosts,the
matching[11,18,29,50].
memorydemandonlyscalelinearlywithrespecttothehor-
In this context, Mayer et al. [29] introduced a method
izontal input resolution, but possibly discards valuable in-
basedoncorrelationforcalculatingcostvolume,followed
formation.
by subsequent work [23,41]. This approach measures the
Incontrast,weproposeasparse-denseapproachthatal-
correlation between the features of two images within a
lows us to consider all disparities, avoiding the limitations
1Dcorrelationlayerappliedhorizontallyalongthedisparity
associated with missing values in sparse representations.
line.
Wecalculatedisparitiesusingasparsemethodinitially,fol-
Concatenation-basedmethods[1,5,22,32],ontheother
lowed by a refinement in a memory-efficient dense man-
hand, follow a different strategy. Kendall et al. [18] con-
ner. Asacrucialsteptoreducetheamountofsparsecandi-
catenated unary features with their corresponding features
dates,weproposetoemploythevisualhull[20]asarough
along the disparity line. They generated a 4D cost vol-
shape estimate that reduces the set of valid disparities to
ume, subsequently processed through an encode-decoder
pointsinsidethehull. Theforegroundsegmentationmasks
network with 3D convolutions across spatial dimensions
required for this are available through the use of chroma-
and disparity. To further regularize the 4D cost volume,
keying[34]ormoresophisticatedimage-levelsegmentation
Changetal.[6]discussedtheimplementationofalearned
approaches [12] in many capturing scenarios and thus the
regularizationusingastackedhourglassnetwork. Address-
visual hull can be computed easily. During the refinement
ing the lack of explicit similarity measures in previous
step,wecanfurtherusethehullasaweakprior.
concatenation-based approaches, Guo et al. [14] proposed
Insummary,ourcontributionsareasfollows:
integratinggroup-wisecorrelationsintothe4Dcostvolume
bydividingfeaturesintosub-groupsandcalculatingcorre-
• Wepresentamethodtoinducepriorknowledgeofvi-
lations for each. To improve the performance even in re-
sualhullsfromauxiliaryviewsintoarecurrentstereo-
gionswithlesstexture,recentwork[47]filterstheconcate-
matchingnetworktoreducetheinitialdisparitysearch
nationvolumewithattentionweightstosuppressunneces-
spaceandasguidancefortheiterativerefinement.
saryinformation.
To overcome storage and runtime limitations, cascad-
• We demonstrate a sparse-dense correlation method
ing cost volumes were created by building a cost volume
that effectively reduces peak memory requirements
pyramidandprogressivelyrefiningdepthestimationwitha
while retaining the accuracy of all-pairs correlation
coarse-to-fine technique [11]. Other cascade formulations
methods through just-in-time computation for the up-
havebeenproposedforevenhigherresolutions[44]orad-
dates.
dressunbalanceddisparitydistributions[39].
• We propose an optimization scheme to realize high-
2.2.IterativeUpdatesinStereoMatching
resolution training of recurrent stereo network archi-
tecturesandshowhowthevisualhull-guidednetwork Initiallyproposedforopticalflowestimation,deeplearn-
canbenefitfrompre-trainingonconventionaltraining ingapproacheshavesuccessfullyemployedtraditionalopti-
databymakingtheinputoptional. mizationmethodsusinglearnedupdatestoimproveperfor-
mance. These methods refine disparity maps through suc-
We share the model and training implementation of our cessiveupdates,asdemonstratedbyRAFT(RecurrentAll-
Visual Hull Stereo (VHS) network and the custom ker- Pairs Field Transforms) [40]. RAFT consists of a feature
nels along with the data used for training and testing at encodingstep,computationofcorrelationvolumescontain-
https://github.com/unlikelymaths/vhs. ing the correlations between all pixel pairs, and a learned
2VisualHullPrior
VHfiltering H0
+kNN
k k′ k′ k′
h h h h
w w w w
g ∆ ∆ ∆
D0 D1 D2 Dn
StereoImagePair FeatureExt. InitialDisparity IterativeRefinementwithConvGRU
Figure2. OverviewofthethreestagesofourdisparityestimationnetworkVHS.FollowingtheFeatureExtractionwecomputeanInitial
Disparity estimate D from a sparse kNN cost volume restricted by the visual hull. Next, we perform an Iterative Refinement of the
0
disparityguidedbythevisualhullpriorusingConvGRUmodulesanddenselocalcorrelationswithwindowsizek′.
updateoperatorthatiterativelyupdatestheopticalflowes- correlation to reduce peak memory use and served as the
timation based on the correlation volumes. Based on this, inspiration for our correlation computation in the iterative
Lipson et al. [24] introduced an adaptation of RAFT for updates. SCV-Net [27] builds a sparse correlation volume
stereo disparity estimation, called RAFT-Stereo, which re- that resembles dilated convolutions controlled via a fixed
currentlyupdatesthedisparitymapusinglocalcostvalues. sparsityvalueandwithoutdependenceontheinputs.Lastly,
Several works introduced modifications to this idea. SCV-Stereo[45]isanalternativeapproachtosparsecorre-
IGEV-Stereo [48] introduces the geometry encoding vol- lation volumes. Different from their method, we use kNN
umetoextendtheall-pairscorrelationvolumeandregressa correlation for the initial disparity estimate instead of zero
betterinitialdisparity. InsteadofusingtheGRUtoupdate initialization and compute dense correlations on an ad hoc
theflowfield,Wangetal.[43]repurposedittopredictthe basisduringtheiterativestages.
depth probability of each pixel. Zhao et al. [51] propose
improvements in the iterative process to preserve detail in 3.VisualHullStereo
the hidden state by decoupling the disparity map from the
The overall structure of our method is based on RAFT-
hidden state and implementing a normalization strategy to
Stereo [24] and is shown in Fig. 2. It consists of three
handle large variations in disparities. EAI-Stereo [52] re-
stages. First,thepairofinputimagesisencodedintoafea-
placedtheGRUwithanerror-awareiterativemodule.
ture representation using a pre-trained encoding network.
These features are then used to compute an initial correla-
2.3.Efficiency
tioncostvolume. Togetherwithpriorinformationattained
In a structured light setting [21,28,42], projected pat- from a set of image masks of the scene, a sparse set of k
ternsaredesignedtouniquelyidentifythedepthofobjects disparities with the highest correlation values is selected
at each position. Hence, the problem can be solved more fromwhichaninitialdisparityvalueisestimated(Secs.3.1
efficientlyforknownlightpatterns,asdemonstratedbye.g. and3.2).Following,thedisparityisiterativelyrefinedusing
Hyperdepth [9] using a random forest approach and the a Convolutional Gated Recurrent Unit (ConvGRU)-based
branchingnetworkinGigadepth[37]. Notethatthisisdif- networkandupsamplingnetwork[48],withouttheneedto
ferentfromoursettingbasedontheworkofGuoetal.[12] holdthefullcostvolumeinmemoryatanytime(Sec.3.3).
where multiple, potentially overlapping, patterns are pro-
3.1.SparseCorrelation
jectedintothescene.
Turning to wider stereo vision challenges, the bottle- Givenarectifiedstereopair,weuseasharedfeatureen-
neck with cost volumes is their large search space, which codingnetwork[48]toextractfeaturesat25%oftheorig-
requires considerable computation and storage to find the inal image size. This representation is used to compute
desireddisparity. Khamisetal.[19]reducedthecomputa- an initial set of the k best matches. First, we define the
tional cost by refining the disparity from a low-resolution cost c (d) ∈ R of disparity d ∈ [0,w] at pixel p ∈ N2
p
cost volume through multiple levels of resolution. Addi- as the inner product of the corresponding feature vectors
tionally, recent works [2,46] stress real-time disparity es- f ,g ,fromtheleftandrightpicturesofsizeh×w,
p p−(0,d)T
timation in stereo vision. While Shamsafar et al. [38] re- whereg representsthefeaturevectoratthepixelin
p−(0,d)T
liesonlightweightarchitecturestooptimizeresources,Gar- therightimageoffsetbyd:
repallietal.introducedDIFT[10]asamobilearchitecture
for optical flow that uses just-in-time computation of the c (d)=f ·g (1)
p p p−(0,d)T
3define disparity boundaries b = (bmin,bmax) based on
p p p
pixellocationp,asillustratedinFigure3. Theinsightthat
the surfaces of the objects are confined within the interval
bm pin bm pax [bm pin,bm pax] can be leveraged to reduce computational re-
quirementswhencomputingtheinitialdisparitymapD0.
Westreamlinethek-nearest-neighborsearch,previously
performed across an expansive set of disparity candidates
D forpixelpasdescribedin(3),byfocusingonlyondis-
p
paritiesconstrainedwithinb :
p
D∗ ={d|bmin ≤d≤bmax}, D∗ ⊆D (4)
p p p p p
This approach allows for a faster computation of the re-
stricted correlation cost volume M∗ by skipping unneces-
p
saryevaluationsofthecorrelation. Accordingly,wedefine
Figure 3. Estimation of the disparity boundaries (bmin,bmax),
p p ourinitialdisparitymapasfollows:
fromtworectifiedviewsofanobject’svisualhull.Thevisualhull
encloses the objects’ surface, so the surface is guaranteed to lie
K
withinthedisparityboundaries. D0 =(cid:88) d ·g(c (d)) , (d,c (d))∈M∗ (5)
p l p l p p
l=1
Storing the full set of correlation values at high resolu- whereg isanattention-basedtransformationnetworkwith
tionscanbeinefficientandresource-intensive,asthedense
asoftmaxfunctionasthelastlayer.
costvolumescalesquadraticallywiththeimagewidthwhen
themaximaldisparityisproperlyadjusted. Todecreasethe 3.3.IterativeDisparityRefinement
memory requirements, we instead use a sparse correlation
WeuseahierarchicalConvGRUnetworkonthreereso-
costvolume,whichassignstoeachpixelpamuchsmaller
lutionstoiterativelyrefinethepredicteddisparitiesstarting
subset of correlation values c and corresponding disparity
withtheinitialvaluesD0,similarto[48]: Thenetworkup-
valuesd, p
dates a hidden state Hi taking the current disparity values
M ={(d,c (d))|d∈DkNN}, (2) and contextual features extracted from the corresponding
p p p
image data, and the correlated features around the current
whereDkNNrepresentsthesetofkbestdisparitiesforeach disparity estimate as input. The new state is used to pre-
p
pixel: dict an offset ∆i p from which the refined disparity values
DkNN = argmax (cid:88) c (d) (3) arecomputedas
p p
D˜ p⊂Dp,|D˜ p|=K d∈D˜ p Di+1 =Di +∆i. (6)
p p p
Here,D isthesetofalldisparitycandidatesforpixelp.
p
MemoryEfficientCorrelation Insteadofsamplingcor-
3.2.VisualHullPrior
relation values from a pre-computed full cost volume, we
This search for the best candidates can be further im- proposetocomputealocalcorrelationvolumeadhoctore-
proved by inducing a prior based on image masks from ducememoryusage. Thisvolumeisboundedwithinawin-
the scene. The visual hull, as defined in [20], provides an dowWi ofsize2r+1, whichiscenteredonthecurrently
p
efficient approximation of an object’s shape derived from estimateddisparityDi,
p
silhouettes captured by multiple cameras. In adherence to
therepresentationproposedin[35],wecomputethevisual Wi =[Di −r,Di +r], (7)
p p p
hull using a collection of masked input images, which is
stored within an octree structure for compact storage and where we fix r = 4 following [48]. We compute the cor-
fast access. The octree is designed such that each leaf relationsgroup-wise,asoriginallyproposedby[14],bydi-
nodeindicateswhetheritisinsideoroutsidethevisualhull. viding the feature vectors into a set of subgroups. Please
Given this information, we calculate the hull boundaries notethat,fortheinitialdisparityD0,westrategicallyomit-
p
by sampling rays projected into the scene from the refer- tedthegroup-wisecorrelationcalculation.Thisisduetothe
enceviewandevaluatingtheseraysfortransitionsbetween complexityofuniquelydefiningkNNforgroup-wisecorre-
outside and inside regions of objects. From these transi- lations,ensuringthatourapproachremainscomputationally
tions,wecreatedepthlimitsforeachcameraviewpointand efficient.
4Visual Hull as Weak Prior As additional information,
we supply the ConvGRU with a flag f (d) that guides the
p
networktopredictavaluewithinthevisualhull,
(cid:40)
1 ifd∈D∗,
f (d)= p (8)
p
−1 otherwise
for each disparity value d within the window Wi. In that
p (a)ReferenceImage (b)GroundTruthDisparity
way,thelimitsb obtainedfromthevisualhulloperateasa
p
weakpriorguidingthedisparityregressionwhileretaining
valuablecorrelationinformationforcasessuchasincorrect
limitsduetomaskingerrors.
One distinct advantage of our visual hull guidance is
that the disparity limits are an optional input to the whole
pipeline. During the initial sparse correlation, we can fall
backtosamplingfromallvaluesbelowapre-definedthresh-
old in the same manner as established models, and during
thedenseupdates,wesetf (d)=0toindicatemissingin-
(c)DisparityLimitbmin (d)DisparityLimitbmax
p
formation. Thisenablestheapplicationofoursparsecorre-
Figure4. SamplefromtheFlyingObjaversetrainingdataset. No-
lationmethodevenwithoutmaskedmeasurementsandpre-
ticehowthetruedisparityisclosetotheupperdisparitylimitex-
trainingofourmethodonexistingdatasets.
ceptforthebasininthebottomright,whichcannotberecovered
fromthevisualhull.
4.TrainingDetails
Given the particular nature of our method in terms of
As a second test set, we used SMPL [25] human models
targetapplicationandrequiredinputs,aboilerplatetraining
withtexturefromSMPLitex[4]toevaluateperformanceon
procedure following the literature would be unproductive.
human subjects. We create 100 scenes by combining ran-
Therefore,wepresentcustomtrainingdetailstailoredtoour
dom poses from the animations with random textures and
usecase,coveringthepreparationofcustomdataalongwith
render2stereopairsforeachscene.
trainingstrategies. Wefurtherintroduceamemory-efficient
approachenablingtrainingatevenhigherresolutions.
4.2.TrainingStrategy
4.1.DatasetPreparation
Having the visual hull guidance as an entirely optional
CommonstereodatasetslikeSceneFlow[29]donotcon-
component, allows our method to harness a more flexible
taingroundtruthmeshesorauxiliaryviews,whichprevents
trainingprocessandtopredictthedisparitymapevenwith-
theextractionofameaningfulvisualhull.Asanalternative,
outanypre-calculatedmasks. Weusethisflexibilityinour
werenderacustomdatasetwithMitsuba3[17]andmeshes
experimentsbypre-trainingabasemodelonSceneflow[29]
from Objaverse-XL [8] to train our network. The dataset
and subsequently fine-tuning the network on our custom
generation loosely follows the approach of SceneFlow by
training data. The training is performed on SceneFlow fi-
placingobjectsonavirtualcapturestage. Eachscenecon-
nalpassfor20epochsusingAdamW[26]withaone-cycle
tains a randomly transformed arrangement of 1 − 10 ob-
learningrateschedulewithalearningrateof0.00015anda
jects, as shown in Fig. 4, with an infrared camera stereo
batchsizeof4.Weuserandomcropsofsize288×640,ran-
setupusingactiveilluminationwithprojectedpatternssim-
domy-jitterandocclusionasaugmentation,andanL loss
ilarto[12]andatotalof68camerasforthemasks,allcap- 1
followingtheweightingofRAFT-Stereo[24]. Thismodel
tured at a resolution of 4608×5328. We render 2 stereo
serves as our baseline for a benchmark evaluation on the
pairsfor500scenes. Fortesting,wefollowthesameren-
SceneFlowtestset. Subsequently,thenetworkisfine-tuned
deringpipelinebutselectmeshesfromdifferentsourcesto
onthesimulateddataofObjaverse-XL(Sec.4.1)forhigh-
avoidcontaminationofthetrainingdataset. Totestperfor-
resolution stereo following the same settings, except for a
mance on difficult lighting effects, we curated scenes with
magnified random cropping of 256 × 2048, batch size of
objects that include challenging reflectance properties and
1andwiththeadditionalvisualhullinputs,whichweran-
finedetailsusinghigh-qualitymeshesfromPolyhaven1and
domly drop for 1 of the samples. Note that we use RGB
buildeightscenes,eachviewedfromfourdifferentangles. 8
inputsforthebenchmarkcomparisonandgreyscaleforthe
1https://polyhaven.com/ simulationofIRimagesforallotherexperiments.
5MemoryEfficientTraining Duringthetrainingofmost
Inference Training
iterative methods, each update of the disparity consumes 20 20
more VRAM since the full compute graph needs to be
15 15
stored in memory. We propose to split the forward and
backward computation in a manner that reduces the mem- 10 10
oryrequirementwhilestillretainingaccurategradientinfor-
5 5
mationasshowninFig.5. Fornconsecutiveupdatesteps
0 0
wecomputethelossesontheupscaleddisparitypredictions
as usual. Then, we backpropagate the partial loss and de- 600 3000
tachthehiddenstatesuchthatthecomputationalgraphcan 500 2500
be erased. To avoid multiple backward passes through the 400 2000
300 1500
costlyfeatureextractionnetwork,weproposetooptionally
200 1000
accumulateallgradientsforthefeaturevectorsfirstbefore
100 500
performing a final backpropagation after all iterations are
0 0
through. 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
Resolution[MP] VHS(ours) IGEV-Stereo CoExNet
Figure 6. Memory and runtime statistics of our method com-
paredtothebest-performing(IGEV)andfastest(CoExNet)base-
linemethods. Wefixtheimageheightat320pxandincreasethe
width,adjustingthemaximumdisparityto 1 ofthelatter.
4
(cid:80)
∆
H D D with custom CUDA kernels for the correlation computa-
0 0 gt
tionsandweusewarp-levelshuffleoperationstomakethe
L
1
initialkNNcorrelationcomputationefficient. Assuch,the
number of candidates is limited to 32, but we use 8 for all
L
2 experimentsfollowing[45]. Allourexperimentswerecon-
ductedonanNVIDIAGeForceRTX4090.
L
3 5.Experiments
L We evaluate our method in terms of average end-point
4
error (EPE) in pixels, proportion of errors (> 4px in %)
and the D1 outlier rate [30]. Runtime and video memory
measurements follow the literature and employ automatic
L mixedprecision.
n−1
5.1.BenchmarkEvaluation
L
n
We first validate the correctness of our sparse-dense
correlation network compared to the state-of-the-art, with
all methods being trained on SceneFlow. Table 2 shows
that our method performs competitively in terms of EPE
Figure5.Memoryefficienttrainingschemeforn=2consecutive
updatesteps.AfterthecomputationofthelossesL andL ,we for disparities within the range that all methods can han-
i i+1
perform backpropagation to accumulate gradients of the update dle. Specifically, for pixels with true disparities less than
networkparametersanddetachthehiddenstateeffectivelyfreeing or equal to 192 (EPE ≤192), our method matches with
(cid:80)
thecomputationalgraph. ∆indicatesanoptionalaccumulation FADNet++ [46], with only three methods achieving better
ofgradientstoavoidmultiplebackwardpassesthroughthefeature scores. Notably,whenevaluatedonallpixels(EPE ),our
all
extractionnetwork. methodsurpassesallbaselinemodelsaswedonothaveany
upperlimittothepossibledisparity.
Also, our method requires less memory during both in-
Technical Details Using CUDA, we build a visual hull ference and training as shown in Fig. 6 and is as fast as
octree from rendered masks from which the disparity lim- IGEV-Stereo [48] during inference while having a minor
its are computed. Our network is implemented in Pytorch runtimeoverheadduringtraining.
6
erutaeF
tnemenfieRevitaretI
gnidocnE
]BG[MARV
]noitareti/sm[emitnuRPolyhaven SMPL
Method
EPE EPE >4px D1 EPE EPE >4px D1
all noc all all all noc all all
CascadeStereo[11]† 16.97 14.37 31.1 6.77 8.31 6.51 13.8 2.97
CFNet[39]† 14.50 11.98 31.4 7.80 13.28 12.48 9.8 3.74
CoExNet[2]* 9.78 8.57 25.9 7.21 2.98 2.38 8.6 1.56
FADNet++[46]* 11.44 10.49 25.3 7.82 2.67 1.85 6.8 1.64
GwcNet[14]† 19.97 17.04 35.8 9.60 11.27 10.34 14.9 3.86
IGEV-Stereo[48]* 5.22 4.10 16.6 3.94 1.68 1.27 6.2 0.83
MSNet2D[38]† 10.08 8.69 44.2 5.67 5.24 4.44 28.9 2.38
MSNet3D[38]† 14.41 11.95 32.3 7.65 9.78 8.36 12.3 3.40
PSMNet[6]† 13.19 11.28 37.8 6.11 17.38 16.31 17.9 4.55
VHS(ours) 0.98 0.55 3.2 0.40 0.54 0.41 0.9 0.10
Table1. Comparisononourdatausingthemodelimplementationsfrom[13]. Methodsmarkedwith*runonhalfresolutionwithinputs
alignedtosetminimumdisparitytozero.†onquarterresolutionwithinputsalignedtosetminimumdisparitytozero.
Method #Params EPE EPE Prior EPE EPE >4px D1
≤192 all all noc all all
CascadeStereo[11] 10.5M 0.67 3.30 No 1.48 0.83 4.6 0.93
CFNet[39] 23.0M 0.96 3.06 Initial 1.29 0.75 4.3 0.68
CoExNet[2] 3.5M 0.69 3.36 Update 1.04 0.57 3.3 0.46
FADNet++[46] 12.4M 0.88 3.55 Both 0.98 0.55 3.2 0.40
GwcNet[14] 6.9M 0.76 3.52
IGEV-Stereo[48] 12.6M 0.48 3.01
MSNet2D[38] 2.3M 1.11 3.76 Table 3. Ablation of the visual hull guidance on the Polyhaven
MSNet3D[38] 1.8M 0.79 3.44 Testset.
PSMNet[6] 5.2M 1.02 3.69
relation volumes are typically limited to 192 pixels. For
VHS(ours) 12.7M 0.89 2.33 this evaluation, we resort to running the models on 2× or
4× downsampled input images and reduce the offsets by
aligningthemusingtheknownminimumground-truthdis-
Table 2. Comparison on SceneFlow final pass test set using the parity of the foreground, selecting the best variant of both
modelimplementationsfrom[13].
resolutionsbasedonthesmallestEPE.
To study the performance benefit of the visual hull, we
perform an ablation study on the Polyhaven test set, as
5.2.VisualHullGuidance
showninTab.3. Whileapplyingvisualhullguidanceonly
To further demonstrate our performance on high- for the initial disparity calculation already shows a minor
resolution data with larger disparities using the additional improvementacrossallmetricscomparedtoanuninformed
visual hull input, we evaluate our method on the two test run,theweakpriorduringtheiterativeupdatesyieldsama-
datasetsafterfinetuningonthetrainingdatasetasdescribed jor gain. Ultimately, we achieved the best results by em-
inSec.4.1. AsshowninTab.1,ourmethodoutperformsall ployingvisualhullguidanceinbothphases. Theimprove-
other methods on both the Polyhaven and SMPL datasets mentisparticularlyremarkableconsideringthatthemajor-
across all metrics. Specifically, we achieve significantly ityoftheobjectpointsdonotliedirectlyonthevisualhull.
lowerEPE andEPE whichindicateshigheroverallac- Asthequalityofthevisualhulldependsonthecorrect-
all noc
curacy,andahigheraccuracyinnon-occludedregions. We nessofthemasks,weadditionallystudytheinfluenceofin-
further highlight the robustness of our method by showing correctmattingontheperformanceofourmethodinFig.8.
the lowest percentage of pixels with large disparity errors We find that our method is robust against binary dilation
(> 4px , D1 ). We present qualitative results in Fig. 7. onthemasks,whilelargerbinaryerosionreducestheaccu-
all all
Note that most baseline models cannot perform inference racy. Intuitively, this makes sense as a correct visual hull
onthefullresolutioninputsusingcommonhardwareasthey alwaysenclosesthetruesurface,whichisalsothecasefor
exceedtheavailablememory(24GBinourcase)andcan- “inflated”visualhullsfromdilatedmasks,while“deflated”
notcapturethelargedisparityvaluesinourdataasthecor- hullsfromerodedmasksviolatethisassumption.
7Reference(Cropped) GTDisparity CoExNet[2] IGEV-Stereo[48] VHS(ours)
Full4608×5328 445−721/596−698 EPE:3.62/1.65 EPE:1.14/0.92 EPE:0.24/0.42
Figure7.QualitativeresultsonsamplesfromthePolyhavenandSMPLtestsets.Notethefaithfulreconstructionoftheplates(top)andthe
chest(bottom)producedbyourmethod.WeshowtherangeofdisparityvaluesbelowtheGTdisparityandtheEPEbelowthemethods.
7
Variant FullBackprop. DetachedFeatures
Erosion
6 GB ms GB ms
Dilation
- 14.18 377 - -
5
16 8.71 441 8.49 586
4 8 5.85 497 5.62 584
4 4.42 611 4.19 583
3
2 3.69 840 3.46 583
2
1 Table4. Peakmemoryandaverageruntimeperiterationcompar-
ingthestandardtrainingprocedure(firstrow)withourproposed
0
1 2 4 8 16 memory-efficient training running backpropagation through the
MaskDilation/Erosion[Steps] fullnetworkeachtime(left)andaccumulatingthefeaturegradi-
entsfirst(right)fordifferentnumbersofconnectedupdates.Mea-
Figure 8. Correlation between mask accuracy and EPE, demon- suredforasinglestereopairat512×1024.
stratingthemethod’srobustnesstobinarydilationstothecorrect
mask.
onSceneFlowinourtrainingprocedure. Anetworktrained
5.3.TrainingScheme using only our Objaverse-XL-based dataset yields an EPE
of 1.33 on the Polyhaven test set, compared to 0.98 of a
To evaluate the impact of the memory-efficient training
full training, indicating a significant benefit of the hybrid
schemeonmemoryusageandruntime,weestimatedthese
approach.
metrics for different numbers of connected updates before
backpropagation in relation to the standard training proce-
dure. We compared a setting with full backpropagation to 6.Conclusion
asettingwiththedetachedfeatureextractionandmeasured
fortheformerareductioninmemoryusageatthecostofin- Wehavepresentedatechniquetoinducevisualhullpri-
creasedruntimeforasmallernumberofconnectedupdates, orsintorecurrentstereonetworkstoimprovematchingper-
asshowninTab.4.Incomparison,thedetachedfeaturesof- formance. Combinedwithanovelsparse-densecorrelation
ferastableruntimeevenatasfewastwoconnectedlayers handling, our approach accurately regresses disparity for
withanevenfurtherreductioninmemoryusagecompared high-resolutionimageswhileretainingafavorablememory
tofullbackpropagation. footprintandwithoutanupperlimitontheachievabledis-
Finally,weevaluatetheimpactofincludingpre-training parity.
8
]xp[EPEAcknowledgements of the IEEE Conference on Computer Vision and Pattern
Recognition,pages5441–5450,2016. 3
This work has been funded by the Ministry of Cul-
[10] Risheek Garrepalli, Jisoo Jeong, Rajeswaran C Ravindran,
tureandScienceNorthRhine-Westphaliaundergrantnum- Jamie Menjay Lin, and Fatih Porikli. Dift: Dynamic iter-
ber PB22-063A (InVirtuo 4.0: Experimental Research in ativefieldtransformsformemoryefficientopticalflow. In
Virtual Environments), and by the state of North Rhine- ProceedingsoftheIEEE/CVFConferenceonComputerVi-
WestphaliaaspartoftheExcellencyStart-upCenter.NRW sionandPatternRecognition,pages2219–2228,2023. 3
(U-BO-GROW) under grant number 03ESCNW18B. Leif [11] XiaodongGu,ZhiwenFan,SiyuZhu,ZuozhuoDai,Feitong
VanHollandacknowledgesthesupportoftheGermanRe- Tan,andPingTan. Cascadecostvolumeforhigh-resolution
search Foundation (DFG) grant KL 1142/11-2 (DFG Re- multi-view stereo and stereo matching. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
searchUnitFOR2535AnticipatingHumanBehavior).
recognition,pages2495–2504,2020. 2,7
[12] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch,
References
Xueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-
Escolano, Rohit Pandey, Jason Dourgarian, et al. The re-
[1] ShamsulFakharAbdGani,MuhammadFahmiMiskon,Ros-
lightables: Volumetricperformancecaptureofhumanswith
tam Affendi Hamzah, Mohd Saad Hamid, Ahmad Fauzan
realisticrelighting. ACMTransactionsonGraphics(ToG),
Kadmin, and Adi Irwan Herman. Refining disparity maps
38(6):1–19,2019. 1,2,3,5
using deep learning and edge-aware smoothing filter. Bul-
[13] XiandaGuo,JuntaoLu,ChenmingZhang,YiqiWang,Yiqun
letinofElectricalEngineeringandInformatics,13(3):1961–
Duan,TianYang,ZhengZhu,andLongChen. Openstereo:
1969,2024. 2
Acomprehensivebenchmarkforstereomatchingandstrong
[2] AntyantaBangunharcana,JaeWonCho,SeokjuLee,InSo
baseline. arXivpreprintarXiv:2312.00343,2023. 7
Kweon,Kyung-SooKim,andSoohyunKim.Correlate-and-
[14] XiaoyangGuo,KaiYang,WukuiYang,XiaogangWang,and
excite: Real-time stereo matching via guided cost volume
Hongsheng Li. Group-wise correlation stereo network. In
excitation. In2021IEEE/RSJInternationalConferenceon
Proceedings of the IEEE/CVF conference on computer vi-
Intelligent Robots and Systems (IROS), pages 3542–3548.
sionandpatternrecognition,pages3273–3282,2019. 1,2,
IEEE,2021. 3,7,8
4,7
[3] Stephen T Barnard and William B Thompson. Disparity
[15] Rostam Affendi Hamzah, Haidi Ibrahim, et al. Literature
analysisofimages. IEEETransactionsonPatternAnalysis
surveyonstereovisiondisparitymapalgorithms. Journalof
andMachineIntelligence,(4):333–340,1980. 1
Sensors,2016,2016. 1
[4] DanCasasandMarcCominoTrinidad. Smplitex:Agenera-
[16] JonathanHeagerty,SidaLi,EricLee,ShuvraBhattacharyya,
tivemodelanddatasetfor3dhumantextureestimationfrom
SujalBista,BarbaraBrawn,BrandonYFeng,SusmijaJab-
singleimage. arXivpreprintarXiv:2309.01855,2023. 5
bireddy, Joseph JaJa, Hernisa Kacorri, et al. Holocamera:
[5] RohanChabra,JulianStraub,ChristopherSweeney,Richard
Advancedvolumetriccaptureforcinematic-qualityvrappli-
Newcombe, and Henry Fuchs. Stereodrnet: Dilated resid-
cations. IEEETransactionsonVisualizationandComputer
ualstereonet. InProceedingsoftheIEEE/CVFconference
Graphics,2024. 1
on computer vision and pattern recognition, pages 11786–
[17] WenzelJakob, Se´bastienSpeierer, NicolasRoussel, Merlin
11795,2019. 2
Nimier-David,DelioVicini,TizianZeltner,BaptisteNicolet,
[6] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo MiguelCrespo,VincentLeroy,andZiyiZhang. Mitsuba3
matchingnetwork.InProceedingsoftheIEEEconferenceon renderer. 2022. https://mitsuba-renderer.org. 5
computervisionandpatternrecognition,pages5410–5418, [18] AlexKendall,HaykMartirosyan,SaumitroDasgupta,Peter
2018. 2,7 Henry,RyanKennedy,AbrahamBachrach,andAdamBry.
[7] AlvaroCollet,MingChuang,PatSweeney,DonGillett,Den- End-to-endlearningofgeometryandcontextfordeepstereo
nis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk, regression.InProceedingsoftheIEEEinternationalconfer-
andSteveSullivan. High-qualitystreamablefree-viewpoint enceoncomputervision,pages66–75,2017. 1,2
video. ACM Transactions on Graphics (ToG), 34(4):1–13, [19] SamehKhamis,SeanFanello,ChristophRhemann,Adarsh
2015. 1 Kowdle, Julien Valentin, and Shahram Izadi. Stereonet:
[8] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Guided hierarchical refinement for real-time edge-aware
Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris- depthprediction.InProceedingsoftheEuropeanconference
tian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli oncomputervision(ECCV),pages573–590,2018. 3
VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia [20] AldoLaurentini.Thevisualhullconceptforsilhouette-based
Gkioxari,KianaEhsani,LudwigSchmidt,andAliFarhadi. imageunderstanding.IEEETransactionsonpatternanalysis
Objaverse-xl:Auniverseof10m+3dobjects.arXivpreprint andmachineintelligence,16(2):150–162,1994. 2,4
arXiv:2307.05663,2023. 5 [21] JJ Le Moigne and Allen Mark Waxman. Structured light
[9] Sean Ryan Fanello, Christoph Rhemann, Vladimir patternsforrobotmobility. IEEEJournalonRoboticsand
Tankovich, Adarsh Kowdle, Sergio Orts Escolano, David Automation,4(5):541–548,1988. 3
Kim, and Shahram Izadi. Hyperdepth: Learning depth [22] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Zi-
from structured light without matching. In Proceedings wei Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and
9ShuaichengLiu. Practicalstereomatchingviacascadedre- Fast high resolution volume carving for 3d plant shoot re-
current network with adaptive correlation. In Proceedings construction. Frontiersinplantscience,8:303692,2017. 4
oftheIEEE/CVFconferenceoncomputervisionandpattern [36] Daniel Scharstein and Richard Szeliski. High-accuracy
recognition,pages16263–16272,2022. 2 stereodepthmapsusingstructuredlight.In2003IEEECom-
[23] Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Wei puter Society Conference on Computer Vision and Pattern
Chen, Linbo Qiao, Li Zhou, and Jianfeng Zhang. Learn- Recognition,2003.Proceedings.,volume1,pagesI–I.IEEE,
ing for disparity estimation through feature constancy. In 2003. 1
ProceedingsoftheIEEEconferenceoncomputervisionand [37] Simon Schreiberhuber, Jean-Baptiste Weibel, Timothy Pat-
patternrecognition,pages2811–2820,2018. 2 ten, and Markus Vincze. Gigadepth: Learning depth from
[24] Lahav Lipson, Zachary Teed, and Jia Deng. Raft-stereo: structuredlightwithbranchingneuralnetworks.InEuropean
Multilevelrecurrentfieldtransformsforstereomatching. In Conference on Computer Vision, pages 214–229. Springer,
2021 International Conference on 3D Vision (3DV), pages 2022. 3
218–227.IEEE,2021. 2,3,5 [38] Faranak Shamsafar, Samuel Woerz, Rafia Rahim, and An-
[25] MatthewLoper,NaureenMahmood,JavierRomero,Gerard dreasZell. Mobilestereonet: Towardslightweightdeepnet-
Pons-Moll, and Michael J Black. Smpl: a skinned multi- works for stereo matching. In Proceedings of the ieee/cvf
personlinearmodel.ACMTransactionsonGraphics(TOG), winterconferenceonapplicationsofcomputervision,pages
34(6):1–16,2015. 5 2417–2426,2022. 3,7
[39] ZhelunShen,YuchaoDai,andZhiboRao. Cfnet: Cascade
[26] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
andfusedcostvolumeforrobuststereomatching. InPro-
regularization. arXivpreprintarXiv:1711.05101,2017. 5
ceedingsoftheIEEE/CVFConferenceonComputerVision
[27] Chuanhua Lu, Hideaki Uchiyama, Diego Thomas, Atsushi
andPatternRecognition,pages13906–13915,2021. 2,7
Shimada, and Rin-ichiro Taniguchi. Sparse cost volume
[40] ZacharyTeedandJiaDeng. Raft: Recurrentall-pairsfield
forefficientstereomatching. Remotesensing,10(11):1844,
transforms for optical flow. In Computer Vision–ECCV
2018. 3
2020:16thEuropeanConference,Glasgow,UK,August23–
[28] Manuel Martinez and Rainer Stiefelhagen. Kinect un-
28,2020,Proceedings,PartII16,pages402–419.Springer,
leashed: Getting control over high resolution depth maps.
2020. 2
InMVA,pages247–250,2013. 3
[41] Alessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mat-
[29] NikolausMayer, EddyIlg, PhilipHausser, PhilippFischer,
toccia, andLuigiDiStefano. Real-timeself-adaptivedeep
DanielCremers,AlexeyDosovitskiy,andThomasBrox. A
stereo. InProceedingsoftheIEEE/CVFconferenceoncom-
large dataset to train convolutional networks for disparity,
putervisionandpatternrecognition,pages195–204,2019.
optical flow, and scene flow estimation. In Proceedings of
2
theIEEEconferenceoncomputervisionandpatternrecog-
[42] Piet Vuylsteke and Andre´ Oosterlinck. Range image ac-
nition,pages4040–4048,2016. 1,2,5
quisition with a single binary-encoded light pattern. IEEE
[30] MoritzMenzeandAndreasGeiger.Objectsceneflowforau-
transactions on pattern analysis and machine intelligence,
tonomousvehicles. InProceedingsoftheIEEEconference
12(2):148–164,1990. 3
on computer vision and pattern recognition, pages 3061–
[43] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, and
3070,2015. 6
MarcPollefeys. Itermvs:Iterativeprobabilityestimationfor
[31] KarstenMu¨hlmann,DennisMaier,Ju¨rgenHesser,andRein-
efficientmulti-viewstereo.InProceedingsoftheIEEE/CVF
hardMa¨nner. Calculatingdensedisparitymapsfromcolor
conference on computer vision and pattern recognition,
stereo images, an efficient implementation. International
pages8606–8615,2022. 3
JournalofComputerVision,47:79–88,2002. 1
[44] FangjinhuaWang,SilvanoGalliani,ChristophVogel,Pablo
[32] Guang-YuNie,Ming-MingCheng,YunLiu,ZhengfaLiang, Speciale, and Marc Pollefeys. Patchmatchnet: Learned
Deng-Ping Fan, Yue Liu, and Yongtian Wang. Multi-level multi-view patchmatch stereo. In Proceedings of the
context ultra-aggregation for stereo matching. In Proceed- IEEE/CVF conference on computer vision and pattern
ings of the IEEE/CVF conference on computer vision and recognition,pages14194–14203,2021. 2
patternrecognition,pages3283–3291,2019. 2 [45] HengliWang,RuiFan,andMingLiu. Scv-stereo:Learning
[33] Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello, stereomatchingfromasparsecostvolume. In2021IEEE
WayneChang,AdarshKowdle,YuryDegtyarev,DavidKim, InternationalConferenceonImageProcessing(ICIP),pages
Philip L Davidson, Sameh Khamis, Mingsong Dou, et al. 3203–3207.IEEE,2021. 2,3,6
Holoportation: Virtual3dteleportationinreal-time. InPro- [46] QiangWang,ShaohuaiShi,ShizhenZheng,KaiyongZhao,
ceedings of the 29th annual symposium on user interface and Xiaowen Chu. Fadnet++: Real-time and accurate dis-
softwareandtechnology,pages741–754,2016. 1 parityestimationwithconfigurablenetworks.arXivpreprint
[34] Carolus Raditya, Muhammad Rizky, Sergio Mayranio, and arXiv:2110.02582,2021. 3,6,7
Benfano Soewito. The effectivity of color for chroma- [47] GangweiXu,JundaCheng,PengGuo,andXinYang.Atten-
keytechniques. ProcediaComputerScience,179:281–288, tion concatenation volume for accurate and efficient stereo
2021. 2 matching. InProceedingsoftheIEEE/CVFConferenceon
[35] HannoScharr,ChristophBriese,PatrickEmbgenbroich,An- Computer Vision and Pattern Recognition, pages 12981–
dreas Fischbach, Fabio Fiorani, and Mark Mu¨ller-Linow. 12990,2022. 2
10[48] GangweiXu,XianqiWang,XiaohuanDing,andXinYang.
Iterativegeometryencodingvolumeforstereomatching. In
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition,pages21919–21928,2023. 3,
4,6,7,8
[49] Haofei Xu and Juyong Zhang. Aanet: Adaptive aggrega-
tion network for efficient stereo matching. In Proceedings
oftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages1959–1968,2020. 1
[50] Feihu Zhang, Victor Prisacariu, Ruigang Yang, and
PhilipHSTorr. Ga-net: Guidedaggregationnetforend-to-
endstereomatching. InProceedingsoftheIEEE/CVFcon-
ference on computer vision and pattern recognition, pages
185–194,2019. 2
[51] Haoliang Zhao, Huizhou Zhou, Yongjun Zhang, Jie Chen,
YitongYang,andYongZhao.High-frequencystereomatch-
ing network. InProceedings ofthe IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 1327–
1336,2023. 3
[52] HaoliangZhao,HuizhouZhou,YongjunZhang,YongZhao,
Yitong Yang, and Ting Ouyang. Eai-stereo: Error aware
iterative network for stereo matching. In Proceedings of
theAsianConferenceonComputerVision, pages315–332,
2022. 3
11