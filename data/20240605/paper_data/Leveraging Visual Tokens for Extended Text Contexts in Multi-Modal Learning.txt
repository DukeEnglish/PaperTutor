Leveraging Visual Tokens for Extended Text Contexts
in Multi-Modal Learning
AlexJinpengWang1 LinjieLi2 YiqiLin1 MinLi3
LijuanWang2 MikeZhengShou1
1ShowLab,NationalUniversityofSingapore 2MicrosoftGenAI 3CentralSouthUniversity
http://fingerrec.github.io/visincontext
Abstract
Trainingmodelswithlongerin-contextlengthsisasignificantchallengeformul-
timodal model due to substantial GPU memory and computational costs. This
exploratorystudydoesnotpresentstate-of-the-artmodels;rather,itintroducesan
innovative method designed to increase in-context text length in multi-modality
large language models (MLLMs) efficiently. We present Visualized In-Context
TextProcessing(VisInContext),whichprocesseslongin-contexttextusingvisual
tokens. This technique significantly reduces GPU memory usage and floating
pointoperations(FLOPs) forbothtrainingandinferenceingstage. For instance,
our methodexpandsthe pre-trainingin-contexttextlength from 256 to 2048 to-
kenswithnearlysameFLOPsfora56billionparameterMOEmodel.Experimen-
talresultsdemonstratethatmodeltrainedwithVisInContextdeliverssuperiorper-
formanceoncommondownstreambenchmarksforin-contextfew-shotevaluation.
Additionally,VisInContext iscomplementarytoexistingmethodsforincreasing
in-contexttextlengthandenhancesdocumentunderstandingcapabilities,showing
greatpotentialindocumentQAtasksandsequentialdocumentretrieval.
1 Introduction
(a)GPUmemoryconsumption. (b)Theflopscomparison.
Figure1: VisInContextsignificantlyincreasesthein-contexttextlengthfrom256to2048duringpre-
trainingonNVIDIAH100GPU.Forourmethod,weincorporateVisInContextafter128texttokens.
We implement PyTorch Flamingo [1] models with different in-context length during pre-training.
Thelanguagemodelisa56BMOE[2]modelloadedwith4-bitquantizationandthebatchsizeon
eachGPUis32withFP16. WetrainthemodelwithDeepSpeed[3]Zero-2.
4202
nuJ
4
]VC.sc[
1v74520.6042:viXraLargeLanguageModels(LLMs),suchasOPT,Mistral,andLLaMA-2[4,5,6],havesignificantly
advancedthefieldofNaturalLanguageProcessing(NLP).Theseadvancementsarepartlyduetothe
increasedcapability of LLMsto processlong contexts, from512 tokens[7] up to 16K tokens[6].
Buildingonthesedevelopments,recentmulti-modallearningresearch[1,8,9,10]hasshiftedfocus
from simple image-textpairs, like those in CC3M [11] and LAION-400M[12], to more complex
andlengthyinterleaveddocumentdatasets. ExamplesincludewebcorporalikeMMC4[13]andthe
OBELICS[14]dataset,aswellasPDFcorporalikeDocVQA[15].
However, training models on these complex datasets presents significantchallenges due to the in-
creased GPU memory and computationaldemandsof extendedcontexts. For instance, while pro-
cessingjust5MdataitemsfromMMC4and10MfromtheOBELICSdataset,OpenFlamingo-9B[9]
resortedtosub-samplingtextandprocessingonly256tokensatatime,yetitstillrequires3280GB
A100GPUsforoverthreedays.Thishighlightstheneedformorecomputation-efficientmethodsto
handlelongcontextlengthseffectively.
Inthe domainof LLMs, two popularmethodsto extendcontextlengthare theuse of memorizing
banks [16] and novel self-attention mechanisms [17, 18]. These methods have inspired advance-
mentsinthemulti-modalitydomainaswell. Forexample,theLargeWorldModel[19]introduces
RingAttention[18],andMA-LMM[20]employsmemorybankstoprocesslongvideounderstand-
ing tasks. While these techniques have shown promise, our approach aims to increase in-context
text length by leveraging the strengths of visual encoders in MLLMs. We first observe that ex-
isting MLLMs usually exploita much lighter visualencoders, comparedto its textdecoders.
Forinstance,Flamingo-9Bconsistsofa304.4MViT-L/16[21]asimageencoder,anda7.1BChin-
chilla[1]modelasthetextdecoder. Additionally,previousworks[22,23]havedemonstratedthat
visualencoderstrainedonpairedimage-textdataalsoexhibitemergentOCRcapabilities.
Motivatedbytheseobservations,weproposeVisualizedIn-ContextTextProcessing(VisInContext),
a method that uses visual tokens to process extended textual contexts, which is complementary
of existing methods in extending context length. Specifically, we convert long textual content
into images and use the visual encoders to extract textual representations. In this way, we
canefficientlyandeffectivelyenablemodelswithmuchlongertextcontexts,asshowninFigure1.
With VisInContext, we show that the in-context text length can be increased by 7 times over the
competingbaseline. Additionally,weobservealmostthesameoverallcomputationFLOPsevenas
in-contextlengthextendssignificantly.OurextensiveexperimentswillalsoshowthatVisInContext
renderssuperiormodelperformanceonconventionalin-contextfew-shotevaluationsanddocument
understanding,withmuchlowercomputationalcost.
Contributions.Insummary,ourcontributionsareasfollows:i.WeintroduceVisualizedIn-Context
Text Processing (VisInContext), a novel method that increases in-context text length using visual
tokens. VisInContext directly compresses text context at input-level, which is complementary to
existingtechniqueswith improvedself-attentionormemorybanks. ii. We demonstratethatVisIn-
Contextis effectiveforbothtrainingandinferencestage with muchlowercomputationalcost. iii.
With extended text context brought by VisInContext, our model improves the average in-context
few-shotperformancefrom55.8%to57.8%overthe competingbaseline. iv. As abyproduct,our
methodalsoshowsgreatpotentialindocumentunderstandingonpopulardocumentQAtasksand
ournewlyproposedsequentialdocumentretrievaltask.
2 Method
The goal of VisInContext is to process in-context text using visual tokens so that the model can
handlelongtextcontextmoreefficiently. WeprimarilybaseourstudyonFlamingo-basedarchitec-
ture[1,9,14],asithasshownsuccessinimprovingamodel’sabilitytolearnfromlongmultimodal
contextthatcontainsarbitrarilyinterleavedtextandimages.
2.1 Terminology
Beforedivingintomodeldetails,wedefinethefollowingterms:
In-contextText Length: The actuallengthof text tokensobservedby the modelwithin a docu-
ment.
2Figure2: VisInContextPipeline. TheVisInContextpipelinebuildsupontheFlamingomodelforin-context
few-shot modeling (represented in gray). VisInContext processes interleaved image-text data by rendering
portionsofthein-contexttextintoimages. ThisapproachmaintainstheTextTokenLengthofthemodelwhile
allowingforasignificantlyextendedIn-contextTextLength.
TextTokenLength: Thelengthofthetextsequence inputdirectlytotheLLM, correspondingto
thetokencountofthissequence.
WithVisInContext,theIn-contextTextLengthisgreaterthanthetexttokenlength,aspartofthetext
isrepresentedusingvisualtokens.
2.2 OverallArchitecture
The implementation and architecture of VisInContext are shown in Figure 2. It is based on a
dual-streamencodermodelthatintegratesbothvisualandtextualdata. To effectivelyhandlelong
interleaved data, we use a pre-sampling strategy as in Flamingo-style works [1, 9, 14]. Specifi-
cally, we sample m images, denoted as I1,I2,...,I
m
∈ I, along with their correspondingtexts
T1,T2,...,T
m
∈ T. These tokensare concatenated,resultingin a sequenceof about256 tokens.
However,sincetheoveralllengthofawebdocumentisgenerallymuchlongerthan256tokens(In-
contextTextLength≥TextTokenLength),thissamplingapproachcanleadtotheomissionofalot
ofrelatedtextcontext.
To address this issue, we convertthese omitted text contextinto visual signals by renderingthem
intoimages. WefirstconcatenateallomittedtextsegmentsanddividethemintoM partstorender
textimages,namedT′ ,T′ ,...,T′ ∈T′. Boththeoriginalimagesandthetext-renderedimagesare
1 2 m
thenprocessedthroughasharedfrozenvisionencoder. Then,weemploytwolearnableresamplers
toextractafixednumberoftokensfromboththerawandtext-renderedimagefeatures,respectively.
To facilitatethe modeltolearnfromrenderedtextimages, weintroducetwonovelmodeldesigns,
TokenMaskingmechanismandText-CentricContrastiveLearning(TCCL).TokenMaskingallows
themodeltoonlyreadfromtextimagetokensbymaskingtherawimagetokenswithmaskingratio
1,whichensuresthatthemodelwon’tsimplybeignoringthetextimagesduringtraining,hencecan
′
learntheassociationbetweentherenderedtextimages{T }andthetexttokens{T }. TCCLaligns
i i
thevisualtextrepresentationfromtheresamplerwiththeembeddingsextractedfromtexttokenizers
in LLM, which reduces the gap between our visual text tokens and the text tokens the LLM is
trainedtoperceive.Withthesedesigns,VisInContextnotonlyreducescomputationaldemands—as
evidencedbyareductioninflopsandinferencetime—butalsoimprovestheOCRability,aswewill
showinourexperiments.
2.3 TextRendering
This moduleconvertstextualdata into a visually rich RGB format, specifically renderingthe text
intoanimagesizeofp ×np ,wherenisthenumberofpatches.WeemploytheHERSHEYfontat
h w
asizeof10px. Onaverage,one16x16patchaccommodatesapproximately1.5OPTtexttokens. A
224x224textimagecontainsabout294texttokens.Consequently,avisualencoderoperatingonthis
renderedtextimagerequiresonly1/3oftokenstoencodeanequivalentamountoftext,compared
to the text tokenizer in language models. The vision encoder is quite lightweight ViT-L (340M)
3comparedto languagemodelMOE (56B),which makesthe processing of renderedtext images
significantlymoreefficientthandirectlyinputtingthetextintoalanguagemodel.
2.4 TokenMasking
Inourinitialexperiments,wefindthatcombiningtokensfromrawimagesandtextimagesdirectly
led to the networkdisregardingthe text-imageinput. To addressthis issue, we introducea Token
Maskingstrategytoforcethemodeltolearntextsemanticsfromvisualinputs. Duringpretraining,
the raw image and text image are first encoded into the same number of tokens after resampler,
andthenwemasktherawimagetokenswithapre-definedprobability. Whenmaskingouttheraw
image tokens, the model can focus on learning the association between rendered text images and
thecomplementarytexttokens. Atinferencetime,weaddthetext-imagetokensandimagetokens
together,toallowthemodeleffectivelyleverageinformationfrombothsources.
2.5 Text-CentricContrastiveLoss(TCCL)
Motivation. Giventhatthevisionencoder,typicallyafrozenVisionTransformer(ViT)[24],never
observes rendered text images during pretraining, it may struggle to derive text semantics from
pixels. Tomitigatethisissue,weintroduceanewtrainingobjective,Text-CentricContrastiveLoss
(TCCL). This objective aims to guide the resampler on rendered text images to interpret visual
representationsoftextwithaproficiencycomparabletotraditionaltexttokenizers,sothatthetextual
semanticscanbeeffectiveextractedfromtherenderedtextimages.
Mechanism. TCCLutilizesrawtexttokenembeddingsfromthetexttokenizerassoftsupervision
signalstosupervisetheresamplertolearntext-centricrepresentation.Toreducetheglobalsemantic
gapbetweentextimageembeddingsandtexttokenembeddings,wefirstaggregatetheseembeddings
withaveragepoolingandthenalignthemwithTCCL.Intuitively,TCCLisdesignedtoturnthejoint
of the vision encoder and resampler into a “visual" text tokenizer, as it promotes the text image
embeddingstoshareasimilarglobalsemanticasthetexttokenembeddings. ThecoreofTCCLis
formulatedasacontrastiveloss:
exp(sim(f ,f )/τ)
L =−log
vi tj
(1)
ij N k=1exp(sim(f vi,f tk)/τ)!
Where L
ij
denotes the contrastive loss foPr comparing the ith text image against the jth text, f
vi
and f represent the feature embeddings of the ith text image and jth text, respectively. τ is a
tj
parameterthat controlthe sharpnessof the outputdistribution. Note that f and f are different
vi ti
featuresextractedfromthesametext,astheithtextimageisadirectrenderingoftheithtext.
3 Experiment
3.1 ExperimentalSetup
Pretraining. We validate VisInContext with Open-Flamingo [9] and CosMo [25]. To enhance
computational efficiency, all models utilize float16 precision. For the 56B MOE [2] model, we
employDeepSpeed’s[3]Zero-2stagewithCPUoffloadingandfurtheroptimizethemodelbyquan-
tizingitto4-bitprecision1. WealsouseFlashAttention[17]tofurtherimprovememoryefficiency.
For all other experiments, we train the model using DeepSpeed Zero-2 without CPU off-loading.
TheOpen-Flamingo9BbaselineisbasedonMistral7B[5].
Our pretraining dataset includes a 180M subset of DataComp1B [26], MMC4 [13], the
OBELICS[14]dataset,andOCRRenderedText[27]. (MoredetailsareprovidedintheAppendix)
Foreach inputdocumentor image-textpair, we rendera textsequenceinto an imagewith a fixed
sizeof16x8192(512patches)bydefault,withp =p =16.
h w
DownstreamEvaluation. Ourobjectiveistodemonstratethatin-contextlengthcanbeextended
usingvisualtokens,therebyenhancingtheunderstandingofcomplexmultimodaldocuments. Con-
sequently,wefocusprimarilyontasksrelatedtolong-contextunderstanding.
1Theimplementationisfromhttps://github.com/TimDettmers/bitsandbytes.
4Method Text ICL Shots VQA Caption Classi. Mean
Tokens↑
okvqa textvqa vizwiz vqav2 coco flickr HM
Open- 0 40.2 21.3 23.3 47.8 82.3 59.4 60.4 47.8
Raw
Flamingo 256 4 42.5 22.2 32.2 49.8 90.5 63.5 63.8 52.1
Text
MOE[9]† 32 46.8 23.2 40.5 49.9 98.2 66.2 66.0 55.8
+ Ren- 0 39.5 26.4 26.3 48.5 84.4 60.5 62.2 49.7
+ VisIn-
dered 2048 4 44.3 28.9 32.0 50.3 94.2 65.3 65.5 54.4
Context
Image 32 46.3 31.2 41.2 51.0 101.3 68.4 65.2 57.8
Table 1: Increasing in-context text length with VisInContext significantly improves performance on
multi-modality downstream tasks. The model is pre-trained with a 56B MOE model. ICL stands for in-
contexttextlength. HMisshortforhatefulmemes. WithVisInContext,weincreasetheICLfrom256to2048,
leadingtoclearimprovementsoverthebaseline.†indicatesourimplementation.
Method Text Text T-Shots VQA Caption Mean
Source To-
kens
okvqa textvqa vizwiz vqav2 coco flickr
Open- 10 0 18.1 14.8 21.5 26.5 40.1 32.1 25.5
Raw
Flamingo9B † 62 4 23.8 18.1 23.7 40.5 57.5 35.3 33.2(7.7↑)
Text
Baseline[9] 426 32 25.2 16.4 25.5 34.6 66.1 38.5 34.4(8.9↑)
10 0 16.2 16.8 15.4 30.6 42.3 33.5 25.8
Rendered
+VisInContext 10 4 17.2 21.8 19.7 35.2 52.4 35.2 30.3(4.5↑)
Image
10 32 21.3 22.6 21.5 38.8 60.3 37.0 33.6(7.8↑)
Table 2: VisInContexteffectivelyincorporatesin-contexttextwithvisualtokens,demonstratingsignif-
icantperformanceimprovementswithconsistenttokenusage. Here, T-shotsrefertotext-onlyin-context
examples.TokensindicatethelengthoftheinputtotheLLM.Textsourcedescribesthepreprocessingmethod
forin-contextexamples.†denotesourimplementationon180Mpretrainingdata.
To evaluate the long-context understanding ability, we adopt the few-shot evaluation setting in
Flamingo [1]. We report answer accuracy on the OK-VQA [28], TextVQA [29], VizWiz [30],
andVQAV2[31]. Additionally,weassessperformanceoncaptioningtasksusingCOCO [32]and
Flickr30K[33]. Moreover,we alsoproposeasettingnamedtext-onlyincontextfew-shotstoex-
plore text-only in-context evaluation. For this setting, we use in-contextsampling without visual
inputtogeneratelong-contextinputsandthevisualinputisnotobservedbythemodel.
Inordertoillustratetheimpactofhavinglongin-contexttext,weevaluatethemodelfordocument
understandingonDocVQA[15]andOCR VQA[34]. Lastly, we introducea newtask, sequential
multimodal document retrieval. This dataset is based on the existing interleaved OBELICS [14]
dataset. Furtherdetailsareprovidedintheappendix.
3.2 In-contextFew-shotEvaluation
Impact of Extended In-Context Text Length. Interleaved documentdatasets typically contain
longtexts. Forinstance,theOBELICS[14]datasethasanaveragetokenlengthof815tokensper
document. DuetoGPUmemoryconstraints,Flamingo-likemodels[1,9,14]onlysub-sample256
tokens during pretraining, which leads to a significant loss of context information. We compare
the baseline modelpre-trainedwith 256 tokens, againstour method with an increasing In-context
Text Length to 2048 tokens. Table 1 shows a clear advantage of VisInContext. For example, on
TextVQA, accuracy improves from 23.2% to 31.2% with 32-shot. Similarly, the average model
performanceacrossalldatasetsshowanincreasefrom55.8%to57.8%.Thesefindingsdemonstrate
that VisInContext effectively increases the In-context Text Length to improve multi-modality
understanding.
Few-shot Evaluation with Text-only In-context Examples. As downstream tasks often differ
in formatfrompretrainingdata, severalworks[1, 9, 14] havetestedthe few-shotabilitiesof mod-
elsusingin-contextexamples. Forinstance, in the VQA dataset, a few question-and-answerpairs
are provided as in-context examples with visual signals. However, for zero-shot evaluation, two
question-and-answer pairs are added as in-context examples without visual signals in [1, 9, 14].
5Method TextSource DocVQA OCRVQA
val test
Open-Flamingo-9BBaseline[9] RawText 45.3 48.2 51.5
+VisInContext RenderedImage 48.5(3.2↑) 52.2(4.0↑) 58.4(6.9↑)
Table3:VisInContextclearlyboostingthebaselineondocumentunderstandingtasks.
Figure3:VisInContextsignificantlyimprovestheOCRabilityofLLM.WepresenttheRenderedText[27]
imagesandthecorrespondingnext-wordpredictionaccuracyonthevalidationset.Usingthesamepre-training
steps, VisInContextachieves significantlybetter resultsinpredictingwordsinvisual images, evenwhen the
fontsaredifficulttorecognize.
Follow the zero-shot setting, we examine the effect of having text-only in-context examples and
extend it to multi-shot setting, by leaving out the corresponding images (See Appendix for more
details). WecomparemodelperformanceofthebaselineOpen-Flamingo9Bandourmethodunder
thesamesetting,wherethedifferenceslieinhowthesetext-onlyin-contextexamplesareprocessed.
Specifically,Open-Flamingodirectlytakesinthemastexttokens,whileVisInContexttakesin the
correspondingrenderedtextimages.
Table 2 summarizes the results across four VQA bench-
marksandtwocaptioningbenchmarks.Notably,compared
to the text-only 0-shot setting, our VisInContext with 32-
shotsignificantlyis improvedonallVQA andcaptioning
benchmarksconsidered. Thoughthe32-shotperformance
ofVisInContextisslightlylowerthanthecompetingbase-
line,wecutdowntheinputtokenstotheLLMfrom426to
only10TextTokenLength,whichleadtosignificantreduc-
tion in the inference cost. These outcomes highlighttwo
key points: i. VisInContext can effectively understand
textrenderedinimages. ii. Textrenderedasimagescan
becomparablyeffectiveasrawtext,whenusedastext-only
Figure 4: VisInContext extends the in- in-contextexamples.
context text length of MOE based MLLM
from1kto9katinferencestage.
ComparisononInferenceCost. Wethenanalyzethein-
ferencecostofVisInContextandcomparetothe baseline. Bothmodelsarebasedona 56BMOE
LLM with a batch size of one to explore the maximum manageable In-context Text Length. The
results,showninFigure4,demonstratethattheIn-contextTextLengthcanbeextendedupto9192
tokensforthe56BMOEmodelon80GBH100GPUswithourmethodatinferencestage.Thisresult
highlightsthe efficiencyand advantagesof VisInContext, also show its potentialin understanding
verylongdocument.
6Figure5:Sequentialmulti-modalretrievalexample.TheinputsequenceisI1,T1,R1,I2,T2,R2thatfrom
interleaveddocumentinOBELICS[14]dataset.
VisualInput TextInput SurroundingTextInput Seq-I Seq-T
RawImage RawText - 16.3 64.8
RawImage RawText RawText 18.9 67.5
RawImage RawText RenderedTextImage 22.7 66.5
Table 4: ThemodelpretrainwithVisInContextsignificantlyimprovessequenceunderstandingability.
WereportthesequenceretrievalresultonOBELICS-Hybrid6.
3.3 Documentunderstanding
In this section, we evaluate the model on documentunderstandingtasks. Unlike common vision-
languagetasksthatusuallyshort-formpairs,thistaskrequirescomprehensionoflongandcomplex
documentdata. We evaluate our modelon DocVQA and OCRVQA. All documentimages are of
size384×384.FollowingPix2Struct[35],wefinetunethemodelonDocVQAtraindataandreport
performanceontheaveragenormalizedLevenshteinsimilarity(ANLS)metric.
Results in Table 3 show that our method significantly outperformsthe baseline. For instance, we
achievea6.9%improvementonOCRVQA.Tofurtheranalyzewhyourmethodenhancesdocument
understanding,wepresentthevalidationaccuracyoftheLLMontheRenderedText[27]datasetdur-
ingpretraininginFigure3. Weobserveasubstantialimprovementinnextwordpredictionaccuracy,
with top-1accuracyincreasingfrom 67.37%to 85.25%(a 16%improvement)and top-5accuracy
risingfrom80.76%to93.38%. Thesefindingsindicatethatthe LLMcaneffectivelyunderstand
textembeddedinvisualsignalswithVisInContext.
3.4 SequentialMulti-modalRetrieval
Inordertofurtheranalyzethebenefitofhavinglongtextcontextinmultimodalmodeling,wepro-
poseanewtask–SequentialMultimodalRetrieval(SMR),basedondocumentdatafrominterleaved
OBELICS [14] dataset. The documentis composedof interleaveddata, consisting of imagesand
textsarrangedinameaningfulsequence.
WeshowonesampleinFigure5anddefinetheinputandoutputofthistaskasbelow:Input: Given
apairofcontentitems,animageandacorrespondingtext(I1,T1,R1,I2,T2,R2),fromadocument
D. I isImage,T isthematchedtextandRisthesurroundingtext. Output: Thetaskistoretrieve
thenextimageI2andthenexttextT2inthesequence.NamedasSeq-IandSeq-T,correspondingly.
Wesamplethefirst1KdocumentsthatcontaindatalikeI1,T1,R1,I2,T2,R2 fromOBELICS[14]
andnameditasOBELICS-Hybrid6,whichhaveatleastthreeframesandthreetexts. (Seeappendix
formoredetails.) Thistaskencouragesthemodeltoleveragethecontextualandsemanticrelation-
shipininterleavedsequencestoeffectivelypredictandretrievethesubsequentpair.
Toenableourmodelwithretrieval,wefollowCosMo[25]toaddasimplecontrastiveheadbetween
visualembeddingandlanguageembeddingfromthemiddlelayers. Recallthatvisualembeddings
are either from raw images or renderedimagesor the addition of the two in our method. Table 4
reportstheresultsfromourmodelwithseveralinputvariants. Weobservetakingsurroundingtext
inputasrenderedtextimageperformsmuchbetterontheSequencetoimageretrieval,whileonpar
7Method PretrainTextSource Task
DocVQA-val
FuYu9B[8]† Raw-Text 42.3
+VisInContext +RenderedImage 44.5(2.2↑)
Table 5: Pretraining with VisInContext helps on long-context understanding task for FuYu
model. †meansourimplementationon180Mdata.
onSequencetotextretrieval,whencomparedwithtakingsurroundingtextinputasrawtext. These
resultsfurthersupportthedesignsofVisInContext inthecontextofdocumentunderstanding.
3.5 ExtensiontoMLLMwithLinearEmbedding
Beyond utilizing the visual encoder, some works [8, 36] also employ linear embeddingto extract
visual features directly from raw images. To show the generality of our method, we also explore
FuYu [8] model as a baseline and integrate VisInContext into the model. (See the appendix for
moredetails.) AsindicatedinTable5,ourmethodissuccessfulinimprovingtheperformanceson
DocVQAdatasetthatrequirelong-contextunderstanding.
TextImage TokenMasking TCCL Ok-VQA TextVqa VizWiz VqaV2
11.5 15.3 8.7 24.2
X 11.3 15.0 9.4 30.1
X X 17.8 18.3 15.3 33.5
X X 13.5 15.3 10.3 30.9
X X X 17.2 21.8 19.7 35.2
Table6:Ablationstudyofthecomponentinourpipelinefortext-only4-shotexample.
FontSize 4 6 8 10 12 Dataset 2 4 8 16 32
TextVQA 15.4 17.2 18.5 21.8 20.3 TextVQA 21.8 20.5 21.3 18.5 15.3
DocVQA 39.8 42.5 45.6 44.3 36.2 DocVQA 44.3 43.2 39.4 40.5 36.6
Table7:Fontsizeablation.Wereporttheresult Table8: Fontintervalthreshablation. Larger
onDocVQAvaldataset. threshleadstofewtextsingeneral.
3.6 AblationStudy
AblationsonModelDesign. Weconductablationstudiesonthefollowingmodelingcomponents
to demonstrate their effectiveness: Text Image, TCCL, and Token Masking. Results are detailed
in Table 6, which reveal two findings: 1. Token Masking is crucial for the model to learn from
rendered text images. Without Token Masking, the model can only perform comparably to the
baseline. Forcingthe modelto learn textsemantics from renderedtext imagesvia token masking
significantly improvesmodel performance. 2. Utilizing TCCL with Token Masking yields better
performancethanusingTokenMaskingalone.
Ablations on Font Size and Interval Threshold. As shown in Table 7, optimal performance
varieswithchangesinfontsize.Wefoundthatadjustingthefontsizeimpactsperformancesimilarly
toalteringthepatchsize—bothmethodseffectivelyincreasethecontextualinformationwithineach
patch. We prefer modifying the font size over the patch size because it allows for more intuitive
adjustments. Ourfindingsindicatethatthemodeldoesnotneedahighlydetailedunderstandingof
eachwordtoperformeffectively.
Another important factor is the font interval threshold. As shown in Table 8, we observed that a
too-largeintervalleadstoinferiorresults.Thisisintuitivebecausealargerthresholdresultsinfewer
textsintherenderedtextimage.
4 Related Work
Multimodal Language Models. Current mainstream Multimodal Large Language Models
(MLLMs)[22, 37,38, 39,40, 41]leveragethecapabilitiesofLargeLanguageModels(LLMs)[6,
842]duetotheirstrongreasoningabilities,asdemonstratedbyrecentadvancements. Thesemodels
typically adoptone of two primary designs for integrating visual information. The first approach
involvestheeffectiveadaptationofvisualrepresentations,whichareacquiredviaaseparatevisual
encoder, into the text-based LLM framework like CLIP, GIT, and BLIP2 [22, 37, 43]. The repre-
sentativemethodinthiscategoryincorporatesvisualrepresentationsintothelanguagemodelusing
cross-attention, as seen in the Flamingo series models [1, 9, 14]. Along this line, recently some
workslike LLaVA [40], EMU2 [44], InternVL [45], DeepSeeker [10], and QWen [41] lead to su-
periorresultsonmulti-modalitytaskswithsupervisedfinetuningonhigh-qualitydata. Thesecond
approachusesvisualembeddingsdirectlyasinput"tokens"fortheLLMs,bypassingthetraditional
useofaseparatevisualencoder. Thismethodprocessesvisualpatcheswithalinearlayeranduses
theresultingembeddingsasdirectinputstotheLLM,asimplementedinmodelslikeViLT[36]and
FuYu[8]. Thisstrategyomitstheneedforanadditionalvisualencoderandsimplifiesthearchitec-
ture.
Inthiswork,weadopttheFlamingo[1]architectureasourmainbaselineforthefollowingreasons:
First,theFlamingomodelemphasizesin-contextfew-shotlearningabilityanddesignscomprehen-
sive few-shot evaluation strategies. Second, our focus is on extending the in-context text length
duringpre-trainingratherthanonsupervisedfine-tuning.
EnhancingTextUnderstandingthroughVisualInputs. Traditionaltexttokenizationprocesses
rawtextefficiently,butitfaceschallengessuchasvulnerabilitytospellingerrorsandlimitedcross-
lingualtransferability[46,47].Theseissueshavepromptedtheexplorationoftokenizer-freemodels,
whichaim to improverobustnessandfacilitate bettercross-languageapplicability. Forinstance, a
single spelling error can lead to entirely different tokens using traditional tokenization methods,
impactingmodelperformance.
Recent developmentshave seen innovative approacheslike the Pixel model [46], which proposes
processing text as an image using both an image encoder and an image decoder. This approach
hassparkedaseriesofstudiesthatprocessnotonlytextualdatabutalsoimages,charts,andtables
throughaunifiedvisualinputsystem[35,46,47,48]. Thesemodelsaretrainedonadiversearray
ofvisualdata,suchaswebpagescreenshotsanduserinterfaceimages,sourcedextensivelyfromthe
internet. They are specifically designed to handle visually-situated text in an end-to-end manner,
offeringthepotentialtosupportawiderangeofapplications.
LongContextModeling. ThechallengeofincorporatingmoretokensintoLLMsisanactivearea
ofresearch[49,50]. Commonapproachesinvolvenovelself-attentionmechanisms[18]ormemory
banks[16]. Someworks[51]exploittensorparallelismorsequenceparallelismtoreducememory
costs. Inmulti-modalityresearch,closed-sourcemodelslikeGemini[52]andGPT-4V[53]support
longcontextinferenceuptomillionsoftokens. Open-sourcemodelssuchasMA-LMMforLong-
TermVideoUnderstanding[20]canprocessuptoonehourofvideousingalongmemorybank.The
mostrelevantworkLargeWorldModel[19]extendstokenlengthusingRingAttention.
Incontrasttothesemethods,ourmethodutilizesoff-the-shelfLLMsandcompressestexttokensinto
visualtokensforefficientprocessing.Ourmethodiscomplementarytotheseexistingtechniquesand
canbeintegratedwiththemtoachievelowercomputationalcostandlongercontextlength.
5 Conclusionand Limitations
This paper centers on multi-modalitylearning and addresses the in-contextlength limitations pre-
sentedbyheavycomputationalcostofLLMsinMLLMs. Ourcontributionisanovelandefficient
methodnamedVisInContext,whichenablesthemodeltoperceivelongtextcontextasrenderedtext
images.ComprehensiveexperimentsshowthatVisInContextiseffectiveonconventionalin-context
few-shotevaluationsanddocumentunderstanding,whilebeingmuchmoreefficient.
Onelimitationofourmethodis, currentlyourmethodrequiresprocessingafixedsizeimageeven
forbrieftexts.Infuturework,weplantodynamicallyreducetokencountswithvariableimagesizes
byretainingonlynon-emptytokensduringpre-training.Weaimtoexpandthismethodtoadditional
tasksandencouragethecommunitytofurtherexplorethisdirection.
9References
[1] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
Arthur Mensch, KatherineMillican, Malcolm Reynolds, et al. Flamingo: avisual language model for
few-shotlearning. AdvancesinNeuralInformationProcessingSystems,35:23716–23736,2022.
[2] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBamford,
Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of
experts. arXivpreprintarXiv:2401.04088,2024.
[3] JeffRasley,SamyamRajbhandari,OlatunjiRuwase,andYuxiongHe. Deepspeed: Systemoptimizations
enabletrainingdeeplearningmodelswithover100billionparameters. InProceedingsofthe26thACM
SIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining,pages3505–3506,2020.
[4] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan,MonaDiab,XianLi,XiVictoriaLin,etal. Opt: Openpre-trainedtransformerlanguagemodels.
arXivpreprintarXiv:2205.01068,2022.
[5] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot,Diego
delasCasas, FlorianBressand, GiannaLengyel, GuillaumeLample, LucileSaulnier, etal. Mistral7b.
arXivpreprintarXiv:2310.06825,2023.
[6] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and
fine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[7] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert:Pre-trainingofdeepbidirec-
tionaltransformersforlanguageunderstanding. InNAACL-HLT(1),2019.
[8] AdeptAI. Fuyu-8B. https://www.adept.ai/blog/fuyu-8b, n.d. Accessed:[insertdatehere].
[9] AnasAwadalla, IrenaGao, JoshGardner, JackHessel, Yusuf Hanafy, WanrongZhu, Kalyani Marathe,
YonatanBitton,SamirGadre,ShioriSagawa,etal.Openflamingo:Anopen-sourceframeworkfortraining
largeautoregressivevision-languagemodels. arXivpreprintarXiv:2308.01390,2023.
[10] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren,
ZhuoshuLi,YaofengSun,etal. Deepseek-vl: towardsreal-worldvision-languageunderstanding. arXiv
preprintarXiv:2403.05525,2024.
[11] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,
hypernymed, imagealt-textdatasetforautomaticimagecaptioning. InProceedingsofthe56thAnnual
Meetingof theAssociationforComputational Linguistics(Volume1: LongPapers), pages2556–2565,
2018.
[12] ChristophSchuhmann,RichardVencu,RomainBeaumont,andKaczmarczyk.Laion-400m:Opendataset
ofclip-filtered400millionimage-textpairs. arXivpreprintarXiv:2111.02114,2021.
[13] WanrongZhu,JackHessel,AnasAwadalla,SamirYitzhakGadre,JesseDodge,AlexFang,YoungjaeYu,
LudwigSchmidt,WilliamYangWang,andYejinChoi. Multimodalc4: Anopen,billion-scalecorpusof
imagesinterleavedwithtext. arXivpreprintarXiv:2304.06939,2023.
[14] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov,
ThomasWang, SiddharthKaramcheti,AlexanderMRush,DouweKiela,etal. Obelics: Anopenweb-
scalefiltereddatasetofinterleavedimage-textdocuments. InThirty-seventhConferenceonNeuralInfor-
mationProcessingSystemsDatasetsandBenchmarksTrack,2023.
[15] MineshMathew,DimosthenisKaratzas,andCVJawahar.Docvqa:Adatasetforvqaondocumentimages.
InProceedingsoftheIEEE/CVFwinterconferenceonapplicationsofcomputervision,pages2200–2209,
2021.
[16] YuhuaiWu,MarkusNRabe,DeLesleyHutchins,andChristianSzegedy.Memorizingtransformers.arXiv
preprintarXiv:2203.08913,2022.
[17] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-
efficient exact attention with io-awareness. Advances in Neural Information Processing Systems,
35:16344–16359,2022.
[18] HaoLiu,MateiZaharia,andPieterAbbeel. Ringattentionwithblockwisetransformersfornear-infinite
context. arXivpreprintarXiv:2310.01889,2023.
10[19] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and
languagewithringattention. arXivpreprintarXiv:2402.08268,2024.
[20] BoHe,HengduoLi,YoungKyunJang,MenglinJia,XuefeiCao,AshishShah,AbhinavShrivastava,and
Ser-NamLim.Ma-lmm:Memory-augmentedlargemultimodalmodelforlong-termvideounderstanding.
arXivpreprintarXiv:2404.05726,2024.
[21] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek, and Hervé Jégou. Three things
everyone shouldknow about visiontransformers. InEuropeanConference onComputerVision, pages
497–515.Springer,2022.
[22] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
naturallanguagesupervision.InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,
2021.
[23] YiqiLin,ConghuiHe,AlexJinpengWang,BinWang,WeijiaLi,andMikeZhengShou. Parrotcaptions
teachcliptospottext. arXivpreprintarXiv:2312.14232,2023.
[24] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUn-
terthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal. Animageisworth
16x16 words: Transformers for image recognition at scale. In International Conference on Learning
Representations,2020.
[25] AlexJinpengWang,LinjieLi,KevinQinghongLin,JianfengWang,KevinLin,ZhengyuanYang,Lijuan
Wang,andMikeZhengShou. Cosmo: Contrastivestreamlinedmultimodalmodelwithinterleavedpre-
training. arXivpreprintarXiv:2401.00849,2024.
[26] AlexFangSamirYitzhakGadre,GabrielIlharco. Datacomp: Insearchofthenextgenerationofmulti-
modaldatasets. arXivpreprintarXiv:2304.14108,2023.
[27] Wendler,Chris.Renderedtextdataset.https://huggingface.co/datasets/wendlerc/RenderedText,
2023. Accessed:2023-05-05.
[28] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual ques-
tionansweringbenchmarkrequiringexternalknowledge. InProceedingsoftheIEEE/cvfconferenceon
computervisionandpatternrecognition,pages3195–3204,2019.
[29] Amanpreet Singh, VivekNatarajan, MeetShah, YuJiang, XinleiChen, DhruvBatra, DeviParikh, and
MarcusRohrbach. Towardsvqamodelsthatcanread. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages8317–8326,2019.
[30] DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,andJef-
freyPBigham. Vizwizgrandchallenge: Answeringvisualquestionsfromblindpeople. InProceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition,pages3608–3617,2018.
[31] YashGoyal, TejasKhot, Douglas Summers-Stay, DhruvBatra, andDevi Parikh. Making thevinvqa
matter: Elevatingtheroleofimageunderstanding invisual questionanswering. InProceedings ofthe
IEEEconferenceoncomputervisionandpatternrecognition,pages6904–6913,2017.
[32] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,
andCLawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InComputerVision–ECCV2014:
13th EuropeanConference, Zurich, Switzerland, September 6-12, 2014, Proceedings, PartV 13, pages
740–755.Springer,2014.
[33] BryanAPlummer,LiweiWang,ChrisMCervantes, JuanCCaicedo, JuliaHockenmaier, andSvetlana
Lazebnik. Flickr30kentities: Collectingregion-to-phrasecorrespondencesforricherimage-to-sentence
models. In Proceedings of the IEEE international conference on computer vision, pages 2641–2649,
2015.
[34] AnandMishra,ShashankShekhar,AjeetKumarSingh,andAnirbanChakraborty. Ocr-vqa: Visualques-
tionansweringbyreadingtextinimages. InICDAR,2019.
[35] KentonLee,MandarJoshi, IuliaRalucaTurc,HexiangHu,FangyuLiu,JulianMartinEisenschlos,Ur-
vashiKhandelwal,PeterShaw,Ming-WeiChang,andKristinaToutanova. Pix2struct:Screenshotparsing
as pretraining for visual language understanding. In International Conference on Machine Learning,
pages18893–18912.PMLR,2023.
11[36] WonjaeKim,BokyungSon,andIldooKim. Vilt: Vision-and-languagetransformerwithoutconvolution
orregionsupervision. InInternationalconferenceonmachinelearning,pages5583–5594.PMLR,2021.
[37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-
trainingwithfrozenimageencodersandlargelanguagemodels. arXivpreprintarXiv:2301.12597,2023.
[38] DannyDriess, FeiXia, Mehdi SMSajjadi,CoreyLynch, Aakanksha Chowdhery, BrianIchter, Ayzaan
Wahid,JonathanTompson, QuanVuong, TianheYu,etal. Palm-e: Anembodiedmultimodallanguage
model. arXivpreprintarXiv:2303.03378,2023.
[39] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,MojtabaSeyedhosseini,andYonghuiWu. Coca:
Contrastivecaptionersareimage-textfoundationmodels. arXivpreprintarXiv:2205.01917,2022.
[40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in
neuralinformationprocessingsystems,36,2024.
[41] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou. Qwen-vl: Aversatilevision-languagemodelforunderstanding,localization,textreading,
andbeyond. 2023.
[42] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
GretchenKrueger,TomHenighan, RewonChild,AdityaRamesh, DanielZiegler,JeffreyWu,Clemens
Winter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,
ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,andDarioAmodei. Languagemod-
elsarefew-shot learners. InH.Larochelle, M.Ranzato, R.Hadsell, M.F.Balcan, andH.Lin,editors,
Advances inNeural InformationProcessingSystems, volume33, pages1877–1901. CurranAssociates,
Inc.,2020.
[43] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,
andLijuanWang. Git: Agenerativeimage-to-texttransformerforvisionandlanguage. arXivpreprint
arXiv:2205.14100,2022.
[44] QuanSun,QiyingYu,YufengCui,FanZhang,XiaosongZhang,YuezeWang,HongchengGao,Jingjing
Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint
arXiv:2307.05222,2023.
[45] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,ZhongMuyan,QinglongZhang,
Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic
visual-linguistictasks. arXivpreprintarXiv:2312.14238,2023.
[46] PhillipRust,JonasFLotz,EmanueleBugliarello,ElizabethSalesky,MiryamdeLhoneux,andDesmond
Elliott. Languagemodellingwithpixels. InternationalConferenceonLearningRepresentations,2023.
[47] Tianyu Gao, ZiruiWang, AdithyaBhaskar, and Danqi Chen. Improving language understanding from
screenshots. arXivpreprintarXiv:2402.14073,2024.
[48] MichaelTschannen,BasilMustafa,andNeilHoulsby. Clippo: Image-and-languageunderstandingfrom
pixelsonly. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages11006–11017,2023.
[49] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXivpreprintarXiv:1904.10509,2019.
[50] IzBeltagy,MatthewEPeters,andArmanCohan. Longformer: Thelong-documenttransformer. arXiv
preprintarXiv:2004.05150,2020.
[51] VijayAnandKorthikanti, JaredCasper,SangkugLym, LawrenceMcAfee, Michael Andersch, Moham-
mad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models.
ProceedingsofMachineLearningandSystems,5,2023.
[52] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu
Soricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighlycapablemulti-
modalmodels. arXivpreprintarXiv:2312.11805,2023.
[53] OpenAI. Gptvsystemcard,2023. Accessed:2024-05-22.
12