Parrot: Multilingual Visual Instruction Tuning
Hai-LongSun1,2,3∗ Da-WeiZhou1,2 YangLi3 ShiyinLu3 ChaoYi1,2
Qing-GuoChen3 ZhaoXu3 WeihuaLuo3 KaifuZhang3 De-ChuanZhan1,2 Han-JiaYe1,2†
1SchoolofArtificialIntelligence,NanjingUniversity
2NationalKeyLaboratoryforNovelSoftwareTechnology,NanjingUniversity
3AIBusiness,AlibabaGroup
Abstract
TherapiddevelopmentofMultimodalLargeLanguageModels(MLLMs)likeGPT-
4Vhasmarkedasignificantsteptowardsartificialgeneralintelligence. Existing
methodsmainlyfocusonaligningvisionencoderswithLLMsthroughsupervised
fine-tuning(SFT)toendowLLMswithmultimodalabilities,makingMLLMs’in-
herentabilitytoreacttomultiplelanguagesprogressivelydeteriorateasthetraining
processevolves. WeempiricallyfindthattheimbalancedSFTdatasets,primarily
composedofEnglish-centricimage-textpairs,leadtosignificantlyreducedperfor-
manceinnon-Englishlanguages. Thisisduetothefailureofaligningthevision
encoderandLLMwithmultilingualtokensduringtheSFTprocess. Inthispaper,
weintroducePARROT,anovelmethodthatutilizestextualguidancetodrivevisual
tokenalignmentatthelanguagelevel. PARROTmakesthevisualtokenscondition
on diverse language inputs and uses Mixture-of-Experts (MoE) to promote the
alignmentofmultilingualtokens. Specifically,toenhancenon-Englishvisualto-
kensalignment,wecomputethecross-attentionusingtheinitialvisualfeaturesand
textualembeddings,theresultofwhichisthenfedintotheMoEroutertoselectthe
mostrelevantexperts. Theselectedexpertssubsequentlyconverttheinitialvisual
tokensintolanguage-specificvisualtokens. Moreover,consideringthecurrentlack
ofbenchmarksforevaluatingmultilingualcapabilitieswithinthefield,wecollect
andmakeavailableaMassiveMultilingualMultimodalBenchmarkwhichincludes
6languages,15categories,and12,000questions,namedasMMMB.Ourmethod
notonlydemonstratesstate-of-the-artperformanceonmultilingualMMBenchand
MMMB,butalsoexcelsacrossabroadrangeofmultimodaltasks. Boththesource
codeandthetrainingdatasetofPARROTwillbemadepubliclyavailable.
1 Introduction
TherapiddevelopmentofLargeLanguageModels(LLMs),suchasGPT-4[52;8;45;46;53;59],
hasgainedsignificantattention. However,LLMsarelimitedtoprocessingasingletextualmodality.
TheexpansionintovisualmodalitieshasendowedLLMswithmultimodalcapabilities[64;2;63;74;
17;27;32;65;71;7],therebyacceleratingthedevelopmentofMultimodalLargeLanguageModels
(MLLMs)andfurtherbringingusclosertotherealizationofArtificialGeneralIntelligence(AGI).
CurrentMLLMsmainlyrelyonpre-trainedLLMsandvisionencoders,focusingonbridgingthe
modalitygapbyaligningvisualfeatureswithlanguageembeddingtokens. Existingresearchpre-
dominantlyemployseitheraQ-Former[31;17;6]oranMLPprojector[38;35;12;34;37;42]to
alignvisionencoderswithLLMs. Thetrainingdata[38]mainlyconsistsofEnglish-centricdatafrom
∗WorkdoneduringaninternshipatAIBusiness,AlibabaGroup.
†Correspondingauthor,email:yehj@lamda.nju.edu.cn.
Preprint.Underreview.
4202
nuJ
4
]VC.sc[
1v93520.6042:viXra🤖 Chinese-CLIP+Qwen1.5-Chat7B 🎃 OpenAI-CLIP + Qwen1.5-Chat 7B
👨💻 请问图中的人手上拿了什么东西？ 图中的人手里拿着一个奖杯。 🤖
图中的人手里拿着一个奖杯。 🎃
👨💻 图中描述的是什么内容？ 图中描述的是一个年轻男子在比赛场上，手持奖杯，
庆祝胜利。他穿着运动服装，肌肉发达，表现出了 🤖
胜利者的自豪和喜悦。
Thepersonintheimageisayoungmanwhoisstandinginastadium,holdingatrophy.Heis 🎃
wearingaredandwhitesportsuniform,whichsuggestthathe is a member of a sports team.
👨💻 图中的人在干什么？ 这个人在庆祝胜利，因为他正在举着一个奖杯。他可能
是一个运动员，因为奖杯通常用于奖励胜利者。他可能 🤖
在庆祝比赛胜利，或者在庆祝团队的成就。
Theimageportraysamanwhoisstandinganarena,holdingatrophy. 🎃
Figure1:TheoutputofOpenAI-CLIP-basedandChinese-CLIP-basedmodelsusingthesameChineseprompts.
WecanobservethattheOpenAI-CLIP-basedmodelexhibitsconfusionbetweenChineseandEnglishresponses.
imagecaptionsandmultimodalconversations. Duringtraining,thealignmentcomponentconverts
thevisualfeaturesintolanguageembeddingtokens. Withencodedvisualfeatures, theLLMcan
integratevisualinformationtorespondtomultimodalinputs.
MultilingualcapabilityinMLLMsentailstheabilitytogenerateresponsesinthesamelanguage
astheinput,cateringtothelinguisticdiversityinherentinconversation. Therefore,itiscritically
importantforprocessinglanguage-specificcontentandculturaldifferences,ensuringequitableaccess
totechnologicalbenefitsforindividualsacrossdiverseregionsandnations[13;24]. ManyLLMs
possessmultilingualcapabilities[60;5;46],enablingdiverselanguageresponsesaccordingtouser
input. However,afterthealignmenttrainingofMLLMs,themodelmayloseitsabilitytounderstand,
process,orgenerateinnon-Englishlanguages,andwecallthisphenomenonmultilingualerosion.
Forexample,LLaVA[38]usuallyrespondsinEnglish,regardlessoftheinputlanguage. Therefore,it
isessentialtoenhanceMLLM’smultilingualcapabilitiesduringmultimodalalignment.
Themainreasonformultilingualerosionisthatthedatausedformultimodalalignmentishighly
imbalancedatthelanguagelevel. DuetothedominanceofEnglish-centricdata,whilethemodel
alignsvisualandtextualtokenswellinEnglish,itperformspoorlyinotherlanguages. Hence,it
iscrucialtoalignvisualandtextualtokenscompatiblyatthelanguagelevel. Wehypothesizethat
multilingualerosionmayarisefromthelackofalignmentbetweenvisualtokensandtextualtokens
inotherlanguages. Fromtheperspectiveofpre-traineddatasets,OpenAI-CLIP[51]istrainedonthe
large-scaleimage-textpairsthroughcontrastivelearning,withthetextcorpusbeingmostlyinEnglish,
potentiallybiasingimageencodingtowardsanEnglishsemanticspace.AsshowninFigure1,wetrain
twoseparatemodelsusingthesamedata: onewithOpenAI-CLIPvisionencoderandtheotherwith
Chinese-CLIP[62]visionencoder. Interestingly,themodelequippedwithOpenAI-CLIPstruggles
togeneratesuitableoutputsaccordingtoChineseinputswhiletheothermodelwithChinese-CLIP
cannotonlyunderstandthequeriesbutalsogenerateappropriateoutputsinChinese. Furthermore,
weobservedanimprovementinperformance,from66.4to68.3,ontheMMBench-CN[39]dataset
whenusingChinese-CLIP.Therefore,thechallengearises: howtouseEnglish-centricmultilingual
image-textdatatobridgethemodalitygapwhileenhancingtheMLLM’smultilingualcapabilities.
Duetothescarcityofnon-Englishmultimodaldata(e.g.,lackoflarge-scale,high-qualityimage-text
data), we require almost the same amount of image-text data as LLaVA to enhance the model’s
multilingualcapabilities.Moreover,motivatedbypreliminaryexperiments,itisnecessarytocondition
thevisualtokensondiverselanguageinputs. Inthispaper,weintroducePARROT,anovelmethod
that utilizes textual guidance to drive visual token alignment at the language level and converts
visualtokensintolanguage-specificembeddingsusingaMixture-of-Experts(MoE)module[25;56].
Specifically,wefirstcalculatethecross-attentionbetweentheclasstokenofvisualfeaturesextracted
bythevisionencoderandthetextembeddingsderivedfromwordtokenembeddings. Theresult
is then passed through the router of MoE to obtain the activated probability distribution of each
languageexpert. Subsequently,demandingtheinputlanguage,theEnglish-biasedvisualtokensare
convertedintolanguage-specificembeddingsusingtheselectedexperts. ThisenablesPARROTnot
onlytoenhanceitsmultilingualcapabilitiesbutalsotobridgethemultimodalgapeffectively.
Toaddressthescarcityofcurrentmultilingualbenchmarks,weintroduceanewbenchmarkencom-
passingsixlanguages: English,Chinese,Portuguese,Arabic,Turkish,andRussian. Thisincludes
an extension of the MMBench-DEV dataset to these six languages and a Massive Multilingual
2MultimodalBenchmark(MMMB)featuring2,000evaluationquestionsperlanguage,totaling12,000
questions. Wecompareseveralopen-sourcemultimodalmethodsandevaluatesomeprivatemodels.
ExtensiveexperimentsvalidatethePARROT’sstate-of-the-artperformanceacrosstwomultilingual
benchmarks. SpecificallyinTurkishandArabic,ourmethodevenoutperformsLLaVA-NeXT[37]
bymorethan10percentagepointsinbothbenchmarks. Additionally,weevaluateourmodelacrossa
broadrangeofmultimodalbenchmarks(e.g.,MME[22],ScienceQA-IMG[41],andSEED-Bench-
IMG[28]),demonstratingitscompetitiveperformanceindiversitytasks.
2 RelatedWork
MultimodalLargeLanguageModels. ThedomainofMLLMshaswitnessedsignificantadvances,
particularly in the enhancement of visual and language processing. Current MLLM is usually a
combinationofvisualencoders[51;57;21;69;48;67],LLMs,andfusionmodules. Innovationslike
Flamingo[2]andOpenFlamingo[4]haveadvancedvisualrepresentationbyintegratingaPerceiver
Resamplerwithvisionencoders. BLIP-2[31]andInstructBLIP[17]employQ-Formertoconnect
thefrozenLLMandvisionencoder. InternVL[14]trainshugeViTandQFormertointegratevisual
modalitiesthroughamulti-stagetrainingmethod. MiniGPT4[74]leveragesbothaQ-Formeranda
linearprojectortobridgethegapbetweenthevisionmoduleandLLM.Furthermore,LLaVA[38]
adopts a simple MLP projector to promote the alignment between the LLM and vision encoder.
mPLUG-Owl[64]introducesanapproachthatbeginstofinetunethevisionencoderandalignvisual
features, followed by tuning the LLM using LoRA [23]. Qwen-VL [6] improves visual module
resolutionto448,aimingtorefinethemodel’svisualprocessingcapabilities. Fuyu-8B[7]directly
projectsimagepatchesbeforeintegrationwithLLM.MM1[43]hasconductedablativestudieson
connectordesignchoices,revealingthatthemodalityadaptertypeislesscriticalthanthenumberof
visualtokensandtheresolution. MiniGemini[34]utilizeshigh-resolutionvisualtokensandhigh-
qualitydatatonarrowtheperformancegapwithGPT-4andGemini. Withtherapidadvancements
inopen-sourcemodels,proprietarymodelssuchasGPT-4V/4o[46;47],Gemini[58;54],Qwen-
VL-Plus/MAX[6],andClaude3[3]haveachievedoutstandingresultsinevaluationsandpractical
applications. Inthiswork,owingtothesimplicityoftheLLaVAarchitecture,weadoptaframework
similartoLLaVAtodesignourmodel.
MultilingualMultimodalModels. Recentyearshavewitnessedrapidprogressintheexpansion
of multimodal models to include a wider variety of languages. M3P [44] leverages English as a
pivot and alternates between English-only vision-language pre-training and multilingual masked
languagemodeling. Incontrast,UC2[73]translatesEnglishcaptionsintovariouslanguagesanduses
imagesastheanchor. mCLIP[10]enhancestheCLIPmodelbyaligningitwithamultilingualtext
encoderthroughknowledgedistillation. Thankstotheexpansionoftheoverallcapabilitiesoflarge
languagemodels[1;5;26;66],theirmultilingualcapacitieshavesignificantlyimproved. Integrating
multilingualLLMswithvisualabilitieshasincreasinglybecomearesearchfocus. Inthedomainof
LLMs,PaLI[13]developsa17Bmultilinguallanguage-imagemodelthatspansover100languages.
Ying-VLM [33] discovers that instruction tuning in English can extend its applicability to other
languages. Ziya-Visual[40]illustratesthetranslationofEnglishimage-textdatasetsintoChinese,
usingin-contextlearningforinstruction-responsegeneration. VisCPM[24]introducesatraining
paradigmthatfine-tunestheMLLMinaquasi-zero-shotmannerbasedonastrongmultilinguallarge
languagemodel. Despitetheseadvancements,theyareprimarilyconfinedtotwolanguagesorrely
onthemassivetranslatedcorpus. Ontheotherhand,thereisnosuitablemultilingualbenchmark
forMLLMstoevaluatetheperformanceofmultiplelanguages. Therearealsosomemultilingual
researchstudiesinotherdomains,suchasmultilingualmachinetranslation[72;49;50;68].
3 MMMB:AMassiveMultilingualMultimodalBenchmark
Inthissection,wefirstdiscussthelimitationsofexistingbenchmarksandthenpresentthecharacter-
isticsthatanidealmultilingualbenchmarkshouldpossess. Furthermore,wedesignandconstructa
newbenchmarkandprovideitscorrespondingevaluationstrategy.
3Istheoutputofthecode‘‘b
islowerthan50’’? What is the capital of Michigan?
A. Pierre
B. Charleston
C. Grand Rapids
D. Lansing
a)Codereasoning b)Logicalreasoning c)Lowrelevancebetweenimageandtext
Figure2: Somebadcasesformultilingualbenchmarkperceive. Left: codereasoningisstronglyrelatedto
English.Middle:logicalreasoningistoochallenging.Right:lackrelevancebetweenimageandtext.
3.1 LimitationsofExistingBenchmarks
There are several existing multilingual benchmarks (e.g., Multi30K [20], M3Exam [70], MM-
Bench[39],andLLaVA-Bench[38;24])forMLLMs,buttheyhavesomelimitations: 1)Outdated
Benchmarks. Multi30kisdesignedforimage-textretrievaltasks,andtheperformancehasnearly
reachedtheupperboundduetotherelativelyeasyproblems. 2)Non-StandardizedEvaluations.
Otherbenchmarks,likeLLaVA-Bench,relyonevaluationsusingGPT-4. DependenceonGPT-4asa
defacto“GroundTruth”mayhinderreproducibility. Meanwhile,sinceLLaVAusesadeprecated
version (GPT-4-0314), using other different versions could result in unfair comparisons. On the
otherhand,becauseM3Examdoesnotofferconsistenttestsamplesacrossdifferentlanguages,it
cannotensurewhetherpoorperformanceisduetotheproblem’sdifficultyorthemodel’slackof
multilingual capabilities. 3) Limited Languages. MMBench and LLaVA-Bench are limited in
EnglishandChinese,whichcannotmeasurethemultilingualcapabilitiesacrossabroadspectrum.
3.2 ConstructionoftheMultilingualBenchmark
TomoresuitablyevaluatethemultilingualcapabilitiesofMLLMs,anidealbenchmarkshouldexhibit
thefollowingcharacteristics:
1)LanguageswithSignificantDifferences. Itshouldcoveradiversearrayoflanguagefamilies,
selectinglanguagesthatareasdistinctandnon-repetitiveaspossible.Thisensuresabroadassessment
ofMLLMs’abilitytoadaptacrosslinguisticvariances.
2)ProblemswithMediumLevelofDifficulty. Theproblemsshouldnotbetoodifficult(e.g.,logical
reasoning)becausetheaimistoassessthemultilingualunderstanding,processing,andgenerating
capabilitiesofMLLMs,notlogicalreasoningskills.
3)TaskswithMultilingualandMultimodal. AsshowninFigure2,datawithindatasetsshouldnot
bestronglyrelatedtoEnglish(e.g.,codereasoning). Itcannotbeinherentlytransformedintomultiple
languagessincetheyarecomposedofEnglishwords. Moreover,imagesshouldbeanindispensable
partwhenMLLMsanswerthequestion. Forinstance,ifgivenamapoftheUnitedStatesandasked
toidentifyitscapital,MLLMsonlyrequirethetext-onlyabilitytoanswerthisquestion. Therefore,it
isessentialthatquestionshighlightasignificantcorrelationbetweenimagesandtexts.
4)ContentConsistencyacrossLanguages.Thegoalofthisbenchmarkistoevaluatethemultilingual
capabilitiesofMLLMs,andweaimtoshowthediscrepanciesacrossdifferentlanguagesfairly. For
example,ifEnglishquestionsmainlyfocusonadditionwithinonehundredwhileChinesequestions
mainlyconcerncalculuscomputation,itbecomesdifficulttoascertainwhetherpoorperformancein
ChinesearisesfromthecomplexityoftheproblemorthelimitedmultilingualcapabilitiesofMLLMs.
Hence,itiscrucialtoensurecontentconsistencyacrosslanguagesforafaircomparison.
Weselectedsixlanguagesforinclusion: English(en),Chinese(zh),Portuguese(pt),Arabic(ar),
Turkish(tr),andRussian(ru).Theselanguagesrepresentadiverserangeoflinguisticfamilies,andwe
listthedetailedinformationandsomemultilingualcasesinFigure3. Intermsofdatasetrequirements
andconsistency,ourbenchmarkincorporatesdatasetsintwomainrespects: 1)SinceMMBench[39]
officiallyincludesEnglishandChineseversions,weextendittotheotherfourlanguages. 2)Forthe
massivemultilingualmultimodalbenchmark,denotedasMMMB,weselectandcleanthesuitable
datafromScienceQA[41],MME[22],andSEED-Bench[28]datasetswithestablishedguidelines.
ThesedatasetsarethenprocessedintoaVisualQuestionAnswering(VQA)format,resultingina
totalof12,000samplesacrossallsixlanguages.
4Can you see any clouds in the sky? Сколько белых стульев на картинке? Qual das seguintes opções está na terceira
A B.. N Tho e, ri et' s i sc no om sp kle yt e vl iy s ic bl le ear A B..1 2 c Ao .l au n ba i? blioteca
C D. . Y Ye es s, , a bn ud t tt hh ee yy aa rr ee sd pe an rs se e C D. .3 4 B C. .o o r de es pt aa ru tr aa mnt ee nto de polícia
English Russian D.a mercearia Portuguese
A B؟ﻲ ارﻨ .ً .ﻨ ﺎﻜ ﻻﻢﺴﻤ ﻌﯾﯾ ﻧ فﻞھ ﺎ ﻄ،ﻊ ﻌﻧط ﻻﺎ اﻘﺘﻟا ﻲﻓ ﺔﺘﻓﻻ ﺪﺟﻮﯾ ﻞھ 图 A B C. ..中 黑 棕灰所 色 色色示 的 的的的长凳是什么颜色？ G t A B Cü . .ö .r r i YSBs ne ö üs al ra l ze fın
ü
çkg l cüçaö ülır elr
l
ae g err rös ru üd la ü yö on rc ?elikli olarak ne
Arabic D. 白色的 Chinese D.Tüplü dalgıçlar Turkish
English Chinese L Ea nn gg lu ia sg he L Ia nn dg ou -a Eg ue rof pam eai nly Lan Gg eu ra mge ang ir coup Geography
Maps
126 ,0L 0a 0ng su aa mg pe ls
es
P ortuguese PoC Arh
t
rui an
g
be
u
is cee
se
InS Adi fn
o
ro
-
oE- -T Aui
r
sb
o
ie apt tea
ia
cn
n R
SS
o
ei
m
mni
a
it
n
ti ic
c ce 15categories···
Arabic Turkish RTu ur sk sii ash
n
IndoA -Elt ua ri oc
pean
T Su lar vk ii cc Count Color
Figure3:OverviewofMMMB.Itincorporates6languages,15categories,and12,000questions.
3.3 EvaluationStrategy
Sincerandomguessingcanleadto∼25%Top-1accuracyfor4-choicequestions,potentiallyreducing
the discernible performance differences between various MLLMs. Additionally, MLLMs may
prefer to predict a certain choice among all given choices [39], which further amplifies the bias
in evaluation. To address these issues, we implement a circular validation strategy inspired by
MMBench. Specifically,MMMBisadaptedtotheformatofYes/Noquestions,whereeachimageis
pairedwithtwoquestions,demanding‘Yes’and‘No’answers,respectively. AsshowninFigure8,
ananswerisconsideredaccurateonlyifbothquestionsareansweredcorrectly;failingeitherresults
in marking the entire instance as incorrect. This strategy ensures a more rigorous evaluation of
MLLMs,reducingthelikelihoodofrandomguessingandpromotingmorevalidatedcomparisons
acrossdifferentmodels.
4 Methods
4.1 Preliminaries: VisualInstructionTuning
ArepresentativeworkinMLLMsisLLaVA[38],whichintroducesasimpleyeteffectivemethodfor
achievingalignmentbetweenthevisionencoderandthepre-trainedLLM.Specifically,foragiven
inputimageX ,LLaVAutilizesthepre-trainedCLIPvisionencoderViT-L/14[51]toextractthe
v
visualfeaturesZ =g(X ).ItthenemploysVicuna[15]astheLLMtoobtainthetextualembeddings
v v
H . ToalignthevisionencoderwiththeLLM,aprojectorintheformofamulti-layerperceptron
t
(MLP)denotedasWislearned. ThisprojectorconvertsZ intolanguageembeddingtokensH ,
v v
effectivelyfacilitatingtheintegrationofmultimodalinformationwithintheLLM’sframework.
H =W·Z , with Z =g(X ). (1)
v v v v
Finally, we input H and H into LLM to generate the model’s responses. However, after the
v t
modalityalignmenttraining,LLaVAlosesitsabilitytoprocessinnon-Englishlanguages.
4.2 PilotStudy
To address the challenge of multilingual erosion in MLLMs due to the dominance of English in
image-textdata,wehypothesizethatthereisaninherentmismatchbetweenvisualtokensH and
v
textualtokensH ,whichtendstobiasthemtowardsEnglishsemantics,makingthemodelmorelikely
t
togenerateoutputsinEnglish. Specifically,thewidely-usedvisionencoderofOpenAI-CLIP[51]is
pre-trainedonavastcorpusofEnglish-centricimage-textpairs,resultinginvisualrepresentations
morealignedwithEnglish.
Toexplorethisphenomenon,wetraintwodistinctmodelsusingthesamedata: oneincorporating
OpenAI-CLIPvisionencoderandtheotherintegratingChinese-CLIP[62]visionencoder. Asshown
inFigure1,themodelequippedwithOpenAI-CLIPstrugglestogeneratesuitableoutputsaccording
5
naissuR setubirttA1.The animal in the picture is a rabbit Pre-training
2.图中的动物是一只兔子
3. Resimdeki hayvan bir tavşandır. Weighted-Sum LLM
MoE
LargeLanguageModel
MLP1 MLP2 MLP3 ··· MLPn Projector
··· ··· VisionEncoder
MultilingualMoE
InstructionTuning
Linear
Cross-Attention LLM
SiLU
Projection WordEmbedding Router Linear MoE
Vision MLPExpert Projector
1.What animal is in the picture?
Encoder 2.图片中的动物是什么？ MultilingualMoEblock VisionEncoder
3. Fotoğraftaki hayvan nedir?
Figure4:TheoverallarchitectureofPARROT.ItconvertsEnglish-biasedfeaturestolanguage-specificfeatures
basedonthemultilingualMoEmodule,aimingtoimprovethemultilingualcapabilities.Thetrainingdetails
withineachstagearepresentedontheright.
totheChineseinputs,whereasthemodelusingChinese-CLIPnotonlyunderstandsthequeriesbut
alsogeneratesappropriateoutputsinChinese. Moreover,tofurtherevaluatethemodel’sChinese
capability,wetestitonChinesedatasetsandobserveimprovedperformance: from66.4to68.3on
MMBench-CNandfrom62.4to66.1onMMMB-zhwhenutilizingChinese-CLIP.
4.3 TextualGuidancetoDriveVisualTokenAlignment
Due to the low-resource nature of non-English multimodal data (e.g., lack of large-scale, high-
qualityimage-textdata),weneedtousenearlythesameamountofdataasLLaVAtoenhancethe
model’smultilingualcapabilities. Furthermore,motivatedbytheseinterestingfindingsandaiming
toalignvisualtokenswithtextualembeddingsatthelanguagelevel,weproposePARROT,anovel
approachthatleveragestextualguidancetofacilitatethemultilingualalignmentofvisualfeatures.
PARROTenablesthetransitionofEnglish-biasedvisualfeaturesacquiredthroughtheOpenAI-CLIP
toaccommodateotherlanguages. Thisapproachensuresthatitcanprovidelanguage-specificvisual
tokenstoLLMbasedonthemultiplelanguageinputs,therebyenhancingitsmultilingualcapabilities.
First, we extract visual features through the vision encoder and transform them into language
embedding tokens H using a projector. We obtain the embeddings H ∈ RN×C derived from
v t
text inputs via the word embedding table. Subsequently, to convert the English-biased features
into language-specific features using textual guidance, we employ a cross-modal cross-attention
mechanismtoobtainH′ ∈RC:
v
(cid:18) HclsHT(cid:19)
H′ =Attention(Q,K,V)=Softmax √v t H , (2)
v t
C
whereQequalsthematrixH ,KandVareequivalenttoH . Hcls ∈RC isthe[CLS]tokenofH .
v t v v
Thisprocessallowsthevisualfeaturestobedynamicallyadjustedbasedonthemultilingualinputs.
SincetheprojectedlanguageembeddingtokensH areEnglish-biased,weneedtoconvertthem
v
tolanguage-specificembeddingsfordifferentlanguages. Tothisend,weintroducealightweight
Mix-of-Experts(MoE)module,whichincludesarouterandseverallanguagetransformationexperts.
TherouterofMoEisalinearlayerthatgeneratesaprobabilitydistributionoverthesetofexperts
E =[e ,e ,··· ,e ],effectivelypredictingtheprobabilityofselectingandactivatingeachexpert.
1 2 E
Each expert is an MLP designed to convert English-biased embeddings into language-specific
embeddings. TheinputstoexpertsE isH ,andtheoutputshavethesamedimensionsastheinputs.
v
Subsequently,toobtainanormalizedprobabilitydistributionforactivatinglanguage-specificexperts,
H′ is fed as input to the router. The router network contains a linear layer that computes the
v
normalizedweightmatrixusingH′ forvoting,producingP ∈RE:
v
P =Softmax(Linear(H′)), (3)
v
whichselectsandactivatesthespecificexperts. Moreover,weprocesstheEnglish-biasedembeddings
H throughtheselectedexpertstoconvertthemintolanguage-specificvisualrepresentations:
v
k
(cid:88)
MoE(H )= P[i]·E(H ) . (4)
v v i
i=1
6ThisapproacheffectivelyalignsEnglish-biasedembeddingswithmultiplelanguages,ensuringamore
accurateandcomprehensiverepresentationacrossdifferentlinguisticcontexts. Tostabilizetraining
andconvertvisualembeddingswithlessvarianceinvisual-semanticinformation,ensuringthemodel
performswellintasksbeyondthemultilingualmultimodaldomain,weutilizeMoEreweightingto
obtainthefinallanguage-specificvisualembeddingsG :
v
G =H +αMoE(x), (5)
v v
whereαisthetrade-offparameter. Inconclusion,wefirstfusethevisualandtextualinputsviaEq.2
totransformthevisualembeddingswithtextualguidance. Moreover,thefusedresultisinputtedinto
theMoEmoduletoselectandactivatethemostrelevantlanguageexpertsviaEq.3andthenobtain
thelanguage-specificembeddingsviaEq. 4. Lastly,weemployMoEreweightingtoconvertvisual
embeddingswithlessvarianceinoriginalvisual-semanticinformation5. Thisapproachenablesusto
endowtheMLLMwithmultilingualcapabilitiesusingaslittlemultilingualdataaspossible. Figure4
illustratesthearchitecture,thedetailedMoEmodule,andthetrainingstagesofPARROT.
4.4 TrainingStage
Our goal is to utilize as little multilin- Table1: Detailsonthe PARROT’strainingdata, derivedfrom
gualdataaspossibletoenhancethemul- publiclyavailabledatasetsandourin-housemultilingualdata.
tilingual capabilities of MLLMs. The
wholetrainingprocedureisdividedinto TrainingStage Datasets Samples Total
twodistinctstages:
LLaVA-1.5-pretrain[38] 558K
Stage1 Laion-Caption∗[55] 12K 1.2M
Stage1: ModalityAlignment. Inthis
CC12M-Caption∗[9] 645K
stage,wekeepboththevisionencoder
andtheLLMweightsfrozen,focusing LLaVA-1.5-finetune[38] 665K
solely on optimizing the projectors to ShareGPT4V-zh∗[12] 71K
align the visual features H v with the Stage2 S Sh ha ar re eG GP PT T4 4V V- -p art ∗∗ [[ 11 22 ]] 11 24 KK 793K
pre-trainedLLMwordembedding. This
ShareGPT4V-tr∗[12] 17K
stage can be likened to training a vi-
ShareGPT4V-ru∗[12] 14K
sual tokenizer that is compatible with
thefrozenLLM.Toenhancethediver-
sityofimages,weextractaportionofdatafromLAION[55]andCC12M[9]datasetsandconstruct
thein-housecaptiondatathroughGPT-4V.
Stage2: InstructionTuningforMultilingualAlignment. Westillkeepthevisionencoderweights
frozenwhilecontinuingtotraintheprojector,MoE,andLLM.DuetothedesignoftheMoEmodule,
PARROTcanrapidlylearntoalignvisualrepresentationsacrossmultiplelanguagesbyusingasmall
amount of multilingual image-text data. As shown in Table 1, we only use nearly 10K training
dataforeachlanguageinstage2. Thisapproachisparticularlybeneficialgiventhescarcityofdata
resourcesinlow-resourcelanguages.
Toaddressthechallengeoflimiteddatainnon-Englishlanguages,weuseasemi-automaticapproach
toacquireimage-textdata. Initially,wepartitiontheShareGPT4Vdataset[12]randomlyforeach
language,extractingaselectionofnon-duplicate,non-parallelimage-textdatafortraining. Subse-
quently,weimplementatranslationandcalibrationschemeusingGPT-4toconvertEnglishtextsinto
textsofotherlanguages. Recognizingthatthisstepmayintroducenoiseandpotentialtranslation
errors,weapplyamanualcalibrationprocesstofurtherfine-tuneandcleanthedata,therebyobtaining
high-qualitymultilingualimage-textdata.
Thistwo-stagetrainingapproachensureseffectivemodalityandmultilingualalignment,evenwith
limitedNon-Englishdata,aligningwellwiththerealitiesofdatascarcityinlow-resourcelanguages.
5 Experiments
Inthissection,webeginwithanoverviewoftheexperimentalframework,providingdetailsonspecific
implementations,evaluationbenchmarks,andMLLMsusedforcomparativeevaluation. Following
this, we conduct a comprehensive comparison of PARROT with the state-of-the-art approaches
usingmultilingualbenchmarks. Additionally,wecomparePARROTwithleadingmodelsacrossa
rangeofmultimodaltasks. Finally,thissectionconcludeswithablationstudiesandvisualizationof
multilingualcases,highlightingtheexceptionalabilityofPARROTinhandlingmultilingualtasks.
75.1 ExperimentalSetup
Implementation Details: In this study, we configure PARROT with the pre-trained CLIP ViT-
L/14[51]asthevisionencoderandQwen1.5-Chat[5]asthebackboneforLLM.Theinitiallearning
ratesforthetwostagesaresetat1e−3 and2e−5,respectively,withthebatchsizeof256and128.
Theentiretrainingprocessisnotablyoptimizedto21hoursonthe16×A100GPUssetup,attributed
totheuseoftherelativelysmalltrainingdatasets. Additionally,BF16andTF32precisionformatsare
employedtometiculouslybalancespeedandaccuracythroughoutthetrainingprocess. Asdefinedin
Eq.4,wesetthenumberofexpertstosixtomatchthenumberoflanguages. EachexpertisanMLP
composedoftwolinearlayerswithSiLU[19]activationfunction. MoredetailsareshowninTable5.
EvaluationBenchmark: Ourevaluationisdividedintotwoparts: oneevaluatesthemultilingual
capabilitiesofMLLMs,whiletheotherassessesitsoverallperformance. Thefirstevaluationisper-
formedontwodatasets: MMBench[39]andanewlydevelopedbenchmarkMMMB.ForMMBench,
weexpandittoincludesixlanguagesthroughtranslationviaGPT-4,followedbymanualverification.
For MMMB, we construct it following the methodology described in Section 3. We present the
accuracyforeachlanguageinTable2. Furthermore,thesecondevaluationcoversawidebroadrange
ofmultimodaltasks,suchasMME[22],MMStar[11],ScienceQA[41],RealWorldQA[61]and
SEED-Bench[28],withperformancereportedusingaradarchartinFigure5b.
Comparison Models: For comprehensive comparisons, we select leading open-source models
in MLLMs, including LLaVA-1.5 [29], LLaVA-NeXT [37], Qwen-VL [6], Monkey [35], Visu-
alGLM [18], VisCPM [24], MiniGPT-4-v2 [74], shareGPT4V [12], InstructBLIP [17], mPLUG-
Owl2[64],Mini-Gemini[34].Furthermore,weincorporateclosed-sourcemethodsinourbenchmarks,
includingGPT-4V[12],Qwen-VL-MAX[6],andGeminiPro[54],todemonstratetheirremarkable
performance.Fortheevaluationprocess,weemploytheVLMEvalKitinOpenCompass[16],ensuring
consistentconfigurationsettingsacrossallmethodstomaintainfairnessincomparison. Formostof
thementionedmethods,wedirectlyusetheVLMEvalKitimplementation. Alternatively,weintegrate
othermethodsnotofficiallyprovidedintothisframeworktoensureconsistencyinevaluation.
5.2 MainResults
In this section, we present the results of the multilingual experiment in Table 2 and the overall
experiment in Figure 5b. According to Table 2, PARROT-14B achieves state-of-the-art (SOTA)
performanceinalllanguagesontheMMBenchbenchmarkandalsoachievestheSOTAperformance
infivelanguagesontheMMMBbenchmark,withEnglishinthesecondplace. Themultilingual
capabilities of PARROT-14B closely reach that of GPT-4V, demonstrating the exceptional ability
ofourproposedarchitecture. Notably,PARROT-7BalsovalidatestheSOTAperformanceonboth
benchmarksacrossalllanguages,evensurpassingtheLLaVA-NeXT-13B.Additionally,asshown
inFigure5b,thisevaluationaimstoshowthatPARROTnotonlypossessesexcellentmultilingual
capabilitiesbutalsoprovidesanoverallunderstandingofPARROT’scapabilitiesinhandlingvarious
complexmultimodaltasks(e.g.,MME[22],MMStar[11],andSEED-Bench[28]). Additionally,
asdepictedinFigure5c,wevisualizetheexpertdistributionswithintheMoE.Itisevidentthatthe
secondexpertispredominantlyactivatedwhenusingtheChineseprompt,indicatingthatdifferent
expertsareutilizedforvariouslanguageprompts. Inexistingmultilingualbenchmarks, PARROT
alsodemonstratescompetitiveperformancewhileusinglessthan1%ofthedatacomparedtoother
multilingualMLLMs,asillustratedinFigure6.
5.3 AblationStudy
Ablationstudyoneachcomponent. Weconductanablationexperimentonthemultilingualdata
and the MoE module. As shown in Figure 5a, using multilingual data improves performance in
eachlanguage. Moreover,theMoEmodulesignificantlyimprovesperformance,demonstratingthe
effectivenessofourproposedmethod.
Ablationstudyondifferentdatasets.AsshowninTable3,itisevidentthattheinclusionofdifferent
multilingualdatasetscontinuallyimprovesperformanceontheMMBenchbenchmark,andallmodels
with7Bparametersareusedforthisexperiment. Thishighlightstherobustnessandscalabilityofour
approachtohandlingmultiplelanguageseffectively.
8Table2:Accuracyperformancecomparisononmultilingualbenchmarks.Wereportallcomparedmethodswith
VLMEvalKit[16].Thebestandsecondresultsareshowninboldandunderline,respectively.
MMMB MMBench
Method LLM
en zh pt ar tr ru en zh pt ar tr ru
Open-sourcemodels
LLaVA-1.5[36] Vicuna-7B 67.07 58.83 59.76 43.50 46.43 59.06 65.37 58.33 59.02 36.16 43.90 56.95
LLaVA-1.5[36] Vicuna-13B 69.76 62.86 60.76 45.49 54.44 62.69 68.98 63.23 62.97 46.56 53.17 61.59
LLaVA-NeXT[37] Vicuna-7B 70.87 61.57 61.81 42.74 46.95 63.85 67.95 60.56 60.39 38.40 45.36 59.62
LLaVA-NeXT[37] Vicuna-13B 74.44 67.19 63.21 45.36 53.09 68.24 70.87 64.51 64.08 45.36 52.92 61.85
Qwen-VL[6] Qwen-7B 52.63 36.37 38.65 36.54 37.42 40.70 42.26 22.25 25.08 18.72 26.37 28.17
Qwen-VL-Chat[6] Qwen-7B 56.02 57.77 46.37 43.04 41.05 48.65 54.29 56.52 43.12 35.73 39.17 42.86
MiniGPT-4-v2[74] LLaMA2-13B 38.71 30.05 31.52 26.60 26.02 29.23 23.88 11.76 14.26 2.49 6.78 12.54
ShareGPT4V[12] Vicuna-v1.5-7B 69.24 60.23 60.29 43.57 45.26 61.23 69.59 61.6 59.62 37.37 43.38 59.45
InstructBLIP[17] Vicuna-7B 39.47 32.92 35.67 23.80 28.36 36.37 27.83 18.81 27.14 3.26 8.50 20.87
mPLUG-Owl2[64] LLaMA2-7B 67.25 60.99 59.70 45.78 45.43 62.63 66.15 59.36 58.24 37.88 47.68 60.39
Monkey[35] Qwen-VL-7B 66.02 58.18 46.31 38.83 37.66 48.59 58.07 53.52 49.57 31.01 31.35 45.18
Monkey-chat[35] Qwen-VL-7B 71.63 66.54 60.35 48.77 46.31 58.59 70.79 65.72 65.03 46.90 48.10 59.36
VisualGLM[18] ChatGLM-6B 31.05 18.07 19.42 15.38 22.81 19.77 23.2 17.18 11.43 2.92 6.62 5.33
VisCPM-Chat[24] CPM-Bee-10B 53.10 47.54 28.19 26.90 26.78 26.84 45.88 46.39 15.81 1.46 9.19 1.20
PARROT Qwen1.5-7B 70.00 68.13 67.31 62.69 58.01 66.26 70.70 70.36 65.12 57.82 58.43 64.00
PARROT Qwen1.5-14B 73.92 71.64 69.82 68.13 64.33 70.18 74.40 72.25 69.16 66.15 64.52 69.33
Closed-sourcemodels
GPT-4V[46] Private 74.97 74.21 71.46 73.51 68.95 73.10 77.60 74.40 72.51 72.34 70.53 74.83
GeminiPro[58] Private 75.03 71.87 70.64 69.94 69.59 72.69 73.63 72.08 70.27 61.08 69.76 70.45
Qwen-VL-MAX[6] Private 77.19 75.26 72.16 70.82 66.02 74.21 76.80 77.58 74.57 75.00 69.07 75.00
MMBench-CN
70 Baseline
w/ Multilingual Data
w/ Multilingual Data and MoE RealWorldQA MME
Layer 2
65
60
Layer 1
55 SQA-IMG SEED-IMG
50 en zh pt ar tr ru MMStar 0% 25% 50% 75% 100%
Mini-Gemini7B LLaVA-NeXT7B Monkey9.8B Expert 1 Expert 2 Expert 3
Languages Qwen-VL-Chat7B mPLUG-Owl2 7B Parrot7B(Ours) Expert 4 Expert 5 Expert 6
(a)Ablationstudy. (b)Multiplemultimodaltasks. (c)Expertdistributions.
Figure5:Left:TheablationstudyofmultilingualdataandtheMoEmoduleusingtheMMBenchbenchmark.
Middle:TheperformanceofPARROTonabroadrangeofmultimodaltaskscomparedwithexistingmodels.
Modelswith7Bparametersareusedforthetwoexperiments.Right:ExpertdistributionsofMoE.Wesummarize
theactivatedexpertsduringthefeed-forwardprocessusingChinesePrompts.
Ablation study on monolingual fine-tuning datasets. The ablation study presented in Table 9
evaluates the performance of different monolingual datasets added incrementally to the baseline
dataset LLaVA-1.5-finetune. It highlights the significant impact of adding different multilingual
datasets to a baseline model. Each dataset incrementally improves performance in its respective
languageand,whencombined,leadstooverallenhancedperformanceacrossallevaluatedlanguages.
Thisindicatestherobustnessandeffectivenessoftheproposedmethodinhandlingmultilingualdata,
makingitascalablesolutionformultilingualtasks.
5.4 VisualizationofMultilingualConversations
To enhance the intuitive understanding of the PARROT’s multilingual capability, we prepare a
comprehensivecasestudyaccompaniedbyillustrativevisuals. Forinstance,asdepictedinFigure6,
ourframeworkdemonstratesremarkablemultilingualcapabilities. ThisunderscoresthePARROT’s
versatility in navigating different languages and presents its potential in bridging linguistic gaps
across diverse domains. Through careful analysis and visualization, we aim to provide a deeper
insightintothemechanismdrivingthiscapability,illustratingitspracticalimplicationsandpotential
applicationsinreal-worldscenarios. ThisvisualizationservesasastrongindicatorofthePARROT’s
solidarchitectureanditsexceptionalabilitytounderstand,process,andgeneratemultiplelanguages
withremarkableefficiency. MoremultilingualconversationcasesareshowninAppendixE.
9
)%(
ycaruccATable3:AblationstudyondifferentmultilingualtrainingdatasetsinMMBenchbenchmark.Modelswith7B
parametersareusedforthisablation.
Dataset English Chinese Portuguese Arabic Turkish Russian
LLaVA-1.5-finetune 69.4 66.6 60.3 55.3 52.1 60.7
+zh 69.2 -0.2 68.6 +2.0 64.1 +3.8 59.1 +3.8 50.9 -1.2 61.6 +0.9
+zhpt 71.1 +1.7 70.4 +3.8 65.4 +5.1 57.9 +2.6 52.1 +0.0 62.9 +2.2
+zhptar 71.0 +1.6 68.6 +2.0 65.7 +5.4 58.6 +3.3 52.2 +0.1 62.2 +1.5
+zhptartr 70.4 +1.0 68.7 +2.1 64.9 +4.6 61.2 +5.9 59.7 +7.6 62.0 +1.3
+zhptartrru 70.7 +1.3 70.4 +3.8 65.1 +4.8 57.8 +2.5 58.4 +6.3 64.0 +3.3
Arabic:؟ةرﻮﺼﻟا ﻲﻓ ﺺﺨﺸﻟا ﺎﮭﺳرﺎﻤﯾ ﻲﺘﻟا ﺔﺿﺎﯾﺮﻟا ﻲھ ﺎﻣ
[Arabic-EnglishTrans:What sport is the person in the picture doing?]
Output: .ﺲﻨﺘﻟا ﺐﻌﻠﯾ ةرﻮﺼﻟا ﻲﻓ ﺺﺨﺸﻟا
[Arabic-EnglishTrans:The person in the picture is playing tennis.]
Turkish:Sporcu hangi etkinliği kazandı ve görsele göre kaç Grand Slam şampiyonluğu kazandı?
[Turkish-EnglishTrans:What event did the athlete win and how many Grand Slam titles has the athlete won according to the image?]
Output:Sporcu, 2023 ABD Açık'ı kazandı ve görsele göre 24 Grand Slam şampiyonluğu kazandı.
[Turkish-EnglishTrans:The athlete won the US Open 2023, and according to the image, he has won 24 Grand Slam titles.]
Portuguese:Descreva as emoções ou expressões capturadas no rosto do atleta na imagem. O que isso sugere sobre os sentimentos dele em relação à sua vitória?
[Portuguese-EnglishTrans:Describe the emotions or expressions captured in the athlete's face in the image. What does this suggest about their feelings regarding their victory?]
Output:O jogador de tênis, que está em pé e segurando uma raquete de tênis, parece estar expressando felicidade e orgulho em relação à sua vitória. Ele está sorrindo, o que
sugere que ele está animado e feliz com o resultado de seu esforço. A imagem captura um momento de realização e conquista, com o jogador no centro do quadro, destacado
por uma cor vibrante e uma fonte proeminente que diz "Campeão". No geral, a imagem transmite uma sensação de orgulho, realização e realização.
[Portuguese-EnglishTrans:The tennis player, standing and holding a tennis racket, appears to be expressing happiness and pride about his victory. He is smiling, which
suggests he is excited and pleased with the outcome of his effort. The image captures a moment of achievement and conquest, with the player at the center of the frame,
highlighted by a vibrant color and a prominent font that says "Champion". Overall, the image conveys a sense of pride, accomplishment, and fulfillment.]
English:How are the overall colors and composition of the image?
Output:The image features a painting of a deer and a swan standing near a river. The deer is positioned on the right side of the
painting, while the swan is on the left side. The river is surrounded by lush greenery, with many different types of flowers
scattered throughout the scene. The painting has a vibrant and colorful composition, with various shades of green, blue, and
white. The painting is done in a watercolor style, which adds to its artistic and artistic qualities.
Chinese:图中有哪些动物？
[Chinese-EnglishTrans:What animals are present in the image?]
Output:这张图片中有两种动物：一只白天鹅在水中游泳，以及一只鹿站在岸边。
[Chinese-EnglishTrans:The picture features two animals: a swan swimming in the water and a deer standing on the shore.]
Russian:Погода на изображении -это солнечный день.
[Russian-EnglishTrans:What is the weather shown in this picture?]
Output:Погода на этой картинке выглядит очень солнечной. Небо чистое, с голубым небом и белыми облаками, ярким
солнечным светом, окружающий пейзаж яркий и полон жизни. Вся обстановка кажется очень спокойной и красивой,
подходит для активного отдыха на свежем воздухе.
[Russian-EnglishTrans:The weather in this picture looks very sunny. The sky is clear, with blue sky and white clouds, bright
sunlight, and the surrounding scenery is bright and full of life. The overall environment appears very peaceful and beautiful,
suitable for outdoor activities.]
Figure6:MultimodalconversationcasesofPARROTinmultiplelanguages.
6 Conclusion
ThispaperaddressesthecriticalchallengeofenhancingthemultilingualcapabilitiesofMLLMs. We
introducePARROT,anovelmethodleveragingtextualguidancetodrivevisualtokenalignmentat
thelanguagelevel,thusenablingthetransitionofEnglish-biasedvisualembeddingsintolanguage-
specificonesusinganMoEmodule.ExtensiveexperimentsconductedonanewlyintroducedMassive
MultilingualMultimodalBenchmark(MMMB)acrosssixlanguagesdemonstratethestate-of-the-art
performanceofPARROTcomparedtoexistingmethods,particularlypresentingremarkableimprove-
mentsinTurkishandArabic. Furthermore, ourmodelexhibitscompetitiveresultsacrossawide
rangeofdiversemultimodalbenchmarks,emphasizingitsefficacyinaddressingbothmultilingual
andmultimodalchallenges. PARROTnotonlyadvancesthefrontierofMLLMsbutalsounderscores
theimportanceofequitableaccesstotechnologicalbenefitsacrosslinguisticandculturaldiversities.
10References
[1] MetaAI. Buildthefutureofaiwithmetallama3. Technicalreport,MetaAI,2024.
[2] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, AntoineMiech, IainBarr, YanaHasson,
KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisual
languagemodelforfew-shotlearning. NeurIPS,2022.
[3] anthropic. Introducingthenextgenerationofclaude. Technicalreport,anthropic,2024.
[4] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,WanrongZhu,Kalyani
Marathe,YonatanBitton,SamirGadre,ShioriSagawa,etal. Openflamingo: Anopen-source
frameworkfortraininglargeautoregressivevision-languagemodels. arXiv:2308.01390,2023.
[5] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,
YuHan,FeiHuang,etal. Qwentechnicalreport. arXiv:2309.16609,2023.
[6] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile
abilities. arXiv:2308.12966,2023.
[7] RohanBavishi,ErichElsen,CurtisHawthorne,MaxwellNye,AugustusOdena,ArushiSomani,
andSag˘nakTas¸ırlar. Introducingourmultimodalmodels,2023.
[8] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. NeurIPS,2020.
[9] SoravitChangpinyo,PiyushSharma,NanDing,andRaduSoricut. Conceptual12m: Pushing
web-scaleimage-textpre-trainingtorecognizelong-tailvisualconcepts. InCVPR,2021.
[10] GuanhuaChen,LuHou,YunChen,WenliangDai,LifengShang,XinJiang,QunLiu,JiaPan,
andWenpingWang. mclip: Multilingualclipviacross-lingualtransfer. InACL,2023.
[11] LinChen,JinsongLi,XiaoyiDong,PanZhang,YuhangZang,ZehuiChen,HaodongDuan,Jiaqi
Wang,YuQiao,DahuaLin,etal. Areweontherightwayforevaluatinglargevision-language
models? arXivpreprintarXiv:2403.20330,2024.
[12] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahua
Lin. Sharegpt4v: Improvinglargemulti-modalmodelswithbettercaptions. arXiv:2311.12793,
2023.
[13] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz,
SebastianGoodman,AdamGrycner,BasilMustafa,LucasBeyer,etal. Pali: Ajointly-scaled
multilinguallanguage-imagemodel. arXivpreprintarXiv:2209.06794,2022.
[14] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,ZhongMuyan,Qinglong
Zhang,XizhouZhu,LeweiLu,etal. Internvl:Scalingupvisionfoundationmodelsandaligning
forgenericvisual-linguistictasks. arXiv:2312.14238,2023.
[15] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephEGonzalez,etal. Vicuna: Anopen-sourcechatbot
impressinggpt-4with90%chatgptquality,2023.
[16] OpenCompassContributors. Opencompass: Auniversalevaluationplatformforfoundation
models. https://github.com/open-compass/opencompass,2023.
[17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose
vision-languagemodelswithinstructiontuning. arXiv:2305.06500,2023.
[18] ZhengxiaoDu, YujieQian, XiaoLiu, MingDing, JiezhongQiu, ZhilinYang, andJieTang.
Glm: Generallanguagemodelpretrainingwithautoregressiveblankinfilling. InProceedingsof
the60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long
Papers),pages320–335,2022.
11[19] StefanElfwing,EijiUchibe,andKenjiDoya. Sigmoid-weightedlinearunitsforneuralnetwork
functionapproximationinreinforcementlearning. Neuralnetworks,107:3–11,2018.
[20] Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. Multi30k: Multilingual
english-germanimagedescriptions. arXivpreprintarXiv:1605.00459,2016.
[21] YuxinFang,QuanSun,XinggangWang,TiejunHuang,XinlongWang,andYueCao. Eva-02:
Avisualrepresentationforneongenesis. arXiv:2303.11331,2023.
[22] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,
Wei Lin, Zhenyu Qiu, Wei Lin, et al. Mme: A comprehensive evaluation benchmark for
multimodallargelanguagemodels. arXiv:2306.13394,2023.
[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.
arXiv:2106.09685,2021.
[24] JinyiHu,YuanYao,ChongyiWang,ShanWang,YinxuPan,QianyuChen,TianyuYu,Hanghao
Wu, Yue Zhao, Haoye Zhang, et al. Large multilingual models pivot zero-shot multimodal
learningacrosslanguages. arXiv:2308.12038,2023.
[25] RobertAJacobs,MichaelIJordan,StevenJNowlan,andGeoffreyEHinton.Adaptivemixtures
oflocalexperts. Neuralcomputation,3(1):79–87,1991.
[26] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,etal. Mistral7b. arXiv:2310.06825,2023.
[27] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu. Otter: A
multi-modalmodelwithin-contextinstructiontuning. arXiv:2305.03726,2023.
[28] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench:
Benchmarkingmultimodalllmswithgenerativecomprehension. InCVPR,2024.
[29] ChunyuanLi,CliffWong,ShengZhang,NaotoUsuyama,HaotianLiu,JianweiYang,Tristan
Naumann,HoifungPoon,andJianfengGao. Llava-med: Trainingalargelanguage-and-vision
assistantforbiomedicineinoneday. arXiv:2306.00890,2023.
[30] JiachenLi,XinyaoWang,SijieZhu,Chia-WenKuo,LuXu,FanChen,JiteshJain,Humphrey
Shi,andLongyinWen. Cumo: Scalingmultimodalllmwithco-upcycledmixture-of-experts.
arXivpreprintarXiv:2405.05949,2024.
[31] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. arXiv:2301.12597,2023.
[32] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip: Bootstrappinglanguage-image
pre-trainingforunifiedvision-languageunderstandingandgeneration. InICML,2022.
[33] LeiLi,YuweiYin,ShichengLi,LiangChen,PeiyiWang,ShuhuaiRen,MukaiLi,Yazheng
Yang,JingjingXu,XuSun,LingpengKong,andQiLiu. M3it: Alarge-scaledatasettowards
multi-modalmultilingualinstructiontuning. arXiv:2306.04387,2023.
[34] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu,
Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision
languagemodels. arXiv:2403.18814,2024.
[35] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,JingxuYang,YaboSun,Yuliang
Liu,andXiangBai. Monkey: Imageresolutionandtextlabelareimportantthingsforlarge
multi-modalmodels. arXiv:2311.06607,2023.
[36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning. arXiv:2310.03744,2023.
[37] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.
Llava-next: Improvedreasoning,ocr,andworldknowledge,January2024.
12[38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.
arXiv:2304.08485,2023.
[39] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelanall-around
player? arXiv:2307.06281,2023.
[40] JunyuLu,DixiangZhang,XiaojunWu,XinyuGao,RuyiGan,JiaxingZhang,YanSong,and
PingjianZhang. Ziya-visual: Bilinguallargevision-languagemodelviamulti-taskinstruction
tuning. arXivpreprintarXiv:2310.08166.,2023.
[41] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,Oyvind
Tafjord,PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathought
chainsforsciencequestionanswering. NeurIPS,2022.
[42] ShiyinLu,YangLi,Qing-GuoChen,ZhaoXu,WeihuaLuo,KaifuZhang,andHan-JiaYe.
Ovis: Structuralembeddingalignmentformultimodallargelanguagemodel. arXivpreprint
arXiv:2405.20797,2024.
[43] BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,Philipp
Dufter,DhrutiShah,XianzhiDu,FutangPeng,FlorisWeers,etal. Mm1: Methods,analysis&
insightsfrommultimodalllmpre-training. arXiv:2403.09611,2024.
[44] MinhengNi,HaoyangHuang,LinSu,EdwardCui,TaroonBharti,LijuanWang,Dongdong
Zhang, andNanDuan. M3p: Learninguniversalrepresentationsviamultitaskmultilingual
multimodalpre-training. InCVPR,2021.
[45] OpenAI. Chatgpt: Alanguagemodelforconversationalai. Technicalreport,OpenAI,2023.
[46] OpenAI. Gpt-4technicalreport. arXiv:2303.08774,2023.
[47] OpenAI. Gpt-4o: Hellogpt-4o. Technicalreport,OpenAI,2024.
[48] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal.Dinov2:Learning
robustvisualfeatureswithoutsupervision. arXiv:2304.07193,2023.
[49] TelmoPessoaPires,RobinMSchmidt,Yi-HsiuLiao,andStephanPeitz. Learninglanguage-
specificlayersformultilingualmachinetranslation. arXivpreprintarXiv:2305.02665,2023.
[50] TaidoPurasonandAndreTättar. Multilingualneuralmachinetranslationwiththerightamount
ofsharing. InProceedingsofthe23rdAnnualConferenceoftheEuropeanAssociationfor
MachineTranslation,pages91–100,2022.
[51] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InICML,2021.
[52] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguage
understandingbygenerativepre-training,2018.
[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[54] MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-
baptisteAlayrac, RaduSoricut, AngelikiLazaridou, OrhanFirat, JulianSchrittwieser, etal.
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
arXiv:2403.05530,2024.
[55] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,
MehdiCherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-
5b: An open large-scale dataset for training next generation image-text models. NeurIPS,
2022.
13[56] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,
andJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts
layer. arXivpreprintarXiv:1701.06538,2017.
[57] QuanSun,YuxinFang,LedellWu,XinlongWang,andYueCao. Eva-clip: Improvedtraining
techniquesforclipatscale. arXiv:2303.15389,2023.
[58] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[59] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXiv:2302.13971,2023.
[60] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXiv:2307.09288,2023.
[61] x.ai. Realworldqadataset. https://x.ai/blog/grok-1.5v,2024.
[62] An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, and Chang
Zhou. Chinese clip: Contrastive vision-language pretraining in chinese. arXiv preprint
arXiv:2211.01335,2022.
[63] Han-Jia Ye, Yi Shi, and De-Chuan Zhan. Identifying ambiguous similarity conditions via
semanticmatching. InCVPR,pages16610–16619,2022.
[64] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,
Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large
languagemodelswithmultimodality. arXiv:2304.14178,2023.
[65] ChaoYi,De-ChuanZhan,andHan-JiaYe. Bridgethemodalityandcapacitygapsinvision-
languagemodelselection. arXivpreprintarXiv:2403.13797,2024.
[66] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,
Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai.
arXiv:2403.04652,2024.
[67] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for
languageimagepre-training. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages11975–11986,2023.
[68] BiaoZhang,AnkurBapna,RicoSennrich,andOrhanFirat. Shareornot? learningtoschedule
language-specificcapacityformultilingualtranslation. InICLR,2021.
[69] HaoZhang,FengLi,ShilongLiu,LeiZhang,HangSu,JunZhu,LionelMNi,andHeung-
YeungShum. Dino: Detrwithimproveddenoisinganchorboxesforend-to-endobjectdetection.
arXiv:2203.03605,2022.
[70] WenxuanZhang,MahaniAljunied,ChangGao,YewKenChia,andLidongBing. M3exam: A
multilingual,multimodal,multilevelbenchmarkforexamininglargelanguagemodels.Advances
inNeuralInformationProcessingSystems,36,2024.
[71] Yi-KaiZhang,Ting-JiHuang,Yao-XiangDing,De-ChuanZhan,andHan-JiaYe. Modelspider:
Learningtorankpre-trainedmodelsefficiently. NeurIPS,36,2024.
[72] XinyuZhao,XuxiChen,YuCheng,andTianlongChen. Sparsemoewithlanguageguided
routingformultilingualmachinetranslation. InICLR,2024.
[73] MingyangZhou,LuoweiZhou,ShuohangWang,YuCheng,LinjieLi,ZhouYu,andJingjing
Liu. Uc2: Universalcross-lingualcross-modalvision-and-languagepre-training. InCVPR,
2021.
[74] DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. Minigpt-4: Enhanc-
ingvision-languageunderstandingwithadvancedlargelanguagemodels. arXiv:2304.10592,
2023.
14A TrainingDatasetsinLLaVA
Inthissection,weanalyzethemultilingualdatainLLaVA[38]. FromTable4andFigure7,itis
evidentthatduringthepre-trainstage,LLaVAsolelyutilizesmultimodalimage-textpairsdatafor
training,comprising558KofEnglishdata. DuringtheSFTstage,bothmultimodalandtext-onlydata
areincorporatedintothetrainingprocess.Multilingualdataappearonlyinthetext-onlydataset.Apart
fromEnglish,themostprominentnon-EnglishdataisChinese,amountingtojust3.1K,constituting
0.25%ofthetotaldataset. Therefore, itisevidentthatLLaVA’sdatasetsareEnglish-centricand
imbalanced. The specific language and abbreviation are as follows: English (en), Chinese (zh),
Korean (ko), Spanish (es), French (fr), Japanese (ja), German (de), Portuguese (pt), Traditional
Chinese(zh-tw),Italian(it).
Table4:ThedetailedinformationaboutLLaVA’sdatasets.
(a)Thedetailedinformationintwostages.
TrainingStage Type TotalSize English OtherLanguages
Multimodal 558K 558K -
Stage1(Pre-train)
Text-only - - -
Multimodal 624K 558K -
Stage2(SFT)
Text-only 41K 31K 10K
(b)Thetop-10multilingualinformation
Language en zh ko es fr ja de pt zh-tw it
Size 31K 3192 1219 1123 1049 551 435 422 305 234
TotalSize
en zh ko es fr ja de pt zh-tw it
Figure7:ThepiechartofLLaVA’smultilingualdata.
B TrainingDetails
AsshowninTable5,weprovidethetraininghyperparametersforPARROT. Throughoutallstages
oftraining,weconsistentlytrainforoneepoch,withabatchsizeof256forthefirststageand128
15Table5:Traininghyperparameters.
Config Stage1 Stage2
Experts - 6
MLPexpertnetwork 2LinearlayerswithSiLU
Deepspeed Zero2 Zero3
Imageresolution 336×336
Imageencoder Clip-ViT-L/14-336
Featureselectlayer -2
Imageprojector 2LinearlayerswithGeLU
Epoch 1
Optimizer AdamW
Learningrate 1e-3 2e-5
Learningratescheduler Cosine
Weightdecay 0.0
Textmaxlength 2048
BatchsizeperGPU 16 8
GPU 16×A100-80G
Precision Bf16
Gradientcheckpoint True
forthesecondstage. Wemaintainanimageresolutionof336x336foralltwostagesandenablethe
gradientcheckpointmodeforeachtrainingstage.
C ExtendedExperiments
Inthissection,wefurtherprovidemoreexperimentsandablationstudiestovalidatethegenerality
andcapabilityofPARROTacrossvarioustasks. Additionally,wepresentmoretrainingdetailsabout
Figure1toofferaclearerunderstandingforreaders.
C.1 BilingualEvaluationonLLaVA-Bench
VisCPM[24]extendstheLLaVA-BenchdatasettotheChineseversionforbilingualevaluation. To
comprehensively compare PARROT with other multilingual models, we conduct experiments on
thisbenchmark. DuetothedeprecationoftheGPT-4-0314versionbyOpenAI,wetestPARROTin
LLaVA-BenchfollowingtheversionofGPT-4-1106-previewforcomparison. AsshowninTable6,
PARROTnotonlydemonstratesexceptionalabilityintheEnglishversionofthisbenchmarkbutalso
presentscompetitiveperformanceintheChineseversion.
Notably,asshowninTable7,VisCPMuses140MEnglishdataand1MChinesedatatotrainthe
model,whileQwen-VL-Chatuses1.1BEnglishdataand300MChinesedata,whereasPARROTonly
utilizesapproximately2Mdataintotal. Despiteusinglessthan1%ofthetrainingdata, PARROT
achievesremarkableperformanceinboththeEnglishandChineseversionsonLLaVA-Bench. Owing
tothearchitectureweproposed,significantimprovementinthemodel’smultilingualcapabilitycan
beachievedwithminimaldatausage.
C.2 RadarChartsonMMBenchandMMMB
ForamoreintuitivedemonstrationofthemultilingualcapabilitiesofPARROT,wepresentradarcharts
forthemultilingualMMBenchandMMMBbenchmarks. AsdepictedinFigure9aandFigure9b,
ourproposedmethodPARROTexhibitssignificantlybetterperformancecomparedtoothermodels.
C.3 MoreExperimentalDetailsaboutDifferentBackbones
Inthissection,weprovidedetailedinformationtoexplainFigure1.Firstly,toensureafaircomparison
betweentheOpenAI-CLIP-basedmodelandtheChinese-CLIP-basedmodel,wetraindistinctmodels
usingthesametrainingdataasLLaVA,asshowninTable4a. Thehyperparametersarelistedin
Table5withouttheMoEhyperparameters. AsdepictedinFigure1,theOpenAI-CLIP-basedmodel
strugglestogenerateChineseoutputswhengivenChinesepromptsduetotheEnglish-centrictraining
16Table 6: Experimental results on LLaVA Test Set accessed by GPT-4. Con: Conversation, DD: Detailed
Description,CR:ComplexReasoning,AVG:theaveragescoreofthreetasks. Thebest/secondbestresults
aremarkedinboldandunderlined,respectively.Thesymbol∗denotesthatthedataarejudgedfollowingthe
versionofGPT-4-1106-previewbecausetheGPT-4-0314versionisdeprecatedbyOpenAI.
LLM English Chinese
Model
Backbone
Con DD CR AVG Con DD CR AVG
MiniGPT-4 Vicuna-13B 65.0 67.3 76.6 69.7 - - - -
English
InstructBLIP Vicuna-13B 81.9 68.0 91.2 80.5 - - - -
Model
LLaVA Vicuna-13B 89.5 70.4 96.2 85.6 - - - -
mPLUG-OWL BLOOMZ-7B 64.6 47.7 80.1 64.2 76.3 61.2 77.8 72.0
En-Zh
VisualGLM ChatGLM-6B 62.4 63.0 80.6 68.7 76.6 87.8 83.6 82.7
Bilingual
Qwen-VL-Chat Qwen-7B 82.4 76.9 91.9 83.8 82.3 93.4 89.5 88.2
Model
VisCPM-Balance CPM-Bee-10B 75.5 64.7 91.3 77.3 85.4 81.4 96.6 88.0
Multilingual
PARROT∗ Qwen1.5-7B 82.5 71.0 89.3 81.1 82.1 88.6 92.3 87.7
Model
Table7:Comparisonofvisionencoders,LLMs,andtrainingdata.
Model visionencoder LLM TrainingData
mPLUG-Owl ViT-L/14(0.3B) BLOOMZ-7B -
VisualGLM Q-Former(1.6B) ChatGLM-6B English:300M;Chinese30M
Qwen-VL-Chat ViT-bigG(1.9B) Qwen-7B English:1.1B;Chinese:300M
VisCPM Muffin(0.7B) CPM-Bee-10B English:140M;Chinese:1M
PARROT ViT-L/14(0.3B) Qwen1.5-Chat-7B English:1.8M;Chinese:71K
data. Incontrast,despitetheextremelyscarceamountofChinesetrainingdata,theChinese-CLIP-
basedmodelnaturallyacquireszero-shotcapabilitytounderstand,process,andgenerateChinese
texts. Furthermore,wecomparebothmodelsonMMBench-CNandMMMB-zhtoevaluatetheir
Chinese capability. As shown in Table 8, the performance of the Chinese-CLIP-based model is
significantlyhigherthanthatoftheOpenAI-CLIP-basedmodel. Ontheotherhand,weempirically
findthatdifferentLLMshaveasignificantimpactonperformance. Qwen[5]demonstratessuperior
ChinesecapabilitycomparedtoVicuna[15],yetitsEnglishcapabilityremainscompetitive.
D BroaderImpactandLimitations
BroaderImpact. PARROTleveragingMoEtoenhancemultilingualalignmentpresentsapositive
social impact by promoting linguistic diversity and inclusivity. To address the challenge of the
No
Is this an apple?
Yes
Yes
Is this a pear?
The answer is wrong.
No
The answer is correct.
Figure8:Anexampleofcircularevaluationstrategy.
17Table8:TheperformanceofdifferentvisionencodersandLLMsonMMBenchandMMMB.MMBrefersto
MMBench.“En/en”representstheEnglishversion,and“CN/zh”representstheChineseversion.
Method Visionencoder LLM MMB-EN MMB-CN MMMB-en MMMB-zh
LLaVA OpenAI-CLIPViT-L/14 Vicuna7B 65.4 58.3 67.1 58.8
LLaVA OpenAI-CLIPViT-L/14 Qwen1.5-Chat7B 68.8 66.4 68.2 62.4
LLaVA Chinese-CLIPViT-L/14 Qwen1.5-Chat7B 68.1 68.3 67.6 66.1
PARROT OpenAI-CLIPViT-L/14 Qwen1.5-Chat7B 70.7 70.4 70.0 68.1
imbalancedlanguagedatainSFTdatasetsandimprovenon-Englishvisualtokensalignment,this
approachcontributestobreakinglanguagebarriersandfacilitatingcross-culturalcommunication,
therebyfosteringunderstandingandcollaborationacrossdiverselinguisticcommunities. Addition-
ally,thecreationoftheMassiveMultilingualMultimodalBenchmark(MMMB)fillsacrucialgap
in evaluating multilingual capabilities, enabling researchers to assess and improve upon models’
performanceacrossdifferentlanguagesandcultures. However,it’scrucialtoacknowledgepotential
negativesocialimpacts,suchastheriskofhallucination. Thiscouldpotentiallyresultinthemodel
generatingmisleadingorincorrectinformation,whichisacommonchallengeobservedinMLLMs.
Limitations.Despiteadvancements,MLLMsmaystillexhibitlimitationsinaccuratelyunderstanding
andrespondingtocomplexlanguage-specificcontexts,leadingtomisinformationormisinterpretation
ofmultilingualinputs. Ontheotherhand,duetothevisualcomponentofPARROTbeingbasedon
CLIP,thereareinherentlimitationsinitsabilitytoprocesshigh-resolutionimages,resultinginthe
inabilitytorecognizeextremelydetailedcontentinsomeimages. Hence,enhancingPARROT’sability
tohandlehigh-resolutionprocessingwillbepartoffuturework.
E MoreVisualizationResults
Inthissection,weincludeadditionalvisualizationresultsbetweenusers’questionsandPARROT’s
responsesusingmultiplelanguages. ThesepicturesareselectedfromLLaVA[38]andCuMo[30].
AsdepictedinFiguresFigures10to15,itisevidentthatPARROTpossessessuperiormultilingual
capabilities for understanding, processing, and generating multilingual texts. In certain specific
cases,PARROTmayalsoexperiencehallucinations. AsdepictedintheuppercaseofFigure10,it
misidentifiesXiaomiSU7asaPorscheTaycan.
18English Monkey-Chat 9.8B English Monkey-Chat 9.8B
72.0 Qwen-VL-Chat 7B 72.0 Qwen-VL-Chat 7B
LLaVA-NeXT 7B LLaVA-NeXT 7B
mPLUG-Owl2 7B mPLUG-Owl2 7B
58.0 Parrot 7B (Ours) 58.0 Parrot 7B (Ours)
Russian Chinese Russian Chinese
72.0 44.0 72.0 72.0 44.0 72.0
58.0 58.0 58.0 58.0
44.0 30.0 44.0 44.0 30.0 44.0
30.0 30.0 30.0 30.0
30.0 30.0 30.0 30.0
44.0 30.0 44.0 44.0 30.0 44.0
58.0 58.0 58.0 58.0
72.0 44.0 72.0 72.0 44.0 72.0
Turkish Portuguese Turkish Portuguese
58.0 58.0
72.0 72.0
Arabic Arabic
(a)MMBench. (b)MMMB.
Figure9:TheradarchartsforthemultilingualMMBenchandMMMBbenchmarks.
Table9: Ablationstudyonmonolingualfine-tuningdatasetinMMMBbenchmark. Thetableshowsan
effectofperformanceonsixlanguageswhenusingfine-tuningdatafromdifferentlanguages.Modelswith7B
parametersareusedforthisablation.
Dataset English Chinese Portuguese Arabic Turkish Russian
LLaVA-1.5-finetune 72.69 67.60 65.61 57.72 48.30 63.80
+zh71k 69.18 69.06 63.92 58.13 48.95 63.63
+pt14k 69.94 68.83 65.67 58.65 51.11 63.04
+ar12k 70.47 68.36 64.39 60.79 51.11 63.16
+tr17k 70.82 69.01 64.85 60.76 60.70 64.39
+ru14k 69.59 68.07 64.27 60.35 53.92 64.15
+zhptartrru 70.00 68.13 67.31 62.69 58.01 66.26
19English:What car is it in this image?
Output:The turquoise blue car isPorsche Taycan.
English:What animals are in this image?
Output:A cat and a dog are in the image.
Figure10:MorevisualizationresultsbetweentheuserandPARROTusingEnglishprompts.Wehighlightthe
hallucinationsfromtheresponsesofPARROT.
Chinese:你知道这幅画是谁画的吗?
[Chinese-EnglishTrans:Doyouknowwhodrewthispainting]
Output:莱昂纳多·达芬奇
[Chinese-EnglishTrans:Leonardo da Vinci]
Chinese:请描述这幅画的主要内容
[Chinese-EnglishTrans:Please describe the main content of this painting.]
Output:这幅画是一幅肖像画，描绘了一个年轻女子的头部和肩膀。她穿着一件黑色连衣裙，上
面有金色的装饰。她的头发是深棕色的，被梳成一个发髻。她的脸上有褐色的斑点，眼睛是棕
色的。她正在微笑，表明她心情愉快。这幅画的风格是肖像画，它以描绘人物的头部和肩膀为
特征。肖像画通常使用逼真的技巧，以捕捉人物的特征和表情。这幅画中人物的服装和环境暗
示着它可能描绘的是文艺复兴时期的艺术家。
[Chinese-EnglishTrans:This painting is a portrait, depicting the head and shoulders of a young woman.
She is dressed in a black dress with gold embellishments. Her hair is dark brown and styled into a bun.
There are brown spots on her face, and her eyes are brown. She is smiling, indicating she is in a good
mood. The style of the painting is portraiture, characterized by the depiction of the subject's head and
shoulders. Portraits often use realistic techniques to capture the features and expressions of the person.
The attire and setting of the figure in this painting suggest it may be depicting an artist from the
Renaissance period.]
Figure11:MorevisualizationresultsbetweentheuserandPARROTusingChineseprompts.
20Portuguese:Qual é o final deste filme?
[Portuguese-EnglishTrans:What's the ending of this movie?]
Output: O filme "Titanic" é um romance histórico de 1997 que apresenta uma narrativa fictícia de uma
mulher chamada Rose e um homem chamado Jack. A narrativa se passa durante o final do século XIX,
quando o navio "Titanic" está se preparando para sua viagem de luxo para Nova York. Jack e Rose se
enamoram e se casam, mas a viagem é marcada por um desastre inesperado.
[Portuguese-EnglishTrans:The movie "Titanic" is a 1997 historical romance that features a fictional
narrative about a woman named Rose and a man named Jack. The story is set during the late 19th century,
as the ship "Titanic" is preparing for its luxury voyage to New York. Jack and Rose fall in love and get
married, but the journey is marked by an unexpected disaster.]
Figure12:MorevisualizationresultsbetweentheuserandPARROTusingPortugueseprompts.
Arabic: ؟ﺎﻨھ ﻲﺗرﺎﯾز ﺪﻨﻋ ﺎﮭﻨﻣ ارًﺬﺣ نﻮﻛأ نأ ﺐﺠﯾ ﻲﺘﻟا ءﺎﯿﺷﻷا ﺎﻣ
[Arabic-EnglishTrans:What's the ending of this movie?]
Output: ﻖﺋاﻮﻋ يأ وأ ،ﺔﯿﺋﺎﻤﻟا ءﺎﯿﺷﻷا وأ ،ﺔﻠﻠﺒﻤﻟا ﺢﻄﺳﻷا ﻞﺜﻣ ،ﺔﻠﻤﺘﺤﻣ ﺮطﺎﺨﻣ يأ ﻦﻣ ارًﺬﺣ نﻮﻜﯾ نأ ﺐﺠﯾ ،ﻊﻗﻮﻤﻟا اﺬھ ةرﺎﯾز ءﺎﻨﺛأ
ﻞﺜﻣ ،ﺔﯿﺌﯿﺒﻟا ﻞﻣاﻮﻌﻟﺎﺑ ﻢﻠﻋ ﻰﻠﻋ اﻮﻧﻮﻜﯾ نأ راوﺰﻟا ﻰﻠﻋ ﺐﺠﯾ ،ﻚﻟذ ﻰﻟإ ﺔﻓﺎﺿﻹﺎﺑ .ﺔﯿﺋﺎﻤﻟا ءﺎﯿﺷﻷا وأ رﻮﺨﺼﻟا وأ رﻮﺨﺼﻟا ﻞﺜﻣ ،ﺔﻠﻤﺘﺤﻣ
.ﺔﯿﺿﺮﻣو ﺔﻨﻣآ ﺔﺑﺮﺠﺗ نﺎﻤﻀﻟ ،ﺔﯿﺋﺎﻤﻟا فوﺮﻈﻟا وأ ،ةراﺮﺤﻟا تﺎﺟرد وأ ،ﺲﻘﻄﻟا تاﺮﯿﻐﺗ
[Arabic-EnglishTrans:During the visit to this site, one should be cautious of any potential hazards, such
as wet surfaces, or water-related objects, or any potential obstacles, like rocks or boulders or aquatic
objects. Additionally, visitors should be aware of environmental factors, such as changes in weather,
temperature, or water conditions, to ensure a safe and satisfying experience.]
Figure13:MorevisualizationresultsbetweentheuserandPARROTusingArabicprompts.
21Turkish:Bu malzemelerle hangi yemekleri yapabilirim?
[Turkish-EnglishTrans: What are the meals that I can cook with these?]
Output: Buzdolabındaki çeşitli malzemelerle, çilekli bir salata, havuçlu bir sandviç, meyveli bir yemek
ve süt ve yoğurtla bir yemek gibi çeşitli yemekler yapılabilir. Buzdolabı, havuç, çilek ve yoğurt gibi
diğer gıda maddelerini de içerir; bunlar çeşitli yemekler yaratmak için kullanılabilir. Birden fazla
yiyecek maddesi ve kapların olması, buzdolabının iyi stoklandığını ve çeşitli yemek seçeneklerine
olanak tanıdığını gösterir.
[Turkish-EnglishTrans:With the various items in the refrigerator, one can cook a variety of meals, such
as a salad with strawberries, a sandwich with carrots, a meal with berries, and a meal with milk and
yogurt. The refrigerator also contains other food items, such as carrots, strawberries, and yogurt, which
can be used to create a diverse range of dishes. The presence of multiple food items and containers
suggests that the refrigerator is well-stocked, allowing for a variety of meal options.]
Figure14:MorevisualizationresultsbetweentheuserandPARROTusingTurkishprompts.
Russian:Пожалуйста, помогите мне представить это место на
фотографии.
[Russian-EnglishTrans: Please help me introduce the place in this
picture.]
Output:Место, изображенное на картинке, -это собор Василия Блаженного, официально
известный как собор Покрова Пресвятой Богородицы на рву. Это знаковое строение находится в
Москве, Россия, на Красной площади.
[Russian-EnglishTrans:The place depicted in the picture is the Saint Basil's Cathedral, officially known
as the Cathedral of the Intercession of the Most Holy Theotokos on the Moat. This iconic structure is
located in Moscow, Russia, on Red Square. ]
Figure15:MorevisualizationresultsbetweentheuserandPARROTusingRussianprompts.
22