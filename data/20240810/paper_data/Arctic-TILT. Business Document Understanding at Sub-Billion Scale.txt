Arctic-TILT
Business Document Understanding at Sub-Billion Scale
ŁukaszBorchmann∗ MichałPietruszka† WojciechJas´kowski DawidJurkiewicz
PiotrHalama PawełJóziak‡ ŁukaszGarncarek PawełLiskowski
KarolinaSzyndler AndrzejGretkowski JulitaOłtusek GabrielaNowakowska§
ArturZawłocki ŁukaszDuhr PawełDyda MichałTurski§
Snowflake
name.surname@snowflake.com
§AdamMickiewiczUniversity †JagiellonianUniversity ‡WarsawUniversityofTechnology
Abstract 1 Introduction
The vast portion of workloads employing General-purpose LLMs and their multi-modal
LLMsinvolvesansweringquestionsgrounded counterparts provide a crucial advantage in pro-
on PDF or scan content. We introduce the cessautomation: theycanbeappliedimmediately,
Arctic-TILT achieving accuracy on par with eliminatingtheexpensiveandtime-consumingef-
models1000×itssizeontheseusecases.Itcan forts of creating dedicated system architecture
befine-tunedanddeployedonasingle24GB
and model development. Though they are suit-
GPU,loweringoperationalcostswhileprocess-
able choices for prototyping and building proof-
ingVisuallyRichDocumentswithupto400k
tokens. Themodelestablishesstate-of-the-art of-concept solutions, once the case is validated,
SNOWFLAKE CORPORATE GOOGLE SLIDES THEME 2024 v1.1
resultsonsevendiverseDocumentUnderstand- it becomes essential to consider the demands of
SNOWFLAKE CORPORATE GOOGLE SLIDES THEME 2024 v1.1
ing benchmarks, as well as provides reliable real-worlddeployments,suchascost-efficiency(Fu
confidencescoresandquickinference,which etal.,2024;Ongetal.,2024),fine-tunability(Liu
areessentialforprocessingfilesinlarge-scale
etal.,2022),andensuringaccurateconfidencecali-
ortime-sensitiveenterpriseenvironments. Software Hardware
bration(VanLandeghem,2024). Semi-Conductors
Arctic-TILT
WeconsidertheseissuesinthecontextofDocu-
mentUnderstanding(DU),whereitiscommonly Arctic-TILT
requiredtointegratetextual,layoutandgraphical
cluestoobtaintherequiredinformationandintro-
Answer Extract Key Summarize Finetune
Questions Information ContendtucetheArctiMc-oTdIeLlT,designedtoaddresstheneeds
ofbroad-usedeployments,costefficiency,anddo-
Answer: How many Fill out the template Get crucial terms of Learn new scenarios
attorneys are listedS NOWFLAKE wCOitRhPO tRhAeTE aGmOOoGLuEn StL,ID dESa TteHE,M E 20t2h4 ev1 .n1on-dmiscaloisnurea daptatioor inncsrefaoser aaccfurraaccyt ion of the cost of the
for the plaintiffs? and IBAN. agreemenlet.ading modeolns e.xisTtihnge onperso.posed solution achieves
2
state-of-the-artaccuracyonbusinessandlongdoc-
ument benchmarks of MP-DocVQA (Tito et al.,
Arctic-TILT 24GB GPU 500 PAGES 2023),DUDE(VaSonftwareLaHnarddwaree ghemetal.,2023),Kleis-
© 2024 Snowflake Inc. All Rights Reserved Semi-Conductors
ter NDA aArcntic-TdILT Charity (Stanislawek et al., 2021), Software
ArXiv-LayandPubMed-Lay(Nguyenetal.,2023),
Arctic-TILT
andremainscompetitivewithordersofmagnitude
Arctic-TILT
largermodelsonotherdocumentVQAdatasets.
Answer Extract Key Summarize Finetune
Questions Information Content Model 2 RelatedWorks
Answer: How many Fill out the template Get crucial terms of Learn new scenarios
attorneys are listed with the aFmioguuntr, eda1te:, Ar tc ht ei c n- oT n-I dL isT cloc so un res umesloorn ignc,rreiacshe layccfuorramcya tted TheappearanceofLLMsisaresultoftrendsthat
for the plaintiffs? and IBANP. DFsgivenaagsrienegmlee,ntc.ost-efficientGonP eUxisatinngd ocnaens.produce havealsobeenstronglyvisibleintheDUfieldin
their summary, answer questions, and extract va2lues, recentyears. Arctic-TILT
outperformingvastlyheavierLLMsandLVLMs.
Traditionally,tasksliketableorinformationex-
∗ SeeAppendixEforcontributions. tractionfromVisuallyRichDocumentswerebro-
© 2024 Snowflake Inc. All Rights Reserved
Answer Extract Key Summarize Finetune
Questions Information Content Model
Answer: How many Fill out the template Learn new scenarios
Get crucial terms of
attorneys are listed with the amount, date, or increase accuracy
the non-disclosure
for the plaintiffs? and IBAN. on existing ones.
agreement.
2
© 2024 Snowflake Inc. All Rights Reserved
4202
guA
8
]LC.sc[
1v23640.8042:viXraTILT Arctic-TILT Fused
embedding
VisionEncodinganditsFusionwithText
sumoftext&image fusionbytensorproduct Sum
firstlayeronly everyencoderlayer
Projection O
PretrainingandFinetuning
400kstepsofadaptation 900kstepsofadaptation
SFTon4datasets SFTon17datasets
Transformer Sum
denseattention,vanilla sparseattention,SLED
max9ktokens max400ktokens
basicoptimization heavyoptimization Multiplication
Table1: ComparisonofTILTandArctic-TILT. Projection V Projection R
kendownintoseparatesteps,suchasformrecogni-
tion,fielddetection,andvalueextraction(Prabhu
Sum
et al., 2021; Medvet et al., 2011; Rusiñol et al.,
2013; Peanho et al., 2012; Tian et al., 2016; Le
etal.,2019;Baeketal.,2019;HoltandChisholm, Dropout
2018; Wang et al., 2021; Carbonell et al., 2019). LayerNorm
Eachofthesestepstypicallyrequireddistinctmod-
elsorheuristicsandprocessingpipelinesthatwere
Stack
laterapproachedinamoreend-to-endmannerem-
ployinggraph-basedapproaches(Liuetal.,2019;
Hwang et al., 2021; Yu et al., 2021; Wang et al.,
Image Text
2024,interalia).
embedding embedding
Ultimately,theDUfieldhasconvergedonformu-
latingtasksinaunifiedtext-to-textformatdueto
itsrobustnessinhandlingvariousproblems,which Figure2:Arctic-TILTmodalityfusion.Itcanbeseenas
LLMs align well with due to their generic input- attentionwithrolevector(Schlagetal.,2019)simplified
outputformat(Mathewetal.,2021b,a;Borchmann concerning we calculate it over a pair of aligned text
etal.,2021). Althoughthisapproachappearsele- andimagetokens.
gantanditseaseofapplicationmakesitappealing
forindustrial-scaleimplementation,treatingdocu-
mentsforsmaller,problem-specificmodels,simi-
mentsaspuretextisofteninsufficient,particularly
larlytoFuetal.(2024);Zhaoetal.(2024)andfo-
where layout-intensive aspects dominate. Hence,
cusoncost-efficientdeployment(Ongetal.,2024).
therehasbeenarecent surgeinextending LLMs
with visual (Li et al., 2023; Wu et al., 2023), lay-
3 Arctic-TILT
outmodality(Fujitake,2024),orboth(Maoetal.,
2024; Li et al., 2024; Tang et al., 2023) to better Our starting point is the TILT encoder-decoder
capture the nuances of document structures and model, built upon T5 (Raffel et al., 2020) by (1)
improveperformanceonlayout-intensivetasks. extendingitssequentialpositionalbiaswithanat-
A separate line of work approaches DU prob- tention bias based on relative horizontal and ver-
lems using vision-only models (Kim et al., 2021, tical distances between each pair of tokens, and
2022;Leeetal.,2023;Beyeretal.,2024),assum- (2)addingcontextualizedimageembeddingsthat
ingonecanaddresstheproblemwithoutspecific coverthetokenimageregionsemanticsinthecon-
architecturalbiases. However,modelswithtextual text of its entire visual neighborhood (Powalski
inputoutperformthemwithanotableexampleof etal.,2021).
GPT-4Visionthatbenefitsfromtheavailabilityof WefurtherenhanceTILT’sperformanceandre-
OCR-recognizedtext(Borchmann,2024). movelimitations. Specifically, wepropose novel
Despitetheseadvancementsandthesignificant modality fusion, introduce attention sparsity, en-
benefitsthescaleofLLMsoffers,weprovideargu- hance training recipe, and optimize training andinference(Table1). Improvedvariantofthemodel
isreferredtoastheArctic-TILT.
Decoder
3.1 FusionofTextandVision
The TILT model’s unique approach to combin-
ingvisualandtextualsemanticsinvolvessumming
Contextualized
F
wordembeddingsandRoI-pooledrepresentations Vision
oftheword’sboundingboxwithavariantoftheU-
Netnetworkusedasanimageencoder. Visualand Feed
Nx
Forward
textual features are integrated once, immediately
afterembeddingbothinputs.
Webeginbyreplacingthepost-embeddingsum- 1D + 2D Multi-Head
mationoftextandvisionmodalitiesfromtheorigi- Biases Attention
nalTILTwithourproposedmechanism,integrated
withineachtransformerblock.
FusionbyTensorProduct. IncontrasttoPowal- Textual
Semantics
ski et al. (2021), we opt for the fusion of modal-
ities inspired by tensor product representations
(Smolensky,1990;Schmidhuber,1993)andtheir Figure3:TheArctic-TILTencoderblockcombinesCon-
Hadamard approximation (Schlag et al., 2019). textualized Vision from U-Net and Textual Semantics
Specifically,giventhetextandimageembeddings frominputembeddingsthroughFusion(F)operation.
t,i ∈ Rd,wecalculatethefusedembeddingwith: The Multi-HeadAttention isaugmentedwith 1Dand
2Dpositionalbiasestocapturespatialandsequential
Fuse(t,i) = O(V(t+i)⊙(1+Rt))+t arrangement. Thisprocedureisrepeatedineachlayer
(Nx),allowingtoprocessintegratedinformationfurther.
whereV,R,andO areRd×d trainableparameters.
Inpractice,weuseavariantofthismechanismwith
additionallayernormanddropout,asdepictedin usedduringthefinetuningandinferenceunderthis
Figure2andListing1. memorybudget.
Moduleplacement. Havingdescribedthemech- Chunkedprocessing. Toaddressthequadratic
anism of integrating two modalities together, the complexity of self-attention computation in the
question arises on the positioning of the fusion encoder, we employ a variant of fusion-in-
modulewithinthetransformerblock. decoder (de Jong et al., 2023), also known as
We found that placing the fusion module after blockwise encoding (Pietruszka et al., 2022) or
FFNsismostbeneficialasthefusionresultsaredi- SLED(Ivgietal.,2022)withzerochunkpadding.
rectlyfedtothenextlayer(Figure3). Additionally, This method restricts the encoder attention ma-
by applying fusion after every encoder layer, we trix to a neighborhood around its diagonal with
mitigate the vanishing gradient effect and enable a bounded width. Without padding, this results
themodeltofocusondifferentvisualfeaturesas in a block diagonal matrix, reducing the number
itscomprehensionofthedocumentimproves. ofnon-zeroattentionweightstoalinearquantity
relativetotheinputsequencelength.
3.2 LongContextSupport
Specifically,forafixedcorechunklengthc,and
Concerning the product-oriented nature of our overlapsizeo,beinghyperparametersofourmodel,
work,itisessentialtocoverasignificantfraction prefix of length l and total input size C = n·c,
of real-world documents of potentially arbitrary we build a sequence of chunks in the following
lengthswhileoperatingwithinlimitedresources. manner: chunknumber1isfilledwithltokensof
The outlined optimizations are guided by the prefix,andwithtokens0,..,c−loftheinput. Say
need to handle as much context as possible on chunknumberialreadyusedinputtokensofupto
widelyavailableA10andL4GPUsequippedwith t. Chunknumberi+1startswithltokensofprefix,
24GBvRAM.Weassumeasingle-GPUsetupand followedwithtokenst−o+1,t−o+2,...,t−
measure the impact of applied techniques and ar- o+c−loftheinput. Inpractice,wefoundacore
chitecturalchangesonthemaximumcontextlength chunk length of 1024 and no overlap to perform(A) TILT (B) Arctic-TILT Randomchunks. Whileourmodificationseffec-
Chunk length tivelyhandletheencoder’smemorylimitations,the
Encoder
lengthofconcatenatedchunkembeddingscanstill
causethedecodercross-attentiontoexceedavail-
able memory. Technically, the model can handle
230ktokensduringthetraining,whichwasfurther
addressedwithasimplemethodthatallowsforex-
tendingthelengthofthedocumentwhilealsoposi-
tivelyimpactingthescores. Werandomlydiscard
chunks,effectivelyexposingthemodeltothediffer-
entpartsoflongerdocumentsacrossepochs. The
firstchunk,typicallycontainingtheinitialpages,is
alwayspreservedtoprovideessentialcontext.
In addition to primary techniques, we employ
Fusion several other optimizations. Specifically, we use
mixedprecisiontrainingwithbfloat16whileturn-
ing off weight caching to save RAM, leading to
Target a2×improvementininferenceinputlength. Sec-
length
ondly,byrecomputingprojectionsforeachdecoder
layerinsteadofusingthekey-valuecache,weex-
Input length Decoder tend the maximum inference context to 389k to-
kens. Next,weoptimizetrainingbyoffloadingthe
Figure 4: An illustration of sparse attention matrices
decoder’sactivationsneededforbackpropagation
assumingatwo-layerencoderanddecoder. Theorigi-
nalTILT(A)consumesthecompleteinputatonce,in fromGPUtoCPU,minimizingpeakmemoryusage
contrasttoArctic-TILT(B)withblockwiseattention oftheGPUbyincreasingprocessingtime. Finally,
implementingmemory-efficientattentionreduces
thememoryoverheadoftheattentionmechanism
robustlyacrossmosttasks(seeAppendixC). (RabeandStaats,2022).
Theresultingsequencesarepassedthroughthe
encoderseparately. Theencoderoutputsarethen Ultimately,ouroptimizationsculminateinsignifi-
recombinedbyremovingtheprefixtokensembed- cantmemoryusageimprovements,allowingusto
dingsfromallbutthefirstchunkandconcatenating effectively train and deploy Arctic-TILT for doc-
theresults. Theresultingencoderoutputispassed uments up to 500 pages1 on a single 24GB GPU.
tothedecoder(seeFigure4). Thestep-by-stepsummaryisstudiedinTable2.
Sincetheunderlyingattentionimplementationis
alreadymemoryefficient,thistechniqueimproves 3.3 PretrainingandFinetuning
thecomputationalefficiencyintrainingandinfer-
Thetrainingprocessbeganwithaself-supervised
ence,resultingina10-foldincreaseininputlength
pretraining phase using the pretrained T5 large
duringtrainingandinference.
model(Raffeletal.,2020). Followingtheintroduc-
tionofTILTarchitecturechanges,whichincluded
Nestedstackcheckpointing. Implementinggra-
U-Net (Ronneberger et al., 2015) and 2D biases,
dient checkpointing over the entire 24-layer en-
as well as text-vision post-fusion, the model un-
coderstackreducesthememoryrequiredforacti-
derwent further self-supervised pretraining for a
vations. Onlythelastlayer’sactivationsarestored,
total of 900k steps based on documents from the
whicharenecessaryforthedecoder. Consequently,
CCpdf(Turskietal.,2022)andOCR-IDL(Biten
memorysavingsaresignificanthere,astherequire-
etal.,2022). Thesetwolarge-scale,publiclyavail-
mentforprocessing1Mtokensreducedfrom96GB
ablePDFresourcescomewithOCRresultsfrom
to merely 4GB for the encoder part, albeit at the
TesseractandAmazonTextract,respectively.
costofanadditionalforwardpassthroughtheen-
coder. Thisimprovementallowedtoquadruplethe
1Specifically, 390k input tokens with an output of 128
inputlengthfortraining. tokens,correspondingto780tokensperpageonaverage.Table 1
PPaaggee Arctic-TILT (MP) GRAM (MP) Arctic-TILT (SlideVQA) Arctic-TILT (MMLong) 32K + OCR
1 - 5 77,1 78,2 57,2 26,6
6 - 10 72,9 74,4 55,3 21,4
11 - 15 69,7 69,6 52,6 18,9
16 - 20 72,6 77,2 50,3 22,2
21 - 30 16,5
31 - 40 14,2
41 - 50 18,4
-50 13,3
Method ANLS Accuracy Page 0 Page 1
TILT finetuned 0,8122 50,7870 0,8639 0,7967
optuna 3
GRAM 0,8032 19,9841 0,8380 0,7854
1-5 26.6
6-10 21.4
11-15 18.9
16-20 22.2
21-30 16.5
31-40 14.2
41-50 18.4
50-10000 13.3
Inference Training MP-DocVQA SlideVQA
80
80
VanillaTILT 9k 4k 77,1
+attentionsparsity 87k 41k 60 72,9 69,7 72,6 60
+mixedprecision 179k 51k 57,2 55,3 52,6 50,3
+memoryefficientattention 183k 56k 40 40
Inference-onlyoptimizations 20
20
+nocross-attentionKVcache 389k
0
0
Training-onlyoptimizations 1-5 6-10 11-15 16- 1-5 6-10 11-15 16-
+nestedcheckpointing 230k
MMLongBench-Doc
+CPUoffloading 256k
30
+randomchunks 389k
26,6
Table 2: Max input length (tokens) consumed during 20 21,4 22,2
18,9 18,4
trainingandinferencegivensingle24GBGPU.Tested 16,5
10 14,2 13,3
fordocumentsupto500pages(389ktokens).
0
1-5 6-10 11-15 16-20 21-30 31-40 41-50 51-
Finally, the model was fine-tuned on QA and
KIEdatasets. Inthisphase,weincreasethenumber Figure5:ScoresofArctic-TILTonMP-DocVQA,Slide-
ofsuperviseddatasetsto17,comparedtoTILT’s VQA,andMMLongBench-Doc,dependingontheevi-
dencelocation(bucketsoffivepages).
original choice of four. The datasets chosen rep-
resentcriticalaspectsofDUtasks,including,but
notlimitedto,forms,financialreports,charts,in- consisting of questions posed to industrial docu-
voices,insurancedocuments,contracts,andlegal ments of up to 20 pages, outperforming GRAM
documents(detailedinAppendixB). (Blau et al., 2024) by 1 point. Further, we were 2
able to surpass the long-standing score of LAM-
4 Experiments
BERT(Garncareketal.,2021)onKleisterCharity
(Stanislaweketal.,2021),withdocumentsreach-
Westartbyexaminingdiversebenchmarksinthe
ing above 300 pages, by 4 points. Similarly, we
DocumentUnderstandingfield,focusingonthese
outperformERNIE-Layout(Pengetal.,2022)on
closertoenterpriseapplicationsduetothedomain
KleisterNDAby6points.
of the included documents and the presence of
ConcerningSlideVQA(Tanakaetal.,2023)that
multi-page inputs that systems encounter in real-
isbasedonvisuallyrichpresentationsofupto20
worldapplications(seeTable3).
slides,weobtain2pointslessthanGPT-4Vision.
Hyperparameters used during finetuning were
OnrecentlyintroducedMMLongBench-Doc(Ma
subjecttooptimizationoutlinedinAppendixC.
etal.,2024)thatevaluateszero-shotperformance
4.1 DocumentVisualQAandKIE ondocumentsaslongas400pages,weoutperform
vastly larger LLMs: Mixtral 8x7B by 8 points,
Regardingthemodel’sintendeduseforprocessing
QWen-Plus by 6 points, and LVLMs: Claude-3
unstructured documents, Document Visual Ques-
Opus by 9 points, InternVL by 11 points. Better
tion Answering and Key Information Extraction
performancewasattainedbymodelssuchasGem-
tasksappearbestsuitedforassessingperformance.
ini 1.5 Pro and GPT-4 Vision Omnia, which are
Multi-page. Arctic-TILTexcelswheninputisa believedtohavehundredsoftimesmoreparame-
long, business document. In the case of DUDE ters. Whereas it was the only task considered in
(VanLandeghemetal.,2023)consistingofPDFs zero-shotsetup,pleasenoteSection4.3studieshow
of up to 25 pages, sourced across industries, we theperformanceofourmodelimprovescompared
outperform GPT-4 Vision Turbo by 4 points and toGPT-4ogivenseveralannotateddocuments.
Gemini1.5Proby12points. Qualitatively,Arctic- Finally, giventhatthreeofdatasetsconsidered
TILT not only outperforms GPT-4Vt in handling under this category contain labeled positions of
non-answerablequestionsbutalsoexceedsstate-of- answerswithindocuments,wecaninvestigatehow
the-artmodelsinlistandabstractiveanalysistasks, themodel’sperformancechangesdependingonthe
anexampleshowingcomplexdatahandlingskills. evidencelocation. TheresultsshowninFigure5
Similarly, we establish state-of-the-art perfor- indicatetheprimacybias,withthehighestscores
manceonMP-DocVQA(Titoetal.,2023)dataset achievedwhenrelevantinformationappearsattheTable 1
PPaaggee Arctic-TILT (MP) GRAM (MP) Arctic-TILT (SlideVQA) Arctic-TILT (MMLong) 32K + OCR
1 - 5 77,1 78,2 57,2 26,6
6 - 10 72,9 74,4 55,3 21,4
11 - 15 69,7 69,6 52,6 18,9
16 - 20 72,6 77,2 50,3 22,2
21 - 30 16,5
31 - 40 14,2
41 - 50 18,4
-50 13,3
Method ANLS Accuracy Page 0 Page 1 Page 2 Page 3 Page 4
TILT finetuned 0,8122 50,7870 0,8639 0,7967 0,7551 0,7312 0,7105
optuna 3
GRAM 0,8032 19,9841 0,8380 0,7854 0,7528 0,7908 0,7452
1-5 26.6
6-10 21.4
11-15 18.9
16-20 22.2
21-30 16.5
31-40 14.2
41-50 18.4
50-10000 13.3
Payment Stubs Ghega Patents
0-shotDataset 52,4 37In,9dustrial Multipage State-of-ptahyes-tuAbsrt: Arctic-TILT
5- 92,2 76,7 0-shot: 52.4
MP-DocVQA ✓ ✓ GRAM 80.3 81.2
10- 93,2 82,8 5-shot: 92.2
KleisterCharity ✓ ✓ LAMBERT 83.6 88.1
15- 93,5 86,1 10-shot: 93.2
KleisterNDA ✓ ✓ ERNIE-Layout 88.1 94.3
20- 93,4 90,1 15-shot: 93.5
DUDE ✓/✗ ✓ GPT-4Vt+OCR 53.9 58.1
25-shot 94,9 89,7 20-shot: 93.4
MMLongBench-Doc† ✓/✗ ✓ GPT-4o 42.8 25.8
30-shot 95,2 90,5 25-shot: 94.9
SlideVQA ✗ ✓ GPT-4Vt+OCR 57.3 55.1
30-shot: 95.2
ArXiv-Lay ✗ ✓ BigBird-Pegasus+Layout 41.2 44.4
PubMed-Lay ✗ ✓ BigBird-Pegasus+Lg ahe yg oa u: t 42.1 44.8
DocVQA ✓ ✗ InternVL2.0Pro 0-shot: 37.9 95.1 90.2
VQA-CD ✓ ✗ QALayout 5-shot: 76.7 42.5 90.7
InfographicsVQA ✗ ✗ InternVL2.0Pro 10-shot: 82.8 86.8 57.0
15-shot: 86.1
20-shot: 90.1
Table3: Arctic-TILTcomparedtothepreviousstate-of-the-art. Ourmodelremainscompetitivedespitehaving
lessthan1Bparametersandexcelswheninputisalong,businessdocument. W25e-suhoste: 8t9h.7eoriginalmetricsforeach
dataset,i.e.,F1forKleisters,AccuracyforMMLongBench-Doc,EMforSlideV3Q0-sAho,t:R 9O0.5UGE-LforArXiv-Layand
PubMed-Lay,andANLSfortheremainingtasks;†denoteszero-shotevaluation.
MP-DocVQA SlideVQA
80 beginningoftheinp8u0 t(Liuetal.,2024). Payment Stubs
60 77,1 72,9 69,7 72,6 60 100
Single-page. In ben5 c7 h,2 m5a5r,k3s 5i2n,v6ol5v0i,3ng single- GPT-4o
40 40 75
pageexcerptsfrommulti-pagedocumentsorstan-
20 dalone images with20 limited input length, our
0 model shows prom 0ising results. While Arcitc- 50
1-5 6-10 11-15 16- 1-5 6-10 11-15 16-
TILTimprovedby2pointsoverTILTonDocVQA
MMLongBench-Doc
(M30athew et al., 2021b) and outperformed GPT-
4V, it particularly excels in the newly introduced 0-shot 5- 10- 15- 20- 25-shot
26,6
V2Q0A-CDda2t1a,4set,whic2h2,i2ncludesinvoicesandpur-
18,9 18,4
chase orders, establishing s1ta6,t5e-of-the-art results Ghega Patents
10 14,2 13,3 100
(SouleimanMahamoudetal.,2022).
Although there is still a gap compared to
0
102B 1I-n5tern6-V10L112-1.05 1P6r-2o0’s21p-3e0rf3o1r-m40an41c-e50 (C51h-en 75
GPT-4o
etal.,2024),especiallyinnon-businessInfograph-
icsVQA(Mathewetal.,2021a),ourachievements 50
highlight significant advancements in handling
multi-modalinputs.
0-shot 5- 10- 15- 20- 25-shot
4.2 Layout-AwareSummarization Figure6: ImprovementofArctic-TILTzero-shotaccu-
racygivenfine-tuningonupto25annotateddocuments.
TosupplementVQAandKIEresults,weexamine
Zero-shotpe2rformanceofGPT-4oforcomparison.
howArctic-TILTexploitslayoutinformationand
captureslong-rangedependenciesintheLoRaLay
collection of summarization tasks where, in con- 4.3 AdaptingtoNovelUseCases
trast to the majority of similar datasets, input is SomeoptimizationsintroducedinArctic-TILTaim
not plain text but a scientific document with rich toimprovetrainingperformanceundertheminimal
structure(Nguyenetal.,2023). memoryregime. Thesecapabilitiesenablefurther
Results presented in Table 3 show that even improvementofthemodelinaproductionenviron-
though,incontrasttothepreviousSOTA,wehad ment,especiallywhenencounteringout-of-domain
nopretrainingobjectiveexplicitlydesignedforthe examples or novel use cases, and appear vital in
summarizationtask,wecouldoutperformthebest linewithpreviousworks,whichhaveshownthat
model by a few points on both ArXiv-Lay and smaller LLMs can outperform larger, prompted
PubMed-Lay. modelsassumingweallowfine-tuning(Zhaoetal.,Table 1
Input Phi-3 Mini TILT (KV) TILT
4000 25 3,4 3,4
6500 41 5,3 5,3 7,73584905660377
16000 103 13,1 13,1
64000 412 51,9 51,9
512000 3298 414,5 775
MP-DocVQA SlideVQA
80
80
77,1
Input Phi-3 Mini TILT (KV) TILT 60 72,9 69,7 72,6
60
4000 27 3,6 3,6 57,2 55,3 52,6 50,3
40
40
6500 41 5,3 5,3
20
20
16000 104 13,6 13,6
0
0
1-5 6-10 11-15 16- 1-5 6-10 11-15 16-
64000 413 53,6 53,6
MMLongBench-Doc
512000 3299 13571 30
26,6
20 21,4 22,2
18,9 18,4
16,5
10 14,2 13,3
0
1-5 6-10 11-15 16-20 21-30 31-40 41-50 51-
1.0 Model Phi-3 Mini Arctic-TILT
95% CI
10000
Ideal
8x less
TFLOPs VQA / KIE
10000
1000
3 298
0.5 1000 775
100 412 415
103
100
10 41 5522
25
1133
10
55
0.0 33
4k 66..55kk 16k 64k 512k
0.0 0.5 1.0
Confidence Figure 8: Arctic-TILT’s computational efficiency
(TFLOPs,lowerisbetter)comparedtoPhi-3Minion
Figure7: Arctic-TILTcalibration.
VQA/KIEgiveninputsrangingfrom4kto512ktokens.
Summarization 13 571
10000
2024;BucherandMartini,2024).
andincorrectpredictions. Italsoshowsourmodel’s 3 299
Westudyhowthezero-shotaccuracyofArctic-
1000
ability to appropriately assign low-confidence to
TILT increases, given fine-tuning on up to 25 an-
predictionsdemandingadditionalhumanreview. 413
notateddocumentsfromholdoutdatasets. Inpar-
ticular, we rely on Ghega patents (Medvet et al., Toexplorethelandscapebeyondasingledataset, 100 104 5544
weprovideresultson18kdatapointssampledfrom
2011)andaprivatedatasetofpaymentstubsand 41
fourteen private and public datasets in Figure 7. 27 1144
comparethemodel’sperformancetoGPT-4o(refer
10
TheanalysisconfirmslowECEandindicatesthat 55
toAppendixDfordataset’sdetails). 44
theArctic-TILTconfidencescoreiswellcalibrated
Results shown in Figure 6 demonstrate that
astheaccuracy(meanscore)followsthediagonal
Arctic-TILT quickly approaches the accuracy of
4k 6.5k 16k 64k 512k
y = xonthecalibrationplot.
GPT-4o with as few as five annotated examples
andoutperformitgivenslightlymore. Thesefind-
4.5 ComputationalEfficiency Phi-3 Mini Arctic-TILT (KV) (no KV)
ingssupporttheargumentforemployingspecial-
ized,‘smaller’LLMsoverasinglegeneral-purpose The imperative for businesses to rapidly and effi-
model in production, emphasizing the solution’s cientlyprocesssubstantialdocumentvolumescalls
Expert (~90%)
cost-effectivenessandadaptability. for models that maximize throughput while also
maximizingoperationalefficiency.
4.4 ConfidenceCalibration
Toaddressthisaspectofthemodel,weanalyze
Following the postulate of Van Landeghem et al. theinferencefloatingpointoperationspersecond
(2023),weevaluatetheExpectedCalibrationError (TFLOP) required for Arctic-TILT compared to
(ECE) and Area Under the Risk-Coverage Curve Phi-3 Mini (Abdin et al., 2024), an example of
(AURC)ontheDUDEdataset. Eachanswer’scon- a decoder-only model featuring 3.8B parameters
fidenceiscomputedfromalistofper-tokenscores. andoptimizedbyresortingtotheattentionsliding
In contrast to some previous works, we take the window. Thelatterwasselectedasawell-known
minimumscoreinthelistratherthanthegeometric reference model concerning the limited memory
meanaswefounditempiricallysuperior. and compute regime we aim at, though it is not
Obtained results show exceptional calibration capableofachievingsatisfactoryaccuracyonDoc-
and confidence assessment, achieving a state-of- umentUnderstandingtasks.
the-art ECE of 7.6, significantly improving upon ResultspresentedinFigure8indicatethatArctic-
the previous best of 19.0. This suggests a closer TILT consistently demands lower TFLOP across
alignmentbetweenmodelconfidenceandaccuracy. all context lengths for our primary use case of
Additionally,ourAURCof25.3,whichsurpasses VQA/KIE,2 reflecting its smaller parameter size.
the previous best of 44.0, demonstrates that our
modelcaneffectivelydiscriminatebetweencorrect 2Weassumetheoutputof8tokens,whichislongerthan
1
)erocs
naem(
ycarucca
ledoMImportantly, concerning the input of 6.5k tokens, Stanisławek, Nikolai Scholz, and Vivek Raghu-
themeaninputlengthforVQA/KIEtasksconsid- nathan,whosesupportandguidanceasmanagers
eredbefore,werequire8×lessoperations. havebeenhelpfulthroughoutthisresearch. Finally,
wethankRafałKobielaforhisassistancewiththe
5 Summary
cloudinfrastructure.
WehaveintroducedtheArctic-TILTmodel,which
addresses TILT’s limitations in handling multi-
References
modal input, suboptimal training procedure, and
Marah Abdin et al. 2024. Phi-3 Technical Report: A
maximumcontextlength. Byanalyzingtheresults
Highly Capable Language Model Locally on Your
andconsideringthecost-efficiencyofthedesigned
Phone.
solution,weprovidedpracticalinsightsintodesign-
ingcapable,lightweightmodelsfortheindustry. In TakuyaAkiba,ShotaroSano,ToshihikoYanase,Takeru
Ohta,andMasanoriKoyama.2019. Optuna: ANext-
particular,we:
generationHyperparameterOptimizationFramework.
InProceedingsofthe25thACMSIGKDDInterna-
• establishedstate-of-the-artperformanceonseven
tionalConferenceonKnowledgeDiscoveryandData
benchmarksdemandingtext,vision,andlayout Mining.
comprehension;
YoungminBaek,BadoLee,DongyoonHan,Sangdoo
Yun, and Hwalsuk Lee. 2019. Character Region
• demonstratedthatwithintheindustrialapplica-
AwarenessforTextDetection.
tions setting and while keeping the parameter
countbelow1B,onecouldachieveperformance LucasBeyeretal.2024. PaliGemma: Aversatile3B
betterorcomparabletovastlylargerLLMsand VLMfortransfer.
LVLMs;
AliFurkanBiten,RubènTito,LluisGomez,ErnestVal-
veny, andDimosthenisKaratzas.2022. OCR-IDL:
• presented a novel modality fusion mechanism OCR Annotations for Industry Document Library
inspired by tensor product representations, and Dataset.
have shown how effectively apply it across the
Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts,
transformerencoder;
RoyGanz,EladBenAvraham,AviadAberdam,Sha-
harTsiper,andRonLitman.2024. GRAM:Global
• demonstratedhow,withwell-designedattention ReasoningforMulti-PageVQA.
sparsity patterns and numerous other optimiza-
ŁukaszBorchmann.2024. NotesonApplicabilityof
tions, consume extensive input sequences dur-
GPT-4toDocumentUnderstanding.
ing training and inference, given a single cost-
efficientGPU,whilemaintainingcompetitiveac- ŁukaszBorchmann,MichałPietruszka,TomaszStanis-
curacyofthemodel; lawek,DawidJurkiewicz,MichałTurski,Karolina
Szyndler,andFilipGralin´ski.2021. DUE:End-to-
endDocumentUnderstandingBenchmark. InThirty-
• provided insights that can be applied to design
fifthConferenceonNeuralInformationProcessing
futuregenerationsofmultimodalmodels,partic-
SystemsDatasetsandBenchmarksTrack(Round2).
ularlyforvisually-richdocumentprocessing.
Martin Juan José Bucher and Marco Martini. 2024.
Ourworkillustratesthatstrategicdesignandopti- Fine-Tuned’Small’LLMs(Still)SignificantlyOut-
perform Zero-Shot Generative AI Models in Text
mizationcanrivalthecapabilitiesoflarger,more
Classification.
resource-intensivemodels.
Manuel Carbonell, Alicia Forn’es, Mauricio Ville-
Acknowledgements gas, and Josep Llad’os. 2019. TreyNet: A Neu-
ralModelforTextLocalization, Transcriptionand
WeexpressoursinceregratitudetoTomaszDwo- Named Entity Recognition in Full Pages. ArXiv,
jak and Daniel Campos for their feedback on the abs/1912.10016.
manuscript and suggestions that have greatly en-
Zhe Chen et al. 2024. How Far Are We to GPT-4V?
hancedthequalityofthiswork. Wealsoextendour ClosingtheGaptoCommercialMultimodalModels
thankstoŁukaszSłabinski,MichałGdak,Tomasz withOpen-SourceSuites.
theaveragetargetlengthofevaluationdatasetsmentionedin Aakanksha Chowdhery et al. 2022. PaLM: Scaling
Section4.1. LanguageModelingwithPathways.KennyDavila,FeiXu,SaleemAhmed,DavidA.Men- GeewookKim,TeakgyuHong,MoonbinYim,Jinyoung
doza,SrirangarajSetlur,andVenuGovindaraju.2022. Park,JinyeongYim,WonseokHwang,SangdooYun,
ICPR 2022: Challenge on Harvesting Raw Tables DongyoonHan,andSeunghyunPark.2021. Donut:
fromInfographics(CHART-Infographics). In2022 DocumentUnderstandingTransformerwithoutOCR.
26thInternationalConferenceonPatternRecogni- CoRR,abs/2111.15664.
tion(ICPR),pages4995–5001.
AnhDucLe,DungVanPham,andTuanAnhNguyen.
Brian Davis, Bryan Morse, Bryan Price, Chris Tens- 2019. DeepLearningApproachforReceiptRecogni-
meyer, CurtisWigington, andVladMorariu.2022. tion.
End-to-endDocumentRecognitionandUnderstand-
ingwithDessurt. Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu,
Fangyu Liu, Julian Eisenschlos, Urvashi Khandel-
Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, wal, Peter Shaw, Ming-Wei Chang, and Kristina
Nicholas FitzGerald, Sumit Sanghai, Fei Sha, and Toutanova. 2023. Pix2struct: Screenshot parsing
WilliamCohen.2023. FiDO:Fusion-in-Decoderop- aspretrainingforvisuallanguageunderstanding.
timized for stronger performance and faster infer-
ence. Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu,
Fangyu Liu, Julian Martin Eisenschlos, Urvashi
Xue-Yong Fu, Md Tahmid Rahman Laskar, Elena Khandelwal, Peter Shaw, Ming-Wei Chang, and
Khasanova, ChengChen, andShashiBhushanTN. Kristina Toutanova. 2022. Pix2Struct: Screenshot
2024. Tiny Titans: Can Smaller Large Language Parsing as Pretraining for Visual Language Under-
ModelsPunchAboveTheirWeightintheRealWorld standing. ArXiv,abs/2210.03347.
forMeetingSummarization?
JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.
MasatoFujitake.2024. LayoutLLM:LargeLanguage 2023. BLIP-2: BootstrappingLanguage-ImagePre-
Model Instruction Tuning for Visually Rich Docu- trainingwithFrozenImageEncodersandLargeLan-
mentUnderstanding. InInternationalConferenceon guageModels.
LanguageResourcesandEvaluation.
XinLi,YunfeiWu,XinghuaJiang,ZhihaoGuo,Ming-
Łukasz Garncarek, Rafał Powalski, Tomasz mingGong,HaoyuCao,YinsongLiu,DeqiangJiang,
Stanisławek, Bartosz Topolski, Piotr Halama, andXingSun.2024. EnhancingVisualDocument
MichałTurski,andFilipGralin´ski.2021. LAMBERT: UnderstandingwithContrastiveLearninginLarge
Layout-AwareLanguageModelingforInformation Visual-LanguageModels.
Extraction, page 532–547. Springer International
Publishing. HaokunLiu,DerekTam,MohammedMuqeeth,JayMo-
hta,TenghaoHuang,MohitBansal,andColinRaffel.
XavierHoltandAndrewChisholm.2018. Extracting 2022. Few-ShotParameter-EfficientFine-Tuningis
StructuredDatafromInvoices. InProceedingsofthe BetterandCheaperthanIn-ContextLearning.
AustralasianLanguageTechnologyAssociationWork-
shop2018,pages53–59,Dunedin,NewZealand. NelsonF.Liu,KevinLin,JohnHewitt,AshwinParan-
jape,MicheleBevilacqua,FabioPetroni,andPercy
WonseokHwang,JinyeongYim,SeunghyunPark,So- Liang. 2024. Lost in the Middle: How Language
heeYang, andMinjoonSeo.2021. SpatialDepen- ModelsUseLongContexts. TransactionsoftheAsso-
dencyParsingforSemi-StructuredDocumentInfor- ciationforComputationalLinguistics,12:157–173.
mationExtraction.
Xiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha
Maor Ivgi, Uri Shaham, and Jonathan Berant. 2022. Zhao.2019. GraphConvolutionforMultimodalIn-
EfficientLong-TextUnderstandingwithShort-Text formationExtractionfromVisuallyRichDocuments.
Models.
YuboMa, YuhangZang, LiangyuChen, MeiqiChen,
Guillaume Jaume, Hazim Kemal Ekenel, and Jean- YizhuJiao,XinzeLi,XinyuanLu,ZiyuLiu,YanMa,
PhilippeThiran.2019. FUNSD:ADatasetforForm XiaoyiDong,PanZhang,LiangmingPan,Yu-Gang
Understanding in Noisy Scanned Documents. In Jiang,JiaqiWang,YixinCao,andAixinSun.2024.
2019InternationalConferenceonDocumentAnaly- MMLongBench-Doc: BenchmarkingLong-context
sisandRecognitionWorkshops(ICDARW),volume2, DocumentUnderstandingwithVisualizations.
pages1–6.
IbrahimSouleimanMahamoud,MickaëlCoustaty,Au-
Geewook Kim, Teakgyu Hong, Moonbin Yim, rélie Joseph, Vincent Poulain d’Andecy, and Jean-
JeongYeonNam,JinyoungPark,JinyeongYim,Won- MarcOgier.2022. QAlayout: QuestionAnswering
seokHwang,SangdooYun,DongyoonHan,andSe- Layout Based on Multimodal Attention for Visual
unghyunPark.2022. OCR-FreeDocumentUnder- Question Answering on Corporate Document. In
standingTransformer. InEuropeanConferenceon DocumentAnalysisSystems,pages659–673,Cham.
ComputerVision(ECCV). SpringerInternationalPublishing.ZhimingMao,HaoliBai,LuHou,JianshengWei,Xin Recognition–ICDAR2021Workshops,pages377–
Jiang, Qun Liu, and Kam-Fai Wong. 2024. Visu- 388,Cham.SpringerInternationalPublishing.
allyGuidedGenerativeText-LayoutPre-trainingfor
DocumentIntelligence. ArXiv,abs/2403.16516. Markus N. Rabe and Charles Staats. 2022. Self-
attentionDoesNotNeedO(n2)Memory.
MineshMathew,VirajBagal,RubènPérezTito,Dimos-
thenisKaratzas,ErnestValveny,andC.VJawahar. ColinRaffel,NoamShazeer,AdamRoberts,Katherine
2021a. InfographicVQA. Lee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJ.Liu.2020. ExploringtheLim-
MineshMathew,DimosthenisKaratzas,andC.V.Jawa-
itsofTransferLearningwithaUnifiedText-to-Text
har.2021b. DocVQA:ADatasetforVQAonDocu-
Transformer. JMRL.
mentImages.
Sachin Raja, Ajoy Mondal, and C. V. Jawahar. 2023.
Eric Medvet, Alberto Bartoli, and Giorgio Davanzo.
ICDAR 2023 Competition on Visual Question An-
2011. Aprobabilisticapproachtoprinteddocument
swering on Business Document Images. In Docu-
understanding. InternationalJournalonDocument
mentAnalysisandRecognition-ICDAR2023,pages
AnalysisandRecognition(IJDAR),14:335–347.
454–470,Cham.SpringerNatureSwitzerland.
LauraNguyen,ThomasScialom,BenjaminPiwowarski,
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
andJacopoStaiano.2023. LoRaLay: AMultilingual
KnowWhatYouDon’tKnow: UnanswerableQues-
andMultimodalDatasetforLongRangeandLayout-
tionsforSQuAD.
AwareSummarization.
JordyvanLandeghem.2024. IntelligentAutomationfor OlafRonneberger,PhilippFischer,andThomasBrox.
AI-driven Document Understanding. Ph.D. thesis, 2015. U-Net: ConvolutionalNetworksforBiomedi-
KULeuven. calImageSegmentation. InMedicalImageComput-
ingandComputer-AssistedIntervention–MICCAI
JordyvanLandeghem,RubénTito,ŁukaszBorchmann, 2015,pages234–241,Cham.SpringerInternational
Michał Pietruszka, Paweł Józiak, Rafał Powalski, Publishing.
DawidJurkiewicz,MickaëlCoustaty,BertrandAck-
aert,ErnestValveny,MatthewBlaschko,SienMoens, Marçal Rusiñol, Tayeb Benkhelfallah, and Vin-
and Tomasz Stanisławek. 2023. Document Under- centPoulaindAndecy.2013. FieldExtractionfrom
standingDatasetandEvaluation(DUDE). AdministrativeDocumentsbyIncrementalStructural
Templates. In201312thInternationalConference
Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin onDocumentAnalysisandRecognition,pages1100–
Chiang,TianhaoWu,JosephE.Gonzalez,MWaleed 1104.
Kadous,andIonStoica.2024. RouteLLM:Learning
toRouteLLMswithPreferenceData. Imanol Schlag, Paul Smolensky, Roland Fernandez,
Nebojsa Jojic, Jürgen Schmidhuber, and Jianfeng
ClaudioAntonioPeanho,HenriqueStagni,andFlavio
Gao.2019. EnhancingtheTransformerwithExplicit
Soares Correa da Silva. 2012. Semantic informa-
RelationalEncodingforMathProblemSolving.
tionextractionfromimagesofcomplexdocuments.
AppliedIntelligence,37:543–557.
J. Schmidhuber. 1993. Reducing the Ratio Between
LearningComplexityandNumberofTimeVarying
Qiming Peng, Yinxu Pan, Wenjin Wang, Bin Luo,
Variables in Fully Recurrent Nets. In ICANN ’93,
Zhenyu Zhang, Zhengjie Huang, Teng Hu, Wei-
pages460–463,London.SpringerLondon.
chongYin,YongfengChen,YinZhang,ShikunFeng,
YuSun,HaoTian,HuaWu,andHaifengWang.2022.
PaulSmolensky.1990. Tensorproductvariablebinding
ERNIE-Layout: LayoutKnowledgeEnhancedPre-
andtherepresentationofsymbolicstructuresincon-
trainingforVisually-richDocumentUnderstanding.
nectionistsystems. ArtificialIntelligence,46(1):159–
216.
MichałPietruszka,ŁukaszBorchmann,andŁukaszGar-
ncarek.2022. SparsifyingTransformerModelswith
IbrahimSouleimanMahamoud,MickaëlCoustaty,Au-
TrainableRepresentationPooling.
rélie Joseph, Vincent Poulain d’Andecy, and Jean-
RafałPowalski,ŁukaszBorchmann,DawidJurkiewicz, MarcOgier.2022. QAlayout: QuestionAnswering
Tomasz Dwojak, Michał Pietruszka, and Gabriela Layout based on multimodal Attention for visual
Pałka.2021. GoingFull-TILTBoogieonDocument questionansweringoncorporateDocument. Acous-
UnderstandingwithText-Image-LayoutTransformer. ticsResearchLettersOnline.
In Document Analysis and Recognition – ICDAR
2021,pages732–747,Cham.SpringerInternational TomaszStanislawek,FilipGralinski,AnnaWróblewska,
Publishing. DawidLipinski,AgnieszkaKaliska,PaulinaRosal-
ska,BartoszTopolski,andPrzemyslawBiecek.2021.
Nishant Prabhu, Hiteshi Jain, and Abhishek Tripathi. Kleister:KeyInformationExtractionDatasetsInvolv-
2021. MTL-FoUn:AMulti-TaskLearningApproach ingLongDocumentswithComplexLayouts. CoRR,
toFormUnderstanding. InDocumentAnalysisand abs/2105.05796.TomaszStanisławek,FilipGralin´ski,AnnaWróblewska, JustinZhao,TimothyWang,WaelAbid,GeoffreyAn-
DawidLipin´ski,AgnieszkaKaliska,PaulinaRosal- gus, Arnav Garg, Jeffery Kinnison, Alex Sherstin-
ska,BartoszTopolski,andPrzemysławBiecek.2021. sky,PieroMolino,TravisAddair,andDevvretRishi.
Kleister: Key Information Extraction Datasets In- 2024. LoRALand: 310Fine-tunedLLMsthatRival
volvingLongDocumentswithComplexLayouts. In GPT-4,ATechnicalReport.
DocumentAnalysisandRecognition–ICDAR2021,
pages564–579, Cham.SpringerInternationalPub- Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang,
lishing. HaozhouZhang,andTat-SengChua.2022. Towards
complexdocumentunderstandingbydiscretereason-
RyotaTanaka,KyosukeNishida,KosukeNishida,Taku ing. InProceedingsofthe30thACMInternational
Hasegawa, Itsumi Saito, and Kuniko Saito. 2023. ConferenceonMultimedia,pages4857–4866.
SlideVQA:ADatasetforDocumentVisualQuestion
AnsweringonMultipleImages.
Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang,
Yang Liu, Chenguang Zhu, Michael Zeng, Cha
Zhang, and Mohit Bansal. 2023. Unifying Vision,
Text,andLayoutforUniversalDocumentProcessing.
Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier
Garcia, Jason Wei, Xuezhi Wang, Hyung Won
Chung, Siamak Shakeri, Dara Bahri, Tal Schuster,
HuaixiuStevenZheng,DennyZhou,NeilHoulsby,
andDonaldMetzler.2023. UL2: UnifyingLanguage
LearningParadigms.
ZhiTian,WeilinHuang,TongHe,PanHe,andYuQiao.
2016. DetectingTextinNaturalImagewithConnec-
tionistTextProposalNetwork.
Rubèn Tito, Dimosthenis Karatzas, and Ernest Val-
veny. 2023. Hierarchical multimodal transformers
forMulti-PageDocVQA.
MichałTurski,TomaszStanisławek,FilipGralin´ski,and
Karol Kaczmarek. 2022. Building the High Qual-
ityCorpusforVisuallyRichDocumentsfromWeb
CrawlData.
DongshengWang,ZhiqiangMa,ArminehNourbakhsh,
KangGu,andSameenaShah.2024. DocGraphLM:
DocumentalGraphLanguageModelforInformation
Extraction.
Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi
Tang,JiaxinZhang,ShuaitaoZhang,QianyingWang,
Y.Wu,andMingxiangCai.2021. TowardsRobust
VisualInformationExtractioninRealWorld: New
DatasetandNovelSolution. ArXiv,abs/2102.06732.
Thomas Wang, Adam Roberts, Daniel Hesslow,
TevenLeScao,HyungWonChung,IzBeltagy,Julien
Launay, and Colin Raffel. 2022. What Language
ModelArchitectureandPretrainingObjectiveWork
BestforZero-ShotGeneralization?
ChenfeiWu, ShengmingYin, WeizhenQi, Xiaodong
Wang,ZechengTang,andNanDuan.2023. Visual
ChatGPT:Talking,DrawingandEditingwithVisual
FoundationModels.
Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, and
RongXiao.2021. PICK:ProcessingKeyInforma-
tion Extraction from Documents using Improved
GraphLearning-ConvolutionalNetworks. In2020
25thInternationalConferenceonPatternRecogni-
tion(ICPR),pages4363–4370.1 class TiltLayerNorm(nn.Module):
2 """
3 This is essentially the T5 modification of layer norm, referred to as RMS norm.
4
5 Args:
6 dim: the dimension of vectors to be normalized, i.e. the last dimension of
the input tensor
7 eps: small positive value added to computed second moment for numerical
stability
8 """
9
10 def __init__(self, dim: int, eps: float = 1e-6) -> None:
11 super().__init__()
12 self.w = nn.Parameter(torch.ones(dim))
13 self.eps = eps
14 self.init_weights()
15
16 def forward(self, inp: Tensor) -> Tensor:
17 dtype = inp.dtype
18 x = inp.to(torch.float32)
19 squared_norm = x.pow(2).mean(dim=-1, keepdim=True)
20 x = x * torch.rsqrt(squared_norm + self.eps)
21 return self.w * x.to(dtype)
22
23 def init_weights(self, factor: float = 1.0) -> None:
24 self.w.data.fill_(factor * 1.0)
25
26
27 class TiltPostFusionModule(nn.Module):
28 """
29 Introduced in the Arctic-TILT paper.
30
31 Args:
32 d_model: dimension of input vectors
33 dropout: probability of dropout applied to input embeddings
34 layer_norm: the module responsible for input embeddings
35 """
36
37 def __init__(self, d_model: int, dropout: float, layer_norm: TiltLayerNorm):
38 super().__init__()
39 self.layer_norm = layer_norm
40 self.to_v = nn.Linear(d_model, d_model, bias=False)
41 self.to_out = nn.Linear(d_model, d_model, bias=False)
42 self.to_r = nn.Linear(d_model, d_model, bias=False)
43 self.dropout = nn.Dropout(dropout)
44
45 def forward(self, text_queries: Tensor, image_queries: Tensor) -> Tensor:
46 """
47 Compute module’s forward pass.
48
49 Args:
50 text_queries (Tensor): Tensor representing the primary input in the
fusion, which is text-based, or mixed.
51 image_queries (Tensor): Tensor representing the secondary input in the
fusion, which is image-based.
52 """
53 bs, l, d = text_queries.shape
54 inputs = torch.stack([text_queries, image_queries], dim=-2)
55 inputs = inputs.view(bs * l, 2, d)
56 normed_inputs = self.dropout(self.layer_norm(inputs))
57 normed_primary_input = normed_inputs[:, 0]
58 out: Tensor = self.to_v(normed_inputs.sum(-2))
59 out = out + out * self.to_r(normed_primary_input)
60 out = self.to_out(out)
61 out = out.view(bs, l, d)
62 return text_queries + out
Listing1: CompleteArctic-TILTmodalityfusionmodule.A WhyTILTasaStartingPoint?
WearguethattheeffectivenessoftheDUmodeldependsprimarilyonitsabilitytounderstandspecific
documentformatsandstructuresinthemostdocument-nativewaypossible,whichcanonlybeguaranteed
byequippingthemodelwithlayout-awarearchitecturalbiasesasearlyaspossible.
Thoughanumberoflargevision-onlymodelshavebeenproposed(Kimetal.,2022;Davisetal.,2022;
Leeetal.,2022),smallermodelswithanexplicitOCRstepstilloutperformthem. Notably,evenGPT-4
VisionbenefitsfromtheavailabilityofOCR-recognizedtext(Borchmann,2024). Althoughdocument
intelligencerequiresvisualfeatures(e.g.,torecognizecheckboxes,signatures,textcolors,andformatting),
thetextanditsspatialarrangementaremostimportant. Thisnecessitatesmodelswithheavytextualand
lightweightvisualencoders,suchasTILT.
Secondly,theimperativeforbusinessestorapidlyandefficientlyprocesssubstantialdocumentvolumes
calls for models that maximize throughput while also maximizing operational efficiency. Smaller,
specializedmodels,tailoredforsuchtasks,oftensurpasstheirlargerLLMcounterparts,whichstruggle
tomeetthesecriteriaduetotheirhighercomputationaldemandsandprocessingtimes. Themotivation
fortheseisnotonlypractical,asregulationssuchasGDPR,CCPA,orChinesedigitallawsmayrequire
specific types of information to be processed locally. This need is fulfilled with smaller, specialized
modelsthatcanbedeployedonbroadlyavailableGPUsandthusarenotrestrictedtoahandfulofregions.
The original TILT offers impressive performance despite keeping the number of parameters below
1B because of a well-balanced parameter budget and relying on encoder-decoder architecture, which,
despite lower popularity compared to decoder-only models, offers better quality in compute-matched
setups(Raffeletal.,2020;Chowdheryetal.,2022;Wangetal.,2022;Tayetal.,2023). Besides,weprefer
thembecauseachievingoptimalattentionsparsitypatternsismorestraightforwardwithseparateencoder
anddecodermodules.
Theencoder-decodermodelwithasizeabletextualbackboneandsmallvisualencoder,equippedwith
layoutarchitecturalbiasthathaspreviouslyestablishedstate-of-the-artresults,appearsaviablestarting
pointforbuildingamodernDUsystem.
B DatasetsforSupervisedFinetuning
TrainingofArctic-TILTincludedSFTphaseontwelvepubliclyavailableandfivein-houseannotated
datasets. The first group included Kleister Charity, Kleister NDA (Stanisławek et al., 2021), CHART-
⋆
Infographics (Davila et al., 2022), DeepForm (Borchmann et al., 2021), DocVQA (Mathew et al.,
2021b),DUDE(VanLandeghemetal.,2023),FUNSD(Jaumeetal.,2019),InfographicVQA(Mathew
etal.,2021a),SQuAD2.0(Rajpurkaretal.,2018),TAT-DQA(Zhuetal.,2022),VQA-CD(Mahamoud
etal.,2022),andVQAonBD(Rajaetal.,2023).
PrivatedatasetswerebasedonQAannotationsofIRS990forms,insurancereports,companyannual
reports,syntheticinvoices,andcharityannualreports. Togivetheresearchcommunityagrasponthe
characteristicsofthiscollection,weprovidethemostimportantstatisticsandexamplesofquestionsin
Figure9andTable4.
C UsedHyperparametersandtheirOptimization
Chunkingsetup. Westudiedthesizeoftheattentionblock,aswellastheoverlapsizeofconsecutive
blocks. Tooursurprise,thebestsetupforinferencewas1024tokensattentionsizewithnooverlap,and
theseconclusionsareindependentofthesetupoverlap/attentionsizeduringtraining.
Learningrateschedulingandprecision. Weobservedanon-trivialinferencebetweenthetwohyper-
parameters. Comparedtofp32pretraining,bf16pretrainingwithmoreaggresivelearningratescheduling
wasabletocatchup,andwithsamelearningrateschedulingwasobservablyworse. Weendedupwith
usingcosine_luhschedulerwith1%trainingstepswithconstantlearningrateof1e-3(warm-up),followed
by89%trainingstepswithlineardecaydownto2e-4,followedbycosineschedulingfortheremaining
10%stepsdecayingto5e-5. Sameobservationsweredrawnduringfine-tuning.Streamlit Apps Copy of Dataset explorer v4 Lukasz Share Run
193
Search objects 194
Create
195
196
AUTOSQL_DATA…
fig = px.histogram(df, x=column, labels={column: header, "count": "Count"}, n1b9i7nst=10)
fig.update_layout(bargap=0.1, barmode="overlay", height=3B0E0N,C HfMoAnRtK_size=24) 198
199
CYBERSYN_URB…
st.plotly_chart(fig, use_container_width=True, show_curve=False, show_rug=Tru2e0,0 config={"displayModeBar": False})
201
DATA_GOV
202
fig = px.histogram(df, x=column, labels={column: header, D"Uc_oPuEnRtSO":N A"LCount"}, n2b0i3ns=10)
fig.update_layout(bargap=0.1, barmode="overlay", height=300, font_size=24) 204
DU_TMP
205
st.plotly_chart(fig, use_container_width=True, show_curveM=FL_aRlEsGeIS,T RsYhow_rug=Tru2e0,6 config={"displayModeBar": False}) View selected documents
207
PUBLIC 208
209
SEMANTIC_MOD…
210
211
SNOWFLAKE
10k 10k
212
# st.metric("Mean ± std", f"{df[column].mean():,.{preScNiOsWioFnLA}KfE}$ G±… {df[colu2m1n3].std():,.{precision}f}")
1000 1000
214
SNOWFLAKE_FI…
215
216 100 100
SNOWFLAKE_SE…
exception_message = "<built-in function open_stream> returned a result with an except2i1o7n set"
218 10 10
219
220
1 1
221
0 100 200 300 0 100k 200k 300k
222
number of pages number of tokens
fig = px.histogram(df[~df.PAGE_COUNT.isin(to_filter_out)], x="PAGE_COUNT", labels2=2{3"PAGE_COUNT": "number of pages", "count": "Count"}, log_y=True, nbins=35)
fig.update_layout(bargap=0.1, barmode="overlay", height=300, font_size=24, yaxis_2r2a4nge=[0, maximum], xaxis_range=[0, 300]) #
Figure9: Lengthsofdocumentsincludedinfiveprivatedatasets(numberoftokensandpages).
225
st.plotly_chart(fig, use_container_width=True, show_curve=False, show_rug=True, c2o2n6fig={"displayModeBar": False})
Table4: OutlineofprivatedatasetsusedforSFT.
0 documents weren't taken into account in PAGE_COUNT, TOKEN_COUNT or
227
228 OCR_TOKEN_COUNT statistics due to OCR file problems.
Dataset SampleQuestions Documents Annotations
fig = px.histogram(df[~df.TOKEN_COUNT.isin(to_filter_out)], x="TOKEN_COUNT", labe2l2s9={"TOKEN_COUNT": "number of tokens", "count": "Count"}, log_y=True, nbins=27)
fig.update_layout(bargap=0.1, barmode="overlay", height=300, font_size=24, yaxis2_3r0ange=[0, maximum], xaxis_range=[0, 300000]) IRS990 Whatisthepercentageofpublicsupportintheyearofthereport? 3,097 38,025
231 WhatisthesumofthetotalliabilitiesinUSdollars?Whatisthe
EmployerIdentificationNumber?
st.plotly_chart(fig, use_container_width=True, show_curve=False, show_rug=True, c2o3n2fig={"displayModeBar": False})
InsuranceReports WhatwasthevalueoftotalpremiumswrittenintheSurplus 50 1,702 Type to search
233
&Self-Procuredcategoryin2016? Whoisthedirectorofthe
234 AlaskaDivisionofInsurance?Forwhosecontributionsweretax
235 creditsclaimedin2015?
236 CompanyAnnualReports Whatisthenameofthechiefexecutiveofficer?Whatisthetotal 648 3,522
netincomeforreportyear?Whatisthetier1capitalratio?
237
# make_chart(df[~df.OCR_TOKEN_COUNT.isin(to_filter_out)], "OCR_TOKEN_COUNT", "Number 2o3f8 OCR tokens", 0) SyntheticInvoices Whatisthenumberofitemsontheinvoice? Whatisthetotal 2,707 17,274
netamountoftheitemdescribedontheinvoice? Whatisthe
239
descriptionoftheitemofthetransaction?
240
CharityAnnualReports Whatistheindependentauditor’sname?Whatarethecharity’s 161 4,025
241
totalfundsinthebankandinhand? Whatisthenameofthe
242 organisation’schairman?
LB
243
Feedback
Trainingprotocol. Thefinetuningphase’shyperparametersaresetas100kstepsatbatchsize128with
theAdamWScaleoptimizer. Wesetlossreductiontomeanandweightdecayto1e−5. Additionally,we
usedcaseaugmentationofthewholetripleconsistingofthedocument,question,andanswer. Specifically,
ifwedetectthatthedocumentisnotalreadycasttoupperorlowercase,wecreateanaugmentedversion
of the three-tuple question-document-answer by casting them all to that case, similarly to Powalski
etal.(2021). Thismeansthatthereareuptothreeversionsofeachdatapoint,suchastheoriginalone,
uppercase,andlowercase.
Downstream tasks evaluation. For downstream task evaluation on benchmarks providing trainset
(DocVQA,MP-DocVQA,DUDE,KleisterCharity,KleisterNDA,SlideVQA,InfographicsVQA,VQA-
CD) we performed additional training with Optuna (Akiba et al., 2019) hyperparameter tuning. We
performed10-40studiesoptimizingthefollowinghyperparameters:
• case augmentation (on, off) – augment dataset with lowercased/uppercased version of training
samples,incasetheyarestatisticallydistinguishable;
• answervariantssampling(on,off)–forquestionswithmultipleversionsofthecorrectanswer(e.g.
100,$100),weeitherpickthesame,orsamplevariantperepoch;
• dropoutsampledwithuniformdistributionfromtheinterval(0,0.2);
tnuoc
🔎
tnuocTable 1
PPaaggee Arctic-TILT (MP) GRAM (MP) Arctic-TILT (SlideVQA) Arctic-TILT (MMLong) 32K + OCR
1 - 5 77,1 78,2 57,2 26,6
6 - 10 72,9 74,4 55,3 21,4
11 - 15 69,7 69,6 52,6 18,9
16 - 20 72,6 77,2 50,3 22,2
21 - 30 16,5
31 - 40 14,2
41 - 50 18,4
-50 13,3
Method ANLS Accuracy Page 0 Page 1 Page 2 Page 3 Page 4
TILT finetuned 0,8122 50,7870 0,8639 0,7967 0,7551 0,7312 0,7105
optuna 3
GRAM 0,8032 19,9841 0,8380 0,7854 0,7528 0,7908 0,7452
1-5 26.6
6-10 21.4
11-15 18.9
16-20 22.2
21-30 16.5
31-40 14.2
41-50 18.4
50-10000 13.3
Payment Stubs Ghega Patents
0-shot 52,4 37,9 paystubs:
5- 92,2 76,7 0-shot: 52.4
10- 93,2 82,8 5-shot: 92.2
15- 93,5 86,1 10-shot: 93.2
20- 93,4 90,1 15-shot: 93.5
25-shot 94,9 89,7 20-shot: 93.4
30-shot 95,2 90,5 25-shot: 94.9
30-shot: 95.2
ghega:
0-shot: 37.9
5-shot: 76.7
10-shot: 82.8
15-shot: 86.1
20-shot: 90.1
25-shot: 89.7
30-shot: 90.5
Trained with 2048/0 Trained with 1024/128
0,70
0,66 0,66
0,63 0,64 0,64 0,64 0,65 0,64 0,64 0,65 0,64 0,65
0,63 0,63
0,61
0,57
0,50
2048 1536 1024/128 1024 768 512/128 512
Figure 10: Impact of chunk size and overlap size (chunk/overlap) on a downstream inference for two models,
assumingin-housedatasetofbusinessusecases. Weobservenopositiveimpactofoverlapforsufficientlylong
Model / param count
inputsequences,suchas1024tokens.
Context length
Table5: FinalpromptsusedforGPT-4obaselines.
Dataset Prompt 2
PaymentStubs Replace[ANSWER]withavalueinthetemplategivenquestionanddocument.←(cid:45)Question:[TEXT]
←(cid:45) Template: Based on the context, the answer to the question would be "[ANSWER]". ←(cid:45) ←(cid:45)
Normalizeamountstotwodecimalplaces,withoutthousandseparatorandwithoutdollarsign. ←(cid:45)
Normalizestatesusingpostalabbreviations,e.g.,TXorNJ.
GhegaPatents Replace[ANSWER]withavalueinthetemplategivenquestionanddocument.←(cid:45)Question:[TEXT]
←(cid:45) Template: Based on the context, the answer to the question would be "[ANSWER]". ←(cid:45) ←(cid:45)
NormalizedatestoYYYY-MM-DDformatexceptquestionaboutprioritywhichshouldremainsimilar
to"DD.MM.RRRR(countrycode)(optionalnumber)."
• weightdecaysampledwithlog-uniformdistributionfromtheinterval(1e−6,1e−2);
• learningratesampledwithlog-uniformdistributionfromtheinterval(1e−4,5e−3).
D FinetuningStudy
GPT-4obaseline. FollowingthefindingsofBorchmann(2024),weassumeinputimagesof2048px
alonglongerdimensions(usuallyheight)andsimilarprompts. Thelatterweresubjecttofurtherper-dataset
optimizationtocovertheconventionusedinconsidereddatasets(finalformpresentedinTable5).
Payment Stubs. The private dataset used for evaluation consists of American payment stubs, i.e.,
documentsobtainedbyanemployeeregardingthesalaryreceived. Thetestsplitcontains39documents
with448annotations. Sinceallcomefromdifferentcompanies,theirlayoutsdiffersignificantly. Questions
aim to extract employee and employer names, dates, addresses and information from payment tables,
whereeachrowconsistsofpaymenttype,hoursworked,andpaymentamount,e.g.,‘Whatisthenameof
theUSstateoftheemployee’saddress?’ or‘Whendoesthepayperiodfinish?’
E Contributions
ŁB. Performingearly-stagearchitectureablations,writingmostofthepaperandpreparingfigures,final
fusionbyTPmoduledesignandrelatedablationstudies,implementationofGPT-4obaselineforArctic-
TILTSFT,analysisofresultsonpublicbenchmarks,overseeinginitialmodelsparsificationexperiments,
studyoflongcontextutilization,self-supervisedpretrainingofthemodel.
ŁD. Variouscontributionsrelatedtoconfigurationsandautomatizationsofexperiments.PD. Optimizationofvariousdatapipelines(imageprocessing,loading,metriccomputation). Datasets
updatesandmodifications(mainfocusonincreasingloadingspeedandOCRcorrectness).
ŁG. Technicalleadershipandparticipationinthecodebaseimplementation,performanceoptimization,
andattentionsparsityefforts. Writingpartsofthepaper.
AG. Efficient implementation of attention sparsity. Contributions to codebase implementation and
memoryoptimizations. Preparationandcleaningofsomedatasets. Experimentswithmodelsizesand
architectures.
PH. Major contributions to memory- and compute-efficiency, including evaluation of long context
approacheswithatheoreticalmemorymodel,implementingnestedcheckpointingandmixed-precision,
andempiricallyevaluatingthecompletesolution’sperformanceandmemoryusagecharacteristics.
WJ. Leading SFT efforts, including fine-tuning the model’s final version, experiments with hyper-
parameters, training protocols, and dataset composition. Model performance improvements. Various
contributionsinthemodelandtrainingcode. Preparationandcleaningofsomedatasets.
PJ. Memoryfragmentationhandling. Implementationandcreationofsemi-syntheticlongdocument
Needle-in-a-Haystackbenchmarkusedinearlyexperiments. Datamanagement,curatingfinaltraining
andperformingafewdownstreamevaluations(DUDE,VQA_CD).Writingpartsofthepaper.
DJ. Leading efforts to increase the context length from 25 to 500 pages (conceptualization, brain-
storming,planning,guidance). Conductinginitialmemoryandthroughputexperiments,aswellasfinal
stresstestsandqualityexperimentsforverylargecontextlengths. ImplementationofCPUoffloading.
Performing few-shot finetuning experiments. Delivering results for SlideVQA, Kleister Charity, and
KleisterNDAdatasets.
PL. ImplementedTPfusionandconductedablationstudiesfocusingonmoduleplacement. Devised
enhancedtrainingprotocol,performedhyperparametertuning,andcontributedtovariousmodelimprove-
ments. PerformedexperimentalevaluationonDocVQAandInfographicsVQA.
GN. Preparationandmanagementofdatasets,theideabehindcreatingsemi-syntheticlongdocuments,
automation of data processing pipeline, conducting experiments and analyzing the results with long
documents.
JO. Selection and improvements of the training datasets (analysis of data quality, filtering the data,
fixingqualityissues),optimizationsofimageencoderanddataprocessing.
MP. Performingearly-stagearchitectureablations(researching,implementing,andstudyingeffects)
thatleadtoco-authoringTPfusion(i.e.,proposinginitialattn-basedversion,moduleplacementstudy,
andfusionineverylayer). Leadingeffortsinwritingpartsofthepaper(technicaloptimizations,training,
relatedworks,analysis,structuringandrewriting).
KS. Theideabehindattentionsparsification,ablationstudiesofvariousapproaches,implementationof
requiredprototypes,andanalysisoftheresults.
MT. Trainingloopoptimization(intermsofprocessingtimeanddataefficiency),performingdown-
streamevaluations(DocVQA,MP-DocVQA,MMLongBench-Doc),datasetpreparationandcleaning,
erroranalysis,andorganizationofwork.
AZ. Variouscontributionstothedevelopmentofthecodebase.