Activation thresholds and expressiveness of polynomial
neural networks
Bella Finkel∗, Jose Israel Rodriguez†, Chenxi Wu, Thomas Yahl
August 9, 2024
Abstract
Polynomial neural networks have been implemented in a range of applications and
present an advantageous framework for theoretical machine learning. A polynomial
neural network of fixed architecture and activation degree gives an algebraic map from
the network’s weights to a set of polynomials. The image of this map is the space of
functionsrepresentable by thenetwork. Its Zariskiclosureis an affinevariety known as
a neurovariety. The dimension of a polynomial neural network’s neurovariety provides
a measure of its expressivity. In this work, we introduce the notion of the activation
threshold of a network architecture which expresses when the dimension of a neurova-
riety achieves its theoretical maximum. In addition, we prove expressiveness results
for polynomial neural networks with equi-width architectures.
Introduction
Polynomial neural networks are important in applications and theoretical machine learning.
The function spaces and dimensions of neurovarieties for deep linear networks have been
studied, and new developments in the polynomial neural network setting have appeared. In
particular, resultsonthechoiceoftheactivationdegreeandthedimensionoftheneurovariety
have improved our understanding of the optimization process of these neural networks and
the ability of shallow and deep neural networks to replicate target functions [21, 27].
These theoretical results possess relevant implications. For appropriate datasets, poly-
nomial activation functions can reduce model complexity and computational costs by in-
troducing higher-order interactions between inputs, making it possible to model non-linear
phenomena more efficiently. Moreover, polynomial neural networks have been found to per-
form well in practice in high-impact fields such as healthcare and finance. Polynomial neural
∗This material is based upon work supported by the National Science Foundation Graduate Research
FellowshipProgramunderGrantNo. 2137424. Any opinions,findings,andconclusionsorrecommendations
expressed in this material are those of the authors and do not necessarily reflect the views of the National
ScienceFoundation. SupportwasalsoprovidedbytheGraduateSchoolandtheOfficeoftheViceChancellor
for Research at the University of Wisconsin-Madison with funding from the Wisconsin Alumni Research
Foundation.
†This researchwas partially supported by the Alfred P. Sloan Foundation.
1
4202
guA
8
]GL.sc[
1v96540.8042:viXranetworks have been successfully used to predict epidemic behavior [13], quantify the nat-
ural frequency of materials [11], analyze financial time series data [14, 25], and improve
three-dimensional shape representation for computer vision [32].
The expressiveness of artificial neural networks of different structures and activation
has been studied from several perspectives [23, 24, 29]. Substantial literature has shown
the expressive power of deep networks, from the perspective of number of linear regions
[24], universal approximation and VC dimensions [31, 29], a measure of complexity called
“trajectory length” [23], and the exact class of functions representable by ReLU networks
of various depths [18]. In another direction, [27] studied the effect of width of convolutional
neural networks on the loss landscape, and [22, 17] provided bounds on network width for
universal approximation for ReLU networks, while [19] provided such a bound for arbitrary
activation functions. The expressiveness of neural networks has also been studied via their
topological complexity, for example [4, 16]. In particular, [28] showed that networks with
non-increasing architecture (each layer is no wider than the previous) result in a connected
decision region. ReLU expressivity results have also been studied using algebraic geometry,
specifically tropical geometry [6, 16, 18].
An intuitive way of characterizing the expressiveness of a neural network is by the dimen-
sion of the space of representable functions as one allows its parameters to vary. For sigmoid
activation functions this has been done in [1], for ReLU activation and non-increasing ar-
chitectures (as well as a few others) this was done by [7, 16]. For polynomial activation
functions, [20, 21] studied this quantity and raised various conjectures, one of which we will
resolve in the current paper.
This paper is structured as follows. In Section 1, we review key definitions for polynomial
neural networks and introduce the notion of activation thresholds (Problem 8). Section 2
determines the expressivity of polynomial neural networks with high activation degree and
proves the existence of the activation threshold. The main result resolves a conjecture by
Kileel, Trager, and Bruna [20, Conjecture 16] and Kubjas, Li, Wiesmann [21, Conjecture
5.2]. In Section 3, we prove the expressivity of networks with equi-width architecture by
computing the activation threshold— in particular, we remove any hypothesis involving
“sufficiently high learning degree” by showing the activation threshold of these networks is
one. In Section 4 we provide an outlook for the future.
1 Polynomial neural networks and neurovarieties
For consistency, the notation introduced in this section is intended to agree with [20] and
[21] as much as possible.
An L-layer feedforward neural network F θ : Rd0 → RdL is a composition of affine-linear
maps f : Rdi−1 → Rdi and non-linear maps σ : Rdi → Rdi,
i i
F θ(x) = (f
L
◦σ
L−1
◦f
L−1
◦···◦f
2
◦σ
1
◦f 1)(x).
The architecture of the neural network F θ is the sequence d = (d 0,...,d L). Here, the affine-
linear maps are given by
f (x) : Rdi−1 → Rdi, x 7→ W x+b ,
i i i
2where the weights of the neural network are the matrices W ,...,W and the biases are the
1 L
vectors b ,...,b . The parameter set θ consists of these weight matrices and biases. The
1 L
activation map σ : Rdi → Rdi is given coordinate-wise by the activation function.
i
Deep learning consists of approximating a target function F : Rd0 → RdL by a neural
network F θ : Rd0 → RdL of a chosen architecture d = (d 0,...,d L) and activation function.
That is, deep learning concerns recovering parameters θ for which F θ most closely resembles
F. The map F θ is associated to the tuple of parameters θ by the parameter map
Ψd,σ : RN → Fun(Rd0,RdL), θ 7→ F θ. (1)
In this article, we are interested when the activation function consists of polynomials.
The most important case are pure powers, and this leads to the following definition.
Definition 1 (Polynomial neural network). A polynomial neural network p w : Rd0 → RdL
with fixed activation degree r and architecture d = (d ,d ,...,d ) is a feedforward neural
0 1 L
network of the form
p w(x) = (W
L
◦σ
L−1
◦W
L−1
◦σ
L−2
◦···◦σ
1
◦W 1)(x)
where W ∈ Rdi×di−1 and the activation maps σ (x) : Rdi → Rdi are given by coordinate-wise
i i
exponentiation to the r-th power,
σ (x) := (xr,...,xr ).
i 1 di
The parameters w are the entries in the matrices W , so that
i
w = (W ,W ,...,W ).
1 2 L
Remark 2. In the definition of polynomial neural network, the biases are taken to be zero.
The affine-linear map f in the i-th layer is then a linear map that we identify with the
i
matrix W .
i
An L-layer polynomial neural network p w with architecture d = (d 0,...,d L) and acti-
vation degree r is a tuple of homogeneous polynomials. The parameter map now takes the
matrix tuple (W ,W ,...,W ) to a tuple of degree rL−1 homogeneous polynomials in d
1 2 L 0
variables of length d
L
p
w
1
Ψd,r : Rd1×d0 ×···×RdL×dL−1 → (Sym rL−1(Rd0))dL, w 7→ p w =  . . . . (2)
p
w
 dL
 
To specify an element in the image of Ψd,r, it suffices to identify its vector of coefficients in
RdL(rL− d1 0+ −d 10−1)
≃ (Sym rL−1(Rd0))dL.
Remark 3 (Special cases). Note that if d = 1, the co-domain of the parameter map is the
L
space of degree rL−1 homogeneous polynomials. If L = 1 then the ambient space consists of
linear maps from Rd0 to RdL.
3The neuromanifold or functional space associated to p w is the family of all polynomial
neural networks with fixed activation degree r and architecture d = (d ,d ,...,d ) where
0 1 L
the weights are allowed to vary.
Definition 4 (Neurovariety). The image of Ψd,r is the neuromanifold Md,r, which is a
semialgebraic set in (Sym rL−1(Rd0))dL. The neurovariety Vd,r is the Zariski closure of Md,r,
and is an affine variety in (Sym rL−1(Rd0))dL.
The dimension of the neurovariety Vd,r provides a measure of the expressivity of a neural
network because it quantifies the degrees of freedom of the space of functions the network
produces [20]. In practice, this is the dimension of the neuromanifold Md,r; as Md,r is
semialgebraic, its dimension is the same as that of Vd,r.
The size of the weight matrices is a primary factor in determining the dimension of
the neurovariety. However, for all network architectures there exists a symmetry in the
weight matrices, known as multi-homogeneity. It follows that the dimension of a fiber of the
parameter map Ψd,r is at least iL =− 11 d i. ([20], Lemma 13).
Lemma 5 ([20], Multi-homogePneity). For all invertible diagonal matrices D ∈ Rdi×di and
i
permutation matrices P
i
∈ Rdi×di (i = 1,...,L − 1), the parameter map Ψd,r returns the
same map under the replacement
W ← P D W
1 1 1 1
W ← P D W
D−rPT
2 2 2 2 1 1
.
.
.
W ← W D−r PT .
L L L−1 L−1
where T indicates matrix transpose. Thus the dimension of a generic pre-image of Ψd,r is at
least
L−1
d .
i=1 i
CoPnsidering the difference of the size of the weight matrices and the dimension of the
space of multi-homogeneities leads to the notion of the expected dimension of a neurovariety.
Definition 6 (Expected dimension). The expected dimension of the neurovariety Vd,r is
L−1 d +rL−1 −1
0
edimVd,r := min d
L
+ (d id
i+1
−d i+1), d
L rL−1
.
( )
i=0 (cid:18) (cid:19)
X
This definition was introduced in [21] and in [20] it was shown that edimVd,r is an upper
bound on the dimension of the neurovariety. However, this expected dimension is not always
equal to the dimension, as demonstrated by the following example.
Example 7. The dimension of a neurovareity is subtle even in simple settings. For in-
stance, when the architecture is (d ,d ,1), a reinterpretation [20, Theorem 9] of the Alexan-
0 1
der–Hirschowitz Theorem [2] determines which neurovarieties have the expected dimension.
In [21], the authors coin the term defect for the difference between the dimension of a neu-
rovariety from its expected dimension. Examples of Vd,r with defect are found in [21, Table
1] and in [20] as well.
4Our main result proves a lower bound on the dimension of a neurovariety for sufficiently
high activation degree by resolving a conjecture in [20]. We introduce the notion of the
activation threshold, whose existence will follow from our main result Theorem 12.
Problem 8 (Activation threshold). Given an architecture d, does there exist a non-negative
integer r˜ such that the following holds:
dimVd,r = edimVd,r for all r > r˜? (3)
Definition 9. The activation threshold is the smallest r˜such that (3) holds, if it exists. We
denote the activation threshold of d by ActThr(d).
2 Expressivity for high activation degree
In this section we resolve the high activation degree conjecture of Kileel, Trager, and Bruna.
Precisely, we prove [20, Conjecture 16] thereby completing the proof of [20, Theorem 14].
The key piece of our argument incorporates the number theoretic results by Newman and
Slater in [26].
2.1 Powers of non-proportional polynomials
We generalize the result [26, Section 5, Equation (2)] of Newman-Slater to multivariate
polynomials. The proof of this is via a simple reduction from multivariate polynomials to
univariate polynomials.
Lemma 10 (Newman-Slater Corollary). Let p ,...,p ∈ C[x ,...,x ] denote multivariate
1 k 1 d
polynomials that are pairwise non-proportional (for any i 6= j, there is no α ∈ C such that
p = αp ). If
i j
pr +···+pr = 0,
1 k
then r ≤ 8(k −1)2 −1.
Proof. Let p ,...,p be given as above and fix a general line in Cm parameterized by ℓ(t).
1 k
As pr = 0, the restriction of the polynomials p to this line yields the following equality
i i i
of univariate polynomials p (ℓ(t)),...,p (ℓ(t)):
1 k
P
(p (ℓ(t)))r +···+(p (ℓ(t)))r = 0.
1 k
As the p are pairwise non-proportional and the line parameterized by ℓ(t) is general, the
i
univariate polynomials p (ℓ(t)) are also pairwise non-proportional as they share no common
i
zeros. By the result of Newman and Slater [26, Section 5, Equation (2)], we conclude that
r ≤ 8(k−1)2 −1.
We generalize Lemma 10 further—we demonstrate that given pairwise non-proportional
multivariate polynomials p ,...,p , the polynomials pr,...,pr are linearly independent for
1 k 1 k
all sufficiently large r.
5Lemma 11. LetKbeasubfieldofC. Givenintegersd,kthereexistsanintegerr˜= r˜(k)with
the following property. If r > r˜and p ,...,p ∈ K[x ,...,x ] are pairwise non-proportional,
1 k 1 d
then pr,...,pr are linearly independent (over K). Moreover, r˜= 8(k −1)2 −1 suffices.
1 k
Proof. We remark that it suffices to consider the case that K = C. Indeed, if p and p are
i j
not linearly dependent over K, then they are not linearly dependent over C. For if there
were a constant α ∈ C such that p − αp = 0, then one finds that α ∈ K by considering
i j
the coefficients of this difference. Further, linear independence of {pr,...,pr} over C implies
1 k
linear independence over any subfield.
We prove the contrapositive statement when K = C. Given integers d,k, let r˜ = 8(k −
1)2 − 1 and r ≥ r˜ be an integer. Fix pairwise non-proportional polynomials p ,...,p ∈
1 k
C[x ,...,x ] with the property that the set {pr,...,pr} is linearly dependent over C. Then
1 d 1 k
there exists α ,...,α ∈ C such that
1 k
α pr +···+α pr = 0.
1 1 k k
Let β be a rth root of α , so that
i i
(β p )r +···+(β p )r = 0.
1 1 k k
By Lemma 10, it follows that for some i 6= j, β p and β p are proportional. Thus, some p
i i j j i
and p are proportional.
j
2.2 Deep networks and high activation degree
As notedin[20], Lemma 11implies that fora fixed architecture d, there exists r˜such thatfor
any activation degree r > r˜, the neurovariety Vd,r has the expected dimension [20, Theorem
14]. That is, Lemma 11 implies the existence of the activation threshold for any architecture
d. We recount the argument given in [20] for completeness.
Theorem 12. For fixed d = (d ,...,d ) satisfying d > 1 (i = 1,...,L), the activation
0 L i
threshold ActThr(d) exists. In other words, there exists r˜ such that whenever r > r˜, the
neurovariety Vd,r has the expected dimension,
L−1
dimVd,r = edimVd,r = d
L
+ (d
i
−1)d i+1.
i=0
X
Moreover, ActThr(d) ≤ 8·(2·max{d ,...,d }−1)2 −1.
1 L−1
Proof. Fix r > r˜= 8·(2·max{d ,...,d }−1)2−1. We show by induction on the number
1 L−1
of layers L that for activation degree r, a general fiber of Ψd,r is equal to the set of multi-
homogeneities of any point of the fiber given by Lemma 5. The dimension of Vd,r is then
equal to the expected dimension.
In the case that L = 1, the neurovariety Vd,r has the expected dimension as the map Ψd,r
is an isomorphism.
6Now assume L > 1 and fix general weights W ,...,W and consider weights W˜ ,...,W˜
1 L 1 L
such that
(W σ W ...σ W )(x) = (W˜ σ W˜ ...σ W˜ )(x). (4)
L L−1 L−1 1 1 L L−1 L−1 1 1
Denote the output of (W L−1σ L−2W L−2···W 2σ 1W 1)(x) by [p w 1,...,p w dL−1] and the output
of (W˜ L−1σ L−2W˜ L−2···W˜ 2σ 1W˜ 1)(x) by [p˜w 1,...,p˜w dL−1]. Since the weights W
i
are general
and d i > 1, the homogeneous polynomials p w i are pairwise non-proportional (there is a non-
empty Zariski open set of the space of weight matrices where the outputs p w ,...,p w
1 dL−1
are non-proportional). By examining the first coordinate of the outputs of (4) we obtain the
decomposition
w pr +···+w pr = w˜ p˜r +···+w˜ p˜r (5)
L11 w
1
L1dL−1 w
dL−1
L11 w
1
L1dL−1 w
dL−1
where w and w˜ denote the (i,j) entry of the weight matrices W and W˜ respectively.
Lij Lij L L
Becausewehavetakenr > r˜, Lemma11guaranteesthat(5)hastwoproportionalsummands.
As p w are not pairwise proportional, no two summands on the left side may be proportional.
i
If two summands on the right side are proportional, we reduce the number of terms on the
right side by combining these terms. Otherwise, by permuting terms as necessary, we may
assume these proportional terms to occur as the first term on both sides. We may then scale
so that p w 1 = p˜w 1 and subtract w˜ L11p˜r w 1 to obtain
(w −w˜ )pr +···+w pr = w˜ p˜r +···+w˜ p˜r . (6)
L11 L11 w
1
L1dL−1 w
dL−1
L12 w
2
L1dL−1 wdL−1
We repeatedly apply Lemma 11 to iteratively reduce the number of terms on the right side,
until the right side of (6) is zero. Then, the pairwise linear independence of the p w and
i
Lemma 11 imply that each coefficient is also zero. Hence, up to scaling and permutation,
[p w 1,...,p w dL−1] = [p˜w 1,...,p˜w dL−1] and the entries of W L and W˜ L agree in the first row.
ThisprocessisrepeatedforeachrowofW andW˜ toshowthattheyareequal (uptoscaling
L L
and permutation). Therefore the fiber Ψ− d,1 r(Ψd,r(W 1,...,W L)) has dimension iL =− 11 d i.
P
3 Equi-width setting
The advantage of depth for neural networks is well-established through both empirical obser-
vations and theoretical results [24, 31, 23, 29]. Width also plays a significant role in network
expressivity and improves the behavior of the loss surface by smoothing the optimization
landscape of the loss function [18, 27, 28]. The study of networks with bounded width
appears in the context of universal approximation theorems [17, 19, 22] as well.
For sufficiently high activation degree, a neurovariety has the expected dimension by
Theorem 12. For specific architectures, better bounds on the activation threshold may be
computed. In this section, we focus on determining the expressivity of polynomial neural
networks with equi-width architecture, meaning the layers have equal widths, i.e., d = d =
0 1
··· = d in Definition 1.
L
We begin with a proposition which will be utilized in the main result of this section.
7Proposition 13. Let d = (d ,...,d ) be an equi-width architecture such that d := d =
0 L 0
d
1
= ··· = d
L
and let σ
i
: Rdi → Rdi be the activation map given by the coordinate-wise
exponentiation to the r-th power. A polynomial neural network
p w(x) = (W Lσ L−1W L−1···W 2σ 1W 1)(x) ∈ Vd,r
has only the trivial zero if and only if all W are invertible.
i
Proof. If each W is invertible, then the preimage of the zero vector under each W and σ
i i i
consists of only the zero vector. Thus, the polynomial neural network
p w(x) = (W Lσ L−1W L−1···W 2σ 1W 1)(x)
has only the trivial zero.
If some W is singular, then let i be the minimal index such that W is singular and
i 0 i0
v ∈ kerW be a non-zero vector. For each j < i , the linear map W is surjective, and each
i0 0 j
σ is surjective as well. Thus, there exists non-zero x ∈ Cd such that
j ∗
(σ W ···W σ W )(x ) = v.
i0−1 i0−1 2 1 1 ∗
As v ∈ kerW i0, it follows that p w(x ∗) = (W Lσ L−1W L−1···W 2σ 1W 1)(x ∗) = 0 and x
∗
is a
non-trivial zero.
We now provide the main result of this section, that the activation threshold of an equi-
width architecture is one. That is, for an equi-width architecture d, the neurovariety Vd,r
has the expected dimension for all r ≥ 2.
This is done by explicitly computing a single fiber of the parameter map Ψd,r. This
dimension provides an upper bound on the generic dimension of a fiber, and so we obtain
a lower bound on the dimension dimVd,r. This lower bound is in fact equal to our upper
bound given by the expected dimension, thereby proving the result.
Theorem 14. If d = (d ,...,d ) is an equi-width architecture with d = d = ··· = d ,
0 L 0 L
L > 1 and d > 1, then the activation threshold is ActThr(d) = 1. That is, if d is equi-width
with L > 1 and d > 1, then for all r > 1 the neurovariety Vd,r has the expected dimension
dimVd,r = Ld2 −(L−1)d.
If L = 1, then ActThr(d) = 0. That is, if d = (d,d), then dimVd,r = d2 for all r ≥ 0.
Proof. In the case that L = 1 (d = (d,d)), the neurovariety Vd,r is the space of linear
functions from Rd to Rd, which has dimension dimVd,r = d2, independent of r. Therefore,
ActThr(d) = 0.
We remark that when L > 1, ActThr(d) > 0. Indeed, if r = 1, then Vd,r is again the
space of linear functions from Rd to Rd so that Vd,r = d2. Then d > 1 implies that
dimVd,r = d2 < Ld2 −(L−1)d = edimVd,r.
We show that when L > 1 and r > 1, a specific fiber of the parameter map
Ψ
V
d,r
:
RdL×dL−1 ×···×Rd1×d0
→ Vd,r
8has dimension (L−1)d and thus ActThr(d) = 1. Consider the neural network
xrL−1
1
.
 . .  = Ψd,r(I d,...,I d).
xrL−1
 d 
 
Precisely, weshow thatthefiber
Ψ−1 (xrL−1,...,xrL−1
)istheset ofmulti-homogeneities given
d,r 1 d
in Lemma 5.
To this end, let W ,...,W be weight matrices such that
1 L
xrL−1
1
.
(W Lσ L−1W L−1···W 2σ 1W 1)(x) =  . . .
xrL−1
 d 
 
We will show by induction that the output of the i-th layer consists of scalar multiples of the
monomials
xri−1,...,xri−1
, which will imply that each W is a (invertible) scaled permutation
1 d i
matrix. Consider the output of the (L−1)-th layer,
p
w
(x)
1
.
 . .  = (W L−1σ L−2W L−2···W 2σ 1W 1)(x).
p
w
(x)
 d 
 
There is then an equality
pr (x) xrL−1
w 1 1
. .
W L . .  =  . .  (7)
pr (x) xrL−1
 w d   d 
   
Since the polynomial neural network
(xrL−1,...,xrL−1
) only has the trivial zero, Proposi-
1 d
tion 13 implies that W is invertible. Multiplying both sides of (7) by W−1 , each pr (x)
L L w i
is a linear combination of x
1rL−1,...,x drL−1
. For r > 1, this is only possible if each p w
1
is a
monomial. Indeed, no linear combination of
xrL−1,...,xrL−1
is a pure r-th power except for
1 d
scalar multiples of these individual monomials. Thus each pr is a scalar multiple of some
w
i
x
jrL−1
and each p w
i
is a scalar multiple of some x
jrL−2
.
It follows that W is a scaled permutation matrix—it is an invertible matrix that sends
L
a set of monomials to a set of scalar multiples of those monomials. As the output of the
(L−1)-th layer is then a set of scalar multiples of
xrL−2
, we inductively apply this argument
i
to show that the fiber has the form of Lemma 5.
The fiber Ψ− d,1 r(Ψd,r(I d,...,I d)) then consists of the set of multi-homogeneities and has
the expected dimension
L−1
d , which is an upper bound on the generic dimension of a
j=1 j
fiber. It follows that
P
L−1 L−1
edimVd,r = d id
i+1
− d
j
i=0 j=1
X X
is a lower bound for dimVd,r. As edimVd,r is also an upper bound, Vd,r has the expected
dimension for all r > 1.
94 Outlook
Bounding the activation threshold
Our bound on the activation threshold for polynomial neural networks is not universally
optimal. We expect that a lemma of [15] used to prove Picard theorems for holomorphic
maps into Fermat hypersurfaces will allow us to improve the bound in Theorem 12. Effective
methods in algebraic geometry to compute the activation threshold of a given architecture
also deserve to be studied. One possible approach is to formalize a connection to patterns
of linear dependence among families of polynomials. Such patterns have notable examples
from number theory independently due to A. H. Desboves [12] and Noam Elkies [10] and
were studied in [30] for a finite set of arbitrary polynomials in terms of tickets. The ticket
T (f) of a finite set of polynomials F = {f } is defined as
j
T(F) = {m ∈ N : {fm} is linearly dependent}.
j
By viewing the polynomial neural network recursively, layer by layer, it should be possible
prove results about the expected dimension by translating tickets into the polynomial neural
network setting.
Non-increasing architectures
One future direction in this research area is to determine the activation thresholds for non-
increasingarchitecturesd = (d ,...,d )—thosearchitecturessatisfyingd ≥ d ≥ ··· ≥ d .
0 L 0 1 L
By generalizing the proof of Theorem 14, one might hope to prove a conjecture of [21] that
for a non-increasing architecture d with d
L
> 1, the neurovariety Vd,r has the expected
dimension for all r > 1. That is, prove that a non-increasing architecture d has activation
threshold less than or equal to one.
Other networks
Another research direction is to introduce the notion of activation thresholds for other linear
and non-linear neural networks. Permitting negative exponents in a polynomial activation
map leads to the notion of rational neural networks [5]. There is experimental evidence
that it is beneficial to choose rational activation maps that are very low degree. Defining
activation thresholds for rational networks would provide a quantitative measure to make
these observations rigorous. Rational neural networks are currently being investigated by
Alexandros Grosdos et al. Certain linear networks may also admit the definition of an
activation threshold as a means to study expressiveness. Tropical rational neural networks
provide a lens to study networks with piecewise linear activation maps [3, 8, 9, 33]. Such
networks encompass ReLU networks; a ReLU network of fixed architecture is described by a
semialgebraic set inside the parameter space of tropical rational functions. In this context,
tropical rational functions provide a means to define activation thresholds to frame and
obtain results on expressiveness. Moreover, because the dimension of the neurovariety is
preserved under tropicalization, activation thresholds may aid in translating expressiveness
results for tropical neural networks to polynomial networks.
10References
[1] F. Albertini and E. D. Sontag. For neural networks, function determines form. Neural
networks, 6(7):975–990, 1993.
[2] J. Alexander and A. Hirschowitz. Polynomial interpolation in several variables. J.
Algebraic Geom., 4(2):201–222, 1995.
[3] M. Alfarra, A. Bibi, H. Hammoud, M. Gaafar, and B. Ghanem. On the decision bound-
aries of neural networks: A tropical geometry perspective. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 45(4):5027–5037, 2022.
[4] M. Bianchini and F. Scarselli. On the complexity of neural network classifiers: A com-
parison between shallow and deep architectures. IEEE transactions on neural networks
and learning systems, 25(8):1553–1565, 2014.
[5] N. Boull´e, Y. Nakatsukasa, and A. Townsend. Rational neural networks. Advances in
neural information processing systems, 33:14243–14253, 2020.
[6] M.-C. Brandenburg, G. Loho, and G. Montu´far. The real tropical geometry of neural
networks, 2024. arXiv:2403.11871.
[7] P. Bui Thi Mai and C. Lampert. Functional vs. parametric equivalence of relu networks.
In 8th International Conference on Learning Representations, 2020.
[8] V. Charisopoulos and P. Maragos. Morphological Perceptrons: Geometry and Training
Algorithms. Springer, 2017.
[9] V.CharisopoulosandP.Maragos. Atropicalapproachtoneuralnetworkswithpiecewise
linear activations, 2019. arXiv:1805.08749.
[10] H. Darmonand A. Granville. On the equations z = F(x,y)and Ax+By = Cz. Bulletin
of the London Mathematical Society, 27(6):513–543, 1995.
[11] S. Dey, S. Naskar, T. Mukhopadhyay, U. Gohs, A. Spickenheuer, L. Bittrich, S. Srira-
mula, S. Adhikari, and G. Heinrich. Uncertain natural frequency analysis of composite
plates including effect of noise – a polynomial neural network approach. Composite
Structures, 143:130–142, 2016.
[12] L. E. Dickson. History of the theory of numbers. Vol. I: Divisibility and primality.
Chelsea Publishing Co., New York, 1966.
[13] S. J. Fong, J. A. L. Marques, G. Li, N. Dey, R. G. Crespo, E. Herrera-Viedma, F. N. B.
Gois, and J. X. Neto. The Comparison of Different Linear and Nonlinear Models Using
Preliminary Data to Efficiently Analyze the COVID-19 Outbreak, pages 65–81. Springer
International Publishing, Cham, 2022.
[14] R. Ghazali, A. J. Hussain, and P. Liatsis. Dynamic ridge polynomial neural network:
Forecastingtheunivariatenon-stationaryandstationarytradingsignals. Expert Systems
with Applications, 38(4):3765–3776, 2011.
11[15] M. L. Green. Some Picard theorems for holomorphic maps to algebraic varieties. Amer.
J. Math., 97:43–75, 1975.
[16] J. E. Grigsby, K. Lindsey, and M. Masden. Local and global topological complexity
measures of relu neural network functions, 2024. arXiv:2204.06062.
[17] B. Hanin and M. Sellke. Approximating continuous functions by relu nets of minimal
width, 2018. arXiv:1710.11278.
[18] C. Hertrich, A. Basu, M. Di Summa, and M. Skutella. Towards lower bounds on the
depth of relu neural networks. SIAM Journal on Discrete Mathematics, 37(2):997–1029,
2023.
[19] P. Kidger and T. Lyons. Universal approximation with deep narrow networks. In
Conference on learning theory, pages 2306–2327. PMLR, 2020.
[20] J. Kileel, M. Trager, and J. Bruna. On the expressive power of deep polynomial neural
networks. Advances in neural information processing systems, 32, 2019.
[21] K. Kubjas, J. Li, and M. Wiesmann. Geometry of polynomial neural networks, 2024.
arXiv:2402.00949.
[22] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive power of neural networks:
a view from the width. In Proceedings of the 31st International Conference on Neural
Information Processing Systems, NIPS’17, page 6232–6240, Red Hook, NY, USA, 2017.
Curran Associates Inc.
[23] H. N. Mhaskar and T. Poggio. Deep vs. shallow networks: An approximation theory
perspective. Analysis and Applications, 14(06):829–848, 2016.
[24] G. Montu´far, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of
deep neural networks. In Proceedings of the 27th International Conference on Neural
Information Processing Systems - Volume 2, NIPS’14, page2924–2932,Cambridge, MA,
USA, 2014. MIT Press.
[25] S. C. Nayak and B. B. Misra. Estimating stock closing indices using a ga-weighted
condensed polynomial neural network. Financial Innovation, 4:1–22, 2018.
[26] D. Newman and M. Slater. Waring’s problem for the ring of polynomials. Journal of
Number Theory, 11(4):477–487, 1979.
[27] Q. Nguyen and M. Hein. Optimization landscape and expressivity of deep CNNs. In
J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on
Machine Learning, volume80ofProceedings of Machine Learning Research, pages3730–
3739. PMLR, 10–15 Jul 2018.
[28] Q. Nguyen, M. C. Mukkamala, and M. Hein. Neural networks should be wide enough
to learn disconnected decision regions. In J. Dy and A. Krause, editors, Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of
Machine Learning Research, pages 3740–3749. PMLR, 10–15 Jul 2018.
12[29] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein. On the expressive
power of deep neural networks. In international conference on machine learning, pages
2847–2854. PMLR, 2017.
[30] B. Reznick. Patterns of dependence among powers of polynomials. In Algorithmic
and Quantitative Aspects of Real Algebraic Geometry in Mathematics and Computer
Science, 2001.
[31] M. Telgarsky. benefits of depth in neural networks. In V. Feldman, A. Rakhlin, and
O. Shamir, editors, 29th Annual Conference on Learning Theory, volume 49 of Proceed-
ings of Machine Learning Research, pages 1517–1539, Columbia University, New York,
New York, USA, 23–26 Jun 2016. PMLR.
[32] M. Yavartanoo, S.-H. Hung, R. Neshatavar, Y. Zhang, and K. M. Lee. Polynet: Poly-
nomial neural network for 3d shape recognition with polyshape representation. In 2021
International Conference on 3D Vision (3DV), pages 1014–1023. IEEE, 2021.
[33] L. Zhang, G. Naitzat, and L.-H. Lim. Tropical geometry of deep neural networks.
In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on
Machine Learning, volume80ofProceedings of Machine Learning Research, pages5824–
5832. PMLR, 10–15 Jul 2018.
13