LogogramNLP: Comparing Visual and Textual Representations
of Ancient Logographic Writing Systems for NLP
DanluChen1,FredaShi2,AditiAgarwal1,JacoboMyerston1,TaylorBerg-Kirkpatrick1
UCSanDiego1,UniversityofWaterloo2
danlu@ucsd.edu
Abstract Input Output
Standard natural language processing (NLP) Photo Translation
?
pipelinesoperateonsymbolicrepresentations Visual Parse Tree
of language, which typically consist of se- Handcopy ? M Dating
quencesofdiscretetokens. However,creating OCR
…
ananalogousrepresentationforancientlogo- Unicode ? ? 弃 慮
graphicwritingsystemsisanextremelylabor- Textual Conventional
intensive process that requires expert knowl- Latin ? ? ? ? ? ? ? ? Visual (novel)
edge.Atpresent,alargeportionoflogographic
data persists in a purely visual form due to
Figure 1: Illustration of the processing flow of Old
theabsenceoftranscription—thisissueposes Chinese (in Bamboo Script), an ancient logographic
a bottleneck for researchers seeking to apply
language, best viewed in color. M denotes the pre-
NLPtoolkitstostudyancientlogographiclan-
trainedmodelusedinthepipeline.Vision-basedmodels
guages: most of the relevant data are images
directlyprocessvisualrepresentations(violet;dashed
ofwriting. Thispaperinvestigateswhetherdi-
lines). ConventionalNLPpipelines(blue;solidlines)
rectprocessingofvisualrepresentationsoflan-
firstconvertvisualrepresentationsintosymbolictext—
guageoffersapotentialsolution. Weintroduce
eitherautomatically,whichisquitenoisy,ormanually,
LogogramNLP,thefirstbenchmarkenabling
which is labor-intensive. However, as shown, some
NLPanalysisofancientlogographiclanguages,
ancient logographic writing systems have symbol in-
featuringbothtranscribedandvisualdatasets
ventoriesthathavenotyetbeenfullymappedintoUni-
forfourwritingsystemsalongwithannotations
code. Even when Unicode codepoints exist, they are
for tasks like classification, translation, and
oftenmutuallyexclusivewiththesymbolinventoriesof
parsing.Ourexperimentscomparesystemsthat
high-resourcelanguages,reducingtheeffectivenessof
employrecentvisualandtextencodingstrate-
transferringfrompre-trainedmodels. Finally,latiniza-
giesasbackbones.Theresultsdemonstratethat
tion (a potential solution for finding common ground
visualrepresentationsoutperformtextualrep-
withpre-traininglanguages)losesinformationfromthe
resentations for some investigated tasks, sug-
originalinput,isnotfullystandardized,andisdifficult
gesting that visual processing pipelines may
toautomate.
unlockalargeamountofculturalheritagedata
oflogographiclanguagesforNLP-basedanal-
yses. Dataandcodeareavailableathttps: in which individual symbols represent entire se-
//logogramNLP.github.io/.
manticunitslikemorphemesorwords.
ThechallengesassociatedwithNLPforancient
1 Introduction
logographiclanguagesmainlycomefromtwoas-
Theapplicationofcomputationaltechniquestothe pects. First, for many ancient languages, most
studyofancientlanguageartifactshasyieldedex- availabledatasourcesareinvisualforms,consist-
citingresultsthatwouldhavebeendifficulttoun- ing of untranscribed photographs or hand-drawn
cover with manual analysis alone (Assael et al., copies (i.e., lineart). Adopting the conventional
2022). Unsurprisingly, one of the biggest chal- NLP pipeline, which requires converting visual
lenges in this domain is data scarcity, which, in representationsintosymbolictext,isthereforenot
turn,meansthattransferringfrompre-trainedsys- straightforward: automatictranscriptionsareoften
tems on well-resourced languages is paramount. noisyduetodatascarcity,whilemanualtranscrip-
However, it is more challenging to adopt similar tionsarelabor-intensiveandrequiredomainexper-
techniquesforancientlogographicwritingsystems, tise. Some logographic writing systems, such as
4202
guA
8
]LC.sc[
1v82640.8042:viXraOldChinese,evenincludesymbolinventoriesthat glyphic, Cuneiform, and Bamboo Script), along
remainnotfullymappedtoUnicode(depictedin with annotations for fine-tuning and evaluating
Figure1). downstreamNLPsystemsonthreetasks,including
Second, even when perfect Unicode transcrip- threeattributeclassificationtasks,machinetransla-
tionsareavailable,theirsymbolinventoriesareof- tion,anddependencyparsing.
tenmutuallyexclusivewiththoseofhigh-resource Weconductexperimentsontheselanguagesand
languages,whichcansubstantiallyreducetheeffec- tasks with a suite of popular textual and visual
tivenessoftransferfrompre-trainedmultilingual encoding strategies. Surprisingly, visual repre-
encoders, such as mBERT (Devlin et al., 2018). sentations perform better than conventional text
Oneprocessingstepthatmightbeusedtomitigate representationsforsometasks(includingmachine
thisissueislatinizationoftheUnicodetranscripts translation),likelyduetovisualencodingallowing
(Rust et al., 2021; Muller et al., 2020). However, forbettertransferfromcross-lingualpre-training.
itischallengingtoLatinizelogographiclanguages Theseresultshighlightthepotentialofvisualrep-
duetouncertainpronunciations(SproatandGutkin, resentationprocessing,anovelapproachtoancient
2021) and the resulting inconsistent latinization languageprocessing,whichcanbedirectlyapplied
schemes across artifacts from the same language toalargerportionofexistingdata.
andwritingsystem. Suchaprocessislaborious—
humanistsmaydevotemonthsorevenyearstode- 2 Dataset: Languages,Tasksand
terminethecorrecttransliteration. Incontrast,once Challenges
acorrecttransliterationisdetermined,translation
intoanotherlanguagemayonlytakeminutes. Ourbenchmarkconsistsoffourrepresentativean-
cientlanguages—LinearA,Egyptianhieroglyphic,
Fortunately,advancesinvisualencodingstrate-
Cuneiform,andBambooscript(§2.1).1 Eachlan-
gies for NLP tasks offer an alternative solution.
guageisassociatedwithauniquewritingsystem
Recent studies have investigated NLP systems
and unique challenges. We refer the readers to
that model text in the pixel space (Rust et al.,
Appendix A for data collection and cleaning de-
2023;Tschannenetal.,2023;Saleskyetal.,2023),
tails. Ourbenchmarkcoversthreetasks: machine
therebyopeningnewpossibilitiesforthedirectuse
translation,dependencyparsing,andattributeclas-
of visual representations of ancient logographic
sification(§2.2).
writingsystems. Theseapproaches,todate,have
primarilybeenappliedtodigitallyrenderedtexts.
2.1 LogographicLanguages
They have not yet been extensively evaluated on
handwrittentexts,suchaslineart,i.e.,neatlyhand-
Amajorcharacteristicoflogographiclanguagesis
copiedversionsoftextsbyscholars.
thatthesizeofsymbolinventoriesissignificantly
Inthispaper,weattempttoanswerthefollowing larger than that in alphabetic languages such as
questions: (1)CanweeffectivelyapplyNLPtoolk- AncientGreek(24letters)orModernEnglish(26
its,suchasclassifiers,machinetranslationsystems, letters). Asummaryofdifferentrepresentationsof
andsyntacticparsers,tovisualrepresentationsof thelanguagesofourinterestisshowninFigure2,
logographicwritingsystems? (2)Doesthisstrategy andTable2summarizesthecurrentstatusofeach
allowforbettertransferfrompre-trainedmodels language.
andleadtobetterperformance? Additionally,as
shown in Figure 1, many logographic languages LinearA. LinearAisanundecipheredlanguage
havemultiplepartiallyprocessedrepresentations, used by the Minoan at Crete and is believed to
includingartifactphotographs,hand-copiedlineart, be not related to ancient Greek. Scholars have
Unicode,Latintransliteration,andnormalization— differentiatedtheglyphsandcarefullyhand-copied
wealsoaimtoempiricallyinvestigatetheextentto them into linearts. We collected a dataset of 772
whichvariousrepresentationsateachstage,includ- tablets(i.e.,manuallydrawing)fromSigLA.2Each
ingtextualandvisualmodalities,facilitateeffective tablet also has a separable glyph with annotated
fine-tuningofdownstreamNLPsystems. Unicode.
WehavecuratedLogogramNLP,abenchmark
1BambooscriptsusuallycombineSealscriptsandClerical
consisting of four representative ancient logo-
scripts.
graphicwritingsystems(LinearA,Egyptianhiero- 2https://sigla.phis.me/browse.htmlVisualFeature TextualFeature Task
Writingsystem Language abbr.
FullDoc Textline Unicode latinization Translation UDParsing Attribute
LinearA Unknown LNA Y Y Y Y
Egyptianhieroglyph AncientEgyptian EGY Y Y Y Y
Cuneiform Akkadian&Sumerian AKK Y Y Y Y Y∗ Y
Bambooscript AncientChinese ZHO Y Y Y Y∗ Y∗
Table 1: A summary of the task availability across four ancient languages with unique writing systems. The
underlinedYindicatesthatthedatahasnotpreviouslybeenusedinamachinelearningsetup,whichdemonstrates
thenoveltyofourbenchmark;andasterisks(∗)indicatethatweconductedextramanuallabeling.
labor-intensity: labor-intensity: labor-intensity:
expertise level: expertise level: expertise level:
Photograph Image Textline Unicode Latinization Translation
Parse Tree
Linear A " # ! qe ra2 u ki ro *79 su
Dating
Egyptian Heiroglyph ! # $ sDm=f hnw m r n tA-wr hAkr grH n sDr.t …
_igi_ ka-bi-sum2
Cuneifrom !"#$%%&'
Not within our
Bamboo Script O(絕)-智(知)-棄-O(辩) N/A proposed dataset
Figure2: Exampleoffourlogographiclanguageswithdifferentrepresentationformats. Thearrowshowsthetypical
processingflowofancientlanguagesbyhumanists. Theworkloadandexpertiserequiredtotranscribethetextfrom
imagesisevengreaterthanthatofdownstreamtaskssuchasmachinetranslation. TheredcircleO(inBamboo
Script)indicatesthecharacterisnotdigitizedasUnicodeyet. GreendashedboxesnotethatUnicodeexistsfor
EgyptianhieroglyphicsandLinearA,butthealignmenttodocumentsisunavailable;thesamegoesforEgyptian
andLinearAphotographs.
status LNA AKK EGY ZHO GRC 2015)3 usingarule-basedsegmenter,andobtained
deciphered None Most Most Most All 891examplesofparalleldata. Additionally,wecol-
differentiated Most Most Most Most All lecteddatafromtheThotSignList(TSL;English
encoded Most Most Some Some All translation)4 andBBAW(Germantranslation)5 for
Latinized All All All None All
2,337and100,736samplesofparalleldata,respec-
tively. However,thetransliterationstandardsdiffer
Table 2: Summary of the status of the ancient logo-
graphic languages presented in our paper. The status amongthesethreesourcesofdata,andBBAWdoes
ismeasuredfromtheperspectiveofpaleography. We notincludehieroglyphimagefeatures. Therefore,
putAncientGreek(GRC),awell-knownancientnon- weonlyusedTSL’sdata.
logographiclanguage,hereforcomparison.
Old Chinese (Bamboo script). We collected
13,770piecesofbambooslipsfromKaom,6 which
Akkadian(Cuneiform). CuneiML(Chenetal.,
comewiththephotographofeachlineofthetext.
2023) is a dataset that contains 36k entries of
TheBaoshancollectioncoversthreegenres: Wen-
cuneiformtablets. EachtabletconsistsofUnicode
shu(Document),Zhanbu(Divine),andBook. The
Cuneiform, lineart, and transliteration. We also
Guodiancollectioncontainsparalleldatatranslated
use the Akkadian Universal Dependencies (UD)
intomodernChinese. Thevocabularysizeis1,303.
dataset(Luukkoetal.,2020),whichcontains1,845
Notably,about40%ofthecharactersdonothavea
sentenceswithdependencyannotations. Sincethe
Unicodecodepointandare,therefore,represented
UDannotationofAkkadianonlykeepsthenormal-
as place-holder triangles or circles. This dataset
ization form of the language, we obtain the Uni-
code by running a dynamic programming-based 3https://mjn.host.cs.st-andrews.ac.uk/
egyptian/texts/corpus/pdf/
matchingalgorithm.
4https://thotsignlist.org/
5https://aaew.bbaw.de/tla/servlet/
AncientEgyptian(Hieroglyph). Wesegmented
TlaLogin
the St Andrews Corpus (Nederhof and Berti, 6http://www.kaom.net
! !⭐ !⭐ ⭐ ⭐ !⭐ !⭐doesnotcomewithhuman-labeledlatinizationdue encoded in Unicode, but there is no standard en-
tothelackoftransliterationstandards. coding for “stacking” multiple glyphs vertically
(Figure3). Therefore,wedonotincludetheUni-
2.1.1 VisualRepresentations
codetextforourancientEgyptiandataastheyare
Sinceancientscriptsdidnotconsistentlyadhereto notavailable.
aleft-to-rightwritingorder,breakingdownmulti-
line documents into images of single-line text is 2.2 Tasks
nontrivial. These historical data, therefore, need Ourbenchmarkcoversthreetasks(Table1): trans-
additionalprocessingtobemachine-readable. Fig- lation, dependency parsing, and attribute classi-
ure3showsexamplesofdifferentprocessingstrate- fication. The model performance on these tasks
gies. We summarize the approaches we used in reflects various aspects of ancient language un-
buildingthedatasetasfollows: derstanding. Tobetterunderstandtheinformation
1. Raw image (no processing): the raw images loss when using a pipeline approach, we also re-
are already manually labeled and cut into text portperformanceusingthismethod: predictingthe
lines of images, and no extra processing is re- transliteration first and using the noisy predicted
quired. transliterationfordownstreamtasks.
2. Montage: wegeneratearowofthumbnailsof
Machinetranslation. Thetaskistotranslatethe
eachglyphusingthemontagetoolinImageMag-
ancientlanguages,representedbyeithertextorim-
ick.7 ThisstrategyisusedforLinearA,asthe
ages,intomodernlanguages,suchasEnglish. Inall
originaltextsarewrittenonastonetablet,and
ofourexperiments,wetranslateancientlanguages
scholarshavenotdeterminedthereadingorder-
intoEnglish.
ingofthisunknownscript.
3. Digitalrendering: wedigitallyrenderthetext Dependency parsing. Given a sentence in the
using computer fonts when the language is al- ancientlanguage,thetaskistopredictthedepen-
ready encoded in Unicode. Given that most dencyparsetree(Tesnière,1959)ofthesentence.
ancientlogographicscriptsarestillundergoing In the dependency parse tree, the parent of each
thedigitizationprocess,thisoptioniscurrently wordisitsgrammaticalhead.
unavailableexceptforCuneiform.
Attributeclassification. Thetaskistopredictan
2.1.2 TextualRepresentations attributeofthegivenartifact,forexample,prove-
Theprocessingoftextualfeaturesforancientlogo- nience(foundplace),timeperiod,orgenre.
graphicscriptsalsorequiresspecialattention. Un-
3 Methods
likemodernlanguages,ancientlogographicwriting
systems can have multiple latinization standards
Inthissection,wewilldescribefeatureencoding
orlackuniversallyagreed-upontranscriptionstan-
methods(§3.1)forbothvisualandtextualinputs,
dards. For example, the cuneiform parsing data
aswellastask-specificlayers(§3.2)foreachtask
isnotinstandardtransliteration(ATF)8 form,but
weconsider.
rather,intheUDnormalizedform. Thismismatch
introducesextradifficultytodownstreamtasks,es- 3.1 FeatureEncoding
peciallyinlow-resourcesettings.
NLP for Low-resource languages has benefitted
AsimilarissuealsoexistsforOldChinese: most
a lot from pre-trained models. However, modern
ancientcharactersdonotevenexistinthecurrent
pre-trained models do not cover the character in-
Unicodealphabet. Whilewemayfindsomemod-
ventories of the considered ancient logographic
ernChinesecharactersthatlooksimilartothean-
languages. Toovercomethisshortage,wesumma-
cient glyphs, they are usually not identical, and
rize solutions to the problem into four categories
such a representation loses information from the
anddescribethemasfollows.
originaltext.
For Egyptian hieroglyphs, most characters are Extendingvocabulary. Inthislineofapproach
(Wang et al., 2020; Imamura and Sumita, 2022),
7https://imagemagick.org
thevocabularyisextendedbyaddingtheunseento-
8ATF is a format used to represent cuneiform text.
kens. Theembeddingsofnewtokenscanbeeither
More details can be found at http://oracc.ub.
uni-muenchen.de/doc/help/ initialized randomly or calculated by a function.Egyptian hierograph (digital figure) Stacking ! # $ Linear A (montage) Linear A (tablet)
Bamboo script (photo)
Cuneiform (digital rendering) concatenate
glyphs in a row
Bamboo script (handcopy) Not in Unicode ! # $ % & &
U1202d U121a0 U12038 U12197 U12363 U12363
Figure3: Imagefeaturesoffourancientwritingsystems. (1)EgyptianhieroglyphsandBambooscriptsarealready
manuallysegmentedintoimagesoflines.InthehandcopyversionoftheBambooscript,thewordwithinparentheses
indicatesthecorrespondingmodernChineseglyph. AlthoughboththeEgyptianandBambooscriptimagesappear
tobeinadigitalfont,theyareonlyaccessibleasimageswithoutunderlyingcodepointmappingstoUnicode. (2)
LinearAtabletsarebelievedtobewritteninhorizontallinesrunningfromlefttoright(Salgarella,2020);therefore,
weusethemontageconcatenationofeachglyphastherepresentation. (3)WedigitallyrenderCuneiformUnicode
usingcomputerfontasthevisualrepresentation.
In the fine-tuning stage, the embeddings of new ear A and Cuneiform), we can encode the full-
tokens are updated together with the rest of the document images directly. We use ResNet-50
model. (He et al., 2016) as the backbone model for full-
documentimageinputs.
Latintransliterationasaproxy. Themajority
ofpastworkoncross-lingualtransferhasfocused
onusingLatintransliterationastheproxytotrans- 3.2 Task-SpecificLayers
ferknowledgefromhigh-resourcetolow-resource
languages (Pires et al., 2019; Fang et al., 2020). Machinetranslation. Afterencodingtheinput
Followingthislineofwork,weinputlatinization tovectors,machinetranslationrequiresadecoder
representationstomBERT(Devlinetal.,2018)to to generate sequential outputs. Encoder-decoder
obtaintheembeddingsoftheancientlanguages. models, such as T5 (Raffel et al., 2020), ByT5,
PIXEL-MT, and BPE-MT (Salesky et al., 2023),
Tokenization-free. Theideaofthetokenization-
use 3/6/12 layers of Transformer blocks as the
free approach is to view tokens as a sequence of
decoders. For Encoder-only models, such as
bytes and directly operate on UTF-8 codepoints
(m)BERTorPIXEL,weattachaGPT2model(Rad-
without an extra mapping step. As representa-
fordetal.,2019)asthedecodertoproducesequen-
tive models, ByT5 (Xue et al., 2022) and CA-
tialoutput. Amongtheaforementionedmodels,T5,
NINE (Clark et al., 2022) use Unicode encod-
ByT5, and PIXEL are pre-trained on large-scale
ingofastringtoresolvethecross-lingualout-of-
textcorporasuchastheCommonCrawl;PIXEL-
vocabulary issues. This work uses ByT5 for ma-
MT and BPE-MT are pre-trained on 1.5M pairs
chinetranslationandCANINEforclassification.
ofsentencesof59modernlanguages;PIXEL-MT
isanencoder-decodermodelwitha6-layerTrans-
PixelEncoderforText. Recently,therehasbeen
formerencoderanda4-layerTransformerdecoder.
anovelapproach(Rustetal.,2023)thataimstore-
solvethedisjoint-character-setproblembyrender-
ingtextintoimagesandthenapplyingastandard Classification. We attach a two-layer ReLU-
image encoder, such as the Vision Transformer activatedperceptron(MLP)withahiddensizeof
withMaskedAutoencoder(ViT-MAE)(Heetal., 512totheencoderforallclassificationtasks. The
2022),toencodethefeatures. Inthiswork,weuse MLP outputs the predicted distribution over the
PIXEL(Rustetal.,2023),apixel-basedlanguage candidateclasses.
modelpre-trainedontheCommonCrawldataset
withamaskedimagemodelingobjective,toencode
Dependency Parsing. After encoding, we use
the visual text lines for ancient languages. Addi-
the deep bi-affine parser (Dozat and Manning,
tionally,weusePIXEL-MT(Saleskyetal.,2023),a
2017) for dependency parsing, which assigns a
pixel-basedmachinetranslationmodelpre-trained
scoretoeachpossibledependencyarcbetweentwo
on59languages,forthemachinetranslationtask.
words. Weusetheminimumspanningtree(MST)
FullDocumentImageEncoding. Whentheim- algorithmduringinferencetofindthebestdepen-
agesofancientartifactsareavailable(e.g.,forLin- dencytreeforeachsentence.
egami
war
.1
egatnom
.2
gniredner
.3Task Model BSZ Steps LR BLEU score across the three languages, outper-
formingthesecond-bestmethodbyalargemargin.
translation visual 64 30,000 5e-4
Modelswithpre-trainingdonotalwaysoutper-
translation textual 56 30,000 5e-4
form those trained from scratch (Gutherz et al.,
translation byT5 64 100,000 1e-3
classification visual/textual 256 30,000 5e-4 2023). We find that all models that take textual
parsing visual/textual 256 1,000 8e-5 (Unicodeorlatinized)inputachieveworseperfor-
mancethanmodelstrainedfromscratchwiththe
Table 3: Hyperparameter configuration. Note that, sametypeoftextualinput,suggestingthatthelack
byT5isparticularlyhardtoconvergecomparedtoother
of overlap in symbol inventories poses a serious
transformer-basedmodels. Fortheparsingtask,dueto
problem for cross-lingual transfer learning. Our
thelow-resourcenatureoftheparsingdata,1,000steps
resultsindicatethatchoosingthecorrectinputfor-
aresufficienttoachievemodelconvergence.
mat is crucial to achieving the full advantage of
pre-training.
4 ExperimentsandAnalysis Inaddition,thePIXEL-MTmodel,pre-trained
onpaireddatainmodernlanguages(TED59),sig-
We describe our general model fine-tuning ap-
nificantlyoutperformsPIXEL+GPT2(pre-trained
proach in §4.1 and analyze model performance
withmaskedlanguagemodeling)acrosstheboard.
ontheaforementionedtasksinthesucceedingsub-
Another model, BERT-MT, which is further pre-
sections.
trained on the same parallel text (TED59) with
BERTinitialization,alsoachievescomparableper-
4.1 GeneralExperimentalSetup
formance. Theseresultsemphasizetheimportance
We use the Huggingface Transformers library ofpre-trainingonmodernpaireddata,empirically
(Wolfetal.,2020)inallexperiments,exceptforma- suggesting that the PIXEL encoder with parallel
chinetranslation,whereweusethePIXEL-MTand textpretrainingisaneffectivecombinationforan-
BPE-MTmodels.9 Wemodifiedcodeandmodel cientlogographiclanguagetranslation.
checkpointsprovidedbySaleskyetal.(2023)based
onfairseq(Ottetal.,2019)forthetwoexceptions.
[Pred] Confucius said: Those who lead the people will be
WeuseAdam(KingmaandBa,2015)astheop-
good at holding on to the superior.
timizerforallmodels,withaninitiallearningrate [Ref ] Confucius said: Those above are fond of
"benevolence", …
specifiedinTable3. Weuseearlystoppingwhen
thevalidationlossfailstoimprovefortenevalua- [Pred] At the beginning of my kingship, in my first regnal
tionintervals(1000iterationperinterval). Fordata year, in the fifth month when I sat on the royal throne, (the
god) Assur, my lord, encouraged me and I gave (them) to the
withoutastandardtestset,werunafixednumber
Hamranu, the Luhutu, Hatalu, Rapiqu, Rapiqu, Rapiqu,
of training iterations and report the performance Nasiru, Gulasi, Nabatu, …
on the validation set after the last iteration. All [Ref ] At the beginning of my reign, in my first palu, in the
fifth month after I sat in greatness on the throne of kingship,
experiments are conducted on an NVIDIA-RTX
(the god) Assur, my lord, encouraged me and I marched
A6000GPU,andthetrainingtimerangesfrom2 against (the Aramean tribes) Hamaranu, Luhu`atu, Hatallu,
Rubbu, Rapiqu, Hiranu, (5) Rabi-ilu, Nasiru, Gulusu, Nabatu,
minutes to 50 hours, depending on the nature of
thetaskandthesizeofthedatasets. Unlessother- [Pred] after Hes Majesty had as to the Shesmet who
wise specified, all parameters, including those in satisfies this August, Sopu, the Lord of the East.
[Ref ] after His Majesty had come to Shesmet while
pre-trainedmodels,aretrainablewithoutfreezing.
satisfying this august god, Sopdu, the lord of the East
WesummarizeotherconfigurationsinTable3.
Figure4: Casestudyformachinetranslationusingthe
4.2 MachineTranslation
PIXEL-MT model. Notably, there are many spelling
errorsinthepredictions,particularlywithuncommon
Wecomparetheperformanceofthemodelsonma-
namedentities.
chinetranslation,wherewetranslateancientEgyp-
tian (EGY), Akkadian (AKK), and Old Chinese
Qualitativeanalysis. AsshowninFigure4,the
(ZHO) into English (Table 4a). We find that the
lowBLEUscoresforZHO-ENtranslationisare-
PIXEL-MT model consistently achieves the best
sultofthetranslationmodelfailingtocapturethe
meaningoftheinput,insteadfocusingonrepeated
9TheprefixPIXELorBPEalsoindicatesthetypeofinput
representationthemodeluses. formatting queues: e.g., “Confucius said:
NE-OHZ
NE-KKA
NE-YGE(a)Machinetranslation(BLEUscore)
Pre-trained? SourceLanguage
Modality Tokenization Input Model
MLM MT EGY AKK ZHO
Dataset size (# lines) 2,337 8,056 500
Visual token-free textline PIXEL+GPT21 (cid:33) ✗ 2.83 7.51 1.14
Visual token-free textline PIXEL-MT ✗ (cid:33) 29.16 44.15 5.45
Textual BPEw/extvocab2 Unicode T5 (cid:33) ✗ n/a 12.42 0.28
Textual byte-level Unicode ByT5 (cid:33) ✗ n/a 4.51 0.53
Textual char-level Unicode Conv-s2s ✗ ✗ - 36.52∗ -
Textual BPE Unicode BPE-MT ✗ (cid:33) 23.26 36.18 1.32
Textual BPE Latin T5 (cid:33) ✗ 21.18 10.67 n/a
Textual char-level Latin Conv-s2s ✗ ✗ - 37.47∗ -
(b)Attributeprediction(F accuracy)
1
Modality Tokenization Input Model LNA AKK EGY ZHO
geo time genre geo time genre
Number of classes 7 16 12 24 14 3
Dataset size (# examples) 772 36,454 36,454 36,454 1,320 302
Majority 14.28 6.25 8.33 4.17 7.14 33.33
Visual token-free photo ResNet 8.24 75.02 45.45 62.99 n/a n/a
Visual token-free textline PIXEL 16.56 72.91 50.84 61.44 16.24 52.17
Textual BPEw/extvocab2 Unicode BERT n/a 0∗∗ 0∗∗ 0∗∗ n/a 74.85
Textual BPE Unicode BERT n/a 72.40 50.85 63.70 n/a 90.30
Textual byte-level Unicode CANINE n/a 82.83 47.88 56.42 n/a 96.43
Textual BPE Latin BERT 32.92 80.91 53.45 65.10 34.71 n/a
Textual BPE Latin mBERT 50.52 83.08 56.71 66.33 36.25 n/a
Table4: (a)Resultsonmachinetranslation(fromeachofthesourcelanguagestoEnglish),intermsofBLEUscores.
MLMdenotesmodelspretrainedonunsuperviseddatawiththemaskedlanguagemodel(MLM)loss,whileMT
denotesmodelspretrainedwithsupervisedparalleldata(TED59). (b)MacroF scoresforattributeprediction.
1
∗: numberstakenfromGutherzetal.(2023),wheretheirmodelsaretrainedfromscratch,i.e.,withoutpretraining.
∗∗: The character set is 100% disjointed without extending the vocabulary of the model, resulting in zero F
1
scores. 1: This model is trained using PIXEL as the encoder and GPT2 as the decoder, with linear projection
layerstoconvertthefinallayerofPIXELintoaprefixinputforGPT2. 2: Thismodelistheonlyoneexperiencing
out-of-vocabulary(OOV)issueswithUnicodeinput. Toaddressthis,weextendedthevocabularywithrandom
initialization. n/a: indicatestherepresentationofaspecificlanguagedoesnotexistinourbenchmark.
Those who...” Indeed, given that the topical wellforsomeoftheseattributeclassificationtasks
domain of the ZHO-EN translation data is philo- asmanyoftherelevantfeaturesarevisual(e.g.,for
sophicalwriting,achievinganaccuratetranslation timeandlocation);but,arenotgenerallyaseffec-
wouldbechallengingevenwithamuchlargerset tiveastextualinputrepresentations. Bycomparing
of parallel translations. For AKK-EN, we found BERT with latinized input and CANINE on Uni-
thattheoverallqualitytobequitegood,despitethe code,wefindthatwhenbothaccuratelatinization
factthaterrorsintranslatingnamedentitiesappear andUnicoderepresentationsareavailable,latiniza-
more often than in standard MT tasks. This case tionisthemostinformativefeature—withtheex-
studysuggeststhattranslationperformancecould ceptionoftimeperiodclassificationforAkkadian.
improvefurtherifwetrainingusingacustomtarget Thisexceptionisalignedwithourunderstanding
language(English)vocabulary. Wealsoshowmore ofAkkadian,asdifferentCuneiformcharactersare
generatedexamplesfromthePIXEL-MTmodelin used across different time periods. Thus, in this
AppendixC. case, Unicode can provide more clues for deter-
miningthetimeperiodofasample. Notethatthe
4.3 AttributeClassification
labeldistributionisnotbalancedformostancient
Table4bsummarizestheperformanceofattribute languageattributiontasks. Formoredetails,refer
classification with different features and models. toChenetal.(2024).
As expected, the image features can work fairlyModality Model Input RIAO MCONG of 2.09, whereas handcopied figures, which typ-
Dataset Size (# tokens) 5k 130k ically provide clearer and more consistent visual
data,resultinahigherBLEUscoreof5.45. Thisre-
Visual PIXEL Image 92.74 85.22
sultsuggeststhatformodelsthatperformimplicit
Textual BERT Latin 92.13 83.88
OCRaspartofthetranslationprocess,theclarity
Table5: DependencyparsingresultonAkkadian(eval- ofthesourcematerialisparamount.
uatedontheUDcorporaRIAOandMCONG),interms
oflabeledattachmentscores(LAS).Notethatthenum- 5.2 TextRecognitionStudy
beroftokensarereported.
Wesimplifythetaskoftranscribingancienttexts
bystartingwithlinesoftextthathavebeenaccu-
4.4 DependencyParsing ratelysegmented. Fordatasetsthatincludeglyph-
levelannotations,weemployglyphclassification
Wecomparethedependencyparsingperformance
torecognizethetext. Detailsonmodelsandconfig-
of models with visual and textual encoders (Ta-
urationofline-levelOCRandglyphclassification
ble5).10 Whileallmodelsachievequitehighpars-
canbefoundinAppendixB.
ing accuracy, we find that models with visual en-
coders perform the best on both investigated cor-
Method Output LNA EGY AKK ZHO
pora(RIAOandMCONG).Duringtraining,mod-
OCR Unicode 57.17 N/A 5.72 71.85
els taking visual input generally converge faster
OCR Latin 63.44 65.88 21.98 N/A
thantheirtextualcounterparts,whichisinlinewith
prior work (Salesky et al., 2023) that uses visual
Table 7: Line-level OCR results with the best valida-
featuresformachinetranslation. tion character error rate (CER) reported. The study
includesvariouswritingsystemsusingKrakentrained
5 AblationStudyonOCRandImage fromscratchonsegmentedtextlines. N/A:eitherthe
Quality UnicodeorLatinversionofthetextisnotavailable.
As mentioned earlier, the majority of data from
ancient times remain in the form of photographs.
103
Wefirstcloselyexaminetwodifferentvisualinput
80
representations for the ZHO-EN translation task,
handcopied figure and photograph (§5.1). Next, 102 60
we examine OCR performance on ancient logo-
graphiclanguagestogainbetterunderstandingof 40 101
thisbottleneckforcurrentNLPpipelines(§5.2). glyph classification
CTC loss
20
5.1 Handcopyv.s. RawImage 0 250 500 750 1000 1250
Number of glyphs |G|
Figure5: GlyphclassificationonOldChinese(ZHO).
Inputrepresentation BLEU
Leftaxis: weplottheerrorrateofglyphclassification.
photograph 2.09 Thedatapointat|G|=50showstheclassificationerror
handcopiedfigure 5.45 calculatedusingthetop50mostfrequentglyphsinthe
dataset. Thepurplehorizontalline(71.85%)represents
Table6: PerformanceonZHO-ENtranslationusingthe theline-leveltextrecognitionCERforZHO,provided
PIXEL-MTmodelwithdifferentvisualinputfeatures. for reference. Right axis: The frequency count (in
orangebars)ofeachglyphinthedataset. Notethatthe
For the ZHO-EN translation data, we have ac- countsareinlogarithmicscale,illustratingthelongtail
distributionofglyphcounts.
cesstobothphotographsofthebambooslipsand
handcopiedtextlinefigures(seetheBambooscript
exampleinFigure3forreference). AsshowninTa- Results. Theline-levelOCRperformanceforthe
ble6,thequalityofthevisualfeaturessignificantly fourlanguagesispresentedinTable7. Whencom-
influences the translation accuracy—translations paringdigitalrenderingsoftexttohandwrittensam-
derivedfromphotographsyieldalowBLEUscore ples,itisevidentthatOldChinese(ZHO)achieves
a CER of 71.85, while Linear A has a CER of
10WeonlyconductexperimentsonAkkadiansinceitisthe
onlylanguagewithoff-the-shelfdependencyannotations. 57.17. AsshowninFigure5,glyphclassification
)%(
etaR
rorrE
)gol(
hpylg
fo
tnuoc
qerFforZHOisapproximately20%lessaccuratethan languages,suchasAncientGreekorLatin(Bam-
line-levelOCR,indicatingthatcontextualfeatures man and Burns, 2020), benefit greatly from mul-
significantly aid in recognizing glyphs. Further- tilingualpre-trainingtechniques,suchasmBERT,
more,thereisarapidincreaseinerrorrateasthe XLM-R(Conneauetal.,2020),andBLOOM(Scao
numberofglyphsincreases,highlightingtheintrin- etal.,2022). Theapplicabilityofthesetechniques
sicchallengeofprocessinglogographiclanguages, is limited when it comes to languages that were
which typically have a large symbol inventories, historically written in obsolete or extinct writing
and their frequency distribution often follows a systems—for instance, languages like Sumerian
long-tailpattern(seetheorangebarsinFigure5). andElamitewererecordedinCuneiformscriptand
Therefore,developingrobustvisualmodelsthatcan ancient Chinese was inscribed on oracle bone or
effectivelyleveragevisualfeaturesiscrucialforim- bamboo. However,observationsbyexistingwork
provingNLPonancientlogographiclanguages. support the potential utility of visual processing
pipelinesforancientlanguages.
6 Relatedwork
Becauseancientlanguagesareoftenlow-resource, Logographicwritingsystems. Logographytypi-
theypresentchallengesthatarecloselyrelatedto callydenotesawritingsysteminwhicheachglyph
other domains of NLP, such as low-resource ma- representsasemanticvalueratherthanaphonetic
chinelearningandmulti-lingualtransferlearning. one,however,allthelanguagesstudiedinourpaper
RecentworkhasexploredtheapplicationofNLP have at least some phonetic component based on
techniquestoancientlanguagesfromthefollowing therebusprinciple. Thispaperemphasizesancient
perspectives: logographies that (i) possess extensive glyph in-
ventories;(ii)havefeatureglyphswithambiguous
Multilingualtransferlearninganddisjointchar-
transliterationsorfunctionaluses;and(iii)arelow-
acter sets. Muller et al. (2020) studied hard-to-
resourcewithmuchofdataremaininginphotofor-
process living languages such as Uyghur, and re-
mats(Capliceetal.,1991;Allen,2000;Woodard,
portedthatanon-contextualbaselineoutperforms
2004). Existingresearchonlogographiclanguages
all pre-trained LM-based methods. Ancient lan-
haspredominantlyfocusedonthosewell-resourced
guagesalsofacethesameproblem,withevenless
and still in use, such as Modern Chinese (Zhang
data available. A major challenge that is mostly
andKomachi,2018;Sietal.,2023),oruseddata
specifictoancientlogographiclanguages,however,
that has already been carefully transcribed into
isthealmostnon-existentoverlapoftheirsymbol
Latin or annotated with extra semantic informa-
inventorieswiththoseofhigh-resourcelanguages.
tion(WiesenbachandRiezler,2019;Gutherzetal.,
Visual representation of languages. Recently, 2023; Jiang et al., 2024). Our paper aims to ad-
several works have studied language processing dressthegapinresources(byproposingnewdata)
based on images of text. Rust et al. (2023) pre- and methodologies (by adapting visual-only ap-
trainedamaskedlanguagemodelondigitallyren- proaches)forencodingandanalyzingancientlogo-
deredtextandachievedcomparableperformance graphiclanguages,leadingtomorecomprehensive
with text-based pre-training strategies on down- understandingofhistoricallinguisticlandscapes.
stream tasks. Salesky et al. (2023) found that a
multi-lingualtranslationsystemwithpixelinputs
7 Conclusion
wasabletooutperformitstextualcounterpart.
Machinelearningforancientlanguages. Som-
By comparing the results on four representative
merschield et al. (2023) surveyed the status of
languagesonthreedownstreamtasks,wedemon-
pipelinesforancientlanguageprocessing. Notably,
stratedthechallengesfacedinapplyingnaturallan-
thestudyconcludesthatapplyingmachinelearning
guageprocessingtechniquestoancientlogographic
methodstoancientlanguagesisbottleneckedbythe
writing systems. Our experiments demonstrate,
costofdigitizationandtranscription. Accordingto
however,thatencodingmorereadilyavailablevi-
theMissingScriptsProject,11 only73of136dead
sual representations of language artifacts can be
writingsystemsareencodedinUnicode. Ancient
a successful strategy for downstream processing
11https://worldswritingsystems.org/ tasks.8 Limitations Labelimbalance. Theclassificationtaskinour
benchmark is label imbalanced. This is known
More discussion on ancient logographic lan-
tobeamajorissueforallmachinelearningtasks
guages. Duetopagelimits,wedonotdiscussan-
relatedtotheancientworld(Sommerschieldetal.,
cientlogographiclanguagesinacriticalway. Tech-
2023;Chenetal.,2024).
nically, there are no logographic languages, only
languageswritteninlogographicwritingsystems Acknowledgements
(akalogography)(GormanandSproat,2023). In
WethankProfessorWenboChenfromtheDepart-
thispaper,weusetheterm“logographiclanguages”
ment of Humanities at Central South University,
to denote languages that are quite different from
China,forhisadviceonOldChinesedatacollec-
those with alphabetic writing systems especially
tionandexplanation. WethankProfessorEdward
when we tried to apply NLP toolkits for compu-
KeltingfromtheDepartmentofLiteratureatUC
tationalpaleography. Asmentionedintherelated
SanDiegoforhisadviceonAncientEgyptiandata
worksection,theselanguagesfeatureglyphsthat
collection and explanation. We thank Jerry You
havemultipletransliterationsorfunctionaluses. In
from http://www.ccamc.org/ for his help
otherwords,theselanguagesarehomophonousor
on Unicode and data processing for ancient lan-
aglyphcanbeusedasaphoneticvalueorsemantic
guages.
value. Therefore, the boundaries between logo-
WethankElizabethSaleskyforherguidancein
graphicandphonographicisnotsharplyseparated.
settingupcross-lingualmachinetranslationexperi-
Including more logographic writing systems. mentsforancientlanguagesusingbothPIXELand
Weselectedthefourlanguagesbecausewewould BPE encoders. We thank Chenghao Xiao for the
liketoincludeatleastonelanguagefromearlyciv- helpsettingupthePIXEL+GPT2experiment.
ilization in Ancient China, Ancient Egypt, Indus WethankKyleGorman,AlexanderGutkinand
Valley Civilization, Mesoamerica, Mesopotamia RichardSproatfortheirinspiringwork(Sproatand
and Minoan Civilization (Woodard, 2004). How- Gutkin, 2021; Gorman and Sproat, 2023), which
ever, we fail to include Mayan hieroglyphs hassignificantlycontributedtoourunderstanding
(Mesoamerica)andOracleBonescript. However, of logographic writing systems from a computa-
Mayanisexcludedbecausethecollection12 isstill tionalperspective.
workinginprocess. OracleBonescriptisprimarily We thank Nikita Srivatsan, Nikolai Vogler, Ke
omittedduetocopyrightissues. Chen, Daniel Spokoyny, David Smith, and the
anonymousARRreviewersfortheirinsightfulfeed-
Textlineimages. Mostancientlanguagesremain
back on the paper draft. This work was partially
asfull-documentimages. Inthispaper,weusedig-
supported by the NSF under grants 2146151 and
itallyrenderedtextasasurrogatevisualfeaturefor
2200333.
Akkadian. In reality, much of Cuneiform data is
stillinhandcopiesorinphotoformat. Inthefuture,
welooktoconductapples-to-applescomparisons
References
foralllanguagesoncethelinesegmentationanno-
JamesPAllen.2000. MiddleEgyptian:Anintroduction
tationsbecomeavailable.
to the language and culture of hieroglyphs. Cam-
Annotation quality and quantity. The study bridgeUniversityPress.
of ancient languages is constantly evolving; hu- YannisAssael,TheaSommerschield,BrendanShilling-
manitiesscholarshavenotagreedonexplanations, ford, Mahyar Bordbar, John Pavlopoulos, Marita
transliterations, or even the distinctions between Chatzipanagiotou, Ion Androutsopoulos, Jonathan
Prag, and Nando de Freitas. 2022. Restoring and
certainglyphsorperiods. Wetryourbesttocare-
attributingancienttextsusingdeepneuralnetworks.
fullyannotatedthedatawithoutbias;however,fu-
Nature,603(7900):280–283.
tureeditionsofthebenchmarkareneededasthings
DavidBammanandPatrickJBurns.2020. Latinbert:
change all the time. A collective platform to cor-
Acontextuallanguagemodelforclassicalphilology.
recterrorsandmakemoredataavailableshouldbe
arXivpreprintarXiv:2009.10053.
consideredforfuturedevelopment.
RichardCapliceetal.1991. Introductiontoakkadian.
12The Maya Hieroglyphic Text and Image Archive:
https://digitale-sammlungen.ulb.uni-bonn. DanluChen,AditiAgarwal,TaylorBerg-Kirkpatrick,
de/maya andJacoboMyerston.2023. Cuneiml: Acuneiformdatasetformachinelearning. JournalofOpenHu- Kenji Imamura and Eiichiro Sumita. 2022. Extend-
manitiesData,9(1). ing the subwording model of multilingual pre-
trained models for new languages. arXiv preprint
DanluChen,JiaheTian,YufeiWeng,andTaylorBerg- arXiv:2211.15965.
KirkpatrickandJacoboMyerston.2024. Classifica-
tion of paleographic artifacts at scale: Mitigating GuangyuanJiang,MatthiasHofer,JiayuanMao,Lionel
confoundsanddistributionshiftincuneiformtablet Wong, Joshua B. Tenenbaum, and Roger P. Levy.
dating. InProceedingsofthe1stInternationalWork- 2024. Findingstructureinlogographicwritingwith
shoponMachineLearningforAncientLanguages. librarylearning. 48.Proceedingsofthe46thAnnual
AssociationforComputationalLinguistics. ConferenceoftheCognitiveScienceSociety(CogSci
2024).
JonathanH.Clark,DanGarrette,IuliaTurc,andJohn
BenjaminKiessling.2022. TheKrakenOCRsystem.
Wieting. 2022. Canine: Pre-training an Efficient
Tokenization-FreeEncoderforLanguageRepresen- Diederik P Kingma and Jimmy Ba. 2015. Adam: A
tation. TransactionsoftheAssociationforComputa- methodforstochasticoptimization. InProceedings
tionalLinguistics,10:73–91. ofthe3rdInternationalConferenceforLearningRep-
resentations(ICLR).
AlexisConneau,KartikayKhandelwal,NamanGoyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco ZhaoLiu.2003. AnnotationandTranslationforGuo-
Guzmán, Édouard Grave, Myle Ott, Luke Zettle- dianBambooSlips. FujianRenminPublisher.
moyer,andVeselinStoyanov.2020. Unsupervised
MikkoLuukko,AleksiSahala,SamHardwick,andKris-
cross-lingualrepresentationlearningatscale. InPro-
terLindén.2020. AkkadiantreebankforearlyNeo-
ceedings of the 58th Annual Meeting of the Asso-
Assyrian royal inscriptions. In Proceedings of the
ciationforComputationalLinguistics,pages8440–
19thInternationalWorkshoponTreebanksandLin-
8451.
guistic Theories, pages 124–134, Düsseldorf, Ger-
many.AssociationforComputationalLinguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
KristinaToutanova.2018. Bert: Pre-trainingofdeep
Benjamin Muller, Antonis Anastasopoulos, Benoît
bidirectionaltransformersforlanguageunderstand-
Sagot, and Djamé Seddah. 2020. When being un-
ing. arXivpreprintarXiv:1810.04805.
seenfrommbertisjustthebeginning: Handlingnew
languageswithmultilinguallanguagemodels.
Timothy Dozat and Christopher D. Manning. 2017.
Deepbiaffineattentionforneuraldependencypars- Mark-JanNederhofandMBerti.2015. Ocrofhandwrit-
ing. InInternationalConferenceonLearningRepre- ten transcriptions of ancient egyptian hieroglyphic
sentations. text. Altertumswissenschaften in a Digital Age:
Egyptology,Papyrologyandbeyond,Leipzig.
MengjieFang,LinlinXu,andPascaleFung.2020. Un-
supervisedcross-lingualtransferlearningforcontex- MyleOtt,SergeyEdunov,AlexeiBaevski,AngelaFan,
tualized word embeddings. In Proceedings of the SamGross,NathanNg,DavidGrangier,andMichael
2020ConferenceonEmpiricalMethodsinNatural Auli. 2019. fairseq: A fast, extensible toolkit for
LanguageProcessing(EMNLP),pages2616–2629. sequencemodeling. InProceedingsofNAACL-HLT
2019: Demonstrations.
KyleGormanandRichardSproat.2023. Mythsabout
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
writingsystemsinspeech&languagetechnology. In
Proceedings of the Workshop on Computation and Howmultilingualismultilingualbert? InProceed-
ingsofthe57thAnnualMeetingoftheAssociation
WrittenLanguage(CAWL2023),pages1–5,Toronto,
forComputationalLinguistics.AssociationforCom-
Canada.AssociationforComputationalLinguistics.
putationalLinguistics.
GaiGutherz,ShaiGordin,LuisSáenz,OmerLevy,and
Alec Radford, Jeff Wu, Rewon Child, David Luan,
JonathanBerant.2023. Translatingakkadiantoen-
DarioAmodei,andIlyaSutskever.2019. Language
glishwithneuralmachinetranslation. PNASnexus,
modelsareunsupervisedmultitasklearners.
2(5):pgad096.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
KaimingHe,XinleiChen,SainingXie,YanghaoLi,Pi- Lee,SharanNarang,MichaelMatena,YanqiZhou,
otrDollár,andRossGirshick.2022. Maskedautoen- Wei Li, and Peter J Liu. 2020. Exploring the lim-
codersarescalablevisionlearners. InProceedings its of transfer learning with a unified text-to-text
oftheIEEE/CVFconferenceoncomputervisionand transformer. JournalofMachineLearningResearch,
patternrecognition,pages16000–16009. 21(140):1–67.
KaimingHe,XiangyuZhang,ShaoqingRen,andJian PhillipRust,JonasF.Lotz,EmanueleBugliarello,Eliz-
Sun.2016. Deepresiduallearningforimagerecog- abethSalesky,MiryamdeLhoneux,andDesmond
nition. In Proceedings of the IEEE conference on Elliott.2023. Languagemodellingwithpixels. In
computervisionandpatternrecognition,pages770– TheEleventhInternationalConferenceonLearning
778. Representations.PhillipRust,JonasPfeiffer,IvanVulic´,SebastianRuder, Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
andIrynaGurevych.2021. Howgoodisyourtok- Chaumond,ClementDelangue,AnthonyMoi,Pier-
enizer? onthemonolingualperformanceofmultilin- ricCistac,TimRault,RémiLouf,MorganFuntowicz,
gual language models. In Proceedings of the 59th JoeDavison,SamShleifer,PatrickvonPlaten,Clara
AnnualMeetingoftheAssociationforComputational Ma,YacineJernite,JulienPlu,CanwenXu,TevenLe
Linguisticsandthe11thInternationalJointConfer- Scao, Sylvain Gugger, Mariama Drame, Quentin
ence on Natural Language Processing (Volume 1: Lhoest,andAlexanderM.Rush.2020. Transform-
LongPapers),pages3118–3135,Online.Association ers: State-of-the-artnaturallanguageprocessing. In
forComputationalLinguistics. Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Elizabeth Salesky, Neha Verma, Philipp Koehn, and Demonstrations,pages38–45,Online.Association
MattPost.2023. Multilingualpixelrepresentations forComputationalLinguistics.
for translation and effective cross-lingual transfer.
In Proceedings of the 2023 Conference on Empiri- RogerDWoodard.2004. TheCambridgeencyclopedia
calMethodsinNaturalLanguageProcessing,pages oftheworld’sancientlanguages. CambridgeUniv.
13845–13861,Singapore.AssociationforComputa- Press.
tionalLinguistics.
LintingXue,AdityaBarua,NoahConstant,RamiAl-
EsterSalgarella.2020. AegeanLinearScript(s): Re- Rfou, Sharan Narang, Mihir Kale, Adam Roberts,
thinkingtheRelationshipbetweenLinearAandLin- andColinRaffel.2022. Byt5: Towardsatoken-free
earB. CambridgeUniversityPress. futurewithpre-trainedbyte-to-bytemodels. Transac-
tionsoftheAssociationforComputationalLinguis-
Teven Le Scao, Angela Fan, Christopher Akiki, El- tics,10:291–306.
lie Pavlick, Suzana Ilic´, Daniel Hesslow, Roman
Castagné,AlexandraSashaLuccioni,FrançoisYvon, LongtuZhangandMamoruKomachi.2018. Neuralma-
Matthias Gallé, et al. 2022. Bloom: A 176b- chinetranslationoflogographiclanguageusingsub-
parameteropen-accessmultilinguallanguagemodel. character level information. In Proceedings of the
arXivpreprintarXiv:2211.05100. ThirdConferenceonMachineTranslation: Research
Papers,pages17–25,Brussels,Belgium.Association
ChengleiSi,ZhengyanZhang,YingfaChen,Fanchao
forComputationalLinguistics.
Qi,XiaozhiWang,ZhiyuanLiu,YashengWang,Qun
Liu, and Maosong Sun. 2023. Sub-character tok-
A Datasetcollection
enization for Chinese pretrained language models.
TransactionsoftheAssociationforComputational
A.1 LinearA
Linguistics,11:469–487.
A.1.1 Attributeclassification
Thea Sommerschield, Yannis Assael, John Pavlopou-
los, VanessaStefanak, AndrewSenior, ChrisDyer, Linear A is a logo-syllabic, undeciphered
John Bodel, Jonathan Prag, Ion Androutsopoulos, writing system that was used in Bronze Age
andNandodeFreitas.2023. Machinelearningfor
Greece (ca. 1800-1450BCE). We crawled a
ancient languages: A survey. Computational Lin-
datasetof772 tabletsfromthepublicly available
guistics,pages1–44.
SigLA database 13. The metadata includes
RichardSproatandAlexanderGutkin.2021. Thetax- find-place, tablet dimensions, total number
onomy of writing systems: How to measure how
of signs and possible transliteration. For the
logographicasystemis. ComputationalLinguistics,
attribute classification task, we use find-place
47(3):477–528.
as the target class, with the following classes
LucienTesnière.1959. Élémentsdesyntaxestructurale. [’Arkhanes’, ’Gournia’, ’Haghia
Triada’, ’Haghios Stephanos’,
MichaelTschannen,BasilMustafa,andNeilHoulsby.
2023. Clippo: Image-and-language understanding ’Kea’, ’Khania’, ’Knossos’,
frompixelsonly. InProceedingsoftheIEEE/CVF ’Kythera’, ’Malia’, ’Mallia’,
ConferenceonComputerVisionandPatternRecog- ’Melos’, ’Mycenae’, ’Papoura’,
nition,pages11006–11017.
’Phaistos’, ’Psykhro’, ’Pyrgos’,
ZihanWang,StephenMayhew,DanRoth,etal.2020. ’Syme’, ’Tylissos’, ’Zakros’]. We
Extending multilingual bert to low-resource lan- only keep classes whose tablets are no less
guages. arXivpreprintarXiv:2004.13640.
than 10, which results in only keeping 7
classes: [’Haghia Triada’, ’Khania’,
Philipp Wiesenbach and Stefan Riezler. 2019. Multi-
taskmodelingofphonographiclanguages: Translat- ’Phaistos’, ’Zakros’, ’Knossos’,
ingmiddleegyptianhieroglyphs. InProceedingsof ’Malia’, ’Arkhanes’].
the 16th International Conference on Spoken Lan-
guageTranslation. 13https://sigla.phis.me/browse.htmlA.1.2 MachineTranslation
A.2 AncientEgyptian
A.2.1 Attributeclassification
We use time periods whose data more than 50 to
conductthistask,resultingin1,230samplesof12
classes.
The classes are as below: [’Unas’,
’Senwosret I Kheperkare’,
’Pepi I Merire’, ’Ramesses II
Usermaatre-Setepenre’, ’Sety
I Menmaatre’, ’Tuthmosis III
Menkheperre (complete reign)’,
’Pepi II Neferkare’, ’Hatshepsut
Maatkare’, ’18th Dynasty’,
’Mentuhotep II Nebhepetre
Figure 6: Sample parallel data of GuoDian Bamboo
(complete reign)’, ’Amenemhat
scriptdataset.
II Nebukaure’, ’Tutankhamun
Nebkheperure’, ’Cleopatra VII
Philopator’, ’12th Dynasty’]
slipcollectionsforattributepredictionandmachine
translation.
A.3 Akkadian(Cuneiform)
A.4.1 Attributeprediction
A.3.1 Attributeprediction
TheBaoShanbambooslipscollectionconsistsof
We use the dataset from Chen et al. (2023), con-
723slips,whichareinthreegenres.
taining34,562photographsofAkkadiancuneiform
tabletswiththeirtranscriptionsincuneiformuni-
A.4.2 MachineTranslation
codeandLatintransliterations. Themetadatacon-
WeusedtheGuoDianbambooslipscollectionfor
tainsattributessuchastimeperiod,genreandgeo-
machine translation, whose major genre is philo-
graphicallocation(foundplace)thatwehaveused
sophical essay. By referring to (Liu, 2003), We
forattributeclassification.
manually labeled each complete sentences with
A.3.2 Dependencyparsing theirtransliterationandtranslationinmodernChi-
nese. After filtering out incomplete sentences or
ThedataisfromLuukkoetal.(2020)andcomes
removing the sentences with high interpretation
inasnormalizationformofAkkadian. Weimple-
difficulty. We extracted and labelled489 lines of
mentedarule-basedmethodtoconvertthenormal-
parallel data. The Bamboo script was labeled by
ization form back to standard transliteration for
atrainedinterdisciplinaryPh.D.studentunderthe
consistency. When the auto-conversion fails, we
guidanceofProfessorWenboChenspecializingin
simplyjustkeepthenormalizationform.
AncientChinesePhilology.
A.3.3 Machinetranslation
B AblationStudyonTextRecognition
The data is from Gutherz et al. (2023), a collec-
tionof8,056linesofAkkadian-Englishpair. The
Line-level OCR. We use Kraken (Kiessling,
AkkadianrepresentationisavailableinbothATF
2022), a state-of-the-art OCR library for histori-
transliterationandCuneiformUnicode.
caldocuments,totranscribetheimageintoLatin
orUnicode. TohandleunseenUnicodecodepoints,
A.4 Bambooscript
we simply extend the vocabulary of the decoder.
There are actually more than one scripts used in ForLatintransliteration,wepredictoutputsatthe
bambooslips,forexample,SealscriptandClerical character level. Similar to most OCR pipelines,
script. Forsimplicity,ourdatasetgenerallycallit thedefaultOCRmodelofKrakenisa2-layerbi-
Bambooscript. Wepicktwofamouscollectionsof directional LSTM with a connectionist temporal
thebambooscript,BaoShanandGuoDianbamboo classification(CTC)loss.Glyph classification. We also applied Convo-
lution Neural Networks (CNNs) to classify seg-
mentedglyphs. WeuseResNet-50asthebackbone
model,followedbyalinearclassificationlayer. We
trained the model using cross-entropy loss with
Adam optimizer. We resize the image of each
glyph to 64 × 64 and apply 20% cropping. In
total,thereare21,687segmentedcharactersinthe
ZHOdataset.
C MoreTranslationCaseStudy
We sample 10 examples from validation set for
eachlanguagepairs.
C.1 AKK-EN
AkkadiantranslationsamplesfromthePIXEL-MT
modelsisshowninFigure7. Weshowcasethefirst
10 examples from the validation set, we find that
someannotationerrorpresentforexample#2and
#6.
C.2 EGY-EN
Ancient Egyptian translation samples from the
PIXEL-MTmodelsisshowninFigure8.
C.3 ZHO-EN
OldChinesetranslationsamplesfromthePIXEL-
MTmodelsisshowninFigure9.BLEU = 39.84 73.1/51.8/40.6/32.6 (BP = 0.842 ratio = 0.853 hyp_len = 1462 ref_len = 1713)
36 2811
0 BLEU = 34.30 71.3/45.3/30.6/20.2 (BP = 0.912 ratio = 0.916 hyp_len = 87 ref_len = 95)
[[pred]] At the beginning of my kingship, in my first regnal year, in the fifth month when I sat on the
royal throne, (the god) Assur, my lord, encouraged me and I gave (them) to the Hamranu, the Luhutu, Hatalu,
Rapiqu, Rapiqu, Rapiqu, Nasiru, Gulasi, Nabatu, Li<unk>ta<unk>u, Kaparu, Malitu, Adadu, Gibre, Gurumu,
Gibre, Gurumu,
[[ref ]] At the beginning of my reign, in my first palu, in the fifth month after I sat in greatness on the
throne of kingship, (the god) Assur, my lord, encouraged me and I marched against (the Aramean tribes)
Hamaranu, Luhu`atu, Hatallu, Rubbu, Rapiqu, Hiranu, (5) Rabi-ilu, Nasiru, Gulusu, Nabatu, Li`ta`u, Rahiqu,
Kapiru, Rummulitu (Rummulutu), Adile, Gibre, Ubudu, Gurumu,
====================
1 BLEU = 26.01 60.4/34.0/21.1/10.6 (BP = 1.000 ratio = 1.085 hyp_len = 154 ref_len = 142)
[[pred]] In the midst of the sea, just as Sarridu (and) NN seized ... in my hand ... I captured 7,999 ...
their ... ; Sarriduri fled to save his life, and as many as the sun had flowed ... with arrow(s), (and) ...
they threw his bed with ... s, as far as the back of the Euphrates, and his bed ... of his royal path,
together with stones, his royal chariot, ... , his ... , many as there were, (and) his .... s, as far as
the border of the Euphrates, his royal chariot(s), his ... , everything that was without number.
[[ref ]] In the midst of that battle, I captured Sarduri's ... I ... 72,950 of their ... from ... (10') ...
In order to save his life, Sarduri fled at night and (thus) escaped very quickly before sunrise... With an
arrow that cuts off lives, I drove him back to the bridge (crossing over) the Euphrates River, on the
border of his land. I took away from him his bed, ... , his royal processional chariot, the cylinder seal
(that hung around) his neck, together with his necklace, his royal chariot, ... , their ... , (and) many
other things, without number.
====================
2 BLEU = 0.00 0.0/0.0/0.0/0.0 (BP = 0.717 ratio = 0.750 hyp_len = 3 ref_len = 4)
[[pred]] ...
[[ref ]] (Completely destroyed)
====================
3 BLEU = 46.04 84.6/76.0/58.3/47.8 (BP = 0.707 ratio = 0.743 hyp_len = 26 ref_len = 35)
[[pred]] I received tribute from the Medes, the land Ellipi, and the city rulers of the mountain regions,
as far as Mount Bikni.
[[ref ]] I received the payment of the Medes, the people of the land Ellipu, and the city rulers of all of
the mountain regions, as far as Mount Bikni: (10)
====================
4 BLEU = 2.07 33.3/6.2/3.6/2.1 (BP = 0.329 ratio = 0.474 hyp_len = 9 ref_len = 19)
[[pred]] , Da'qa-il of the city Saakka, Ilili-Arbail.
[[ref ]] I captured (and) defeated the cities Daiqansa, Sakka, Ippa, Elizansu, (5)
====================
5 BLEU = 43.94 71.0/47.5/38.3/28.8 (BP = 1.000 ratio = 1.107 hyp_len = 62 ref_len = 56)
[[pred]] (As for) every royal treasure, live male sheep whose wool is sweet, the bird of the sky, the
winged bird of the sky whose wings are dyed with red-purples, which are dyed with red-purple wool, I
received horses, mules, oxen, and sheep and goats, camels, together with their camels.
[[ref ]] all kinds of precious things from the royal treasure, live sheep whose wool is dyed red-purple,
flying birds of the sky whose wings are dyed blue-purple, horses, mules, oxen, and sheep and goats, camels,
she-camels, together with their young, I received (from them).
====================
6 BLEU = 3.39 8.3/4.5/2.5/1.4 (BP = 1.000 ratio = 12.000 hyp_len = 12 ref_len = 1)
[[pred]] whose reign is unalterable, whose reign is Samas is firm.
[[ref ]] ,
====================
7 BLEU = 44.74 88.1/73.2/57.5/41.0 (BP = 0.717 ratio = 0.750 hyp_len = 42 ref_len = 56)
[[pred]] ... ni, the Alaya, ... the land Ursalma and ... Dalta of the land Ellipi, ... horses, mules,
donkeys, oxen, and sheep and goats ...
[[ref ]] ... the land ... ni, the land Ayyalaya, ... , the land Niksamma, ... I received the payment of
Dalta of the land Ellipu: ... horses, mules, Bactrian camels, oxen, and sheep and goats, ...
====================
8 BLEU = 52.45 80.4/63.6/55.6/47.2 (BP = 0.867 ratio = 0.875 hyp_len = 56 ref_len = 64)
[[pred]] (As for) Sardardari of the land Urartu, revolted against me and conspired with Mati'-ilu. In the
lands Kistan and Halpi, districts of the city Kummuhu, he became frightened of the terrifying radiance of
my weapons, and (then) fled to him in order to save his life.
[[ref ]] Sarduri of the land Urartu revolted against me and conspired with Mati`-il (against me). In the
lands Kistan and Halpi, districts of the city Kummuhu, I defeated him and took his entire camp away from
him. He became frightened of the terrifying radiance of my weapons and fled alone in order to save his
life.
====================
9 BLEU = 49.33 83.0/59.6/47.1/40.0 (BP = 0.893 ratio = 0.898 hyp_len = 53 ref_len = 59)
[[pred]] I fashioned image(s) of the gods, my lords, and my royal image made of gold (and) erected (it) in
the palace of the city Gaza. I regarded (them) as gods of their lands and (them) their regular offerings.
[[ref ]] I fashioned (a statue bearing) image(s) of the gods, my lords, and my royal image out of gold,
erected (it) in the palace of the city Gaza, (and) I reckoned (it) among the gods of their land; I
established their sattukku offerings.
====================
10 BLEU = 0.00 0.0/0.0/0.0/0.0 (BP = 0.368 ratio = 0.500 hyp_len = 3 ref_len = 6)
[[pred]] the city Labbe
[[ref ]] The cities Lab`u,
Figure7: AkkadiantranslationsamplesfromthePIXEL-MTmodels.0 BLEU = 6.90 25.0/9.1/5.0/2.8 (BP = 0.920 ratio = 0.923 hyp_len = 12 ref_len = 13)
[[pred]] for the Great Heroon who is in front of the curtain.
[[ref ]] before the Great Heron which went forth (from) the the garden
====================
1 BLEU = 35.10 69.6/45.5/28.6/20.0 (BP = 0.957 ratio = 0.958 hyp_len = 23 ref_len = 24)
[[pred]] The king of Upper and Lower Egypt gave me two, or gold: two rings, two necropolis, one bracelet.
[[ref ]] The king of Upper and Lower Egypt has given to me, of gold: 2 rings, 2 necklaces, one bracelet,
====================
2 BLEU = 22.03 66.7/35.3/18.8/6.7 (BP = 0.946 ratio = 0.947 hyp_len = 18 ref_len = 19)
[[pred]] the butchers, the mother of the mother, the Ba of Re of the prime time.
[[ref ]] father of the fathers, mother of the mothers, the ba of Re of the primival times.
====================
3 BLEU = 66.87 80.0/75.0/66.7/50.0 (BP = 1.000 ratio = 1.250 hyp_len = 5 ref_len = 4)
[[pred]] The messenger of Ra.
[[ref ]] The messenger of Ra
====================
4 BLEU = 51.93 72.7/60.0/44.4/37.5 (BP = 1.000 ratio = 1.000 hyp_len = 11 ref_len = 11)
[[pred]] like the brewer or one who is under the knife.
[[ref ]] like the burden of one who is under the knife,
====================
5 BLEU = 11.37 57.1/20.0/5.3/2.8 (BP = 1.000 ratio = 1.050 hyp_len = 21 ref_len = 20)
[[pred]] after Hes Majesty had as to the Shesmet who satisfies this August, Sopu, the Lord of the East.
[[ref ]] after His Majesty had come to Shesmet while satisfying this august god, Sopdu, the lord of the East
====================
6 BLEU = 22.25 55.6/35.3/18.8/6.7 (BP = 1.000 ratio = 1.125 hyp_len = 18 ref_len = 16)
[[pred]] This Pepi Neferkare is on the right side of this island of the land of Pepi Neferkare.
[[ref ]] This Pepi Neferkare is one righteous near this island of land that Pepi Neferkare swum to
====================
7 BLEU = 7.59 25.0/14.3/8.3/5.0 (BP = 0.687 ratio = 0.727 hyp_len = 8 ref_len = 11)
[[pred]] Pepi Neferkare has brought down the aura.
[[ref ]] Pepy Neferkare has requested his need as ruler, 2 arouras
====================
8 BLEU = 19.51 83.3/60.0/25.0/16.7 (BP = 0.513 ratio = 0.600 hyp_len = 6 ref_len = 10)
[[pred]] Taking the rope of the sky
[[ref ]] Taking the run for the lady of the sky,
====================
9 BLEU = 20.39 66.7/45.5/20.0/5.6 (BP = 0.846 ratio = 0.857 hyp_len = 12 ref_len = 14)
[[pred]] for the one who speaks the monuments for Amun in Karnakhnak.
[[ref ]] for the ka of the one who calls the monuments for Amon in Karnak
Figure8: AncientEgyptiantranslationsamplesfromthePIXEL-MTmodels.0 BLEU = 0.83 8.8/0.7/0.4/0.2 (BP = 1.000 ratio = 1.030 hyp_len = 68 ref_len = 66)
[[pred]] Associating with those above is close to serving the king, associating with those below is close
to serving the king, and self-cultivation through self-reflection through self-reflection through self-
reflection through self-reflection through self-reflection through self-reflection through self-reflection
through self-reflection through self-reflection through self-reflection through self-reflection through
self-reflection through self-rection through self-rection through self-reflection through self-rection
through self-rection through self-rection through self-rection through self-rection through self-rection
through self-reflection
[[ref ]] If you are engaged in learning, your knowledge will increase day by day; if you are engaged in
"Tao", your "doing" will decrease day by day. Reduce it again and again until it reaches "inaction". Don’t
take the initiative to do something, but obey the natural “behavior” of all things in the world.
====================
1 BLEU = 2.44 18.2/2.4/1.2/0.7 (BP = 1.000 ratio = 1.294 hyp_len = 22 ref_len = 17)
[[pred]] The gentleman hears "Shang Shu Jun Chen" says: "Be careful and no longer tell me good."
[[ref ]] When the inferior people heard "Tao", they laughed and did not believe it.
====================
2 BLEU = 30.90 57.1/50.0/40.0/25.0 (BP = 0.751 ratio = 0.778 hyp_len = 7 ref_len = 9)
[[pred]] Wisdom is the source of virtue.
[[ref ]] Saint wisdom is the source of ritual and music
====================
3 BLEU = 4.72 31.8/4.8/2.5/1.3 (BP = 1.000 ratio = 1.222 hyp_len = 22 ref_len = 18)
[[pred]] There are thousands of people waiting before the birth of things, and they can preside over the
affairs of things.
[[ref ]] We must do something before things happen, and we must manage things before they become chaotic.
====================
4 BLEU = 2.83 20.0/3.6/1.9/1.0 (BP = 0.819 ratio = 0.833 hyp_len = 15 ref_len = 18)
[[pred]] Take the rule of thumb, and the rule of thumb will change accordingly.
[[ref ]] If this principle is implemented in a country, the country's "virtue" will be rich;
====================
5 BLEU = 3.13 13.3/3.6/1.9/1.0 (BP = 1.000 ratio = 1.500 hyp_len = 15 ref_len = 10)
[[pred]] Being able to hold on to the superior is not satisfied with the superior.
[[ref ]] Those who maintain this path should not worship abundance.
====================
6 BLEU = 2.37 40.0/25.0/16.7/12.5 (BP = 0.111 ratio = 0.312 hyp_len = 5 ref_len = 16)
[[pred]] Simplicity cannot last long.
[[ref ]] If you are not smart, you will not be able to think for long.
====================
7 BLEU = 2.81 57.1/16.7/10.0/6.2 (BP = 0.180 ratio = 0.368 hyp_len = 7 ref_len = 19)
[[pred]] Respect is the principle of self-reflecting.
[[ref ]] Being rich and arrogant will bring disaster to oneself. Retiring after success is the way of
heaven.
====================
8 BLEU = 1.42 10.5/2.8/1.5/0.8 (BP = 0.591 ratio = 0.655 hyp_len = 19 ref_len = 29)
[[pred]] Those who are happy after hearing the "Tao" can't do things without speaking because they can't.
[[ref ]] Confucius said: A gentleman does not say what he can say but cannot do; he does not do what he can
do but cannot tell others.
====================
9 BLEU = 9.03 35.7/15.4/8.3/4.5 (BP = 0.751 ratio = 0.778 hyp_len = 14 ref_len = 18)
[[pred]] If serious crimes are not committed, people will be punished without faults.
[[ref ]] If he acts with true feelings, people will trust him even before he starts taking action.
====================
10 BLEU = 9.52 40.0/14.3/7.7/4.2 (BP = 0.819 ratio = 0.833 hyp_len = 15 ref_len = 18)
[[pred]] There are seven types of poisonous things happening because they have something to do.
[[ref ]] There are seven kinds of love, and only natural love is close to "benevolence".
Figure9: OldChinesetranslationsamplesfromthePIXEL-MTmodels.