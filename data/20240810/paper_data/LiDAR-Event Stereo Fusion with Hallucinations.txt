LiDAR-Event Stereo Fusion with Hallucinations
Luca Bartolomei , Matteo Poggi , Andrea Conti , and
Stefano Mattoccia
University of Bologna, Italy
https://eventvppstereo.github.io/
(a) (b) (c) (d)
Stereo Matcher Stereo Matcher Stereo Matcher Stereo Matcher
+Guided +VSH (Ours) +BTH (Ours)
Error > 1px: 89.68% Error > 1px: 69.23% Error > 1px: 12.07% Error > 1px: 12.60%
Fig.1: LiDAR-Event Stereo Fusion with Hallucinations.Intheabsenceofmo-
tion or brightness changes, sparse event streams lead stereo models to catastrophic
failures (a). A LiDAR sensor can be used with existing strategies [49] to soften this
problem, yet with limited impact (b), whereas our proposals are superior (c,d).
Abstract. Eventstereomatchingisanemergingtechniquetoestimate
depth from neuromorphic cameras; however, events are unlikely to trig-
gerintheabsenceofmotionorthepresenceoflarge,untexturedregions,
making the correspondence problem extremely challenging. Purposely,
we propose integrating a stereo event camera with a fixed-frequency ac-
tive sensor – e.g., a LiDAR – collecting sparse depth measurements,
overcoming the aforementioned limitations. Such depth hints are used
by hallucinating – i.e., inserting fictitious events – the stacks or raw in-
put streams, compensating for the lack of information in the absence of
brightness changes. Our techniques are general, can be adapted to any
structuredrepresentationtostackeventsandoutperformstate-of-the-art
fusion methods applied to event-based stereo.
1 Introduction
Depth estimation is a fundamental task with many applications ranging from
robotics, 3D reconstruction, and augmented/virtual reality to autonomous ve-
hicles. Accurate, prompt, and high-resolution depth information is crucial for
most of these tasks, but obtaining it remains an open challenge. Among the
many possibilities, depth-from-stereo is one of the longest-standing approaches
4202
guA
8
]VC.sc[
1v33640.8042:viXra2 L. Bartolomei et al.
to deal with it, with a large literature of deep architecture [51] proposed in the
last decade for processing rectified color images.
Eventcameras[18](orneuromorphiccameras)arerecentlyemergingasanal-
ternativetoovercomethelimitationsoftraditionalimagingdevices,suchastheir
low dynamic range or the motion blur caused by fast movements. Unlike their
traditional counterparts, event cameras do not capture frames at synchronous
intervals. Instead, they mimic the dynamic nature of human vision by reporting
pixelintensitychanges,whichcanhavepositive ornegative polarities,assoonas
theyhappen.Thispeculiarityendowsthemwithunparalleledfeatures–notably
microsecond temporal resolution, and an exceptionally high dynamic range –
making them perfectly suited for applications where fast motion and challeng-
ing light conditions are persistent issues (e.g. autonomous driving). The events
streamsareoftenencodedinW H Ctensors,thusbeingfullycompatiblewith
× ×
the CNNs used for classical stereo [43], capable of estimating dense disparity
maps driven by data, despite the sparse nature of events.
However,astheeventstriggeronlywithbrightnesschangesanyderiveddata
is semi-dense and uninformative, for instance, when facing large untextured
regions or in the absence of any motion – e.g., as in the example in Fig. 1. This
makes the downstream stereo network struggle to match events across left and
right cameras, as shown in Fig. 1 (a). According to the RGB stereo literature,
fusing color information with sparse depth measurements from an active sensor
[4,12,49,82](e.g.,aLiDAR)considerablysoftenstheweaknessesofpassivedepth
sensing, despite the much lower resolution at which depth points are provided.
We argue that such a strategy would counter the aforementioned issues even if
applied to the event stereo paradigm, yet with a notable nodus caused by the
fixed rate at which depth sensors work – usually, 10Hz for LiDARs – being in
contrast with the asynchronous acquisition rate of event cameras. This would
cause to either i) use depth points only when available, harming the accuracy of
mostfusionstrategiesknownfromtheclassicalstereoliterature[12,49,82],orii)
limitingprocessingtotheLiDARpace,nullifyingoneofthegreateststrengthof
event cameras – i.e., microseconds resolution. Nonetheless, this track on event
stereo/active sensors fusion has remained unexplored so far.
In this paper, starting from the RGB literature [4,12,49,82], we embark on
a comprehensive investigation into the fusion of event-based stereo with sparse
depth hints from active sensors. Inspired by [4], which projects distinctive color
patterns on the images consistently with measured depth, we design a halluci-
nation mechanism to generate fictitious events over time to densify the stream
collected by the event cameras. Purposely, we propose two different strategies,
respectively consisting of i) creating distinctive patterns directly at the stack
level, i.e. a Virtual Stack Hallucination (VSH), just before the deep net-
work processing, or ii) generating raw events directly in the stream, starting
from the time instant t for which we aim to estimate a disparity map and
d
performing Back-in-Time Hallucination(BTH).Bothstrategies,despitethe
differentconstraints–VSHrequiresexplicitaccesstothestackedrepresentation,
whereasBTHdoesnot–dramaticallyimprovetheaccuracyofpureevent-basedLiDAR-Event Stereo Fusion 3
stereo systems, overcoming some of their harshest limitations as shown in Fig.
1 (c,d). Furthermore, despite depth sensors having a fixed acquisition rate that
is in contrast with the asynchronous capture rate of event cameras, VSH and
BTHcan leverage depthmeasurementsnot synchronized witht (thus collected
d
at t < t ) with marginal drops in accuracy compared to the case of perfectly
z d
synchronizeddepthandeventsensors(t =t ).Thisstrategyallowsforexploit-
z d
ing both VSH and BTH while preserving the microsecond resolution peculiar of
event cameras. Exhaustive experiments support the following claims:
– We prove that LiDAR-stereo fusion frameworks can effectively be adapted
to the event stereo domain
– Our VSH and BTH frameworks are general and work effectively with any
structured representation among the eight we surveyed
– Our strategies outperform existing alternatives inherited from RGB stereo
literature on DSEC [21] and M3ED [9] datasets
– VSH and BTH can exploit even outdated LiDAR data to increase the event
stream distinctiveness and ease matching, preserving the microsecond reso-
lutionofeventcamerasandeliminatingtheneedforsynchronousprocessing
dictated by the constant framerate of the depth sensor
2 Related Work
Stereo Matching on color images. It is a longstanding open problem, with
a large body of literature spanning from traditional approaches grounded on
handcrafted features and priors [5,24,31,36,62,68,75,76,78] to contemporary
deep learning approaches that brought significant improvements over previous
methods,startingwith[79].Nowadays,themosteffectivesolutionshaveemerged
asend-to-enddeepstereonetworks[51],replacingthewholestereopipelinewith
adeepneuralnetworkarchitecturethrough2Dand3Darchitectures.Theformer,
inspiredbytheU-Netmodel[53],adoptsanencoder-decoderdesign[37,42,45,50,
54,59,63,64,74,77].Incontrast,thelatterconstructsafeaturecostvolumefrom
imagepairfeaturesandestimatesthedisparitymapthrough3Dconvolutionsat
the cost of substantially higher memory and runtime demands [10,11,13,16,23,
27,28,57,70,73,80]. A recent trend in this field [34,38,65,71,83,84] introduced
innovative deep stereo networks that embrace an iterative refinement paradigm
or use Vision Transformers [22,35].
Stereo Matching with event cameras. This topic attracted significant
attention due to the unique advantages of event sensors over traditional frame-
based cameras. Similarly to conventional stereo matching, the first approaches
focused on developing traditional algorithms by building structured represen-
tations, such as voxel grids [56], matched through handcrafted similarity func-
tions [30,56,60,85]. However, pseudo-images lose the high temporal resolution
of the stream: to face this problem, [8,52] handle events without an intermedi-
ate representation using an event-to-event matching approach, where for each
reference event, a set of possible matches is given. Camuñas-Mesa et al. [7]4 L. Bartolomei et al.
Fig.2: Event cameras vs LiDARs – strengths and weaknesses.Eventcameras
provide rich cues at object boundaries where LiDARs cannot (cyan), yet LiDARs can
measure depth where the lack of texture makes event cameras uninformative (green).
add filters to exploit orientation cues and increase matching distinctiveness. In-
stead, [47] revisited the cooperative network from [41]. Neural networks also
showed promising results on event stereo matching with models directly pro-
cessing raw events or using structured representation. The former are often in-
spired by [41] and typically employ Spiking Neural Networks (SNN) [1,15,44].
The latter adopts data-driven Convolutional Neural Networks (CNNs) to infer
dense depth maps [43,66,67]. A detailed review of different event-based stereo
techniques can be found in [17].
Sensor fusion for stereo. Recent research has delved into the fusion of
color-cameras stereo vision with active sensors, starting with handcrafted algo-
rithms: Badino et al. [2] integrated LiDAR data directly into the stereo algo-
rithmusingdynamicprogramming,Gandhietal.[19]proposedanefficientseed-
growingalgorithmtofusetime-of-flight(ToF)depthdatawithstereopairs,while
Marin et al. [40] and Poggi et al. [48] exploited confidence measures. Eventu-
ally,contemporaryapproachesintegrateddepthfromsensorswithmodernstereo
networks, either by concatenating them to images as input [12,46,69,81] or by
using them to guide the cost optimization process by modulating existing cost
volumes [25,49,69,82]. More recently, Bartolomei et al. [4] followed a different
pathwithVirtualPatternProjection(VPP).AlthoughLiDARsensorsandevent
camerashavebeendeployedtogetherforsomeapplications[6,14,20,33,55,58,61],
thispaperrepresentsthefirstattemptatcombiningLiDARwithaneventstereo
framework. We argue that the two modalities are complementary, as shown in
Fig. 2 – e.g., the lack of texture and motion makes an event camera uninforma-
tive, whereas this does not affect LiDAR systems.
3 Preliminaries: Event-based Deep Stereo
Eventcamerasmeasurebrightnesschangesasanasynchronousstreamofevents.
Accordingly, an event e =(x ,y ,p ,t ) is triggered at time t if the intensity
k k k k k k
sensedbypixel(x ,y )ontheW Hsensorgridchangesandsurpassesaspecific
k k
×
contrast threshold. Depending on the sign of this change, it will have polarity
p 1,1 .SincethisunstructuredflowisnotsuitableforstandardCNNs–as
k
∈{− }
thoseproposedintheclassicalstereoliterature[51]–convertingitintoW H C
× ×
structured representations is necessary if we are interested in obtaining a dense
disparitymap[21].Purposely,givenatimestampt atwhichwewanttoestimate
dLiDAR-Event Stereo Fusion 5
a disparity map, events are sampled backward in time from the stream, either
based on a time interval (SBT) or a maximum number of events (SBN), and
stacked according to various strategies – among them:
Histogram [39]. Events of the two polarities are counted into per-pixel his-
tograms, yielding a W H 2 stack.
× ×
Voxel grid [86]. The timelapse from which events are sampled is split into
B uniform bins: polarities are accumulated in each bin of a W H B stack.
× ×
Mixed-DensityEventStack(MDES)[43].Similartothevoxelgridstrat-
egy, the timelapse is split into bins covering 1,1,1,..., 1 , 1 of the total
2 4 2N −2 2N −1
interval. The latest event in each bin is kept, yielding a W H N binary stack.
× ×
Concentrated stack [43]. A shallow CNN is trained to process a pre-
computed stack (e.g., an MDES) and aggregate it to a W H 1 data structure.
× ×
Time-Ordered Recent Event (TORE) [3]. It stores event timestamps
into Q per-pixel queues for each polarity, yielding a W H 2Q stack.
× ×
Time Surface [32]. A surface is derived from the timestamp distributions
of the two polarities. S values are sampled for each, yielding a W H 2S stack.
× ×
ERGO-12 [87]. An optimized representation of 12 channels, each built ac-
cording to different strategies from the previous. It yields a W H 12 stack.
× ×
Tencode [26]. A color image representation in which R and B channels
encode positive and negative polarities, with G encoding the timestamp relative
to the total timelapse. It produces an RGB image, i.e. a W H 3 stack.
× ×
We can broadly classify stereo frameworks using these representations into
three categories: i) white boxes, for which we have full access to the implemen-
tation of both the stereo backbone and the stacked event construction; ii) gray
boxes,incasewedonothaveaccesstothestereobackbone;iii)blackboxes,when
the stacked event representation is not accessible neither.
4 Proposed Method
According to the sensor fusion literature for conventional cameras, the main
strategies for combining stereo images with sparse depth measurements from
activesensorsconsistofi)concatenatingthetwomodalitiesandprocessingthem
as joint inputs with a stereo network [12,46,69,81], ii) modulating the internal
costvolumecomputedbythebackboneitself[25,49,69,82]or,morerecently,iii)
projecting distinctive patterns on images according to depth hints [4].
We follow the latter path, since it is more effective and flexible than the
alternatives – which can indeed be applied to white box frameworks only. For
this purpose, we design two alternative strategies suited even for gray and black
box frameworks, respectively, as depicted in Fig. 3.
4.1 Virtual Stack Hallucination – VSH
Givenleftandrightstacks , ofsizeW H CandasetZ ofdepthmeasure-
L R
S S × ×
ments z(x,y) by a sensor, we perform a Virtual Stack Hallucination (VSH), by
augmenting each channel c C, to increase the distinctiveness of local patterns
∈6 L. Bartolomei et al.
(a) (b) (c)
Fig.3: Overview of a generic event-based stereo network and our hallucina-
tion strategies. State-of-the-art event-stereo frameworks (a) pre-process raw events
to obtain event stacks fed to a deep network. In case the stacks are accessible, we
definethemodelasagray box,otherwiseasablack box.Intheformercase(b),wecan
hallucinate patterns directly on it (VSH). When dealing with a black box (c), we can
hallucinate raw events that will be processed to obtain the stacks (BTH).
and thus ease matching. This is carried out by injecting the same virtual stack
(x,y,x ′,c) into L,
R
respectively at coordinates (x,y) and (x ′,y).
A S S
(x,y,c) (x,y,x,c)
L ′
S ←A (1)
(x,y,c) (x,y,x,c)
R ′ ′
S ←A
with x ′ obtained as x d(x,y), with disparity d(x,y) triangulated back from
depth z(x,y) as bf ,− according to the baseline and focal lengths b,f of the
z(x,y)
stereo system. We deploy a generalized version of the random pattern operator
proposed in [4], agnostic to the stacked representation:
A
(x,y,x ′,c) ( −, +) (2)
A ∼U S S
with − and + the minimum and maximum values appearing across stacks
S S
, and a uniform random distribution. Following [4], the pattern can
L R
S S U
either cover a single pixel or a local window. This strategy alone is sufficient al-
ready to ensure distinctiveness and to dramatically ease matching across stacks,
even more than with color images [4], since acting on semi-dense structures –
i.e.,stacksareuninformativeintheabsenceofevents.Italsoensuresastraight-
forwardapplicationofthesameprinciplesusedonRGBimages,e.g.,tocombine
theoriginalcontent(color)withthevirtualprojection(pattern)employingalpha
blending [4]. Nevertheless, we argue that acting at this level i) requires direct
access to the stacks, i.e., a gray-box deep event-stereo network, and ii) might be
sub-optimal as stacks encode only part of the information from streams.
4.2 Back-in-Time Hallucination – BTH
Ahigherdistinctivenesstoeasecorrespondencecanbeinducedbyhallucinating
patterns directly in the continuous events domain. Specifically, we act in the
so-called event history: given a timestamp t at which we want to estimate
dLiDAR-Event Stereo Fusion 7
(densifiedfor visualizationpurpose)
tz= td
tz= td-15
td
td
td
(a)
td
td
td
td
td
td
(b)td
td
td
Fig.4: Overview of Back-in-Time Hallucination (BTH). To estimate disparity
at t d, if LiDAR data is available – e.g., at timestamp t z = t d (green) or t z = t d−15
(yellow) – we can naïvely inject events of random polarities at the same timestamp t z
(a). More advanced injection strategies can be used – e.g. by hallucinating multiple
events, starting from t d, back-in-time at regular intervals (b).
disparity, raw events are sampled from the left and right streams starting from
t andgoingbackward,accordingtoeitherSBNorSBTstackingapproaches,to
d
obtain a pair of event histories = eL N and = eR M , where eL,eR
EL k k=1 ER k k=1 k k
are the k-th left and right events. Events in the history are sorted according to
(cid:8) (cid:9) (cid:8) (cid:9)
theirtimestamp–i.e.,inequalityt t holdsforeverytwoadjacente ,e .
k k+1 k k+1
≤
At this point, we intervene to hallucinate novel events: given a depth mea-
surement z(xˆ,yˆ), triangulated back into disparity d(xˆ,yˆ), we inject a pair of
fictitious events eˆL = (xˆ,yˆ,pˆ,tˆ) and eˆR = (xˆ ′,yˆ,pˆ,tˆ) respectively inside
L
and
, producing ˆ = eL,...,eˆL,...,eL and ˆ = eR,...,eˆR,...,eRE . By
ER EL 1 N ER 1 M
construction, eˆL and eˆR adhere to i) the time ordering constraint, ii) the geom-
(cid:8) (cid:9) (cid:8) (cid:9)
etry constraint xˆ ′ =xˆ d(xˆ,yˆ) and iii) a similarity constraint – i.e., pˆ,tˆare the
−
same for eˆL and eˆR. Fictitious polarity pˆand fictitious timestamp tˆare two de-
greesoffreedomusefultoensuredistinctivenessalongtheepipolarlineandease
matching,accordingtowhichwecanimplementdifferentstrategiessummarized
in Fig. 4, and detailed in the remainder.
Single-timestamp injection. The simplest way to increase distinctiveness
istoinsertsynchronizedeventsatafixedtimestamp.Accordingly,foreachdepth
measurement d(xˆ,yˆ), a total of K pairs of fictitious events are inserted in
xˆ,yˆ
, ,havingpolaritypˆ randomlychosenfromthediscreteset 1,1 .Times-
L R k
E E {− }
tamptˆisfixedandcanbe,forinstance,t atwhichthesensorinfersdepth,that
z
can coincide with timestamp t at which we want to estimate disparity – e.g.,
d
t = t = 0 in the case depicted in Fig. 4 (a). Inspired by [4], events might be
z d
optionally hallucinated in patches rather than single pixels. However, as depth
sensors usually work at a fixed acquisition frequency – e.g., 10Hz for LiDARs –
sparsepointsmightbeunavailableatanyspecifictimestamp.Nonetheless,since
, encode a time interval, we can hallucinate events even if derived from
L R
E E
depth scans performed in the past – e.g., at t < t , – by placing them in the
z d
proper position inside , .
L R
E E
Repeatedinjection.Thepreviousstrategydoesnotexploitoneofthemain
advantages of events over color images, i.e. the temporal dimension, at its best.
Purposely, we design a more advanced hallucination strategy based on repeated
naïve injections performed along the time interval sampled by , . As long
L R
E E
as we are interested in recovering depth at t only, we can hallucinate as many
d8 L. Bartolomei et al.
eventsaswewantinthetimeintervalbefore t–i.e.,fort =t =0,overtheen-
z d
tire interval as shown in Fig. 4 (b) – consistent with the depth measurements at
t itself,whichwillincreasethedistinctivenessintheeventhistoriesandwillease
d
the match by hinting the correct disparity. Inspired by the stacked representa-
tions introduced in Sec. 3, we can design a strategy for injecting multiple events
along the stream. Accordingly, we define the conservative time range [t −,t+] of
the events histories EL, ER, with t − =min tL 0,tR 0 and t+ =max tL N,tR M and
divide it into B equal temporal bins. Then, inspired by MDES [43], we run B
(cid:8) (cid:9) (cid:8) (cid:9)
single-timestamp injections at tˆ
b
= 2b 2−b1(t+ −t −)+t −, with b
∈
{1,...,B }.
Additionally, each depth measurement is used only once – i.e., the number of
fictitious events K in the b-th injection is set as K K δ(b,D )
b,xˆ,yˆ b,xˆ,yˆ xˆ,yˆ xˆ,yˆ
←
where δ(, ) is the Kronecker delta and D
xˆ,yˆ
round(X U(B 1)+1) is a ran-
· · ← −
dom slot assignment. We will show in our experiment how this simple strategy
can improve the results of BTH, in particular increasing its robustness against
misaligned LiDAR data – i.e., measurements retrieved at a timestamp t <t .
z d
5 Experiments
5.1 Implementation and Experimental Settings
We implement VSH and BTH in Python, using the Numba package.
General framework. We build our code base starting from SE-CFF [43] –
state-of-the-art for event-based stereo – assuming the same stereo backbone as
intheirexperiments,i.e.derivedfromAANet[72],andrunSBNtogeneratethe
event history to be stacked. While we select a single architecture, we implement
a variety of stacked representations: purposely, we implement a single instance
of the stereo backbone for any stacked representations introduced in Sec. 3,
takingtheopportunitytoevaluatetheirperformancewiththeeventstereotask.
For Concentration representation, we use MDES as the prior stacking function
following [43] and avoid considering future events during training. Furthermore,
in this case, VSH is applied before the concentration network since it would
interfere with gradient back-propagation during training – while this cannot
occur with BTH. From [4], we adapt occlusion handling and hallucination on
uniform/not uniform patches. We also implement alpha-blending, for VSH only
– as it loses its purpose when acting on the raw streams. For all our methods,
we inherit the same hyper-parameters from [4]; yet, we discard occluded points
as the occlusion handling strategy for BTH since an equivalent strategy to deal
with sparse event histories is not trivial. For VSH on Voxel Grids, we use the
5-thand95-thpercentiletocalculate − and + duetothefrequentpresenceof
S S
extreme values in the stack. For BTH, we perform 12 injections (i.e., B =12).
Existing fusion methodologies. We compare our proposal with existing
methods from the RGB stereo literature, consisting of i) modulating the cost
volume built by the backbone – Guided Stereo Matching [49], ii) concatenating
the sparse depth values to the inputs to the stereo network – e.g., as done
by LidarStereoNet [12], iii) a combination of both the previous strategies – inLiDAR-Event Stereo Fusion 9
DSEC[21] M3ED[9]
Fig.5: Qualitative comparison – DSEC vs M3ED. DSEC features 640×480
eventcamerasanda16-lineLiDAR,M3EDhas1280×720eventcamerasanda64-line
LiDAR. LiDAR scans have been dilated with a 7×7 kernel to ease visualization.
analogy to CCVNorm [69]. Any strategy is adapted to the same common stereo
backbone [43] (see supplementary material). Running BTH and VSH adds
respectively 10ms and 2-15ms (depending on representations) on the CPU.
Training protocol. Any model we train – either the original event stereo
backbones or those implementing fusion strategies – runs for 25 epochs with a
batchsizeof4andamaximumdisparitysetto192.WeuseAdam[29]withbeta
(0.9, 0.999) and weight decay set to 10 −4. The learning rate starts at 5 10 −4
·
and decays with cosine annealing. We apply random crops and vertical flips to
augment data during training.
5.2 Evaluation Datasets & Protocol
We introduce datasets and metrics used in our experiments.
DSEC [21]. An outdoor event stereo dataset, captured using wide-baseline
(50 cm) stereo event cameras at 640 480 resolution. Ground-truth disparity
×
is obtained by accumulating 16-line LiDAR scans, for a total of 26384 maps
organized into 41 sequences. We split them into train/test sets following [43].
From the training set, we retain a further search split for hyper-parameters
tuning and ablation experiments. Sparse LiDAR measurements are obtained by
aligning the raw scans with the ground-truth – both provided by the authors
– by running a LiDAR inertial odometry pipeline followed by ICP registration
(see the supplementary material for details).
M3ED [9]. This dataset provides 57 indoor/outdoor scenes collected with
a compact multi-sensor block mounted on three different vehicles – i.e., a car,
a UAV, and a quadruped robot. A 64-line LiDAR generates semi-dense ground-
truth depth, while the event stereo camera has a shorter baseline (12 cm) and
a higher resolution (1280 720). We use 5 sequences from this dataset for eval-
×
uation purposes only – some of which contain several frames acquired with the
camerasbeingstatic–toevaluatethegeneralizationcapacityofthemodelsboth
to different domains and the density of the LiDAR sensor. Similarly to DSEC,
we derived sparse LiDAR depth maps from the raw scans. Thanks to the SDK
madeavailablebytheauthors,wecouldderiveLiDARmeasurementsalignedto
anydesiredtemporaloffsetaccordingtolinearinterpolationoftheground-truth
poses(seethesupplementary materialfordetails).Thisallowsustorunded-10 L. Bartolomei et al.
Fig.6: Hyperparameters search. Results on DSEC search split. On top, we study
the impact of (a) patch size, (b) uniform patches, and (c) alpha blending on VSH. At
the bottom, we consider (d) single vs repeated injection, (e) patch size, (f) uniform
patches, (g) number of fictitious events, and (h) uniform polarities on BTH.
icated experiments to assess the effect of time-misaligned depth measurements.
Fig. 5 shows a qualitative comparison between the two datasets.
Evaluation Metrics. We compute the percentage of pixels with an error
greater than 1 or 2 pixels (1PE, 2PE), and the mean absolute error (MAE). We
highlight the best and second best methods per row on each metric.
5.3 Ablation Study
Weranhyper-parameterssearchandablationexperimentsforVSHandBTHon
theDSECsearchsplit,reportingthe1PEerror.Weconductedtheseexperiments
using any representation listed in Sec. 3 – except for Concentration [43], which
starts from pre-computed MDES stacks – and report the average results.
VSH. Fig. 6 (top) shows the impact of different hyper-parameters on VSH
strategy. In (a), we can observe how VSH is improved by using 3 3 patches,
×
while5 5cannotyieldfurtherbenefits.Consequently,weselectitasthedefault
×
configuration from now on. In (b), we show that uniform patterns are more
effective than random ones, and in (c) alpha equal to 0.5 works the best.
BTH. Fig. 6 (bottom) focuses on our second strategy. In (d) we show how
repeated injection can improve the results; thus, we select it as the default con-
figuration from now on. In the remainder, we will better appreciate how this
setting is much more robust when dealing with misaligned LiDAR data. Next,
(e) outlines how hallucinating events with 3 3 patches lead to the best results.
×
Applying a uniform patch of events following (f) yields, again, better results.
In (g), we tested different numbers K of injected fictitious events. Injecting
xˆ,yˆ
more than one event is beneficial, yet saturating with two. Finally, (h) shows
that using uniform polarities yields lower errors.
5.4 Experiments on DSEC
We now report experiments on the DSEC testing split, either when applying
fusion strategies to pre-trained stereo models without retraining them or when
training the networks from scratch to exploit LiDAR data.LiDAR-Event Stereo Fusion 11
Table1:ResultsonDSEC[21]–pre-trained.Wetestthedifferentstackedrepre-
sentations(rows)withseveralfusionstrategiesappliedtopre-trainedstereobackbones.
Stacked Baseline Guided[49] VSH(ours) BTH(ours)
representation 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE
(A) Histogram[39] 16.21 4.73 0.74 16.07 4.68 0.73 13.71 4.20 0.69 13.32 3.92 0.66
(B) MDES[43] 15.32 4.40 0.70 15.13 4.34 0.70 12.94 3.52 0.63 12.61 3.50 0.62
(C) Concentration[43] 15.97 4.33 0.70 15.79 4.27 0.70 13.70 3.60 0.65 14.66 3.77 0.66
(D) Voxelgrid[86] 16.49 4.56 0.72 16.29 4.50 0.71 13.12 3.69 0.65 12.44 3.60 0.62
(E) TORE[3] 15.91 4.57 0.71 15.72 4.50 0.71 12.53 3.65 0.63 12.27 3.68 0.62
(F) TimeSurface[32] 15.33 4.29 0.70 15.18 4.24 0.69 12.16 3.38 0.62 12.28 3.45 0.62
(G) ERGO-12[87] 15.02 4.20 0.68 14.87 4.14 0.68 12.02 3.40 0.61 11.98 3.42 0.61
(H) Tencode[26] 14.46 4.17 0.68 14.29 4.11 0.67 12.12 3.37 0.61 11.86 3.45 0.61
Avg.Rank. - 3.00 3.00 3.00 1.75 1.38 1.50 1.25 1.63 1.13
Table 2: Results on DSEC [21] – retrained. We test different stacked represen-
tations (rows) with several fusion strategies applied during training.
Concat[12] Guided+Concat[69] Guided[49] VSH(ours) BTH(ours)
1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE
(A) 12.57 3.37 0.62 12.81 3.41 0.63 15.57 4.58 0.72 9.90 3.26 0.53 10.91 3.41 0.59
(B) 12.37 3.17 0.61 12.40 3.25 0.61 14.66 4.36 0.70 9.31 3.01 0.51 9.62 3.01 0.54
(C) 12.38 3.41 0.63 12.74 3.44 0.66 15.15 4.45 0.71 9.70 3.04 0.53 9.66 2.98 0.55
(D) 12.23 3.18 0.60 11.90 3.10 0.60 14.52 4.21 0.68 10.16 3.20 0.56 9.68 2.90 0.54
(E) 12.99 3.33 0.62 12.62 3.25 0.61 16.00 4.56 0.73 9.91 3.05 0.53 9.83 2.98 0.54
(F) 12.18 3.09 0.61 12.47 3.17 0.61 14.40 4.21 0.68 9.47 2.90 0.52 9.58 2.92 0.54
(G) 12.43 3.14 0.61 12.82 3.19 0.62 13.85 3.97 0.66 9.25 2.88 0.50 9.37 2.87 0.54
(H) 11.95 3.08 0.60 11.75 3.10 0.60 14.72 4.21 0.69 9.39 3.00 0.52 9.59 2.97 0.55
3.38 3.00 3.13 3.63 3.50 3.38 5.00 5.00 5.00 1.38 1.88 1.13 1.63 1.38 1.88
Pre-trained models. Tab. 1 reports, on each row, the results yielded by
using a specific stacked representation. In the columns, we report the different
fusion strategies involved in our experiments, starting with the baseline – i.e.,
a stereo backbone processing events only. In the last row, we report the average
ranking – for the three metrics – achieved by any fusion strategy over the eight
representations. Starting from baseline models, we can notice how the different
representations have an impact on the accuracy of the stereo backbone, with
those modeling complex behaviors – e.g., Time Surface [32] or ERGO-12 [87]
– yielding up to 2% lower 1PE than simpler ones such as Histogram [39]. The
Guided framework [49] can improve the results only moderately: this is caused
by the very sparse measurements retrieved from the 16-line LiDAR sensor used
in DSEC, as well as by the limited effect of the cost volume modulation in
regionswhereeventsarenotavailableformatching.Nonetheless,VSHandBTH
consistently outperform Guided, always improving the baseline by 2-3% points
on1PE.Ingeneral,BTHachievesthebest1PEandMAEmetricsinmostcases;
this strategy is the best when re-training the stereo backbone is not feasible.
Training from scratch.Tab.2reportstheresultsobtainedbytrainingthe
stereobackbonesfromscratchtoperformLiDAR-eventstereofusion.Thisallows
eitherthedeploymentofstrategiesthatprocesstheLiDARdatadirectlyasinput
[12,69] or those not requiring it, i.e., [49] and ours. Specifically, Concat [12] and
Guided+Concat [69] strategy achieve results comparable to those by VSH and
BTH observed before, thus outperforming Guided [49] which, on the contrary,
cannot benefit much from the training process. When deploying our solutions
during training, their effectiveness dramatically increases, often dropping 1PE12 L. Bartolomei et al.
Table 3: Results on M3ED [9] – pre-trained.Wetestthedifferentstackedrepre-
sentations(rows)withseveralfusionstrategiesappliedtopre-trainedstereobackbones.
Stacked Baseline Guided[49] VSH(ours) BTH(ours)
representation 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE
(A) Histogram[39] 37.70 19.49 1.76 37.18 19.29 1.75 20.19 11.19 1.19 22.32 12.37 1.27
(B) MDES[43] 43.17 19.50 1.85 42.27 19.16 1.83 29.42 14.80 1.52 22.58 12.20 1.30
(C) Concentration[43] 45.78 20.84 1.82 45.06 20.57 1.80 33.63 16.19 1.53 25.22 12.68 1.28
(D) Voxelgrid[86] 37.33 17.66 1.70 36.64 17.38 1.68 20.40 11.41 1.22 20.94 11.72 1.23
(E) TORE[3] 41.70 19.09 1.81 41.00 18.78 1.80 28.25 14.01 1.47 21.91 12.34 1.30
(F) TimeSurface[32] 38.58 18.52 1.72 37.91 18.23 1.70 24.89 13.34 1.37 22.60 12.77 1.31
(G) ERGO-12[87] 36.33 17.81 1.66 35.61 17.50 1.64 22.53 12.33 1.26 20.41 11.69 1.21
(H) Tencode[26] 43.56 20.07 1.82 42.66 19.76 1.80 28.24 14.46 1.43 22.61 12.75 1.26
Avg.Rank. - 3.00 3.00 3.00 1.75 1.75 1.75 1.25 1.25 1.25
Table 4: Results on M3ED [9] – retrained. We test different stacked representa-
tions (rows) with several fusion strategies applied during training.
Concat[12] Guided+Concat[69] Guided[49] VSH(ours) BTH(ours)
1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE
(A) 34.67 15.21 1.92 38.65 17.00 1.94 37.45 18.98 1.76 19.34 12.93 1.46 19.83 13.20 1.39
(B) 37.72 16.91 1.85 37.32 17.16 2.14 37.00 18.66 1.76 19.24 13.17 1.44 18.70 11.79 1.24
(C) 39.88 19.01 2.33 38.45 17.76 2.47 38.14 19.62 1.80 19.84 13.68 1.90 19.46 12.29 1.35
(D) 33.89 16.21 1.89 33.54 15.85 1.75 37.85 18.81 1.74 18.56 11.76 1.32 21.02 14.30 1.80
(E) 38.83 18.38 2.27 35.80 16.63 2.05 40.51 19.96 1.95 20.03 13.97 1.86 20.04 12.65 1.39
(F) 40.26 18.44 2.19 35.48 17.74 2.15 38.77 18.41 1.75 19.61 13.01 1.55 21.91 14.33 1.72
(G) 42.43 19.31 2.31 42.24 18.42 2.34 37.95 17.83 1.76 18.45 12.31 1.55 19.12 11.60 1.24
(H) 37.46 17.87 2.15 33.69 16.47 1.95 39.78 19.42 1.82 19.49 12.21 1.38 19.28 11.68 1.33
4.38 4.00 4.50 3.63 3.38 4.38 4.00 4.63 2.75 1.38 1.63 1.88 1.63 1.38 1.50
error below 10%. VSH often yields the best 1PE and MAE overall, nominating
it as the most effective – yet intrusive – among our solutions.
5.5 Experiments on M3ED
We test the effectiveness of BTH and alternative approaches on M3ED, using
the backbones trained on DSEC without any fine-tuning on M3ED itself.
Pre-trained models.Tab.3collectstheoutcomeofthisexperimentbyap-
plying Guided, VSH, and BTH to pre-trained models. Looking at the baselines,
wecanappreciatehowM3EDisverychallengingformodelstrainedinadifferent
domain, with 1PE errors higher than 30%. This is caused by both the domain
shiftandthehigherresolutionoftheeventcamerasused.Evenso,complexevent
representations – e.g., ERGO-12 [87] – can better generalize. Guided confirms
its limited impact, this time mainly because of the ineffectiveness of the cost
volume modulation in the absence of any information from the events domain.
On the contrary, we can appreciate even further the impact of VSH and BTH,
almost halving the 1PE error. Specifically, BTH is the absolute winner with 6
out of 8 representations, and the best choice for pre-trained frameworks.
Training from scratch. Tab. 4 resumes the results obtained when train-
ingonDSECthebackbonesimplementingLiDAR-eventstereofusionstrategies.
The very different distribution of depth points observed across the two datasets
– sourced respectively from 16 and 64-line LiDARs – yields mixed results for
existing methods [12,49,69], with rare cases for which they fail to improve
the baseline model (e.g., Concat and Guided with Time Surface and ERGO-LiDAR-Event Stereo Fusion 13
Events&LiDAR Baseline Guided[49] BTH(ours) BTH(ours,retrain)
1PE:50.84% 1PE:48.94% 1PE:32.16% 1PE:22.83%
Events&LiDAR Baseline Guided[49] VSH(ours) BTH(ours)
1PE:53.19% 1PE:50.26% 1PE:10.94% 1PE:13.50%
Fig.7:Qualitativeresults.ResultsonDSECzurich_10_b withVoxelgrid[86](top)
and M3ED spot_indoor_obstacles with Histogram [39] (bottom).
12, Guided+Concat with Histogram). On the contrary, backbones trained with
VSH and BTH consistently improve over the baseline, often with larger gains
compared to their use with pre-trained models. Overall, BTH is the best on
2PE and MAE, confirming it is better suited for robustness across domains and
different LiDAR sensors.
Fig.7showsqualitativeresults.OnDSEC(top),BTHdramaticallyimproves
resultsoverthebaselineandGuided,yetcannotfullyrecoversomedetailsinthe
scene except when retraining the stereo backbone. On M3ED (bottom), both
VSH and BTH with pre-trained models reduce the error by 5 .
×
5.6 Experiments on M3ED – Time-misaligned LiDAR
Weconcludebyassessingtherobustnessoftheconsideredstrategiesagainstthe
useofLiDARnotsynchronizedwiththetimestampatwhichwewishtoestimate
disparity – occurring if we wish to maintain the microsecond resolution of the
event cameras. Purposely, we extract raw LiDAR measurements collected 3, 13,
32, 61, and 100 ms in the past with the M3ED SDK.
Fig. 8 shows the trend of the 1PE metric achieved by Guided (red), VSH
(yellow)andBTH(blackandgreen)onpre-trainedbackbones.Notsurprisingly,
the error rates arise at the increase of the temporal distance: while this is less
evidentwithGuidedbecauseofitslimitedimpact,thisbecomesclearwithVSH
andBTH.Nonetheless,bothcanalwaysretainasignificantgainoverthebaseline
model (blue) – i.e., the stereo backbone processing events only – even with the
farthest possible misalignment with a 10Hz LiDAR (100ms). We can appreciate
how BTH is often better than VSH (coherently with Tab. 3), yet only when
repeatedinjectionsareperformed(green).Indeed,usingasingleinjection(black)14 L. Bartolomei et al.
Fig.8: Experiments with time-misaligned LiDAR on M3ED [9] – pre-
trained. We measure the robustness of different fusion strategies against the use of
out-of-sync LiDAR data, without retraining the stereo backbone.
Fig.9: Experiments with time-misaligned LiDAR on M3ED [9] – retrained.
We measure the robustness of different fusion strategies against the use of out-of-sync
LiDAR data when training the stereo backbone from scratch.
rapidlyleadsBTHtoanaccuracydropwhenincreasingthemisalignment,except
when using Histogram representation. Overall, BTH with ERGO-12 is the most
robust solution. Fig. 9 shows the results achieved by VSH (yellow) and BTH
(green) after retraining, against the best competitor according to average ranks
in Tab. 4 – i.e., Guided+Concat (red). The impact of this latter is limited and
sometimes fails to improve the baseline (see Histogram and ERGO-12). On the
contrary, our solutions confirm their robustness and effectiveness even when
dealing with time-misaligned LiDAR data.
6 Conclusion
This paper proposes a novel framework for implementing event stereo and Li-
DAR fusion. It works by hallucinating fictitious events either in the stacked
representation processed by stereo backbones or the continuous streams sensed
by event cameras, easing the matching process to the downstream stereo model
estimating disparity. Our exhaustive experiments prove that our solutions, VSH
and BTH, dramatically outperform alternative fusion strategies from the RGB
stereo literature, retaining the microsecond resolution typical of event cameras
despite the discrete frame rate of LiDARs and depth sensors in general.
Limitations. Despite the robustness shown with misaligned LiDAR data, a
marginaldropinaccuracycomparedtothecaseofhavingLiDARmeasurements
at the very same timestamp at which we aim to infer disparity maps occurs.
Future work will focus on studying new design mechanisms to deal with it.LiDAR-Event Stereo Fusion 15
Acknowledgement. This study was carried out within the MOST – Sus-
tainable Mobility National Research Center and received funding from the Eu-
ropeanUnionNext-GenerationEU–PIANONAZIONALEDIRIPRESAERE-
SILIENZA(PNRR)–MISSIONE4COMPONENTE2,INVESTIMENTO1.4–
D.D. 1033 17/06/2022, CN00000023. This manuscript reflects only the authors’
views and opinions, neither the European Union nor the European Commission
can be considered responsible for them.
We acknowledge the CINECA award under the ISCRA initiative, for the
availability of high-performance computing resources and support.
References
1. Andreopoulos, A., Kashyap, H.J., Nayak, T.K., Amir, A., Flickner, M.D.: A low
power, high throughput, fully event-based stereo system. In: Proceedings of the
IEEEconferenceoncomputervisionandpatternrecognition.pp.7532–7542(2018)
2. Badino, H., Huber, D.F., Kanade, T.: Integrating lidar into stereo for fast and
improved disparity computation. 2011 International Conference on 3D Imaging,
Modeling, Processing, Visualization and Transmission pp. 405–412 (2011)
3. Baldwin, R.W., Liu, R., Almatrafi, M., Asari, V., Hirakawa, K.: Time-ordered re-
centevent(tore)volumesforeventcameras.IEEETransactionsonPatternAnal-
ysis and Machine Intelligence 45(2), 2519–2532 (2022)
4. Bartolomei,L.,Poggi,M.,Tosi,F.,Conti,A.,Mattoccia,S.:Activestereowithout
pattern projector. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV). pp. 18470–18482 (October 2023)
5. Boykov, Y., Veksler, O., Zabih, R.: Fast approximate energy minimization via
graphcuts.IEEETransactionsonpatternanalysisandmachineintelligence23(11),
1222–1239 (2001)
6. Brebion,V.,Moreau,J.,Davoine,F.:Learningtoestimatetwodensedepthsfrom
lidarandeventdata.In:ScandinavianConferenceonImageAnalysis.pp.517–533.
Springer (2023)
7. Camuñas-Mesa, L.A., Serrano-Gotarredona, T., Ieng, S.H., Benosman, R.B.,
Linares-Barranco, B.: On the use of orientation filters for 3d reconstruction in
event-driven stereo vision. Frontiers in neuroscience 8, 48 (2014)
8. Carneiro, J., Ieng, S.H., Posch, C., Benosman, R.: Event-based 3d reconstruction
from neuromorphic retinas. Neural Networks 45, 27–38 (2013)
9. Chaney, K., Cladera, F., Wang, Z., Bisulco, A., Hsieh, M.A., Korpela, C., Ku-
mar, V., Taylor, C.J., Daniilidis, K.: M3ed: Multi-robot, multi-sensor, multi-
environmenteventdataset.In:ProceedingsoftheIEEE/CVFConferenceonCom-
puter Vision and Pattern Recognition (CVPR) Workshops. pp. 4015–4022 (June
2023)
10. Chang,J.R.,Chen,Y.S.:Pyramidstereomatchingnetwork.In:IEEE/CVFConfer-
enceonComputerVisionandPatternRecognition(CVPR).pp.5410–5418(2018)
11. Cheng, X., Wang, P., Yang, R.: Learning depth with convolutional spatial prop-
agation network. IEEE transactions on pattern analysis and machine intelligence
42(10), 2361–2379 (2019)
12. Cheng,X.,Zhong,Y.,Dai,Y.,Ji,P.,Li,H.:Noise-awareunsuperviseddeeplidar-
stereo fusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 6339–6348 (2019)16 L. Bartolomei et al.
13. Cheng, X., Zhong, Y., Harandi, M., Dai, Y., Chang, X., Li, H., Drummond, T.,
Ge,Z.:Hierarchicalneuralarchitecturesearchfordeepstereomatching.Advances
in Neural Information Processing Systems 33 (2020)
14. Cui, M., Zhu, Y., Liu, Y., Liu, Y., Chen, G., Huang, K.: Dense depth-map esti-
mation based on fusion of event camera and sparse lidar. IEEE Transactions on
Instrumentation and Measurement 71, 1–11 (2022). https://doi.org/10.1109/
TIM.2022.3144229
15. Dikov,G.,Firouzi,M.,Röhrbein,F.,Conradt,J.,Richter,C.:Spikingcooperative
stereo-matchingat2mslatencywithneuromorphichardware.In:Biomimeticand
BiohybridSystems:6thInternationalConference,LivingMachines2017,Stanford,
CA, USA, July 26–28, 2017, Proceedings 6. pp. 119–137. Springer (2017)
16. Duggal, S., Wang, S., Ma, W.C., Hu, R., Urtasun, R.: Deeppruner: Learning
efficient stereo matching via differentiable patchmatch. In: Proceedings of the
IEEE/CVF international conference on computer vision. pp. 4384–4393 (2019)
17. Gallego, G., Delbrück, T., Orchard, G., Bartolozzi, C., Taba, B., Censi, A.,
Leutenegger, S., Davison, A.J., Conradt, J., Daniilidis, K., et al.: Event-based
vision: A survey. IEEE transactions on pattern analysis and machine intelligence
44(1), 154–180 (2020)
18. Gallego, G., Delbrück, T., Orchard, G., Bartolozzi, C., Taba, B., Censi, A.,
Leutenegger,S.,Davison,A.J.,Conradt,J.,Daniilidis,K.,Scaramuzza,D.:Event-
basedvision:Asurvey.IEEETransactionsonPatternAnalysisandMachineIntel-
ligence 44(1), 154–180 (2022). https://doi.org/10.1109/TPAMI.2020.3008413
19. Gandhi,V.,Čech,J.,Horaud,R.:High-resolutiondepthmapsbasedontof-stereo
fusion. In: 2012 IEEE International Conference on Robotics and Automation. pp.
4742–4749. IEEE (2012)
20. Gao, L., Liang, Y., Yang, J., Wu, S., Wang, C., Chen, J., Kneip, L.: Vector: A
versatile event-centric benchmark for multi-sensor slam. IEEE Robotics and Au-
tomation Letters 7(3), 8217–8224 (2022)
21. Gehrig, M., Aarents, W., Gehrig, D., Scaramuzza, D.: Dsec: A stereo event cam-
era dataset for driving scenarios. IEEE Robotics and Automation Letters (2021).
https://doi.org/10.1109/LRA.2021.3068942
22. Guo,W.,Li,Z.,Yang,Y.,Wang,Z.,Taylor,R.H.,Unberath,M.,Yuille,A.,Li,Y.:
Context-enhancedstereotransformer.In:ProceedingsoftheEuropeanConference
on Computer Vision (ECCV) (2022)
23. Guo, X., Yang, K., Yang, W., Wang, X., Li, H.: Group-wise correlation stereo
network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 3273–3282 (2019)
24. Hirschmuller, H.: Stereo processing by semiglobal matching and mutual informa-
tion. IEEE Transactions on pattern analysis and machine intelligence 30(2), 328–
341 (2007)
25. Huang,Y.K.,Liu,Y.C.,Wu,T.H.,Su,H.T.,Chang,Y.C.,Tsou,T.L.,Wang,Y.A.,
Hsu, W.H.: S3: Learnable sparse signal superdensity for guided depth estimation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 16706–16716 (2021)
26. Huang, Z., Sun, L., Zhao, C., Li, S., Su, S.: Eventpoint: Self-supervised interest
point detection and description for event-based camera. In: Proceedings of the
IEEE/CVFWinterConferenceonApplicationsofComputerVision(WACV).pp.
5396–5405 (January 2023)
27. Kendall,A.,Martirosyan,H.,Dasgupta,S.,Henry,P.,Kennedy,R.,Bachrach,A.,
Bry, A.: End-to-end learning of geometry and context for deep stereo regression.
In: The IEEE International Conference on Computer Vision (ICCV) (Oct 2017)LiDAR-Event Stereo Fusion 17
28. Khamis,S.,Fanello,S.,Rhemann,C.,Kowdle,A.,Valentin,J.,Izadi,S.:Stereonet:
Guidedhierarchicalrefinementforreal-timeedge-awaredepthprediction.In:Pro-
ceedings of the European Conference on Computer Vision (ECCV). pp. 573–590
(2018)
29. Kingma,D.P.,Ba,J.:Adam:Amethodforstochasticoptimization.arXivpreprint
arXiv:1412.6980 (2014)
30. Kogler, J., Sulzbachner, C., Humenberger, M., Eibensteiner, F.: Address-event
based stereo vision with bio-inspired silicon retina imagers. Advances in theory
and applications of stereo vision pp. 165–188 (2011)
31. Kolmogorov, V., Zabin, R.: What energy functions can be minimized via graph
cuts? IEEE transactions on pattern analysis and machine intelligence 26(2), 147–
159 (2004)
32. Lagorce, X., Orchard, G., Galluppi, F., Shi, B.E., Benosman, R.B.: Hots: a hier-
archy of event-based time-surfaces for pattern recognition. IEEE transactions on
pattern analysis and machine intelligence 39(7), 1346–1359 (2016)
33. Li,B.,Meng,H.,Zhu,Y.,Song,R.,Cui,M.,Chen,G.,Huang,K.:Enhancing3-d
lidarpointcloudswithevent-basedcamera.IEEETransactionsonInstrumentation
and Measurement 70, 1–12 (2021)
34. Li, J., Wang, P., Xiong, P., Cai, T., Yan, Z., Yang, L., Liu, J., Fan, H., Liu, S.:
Practical stereo matching via cascaded recurrent network with adaptive correla-
tion.In:ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition. pp. 16263–16272 (2022)
35. Li, Z., Liu, X., Drenkow, N., Ding, A., Creighton, F.X., Taylor, R.H., Unberath,
M.: Revisiting stereo depth estimation from a sequence-to-sequence perspective
withtransformers.In:ProceedingsoftheIEEE/CVFInternationalConferenceon
Computer Vision. pp. 6197–6206 (2021)
36. Liang, C.K., Cheng, C.C., Lai, Y.C., Chen, L.G., Chen, H.H.: Hardware-efficient
beliefpropagation.IEEETransactionsonCircuitsandSystemsforVideoTechnol-
ogy 21(5), 525–537 (2011)
37. Liang, Z., Feng, Y., Guo, Y., Liu, H., Chen, W., Qiao, L., Zhou, L., Zhang, J.:
Learning for disparity estimation through feature constancy. In: Proceedings of
theIEEEConferenceonComputerVisionandPatternRecognition(CVPR)(June
2018)
38. Lipson,L.,Teed,Z.,Deng,J.:Raft-stereo:Multilevelrecurrentfieldtransformsfor
stereo matching. In: International Conference on 3D Vision (3DV) (2021)
39. Maqueda, A.I., Loquercio, A., Gallego, G., García, N., Scaramuzza, D.: Event-
based vision meets deep learning on steering prediction for self-driving cars. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 5419–5427 (2018)
40. Marin, G., Zanuttigh, P., Mattoccia, S.: Reliable fusion of tof and stereo depth
driven by confidence measures. In: European Conference on Computer Vision
(ECCV). pp. 386–401 (2016)
41. Marr, D.C., Poggio, T.A.: Cooperative computation of stereo disparity. Science
194 4262, 283–7 (1976)
42. Mayer, N., Ilg, E., Hausser, P., Fischer, P., Cremers, D., Dosovitskiy, A., Brox,
T.: A large datasetto train convolutional networks for disparity,optical flow, and
sceneflowestimation. In:The IEEEConference onComputerVision and Pattern
Recognition (CVPR) (June 2016)
43. Nam, Y., Mostafavi, M., Yoon, K.J., Choi, J.: Stereo depth from events cameras:
Concentrateandfocusonthefuture.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 6114–6123 (2022)18 L. Bartolomei et al.
44. Osswald, M., Ieng, S.H., Benosman, R., Indiveri, G.: A spiking neural network
modelof3dperceptionforevent-basedneuromorphicstereovisionsystems.Scien-
tific reports 7(1), 40703 (2017)
45. Pang,J.,Sun,W.,Ren,J.S.,Yang,C.,Yan,Q.:Cascaderesiduallearning:Atwo-
stageconvolutionalneuralnetworkforstereomatching.In:TheIEEEInternational
Conference on Computer Vision (ICCV) (Oct 2017)
46. Park,K.,Kim,S.,Sohn,K.:High-precisiondepthestimationwiththe3dlidarand
stereofusion.In:2018IEEEInternationalConferenceonRoboticsandAutomation
(ICRA). pp. 2156–2163. IEEE (2018)
47. Piatkowska, E., Belbachir, A., Gelautz, M.: Asynchronous stereo vision for event-
driven dynamic stereo sensor using an adaptive cooperative approach. In: Pro-
ceedings of the IEEE International Conference on Computer Vision Workshops.
pp. 45–50 (2013)
48. Poggi,M.,Agresti,G.,Tosi,F.,Zanuttigh,P.,Mattoccia,S.:Confidenceestimation
for tof and stereo sensors and its application to depth data fusion. IEEE Sensors
Journal20(3),1411–1421(2020).https://doi.org/10.1109/JSEN.2019.2946591
49. Poggi, M., Pallotti, D., Tosi, F., Mattoccia, S.: Guided stereo matching. In: Pro-
ceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.
pp. 979–988 (2019)
50. Poggi,M.,Tosi,F.:Federatedonlineadaptationfordeepstereo.In:CVPR(2024)
51. Poggi, M., Tosi, F., Batsos, K., Mordohai, P., Mattoccia, S.: On the synergies
between machine learning and binocular stereo for depth estimation from images:
Asurvey.IEEETransactionsonPatternAnalysisandMachineIntelligence44(9),
5314–5334 (2022)
52. Rogister, P., Benosman, R., Ieng, S.H., Lichtsteiner, P., Delbruck, T.: Asyn-
chronous event-based binocular stereo matching. IEEE Transactions on Neural
Networks and Learning Systems 23(2), 347–353 (2011)
53. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-
ical image segmentation. In: Medical Image Computing and Computer-Assisted
Intervention–MICCAI2015:18thInternationalConference,Munich,Germany,Oc-
tober 5-9, 2015, Proceedings, Part III 18. pp. 234–241. Springer (2015)
54. Saikia, T., Marrakchi, Y., Zela, A., Hutter, F., Brox, T.: Autodispnet: Improving
disparityestimationwithautoml.In:ProceedingsoftheIEEE/CVFInternational
Conference on Computer Vision. pp. 1812–1823 (2019)
55. Saucedo, M.A., Patel, A., Sawlekar, R., Saradagi, A., Kanellakis, C., Agha-
Mohammadi, A.A., Nikolakopoulos, G.: Event camera and lidar based human
tracking for adverse lighting conditions in subterranean environments. IFAC-
PapersOnLine 56(2), 9257–9262 (2023)
56. Schraml, S., Belbachir, A.N., Milosevic, N., Schön, P.: Dynamic stereo vision sys-
temforreal-timetracking.In:Proceedingsof2010IEEEInternationalSymposium
on Circuits and Systems. pp. 1409–1412. IEEE (2010)
57. Shen,Z.,Dai,Y.,Rao,Z.:Cfnet:Cascadeandfusedcostvolumeforrobuststereo
matching.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition (CVPR). pp. 13906–13915 (June 2021)
58. Song,R.,Jiang,Z.,Li,Y.,Shan,Y.,Huang,K.:Calibrationofevent-basedcamera
and 3d lidar. In: 2018 WRC Symposium on Advanced Robotics and Automation
(WRC SARA). pp. 289–295. IEEE (2018)
59. Song, X., Zhao, X., Hu, H., Fang, L.: Edgestereo: A context integrated residual
pyramid network for stereo matching. In: ACCV (2018)LiDAR-Event Stereo Fusion 19
60. Sulzbachner,C.,Zinner,C.,Kogler,J.:Anoptimizedsiliconretinastereomatching
algorithm using time-space correlation. In: CVPR 2011 WORKSHOPS. pp. 1–7.
IEEE (2011)
61. Ta,K.,Bruggemann,D.,Brödermann,T.,Sakaridis,C.,VanGool,L.:L2e:Lasers
toeventsfor6-dofextrinsiccalibrationoflidarsandeventcameras.In:2023IEEE
International Conference on Robotics and Automation (ICRA). pp. 11425–11431.
IEEE (2023)
62. Taniai,T.,Matsushita,Y.,Naemura,T.:Graphcutbasedcontinuousstereomatch-
ing using locally shared labels. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition. pp. 1613–1620 (2014)
63. Tankovich, V., Hane, C., Zhang, Y., Kowdle, A., Fanello, S., Bouaziz, S.: Hit-
net: Hierarchical iterative tile refinement network for real-time stereo matching.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 14362–14372 (June 2021)
64. Tonioni, A., Tosi, F., Poggi, M., Mattoccia, S., Stefano, L.D.: Real-time self-
adaptive deep stereo. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) (June 2019)
65. Tosi, F., Tonioni, A., De Gregorio, D., Poggi, M.: Nerf-supervised deep stereo.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 855–866 (June 2023)
66. Tulyakov, S., Fleuret, F., Kiefel, M., Gehler, P., Hirsch, M.: Learning an event
sequence embedding for dense event-based deep stereo. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 1527–1537 (2019)
67. Uddin,S.N.,Ahmed,S.H.,Jung,Y.J.:Unsuperviseddeepeventstereofordepthes-
timation.IEEETransactionsonCircuitsandSystemsforVideoTechnology32(11),
7489–7504 (2022)
68. Veksler, O.: Stereo correspondence by dynamic programming on a tree. In: 2005
IEEEComputerSocietyConferenceonComputerVisionandPatternRecognition
(CVPR’05). vol. 2, pp. 384–390. IEEE (2005)
69. Wang, T.H., Hu, H.N., Lin, C.H., Tsai, Y.H., Chiu, W.C., Sun, M.: 3d lidar and
stereofusionusingstereomatchingnetworkwithconditionalcostvolumenormal-
ization. In: 2019 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS). pp. 5895–5902. IEEE (2019)
70. Wang, Y., Lai, Z., Huang, G., Wang, B.H., Van Der Maaten, L., Campbell, M.,
Weinberger, K.Q.: Anytime stereo image depth estimation on mobile devices. In:
2019 International Conference on Robotics and Automation (ICRA). pp. 5893–
5900 (2019)
71. Xu, G., Wang, X., Ding, X., Yang, X.: Iterative geometry encoding volume for
stereo matching. In: Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition. pp. 21919–21928 (2023)
72. Xu,H.,Zhang,J.:Aanet:Adaptiveaggregationnetworkforefficientstereomatch-
ing.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition. pp. 1959–1968 (2020)
73. Yang,G.,Manela,J.,Happold,M.,Ramanan,D.:Hierarchicaldeepstereomatch-
ing on high-resolution images. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 5515–5524 (2019)
74. Yang,G.,Zhao,H.,Shi,J.,Deng,Z.,Jia,J.:Segstereo:Exploitingsemanticinfor-
mation for disparity estimation. In: ECCV. pp. 636–651 (2018)
75. Yang, Q., Wang, L., Ahuja, N.: A constant-space belief propagation algorithm for
stereomatching.In:2010IEEEComputerSocietyConferenceonComputerVision
and Pattern Recognition. pp. 1458–1465. IEEE (2010)20 L. Bartolomei et al.
76. Yang, Q., Wang, L., Yang, R., Stewénius, H., Nistér, D.: Stereo matching with
color-weightedcorrelation,hierarchicalbeliefpropagation,andocclusionhandling.
IEEE transactions on pattern analysis and machine intelligence 31(3), 492–504
(2008)
77. Yin, Z., Darrell, T., Yu, F.: Hierarchical discrete distribution decomposition for
matchdensityestimation.In:ProceedingsoftheIEEE/CVFConferenceonCom-
puter Vision and Pattern Recognition. pp. 6044–6053 (2019)
78. Zabih,R.,Woodfill,J.:Non-parametriclocaltransformsforcomputingvisualcor-
respondence. In: Third European Conference on Computer Vision (Vol. II). pp.
151–158.3rdEuropeanConferenceonComputerVision(ECCV),Springer-Verlag
New York, Inc., Secaucus, NJ, USA (1994)
79. Zbontar, J., LeCun, Y., et al.: Stereo matching by training a convolutional neural
networktocompareimagepatches.J.Mach.Learn.Res.17(1),2287–2318(2016)
80. Zhang, F., Prisacariu, V., Yang, R., Torr, P.H.: GA-Net: Guided aggregation net
for end-to-end stereo matching. In: IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) (2019)
81. Zhang, J., Ramanagopal, M.S., Vasudevan, R., Johnson-Roberson, M.: Listereo:
Generate dense depth maps from lidar and stereo imagery. In: 2020 IEEE Inter-
national Conference on Robotics and Automation (ICRA). pp. 7829–7836. IEEE
(2020)
82. Zhang, Y., Zou, S., Liu, X., Huang, X., Wan, Y., Yao, Y.: Lidar-guided stereo
matchingwithaspatialconsistencyconstraint.ISPRSJournalofPhotogrammetry
and Remote Sensing 183, 164–177 (2022)
83. Zhao,H.,Zhou,H.,Zhang,Y.,Chen,J.,Yang,Y.,Zhao,Y.:High-frequencystereo
matching network. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 1327–1336 (2023)
84. Zhao, H., Zhou, H., Zhang, Y., Zhao, Y., Yang, Y., Ouyang, T.: Eai-stereo: Error
aware iterative network for stereo matching. In: Proceedings of the Asian Confer-
ence on Computer Vision. pp. 315–332 (2022)
85. Zhou, Y., Gallego, G., Rebecq, H., Kneip, L., Li, H., Scaramuzza, D.: Semi-dense
3d reconstruction with a stereo event camera. In: Proceedings of the European
conference on computer vision (ECCV). pp. 235–251 (2018)
86. Zhu,A.Z.,Yuan,L.,Chaney,K.,Daniilidis,K.:Unsupervisedevent-basedlearning
of optical flow, depth, and egomotion. In: Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. pp. 989–997 (2019)
87. Zubić,N.,Gehrig,D.,Gehrig,M.,Scaramuzza,D.:Fromchaoscomesorder:Order-
ing event representations for object recognition and detection. In: Proceedings of
theIEEE/CVFInternationalConferenceonComputerVision(ICCV).pp.12846–
12856 (October 2023)21
LiDAR-Event Stereo Fusion with Hallucinations
Supplementary Material
This document provides additional details regarding ECCV 2024 paper
“LiDAR-Event Stereo Fusion with Hallucinations". Specifically, we report:
– page 21: the impact of our framework on event streams distinctiveness
– pages 22-25:thecompositionofthedatasetssplitusedinourexperiments,
as well as the pre-processing pipeline necessary to obtain data suitable for
our purposes from both DSEC [7] and M3ED [5]
– pages 26-27: detailed description of the LiDAR-stereo fusion strategies in-
herited from classical deep stereo literature [6,13,16]
– page 28: additional ablation studies
– pages 29-32: more qualitative results on the M3ED dataset [5]
DS(P) = mini |Qi - P| DS(P) = 0 ⟺ Qk = P
|Q -0-P| |Q -1-P| |Q -2-P| |Q -3-P| |Q -4-P| |Q -5-P| |Q -6-P| |Q -7-P| P VSH / BTH Q k P
DS(P) > 0 ⟺ Qk ≠ P
P’ Q k P’
Q 0 Q 1 Q 2 Q 3 P Q 4 Q 5 Q 6 Q 7 with k = argmini |Qi – P| Single-patch Stereo Correlation Sd ci osp ra er sity
(a) (b)
Fig.I: Distinctiveness before/after VSH/BTH (a), correlation scores (b).
1 Impact on Distinctiveness
In this section, we discuss the reasons behind the effectiveness of our halluci-
nation strategies from a probabilistic perspective. In the stereo matching tasks,
ambiguities arise when the patches to match are not distinctive – i.e., multiple
patchesalongthehorizontalepipolarlineareidentical.ForasinglepatchP and
itsneighborsQ ,wecandefineitsdistinctiveness[10]DS(P)asmin P Q .
i i i
| − |
Such distinctiveness is 0 if at least one Q is identical to P – see Fig. I (a). This
i
is very likely to occur for patches for which no event at all is triggered by the
camera, as for the 30% of the patches on the M3ED dataset.
≈
As VSH puts a random P ′ on both left and right frames, it will be suffi-
cient for P ′ to have DS(P ′) > 0 to ease the matching of those empty patches.
A sibs lepe pc eifi r-c piP xe′ lis vag le un ee sra (et .e gd .,w 2i 8th fop rr uob ina tb 8i )l ,ity wip th(P p′) at= chV siN z1 2 e× NB, be Ning anV dt Bhe stp ao cs k-
×
channels. Accordingly, the probability p(DS(P ′) = 0) equals the probability of
having another patch on the horizontal scanline identical to P ′, i.e., W p(P ′)
×
if we assume patches to be independent (in the worst case scenario), being W
the image width. On M3ED, p(DS(P ′) = 0) = 1280 p(P), that is 5e −41
× ≈
and e −257 for Histogram and Voxel Grid respectively. This means VSH has
≈
probability 1 to ease matching for empty patches if a LiDAR hint is available
≈
for them. We confirm this expected behavior in Fig. I (b), showing the correla-
tion curve computed by the stereo network on an empty patch, peaked on the
LiDAR value after VSH. Similar derivation can be done for BTH.
noitalerroc
tnih RADiL22
Fig.II: DSEC [7] processing scheme (1).
2 Evaluation Datasets and Pre-processing
In this section, we provide further details concerning the datasets used in our
workandthepre-processingwecarriedout.Inparticular,wedescribetheDSEC
[7]searchsplitweusedforthehyperparameterssearchconcerningourproposals
VSHandBTH,theM3ED[5]evaluationsplitweselected,andhowwemanaged
to process both datasets to extract raw LiDAR and, on M3ED dataset [5], for
obtaining misaligned LiDAR measurements with respect to the timestamp at
which we estimate disparity maps – and thus, at which we have ground-truth
depth for evaluation.
2.1 DSEC [7] Dataset
WestartwithDSEC[7],whichweusefori)tuningthehyper-parametersinour
solutions, and ii) training the models involved in our experiments.
Search split. We select three sequences from the training set:
zurich_city_00_b, interlaken_00_c and zurich_city_09_c.
Processing Scheme. We managed to extract raw LiDAR disparity maps
directly from the rosbag files provided by the authors. Our extraction pipeline
is detailed in Figs. II and III. Differently from M3ED [5], this dataset does
not provide any ground-truth pose. Consequently, as the first step detailed in
Fig. II, we deployed a LiDAR inertial odometry framework – FASTER-LIO
[2] – to obtain point clouds without rolling shutter effects (de-skewed), their
corresponding poses and timestamps.
Assumingasharedclockbetweenground-truthandrawtimestamps,weaim
to synchronize each ground-truth disparity map at time t with the nearest de-
d
skewed LiDAR scan M . As shown in Fig. III, we can achieve this goal using a
d
mappingfunctionthatlinksrawwithground-truthdatausingtimestamps.Next,
wecanusetheestimatedposestoalignM toitscorrelatedground-truthframe,
d
using linear interpolation. Unfortunately, we empirically noticed that this step
is insufficient to guarantee an acceptable alignment. To further refine this align-
ment, we deployed point-to-plane ICP algorithm [14] with L1 robust kernel [1]23
Fig.III: DSEC [7] processing scheme (2).
between M and the ground-truth point cloud (PCD) obtained by reprojection
d
oftheground-truthdisparitymapto3D.Toensurethebestalignmentpossible,
we repeated the process described in Fig. III also using the previous and the
next LiDAR scans M ,M and skipping the pose interpolation sub-step.
d 1 d+1
−
As a result, for each ground-truth frame we have six possible candidates – i.e.,
(M ,I),(M ,P ),(M ,I),(M ,P ),(M ,I),(M ,P ) ,whereI
d 1 d 1 d 1 d d d d+1 d+1 d+1
{ − − − }
(identity rigid transformation) and P ,P ,P (interpolated rigid transfor-
d 1 d d+1
−
mation respectively for ground-truth frame d 1,d,d+1) represent the initial
−
state for ICP algorithm. We select the one that best reduces the mean repro-
jection error (MAE). As the last step, raw point clouds are roto-translated into
the left rectified event camera point of view and then projected into the recti-
fied image plane to obtain depth maps, that are converted into disparity maps.
Finally, we measure the MAE between each raw LiDAR depth map and its cor-
responding ground-truth depth map and discard those with an error exceeding
0.5 meters from training/search/testing splits.
2.2 M3ED [5] Dataset.
In this second dataset, we evaluate the generalization performance by any of
the models involved in our evaluation, as well as we measure the robustness of
LiDAR-event stereo frameworks when processing misaligned LiDAR data.
Evaluation Split. We select both some outdoor (car_forest_tree_tunnel
andcar_urban_day_penno_small_loop,withtheformerfeaturingafully-static
setofframes)andindoor(falcon_indoor_flight_1,falcon_indoor_flight_2 and
falcon_indoor_flight_3) sequences.
Processing Scheme. Figs. IV to VI sketch the main stage of our pre-
processing pipeline. For each M3ED sequence, the authors provide data file and
depth_gt file. The former provides raw LiDAR measurements, left and right
distorted, unrectified stereo events, calibration parameters for each sensor, and
other data not meaningful for our scope. The latter store ground-truth depth
maps aligned to the left event camera and ground-truth poses for LiDAR (i.e.,24
Fig.IV: M3ED [5] processing scheme (1).Extractionofstereohistoriesandrec-
tification maps from the data file.
Fig.V: M3ED [5] processing scheme (2). After rectification, ground-truth depth
maps are converted to disparity maps.
Ln_T_L0) and left event camera (i.e., Cn_T_C0). This eases the LiDAR to
ground-truthalignmentwithrespecttoDSECdataset.Allsensors,ground-truth
depths, and poses are time-synchronized using a global clock. For more details,
please refer to the original paper [5].
We extract the stereo event histories directly from the data file, as displayed
in Fig. IV. Undistortion and rectification are managed at run-time using a for-
ward mapping, stored in a single rectify_map file for each event camera. To
create those mappings, we fed intrinsic, extrinsic, and distortion coefficients to
the OpenCV functions. Furthermore, since OpenCV returns backward mapping
functions, we used an iterative strategy to obtain left and right forward map-
pings. We maintain intermediate and final products of this step in memory, as
they will be required by subsequent steps.
After the previous step, we process raw ground-truth depth maps from the
depth_gt file (Fig. V). We rectify depth maps using the same left-forward map-
ping obtained in the previous step. For each depth map, we assume t = t –
d z
i.e., the timestamp t at which we want to estimate disparity is the same as the
d
timestamp t at which the depth map is captured. Finally, depth maps are con-
z
verted into disparity maps using stereo camera parameters obtained previously
from stereo calibration.
The final step consists of processing raw LiDAR scans from the data file
(Fig. VI). For each LiDAR scan, we linearly interpolate the pose at which the
scanstartsandtheposeatwhichthescanendsasweneedthemtoobtainapoint
cloudwithoutrollingshuttereffects(de-skewed).Weaimtoacquirebothin-sync
LiDAR disparity maps (i.e., t = t ,∆t = 0) and out-of-sync (or misaligned)
d z o25
Fig.VI: M3ED [5] processing scheme (3). We used interpolated poses to correct
rawLiDARpointcloudsaffectedbyrollingshuttereffects.Thenpointcloudsareroto-
translated, projected into image plane, rectified and converted into disparity maps.
LiDAR disparity maps – i.e., t ′z =t d −∆t o,∆t o >0, where ∆t o is the temporal
misalignmentchosenarbitrarily(e.g.,3,13,32,61and100ms)–theselatterare
used for experiments in Section 5.6. Given M as the closest de-skewed LiDAR
d
scan to t (mapping step in Fig. VI) and the temporal misalignment ∆t , the
d o
former goal is achieved by roto-translating M as if it had been captured at
d
t ′z = t d
−
∆t o. After that, we linearly interpolate the pose at time t ′z using
the two temporally nearest LiDAR poses (i.e., Ln_T_L0). Next, point clouds
are roto-translated into the left event camera point of view, and then projected
into the image plane to obtain depth maps. Finally, depth maps are rectified
andconvertedintodisparitymaps.Beforetheevaluation,wecalculatetheMAE
between each raw LiDAR depth map and its corresponding ground-truth depth
mapanddiscardthemiftheerrorexceeds0.1meters,aswellasdiscardthefirst
and last 50 frames where raw LiDAR data fully overlaps with ground-truth.26
Fig.VII: SE-CFF architecture.
3 LiDAR-Stereo Matching Fusion Architectures
We report additional details about the deep architectures used in our experi-
ments, starting from the baseline network, SE-CFF [12], and then showing the
fusion strategies ported from classical deep stereo literature [6,13,15]
Baseline Model: SE-CFF. Fig. VII provides an overview of the SE-CFF
[12] architecture assumed as the baseline in our work: i) rectified event streams
areorganizedintoMDESrepresentationsandthenprocessedbyaconcentration
network,producingsingle-channelstacks;ii)theseareprocessedbytwofeatures
extractor with shared weights, producing outputs respectively at 1,1, 1 of the
3 6 12
original resolution; iii) these are used to build a multi-scale cost-volume com-
putingthecorrelationbetweenleftandrightfeaturesalongtheepipolarline;iv)
a multi-scale, 2D network made with deformable convolutions is used to refine
the cost volumes; v) an initial disparity map is obtained through a soft-argmax
operator;vi)amulti-scalerefinementnetworkproducesasetofrefineddisparity
maps. We deploy eight variants of this architecture, one for each stacked rep-
resentation considered in our experiments – i.e., stage i) is replaced with the
different representations.
Guided Stereo [13].Thefirstamongthestrategiesinheritedfromclassical
deep stereo literatureis depicted in Fig. VIII. Specifically,LiDARdata are used
to modulate any of the multi-scale cost volumes , according to a Gaussian
F
function
= 1 v
ij
+v
ij
k e−(d − 2g ci 2j)2 (1)
G − · · ·F
(cid:18) (cid:19)
with k,c being the height and width of the Gaussian. Following [13], LiDAR
points are downsampled through nearest-neighbor interpolation to act at the
different resolutions.
Concat [16]. An alternative strategy consists of providing the raw LiDAR
data as an input to the stereo model. Fig. IX gives an overview of this lat-27
Fig.VIII: Guided Stereo Matching [13] Framework.
Fig.IX: Concat [6] architecture.
ter approach. LiDAR data are projected on left and right camera frames and
processed by two feature extractors with shared-weights and made of Sparsity-
InvariantConvolutions.Thesefeaturesareconcatenatedtothoseextractedfrom
images – or stacked events, in our case – before the cost volumes are computed.
Guided+Concat [6]. This strategy is inspired by CCVNorm architecture
[6]andre-adaptedtobedeployedwithSE-CFF.Specifically,CCVNormdeploys
LiDAR data both as the input to the model, as well as to modulate the cost-
volume at different stages during inference. Fig. X shows how we tailor this
strategy to SE-CFF, basically by combining the two previous strategies.28
Fig.X: Guided+Concat [15] architecture.
Table I: Ablation on VSH (top) and BTH (bottom).Wereport1PEforstereo
backbones with different stacked representations on the DSEC search split.
EventRepresentations
Configuration Histogram MDES VoxelGrid TORE T.Surface ERGO-12 Tencode
[11] [12] [16] [3] [9] [17] [8]
Baseline 13.33 12.07 12.75 12.23 11.69 11.58 11.15
(A) VSH 12.29 10.33 11.71 10.49 10.20 10.07 9.84
(B) (A)+Occlusionhandling[4] 12.28 10.33 11.72 10.50 10.19 10.06 9.83
(C) (B)+Splatting[4] 12.00 10.33 11.70 10.51 10.14 10.04 9.82
EventRepresentations
Configuration Histogram MDES VoxelGrid TORE T.Surface ERGO-12 Tencode
[11] [12] [16] [3] [9] [17] [8]
Baseline 13.33 12.07 12.75 12.23 11.69 11.58 11.15
(A) BTH 10.99 10.53 10.52 10.34 10.13 10.09 9.78
(B) (A)+Occ.disc.[4] 10.99 10.52 10.53 10.33 10.12 10.08 9.77
(C) (B)+Splatting 11.00 10.54 10.53 10.34 10.13 10.08 9.77
4 Ablation Study – Additional Experiments
We complement the ablation studies already shown in the main paper. Specif-
ically, Tab. I shows the results achieved by VSH and BTH on the search split,
respectively on top and bottom.
Starting from VSH, we show how handling occlusions according to [4] (B)
allows for slightly improving the results, with further improvements achieved
by applying sub-pixel splitting [4] (C). When it comes to BTH, we observed
empirically that discharging occlusions yields the best results (B). We also im-
plemented a revised version of pixel splatting applied to the event streams (C),
yet without noticeable improvements.29
Event left Event right Raw LiDAR Ground-truth
Baseline Guided [13] VSH BTH
1PE: 85.12% 1PE: 84.50 % 1PE: 14.95 % 1PE: 11.09 %
Fig.XI: Qualitative results – baseline stereo backbone. Results on
car_forest_tree_tunnel, with MDES [12] representation.
Event left Event right Raw LiDAR Ground-truth
Baseline Guided[13] Concat[6] Guided+Concat[15] BTH
1PE: 85.12% 1PE: 85.10 % 1PE: 48.00 % 1PE: 44.52 % 1PE: 11.31 %
Fig.XII: Qualitative results – re-trained stereo backbone. Results on
car_forest_tree_tunnel, with MDES [12] representation.
5 Qualitative Results
In conclusion, we present some qualitative results to support the efficacy of our
proposal. We display eight (i.e., Figs. XI to XVIII) different figures from the
M3ED [5] dataset, using raw LiDAR measurements as guidance for all fusion
frameworks.
Figs. XI and XII show an example from the car_forest_tree_tunnel se-
quence, respectively spotlighting fusion strategies applied without re-retraining
thestereobackbone(theformer)orwhenthenetworkistrainedfromscratchto
performfusion(thelatter),processingMDES[12]representationsinbothcases.
Intheformercase,Guided[13]isnearlyineffective,whereasbothVSHandBTH
largely improve the results. In the latter case, Concat [6] and Guided+Concat
[15] can reduce the error by about 40%, yet far behind the improvement yielded
by BTH (more than 70% error rate reduction).30
Event left Event right Raw LiDAR Ground-truth
Baseline Guided [13] VSH BTH
1PE: 48.09% 1PE: 46.59 % 1PE: 11.78 % 1PE: 12.20 %
Fig.XIII: Qualitative results – baseline stereo backbone. Results on
spot_indoor_obstacles, with Voxel Grid [16] representation.
Event left Event right Raw LiDAR Ground-truth
Baseline Guided[13] Concat[6] Guided+Concat[15] BTH
1PE: 43.74% 1PE: 40.13 % 1PE: 24.94 % 1PE: 18.21 % 1PE: 11.56 %
Fig.XIV: Qualitative results – re-trained stereo backbone. Results on
spot_indoor_obstacles, with ERGO-12 [17] representation.
Figs. XIII and XIV show an example from an indoor sequence – i.e.,
spot_indoor_obstacles – again when not re-training or training the stereo back-
bonefromscratch.Intheformercase,wereportresultsbyprocessingVoxelGrid
representations [16], which confirm the trend observed in the previous example.
Indeed, our proposal confirms again the best solution for exploiting raw LiDAR
measurements and improve the accuracy of event-based stereo networks.
Figs. XV and XVI showcase another example from outdoor, with a person
being framed by the event cameras – i.e., spot_outdoor_day_skatepark_1. In
this case, we report results obtained by processing TORE [3] representations
when re-training the stereo backbones, confirming again how our proposal con-
sistently yields the largest drop of the error rate independently of the chosen
representation.
Toconclude,weevaluatetheeffectivenessofdifferentfusionstrategiesagainst
partially filled raw LiDAR depth maps both in indoor (i.e., falcon_indoor_-31
Event left Event right Raw LiDAR Ground-truth
Baseline Guided [13] VSH BTH
1PE: 63.98% 1PE: 63.07 % 1PE: 36.33 % 1PE: 29.27 %
Fig.XV: Qualitative results – baseline stereo backbone. Results on
spot_outdoor_day_skatepark_1, with MDES [12] representation.
Event left Event right Raw LiDAR Ground-truth
Baseline Guided[13] Concat[6] Guided+Concat[15] BTH
1PE: 60.89% 1PE: 58.28 % 1PE: 58.22 % 1PE: 55.54 % 1PE: 22.22 %
Fig.XVI: Qualitative results – re-trained stereo backbone. Results on
spot_outdoor_day_skatepark_1, with TORE [3] representation.
flight_1)andoutdoor(i.e.,falcon_outdoor_day_penno_parking_1)scenarios.
The former case (Fig. XVII) highlights the behaviour of not-retrained stereo
backbones at recovering fine details such as the tip of the cone and the upper
partoftheleft-mostforegroundobject.Comparedtoothermethodologies,BTH
managestopreservemorethindetails.Furthermore,thelattercase(Fig.XVIII)
stress the performance of retrained stereo backbones in case of large uniform
areas – i.e., the road. Guided [13] is almost ineffective, while using both Concat
[6]andGuided+Concat[15]leadsto20%errorreduction.However,whendealing
with large homogeneous regions where LiDAR coverage is limited, BTH clearly
dominates alternative methods, achieving a remarkable 80% reduction in error.32
Event left Event right Raw LiDAR Ground-truth
Baseline Guided [13] VSH BTH
1PE: 50.15% 1PE: 49.21 % 1PE: 27.86 % 1PE: 28.02 %
Fig.XVII: Qualitative results – baseline stereo backbone. Results on fal-
con_indoor_flight_1, with Voxel Grid [16] representation.
Event left Event right Raw LiDAR Ground-truth
Baseline Guided[13] Concat[6] Guided+Concat[15] BTH
1PE: 94.85% 1PE: 90.37 % 1PE: 74.83 % 1PE: 74.94 % 1PE: 14.02 %
Fig.XVIII: Qualitative results – re-trained stereo backbone. Results on fal-
con_outdoor_day_penno_parking_1, with MDES [12] representation.
References
1. Babin, P., Giguere, P., Pomerleau, F.: Analysis of robust functions for registra-
tion algorithms. In: 2019 International Conference on Robotics and Automation
(ICRA). pp. 1451–1457. IEEE (2019)
2. Bai,C.,Xiao,T.,Chen,Y.,Wang,H.,Zhang,F.,Gao,X.:Faster-lio:Lightweight
tightly coupled lidar-inertial odometry using parallel sparse incremental voxels.
IEEE Robotics and Automation Letters 7(2), 4861–4868 (2022). https://doi.
org/10.1109/LRA.2022.3152830
3. Baldwin, R.W., Liu, R., Almatrafi, M., Asari, V., Hirakawa, K.: Time-ordered re-
centevent(tore)volumesforeventcameras.IEEETransactionsonPatternAnal-
ysis and Machine Intelligence 45(2), 2519–2532 (2022)
4. Bartolomei,L.,Poggi,M.,Tosi,F.,Conti,A.,Mattoccia,S.:Activestereowithout
pattern projector. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV). pp. 18470–18482 (October 2023)33
5. Chaney, K., Cladera, F., Wang, Z., Bisulco, A., Hsieh, M.A., Korpela, C., Ku-
mar, V., Taylor, C.J., Daniilidis, K.: M3ed: Multi-robot, multi-sensor, multi-
environmenteventdataset.In:ProceedingsoftheIEEE/CVFConferenceonCom-
puter Vision and Pattern Recognition (CVPR) Workshops. pp. 4015–4022 (June
2023)
6. Cheng,X.,Zhong,Y.,Dai,Y.,Ji,P.,Li,H.:Noise-awareunsuperviseddeeplidar-
stereo fusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 6339–6348 (2019)
7. Gehrig, M., Aarents, W., Gehrig, D., Scaramuzza, D.: Dsec: A stereo event cam-
era dataset for driving scenarios. IEEE Robotics and Automation Letters (2021).
https://doi.org/10.1109/LRA.2021.3068942
8. Huang, Z., Sun, L., Zhao, C., Li, S., Su, S.: Eventpoint: Self-supervised interest
point detection and description for event-based camera. In: Proceedings of the
IEEE/CVFWinterConferenceonApplicationsofComputerVision(WACV).pp.
5396–5405 (January 2023)
9. Lagorce, X., Orchard, G., Galluppi, F., Shi, B.E., Benosman, R.B.: Hots: a hier-
archy of event-based time-surfaces for pattern recognition. IEEE transactions on
pattern analysis and machine intelligence 39(7), 1346–1359 (2016)
10. Manduchi,R., Tomasi,C.: Distinctiveness mapsforimagematching.In:Proceed-
ings 10th International Conference on Image Analysis and Processing. pp. 26–31.
IEEE (1999)
11. Maqueda, A.I., Loquercio, A., Gallego, G., García, N., Scaramuzza, D.: Event-
based vision meets deep learning on steering prediction for self-driving cars. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 5419–5427 (2018)
12. Nam, Y., Mostafavi, M., Yoon, K.J., Choi, J.: Stereo depth from events cameras:
Concentrateandfocusonthefuture.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 6114–6123 (2022)
13. Poggi, M., Pallotti, D., Tosi, F., Mattoccia, S.: Guided stereo matching. In: Pro-
ceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.
pp. 979–988 (2019)
14. Rusinkiewicz,S.,Levoy,M.:Efficientvariantsoftheicpalgorithm.In:Proceedings
third international conference on 3-D digital imaging and modeling. pp. 145–152.
IEEE (2001)
15. Wang, T.H., Hu, H.N., Lin, C.H., Tsai, Y.H., Chiu, W.C., Sun, M.: 3d lidar and
stereofusionusingstereomatchingnetworkwithconditionalcostvolumenormal-
ization. In: 2019 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS). pp. 5895–5902. IEEE (2019)
16. Zhu,A.Z.,Yuan,L.,Chaney,K.,Daniilidis,K.:Unsupervisedevent-basedlearning
of optical flow, depth, and egomotion. In: Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. pp. 989–997 (2019)
17. Zubić,N.,Gehrig,D.,Gehrig,M.,Scaramuzza,D.:Fromchaoscomesorder:Order-
ing event representations for object recognition and detection. In: Proceedings of
theIEEE/CVFInternationalConferenceonComputerVision(ICCV).pp.12846–
12856 (October 2023)