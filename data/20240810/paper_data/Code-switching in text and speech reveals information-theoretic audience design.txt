Graphical Abstract
Code-switching in text and speech reveals information-theoretic
audience design
Debasmita Bhattacharya, Marten van Schijndel
Why do bilinguals code-switch? 地下室 。
洗衣房在 ...
❖ Bilinguals are known to alternate between one
language and another in writing and speech. basement 。
❖ Is this purely to make language production easier? Or is
(= The laundry room is in the basement.)
there an additional signaling benefit for the listener?
❖ To answer this, we use language modeling to obtain
information load (i.e. surprisal) statistics for written and
spoken Chinese-to-English code-switched corpora.
English Surprisal / Chinese Surprisal /
Mean English Surprisal Mean Chinese Surprisal
Conclusion: switching to English is “harder” than
remaining in Chinese would have been in text and speech
⟹ code-switching involves audience-driven pressures.
4202
guA
8
]LC.sc[
1v69540.8042:viXra
slasirprus
fo
oitaRHighlights
Code-switching in text and speech reveals information-theoretic
audience design
Debasmita Bhattacharya, Marten van Schijndel
• Languagemodelsestimatelikelihoodofalternativecontinuationsintwo
languages.
• Code-switching operates as a signal to listeners of high information
content regions.
• This audience-driven effect is present across written and spoken code-
switching.Code-switching in text and speech reveals
information-theoretic audience design
Debasmita Bhattacharyaa, Marten van Schijndelb
aColumbia University Department of Computer Science, Mudd Building, 500 West 120th
Street, New York, 10027, NY, USA
bCornell University Department of Linguistics, Morrill Hall, 159 Central
Avenue, Ithaca, 14850, NY, USA
Abstract
In this work, we use language modeling to investigate the factors that in-
fluence code-switching. Code-switching occurs when a speaker alternates
between one language variety (the primary language) and another (the sec-
ondary language), and is widely observed in multilingual contexts. Recent
work has shown that code-switching is often correlated with areas of high
information load in the primary language, but it is unclear whether high pri-
mary language load only makes the secondary language relatively easier to
produceatcode-switchingpoints(speaker-drivencode-switching),orwhether
code-switching is additionally used by speakers to signal the need for greater
attention on the part of listeners (audience-driven code-switching). In this
paper, we use bilingual Chinese-English online forum posts and transcripts
of spontaneous Chinese-English speech to replicate prior findings that high
primary language (Chinese) information load is correlated with switches to
the secondary language (English). We then demonstrate that the informa-
tion load of the English productions is even higher than that of meaning
equivalent Chinese alternatives, and these are therefore not easier to pro-
duce, providing evidence of audience-driven influences in code-switching at
the level of the communication channel, not just at the sociolinguistic level,
in both writing and speech.
Keywords: code-switching, bilingualism, predictability, audience design,
language modeling
Preprint submitted to Journal of Memory and Language August 9, 20241. Introduction
The majority of the world’s population speaks more than one language
(Ansaldo et al., 2008). Among such multilingual speakers, code-switching is
a commonly observed phenomenon. Code-switching occurs when a speaker
alternates between one language variety and another during written or spo-
ken linguistic communication (Poplack, 1980). Code-switches are produced
across speakers of varying ages (Meisel, 1994), as well as between many dif-
ferent language pairs (Muysken, 2000). For instance, in the below example,
the Chinese word for basement has been replaced by its English equivalent,
demonstrating a code-switch.
Many psycholinguistic and sociolinguistic factors have been shown to
affect the prevalence of code-switching in a bilingual person’s speech. At
the utterance-level, these include speaker competency, the linguistic con-
text, the emotional state of the speaker, and the type of information that
the speaker wishes to convey, among others (Dornic, 1978; Gardner-Chloros,
2009; Broersma, 2009). In contrast to such speaker-centric factors, previous
work has proposed that the identity of the listener in a conversation affects
code-switching prevalence in an utterance, suggesting that there are also
audience-driven pressures on code-switching (Bell, 1984; Dahl, 2009; Hassan,
2019). At the word-level, information load, also referred to as predictabil-
ity and measured by word and sentence completion, has been correlated
with code-switching in previous work (Myslín and Levy, 2015; Calvillo et al.,
2020), but as Myslín and Levy point out, this influence could be produced
by either speaker-driven or audience-driven pressures.
In this work, we aim to disentangle these two hypotheses of how informa-
tionload influencescode-switching, by analyzingbilingualonline forumposts
and transcripts of spontaneous speech. Under the speaker-driven view, one
(primary) language is generally most salient to a speaker. However, in areas
of high information load, the relative linguistic complexity of the primary
language continuation may make the secondary language easier to access for
2the speaker, increasing the chance that the speaker will slip into the sec-
ondary language and produce a code-switch (Poplack, 1980; Owens, 2005;
Beatty-Martínez et al., 2020). Alternatively, under an audience-driven view,
code-switching tends to happen at areas of high linguistic complexity and
information load precisely because those are the areas where speakers want
to warn listeners to increase the available cognitive resources (e.g., attention)
for comprehension.
We analyze the influence of the secondary language (English, in this case)
on code-switching to investigate whether the influence of information load on
code-switching stems mainly from audience-driven or speaker-driven pres-
sures.1 We conclude that information load serves an audience-driven role in
code-switching, which operates at the level of the communication channel
and not just at the sociolinguistic level, rather than a purely speaker-driven
role.
2. Related Work
Sociolinguistic and ethnographic studies like Bell (1984), Hassan (2019),
and Liu (2021) have shown that speakers alter their code-switching behavior
based on the features of the listener (e.g., whether the listener speaks the
secondarylanguage),providingevidenceforidentity-basedaudiencedesignin
code-switching. The more recent of these studies, i.e. Hassan (2019) and Liu
(2021), particularly studied written code-switches on social media platforms
like WeChat. However, these involved in-depth ethnographic analyses of a
small number of users (e.g., Liu, 2021, studied 3 users), rather than analysis
of broader population-level code-switching behavior. In the spoken domain,
few studies have considered spontaneous code-switched speech through the
framework of information theory, though a few, e.g. Liu et al. (2023) and
HofweberandMarinis(2023), havestudiedlanguageprocessingcostsintask-
oriented bilingual settings.
Most similar to our work overall is that of Calvillo et al. (2020), who
studiedtheinfluenceofinformationload(alsoreferredtoaspredictability)on
code-switching in written text, and Myslín and Levy (2015), who studied the
same in informal speech. Both (Calvillo et al., 2020) and (Myslín and Levy,
1We note that there are clearly both audience- and speaker-driven influences on code-
switching, butinthisworkwearespecificallyfocusedonwhetherinformationloadhasan
audience-driven component.
32015) found similar effects and showed that in addition to identity factors,
code-switchingisinfluencedbylinguisticfactorssuchasinformationloadand
word length. In particular, Calvillo et al. (2020) focused on a corpus of code-
switched online interactions between Chinese-English bilinguals who live and
study in the US. In this work the authors examined whether information load
(measured specifically by word surprisal, i.e. the negative log probability of
a word given its context, as in the below formula) affected the likelihood of
code-switching into English.
surprisal(w ) = −log P(w |w ,...,w )
i 2 i i−1 i−t
First, Calvilloetal. translatedthecode-switchedsentencesfullyintoChi-
nese to enable analysis with a monolingual Chinese language model.2 Then,
in order to isolate information load from other factors, they paired their
code-switched sentences with structurally similar sentences that lacked code-
switches, generating minimal pairings between sentences that manifested
code-switches and those that did not. By training a logistic classifier to pre-
dict which sentence in each pair originally contained a code-switch, Calvillo
et al. found a positive relationship between surprisal and code-switching. In
other words, higher surprisal was correlated with increased code-switching.
However, it remains unclear whether that correlation arose from speaker con-
straints or as a function of audience design.
3. Research Questions and Hypotheses
While previous work has either focused on the primary language or con-
sidered task-oriented settings, we attempt to disentangle the speaker-driven
and audience-driven hypotheses in this work by studying the features of both
the primary language and the secondary language in undirected communi-
cation settings. We acknowledge that it is natural for both speaker-driven
and audience-driven influences to exist, but we are interested in whether
there are any audience-driven influences on code-switching arising from the
2Duetotherelativeinfrequencyofcode-switching,itisdifficulttocapturegeneralizable
code-switching statistics using a standard language model (e.g., restricting the training
data to only sentences containing code-switches would require a prohibitive amount of
data for training). Therefore, computational approaches usually attempt to model code-
switching using monolingual language models.
4communication channel itself (e.g., beyond knowledge of the listener’s mul-
tilingual proficiency). By using language models to probe the surprisal of
the sentence or utterance in the two different languages, we can address our
main research questions:
RQ1: Whilethereareclearlyspeaker-drivencomponentstocode-switching,
doestheinfluenceofinformationloadoncode-switchingalsoreflectaudience-
driven pressures?
RQ2: Does the influence of information load on code-switching general-
ize across writing and speech?
Based on the previous literature, we expect code-switched regions of writ-
ing and speech in the primary language (the language used pre-code-switch)
to have high information load, but the speaker-driven and audience-driven
theories make different predictions about the relative information load of
each language in the code-switched region. If the speaker-driven hypothesis
is correct, the information load of code-switches in the secondary language
(the language that is switched into) must be lower than that in the primary
language. Whereas, in the audience-driven case, the information load of
code-switches in the secondary language could actually be higher than that
in the primary language. In the latter case, the higher inherent information
load of some part of the message might induce speakers to produce a code-
switch with slightly higher information load in order to increase the salience
of the region and signal to listeners that a region is difficult to process.
Whetherspeaker-oraudience-driveninfluencesgeneralizeacrossthewrit-
ten and spoken modalities of code-switched language production is harder to
predict, but we expect any influence of information load to generalize across
written and spoken communication.
4. Corpora
4.1. Written code-switched data
In our work, we first expanded the original Calvillo et al. (2020) corpus,3
which was comprised of Chinese-English code-switched sentences and their
3https://github.com/lfang1/CodeSwitchingResearch
5correspondingfullyChinesetranslations,4 aswellasfullyChinesecontrolsen-
tences that lacked code-switches but exhibited syntactic configurations sim-
ilar to the code-switched sentences. The original corpus was collected from
web-based, typed communication on public forums between Chinese-English
bilingual university students in the United States. The Chinese portions of
the data are written in simplified Chinese. We assume that the speakers
within the corpus are highly proficient in both languages, given that they
are conversing on a Chinese language forum and are attending American
universities that have English language proficiency requirements.
To this written corpus, we added automatic English translations of the
code-switchedsentencesandnon-code-switchedsentencesbyusingtheChinese-
English translator Fanyi Youdao.5 We ensured that the fully English transla-
tions retained the original code-switched word through manual examination
of the pre- and post-translated data, with changes in inflection when required
to maintain correctness in English. Two native English speakers confirmed
the fluency of the translated English sentences. A fluent bilingual Chinese-
English speaker also checked 10% of the automatic translations to confirm
their accuracy and meaning preservation. In sum, our expanded version of
Calvillo et al.’s corpus contained 1476 code-switched sentences, each present
twice: once fully in Chinese and once fully in English. It also contained 1476
non-code-switched sentences, each present twice: once fully in Chinese and
once fully in English.
4.2. Spoken code-switched data
In addition to our expanded corpus of written code-switches, we use a
subset of the South East Asia Mandarin-English (SEAME) speech corpus of
informal conversations and interviews (Lyu et al., 2010). This corpus origi-
nally contained 192 hours of speech corresponding to 256 dialogues between
156 speakers. We filter this corpus to include speech transcripts of utterances
that are either monolingual Chinese or code-switched from Chinese to En-
glish, to maintain consistency with the written data set. As with the written
corpus, we added automatic Chinese translations of the code-switched sen-
4These fully Chinese translations of the code-switched sentences were generated and
validated by native Chinese speakers at the time of the data set’s creation.
5We also experimented using Google Translate, but that produced notably less fluent
andmoresurprisingEnglishcontinuations. PleaseseetheAppendixforfurthercomparison
between translations produced by Fanyi Youdao and Google Translate.
6tences, and automatic English translations of the code-switched sentences
and monolingual Chinese sentences. The code-switched subset of the spoken
code-switched corpus we use contains 6171 utterances, while the monolingual
Chinese subset contains 22770 utterances.6 From a practical standpoint, the
scale of our data set makes human translations intractable and automatic
ones a reasonable alternative.
While we rely on automatic translation for our monolingual data, auto-
matic (machine) translation is considered a “mature technology” when one of
thelanguagesofinterestisEnglish(Isbisteretal.,2021), asinourwork. This
is largely due to the existence of larger, high-performance English language
models compared to smaller models in relatively lower-resourced languages.
Therefore, we consider it reliable to use automatic translation to generate
our monolingual data.
4.3. Data availability
All of our materials, data, analysis code, and computational models can
befoundatthefollowinglink: https://github.com/db758/code-switching.
5. Metrics
Since Calvillo et al. found that 5-gram surprisal was predictive of code-
switchinginwriting,wetrainedtwomonolingualKneser-Essen-Neysmoothed
5-gram language models on Chinese7 and English8 Wikipedia data sets,9 con-
sisting of approximately 360 million and 101 million tokens respectively, and
weusedthese5-grammodelstoobtainword-levelsurprisalvaluesforthefully
Chinese and fully English sentences in our written corpus of code-switches.
We also obtain word-level surprisal values for the same using bert-base-
chinese10 for our analyses of the monolingual Chinese data and GPT-2
(Radford et al., 2019) for our analyses of the monolingual English data, both
6We use a random sample of 6171 monolingual utterances when performing logistic
regressions in Section 6.
7https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.
xml.bz2
8https://huggingface.co/datasets/wikitext
9Training took roughly 3 hours per model on a laptop CPU. Preprocessing was done
using WikiExtractor.
10https://huggingface.co/google-bert/bert-base-chinese
7Figure1: Schematicillustratingtheshorthandterminologyusedthroughouttheremainder
of this paper on (1) original code-switched sentences, (2) fully Chinese code-switched
sentences12 and (3) non-code-switched sentences in the written data set.
of which are large language models trained on Internet text without rein-
forcement learning from human feedback. We interface with these LLMs
using the minicons python library (Misra, 2022).11
To obtain word-level surprisal values for our spoken code-switched data,
we train two 5-gram language models on monolingual conversational speech
transcripts from the CallHome (English) (Canavan et al., 1997) and HUB5
(Chinese) (Consortium, 2018) corpora. These consist of approximately 170
and 190 thousand tokens respectively.
For our analyses on written code-switches, we partitioned our data into
differenttypes. Wedistinguishedbetweensentencescontainingcode-switches
(CS) and sentences that lacked any code-switch (Non-CS). Calvillo et al.
paired each CS sentence with a Non-CS sentence that has a word in a similar
syntactic configuration to the first code-switched word in the CS sentence.
We designated this “code-switch word” in both CS and Non-CS sentences
11Throughout this work, we find the same results for LLMs and 5-gram models, so we
onlydiscusstheresultsfor5-gramsinthepaper. Pleasesee Appendix AforLLMresults.
12Note that the CS1 word in sentence (2) has the meaning "available in schedule"
rather than the more context-appropriate you3, meaning "vacant". This kind of strange
translation of CS1 is present in many of the fully Chinese code-switched sentences in
the original version of the Calvillo et al. (2020) data set, which we use and expand. All
translations of CS1 were originally provided by native speakers of Chinese for Calvillo
et al. (2020).
8as CS1. We designated all other words in Non-CS sentences as Non-CS
words. WedeterminedChinesewordsegmentationsusingthejiebapython
library’s tokenizer. See Figure 1 for a visual schematic of these regions.
For our analyses on spoken code-switches, we similarly distinguished ut-
terancescontainingcode-switches(CS)frommonolingualChineseutterances
(Non-CS). However, only the CS utterances in the spoken code-switched
corpus contain a CS1 word, as the Non-CS utterances were not syntacti-
cally paired with CS utterances in this data set, i.e. all words in Non-CS
utterances are Non-CS words in the spoken corpus of code-switches.
6. Experiments
6.1. High information load contributes to code-switching at CS1 in writing
6.1.1. Validating prior findings from literature
We first confirmed that prior findings from literature replicated on our
data. We wanted to validate both the written code-switched data and our
corresponding models by replicating the findings of Calvillo et al. (2020) in
particular. To do this, we began by using our Chinese language model to
confirm the Calvillo et al. (2020) finding that code-switches occur in regions
of high surprisal in writing. That is, we compared the distribution of Chi-
nese CS1 surprisal between CS and Non-CS sentences (see Figure 2). We
confirmed that CS1 was significantly more surprising in CS sentences than
in Non-CS sentences (p = 1.448e-09),13 but the surprisal difference between
the sentence types was very small, with a mean ∆ of 1 bit.
However, the CS1 surprisal values in both CS and Non-CS sentences
were significantly higher (both p < 2.2e-16) than the surprisal values of Non-
CS words in Non-CS sentences. We also found that the surprisal of Non-
CS words was similar to that of held-out monolingual Chinese Wikipedia
data (see Figure A.7 in the Appendix). To summarize, mean surprisal in
monolingual Chinese was much lower than at CS1 (mean ∆ of monolingual
Chinese was 4.39 bits from CS1 in CS sentences, and 3.41 bits from CS1 in
Non-CS sentences).
Overall, these results support the conclusions of Calvillo et al. (2020)
that code-switched words occur in regions of high information load in writ-
ing; however, the surprisal of CS1 words in non-code-switched sentences was
13Throughout this paper, we determined significance using Welch’s 2-sample t-tests.
9Figure 2: Comparing surprisal of CS1 words in code-switched and non-code-switched
sentences to Non-CS words.
generally much higher than that of Non-CS words, suggesting that CS1, even
in cases where no code-switching occurs, differs from standard monolingual
written Chinese.
Given the non-deterministic nature of code-switching, we interpret this to
be an indication that CS1-like sites are generally regions of high information
load that are likely to exhibit code-switching (as occurred in the paired CS
sentences in the written corpus). By comparing CS1 in CS sentences with
CS1 in Non-CS sentences, Calvillo et al. (2020) demonstrated that surprisal
is a driver of code-switching after controlling for the other influences that
define CS1-like sites. However, ideally we would like to quantify the influence
of any factors that differentiate CS1 sites from monolingual Chinese, rather
than only those factors that push CS1 sites to switch more often.
6.1.2. Extending prior findings
To better quantify the full impact of surprisal alongside other factors that
influence written code-switching, we implemented a binary logistic regression
model14 to differentiate between CS1 words in code-switched sentences and
Non-CS words in Non-CS sentences. For the CS sentences, we used the
14Throughoutthiswork,weusedscikit-learn 1.1.0toimplementregressionmod-
els.
10Factor coef std err t
Intercept 0.5113 0.006 88.964
POS=verb 0.0162 0.010 1.608
POS=other -0.0371 0.009 -3.910
Frequency 0.8884 0.008 118.065
Word length 0.0578 0.009 6.390
Word position -0.0184 0.007 -2.642
Surprisal 0.0199 0.009 2.270
Table 1: Summary of the logistic regression model for CS1 (coded 1) versus random Non-
CS1 (coded 0) in the data set of written code-switches.
characteristics of CS1. For the Non-CS sentences, we randomly sampled a
Non-CS word rather than CS1 itself, unlike Calvillo et al.
We used most of the same regression features as those used by Calvillo
et al. We partitioned part-of-speech tags into three categories: nouns, verbs,
and other. We used dummy-coding with noun as the reference tag (i.e. all
part-of-speech results were relative to nouns). We replaced sentence length
with the relevant word’s position in the sentence, following a 0-indexing con-
vention.15 We also replaced word length in terms of Chinese character length
with word length in terms of pinyin character length,16 a metric that more
directly corresponds to typing effort via number of keystrokes.17 We stan-
dardized all numerical features – i.e. word length, word frequency, word posi-
tion, and surprisal – used in the regressions such that their means were 0 and
their standard deviations were 0.5, in keeping with the methodology followed
15We find in our data that code-switched sentences tend to be longer than non-code-
switched ones (see Figure A.8 in the Appendix). Given this, we were interested both in
finding out whether code-switches occur later in these longer sentences, and in keeping all
of the features of our regression consistent with word-level analyses.
16We obtain these values by using the python-pinyin library, accessed via https:
//github.com/mozillazg/python-pinyin/tree/master.
17Due to the logographic nature of Chinese orthography, there is very little variance in
the word lengths in terms of Chinese character length in our written corpus. Without
conversion to pinyin word length, median word length is greater for CS1 words than for
Non-CS words, but Non-CS has some of the longest words in the written data set (see
Table A.3 in the Appendix). Mean word length in terms of Chinese character length is
fewer than 2 characters, regardless of region.
11by Calvillo et al. We transformed frequency values with a negative log op-
2
eration to make the values comparable to our surprisal values, meaning that
a large frequency value corresponds to a less common word in our analyses
(i.e. frequency is “unigram surprisal”).
Our new regression supported our previous finding that higher surprisal
was correlated with code-switching (see Table 1).
In line with prior studies, we also found that rarer Chinese words were
more likely to be code-switched (Calvillo et al., 2020; D’Amico et al., 2001;
Forster and Chambers, 1973), longer Chinese words, i.e. words that required
greater typing effort, were more likely to be code-switched (Calvillo et al.,
2020; Myslín and Levy, 2015), and non-noun/non-verb parts of speech were
lesslikelytobecode-switched(Myers-Scotton,1993;MyslínandLevy,2015).
Somewhatunexpectedly, wealsofoundthatlaterwordpositionsinasentence
were less likely to be code-switched, which contrasts with our and Calvillo
et al.’s observation that longer sentences tend to contain code-switching.
Overall, our results confirmed that higher surprisal regions are more likely
to exhibit code-switching in writing. Note that the results in this section
replicate when we replace all surprisal values with those sourced from a large
language model - in this case, bert-base-chinese (see Table A.4 in the
Appendix).
6.2. Code-switchinginwritinghasinformation-theoreticaudience-drivencom-
ponents
6.2.1. Background and Motivation
There are two basic information-theoretic hypotheses about the underly-
ing process driving code-switching. First, a speaker may produce whichever
language is most accessible or salient at a given point in time (Poplack,
1980; Owens, 2005; Beatty-Martínez et al., 2020). Under this speaker-driven
theory of code-switching, the primary language is generally most salient to a
speaker. Once they begin speaking in that language, their continuations stay
in the primary language due to co-occurrence statistics. In areas of high in-
formation load, however, the difficulty of the primary language continuation
may make the secondary language easier to access for the speaker, producing
a code-switch.
Alternately, speakers may code-switch to signal to the audience that this
part of the utterance has a high information load. The code-switch may
make a region more salient and signal to the recipient that they need to
allocate more cognitive resources (e.g., attention) to the utterance at this
12point, as speculated in Myslín and Levy (2015). Under this audience-driven
theory, code-switching tends to happen at areas of high linguistic complexity
precisely because those are the areas where speakers want to warn listeners
to increase the available resources for comprehension.
6.2.2. Pre-processing jargon
Toexplorethequestionofwhethercode-switchinginwritinghasaudience-
driven influences or is primarily speaker-driven, we examined the features of
thesecondarylanguage,English,atCS1incode-switchedsentences. Tostart,
we observed that many of the code-switched English words in the corpus are
jargon-like and/or likely to be used far more in the secondary language con-
texts (in this case, English college) than in the primary language contexts
(in this case, Chinese pre-college). Such words include rental-related terms
like "lease", "apartment", "garage", "utility", "studio", and "house", as well
as terms to do with bed sizes such as "king" and "queen", and account for
about 20 percent of the code-switched words in the corpus. Technical terms
like "email" and "PhD" also appear frequently, accounting for an additional
5 percent of the code-switched English in the corpus, while proper nouns
like "JFK" account for almost 1 percent of the corpus. These observations
highlight some of the sociological factors behind code-switching which are
important but which we do not explore further in this work. We note that
since students are less likely to have leased apartments on their own prior
to attending university in America, the fact that they use English terms
for leasing is not surprising, even if English is their less dominant language.
What is relevant for our investigation of information load are cases when
they switched even in contexts for which they otherwise could have used
their dominant language.
Since our models for analysis of written code-switches were trained on
Wikipedia data that does not often cover such jargon-like topics, we had
reason to believe that our models would not account for the sociological
factors underlying code-switching in our written data set and would not be
able to correctly model the jargon-like terms. Testing our models on such
terms would then likely produce unreliable measures of surprisal. In order
to avoid this effect on our analyses, we removed the jargon-like terms before
conducting the remaining analyses in this section.18 However, we did confirm
18Wealsoconsideredapotentialconfoundofthisstudywherebypeoplemaycode-switch
13(a) (b) (c) (d)
Figure3: ComparingCS1inEnglishandmonolingual(ML)Englishacross(a)wordlength,
(b) part-of-speech tag distribution, (c) word frequency, and (d) surprisal, in writing.
that performing our analyses on the subset of CS1 data that includes the
jargon-liketermsproducedeffectswithidenticalsignstoourreportedresults,
though the magnitudes of the effects differed (see Appendix for details).
6.2.3. CS1 English is more complex than monolingual English in writing
We first compared the lengths of English words at CS1 to the monolin-
gual English present in Wikipedia. From Figure 3a, we can see that the
code-switched English word at CS1 tended to be longer than the average
monolingual English word (∆ = 0.47 characters; p = 6.895e-15). This is in
contrast with previous work such as D’Amico et al. (2001) and Forster and
Chambers (1973), which hypothesized that speakers prefer the language in
whichtherelevantwordisshorterandthuseasiertoproduceduringlanguage
production. Therefore, this finding is inconsistent under the speaker-driven
framework that speakers would code-switch to English and produce longer
words than standard monolingual English, but this finding is consistent with
the audience-driven hypothesis.
Next, we compared the distributions of part-of-speech tags at English
when there is a more precise way to express their meaning in the language they are code-
switching to. In order to address this confound, we asked three native Mandarin speakers
toannotateasubset(10%sample)ofthedata,comparingEnglishandChinesetranslations
of each code-switched sentence and marking which of the two is more precise in meaning
with respect to the target (code-switched) word. We found that in more than two-thirds
of the cases, the Chinese continuation was more precise than or equally as precise as the
English alternative, which suggests that precision is not a dominant factor influencing
code-switching in the vast majority of cases. Thus, we conclude that precision of meaning
is not a significant confound of the study.
14CS1 and in monolingual English. Recall that we had binned parts of speech
into noun, verb, and other. We found that the large majority of English
words at CS1 were nouns (see Figure 3b). This aligns with the Closed Class
Constraint19 (Joshi, 1982), as well as previous work including Myslín and
Levy (2015), which has found that nouns are the most code-switched part-
of-speech, followed by verbs, and finally all other parts of speech. However,
in our written data, verbs were less common than other parts of speech. This
may be driven by the fact that this analysis is based on code-switching in
writing, which may contain more non-verb modifiers than in the previous
speech-based work from the literature. In contrast, we found that the large
majority of words in monolingual English were non-noun/non-verb parts of
speech (see Figure 3b). Second-most represented were nouns, followed by
verbs.
Finally, we analyzed the word frequencies and surprisal of CS1 words and
monolingual English. CS1 words in English were significantly less frequent
than monolingual English words (Figure 3c; p < 2.2e-16), and similarly,
English CS1 words were much more surprising than monolingual English
(Figure 3d; ∆ = 11.23 bits; p < 2.2e-16). Again, these surprisal results are
replicated with surprisal values from a LLM, in this case, GPT-2 (see Fig-
ure A.12 in the Appendix). These findings contrast with previous work (e.g.,
D’Amico et al., 2001; Forster and Chambers, 1973; Calvillo et al., 2020)
which speculated that higher frequency and less surprising words may be
easier to access, and thus may be more likely candidates for code-switches
than lower frequency and more surprising words. However, since these find-
ings were based only on analyses of the primary language (Chinese, in our
case), these previous studies could not test this hypothesis. Our findings
that speakers choose less frequent or more surprising English continuations
than standard monolingual English once again seem counter-intuitive under
the speaker-driven framework, but are consistent with the audience-driven
hypothesis.
So far, our exploration of the features of the secondary language has sug-
gested that code-switched written English is different from standard written
monolingual English along a number of dimensions. Our investigation of
word length, word frequency, and surprisal has reinforced the notion of a
19This constraint suggests that closed-class items, such as pronouns, prepositions, con-
junctions, etc., cannot be code-switched except as part of a larger phrase.
15linguistic difference between code-switched and non-code-switched language,
by demonstrating that code-switched written English at CS1 tends to be
more difficult or linguistically complex than standard written monolingual
English.
In addition to the relative complexity of code-switched English at CS1
compared to standard monolingual English, our exploration of the features
of the secondary language has also indicated that the characteristics of CS1
English words are different from those of code-switches that have been exam-
ined in previous work. In particular, CS1 English words are less accessible
than we would expect them to be under a speaker-driven framework. Thus,
we already have some indications of audience-driven, rather than exclusively
speaker-driven, code-switching occurring in our written data. These findings
may be attributed to the fact that our work differs from previous studies in
examining the secondary language involved in instances of code-switching,
and not just the primary language.
6.2.4. CS1 English is relatively more complex than CS1 Chinese in writing
Our findings so far have revealed that CS1 English tends to be longer,
less frequent, and more surprising than monolingual English, which suggests
thatEnglishcode-switchesmayberelativelycostlytoproduceinwriting. For
the information-theoretic hypothesis supporting speaker-driven influences of
code-switching to be correct, the CS1 in Chinese would need to have an even
larger complexity difference with monolingual Chinese than CS1 in English
does with monolingual English. Such an overriding complexity difference
in Chinese could suggest that the English continuation could be relatively
simpler than continuing in Chinese, in effect making it easier for a speaker
to switch to producing English in writing than remaining in Chinese.
Tomakethiscomparison,wecomparedtherelativesurprisalofCS1words
in both languages to the mean surprisal that can be expected in that lan-
guage (see Figure 4). In order to obtain the relative English surprisal, we
normalized each CS1 English surprisal value by the mean surprisal value of
the monolingual English data. Similarly, we normalized each Chinese CS1
surprisal value by the mean surprisal value of the monolingual Chinese data.
We found that CS1 words in English were more surprising than in Chi-
nese, even after normalizing by the expected surprisal for the monolingual
version of that language. This was also true for surprisals sourced from large
language models (see Figure A.13 in the Appendix). Not only was CS1 in
English more complex than monolingual English according to a number of
16Figure 4: Comparing normalized CS1 surprisal in English to Chinese in writing.
measures, it was even more surprising than code-switched Chinese was in
comparison to monolingual Chinese. Thus, code-switching into English at
CS1 makes a sentence less accessible to a writer than if they had remained
in Chinese. This provides further support for the audience-driven hypothesis
of information load, suggesting that code-switches help to signal regions of
linguistic complexity for the listener rather than being easier for the speaker.
That is, because the Chinese word at CS1 was harder than typical Chinese
constructions, producers code-switched to English to signal that difficulty for
their audience.
6.3. High information load contributes to code-switching at CS1 in speech
Given our results on the influence of audience-driven pressures on writ-
ten code-switching, we next turned to examining the information load of
code-switching in speech. Again, we implemented a binary logistic regres-
sion model to quantify the impact of surprisal on code-switched speech by
distinguishing CS1 words in code-switched utterances from Non-CS words
in Non-CS (i.e. monolingual Chinese) utterances. Since the Non-CS utter-
ances in our data set of spoken code-switches did not each contain a CS1
we simply chose a random word in each monolingual Chinese utterance to
represent the characteristics of a Non-CS word. In the aggregate, these 6171
samples provide us with the characteristics of Non-CS language. For the CS
utterances, we used the characteristics of the CS1, as before.
17Factor coef std err t
Intercept 0.6811 0.008 86.925
POS=verb -0.2605 0.012 -22.422
POS=other -0.2819 0.012 -24.224
Frequency -0.2695 0.010 -26.817
Word length 0.1771 0.010 18.049
Word position -0.0554 0.008 -7.257
Surprisal 0.3160 0.013 24.856
Table 2: Summary of the logistic regression model for CS1 (coded 1) versus random Non-
CS1 (coded 0), in the data set of spoken code-switches.
We used all of the same regression features as for our written code-
switched data, with the exception of measuring word length in terms of
phonemes20 rather than number of keystrokes, to account for the modal-
ity of the data. Again, our regressions revealed that higher surprisal was
correlated with code-switching (see Table 2).
Similartoourpreviouswrittenresults, wefindthatlongerChinesewords,
i.e. words made up of a greater number of phonemes, were more likely to be
code-switched, earlier word positions in an utterance were more likely to be
code-switched, and nouns were more likely to be code-switched, compared
to verbs and other parts of speech. Consistent with our audience-driven hy-
pothesis, we also found that more frequent Chinese words were more likely to
be code-switched in speech. For the most part, our results on code-switched
speech are consistent with those we previously found on code-switched text,
and confirm that higher surprisal regions are more likely to exhibit code-
switching in speech.
6.4. Code-switchinginspeechhasinformation-theoreticaudience-drivencom-
ponents
6.4.1. CS1 English is more complex than monolingual English in speech
As with our experiments on written code-switches, we examine the fea-
tures of the secondary language, English, at CS1 in code-switched utter-
ances. Unlike our written data set, our data set of spoken code-switches
spans a much wider variety of topics and thus does not display any obvious
20This was calculated using the phonemizer python library.
18groupings of jargon-like words. Given this, we retain all of the data for the
remainder of our analyses on the spoken code-switches.
Similar to our previous analyses, we first compared the lengths of English
words at CS1 to the monolingual English present in the speech transcripts of
the CallHome corpus. From Figure 5a, we again see that the code-switched
English word at CS1 tended to be longer than the average monolingual En-
glish word (∆ = 2.013 phonemes; p < 2.2e-16). We note that the ∆ between
code-switched English at CS1 and the average monolingual English word is
larger in speech than in writing. This may be due to differences in effort
required to produce speech compared to typing and the threshold at which
words begin to “feel" longer to speakers, though further work is required to
confirm this hypothesis.
Next, wecomparedthedistributionsofpart-of-speechtagsatEnglishCS1
and in monolingual English. We found, as before, that the large majority
of English words at CS1 were nouns, followed by other parts of speech, and
verbs (see Figure 5a). As before, the large majority of words in monolingual
Englishwerenon-noun/non-verbpartsofspeech. Thiswasfollowedbyverbs,
and finally nouns.
Finally, we analyzed the word frequencies and surprisal of CS1 words and
monolingual English. CS1 words in English were significantly less frequent
than monolingual English words (Figure 5c; p < 2.2e-16), and similarly,
English CS1 words were much more surprising than monolingual English
(Figure 5d; ∆ = 6.515 bits; p < 2.2e-16). Here, we note that the ∆ for
surprisal between code-switched English at CS1 and the average monolingual
English word is smaller in speech than that in writing. This might suggest
that the extent to which word accessibility plays a role in word and language
selection may be greater in written code-switching compared to spoken code-
switching. This is hard to compare, though, since the written language and
spoken language models are trained on different types of data and are of
different sizes.
Our exploration of the features of the secondary language has again sug-
gested that code-switched spoken English is not only different from but also
more linguistically complex than standard (conversational) spoken monolin-
gual English.
6.4.2. CS1 English is relatively more complex than CS1 Chinese in speech
We close out our experiments on the code-switched spoken data by com-
paring the relative surprisal of CS1 words in both languages to the mean
19(a) (b) (c) (d)
Figure5: ComparingCS1inEnglishandmonolingual(ML)Englishacross(a)wordlength,
(b) part-of-speech tag distribution, (c) word frequency, and (d) surprisal, in speech.
surprisal that can be expected in that language (see Figure 6). Similar to
before, we normalized each CS1 English surprisal value by the mean surprisal
value of the monolingual CallHome English data. We also normalized each
Chinese CS1 surprisal value by the mean surprisal of the monolingual Hub5
Chinese data.
We found that CS1 words in English were more surprising than in Chi-
nese, even after normalizing by the expected surprisal for the monolingual
version of that language. In other words, CS1 in English was both more
complex than monolingual English across a number of linguistic features and
more surprising than code-switched Chinese was in comparison to monolin-
gual Chinese. Thus, code-switching into English at CS1 makes an utterance
even more surprising and thus likely less salient for speakers during produc-
tion than if the speaker had remained in Chinese. As with our analyses of
written code-switches, we have found evidence of audience-driven pressures
on information load that seem to use code-switching to help to signal regions
of linguistic complexity for the listener, rather than exclusively making pro-
duction easier for the speaker. We note, however, that compared to written
code-switches, the ∆ for normalized surprisal between English at CS1 and
Chinese at CS1 in speech is smaller. This mirrors our previous observation of
asmallersurprisal∆betweenCS1EnglishandmonolingualEnglishinspeech
relative to writing, providing further support for audience-driven pressures
playing a greater information-theoretic role in code-switching in text than
speech.
20Figure 6: Comparing normalized CS1 surprisal in English to Chinese in speech.
7. Discussion
In this work, we used language modeling to investigate whether, in ad-
dition to the influence of speaker-driven factors, audience-driven pressures
play an information-theoretic role in Chinese-English code-switching. Rather
than focus exclusively on analyses of the primary language (Chinese, in this
case) to draw conclusions about code-switching, as has been common in prior
work, weconductedanumberofanalysesofthesecondarylanguage(English,
in this case) as well, enabling us to resolve a number of details about code-
switching and its relationship with information load in bilingual online forum
posts and transcripts of spontaneous speech.
All of our analyses confirmed prior findings that information load influ-
ences code-switching (Myslín and Levy, 2015; Calvillo et al., 2020). We addi-
tionally found that code-switched English is more linguistically complex than
standard monolingual English and that its features make the code-switches
lessaccessiblethanonewouldexpectunderaspeaker-drivenframework. Fur-
thermore, while surprisal in the primary language is higher at code-switched
regions relative to non-code-switched Chinese, the surprisal in the secondary
language is even higher relative to non-code-switched English. These pat-
terns hold true across both written and spoken modalities. Combined, these
results suggest that code-switching actually increases the information load
compared to a counterfactual monolingual production, though this effect is
21somewhat more pronounced for written code-switches compared to spoken
ones. While prior work has shown the existence of audience-driven identity-
focusedconstraintswhichinfluencecode-switching(e.g., thelistener’sfluency
in a language), our work is the first to demonstrate that code-switching in-
creases information load, providing strong evidence that code-switching in
either modality partially operates as a signal to the audience that a region
has high information content.
Overall, we show that information load serves an audience-driven role
in code-switching, instead of a purely speaker-driven one. In addition, we
replicated our results from traditional n-gram language models using large
neurallanguagemodels, andthusshowedthatlanguagemodelsofeitherkind
are appropriate tools for identifying the patterns we found in our work.
Limitations
While we propose that the method we use in our work is scalable to larger
data sets and other languages, we have analyzed only a single language pair.
Possible fruitful extensions of our work would consider additional language
pairs. We expect that these kinds of analyses may also be influenced by the
degree of overlap between the two languages (e.g., frequency of cognates,
syntactic configurations, etc).
We also acknowledge that our models trained on speech transcripts are
several orders of magnitude smaller than those we trained on written text.
While future work could train larger models and attempt to replicate this
study to confirm the reliability of our findings, we also point out the im-
portance of having smaller, and therefore more accessible and less resource-
intensive, models.
Separately, we relied heavily on automatic translations in our analyses.
We were partially inspired to do so by the general approach of Isbister et al.
(2021), who argue for leveraging the “mature technology” of machine trans-
lation and existing larger, high-performance English language models over
building smaller native language models wherever possible. While we would
expect these automatic translations to provide conservative surprisal values,
we would ideally utilize human translations instead.
We also focused exclusively on the core code-switched regions of code-
switchedsentencesandutterances. Itmaybeworthapplyingsimilaranalyses
to other regions of code-switched sentences and utterances, such as the words
preceding a code-switch, to more deeply investigate the role of code-switch
22triggers, or to investigate information load changes over the course of a code-
switched sentence.
Importantly, there is no way to tell what options speakers had to con-
tinue utterances in Chinese. We are currently planning human experiments
to solicit this information, but such work would be beyond the scope of the
present paper. We viewed this work as simply a first step, showing how one
could use corpora to probe why code-switches occur at areas of high infor-
mation load using written text and speech transcripts from existing corpora.
We acknowledge that human experiments are a clear future direction to take.
Finally, there are differences between our test written data (written in a
conversational style) and the Wikipedia data (written in a more structured
and formal style) used to compute the base model. We originally tried to
scrape monolingual Chinese-language forums to train our models on data of
the same genre as our dataset, but we were unable to scrape enough data
to produce good models. We instead switched to using Wikipedia since this
data was already freely available, and we confirmed our findings using two
LLMs trained on broad internet data.
Ethics Statement
This study was conducted on secondary data only, and did not require
human experiments. We did not access any information that could uniquely
identify individual users within the corpus.
CRediT author statement
Debasmita Bhattacharya: Conceptualization, Methodology, Software,
Formal Analysis, Investigation, Data Curation, Writing - Original Draft,
Writing - Review and Editing, Visualization.
Marten van Schijndel: Conceptualization, Methodology, Formal Anal-
ysis, Resources, Writing - Original Draft, Writing - Review and Editing,
Supervision.
Acknowledgements
This research did not receive any specific grant from funding agencies in
the public, commercial, or not-for-profit sectors.
23Appendix A. Appendix
Figure A.7: Comparing surprisal of non-code-switched Chinese to monolingual Chinese
(p< 2.2e-16; ∆=1.73832 bits).
Figure A.8: Comparing sentence length of code-switched to non-code-switched sentences
(p= 9.549e-12; ∆=1.51694 bits).
24Non-CS CS1 Non-CS Random Non-CS1
word length word length
Min. 1.000 1.000
1st Qu. 1.000 1.000
Median 2.000 1.000
Mean 1.768 1.459
3rd Qu. 2.000 2.000
Max. 3.000 4.000
Table A.3: Summary of Non-CS CS1 and Non-CS Random Non-CS1 word lengths.
Factor coef std err t
Intercept 0.5121 0.006 89.815
POS=verb 0.0185 0.010 1.835
POS=other -0.0404 0.009 -4.362
Frequency 0.8915 0.007 120.334
Word length 0.0572 0.009 6.393
Word position -0.0293 0.008 -3.872
Surprisal 0.0217 0.008 2.657
Table A.4: Summary of the logistic regression model for CS1 (coded 1) versus random
Non-CS1 (coded 0), using LLM surprisal.
POS Length Position Frequency
NN: 888 Min.: 1.0 Min.: 0.000 Min.: 43.02
VV: 214 1st Qu.: 2.0 1st Qu.: 1.000 1st Qu.: 51.88
JJ: 121 Median: 2.0 Median: 3.000 Median: 55.25
AD: 68 Mean: 1.9 Mean: 4.224 Mean: 55.28
VA: 46 3rd Qu.: 2.0 3rd Qu.: 6.000 3rd Qu.: 58.06
CD: 30 Max.: 3.0 Max.: 29.000 Max.: 67.40
(Other): 109
TableA.5: SummaryofCS1features: part-of-speechtag,length,positioninsentence,and
frequency.
25(a) (b) (c) (d)
Figure A.9: Comparing CS1 in English, jargon-like CS1 words in English, and monolin-
gual (ML) English across (a) word length, (b) part-of-speech tag distribution, (c) word
frequency, and (d) surprisal, in writing.
Factor coef std err t
Intercept 0.5176 0.007 75.819
POS=verb 0.0128 0.012 1.106
POS=other -0.0489 0.011 -4.638
Frequency 0.8950 0.009 102.669
Word length -0.0386 0.010 -4.039
Sentence length 0.0427 0.008 5.283
Surprisal 0.0716 0.010 7.109
Table A.6: Summary of the logistic regression model for CS1 excluding jargon-like words
(coded 1) versus random Non-CS1 (coded 0).
Factor coef std err t
Intercept 0.5494 0.010 54.088
POS=verb -0.0215 0.022 -0.968
POS=other -0.1417 0.019 -7.417
Frequency 0.8743 0.015 58.396
Word length -0.0778 0.017 -4.524
Sentence length 0.0240 0.013 1.820
Surprisal 0.0871 0.016 5.511
Table A.7: Summary of the logistic regression model for jargon-like CS1 (coded 1) versus
random Non-CS1 (coded 0).
26Figure A.10: Comparing surprisal of CS1 Eng as produced by Fanyi Youdao (FY), CS1
Eng as produced by Google Translate (GT), and monolingual (ML) English. Since Fanyi
Youdaigavelowersurprisalresults, wechosetousethatforouranalysessinceitprovided
the most conservative test for our investigations.
Figure A.11: Comparing normalized CS1 surprisal in English produced by Fanyi Youdao
(FY), English produced by Google Translate (GT), and Chinese.
27Figure A.12: Comparing LLM surprisal of CS1 in English to monolingual English. (p <
2.2e-16; ∆=3.674112 bits).
Figure A.13: Comparing normalized CS1 surprisal from LLMs in English to Chinese.
28References
Ansaldo, A.I., Marcotte, K., Scherer, L., Raboyeau, G., 2008. Lan-
guage therapy and bilingual aphasia: Clinical implications of psy-
cholinguistic and neuroimaging research. Journal of Neurolinguistics
21, 539–557. URL: https://www.sciencedirect.com/science/article/
pii/S0911604408000146, doi:https://doi.org/10.1016/j.jneuroling.
2008.02.001.acquisition, ProcessingandLossofL2: Functional, cognitive
and neural perspectives.
Beatty-Martínez, A.L., Navarro-Torres, C.A., Dussias, P.E., 2020.
Codeswitching: A bilingual toolkit for opportunistic speech planning.
Frontiers in Psychology 11. URL: https://www.frontiersin.org/
article/10.3389/fpsyg.2020.01699, doi:10.3389/fpsyg.2020.01699.
Bell, A., 1984. Language style as audience design. Language in Society 13,
145–204. URL: http://www.jstor.org/stable/4167516.
Broersma, M., 2009. Triggered codeswitching between cognate languages.
Bilingualism: Language and Cognition 12, 447–462. doi:10.1017/
S1366728909990204.
Calvillo, J., Fang, L., Cole, J., Reitter, D., 2020. Surprisal predicts
code-switching in Chinese-English bilingual text, in: Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP), Association for Computational Linguistics, Online. pp. 4029–
4039. URL: https://aclanthology.org/2020.emnlp-main.330, doi:10.
18653/v1/2020.emnlp-main.330.
Canavan, A., Graff, D., Zipperlen, G., 1997. CALLHOME American English
Speech.
Consortium, L.D., 2018. HUB5 Mandarin Telephone Speech and Transcripts
Second Edition LDC2018S18.
Dahl, K.L., 2009. Audience design and code-switching in Bayside, Texas.
Ph.D. thesis. The University of Texas at Austin.
D’Amico, S., Devescovi, A., Bates, E., 2001. Picture naming and lexical
accessinitalianchildrenandadults. JournalofCognitionandDevelopment
- J COGN DEV 2. doi:10.1207/S15327647JCD0201_4.
29Dornic, S., 1978. The Bilingual’s Performance: Language Dominance,
Stress, and Individual Differences. Springer US, Boston, MA. chapter null.
pp. 259–271. URL: https://doi.org/10.1007/978-1-4615-9077-4_23,
doi:10.1007/978-1-4615-9077-4_23.
Forster, K.I., Chambers, S.M., 1973. Lexical access and naming time. Jour-
nal of Verbal Learning and Verbal Behavior 12, 627–635. URL: https:
//www.sciencedirect.com/science/article/pii/S0022537173800428,
doi:https://doi.org/10.1016/S0022-5371(73)80042-8.
Gardner-Chloros, P., 2009. Code-switching. Cambridge University Press.
Hassan, A.A., 2019. English-Arabic Code-Switching of the Arabic Language
Speakers in Instant Messaging: Motivations and Structure. Cairo Studies
in English 2019, 39–59. doi:10.21608/cse.2019.66656.
Hofweber, J., Marinis, T., 2023. What sentence repetition tasks can re-
veal about the processing effort associated with different types of code-
switching. Languages 8. URL: https://www.mdpi.com/2226-471X/8/1/
70, doi:10.3390/languages8010070.
Isbister, T., Carlsson, F., Sahlgren, M., 2021. Should we stop training
moremonolingualmodels, andsimplyusemachinetranslationinstead?, in:
Proceedings of the 23rd Nordic Conference on Computational Linguistics
(NoDaLiDa), Linköping University Electronic Press, Sweden, Reykjavik,
Iceland (Online). pp. 385–390.
Joshi, A.K., 1982. Processing of sentences with intra-sentential code-
switching, in: Coling 1982: Proceedings of the Ninth International Confer-
enceonComputational Linguistics, p. null. URL:https://aclanthology.
org/C82-1023.
Liu, H., Liu, Z., Yuan, M., Chen, T., 2023. The effect of
cognitive load on code-switching. International Journal of
Bilingualism 0, 13670069231170142. URL: https://doi.org/
10.1177/13670069231170142, doi:10.1177/13670069231170142,
arXiv:https://doi.org/10.1177/13670069231170142.
Liu, K., 2021. Language choices as audience design strategies in Chinese
multilingual speakers’ Wechat posts. Global Media and China 6, 391–
30415. URL: https://doi.org/10.1177/20594364211035201, doi:10.1177/
20594364211035201.
Lyu, D.C., Tan, T.P., Chng, E., Li, H., 2010. Mandarin–English code-
switching speech corpus in South-East Asia: SEAME, in: Language Re-
sources and Evaluation, pp. 1986–1989. doi:10.1007/s10579-015-9303-x.
Meisel, J.M., 1994. Code-switching in young bilingual children: The acquisi-
tion of grammatical constraints. Studies in Second Language Acquisition
16, 413–439. URL: http://www.jstor.org/stable/44487780.
Misra, K., 2022. minicons: Enabling flexible behavioral and representational
analyses of transformer language models. arXiv preprint arXiv:2203.13112
.
Muysken, P., 2000. Bilingual speech: a typology of code-mixing. Cambridge
University Press.
Myers-Scotton, C., 1993. Duelling Languages. Oxford: Clarendon Press.
Myslín, M., Levy, R.P., 2015. Codeswitching and predictability of meaning
in discourse. Language 91, 871 – 905.
Owens, J., 2005. Bare forms and lexical insertions in code-switching: A
processing-based account. Bilingualism: Language and Cognition 8, 23–
38.
Poplack, S., 1980. Sometimes I’ll start a sentence in Spanish Y TERMINO
EN ESPAÑOL: toward a typology of code-switching. Linguistics 18, 581–
618. doi:10.1515/ling.1980.18.7-8.581.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., 2019.
Language models are unsupervised multitask learners. null .
31