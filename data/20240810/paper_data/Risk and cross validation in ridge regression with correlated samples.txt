PublishedasaconferencepaperatICLR2024
RISK AND CROSS VALIDATION IN RIDGE REGRESSION
WITH CORRELATED SAMPLES
AlexanderAtanasov1,2,∗,JacobA.Zavatone-Veth2,3,∗,andCengizPehlevan2,4,5
1DepartmentofPhysics,
2CenterforBrainScience,
3SocietyofFellows,
4JohnA.PaulsonSchoolofEngineeringandAppliedSciences,
5KempnerInstitutefortheStudyofNaturalandArtificialIntelligence,
HarvardUniversity
Cambridge,MA02138USA
{atanasov@g,jzavatoneveth@fas,cpehlevan@seas}.harvard.edu
ABSTRACT
Recent years have seen substantial advances in our understanding of high-
dimensionalridgeregression,butexistingtheoriesassumethattrainingexamples
are independent. By leveraging recent techniques from random matrix theory
andfreeprobability,weprovidesharpasymptoticsforthein-andout-of-sample
risks of ridge regression when the data points have arbitrary correlations. We
demonstratethatinthissetting,thegeneralizedcrossvalidationestimator(GCV)
failstocorrectlypredicttheout-of-samplerisk. However,inthecasewherethe
noiseresidualshavethesamecorrelationsasthedatapoints,onecanmodifythe
GCVtoyieldanefficiently-computableunbiasedestimatorthatconcentratesinthe
high-dimensionallimit,whichwedubCorrGCV.Wefurtherextendourasymptotic
analysistothecasewherethetestpointhasnontrivialcorrelationswiththetraining
set,asettingoftenencounteredintimeseriesforecasting. Assumingknowledge
ofthecorrelationstructureofthetimeseries,thisagainyieldsanextensionofthe
GCVestimator,andsharplycharacterizesthedegreetowhichsuchtestpointsyield
anoverlyoptimisticpredictionoflong-timerisk. Wevalidatethepredictionsof
ourtheoryacrossavarietyofhighdimensionaldata.
1 INTRODUCTION
Statisticsclassicallyassumesthatonehasaccesstoindependentandidenticallydistributedsamples,
whetherinloworhighdimension. However,thisfundamentalassumptionisoftenviolatedwhenone
considersdatasampledfromatimeseries—e.g.,inthecaseoffinancial,climate,orneuroscience
data(Bouchaud&Potters,2003;Mudelsee,2014;Williams&Linderman,2021)—renderingresults
obtainedunderthei.i.d. assumptioninapplicable. Intheparadigmaticsettingofridgeregression,a
significantbodyofrecentresearchhasaimedtoprovidesharpasymptoticcharacterizationsofthe
out-of-sampleriskwhenthein-sampleriskisestimatedusingi.i.d. samples(Advani&Ganguli,
2016;Atanasovetal.,2024;Bordelonetal.,2020;Canataretal.,2021b;Dobriban&Wager,2018;
Geraceetal.,2020;Hastieetal.,2022;Jacotetal.,2020;Loureiroetal.,2021;Mel&Ganguli,2021;
Mel&Pennington,2021). Theseomniscientcharacterizationsoftheout-of-samplerisk—socalled
becausetheyassumeknowledgeofthetruedistributionofthecovariates—areintimatelyrelatedto
non-omniscientestimatorsbasedongeneralizedcrossvalidation(GCV),whichsimilarlyassume
independence(Atanasovetal.,2024;Batesetal.,2024;Jacotetal.,2020). Howtheseresultsextend
tocorrelatedsamplesisasyetpoorlyunderstood.
Here,wefillthisgapinunderstandingbyprovidingadetailedasymptoticcharacterizationofridge
regressionwithcorrelatedsamples. Wefirstshowthatevenunderrelativelymildsample-sample
correlationspreviously-obtainedomniscientriskasymptoticsassumingi.i.d. dataarenotpredictive.
Correspondingly, the ordinary GCV estimator fails to accurately estimate the out-of-sample risk.
∗AAandJAZVcontributedequallytothiswork.
1
4202
guA
8
]LM.tats[
1v70640.8042:viXraPublishedasaconferencepaperatICLR2024
Exponential Correlations N=100 =1.8, r=0.3 Correlated Regression N=100 =1.8, r=0.3
=1e-02, =1e-04, =0.05 =1e+02, =1e-04, =0.03
100
102
101
103
R
102 R
104 Bias2
103 VarX
VarX,
105 Naive GCV1
104 Naive GCV2
CorrGCV
106
105
107
106
101 102 103 101 102 103
T T
Figure1: EmpiricalriskRˆ,out-of-sampleriskR,andfine-grainedbias-variancedecompositionsfor
ridgeregressionwithstructuredfeaturesandcorrelatedexamples. Theoryisplottedinsolidlines.
Experimentswitherrorbarsover10datasetrepetitionsareplottedasmarkers. Thedatapointsare
exponentiallycorrelatedasE[x ·x ] ∝ e|t−s|/ξ. Left: Weakcorrelations, ξ = 10−2. Here, the
t s
generalizedcrossvalidationmethod(orchid)aswellasitsotherproposedcorrectionsinthepresence
ofcorrelations(pink,purple)allagreeandareoverlaid. Right: Strongcorrelations,ξ =102. Here,
weseethatthenaiveestimatesoftheGCVproposedinpriorworksfailinthissetting. Theyeither
underestimate(purple)oroverestimate(pink)theout-of-samplerisk. WedefinethenaiveGCVs
inAppendixA,andconnectthemwithpriorproposalsinAppendixG.Bycontrast,ourproposed
estimator,CorrGCV,correctlypredictstheout-of-sampleriskinallsettings.
Wethencomputesharphigh-dimensionalasymptoticsfortheout-of-sampleriskwhenthetraining
examplesaredrawnfromageneralmatrixGaussianwithanisotropiccorrelationsacrossbothfeatures
and samples. In the case when the test point is uncorrelated with the training data and the label
noisehasthesamecorrelationstructureasthecovariates,weshowthatthereexistsacorrectedGCV
estimator. WetermthisestimatortheCorrGCV.Unlikepreviousattemptstocorrecttheoriginal
GCV,CorrGCVisasymptoticallyexact(Figure1).
InderivingtheCorrGCV,weuncoveraninterestingdualitybetweentrain-testcovariateshiftand
a mismatch in the covariate and noise correlations. Finally, we extend all of these results to the
casewhenthetestpointiscorrelatedwiththetrainingpoints. Focusingonthesettingoftimeseries
regression,thisgivesusasharpcharacterizationofhowaccuracydependsonpredictionhorizon,
andmakesprecisethenotionthattestingonnear-horizondatagivesanoverlyoptimisticpictureof
long-termforecastaccuracy. Inall,ourresultsbothadvancethetheoreticalunderstandingofridge
regressioninhighdimensionsandprovideapplicationtotimeseriesdata.
1.1 RELATEDWORK
Wenowbrieflyreviewtwothreadsofpastworkrelatedtoourresults: sharpasymptoticsforridge
regression,andcross-validationestimatorsofout-of-samplerisk. Webeginwithwiththeformer
topic. Recentyearshaveseenanexplosionofinterestinhigh-dimensionalasymptoticsforridge
regression(Advani&Ganguli,2016;Atanasovetal.,2024;Bordelonetal.,2020;Canataretal.,
2021b;Dobriban&Wager,2018;Geraceetal.,2020;Hastieetal.,2022;Jacotetal.,2020;Loureiro
et al., 2021; Mel & Ganguli, 2021; Mel & Pennington, 2021). The crucial feature of these high-
dimensionalasymptoticsisthattheyallowboththedimensionalityofthecovariatesandthenumber
ofexamplestobelarge. Incontrast,classicalasymptoticsassumethatthedimensionissmallrelative
tothenumberofexamples(Hastieetal.,2009). Thisbodyofworkrevealstwobroadprinciples:
First,ridgeregressionhasaspectralbiastowardslearningfunctionsalignedwitheigenfunctionsof
thefeaturecovariance. Second,ridgeregressiondisplaysGaussianuniversalityinhighdimension,
i.e.,theout-of-sampleriskforagivendatasetwillbeasymptoticallyidenticaltothatforaGaussian
datasetwithmatchedfirstandsecondmoments(Hastieetal.,2022;Hu&Lu,2022;Montanari&
Saeed,2022). Importantly,nearlyalloftheseworksassumethetrainingexamplesarei.i.d.. Arare
exceptionisarecentpaperbyBigotetal.(2024),whoallowforindependentbutnon-identically
distributedGaussiandatawithper-examplemodulationoftotalvariance.
2
rorrE rorrEPublishedasaconferencepaperatICLR2024
These sharp asymptotics are omniscient risk estimates: they assume one has access to the true
jointdistributionofcovariatesandlabelsfromwhichthetrainingsetissampled. Ifonewantsto
performhyperparametertuninginpractice—forridgeregressionthisofcoursemeanstuningthe
ridgeparameter—anon-omniscientestimateisrequired. Astandardapproachtothisproblemisthe
methodofgeneralizedcross-validation,whichdatesbackatleasttothe1970s(Batesetal.,2024;
Craven&Wahba,1978;Golubetal.,1979). TheGCVestimatestheout-of-sampleriskbyapplying
amultiplicativecorrectiontothein-samplerisk,whichitselfcanbeestimatedfromthedata. Recent
works have shown that the GCV is asymptotically exact in high dimensions: the limiting GCV
estimatecoincideswiththeomniscientasymptotic(Atanasovetal.,2024;Hastieetal.,2022;Jacot
etal.,2020). Thoughclassicalcross-validationandtheGCVestimatorassumei.i.d. trainingdata
points,severalworkshaveaimedtoextendthesemethodstocasesinwhichtherearecorrelations
(Altman,1990;Carmacketal.,2012;Lukas,2010;Opsomeretal.,2001;Rabinowicz&Rosset,
2022). However,mostoftheseworksfocusoncorrelationsonlyinthelabelnoise,andnoneofthem
showthattheirestimatorsareasymptoticallyexactinhighdimensions. Indeed,wewillshowthat
noneofthepreviously-proposedcorrectionstotheGCVareasymptoticallyexactinhighdimensions,
andrequirefurthermodificationtoaccuratelypredictout-of-samplerisk.
2 REVIEW OF FREE PROBABILITY AND DETERMINISTIC EQUIVALENCE
2.1 SETUPANDNOTATION
WeconsiderridgeregressiononadatasetD = {x ,y }T ofT datapoints,withN-dimensional
t t t=1
covariatesx ∈RN andscalarlabelsy . Weminimizethemean-squarederroroverthisdataset,with
t t
aridgepenalty:
T
1 (cid:88)
L(w)= (y −x⊤w)2+λ|w|2. (1)
T t t
t=1
Wewritewˆ =argmin L(w). Aslongasλ>0thesolutionisunique. Wewillconsiderboththe
w
underparameterizedcaseofclassicalstatisticswhereT >N aswellasthemodernoverparameterized
settingwhereN > T. Wedefinetheoverparameterizationratioq ≡ N/T. Allofourresultswill
holdexactlyinthelimitofN,T →∞withqfixed.
Wenowstateourstatisticalassumptionsonthedata. DefiningthedesignmatrixX ∈RT×N such
thatX =[x ] andcollectingthelabelsintoavectory ∈RT,weassumethelabelsaregenerated
ti t i
fromadeterministiclinear“teacher”w¯ ∈SN−1plusnoiseϵ∈RT as:
y =Xw¯ +ϵ. (2)
WeassumethatX hasthefollowingcorrelationstructure:
E[x x ]=Σ K (3)
i,t j,s ij ts
for a feature-feature covariance Σ ∈ RN×N and a sample-sample correlation K ∈ RT×T. We
takeK = 1. WehavetherepresentationX = K1/2ZΣ1/2 forZ ∈ RT×N amatrixwithi.i.d.
tt
standard Gaussian elements Z ∼ N(0,1), where Σ1/2 denotes the principal square root of the
ti
positive-definite symmetric matrix Σ. We assume that the noise ϵ is independent of X, and is
Gaussianwithmeanzeroandcovariance
E[ϵ ϵ ]=σ2K′ . (4)
t s ϵ ts
WecontrastthiswithpriortreatmentsoflinearregressionintheproportionallimitwhereK,K′were
chosentobetheidentitymatrix(Advani&Ganguli,2016;Atanasovetal.,2024;Bordelonetal.,
2020;Canataretal.,2021b;Dobriban&Wager,2018;Geraceetal.,2020;Hastieetal.,2022;Jacot
etal.,2020;Loureiroetal.,2021;Mel&Ganguli,2021;Mel&Pennington,2021). InSection3.2
weconsiderthecasewhereK =K′. There,weshowthatthegeneralized-cross-validation(GCV)
estimator(GCV)(Golubetal.,1979)hasanaturalanalogue. InSection3.3weconsiderthemore
generalcaseandshowthatthereisanobstructiontoaGCVestimator.
We define the empirical covariance Σˆ ≡ 1X⊤X and kernel Gram matrix Kˆ ≡ 1XX⊤.
T T
Writingy ∈RT asthevectoroftraininglabelsandϵ∈RT asthevectoroflabelnoises,wehave:
X⊤y X⊤ϵ
wˆ =(Σˆ +λ)−1 =Σˆ(Σˆ +λ)−1w¯ +(Σˆ +λ)−1 . (5)
T T
3PublishedasaconferencepaperatICLR2024
Wealsoletyˆ=Xwˆ bethevectorofpredictionsonthetrainingset. Weareinterestedinanalytically
characterizingthein-sampleandout-of-samplerisks. Thein-sampleriskisdefinedas:
1
Rˆ (wˆ)≡ |y−yˆ|2. (6)
in T
Fortheoutofsamplerisk,weweconsideraheld-outtestpointxandlabelnoiseϵdrawnfromthe
samemarginaldistributionasasingleexamplex ,ϵ .
t t
R (wˆ)≡E |x⊤w¯ +ϵ−x⊤wˆ|2 =(w¯ −wˆ)⊤Σ(w¯ −wˆ)⊤+σ2. (7)
out x,ϵ ϵ
(cid:124) (cid:123)(cid:122) (cid:125)
Rg
HerewehaveidentifiedthegeneralizationerrorR astheexcessrisk,i.e.,R minustheBayes
g out
errorσ2. Westudythecasewherethetestpointandlabelnoisearedrawnindependentlyfromthe
ϵ
testsetinSection3. Westudythemoregeneralcasewherex,ϵhavenontrivialcorrelationswiththe
trainingsetinSection4.
Weremarkatthispointthatonecouldinsteadconsideranestimatorbasedonaweightedloss
1
L (w)= (Xw−y)⊤M(Xw−y)+λ|w|2 (8)
M T
for some positive-definite matrix M (Bouchaud & Potters, 2003; Hastie et al., 2009). Indeed,
the Bayesian minimum mean squared error estimator is equivalent to minimizing this loss with
M = (K′)−1 (Appendix H). However, under our assumptions on the data this is equivalent to
consideringanisotropiclossunderthemappingK ←M1/2KM1/2 andK′ ←M1/2K′M1/2.
Asaresult,ourasymptoticsapplyimmediatelytogeneralchoicesofM.
2.2 THES-TRANSFORMANDDETERMINISTICEQUIVALENCE
WedefinethefirstandseconddegreesoffreedomofamatrixA∈RN×N as
1 1
df1 (λ)= Tr[A(A+λ)−1], df2 (λ)= Tr[A2(A+λ)−2]. (9)
A N A N
Whenitisclearfromcontext,wewillwritetheseasdf ,df . TheS-transformS(df)isaformal
1 2
functionofavariabledf definedas
1−df
S (df)≡ . (10)
A dfdf−1(df)
A
Heredf−1isthefunctionalinverseoftheequationdf =df1 (λ),i.e.,df−1(df1 (λ))=λ. Wewill
A A A A
usethesymboldf tohighlightitsroleasaformalvariableintheS-transform,whereaswewillcall
df the actual value of the degrees of freedom for a given matrix A at a ridge λ. Our definition
1
ofS differsbyasignfromthecommondefinitionoftheS transformintermsofthet-function
A
t (λ)=−df1 (−λ). Equation(10)impliesthat:
A A
1
df1 (λ)= . (11)
A 1+λS (df1 (λ))
A A
TheS-transformplaysacrucialroleinhigh-dimensionalrandommatrixtheoryandfreeprobability
because for two symmetric matrices A,B that are free of one another, taking the symmetrized
productA∗B ≡A1/2BA1/2withA1/2theprincipalmatrixsquarerootyields:
S (df)=S (df)S (df). (12)
A∗B A B
Thisimpliesthefollowingsubordinationrelation:
1
df1 (λ)=
A∗B 1+λS (df1 (λ))
A∗B AB (13)
1
= =df1 (λS (df1 (λ))).
1+λS (df1 (λ))S (df1 (λ)) A B AB
A AB B AB
We define what it means for two random variables to be free, and give a longer discussion on
subordinationrelationsandRandS transformsinAppendixB.
4PublishedasaconferencepaperatICLR2024
Often, one views B as a source of multiplicative noise, and A as deterministic. Then A∗B is
arandommatrix. Theaboveequationthusrelatesthedegreesoffreedomofarandommatrixto
thedegreesoffreedomofadeterministicone. Forthisreasonitisknownas(weak)deterministic
equivalence. Theweakdeterministicequivalence(13)extendstoan“equality”ofmatrices
AB(AB+λ)−1 ≃A(A+κ)−1, κ=λS(df1 (λ)), (14)
AB
where≃denotesthattheaboveexpressionreducestoanasymptoticequalitywhentracedagainstany
testmatrixoffinitespectralnorm. Thisisknownasstrongdeterministicequivalence. Inphysical
terms,wecaninterpret(14)asarenormalizationeffect: theeffectoftherandomfluctuationsinB
canbeabsorbedintoarenormalizedridgeκ(Atanasovetal.,2024). Thisrenormalizationgenerates
implicitregularization. Eveninthelimitofzeroregularization,onecanhavelim κ > 0(See
λ→0
AppendixF).
Because it involves a single matrix inverse of a random matrix, we refer to (14) as a one-point
deterministicequivalent. Wewillalsorequiretwo-pointdeterministicequivalentsthatgivethe
asymptoticsoftracesofpairsofresolventsagainsttwotestmatrices. Wederivetheseequivalentsin
AppendixC.
3 PREDICTING AN UNCORRELATED TEST SET FROM CORRELATED DATA
3.1 WARMUP: LINEARREGRESSIONWITHOUTCORRELATIONS
Webeginbyreviewingthepriorknownresultsinthecasewherethedatapointsareassumedtobe
drawni.i.d. fromadistributionwithcovarianceΣ. Inthiscase,wetakeadesignmatrixX =ZΣ1/2
wheretheelementsofZ aredrawni.i.d. fromadistributionofmeanzeroandvariance1. Thisis
caseofK =Iintroducedabove. Then,thedeterministicequivalence(14)yields
Σˆ(Σˆ +λ)−1 ≃Σ(Σ+κ)−1. (15)
Bytakingthenormalizedtraceofbothsides,thisyieldsdf1 (λ)=df1 (κ). Here,κsatisfies:
Σˆ Σ
λ
κ=λS (df )= , df ≡df1 (κ)≃df1 (λ). (16)
T1Z⊤Z 1 1−qdf
1
1 Σ Σˆ
HerewehaveusedthattheS-transformofaWishartmatrixS =(1−qdf )−1. Atthispoint,
1Z⊤Z 1
T
wenotethatdependingonwhetheronepicksdf = df1 (κ)ordf = df1 (λ),thiseithergivesa
1 Σ 1 Σˆ
self-consistentequationgivenomniscientknowledgeofΣorawaytoestimateκfromthedataalone,
namelybycomputingdf (λ). Then,thegeneralizationerrorcanbewrittenas
Σˆ
κ2 γ
R =(w¯ −wˆ)⊤Σ(w¯ −wˆ)= w¯⊤Σ(Σ+κ)−2w¯ + σ2, γ =qdf . (17)
g 1−γ 1−γ ϵ 2
This result is well-known (Atanasov et al., 2024; Bordelon et al., 2020; Canatar et al., 2021b;
Dobriban&Wager,2018;Hastieetal.,2022;Loureiroetal.,2021). Itfollowsasaspecialcaseof
ourderivationsinAppendixC.4. Similarly,onecancalculate:
1 λ2 1
Rˆ ≃ w¯⊤Σ(Σ+κ)−2w¯ + σ2. (18)
in 1−γ κ21−γ ϵ
Immediately,thisyields:
κ2
R ≃ Rˆ =S2Rˆ . (19)
out λ2 in in
SinceS dependsonlyondf,whichcanbeestimatedsolelyfromthedata,therighthandsideofthis
equationyieldsawaytoestimatetheoutofsampleerrorfromthetrainingerroralone. Thisisknown
asthegeneralized-cross-validationestimatororGCV(Craven&Wahba,1978;Golubetal.,1979).
IthasalsoappearedinrecentliteratureasthekernelalignmentriskestimatororKARE(Jacotetal.,
2020). ThefactthattheGCVestimatorisdirectlyrelatedtotheS-transformwasfirstpointedoutby
Atanasovetal.(2024).
5PublishedasaconferencepaperatICLR2024
Exponential Correlations N=5000 =1.2, r=0.8 Power Law Correlations N=5000 =1.2, r=0.4
=1e+02, =1e-06, =0.0 =0.3, =0e+00
101
100
102 R
102 Bias2
VarX
104 103 N N Coa a ri iv v rGe e CG G VC CV V1 2
T2min(r,1)
104
106
105
101 102 103 101 102 103
T T
(a) ExponentialCorrelations (b) PowerLawCorrelations
Figure2: Powerlawscalingsfordatawitha)exponentialcorrelationswithξ =102 andb)power
lawcorrelationsEx⊤x ∝τ−χ withχ=0.3. Inbothcases,thecorrelationsofthedatadonot
t t+τ
affectthescalingofthegeneralizationerrorasafunctionofT,whichgenerallygoesasT−2αmin(r,1),
asderivedinpriorworks. Althoughotherestimatorscorrectlypredicttherateofdecay,onlythe
CorrGCVcorrectlyrecoverstheexactrisk.
3.2 CORRELATEDDATAWITHIDENTICALLYCORRELATEDNOISE
Wenowstatetheresultsforgeneralcorrelateddatawithmatchedcorrelationsbetweenthecovariates
andnoise(K =K′). FromthepropertiesoftheS-transform,wehavetheone-pointdeterministic
equivalences:
Σˆ(Σˆ +λ)−1 ≃Σ(Σ+κ)−1, Kˆ(Kˆ +λ)−1 ≃K(K+κ˜)−1. (20)
Here,themultiplicativenoiseis 1Z⊤KZ inthefirstcaseand 1ZΣZ⊤inthesecond. Wedefine
T T
df ≡df1 (κ),d˜f ≡df1 (κ˜),df ≡df2 (κ),andd˜f ≡df2 (κ˜). Asbefore,κ,κ˜aredefinedvia:
1 Σ 1 K 2 Σ 2 K
κ=λS(df ), κ˜ =λS˜(d˜f ). (21)
1 1
BecausethemultiplicativenoiseisnowastructuredWishartmatrix,theS-transformsappearingthe
κequationsaremodifiedto:
(cid:18) (cid:19) (cid:18) (cid:19)
N T
S(df )=S (df )S df , S˜(d˜f )=S (d˜f )S d˜f . (22)
1 T1Z⊤Z 1 K T 1 1 T1ZZ⊤ 1 Σ N 1
Usingtheone-pointdeterministicequivalents,wefindthatκandκ˜arerelatedbytheidentity
qdf ≡qdf1 (κ)≃qdf1 (λ)=df1 (λ)≃df (κ˜)≡d˜f (23)
1 Σ Σˆ Kˆ K 1
Inaddition,weshowinAppendixCthatκandκ˜satisfythedualityrelation: κκ˜/λ=1/d˜f .
1
Theproofsofthenecessarytwo-pointdeterministicequivalencesaswellasadditionalidentitiesare
providedinAppendixC.IntermsofthesequantitieswederiveinAppendixC.4that:
κ2 γ df d˜f
R = w¯⊤Σ(Σ+κ)−2w¯ + σ2, γ ≡ 2 2. (24)
g 1−γ 1−γ ϵ df 1d˜f
1
Similarly,weobtain:
d˜f −d˜f κ2 d˜f −d˜f 1
Rˆ = 1 2 w¯⊤Σ(Σ+κ)−1w¯ + 1 2 σ2. (25)
in Sd˜f 1−γ Sd˜f 1−γ ϵ
1 1
ThisyieldsanextensionoftheGCVestimatortocorrelateddata. WecallthistheCorrGCVfor
correlatedgeneralizedcross-validation:
d˜f
R =S(df ) 1 Rˆ . (26)
out 1 d˜f −d˜f in
1 2
6
rorrE rorrEPublishedasaconferencepaperatICLR2024
ThisestimatorisunbiasedandasymptoticallyexactintheproportionallimitofT,N →∞.Moreover,
itconcentratesoverdrawsofthedatasetforT sufficientlylarge.LiketheordinaryGCV,theCorrGCV
canbeestimatedfromthetrainingdataalone. WegivetheexplicitalgorithminAppendixA.In
AppendixGwecomparethisestimatorwithotherpreviouslyproposedestimatorsofout-of-sample
riskincorrelateddata. Weverifythattheseformulaeworkacrossavarietyoftaskswithvarious
correlationstructureinAppendixJ.
In Figure 2, we study the scaling of R as a function of T when the covariates have power law
g
correlation. Thatis,Σhaseigenvaluesλ ∼k−αforanexponentαknownasthecapacity. Further,
k
thesignalw¯ hasthatλ w¯2 ∼k−(2αr+1)foranexponentrknownasthesource. Forcorrelateddata,
k k
thescalingisunchangedfrompriorpredictionsofoptimalratesinBordelonetal.(2020);Caponnetto
& De Vito (2007); Cui et al. (2021); Spigler et al. (2020), namely that R ∼ T−2αmin(r,1). See
g
AppendixEforfurtherdiscussion.
3.3 MISMATCHEDCORRELATIONSANDOODGENERALIZATION
WegeneralizetheaboveresulttoasettingwhereϵdoesnothavethesamecorrelationstructureasK.
Thatis,E[ϵ ϵ ]=σ2K′ . HerewetakeK =1forallt.
t s ϵ ts tt
Infact,wecanconsideranevenmoregeneralcase: namelywhenthecovarianceofthetestpoint
Σ′isalsodifferentfromthecovarianceofthetrainingsetΣ. Thisisthecaseofout-of-distribution
generalizationundercovariateshift. Thesetwodifferentmismatchesexhibitasurprisingduality,
whichwehighlight. ThefullgeneralizationerrorisderivedinC.4tobe:
γ γ
R ≃κ2w¯(Σ+κ)−1Σ′(Σ+κ)−1w¯+κ2 ΣΣ′ w¯Σ(Σ+κ)−2w¯+ ΣΣ′KK′ σ2. (27)
g 1−γ 1−γ ϵ
(cid:124) (cid:123)(cid:122) (cid:125)
Bias2 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
VarX VarXϵ
Here,forclarityhavehighlightedhowtherisknaturallysplitsintothreetermsgivenbyrespective
biasandvariancecomponents,asoutlinedinAppendixD.Wehavealsodefined:
df2 df2 df2 df2
γ ≡ ΣΣ′ K, γ ≡ ΣΣ′ KK′, (28)
ΣΣ′ df d˜f ΣΣ′KK′ df d˜f
1 1 1 1
df2 ≡ 1 Tr(cid:2) ΣΣ′(Σ+κ)−2(cid:3) , df2 ≡ 1 Tr(cid:2) KK′(K+κ)−2(cid:3) . (29)
ΣΣ′ N KK′ T
ThetrainingerrorisalsoderivedinC.4andcanbewrittenas:
d˜f −d˜f κ2 (cid:20) 1 df −df df2 (cid:21)
Rˆ ≃ 1 2 w¯⊤Σ(Σ+κ)−2w¯+σ2κ˜ TrK′(K+κ˜)−1− 1 2 KK′ . (30)
Sd˜f
1
1−γ ϵ T 1−γ df
1
Wevalidatetheseformulaeacrossseveralexperiments. Asaspecialcase,settingK =I recovers
T
previously-obtainedasymptoticsfortheriskofridgeregressionundercovariateshift(Canataretal.,
2021a;Patiletal.,2024;Tripuranenietal.,2021). Ingeneral,thetrainingerrorisproportionaltothe
generalizationerroronlyinthesettingwhereΣ=Σ′andK =K′. Thus,itisonlyinthissetting
thataGCVexists.
3.4 EFFECTONDOUBLEDESCENT
Itisinterestingtoconsiderhowcorrelationstructureaffectsdoubledescent. FromEquations(21),
(22)onehasinthestructuredcasethat:
λ
κ= S (qdf ); (31)
1−qdf K 1
1
comparewithEquation(16)intheuncorrelatedsetting. Intheridgelesslimit,κwillbesuchthat
qdf1 (κ)=1,whichisunchangedfromtheuncorrelatedsetting(AppendixF).Thedoubledescent
Σ
behaviorinthatcaseremainsunchanged. Atfiniteridge,weseethattrainingoncorrelateddatais
similartoreplacingλ→λS (qdf )withuncorrelateddata. Nearthedoubledescentpeak,thisgoes
K 1
asλS (1). InAppendixF,weshowthatS ≥ 1pointwise. Thus,correlationswillenhancethe
K K
ridgeneartheinterpolationthreshold,meaningthatthedouble-descentpeakshouldbemollified. For
strongexponentialcorrelationsthisisapproximatelyλξ. Thisleadstothelesssharpdoubledescent
7PublishedasaconferencepaperatICLR2024
Correlated Regression N=100, Isotropic Data
Correlated Regression, N=100, Isotropic Data Correlated Regression, N=100, Isotropic Data =1e+02, =1e-04, =0.5
=1e-02, =1e-04, =0.5 =1e+02, =1e-04, =0.5 (uncorrelated noise)
101 101 102
100 100
101
R
R
100 Bias2 101 101 VarX
101 V Na ar ivX,
e GCV1
102 102
102
N Coa riv rGe CG VCV2
103 103 103
104 101 102 103 104 101 102 103 104 101 102 103
T T T
(a) (b) (c)
Figure3: Thepreciseasymptoticsforlinearregressionundergoingdoubledescentonunstructured
dataacrossvariouscorrelations. Wechooseanexponentialcorrelationwithcorrelationlengthξand
varyξ. a)Weaklycorrelateddataandnoise,givingrisetothetraditionaldoubledescentcurveas
analyzedinAdvanietal.(2020);Hastieetal.(2022). AllGCV-relatedestimatorsagreeandcorrectly
estimatetheout-of-samplerisk. b)Stronglycorrelateddatawithmatchednoisecorrelations. Inthis
case,thedoubledescentpeakismollified. c)Stronglycorrelateddatabutuncorrelatednoise. Inthis
casethedoubledescentpeakisexacerbated. Thismismatchincorrelationsviolatestheassumptions
oftheCorrGCV,andthusnoGCVcanasymptoticallymatchitwithoutknowledgeofthenoiselevel
σ . Acrossallsettingsthetheorycurves(solidlines)findexcellentagreementwiththeexperiments
ϵ
(solidmarkerswitherrorbarsover10differentdatasets).
peakin3(b). Thus,correlationsmollifythedoubledescentphenomenon. Wedonotsaythatthey
regularizetheeffect,becausetheriskstillexplodesintheridgelessq →1limit.
Noisecorrelationsaffectthegeneralizationerroronlyifthecovariatesarealsocorrelated. Ifthe
covariatesareuncorrelated,namelyK =I thenK′ entersonlythrough 1 TrK′,whichis1by
T T
definition. Conversely,wecanconsiderthecasewherethecovariatesarecorrelatedbutthenoiseis
uncorrelated,i.e.,K′ = I . Intheoverparameterizedregime,intheridgelesslimitthisyieldsan
T
errorequaltothatwithuncorrelateddatamultipliedby 1 Tr(K−1),whichisgreaterthanoneso
T
longasK ̸=I (AppendixF).Thisimpliesthathavinguncorrelatednoiseisgenerallyworsethan
T
havingnoisewithmatchedcorrelations. WeillustratethisbehaviorinFigure3(c),wherethiseffectis
visibleasastrongmagnificationofdouble-descent.
4 CORRELATED DATA WITH CORRELATED TEST SET
Finally,weconsiderthesettingwherethedatapointxonwhichwetestonhasanontrivialcorrelation
kwitheachx . Thatis,E[x ·x]∝k . Forsimplicity,weassumethatthecovariatesandnoiseare
t t t
identicallycorrelated. Suchasettingarisesnaturallyinthecaseofforecastingtimeseries,whereone
trainsamodelonawindowofdataandthenaimstopredictatafuturetimewithinthecorrelation
timeoftheprocess. WedenotebyRkthegeneralizationerrorwhenthetestpointhascorrelation[k]
g t
withdatapointx . Inthiscase,thetestpointxisconditionallyGaussianwith
t
E[x|X]=X⊤α, Var[x|X]=(1−ρ)Σ, (32)
where α = K−1k and ρ = k⊤K−1k. In Appendix C.5 we derive the following result for the
generalizationerrorinthissetting,assumingtheϵ hasthesamecorrelationasx ,namelyK. We
t t
findthat
Rk ≃Rk=0(cid:2) 1−ρ+κ˜2α⊤K(K+κ˜)−2α(cid:3) . (33)
g g
Wenotethatbecauseκ˜ > 0,thespectrumofκ˜2(K +κ˜)−2 isstrictlyboundedbetweenzeroand
one. Thus,thelastterminbracketsisstrictlyboundedfromabovebyρ,meaningthatRk isalways
g
lessthanRk=0. Thispreciselyquantifiestheover-optimismofcross-validationwhentheheld-out
g
samplesarenearbyintime.
8
rorrE rorrE rorrEPublishedasaconferencepaperatICLR2024
Correlated Unstructured Regression Correlated Unstructured Regression N=50, T=80
r=0e+00 =1e-04, =0.5 r=0e+00 =1e-04, =0.5
102
100
101
100
101
101
102 R
R
Naive GCV
103 Naive GCV 2 R
CorrGCV CorrGCV
(1-) CorrGCV (1 )CorrGCV
104
101 102 103 100 101 102 103
N/T
(a) Isotropic,CorrelatedTest (b) Isotropic,CorrelatedTest
Figure4: StronglyexponentiallycorrelateddataasinFigure1b),withEx x ∼e−τ/ξ,ξ =102.
t t+τ
a)Comparisonoftheout-of-sampleriskfortestingonacorrelatedpointthatisτ =5timepointsin
thefuturefromthetrainingdata. WeemphasizethattheCorrGCVhereistheCorrGCVfortestingon
anuncorrelatedtestpoint,whichispessimisticrelativetotheriskforthecorrelatedpointatτ =5. b)
Plotoftheout-of-sampleriskasafunctionofthehorizontimethatwetestout. Weseethattestingon
closertimesispredictablymoreoptimistic. Weseethat(1−ρ)timestheCorrGCVprovidesagood
approximationoftheoptimismoftestingonacorrelatedpoint. Givenaknowledgeofthecorrelation
structureofK,thiscanbeefficientlycomputedforstationarytimeseriesdata.
Thetrainerrorisnotaffectedbythecorrelatedtestpoint.Consequently,thisyieldsafurtherextension
oftheGCVestimatortothissetting.
R ≃
Sd˜f
1 (cid:2) 1−ρ+κ˜2α⊤K(K+κ˜)−2α(cid:3) Rˆ . (34)
out d˜f −d˜f in
1 2
InpracticewefindthatthesimpleapproximationRk =(1−ρ)Rk=0 worksverywell,asthelast
terminEquation(34)isquitesmall(Figure4).
5 CONCLUSION
Inthispaper, wehaveprovidedacomprehensivecharacterizationoftheasymptoticriskofhigh-
dimensionalkernelregressionwithcorrelateddatapoints. Ourresultsshowthatpreviously-proposed
extensions of the GCV estimator to non-i.i.d. data are asymptotically biased, and immediately
givetheformulaforthecorrectedGCV.Thiscorrectionfactorrequiresonetoestimatemorefine-
grainedspectralstatisticsofthesample-samplecovariancethantheusualGCV.However,aswehave
showninthefiguresanddiscussedinAppendixA,obtaininganexcellentestimateappearsrelatively
straightforward.
Thoughourresultsarequitegeneral,theyarenotwithoutlimitations,andthereareseveraloppor-
tunities for further theoretical inquiry. First, our matrix-Gaussian model for the training data (3)
couldberelaxedtoallowforsample-dependentcovariancebetweenfeatures. Moreover,onecould
considernon-Gaussiandata,thoughintheridgeregressionsettingweexpectGaussianuniversality
toholdunderratherweakconditions(Hu&Lu,2022;Misiakiewicz&Saeed,2024;Montanari&
Saeed,2022). Onthetechnicalside,onemightalsowanttoestablishdimension-freedeterministic
equivalentswithrelativeerrorbounds,asinMisiakiewicz&Saeed(2024). However,thesearelargely
technical,ratherthanconceptual,limitations.
Weconcludebycommentingbrieflyonapplicationsofourresults. Firstandforemost,weanticipate
thatourasymptoticallyexactcorrectiontotheGCVshouldbeofsomeutilityintimeseriesregression
settings, whether in finance (Bouchaud & Potters, 2003), neuroscience (Williams & Linderman,
2021),orelsewhere. Moreover,ithasnotescapedournoticethatourresultsmightbeofuseinthe
9
rorrE rorrEPublishedasaconferencepaperatICLR2024
studyofin-contextlearninginlanguagemodels,wheremosttheoreticalinvestigationsneglectforthe
sakeofanalyticalconveniencetherichcorrelationspresentinlanguage(Luetal.,2024).
CODEAVAILABILITY
Thecodetoreproduceallfiguresinthispapercanbeaccessedat:
https://github.com/Pehlevan-Group/S transform.
ACKNOWLEDGMENTS
AAisgratefultoHoldenLeslie-BoleandJacobPrinceforusefulconversations. JAZVissupported
byaJuniorFellowshipfromtheHarvardSocietyofFellows. CPandthisresearchweresupported
byNSFAwardDMS-2134157andNSFCAREERAwardIIS-2239780. CPisfurthersupportedby
aSloanResearchFellowship. ThisworkhasbeenmadepossibleinpartbyagiftfromtheChan
ZuckerbergInitiativeFoundationtoestablishtheKempnerInstitutefortheStudyofNaturaland
ArtificialIntelligence.
REFERENCES
Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained bias-
variancedecomposition. Advancesinneuralinformationprocessingsystems,33:11022–11032,
2020.
Madhu Advani and Surya Ganguli. Statistical mechanics of optimal convex inference in high
dimensions. Phys.Rev.X,6:031034,Aug2016. doi:10.1103/PhysRevX.6.031034. URLhttps:
//link.aps.org/doi/10.1103/PhysRevX.6.031034.
MadhuSAdvani,AndrewMSaxe,andHaimSompolinsky. High-dimensionaldynamicsofgeneral-
izationerrorinneuralnetworks. NeuralNetworks,132:428–446,2020.
N.S.Altman. Kernelsmoothingofdatawithcorrelatederrors. JournaloftheAmericanStatistical
Association, 85(411):749–759, 1990. doi:10.1080/01621459.1990.10474936. URL https:
//www.tandfonline.com/doi/abs/10.1080/01621459.1990.10474936.
AlexanderBAtanasov,JacobAZavatone-Veth,andCengizPehlevan. Scalingandrenormalization
inhigh-dimensionalregression. arXivpreprintarXiv:2405.00592,2024.
Francis Bach. High-dimensional analysis of double descent for linear regression with random
projections. SIAMJournalonMathematicsofDataScience,6(1):26–50,2024.
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neu-
ral scaling laws. Proceedings of the National Academy of Sciences, 121(27):e2311878121,
2024. doi:10.1073/pnas.2311878121. URL https://www.pnas.org/doi/abs/10.
1073/pnas.2311878121.
Anirban Basak, Arup Bose, and Sanchayan Sen. Limiting spectral distribution of sample au-
tocovariance matrices. Bernoulli, 20(3):1234 – 1259, 2014. doi:10.3150/13-BEJ520. URL
https://doi.org/10.3150/13-BEJ520.
Stephen Bates, Trevor Hastie, and Robert Tibshirani. Cross-validation: What does it estimate
and how well does it do it? Journal of the American Statistical Association, 119(546):1434–
1445, 2024. doi:10.1080/01621459.2023.2197686. URL https://doi.org/10.1080/
01621459.2023.2197686.
Je´re´mieBigot,Issa-MbenardDabo,andCamilleMale. High-dimensionalanalysisofridgeregression
fornon-identicallydistributeddatawithavarianceprofile. arXiv,2024. URLhttps://arxiv.
org/abs/2403.20200.
BlakeBordelon,AbdulkadirCanatar,andCengizPehlevan. Spectrumdependentlearningcurvesin
kernelregressionandwideneuralnetworks. InInternationalConferenceonMachineLearning,
pp.1024–1034.PMLR,2020.
10PublishedasaconferencepaperatICLR2024
BlakeBordelon,AlexanderAtanasov,andCengizPehlevan. Adynamicalmodelofneuralscaling
laws. arXivpreprintarXiv:2402.01092,2024.
Jean-Philippe Bouchaud and Marc Potters. Theory of Financial Risk and Derivative Pricing:
From Statistical Physics to Risk Management. Cambridge University Press, 2 edition, 2003.
doi:https://doi.org/10.1017/CBO9780511753893.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax.
Joe¨lBun,RomainAllez,Jean-PhilippeBouchaud,andMarcPotters. Rotationalinvariantestimator
forgeneralnoisymatrices. IEEETransactionsonInformationTheory,62(12):7475–7490,2016.
ZBurda,RAJanik,andMANowak. MultiplicationlawandS transformfornon-Hermitianrandom
matrices. PhysicalReviewE,84(6):061125,2011.
AbdulkadirCanatar,BlakeBordelon,andCengizPehlevan. Out-of-distributiongeneralizationin
kernelregression.InM.Ranzato,A.Beygelzimer,Y.Dauphin,P.S.Liang,andJ.WortmanVaughan
(eds.),AdvancesinNeuralInformationProcessingSystems,volume34,pp.12600–12612.Curran
Associates, Inc., 2021a. URL https://proceedings.neurips.cc/paper_files/
paper/2021/file/691dcb1d65f31967a874d18383b9da75-Paper.pdf.
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model align-
ment explain generalization in kernel regression and infinitely wide neural networks. Nature
communications,12(1):2914,2021b.
AndreaCaponnettoandErnestoDeVito. Optimalratesfortheregularizedleast-squaresalgorithm.
FoundationsofComputationalMathematics,7:331–368,2007.
AndreaCaponnettoandErnestoDeVito. Fastratesforregularizedleast-squaresalgorithm. Techni-
calreport,MassachusettsInstituteofTechnologyComputerScienceandArtificialIntelligence
Laboratory,2005.
Patrick S. Carmack, Jeffrey S. Spence, and William R. Schucany. Generalised cor-
related cross-validation. Journal of Nonparametric Statistics, 24(2):269–282, 2012.
doi:10.1080/10485252.2012.655733. URL https://doi.org/10.1080/10485252.
2012.655733.
LenaicChizat,EdouardOyallon,andFrancisBach. Onlazytrainingindifferentiableprogramming.
Advancesinneuralinformationprocessingsystems,32,2019.
PeterCravenandGraceWahba. Smoothingnoisydatawithsplinefunctions: estimatingthecorrect
degreeofsmoothingbythemethodofgeneralizedcross-validation. Numerischemathematik,31
(4):377–403,1978.
Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova´. Generalization error rates
in kernel regression: The crossover from the noiseless to noisy regime. Advances in Neural
InformationProcessingSystems,34:10131–10143,2021.
Leonardo Defilippis, Bruno Loureiro, and Theodor Misiakiewicz. Dimension-free deterministic
equivalentsforrandomfeatureregression. arXivpreprintarXiv:2405.15699,2024.
EdgarDobribanandStefanWager. High-dimensionalasymptoticsofprediction: Ridgeregression
andclassification. TheAnnalsofStatistics, 46(1):247–279,2018. doi:10.1214/17-AOS1549.
URLhttps://doi.org/10.1214/17-AOS1549.
FedericaGerace,BrunoLoureiro,FlorentKrzakala,MarcMe´zard,andLenkaZdeborova´. General-
isationerrorinlearningwithrandomfeaturesandthehiddenmanifoldmodel. InInternational
ConferenceonMachineLearning,pp.3452–3462.PMLR,2020.
GeneHGolub, MichaelHeath, andGraceWahba. Generalizedcross-validationasamethodfor
choosingagoodridgeparameter. Technometrics,21(2):215–223,1979.
11PublishedasaconferencepaperatICLR2024
TrevorHastie,RobertTibshirani,JeromeHFriedman,andJeromeHFriedman. Theelementsof
statisticallearning: datamining,inference,andprediction,volume2. Springer,2009.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensionalridgelessleastsquaresinterpolation. TheAnnalsofStatistics,50(2):949–986,2022.
HongHuandYueMLu. Universalitylawsforhigh-dimensionallearningwithrandomfeatures.
IEEETransactionsonInformationTheory,69(3):1932–1964,2022.
Arthur Jacot, Franck Gabriel, and Cle´ment Hongler. Neural tangent kernel: Convergence and
generalizationinneuralnetworks. Advancesinneuralinformationprocessingsystems,31,2018.
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Cle´ment Hongler, and Franck Gabriel. Kernel
alignment risk estimator: Risk prediction from training data. Advances in neural information
processingsystems,33:15568–15578,2020.
JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,RewonChild,Scott
Gray,AlecRadford,JeffreyWu,andDarioAmodei. Scalinglawsforneurallanguagemodels.
arXivpreprintarXiv:2001.08361,2020.
ReimerKu¨hnandPeterSollich. Spectraofempiricalauto-covariancematrices. EurophysicsLetters,
99(2):20008,2012.
Licong Lin and Edgar Dobriban. What causes the test error? going beyond bias-variance via
anova. JournalofMachineLearningResearch,22(155):1–82,2021. URLhttp://jmlr.org/
papers/v22/20-1211.html.
Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard,
and Lenka Zdeborova´. Learning curves of generic features maps for realistic datasets with a
teacher-studentmodel. AdvancesinNeuralInformationProcessingSystems,34:18137–18151,
2021.
YueM.Lu,MaryI.Letey,JacobA.Zavatone-Veth,AninditaMaiti,andCengizPehlevan.Asymptotic
theoryofin-contextlearningbylinearattention. arXiv,2024. URLhttps://arxiv.org/
abs/2405.11751.
MarkA.Lukas. RobustGCVchoiceoftheregularizationparameterforcorrelateddata. Journal
ofIntegralEquationsandApplications,22(3):519–547,2010. doi:10.1216/JIE-2010-22-3-519.
URLhttps://doi.org/10.1216/JIE-2010-22-3-519.
AlexanderMaloney,DanielARoberts,andJamesSully. Asolvablemodelofneuralscalinglaws.
arXivpreprintarXiv:2210.16859,2022.
GabrielMelandSuryaGanguli. Atheoryofhighdimensionalregressionwitharbitrarycorrelations
between input features and target functions: sample complexity, multiple descent curves and
a hierarchy of phase transitions. In Marina Meila and Tong Zhang (eds.), Proceedings of the
38th International Conference on Machine Learning, volume 139 of Proceedings of Machine
LearningResearch,pp.7578–7587.PMLR,18–24Jul2021. URLhttps://proceedings.
mlr.press/v139/mel21a.html.
GabrielMelandJeffreyPennington. Anisotropicrandomfeatureregressioninhighdimensions. In
InternationalConferenceonLearningRepresentations,2021.
JamesAMingoandRolandSpeicher. Freeprobabilityandrandommatrices,volume35. Springer,
2017.
TheodorMisiakiewiczandBasilSaeed. Anon-asymptotictheoryofkernelridgeregression: deter-
ministicequivalents,testerror,andGCVestimator. arXivpreprintarXiv:2403.08938,2024.
AndreaMontanariandBasilN.Saeed. Universalityofempiricalriskminimization. InPo-LingLoh
andMaximRaginsky(eds.),ProceedingsofThirtyFifthConferenceonLearningTheory,volume
178ofProceedingsofMachineLearningResearch,pp.4310–4312.PMLR,02–05Jul2022. URL
https://proceedings.mlr.press/v178/montanari22a.html.
12PublishedasaconferencepaperatICLR2024
MMudelsee. Climatetimeseriesanalysis: Classicalstatisticalandbootstrapmethods. Atmospheric
andOceanographicSciencesLibrary,51:1–454,2014.
AlexandruNicaandRolandSpeicher. Lecturesonthecombinatoricsoffreeprobability,volume13.
CambridgeUniversityPress,2006.
SilviaNoschese,LionelloPasquini,andLotharReichel. TridiagonalToeplitzmatrices: properties
and novel applications. Numerical Linear Algebra with Applications, 20(2):302–326, 2013.
doi:https://doi.org/10.1002/nla.1811. URLhttps://onlinelibrary.wiley.com/doi/
abs/10.1002/nla.1811.
Jean Opsomer, Yuedong Wang, and Yuhong Yang. Nonparametric Regression with Correlated
Errors. StatisticalScience,16(2):134–153,2001. doi:10.1214/ss/1009213287. URLhttps:
//doi.org/10.1214/ss/1009213287.
PratikPatil,Jin-HongDu,andRyanJ.Tibshirani. Optimalridgeregularizationforout-of-distribution
prediction. arXiv,2024. URLhttps://arxiv.org/abs/2404.01233.
MarcPottersandJean-PhilippeBouchaud. Afirstcourseinrandommatrixtheory: forphysicists,
engineersanddatascientists. CambridgeUniversityPress,2020.
AssafRabinowiczandSaharonRosset. Cross-validationforcorrelateddata. JournaloftheAmerican
StatisticalAssociation,117(538):718–731,2022.
StefanoSpigler,MarioGeiger,andMatthieuWyart. Asymptoticlearningcurvesofkernelmethods:
empiricaldataversusteacher–studentparadigm. JournalofStatisticalMechanics: Theoryand
Experiment,2020(12):124001,2020.
Gerardus’tHooft. Aplanardiagramtheoryforstronginteractions. Nucl.Phys.B,72(CERN-TH-
1786):461–473,1973.
NileshTripuraneni,BenAdlam,andJeffreyPennington. Covariateshiftinhigh-dimensionalrandom
featureregression. arXivpreprintarXiv:2111.08234,2021.
DanVVoiculescu. Freeprobabilitytheory,volume12. AmericanMathematicalSoc.,1997.
DanVVoiculescu,KenJDykema,andAlexandruNica. Freerandomvariables. AmericanMathe-
maticalSociety,1992.
DonWeingarten. Asymptoticbehaviorofgroupintegralsinthelimitofinfiniterank. Journalof
MathematicalPhysics,19(5):999–1001,1978.
Alex H. Williams and Scott W. Linderman. Statistical neuroscience in the single
trial limit. Current Opinion in Neurobiology, 70:193–205, 2021. ISSN 0959-4388.
doi:https://doi.org/10.1016/j.conb.2021.10.008. URL https://www.sciencedirect.
com/science/article/pii/S0959438821001203. ComputationalNeuroscience.
JacobAZavatone-VethandCengizPehlevan. LearningcurvesfordeepstructuredGaussianfeature
models. InAdvancesinNeuralInformationProcessingSystems,2023.
13PublishedasaconferencepaperatICLR2024
CONTENTS
1 Introduction 1
2 ReviewofFreeProbabilityandDeterministicEquivalence 3
3 Predictinganuncorrelatedtestsetfromcorrelateddata 5
4 CorrelatedDatawithCorrelatedTestSet 8
5 Conclusion 9
A AlgorithmicImplementationoftheCorrGCV 15
B ReviewofFreeProbability 17
C Derivations 18
D Bias-VarianceDecompositions 27
E ScalingAnalysis 28
F DoubleDescentAnalysis 30
G Asymptoticsofpreviously-proposedextensionsoftheGCV 34
H WeightedrisksandtheMMSEestimator 37
I S-transformsforcertainToeplitzcovariancematrices 37
J FurtherExperiments 39
K ExperimentalDetails 44
14PublishedasaconferencepaperatICLR2024
GCV 1 = (1−q1 df1)2 GCV 2 =S2 = λκ2 2
df ≡df1 ≃df1 κ=λSK(qdf1) df ≡df2
1 Σˆ Σ 1−qdf1 2 Σ
λ qdf =d˜f κκ˜ = 1 CorrGCV= κ d˜f1
1 1 λ d˜f1 λd˜f1−d˜f2
d˜f ≡df1 ≃df1 κ˜ d˜f ≡df2
1 Kˆ K 2 K
Figure5: AgraphicalrepresentationoftheprogramneededtoobtaintheCorrGCVempiricallyfrom
agivendataset. Theasymmetryofthediagramarisesfromthefactthatweestimateκfirstratherthan
κ˜. ThisisbecauseitismorereasonabletoassumeagoodestimateofthecorrelationsK,whichoften
havepropertiessuchasstationaritythatimprovetheestimationprocess,comparedtoestimatingΣ.
Asaresult,itiseasiertouseeitheranexactformoradifferentiableinterpolationofS (df)rather
K
thanS .
Σ
A ALGORITHMIC IMPLEMENTATION OF THE CORRGCV
Inthissection,givenknowledgeofthefunctionS (qdf ),wegiveamethodbywhichtoobtainthe
K 1
CorrGCV.ThediagramforthiscalculationisgiveninFigure5. Inwhatfollows,wewillusethe
notationb ← atoindicatevariableassignmentofbgiventhatahasbeencomputed. Wealsouse
a ≡ btohighlightthatthevariableaisshorthandforb. Thisallowsonetoeasilytrackthecausal
chainofhowtherelevantvariablesareestimated.
First,givenΣˆ alone,wecancalculateκaslongasthefunctionalformofS isknown. Weobtain:
K
S (qdf )
df ←df1 (λ), κ←λ K 1 . (35)
1 Σˆ 1−qdf
1
Atthispoint,theordinaryGCVisdirectlygivenbyS2Rˆ . Westressthatgivenafunctionalform
in
ofS thisgivesκasadifferentiableprogramofλ. Derivativescanbeefficientlyevaluatedwith
K
autograd,forexampleusingtheJAXlibrary(Bradburyetal.,2018). Thiswillbeimportantforthe
nextsteps.
Second,byapplyingthedualityrelation(seeAppendixC.3),weobtainestimatesford˜f ≡df1 (λ),
1 K
κ˜:
λ
d˜f ←qdf , κ˜ ← . (36)
1 1 κqdf
1
Third,weleverageautogradtoestimatedf ≡df2 byapplyinganotherdualityrelationshipgivenby
2 Σ
Equation(91).
∂ df
df ←df + λ 1 . (37)
2 1 ∂ logκ
λ
Fourth, by using this and applying autograd again, we obtain an estimate for d˜f ≡ df2 using
2 K
Equation(90):
∂ logκ
d˜f ←d˜f −q λ (df −df ). (38)
2 1 ∂ logκ˜ 1 2
λ
Finally,fromthisweobtaintheCorrGCVestimator:
d˜f
E ←Rˆ S 1 . (39)
CorrGCV in d˜f −d˜f
1 2
15PublishedasaconferencepaperatICLR2024
1 # R_in: In-sample risk, AKA training Error
2 # lamb: Value of the ridge
3 # q: Ratio N/T
4 # eigs_Sh: Eigenvalues of empirical covariance Sigma hat (Sh)
5 # S_K: Function of df given by the S-transform S_K
6 from jax import grad
7 import jax.numpy as jnp
8
9 df1_fn_Sh = lambda l: jnp.mean(eigs_Sh/(l+eigs_Sh))
10 d_df1_fn_Sh = grad(df1_fn_Sh)
11
12 df1_Sh_est = df1_fn_Sh(lamb)
13 df1_Kh_est = q * df1_Sh_est
14
15 R_GCV1 = R_in/(1-q*df1_est)**2
16
17 S_est = 1/(1-q*df1_est) * S_K(q*df1_est)
18 R_GCV2 = R_in * S_est**2
19
20 kappa1_fn = lambda l: l/(1-q*df1_fn_Sh(l)) * S_K(q*df1_fn_Sh(l))
21 kappa2_fn = lambda l: l/(q * df1_fn_Sh(l) * k1_fn(l))
22 d_log_kappa1_fn = grad(lambda l: jnp.log(kappa1_fn(l)))
23 d_log_kappa2_fn = grad(lambda l: jnp.log(kappa2_fn(l)))
24 df2_fn_Sh = lambda l: df1_fn_Sh(l) + d_log_kappa1_fn(l) * d_df1_fn_Sh(l)
25
26 df2_Kh_est = q*df1_est - q*(df1_fn_Sh(lamb) - df2_fn_Sh(lamb))*
d_log_k1_fn(lamb)/d_log_k2_fn(lamb)
27
28 R_CorrGCV = R_in * S_est * df1_Kh_est/(df1_Kh_est - df2_Kh_est)
CodeBlock1: PythoncodeforcomputingCorrGCVestimatorgivenrelevantparameters.
A.1 CODEFORIMPLEMENTATION
InCodeBlock1wegivethecodeforimplementingtheCorrGCVestimatorinJAXgivenaridge
λ,trainingerrorRˆ ,overaparameterizationratioq,empiricalcovariancematrixΣˆ,andfunction
in
S (df)givenbytheS-transformofthecorrelationstructure.
K
A.2 ESTIMATINGS K WHENK ISNOTKNOWN
Ifthefunctionalformofthecorrelationsisnotexactlyknown,thereareseveraloptions.
1. From empirical data, one can impose an ansatz for form of the data point correlations.
Suchcorrelationscanbefitempiricallytotheformofe.g. anexponentialoftheforme−rt.
Givensuchananalyticform,theS-transformS canbecalculatedexactly. Wegivesome
K
examplesofthisinAppendixI.
2. Foratimeserieswiththeassumptionofstationarity,onecanestimateKmuchmorereliably
compared to the estimation of an arbitrarily structured high-dimensional covariance. A
reliableestimateoftheT×T KmatrixcouldthenbeobtainedreliablyfromonlyT samples.
3. FromKˆ,onecancalculatedf (λ)asafunctionofλ. Bymakinguseofasolver(suchasa
Kˆ
bisectionmethod),onecanevaluatedf−1(df)andthenget
Kˆ
1−d˜f S (d˜f) (q−d˜f)(1−d˜f)
S (d˜f)= = K ⇒S (d˜f)= . (40)
Kˆ d˜fdf−1(d˜f) q−d˜f K d˜fdf−1(d˜f)
Kˆ Kˆ
Wecanthenevaluateκ asfollows:
1
• First,sampleafinegridofλ.
• Fromthedata,weestimatedf = df1 (λ)foreachlambda,leadingtoafinegridof
1 Σˆ
df andd˜f =qdf
1 1 1
16PublishedasaconferencepaperatICLR2024
• Giventhebisection-basedalgorithmtoevaluatedf−1. WecanthenevaluateS (d˜f)
Kˆ K
usingEquation(40). ThisleadstoanexactformulaforS onthecorrespondingfine
K
gridofd˜f =qdf .
1 1
• Usingthedefinitionκ= SK(qdf1),weobtainafinegridofκasafunctionofλ.
1−qdf1
• Wenowapplyanappropriateinterpolationmethodthatwecandifferentiatethrough
andguaranteethatasthegridgetsfiner,boththefunctionanditsderivativesconverge
tothetrueκand∂ κ.
λ
Thelastbulletpointrequiresfurtherdiscussion. Inpractice,becauseκcanvaryoverseveral
ordersofmagnitude,andbecauseestimatingtheCorrGCVrequireslogarithmicderivatives,
it is often best to interpolate logκ against logλ. A good interpolator compatible with
autograd can be a Gaussian process with appropriately chosen covariance, a piece-wise
smoothsplinefunction,polynomialinterpolator,orawaveletbasedmethodthatconverges
intheappropriateSobolevnorm. Weusedasimpledegree5polynomialinterpolationof
S togeneratethepowerlawcorrelationlearningcurvesinFigure2b).
K
B REVIEW OF FREE PROBABILITY
B.1 DEFINITIONOFFREEDOM
Wenowdefinewhatitmeansforasetofnrandommatrices{A }n tobejointly(asymptotically)
i i=1
free. AllofthesematricesareN ×N andweconsiderthelimitofN → ∞. Thisisthelimitin
whichfreeprobabilitytheoryappliesforrandommatrices. Thereareseveraltextsonthisrichsubject.
See,forexampleMingo&Speicher(2017);Nica&Speicher(2006);Voiculescu(1997).
Wesaythatapolynomialp(A )hasthemeanzeropropertyif
i
1
Tr[p(A )]≃0. (41)
N i
Above,≃0meansthatthisconvergestozeroinprobabilityasN →∞.
Takeasetofmpolynomials{p }m ,eachwiththemeanzeroproperty. Further,takealabeling
k k=1
{i }n witheachi ∈{1,...,n}sothati ̸=i forallk. The{A }n arejointlyasymptoti-
k k=1 k k k+1 i i=1
callyfreeifandonlyif
1
Tr[p (A )···p (A )]≃0 (42)
N 1 i1 m im
foranym,anylabeling{i },andanysetofpolynomials{p }m satisfyingthepropertiesabove.
k k k=1
For our purposes we only require that the 1Z⊤Z and 1ZZ⊤ matrices are free of any fixed
T T
deterministicmatrix,specificallyΣandK.
B.2 THERANDS TRANSFORMS
WereviewherebasicpropertiesoftheRandS transformsoffreeprobabilityVoiculescu(1997);
Voiculescu et al. (1992). The recent book by Potters & Bouchaud (2020) provides an accessible
introductiontothesetechniquesinthecontextofrandommatrixtheory.
GivenaN ×N symmetricrandommatrixA,oneconsiderstheresolvent,alsoknownastheStiltjes
Transform:
g (λ)≡
1 Tr(cid:2) (A+λ)−1(cid:3)
. (43)
A N
TheeigenvaluedensityofA,ρ ,canbeobtainedfromtheinverseStiltjestransform:
A
1
ρ (λ)= lim Img (−λ+iϵ). (44)
A ϵ→0π A
Similarly,onedefinesthedegreesoffreedomdf1 (λ)asinEquation(9). Onehastherelationship:
A
1−df1 (λ)
g (λ)= A . (45)
A λ
17PublishedasaconferencepaperatICLR2024
Weletg−1,df−1denotetherespectivefunctionalinverses. TheRandS transformsarerespectively
A A
functionsofformalvariablesg,df givenby:
1 1−df
R (g)≡g−1(g)− , S (df)≡ . (46)
A A g A dfdf−1(df)
A
TheyhavethepropertythatforanytwomatricesA,Bthatarefreeofoneanother:
R (g)=R (g)+R (g), S (df)=S (df)S (df). (47)
A+B A B A∗B A B
Fromthesetworespectiveproperties,togetherwiththedefinitions(46)weobtainthesubordination
relations,alsoknownas(weak)deterministicequivalence:
g (λ)=g (λ+R ), df (λ)=df (λS ). (48)
A+B A B A∗B A B
Here,theadditiveandmultiplicativerenormalizationsofλ,givenbyR andS respectively,can
B B
eachbeevaluatedintwodifferentways. ThefirstwayisfromtheoriginalnoisymatricesA+Band
A∗B. ThisistheanalogueoftheempiricalestimateofS,κdiscussedinthetext.
R ≡R (g (λ)), S ≡S (df1 (λ)). (49)
B B A+B B B A∗B
ThesecondwayisfromthecleanmatrixAitself. Thisiswhatgivestheomniscientestimateofthe
renormalizedridges. Writingκ ,κ forλ+R ,λS respectively,weobtaintheself-consistent
+ ∗ B B
equations:
κ =λ+R (g (κ )), κ =λS (df1 (κ )). (50)
+ B A + ∗ B A ∗
B.3 STRONGDETERMINISTICEQUIVALENCE
Thedeterministicequivalencesin(48)extendtothematricesthemselves. TakingAdeterministic
andBrandomandfreeofA,wehave:
(A+B+λ)−1 ≃(A+κ )−1, A∗B(A∗B+λ)−1 ≃A(A+κ )−1. (51)
+ ∗
Here,fortwomatrices,weusetherelation≃todenotethatthetracesofthesequantitiesagainstany
testmatrixofboundedspectralnormwillconvergeinprobabilitytothesamequantityasN →∞.
Theabovetwoformulascanbederivedusingreplicatheory(Bunetal.,2016;Potters&Bouchaud,
2020),fromdiagrammatics(Atanasovetal.,2024;Burdaetal.,2011),orfromcavityarguments
(Bach,2024).
Inthispaper,weonlyrequirethepropertiesoftheS-transform. Theabovedeterministicequivalences
arecalled“onepoint”equivalences,astheonlyinvolveasinglematrixinverseincalculatingthem.
InsectionC.2
C DERIVATIONS
C.1 WARMUP: 1POINTDETERMINISTICEQUIVALENTS
Inthissection,werepeatadiagrammaticargumentfromAtanasovetal.(2024),specializedtothe
specificcaseofWishartnoisematricesB. Theargumentisdiagrammatic,appealingtothefactthatat
largeN,freerandommatricesobeynon-crossingpropertiesintheirdiagrammatics(’tHooft,1973).
TheargumentholdsrigorouslyonlyintheN →∞limit. However,itcanrigorouslybeshowntobe
theleadingterminanasymptoticseriesin1/N byleveragingknownpropertiesofthehigher-order
corrections(Weingarten,1978).
√
Inwhatfollows, forthesakeofnotationallevityweabsorbafactorof1/ T intoeachinsertion
of Z or Z⊤. Because each pair of Z,Z⊤ insertions will necessarily be averaged over via Wick
contraction,thisisthesameasassociatinganfactorof1/T witheachWickcontraction.
18PublishedasaconferencepaperatICLR2024
E Σˆ(Σˆ +λ)−1 =(−λ)−1
Z Σ1/2Z⊤ K Z Σ1/2
+(−λ)−2
Σ1/2Z⊤ K Z Σ Z⊤ K Z Σ1/2
(52)
+(−λ)−2 +...
Σ1/2Z⊤ K Z Σ Z⊤ K Z Σ1/2
Suchasuppressionofcrossingdiagramsisadefiningcharacteristicoffreeprobability. Because
each loop contributes a trace, while each insertion of Z⊤Z contributes a factor of 1/T, in order
for a diagram to give an order 1 contribution we will need to have as many loops as there are
pairsofZ⊤,Z. Consequently,crossingdiagramssuchasthefollowingaresuppressedinthelarge
N,T →∞:
Σ1/2Z⊤ K Z Σ Z⊤ K Z Σ1/2
Becausecrossingdiagramsdonotcontribute,onecanobservethefollowingpattern. Anydiagram
thatappearswillbealinkofaveragesfromoneZ⊤tosomelaterZ thatcreatesanarc. Beneaththat
arc,allaveragescanonlybebetweenthematriceswithinthearc,bynon-crossing. Assuch,wecan
expand:
1/S
EΣˆ(Σˆ +λ)−1 =(−λ)−1
Σ1/2 Σ1/2
1/S 1/S
+(−λ)−2 +...
Σ1/2 Σ Σ1/2
(53)
Thetermsinthedashedlinesarepre-emptivelydenotedby1/S. Wemaketwoobservations.
• BecausetheisotropicmatrixZ isright-invarianttorotationsinRN,1/S mustbeadeter-
ministicmatrixthatisinvariantunderrotation. Theonlysuchmatricesareconstantstimes
theidentity.
• BecausetheisotropicmatrixZ isalsoleft-invarianttorotationsinRT,1/S isarotationally
invariant scalar functional of the product of matrices beneath the arc. The only such
rotationallyinvariantscalarfunctionalisanyconstantmultipleofthetrace.
Thefactthattheshadedpartsarescalarsimmediatelyimplieswecanresumthisas:
∞
E Σˆ(Σˆ +λ)−1 = (cid:88) Σn 1 =Σ(Σ+λS)−1. (54)
Z (−λS)n
n=1
19PublishedasaconferencepaperatICLR2024
Animmediateconsequenceis:
1 (cid:104) (cid:105)
E (Σˆ +λ)−1 = 1−E Σˆ(Σˆ +λ)−1 =S(Σ+λS)−1. (55)
Z λ Z
ItnowremainstoevaluateS. Wehave
1
= + +...
S Z⊤ K Z Z⊤ K Z Σ Z⊤ K Z
√
WeimmediatelyrecognizethisasthetraceofλK(Kˆ +λ)−1. BecauseZhasentriesgoingas1/ P
thisimplies:
1 1 (cid:104) (cid:105)
=E λ Tr K(Kˆ +λ)−1 . (56)
S Z T
Here,becausethetraceisself-averaging,wecoulddroptheE andstillkeepthisasanequalityin
Z
probabilityasT,N →∞.
Byaslightadjustmentoftheabovediagrammaticargument,swappingtheroleofΣandK,wehave
ananalogueofEquation(55):
1 1 (cid:104) (cid:105)
E (Kˆ +λ)−1 =S˜(K+λS˜)−1, =E λ Tr Σ(Σˆ +λ)−1 . (57)
Z S˜ Z P
Wenowdefine
κ≡λS, κ˜ ≡λS˜, df ≡ 1 Tr(cid:2) Σ(Σ+κ)−1(cid:3) , d˜f ≡ 1 Tr(cid:2) K(K+κ˜)−1(cid:3) . (58)
1 N 1 T
Notethatatthisstagewehaveshowntheequivalence:
d˜f ≡df1 (λS˜)≃df1 (λ)=qdf1 (λ)≃qdf1 (λS)≡qdf . (59)
1 K Kˆ Σˆ Σ 1
PuttingEquation(57)backintoEquation(56)yields:
λ
=d˜f . (60)
κκ˜ 1
Werecognizeasthedualityrelation. WecanalsogetanexplicitexpressionforS intermsofS
K
anddf bydefiningtheformalfunctionofdf
1−df
S (df)≡ (61)
A dfdf−1(df)
A
andplugginginA=K,df =df1 (κ˜)=d˜f intothis:
K 1
1−df
κ˜d˜f = 1 . (62)
1 S (d˜f )
K 1
Usingd˜f =qdf thisyieldsthedesiredrelationship:
1 1
S (qdf)
κ=λ K . (63)
1−qdf
Thus,givenknowledgeofλ,df andS ,onecancalculateκexactly. Foradiscussionofhowto
1 K
obtainS fromagivencorrelateddataset,seeA.2.
K
20PublishedasaconferencepaperatICLR2024
C.2 2POINTDETERMINISTICEQUIVALENTS
Inthissection,weextendthisdiagrammatictechniquetocalculatethenecessary2-pointdeterministic
equivalents. Someofthesewerederivedusingleave-one-out(cavity)argumentsintherecentpaper
ofBach(2024)inthecaseofK = I. Theremainingresultsthatwederivearetoourknowledge
novel. Theymayhaveusefulapplicationstohigh-dimensionalregressionproblemsbeyondjustthose
studiedinthecontextofthispaper.
√
Asinthepriorsection,weabsorbafactorof1/ T intoeachZ orZ⊤ insertion,orequivalently
associateafactorof1/T witheachWickcontractoin.
Wefirstevaluatethetwo-pointequivalent
(Σˆ +λ)−1Σ′(Σˆ +λ)−1. (64)
forarbitrarymatrixΣ′betweentheresolvents. Inperformingthisaverage,wenotethatthetypesof
diagramsthatappearsplitintotwoclasses. ThefirstarethosewithnoarcsoverΣ′:
(65)
Σ1/2Z⊤ K Z Σ1/2 Σ′ Σ1/2Z⊤ K Z Σ1/2
Becausenoarcsconnecttheresolventontheleftwiththeresolventontheright,wecantakethe
averagesoftheleftandrightresolventseparately,andobtain:
S2(Σ+λ)−1Σ′(Σ+λ)−1. (66)
Thesecondclassoftermhasarcsconnectingthetworesolventsonbothsides. Anexampleofsucha
diagramis:
(67)
Σ1/2Z⊤ K Z Σ1/2 Σ′ Σ1/2Z⊤ K Z Σ1/2
AnyterminthesecondclasswillhavesomenumberofarcsoverΣ′. Notethatthiswillnecessarily
beanevennumber2n,astheywillalternatebetweenaveragingaZ⊤ ontheleftwithaZ onthe
rightandviceversa. Thiswillgive2nloopsthataretracedover. TherewillbenloopsinvolvingK
matrices,whichwewillcallK loops,andnloopsinvolvingΣmatriceswhichwewillcallΣloops.
Theabovetermhasn=1. AnexampleofaΣloopis:
(68)
Z⊤ Σ+... Z Z⊤ Σ+... Z
Here,oneachsidewedenotebyΣ+... theseries:
Σ+(−λ)−1ΣZ⊤ZΣ+(−λ)−2ΣZ⊤ZΣZ⊤ZΣ+···=λΣ1/2(Σˆ +λ)−1Σ1/2. (69)
21PublishedasaconferencepaperatICLR2024
Werecognizethisasaresolvent. Becausewehaveexplicitlyaccountedforarcsconnectingtermsin
theleftresolventwiththetermsontherightresolvent,withineachloopwecantreattheleftandright
resolventaverageseparately. ThisaverageisgiveninEquation(55),andsimilarlyforK. Thisyields
1 (cid:104) (cid:104) (cid:105) (cid:104) (cid:105) (cid:105)
Σ-loop=λ2 Tr Σ1/2E (Σˆ +λ)−1 ΣE (Σˆ +λ)−1 Σ1/2 =qκ2df , (70)
T Z Z 2
1 (cid:104) (cid:104) (cid:105) (cid:104) (cid:105) (cid:105)
K-loop=λ2 Tr K1/2E (Kˆ +λ)−1 KE (Kˆ +λ)−1 K1/2 =κ˜2d˜f . (71)
T Z Z 2
TheinnermostloopwillhaveaninsertionofΣ′ inbetweenthetworesolvents, eachofwhichis
separatelyaveraged,yielding:
S2 1
Tr[Σ1/2(Σ+κ)−1Σ′(Σ+κ)−1Σ1/2]≃S2 Tr[ΣΣ′(Σ+κ)−2]. (72)
T T
WeadopttheshorthandintroducedinEquation(29)todenotethistermbyqS2df2 . Thismust
ΣΣ′
necessarilybefollowedbyaK loop. Bymakinguseofthedualityrelationship,wecanwritethis
jointcontributionas:
κ2 df2 d˜f
qdf2 κ˜2d˜f = ΣΣ′ 2 ≡γ . (73)
λ2 ΣΣ′ 2 df d˜f ΣΣ′
1 1
BetweenthisinnermostΣ′andK loopandtheoutside,therecanbeanarbitrarynumberofpairsof
closedK andΣloopsinbetween. Againapplyingthedualityrelation,weseeeachpaircontributes:
1 df d˜f
qκ2df κ˜2d˜f = 2 2 ≡γ (74)
λ2 2 2 df d˜f
1 1
Here we divide by λ2 because each ZZ⊤ pair introduces a factor of (−λ)−1. This gives an
interpretationofγ asthecontributionofapairofΣandK loops. Itremainstosumoverntogetthe
finalcontributionfromalloftheloops:
∞
γ (cid:88) γn = γ ΣΣ′ . (75)
ΣΣ′
1−γ
n=0
Finally,outsideoftheloops,wecanperformtheaverageoftheleftresolventandtherightresolvent
separately. Thisgivesthedesiredrelation:
γ
(Σˆ +λ)−1Σ′(Σˆ +λ)−1 ≃S2(Σ+κ)−1Σ′(Σ+κ)−1+S2(Σ+κ)−2Σ ΣΣ′ . (76)
1−γ
WhenK =I,thisrecoverstheearlierresultofBach(2024). Byapplyingthesameargumentwith
minimalmodifications,oneobtainsthedeterministicequivalenceforkernelresolvents,namely:
γ
(Kˆ +λ)−1K′(Σˆ +λ)−1 ≃S˜2(K+κ˜)−1K′(K+κ˜)−1+S˜2(K+κ˜)−2K KK′ . (77)
1−γ
Hereγ KK ≡ df d2 Σ fd 1f d˜2 K f1K′.
Wenextevaluatethetwopointequivalentfor
(Σˆ +λ)−1X⊤K′X(Σˆ +λ)−1
(78)
=(Σˆ +λ)−1Σ1/2Z⊤K1/2K′K1/2ZΣ1/2(Σˆ +λ)−1.
Thistime,becausethereareanoddnumberofZsonboththeleftandtherightsideofK′ inall
diagrams,thedisconnectedtermswhichaverageeachsideseparatelywillvanish. Weareleftwith
justtheconnectedterm.
Again,wesumovertheK andΣloops. Thefirstterm(whichinvolvesnoΣloops)is:
(79)
Σ1/2 Z⊤ K1/2K′K1/2 Z Σ1/2
22PublishedasaconferencepaperatICLR2024
TheinnermostloopwillbeaKloopinvolvinganinsertionofK′inthemiddle. Asinthecalcualtion
oftheΣ′loopinthepriorcase,thiscontributes
1
S˜2 Tr[KK′(K+κ˜)−2]≡S˜2df2 . (80)
T KK′
Between this innermost loop and the outer resolvents, there must be an even number 2n ≥ 0 of
alternatingΣ,K loops. Thecontributionsofsuchloopsareunchanged. Eachpairgivesafactorofγ.
Wethusget:
df2
(Σˆ +λ)−1X⊤K′X(Σˆ +λ)−1 ≃S2S˜2(Σ+λ)−2Σ KK′. (81)
1−γ
ContractingthiswithanarbitrarymatrixΣ′andmultiplyingby λ2 givesthemoresymmetricform:
T
λ2 (cid:104) (cid:105) df2 df2 1
Tr Σ′(Σˆ +λ)−1X⊤K′X(Σˆ +λ)−1 ≃ ΣΣ′ KK′ . (82)
T df d˜f 1−γ
1 1
Wecanfurthersimplifythisbyadoptingtheshorthandγ ΣΣ′KK′ = df2 Σ dΣ f′ 1d d˜f f2 K 1K′.
This result can alternatively be derived using a straightforward but tedious and significantly less
conceptuallyilluminatingargumentbasedonaone-pointdeterministicequivalentfora‘sourced’
resolvent. Concretely,thisargumentstartsbywriting
(cid:12)
(Σˆ +λ)−1X⊤K′X(Σˆ +λ)−1 = ∂ [Σ1/2Z⊤(K+JK1/2K′K1/2)ZΣ1/2+λ]−1(cid:12) (cid:12) ,
∂J (cid:12)
J=0
(83)
and then proceeds by using a one-point deterministic equivalent before implicitly differentiating
theresultingS-transforms. Then,variousapplicationsofthedualityrelationallowtheresulttobe
simplifiedintotheformobtainedusingdiagrammaticsabove.
C.3 DUALITYRELATIONSANDDERIVATIVES
Here, we explicitly state all duality relationships that one can derive. In the section on 1-point
deterministicequivalents,wehaveproventhat:
qdf ≡qdf1 (κ)≃qdf1 (λ)=df1 (λ)≃df1 (κ˜)≡d˜f (84)
1 Σ Σˆ Kˆ K 1
Wehavealsoseenthefirstdualityrelation,namelythat:
κκ˜ 1 1
=λSS˜= = (85)
λ qdf
1
d˜f
1
Logarithmicdifferentiationofthisyieldsaseconddualityrelation:
dlogκ
+
dlogκ˜
=1+
df 1−df2 Σˆ(λ)
. (86)
dlogλ dlogλ df
1
Herewehaveusedthat
ddf df2 (λ)−df1 (λ)
1 = Σˆ Σˆ . (87)
dλ λ
Westressthatunlikewithdf ,df2 (λ)̸=df2 (κ)=df . Sincetherighthandsidecanbeestimated
1 Σˆ Σ 2
fromthedata,thisgivesusawaytoturnanestimateof dlogκ intoanestimatefor dlogκ˜ orvice-versa.
dlogλ dlogλ
Next,werelatedf tod˜f . Wewillbeexplicitandwritealldfswithsubscriptstoavoidconfusion.
2 2
Wehave,bydifferentiatingEquation(84):
κdλ κdλ κdκ
qκ∂ df1 (κ)=q λ∂ df1 (λ)= λ∂ df1 (λ)= κ˜∂ df1 (κ˜) (88)
κ Σ λdκ λ Σˆ λdκ λ Kˆ κ˜dκ˜ κ˜ K
Evaluatingtheleftandrightsidesusing(87)weget:
dlogκ˜
q(df2 (κ)−df1 (κ))= (df2 (κ˜)−df1 (κ˜)) (89)
Σ Σ dlogκ K K
23PublishedasaconferencepaperatICLR2024
Thisyields:
dlogκ˜
d˜f ≡df2 (κ˜)=qdf1 (κ)+q (df2 (κ)−df1 (κ)) (90)
2 K Σ dlogκ Σ Σ
Similarlyonecanwriteanestimateofdf fromjustthedataalone:
2
dlogλ dlogλ
df ≡df2 (κ)=df1 (λ)+ (df2 (λ)−df1 (λ))=df1 (λ)+ ∂ df1 (λ). (91)
2 Σ Σˆ dlogκ Σˆ Σˆ Σˆ dlogκ λ Σˆ
Pluggingthisbackinto(90)givesanistimateofd˜f fromthedataalone. Thisiscrucialinallowing
2
theCorrGCVtobeefficientlycomputed.
Wenowcalculatethederivativeofκ,κ˜onλ. Wehave
dlogλ dlog1/S
κ=Sλ⇒ =1+
dlogκ dlogκ
dlog1/Sdlogdf
=1+ 1 (92)
dlogdf dlogκ
1
dlog1/Sdf −df
=1− 1 2.
dlogdf df
1 1
Usingd˜f =qdf wewrite
1 1
1 1
S =
1−d˜f
S K(d˜f 1)=
d˜f df−1(d˜f
). (93)
1 1 K 1
Differentiatingthisgives:
dlog1/S d˜f d˜f d˜f
=1+ 1 =1− 1 =− 2 . (94)
dlogd˜f κ˜df′(κ˜) d˜f −d˜f d˜f −d˜f
1 1 1 2 1 2
Alltogetherthisis:
dlogλ d˜f df −df
=1+ 2 1 2
dlogκ d˜f 1−d˜f
2
df
1
df d˜f −df d˜f
= 1 1 2 2 (95)
df d˜f −df d˜f
1 1 1 2
1−γ
= .
1−
d˜f2
d˜f1
Thisfinallyyields:
∂κ =S1− dd ˜˜ ff 12
. (96)
∂λ 1−γ
Ananalogousargumentyields:
∂κ˜ 1− df2
=S˜ df1. (97)
∂λ 1−γ
NotealsothatwhenK =I,d˜f =(d˜f )2 =q2df2,yielding
2 1 1
df d˜f
γ ≡ 2 2 =qdf . (98)
df d˜f 2
1 1
Thisrecoverstheuncorrelatedridgeregressionsetting.
C.4 CORRELATEDSAMPLESANDNOISE,UNCORRELATEDTESTPOINT
Byapplyingthedeterministicequivalencesprovedinthepriorsection,wecandirectlyobtainan
exactformulafortheasymptoticformofthetrainingandgeneralizationerrorforalinearmodel
trainedonacorrelateddatasetandevaluatedonanuncorrelatedtestpoint. Infact,wecandobetter.
Thedeterministicequivalence(76)allowsustoeasilyconsiderthecasewherethetestpointhasa
24PublishedasaconferencepaperatICLR2024
differentcovarianceΣ′fromthecovarianceΣofthetrainingset. Thisallowsustostatetheformula
forgeneralizationundercovariateshiftaswell. Recalling:
X⊤ϵ
wˆ =Σˆ(Σˆ +λ)−1w¯ +(Σˆ +λ)−1 , E [ϵϵ⊤]=σ2K′ (99)
T ϵ ϵ
Wehavethatthegeneralizationerror(foratestpointwhosedistributionhascovarianceΣ′)is:
R =(w¯ −wˆ)⊤Σ′(w¯ −wˆ)
g
λ2
=λ2w¯⊤(Σˆ +λ)−1Σ′(Σˆ +λ)−1w¯+σ2 Tr[Σ′(Σˆ +λ)−1X⊤K′X(Σˆ +λ)−1]. (100)
ϵ T
(cid:124) (cid:123)(cid:122) (cid:125)
Signal (cid:124) (cid:123)(cid:122) (cid:125)
Noise
TheSignaltermisimmediatelyobtainedbyapplyingthedeterministicequivalenceinEquation(76).
γ
Signal=κ2w¯⊤Σ′(Σˆ +λ)−2Σw¯+κ2w⊤Σ(Σˆ +λ)−2Σw¯ ΣΣ′ .
(cid:124) (cid:123)(cid:122) (cid:125) 1−γ (101)
Bias2 (cid:124) (cid:123)(cid:122) (cid:125)
VarX
Herewehaveexplicitlydelineatedwhichpartsofthesignaltermareduetothebiasoftheestimator,
andwhichtermsareVar .Thelattercanberemovedbybaggingtheestimatoroverdifferentdatasets.
X
Notethatindiagrammaticlanguagethebiastermcorrespondsexactlythetothedisconnectedaverages
oftheleftandrightresolventsseparately,whichmakessense,asitisthegeneralizationobtainedby
firstaveragingthepredictoroverdifferenttrainingsetsbeforecalculatingthetestrisk.
Similarly,byapplyingthedeterministicequivalence(82),weobtain:
γ
Noise=σ2 ΣΣ′KK′ . (102)
ϵ 1−γ
(cid:124) (cid:123)(cid:122) (cid:125)
VarXϵ
ThisyieldsEquation(27). SpecializingtothecaseofΣ=Σ′,K =K′ yieldsthein-distribution
matched-noise-correlationsettingofEquation(24).
Wenexttreatthetrainingerror. Wehave
1 1
Rˆ = |y−yˆ|2 = |Xw+ϵ−Xwˆ|2
in T T
= 1 |X(Σˆ +λ)−1w¯|2+ 1
(cid:12)
(cid:12) (cid:12)ϵ−X(Σˆ
+λ)−1X⊤ϵ(cid:12)
(cid:12)
(cid:12)2
T T (cid:12) T (cid:12) (103)
λ2 (cid:104) (cid:105)
=λ2w¯⊤Σˆ(Σˆ +λ)−2w¯+ Tr K′(Kˆ +λ)−2 .
T
(cid:124) (cid:123)(cid:122) (cid:125)
Signal (cid:124) (cid:123)(cid:122) (cid:125)
in Noisein
Onecanevaluatethesignaltermbyrecognizingitasaderivativeandapplyingthechainrule:
d
Signal =−λ2 w¯⊤Σˆ(Σˆ +λ)−1w¯
in dλ
dκ
=λ2 w¯⊤Σ(Σ+κ)−2w¯
(104)
dλ
d˜f −d˜f 1
=λ2S 1 2 w¯⊤Σ(Σ+κ)−2w¯.
d˜f 1−γ
1
Inthelastline,wehaveinsertedtheformof dκ fromEquation(96).
dλ
ThenoisetermforgenericK′isslightlymoreinvolved. Wecanwriteitas:
λ2 (cid:104) (cid:105)
Noise =−σ2 ∂ Tr K′(Kˆ +λ)−1
in ϵ T λ
=−σ2λ2
∂
(cid:20) κ˜ Tr(cid:2) K′(K+κ˜)−1(cid:3)(cid:21)
(105)
ϵ T λ λ
=σ2κ˜ Tr(cid:2) K′(K+κ˜)−1(cid:3) −σ2λ dκ˜ Tr(cid:2) KK′(K+κ˜)−2(cid:3)
ϵT ϵT dλ
25PublishedasaconferencepaperatICLR2024
NowapplyingthederivativerelationshipEquation(97)weget:
(cid:20) (cid:21)
1 df −df 1
Noise =σ2κ˜ TrK′(K+κ˜)−1− 1 2 df (106)
in ϵ T df 1−γ KK′
1
InthecaesofK =K′thissimplifiesto:
(cid:20) df −df (cid:21) σ2d˜f −d˜f 1
Noise =σ2κ˜d˜f 1− 1 2 d˜f = ϵ 1 2 . (107)
in ϵ 1 df d˜f −df d˜f 2 S d˜f 1−γ
1 1 2 2 1
C.5 CORRELATEDTESTPOINT
Itisoftenthecasethattestingonnear-horizondatagivesanoptimisticoverestimateofthegeneraliza-
tionerrorfortimeseriesprediction. Westudythiseffectprecisely. Specifically,oursettingisone
whereweallowthetestpointxtohavenon-vanishingcorrelationwiththetrainingset. Similarly,we
assumethatthelabelnoiseonthetestpoint,ϵhasnontrivialcorrelationwiththelabelnoiseonthe
testset. Weassumeageneralmatrix-Gaussianmodelforthecovariates:
(cid:18) (cid:19) (cid:18) (cid:18) (cid:19) (cid:19)
x⊤ 1 k
∼N 0, ⊗Σ (108)
X k⊤ K
wherethevectork∈Rpgivesthecorrelationbetweenthetrainingpointsandthetestpoint,i.e.,
E[x x ]=Σ k . (109)
i tj ij t
Similarlyforϵ,ϵwewrite:
(cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
ϵ 1 k⊤
∼N 0,σ2 (110)
ϵ ϵ k K
Wefurtherassumethattheseareindependentofthexcovariates. Thatis,E[x ϵ]=E[x ϵ ]=0.
ti i s
BytheusualformulasforGaussianconditioning,thisgenerativemodelimpliesthat
x|X ∼N(X⊤α,(1−ρ)Σ) (111)
and
ϵ|ϵ∼N(α⊤ϵ,(1−ρ)σ2) (112)
ϵ
wherewewrite
ρ=k⊤K−1k, α=K−1k. (113)
Then,theout-of-sampleriskis
R =E[x⊤(w¯ −wˆ)+ϵ]2. (114)
g
Usingtheexpressionforwˆ inEquation(5)wewrite
1
w¯ −wˆ =λ(Σˆ +λ)−1w¯ − (Σˆ +λ)−1X⊤ϵ. (115)
T
GiventhatE[[x ] ϵ ]=0,wehave:
t i s
(cid:20)
1
(cid:21)2
R =λ2E[x⊤(Σˆ +λ)−1w¯]2+E ϵ− x⊤(Σˆ +λ)−1X⊤ϵ . (116)
g T
(cid:124) (cid:123)(cid:122) (cid:125)
Signal (cid:124) (cid:123)(cid:122) (cid:125)
Noise
Here, we have again identified the two terms that appear as signal and noise terms. Taking an
expectationoverx|X,wehaveforthesignalterm
Signal=(1−ρ)λ2Ew¯⊤(Σˆ +λ)−1Σ(Σˆ +λ)−1w¯
+λ2Ew¯⊤(Σˆ +λ)−1X⊤αα⊤X(Σˆ +λ)−1w¯. (117)
26PublishedasaconferencepaperatICLR2024
Leveragingthedeterministicequivalences(76),(81),weobtain
Signal=
κ2
w¯⊤Σ(Σ+κ)−2w¯(cid:2) 1−ρ+κ˜2α⊤K(K+κ˜)−2α(cid:3)
(118)
1−γ
Similarly,takinganexpectationfirstoverϵ|ϵandthenoverx|X,wehave
(cid:20)
1
(cid:21)2
Noise=(1−ρ)σ2+E α⊤ϵ− x⊤(Σˆ +λ)−1X⊤ϵ (119)
ϵ T
(cid:20) (cid:21)
1
=(1−ρ) Eϵ⊤X(Σˆ +λ)−1Σ(Σˆ +λ)−1Xϵ+σ2
T2 ϵ
(cid:104) (cid:105)2
+E α⊤ϵ−α⊤Kˆ(Kˆ +λ)−1ϵ . (120)
Notingthat
I −Kˆ(Kˆ +λ)−1 =λ(Kˆ +λ)−1 (121)
T
thissimplifiesto
(cid:20) 1 (cid:21) (cid:104) (cid:105)2
Noise=(1−ρ) Eϵ⊤X(Σˆ +λ)−1Σ(Σˆ +λ)−1Xϵ+σ2 +λ2E α⊤(Kˆ +λ)−1ϵ .
T2 ϵ
(122)
Finally,takingtheremainingexpectationoverϵ,wehave
(cid:20) (cid:21)
1
Noise=σ2(1−ρ) E Tr[X(Σˆ +λ)−1Σ(Σˆ +λ)−1XK]+1
ϵ T2
+σ2λ2Eα⊤(Kˆ +λ)−1K(Kˆ +λ)−1α. (123)
ϵ
Nowagainleveragingdeterministicequivalence(81)aswellas(77)weobtain:
(cid:20) γ (cid:21) σ2
Noise=σ2(1−ρ) +1 + ϵ κ˜2α⊤K(K+κ˜)−2α
ϵ 1−γ 1−γ
(124)
=
σ ϵ2 (cid:2) 1−ρ+κ˜2α⊤K(K+κ˜)−2α(cid:3)
.
1−γ
AlltogetherEquations(118)and(124)togethergivethedesiredresult:
Rk =Rk=0(cid:2) 1−ρ+κ˜2α⊤K(K+κ˜)−2α(cid:3) . (125)
g g
Onecouldstraightforwardlyextendthistothecasewherethelabelnoisecorrelationsaredifferent
fromthoseofthecovariateswithrelativeease. Asdoingsoaddscomplexitytotheformulaswithout
muchconceptualgain,wewillnotdosohere.
D BIAS-VARIANCE DECOMPOSITIONS
Ofteninpriorwork,ithasbeenconventiontocalltheproportionaltothesignalw¯ theBias2 and
thetermproportionaltothenoiseσ2 thevariancecomponentofthetotalrisk. Strictlyspeaking
ϵ
fromtheperspectiveofafine-grainedanalysisofvariance,thisisnottrue. Thiswaspointedoutin
detailbyAdlam&Pennington(2020);Lin&Dobriban(2021),wherefine-grainedbiasvariance
decompositionswereperformedforrandomfeaturemodels.
Theestimatorwˆ isgivenasinEquation(5)by:
X⊤ϵ
wˆ =Σˆ(Σˆ +λ)−1w¯ +(Σˆ +λ)−1 . (126)
T
WeseethattheestimatorissensitivetoboththedrawofϵandthechoiceofX. Eveninthecaseof
nonoise,σ2 =0,theestimatorwouldstillhavevarianceoverdifferentdrawsofX. Thisvariance
ϵ
27PublishedasaconferencepaperatICLR2024
canberemovedifonehadaccesstomultipledatasetsbybaggingoverthedata. Inthelimitofinfinite
bagging,thisamountstoadataaverage. Thisyields,bydeterministicequivalence:
E wˆ =Σ(Σ+κ)−1w¯. (127)
X
Consequentlytheriskofthisestimatorwouldbejustthebias:
Bias2 =R(E wˆ)=κ2w¯⊤Σ(Σ+κ)−2w¯. (128)
X,ϵ
Similarly,ifonefixedthedatasetX butaveragedoverdifferentdrawsofthenoiseϵ,theriskofthis
averagedpredictorwouldbethesameastheriskofapredictortrainedonanoiselessdatasetσ2 =0.
ϵ
NotethatthispredictorstillhasvarianceoverthedrawofX. Wethusgetthat:
γ
Var =R(E wˆ)−Bias2 =κ2w¯⊤Σ(Σ+κ)−2w¯ . (129)
X ϵ 1−γ
TheremainingtermisVar ,sinceitisremovableeitherbybaggingoverdatasetsXorbyaveraging
X,ϵ
overnoiseϵ. Itisgivenby:
γ
Var =R(wˆ)−Bias2−Var = σ2. (130)
X,ϵ X 1−γ ϵ
Westressthisistrueunderbothcorrelatedanduncorrelateddata,giventhatκ,γ isappropriately
defined. This decomposition was computed for linear models in Canatar et al. (2021b). These
equationsalsoholdforoutofdistributionrisk,asgiveninEquation(27).
E SCALING ANALYSIS
E.1 REVIEWOFOPTIMALRATES
Theanalysisofthescalingpropertiesoflinearregressionunderpower-lawdecayinthecovarianceΣ
andtargetw¯ alongtheprincipalcomponentsofΣwasstudiedindetailinCaponnetto&DeVito
(2007); Caponnetto & Vito (2005). It has received renewed attention given that it can sharply
characterize the scaling properties of kernel methods, especially the neural tangent kernel (Jacot
etal.,2018)onavarietyofrealisticdatasets(Bordelonetal.,2020;Cuietal.,2021;Spigleretal.,
2020). Thishasimplicationsforneuralscalinglaws(Bahrietal.,2024;Kaplanetal.,2020)inthe
lazyregimeofneuralnetworktrainingidentifiedinChizatetal.(2019).
One defines the source and capacity exponents α,r respectively by looking at the decay of the
eigenvaluesλ ofΣandcomponentsw¯ ofw¯ alongthoseeigendirections. Then, formanyreal
k k
datasetsoneobservesthepowerlaws:
λ ∼k−α,w¯ ∼k−β. (131)
k k
Itisadvantageoustowriteβ =2αr+1,astheexponentrappearsnaturallyinthefinalrates. Then,
intheridgelesslimitonehasqdf =1. Fromthisonecaneasilyobtainthat
1
κ∼T−α. (132)
Atfiniteridge,moregenerallyonehas
κ∼max(λ,T−α). (133)
Forthisreason,κisoftencalledtheresolutionorthesignalcapturethreshold. Itdetermineswhich
eigencomponentsarelearnedandwhicharenot. Further,takingσ =0,onegetsthescaling:
ϵ
R =R ∼κ−2min(r,1) (134)
out g
Ifonetakestheridgetoscalewiththedataasλ∼T−ℓasinCuietal.(2021),thefullequationinthe
noiselesssettingbecomes
R ∼T−2min(α,ℓ)min(r,1) (135)
out
ThesearetheratespredictedinCaponnetto&DeVito(2007).
IfonepassesdatathroughN linearrandomfeaturesbeforeperforming,onecanchangetheresolution
scalingbyaddingafurtherbottleneck:
κ∼max(λ,T−α,N−α). (136)
Thisplaysanimportantroleincurrentmodelsofneuralscalinglaws(Atanasovetal.,2024;Bahri
et al., 2024; Bordelon et al., 2024; Defilippis et al., 2024; Maloney et al., 2022). It is therefore
interestingtoaskwhetherthepresenceofstrongcorrelationscouldalsoaffectthescalinglawsby
addingafurtherbottlenecktoEquation(136). Weanswerthisinthenegative.
28PublishedasaconferencepaperatICLR2024
Power Law Toeplitz Min Eigenvalue
100 Analytic Formula
Empirics
101
102
103
104
104 103 102 101 100 101 102
Figure 6: Verification of the minimum eigenvalue for a Toeplitz matrix with power law decay
K =(1+|t−s|)−χ. WeseeagreatmatchbetweenEquation(140)andtheempirics. Inparticular,
ts
thisguaranteesthatthematrixwillneverbesingularevenatverysmallpositiveχ,correspondingto
strongpowerlawcorrelations.
E.2 STRONGCORRELATIONSDONOTAFFECTSCALINGEXPONENTS
Here,weaskwhetherstronglycorrelateddatacouldchangethescalinginEquation(133). First,we
notethatforcorrelateddata,aslongaseachnewsampleislinearlyindependentoftheprioronesin
RT,wehavethatK doesnothaveanyzeroeigenvalues. Ingeneral,theT ×T correlationmatrixK
willhaveToeplitzstructurewhenthecorrelationsarestationaryandwillbeinvertibleaslongaseach
newdatapointisnotalinearcombinationofthepriorones. Itcouldhoweverbethatthesmallest
eigenvaluegoestozeroasT →∞. ThiswouldleadS toblowupandcouldleadtoaworsescaling
K
law. Weshowthatthiswillnothappenforexponential,nearestneighbor,andpowerlawcorrelations.
Inthelattercase,werequiresomeassumptionsonthepowerlawexponent.
Proposition E.1. Under exponential, nearest neighbor, and power law correlations, the T ×T
correaltionmatrixK remainsnonsingularasT →∞.
Proof. We can study the invertibility of the limiting T → ∞ matrix by applying the first Szego
limittheoremforToeplitzmatrices. LettheautocorrelationbeK = a ,andwriteitsFourier
t,t+τ τ
transform,knownasthesymbolofT,as:
∞
(cid:88)
w(θ)= a eikθ (137)
k
k=−∞
Then,undermildregularityconditionsonw(θ)wehave
1 1 (cid:90) 2π
lim logdetK = logw(θ)dθ. (138)
T→∞T 2π
0
Inthecaseofexponentialandnearest-neighborcorrelations,thecoefficientsa decayquicklyenough
k
tosatisfytheSzego˝ limittheoremconditionsandensurethattherighthandsideoftheaboveequation
isfinite. Moreover,wecancalculatedf1 analyticallyinthesecasesandverifythattherearenozero
K
eigenvaluesinthespectrum.
Forpowerlawcorrelations,wecanwritew(θ)intermsofpolylogarithms:
(cid:88)∞ eikθ
w(θ)= =e−iθLi (e−iθ)+eiθLi (e−iθ)−1. (139)
(1+|k|)χ χ χ
k=−∞
Thisisarealandpositivefunctionon[0,2π],andisminimizedatθ =π,whereittakesthevalue:
w(π)=2(1−21−χ)ζ(χ)−1. (140)
ThisgivesustheminimumeigenvalueasT → ∞atanyχ,withχ = 1understoodinalimiting
sense.1 Moreover,thisinfactgivesalowerboundontheminimumeigenvalueatfiniteN,asnoted
inarelatedMathOverflowpost.2 WeverifythisformulanumericallyinFigure6.
1Inparticular,wehavelim w(π)=log4−1≃0.386....
χ→1
2SeethisHTTPSURL.
29
nimPublishedasaconferencepaperatICLR2024
UndertheassumptionthatKremainsinvertibleinthelargeT limit,wehavethatlim df (λ)=1
λ→0 K
atanyvalueofT. Moregenerallydf isacontinuousmonotonicallydecreasingfunctionofλthat
K
goesfrom1whenλ = 0to0asλ → ∞. Asλ → ∞wecanexpandandgetatlinearorderthat
df = 1 1TrK = 1. Writing:
K λT λ
S (df (λ))−1
df (λ)= K K (141)
K λ+S (df (λ))−1
K K
impliesthatatsmallλ,S isboundedfromabovebyaT-independentconstantatanyT. S isalso
K K
boundedfrombelowby1(seeF.1). Then,writing
λ
κ= S (qdf ), (142)
1−qdf K 1
1
weseethatS willnotcontributeanypoleorzerothatwouldeffectthescalingpropertiesofκas
K
λ→0. Consequently,correlateddatadoesnotaffectthescalinglaw.
F DOUBLE DESCENT ANALYSIS
Inthisappendix,westudyhowdoubledescentiseffectedbycorrelations. Wealsomoregenerally
studyhowcorrelationsaffectthekeyquantitiesofinterestrelativetotheuncorrelatedsetting.
F.1 BOUNDSONRENORMALIZEDRIDGES
Lemma F.1. For positive-definite K such that 1 Tr(K) = 1 we have that S (d˜f) ≥ 1 for all
T K
valuesofd˜f ∈(0,1].
Proof. WeadaptanargumentfromZavatone-Veth&Pehlevan(2023). Werecognizethatforfixed
κ˜ >0thefunction:
ρ
ρ→ (143)
κ˜+ρ
isconcave. Consquently,Jensen’sinequalityyields:
ρ E ρ 1
df (κ˜)=E ≤ ρ = =df (κ˜) (144)
K ρκ˜+ρ κ˜+E ρ κ˜+1 I
ρ
pointwise in κ˜. Here E is the expectation over the spectrum of K. We have also used that
ρ
E [ρ]= 1 Tr(K)=1. TheaboveinequalityisstrictunlessK =I orκ˜ =0.
ρ T T
Wenextobservethatbothdf (λ)anddf (λ)aremonotonicallydecreasingfunctioninλthatare
K I
equal to 1 only when λ = 0. This means that the solutions to the equations d˜f = df1 (λ) and
K
d˜f =df1(λ)areuniqueforalldf ∈(0,1]andisalsomonotonicallydecreasing. Consequentlywe
I
have:
1−d˜f
df−1(d˜f)≤df−1(d˜f)= (145)
K I d˜f
Upondividingbothsidesbydf−1 wegetthedesiredequality⇒S (d˜f)≥1. Thisisanequality
K K
whenK =I . Whenκ˜ =0andthusd˜f =1,S (d˜f)maystillbegreaterthanone.
T K
PropositionF.2. Letκ betherenormalizedridgeλS whenthedatahascorrelationstructureK
c
andletκ bethecorrespondingvalueofκwhenthereisnocorrelationbetweendatapoints. We
u
haveκ ≥κ . Moreover,intheridgelesslimitκ =κ .
c u c u
Proof. Inthecorrelatedsetting,werecallthattherenormalizedridgeκ solves
c
λS (qdf1 (κ ))
κ = K Σ c . (146)
c 1−qdf1 (κ )
Σ c
30PublishedasaconferencepaperatICLR2024
Callthenumeratorλ˜. Fromtheprecedinglemmawehavethatλ˜ ≥ λ. Thus, wehavethatκ is
c
equivalenttotheself-consistentsolutionfortheequation
λ˜
κ = . (147)
c 1−qdf1 (κ )
Σ c
Butthisisthesameastheself-consistentequationforalinearregressionproblemwithanexplicit
ridgeλ˜ ≥λ. Thepropositionthenfollowsfromthefactthatκismonotonicinλintheuncorrelated
ridgeregressionsetting.
Asλ → 0,κisentirelydeterminedbythepolestructureofqdf1 = 1. Thisisindependentofany
Σ
structureonK andsointheridgelesslimit,κ =κ .
c u
CorollaryF.3. Fixq. Thepresenceofcorrelationseitherdecreasesorkeepsconstantdf ,d˜f . In
1 1
theridgelesslimit,theyareunchanged.
Proof. Evaluatingdf asdf =df1 (κ)wehavesinceκ ≥κ anddf ismonotonedecreasingin
1 1 Σ c u 1
κthatdf decreases. Sinced˜f =qdf ,thisalsodecreases. Becauseκisunchangedintheridgeless
1 1 1
limit,thelastpartofthecorollaryfollows.
CorollaryF.4. Fixq. Thepresenceofcorrelationseitherdecreasesorkeepsconstantdf . Inthe
2
ridgelesslimit,itisunchanged.
Proof. Theproofisasinthepriorcorollary,notingthatdf =df2 (κ)ismonotonedecreasinginκ
2 Σ
andκ ≥κ .
c u
PropositionF.5. Letκ˜ betherenormalizedridgeλS˜whenthedatahascorrelationstructureK
c
andletκ˜ bethecorrespondingvalueofκ˜ whenthereisnocorrelationbetweendatapoints. We
u
haveκ˜ ≤κ˜ . Moreover,intheridgelesslimitκ˜ ≤κ˜ still.
c u c u
Proof. Whenλ̸=0wecanrewritethedualityrelation(85)as
q 1
κdf1 (κ)= . (148)
λ Σ κ˜
Thisseparatelyholdstrueforthepairsκ ,κ˜ andκ ,κ˜ . Wenotethatonthelefthandsideκdf1 (κ)
u u c c Σ
isasumof κ overtheeigenspectrumλ ofΣ. Eachtermismonotoneincreasinginκ,andthus
λi+κ i
increasesaswegofromtheuncorrelatedκ tothecorrelatedκ . Consequently,κ˜ ≤κ˜ .
u c c u
Intheridgelesslimit,bythepriorcorollary,d˜f isunchangedbetweencorrelatedanduncorrelated
data. Consequently,wehave:
1 1
=df1 (κ )=df1 (κ )≤ (149)
1+κ˜ IT u K c 1+κ˜
u c
Thus,weagainhaveκ˜ ≤κ˜ withequalityonlyifK =I orκ =0.
c u T c
F.2 RIDGELESSLIMIT
The limiting behavior of the two renormalized ridges κ and κ˜ depends on whether one is in the
underparameterizedregimeT > N (q < 1)ortheoverparameterizedregimeT < N (q > 1). In
theunderparameterizedregime,wehavethatκ↓0asλ↓0,whilegenericallyκ˜remainsnon-zero
andsolvestheself-consistentequationq = d˜f (κ˜). Conversely,intheoverparameterizedregime
1
κ˜ ↓0asλ↓0whileκremainsnon-zeroandsolves1/q =df (κ). Wenumericallyillustratethese
1
complementarylimitingbehaviorsinAppendixJ.
Ourtaskisnowtodeterminethecorrespondinglimitsoftheout-of-samplerisk. First,weconsider
the case of matched correlations. There, in the underparameterized regime we have γ → d˜f /q,
2
whileintheoverparameterizedregimewehaveγ →qdf . Asaresult,wefindthat
2
 d˜f
q−d2
˜f
σ ϵ2 q <1,
limR ≃ 2 (150)
g
λ↓0 1−κ q2
df
w¯⊤Σ(Σ+κ)−2w¯ +
1−qd qf
d2
f
σ ϵ2 q >1,
2 2
31PublishedasaconferencepaperatICLR2024
100 100
100
102
1 corr
104 2
1
c fro er er 101
106 2 free d df f1
1
c co or rr
r
d df f2
2
c co or rr
r
108
101
d df f1
1
f fr re ee
e 102
d df f2
2
f fr re ee
e
101 102 103 101 102 103 101 102 103
T T T
(a) (b) (c)
Figure7: Theorycurvescomparingκ,κ˜,df ,d˜f ,df ,d˜f forcorrelateddataK ̸=Ivsuncorrelated
1 1 2 2
dataK =Iatλ=10−8forN =100onisotropicdata. Forthecorrelations,wechooseexponential
correlationswithlengthξ =102. a)WeseethatwhenT <N,κ,κ˜stronglyagree,whereaswhen
T > N,κbecomesofordertheridgewhileκ˜ becomesorder1. Theeffectsofcorrelationsonκ˜
arethereforenoticeableinthislimit. b)Weseethatthedf ,d˜f areunchangedbetweencorrelated
1 1
(solid)anduncorrelated(dashed)datawhentheridgeissmall. c)Weseeasimilarbehaviorfordf
2
butnotford˜f whenT >N. Still,atfirstordernearT =N,weseeagreementofd˜f betweenthe
2 2
correlatedanduncorrelatedsettings,aspredictedbyourtheory.
Intheoverparameterizedridgelesssetting,sinceneitherκ,df ordf ismodified,thegeneralization
1 2
errorisexactlyidenticalasthatforlinearregression. Intheunderparameterizedlimit,westudyhow
d˜f behaves. Wecanwrite
2
(cid:18)
ρ
(cid:19)2 (cid:20) (cid:18)
ρ
(cid:19)(cid:21)2
df2 (κ˜ )=E ≥ E =df1 (κ˜ )2 =df1(κ )2 =df2(κ ), (151)
K c ρ ρ+κ˜ ρ ρ+κ˜ K c I u I u
c c
wheretheexpectationistakenovertheeigenspectrumofK. Moreover,nearq =1whenκ˜issmall
wehavethatatlinearorder:
1
df2 (κ˜ )=1−2κ˜ Tr[K−1]+O(κ˜2)=df1 (κ˜ )2+O(κ˜2)=df2(κ˜ )+O(κ˜2). (152)
K c cT c K c c I u u
Thus,atfirstorderthedoubledescentpeakisunaffectedbycorrelationsintheunderparameterized
regimeaswell. WeillustratetheseobservationsinFigure7.
F.3 EFFECTIVERIDGEENHANCEMENT
Ifλisfinite,wehavebyPropositionF.2thatbyF.5thatκ >κ andκ˜ <κ˜ . Specifically,letΣˆ
c u c u K
betheempiricalcovarianceofthedatawhenthedatapointshavecorrelationsK. Wecanwritethe
followingequivalence:
Σˆ (Σˆ +λ)−1 ≃Σˆ (Σˆ +λS (qdf ))−1 (153)
K K I I K 1
Thusaddingcorrelationsamountstoincreasingtheridge. BecauseΣ isboundedfromabovefrom
K
PropositionE.1,intheridgelesslimittheλS willremainzero.
K
Whenthereisexplicitridge,wehavethatdf ,d˜f ,df willshrinkfromtheiruncorrelatedvalues.
1 1 2
Because to linear order in κ,κ˜, we have d˜f = (d˜f )2 at q = 1 and because correlations cause
2 1
d˜f2 todecreaseatfirstorderinκatthispoint,wegetthatcorrelationscaused˜f todecreaseina
1 2
neighborhoodofq =1.
Asaresultofthis,atfirstorderinκ,κ˜,wecanwriteγ =qdf (κ). Nearq =1,thiswillalsoshrink
2
inthecorrelatedcaserelativetotheuncorrelatedcase. Thus,thedoubledescenteffectwillbefurther
reduced. WeillustratetheseobservationsinFigure8.
F.4 MISMATCHEDCORRELATIONS
Now we consider the case of mismatched correlations. We consider the fully general setting in
whichΣ′ ̸=ΣandK′ ̸=K. Intheunderparameterizedregime,wehavedf2 → 1 Tr(Σ−1Σ′),
ΣΣ′ N
32PublishedasaconferencepaperatICLR2024
100 100
101
100 1 corr
2 corr 101
1 free
101 2 free df1 corr df2 corr
df1 corr df2 corr
df1 free df2 free
102
101 df1 free 102 df2 free
101 102 103 101 102 103 101 102 103
T T T
(a) (b) (c)
Figure8: Theorycurvescomparingκ,κ˜,df ,d˜f ,df ,d˜f forcorrelateddataK ̸=Ivsuncorrelated
1 1 2 2
dataK =Iatλ=10−1forN =100onisotropicdata. Forthecorrelations,wechooseexponential
correlationswithlengthξ =102. a)Weseeacrosstheboardthatinthepresenceofexplicitridge,
κgrowswhileκ˜shrinksundercorrelationsb)Weseethatthedf ,d˜f arealwaysdecreasedbythe
1 1
presenceofcorrelationswhentheridgeispresent. c)Weseeasimilarbehaviorfordf . Ford˜f ,we
2 2
alsoseeadecreaseintheneighborhoodofthedoubledescentpeakT =N. Still,atfirstordernear
T =N,weseeagreementofd˜f betweenthecorrelatedanduncorrelatedsettings,aspredictedby
2
ourtheory.
so γ → q−1d˜f 1 Tr(Σ−1Σ′). Similarly, γ → q−1df2 1 Tr(Σ−1Σ′). In the
ΣΣ′ 2N ΣΣ′KK′ KK′N
overparameterizedregime,wesimilarlyhavedf2 → 1 Tr(K−1K′),whichleadstoγ →
KK′ T ΣΣ′
qdf2 andγ →qdf2 1 Tr(K−1K′). Combiningtheseresults,wefindthat
ΣΣ′ ΣΣ′KK′ ΣΣ′T
 q−1df2 1 Tr(Σ−1Σ′)
 K 1K −′N
q−1d˜f
2
σ ϵ2
qdf2
q <1,
limR
g
≃ κ2w¯(Σ+κ)−1Σ′(Σ+κ)−1w¯ +κ2 ΣΣ′ w¯Σ(Σ+κ)−2w¯ (154)
λ↓0 
+qdf2
ΣΣ′
1T1 −T qr d(K
f
−1K′)
σ ϵ2
1−qdf 2
q >1
2
Theeffectofmismatchiseasiesttounderstandintheoverparameterizedregime,wherethefactor
1 Tr(K−1K′) multiplies the same expression for the noise term that appears for uncorrelated
T
datapoints. InthespecialcaseK′ =I ,wecanuseJensen’sinequalitytobound
T
1 1
Tr(K−1)≥ =1 (155)
T Tr(K)/T
withequalityiffK =I ,henceinthisspecialcasemismatchgenericallyincreasestheerror.
T
F.5 FURTHERCOMMENTSONTHEEFFECTOFMISMATCHEDCORRELATIONS
Here, we briefly comment further on the case in which the noise is uncorrelated (K′ = I ). In
T
greatestgenerality,weseektobound
1
df2 (κ˜)= Tr[K(K+κ˜)−2] (156)
K,K′=IT T
intermsof
1
d˜f (κ˜)= Tr[K2(K+κ˜)−2] (157)
2 T
Weproveasimpleupperbound,whichfollowsfromanegativeassociationargument:
PropositionF.6. Foranyκ˜ ≥0andK invertible,wehave
(cid:18) (cid:19)
1
df2 (κ˜)≤ Tr(K−1) d˜f (κ˜), (158)
K,K′=IT T 2
withequalitywhenκ˜ =0.
33PublishedasaconferencepaperatICLR2024
Proof. Ifκ˜ =0,theclaimobviouslyholdswithequalitysolongasK isinvertible. Then,forany
fixedκ˜ >0,define
ρ2
f(ρ)= (159)
(ρ+κ˜)2
and
1
g(ρ)= , (160)
ρ
suchthat
df2 (κ˜)=E[f(ρ)g(ρ)] (161)
K,K′=IT
and
d˜f (κ˜)=E[g(ρ)] (162)
2
whereexpectationistakenwithrespecttothedistributionofeigenvaluesofK. Observethatf(ρ)
isforanyκ˜ > 0amonotoneincreasingfunctionon(0,∞),whileg(ρ)isamonotonedecreasing
functionon(0,∞). Then,foranyρ ,ρ ∈(0,∞),wehave
1 2
[f(ρ )−f(ρ )][g(ρ )−g(ρ )]≤0, (163)
1 2 1 2
soupontakingexpectationsforρ ,ρ independentlydrawnfromtheeigenvaluedistributionofK
1 2
wehave
(cid:20) (cid:21)
0≥E [f(ρ )−f(ρ )][g(ρ )−g(ρ )] =2E[f(ρ)g(ρ)]−2E[f(ρ)]E[g(ρ)] (164)
1 2 1 2
hence
E[f(ρ)g(ρ)]≤E[f(ρ)]E[g(ρ)]. (165)
Usingthedefinitionsoff andg,thisconcludestheproof.
G ASYMPTOTICS OF PREVIOUSLY-PROPOSED EXTENSIONS OF THE GCV
Here,wecomputethehigh-dimensionalasymptoticsofpreviously-proposedextensionstotheGCV
inthepresenceofcorrelations. Wewriteeachestimatorintheform
Eˆ =GRˆ (166)
predicted in
Moreover,weintroducethenotation
H =(Kˆ +λ)−1Kˆ (167)
forthesmoothingmatrix,intermsofwhichthepredictionsonthetrainingsetaregivenas
yˆ=Hy, (168)
astheseestimatorsmakeuseofthismatrix. Forcomparison,werecallfromthemaintextthatthe
asymptoticallypreciseCorrGCVestimatoris
d˜f
G =S(df ) 1 (169)
CorrGCV 1 d˜f −d˜f
1 2
G.1 THEGCV
ThestandardGCVcanbewrittenas
1
G = , (170)
GCV [1− 1 Tr(H)]2
T
hencewehaveimmediatelythat
1
G ≃ . (171)
GCV [1−d˜f (κ˜)]2
1
WedenotethisestimatorbyGCV inFigure5andinallplots.
1
34PublishedasaconferencepaperatICLR2024
Nearest-Neighbor Correlations, N=100, Isotropic Data Nearest-Neighbor Correlations N=100 =1.3, r=0.7
=1e+02, =1e-04, =0.5 =1e+02, =1e-04, =0.05
103
101
100 101 R
R
Bias2
101 101 V Va ar rX
X,
Naive GCV1
102 103 N Caa riv me
a
G
k
C GV C2
V
CorrGCV
103 105
104
101 102 103 101 102 103
T T
(a) UnstructuredCovariates (b) StructuredCovariates
Figure9: PlotoftheCarmacketal.(2012)estimatorrelativetootherGCVestimatorsdefinedearlier.
WenotethatsimilartothenaiveGCVs1and2,theCarmacketal.(2012)estimatorfailstocorrectly
predictR. a)UnstrucutredCovariates. b)StructuredCovariateswithsourceandcapacityexponents
aslabelled.
G.2 THEESTIMATOROFALTMAN(1990)ANDOPSOMERETAL.(2001)
StartingfromtheGCV,Altman(1990)andOpsomeretal.(2001)considerregressionwithcorrelated
errors
cov(ϵ)=σ2K (172)
ϵ ϵ
andandconsidertheestimator
1
G = (173)
Altman [1− 1 Tr(HK )]2
T ϵ
Asymptotically,wehaveimmediatelythat
1
G ≃ (174)
Altman {1− 1 Tr[K K(K+κ˜)−1]}2
T ϵ
PuttingK =K,wecanwrite
ϵ
1 1 1
Tr[K2(K+κ˜)−1]= Tr[K(I−κ˜(K+κ˜)−1)]= Tr(K)−κ˜d˜f , (175)
T T T 1
hence,as 1 Tr(K)=1bynormalization,wehave
T
1
G ≃ (176)
Altman κ˜2d˜f2
1
Usingthedualityrelation
κκ˜ 1
= , (177)
λ d˜f
1
thisreducesto
κ2
G ≃ =S2. (178)
Altman λ2
WedenotethisestimatorbyGCV inFigure5andinallplots.
2
G.3 THEGCCVESTIMATOROFCARMACKETAL.(2012)
Inournotation,Carmacketal.(2012)againassumethatthelabelnoisehas
cov(ϵ)=σ2K (179)
ϵ ϵ
35
rorrE rorrEPublishedasaconferencepaperatICLR2024
andconsidertheestimator
1
G = . (180)
Carmack [1− 1 Tr(2HK −HK H⊤)]2
T ϵ ϵ
ThefirsttermisidenticaltotheAltman(1990)estimator,whilethesecondisanadditionalcorrection.
Wehave
1 1
Tr(HK )≃ Tr[K K(K+κ˜)−1] (181)
T ϵ T ϵ
and
1 1 1
Tr(HK H⊤)= Tr[K Kˆ(Kˆ +λ)−1]−λ Tr[K Kˆ(Kˆ +λ)−2] (182)
T ϵ T ϵ T ϵ
1 1
= Tr[K Kˆ(Kˆ +λ)−1]+λ∂ Tr[K Kˆ(Kˆ +λ)−1] (183)
T ϵ λT ϵ
1 ∂κ˜ 1
≃ Tr[K K(K+κ˜)−1]−λ Tr[K K(K+κ˜)−2], (184)
T ϵ ∂λT ϵ
so
1 1 ∂κ˜ 1
Tr(2HK −HK H⊤)≃ Tr[K K(K+κ˜)−1]+λ Tr[K K(K+κ˜)−2]. (185)
T ϵ ϵ T ϵ ∂λT ϵ
SettingK =K,wehave
ϵ
1 ∂κ˜
1− Tr(2HK−HKH⊤)≃κ˜d˜f −λ d˜f (186)
T 1 ∂λ 2
as 1 Tr(K)=1. Now,usingEquation(97)andthedefiningequationλS˜=κ˜,wehave
T
1 1− df2
1− Tr(2HK−HKH⊤)≃κ˜d˜f −κ˜ df1d˜f (187)
T 1 1−γ 2
with
df d˜f
γ = 2 2 (188)
df 1d˜f
1
asdefinedinEquation(24). Then,wehave
(cid:34) 1− df2 (cid:35)−2
G ≃ κ˜d˜f −κ˜ df1d˜f . (189)
Carmack 1 1−γ 2
Wenowwrite
df d˜f
2 =γ 1 (190)
df
1
d˜f
2
andusethedualityrelationtoexpandκ˜d˜f =λ/κ=1/S,whichuponcombiningtermsgives
1
(cid:34) (cid:35)−2
1 1 d˜f −d˜f
G ≃ 1 2 . (191)
Carmack S1−γ d˜f
1
WerecognizetheterminbracketsasthenoisetermwhenK =K′inEquation(107). Ingeneralthis
impliesthat
G
Carmack
=(1−γ)2S
(cid:124)2(cid:32)
d˜f 1
(cid:123)d˜ (cid:122)−f
1
d˜f
2(cid:33) (cid:125)2 =S2 1 1− −dd ff
d d˜
˜12
f
fdd ˜˜
1
2ff 12 2
. (192)
CorrGCV2
WhenK =I ,d˜f /(d˜f −d˜f )=1/(1−1/(κ˜+1))
T 1 1 2
WeplotanexampleofthisestimatorforcorrelateddatainFigure9;notethesubstantialdeviation.
Thisistobeexpected,astherewasnoclaiminCarmacketal.(2012)thatthisestimatorshouldwork
forX correlations.
36PublishedasaconferencepaperatICLR2024
H WEIGHTED RISKS AND THE MMSE ESTIMATOR
InthisAppendix,webrieflycommentonrelatedrisksandtheMMSEestimator. Asdiscussedinthe
maintext,onemightconsideraweightedloss
1
L (w)= (Xw−y)⊤M(Xw−y)+λ∥w∥2; (193)
M P
for M a general positive-definite symmetric weighting matrix. Under our Gaussian statistical
assumptions,thisisequivalenttousinganunweightedlosswithM ← I ,K ← M1/2KM1/2,
T
andK′ ←M1/2K′M1/2.
WenowobservethattheBayesianMMSEestimatorisequivalenttominimizingaweightedloss
withtheparticularchoiceM =(K′)−1. Underourstatisticalassumptions,theMMSEestimatoris
simplygivenbytheposteriormean:
(cid:90)
wˆ = dwwp(w|X,y). (194)
MMSE
UsingourGaussianassumptionsonthedataandaGaussianpriorcorrespondingtotheridgepenalty,
theposterioris
p(w|X,y)∝p(X,y|w)p(w) (195)
=p(y|X,w)p(X)p(w) (196)
(cid:18) (cid:19)
1 1 1
∝exp − (y−Xw)⊤(K′)−1(y−Xw)− Tr(K−1XΣ−1X⊤)− λ∥w∥2 .
2 2 2
(197)
Discardingw-independenttermsandcompletingthesquare,thismeansthattheposterioroverwis
Gaussianwithmean
(X⊤(K′)−1X+λ)−1X⊤(K′)−1y (198)
andcovariance
(X⊤(K′)−1X+λ)−1. (199)
Therefore,theMMSEestimatorcorrespondstominimizingaweightedlosswith
M =(K′)−1. (200)
IfK′ =K,thisthencorrespondstominimizinganunweightedlossforuncorrelateddatapoints.
However,wenowobservethatthisprocedureisonlypossibleifonehasomniscientknowledgeof
K′,asreliablyestimating(K′)−1fromsamplesischallenging.
I S-TRANSFORMS FOR CERTAIN TOEPLITZ COVARIANCE MATRICES
InthisAppendix,werecordformulasforthespectralstatisticsofafewtractableandpractically-
relevantclassesofToeplitzcovariancematrices. WedirecttheinterestedreadertoworkbyKu¨hn
&Sollich(2012)orBasaketal.(2014)forstudiesofthelimitingspectralpropertiesofempirical
autocovariancematrices.
I.1 NEAREST-NEIGHBORCORRELATIONS
Webeginwiththesimplestnon-trivialexample: nearest-neighborcorrelations. Inthiscasewegive
aself-containedanalysis,whichhintsatsomeoftheapproachesthatcanbeusedformoregeneral
classesofToeplitzmatrices. Supposethat

1 t=s

K = b/2 t=s±1 (201)
ts
0 otherwise
37PublishedasaconferencepaperatICLR2024
isasymmetrictridiagonalToeplitzmatrixwithdiagonalelements1andoff-diagonalelementsb/2.
We have set the diagonal elements equal to 1 without loss of generality as this is equivalent to
choosinganoverallscale. TridiagonalToeplitzmatricesareexactlydiagonalizedbyFouriermodes
(Noscheseetal.,2013),andtheireigenvaluesareknowntobe
πt
λ =1+bcos (202)
t T +1
wheret = 1,...,T. Clearly,forthematrixtobepositive-definitewemusthave|b| < 1;wewill
assumeb>0withoutlossofgeneralityasthecosinetermissymmetric. Foratestfunctionϕ,we
thereforehave
T T (cid:18) (cid:19)
1 (cid:88) 1 (cid:88) πt
ϕ(λ )= ϕ 1+bcos . (203)
T t T T +1
t=1 t=1
RecognizingthisasaRiemannsumforanintegralwithrespecttox=t/(T+1),itiseasytoseethat
1 (cid:88)T (cid:90) 1
lim ϕ(λ )= dxϕ(1+bcosπx). (204)
T→∞T
t=1
t
0
Puttingλ = 1+bcos(πx),wehaveλ = 1+bwhenx = 0andλ = 1−bwhenx = π. Inthis
range,wecaninverttherelationshiptofindthatx=arccos[(λ−1)/b]/π. Differentiating,wehave
dx=− √ 1 dλ. Thus,theintegralbecomes
π (λ−1+b)(1+b−λ)
(cid:90) 1+b 1
dλ ϕ(λ). (205)
(cid:112)
π (λ−1+b)(1+b−λ)
1−b
Thisshowsthatforanyb>0thelimitingdensityofeigenvaluesis
1
1 , (206)
(cid:112) λ∈[1−b,1+b]
π (λ−1+b)(1+b−λ)
whichisanarcsinedistributioncenteredat1ofwidthb. Inotherwords,thelimitingdistribution
is simply the pushforward of the uniform measure on [0,1] by x (cid:55)→ 1+bcos(πx), much as the
distributionatfinitesizeisthepushforwardoftheuniformmeasureon{1/(T +1),...,T/(T +1)}
bythesamefunction.
Applyingtheresultofthisdigression,thelimitofthetracedresolventis
(cid:90) 1+b 1 1
g (z)→ dλ (207)
K π(cid:112) (λ−1+b)(1+b−λ)z−λ
1−b
1
= (208)
(cid:112)
(z−1)2−b2
Fromthis,wecanobtainwithabitofalgebrathecorrespondingS-transform
(cid:112)
(1+t)− 1+b2t(1+t)
S (t)= . (209)
K (1−b2)t
Weobservethatthelimitasb↓0oftheseresultsgives
1
limg (z)= (210)
b↓0 K z−1
limS (t)=1, (211)
K
b↓0
whichrecovertheexpectedresultsfortheidentitymatrix.
I.2 EXPONENTIALCORRELATIONS
Nowweconsiderthecaseofexponentialcorrelations
K =e−|t−s|/ξ (212)
ts
38PublishedasaconferencepaperatICLR2024
forsomecorrelationlengthξ,whichistreatedinthetextbookofPotters&Bouchaud(2020). Though
theeigenvectorsofthismatrixarenotpreciselyFouriermodesatfinitesize,onecanarguethatthe
approximationerrorintheresolventresultingfromtreatingK ascirculantbecomesnegligible,and
fromthatdeterminethelimit. Intheend,onefindsthat
t+1
S (t)= , b=coth1/ξ. (213)
K (cid:112)
bt+ 1+(b2−1)t2
J FURTHER EXPERIMENTS
J.1 EXPONENTIALCORRELATIONS
Resolutions, N=100, Isotropic Data Resolutions, N=100, Isotropic Data
=1e-03, =1e-02 Exponential =1e-04, =1e+02 Exponential
101 101
100 100
101
101
102
GCV
102
GCV 103
103 104
GCV
104 105 GCV
101 102 103 101 102 103
T T
Resolutions N=100, =1.8 Resolutions N=100, =1.8
=1e-06, =1e-02 Exponential =1e-06, =1e+02 Exponential
101
100 101
101
102
102
T T
103 GCV
103
GCV
104 GCV 104 GCV
105
105
106
101 102 103 101 102 103
T T
Figure10: Plotsoftheresolutionsκ,κ˜underexponentialcorrelations. Top: IsotropicData. Bottom:
Anisotropic power law data with the eigenvalues of Σ having power law decay k−α, α = 1.8.
Overlaid in dashed is the expected scaling in the overparameterized regime. Left: ξ = 10−2,
essentiallyuncorrelated. Right: ξ =102,stronglycorrelated. WefixN =100andvaryT. Overlaid
indashedisthescalingexpectedintheoverparameterizedregimewhenα>1.
Inthissectionwestudythepredictionsofthetheoryforvariousrelevantquantitiesinthecaseof
bothstrongandweakexponentialcorrelations. Plotsoftheerrorcurvesinthissettingarenumerous
inthemaintext. Wetestthisacrossbothisotropicandstructureddata.
InFigure10,weplotseveraldifferentcurvesforκ,κ˜asT variesfromsmalltolargeacrossdifferently
structureddatasets. WethenplotthecorrespondingdegreesoffreedominFigure11. Finally,we
verifythedualityrelationholdsempiricallyinFigure12.
39PublishedasaconferencepaperatICLR2024
Degrees of Freedom, N=100, Isotropic Data Degrees of Freedom, N=100, Isotropic Data
=1e-03, =1e-02 Exponential =1e-04, =1e+02 Exponential
1.0 1.0
0.8 0.8
0.6 d df f1 K
1
0.6 d df f1 K
1
df2 df2
0.4 dfK
2
0.4 dfK
2
0.2 0.2
0.0 0.0
101 102 103 101 102 103
T T
Degrees of Freedom, N=100, =1.8 Degrees of Freedom, N=100, =1.8
=1e-06, =1e-02 Exponential =1e-06, =1e+02 Exponential
1.0 1.0
0.8 0.8
df1 df1
0.6 dfK
1
0.6 dfK
1
df2 df2
0.4 dfK
2
0.4 dfK
2
0.2 0.2
0.0 0.0
101 102 103 101 102 103
T T
Figure11: Plotsofthethedegreesoffreedomdf ,d˜f ,df ,d˜f underexponentialcorrelationsas
1 1 2 2
inFigure10. Solidlinesaretheory,dotswitherrorbarsareempirics. Thelackofsubstantialerror
barsstemsfromthefactthatallquantitiesconcentrate. Wefindthatalthoughdf canbenumerically
2
sensitive,d˜f remainsnumericallypreciseeveninthepresenceofstrongcorrelations.
2
Checking Duality Relation, N=100, Isotropic Data Checking Duality Relation, N=100, Isotropic Data
=1e-03, =1e-02 Exponential =1e-04, =1e+02 Exponential
101 theory 101 theory
/ /
1/df1K 1/df1K
100
100
101 100 101 101 100 101
q q
Checking Duality Relation, N=100, =1.8 Checking Duality Relation, N=100, =1.8
=1e-06, =1e-02 Exponential =1e-06, =1e+02 Exponential
101 theory 101 theory
/ /
1/df1K 1/df1K
100 100
101 100 101 101 100 101
q q
Figure12: Plotsverifyingthedualityrelation κκ˜ = 1 holdforthesettingsinFigures10and11
λ d˜f1
underexponentialcorrelations. Dashedblacklineslinesaretheory,dotswitherrorbarsareempirics.
Thelackofsubstantialerrorbarsstemsfromthefactthatallquantitiesconcentrate.
40PublishedasaconferencepaperatICLR2024
J.2 NEARESTNEIGHBORCORRELATIONS
Nearest-Neighbor Correlations, N=100, Isotropic Data Nearest-Neighbor Correlations N=100 =1.8, r=0.3
b=0.80, =1e-04, =0.5 b=0.95, =1e-04, =0.05
101
101
102
R R
R R
Bias2 Bias2
100 V Va ar rX
X,
103 V Va ar rX
X,
Naive GCV1 Naive GCV1
Naive GCV2 Naive GCV2
101 CorrGCV 104 CorrGCV
102 105
101 102 103 101 102 103
T T
Figure13: Risks,SourcesofVariance,andRiskEstimators,similartoFigure1,butwithnearest
neighborcorrelations. a)Isotropiccovariates. b)Structuredcovariatesundersourceandcapacity
conditions.
TheS-transformforacorrelationmatrixK whereonlynearestneighbordatapointsarecorrelated
canbestraightforwardlyobtained. WedosoinSectionI.1. Ingeneral,theeffectofcorrelationsis
quiteweakunlesstheoff-diagonalcorrelationelementb/2hasbcloseto1. InFigure13,weplotthe
riskcurvesandsourcesofvarianceinboththesettingofisotropicandanisotropicdata. Ingeneral,
allestimatorstendtoagreebetterforthechoicesofblistedcomparedtothestronglyexponentially
correlateddata. However,onlytheCorrGCVconsistentlycorrectlyestimatestheout-of-samplerisk
acrossT,b,λ,andσ . EspeciallyatsmallvalueofT,weseethatonlytheCorrGCVestimatoristhe
ϵ
mostreliable.
WefurtherplottheresolutionsinFigure14anddegreesoffreedominFigure15acrosstwodifferent
valuesofb=0.5,0.95inboththeisotropicandanisotropiccase.
Resolutions, N=100, Isotropic Data Resolutions, N=100, Isotropic Data
=1e-03, b=0.50 Nearest-Neighbor =1e-03, b=0.95 Nearest-Neighbor
101 101
100 100
101 101
GCV GCV
102
GCV
102
GCV
103 103
104 104
101 102 103 101 102 103
T T
Resolutions N=100, =1.2 Resolutions N=100, =1.2
=1e-05, b=0.50 Nearest-Neighbor =1e-05, b=0.95 Nearest-Neighbor
101 101
100 100
101 101
102 T 102 T
GCV GCV
103 GCV 103 GCV
104 104
105 105
101 102 103 101 102 103
T T
Figure 14: Plots of the resolution κ,κ˜ under nearest neighbor correlations. Top: Isotropic Data.
Bottom:AnisotropicpowerlawdatawiththeeigenvaluesofΣhavingpowerlawdecayk−α,α=1.2.
Left: b=0.5,weaklycorrelated. Right: b=0.95,stronglycorrelated. WefixN =100andvaryT.
Solidlinesaretheorydotswitherrorbarsareempirics. Overlaidindashedistheexpectedscalingin
theoverparameterizedregimewhenα>1.
41
rorrE rorrEPublishedasaconferencepaperatICLR2024
Degrees of Freedom, N=100, Isotropic Data Degrees of Freedom, N=100, Isotropic Data
=1e-03, b=0.50 Nearest-Neighbor =1e-03, b=0.95 Nearest-Neighbor
1.0 1.0
0.8 0.8
0.6 0.6
0.4
0.4
0.2
df1 0.2 df1
0.0 dfK 1 dfK 1
0.2 d df f2 K
2
0 0. .0
2
d df f2 K
2
101 102 103 101 102 103
T T
Degrees of Freedom, N=100, =1.2 Degrees of Freedom, N=100, =1.2
=1e-05, b=0.50 Nearest-Neighbor =1e-05, b=0.95 Nearest-Neighbor
1.0
1.0
0.8 0.8
df1 df1
0.6 dfK
1
0.6 dfK
1
0.4 d df f2 K
2
0.4 d df f2 K
2
0.2 0.2
0.0
0.0
101 102 103 101 102 103
T T
Figure15: Plotsofthethedegreesoffreedomdf ,d˜f ,df ,d˜f undernearest-neighborcorrelations
1 1 2 2
asinFigure14Solidlinesaretheory,dotswitherrorbarsareempirics. Thelackofsubstantialerror
barsstemsfromthefactthatallquantitiesconcentrate. Wefindthatalthoughdf canbenumerically
2
sensitive,d˜f remainsnumericallypreciseeveninthepresenceofstrongcorrelations.
2
J.3 POWERLAWCORRELATIONS
Forpowerlawcorrelations,wedonothaveanexplicitanalyticformulaforS . Weinsteadestimate
K
itbyinterpolatingdf asafunctionofλandesimatingthefunctionalinversedf−1,asdiscussedin
K K
SectionA.2. WeensureourinterpolatoriscompatiblewithautogradsothatwecanrunAlgorithm1
togetestimatorsfordf ,d˜f andthusfortheCorrGCV.
2 2
Weshowtwoexampleswithpower-lawcorrelationsinFigure16. Wefurtherplottheresolutionsin
Figure17anddegreesoffreedominFigure18acrosstwodifferentvaluesofχ=1.5,0.1inboththe
isotropicandanisotropiccase.
Power Law Correlations, N=100, Isotropic Data Power Law Correlations N=100 =1.8, r=0.3
=1e-02, =0.7, =0.5 =1e-02, =0.7, =0.05
101 102
100 R 103 R
R R
101 B Vaia rs X2 104 B Vaia rs X2
VarX, VarX,
102 N Na ai iv ve e G GC CV V1 2 105 N Na ai iv ve e G GC CV V1 2
CorrGCV CorrGCV
106
103
107
104
101 102 103 101 102 103
T T
Figure 16: Risks, Sources of Variance, and Risk Estimators, similar to Figure 1, but with power
lawcorrelationsE[x ·x ] ∝ (1+|t−s|)−χ withexponentχ = 0.7. a)Isotropiccovariates. b)
t s
Structuredcovariatesundersourceandcapacityconditions.
42
rorrE rorrEPublishedasaconferencepaperatICLR2024
Resolutions, N=100, Isotropic Data Resolutions, N=100, Isotropic Data
=1e-03, =1.5 Power Law =1e-03, =0.1 Power Law
101 101
100 100
101 101
GCV GCV
102
GCV
102
GCV
103 103
104 104
101 102 103 101 102 103
T T
Resolutions N=100, =1.2 Resolutions N=100, =1.2
=1e-05, =1.5 Power Law =1e-05, =0.1 Power Law
101
100
100
101 101
102 T 102 T
GCV GCV
103 GCV 103 GCV
104
104
105
101 102 103 101 102 103
T T
Figure17: Plotsoftheresolutionκ,κ˜underpowerlawcorrelations. Top: IsotropicData. Bottom:
Anisotropic power law data with the eigenvalues of Σ having power law decay k−α, α = 1.2.
Overlaidindashedistheexpectedscalingintheoverparameterizedregime. Left: χ=1.5,weakly
correlated. Right: χ=0.1,stronglycorrelated. WefixN =100andvaryT. Forstronglycorrelated
data, we see a slight deviation between κ in theory and empirics. This is due to the low-degree
polynomialinterpolationestimatorofS (qdf )performingmorepoorlynearqdf =1. Thiscanbe
K 1 1
resolvedwithamoreflexibleinterpolant.
Degrees of Freedom, N=100, Isotropic Data Degrees of Freedom, N=100, Isotropic Data
=1e-03, =1.5 Power Law =1e-03, =0.1 Power Law
1.0 1.0
0.8 0.8
df1 df1
0.6 dfK
1
0.6 dfK
1
0.4 d df f2 K
2
0.4 d df f2 K
2
0.2 0.2
0.0 0.0
101 102 103 101 102 103
T T
Degrees of Freedom, N=100, =1.2 Degrees of Freedom, N=100, =1.2
=1e-05, =1.5 Power Law =1e-05, =0.1 Power Law
1.0 1.0
0.8 0.8
df1 df1
0.6 dfK
1
0.6 dfK
1
0.4 d df f2 K
2
0.4 d df f2 K
2
0.2 0.2
0.0 0.0
101 102 103 101 102 103
T T
Figure18: Plotsofthethedegreesoffreedomdf ,d˜f ,df ,d˜f underpowerlawcorrelationsas
1 1 2 2
inFigure17. Solidlinesaretheory,dotswitherrorbarsareempirics. Thelackofsubstantialerror
barsstemsfromthefactthatallquantitiesconcentrate. Wefindthatalthoughdf canbenumerically
2
sensitive,d˜f remainsnumericallypreciseeveninthepresenceofstrongcorrelations.
2
43PublishedasaconferencepaperatICLR2024
K EXPERIMENTAL DETAILS
Error bars are always reported over ten runs of the same regression over different datasets. We
ensembledoverthesedatasetstocalculatetheVar termempirically. Wealsoheldoutatargetfree
X
oflabelnoisetocalculatetheVar componentofthevarianceempirically. Weplantomakeall
Xϵ
codepubliclyavailabletoencouragereproducibilityandextensionsofourresults.
WeusedJAX(Bradburyetal.,2018)toperformallthelinearalgebraicmanipulations.Allexperiments
weredoneprimarilyonaCPUwithverylittlecomputerequired. Wealsotestedaspeedupofrunning
ourcodeonaGPU,amountingtolessthan1GPU-dayofcomputeusage.
44