[
    {
        "title": "LiDAR-Event Stereo Fusion with Hallucinations",
        "authors": "Luca BartolomeiMatteo PoggiAndrea ContiStefano Mattoccia",
        "links": "http://arxiv.org/abs/2408.04633v1",
        "entry_id": "http://arxiv.org/abs/2408.04633v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04633v1",
        "summary": "Event stereo matching is an emerging technique to estimate depth from\nneuromorphic cameras; however, events are unlikely to trigger in the absence of\nmotion or the presence of large, untextured regions, making the correspondence\nproblem extremely challenging. Purposely, we propose integrating a stereo event\ncamera with a fixed-frequency active sensor -- e.g., a LiDAR -- collecting\nsparse depth measurements, overcoming the aforementioned limitations. Such\ndepth hints are used by hallucinating -- i.e., inserting fictitious events --\nthe stacks or raw input streams, compensating for the lack of information in\nthe absence of brightness changes. Our techniques are general, can be adapted\nto any structured representation to stack events and outperform\nstate-of-the-art fusion methods applied to event-based stereo.",
        "updated": "2024-08-08 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04633v1"
    },
    {
        "title": "Arctic-TILT. Business Document Understanding at Sub-Billion Scale",
        "authors": "Łukasz BorchmannMichał PietruszkaWojciech JaśkowskiDawid JurkiewiczPiotr HalamaPaweł JóziakŁukasz GarncarekPaweł LiskowskiKarolina SzyndlerAndrzej GretkowskiJulita OłtusekGabriela NowakowskaArtur ZawłockiŁukasz DuhrPaweł DydaMichał Turski",
        "links": "http://arxiv.org/abs/2408.04632v1",
        "entry_id": "http://arxiv.org/abs/2408.04632v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04632v1",
        "summary": "The vast portion of workloads employing LLMs involves answering questions\ngrounded on PDF or scan content. We introduce the Arctic-TILT achieving\naccuracy on par with models 1000$\\times$ its size on these use cases. It can be\nfine-tuned and deployed on a single 24GB GPU, lowering operational costs while\nprocessing Visually Rich Documents with up to 400k tokens. The model\nestablishes state-of-the-art results on seven diverse Document Understanding\nbenchmarks, as well as provides reliable confidence scores and quick inference,\nwhich are essential for processing files in large-scale or time-sensitive\nenterprise environments.",
        "updated": "2024-08-08 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04632v1"
    },
    {
        "title": "Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics",
        "authors": "Ruining LiChuanxia ZhengChristian RupprechtAndrea Vedaldi",
        "links": "http://arxiv.org/abs/2408.04631v1",
        "entry_id": "http://arxiv.org/abs/2408.04631v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04631v1",
        "summary": "We present Puppet-Master, an interactive video generative model that can\nserve as a motion prior for part-level dynamics. At test time, given a single\nimage and a sparse set of motion trajectories (i.e., drags), Puppet-Master can\nsynthesize a video depicting realistic part-level motion faithful to the given\ndrag interactions. This is achieved by fine-tuning a large-scale pre-trained\nvideo diffusion model, for which we propose a new conditioning architecture to\ninject the dragging control effectively. More importantly, we introduce the\nall-to-first attention mechanism, a drop-in replacement for the widely adopted\nspatial attention modules, which significantly improves generation quality by\naddressing the appearance and background issues in existing models. Unlike\nother motion-conditioned video generators that are trained on in-the-wild\nvideos and mostly move an entire object, Puppet-Master is learned from\nObjaverse-Animation-HQ, a new dataset of curated part-level motion clips. We\npropose a strategy to automatically filter out sub-optimal animations and\naugment the synthetic renderings with meaningful motion trajectories.\nPuppet-Master generalizes well to real images across various categories and\noutperforms existing methods in a zero-shot manner on a real-world benchmark.\nSee our project page for more results: vgg-puppetmaster.github.io.",
        "updated": "2024-08-08 17:59:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04631v1"
    },
    {
        "title": "LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP",
        "authors": "Danlu ChenFreda ShiAditi AgarwalJacobo MyerstonTaylor Berg-Kirkpatrick",
        "links": "http://arxiv.org/abs/2408.04628v1",
        "entry_id": "http://arxiv.org/abs/2408.04628v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04628v1",
        "summary": "Standard natural language processing (NLP) pipelines operate on symbolic\nrepresentations of language, which typically consist of sequences of discrete\ntokens. However, creating an analogous representation for ancient logographic\nwriting systems is an extremely labor intensive process that requires expert\nknowledge. At present, a large portion of logographic data persists in a purely\nvisual form due to the absence of transcription -- this issue poses a\nbottleneck for researchers seeking to apply NLP toolkits to study ancient\nlogographic languages: most of the relevant data are images of writing.\n  This paper investigates whether direct processing of visual representations\nof language offers a potential solution. We introduce LogogramNLP, the first\nbenchmark enabling NLP analysis of ancient logographic languages, featuring\nboth transcribed and visual datasets for four writing systems along with\nannotations for tasks like classification, translation, and parsing. Our\nexperiments compare systems that employ recent visual and text encoding\nstrategies as backbones. The results demonstrate that visual representations\noutperform textual representations for some investigated tasks, suggesting that\nvisual processing pipelines may unlock a large amount of cultural heritage data\nof logographic languages for NLP-based analyses.",
        "updated": "2024-08-08 17:58:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04628v1"
    },
    {
        "title": "Quantifying the Impact of Population Shift Across Age and Sex for Abdominal Organ Segmentation",
        "authors": "Kate ČevoraBen GlockerWenjia Bai",
        "links": "http://arxiv.org/abs/2408.04610v1",
        "entry_id": "http://arxiv.org/abs/2408.04610v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04610v1",
        "summary": "Deep learning-based medical image segmentation has seen tremendous progress\nover the last decade, but there is still relatively little transfer into\nclinical practice. One of the main barriers is the challenge of domain\ngeneralisation, which requires segmentation models to maintain high performance\nacross a wide distribution of image data. This challenge is amplified by the\nmany factors that contribute to the diverse appearance of medical images, such\nas acquisition conditions and patient characteristics. The impact of shifting\npatient characteristics such as age and sex on segmentation performance remains\nrelatively under-studied, especially for abdominal organs, despite that this is\ncrucial for ensuring the fairness of the segmentation model. We perform the\nfirst study to determine the impact of population shift with respect to age and\nsex on abdominal CT image segmentation, by leveraging two large public\ndatasets, and introduce a novel metric to quantify the impact. We find that\npopulation shift is a challenge similar in magnitude to cross-dataset shift for\nabdominal organ segmentation, and that the effect is asymmetric and\ndataset-dependent. We conclude that dataset diversity in terms of known patient\ncharacteristics is not necessarily equivalent to dataset diversity in terms of\nimage features. This implies that simple population matching to ensure good\ngeneralisation and fairness may be insufficient, and we recommend that fairness\nresearch should be directed towards better understanding and quantifying\nmedical image dataset diversity in terms of performance-relevant\ncharacteristics such as organ morphology.",
        "updated": "2024-08-08 17:28:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04610v1"
    }
]