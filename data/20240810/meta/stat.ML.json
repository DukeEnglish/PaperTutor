[
    {
        "title": "Risk and cross validation in ridge regression with correlated samples",
        "authors": "Alexander AtanasovJacob A. Zavatone-VethCengiz Pehlevan",
        "links": "http://arxiv.org/abs/2408.04607v1",
        "entry_id": "http://arxiv.org/abs/2408.04607v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04607v1",
        "summary": "Recent years have seen substantial advances in our understanding of\nhigh-dimensional ridge regression, but existing theories assume that training\nexamples are independent. By leveraging recent techniques from random matrix\ntheory and free probability, we provide sharp asymptotics for the in- and\nout-of-sample risks of ridge regression when the data points have arbitrary\ncorrelations. We demonstrate that in this setting, the generalized cross\nvalidation estimator (GCV) fails to correctly predict the out-of-sample risk.\nHowever, in the case where the noise residuals have the same correlations as\nthe data points, one can modify the GCV to yield an efficiently-computable\nunbiased estimator that concentrates in the high-dimensional limit, which we\ndub CorrGCV. We further extend our asymptotic analysis to the case where the\ntest point has nontrivial correlations with the training set, a setting often\nencountered in time series forecasting. Assuming knowledge of the correlation\nstructure of the time series, this again yields an extension of the GCV\nestimator, and sharply characterizes the degree to which such test points yield\nan overly optimistic prediction of long-time risk. We validate the predictions\nof our theory across a variety of high dimensional data.",
        "updated": "2024-08-08 17:27:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04607v1"
    },
    {
        "title": "Inference with the Upper Confidence Bound Algorithm",
        "authors": "Koulik KhamaruCun-Hui Zhang",
        "links": "http://arxiv.org/abs/2408.04595v1",
        "entry_id": "http://arxiv.org/abs/2408.04595v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04595v1",
        "summary": "In this paper, we discuss the asymptotic behavior of the Upper Confidence\nBound (UCB) algorithm in the context of multiarmed bandit problems and discuss\nits implication in downstream inferential tasks. While inferential tasks become\nchallenging when data is collected in a sequential manner, we argue that this\nproblem can be alleviated when the sequential algorithm at hand satisfies\ncertain stability property. This notion of stability is motivated from the\nseminal work of Lai and Wei (1982). Our first main result shows that such a\nstability property is always satisfied for the UCB algorithm, and as a result\nthe sample means for each arm are asymptotically normal. Next, we examine the\nstability properties of the UCB algorithm when the number of arms $K$ is\nallowed to grow with the number of arm pulls $T$. We show that in such a case\nthe arms are stable when $\\frac{\\log K}{\\log T} \\rightarrow 0$, and the number\nof near-optimal arms are large.",
        "updated": "2024-08-08 17:11:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04595v1"
    },
    {
        "title": "Activation thresholds and expressiveness of polynomial neural networks",
        "authors": "Bella FinkelJose Israel RodriguezChenxi WuThomas Yahl",
        "links": "http://arxiv.org/abs/2408.04569v1",
        "entry_id": "http://arxiv.org/abs/2408.04569v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04569v1",
        "summary": "Polynomial neural networks have been implemented in a range of applications\nand present an advantageous framework for theoretical machine learning. A\npolynomial neural network of fixed architecture and activation degree gives an\nalgebraic map from the network's weights to a set of polynomials. The image of\nthis map is the space of functions representable by the network. Its Zariski\nclosure is an affine variety known as a neurovariety. The dimension of a\npolynomial neural network's neurovariety provides a measure of its\nexpressivity. In this work, we introduce the notion of the activation threshold\nof a network architecture which expresses when the dimension of a neurovariety\nachieves its theoretical maximum. In addition, we prove expressiveness results\nfor polynomial neural networks with equi-width~architectures.",
        "updated": "2024-08-08 16:28:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04569v1"
    },
    {
        "title": "Hybrid Reinforcement Learning Breaks Sample Size Barriers in Linear MDPs",
        "authors": "Kevin TanWei FanYuting Wei",
        "links": "http://arxiv.org/abs/2408.04526v1",
        "entry_id": "http://arxiv.org/abs/2408.04526v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04526v1",
        "summary": "Hybrid Reinforcement Learning (RL), where an agent learns from both an\noffline dataset and online explorations in an unknown environment, has garnered\nsignificant recent interest. A crucial question posed by Xie et al. (2022) is\nwhether hybrid RL can improve upon the existing lower bounds established in\npurely offline and purely online RL without relying on the single-policy\nconcentrability assumption. While Li et al. (2023) provided an affirmative\nanswer to this question in the tabular PAC RL case, the question remains\nunsettled for both the regret-minimizing RL case and the non-tabular case.\n  In this work, building upon recent advancements in offline RL and\nreward-agnostic exploration, we develop computationally efficient algorithms\nfor both PAC and regret-minimizing RL with linear function approximation,\nwithout single-policy concentrability. We demonstrate that these algorithms\nachieve sharper error or regret bounds that are no worse than, and can improve\non, the optimal sample complexity in offline RL (the first algorithm, for PAC\nRL) and online RL (the second algorithm, for regret-minimizing RL) in linear\nMarkov decision processes (MDPs), regardless of the quality of the behavior\npolicy. To our knowledge, this work establishes the tightest theoretical\nguarantees currently available for hybrid RL in linear MDPs.",
        "updated": "2024-08-08 15:26:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04526v1"
    },
    {
        "title": "Robustness investigation of quality measures for the assessment of machine learning models",
        "authors": "Thomas MostLars GräningSebastian Wolff",
        "links": "http://arxiv.org/abs/2408.04391v1",
        "entry_id": "http://arxiv.org/abs/2408.04391v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04391v1",
        "summary": "In this paper the accuracy and robustness of quality measures for the\nassessment of machine learning models are investigated. The prediction quality\nof a machine learning model is evaluated model-independent based on a\ncross-validation approach, where the approximation error is estimated for\nunknown data. The presented measures quantify the amount of explained variation\nin the model prediction. The reliability of these measures is assessed by means\nof several numerical examples, where an additional data set for the\nverification of the estimated prediction error is available. Furthermore, the\nconfidence bounds of the presented quality measures are estimated and local\nquality measures are derived from the prediction residuals obtained by the\ncross-validation approach.",
        "updated": "2024-08-08 11:51:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04391v1"
    }
]