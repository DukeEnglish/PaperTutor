[
    {
        "title": "Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification",
        "authors": "Kaijie RenLei Zhang",
        "links": "http://arxiv.org/abs/2403.11708v1",
        "entry_id": "http://arxiv.org/abs/2403.11708v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11708v1",
        "summary": "Visible-Infrared Person Re-identification (VI-ReID) is a challenging\ncross-modal pedestrian retrieval task, due to significant intra-class\nvariations and cross-modal discrepancies among different cameras. Existing\nworks mainly focus on embedding images of different modalities into a unified\nspace to mine modality-shared features. They only seek distinctive information\nwithin these shared features, while ignoring the identity-aware useful\ninformation that is implicit in the modality-specific features. To address this\nissue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)\nnetwork to uncover and leverage the implicit discriminative information\ncontained within the modality-specific. First, we extract modality-specific and\nmodality-shared features using a novel dual-stream network. Then, the\nmodality-specific features undergo purification to reduce their modality style\ndiscrepancies while preserving identity-aware discriminative knowledge.\nSubsequently, this kind of implicit knowledge is distilled into the\nmodality-shared feature to enhance its distinctiveness. Finally, an alignment\nloss is proposed to minimize modality discrepancy on enhanced modality-shared\nfeatures. Extensive experiments on multiple public datasets demonstrate the\nsuperiority of IDKL network over the state-of-the-art methods. Code is\navailable at https://github.com/1KK077/IDKL.",
        "updated": "2024-03-18 12:12:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11708v1"
    },
    {
        "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
        "authors": "Ruyi XuYuan YaoZonghao GuoJunbo CuiZanlin NiChunjiang GeTat-Seng ChuaZhiyuan LiuMaosong SunGao Huang",
        "links": "http://arxiv.org/abs/2403.11703v1",
        "entry_id": "http://arxiv.org/abs/2403.11703v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11703v1",
        "summary": "Visual encoding constitutes the basis of large multimodal models (LMMs) in\nunderstanding the visual world. Conventional LMMs process images in fixed sizes\nand limited resolutions, while recent explorations in this direction are\nlimited in adaptivity, efficiency, and even correctness. In this work, we first\ntake GPT-4V and LLaVA-1.5 as representative examples and expose systematic\nflaws rooted in their visual encoding strategy. To address the challenges, we\npresent LLaVA-UHD, a large multimodal model that can efficiently perceive\nimages in any aspect ratio and high resolution. LLaVA-UHD includes three key\ncomponents: (1) An image modularization strategy that divides native-resolution\nimages into smaller variable-sized slices for efficient and extensible\nencoding, (2) a compression module that further condenses image tokens from\nvisual encoders, and (3) a spatial schema to organize slice tokens for LLMs.\nComprehensive experiments show that LLaVA-UHD outperforms established LMMs\ntrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our\nmodel built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)\nresolution images using only 94% inference computation, and achieves 6.4\naccuracy improvement on TextVQA. Moreover, the model can be efficiently trained\nin academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of\nLLaVA-1.5). We make the data and code publicly available at\nhttps://github.com/thunlp/LLaVA-UHD.",
        "updated": "2024-03-18 12:04:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11703v1"
    },
    {
        "title": "A Spatial-Temporal Progressive Fusion Network for Breast Lesion Segmentation in Ultrasound Videos",
        "authors": "Zhengzheng TuZigang ZhuYayang DuanBo JiangQishun WangChaoxue Zhang",
        "links": "http://arxiv.org/abs/2403.11699v1",
        "entry_id": "http://arxiv.org/abs/2403.11699v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11699v1",
        "summary": "Ultrasound video-based breast lesion segmentation provides a valuable\nassistance in early breast lesion detection and treatment. However, existing\nworks mainly focus on lesion segmentation based on ultrasound breast images\nwhich usually can not be adapted well to obtain desirable results on ultrasound\nvideos. The main challenge for ultrasound video-based breast lesion\nsegmentation is how to exploit the lesion cues of both intra-frame and\ninter-frame simultaneously. To address this problem, we propose a novel\nSpatial-Temporal Progressive Fusion Network (STPFNet) for video based breast\nlesion segmentation problem. The main aspects of the proposed STPFNet are\nthreefold. First, we propose to adopt a unified network architecture to capture\nboth spatial dependences within each ultrasound frame and temporal correlations\nbetween different frames together for ultrasound data representation. Second,\nwe propose a new fusion module, termed Multi-Scale Feature Fusion (MSFF), to\nfuse spatial and temporal cues together for lesion detection. MSFF can help to\ndetermine the boundary contour of lesion region to overcome the issue of lesion\nboundary blurring. Third, we propose to exploit the segmentation result of\nprevious frame as the prior knowledge to suppress the noisy background and\nlearn more robust representation. In particular, we introduce a new publicly\navailable ultrasound video breast lesion segmentation dataset, termed UVBLS200,\nwhich is specifically dedicated to breast lesion segmentation. It contains 200\nvideos, including 80 videos of benign lesions and 120 videos of malignant\nlesions. Experiments on the proposed dataset demonstrate that the proposed\nSTPFNet achieves better breast lesion detection performance than\nstate-of-the-art methods.",
        "updated": "2024-03-18 11:56:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11699v1"
    },
    {
        "title": "Urban Scene Diffusion through Semantic Occupancy Map",
        "authors": "Junge ZhangQihang ZhangLi ZhangRamana Rao KompellaGaowen LiuBolei Zhou",
        "links": "http://arxiv.org/abs/2403.11697v1",
        "entry_id": "http://arxiv.org/abs/2403.11697v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11697v1",
        "summary": "Generating unbounded 3D scenes is crucial for large-scale scene understanding\nand simulation. Urban scenes, unlike natural landscapes, consist of various\ncomplex man-made objects and structures such as roads, traffic signs, vehicles,\nand buildings. To create a realistic and detailed urban scene, it is crucial to\naccurately represent the geometry and semantics of the underlying objects,\ngoing beyond their visual appearance. In this work, we propose UrbanDiffusion,\na 3D diffusion model that is conditioned on a Bird's-Eye View (BEV) map and\ngenerates an urban scene with geometry and semantics in the form of semantic\noccupancy map. Our model introduces a novel paradigm that learns the data\ndistribution of scene-level structures within a latent space and further\nenables the expansion of the synthesized scene into an arbitrary scale. After\ntraining on real-world driving datasets, our model can generate a wide range of\ndiverse urban scenes given the BEV maps from the held-out set and also\ngeneralize to the synthesized maps from a driving simulator. We further\ndemonstrate its application to scene image synthesis with a pretrained image\ngenerator as a prior.",
        "updated": "2024-03-18 11:54:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11697v1"
    },
    {
        "title": "TrajectoryNAS: A Neural Architecture Search for Trajectory Prediction",
        "authors": "Ali Asghar SharifiAli ZoljodiMasoud Daneshtalab",
        "links": "http://arxiv.org/abs/2403.11695v1",
        "entry_id": "http://arxiv.org/abs/2403.11695v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11695v1",
        "summary": "Autonomous driving systems are a rapidly evolving technology that enables\ndriverless car production. Trajectory prediction is a critical component of\nautonomous driving systems, enabling cars to anticipate the movements of\nsurrounding objects for safe navigation. Trajectory prediction using Lidar\npoint-cloud data performs better than 2D images due to providing 3D\ninformation. However, processing point-cloud data is more complicated and\ntime-consuming than 2D images. Hence, state-of-the-art 3D trajectory\npredictions using point-cloud data suffer from slow and erroneous predictions.\nThis paper introduces TrajectoryNAS, a pioneering method that focuses on\nutilizing point cloud data for trajectory prediction. By leveraging Neural\nArchitecture Search (NAS), TrajectoryNAS automates the design of trajectory\nprediction models, encompassing object detection, tracking, and forecasting in\na cohesive manner. This approach not only addresses the complex\ninterdependencies among these tasks but also emphasizes the importance of\naccuracy and efficiency in trajectory modeling. Through empirical studies,\nTrajectoryNAS demonstrates its effectiveness in enhancing the performance of\nautonomous driving systems, marking a significant advancement in the\nfield.Experimental results reveal that TrajcetoryNAS yield a minimum of 4.8\nhigger accuracy and 1.1* lower latency over competing methods on the NuScenes\ndataset.",
        "updated": "2024-03-18 11:48:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11695v1"
    }
]