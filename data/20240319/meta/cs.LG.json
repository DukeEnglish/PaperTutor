[
    {
        "title": "Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation",
        "authors": "Johannes PöppelbaumAndreas Schwung",
        "links": "http://arxiv.org/abs/2403.11722v1",
        "entry_id": "http://arxiv.org/abs/2403.11722v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11722v1",
        "summary": "We propose a novel quaternionic time-series compression methodology where we\ndivide a long time-series into segments of data, extract the min, max, mean and\nstandard deviation of these chunks as representative features and encapsulate\nthem in a quaternion, yielding a quaternion valued time-series. This\ntime-series is processed using quaternion valued neural network layers, where\nwe aim to preserve the relation between these features through the usage of the\nHamilton product. To train this quaternion neural network, we derive quaternion\nbackpropagation employing the GHR calculus, which is required for a valid\nproduct and chain rule in quaternion space. Furthermore, we investigate the\nconnection between the derived update rules and automatic differentiation. We\napply our proposed compression method on the Tennessee Eastman Dataset, where\nwe perform fault classification using the compressed data in two settings: a\nfully supervised one and in a semi supervised, contrastive learning setting.\nBoth times, we were able to outperform real valued counterparts as well as two\nbaseline models: one with the uncompressed time-series as the input and the\nother with a regular downsampling using the mean. Further, we could improve the\nclassification benchmark set by SimCLR-TS from 81.43% to 83.90%.",
        "updated": "2024-03-18 12:22:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11722v1"
    },
    {
        "title": "Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models",
        "authors": "Emilian PostolacheGiorgio MarianiLuca CosmoEmmanouil BenetosEmanuele Rodolà",
        "links": "http://arxiv.org/abs/2403.11706v1",
        "entry_id": "http://arxiv.org/abs/2403.11706v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11706v1",
        "summary": "Multi-Source Diffusion Models (MSDM) allow for compositional musical\ngeneration tasks: generating a set of coherent sources, creating\naccompaniments, and performing source separation. Despite their versatility,\nthey require estimating the joint distribution over the sources, necessitating\npre-separated musical data, which is rarely available, and fixing the number\nand type of sources at training time. This paper generalizes MSDM to arbitrary\ntime-domain diffusion models conditioned on text embeddings. These models do\nnot require separated data as they are trained on mixtures, can parameterize an\narbitrary number of sources, and allow for rich semantic control. We propose an\ninference procedure enabling the coherent generation of sources and\naccompaniments. Additionally, we adapt the Dirac separator of MSDM to perform\nsource separation. We experiment with diffusion models trained on Slakh2100 and\nMTG-Jamendo, showcasing competitive generation and separation results in a\nrelaxed data setting.",
        "updated": "2024-03-18 12:08:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11706v1"
    },
    {
        "title": "Coarsening of chiral domains in itinerant electron magnets: A machine learning force field approach",
        "authors": "Yunhao FanSheng ZhangGia-Wei Chern",
        "links": "http://arxiv.org/abs/2403.11705v1",
        "entry_id": "http://arxiv.org/abs/2403.11705v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11705v1",
        "summary": "Frustrated itinerant magnets often exhibit complex noncollinear or\nnoncoplanar magnetic orders which support topological electronic structures. A\ncanonical example is the anomalous quantum Hall state with a chiral spin order\nstabilized by electron-spin interactions on a triangular lattice. While a\nlong-range magnetic order cannot survive thermal fluctuations in two\ndimensions, the chiral order which results from the breaking of a discrete\nIsing symmetry persists even at finite temperatures. We present a scalable\nmachine learning (ML) framework to model the complex electron-mediated\nspin-spin interactions that stabilize the chiral magnetic domains in a\ntriangular lattice. Large-scale dynamical simulations, enabled by the ML\nforce-field models, are performed to investigate the coarsening of chiral\ndomains after a thermal quench. While the chiral phase is described by a broken\n$Z_2$ Ising-type symmetry, we find that the characteristic size of chiral\ndomains increases linearly with time, in stark contrast to the expected\nAllen-Cahn domain growth law for a non-conserved Ising order parameter field.\nThe linear growth of the chiral domains is attributed to the orientational\nanisotropy of domain boundaries. Our work also demonstrates the promising\npotential of ML models for large-scale spin dynamics of itinerant magnets.",
        "updated": "2024-03-18 12:07:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11705v1"
    },
    {
        "title": "Generalization error of spectral algorithms",
        "authors": "Maksim VelikanovMaxim PanovDmitry Yarotsky",
        "links": "http://arxiv.org/abs/2403.11696v1",
        "entry_id": "http://arxiv.org/abs/2403.11696v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11696v1",
        "summary": "The asymptotically precise estimation of the generalization of kernel methods\nhas recently received attention due to the parallels between neural networks\nand their associated kernels. However, prior works derive such estimates for\ntraining by kernel ridge regression (KRR), whereas neural networks are\ntypically trained with gradient descent (GD). In the present work, we consider\nthe training of kernels with a family of $\\textit{spectral algorithms}$\nspecified by profile $h(\\lambda)$, and including KRR and GD as special cases.\nThen, we derive the generalization error as a functional of learning profile\n$h(\\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional\ntranslation-invariant model. Under power-law assumptions on the spectrum of the\nkernel and target, we use our framework to (i) give full loss asymptotics for\nboth noisy and noiseless observations (ii) show that the loss localizes on\ncertain spectral scales, giving a new perspective on the KRR saturation\nphenomenon (iii) conjecture, and demonstrate for the considered data models,\nthe universality of the loss w.r.t. non-spectral details of the problem, but\nonly in case of noisy observation.",
        "updated": "2024-03-18 11:52:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11696v1"
    },
    {
        "title": "Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates",
        "authors": "Riccardo GrazziMassimiliano PontilSaverio Salzo",
        "links": "http://arxiv.org/abs/2403.11687v1",
        "entry_id": "http://arxiv.org/abs/2403.11687v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11687v1",
        "summary": "We study the problem of efficiently computing the derivative of the\nfixed-point of a parametric non-differentiable contraction map. This problem\nhas wide applications in machine learning, including hyperparameter\noptimization, meta-learning and data poisoning attacks. We analyze two popular\napproaches: iterative differentiation (ITD) and approximate implicit\ndifferentiation (AID). A key challenge behind the nonsmooth setting is that the\nchain rule does not hold anymore. Building upon the recent work by Bolte et al.\n(2022), who proved the linear convergence of non-differentiable ITD, we provide\nrefined linear convergence rates for both ITD and AID in the deterministic\ncase. We further introduce NSID, a new method to compute the implicit\nderivative when the fixed point is defined as the composition of an outer map\nand an inner map which is accessible only through a stochastic unbiased\nestimator. We establish rates for the convergence of NSID to the true\nderivative, encompassing the best available rates in the smooth setting. We\npresent illustrative experiments confirming our analysis.",
        "updated": "2024-03-18 11:37:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11687v1"
    }
]