[
    {
        "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
        "authors": "Ruyi XuYuan YaoZonghao GuoJunbo CuiZanlin NiChunjiang GeTat-Seng ChuaZhiyuan LiuMaosong SunGao Huang",
        "links": "http://arxiv.org/abs/2403.11703v1",
        "entry_id": "http://arxiv.org/abs/2403.11703v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11703v1",
        "summary": "Visual encoding constitutes the basis of large multimodal models (LMMs) in\nunderstanding the visual world. Conventional LMMs process images in fixed sizes\nand limited resolutions, while recent explorations in this direction are\nlimited in adaptivity, efficiency, and even correctness. In this work, we first\ntake GPT-4V and LLaVA-1.5 as representative examples and expose systematic\nflaws rooted in their visual encoding strategy. To address the challenges, we\npresent LLaVA-UHD, a large multimodal model that can efficiently perceive\nimages in any aspect ratio and high resolution. LLaVA-UHD includes three key\ncomponents: (1) An image modularization strategy that divides native-resolution\nimages into smaller variable-sized slices for efficient and extensible\nencoding, (2) a compression module that further condenses image tokens from\nvisual encoders, and (3) a spatial schema to organize slice tokens for LLMs.\nComprehensive experiments show that LLaVA-UHD outperforms established LMMs\ntrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our\nmodel built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)\nresolution images using only 94% inference computation, and achieves 6.4\naccuracy improvement on TextVQA. Moreover, the model can be efficiently trained\nin academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of\nLLaVA-1.5). We make the data and code publicly available at\nhttps://github.com/thunlp/LLaVA-UHD.",
        "updated": "2024-03-18 12:04:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11703v1"
    },
    {
        "title": "HDLdebugger: Streamlining HDL debugging with Large Language Models",
        "authors": "Xufeng YaoHaoyang LiTsz Ho ChanWenyi XiaoMingxuan YuanYu HuangLei ChenBei Yu",
        "links": "http://arxiv.org/abs/2403.11671v1",
        "entry_id": "http://arxiv.org/abs/2403.11671v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11671v1",
        "summary": "In the domain of chip design, Hardware Description Languages (HDLs) play a\npivotal role. However, due to the complex syntax of HDLs and the limited\navailability of online resources, debugging HDL codes remains a difficult and\ntime-intensive task, even for seasoned engineers. Consequently, there is a\npressing need to develop automated HDL code debugging models, which can\nalleviate the burden on hardware engineers. Despite the strong capabilities of\nLarge Language Models (LLMs) in generating, completing, and debugging software\ncode, their utilization in the specialized field of HDL debugging has been\nlimited and, to date, has not yielded satisfactory results. In this paper, we\npropose an LLM-assisted HDL debugging framework, namely HDLdebugger, which\nconsists of HDL debugging data generation via a reverse engineering approach, a\nsearch engine for retrieval-augmented generation, and a retrieval-augmented LLM\nfine-tuning approach. Through the integration of these components, HDLdebugger\ncan automate and streamline HDL debugging for chip design. Our comprehensive\nexperiments, conducted on an HDL code dataset sourced from Huawei, reveal that\nHDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional\neffectiveness in HDL code debugging.",
        "updated": "2024-03-18 11:19:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11671v1"
    },
    {
        "title": "Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring",
        "authors": "Andrei BuligaChiara Di FrancescomarinoChiara GhidiniIvan DonadelloFabrizio Maria Maggi",
        "links": "http://arxiv.org/abs/2403.11642v1",
        "entry_id": "http://arxiv.org/abs/2403.11642v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11642v1",
        "summary": "Counterfactual explanations suggest what should be different in the input\ninstance to change the outcome of an AI system. When dealing with\ncounterfactual explanations in the field of Predictive Process Monitoring,\nhowever, control flow relationships among events have to be carefully\nconsidered. A counterfactual, indeed, should not violate control flow\nrelationships among activities (temporal background knowledege). Within the\nfield of Explainability in Predictive Process Monitoring, there have been a\nseries of works regarding counterfactual explanations for outcome-based\npredictions. However, none of them consider the inclusion of temporal\nbackground knowledge when generating these counterfactuals. In this work, we\nadapt state-of-the-art techniques for counterfactual generation in the domain\nof XAI that are based on genetic algorithms to consider a series of temporal\nconstraints at runtime. We assume that this temporal background knowledge is\ngiven, and we adapt the fitness function, as well as the crossover and mutation\noperators, to maintain the satisfaction of the constraints. The proposed\nmethods are evaluated with respect to state-of-the-art genetic algorithms for\ncounterfactual generation and the results are presented. We showcase that the\ninclusion of temporal background knowledge allows the generation of\ncounterfactuals more conformant to the temporal background knowledge, without\nhowever losing in terms of the counterfactual traditional quality metrics.",
        "updated": "2024-03-18 10:34:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11642v1"
    },
    {
        "title": "QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation",
        "authors": "Zhizhen ZhouYejing HuoGuoheng HuangAn ZengXuhang ChenLian HuangZinuo Li",
        "links": "http://arxiv.org/abs/2403.11626v1",
        "entry_id": "http://arxiv.org/abs/2403.11626v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11626v1",
        "summary": "The study of music-generated dance is a novel and challenging Image\ngeneration task. It aims to input a piece of music and seed motions, then\ngenerate natural dance movements for the subsequent music. Transformer-based\nmethods face challenges in time series prediction tasks related to human\nmovements and music due to their struggle in capturing the nonlinear\nrelationship and temporal aspects. This can lead to issues like joint\ndeformation, role deviation, floating, and inconsistencies in dance movements\ngenerated in response to the music. In this paper, we propose a\nQuaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a\nquaternion perspective, which consists of a Spin Position Embedding (SPE)\nmodule and a Quaternion Rotary Attention (QRA) module. First, SPE embeds\nposition information into self-attention in a rotational manner, leading to\nbetter learning of features of movement sequences and audio sequences, and\nimproved understanding of the connection between music and dance. Second, QRA\nrepresents and fuses 3D motion features and audio features in the form of a\nseries of quaternions, enabling the model to better learn the temporal\ncoordination of music and dance under the complex temporal cycle conditions of\ndance generation. Finally, we conducted experiments on the dataset AIST++, and\nthe results show that our approach achieves better and more robust performance\nin generating accurate, high-quality dance movements. Our source code and\ndataset can be available from https://github.com/MarasyZZ/QEAN and\nhttps://google.github.io/aistplusplus_dataset respectively.",
        "updated": "2024-03-18 09:58:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11626v1"
    },
    {
        "title": "Optimal Layout Synthesis for Deep Quantum Circuits on NISQ Processors with 100+ Qubits",
        "authors": "Irfansha ShaikJaco van de Pol",
        "links": "http://arxiv.org/abs/2403.11598v1",
        "entry_id": "http://arxiv.org/abs/2403.11598v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11598v1",
        "summary": "Layout synthesis is mapping a quantum circuit to a quantum processor. SWAP\ngate insertions are needed for scheduling 2-qubit gates only on connected\nphysical qubits. With the ever-increasing number of qubits in NISQ processors,\nscalable layout synthesis is of utmost importance. With large optimality gaps\nobserved in heuristic approaches, scalable exact methods are needed. While\nrecent exact and near-optimal approaches scale to moderate circuits, large deep\ncircuits are still out of scope.\n  In this work, we propose a SAT encoding based on parallel plans that apply 1\nSWAP and a group of CNOTs at each time step. Using domain-specific information,\nwe maintain optimality in parallel plans while scaling to large and deep\ncircuits. From our results, we show the scalability of our approach which\nsignificantly outperforms leading exact and near-optimal approaches (up to\n100x). For the first time, we can optimally map several 8, 14, and 16 qubit\ncircuits onto 54, 80, and 127 qubit platforms with up to 17 SWAPs. While adding\noptimal SWAPs, we also report near-optimal depth in our mapped circuits.",
        "updated": "2024-03-18 09:19:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11598v1"
    }
]