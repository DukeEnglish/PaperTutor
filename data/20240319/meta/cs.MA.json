[
    {
        "title": "Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective",
        "authors": "Muhammad Aneeq uz ZamanAlec KoppelMathieu LaurièreTamer Başar",
        "links": "http://arxiv.org/abs/2403.11345v1",
        "entry_id": "http://arxiv.org/abs/2403.11345v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11345v1",
        "summary": "We address in this paper Reinforcement Learning (RL) among agents that are\ngrouped into teams such that there is cooperation within each team but\ngeneral-sum (non-zero sum) competition across different teams. To develop an RL\nmethod that provably achieves a Nash equilibrium, we focus on a\nlinear-quadratic structure. Moreover, to tackle the non-stationarity induced by\nmulti-agent interactions in the finite population setting, we consider the case\nwhere the number of agents within each team is infinite, i.e., the mean-field\nsetting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We\ncharacterize the Nash equilibrium (NE) of the GS-MFTG, under a standard\ninvertibility condition. This MFTG NE is then shown to be $\\mathcal{O}(1/M)$-NE\nfor the finite population game where $M$ is a lower bound on the number of\nagents in each team. These structural results motivate an algorithm called\nMulti-player Receding-horizon Natural Policy Gradient (MRPG), where each team\nminimizes its cumulative cost independently in a receding-horizon manner.\nDespite the non-convexity of the problem, we establish that the resulting\nalgorithm converges to a global NE through a novel problem decomposition into\nsub-problems using backward recursive discrete-time Hamilton-Jacobi-Isaacs\n(HJI) equations, in which independent natural policy gradient is shown to\nexhibit linear convergence under time-independent diagonal dominance.\nExperiments illuminate the merits of this approach in practice.",
        "updated": "2024-03-17 21:11:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11345v1"
    },
    {
        "title": "Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving",
        "authors": "Steffen HagedornMarcel MilichAlexandru P. Condurache",
        "links": "http://arxiv.org/abs/2403.11304v1",
        "entry_id": "http://arxiv.org/abs/2403.11304v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11304v1",
        "summary": "Planning the trajectory of the controlled ego vehicle is a key challenge in\nautomated driving. As for human drivers, predicting the motions of surrounding\nvehicles is important to plan the own actions. Recent motion prediction methods\nutilize equivariant neural networks to exploit geometric symmetries in the\nscene. However, no existing method combines motion prediction and trajectory\nplanning in a joint step while guaranteeing equivariance under\nroto-translations of the input space. We address this gap by proposing a\nlightweight equivariant planning model that generates multi-modal joint\npredictions for all vehicles and selects one mode as the ego plan. The\nequivariant network design improves sample efficiency, guarantees output\nstability, and reduces model parameters. We further propose equivariant route\nattraction to guide the ego vehicle along a high-level route provided by an\noff-the-shelf GPS navigation system. This module creates a momentum from\nembedded vehicle positions toward the route in latent space while keeping the\nequivariance property. Route attraction enables goal-oriented behavior without\nforcing the vehicle to stick to the exact route. We conduct experiments on the\nchallenging nuScenes dataset to investigate the capability of our planner. The\nresults show that the planned trajectory is stable under roto-translations of\nthe input scene which demonstrates the equivariance of our model. Despite using\nonly a small split of the dataset for training, our method improves L2 distance\nat 3 s by 20.6 % and surpasses the state of the art.",
        "updated": "2024-03-17 18:53:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11304v1"
    },
    {
        "title": "GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment",
        "authors": "Lance YingKunal JhaShivam AaryaJoshua B. TenenbaumAntonio TorralbaTianmin Shu",
        "links": "http://arxiv.org/abs/2403.11075v1",
        "entry_id": "http://arxiv.org/abs/2403.11075v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11075v1",
        "summary": "Verbal communication plays a crucial role in human cooperation, particularly\nwhen the partners only have incomplete information about the task, environment,\nand each other's mental state. In this paper, we propose a novel cooperative\ncommunication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates\nverbal communication as a planning problem that minimizes the misalignment\nbetween the parts of agents' mental states that are relevant to the goals. This\napproach enables an embodied assistant to reason about when and how to\nproactively initialize communication with humans verbally using natural\nlanguage to help achieve better cooperation. We evaluate our approach against\nstrong baselines in two challenging environments, Overcooked (a multiplayer\ngame) and VirtualHome (a household simulator). Our experimental results\ndemonstrate that large language models struggle with generating meaningful\ncommunication that is grounded in the social and physical context. In contrast,\nour approach can successfully generate concise verbal communication for the\nembodied assistant to effectively boost the performance of the cooperation as\nwell as human users' perception of the assistant.",
        "updated": "2024-03-17 03:52:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11075v1"
    },
    {
        "title": "Resilient Fleet Management for Energy-Aware Intra-Factory Logistics",
        "authors": "Mithun GouthamStephanie Stockar",
        "links": "http://arxiv.org/abs/2403.11034v1",
        "entry_id": "http://arxiv.org/abs/2403.11034v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11034v1",
        "summary": "This paper presents a novel fleet management strategy for battery-powered\nrobot fleets tasked with intra-factory logistics in an autonomous manufacturing\nfacility. In this environment, repetitive material handling operations are\nsubject to real-world uncertainties such as blocked passages, and equipment or\nrobot malfunctions. In such cases, centralized approaches enhance resilience by\nimmediately adjusting the task allocation between the robots. To overcome the\ncomputational expense, a two-step methodology is proposed where the nominal\nproblem is solved a priori using a Monte Carlo Tree Search algorithm for task\nallocation, resulting in a nominal search tree. When a disruption occurs, the\nnominal search tree is rapidly updated a posteriori with costs to the new\nproblem while simultaneously generating feasible solutions. Computational\nexperiments prove the real-time capability of the proposed algorithm for\nvarious scenarios and compare it with the case where the search tree is not\nused and the decentralized approach that does not attempt task reassignment.",
        "updated": "2024-03-16 22:46:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11034v1"
    },
    {
        "title": "A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems",
        "authors": "Chinmay Vilas SamakTanmay Vilas SamakVenkat Krovi",
        "links": "http://arxiv.org/abs/2403.10996v1",
        "entry_id": "http://arxiv.org/abs/2403.10996v1",
        "pdf_url": "http://arxiv.org/pdf/2403.10996v1",
        "summary": "This work presents a sustainable multi-agent deep reinforcement learning\nframework capable of selectively scaling parallelized training workloads\non-demand, and transferring the trained policies from simulation to reality\nusing minimal hardware resources. We introduce AutoDRIVE Ecosystem as an\nenabling digital twin framework to train, deploy, and transfer cooperative as\nwell as competitive multi-agent reinforcement learning policies from simulation\nto reality. Particularly, we first investigate an intersection traversal\nproblem of 4 cooperative vehicles (Nigel) that share limited state information\nin single as well as multi-agent learning settings using a common policy\napproach. We then investigate an adversarial autonomous racing problem of 2\nvehicles (F1TENTH) using an individual policy approach. In either set of\nexperiments, a decentralized learning architecture was adopted, which allowed\nrobust training and testing of the policies in stochastic environments. The\nagents were provided with realistically sparse observation spaces, and were\nrestricted to sample control actions that implicitly satisfied the imposed\nkinodynamic and safety constraints. The experimental results for both problem\nstatements are reported in terms of quantitative metrics and qualitative\nremarks for training as well as deployment phases. We also discuss agent and\nenvironment parallelization techniques adopted to efficiently accelerate MARL\ntraining, while analyzing their computational performance. Finally, we\ndemonstrate a resource-aware transition of the trained policies from simulation\nto reality using the proposed digital twin framework.",
        "updated": "2024-03-16 18:47:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.10996v1"
    }
]