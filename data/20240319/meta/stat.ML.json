[
    {
        "title": "Generalization error of spectral algorithms",
        "authors": "Maksim VelikanovMaxim PanovDmitry Yarotsky",
        "links": "http://arxiv.org/abs/2403.11696v1",
        "entry_id": "http://arxiv.org/abs/2403.11696v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11696v1",
        "summary": "The asymptotically precise estimation of the generalization of kernel methods\nhas recently received attention due to the parallels between neural networks\nand their associated kernels. However, prior works derive such estimates for\ntraining by kernel ridge regression (KRR), whereas neural networks are\ntypically trained with gradient descent (GD). In the present work, we consider\nthe training of kernels with a family of $\\textit{spectral algorithms}$\nspecified by profile $h(\\lambda)$, and including KRR and GD as special cases.\nThen, we derive the generalization error as a functional of learning profile\n$h(\\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional\ntranslation-invariant model. Under power-law assumptions on the spectrum of the\nkernel and target, we use our framework to (i) give full loss asymptotics for\nboth noisy and noiseless observations (ii) show that the loss localizes on\ncertain spectral scales, giving a new perspective on the KRR saturation\nphenomenon (iii) conjecture, and demonstrate for the considered data models,\nthe universality of the loss w.r.t. non-spectral details of the problem, but\nonly in case of noisy observation.",
        "updated": "2024-03-18 11:52:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11696v1"
    },
    {
        "title": "Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates",
        "authors": "Riccardo GrazziMassimiliano PontilSaverio Salzo",
        "links": "http://arxiv.org/abs/2403.11687v1",
        "entry_id": "http://arxiv.org/abs/2403.11687v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11687v1",
        "summary": "We study the problem of efficiently computing the derivative of the\nfixed-point of a parametric non-differentiable contraction map. This problem\nhas wide applications in machine learning, including hyperparameter\noptimization, meta-learning and data poisoning attacks. We analyze two popular\napproaches: iterative differentiation (ITD) and approximate implicit\ndifferentiation (AID). A key challenge behind the nonsmooth setting is that the\nchain rule does not hold anymore. Building upon the recent work by Bolte et al.\n(2022), who proved the linear convergence of non-differentiable ITD, we provide\nrefined linear convergence rates for both ITD and AID in the deterministic\ncase. We further introduce NSID, a new method to compute the implicit\nderivative when the fixed point is defined as the composition of an outer map\nand an inner map which is accessible only through a stochastic unbiased\nestimator. We establish rates for the convergence of NSID to the true\nderivative, encompassing the best available rates in the smooth setting. We\npresent illustrative experiments confirming our analysis.",
        "updated": "2024-03-18 11:37:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11687v1"
    },
    {
        "title": "The Value of Reward Lookahead in Reinforcement Learning",
        "authors": "Nadav MerlisDorian BaudryVianney Perchet",
        "links": "http://arxiv.org/abs/2403.11637v1",
        "entry_id": "http://arxiv.org/abs/2403.11637v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11637v1",
        "summary": "In reinforcement learning (RL), agents sequentially interact with changing\nenvironments while aiming to maximize the obtained rewards. Usually, rewards\nare observed only after acting, and so the goal is to maximize the expected\ncumulative reward. Yet, in many practical settings, reward information is\nobserved in advance -- prices are observed before performing transactions;\nnearby traffic information is partially known; and goals are oftentimes given\nto agents prior to the interaction. In this work, we aim to quantifiably\nanalyze the value of such future reward information through the lens of\ncompetitive analysis. In particular, we measure the ratio between the value of\nstandard RL agents and that of agents with partial future-reward lookahead. We\ncharacterize the worst-case reward distribution and derive exact ratios for the\nworst-case reward expectations. Surprisingly, the resulting ratios relate to\nknown quantities in offline RL and reward-free exploration. We further provide\ntight bounds for the ratio given the worst-case dynamics. Our results cover the\nfull spectrum between observing the immediate rewards before acting to\nobserving all the rewards before the interaction starts.",
        "updated": "2024-03-18 10:19:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11637v1"
    },
    {
        "title": "Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)",
        "authors": "Paul NovelloJoseba DalmauLéo Andeol",
        "links": "http://arxiv.org/abs/2403.11532v1",
        "entry_id": "http://arxiv.org/abs/2403.11532v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11532v1",
        "summary": "Research on Out-Of-Distribution (OOD) detection focuses mainly on building\nscores that efficiently distinguish OOD data from In Distribution (ID) data. On\nthe other hand, Conformal Prediction (CP) uses non-conformity scores to\nconstruct prediction sets with probabilistic coverage guarantees. In this work,\nwe propose to use CP to better assess the efficiency of OOD scores.\nSpecifically, we emphasize that in standard OOD benchmark settings, evaluation\nmetrics can be overly optimistic due to the finite sample size of the test\ndataset. Based on the work of (Bates et al., 2022), we define new conformal\nAUROC and conformal FRP@TPR95 metrics, which are corrections that provide\nprobabilistic conservativeness guarantees on the variability of these metrics.\nWe show the effect of these corrections on two reference OOD and anomaly\ndetection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al.,\n2022). We also show that the benefits of using OOD together with CP apply the\nother way around by using OOD scores as non-conformity scores, which results in\nimproving upon current CP methods. One of the key messages of these\ncontributions is that since OOD is concerned with designing scores and CP with\ninterpreting these scores, the two fields may be inherently intertwined.",
        "updated": "2024-03-18 07:35:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11532v1"
    },
    {
        "title": "State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards",
        "authors": "Yuto TanimotoKenji Fukumizu",
        "links": "http://arxiv.org/abs/2403.11520v1",
        "entry_id": "http://arxiv.org/abs/2403.11520v1",
        "pdf_url": "http://arxiv.org/pdf/2403.11520v1",
        "summary": "While many multi-armed bandit algorithms assume that rewards for all arms are\nconstant across rounds, this assumption does not hold in many real-world\nscenarios. This paper considers the setting of recovering bandits (Pike-Burke &\nGrunewalder, 2019), where the reward depends on the number of rounds elapsed\nsince the last time an arm was pulled. We propose a new reinforcement learning\n(RL) algorithm tailored to this setting, named the State-Separate SARSA\n(SS-SARSA) algorithm, which treats rounds as states. The SS-SARSA algorithm\nachieves efficient learning by reducing the number of state combinations\nrequired for Q-learning/SARSA, which often suffers from combinatorial issues\nfor large-scale RL problems. Additionally, it makes minimal assumptions about\nthe reward structure and offers lower computational complexity. Furthermore, we\nprove asymptotic convergence to an optimal policy under mild assumptions.\nSimulation studies demonstrate the superior performance of our algorithm across\nvarious settings.",
        "updated": "2024-03-18 07:14:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.11520v1"
    }
]