PublishedasaconferencepaperatICLR2024
GENERALIZATION ERROR OF SPECTRAL ALGORITHMS
MaksimVelikanov1,2,MaximPanov3,DmitryYarotsky4
1TechnologyInnovationInstitute,2EcolePolytechnique,3MBZUAI,4Skoltech
maksim.velikanov@tii.ae, maxim.panov@mbzuai.ac.ae, d.yarotsky@skoltech.ru
ABSTRACT
Theasymptoticallypreciseestimationofthegeneralizationofkernelmethodshas
recentlyreceivedattentionduetotheparallelsbetweenneuralnetworksandtheir
associated kernels. However, prior works derive such estimates for training by
kernelridgeregression(KRR),whereasneuralnetworksaretypicallytrainedwith
gradient descent (GD). In the present work, we consider the training of kernels
withafamilyofspectralalgorithmsspecifiedbyprofileh(λ),andincludingKRR
and GD as special cases. Then, we derive the generalization error as a func-
tional of learning profile h(λ) for two data models: high-dimensional Gaussian
andlow-dimensionaltranslation-invariantmodel. Underpower-lawassumptions
onthespectrumofthekernelandtarget,weuseourframeworkto(i)givefullloss
asymptotics for both noisy and noiseless observations (ii) show that the loss lo-
calizesoncertainspectralscales,givinganewperspectiveontheKRRsaturation
phenomenon(iii)conjecture,anddemonstratefortheconsidereddatamodels,the
universalityofthelossw.r.t. non-spectraldetailsoftheproblem,butonlyincase
ofnoisyobservation.
1 INTRODUCTION
Quantitativedescriptionofvariousaspectsofneuralnetworks,mostnotably,generalizationperfor-
manceaftertraining, isanimportantbutchallengingquestionofdeeplearningtheory. Oneofthe
centralapproachestothisquestionisbuiltontheconnectionbetweenneuralnetworksanditsneural
tangent kernel, first established for infinitely wide networks (Jacot et al., 2018; Lee et al., 2020;
Chizatetal.,2019),andthenfurthertakentotherichrealmoffinitepracticalnetworks(Fortetal.,
2020;Maddoxetal.,2021;Long,2021;Kopitkov&Indelman,2020;Vyasetal.,2023).
Considerataskoflearningtargetfunctionf∗(x)fromtrainingdatasetD ={x }N and(possibly)
N i i=1
noisy observation y = f∗(x )+σε , ε ∼ N(0,1), using given kernel K(x,x′). Then, many
i i i i
authors (Bordelon et al., 2020; Jacot et al., 2020a; Wei et al., 2022) derive asymptotic N → ∞
generalizationerrorforkernelridgeregression(KRR)algorithmwithregularizationη:
L (η)= 1∂η eff (cid:88)(η effc l)2+λ2 l σ N2 , 1= η + 1 (cid:88) λ l , (1)
KRR 2 ∂η (η +λ )2 η N λ +η
eff l eff l eff
l l
where λ are population eigenvalues of K(x,x′) and c are respective eigencoefficients of f∗(x)
l l
(seedefinitionin(4)). Themainmotivationofourworkiswhathappenswith(1)when,asrequired
by association with neural networks, KRR is replaced with GD or an even more general learning
algorithm.
Importantly, aresultofthetype(1)maygivepreciseinsightsforthefamilyofpower-lawspectral
distributions,closelyrelatedtocapacityandsourceassumptionsofnon-parametricstatistics:
λ ∼l−ν (withν >1), c2 ∼l−κ−1 (withκ>0). (2)
l l
Power-lawconditions(2)exhibitarichpictureofconvergencerates. Forthecaseofnoisyobserva-
tionsσ2 > 0,(Caponnetto&DeVito,2007)givesminimaxrateO(N− κ+κ 1). Fornoiselessobser-
vationsσ2 =0,theoptimalestimationratesignificantlyimproves(Bordelonetal.,2020;Cuietal.,
2021)becomingO(N−κ). However,theoptimalratesarenotalwaysachievableforsomeclassical
algorithms. For example, in the case when κ > 2 in the noisy case the rate of the KRR becomes
ν
1
4202
raM
81
]GL.sc[
1v69611.3042:viXraPublishedasaconferencepaperatICLR2024
Noisyobservationsσ2 >0 Noiselessobservations
KRR GF&Optimal KRR&GF&Optimal
ExponentinL=O(N−#) κ+κ
1
(cid:12) (cid:12)
(cid:12)
2ν2 +ν
1
κ+κ
1
κ(cid:12) (cid:12)2ν
Spectrallocalizationscales κ+ν
1
(cid:12) (cid:12) (cid:12)(cid:8) 0, 2νν +1(cid:9) κ+ν
1
ν (cid:12) (cid:12)0
Universality yes yes no
Table1: Ourresultsforpower-lawspectraldistributions(2)andthreealgorithms: optimallyregu-
larizedKRR,optimallystoppedGradientFlow(GF),andtheoptimalalgorithm(seeSection3). For
quantitiesexhibitingsaturationat κ = 2,theverticalline·(cid:12) (cid:12)·separatessaturatedandnon-saturated
ν
values. Thespectrallocalizationatscalesmeansthattheerrorisaccumulatedateigenvaluesλof
theorderN−s (seeSection5.1). Inthelastline,byuniversality,wemeantheasymptoticequality
of the errors for different problems with the same population spectrum λ ,c . While we show the
l l
universality only for our two data models, we would expect it to hold for a broader class of data
models.
O(N− 2ν2 +ν 1), i.e. KRR doesn’t attain the minimax lower bound (Li et al., 2023). Such an effect
is usually called saturation and is well-known in various non-parametric problems (Mathe´, 2004;
Baueretal.,2007). However,thesaturationeffectcanberemovedwithalgorithmsotherthanKRR,
forexamplespectralcut-off(Baueretal.,2007)orgradientdescent(Pillaud-Vivienetal.,2018). In
noiseless case, (Bordelon et al., 2020; Cui et al., 2021) show saturation at the same point κ > 2,
ν
with the rate changing to O(N−2ν). Whether noiseless saturation can be removed by algorithms
otherthanKRR,tothebestofourknowledge,wasnotstudiedintheliterature.
Ourcontribution. Inthiswork,weaugmenttheabovepictureinseveraldirections,assummarized
inTable1andthefollowingthreeblocks
Loss functional. In Section 3, we introduce a new kernel learning framework by specifying the
learningalgorithmwithaspectralprofileh(λ)andexpressingthegeneralizationerrorasaquadratic
functional of this profile. While specific choices of h(λ) can give KRR, Gradient Flow (GF), or
anyiterativefirst-orderalgorithm,wegobeyondtheseclassicalexamplesandconsidertheoptimal
learningalgorithmastheminimizerofthelossfunctionalforagivenproblem.
The models. As the loss functional is problem-specific, we consider two different models: a
Wishart-typemodelwithGaussianfeaturesandatranslational-invariantmodelonacircle. InSec-
tion4,wederivelossfunctionalsforthesemodels. Inaddition,weintroduceasimpleNaiveModel
forNoisyObservations(NMNO).Inthepresenceofobservationsnoise,forreasonablelearningal-
gorithms,andatleastforpower-lawspectrum,NMNOmodelgivesasymptoticallyexactlossvalues
forbothWishartandCirclemodelsdespitetheirdifferences. Thissuggestsapossibleuniversality
propertyforalargerfamilyofproblems,includingourWishartandCirclemodelsasspecialcases.
Results for power-law spectrum. In Section 5, we reach specific conclusions by restricting both
kerneleigenvaluesandthetargetfunctioneigencoefficientstobedescribedbypower-laws(2).
• For both noisy and noiseless observations, we derive full loss asymptotics. While the re-
sultingratesareindeedconsistentwiththepriorworks,precisecharacterizationofleading
orderasymptotictermgivesaccesstofinerdetails,suchastheshapeh∗(λ)oftheoptimal
learningalgorithm.
• Weintroducethenotionofspectrallocalization-thescaleofkerneleigenvaluesoverwhich
thelossisaccumulated-andquantifyitforalgorithmsunderconsideration. Inparticular,
this perspective provides a simple explanation of KRR saturation phenomenon as the in-
abilitytosufficientlyfitthemainfeaturesofthetargetfunction.
• By characterizing the shape of the optimal algorithm, we point to the cases where it is
optimaltooverlearnthetrainingdata,akintoKRRwithnegativeregularization.Moreover,
wespecifythevaluesofpower-lawexponentsatwhichoverlearningisbeneficial.
2PublishedasaconferencepaperatICLR2024
2 SETTING
Generalization error. We evaluate the estimator f(cid:98)(x) with its squared prediction error |f(cid:98)(x)−
f∗(x)|2,averagedoverinputsxdrawnfromapopulationdensityp(x),andthenalltherandomness
intrainingdatasetD :
N
L f(cid:98)= 21 E DN,ε(cid:13) (cid:13)f(cid:98)−f∗(cid:13) (cid:13)2 = 1 2(cid:90) E DN,ε(cid:104)(cid:0) f(cid:98)(x)−f∗(x)(cid:1)2(cid:105) p(x)dx, (3)
whereε = (ε ,...,ε ) ∼ N(0,I), ∥f∥2 ≡ ⟨f,f⟩, andanglebracketsdenotethescalarproduct
1 N
(cid:82)
⟨f,g⟩≡ f(x)g(x)p(x)dx.
Populationspectrum. CentraltoourapproachistheconnectionbetweengeneralizationerrorL
f(cid:98)
andspectraldistributionsλ ,c oftheproblem,definedbyMercertheoremas
l l
(cid:88) (cid:88)
K(x,x′)= λ ϕ (x)ϕ (x′), f∗(x)= c ϕ (x), ⟨ϕ ,ϕ ⟩=δ . (4)
l l l l l l l′ ll′
Here,λ arethekerneleigenvalues,c arethetargetfunctioncoefficients,andϕ (x)aretheeigen-
l l l
featuresofthekernel. Inthemostinterestingscenarios,thenumberP offeaturesϕ isinfinite,and
l
therespectiveeigenvaluesλ →0asl→∞.
l
Inthiswork,weaimattwolevelsofresultsw.r.t. thepopulationspectrumλ ,c . First,wewantto
l l
obtainacharacterizationofL forageneralλ ,c ,similartowhatisdoneintheclassicresult(1).
f(cid:98) l l
Second,wewillassumethepower-laws(2)toobtainamoredetaileddescriptionofthegeneraliza-
tionerrorL .
f(cid:98)
An important object is the empirical kernel matrix K ∈ RN×N composed of evaluation of the
kernel on training points (K) = K(x ,x ). Let us additionally denote by y = f∗ +ε ∈ RN
ij i j
the observation vector with components (y) = f∗(x )+ε ; by Λ ∈ RP×P the diagonal matrix
i i i
with (Λ) = λ ; and by Φ ∈ RP×N the matrix of kernel features evaluated at training points,
ll l
(cid:0) (cid:1)
Φ =ϕ (x ). Then,spectraldecomposition(4)allowstowriteempiricalkernelmatrixas
li l i
K=ΦTΛΦ. (5)
Data models. A standard approach to analyzing the generalization error consists in considering
generalfamiliesofkernelsK(x,x′)andtargetsf∗(x),typicallydefinedbyregularityassumptions,
andderivingupperandlowergeneralizationbounds(e.g.,see(Caponnetto&DeVito,2007)). We
adoptadifferentapproachthatallowsustogobeyondjusttheboundsanddescribegeneralization
errorL withmorequantitativedetail. Tothisend,weconsidertwoparticularmodels, Circleand
f(cid:98)
Wishart,thatrepresentextremelow-andhigh-dimensionalcasesofthekernellearningsetting.
Wishart model. This model is a common choice for our setting: it was explicitly assumed in (Cui
etal.,2021;Jacotetal.,2020b;Simonetal.,2023)andiscloselyrelatedtosettingsof(Jacotetal.,
2020a; Bordelon et al., 2020; Canatar et al., 2021; Wei et al., 2022). Specifically, assume that the
kernelfeaturesϕ (x)anddatadistributionp(x)aresuchthatfeaturematrixΦhasi.i.d. Gaussian
l
entries: ϕ (x ) ∼ N(0,1). Then, the resulting empirical kernel matrix in (5) is called Wishart
l i
matrixinRandomMatrixTheory(RMT)context. Intuitively,wecanthinkaboutWishartmodelas
a model of some high-dimensional data with all the structural information about data distribution
andkernelbeingwipedoutbyhigh-dimensionalfluctuations.
Circle model. To describe the generalization of a kernel estimator trained by completely fitting
(i.e. interpolating) training data, Spigler et al. (2020) and Beaglehole et al. (2023) used a model
with translations-invariant kernel and training inputs forming a regular lattice in a hypercube. In
this work, we consider, for transparency, a one-dimensional version of this model. Yet, the one-
dimensionalversionwilldisplayalltheimportantphenomenaweplantodiscuss. Specifically, let
inputs come from the circle x ∈ S = R mod 2π, and the training set D = {u + 2πi}N−1.
N N i=0
Here, u is an overall shift of the training data lattice, which we sample uniformly u ∼ U([0,2π])
tointroducesomerandomnessinotherwisedeterministictrainingsetD . Then,thekernelandthe
N
targetaredefinedinthebasisofFourierharmonicsas
∞ ∞
K(x,x′)= (cid:88) λ eil(x−x′), f∗(x)= (cid:88) c eilx. (6)
l l
l=−∞ l=−∞
Weassumeλ =λ ∈Randc =c toensurethatboththekernelandthetargetarereal-valued.
l −l −l l
3PublishedasaconferencepaperatICLR2024
3 SPECTRAL ALGORITHMS AND THEIR GENERALIZATION ERROR
Severalauthors,e.g. Baueretal.(2007);Rastogi&Sampath(2017);Linetal.(2020),considered
SpectralAlgorithmstogeneralizeandextendthetypeofregularizationperformedbyclassicalmeth-
odssuchasKRRandGD.Indeed,bothKRRandGDfitobservationvectory toadifferentextent
indifferentspectralsubspacesoftheempiricalkernelmatrixK. Forspectralalgorithms,thisfitting
degreeisspecifiedbyaprofileh(λ)sothattheestimatorisgivenby
f(cid:98)(x)=k(x)TK−1h(cid:0)1K(cid:1)
y. (7)
N
Here k(x) ∈ RN has components (cid:0) k(x)(cid:1) = K(x,x ). Function h(λ) is applied to the kernel
i i
matrix 1Kintheoperatorsense: h(·)actselement-wiseondiagonalmatrices,andforanarbitrary
N
positivesemi-definitematrixwitheigendecompositionA=UTDUwehaveh(A)=UTh(D)U.
Letusshowhowclassicalalgorithmscanbewrittenintheform(7)withaspecificchoiceofh(λ):
1. KernelRidgeRegressionwithregularizationη isobtainedwithh (λ) = λ . Then, (7)
η λ+η
transformsintotheclassicalformulaforKRRpredictor:
f(cid:98)(x)=k(x)T(cid:0) K+NηI(cid:1)−1
y.
2. GradientFlowbytimetisobtainedwithh (λ)=1−e−tλ.(Forthisandthenextexample,
t
weprovidetherespectivederivationsinSectionB.1.)
3. For an arbitrary general first-order iterative algorithm at iteration t, h (λ) is given by the
t
associateddegree-tpolynomialwithh (λ = 0) = 0(seeSectionB.1). Forexample,GD
t
withalearningrateαisgivenbyh (λ)=1−(1−αλ)t.
t
Now, we make a simple observation that is crucial to the current work. Note that generalization
error (3) is quadratic in the estimator f(cid:98), while f(cid:98)is linear in h according to (7). Thus, for any
problem, the generalization error is quadratic in the profile h. This observation is formalized (see
theproofinSectionA)in
Proposition1. Thereexistsignedmeasuresρ(2)(dλ ,dλ ),ρ(1)(dλ)andρ(ε)(dλ)(giveninequa-
1 2
tions (37)-(39)) such that the map h (cid:55)→ f(cid:98)(cid:55)→ L given by (7), (3) is expressed as the quadratic
f(cid:98)
functional
1(cid:20)(cid:90) (cid:90) (cid:90) (cid:21)
L[h]= h(λ )h(λ )ρ(2)(dλ ,dλ )−2 h(λ)ρ(1)(dλ)+∥f∗(x)∥2
2 1 2 1 2
(8)
1σ2 (cid:90)
+ h2(λ)ρ(ε)(dλ).
2 N
We will refer to the measures ρ(1),ρ(2) and ρ(ε) as the learning measures. Proposition 1 shows
that the loss functional is completely specified by these measures. The first line in (8) describes
the estimation of the target function from the signal part f∗ of the observation vector y, which is
hinderedbyinsufficiencyofN observationstocapturefinedetailsoff∗(x). Similarly,thesecond
linein(8)describestheeffectof(unwanted)learningofthenoisepartεofobservationsy.
The functional (8) makes the relation between the learning algorithm h(λ) and the generalization
errormaximallyexplicit. However,propertiesoftheunderlyingkernelanddataarereflectedinthe
learningmeasuresρ(2)(dλ ,dλ ),ρ(1)(dλ)andρ(ε)(dλ)inafairlycomplicatedway. InSectionA,
1 2
weshowsomegeneralconnectionsbetweenthekerneleigenvaluesλ , thefeaturesϕ (x), andthe
l l
learning measures. Yet, the explicit characterization of learning measures is challenging even for
ourtwodatamodels,andconstitutesthemaintechnicalstepofourwork.
Optimal algorithm. Consider a regression problem and its associated loss functional (8). Since
thelossfunctionalispositivesemi-definite,undersuitableregularityassumptionsithasa(possibly
non-unique)minimizer
h∗(λ)=argminL[h] (9)
thatachievestheminimalpossiblegeneralizationerrorinagivenproblem. Werefertothespectral
algorithmwithprofileh∗(λ)asoptimal. Inthecontextofmodelswithpower-lawspectra,wewill
alsospeakofoptimalalgorithmsinabroadersense,asthoseprovidingtheoptimalerrorscalingwith
N. WewillanalyzetheconditionsofoptimalityintheCircleandWishartmodelsandshowthatin
thenoisysettingtheyhavethesamestructure,easilyunderstoodusingasimplifiedlossmodel.
4PublishedasaconferencepaperatICLR2024
4 EXPLICIT FORMS OF THE LOSS FUNCTIONAL
Circle model. The main advantage of this model is that it admits an exact and explicit solution.
Below,wedescribeitsmainproperties,withderivationsandproofsgiveninSectionD.
Duetothefactthattraininginputsx
i
formaregularlattice,theeigenvaluesλ(cid:98)k ofempiricalkernel
matrixKbecomedeterministic: λ(cid:98)k =
(cid:80)∞
n=−∞λ l+Nn. Behindthisrelationisthelearningpicture
based on aliasing. For a given k ∈ 0,N −1, the information about the target function contained
inallFourierharmonicswithfrequenciesl = k+Nn, n ∈ Ziscompressedintothesinglek-th
harmonic,andthenprojectedbacktotheoriginall = k+Nnharmonicsoftheestimatorf(cid:98). This
leadstoatransformationofpopulationquantitiesλa|c |2bthatwecallN-deformation:
l l
∞
(cid:2) λa|c |2b(cid:3) ≡ (cid:88) λa |c |2b. (10)
l l N l+Nn l+Nn
n=−∞
Itisperiodic: (cid:2) λa l|c l|2b(cid:3)
N
=(cid:2) λa l+N|c l+N|2b(cid:3) N. Also,λ(cid:98)k =(cid:2) λ k(cid:3) N. Then,wehave
Theorem1. LossfunctionaloftheCirclemodelisgivenby
L[h]= 1 2N k(cid:88) =− 01(cid:34) (cid:16) σ N2 +(cid:2) |c k|2(cid:3) N(cid:17)(cid:2) (cid:2)λ λ2 k k(cid:3) (cid:3)N
2
Nh2(λ(cid:98)k)−2(cid:2) λ (cid:2)k λ|c kk (cid:3)| N2(cid:3) Nh(λ(cid:98)k)+(cid:2) |c k|2(cid:3) N(cid:35) . (11)
Thespecialfeatureofthelossfunctional(11)comparedtothegeneralform(8)inthatthereareno
off-diagonal λ ̸= λ contributions to the loss. Then, the optimal algorithm is found by a simple
1 2
point-wiseminimization:
[λ |c |2] [λ ]
h∗(λ(cid:98)k)=
(cid:0)σ2
+k
(cid:2)
|k
c
|2N
(cid:3)
(cid:1)k [λN
2]
. (12)
N k N k N
Wishart model. This model, although more common in the literature, does not enjoy an exact
solution like the Circle model. However, using two approximations, in Section E we derive an
explicit form of the measures ρ(2),ρ(1),ρ(ε) describing the loss by equation (8). We give now an
outlineofourderivation.
First,wepointoutthatthelearningmeasuresfrom(8)canbereducedtothefirstandsecondmoment
oftheimaginarypartoftheresolventR(cid:98)(z)=( NK−zI)−1computedatthepointsz =λ+i0 +near
the real line. Then, we make standard RMT assumptions to describe the resolvent in terms of the
Stieltjestransformr(z)ofspectralmeasureofK,whichsatisfiesthefixed-pointequation
1=−zr(z)+ 1 (cid:88) r(z)λ l . (13)
N r(z)λ +1
l
l
Thefirstresolventmomentiscomputedstraightforwardlyandthesecondmomentatthesamepoint
z =z canbecomputedwithdifferentiationtrick(Simonetal.,2023),butforthesecondmoment
1 2
atz ̸=z anewtoolisrequired. Forthat,weemployWick’stheoremofcomputingaveragesover
1 2
Gaussianfields,wherewetakeintoaccountleadingorderpairingsandneglectsubleadingO(N−1)
terms. TheaboveprocedureexpressesthemomentsofR(cid:98)(z)intermsofthreeauxiliaryfunctions
v(z)=(cid:88) c2
l ,
u(z)=(cid:88) λ lc2
l ,
w(z)=(cid:88) λ2
l . (14)
λ +r−1(z) λ +r−1(z) λ +r−1(z)
l l l
l l l
Finally,weobtainlossfunctional(8)byspecifyingeachofthelearningmeasures.Usingthenotation
ℑ{z}forimaginarypartofz,wefindfirstmomentofsignalmeasureandthenoisemeasuretobe
ρ(1)(dλ) ℑu(λ) ρ(ε)(dλ) ℑw(λ)
= , = . (15)
dλ πλ dλ πλ2
Thesecondmomentofsignalmeasurehasdiagonalλ =λ andoff-diagonalλ ̸=λ parts
1 2 1 2
ρ(2)(dλ ,dλ ) |r−1(λ )|2
1 2 = 1 ℑ{v(λ )}δ(λ −λ )
dλ dλ πλ2 1 1 2
1 2 1 (16)
1 ℑ(cid:8) u(λ )(cid:9) ℑ(cid:8) r−1(λ )(cid:9) −ℑ(cid:8) u(λ )(cid:9) ℑ(cid:8) r−1(λ )(cid:9)
2 1 1 2
+ .
π2λ λ λ −λ
1 2 1 2
5PublishedasaconferencepaperatICLR2024
=0.5 =5
KRR with =N /( +1) GF with t=N/( +1) KRR with =N /(2 +1) (saturation) GF with t=N/( +1)
100 CN /(+1) 100 CN /(+1) 100 100 CN /(+1)
101 101
102 102
101 101 103 103
104 104
102 102
105 C Cw cii rs ch lear NtN 22 /(2/(2 ++ 1)1) 105
101 103 105 101 103 105 101 103 105 101 103 105
N N N N
Wishart Cosine Wishart NMNO (Wishart & Cosine Wishart) Circle NMNO (Circle)
Figure1:Generalizationerrorofdifferentdatamodelsinpresenceofobservationnoiseconvergesto
ourNMNOmodel(solid)asN →∞,whichinturnconvergestoitsO(N−#)asymptotic(dashed).
Allplotshaveν = 1.5. CosineWishart isanadditionaldatamodelnotcoveredbyourtheoryyet
convergingtoNMNO.ThedifferencebetweenCircleandWishartasymptoticontheplot3isdueto
localizationoftheerroronscales=0atsaturation. FordetailsandextendeddiscussionseeSec.F.
Non-saturated KRR: * Non-saturated KRR: = * Saturated KRR: = *
Scales
1 1 1 Noise
before learning
+1 Signal
2 before learning
2+1 Noise
after learning
+1 +1 Signal
after learning
Total
Error localization
0 0 0
+1 +1 +12+1
eigenvalue scale s eigenvalue scale s eigenvalue scale s
Figure2: ScalediagramsofdifferentKRRregimesfornoisyobservations. Allplotshaveν = 1.2,
whileκ = 1.0inthenon-saturatedcase(leftandcenter)andκ = 5.0inthesaturatedcase(right).
Thedottedlinesrepresentthenoise1−s andsignal κstermsinequation(18).Thesolidlinesshow
ν ν
thesametermswithaddedcomponents2S(h) and2S(1−h). Left: thesub-optimal(s > s )non-
h ∗
saturatedcase. Center: theoptimal(s = s )non-saturatedcase. Right: thesaturated(κ > 2ν)
h ∗
casewiththechoices = ν optimalforKRR,butsub-optimalamonggeneralalgorithmsh.
η 2ν+1
NaiveModelofNoisyObservations. AswecanseefromourresultsforCircleandWishartmodel
above, the loss functional of a given model can be quite complicated. We will see, however, that
inthepresenceofobservationnoiseσ2 > 0theasymptotic(N → ∞)generalizationpropertiesof
boththesemodelsarewelldescribedbyasimpleartificialmodel(NMNO),introducedbelow. We
conjecturethismatchtobeauniversalphenomenon,validforawiderangeofmodels.
For a large dataset sizes N, we expect all the complex details of the problem to be concentrated
ateigendirectionsl withsmallλ , whosefeaturesϕ (x)cannotbewellcapturedbytheempirical
l l
kernelmatrixKofsizeN. Onthecontrary,Kshouldsucceedincapturingϕ (x)withmoderateλ ,
l l
andempiricalandpopulationeigendecompositionsshouldbeclosetoeachother.
Letusthereforeassumethatwecanignorethesmalleigenvaluesanddeterminethegeneralization
error only using components l with moderate λ (later, we will explain this by loss localization
l
at moderate spectral scales). Then, the contribution of the l-th component to the generalization
error can be approximated by (1−h(λ ))2c2 for signal fitting part, and σ2h2(λ ) for the learned
l l N l
observation noise. This completely determines the associated loss functional. Let us describe the
(cid:80)
population spectral data λ ,c by the spectral eigenvalue measure µ (dλ) = δ (dλ) and the
coefficientmeasureµ (dλ)l =l (cid:80) c2δ (dλ). Then,wedefinetheNMλ NOmodelbl yλ thl efunctional
c l l λl
1 1
L(nmno)[h]= σ2 (cid:90) h2(λ)µ (dλ)+ 1 (cid:90) (cid:0) 1−h(λ)(cid:1)2 µ (dλ). (17)
2N λ 2 c
λmin λmin
Here,λ isareferenceminimalpopulationeigenvaluedefinedbytheconditionµ ([λ ,1])=N
min λ min
(i.e.,suchthatthesegment[λ ,1]containsexactlyN populationeigenvalues).
min
6
selacs
rorrEPublishedasaconferencepaperatICLR2024
5 RESULTS UNDER POWER-LAW SPECTRAL DISTRIBUTIONS
InthissectionweperformadeeperstudyoftheCircle,WishartandNMNOmodelsofSection4in
thesettingofpower-lawdistributions(2).Prioritizingtransparencyovergenerality,weassumeλ ,c
l l
tobeexactpower-lawsintheformconvenientforagivenmodel. Specifically,forCirclemodelwe
takeλ = (|l|+1)−ν and|c |2 = (|l|+1)−κ−1, whileforWishartmodelweassumecontinuous
l l
analogsofthesespectraldistributions,namelyassumethemtohavesmoothdensitiessupportedon
[0,1]: µ λ(dλ)= ν1λ−1− ν1dλandµ c(dλ)= ν1λκ ν−1dλ.
5.1 SCALINGANALYSISANDITSAPPLICATIONTOTHENMNOMODEL
Underpower-lawspectralassumptions,itiskeytoobservethatvariousimportantquantitiesscaleas
suitablepowersofthetrainingsetsizeN. Ingeneral,givenasequencea ,wewillsaythatithas
N
scalesifforanyϵ > 0wehave|a | = o(N−s+ϵ)and|a | = ω(N−s−ϵ). Ifonlythefirstorthe
N N
secondconditionholds,wesaythatthescaleofa isnotlessornotgreaterthans(respectively).
N
Suppose that g (λ) is a sequence of functions on the spectral interval (0,1]. We say that this
N
sequence has a scaling profile S(g)(s),s ≥ 0, if for any sequence λ ∈ (0,1] that has scale s
N
thesequence|g (λ )|hasscaleS(g)(s). Itiseasytocheckthatthescalingprofile, ifexists, isa
N N
continuous function of s (see Lemma 1). The basic example of a sequence of functions having a
scalingprofileisthesequenceg (λ)=Naλbwithconstanta,b;inthiscaseS(g)(s)=bs−a.
N
Integralsoffunctionswithascalingprofilealsohaveaspecificscaling:
Proposition2(seeproofinSectionC). Letg (λ)beasequenceoffunctionswithascalingpro-
N
file S(g)(s), and let a > 0 be a sequence of scale a > 0. Then, the sequence of integrals
N
(cid:82)1 |g (λ)|dλhasscales =min (S(g)(s)+s).
aN N ∗ 0≤s≤a
Whenprop.2canbeappliedtothefunctional(8),wecallthesetS =argmin (S(g)(s)+s)
loc 0≤s≤a
ofscaleswhichdominatethelossintegralasspectrallocalizationscalesofthegeneralizationerror.
Intherestofthepaper,wereservetheletterstodenotethescaleofeigenvaluesλ.
ApplicationtoNMNO. NowweapplythescalingargumentstotheNMNOmodel(17),forwhich
itwillbeeasytofindexplicitoptimalityconditions. Supposethattheproblemhaseitherdiscreteor
continuouspower-lawspectrumwithexponentsν,κasdescribedabovefortheCircleandWishart
model. Then,λ inequation(17)hasfinitescaleν. Supposethatthefunctionshand1−hhave
min
scalingprofilesS(h) andS(1−h). Then,applyingProposition2inthecontinuouscaseoranalogous
Proposition5inthediscretecase,weobtain
Proposition3. TheNMNOlossL(nmno)[h]hasscaling
Snmno[h]= min (cid:104)(cid:0) 1− s +2S(h)(s)(cid:1) ∧(cid:0)κs+2S(1−h)(s)(cid:1)(cid:105) . (18)
ν ν
0≤s≤ν
Here,∧ = min;thefirstandsecondargumentsin∧comefromthenoiseandsignaltermsin(17),
respectively. In the continuous case we use the fact that N−1λ−1−1/νh(λ)2 has scaling profile
1−s(1+ 1)+2S(h)whileλκ/ν−1(1−h(λ))2hasscalingprofiles(κ −1)+2S(1−h).
ν ν
Clearly, forany particular s only one ofthe values S(h)(s) and S(1−h)(s) can be strictly positive.
Thisimpliesaboundonfeasiblelossscalings:
Snmno[h]≤ min (cid:16)(cid:0) 1− s(cid:1) ∨ κs(cid:17) = κ . (19)
ν ν κ+1
0≤s≤ν
Theminimumhereisattainedatthescales = ν . Moreover,analgorithmhattainstheoptimal
∗ κ+1
scale κ exactlywhenforeachs∈[0,ν]wehave(cid:0) 1−s+2S(h)(s)(cid:1) ∧(cid:0)κs+2S(1−h)(s)(cid:1) ≥ κ :
κ+1 ν ν κ+1
Proposition 4. Snmno[h] ≤ κ , and the equality occurs when 1) S(h)(s) ≥ 1(cid:0)s − 1 (cid:1) for
κ+1 2 ν κ+1
s≥s = ν and2)S(1−h)(s)≥ 1(cid:0) κ − κs(cid:1) fors≤s .
∗ κ+1 2 κ+1 ν ∗
These results provide a simple picture of spectral algorithms close to optimality for the NMNO
1 1 ν
model: oneshouldchoosethespectralfunctionhsothat|h(λ)| ≲ N2(κ+1)λ2ν forλ ≲ N− κ+1
7PublishedasaconferencepaperatICLR2024
andsothat|1−h(λ)|≤N− 2(κκ +1)λ− 2κ ν forλ≳N− κ+ν 1. Thevalues ∗ = κ+ν 1 canbereferredto
asthelosslocalizationscalefortheoptimalalgorithm.
LetusapplytheobtainedconditionstoKRRandGF.KRRwithregularizationηhash (λ)= λ .
η λ+η
Supposethatη hasscales ,thenh hasthescalingprofileS(h)(s) = (s−s )∨0,while1−h
η η η η
equals η andhasthescalingprofileS(1−h)(s)=(s −s)∨0.Recallingthatν >1,weseethat
λ+η η
condition1)inProposition4issatisfiediffs ≤s = ν . Condition2)ismoresubtle: if κ ≤1,
η ∗ κ+1 2ν
thenitissatisfiediffs ≥ s ,butinthecase κ > 1itisrathersatisfiediffs ≥ κ . Wesee,
η ∗ 2ν η 2(κ+1)
inparticular,thatinthecase κ ≤ 1conditions1)and2)aresimultaneouslysatisfiediffs = s ,
2ν η ∗
whileinthecase κ > 1theycannotbesimultaneouslysatisfied(andsoKRRcannotachievethe
2ν
optimalscaling κ )–aneffectcalledsaturation(Mathe´,2004;Baueretal.,2007). SeeFigure2
κ+1
for an illustration. In contrast, GF h (λ) = 1−e−tλ has no saturation: choosing t to be of scale
t
−s satisfiesbothconditionsofProposition4forallν >1andκ>0.
∗
5.2 NOISYOBSERVATIONSANDMODELEQUIVALENCE
Forourtwodatamodels, CircleandWishart, theintuitionbehindtheNMNOansatzcanberigor-
ously justified by showing that this ansatz represents the leading contribution to the true loss. For
instance,considerthecirclemodelwithlossfunctional[(11)specifiedby(10). Theempiricaleigen-
valuesλ(cid:98)k = [λ k]
N
= λ
k
+O(N−ν)for|k| < N/2, sothescaleν ofthecorrection|λ(cid:98)k −λ k|is
higherthanthescalesoftheeigenvaluesλ exceptfortheeigenvaluesofthehighestscales = ν.
k
Thisshowsthattheempiricalandpopulationquantitiesaresignificantlydifferentonlyonthehighest
spectralscales=ν. Continuingthislineofarguments,weget
Theorem2. Assumethatthelearningalgorithmh(λ)issuchthath(λ)and1−h(λ)havescaling
profilesS(h)(s)andS(1−h)(s).Assumethatthemapslogλ(cid:55)→log|h(λ)|andlogλ(cid:55)→log|1−h(λ)|
aregloballyLipschitz,uniformlyinN.Then,ifν =sisnotalocalizationpointofNMNOfunctional
L(nmno)[h],Circlemodelspecifiedby(11)andWishartmodelspecifiedby(16),(15)areequivalent
totheNMNOmodelinthelimitN →∞:
L(nmno)[h]=L(circle)[h](cid:0) 1+o(1)(cid:1) =L(wishart)[h](cid:0) 1+o(1)(cid:1)
. (20)
We prove the theorem separately for Circle and Wishart models in Sections D.2 and E.3.3. Note
thattheconditionofequivalenceisspecifiedusingonlyNMNOmodel. Thus,ifsatisfied,itallows
analyzingthesimplefunctional(17)insteadofthemorecomplicatedones(16),(15)and(11). The
requirement that s = ν is not a localization point is reasonable as the heuristic derivation of the
NMNOmodelinSection4includedanassumptionthatthelossislocalizedatmoderatescales.
5.3 NOISELESSOBSERVATIONS
WefocusonCirclemodeltodescribemainobservations,withderivationdeferredtoSectionDand
WishartmodelresultsdeferredtoSectionE.Letuswritethelossfunctionalperturbatively
N
L[h]= (cid:88)2 (cid:104)|c k 2|2 (cid:0) 1+o(τ)(cid:1)(cid:16) h(λ(cid:98)k)−h∗(λ(cid:98)k)(cid:17)2 +N−κ−1(cid:0) O(1)+O(τ2ν−κ−1)(cid:1)(cid:105) (21)
k=−N
2
withτ ≡ |k|+1 asasmallparameter. Fromthis,wemakeseveralobservations. First,takeh = h∗.
N
Then,ifκ<2ν,thelosslocalizesons=ν(i.e.thesumisaccumulatedat|k|∼N)andhastherate
O(N−κ). Thisrateisnaturalandreflectsthatweareabletolearntargetfunctioneverywhereexcept
atinaccessiblescalesλ≪N−ν. Moreover,byexaminingthefirsttermin(21),weseethatthisrate
isnotdestroyedbylearningalgorithmssufficientlyclosetotheoptimal:|h(λ(cid:98)k)−h∗(λ(cid:98)k)|2 =o(τκ).
The situation changes dramatically for κ > 2ν: the optimal loss L[h∗] becomes dominated by
O(τ2ν−κ−1)termin(21)andlocalizesats = 0(i.e. thesumisaccumulatedat|k| ∼ 1),changing
theratetoO(N−2ν).WecallthisbehaviorsaturationsinceithasfeaturessimilartoKRRsaturation
effect for noisy observations: transition at κ = 2ν; change of error rate; localization at s = 0.
However, noisy KRR saturation is algorithm-driven and can be removed by replacing KRR with
GF, while saturation in (21) persists even for optimal algorithm h∗(λ). Interestingly, the optimal
8PublishedasaconferencepaperatICLR2024
Generalization error at finite N. =0.1 Constant in the error asymptotic L CN Algorithms profile. =0.25 Algorithms profile. =1.5
12 2.00 2.00
10 1.75 1.75
1.50 1.50
8 1.25 1.25
101 6 1.00 1.00
4 0.75 0.75
0.50 0.50
2 O Sav te ur rle ata ir on ni n tg ra t nr sa in tis oi nti o n = 2= 1 0.25 0.25
6×100 0 0.00 0.00
100 101 102 103 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 10 20 30 40 50 0 10 20 30 40 50
N ×N ×N
Optimally stopped GF Optimally regularized KRR The optimal algorithm Interpolation
Figure 3: Generalization error (left) and profiles h(λ) (right) of various algorithms applied to the
Circlemodelwithν = 1.5andnoiselessobservationswithdifferentκ. Beforeoverlearningtransi-
tionκ = ν −1optimalalgorithmsunderlearnobservations(h(λ) < 1)whilestartingtooverlearn
them(h(λ)>1)afterthetransition. FordetailsandextendeddiscussionseeSectionF.
lossinthesaturatedphasecanbeachieved(asymptotically)byKRRwithanegativeregularization:
L[h∗]=L[h ](1+o(1)), η∗ <0. DenotingRiemannzetafunctionasζ(α),wehave
η∗
L[h ]= 1(cid:16) (ηNν
+2ζ(ν))2+2ζ(2ν)(cid:17)(cid:34) (cid:88)∞
|c
l|2(cid:35)
N−2ν(cid:0) 1+o(1)(cid:1) , (22)
η 2 λ2
l=−∞ l
with minimum at η∗ = −2ζ(ν)N−ν. The benefit of negative regularization was empirically ob-
servedin(Kobaketal.,2020)andtheoreticallyapproachedin(Tsigler&Bartlett,2023).Withinour
framework, KRR with negative regularization can be thought of as a special case of overlearning
theobservations.
Infact,overlearningalsooccursintheκ<2νphase.InSectionD.3weshowthatthelossfunctional
canbeasymptoticallywritten(see(138))usingτ = |k|+1 for(continuous)eigenspaceindexing,for
N
exampleλ(cid:98)τ ←λ(cid:98)k.Then,denotingHurwitzzetafunctionasζ(α,x),theoptimalalgorithmbecomes
ζ(ν+κ+1)ζ(ν)
h∗(λ(cid:98)τ)= ζτ (κ+1)ζ(2τ
ν)
, ζ x(α) ≡ζ(α,x)+ζ(α,1−x). (23)
τ τ
The optimal algorithm (23) has an intriguing property (see also Figure 3): it interpolates the data
h∗(λ) = 1 when κ = ν − 1, and, otherwise, the sign of 1 − h∗(λ) coincides with the sign of
ν−1−κ. Inotherwords,forhardtargetswithκ < ν−1,classicalregularizationbyunderfitting
the observation is required for optimal performance. But, for easier targets with κ > ν − 1 it
becomesoptimaltooverlearnthetrainingdataincontrasttoconventionalwisdom. Thesameholds
forWishartmodel(seeSec.E.3.4).Thus,weidentifythepointκ=ν−1asoverlearningtransition.
6 DISCUSSION
Wehaveextendedresultsoftype(1)togeneralspectralalgorithms,asgivenbyourlossfunctional
(11) for Circle model and (15),(16) for Wishart model. It allows to address questions that require
goingbeyondspecific(KRR,GF)algorithms. Forexample,weshowthatthenatureofsaturationat
κ = 2ν isdifferentfornoisyandnoiselessobservations,withthelatterbeinganintrinsicproperty
ofthegivenkernelanddatamodel,andcannotberemovedbyanychoiceofthelearningalgorithm.
Our formalism of spectral localization and scaling, while being compact, provides a simple and
transparentpictureofthevarietyofconvergenceratesunderpower-lawspectradistributions. Also,
the equivalence result between our two data models and naive model of noisy observations (17)
reliesonthestraightforwardestimationofthescaleofperturbationofpopulationquantitiesbyfinite
sizeN ofthetrainingdataset. Thus,aninterestingdirectionforfutureresearchwouldbetocheck
whethertheequivalenceholdsforotherdataandkernelmodels.
Finally, let us mention the advantage of full loss asymptotic L = CN−#(1+o(1)) compared to
the rates L = O(N−#). In this work, we used the full asymptotic to obtain the shape of optimal
algorithm h∗(λ). In the noiseless case, the knowledge of h∗(λ) allowed us to characterize the
overlearning transition at κ = ν −1, which otherwise would be invisible on the level of the rate
O(N−κ). Investigating whether κ = ν −1 remains the point of overlearning phase transition for
moregeneraldatamodelsisaninterestingdirectionforfutureresearch.
9
C )(h )(hPublishedasaconferencepaperatICLR2024
ACKNOWLEDGMENTS
WethankEricMoulinesforinsightfuldiscussionsduringtheinitialstageofthework.
REFERENCES
Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco. On regularization algorithms in learning
theory. Journalofcomplexity,23(1):52–72,2007.
Daniel Beaglehole, Mikhail Belkin, and Parthe Pandit. On the inconsistency of kernel ridgeless
regression in fixed dimensions. SIAM Journal on Mathematics of Data Science, 5(4):854–872,
2023. doi: 10.1137/22M1499819. URLhttps://doi.org/10.1137/22M1499819.
BlakeBordelon,AbdulkadirCanatar,andCengizPehlevan. Spectrumdependentlearningcurvesin
kernelregressionandwideneuralnetworks. InInternationalConferenceonMachineLearning,
pp.1024–1034.PMLR,2020.
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model align-
ment explain generalization in kernel regression and infinitely wide neural networks. Na-
ture Communications, 12(1), may 2021. doi: 10.1038/s41467-021-23103-1. URL https:
//doi.org/10.1038%2Fs41467-021-23103-1.
AndreaCaponnettoandErnestoDeVito. Optimalratesfortheregularizedleast-squaresalgorithm.
FoundationsofComputationalMathematics,7:331–368,2007.
LenaicChizat,EdouardOyallon,andFrancisBach. Onlazytrainingindifferentiableprogramming.
Advancesinneuralinformationprocessingsystems,32,2019.
Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova´. Generalization error rates
in kernel regression: The crossover from the noiseless to noisy regime. Advances in Neural
InformationProcessingSystems,34:10131–10143,2021.
Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy,
and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape
geometryandthetimeevolutionoftheneuraltangentkernel. InAdvancesinNeuralInformation
ProcessingSystems,volume33,pp.5850–5861,2020.
ArthurJacot,FranckGabriel,andClementHongler. Neuraltangentkernel: Convergenceandgen-
eralizationinneuralnetworks. InS.Bengio, H.Wallach, H.Larochelle, K.Grauman, N.Cesa-
Bianchi,andR.Garnett(eds.),AdvancesinNeuralInformationProcessingSystems,volume31.
Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_
files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf.
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clement Hongler, and Franck Gabriel. Ker-
nel alignment risk estimator: Risk prediction from training data. In H. Larochelle,
M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural In-
formation Processing Systems, volume 33, pp. 15568–15578. Curran Associates, Inc.,
2020a. URL https://proceedings.neurips.cc/paper_files/paper/2020/
file/b367e525a7e574817c19ad24b7b35607-Paper.pdf.
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clement Hongler, and Franck Gabriel. Implicit
regularizationofrandomfeaturemodels.InHalDaume´IIIandAartiSingh(eds.),Proceedingsof
the37thInternationalConferenceonMachineLearning,volume119ofProceedingsofMachine
LearningResearch,pp.4631–4640.PMLR,13–18Jul2020b.URLhttps://proceedings.
mlr.press/v119/jacot20a.html.
HuiJin,PradeepKr.Banerjee,andGuidoMontufar.Learningcurvesforgaussianprocessregression
with power-law priors and targets. In International Conference on Learning Representations,
2022. URLhttps://openreview.net/forum?id=KeI9E-gsoB.
Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez. The optimal ridge penalty for real-world
high-dimensionaldatacanbezeroornegativeduetotheimplicitridgeregularization. Journalof
MachineLearningResearch,21(1):6863–6878,2020.
10PublishedasaconferencepaperatICLR2024
DmitryKopitkovandVadimIndelman. Neuralspectrumalignment: Empiricalstudy. InArtificial
NeuralNetworksandMachineLearning–ICANN2020: 29thInternationalConferenceonArtifi-
cialNeuralNetworks,Bratislava,Slovakia,September15–18,2020,Proceedings,PartII29,pp.
168–179.Springer,2020.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Journal of Statistical Mechanics: Theory and Experiment, 2020(12):
124002, dec 2020. doi: 10.1088/1742-5468/abc62b. URL https://doi.org/10.1088%
2F1742-5468%2Fabc62b.
Yicheng Li, Haobo Zhang, and Qian Lin. On the saturation effect of kernel ridge regression. In
The Eleventh International Conference on Learning Representations, 2023. URL https://
openreview.net/forum?id=tFvr-kYWs_Y.
Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, and Volkan Cevher. Optimal rates for spectral
algorithms with least-squares regression over hilbert spaces. Applied and Computational Har-
monic Analysis, 48(3):868–890, 2020. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.
2018.09.009. URL https://www.sciencedirect.com/science/article/pii/
S1063520318300174.
PhilipM.Long. Propertiesoftheafterkernel. ArXiv,abs/2105.10585,2021.
WesleyMaddox,ShuaiTang,PabloMoreno,AndrewGordonWilson,andAndreasDamianou. Fast
adaptationwithlinearizedneuralnetworks. InInternationalConferenceonArtificialIntelligence
andStatistics,pp.2737–2745.PMLR,2021.
Peter Mathe´. Saturation of regularization methods for linear ill-posed problems in hilbert spaces.
SIAMjournalonnumericalanalysis,42(3):968–973,2004.
YuriiNesterov. Introductorylecturesonconvexoptimization: Abasiccourse,volume87. Springer
Science&BusinessMedia,2003.
LoucasPillaud-Vivien,AlessandroRudi,andFrancisBach. Statisticaloptimalityofstochasticgra-
dientdescentonhardlearningproblemsthroughmultiplepasses.AdvancesinNeuralInformation
ProcessingSystems,31,2018.
AbhishakeRastogiandSivananthanSampath. Optimalratesfortheregularizedlearningalgorithms
undergeneralsourcecondition. FrontiersinAppliedMathematicsandStatistics,3:3,2017.
James B Simon, Madeline Dickens, Dhruva Karkada, and Michael Deweese. The eigenlearning
framework: Aconservationlawperspectiveonkernelridgeregressionandwideneuralnetworks.
TransactionsonMachineLearningResearch,2023.
StefanoSpigler,MarioGeiger,andMatthieuWyart. Asymptoticlearningcurvesofkernelmethods:
empirical data versus teacher–student paradigm. Journal of Statistical Mechanics: Theory and
Experiment, 2020(12):124001, dec 2020. doi: 10.1088/1742-5468/abc61d. URL https://
dx.doi.org/10.1088/1742-5468/abc61d.
AlexanderTsiglerandPeterLBartlett. Benignoverfittinginridgeregression. JournalofMachine
LearningResearch,24:123–1,2023.
NikhilVyas,YaminiBansal,andPreetumNakkiran.Empiricallimitationsofthentkforunderstand-
ingscalinglawsindeeplearning. TransactionsonMachineLearningResearch,2023.
AlexanderWei,WeiHu,andJacobSteinhardt.Morethanatoy:Randommatrixmodelspredicthow
real-worldneuralrepresentationsgeneralize. InInternationalConferenceonMachineLearning,
pp.23549–23588.PMLR,2022.
11PublishedasaconferencepaperatICLR2024
CONTENTS
1 Introduction 1
2 Setting 3
3 Spectralalgorithmsandtheirgeneralizationerror 4
4 Explicitformsofthelossfunctional 5
5 Resultsunderpower-lawspectraldistributions 7
5.1 ScalinganalysisanditsapplicationtotheNMNOmodel . . . . . . . . . . . . . . 7
5.2 Noisyobservationsandmodelequivalence . . . . . . . . . . . . . . . . . . . . . . 8
5.3 Noiselessobservations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
6 Discussion 9
References 10
A Spectralperspectives 13
A.1 Populationperspective: transfermatrix . . . . . . . . . . . . . . . . . . . . . . . . 13
A.2 Empiricalperspective: learningmeasure . . . . . . . . . . . . . . . . . . . . . . . 14
A.3 Jointpopulation-empiricalperspective: transfermeasure . . . . . . . . . . . . . . 15
B Gradient-basedalgorithms 16
B.1 Kernelformofpredictors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B.2 ImplementingspectralalgorithmswithapairofGradientFlows . . . . . . . . . . 18
C Scalingstatements 18
D Circlemodel 20
D.1 Lossfunctional . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
D.2 Power-lawansatz: noisyobservations . . . . . . . . . . . . . . . . . . . . . . . . 23
D.3 Power-lawansatz: noiselessobservations . . . . . . . . . . . . . . . . . . . . . . 25
D.3.1 Perturbativeexpansionofthelossfunctional . . . . . . . . . . . . . . . . 25
D.3.2 Optimallyscaledlearningalgorithms . . . . . . . . . . . . . . . . . . . . 26
D.3.3 Saturatedphaseκ>2ν . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
D.3.4 Non-saturatedphaseκ<2ν . . . . . . . . . . . . . . . . . . . . . . . . . 30
D.3.5 Theoverlearningtransitionpoint . . . . . . . . . . . . . . . . . . . . . . . 30
E Wishartmodel 31
E.1 Resolvent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
E.1.1 Computingtheresolventmoments . . . . . . . . . . . . . . . . . . . . . . 33
12PublishedasaconferencepaperatICLR2024
E.2 Lossfunctional . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
E.3 Power-lawansatz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
E.3.1 Solvingfixedpointequation . . . . . . . . . . . . . . . . . . . . . . . . . 40
E.3.2 Learningmeasures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
E.3.3 Noisyobservations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
E.3.4 Noiselessobservations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
F Experiments 47
F.1 Figure1: detailsanddiscussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
F.2 Figure3: detailsanddiscussion. . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
F.3 CosineWishartmodel. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
A SPECTRAL PERSPECTIVES
Inthissection,weshowgeneralrelationsbetweengeneralizationerror(3),populationspectraλ ,c ,
l l
and learning algorithms h(λ). Recall that profile h(λ) is applied to the eigenvalues of the kernel
matrix K (i.e. values of the kernel function evaluated on the training dataset D ). Therefore,
N
relatinggeneralizationerrorandh(λ)involvesthepropertiesoftheempiricalspectrum:eigenvalues
ofKandtherelatedeigendecompositionoftheobservationvectory.
With these remarks in mind, we may say that we are dealing with population and empirical per-
spectives on generalization error (3). We start with population perspective in Section A.1, which
isbehindtheclassicalresult(1). Then, weproceedwiththeempiricalperspectiveinSectionA.2,
whichbasicallyamountstoprovingProposition1andintroducinglearningmeasuresρ(2),ρ(1),ρ(ε).
Finally, In Section A.3, we combine population and empirical perspectives. While probably less
conceptualthanthefirsttwoperspectives,thejointpopulation-empiricalperspectiveisanessential
stepinourderivationofthelossfunctionalfortheWishartmodel.
A.1 POPULATIONPERSPECTIVE: TRANSFERMATRIX
Acentralobjectforthepopulationperspectiveisthetransfermatrix T(cid:98)ll′ introducedexplicitly, for
example, in(Simonetal.,2023)inthecontextofKRR.Specifically, letusdecomposethepredic-
(cid:80)
tion(7)overkerneleigenfunctionsϕ l(x)asf(cid:98)(x) = l(cid:98)c lϕ l(x). Then,thepredictioncoefficients
c canbewrittenas
(cid:98)l
(cid:98)c
l
=(cid:88) T(cid:98)ll′c
l′
+σε (cid:98)l, T(cid:98)ll′ =λ lϕT
l
K−1h(cid:0) N1K(cid:1) ϕ l′, ε
(cid:98)l
=λ lϕT
l
K−1h(cid:0) N1K(cid:1) ε, (24)
l′
whereϕ isthevectorofeigenfunctionscomputedatthedatasetinputs(ϕ ) =ϕ (x ),and(ε) =
l l i l i i
ε
i
isthevectorofobservationnoise. NotethatthetransfermatrixT(cid:98)ll′ hasaclearinterpretationof
the rate at which the information c contained in spectral component l′ is transferred to spectral
l′
component l. The population noise component ε describes how much of the of the observation
(cid:98)l
noiseεwaslearnedinthel-thpopulationspectralcomponent.
Thepopulationloss(3)(sometimesweusethistermasasynonymtogeneralizationerror)isstraight-
forwardlyexpressedthroughthefirstandsecondmomentsofthetransfermatrix, andthevariance
ofpopulationnoisecomponents
L = 1 E (cid:88)(cid:0) c −c (cid:1)2 = 1 (cid:88) c (cid:0)(cid:88) T(2) −2T(1) +δ (cid:1) c + σ2 (cid:88) ε(2), (25)
f(cid:98) 2 DN,ε (cid:98)l l 2 l1 l1l′l′l2 l1l2 l1l2 l2 2N l
l l1,l2 l′ l
13PublishedasaconferencepaperatICLR2024
where
(cid:104) (cid:105)
T l( l1 ′) =E
DN
T(cid:98)ll′ , (26)
(cid:104) (cid:105)
T l( 12 l)
1′l 2′l2
=E
DN
T(cid:98)l 1′l1T(cid:98)l
2′l2
, (27)
ε(2) =NE [(ε )2]=E
(cid:20)
λ2
1 ϕT(cid:16)(cid:0)1K(cid:1)−1 h(cid:0)1K(cid:1)(cid:17)2
ϕ
(cid:21)
. (28)
l DN,ε (cid:98)l DN l N l N N l
The representation (25) makes the most explicit dependence of the loss on population coefficients
c , while the dependence on the learning algorithm h(λ) and population eigenvalues λ is hidden
l l
inside moments T(1), T(2) of the transfer matrix and noise variance ε(2). Yet, for the case of
ll′ l1l 1′l 2′l2 l
KRRtheresults(1)showsthatthedependenceonλ canbemadefairlyexplicit.
l
A.2 EMPIRICALPERSPECTIVE: LEARNINGMEASURE
Asthisperspectivefocusesonempiricalspectrumofkernelmatrixandobservationvector,westart
withwritingeigendecompositionofK,f∗ =y−σεandεas
N
1 (cid:88)
NK= λ(cid:98)ku kuT k, uT ku
k′
=δ kk′, (29)
k=1
N
1 (cid:88)
√ f∗ = c u , (30)
(cid:98)k k
N
k=1
N
(cid:88)
ε= ε u , (31)
(cid:98)k k
k=1
whereinthelastlineε arei.i.d. normalGaussianbecauseorthogonaltransformationtoempirical
(cid:98)k
eigenbasis{u }N leavesthedistributionofisotropicGaussianvectorsε∼N(0,I)unchanged.
k k=1
Then, inserting spectral decomposition (29) of the empirical kernel matrix into the prediction (7)
gives
(cid:34) N (cid:35) N
f(cid:98)h(x)=(cid:16) k(x)(cid:17)T N1 k(cid:88) =1u kuT
k
h( λ(cid:98)λ(cid:98) kk) f∗+ Nσ k(cid:88) =1ε (cid:98)kh( λ(cid:98)λ(cid:98) kk)(cid:0) k(x)(cid:1)T u
k
(32)
(cid:90) σ (cid:90)
= h(λ)ρ(f)(x,dλ)+ √ h(λ)ρ(ε)(x,dλ),
(cid:98) (cid:98)
N
whereρ(f)(x,dλ)andρ(ε)(x,dλ)aretargetandnoiselearningmeasures:
(cid:98)N (cid:98)N
ρ(f)(x,dλ)=(cid:0) k(x)(cid:1)T (cid:34) 1 (cid:88)N u kuT k δ (cid:35) f∗, (33)
(cid:98) N λ λ(cid:98)k
k=1
ρ(ε)(x,dλ)=
√1 (cid:88)N
ε
(cid:0) k(x)(cid:1)T u
k δ . (34)
(cid:98) N (cid:98)k λ λ(cid:98)k
k=1
The target learning measure defines what pattern is learned from the target function at the neigh-
borhood dλ of the empirical spectral position λ. Similarly, the noise learning measure defines the
patternsofthenoiselearnedintheneighborhoodofλ.
14PublishedasaconferencepaperatICLR2024
As for the population perspective, we substitute the expression of prediction in terms of learning
measures(32)intothepopulationloss(3)
L f(cid:98)=1 2E DN,ε(cid:34)(cid:13) (cid:13) (cid:13) (cid:13)(cid:90) h(λ)ρ (cid:98)(f)(x,dλ)+ √σ
N
(cid:90) h(λ)ρ (cid:98)(ε)(x,dλ)−f∗(x)(cid:13) (cid:13) (cid:13) (cid:13)2(cid:35)
(cid:34)
=1 (cid:90) (cid:90) h(λ )h(λ )E (cid:10) ρ(f)(x,dλ ),ρ(f)(x,dλ )(cid:11)
2 1 2 DN (cid:98) 1 (cid:98) 2
(cid:90)
−2 h(λ)E (cid:10) f∗(x),ρ(f)(x,dλ)(cid:11) +⟨f∗(x),f∗(x)⟩ (35)
DN (cid:98)
σ (cid:28) (cid:90) (cid:90) (cid:29)
+2√ E f∗(x)− h(λ)ρ(f)(x,dλ), h(λ)E ρ(ε)(x,dλ)
N
DN ε
(cid:35)
+
σ2 (cid:90) (cid:90)
h(λ )h(λ )E (cid:10) ρ(ε)(x,dλ ),ρ(ε)(x,dλ )(cid:11) .
N 1 2 DN,ε (cid:98) 1 (cid:98) 2
Now, observe that the term in the second-to-last line in (35) is linear in noise learning measure
(cid:0) (cid:1)T
averagedoverεwhichiszero:E ερ(ε)(x,dλ)= √1
N
(cid:80)N
k=1
k(x)
λ
ukδ λ(cid:98)kE εε
(cid:98)k
=0sinceE εε
(cid:98)k
=0.
Similarly,takingtheexpectationoverobservationnoiseεhelpstosimplifythelastterm
E (cid:10) ρ(ε)(x,dλ ),ρ(ε)(x,dλ )(cid:11) = (cid:88) δ (dλ )δ (dλ )⟨uT k1k(x),uT k2k(x)⟩ E ε ε
ε (cid:98) 1 (cid:98) 2 λ(cid:98)k1 1 λ(cid:98)k2 2 Nλ 1λ
2
ε(cid:98)k1(cid:98)k2
k1,k2
(36)
1 (cid:88) ∥uTk(x)∥2
=δ (dλ ) δ (dλ ) k ,
λ1 2 N λ(cid:98)k 2 λ2
k
wherewehaveusedEε ε = δ . Now,onecanrecognizethelossfunctionalstatedinPropo-
(cid:98)k1(cid:98)k2 k1k2
sition1: thefirst3termsandthelasttermof(35)correspondtotherespectivetermsof(8). Inother
words,thelearningmeasuresannouncedinProposition1aregivenby
ρ(1)(dλ)=E
(cid:104)(cid:10) f∗(x),ρ(f)(x,dλ)(cid:11)(cid:105)
, (37)
DN (cid:98)
ρ(2)(dλ ,dλ )=E
(cid:104)(cid:10)
ρ(f)(x,dλ ),ρ(f)(x,dλ
)(cid:11)(cid:105)
, (38)
1 2 DN (cid:98) 1 (cid:98) 2
(cid:34)
1
(cid:88)N (cid:13) (cid:13)uTk(x)(cid:13) (cid:13)2 (cid:35)
ρ(ε)(dλ)=E k δ , (39)
DN N λ2 λ(cid:98)k
k=1
Again,thelossfunctional(8)representstheempiricalperspectiveonthegeneralizationerror,mak-
ingthedependenceonthelearningalgorithmh(λ)veryexplicit. But,thedependenceontheprob-
lem’skernelstructureandtargetfunctionishiddeninsidemeasuresρ(1)(dλ)andρ(2)(dλ ,dλ ).
1 2
A.3 JOINTPOPULATION-EMPIRICALPERSPECTIVE: TRANSFERMEASURE
Tocombinetoperspectivedescribedabove,consideral-thspectralcomponentoflearningmeasure
ρ(f)(dλ)≡⟨ϕ (x),ρ(f)(x,dλ)⟩.Then,insertingdecomposition(4)ofthetargetfunctionintotarget
(cid:98)l l (cid:98)
learningmeasure(33)allowstowrite
ρ(f)(dλ)≡⟨ϕ (x),ρ(f)(x,dλ)⟩=(cid:88) c ρ(f)(dλ), (40)
(cid:98)l l (cid:98) l′(cid:98)ll′
l′
where
N
ρ(f)(dλ)= λ l 1 (cid:88)(cid:0) ϕTu (cid:1)(cid:0) uTϕ (cid:1) δ (41)
(cid:98)ll′ λ N l k k l′ λ(cid:98)k
k=1
canbenaturallycalledatransfermeasure. Now,weinsertdecomposition(40)into(37)and(38),as
wellaspopulationeigendecomposition(4)into(39). Thescalarproductsin(37)and(38)become
(cid:10) f∗(x),ρ(f)(x,dλ)(cid:11) =(cid:88) c ρ(f)(dλ)=(cid:88) c c ρ(f)(dλ),
(cid:98) l(cid:98)l l l′(cid:98)ll′
l l,l′
(42)
⟨ρ(f)(x,dλ ),ρ(f)(x,dλ )(cid:11) =(cid:88) ρ(f)(dλ )ρ(f)(dλ )= (cid:88) c c ρ(f)(dλ )ρ(f)(dλ ).
(cid:98) 1 (cid:98) 2 (cid:98)l 1 (cid:98)l 2 l1 l2(cid:98)ll1 1 (cid:98)ll2 2
l l,l1,l2
15PublishedasaconferencepaperatICLR2024
Asforthenorm(cid:13) (cid:13)uTk(x)(cid:13) (cid:13)2
in(39),wecanuse
k
(cid:68) k(x),(cid:0) k(x)(cid:1)T(cid:69) =(cid:88) λ2ϕ ϕT. (43)
l l l
l
Combining the expressions above and noting that ∥f∗(x)∥2 = (cid:80) c2 gives yet another represen-
l l
tation of the population loss in terms of the first and second moment of the transfer measure and
populationdecompositionofnoisevariancemeasure.
L[h]= 1 (cid:88) c (cid:16)(cid:90) h(λ )h(λ )(cid:88) ρ(2) (dλ ,dλ )−2(cid:90) h(λ)ρ(1) (dλ)+δ (cid:17) c (44)
2 l1 1 2 l1l′l′l2 1 2 l1l2 l1l2 l2
l1,l2 l′
+
σ2 (cid:88)(cid:90)
h(λ)2ρ(ε)(dλ), (45)
2N l
l
where
(cid:104) (cid:105)
ρ(1)(dλ)=E ρ(f)(dλ) , (46)
ll′ DN (cid:98)ll′
(cid:104) (cid:105)
ρ(2) (dλ ,dλ )=E ρ(f)(dλ )ρ(f)(dλ ) , (47)
l1l 1′l 2′l2 1 2 DN (cid:98)l 1′l1 1 (cid:98)l 2′l2 2
ρ(ε)(dλ)=E (cid:34) λ2 l 1 (cid:88)N (cid:0) ϕTu (cid:1)2 δ (cid:35) . (48)
l DN λ2N l k λ(cid:98)k
k=1
If the moments of transfer measure ρ(1)(dλ), ρ(2) (dλ ,dλ ) and noise variance measure
ll′ l1l 1′l 2′l2 1 2
ρ(ε)(dλ)areknown,therepresentation(44)connectsspectraldistributionλ ,c oftheproblemand
l l l
learningalgorithmh(λ)withthepopulationloss,thusjustifyingthenamejointpopulation-empirical
spectralperspective.
B GRADIENT-BASED ALGORITHMS
The purpose of this section is two-fold. First, in Section B.1, we support our examples of h(λ)
providedinSection3withtherespectivederivations. Thisamountstoshowthatforlinearmodels
trained with a gradient-based algorithm, the predictor during optimization can be written in the
form (7) with a specific choice of h(λ). Second, in Section B.2 try to connect general spectral
algorithmsspecifiedbysomeprofileh(λ)withgradient-basedoptimization,whichwasnotincluded
inthemainpaperduetothespaceconstraints. Forthat,weprovideasimpleconstructionbasedon
apairofGFprocesses.
B.1 KERNELFORMOFPREDICTORS
Toconsidergradient-basedoptimizationforthekernelmethodsettingdiscussedinthemainpaper,
weneedtointroducealinearparametricmodelf(cid:98)(w,x)whoseparameterswwillbeupdatedduring
theoptimizationprocess. StartingwithakernelK(x,x′)withpopulationdecomposition(4),letus
√
define the model features ψ (x) = λ ϕ (x). Then, combining the features in a vector ψ(x) ∈
l l l
RP, (cid:0) ψ(x)(cid:1) =ψ (x),thelinearmodelisdefinedas
l l
f(cid:98)(w,x)=⟨w,ψ(x)⟩, w∈RP. (49)
For positive definite kernels P = ∞, and both model’s features and parameters belong to RKHS
H ofthekernelK: w,ψ(x)∈H .
K K
The (neural) tangent kernel (Jacot et al., 2018) of the model (49) is given by NTK (x,x′) =
f(cid:98)
⟨ψ(x),ψ(x′)⟩ = (cid:80) λ ϕ (x)ϕ (x′) = K(x,x′), thus reproducing our original kernel we have
l l l l
started with. Note that one can go in the opposite direction: start from the linear (49) and then
define a kernel method specified by the tangent kernel (NTK) of the linear model. An especially
interestingexampleofthelatterdirectionisa(non-linear)neuralnetworkf(θ,x)linearizedatθ
0
resulting in f (θ,x) = f(θ ,x)+⟨θ −θ ,∇ f(θ ,x)⟩. If constant prediction f(θ ,x) is ig-
lin 0 0 θ 0 0
nored,thelinearizedneuralnetworkisalsodescribedby(49)withgradientsasthemodelfeatures
ψ(x)=∇ f(θ ,x)andthedisplacementfromθ asmodelparametersw=θ−θ .
θ 0 0 0
16PublishedasaconferencepaperatICLR2024
To finalize the connection between the parameter-based setting and kernel-based setting from the
mainpaper,linearmodel(49)needstobetrainedbyminimizingquadraticlossontraindatasetD
N
N
1 (cid:88)(cid:0) (cid:1)2
L DN(w)≡
2N
f(cid:98)(w,x i)−y
i
i=1
(50)
N
= 1 (cid:88)(cid:0) ⟨w,ψ(x )⟩−⟨w∗,ψ(x )⟩(cid:1)2 = 1 (w−w∗)TH(w−w∗),
2N i i 2
i=1
where H = 1 (cid:80)N ψ(x )⊗ψ(x ) is the Hessian of the train loss. In the following, it will be
N i=1 i i
(cid:0) (cid:1)
convenienttodenoteΨthematrixoffeaturescalculatedonthetrainingdataset Ψ =ψ (x ),and
li l i
usefinite-dimensionalnotationforinnerandouterproductintheparameterspace: i.e. theHessian
H= 1ΨΨT andempiricalkernelmatrixK=ΨTΨ. In(50),wehaveassumedthatthereexistsa
N
parametervaluew∗sothatthemodel(49)completelyfitstheobservationsΨTw∗ =y.Considering
thetypicalcaseP >N,thisamountstoafeaturematrixhavingfullrankrank(Ψ)=N1.
Now, let us proceed with showing how gradient-based optimization fits into the family of spectral
algorithmsgivenby(7). WestartwiththebasicexampleofvanillaGradientDescentwithlearning
rateα,havingparameterupdaterulew =w −α∇ L (w ). Forthequadraticloss(50),this
t+1 t w DN t
reducestow =w −αH(w −w∗)=w∗+(I−αH)(w −w∗),or,equivalently,
t+1 t t t
w −w∗ =(I−αH)t(w −w∗)=p (H)(w −w∗). (51)
t+1 0 t 0
Here we introduced the polynomial p (λ) = (1 − αλ)t that will prove a useful notation in the
t
followingandisrelatedtotheprofileh (λ)asp (λ) = 1−h (λ). Toobtaintherepresentation(7)
t t t
forthelearnedpredictionf(cid:98)t(x)=⟨w t,ψ(x)⟩weadditionallyneedtosetw
0
=0. Then,
f(cid:98)t(x)=⟨w∗+p t(H)(w 0−w∗),ψ(x)⟩=(cid:10)(cid:0) I−p t(H)(cid:1) w∗,ψ(x)(cid:11) =⟨h t(H)w∗,ψ(x)⟩ (52)
Next, note that polynomial p (λ) is often called residual polynomial due to its normalization at
t
λ=0asp (0)=1,orequivalentlyh (0)=0. Thelatterimpliesthatwecanwriteh (λ)=λq (λ)
t t t t
withsomepolynomialq (λ)ofdegreet−1.UsinganalgebraicidentityJTJq(JTJ)=JTq(JJT)J
t
forarbitrarymatrixJandpolynomialqallowsustofinallyobtain(7)
1
f(cid:98)t(x)=⟨Hq t(H)w∗,ψ(x)⟩= N⟨ΨΨTq t( N1ΨΨT)w∗,ψ(x)⟩
1
= ⟨Ψq (1ΨTΨ)ΨTw∗,ψ(x)⟩ (53)
N t N
( =1) k(x)T 1 q (K)y=k(x)TK−1 1Kq (1K)y=k(x)TK−1h (1K)y,
N t N N t N t N
wherein(1)wehaveusedthatΨTw∗ =yand⟨Ψ,ψ(x)⟩=k(x)T. Thus,wehaveshownthatfor
GDwithlearningrateαrepresentation(7)holdswithh(λ)=1−(1−αλ)t.
The argument above can be easily extended to the case of Gradient Flow (GF). First note that un-
der GF dynamics dw = −∇L (w ) the parameters are w −w∗ = e−Ht(w −w∗), thus
dt t DN t t 0
implying p (λ) = e−λt and h (λ) = 1 − e−λt. Then, for q (λ) = ht(λ) = 1−e−λt we also
t t t λ λ
have JTJq(JTJ) = JTq(JJT)J, which can be seen, for example, by from the Taylor expansion
q
(λ)=t(cid:80)∞ (−tλ)n
. Therestoftheargumentisunchanged.
t n=0 (n+1)!
Gradient descent is also easily extended to arbitrary first-order iterative optimization algorithms.
Forallsuchalgorithms,theparameterchangew −w oniterationtbelongstoanordertKrylov
t 0
subspace:w −w ∈span{H(w −w∗),H2(w −w∗),...,Ht(w −w∗)}(see,e.g. (Nesterov,
t 0 0 0 0
2003),page42). Thisisequivalenttosayingthatw −w∗ =p (H)(w −w∗)withp (λ)being
t+1 t 0 t
anarbitraryresidualpolynomial(i.e. normalizedasp (0) = 1). SinceinourGDargumentwedid
t
notuseanypropertyofitsp (λ)exceptforresidualnormalization,theargumentcontinuestohold,
t
makingtherepresentation(7)forallfirst-orderiterativeoptimizationalgorithms.
1Asimilarassumptionwasimplicitlymadeinthemainpaperinthedefinitionofthespectralalgorithm(7).
Indeed,theexistenceofinverseK−1 alsorequiresrank(Ψ) = N. Thisisanaturalassumptionforpositive
definitekernelsK: empiricalkernelmatrixhasfullrankifevaluatedonasetofdistinctinputsx ,whichin
i
turnhappensalmostsurelyfortypicalgenerationprocessesofD suchasi.i.d. drawnx . Inprinciple,one
N i
canalsoconsiderthecaseofnon-fullrankofK, oralternativelynon-existenceofw∗ completelyfittingthe
observations,andreplaceK−1in(7)withpseudoinverse.Forsimplicity,weleavesuchcasestofuturework.
17PublishedasaconferencepaperatICLR2024
B.2 IMPLEMENTINGSPECTRALALGORITHMSWITHAPAIROFGRADIENTFLOWS
RecallthatinthesectionabovewegetGDresidualp (λ)=1−h (λ)=e−λt. Analternativeway
t t
togetthiswouldbetodeclarew −w∗ =p (H)(w −w∗)andthenrewriteoriginalGradientFlow
t t 0
ODE dw =−∇ L (w )intermsoftheresidualp (λ)as
dt t w DN t t
∂ p (λ)=−λp (λ), p (λ)=1, (54)
t t t 0
withanimmediatesolutionp (λ)=e−λt.
t
Now,supposewearegivensometargetprofile(cid:101)h(λ)withrespectiveresidualp (cid:101)(λ) = 1−(cid:101)h(λ) ̸= 0
that needs to be implemented. Our strategy is to design such GF process with residual q (λ) that
t
convergetothedesiredprofileatlongtrainingtimesq (λ)≡lim q (λ)=p(λ). ThebasicGF
∞ t→∞ t (cid:101)
processgivenby(54)alwaysconvergestofullinterpolationofthetrainingdata: lim p (λ) =
t→∞ t
lim e−λt = 0. Weproposetoovercomethisinterpolationpropertybyusingtwooptimization
t→∞
processes–thefirstprocessisstandardGF(54)convergingto0,andthesecondGFprocesswilluse
gradientsofthefirstGFtoconvergetop(λ)̸=0. ThesetwoprocessaredefinedbyapairofODEs
(cid:101)
(cid:26)dw
=−∇ L (w ),
d dt ut =−(cid:0) 1w +gD (N t)(cid:1) ∇t
L (w ),
(55)
dt t w DN t
wherew andu aretheparametersofthefirstandthesecondprocessrespectively,andtheinitial
t t
conditionsareassumedtobeidenticalw = u . Associatingresidualsp (λ),q (λ)toparameters
0 0 t t
w,u,ODEsystem(55)isrewrittenas
(cid:26)
∂ p (λ) =−λp (λ),
∂t qt
(λ)
=−(cid:0) 1t +g(t)(cid:1)
λp (λ),
p 0(λ)=q 0(λ)=1. (56)
t t t
Hereg(t)isafunctioncontrollingthefinalsolutionq (λ),andthereforeneedstobechosenbased
∞
onthedesiredsolutionp(λ). Atagivencontrolfunctiong(t)thefinalsolutionq (λ)canbeeasily
(cid:101) ∞
(cid:82)∞
found by integrating the second equation as ∂ q (λ) = q (λ) − q (λ) and substituting the
0 t t ∞ 0
solutionofthebasicGFp (λ) = e−λt into∂ q (λ). Then, settingthefinalsolutiontothedesired
t t t
valueq (λ)=p(λ)leadstoanintegralequationonthecontrolg(t)
∞ (cid:101)
(cid:90) ∞
(cid:0) 1+g(t)(cid:1) λe−λtdt=1−p(λ). (57)
(cid:101)
0
Since(cid:82)∞ λe−λtdt = 1, wecancel1frombothsides, arrivingatLaplacetransformofg(t)onthe
0
left-handside
(cid:90) ∞ p(λ)
g(t)e−λtdt=−(cid:101) . (58)
λ
0
Thus,choosingg(t)asaninverseLaplacetransformof−p(cid:101)(λ) implementsthedesiredspectralalgo-
λ
rithm(cid:101)h(λ).
C SCALING STATEMENTS
In the section, we give rigorous versions of the scaling statements outlined in Section 5.1. In the
endofthesectionwealsoprovideProposition5asadiscreteanalogofProposition2,whichwillbe
requiredfortheCirclemodel.
Intuitivederivation. Beforeproceedingwithrigorousproofs,letusgiveasimpleintuitionbehind
thescaleofsumsandintegralsstatedinPropositions2and5.
We start with the integral case, following notations and assumptions from Proposition 2. To find
(cid:82)1
thescaleoftheintegral |g (λ)|dλ,letusdividetherangeofscaless ∈ [0,a]intomanysmall
aN N
segmentsandlookatthecontributionfromasinglesegment[s ,s +ε],correspondingtotheinterval
0 0
ofeigenvaluesΛ
s0
=[N−ελ 0,λ 0], λ
0
=N−s0. DuetothecontinuityofscalingprofileSg(s)(see
Lemma 1 below), we can neglect the change of the g (λ) on Λ . Approximating the length of
N s0
18PublishedasaconferencepaperatICLR2024
eigenvaluesintervalas|Λ |≈λ ,wecanestimatethecontributiontotheintegralfrom[s ,s +ε]
s0 0 0 0
as
|g (λ)|×|Λ |∼|g (λ)|×λ
∼N−Sg(s0)−s0,
(59)
N s0 N 0
which is exactly the expression under the minimum in Proposition 2. To see that only the scales
which minimize G(s) = Sg(s)+s give a non-vanishing contribution to the final result, take two
scaless ,s suchthatG(s )−G(s )=δ >0. Then,accordingto(59),thecontributionfromscale
1 2 2 1
segment[s ,s +ε]willbeNδ timessmallerthanfromthescalesegment[s ,s +ε],andtherefore
2 2 1 1
willvanishinthelimitN →∞.
We can summarize the above with the following simple heuristic: replace dλ with λ under the
integralandmaximizetheresultingexpressiontogetanestimationoftheintegralscale. Thecase
(cid:80)
ofadiscretesumgoesalongthesamelines, leadingtotheheuristicofreplacingthesum with
k
currentindexk: (cid:80) |g (λ(N))|→k×|g (λ(N))|,andthenmaximizingtheresultingexpression.
k N k N k
Continuityofscalingprofiles.
Lemma1. ThescalingprofileSg(s),ifexists,isacontinuousfunctionofs.
Proof. Suppose that Sg exists but is not continuous. Then there exists s ≥ 0 and a sequence
∗
s →s suchthat|Sg(s )−Sg(s )|>cforallmandsomepositiveconstantc. Supposew.l.o.g.
m ∗ m ∗
thatSg(s ) < Sg(s )−cforallm. Considersomefixedmandchoosesomesequenceλ(m) of
m ∗ N
scales . Inparticular,wethenhave
m
λ(m) <N−sm+1/mandλ(m) >N−sm−1/m (60)
N N
forN >N withsomeN . Bydefinitionofthescalingprofile,wealsohave
m m
|g
(λ(m))|>N−S(g)(sm)−c/2 >N−S(g)(s∗)+c/2
(61)
N N
forN largeenough;sayforN >N withthesameN asbefore. Wecanassumew.l.o.g. thatN
m m m
ismonotoneincreasing. Nowdefinethesequenceλ by
N
λ =λ(m), N <N ≤N . (62)
N N m m+1
Thissequencehasscales ∗,but|g N(λ N)|>N−S(g)(s∗)+c/2forallsufficientlylargeN,contradict-
ingthefactthatthatg (λ )musthavescaleS(g)(s ).
N N ∗
ProofofProposition2. Part1. Letusfirstshowthepartofthestatementthatsaysthatforany
ϵ>0
(cid:90) 1
|g (λ)|dλ=o(N−s∗+ϵ). (63)
N
aN
Supposethatthisisnotthecase,andthereisϵ>0andasubsequenceN suchthat
m
(cid:90) 1
|g (λ)|dλ>N−s∗+ϵ. (64)
N m
aN
Dividetheinterval[0,a]intofinitelymanysubintervalsI =[b ,b ]oflengthlessthanϵ/2. For
r r r+1
eachsubintervalI ,define
r
λ = argmax |g (λ)|. (65)
m,r Nm
λ:−log Nmλ∈Ir
Notethatforeachrthesequencem(cid:55)→−log λ takesvaluesinthecompactintervalI ,soit
hasalimitpoints∗ ∈ I . BygoingtoasubseN qum encm e, ,r wecanassumew.l.o.g. thatthelimitpr ointis
r r
unique,i.e. isthelimit. Thenwecandefineforeachrthesequenceλ(r) bysettingλ(r) = λ if
N N m,r
N = N andsomehowcomplementingitforN ̸= N sothatthesequenceλ(r) hasscales∗. By
m m N r
scalingassumption,wethenhave
g N(λ( Nr))=o(N−S(g)(s∗ r)+ϵ/2) (66)
andinparticular
−S(g)(s∗)+ϵ/2
g Nm(λ m,r)=o(N m r ). (67)
19PublishedasaconferencepaperatICLR2024
Bydefinitionofλ ,
m,r
(cid:90) 1 (cid:90) 0 dN−q (cid:88)
|g (λ)|dλ= |g (N−q)| m dq ≤ g (λ )N−br. (68)
Nm Nm m dq Nm m,r m
aNm −log NmaNm r
Itfollowsthat
(cid:90) 1
|g
Nm(λ)|dλ=o(cid:16)(cid:88)
N
m−S(g)(s∗ r)+ϵ/2
N
m−br(cid:17)
(69)
aN r
(cid:16)(cid:88) −(S(g)(s∗)+s∗)+ϵ(cid:17)
=o N m r r (70)
r
=o(N−s∗+ϵ) (71)
m
contradictingassumption(64).
Part2. Nowweprovetheoppositeinequality:
(cid:90) 1
|g (λ)|dλ=ω(N−s∗−ϵ). (72)
N
aN
Let q = argmin (S(g)(s)+s). By continuity of S(g), there exists an interval I = [q −
∗ 0≤s≤a ∗
δ,q +δ]whereS(g)(s) < S(g)(q )+ϵ/2. ArguingasinPart1,wethendeducefromthescaling
∗ ∗
assumptiononS(g)that
min |g
(λ)|=ω(N−S(g)(q∗)−ϵ).
(73)
N
λ:−log λ∈I
N
Itfollowsthat
(cid:90) 1
|g (λ)|dλ≥(N−q∗+δ−N−q∗−δ) min |g (λ)| (74)
N N
aN λ:−log Nmλ∈I
=ω(N−S(g)(q∗)−ϵN−(q∗−δ))
(75)
=ω(N−S(g)(q∗)−q∗−ϵ)
(76)
=ω(N−s∗−ϵ) (77)
asdesired. ThiscompletestheproofofProposition2.
Discrete spectrum. Suppose that {λ(N) ∈ (0,1]}N is a N-dependent, size-N multiset (with
k k=1
possiblyrepeatedelements)suchthatthesequenceoftherespectivedistributionfunctionsF (λ)=
N
|{k ∈ 1,N : λ < λ ≤ 1}|hasascalingprofileS(F)(s). Observe,inparticular,thatintheCircle
k
modelwiththepopulationeigenvaluesλ =(|l|+1)−ν thedistributionoftheempiricaleigenvalues
l
λ(cid:98)k (aswellasofthepopulationeigenvaluesλ
k
forkrestrictedtotheinterval(−N/2,N/2])hasthe
scalingprofileS(F)(s)=−s.
ν
Proposition5. Assumingthatmin{(λ(N))N }=ω(N−a)withsomea>0andS(F)(s)isstrictly
k k=1
monotonedecreasing,thesequenceofsums(cid:80)N |g (λ(N))|hasscales =min (S(g)(s)+
k=1 N k ∗ 0≤s≤a
S(F)(s)).
TheproofofthispropositionisanalogoustotheproofofProposition2.
D CIRCLE MODEL
Here,wegiveallourderivationsrelatedtoCirclemodel.
D.1 LOSSFUNCTIONAL
InthissectionweprovidetheproofofTheorem1.
20PublishedasaconferencepaperatICLR2024
The main technical motivation behind our Circle model is to simplify the empirical kernel matrix
K. Indeed,Kbecomesasymmetriccirculantmatrix
∞
(cid:88) 2πl(i−j)
(K)
ij
= λ lei N . (78)
l=−∞
To establish the relationship between the (complex) eigendecomposition of empirical kernel ma-
trix (29) and observation vector (y) = y on one side, and the population spectra λ ,c ,c on
i i l +,l −,l
theotherside,wewrite
N−1 ∞ N−1
(cid:88) (cid:88) 2π(k+Nn)(i−j) (cid:88) 2πk(i−j)
(K)
ij
= λ k+Nnei N = λ(cid:98)kei N . (79)
k=0 n=−∞ k=0
Thisleadstoempiricaleigenvalues
∞
(cid:88) 1 2πki
λ(cid:98)k = λ k+Nn, (u k)
i
= √ Nei N . (80)
n=−∞
Note that empirical eigenvalues λ(cid:98)k turned out to be non-random, which is a consequence of the
regularityoftrainingdatasetD N. Observethateachempiricaleigenvalueexceptλ(cid:98)0istwicedegen-
erate: λ(cid:98)k = λ(cid:98)N−k for0 < k < N/2. Thisistheconsequenceofthefactthatwetookthekernel
functionK(x−x′)tobeeven.
Turningtothetargetfunction,wehave
(cid:88) 2πi
y i =σε i+ c leil(u+ N ). (81)
l
Therespectiveempiricalcoefficientsinthedecomposition √1 Ny=(cid:80) ky (cid:98)ku
k
are
N−1 ∞
y
(cid:98)k
= N1 (cid:88) y ie−i2π Nki = √σε Nk + (cid:88) c k+Nsei(k+Nn)u. (82)
i=0 n=−∞
Here ε
k
= √1
N
(cid:80) iε ie−i2π Nki ∈ C are complex Gaussian random variables. They are i.i.d. up
to a few dependencies, for example, ε = ε (overline denotes complex conjugation here).
k N−k
Therefore, later we will use the definition of ε in terms of ε to avoid accurate formulation of its
k i
statistics.
Finally, let us use the obtained eigendecomposition of the empirical kernel and target to write an
expressionforthepredictioncomponentsc
(cid:98)l
(cid:98)c
l
= 21
π
(cid:90) 02π f(cid:98)(x)e−ilxdx=λ lN k(cid:88) =− 01 h( λ(cid:98)λ(cid:98) kk) y (cid:98)k(cid:34) N1 N (cid:88) i=− 01 eik2 Nπie−il(u+2 Nπi)(cid:35)
(83)
λ
= λ(cid:98)kl lh(λ(cid:98)kl)e−iluy (cid:98)kl,
where k = l mod N. Note that representations (83) and (82) define transfer measure ρ(f)(dλ)
l (cid:98)ll′
introduced in Section A.3. Due to the regular structure of the training dataset in our setting, the
transfermeasurealsohasaspecificregularstructure.
1. In the basis of Fourier harmonics {eilx}∞ , the information is transferred from l′ to l
l=−∞
onlyifl−l′isdivisiblebyN (wecansaythatsuchl,l′arecompatible).
2. Ifl,l′ arecompatible,theinformationistransferredonlythroughasingleempiricaleigen-
valueλ .
N,kl
Now, we start to derive the specific form of (44) for our translation-invariant setting. It will be
convenienttodividethecontributiontothefinallossL [h]intoabiastermandtwovarianceterms
N
responsibleforrandomnessw.r.t. touandε :
i
L[h]=LBias[h]+LVar,u[h]+LVar,ε[h]. (84)
21PublishedasaconferencepaperatICLR2024
Biasterm. Thistermisthelossofthemeanpredictionwithpopulationcoefficients
E u,ε[ (cid:98)c l]=
λ(cid:98)λ
kl
lh(λ(cid:98)kl)(cid:90) 02π
e−ilu
n=(cid:88)∞
−∞c l+Nnei(l+Nn)u
2d πu
=
λ(cid:98)λ
kl lh(λ(cid:98)kl)c l, (85)
whosesubstitutioninto∥f∗(x)−E u,ε[f(cid:98)(x)]∥2gives
∞ (cid:32) (cid:33)2
LB Nias[h]= 1
2
l=(cid:88)
−∞
1− λ(cid:98)λ kl lh(λ(cid:98)kl) |c l|2. (86)
Hereweseethathowwellthecomponentlcanbelearneddependsontheclosenessbetweenλ and
l
λ(cid:98)kl: forl≪N wehavek
l
=landtypicallyλ
l
≈λ(cid:98)l(assumingthatλ ldecayfastwithl). Thus,the
targetfunctioncanbelearnedwellbysettingh(λ)=1.
Noise variance term. Since the prediction is linear in the noise ε , its contribution to the loss
i
comes only from the terms which are quadratic in ε . Then, we define the noise variance by the
i
contribution of such terms to the loss. Denoting the contribution of the noise to the prediction as
f(cid:98)(ε),wecalculateitssecondmoment
(cid:32) (cid:33)2 (cid:32) (cid:33)2
E u,ε(cid:104) f(cid:98)(ε)f(cid:98)(ε)(cid:105) = λ lh λ(cid:98)( kλ(cid:98) lkl) N1
2
i(cid:88) 1,i2e−ikl2π(i N1−i2)E ε[ε i1ε i2]= λ lh λ(cid:98)( kλ(cid:98) lkl) σ N2 , (87)
whereweusedthat|f(cid:98)(ε)|2isindependentofumakingexpectationE utrivial. Therespectivecontri-
butiontothelossis
LVar,ε[h]=
1 (cid:88)∞ (cid:32) λ lh(λ(cid:98)kl)(cid:33)2 σ2
. (88)
2
l=−∞
λ(cid:98)kl N
Datasetvarianceterm. Thispartissimplythecontributionoftherestofthevarianceprediction.
Therespectivesecondmomentis
E
(cid:104)
(c −c(ε))(c
−c(ε))(cid:105) =(cid:32) λ lh(λ(cid:98)kl)(cid:33)2 (cid:90) (cid:88)∞
c c
eiN(n1−n2)udu
u,ε (cid:98)l (cid:98)l (cid:98)l (cid:98)l λ(cid:98)kl
n1,n2=−∞
l+Nn1 l+Nn2 2π
(89)
(cid:32) (cid:33)2 ∞
=
λ lh(λ(cid:98)kl) (cid:88)
|c |2.
l+Nn
λ(cid:98)kl
n=−∞
Subtractingthemean(85)fromthesecondmoment,wegetthedatasetvariancelossterm
∞ (cid:32) (cid:33)2
LVar,u[h]=
1 (cid:88) λ lh(λ(cid:98)kl) (cid:88)
|c |2. (90)
2
l=−∞
λ(cid:98)kl
n̸=0
l+Nn
Finalexpression. LetusFirstcombinebiasanddatasetvarianceterms.
LVar,u[h]+LBias[h]=
1 (cid:88)∞ (cid:20)
|c |2−2|c
|2h(λ(cid:98)kl)
λ
+(cid:16)
λ
h(λ(cid:98)kl)(cid:17)2 (cid:88)∞
|c
|2(cid:21)
N 2
l=−∞
l l λ(cid:98)kl l l λ(cid:98)kl
n=−∞
l+Nn
(91)
=
1 (cid:88)∞ (cid:20)
(|c |2−2|c
|2h(λ(cid:98)kl)
λ +|c
|2(cid:16)h(λ(cid:98)kl)(cid:17)2 (cid:88)∞
λ2
(cid:21)
,
2
l=−∞
l l λ(cid:98)kl l l λ(cid:98)kl
n=−∞
l+Nn
wherewehaverearrangedthesumoverlwithfixedk inthequadraticterm.
l
Addingthenoisevarianceterm,wearenowabletowritethefinalexpressionforthegeneralization
error
L[h]= 1
2
l=(cid:88)∞ −∞ |c l|2 1−2h( λ(cid:98)λ(cid:98) kk ll) λ l+(cid:32) h( λ(cid:98)λ(cid:98) kk ll)(cid:33)2 (cid:2) λ2 l(cid:3) N + σ N2 (cid:32) λ lh λ(cid:98)( kλ(cid:98) lkl)(cid:33)2 . (92)
22PublishedasaconferencepaperatICLR2024
where we have used N-deformations (10) notation for the sum (cid:80)∞ λ2 (cid:2) λ2(cid:3) . As a
n=−∞ l+Nn l N
last step, we observe that the sum over indices l the same fixed k = k mod N again leads
l
to N-deformations (10), allowing to rewrite the full sum over l ∈ Z into the sum over k ∈
l
0,1,...,N −1. Then,denotingk simplyask,wehave
l
L[h]= 1 2N k(cid:88) =− 01(cid:34) (cid:16) σ N2 +(cid:2) |c k|2(cid:3) N(cid:17)(cid:2) (cid:2)λ λ2 k k(cid:3) (cid:3)N
2
Nh2(λ(cid:98)k)−2(cid:2) λ (cid:2)k λ|c kk (cid:3)| N2(cid:3) Nh(λ(cid:98)k)+(cid:2) |c k|2(cid:3) N(cid:35) , (93)
whichprovesTheorem1.
Letusnowdescribetheoptimallearningalgorithm(9). Notethatthefunctional(93)isfullylocal,
andthereforetheoptimalalgorithmiswelldefinedandisgivenbypointwiseminimizationateach
λ(cid:98)k. Theresultingoptimalalgorithmh∗(λ(cid:98)k)is
[λ |c |2] [λ ]
h∗(λ(cid:98)k)=
(cid:0)σ2
+k
(cid:2)
|k
c
|2N
(cid:3)
(cid:1)k [λN
2]
. (94)
N k N k N
Itmayalsobeconvenienttogivea“completedsquare”formofthelossfunctionalL[h] = δL[h−
h∗]+L[h∗], δL[h′] ≥ 0 that separates the minimal possible error L[h∗] with an excess positive
errorifthealgorithmisnon-optimalh−h∗ ̸=0
L[h]= 1 2N (cid:88)−1(cid:34) (cid:16) σ N2 +(cid:2) |c k|2(cid:3) N(cid:17)(cid:2) (cid:2)λ λ2 k(cid:3) (cid:3)N
2
(cid:16) h(λ(cid:98)k)−h∗(λ(cid:98)k)(cid:17)2
k=0 k N (95)
(cid:35)
[λ |c |2]2
+[|c |2] − k k N .
k N (cid:0)σ2 +(cid:2)
|c
|2(cid:3) (cid:1)
[λ2]
N k N k N
Finally,wenotethatonecanusetranslationsymmetryk →k+N ofN-deformations(10)inorder
toshiftthesummationin(93)tovalues−N ≤ k ≤ N,forexample,k ∈ {−⌊N⌋,...,⌈N⌉−1}.
2 2 2 2
(cid:80)N
Likein(21),wedenotesuchsummationrangesimplyas 2 . Thepurposeofthisshiftisto
k=−N
putallhighempiricalspectralquantities(cid:2) λa|c |2b(cid:3) intheregio2 n|k| ≪ N,allowingtowrite,for
k k N
example,λ(cid:98)k
=O(cid:0) (|k|+1)−ν(cid:1)
.
D.2 POWER-LAWANSATZ: NOISYOBSERVATIONS
Now,weturntoamoredetailedanalysisoftheCirclemodel. Asinthemainpaper,weseparately
considerthenoisyσ2 >0caseinthissectionandthenoiselessσ2 =0caseinthenextSectionD.3.
Recallthatweadaptbasicpower-lawspectrumλ = l−ν, c2 = l−κ−1, l ≥ 1sinceforthecircle
l l
modelthepopulationisnaturallyindexedbythewholeintegersetZ,leadingto
λ =(|l|+1)−ν, c2 =(|l|+1)−κ−1, l∈Z. (96)
l l
ThepurposeofthecurrentsectionistoshowtheequivalenceofcirclemodelandNMNOmodels,
thus proving the respective part of Theorem 2. The intuition behind NMNO relies on the close-
nessofpopulationandempiricalspectraldistributionsintheeigenspacesdistantfromthespectrum
edge|l| ≪ N. Thus,weneedtocompareN-deformations[λa|c |2b] withtheirpopulationcoun-
k k N
terparts λa|c |2b, considering the values |k| ≤ N relevant for the loss functional (11). From the
k k 2
definition(10)weget
(cid:2) λa|c |2b(cid:3) =λa|c |2b+(cid:88) λa |c |2b
k k N k k k+Nn k+Nn
n̸=0
(cid:88)∞ N−aν−b(κ+1) (cid:88)∞ N−aν−b(κ+1)
=λa|c |2b+ + (97)
k k (cid:0)
n +
1+k(cid:1)aν+b(κ+1) (cid:0)
n +
1−k(cid:1)aν+b(κ+1)
n1=1 1 N n2=1 2 N
(cid:16) (cid:17)
=λa|c |2b+O(N−aν−b(κ+1))=λa|c |2b 1+O(τaν+b(κ+1)) ,
k k k k
where,inthelastline,wehaveassumedthataν +b(κ+1) > 1sothatbothn andn seriesare
1 2
converging. Also,wehaverecalledthenotationτ = |k|+1 introducedinSection5.3.
N
23PublishedasaconferencepaperatICLR2024
It turns out that the relation (cid:2) λa|c |2b(cid:3) = λa|c |2b +O(N−aν−b(κ+1)) is sufficient to establish
k k N k k
equivalence to NMNO model. For that, write the Circle model loss functional (11) as NMNO
functionalL(nmno)[h],definedin(17),plustwocorrectionsterms
L[h]=L(nmno)[h]+δL [h]+δL [h], (98)
alg coeff
wherethecorrectionduetothedisplacementsbetweenthepopulationandempiricaleigenvaluesin
theargumentofthelearningalgorithmis
δL alg[h]= 1
2
(cid:88)N 2 (cid:20) σ N2(cid:0) h2(λ(cid:98)k)−h2(λ k)(cid:1) +c2 l(cid:0)(cid:0) 1−h(λ(cid:98)k)(cid:1)2 −(cid:0) 1−h(λ k)(cid:1)2(cid:1)(cid:21) , (99)
k=−N
2
andthecorrectionduetodifferencebetweenpopulationλa|c |2b andempirical(cid:2) λa|c |2b(cid:3) coeffi-
k k k k N
cientsinthelossfunctional
δL coeff[h]= 1
2
(cid:88)N 2 (cid:20) σ N2(cid:2) λ2 k(cid:3) N
(cid:2)
λ− (cid:3)2(cid:2) λ k(cid:3)2 Nh2(λ(cid:98)k)+(cid:16)(cid:2) |c k|2(cid:3) N(cid:2) (cid:2)λ λ2 k(cid:3) (cid:3)N
2
−|c k|2(cid:17) h2(λ(cid:98)k)
k=−N k N k N (100)
2
−2(cid:16)(cid:2) λ (cid:2)k λ|c k (cid:3)|2(cid:3) N −|c k|2(cid:17) h(λ(cid:98)k)+(cid:2) |c k|2(cid:3)
N
−|c k|2(cid:21) .
k N
Fromthispoint,ourstrategyistospecifythescalesofallthetermsofNMNOfunctionalL(nmno)[h],
andofthetwocorrectionsδL [h],δL [h]. Then,wecaninvokethescalingargumentofPropo-
alg coeff
sition 2 (actually its discrete version in Proposition 5) to show that all the correction terms give
negligiblecontributiontotheloss.
First,recallfromSection5.1thatthescalingofNMNOtermsisgivenby
(cid:104) (cid:105)
S σ2h2(λ ) =1+2S(h)(s), (101)
N k
S(cid:104) c2(cid:0) 1−h(λ )(cid:1)2(cid:105) = κ+1s+2S(1−h)(s). (102)
k k ν
Next, we proceed to the δL [h] correction. To bound its terms one needs a certain smoothness
alg
assumption on h(λ). Currently, in Theorem 2, we require that the maps logλ (cid:55)→ log|h(λ)|
and logλ (cid:55)→ log|1 − h(λ)| are globally Lipschitz, but maybe a weaker smoothness condition
is possible. To understand the application of this condition, take some function g(x) such that
a mapping logx → logg(x) is Lipschitz with constant C, i.e.
(cid:12) (cid:12)logg(x+∆x)(cid:12)
(cid:12) ≤
C(cid:12) (cid:12)logx+∆x(cid:12)
(cid:12).
g(x) x
Then, taking any constant C′ > C, there is δ > 0 such that for all |∆x| < δ we have
(cid:12) (cid:12)g(x+∆x)−g(x)(cid:12) (cid:12)<C′g(x)(cid:12) (cid:12)∆x(cid:12) (cid:12).Comingbacktothedifferencebetweenempirx
icalandpopulation
x
eigenvaluesandusing(97)gives λ(cid:98)k λ− kλk =O(τν),andtherefore|h2(λ(cid:98)k)−h2(λ k)|=O(cid:0) h2(λ k)τν(cid:1)
(and similar estimate for (1−h(λ))2). Recalling the scale S[τ] = S(cid:2)|k|+1(cid:3) = 1− s, we get a
N ν
boundonthescaleoftherespectivecorrectionterms
(cid:20) σ2(cid:16) (cid:17)(cid:21)
S
N
h2(λ(cid:98)k)−h2(λ k) ≥1+ν−s+2S(h)(s), (103)
S(cid:104) c2 k(cid:16)(cid:0) 1−h(λ(cid:98)k)(cid:1)2 −(cid:0) 1−h(λ k)(cid:1)2(cid:17)(cid:105) ≥ν+ κ+ ν1−νs+2S(1−h)(s). (104)
Finally,repetitiveapplicationof(97)toallthetermsofδL [h]givestheremainingscales
coeff
S(cid:34) σ N2(cid:2) λ2 k(cid:3)
N
(cid:2)
λ− (cid:3)2(cid:2) λ k(cid:3)2 Nh2(λ(cid:98)k)(cid:35)
≥1+(ν−s)+2S(h)(s), (105)
k N
S(cid:34) (cid:16)(cid:2) |c k|2(cid:3) N(cid:2)(cid:2) λ2 k (cid:3)(cid:3) 2N −|c k|2(cid:17) h2(λ(cid:98)k)(cid:35) ≥(κ+1)∧(cid:0) ν+ κ+ ν1−νs(cid:1) +2S(h)(s), (106)
λ
k N
(cid:34) (cid:16)(cid:2)
λ |c
|2(cid:3)
(cid:17)
(cid:35)
S (cid:2)k λk
(cid:3)
N −|c k|2 h(λ(cid:98)k) ≥ν+ κ+ ν1−νs+S(h)(s), (107)
k N
S(cid:104)(cid:2)
|c
|2(cid:3)
−|c
|2(cid:105)
≥κ+1. (108)
k N k
24PublishedasaconferencepaperatICLR2024
Here, all the estimations are relatively straightforward, except for, possibly, the bound (106) that
involvestwovaluesdependingonwhetherκ+1>ν ortheopposite(areminiscentofoverlearning
transitionofSection5.3here!). Wedemonstratetheboundingofthistermindetail
(cid:2) λ2(cid:3) λ2(cid:0) 1+O(τ2ν)(cid:1)
(cid:2) |c |2(cid:3) k N =|c |2(cid:0) 1+O(τκ+1)(cid:1) k
k N(cid:2) λ k(cid:3)2
N
k (cid:16) λ k(cid:0) 1+O(τν)(cid:1)(cid:17)2
=|c
|2(cid:0) 1+O(τκ+1)+O(τν)(cid:1)
=|c
|2(cid:16) 1+O(cid:0) τ(κ+1)∧ν(cid:1)(cid:17) (109)
k k
(cid:16)(|k|+1)0∧(ν−κ−1)(cid:17)
=|c |2+O .
k N(κ+1)∧ν
Thisimmediatelyimpliestheboundonthescaleof[|c k|2] N[[λ λ2 k k] ]N
2
N
−|c k|2usedin(106).
Now,havingspecifiedthescaleofallthecorrections,ourtaskistoshowthattheycontributeneg-
ligiblytotheloss. Inotherwords,thescaleofthetotalcontributionofallthecorrectionshastobe
strictlylowerthanthatofNMNOloss. UsingProposition5thisiswrittenas
min (cid:104)(cid:0) 1− s +2S(h)(s)(cid:1) ∧(cid:0)κs+2S(1−h)(s)(cid:1)(cid:105) (110)
ν ν
0≤s≤ν
< min
(cid:104)(cid:0) 1+(ν−s)+2S(h)(s)(cid:1)
(111)
0≤s≤ν
∧(cid:0) (κ+1)∧(cid:0)
ν+
κ+1−νs(cid:1) +2S(h)(s)(cid:1)
(112)
ν
∧(cid:0)
ν+
κ+1−νs+S(h)(s)(cid:1)
(113)
ν
∧(cid:0)
ν+
κ+1−νs+2S(h)(s)(cid:1)
(114)
ν
(cid:105)
∧(κ+1)− s . (115)
ν
It is easy to see that this strict inequality can only be violated (by becoming equality) when the
minsareattainedats = ν sothatther.h.s. of(105)isequaltother.h.s. of(101)). However, this
possibilityisexcludedbyhypothesisofthetheorem. ThiscompletestheproofofTheorem2forthe
Circlemodel.
WeremarkthattheLipschitzconditioninTheorem2,usedtocomparealgorithmh(λ)evaluatedat
thepopulationandempiricaleigenvalues,isrequiredonlyforthediscrete(Circle)problem.Itiseasy
tocheckthatthisconditionholdsfortheKRRalgorithmwithh (λ)= λ . However,itisviolated
η λ+η
forGFwithh t(λ) = 1−e−λt: fortimet ∼ Nst themappinglogλ → log(1−h t(λ)) = −λtis
notLipschitzonthescaless<s . Fortunately,(1−h (λ))2 onsuchscalesisexponentiallysmall,
t t
andthecontributiontothelossfromcorrespondingterms,bothNMNOanditscorrections,canbe
ignored. Thus,theequivalencebetweenCircleandNMNOmodelsstillholdsforGFalgorithm. We
expectthattheLipschitzconditionofTheorem2canbeweakenedtotakeintoaccountGFalgorithm
butleaveitforfuturework.
D.3 POWER-LAWANSATZ: NOISELESSOBSERVATIONS
Inthissection,wederivetheresultspresentedinSection5.3,whilealsogivingfullversionsofthe
resultsthatwerediscussedonlypartiallyinthemainpaper. Fortheconvenienceofexposition,the
orderinthissectionrepeatsthatofSection5.3.
D.3.1 PERTURBATIVEEXPANSIONOFTHELOSSFUNCTIONAL
Inthemainpaper,wereliedonequation(21)asastartingpointtoquiteeasilyconcludethattheloss
localizesonthesmallestspectralscales = 0inthesaturatedphaseκ > 2ν whilelocalizingonthe
highestscales=νinthenon-saturatedphaseκ<2ν. Essentially,equation(21)ignoresthedetails
oftheproblematsubspacescorrespondingtosmalleigenvaluesλ(cid:98)k ∼ λ(cid:98)N/2 (and|k| ∼ N), while
providingasimpleestimationofdifferentlossfunctionaltermsforlargeeigenvaluesubspaceswith
λ(cid:98)k ≫λ(cid:98)N/2 (and|k|≪N). Alternatively,ifonestartsfromexactlossfunctional(11),thereseems
tobenoclearpathtodeducingtheexistenceofsaturatedandnon-saturatedphases, andobtaining
theirconvergencerates.
25PublishedasaconferencepaperatICLR2024
Theabovediscussionilluminatestheimportanceofperturbativeexpansionofthelossfunctionalin
the small parameter τ = |k N|+1, or, in other words, perturbative corrections λ(cid:98)k −λ
k
and (cid:98)c
k
−c
k
of population spectral distributions in the presence of finite dataset size N. For the circle model,
the effect of finite dataset size N is captured by deviation of N-deformations [λa|c |2b] from
k k N
theirpopulationcounterpartsλa|c |2b. Thesimplestformofthisdeviationwasalreadyobtainedin
k k
equation(97),whichwewillusebelowtoderiveequation(21).
Letusstartbywritingdownthelossfunctionalin“completedsquare”from(95)andintheabsence
ofobservationnoise
L[h]= 1
2
k=(cid:88)N −2
N
(cid:34)(cid:2) |c (cid:0)k (cid:2)|2 λ(cid:3) kN
(cid:3)
N(cid:2) λ (cid:1)2 k 2(cid:3) N(cid:16) h(λ(cid:98)k)−h∗(λ(cid:98)k)(cid:17)2 +[|c k|2]
N
− (cid:2)(cid:0) |[ cλ kk |2|c (cid:3)k N|2 [λ] N
2
k](cid:1) N2(cid:35) . (116)
2
Notethatthefirsttermhere,i.e.
thefactorinfrontof(cid:0) h(λ(cid:98)k)−h∗(λ(cid:98)k)(cid:1)2
,wasalreadyestimatedin
thesecondlineof(109):itagreeswiththefactor
|ck|2(cid:0) 1+o(τ)(cid:1)
appearingin(21)uptoreplacement
2
O(τ(κ+1)∧ν)=o(τ)validdueto(κ+1)∧ν >1. Weusethissimplificationin(21)becausewedo
notneedamoreaccurateestimationofthecorrectiontermatthismoment.
The second term of (21) corresponds to the generalization error of the optimal algorithm, which
allowstodenoteitasL [h∗]sinceL[h∗]=(cid:80)N/2 L [h∗]. Thistermisestimatedasfollows
k k=−N/2 k
(cid:0) [λ |c |2] (cid:1)2
L [h∗]=[|c |2] − k k N
k k N (cid:2) |c |2(cid:3) [λ2]
k N k N
|c
|4λ2(cid:0) 1+O(τκ+1+ν)(cid:1)
=|c |2(cid:0) 1+O(τκ+1)(cid:1) − k k
k
|c
|2λ2(cid:0) 1+O(τκ+1)+O(τ2ν)(cid:1)
k k
(cid:16) (cid:17) (117)
=|c |2 O(τκ+1)+O(τκ+1+ν)+O(τ2ν)
k
(cid:16) (cid:17)
=(τN)−κ−1 O(τκ+1)+O(τ2ν)
(cid:16) (cid:17)
=N−κ−1 O(1)+O(τ2ν−κ−1) .
D.3.2 OPTIMALLYSCALEDLEARNINGALGORITHMS
In Section 5.3 we mentioned that the rate O(N−κ) of the optimal algorithm h∗(λ) in the non-
saturatedphasealsoholdsforlearningalgorithmsh(λ)withinasuitableneighbourhoodofh∗(λ),
characterizedbyacondition|h(λ(cid:98)k)−h∗(λ(cid:98)k)|2 =o(τκ). Inthissection,wederivethisresultwhile
also providing a more systematic discussion of learning algorithms that do not destroy the rate of
theoptimalalgorithm.
(cid:2) (cid:3)
Letuscallanalgorithmh(λ)optimallyscaledifthescaleS L[h] ofassociatedgeneralizationerror
L[h]isthesameasthatoftheoptimalalgorithmh∗(λ)
S(cid:2) L[h](cid:3) =S(cid:2) L[h∗](cid:3)
. (118)
Whileonemighttrytofindallalgorithmssatisfying(118),herewetakealessambitiousapproach
by considering a simple family of conditions on |h(λ) − h∗(λ)| and then choosing the weakest
conditionwithinthefamily. Specifically, foranytwoconstantsa,b, considerthefollowingbound
onthescaleofdeviationofanalgorithmh(λ)fromtheoptimalone:
S(cid:2) |h(λ)−h∗(λ)|2(cid:3)
≥as+b, s∈[0,ν], (119)
whichisaslightlyweaker(e.g. uptoalogN factors)versionof|h(λ)−h∗(λ)|2 = O(λ−aN−b).
Herewelimitedthescaleofλtos ∈ [0,ν]becausethisispreciselytheintervalofscalesoccupied
bythesetofempiricaleigenvalues{λ(cid:98)k}N k=− 01passedasaninputtoalgorithmsh,h∗.
Itiseasytocheckwhetherallalgorithmssatisfyingcondition(119)areoptimallyscaled. First,we
canequivalentlyrewrite(118)asS(cid:2) L[h]−L[h∗](cid:3) ≥S(cid:2) L[h∗](cid:3)
.Then,applyingProposition5to(21)
yields
26PublishedasaconferencepaperatICLR2024
S(cid:2) L[h]−L[h∗](cid:3)
= 0≤m si ≤n
ν(cid:16) S(cid:2)
|c
k|2(cid:3) +S(cid:2) |h(λ(cid:98)k)−h∗(λ(cid:98)k)|2(cid:3)
−
νs(cid:17)
(cid:16) s s(cid:17) (cid:26) b, a≥−κ (120)
≥ min (κ+1) +as+b− = ν
0≤s≤ν ν ν b+κ+aν, a<−κ.
ν
Accordingtotheabovecalculation, thesetofallpairsa,bthatguaranteealgorithmsundercondi-
tion(119)tobeoptimallyscaledisgivenbyA = {(a,b) ∈ R2 | b+0∧(κ+aν) ≥ S(cid:2) L[h∗](cid:3) }.
Now,ifwewishtopickapair(a,b)∈Asuchthatcondition(119)istheweakestataspectralscale
s,wehavetominimizeas+b. Fortunately,thereisauniquepair(a∗,b∗)thatprovidestheweakest
conditionacrossallrelevantspectralscales
(a∗,b∗)=(cid:16)
−
κ ,S(cid:2) L[h∗](cid:3)(cid:17)
=argminas+b ∀s∈[0,ν]. (121)
ν
(a,b)∈A
Applying this result to saturated and non-saturated phases, with respective convergence rates
O(N−2ν)andO(N−κ),givesthedesiredconditionsforoptimallyscaledalgorithms
S(cid:2) |h(λ)−h∗(λ)|2(cid:3) ≥−κ
s+κ inthenon-saturatedphaseκ<2ν, (122)
ν
S(cid:2) |h(λ)−h∗(λ)|2(cid:3) ≥−κ
s+2ν inthesaturatedphaseκ>2ν. (123)
ν
A stronger version of optimally scaled algorithm condition. Recall that the loss of the opti-
mal algorithm h∗ in saturated and non-saturated phases localize at s = 0 and s = ν respectively.
However,conditions(122),(123)donotprovidethesamelocalizationpropertyfortheexcesserror
L[h]−L[h∗]. Toseethis,take|h(λ)−h∗(λ)|2 = O(λ−κ νN−κ∧2ν),whichcorrespondstovalues
(a∗,b∗)givenin(121). Substitutingthesevaluesinexcesserrorscalecalculation(120)makesthe
function under the minimum s-independent, implying that the excess loss localizes on all spectral
scaless∈[0,ν]. Suchspreadoflocalizationscalesintroducesalogarithmicfactorintheerrorrate.
Taking,forsimplicity,|h(λ(cid:98)k)−h∗(λ(cid:98)k)|2 =(|k|+1)−κN−κ∧2ν,andusing(21)gives
N
(cid:88)2
L[h]−L[h∗]= c2 k(1+o(τ))|h(λ(cid:98)k)−h∗(λ(cid:98)k)|2
k=−N
2 (124)
N
(cid:88)2 1+o(τ)
=N−κ∧2ν =O(N−κ∧2νlogN).
|k|+1
k=−N
2
Toavoidtheseissues,letusintroduceaslightlystrongerversionofconditions(122),(123),specified
bypickingasmallparameterε>0:
|h(λ)−h∗(λ)|2 =O(cid:0) (λν1N)−κ−ε(cid:1) inthenon-saturatedphaseκ<2ν, (125)
|h(λ)−h∗(λ)|2 =O(cid:0) λ−κ ν+εN−2ν(cid:1) inthesaturatedphaseκ>2ν. (126)
For any ε > 0, the above conditions guarantee localization of the excess error either at s = 0
(saturated phase) or s = ν (non-saturated phase), as can be seen from (120). Also, using these
conditionsincomputationlike(124)producestherateoftheoptimalalgorithmwithoutlogN factor
L[h]−L[h∗]=O(N−κ∧2ν).
Finally,letuscommenton|h(λ(cid:98)k)−h∗(λ(cid:98)k)|2 =o(τκ)conditionfornon-saturatedphase,mentioned
1
inthemainpaper. Sinceτ = (λνN)−1,inmanycasesonecanreplaceo(τκ)withO(τκ+ε)with
k
someεandthussatisfycondition(125).Whennoε>0canprovidesuchreplacement,itispossible
to show that the rate of the excess error remains L[h]−L[h∗] = O(N−κ), although with a more
technicallyinvolvedversionofcomputation(124).
27PublishedasaconferencepaperatICLR2024
D.3.3 SATURATEDPHASEκ>2ν
When discussing the saturated phase in Section 5.3, we first deduced from O(τ#) terms in equa-
tion(21)thatthelosswilllocalizeonthescales=0(correspondingto|k|∼1),andthenstatedthat
thelossoftheoptimalalgorithmcanbeachievedbyoptimallyregularizedKRR.Inthissection,we
derive an asymptotic expression of the loss functional in the saturated phase, which both explains
the statements made in the main paper and gives a more systematic picture of the loss-algorithm
relationsinthesaturatedphase.
Generalization error of optimal algorithm L[h∗]. The asymptotic of L[h∗] originates from
O(τ2ν−κ−1) term in equation (21), which dominates the whole loss functional for κ > 2ν. To
verifythatthisisindeedthecase,weneedamoreaccuratecharacterizationofN-deformationcor-
rectiontermsthanin(97).Forthat,werecognizethatthesumsovern ,n inthesecondlineof(97)
1 2
are Hurwitz zeta functions ζ(α,x) ≡ (cid:80)∞ (n+x)−α evaluated at α = aν +b(κ+1) and two
n=0
specificvaluesofx:
(cid:2) λa|c |2b(cid:3) α=aν+ =b(κ+1) λa|c |2b+N−α(cid:16) ζ(cid:0) α,1+ 1+k(cid:1) +ζ(cid:0) α,1+ 1−k(cid:1)(cid:17) . (127)
k k N k k N N
SubstitutingTaylorexpansionζ(α,1+ϵ)=ζ(α)−αζ(α+1)ϵ+O(ϵ2)ofHurwitzzetafunction
atx=1into(127)wegetamoredetailedversionof(97)
(cid:2) λa|c |2b(cid:3) =λa|c |2b+2ζ(α)N−α+O(N−α−1)+O(τ2N−α). (128)
k k N k k
Equippedwith(128),weturntoderivationoftheleadingasymptoticofO(τ2ν−κ−1)termdominat-
ingtheloss.Bylookingat(117),wecanseethatthetermofinterestcomesfromfiniteN corrections
of(cid:2) λ2(cid:3)
.
Then,mimickingthederivation(117)andusing(128)for(cid:2) λ2(cid:3)
,wehave
k N k N
|c
|2λ2(cid:0) 1+O(τκ+1)(cid:1)
L [h∗]=|c |2(cid:0) 1+O(τκ+1)(cid:1) − k k
k k λ2 +2ζ(2ν)N−2ν(cid:0) 1+O(N−1)+O(τ2)(cid:1)
k (129)
=2ζ(2ν)|c k|2 N−2ν(cid:0) 1+O(N−1)+O(τ2)(cid:1) +O(N−κ−1).
λ2
k
Substitutingtheaboveinthelossfunctionalsumresultsgives
N
L[h∗]= 1 (cid:88)2 (cid:104) 2ζ(2ν)|c k|2 N−2ν(cid:0) 1+O(N−1)+O(τ2)(cid:1) +O(N−κ−1)(cid:105)
2 λ2
k=−N k
2
N
=
1 2ζ(2ν)N−2ν(cid:0) 1+o(1)(cid:1) (cid:88)2 |c l|2 (130)
2 λ2
k=−N l
2
=
1 2ζ(2ν)N−2ν(cid:0) 1+o(1)(cid:1) (cid:88)∞ |c l|2
.
2 λ2
l=−∞ l
Here in the last equality we extended the summation to all population eigenspaces due to conver-
genceoftheseries(cid:80)∞ |cl|2
atκ > 2ν,whichreflectslocalizationoftheerroronscales = 0,
l=−∞ λ2
l
correspondingto|l|∼1.
Notethatin(130)wecouldhaveevaluatedthesumoverthepopulationspectraas
(cid:88)∞ |c l|2
=
(cid:88)∞
(|l|+1)2ν−κ−1 =2ζ(κ+1−2ν)−1. (131)
λ2
l=−∞ l l=−∞
However, we view the version with the sum as being more general. Intuitively, it stays valid in
the case of population spectra having power-law form only asymptotically: λ = (|l|+1)−ν(1+
l
o(1)) and |c |2 = (|l|+1)−κ−1(1+o(1)). In that case, the value of the sum is not fixed by the
l
asymptoticsofλ ,|c |2 andmayvarysubstantially. Incontrast,wecanseefromcomputation(129)
l l
andcorrectionstoN-deformations(97)and(128)thatthefactor2ζ(2ν)isdeterminedbypopulation
spectrumλ withindiceslnearthevaluesl = N,−N,2N,−2N,.... Therefore,thefactor2ζ(2ν)
l
28PublishedasaconferencepaperatICLR2024
in the loss is determined purely by asymptotic behavior of λ . Overall, this creates an interesting
l
situationwheretheloss(130)islocalizedonthesmallestscales=0,buttheasymptoticshapeofthe
populationspectrumalmostfullyspecifiesthegeneralizationerrorasymptoticL[h∗]=CN−2ν(1+
o(1)),withonlytheconstantfactor(cid:80) |cl|2
determinedbythedetailslivingonthelocalizationscale
l λ2
l
s=0.
FullgeneralizationerrorL[h]. ThefullgeneralizationerrorisobtainedbycombiningL[h∗]with
thepartof(116)associatedwithdeviationfromtheoptimalalgorithmh(λ)−h∗(λ). Wewillcon-
sideronlythealgorithmsh(λ)withmilddeviationfromtheoptimal,asgivenbycondition(125).As
demonstratedinSectionD.3.2,thisconditionguaranteestheratenotworsethantherateO(N−2ν)
oftheoptimalalgorithm,andalsolocalizationoftheexcesserroronthesamescales=0.
Havingensuredlocalizationoftheexcesserroronthescales=0,letusfirstlookattheperturbative
expressionoftheoptimalalgorithmh∗(λ)atthisscale. Substituting(128)into(12),weget
h∗(λ(cid:98)k)=1+2ζ(ν)N λ−ν (cid:0) 1+O(N−1)+O(τ)(cid:1)
. (132)
k
Usingtheaboveexpressionoftheoptimalalgorithm,wecomputethefulllossfunctionalasfollows
N
L[h]=
21 k=(cid:88) −2
N
|c
k|2(cid:0) 1+o(τ)(cid:1)(cid:16) h(λ(cid:98)k)−1−2ζ(ν)N λ(cid:98)− kν (cid:0)
1+O(
N1 )+O(τ)(cid:1)(cid:17)2
2
N
+
1 (cid:88)2 |c k|2
2ζ(2ν)N−2ν(1+o(1)) (133)
2 λ2
k=−N k
2
= 1 N−2ν(1+o(1)) (cid:88)N 2 |c k|2 (cid:20)(cid:16) λ Nν(cid:0) h(λ )−1(cid:1) −2ζ(ν)(cid:17)2 +2ζ(2ν)(cid:21)
2 λ2 k k
k=−N k
2
WhiletheaboveexpressionforL[h]isalreadyavalidasymptoticforthelossinthesaturatedphase,
wemakeafewfurtherrefinements.
First,notethat(133)reliesoncondition(125)forthealgorithmh(λ). Thisconditionmightbequite
difficulttoverifyinpractice,asitrequirestheknowledgeoftheoptimalalgorithmh(λ). However,
perturbativeexpansion(132)showsthatoptimalalgorithmsatisfies|h∗(λ)−1|2 = O(λ−2N−2ν).
Since in saturated phase κ > 2, we can always make λ−2N−2ν smaller than λ−κ ν+εN−2ν by
ν
choosingε< κ −2. Thus,employingtriangularinequalityshowsthattheoriginalcondition(125)
ν
isequivalentto
κ
|h(λ)−1|2 =O(λ−κ ν+εN−2ν), 0<ε< −2. (134)
ν
Themainadvantageofcondition(134)comparedto(126)isthatitiseasilyverifiable. Forinstance,
KRRhas1−h (λ)= η whichsatisfies(134)for|η|=O(N−ν). Similarly,forgradientflowwe
η λ+η
have1−h t(λ) = e−tλ whichsatisfies(134)witht = Ω(N2ν κ2 +ε′)andsomeε′ > 0depending
onε.
Note that KRR and GF examples of h(λ) considered above satisfy (134) not only on the scales
s ∈ [0,ν] but for all s ≥ 0, or equivalently λ ≤ 1. Based on this observation, let us impose
condition(134)onallλ ≤ 1. Then,thesummationindicesin(133)toallpopulationvaluesl ∈ Z,
similarlytohowitwasdoneforoptimalalgorithmlossin(130).
Tosummarize,throughthederivationsanddiscussionsabovewehaveproved
Theorem 3. Consider the Circle model in saturated phase κ > 2ν. Then, if an algorithm h(λ)
satisfies(134)forallλ≤1,thelossfunctionalisgivenby
L[h]= 1 N−2ν(1+o(1)) (cid:88)∞ |c l|2 (cid:20)(cid:16) λ Nν(cid:0) h(λ )−1(cid:1) −2ζ(ν)(cid:17)2 +2ζ(2ν)(cid:21) . (135)
2 λ2 l l
l=−∞ l
29PublishedasaconferencepaperatICLR2024
Finally, let us comment on the KRR result (22) presented in the main paper. Observe that the
combinationλ(h(λ)−1)enteringthelossfunctional(135)isgiven,incaseofKRR,by
η (cid:16) η (cid:17)
λ(h (λ)−1)=− =−η 1+O( ) . (136)
η 1+ η λ
λ
Substitutionoftheaboveintolossfunctional(135)leadstotheKRRexpression(22). Accordingto
condition(134), thisexpression isapplicableonlyforsmall enoughregularizationstrengths |η| =
O(N−ν).
D.3.4 NON-SATURATEDPHASEκ<2ν
Inthissection, weobtainalimitingformoflossfunctionalinthenon-saturatedphase. Whilethis
form was not discussed explicitly in the main text, it was used to produce the first two plots in
Figure3.
Animportantfeatureofthenon-saturatedphaseisthelocalizationofthelossonthehighestspectral
scales = ν,whichisensuredforalgorithmsh(λ)satisfying(125). Suchlocalizationcorresponds
tovaluesτ ∼ 1,sowecannotrelyonperturbativeexpansioninsmallparameterτ → 0,aswedid
inthesectionsabove. Instead,foralltermsofthelossfunctional(116)weneedtotakeintoaccount
theirlimitingN →∞formatfixedτ.ForN-deformations,suchlimitleadstosymmetrizedHurwitz
zeta function ζ(α) = ζ(α,τ)+ζ(α,1−τ) introduced in equation (23) of the main text. Indeed,
τ
slightlyrearranging(127),weget
(cid:2) λa|c |2b(cid:3) =N−α(cid:16) ζ(cid:0) α,τ(cid:1) +ζ(cid:0) α,1−τ + 2 (cid:1)(cid:17) −τN −=−→ c−o∞ n−s→, t N−αζ(α). (137)
k k N N τ
Next, observe that on the scale s = ν the density of eigenvalues λ is very high, in a sense that
k
N→∞,
|λk+1−λk| = O(τ−1N−1) −τ−=−c−on−s→t 0. Thus, in the limit N → ∞ the summation over spectral
λk
indexkinthelossfunctional(116)translatesintoanintegral:(cid:80)N
2
→(cid:82) N
2 dk
→2N−1(cid:82) 1
2 dτ,
k=−N −N 0
2 2
whereforthelasttransitionrecallthatτ = |k|+1 andN-deformations[λa|c |2b] areevenfunctions
N k k N
ofk. Thisleadstothefollowingcontinuousformofthelossfunctional(116)
L(cont)[h]=N−κ(cid:90)1 2(cid:34) ζ τ(κ (cid:0)+ ζ(1 ν) )ζ (cid:1)τ( 22ν)(cid:16) h(λ(cid:98)τ)−h∗(λ(cid:98)τ)(cid:17)2
+ζ τ(κ+1)−
(cid:0) ζζ (τ( κν ++ 1κ )+ ζ(1 2) ν(cid:1) )2(cid:35)
dτ
τ τ τ
0
(138)
(cid:90)21(cid:34)
ζ(κ+1)ζ(2ν) ζ(ν+κ+1)
(cid:35)
=N−κ τ
(cid:0)
ζ(ν)(cid:1)τ
2
h2(λ(cid:98)τ)−2 τ
ζ(ν)
h(λ(cid:98)τ)+ζ τ(κ+1) dτ,
τ τ
0
whereλ(cid:98)τ =N−νζ τ(ν)andtheoptimalalgorithmh∗(λ(cid:98)τ)isgivenby(23).
D.3.5 THEOVERLEARNINGTRANSITIONPOINT
Observethatifκ=ν−1,theninthecaseofzeronoiseσ2 =0theoptimalalgorithm(12)becomes
h∗(λ(cid:98)k) ≡ 1, and the same holds for the limiting (N → ∞) version (23). We prove now that for
larger(smaller)κtheoptimalalgorithmbecomesoverlearning(underlearning). Wegivetheproof
forthelimiting(N →∞)version,butitiseasytoseethattheproofextendswithoutchangetothe
originaldiscreteversionaswell.
Lemma2. ConsidertheN →∞limitoftheoptimalalgorithmgivenby(23):
ζ(ν+κ+1)ζ(ν)
h∗(λ(cid:98)τ)= ζτ (κ+1)ζ(2τ
ν)
, ζ x(α) ≡ζ(α,x)+ζ(α,1−x). (139)
τ τ
Thenforanyτ ∈(0,1)

<1, κ+1<ν,

h∗(λ(cid:98)τ) =1, κ+1=ν, (140)
>1, κ+1>ν.
30PublishedasaconferencepaperatICLR2024
Proof. Wehave
(cid:90) κ+1 (cid:90) α+ν
lnh∗(λ(cid:98)τ)=(lnζ τ(ν+κ+1)−lnζ τ(κ+1))−(lnζ τ(2ν)−lnζ τ(ν))= dα dβ dd β2
2
lnζ τ(β).
ν α
(141)
To prove the lemma, it suffices to show that the function f(α) = ζ(α) is strictly log-convex for
τ
α>1,i.e. ff′′ >(f′)2.Notethat
∞ ∞
dm f(α)= (cid:88) (−ln(n+τ))m(n+τ)−α+(cid:88) (ln(n+1−τ))m(n+1−τ)−α. (142)
dαm
n=0 n=0
Then,theinequalityff′′ >(f′)2isjusttheCauchyinequalityforthevectors
(...,(n+τ)−α/2,...,(n+1−τ)−α/2,...) (143)
and
(...,−ln(n+τ)(n+τ)−α/2,...,ln(n+1−τ)(n+1−τ)−α/2,...). (144)
Theinequalityisstrictbecausethevectorsarenotcollinear.
The over/under-learning property h∗(λ ) ≷ 1 is clearly visible in the right two subfigures of
τ
Figure 3. At κ > ν − 1, the optimal regularized KRR is also overlearning (with a nega-
tive regularization). The optimally stopped GF in this regime is GF continued to infinity, i.e.
h(λ)=lim (1−e−tλ)=1.
t→∞
E WISHART MODEL
Asforthecirclemodel,inthissectionwecollectallourderivationsfortheWishartmodel.
Ourstrategyincomputinglossfunctional(8)reliesontheresolventR(cid:98)(z)ofempiricalkernelmatrix
K=ΦTΛΦ:
(cid:18)
1
(cid:19)−1 (cid:32)
1 (cid:88)
(cid:33)−1
R(cid:98)(z)= NΦTΛΦ−zI =
N
λ lϕ lϕT
l
−zI , z ∈C, (145)
l
whereϕ arethecolumnsoffeaturematrixΦ.
l
Wenotethattheprojectionsonempiricaleigenvaluesandeigenvectors,appearinginempiricaltrans-
fermeasure(41)arerelatedtotheresolventviathelimitinthecomplexplane
(cid:88)N
1
(cid:18)
(cid:88)
(cid:19)−1
k=1δ λ(cid:98)k(dλ)u kuT
k
=
π
yl →im 0+ℑ
k
λ(cid:98)ku kuT
k
−(λ+iy)I dλ
(146)
1
= lim ℑR(cid:98)(λ+iy)dλ,
π y→0+
whereℑzdenotestheimaginarypartofz ∈C.
Now,denotetheresolventprojectedonfeaturesϕ as
l
1
R(cid:98)ll′(z)= NϕT
l
R(cid:98)(z)ϕ l′, (147)
andthefirsttwomomentsofthelatter
(cid:104) (cid:105)
R ll′(z)=E
Φ
R(cid:98)ll′(z) , (148)
(cid:104) (cid:105)
R
l1l 1′l
2′l2(z 1,z 2)=E
Φ
R(cid:98)l1l 1′(z 1)R(cid:98)l 2′l2(z 2) . (149)
Then, substituting (146) into the transfer measure (41) immediately connects it to the projected
resolvent
31PublishedasaconferencepaperatICLR2024
ρ (cid:98)( llf ′)(dλ)= πλ λl yl →im 0+ℑ(cid:8) R(cid:98)ll′(λ+iy)(cid:9) dλ. (150)
Carryingtherelation(150)overtothefirstandsecondmomentsofthetransfermeasuregives
λ
ρ(1)(dλ)= l lim R(im)(λ+iy)dλ, (151)
ll′ πλy→0+ ll′
λ λ
ρ(2) (dλ ,dλ )= l 1′ l 2′ lim R(im) (λ +iy ,λ +iy )dλ dλ , (152)
l1l 1′l 2′l2 1 2 π2λ 1λ 2 y1→0+ l1l 1′l 2′l2 1 1 2 2 1 2
y2→0+
λ2
ρ(ε)(dλ)= l lim R(im)(λ+iy)dλ, (153)
l πλ2y→0+ ll
whereR(im))andR(im) arethemomentsoftheimaginarypartoftheresolventprojections
ll′ l1l 1′l 2′l2
R l( li ′m)(z)=E
DN
(cid:104) ℑ(cid:8) R(cid:98)ll′(z)(cid:9)(cid:105) , (154)
R l( 1im
l
1′)
l
2′l2(z 1,z 2)=E
DN
(cid:104) ℑ(cid:8) R(cid:98)l1l 1′(z 1)(cid:9) ℑ(cid:8) R(cid:98)l 2′l2(z 2)(cid:9)(cid:105) . (155)
Then,thelearningmeasuresdefiningthelossfunctional(8)areobtainedbysummationoverpopu-
lationindicesasin(44).
E.1 RESOLVENT
Giventheconnectionbetweenresolventandlearningmeasuresρ(2),ρ(1),ρ(ε) describedabove,we
seethattheresolventmoments(148)and(149)arefundamentalbuildingblocksforthelossfunc-
tional (8). In this section, we try to calculate these moments and simplify the result as much as
possible.
To start, recall that the 1 Tr[R(cid:98)(z)] gives the Stieltjes transform of the empirical spectral measure
N
µ = 1 (cid:80)N δ . AcentralquantityinourcalculationswillbeStieltjestransformoftheaverage
s(cid:98) pectrN almek a= s1 ureλ(cid:98)k
µ=E[µ]
(cid:98)
(cid:90) ∞ µ(dt) 1 (cid:104) (cid:105)
r(z)≡ = E Tr[R(cid:98)(z)] . (156)
z−t N
−∞
Next,denoteresolventsofkernelmatriceswithoneortwospectralcomponentsremovedas
 −1
(cid:18)
1
(cid:19)−1
1 (cid:88)
R(cid:98)−l(z)= R(cid:98)−1(z)− Nλ lϕ lϕT
l
=
N
λ mϕ mϕT m−zI , (157)
m̸=l
 −1
(cid:18)
1
(cid:19)−1
1 (cid:88)
R(cid:98)−l−l′(z)= R(cid:98)− −1 l(z)− Nλ l′ϕ l′ϕT
l′
=
N
λ mϕ mϕT m−zI , (158)
m̸=l,l′
andtherespectiveStieltjestransformsasr (z), r (z).
−l −l−l′
Inourcalculations,wewillusetwoassumptions
Assumption1. (Self-averagingproperty). ForarandomGaussianvectorϕ ∼ N(0,I)indepen-
dentfromΦ
1 1 (cid:104) (cid:105)
ϕTR(cid:98)(z)ϕ≈ E ϕTR(cid:98)(z)ϕ =r(z).
N N
Assumption2. (Stabilityundereigenvalueremoval)
r (z)≈r (z)≈r(z).
−l−l′ −l
In classical RMT settings, e.g. that of Marchenko-Pastur law, both of these assumptions can be
rigorously shown. Specifically, both the statistical fluctuations of 1ϕTR(cid:98)(z)ϕ and the change of
N
32PublishedasaconferencepaperatICLR2024
r(z)aftereigenvalueremovalgivenocontributiontothelimitingspectralmeasureasN →∞. We
leaveforthefutureworkthevalidityoftheseassumptionsinoursettings.
Asafinalpreparationstep,letuswritedownaquickwayofderivingtheself-consistent(fixed-point)
equationforr(z)usingtheaboveassumptions. Startingwiththetrivialrelation1= 1 Tr[I],weget
N
(cid:34) (cid:35)
1 (cid:104) (cid:16) (cid:88) (cid:17)(cid:105) 1 (cid:88) (cid:104) (cid:105)
1= NE Tr R(cid:98)(z) N1 λ lϕ lϕT
l
−zI =
N
λ lE N1ϕT
l
R(cid:98)(z)ϕ
l
−zr(z). (159)
l l
Next, we relate ϕT
l
R(cid:98)(z)ϕ
l
to ϕT
l
R(cid:98)−l(z)ϕ
l
via Sherman-Morrison formula in order to break the
dependencebetweenϕ andthematrixinside.
l
(cid:16) (cid:17)2
N1ϕT
l
R(cid:98)(z)ϕ
l
= N1ϕT
l
R(cid:98)−l(z)ϕ l− λ 1l +λN1 lϕ N1T l ϕR(cid:98)
T
l− R(cid:98)l( −z l) (ϕ z)l
ϕ l
= 1+N1 λϕ lN1T l ϕR(cid:98)
T
l− R(cid:98)l( −z) lϕ (zl
)ϕ l (160)
( =1) r −l(z) ( =2) r(z)
,
1+λ r (z) 1+λ r(z)
l −l l
wherein(1)and(2)weusedtheassumptions1and2respectively.Substitutingthisinto(159)gives
theself-consistentequation
1=−zr(z)+ 1 (cid:88) r(z)λ l , (161)
N 1+r(z)λ
l
l
whichisoftenwritteninafixed-pointformas
(cid:30)(cid:32) (cid:33)
r(z)=1 −z+ 1 (cid:88) λ l . (162)
N 1+r(z)λ
l
l
Forz =−η <0,wehaver(z)=η−1 foreffectiveregularizationη definedin(1).
eff,N eff,N
E.1.1 COMPUTINGTHERESOLVENTMOMENTS
Reflectionsymmetry. Here, wemainlyrepeatSimonetal.(2023)toestablishusefulexactrela-
tionsforresolventexpectations. RecallthatdistributionofaGaussianrandomvectorz ∼ N(0,I)
remainsinvariantunderorthogonaltransformations:Uz∼N(0,I),whereUisanarbitraryorthog-
onalmatrix.Now,wenoticethatouraveragesofinterest(154)and(155)haveaformE (cid:2) f(K,Φ)(cid:3)
Φ
for some function f(·,·) of empirical kernel matrix K = ΦΛΦT and “features” Φ. Applying
transformationΦ→UΦtoperturbedkernelmatrixΦTUΛUTΦgives
E(cid:2) f(ΦTUΛUTΦ,Φ)(cid:3) =E(cid:2) f(ΦTΛΦ,UΦ)(cid:3)
. (163)
If we take U to be a reflection along one of the basis axes, e.g. (U(m)) = δ (1−2δ ) for
ll′ ll′ lm
reflectionalongaxism,thenU(m)Λ(U(m))T =Λ. Thisimplies
E(cid:2) f(ΦTΛΦ,Φ)(cid:3) =E(cid:104) f(ΦTΛΦ,U(m)Φ)(cid:105)
. (164)
Applying(164)to(154)and(155)givestheirnon-zeroelementsareonlythosewithpairedindices:
R (z)forthefirstmomentandR (z ,z ), R (z ,z ), R (z ,z )forthesecondmoment.
ll lll′l′ 1 2 ll′ll′ 1 2 ll′l′l 1 2
Moreover,notethatweareinterestedonlyinthosecomponentsoftheresolventsecondmomentthat
contributetotheloss(44). ThisleavesR (z ,z )tobetheonlyrelevantcomponents.
ll′l′l 1 2
Firstmoment. Here,allthenecessarycomputationswerealreadydonein(160),leadingto
r(z)
R (z)= . (165)
ll 1+λ r(z)
l
33PublishedasaconferencepaperatICLR2024
Second moment at z = z . In this case, the second moment is connected to the derivative of
1 2
thefirstmoment,whichwasutilizedbySimonetal.(2023)andBordelonetal.(2020)toobtainthe
generalizationerrorofKRR.Indeed,
(cid:20) (cid:21)
R ll′l′l(z,z)=E N1 ϕT
l
R(cid:98)(z)ϕ l N′ϕT l′R(cid:98)(z)ϕ
l
=−E

1 ϕT∂(cid:16) (cid:80) mλ mϕm NϕT m −zI(cid:17)−1
ϕ

=−∂ R (z).
(166)
N l ∂λ l λ l′ ll
l′
As prerequisite for computing the derivative ∂ R (z), we compute similar derivative of inverse
λ l′ ll
Stieltjes transform r−1(z). Differentiating the fixed point equation (161) in the form r−1 +z =
N1 (cid:80)
l
λλ l+lr r− −1
1
w.r.t. toeitherzorλ
l′
gives
(cid:34) (cid:35)
∂r−1 1− 1 (cid:88) λ l + 1 (cid:88) λ lr−1 =−1, (167)
∂z N λ +r−1 N (λ +r−1)2
l l
l l
(cid:34) (cid:35)
∂r−1 1− 1 (cid:88) λ l + 1 (cid:88) λ lr−1 = 1 r−2 , (168)
∂λ N λ +r−1 N (λ +r−1)2 N (λ +r−1)2
l′ l l l′
l l
leadingto
∂r−1 ∂r−1 1 r−2
=− , (169)
∂λ ∂z N (λ +r−1)2
l′ l′
∂r−1 r−1
= . (170)
∂z z− 1 (cid:80) λlr−2
N l (λl+r−1)2
Usingthis,wesecondmomentbecomes
δ 1 r−2∂ r−1
R (z,z)=−∂ R (z)= ll′ − z . (171)
ll′l′l λ l′ ll (λ +r−1)2 N (λ +r−1)2(λ +r−1)2
l l l′
Second moment at z ̸= z with l = l′. Here and in the next case l ̸= l′ we will again ap-
1 2
ply Sherman-Morrison formula, including two subsequent applications to break dependence with
ϕ l,ϕ l′.DenotingR(cid:98)#(z)theresolventwith,possibly,removedeigenvalue,removinganextraeigen-
valuecanbewritteninasimplifiedformunderassumptions1and2
R(cid:98)#(z)=R(cid:98)#−l(z)− 1+λλ l r(z)R(cid:98)#−l(z)ϕl NϕT l R(cid:98)#−l(z)=q(cid:16) R(cid:98)#−l(z),ϕl NϕT l ,a l(z)(cid:17) , (172)
l
whereq(X,Y,a)=X−aXYXisapolynomialintwomatricesX,Yandascalarvariablea,and
a (z)isashorthandnotationforλ /(1+λ r(z)).
l l l
Usingrepresentation(172)wecanwriteoursecondmomentelementas
R (z ,z )=E[Tr[Yq(X ,Y,a )Yq(X ,Y,a )]], (173)
llll 1 2 1 1 2 2
wherewehavedenoted
X 1 =R(cid:98)−l(z 1), X 2 =R(cid:98)−l(z 2), Y = ϕl NϕT l , a 1 =a l(z 1), a 2 =a l(z 2). (174)
NowweexploittheindependencebetweenX ,X andYbyfirsttakingtheexpectationsw.r.t. Y.
1 2
Since Y is product of two Gaussian random variables and (173) is polynomial in Y containing
monomialsofdegreefrom2to4,weneedtocomputeGaussianmomentsoftheorderfrom4to8.
ThiscanbeconvenientlydoneusingWick’stheoremforcomputingmomentsofGaussianrandom
variables,whichequates2m-thmomenttothesumoverallpairingsof2mvariablesoftheproducts
ofmintra-paircovariances. Specifically,fornormalvectorx ∼ N(0,I),itreducestotheproducts
ofDiracdeltas
m m
E(cid:2)
x x ...x x
(cid:3)
=
1 (cid:88) (cid:89) E(cid:2)
x x
(cid:3)
=
1 (cid:88) (cid:89)
δ ,
i1 i2 i2m−1 i2m 2m iσ(2j−1) iσ(2j) 2m iσ(2j−1)iσ(2j)
σ∈S2mj=1 σ∈S2mj=1
(175)
34PublishedasaconferencepaperatICLR2024
whereS denotesthegroupofpermutationsoftheindexset{1,2,...,2m−1,2m}.Nowweapply
2m
Wick’stheoremseparatelytoeachmonomialfrom(173). Forthesimplestorder-twomonomial,we
have
E [Tr[YX YX ]]=N−2 (cid:88) E (cid:2) ϕ ϕ (X ) ϕ ϕ (X ) (cid:3)
Y 1 2 ϕ∼N(0,I) i+ i− 1 i−j+ j+ j− 2 j−i+
i+i−j+j−
=N−2(Tr[X ]Tr[X ]+2Tr[X X ])
1 2 1 2
(cid:18) Tr[X ]−Tr[X ](cid:19) (176)
=N−2 Tr[X ]Tr[X ]+2 1 2
1 2 z −z
1 2
2 r(z )−r(z )
=r(z )r(z )+ 1 2 =r r +O(N−1),
1 2 N z −z 1 2
1 2
wherer ,r isashorthandforr(z ),r(z ). HerewefirstappliedWick’stheorem,thentransformed
1 2 1 2
theproductX X =(X −X )/(z −z ).Then,wewrotetheresolventtracesintermsofStieltjes
1 2 1 2 1 2
transformTr[X ]=r(z )usingassumption(1),thusremovingtheneedtotakeexpectationwith
1,2 1,2
respectX ,X intheremainingcalculationofthesecondmomentR (z ,z ). Animportantob-
1 2 llll 1 2
servationisthatwhencomputingthemomentofordern,theleadingtermin 1 hastobetheproduct
N
oftracesTr[X ],Tr[X ],whilealltheotherterms(containingatleastonetraceofaproduct)will
1 2
giveO(N−1)contribution. Usingtheaboveobservation,wecaneasilycomputeleadingtermsfor
alltheotheraverages:
E [Tr[YX YX YX ]]=N−3Tr[X ]2Tr[X ]+O(N−1)=r2r +O(N−1),
Y 1 1 2 1 2 1 2
E [Tr[YX YX YX ]]=N−3Tr[X ]Tr[X ]2+O(N−1)=r r2+O(N−1), (177)
Y 1 2 2 1 2 1 2
E [Tr[YX YX YX YX ]]=N−4Tr[X ]2Tr[X ]2+O(N−1)=r2r2+O(N−1).
Y 1 1 2 2 1 2 1 2
Combiningallthemonomials,wecansummarizethewholecomputationasfollows
R (z ,z )=q(cid:0) r ,1,a (cid:1) q(cid:0) r ,1,a (cid:1) +O(N−1)= 1 +O(N−1), (178)
llll 1 2 1 1 2 2 (λ +r−1)(λ +r−1)
l 1 l 2
whereinthelastequalityweusedq(r(z),1,a (z))= 1 . Notethatinthelimitz →z ,the
l λl+r−1(z) 1 2
expressionsabovecoincidewithpreviouslyderived(171)uptosubleadingO(N−1)terms.
Secondmomentatz ̸=z withl̸=l′. Hereweneedtoremovetwoeigenvaluesλ andλ . The
1 2 l l′
respectiveresolventexpressionis
R(cid:98)(z)=q(R(cid:98)−l(z),ϕl NϕT l ,a l(z))=q(q(R(cid:98)−l−l′(z),ϕ l N′ϕT l′,a l′(z)),ϕl NϕT l ,a l(z)))
(179)
=q (cid:101)(R(cid:98)−l−l′(z),ϕl NϕT l ,ϕ l N′ϕT l′,a l(z),a l′(z)),
where
q(X,Y,Y′,a,a′)=q(q(X,Y′,a′),Y,a)
(cid:101)
(180)
=X−aXYX−a′XY′X+aa′(cid:0) XYXY′X+XY′XYX(cid:1)
.
Substituting(179)into(149)willproduceanexpressionoftheform
R (z ,z )=E[Tr[Y′q(X ,Y,Y′,a ,a′)Yq(X ,Y,Y′,a ,a′)]], (181)
ll′l′l 1 2 (cid:101) 1 1 1 (cid:101) 2 2 2
whereinadditionto(174)wehavedenotedY′ = ϕ l N′ϕT l′,a′
1
=a l′(z 1),anda′
2
=a l′(z 2).
Now,letusagaincalculatetheexpectationsoverY,Y′ usingWick’stheorem. Thedifferencewith
thel=l′ caseisthatthepairingsbetweenϕ andϕ producezerosduetotheirindependence. For
l l′
example,theexpectationforthesimplestmonomialis
1 (cid:88) (cid:104) (cid:105)
E [Tr[Y′X YX ]]= E ϕ′ ϕ′ (X ) ϕ ϕ (X )
Y,Y′ 1 2 N2 ϕ,ϕ′∼N(0,I) i+ i− 1 i−j+ j+ j− 2 j−i+
i+i−j+j− (182)
1 r(z )−r(z )
=N−2Tr[X X ]=− 1 2 .
1 2 N z −z
1 2
35PublishedasaconferencepaperatICLR2024
Observe that the monomial above has 1 magnitude. This is the consequence that N0 terms from
N
the l = l′ case arose from pairings between Y′ and Y, which are zero in the l ̸= l′ case. Taking
into account the symmetry R (z ,z ) = R (z ,z ), we proceed similarly and calculate the
ll′ll′ 1 2 ll′ll′ 2 1
leadingtermsforalltheotherindependentmonomials.
TermswithX :
2
E [Tr[Y′X Y′X YX ]]=N−3Tr[X X ]Tr[X ]+O(N−2),
Y,Y′ 1 1 2 1 2 1
E [Tr[Y′X YX YX ]]=N−3Tr[X X ]Tr[X ]+O(N−2),
Y,Y′ 1 1 2 1 2 1
(183)
E [Tr[Y′X Y′X YX YX ]]=N−4Tr[X X ]Tr[X ]Tr[X ]+O(N−2),
Y,Y′ 1 1 1 2 1 2 1 2
E [Tr[Y′X YX Y′X YX ]]=3N−43Tr[X X ]Tr[X2]+O(N−3)=O(N−2).
Y,Y′ 1 1 1 2 1 2 1
NewtermswithX YX :
2 2
E [Tr[Y′X Y′X YX YX ]]=N−4Tr[X X ]Tr[X ]Tr[X ]+O(N−2),
Y,Y′ 1 1 2 2 1 2 1 2
E [Tr[Y′X YX YX YX ]]=N−4Tr[X X ]Tr[X ]Tr[X ]+O(N−2),
Y,Y′ 1 1 2 2 1 2 1 2
E [Tr[Y′X Y′X YX YX YX ]]=N−5Tr[X X ]Tr[X ]2Tr[X ]+O(N−2), (184)
Y,Y′ 1 1 1 2 2 1 2 1 2
E [Tr[Y′X YX Y′X YX YX ]]=
Y,Y′ 1 1 1 2 2
=3N−5Tr[X X ]Tr[X2]Tr[X ]+O(N−3)=O(N−2).
1 2 1 2
NewtermswithX Y′X :
2 2
E [Tr[Y′X YX YX Y′X ]]=N−4Tr[X X ]Tr[X ]Tr[X ]+O(N−2),
Y,Y′ 1 1 2 2 1 2 1 2
E [Tr[Y′X Y′X YX YX Y′X ]]=N−5Tr[X X ]Tr[X ]2Tr[X ]+O(N−2),
Y,Y′ 1 1 1 2 2 1 2 1 2
(185)
E [Tr[Y′X YX Y′X YX Y′X ]]=
Y,Y′ 1 1 1 2 2
=3N−5Tr[X X ]Tr[X2]Tr[X ]+O(N−3)=O(N−2).
1 2 1 2
NewtermswithX YX Y′X :
2 2 2
E [Tr[Y′X Y′X YX YX YX Y′X ]]=
Y,Y′ 1 1 1 2 2 2
=N−6Tr[X X ]Tr[X ]2Tr[X ]2+O(N−2),
1 2 1 2
(186)
E [Tr[Y′X YX Y′X YX YX Y′X ]]=
Y,Y′ 1 1 1 2 2 2
=3N−6Tr[X X ]Tr[X ]2Tr[X ]2+O(N−3)=O(N−2).
1 2 1 2
AnewtermwithX Y′X YX :
2 2 2
E [Tr[Y′X YX Y′X YX Y′X YX ]]=
Y,Y′ 1 1 1 2 2 2
(187)
=N−6(cid:0) 6Tr[X X ]3+9Tr[X X ]Tr[X ]2Tr[X ]2(cid:1) +O(N−4)=O(N−3).
1 2 1 2 1 2
To summarize, we observe two types of monomials. The first type has the order O(N−2) and
is composed of those monomials which take X YX Y′X from q(X ,Y,Y′,a ,a′) and/or
1 1 1 (cid:101) 1 1 1
X Y′X YX from q(X ,Y,Y′,a ,a′). The rest of the monomials g(·,·,·,·), contain exactly
2 2 2 (cid:101) 2 2 2
onefactorTr[X 1X 2]=N zr1 1− −r z2 2,havetheorderO(N−1),andsatisfy
(cid:0) (cid:1)
E (cid:2) g(cid:0) X ,X ,Y,Y′(cid:1)(cid:3) = 1 r 1−r 2 g r 1,r 2,1,1 +O(N−2)
Y,Y′ 1 2 N z −z r r
1 2 1 2 (188)
=− 1 r 1−1−r 2−1 g(cid:0) r ,r ,1,1(cid:1) +O(N−2).
N z −z 1 2
1 2
Thus,theleadingO(N−1)termforthesecondmomentwithl̸=l′isgivenby
R (z ,z )=− 1 r 1−1−r 2−1 (cid:89) (cid:2) r −(a +a′)r2+a a′r3(cid:3) +O(N−2)
ll′l′l 1 2 N z −z s s s s s s s
1 2
s=1,2 (189)
1 r−1−r−1 r−1r−1
=− 1 2 1 2 +O(N−2).
N z −z (λ +r−1)(λ +r−1)(λ +r−1)(λ +r−1)
1 2 l 1 l′ 1 l 2 l′ 2
Again,inthelimitz →z ,theresultabovecoincideswithpreviouslyderived(171)uptosublead-
1 2
ingO(N−2)terms.
36PublishedasaconferencepaperatICLR2024
Summationsoverl′ andl in(44). Forthelossfunctionalwewillneednotthebaresecondmo-
mentR (z ,z )butthesum(cid:80) λ2R (z ,z ). First,letusstartwiththecasez =z where
ll′l′l 1 2 l′ l′ ll′l′l 1 2 1 2
thesecondmomentisgivenby(171). Theessentialsumtocomputeis(cid:80) λ2 l′ . Usingthe
l′ (λ l′+r−1)2
fixed-pointequation(161)gives
1 (cid:88) ∂ zr−1λ2 l′ = 1 (cid:88) ∂ r−1λ l′ = ∂ (r−1+z)=1+∂ r−1. (190)
N (λ +r−1)2 N ∂zλ +r−1 ∂z z
l′ l′
l′ l′
Wecansubstitutethisresultintothesecondmomenttogetthedesiredsecondmomentsum.
(cid:88)
λ2R (z,z)=
λ2
l
−(1+∂ zr−1)r−2
. (191)
l′ ll′l′l (λ +r−1)2
l
l′
Now,weproceedtothecasez ̸=z .Similarly,from(189)weseethattheessentialsumtocompute
1 2
is
1 r 1−1−r 2−1 (cid:88) λ2
l′ =
1 1 (cid:88)(cid:18) λ l′r 1−1
−
λ l′r 2−1 (cid:19)
N z −z (λ +r−1)(λ +r−1) N z −z λ +r−1 λ +r−1
1 2 l′ l′ 1 l′ 1 1 2 l′ l′ 1 l′ 2 (192)
r−1−r−1
=1+ 1 2 ,
z −z
1 2
whichisbasicallyafinitedifferenceversionofthesumforz =z . Again,wesubstitutethisresult
1 2
intosecondmomentsumtoget
λ2−r−1r−1(cid:16) 1+ r 1−1−r 2−1(cid:17)
(cid:88) λ2R (z ,z )= l 1 2 z1−z2 +O(N−1). (193)
l′ ll′l′l 1 2 (λ +r−1)(λ +r−1)
l′ l 1 l 2
Nowletusconsiderthesumoverlin(44).First,observethatthefirst-momenttermfrom(44)enters
intheform
(cid:88)
c2λ R
(z)=(cid:88) λ lc2
l =u(z). (194)
l l ll λ +r−1(z)
l
l l
Hereweencounteredthefirstauxiliaryfunctionu(z)introduced(14).However,directlyperforming
the respective sum for the second-moment term does not automatically reduce it to an expression
with“decoupled”factors(dependingonlyonz orz butnotjointlyonz ,z ). Yet,wecanperform
1 2 1 2
anadditionaltransformationtoexpresstheresultintermsofdecoupledfactors. Representationof
fractionsproductasadifference,similarto(192),givesfortwopartsof(cid:80) c2λ2R (z ,z )
l,l′ l l′ ll′l′l 1 2
(cid:88) c2 l(λ2
l
−r 1−1r 2−1) =(cid:88)(cid:16) λ lc2
l +
λ lc2
l
−c2(cid:17)
=u +u
−(cid:88)
c2, (195)
(λ +r−1)(λ +r−1) λ +r−1 λ +r−1 l 1 2 l
l l 1 l 2 l l 1 l 2 l
(cid:88) −c2 lr 1−1r 2−1r 1− z1 1− −r z2 2−1 =r−1r−1(cid:80) l λl+c2 l r 1−1 −(cid:80) l λl+c2 l r 2−1 =r−1r−1v 1−v 2,
(196)
(λ +r−1)(λ +r−1) 1 2 z −z 1 2 z −z
l l 1 l 2 1 2 1 2
wherewehaveencounteredsecondauxiliaryfunctionv(z)definedin(14). Summarizingourcom-
putationofthesecondmoment,wehaveobtained
(cid:88)
c2λ2R (z ,z )=u(z )+u(z )+r−1(z )r−1(z
)v(z 1)−v(z 2) −(cid:88)
c2. (197)
l l′ ll′l′l 1 2 1 2 1 2 z −z l
1 2
l,l′ l
Thus,wefullyexpressedthepopulationsumsofresolventmomentsintermsoftwoauxiliaryfunc-
tionsv(z)andu(z),whichwouldlaterdefineourfinalresultforthelossfunctional.
E.2 LOSSFUNCTIONAL
Inthissection,weusingtheresolventmomentsderivedinSectionE.1.1tocomputethelossfunc-
tional(44).
37PublishedasaconferencepaperatICLR2024
KRRcase. Asasanitycheck,let’sfirstcomputethelossforthecaseofKRRlearningalgorithm
h (z) = z , withtheexpectationtorecovertheoriginalexpression(1). Intheabsenceoftarget
η z+η
noiseσ2 =0,wehave
L [h ]σ2 ==0(cid:88) λ2R (−η,−η)−2λ R (−η)+1
l η l′ ll′l′l l ll
l′ (198)
λ2−η2 (1−∂ η ) λ η2 ∂ η
= l eff η eff −2 l +1= eff η eff .
(λ +η )2 λ +η (λ +η )2
l eff l eff l eff
Forthecontributionofnoisetermwehave
1 (cid:104) (cid:105) 1 (cid:104) (cid:105)
ε( l2) =λ2
l
NE ϕT
l
R(cid:98)2(−η)ϕ
l
=−λ2 l∂ ηNE ϕT
l
R(cid:98)(−η)ϕ
l
(199)
λ2∂ η
=−λ2∂ R (−η)= l η eff .
l η ll (λ +η )2
l eff
Combiningnoiselessandnoisecontributions,weobtain
1 η2 ∂ η σ2 λ2∂ η
L (η)= eff η eff + l η eff , (200)
KRR 2(λ +η )2 2N (λ +η )2
l eff l eff
whichisthesameas(1).
Generalcase. Nowweturntoourmaingoalofdescribinglearningmeasuresρ(2),ρ(1),ρ(ε). De-
note,u(λ)=lim u(λ+iy)andv(λ)=lim v(λ+iy). Then,forthefirstmomentofthe
y→0+ y→0+
learningmeasure,wehave
ρ(1)(dλ)=(cid:88) c c ρ(1)(dλ)= 1 lim (cid:88) c2λ ℑR (λ+iy)dλ= ℑu(λ) dλ. (201)
l,l′
l l′ ll′ πλy→0+
l
l l ll πλ
Fornoisemeasureρ(ε)(dλ)wesimilarlyobtain
l
1 (cid:88) ℑw(λ)
ρ(ε)(dλ)= lim λ2ℑR (λ+iy)dλ= dλ, (202)
πλ2y→0+ l ll πλ2
l
wherew(λ)=lim w(λ+iy)forthelastauxiliaryfunctionw(z)definedin(14).
y→0+
Nowweproceedtothecomputationforthesecondmomentofthelearningmeasureρ(2)(dλ ,dλ )
1 2
usingtherelation(152). Wehave
ρ(2)(dλ ,dλ )= 1 lim (cid:88) c2λ2R(im) (λ +iy ,λ +iy )dλ dλ . (203)
1 2 π2λ 1λ 2y y1 2→ →0 0+
+
l,l′ l l′ l1l 1′l 2′l2 1 1 2 2 1 2
Note that while for the first moment we simply have R(im)(z) = ℑR (z), the relation between
ll′ ll′
R(im) (z ,z ),andpreviouslycomputedR (z ,z )islessstraightforwardsincetheproduct
l1l 1′l 2′l2 1 2 l1l 1′l 2′l2 1 2
oftwoimaginarypartshastobetakenoutofexpectation. Thiscanbedonewiththefollowingtrick:
fortworandomvariablesw ,w wehave
1 2
(cid:20) w w −w w (cid:21) E[w w ]−E[w w ]
E[ℑw ℑw ]=E ℜ 1 2 1 2 =ℜ 1 2 1 2 . (204)
1 2 2 2
Taking w
1
= R(cid:98)ll′(z 1),w
2
= R(cid:98)l′l(z 2), and noting that R(cid:98)(z) = R(cid:98)(z), we get the desired second
momentofimaginaryparts
(cid:104) (cid:105) (cid:20) R (z ,z )−R (z ,z )(cid:21)
R l( li ′m l′) l(z 1,z 2)=E ℑR(cid:98)ll′(z 1)ℑR(cid:98)l′l(z 2) =ℜ ll′l′l 1 2
2
ll′l′l 1 2 . (205)
Observethatthepartu(z )+u(z )−(cid:80) c2of(cid:80) c2λ2R (z ,z )givesnocontributionwhen
1 2 l l l,l′ l l′ l′ll′l 1 2
substitutedinto(205):
(cid:34) (cid:35)
ℜ
(cid:0)
u(z )+u(z
)−(cid:88) c2(cid:1) −(cid:0)
u(z )+u(z
)−(cid:88) c2(cid:1) =ℜ(cid:2)
u(z )−u(z
)(cid:3)
=0. (206)
1 2 l 1 2 l 2 2
l l
38PublishedasaconferencepaperatICLR2024
Indeed, the remaining part has a non-trivial contribution that we will compute below in the limit
z →λ +i0,z →λ +i0.
1 1 2 2
lim (cid:88) c2λ2R(im) (λ +iy ,λ +iy )
y y1 2→ →0 0+
+
l,l′ l l′ l1l 1′l 2′l2 1 1 2 2
1 v(λ +iy )−v(λ +iy )
=− ℜ lim r−1(λ +iy )r−1(λ +iy ) 1 1 2 2
2 y1→0+ 1 1 2 2 λ 1−λ 2+i(y 1−y 2) (207)
y2→0+
1 v(λ +iy )−v(λ −iy )
+ ℜ lim r−1(λ +iy )r−1(λ −iy ) 1 1 2 2 .
2 y1→0+ 1 1 2 2 λ 1−λ 2+i(y 1+y 2)
y2→0+
Thefirstlimitherecanbeeasilytakeninjointwayy =y =y →0 ,leadingto
1 2 +
v(λ +iy )−v(λ +iy ) v(λ )−v(λ )
lim r−1(λ +iy )r−1(λ +iy ) 1 1 2 2 = 1 2 . (208)
y1→0+ 1 1 2 2 λ 1−λ 2+i(y 1−y 2) r(λ 1)r(λ 2)(λ 1−λ 2)
y2→0+
Notethatthisexpressionisregularatλ =λ sinceweassumev(λ)tobedifferentiable.
1 2
Thesecondlimitmighthaveanon-vanishingsingularityatλ =λ ,forwhichwewillneedtouse
1 2
Sokhotski–Plemeljformulalim 1 =iπδ(x)+P(1),whereP denotestheCauchyprincipal
ε→0+ x−iε x
value.
v(λ +iy )−v(λ −iy )
lim r−1(λ +iy )r−1(λ −iy ) 1 1 2 2
y1→0+ 1 1 2 2 λ 1−λ 2+i(y 1+y 2)
y2→0+
v(λ +iy )−v(λ )
= lim r−1(λ +iy )r−1(λ ) 1 1 2
y1→0+ 1 1 2 λ 1−λ 2+iy 1
(cid:32) (cid:33)
v(λ )−v(λ )
=|r−1(λ )|22πℑ{v(λ )}δ(λ −λ )+r−1(λ )r−1(λ )P 1 2 .
1 1 1 2 1 2 λ −λ
1 2
(209)
Note that the singularity at λ = λ under the Cauchy principal value is purely imaginary and,
1 2
therefore, will disappear after taking the real part in (207). Next, let us combine (208) with the
secondtermfrom(209)
1 (cid:110)r−1(λ )r−1(λ )(cid:0) v(λ )−v(λ )(cid:1) −r−1(λ )r−1(λ )(cid:0) v(λ )−v(λ )(cid:1) (cid:111)
1 2 1 2 1 2 1 2
ℜ
2 λ −λ
1 2
ℑ(cid:8) r−1(λ )(cid:9) ℑ(cid:8) r−1(λ )v(λ )(cid:9) −ℑ(cid:8) r−1(λ )(cid:9) ℑ(cid:8) r−1(λ )v(λ )(cid:9)
= 2 1 1 1 2 2 (210)
λ −λ
1 2
ℑ(cid:8) u(λ )(cid:9) ℑ(cid:8) r−1(λ )(cid:9) −ℑ(cid:8) u(λ )(cid:9) ℑ(cid:8) r−1(λ )(cid:9)
2 1 1 2
= ,
λ −λ
1 2
whereinthelastlinewehaveusedtherelationr−1v =(cid:80) c2−u.Finally,wecombinealltheterms
l l
intothelearningmeasuresecondmoment
ρ(2)(dλ ,dλ ) |r−1(λ )|2
1 2 = 1 ℑ{v(λ )}δ(λ −λ )
dλ dλ πλ2 1 1 2
1 2 1 (211)
1 ℑ(cid:8) u(λ )(cid:9) ℑ(cid:8) r−1(λ )(cid:9) −ℑ(cid:8) u(λ )(cid:9) ℑ(cid:8) r−1(λ )(cid:9)
2 1 1 2
+ .
π2λ λ λ −λ
1 2 1 2
Thus,wehavederivedtheexpressions(15)and(16)ofthemainpaper.
E.3 POWER-LAWANSATZ
The analysis of the loss functional given by (15) and (16) is much more tractable for continuous
approximationofourbasicpowerdistributions(2)
(cid:88) δ λl(dλ)→ ν1 λ− ν1−1dλ=µ λ(dλ), (cid:88) c2 lδ λl(dλ)→ ν1 λκ ν−1dλ=µ c(dλ). (212)
l l
39PublishedasaconferencepaperatICLR2024
Inparticular,thefixed-pointequation(161)forStieltjestransformr(z)becomes
z =−r−1(z)+
1 (cid:90) 1 r−1(z) ν1λ− ν1dλ
. (213)
N λ+r−1(z)
0
Wewillencountermanyintegralssimilartotheoneabove,withthegeneralform
(cid:90) 1 λadλ
F (x)≡ , a>−1, x∈/ [−1,0]. (214)
a λ+x
0
SuchintegralcanbeexpressedintermsofHypergeometricfunction F . Thiscanbeimmediately
2 1
usedtogetasymptoticx→0expansion,whichwillbeveryusefullater
F
(cid:0) 1,1+a,2+a,−1(cid:1)
π 1 x
F (x)= 2 1 x =− xa+ + +O(x2). (215)
a (1+a)x sin(πa) a 1−a
Forthisasymptoticexpansion,weassumethatxhasacutalongR anda∈/ Z.
−
BelowwedescribetheessentialstepsincomputingthelossfunctionalfortheWishartmodel.
E.3.1 SOLVINGFIXEDPOINTEQUATION
Inthissection, wewillanalyzeasymptoticN → ∞solutionsoffixed-pointequation(213). Note
thatweareinterestedinthesolutionofthisequationwhenzapproachesthereallinefromabove:
r(λ)= lim r(λ+iε). (216)
ε→0+
First, let us find values of λ when ℑr(λ) = πµ(λ) > 0, which corresponds to support of the
empiricalspectraldensityµ(λ)ofK. Forthis,letuswriter−1(λ)=−τ −iυ,andrewrite(213)in
thelimitε→0asapairofrealequations
1 (cid:90) λ′(τ2+υ2)−(λ′)2τ
λ=τ + µ (dλ′)
N (λ′−τ)2+υ2 λ
1 (cid:90) (λ′)2υ
0=υ− µ (dλ′) (217)
N (λ′−τ)2+υ2 λ
(cid:18) 1 (cid:90) (λ′)2 (cid:19)
=υ 1− µ (dλ′) .
N (λ′−τ)2+υ2 λ
Now, let us fix value of r corresponding to the point outside of the support of µ, where υ = 0,
and therefore τ ∈/ supp(µ ) = [0,1] to ensure convergence of the integral. Since the solution
λ
of the fixed point equation for z ∈ C is unique, there should be no value of λ and υ satisfying
+
equations(217). Butsinceλcanbedefinedforanyvalueofτ,υ,thesecondequationshouldhave
no solutions with υ ̸= 0. Due to the monotonicity of the expression in the brackets, this gives a
necessaryconditionforτ correspondingtothepointoutsidethesupport:
1 (cid:90) (λ′)2
µ (dλ′)<1. (218)
N (λ′−τ)2 λ
Additionally,itiseasytoseethatforthevaluesofτ notsatisfyingtheinequalityabove,thereisa
solutionofthesecondequationin(217)withυ <0,whichinducesthesolutionofthefirstequation
withsomeλ,meaningthatthetriple(λ,τ,υ)correspondsthepointinsideofthesupportofµ.
Theargumentabovefullycharacterizesthesupportofµ: theretwosupportedgesλ ,λ withthe
− +
respectivevaluesτ < 0,τ > 1givenbytheequalityversionof(218). Therightedgeτ should
− + +
beatadistance∼N−1fromλ=1,wherewehave
1 (cid:90) (λ′)2 1+o(1)(cid:90) 1 µ (1)
µ (dλ′)= λ dλ′
N (λ′−τ )2 λ N (λ′−τ )2
+ 0 + (219)
µ (1)(cid:16) 1 1 (cid:17)
= λ − (1+o(1))=1.
N τ −1 τ
+ +
40PublishedasaconferencepaperatICLR2024
Fromthecalculationaboveandthefirstequationin(217),therightedgeisgivenby
µ (1)
τ =1+ λ (1+o(1))
+ N
(220)
λ =τ −
µ λ(1) log(cid:0)
τ
−1(cid:1)
(1+o(1))=1+µ
(1)logN
(1+o(1)).
+ + N + λ N
Turningtotheleftedgeofthesupport,wenotethatithastheorderτ ∼ N−ν. Thus,wecanuse
−
asymptoticexpansionofHypergeometricfunction(215)witha = −1. leadingtoasymptoticform
ν
offixedpointequation
C π/ν
z =−r−1(z)+ Nνr−1+ ν1(z), C
ν
= sin(π/ν), (221)
wherewe,forsimplicity,donotwrite(1+o(1))correctionfactorsintheasymptotic.
Then,fortheleftedgeofthesupportwehave
1=− N1 ν1∂F − ν1( i− ∂τ υ−−iυ)(cid:12) (cid:12) (cid:12)
(cid:12)
=−C Nν ∂(−τ − i∂− υiυ)1− ν1 (cid:12) (cid:12) (cid:12)
(cid:12)
= C Nν ν−
ν
1 (−τ −)− ν1, (222)
υ=0 υ=0
whichgivestherespectiveleftedgevalues
τ =−(cid:0)ν−1C (cid:1)ν N−ν,
− ν ν
1 (ν−1)ν−1(cid:0) C (cid:1)ν (223)
λ = (−τ )= ν N−ν.
− ν−1 − νν
Now,letusgivemoreexplicitsolutionsoffixed-pointequationindifferentscenarios.
To the left of the support (λ < λ ). In this region the solution of fixed point equation is real,
−
and will in fact has a lot of parallels with KRR applied to the Wishart model. Thus, we translate
λ and r(λ) to their KRR notations: η = −λ is the KRR regularization and η = r−1 is the
eff
effectiveimplicitregularization, appearingin(1). Inthesenotations, theasymptoticformoffixed
pointequationbecomes
1=
η
+
C νη− ν1
. (224)
η N eff
eff
When η has the scaling s ≥ ν, we can write η = η N−ν and η = ηN−ν, which satisfies
eff eff (cid:101)eff (cid:101)
N-independentequation
1= η (cid:101) +C η− ν1 . (225)
η ν(cid:101)eff
(cid:101)eff
Theequationabovegivesanontrivialrelationsbetweenηandη . However,whenN−ν ≪η ≪
(cid:101) (cid:101)eff eff
1,orinotherwordsithasthescaling0<s<ν,therelationsimplifytoanexplicitone:
C
η
eff
=η+ Nνη1− ν1. (226)
Insidethesupport(λ < λ < λ ). Inthisregionitisconvenienttowriter(λ) = r (λ)eiϕ(λ),
− + 0
withthephasetakingvaluesintheupperhalf-circle: 0 < ϕ < π. Substitutingr (λ)eiϕ(λ) intothe
0
limitform(221)offixedpointequationwegetapairofrealequations
(cid:18) (cid:19)
λ=r 0−1 −cosϕ+ C Nν(r 0)ν1 cos(cid:0) (1− ν1)ϕ(cid:1) (227)
0=r−1sinϕ− C νr−1+ ν1 sin(cid:0) (1− 1)ϕ(cid:1) . (228)
0 N 0 ν
Letusrewritethesecondequationhereusingtheleftedgevaluer =(−τ )−1
− −
(cid:32) (cid:33)−ν (cid:32) (cid:33)−ν
(1− 1)sinϕ sinϕ
r−1 =r−1 ν =N−ν . (229)
0 − sin(cid:0) (1− 1)ϕ(cid:1) C sin(cid:0) (1− 1)ϕ(cid:1)
ν ν ν
41PublishedasaconferencepaperatICLR2024
Thus, we see that the solution of fixed point inside the support is mostly conveniently described
byusing thephase ϕ asa freevariable, and thenspecifythe restof thevariablesby themappings
ϕ (cid:55)→ r and (r ,ϕ) (cid:55)→ λ given by equations (229) and (227) respectively. In this representation,
0 0
ϕ=0correspondstotheleftedgeλ ofthesupport,whileϕ→πcorrespondstotherightedgeof
−
thesupport.
Asforoutsideofthesupportcase,weseethatonthescaleN−ν thefixed-pointequationadmitsN-
independentform. Specifically,setλ=λ(cid:101)N−ν andr
0
=r (cid:101)0Nν. Then,thetripletr (cid:101)0,ϕ,λ(cid:101)satisfies
λ(cid:101)=−(r (cid:101)0)−1cosϕ+C ν(r (cid:101)0)−1+ ν1 cos(cid:0) (1− ν1)ϕ(cid:1) ,
(cid:32) (cid:33)−ν (230)
sinϕ
r−1 = .
(cid:101)0
C
sin(cid:0)
(1−
1)ϕ(cid:1)
ν ν
Finally,weturntothevaluesN−ν ≪λ≪1,correspondingtothescaling0<s<ν. Inthiscase,
thepairofequations(227),(229)becomes
λ=r−1− C νr−1+ ν1
cos(π)=(cid:18)
π/ν
(cid:19)ν
(cid:0) 1+(π−ϕ)cot(π)(cid:1) ,
0 N 0 ν N(π−ϕ) ν
(231)
(cid:18)
π/ν
(cid:19)ν
r−1 = .
0 N(π−ϕ)
Notingthatintheleadingasymptoticorderr =λ−1,thesecondequationcanberewrittenas
0
λ− ν1 π
π−ϕ= . (232)
N ν
Wecansummarizetheaboveequationsinasinglecomplexequation
(cid:32) (cid:33)
r(λ)=−λ−1 1−C
cos(π/ν)λ− ν1 +iπλ−1− ν1
. (233)
ν N ν N
Inparticular,takingtheimaginarypartofStieltjestransformabovegivesℑr(λ)= 1πµ (λ). This
N λ
implies that for N−ν ≪ λ ≪ 1 the (normalized) empirical eigenvalue density Nµ(λ) coincides
withpopulationdensityµ (λ)asexpected.
λ
E.3.2 LEARNINGMEASURES
In this section, we specify the form of empirical learning measures ρ(2)(dλ ,dλ ), ρ(1)(dλ) and
1 2
ρ(ε)(dλ),derivedin(201),(202)and(16)respectively,forthecaseofpower-lawdistributions(212).
Notethatallthreelearningmeasuresareexpressedintermsofr(λ)andimaginarypartsof3aux-
iliary functions v(λ),u(λ),w(λ), whose asymptotic form we will now analyze. In the continuous
approximation(212)thesefunctionsaregivenby
v(λ)= 1
(cid:90) 1 (λ′)κ ν−1dλ′
, u(λ)= 1
(cid:90) 1 (λ′)κ νdλ′
, w(λ)= 1
(cid:90) 1 (λ′)1− ν1dλ′
. (234)
ν λ′+r−1(λ) ν λ′+r−1(λ) ν λ′+r−1(λ)
0 0 0
Sinceallofthesefunctionshavethesamefunctionalform(cid:82) (λ′)adλ′
, letuswriteanasymptotic
λ′+r−1(λ)
expansion of the imaginary part of this integral, using (215) as a basis. First, we consider λ ≪ 1
and use the leading term of asymptotic expansion (215) together with asymptotic formulas (227)
and(229).
ℑ(cid:90) 1 (λ′)adλ′
=Γ(1+a)Γ(−a)ℑ(cid:8) r−ae−iaϕ(cid:9)
λ′+r−1(λ) 0
0
(cid:32) (cid:33)−aν (235)
sin(aϕ) Nsinϕ
=π .
sin(aπ) C
sin(cid:0)
(1−
1)ϕ(cid:1)
ν ν
42PublishedasaconferencepaperatICLR2024
Importantly, away from left edge λ ≫ λ , or equivalently π − ϕ ≪ 1, the above expression
−
significantlysimplifies
(cid:90) 1 (λ′)adλ′ (cid:18) N(π−ϕ)(cid:19)−aν(cid:16) (cid:17)
ℑ =π 1+O(π−ϕ)
λ′+r−1(λ) π/ν
0 (236)
=πλa(cid:16) 1+O(cid:0)λ−ν1 (cid:1)(cid:17)
.
N
The the leading term above is expected: since ℑr−1(λ) ≪ λ for λ ≫ λ , the fraction can be
−
approximatedwithSokhotskiformulaℑ 1 ≈πδ (λ′). However,ourapproachwithasymp-
λ′+r−1(λ) λ
toticexpansionofHypergeometricfunctionin(215)alsoprovidesanestimationofcorrectionterm
totheSokhotskiformula,whichwouldbedifficulttoobtaindirectly.
Letusnowtakeintoaccounttheomittedsubleadingtermsintheasymptoticexpansion(215). These
termsmakearegularpowerseriesinx,andthereforetheirimaginarypartcanbeestimatedO(ℑx)=
O(r−1(λ))=O(cid:0)λ1−ν1 (cid:1)
. Thus,wecansummarizeourcomputationforλ ≪λ≪1as
N −
ℑ(cid:90) 1 (λ′)adλ′ =πλa+O(cid:0)λa−ν1 (cid:1) +O(cid:0)λ1−ν1 (cid:1)
. (237)
λ′+r−1(λ) N N
0
Notethatwhiletheasymptoticformaboveismostlymeaningfulforλ ≫ λ whenthecorrections
−
as small, we can formally use at for λ ∼ λ but the corrections become of the same order as the
−
leadingterms.
The values of functions ℑv(λ), ℑu(λ), ℑw(λ), can be obtained by using either (235) or (237)
dependingonthescaleofλ. Inparticular,whenλ ≪λ≪1wehave
−
(cid:16)λ1∧(κ ν−1)− ν1 (cid:17)
ℑv(λ)=πµ (λ)+O ,
c N
ℑu(λ)=πλµ
(λ)+O(cid:16)λ1∧κ ν− ν1 (cid:17)
, (238)
c N
ℑw(λ)=πλ2µ
(λ)+O(cid:16)λ1− ν2 (cid:17)
.
λ N
As an important application of the expressions above, let us estimate the scale of the off-diagonal
partρ(2)(λ ,λ )ofthelearningmeasureρ(2)(λ ,λ )givenin(16). Using(238)and(233)weget
off 1 2 1 2
λ1− ν1 λκ ν(cid:0) 1+O(λ−
2
ν1 )+O(λ−
1
ν1 )(cid:1) −λ1− ν1 λκ ν(cid:0) 1+O(λ−
2
ν1 )+O(λ−
1
ν1 )(cid:1)
ρ(2)(λ ,λ )= 2 1 N N 1 2 N N .
off 1 2 π2λ λ N(λ −λ )
1 2 1 2
(239)
Now we will estimate the scale S[ρ(2)] of ρ(2)(λ ,λ ) assuming that λ and λ have the scales
off off 1 2 1 2
s and s respectively. First, assume that λ −λ has the same scale as their maximum λ ∨λ .
1 2 1 2 1 2
Then,thecorrectionstotheleadingtermsinthenumeratorcanbeneglected,andthescalesofboth
denominatorandnumeratoraregivenbyminimalscaleoftwosubtractedterms. Specifically,
S[ρ(2)](s ,s )=(cid:0) (ν−1s + κs )∧(κs + ν−1s )(cid:1) −(s +s −1+s ∧s )
off 1 2 ν 1 ν 2 ν 1 ν 2 1 2 1 2 (240)
≥1−s −s .
1 2
Yet,thescalederivedabovemaynotbevalidwhenλ andλ aretoclosetoeachother.Thisdoesnot
1 2
happen,whichcanbeseen,forexample,bywritingthedifferenceintheenumeratorofρ(2)(λ ,λ )
off 1 2
andobservingthatthewholeexpressionbehavesregularlyasλ →λ .Then,similarargumentation
1 2
togetherwith(235)showsthatthescalederivedin(201)holdswhenboths = s = ν. Thus,for
1 2
allλ ,λ insidethesupportofµwehaveS[ρ(2)](s ,s )≥1−s −s .
1 2 off 1 2 1 2
Finally,weestimatethecontributionofoff-diagonalpartofthesecondmomenttothelossL [h]=
off
1(cid:82) h(λ )h(λ )ρ(2)(dλ ,dλ ). For that, we can use a two-dimensional analog of proposition 2,
2 1 2 off 1 2
whichcanbeprovensimilarly.
43PublishedasaconferencepaperatICLR2024
Proposition6. Letg (λ ,λ )beasequenceoffunctionswithascalingprofileS(g)(s ,s ),andlet
N 1 2 1 2
(cid:82)1 (cid:82)1
a >0beasequenceofscalea>0. Then,thesequenceofintegrals |g (λ ,λ )|dλ dλ
N aN aN N 1 2 1 2
hasthescale
S(cid:104)(cid:90) 1 (cid:90) 1 |g (λ ,λ )|dλ dλ (cid:105) = min (cid:0) S(g)(s ,s )+s +s (cid:1) . (241)
N 1 2 1 2 1 2 1 2
aN aN 0≤s1,s2≤a
ApplyingthisstatementtoL [h]= 1(cid:82) h(λ )h(λ )ρ(2)(dλ ,dλ ),weget
off 2 1 2 off 1 2
(cid:104) (cid:105)
S[L ]≥ min 1−s −s +S(h)(s )+S(h)(s )+s +s ≥1. (242)
off 1 2 1 2 1 2
0≤s≤ν
Inotherwords,wehaveshownthat
(cid:90)
h(λ )h(λ )ρ(2)(dλ ,dλ )=O(N−1). (243)
1 2 off 1 2
Actually, the estimation above is tight, which can be shown, for example, by taking h(λ) = 1
computingtheleadingordertermabove,whichwillbeexactly∼ N−1. NotethepresenceofN−1
term in the loss, regardless of the value of κ,ν is unnatural for our problem because we expect
the rate of the optimal algorithm to be N−κ∧2ν, which can be smaller than N−1. The reason for
this unnaturalterm can betraced back toour calculation ofthe resolvent secondmoment in (189)
and (178), where in the application of the Wick theorem we took into account only the pairings
producing leading order terms in N. Thus, we might expect that taking into account subleading
orderpairingsinWicktheoremwouldliftthe(243)fromO(N−1)toO(N−2). Whatwouldbethe
role of the off-diagonal ρ(2) if we took into account all pairings (i.e. performed non-perturbative
off
computation)isnotclearatthemomentandisaninterestingdirectionforfuturework.
E.3.3 NOISYOBSERVATIONS
For noisy observations, our goal is to show the equivalence between full loss functional and the
NMNOfunctional(Theorem2). Intermsofthepopulationdensitiesµ (λ)andµ (λ),theNMNO
λ c
iswrittenas
1
L(nmno)[h]= 1 (cid:90) (cid:20) σ2 h2(λ)µ (λ)+(cid:0) 1−h(λ)(cid:1)2 µ (λ)(cid:21) dλ. (244)
2 N λ c
N−ν
Letusdecomposethedifferencebetweentwofunctionalsas
1
1 (cid:90) (cid:20) σ2(cid:16)ℑw(λ) (cid:17)
L[h]−L(nmno)[h]= −µ (λ) h2(λ)
2 N πλ2 λ
λ−
(cid:16)|r−1(λ)|2ℑv(λ) (cid:17)
+ −µ (λ) h2(λ)
πλ2 c
(cid:16)ℑu(λ) (cid:17) (cid:21) (245)
−2 −µ (λ) h(λ) dλ
πλ c
N−ν
+1 (cid:90) (cid:20) σ2 h2(λ)µ (λ)+(cid:0) 1−h(λ)(cid:1)2 µ (λ)(cid:21) dλ+O(N−1).
2 N λ c
λ−
Now,similarlytothetranslation-invariantmodelonacircle,wewritedownthescalesofalltermsin
thedifferencebetweentwofunctionals,assumingthatλhasthescales. Forthat,weuseasymptotic
expressions(238)forfunctionsv,u,w,andalsoexpression(233)forr−1(λ).
44PublishedasaconferencepaperatICLR2024
(cid:20) σ2(cid:16)ℑw(λ) (cid:17) (cid:21)
S −µ (λ) h2(λ) ≥1−s− s +(1− s)+2S(h)(s), (246)
N πλ2 λ ν ν
(cid:20)(cid:16)|r−1(λ)|2ℑv(λ) (cid:17) (cid:21)
S −µ (λ) h2(λ) ≥ κs−s+(1− s)+0∧(2ν−κs)+2S(h)(s), (247)
πλ2 c ν ν ν
(cid:20)(cid:16)ℑu(λ) (cid:17) (cid:21)
S −µ (λ) h(λ) ≥ κs−s+(1− s)+0∧(ν−κs)+S(h)(s), (248)
πλ c ν ν ν
(cid:20) σ2 (cid:21)
S h2(λ)µ (λ) =1−s− s +2S(h)(s), (249)
N λ ν
S(cid:104)(cid:0) 1−h(λ)(cid:1)2 µ (λ)(cid:105) = κs−s+2S(1−h)(s). (250)
c ν
Forthelasttwoterms,wedoitonthelevelofthewholeintegral,whichistakenoverasinglescale
s=ν.
 
N−ν
(cid:90) σ2
S h2(λ)µ (λ)dλ=2S(h)(ν), (251)
 N λ 
λ−
 
N−ν
(cid:90)
S (cid:0) 1−h(λ)(cid:1)2 µ (λ)dλ=κ+2S(1−h)(ν). (252)
 c 
λ−
Using the scales derived above, we need to obtain the conditions on the learning algorithm scales
S(1−h),S(h)forwhichthescaleofallcorrectionstothelossisgreaterthanthescaleofNMNOloss
given,asusual,by
S(cid:104) L(nmno)[h](cid:105) = min (cid:104)(cid:0)κs+2S(1−h)(cid:1) ∧(cid:0) 1− s +2S(1−h)(s)(cid:1)(cid:105) ≤ κ . (253)
0≤s≤ν ν ν κ+1
Again, this means that we can neglect all corrections whose scale (after integration with dλ) is
greaterthan κ . Inparticular, wecanneglectO(N−1)correctioncomingfromoff-diagonalpart
κ+1
ofthesecondmoment(243).
First, note that the terms (251) and (252) exactly equal the contribution of noise (249) and sig-
nal (250) parts at scale s = ν. Thus, if these terms are to be neglected, s = ν should not be a
localizationscaleofL(nmno)[h].
Second,observethattherightpartsof0∧(2ν−κs)and0∧(ν−κs)in(247)and(248),whenactivated,
ν ν
giveatmostO(N−1)contributiontothetotalloss,andthereforecanalsobeneglected. Then,after
choosingtheleft0option,weseethat(248)isneverlessthan(247). Thus,thetotalcontributionof
thesetwotermscanbeeffectivelydescribedbythescale κs−s+(1− s)+S(h)(s).
ν ν
Lastly,wecanseethat(246)isalwayslargerthancorrespondingNMNOnoisescale(249),except
fors=ν wheretheyareequal. Butsincewealreadyrequires=ν notbeingalocalizationscaleof
L(nmno)[h],theterm(246)canbeneglected.
Thus, all requirements in addition to s = ν not being a localization scale of L(nmno)[h] can be
summarizedas
min (cid:104) κs+(1− s)+S(h)(s)(cid:105) > min (cid:104)(cid:0)κs+2S(1−h)(cid:1) ∧(cid:0) 1− s +2S(1−h)(s)(cid:1)(cid:105) . (254)
ν ν ν ν
0≤s≤ν 0≤s≤ν
However,wehave
(cid:104) (cid:105) κ
min κs+(1− s)+S(h)(s) ≥1∧κ> , (255)
0≤s≤ν ν ν κ+1
therefore, the requirement (254) is satisfied automatically. Summarizing, the corrections to the
NMNOfunctionalareasymptoticallyvanishingwhens=νisnotalocalizationscaleofL(nmno)[h].
45PublishedasaconferencepaperatICLR2024
E.3.4 NOISELESSOBSERVATIONS
Now,wetakeσ2 =0andanalyzetherespectivelossfunctional. Recallthatourcurrentcalculations
are accurate up to O(N−1), as we discussed before. Therefore, we will neglect all the terms that
givecontributionsoftheorderO(N−1), sincetheyarebeyondourcurrentlevelofaccuracy. This
meansseveralthings:
1. Weignorethecontributiontothelossoftheoff-diagonalpart(cid:82) h(λ )h(λ )ρ(2)(dλ ,dλ )
1 2 off 1 2
ofthefunctional.
2.
WeignoretheO(λ1−ν1
)correctiontoℑv(λ)andℑu(λ)in(238): intheprevioussection
N
wehaveshownthattheyhaveatmostO(N−1)contributiontotheloss.
3. Werestrictourselveswiththevaluesofsignalexponentκ < 1. Sincetheoptimalscaling
ofthelossinthenoiselesscaseisexpectedtobeN−κ,wewillnotbeabletocaptureitfor
κ≥1.
4. Thepreviouspointalsoimpliesthatwecannotaccessvaluesκ>2ν >2correspondingto
thenoiselesssaturationphasesinceeigenvalueexponentvaluesareconfinedtothephysical
regionν >1.
Withtheaboveremarkstakenintoaccount,westartderivingthelossfunctionaland,forcompact-
ness,ignorealltheO(N−1)terms. First,duetothediagonalityoftheremainingpartofthelearning
measure ρ(2)(dλ ,dλ ), we can immediately specify the optimal algorithm and the respective de-
1 2
compositionofthefunctional
L[h]=
1
(cid:90)1(cid:34)
|r−1(λ)|2ℑv(λ)(cid:0) h(λ)−h∗(λ)(cid:1)2 +(cid:16)
µ (λ)−
(cid:0) ℑu(λ)(cid:1)2 (cid:17)(cid:35)
dλ, (256)
2 πλ2 c π|r−1(λ)|2ℑv(λ)
λ−
h∗(λ)=
λℑu(λ) =1+O(λ−ν1
), (257)
|r−1(λ)|2ℑv(λ) N
where we have used asymptotics (238) to estimate the deviation of the optimal algorithm from
h(λ) = 1.
Againusing(238),weseethatthefreetermin(256)hastheorderO(λκ− ν1−1
),which,
N
given κ < 1, always localizes on the maximal scale s = ν. If we additionally assume that the
(cid:113)
learning algorithms fits higher eigenvalues well enough: |1−h(λ)| = O(
λ−ν1
), the first term
N
in(256)willalsoalwayslocalizeonthemaximalscales=ν.
Now,relyingonthefactthatthelosslocalizesons = ν,wecanuseexpressionsofallthetermsin
thefunctionalthroughthephaseϕ. Eigenvalueλisdefinedbyanexplicitformof(227)
(cid:32) (cid:33)−ν
λ=N−ν sinϕ sinϕ(cid:0) cot(ν−1ϕ)−cotϕ(cid:1) . (258)
C sin(ϕ− ϕ) ν
ν ν
Using(235)and(229),thetermsofthefunctionalbecome
(cid:32) (cid:33)−κ+ν
|r−1(λ)|2ℑv(λ) 1 sinϕ sin(ϕ− κϕ)
= ν (259)
πλ2 ν C νsin(ϕ− ϕ ν) sin(κ νπ)sin2ϕ(cid:0) cot(ν− ν1ϕ)−cotϕ(cid:1)2
(cid:32) (cid:33)−κ+ν κ
µ (λ)=
1 sinϕ (cid:16) sinϕ(cid:0) cot(ν−1ϕ)−cotϕ(cid:1)(cid:17) ν−1
(260)
c ν C sin(ϕ− ϕ) ν
ν ν
(cid:0) ℑu(λ)(cid:1)2 1 (cid:32) sinϕ (cid:33)−κ+ν sin2(κϕ)
= ν , (261)
π|r−1(λ)|2ℑv(λ) ν C sin(ϕ− ϕ) sin(ϕ− κϕ)sin(κπ)
ν ν ν ν
Theoptimalalgorithmis
cot(ν−1ϕ)−cotϕ
h∗(λ)= ν . (262)
cot(κϕ)−cotϕ
ν
46PublishedasaconferencepaperatICLR2024
Substitutingeverythinginthelossfunctionalgives
N−κ
π (cid:90)− νπ N(cid:32)
sinϕ
(cid:33)−κ+ν(cid:34)
sin(ϕ−
κϕ)(cid:0) h(λ)−h∗(λ)(cid:1)2
L[h]= ν
2
0
C νsin(ϕ− ϕ ν) νsin(κ νπ)sin2ϕ(cid:0) cot(ν− ν1ϕ)−cotϕ(cid:1)2
(263)
+
1(cid:16) sinϕ(cid:0) cot(ν−1ϕ)−cotϕ(cid:1)(cid:17)κ ν−1
−
sin2(κ νϕ) (cid:35) d(Nνλ)
dϕ,
ν ν νsin(ϕ− κϕ)sin(κπ) dϕ
ν ν
Finally, we note that the ϕ → π asymptotic of the expressions under the integral above can be
inferredfromtherespective
λ−ν1
→0asymptoticoftheoriginalfunctional,since
N
π−ϕ=
πλ−ν1
(1+o(π−ϕ)). Thus,wecanwrite
ν N
π− π
L[h]= N−κ (cid:90)νN (cid:104) O(cid:16) (π−ϕ)−κ+ν(cid:17)(cid:0) h(λ )−h∗(λ )(cid:1)2 +O(cid:16) (π−ϕ)−κ+ν+1(cid:17)(cid:105) dϕ. (264)
2 ϕ ϕ
0
√
Fromtheaboveexpressionweseethat,if|h(λ )−h∗(λ )|=O(|h(λ )−1|)=O( π−ϕ),the
ϕ ϕ ϕ
expressionundertheintegralisintegrableatϕ = π. Therefore, theupperintegrationlimitcanbe
shiftedtoπwithoutchangingleadingorderasymptoticoftheloss.
Overlearningtransition. Aswediscussedabove,ouranalysisofWishartmodelinthenoiseless
case is restricted to the values of target exponent κ < 1 due to O(N−1) accuracy of calculations.
Thus,ifν < 2,wecanaccessoverlearningtransitionpointκ = ν −1 < 1andverifywhetherthe
transition holds for the Wishart model. Proceeding similarly to respective Lemma 2 for the circle
model,wehave
Lemma3. Considertheoptimalalgorithm(262):
cot(ν−1ϕ)−cotϕ
h∗(λ)= ν (265)
cot(κϕ)−cotϕ
ν
witheigenvalueλ=λ(ϕ),ϕ∈(0,π)parameterizedaccordingto(258). Then,assumingκ<1,for
anyϕ∈(0,π)

<1, κ+1<ν,

h∗(λ) =1, κ+1=ν, (266)
>1, κ+1>ν.
Proof. Observe that both the nominator and denominator of (265) have the form g(a,ϕ) ≡
cot(aϕ) − cot(ϕ) with a ∈ (0,1), ϕ ∈ (0,π). Since cot(ϕ) is strictly decreasing on (0,π),
thefunctiong(a,ϕ)is1)positive2)atfixedϕisstrictlydecreasingina. Thus,fortheratioofsuch
functions,wehave

<1, a>b,
g(a,ϕ)
=1, a=b, (267)
g(b,ϕ)
>1, a<b.
Theaboveisequivalenttothestatementofthelemmaoncea= ν−1, b= κ.
ν ν
The above result shows that the behavior of the sign of h(λ) − 1, which indicates overlearn-
ing/underlearning, is exactly the same in the Wishart and Circle models. This makes us believe
thatoverlearningtransitioncanbeaquitegeneralphenomenongoingbeyondtwodatamodelscon-
sideredinthecurrentwork.
F EXPERIMENTS
F.1 FIGURE1: DETAILSANDDISCUSSION.
Letusstartbydescribingtheexperimentsettinganddetails. BothKRRandGFplotsuseoptimally
scaledregularizationη andtimet, asderivedinSectionC.Forallthreedatamodels, weconsider
47PublishedasaconferencepaperatICLR2024
ideal power-law population spectrum: λ = l−ν, c2 = l−κ−1 (truncated at P = 4×104 due to
l l
computationallimitations),andanadaptedversionλ =(2(|l|+1))−ν,|c |2 =(2(|l|+1))−κ−1, l∈
l l
ZforCirclemodel. Theextrafactorof2hereisneededtoasymptoticallyalignpopulationspectrum
atsmallλ: forallthreemodelswehaveµ c([0,λ])→ κ1λκ ν andµ λ([λ,∞))→λ− ν1 asλ→0.
Thefigurecontains3typesofdatathatarecomputedindifferentways. Thefirsttypeisscatterplot
markersandcorrespondstotheestimationofgeneralizationlossviadirectsimulation. ForWishart
andCosineWishart(seeSectionF.3)models,thisamountstosamplingempiricalkernelmatrixK
andobservationvectory,calculatingthegeneralizationerrorfortheresultingsampledrealization,
and finally averaging the result over n = 100 repetitions of the above procedure to estimate the
expectation over training dataset D in (3). Due to computational limitations, we were able to
N
execute this procedure for sizes of empirical kernel matrix only up to N = 4×103. For Circle
model, Theorem 1 gives an exact value of the expected loss (3) (i.e. no approximations are made
duringthederivation). SincefortheconsideredtypeofpopulationspectratheN-perturbations(10)
canbeexpressedintermsofHurwitzzetafunction(see(127)),wecomputeanalyticallyalltheterms
in(11). Thisway,weareabletoreachmuchlargervaluesofN forthecirclemodel.
Letusalsocommentontheestimationofthegeneralizationerror(3)ofagivenrealizationofWishart
orCosineWishartmodels. Theexpectationoverxrequiresscalarproducts
(cid:88) (cid:88)
⟨K(x ,x),K(x,x )⟩= λ2ϕ (x )ϕ (x ), ⟨f∗(x),K(x,x )⟩= λ c ϕ (x ) (268)
i j l l i l j j l l l j
l l
for points x ,x from sampled dataset D . The expressions above can be used to calculate these
i j N
scalar products experimentally since we have access to λ ,c and already sampled feature values
l l
ϕ (x ).
l i
The second type of data is depicted with solid lines and corresponds to the direct calculation of
NMNOloss(17),whichbecomesasumfordiscretepopulationspectrum. Assumingλ aresorted
l
indescendingorder,NMNOlossbecomes
1(cid:88)N (cid:20) σ2 (cid:21)
L(nmno)[h]= c2(1−h(λ ))2+ h(λ )2 , (269)
2 l l N l
l=1
whichwecaneasilycomputeforquitelargevaluesofN.
Finally, the third type of data is N → ∞ loss asymptotic L = CN−#(1+o(1)) depicted with
dashedlines.TheconstantCwecomputeanalytically,similarlytolimitingexpressions(138),(135)
fornoiselessCirclemodeland(263)forWishartmodel. Forthat, we1)recall(seeSectionC)the
loss localization scale s for the considered algorithms: s = ν for GD and non-saturated
loc loc κ+1
KRR,ands ∈ {0, ν }forsaturatedKRR2)andthengetthelimitingexpressionsofNMNO
loc 2ν+1
lossfunctional(17).
ForGF,wehavealgorithmprofileh (λ)=1−e−tλandlosslocalizationscaleiss = ν .Thus,
t loc κ+1
we make a change of variables λ = λ′N− κ+ν 1 and t = t′Nκ+ν 1, leading to the following limiting
loss
1 1 (cid:90) ∞(cid:104) (cid:105)dλ′
L(nmno)[h t]= 2N− κ+κ 1
ν
e−2t′λ′ (λ′)κ ν +σ2(1−e−t′λ′ )2(λ′)− ν1
λ′
. (270)
0
Notethattheintegralaboveconvergesbothatλ′ → 0andλ′ → ∞,whichreflectsthatourchange
ofvariableshasusedthecorrectlosslocalizationscale.Theintegralin(270)canbecomputedeither
numericallyoranalyticallybyreducingittoGammafunctions.
For KRR algorithm profile is h (λ) = λ and loss localization changes between saturated and
η λ+η
non-saturatedphases. Inthenon-saturatedphase,wemakeachangeofvariablesλ=λ′N− κ+ν 1 and
η =η′N− κ+ν 1,leadingto
L(nmno)[h η]= 21 N− κ+κ 1 ν1 (cid:90) ∞ (η′)2(λ (′) λκ ν ′++ ησ ′2 )2(λ′)2− ν1 d λλ ′′ , κ<2ν. (271)
0
In the saturated phase, the noise part localize at s = ν , which coincides with the scale of
loc 2ν+1
regularization η = η′N− 2νν +1. The signal part localizes at s
loc
= 0 where the loss functional
48PublishedasaconferencepaperatICLR2024
remainsdiscrete, andwecan useaperturbativeexpressionfor thelearningalgorithmh (λ) ≈ λ.
η η
Thisleadsto
(cid:34) (cid:35)
L(nmno)[h η]=
1
2N− 2ν2 +ν 1
ν1 (cid:90) ∞
σ2
(λ(λ ′′ +)2− η′ν1 )2d λλ ′′ +(η′)2(cid:88)|c
λl
2|2
, κ>2ν. (272)
0 l l
AsfortheGF,theintegralsin(271)and(272)canbecomputedeithernumericallyoranalytically
byreducingthemtoBetafunctionintegralswithachangeofintegrationvariablez =
η′
.
λ′+η′
Discussion. Themainconclusionofthefigureis,indeed,thematchbetweentheactualvaluesof
the loss for a given model (scatter markers) and NMNO values (solid lines) for large enough N.
ThisvalidatesexperimentallythestatementofTheorem2. Importantly, theCosineWishartmodel
isnotcoveredbyourtheorybutstilldemonstratestheequivalence. Weinterpretitasanindication
thattheequivalenceholdsforabroaderclassofmodels. Atthemoment, wearenotreadytogive
any potential candidates for such class of models since it requires a different set of tools than the
oneusedinthiswork.
Observe that the match between the actual loss of a data model and its NMNO analog happens at
quite small N for big value of target exponent κ = 5 (two right subfigures) whereas much larger
N are required for small κ = 0.5. This is a general manifestation of the fact that slower power-
lawsrequiremuchlargersizesN forasymptoticN → ∞behaviortostartworking. Forinstance,
NMNOloss(17)ignorestheerrorassociatedwithunlearnedsignalateigenvaluesλ < λ . The
min
contribution of this part can be roughly estimated as µ ([0,λ ])/µ ([0,1]) ≈ N−κ = e−logNκ
c N c
whichbecomesnegligibleatexponentiallylargevaluesofdatasetsizeN ≫eκ1.
Finally, we note that only the third subplot of the figure (corresponding to saturated KRR) has
differentlossasymptoticsfortheCirclemodelononesideandWishartandCosineWishartonthe
other side. This difference is due to the respective population spectra which, although matching
asymptoticallyatλ → 0,aredifferentattheheadofthespectrumλ ∼ 1. Then,sincethelossfor
saturatedKRRlocalizesatthescales = 0(i.e. λ ∼ 1)asdemonstratedinFigure2,thedifference
betweenpopulationspectraatλ∼1starttoaffectthelossvalues. Onthecontrary,whenthereisno
saturation(therestoftheplotsonthefigure),thelosslocalizesonsomescales > 0wherethetwo
populationspectrabecome(asymptotically)thesame. Thisisthereasonwhy1,2,4subplotshavea
singlecommonlossasymptoticcoloredingrey.
F.2 FIGURE3: DETAILSANDDISCUSSION.
Thisfigure’sprimaryfocusisthecomparisonofdifferentalgorithms. Profilesofeachof4consid-
eredalgorithmswereobtainedasfollows: interpolationissimplyh(λ) = 1,theoptimalalgorithm
was calculated from (23), optimally stopped GF was obtained by first evaluating the asymptotic
loss(138)onawideanddensegridoftvaluessubsequentlypickingt∗ astheoptimalamongthose
values,andforoptimallyregularizedKRRthesameprocedure(includingbothnegativeandpositive
regularizationvalues)wasused.
Onthefirstplot,wevalidatetheasymptoticlossvalueL(asym) = CN−κ (dashedlines)computed
from(138)withexactlossvalueatfiniteN givenby(11)(scattermarkers)computedsimilartothe
respective values from Figure 1. On the remaining 3 plots we already exclusively use asymptotic
expression(138).
Discussion. The first plot indeed confirms that asymptotic expression L(asym) = CN−κ given
by(138)canaccuratelydescribethelossvalueswiththecorrectconstantC.
ThesecondplotexaminesthebehavioroftheconstantC acrosstherangeoftargetexponentvalues
κ∈(0,2ν),whilesaturatedvaluesκ>2ν havedifferentrateN−2ν andthusnotconsideredonthe
figure. Inparticular,inthevicinityofthesaturationtransitiontheconstantstartstoblowupC →∞
as κ → 2ν −0. The overlearning transition at κ = ν −1 is also quite visible: for κ < ν −1
all the considered algorithms have very close loss values, while for κ > ν −1 a significant gap
appearsbetweenoptimalKRRandtheoptimalalgorithmononesideandinterpolationandoptimal
GFontheotherside.Thisdemonstratesthatsignificantoverlearningh(λ)>1isrequiredforstrong
49PublishedasaconferencepaperatICLR2024
performance. NotethatGFprofileh (λ)=1−e−tλ ≤1whichforcest∗ =∞forκ>ν−1,thus
t
explainingexactmatchbetweengreenandgreylinesforκ<ν−1.
Actually, itisquite difficulttodistinguishlosscurves ofdifferentalgorithms(exceptforthe over-
learninggapdiscussedabove)onthetwoleftplots. However,thereisawell-defined(i.e. notdueto
numericalerrors)differencebetweenthealgorithms(highzoomisrequiredtoseeit!). Suchasmall
differenceseemstobeanintrinsicpropertyoftheCirclemodel: wehavetriedwiderangesofκand
ν and observed that in all of them, the relative difference between different algorithms was of the
orderof1%orsmaller.
F.3 COSINEWISHARTMODEL
Themodelisconstructedasfollows. Givenapopulationspectrumλ ,c ∈R, l=0,1,2,3,...,the
l l
featuresϕ (x)withx∈S aredefinedas
l
(cid:26)
1, l=0,
ϕ (x)= √ (273)
l 2cos(lx), l≥1,
andthetrainingdatasetinputsx ∈ D aresampledi.i.d. fromauniformdistributiononthecircle
i N
S. Inparticular,(273)ensuresthatE [ϕ (x)ϕ (x)]=δ .
x l l′ ll′
The Cosine Wishart model can be thought as something intermediate between our main Wishart
and Circle models. On the one side, the population quantities of Cosine Wishart models are the
same as those for the Circle given in (6), except for the presence of eil(x+x′) terms in the kernel
K(x,x′)makingitnon-translationinvariant. ThislastaspectdoesnotmoveCosineWishartmodel
toomuchfromCircle,astherespectiveempiricalkernelmatrixKwouldstillbealmostdiagonalized
bydiscreteFourierharmonicsei2π Nki iftheinputswereformingaregularlattice. Themoreimpor-
tant difference is that inputs x of the Cosine Wishart model are sampled i.i.d., which completely
i
eliminates the possibility to analytically diagonalize K, which is a core step of the Circle model
solution.
Ontheotherside,foragivenpopulationspectrumλ ,c ,thestructureofCosineWishartmodeland
l l
Wishart model are similar in the sense that in both cases, random realizations of empirical kernel
matrixKareobtainedbysamplingcomponentsofthefeaturematrixΦwithfirsttomomentsbeing
EΦ = 0andEΦ2 = 1,exceptforEΦ = 1forCosineWishartwhichonlyamountstoasingle
li li 0i
spikeinK. However,themostimportantdifferenceisthatforWishartmodel,allentriesofΦare
√
independent,whileforCosineWishartthereisaverystrongcorrelationofentriesΦ = 2cos(lx )
li i
with different l but the same i. This correlation turns out to be crucial and makes the Stieltjes
(cid:104)(cid:0) (cid:1)−1(cid:105)
transformr(z)=Tr K−zI nolongerdeterministic,whichwasthecoreassumptionforour
analysisofWishartmodelbasedonthefixed-pointequation(13).
50