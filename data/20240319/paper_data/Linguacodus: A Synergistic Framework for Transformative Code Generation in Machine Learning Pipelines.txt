1
Graphical Abstract
Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learn-
ingPipelines
EkaterinaTrofimova,EmilSataev,AndreyE.Ustyuzhanin
Linguacodus: minimal human interaction ML problem natural
Problem Statement
description to code transformation
Plain English description Plain English description The best high-level code
of a Machine Learning Code for ML task of a ML task instructions Code for ML task
(ML) task
Vanilla Fine-tuned
GPT-3.5
Llama-2
inference
Vanilla GPT-3.5 generated code part by natural ML task description
•No model specified
•No ability for the solution quality assessment
•Too generic solution
Linguacodus generated code part by natural ML task description
Conclusion:
•Linguacodus is a flexible and efficient approach to transforming unstructured ML task
descriptions into executable code.
•Linguacodus is tested but not limited to the Python language, showcasing its
adaptability and applicability across diverse programming languages.
1Hereandlater,"***"designatestheomittedportionsofcode,simplifiedforclaritybytheauthors.
4202
raM
81
]GL.sc[
1v58511.3042:viXraLinguacodus: A Synergistic Framework for Transformative Code
Generation in Machine Learning Pipelines⋆
EkaterinaTrofimovaa,∗, EmilSataeva and AndreyE.Ustyuzhaninb,c
aComputerScienceDepartment,HSEUniversity,11Pokrovskyblvd.,Moscow,109028,Russia
bConstructorUniversity,CampusRing1,Bremen,28759,Germany
cIFIM,NationalUniversityofSingapore,4ScienceDrive2,117544,Singapore
ARTICLE INFO ABSTRACT
Keywords: In the ever-evolving landscape of machine learning, seamless translation of natural language de-
automatedcodegeneration scriptionsintoexecutablecoderemainsaformidablechallenge.ThispaperintroducesLinguacodus,
largelanguagemodels an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that
machinelearningpipelines iterativelytransformsnaturallanguagetaskdescriptionsintocodethroughhigh-leveldata-shaping
autoML instructions.ThecoreofLinguacodusisafine-tunedlargelanguagemodel(LLM),empoweredto
evaluatediversesolutionsforvariousproblemsandselectthemostfittingoneforagiventask.This
paperdetailsthefine-tuningprocess,andshedslightonhownaturallanguagedescriptionscanbe
translatedintofunctionalcode.Linguacodusrepresentsasubstantialleaptowardsautomatedcode
generation, effectively bridging the gap between task descriptions and executable code. It holds
great promise for advancing machine learning applications across diverse domains. Additionally,
we propose an algorithm capable of transforming a natural description of an ML task into code
withminimalhumaninteraction.Inextensiveexperimentsonavastmachinelearningcodedataset
originatingfromKaggle,weshowcasetheeffectivenessofLinguacodus.Theinvestigationshighlight
itspotentialapplicationsacrossdiversedomains,emphasizingitsimpactonappliedmachinelearning
invariousscientificfields.
1. Introduction Horvitz, Kamar, Lee, Lee, Li, Lundberg et al., 2023), and
CodeLlama(Roziere,Gehring,Gloeckle,Sootla,Gat,Tan,
Automatedcodegenerationfromnaturallanguageaims
Adi,Liu,Remez,Rapinetal.,2023)haveplayedasignificant
to make programming more accessible and streamline the
role in these advancements. These models have proved to
software development process, especially in the field of
be a strong instrument for writing code based on a natural
machinelearning(ML).Byconvertinghumanlanguageinto
languagedescription.
executablecode,systemslikeLinguacodusenablequickpro-
Despite progress in code generation, there remains a
totyping,easeiteration,andfacilitatethedeploymentofML
gap in effectively transforming complex machine learning
models, potentially democratizing software development.
(ML)taskdescriptionsintoprecisecode.Currentgenerative
Thisbreakthroughallowsindividualswithoutextensivecod-
modelsoftenproducecommonyetsuboptimalcodesnippets
ingskillstoengageincreatingcomplexMLtasks,promoting
basedonthetext.Thechallengeliesinconvertingdetailed
innovation across various disciplines. The drive for such
ML task narratives into a structured series of code com-
technology underlines a vision to broaden ML’s reach and
ponents, as LLMs excel more with direct instructions. By
impact,simplifyingthedevelopmentprocessandinvitinga
"instructions"wemeenthehigh-levelguidanceprovidedto
wideraudiencetocontributetotechnologicaladvancements.
themodelforgeneratingspecificoutputs.(SeeSection3.1)
Inrecentyears,therehavebeensignificantadvancements
Addressing these challenges, our approach, Linguaco-
inthefieldofcodegenerationandtext-to-codeconversion,
dus,seekstoprovideamoreaccurateandflexiblesolution.
primarilypropelledbytheadventofLargeLanguageModels
Itinvolvesatwo-stepprocess:first,ittransformsthehuman-
(LLMs).Noteworthycontributionsfromleadingresearchers
provided ML task descriptions into a set of explicit, high-
such as GPT models (Achiam, Adler, Agarwal, Ahmad,
level instructions. This step ensures the instructions are
Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anad-
clear, verifiable, and understandable to the user, laying a
kat et al., 2023) (Bubeck, Chandrasekaran, Eldan, Gehrke,
solid foundation for the next phase. Then, these high-level
⋆The reported study utilized the supercomputer resources of HSE instructions are translated into machine-compilable code
University.Theresearchleadingtotheseresultshasreceivedfundingfrom representation, specifically Python code in our case, with
thegrantforresearchcentersinthefieldofAIprovidedbytheAnalytical potential for extension to other programming languages.
CenterfortheGovernmentoftheRussianFederation(ACRF)inaccordance
Thismethodnotonlyaccommodatestheintricatenatureof
withtheagreementontheprovisionofsubsidies(identifieroftheagreement
000000D730321P5Q0002)andtheagreementwithHSEUniversityNo.70- MLtasksbutalsoenhancesthecontrolandprecisioninthe
2021-00139.Thefundershadnoroleinstudydesign,datacollectionand codegenerationprocess,meetingtheneedforunderstanding
analysis,decisiontopublish,orpreparationofthemanuscript. andcontrolledproductionofcodeinMLapplications.
∗Correspondingauthor
Ourmaincontributionscanbesummarizedasfollows:
etrofimova@hse.ru(E.Trofimova)
ORCID(s):0000-0001-5436-8511(E.Trofimova);0009-0000-2891-0792
AControllableTransformationFramework:Wepresent
(E.Sataev);0000-0001-7865-2357(A.E.Ustyuzhanin) a framework for the controlled transformation of ML task
E Trofimova et al.: PreprintsubmittedtoElsevier Page1of16Linguacodus: transformative ML code generation
descriptionsintosolutioninstructions,involvingfine-tuning is a benchmarking platform for evaluating text generation
theLlama-2modelusingpairsofMLtaskdescriptionsand models,includingthosedesignedforcodegeneration.This
instructionsretrievedwithGPT-3.5. platformfacilitatestheassessmentofcontrollablecodegen-
Instruction-Based Sequential Generation: We demon- eration models by offering standardized evaluation metrics
strate the efficacy of executing instructions for sequential andtasks.
generation, producing compilable code with promising re- In automating problem-solving tasks, researchers have
sultsbasedonevaluationmetrics. actively explored solutions such as AutoGluonTabular (Er-
Therestofthepaperisorganisedasfollowing.Section2 ickson, Mueller, Shirkov, Zhang, Larroy, Li and Smola,
explorestheapplicationofLargeLanguageModels(LLMs) 2020)andH2OAutoML(LeDellandPoirier,2020).Inpar-
in code generation, text-to-code conversion, controllable ticular, LightAutoML (Vakhrushev, Ryzhkov, Savchenko,
generation,andautomatingproblem-solvingtasks,shedding Simakov, Damdinov and Tuzhilin, 2021) tailors itself to
light on the limitations of LLMs in ML code synthesis. the distinctive needs of large financial services companies,
Section3providesanoverviewoftheLinguacodusframe- providingsolutionsforhandlinglargedatasetswithdiverse
work,whileSection4describestheexperiments,validation types, non-stationary data, and specific validations. Hug-
of the approach. Section 5 showcases the effectiveness of gingGPT (Shen, Song, Tan, Li, Lu and Zhuang, 2024),
Linguacodus in transforming plain English descriptions of another recent AutoML framework, utilizes ChatGPT for
MLtasksintoexecutablecode.Section6criticallyexamines taskplanning,modelselection,subtaskexecution,andresult
thelimitationsanddiscussourapproach.Finally,Section7 summarization,demonstratingversatilityacrossAItasks.
summarizesandconcludesthepaper. Nair, Schumacher, Tso and Kannan (2023) present the
dialog-enabled resolving agents (DERA), aiming for accu-
rate output generation. DERA enhance the conversational
2. RelatedWork
abilitiesofLLMsbyincorporatingtwodistinctagenttypes:a
Automated code generation has undergone remarkable Researcher,responsibleforprocessinginformationandiden-
progress, primarily attributed to the emergence of LLMs. tifyingcriticalproblemcomponents,andaDecider,capable
CodeBERT (Feng, Guo, Tang, Duan, Feng, Gong, Shou, of autonomously integrating the Researcher’s information
Qin, Liu, Jiang et al., 2020) is a pre-trained model that and making judgments on the final output. Although the
can generate code and handle natural language tasks. Such DERA paradigm was initially used in healthcare, one can
modelsofferversatilityincode-relatedtasks,includingcode notice the potential applicability of multi-agent GPT LLM
generation,summarization,andrecommendation. invarioustrainingfields.
CoditT5 (Zhang, Panthaplackel, Nie, Li and Gligoric, While automated machine learning offers structured
2022) is another language model that generates edit-based workflow optimization, ML code generation based on nat-
output sequences from corrupted input sequences. Models ural language descriptions provides seamless integration
likeCoditT5enhancecodegenerationcapabilities,aligning intoexistingsystemsandcustomizationfordomain-specific
themmorecloselywithuserrequirements. tasks.
Moderncodegenerationapproachesoftenrelyongeneral- The advancements in code generation driven by LLMs
purposetransformers,exemplifiedbyGPT-3.Codex(Chen, havewitnessednotableprogress.Thus,OpenAIGPTmodels
Tworek,Jun,Yuan,Pinto,Kaplan,Edwards,Burda,Joseph, (Achiam et al., 2023) (Bubeck et al., 2023), although not
Brockman et al., 2021), a notable model in this cate- explicitly designed for code generation, have demonstrated
gory, showcases the potential to generate code snippets proficiency in generating code snippets and understanding
directly from natural language prompts. AlphaCode (Li, programming-related prompts. The generative capabilities
Choi, Chung, Kushman, Schrittwieser, Leblond, Eccles, of GPT models make them versatile tools for interpreting
Keeling,Gimeno,DalLagoetal.,2022)extendsthisfoun- andtranslatingnaturallanguagedescriptionsintoexecutable
dation, emphasizing the significance of code diversity and code.
improvingcontextualunderstandinginLLMs. Google’s PaLM 2 (Anil, Dai, Firat, Johnson, Lepikhin,
In parallel, text-to-code conversion has gained promi- Passos, Shakeri, Taropa, Bailey, Chen et al., 2023) under-
nence. PaLM-Coder presents a method for converting nat- goespre-trainingonavastdatasetencompassingwebpages
urallanguagedescriptionsintocode,focusingonJavacode and source code, making it valuable for code debugging,
generation.OpenAImodels(Achiametal.,2023),(Bubeck completion, and generation across multiple programming
etal.,2023)havefurtherextendedthecapabilitiesofLLMs languages.Themodel’sdualfocusonsemanticparsingand
inunderstandingandgeneratingcodefromtextualprompts. languagemodelpre-trainingenhancesitsabilitytocompre-
Controllable code generation is an emerging subfield hend and generate code based on diverse natural language
with significant potential. CTRL (Keskar, McCann, Varsh- inputs.
ney, Xiong and Socher, 2019) is a conditional language OneoftheleadingpubliclyavailableLLMforcodegen-
models for controlled code generation. The model focuses erationisCodeLlama(Rozière,Gehring,Gloeckle,Sootla,
on allowing users to specify conditions that influence the Gat,Tan,Adi,Liu,Sauvestre,Remez,Rapin,Kozhevnikov,
generatedcode,providingalevelofcontrolovertheoutput. Evtimov, Bitton, Bhatt, Ferrer, Grattafiori, Xiong, Défos-
Texygen(Zhu,Lu,Zheng,Guo,Zhang,WangandYu,2018) sez, Copet, Azhar, Touvron, Martin, Usunier, Scialom and
E Trofimova et al.: PreprintsubmittedtoElsevier Page 2 of 16Linguacodus: transformative ML code generation
Synnaeve,2024).AnextensionofLlama-2(Touvron,Mar- a code generation problem compared to direct generation
tin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, of code by using task description as a prompt. However,
Bhargava, Bhosale et al., 2023), Code Llama comes in Code4MLlacksannotationforallcodesnippets.Thislimi-
two variations: a code producer and its instruction-specific tationisaddressedthroughthetaxonomy-basedcategoriza-
refinement, Code Llama - Instruct. Code Llama - Instruct tionintroducedby(Berezovskiy,Gorodilova,Trofimovaand
surpassesCodeLlamainprovidingmorehelpfulandsecure Ustyuzhanin,2023).
responsesinnaturallanguage,ensuringamoredependable
performance. However, the generated instructions are gen-
data type conversions correct missing values
erally broad-purpose and lack easy assessability regarding randomize order remove duplicates
concatenate
theirsuitabilityforspecifictasks. drop columns split normalization create dataframe
WhileOpenAI’sChatGPTandsimilarLLMshavedemon- compute train metric train model rename columns sort values filter augment reshapp erepare x and y
strated remarkable capabilities in various natural language load pretrainedpredict on train choose model class feature en cg ain tee ge ori rn ifg y TraD na st fa orm groum pbe yrge string ctr oa un ns tf o ur nm ique values count data types
u innd the ers cta on nd tei xn tg ot fas Mks L,t ch oe dy ed go enh eav rae tis oo nm :einherentlimitations init hyperparams save m bo ud ie ldl layers Model Train Ex Ap nDl ao a lr yta a sto isry count missing values count duplicates
1. L pea tc sk tho af tS lp ae cc kifi sc pi et cy i: fiL cL ityM fs oo rf ste pn ecg ifien ce Mrat Le c tao sd ke s.sn Ti hp e- cc ro opm sr se sp t d avu i tact ie st li t do it ce an as t et lt e tm est se ttric print shapE lev yM a cl oo u ed a fe ft sil on InteM rpo red te al tion Cod Te y pB elock ExD traa cta tion load frolo ma d u rfr lom z pi l rp o ea pd
a
rf ero dmlo
a
a td ad i s lof k lr o ao a dm d
e
rc frs ov m sql
features selection
generated code may be overly general and not finely define search model Hyp Te ur np ina gram Visualization Debug list files Data exportEnvironment set options install modules
tailored to the requirements of complex machine define search space missing values show table show shape show data types save to csv import modules
2. Lle ia mrn iti en dg Cw oo nrk trfl oo lw Os v. er Code Generation: Users have find bestt r mai on d o en l cg lr aid s fs ind bfin ed st b se cs ot r eparam les arning history d tii mst eri b su et ri io en s im ha eg ae tms ar pelati po ln os t h mip etricscommented show cosh luo mw n u snique values prepare output
plot predictions model coefficients wandb show table attributes
limitedcontroloverthefine-tuningprocessofLLMs,
making it challenging to enforce specific guidelines
Figure 1: Code4ML taxonomy tree. Reproduced from (Droz-
orconstraintsduringthegenerationofMLcode.This
dova et al., 2023), with permission of the authors.
lackofcontrolmayresultinvariationsincodequality
andsuitabilityfordifferenttasks.
3. Handling Ambiguity: Natural language descriptions
ofMLtaskscanbeinherentlyambiguous.LLMsmay
3. Approach
struggle to disambiguate between multiple potential
interpretations,leadingtocodesnippetsthatmaynot Our research relies on the Code4ML dataset, focusing
accuratelycapturetheintendedmeaningofthetask. onKagglecompetitionsencompassingallmetriccategories
4. Inability to Learn Task-Specific Patterns: while pro- except’points,’’significance,’and’customloss.’Wecurate
ficient in learning patterns from diverse data, LLMs the top 75 solutions for retrieving high-level instructions
mayfacechallengesincapturingtask-specificpatterns from these competitions. It is essential to highlight that
relevanttoMLcodegeneration.Thislimitationcanre- specificcontestsmayhavefewerthan75solutionsavailable
sultingeneratedcodethatlacksthespecifityrequired forselection.
forspecializedtasks. Asaresult,ourdatasetcomprises395naturallanguage
ML task descriptions paired with 7023 corresponding in-
5. Evaluation Metrics and Validation: The evaluation
structions,obtainedthroughtheassistanceofGPT-3.5,ex-
metrics for assessing the quality of generated code
tractedfromKagglesolutions.Figure2overviewsthepreva-
may not always align with the specific requirements
lentmodelsfeaturedintheselectedsolutions.Figure3illus-
ofMLtasks.LLMsmayprioritizegeneratingsyntac-
tratesthediversityofdatatypesusedinthechosenKaggle
tically correct code without necessarily ensuring the
competitions. This work emphasizes ML tasks involving
semanticcorrectnessoroptimizationofthegenerated
tabular data. However, we do not restrict competitions to
solutions.
numeric tabular formats and consider those involving time
Addressingthesechallengesrequiresahybridapproach seriesortextdatapresentedwithtables.
involvingspecializedMLcodedatasetsanddimensionalre- BuildingupontheinsightsfromtheoriginalCode4ML
ductionwithinthelearningspaceforLLMfine-tuning.The framework(Drozdovaetal.,2023)andthetaxonomy-based
Code4ML(Drozdova,Trofimova,Guseva,Scherbakovaand categorizationintroducedby(Berezovskiyetal.,2023),our
Ustyuzhanin, 2023) is a comprehensive corpus comprising approach addresses the dimensionality reduction challenge
of a) Kaggle challenge descriptions in natural language, b) insynthesizingmachinelearning(ML)taskdescriptionsinto
Jupyternotebooksandtheirscores,c)Pythoncodesnippets, code.
andd)competition-relatedmetadata.Thismetadataincludes Diverging from the original framework, which primar-
formal descriptions of challenge datasets and scoring met- ily classifies individual code snippets, our emphasis lies
rics. Code4ML relies on a knowledge taxonomy tree (Fig- in capturing high-level information that embodies the core
ure1)tocategorizevariousJupyternotebookcodesnippets. elementsofgeneralizedMLsolutions.
Adescriptionofachallengesolutionintermsoftheclasses
ofthistaxonomysignificantlyreducesthedimensionalityof
E Trofimova et al.: PreprintsubmittedtoElsevier Page 3 of 16Linguacodus: transformative ML code generation
task description, metric details, and data type information.
da mta e_ tt ry icpe/ AUC cat ae cg co ur ri az ca ytion per ec re ron rt sage MSE mF u- ls tc ico lr ae s s Multi lc ol sa sss log Log loss MAE The prompt-completion pair used in this stage is visually
Audio lightgbm - - - resnet-34 - - - depicted in Figure 6, with the separation marked by the
[/INST] token. This comprehensive approach enhances the
Image densenet-161 efficientnet - Sequential efficientnetb7 vgg16 rel go rg ei ss sti ic o n lightgbm
fine-tuning process, incorporating the quality ranking of
Tabular lightgbm Ran cld ao sm sifF io errest lightgbm lightgbm lightgbm xgboost lightgbm lightgbm instructions and pertinent task details for optimal model
Text Ran cld ao sm sifF io errest rel go rg ei ss sti ic o n AUC rB egag reg si sn og r bert rel go rg ei ss sti ic o n xgboost - adaptation. Llama models have been pre-trained on vast
amountsofdata.Byfine-tuning,weleveragethisextensive
Video - - - - - - myresnext -
knowledge and adapt it to specific tasks, often achieving
Time series - Ran cld ao sm sifF io errest Ra Rnd ego rm esF so or rest lightgbm - bert rel go rg ei ss sti ic o n - state-of-the-art results with less data and time. The fine-
tuningdetailsaresummarisedinAppendixA.
Figure 2: The most popular model choice among retrieved
3.LlamaInference:Next,weinferLlama-2toselectthe
Kaggle solutions based on metric and data type.
top-3 most valuable instructions by specifying their rank
usingadedicatedprompt,asshowninFigure7.
time series 4. Iterative enhancing LLM responses through multi-
5 %
audio agent GPT: The inferred instructions then undergo further
2 % refinement with the assistance of multi-agent GPT. The
text data
primary goal of multi-agent GPT is to identify any logical
12 %
errorsintheprovidedinstructionsandsubsequentlychoose
thebestoptionfromthethreevariants,therebyenhancingthe
overallqualityoftheinstructions.Thisintelligentprocessing
tabular data iselucidatedinFigure8.
52 %
3.2. InstructionstoCodeGeneration
image data
29 % Thesecondphaseofourapproachcentersontheactual
code generation, building upon the instructions obtained
in the previous step. In this phase, we harness the capa-
bilities of language models to transform these instructions
intofunctionalandwell-structuredcodethatalignswiththe
Figure 3: The competitions distribution based on the data underlyingMLtasks.
type. Figure 9 visually represents the sequential pipeline in-
volved in the instruction-to-code transformation. We have
separated the code synthesis into the stages for Data Pre-
3.1. GeneratingInstructions. processing, Model Architecture, and Model Training. Ad-
To extract the high-level code instructions, we’ve de- ditionally, we have also introduced a submission block to
visedafour-stageframework,depictedinFigure4: enablethetestingofresultsontheKaggleplatform.Thenext
1. High-Level Solution Representation: We begin by step in this pipeline involves integrating all the generated
creatinghigh-levelrepresentationsofMLsolutions.Lever- code segments. Empirically, GPT-3.5 generates code with
aging the capabilities of GPT-3.5, we extract critical in- occasionalerrors.Toaddressthis,weemployanerror-fixing
formationregardingdatapreprocessing,modelarchitecture, procedure,runningituptothreetimes.Inthisprocess,the
and the training procedure from existing code solutions. same GPT agent, responsible for integrating all code com-
The selection of GPT-3.5 is driven by the consideration ponentsiteratively,inputstheerrorswithoutanyadditional
of balancing quality and inference time using the OpenAI specifications.
API.Figure5illustratesthepreciseinputpromptpresented This phase forms the critical bridge between the high-
to the model. Consequently, GPT-3.5 generates the high- levelMLinstructionsandtheexecutablecode,ensuringthat
level ML instructions. To refine the quality of our dataset, thegeneratedcodeadherestotheprovidedinstructionsand
theextractedsolutionsundergoarankingprocessbasedon producespracticalsolutionsfortheintendedMLtasks.
theirscores.Eachsolutionisintricatelylinkedtothenatural Our approach collectively forms the foundation of our
languagedescriptionoftheMLtask. approach, enabling the seamless transition from unstruc-
2. Llama Fine-Tuning: Then, we utilize the acquired tured ML task descriptions to precise, high-level solution
instructionsasinputsforfine-tuningtheopen-sourceLlama- instructionsand,ultimately,toexecutablecode.
2 7b model. To ensure the relevance of the instructions to
the machine learning (ML) task, we leverage the original
4. Experiments
code’squalityevaluationintheformofascore.Theretrieved
instructions are ranked based on their significance to the Tounderscorethesignificanceoftheresearch,wecom-
ML task. Furthermore, we furnish the Llama model with pare the instructions generated by the fine-tuned Llama-2
essential information presented as prompts, including the model and those inferred from Code Llama - Instruct. Our
E Trofimova et al.: PreprintsubmittedtoElsevier Page 4 of 16Linguacodus: transformative ML code generation
High o-l fe Mve Ll r se op lr ue ts ioen nt sation Llama fine-tuning inL fela rm ena
c e
Multi—agent GPT
Data
type
Prompt Metric ML tasks #1 Prompt Prompt
type descriptions
#2 Best
Ranked Prompt Response #3 instruction
Ranked solutions Instructions
Faulty Find best,
Llama logic improve
Figure 4: Linguacodus ML task description to instruction transformation framework. The framework comprises generating
instructions most suitable for a given ML task and automatically selecting the best instruction within the multi-agent GPT
paradigm.
Prompt for fine-tuned Llama-2
Prompt for GPT-3
Imagine that you are a data analyst. Your objective is writing the
"Get the main information about data preprocessing, model architecture
**3rd** place instruction for solving this machine learning task. Task:
and model training from the code. Code: **Kaggle code**.
**Task description**. The **Data type** data is used for the problem.
The metric type is **Metric type** for the problem. Your response
contains the main information about data preprocessing, model
Figure 5: GPT-3.5 prompt for ML instructions retrieving. architecture and model training.
Prompt for GPT-3 Figure 7: Prompt for Llama inference.
<s> token [INST] Get the main information about data preprocessing,
model architecture and model training for this problem. This solution
has the 1 place in rating. Data type: **Data type**. Metric type is: Prompt 1 for GPT-4
**Metric type**. Problem: **Task description**. [/INST] **GPT-
resulted instruction** </s> You are a researcher tasked with investigating the 3 options of
instruction for solving this machine learning task. Task: **Task
description**. The **Data type** data is used for the problem. The
metric type is **Metric type** for the problem.
Figure6:Llamafine-tuneforMLinstructiongenerationinput.
Your response contains the main information about data preprocessing,
model architecture and model training. List the flaws and faulty logic of
each instruction option. Let’s work this out in a step by step way to be
sure we have all the errors.
evaluationextendsbeyondtheKaggleplatform,encompass-
Instruction option 1: **Instruction 1**
ing ML competitions hosted on Codalab (Pavao, Guyon, Instruction option 2: **Instruction 2**
Letournel,Tran,Baro,Escalante,Escalera,ThomasandXu, Instruction option 3: **Instruction 3**
2023) to ensure a thorough analysis. All the data used for
Prompt 2 for GPT-4
validation and testing is not included to the training set.
You are resolver tasked with 1) find which of the instruction options the
We manually select the best instruction from the top three
researcher thought was best 2) improving the instruction 3) printing the
inferred by Llama-2. Additionally, we include examples of improved instruction in full. Let’s work this out in a step by step way to
be sure we have the right meaningful instruction.
instructions automatically improved with the multi-agent
GPT technique through the proposition of more advanced
modelsfortraining. Figure 8: Multi-agent GPT for best instruction choice and
Instructions produced by Code Llama - Instruct gener- improvement.
ally focus on the high-level approach and conceptual steps
involved in training a model. They emphasize data prepro-
cessing,modelarchitecture,andtraininggoalswithoutdelv- are crucial for code generation. Our pipeline, enhanced by
ingintospecificimplementationdetails.Incontrast,thefine- multi-agent GPT, can synthesize code via instructions of
tuned Llama-2 instructions provide detailed, step-by-step predefinedquality,makingourapproachuniqueandpromis-
breakdowns of the data preprocessing, model architecture, ingforassistanceinMLcodegeneration.AppendixCpro-
and model training processes. While the former offers a videssamplecodegeneratedbyvanillaGPT-3.5withauto-
broaderoverviewsuitableforunderstandingtheoverallflow maticallyimprovedinstructionsandplaintaskdescriptions.
of the task, the latter caters to individuals seeking a more Raw GPT-3.5 often includes code that cannot be compiled
granular understanding, providing a comprehensive guide asitneedsmorespecificmodelstotrain,whileLinguacodus
with specific library references and functions used at each producestheready-to-runcode.
stage(seeAppendixB).
Generatingacompleteandfunctionalcodesolutionus-
5. Results
ingaLargeLanguageModel(LLM)involvesprovidingthe
model with a detailed prompt or context outlining the task Toassesstheeffectivenessofourapproach,weemploy
orproblemstatement.Thus,thebest-suitedtaskinstructions Kaggle competitions that are recent and popular, featuring
more than 500 participating teams, ensuring that the tasks
E Trofimova et al.: PreprintsubmittedtoElsevier Page 5 of 16Linguacodus: transformative ML code generation
Y p a g insr eo so s nu tig e rsr ur rt a aa a cm nr t tee it o m . c a nL o e :P e dry t e'st h f oo rn t his U c P c A ino ors r sod d ci tn ce e rh ue g if fs t c o o et s th r r c ii o ne tt M u nh g g r :e , o e e l d Dn e fee t oa ' lr s r ta a a tt he d id d s U c P c A ino ors r sod d ci tn ce e rh ue g if fs t c o o et s th r r c ii o ne tt M u nh g g r :e , o e e l d Dn e fee t oa ' lr s r ta a a tt he d id d s
Using the
ML task ML task ML task generated Data
description description description P Mro oc de es l sing, L the et ' gs ec no em rab ti en de Error
Data Preprocessing: Model Architecture: Model Training: A Mr oc dh eit le tc rt au inre in, gan , d c Po rod ce e f so sr i nD ga , ta
lets generate Model
submission. Architecture,
instB rue cs tt i on pr de ep srD co ra c ipt ea ts i s oi nng a dr ecM sh ci ro t ied pce tt il u o r ne M do ed se cl r t ir pa ti in oi nng iS nu fb om rmis as ti io on n M a suno bdd m e il s str ia oi nn .ing
Data Information: Data Information: Data Information:
Data Data Data
information information information
Figure 9: Linguacodus instruction to code sequential transformation scheme. The programming code for a given ML task is
iteratively generated with GPT-3.5. The resulting code undergoes an error-fixing procedure within GPT-3.5 up to three times.
wereunseenbyourmodel.Toapproximatethedistribution natural language task descriptions may be limited. Specif-
ofthetrainingcompetitionspace,werandomlyselecteight ically, CodeBERT and CoditT5 are primarily trained for
machinelearningtasks,withamajorityoperatingonnumer- synthesizing code snippets rather than entire pipelines or
icaldataandoneeachfortext,timeseries,andimagedata. comprehensive solutions. Therefore, GPT-3.5 serves as a
TheKagglescoresforthegeneratedcodeacrossthesetasks more relevant and established comparison point in trans-
aredetailedinTable1.Thealignmentwithevaluationmet- forming natural language into complete machine learning
ricsisillustratedbyshowcasingthepositionsoftheprovided pipelines. Additionally, GPT-3.5 demonstrates greater effi-
solutionsamongthepublicscoresforeachcompetition. ciencyincomparisontoLlama-2HuggingFaceanddoesnot
Table1includesacomparativeanalysiswithresultspro- requirepayment,asGPT-4.Section5hasdemonstratedthe
ducedbyvanillaGPT-3.5.TheLinguacodusframeworkcon- superiorityoftheLinguacodusframeworkovervanillaGPT-
sistently produces compilable code, outperforming vanilla 3.5inthedomainofMLcodegenerationfromplainEnglish
GPT-3.5 solutions across specified machine learning met- descriptionsofMLtasks.
rics.BothLinguacodusandvanillaGPT-3.5receivenatural Linguacodus addresses challenges, highlighted in sec-
languagedescriptionsandnecessarymetadataforeachma- tion2:
chine learning task as input. To ensure a fair and unbiased
1. Lack of Specificity: Linguacodus aims to provide
comparison,thecodegeneratedbybothapproachesunder-
morespecificandtailoredinstructionsforMLtasksby
goesuptothreeiterationsoferrortreatment.
focusing on high-level information extraction rather
It’sworthnotingthatKaggle,asacompetitiveplatform,
thandetailedcodesnippetclassification.
traditionally demands a significant investment of time and
2. Limited Control Over Code Generation: Ranked in-
expertisefromitsparticipants.EngaginginKagglecompe-
structions allow for a controlled transformation pro-
titionsoftenrequiresadeepunderstandingofthefieldanda
cess,providingastructuredframeworkforcodegen-
substantialtimecommitment.
eration.Userscanchoosefromthetop-rankedinstruc-
In contrast, our pipeline for transforming ML task de-
tions,offeringcontroloverthegeneratedcode.
scriptions into code offers a significantly more efficient
3. Handling Ambiguity: The ranking process, coupled
alternative.Thisapproachminimizesthetime(ittakesless
withthefine-tuningofLLMs,enhancestheprecision
than1minutetogenerateasolution)andexpertiserequired
of instructions by prioritizing those that align most
tobridgethegapbetweentaskdescriptionsandexecutable
closelywiththetaskdescriptions,mitigatingpotential
code,makingmachinelearningdevelopmentmoreaccessi-
ambiguities.
bleandefficient.
4. InabilitytoLearnTask-SpecificPatterns:Leveraging
the fine-tuning process with Llama-2 7b on task-
6. Discussion
specific details allows the model to adapt and learn
Itiscrucialtonotethatourcomparisonprimarilyfocuses
patterns specific to ML tasks, enhancing the quality
onGPT-3.5asabenchmark,consideringitsprominenceas
andrelevanceofthegeneratedinstructions.
a leading tool in natural language generation tasks. While
other models exist, such as CodeBERT, CoditT5, PalM-
Coder,andCTRL,theirsuitabilityforgeneratingcodefrom
E Trofimova et al.: PreprintsubmittedtoElsevier Page 6 of 16Linguacodus: transformative ML code generation
Table 1
The sample results of generated ML code validated on the Kaggle platform.
Data Teams Linguacodus Linguacodus GPT-3.5 GPT-3.5
Competition name Metric
type number score percentile score percentile
"Feature Imputation with
a Heat Flux Dataset" tabular 694 rmse 0.059 0 × ×
(Reade, and Chow, 2023c)
"Binary Classification of
Machine Failures" tabular 1504 roc-auc 0.948 66 0.943 68
(Reade, and Chow, 2023a)
Predict CO2 Emissions in time
1442 rmse 15.409 0 24.136 21
Rwanda (D. Moruri and Chow, 2023) series
CommonLit - Evaluate Student
text 1770 mcrmse 0.470 58 0.548 81
Summaries (Franklin, 2023)
Binary Classification with a
Tabular Credit Card Fraud tabular 643 roc-auc 0.773 77 0.752 80
Dataset (Reade, and Chow, 2023b)
mean
Stable Diffusion - Image to
image 1231 cosine 0.714 0 × ×
Prompts (W. Reade, 2023c)
similarity
Regression with a Tabular California
tabular 690 rmse 0.578 69 0.600 80
Housing Dataset (W. Reade, 2023b)
Regression with a Crab
tabular 1431 mae 1.387 52 1.978 96
Age Dataset (W. Reade, 2023a)
5. Evaluation Metrics and Validation: The ranking pro- competition-related metadata, data types, and scoring met-
cess,involvingevaluationscoresandtask-specificde- rics.
tails,isarobustvalidationmechanismforthegener- Inspiredbytheknowledgetaxonomytreeintroducedin
ated instructions, ensuring their alignment with ML Drozdova et al. (2023), we adopt a similar organizational
tasks and promoting solutions that adhere to evalua- framework to achieve dimensional reduction in our ML
tionmetrics. task description-to-code synthesis approach. However, our
approachdiffersinthatitfocusesonhigh-levelinformation
However, we also analyse the limitations of the frame-
extractionratherthanindividualcodesnippetclassification.
work. Multi-agent GPT occasionally exhibits suboptimal
Thisstrategicshiftsimplifiesandstreamlinesthecodegen-
performancecomparedtounprocessedLinguacodusinstruc-
erationprocess,makingitmoreefficientandadaptable.
tions,emphasizingtheroleofcontextintasks’complexity.
Linguacodusisstructuredintotwophases:synthesizing
Ethicalconsiderationssurroundingbiasesandpotentialmis-
high-levelMLsolutioninstructionsandtransformingthese
useofgeneratedcodehighlighttheneedforresponsiblede-
instructions into functional code. GPT-3.5 is employed to
ployment.Linguacodusfaceschallengeswhentasksdeviate
extractessentialinformationfromtheprovidedcode,which
significantly from those fine-tuned on Llama-2, suggesting
is the basis for generating high-level instructions. These
a need for dataset enrichment. Insufficiently detailed in-
instructionsarethenfine-tunedusingLlama.Thetopthree
structionsarisewhentaskslackcomprehensivedescriptions,
instructions are selected and further refined with the as-
callingformoreexplicittaskinformation.
sistance of multi-agent GPT, ensuring the highest quality
Recognizingthatmulti-agentGPTmaynotconsistently
instructionsforsubsequentcodegeneration.
outperform initially inferred instructions; human interven-
The second phase involves translating these refined in-
tion is proposed to select the best instruction, highlighting
structions into well-structured and executable code seg-
theneedforabalancedapproachthatcombinesthestrengths
ments, encompassing data preprocessing, model architec-
ofautomatedmodelswithhumanjudgmentinrefiningout-
ture,modeltraining,andsubmissionblockgeneration.This
puts.
transformation bridges the gap between high-level ML in-
structions and practical code, ensuring alignment with the
7. Conclusion
underlyingMLtasks.
Inconclusion,ourresearchintroducesacomprehensive Ourapproach’seffectivenessisvalidatedthroughexper-
approach to transforming unstructured ML task descrip- imentsonKagglecompetitionsthatarenotpartofourtrain-
tionsintoexecutablecode,presentingthenovelLinguacodus ing data. The results demonstrate that the generated code
model. Leveraging the Code4ML dataset, which encom- is compilable and aligns well with the evaluation metrics.
passes a rich collection of Python code snippets, contest Wealsocomparetheperformanceofmulti-agentGPTand
summaries, and data descriptions from Kaggle competi- unprocessed Code Llama - Instructions, highlighting the
tions,ourmethodologycapitalizesonthedataset’svaluable
E Trofimova et al.: PreprintsubmittedtoElsevier Page 7 of 16Linguacodus: transformative ML code generation
needforfurtherrefinementinmulti-agentGPT’salgorithmic
Table 2
approachtoachievesuperiorsolutionqualityconsistently. Llama-2 fine-tuning hyper-parameters.
In summary, our research provides an innovative and
LoRA Parameters
efficientsolutionforcodegenerationfromMLtaskdescrip- LoRA attention dimension 64
tions,showcasingthecapabilitiesofLinguacodus.Bycapi- Alpha parameter for LoRA scaling 16
talizingontheCode4MLdataset’swealthofresourcesand Dropout probability for LoRA layers 0.1
introducing a structured approach to instruction synthesis 4-Bit Precision Parameters
and code generation, we bridge the gap between natural 4-bit precision base model loading True
language task descriptions and executable code, making Compute dtype for 4-bit base models float16
machinelearningdevelopmentmoreaccessibleandefficient. Quantization type nf4
Nested quantization for 4-bit base models False
CRediTauthorshipcontributionstatement
Ekaterina Trofimova: Conceptualization, Investiga- TrainingArguments Parameters
Number of training epochs 1
tion, Methodology, Software, Validation, Formal analysis,
Enable fp16/bf16 training False/False
Writing - Original Draft, Writing - Review & Editing,
Batch size per GPU for training 4
Visualization,Supervision,Projectadministration.EmilSa-
Batch size per GPU for evaluation 4
taev:Investigation,Software,Methodology,DataCuration,
Enable gradient checkpointing True
Formal analysis. Andrey E. Ustyuzhanin: Conceptualiza- Maximum gradient normal 0.3
tion, Supervision, Formal analysis, Funding acquisition, Initial learning rate 2e-4
Methodology,Writing–Review&Editing. Weight decay 0.001
Declarationofcompetinginterest Optimizer AdamW
Theauthorsdeclarethattheyhavenoknowncompeting Learning rate schedule constant
finan-cialinterestsorpersonalrelationshipsthatcouldhave Number of training steps -1
Ratio of steps for a linear warmup 0.03
appearedtoinfluencetheworkreportedinthispaper.
Group sequences into same length batches True
Dataavailability
Save checkpoint every X updates steps 500
Thecodelink:https://anonymous.4open.science/r/NL4ML-
Log every X updates steps 25
3AE4.
Sequence Fine-Tuning Parameters
Acknowledgments
Maximum sequence length None
We would like to express our appreciation to Denis
DerkachandArtemMaevskiyfortheinvaluablecomments
andsupport. Tables 5, 6, 7 showcase the retrieved instructions for
Thereportedstudyutilizedthesupercomputerresources CodalabcompetitionC1.Thefirstinstructionadoptsahigh-
oftheNationalResearchUniversityHigherSchoolofEco- level approach, focusing on the overarching strategy and
nomics. The research leading to these results has received conceptual steps involved in training a model to identify
funding from the Russian Science Foundation under grant "hallucinations"inneurallanguagemodeloutputs.Itsignif-
agreementn◦19-71-30020.
icantly emphasizes data preprocessing, model architecture,
and training objectives while avoiding intricate implemen-
A. Llamafine-tuningdetails tationdetails.
Incontrast,thesubsequentinstructionprovidesametic-
In this section, the hyperparameters for Llama-2 fine-
ulous, step-by-step breakdown of the data preprocessing,
tuningarereflectedinTable2.
modelarchitecture,andmodeltrainingprocedures.Itintri-
cately outlines actions such as data loading using pandas,
B. SampleinstructionsinferredbyCode tokenizationwiththespecificTokenizerclass,fine-tuninga
BERT-basedmodelthroughtheTensorFlowBERTlibrary,
Llama-Instructandfine-tunedLlama-2
and tracking training progress using the TensorFlow Ten-
Thissectionpresentsacomparativeanalysisofexample sorBoardAPI.Thislevelofdetailmakesitmoresuitablefor
instructions inferred by Code Llama - Instruct, manually directimplementation.
selected best instructions inferred by fine-tuned Llama-2, Aninstructionautomaticallyimprovedwithmulti-agent
and instructions automatically improved with multi-agent GPT goes beyond by suggesting a more advanced model
GPT,allpertainingtodifferentmachinelearningtasks. for solving an ML task. Additionally, it specifies the opti-
We have chosen two CodaLab competitions (C1, C2) mization algorithm and training metric, providing a more
and three Kaggle competitions (K1, K2, K3) to conduct refinedandadvancedsetofinstructionsforusers.Thus,the
this comparison. The respective competition names and potential of multi-agent GPT in enhancing the quality and
task descriptions used for prompting are listed in Table 3. sophisticationofinstructionsformachinelearningtasks.
Additionally,Table4providesasummaryoftheinformation
regardingthedataandmetrictypeassociatedwitheachML
task.Notably,ourcomparisonencompassesadiversesetof
tasks.
E Trofimova et al.: PreprintsubmittedtoElsevier Page 8 of 16Linguacodus: transformative ML code generation
Table 3 Table 4
Competitions information. Competitions meta-information.
Id Competition name Task description Id Data Type Metric Type
C1 "SHROOM - a Shared- SHROOM challenges C1 text accuracy
task on Hallucinations participants to identify outputs C2 text f1 score
andRelatedObservable that are grammatically correct K1 images mean cosine similarity
Overgeneration but semantically inaccurate, in K2 images categorization accuracy
Mistake" (Pavao a binary classification task. The K3 tabular rmse
et al., 2023) competitionfocusesondetecting
"hallucinations" in outputs from
As a data analyst, I would start by preprocessing the text
neural language models across
data to ensure that it is in a format that can be easily
definition modeling, machine
analyzed and processed. This may involve removing stop
translation, and paraphrase
words, stemming or lemmatizing the text, and converting
generation, with or without
thetexttoanumericalrepresentationusingtechniquessuch
access to the model that
as bag-of-words or word embeddings.
generated these outputs
Once the data is preprocessed, I would then build a binary
C2 "Climate Activism "The CASE 2024 competition
classification model that can accurately detect grammat-
Stance and Hate Event at EACL 2024 focuses on three
ically correct but semantically inaccurate outputs. This
Detection Shared subtasks in the context of cli-
model architecture could include a combination of tech-
Task" (Pavao et al., mate activism: identifying hate
niquessuchaswordembeddings,recurrentneuralnetworks
2023) speech,detectingtargetsofhate
(RNNs), and convolutional neural networks (CNNs) to
speech (individuals, organiza-
capturethenuancesoflanguageanddetectpatternsinthe
tions, communities), and deter-
data.
mining the stance (support, op-
To train the model, I would use a dataset of labeled
pose, neutral) of given texts."
examples, where each example is annotated with a binary
K1 "Stable Diffusion The competition aims to de-
labelindicatingwhethertheoutputisgrammaticallycorrect
- Image to velop a model that can pre-
but semantically inaccurate or not. I would then use this
Prompts" (W. Reade, dict the text prompt that corre-
datasettotrainthemodelusingasuitablemachinelearning
2023c) sponds to a given generated im-
algorithm, such as logistic regression or support vector
age,challengingtheunderstand-
machines (SVMs), and evaluate its performance using
ing of the relationship between
metrics such as accuracy, precision, recall, and F1 score.
textpromptsandimagesintext-
Overall,thegoalofthistaskistodevelopamodelthatcan
to-image models. The goal is to
accurately detect "hallucinations" in outputs from neural
create embeddings for predicted
language models, and to evaluate its performance using
prompts,ensuringrobustprompt
appropriate metrics. By preprocessing the data and using
similarity assessment, and ex-
asuitablemodelarchitecture,IbelievethatIcandevelopa
ploringpromptengineeringinthe
model that can accurately detect these hallucinations and
context of image generation.
improve the quality of the outputs generated by the neural
K2 "Hat or no hat – The problem is simple. Bring
language models.
that is the ques- your Deep Learning knowledge
tion"(Ashis@CUDenver,to detect if a given image con- Table 5
2023) tains a person with hat or not. AsampleinstructiongeneratedforCB1competitionbyCode
You are not given any training Llama - Instruct.
dataset.But,youneedtobuilda
deep learning model and predict
all the test images for the hat- seamlesslywiththepromptspecificationsandachievingthe
ness test, i.e., if image contains highestsolutionrating.
a person with a hat or not. You Multi-agent GPT enriches the instructions by delving
know what to do, right?
deeperandjustifyingtheMLsteps.Thisenhancementcon-
K3 "Regression with a Regression with a Tabular Paris
tributes to the clarity of the instructions and adds a layer
Tabular Paris Housing Housing Price Dataset
oftransparency,aidingusersinunderstandingtherationale
Price Dataset" (Reade,
behind the suggested approach. The collaborative synergy
and Chow, 2023d)
between fine-tuned Llama-2 and multi-agent GPT demon-
stratesthepotentialforleveragingadvancedmodelstorefine
andaugmentmachine-generatedinstructions.
Tables 8– 19 depict the triple instructions for competi-
tionsC2,K1,K2andK3,respectively.Onceagain,thefine-
tuned Llama-2 yields precise and straightforward instruc-
tions for generating accurate code. Notably, these instruc-
tionsaretheoptimalchoicefortheassignedtasks,aligning
E Trofimova et al.: PreprintsubmittedtoElsevier Page 9 of 16Linguacodus: transformative ML code generation
Data Preprocessing: Data Preprocessing:
-ThetrainingdataisloadedfromaCSVfileusingpandas. • Load the training and test data from CSV files, ensuring
- The test data is loaded from a CSV file using pandas. that the data is properly formatted.
-Thetrainingdataissplitintotrainingandvalidationsets • Preprocess the text data to prepare it for model input.
using the train_test_split function from sklearn. This may include tokenization and data cleaning if neces-
- The training data is further processed by converting the sary.
textdataintotokenizedsequencesusingtheTokenizerclass Model Architecture:
from nltk. • Choose an appropriate model architecture for this binary
- The tokenized sequences are then converted into input classification task. Consider using pre-trained models like
IDs and attention masks using the TensorFlow Sequence- BERT or GPT-3 as a starting point.
Classification API. •Fine-tunetheselectedmodelonthetrainingdatatoadapt
Model Architecture: ittothespecifictask.Ensurethatthearchitectureincludes
- The model architecture used is a BERT-based model. appropriate layers for text processing and classification.
-TheBERTmodelisloadedfromapre-trainedcheckpoint Model Training:
using the BERTCheckpoint class from the TensorFlow • Set up the training process with the following compo-
BERT library. nents:
- The BERT model is then fine-tuned on the training data ◦ Loss Function: Use binary cross-entropy as the loss
using the TensorFlow TensorBoard API. function for binary classification.
-Thefine-tunedBERTmodelisusedasthebasemodelfor ◦ Optimizer: Select an optimizer like Adam.
the classification task. ◦ Metrics: Track accuracy as the evaluation metric.
Model Training: • Train the model for a specified number of epochs,
- The model is trained using the TensorFlow TensorBoard monitoring the training progress to avoid overfitting.
API. •Splitthedataintotrainingandvalidationsetstoevaluate
-Thetrainingdataisusedtotrainthemodelforaspecified the model’s performance during training.
number of epochs. Model Evaluation:
- The training progress is monitored using the TensorFlow • After training, evaluate the model on the validation set
TensorBoard API. to assess its accuracy and other relevant metrics.
- The trained model is then used to make predictions on • Make necessary adjustments to the model or training
the test data. process based on the evaluation results.
- The predictions are saved in a submission file for evalua- Model Evaluation:
tion. • Use the trained and validated model to make predictions
on the test data.
Table 6
• Save the predictions in a submission file for evaluation.
A manually chosen among the top-3 best sample instruction
generated for C1 competition by fine-tuned Llama-2. Table 7
Anautomaticallychosenandimprovedwithmulti-agentGPT
bestsampleinstructiongeneratedforC1competitionbyfine-
C. SamplecodegeneratedbyGPT-3.5using tuned Llama-2.
taskdescriptionsandourrefined
instructions.
13 train_data = pd.read_csv('train_data.csv')
ThissectionpresentsexamplesofcodeinferredbyGPT- 14 test_data = pd.read_csv('test_data.csv')
15
3.5withtwovariationsoftask-describingprompts:onewith 16 # Preprocess text prompts
and one without the automatically chosen best instruction. 17 tokenizer = Tokenizer()
Using a pure task description prompt may result in incom- 18 tokenizer.fit_on_texts(train_data['prompt'])
plete ML code generation, lacking an adequately defined 19 train_sequences = tokenizer.texts_to_sequences(train_data[
'prompt'])
model, for example. Conversely, when the task description 20 test_sequences = tokenizer.texts_to_sequences(test_data['
promptisenrichedwiththeinstruction,GPT-3.5isdrivento prompt'])
producecompilableresults(Listings1–4) 21
22 # Pad sequences to a fixed length if necessary
1 23 max_seq_length = 20
2 # Import necessary libraries 24 train_sequences = tf.keras.preprocessing.sequence.
3 *** pad_sequences(train_sequences, maxlen=max_seq_length)
4 import tensorflow as tf 25 test_sequences = tf.keras.preprocessing.sequence.
5 from tensorflow.keras.preprocessing.image import pad_sequences(test_sequences, maxlen=max_seq_length)
ImageDataGenerator 26
6 from tensorflow.keras.preprocessing.text import Tokenizer 27 # Image preprocessing (resize, normalize)
7 from tensorflow.keras.layers import Input, Embedding, LSTM 28 image_size = (128, 128)
, Conv2D, Flatten, concatenate 29 train_datagen = ImageDataGenerator(rescale=1./255,
8 from tensorflow.keras.models import Model validation_split=0.2)
9 *** 30 train_generator = train_datagen.flow_from_dataframe(***)
10 31 valid_generator = train_datagen.flow_from_dataframe(***)
11 # Data Preprocessing 32
12 # Load training and test data 33 # Model Architecture
E Trofimova et al.: PreprintsubmittedtoElsevier Page 10 of 16Linguacodus: transformative ML code generation
Asadataanalyst,Iwouldstartbypreprocessingthedatato Data Preprocessing:
ensurethatitisinaformatthatcanbeeasilyanalyzedand - Read the training and test data from CSV files.
processed. This may involve cleaning the data, removing - Perform data cleaning and preprocessing steps, such as
any irrelevant or redundant information, and transforming handlingmissingvalues,convertingcategoricalvariablesto
the data into a format that can be easily analyzed by a numerical codes, and encoding text data.
machine learning model. - Split the data into training and testing sets.
Once the data is preprocessed, I would then move on to Model Architecture:
designing the model architecture. For the task of identi- -Definethemodelarchitectureusingapre-trainedlanguage
fying hate speech, detecting targets of hate speech, and model (e.g., BERT) and a classification head.
determining the stance of given texts, a combination of - The model will be trained using a combination of binary
natural language processing (NLP) and machine learning cross-entropy loss and categorical cross-entropy loss.
techniques would be necessary. Model Training:
To identify hate speech, I would use a technique called - Use the training data to train the model.
sentiment analysis, which involves analyzing the sentiment - Use the validation data to evaluate the model’s perfor-
of a piece of text to determine whether it is positive, mance.
negative, or neutral. To detect targets of hate speech, - Use the testing data to make predictions
I would use a technique called entity recognition, which
Table 9
involves identifying and extracting specific entities such as
A sample instruction generated for C2 competition by Code
individuals, organizations, and communities from the text.
Llama - Instruct.
To determine the stance of given texts, I would use a
techniquecalledstancedetection,whichinvolvesanalyzing Data Preprocessing:
the sentiment of a piece of text to determine whether it is • Clean and normalize the data, removing irrelevant or
supportive, opposing, or neutral. redundant information.
• Handle missing values and outliers appropriately.
Oncethemodelarchitectureisdesigned,Iwouldthentrain
themodelusingthepreprocesseddata.Thetrainingprocess Model Architecture:
wouldinvolvefeedingthepreprocesseddataintothemodel • Select a suitable machine learning algorithm for text
andadjustingthemodel’sparameterstominimizetheerror classification based on the task’s requirements and data
between the model’s predictions and the true labels. characteristics. Consider using natural language processing
(NLP) techniques.
Throughout the training process, I would monitor the Data Splitting:
model’s performance using metrics such as accuracy, pre- • Split the dataset into training, validation, and test sets
cision, recall, and F1-score. Once the model is trained, I to ensure proper model evaluation.
would evaluate its performance on a separate test set to Model Training and Hyperparameter Tuning:
ensure that it is generalizing well to new data. •Trainthemodelonthetrainingdataset,adjustingparam-
Overall, the key to success in this machine learning task eters to minimize error. Perform hyperparameter tuning to
is to carefully preprocess the data, design an appropriate optimize model performance.
model architecture, and train the model using a large and Evaluation:
diverse dataset. By following these steps, I believe that it • Calculate the F1-score to evaluate the model’s perfor-
is possible to achieve high performance on the CASE 2024 mance,ensuringitalignswiththecompetition’smetrictype
competition at EACL 2024. Handling Imbalanced Data:
•Implementtechniquestoaddressimbalancedclassdistri-
Table 8 bution if necessary.
A sample instruction generated for C2 competition by Code
Llama - Instruct. Table 10
Anautomaticallychosenandimprovedwithmulti-agentGPT
bestsampleinstructiongeneratedforC2competitionbyfine-
34 # Define text input tuned Llama-2.
35 text_input = Input(shape=(max_seq_length,))
36 text_embedding = Embedding(input_dim=len(tokenizer.
word_index) + 1, output_dim=128)(text_input)
1
37 text_lstm = LSTM(128)(text_embedding) 2 import torch
38 3 import torchvision.transforms as transforms
39 # Define image input 4 from some_image_to_text_model import ImageToTextModel
40 image_input = Input(shape=(image_size[0], image_size[1], 5 from some_text_embedding_model import TextEmbeddingModel
3)) 6 import pandas as pd
41 image_cnn = Conv2D(64, (3, 3), activation='relu')( 7
image_input) 8 # Load your pre-trained models
42 image_flatten = Flatten()(image_cnn) 9 image_to_text_model = ImageToTextModel.load_pretrained('
43 path_to_image_to_text_model')
44 *** 10 text_embedding_model = TextEmbeddingModel.load_pretrained(
Listing1:CodepartgeneratedforKagglecompetitionK1 'path_to_text_embedding_model')
withanenrichedinstructionprompt. 11
12 # Function to predict prompt from image
13 def predict_prompt(image_path):
E Trofimova et al.: PreprintsubmittedtoElsevier Page 11 of 16Linguacodus: transformative ML code generation
Asadataanalyst,Iwouldstartbypreprocessingtheimage Data Preprocessing:
data to ensure that it is in a format that can be used for • The training data consists of 100,000 images and their
machine learning. This may involve resizing the images, corresponding text prompts. The test data consists of
normalizing the pixel values, and possibly applying data 10,000 images and their corresponding text prompts.
augmentationtechniquestoincreasethesizeofthedataset. • The images are resized to a fixed size of 224x224 pixels.
Oncethedataispreprocessed,Iwouldthendesignamodel • The text prompts are tokenized using the tokenizer from
architecture that is suitable for the task at hand. This the BERT model.
may involve using a combination of convolutional neural • The tokenized text prompts are converted to integers
networks(CNNs)andrecurrentneuralnetworks(RNNs)to using the tokenizer’s 𝑐𝑜𝑛𝑣𝑒𝑟𝑡_𝑡𝑜𝑘𝑒𝑛𝑠_𝑡𝑜_𝑖𝑑𝑠 method.
capturetherelationshipsbetweentextpromptsandimages. •TheimageIDsandtextpromptIDsarestoredinseparate
To train the model, I would use a mean cosine similarity arrays.
metric to measure the similarity between the predicted Model Architecture:
prompts and the actual prompts. I would also use a • The model architecture used is a BERT-based model.
technique called prompt engineering to ensure that the •TheBERTmodelisloadedfromapre-trainedcheckpoint.
predicted prompts are robust and accurate. This involves • The BERT model is used as a feature extractor.
usingacombinationoftechniquessuchaswordembedding, • The extracted features are passed through a linear layer
attention mechanisms, and recurrent neural networks to with a sigmoid activation function.
improve the performance of the model. • The sigmoid activation function is used to convert the
Overall, the goal of this task is to develop a model that output of the linear layer to a probability value.
can predict the text prompt that corresponds to a given Model Training:
generated image, and to explore the relationship between • The model is trained using the training data.
text prompts and images in the context of text-to-image •Thetrainingdataissplitintotrainingandvalidationsets.
models. By using a combination of data preprocessing, • The model is trained using the Adam optimizer and a
model architecture, and model training, I believe that we learningrateof1e-5.Thetrainingprocessisrepeatedfora
can achieve this goal and make significant contributions specified number of epochs.
to the field of natural language processing and computer • The model is evaluated on the validation set during
vision. training.
• The training and validation loss and accuracy are
Table 11
recorded.
A sample instruction generated for K1 competition by Code
Additional Information:
Llama - Instruct.
• The model is saved after training.
• The saved model is loaded for use in the test data.
• The test data is passed through the model to generate
14 # Load and preprocess the image predictions.
15 image = Image.open(image_path) • The predicted text prompts are stored in a list.
16 transform = transforms.Compose([ •Thepredictedtextpromptsarethenconvertedtointegers
17 transforms.Resize((224, 224)),
18 transforms.ToTensor(), using the tokenizer’s 𝑐𝑜𝑛𝑣𝑒𝑟𝑡_𝑡𝑜𝑘𝑒𝑛𝑠_𝑡𝑜_𝑖𝑑𝑠 method.
19 ]) •Thepredictedtextpromptsaresavedinasubmissionfile.
20 image = transform(image).unsqueeze(0)
Table 12
21
22 # Use the model to predict the prompt A manually chosen among the top-3 best sample instruction
23 predicted_prompt = image_to_text_model.predict(image) generated for K1 competition by fine-tuned Llama-2.
24 return predicted_prompt
25
26 # Function to convert prompt to embedding
27 def prompt_to_embedding(prompt): 11 test_data = pd.read_csv('test.csv')
28 # Convert text prompt to embedding 12
29 embedding = text_embedding_model.encode(prompt) 13 # 2. Check for missing values in the dataset and fill them
30 return embedding with appropriate values.
31 14 train_data.fillna(train_data.mean(), inplace=True)
32 *** 15 test_data.fillna(test_data.mean(), inplace=True)
Listing2:CodepartgeneratedforKagglecompetitionK1 16
17 ***
withoutinstruction.
18
19 # 5. Scale numerical variables using standard scaling
techniques.
1 import pandas as pd 20 scaler = StandardScaler()
2 import numpy as np 21 train_data[['squareMeters', 'numberOfRooms', 'floors', '
3 from sklearn.model_selection import train_test_split numPrevOwners', 'made', 'garage']] = \
4 from sklearn.preprocessing import StandardScaler 22 scaler.fit_transform(train_data[['squareMeters', '
5 from sklearn.linear_model import LinearRegression, Ridge, numberOfRooms', 'floors', 'numPrevOwners', 'made', '
Lasso, ElasticNet
garage']])
6 from sklearn.metrics import mean_squared_error 23 test_data*** = \
7 24 scaler.transform(***)
8 # Data Preprocessing
25
9 # 1. Read the training and testing data from CSV files. 26 # Model Selection and Training
10 train_data = pd.read_csv('train.csv')
E Trofimova et al.: PreprintsubmittedtoElsevier Page 12 of 16Linguacodus: transformative ML code generation
Data Preprocessing: As a data analyst, I understand that the objective is to
•Loadthetrainingandtestdata,ensuringthatbothimage buildadeeplearningmodelthatcandetectwhetheragiven
and text prompt data are correctly loaded and aligned. imagecontainsapersonwithahatornot.Sincenotraining
• Apply any necessary data preprocessing, including han- dataset is provided, we will need to preprocess the image
dlingmissingvalues,cleaningthedata,andaddressingclass data and build a suitable model architectur
imbalances if present. Data Preprocessing:
• Perform image preprocessing, such as resizing and nor- 1. Image Resizing: Resize the images to a consistent
malizing pixel values. size to reduce the impact of image size on the model’s
• Tokenize the text prompts using a suitable tokenizer, performance.
considering any specific requirements for this task. 2.DataAugmentation:Applyrandomtransformationssuch
• Convert tokenized text prompts into numerical represen- as rotation, flipping, and color jittering to increase the size
tations (embeddings) using a method that captures the ofthetrainingdatasetandimprovethemodel’srobustness
semantic meaning of the text effectively. to variations in the input data.
Model Architecture: 3. Normalization: Normalize the pixel values of the images
• Design a model architecture that combines image and to a common range to improve the model’s performance.
text information effectively. Consider using a combination Model Architecture:
of convolutional neural networks (CNNs) for image data 1. Convolutional Neural Network (CNN): A CNN is a
and recurrent neural networks (RNNs) for text data. suitable architecture for image classification tasks. The
•Implementamechanismformergingorconcatenatingthe model will consist of multiple convolutional layers followed
image and text features within the model architecture. by pooling layers to reduce the spatial dimensions of the
•Utilizepre-trainedembeddingsormodelsifavailableand feature maps.
relevant to improve model performance. 2.FullyConnectedLayers:Theoutputoftheconvolutional
Model Training and Evaluation: layerswillbefedintofullyconnectedlayerstoproducethe
• Split the training data into training and validation sets final output.
formodeltraining.Implementasuitablevalidationstrategy, 3. Softmax Layer: The final output of the model will be a
suchask-foldcross-validation,toassessmodelperformance probability distribution over the possible classes (hat or no
robustly. hat).
• Train the model with appropriate hyperparameters, in- Model Training:
cluding learning rate, batch size, and optimizer choice. 1. Split the data into training and validation sets.
Monitor training with metrics relevant to the task, such 2. Train the model on the training set using a suitable
as mean cosine similarity. optimizer and loss function (e.g., cross- entropy loss).
• Implement early stopping to prevent overfitting and save 3. Evaluate the model on the validation set to monitor
the best-performing model during training. overfitting and adjust the hyperparameters as needed.
• Evaluate the model using metrics like mean cosine 4. Repeat the training and evaluation process until the
similarity on the validation set to assess its performance. model achieves the desired level of accuracy.
• Consider strategies like learning rate schedules or fine-
tuning pre-trained models to improve convergence and Byfollowingthesesteps,wecanbuildadeeplearningmodel
model generalization. that can accurately detect whether a given image contains
a person with a hat or not.
Table 13
Anautomaticallychosenandimprovedwithmulti-agentGPT Table 14
bestsampleinstructiongeneratedforK1competitionbyfine- A sample instruction generated for K2 competition by Code
tuned Llama-2. Llama - Instruct.
27 42 rmse = np.sqrt(mean_squared_error(y_test, y_pred))
28 # 1. Choose an appropriate regression algorithm 43 print(f'Root Mean Squared Error (RMSE): {rmse}')
considering the dataset's characteristics. 44
29 # Let's use Ridge Regression as an example. 45 # Model Prediction and Submission
30 regression_model = Ridge(alpha=1.0) # You can adjust alpha 46
as needed. 47 # 1. Use the trained model to predict the target variable
for the testing data.
31
32 # 3. Split the data into training and testing sets for 48 test_features = test_data.drop(columns=['id'])
model evaluation. 49 predicted_prices = regression_model.predict(test_features)
33 X = train_data.drop(columns=['id', 'price']) 50 ***
34 y = train_data['price'] Listing3:CodepartgeneratedforKagglecompetitionK3
35 X_train, X_test, y_train, y_test = train_test_split(X, y,
withanenrichedinstructionprompt.
test_size=0.2, random_state=42)
36
37 # 4. tT rr aa ii nn int ghe das te al .ected regression model using the 1 import pandas as pd
38 regression_model.fit(X_train, y_train) 2 3 f fr ro om m s sk kl le ea ar rn n. .em no sd ee ml b_ ls eele ic mt pi oo rn t i Rm ap no dr ot mFot rr ea si tn R_ et ge rs et s_ ss op rlit
3 4 49 0
1
# y_p5. redEv =alu ra et ge resth se ionm _o md oel delu .si pn rg edit ch te (Xt _e ts et si tng
)
data. 4
5
f ir mo pm ortskl ne ua mr pn y. am setr npics import mean_squared_error
6
E Trofimova et al.: PreprintsubmittedtoElsevier Page 13 of 16Linguacodus: transformative ML code generation
Data Preprocessing: Data Preprocessing:
-Thedatasetusedfortrainingandtestingthemodelisthe •LoadthetrainingdatafromaCSVfilenamed“train.csv."
ImageNet dataset. • Load the test data from a CSV file named “test.csv."
-The images are resized to a fixed size of 224x224 pixels. •Preprocesstheimagedatabyresizingallimagestoafixed
-The images are normalized using the mean and standard size of 224x224 pixels.
deviation of the ImageNet dataset. • Tokenize the text prompts using a pre-trained BERT
-The labels are one-hot encoded using the LabelEncoder model tokenizer.
from the sklearn library. • Convert the tokenized text prompts into
Model Architecture: numerical embeddings using the BERT tokenizer’s
-The model architecture used is a convolutional neural 𝑐𝑜𝑛𝑣𝑒𝑟𝑡_𝑡𝑜𝑘𝑒𝑛𝑠_𝑡𝑜_𝑖𝑑𝑠 method.
network (CNN) with multiple convolutional layers and Model Architecture:
pooling layers. • Build a neural network model that combines a convolu-
-The CNN has a total of 10 convolutional layers with tionalneuralnetwork(CNN)andalongshort-termmemory
different filter sizes and activation functions. (LSTM) architecture.
-TheoutputoftheCNNispassedthroughafullyconnected • The CNN consists of multiple convolutional layers with
layer with 10 units. batch normalization and ReLU activation.
-Thefinallayerhasasoftmaxactivationfunctiontooutput • The LSTM consists of multiple LSTM layers with batch
the probabilities of the different classes. normalization and ReLU activation.
Model Training: • Concatenate the outputs of the CNN and LSTM layers.
-The model is trained using the Adam optimizer with a • Add a fully connected layer with ReLU activation.
learning rate of 0.001. • The final output layer consists of a fully connected
-The model is trained for 10 epochs with a batch size of layer with sigmoid activation, aligning with the multi-label
128. classification nature of the problem.
-The training data is augmented using random rotation, Model Training:
zoom, and shift. • Split the training data into training and validation sets
-The model is evaluated using the accuracy metric on the using an appropriate split ratio.
training data. •TrainthemodelusingtheAdamoptimizerwithalearning
Model Evaluation and Testing: rate chosen through hyperparameter tuning.
-Themodelisevaluatedonthetestdatausingtheaccuracy • Monitor the training process for early stopping based on
metric. validation loss to prevent overfitting.
- The predictions are saved in a submission file for further • Record training and validation loss and accuracy metrics
analysis. during training.
Note: The code provided is just an example and may need Model Evaluation:
to be modified based on the specific requirements of the • Evaluate the model on the validation set using mean
task. cosine similarity as the primary metric, aligning with the
competition’s requirements.
Table 15
• Continuously optimize hyperparameters based on valida-
A manually chosen among the top-3 best sample instruction
tion performance.
generated for K2 competition by fine-tuned Llama-2.
• Once the model achieves satisfactory validation perfor-
mance, evaluate it on the test set and record mean cosine
similarity.
7 # Load the data
8 train_data = pd.read_csv('train.csv') Table 16
9 test_data = pd.read_csv('test.csv') Anautomaticallychosenandimprovedwithmulti-agentGPT
10 bestsampleinstructiongeneratedforK2competitionbyfine-
11 # Separate features and target tuned Llama-2.
12 X = train_data.drop('price', axis=1)
13 y = train_data['price']
14
15 # Split the data into training and validation sets 31 # Predict on test data
16 X_train, X_val, y_train, y_val = train_test_split(X, y, 32 test_predictions = model.predict(test_data)
test_size=0.2, random_state=42) 33 ***
17 Listing4:CodepartgeneratedforKagglecompetitionK3
18 # Initialize the model
withoutinstruction.
19 model = RandomForestRegressor(random_state=42)
20
21 # Train the model
22 model.fit(X_train, y_train)
References
23
24 # Predict on validation set
25 val_predictions = model.predict(X_val) Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,Aleman,F.L.,
Almeida,D.,Altenschmidt,J.,Altman,S.,Anadkat,S.,etal.,2023.Gpt-
26
27 # Calculate RMSE 4technicalreport.arXivpreprintarXiv:2303.08774.
28 rmse = np.sqrt(mean_squared_error(y_val, val_predictions)) Anil, R., Dai, A.M., Firat, O., Johnson, M., Lepikhin, D., Passos, A.,
29 print(f'Validation RMSE: {rmse}') Shakeri,S.,Taropa,E.,Bailey,P.,Chen,Z.,etal.,2023.Palm2technical
report.arXivpreprintarXiv:2305.10403.
30
E Trofimova et al.: PreprintsubmittedtoElsevier Page 14 of 16Linguacodus: transformative ML code generation
As a data analyst, I would follow these steps to solve the Data Preprocessing:
regression problem with the tabular Paris housing price -Read the training and testing data from CSV files.
dataset: -Check for missing values in the dataset and fill them with
appropriatevalues.Performexploratorydataanalysistoun-
Data Preprocessing: derstand the distribution of variables and any correlations.
Check for missing values and handle them appropri- -Transform categorical variables into numerical codes.
ately.Scale the data using a standard scaler or normalizer -Scale numerical variables using standard scaling tech-
to ensure that all features are on the same scale.Split the niques.
data into training and testing sets Model Architecture:
Model Architecture: -Define the model architecture using a suitable regression
Use a linear regression model with a single hidden layer to algorithm(e.g.,LinearRegression,RidgeRegression,Lasso
predict the housing prices.The model should have an input Regression, Elastic Net Regression).
layer with the number of features in the dataset, a hidden -Define the hyperparameters for the model (e.g., regular-
layer with a suitable number of neurons, and an output ization strength, learning rate).
layer with a single neuron.The model should also have an Model Training:
activation function for the hidden layer, such as ReLU or -Split the data into training and testing sets.
tanh -Train the model using the training data.
Model Training: -Evaluate the model using the testing data.
Trainthemodelusingthetrainingsetandevaluateitsper- -Fine-tune the model by adjusting the hyperparameters.
formanceonthe testingsetusingtheRMSEmetric.Adjust Model Prediction:
thehyperparametersofthemodel,suchasthelearningrate, -Usethetrainedmodeltopredictthetargetvariableforthe
number of hidden neurons, and regularization strength, to testing data.
improve the model’s performance.Repeat the training and -Calculate the root mean squared error (RMSE) between
evaluation process until the model’s performance on the the predicted values and the actual values.
testing set is satisfactory. -Save the predicted values in a CSV file for submission.
By following these steps, I can help solve the regression
problem with the tabular Paris housing price dataset using Note: The code provided is just an example and may need
a linear regression model with a single hidden layer. to be modified based on the specific requirements of the
task
Table 17
A sample instruction generated for K3 competition by Code Table 18
Llama - Instruct. A manually chosen among the top-3 best sample instruction
generated for K3 competition by fine-tuned Llama-2.
Ashis@CUDenver,2023.Hatornohat–thatisthequestion.URL:https:
//kaggle.com/competitions/hat-or-not-hat-that-is-the-question.
LeDell, E., Poirier, S., 2020. H2o automl: Scalable automatic machine
Berezovskiy, V., Gorodilova, A., Trofimova, E., Ustyuzhanin, A., 2023. learning,in:ProceedingsoftheAutoMLWorkshopatICML,ICML.
Machinelearningcodesnippetssemanticclassification.PeerJComputer Li,Y.,Choi,D.,Chung,J.,Kushman,N.,Schrittwieser,J.,Leblond,R.,Ec-
Science9,e1654. cles,T.,Keeling,J.,Gimeno,F.,DalLago,A.,etal.,2022.Competition-
Bubeck,S.,Chandrasekaran,V.,Eldan,R.,Gehrke,J.,Horvitz,E.,Kamar, levelcodegenerationwithalphacode.Science378,1092–1097.
E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S., et al., 2023. Sparks of Nair, V., Schumacher, E., Tso, G., Kannan, A., 2023. Dera: enhancing
artificial general intelligence: Early experiments with gpt-4. arXiv largelanguagemodelcompletionswithdialog-enabledresolvingagents.
preprintarXiv:2303.12712. arXivpreprintarXiv:2303.17071.
Chen,M.,Tworek,J.,Jun,H.,Yuan,Q.,Pinto,H.P.d.O.,Kaplan,J.,Ed- Pavao,A.,Guyon,I.,Letournel,A.C.,Tran,D.T.,Baro,X.,Escalante,H.J.,
wards,H.,Burda,Y.,Joseph,N.,Brockman,G.,etal.,2021.Evaluating Escalera,S.,Thomas,T.,Xu,Z.,2023.Codalabcompetitions:Anopen
largelanguagemodelstrainedoncode.arXivpreprintarXiv:2107.03374 sourceplatformtoorganizescientificchallenges. JournalofMachine
. LearningResearch24,1–6.
D.Moruri,A.Bray,W.R.,Chow,A.,2023.Predictco2emissionsinrwanda. Reade,,W.,Chow,A.,2023a. Binaryclassificationofmachinefailures.
URL:https://kaggle.com/competitions/playground-series-s3e20. URL:https://kaggle.com/competitions/playground-series-s3e17.
Drozdova,A.,Trofimova,E.,Guseva,P.,Scherbakova,A.,Ustyuzhanin, Reade,, W., Chow, A., 2023b. Binary classification with a tabular
A.,2023.Code4ml:alarge-scaledatasetofannotatedmachinelearning credit card fraud dataset. URL: https://kaggle.com/competitions/
code.PeerJComputerScience9,e1230. playground-series-s3e4.
Erickson,N.,Mueller,J.,Shirkov,A.,Zhang,H.,Larroy,P.,Li,M.,Smola, Reade,,W.,Chow,A.,2023c. Featureimputationwithaheatfluxdataset.
A.,2020.Autogluon-tabular:Robustandaccurateautomlforstructured URL:https://kaggle.com/competitions/playground-series-s3e15.
data.arXivpreprintarXiv:2003.06505. Reade,, W., Chow, A., 2023d. Regression with a tabular paris
Feng,Z.,Guo,D.,Tang,D.,Duan,N.,Feng,X.,Gong,M.,Shou,L.,Qin, housing price dataset. URL: https://kaggle.com/competitions/
B.,Liu,T.,Jiang,D.,etal.,2020. Codebert:Apre-trainedmodelfor playground-series-s3e6.
programmingandnaturallanguages.arXivpreprintarXiv:2002.08155. Roziere,B.,Gehring,J.,Gloeckle,F.,Sootla,S.,Gat,I.,Tan,X.E.,Adi,Y.,
Franklin,A.,2023.Commonlit-evaluatestudentsummaries.URL:https:
Liu,J.,Remez,T.,Rapin,J.,etal.,2023.Codellama:Openfoundation
//kaggle.com/competitions/commonlit-evaluate-student-summaries.
modelsforcode.arXivpreprintarXiv:2308.12950.
HuggingFace,. LMSYSchatbotarenaleaderboard. https://huggingface. Rozière,B.,Gehring,J.,Gloeckle,F.,Sootla,S.,Gat,I.,Tan,X.E.,Adi,Y.,
co/spaces/lmsys/chatbot-arena-leaderboard.Accessed:2024-03-08.
Liu,J.,Sauvestre,R.,Remez,T.,Rapin,J.,Kozhevnikov,A.,Evtimov,I.,
Keskar,N.S.,McCann,B.,Varshney,L.R.,Xiong,C.,Socher,R.,2019. Bitton,J.,Bhatt,M.,Ferrer,C.C.,Grattafiori,A.,Xiong,W.,Défossez,
Ctrl:Aconditionaltransformerlanguagemodelforcontrollablegenera- A.,Copet,J.,Azhar,F.,Touvron,H.,Martin,L.,Usunier,N.,Scialom,
tion.arXiv:1909.05858.
T.,Synnaeve,G.,2024.Codellama:Openfoundationmodelsforcode.
E Trofimova et al.: PreprintsubmittedtoElsevier Page 15 of 16Linguacodus: transformative ML code generation
Data Preprocessing:
Ekaterina Trofimova received the B.S. degree in
• Read the training and testing data from CSV files.
EconomicsfromLomonosovMoscowStateUni-
•Checkformissingvaluesinthedatasetandfillthemwith
versity, Moscow, Russia, in 2015 and the M.S.
appropriate values. degreesinmathematicsandcomputersciencefrom
• Perform exploratory data analysis to understand the Skolkovo Institute of Science and Technology,
distribution of variables and any correlations. Moscow,Russia,andappliedmathematicsandin-
• Handle categorical variables by transforming them into formaticsfromHSEUniversity,Moscow,Russia,
numerical codes or using one-hot encoding. in2020.SheiscurrentlypursuingthePh.D.degree
• Scale numerical variables using standard scaling tech- incomputerscienceatHSEUniversity,Moscow,
Russia.
niques.
Since 2020 she has been a Junior Research
Model Selection and Training:
FellowwithHSEUniversity,Moscow,Russia.Her
• Choose an appropriate regression algorithm considering
researchinterestsincludethegenerativemodelling,
the dataset’s characteristics. You may consider Linear
graphneuralnetworks,naturallanguageprocess-
Regression, Ridge Regression, Lasso Regression, or Elastic ingandapplicationofmachinelearningmethods
Net Regression. The choice should be based on the nature toscientificproblems.
of the problem, linearity assumptions, and potential multi-
collinearity.
• Define the hyperparameters for the selected regression EmilSataevreceivedtheB.S.degreeinElectronics
model. Pay attention to regularization strength, learning andNanoelectronicsinNationalResearchUniver-
sityofElectronicTechnology,Moscow,Russiain
rate, and any other relevant hyperparameters.
2022.HeiscurrentlypursuinganM.S.degreein
• Split the data into training and testing sets for model
AppliedElectronicsandPhotonicsattheNational
evaluation. Additionally, consider using cross-validation for
ResearchUniversityHigherSchoolofEconomics.
a more robust assessment.
Since March 2023, Emil has been an intern
• Train the selected regression model using the training researcher at the Laboratory of Methods for Big
data. • Validate the model assumptions such as linearity, DataAnalysis,HSEUniversity.Heisalsodevel-
homoscedasticity, and independence of errors. opinganintegratedopticalneuralnetworkatthe
• Evaluate the model using the testing data. Calculate the HSE Tikhonov Moscow Institute of Electronics
root mean squared error (RMSE) to assess the model’s and Mathematics. His research interests include
predictive performance. generative models, natural language processing,
andneuromorphicphotonics.
• Fine-tune the model by adjusting hyperparameters as
needed to improve its performance.
Table 19 Andrey Ustyuzhanin received the B.S. degree in
appliedmathematicsandtheM.S.degreesinap-
Anautomaticallychosenandimprovedwithmulti-agentGPT
pliedmathematicsandphysics,autonomouscon-
bestsampleinstructiongeneratedforK3competitionbyfine-
trolsystemsfromMoscowInstituteofPhysicsand
tuned Llama-2.
Technology,Moscow,Russia,in1998and2000,
respectively, and the Ph.D. degree in computer
science from Institute of System Programming,
arXiv:2308.12950. Moscow,Russia,in2007.
Shen,Y.,Song,K.,Tan,X.,Li,D.,Lu,W.,Zhuang,Y.,2024.Hugginggpt: HeiscurrentlyaDirectorofAI/MLresearch
Solvingaitaskswithchatgptanditsfriendsinhuggingface. Advances atAcronis,AdjunctProfessoratConstructoruni-
inNeuralInformationProcessingSystems36. versity and a Visiting research professor at Na-
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, tional University of Singapore. His team is the
Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al., 2023. memberoffrontierresearchinternationalcollab-
Llama2:Openfoundationandfine-tunedchatmodels. arXivpreprint orations: LHCb - collaboration at Large Hadron
arXiv:2307.09288. Collider,SHiP(SearchforHiddenParticles)-the
Vakhrushev,A.,Ryzhkov,A.,Savchenko,M.,Simakov,D.,Damdinov,R., experimentisdesignedfortheNewPhysicsdis-
Tuzhilin,A.,2021. Lightautoml:Automlsolutionforalargefinancial covery.Hisgroupisuniqueforbothcollaborations
servicesecosystem.arXivpreprintarXiv:2109.01528. sincethemajorityoftheteammembersarecoming
W.Reade,A.C.,2023a. Regressionwithacrabagedataset. URL:https: fromtheComputerandDataScienceworlds.The
//kaggle.com/competitions/playground-series-s3e16. primary priority of his research is the design of
W. Reade, A.C., 2023b. Regression with a tabular california newMachineLearningmethodsandusingthemto
housing dataset. URL: https://kaggle.com/competitions/ solvetoughscientificenigmasthusimprovingthe
playground-series-s3e1. fundamentalunderstandingofourworld.
W. Reade, W. Cukierski, A.C., 2023c. Stable diffusion - Dr. Ustyuzhanin received the Imagine Cup
image to prompts. URL: https://kaggle.com/competitions/ 2012BestFacultySupportAward.
stable-diffusion-image-to-prompts.
Zhang,J.,Panthaplackel,S.,Nie,P.,Li,J.J.,Gligoric,M.,2022. Coditt5:
Pretrainingforsourcecodeandnaturallanguageediting,in:Proceedings
ofthe37thIEEE/ACMInternationalConferenceonAutomatedSoft-
wareEngineering,pp.1–12.
Zhu,Y.,Lu,S.,Zheng,L.,Guo,J.,Zhang,W.,Wang,J.,Yu,Y.,2018.
Texygen:Abenchmarkingplatformfortextgenerationmodels,in:The
41stinternationalACMSIGIRconferenceonresearch&development
ininformationretrieval,pp.1097–1100.
E Trofimova et al.: PreprintsubmittedtoElsevier Page 16 of 16