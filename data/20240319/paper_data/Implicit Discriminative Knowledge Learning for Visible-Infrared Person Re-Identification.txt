Implicit Discriminative Knowledge Learning for Visible-Infrared Person
Re-Identification
KaijieRen,LeiZhang*
SchoolofMicroelectronicsandCommunicationEngineering,ChongqingUniversity,China
kaijieren@cqu.edu.cn, leizhang@cqu.edu.cn
Abstract
V(visible) I(infrared)
color
Visible-Infrared Person Re-identification (VI-ReID) is a modality-
challenging cross-modal pedestrian retrieval task, due to spectrum specific
significant intra-class variations and cross-modal discrep- saturation branch
ancies among different cameras. Existing works mainly contrast discriminator
focus on embedding images of different modalities into a ...
unified space to mine modality-shared features. They only implicit discriminative
seek distinctive information within these shared features, knowledge
whileignoringtheidentity-awareusefulinformationthatis
purify
implicit in the modality-specific features. To address this
modality-shared
issue, we propose a novel Implicit Discriminative Knowl-
feature
edge Learning (IDKL) network to uncover and leverage
theimplicitdiscriminativeinformationcontainedwithinthe
modality-specific. First, we extract modality-specific and Figure1. Previousmethodsfocusedonseekingdiscriminativein-
modality-shared features using a novel dual-stream net- formation within modality-shared features, overlooking the fact
that there are discriminative clues implicit in modality-specific
work. Then, the modality-specific features undergo purifi-
features.Itisworthconsideringutilizationoftheimplicitdiscrim-
cation to reduce their modality style discrepancies while
inativeinformationtoenhancesharedinvariantfeature.
preserving identity-aware discriminative knowledge. Sub-
sequently, this kind of implicit knowledge is distilled into
the modality-shared feature to enhance its distinctiveness. solelyonvisibleimages,lessefficientforVI-ReIDtasks.
Finally,analignmentlossisproposedtominimizemodality Recently, numerous advanced methods have emerged
discrepancyonenhancedmodality-sharedfeatures. Exten- in the field of VI-ReID. In conclusion, these approaches
sive experiments on multiple public datasets demonstrate can generally be categorized into two types: the first aims
the superiority of IDKL network over the state-of-the-art to learn modality-shared features directly from raw modal
methods. Codeisavailableathttps://github.com/ data, andthesecondseekstoincorporateadditionalmodal
1KK077/IDKL. informationtorefinethefeaturespaceorbridgethemodal
gap,therebyfacilitatingthesearchformodality-sharedfea-
tures. Raw modal data based method [2, 4, 7, 8, 16, 18,
33,49]istoembeddifferentmodalityimagesintothesame
1.Introduction
space and align them on feature-level so that model learn
Visible-Infrared Person Re-Identification (VI-ReID) modalityinvariantfeaturedirectly. Despiteoriginalimages
aims to match pedestrian images across multiple non- basedmethodshasachieveddesirableresults, theystillre-
overlapping camera views and different modalities. Ad- main a huge gap between different modalities in feature
vancedsurveillancesystemstodayarecapableofautomat- space. In order to bridge the gap between visible and in-
ically switching from visible to infrared mode at nightfall, fraredmodalitiesandconstructacontinuousspaceforbetter
ensuring an ample supply of trainable data. However, the learning modality-shared features, various methods based
unique modality of infrared images create significant do- onintroducingextrainformation[14,29,30,34,38,42,45]
maindiscrepanciesandamorecomplexenvironment. This haveemergedconstantly.
makes previous single-modality methods, which are based Although significant improvements have been made in
4202
raM
81
]VC.sc[
1v80711.3042:viXra
I+V
resufnoccurrent VI-ReID methods, these models inevitably discard knowledge implicit in the modality-specific features to
some discriminative information that relies on modality- enhancetheupperboundofdiscriminativepowerforthe
specific features, which is not fully exploited and utilized modality-sharedfeature.
previously.Thesediscriminativecuesthatexistinmodality- • To reduce the modality style discrepancy without losing
specific features can be referred to as implicit discrimi- discriminativeinformationofmodality-specificinforma-
native information, as shown by the green area in Fig. 1, tion, weproposeanIN-guidedInformationPurifier(IP),
suchascolor,grayscalespectrum,contrast,saturation,and whichissupervisedbythediscriminationenhancingloss
so on. Therefore, relying solely on modality-shared cues anddiscrepancyreducingloss.
can limit the upper boundary of the discrimination ability • The novel TGSA loss is developed to distill the dis-
of the feature representation. Utilizing implicit modality- criminative modality-specific information into modality-
specific characteristics effectively is essential to enhance shared feature and mitigate inter-modality discrepancy
the distinctiveness of modality-invariant features. How- of modality-shared feature sufficiently. The substantial
ever, we cannot directly use the modality-specific knowl- experimental results demonstrate the superiority of our
edge due to the modality discrepancies it contains. In- method.
stead,weneedtoreducethesediscrepancieswhilepreserv-
ing the identity-aware discriminative information inherent 2.RelatedWork
intheimplicitknowledge. Meanwhile,traditionalVI-ReID
Visible Modality Person ReID. Person ReID has re-
methodsinvolvingdistillation,alignment,andmutuallearn-
ceived increasing success in recent years, which aims to
ing typically rely on logits [35, 44, 47]. However, there is
implement pedestrian retrieval between visible images. It
no involvement of the classifier during the testing phase,
hassufferedhugechallengesincludingvariousviewpoints,
with matching performed only at the feature level. There-
illuminations, postures and so on. To solve these prob-
fore, conducting discriminative information distillation at
lems,mostmethodologies[20,24,25,39,48]aredesigned
thefeaturelevelisalsoessential.
to obtain unified intra-class and discriminative inter-class
To address the above limitations, in this paper, we
representation by training CNN network. To further en-
propose an Implicit Discriminative Knowledge Learning
hancethedistinguishabilityoffeatures,Sunetal.[27]pro-
(IDKL) framework that captures implicit invariant infor-
posedaligningpartfeaturedirectlyinsteadofusingexternal
mation from modality-specific features and distills it into
cues. He et al. [9] caught the hot spot of transformer and
modality-sharedfeaturestoenhancetheirdiscriminativeca-
first proposed the transformer variant applying on single-
pability. We first extract modality-specific and modality-
modality person ReID. Simultaneously, person ReID has
sharedfeaturesusingthemodalitydiscriminatorandmodal-
developed some significant branches yet, such as Unsu-
ity confuser, respectively. The modality discriminator ef-
pervised Domain Adaptive Person ReID (UDA-ReID) and
fectively distinguishes between different modal features,
DomainGeneralizationPersonReID(DG-ReID)[41]. Al-
endowing them with specific characteristics; whereas the
thoughexistingmethodshavemadewideprogressinvisible
modalityconfuserisunabletodifferentiatebetweenmodal
modalitypersonReID,theysufferperformancedegradation
features, thereby imparting shared characteristics to them.
whenappliedtovisible-infraredpersonReIDduetothese-
Sincethemodality-specificfeatureafterpreviousstagecon-
verecross-modalitydiscrepancy.
tains substantial modality discrepancies, it is not suitable
Visible-Infrared Person ReID (VI-ReID). VI-ReID is
for direct distillation into the shared feature. We initially
a cross-modality person retrieval problem, which aims at
employ Instance Normalization to reduce domain discrep-
matchingdaytimevisibleandnighttimeinfraredimages. It
ancies. However, it is crucial to acknowledge that IN in-
notonlyfacesdifficultiesencounteredintraditionalsingle-
evitablyresultsinthelossofsomediscriminativefeatures.
motality person ReID task, but also the main challenge of
Therefore,weaimtoreduceitsmodalitystylediscrepancy
hugemodalitydiscrepancywhichcausedbydifferentcam-
while preserving identity-aware discriminative knowledge.
eraspectra. Tosolvethetheseissues,numerousapproaches
Subsequently, we distill this implicit knowledge into the
are proposed to search shared feature[1, 5, 17, 19, 32, 36,
modality-shared feature at both the feature-level through
39, 46]. Wu et al. [32] first formulated the VI-REID is-
featuregraphstructure, andthesemantic-level throughthe
sue and contributed a new cross-modality dataset SYSU-
logitvectortoenhanceitsdistinctiveness. Finally,analign-
MM01, which is of great significance to the following re-
mentlossisproposedtominimizemodalitydiscrepancyon
search. Ye et al. [36] proposed a part-level feature in-
enhancedmodality-sharedfeatures.
teraction and graph structure attention to enhance the dis-
Themaincontributionsofthispapercanbesummarized crimination of invariant feature. Meanwhile, some work
asfollows: begin to introduce additional modal information to jointly
• We propose the Implicit Discriminative Knowledge search for invariant features [14, 21, 28, 30, 45]. Wang et
Learning (IDKL) network to utilize the discriminative al.[30]combinedRGBthreechannelsfeatureandIRsingleD 0 m o d a l i t y d i s c r i m i n a t o r D 1 The Modality Specific Branch
DDDDD11
(cid:23) (cid:24) Information Purifier
(cid:3) (cid:78) (cid:70) (cid:3) (cid:78) (cid:70) rrrr C
(cid:82) (cid:82)
(cid:20) (cid:3)
(cid:78)
(cid:21) (cid:3)
(cid:78)
(cid:22) (cid:3)
(cid:78)
(cid:79)
(cid:37)
(cid:79)
(cid:37) specific feature e c n
a
ts n
Im
ro
N
gniloop
ff
sssppp
(cid:70) (cid:70) (cid:70) TGSA KKKnnnooowwlleeddggee DDDiissttiillll CSA
(cid:82) (cid:82) (cid:82)
(cid:79) (cid:79) (cid:79)
(cid:37) (cid:37) (cid:37) shared feature
(cid:23) (cid:24) eeee
(cid:3) (cid:3)
(cid:78) (cid:78) aaaaaffffffffffiiiiiiiiiiiiiiiiinnnnnnnnnnnnnnnnniiiiiiiiiiiiiiiiittttttttttttttttttttyyyyyyyyyyyyyyyyyyyy mmmmmmmmmmmmmmmmmmaaaaaaaaaaaaaaaaaattttttttttttttttttrrriiixxx C
(cid:70) (cid:82) (cid:70) (cid:82) F sh pooling
(cid:79) (cid:79) classifier
(cid:37) (cid:37) logit
m o d a l i t y c o n f u s e r C1
C 0 G R L CCCCCC 11 C 2 The Modality Shared Branch
Figure2. FrameworkoftheproposedImplicitDiscriminativeKnowledgeLearning(IDKL)model. Thedualone-streamnetworkbuiltby
resnetblocksfirstextractsthemodality-specificF andmodalitysharedfeatureF undertheconstraintofmodalitydiscriminatorand
sp sh
modalityconfuseraccordingly,whilethecommonReIDlossisusedtooptimizenetworkbasely.Then,themodality-specificfeatureisfed
intotheinformationpurifiertoregulatethemodalitystylediscrepancywhilepreservingtheimplicitdiscriminativeinformationandobtain
the purified modality-specific feature F(cid:101)sp. Subsequently, this implicit knowledge is distilled into the modality-shared feature through
TGSAandCSA.Finally,theL isfurtherproposedtominimizemodalitydiscrepancywithintheenhancedmodality-sharedfeature.
mdr
channelfeatureintounitedfourchannelsfeaturetoreduce andmodalityconfuser, respectively. Simultaneously, com-
modaldifferences. Jiangetal.[14]employedaglobalpro- mon ReID loss is used to endow feature with representa-
totypetogeneratethemissingmodalitycounterpartthrough tion. Then,anInformationPurifier(IP)isdevelopedtore-
transformer. However, these generation based compensa- ducetheimpactofstylevarianceswhileretainingidentity-
tion methods inevitably introduce much noise and most of aware and discriminative knowledge in modality-specific
presentmethodsaimtoalignmodaldiscrepancyandextract features. Finally, we distill the implicit discriminative
modality invariant feature. They ignored use of beneficial knowledge across two branches into the modality-shared
style information contained in specific features to increase featurethroughTripletGraphStructureAlignment(TGSA)
distinctiveness. at the feature-level and Class Semantic Alignment (CSA)
Interactive Learning. In knowledge distillation, a at the logit-level. Additionally, to reduce modality dis-
teacher-student model is used to transfer the knowledge crepancyofsharedfeature,weproposedtheModalityDis-
learnedbyalargercomplexteachermodeltoasmallersim- crepancyReduction(MDR)losswithinthemodality-shared
plestudentmodel. Differentfromtheone-waytransferbe- branch. As the discrminative information increases and
tween a teacher and a student, deep mutual learning [44] modality discrepancy decreases constantly, the enhanced
isanensembleofstudentswhichlearnscollaborativelyand modality-sharedfeaturecanbeobtained.
teaches each other throughout the training process. Some
Formally, we represent visible and infrared images in a
workappliedthisideaintoVI-ReIDforinteractivelearning dataset as V = {xV}NV and I = {xI}NI , respectively.
i i=1 i i=1
in different modalities. Zheng et al. [47] enhanced invari-
Typically,thenumberofimagessampledinamini-batchis
antfeatureslearningbyusinginteractivelearningbetween equalacrossbothmodalities,i.e.,N =N =N =P×K,
V I
twomodality. Yeetal.[35]employedmulti-classifiertore- where N and N denote the number of images sampled
V I
duce modality discrepancy on logits-level. Wu et al. [33]
fromthevisibleandinfraredmodalities,respectively. Here,
distilledRGBandIRknowledgemutuallybyfourclassifier N represents the number of images from a single modal-
toachieveintersharedrepresentation. However, theyonly ity,2N isthetotalnumberofimagesinamini-batch,P is
learntheknowledgewithinthesemantic-level. the number of distinct person classes, and K is the num-
berofimagesfromeachclasswithinasinglemodality. We
3.Methodology
canthereforerepresenttheimagesandtheircorresponding
Inthissection,wedetailtheproposedImplicitDiscrim- labels in a mini-batch as X = {x i|x i ∈ V ∪I}2 i=N 1 and
inative Knowledge Learning (IDKL) framework as shown Y = {y }Np=P , respectively. These images X are fed
i i=1
in Fig. 2. IDKL first distinguishes modality-specific fea- intotwoseparatenetworkbranchestoextractthemodality-
tures from modality-shared features through two network specificfeaturesF whichcontainsF ,F andthe
sp sp,V sp,I
branches,whichareconstrainedbymodalitydiscriminator shared features F which contains F ,F as fol-
sh sh,V sh,I
F
sp
rrrrr deeeeeiiiiid
ˆF sp
mm
mmm
e
r
dF
sp
ˆ mF sp
+
- FF
spsp
fsh
rrrrr deeeeeiiiiid
mmmmm dddddrrrrrlows: where f ∈ RB×C is the pooling feature corresponding to
F ∈RB×C×H×W.
F =E (x|Θ,Ψ),F =E (x|Θ,Φ), (1)
sp sp sh sh Thus,thebaselossforourmodelisformulatedas:
where E and E denote the specific and the shared
sp sh L =L +L (6)
feature extractor by employing ResNet-50, respectively. b sh sp
Among the sturctures of ResNet-50, the global average 3.2.InformationPurifier
pooling replaced by Gem pooling[24] which is a pooling
The Information Purifier (IP) is designed to minimize
operationbetweenmaximumpoolingandaveragepooling.
theimpactofstylevarianceswhileretainingidentity-aware
And Θ is the shallow layer parameters with the first three
anddiscriminativeknowledgeinmodality-specificfeatures.
blocksofResNet-50,ΨandΦisthedeeplayerparameters
The MC integrates Instance Normalization (IN), which is
ofdifferentbrancheswiththelasttwoblocksofResNet-50.
knowntoreducedomaindiscrepancies[13,23,48]. How-
3.1.ModalityConfuserandDiscriminator ever,itisimportanttorecognizethatINinevitablyleadsto
thelossofsomediscriminativefeatures[11,15],potentially
Modality Confuser. For each sample image x , there
i hinderingthehighperformanceofReID.
isamodalitylabelt ∈ {0,1}. Tolearnmodality-irrelated
i To overcome the aforementioned issues, we have de-
sharedinformation,similarto[6],ourgoalistoconfusedif-
signed a dual-mask network guided by Instance Normal-
ferent domains such that a domain classifier cannot distin-
ization(IN)toalleviatemodalitystylediscrepancieswhile
guishthedomainoforiginforasample. Weemployanad-
preserving implicit discriminative knowledge. Firstly, we
versarialmodalityclassifierbasedontheGradientReversed
applyInstanceNormalization(IN)tothemodality-specific
Layer(GRL)asthemodalityconfuser. Theconstraintloss
forthemodalityconfuserisgivenby:
featuretoobtainF(cid:98)spby:
F −E[F ]
L
Cj
=− 21
N
(cid:88)2N t i·logp(cid:0) C j(cid:0) GRL(cid:0) Fi sh(cid:1)(cid:1)(cid:1) , (2) F(cid:98)sp =IN(F sp)= (cid:112) Vsp ar[F sp]s +p ϵ, (7)
i=1 wheretheϵrepresentsasafetyfactortoensurethedenom-
where C j represents the j-th modality confuser in the inator is not zero. The mean E[·] and variance Var[·] are
modality-shared branch, p(·) is the prediction probability calculatedalongeachchannel.
obtained via the softmax function, and t i is the modality FollowingtheapproachofSE-Net[10],wegeneratetwo
label. channel-wisemasksm andm by:
e r
ModalityDiscriminator. Toadequatelylearnmodality-
related specific information, we employ a modality classi-
m
e
=σ(W 2δ(W 1g(F sp))),m
r
=σ(W 4δ(W 3g(F(cid:98)sp))),
(8)
fier as the Modality Discriminator. This classifier, which
does not use GRL, is applied on the specific branch to ex-
whereg(·)denotesthepoolingoperation,W 1,W
3
∈R rc×c
tract modality-specific features. The classification loss is
and W 2,W
4
∈ Rc× rc are learnable parameters in the
fourfully-connected(FC)layerswhicharefollowedbythe
formulatedasfollows:
ReLU activation function δ(·) and the sigmoid activation
2N
L =− 1 (cid:88) t ·logp(cid:0) D (cid:0) Fi (cid:1)(cid:1) , (3) function σ(·). To balance the calculate consumption, the
Dj 2N i j sp dimensionreductionratiorissetto16.
i=1
The channel-wise masks m and m indicate enhance-
e r
where D j denotes the j-th modality discriminator in the mentofdiscriminativecharacteristicsandreductionofdis-
modality-specificbranch. crepancies attention mask, respectively. So we can obtain
Thecombinedlossfromthemodalityconfuseranddis- the stronger distinctiveness Fd+ and the smaller modality
sp
criminatorisgivenby: m−
differencesF(cid:98) by:
sp
K K
L C =(cid:88) L Cj, L D =(cid:88) L Dj (4) Fd sp+ =m e⊙F sp, F(cid:98)m sp− =m r⊙F(cid:98)sp. (9)
j=1 j=1
Subsequently,thediscriminationenhancinglossL and
e
To efficiently extract both modality-specific and
thediscrepancyreducinglossL arecalculatedtosupervise
r
modality-shared features, we combine these modality
m andm respectivelyas:
e r
classifier losses with the standard ReID loss L , which
reid
(cid:16) (cid:16) (cid:16) (cid:17)(cid:17) (cid:17)
includes cross-entropy and hard triplet loss. These are L =Softplus h C fd+ −h(C (f )) (10)
e sp sp sp sp
applied to both the modality-specific and modality-shared
branchesasfollows: (cid:16) (cid:16) m− m−(cid:17) (cid:16) (cid:17)(cid:17)
L
r
=Softplus d f(cid:98) sp,V,f(cid:98)
sp,I
−d f(cid:98) sp,V,f(cid:98)
sp,I
.
(cid:0) (cid:1)
L =L f +L ,L =L (f )+L , (5) (11)
sp reid sp D sh reid sh CHere,L e aimstoimbuethegeneratedFd sp+ withgreater (cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)
semantic distinctiveness compared to F , while L seeks (cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3) sp r b
to ensure that F(cid:98)m− exhibit smaller modality differences KL
sp align
thanF(cid:98)sp. AndSoftplus(·) = ln(1+exp(·))isafunction
withmonotonicincrease,designedtoalleviateoptimization
challengesbycircumventingnegativevaluesinloss.
Finally,analternatingintegrationstrategyisemployedto Figure 3. Illustration of the proposed TGSA loss: ’a’ and ’b’
extract the distinctive information from F(cid:98)m− by applying denote two different types of features. ’A’ represents the graph
sp
m and the invariant information from Fd+ by applying structure affinity matrix. After aligning the three affinity matri-
e sp
m . Thus, The two sets of features extracted in this way ces, the discrepancy in the graph structure distribution between
r
bothdemonstratesmallermodalitydifferencesandstronger features’a’andfeatures’b’willbeeliminated.
implicit discriminative information. By integrating them, Due to we utilize graph structure to align and distill
wederivetheoutputpurifiedmodality-specificfeatureF(cid:101)sp knowledge, insteadofaugmentingthefeature, andtheEu-
as: clidean space distribution is more interesting for features,
F(cid:101)sp =m e⊙F(cid:98)m sp− +m r⊙Fd sp+, (12) thus we replace the linear transformation with Euclidean
distancetocomputeattentionscoresandredefinethegraph
Intuitively, the pooling feature f˜ of implicit feature
sp structureexpressionfortwogroupsoffeaturesasfollows:
F(cid:101)sp is also constraint with ReID loss. So the information
(cid:16) (cid:16) (cid:17)(cid:17)
purifylossL ofthissectionissummerizedas: exp D fi,fj
ip a b
A ={α } = ,
L =L +L +L
(cid:16)
f˜
(cid:17)
(13)
(a;b) ij i,j∈N (cid:80) k∈Nexp(cid:16) D(cid:16) fi a,fk b(cid:17)(cid:17)
ip e r reid sp
(15)
whereα denotestheelementoftheaffinitymatrix,N cor-
3.3.ImplicitKnowledgeDistillation ij
responds to the entirety of samples within one modality, a
To ensure that the modality-shared feature comprehen- andbrepresenttwomodalities,D(·)meansEuclideandis-
sively learns and integrates implicit information, we per- tance.
formdistillationfromboththefeature-levelthroughTGSA Specifically, the triplet graph structure alignment loss
andlogit-levelthroughCSA. L is developed for cross modality ReID to align two
tgs
different modality types, enabling them to conform to the
same graph structure distribution and reduce modality dis-
3.3.1 TripletGraphStructureAlignment(TGSA)
crepancy. This loss encompasses two self-modal affinity
To endow the shared feature with discriminative infor- matrices and one cross-modal affinity matrix, with the lat-
mationandreducemodalitydiscrepancyatthefeaturelevel, terensuringtheoverallconsistencyofthegraphstructure’s
wedevelopatripletfeaturegraphstructurealignmentloss. distribution as shown in Fig. 3. These three matrices are
Thisapproachismotivatedbythefactthatthefeaturegraph aligned pairwise through the utilization of the Kullback-
structurecontainsabundantinformationabouttherelation- Leibler (KL) divergence. Therefore, the alignment loss
shipsanddistributionbetweenfeatures, suchasinter-class L oftwodistinctmodalitytypesisdefinedas:
tgsa
distinctiveness and intra-class diversity. These character-
istics are utilized to unearth potential feature relationships
L(a;b)
=(cid:88)P (cid:88)K (cid:16) KL(cid:16)
Apk ,Apk
(cid:17)
and enhance the feature representation in [16, 36]. The tgsa (a;a) (b;b)
graphstructureaffinitymatrixwhichindicatestherelation- p=1k=1
(cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
shipsbetweenfeaturesiscalculatedby: + KL Apk ,Apk +KL Apk ,Apk .
(a;a) (a;b) (a;b) (b;b)
(cid:0) (cid:0)(cid:2) (cid:0) (cid:1)(cid:3) (cid:1)(cid:1) (16)
α = exp L l(f i)∥l f j ·w , (14) Here, Apk representsthegraphstructuredistributionof
ij (cid:80) k∈Niexp(L([l(f i)∥l(f k)]·w)) the k-th sample in the p-th class, where P is the number
ofpersonclasses,andK denotesthenumberofimagesfor
where the L denote LeakyReLU activation function, [·∥·] eachclasswithinasinglemodality.
denote concatenate operation and N i denotes the neighbor In order to convey the discriminative modality-specific
sampleswhichareusedtonormalizefortheithsample.l(·)
knowledgetosharedfeatureonfeature-level,thedistillation
isfeaturedimensiontransformationlayerandw isthefull lossacrosstwobranchesonhomogeneousfeaturesthrough
connectionlayertocalculatescoresinapairfeatures. The TGSAcanbeformulatedas:
affinitymatrixisacquiredmakingthesescoresthroughsoft-
maxfunctionnormalization. L =L(sp,V;sh,V)+L(sp,I;sh,I). (17)
tgsa tgsa tgsa
a A a a( ; )
A
a b( ; )
A b b( ; )3.3.2 ClassSementicAlignment(CSA) origin sh sp(im) enhanced
CSA is used to distill the semantic information of im-
plicitmodality-specificknowledgeintothemodality-shared
e
branch for enhancing the feature representation of shared lb
is
features. CSAoperatesonhomogeneous features between iV
two branches at logit-level. The logit matrix behind the
classifiercanbeformulatedas:
(cid:0) (cid:1)
Z =C f ,Z =C (f ), (18)
sp sp sp sh sh sh
d
where C sp and C sh is the modality-specific and modality- e ra
shared classifier separately. And the logit Z and Z ∈ r
sp sh fn
R2N×C of specific and shared branch both contain visible I
modalityZ andinfraredmodalityZ ∈RN×C,C is
sp,V sp,I
thetotalnumberoftraindatasetidentities.
Figure 4. Observation the implicit discriminative information
For learning the implicit discriminative modality-
by Grad-CAM. And ’sh’ and ’sp(im)’ present the modality-
specific knowledge on semantic-level, the CSA loss is im-
sharedfeatureandthemodality-specificfeatureoftrainedIDKL
plemented on the same modality logit between the two
w/o knowledge distillation, respectively; ’enhanced’ denotes the
branches,whichisformulatedas:
modality-sharedfeatureofIDKLw/knowledgedistillation.
N
L =(cid:88)(cid:0) KL(cid:0) Zi ,Zi (cid:1) +KL(cid:0) Zi ,Zi (cid:1)(cid:1)
csa sh,V sp,V sh,I sp,I 4.Experiments
i=1
(19)
4.1.DatasetsandExperimentalSettings
3.4.ModalityDiscrepancyReduction(MDR)
Datasets. ThreepublicVI-ReIDdatasetsSYSU-MM01
At this part, to guarantee the invariant representation
[28],LLCM[43]andRegDB[22]areemployedtoevaluate
of modality-shared feature, the TGSA and CSA are fur-
our model. SYSU-MM01 is a popular large-scale dataset
ther used to reduce modality discrepancy within modality-
collected by four visible cameras and two near-infrared
sharedbranchasfollow:
cameras,includingindoorandoutdoorenvironments. And
N the test protocols consist of all-search and indoor-search.
L =L(sh,V;sh,I)+(cid:88) KL(cid:0) Zi ,Zi (cid:1) , (20) LLCMdatasetisalarge-scaleandlow-lightcross-modality
mdr tgsa sh,V sh,I
i=1 dataset, which is divided into training and testing sets at a
2:1 ratio. RegDB is collected using dual-camera systems,
In this way, the visible feature and infrared feature in
where visible and infrared images are captured in pairs.
modality-shared branch can achieve mutual learning from
Both LLCM and RegDB contain visible to infraed and in-
feature-level and semantic-level. It makes two modalities
fraredtovisibletwosearchmodes.
feature aligning information each other, while alleviating
modalgapandmaintainingtheinvariantofmodality-shared Evaluationmetrics. Thestandardrank-k matchingac-
feature. curacy and mean Average Precision (mAP) are adopted as
theevaluationmetrics. Allthereportedresultsaretheaver-
3.5.Optimization
ageof10trials.
Ultimately, by continuously distilling implicit dis- Implementation details. The proposed method and all
criminative knowledge from the modality-specific feature experimentsareimplementedonasingleNVIDIAGeForce
and consistently reducing modality discrepancies in the 3090 GPU with PyTorch framework. The baseline model
modality-shared feature, we can achieve a more discrimi- adoptstheResNet-50pre-trainedonImageNetwiththeL .
b
nativeandinvariantmodality-sharedfeature. The input images are resized to 3×384×128. The train
ThetotallossofthemodelIDKLisdefinedas: mini-batch size is set as 120, which contains 12 random
identitiesand10imagesforeveryidentity. Adamoptimizer
L =L +λ L +λ L +λ L +L withaninitiallearningrate3×10−5isexploited,whichde-
total b 1 ip 2 tgsa 3 csa mdr
(21) caysatthe60thand100thepochwithadecayfactorof0.1.
The hype-parameters λ , λ and λ are set to 0.1, 0.6 and
1 2 3
where λ , λ and λ are hype-parameters to balance the 0.8. Duringthetestingphase,onlymodality-sharedfeature
1 2 3
contributionofindividuallossterm. isusedtoevaluateperformance.Table1.ComparisonofCMC(%)andmAP(%)performanceswiththestate-of-the-artmethodsonSYSU-MM01dataset.
All-search Indoor-search
Methods Single-shot Multi-shot Single-shot Multi-shot
r=1 r=10 r=20 map r=1 r=10 r=20 map r=1 r=10 r=20 map r=1 r=10 r=20 map
Zero-Padding[32] 14.80 54.12 71.33 15.95 19.13 61.40 78.41 10.89 20.58 68.38 85.79 26.92 24.43 75.86 91.32 18.86
D-HSME[8] 20.68 62.74 77.95 23.12 - - - - - - - - - - - -
AlignGAN[28] 42.40 85.00 93.70 40.70 51.50 89.40 95.70 33.90 45.90 87.60 94.40 54.30 57.10 92.70 97.40 45.30
DDAG[36] 54.75 90.39 95.81 53.02 - - - - 61.02 94.06 98.41 67.98 - - - -
NFS[2] 56.91 91.34 96.52 55.45 63.51 94.42 97.81 48.56 62.79 96.53 99.07 69.79 70.03 97.70 99.51 61.45
PIC[47] 57.51 89.35 95.03 55.14 - - - - 60.40 - - 67.70 - - - -
MID[12] 60.27 92.90 - 59.40 - - - - 64.86 96.12 - 70.12 - - - -
cm-SSFT[21] 61.60 89.20 93.90 63.20 63.40 91.20 95.70 62.00 70.50 94.90 97.70 72.60 73.00 96.30 99.10 72.40
MCLNet[7] 65.40 93.33 97.14 61.98 - - - - 72.56 96.98 99.20 76.58 - - - -
FMCNet[42] 66.34 - - 62.51 73.44 - - 56.06 68.15 - - 74.09 78.86 - - 63.82
SMCL[31] 67.39 92.87 96.76 61.78 72.15 90.66 94.32 54.93 68.84 96.55 98.77 75.56 79.57 95.33 98.00 66.57
CAJ[37] 69.88 95.71 98.4 66.89 - - - - 76.26 97.88 99.49 80.37 - - - -
MPANet[33] 70.58 96.21 98.80 68.24 75.58 97.91 99.43 62.91 76.74 98.21 99.57 80.95 84.22 99.66 99.96 75.11
CMT[14] 71.88 96.45 98.87 68.57 80.23 97.91 99.53 63.13 76.9 97.68 99.64 79.91 84.87 99.41 99.97 74.11
DEEN[43] 74.7 97.6 99.2 71.8 - - - - 80.3 99.0 99.8 83.3 - - - -
SAAI[3] 75.90 - - 77.03 82.8 - - 82.39 83.20 - - 88.01 90.73 - - 91.30
MUN[40] 76.24 97.84 - 73.81 - - - - 79.42 98.09 - 82.06 - - - -
MSCLNet[45] 76.99 97.63 99.18 71.64 - - - - 78.49 99.32 99.91 81.17 - - - -
PartMix[14] 77.78 - - 74.62 80.54 - - 69.84 81.52 - - 84.38 87.99 - - 79.95
IDKL(Ours) 81.42 97.38 98.89 79.85 84.34 98.89 99.73 78.22 87.14 98.28 99.26 89.37 94.30 99.71 99.93 88.75
curacy of 94.72% in visible to infrared mode, and rank-1
accuracyof94.22%ininfraredtovisiblemode.
Comparison on LLCM dataset. The IDKL model
achievessignificantimprovementsonthelargeandcomplex
LLCMdatasetasshowninTab.3,demonstratingexcellent
rank-1accuracyof72.2%and70.7%ontwomodes,respec-
Figure5. Ablationanalysisofhyper-parameterλ andλ ,λ for tively. This indicates that the IDKL model exhibits strong
1 2 3
L ip,L tgsa,andL csarespectivelyonSYSU-MM01dataset. robustnessincomplexandmultimodalscenarios.
4.2.ComparisonwithState-of-the-artMethods
Table2.ComparisonoftheCMC(%)andmAP(%)performances
We compare our IDKL model with the state-of-the-art withstate-of-the-artmethodsonRegDBdataset.
VI-ReID methods published recent years on public VI- Visibletoinfrared Infraredtovisible
Methods
ReIDdatasetsSYSU-MM01,RegDBandLLCM. rank-1 map rank-1 map
Zero-Padding[32] 17.8 18.9 16.7 17.9
ComparisononSYSU-MM01anddataset. Thecom-
AlignGAN[28] 57.9 53.6 56.3 53.4
parison experimental results is shown in Tab. 1 which
DDAG[36] 69.34 63.46 68.06 61.80
displays the proposed IDKL method outperforms exist-
cm-SSFT[21] 72.3 72.9 71.0 71.7
ing cutting-edge methods. Specifically, the IDKL method
MCLNet[33] 80.31 73.07 75.93 69.49
achieves the accuracy of 81.42% rank-1 and 79.85% map
PIC[47] 83.6 79.6 79.5 77.4
with single-shot all search protocol , while the accuracy MPANet[33] 83.7 80.9 82.8 80.7
of87.14%rank-1and89.37%mapwithsingle-shotindoor SMCL[31] 83.93 79.83 83.05 78.57
searchprotocol.ThecomparedSOTAsincludevariousbase MSCLNet[45] 84.17 80.99 83.86 78.31
methods, i.e., for the methods of learning the shared fea- CAJ[37] 85.03 77.82 84.75 77.82
turethroughnetworkandlossfunctionimmediately,which MID[12] 87.45 84.85 84.29 81.41
FMCNet[42] 89.12 84.43 88.38 83.86
containD-HSME[8],NFS[2],CMT[14],DEEN[14]and
SAAI[3] 91.07 91.45 92.09 92.01
MCLNet [7]. Based on graph structure augment method
DEEN[43] 91.1 85.1 89.5 83.4
DDAG[36]andthemutuallearningbylogitsskill(PIC[47]
CMT[14] 95.17 87.3 91.97 84.46
and MPANet [33]). Comparing with several other based
MUN[40] 95.19 87.15 91.86 85.01
modality-specific methods cm-SSFT [40], MUN[21] and
IDKL(Ours) 94.72 90.19 94.22 90.43
MSCLNet[45],ourresultsoutperformthembyamargin.
Comparison on RegDB dataset. We also evaluate
4.3.AblationStudy
IDKLonasmall-scaledatasetRegDBasshowninTab.2.
ThereisstrongperformanceIDKLshowedandoutperforms Inthissubsection,weconducttheablationexperimentto
the existing solutions. Specifically, we achieve rank-1 ac- evaluateourproposedmodelexhaustively.Table3.ComparisonoftheCMC(%)andmAP(%)performances
withstate-of-the-artmethodsonLLCMdataset.
Visibletoinfrared Infraredtovisible
Methods
rank-1 map rank-1 map
DDAG[36] 40.3 48.4 48.0 52.3
CAJ[37] 56.5 59.8 48.8 56.6
DEEN[43] 62.5 65.8 54.9 62.9
IDKL(Ours) 72.22 66.43 70.72 65.19
Table4. Evaluationtheimpactofdifferentcomponentsinterms
ofrank-1(%)andmAP(%)onSYSU-MM01dataset.
SYSU-MM01
L L L L L
b ip tgsa csa mdr
rank1 map
✓ × × × × 67.60 66.47
✓ × ✓ × × 68.38 67.30
✓ ✓ ✓ × × 76.40 74.83
✓ ✓ × ✓ × 76.28 74.34
✓ ✓ × × ✓ 77.04 75.66
✓ ✓ ✓ ✓ ✓ 81.42 79.85 Figure6. Visualizationoflearnedfeaturesbyt-SNE.Where”a”
meansthepre-trainingresnet50onImageNet; ”b”representsthe
baseline;”c”istheIDKLmodelw/oTGSA;”d”isIDKLmodel
Effectiveness of each component. We evaluate the
w/TGSA.
effectiveness of each component on the SYSU-MM01
dataset under all search single-shot mode. Each compo- asignificantdiscrepancybetweenthetwomodalitiesisev-
nent is added independently to reveal its the performance ident. Fig.6(b)showsareductioninmodalitydiscrepancy
asTab.4. Thisindicatesthateachcomponentishighlyuse- and the model exhibiting some discriminative capabilities.
ful,withL mdr achievingsignificantperformanceimprove- ComparingFig.6(b)withFig.6(c),itisobservablethatthe
ments. ThisfurthersuggeststheeffectivenessofTGSAand IDKLmodelhasasmallerintra-classdiscrepancyandbet-
CSA in reducing modality discrepancies. Comparing the ter inter-class discrimination. In Fig. 6(d), the dark blue
secondrowwiththethirdrowdemonstratesthenecessityof classappearsmorescatteredthaninFig.6(c),andthegraph
purifying implicit modality-specific information, and also structuresofthetwomodalitiesaremoresimilarandclosely
provestheeffectivenessofourInformationPurifiermodule. aligned,demonstratingtheeffectivenessofTGSAinreduc-
Hyper-parametersanalysisofIP,TGSAandCSA.In ingmodalitydiscrepancies.
this part, we present a line chart to examine the detail in-
fluence of IP, TGSA and CSA by gradually increasing the 5.Conclusion
value of hyperparameters. As depicted in Fig. 5, the max-
This paper harnesses the implicit discriminative infor-
imum contributions of IP, TGSA and CSA are reached at
mation within modality-specific features and introduces a
0.1,0.6and0.8,respectively.Theupwardtrendofthecurve
robust model IDKL to exploit the potential discrimination
demonstratestheeffectivenessofeachmodule.
of heterogeneous-related features and enhance the shared
4.4.VisualizationAnalysis feature. TheIDKLmodelcomprisesadualone-streamnet-
work,anovelIN-guidedinformationpurifier,atripletgraph
AttentionmapsVisualization. Tofurtherillustratethe
structurealignmentsolution,andrefineddistillationonlog-
effectivenessofIDKL,Grad-CAM[26]isutilizedforavi-
its.Collectively,thesecomponentsdemonstrateexceptional
sual examination of different feature heatmaps. In Fig. 4,
effectivenessandcontributetoimprovedresults.
the areas of focus for implicit discriminative information
typicallydifferfromthoseofthesharedfeature,indicating
Acknowledgments
thateffectivelyandjudiciouslyutilizingthisinformationto
strengthenthesharedfeaturecanbehighlybeneficial. ThisworkwaspartiallysupportedbyNationalKeyR&D
Feature Distribution Visualization. We utilize t-SNE Program of China (2021YFB3100800), National Natural
[22] feature map visualization to observe the impact of ScienceFundofChina(62271090,61771079),Chongqing
the IDKL model and TGSA on the model. As shown in NaturalScienceFund(cstc2021jcyj-jqX0023)andNational
Fig. 6, each color represents a different identity, while the Youth Talent Project. This work is also supported by
shapes of circles and triangles indicate the visible and in- Huawei computational power of Chongqing Artificial In-
frared modality information, respectively. From Fig. 6(a), telligenceInnovationCenter.References for visible-infrared person re-identification. In Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv,
[1] Cuiqun Chen, Mang Ye, Meibin Qi, Jingjing Wu, Jianguo
Israel, October23–27, 2022, Proceedings, PartXIV,pages
Jiang,andChia-WenLin. Structure-awarepositionaltrans-
480–496.Springer,2022. 1,2,3,7
former for visible-infrared person re-identification. IEEE
[15] Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Li
TransactionsonImageProcessing,31:2352–2364,2022. 2
Zhang. Stylenormalizationandrestitutionforgeneralizable
[2] Yehansen Chen, Lin Wan, Zhihang Li, Qianyan Jing, and
person re-identification. In proceedings of the IEEE/CVF
ZongyuanSun.Neuralfeaturesearchforrgb-infraredperson
conference on computer vision and pattern recognition,
re-identification. In 2021 IEEE/CVF Conference on Com-
pages3143–3152,2020. 4
puter Vision and Pattern Recognition (CVPR), pages 587–
[16] Xulin Li, Yan Lu, Bin Liu, Yating Liu, Guojun Yin, Qi
597,2021. 1,7
Chu, Jinyang Huang, Feng Zhu, Rui Zhao, and Nenghai
[3] XingyeFang,YangYang,andYingFu.Visible-infraredper-
Yu. Counterfactualinterventionfeaturetransferforvisible-
sonre-identificationviasemanticalignmentandaffinityin-
infraredpersonre-identification. InComputerVision–ECCV
ference.InProceedingsoftheIEEE/CVFInternationalCon-
2022: 17th European Conference, Tel Aviv, Israel, Octo-
ferenceonComputerVision,pages11270–11279,2023. 7
ber 23–27, 2022, Proceedings, Part XXVI, pages 381–398.
[4] Chaoyou Fu, Yibo Hu, Xiang Wu, Hailin Shi, Tao Mei,
Springer,2022. 1,5
and Ran He. Cm-nas: Cross-modality neural architecture
[17] Wenqi Liang, Guangcong Wang, Jianhuang Lai, and Xiao-
searchforvisible-infraredpersonre-identification. In2021
hua Xie. Homogeneous-to-heterogeneous: Unsupervised
IEEE/CVF International Conference on Computer Vision
learning for rgb-infrared person re-identification. IEEE
(ICCV),pages11803–11812,2021. 1
TransactionsonImageProcessing,30:6392–6407,2021. 2
[5] XiaoweiFu,FuxiangHuang,YuhangZhou,HuiminMa,Xin
[18] Haijun Liu, Xiaoheng Tan, and Xichuan Zhou. Parameter
Xu, andLeiZhang. Cross-modalcross-domaindualalign-
sharingexplorationandhetero-centertripletlossforvisible-
mentnetworkforrgb-infraredpersonre-identification.IEEE
thermalpersonre-identification.IEEETransactionsonMul-
TransactionsonCircuitsandSystemsforVideoTechnology,
timedia,23:4414–4425,2021. 1
32(10):6874–6887,2022. 2
[19] Jialun Liu, Yifan Sun, Feng Zhu, Hongbin Pei, Yi Yang,
[6] YaroslavGaninandVictorLempitsky.Unsuperviseddomain
and Wenhui Li. Learning memory-augmented unidirec-
adaptationbybackpropagation. InInternationalconference
tionalmetricsforcross-modalitypersonre-identification. In
onmachinelearning,pages1180–1189.PMLR,2015. 4
2022 IEEE/CVF Conference on Computer Vision and Pat-
[7] Xin Hao, Sanyuan Zhao, Mang Ye, and Jianbing Shen.
ternRecognition(CVPR),pages19344–19353,2022. 2
Cross-modality person re-identification via modality con-
fusion and center aggregation. In 2021 IEEE/CVF In- [20] ZhipuLiu,LeiZhang,andDavidZhang.Neuralimageparts
ternational Conference on Computer Vision (ICCV), pages
groupsearchforpersonre-identification.IEEETransactions
onCircuitsandSystemsforVideoTechnology,2022. 2
16383–16392,2021. 1,7
[8] YiHao,NannanWang,JieLi,andXinboGao. Hsme: Hy- [21] YanLu,YueWu,BinLiu,TianzhuZhang,BaopuLi,QiChu,
perspheremanifoldembeddingforvisiblethermalpersonre- and Nenghai Yu. Cross-modality person re-identification
identification. InProceedingsoftheAAAIconferenceonar- with shared-specific feature transfer. In 2020 IEEE/CVF
tificialintelligence,pages8385–8392,2019. 1,7 Conference on Computer Vision and Pattern Recognition
(CVPR),pages13376–13386,2020. 2,7
[9] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,
and Wei Jiang. Transreid: Transformer-based object re- [22] Dat Tien Nguyen, Hyung Gil Hong, Ki Wan Kim, and
identification. In2021IEEE/CVFInternationalConference KangRyoungPark. Personrecognitionsystembasedona
onComputerVision(ICCV),pages14993–15002,2021. 2 combinationofbodyimagesfromvisiblelightandthermal
[10] JieHu,LiShen,andGangSun. Squeeze-and-excitationnet- cameras. Sensors,17(3):605,2017. 6,8
works. InProceedingsoftheIEEEconferenceoncomputer [23] XingangPan,PingLuo,JianpingShi,andXiaoouTang.Two
visionandpatternrecognition,pages7132–7141,2018. 4 at once: Enhancing learning and generalization capacities
[11] XunHuangandSergeBelongie. Arbitrarystyletransferin viaibn-net. InProceedingsoftheEuropeanConferenceon
real-timewithadaptiveinstancenormalization. InProceed- ComputerVision(ECCV),pages464–479,2018. 4
ings of the IEEE international conference on computer vi- [24] Filip Radenovic´, Giorgos Tolias, and Ondˇrej Chum. Fine-
sion,pages1501–1510,2017. 4 tuningcnnimageretrievalwithnohumanannotation. IEEE
[12] ZhipengHuang,JiaweiLiu,LiangLi,KechengZheng,and TransactionsonPatternAnalysisandMachineIntelligence,
Zheng-JunZha. Modality-adaptivemixupandinvariantde- 41(7):1655–1668,2019. 2,4
compositionforrgb-infraredpersonre-identification.InPro- [25] Min Ren, Lingxiao He, Xingyu Liao, Wu Liu, Yunlong
ceedings of the AAAI Conference on Artificial Intelligence, Wang, and Tieniu Tan. Learning instance-level spatial-
pages1034–1042,2022. 7 temporal patterns for person re-identification. In 2021
[13] Jieru Jia, Qiuqi Ruan, and Timothy M Hospedales. Frus- IEEE/CVF International Conference on Computer Vision
tratinglyeasypersonre-identification: Generalizingperson (ICCV),pages14910–14919,2021. 2
re-idinpractice. arXivpreprintarXiv:1905.03422,2019. 4 [26] RamprasaathRSelvaraju,MichaelCogswell,AbhishekDas,
[14] KongzhuJiang,TianzhuZhang,XiangLiu,BingqiaoQian, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
YongdongZhang,andFengWu.Cross-modalitytransformer Grad-cam: Visual explanations from deep networks viagradient-basedlocalization. InProceedingsoftheIEEEin- [38] Mang Ye, Jianbing Shen, and Ling Shao. Visible-infrared
ternationalconferenceoncomputervision,pages618–626, person re-identification via homogeneous augmented tri-
2017. 8 modal learning. IEEE Transactions on Information Foren-
[27] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin sicsandSecurity,16:728–739,2021. 1
Wang. Beyond part models: Person retrieval with refined [39] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling
partpooling(andastrongconvolutionalbaseline). InPro- Shao, and Steven C. H. Hoi. Deep learning for person re-
ceedings of the European conference on computer vision identification: A survey and outlook. IEEE Transactions
(ECCV),pages480–496,2018. 2 onPatternAnalysisandMachineIntelligence, 44(6):2872–
[28] Guan’an Wang, Tianzhu Zhang, Jian Cheng, Si Liu, Yang 2893,2022. 2
Yang,andZengguangHou.Rgb-infraredcross-modalityper- [40] Hao Yu, Xu Cheng, Wei Peng, Weihao Liu, and Guoying
sonre-identificationviajointpixelandfeaturealignment. In Zhao.Modalityunifyingnetworkforvisible-infraredperson
2019IEEE/CVFInternationalConferenceonComputerVi- re-identification. InProceedingsoftheIEEE/CVFInterna-
sion(ICCV),pages3622–3631,2019. 2,6,7 tionalConferenceonComputerVision,pages11185–11195,
[29] Guan-An Wang, Tianzhu Zhang, Yang Yang, Jian Cheng, 2023. 7
Jianlong Chang, Xu Liang, and Zeng-Guang Hou. Cross- [41] LeiZhang,ZhipuLiu,WenshengZhang,andDavidZhang.
modality paired-images generation for rgb-infrared person Styleuncertaintybasedself-pacedmetalearningforgeneral-
re-identification. InProceedingsoftheAAAIconferenceon izablepersonre-identification. IEEETransactionsonImage
artificialintelligence,pages12144–12151,2020. 1 Processing,32:2107–2119,2023. 2
[30] Zhixiang Wang, Zheng Wang, Yinqiang Zheng, Yung-Yu [42] Qiang Zhang, Changzhou Lai, Jianan Liu, Nianchang
Chuang,andShin’ichSatoh. Learningtoreducedual-level Huang, andJungongHan. Fmcnet: Feature-levelmodality
discrepancyforinfrared-visiblepersonre-identification. In compensationforvisible-infraredpersonre-identification.In
2019 IEEE/CVF Conference on Computer Vision and Pat- 2022 IEEE/CVF Conference on Computer Vision and Pat-
ternRecognition(CVPR),pages618–626,2019. 1,2 ternRecognition(CVPR),pages7339–7348,2022. 1,7
[31] Ziyu Wei, Xi Yang, Nannan Wang, and Xinbo Gao. Syn- [43] YukangZhangandHanziWang. Diverseembeddingexpan-
cretic modality collaborative learning for visible infrared sion network and low-light cross-modality benchmark for
person re-identification. In 2021 IEEE/CVF International visible-infrared person re-identification. In Proceedings of
Conference on Computer Vision (ICCV), pages 225–234, theIEEE/CVFConferenceonComputerVisionandPattern
2021. 7 Recognition,pages2153–2162,2023. 6,7,8
[32] Ancong Wu, Wei-Shi Zheng, Hong-Xing Yu, Shaogang [44] Ying Zhang, Tao Xiang, Timothy M Hospedales, and
Gong,andJianhuangLai. Rgb-infraredcross-modalityper- HuchuanLu. Deepmutuallearning. InProceedingsofthe
sonre-identification.In2017IEEEInternationalConference IEEE conference on computer vision and pattern recogni-
onComputerVision(ICCV),pages5390–5399,2017. 2,7 tion,pages4320–4328,2018. 2,3
[33] QiongWu,PingyangDai,JieChen,Chia-WenLin,Yongjian [45] Yiyuan Zhang, Sanyuan Zhao, Yuhao Kang, and Jianbing
Wu, FeiyueHuang, BinengZhong, andRongrongJi. Dis- Shen.Modalitysynergycomplementlearningwithcascaded
covercross-modalitynuancesforvisible-infraredpersonre- aggregationforvisible-infraredpersonre-identification. In
identification. In2021IEEE/CVFConferenceonComputer ComputerVision–ECCV2022: 17thEuropeanConference,
VisionandPatternRecognition(CVPR),pages4328–4337, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part
2021. 1,3,7 XIV,pages462–479.Springer,2022. 1,2,7
[46] Jiaqi Zhao, Hanzheng Wang, Yong Zhou, Rui Yao,
[34] Yang Yang, Tianzhu Zhang, Jian Cheng, Zengguang Hou,
Silin Chen, and Abdulmotaleb El Saddik. Spatial-
Prayag Tiwari, Hari Mohan Pandey, et al. Cross-modality
channelenhancedtransformerforvisible-infraredpersonre-
paired-imagesgenerationandaugmentationforrgb-infrared
person re-identification. Neural Networks, 128:294–304,
identification.IEEETransactionsonMultimedia,pages1–1,
2022. 2
2020. 1
[47] XiangtaoZheng,XiumeiChen,andXiaoqiangLu. Visible-
[35] Mang Ye, Xiangyuan Lan, Qingming Leng, and Jianbing
infraredpersonre-identificationviapartiallyinteractivecol-
Shen. Cross-modalitypersonre-identificationviamodality-
laboration. IEEE Transactions on Image Processing, 31:
awarecollaborativeensemblelearning. IEEETransactions
6951–6963,2022. 2,3,7
onImageProcessing,29:9387–9399,2020. 2,3
[48] Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, and
[36] Mang Ye, Jianbing Shen, David J. Crandall, Ling Shao,
Tao Xiang. Omni-scale feature learning for person re-
and Jiebo Luo. Dynamic dual-attentive aggregation learn-
identification.InProceedingsoftheIEEE/CVFinternational
ingforvisible-infraredpersonre-identification.InComputer
conferenceoncomputervision,pages3702–3712,2019. 2,
Vision–ECCV 2020: 16th European Conference, Glasgow,
4
UK,August23–28,2020,Proceedings,PartXVII16,pages
229–247.Springer,2020. 2,5,7,8 [49] YuanxinZhu,ZhaoYang,LiWang,SaiZhao,XiaoHu,and
Dapeng Tao. Hetero-center loss for cross-modality person
[37] Mang Ye, Weijian Ruan, Bo Du, and Mike Zheng Shou.
re-identification. Neurocomputing,386:97–109,2020. 1
Channelaugmentedjointlearningforvisible-infraredrecog-
nition. In 2021 IEEE/CVF International Conference on
ComputerVision(ICCV),pages13547–13556,2021. 7,8