Nonsmooth Implicit Differentiation:
Deterministic and Stochastic Convergence Rates
Riccardo Grazzi1, Massimiliano Pontil1,2, and Saverio Salzo1,3
1
CSML,IstitutoItalianodiTecnologia,ViaEnricoMelen83,16152Genova,Italy
2DepartmentofComputerScience,UCL,MaletPlace,LondonWC1E6BT,UK
3DIAG,SapienzaUniversityofRome,ViaAriosto,25,00185Roma,Italy
Abstract
Westudytheproblemofefficientlycomputingthederivativeofthefixed-pointofaparametric
non-differentiable contraction map. This problem has wide applications in machine learning,
includinghyperparameteroptimization, meta-learninganddatapoisoningattacks. Weanalyze
twopopularapproaches: iterativedifferentiation(ITD)andapproximateimplicitdifferentiation
(AID).Akeychallengebehindthenonsmoothsettingisthatthechainruledoesnotholdanymore.
Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of
non-differentiableITD,weproviderefinedlinearconvergenceratesforbothITDandAIDinthe
deterministiccase. WefurtherintroduceNSID,anewmethodtocomputetheimplicitderivative
when the fixed point is defined as the composition of an outer map and an inner map which is
accessibleonlythroughastochasticunbiasedestimator. Weestablishratesfortheconvergence
ofNSIDtothetruederivative,encompassingthebestavailableratesinthesmoothsetting. We
presentillustrativeexperimentsconfirmingouranalysis.
Keywords. Bileveloptimization,hyperparameteroptimization,stochasticalgorithms,nonsmoothoptimization,
implicitdifferentiation,conservativederivatives.
1 Introduction
Inthispaper,westudytheproblemofefficientlyapproximatingageneralizedderivative(orJacobian)
of the solution map of the parametric fixed point equation
w(λ) = Φ(w(λ),λ) (λ ∈ Rm), (1)
whenΦisnotdifferentiable,butonlydifferentiablealmosteverywhere. Weaddressboththecasethat
Φ can be explicitly evaluated, and the case that Φ has the composite form
Φ(w,λ) = G(T(w,λ),λ)
(2)
T(w,λ) = E[Tˆ(w(λ),λ,ξ)],
where the external map G can be evaluated, but the inner map T is accessible only via a stochastic
estimator Tˆ , with ξ a random variable.
1
4202
raM
81
]LM.tats[
1v78611.3042:viXraA main motivation for computing the implicit derivative of (1) is provided by bilevel optimization,
which aims to minimize an upper level objective function of w(λ). Important examples are given by
hyperparameter optimization and meta-learning (Franceschi et al., 2018; Lee et al., 2019), where (1)
expresses the optimality conditions of a lower-level minimization problem. Further examples include
learningasurrogatemodelfordatapoisoningattacks(Xiaoetal.,2015;Mun˜oz-Gonza´lezetal.,2017),
deep equilibrium models (Bai et al., 2019) or OptNet (Amos & Kolter, 2017). All these problems
may present nonsmooth mappings Φ. For instance, consider hyperparameter optimization or data
poisoning attacks for SVMs, or meta-learning for image classification, where Φ is evaluated through
the forward pass of a neural net with RELU activations (Bertinetto et al., 2019; Lee et al., 2019;
Rajeswaran et al., 2019). In addition, when such settings are applied to large datasets, evaluating
the map Φ would be too costly, but we can usually apply stochastic methods through the composite
stochastic structure in (2), where only T involves a computation on the full training set (e.g. a
gradient descent step).
Nowadays, automatic differentiation techniques (Griewank & Walther, 2008) popular for deep
learning, can also be used to efficiently, i.e. with a cost of the same order of that of approximating
w(λ), approximate Jacobian-vector (or vector-Jacobian) products of w(λ) by relying only on an
implementation of an iterative solver for problem (1). There are two main approaches to achieve
this: ITerative Differentiation (ITD) (e.g. Maclaurin et al. (2015); Franceschi et al. (2017)), which
differentiates through the steps of the solver for (1), and Approximate Implicit Differentiation (AID)
(e.g. Pedregosa (2016); Lorraine et al. (2020)), which relies on approximately solving the linear
system emerging from the implicit expression for the Jacobian-vector product. Despite the analysis of
such methods has been usually done in the case that Φ is smooth, there are now several open source
implementationsrelyingonpopulardeeplearningframeworks(e.g.Grazzietal.(2020);Blondeletal.
(2022); Liu & Liu (2021)), which practitioners can use even when Φ is not differentiable. However,
when Φ is not differentiable despite existing algorithmic proposals (Ochs et al., 2015; Frecon et al.,
2018), establishing theoretical convergence guarantees is challenging, since even if the solution map
w is almost everywhere differentiable and the Clarke subgradient is well defined, the chain rule of
differentiation, exploited by AID and ITD approaches, does not hold.
Recently Bolte & Pauwels (2021) introduced the notion of conservative derivatives as an effec-
tive tool to rigorously address automatic differentiation of neural networks with non-differentiable
activations (e.g. ReLU). Moreover, if Φ(·,λ) is a contraction and under the general assumption that
Φ if piecewise Lipschitz smooth with finite pieces, Bolte et al. (2022) provide an asymptotic linear
convergence rate for deterministic ITD.1 However, we are not aware of any result of such type for
the AID method and for the stochastic setting of problem (2), even when G(v,λ) = v. In particular
the compositional structure (2) allows us to cover e.g. proximal stochastic gradient methods, which
are a common and practical example of nonsmooth optimization algorithms, but it adds additional
challenges since we do not have access to an unbiased estimator of Φ as for the smooth stochastic
case studied in (Grazzi et al., 2021, 2023).
Contributions We present theoretical guarantees on AID and ITD for the approximation of the
conservative derivative of the fixed point solution of (1), building upon the analysis of Bolte et al.
(2022). Specifically:
• We prove non-asymptotic linear convergence rates for ITD and AID approaches which extends
the results given in (Grazzi et al., 2020) for the case where Φ is Lipschitz smooth. The given
1Therein,referredtoaspiggybackautomaticdifferentiation.
2bounds indicate that AID converges faster than ITD, which we verify empirically. We also
identify cases in which this difference in performance in favor of AID might be large due to
non-differentiable regions.
• We propose the first stochastic AID approach with proven convergence rates, which we name
nonsmooth stochastic implicit differentiation (NSID). Notably, we prove that NSID can converge
to the true conservative Jacobian-vector product with rate O(1/k), where k is the number of
samples, provided that the fixed-point problem is solved with rate O(1/k).
Finally, we provide experiments on two bilevel optimization problems, hyperparameter optimization
and adversarial poisoning attacks, confirming our theoretical finding.
Related Work When Φ is differentiable and under some regularity assumptions, approximation
guarantees have been established for AID and ITD approaches in the deterministic setting (Pedregosa,
2016; Grazzi et al., 2020), and for AID approaches in the special case of the stochastic setting (2)
where G(v,λ) = v and we have access only to Tˆ (Grazzi et al., 2021, 2023). Furthermore, several
works established convergence rates and, in the stochastic setting, sample complexity results for
bilevel optimization algorithms relying on AID and ITD approaches, see e.g. (Ghadimi & Wang, 2018;
Ji et al., 2021; Arbel & Mairal, 2021; Chen et al., 2021).
Aside from (Bolte et al., 2022), in the nonsmooth case, Bertrand et al. (2020, 2022) present de-
terministic and sparsity-aware nonsmooth ITD and AID procedures together with asymptotic linear
convergence guarantees when w(λ) is the solution of a composite minimization problem where one
component has a sum structure. Contrary to this work and to (Bolte et al., 2022), their results rely
on some differentiability assumptions on the algorithms, which is verified after a finite number of
iterations. For bilevel optimization, some recent works have provided stochastic algorithms with
convergenceratesforthespecialcasewherethelower-levelproblemhaslinear(Khandurietal.,2023)
or equality (Xiao et al., 2023) constraints.
2 Preliminaries
Notation If X and Y are two nonempty sets, we denote by F: X ⇒ Y a set-valued mapping, which
associates to an element of X a subset of Y. A selection of F is a single-valued function f: X → Y
such that, for every x ∈ X, f(x) ∈ F(x). We denote with ∥·∥ the Euclidean and operator norm when
applied to vectors and matrices, respectively. Set inclusion is denoted by ⊂. We define Minkowski
operations on sets of matrices as follows: if A,B ⊂ Rn×p and C ⊂ Rp×d then
A+B := {A+B|A ∈ A,B ∈ B},
AC := {AB|A ∈ A,C ∈ C}
Ax := {Ax|A ∈ A} with x ∈ {⊤,−1}.
We let co(A) be the convex envelope of A, and define ∥A∥ = sup{∥A∥|A ∈ A}. For any integer
sup
m ≥ 1 we set [m] = {1,...,m}. If F: Rp1+p2 → Rd is a differentiable mapping, we denote by
F′(x) ∈ Rd×(p1+p2) the derivative of F (its Jacobian) and by ∂ 1F(x) ∈ Rd×p1 and ∂ 2F(x) ∈ Rd×p2
the partial derivatives of F with respect to the first and second block of variables respectively. For a
randomvectorξ ∈ Rd,wedenotewithE[ξ]itsexpectationandwithVar[ξ] = E∥ξ−E[ξ]∥2 itsvariance.
In our assumptions we will consider the class of so called definable functions, which includes the large
majority of functions used for machine learning applications (see Appendix A).
32.1 Conservative Derivatives
Weprovidesomedefinitionsrelatedtopathdifferentiabilityandsetsofmatricesandvectors. Theyare
mostly borrowed, possibly with slight modifications, from (Bolte & Pauwels, 2021), where additional
details can be found.
Definition 2.1 (Conservative Derivatives). Given a locally Lipschitz continuous mapping F: U ⊂
Rp → Rd definedontheopensetU,wesaythataset-valuedmappingD : U ⇒ Rd×p isaconservative
F
derivative (CD) of F, if D has closed graph, nonempty compact values, and for every absolutely
F
continuous curve γ: [0,1] → Rp we have, for almost all t ∈ [0,1], that
d
F(γ(t)) = Vγ′(t), ∀V ∈ D (γ(t)). (3)
F
dt
A function F: U ⊂ Rp → Rd is called path differentiable if it is locally Lipschtz continuous and has a
conservative derivative.
Conservativederivativesareextensivelyanalyzedin(Bolte&Pauwels,2021). Somekeyproperties
are that: (1) they are almost everywhere single-valued and equal to classical derivatives; (2) for path
differentiable functions, the Clarke subgradient is the minimal conservative derivative; (3) chain rule
holds for conservative derivatives; (4) locally Lipschitz and definable mappings admit conservative
derivatives. We also point out that conservative derivatives are not unique: two valid CDs for F can
be different in sets of zero Lebesgue measure. This accounts for the fact there are in general multiple
ways to express a path differentiable function as a composition of others but applying the chain rule
produces always valid CDs.
Similarly to (Bolte et al., 2022), to address the fact that conservative derivatives are set-valued
mappings, we will use the following quantity to measure the error in the conservative derivative
approximation.
Definition 2.2 (Excess). Let A and B be two bounded subsets of matrices or vectors. The excess2 of
A over B is
e(A,B) := sup inf ∥A−B∥.
A∈AB∈B
Note that ∥A∥ = e(A,{0}). The excess satisfies several properties similar to the ones of a
sup
distance, even though it is not symmetric (see Lemma B.1). Similarly to (Scholtes, 2012) we give the
following definition (which is slightly more general than that given in (Bolte & Pauwels, 2021)).
Definition 2.3. Let F ,...,F : U ⊂ Rp → Rd be continuous mappings defined on a nonempty
1 m
open set U. A continuous selection of F ,...,F is a continuous mapping F: U → Rd such that for
1 m
every x ∈ Rp: F(x) ∈ {F (x),...,F (x)}. In such case the active index set mapping is the set-valued
1 m
mapping I : U ⇒ [m], with I (x) = {i ∈ [m]|F (x) = F(x)}. Moreover, if the F ’s are differentiable
F F i i
we set Ds : U ⊂ Rp ⇒ Rd×p such that
F
Ds(x) = co({F′(x)|i ∈ I (x)}), (4)
F i F
where F′(x) is the classical derivative (Jacobian) of F at x.
i i
2e isreferredin(Bolteetal.,2022)asgap,whilethestandardnameisexcess(Beer,1993,Section1.5).
4Theorem2.4. LetF: U ⊂ Rp → Rd beacontinuousselectionofdefinableandcontinuouslydifferentiable
mappings F ,...,F : U → Rd. Then F is definable if and only if I : Rp ⇒ [m] is definable, and in
1 m F
such case Ds is a conservative derivative of F.
F
We can also define partial conservative derivatives. If p = p 1+p
2
and F: U ⊂ Rp1+p2 → Rd, we
have D F: U ⇒ Rd×(p1+p2) and we set D F,1: U ⇒ Rd×p1 and D F,2: U ⇒ Rd×p2 such that
D (x) =
(cid:8)
A|[A,B] ∈
Ds(x)(cid:9)
and D (x) =
(cid:8)
B|[A,B] ∈
Ds(x)(cid:9)
.
F,1 F F,2 F
Finally, we denote with F′(x) an arbitrary element of the conservative derivative D (x) and by
F
∂ F(x) an arbitrary element of D (x), which correspond to the classical (partial) derivatives if F is
k F,k
differentiable. By building on (Bolte et al., 2022, Lemma 3), we prove the following result (the proof
is in Appendix B).
Lemma 2.5. Let F: U ⊂ Rp → Rd be a continuous definable selection of the definable Lipschitz smooth
mappings F ,...,F : U → Rd. Let L be the Lipschitz constant of F′ and set L = max L . Then
1 m i i 1≤i≤m i
for every x ∈ Rp, there exist R > 0 such that
x
∀x′ ∈ U: e(Ds(x′),Ds(x)) ≤ L (x′)∥x−x′∥, (5)
F F x
where
(cid:40)
L if ∥x−x′∥ ≤ R
L (x′) := x and M := max min ∥F′(x)−F′(x)∥.
x x i j
L+M x/R x otherwise i∈[m]j∈IF(x)
Note that in the smooth case (m = 1), (5) corresponds to global L-smoothness (since M = 0),
x
while in general it is weaker. In particular, the quantity L + Mx is well defined even when F is
Rx
not differentiable at x, but blows up when x approaches a point of non-differentiability, e.g. for
ReLU(x) = max(0,x), lim M /R = ∞.
x→0+ x x
3 Differentiating a Parametric Fixed Point
Instances of Parametric Fixed Point Equations A general class of problems that can be recast in
the form (1) is that of the parametric monotone inclusion problem
0 ∈ A (w)+B (w), (6)
λ λ
where A : Rd ⇒ Rd and B : Rd → Rd are multi-valued and single-valued maximal monotone
λ λ
operators respectively. These types of problems are at the core of convex analysis and can cover a
number of optimizations problems including minimization problems as well as variational inequalities
and saddle points problems. It is a standard fact that (6) can be rewritten as the equation
R (w−γB (w)) = w (γ > 0),
γA λ λ
whereR istheresolventoftheoperatorγA . Thisgivesafixed-pointequationofacompositeform,
γA λ λ
andcomparingwith(2),itisclearthatwecanalsoaddresssituationsinwhichB = E[Bˆ (·,ξ)]. Bolte
λ λ
et al. (2024) investigates conservative derivatives of the solution map of such monotone inclusion
problems in nonsmooth settings.
5A special case of (6) is the minimization problem
minE[fˆ(w,ξ)]+g (w), (7)
λ λ
w
where f = Efˆ(·,ξ) is L-smooth and convex while g is convex lower semicontinuous extended-
λ λ λ
real valued. This can be cast into (2) by setting η ∈ [0,2/L[, Tˆ(w,λ,ξ) = w − η∇ˆf (w,ξ) and
λ
G(w,λ) = Prox (w) with Prox (x) = argmin (h(x)+(1/2)∥x−y∥2) being the proximal operator
ηg λ h y
of h. Several machine learning problems can be written in form (7), e.g. LASSO, elastic net, (dual)
SVM, where g is not smooth.
λ
Main assumptions Referring to problem (1), when Φ is differentiable and ∥∂ Φ(w(λ),λ)∥ ≤ q < 1,
1
by differentiating (1) we have
w′(λ) = ∂ Φ(w(λ),λ)w′(λ)+∂ Φ(w(λ),λ)
1 2
(8)
w′(λ) = (I −∂ Φ(w(λ),λ))−1∂ Φ(w(λ),λ).
1 2
The first relation above shows that w′(λ) ∈ Rd×p is a fixed point of the linear contraction X (cid:55)→
∂ Φ(w(λ),λ)X +∂ Φ(w(λ),λ).
1 2
Here,dealingwiththenonsmoothcase,wewillmimictheaboveformulas. Thefollowingwillbea
crucial assumption of our analysis.
Assumption 3.1. Let O ⊂ Rm be open, with Λ ⊂ O be nonempty closed and convex.
Λ Λ
(i) Φ: Rd×O → Rd is definable and a continuous selection of the L-Lipschitz smooth definable
Λ
mappings Φ ,...,Φ and we set D : Rd×O ⇒ Rd×(d+p),
1 m Φ Λ
D (u,λ) = Ds (u,λ)×Ds (u,λ), (9)
Φ Φ,1 Φ,2
where Ds (u,λ) = co({∂ Φ (u,λ)|i ∈ I (u,λ)}), with k = 1,2, are the partial conservative
Φ,k k i Φ
derivatives.
(ii) For every (u,λ) ∈ Rp×O , ∥D (u,λ)∥ ≤ q < 1.
Λ Φ,1 sup
The results in the previous section and Lemma 3 and Lemma 4 in (Bolte & Pauwels, 2021), ensure
that D , as defined in (9), is a conservative derivative of Φ. Moreover, recalling (3), it is easy to see
Φ
that item (ii) in the above assumption ensures that Φ(·,λ) is a q-contraction and hence that there
exists a unique fixed point of Φ(·,λ) that we will denote by w(λ). Finally, if A ∈ D (u,λ), we have
Φ,1
∥A∥ < 1 and hence I −A is invertible. Thus, mimicking what happens for the smooth case (see (8))
one defines
Dimp(λ) = (I −D (w(λ),λ))−1D (w(λ),λ) (10)
w Φ,1 Φ,2
Dfix: λ ⇒ fix[D (w(λ),λ)], (11)
w Φ
where fix[D (u,λ)] is the unique fixed “point” of the map X (cid:55)→ D (u,λ)X + D (u,λ), acting
Φ Φ,1 Φ,2
betweencompactsetsofd×pmatrices. In(Bolteetal.,2022)itisprovedthatifΦispathdifferentiable
andAssumption3.1(ii)holds,theset-valuedmappingsDimp andDfix arebothconservativederivatives
w w
of w(λ) and Dimp(λ) ⊂ Dfix(λ). We note that the product structure (9) of D allows us to express
w w Φ
Dimp and Dfix, originally introduced in (Bolte et al., 2022), in terms of Minkowski set operations.
w w
This will facilitate deriving convergence rates.
Assumption 3.1 yields the following lemma through an application of Lemma 2.5 (the proof is in
Appendix B).
6Lemma 3.2. Under Assumption 3.1(i), for every λ ∈ Λ, there exist R > 0 such that for k = 1,2
λ
∀u ∈ Rd: e(D (u,λ),D (w(λ),λ)) ≤ C (u)∥u−w(λ)∥,
Φ,k Φ,k λ
where
(cid:40)
L if ∥u−w(λ)∥ ≤ R
λ
C (u) := (12)
λ
L+M /R otherwise
λ λ
and M := max min ∥Φ′(w(λ),λ)−Φ′(w(λ),λ)∥.
λ i j
i∈[m]j∈I(w(λ),λ)
Lemma 3.2 can be used as a substitute for the Lipschitz smoothness of Φ with respect to the first
variable, indeed note that in our analysis λ (and hence w(λ)) is fixed.
4 Deterministic Iterative and Approximate Implicit Differentiation
We now formalize two deterministic methods for approximating the conservative derivative of the
solution map w.
Iterative Differentiation (ITD) This method approximates Dfix(λ) through the iterative procedure
w
for t = 1,2...
(cid:36)
w t(λ) = Φ(w t−1(λ),λ) (13)
D (λ) = D (w (λ),λ)D (λ)+D (w (λ),λ).
wt Φ,1 t−1 wt−1 Φ,2 t−1
NotethattheiterationforD (λ)isbasedonthechainruleandresultsinaconservativederivativeof
wt
w (λ). UndertheproductstructureofD in(9),thisisthesameset-valuediterationstudiedin(Bolte
t Φ
et al., 2022). We note that if Φ(·,λ) is a q-contraction, it holds ∥w (λ)−w(λ)∥ = O(qt).
t
Approximate Implicit Differentiation with Fixed Point (AID-FP) An alternative method for
approximating the implicit conservative derivativeis the following. Assume that w (λ) is generated by
t
any algorithm converging to w(λ) (for instance the one in (13)), then, starting from D0 (λ) = {0},
wt
define
for k = 1,2...
(cid:4) Dk (λ) = D (w (λ),λ)Dk−1(λ)+D (w (λ),λ). (14)
wt Φ,1 t wt Φ,2 t
Efficient Implementation Let x ∈ Rm and y ∈ Rd. The ITD method exploits automatic differenti-
ation to efficiently compute an element of the conservative Jacobian-vector products D (λ)⊤y (in
wt
reverse mode) and D (λ)x (in forward mode). Similarly AID efficiently computes an element in
wt
Dk (λ)⊤y. Thanks to Automatic Differentiation, if k = t the standard implementation of both AID-FP
wt
and ITD has a cost in time of the same order of that of computing w (λ). However, while AID-FP
t
only uses w (λ), ITD has a larger Θ(t) memory cost , since it needs to store the entire optimization
t
trajectory (w (λ)) .
i 0≤i≤t
7Convergence Guarantees In the Lipschitz smooth case Grazzi et al. (2020) proved non-asymptotic
linear convergence rates for both methods, revealing that AID-FP is slightly faster than ITD. We now
extend this analysis to nonsmooth ITD and AID-FP, focusing on the convergence of the set-valued
iterations in (13) and (14). Thanks to Lemma 3.2 and the properties of the excess, the proof (in
Appendix C) can proceed similarly to that given in (Grazzi et al., 2020) for the smooth case.
Theorem 4.1 (nonsmooth ITD and AID-FP Rates). Let Assumption 3.1 hold. For every λ ∈ Λ, let R
λ
and M be the quantities defined in Lemma 3.2 and B := ∥D (w(λ),λ)∥ . For every t,k ∈ N, let
λ λ Φ,2 sup
∆ = ∥w (λ)−w(λ)∥, E (t) := 1{∆ > R } and E¯ (t) = t−1(cid:80)t−1E (i). Then the following hold.
t t λ t λ λ i=0 λ
(i) If D (λ) = {0}, the ITD iteration in (13) satisfies
w0
B B +1(cid:16) M (cid:17)
e(D (λ),Dfix(λ)) ≤ λ qt+ λ L+ λ E¯ (t) ∆ tqt−1. (15)
wt w 1−q 1−q R λ 0
λ
(ii) The AID-FP iteration in (14) satisfies
B B +1(cid:16) M (cid:17)1−qk
e(Dk (λ),Dfix(λ)) ≤ λ qk + λ L+ λ E (t) ∆ . (16)
wt w 1−q 1−q R λ 1−q t
λ
Moreover, if w (λ) = Φ(w (λ),λ), then ∆ ≤ q∆ ≤ qt∆ and there exists τ ∈ N such that
t t−1 t t−1 0 λ
E(t) = 1{t < τ } and thus E (t) ≤ E¯ (t) ≤ 1.
λ λ λ
To compare the two rates in Theorem 4.1, let t = k and w (λ) = Φ(w (λ),λ), so that both
t t−1
AID-FP and ITD have time complexity of the order of computing w (λ). In that situation, since
t
1−qk = 1−qt < q−1(1−q)tandE (t) ≤ E¯ (t),theupperboundofAID-FPisalwayslowerthanthat
λ λ
of ITD. Moreover, if we let κ = (1−q)−1 to play a similar role to the condition number, we observe
that both methods converge linearly: AID-FP as O(κ2e−t/κ), while ITD slightly slower as O(κte−t/κ).
When t ≥ τ , E (t) = 0 while E¯ (t) = τ /t, which might cause a wide difference between the two
λ λ λ λ
bounds if M /R is large, and such ratio can get arbitrarily large the closer (w(λ),λ) is to regions
λ λ
where Φ is not differentiable. Finally, if we replace Lemma 3.2 with the L-smoothness of Φ, we
essentially recover the same bounds reported by Grazzi et al. (2020), where the terms E ,E¯ do not
λ λ
appear.
Note that since the iterations (13) and (14) are set-valued, our analysis works even if the element
of the conservative derivative selected at each iteration of AID-FP and ITD changes from one iteration
to the other. However, in practice this is rarely the case, instead the same element Φ′ ∈ D is used
Φ
at every iteration. This choice yields that both AID-FP and ITD are guaranteed to converge, up to a
subsequence, to an element of Dfix(λ).
w
5 Nonsmooth Stochastic Implicit Differentiation
In this section we study the stochastic fixed point formulation in (2) and present an algorithm that,
givenarandomvectory ∈ Rd andanapproximatesolutionw (λ),efficientlyapproximatesanelement
t
of Dimp(λ)⊤y accessing only Tˆ , G and their conservative derivatives. Similarly to the deterministic
w
case of AID, here we assume that w (λ) is generated by a stochastic algorithm which converges in
t
mean square to w(λ). Several algorithms can ensure such convergence guarantees for the composite
minimization problems in (7) (e.g, Rosasco et al. (2020) provide a proximal stochastic gradient
algorithm with rate O(1/t)) and composite monotone inclusions (Rosasco et al., 2014).
We consider the following assumptions
8Assumption 5.1.
(i) T and G satisfy Assumption 3.1(i) individually, with constant L and L respectively. Let
T G
also T′ and G′ be selections of the conservative derivatives D and D respectively. Also,
T G
Φ(u,λ) = G(T(u,λ),λ).
(ii) For every (u,λ) ∈ Rd × Λ, ∥D (u,λ)∥ ≤ 1 and ∥D (u,λ)∥ ≤ 1 and either T or G
T,1 sup G,1 sup
satisfies Assumption 3.1(ii).
(iii) y ∈ Rd is a random vector.
Assumption 5.2.
(i) Tˆ: Rd×O ×Ξ → Rd, ξ is a random variable with value in Ξ and E[Tˆ(u,λ,ξ)] = T(u,λ).
Λ
(ii) For every x ∈ Ξ, Tˆ(·,·,x) is path differentiable and
∂ Tˆ: Rd×O ×Ξ → Rd×d, ∂ Tˆ: Rd×O ×Ξ → Rd×p
1 Λ 1 Λ
are selections of D and D respectively and there exist σ ,σ ,σ′,σ′ ≥ 0 such that for every
Tˆ,1 Tˆ,2 1 2 1 2
u ∈ Rd and λ ∈ Λ
E[∂ Tˆ(u,λ,ξ)] = ∂ T(u,λ) ∈ D (u,λ), E[∂ Tˆ(u,λ,ξ)] = ∂ T(u,λ) ∈ D (u,λ),
1 1 T,1 2 2 T,2
Var[Tˆ(u,λ,ξ)] ≤ σ +σ ∥u−T(u,λ)∥2, Var[∂ Tˆ(u,λ,ξ)] ≤ σ′, Var[∂ Tˆ(u,λ,ξ)] ≤ σ′.
1 2 1 1 2 2
In view of the chain rule for conservative derivatives Assumption 5.1 ensures that Φ is path
differentiable and ∥D (u,λ)∥ ≤ q < 1. Thus, w(λ) is well defined and it has conservative
Φ,1 sup
derivatives Dimp and Dfix. Assumption 5.2 is a nonsmooth generalization of the corresponding one in
w w
(Grazzi et al., 2021, 2023). Finally, recalling (10), we note that
∂ Φ(w(λ),λ)⊤v(w(λ),λ) ∈ Dimp(λ)⊤y (17)
2 w
where, for every u ∈ Rd, v(u,λ) is a solution of the linear system
(I −∂ T(u,λ)⊤∂ G(T(u,λ),λ)⊤)v = y. (18)
1 1
Algorithm and convergence guarantees Our method is inspired by (17) and (18) but it uses mini-
batch estimators of T and ∂ Φ. To that purpose we assume to have two independent sets of samples
2
ξ(1) = (ξ(1) ) and ξ(2) = (ξ(2) ) , being i.i.d. copies of the random variable ξ. Moreover, we
j 1≤j≤J i 1≤i≤k
define
J
T¯(u,λ)=1 (cid:88) Tˆ(u,λ,ξ(1) ), Φ¯(u,λ) = G(T¯(u,λ),λ).
J j
j=1
The function
Φ¯
is nonsmooth and we can define its partial conservative derivatives as
D (u,λ) =D (T¯(u,λ),λ)D (u,λ)
Φ¯,1 G,1 T¯,1
D (u,λ) =D (T¯(u,λ),λ)D (u,λ)+D (T¯(u,λ)).
Φ¯,2 G,1 T¯,2 G,2
9In fact our approach replaces the linear system (18) with
(I −∂ T(w (λ),λ)⊤∂ G(T¯(w (λ),λ),λ)⊤)v = y, (19)
1 t 1 t
where the solution is in turn approximated by a stochastic sequence (v k) k∈N, which has access only to
Tˆ,G, and w (λ).
t
We first provide a general bound for the mean square error of an estimator of an element of the
Jacobian vector product
Dimp(λ)⊤y,
which is agnostic with respect to the algorithms solving the fixed
w
point equation (1) and the linear system (19). The proof (in Appendix D) uses similar techniques as
the one for the smooth case in (Grazzi et al., 2021, 2023).
Assumption5.3. Letρ : N → R ,σ : N → R besuchthatlim ρ (t) = 0,lim σ (k) = 0.
λ + λ + t→+∞ λ k→+∞ λ
(i) (w t(λ)) t∈N is a sequence of random vectors in Rd and
E[∥w (λ)−w(λ)∥2] ≤ ρ (t),
t λ
(ii) For every (u 1,u 2) ∈ Rd ×Rd, (v k(u 1,u 2)) k∈N is a sequence of random vectors in Rd which is
independent on (w t(λ)) t∈N and such that
E[∥v (u ,u )−v¯(u ,u )∥2|y] ≤ ∥y∥2σ (k),
k 1 2 1 2 λ
wherev¯(u ,u )istheuniquefixedpointoftheaffinemappingv (cid:55)→ ∂ T(u ,λ)⊤∂ G(u ,λ)⊤v+y.
1 2 1 1 1 2
(iii) The r.v. y satisfies E[∥y∥2|w (λ)] ≤ B2 a.s.
t
Theorem 5.4. Under Assumption 5.1, 5.2, and 5.3, let κ = (1−q)−1. We define the estimator
(w′(λ)⊤y)∧ := ∂ Φ¯(w (λ),λ)⊤v (w (λ),T¯(w (λ),λ)).
2 t k t t
Then for every t,k,J ∈ N, we have
(cid:18) (cid:18) (cid:19)(cid:19)
E(cid:2) e(cid:0) (w′(λ)⊤y)∧,Dimp(λ)⊤y(cid:1)2(cid:3) = B2×O σ (k)+κ4 1 +ρ (t) .
w λ J λ
We preset the full procedure, named nonsmooth stochastic implicit differentiation (NSID), in
Algorithm 1, where the sequence v considered in Assumption 5.3(ii) is generated by a simple
k
stochastic fixed-point iteration algorithm (described in (Grazzi et al., 2021)) with step sizes (η ) .
i 1≤i≤k
Algorithm 1: NSID
1: Input: k,J ∈ N, w t(λ),y ∈ Rd, Tˆ , G, ξ(1),ξ(2)
2: T¯ t(λ) ← T¯(w t(λ),λ) (usingξ(1))
3: Ψˆ: (v,x) (cid:55)→ ∂ 1Tˆ(w t(λ),λ,x)⊤∂ 1G(T¯ t(λ),λ)⊤v+y
4: for i = 1 to k do
5: v i ← (1−η i)v i−1+η iΨˆ(v i−1,ξ i(2) )
6: end for
7: Return (w′(λ)⊤y)∧ := ∂ 2Φ¯(w t(λ),λ)⊤v k
Note that all steps can be efficiently implemented via automatic differentiation by using only
vector-valued function evaluations and conservative Jacobian-vector products without the expensive
10computation of the full matrix derivatives. Furthermore, using a fixed selection for the conservative
derivative corresponds to the standard implementation.
If G(·,λ) is the identity and T is smooth, we recover exactly the same procedure given in (Grazzi
et al., 2021, 2023). We stress that handling a general G provides an additional challenge since we
do not have access anymore to an unbiased estimator of Φ. However, we could overcome this issue
by using different samples sequences for the two factors occuring in
Ψˆ
. Incidentally, one of those
can be the one used to compute a mini-batch estimator of ∂Φ . Ultimately, this does not call for any
2
additional samples compared to the smooth version.
Finally, we specialize the result of Theorem 5.4 to Algorithm 1. The proof is in Appendix D.
Theorem 5.5. Under Assumption 5.1, 5.2, and 5.3(i)(iii), let (w′(λ)⊤y)∧ be generated by Algorithm 1
with η = Θ(i−1) and assume that ρ (t) = O(καt−1), with α > 0. Then
i λ
E(cid:2) e(cid:0) (w′(λ)⊤y)∧,Dimp(λ)⊤y(cid:1)2(cid:3)
=
O(cid:18) κ5
+
κ4
+
κ4+α(cid:19)
.
w k J t
Hence if J = O(t), k = O(t), the mean square error is ≤ ϵ after O(κ5+αϵ−1) samples.
Note that the sample complexity O(ϵ−1) matches the performance of SGD for minimizing strongly
convex and Lipschitz smooth functions (Bottou et al., 2018), which are a special cases of Problem
(2). Furthermore it is the same one that the SID algorithm by Grazzi et al. (2020, 2023) attains when
G(v,λ) = v and Φ is Lipschitz smooth. A limitation is the choice of step-sizes (η ), problematic in
i
practice.
6 Application to Bilevel Optimization
In this section, we consider the following bilevel problem with the fixed point problem in (1) at the
lower level
min{E(w(λ),λ) : w(λ) = Φ(w(λ),λ)}, (20)
λ∈Λ
where E: Rd ×O → R. We will show how we can use AID-FP, ITD and NSID to approximate an
Λ
element of the conservative derivative of the bilevel objective f(λ) := E(w(λ),λ) and retain the same
convergence rates.
First, note that if E and Φ are path differentiable with conservative derivatives D and D
E Φ
respectively and if ∥D (u,λ)∥ ≤ q < 1 for every u ∈ Rd, λ ∈ Λ, then
Φ sup
Dimp(λ):=Dimp(λ)⊤D (w(λ),λ)+D (w(λ),λ)
f w E,1 E,2
Dfix(λ):=Dfix(λ)⊤D (w(λ),λ)+D (w(λ),λ)
f w E,1 E,2
are conservative derivatives for f. We further assume that E satisfies Assumption 3.1(i) with corre-
sponding derivative D . Let also set f (λ) := E(w (λ),λ), where w (λ) is an approximate solution for
E t t t
the fixed point problem. In the following we let z = (w (λ),λ) for brevity.
t t
Deterministic Case Bilevel AID-FP (BAID-FP) and Bilevel ITD (BITD) approximate D by setting
f
t,k ∈ N, and returning an element of the sets
(BITD) D (λ):=D (λ)⊤D (z )+D (z )
ft wt E,1 t E,2 t
(BAID-FP) Dk (λ):=Dk (λ)⊤D (z )+D (z )
ft wt E,1 t E,2 t
11This guarantees the same convergence rates, this time to Dfix(z), as ITD and AID (Theorem E.3).
f
Stochastic Case In the stochastic case, the bilevel problem takes the form
minf(λ) := E[Eˆ(w(λ),λ,ζ)],
λ∈Λ (21)
w(λ) =
G(cid:0)E[Tˆ(w(λ),λ,ξ)],λ(cid:1)
.
where ζ is a random variable. We consider Algorithm 2, which additionally computes a minibatch
gradient estimator of E′ ∈ D , using the sequence ζ(1) = (ζ(1)) of i.i.d. copies of ζ.
E 1≤i≤J1
Algorithm 2: NSID-Bilevel
1: Input: k,J 1,J 2, w t(λ), Eˆ , Tˆ , G, ξ(1),ξ(2),ζ(1)
2: E¯′(z t) ← J 1−1(cid:80)J i=1 1Eˆ′(z t,ζ i(1) )
3: r(z t)←NSID(k,J 2,w t(λ),∂ 1E¯(z t),Tˆ,G,ξ(1),ξ(2))
4: Return ∇ˆf(λ) := r(z t)+∂ 2E¯(z t)
With additional mild assumptions on the variance of Eˆ and when E(·,λ) is Lipschitz, we recover
the same convergence rates as NSID, but this time to
Dimp(λ)
(Theorem E.6).
f
On the convergence of the bilevel problem Despite these encouraging results and the fact that
in the smooth case several works provide convergence rates to a stationary point of the gradient of
f (Ji et al., 2021; Arbel & Mairal, 2021; Grazzi et al., 2023), proving such type of results or even
asymptotic convergence (without rates) in our nonsmooth case is more challenging and we leave it
forfuturework. Onecrucialissueisthatintheanalysis,theconstantdefinedinLemma2.5,whichwe
useinplaceofthatofLipschitzsmoothness, cannotbeproperlycontrolledonthewholeΛasrequired
in the smooth case: it becomes arbitrarily large when (w(λ),λ) approaches non-differentiable regions
of Φ.
7 Experiments
The experiments aim to achieve two primary goals. Firstly, we aim to empirically demonstrate the
practical manifestation of distinct behaviors between AID and ITD, as outlined in the theoretical
findings of Section 4. Emphasis is placed on aspects specific to the nonsmooth analysis. Secondly, we
intend to evaluate the empirical performance of our stochastic method NSID presented in Algorithm
1. We implement NSID by relying on PyTorch automatic differentiation for the computation of
Jacobian-vector products. For AID and ITD, we use the existing PyTorch implementations3.
Experimental Setup We consider two problems where we are interested in approximating an
element of the conservative Jacobian-vector product of the solution map Dfix(λ)⊤y for y ∈ Rd. With a
w
focus on bilevel optimization, we set y as the gradient of the validation loss in w (λ), as explained in
t
Section 6, while to compute the approximation error we use the procedure described in Appendix F.1.
Elastic Net Let (X,y) ∈ Rn×p×Rn be a training regression dataset. The elastic net solution w(λ) is
3https://github.com/prolearner/hypertorch
12=0.002, =0.002 =0.002, =0.02
1 2 1 2
103
102 101
101
10 1
100
10 3
10 1 ITD ITD
AID-FP (k=t) 10 5 AID-FP (k=t)
10 2 AID-CG (k=t) AID-CG (k=t)
0 500 1000 0 100 200 300
t t
Figure 1: AID vs ITD for synthetic elastic-net. t corresponds to the number of steps to find an
approximate solution to the fixed point and the dashed vertical line is the step where the support is
identified. AID-FP converges faster than ITD; note that after support identification there is a wide gap
between the methods, as anticipated by our theoretical bounds. AID-CG does not converge in plot on
the right, probably due to sensitivity to numerical errors.
the minimizer of the objective function 1∥Xw−y∥2+ λ1∥w∥2+λ ∥w∥ , where λ = (λ ,λ ) are the
n 2 2 2 1 1 2
regularization hyperparameters.
Data Poisoning We consider a data poisoning scenario similar to the one in (Xiao et al., 2015), where
an attacker would like to corrupt part of the training dataset by adding noise in order to decrease the
accuracyofanElastic-netregularizedlogisticregressionmodelaftertraining. Inparticular,letcbethe
number of classes and (X˜,y˜) ∈ Rn′×p ×[c]n′ be the examples to corrupt while (X,y) ∈ Rn×p ×[c]n
are the clean ones. Let also Γ ∈ Rn′×p represent the noise and define the data poisoning elastic net
solutionasw(Γ) = argmin w∈Rdf(Γ,w)+λ 21∥w∥ 2+λ 2∥w∥ 1,wheref(Γ,w) = ℓ(Xw,y)+ℓ((X˜+Γ)w,y˜)
and ℓ is the cross-entropy loss. A strategy to find Γ would be by approximating an element of the
conservative Jacobian-vector product D (Γ)⊤y where y is the gradient of the cross-entropy loss on an
w
hold out set. This setting is of particular interest, since Γ is high dimensional and hence zero-order
methodslikegridorrandomsearcharelessappropriate. Forbothsettingsandallconsideredmethods,
we find an approximate solution w (λ) always by iterating the contraction map which describes the
t
iterates of the deterministic iterative soft-thresholding algorithm (see e.g. (Combettes & Wajs, 2005)).
Although this may be inefficient in the stochastic setup, it yields a fairer comparison, since both the
stochastic and deterministic algorithms will have the same w (λ) as input. Additional details are in
t
the appendix.
AID and ITD We consider the Elastic Net scenario and construct a synthetic supervised linear
regression problem with 500 examples and 100 features, of which 30 are informative. As the fixed
point map Φ we use one step of iterative soft-thresholding. The appropriate choice for the step-size
guarantees that Φ is a contraction, in our case we set it equal to 2/(L+µ+2λ ), where L and µ are
1
the largest and smallest eigenvalues values of n−1X⊤X.
We compare ITD, AID-FP, and AID-CG a variant of AID which uses conjugate gradient to solve the
13
rorre
noitamixorppA
rorre
noitamixorppA=(0.1,0.01) Data Poisoning. =(0.02,0.1)
AID-FP AID-FP
NSID dec NSID dec
SID dec NSID const
100
10 1
10 4
10 2
0 20 40 60 0 50 100 150 200
# of epochs # of epochs
Figure 2: Stochastic implicit differentiation for elastic net (left) and data poisoning (right). Mean
(solid line) and the geometric standard deviation (shaded region) of the approximation error over 10
runs. SID does not converge on elastic net for a specific choice of λ and diverges in data poisoning
(hence we do not report it), while NSID converges faster (at the beginning) than the Deterministic
AID-FP. Note that decreasing step-sizes provide a favorable choice.
linear system (Grazzi et al., 2020), where the vector y for the Jacobian-vector product is the gradient
of thesquare loss on avalidation set, computed on thet-the iterate(∇E(w (λ),λ) where E is defined
t
in (20)). In Figure 1 we can see two runs, each one for two particular choices of λ which highlight a
wide gap in performance after support identification, i.e. when both w (λ) and w(λ) have the same
t
non-zero elements. This was predicted by Theorem 4.1, since support identification coincides with
∥w (λ)−w(λ)∥ ≤ R .
t λ
Stochastic Methods WecompareourstochasticmethodNSID(Algorithm1)againstAID-FPandthe
algorithm SID in (Grazzi et al., 2023). In particular, for NSID
Tˆ
corresponds to one step of gradient
descent on a minibatch of training points, while G is soft-thresholding. We implement SID by setting
in NSID G(u,λ) = u and using Φˆ(u,λ,ξ) = G(Tˆ(u,λ,ξ),λ) in place of Tˆ . Note that although the
theoretical convergence guarantee for SID do not hold due to
Φˆ
being biased, the performance of SID
still effectively measures the impact of such bias in practice.
Weconsiderboththeelasticnetandthedatapoisoningsetups;seetheappendixformoreinformation.
The results are shown in Figure 2. For elastic net, each run corresponds to a different sampling of
the covariance matrix, training points, true solution vector and minibatches used by the stochastic
algorithms. For Data poisoning, each run corresponds to different sampling of the noise Γ (sampled
from a normal and then each component projected in [−.1,.1]) and the mini-batches used by the
stochastic algorithms. For AID-FP, each epoch corresponds to one iteration, since it uses the entire
dataset, while for NSID and SID the number of epochs is equal to (k+J)(n′+n)/b, where b is the
minibatch size, which we set to 10% of the training set, i.e. b = (n′+n)/10. Note that for each point
in the plots for NSID and SID, we need to start the algorithm from scratch since we increase both k
and J simultaneously. In particular we set k = J for elastic net and J = ⌈k/20⌉ for data poisoning.
148 Conclusions
We established convergence guarantees for nonsmooth implicit differentiation methods. Leveraging
the foundation laid by (Bolte et al., 2022), we developed tools facilitating the translation of results
from the smooth case. Specifically, we provided non-asymptotic linear convergence rates for AID-FP
and ITD, focusing on deviations from their smooth analogs. Additionally, we introduced NSID, a
principled stochastic algorithm. Numerical experiments underscored the distinctive behaviors of
AID-FP and ITD, along with the good performance of NSID, which may be useful in large scale bilevel
optimization problems in the future.
References
Amos, B. and Kolter, J. Z. Optnet: Differentiable optimization as a layer in neural networks. In
International Conference on Machine Learning, pp. 136–145. PMLR, 2017.
Arbel, M. and Mairal, J. Amortized implicit differentiation for stochastic bilevel optimization. In
International Conference on Learning Representations, 2021.
Bai,S.,Kolter,J.Z.,andKoltun,V. Deepequilibriummodels. AdvancesinNeuralInformationProcessing
Systems, 32, 2019.
Beer, G. Topologies on Closed and Closed Convex Sets, volume 268. Springer Science & Business Media,
1993.
Bertinetto, L., Henriques, J., Torr, P., and Vedaldi, A. Meta-learning with differentiable closed-form
solvers. In International Conference on Learning Representations (ICLR), 2019, 2019.
Bertrand, Q., Klopfenstein, Q., Blondel, M., Vaiter, S., Gramfort, A., and Salmon, J. Implicit differenti-
ation of lasso-type models for hyperparameter optimization. In International Conference on Machine
Learning, pp. 810–821. PMLR, 2020.
Bertrand, Q., Klopfenstein, Q., Massias, M., Blondel, M., Vaiter, S., Gramfort, A., and Salmon, J.
Implicit differentiation for fast hyperparameter selection in non-smooth convex learning. The
Journal of Machine Learning Research, 23(1):6680–6722, 2022.
Blondel, M., Berthet, Q., Cuturi, M., Frostig, R., Hoyer, S., Llinares-Lo´pez, F., Pedregosa, F., and
Vert, J.-P. Efficient and modular implicit differentiation. Advances in Neural Information Processing
Systems, 35:5230–5242, 2022.
Bolte, J. and Pauwels, E. Conservative set valued fields, automatic differentiation, stochastic gradient
methods and deep learning. Mathematical Programming, 188:19–51, 2021.
Bolte, J., Pauwels, E., and Vaiter, S. Automatic differentiation of nonsmooth iterative algorithms.
Advances in Neural Information Processing Systems, 35:26404–26417, 2022.
Bolte,J.,Pauwels,E.,andSilveti-Falls,A. Differentiatingnonsmoothsolutionstoparametricmonotone
inclusion problems. SIAM Journal of Optimization, 34:71–97, 2024.
Bottou, L.,Curtis, F.E., andNocedal,J. Optimizationmethodsforlarge-scalemachinelearning. SIAM
review, 60(2):223–311, 2018.
15Chen, T., Sun, Y., and Yin, W. Closing the gap: Tighter analysis of alternating stochastic gradient
methods for bilevel problems. Advances in Neural Information Processing Systems, 34:25294–25307,
2021.
Combettes, P. L. and Wajs, V. R. Signal recovery by proximal forward-backward splitting. Multiscale
Modeling & Simulation, 4(4):1168–1200, 2005.
Franceschi, L., Donini, M., Frasconi, P., and Pontil, M. Forward and reverse gradient-based hyper-
parameter optimization. In International Conference on Machine Learning, pp. 1165–1173. PMLR,
2017.
Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., and Pontil, M. Bilevel programming for hyper-
parameter optimization and meta-learning. In International Conference on Machine Learning, pp.
1568–1577. PMLR, 2018.
Frecon, J., Salzo, S., and Pontil, M. Bilevel learning of the group lasso structure. Advances in Neural
Information Processing Systems, 31, 2018.
Ghadimi, S. and Wang, M. Approximation methods for bilevel programming. arXiv preprint
arXiv:1802.02246, 2018.
Grazzi, R., Franceschi, L., Pontil, M., and Salzo, S. On the iteration complexity of hypergradient
computation. In International Conference on Machine Learning, pp. 3748–3758. PMLR, 2020.
Grazzi, R., Pontil, M., and Salzo, S. Convergence properties of stochastic hypergradients. In Interna-
tional Conference on Artificial Intelligence and Statistics, pp. 3826–3834. PMLR, 2021.
Grazzi, R., Pontil, M., and Salzo, S. Bilevel optimization with a lower-level contraction: Optimal
sample complexity without warm-start. Journal of Machine Learning Research, 24(167):1–37, 2023.
Griewank,A.andWalther,A. EvaluatingDerivatives: PrinciplesandTechniquesofAlgorithmicDifferenti-
ation, volume 105. SIAM, 2008.
Ji, K., Yang, J., and Liang, Y. Bilevel optimization: Convergence analysis and enhanced design. In
International Conference on Machine Learning, pp. 4882–4892. PMLR, 2021.
Khanduri, P., Tsaknakis, I., Zhang, Y., Liu, J., Liu, S., Zhang, J., and Hong, M. Linearly constrained
bileveloptimization: Asmoothedimplicitgradientapproach. InInternationalConferenceonMachine
Learning, pp. 16291–16325. PMLR, 2023.
Lee, K., Maji, S., Ravichandran, A., and Soatto, S. Meta-learning with differentiable convex optimiza-
tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
10657–10665, 2019.
Liu, Y. and Liu, R. Boml: A modularized bilevel optimization library in python for meta learning. In
2021 IEEE International Conference on Multimedia & Expo Workshops (ICMEW), pp. 1–2. IEEE, 2021.
Lorraine, J., Vicol, P., and Duvenaud, D. Optimizing millions of hyperparameters by implicit differen-
tiation. In International Conference on Artificial Intelligence and Statistics, pp. 1540–1552. PMLR,
2020.
Maclaurin, D., Duvenaud, D., and Adams, R. Gradient-based hyperparameter optimization through
reversible learning. In International Conference on Machine Learning, pp. 2113–2122. PMLR, 2015.
16Mun˜oz-Gonza´lez, L., Biggio, B., Demontis, A., Paudice, A., Wongrassamee, V., Lupu, E. C., and Roli, F.
Towards poisoning of deep learning algorithms with back-gradient optimization. In ACM Workshop
on Artificial Intelligence and Security, pp. 27–38, 2017.
Ochs, P., Ranftl, R., Brox, T., and Pock, T. Bilevel optimization with nonsmooth lower level problems.
InScaleSpaceandVariationalMethodsinComputerVision: 5thInternationalConference,pp.654–665.
Springer, 2015.
Pedregosa,F. Hyperparameteroptimizationwithapproximategradient. InInternationalConferenceon
Machine Learning, pp. 737–746, 2016.
Rajeswaran,A.,Finn,C.,Kakade,S.M.,andLevine,S. Meta-learningwithimplicitgradients. Advances
in Neural Information Processing Systems, 32, 2019.
Rosasco, L., Villa, S., and Vu˜, B. C. A stochastic forward-backward splitting method for solving
monotone inclusions in hilbert spaces. arXiv preprint arXiv:1403.7999, 2014.
Rosasco, L., Villa, S., and Vu˜, B. C. Convergence of stochastic proximal gradient algorithm. Applied
Mathematics & Optimization, 82:891–917, 2020.
Scholtes, S. Introduction to Piecewise Differentiable Equations. Springer Science & Business Media,
2012.
Xiao, H., Biggio, B., Brown, G., Fumera, G., Eckert, C., and Roli, F. Is feature selection secure against
training data poisoning? In International Conference on Machine Learning, pp. 1689–1698. PMLR,
2015.
Xiao, Q., Shen, H., Yin, W., and Chen, T. Alternating projected sgd for equality-constrained bilevel
optimization. In International Conference on Artificial Intelligence and Statistics, pp. 987–1023.
PMLR, 2023.
17Appendices
This supplementary material is organized as follows. In App. A we recall the notion of definable
mappings. App. B gives some auxiliary results and proof of lemmas in the main body. In App. C we
present the proof of Theorem 4.1. App. D gives the proof of Theorem 5.4. In App. E we address
bilevel optimization. Finally, App. F contains more information on the numerical experiments.
A Definable Mappings
The concept of definable sets and functions is part of the so called tame geometry. Here we give
just a very brief account (additional details can be found in (Bolte & Pauwels, 2021)). An o-minimal
structure on (R,+,·) (‘o’ stands for ’ordinal’) is a collection of sets O = (O p) p∈N, where each O
p
is a
Boolean algebra4 on Rp containing the algebraic sets (zeros of polynomial functions in p variables),
which enjoys also additional compatibility properties. Subsets of Rp which belongs to an o-minimal
structure O are called definable in O and set-valued mappings F: Rd ⇒ Rp are said definable in O if
their graphs (as a subset of Rd+p) is definable in O.
There are several examples of o-minimal structures. The smallest one is that of real semialgebraic
sets, meaning finite unions of sets which are solutions of a system of polynomial equations and
inequalities. Here we consider the larger class of log−exp structure, which additionally contains the
graph of the exponential function and includes most of the functions considered in machine learning,
includingdeeplearning. So,inthispaperdefinableismeanttobedefinableinthelog−expo-minimal
structure.
B Auxiliary Lemmas
Lemma B.1 (Properties of the excess). Let A,B,A′,B′ ⊂ Rn×p and C ⊂ Rd×n, D ⊂ Rp×d be nonempty
sets of matrices. The following hold true:
(i) e(A,C) ≤ e(A,B)+e(B,C)
(ii) e(A+A′,B+B′) ≤ e(A,B)+e(A′,B′)
(iii) e(C ·A,C ·B) ≤ ∥C∥ e(A,B) and e(A·D,B·D) ≤ ∥D∥ e(A,B)
sup sup
(iv) If B ⊂ B′, then e(A,B′) ≤ e(A,B).
(v) Suppose that n = p and that all the elements in A and B are invertible. Then
e(A−1,B−1) ≤ ∥A−1∥ ∥B−1∥ e(A,B).
sup sup
(vi) Suppose that p = p +p and set, for k = 1,2
1 2
A = pr (A) = {A ∈ Rn×p1|[A ,A ] ∈ A}
k k k 1 2
B = pr (A) = {B ∈ Rn×p1|[B ,B ] ∈ A},
k k k 1 2
where pr : Rd×p → Rd×p1 and pr : Rd×p → Rd×p2 are the canonical projections. Then
1 2
e(A ,B ) ≤ e(A,B).
k k
4meaninganonemptyfamilyofsubsetofRpwhichisstablebycomplementationsandfiniteunionsandintersections.
18Proof. InthefollowingwhenAisamatrixandB isasetofmatriceswesetd(A,B) := inf ∥A−B∥,
B∈B
which is the distance from A to the set B.
(i): Let A ∈ A and B ∈ B. Then
(∀C ∈ C) d(A,C) ≤ ∥A−C∥ ≤ ∥A−B∥+∥B−C∥
=⇒ d(A,C)−∥A−B∥ ≤ ∥B−C∥.
Thus
d(A,C)−∥A−B∥ ≤ d(B,C) ≤ e(B,C)
and hence
(∀B ∈ B) d(A,C)−e(B,C) ≤ ∥A−B∥.
So, d(A,C)−e(B,C) ≤ d(A,B) ≤ e(A,B) =⇒ d(A,C) ≤ e(B,C)+d(A,B). Taking the sup in A ∈ A
the statement follows.
(ii): Let A ∈ A,A′ ∈ A. Then,
(∀B ∈ B)(∀B′ ∈ B′) d(A+A′,B+B′) ≤ ∥(A+A′)−(B+B′)∥
≤ ∥A−B∥+∥A′−B′∥.
Thus,
d(A+A′,B+B′) ≤ d(A,B)+d(A′,B′) ≤ e(A,B)+e(A′,B′).
Since A and A′ are arbitrary in A and A′ respectively, the statement follows.
(iii): Let A ∈ A, B ∈ B and C ∈ C. Then
d(CA,CB) ≤ ∥CA−CB∥ ≤ ∥C∥∥A−B∥ ≤ ∥C∥ ∥A−B∥.
sup
Taking the infimum over B ∈ B we get
d(CA,CB) ≤ ∥C∥ inf ∥A−B∥ ≤ ∥C∥ e(A,B).
sup sup
B∈B
Now, taking the supremum over C ∈ C and A ∈ A, the statement follows. A similar proof can be
applied for the other case.
(v): Let A ∈ A and B ∈ B. Then A−1−B−1 = A−1(B−A)B−1 and hence
∥A−1−B−1∥ ≤ ∥A−1∥∥A−B∥∥B−1∥ ≤ ∥A−1∥ ∥B−1∥ ∥A−B∥.
sup sup
Thus
inf ∥A−1−B−1∥ ≤ ∥A−1∥ ∥B−1∥ inf ∥A−B∥
sup sup
B∈B B∈B
≤ ∥A−1∥ ∥B−1∥ e(A,B).
sup sup
Taking the supremum in A ∈ A, the statement follows.
(vi): We first note that if A = [A 1,A 2] ∈ Rd×(p1+p2) we have
(cid:13) (cid:20) x(cid:21)(cid:13)
∥A ∥ = sup ∥A x∥ = sup (cid:13)[A A ] (cid:13) ≤ ∥A∥
1 1 (cid:13) 1 2 0 (cid:13)
∥x∥≤1 ∥(x,0)∥≤1
19and similarly ∥A ∥ ≤ ∥A∥. Now let A ∈ A and B = [B B ] ∈ B. Then there exists A such that
2 1 1 1 2 2
A = [A A ] ∈ A and hence
1 2
d(A ,B ) ≤ ∥A −B ∥ ≤ ∥A−B∥.
1 1 1 1
Since the above inequality holds for every B ∈ B we have
d(A ,B ) ≤ inf ∥A−B∥ ≤ e(A,B)
1 1
B∈B
which in turns holds for every A ∈ A . Thus, taking the supremum in A ∈ A the statement follows
1 1 1 1
with k = 1. The other case is proved in the same manner.
We now recall the following result from (Bolte et al., 2022) (Lemma 4 in the Appendices), which
is stated in a slightly more general form.
TheoremB.2. LetF: U ⊂ Rp → Rd beacontinuousselectionofthedefinableLipschitzsmoothmappings
F ,...,F : U ⊂ Rp → Rd. Let L be the Lipschitz constant of ∂F and set L = max L . Then, for
1 m i i 1≤i≤m i
any x ∈ U there exists R > 0 such that
x
∀x′ ∈ U with ∥x′−x∥ ≤ R : e(Ds(x′),Ds(x)) ≤ L∥x′−x∥.
x F F
Proof. Similarly to (Bolte et al., 2022) we define
g: ]0,+∞[ ⇒ [m] such that g(r) = I (B (x)),
F r
whereB (x)istheclosedballofradiusr > 0centeredatx. Now,wenotethatg isthecompositionof
r
the maps
φ: ]0,+∞[ ⇒ Rp: r ⇒ B (x), and I : Rp ⇒ [m].
r F
Thefirstoneisclearlysemialgebraicandhencedefinableandthesecondmapisdefinablebydefinition
(since the F ’s are definable, it is easy to see that F is definable if and only if I is definable). Thus,
i F
beingg compositionofdefinableset-valuedmappingsitisdefinable. Then,foreveryI ⊂ [m],wehave
thattheset[g = I] = {r ∈ ]0,+∞[ |g(r) = I}isdefinableandsettingJ = {g(r)|r ∈ ]0,+∞[} ⊂ 2[m],
we have that ([g = I]) , is a finite partition of ]0,+∞[ made of definable sets of the real line. Thus,
I∈J
eachoneofthemmustbefiniteunionsofdisjointsintervals,whichshowsthatg ispiecewiseconstant.
It follows that there exists R > 0 and I ⊂ [m] such that for every r ∈ ]0,R ] g(r) = I. The proof
x x
continues as in Lemma 4 in (Bolte et al., 2022).
Proof of Lemma 2.5. Let x ∈ U. Let ∆ = {α ∈ Rm| (cid:80)m α = 1} be the unit simplex of Rm and
m + i=1 i
∆x
m
= {α ∈ ∆ m|∀i ∈ [m]\I F(x): α
i
= 0} (which is essentially the unit simplex of RIF(x)). Set
A = co({∂F (x)|i ∈ I (x′)}). Then
i F
e(Ds(x′),Ds(x)) ≤ e(Ds(x′),A)+e(A,Ds(x)).
F F F F
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(1) (2)
We will bound the two terms (1) and (2) separately. We recall that
Ds(x′) = co({F′(x′)|i ∈ I (x′)}) and Ds(x) = co({F′(x)|i ∈ I (x)}).
F i F F i F
20Then
(cid:13) (cid:88) (cid:88) (cid:13)
(1) = sup inf (cid:13) α F′(x′)− β F′(x)(cid:13)
(cid:13) i i i i (cid:13)
α∈∆x m′β∈∆x m′ i∈I(x′) i∈I(x′)
(cid:13) (cid:88) (cid:13)
≤ sup (cid:13) α (F′(x′)−F′(x))(cid:13)
(cid:13) i i i (cid:13)
α∈∆x m′ i∈I(x′)
(cid:88) (cid:88)
≤ sup α ∥F′(x′)−F′(x)∥ ≤ sup α L∥x−x′∥ = L∥x−x′∥.
i i i i
α∈∆x m′ i∈I(x′) α∈∆x m′ i∈I(x′)
Moreover,
m
(cid:13) (cid:88) (cid:88) (cid:13) (cid:13)(cid:88) (cid:13)
(2) = sup inf (cid:13) α F′(x)− β F′(x)(cid:13) = sup inf (cid:13) (α −β )F′(x)(cid:13)
(cid:13) i i i i (cid:13) (cid:13) i i i (cid:13)
α∈∆x
m′β∈∆x
m i∈I(x′) i∈I(x) α∈∆x
m′β∈∆x
m i=1
m
(cid:13)(cid:88) (cid:13)
≤ sup inf (cid:13) (α −β )F′(x)(cid:13) =: (∗).
(cid:13) i i i (cid:13)
α∈∆mβ∈∆x
m i=1
Now we note that
m
(cid:13)(cid:88) (cid:13)
φ(α,β) = (cid:13) (α −β )F′(x)(cid:13)+ι (α)+ι (β)
(cid:13) i i i (cid:13) ∆m ∆x m
i=1
isjointlyconvex,henceα (cid:55)→ inf φ(α,β)isconvexanditsmaximumisachievedattheverticesof∆ .
β m
Thus, if we set e = (δi) the canonical basis of Rm, we have
i j 1≤j≤m
m m
(cid:13)(cid:88) (cid:13) (cid:13) (cid:88) (cid:13)
(∗) = max inf (cid:13) (δi −β )F′(x)(cid:13) = max inf (cid:13)F′(x)− β F′(x)(cid:13)
(cid:13) j j j (cid:13) (cid:13) i j j (cid:13)
1≤i≤mβ∈∆x 1≤i≤mβ∈∆x
m j=1 m j=1
(cid:13) (cid:13)
≤ max inf (cid:13)F′(x)−F′(x)(cid:13) = M .
(cid:13) i j (cid:13) x
1≤i≤mj∈I(x)
In the end
e(Ds(x′),Ds(x)) ≤ M +L∥x′−x∥.
F F x
Now, let R > 0 be as in Theorem B.2. Then if ∥x′−x∥ > R we have ∥x′−x∥/R > 1 and hence
x x x
(cid:18) (cid:19)
M M
e(Ds(x′),Ds(x)) ≤ x ∥x′−x∥+L∥x′−x∥ = x +L ∥x′−x∥,
F F R R
x x
otherwise, if ∥x−x′∥ ≤ R , then by Theorem B.2, we have
x
(cid:18) (cid:19)
M
e(Ds(x′),Ds(x)) ≤ L∥x′−x∥ ≤ x +L ∥x′−x∥.
F F R
x
The statement follows.
21Proof of Lemma 3.2. It follows from Lemma 2.5 that the set-valued mapping
(u,λ) ⇒ Ds(u,λ) = co({Φ′(u,λ)| ∈ I (u,λ)})
Φ i Φ
is a conservative derivative of Φ and for every λ ∈ Λ there exists R > 0 (attached to (w(λ),λ)) such
λ
that
e(Ds(u,λ),Ds(w(λ),λ)) ≤ C (u)∥u−w(λ)∥ (22)
Φ Φ λ
where
(cid:40)
L if ∥u−w(λ)∥ ≤ R
λ
C (u) :=
λ
L+M /R otherwise
λ λ
andM := max min ∥Φ′(w(λ),λ)−Φ′(w(λ),λ)∥. Now,asalreadynotedinthepaper,itfollows
λ i j
i∈[m]j∈IΦ(w(λ),λ)
from Lemma 3 and Lemma 4 in (Bolte & Pauwels, 2021) that we can define a new conservative
derivative of Φ as follows
D (u,λ) = Ds (u,λ)×Ds (u,λ)
Φ Φ,1 Φ,2
and clearly D (u,λ) = Ds (u,λ) = co({∂ Φ (u,λ)|i ∈ I (u,λ)}) for k = 1,2. Therefore the
Φ,k Φ,k k i Φ
statement follows from (22) and Lemma B.1(vi).
Lemma B.3. Under Assumption 3.1(ii), for every (u,λ) ∈ Rp×Λ,
1 ∥D (w(λ),λ)∥
∥(I −D (u,λ))−1∥ ≤ , ∥Dimp(λ)∥ ≤ ∥Dfix(λ)∥ ≤ Φ,2 sup .
Φ,1 1−q w sup w sup 1−q
Proof. For the first inequality, since for any matrix A such that ∥A∥ ≤ q < 1 we have (I −A)−1 =
(cid:80)∞ An and hence ∥I −A∥ ≤ (cid:80)+∞∥A∥n ≤ (cid:80)+∞ qn = 1/(1−q). Thus, if we let A = D (u,λ)
n=0 n=0 n=0 Φ,1
we have that
1
∥(I −A)−1∥ = sup∥(I −A)−1∥ ≤ .
sup
1−q
A∈A
The second inequality holds since Dimp(λ) ⊂ Dfix(λ). For the last inequality note that, from the
w w
definition of Dfix, we have that
Φ
Dfix(u,λ) = D (u,λ)Dfix(λ)+D (u,λ)
Φ Φ,1 w Φ,2
and hence, from the properties of the excess in Lemma B.1 and from the fact that for every v ∈ Rd
∥D (v,λ)∥ ≤ q < 1 we have
Φ,1 sup
∥Dfix(λ)∥ ≤ ∥D (u,λ)∥ ∥Dfix(λ)∥ +∥D (u,λ)∥
w sup Φ,1 sup w sup Φ,2 sup
≤ q∥Dfix(λ)∥ +∥D (u,λ)∥
w sup Φ,2 sup
which implies the last inequality, after rearranging the terms.
We also present the following lemma, useful for the stochastic analysis.
22Lemma B.4. Under Assumption 3.1(ii) and 3.1, let B := ∥Ds (w(λ),λ)∥ and C be defined as in
λ Φ,2 sup λ
(12). Then, for every (u,λ) ∈ Rp×Λ, and also, with a slight abuse of notation set
Dfix(u,λ) := Dfix(u,λ)D (u,λ)+D (u,λ)
Φ Φ Φ,1 Φ,2
Dimp(u,λ) := (I −D (u,λ))−1D (u,λ).
Φ Φ,1 Φ,2
If we set w = w(λ), We have
(B +1)
e(Dfix(u,λ),Dfix(w,λ)) ≤ λ C (u)∥u−w∥.
Φ Φ (1−q)2 λ
The same holds also for
e(Dimp(u,λ),Dimp(w,λ)).
Φ Φ
Proof . Set, for the sake of brevity w = w(λ).
Then, by definition we have
Dfix(u,λ) = Ds (u,λ)Dfix(u,λ)+Ds (u,λ) and Dfix(w,λ) = Ds (w,λ)Dfix(w,λ)+Ds (w,λ).
Φ Φ,1 Φ Φ,2 Φ Φ,1 Φ Φ,2
Thus, from the properties of the excess in Lemma B.1 we have
e(Dfix(u,λ),Dfix(w,λ))
Φ Φ
≤ e(Ds (u,λ)Dfix(u,λ),Ds (w,λ)Dfix(w,λ))+e(Ds (u,λ),Ds (w,λ))
Φ,1 Φ Φ,1 Φ Φ,2 Φ,2
≤ e(Ds (u,λ)Dfix(u,λ),Ds (u,λ)Dfix(w,λ))
Φ,1 Φ Φ,1 Φ
+e(Ds (u,λ)Dfix(w,λ),Ds (w,λ)Dfix(w,λ))+e(Ds (u,λ),Ds (w,λ))
Φ,1 Φ Φ,1 Φ Φ,2 Φ,2
≤ ∥Ds (u,λ)∥ e(Dfix(u,λ),Dfix(w,λ))
Φ,1 sup Φ Φ
+∥Dfix(w,λ)∥ e(Ds (u,λ),Ds (w,λ))+e(Ds (u,λ),Ds (w,λ))
Φ sup Φ,1 Φ,1 Φ,2 Φ,2
≤ q·e(Dfix(u,λ),Dfix(w,λ))
Φ Φ
+∥Dfix(w,λ)∥ e(Ds (u,λ),Ds (w,λ))+e(Ds (u,λ),Ds (w,λ)). (23)
Φ sup Φ,1 Φ,1 Φ,2 Φ,2
Hence
∥Dfix(w,λ)∥ e(Ds (u,λ),Ds (w,λ))
e(Dfix(u,λ),Dfix(w,λ)) ≤ Φ sup e(Ds (u,λ),Ds (w,λ))+ Φ,2 Φ,2 .
Φ Φ 1−q Φ,1 Φ,1 1−q
Now, by Lemma B.3 we have ∥Dfix(w,λ)∥ ≤ ∥Ds (w,λ)∥ /(1−q) so that we get
Φ sup Φ,2 sup
B e(Ds (u,λ),Ds (w,λ))
e(Dfix(u,λ),Dfix(w,λ)) ≤ λ e(Ds (u,λ),Ds (w,λ))+ Φ,2 Φ,2
Φ Φ (1−q)2 Φ,1 Φ,1 1−q
The statement follows from Lemma 3.2.
For the second part, let A = D (u,λ) and B = D (w,λ) for i = 1,2 and recall that
i Φ,i i Φ,i
Dimp(u,λ) = (I −D (u,λ))−1D (u,λ) = (I −A )−1A
Φ Φ,1 Φ,2 1 2
Dimp(w,λ) = (I −D (w,λ))−1D (w,λ) = (I −B )−1B .
Φ Φ,1 Φ,2 1 2
23Then, using the properties in Lemma B.1(i)-(iii)-(v)-(vi), we get
e(Dimp(u,λ),Dimp(w,λ))
Φ Φ
≤ e((I −A )−1A ,(I −A )−1B )+e((I −A )−1B ,(I −B )−1B )
1 2 1 2 1 2 1 2
≤ ∥(I −A )−1∥ e(A ,B )+∥B ∥ e((I −A )−1,(I −B )−1)
1 sup 2 2 2 sup 1 1
≤ ∥(I −A )−1∥ e(A ,B )+∥B ∥ ∥(I −A )−1∥ ∥(I −B )−1∥ e(A ,B )
1 sup 2 2 2 sup 1 sup 1 sup 1 1
1 ∥D (w,λ)∥
Φ,2 sup
≤ e(D (u,λ),D (w,λ))+ e(D (u,λ),D (w,λ))
1−q Φ,2 Φ,2 (1−q)2 Φ,1 Φ,1
The statement follows again from Lemma 3.2.
C Iterative and Approximate Implicit Differentiation
Note that if κ = 1/(1−q), then qt = exp(−log(1/q)t) ≤ exp(−t/κ).
Proof of Theorem 4.1. Let λ ∈ Λ and t ∈ N, t ≥ 1. We recall that
D (λ) = D (w (λ),λ)D (λ)+D (w (λ),λ),
wt Φ,1 t−1 wt−1 Φ,2 t−1
Dfix(λ) = D (w(λ),λ)Dfix(λ)+D (w(λ),λ).
w Φ,1 w Φ,2
For simplicity, we let b = (cid:0) ∥Dfix(λ)∥ +1(cid:1) C (w (λ)) and
λ,t w sup λ t
A = D (w (λ),λ), A = D (w (λ),λ), B = D (w(λ),λ), B = D (w(λ),λ).
1,t Φ,1 t 2,t Φ,2 t 1 Φ,1 2 Φ,2
We also note that from the definition E (w ) = 1{∥w (λ)−w(λ)∥ > R } and hence
λ t t λ
M
λ
C (w ) = L+ E (w )
λ t λ t
R
λ
ITD (15): Let ∆′ := e(D (λ),Dfix(λ)). Using the properties of the excess in Lemma B.1 we have
t wt w
∆′ = e(A D (λ)+A ,B Dfix(λ)+B )
t 1,t−1 wt−1 2,t−1 1 w 2
≤ e(A D (λ),A Dfix(λ)+e(A Dfix(λ),B Dfix(λ))
1,t−1 wt−1 1,t−1 w 1,t−1 w 1 w
+e(A ,B )
2,t−1 2
≤ ∥A ∥ ∆′ +∥Dfix(λ)∥ e(A ,B )+e(A ,B )
1,t−1 sup t−1 w sup 1,t−1 1 2,t−1 2
≤ q∆′ +b ∆
t−1 λ,t−1 t−1
where for the last inequality we used the contraction assumption and Lemma 3.2. By unrolling the
recursive inequality and using the inequality ∆ ≤ qi∆ we obtain
i 0
t−1 t−1
(cid:88) (cid:88)
∆′ ≤ qt∆′ + qt−1−ib ∆ ≤ qt∆′ +qt−1∆ b
t 0 λ,i i 0 0 λ,i
i=0 i=0
≤ qt∥Dfix(λ)∥ +∆ qt−1t(∥Dfix(λ)∥ +1)(L+M R−1E¯ (t)),
w sup 0 w sup λ λ λ
where, in the last inequality, we used ∆′ ≤ ∥Dfix(λ)∥ and the definitions of E¯ (t) and C (w (λ)).
0 w sup λ λ t
Applying Lemmas B.3 and B.4, factoring out t and using the definition of B gives the final result.
λ
24AID-FP (16): In this case we have
Dk (λ) = D (w (λ),λ)Dk−1(λ)+D (w (λ),λ).
wt Φ,1 t wt Φ,2 t
Set ∆′ := e(Dk (λ),Dfix(λ)). Then using again Lemma B.1 we have
k wt w
∆′ = e(A Dk−1(λ)+A ,B Dfix(λ)+B )
k 1,t wt 2,t 1 w 2
≤ e(A Dk−1(λ),A Dfix(λ))+e(A Dfix(λ),B Dfix(λ))+e(A ,B )
1,t wt 1,t w 1,t w 1 w 2,t 2
≤ ∥A ∥ ∆′ +∥Dfix(λ)∥ e(A ,B )+e(A ,B )
1,t sup k−1 w sup 1,t 1 2,t 2
≤ q∆′ +b ∆
k−1 λ,t t
where for the last inequality we used Assumption 3.1(i) and Lemma 3.2. By unrolling the inequality
recursion we obtain
(cid:88)k−1 1−qk
∆′ ≤ qk∆′ +b ∆ qi = qk∥Dfix(λ)∥ +b ∆
k 0 λ,t t w sup λ,t 1−q t
i=0
applying Lemmas B.3 and B.4 and using the definition of b , C and E gives the final result.
λ,t λ λ
Forthefinalcomment,ifw (λ) = Φ(w (λ),λ),duetothecontractionpropertyofΦ,∆ = q∆ <
t t t t−1
∆ andthereexistτ ∈ {0,...,t′ }witht′ := ⌈log(∆ /R )/log(1/q)⌉,suchthat∥w (λ)−w(λ)∥ ≤
t−1 λ λ λ 0 λ τ λ
R , and if τ ̸= 0, ∥w (λ)−w(λ)∥ > R . Thus, for every i ∈ N E (w ) = 1{i < τ } and therefore
λ λ τ λ−1 λ λ i λ
for every t, E (w ) ≤ E¯ (t) ≤ 1.
λ t λ
D Stochastic Implicit Differentiation
Thanks to the chain rule of conservative derivatives, if Assumption 5.1(i) holds and we set, for every
u ∈ Rd,λ ∈ O
Λ
D (u,λ) = D (u,λ)×D (u,λ)
Φ Φ,1 Φ,2
with
D (u,λ) = D (T(u,λ),λ)D (u,λ)
Φ,1 G,1 T,1
D (u,λ) = D (T(u,λ),λ)D (u,λ)+D (T(u,λ),λ),
Φ,2 G,1 T,2 G,2
then D is a conservative derivative for Φ. Furthermore if Assumption 5.1(i) is satisfied, then
Φ
∥D (u,λ)∥ ≤ q < 1 and Dfix and Dimp in (11) and (10) are well defined and conservative
Φ,1 sup w w
derivatives of w.
The following result is similar to Lemma 3.2 and follows again from Lemma 2.5. The only
difference is that the constants are majorized so to be independent on u. This is done only to simplify
the analysis.
Lemma D.1. Under Assumption 5.1, for every λ ∈ Λ, there exist R ,R > 0 such that for every
G,λ T,λ
u ∈ Rd and k ∈ {1,2}
e(D (u,λ),D (T(w(λ),λ),λ) ≤ C ∥u−T(w(λ),λ)∥
G,k G,k G,λ
e(D (u,λ),D (w(λ),λ)) ≤ C ∥u−w(λ)∥,
T,k T,k T,λ
25where
C := L +M /R C := L +M /R (24)
G,λ G G,λ G,λ T,λ T T,λ T,λ
with M := max min ∥X′(w(λ),λ) − X′(w(λ),λ)∥ and L ,I satisfy Assump-
X,λ i∈{1,...,m} j∈IX(w(λ),λ) i j X X
tion 3.1(i) and X ∈ {G,T}.
Proof. We proceed similarly to the proof of Lemma 3.2 with Φ replaced by T and G and, in the case
of G we also replace w(λ) with T(w(λ),λ).
We now present the proof of Theorem 5.4.
Proof of Theorem 5.4. Set, for the sake of brevity, z = (w (λ),λ) and z = (w(λ),λ). We also set
t t
v = v (w (λ),T¯(λ)) and v¯= v¯(w (λ),T¯(λ)).
k k t t t t
Let ∆ = ∥w(λ)−T(z)∥. Then from Assumption 5.2 on the variance of Tˆ and since T(·,λ) is
λ
1-Lipschitz we have
Var[Tˆ(z ,ξ)|w (λ),y] ≤ σ +σ ∥w (λ)∓w(λ)−T(w (λ),λ)±T(w(λ),λ)∥2
t t 1 2 t t
≤ σ +3σ (2∆2+∆2) =: B (∆2). (25)
1 2 t λ 3 t
Using the properties of the excess in Lemma B.1 and recalling that
Dimp(λ) = (I −D (z))−1D (z),
w Φ,1 Φ,2
we have the following error decomposition
e(∂ Φ¯(z )⊤v ,Dimp(λ)⊤y) ≤ e(∂ Φ¯(z )⊤v ,D (z)⊤v )+e(D (z)⊤v ,D (z)⊤(I −D (z)⊤)−1y)
2 t k w 2 t k Φ,2 k Φ,2 k Φ,2 Φ,1
≤ e(∂ Φ¯(z ),D (z))∥v ∥+∥D (z)∥ e(v ,(I −D (z)⊤)−1y),
2 t Φ,2 k Φ,2 sup k Φ,1
and hence, squaring and taking the expectation of both sides and using the law of total expectation
yields
E[e(∂ 2Φ¯(z t)⊤v k,D wimp(λ)⊤y)2]|w t(λ),y] ≤ 2E(cid:2)E[∥v k∥2|w t(λ),y,ξ(1)]e(∂ 2Φ¯(z t),D Φ,2(z))2(cid:12) (cid:12)w t(λ),y(cid:3)
(cid:124) (cid:123)(cid:122) (cid:125)
(1)
+2∥D (z)∥2 E[e(v ,(I −D (z)⊤)−1y)2|w (λ),y].
Φ,2 sup k Φ,1 t
(cid:124) (cid:123)(cid:122) (cid:125)
(2)
(26)
Bound for the first term in the RHS of (26) We have that
E[∥v ∥2|w (λ),y,ξ(1)] ≤ 2E[∥v −v¯∥2+∥v¯∥2|w (λ),y,ξ(1)]
k t k t
≤ 2(cid:0)E[∥v −v¯∥2|w (λ),y,ξ(1)]+∥y∥2/(1−q)2(cid:1)
k t
≤
2∥y∥2(cid:0)
σ
(k)+1/(1−q)2(cid:1)
.
λ
where in the second last inequality we used Assumption 5.3(ii). Note that the bound above is
independent from ξ(1) and hence
(1) ≤ 4∥y∥2(cid:0) σ (k)+1/(1−q)2(cid:1)E[e(∂ Φ¯(z ),D (z))2|w (λ),y].
λ 2 t Φ,2 t
26Now recall that
∂ Φ¯(z ) = ∂ G(T¯(z ),λ)∂ T¯(z )+∂ G(T¯(z ),λ)
2 t 1 t 2 t 2 t
D (z) = D (T(z),λ)D (z)+D (T(z),λ),
Φ,2 G,1 T,2 G,2
therefore using the properties of excess in Lemma B.1 we have
e(cid:0) ∂ Φ¯(z ),D (z)(cid:1)
2 t Φ,2
≤ e(cid:0) ∂ G(T¯(z ),λ)∂ T¯(z ),∂ G(T¯(z ),λ)D (z)(cid:1)
1 t 2 t 1 t T,2
+e(cid:0) ∂ G(T¯(z ),λ)D (z),D (T(z),λ)D (z)(cid:1) +e(cid:0) ∂ G(T¯(z ),λ),D (T(z),λ)(cid:1)
1 t T,2 G,1 T,2 2 t G,2
≤ ∥∂ G(T¯(z ),λ)∥e(∂ T¯(z ),D (z))
1 t 2 t T,2
+∥D (z)∥ e(cid:0) ∂ G(T¯(z ),λ),D (T(z),λ)(cid:1) +e(cid:0) ∂ G(T¯(z ),λ),D (T(z),λ)(cid:1)
T,2 sup 1 t G,1 2 t G,2
≤ ∥∂ T¯(z )−∂ T(z )∥+e(∂ T(z ),D (z))
2 t 2 t 2 t T,2
+∥D (z)∥ e(cid:0) ∂ G(T¯(z ),λ),D (T(z),λ)(cid:1) +e(cid:0) ∂ G(T¯(z ),λ),D (T(z),λ)(cid:1)
T,2 sup 1 t G,1 2 t G,2
≤ ∥∂ T¯(z )−∂ T(z )∥+C ∥w (λ)−w(λ)∥
2 t 2 t T,λ t
+C (1+∥D (z)∥ )(∥T¯(z )−T(z )∥+∥T(z )−T(z)∥)
G,λ T,2 sup t t t
≤ ∥∂ T¯(z )−∂ T(z )∥+[C +C (1+∥D (z)∥ )]∆
2 t 2 t T,λ G,λ T,2 sup t
+C (1+∥D (z)∥ )∥T¯(z )−T(z )∥.
G,λ T,2 sup t t
Hence, recalling Assumption 5.2 and (25), we have
E(cid:2) e(cid:0) ∂ 2Φ¯(z t),D Φ,2(z)(cid:1)2(cid:12) (cid:12)w t(λ),y(cid:3) ≤ 3Var[∂ 2T¯(z t)|w t(λ),y]
+3[C +C (1+∥D (z)∥ )]2∆2
T,λ G,λ T,2 sup t
+3C2 (1+∥D (z)∥ )2Var[T¯(z )|w (λ),y]
G,λ T,2 sup t t
3σ′
≤ 2 +3[C +C (1+∥D (z)∥ )]2∆2
J T,λ G,λ T,2 sup t
B (∆2)
+3C2 (1+∥D (z)∥ )2 3 t .
G,λ T,2 sup J
In the end we have
(cid:18) 1 (cid:19)(cid:18) σ′ B (∆2)(cid:19)
(1) ≤ 12∥y∥2 σ (k)+ 2 +[C +C M ]2∆2+C2 M2 3 t ,
λ (1−q)2 J T,λ G,λ T,λ t G,λ T,λ J
where we set M = 1+∥D (w(λ),λ)∥ .
T,λ T,2 sup
Bound for the second term in the RHS of (26) We have
e(v ,(I −D (z)⊤)−1y) ≤ ∥v −v¯∥+e(v¯,(I −D (z))−⊤y).
k Φ,1 k Φ,1
Now we recall that v¯= (cid:0) I −∂ G(T¯(z ),λ)∂ T(z )(cid:1)−⊤ . Hence, setting A = I −∂ G(T¯(z ),λ)∂ T(z )
1 t 1 t 1 t 1 t
and B = I −D (z), and using the properties of Lemma B.1(iv)(v), Lemma D.1, and noting that
Φ,1
27∥∂ T(z )∥,∥D (z)∥ ≤ 1 and ∥A−1∥,∥B−1∥ ≤ 1/(1−q) (Lemma B.3) we obtain
1 t G,1 sup sup
e(v ,(I −D (z)⊤)−1y)
k Φ,1
≤ ∥v −v¯∥+∥y∥∥A−1∥∥B−1∥ e(A,B)
k sup
≤ ∥v −v¯∥+ ∥y∥ e(cid:0) ∂ G(T¯(z ),λ)∂ T(z ),D (T(z),λ)D (z)(cid:1)
k (1−q)2 1 t 1 t G,1 T,1
≤ ∥v −v¯∥+ ∥y∥ (cid:104) e(cid:0) ∂ G(T¯(z ),λ)∂ T(z ),D (T(z),λ)∂ T(z )(cid:1)
k (1−q)2 1 t 1 t G,1 1 t
(cid:105)
(cid:0) (cid:1)
+e D (T(z),λ)∂ T(z ),D (T(z),λ)D (z)
G,1 1 t G,1 T,1
≤ ∥v −v¯∥+ ∥y∥ (cid:104) ∥∂ T(z )∥e(cid:0) ∂ G(T¯(z ),λ),D (T(z),λ)(cid:1)
k (1−q)2 1 t 1 t G,1
(cid:105)
(cid:0) (cid:1)
+∥D (T(z),λ)∥ e ∂ T(z ),D (z)
G,1 sup 1 t T,1
≤ ∥v −v¯∥+ ∥y∥ (cid:104) e(cid:0) ∂ G(T¯(z ),λ),D (T(z),λ)(cid:1) +e(cid:0) ∂ T(z ),D (z)(cid:1)(cid:105)
k (1−q)2 1 t G,1 1 t T,1
≤ ∥v −v¯∥+ ∥y∥ (cid:2) C (∥T¯(z )−T(z )∥+∥T(z )−T(z)∥)+C ∥w (λ)−w(λ)∥(cid:3)
k (1−q)2 G,λ t t t T,λ t
≤ ∥v −v¯∥+ ∥y∥ (cid:2) C ∥T¯(z )−T(z )∥+(C +C )∆ (cid:3) .
k (1−q)2 G,λ t t G,λ T,λ t
Therefore,
E(cid:2)
e(v k,(I −D
Φ,1(z)⊤)−1y)2(cid:12)
(cid:12)w
t(λ),y,ξ(1)(cid:3)
≤
3(cid:18)
∥y∥2σ (k)+
∥y∥2
(cid:2) C2 ∥T¯(z )−T(z )∥2+(C +C
)2∆2(cid:3)(cid:19)
λ (1−q)4 G,λ t t G,λ T,λ t
and hence
E(cid:2)
e(v k,(I −D
Φ,1(z)⊤)−1y)2(cid:12)
(cid:12)w
t(λ),y(cid:3)
(cid:18) (cid:19)
≤ 3∥y∥2 σ (k)+ 1 (cid:2) C2 Var[T¯(z )|w (λ),y]+(C +C )2∆2(cid:3)
λ (1−q)4 G,λ t t G,λ T,λ t
(cid:18) 1 (cid:16) Var[Tˆ(z ,ξ)|w (λ),y] (cid:17)(cid:19)
= 3∥y∥2 σ (k)+ C2 t t +(C +C )2∆2
λ (1−q)4 G,λ J G,λ T,λ t
(cid:18) 1 (cid:16) B (∆2) (cid:17)(cid:19)
≤ 3∥y∥2 σ (k)+ C2 3 t +(C +C )2∆2
λ (1−q)4 G,λ J G,λ T,λ t
In the end we have
(cid:18) 1 (cid:16) B (∆2) (cid:17)(cid:19)
(2) ≤ 6∥D (z)∥2 ∥y∥2 σ (k)+ C2 3 t +(C +C )2∆2 .
Φ,2 sup λ (1−q)4 G,λ J G,λ T,λ t
28Combined bound By combining the above results we finally obtain
E[e(∂ Φ¯(z )⊤v ,Dimp(λ)⊤y)2]|w (λ),y]
2 t k w t
≤ 12∥y∥2(cid:0) σ (k)+κ2(cid:1)(cid:18) σ 2′ +[C +C M ]2∆2+C2 M2 σ 1+3σ 2(2∆2 t +∆2 λ)(cid:19)
λ J T,λ G,λ T,λ t G,λ T,λ J
(cid:18) (cid:16) σ +3σ (2∆2+∆2) (cid:17)(cid:19)
+6∥y∥2∥D (w(λ),λ)∥2 σ (k)+κ4 C2 1 2 t λ +(C +C )2∆2 ,
Φ,2 sup λ G,λ J G,λ T,λ t
where in the last equation we used the expression for B (∆2) and κ = (1 − q)−1. Taking the
3 t
expectation E[·|w (λ)] and recalling the hypothesis on ∥y∥2 and ∆2 in Assumption 5.3(i)(iii), the
t t
statement follows.
Before reporting the proof for the linear system rate, we rewrite for reader’s convenience the
following result from (Grazzi et al., 2021), which establishes a convergence rate for stochastic
fixed-point iterations with a decreasing step size.
Lemma D.2. (Grazzi et al., 2021, Theorem 4.2) Let Ψ: Rd → Rd be a q-contraction (0 ≤ q < 1), ξ a
random variable with values in Ξ and Ψˆ: Rd×Ξ → Rd be such that for every v ∈ Rd
E[Ψˆ(v,ξ)] = Ψ(v) and Var[Ψˆ(v,ξ)] ≤ σˆ +σˆ ∥Ψ(v)−v∥2,
1 2
forsomeσˆ 1,σˆ 2 > 0. Letη i = β/(γ+i),withβ > 1/(1−q2)andγ ≥ β(1+σˆ 22). Let(ξ i) i∈N beasequence
of i.i.d copies of ξ and let (v i) i∈N be such that v 0 = 0 and for i = 0,1,...
v = v +η (Ψˆ(v ,ξ )−v ).
i+1 i i i i i
Then for every i ∈ N
1 (cid:110) β2σˆ (cid:111)
E[∥v −v¯∥2] ≤ max γ∥v¯∥2, 1 ,
i γ +i β(1−q2)−1
where v¯is the (unique) fixed point of Ψ.
WenowpresenttherateforthealgorithmusedtosolvethelinearsysteminAlgorithm1. Consider
the procedure in Algorithm 3
Algorithm 3: Stochastic fixed point iterations
1: Input: k ∈ N, u 1,u 2,y ∈ Rd, Tˆ , G, ξ(2) = (ξ i(2) ) 1≤i≤k.
2: Ψˆ: (v,x) (cid:55)→ ∂ 1Tˆ(u 1,λ,x)⊤∂ 1G(u 2,λ)⊤v+y
3: v 0 = 0
4: for i = 1 to k do
5: v i ← (1−η i)v i−1+η iΨˆ(v i−1,ξ i(2) )
6: end for
7: Return v k
Note that v in Algorithm 1 is exactly the output of Algorithm 3 with u = w (λ), u = T¯(λ).
k 1 t 2 t
Moreover, we obtain the following convergence rate which is completely independent from the inputs
u and u .
1 2
29Lemma D.3 (Linear system rate). Under Assumption 5.1 and 5.2, let σˆ = 2σ′(1−q)−2, σˆ = σˆ ∥y∥2,
2 1 1 2
andconsiderthestochasticfixedpointiterationsinAlgorithm3withη = β/(γ+i),withβ >
1/(cid:0) 1−q2(cid:1)
i
and γ ≥ β(1+σˆ ). For any u ,u ,y ∈ Rd let the solution of the linear system be
2 2 1
v¯:= (I −∂ T(u ,λ)⊤∂ G(u ,λ)⊤)−1y.
1 1 1 2
Then we have
∥y∥2 (cid:26) γ β2σˆ (cid:27)
E[∥v −v¯∥2] ≤ max , 2 . (27)
k γ +k (1−q)2 β(1−q2)−1
In particular, if we set β = 2/(1−q2),γ = 2(1+σˆ )/(1−q2), we obtain
2
1 2∥y∥2(1+4σ′)
E[∥v −v¯∥2] ≤ · 1 .
k k (1−q)5
Proof. Let
Ψ(v) := E[Ψˆ(v,ξ)] = ∂ T(u ,λ)⊤∂ G(u ,λ)⊤v+y.
1 1 1 2
Since ∥∂ T(u ,λ)⊤∂ G(u ,λ)⊤∥ ≤ q, Ψ is a q-contraction with fixed point v¯. It is immediate to see
1 1 1 2
that
Var[Ψˆ(v,ξ)] ≤ Var[∂ Tˆ(u ,λ,ξ)]∥v∥2.
1 1
Moreover, we have
∥v∥ ≤ ∥v−Ψ(v)∥+∥Ψ(v)−Ψ(0)∥+∥Ψ(0)∥ ≤ ∥v−Ψ(v)∥+q∥v∥+∥y∥
and hence
(1−q)∥v∥ ≤ ∥v−Ψ(v)∥+∥y∥, (28)
which, recalling Assumption 5.2 on the variance of T′, ultimately yields
1
Var[Ψˆ(v,ξ)] ≤ 2 Var[∂ Tˆ(u ,λ,ξ)](cid:0) ∥v−Ψ(v)∥2+∥y∥2(cid:1) ≤ 2σ 1′ ∥Ψ(v)−v∥2+2σ 1′∥y∥2 .
(1−q)2 1 1 (1−q)2 (1−q)2
Therefore, the first part of the statement follows from Lemma D.2 and from ∥v¯∥ ≤ ∥y∥(1−q)−1 (a
consequence of (28)). The last part follows by (27) and the fact that
2 (cid:16) 2σ′ (cid:17) 2(1+2σ′) 8σ′
γ = 1+ 1 ≤ 1 and β2σˆ = 1
1−q2 (1−q)2 (1−q2)(1−q)2 2 (1−q)2(1−q2)2
and the fact that (1−q2)−1 ≤ (1−q)−1 when q < 1.
Proof of Theorem 5.5. By applying Lemma D.3 with u = w (λ) and u = T¯(w (λ),λ) we obtain that
1 t 2 t
Assumption 5.3(ii) (the rate on the mean square error of v ) is satisfied with σ (k) = O(κ5k−1). The
k λ
statement follows by applying Theorem 5.4 and substituting the rates ρ (t) and σ (k).
λ λ
30E Bilevel Optimization
In this section we consider Problem Equation (20) and we make the following assumption.
Assumption E.1. The map E satisfies Assumption 3.1(i) with constant L and corresponding conser-
E
vative derivative D .
E
Notethat similarlytoΦ, sinceE satisfiesAssumption E.1, anapplicationof Lemma2.5 tothemap
E yields
Lemma E.2. Under Assumption E.1, for every λ ∈ Λ, there exist R > 0 such that for every u ∈ Rd
E,λ
and k ∈ {1,2}
e(D (u,λ),D (w(λ),λ)) ≤ C ∥u−w(λ)∥
E,k E,k E,λ
whereC := L +M /R ,withM := max min ∥E′(w(λ),λ)−E′(w(λ),λ)∥.
E,λ E E,λ E,λ E,λ i∈{1,...,m} j∈IE(w(λ),λ) i j
E.1 Deterministic Case
Theorem E.3. Let Assumption 3.1 and E.1 hold. Then for every λ ∈ Λ and every t,k ∈ N we have that
for BAID-FP we get
e(Dk (z ),Dfix(z)) = O(κe−k/κ+κ2∆ )
ft t f t
while if w (λ) = Φ(w (λ),λ), for BITD we get
t t−1
e(D (z ),Dfix(z)) = O(κte−κ/t).
ft t f
Proof. Recalling the expressions for D and Dfix we obtain
ft f
e(D (z ),Dfix(z)) ≤ ∥D (z )∥ e(D (λ),Dfix(λ))
ft t f E,1 t sup wt w
+∥Dfix(λ)∥ e(D (z ),D (z))
w sup E,1 t E,1
+e(D (z ),D (z))
E,2 t E,2
≤ (∥D (z)∥ +C ∆ )×O(κte−t/κ)
E,1 sup E,λ t
(cid:16)∥D (z )∥ (cid:17)
Φ,2 t sup
+ +1 C ∆
E,λ t
1−q
≤ (∥D (z)∥ +C ∆ )×O(κte−t/κ)
E,1 sup E,λ 0
(cid:124) (cid:123)(cid:122) (cid:125)
(∗)
(cid:16)∥D (z)∥ +C ∆ (cid:17)
+ Φ,2 sup Φ,λ 0 +1 C ∆ e−t/κ,
E,λ 0
1−q
where we used the ITD bound in Theorem 4.1 and the Lemma E.2. A very similar proof can be done
for AID-FP by changing the (∗) term to O(κe−k/κ+κ2e−t/κ).
E.2 Stochastic Case
We consider the special case of Problem (20) with
E(w,λ) = E[Eˆ(w,λ,ζ)], Φ(w,λ) = G(E[Tˆ(w,λ,ξ)],λ).
In addition to Assumption E.4, as for the smooth case in (Grazzi et al., 2023), we consider the
following assumption on E
31Assumption E.4. For any λ ∈ Λ there exists B ≥ 0 such that
E,λ
∀w ∈ Rd: ∥D (w,λ)∥ ≤ B .
E,1 sup E,λ
The assumption above is verified e.g. for the logistic and for the cross-entropy loss. Moreover, the
assumptions on
Eˆ
are the following.
Assumption E.5.
(i) Eˆ: Rd×O ×Z → Rd, ζ is a random variable with value in Z and E[Eˆ(u,λ,ζ)] = E(u,λ).
Λ
(ii) For every z ∈ Z, Eˆ(·,·,z) is path differentiable and
∂ Eˆ: Rd×O ×Z → Rd, ∂ Eˆ: Rd×O ×Z → Rd
1 Λ 1 Λ
are selections of D and D respectively and there exist σ ,σ ≥ 0 such that for every
Eˆ,1 Eˆ,2 E,1 E,2
u ∈ Rd and λ ∈ Λ
E[∂ Eˆ(u,λ,ζ)] = ∂ E(u,λ) ∈ D (u,λ), E[∂ Eˆ(u,λ,ζ)] = ∂ E(u,λ) ∈ D (u,λ),
1 1 E,1 2 2 E,2
Var[∂ Eˆ(u,λ,ζ)] ≤ σ , Var[∂ Eˆ(u,λ,ζ)] ≤ σ .
1 E,1 2 E,2
Theorem E.6. Let Assumption 5.1, 5.2, E.1, E.4, E.5 hold and let κ = (1−q)−1. Also, suppose that
E[∥w (λ)−w(λ)∥] ≤ ρ (t),foreveryt ∈ N. Thentheoutput∇ˆf(λ)ofNSID-Bilevel(Algorithm2)where
t λ
NSID uses step sizes η = O(i−1) satisfies
i
(cid:18) κ5 (cid:18) 1 (cid:19) κ2(cid:19)
E[e(∇ˆf(λ),Dimp(λ))2] = O +κ4 +ρ (t) + .
f k J λ J
2 1
Furthermore, if ρ (t) = O(καt−1) (α > 0), then
λ
E[e(∇ˆf(λ),Dimp(λ))2] = O(cid:0) κ2J−1+κ5(k−1+J−1+καt−1)(cid:1) .
f 1 2
Therefore, by setting e.g. t = k = J = J we have
1 2
E[e(∇ˆf(λ),Dimp(λ))2] = O(κ5+αt−1)
f
which has the same dependency on t as stochastic gradient descent on strongly convex and Lipschitz
smooth objectives (Bottou et al., 2018).
Proof. We recall that
Dimp(λ):=Dimp(λ)⊤D (w(λ),λ)+D (w(λ),λ)
f w E,1 E,2
∇ˆf(λ) = r(z )+∂ E¯(z ).
t 2 t
32Then
e(∇ˆf(λ),Dimp(λ))
f
≤ e(r(z ),Dimp(λ)⊤D (w(λ),λ))+e(∂ E¯(z ),D (w(λ),λ))
t w E,1 2 t E,2
≤ e(r(z ),Dimp(λ)⊤∂ E¯(z ))+e(Dimp(λ)⊤∂ E¯(z ),Dimp(λ)⊤D (w(λ),λ))
t w 1 t w 1 t w E,1
+∥∂ E¯(z )−∂ E(z )∥+e(∂ E(z ),D (w(λ),λ))
2 t 2 t 2 t E,2
≤ e(r(z ),Dimp(λ)⊤∂ E¯(z ))+∥Dimp(λ)∥ (cid:0) ∥∂ E¯(z )−∂ E(z )∥+e(∂ E(z ),D (w(λ),λ))(cid:1)
t w 1 t w sup 1 t 1 t 1 t E,1
+∥∂ E¯(z )−∂ E(z )∥+C ∥w (λ)−w(λ)∥
2 t 2 t E,λ t
≤ e(r(z ),Dimp(λ)⊤∂ E¯(z ))+∥Dimp(λ)∥ (cid:0) ∥∂ E¯(z )−∂ E(z )∥+C ∥w (λ)−w(λ)∥(cid:1)
t w 1 t w sup 1 t 1 t E,λ t
+∥∂ E¯(z )−∂ E(z )∥+C ∥w (λ)−w(λ)∥
2 t 2 t E,λ t
≤ e(r(z ),Dimp(λ)⊤∂ E¯(z ))+C (1+∥Dimp(λ)∥ )∆
t w 1 t E,λ w sup t
+∥∂ E¯(z )−∂ E(z )∥+∥Dimp(λ)∥ ∥∂ E¯(z )−∂ E(z )∥
2 t 2 t w sup 1 t 1 t
Hence
E[e(∇ˆf(λ),Dimp(λ))2|w (λ)]
f t
(cid:18)
≤ 4 E[e(r(z ),Dimp(λ)⊤∂ E¯(z ))2|w (λ)]+C2 (1+∥Dimp(λ)∥ )2∆2
t w 1 t t E,λ w sup t
Var[∂ Eˆ(z ,ζ)|w (λ)] Var[∂ Eˆ(z ,ζ)|w (λ)](cid:19)
+ 2 t t +∥Dimp(λ)∥2 1 t t
J w sup J
1 1
(cid:18) (cid:18)
∥D (w(λ),λ)∥
(cid:19)2
≤ 4 E[e(r(z ),Dimp(λ)⊤∂ E¯(z ))2|w (λ)]+C2 1+ Φ,2 sup ∆2
t w 1 t t E,λ 1−q t
σ ∥D (w(λ),λ)∥2 σ (cid:19)
E,2 Φ,2 sup E,1
+ + . (29)
J (1−q)2 J
1 1
Now, we note that
2σ
E(cid:2) ∥∂ E¯(z )∥2|w (λ)(cid:3) ≤ 2E(cid:2) ∥∂ E¯(z )−∂ E(z )∥2|w (λ)(cid:3) +2∥D (z )∥2 ≤ E,1 +2B . (30)
1 t t 1 t 1 t t E,1 t sup J E,λ
1
Therefore, taking the total expectation in (29) and recalling Theorem 5.4, we get
E[e(∇ˆf(λ),Dimp(λ))2]
f
(cid:18) (cid:18) (cid:19)(cid:18) (cid:19)(cid:19)
2σ 1
≤ O σ (k)+κ4 E,1 +2B +ρ (t)
λ E,λ λ
J J
1 2
+4C2 (cid:0) 1+κ∥D (w(λ),λ)∥ (cid:1)2 ρ (t)+ 1 (cid:0) σ +σ κ2∥D (w(λ),λ)∥2 (cid:1)
E,λ Φ,2 sup λ J E,2 E,1 Φ,2 sup
1
(cid:18) (cid:18)
1
(cid:19) κ2(cid:19)
= O σ (k)+κ4 +ρ (t) + .
λ λ
J J
2 1
The first part of the statement follows by noting that for NSID we have σ (k) = O(κ5/k), where
λ
κ = 1/(1−q). The second part is immediate.
33F Experimental Details
We give more information on the numerical experiments in Section 7.
F.1 Computing the approximation Error.
Let c ∈ Rm, be the output of an algorithm approximating the jacobian vector product Dfix(λ)⊤y. We
w
call approximation error the quantity
e(c,Dfix(λ)⊤y).
w
SinceDfix(λ)⊤y issetvaluedandeachelementisnotavailableinclosedform,weinsteadapproximate
w
an upper bound to this quantity using AID-FP for enough iterations k, which as we mention in
Section 4, generates a subsequence linearly converging to an element of Dfix(λ)⊤y. Also, as a starting
w
point to AID-FP we use w (λ) = Φ(w (λ),λ), with sufficiently large t and starting from w (λ) = 0,
t t−1 0
so to be sufficiently close to the fixed point solution w(λ), also not available in closed form.
F.2 Constructing the fixed-point map.
In all the experiments, we consider composite minimization problems in the form
minf (u)+g (u).
λ λ
u
To convert it to fixed point we set a step size η > 0 and set
λ
Φ(u,λ) = G(T(u,λ),λ),
with
G(u,λ) = Prox (u) T(w,λ) = u−η ∇f (u).
η λg λ λ λ
In particualr, since we consider Elastic net, Prox is the soft-thresholding. In particular, in the case of
elastcnetwithparametersλ ,λ wesetη = 2/(L+µ+2λ ),whereL,µarethelargestandsmallest
1 2 λ 2
eigenvalues of X⊤X, where X is the design matrix of the training set. Since this theoretical estimate
is too conservative for data-poisoning we set η = 20/(L+µ+2λ ), 10 times the optimal theoretical
λ 2
value, and we set µ = 0 since we are dealing with the cross-entropy loss.
F.3 Details for the AID and ITD Experiments
We construct the synthetic dataset by sampling each element of the matrix X ∈ Rn×p and the vector
w from a normal distribution. Subsequently, we set the non-informative features of w to zero and we
compute the vector y as y = Xw+ϵ, where ϵ is Gaussian noise with mean 0.1 and unit variance. For
i
this experiment we set n = 500 and p = 100 of which 30 are informative.
F.4 Details for the Stochastic Experiments
For elastic net, we enhance the setup used for the deterministic methods by sampling the population
covariance matrix randomly for the informative features. To do so, we first sample a matrix A
1
34from a standard normal, then we normalize all eigenvalues by diving all of them by their maximum
obtaining A , finally we use the normalized A⊤A as the covariance matrix of a Gaussian distribution
2 2 2
for the informative features. This introduces correlations among the features, thereby increasing the
complexity of the problem. We also increase the number of training points from 500 to 10K.
For the data poisoning setup use the MNIST dataset. We split the MNIST original train set into
30K example for training and 30K examples for validation. Additionally, we perform a random
split of the training set into X ∈ Rn×p and X˜ ∈ Rn′×p, with p = 784 representing the number
of features for MNIST images. Notably, n′ = 9K denotes the number of corrupted examples. It
is essential to highlight that Γ ∈ Rn′×p and n′p is approximately 7 million, posing a significant
challenge for derivative estimation using zero-order methods. For data poisoning, we observed that
the theoretical value for the step size was too conservative and hence we multiply it by 10, to have
improved convergence. We set the regularization parameters λ = (0.02,0.1) since with this setup, the
final uncorrputed linear model achieves a validation accuracy of around 80% with around 90% of
components set to zero. We note that NSID and SID require to choose the step sizes (η ), which we
i
foundtobedifficult,sincethetheoreticalvaluesareoftenconservativeestimatesforthisproblem. We
trytwo policies: constant anddecreasing (asΘ(1/i))step sizes, indicatedwith “const”and “dec”after
the method name respectively. Note that only when the step sizes are decreasing NSID is guaranteed
to converge. To simplify the setup, we always set them equal at i = 0. Moreover, we set the step size
of NSID equal to that of SID, when they use the same step sizes policy.
More speifically, we set η = a /(a + i) for NSID dec and η = a /a for NSID const, where
i 1 2 1 2
a = b β and a = b β, where beta is set to the theretical value suggested in Lemma D.3 (2/(1−q2)).
1 1 2 2
We tuned a ,a manually for each setting. In particular we set a = 0.5,a = 2 for the synthetic
1 2 1 2
Elastic net experiment and a = 2,a = 0.01 for Data poisoning.
1 2
35