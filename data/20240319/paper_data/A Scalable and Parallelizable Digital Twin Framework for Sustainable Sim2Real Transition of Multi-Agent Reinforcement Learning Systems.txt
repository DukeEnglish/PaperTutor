A Scalable and Parallelizable Digital Twin Framework for Sustainable
Sim2Real Transition of Multi-Agent Reinforcement Learning Systems
Chinmay V. Samak∗ , Tanmay V. Samak∗ and Venkat N. Krovi
Abstract—Thisworkpresentsasustainablemulti-agentdeep
reinforcementlearningframeworkcapableofselectivelyscaling
parallelized training workloads on-demand, and transferring
the trained policies from simulation to reality using minimal
hardwareresources.WeintroduceAutoDRIVEEcosystemasan
enabling digital twin framework to train, deploy, and transfer
cooperative as well as competitive multi-agent reinforcement
learningpoliciesfromsimulationtoreality.Particularly,wefirst (a) Cooperative MARL using Nigel.
investigate an intersection traversal problem of 4 cooperative
vehicles(Nigel)thatsharelimitedstateinformationinsingleas
wellasmulti-agentlearningsettingsusingacommonpolicyap-
proach. We then investigate an adversarial autonomous racing
problem of 2 vehicles (F1TENTH) using an individual policy
approach.Ineithersetofexperiments,adecentralizedlearning
architecture was adopted, which allowed robust training and
testing of the policies in stochastic environments. The agents
(b) Competitive MARL using F1TENTH.
were provided with realistically sparse observation spaces,
and were restricted to sample control actions that implicitly Fig. 1: Resource-aware digital twin framework for multi-
satisfied the imposed kinodynamic and safety constraints. The
agent deep reinforcement learning.
experimental results for both problem statements are reported
in terms of quantitative metrics and qualitative remarks for
training as well as deployment phases. We also discuss agent
and environment parallelization techniques adopted to effi-
and efficiently navigating road networks. It mirrors traffic
cientlyaccelerateMARLtraining,whileanalyzingtheircompu-
situations where vehicles must work together, such as in-
tationalperformance.Finally,wedemonstratearesource-aware
transitionofthetrainedpoliciesfromsimulationtorealityusing tersection management or platooning scenarios. Challenges
the proposed digital twin framework. in cooperative MARL include coordinating vehicle actions
to minimize congestion, maintaining safety margins, and
Index Terms—Multi-Agent Systems, Autonomous Vehicles,
ensuring smooth interactions between self-interested agents.
Reinforcement Learning, Digital Twins, Real2Sim, Sim2Real
Ontheotherhand,competitiveMARL[7]–[10]introduces
a competitive edge to autonomous driving, simulating sce-
I. INTRODUCTION narios such as overtaking, merging in congested traffic, or
competitive racing. In this paradigm, autonomous vehicles
In the rapidly evolving landscape of connected and au-
strive to outperform their counterparts, vying for advantages
tonomous vehicles (CAVs), the pursuit of intelligent and
while navigating complex and dynamic environments. Chal-
adaptive driving systems has emerged as a formidable chal-
lenges in competitive MARL encompass strategic decision-
lenge. Multi-agent reinforcement learning (MARL) stands
making, opponent modeling, and adapting to aggressive
out as a promising avenue in the quest to develop au-
driving behaviors while preserving safety standards.
tonomous vehicles capable of navigating complex and dy-
In this paper, we present AutoDRIVE Ecosystem [11],
namic environments while considering the nature of interac-
[12] as an enabler to effectively bridge the reality-to-
tions with their peers. While cooperative MARL encourages
simulation (real2sim) gap by developing physically accu-
agents to collaborate and share information to achieve com-
rate and graphically realistic digital twins of two scaled
mon objectives, competitive MARL introduces elements of
autonomous vehicles, viz. Nigel [13] and F1TENTH [14],
rivalryandadversaryamongagents,whereindividualsuccess
in Section II. We then present MARL formulation, training,
may be prioritized over coordination.
deployment and simulation-to-reality (sim2real) transfer for
Cooperative MARL [1]–[6] fosters an environment where
acooperativenon-zero-sumuse-caseofintersectiontraversal
autonomous vehicles collaborate to accomplish collective
(refer Fig. 1(a)) in Section III, and a competitive zero-
objectives such as optimizing traffic flow, enhancing safety,
sum use-case of head-to-head autonomous racing (refer
∗Theseauthorscontributedequally. Fig. 1(b)) in Section IV. These sections also discuss the
C. V. Samak, T. V. Samak, and V. N. Krovi are with the Department agent/environment parallelization techniques adopted to ac-
of Automotive Engineering, Clemson University International Center for
celerateMARLtraining.Finally,inSectionV,wepresentan
Automotive Research (CU-ICAR), Greenville, SC 29607, USA. Email:
{csamak, tsamak, vkrovi}@clemson.edu overallsummaryofourworkwithpotentialfuturedirections.
4202
raM
61
]OR.sc[
1v69901.3042:viXrausing a two-piece cubic spline, defined as F(S) =
(cid:26)
f (S); S ≤S <S
0 0 e, where f (S)=a ∗S3+b ∗S2+
f (S); S ≤S <S k k k
1 e a
c ∗S+d isacubicpolynomialfunction.Thefirstsegment
k k
of the spline ranges from zero (S ,F ) to an extremum
0 0
point (S ,F ), while the second segment ranges from the
e e
extremum point (S ,F ) to an asymptote point (S ,F ).
e e a a
Thetireslipisinfluencedbyfactorsincludingtirestiffness
iC , steering angle δ, wheel speeds iω, suspension forces
α
iF ,andrigid-bodymomentumiP.Thesefactorsimpactthe
(a) Modeling and simulation of Nigel’s digital twin. s
longitudinal and lateral components of the vehicle’s linear
velocity. The longitudinal slip iS of i-th tire is calculated
x
by comparing the longitudinal components of the surface
velocity of the i-th wheel v with the angular velocity iω of
x
thei-thwheel:iS
x
= ir∗i vω x−vx.ThelateralslipiS
y
depends
on the tire’s slip angle α and is determined by comparing
thelongitudinalv andlateralv componentsofthevehicle’s
(b) Modeling and simulation of F1TENTH’s digital twin. x y
linear velocity: iS =tan(α)= vy .
Fig.2:Simulationofvehicledynamics,sensorsandactuators
y |vx|
for Nigel and F1TENTH digital twins.
B. Sensor Models
Throttle (τ) and steering (δ) sensors are simulated using
II. DIGITALTWINCREATION an instantaneous feedback loop. Incremental encoders are
simulated by measuring the rotation of the rear wheels:
In order to bridge the sim2real gap [15], we created
iN = iPPR∗iGR∗iN , where iN and iPPR
accurate digital twin models of Nigel (refer Fig. 2(a)) as ticks rev ticks
respectivelyrepresentthemeasuredticksandbaseresolution
wellasF1TENTH(referFig.2(b))withinAutoDRIVESim-
(pulses per revolution) of the i-th encoder, while iGR and
ulator [16], [17] by calibrating them against their physical
iN respectively represent the gear ratio and output shaft
counterparts. The following sections only discuss the virtual rev
revolutions of the i-th motor.
vehicles,sensors,actuators,andenvironmentsadoptedinthis
work. Further details are available in [18]. Theindoorpositioningsystem(IPS)andinertialmeasure-
mentunit(IMU)aresimulatedbasedontemporally-coherent
From a computing perspective, the said simulation frame-
rigid-body transform updates of the vehicle {v} w.r.t. the
work was developed modularly using object-oriented pro-
(cid:20) (cid:21)
R t
gramming (OOP) constructs. This allowed selectively scal- world {w}: wT = 3×3 3×1 ∈ SE(3). IPS pro-
v 0 1
ing the parallel agent/environment instances on demand. 1×3
vides 3-DOF positional coordinates {x,y,z} of the vehicle,
Additionally, the simulator took advantage of CPU multi-
whileIMUsupplieslinearaccelerations{a ,a ,a },angular
threadingaswellasGPUinstancing(onlyifavailable)toef- x y z
velocities{ω ,ω ,ω },and3-DOForientationofthevehicle
ficientlyparallelizevarioussimulationobjectsandprocesses, x y z
as Euler angles {ϕ ,θ ,ψ } or quaternion {q ,q ,q ,q }.
with cross-platform support. x y z 0 1 2 3
LIDAR simulation employs iterative ray-casting
A. Vehicle Dynamics Models raycast{wT , R⃗, r } for each angle θ ∈
l max
The vehicle model is a combination of a rigid body and a [θ min :θ res :θ max] at a specified update rate. Here,
collection of sprung masses iM, where the total mass of the wT l = wT v × vT l ∈ SE(3) represents the relative
rigidbodyisdefinedasM =(cid:80)iM.Therigidbody’scenter transformation of the LIDAR {l} w.r.t the vehicle {v}
ofmass,X = (cid:80)iM∗iX,connectstheserepresentations, and the world {w}, R⃗ = [cos(θ) sin(θ) 0]T defines
COM (cid:80)iM
the direction vector of each ray-cast, r , r , θ
with iX representing the coordinates of the sprung masses. min max min
and θ denote the minimum and maximum linear and
The suspension force acting on each sprung mass is max
computedasiM∗iZ¨+iB∗(iZ˙−iz˙)+iK∗(iZ−iz),whereiZ angular ranges, and θ res represents the angular resolution
of the LIDAR. The laser scan ranges are determined by
andiz arethedisplacementsofsprungandunsprungmasses,
checking ray-cast hits and then applying a threshold to
and iB and iK are the damping and spring coefficients
the minimum linear range of the LIDAR, calculated as
of the i-th suspension, respectively. The unsprung mass (cid:40)
d if ray[i].hit and d ≥r
m is also subject to gravitational and suspension forces: ranges[i]= hit hit min,
im∗iz¨+iB∗(iz˙−iZ˙)+iK∗(iz−iZ). ∞ otherwise
Tire forces are computed based on the friction curve where ray.hit is a Boolean flag indicating whether
(cid:26)iF =F(iS ) a ray-cast hits any colliders in the scene, and
for each tire, represented as iFtx =F(iSx ), where iS x d = (cid:112) (x −x )2+(y −y )2+(z −z )2
ty y hit hit ray hit ray hit ray
and iS are the longitudinal and lateral slips of the i- calculates the Euclidean distance from the ray-cast source
y
th tire, respectively. The friction curve is approximated {x ,y ,z } to the hit point {x ,y ,z }.
ray ray ray hit hit hitC. Actuator Models B. State Space
Thedrivingactuatorsapplytorquetothewheels:iτ drive = The state space S for the intersection traversal problem
iI w ∗ iω˙ w, where iI w = 21 ∗ im w ∗ ir w2 represents the could be divided into observable so ⊂S and hidden sh ⊂S
momentofinertia,iω˙ w istheangularacceleration,im w isthe components. The observable component included the vehi-
mass, and ir w is the radius of the i-th wheel. Additionally, cle’s2Dposeandvelocity,denotedasso t =[p x,p y,ψ,v]t∈
the driving actuators simulate idle torque by applying an R4. The hidden component encompassed the agent’s goal
equivalent braking torque, i.e., iτ idle =iτ brake. coordinates, represented as sth = [g x,g y]
t
∈ R2. Thus,
The front wheels are steered at the commanded steering each agent could observe the pose and velocity of its peers
angle δ using a steering actuator. The individual turning (via V2V communication) but kept its goal location hidden
angles, δ l and δ r, for the left and right wheels, respectively, from others. Consequently, the complete state space of an
are computed based on the Ackermann steering geometry agent participating in this problem was a vector containing
defined by the wheelbase l and track width w, as follows: all observable and hidden states: s =(cid:2) so,sh(cid:3) .
 (cid:16) (cid:17) t t t
δ
l
=tan−1 2∗2 l∗ +l∗ wta ∗n ta( nδ ()
δ)
(cid:16) (cid:17) C. Observation Space
δ
r
=tan−1 2∗2 l∗ −l∗ wta ∗n ta( nδ ()
δ)
Based on the state space defined earlier, each agent, i
D. Environment Models (where 0 < i < N), employed an appropriate subset of its
(cid:104) (cid:105)
Simulated environments can be developed using Auto- sensor suite to collect observations: oi = gi,p˜i,ψ˜i,v˜i ∈
t
DRIVE’s modular infrastructure development kit (IDK), or t
R2+4(N−1). This included IPS for positional coordinates
imported from third-party tools and open standards. Specifi-
[p ,p ]t ∈ R2, IMU for yaw ψt ∈ R1, and incremental
x y
cally, the intersection traversal scenario was designed using
encoders for estimating vehicle velocity v ∈R1.
t
AutoDRIVEIDK,byconfiguringterrainmodules,roadmod- This formulation allowed gi =(cid:2) gi −pi,gi −pi(cid:3) t∈R2
ules,andtrafficelements.Ontheotherhand,theautonomous t x x y y
to represent the ego agent’s goal location relative to itself,
racing scenario was created based on the binary occupancy p˜ti =(cid:2) pj −pi,pj −pi(cid:3) t∈R2(N−1) todenotetheposition
gridmapofareal-worldF1TENTHracetrack,called“Porto”, x x y y
ofeverypeeragentrelativetotheegoagent,ψ˜ti =ψj−ψi ∈
usingathird-party3Dmodelingsoftware,andwasimported t t
RN−1 to express the yaw of every peer agent relative to the
and post-processed within AutoDRIVE Simulator to make it
ego agent, and v˜ti = vtj ∈ RN−1 to indicate the velocity
physically and graphically “sim-ready”.
of every peer agent. Here, i represented the ego agent, and
These environments are simulated by conducting mesh-
j ∈ [0,N −1] represented every other (peer) agent in the
mesh interference detection and computing contact forces,
scene, with a total of N agents.
frictional forces, momentum transfer, as well as linear and
angular drag acting on all rigid bodies in the scenario.
D. Action Space
III. COOPERATIVEMULTI-AGENTSCENARIO
Thevehiclesweredesignedasnon-holonomicrear-wheel-
Inspired by [6], this use-case encompassed both single- drive models featuring an Ackermann steering mechanism.
agentandmulti-agentlearningscenarios,whereeachagent’s As a result, the action space of an agent comprised of longi-
objectivewastotraversea4-wayintersectionwithoutcollid- tudinal (throttle/brake) and lateral (steering) motion control
ing or overstepping lane bounds. Each vehicle perceived its commands: ai = (cid:2) τi,δi(cid:3) ∈ R2. For longitudinal control,
t t t
intrinsic states and received limited state information from the throttle command τ was discretized as τ ∈ {0.5,1.0},
t t
its peers; no external sensing modalities were employed. which allowed the agents to slow down if necessary. The
steering command δ was discretized as δ ∈ {−1,0,1} to
A. Problem Formulation t t
steertheagentsleft,straight,orrightforcollisionavoidance.
In single-agent learning scenario, only the ego vehicle
learned to traverse the intersection, while peer vehicles were
E. Reward Function
controlled using a randomized heuristic. Vehicles shared
The extrinsic reward function was defined as follows:
theirstatesviaV2Vcommunicationandwereresettogether, 
r if safe traversal
making this scenario quite deterministic.  goal
In multi-agent learning scenario, all vehicles learned to r ti = −k p∗(cid:13) (cid:13)g ti(cid:13) (cid:13)
2
if traffic violation
traverse the intersection in a decentralized manner. Vehicles k r∗(0.001+(cid:13) (cid:13)g ti(cid:13) (cid:13) 2)−1 otherwise
shared their states via V2V communication and were reset This function rewarded each agent with r = +1 for
goal
independently, resulting in a highly stochastic scenario. successfully traversing the intersection and penalized them
In both the scenarios, the challenge revolved around au- proportional to their distance from the goal, represented
(cid:13) (cid:13)
tonomousnavigationinanunknownenvironment.Theexact as k
p
∗ (cid:13)g ti(cid:13) 2, for collisions or lane boundary violations.
structure/map of the environment was not known apriori to Finally, each agent was continuously rewarded inversely
any agent. Consequently, this decision-making problem was proportionaltoitsdistancefromthegoal,therebynegotiating
framed as a partially observable Markov decision process thesparse-rewardproblem.Thereward(k )andpenalty(k )
r p
(POMDP), which captured hidden state information through constants were set to 0.01 and 0.425 respectively, resulting
environmental observations. in a maximum reward/penalty of 1 per time step.(a) (b) (c) (d) (e) (f)
Fig. 3: Training results for (a)-(c) single-agent and (d)-(f) multi-agent intersection traversal scenarios: (a) and (d) denote
cumulative reward, (b) and (e) denote episode length, while (c) and (f) denote policy entropy w.r.t. training steps.
(a) (b) (c) (d) (e) (f)
Fig. 4: Deployment results for (a)-(c) single-agent and (d)-(f) multi-agent intersection traversal scenarios: (a) and (d) denote
first frozen snapshot, (b) and (e) denote second frozen snapshot, while (c) and (f) denote third frozen snapshot.
Thisencouragedtheagentstogetclosertotheirrespective H. Simulation Parallelization
goals,reducingpenaltiesandultimatelyleadingtoapositive
This use-case adopted environment parallelization for ac-
reward,r .Thisapproachnotonlyexpeditedconvergence
goal celerating the MARL training. Fig. 5(a) depicts a snapshot
but also restricted reward hacking.
of25intersection-traversalenvironmentstraininginparallel.
F. Optimization Problem This clearly differentiates the architecture of environment
parallelization from agent/actor parallelization (refer Fig.
The extrinsic reward function described earlier motivated
8(a)).
eachagenttomaximizeitsexpectedfuturediscountedreward
(cid:34) ∞ (cid:35) We analyzed the effect of parallelizing an intersection-
(cid:88)
argmax E γtr by learning a policy π (a |o ), traversalenvironmentfromasingleinstance(4agents)upto
t θ t t
πθ(at|ot) t=0 25instances(100agents)traininginparallel.Allthetraining
which over time, transitioned into the optimal policy π∗.
experiments were carried out on a single laptop PC having
G. Training 12th Gen Intel Core i9-12900H 2.50 GHz CPU, NVIDIA
GeForce RTX 3080 Ti GPU and 32.0 GB (31.7 GB usable)
Theagentsweretrainedusinganintegratedmachinelearn-
RAM, with PyTorch 1.7.1 installed over CUDA 11.0.
ing (ML) framework [19] within AutoDRIVE Simulator.
As observed in Fig. 5(b)-(c) the reduction in training time
At each time step t, each parallelized agent i collected an
observation vector oi and an extrinsic reward ri. Based on wasquitenon-linearsincethesimulationworkloadincreased
t t
these inputs, it took an action ai determined by the policy with increasing parallelization. As a result, we can notice
t
the curves nearly saturate after a point, which is subject
π ,whichwasrecursivelyupdatedusingtheproximalpolicy
θ
to change with a different hardware/software configuration.
optimization(PPO)algorithm[20]tomaximizetheexpected
Additionally, it should be noted that parallelization beyond
future discounted reward.
a certain point can hurt, wherein the increased simulation
This use-case employed a fully connected neural network
workload may slow down the training so much that parallel
(FCNN) as a function approximator for π (a |o ). The
θ t t
network had R14 inputs, R1 outputs, and 3 hidden layers policy optimization can no longer accelerate it.
with 128 neural units each. The policy parameters θ ∈ Rd
I. Deployment
were defined in terms of the network’s parameters.
Fig. 3 depicts key training metrics used to analyze the The trained policies were deployed onto the simulated
learning process. A general indication of “good” training is vehicles,separatelyforthesingle-agentandmulti-agentsce-
that the cumulative reward is maximized and then saturated, narios. As previously mentioned, the single-agent scenario
the episode length is adequate (longer duration implies waslessstochastic,andtheegovehiclecouldsafelytraverse
agents wandering off in the environment, while very short the intersection in most cases. In contrast, the multi-agent
duration may be indicative of agents colliding/overstepping scenario was highly stochastic, resulting in a significantly
lane bounds), and the policy entropy (i.e., randomness) has lower success rate, especially with all vehicles navigating
decreased steadily as the training progressed. It is to be the intersection simultaneously.
noted thatthe predominantcause forthe differencein trends Fig. 4(a)-(c) present three key stages of the single-agent
of training metrics for single and multi-agent scenarios is intersection traversal scenario. The first stage depicts the
the higher stochasticity of the multi-agent scenario, which is ego vehicle approaching the conflict zone, where it could
especially evident from the policy entropy. potentially collide with peer vehicles. The second stage(a) (b) (c)
Fig. 5: Computational results for training intersection traversal scenario through environment parallelization: (a) depicts
a snapshot of 25 environments training in parallel, (b) denotes the training time for different levels of environment
parallelization, and (c) denotes the percentage reduction in training time for different levels of environment parallelization.
shows the vehicle executing a left turn to avoid collisions. A. Problem Formulation
The third stage illustrates the vehicle performing a final
Thisuse-caseaddressedtheproblemofautonomousracing
correctiontoreachitsgoal.Fig.4(d)-(f)displaythreecritical
in an unknown environment. The exact structure/map of the
stages of the multi-agent intersection traversal scenario. In
environment was not known apriori to any agent. Conse-
the first frame, vehicles 1 and 4 successfully avoid collision.
quently, this decision-making problem was also framed as
The second frame depicts vehicle 1 finding a gap between
a POMDP, which captured hidden state information through
vehicles2and3toreachitsgoal.Inthethirdframe,vehicles
environmental observations.
2 and 3 avoid collision, while vehicle 4 approaches its goal,
We adopted an equally weighted hybrid imitation-
and vehicle 1 is re-spawned.
reinforcementlearningarchitecturetoprogressivelyinculcate
autonomousdrivingandracingbehaviorsintotheagents.We
J. Sim2Real Transfer
hypothesized that such a hybrid learning architecture would
After simulation-based verification of the trained policies,
guidetheagents’exploration,therebyreducingtrainingtime
they were transferred onto a physical vehicle, which was
significantly. Consequently, we recorded 5 laps worth of in-
embedded within a true digital twin framework as depicted
dependentdemonstrationdatasetsforeachagentbymanually
inFig.1(a)(capturedat1Hz).Particularly,asinglephysical
driving the vehicles in sub-optimal trajectories.
vehicle was deployed in the loop with 3 virtual vehicles,
which collectively embodied the multi-agent system. The B. Observation Space
physical vehicle used LIDAR-based range flow odometry At each time step t, the agent collected a vectorized
[12] for onboard state estimation (a map-based localization observation: oi = (cid:2) vi,mi(cid:3) ∈ R28. These observations
t t t
algorithm or a motion capture system can substitute this were obtained using velocity estimation and exteroceptive
stage). The states of the physical vehicle were then relayed ranging modalities mounted on the virtual vehicle(s). Here,
backtothesimulatortoupdateitsdigitaltwinrepresentation vi ∈ R1 represents the forward velocity of i-th agent, and
t
in the virtual environment. The digital twin then utilized the mi = (cid:2)1mi,2mi,··· ,27mi(cid:3) ∈ R27 is the measurement
t t t t
simulationframeworktoestablishV2Xcommunicationwith vector providing 27 range readings up to 10 meters. These
its virtual peers and planned its future actions in the digital readings are uniformly distributed over 270◦ around each
space. These action sequences were then relayed back to the side of the heading vector, spaced 10◦ apart.
physical vehicle to be executed in the real world.
C. Action Space
This experiment validated the sim2real transferability of
the trained MARL policy, without over-utilizing the hard- The action space to control the Ackermann-steered ve-
ware resources. It is worth mentioning that the digital twin hicles was defined as ai = (cid:2) τi,δi(cid:3) ∈ R2. Here, τi ∈
t t t t
framework discussed herein supports incrementally deploy- {0.1,0.5,1.0} represent the discrete throttle commands at
ingmultiplevehiclesintherealworldbasedonthehardware 10%, 50% and 100% PWM duty cycles for torque limited
resources at disposal. Such incremental digital twin deploy- (85.6N-m)driveactuators,andδi ∈{−1,0,1}representthe
t
ments can also ensure minimal physical damage, especially discretesteeringcommandsforleft,straight,andrightturns.
while deploying nascent MARL algorithms.
D. Reward Function
IV. COMPETITIVEMULTI-AGENTSCENARIO The agents were guided by the following signals:
Inspired by [9], this use-case encompassed a multi-agent • BehavioralCloning:Thiswasthecoreimitationlearn-
learning scenario, where each agent’s objective was mini- ingalgorithm,whichupdatedthepolicyinasupervised
mizing its lap time without colliding with the track or its fashion with respect to the recorded demonstrations.
opponent. Each vehicle possessed intrinsic state information The behavioral cloning (BC) [21] update was carried
and sparse LIDAR measurements; no state information was out every once in a while, mutually exclusive of the
shared among the competing vehicles. reinforcement learning update.(a) (b) (c) (d) (e) (f)
Fig. 6: Training results for multi-agent autonomous racing: (a) denotes BC loss, (b) denotes GAIL reward, (c) denotes
curiosity reward, (d) denotes extrinsic reward, (e) denotes episode length, and (f) denotes policy entropy w.r.t. training steps.
(a) (b) (c) (d) (e) (f)
Fig.7:Deploymentresultsformulti-agentautonomousracing:(a)-(c)denotethreefrozensnapshotsofablock-block-overtake
sequence, and (d)-(f) denote three frozen snapshots of a let-pass-and-overtake sequence.
• GAIL Reward: The generative adversarial imitation whichsimultaneouslyalsominimizedthebehavioralcloning
learning (GAIL) reward [22] gr
t
ensured that the agent
loss L : argmax
η(cid:32) E(cid:34) (cid:88)∞ γtri(cid:35)(cid:33)
−(1−η)L ,
optimized its actions safely and ethically by rewarding BC t BC
proportional to the closeness of new observation-action π θi(at|ot) t=0
where η is a weighting factor that controls the degree
pairs to those from the recorded demonstrations. This
of imitation versus reinforcement learning. The cumulative
allowed the agent to retain its autonomous driving
reward in this case was a sum of GAIL, curiosity, and
ability throughout the training process. extrinsic rewards, i.e., ri = gri+cri+eri.
• Curiosity Reward: The curiosity reward [23] cr t pro- t t t t
moted exploration by rewarding proportional to the F. Training
difference between predicted and actual encoded obser- This use-case also employed a fully connected neural
vations.Thisensuredthattheagenteffectivelyexplored network (FCNN) as a function approximator for π (a |o ).
θ t t
itsenvironment,eveniftheextrinsicrewardwassparse. The network had R28 inputs, R2 outputs, and three hidden
• Extrinsic Reward:InthecontextofRL,theobjectives layerswith128neuralunitseach.Thepolicyparametersθ ∈
of lap time reduction and motion constraints were Rd were defined in terms of the network’s parameters. The
handled using a novel extrinsic reward function er , policywastrainedtopredictthrottleandsteeringcommands
t
which guided the agent towards optimal behavior using directly based on collected observations, utilizing the inte-
thePPOalgorithm[20].Theagentreceivedarewardof grated ML framework [19] within AutoDRIVE Simulator.
r =+0.01 for passing each of the 19 check- Fig. 6 depicts key training metrics used to analyze the
checkpoint
points c , where i ∈ [A,B,··· ,S] on the racetrack, learning process. A general indication of “good” training
i
r = +0.1 upon completing a lap, r = +0.7 is that the behavioral cloning loss has decayed smoothly,
lap bestlap
upon achieving a new best lap time, and a penalty of the GAIL, curiosity, and extrinsic rewards are maximized
r = −1 for colliding with the track bounds or and then saturated, the episode length is adequate (longer
collision
peer agent (in which case both agents were penalized duration implies agents driving slowly, while very-short
equally). Additionally, the agent received continuous duration may be indicative of agents colliding without lap
rewards proportional to its velocity v , encouraging it completion), and the policy entropy (i.e., randomness) has
t
to optimize its trajectory spatio-temporally. decreasedsteadilyasthetrainingprogressed.Itistobenoted
 that the non-zero offset in behavioral cloning loss indicates
r
rc co hl el cis ki po on
int
i if
f
c co hl el cis ki po on ino tc ic sur ps
assed
t th ha eyt t hh ae veag ee xn pt ls orh ea dv te heno st tao teve sr p- afi ct et qh ue itd ee wm eo ln ls tt ora mti ao xn is m; ir za eth te hr e,
er ti = r lap if completed lap extrinsic reward by adopting aggressive “racing” behaviors.
r
0b .0es 1t ∗la vp
i
oif thn ee rw wib seest lap time is achieved
G. Simulation Parallelization
t This use-case adopted actor/agent parallelization for ac-
E. Optimization Problem celerating the MARL training. The core actor/agent par-
Thetaskofhead-to-headautonomousracingusingdemon- allelization architecture involved the creation and isolation
stration guided reinforcement learning converged to a multi- of appropriate simulation objects in different virtual “lay-
objective problem of maximizing the expected future dis- ers”. For instance, the environment was simulated on the
counted reward by learning an optimal policy π∗(a |o ), “default” layer, whereas each family of the multi-agent
θ t t(a) (b) (c)
Fig. 8: Computational results for training autonomous racing scenario through actor/agent parallelization: (a) depicts a
snapshot of 10×2 agents training in parallel, (b) denotes the training time for different levels of actor/agent parallelization,
and (c) denotes the percentage reduction in training time for different levels of actor/agent parallelization.
system was simulated on a different layer, with the agents overtake it. Fig. 7(d)-(f) display three snapshots of a let-
of the same family being simulated on the same layer. This pass-and-overtake sequence, wherein the blue agent found
allowed selective interactions and collision checks between a gap between the red agent and inside edge of the track
specificlayersofthesimulation.Parallelizationofperception and opportunistically overtook it. However, due to its under-
modalities involved appropriately instantiating and isolating steering characteristic, it went wider in the corner, thereby
the interoceptive sensors (e.g., encoders, IPS, IMU, etc.) allowingtheredagenttoovertakeitandre-claimtheleading
on respective layers. Exteroceptive sensors such as cameras position.
were effectively parallelized by enabling culling mask bits
I. Sim2Real Transfer
only for specific layers, whereas LIDARs were parallelized
by returning raycast hits only for specific layers. Fig. 8(a) AshighlightedinSectionIII-J,wefirstverifiedtheability
depictsasnapshotof10×2agentstraininginparallelwithin of trained policies to complete several laps around the race
the same environment. Notice that the agents can collide, track, both with and without an opponent. It was only
perceive or interact only with their “true” opponents, and after extensive verification, that the trained policies were
none of the “parallel” instances. This clearly differentiates transferred from simulation to reality.
the architecture of actor/agent parallelization from environ- The real-world experiments were conducted using a true
ment parallelization (refer Fig. 5(a)). digital twin framework (refer Fig. 1(b), captured at 1 Hz),
We analyzed the effect of parallelizing the 2-agent adver- so as to exploit the real-world characteristics of the vehicle
sarialracingfamilyfromasingleinstance(2agents)upto10 dynamics and tire-road interconnect while being resource-
suchfamilies(20agents)traininginparallel,withinthesame altruistic by augmenting the environmental elements (e.g.
environment. All the training experiments were carried out race track) and peer agents (e.g. other F1TENTH vehicles)
on a single laptop PC with the same hardware and software in the digital space. We utilized the Vedder electronic speed
configuration as noted in Section III-H. controller (VESC) based odometry to estimate the states
As observed in Fig. 8(b)-(c) the reduction in training time of the physical vehicle, which were then relayed back to
was quite non-linear in this case as well. This could be pri- the simulator to update its digital twin representation in the
marilyattributedtotheincreaseinsimulationworkloadwith virtualenvironment.Thesimulated2DLIDARonthedigital
increasing parallelization. As a result, training time started twin then filled in the remaining observation space of the
saturating after a point, which is subject to change with a ego agent, which planned its future actions in the digital
different hardware/software configuration. Additionally, as space. These action sequences were then relayed back to the
mentioned earlier, parallelization beyond a certain point can physical vehicle to be executed in the real world.
hurt, wherein the increased simulation workload may slow However, in this case, owing to the requirement of agile
down the training so much that parallel policy optimization maneuvers, it was observed that even the slightest commu-
can no longer accelerate it. nication latency and occasional packet drops would affect
the policy to not yield the right actions at the right time. As
H. Deployment
a result, the velocity of the physical vehicle was limited to
The trained policies were deployed onto the respective 3 m/s to account for such delays; we intend to remedy this
virtual vehicles, which were set to race head-to-head on networking-relatedbottleneckinthefuture.Nevertheless,the
the same track with a phase-shifted initialization (as in real virtualpeervehicleoperatingcompletelyinthedigitalspace
F1TENTH competitions). was driving at a much higher velocity limit of 10 m/s.
Fig. 7(a)-(c) present three snapshots of a block-block- Finally, as described earlier, the proposed digital twin
overtake sequence, wherein the red agent kept blocking the frameworkisflexibleenoughtoallowadding/removingenvi-
blue agent throughout the straight, but the blue agent took ronmental components, sensor modules, and vehicles in the
a wider turn with higher velocity and leveraged its under- real world depending on the space availability and hardware
steering characteristic to cut in front of the red agent and resources at disposal.V. CONCLUSION [8] Y. Song, H. Lin, E. Kaufmann, P. Du¨rr, and D. Scaramuzza, “Au-
tonomous Overtaking in Gran Turismo Sport Using Curriculum Re-
This work presented a scalable and parallelizable multi- inforcement Learning,” in 2021 IEEE International Conference on
RoboticsandAutomation(ICRA),2021,pp.9403–9409.
agent reinforcement learning framework for imbibing co-
[9] C.V.Samak,T.V.Samak,andS.Kandhasamy,“AutonomousRacing
operative and competitive behaviors within autonomous ve- using a Hybrid Imitation-Reinforcement Learning Architecture,”
hicles. We discussed representative use-cases for each be- 2021.[Online].Available:https://arxiv.org/abs/2110.05437
[10] J.Betz,H.Zheng,A.Liniger,U.Rosolia,P.Karle,M.Behl,V.Krovi,
havior type in detail, where we deliberately formulated the
and R. Mangharam, “Autonomous Vehicles on the Edge: A Survey
two problems with distinct observation spaces and reward on Autonomous Vehicle Racing,” IEEE Open Journal of Intelligent
functions, but more importantly, we also varied the learning TransportationSystems,vol.3,pp.458–488,2022.
[11] T. Samak, C. Samak, S. Kandhasamy, V. Krovi, and M. Xie,
architecture from vanilla MARL to demonstration-guided
“AutoDRIVE: A Comprehensive, Flexible and Integrated Digital
MARL. We analyzed the training metrics in each case and Twin Ecosystem for Autonomous Driving Research & Education,”
also noted the non-linear effect of agent/environment par- Robotics, vol. 12, no. 3, p. 77, May 2023. [Online]. Available:
http://dx.doi.org/10.3390/robotics12030077
allelization on the training time, with a hardware/software-
[12] T. V. Samak and C. V. Samak, “AutoDRIVE - Technical Report,”
specific point of diminishing return. Finally, we presented 2022.[Online].Available:https://doi.org/10.48550/arXiv.2211.08475
a resource-aware sim2real transfer of the trained policies [13] C.Samak,T.Samak,andV.Krovi,“TowardsMechatronicsApproach
of System Design, Verification and Validation for Autonomous
using a single physical vehicle, which was plugged into the
Vehicles,”in2023IEEE/ASMEInternationalConferenceonAdvanced
proposed digital twin framework to interact with its virtual Intelligent Mechatronics (AIM), 2023, pp. 1208–1213. [Online].
peers within the virtual environment. Additionally, we also Available:https://doi.org/10.1109/AIM46323.2023.10196233
[14] M. O’Kelly, V. Sukhil, H. Abbas, J. Harkins, C. Kao, Y. V. Pant,
discussedtheemergingpossibilitiesofadaptingandadopting
R. Mangharam, D. Agarwal, M. Behl, P. Burgio, and M. Bertogna.
this approach. (2019)F1/10:AnOpen-SourceAutonomousCyber-PhysicalPlatform.
A natural extension to this work would be to apply the [Online].Available:https://arxiv.org/abs/1901.08567
[15] C. V. Samak, T. V. Samak, and V. Krovi, “Towards Sim2Real
proposed approach to mid-scale and full-scale autonomous
Transfer of Autonomy Algorithms using AutoDRIVE Ecosystem,”
vehicles with different perception and actuation configura- 2023.[Online].Available:https://doi.org/10.48550/arXiv.2307.13272
tions. Additionally, analyzing MARL training across differ- [16] T. V. Samak, C. V. Samak, and M. Xie, “AutoDRIVE Simulator: A
Simulator for Scaled Autonomous Vehicle Research and Education,”
enttypesofcomputingplatforms(bothlocalaswellascloud
in 2021 2nd International Conference on Control, Robotics and
computing resources) and configurations could potentially IntelligentSystem,ser.CCRIS’21. NewYork,NY,USA:Association
help us gain better insights from a computing perspec- for Computing Machinery, 2021, p. 1–5. [Online]. Available:
https://doi.org/10.1145/3483845.3483846
tive. Finally, innovation from the MARL front could be in
[17] T. V. Samak and C. V. Samak, “AutoDRIVE Simulator - Technical
the form of applying physics-informed RL to multi-agent Report,” 2022. [Online]. Available: https://doi.org/10.48550/arXiv.
settings for modeling agents’ state transitions, formulating 2211.07022
[18] T. V. Samak, C. V. Samak, and V. N. Krovi, “Towards
physics-guided reward functions, modeling and quantifying
Validation of Autonomous Vehicles Across Scales using an
uncertainties, ensuring safety guarantees, etc. Integrated Digital Twin Framework,” 2024. [Online]. Available:
https://doi.org/10.48550/arXiv.2402.12670
[19] A. Juliani, V.-P. Berges, E. Teng, A. Cohen, J. Harper, C. Elion,
REFERENCES C. Goy, Y. Gao, H. Henry, M. Mattar, and D. Lange, “Unity: A
General Platform for Intelligent Agents,” 2018. [Online]. Available:
[1] S. H. Semnani, H. Liu, M. Everett, A. de Ruiter, and J. P. How, https://arxiv.org/abs/1809.02627
“Multi-agentMotionPlanningforDenseandDynamicEnvironments [20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
via Deep Reinforcement Learning,” IEEE Robotics and Automation “Proximalpolicyoptimizationalgorithms,”2017.[Online].Available:
Letters,vol.5,no.2,pp.3221–3226,2020. https://arxiv.org/abs/1707.06347
[21] M.BainandC.Sammut,“AFrameworkforBehaviouralCloning,”in
[2] P. Long, T. Fan, X. Liao, W. Liu, H. Zhang, and J. Pan, “Towards
MachineIntelligence15,1995.
Optimally Decentralized Multi-Robot Collision Avoidance via Deep
[22] J.HoandS.Ermon,“GenerativeAdversarialImitationLearning,”in
ReinforcementLearning,”in2018IEEEInternationalConferenceon
Proceedingsofthe30thInternationalConferenceonNeuralInforma-
RoboticsandAutomation(ICRA),2018,pp.6252–6259.
tionProcessingSystems,ser.NIPS’16. RedHook,NY,USA:Curran
[3] S. Aradi, “Survey of Deep Reinforcement Learning for Motion
AssociatesInc.,2016,p.4572–4580.
PlanningofAutonomousVehicles,”IEEETransactionsonIntelligent
[23] D.Pathak,P.Agrawal,A.A.Efros,andT.Darrell,“Curiosity-Driven
TransportationSystems,vol.23,no.2,pp.740–759,2022.
Exploration by Self-Supervised Prediction,” in Proceedings of the
[4] D.Wang,H.Deng,andZ.Pan,“MRCDRL:Multi-RobotCoordination
34thInternationalConferenceonMachineLearning-Volume70,ser.
with Deep Reinforcement Learning,” Neurocomputing, vol. 406, pp.
ICML’17. JMLR.org,2017,p.2778–2787.
68–76, 2020. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/S0925231220305932
[5] X.Zhou,P.Wu,H.Zhang,W.Guo,andY.Liu,“LearntoNavigate:
Cooperative Path Planning for Unmanned Surface Vehicles Using
Deep Reinforcement Learning,” IEEE Access, vol. 7, pp. 165262–
165278,2019.
[6] K. Sivanathan, B. K. Vinayagam, T. Samak, and C. Samak,
“Decentralized Motion Planning for Multi-Robot Navigation using
DeepReinforcementLearning,”in20203rdInternationalConference
on Intelligent Sustainable Systems (ICISS), 2020, pp. 709–
716. [Online]. Available: https://doi.org/10.1109/ICISS49785.2020.
9316033
[7] F.Fuchs,Y.Song,E.Kaufmann,D.Scaramuzza,andP.Du¨rr,“Super-
Human Performance in Gran Turismo Sport Using Deep Reinforce-
mentLearning,”IEEERoboticsandAutomationLetters,vol.6,no.3,
pp.4257–4264,2021.