STATE-SEPARATED SARSA: A PRACTICAL SEQUENTIAL
DECISION-MAKING ALGORITHM WITH RECOVERING REWARDS
YutoTanimoto KenjiFukumizu
DepartmentofStatisticalScience TheInstituteofStatisticalMathematics
TheGraduateUniversityforAdvancedStudies fukumizu@ism.ac.jp
tanimoto@ism.ac.jp
ABSTRACT
While many multi-armed bandit algorithms assume that rewards for all arms are constant across
rounds,thisassumptiondoesnotholdinmanyreal-worldscenarios.Thispaperconsidersthesettingof
recoveringbandits(Pike-BurkeandGrunewalder,2019),wheretherewarddependsonthenumberof
roundselapsedsincethelasttimeanarmwaspulled. Weproposeanewreinforcementlearning(RL)
algorithmtailoredtothissetting,namedtheState-SeparateSARSA(SS-SARSA)algorithm,which
treatsroundsasstates. TheSS-SARSAalgorithmachievesefficientlearningbyreducingthenumber
ofstatecombinationsrequiredforQ-learning/SARSA,whichoftensuffersfromcombinatorialissues
forlarge-scaleRLproblems. Additionally,itmakesminimalassumptionsabouttherewardstructure
andofferslowercomputationalcomplexity. Furthermore,weproveasymptoticconvergencetoan
optimalpolicyundermildassumptions. Simulationstudiesdemonstratethesuperiorperformanceof
ouralgorithmacrossvarioussettings.
1 Introduction
Themulti-armedbandit(MAB)problem(LattimoreandSzepesvári,2020)isasequentialdecision-makingproblem
betweenanagentandenvironment. Foreachround, theagentpullsanarmfromafixedsetandreceivesareward
fromtheenvironment. Theobjectiveistomaximizethecumulativerewardsoveracertainnumberofrounds. Thisis
equivalenttoregretminimization,whichiscommonlyusedtoevaluatealgorithms(LattimoreandSzepesvári,2020).
Forsuperiorperformance,thekeyingredientisanexploration-exploitationtradeoff. Duringtheinitialrounds,theagent
exploresarmsatrandomtogatherinformationabouttheenvironment. Afterthat,theagentexploitstheknowledge
obtainedduringtheexplorationphasetochoosethebestarm.
TheMABframeworkiswidelyusedandfindsapplicationsinvariousdomains(Bouneffoufetal.,2020). Forinstance,
inthecontextofitemrecommendation(Ganganetal.,2021),MABalgorithmscanbeappliedbyinterpretingarms
asitemsandrewardsasconversionrates. Anotherexampleisdynamicpricing(Misraetal.,2019),wherearmsand
rewardscorrespondtopriceandprofit,respectively. Moreover,inalanguage-learningapplicationdescribedinYancey
andSettles(2020), MABalgorithmsareemployedforpushnotifications,witharmsrepresentingnotificationsand
rewardsrepresentingappusage.
Manybanditalgorithmsassumerewardstationarity,meaningconstantrewardsacrossrounds(GarivierandMoulines,
2011). Insuchcases,continuingtodrawthearmwiththehighestexpectedrewardisoptimalforcumulativerewards.
However,thisassumptiondoesnotholdintheexamplespresentedabove. Instead,rewardsoftendependonthetiming
ofarmselections. Inrecommendationsystems,forexample,commoditiesshouldbepurchasedmorefrequentlythan
otherexpensiveitems. Assumingtherewardstationarity,banditalgorithmswouldrepeatedlyrecommendthesame
item. Onthecontrary,thepurchaseprobabilitywouldincreaseiftherecommendationismadeaftersuggestingavariety
ofitems. Indynamicpricing, itmaybemoreprofitable, inthelongrun, todiscountoccasionallythantocontinue
discountingforimmediateprofit. Similarly,occasionalpushnotificationsmaybemoreeffectiveatcapturingattention
thanfrequentnotificationsconveyingthesamemessage,supportedby(YanceyandSettles,2020)throughofflineand
onlineexperiments.
4202
raM
81
]GL.sc[
1v02511.3042:viXraToaddressthissituation,weconsiderthecasewheretherewardfunctiondependsonthetimeelapsedsincethelast
armwaspulled,knownasrecoveringbandits(Pike-BurkeandGrunewalder,2019). Variousalgorithmshavebeen
proposedintheMABframework,butmostassumespecificrewardstructures(YanceyandSettles(2020);Simchi-Levi
etal.(2021);KleinbergandImmorlica(2018);Leqietal.(2021);Moriwakietal.(2019);Warlopetal.(2018))orare
computationallyexpensive(Laforgueetal.,2022). Implementingsuchalgorithmsforunknownrewardfunctionsovera
longsequenceofroundswouldbechallenging.
Analternativeapproachtodealingwiththechangeinrewardsistoapplyareinforcementlearning(RL)algorithm
(Sutton and Barto, 2018), considering states as the elapsed rounds for each arm. However, popular tabular RL
algorithms, such as Q-learning (Watkins and Dayan, 1992) and SARSA (Rummery and Niranjan, 1994)), face a
combinatorialproblemintermsofthenumberofarmsandstates. Specifically,weneedtoestimatetheQ-functionfor
allpossiblecombinationsacrossthemaximumnumberofstatesforeacharm. Consequently,tabularRLalgorithmsare
computationallyprohibitive,exceptforasmallnumberofarms.
Tomitigatethecombinatorialissue,thispaperproposesanewtabularSARSA,calledState-SeparatedSARSA(SS-
SARSA).WeintroduceforeacharmState-SeparatedQ-function(SS-Q-function),whichdependsonthestatesforboth
theassociatedandapulledarm,andsimilarlyupdatethemtothestandardtabularSARSA.Theupdatethusrequires
onlythestatesofthetwoarms. Asaresult,thenumberofQfunctionstobeestimatedissignificantlyreduced,leading
tomoreefficientestimation. Furthermore,thisalgorithmguaranteesconvergencetoBellmanoptimalityequationfor
Q-functions, meaning it achieves an optimal policy asymptotically. Additionally, since our algorithm is a slightly
modifiedversionofSARSA,itcanbesolvedinlineartimeforroundsandisfasterthantherelatedwork(Laforgue
etal.,2022). Also,weintroduceanewpolicycalledUniform-Explore-First;duringtheexplorationphase,itpullsthe
leastfrequentlyselectedarmforgivenstatestoupdateQ-functionsuniformly. Subsequently,theagentpullsarmsto
maximizecumulativerewards. Notethatevenrandomexplorationsuchasϵ-greedy(SuttonandBarto,2018)does
notupdateQ-functionsuniformlyinoursetting. Finally,comparedtopopularRLalgorithmsandrecoveringMAB
algorithms(Pike-BurkeandGrunewalder,2019),simulationresultsacrossvariousrewardsettingsdemonstratethe
superiorityofouralgorithmintermsofcumulativerewardsandoptimalpolicy.
Thecontributionsofthisworkaresummarizedasfollows.
• Intherecoveringrewardsetting,theproposedalgorithmSS-SARSAcanmitigatethecombinatorialcomputa-
tionandcanbesolvedinlineartime.
• Itistheoreticallyguaranteedthattheproposedalgorithmobtainsoptimalpolicyasymptoticallyforanyreward
structure.
• Theproposedpolicy,Uniform-Explore-First,updateseachQ-functionuniformlyforefficientexploration.
• Invarioussettings,simulationresultsshowthesuperiorityofouralgorithmintermsofcumulativerewards
andoptimalpolicyoverrelatedworks.
Theremainderofthispaperisorganizedasfollows. Section2reviewsrelatedliteratureanddiscussesthedifferences
fromourwork. WedefineaformalproblemsettinginSection3andtheproposedalgorithminSection4. InSection5,
wepresentaconvergenceanalysisfortheproposedalgorithm. Section6showsthesimulationresultsandadvantages
overtherelatedmethods. Finally,westateourconclusioninSection7.
2 RelatedWork
Insequentialdecision-makingproblems,twotypicalapproachesareMulti-ArmedBandits(MAB)andReinforcement
Learning(RL).MABdoesnotuseastateinmodeling,whileRLincorporatesastate. Recoveringbanditproblemshave
predominantlybeenaddressedintheMABcontext,sowemainlysurveythatarea.
Instationarybandits,whereexpectedrewardsareconstantovertimeforeacharm,algorithmsaimtochoosethearm
withthehighestexpectedvaluetomaximizecumulativerewards. Numerousalgorithmshavebeenproposedforboth
parametricandnonparametricrewardsettings,suchasKL-UCB(Laietal.,1985;GarivierandCappé,2011),Thompson
sampling(Thompson,1933;ChapelleandLi,2011),ϵ-greedy(SuttonandBarto,2018),andUCB1(Aueretal.,2002).
However,thissettingignoresrewardchanges,acrucialaspectaddressedinthispaper.
Thenon-stationarybanditproblemconsidersscenarioswhererewardscanchangeovertime. Restlessbanditsallow
rewardstovaryoverroundsindependentlyofthehistoryofpulledarms,incorporatingsettingslikepiecewisestationary
rewards(GarivierandMoulines,2011;Liuetal.,2018)andvariationbudgets(Besbesetal.,2014;Russacetal.,2019).
However,eveninthesesettings,thevariationofrewardsduetothehistoryofpulledarmsisnotaccountedfor.
2Incontrast,RestedBanditsinvolverewardsthatdependonthehistoryofthearmspulled. Oneofthetypicalsettingsis
thateacharm’srewardchangesmonotonicallyeachtimethearmispulled(Heidarietal.,2016). Examplesinclude
Rotting bandits (Levine et al., 2017; Seznec et al., 2019), handling monotonically decreasing rewards, and Rising
bandits(Lietal.,2020;Metellietal.,2022),dealingwithmonotonicincrease.
AnothersettinginRestedBanditsisRecoveringbandits(Pike-BurkeandGrunewalder,2019)(orRechargingbandits
(Kleinberg and Immorlica, 2018)), where rewards depend on the elapsed rounds since the last pull for each arm.
Numerous algorithms have been proposed in this context, with many assuming a monotonous increase in rewards
as rounds progress (Yancey and Settles (2020); Simchi-Levi et al. (2021); Kleinberg and Immorlica (2018)). An
alternativeapproachinvolvesfunctionalapproximationwithpastactionsascontexts,exemplifiedbystochasticbandits
with time-invariant linear dynamical systems (Leqi et al., 2021), contextual bandits (Moriwaki et al., 2019), and
linear-approximationRL(Warlopetal.,2018). Incontrasttotheseapproaches,weproposeanalgorithmthatdoesnot
relyonaspecificstructureoftherewards.
Severalarticlesmakefewerassumptionsaboutrewards(Laforgueetal.,2022;Pike-BurkeandGrunewalder,2019).
Laforgueetal.(2022)proposeanalgorithmbasedonCombinatorialSemi-Bandits,applyingintegerlinearprogramming
foreachblockofprespecifiedroundstodeterminethesequenceofpullingarms. However,itstimecomplexityismore
thanquadraticintotalrounds,makingitimpracticalforlargetotalrounds. Incontrast,ouralgorithm,amodifiedversion
ofSARSA,canbecomputedinlineartime.
Pike-BurkeandGrunewalder(2019)usesGaussianprocess(GP)regressionandprovestheBayesiansublinearregret
boundwithoutrewardmonotonicityandconcavity. However,theiralgorithmsonlyconsideredshort-termlookahead
anddidnotguaranteetoachievetheoptimalpolicyinthelongrun,asdeemedinRL.Incontrast,ourRLapproachcan
realizetheoptimalpolicyasymptotically.
3 ProblemSetting
Inthissection,weintroducerecoveringbandits(Pike-BurkeandGrunewalder,2019)withintheframeworkoftheMarkov
DecisionProcess(MDP)tofacilitateRLalgorithms. The(discounted)MDPisdefinedasM=(A,S,f,r,γ),where
A=[K]:={1,2,··· ,K}istheindexsetofK arms1,Sk :=[s ]denotesthestatesofthearmk(k =1,...,K),
max
andS =(cid:81)K Skistheirdirectproduct. Additionally,weletf :Sk×A→Skdenoteadeterministicstatetransition
k=1 k
functionforarmk,andf =(f ,f ,··· ,f )abundledvector. Thestochasticboundedrewardfunctionisdenotedby
1 2 K
r :S×A→R,andγ ∈(0,1]servesasadiscountrate.
Key distinctions from standard MDP lie in the state structure and the state transition. In the K-dimensional state
s:=(s ,s ,··· ,s ,··· ,s ),thecomponents ∈[s ]signifiestheelapsednumberofroundsforthearmksince
1 2 k K k max
its last pull. We limit s at s even if it surpasses s rounds. Consequently, the cardinality of the states in s
k max max
issK . Withthemulti-dimensionalstates, anagentinteractswithanenvironmentoverT (allowing∞)roundsas
max
follows. Atroundt ∈ [T],givenstates := (s ,s ,··· ,s ,··· ,s ),theagentdrawsanarma fromtheK
t t,1 t,2 t,k t,K t
arms. This choice is governed by a policy π (a |s ), which is a map from S to ∆A, the probability distributions
t t t
on the K arms. The environment then returns a stochastic reward r(s ,a ), which depends on only a and the
t,at t t
correspondingstates . Notethatrisindependentoftheotherarmstates. Finally,thenextstates isupdatedas
t,at t+1
s =f(s ,a ):=min{s +1,s }fork ̸=a and:=1fork =a ,asstatedinthepreviousparagraph.
t+1,k t,k t t,k max t t
WiththeaboveMDP,ourgoalistomaximizetheexpected(discounted)cumulativerewards,whichisdefinedby
(cid:34) T (cid:35)
(cid:88)
V (s):=E γtr(s ,a )|s =s ,
π π t,at t 0
t=0
whereπisagivenstationarypolicy,whichdoesnotdependontimetandsisaninitialstate. Theoptimalpolicyis
definedasπ thatmaximizesV (s)foranyinitialstate. InourMDP,whenT = ∞andγ < 1(i.e.infinite-horizon
π
discounted MDP), it is known (Puterman, 2014) that there exists an optimal policy π∗, which is stationary and
deterministic,meaningthatπ∗isinvariantovertime,andforanys∈S,π(a|s)=1forsomea∈A.
4 Algorithm
This section presents a novel algorithm to learn the optimal policy. Section 4.1 introduces Q-functions called the
State-SeparatedQ-functions(SS-Q-functions),consideringthestatestructure. UsingtheseSS-Q-functions,Section
1InthecontextofRL,armsareoftenreferredtoasactions,butweusetheterm"arm"followingtheoriginalrecoveringbandits
(Pike-BurkeandGrunewalder,2019).
34.2proposestheState-SeparatedSARSA(SS-SARSA)algorithmforefficientlearning. Thissectionfocusesonthe
discountedMDPwithinfinitehorizons(i.e.T =∞andγ ∈(0,1)).
4.1 State-SeparatedQ-function: ConstructingtheMDPwithreducedstatecombinations
WestartwiththeproblemofthetabularRLapproach: thecombinatorialexplosionassociatedwiththeQ-function
(cid:34) T (cid:35)
(cid:88)
Q(s,a):=E γtr(sat,a )|s =s,a =a . (1)
π t t 0 0
t=0
(1)canalsobeexpressedinBellman-equationform(SuttonandBarto,2018):
Q(s,a)=E [r(s ,a)]+γQ(s′,a′). (2)
r a
Here,s′ anda′ representthenextstateandarmaftersanda,respectively; s′ = f(s,a)anda′ ∼ π(·|s′). Wealso
definetheBellmanoptimalityequationforQ-functionas
Q∗(s,a)=E [r(s ,a)]+γmaxQ∗(s′,a′), (3)
r a
a′∈A
whereQ∗istheQ-functionconcerningtheoptimalpolicy.
UnliketheMDPwithaprobabilisticstatetransition,thereisnoneedtotaketheexpectationofs′inthesecondterm
of(3). Then,sincethecardinalityofsissK ,tabularQ-learning(WatkinsandDayan,1992)/SARSA(Rummery
max
andNiranjan,1994)hastoestimatetheQ-functionforsK ×K combinationsoftheargument. Thesealgorithmsare
max
updatedwiththefollowingrulesrespectively:
(cid:16) (cid:17)
Q-learning: Qˆ(s,a)←Qˆ(s,a)+α r(s ,a)+γmaxQˆ(s′,a′)−Qˆ(s,a) (4)
a
a′
(cid:16) (cid:17)
SARSA: Qˆ(s,a)←Qˆ(s,a)+α r(s ,a)+γQˆ(s′,a′)−Qˆ(s,a) (5)
a
Thus,thesealgorithmsmustlearnforacombinatorialnumberofstates,whichisprohibitiveunlesss andK are
max
small.
Tomitigatethiscomputationaldifficulty,weintroduceSS-Q-functions. CombiningthesenewQ-functionsresultsina
significantreductioninstatecombinationscomparedtotheoriginalQ-functions.
Morespecifically,State-SeparatedQ-function(SS-Q-function)isdefinedbyaformofBellman-equation
Q (s ,s ,a):=E [r(s ,a)]+γQ (s′,s′ ,a′) (6)
SS,k k a r a SS,k k a′
for each k ∈ [K] and a ∈ [K]. It is similar to the Bellman equation of the original Q-function but involves only
two-dimensionalstates;itdependssolelyonthestates andthestateofthepulledarms .
k a
WecanrecovertheoriginalQ-functionbyaggregatingthesenewQ-functions. Forafixeda∈[K],byadding(6)over
allk ∈[K]anddividingitbyK,
K K
1 (cid:88) 1 (cid:88)
Q (s ,s ,a)=E [r(s ,a)]+γ Q (s′,s′ ,a′). (7)
K SS,k k a r a K SS,k k a′
k=1 k=1
Sincer(s ,a)isindependentofk ∈[K],theinstantaneousrewardremainsunchangedevenaftertheaggregation. Let
a
theleft-handsideof(7)bedenotedbyQ(s,a),i.e.
K
1 (cid:88)
Q(s,a):= Q (s ,s ,a)
K SS,k k a
k=1
foreachs∈[s ]K anda∈[K]. Then(7)isequivalentto
max
Q(s,a)=E [r(s ,a)]+γQ(s′,a′), (8)
r a
whichcoincideswiththedefinitionoftheoriginalQ-function. NotethatcomputationwithSS-Q-functionsrequires
onlys2 K2variables,whilethenaiveimplementationoftheoriginalQ-functionneedssK ×K. Similarly,wecan
max max
alsoconstructtheBellman-optimalequationbyaggregatingSS-Q-functions.
44.2 State-SeparatedSARSA:AnovelefficientRLalgorithminrecoveringbandits
We introduce State-Separated SARSA (SS-SARSA) building on the SS-Q-functions and explain its advantages in
comparisontoconventionaltabularRLandMABapproaches. WealsoproposeapolicytailoredtoourMDP,which
achievesefficientuniformexplorationacrossallthevariablesoftheQ-function.
Tobegin,weoutlineSS-SARSA,illustratedinAlgorithm1. GiveninputparametersT,γ,andinitialstatess ,the
0
estimatesofSS-Q-functionsQˆ (s ,s ,a)areinitializedtozero. Thealgorithmthenproceedsinthefollowing
SS,k k a
way. TheagentpullsanarmafromK armsaccordingtoaproposedpolicy,Uniform-Explore-Firstπ,whichwillbe
discussedlater. FollowingthestatetransitionruledescribedinChapter3,thenextstatess′ transitiontoonewhere
k = a,andforthestatesofotherarms,theytransitiontomin{s+1,s }. Then,forthepulledarmaandforall
max
k ∈[K]theSS-Q-functionsareupdatedasfollows:
Qˆ (s ,s ,a)←Qˆ (s ,s ,a)+α(r(s ,a)+γQˆ (s′,s′ ,a′)−Qˆ (s ,s ,a)), (9)
SS,k k a SS,k k a a SS,k k a′ SS,k k a
whereα∈[0,1]representsalearningratethatisindependentof(s ,s ,a). Theaboveexpressionisanalogoustothe
k a
updateruleintabularSARSAgivenin(5). BydefiningQˆ(s,a):= 1 (cid:80)K Qˆ (s ,s ,a)andcombiningasin(7)
K k=1 SS,k k a
and(8)usingtheseestimatedSS-Q-functions,wecanupdate
K
Qˆ(s,a)←Qˆ(s,a)+α 1 (cid:88) (r(s ,a)+γQˆ (s′,s′ ,a′)−Qˆ (s ,s ,a)). (10)
K a SS,k k a′ SS,k k a
k=1
(cid:16) (cid:17)
⇐⇒Qˆ(s,a)←Qˆ(s,a)+α r(s ,a)+γQˆ(s′,a′)−Qˆ(s,a) (11)
a
,whichhasthesameformasSARSA(5).
Compared to the related works, SS-SARSA has three advantages: reduced combinations of the estimated SS-Q-
functions,lowtimecomplexity,andlong-termlookahead. First,thecombinationofSS-Qfunctionstobeestimatedisat
mosts2 K2,whichissignificantlysmallerthansK ofQ-functionsintabularRLalgorithms. Second,ouralgorithm
max max
updatesK SS-Q-functionsperround,similartotabularSARSA,thusitstimecomplexityisjustO(KT). Ithasbetter
computationalefficiencycomparedtotheO(K5/2T9/4)timecomplexityassociatedwithregretguaranteesinLaforgue
etal.(2022). Finally,ourRLapproachaimstoidentifytheoptimalpolicyovertheentiredurationoftotalrounds. In
contrast,MABapproachdeterminestheoptimalsequenceofselectedarmsonlyforshortrounds(Laforgueetal.,2022;
Pike-BurkeandGrunewalder,2019).
Remark4.1. (WhydoesouralgorithmadoptSARSAupdateruleinsteadofQ-learningupdate?)Ouralgorithmupdates
theSS-Q-functionsusingtheSARSAupdaterule(5)andcombinetheseSS-Q-functions(10). Anotherpossibilitywould
betouseQ-learning(4). However,itwouldnotfitoursettingduetothemaxoperator. Infact,ifweapply(4)toeach
Q andcombinethemwiththelearningrateα,themaxoperatorpartbecomes
SS,k
K
1 (cid:88)
αmaxQ (s′,s′ ,a′), (12)
K a′ SS,k k a′
k=1
whichshouldbeαmax 1 (cid:80)K Q (s′,s′ ,a′)=αmax Q(s′,a′)toupdatetheformofQ-learning. Therefore
a′ K k=1 SS,k k a′ a′
convergencetotheoptimalpolicywouldbequestionable.
Beforeintroducingournewpolicy,wediscusswhyrandomexplorationdoesn’tworkwellinourMDP.Underrandom
exploration,wecancomputetheprobabilitythatarmkispulledatstates = iinroundt,denotedbyp . Since
k t,i,k
π(a|s)= 1 underarandompolicy,wehavep = 1 ×q ,whereq istheprobabilitythatarmkisinstatei
K t,i,k K t,i,k t,i,k
atroundt. Whent≥s ,theprobabilityp doesnotdependont2. Theprobabilityforeachstateisasfollows.
max t,i,k
Since for any s ∈ [s ] the state of arm k transitions to one when pulling arm k, by the total law of probability,
k max
q = 1 (cid:80)smax q = 1 and thus p = 1 . For i = 2,3,··· ,s −1, an arm reaches state i exactly
t,1,k K j=1 t−1,j,k K t,1,k K2 max
whenarmkisnotpulledatstate(i−1),whichmeansq =(1− 1)q . Thus,q =(1− 1)i−1 1,and
t,i,k K t−1,i−1,k t,i,k K K
p = (1− 1)i−1 1 . Fori = s , bythelawoftotalprobability, p = 1−(cid:80)smax−1p = 1− 1 −
t,i,k K K2 max t,smax,k j=1 t,j,k K2
(cid:80)smax−1(1− 1)i−1 1 ,whichisstrictlymorethan 1 becausep = 1 andp < 1 fori=2,3,···s −1.
i=2 K K2 K2 t,1,k K2 t,i,k K2 max
Thisresultimpliesthatevenwhenapplyingpoliciesthatrandomlyselectanarmgivenastate(e.g.,randomexploration
with a positive probability ϵ in ϵ-greedy (Sutton and Barto, 2018)), SS-Q-functions are frequently updated at s
max
2Ift < s , somestatescannotbereached. Forexample, ifs = s , ittakesatleasts roundstovisitthestateat
max 0,k max max
s =s −1.
k max
5compared to other states. This property is notable in the case where s < K. As a numerical example, when
max
K =s =6,foranytandk,p ,≈0.067andp ≈0.013. Incontrast,whenK =6ands =3,for
max t,smax,k t,smax−1,k max
anytandk,p ≈0.116andp ≈0.023. Variationinthefrequencyoftheseupdateshasanegativeimpact,
t,smax,k t,smax−1,k
especiallywhenthestatewiththehighestrewardisnots .
max
Inthispaper,weproposeanalternativepolicy,namedUniform-Explore-First,whichfollowsastrategyofexploring
informationuniformlyinitiallyandsubsequentlyexploitingthisexploredinformation. Specifically,duringthespecified
initialroundsE,theagentexploresthearmwiththeminimumnumberofvisitsoverthe(s,a)pairs,giventhecurrent
statesuptoroundt,denotedasv (s,a). Ifv (s,a)istiedformultiplearms,theselectionofthearmcanbearbitrary.
t t
After the exploration phase, the agent adopts a greedy approach by pulling the arm with the maximum Qˆ(s,a) to
maximizethe(discounted)cumulativerewards. Incontrasttorandomexploration,ourexplorationstrategyuniformly
pullsarmsatstatesotherthans .
max
Algorithm1State-SeparatedSARSA(SS-SARSA)
Input: T,γ,α
Initialize: Qˆ (s ,s ,a)←0forallk ∈[K]anda∈[K]
SS,k k a
fort=1,2,··· ,T do
(cid:26)
argmin v (s,a)whent≤E
a← a∈[K] t ▷Uniform-Explore-First
argmax Qˆ (s,a)whent>E
a∈[K] t
r ←r(s ,a)
a
(cid:26)
1 fork =a
s′ ←
k min{s +1,s } fork ∈[K]\{a}
k max
UpdateQˆ (s ,s ,a)forallk ∈[K]:
SS,k k a
Qˆ (s ,s ,a)←Qˆ (s ,s ,a)+α(r(s ,a)+γQˆ (s′,s′ ,a′)−Qˆ (s ,s ,a))
SS,k k a SS,k k a a SS,k k a′ SS,k k a
endfor
5 ConvergenceAnalysis
Thissectionpresentsaconvergenceanalysisandprovidesremarksonourproblemsetting. Throughoutthissection,our
focusisontheinfinite-horizondiscountedMDP,i.e.,T =∞andγ <1.
Theorem 5.1. (Convergence of Q-functions) Suppose that the variance of the (stochastic) reward r is finite and
α = 1 wheret issomeconstantvalue. ThislearningratesatisfiesRobbins-Monroscheme(i.e.(cid:80)∞ α = ∞
ant d(cid:80)t+ ∞t0
α2 <
∞0
). Supposethatπ
(s|a)isaGLIEpolicy,thatis,itvisitseach(s,a)infinitelyoftenat= nd1 cht
ooses
t=1 t t
anarmgreedilywithprobabilityoneinthelimit(i.e.π (s|a)=argmax Qˆ(s,a)w.p. 1ast→∞). Also,foreach
t a
(s,a)pair,definetheerroroftheQ-functionatroundt ∈ [T]asQ (s,a) := |Qˆ (s,a)−Q∗(s,a)|. Defineaset
err,t t
V :={(s,a)|visit(s,a)forsomepolicy}. Then,forany(s,a)∈V,Q (s,a)→0withprobability1asT →∞.
err,t
Proof: AsindicatedinSection4,sinceweadoptthelearningratewhichisindependentof(s ,s ,a),SS-SARSAcan
k a
beconsideredSARSAaftercombiningtheSS-Qfunctions. Consequently,wecanprovetheconvergencetheorem
analogoustoSinghetal.(2000),underthesameassumptionasSARSA.
Remark5.2. (Novisitatsomestates)WhiletheconvergencetheoreminMDP(WatkinsandDayan,1992;Singhetal.,
2000)assumesinfinitevisitsforeach(s,a)pair,inourMDPsettingscertainstatesarenevervisitedforanypolicy. For
instance,(s,a)withs = 1formultiplek neverappears,sinceitmeansmultiplearmswerepulledatthelasttime
k
point. OnlythevalueofQ-functionwithinV isneededtolearnapolicy.
Remark5.3. (ConvergencetheoremforUniform-Explore-First)TheUniform-Explore-FirstpolicyisconsideredGLIE
becauseitgreedilypullsanarmaftertheexplorationphase. However,duetotheseparationoftheexplorationand
the exploitation phase, our policy does not guarantee the infinite updating of all Q-functions in V. Nevertheless,
in practice, by taking sufficient rounds for uniform exploration, the estimated Q-function approaches the Bellman
optimalityequationforQ-functionofeach(s,a)∈V.
6 Experiments
Inthissection,wepresentsimulationresultstoempiricallyverifytheperformanceofouralgorithm. Section6.1gives
thesimulationsettings,andSection6.2showsthesuperiorityofouralgorithminsomemetrics.
66.1 SimulationSettings
Inthissubsection,westateMDPenvironments,metrics,SS-SARSAparameters,andcompetingalgorithmsrelatedto
ourwork.
Initialstate: Throughoutthesimulation,initialstateisassumedtobes foreveryk ∈[K]. Initemrecommendation,
max
iftheconversionrateincreaseswiththenumberofstates,representingthefreshnessofitems,thisassumptionreflects
theagentbeforetheitemrecommendationisexposed. Notethatthisassumptionismadepurelyforsimplicityandthat
ouralgorithmcanbegeneralizedtoanyotherinitialstates.
Discountrate: Weconductsimulationsinbothdiscounted(γ ∈ (0,1))andnon-discounted(γ = 1)settings. The
discountedsettingiscommonlyemployedininfinitehorizonMDP.Wesetγ to(1−10−T),dependingonT,becauseit
makestherewardsinlaterroundsnotsignificantlysmaller. Ontheotherhand,thenon-discountedsettingisthestandard
settinginMAB.Asdiscussedintheprevioussection,ouralgorithmdoesnotsatisfytheassumptionfortheconvergence
theorem. Nevertheless,wewillverifythatouralgorithmperformswellinpracticeforbothsituations.
Reward: Tobegin,wedemonstratewithasmallstatecombination,specificallysettingK =s =3andT =105.
max
The first arm is nonstationary, with an expected reward of 0.1 for s ∈ {1,2} and an increase to 0.3 at s = 3.
1 1
Conversely,theremainingtwoarmsarestationary,withanexpectedrewardof0.2irrespectiveofthestateofeach
arm. NotethatourclaimofsuperiorityisagainstMABalgorithmsratherthanthetabularRLalgorithms. Indeed,the
cardinalityofSS-QfunctionsinSS-SARSAandtheQ-functionforQ-learning/SARSAarethesame.
Next,weaddresslarger-scaleproblemsandprovideacomprehensiveframeworkforallremainingexperiments. We
considervarioussettingsinvolvedinrewardheterogeneityandrewardincrements.
Forrewardheterogeneity,weconsiderK bestarmsandtheremaining(K−K )sub-bestarms. WhenK =K ,
best best best
armsarehomogeneous: changesinexpectedrewardsperstateincrementforallarmsarethesame,asillustratedin
6-Homoand10-HomoinTable1. Incontrast,whenK >K ,armsareheterogeneous: changesinexpectedrewards
best
perstateincrementdifferforbestarmsandsub-bestarms,asillustratedin6-Heteroand10-HeteroinTable1.
For reward increments, we consider two cases of changes in expected rewards with state increments: monotone-
increasingrewardsandincreasing-then-decreasingrewards. Inthemonotone-increasingcase,theexpectedrewardfor
eacharmisasfollows: forthebestarms,theexpectedrewardsstartat0.1fors =1andincreasebyV perstate
a best
increment,reaching0.6ats . Forthesub-bestarms,theexpectedrewardsats =1remainthesame,butincrease
max a
onlybyV (<V )perstateincrement,reaching0.5ats .
sub-best best max
Theincreasing-then-decreasingcaseissimilartothemonotonecase,butthestateofpeakrewarddiffers. Specifically,
forthebestarms,theexpectedrewardsats =1arethesameasinthemonotonecaseandincreasebyV perstate
a best
increment,butreach0.6ats −1anddecreasebyV ats . Similarly,forthesub-bestarms,therewardatthe
max best max
peakstateis0.5ats −1anddecreasesbyV ats .
max sub-best max
Finally, we state the total rounds and reward distributions. Considering the cardinality of the states, T = 105 for
K =3,6andT =106forK =10. Moreover,weuseBernoulliandnormaldistributionsasrewarddistributionsfor
allsimulations. Inthenormalcase,thevarianceis0.5. Thenormalcaseismoredifficultduetoitshighervariance
comparedtotheBernoullicase.
Name K s |s| T K V V
max best best sub-best
6-Hetero 6 3 36(>7×102) 105 3 1(1) 1(2)
4 2 5 5
6-Homo 6 6 66(>4.6×104) 105 6 1 (1) –
10 8
10-Hetero 10 5 510(>9×106) 106 5 1(1) 1 ( 2 )
8 6 10 15
10-Homo 10 10 1010 106 10 1 ( 1 ) –
18 16
Table1: Monotone-increasing(increasing-then-decreasing)rewardsettings(ThevaluesenclosedinparenthesesinV
best
andV pertaintotheincreasing-then-decreasingcase.)
sub-best
Metrics: We introduce some metrics for comparing algorithm performance. The first metric is cumulative regret,
which is the difference in (discounted) cumulative expected rewards between the best and learning policies. This
metricmeasurestheclosenessofthelearningpolicytotheoptimalone,sosmallerisbetter,andisoftenusedinMAB
(LattimoreandSzepesvári,2020).
Ifwecandefinetheoptimalpolicy,wealsousetherateofoptimalpolicy,whichmeasureshowoftentheoptimalpolicy
isobtainedoutofathousandsimulations. Theoptimalpolicyisdefinedashavingnoregretduringthelast3×s
max
rounds,makingiteasytoassessthequalityofexploration. Notethatthismetriconlyreflectstheperformanceafter
7exploration,whilecumulativeregret/rewardscoverboththeexplorationandexploitationphases. Therefore,evenif
somealgorithmshavesmallerregretthanothers,theymaynotnecessarilyachieveoptimalpolicy.
Inthe(first)small-scaleandthemonotone-increasingcase,theoptimalpolicycanbeobtainedexplicitly. Intheformer
case,theoptimalpolicyistoselectthefirstarmonlywhens =s =3andtochoosetheotherarmsotherwise. In
1 max
themonotone-increasingcase,aslongass ≤K ,theagenthasthechancetopullthebestarmats giventhe
max best max
initialstates =s . Therefore,theoptimalpolicycyclicallypullsonlyK bestarmsats .
0 max best max
However,intheincreasing-then-decreasingrewards,givens =s ,thereisnochancetopullthebestarmats −1
0 max max
inthefirstround. Thus,optimalpolicyisnottrivial.3 Insuchacase,weuse(discounted)cumulativeexpectedrewards
withoutoptimalpolicyasanothermetric. Thelargerthemetric,thebetter,althoughitsmaximizationisequivalentto
theminimizationofcumulativeregret.
SS-SARSAparameters: Wesetthelearningrateandtheexplorationhorizoninouralgorithm. Aspointedoutinthe
previoussection,weuseα = 1 . Thesmallerthet value,thegreatertheeffectoftheupdateintheearlyvisit,
t t+t0 0
causingunstablelearningduetothestochasticityofrewards. Thus,alarget isbetterformitigatingthisissue,andwe
0
sett =5000throughthesimulations.
0
Thesizeoftheexplorationshouldbedeterminedbyconsideringthetrade-offbetweenexplorationandexploitation.
Over-exploration,characterizedbyuniformlypullingarmsinalargefractionofrounds,failstosufficientlyexploitthe
bestarm,leadingtolargeregretorsmallcumulativerewards. Conversely,under-exploration,characterizedbyuniformly
pullingarmsinasmallfractionofrounds,failstogatheradequateinformationabouteachQ-function,resultinginalow
probabilityofselectingthebestarm. Takingthistradeoffintoaccount,weallocateuniformexplorationto10%ofthe
totalrounds. (i.e.E =0.1T).
Compared algorithms: We introduce other algorithms to compare their performance to our algorithm. The first
algorithmsaretheoriginaltabularmodel-freeRLalgorithms: Q-learning(WatkinsandDayan,1992)andSARSA
(RummeryandNiranjan,1994). ThesealgorithmsalsohaveconvergenceresultstotheBellmanoptimalityequationfor
Q-function.However,inoursetting,asdescribedinsection4.1,thesealgorithmshavetoestimateenormousQ-functions
unless s and K are small, leading to few updates after many rounds. For the comparison of our algorithm, we
max
maintainthelearningrate,policy,andexplorationsizeidenticaltothoseofSS-SARSA.
Another algorithm is the dRGP-TS algorithm (Pike-Burke and Grunewalder, 2019). This approach utilizes GP
regression for each arm to estimate the reward distribution. After sampling rewards from the distribution of each
armforpredetermineddrounds,theagentpullsthesequenceofarmswiththehighesttotalrewards. DuetotheKd
combinationstodrawarmsforeachdround,alargedbecomesunrealistic4. Therefore,inourexperiments,weconsider
d=1,2. Thisapproachhelpstoavoidthestate-combinationprobleminQ-learning/SARSAandshowsaBayesian
regretupperbound. However,theupperboundisderivedbyrepeatedlyapplyingthed-stepregret. Inthissimulation,
Weevaluatefull-horizonregretthatisdefinedinmetrics. Additionally,weuseanalternativeimplementationthatis
equivalenttotheoriginalbutreducescomputationalcomplexity. Thepseudo-codedetailsandparametersettingsare
providedinAppendixA.
6.2 Simulationresults
Weshowonlytheresultswithdiscountedrewardshere,andtheundiscountedcaseswillbedeferredtoAppendixB.
Ourcodeisavailableathttps://github.com/yutanimoto/SS-SARSA.
6.2.1 Small-scaleproblem
Foreachrewarddistribution,Figure1showsthecumulativeregrettransitions(left),boxplotsofcumulativeregretin
thefinalround(middle),andtherateoftheoptimalpolicy(right).
Inthecumulativeregret(left),thesolidlineandthefilledarearepresentthemedianandthe90%confidenceinterval,
calculatedwithathousandsimulations. Explorationofeachalgorithmincreasesthecumulativeregretintheearlystage,
andthentheagentexploitsthelearnedpolicy. Intheboxplotsofcumulativeregret,theblackcircle,theboxes,the
bottomlinesintheboxes,themiddlelinesintheboxes,andthetoplinesintheboxesrepresentthemean,50%box,the
25th,50th,and75thpercentiles,respectively. Allpointsoutsidetheinterquartilerangeofthebox,whichis1.5timesthe
3Evenifwesettheinitialstateass −1,afterthestatetransition,eacharmstateiseither1ors .Thus,inthesecondround,
max max
thesameproblemoccursinthecaseofs =s .
0 max
4Pike-BurkeandGrunewalder(2019)introducesacomputationallyefficientalgorithmforsearchinganoptimalsequenceofarms
underlargevaluesofKandd.However,experimentalresultsshowedsimilarperformanceford=1,3.Additionally,intheregret
comparisonwithotheralgorithms,onlythecased=1isconsidered.
8(a)Bernoullirewards
(b)Normalrewards
Figure1: Small-scaleproblem(K =3,s =3,γ =0.99999)
max
differencebetweenthetopandbottomlines,correspondtooutliers. Theupperandlowerlinesrepresentthemaximum
andminimumvaluesafterremovingoutliers. Thesegraphsreflectthestabilityofthealgorithmsthroughthevariation
incumulativeregret. Therateofoptimalpolicyoverathousandsimulationsassessesthequalityofexploration.
Figure1showstheresultsofK = 3ands = 3. InthecaseofBernoullirewards(1(a)),allthemethodsexcept
max
2RGP-TSgenerallyachievetheoptimalpolicyduetothesimplesettingoftheproblem. 1RGP-TSshowsslightly
unstableresults. 2RGP-TSdoesnotperformwell,suggestingthatthismethodfailstoidentifytheoptimalsequenceof
selectedarmsevenwhenconsideringalimitednumberofcombinations.
6.2.2 Monotone-increasingrewards
InFigures2and3,thetypeandarrangementofthegraphsarethesameasbefore.
Figure2showstheresultsofK =6. Withalargerstatespace,thestandardQ-learningandSARSAdonotachievethe
optimalpolicy. Forboth6-Heteroand6-Homo,SS-SARSAstablyachievestheoptimalpolicyinmosttrialswithin
theexplorationphase,asseenbynoincreaseofregretafterthatround. Incontrast,in6-Hetero,1RGP-TS,therateof
optimalpolicyislower. Thisiscausedbytheoccasionalfailuretofindtheoptimalpolicy,whichcanbeseenbythe
increaseofcumulativeregret,especiallynotableinnormalrewards. Similarly,in6-Hetero,2RGP-TStendstofailin
findingtheoptimalpolicyforthesamereasonastheexperimentswithK =3.
BycomparingtheresultswithK =3andK =6,wecanseethatSS-SARSAaswellas1RGP-TScanhandlethelarge
statespacemuchmoreefficientlythanQ-learningandSARSA,whichsufferfromcomplexityevenwithK =6.
Figure3depictstheresultsfor10arms. TheresultsofSARSAandQ-learningarenotincludedduetotheirrequirement
ofalargememoryforQ-functionsandpoorperformanceinthecaseof6arms. In10-Hetero,forbothBernoulli(a)and
normal(b)rewards,SS-SARSAhaslowregretandiscompetitivewith1RGP-TSinobtainingtheoptimalpolicyforall
simulations. However,WhileSS-SARSAhasahigherrateoftheoptimalpolicyandmorestablecumulativeregret
than1RGP-TS.2RGP-TSperformstheworstinamannersimilartothepreviouscases. In10-homo,ouralgorithmhas
largerregretthan1RGP-TSduetotheexplorationphase,buttherateofoptimalpolicyremainscompetitive.
TheobtainedresultsindicatethatSS-SARSAisthemoststabilityforanycase. Notably,inheterogeneousrewards,our
algorithmdemonstratesthemoststableperformancewithlowcumulativeregretandthehighestrateofoptimalpolicy.
1RGP-TSperformsslightlybetterthanouralgorithminthecaseofhomogeneousrewards,butitbecomesunstable
whendealingwithheterogeneousandhigh-variancerewards.
9(a)6-Hetero(Bernoullirewards)
(b)6-Hetero(Normalrewards)
(c)6-Homo(Bernoullirewards)
(d)6-Homo(Normalrewards)
Figure2: Increasingrewards(K =6,γ =0.99999)
6.2.3 Increasing-then-decreasingrewards
First,notethatinthecaseofincreasing-then-decreasingrewards,weemploythecumulativerewardsinsteadofregretin
theleftandcentergraphsofFigures4and5,sothelargervaluesarebetter.
ThoseresultsshowthattheproposedSS-SARSAperformsbetterorcompetitivelycomparedtotheothermethods. In
contrasttothemonotonerewards,SS-SARSAdoesnothavehighercumulativeregretthan1RGP-TSeveninthecaseof
homogeneousrewards. Thereasonforthisistheproposedpolicy;Uniform-Explore-FirstenforceseachSS-Q-function
10(a)10-Hetero(Bernoullirewards)
(b)10-Hetero(Normalrewards)
(c)10-Homo(Bernoullirewards)
(d)10-Homo(Normalrewards)
Figure3: Increasingrewards(K =10,γ =0.999999)
toupdateuniformlyacrossallstates,whiletheexplorationin1RGP-TSdoesnottakeintoaccountthestructureinherent
inourMDP.
Overall,SS-SARSAoutperformsotheralgorithmsintermsofstabilityofregretandrateofoptimalpolicy,regardlessof
rewarddistribution,heterogeneity,andstatecardinality.TheonlyalgorithmthatcomesclosetoSS-SARSAis1RGP-TS,
whichiscompetitiveorslightlysuperiorincasesofhomogeneousandmonotone-increasingrewards. However,its
performancesignificantlydecreaseswithheterogeneousornon-monotonerewards.
11(a)6-Hetero(Bernoullirewards)
(b)6-Hetero(Normalrewards)
(c)6-Homo(Bernoullirewards)
(d)6-Homo(Normalrewards)
Figure4: Increasing-then-decresingrewards(K =6,γ =0.99999)
7 Conclusion
WeproposeanRLalgorithm,calledSS-SARSA,tosolvetherecoveringbanditproblem. ThisalgorithmestimatesQ-
functionsbycombiningSS-Q-functionsandupdateslikeSARSA,leadingtoefficientlearningandlowtimecomplexity.
Weprovetheconvergencetheoremfortheoptimalpolicy. Furthermore,ouralgorithmperformswellinbothmonotone
andnon-monotonerewardscenarios,asdemonstratedthroughsimulations.
Thealgorithmhasseveraladvantages,butitalsohassomelimitations. Firstly,whentherearemanyarmsandalarge
s ,evenouralgorithmstruggleswithtoomanycombinations. Insuchcases,afunctionalapproximationofQ-function
max
isconsideredforefficientlearning. Secondly,weonlypresentedresultsfromsimulations,notfromreal-worlddata.
Oursettingsrequireasubstantialamountofdatapointsforaperson,buttoourknowledgesuchdatadonotexist. The
finalisafinitesampleanalysis. Regretboundsandsamplecomplexityareneededwithoutrelyingonstrongreward
structures. Thesepointsareleftinfutureworks.
12(a)10-Hetero(Bernoullirewards)
(b)10-Hetero(Normalrewards)
(c)10-Homo(Bernoullirewards)
(d)10-Homo(Normalrewards)
Figure5: Increasing-then-decresingrewards(K =10,γ =0.999999)
References
Auer,P.,Cesa-Bianchi,N.,andFischer,P.(2002). Finite-timeAnalysisoftheMultiarmedBanditProblem. Machine
Learning,47(2):235–256.
Besbes, O., Gur, Y., and Zeevi, A. (2014). Stochastic multi-armed-bandit problem with non-stationary rewards.
Advancesinneuralinformationprocessingsystems,27.
Bouneffouf,D.,Rish,I.,andAggarwal,C.(2020). Surveyonapplicationsofmulti-armedandcontextualbandits. In
2020IEEECongressonEvolutionaryComputation(CEC),pages1–8.IEEE.
Chapelle, O. and Li, L. (2011). An empirical evaluation of thompson sampling. Advances in neural information
processingsystems,24.
Gangan,E.,Kudus,M.,andIlyushin,E.(2021). Surveyofmultiarmedbanditalgorithmsappliedtorecommendation
systems. 9(4):16.
13Garivier, A. and Cappé, O. (2011). The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond. In
Proceedingsofthe24thAnnualConferenceonLearningTheory,pages359–376.JMLRWorkshopandConference
Proceedings. ISSN:1938-7228.
Garivier,A.andMoulines,E.(2011). Onupper-confidenceboundpoliciesfornon-stationarybanditproblems. In
AlgorithmicLearningTheory,pages174–188.
Heidari, H., Kearns, M., andRoth, A.(2016). Tightpolicyregretboundsforimprovinganddecayingbandits. In
ProceedingsoftheTwenty-FifthInternationalJointConferenceonArtificialIntelligence,pages1562–1570.
Kleinberg,R.andImmorlica,N.(2018). Rechargingbandits. In2018IEEE59thAnnualSymposiumonFoundationsof
ComputerScience(FOCS),pages309–319.IEEE.
Laforgue,P.,Clerici,G.,Cesa-Bianchi,N.,andGilad-Bachrach,R.(2022). ALastSwitchDependentAnalysisof
SatiationandSeasonalityinBandits. InProceedingsofThe25thInternationalConferenceonArtificialIntelligence
andStatistics,pages971–990.PMLR. ISSN:2640-3498.
Lai,T.L.,Robbins,H.,etal.(1985).Asymptoticallyefficientadaptiveallocationrules.Advancesinappliedmathematics,
6(1):4–22.
Lattimore,T.andSzepesvári,C.(2020). BanditAlgorithms. CambridgeUniversityPress,1edition.
Leqi,L.,Kilinc-Karzan,F.,Lipton,Z.C.,andMontgomery,A.L.(2021). ReboundingBanditsforModelingSatiation
Effects. arXiv:2011.06741[cs,stat].
Levine,N.,Crammer,K.,andMannor,S.(2017). Rottingbandits. Advancesinneuralinformationprocessingsystems,
30.
Li, Y., Jiang, J., Gao, J., Shao, Y., Zhang, C., and Cui, B. (2020). Efficient automatic cash via rising bandits. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages4763–4771.
Liu,F.,Lee,J.,andShroff,N.(2018). Achange-detectionbasedframeworkforpiecewise-stationarymulti-armed
banditproblem. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume32.
Metelli,A.M.,Trovo,F.,Pirola,M.,andRestelli,M.(2022). Stochasticrisingbandits. InInternationalConferenceon
MachineLearning,pages15421–15457.PMLR.
Misra,K.,Schwartz,E.M.,andAbernethy,J.(2019). DynamicOnlinePricingwithIncompleteInformationUsing
MultiarmedBanditExperiments. MarketingScience,38(2):226–252.
Moriwaki, D., Fujita, K., Yasui, S., and Hoshino, T. (2019). Fatigue-aware ad creative selection. arXiv preprint
arXiv:1908.08936.
Pike-Burke, C. and Grunewalder, S. (2019). Recovering Bandits. In Advances in Neural Information Processing
Systems,volume32.CurranAssociates,Inc.
Puterman,M.L.(2014). Markovdecisionprocesses: discretestochasticdynamicprogramming. JohnWiley&Sons.
Rummery,G.A.andNiranjan,M.(1994). On-lineQ-learningusingconnectionistsystems,volume37. Universityof
Cambridge,DepartmentofEngineeringCambridge,UK.
Russac,Y.,Vernade,C.,andCappé,O.(2019). Weightedlinearbanditsfornon-stationaryenvironments. Advancesin
NeuralInformationProcessingSystems,32.
Seznec,J.,Locatelli,A.,Carpentier,A.,Lazaric,A.,andValko,M.(2019). Rottingbanditsarenoharderthanstochastic
ones. InProceedingsoftheTwenty-SecondInternationalConferenceonArtificialIntelligenceandStatistics,pages
2564–2572.PMLR. ISSN:2640-3498.
Simchi-Levi, D., Zheng, Z., and Zhu, F. (2021). Dynamic Planning and Learning under Recovering Rewards. In
Proceedingsofthe38thInternationalConferenceonMachineLearning,pages9702–9711.PMLR. ISSN:2640-3498.
Singh, S., Jaakkola, T., Littman, M. L., and Szepesvári, C. (2000). Convergence results for single-step on-policy
reinforcement-learningalgorithms. Machinelearning,38:287–308.
Sutton,R.S.andBarto,A.G.(2018). Reinforcementlearning: Anintroduction. MITpress.
Thompson,W.R.(1933). OntheLikelihoodthatOneUnknownProbabilityExceedsAnotherinViewoftheEvidence
ofTwoSamples. Biometrika,25(3/4):285–294. Publisher: [OxfordUniversityPress,BiometrikaTrust].
Warlop,R.,Lazaric,A.,andMary,J.(2018). FightingBoredominRecommenderSystemswithLinearReinforcement
Learning. InAdvancesinNeuralInformationProcessingSystems,volume31.CurranAssociates,Inc.
Watkins,C.J.andDayan,P.(1992). Q-learning. Machinelearning,8:279–292.
Yancey,K.P.andSettles,B.(2020). ASleeping,RecoveringBanditAlgorithmforOptimizingRecurringNotifications.
InProceedingsofthe26thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining,pages
3008–3016,VirtualEventCAUSA.ACM.
14A AlternativeImplementationofdRGP-TS
ThissectiondetailsthealternativeimplementationofdRGP-TSandparametersettings.
Beforeintroducingthealternative,webrieflyreviewtheGPupdateindRGP-TSalgorithm(Pike-BurkeandGrunewalder,
2019) 5. The original paper formulates the reward mechanism as r(s ,a) = f (s )+ϵ where f is an unknown
a a a a
functiondependingonthestateforarmaandϵ∼N(0,σ2)isaGaussiannoise(σisknown). WhenwesetaGPprior
onf andreceiveN observedrewardsR =(r ,r ,··· ,r )T andstatesS =(s ,s ,··· ,s )forarma,its
a a,N 1 2 N a,N 1 2 N
posteriorisalsoGP.Thatis,fork (s)=(k(s ,s),k(s ,s),··· ,k(s ,s))T andpositivesemi-definitekernelmatrix
a,N 1 2 N
K =[k(s ,s )]N ,themeanandcovarianceoftheposteriorafterN observationsare,
a,N i j i,j=1
µ (s;N)=k (s)T(K +σ2I)−1R k (s,s′;N)=k(s,s′)−k (s)T(K +σ2I)−1k (s′). (13)
a a,N a,N a,N a a,N a,N a,N
Moreover, when s = s′, k (s,s′;N) is equivalent to the variance σ2(s;N). A bottleneck in the above is the time
a a
complexityforcomputingtheinversematrix. IndGRP-TS,foreverydrounds,thetimecomplexityfortheinverse
matrix is O(c (a)3), where c (a) represents the number of times an arm a is pulled up to round t. Therefore, the
t t
originaldRGP-TSisimpracticalforlargeT.
Instead,weintroduceanalternativeimplementationofdRGP-TS,whichisanequivalentupdateto(13)butreduces
itstimecomplexity. Sincethestatesofeacharmonlytakediscretevaluesfromonetos ,wecanreformulatethe
max
argumentofeacharm’sGPfunctionusingas dimensionalnormaldistribution. Toseethis,wedefinef thereward
max a
distributionofrewardforarma∈[K]withs -dimensionalnormaldistributionasfollows.
max
 
 f a(1)    µ a,1   k a2 ,1 k a,12 ... k a,1smax 
f a =   f a( . . .2)   ∼N        µ a . . .,2   ,    k a . . .,21 k a2 . . .,2 . .. .. . k a,2 . . .smax        (14)
f a(s max)   (cid:124)µ a (cid:123),s (cid:122)max
(cid:125)
(cid:124)k a,smax1 k a,smax (cid:123)2
(cid:122)
... k a2 ,smax (cid:125) 
:=µa,smax :=Ka,smax
,wheref (s)isrewardfunctionforarmaandstates∈[s ],µ isthemeanforarmaandstatesandk isthe
a max a,s a,ss′
covarianceofthestatesands′forarma(itisalsovariancewhens=s′). Additionally,wedefines-thordercolumnof
K ask (s).
a,smax a,smax
Next,wewillexplainhowtoupdatetheposteriortoreducethetimecomplexity. Whenusingdiscreteinput,wecompute
onlythes -dimensionalinversematrixtoupdatetheposteriorasin(13). Todoso,weintroducethecountmatrixfor
max
arma,C ,whichisN ×s matrix,andits(i,j)elementisonewhentheagentpullsthearmaatstatej ∈[s ]for
a max max
thei-thtime,otherwisezero. Then,wecaneasilyverifythatk (s)T =k (s)TCT andK =C K CT.
a,N a,smax a a,N a a,smax a
Thus,iftherewardweredeterministic,µ (s;N)in(13)couldberewrittenasfollows.
a
µ (s;N)=k (s)T(K +σ2I)−1R
a a,N a,N a,N
=kT CT(C K CT +σ2I)−1R
a,smax a a a,smax a a,N
=kT CT (C K CT +σ2I)−1C R , (15)
a,smax a a a,smax a a a,smax
(cid:124) (cid:123)(cid:122) (cid:125)
⃝1
whereR isthemeanrewardvectorfromonetos forarmaandweuseR =C R . Inpractice,since
a,smax max a,N a a,smax
therewardisstochastic,theequalityin(15)isreplacedbyapproximation,yetsuchanapproximationsavesmemoryfor
R N. AfterapplyingSherman–Morrison–Woodburyformulain⃝1 ,
1 1 1 1
⃝1 = σ2I− σ2C a(K− a,1
smax
+ σ2C aTC a)−1 σ2C aT. (16)
Thus,by(16),wecanrewrite(15)asfollows.
(cid:26) (cid:27)
1 1 1 1
µ (s;N)=kT CT I− C (K−1 + CTC )−1 CT C R
a a,smax a σ2 σ2 a a,smax σ2 a a σ2 a a a,smax
(cid:26) (cid:27)
1 1 1 1
=kT CTC − CTC (K−1 + CTC )−1 CTC R
a,smax σ2 a a σ2 a a smax σ2 a a σ2 a a a,smax
=kT (cid:8) C −C (K−1 +C )−1C (cid:9) R , (17)
a,smax a,σ a,σ a,smax a,σ a,σ a,smax
5Somenotationsdifferfromtheoriginal(Pike-BurkeandGrunewalder,2019)tomatchthenotationsofourpaper.
15where C := 1 CTC . Therefore, we need to compute only s -dimensional inverse matrix for updating the
a,σ σ2 a a max
posteriormean. Inthesameway,theposteriorcovariancematrixcanbeupdatedasfollows.
k (s,s′;N)=k(s,s′)−k (s)T (cid:8) C −C (K−1 +C )−1C (cid:9) k (s′). (18)
a a,smax a,σ a,σ a,smax a,σ a,σ a,smax
Thepseudo-codeforthealgorithmisprovidedinAlgorithm2. TheinputsaretotalroundsT,astandarderrorofthe
noiseinGPregressionσ >0,alengthscaleofRBFkernelc>0,andasizeoflookaheadd. Withinitialpriorwitha
meansettozeroandcovariancematrixwithRBFkernel,andinitialstates,thealgorithmproceedsasfollows. Forevery
dround,theagentselectscombinationsofarmsI tomaximizethetotalreward,(cid:80)d−1r(s(i) ,a(i)),wheres(i) and
d,t i=0 a(i) a(i)
a(i)representthestateforarma(i)andarmafteristepsofsanda,respectively(fori=0,s(0) =s anda(0) =a).
a(0) a
Thesevaluesaresampledfromnormaldistributionswithestimatedmeansandvariances. Forthearma=I(l)selected
d,t
inlthstep,theagentreceivesitsreward,updatestheposteriormean(17)andcovariance(18),andtranstothenextstate.
Sincewerepeattheaboveprocedureforroundt=1,2,··· ,⌊T/d⌋,itstimecomplexityisO(KdT).
Whenwerunsimulations,wesettheinputparametersasfollows. AsdiscussedinSection6.1,wespecifyd=1,2. The
remainingparameters,σ =1.0andc=2.5,areconsistentwiththoseutilizedinthesimulationpresentedinPike-Burke
andGrunewalder(2019).
Algorithm2dRGP-TS(AlternativeImplementation)
Input: T,σ: standarderrorofnoise,c: lengthscaleofRBFkernel,d: sizeoflookahead
Initialize: µ =0∀a∈[K],and∀s ∈[s ];k (i,j)=e−(i−j)2/2l2 ∀a∈[K]and∀i,j ∈[s ];s←s
sa,a a max a max max
fort=1,2,··· ,⌊T/d⌋do
Pull a d-sequence of arms I = argmax (cid:80)d−1r(s(i) ,a(i)), where r(s(i) ,a(i)) ∼
d,t (a,a′,...,a′(d−1)) i=0 a(i) a(i)
N(µ ,k (s(i) ,s(i) ))∀s(i) ∈[s ],∀a(i) ∈[K],and∀i∈{0,...,d−1}
s(i) ,a(i) a(i) a(i) a(i) a(i) max
a(i)
forl=1,2,··· ,ddo
a←I(l)
d,t
r ←r(s ,a)
a
Updateµ using(17)
sa,a
form=1,2,··· ,s do
max
Updatek (s ,m)using(18)
a a
endfor
(cid:26)
1 fork =a
s′ ←
k min{s +1,s } fork ∈[K]\{a}
k max
endfor
endfor
B SimulationResultswithUndiscountedrewards
Inthissection,weshowtheperformanceofouralgorithmoverthecompetitorsinundiscountedcases(i.e.γ = 1),
whichisthedefaultsettinginMAB.Ineachcase,therateoftheoptimalpolicyissimilartothediscountedcase,but
sincetherewardsinthelaterroundsareundiscounted,thecumulativeregret/rewardsisdifferent. Themostnotable
caseisthatofincreasing-then-decreasingrewards(Figure9and10);fromtheleftandrightgraphs,thedifferencein
cumulativerewardsbetweenSS-SARSAand1RGP-TSisgreater.
16(a)Bernoullirewards
(b)Normalrewards
Figure6: Small-scaleproblem(K =3,s =3,γ =1)
max
17(a)6-Hetero(Bernoullirewards)
(b)6-Hetero(Normalrewards)
(c)6-Homo(Bernoullirewards)
(d)6-Homo(Normalrewards)
Figure7: Increasingrewards(K =6,γ =1)
18(a)10-Hetero(Bernoullirewards)
(b)10-Hetero(Normalrewards)
(c)10-Homo(Bernoullirewards)
(d)10-Homo(Normalrewards)
Figure8: Increasingrewards(K =10,γ =1)
19(a)6-Hetero(Bernoullirewards)
(b)6-Hetero(Normalrewards)
(c)6-Homo(Bernoullirewards)
(d)6-Homo(Normalrewards)
Figure9: Increasing-then-decresingrewards(K =6,γ =1)
20(a)10-Hetero(Bernoullirewards)
(b)10-Hetero(Normalrewards)
(c)10-Homo(Bernoullirewards)
(d)10-Homo(Normalrewards)
Figure10: Increasing-then-decresingrewards(K =10,γ =1)
21