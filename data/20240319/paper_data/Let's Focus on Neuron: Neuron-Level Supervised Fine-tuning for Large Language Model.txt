Let’s Focus on Neuron:
Neuron-Level Supervised Fine-tuning for Large Language Model
HaoyunXu♣♠*† RunzheZhan♣† DerekF.Wong♣‡ LidiaS.Chao♣
♣NLP2CTLab,DepartmentofComputerandInformationScience,UniversityofMacau
♠TigerResearch,Shanghai
nlp2ct.{haoyun, runzhe}@gmail.com, {derekfw, lidiasc}@um.edu.mo
Abstract suggestingthatnotallneuronsneedtobeactive,a
traitthatbecomesmorepronouncedinlargermod-
LargeLanguageModels(LLMs)arecomposed
els(Maetal.,2023;FrantarandAlistarh,2023;Liu
ofneuronsthatexhibitvariousbehaviorsand
etal.,2023;Kurticetal.,2023;Songetal.,2023).
roles,whichbecomeincreasinglydiversifiedas
LLMs commonly adapt to specific tasks
models scale. Recent studies have revealed
that not all neurons are active across differ- through full-parameter supervised fine-tuning
ent datasets, and this sparsity correlates pos- (SFT). Parameter-efficient fine-tuning (PEFT),
itively with the task-specific ability, leading which operates on a layer-level modular param-
to advancements in model pruning and train- eter selection basis (Houlsby et al., 2019; Li and
ing efficiency. Traditional fine-tuning meth-
Liang,2021;Lesteretal.,2021;Huetal.,2022),
ods engage all parameters of LLMs, which
seeks to reduce the trainable parameters during
iscomputationallyexpensiveandmaynotbe
modeltraining. Buildingoninsightsfrommodel necessary. In contrast, Parameter-Efficient
Fine-Tuning (PEFT) approaches aim to min- interpretabilityresearch,weproposethatthegran-
imizethenumberoftrainableparameters,yet ularityofparametertrainingcanberefinedtothe
they still operate at a relatively macro scale neuronlevel. Consequently,wepresentaNeuron-
(e.g.,layer-level). WeintroduceNeuron-Level LevelFine-Tuning(NeFT)approach,designedto
Fine-Tuning (NeFT), a novel approach that
improvemodelperformancebyselectivelyupdat-
refines the granularity of parameter training
ingneuronsidentifiedassensitive.
downtotheindividualneuron,enablingmore
Inaninitialexperiment,weidentifiedsensitive
precise and computationally efficient model
neuronsfortheNLItaskonLlama-2-7b-chat(Tou-
updates. The experimental results show that
NeFT not only exceeded the performance of vronetal.,2023)andusedatrainedprobetoselect
full-parameterfine-tuningandPEFTbutalso asubsetofneurons. Thepreliminaryexperiment
providedinsightsintotheanalysisofneurons. suggestthattrainingontheseneuronsoutperform
thefull-parameterfine-tuning. Toidentifysensitive
1 Introduction
neuronsformorecomplextasksliketranslationand
Neurons,asfundamentalcomponentsofLargeLan- summarization,wedevisedamethodthatevaluates
guageModels(LLMs),fulfilldiverserolesacross neuronsimilaritypre-andpost-SFT,treatingthose
modelregions. Aslanguagemodelsscale,theneu- with low similarity as sensitive. The NeFT sur-
ronsdisplayvaryingphenomena. Increaseinmodel passesthefull-parameterfine-tuningmodelintask
sizeenhancesthethecapabilitytogeneralizefrom performance. Tounderstandwhyitworks,wefur-
texttobasicandevenunseenconcepts(Pateland thercategorizedneuronsintothreetypes,namely
Pavlick,2022). Theinternalconceptsmaybedis- strongly affected, suppressed, and indirectly af-
tributedacrossnumerousneurons,andasignificant fected neurons, and introduced “rank difference”
proportionofneuronscanbecomeinactive,never toassessneuronutilization. Ourfindingsindicate
triggering across diverse datasets (Durrani et al., that: 1)Neuronsexhibitvaryingdegreesofsensi-
2020; Voita et al., 2023). This sparsity has been tivityduringtheSFTprocess. 2)Neuronsstrongly
substantiatedbyrecentstudiesutilizingittoprune affectedbySFTelicitsignificantalterationsinpa-
LLMsorenhancetheefficiencyofLLMinference, rameter utilization patterns. 3) Neurons that are
importantforonetasktendtoberelevantforothers.
*WorkwasdonewhileinterningatTigerResearch.
†EqualContribution. Thisconsistencyimpliesthatneuronsidentifiedin
‡CorrespondingAuthor. onecontextmaybebeneficialfortransferlearning
1
4202
raM
81
]LC.sc[
1v12611.3042:viXrainsimilardatasets. cess leverages various metrics, such as Fisher in-
formation(Sungetal.,2021)orL regularization
0
2 Background
(Guo et al., 2021), to determine which parame-
tersareessential. Theseidentifiedparametersare
Parameter Efficient Fine-tuning PEFT tech-
then specifically targeted in the subsequent train-
nique aims to enhance the performance of pre-
ingphase. LotteryTicketSparseFine-Tuning,on
trainedmodelsontaskswhileminimizingthenum-
the other hand, targets parameters exhibiting the
beroftrainableparametersandcomputationalcom-
mostsignificantchangesduringaninitialepochsof
plexity. Thisapproachisparticularlybeneficialfor
full-parameterfine-tuning(Anselletal.,2022). To
reducing the training costs associated with large
date,sparsetrainingtechniques,havenotbeenthor-
pre-trainedmodels.
oughly investigated within the context of LLMs
PEFTcanbeaempiricalchoiceofspecificlay-
(Anselletal.,2024),norhavetheybeenexamined
ersormodules,andsomestudieshaveshownthat
fromaneuron-levelconsideration.
trainingonlyasinglelayercansometimesoutper-
formfull-parameterfine-tuning. Forinstance,Yuan
3 PreliminaryExperiment
etal.(2023b)foundthatfortranslationtasks,em-
beddingfine-tuningiseffectivethanfull-parameter
Previous interpretability analyses within feedfor-
fine-tuningexceptlow-resourcesettings. However,
wardnetworks(FFNs)haverevealednotablephe-
the empirical choice and layer-wise searching is
nomenaattheneuronlevel. ResearchbyGurnee
time-consuming while recent advances focus on
andTegmark(2023)hasdemonstratedthatcertain
leveragingexternalmoduletoupdateallthelayer-
neuronsexhibitsensitivitytospecificentitiesand
levelparameters.
can encapsulate world knowledge. Motivated by
Adapter The goal of adapter (Houlsby et al., this insight, we begin our investigation to select
2019; Pfeiffer et al., 2021; Rücklé et al., 2021; certainneuronsusingthesimilarprobingapproach
Liuetal.,2022)istoinsertasmallnumberofpa- andexaminetheireffectivenessduringSFT.
rametersintothemodel,andthentrainonlythese
3.1 SelectNeurons
parameters when fine-tuning a downstream task,
leavingtheoriginalparametersofthepre-trained Given that NLI classification task is well-suited
modelunchanged. Thismakesthetrainableparam- forprobing,wechooseitasourtestbedofprelimi-
eters more efficient and ensures that the original naryexperimenttodiscoverneuronsrelatedtothe
knowledgeisnotforgotten. NLI task. We first processed the XNLI dataset1
throughtheLlama-2-7b-chatmodel(Touvronetal.,
LoRA Low-Rank Adaptation (LoRA; Hu et al.
2023) to obtain the hidden states for each layer.
2022) has emerged as one of the most prevalent
Foreachsentenceinthedataset,weaveragedthe
methods in both academic research and industry
hidden states across all tokens, resulting in a set
applications. LoRA’s principal concept involves
of hidden states for each layer. We paired each
decomposingalargeweightmatrixintotwolow-
hiddenstatedatawiththecorrespondingtargetla-
rankmatrices,significantlyreducingthenumberof
bels,i.e.,Entailment,Neutral,andContradiction,
trainable parameters. The effectiveness of LoRA
to train a Ridge classifier as a probe. Next, we
dependsonthechosenrankandthespecificstruc-
usethetrainedprobetoidentifytheneuronsmost
tures to which it is applied. Although the two
sensitivetoXNLItasks. Specifically,weselected
low-rankmatricesintroducedbyLoRAaddtothe
100,000neuronsassensitiveneuronsaccordingto
model’s architecture, they do not introduce addi-
thecosinesimilaritybetweentheneuronsandthe
tionalcomputationalcostsduringinferenceasthey
hiddenstatesofprobedecisionspace.
functionconcurrentlywiththeoriginalstructures.
For the sake of practical implementation, LoRA
3.2 ExperimentalResults
generallyappliedtoaffectingthecomputationof
linearormulti-headattentionmechanisms. Uponisolatingthesensitiveneurons,weimplement
agradientmaskingtechniqueonthenon-sensitive
Sparse Fine-Tuning Sparse fine-tuning distin-
neurons throughout SFT process. This approach
guishesitselffrommethodsthataddexternalmod-
guarantees that updates are confined exclusively
uleslikeadaptersorLoRAbyintroducinganini-
tialsteptopinpointcriticalparameters. Thispro- 1https://github.com/facebookresearch/XNLI
2Prepare Select Train
Models Neurons Neurons
Select Some Neurons with the
Lowest Similarity Score
Compute Similarity
Generate Gradient Mask
(1) (2) (3)
Figure1: ThisdiagramshowsthewholeprocessofourproposedNeuron-LevelFine-Tuningmethod. (1)Prepare
two models, one is the original model (M ) and the other is the model (M ) trained with full-parameter
Org FT
fine-tuning. (2)CalculatethecosinesimilarityforeachpairofneuronsinthecorrespondingpositionsofM and
Org
M andselectthex%neuronswiththelowestscoreandrefertotheseneuronsassensitiveneurons. (3)Maskthe
FT
gradientsofnon-sensitiveneuronsduringSFTtrainingtoensurethatonlytheselectedneuronsareupdated.
tothepreviouslyidentifiedsensitiveneurons. No- 1. to facilitate the identification of sensitive neu-
tably, our NeFT strategy does not require the in- ronsthataremostinfluentialtomodelperformance
corporation of additional architectural elements, improvements.
resulting that a mere 6% of the total parameters
weremodifiedduringtraining. Allremainingtrain- 4.1 PrepareModels
ing parameters are maintained in alignment with
Infact,trainingamodelentailsthediscernmentand
thefull-parameterfine-tuning.
engagement of sensitive neurons. By evaluating
TheresultspresentedinTable1provethatNeFT
neurons that deviate most from their initial state,
ispossibletooutperformfull-parameterfine-tuning
wecanascertainwhichonesthemodelhasprior-
bytrainingonlyonasmallnumberofsensitiveneu-
itized during its training, thereby revealing their
rons. However, this experiment on the NLI task
sensitivity to the task at hand. Building on this
may not fully exemplify the generality of NeFT.
premise, we begin by preparing an initial model
In the subsequent sections, we will introduce a
alongsideitsfine-tunedcounterpartforthedesig-
methodologydesignedtoidentifyandtrainsensi-
natedtask. WeemploytheSFTdatasetD totrain
tiveneuronsinmoreintricatecontexts,therebyex-
initialmodelM withallparameters,resulting
Org
tendingtheapplicabilityofNeFTacrossabroader
inthefine-tunedmodelM . Inpractice,forthe
FT
spectrumoftasks.
sakeofefficiency,thefine-tunedmodelalsocanbe
derivedwithinalimitednumberoftrainingsteps.
English French German Ouranalysisinsubsequentsectionssuggeststhat
this abbreviated training regimen yields a model
FT-full 82.8 70.7 67.0
whose performance is not significantly different
NeFT 84.9 76.9 80.2
fromonethathasbeentrainedtoconvergence.
Table1: PreliminaryexperimentofNeFTeffectiveness
wasconductedonXNLItasks. NeFTdemonstratedsu- 4.2 SelectNeuronsbyModelItself
periorperformancecomparedtofull-parametertraining.
Wedefineanindividualneuronasadistinctentry
withintheweightmatrix. Forexample,inalinear
layer of an MLP with a weight matrix of dimen-
4 Methodology
sionRm×n,asingleneuroncorrespondstoarow
Currentprobingtechniquesfallshortinaddressing inthismatrix,whichisdimensionallyrepresented
complextasks. Toremedythis,weproposeaflexi- as R1×n. We then calculate the cosine similarity
bleapproachasshowninFigure1andAlgorithm foreachcorrespondingpairofneuronsbetweenthe
3
...
...
...
...
...
...
...
...originalmodelandthefine-tunedmodel. Aneuron wesourcedourtraininganddevelopmentdatasets
at a given position (i,n) within the models is de- from the News Commentary (Tiedemann, 2012)
notedbyW(i,n) andW(i,n) ,whereirepresentsthe and Lego-MT (Yuan et al., 2023a). The test set
Org FT
layerindexandnindicatesthen-thneuronwithin wascollatedfromFlores-101(Goyaletal.,2022)
thatlayer. FollowingtheformulainEquation1,we andLego-MT,ensuringadiverserangeoflinguis-
calculatethecosinesimilarityscore{S(i,n)}: tic challenges. Regarding the cross-lingual text
summarization task, our dataset was taken from
CrossSum(Bhattacharjeeetal.,2023). Duetocom-
w ·w
Org FT
S(w Org,w FT) = (1) putational resource limitations, we restricted our
||w ||×||w ||
Org FT
trainingtosentencescomprisingfewerthan1024
(i,n) (i,n) tokens,therebytargetingonalow-resourcesetting.
wherew ∈W ,w ∈W . Thek neu-
Org Org FT FT
rons with the lowest cosine similarity scores are
TrainingSetup Weconductedourexperiments
thenselectedforfurthertrainingprocess.
ontheLlama-2-7b-chatmodel. Wefine-tunedall
modelswithabatchsizeof6fortranslationtasks
4.3 Neuron-LevelFine-Tuning
andabatchsizeof3forsummarizationtasksun-
Aftercalculatingthesimilarityscoresforeachneu-
tiltheyreachedconvergence. Theoptimalcheck-
ron pair, we rank the neurons according to their
points were determined based on the lowest vali-
similarityscorestopinpointtheonestowhichthe
dationlossobservedonthedevelopmentdatasets.
model allocates the most attention during train-
Typically,thecheckpointcorrespondingtothelow-
ing,characterizedbytheirlowersimilarityscores.
est evaluation loss emerged from the first epoch.
We record the location of these neurons, repre-
However, in the summarization task, particularly
sentedbythepositionalinformation(i,n). Totrain
when dealing with smaller datasets, this conver-
solely the identified neurons, we modify the gra-
gencepointmightoccurduringthesecondepoch.
dient g(W ). Prior to each update during the
Org FortrainingwithLoRA,convergencewastypically
trainingprocess,werefertothepreviouslysaved
attainedwithin2to3epochs. InthecaseofNeFT,
positionalinformation(i,n)todecideifthegradi-
when a certain percentage x% of the model pa-
ent of a specific neuron should be retained or set
rametersweretrained,wedenotedthisasNeFT .
x%
tozero. Thisgradientmaskingpolicyensuresthat
Similarly, for LoRA configurations, if the LoRA
onlythespecificneuronsareupdated.
rankwassettor,werepresentedthisasLoRA .
r
Algorithm1Neuron-LevelFine-Tuning(NeFT)
Baselines Ourmethodwasevaluatedagainstfour
Require: SFTDatasetD,modelM Org. baselines:
1: Train full parameters of M Org on dataset D
withinlimitedstepsK andobtainM . • FT-full: Thisstrategyfine-tunesallparame-
FT
2: Calculatesimilarityscores{S(i,n)}byEq.1. tersoftheLLM.
3: Select x% of the neurons {w(i,n)} ←
argmin Select(W ,{i},{n}). • FT-{in | out}: This approach fine-tunes the
S(i,n) Org
4: for1,...,Epochsdo weightsofinputandoutputprojectionlayers
5: for1,...,Batchesdo ofalltheMLPs.
6: Calculategradientgofeachbatch.
7: g({W Org \{w(i,n)}}) ← 0 • FT-embed (Yuan et al., 2023b): This tech-
8: Backpropagatethegradientg. nique focuses solely on fine-tuning the em-
9: endfor beddinglayer.
10: endfor
• LoRA(Huetal.,2022): Weapplyittofine-
tunealllinearstructureswithineachlayer(ex-
5 Experiments cludingtheheadlayeroflanguagemodel).
5.1 ExperimentalSettings
Tomaintaintheintegrityofthecomparison,wead-
Data Our primary experiments are centered on justedthenumberoftrainableparametersofNeFT
machinetranslationandcross-lingualtextsumma- tocloselyalignwiththoseemployedintheLoRA
rization tasks. For the machine translation task, configurations.
4En→Zh En→Fr Fr→Zh Hi→Zh Hi→Fr En→Mi En→Bs
Method % 20k 100k 20k 100k 20k 100k 4.5k 4.5k 10k 10k
Para.
FT-full 100 22.22 26.42 26.65 31.00 18.20 22.22 6.21 6.37 14.50 7.35
FT-embed 2 23.51 25.40 29.71 30.85 19.33 20.82 6.91 6.78 2.37 4.33
FT-{in|out} 41 24.35 27.35 28.70 33.65 20.91 22.78 9.03 7.55 15.11 7.82
LoRA 9 27.15 26.22 33.14 33.52 22.86 22.25 8.64 5.66 4.55 8.22
r=256
NeFT 9 28.70 30.48 34.86 36.71 24.08 25.62 10.69 10.23 13.90 10.04
9%
Table 2: Performance comparison of each method on the translation task. For LoRA and NeFT, we report the
performance when tuning 9% of model parameters. In the majority of cases, NeFT outperforms the baseline
methods. MoredetailedresultsareavailableinAppendixTable6.
En→Zh Fr→Zh Hi→Zh
2.3k 0.1k 0.1k
Method % R1 R2 RL R1 R2 RL R1 R2 RL
Para.
FT-full 100 21.88 17.39 20.71 30.24 19.79 25.36 10.16 4.35 9.42
FT-embed 2 1.35 0.67 1.09 10.48 6.39 8.32 0.73 0.19 0.67
FT-{in|out} 41 23.13 18.16 22.07 32.45 26.59 29.24 15.51 8.21 13.60
LoRA 9 19.94 15.73 18.74 4.48 2.72 4.48 2.46 0.45 2.16
r=256
NeFT 9 24.31 19.27 23.29 33.57 26.67 31.46 15.23 9.70 14.21
9%
NeFT 12 23.38 18.71 22.54 29.96 22.26 25.33 13.56 8.12 12.58
12%
NeFT 12 26.05 21.06 25.07 30.85 22.86 26.60 15.78 9.32 14.36
9%Union
Table3: Performancecomparisonofeachmethodoncross-lingualsummarizationtask. ForcomparingLoRAwith
NeFT,wereporttheperformancewhentuning9%ofmodelparameters. TheresultsindicatethatNeFTremains
effectiveeveninlow-resourcesettings.
Evaluation WeemploytheordinaryBLEUscore apotentialtoimprovecross-lingualgeneralization.
(Papinenietal.,2002)toassessthequalityofma- Forsummarizationtask,wemergeanequalnum-
chinetranslations,whichmeasuresthesimilarityof berofneuronsidentifiedfromthetranslationtask
n-gramsbetweenthemachine-generatedtextand ofaspecificlanguagepairwiththosefromthesum-
thereferencetranslation. HigherBLEUscoresindi- marization task. For instance, neurons from the
catebettertranslationquality. Inthecontextoftext NeFT configuration in the summarization task
6%
summarization,weutilizetheROUGEmetric(Lin, werecombinedwiththosefromtheNeFT setting
6%
2004)toevaluatetheextenttowhichthemachine- in the translation task. This amalgamated config-
generated summary encapsulates the core points urationisreferredtoasNeFT . Ourobser-
6%Union
of the reference summary. ROUGE assesses the vations indicate that the potential for further im-
recallrateofthegeneratedsummariesbycalculat- provingcross-lingualsummarizationwithneurons
ingtheoverlapofN-grams,whicharerepresented identified in translation task is more pronounced
as“R(N)”inthereportedresults,whereN corre- when dealing with high-resource language pairs
spondstothelengthoftheN-gram. likeEnglish-to-Chinesetranslation.
5.2 MainResults
Performance and Generalization In Table 2 ComparewithLoRA Weconductedacompre-
and 3, we present PEFT performance for each hensivecomparisonwithLoRAasshowninFigure
method. It is evident that NeFT outperforms the 2. We varied LoRA rank between 8 and 256 in
othermethodsinthemajorityofcases,regardless order to maintain a comparable number of train-
of task and language pair. Notably, in machine ableparameterswithNeFTneurons. Ourfindings
translationexperimentsacrossvariousdatascales, reveal that NeFT outperforms LoRA in terms of
wefine-tunethesamesubsetofneurons. translationquality,particularlyintheX-to-Chinese
In addition to its superior performance on tar- translationdirection. Fordetailedcomparisonsand
getedlanguagepairs,wealsofindthatNeFThave scores,pleaserefertoAppendixTable6.
5 1 H ) 7  : L Q  / R 5 $  : L Q Avg ΔRank (All)
20000
 ( Q    = K    
 ) U    = K    
10000
 + L    = K    
0
0 0.010.030.1 0.3 0.5 0.7 0.90.970.99 1
Figure2: ComparisonofNeFTandLoRAacrossdif- Percentiles (Top %) of Pearson Scores
ferenttrainableparametersettings. NeFTconsistently
utilizesfewerparametersthanLoRAateachlevel. The Figure3: AveragerankdifferencesbetweenNeFT
6%
detailsarepresentedinAppendixTable6. and NeFT were calculated for neurons. The ranks
3%
weresortedbasedontheirpairwisePearsonscoresin
descendingorder.
6 Analysis
6.2 EffectsofNeuronSelectionSettings
6.1 UtilizationofNeurons PerformanceComparison Toinvestigatetheim-
pactofneuronselectionbasedonsimilarityscore
Metric Toexplorethedivergenceofneuronuti- on NeFT training, we incorporated neurons with
lization,wecomparedmodelstrainedundervarious bothhighandlowcosinesimilarityscoresintothe
NeFTconfigurationsbyanalyzingneuronutiliza- NeFT model and examined the respective per-
3%
tion. Duringinferenceprocess,weretainedthehid- formancetrends. Thestudyusedadatasetcompris-
denstatesforeachlayerandcalculatedthePearson ing20,000English-to-Chinesetranslationtraining
correlationcoefficientforeachneuron,recording instances. Adding neurons with high similarity
thehighestvalueperneuron. Wethenorderedthe scoreswerelabeledReversed forclarity. Hence,
x%
neurons by their maximum Pearson score in de- thenotation“NeFT +Reversed ”representsa
3% x%
scendingorder,denotingtherankingofneuronsas hybrid selection strategy by combining neurons
Rank. To discern the contrast in neuron utiliza- fromNeFT andReversed . Accordingtothe
3% x%
tionbetweenmodels,wecomputetherankdispar- results presented in Figure 4, the original NeFT
ity for each neuron across the two models, repre- neuronselectionstrategyconsistentlyoutperforms
sentedas∆Rank. Wethendeterminedthemean thecontrastingstrategy. Moreover,italsoshowsa
of the absolute values of these rank differences, declineinmodelperformanceastheproportionof
Avg(∆Rank),toquantifytheoveralldivergence high-similarityneuronsincreases.
inutilizationoftheneurons.
DynamicsofNeuronUtilization Figure5pro-
vides insights into how neuron utilization is in-
ShiftsofNeuronUtilization Toanalyzehowuti- fluenced by different NeFT training configura-
lizationofneuronsshiftafterfine-tuning,wecate- tions. In Figure 5 (a), the comparison with
gorizetheneuronsintodifferentbucketsaccording the NeFT % model which employs the original
3
theirPearsoncorrelationscoresandcalculatethe selection strategy, shows that neuron utilization
average rank differences of each bucket between discrepancies becomes more pronounced when
NeFT %andNeFT %,denotedasAvg(∆Rank). the model is trained with high-similarity neu-
6 3
Thebucketingstrategyreliesonthetoppercentiles rons. This observation implies that the contrast-
of the overall Pearson scores. Figure 3 demon- ing strategy, NeFT %+Reversed %, tends to in-
3 x
stratesthattheshiftinutilizationforneuronswithin duce greater volatility within the neuron utiliza-
thetop0%to0.03%rangeisnegligible,whereas tionpatterns. Figure5(b)analyzesthedifferences
neurons with intermediate correlation scores ex- in utilization between models with a larger pro-
hibitthemostsignificantsensitivitytofine-tuning. portion of trainable parameters and those with
Thesefindingssuggestthatourselectionstrategyof a smaller proportion, while employing the same
sensitiveneuronsisreasonable. Furthermore,we fine-tuningstrategy. Thiscomparisonismadebe-
alsofoundthatthisobservationisconsistentacross tween both the original selection strategies (e.g.,
varioustrainingsettingsforotherNeFTmodels. NeFT % vs. NeFT %) and the contrasting se-
12 x
6Category of Neurons Based on the observed
29.0
28.83 rankingshifts,wefurtherclassifyneuronsintoone
28.67 28.71 28.7
of three different categories: “Strongly Affected
28.5 28.4
Neurons”,“SuppressedNeurons”,and“Indirectly
AffectedNeurons”.
NeFT
28.0 3+x%
NeFT 3%+Reversed x% • Strongly Affected Neurons: Neurons ex-
27.59 27.59
hibitingarankdifference∆Rankexceeding
27.5 27.38
100,000arecategorizedasStronglyAffected
27.22
Neurons,indicatingsignificantinfluencefrom
27.0
27.0
NeFTtraining.
3 4 6 7 9
Additional Selection Percentage (%) • Suppressed Neurons: Within the subset of
strongly affected neurons, not all exhibited
Figure4: BLEUscoresofmodelstrainedwithdifferent upward movement in their rankings. Conse-
NeFTsettings. ByusingNeFT asabasesetting,neu- quently,weclassifyneuronsthatdecreasedin
3%
ronshavehighsimilarityscoresandthosewithlowsim- rankasSuppressedNeurons.
ilarityscoreswereseparatelyincorporatedandtrained
using20kEnglish-Chinesetranslationdata. • Indirectly Affected Neurons: The training
ofaspecificsubsetofneuronsinherentlyim-
pactstheremaining,untrainedneuronsduring
lectionstrategies(e.g., NeFT %+Reversed %vs.
3 9 the inference process. Within the subset of
NeFT %+Reversed %). Fromtheobserveddata,
3 x stronglyaffectedneurons,weobservethata
different from Figure 5 (a), it is apparent that
substantial number of strongly affected neu-
models employing the contrasting neuron selec-
ronswerenotdirectlyengagedinNeFTtrain-
tionstrategyarelessaffectedbyanincreaseinthe
ing. These neurons are thus designated as
numberoftrainableparameters. Thissuggeststhat
IndirectlyAffectedNeurons.
theoriginalneuronselectionstrategyhasahigher
degree of stability with respect to changes in the
scaleoftrainableparameters.
NeFT 6% 17172 NeFT 6% 23534
NeFT 18860
7% NeFT 21680
7%
NeFT 22295
9%
NeFT 21757
9%
NeFT 27005
10%
NeFT 12% 26135 NeFT 10% 22055
NeFT 3%+Reversed 3% 36529 NeFT 3%+Reversed 3% 15331
NeFT +Reversed 36090
3% 4%
NeFT +Reversed 14107
3% 4%
NeFT 3%+Reversed
6%
38135
NeFT +Reversed 9762
NeFT +Reversed 37769 3% 6%
3% 7%
NeFT 3%+Reversed 9% 39948 NeFT 3%+Reversed 7% 9989
0 25000 0 20000
(a) Avg ΔRank with NeFT (b) Avg ΔRank with NeFT &
3% 12%
Avg ΔRank with NeFT +Reversed
3% 9%
Figure 6: The distribution of three types of neurons
Figure5: RankdifferenceAvg(∆Rank)iscalculated
(Stronglyaffected,Suppressed,andIndirectlyaffected)
in order to assess the shifts in the utilization of neu-
acrossmodelsundervarioustrainingsettings.
rons. Overall,theneuronutilizationoforiginalneuron
selectionstrategyNeFT ismorestablethanthatof
x%
contrastingselectionstrategyNeFT 3%+Reversed x%. CategorizedComparison Additionally,weas-
sessedthequantityofdifferentneurontypeswithin
7
UELBthe original selection strategy NeFT and the showninTable4,theneuronsidentifiedbytheun-
x%
contrasting strategy NeFT %+Reversed . The convergedmodelalsocandeliverssatisfactoryre-
3 x%
categorizationincludesstronglyaffectedneurons, sults,indicatingthateffectiveneuronselectiondoes
suppressed neurons, and indirectly affected neu- notnecessarilyrequireafullyconvergedmodel.
rons, as illustrated in Figure 6. The trend indi-
NeuronGeneralization Toexploretheinfluence
cates a higher count of strongly affected neurons
ofidentifiedneuronsonadditionaldatasetswithin
in the contrasting strategy compared to the origi-
thesamedomain,wecreatedanadditionalEnglish-
nal selection strategy (Figure 6 (a) and Figure 6
to-Chinese(En-Zh)translationtrainingdataset. We
(b)). In the case of suppressed neurons, there is
thenappliedtheneuronsidentifiedfromtheorigi-
a noticeable trend: as the number of strongly af-
naldataset,denotedasNeFT ,totrainmod-
fected neurons rises, the quantity of suppressed x%Trans
elsontheseunseendatainstances. Theresultsin
neuronsincreasescorrespondingly,maintaininga
Table4indicatethatneurons,onceidentifiedbythe
consistent proportion. In Figure 6 (c), the num-
model,canbeeffectivelyemployedacrosssimilar
berofstronglyaffactedneuronfoundbetweentwo
datasets,maintainingconsistentperformance. This
NeFT %+Reversed settingsismuchlowerthan
3 x% suggestthepotentialforneuronsidentifiedinone
thatbetweenNeFT %+Reversed andNeFT %.
3 x% 3 contexttobeextrapolatedtoothercontextswithin
Also,theproportionofindirectlyaffectedneurons
thesamedomain.
isverylowwhichdifferentfromFigure6(a)and
Figure6(b). OverlapAnalysis Weconductedfurtheranalysis
Based on the above observations discussed in ontheoverlapofneuronsidentifiedunderdifferent
thissection,weinferthattheinclusionofneurons settingsaspertheablationexperimentspreviously
withhighsimilarityscorecausesdrasticchangesin described. ThedatapresentedinTable4revealthat
themodel’sneuronalutilizationpatterns. However, whilethespecificneuronsselectedacrossdifferent
asthenumberofneuronsincreases,theextentof settings are not identical, there is a considerable
changebecomesrelativelysmall. Additionally,the degreeofoverlap. Furthermore,theextentofthis
trendobservedwiththeinclusionofmoreneurons overlap escalates with an increase in the number
mayimplythatonlyaminimalnumberofstrongly ofneuronsdesignatedforselection. Despitesome
affectedneuronsaresubjecttoindirectinfluence, variability in the specific neurons identified, the
leadingtominimalshiftsintheneurons’utilization. overall impact on the performance of models re-
mainslargelyconsistent,asevidencedbytheprevi-
6.3 AblationStudy ousresults.
NeuronsOverlap NeFTx%
OriginalData(20k) 3% 6% 9% 12%
Original(20k)|Separated(20k) 63% 68% 72% 76%
FT-full 22.22
Original(20k)|Original(100k) 61% 65% 68% 71%
NeFT 28.70 Convergence(20k)|Non-convergence(20k) 63% 68% 72% 75%
9%
NeFT 28.51
9%Early
Table5: Overlapproportionofneuronsacrossdifferent
SeparatedData(20k)
dataandtrainingsettings.
FT-full 22.12
NeFT 28.46 Takingintoaccounttheinsightsfromthepreced-
6%
NeFT 6%Trans 28.57 ingsections,wecandeducethatthecrucialfactor
regardingneuronselectionisnottheinclusionofa
Table4: BLEUscoresofNeFT-tunedEnglish-Chinese
substantialquantityofnon-sensitiveReversed %
x
translationmodelunderdifferentablationsettings.
neurons. Regarding NeFT % neurons, the selec-
x
tioncriteriadonotrequireexcessiveprecision,ap-
NeuronSelection Wealsofocusonassessingthe proximateselectionsarefoundtobeadequate.
impactofneuronsselectedbythemodelpriortoits
7 Conclusion
convergence. We selected neurons from a model
that had been trained on the English-to-Chinese In this study, we introduced Neuron-level Super-
(En-Zh)translationdatasetforonly800steps,and visedFine-tuning, NeFT,aninnovativeapproach
these neurons are denoted as NeFT and tosupervisedfine-tuning. NeFTfocusesoniden-
x%Early
would be used to select sensitive neurons. As tifying task-specific neurons within a model by
8evaluating the similarity between the neurons of (GrantNo. MYRG-GRG2023-00006-FST-UMDF).
the trained and the original model. By selecting ThisworkwasperformedinpartatSICCwhichis
andtrainingonneuronsthatexhibitlowsimilarity. supported by SKL-IOTSC, and HPCC supported
OurempiricalresultsdemonstratethatNeFTgener- byICTOoftheUniversityofMacau.
allysurpassesfull-parameterfine-tuningandother
fine-tuningmethodologiesacrossvarioussettings.
References
When compared with LoRA, NeFT consistently
showssuperiororcomparableresultsatmostlevels AlanAnsell,EdoardoMariaPonti,AnnaKorhonen,and
of parameter counts. Furthermore, our analyses IvanVulic.2022. Composablesparsefine-tuningfor
cross-lingualtransfer. InProceedingsofthe60thAn-
shed light on the functioning of distinct neurons
nualMeetingoftheAssociationforComputational
intheNeFTprocessandtheimpactofvariousset-
Linguistics (Volume 1: Long Papers), ACL 2022,
tings. This understanding contributes to a more Dublin,Ireland,May22-27,2022,pages1778–1796.
nuancedviewofthefine-tuningprocessandopens AssociationforComputationalLinguistics.
avenuesforfurtheroptimizationandrefinementof
Alan Ansell, Ivan Vulic, Hannah Sterz, Anna Ko-
neuron-leveltrainingstrategies. Inthefuture,we rhonen, and Edoardo M. Ponti. 2024. Scaling
willkeepapplyingNeFTmethodtomoregenera- sparsefine-tuningtolargelanguagemodels. CoRR,
abs/2401.16405.
tionandreasoningtasks.
AbhikBhattacharjee,TahmidHasan,WasiUddinAh-
Limitation mad, Yuan-Fang Li, Yong-Bin Kang, and Rifat
Shahriyar.2023. Crosssum: Beyondenglish-centric
This work encounters certain constraints due to cross-lingual summarization for 1, 500+ language
thecapabilitiesofcurrentdistributedframeworks, pairs. InProceedingsofthe61stAnnualMeetingof
theAssociationforComputationalLinguistics(Vol-
particularlyregardinggradientoperations. Specif-
ume1: LongPapers),ACL2023,Toronto,Canada,
ically,weareunabletoapplygradientoperations
July9-14,2023,pages2541–2564.Associationfor
universallyacrossallstructuralcomponentsofthe ComputationalLinguistics.
model. Consequently, our experiments are con-
Nadir Durrani, Hassan Sajjad, Fahim Dalvi, and
finedtotheup_projanddown_projprojections
YonatanBelinkov.2020. Analyzingindividualneu-
within the Llama-2-7b-chat architecture, and we ronsinpre-trainedlanguagemodels. InProceedings
havenotextendedourinvestigationtoincludethe ofthe2020ConferenceonEmpiricalMethodsinNat-
gate_projprojections. Despitetheserestrictions, ural Language Processing, EMNLP 2020, Online,
November16-20,2020,pages4865–4880.Associa-
theNeFTmethodologyhasdemonstratedimpres-
tionforComputationalLinguistics.
siveresults. Forfutureanalyses,thereareexisting
neuron-pruningmethodsthattreatneuronsasdis- Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi
Mi,andXinchaoWang.2023. Depgraph: Towards
crete units, as referenced by (Fang et al., 2023;
any structural pruning. In IEEE/CVF Conference
Zhuang et al., 2020; Han et al., 2015; Sun et al.,
onComputerVisionandPatternRecognition,CVPR
2023;Maetal.,2023). Thesemethodscouldpoten- 2023, Vancouver, BC, Canada, June 17-24, 2023,
tiallybeleveragedinmodelpruningtoeliminate pages16091–16101.IEEE.
certainneurons. Thiswouldenableamoreconcen-
EliasFrantarandDanAlistarh.2023. Sparsegpt: Mas-
trated examination of the neurons that contribute sive language models can be accurately pruned in
themostvaluetothemodel’sperformance,thereby one-shot. InInternationalConferenceonMachine
Learning, ICML 2023, 23-29 July 2023, Honolulu,
optimizingthefine-tuningprocessbyfocusingon
Hawaii,USA,volume202ofProceedingsofMachine
themostimpactfulneurons.
LearningResearch,pages10323–10337.PMLR.
Acknowledgements NamanGoyal,CynthiaGao,VishravChaudhary,Peng-
Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-
This work was supported in part by the ishnan,Marc’AurelioRanzato,FranciscoGuzmán,
and Angela Fan. 2022. The flores-101 evaluation
Science and Technology Development Fund,
benchmark for low-resource and multilingual ma-
Macau SAR (Grant Nos. FDCT/060/2022/AFJ,
chinetranslation. Trans.Assoc.Comput.Linguistics,
FDCT/0070/2022/AMJ),NationalNaturalScience 10:522–538.
Foundation of China (Grant No. 62261160648),
DemiGuo,AlexanderM.Rush,andYoonKim.2021.
Ministry of Science and Technology of China
Parameter-efficienttransferlearningwithdiffprun-
(Grant No. 2022YFE0204900), and the Multi-
ing. InProceedingsofthe59thAnnualMeetingof
yearResearchGrantfromtheUniversityofMacau theAssociationforComputationalLinguisticsand
9the11thInternationalJointConferenceonNatural ZichangLiu,JueWang,TriDao,TianyiZhou,Binhang
LanguageProcessing,ACL/IJCNLP2021,(Volume1: Yuan,ZhaoSong,AnshumaliShrivastava,CeZhang,
LongPapers),VirtualEvent,August1-6,2021,pages Yuandong Tian, Christopher Ré, and Beidi Chen.
4884–4896.AssociationforComputationalLinguis- 2023. Dejavu: Contextualsparsityforefficientllms
tics. at inference time. In International Conference on
Machine Learning, ICML 2023, 23-29 July 2023,
WesGurneeandMaxTegmark.2023. Languagemodels Honolulu,Hawaii,USA,volume202ofProceedings
representspaceandtime. CoRR,abs/2310.02207. ofMachineLearningResearch,pages22137–22176.
PMLR.
SongHan,JeffPool,JohnTran,andWilliamJ.Dally.
2015. Learning both weights and connections for XinyinMa,GongfanFang,andXinchaoWang.2023.
efficientneuralnetworks. CoRR,abs/1506.02626. Llm-pruner: Onthestructuralpruningoflargelan-
guagemodels. CoRR,abs/2305.11627.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
BrunaMorrone,QuentindeLaroussilhe,AndreaGes- KishorePapineni,SalimRoukos,ToddWard,andWei-
mundo, Mona Attariyan, and Sylvain Gelly. 2019. JingZhu.2002. Bleu: amethodforautomaticevalu-
Parameter-efficienttransferlearningforNLP. InPro- ationofmachinetranslation. InProceedingsofthe
ceedingsofthe36thInternationalConferenceonMa- 40thAnnualMeetingoftheAssociationforCompu-
chineLearning,ICML2019,9-15June2019,Long tationalLinguistics,July6-12,2002,Philadelphia,
Beach,California,USA,volume97ofProceedings PA,USA,pages311–318.ACL.
of Machine Learning Research, pages 2790–2799.
PMLR. RomaPatelandElliePavlick.2022. Mappinglanguage
modelstogroundedconceptualspaces. InTheTenth
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan International Conference on Learning Representa-
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and tions,ICLR2022,VirtualEvent,April25-29,2022.
WeizhuChen.2022. Lora: Low-rankadaptationof OpenReview.net.
largelanguagemodels. InTheTenthInternational
ConferenceonLearningRepresentations,ICLR2022, Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,
VirtualEvent,April25-29,2022.OpenReview.net. Kyunghyun Cho, and Iryna Gurevych. 2021.
Adapterfusion: Non-destructivetaskcompositionfor
EldarKurtic,DenisKuznedelev,EliasFrantar,Michael transfer learning. In Proceedings of the 16th Con-
Goin, and Dan Alistarh. 2023. Sparse fine-tuning ferenceoftheEuropeanChapteroftheAssociation
forinferenceaccelerationoflargelanguagemodels. forComputationalLinguistics: MainVolume,EACL
CoRR,abs/2310.06927. 2021, Online, April 19 - 23, 2021, pages 487–503.
AssociationforComputationalLinguistics.
BrianLester,RamiAl-Rfou,andNoahConstant.2021.
The power of scale for parameter-efficient prompt AndreasRücklé,GregorGeigle,MaxGlockner,Tilman
tuning. InProceedingsofthe2021Conferenceon Beck, Jonas Pfeiffer, Nils Reimers, and Iryna
EmpiricalMethodsinNaturalLanguageProcessing, Gurevych. 2021. Adapterdrop: On the efficiency
EMNLP2021,VirtualEvent/PuntaCana,Domini- of adapters in transformers. In Proceedings of the
can Republic, 7-11 November, 2021, pages 3045– 2021ConferenceonEmpiricalMethodsinNatural
3059.AssociationforComputationalLinguistics. LanguageProcessing,EMNLP2021,VirtualEvent
/PuntaCana,DominicanRepublic,7-11November,
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: 2021,pages7930–7946.AssociationforComputa-
Optimizing continuous prompts for generation. In tionalLinguistics.
Proceedingsofthe59thAnnualMeetingoftheAsso-
ciationforComputationalLinguisticsandthe11th Yixin Song, Zeyu Mi, Haotong Xie, and Haibo
InternationalJointConferenceonNaturalLanguage Chen. 2023. Powerinfer: Fast large language
Processing, ACL/IJCNLP 2021, (Volume 1: Long modelservingwithaconsumer-gradeGPU. CoRR,
Papers),VirtualEvent,August1-6,2021,pages4582– abs/2312.12456.
4597.AssociationforComputationalLinguistics.
MingjieSun,ZhuangLiu,AnnaBair,andJ.ZicoKolter.
Chin-Yew Lin. 2004. ROUGE: A package for auto- 2023. Asimpleandeffectivepruningapproachfor
maticevaluationofsummaries. InTextSummariza- largelanguagemodels. CoRR,abs/2306.11695.
tionBranchesOut,pages74–81,Barcelona,Spain.
AssociationforComputationalLinguistics. Yi-LinSung,VarunNair,andColinRaffel.2021. Train-
ingneuralnetworkswithfixedsparsemasks. InAd-
HaokunLiu,DerekTam,MohammedMuqeeth,JayMo- vancesinNeuralInformationProcessingSystems34:
hta,TenghaoHuang,MohitBansal,andColinRaffel. AnnualConferenceonNeuralInformationProcess-
2022. Few-shotparameter-efficientfine-tuningisbet- ing Systems 2021, NeurIPS 2021, December 6-14,
terandcheaperthanin-contextlearning. InAdvances 2021,virtual,pages24193–24205.
in Neural Information Processing Systems 35: An-
nualConferenceonNeuralInformationProcessing Jörg Tiedemann. 2012. Parallel data, tools and inter-
Systems2022,NeurIPS2022,NewOrleans,LA,USA, faces in OPUS. In Proceedings of the Eighth In-
November28-December9,2022. ternationalConferenceonLanguageResourcesand
10Evaluation, LREC2012, Istanbul, Turkey, May23- A Appendix
25,2012,pages2214–2218.EuropeanLanguageRe-
sourcesAssociation(ELRA). Table 6 presents the results of NeFT and LoRA
under various training settings for translation of
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
eachlanguagepair. Italsoincludesacomparison
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti between NeFT and LoRA at similar trainable pa-
Bhosale,DanBikel,LukasBlecher,CristianCanton- rameterscales.
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
CynthiaGao,VedanujGoswami,NamanGoyal,An-
thonyHartshorn,SagharHosseini,RuiHou,Hakan
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Melanie Kambadur, Sharan Narang, Aurélien Ro-
driguez,RobertStojnic,SergeyEdunov,andThomas
Scialom.2023. Llama2: Openfoundationandfine-
tunedchatmodels. CoRR,abs/2307.09288.
ElenaVoita,JavierFerrando,andChristoforosNalmpan-
tis.2023. Neuronsinlargelanguagemodels: Dead,
n-gram,positional. CoRR,abs/2309.04827.
FeiYuan,YinquanLu,WenhaoZhu,LingpengKong,
LeiLi,YuQiao,andJingjingXu.2023a. Lego-mt:
Learningdetachablemodelsformassivelymultilin-
gual machine translation. In Findings of the Asso-
ciation for Computational Linguistics: ACL 2023,
Toronto, Canada, July 9-14, 2023, pages 11518–
11533.AssociationforComputationalLinguistics.
FeiYuan,ShuaiYuan,ZhiyongWu,andLeiLi.2023b.
How multilingual is multilingual llm? CoRR,
abs/2311.09071.
Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi
Zeng,KaiShuang,andXiangLi.2020. Neuron-level
structuredpruningusingpolarizationregularizer. In
AdvancesinNeuralInformationProcessingSystems
33: AnnualConferenceonNeuralInformationPro-
cessingSystems2020,NeurIPS2020,December6-
12,2020,virtual.
11#LangEn→Zh(20k) BLEU BLEU
NeFT|Params En→Zh Fr→Zh Hi→Zh LoRA|Params En→Zh Fr→Zh Hi→Zh
Rank8|20.0M 27.13 23.05 2.85
NeFT |25.6M 27.21 22.74 2.40 Rank16|40.0M 27.29 23.04 2.88
0.4%
NeFT |102.4M 28.32 23.30 2.17 Rank64|159.9M 27.34 22.21 0.90
1.5%
NeFT |204.8M 28.34 23.78 0.99 Rank128|319.8M 27.27 22.81 0.20
3%
NeFT |409.6M 28.67 23.76 0.81 Rank256|639.6M 27.15 22.60 0.09
6%
NeFT |614.4M 28.70 24.02 0.46
9%
NeFT |819.2M 28.40 23.80 1.30
12%
#LangFr→Zh(20k) BLEU BLEU
NeFT|Params En→Zh Fr→Zh Hi→Zh LoRA|Params En→Zh Fr→Zh Hi→Zh
Rank8|20.0M 24.19 22.77 1.18
NeFT |25.6M 24.49 23.00 8.61 Rank16|40.0M 24.27 22.55 1.71
0.4%
NeFT |102.4M 24.99 23.31 7.94 Rank64|159.9M 24.57 22.57 0.62
1.5%
NeFT |204.8M 24.95 23.67 8.09 Rank128|319.8M 20.87 22.83 0.56
3%
NeFT |409.6M 25.32 23.65 6.36 Rank256|639.6M 24.51 22.86 0.57
6%
NeFT |614.4M 24.74 24.08 6.96
9%
NeFT |819.2M 23.90 23.97 3.34
12%
#LangHi→Zh(4.5k) BLEU BLEU
NeFT|Params En→Zh Fr→Zh Hi→Zh LoRA|Params En→Zh Fr→Zh Hi→Zh
Rank8|20.0M 11.98 0.67 9.55
NeFT |25.6M 8.49 1.84 8.57 Rank16|40.0M 13.15 1.49 9.71
0.4%
NeFT |102.4M 2.01 0.91 9.45 Rank64|159.9M 11.75 6.12 10.01
1.5%
NeFT |204.8M 1.38 0.39 9.15 Rank128|319.8M 11.91 7.35 9.18
3%
NeFT |409.6M 2.30 5.18 9.68 Rank256|639.6M 10.82 8.10 8.64
6%
NeFT |614.4M 1.22 0.47 10.69
9%
NeFT |819.2M 0.05 0.04 11.21
12%
#LangEn→Fr(20k) BLEU BLEU
NeFT|Params en→Fr Hi→Fr LoRA|Params En→Fr Hi→Fr
Rank8|20.0M 33.13 1.73
Rank16|40.0M 33.77 1.90
Rank64|159.9M 33.18 0.26
NeFT |204.8M 34.25 3.66 Rank128|319.8M 32.98 0.17
3%
NeFT |409.6M 34.90 3.84 Rank256|639.6M 33.14 0.07
6%
NeFT |614.4M 34.86 3.52
9%
NeFT |819.2M 35.08 4.05
12%
#LangHi→Fr(4.5k) BLEU BLEU
NeFT|Params En→Fr Hi→Fr LoRA|Params En→Fr Hi→Fr
Rank8|20.0M 20.18 8.00
Rank16|40.0M 21.29 7.90
Rank64|159.9M 19.22 7.14
NeFT |204.8M 11.40 6.47 Rank128|319.8M 18.62 6.05
3%
NeFT |409.6M 12.45 8.59 Rank256|639.6M 16.90 5.66
6%
NeFT |614.4M 9.08 10.23
9%
NeFT |819.2M 0.26 10.48
12%
#LangEn→Mi(10k) BLEU BLEU
NeFT|Params En→Mi LoRA|Params En→Mi
Rank8|20.0M 6.22
Rank16|40.0M 5.88
Rank64|159.9M 5.56
Rank128|319.8M 4.87
NeFT |614.4M 13.90 Rank256|639.6M 4.55
9%
NeFT |819.2M 14.04
12%
#LangEn→Bs(10k) BLEU BLEU
NeFT|Params En→Bs LoRA|Params En→Bs
Rank8|20.0M 8.66
Rank16|40.0M 8.15
Rank128|319.8M 8.83
NeFT |614.4M 10.04 Rank256|639.6M 8.22
9%
Table6: TheBLEUscoresforallLoRAsettingsandthecomparisonwithNeFTofthecorrespondingparameter
magnitudeinthemachinetranslationexperiment.
12