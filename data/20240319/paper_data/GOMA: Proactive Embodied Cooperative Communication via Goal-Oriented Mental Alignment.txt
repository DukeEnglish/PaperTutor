GOMA: Proactive Embodied Cooperative Communication via
Goal-Oriented Mental Alignment
Lance Ying1,2, Kunal Jha3, Shivam Aarya4, Joshua B. Tenenbaum2, Antonio Torralba2, Tianmin Shu4
Abstract—Verbal communication plays a crucial role in (a) Cooperation with a shared mind (b) Cooperation with unaligned minds
human cooperation, particularly when the partners only have Shared mind Human’s mind Robot’s mind
incomplete information about the task, environment, and each Physical state Physical state Physical state
other’s mental state. In this paper, we propose a novel co-
Robot’s belief Human’s belief
operative communication framework, Goal-Oriented Mental Shared goal
Alignment(GOMA).GOMAformulatesverbalcommunication Robot’s goal Human’s goal
asaplanningproblemthatminimizesthemisalignmentbetween
thepartsofagents’mentalstatesthatarerelevanttothegoals.
This approach enables an embodied assistant to reason about
when and how to proactively initialize communication with Sharedjoint plan = Joint plan in human’s mind = Joint plan in robot’s mind =
{Human’s plan, Robot’s plan} {Human’s plan, Robot’s plan} {Human’s plan, Robot’s plan}
humans verbally using natural language to help achieve better
cooperation.Weevaluateourapproachagainststrongbaselines (c) Optimize verbal communication to align the joint plans
in two challenging environments, Overcooked (a multiplayer
Fig. 1: Illustration of cooperation with a shared mind or game) and VirtualHome (a household simulator). Our experi-
mentalresultsdemonstratethatlargelanguagemodelsstruggle misaligned minds and communication optimized via goal-
with generating meaningful communication that is grounded orientedmentalalignment.(a)Whenhumanandrobotminds
in the social and physical context. In contrast, our approach are perfectly signed (i.e., a shared mind), they share the
cansuccessfullygenerateconciseverbalcommunicationforthe
same belief of the physical state and the same goal, which
embodied assistant to effectively boost the performance of the
leads to the same joint plan shared by both agents. This
cooperationaswellashumanusers’perceptionoftheassistant.
is the ideal condition for reaching optimal cooperation. (b
I. INTRODUCTION However, in real-world cooperation, human and robot minds
are typically unaligned, leading to two different (and often
Richverbalcommunicationnaturallyemergesfromhuman
conflicting)jointplansintheirminds.(c)Toachieveashared
cooperationwhenpeopleonlyhavepartialinformationabout
joint plan that optimizes cooperation, we optimize verbal
the environments and/or about each other’s mental states
communication initiated by the robot to actively align the
[1]. It serves as a complementary source of information, in
joint plans in both agents’ minds.
addition to the visual inputs, to help achieve better coopera-
tion by aligning each other’s mental states (including goals,
beliefs, and eventually plans [2], [3], [4]). Recent advances
in large language models (LLM) and machine Theory of about the avocados.” In this scenario, your mom decides to
Mind (ToM) have sparked interest in building cooperative communicatewithyoubecausesheisuncertainwhetheryou
robots that can not only physically cooperate with humans have the same beliefs regarding the weather forecast and the
but also verbally communicate with humans using natural required grocery items as you walk out the door.
language [5], [6]. However, it remains challenging to enable When cooperating with one another, each agent not only
robots to actively initiate verbal communication that is both needs to plan for itself but also has to imagine the plans
concise (only communicate when necessary) and consistent of its partners. Such planning process is termed as joint
with the physical environment and the social context (e.g., planning [8], [9]. To achieve joint planning, prior works
what humans want to do, believe, know, and need to know). typically assumed that both agents have full observability
A long history of research in psychology has shown that andcompleteknowledgeaboutthetask.Inotherwords,they
proactive verbal communication serves to align the mental have a shared mind, based on which they can derive the
states of agents [7]. Imagine you are going to get some samejointplan(Fig.1(a)).However,inreal-worldembodied
groceriesforyourmom.Asyouputonyourshoesandwalk cooperation, robot assistants only have partial observations
towards the door, your mom gets out of the kitchen and and often do not know the true human goals (Fig. 1(b)).
says “It’s going to rain, get your umbrella, and don’t forget The goal of cooperative communication is then to reach
a shared mind (two agents’ are perfectly aligned) so that
1Harvard University, Cambridge, MA 02138, USA the resulting joint plans in both agents’ minds are the same
lanceying@seas.harvard.edu
2Massachusetts Institute of Technology, Cambridge, MA 01239, USA (Fig. 1(c)). Once we reach such mental alignment condition,
jbt@mit.edu, torralba@csail.mit.edu both agents know exactly what each other plans to do, and
3Dartmouth College, Hanover, NH 03755, USA therefore achieve optimal cooperation. However, an agent
kunal.a.jha.24@dartmouth.edu
belief can be about any part of a state. If the state is high
4JohnsHopkinsUniversity,Baltimore,MD21218,USA{saarya1,
tianmin.shu}@mit.edu dimensional (such as the state in a real-world home), it is
4202
raM
71
]CH.sc[
1v57011.3042:viXraextremely difficult to make sure two beliefs are the same. one-directional communication where the human instructs
Our key insight is that we only need to align the part of the robot [13], [14]. Some recent studies have proposed
the belief that is relevant to reaching the goal. bi-directional communication. For example, [15] proposes
Following this insight, we propose a novel cooperative a bi-directional human-robot collaborative communication
communication framework, Goal-Oriented Mental Align- framework that allows the robot to communicate decisions
ment (GOMA). In this framework, we aim to generate opti- with explanations from human feedback. [12] introduced
mal communication in the belief space. That is, verbal com- CommPlann,abi-directionalcommunicationframeworkthat
munication, by exchanging information, can help reshape allows the robot to ask for human’s intent, share the robot’s
agents’ beliefs. In particular, GOMA first seeks to detect intent, and give commands to humans. There have been
misalignment in agents’ goal-relevant beliefs via divergence recent works that use LLMs as a communication module in
between the joint plans based on an agent’s own belief bi-directional human-robot communication, (e.g., [5], [16],
and a simulated hypothetical shared mind after acquiring [17], [2], [18]). While these recent LLM-based agents can
additional knowledge from another agent via hypothetical achieve certain success, the communication generated by
communication. We then optimize the communication using LLMs is often redundant and/or not grounded in agents’
a proxy reward derived from the divergence between the mental states, actions, and plans.
plans. The resulting communication can then help us min- In addition, most human-robot communication frame-
imize the difference between the joint plan in each agent’s works, such as [12], [19], [20], assume full agent observ-
mind and the true joint plan given a true shared mind. By ability. The resulting communication is thus only restricted
doing so, we can optimize the cooperation. to informing and inquiring about goals and plans. Our work
WeevaluateGOMAintwopopularhuman-AIcooperation attemptstoextendtoscenarioswherebothagentshavepartial
domains, Overcooked and VirtualHome. Our experimental observability of the environment and allow the robot to
resultswithasimulatedhumanagentandrealhumanpartici- communicate to resolve partial knowledge and false beliefs
pantsshowthatourGOMAoutperformsstrongbaselines(in- about the environment state. This requires agents to model
cluding a recent LLM-based baseline). The GOMA-enabled andreasonabouteachother’smentalstatesrecursively(e.g.,
assistant also receives higher subjective ratings from human the robot thinks the human thinks the glass is in the fridge,
participants. but it knows that the glass is actually in the cabinet), which
Insum,ourcontributionsinclude(1)anovelembodiedco- remains a challenge for LLMs today [21]. As a result,
operativecommunicationframework–GOMA,(2)extensive such cooperative communication capacity remains an open
evaluationofstrongbaselinesandGOMAintwochallenging research question in embodied cooperation.
domains, and (3) a human user study that evaluates the
task performance of AI assistants and humans’ perception C. Theory of Mind for Cooperative Robot Planning
of them.
There have been many studies on inferring an agent’s
II. RELATEDWORK goals and beliefs (e.g., [22], [23], [24], [25], [26], [2], [27]),
commonly referred to as the Theory of Mind reasoning,
A. Communication in Collaboration
to better coordinate with humans in collaborative tasks.
Human communication is grounded in cooperative inten-
Previous studies have leveraged explicit mental reasoning to
tions. [7] argues that language communication is a joint
improvecooperativerobotplanning.Thisincludesgenerating
activity that attempts to achieve mutual understanding. [1]
more expressive or explainable plans to improve humans’
proposes three communicative motives: requesting help or
understanding of robots’ plans [28], [29], [30], [31], [4],
information,informingtheotheragents,andsharingfeelings
[32] or better understanding of humans’ cooperative actions
or attitudes. These communicative motives help to align the
[3], all via reasoning about humans’ mental models of
mental states of the agents. Through verbal communication,
the robot. There have also been works on developing a
agents can assess others’ goals, knowledge, emotions, and
shared joint planner in two agents’ minds to reach optimal
beliefs, which they can then use to plan for the next actions.
coordination by reasoning about one agent’s own plan and
However, verbal communication can also be costly, as
theotheragent’splanjointly.However,existingworksdonot
it demands cognitive resources and distracts agents when
allow verbal communication between humans and robots in
performingactions[10].Priorworksonmulti-agentteaming
addition to action planning. Our work aims to fill this gap
have formalized communication costs in collaborative set-
by jointly planning for actions that change the physical state
tings[10],[11],[12],showingthatexcessivecommunication
but verbal communication that changes the mental states of
can degrade the performance of the team. Therefore, when
humans and robots.
designing communication policies, the AI assistant needs to
communicate useful, concise, and relevant information yet
III. PROBLEMFORMULATION
not too frequently.
In this work, we consider two agents, a human user and a
B. Collaborative and Communicative AI Agent
robot assistant. To successfully communicate and cooperate,
Communicationbetweenhumansandrobotshasalsobeen the two agents must infer each other’s mind. We adopt the
extensively studied. Most existing literature has focused on Interactive Partially Observable Markov Decision Process(I-POMDP) [33], [34] to formulate the mental reasoning • Condition2:Therobot’sgoalisthehumangoalinferred
between the human and the robot. by the robot, and the human user knows that the robot
is trying to help with the inferred human goal.
A. Background: I-POMDP
Condition 1 models human-robot teaming, in which the
I-POMDP is a framework that enables an agent to recur- human and robot agents are teammates who work on the
sively model other agents, which captures complex social same task assigned to them a priori. Condition 2 models
interactions between agents. Here, we consider the inter- robot assistance, in which the human’s true goal is unknown
actions between two agents, i and j, in which agent i to the robot a priori, thus the robot must infer the human’s
infers agent j’s mental state recursively. In an I-POMDP, goal and provide assistance. In both cases, agents only have
there are states st; agents’ observations, ot and ot, sampled partialobservabilityofthephysicalstate,andthustheyhave
i j
from their conditional observation probabilities, O (ot|st) to infer both the physical state and each other’s belief about
i i
and O (ot|st); and agents’ actions at and at. Agents have the physical state. It is worth noting that our formulation
j j i j
their beliefs, bt and bt, and goals, θ and θ . To model the departs from most previous assistance-game setups, which
i j i j
recursive mental reasoning, we define interactive states for either assume that the agents have full observability or that
the agents, i.e., is and is , at level-ℓ. From agent i’s they share a known goal. As in collaborative tasks, agents
i,ℓ j,ℓ
perspective, we define its interactive state at each level as often do not have perfect knowledge of the environment
and thus need to represent other agents’ beliefs differently
• Level 0: is i,0 =s from theirs and communicate and coordinate their actions,
• Level 1: is i,1 =(s,b j,0,θ j) our formulation is more aligned with real-world embodied
• ···
cooperation.
• Level ℓ: is i,ℓ =(s,b j,ℓ−1,θ j)
Thelevel-ℓinferenceforagentiistoinferthebeliefbt =
i,ℓ IV. GOAL-ORIENTEDMENTALALIGNMENT
p(ist |o1:t,a1:t−1). Since the level-ℓ agent i’s interactive
i,ℓ i i
state, ist = (st,bt ,θ ), includes j’s belief at level As Fig. 1 illustrates, when there is a shared mind, two
i,ℓ j,ℓ−1 j
ℓ−1 (bt ), the inference at level ℓ depends on inference agents will share the same joint plan. In our Goal-oriented
j,ℓ−1
at level ℓ−1 which depends on inference at level ℓ−2, Mental Alignment (GOMA) framework, we formulate com-
and so on. This recursive inference terminates at level 0. munication optimization as the convergence of the current
That is, the belief at level-0 is only about the physical state, joint plan and the joint plan given a shared mind achieved
b = p(st|o1:t,a1:t−1). This becomes a standard POMDP by exchanging information through verbal communication.
i,0 i i
[35] which does not model other agents. In particular, we consider two types of communication –
sharing information and requesting information. These are
B. Two-level Reasoning for Embodied Cooperative Cooper- two dominant types of verbal communication in human
ation cooperation [1]. We hypothesize that these are also two
typesofcommunicationthatarobotassistantcanproactively
Theoretically, the level of agents’ reasoning about other
initiatetoachievejointplanalignment.Toreasonwhetherto
agents’ minds can go to infinity (e.g. robot thinks human
communicate and what to communicate, we define a proxy
thinks robot thinks...) yet we cap the depth at two in
reward for minimizing the divergence between plans before
our model, which is in line with most empirical evidence
andafteronetypeofcommunication.WesummarizeGOMA
suggestingthathumansrarelyengageingreaterthan2levels
in Algorithm 1, which works with any off-the-shelf action
of recursive Theory of Mind reasoning [36]. Therefore, we
planner. We introduce key components of the algorithm in
adoptatwo-levelI-POMDPformodelingthementalreason-
the rest of the section.
ing between a human user and a robot assistant in embodied
cooperation. In particular, we define the mind of each agent
A. Gaol Inference and Joint Planning for the Robot
as the belief of the level-1 interactive state of the agent.
For the human user’s mind, we have m H = b(is H,1) = Unless the human goal is given to the robot a priori (i.e.,
{b H,0,b(b R,0),b(g R)}, where b R,0 is the robot’s interactive condition 1 defined in Section III-B), the robot must infer
state at level 0, i.e., its belief about the physical state; and the human goal. We adopt the approach introduced by [2],
g R is the robot’s goal. Similarly, for the robot assistant, we which leverages an LLM to conduct goal inference based
define its mind as m R =b(is R,1)={b R,0,b(b H,0),b(g H)}, on the observed human actions and messages (Line 8-9 in
where b H,0 is the human’s belief about the physical state, Algorithm 1). We then sample the possible goals of humans
andg H istherobot’sgoal.Intuitively,eachmindmodelsthe The joint plan for the robot includes two components.
agent’sbeliefabout(1)thephysicalstate,(2)anotheragent’s First, the robot’s policy given its goal and its belief, i.e.,
belief about the physical state, and (3) the goal of another π (a |b ,g ). Second, the expected human’s policy in-
R R R,0 R
agent. Due to the cooperative nature of our problem setting, ferred by the robot, i.e., E [π (a |b ,g )].
we further constrain the goal inference to be either one of In practice, we can estimateb( tb hH is,0) e,b x( pgH ec) tatH ion H viaH s, a0 mpH ling
the following two conditions: particles of possible human beliefs (i.e., {b(l) (st)}L in
H,0 l=1
• Condition 1: Both agents share a known common goal; Algorithm 1) and possible goals (Line 11 in Algorithm 1).Algorithm 1 GOMA C. Shared Mind Augmented by An Agent’s Knowledge
1: Input:Planner(),Tmax An agent i can imagine a shared mind after acquiring
2: I {n bi (t li )al (iz sa 0t )i }o Ln: b(gH), bR,0(s0), particles of sampled human beliefs:
knowledge about a sub-state, b j,0(s n) ∈ K i, from another
3: t←H,0 1,u0 =l=1 None agent j via verbal communication, as both agents would
R
4: repeat sharethisknowledgeafterthecommunication.Wedefinethis
5 6: : O Ub ps de ar tv ee leo vt R el-a 0nd ber le iec fe :iv be Rh ,0u (m sa tn )bm ae ss es dag oe nu bt H o− th1 ot
R
andut H−1 a Ss peth cie ficb ae ll li ye ,f tm hie srg me ero gp eer oa pti eo rn atib o+ i n,0sn w= illM see tr tg he e(b bi e,0 li, eb fj, o0( fs sn u) b) -.
87 :: URo pb do att ek hn uo mwl ae ndg ge o: alK inRt fe= renK ceR :(bR,0(st)) statenofagentitothatofagentj,i.e.,b+ i,0sn(s n)=b j,0(s n).
9: b(gH)∝P(at H−1|gH)P(ut H−1|gH)b(gH),∀gH ∈G
10: foralll=1,···,Ldo D. Divergence Between Plans as Proxy Reward
11: Sampleahumangoalbasedonthegoalinference:gˆ H(l)∼b(gH)
It is hard to directly estimate the effect of an utterance
12: Settherobotgoalastheinferredhumangoal:g(l)←gˆ(l) on the overall task performance. To directly reason what
R H
13: Sampleanenvironmentstatest∼bR,0(st) knowledge is critical for aligning the joint plans between
14: Sampleinferredhumanobservationsoˆt
H
∼OH(oˆt H|st)
agents, we define a proxy reward for communicating about
15: Updateb(l) (st)basedonbothoˆt andut−1
H,0 H R the knowledge of an agent’s knowledge. Since the goal of
16: Humanplangiventheinferredhumanbelief:
17: πH(at H|bH,0,gˆ H(l))←Planner(bH,0,gˆ H(l)) thisworkistogenerateproactivecommunicationinitiatedby
18: Humanplansgiventhesharedmindsaugmentedbydifferentsub- the robot, we model the proxy reward from the perspective
statesinrobotknowledge: of the robot.
19: {πH(at H|b H+s ,0n,gˆ H(l))←Planner(b+ Hs ,0n,gˆ H(l));∀st n∈K Rt}
Wefirstdefinetherewardofsharingtherobot’sknowledge
20: Robotplangiventherobotbelief:
21: πR(at R|bR,0,g R(l))←Planner(bR,0,g R(l)) of sub-state s n with the human user as follows:
22: Robotplansgiventhesharedmindsaugmentedbydifferentsub-
statesinhumanknowledge: R(share s n,M R)=
23: {πR(at R|b+ R,s 0n,g R(l)) ← Planner(b+ R,s 0n,g R(l)); ∀st
n
∈ KL(cid:16)
E[π (a |b+sn,g )]||E[π (a |b ,g
)](cid:17)
−C, (2)
K(b(l) (st))} H H H,0 H H H H,0 H
H,0
24: endfor
25: mR←(bR,0(st),{b( Hl) ,0(st)}L l=1,b(gH)) where b+ Hs ,0n = Merge(b H,0,b R,0(s n)) and C is the cost for
26: Allpossiblehumanknowledge:Kˆt =∪L K(b(l) (st)) communication at a time step.
H l=1 H,0
27: ConstructtheutterancespaceU basedontherobotknowledgeKt We then define the reward of requesting possible human
R
andallpossiblekumanknowledgeKˆt knowledge of sub-state s to inform the robot’s plan:
H n
28: ComputeR(u,MR),∀u∈Uusingtheplansgeneratedabovebased
onEq.(2-4) R(request s ,M )=
n R
29: Selectrobotutterancebasedontheproxyreward: (cid:16) (cid:17)
3 30 1:
:
u Set R le= cta ror bg om taa cx tiu o∈ nU baR s( eu d, om nR th)
eaverageplan:
KL π R(a R|b+ R,s 0n,g R)||π R(a R|b R,0,g R) −C, (3)
3 32 3: : a Et R xec= uta er tg hem ra ox ba oR ta∈ cA tiR on(cid:80) atL l= a1 nπ dR s( ea nR d| tb hR e, r0 o, bg oR( tl) u) t/ teL ranceut where b+ R,s 0n =Merge(b R,0,b H,0(s n)).
34: t←t+1 R R The plans used to compute the KL-divergence for the
35: untilt=Tmax orthetruegoalhasnotbeenreached proxy rewards can be generated by running an off-the-shelf
planner given the corresponding beliefs and goals (Line 16-
23 in Algorithm 1).
B. Agent Knowledge From Level-0 Belief Wealsodefinetherewardfornotcommunicatingatastep
as follows:
Recall that the level-0 belief of an agent b represents
i,0
the agent’s belief of the physical state s. If we partition the R(None,M R)=0. (4)
state s into multiple sub-states such as states of all objects
E. Communication Optimization
in the environment, then we can evaluate the uncertainty in
Given the proxy rewards defined above, we can then
the belief of each sub-states. We define the sub-states that
choose whether and what to communicate based on the
have certain belief distributions as knowledge of an agent.
Formally,letusdenoteastatepartitionass={s }N with robot’smindateachstep(Line27-30inAlgorithm1).Inpar-
n n=1
ticular,theutterancespaceisU ={None}∪{share s ;s ∈
Nsub-statesandb (s )asthelevel-0beliefofthesub-state n n
i,0 n K } ∪ {request s ;s ∈ Kˆ }, where Kˆ is the inferred
s n. For instance, if s is object n’s state, then b (s ) is R n H H
i n i,0 n
humanknowledgeestimatedfromthehumanbeliefparticles:
the belief of the object i’ state. Consequently, we define the
knowledge of agent i as Kˆ H = ∪L l=1K(b( Hl) ,0). We select the best robot utterance at
step t as follows:
K =K(b )
i i,0
ut =argmaxR(u,M ). (5)
={b (s ):H(b (s ))<H ,n=1,··· ,N}, (1) R R
i,0 n i,0 n max u∈U
where H is the entropy of a belief distribution and H Wecanfurthergenerateanaturallanguagemessagebased
max
is maximum entropy that is considered to be certain. In the on the utterance ut to enable communication with real hu-
R
exampleofobjectstatesassub-states,knowledgeconsistsof mans.ThiscanbeachievedbyusingGPT-4[37]totranslate
objects over which the agent has beliefs with high certainty. ut to natural language through few-show prompting.
RRecipeName IngredientList Goals GoalSpecification
Burger Cooked(Patty),Cooked(Potato),Chopped(Lettuce), Setuptable Put[Nforks,Nplates,Nwaterglassesor
Chopped(Tomato) wineglasses]on[kitchentable,coffeetable]
Pasta Cooked(Spaghetti),Cooked(Mushroom), Putgroceries Put[Napple,Nsalmon,Npudding,Ncupcakes]
Cooked(Cream),Chopped(Basil) inside[cabinet,fridge]
Ramen Cooked(Noodle),Cooked(Mushroom), Preparefood Put[Napple,Nsalmon,Npudding,Ncupcakes]
Cooked(Egg),Chopped(Scallion) on[kitchentable,coffeetable]
Steak&Fries Cooked(Beef),Cooked(Potato),Chopped(Parsley) Loaddishwasher Put[Nforks,Nplates,Nwaterglassesor
wineglasses]inside[dishwasher]
TABLE I: Overcooked recipe specifications.
TABLE II: VirtualHome goal specifications.
Inthisstudy,weextendedtheOvercookedsimulatorfrom[9]
by assuming partial observability where each agent cannot
observe the other room as shown in Fig. 2. At each step, the
AI assistant may share its progress on the task or ask about
the human’s progress.
Fig. 2: Example Overcooked environment. In each environ- The goal of the collaborating agents is to complete the
ment, there are two rooms. The two agents are always in dishes in the shortest amount of time. To simulate more re-
different rooms. An agent cannot observe the other room. alistic cooking scenarios, we augment the existing simulator
Thus it has to rely on verbal communication to infer the with dynamics that cooked ingredients will gradually cool
states of the objects in the other room. down. If cooked ingredients are not at the ideal temperature
when the dish is served, the team will receive a penalty.
This requires both agents to coordinate better to avoid
F. Multimodal Mental Update
misalignment in their plans for cooking the ingredients. For
Ateachstep,therobotwillupdateitsmindbasedonboth instance,oneagentcannotfinishmakingtheburgertooearly
itsobservationot andthemessagesitsendsandreceives.In if the other agent has not started cooking the French fries.
R
particular, we extract human knowledge b (s ) from the Therefore,theagentsneedtocoordinateandaligntheirplans
H,0 n
humanmessageut viaGPT-4anduseittoupdatetherobot’s suchthattheyfinishcookingatthesametime.Theagentscan
h
level-0 belief b jointly with ot (Line 6 in Algorithm 1) aligntheirplanbychoosingtowaitfortheotheragents(e.g.
R,0 R
operation.Forinstance,ifthehumaninformstherobotofthe I will start cooking A as soon as the other agent finishes B).
locationofanobject,wecanupdatetherobot’slevel-0belief There are four recipes in our experiment (Table I): Burger,
with the knowledge of the object’s location. Additionally, if Spaghetti, Ramen, and Steak, each in a unique room layout.
the robot shares knowledge b (s ) in its utterance, then We simulate a human agent using the planner in [9], which
R,0 n
the robot can assume that the human’s level-0 belief will does not proactively communicate with the AI Assistant.
also be updated accordingly. Thus, in robot mind M , we Each recipe is run 10 times with different seeds and we
R
can update b(b ) using both the shared robot knowledge report the aggregate results.
H,0
and the human observation (Line 15 in Algorithm 1). Note Baselines. We evaluate three baselines: Single-agent, No-
that we can sample possible human observations based on Communication (No-Comm), and Heuristic-based Commu-
the state inferred by the robot’s level-0 belief (Line 13-14 nication (Heur-Comm). In Single-Agent, the human com-
in Algorithm 1). All beliefs are initialized with a uniform pletes all the tasks alone. In the No-Comm baseline, no
distribution (Line 2 in Algorithm 1). messagesareexchanged.IntheHeur-Commbaseline,theAI
Assistantfollowsasimpleheuristicthatsharesupdatesevery
V. EXPERIMENTS
time a sub-goal has been completed and periodically asks
We evaluate our model in two human-AI domains Over-
for the human’s progress. The action planner in all methods
cooked and VirtualHome. These two domains cover two
including GOMA is the same as the planner in [9].
distinct alignment objectives. In both domains, there are
Metrics. We use two performance metrics: speedup and
two agents – a human user and an embodied AI assistant.
totalplancosts.Speedupiscalculatedbycomparingtheplan
In Overcooked, the agent’s goal is to align their plans
length in each team condition, where the human is working
temporally so that certain joint actions can be performed
with one of the four collaborative AI models, to the single
at similar time steps, whereas in VirtualHome, the agents
agent baseline, i.e. Speedup=L /L −1.
align their beliefs about the location of the objects they try single team
to collect. We describe each in detail below. Totalplancostisthesumofallactionandcommunication
costs with penalties applied for sub-optimal dish states due
A. Overcooked
to time lapse between the completion of a hot sub-task (e.g.
Overcooked is a popular multiagent game where agents cooked noodle) and the end of the trial, i.e. TotalCost =
(cid:80)
needtocollaboratetoprepareandcookingredients,whichis L+U+ ∆(L ,L)whereU isthetotalnumber
i∈hotitems i
also widely used for evaluating human-AI cooperation (e.g., of utterances in a trial, L is the plan length and L refers to
i
[38],[9]).Intheoriginalgame,agentshavefullobservability. the time step where item i is completed.(a) Overcooked Simulation (b) Virtual-Home Simulation
90 35
Plan Length 0.6 Plan Length
0.4 80 Utterances 30 Utterances
Penalty 0.3 70 0.4 25
60
0.2 20
50 0.2
0.1 40 15
0.0 30 0.0 10
No-Comm Heur-Comm GOMA Single No-CommHeur-Comm GOMA No-CommGoal-Ag CoELA GOMA Single No-CommGoal-Ag CoELA GOMA
(c) Virtual-Home Human Experiment (d) Virtual-Home Human Rating
30 No-Comm Goal-Agnostic CoELA GOMA (ours)
Plan Length 7
0.8 25 Utterances 6
0.6 5
20 4
0.4 3
0.2 15 2
1
0.0 10 Assistance Goal Communication Communication
No-CommGoal-Ag CoELA GOMA Single No-CommGoal-Ag CoELA GOMA Helpfulness Understanding Usefulness Frequency
Fig.3:ExperimentalresultsinOvercookedandVirtualHome.Thequantitativeresultsfromexperiments(a,b,c)demonstrate
that GOMA led to the greatest speedup (left) and least plan cost (right) compared to other baselines. In human subjective
ratings (d), participants find GOMA to be more helpful and communicate more useful information than other models.
(a) Goal and plan inference based on action and command scenariosinVirtualHomeacross4goaltypesand5simulated
Can you find two apartments. Each episode is run 3 times and we report
water-glasses? AI’s Mind:
the averaged results. We simulate the human agent using
The goal is to
set-up table for 2. the MCTS planner from [39]. The simulated human agent
Human is looking for requests help by sampling a subset of the goal predicates
2 plates, 2 forks
and replies to the AI assistant’s questions. We compare
Human agent gives a AI Assistant infers the joint
command to the AI Assistant goal and plan our proposed method against four baselines: Single-agent,
(b) Request or share information about goal-relevant observations No-Communication (No-Comm), Goal-Agnostic (Goal-Ag),
The plates are on Found it, thanks! and LLM agent. The first two are identical to the ones
coffeetable.231
in Overcooked. The Goal-Ag baseline does not infer the
jointgoalsandplanandinsteadrandomlysharesinformation
aboutanyobjectsthatthehumandoesn’tknow.FortheLLM
agent, we use COELA [5], which achieved state-of-the-art
Human agent can’t find AI Assistant shares Human agent finds and
performance on human-AI cooperation in VirtualHome.
plates in the fridge information grabs plates
Fig. 4: Example of typical communication enabled by Human Experiment. We developed an online human
GOMA in VirtualHome. (a) Once the human (in the blue interface to conduct a human experiment. The interface
shirt) gives a command to the AI Assistant (in the orange follows the same task setup as the simulation study with 5
shirt), it infers the human goal and reasons that the human conditions:Single-Agent,No-Comm,Goal-Ag,CoELA,and
needs2platesand2forks.(b)AstheAIwatchesthehuman GOMA (Ours). The participants controlled the human user
agentopeningthefridge,GOMAinformsthehumanthatthe agent to either perform the task alone (Single-Agent) or to
platesareonthecoffeetable.Consequently,thehumangoes work with an AI assistant driven by one of the methods. In
to the coffee table to pick up a plate. all collaborative conditions, the interface includes a chatbox
thatallowstheparticipantandtheAIagenttosendmessages
to each other. We recruited 10 participants who had no
B. VirtualHome priorexperiencewiththesimulator.Theycompleted60trials
over 20 tasks. After completing a trial with an AI assistant,
VirtualHome [39] is a multiagent household simulator. In
the participants were asked to rate the AI assistant based
VirtualHome,agentscollaboratetocompletedailyhousehold
on four criteria: 1) the assistant is helpful; 2) the assistant
tasks. In our experiments, we include four common types of
understands your goal; 3) the assistant’s communication
household tasks: Set Table, Load Dishwasher, Get Snacks,
is useful; and 4) the assistant communicates more than
Stock Fridge. The goal for each task is defined as a set of
necessary. Each criterion is rated on a 7-point Likert scale
goal predicates and their counts as defined in Table II. In
(1 = Strongly Disagree, 7 = Strongly Agree).
VirtulHome, each object is associated with a unique object
ID, which we use in agents’ communication to distinguish Metrics. In line with previous studies on VirtualHome
the referent from others (e.g. cabinet.145). [39], we evaluate the models’ performance by computing 1)
Simulation Experiment. We simulate 25 collaborative speedups: counting the number of steps taken to complete
pudeepS
pudeepS
tsoC
latoT
tsoC
latoT
pudeepS
gnitaR
tsoC
latoTHuman Human Human agent goal:
ON(Waterglass, Coffee-
AI Assistant AI Assistant table.232)
Human agent knowledge:
INSIDE(Plate.357, Cabi-
2 net.128)
AI Assistant goal:
1
ON(Plate, Coffeetable.232)
ON(Fork, Coffeetable.232)
Communication AI Assistant knowledge:
No Communication 1 AI : Waterglass.454 is in the fridge INSIDE(Waterglass.454,
2 AI : Have you seen any plates? Fridge.171)
Human : Plate.357 is in cabinet.128.
(a) No Communication (b) GOMA
Fig. 5: Agents’ trajectories with No-Comm (left) and with GOMA (right) in a VirtualHome environment. In this example,
the AI Assistant needs to find a plate and a fork while the human is looking for a water glass. Both agents have knowledge
about the items that the other agent is looking for but not their own goal objects. In the No-Comm setting, the agents
cannot share knowledge and have to open many containers to search for goal items. By inferring the other agent’s goal and
communicating goal-relevant knowledge, GOMA drastically reduces the total number of steps taken to complete the task.
the task, and 2) total costs: an overall cost metric that sums qualitative examples of GOMA in VirtualHome simulations
up the action and communication cost over the episode. inFig.4and5.Intheseexamples,weshowthatduetopartial
observability,theAIAssistantandthehumanhaveexclusive
VI. RESULTS
knowledge about certain objects relevant to other agent’s
A. Simulation Experiment subgoals. GOMA allows the AI Assistant to inquire and
The simulation results are shown in Fig. 3ab. The advan- informanotheragentaboutthisgoal-relevantinformation.As
tage of collaboration is evident as the Single-agent baseline a result, the agents can find the goal objects quickly without
performed significantly worse than all other collaborative exhaustively opening and checking all containers.
models. Overall, we find that across both Overcooked and
B. Human Experiment
Virtual-Home experiments, our model outperformed other
baselinesinallmetrics.ThedifferencesbetweenGOMAand The human experiment results are shown in Fig. 3cd.
other baselines are all statistically significant with p<0.01 Similar to the simulation results, our proposed method had
across two performance metrics. thegreatestspeedupoverasingleagentandoutperformedall
In Overcooked, GOMA took on average 46.76 steps to baselinesintermsofplancosts.Incontrasttothesimulation
complete the task, achieving a 44.61% speedup. Our model study, the Goal-agnostic model here performed no better
completed the tasks with the lowest costs (M = 58.06) than No-Comm and CoELA as participants stopped paying
compared to the Heuristic-based model (M = 72.0) and No- attention to the assistant after it made too many statements
Commbaseline(M=65.05).Additionally,GOMAdelivered irrelevant to the goal. This is shown in the participants’
the dishes in the best condition among all tested models, as subjective ratings where participants reported that the Goal-
signaled by the lowest coldness penalty (7.85). Agnostic baseline communicated more than necessary.
In VirtualHome, GOMA took on average 20.08 steps to The participants gave a higher subjective rating to our
complete the task with a 55.8% speedup. Despite having model than other baselines on all 4 items. Interestingly,
observedobjectsrelevanttothehumanagent’sgoal,CoELA even though CoELA and GOMA performed goal inference
made few utterances (Mean = 3.03) and focused exclusively with the same method, the participants thought that only
on communicating observations of its own goal. For ex- GOMA understood the human’s goal. This is because by
ample, when given a command ”Please help me find a communicating goal-relevantinformation, GOMA implicitly
fork.”, CoELA would respond later ”I found fork 323 in expressed its understanding of the user’s goal, whereas
cabinet 132.” and did not share any knowledge that may CoELAonlycommunicatedtheprogressofitsownsubgoal.
be useful for the human’s subgoal and plan. The Goal-
VII. CONCLUSION
Agnostic model makes frequent (Mean = 5.41) but mostly
irrelevant utterances about possible goal objects. However, In this paper, we introduce GOMA, which enables an
it did perform slightly better than the No-Comm baseline embodied AI assistant to efficiently and effectively commu-
because, with enough utterances, it occasionally mentions nicate with a human user to achieve optimal cooperation.
useful information to the human agent. GOMA achieves this by reasoning about the other agent’s
Unlike baselines, GOMA can communicate and inquire mental state, assessing the misalignment between mental
about useful goal-relevant information with the human, states, and then proactively initiating necessary communica-
leading to improved team performance. We include two tion to exchange goal-relevant information. Our experimentsin Overcooked and VirtualHome demonstrate that embodied [18] Z.Mandi,S.Jain,andS.Song,“Roco:Dialecticmulti-robotcollabo-
AIassistantsbuiltwithGOMAcannotonlyhelpachievethe rationwithlargelanguagemodels,”arXivpreprintarXiv:2307.04738,
2023.
human goal faster with lower total plan cost but also receive
[19] S.DevinandR.Alami,“Animplementedtheoryofmindtoimprove
higher subjective ratings from human participants. human-robotsharedplansexecution,”in201611thACM/IEEEInter-
Ourstudyisnotwithoutlimitations.Wehavenotevaluated nationalConferenceonHuman-RobotInteraction(HRI). IEEE,2016,
pp.319–326.
GOMA on real-world robot assistants, which we intend to
[20] K.E.Schaefer,E.R.Straub,J.Y.Chen,J.Putney,andA.W.EvansIII,
studyinthefuture.Wealsoplantoenhancetheflexibilityof “Communicating intent to develop shared situation awareness and
the communication generation, so that it can communicate engender trust in human-agent teams,” Cognitive Systems Research,
vol.46,pp.26–39,2017.
about any information relevant to the task in an open-ended
[21] T.Ullman,“Largelanguagemodelsfailontrivialalterationstotheory-
manner. Finally, we also aim to investigate more general of-mindtasks,”arXivpreprintarXiv:2302.08399,2023.
belief representations that go beyond object states. [22] C.L.Baker,J.Jara-Ettinger,R.Saxe,andJ.B.Tenenbaum,“Rational
quantitative attribution of beliefs, desires and percepts in human
mentalizing,”NatureHumanBehaviour,vol.1,no.4,pp.1–10,2017.
REFERENCES
[23] T. Zhi-Xuan, J. Mann, T. Silver, J. Tenenbaum, and V. Mansinghka,
“Online bayesian goal inference for boundedly rational planning
[1] M.Tomasello,Originsofhumancommunication. MITpress,2010.
agents,”AdvancesinNeuralInformationProcessingSystems,vol.33,
[2] L.Ying,T.Zhi-Xuan,V.Mansinghka,andJ.B.Tenenbaum,“Inferring
2020.
thegoalsofcommunicatingagentsfromactionsandinstructions,”in
[24] T. Shu, A. Bhandwaldar, C. Gan, K. Smith, S. Liu, D. Gutfreund,
Proceedings of the AAAI Symposium Series, vol. 2, no. 1, 2023, pp.
E. Spelke, J. Tenenbaum, and T. Ullman, “Agent: A benchmark for
26–33.
corepsychologicalreasoning,”inInternationalconferenceonmachine
[3] D. Hadfield-Menell, S. J. Russell, P. Abbeel, and A. Dragan, “Co-
learning. PMLR,2021,pp.9614–9625.
operative inverse reinforcement learning,” in Advances in Neural
[25] C. Jin, Y. Wu, J. Cao, J. Xiang, Y.-L. Kuo, Z. Hu, T. Ullman,
InformationProcessingSystems,2016,pp.3909–3917.
A. Torralba, J. B. Tenenbaum, and T. Shu, “Mmtom-qa: Multimodal
[4] X. Gao, R. Gong, Y. Zhao, S. Wang, T. Shu, and S.-C. Zhu, “Joint
theoryofmindquestionanswering,”arXivpreprintarXiv:2401.08743,
mind modeling for explanation generation in complex human-robot
2024.
collaborative tasks,” in 2020 29th IEEE international conference on
[26] L.Ying,K.M.Collins,M.Wei,C.E.Zhang,T.Zhi-Xuan,A.Weller,
robotandhumaninteractivecommunication(RO-MAN). IEEE,2020,
J.B.Tenenbaum,andL.Wong,“Theneuro-symbolicinverseplanning
pp.1119–1126.
engine(nipe):Modelingprobabilisticsocialinferencesfromlinguistic
[5] H.Zhang,W.Du,J.Shan,Q.Zhou,Y.Du,J.B.Tenenbaum,T.Shu,
inputs,”arXivpreprintarXiv:2306.14325,2023.
and C. Gan, “Building cooperative embodied agents modularly with
[27] L. Ying, T. Zhi-Xuan, L. Wong, V. Mansinghka, and J. Tenenbaum,
largelanguagemodels,”arXivpreprintarXiv:2307.02485,2023.
“Groundinglanguageaboutbeliefinabayesiantheory-of-mind,”arXiv
[6] A. Hong, N. Lunscher, T. Hu, Y. Tsuboi, X. Zhang, S. F. dos
preprintarXiv:2402.10416,2024.
Reis Alves, G. Nejat, and B. Benhabib, “A multimodal emotional
[28] A.DraganandS.Srinivasa,“Generatinglegiblemotion,”2013.
human–robotinteractionarchitectureforsocialrobotsengagedinbidi-
[29] F.Stulp,J.Grizou,B.Busch,andM.Lopes,“Facilitatingintentionpre-
rectionalcommunication,”IEEEtransactionsoncybernetics,vol.51,
dictionforhumansbyoptimizingrobotmotions,”in2015IEEE/RSJ
no.12,pp.5954–5968,2020.
international conference on intelligent robots and systems (IROS).
[7] H.H.Clark,Usinglanguage. Cambridgeuniversitypress,1996.
IEEE,2015,pp.1249–1255.
[8] M.Kleiman-Weiner,M.K.Ho,J.L.Austerweil,M.L.Littman,and
[30] M. Kwon, S. H. Huang, and A. D. Dragan, “Expressing robot
J.B.Tenenbaum,“Coordinatetocooperateorcompete:abstractgoals
incapability,” in Proceedings of the 2018 ACM/IEEE International
andjointintentionsinsocialinteraction,”inCOGSCI,2016.
ConferenceonHuman-RobotInteraction,2018,pp.87–95.
[9] S.A.Wu,R.E.Wang,J.A.Evans,J.B.Tenenbaum,D.C.Parkes,
[31] Y. Zhang, S. Sreedharan, A. Kulkarni, T. Chakraborti, H. H. Zhuo,
and M. Kleiman-Weiner, “Too many cooks: Bayesian inference for
andS.Kambhampati,“Planexplicabilityandpredictabilityforrobot
coordinating multi-agent collaboration,” Topics in Cognitive Science,
taskplanning,”in2017IEEEinternationalconferenceonroboticsand
vol.13,no.2,pp.414–432,2021.
automation(ICRA). IEEE,2017,pp.1313–1320.
[10] J.MacMillan,E.E.Entin,andD.Serfaty,“Communicationoverhead:
[32] X.Gao,L.Yuan,T.Shu,H.Lu,andS.-C.Zhu,“Showmewhatyou
Thehiddencostofteamcognition.”2004.
cando:Capabilitycalibrationonreachableworkspaceforhuman-robot
[11] E. Horvitz and J. Apacible, “Learning and reasoning about interrup-
collaboration,” IEEE Robotics and Automation Letters, vol. 7, no. 2,
tion,” in Proceedings of the 5th international conference on Multi-
pp.2644–2651,2022.
modalinterfaces,2003,pp.20–27.
[33] P. J. Gmytrasiewicz and P. Doshi, “A framework for sequential
[12] V.V.Unhelkar,S.Li,andJ.A.Shah,“Decision-makingforbidirec-
planning in multi-agent settings,” Journal of Artificial Intelligence
tionalcommunicationinsequentialhuman-robotcollaborativetasks,”
Research,vol.24,pp.49–79,2005.
in Proceedings of the 2020 ACM/IEEE International Conference on
[34] P.DoshiandP.J.Gmytrasiewicz,“MonteCarlosamplingmethodsfor
Human-RobotInteraction,2020,pp.329–341.
approximatinginteractivePOMDPs,”JournalofArtificialIntelligence
[13] E.C.Williams,N.Gopalan,M.Rhee,andS.Tellex,“Learningtoparse
Research,vol.34,pp.297–337,2009.
naturallanguagetogroundedrewardfunctionswithweaksupervision,”
[35] L.P.Kaelbling,M.L.Littman,andA.R.Cassandra,“Planningand
in2018IEEEInternationalConferenceonRoboticsandAutomation
acting in partially observable stochastic domains,” Artificial intelli-
(ICRA). IEEE,2018,pp.4430–4436.
gence,vol.101,no.1-2,pp.99–134,1998.
[14] T. Zhi-Xuan, L. Ying, V. Mansinghka, and J. B. Tenenbaum,
[36] A.Bosch-Domenech,J.G.Montalvo,R.Nagel,andA.Satorra,“One,
“Pragmatic instruction following and goal assistance via cooperative
two,(three), infinity,...: Newspaper and lab beauty-contest experi-
language-guidedinverseplanning,”arXivpreprintarXiv:2402.17930,
ments,” American Economic Review, vol. 92, no. 5, pp. 1687–1701,
2024.
2002.
[15] L. Yuan, X. Gao, Z. Zheng, M. Edmonds, Y. N. Wu, F. Rossano,
[37] OpenAI,“Gpt-4technicalreport,”2023.
H.Lu,Y.Zhu,andS.-C.Zhu,“Insitubidirectionalhuman-robotvalue
[38] M. Carroll, R. Shah, M. K. Ho, T. Griffiths, S. Seshia, P. Abbeel,
alignment,”Sciencerobotics,vol.7,no.68,p.eabm4183,2022.
andA.Dragan,“Ontheutilityoflearningabouthumansforhuman-
[16] C.Zhang,J.Chen,J.Li,Y.Peng,andZ.Mao,“Largelanguagemodels aicoordination,”Advancesinneuralinformationprocessingsystems,
for human-robot interaction: A review,” Biomimetic Intelligence and
vol.32,2019.
Robotics,p.100131,2023.
[39] X.Puig,T.Shu,S.Li,Z.Wang,Y.-H.Liao,J.B.Tenenbaum,S.Fidler,
[17] B. Ichter, A. Brohan, Y. Chebotar, C. Finn, K. Hausman, . ..., and A. Torralba, “Watch-and-help: A challenge for social perception
and C. Kelly, “Do as i can, not as i say: Grounding language in andhuman-aicollaboration,”arXivpreprintarXiv:2010.09890,2020.
roboticaffordances,”inProceedingsofThe6thConferenceonRobot
Learning, ser. Proceedings of Machine Learning Research, K. Liu,
D.Kulic,andJ.Ichnowski,Eds.,vol.205. PMLR,14–18Dec2023,
pp.287–318.