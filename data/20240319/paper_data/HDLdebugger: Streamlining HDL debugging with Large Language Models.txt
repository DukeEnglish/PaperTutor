HDLdebugger: Streamlining HDL debugging with Large
Language Models
XufengYao∗ HaoyangLi∗ TszHoChan∗ WenyiXiao
CUHK&Huawei Huawei Huawei Huawei
HongKongSAR,China HongKongSAR,China HongKongSAR,China HongKongSAR,China
xfyao@cse.cuhk.edu.hk li.haoyang1@huawei.com chantszho1@huawei.com wxiaoae@cse.ust.hk
MingxuanYuan YuHuang LeiChen† BeiYu†
Huawei HiSilicon HKUST&HKSUT(GZ) CUHK
HongKongSAR,China ShenZhen,China HongKongSAR,China HongKongSAR,China
Yuan.Mingxuan@huawei.com huangyu61@hisilicon.com leichen@cse.ust.hk byu@cse.cuhk.edu.hk
ABSTRACT ACMReferenceFormat:
Inthedomainofchipdesign,HardwareDescriptionLanguages XufengYao∗,HaoyangLi∗,TszHoChan∗,WenyiXiao,MingxuanYuan,
YuHuang,LeiChen†,andBeiYu†.2024.HDLdebugger:StreamliningHDL
(HDLs)playapivotalrole.However,duetothecomplexsyntaxof
HDLsandthelimitedavailabilityofonlineresources,debugging
debuggingwithLargeLanguageModels.InProceedingsof (KDD’24).ACM,
NewYork,NY,USA,13pages.https://doi.org/XXXXXXX.XXXXXXX
HDLcodesremainsadifficultandtime-intensivetask,evenfor
seasonedengineers.Consequently,thereisapressingneedtode-
velopautomatedHDLcodedebuggingmodels,whichcanalleviate 1 INTRODUCTION
theburdenonhardwareengineers.Despitethestrongcapabilities HardwareDescriptionLanguages(HDLs)arecrucialintherealmof
ofLargeLanguageModels(LLMs)ingenerating,completing,and chipdesign,servingasthecornerstoneforcreating,testing,andim-
debuggingsoftwarecode,theirutilizationinthespecializedfield plementingdigitalsystems[9,13,48].Duetotheircriticalrole,the
ofHDLdebugginghasbeenlimitedand,todate,hasnotyielded domainofHDLdebugginghasreceivedcomparativelyscantatten-
satisfactoryresults.Inthispaper,weproposeanLLM-assistedHDL tion.Traditionaldebuggingapproachesprimarilyinvolvemanual
debuggingframework,namelyHDLdebugger,whichconsistsof codecorrectionbasedonsyntacticguidelines,followedbyiterative
HDLdebuggingdatagenerationviaareverseengineeringapproach, testingthroughcompilers.Thisprocess,whilestraightforwardfor
asearchengineforretrieval-augmentedgeneration,andaretrieval- languagessuchasPythonandJava,becomesmarkedlymorecom-
augmentedLLMfine-tuningapproach.Throughtheintegration plexforHDLsduetotheirsophisticatedsyntaxandthescarcity
ofthesecomponents,HDLdebuggercanautomateandstreamline ofaccessibleresourcesonline.Furthermore,compiler-basedtest-
HDLdebuggingforchipdesign.Ourcomprehensiveexperiments, ingofHDLcode,especiallyinthecontextofchipdevelopment,is
conductedonanHDLcodedatasetsourcedfromHuawei,reveal exceptionallytime-consumingandresource-intensive.
that HDLdebugger outperforms 13 cutting-edge LLM baselines, DespitethehighdemandintheindustryforeffectiveHDLdebug-
displayingexceptionaleffectivenessinHDLcodedebugging. gingtechniquesandthepromisingdirectionstheyoffer,existing
methodologiesoftenfallshortinaddressingthecomplexitiesofthe
CCSCONCEPTS problem.Forexample,thetemplate-basedmethod[15,17,20,25],a
traditionalstrategyincodedebugging,utilizesexpert-definedcode
•Theoryofcomputation→Programsemantics;•Computing
patternsorheuristicstoidentifyandcorrecterrors.However,this
methodologies→Naturallanguageprocessing;•Hardware
approachisinherentlylimited,capableofrectifyingonlythoseer-
→Hardwaredescriptionlanguagesandcompilation.
rorswithpredefinedpatterns.Consequently,itlackstheflexibility
andadaptabilitynecessarytotackleadiversearrayofbugs.
KEYWORDS
Recently,researchershavedelvedintothedirectapplicationof
Code Debugging, Large Language Model, Retrieval Augmented
largelanguagemodels(LLMs)torectifybuggycode.Theunderly-
Generation
inghypothesisisthatLLMs,pre-trainedonextensiverepositories
∗Equalcontribution,†Correspondingauthor, ofopen-sourcecodesnippetsandtext,suchasPython,caneffec-
ThisworkwascompletedduringXufengYao′sinternshipatHuawei. tivelydiscernbugpatternsandautomaticallyrepairbuggycode.
ToassesstheefficacyofcurrentLLM-basedapproachesinaddress-
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor ingindustry-levelHDLdebuggingchallenges,weconductapilot
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation experimentonthreetypicalmethodsasshowninTable1.Among
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe themethodsevaluated,GPT4[1]isthecurrentstate-of-artLLM,
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
RTLFixer[35]leveragesretrievalaugmentedgeneration(RAG)[12]
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. and advanced prompt engineering [46], specifically tailored for
KDD’24,August25–29,2024,Barcelona,Spain HDLdebuggingtasks.VeriGen[34]isahardwarelargelanguage
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. modeltrainedbyaself-containedhardwaredataset.Despitethese
ACMISBN978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX advancedapproaches,ourobservationsindicatethatnoneofthe
4202
raM
81
]RA.sc[
1v17611.3042:viXraKDD’24,August25–29,2024,Barcelona,Spain TrovatoandTobin,etal.
Table1:PilotDebuggingExperiments Table2:Importantnotations.
Method GPT4[1] RTLFixer[35] VeriGen[34] Notation Description
Pass-rate@1 6.35% 28.35% 1.34% 𝑏𝑖 Buggycode
𝑒𝑗 Errorid
𝑚𝑖 Errormessagesfor𝑏𝑖
𝑐𝑖 Correctcodefor𝑏𝑖
methodsdeliveredresultsthatmetourcriteriaforsatisfactionin
(𝑑𝑗,𝑟𝑗,𝑠𝑗) Descriptions𝑑𝑗,reasons𝑟𝑗,andpotentialsolutions𝑠𝑗 for𝑒𝑗
thecontextofindustry-levelHDLdebuggingscenarios.Aprimary 𝐷𝑒,𝐷𝑐 errordatabase,codedatabase
contributingfactortothisshortfallistheinsufficiencyofHDLcode z𝑖𝑤 Keywordvectorforbuggycode𝑏𝑖witherrormessage𝑚𝑖
resourcesfortraining.Consequently,thesepre-trainedLLMsstrug- 𝑠𝑖𝑚(𝐼𝑖,𝐼𝑗) Similaritybetweenbuggycodes𝐼𝑖and𝐼𝑗
gletoaccuratelycomprehendthesyntaxandfunctionalityinherent 𝑞=(𝑏,𝑒) Codequeryconsistingofbuggycode𝑏anderrormessage𝑚
toHDLcodes. 𝑟𝑎𝑔 𝑖𝑑 ThedocumentRAGbasedonerrormessage𝑚𝑖and𝑏𝑖
Totackletheproblem,weproposeanHDLdebuggingframe- 𝑟𝑎𝑔 𝑖𝑐 ThecodeRAGbasedonbuggycode𝑏𝑖
work,namelyHDLdebugger,whichconsistsofthreecomponents,
𝑝𝑡 Promptofthoughtgeneration
i.e.,datageneration,searchengine,andretrieval-augmentedLLM
𝑝𝑐 Promptofbuggycodecorrection
fine-tuning.Firstly,thedatagenerationproceduretargetsovercom-
𝑡𝑖 Thethoughtforsolvingbuggycode𝑏𝑖
ingtheobstacleofthelimitedavailabilityofHDLbugs.Specifically,
weemployreverseengineeringtoinsertspecificmodificationsinto
theoriginalerror-freecode.Therefore,wecanproducecorrespond-
ingbuggyversionsanderrormessagesviacompilers,whichare
usedtoconstructacodedatabaseandfurtherfine-tuneLLMs.Sec-
Buggy Code Error Message
ondly,weproposeaneffectiveandefficientsearchengine,which Data Generation Input
issupportedbythecodedatabaseconstructedbythedatagener- Database Search
Input
ationapproachandthedocumentdatabasewithvariousinternal
HDL documents which contain relevant information for buggy
Code RAG
codes.Givenabuggycodeanditserrormessage,thesearchen-
Search Engine
gineretrievesrelevantinformation(i.e.,documentRAG)andbuggy LLM Thoughts Correct Code
codes(i.e.,codeRAG)withsimilarpatternsfromthedocument Database Input
databaseandcodedatabase,respectively.ThedocumentRAGand
Doc RAG
codeRAGarecrucialforboththefine-tuningandinferencestages HDL doc
ofourretrieval-augmentedLLM,enhancingtheabilityofLLMs Figure1:FrameworkoverviewofHDLdebugger.
tocomprehensivelyunderstandtheHDLbuggycodeandrepair
iteffectively.Thirdly,toenhancetheabilityofLLMstogenerate
accuratecodesolutions,weproposeanovelfine-tuningapproach
2 METHODOLOGY
forLLMs.Thisapproachincorporatesaself-guidedthoughtgener-
ationmechanismandaretrieval-augmentedfine-tuningprocess, Thissectionprovidesacomprehensiveoverviewoftheproposed
significantlyimprovingtheLLM’sperformanceindebuggingHDL HDLdebugger.Initially,wedelveintothebuggycodegeneration
code. pipeline,asdetailedinSection2.1.Subsequently,thesearchengine
Ourcontributionsaresummarizedasfollows: mechanismtailoredforRetrieval-AugmentedGeneration(RAG)is
presentedinSection2.2.TheRetrieval-AugmentedLLMfine-tuning
• WeintroduceanadvancedLLM-basedHDLdebuggingframe- iselaborateduponinSection2.3.Theimportantnotationsinour
worksupportingchipdesignsintheindustry,namelyHDLde- paperareshowninTable2.
bugger,whichconsistsofbuggydatageneration,searchengine, Framework.AsshowninFig.1,givenabuggycodeanditsasso-
andretrieval-augmentedLLMfine-tuning. ciatederrormessage,ourproposedHDLdebuggertargetstorepair
• Toaddressthescarcityofhigh-qualityHDLdebuggingtraining thisbuggycodeintothecorrectone.Specifically,wefirstproposea
data,weproposeadatagenerationapproachbasedonreverse datagenerationapproachinSec.2.1togenerateasetofHDLcode
engineeeringtocomprehensivelygeneratediverseandrealistic instances,whereeachinstanceconsistsofbuggycode,errormes-
HDLbuggycodeswiththecorrectversion. sages,anditscorrectversion.ThesegeneratedHDLcodeinstances
• WeproposeasearchenginetocreatecodeRAG(resp.docRAG) willbeusedtoprovidecontextforbuggycodequeriesandfine-tune
forHDLbuggycode(resp.relevantinformation)effectivelyand theLLMs.Second,weproposeasearchengineinSec.2.2,which
efficiently,enhancingthefine-tuningandinferenceofLLMs. targetstoretrieverelevanttextinformation(i.e.,documentRAG)
• Wepresentanovelretrieval-augmentedfine-tuningapproach forerrormessagesandretrievebuggycodeswithsimilarbuggy
forHDLdebugging,whichintegratesself-guidedthoughtgen- patterns(i.e.,codeRAG).Then,HDLdebuggertakesthebuggycode,
erationwithRAG-basedfine-tuningstrategies. itserrormessage,taskprompt,documentRAG,andcodeRAGto
• ExtensiveexperimentsontheHDLcodedatasetfromHuawei theLLMs,andenablesLLMstopredictthecorrectcode.Specifically,
demonstratesuperiorperformanceagainst13state-of-the-art weintroducearetrieval-augmentedfine-tuneapproachtofine-tune
baselines,includingGPT4andvariousHDLdebuggingLLMs. theLLMsforHDLcodedebugginginSec.2.3.HDLdebugger:StreamliningHDLdebuggingwithLargeLanguageModels KDD’24,August25–29,2024,Barcelona,Spain
Step 1: Modification Function Generation
HDL Documents Error Reasons Modification Functions Q Eu rre or ry InR foe rle mv aa tn iot n L Sin ee lfa -r A+ tS teo nft tm ioa nx
Wrong values in script FuncValue_modify Message
HDLmanualExpertnotesUser logs ReC dy uc nle d alo ns tt i nin s ts rc ur ci tp iot n FF unu cnc InC sy trc ul ce t_ iod nel _e at de d Parser Search DaE tr ar bo ar se Bi-LSTM
S onu m thm e a Hri Dze L eP mrr r aoo nrm u r ap e lat ,s s eo xn ps e b rta s ne od te s, LLM Wr po lani cgn e a s isc nsr ii sgp cnt rm ipe tnt FuncAssig …nment_move Documents Store BERT BERT BERT BERT
and user logs. … 𝑠! 𝑠" … 𝑠#
(a) RAG for the error message (b) BERT-LSTM
Step 2: Sample Generation
BugQ gyue Cr oy de+ TF-IDF Search Keywords
, , Error Message Extractor Store Vector DB Top-𝑁 Top-𝐾
buggy codeerror messagecorrect code Code Database Search Candidates Reranker Candidates
Generated Samples Error Message Compiler Buggy Code Correct Code (BuggyCode+ BERT- Semantic
Error Message, LSTM Store Vector DB
Correct Code) (c) RAG for the buggy code
Figure2:ReverseEngineeringPipelineforinducingerrorto
correctcodebydiversemodificationfunctionssummarized Figure3:Thesearchenginetoretrieverelevantinformation
fromHDLdocuments. givenerrormessagesandretrievesimilarbuggycodesbased
onabuggycodequery.
2.1 HDLBuggyDataGeneration
UnlikesoftwarelanguageslikePythonorC++,wherecodecan codes,wesystematicallyintroduceerrors,therebyproducingaset
oftenbecrawledfromopenwebsitesandplatformslikeGitHub, ofbuggycodes.Particularly,wecanapplyvariousmodification
HDLcodes,particularlythoseusedforchiptesting,arerarelymade functionstooneHDLcode,whichallowsustogeneratemultiple
publicduetoprivacyandcommercialconcerns.Thislimitation instancesofbuggycode.Next,weemployanHDLcompilerto
presentsasignificantobstacleonfine-tuningLLMsinthedomain compiletheseintentionallybuggycodesontheirrespectivede-
ofHDL.Inthissection,wewillintroduceareverseengineering sign,whichinevitablyresultsincompilationerrors.Then,foreach
pipelineforgeneratinghigh-qualityHDLcodepairsthatconsistof correctHDLcode𝑐 𝑖 ∈𝐶,wecancollectoneofitsbuggycode𝑏 𝑖
bothbuggyandcorrectedversions.AsshowninFig.2,TheHDL withassociatederrormessage𝑚 𝑖 asaninstance𝐼 𝑖 = (𝑏 𝑖,𝑚 𝑖,𝑐 𝑖)
datagenerationconsistsoftwosteps,i.e.,modificationfunction oftrainingdata.Normally,diagnosingandrectifyingHDLerrors
generationandsamplegeneration. requiretheexpertiseofchipengineers.Giventhatourinstructions
systematically brought the errors, we can employ reverse engi-
2.1.1 ModificationFunctionGeneration. Thefirststeptargetsto neeringtoidentifythefaultsandproducesolutionsdirectly.This
generateasetofhigh-qualitymodificationfunctions.Thesefunc- bypassestheneedformanualerrordiagnosis,streamingtheprocess
tionsarethenemployedtomodifyHDLcodesprovidedbyindustry ofcreatingavastarrayofcomprehensivedatasetsoferrorscripts,
engineers,therebygeneratingadiversecollectionofbuggycodeex- errormessages,andcorrespondingsolutions.Thismethodensures
amples.Toensurethatthemodifiedcodesexhibitarangeofrealistic arichdiversityinthetypesoferrorsproduced,whichiscriticalfor
a bn yd led vi ev re ar gse ine grr to hr ep ca at pt ae brn ils it, iw ese oc fo Ln Lst Mru sc at nth de am coo mdi pfi rc ea hti eo nn sif vu en cc oti lo len cs
-
creatinganextensiveandeffectivetrainingdataset𝐷
𝑐
={𝐼 𝑗}| 𝑗𝐷 =1𝑐|.
tionofindustrialHDLdocuments.Specifically,asshowninFig.2,
2.2 SearchEngineforRAG
wefirstcollectasetofHDLdocuments,includingtheHDLmanual,
expertnotes,anduserlogs.Subsequently,wethencarefullydesign Inthissubsection,weproposeasearchenginetooptimizeretrieval-
apromptthatguidestheLLMstoextractandsummarizeprevalent augmentedgenerations(RAG)forretrievingrelevantinformation
andcriticalerrorpatternsinHDL,suchassyntaxmisuseorlogical intheHDLdocumentsandcodesinstances.TheretrievedRAG
errors.Withtheseinsights,weproceedtodevelopthemodification contentwillserveascontextualinformationforqueries,thereby
functions.Thedistilledfunctionsfocusonsimpleoperationssuch enhancingthecapabilityofLLMstounderstandbuggycontext
asadding,deleting,modifying,andadjustingsegmentswithHDL informationandidentifyissueswithinbuggycodes.Weillustrate
scripts,whiletheuniquesetofrulesgoverningHDLensuresthat theoverviewofsearchengineframeworkinFig.3.
similaroperationscanresultinvastlydifferenterrorsrecordedin
HDLdocuments.Thesefunctionsexplicitlyintroduceerrorsinto 2.2.1 DocumentRAG. AsshowninFig.3(a),wefirstcollecta
comprehensivecollectionofinstructionaldocumentsforthisHDL,
theoriginalcorrectHDLcodesthatmirrorthosecommonlyen-
encompassinglanguagespecifications,errordiagnostics,andtrou-
counterederrorsinindustry,therebycreatinganinvaluabledataset
bleshootingtechniques.Then,wemeticulouslycuratethecontent
tofine-tuneLLMs.
ofdocuments,distillingadedicatederrordatabase𝐷 tailoredto
𝑒
2.1.2 SampleGeneration. Inthisstep,wegatherabroadrangeof thisHDL.Thiserrordatabasecontainsdetailederrordescriptions
accurateandhigh-qualityHDLcodes𝐶 ={𝑐 𝑖} 𝑖|𝐶 =1| fromexperienced 𝑑 𝑗,underlyingreasons𝑟 𝑗,andsuggestedremedialstrategies𝑠 𝑗 for
chipengineers.Thesecodes,whichhavebeenutilizedacrossvari- eacherrorid𝑒 𝑗. Weillustrateexamplesoferrorinformationinerror
ouschipdesigns,arecomprehensivetoencompassawiderange databaseinTab.7intheAppendix.Givenanerrormessagequery
offunctionaltestingscenariosforchips.ThesevariousHDLcodes 𝑚 𝑖,wefirstparse𝑚 𝑖 toextract𝑛 𝑒,𝑖 constituenterrorcodes{𝑒 𝑗}𝑛 𝑗=𝑒, 1𝑖.
serveastheseedcodeforerrorcaseconstruction.Specifically,by Subsequently,weretrievethedescriptions,reasons,andpotential
applyingthepreviousmodificationfunctionstothesecorrectHDL solutionsforeachidentifiederrorcodefromourerrordatabase.KDD’24,August25–29,2024,Barcelona,Spain TrovatoandTobin,etal.
Then, the document RAG of the query𝑚 can be assembled as similarityscore.Inthesecond-rankingstage,ourgoalistopinpoint
𝑖
𝑟𝑎𝑔 𝑖𝑑 = {(𝑒 𝑗,𝑑 𝑗,𝑟 𝑗,𝑠 𝑗)}𝑛 𝑗=𝑒, 1𝑖,therebyhelpingLLMstounderstand thetop-𝑘 relevantyetdiversebuggycodes.Theseselectionsare
theerrormessage𝑚 . aimedatprovidingabroaderrangeofbuggypatterns,whichcan
𝑖
helpLLMrepairthebugsinthequery.Formally,given𝑁 buggy
2.2.2 BuggyCodeRAG. AsshowninFig.3(c),inthecoderetrieval instances𝐷ˆ 𝑐𝑞 andthequerybuggycode𝑞 = (𝑏,𝑒),weselectthe
component,wemaintainacodedatabase𝐷 𝑐 = {𝐼 𝑖} 𝑖|𝐷 =1𝑐|,where top-𝑘relevantanddiversecode𝐷 𝑐𝑞 bymaximizingthefollowing
each code instance𝐼 𝑖 = (𝑏 𝑖,𝑚 𝑖,𝑐 𝑖) consists of a buggy code𝑏 𝑖, objective:
itsassociatederrormessages𝑚 𝑖,andthecorrectcode𝑐 𝑖.Givena ∑︁ 1 ∑︁ 𝑞
q asu se or cy ia𝑞 te= d( e𝑏 r, r𝑚 or) mth ea st sai gn ecl 𝑒u ,d te hs ea ais mnip op fe tht eof cob du egg Ry Ac Go id se t𝑏 ow idi et nh tii ft ys 𝐷m 𝑐𝑞 ⊆a 𝐷x ˆ 𝑐𝑞
𝐼𝑖∈𝐷
𝑐𝑞𝑠𝑖𝑚(𝑞,𝐼 𝑖)+ 𝑘 ·
𝐼𝑖∈𝐷
𝑐𝑞𝑑𝑖𝑠(𝐼 𝑖,𝐷 𝑐), (2)
a simsu ib las ret to𝐷 t𝑐𝑞 he⊆ b𝐷 ug𝑐 go yf ct oh de et 𝑏op i- n𝑘 thco ed qe ui en rs yt .a In nc se us ct hha at wh aa yv ,e thth ee bm ugo gs yt wheredistance𝑑𝑖𝑠(𝐼 𝑖,𝐷 𝑐𝑞 ) =min 𝐼𝑗∈𝐷 𝑐𝑞 \𝐼𝑖 (2−𝑠𝑖𝑚(𝐼 𝑖,𝐼 𝑗))denotes
thediversityvaluebetweeneachinstance𝐼 andtheotherinstances
𝑖
code𝑏 hassimilarbuggypatternswiththebuggycodesineach 𝑞 𝑞
𝑞 in𝐷 𝑐 and𝑑𝑖𝑠(𝐼 𝑖,𝐷 𝑐) ∈ [0,2].Eq.(2)isanNP-hardproblem,which
instance𝐼 𝑖 = (𝑏 𝑖,𝑚 𝑖,𝑐 𝑖) ∈ 𝐷 𝑐.Thus,thecorrectcode𝑐 𝑖 foreach can be reduced from the well-known𝑘-clique problem [36] by
buggy code𝑏 can be provided to LLMs. As a result, LLMs can
𝑖 setting𝑠𝑖𝑚(𝑞,𝐼 𝑖) = 1 for all 𝐼 𝑖 ∈ 𝐷 𝑐. Therefore, we propose a
usethelearnedpatternsfromthetop-𝑘similarinstancestofixfor
greedyalgorithmwithanapproximationratio1−1/𝑒toidentify
thebuggycode𝑏inthequery.Specifically,wefirstintroducehow
thetop-𝑘relevantbuggycodes.Thedetailsofthegreedyalgorithm
tolearnlow-dimensionalvectorsforbuggycodesandthenthen
andapproximationratioareintroducedinAppx.B.2.Thus,given
introduceatwo-stagerankerthatretrievesthetop-𝑘buggycode
abuggycode𝑏 anderrormessage𝑚 ,wecangeneratethecode
𝑖 𝑖
instancesforcodequery𝑞. retrieval-augmentedgeneration𝑟𝑎𝑔 𝑖𝑐 ={(𝑏 𝑗,𝑚 𝑗,𝑐 𝑗)}𝑘 𝑗=1.
VectorDatabaseConstruction.Giventhecomplexityandlength
of HDL code and associated error messages, as well as similar- 2.3 Retrieval-augmentedLLMFine-tuning
lookingcodesnippetscontainingdistinctbuggypatterns,comput-
Inthissubsection,weintroducehowtofine-tunetheLLMsbased
ingthesimilaritybetweenbuggycodesischallenging.Toaddress
onthetrainingdatasetconstructedinSec.2.1andthesearchengine
thischallenge,weproposetomeasurethesimilaritybetweenbuggy
proposedinSec.2.2.Specifically,wefirstproposetogeneratea
fromtwoaspects,i.e.,keywordsimilarityandsemanticsimilarity.
thoughttohelpLLMrepaireachbuggycodeinthetrainingdataset.
First,weextractwordsfromallbuggycodesandtheirerrormes-
Second,basedonthegeneratedthoughtandtheretrievedbuggy
sages and use the TF-IDF [2] technique to compute the weight
codes,weproposearetrieval-augmentedsupervisedfine-tuning
ofeachword.Then,wecangeneratethekeywordvectorz 𝑖𝑤 for
techniqueforLLMs.
eachbuggycode𝑏 withitserrormessage𝑚 .Second,wedesigna
𝑖 𝑖
BERT-LSTMmodelthatcombinesBERT,foritspowerfullanguage 2.3.1 Self-guidedThoughtGeneration. Onestraightforwardway
understandingcapabilities,withanLSTM,foritssequentialdata istofeedbuggycodeanderrormessagestoLLMsandletLLMs
processingstrengths.TheBERT-LSTMmodelencodesabuggycode directlypredictthecorrectcodes.However,thisapproachisinsuf-
𝑏
𝑖
withitserrormessage𝑚
𝑖
intoalow-dimensionalembeddingz𝑠 𝑖. ficientforLLMstodeeplycomprehendtheproblemandprovide
Specifically,asintroducedinSec.2.1,thebuggycodesaregener- accuratesolutions.Recentresearch[40,45]suggeststhatwhen
atedbydifferentmodificationfunctions,andherewetakethese LLMsarepromptedtoproduceaseriesofintermediateandex-
modificationfunctionsasthelabelsforeachbuggyandoptimize planatory thought before finally outputting the solution to the
theBERT-LSTMmodel.Then,weusethefinalrepresentationof giventask,theperformanceofLLMscanbesignificantlyimproved.
BERT-LSTMasthesemanticembeddingforeachbuggycodeand Itisbecausethesereasoningthoughtsimprovetheunderstanding
itserrormessages.ThearchitecturedetailsofBERT-LSTMmodels ofLLMsontheinputtasksandthusgeneratemorerelevantandac-
areintroducedinAppx.B.1.BasedontheTF-IDFandBERT-LSTM curateoutputs.Therefore,beforefine-tuningtheLLMs,wepropose
models,webuildakeywordvectordatabaseandasemanticvector togeneratehigh-qualitythoughtforeachtrainingcodeinstance.
database,whichtogetherfacilitatearobustframeworkforanalyz- Specifically,asillustratedinFig.4,wefirstdesignapreciseand
ingthesimilaritybetweeninstancesofbuggyHDLcode. explicitprompt𝑝 𝑡 toclarifythethoughtgenerationtaskforLLMs.
Followingthis,weinputthethoughtgenerationprompt𝑝 ,the
Two-stageRanker.Ingeneral,wedesignatwo-stageranking 𝑡
buggycode𝑏 ,itsassociatederrormessage𝑚 ,andthedocument
approachtoidentifythetop-𝑘mostrelevantbuggycodeinstances 𝑖 𝑖
RAG𝑟𝑎𝑔𝑑,anditscorrectversion𝑐 intotheLLMwithitsinference
toagivenqueryofbuggycode.Inthefirst-rankingstage,forany 𝑖 𝑖
givenbuggycodequery𝑞=(𝑏,𝑒),wefirstusetheTF-IDFencoder
mode𝐿𝐿𝑀1.ThisenablestheLLMtogenerateathought𝑡
𝑖
onhow
andBERT-LSTMencodertogeneratethekeywordvectorz𝑤 and torepairthebuggycode𝑏 𝑖 intothecorrectcode𝑐 𝑖 asfollows:
𝑖
semantic vector z𝑠 𝑖. Then, we define the similarity between the 𝑡
𝑖
=𝐿𝐿𝑀1(𝑝
𝑡
◦𝑏
𝑖
◦𝑚
𝑖
◦𝑐
𝑖
◦𝑟𝑎𝑔 𝑖𝑑 ), (3)
querycode𝑞andeachinstance𝐼
𝑖
∈𝐷
𝑐
inthecodedatabase𝐷
𝑐
as:
where◦meansconcatenateoperator.Weomitthegeneraltask
𝑠𝑖𝑚(𝑞,𝐼 𝑖)=𝜆·𝑐𝑜𝑠𝑖𝑛𝑒(z𝑤,z𝑖𝑤 )+(1−𝜆)·𝑐𝑜𝑠𝑖𝑛𝑒(z𝑠,z𝑠 𝑖)+1, (1) requirementspromptforsimplification.Empirically,wefindthat
whenLLMsgeneratethethought𝑡 onlyonce,theoutputmightbe
𝑖
where𝜆∈ [0,1]isahyper-parameterbetweensemanticsimilarity irrelevanttothebuggycode𝑏 𝑖orincorrectduetothehallucination
andkeywordsimilarityand𝑠𝑖𝑚(𝑞,𝐼 𝑖) ∈ [0,2].Weselectthetop- phenomenonandrandomnessofLLMs.Therefore,toguaranteethe
𝑁
similarinstances𝐷ˆ𝑞
fromthecodedatabase𝐷 basedonthe qualityofthegeneratedthought,weiterativelygenerate𝐿different
𝑐 𝑐HDLdebugger:StreamliningHDLdebuggingwithLargeLanguageModels KDD’24,August25–29,2024,Barcelona,Spain
thoughts𝑇 𝑖 ={𝑡 𝑖,𝑗}𝐿 𝑗=1foreachbuggycode𝑏 𝑖 byrunning𝐿𝐿𝑀1𝐿 #1: Task Requirement Prompt
timesfollowingEq.(3).Thisisachievedbymodifyingtemperature Now you are expert in HDL(Hardware Description Language).
parametersandthedetailsaredescribedinappendix. Your task involves debugging in HDL. You are given snippets of HDL
script that contain errors.
Toassessthequalityofeachthought𝑡
𝑖,𝑗
∈𝑇 𝑖,weadoptaself-
Your objective is to identify these errors and provide helpful
guidancestrategytoselectthehighestqualitythoughtfromthe instructions to fix the bug.
thoughtset𝑇 forthebuggycode𝑏 .Specifically,wefeedthebuggy
𝑖 𝑖
#2: Buggy Code #3: Correct Code
code𝑏 ,itsassociatederrormessage𝑚 ,andthedocumentRAG
𝑖 𝑖
𝑟𝑎𝑔𝑑 totheLLM.Aprompt𝑝 ,"Basedontheanalysis,thecorrect cycle { cycle {
𝑖 𝑐 assign clk_s_top 0 assign clk_s_top 0
scriptis,"isappendedtoguidetheLLMtowardsgeneratingthe
assign clk_top_core_3 0 }
predictedcorrectscript,representedas:
} cycle {
𝑐ˆ
𝑖,𝑗
=𝐿𝐿𝑀1(𝑝
𝑐
◦𝑏
𝑖
◦𝑚
𝑖
◦𝑟𝑎𝑔 𝑖𝑑 ◦𝑡 𝑖,𝑗). (4) c y c al se
s
i{
gn clk_s_top 1 }
assign clk_s_top 1
Afterweobtaintheoutput𝑐ˆ 𝑖,𝑗 foreachthought𝑡 𝑖,𝑗,weemploy } (cid:1)
theeditdistancemetric[26]toevaluatethesimilaritybetweenthe (cid:1)
predictedcorrectcode𝑐ˆ andtheground-truthofcorrectedcode
𝑖,𝑗
#4: Error Messages #5: Retrieved Information
𝑐 as
𝑖 … …
𝑑 𝑖,𝑗 =𝐸𝑑𝑖𝑡𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑐 𝑖,𝑐ˆ 𝑖,𝑗). (5) Error: Can't find core when The port defined at the core level is
set prt cat top hierarchy. not consistent with the actual port.
Intuitively,ifthedistance𝑑 betweenthepredictedcorrectcode
𝑖,𝑗 Error: RULE(PRT-1)() test Reconnect the port that is not
𝑐ˆ 𝑖,𝑗 andthegroundtruthofcorrectedcode𝑐 𝑖issmaller,thethought failed" connected to the top-level.
𝑡 ismorehelpfultoLLMstorepairthebuggycode𝑏 .Therefore, … …
𝑖,𝑗 𝑖
weselectthethoughtwiththesmallesteditdistancesforeach𝑏 ,
𝑖
i D.e., =𝑡 𝑖 {= (𝑏 𝑖m ,𝑚in 𝑖𝑡 ,𝑖 𝑟,𝑗 𝑎∈ 𝑔𝑇 𝑖𝑑𝑖𝑑 ,𝑟𝑖, 𝑎𝑗 𝑔. 𝑖𝑐F ,i 𝑡n 𝑖,a 𝑐l 𝑖ly ), } 𝑖w |𝐷 =1e 𝑐|c .a Tn heob dt ea ti an ilsth oe ft thra ein thin og ugd ha tta gs ee nt - # P bul6 e g: a g sP ye r cpo oro dm v ei p bd at e s I e efn df is oct nir e u n thtc eat n eio rd r n oc rl e mar e sin sast gr eu sc t ai no dn s c f oo rr r em cto cd oif dy ei n pg ro t vh ie d g ediv . en
erationareillustratedinAlg.2intheAppendix. Your instructions should help improve the buggy code without
directly including any information from the correct code. The helpful
2.3.2 Retrieval-augmentedFine-tuning. Afterobtainingthefinal instruction is: Based on the error message
trainingdatasetD ={(𝑏 𝑖,𝑚 𝑖,𝑟𝑎𝑔 𝑖𝑑,𝑟𝑎𝑔 𝑖𝑐,𝑡 𝑖,𝑐 𝑖)} 𝑖|𝐷 =1𝑐|,wesupervise
#7: Thought Generation
fine-tunetheLLMsbasedonbuggycodesandretrieval-augmented
Based on the error message, it appears that there is an inconsistency
generationinSec.2.2.Specifically,giveneachtraininginstance between the ports defined at the core level and the actual ports. To fix
𝐷 𝑖 = (𝑏 𝑖,𝑚 𝑖,𝑟𝑎𝑔 𝑖𝑑,𝑟𝑎𝑔 𝑖𝑐,𝑡 𝑖,𝑐 𝑖) ∈ D,wefirstfeedthoughtgener- t th oi ps
-
lb eu vg e,
l
.y Io nu
t
hsh iso cu ald
se
r ,e tc ho en pn oe rc tt "th cle
k
p _o tor pt _th ca ot
r
eis
_
n 3o
"
t
i
sc o nn on
t
e cc ot ne nd
e
t co
t
eth de
t o
ationprompt𝑝 ,buggycode𝑏 ,itserrormessage𝑚 ,document
𝑡 𝑖 𝑖 the top-level, so you should reconnect it
RAG𝑟𝑎𝑔 𝑖𝑑,codeRAG𝑟𝑎𝑔 𝑖𝑐,toatargetLLM,𝐿𝐿𝑀2togeneratethe
Figure4:Thoughtgenerationexample.
predictedthought𝑡ˆ asfollows:
𝑖
𝑡ˆ
𝑖
=𝐿𝐿𝑀2(𝑝
𝑡
◦𝑝
𝑐
◦𝑏
𝑖
◦𝑚
𝑖
◦𝑟𝑎𝑔 𝑖𝑑 ◦𝑟𝑎𝑔 𝑖𝑐 ). (6)
3 EXPERIMENTS
Then,basedonthepredictedthought𝑝ˆ andcodecorrectionprompt
𝑡
𝑝 𝑐,LLM𝐿𝐿𝑀2predictsthecorrectcode𝑐ˆ
𝑖
asfollows: 3.1 ExperimentSetting
𝑐ˆ
𝑖
=𝐿𝐿𝑀2(𝑝
𝑡
◦𝑝
𝑐
◦𝑏
𝑖
◦𝑚
𝑖
◦𝑟𝑎𝑔 𝑖𝑑 ◦𝑟𝑎𝑔 𝑖𝑐 ◦𝑡ˆ 𝑖). (7) 3 H.1 D.1 LcoH dD eL filD esat fa rs oe mts Hin uH awua ew i.e Ti h. eW see fig la et sh ae rr ea md eiv tie cr us le ouco sll yle cc uti ro an teo df
,
Following[33,39],wefine-tunethetargetLLMwithitstraining witheachfilebeingspecificallyutilizedfordistinctchipdesign
mode𝐿𝐿𝑀2byusingtheconventionalnext-tokenpredictionobjec- scenarios,reflectingthevariedandspecializeddemandsofcircuit
tiveandminimizethecross-entropylossL D: design.EachHDLcodefilecontainsanextensivearrayofdata,
∑︁ includingvariableassignments,detailedcircuitdesigns,clocking
L𝑡𝑖 = log𝑝 𝐿𝐿𝑀2(𝑤 𝑖,𝑗|𝐶◦𝑤 𝑖,<𝑗),
information,functiontestingprotocols,andvarioustestitemsthat
𝑤𝑖,𝑗∈𝑡𝑖
arecriticalforthechipdesignprocess.BasedontheseHDLcode
L𝑐𝑖 = ∑︁ log𝑝 𝐿𝐿𝑀2(𝑤 𝑖,𝑘|𝐶◦𝑡ˆ 𝑖 ◦𝑤 𝑖,<𝑘), files,weusethedatagenerationinSec.2.1togenerate92,143dis-
𝑤𝑖,𝑘∈𝑐𝑖 tinctHDLtrainingcodeinstances.EachHDLtrainingcodeinstance
1 ∑︁ consistsofthebuggyHDLcode,errormessages,andthecorrect
L D = 2|D| (L𝑡𝑖 +L𝑐𝑖), (8) HDLcode.Specifically,intheexperiments,wesplitthedatainto
𝐷𝑖∈D
trainingandtestingsetsataratioof8:2,respectively.
wherethecontext𝐶 =𝑝
𝑡
◦𝑝
𝑐
◦𝑏
𝑖
◦𝑚
𝑖
◦𝑟𝑎𝑔 𝑖𝑑 ◦𝑟𝑎𝑔 𝑖𝑐 denotesthe
concatenateoftheinputsforclarification𝑤 𝑖,𝑗 ∈ 𝑡 𝑖 denotesthe 3.1.2 Baselines. WecompareourproposedHDLdebuggerwith13
𝑗-thwordin𝑡 and𝑤 denotesasetofwordsin𝑡 before𝑤 ,
𝑖 𝑖,<𝑗 𝑖 𝑖,𝑗 baselinesinthreetypesofcodedebuggingapproachesasfollows:
and𝑝 𝐿𝐿𝑀2(𝑤 𝑖,𝑗|𝑤 𝑖,<𝑗)denotetheprobabilityof𝑤 𝑖,𝑗.L𝑡𝑖 andL𝑐𝑖
denotethepredictioncross-entropylossonthoughtgroundtruth • Fivelargelanguagemodels:WecompareHDLdebuggeragainst
𝑡 andcorrectcodegroundtruth𝑐 regardingthebuggycode𝑏 , twoprofoundandstate-of-the-artLLMsavailablethroughAPI
𝑖 𝑖 𝑖
respectively. services,includingChatGPT[4]andGPT-4[1].Additionally,KDD’24,August25–29,2024,Barcelona,Spain TrovatoandTobin,etal.
wecompareHDLdebuggerwiththreeopen-sourceLLMs:Open- Table3:MainResults.Pass-Ratedescribestheabsolutevalue,
Chat[37],Orca2[24],andMistral[16].Theseopen-sourcemod- whichisthehigherthebetter.<1%describesthePass-Rate
elshaveshownperformanceonparwithChatGPTacrossvari- lessthan1%.Run-TimeandEdit-Distancearerelativevalues
ousopenLLMbenchmarks. comparedwithours,whichareboththelowerthebetter.
• FourCodeDebuggingandHDLModels:WecompareHDLde- Method Pass-Rate Run-Time Edit-Distance
buggerwithtwocodedebuggingandhardwarecodegeneration
ChatGPT∗ 3.01% 2.25 31.28
models,i.e.,Self-debug[7]andRTLfixer[35].Self-debug[7]
GPT4∗ 6.35% 1.94 18.17
isoneofthemostclassicalmethodsofcodedebugging.RTL-
OpenChat <1% 2.51 34.47
fixer[35]isproposedtosolveHDLdebuggingproblems.Veri-
OpenChatw/RAG 3.01% 2.21 10.37
Gen[34]andRTLCoder[22]aretwoLLMstargetinghardware Orca2 2.68% 2.08 2.94
language. Orca2w/RAG 9.03% 2.10 4.74
• FourCodeLanguageModels:WecomparefourSOTApre- Mistral 7.36% 2.39 6.68
trainedcodeLLMs,i.e.,Deepseek[3],Starcoder[18],Stable- Mistralw/RAG 24.75% 2.01 9.88
code[31],andWizardCoder[23].
Self-debug 5.02% 2.49 10.24
ForbaselinesexceptforChatGPT[4]andGPT-4[1].,weadopt RTLfixer 28.35% 2.11 11.05
threestrategies,i.e.,therawmodel,therawmodelwithRAG,and VeriGen <1% 2.46 49.16
therawmodelwithsupervisedfine-tuning(SFT). VeriGenw/RAG 1.34% 2.56 37.12
VeriGenw/SFT 67.55% 1.35 1.38
• Rawmodel:Weonlyfeedbuggycodeanderrormessagestothe RTLCoder <1% 2.49 43.08
modelandenablerawmodelstoinferthecorrectcodedirectly. RTLCoderw/RAG <1% 2.65 34.85
• RawModelwithRAG:Forbuggycode,wefeedthebuggy RTLCoderw/SFT 64.21% 1.53 3.83
code,itserrormessage,andthedocumentRAGandcodeRAG
Deepseek 2.34% 2.58 32.17
obtainedinSec.2.2totherawmodelandenabletherawmodels
Deepseekw/RAG 3.34% 2.51 34.38
toinferthecorrectcodedirectly. Deepseekw/SFT 51.63% 1.64 3.35
• RawModelwithSFT:Wetakethebuggycodeanderrormes- Starcoder <1% 2.58 28.85
sageasinputsofLLMsandusethecorrectcodeasground-truth Starcoderw/RAG <1% 2.56 34.85
tofine-tunetherawmodels. Starcoderw/SFT 68.27% 1.21 1.57
Stablecode 6.69% 2.46 4.56
Specifically,weselectedCodeLlama-13basourbasemodelfromthe
Stablecodew/RAG 8.02% 2.50 6.25
availablecodeLLMs.Thechoiceof13bwasdrivenbyitsoptimal
Stablecodew/SFT 41.47% 2.38 6.10
modelsize,whichstrikesabalancebetweentraininganddeploy-
WizardCoder 3.01% 2.41 7.19
ment,takingintoaccountbothperformanceandcostfactors. WizardCoderw/RAG 4.68% 2.58 8.67
WizardCoderw/SFT 71.57% 1.04 1.06
3.1.3 EvaluationMetrics. Fortheoveralldebugsystem,wemainly
evaluateitspassrateforcorrectingcodes,relativecoderuntimes, HDLdebugger(ours) 81.93% 1.00 1.00
andeditdistancebetweencorrectcodeandbuggycode.Thecalcu-
lationofthesemetricsislistedbelow.
• MRR@K:Meanreciprocalrank(MRR)fortop-𝐾 resultson𝑛 𝑡
• P ma es ts h- oR da it se: dP efias ns er dat ae sf 𝑃or =exe (cid:205)c 𝑛 𝑖u =𝑐 1t 𝑛i S 𝑐n (g 𝑦𝑖c )o ,d we hfi el re ec 𝑦o 𝑖rr se tc at ne dd sb fy orea thch e queriesisformulatedas𝑀𝑅𝑅@𝐾 = 𝑛1 𝑡 (cid:205)𝑛 𝑖=𝑡 1 𝐾1 (cid:205) 𝑘𝐾 =1 I(𝑦𝑖, 𝑘𝑘,𝑦𝑖).
correctedcode,Sdenotesforexecutingcodesuccessfully.
• Run-Time:TherelativelyaveragecompilationtimeforHuawei’s 3.2 MainResults
internalHDLcompilertoexecutealltestcodefiles.TheRun-Time Table3demonstratesthemainperformanceofourresultsandother
forresultsfromourHDLdebuggerissetasthebaseunit. methods.It’sclearthatourmethodoutperformsothermethodsby
• Edit-Distance:Edit-Distancecalculatestheminimumnumber allmeansbyalargemarginincludingdirectapproach,RAGand
ofoperations(insertion,deletion,substitution)requiredtotrans- SFT,whichdemonstratestheeffectivenessofourframework.For
formonecodesnippetintotheother. bothruntimeandeditdistancemetrics,wenormalizeallresults
Also,weevaluateourcodesearchengineinSec.2.2bythehit andpresentonlytherelativevaluesincomparisonwithoursto
ratio,meanaverageprecision,andmeanreciprocalrankmetrics, enhancetheclarityandeffectivenessofthecomparison.
whichareformulatedasfollows.
3.2.1 ComparisonwithdifferenttypesofLLMs. Inourstudy,we
• H@K:Hitratiofortop-𝐾 recommendationresultson𝑛 𝑡 code conductedacomparativeevaluationofbothgeneral-purposeLLMs
queriesisformulatedas𝐻@𝐾 = 𝑛1 𝑡 (cid:205)𝑛 𝑖=𝑡 1 𝐾1 (cid:205) 𝑘𝐾 =1I(𝑦 𝑖,𝑘,𝑦 𝑖),where suchasChatGPT,GPT-4,OpenChat,Orca,Mistral,Deepseek,Star-
𝑦 𝑖,𝑘 denotestheerrortypeoftheretrieved𝑘-thbuggycodefor coder,Stablecode,WizardCoder,andspecializedcodeLMsinclud-
querycode𝑏 𝑖,andtheindicatorfunctionI(𝑦 𝑖,𝑘,𝑦 𝑖)=1if𝑦 𝑖,𝑘 =𝑦 𝑖 ingSelf-debug,RTLfixer,VeriGen,RTLCoder,withintheHDLde-
• MAP@K:Meanaverageprecision(MAP)fortop-𝐾resultson𝑛 𝑡 buggingscenario.Duetoprivacyconcerns,certaincodespecifica-
queriesisdefinedas𝑀𝐴𝑃@𝐾 = 1 (cid:205)𝑛𝑡 1 (cid:205)𝐾 I(𝑦𝑖,𝑘,𝑦𝑖)·𝑛(𝑏𝑖,≤𝑘), tionshavebeenomittedfortestingpurposeswhenusingChatGPT
𝑛𝑡 𝑖=1 𝐾 𝑘=1 𝑘
where𝑛(𝑏 𝑖,≤𝑘)denotesthenumberofrecommendationsinthe orGPT4.Todistinguishtheseversions,wewillrefertothemas
firsttop-𝑘thathasthesameerrorlabelwithquery𝑏 . ChatGPT∗ and GPT4∗. It is evident that our approach exhibits
𝑖HDLdebugger:StreamliningHDLdebuggingwithLargeLanguageModels KDD’24,August25–29,2024,Barcelona,Spain
Table4:Ablationondifferentstrategies Table5:EvaluationonthesearchengineinSec.2.2onthe
top-𝑘recommendation.XGBandRFdenotetheXFBoostand
Method Pass-Rate Run-Time Edit-Distance
RandomForest,respectively.MAP@1andMRR@1arethe
Direct(CodeLlama) 4.01% 2.26 35.51 sameasH@1mathematically.ThefirstrowOptimalisthe
+RAG 15.05% 2.20 5.34
optimalperformanceundereachmetric.
+SFT 70.56% 1.19 1.19
H@1 H@3 H@10 MAP@3 MAP@10 MRR@3 MRR@10
+SFTw/th 74.91% 1.09 1.13
Optimal 1.00 1.00 1.00 1.00 1.00 0.61 0.29
+RAG&SFTw/th 81.93% 1.00 1.00 TF-IDF 0.97 0.87 0.75 0.86 0.71 0.55 0.24
BM25 0.95 0.80 0.59 0.81 0.54 0.53 0.22
XGB 1.00 0.35 0.18 0.34 0.14 0.34 0.12
superiorperformanceagainstall13state-of-the-artbenchmarks,in- RF 0.31 0.43 0.41 0.34 0.29 0.25 0.11
cludingGPT-4andotherdomain-specifichardware-basedlanguage Ours 1.00 0.98 0.94 0.98 0.93 0.60 0.28
models.
HDLdebugger HDLdebugger
3.2.2 AnalysisofDifferentStrategies. Inthisstudy,weimplement 84 2.0
threedistinctevaluationstrategiestoassesstheefficacyofvarious 82 1.8
methodologies:withretrieval-augmentedgeneration(RAG),with 80 1.6
78 1.4
supervisedfine-tuning(SFT),andviaadirectapproach.Within 76
thecontextofHDLdebugging,ouranalysisrevealsthatSFTholds 74 1.2
72 1.0
greatersignificanceandapplicabilityacrossallevaluatedbaselines, 0 1 3 5 7 9 0 1 3 5 7 9
includingVeriGen,RTLCoder,Deepseek,Starcoder,Stablecode,and NumberofRetrievedCodeInstances NumberofRetrievedCodeInstances
WizardCoder.Inthemajorityofscenarios,wenotethatmethods Figure5:Pass-rateandcode Figure6:Inferencetimeand
enhancedthroughSFTconsistentlyoutperformthoseaugmented retrievedcodeinstances retrievedcodeinstances
withRAGbyasubstantialmargin.Besides,ourproposedmethod
integratesbothRAGandSFTstrategies,achievingunparalleled 85.00
performance,indicatingthatitisbettertoincorporateSFTand 80.00 Pass-rate@5
RAGfortheHDLdebuggingtask.
75.00 Pass-rate@3
3.2.3 ImpactonDomain-SpecificSolutions. Inadditiontogeneral
Pass-rate@1
andcode-specificLanguageModels(LLMs),methodologiessuch 70.00
asSelf-debugandRTLfixerarespecificallydevisedtoaddresscode 65.00
0.10.2 0.4 0.6 0.8 1.0 1.2
debuggingscenarios.Whiletheseapproachesdemonstrateimprove-
Temperature
mentsoverothergeneralandcodeLLMs,theireffectivenessstill
Figure7:Pass-rate@kandTemperature
fallsshortofbeingfullysatisfactory.Ouranalysisextendstoevalu-
atingourdatasetwithLLMsexclusivelytrainedonhardwarelan-
guages,namelyVeriGenandRTLCoder.Contrarytoexpectations, low-dimensionalrepresentationsforcodesanderrormessages,sim-
thesespecializedhardwarelanguagemodelsdonotoutperform ilartoourBERT-LSTMmodel’sapproachtocomputingrelevance
theirgeneralandcodeLLMcounterpartsinourHDLdebugging throughrepresentationsimilarity.
context,suggestingthepossibilityofaninherenttaskdomaingener- Table5indicatesourenginesurpassesallbaselinesinaccurately
alizationissuewithinHDLdebuggingscenarios.Ontheotherhand, retrievingandcorrectingbuggycodequeries,highlightingitssu-
ourapproachconsistentlysurpassesdomain-specificsolutionsre- periorabilitytodecodecomplexbuggycodepatternsbeyondthe
gardlessofthevariedpromptengineeringtechniquesemployedor capabilitiesoftraditionalandmachinelearningmodels.Traditional
thedomain-specificdatausedfortraining,whichunderscoresthe methodslikeTF-IDFandBM25lackthedepthtounderstandcom-
effectivenessofourmethodology. plexcodebugs,whilemodelslikeXGBoostandRandomForest
fallshortinsemanticcomprehension.Ourengineeffectivelycom-
3.3 AblationStudies binestextualandsemanticanalysis,enhancingbugdetectionand
Firstly,weprovideadetailedanalysisoftheimpactofdifferent correction.
strategiesofourmethod.Table4illustratestheperformanceof
differentstrategies.Forbaseline,weonlyusedirectinferencestrat- 3.4 ParameterSensitivity
egyonbasemodel,i.e.,CodeLlama.SFTw/thindicatessupervised
Inthefollowingexperiments,weevaluateparametersensitivity
fine-tuningwithgeneratedthoughts.IntermsofRAG&SFTw/
acrossdifferenthyper-parametersincludingretrievedcodeinstances
th,wemainlyrefertoretrievalaugmentedLLMfine-tuningwhere
samplesandrelatedinferencetime.Wealsoconsidertemperature
bothretrievedcodeinstancesandrelevantinformationarecom-
andpass-rate@𝑘forvarious𝑘.
binedtogetherforLLMfine-tuning.FromTable4wecanobserve
thatSFTw/thoutperformsbaselinebyalargemargin.Moreover, 3.4.1 Thenumberofretrievedcodeinstances. Figure5depictsthe
combingRAG&SFTalsosignificantlyimprovestheperformance. impactofvaryingthenumberofretrievedcodeinstancesonmodel
Besides, we evaluate our search engine for RAG, comparing performance.Wefindthatperformanceimproveswitheachad-
itwithfourmethods:F-IDF[2],BM25[30],randomforest[29], ditionalinstancebetween1to5,achievingthemostsignificant
and XGBoost [6]. TF-IDF and BM25 assess relevance scores be- gains.However,beyondfiveinstances,gainsplateauorevende-
tweenbuggycodes,whileRandomForestandXGBoostgenerate crease,indicatingdiminishingreturns.Thissuggeststhatwhile
)%(etar-ssaP
)%(k@etar-ssaP
emit-ecnerefnIKDD’24,August25–29,2024,Barcelona,Spain TrovatoandTobin,etal.
addingretrievedcodeinstancesenhancesmodelperformanceup therepairofdefectivecode.Contemporarystrategiesemploying
toapoint,increasinginstancesbeyondthisthresholdleadstoinef- LLMspredominantlyutilizeretrievalaugmentedgeneration(RAG)
ficiency.Moreover,weobserveatendencyforthelanguagemodel andsophisticatedpromptengineeringtechniquestoaddressdebug-
togeneraterepetitiveorredundantcontentderivedfromprevious gingchallenges.Notably,Self-debug[7]representsapioneering
inputasthenumberofretrievedcodeinstancesincreases.This effortinapplyingLLMstocodedebugging,employingtargeted
phenomenonunderscoresacriticalareaforfutureexploration. promptengineeringforenhancedeffectiveness.RTLfixer[35]uti-
lizesbothRAGandpromptengineeringtotackletheHDLdebug-
3.4.2 FeasibilityonInference. Weassesstheimpactofincorporat- gingproblem.However,thesemethodsdonotshowsatisfactory
ingtheretrievedcodeinstancesontheadditionalinferencebud-
resultsinourindustry-levelcasesduetothelackofrequisiteknowl-
get.Figure6illustratesthenormalizedinferencetimestoclearly
edgeofHDLcodes.Fine-tuningwithHDLcoderesourcesisone
highlighttheincrementalbudgetrequired.Avalueof0indicates
alternativetotackletheproblem.Nevertheless,theseapproaches
theabsenceofretrievedcodeinstances.Ourobservationsreveal
needabundantandhigh-qualitylabeleddata,whichisnotsuit-
thatasthenumberofretrievedcodeinstancesincreases,thecor-
ableforHDLcodes,sincetherelatedHDLcodesarelimiteddueto
respondinginferencetimeexhibitsaslow,logarithmicincrease
privacyandcommercialissues.
ratherthanalinearone.Thispatternunderscorestheefficiency
ofourapproach,demonstratingthatintegratingretrievedcodein- 4.2 largelanguagemodelsforCodeGeneration
stancessignificantlyenhancesperformancewithoutproportionally Largelanguagemodels(LLMs)havetransformedthelandscape
increasingtheinferenceoverhead. of code generation by leveraging vast amounts of code data to
predictandgeneratesyntacticallyandsemanticallycorrectcode
3.4.3 Pass-rate@kandTemperature. Weexploretheeffectsofvary-
snippets[38].NotableamongthesemodelsisOpenAI’sCodex[5],
ingthetemperaturesettingsandthepass-rate@kfordifferentval-
whichpowersGitHubCopilot,offeringcontext-awarecodesug-
ues of𝑘, where𝑘 represents the number of answers generated
gestionsandcompletionstodevelopersdirectlywithintheirIDEs.
bytheLLM,asdiscussedin[5].Typically,alowertemperature
Another key contribution is from DeepMind’s AlphaCode [19],
settingyieldsmoredeterministicoutcomes,whereashighertem-
whichexcelsingeneratingcodesolutionsforcompetitiveprogram-
peraturesresultinmorevariedoutputs.Figure7demonstratesthat
mingchallengesandobtainsthetoppercentileofparticipantsin
as𝑘increases,sodoesoverallperformance.Specifically,atalower
codingcompetitions.RecentadvancementsinLLMstailoredforthe
temperature,suchas0.1,outputsaremoreconsistent,leadingtoa
codingdomainhaveseensignificantcontributions,withnotable
narrowerperformancerange.Conversely,athighertemperatures,
examplesincludingDeepSeek[3],Starcoder[18],Stabelcode[31],
like1.2,outputsbecomemorevaried,enhancingthelikelihood
Codellama[32],andWizardCoder[23].Thesemodelshavebeen
ofgeneratingcorrectanswersas𝑘 increases.Notably,forapass-
trainedonextensivedatasets,bothproprietaryandopen-source,to
rate@5,theperformanceatatemperatureof1.2surpassesthat
enhancecapabilitiesincodegeneration,completion,anddebugging.
at0.7,indicatingthatincreasedtemperaturesettingscanimprove
Withintherealmofhardwaredescriptionlanguages(HDLs),Chip-
outcomes,particularlyathighervaluesof𝑘.
NeMo[21]representsapioneeringeffortindevelopingadomain-
specificLLM,highlightingthechallengesandpotentialofapply-
4 RELATEDWORK
ingLLMstothehardwaredomain.Despitebeingtrainedonvast
4.1 AutomaticCodeDebugging hardware-specificdatasets,ChipNeMoachievesperformancethat,
Automaticcodedebugginghasemergedasapromisingareawithin whilecompetitive,doesnotsurpassthatofstate-of-the-artgeneral
softwareengineering[15,25].Givenacodewithbugs,thetaskisto LLMs,suchasGPT-4.Thisunderscorestheinherentcomplexitiesof
automaticallyfixthecodebugswiththecorrectfunctions,which adaptingLLMstothenuancesofHDL.Concurrently,initiativeslike
alleviatestheburdenofmanualdebuggingandfixingcodefaults. VeriGen[34]andRTLCoder[22],whichfocusonfine-tuningLLMs
Classictechniquescanbemainlyclassifiedastemplate-based[17, usingspecializeddatasets,havedemonstratedremarkableresults.
20],heuristic-based[41,47],constraint-based[43,44],andneural ThesedevelopmentsunderscoretheevolvinglandscapeofLLM
network-basedapproaches[8,11,49].Specifically,template-based applicationsinHDLdebugging,highlightingbothachievements
approachesapplyexpert-definedcodepatternstofixbugs.Theseap- andareasripeforfurtherexploration.
proachescanonlyrepaircodesinspecificpatternsandlackgeneral-
izationtootherbugs.Heuristic-basedapproachesapplypredefined 5 CONCLUSION
heuristicsandcannotcoveralltypesofbugs.Constraint-basedap- Inthispaper,weproposeanLLM-assistedHDLdebuggingframe-
proachesrepairbuggycodesbysolvingaconstraintproblem.These work,namelyHDLdebugger,whichconsistsofHDLdebuggingdata
methodscanbeaccurate,buttheyarecomputationallyexpensive. generation,asearchengine,andaretrieval-augmentedLLMfine-
Neuralnetwork-basedapproachesneednumeroushigh-qualityla- tuningapproach.Throughextensiveandvariedexperimentation
beledtrainingdatapairs(i.e.,pairsofbuggycodesandfixedcodes) withmultipleLLMs,wehaveunearthedpivotalfindingswithinthe
tooptimizeparameters,whichistime-consumingtocollectthe domain.Ourmethodsignificantlysurpassesexistingtechniques,
high-qualitycodepairs. achievinganexceptionalpass-rateofupto81.93%,indicatingthat
Recently,largelanguagemodels(LLMs)haveshednewlighton HDLdebuggercouldautomateandstreamlineHDLdebuggingfor
automaticcodedebugging.Theprevailinghypothesissuggeststhat chipdesign.Inaddition,weprovidein-depthexperimentalanalysis
LLMs,throughtrainingonextensiverepositoriesofopen-source andoutlinepotentialfuturedirectionsforHDLdebuggingwith
codesnippets,areadeptatidentifyingbugpatternsandfacilitating LLMsinAppendixD.HDLdebugger:StreamliningHDLdebuggingwithLargeLanguageModels KDD’24,August25–29,2024,Barcelona,Spain
REFERENCES [26] GonzaloNavarro.2001.Aguidedtourtoapproximatestringmatching.ACM
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,Ale- computingsurveys(CSUR)33,1(2001),31–88.
man,etal.2023.Gpt-4technicalreport.arXivpreprintarXiv:2303.08774(2023). [27] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
[2] AkikoAizawa.2003.Aninformation-theoreticperspectiveoftf–idfmeasures. Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.
InformationProcessing&Management39,1(2003),45–65. Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advances
[3] XiaoBi,DeliChen,GuantingChen,ShanhuangChen,DamaiDai,etal.2024. inneuralinformationprocessingsystems32(2019).
DeepSeekLLM:ScalingOpen-SourceLanguageModelswithLongtermism.arXiv [28] JeffRasley,SamyamRajbhandari,OlatunjiRuwase,andYuxiongHe.2020.Deep-
preprintarXiv:2401.02954(2024). speed:Systemoptimizationsenabletrainingdeeplearningmodelswithover
[4] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, 100billionparameters.InProceedingsofthe26thACMSIGKDDInternational
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda ConferenceonKnowledgeDiscovery&DataMining.3505–3506.
Askell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneural [29] StevenJRigatti.2017.Randomforest.JournalofInsuranceMedicine47,1(2017),
informationprocessingsystems33(2020),1877–1901. 31–39.
[5] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveira [30] StephenRobertson,HugoZaragoza,andMichaelTaylor.2004. SimpleBM25
Pinto,JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman, extensiontomultipleweightedfields.InProceedingsofthethirteenthACM
etal.2021. Evaluatinglargelanguagemodelstrainedoncode. arXivpreprint internationalconferenceonInformationandknowledgemanagement.42–49.
arXiv:2107.03374(2021). [31] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörn
[6] TianqiChenandCarlosGuestrin.2016.Xgboost:Ascalabletreeboostingsystem. Ommer.2022. High-resolutionimagesynthesiswithlatentdiffusionmodels.
InProceedingsofthe22ndacmsigkddinternationalconferenceonknowledge InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
discoveryanddatamining.785–794. recognition.10684–10695.
[7] XinyunChen,MaxwellLin,NathanaelSchärli,andDennyZhou.2023.Teaching [32] BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,Xiao-
largelanguagemodelstoself-debug.arXivpreprintarXiv:2304.05128(2023). qingEllenTan,YossiAdi,JingyuLiu,TalRemez,JérémyRapin,etal.2023.Code
[8] ZiminChen,SteveKommrusch,andMartinMonperrus.2022.Neuraltransfer llama:Openfoundationmodelsforcode.arXivpreprintarXiv:2308.12950(2023).
learningforrepairingsecurityvulnerabilitiesinccode.IEEETransactionson [33] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,Carlos
SoftwareEngineering49,1(2022),147–165. Guestrin,PercyLiang,andTatsunoriB.Hashimoto.2023.StanfordAlpaca:An
[9] JasonCong,BinLiu,StephenNeuendorffer,JuanjoNoguera,KeesVissers,and Instruction-followingLLaMAmodel. https://github.com/tatsu-lab/stanford_
ZhiruZhang.2011.High-levelsynthesisforFPGAs:Fromprototypingtodeploy- alpaca.
ment.IEEETransactionsonComputer-AidedDesignofIntegratedCircuitsand [34] ShailjaThakur,BaleeghAhmad,HammondPearce,BenjaminTan,BrendanDolan-
Systems30,4(2011),473–491. Gavitt,RameshKarri,andSiddharthGarg.2023.Verigen:Alargelanguagemodel
[10] TriDao,DanFu,StefanoErmon,AtriRudra,andChristopherRé.2022.Flashat- forverilogcodegeneration.arXivpreprintarXiv:2308.00708(2023).
tention:Fastandmemory-efficientexactattentionwithio-awareness.Advances [35] YunDaTsai,MingjieLiu,andHaoxingRen.2023.RTLFixer:AutomaticallyFixing
inNeuralInformationProcessingSystems35(2022),16344–16359. RTLSyntaxErrorswithLargeLanguageModels.arXivpreprintarXiv:2311.16543
[11] MichaelFu,ChakkritTantithamthavorn,TrungLe,VanNguyen,andDinh (2023).
Phung.2022.VulRepair:aT5-basedautomatedsoftwarevulnerabilityrepair.In [36] CharalamposTsourakakis.2015. Thek-cliquedensestsubgraphproblem.In
Proceedingsofthe30thACMJointEuropeanSoftwareEngineeringConference Proceedingsofthe24thinternationalconferenceonworldwideweb.1122–1132.
andSymposiumontheFoundationsofSoftwareEngineering.935–947. [37] GuanWang,SijieCheng,XianyuanZhan,XiangangLi,SenSong,andYangLiu.
[12] YunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YiDai, 2023.Openchat:Advancingopen-sourcelanguagemodelswithmixed-quality
JiaweiSun,andHaofenWang.2023.Retrieval-augmentedgenerationforlarge data.arXivpreprintarXiv:2309.11235(2023).
languagemodels:Asurvey.arXivpreprintarXiv:2312.10997(2023). [38] JunjieWang,YuchaoHuang,ChunyangChen,ZheLiu,SongWang,andQing
[13] MikeGordon.1995.ThesemanticchallengeofVerilogHDL.InProceedingsof Wang.2023.Softwaretestingwithlargelanguagemodel:Survey,landscape,and
tenthannualIEEEsymposiumonlogicincomputerscience.IEEE,136–145. vision.arXivpreprintarXiv:2307.07221(2023).
[14] DoritSHochbaum.1996.Approximatingcoveringandpackingproblems:set [39] YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahASmith,Daniel
cover,vertexcover,independentset,andrelatedproblems. InApproximation Khashabi,andHannanehHajishirzi.2022.Self-instruct:Aligninglanguagemodel
algorithmsforNP-hardproblems.94–143. withselfgeneratedinstructions.arXivpreprintarXiv:2212.10560(2022).
[15] KaiHuang,ZhengziXu,SuYang,HongyuSun,XuejunLi,ZhengYan,andYuqing [40] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,
Zhang.2023.ASurveyonAutomatedProgramRepairTechniques.arXivpreprint QuocVLe,DennyZhou,etal.2022.Chain-of-thoughtpromptingelicitsreasoning
arXiv:2303.18184(2023). inlargelanguagemodels.AdvancesinNeuralInformationProcessingSystems
[16] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,De- 35(2022),24824–24837.
vendraSinghChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel, [41] MingWen,JunjieChen,RongxinWu,DanHao,etal.2018. Context-aware
GuillaumeLample,LucileSaulnier,etal.2023. Mistral7B. arXivpreprint patchgenerationforbetterautomatedprogramrepair.InProceedingsofthe40th
arXiv:2310.06825(2023). internationalconferenceonsoftwareengineering.1–11.
[17] JiajunJiang,YingfeiXiong,HongyuZhang,QingGao,andXiangqunChen. [42] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,
2018.Shapingprogramrepairspacewithexistingpatchesandsimilarcode.In AnthonyMoi,PierricCistac,TimRault,RémiLouf,MorganFuntowicz,etal.
Proceedingsofthe27thACMSIGSOFTinternationalsymposiumonsoftware 2019.Huggingface’stransformers:State-of-the-artnaturallanguageprocessing.
testingandanalysis.298–309. arXivpreprintarXiv:1910.03771(2019).
[18] RaymondLi,LoubnaBenAllal,YangtianZi,NiklasMuennighoff,DenisKocetkov, [43] YingfeiXiong,JieWang,RunfaYan,JiachenZhang,ShiHan,GangHuang,andLu
ChenghaoMou,MarcMarone,ChristopherAkiki,JiaLi,JennyChim,etal.2023. Zhang.2017.Preciseconditionsynthesisforprogramrepair.In2017IEEE/ACM
StarCoder:maythesourcebewithyou!arXivpreprintarXiv:2305.06161(2023). 39thInternationalConferenceonSoftwareEngineering(ICSE).IEEE,416–426.
[19] YujiaLi,DavidChoi,JunyoungChung,NateKushman,etal.2022.Competition- [44] JifengXuan,MatiasMartinez,FavioDemarco,MaximeClement,SebastianLame-
levelcodegenerationwithalphacode.Science378,6624(2022),1092–1097. lasMarcote,ThomasDurieux,DanielLeBerre,andMartinMonperrus.2016.
[20] KuiLiu,AnilKoyuncu,etal.2019.TBar:Revisitingtemplate-basedautomatedpro- Nopol:Automaticrepairofconditionalstatementbugsinjavaprograms.IEEE
gramrepair.InProceedingsofthe28thACMSIGSOFTInternationalSymposium TransactionsonSoftwareEngineering43,1(2016),34–55.
onSoftwareTestingandAnalysis.31–42. [45] ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,ThomasLGriffiths,YuanCao,
[21] MingjieLiu,Teodor-DumitruEne,RobertKirby,ChrisCheng,NathanielPinckney, andKarthikNarasimhan.2023. Treeofthoughts:Deliberateproblemsolving
RongjianLiang,JonahAlben,HimyanshuAnand,SanmitraBanerjee,Ismet withlargelanguagemodels.arXivpreprintarXiv:2305.10601(2023).
Bayraktaroglu,etal.2023. Chipnemo:Domain-adaptedllmsforchipdesign. [46] ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,
arXivpreprintarXiv:2311.00176(2023). andYuanCao.2022.React:Synergizingreasoningandactinginlanguagemodels.
[22] ShangLiu,WenjiFang,YaoLu,QijunZhang,HongceZhang,andZhiyaoXie.2023. arXivpreprintarXiv:2210.03629(2022).
Rtlcoder:Outperforminggpt-3.5indesignrtlgenerationwithouropen-source [47] YuanYuanandWolfgangBanzhaf.2018. Arja:Automatedrepairofjavapro-
datasetandlightweightsolution.arXivpreprintarXiv:2312.08617(2023). gramsviamulti-objectivegeneticprogramming.IEEETransactionsonsoftware
[23] ZiyangLuo,CanXu,PuZhao,QingfengSun,XiuboGeng,etal.2023.Wizard- engineering46,10(2018),1040–1067.
Coder:EmpoweringCodeLargeLanguageModelswithEvol-Instruct. arXiv [48] ChenZhang,PengLi,GuangyuSun,YijinGuan,BingjunXiao,andJasonCong.
preprintarXiv:2306.08568(2023). 2015.OptimizingFPGA-basedacceleratordesignfordeepconvolutionalneural
[24] ArindamMitra,LucianoDelCorro,ShwetiMahajan,AndresCodas,Clarisse networks.InProceedingsofthe2015ACM/SIGDAinternationalsymposiumon
Simoes,SahajAgarwal,XuxiChen,AnastasiaRazdaibiedina,ErikJones,Kriti field-programmablegatearrays.161–170.
Aggarwal,etal.2023.Orca2:Teachingsmalllanguagemodelshowtoreason. [49] XiaoyuZhang,JuanZhai,ShiqingMa,andChaoShen.2021.Autotrainer:An
arXivpreprintarXiv:2311.11045(2023). automaticdnntrainingproblemdetectionandrepairsystem.In2021IEEE/ACM
[25] MartinMonperrus.2018. Automaticsoftwarerepair:Abibliography. ACM 43rdInternationalConferenceonSoftwareEngineering(ICSE).IEEE,359–371.
ComputingSurveys(CSUR)51,1(2018),1–24.KDD’24,August25–29,2024,Barcelona,Spain TrovatoandTobin,etal.
A ADDITIONALMATERIALSFORDATA ThebasicideaofAlg.1istogreedilyselectthebuggycodeinstance
GENERATION thatcanbringthemaximuminformationgainintotheselected
𝑞
instanceset𝐷 untilitexceedsthenumber𝑘.Specifically,given
Table6indicatesthecommonerrortypeinHDLcodesummarized 𝑐
𝑞
theinstanceset𝐷 ,wefirstdefinethemarginalinformationgain
byLLM.Bylocatingthemaincauseoferrors,wecandesignmulti- 𝑐
of𝐼 asfollows:
plemodificationfunctionstorecurtheerrorsinagivenHDLcode. 𝑗
Itiseasytorevealthatsomesimpleandsimilarmodificationscan △𝑆(𝐼 𝑗|𝐷 𝑐𝑞 )=𝑆(𝐼 𝑗 ∪𝐷 𝑐𝑞 )−𝑆(𝐷 𝑐𝑞 ) (9)
resultintotallydifferenterrorswhenmatchingtheerrorcauses 𝑞
AsillustratedinAlg.1,wefirstinitializetheinstanceset𝐷 as
andmodificationfunctions.Thesemodificationfunctionsthenbe- 𝑐
c foo um ne daa tn ioe nss oe fn sti oa ll up tia or nt so if nth the ere Rv Ae Grse see an rg ci hne ee nr gi in ng e.pipelineandthe 𝐼∅ 𝑗( ∈lin 𝐷ˆe 𝑐𝑞1) (. liT nh esen 2, -4w ).e Nc eo xm t,p wu ete ct oh me ps uim teil ta hr eit my as rc go ir ne a𝑠 l𝑖 i𝑚 nf( o𝑞 r, m𝐼 𝑗 a) tf ioo nre ga ac inh
The error message and solution database of the RAG search ofeach𝐼 𝑗 ∈𝐷ˆ 𝑐𝑞 andselectnode𝐼∗withthemaximum△𝑆(𝐼 𝑗|𝐷 𝑐𝑞 )
followingEq.(9)(line6-9).Then,weadd𝐼∗into𝐷𝑞
,andremoveit
engine is built on the foundation of the modification functions. 𝑐
Table7indicatesinformationinthedatabase.Foreachcommon from𝐷ˆ 𝑐𝑞 (lines10-11).Werepeattheselectionprocedureuntilwe
errorcodeinHDLcodecompilation,itprovidesrelateddescriptions haveselected𝑘buggycodeinstances(lines5-12).
anderrorrootreasonsthathelpdebug.Andmostessentially,the TheoremB.1. Alg.1canachievea1−1/𝑒approximationratio.
databasecollectsrecommendingsolutionsforagivenerrorcode.
𝑞
WhenretrievingknowledgewiththeRAGsearchengine,theroot Proof. The𝑆(𝐷 𝑐)inEq.(2)ismonotoneandsubmodular.
reasonandsolutiontogetherwiththeerrormessageandsolution • Monotone: Givenany𝐼 𝑖 ∈𝐷ˆ 𝑞𝑐 andselectedinstancesset𝐷 𝑞𝑐,we
willbereturnedtoformLLMinput. canobtain𝑆(𝐷 𝑐𝑞 ∪𝐼 𝑖)−𝑆(𝐷 𝑐𝑞 ) ≥ 0.Thus,𝑆(𝐷 𝑐𝑞 )ismonotone
increasing.
B ADDITIONALMATERIALSFORRAG • Submodularity: Given𝐷˜ 𝑐𝑞 ⊂ 𝐷 𝑐𝑞 ,andabuggycodeinstance
B.1 BERT-LSTM 𝐼 𝑖 ∉𝐷˜ 𝑐𝑞 ,𝐷 𝑐𝑞 ,wecanobtain:
T bih lie tiB esE oR fT B-L ES RT TM (Bm ido id ree cl ts iy on ne ar lg Ei nze cs odth ere Rco epn rte ex setu na tal te iom nb se fd rd oi mng Tc ra ap na s-- 𝑆(𝐷˜ 𝑐𝑞 ∪𝐼 𝑖)−𝑆(𝐷˜ 𝑐𝑞 )=𝑠𝑖𝑚(𝑞,𝐼 𝑖)+ 𝑘1 𝑑𝑖𝑠(𝐼 𝑖,𝐷˜ 𝑐𝑞 ) (10)
(f Lo orm nger Ss h) ow rti -t Th erth me Mse eq mu oe rn yt )ia nl ed twat oa rp kr so .Sce ps es ci in fig cas lt lr ye ,n gg ivth eno af bL uS gT gM
y
𝑆(𝐷 𝑐𝑞 ∪𝐼 𝑖)−𝑆(𝐷 𝑐𝑞 )=𝑠𝑖𝑚(𝑞,𝐼 𝑖)+ 𝑘1 𝑑𝑖𝑠(𝐼 𝑖,𝐷 𝑐𝑞 ) (11)
c sio nd ge le𝑏 sa en qd ueit ns ce err 𝑞o 𝑠r =me [s Cs La Sg ,e 𝑏𝑚 ,S, Ew P,e 𝑚fi ]r .s St ic no cn ec ta hte en ta ot ke et nhe lem ngin thto oa f wSi enc ce an𝑑𝑖 o𝑠 b(𝐼 t𝑖 a, i𝐷 n˜ 𝑐𝑞 t) he= im nei qn u𝐼𝑗 a∈ l𝐷 i˜ t𝑐 y𝑞( a2 s− : 𝑠𝑖𝑚(𝐼 𝑖,𝐼 𝑗))and𝐷˜ 𝑐𝑞 ⊂𝐷 𝑐𝑞 ,thus
buggycodeanderrormessageistoolong,weseparatethequery
s sueq bu see qn uc ee n𝑞 c𝑠 ehin at so 𝑛a
𝑠
ts oe kt eo nf ss .u Tb h- es ne ,q wue en fc ee ds e{ a𝑠 c𝑘 h} s𝑘|𝑞 u=𝑠 b1| s/𝑛 eq𝑠, uw enh ce er 𝑠e 𝑘e ia nc th
o
Itdemon𝑆
s
𝑞( t𝐷 r˜ a𝑐𝑞 te∪ st𝐼 h𝑖) at− 𝑆𝑆 (( 𝐷𝐷˜ 𝑐𝑞𝑐𝑞 )) in≥ E𝑆 q( .𝐷 (2𝑐𝑞 )∪ is𝐼 s𝑖) ub− m𝑆 o( d𝐷 u𝑐𝑞 l) a.
r.
BERT,andBERTwilloutputasentence-levelvectorh𝑘 =𝐵𝐸𝑅𝑇(𝑠 𝑘), Since𝑆(𝐷 𝑐)ismonotoneandsubmodular,accordingto[14],the
whereh𝑘 capturesthecontextinformationoftokensin𝑠 𝑘.After approximationratioofAlg.1is1−1/𝑒. □
wob eta ei mni pn lg oyall as Bu ib -s Le Sq Tu Menc toee cm apb te ud rd ein log ns g{ -h ra𝑘 n} 𝑘 g|𝑞 = e𝑠 1| d/𝑛 e𝑠 pefo nr d{ e𝑠 n𝑘 c} i𝑘 e|𝑞 s=𝑠 1| a/ n𝑛𝑠 d, TimeComplexity.Assumethedimensionofz𝑤 andz𝑠 are𝑑 𝑤
intricatepatternsacrossdifferentsubsequencesas{h 𝑘′} 𝑘|𝑞 =𝑠 1|/𝑛𝑠 = a sin md il𝑑 a𝑠 r, itr yes sp ce oc retiv 𝑠𝑖e 𝑚ly (. 𝑞F ,i 𝐼r 𝑗s )t, foit rt ea ak ce hs 𝐼𝑂 𝑗( ∈𝑁 𝐷ˆ(𝑑 𝑐𝑞𝑤 (l+ in𝑑 e𝑠 2) -) 4t )o .Tc ho em np ,iu tt te akth ee s
BiLSTM({h𝑘} 𝑘|𝑞 =𝑠 1|/𝑛𝑠).UponacquiringthesequenceofBi-LSTM 𝑂(𝑘2(𝑑 𝑤+𝑑 𝑠))toselectthetop-𝑘buggycodeinstancesfrom𝐷ˆ 𝑐𝑞
outputs,weuseaself-attentionmechanismtogeneratethefinal (line 5-12). Thus, the time complexity of the top-𝑘 buggy code
representationof𝑞𝑠 asz𝑠 =Self_Attention({h′}|𝑞|/𝑛𝑠).Theself- selectionalgorithmis𝑂((𝑁 +𝑘2)(𝑑 𝑤+𝑑 𝑠))intotal.
𝑘 𝑘=1
attentionblocksattendtodifferentpartsoftheLSTMsequence,
enablingthemodeltofocusonthemostrelevantfeaturesforclassi- C RETRIEVAL-AUGMENTEDLLM
fication.Then,wefeedz𝑠 toalinearlayertopredictbuggypattern FINE-TUNING
probabilityforthebuggycodequery(𝑏,𝑒).
C.1 ImplementationDetails
We implement our approach in PyTorch [27], and fine-tune on
B.2 GreedyAlgorithm
CodeLlama[32]whichisprovidedbyhuggingface[42]modelzoo.
AsintroducedinSec.2.2,thetop-𝑘buggycodeprobleminEq.(2)is
The foundation of our code is built upon the FastChat and Al-
NP-hard,indicatingthatwecannotobtaintheoptimaltop-𝑘buggy
pacaframeworks,incorporatingcutting-edgetechnologiessuchas
𝑞
codeinstances𝐷 inanypolynomialtime.Thus,weproposea
𝑐 flashattention[10],deepspeed[28],toenhanceeffectivenessdur-
greedyalgorithminAlg.1withatheoreticalguaranteetooptimize
ingbothtrainingandinferencephases.Ourexperimentalsetup
there-rankobjectiveinEq.(2).Forclarification,wedenotethe
utilizeseightNVIDIA-GTXA100GPUswith80Gmemorytoen-
𝑞
Eq.(2)by𝑆(𝐷 𝑐)asfollows.
sureenoughcomputationalcapacity.Fortraining,weprimarily
𝑞 ∑︁ 1 ∑︁ 𝑞 adheretothedefaulthyperparameters.Duringtheinferencestage,
𝑆(𝐷 𝑐)= 𝐷m
𝑐𝑞
⊆a 𝐷x
ˆ 𝑐𝑞
𝐼𝑖∈𝐷
𝑐𝑞𝑠𝑖𝑚(𝑞,𝐼 𝑖)+
𝑘
·
𝐼𝑖∈𝐷
𝑐𝑞𝑑𝑖𝑠(𝐼 𝑖,𝐷 𝑐)
w ine Ce hm ipp Nlo ey ma ogr [e 2e 1d ],y td oe mco id tii gn ag test tr ha ete sg iy g, na ik fii cn at no tt ch oe ma pp ip laro tia oc nh cu os se td
s
associatedwiththisprocess.HDLdebugger:StreamliningHDLdebuggingwithLargeLanguageModels KDD’24,August25–29,2024,Barcelona,Spain
Table6:SampleHDLerrorandmodificationfunctions.
ErrorType ErrorCause ModificationFunction
Memory RAMconnectionorRAMport Pulsesignalmodification
Clock Clockunitconnectionandavailability Pulsesignalorassignmentmodification
Trace Traceunitavailabilityandusage Assignmentmodification
Compression CompressionunitdefinitionandI/O Registerorassignmentmodification
Data DataValue Registerorpulsesignalmodification
TestPoint Procedurescompleteness Allvariablesmodification
PatternReform Patternequivalence Assignmentandprobemodification
SyntaxE Sscriptsyntaxerror Syntaxmodification
NetlistE Netlistattribute Netlistmodification
SimulationE Simulationconfig Configmodification
1 Theerrorshereareabstractedandmodifiedforinformationsecurity,butstillreflectthecategoriesandcauseswithoutrevealing
details.
Table7:Examplesoferrorinformation.
Errorcode Descriptions Root-Reasons Solutions
C-error-1 Netlistnotcorrectlyobtaindefinedsignal Definitionnotmatchnetlist Modifydefinitionornetlist
C-error-2 Notdefinetopportasoutput Signalnotoutput Modifytopportasoutput
C-error-6 Clocksignalnotoffduring** **getwrongsignal Correctlyoffclocksignal
T-error-2 Clockdefinitionduplicate clocknameconflict Deleteonedefinition
T-error-4 Probeinitilizedas0in** Invalidinitialization Modifyprobeinitializationtonon-0
T-error-18 Assignmentinnon-initializationstage Wrongassignment Deleteassignment
T-error-27 Pulsenon-existvariable wrongpulse Deletepulse
M-error-1 Memoryclosewhenread Wronglyoffmemory Correctmemoryandconstraint
M-error-17 Logicalloopinnetlist Logicalloopinnetlist Modifynetlist
P-error-8 Definitionlackofshift Wrongdefinition Modifydefinition
1 Theruleshereareabstractedandmodifiedforinformationsecurity,butstillreflectthemeanings,causesandsolutionswithoutrevealingdetails.
Algorithm2ThoughtsGenerationFlow Algorithm1Top-𝑘relevantcodeselection
Input: Thoughtsgenerationprompt𝑝 𝑡,correctresponseprompt Input: Buggycodequery𝑞=(𝑏,𝑒)and𝑁 codeinstances𝐷ˆ 𝑐𝑞 ,and
𝑝 𝑐,buggycode𝑏,correctcode𝑐,errormessageas𝑚,retrieved parameter𝑘.
informationas𝑟𝑎𝑔𝑑,largelanguagemodelas𝐿𝐿𝑀,Numberof Output: Top-𝑘instances𝐷 𝑐𝑞 ⊆𝐷ˆ 𝑐𝑞 .
thoughtstogenerate𝑘. 1: Initialize:𝐷ˆ 𝑐𝑞 ←∅.
Output: Generatedthoughtsti. 2: for𝐼 𝑗 ∈𝐷ˆ 𝑐𝑞 do
1: Initialize:𝐺 ←{},𝐶 ←{},𝐷 ←{}. 3: 𝑆𝑖𝑚(𝑞,𝐼 𝑖)←Eq.(1)
2: for𝑗 =1to𝑘do 4: endfor
3: 𝑡 𝑗 ←𝐿𝐿𝑀(𝑝 𝑡 ◦𝑏◦𝑐◦𝑚◦𝑟𝑎𝑔𝑑)//Generate thoughts. 5: for𝑖 =1to𝑘do
4: 𝐺 ←𝐺∪𝑡 𝑗 //Append 𝑡 𝑗 to 𝐺. 6: for𝐼 𝑗 ∈𝐷ˆ 𝑐𝑞 do
5: endfor 7: △𝑆(𝐼 𝑗|𝐷 𝑐𝑞 )=𝑆(𝐼 𝑗 ∪𝐷 𝑐𝑞 )−𝑆(𝐷 𝑐𝑞 )
6: foreach𝑡 𝑗 in𝐺 do 8: endfor
87 ::
scr
𝐶𝑐 i𝑗
p
←t←
.
𝐶𝐿𝐿 ∪𝑀
𝑐
𝑗(𝑝 /𝑐 /◦ Ap𝑏 p◦ en𝑚 d◦ 𝑐𝑟 𝑖𝑎𝑔 t𝑑 o◦ 𝐶𝑡 .𝑗)//Predicted correct
1 10
19
:
:: 𝐼
𝐷
𝐷ˆ∗
𝑐 𝑐𝑞
𝑞=
=
=a
𝐷
𝐷r ˆg
𝑐 𝑐𝑞
𝑞m
\
∪a
𝐼
𝐼x
∗
∗𝐼𝑗∈𝐷ˆ 𝑐𝑞△𝑆(𝐼 𝑗|𝐷 𝑐𝑞 )
9: endfor
12: endfor
11 10 :: for Cea alc ch u𝑐 la𝑗 tein ed𝐶 itd do
istance𝑑 𝑗 ←EditDistance(𝑐,𝑐 𝑗)
13: return𝐷 𝑐𝑞 .
12: 𝐷 ←𝐷∪𝑑 𝑗 //Append 𝑑 𝑗 to 𝐷.
13: endfor C.2 ThoughtsGeneration
14: Sort𝐷andobtainrelatedthoughts𝑡∗ 𝑗, AsAlgorithm2illustratetheflowofthoughtsgeneration,Forsim-
15: return𝑡∗ 𝑗. plicity,weomitthesampleindex.Theprocessmainlyconsistof4
majorsteps.WefirstlyconcatenateThoughtsgenerationprompt𝑝 ,
𝑡
Buggycode𝑏,Correctcode𝑐,Errormessage𝑚,relevantinforma-
tion𝑟𝑎𝑔𝑑,andpasstoLLMforinference,resultofwhichstandsfor
generatedthoughts𝑡1...𝑡 𝑘.Afterobtainingthethoughtsforthe
errorcode,wethenconcatenatecorrectresponseprompt𝑝 ,Buggy
𝑐KDD’24,August25–29,2024,Barcelona,Spain TrovatoandTobin,etal.
#1: Task Requirement Prompt thecorrectcodeas𝑐 𝑖andthoughtsas𝑡 𝑖.Generally,compute𝑝(𝑐 𝑖|𝑡 𝑖)
proveschallengingduetotheinaccessibilityofsuitablethoughts.
Below is an error snippets, paired with an error message that provides
error information. ByleveragingBayes’rule,wecanreparameterizetheformulation
Return the correct snippets based on the error message. as𝑝(𝑐 𝑖|𝑡 𝑖) ∝𝑝(𝑡 𝑖|𝑐 𝑖)𝑝(𝑐 𝑖).Byfocusingon𝑝(𝑡 𝑖|𝑐 𝑖),weencourage
theLLMtoalignitsoutputswithverifiedcorrectcode.
#2: Retrieved Code Instances
Example1: Example2: Example3: Example4: C.3 Temperaturesetting
Buggy Code: Buggy Code: Buggy Code: Buggy Code: … Bymodifyingtemperaturewecangeneratemultiplethoughtsdiffer-
… … … … ently.Weusemultinomialsamplingtogeneratesamplesrandomly.
Correct Code: Correct Code: Correct Code: Correct Code: Defineasequenceofinputtokensx={𝑥1,𝑥2,...,𝑥 𝑛},denote𝜋 𝜃 as
… … … … theLLMgenerationwithoutpassingfinalsoftmaxlayer.Thenext
token𝑥 𝑛+1canbeobtainedby:
…#3 : Buggy Code #4: Error M Soe ls usa tig oe
n
&
s
Retrieved 𝑜𝑢𝑡 =𝜋 𝜃(x)
assign_pi
1 procedure rule TPT-83 occurred(Not
𝑝𝑟 =𝑠𝑜𝑓𝑡𝑚𝑎𝑥 𝑇(𝑜𝑢𝑡) (12)
ap sro sib ge n_ rp so
t_n 1
a …llow to pulse non-clock pin in timeplate) 𝑥 𝑛+1∼𝐶𝑎𝑡𝑒𝑔𝑜𝑟𝑖𝑐𝑎𝑙(𝑝𝑟),
pulse_clocks The reason is that the define_timeplate where𝑜𝑢𝑡 isthenexttokenlogitsoutput.𝑠𝑜𝑓𝑡𝑚𝑎𝑥 𝑇 meansthe
… block –clocks contains a non-clock pin softmax function with temperature where the formula of prob-
…
ability 𝑝𝑟 is
𝑒𝑜𝑢𝑡𝑖/𝑇
.𝑇 is the temperature parameter where
𝑖 (cid:205) 𝑗𝑒𝑜𝑢𝑡𝑗/𝑇
#5: Prompt Instruction higher𝑇 makestheoutputdistributionmoreuniform,thusintro-
P#l7ea: sTe hreotuurng hthtes cGorerencet rsnaitpipoents based on the current error snippets. ducingmorerandomness.𝐶𝑎𝑡𝑒𝑔𝑜𝑟𝑖𝑐𝑎𝑙 meansCategoricaldistribu-
tion.Forexample,assumethenexttokenprobabilitydistribution
#6: Response is{𝑑𝑜𝑔 : 0.4,𝑐𝑎𝑡 : 0.5,𝑏𝑖𝑘𝑒 : 0.1},thenthenexttokenisselected
It appears that the primary issue in your HDL script is related accordingtotheirprobabilities.Wecangenerate𝐾thoughtsbythis
to the incorrect use or definition of a clock signal. The error samplingstrategy.Therefore,wecangeneratethoughtsrandomly
message "A floating pin is defined as a clock" suggests that bycontrollingthetemperatureparameter.
there is a problem with how a clock It appears that the primary
issue in your HDL script is related to the incorrect use or
C.4 Debuggingexample
definition of a clock signal. The error message "A floating pin
is defined as a clock" suggests that there is a problem with how Figure8illustratesanexampleofHDLdebugging.Whengiven
a clock signal is being handled or defined in your script. The anerrorscriptandrelatederrormessages,wefirstuseoursearch
error seems to be associated with the rst_n signal. is being enginetoretrievedrelatedexpertsolutionsandsimilarin-context
handled or defined in your script. The error seems to be samples (default 5 buggy code instances). Then we feed buggy
associated with the rst_n signal.
codes,errormessages,andrelatedin-contextsamplesandexpert
solutionstoourfine-tunedLLMs.Finally,theLLMwillproducea
The correct script is:
correspondinganalysisandrelatedcorrectscript.
(cid:1)
assign_pi
probe_po D ANALYSISANDDISCUSSION
pulse_clocks This section delves into a detailed examination of our research
(cid:1) findings,focusingontheutilityandimplicationsofleveragingLLMs
Figure8:AcaseofHDLdebugging. fordebuggingwithinHDLenvironments.Weprovideathorough
assessmentofLLM-baseddebuggingcapabilitiesandtheeffects
code𝑏,Errormessage𝑚,,andgeneratedthought𝑡 𝑗 toconstruct ofiterativedebuggingprocedures.Additionally,weexploreboth
thestep2inputforLLM.Theinferenceresultdenotesthepredicted thechallengesandprospectsofincorporatingLLMsintotheHDL
correct code script𝑐1...𝑐 𝑘 for correction the given error code. debuggingframework.
Givenabovetwosteps,thecodecorrectionispreparedsuccessfully.
Thereststepswillfocusonfindingthebestsuitedcorrectionfor D.1 Trade-offsinRAGandSFT
givenerrorcode.Toaccomplishthat,forgivenpredictedcorrect
Typically,inaRetrieval-AugmentedGeneration(RAG)system,the
codescript𝑐1...𝑐 𝑘,weobtaintheeditdistance𝑑1...𝑑
𝑘
fromeach
LLMactsasanagentthatremainsuntunedtopreserveitsgen-
tooriginalcorrectcode𝑐.Thefinalprocedureistosorttheedit
eralizationability,whichmightbecompromisedbytask-specific
distancesandretrievethegeneratedthought𝑡∗correspondingto
𝑗 parameteroptimization.However,toenhanceitsefficacyindebug-
thelowesteditdistance𝑑 𝑗.Thefinal𝑡∗ 𝑗 servesasourthoughoutput ging,wefinditessentialtofine-tunetheLLM,whichpresentsa
fortheflow. significantchallengeasLLMsareexpectedtoaddressabroaderar-
Wedelvedeeperintotheprocessofself-guidedthoughtgenera- rayofproblems.Onesolutionincorporateawiderrangeofcommon
tionThecoreprincipleofourapproachinvolvessubmittingboth instructionaldataduringtrainingtoretainitsgeneralapplicability.
thebuggycodeanditscorrectedversiontotheLLM,subsequently Anotherinnovativestrategyinvolvesconstructingamulti-LLM
promptingthemodeltogenerateinstructiveguidance.WedenoteHDLdebugger:StreamliningHDLdebuggingwithLargeLanguageModels KDD’24,August25–29,2024,Barcelona,Spain
architecturehousingnumerous"expert"modelstotacklespecific D.3 EnhancingtheUserExperiencein
domainchallenges.Additionally,HDLdebuggingencompassesvar- DebuggingSystems
ioustasksacrossdifferentstagesofelectronicdesignautomation,
Withinourdebuggingframework,wecanquantitativelyassess
promptingustoconsiderdevelopingacomprehensiveframework
performancemetricssuchaspassratesandexecutiontimes.How-
infutureinvestigations.
ever,duringthedeploymentphase,weobservedthattraditional
debuggingapproaches—transformingerroneousscriptsintocorrect
D.2 IterativeDebugging
onesareinsufficient.InthecontextofHDL,theemphasisextends
Ourinvestigationpredominantlyconcentratesonsingle-roundde- beyondmereerrorcorrectiontoenhancingthequalityoftheimple-
bugging,notingthatexistingstudies,suchas[35],demonstrate mentation,giventhedirectimpactofhardwarelanguageonchip
thatavastmajorityofissues(about90%)areresolvableinasingle performance.Codethatsuccessfullycompilesmaynotnecessarily
iteration.Nonetheless,complexscenariosnecessitatemultiplede- optimizechipdesignperformance.Additionally,exactsolutionsare
buggingrounds,posingasubstantialchallengeduetothehighcosts notalwaysrequired;engineersoftenbenefitfromsuggestive"hints"
associatedwithgatheringmulti-rounddata.Moreover,effective thatinspiresolutionstocomplexissues.Nonetheless,developinga
iterativedebuggingdemandsLLMscapableofenhancedcontextual metrictoevaluatethequalityofthesehintsrepresentsasignificant
analysisandcomprehension.Thisareawillbeafocalpointofour challenge.
subsequentresearchendeavors.