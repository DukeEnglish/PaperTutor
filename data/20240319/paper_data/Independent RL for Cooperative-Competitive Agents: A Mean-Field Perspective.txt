Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective
MuhammadAneequzZaman1 AlecKoppel2 MathieuLaurie`re3 TamerBas¸ar1
Abstract tion(Toumietal.,2020),financialmarkets(Lussangeetal.,
2021), and negotiations in markets (Krishna & Ramesh,
We address in this paper Reinforcement Learn-
1998). It is known that, finding equilibrium policies, i.e.,
ing (RL) among agents that are grouped into
theNashequilibrium(NE),foreachagentinsuchageneral-
teamssuchthatthereiscooperationwithineach
sumstochasticgameisingeneralNP-hard(Jinetal.,2021).
teambutgeneral-sum(non-zerosum)competition
Furthermore,inmanyreal-worldscenarios,agentsbehavein
acrossdifferentteams. TodevelopanRLmethod
groups,withcooperationinsidethegroupandcompetition
that provably achieves a Nash equilibrium, we
betweengroups. Therefore,inthiswork,westudymixed
focusonalinear-quadraticstructure. Moreover,
Cooperative-Competitive(CC)teamsettings,andseekto
to tackle the non-stationarity induced by multi-
understandconditionsforwhichaNEisachievable.
agentinteractionsinthefinitepopulationsetting,
weconsiderthecasewherethenumberofagents Toenableatractableformulation,wemaketwostructural
withineachteamisinfinite, i.e., themean-field specifications: (i) agents’ dynamics are linear and their
setting. ThisresultsinaGeneral-SumLQMean- costs1 arequadratic,i.e.,thelinear-quadratic(LQ)setting
FieldTypeGame(GS-MFTGs). Wecharacterize (Bas¸ar&Olsder,1998);and(ii)thenumberofagentswithin
theNashequilibrium(NE)oftheGS-MFTG,un- ateamapproachesinfinitysuchthatitmaybeapproximated
derastandardinvertibilitycondition. ThisMFTG byitsmean-field(MF)limit(Huangetal.,2006;Lasry&
NEisthenshowntobeO(1/M)-NEforthefinite Lions, 2006). This setting results in a General Sum LQ
populationgamewhereM isalowerboundon Mean-Field Type Game (GS-MFTG). We provide more
thenumberofagentsineachteam. Thesestruc- backgroundonthesetwospecifications.
turalresultsmotivateanalgorithmcalledMulti-
The LQ specification is motivated by a recent study of
playerReceding-horizonNaturalPolicyGradient
RL methods in the LQ Regulator (LQR) setting, which
(MRPG),whereeachteamminimizesitscumula-
hasgainedtractionforitsroleasabenchmarkproblemin
tivecostindependentlyinareceding-horizonman-
whichonecanestablishrigorousperformanceguarantees
ner.Despitethenon-convexityoftheproblem,we
(Fazeletal.,2018;Maliketal.,2019), aswellassolvea
establish that the resulting algorithm converges
varietyofpracticalproblemswithouttheopacityofneural
toaglobalNEthroughanovelproblemdecom-
networks (Ivanov & Lomev, 2012). The LQ setting has
positionintosub-problemsusingbackwardrecur-
severalreal-worldapplicationsasinfinance(linearquadratic
sivediscrete-timeHamilton-Jacobi-Isaacs(HJI)
permanent income theory (Sargent & Ljungqvist, 2000),
equations, in which independent natural policy
portfoliomanagement(Cardaliaguet&Lehalle,2018))and
gradientisshowntoexhibitlinearconvergence
engineering(WirelessPowerControl(Huangetal.,2003))
undertime-independentdiagonaldominance. Ex-
etc.Asidefromthesedirectusecases,LQsystemtheoryhas
perimentsilluminatethemeritsofthisapproach
beenessentialinobtainingnon-asymptoticsamplebounds
inpractice.
forRLalgorithmslikePolicyGradient(Fazeletal.,2018)
andActor-Critic(Yangetal.,2019),hencepavingtheway
forlaterworkstoobtainsimilarguaranteesinmoregeneral
1.Introduction
settings(Agarwaletal.,2021;Quetal.,2021). Ourgoalis
tounderstandtowhatextentwecanbroadenthescopeof
Multi-agent reinforcement learning (MARL) has gained
theLQsettingtoprovideadiscernibleproblemclassinCC
popularityinrecentyearsforitsabilitytoaddresssequen-
multi-agentsettings.
tialdecision-makingproblemsamongagents(Zhangetal.,
2021;Lietal.,2021). Whileasubstantialefforthasgone The mean-field approximation is motivated by the fact
into developing algorithms and performance guarantees that the complexity of equilibria in finite population CC
when agents interact in a purely cooperative setting, rel- settingsgrowswiththesizeoftheteams(Carmonaetal.,
ativelylessefforthasgoneintosettingswhereagents’objec-
tivesmaybeinopposition(Littman,1994),suchasconges-
1Costsarenegativepayoffs/rewards.
1
4202
raM
71
]GL.sc[
1v54311.3042:viXraIndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
2015;Sanjarietal.,2022),andthusthetransienteffectof 2023). Finally, whenthepolicygradientapproxima-
competitiveagents’policiesonstochasticstatetransitions tionerrorpertime-stepisO(ϵ),theresultingerrorin
appearsinanygradientestimateofthecost,whichcannot NEcomputationisshowntobeO(ϵ)(Theorem4.5).
beannihilatedunlessoneholdsotheragents’policiesfixed.
• FinallyweverifyconvergenceofMRPGexperimen-
The aforementioned structures yield a GS-MFTG, which tally, and provide a comparison with several bench-
upon first glance may seem a pristine setting, but in ac- marks.
tuality, even this simplified setting exhibits fundamental
technicalchallenges. SimilartotheRLforLQRsetting,the
objectiveisnon-convex,whichinprincipleshouldpreclude
finding a NE. This was already observed for the simpler
zero-sum (purely competitive setting) in (Carmona et al.,
2020). Therefore,inthiswork,weposethefollowingques-
tion:
Isitpossibletoconstructadatadrivenmethod
toachievetheNashEquilibriuminCCGames?
Inthiswork,weanswerthisquestionaffirmativelyandour
maincontributionsareasfollows: Figure1.MRPG Algorithm employs Natural Policy Gradient
(NPG) for each agent at timestep t, starting from t = T −1
andmovinginarecedinghorizonmanner(backwards-in-time),to
• WeformalizetheCCgameinafinitepopulationLQ
approximatetheNEofthegame.
frameworkandderiveitsmean-fieldapproximationas
a MFTG. This approximation introduces a bias that
MRPGIllustration. Figure1showstheflowoftheMRPG
isO(1/M)(Theorem2.2),whereM istheminimum algorithm. ThealgorithmutilizesNaturalPolicyGradient
numberofagentsinanyteam. Inspiredbyopen-loop (NPG) to converge to the NE πi∗,∀i first for t = T −
t
controlanalysis(AppendixA),weemployadecompo- 1, then t = T − 2 and continues in a receding horizon
manner(backwards-in-time). MRPGalgorithmsolvesthe
sitiontoestablishexistence,uniquenessandcharacteri-
HJI equations (Bas¸ar & Olsder, 1998) in an approximate
zationoftheNashequilibrium(NE)oftheGS-MFTG
manner. According to the HJI equations the NE can be
(Theorem2.3),understandardinvertibilityconditions. computedbackwardsintimeusing,
• To learn the NE of the GS-MFTG we develop a πi∗ =argminCi(π,π−i∗|π∗ ), t∈{0,...,T −1}
t t t [t+1,T−1]
Multi-playerReceding-horizonNaturalPolicyGradi- π
ent(MRPG)algorithm,inwhichtheplayersindepen- foralli,whereCi isthepartialcostofagentiandπ
t [t,t′]
dently update their policies using natural policy gra- isthesetofpoliciesforallagentsfromtimettot′. MRPG
dientsinareceding-horizonmanner(Zhang&Bas¸ar, utilizesNPGtoperformtheaboveshownminimization(for
2023). The receding-horizon approach decomposes eachtimestept)inanindependentdata-drivenstochastic
theharderproblemoflearningNEpoliciesforalltime- mannertoobtainπ˜i∗whichisshowntobe
t
steps,intosimplersub-problemsoflearningNEpoli-
ciesforeachtime-step,inaretrogrademanner. This π˜ ti∗ ≈argminC ti(π,π˜ t−i∗|π˜ [∗ t+1,T−1]), t∈{0,...,T −1}
π
approach is inspired by the Hamilton-Jacobi-Isaacs
(HJI)equations. InthispaperweshowthatduetotheLQframeworkand
underthediagonaldominanceconditionMRPGconverges
• WeestablishconvergenceofMRPGintwosteps. First, linearlytotheexactNEofthegamei.e. π˜i∗ ≈πi∗∀i,t.
t t
weestablishthatforeachtime-steptheMRPGalgo-
RelatedWorkInthepurelynon-cooperativesetting,consid-
rithmconvergestotheNEpolicyatalinearrate(The-
erableresearchactivityhastakenplace,inReinforcement
orem 4.4) under a time-independent diagonal dom-
Learning (RL) both in the finite population case (Zhang
inance condition2. This new condition generalizes
etal.,2021;Hamblyetal.,2023;Maoetal.,2022)andin
(whilebeingmucheasiertoverify)thesystemnoise
themean-fieldlimit(Mean-FieldGamesorMFGsforshort)
conditionof(Hamblyetal.,2023)foratleastsingle-
(Guo et al., 2019; Subramanian & Mahajan, 2019; Elie
shotgames(T =1). Furthermore,wealsoobviatethe
etal.,2020;Zamanetal.,2021;Angiulietal.,2022;Lau-
needforcovariancematrixestimation(Hamblyetal.,
riereetal.,2022;Zamanetal.,2022;2023);see(Laurie`re
2Wefurtherrelaxthediagonaldominanceconditionusinga et al., 2022) for a recent survey. Among the literature in
cost-augmentationtechniqueinAppendixH. competitivemulti-agentRL,two-playerZero-Sumgames
2IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
have proven to be particularly amenable to analysis with anteesforthefiniteagentCooperative-Competitivegame.
workssuchas(Zhangetal.,2019;Buetal.,2019;Carmona
LinearQuadraticCooperative-CompetitiveGames. We
etal.,2021). Converselytherehavealsobeenresultsindata thus begin by considering the finite agent Cooperative-
driventechniquesforsolvingequilibriawheretheutilitiesof Competitive(CC)gameproblem. InaCCgame,theagents
theagentssatisfyapotentialfunctioncondition(Dingetal., are grouped into N teams, with each team i ∈ [N] hav-
ing M agents and agent j in team i having linear dy-
2022). Apart from this work, RL for (General-Sum) CC i
namics, in that it is driven by a linear function of the
agentsremainspredominantlyanunchartedterritoryfrom
agent’sstatexi,j,theagent’sactionui,j,i,theaveragestate
atheoreticalanalyticalstandpointbesidesafewempirical t t
x¯i = (cid:80)Mi xi,j/M of population i, and the average ac-
studies(Loweetal.,2017). Ontheotherhand, therehas t j=1 t i
beensomerecentworksinRLforthepurelycooperative tionsu¯i t,k = (cid:80)M j=i 1ui t,j,k/M i ofpopulationi. Thecontrol
setting(alsocalledMean-FieldControl)(Subramanian& actionui,j,ireferstothatofagentjinteami,whereasui,j,k
t t
Mahajan,2019;Carmonaetal.,2019;Guetal.,2021;Car- fork ∈[N]\ireferstotheadversarialcontrolinputofplayer
monaetal.,2023)orfortheZero-Sumgamesettingwith kintothedynamicsofagentjofteami. Inaddition,thedy-
namicsareaffectedbyGaussiannoisewithateam-specific
finitelymany(Zhangetal.,2020)orinfinitelymanyagents
covariance, ωi,j ∼ N(0,Σi), which are independent for
(Carmonaetal.,2021). t
every (i,j), as well as common noise ω0,i ∼ N(0,Σ0).
t
Theworkof(Hamblyetal.,2023)isparticularlysalient Altogether,theseleadtothelineardynamicalsystem:
duetoalackofstructure(Zero-SumorPotential)intheutil-
N
itiesoftheagents. Thisworkprovesthatnaturalpolicygra- xi,j =Aixi,j+A¯ix¯i+(cid:88)(cid:0) Bi,kui,j,k+B¯i,ku¯i,k(cid:1) (1)
dientconvergestotheGeneral-SumNashEquilibriumgiven t+1 t t t t t t t t
k=1
knowledgeofmodelparametersandasystemnoiseinequal-
+ω0,i +ωi,j ,
ity(Assumption3.3(Hamblyetal.,2023)). Thisinequality t+1 t+1
ishardtoverifyastheLHSincreaseswithincreasingsystem for t ∈ {0,...,T − 1}, where Ai,A¯i ∈ Rm×m and
t t
noisebuttheRHSmaynotdecreaseduetothedependence Bi,k,B¯i,k ∈ Rm×p. For simplicity of analysis, we also
t t
of cost on system noise. We generalize this convergence assumethatagents’initialstatesarenullexceptfortheex-
resulttothedata-driven(withunknownmodelparameters) ogenousinputnoise: xi,j =ωi,j +ω0. Allagentsinteam
0 0 0
CCsettingunderamoregeneral,time-independentandver- i aim to optimize a single cost function Ji over a finite
M
ifiablediagonaldominanceconditionandobviatetheneed horizon,thatdependsonboththeteam’spolicyui ∈Ui
M
forcovariancematrixestimation. Thisismadepossibleby and those of other teams u−i ∈ Ui , where the set of
M
employing the receding-horizon approach introduced for policiesUi isadaptedtothestateprocessesofallagents
M
theLQRproblemunderperfect(Zhang&Bas¸ar,2023)and (xi,j) inacausalmanner.
imperfect(Zhangetal.,2023)information.
t i∈[N],j∈[Mi],∀t
T−1
Notation: We use [N] := {1,...,N} for any N ∈ N. Ji (ui,u−i)= 1 E (cid:88) (cid:88) ∥xi,j−x¯i∥2 +∥x¯i∥2 (2)
Wehave∥x∥ Arepresenting(x⊤Ax)1/2foranonnegative- M M i
j∈[Mi]t=0
t t Qi t t Q¯i t
definitematrixA,andwithA=I,∥·∥:=∥·∥ 2. Asingame N
theoreticformalism, a−i := (aj) representsthesetof +(cid:88) ∥uk,j,i−u¯k,i∥2 +∥u¯k,i∥2 +∥xi,j−x¯i ∥2 +∥x¯i ∥2
valuesaforplayersotherthani.
Gj a̸= ui
ssiandistributionwith k=1
t t Rtk,i t R¯ tk,i T T Qi
T
T Q¯i
T
meanµandcovarianceΣisdenotedbyN(µ,Σ). whereRik,R¯ik ≻0andQi,Q¯i ⪰0aresymmetricmatrices
t t t t
ofsuitabledimensions. ThesubscriptM ofJ referstothe
2.Setup&EquilibriumCharacterization factthatitisforthefinite-agentsetting. Thecostcontains
aconsensustermthatpenalizesdeviationfromtheaverage
We consider a general-sum game among multiple teams,
andregulationoftheaveragestatesandcontrolactions.
where agents within a team are cooperative, and distinct
teamscompete. Initially,weposethisprobleminthefinite-
Foreachagenttominimize(2)withrespecttopoliciesui,
agentsettingforthecasewhereagents’dynamicsarelinear theappropriatesolutionconceptisNashEquilibrium,which
stochasticandcosts(negativerewards)arequadratic,i.e., we subsequently define. A set of policies (ui∗) i∈[N] is a
the LQ framework. Subsequently, to enable the charac-
Nashequilibrium(NE)ifJi (ui∗,u−i∗)≤Ji (ui,u−i∗)
M M
terizationofNashequilibriaofthegame,weconsiderthe holdsforallalternativepolicyselectionsui ∈Ui .
M
mean-fieldapproximationwithineachteam,i.e.,thenumber
Mean Field Approximation (GS-MFTG). In general,
ofagentswithineachteamtendstoinfinity,whichalleviates
equilibrium policies in finite-player dynamic games are
thetransienteffectofotheragents’decisionsonthesystem
functionsofeveryplayer’sstate. Thiscauseschallengesin
dynamics(Carmonaetal.,2015). TheresultisaLQmean-
thecomputability andlearnability ofthe NEwitha large
fieldtypegame(MFTG).Inthissectionwedelineatethe
numberofagents. Duetothisdifficulty,weshiftfocusto
NashequilibriaoftheGS-MFTGandprovideϵ-Nashguar-
themean-fieldsettingwherethenumberofagentsineach
3IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
teamM →∞. TheNEpoliciesinthemean-fieldsetting boundingϵwithafunctionofthisdifference. Thecomplete
i
areshowntodependonthestateofthegenericagentand proof is provided in Appendix C. Although a similar (al-
√
theaveragestate(mean-field),thusresolvingthescalability beit slowerO(1/ N) bound has been shown in the case
problem. ThislimitinggameistermedtheGS-MFTG. ofpurelycompetitiveMFGs (Zaman etal.,2022), toour
knowledge,thisworkisthefirsttoquantifytheequilibrium
The state dynamics in the MFTG can be formulated by
concatenatingthestatedynamicsofthejthagent(forany gapbetweenfinitepopulationCCgamesandGS-MFTGs.
j ∈ N) from each team and discarding the superscript j.
CharacterizationofNashEquilibria. Next,westudya
Using(1),thedynamicsofthisjointstatewillbeasfollows
decompositionoftheGS-MFTGintotwosub-problems,the
N firstonepertainingtothemean-fieldandthesecondoneto
x =A x +A¯ x¯ +(cid:88)(cid:0) Biui+B¯iu¯i(cid:1) +ω0 +ω thedeviationfromthemean-field. Thisdecompositionis
t+1 t t t t t t t t t+1 t+1
i=1 inspiredbytheanalysisoftheopen-loopNashequilibrium
(3)
oftheMFTG,whichisdeferredtoAppendixA.Thenwe
use the discrete-time Hamilton-Jacobi-Isaacs (HJI) equa-
for t = {0,...,T − 1}, where A = diag((Ai) ),
t t i∈[N] tionstocharacterizetheNEandpresentcertaininvertibility
Bi = diag((Bi,k) ) and B¯i = diag((B¯i,k) ),
t t k∈[N] t t k∈[N] conditionstoguaranteeitsexistenceanduniqueness. These
x¯ =E[x |(ω0) ]andu¯i =E[ui |(ω0) ]fori∈[N].
t t s s≤t t t s s≤t conditionsareamainstayofscenarioswithfinitelymany
The policies of the players belong to the feasible sets
competingplayers(Bas¸ar&Olsder,1998);itmightalsobe
ui ∈Ui,whereUiisthesetofallpoliciescausallyadapted
to the state and mean-field process {x ,x¯ ,...,x ,x¯ }. notedthattheMFGframeworkwithinfinitelymanycom-
0 0 t t
Thecostofplayeri∈[N]is petingplayerstypicallyrequiresadifferentsetofconditions
(Huangetal.,2006;Lasry&Lions,2006).
Ji(ui,u−i)=E(cid:104)T (cid:88)−1
∥x −x¯ ∥2 +∥x¯ ∥2 +∥ui−u¯i∥2 Tofurtherelaborateonthis,letusdefiney t =x t−x¯ tand
t t Qi t t Q¯i t t t Rti vi =ui−u¯i. Thedynamicsofy andx¯ canbewrittenin
t=0 t t t t t
(cid:105) adecoupledmannerusing(3),
+∥u¯i∥2 +∥x −x¯ ∥2 +∥x¯ ∥2 (4)
t R¯ ti T T Qi
T
T Q¯i
T N
where Ri = diag(cid:0) (Ri,k) (cid:1) and R¯i = y t+1 =A ty t+(cid:88) B tiv ti+ω t+1,
t t k∈[N] t i=1
d ceia sg Q(cid:0)
i
t( ,R¯ Q¯ti,
i
tk) ⪰k∈ 0[N a] r(cid:1) eb≻ lock0 ma ar te rics ey sm sum ce htr ti hc a. t(QT
i
th )e
ii
m =a Qtri
i
t-
x¯
t+1
=A˜ tx¯
t+(cid:88)N
B˜ tiu¯i t+ω t0 +1, (5)
and (Q¯i) = Q¯i and 0 otherwise. Each player i ∈ [N] i=1
t ii t
aims to minimize its cost function Ji using its policy whereA˜i =Ai+A¯iandB˜i =Bi+B¯i.Sincethedynamics
t t t t t t
ui ∈ Ui where now we formally introduce the solution ofprocessesy andx¯ aredecoupled,wecandecompose
t t
conceptofNashequilibriumfortheMFTG. thecostofith player, Ji (4), intotwodecoupledpartsas
well:
Definition2.1. Thesetofpolicies(ui∗) areinNash
i∈[N]
equilibrium (NE) for the MFTG if for each i ∈ [N], Ji(ui,u−i)=Ji(vi,v−i)+Ji(u¯i,u¯−i),
y x˜
Ji(ui∗,u−i∗)≤Ji(ui,u−i∗)forallui ∈Ui.
Ji(vi,v−i)=E(cid:104)T (cid:88)−1 (cid:2) ∥y ∥2 +∥vi∥2 (cid:3) +∥y ∥2 (cid:105) ,
By the definition of the class of policies Ui this NE is
y
t=0
t Qi
t
t Rti T Qi
T
s py om licm iee st .ric, i.e., cooperating agents will have symmetric J x˜i(u¯i,u¯−i)=E(cid:104)T (cid:88)−1 (cid:2) ∥x¯ t∥2
Q¯i
t
+∥u¯i t∥2
R¯
ti(cid:3) +∥x¯ T∥2
Q¯i
T(cid:105) . (6)
t=0
ApproximateNashEquilibria. TheNEoftheGS-MFTG
ThisresultsintwodecoupledN-playerLQgameproblems,
isshowntobeaO(1/M)-NashequilibriumoftheCCgame andhenceweusethediscrete-timeHJIequationstochar-
(1)-(2)whereM :=min i∈[N]M iistheminimumnumber acterizetheNEoftheGS-MFTGinthefollowingtheorem.
of agents across all teams i. This result guarantees that BeforestatingthetheoremweintroducethecoupledRiccati
equationsforN-playerLQgames(Bas¸ar&Olsder,1998;
the NE found using RL techniques (in Section 3) will be
Hamblyetal.,2023). Considercontrolmatrices
arbitrarilyclosetotheNEofthefinitepopulationCCgame.
Theorem2.2. TheNEoftheMFTGisϵ-Nashforthefinite Ki∗ =(Ri+(Bi)⊤Pi∗ Bi)−1(Bi)⊤Pi∗ Li, ∀i∈[N]
t t t t+1 t t t+1 t
agentCCgame(1)-(2)whereϵ=O(1/min M )i.e.
i∈[N] i wherePi∗aredeterminedusingCoupledRiccatiequations,
t
(cid:18) (cid:19)
Tσ
J Mi (ui∗,u−i∗)− uii ∈n Uf
i
J Mi (ui,u−i∗)=O
min iM i
P ti∗ =L⊤
t
P ti +∗ 1L t+(K ti∗)⊤R tiK ti∗+Qi t, P Ti∗ =Qi T, (7)
M
s.t. L =A −(cid:80)N BiKi∗andLi =A −(cid:80) BjKj∗.
t t i=1 t t t t j̸=i t t
Theguaranteeisobtainedbyanalyzingthedifferencebe- TheexpressionsforK¯i∗andP¯i∗canbeobtainedbyreplac-
t t
tweenfiniteandinfinitepopulationcostfunctionsandby ingA ,Bi,Qi andRi matricesbyA˜ ,B˜i,Q¯i andR¯i.
t t t t t t t t
4IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Theorem2.3. Thesetofpolicies(ui∗) formaNEif andaveragecontrolu˜i = 1 (cid:80)M ui,j. Underthesecon-
i∈[N] t M j=1 t
andonlyif, trollaws,thedynamicsofagentj ∈[M]andthedynamics
oftheempiricalmean-fieldx˜ arelinear. Specificallythe
ui∗(x )=−Ki∗(x −x¯ )−K¯i∗x¯ , (8) t
t t t t t t t dynamicsoftheaveragestatex˜ is
t
i ∈ [N],t ∈ {0,....T −1}wherethecontrolparameters
Ki∗ areguaranteedtoexistandbeuniqueifthematrices x˜ t+1 =L¯ tx˜ t+ω˜ t0 +1, (9)
t
Φ andΦ¯ areinvertible.
t t where L = A −(cid:80)N BiKi, L¯ = A˜ −(cid:80)N B˜iK¯i
t t i=1 t t t t i=1 t t
The matrices Φ t are block diagonal matrices with the ith andω˜0 =ω0 +(cid:80)M ωj /M. NowasintheSection
diagonalentryRi+(Bi)⊤Pi∗ Bi andijthnon-diagonal t+1 t+1 j=1 t+1
t t t+1 t 2,weintroducethedeviationyj =xj −x˜ withdynamics
entries (Bi)⊤Pi∗ Bj. The emergence of the invertibil- t t t
t t+1 t
ityconditionisaconsequenceoftheHJIequationsandit yj =L yj +ω˜j , (10)
t+1 t t t+1
naturally arises in similar games with a finite number of
competingentities,e.g. LQGames(Bas¸ar&Olsder,1998). where ω˜j = (M −1)ωj /M −(cid:80) ωk /M. The
t+1 t+1 k̸=j t+1
costofanyagentj ∈ [M]andplayer/teami ∈ [N]under
3.Multi-playerReceding-horizonNPG controllaws(Ki,K−i)whereKi =(Ki,K¯i) forany
t t t∈[T]
(MRPG) i∈[N]canbedecomposedinasimilarmanner.
Inthissection, wediscussthechallengesthatarisewhen J˜i,j(Ki,K−i)=J˜i,j(Ki,K−i)+J˜i(K¯i,K¯−i)
y x˜
solvingtheNEthroughadata-drivenapproach,havinges-
T−1
tablisheditslinearformasshownin(8). InourCCsetting,
J˜i,j(Ki,K−i)=E(cid:104)(cid:88)
∥yj∥2 +∥yj∥2
(cid:105)
y t Qi+(Ki)⊤RiKi T Qi
finding the NE is elusive as the cost function even for a t t t t T
t=0
singleagentLQcontrolproblemisnon-convex(Fazeletal.,
T−1
2018;Huetal.,2023). Additionally,vanillapolicygradi-
J˜i(K¯i,K¯−i)=E(cid:104)(cid:88)
∥x˜ ∥2 +∥x˜ ∥2
(cid:105)
,
x˜ t Q¯i+(K¯i)⊤R¯iK¯i T Q¯i
ent is known to diverge, in purely competitive N-player t t t t T
t=0
LQgames(Mazumdaretal.,2019). (Hamblyetal.,2023)
hasproventheconvergenceofnaturalpolicygradientfor whereKi =(K ti) t∈[T]andK¯i =(K¯ ti) t∈[T]. Thisproblem
purelycompetitiveagentsalbeitundercompleteknowledge re-parameterizationallowsustodecouplethecostfunction
ofthemodelparametersandasystemnoisecondition. We intotermsofconsensuserroryandmeanfieldx˜.
show the linear rate of convergence of the natural policy
RecedingHorizonMechanism. Wesolveeachproblem
gradient to the CC NE even when model parameters are
usingthereceding-horizonapproach. Thisapproachisin-
unknownunderadiagonaldominanceconditionwhichis
spiredbytheHJIequations(Bas¸ar&Olsder,1998)which
showntogeneralizethesystemnoiseconditionforT =1.
obtaintheNEbysolvingfortheNEpolicyattimeT −1
Wealsoeliminateapotentialsourceoferrorintheanalysis
andthenmovinginretrograde-time. Thereceding-horizon
byobviatingtheneedforcovariancematrixestimation.
approachisadata-drivenversionofthediscrete-timeHJI
Wenowprovidesomedetailsofouralgorithmicconstruc- equations. Next,weprovidedetailsofthereceding-horizon
tion to achieve this result. The key idea is the use of approachfortheprocessy. Ateachtime-steptwesolve
receding-horizon approach (inspired by the discrete-time forthesetofcontrollersattimet,(K ti) i∈[N],whichmini-
Hamilton-Jacobi-Isaacs(HJI)equations)wherebyfinding mizethecostJ˜i,1(Ki,K−i),whilekeepingthecontrollers
y,t
policies for all the agents at a fixed time t (and moving (K ) fixed
s t<s<T
backwards-in-time) reveals a quadratic cost structure, al-
lowinglinearconvergenceguarantee. Thenetresultisthat minJ˜i (Ki,K−i)= (11)
y,t
teamscomprisingoffinitelymanyagents(M ≥2)follow- K ti
ingtheMRPGalgorithmapproachtheNE(Definition2.1) (cid:104) (cid:88)T (cid:105)
E ∥y1∥2 + ∥y1∥2 ,
oftheGS-MFTG(3)-(4). t Qi+(Ki)⊤RiKi s Qi+(Ki)⊤RiKi
t t t t s s s s
s=t+1
DisentanglingtheMeanFieldanddeviationfromMean-
Field. Let us define a joint state xj for j ∈ [M] by con- wherey ∼N(0,Σ ),Σ ≻0andKi =0foreachplayer
t t y y T
catenating the states of jth agents in all teams at time t i∈[N]. Noticethatthechoiceofagent1isarbitrary. The
such that xj = [(x1,j)⊤,...,(xN,j)⊤]⊤. Due to the lin- minimization problem at time t, (11) (due to forward-in-
t t t timecontrollers(K ) beingfixed)isquadraticinthe
earformofNE(8)werestrictthecontrollerstobelinear s t<s<T
instateandempiricalmean-field,xj andx˜ ,respectively, controlparameterK ti whichallowsittosatisfyaspecific
t t PLcondition(Lemma4.1)whichensuresthelinearrateof
suchthatui,j = −Ki(xj −x˜ )−K¯ix˜ ,whichresultsin
t t t t t t convergenceofnaturalpolicygradienttotheNE.Similarly
u˜i = −K¯ix˜ , where the average state x˜ = 1 (cid:80)M xj fortheprocessx˜ ateachtime-stept ∈ {T,...,0}wefix
t t t t M j=1 t
5IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
the controllers forward-in-time (K¯ ) and solve for Algorithm1MRPGforGS-MFTG
s t<s<T
thesetofcontrollers(K¯ ti) i∈[N]whichminimizethecost 1: Initialize Ki = 0,K¯i = 0 for all i ∈ [N],t ∈
t t
{0,...,T −1}
minJ˜i (K¯i,K¯−i):= (12)
K¯ ti x˜,t 2: fort=T −1,...,1,0,do
3: fork =1,...,K do
E(cid:104)
∥x˜ ∥2 +
(cid:88)T
∥x˜ ∥2
(cid:105)
4: NaturalPolicyGradientfori∈[N]
t Q¯i t+(K¯ ti)⊤R¯ tiK¯ ti s Q¯i s+(K¯ si)⊤R¯ siK¯ si
s=t+1 (cid:18) Ki(cid:19) (cid:18) Ki(cid:19) (cid:18) ∇˜i (Ki,K−i)Σ−1(cid:19)
t ← t −ηi y,t y (16)
wherex˜ ∼N(0,Σ ),Σ ≻0andK¯i =0,∀i∈[N]. K¯i K¯i k ∇˜i (K¯i,K¯−i)Σ−1
t x˜ x˜ T t t x˜,t x˜
MRPGAlgorithmConstruction. Beforeintroducingthe
MRPGalgorithm, wefirstprovidedetailsofwhatconsti- 5: endfor
tutes a valid search direction in policy space. We begin 6: endfor
bypresentingtheexpressionforthepolicygradientsinthe
receding-horizonsetting.
gradientandthenmovesonestepbackwards-in-time.Notice
Lemma 3.1. In a receding-horizon setting for a fixed
that(inAlgorithm1)tocomputethenaturalpolicygradients,
t ∈ {T − 1,...,0} the policy gradient of cost
onlytheperturbedcostsarerequired,asshownin(15),and
J˜i (Ki,K−i) with respect to Ki is ∇i (Ki,K−i) :=
y,t t y,t estimatingthecovariancematrixisnotrequiredasinthe
δJ˜i (Ki,K−i)/δKiwhere
y,t t literature (Hambly et al., 2023). This is an independent
learningalgorithmasallteamsindependentlycomputetheir
∇i y,t(Ki,K−i)=2(cid:0) (R ti+(B ti)⊤P yi ,t+1B ti)K ti (13) naturalpolicygradientsandthelearningratesηi arealso
k
−(Bi)⊤Pi (cid:0) A −(cid:88) BjKj(cid:1)(cid:1) Σ , independent.
t y,t+1 t t t y
j̸=i
4.AchievingNashEquilibrium
withPi isdefinedintermsofcontrollers(Ki)
t+1 s i∈[N],t<s<T
InthissectionweanalyzetheMRPGalgorithmandshow
Pi =Qi+(Ki)TRiKi+L⊤Pi L ,Pi =Qi . (14)
y,t t t t t t y,t+1 t y,T T linearrateofconvergencetotheNE.Toestablishourmain
result, several key steps are needed. Some are standard,
The policy gradient ∇i (Ki,K−i) := suchasunbiasednessandsmoothnesspropertiesofgradient
x˜,t
δJ˜i (K¯i,K¯−i)/δK¯i has a similar expression. Com- estimators(Lemma4.2). Moreuniquetothisworkisthe
x˜,t t
paring(13)withthepolicygradientin(Hamblyetal.,2023) establishmentofaPolyak-Łojasiewicz(PL,alsoknownas
we notice that due to the receding-horizon approach the gradientdominance)inequalitythatrelatesthedifference
matrixP˜i (14)isfixedandthepolicygradient∇i isa of costs with the update direction as Lemma 4.1. While
y,t y,t
function of Σ y which is chosen in the receding-horizon sucharesultisexpectedifanobjectivefunctionisstrongly
approach to be positive definite. This allows us to convex,inanon-convexsettingitgenerallydoesnothold.
circumvent the system noise condition (Hambly et al.,
Thatitdoesinthesingle-agentLQRsettingisthecentral
2023). We use mini-batched zero-order techniques as in
contributionof(Fazeletal.,2018). Here,wegeneralizeit
(Fazeletal.,2018;Maliketal.,2019)toapproximatethe
policygradientswith∇˜i (Ki,K−i)and∇˜i (K¯i,K¯−i), totheLQGS-MFTGsetting. Inparticular,weestablisha
y,t x˜,t PLconditionofcostJ˜i whichismuchsimplerthanthe
whicharethestochasticgradientsofthecostsJ˜i andJ˜i y,t
y,t x˜,t priorwork(Hamblyetal.,2023). Weproceedthenwiththe
followingtechnicallemma.
∇˜i (Ki,K−i)=
m
(cid:88)Nb
J˜i (Kˆi(e ,t),K−i)e (15) Lemma4.1. (Polyak-Łojasiewiczinequality)Thecostfunc-
y,t N br2
j=1
y,t j j tionJ˜ yi ,t(K˜i,K−i∗)satisfiesthefollowinggrowthcondition
withrespecttogradient∥∇i (K˜i,K−i∗)∥2
respectively, where e ∼ SpN×mN(r) is the perturbation y,t F
j
and Kˆi(e,t) := (Ki + e,...,Ki ) is the perturbed J˜i (K˜i,K−i∗)−J˜i (Ki∗,K−i∗)
t T−1 y,t y,t
controller set at time-step t. N b denotes the mini-batch ≤ ∥Σ K∗∥ ∥∇i (K˜i,K−i∗)∥2
sizeandrthesmoothingradiusofthestochasticgradient. σ σ2 y,t F
∇˜i (K¯i,K¯−i) is computed in a similar manner. In the R y
Apx˜ p,t endix I we generalize to the sample path-cost oracle where K˜i = (K ti,K ti +∗ 1,...,K Ti∗ −1) and σ y is the mini-
fromtheexpectedcostoracleof(12). mumeigenvalueofΣ y.
NowwestatetheMRPGalgorithm. Thealgorithmisquite ThecostJ˜i alsosatisfiesasimilarPLcondition. Inthis
x˜,t
simple;startingattimet=T −1eachteam/playeri∈[N] sectionweanalyzetheMRPGalgorithmandshowthelin-
updatesitscontrolparameters(Ki,K¯i)usingnaturalpolicy ear rate of convergence to the NE. Let us first introduce
t t
6IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
the smoothed gradients (Fazel et al., 2018; Malik et al., functionofmodelparametersandcanbeverifiedbydirect
2019;Huetal.,2023)ofcostsJ˜i andJ˜i ,respectivelyas, computation. MoreoverAssumption4.3isindependentof
y,t x˜,t
∇¨i (Ki,K−i)and∇¨i (K¯i,K¯−i)whicharetheexpecta- thetime-horizonT whereastheSNconditiongetsharder
y,t x˜,t
tions of the stochastic gradients. The smoothed gradient tosatisfywithincreasingT. Finallyweestablish(inproof
∇¨i (Ki,K−i)isgivenby ofTheorem4.4)thatforT =1Assumption4.3generalizes
y,t
theSNassumption. NowunderAssumption4.3weshow
∇¨i (Ki,K−i)= m E (cid:2) J˜i (Kˆi(e,t),K−i)e(cid:3) , thelinearrateofconvergenceofreceding-horizonupdate
y,t r e y,t t (16)totheNE.
wheree∼Sm−1(r). ∇¨i (K¯i,K¯−i)hasasimilarexpres- Theorem4.4. UnderAssumption4.3foreacht ∈ {T −
x˜,t
sion. Nowwestatesomeresultsfromtheliterature. The 1,...,0}, if conditions in Theorem 2.3 are satisfied, k =
followingresultshowsthatthestochasticgradientisanun- Θ(cid:0) log(cid:0)1(cid:1)(cid:1) , ηi is upper bounded by model parameters,
ϵ k
biasedestimatorofthesmoothedgradientandquantifiesthe theapproximationerrorinthestochasticgradientisδ t =
biasbetweenthesmoothed,stochasticandpolicygradients. O(ϵ), then the optimality gap ∥Ki,k −Ki∗∥ = O(ϵ) and
t t
Lemma4.2((Maliketal.,2019)). Considerthesmoothed
∥K¯ ti,k−K¯ ti∗∥=O(ϵ)foralli∈[N].
gradient∇¨i (Ki,K−i)in(15)fortheper-teamconsensus
y,t
Closed-formexpressionsoftheboundscanbefoundinthe
error yj for team j at time t, as well as the stochastic
t proof. Theseresultsaremoregeneralthanthosein(Lietal.,
gradient ∇˜i (Ki,K−i) of the receding horizon cost in
y,t 2021;Fazeletal.,2018;Huetal.,2023;Maliketal.,2019)
Lemma3.1. Thefollowingconditionshold:
duetothepresenceofcompetingplayerslearninginaninde-
E[∇˜i (Ki,K−i)]=∇¨i (Ki,K−i), pendentfashion. Duetothereceding-horizonapproach,the
y,t y,t
costfunctions(11)-(12)tobeminimizedhaveaquadratic
∥∇¨i (Ki,K−i)−∇i (Ki,K−i)∥ =O(r),
y,t y,t 2 structure,whichsatisfiesaPLcondition(Lemma4.1)and
(cid:18)(cid:112) logδ−1(cid:19)
allows for the linear rate of convergence. Moreover, the
∥∇˜i (Ki,K−i)−∇¨i (Ki,K−i)∥ =O
y,t y,t 2 N r receding-horizonapproachboundsthedifferencebetween
b
the target controller (one which solves (12)) and the NE
wherethelastinequalityfollowswithprobability1−δ,N b Ki∗,bycontrollingtheerrorintheP˜i matrix(14). Finally
isamini-batchsize,andristhesmoothingparameter. t y,t
the receding-horizon approach obviates the system noise
conditionin(Hamblyetal.,2023)byexplicitlydesigning
Similar bounds can also be obtained for the gra-
thecovariancematrixΣ tobepositivedefinite. Allthese
dients pertaining to process x˜, ∇i (Ki,K−i), y
x˜,t factorscombinetoensurethatifK =O(log(1/ϵ))theer-
∇˜i (K¯i,K¯−i) and ∇¨i (K¯i,K¯−i). These bounds
x˜,t x˜,t ror in control parameters areO(ϵ). Using this result now
show that the approximation error in the stochas-
westatethemainresultofthepaper,presentingthefinite
tic gradient δi := max(∥∇˜i (Ki,K−i) −
t y,t sampleconvergenceboundsoftheAlgorithm1.
∇i (Ki,K−i)∥,∥∇˜i (K¯i,K¯−i)−∇i (Ki,K−i)∥)=
y,t x˜,t x˜,t Theorem4.5. IfallconditionsinTheorem4.4aresatisfied,
O(ϵ)withprobability1−δifthesmoothingradiusr =O(ϵ)
then eK := max ∥Kj − Kj∗∥ = O(ϵ) for a small
and mini-batch size N = Θ(log(1/δ)/ϵ2). Now we t j∈[N] t t
b ϵ>0andt∈{T −1,...,0}.
introducethediagonaldominancecondition.
Assumption4.3(DiagonalDominance). Thediagonaldom- The error eK is due to a combination of the approxi-
inanceconditionentailsthat t
mation error in the stochastic gradient δ at time t and
t
σ(Ri)≥(cid:112) 2m(N −1)γ2 γi the accumulated error in the forward-in-time controllers
t B,t P,t+1 eK ,...,eK . Wefirstcharacterizethesetwoquantities
t+1 T−1
where γ B,t := max i∈[N]∥B ti∥ and γ Pi ,t = ∥P ti∗∥ where andthenshowthatiftheapproximationerrorδ t =O(ϵ)and
matricesPi∗aredefinedin(7). thenumberofinner-loopiterationsK =O(log(1/ϵ)),then
t
theaccumulatederroratanytimet∈{0,...,T −1}never
Thisconditionissimilartoconditionsinthestaticgames exceedsϵscaledbyaconstantmultiplier. Oneimportant
literature(Frihaufetal.,2011)andensuresthatthematri- pointtonoteisthatϵ=O(1/N)tokeeptheapproximation
cesΦ andΦ¯ (Theorem2.3)arediagonallydominantand errorδ small,whichavoidsinstabilityinthealgorithm.
t t t
henceinvertible. IntheAppendix(SectionH)wepropose
acostaugmentationmechanismwhichensuresthediago- 5.NumericalAnalysis
naldominancecondition. TheearlierSystemNoise(SN)
condition(Assumption3.3)in(Hamblyetal.,2023)ishard We first simulate the MRPG algorithm for time horizon
toverifyastheLHSincreaseswithincreasedsystemnoise T = 2, number of teams N = 2, number of agents per
but the RHS may not decrease due to the dependence of teamM =1000andagentshavescalardynamics. Notice
thecostonsystemnoise. IncontrastAssumption4.3isa thatincontrasttoZero-SumGameworksintheliterature
7IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Figure2.NumericalAnalysisofMRPGalgorithm.(left)compari-
sonwithVanillaNaturalPolicyGradient(NPG)andExact-MRPG,
(center)performancewrtdifferentvaluesoflearningrateηi,and
k
(right)mini-batchsizeN .
b
(Jinetal.,2021),weconsideraGeneral-Sumsettingwhere
N =2,whichessentiallycarriesthesamekindofdifficulty
as N > 2 albeit with a different sample complexity. For Figure3.Error convergence for exact versions of MRPG, MF-
MARLandMADPGforN ={3,6,9},T =3andm=p=2.
each time-step the number of inner-loop iterations K =
1000, the mini-batch size N = 5000 and learning rate
b
ηi = 0.001. In Figure 2(left) we first compare the error
k paresthealgorithmsforincreasingnumberofplayers/teams
convergenceintheMRPGalgorithmwithVanillaNatural
N = {3,6,9},T = 3,m = p = 2. For smaller N
PolicyGradient(NPG)whichdoesnotutilizethereceding-
(N = 3,6) MF-MARL shows an offset in error conver-
horizonapproachandtheExact-MRPGwhichusesexact
gencecomparedtoMRPG.ThisisduetothefactthatMF-
naturalpolicygradientsforupdate. Asexpected,theExact-
MARLcomputesamean-fieldequilibriumwhichwillbe
MRPGconvergesverywellforeachtime-stepandMRPG
ϵ-Nash with ϵ → 0 as N gets large. On the other hand,
alsoconvergesalbeitwithvarianceduetonoiseinthepolicy
increasingN isshowntocauseanovershootinerrorconver-
gradients. TheNPGalgorithm,ontheotherhand,isseen
genceofMADPGcomparedtothesteadydecreaseinerror
tobeslowatconvergencewithhighvariance. Thesuperior
forMRPG.Thisisduetothefactthattheerrorinearlier
convergenceisduetothefactthatMRPGdecomposesthe
time-stepsissignificantlyaffectedbyerrorinthelatertime-
problem and solves each problem in retrograde-time. In
steps(HJI).Duetothereceding-horizonnatureofMRPG,
Figure2(center)weevaluateMRPGperformanceforN =
itlearnsbackwards-in-time,thusallowingMRPGtocontrol
1,T = 1 and different values of learning rate ηi. ηi =
k l theerrorinlatertime-steps,andconsequentlyavoidingthe
0.001isshowntoprovidethebestconvergenceproperties
overshootdisplayedbyMADPG.HenceMRPGshowsgood
withfastdecreaseinerror(unlikeslowconvergencewith
performanceforawiderangeofN.
ηi = 0.001) and reliable performance (unlike the high
k
variancewithηi = 0.01). Figure2(right)showsthatthe
k
6.Conclusion
variance of the natural policy gradient update decreases
withincreasingmini-batchsizeN butatthecostofhigher
b ThispaperhasaddressedtheproblemofachievingaNash
samplecomplexity.
equilibriuminaGeneral-SumMean-FieldTypeGame(GS-
In Figure 3 we provide a comparison of MRPG with MFTG)withinaCooperative-Competitive(CC)multi-agent
MADPG(Loweetal.,2017)andMF-MARL(Yangetal., setting.ThepaperhasdevelopedtheMulti-playerReceding-
2018)inCCsetting. NoticethatsinceCCsettingisquite horizon Natural Policy Gradient (MRPG) algorithm. We
novel, direct comparison is only possible with a limited havethenrigorouslyprovenlinearconvergencetotheNash
numberofworks. Weuseexactversionsofallalgorithms equilibrium(NE)oftheMFTG,relaxingtheneedofsystem
allowingfasterconvergence(comparethenumberofiter- noiseconditionsandcovariancematrixestimation,witha
ations in Figures 2 and 3), and extension to data-driven diagonaldominancecondition. Thetheoreticalresultshave
stochasticversionsappeartobestraightforward. TheMF- been verified through numerical analysis and a compari-
MARLalgorithmisalsoareceding-horizonalgorithm(al- sonwithbenchmarkalgorithms(MADPGandMF-MARL),
beit for finite state and action spaces) but assume a large showinggoodconvergenceofMRPGforalargerangeof
numberofcompetingplayers(noticethatMRPGcandeal N. Themainlimitationofthepresentworkisthatinorder
with any number of competing players). The MADPG to have a full analysiswe workedwith a linear-quadratic
algorithm (Lowe et al., 2017) is a variant of the actor- structure. Infuturework,weplantostudymorecomplex
criticmethodwhereeachagenthasanactor-criticandthe CC settings, based on the multi-player receding-horizon
critic is learned in a centralized manner. Figure 3 com- algorithmwehavedevelopedhere.
8IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
ImpactStatement Elie,R.,Perolat,J.,Laurie`re,M.,Geist,M.,andPietquin,
O. Ontheconvergenceofmodelfreelearninginmean
Thispaperpresentsworkwhosegoalistoadvancethefield
fieldgames. InProceedingsoftheAAAIConferenceon
of Machine Learning. There are many potential societal
ArtificialIntelligence,volume34,pp.7143–7150,2020.
consequences of our work, none which we feel must be
specificallyhighlightedhere. Fazel, M., Ge, R., Kakade, S., and Mesbahi, M. Global
convergence of policy gradient methods for the linear
References quadraticregulator. InInternationalconferenceonma-
chinelearning,pp.1467–1476.PMLR,2018.
Agarwal, A., Kakade, S.M., Lee, J.D., andMahajan, G.
On the theory of policy gradient methods: Optimality, Frihauf, P., Krstic, M., and Bas¸ar, T. Nash equilibrium
approximation, and distribution shift. The Journal of seekinginnoncooperativegames. IEEETransactionson
MachineLearningResearch,22(1):4431–4506,2021. AutomaticControl,57(5):1192–1207,2011.
Angiuli,A.,Fouque,J.-P.,andLaurie`re,M. Unifiedrein- Gu,H.,Guo,X.,Wei,X.,andXu,R. Mean-fieldcontrols
forcement Q-learning for mean field game and control withQ-learningforcooperativeMARL:convergenceand
problems. MathematicsofControl,Signals,andSystems, complexityanalysis. SIAMJournalonMathematicsof
34(2):217–271,2022. DataScience,3(4):1168–1196,2021.
Bas¸ar,T.andOlsder,G.J. Dynamicnoncooperativegame
Guo,X.,Hu,A.,Xu,R.,andZhang,J. Learningmean-field
theory. SIAM,1998.
games. InAdvancesinNeuralInformationProcessing
Bu,J.,Ratliff,L.J.,andMesbahi,M.Globalconvergenceof Systems,2019.
policygradientforsequentialzero-sumlinearquadratic
Hambly,B.,Xu,R.,andYang,H. Policygradientmethods
dynamicgames. arXivpreprintarXiv:1911.04672,2019.
findtheNashequilibriuminN-playergeneral-sumlinear-
Cardaliaguet,P.andLehalle,C.-A. Meanfieldgameofcon- quadraticgames. JournalofMachineLearningResearch,
trolsandanapplicationtotradecrowding. Mathematics 24(139),2023.
andFinancialEconomics,12(3):335–363,2018.
Hu,B.,Zhang,K.,Li,N.,Mesbahi,M.,Fazel,M.,andBas¸ar,
Carmona, R., Fouque, J.-P., and Sun, L.-H. Mean field T. Towardatheoreticalfoundationofpolicyoptimization
gamesandsystemicrisk. CommunicationsinMathemati- forlearningcontrolpolicies. AnnualReviewofControl,
calSciences,13(4):911–933,2015. Robotics,andAutonomousSystems,6:123–158,2023.
Carmona,R.,Laurie`re,M.,andTan,Z. Linear-quadratic
Huang, M., Caines, P. E., and Malhame´, R. P. Individ-
mean-fieldreinforcementlearning: convergenceofpol-
ual and mass behaviour in large population stochastic
icygradientmethods. arXivpreprintarXiv:1910.04295,
wirelesspowercontrolproblems: CentralizedandNash
2019.
equilibriumsolutions. InIEEEInternationalConference
onDecisionandControl,volume1,pp.98–103.IEEE,
Carmona, R., Hamidouche, K., Laurie`re, M., and Tan, Z.
2003.
Policyoptimizationforlinear-quadraticzero-summean-
fieldtypegames. In202059thIEEEConferenceonDe-
Huang,M.,Malhame´,R.P.,Caines,P.E.,etal. Largepopu-
cisionandControl(CDC),pp.1038–1043.IEEE,2020.
lationstochasticdynamicgames: Closed-loopMcKean-
Carmona, R., Hamidouche, K., Laurie`re, M., and Tan, Z. VlasovsystemsandtheNashcertaintyequivalenceprin-
Linear-quadraticzero-summean-fieldtypegames: Op- ciple. CommunicationsinInformation&Systems,6(3):
timalityconditionsandpolicyoptimization. Journalof 221–252,2006.
Dynamics&Games,8(4),2021.
Ivanov,I.andLomev,B. Numericalpropertiesofstochas-
Carmona,R.,Laurie`re,M.,andTan,Z. Model-freemean- tic linear quadratic model with applications in finance.
fieldreinforcementlearning: mean-fieldmdpandmean- TOJSAT,2(3):41–46,2012.
fieldq-learning. TheAnnalsofAppliedProbability,33
(6B):5334–5381,2023. Jin, C., Liu, Q., Wang, Y., and Yu, T. V-learning–a sim-
ple,efficient,decentralizedalgorithmformultiagentRL.
Ding,D.,Wei,C.-Y.,Zhang,K.,andJovanovic,M. Inde- arXivpreprintarXiv:2110.14555,2021.
pendentpolicygradientforlarge-scalemarkovpotential
games: Sharperrates,functionapproximation,andgame- Krishna,V.andRamesh,V. Intelligentagentsfornegotia-
agnostic convergence. In International Conference on tionsinmarketgames.i.model. IEEETransactionson
MachineLearning,pp.5166–5220.PMLR,2022. PowerSystems,13(3):1103–1108,1998.
9IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Lasry, J.-M. and Lions, P.-L. Jeux a` champ moyen. i–le Sanjari, S., Saldi, N., and Yu¨ksel, S. Nash equilibria for
casstationnaire. ComptesRendusMathe´matique,343(9): exchangeableteamagainstteamgamesandtheirmean
619–625,2006. fieldlimit. arXivpreprintarXiv:2210.07339,2022.
Laurie`re, M., Perrin, S., Geist, M., and Pietquin, O. Sargent,T.J.andLjungqvist,L. Recursivemacroeconomic
Learning mean field games: A survey. arXiv preprint theory. MassachusetssInstituteofTechnology,2000.
arXiv:2205.12944,2022.
Seber,G.A.andLee,A.J. Linearregressionanalysis. John
Lauriere, M., Perrin, S., Girgin, S., Muller, P., Jain, A., Wiley&Sons,2012.
Cabannes,T.,Piliouras,G.,Pe´rolat,J.,Elie,R.,Pietquin,
Subramanian,J.andMahajan,A.Reinforcementlearningin
O.,etal.Scalabledeepreinforcementlearningalgorithms
stationarymean-fieldgames. InInternationalConference
for mean field games. In International Conference on
onAutonomousAgentsandMultiagentSystems,pp.251–
MachineLearning,pp.12078–12095.PMLR,2022.
259,2019.
Li,Y.,Tang,Y.,Zhang,R.,andLi,N. Distributedreinforce-
Toumi,N.,Malhame´,R.,andLeNy,J. Atractablemean
mentlearningfordecentralizedlinearquadraticcontrol:
field game model for the analysis of crowd evacuation
A derivative-free policy optimization approach. IEEE
dynamics. In202059thIEEEConferenceonDecision
TransactionsonAutomaticControl,2021.
andControl(CDC),pp.1020–1025.IEEE,2020.
Littman, M.L. Markovgamesasaframeworkformulti-
Yang,Y.,Luo,R.,Li,M.,Zhou,M.,Zhang,W.,andWang,
agentreinforcementlearning. InMachineLearningPro-
J. Meanfieldmulti-agentreinforcementlearning. arXiv
ceedings1994,pp.157–163.Elsevier,1994.
preprintarXiv:1802.05438,2018.
Lowe, R., Wu, Y. I., Tamar, A., Harb, J., Pieter Abbeel,
Yang,Z.,Chen,Y.,Hong,M.,andWang,Z.Provablyglobal
O.,andMordatch,I. Multi-agentactor-criticformixed
convergenceofactor-critic: Acaseforlinearquadratic
cooperative-competitiveenvironments. AdvancesinNeu-
regulatorwithergodiccost. InAdvancesinNeuralInfor-
ralInformationProcessingSystems,30,2017.
mationProcessingSystems,pp.8351–8363,2019.
Lussange, J., Lazarevich, I., Bourgeois-Gironde, S.,
Zaman,M.A.U.,Bhatt,S.,andBas¸ar,T.Adversariallinear-
Palminteri,S.,andGutkin,B. Modellingstockmarkets
quadraticmean-fieldgamesovermultigraphs. In2021
bymulti-agentreinforcementlearning. Computational
60thIEEEConferenceonDecisionandControl(CDC),
Economics,57:113–147,2021.
pp.209–214.IEEE,2021.
Malik,D.,Pananjady,A.,Bhatia,K.,Khamaru,K.,Bartlett, Zaman,M.A.U.,Miehling,E.,andBas¸ar,T.Reinforcement
P.,andWainwright,M. Derivative-freemethodsforpol- learningfornon-stationarydiscrete-timelinear–quadratic
icyoptimization: Guaranteesforlinearquadraticsystems. mean-field games in multiple populations. Dynamic
InThe22ndInternationalConferenceonArtificialIntel- GamesandApplications,pp.1–47,2022.
ligenceandStatistics,pp.2916–2925.PMLR,2019.
Zaman, M. A. U., Koppel, A., Bhatt, S., and Bas¸ar, T.
Mao, W., Yang, L., Zhang, K., andBas¸ar, T. Onimprov- Oracle-freereinforcementlearninginmean-fieldgames
ingmodel-freealgorithmsfordecentralizedmulti-agent alongasinglesamplepath. InInternationalConference
reinforcementlearning. InInternationalConferenceon onArtificialIntelligenceandStatistics,pp.10178–10206.
MachineLearning,pp.15007–15049.PMLR,2022. PMLR,2023.
Mazumdar,E.,Ratliff,L.J.,Jordan,M.I.,andSastry,S.S. Zhang, K., Yang, Z., and Bas¸ar, T. Policy optimization
Policy-gradientalgorithmshavenoguaranteesofconver- provably converges to nash equilibria in zero-sum lin-
genceincontinuousactionandstatemulti-agentsettings. ear quadratic games. Advances in Neural Information
arXivpreprintarXiv:1907.03712,2019. ProcessingSystems,32,2019.
Moon,J.,Duncan,T.E.,andBas¸ar,T. Risk-sensitivezero- Zhang,K.,Sun,T.,Tao,Y.,Genc,S.,Mallya,S.,andBas¸ar,
sumdifferentialgames. IEEETransactionsonAutomatic T. Robustmulti-agentreinforcementlearningwithmodel
Control,64(4):1503–1518,2018. uncertainty. Advancesinneuralinformationprocessing
systems,33:10571–10583,2020.
Qu,G.,Yu,C.,Low,S.,andWierman,A. Exploitinglinear
models for model-free nonlinear control: A provably Zhang,K.,Yang,Z.,andBas¸ar,T. Multi-agentreinforce-
convergentpolicygradientapproach. In202160thIEEE mentlearning: Aselectiveoverviewoftheoriesandal-
ConferenceonDecisionandControl(CDC),pp.6539– gorithms. Handbook of Reinforcement Learning and
6546.IEEE,2021. Control,pp.321–384,2021.
10IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Zhang,X.andBas¸ar,T. RevisitingLQRcontrolfromthe
perspectiveofreceding-horizonpolicygradient. IEEE
ControlSystemsLetters,2023.
Zhang, X., Hu, B., and Bas¸ar, T. Learning the Kalman
filterwithfine-grainedsamplecomplexity. arXivpreprint
arXiv:2301.12624,2023.
11IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Appendix
A.AdaptedOpen-LoopAnalysisofGS-MFTG
InthissectionwecharacterizetheAdaptedOpen-LoopNashequilibrium(inshortOLNE(Moonetal.,2018)), which
isshowntosatisfythenecessaryconditionsforNashEquilibrium(Theorem6.1). ThestructureoftheOLNEinspiresa
decompositionwhichsimplifiesthecomputationoftheNashequilibrium(NE)andenablesustoestablishitsexistenceand
uniqueness(Theorem2.3). TherequiredconditionsforthisresultaresimilartothoseofLQGames. Weconsiderasetof
controlsreferredtoasadaptedopen-loopUi (Moonetal.,2018)wherecontrolactionattimet,ui,isadaptedtothenoise
o t
process{ω0,ω ,...,ω0,ω }. IfUi =Ui thenthecorrespondingNEiscalledAdaptedOpen-LoopNashEquilibrium(in
0 0 t t o
shortOLNE).BelowwecharacterizetheOLNEandthenprovethattheOLNEisabroaderclassofNEthantheNE.This
allowsadecompositionoftheGS-MFTGwhichsimplifiesthecharacterizationoftheNE.Thisresultisdistinctfromother
resultsinliteraturesuchasLQ-MFGsandZero-SumMFTGs,duetothegeneral-sumcompetitive-cooperativenatureofthe
problem.
Theorem6.1. (1)AllOLNEpolicies(ui∗) arelinearintheadjointprocesses(pi) (32)and(33),
i∈[N] t i∈[N],0≤t<T
u¯i∗ =−(Li+L¯i)p¯i, ui∗ =−Lipi+L¯ip¯i,
t t t t t t t t t
where∀i∈[N],t∈{0,....T −1}theadjointprocessesdependonsolutionsofRiccatiequations(36)and(39).
(2)Furthermore,theOLNE’sfeedbackrepresentationislinearin(x −x¯ )andx¯ ,
t t t
ui =−LiPi(x −x¯ )−(Li+L¯i)P¯ix¯ , (17)
t t t t t t t t t
∀i∈[N],t∈{0,....T −1}.
(3)Finally,everyOLNEsatisfiesthenecessaryconditionsforNE.
IntheproofwefirstdevelopnecessaryconditionsforOLNEusingtheStochasticMaximumPrincipleandtheseconditions
areshowntobe sufficient duetothequadratic structureofthecost. TheOLNEisthen shownto satisfythenecessary
conditionsforNE.ThisfactcoupledwiththefeedbackstructureofOLNE(17)suggeststhattheNEwillbeafunctionof
thestatedeviationfrommean-field,(x −x¯ ),andthemean-fieldx¯ .
t t t
Proof. WewillusetheStochasticMinimumPrincipletofirstcharacterizethenecessaryconditionsfortheOLNEinterms
ofanadjointprocess. Wethenstudytheadjointprocessandprovidesufficientconditionsforitsexistenceanduniquenessin
termsofsolutionstoasetofRiccatiequations. WethenprovethatthenecessaryconditionsfortheOLNEarealsosufficient
duetothequadraticnatureofthecost. ThisanalysisisageneralizationoftheZero-SumMFTGadaptedopen-loopanalysis
in(Carmonaetal.,2021)totheGeneral-Sumsetting. AdditionallyweshowthatOLNEsatisfiesthenecessaryconditionsof
theNE,byfirstcharacterizingthenecessaryconditionsforNEandthenshowingthateveryOLNEsatisfiestheseconditions.
WefirstcharacterizethenecessaryconditionsforOLNE.LetuswritedowntheexpressionforAdaptedOpen-Loop
NashEquilibrium(OLNE).AnOLNEisatupleofpolicies(ui∗) (whereeachui∗ ∈Ui ismeasurablewithrespectto
i∈[N] o
thenoiseprocess)suchthat,
Ji(ui∗,u−i∗)≤Ji(ui,u−i∗), ∀ui ∈Ui.
o
FirstwefindtheequilibriumconditionoftheOLNE.Letusfirstdefineζ =(x ,x¯ ,(ui,u¯i) )and
t t t t t i∈[N]
N
b (ζ )=(A −I)x +A¯ x¯ +(cid:88) (Biui+B¯iu¯i)=x −x −ω0−ω
t t t t t t t t t t t+1 t t t
j=1
fort∈{0,...,T −1}. Nowweintroducetheadjointprocesspi =(pi) whichisanF-adaptedprocessanddefine-
t 0≤t<T t
hi(ζ ,pi)=b (ζ )⊤pi+ci(ζ )
t t t t t t t t
=b (ζ )⊤pi+(x −x¯ )⊤Qi(x −x¯ )+x¯⊤Q¯ix¯ +(ui−u¯i)⊤Ri(ui−u¯i)+(u¯i)⊤R¯iu¯i
t t t t t t t t t t t t t t t t t t t
Nowweevaluatetheformofderivativesofhi andconvexityofhi withrespectto(ui,u¯i).
t t
12IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Lemma6.2. IfRi ≻0andR¯i ≻0,thenhi isconvexwithrespectto(ui,u¯i)and
t t t
δ hi(ζ ,pi)=(pi)⊤(A −I)+2(x −x¯ )⊤Qi,
xt t t t t t t t t
δ hi(ζ ,pi)=(pi)⊤A¯ −2(x −x¯ )⊤Qi+2x¯⊤Q¯i,
x¯t t t t t t t t t t t
δ hi(ζ ,pi)=(pi)⊤Bi+2(ui−u¯i)⊤Ri,
ui t t t t t t t t
t
δ hi(ζ ,pi)=(pi)⊤Bj, j ̸=i,
uj t t t t t
t
δ hi(ζ ,pi)=(pi)⊤B¯i−2(ui−u¯i)⊤Ri+2(u¯i)⊤R¯i,
u¯i t t t t t t t t t t
t
δ hi(ζ ,pi)=(pi)⊤B¯j, j ̸=i.
u¯j t t t t t
t
Proof. Thepartialderivativesfollowbydirectcomputationandtheconvexityisduetothefactthat
(cid:18) 2Ri −2Ri (cid:19)
δ2 hi(ζ ,pi)= t t ≻0.
(ui,u¯i),(ui,u¯i) t t t −2Ri 2(Ri+R¯i)
t t t t t t t
Thiscompletestheproof.
TheconvexitypropertywillbeusedlatertocharacterizethesufficientconditionsfortheOLNE.Wehypothesizethatthe
adjointprocessforeachplayerfollowsthebackwardsdifferenceequations
pi =E(cid:2) A⊤ pi +A¯⊤ p¯i +2Qi (x −x¯ )+2Q¯i x¯ |F(cid:3) (18)
t t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t
wherep¯i =E[pi|(ω0) ]. ThishypothesiswillshowntobetruewhilecharacterizingthenecessaryconditionsforOLNE.
t t s 0≤s≤t
ThefirststeptowardsobtainingthenecessaryconditionsforOLNEistocomputetheGateauxderivativeofthecostwith
respecttoperturbationinjustthecontrolpolicyoftheithagentitself. TheGateuxderivativeisshowntobeafunctionofthe
adjointprocess.
Lemma6.3. Iftheadjointprocessisdefinedasin(18),thentheGateauxderivativeofJiinthedirectionofβi ∈Ui is
o
T−1
DJi(ui,u−i)(βi,0)=E(cid:88)(cid:2) ((pi)⊤Bi+2(ui−u¯i)⊤Ri+(p¯i)⊤B¯i+2(u¯i)⊤R¯i)⊤βi(cid:3)
t t t t t t t t t t
t=0
Proof. WecomputetheGateauxderivativebyfirstintroducingaperturbationinthecontrolsequenceofagenti,which
resultsintheperturbedcontrolui′. ThentheGateauxderivativeiscomputedbycomparingthecostsundertheperturbed
andunperturbedcontrolsequences.
Letusdenoteforeachi∈[N]thecontrolsequenceui =(ui) andtheperturbedcontrolsequenceui′ =(ui′) ,and
t t≥0 t t≥0
thecorrespondingstateprocessesx andx′ underthesamenoiseprocessandwrite
t t
N
x −x′ =A (x −x′)+A¯ (x¯ −x¯′)+(cid:88)(cid:0) Bi(ui−ui′)+B¯i(u¯i−u¯i′)(cid:1) .
t+1 t+1 t t t t t t t t t t t t
i=1
whereu¯i =E[ui|F0]andu¯i′ =E[ui′|F0]. Letusintroduceζ =(x ,x¯ ,(ui,u¯i) )andζ′ =(x′,x¯′,(ui′,u¯i′) ).
t t t t t t t t t i∈[N] t t t t t i∈[N]
Nextletuscomputethedifferencebetweenthecosts,
T−1 T−1
(cid:88) (ci(ζ′)−ci(ζ ))= (cid:88)(cid:0) ci(ζ′)−ci(ζ )+(b (ζ′)−b (ζ ))⊤pi−(b (ζ′)−b (ζ ))⊤pi(cid:1) ,
t t t t t t t t t t t t t t t t t t
t=0 t=0
T−1
= (cid:88)(cid:0) hi(ζ′,pi)−hi(ζ ,pi)−(x′ −x )⊤pi+(x′ −x )⊤pi(cid:1)
t t t t t t t+1 t+1 t t t t
t=0
T−2
= (cid:88)(cid:0) hi(ζ′,pi)−hi(ζ ,pi)+(x′ −x )⊤(pi −pi)(cid:1) +
t t t t t t t+1 t+1 t+1 t
t=0
hi (ζ′ ,pi )−hi (ζ ,pi )+(x′ −x )⊤pi (19)
T−1 T−1 T−1 T−1 T−1 T−1 T T T−1
13IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Letusdenotetheperturbationinui′astheF-adaptedstochasticprocessβiscaledbyaconstantϵsuchthatui′ =ui+ϵβi
t t
foralli∈[N](consequentlyu¯i′ =u¯i+ϵE[βi|F0]). InordertocomputetheGateauxderivativewedefinetheinfinitesimal
changeinstateprocessandthemean-fieldas
1 1
V = lim (x′ −x ), V¯ = lim (x¯′ −x¯ ) (20)
t ϵ→0 ϵ t t t ϵ→0 ϵ t t
Now we compute the Gateaux derivative of Ji in the direction of βi ∈ Ui. By setting pi = 0 we get the simplified
o T
expression
DJi(ui,u−i)(βi,0)
T−1
=E(cid:88)(cid:2) V⊤ (pi −pi)+δ hi(ζ ,pi)⊤V +δ hi(ζ ,pi)⊤V¯
t+1 t+1 t xt t t t t x¯t t t t t
t=0
+δ hi(ζ ,pi)⊤βi+δ hi(ζ ,pi)⊤β¯i(cid:3)
ui t t t t u¯i t t t t
t t
T−1
=E(cid:88)(cid:2)
V⊤ (pi −pi)+((pi)⊤(A −I)+2(x −x¯ )⊤Qi)⊤V
t+1 t+1 t t t t t t t
t=0
+((pi)⊤A¯ −2(x −x¯ )⊤Qi+2x¯⊤Q¯i)⊤V¯ +((pi)⊤Bi+2(ui−u¯i)⊤Ri)⊤βi
t t t t t t t t t t t t t t
+((pi)⊤B¯i−2(ui−u¯i)⊤Ri+2(u¯i)⊤R¯i)⊤β¯i(cid:3)
. (21)
t t t t t t t t
Nextusingtechniquessimilarto(Carmonaetal.,2020)wededucethat
((pi)⊤A¯ −2(x −x¯ )⊤Qi+2x¯⊤Q¯i)⊤V¯ =((p¯i)⊤A¯ +2x¯⊤Q¯i)⊤V .
t t t t t t t t t t t t t
Usingthedefinitionoftheadjointprocess(18)wecansimplifythefirstthreetermsin(21)tobe
T−1
E(cid:88)(cid:2) V⊤ (pi −pi)+((pi )⊤(A −I)+(p¯i )⊤A¯ +2(x −x¯ )⊤Qi
t+1 t+1 t t+1 t+1 t+1 t+1 t+1 t+1 t+1
t=0
+2x¯⊤ Q¯i )⊤V (cid:3) =0. (22)
t+1 t+1 t+1
Nextusingtechniquessimilarto(Carmonaetal.,2020)wecanalsodeducethat
((pi)⊤B¯i−2(ui−u¯i)⊤Ri+2(u¯i)⊤R¯i)⊤β¯i =((p¯i)⊤B¯i+2(u¯i)⊤R¯i)⊤βi. (23)
t t t t t t t t t t t t t
Using(21)-(23)weobtain
T−1
DJi(ui,u−i)(βi,0)=E(cid:88)(cid:2) ((pi)⊤Bi+2(ui−u¯i)⊤Ri+(p¯i)⊤B¯i+2(u¯i)⊤R¯i)⊤βi(cid:3)
t t t t t t t t t t
t=0
ThenecessaryconditionsforOLNErequiresstationarityoftheGateauxderivative. Nowusingthisconditionandtheform
oftheGateauxderiativewestatethefirstordernecessaryconditionsfortheOLNEtheNEoftheN-playerLQ-MFTGs.
Proposition6.4(OLNENecessaryConditions). Ifthesetofpolicies(ui∗) constitutesOLNE,thenforalli∈[N]and
i∈[N]
t∈{0,...,T −1},
(pi)⊤Bi+2(ui∗−u¯i∗)⊤Ri+(p¯i)⊤B¯i+2(u¯i∗)⊤R¯i =0
t t t t t t t t t
wherepi =E(cid:2) A⊤ pi +A¯⊤ p¯i +2Qi (x −x¯ )+2Q¯i x¯ |F(cid:3) .
t t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t
Proof. Letusfixi∈[N]. ThenusingthenecessaryconditionsforOLNEandLemma6.3,wehave
T−1
DJi(ui,u−i)(βi,0)=E(cid:88)(cid:2) ((pi)⊤Bi+2(ui−u¯i)⊤Ri+(p¯i)⊤B¯i+2(u¯i)⊤R¯i)⊤βi(cid:3)
=0,
t t t t t t t t t t
t=0
forany0 ̸= βi ∈ Ui,asthisstatementhastobetrueforanyperturbationβi eachsummandhastobeequalto0which
resultsinthestatementofthetheorem.
14IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
UsingthenecessaryconditionsforOLNEwenowidentifytheformoftheOLNEpoliciesoftheN-playerLQ-MFTG.
Proposition6.5. TheAdaptedOpen-LoopNashEquilibrium(OLNE)policiesoftheN-playerLQ-MFTGhastheformand
thepreciseexpressiongivenby
ui∗ =−Lipi−L¯ip¯i, u¯i∗ =−(Li+L¯i)p¯i (24)
t t t t t t t t t
where
Li = 1 (Ri)−1(Bi)⊤, L¯i = 1 (Ri)−1(cid:0) Ri(R¯i)−1(Bi+B¯i)⊤−(Bi)⊤(cid:1) (25)
t 2 t t t 2 t t t t t t
Furthermore,thefeedbackrepresentationoftheOLNEis
ui =−LiPi(x −x¯ )−(Li+L¯i)P¯ix¯ , u¯i =−(Li+L¯i)P¯ix¯
t t t t t t t t t t t t t t
wherethematricesPiandP¯iarecomputedrecursivelyusing(27)-(28).
t t
Proof. InthisprooftheformoftheOLNEcontrolpoliciesiscomputedbyutilizingthenecessaryconditionsinProposi-
tion6.4.Thenbyintroducingdeterministicprocessesandreformulatingtheadjointprocessthenecessaryconditionsare
transformedintoasetofforward-backwardsequations(32). Moreover,theseforward-backwardsequationsareshownto
beequivalenttoasetofRiccatiequations(27)-(28). FinallyutilizingtheseRiccatiequationsweformalizethefeedback
representationoftheOLNE.
Letusdefineasetofdeterministicprocesses,
Z0,i =(A⊤+A¯⊤)P¯i+2Q¯i, Z1,i =A⊤Pi+2Qi, (26)
t t t t t t t t t
wherethematricesP¯iandPiaredefinedasfollows:
t t
N
P¯i =(cid:0) (A +A¯ )P¯i +2Q¯i(cid:1)(cid:0) A +A¯ −(cid:88) (Bj +B¯j)(Lj +L¯j)P¯j(cid:1) , (27)
t t t t+1 t t t t t t t t
j=1
N
(cid:88)
Pi =(A⊤Pi +2Qi)(A − BjLjPj). (28)
t t+1 t t t t t
j=1
Nowwegivetheformoftheadjointprocesscompatiblewith(18):
pi =A⊤ pi +A¯⊤ p¯i +2Qi (x −x¯ )+2Q¯i x¯ −Z0,i ω0 −Z1,i ω . (29)
t t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1
UtilizingthenecessaryconditionsforOLNEinProposition6.4andtakingconditionalexpectationE[·|F0]weobtain,
1
u¯i∗ =− (R¯i)−1(Bi+B¯i)⊤p¯i =−(Li+L¯i)p¯i (30)
t 2 t t t t t t t
SubstitutingthisbackintotheOLNEinProposition6.4,weget
ui∗ =−1 (Ri)−1(Bi)⊤pi− 1 (Ri)−1(cid:0) Ri(R¯i)−1(Bi+B¯i)⊤−(Bi)⊤(cid:1) p¯i =−Lipi+L¯ip¯i. (31)
t 2 t t t 2 t t t t t t t t t t t
Substituting(30)and(31)into(3)andrestatingtheadjointprocess,wearriveattheforward-backwardnecessaryconditions
fortheOLNE:
N
x =A x +A¯ x¯ −(cid:88)(cid:0) Bi(Lipi+L¯ip¯i)+B¯i(Li+L¯i)p¯i(cid:1) +ω0 +ω ,
t+1 t t t t t t t t t t t t t t+1 t+1
i=1
pi =A⊤ pi +A¯⊤ p¯i +2Qi (x −x¯ )+2Q¯i x¯ −Z0,i ω0 −Z1,i ω , (32)
t t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1
15IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
andpi =0. TakingconditionalexpectationE[·|F0],weobtain
T
N
x¯ =(A +A¯ )x¯ −(cid:88) (Bi+B¯i)(Li+L¯i)p¯i+ω0 ,
t+1 t t t t t t t t t+1
i=1
p¯i =(A⊤ +A¯⊤ )p¯i +2Q¯i x¯ −Z0,i ω0 , p¯i =0. (33)
t t+1 t+1 t+1 t+1 t+1 t+1 t+1 T
Letusintroducetheansatz
p¯i =P¯ix¯ (34)
t t t
andsubstituteinto(33):
N
x¯ =(cid:0) (A +A¯ )−(cid:88) (Bi+B¯i)(Li+L¯i)P¯i(cid:1) x¯ +ω0 ,
t+1 t t t t t t t t t+1
i=1
P¯ix¯ =(cid:0) (A⊤ +A¯⊤ )P¯i +2Q¯i (cid:1) x¯ −Z0,i ω0 , p¯i =0. (35)
t t t+1 t+1 t+1 t+1 t+1 t+1 t+1 T
Using(35)wearriveattheRiccatiequation,
N
P¯i =(cid:0) (A⊤ +A¯⊤ )P¯i +2Q¯i (cid:1)(cid:0) (A +A¯ )−(cid:88) (Bi+B¯i)(Li+L¯i)P¯i(cid:1) (36)
t t+1 t+1 t+1 t+1 t t t t t t t
i=1
Nextwewritetheforward-backwardequations(32)-(33)intermsofx −x¯ andpi−p¯i:
t t t t
N
(cid:88)
x −x¯ =A (x −x¯ )− BiLi(pi−p¯i)+ω ,
t+1 t+1 t t t t t t t t+1
i=1
pi−p¯i =A⊤ (pi −p¯i )+2Qi (x −x¯ )−Z1,i ω ,pi =0. (37)
t t t+1 t+1 t+1 t+1 t+1 t+1 t+1 t+1 T
Asbefore,letusintroduceanotheransatz
pi−p¯i =Pi(x −x¯ ). (38)
t t t t t
WearriveatthesecondRiccatiequation,
N
(cid:88)
Pi =(A⊤Pi +2Qi)(A − BjLjPj). (39)
t t+1 t t t t t
j=1
Furthermore,using(34)-(38)wecandeducethefeedbackrepresentationoftheOLNE:
ui =−Lipi−L¯ip¯i =−LiPi(x −x¯ )−(Li+L¯i)P¯ix¯ ,
t t t t t t t t t t t t t
u¯i =−(Li+L¯i)p¯i =−(Li+L¯i)P¯ix¯
t t t t t t t t
Thisconcludestheproofoftheproposition.
HavingprovidedthestructureoftheOLNEfollowingfromthenecessaryconditions,andintermsoftheRiccati
equations(36)-(39)wenowprovethatthenecessaryconditionsarealsosufficient. Thisfollowsformtheconvexity,in
particular,thequadraticnatureofthecostfunctionJiwithrespecttoperturbationsincontrolpolicy.
Proposition 6.6 (OLNE Sufficient Condition). If there exists a state process (x ) and an adjoint process
t t≥0
(pi,Z0,i,Z1,i) satisfying (32) and (26), then the control laws given by (24) constitute the OLNE of the N-player
i∈[N]
LQ-MFTG.
16IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Proof. TheproofofthispropositionstartsbyintroducingaperturbationaroundthecandidateOLNEcontrols(i.e. thesetof
controlswhichsatisfytheOLNEnecessaryconditions). Thenusingasecond-orderexpansionofthecostitisshownthatthe
theperturbationaroundthecandidateOLNEcontrolswillalwaysleadtoastrictlyhighercost. Thisisduetothequadratic
natureofthecost. ThisprovesthatthenecessaryconditionsfortheOLNEarealsothesufficientconditionsfortheOLNE.
Letusfirstwriteasecond-orderexpansionforcostJiatcontrolpolicies(ui) ∈Uiand(ui′ =ui+ϵβi) ,βi ∈Ui.
i∈[N] i∈[N]
Weintroducethedeterministicprocesscorrespondingtothecontrolperturbations(βi) ,andletV =(x −x′)/ϵwhere
i∈[N] t t t
(x ) isthestateprocessundercontrolpolicy(ui) and(x′) isthestateprocessundercontrolpolicy(ui′) .
t t≥0 i∈[N] t t≥0 i∈[N]
Then,
N
V =A V +A¯ V¯ +(cid:88) Biβi+B¯iβ¯i
t+1 t t t t t t t t
i=1
Thedifferenceincostsduetothecontrolperturbationis:
Ji(ui′,u−i′)−Ji(ui,u−i)=Ji(ui+ϵβi,u−i+ϵβ−i)−Ji(ui,u−i)
T−1 T−1
=ϵE(cid:88)(cid:2) V⊤(pi −pi)+δ hi(ζ ,pi)ζˇ(cid:3) + 1 ϵ2E(cid:88)(cid:2) δ2 hi(ζ ,pi)ζˇ ·ζˇ(cid:3) (40)
t t+1 t ζt t t t t 2 ζt,ζt t t t t t
t=0 t=0
whereζ =(x ,x¯ ,(ui,u¯i) ),ζ′ =(x′,x¯′,(ui′,u¯i′) )andζˇ =(ζ′−ζ )/ϵ=(V ,V¯,(βi,β¯i) ).Ifweassume
t t t t t i∈[N] t t t t t i∈[N] t t t t t t t i∈[N]
that(ui) = (ui∗) ,meaningthattheysatisfythenecessaryconditionsfortheOLNE(Proposition6.4)andthat
i∈[N] i∈[N]
thereexistsanadjointprocesssuchthat(32)and(26)aresatisfied,thenthefirstordertermsintheabovegivenequation
vanish,thatisE(cid:80)T−1[V⊤(pi −pi)+δ hi(ζ ,pi)ζˇ]=0. Tocomputethelasttermwecomputethesecondderivative
t=0 t t+1 t ζt t t t t
ofhi withrespecttoζ :
t t
δ2 hi(ζ ,pi)=2Qi, δ2 hi(ζ ,pi)=δ2 hi(ζ ,pi)=−2Qi, δ2 hi(ζ ,pi)=2(Qi+Q¯i),
xt,xt t t t t xt,x¯t t t t x¯t,xt t t t t x¯t,x¯t t t t t t
δ2 hi(ζ ,pi)=2Ri, δ2 hi(ζ ,pi)=δ2 hi(ζ ,pi)=−2Ri, δ2 hi(ζ ,pi)=2(Ri+R¯i)
ut,ut t t t t ut,u¯t t t t u¯t,ut t t t t u¯t,u¯t t t t t t
Nowwecharacterizethelasttermin(40):
T−1
Ji(ui′,u−i′)−Ji(ui,u−i)= 1 ϵ2E(cid:88)(cid:2) δ2 hi(ζ ,pi)ζˇ ·ζˇ(cid:3)
2 ζt,ζt t t t t t
t=0
T−1
= 1 ϵ2E(cid:88)(cid:2) (V −V¯)⊤Qi(V −V¯)+V¯⊤Q¯iV¯ +(βi−β¯i)⊤Ri(βi−β¯i)+(β¯i)⊤R¯iβ¯i(cid:3) >0
2 t t t t t t t t t t t t t t t t
t=0
Thereforeifthereexistsastateprocess(x ) andanadjointprocess(pi,Z0,i,Z1,i) satisfying(32)and(26)thenthe
t t≥0 i∈[N]
controllawsgivenby(24)satisfythenecessaryandsufficientconditionsforOLNEoftheN-playerLQ-MFTG.
Finally,weprovethateveryOLNEsatisfiesthenecessaryconditionsforNashEquilibria(NE).Westartbycomputing
theGateauxderivativeofJibutnowwiththecontrolactionsuj,j ̸=ibeingfunctionsofthestateattimet,x . Thiswill
t t
giveusnecessaryconditionsfortheNEsinceintheNE(duetothebackwardsnatureofthediscrete-timeHJIequations)
willhaveafeedbackstructurei.e. theNEcontrolactionsattimetwilldependonstatex . Recalling(19)and(20),wecan
t
17IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
writedown
DJi(ui,u−i)(βi,0)
=ET (cid:88)−1
(cid:2) V⊤ (pi −pi)+δ hi(ζ ,pi)⊤V
+(cid:18) (cid:88)δuj
tδ hi(ζ ,pi)+
δu¯j
tδ hi(ζ
,pi)(cid:19)⊤
V
t+1 t+1 t xt t t t t δx t uj t t t t δx t u¯j t t t t t
t=0 j̸=i
+δ hi(ζ ,pi)⊤V¯
+(cid:18) (cid:88)δuj
tδ hi(ζ ,pi)+
δu¯j
tδ hi(ζ
,pi)(cid:19)⊤
V¯ +δ hi(ζ ,pi)⊤βi
x¯t t t t t δx¯ t uj t t t t δx¯ t u¯j t t t t t ui t t t t t
j̸=i
+δ hi(ζ ,pi)⊤β¯i(cid:3)
u¯i t t t t
t
T−1 (cid:18)
=E(cid:88)(cid:2)
V⊤ (pi −pi)+ (pi)⊤(A −I)+2(x −x¯ )⊤Qi
t+1 t+1 t t t t t t
t=0
+(cid:88)(cid:18) δuj
tBj +
δu¯j tB¯j(cid:19)⊤ pi(cid:19)⊤
V
+(cid:18)
(pi)⊤A¯ −2(x −x¯ )⊤Qi+2x¯⊤Q¯i
δx t δx t t t t t t t t t t
t t
j̸=i
+(cid:88)(cid:18) δuj
tBj +
δu¯j tB¯j(cid:19)⊤ pi(cid:19)⊤
V¯ +((pi)⊤Bi+2(ui−u¯i)⊤Ri)⊤βi
δx¯ t δx¯ t t t t t t t t t
t t
j̸=i
+((pi)⊤B¯i−2(ui−u¯i)⊤Ri+2(u¯i)⊤R¯i)⊤β¯i(cid:3)
.
t t t t t t t t
Nextusingtechniquessimilarto(Carmonaetal.,2020)wededucethat
(cid:18)
(pi)⊤A¯ −2(x −x¯
)⊤Qi+2x¯⊤Q¯i+(cid:88)(cid:18) δuj
tBj +
δu¯j tB¯j(cid:19)⊤ pi(cid:19)⊤
V¯
t t t t t t t δx¯ t δx¯ t t t
t t
j̸=i
=(cid:18)
(p¯i)⊤A¯
+2x¯⊤Q¯i+(cid:88)(cid:18) δuj
tBj +
δu¯j tB¯j(cid:19)⊤ p¯i(cid:19)⊤
V .
t t t t δx¯ t δx¯ t t t
t t
j̸=i
Nowlettheadjointprocesssatisfythefollowingcondition:
pi
=E(cid:20)
A⊤ pi +2Qi(x −x¯
)+(cid:88)(cid:18)δuj
t+1Bj +
δu¯j
t+1B¯j
(cid:19)⊤
pi +A¯⊤ p¯i
t t+1 t+1 t t+1 t+1 δx t+1 δx t+1 t+1 t+1 t+1
t+1 t+1
j̸=i
+2Q¯i t+1x¯ t+1+(cid:88)(cid:18)δ δu x¯j t+1B tj +1+ δ δu x¯¯j t+1B¯ tj +1(cid:19)⊤ p¯i t+1(cid:12) (cid:12) (cid:12) (cid:12)F t(cid:21) . (41)
t+1 t+1
j̸=i
ThentheGateauxderivativewillhavetheform:
T−1
DJi(ui,u−i)(βi,0)=E(cid:88)(cid:2) ((pi)⊤Bi+2(ui−u¯i)⊤Ri+(p¯i)⊤B¯i+2(u¯i)⊤R¯i)⊤βi(cid:3)
,
t t t t t t t t t t
t=0
which,usingthesametechniquesasbefore,willresultin
ui∗ =−Lipi−L¯ip¯i, u¯i∗ =−(Li+L¯i)p¯i
t t t t t t t t t
similartoLemma6.3hencewehaveobtainedthenecessaryconditionsforNE.Noticethatsinceeverysolutionof(18)also
satisfiestherelation(41),everyOLNEsatisfiesthenecessaryconditionsforNE.Thisanalysisissimilartotheanalysis
of(Bas¸ar&Olsder,1998)(Chapter6.2)betweenOpen-loop(OL)andFeedback(FB)NEfordeterministicN-playerLQ
games,butthereithasbeenshownthatOL(non-adapted)andFBNEarenotrelated,thatisonecannotbederivedfromthe
other.
B.ProofofTheorem2.3
Proof. WewillsolvefortheNEoftheGS-MFTGusingthediscrete-timeHamilton-Jacobi-Isaacs(HJI)equations(Bas¸ar&
Olsder,1998). WewillsolvetheproblemoffindingNE(vi∗) . Theprocedureforcomputing(u¯i∗) issimilar,and
i∈[N] i∈[N]
18IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
henceisomitted. Wefirstintroducethefollowingbackwardsrecursiveequations
(cid:0) Ri+(Bi)⊤Zi Bi(cid:1) Ki∗+(Bi)⊤Zi (cid:88) (Bj)⊤Kj∗ =(Bi)⊤Zi A , (42)
t t t+1 t t t t+1 t t t t+1 t
j̸=i
(cid:0) R¯i+(B˜i)⊤Z¯i B˜i(cid:1) K¯i∗+(B˜i)⊤Z¯i (cid:88) (B˜j)⊤K¯j∗ =(B˜i)⊤Z¯i A˜ , i∈[N]
t t t+1 t t t t+1 t t t t+1 t
j̸=i
andthematricesZiaredeterminedasfollows,
t
Zi =F⊤Zi F +(Ki∗)⊤RiKi∗+Qi, Zi =Qi , (43)
t t t+1 t t t t t T T
Z¯i =F¯⊤Z¯i F¯ +(K¯i∗)⊤R¯iK¯i∗+Q¯i, Z¯i =Q¯i , i∈[N]
t t t+1 t t t t t T T
whereF =A −(cid:80)N BiKi∗andF¯ =A˜ −(cid:80)N B˜iK¯i∗. Westartbywritingdownthediscrete-timeHamilton-Jacobi-
t t i=1 t t t t i=1 t t
Isaacs(HJI)equations(Bas¸ar&Olsder,1998)inordertofindthesetofcontrols(vi∗) :
i∈[N]
Vi(y)=minE[gi(f˜i∗(y,vi),vi∗(y),...,vi(y),...,vN∗(y),y)+Vi (f˜i∗(y,vi))|v],
t t t t t t t t+1 t t
vi
t
=E[gi(f˜i∗(y,vi∗),vi∗(y),...,vi∗,...,vN∗(y),y)+Vi (f˜i∗(y,vi∗))|y],
t t t t t t t+1 t t
Vi(y)=0 (44)
T
where
f˜i(y,vi)=f (y,v1∗(y),...,vi,...,vN∗(y)).
t t t t t t
Thedynamicsofthedeviationprocessy anditscorrespondinginstantaneouscostsare
t
N
(cid:88)
f (y ,v1,...,vN)=A y + Bivi+ω ,
t t t t t t t t t+1
i=1
1(cid:16) (cid:17)
gi(y ,v1,...,vN,y )= y⊤ Qi y +(vi)⊤Rivi , gi (y )=0.
t t+1 t t t 2 t+1 t+1 t+1 t t t T T
Noticethatscalingthecostby1/2doesnotchangethenatureoftheproblembutmakestheanalysismorecompact. Hence
startingattimeT −1andusingtheHJIequations(44),weget
Vi (y )= minE[y⊤Qi y +(vi )⊤Ri vi |y ],
T−1 T−1 T T T T−1 T−1 T−1 T−1
vi
T−1
= min[(vi )⊤(Ri +(Bi )⊤Qi Bi )vi
T−1 T−1 T−1 T T−1 T−1
vi
T−1
+2(Bi vi )⊤Qi F˜i y +(F˜i (y ))⊤Qi F˜i (y )|y ]+Tr(Qi Σ) (45)
T−1 T−1 T T−1 T−1 T−1 T−1 T T−1 T−1 T−1 T
whereF˜i(y )=A y +(cid:80) Bjvj∗(y ). Nowdifferentiatingwithrespecttovi,thenecessaryconditionsforNEbecome
t t t t j̸=i t t t t
(cid:88)
−(Ri +(Bi )⊤Qi Bi )vi∗ (y )−(Bi )⊤Qi vj∗ (y )
T−1 T−1 T T−1 T−1 T−1 T−1 T T−1 T−1
j̸=i
=(Bi )⊤Qi A y
T−1 T T−1 T−1
∀i∈[N]. Hence,vi∗ (x )islinearinx ,vi∗ (x )=−Ki∗ x . Hence,weget
T−1 T−1 T−1 T−1 T−1 T−1 T−1
(cid:88)
(Ri +(Bi )⊤Qi Bi )Pi y +(Bi )⊤Qi Pj (y )
T−1 T−1 T T−1 T−1 T−1 T−1 T T−1 T−1
j̸=i
=(Bi )⊤Qi A y
T−1 T T−1 T−1
ThevaluefunctionattimeT −1cannowbecalculatedas:
Vi (x )=y⊤ [F⊤ Qi F +(Ki∗ )⊤Ri Ki∗ ]y +Tr(Qi Σ),
T−1 T−1 T−1 T−1 T T−1 T−1 T−1 T−1 T−1 T
=y⊤ [Zi −Qi ]y +Tr(Qi Σ)
T−1 T−1 T−1 T−1 T
19IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
whereZi =F⊤ZiF +(Ki∗ )⊤Ri Ki∗ +Qi . Nowletustake
T−1 t T t T−1 T−1 T−1 T−1
T
(cid:88)
Vi (y)=y⊤(Zi −Qi )x+ Tr(ZiΣ), i∈[N].
t+1 t+1 t+1 s
s=t+2
UsingtheHJIequations(44),
T
Vi(y )= min[(F˜i(y )+Bivi)⊤Zi (F˜i(y )+Bivi)+(vi)⊤Rivi]+ (cid:88) Tr(ZiΣ)
t t t t t t t+1 t t t t t t t s
vi∈Ui
t t s=t+1
anddifferentiatingwithrespecttoviandusingthenecessaryconditionsofNEweget
t
−(cid:0) Ri+(Bi)⊤Zi Bi(cid:1) vi∗(x )−(Bi)⊤Zi (cid:88) (Bj)⊤vj∗(x )=(Bi)⊤QiA .
t t t+1 t t t t t+1 t t t t t t
j̸=i
Againwecannoticethatvi∗islineariny ,andthuswegetvi∗(x )=−Ki∗y andrecover(42). Usingthisexpressionof
t t t t t t
vi∗thevaluefunctionforagentibecomes
t
T
(cid:88)
Vi(y )=y⊤[F⊤Z F +(Ki∗)⊤RiKi∗]y + Tr(ZiΣ),
t t t t t+1 t t t t t s
s=t+1
T
(cid:88)
=y⊤(Zi−Qi)y + Tr(ZiΣ)
t t t t s
s=t+1
wherethesecondinequalityisobtainedusing(43). Hencewehavecompletedthecharacterizationof(vi∗) ,andthe
i∈[N]
characterizationof(u¯i∗) followssimilartechniques,andhenceisomitted. AsaresulttheNEhasthefollowinglinear
i∈[N]
structure
ui∗(x )=vi∗(y )+u¯i∗(x¯ )=−Ki∗(x −x¯ )−K¯i∗x¯ , i∈[N],t∈{0,....T −1} (46)
t t t t t t t t t t t
such that the matrices (Ki∗,K¯i∗) satisfy (42)-(43). The sufficient condition for existence and uniqueness of
t t 0≤t≤T−1
solutionto(42)canbeobtainedbyconcatenating(42)foralli∈[N]andrequiringthatthematricesΦ andΦ¯ beinvertible,
t t
where
 R1+(B1)⊤Z1 B1 (B1)⊤Z1 B2 ···
t t t+1 t t t+1 t
Φ
t
=

(B t2)⊤Z
.
.
.t2 +1B t1 R t2+(B t2)
.
.
.⊤Z t2 +1B t2 · .· ..· ,
 R¯1+(B˜1)⊤Z¯1 B˜1 (B˜1)⊤Z¯1 B˜2 ···
t t t+1 t t t+1 t
Φ¯
t
=

(B˜ t2)⊤Z¯ t2 +1B¯ t1 R¯ t2+(B˜ t2)⊤Z¯ t2 +1B¯ t2 ··· .
. .
.
. .
.
...
These sufficientconditions aresimilar tothe sufficientconditions forthe N-player LQgames (Corollary6.1 (Bas¸ar&
Olsder,1998)). IncaseΦ andΦ¯ areinvertible,thecontrolmatricesKi∗andK¯i∗canbecomputedas
t t t t
 K1∗  (B1)⊤Q1A   K¯1∗  (B˜1)⊤Q¯1A 
t t t t t t
 . . =Φ−1 . . ,  . . =Φ¯−1 . . .
 .  t  .   .  t  . 
KN∗ (BN)⊤QNA K¯N∗ (B˜N)⊤QNA
t t t t t t
Thiscompletestheproof.
20IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
C.ProofofTheorem2.2
Proof. Forthisproofwewillanalyzetheϵ-Nashpropertyforafixedi∈[N]. Centraltothisanalysisisthequantificationof
thedifferencebetweenthefiniteandinfinitepopulationcostsforagivensetofcontrolpolicies. Firstweexpressthestate
andmean-fieldprocessesintermsofthenoiseprocesses,forthefiniteandinfinitepopulationsettings. Thisthenallows
ustowritethecosts(inbothsettings)asquadraticfunctionsofthenoiseprocess,whichsimplifiesquantificationofthe
differencebetweenthesetwocosts.
Letusfirstconcatenatethestatesof jth agentsinallteamssuchthat xj = [(x1,j)⊤,...,(xN,j)⊤]⊤. Forsimplicityof
t t t
analysisweassumeM =M. Ifforsomei∈[N],M ̸=M thenwecanredefineM =min M . Considerthedynamics
i i i i
ofjointstatexj undertheNEoftheGS-MFTG(Theorem2.3)
N N
xj∗,M =(A∗−(cid:88) BjKj∗)xj∗,M +(A¯∗−(cid:88) B¯jK¯j∗)x¯M∗+ωj +ω0 (47)
t+1 t t t t t t t t t+1 t+1
j=1 j=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
L∗
t
L¯∗
t
wherethesuperscriptM denotesthedynamicsinthefinitepopulationgame(1)-(2)andx¯M∗ = 1 (cid:80) xj∗,M isthe
M j∈[M] t
empiricalmean-field. Wecanalsowritethedynamicsoftheempiricalmean-fieldas
N
x¯M∗ =(cid:0) A +A¯ −(cid:88) (BjKj∗+B¯jK¯j∗)(cid:1) x¯M∗+ 1 (cid:88) ωj +ω0 . (48)
t+1 t t t t t t t M t+1 t+1
j=1 j∈[M]
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
L˜∗ ω¯M
t t+1
Forsimplicityweassumethatxj∗,M = ωj +ω0 whichalsoimpliesthatx¯M∗ = ω¯M. Using(48)wegettherecursive
0 0 0 0 0
definitionofx¯M as
t
t
x¯M∗ =(cid:88) L˜∗ ω¯N, whereL˜∗ :=L˜∗L˜∗ ...L˜∗,ifs≥t.andL˜∗ =I otherwise.
t [t−1,s] s [s,t] s s−1 t [s,t]
s=0
Hencex¯M∗canbecharacterizedasalinearfunctionofthenoiseprocess
t
 I 0 0 ...  ω¯M
x¯M t ∗ =(cid:0) Ψ¯∗ω¯M(cid:1) t, whereΨ¯∗ =      LLL ˜˜˜ ∗ [∗ [∗ [ 210 .
.
.,,, 000 ]]] LL ˜˜ ∗ [∗ [I 21 .
.
.,, 11 ]] L˜∗ [I0 2 .
.
.,2] . .. .. .. .... .       andω¯M =     ω ωω ¯ ¯¯ 2 T10 . . .M MM      
where (M) denotes the tth block of matrix M and the covariance matrix of ω¯M is E[ω¯M(ω¯M)⊤] = diag((Σ/M +
t
Σ0) ). Similarlyusing(47)wecanwrite
0≤t≤T
 I 0 0 ...
L∗ I 0 ...
 [0,0] 
x tj∗,M =(cid:0) Ψ∗ω˜j∗,M(cid:1) t, whereΨ∗ =   LL ∗∗ [1,0] LL ∗∗ [1,1] L∗I . .. ...  

 [2,0] [2,1] [2,2] 
. .
.
. .
.
. .
.
...
and
ωj +ω0 
0 0 0 0
... I 0 0 ... ω¯M
ω˜j∗,M =      ωω ω Tj1 20 j j + + +. . . ω ω ω1 20 T0 0 0     +     L¯ 0 0 .
.
.∗ 0 L¯ 00 .
.
.∗ 1 L¯0 0 .
.
.∗ 2 0 0 0 .
.
.
. . . .. . . ... . .           LL L ˜˜ ˜ ∗ [∗ [ ∗ [ 20 1
.
.
.,, , 00 0 ]] ] LL ˜˜ ∗ [∗ [I 21
.
.
.,, 11 ]] L˜∗ [I0 2
.
.
.,2] . .. .. .. .... .           ω ω ω¯ ¯ ¯0 1 2 T. . .M M M     .
(cid:124) (cid:123)(cid:122) (cid:125)
L∗
21IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Consideringtheinfiniteagentlimitx¯M∗ −M −−→ −∞ →x¯∗andxj∗,M −M −−→ −∞ →x∗,wehave

ω
+ω0
0 0
 ω 00 ω 1+ω 10 
x¯∗
t
=(cid:0) Ψ¯∗ω0(cid:1) t, ω0 =

. .
.


andx∗
t
=(cid:0) Ψ∗ω˜(cid:1) t, ω˜ =  ω 2+
.
ω 20  +L∗ω0
ω0   . .  
T
ω +ω0
T T
wherethecovarianceofω0isE[ω0(ω0)⊤]=diag((Σ0) ). Similarlywecharacterizethedeviationprocessxj∗,M −
0≤t≤T t
x¯M∗,using(47)and(48)
t
M −1 1 (cid:88)
xj∗,M −x¯M∗ =L∗(xj∗,M −x¯M∗)+ ωj + ωk .
t+1 t+1 t t t M t+1 M t+1
k̸=j
(cid:124) (cid:123)(cid:122) (cid:125)
ω¯j,M
t+1
Hence
t
(cid:88)
xj∗,M −x¯M∗ = L∗ ω¯j,M
t t [t−1,s] s
s=0
 ωj   (cid:80) ωk 
0 k̸=j 0
=(cid:0) Ψ∗ω¯j,M(cid:1) , whereω¯j,M = M −1  . . + 1  . . 
t M  .  M  . 
ωj (cid:80) ωk
T−1 k̸=j T−1
wherethecovariancematrixofω¯j,M isE[ω¯j,M(ω¯j,M)⊤]=diag(((M −1)/M ×Σ) ). Similarlytheinfiniteagent
0≤t≤T
limitofthisprocessisx∗−x¯∗ =(Ψ∗ω) whereω =(ω⊤,...,ω⊤ )⊤whosecovarianceisE[ωω⊤]=diag((Σ) ).
t t t 0 T−1 0≤t≤T
Nowwecomputethefiniteagentcostintermsofthenoiseprocesses,
(cid:20) T (cid:21)
1 (cid:88) (cid:88)
Ji (u∗)=E ∥xj∗,M −x¯M∗∥2 +∥x¯M∗∥2 +∥uj∗,M −u¯M∗∥2 +∥u¯M∗∥2
M M t t Qt t Q¯ t t t Rt t R¯ t
j∈[M]t=0
(cid:20) T (cid:21)
1 (cid:88) (cid:88)
=E ∥xj∗,M −x¯M∗∥2 +∥x¯M∗∥2
M t t Qt+(K t∗)⊤RtK t∗ t Q¯ t+(K¯ t∗)⊤R¯ tK¯ t∗
j∈[M]t=0
(cid:20)
=E 1 (cid:88) (Ψ∗ω¯j,M)⊤(cid:0) Q+(K∗)⊤RK∗(cid:1) Ψ∗ω¯j,M
M
j∈[M]
(cid:21)
+(Ψ¯∗ω¯M)⊤(cid:0) Q¯+(K¯∗)⊤R¯K¯∗(cid:1) Ψ¯∗ω¯M
=Tr(cid:0) (Ψ∗)⊤(cid:0) Q+(K∗)⊤RK∗(cid:1) Ψ∗E(cid:2) ω¯j,M(ω¯j,M)⊤(cid:3)(cid:1)
+Tr(cid:0) (Ψ¯∗)⊤(cid:0) Q¯+(K¯∗)⊤R¯K¯∗(cid:1) Ψ¯∗E(cid:2) ω¯M(ω¯M)⊤(cid:3)(cid:1)
whereQ=diag((Q ) ),R=diag((R ) )andK∗ =diag((K∗) )withK∗ =0. Usingasimilartechniquewe
t 0≤T t 0≤T t 0≤T T
cancomputetheinfiniteagentcost:
(cid:20) T (cid:21)
(cid:88)
Ji(u∗)=E ∥x∗−x¯∗∥2 +∥x¯∗∥2 +∥u∗−u¯∗∥2 +∥u¯∗∥2
t t Qt t Q¯
t
t t Rt t R¯
t
t=0
=Tr(cid:0) (Ψ∗)⊤(cid:0) Q+(K∗)⊤RK∗(cid:1) Ψ∗E(cid:2) ωω⊤(cid:3)(cid:1)
+Tr(cid:0) (Ψ¯∗)⊤(cid:0) Q¯+(K¯∗)⊤R¯K¯∗(cid:1) Ψ¯∗E(cid:2) ω¯0(ω¯0)⊤(cid:3)(cid:1)
.
22IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Nowevaluatingthedifferencebetweenthefiniteandinfinitepopulationcosts:
Ji (u∗)−Ji(u∗)
M
=Tr(cid:0) (Ψ∗)⊤(cid:0) Q+(K∗)⊤RK∗(cid:1) Ψ∗(cid:0)E(cid:2) ω¯j,M(ω¯j,M)⊤−E(cid:2) ωω⊤(cid:3)(cid:1)(cid:3)(cid:1)
+Tr(cid:0) (Ψ¯∗)⊤(cid:0) Q¯+(K¯∗)⊤R¯K¯∗(cid:1) Ψ¯∗(cid:0)E(cid:2) ω¯M(ω¯M)⊤(cid:3) −E(cid:2) ω¯0(ω¯0)⊤(cid:3)(cid:1)(cid:1)
≤(cid:16) ∥(Ψ∗)⊤(cid:0) Q+(K∗)⊤RK∗(cid:1) Ψ∗∥ +∥(Ψ¯∗)⊤(cid:0) Q¯+(K¯∗)⊤R¯K¯∗(cid:1) Ψ¯∗∥ (cid:17) Tr(cid:0) diag((Σ/M) )(cid:1)
F F 0≤t≤T
(cid:124) (cid:123)(cid:122) (cid:125)
C1
σT
≤C (49)
1M
whereσ =∥Σ∥ . Nowletusconsiderthesamedynamicsbutundernon-NEcontrols. Thefiniteandinfinitepopulation
F
costsunderthesecontrolsare
Ji (u)=Tr(cid:0) Ψ⊤(cid:0) Q+K⊤RK(cid:1) ΨE(cid:2) ω¯j,M(ω¯j,M)⊤(cid:3)(cid:1) +Tr(cid:0) Ψ¯⊤(cid:0) Q¯+K¯⊤R¯K¯(cid:1) Ψ¯E(cid:2) ω¯M(ω¯M)⊤(cid:3)(cid:1)
M
Ji(u)=Tr(cid:0) Ψ⊤(cid:0) Q+K⊤RK(cid:1) ΨE(cid:2) ωω⊤(cid:3)(cid:1) +Tr(cid:0) Ψ¯⊤(cid:0) Q¯+K¯⊤R¯K¯(cid:1) Ψ¯E(cid:2) ω¯0(ω¯0)⊤(cid:3)(cid:1)
with all matrices defined accordingly. Let us denote the control which infimizes the M agent cost as u˜M, meaning
inf Ji (u)=Ji (u˜). Usingthesametechniquesasbeforeweget
u M M
Ji (u˜)−Ji(u˜)
M
=Tr(cid:16)(cid:0) Ψ˜¯⊤(cid:0) Q¯+K˜¯⊤R¯K˜¯(cid:1) Ψ˜¯ +Ψ⊤(cid:0) Q+K⊤RK(cid:1) Ψ(cid:1)
diag((Σ/M)
)(cid:17)
0≤t≤T
(cid:0) (cid:1)
≥λ (Q )Tr diag((Σ/M) )
min 0 0≤t≤T
σT
=λ (Q ) .
min 0 M
Usingthiswecanfurtherdeduce
σT σT
Ji (u˜)≥Ji(u˜)+λ (Q ) ≥Ji(u∗)+λ (Q ) . (50)
M min 0 M min 0 M
Hencewededuce
σT
Ji (u∗)−infJi (u)=Ji (u∗)−Ji(u∗)+Ji(u∗)−infJi (u)≤(C +λ (Q ))
M u M M u M 1 min 0 M
whichcompletestheproof.
D.ProofofLemma3.1
Proof. We characterize the policy gradient of the cost by first proving the quadratic structure of the cost. For a fixed
t∈{0,...,T −1}andagivensetofcontrollers(Ki,K−i)considerthepartialcost
T (cid:20) (cid:21)
J˜i (Ki,K−i)= (cid:88) E y⊤(cid:0) Qi +(Ki)⊤RiKi(cid:1) y
y,[t′,T] s s s s s s
s=t′
fort≤t′ ≤T andK =0. FromdirectcalculationJ˜i =J˜i . FirstwillshowthatJ˜i hasaquadraticstructure
T y,[t,T] y,t y,[t′,T]
J˜i (Ki,K−i)=E[y⊤Pi y ]+Ni (51)
y,[t′,T] t′ y,t′ t′ y,t′
wherePi isdefinedasin(14)and
y,t
Ni =Ni +Tr(ΣPi ), Ni =0
y,t y,t+1 y,t+1 y,T
23IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
where Σ = diag((Σi) ). The hypothesis is true for the base case since cost J˜i (Ki,K−i) = E[y⊤Qi y ] =
i∈[N] y,[T,T] T T T
E[y⊤Pi y ]+Ni .Nowassumeforagivent′ ∈{t,...,T−1},J˜i (Ki,K−i)=E[y⊤ Pi y ]+Ni .
T y,T T y,T y,[t′+1,T] t′+1 y,t′+1 t′+1 y,t′+1
J˜i (Ki,K−i)=E(cid:2) y⊤(cid:0) Qi +(Ki)⊤Ri Ki(cid:1) y (cid:3) +J˜i ,
y,[t′,T] t′ t′ t′ t′ t′ t′ y,[t′+1,T]
=E(cid:2) y⊤(cid:0) Qi +(Ki)⊤Ri Ki(cid:1) y (cid:3) +E[y⊤ Pi y ]+Ni
t′ t′ t′ t′ t′ t′ t′+1 y,t′+1 t′+1 y,t′+1
(cid:20) (cid:18) N N (cid:19) (cid:21)
=E y⊤ Qi +(Ki)⊤Ri Ki +(cid:0) A −(cid:88) BjKj(cid:1)⊤ Pi (cid:0) A −(cid:88) BjKj(cid:1) y
t′ t′ t′ t′ t′ t′ t′ t′ y,t′+1 t′ t t′ t′
j=1 j=1
+Tr(ΣPi )+Ni
y,t′+1 y,t′+1
=E[y⊤Pi y ]+Ni .
t′ y,t′ t′ y,t′
Hencewehaveshown(51),whichimplies
J˜i (Ki,K−i)=E[y⊤Pi y ]+Ni .
y,t t y,t t y,t
Using(14)wecanalsowritethecostJ˜i (Ki,K−i)intermsofthecontrollerKias
y,t t
(cid:34) (cid:18)
J˜i (Ki,K−i)=Ni +E y⊤ Qi+(Ki)⊤(cid:0) Ri+(Bi)⊤Pi Bi(cid:1) Ki
y,t y,t t t t t t y,t+1 t t
−2(BiKi)⊤Pi (cid:0) Ai−(cid:88) BjKj(cid:1)
t t y,t+1 t t t
j̸=i
(cid:19) (cid:21)
+(cid:0) Ai−(cid:88) BjKj(cid:1)⊤ Pi (cid:0) Ai−(cid:88) BjKj(cid:1) y .
t t t y,t+1 t t t t
j̸=i j̸=i
TakingthederivativewithrespecttoKiandusingthefactthatE[y y⊤]=Σ wecanconcludetheproof:
t t t y
δJ˜ yi ,t(Ki,K−i)
=2(cid:0) (Ri+(Bi)⊤Pi Bi)Ki−(Bi)⊤Pi (cid:0) A −(cid:88) BjKj(cid:1)(cid:1) Σ .
δKi t t y,t+1 t t t y,t+1 t t t y
t j̸=i
E.ProofofLemma4.1
Proof. Weshowthatthegradientdominationproperty(PLcondition)ismuchsimplerthanthoseintheliterature,asthe
receding-horizonapproachobviatestheneedforcomputingtheadvantagefunctionandthecostdifferencelemmaasin(Li
etal.,2021;Fazeletal.,2018;Maliketal.,2019). EspeciallyuponcomparisonwithwithLemma3.9in(Hamblyetal.,
2023)onecanseethatthereisnosummationinthePLconditionandmoreoverthePLconditiondoesnotdependonthe
covariance∥Σ ∥butinsteaditdependson∥Σ ∥whichisaparamterwecanmodify. Letusstartbydefiningthematrix
K∗ y
sequencesPi∗ andNi∗ suchthat
y,t y,t
N N
(cid:88) (cid:88)
Pi∗ =Qi+(Ki∗)TRiKi∗+(A − BjKj∗)⊤Pi∗ (A − BjKj∗),Pi∗ =Qi . (52)
y,t t t t t t t t y,t+1 t t t y,T T
j=1 j=1
and
Ni∗ =Ni∗ +Tr(ΣPi∗ ), Ni∗ =0
y,t y,t+1 y,t+1 y,T
From(51)inproofofLemma3.1weknowthat
J˜i (Ki∗,K−i∗)=E[y⊤Pi∗y ]+Ni∗ andJ˜i (K˜i,K−i∗)=E[y⊤Pi′ y ]+Ni′
y,t t y,t t y,t y,t t y,t t y,t
24IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
whereNi′ =Ni∗ +Tr(ΣPi∗ )=Ni∗and
y,t y,t+1 y,t+1 t
N N
Pi′ =Qi+(Ki)TRiKi+(A −(cid:88) BjKj∗−BiKi)⊤Pi∗ (A −(cid:88) BjKj∗−BiKi) (53)
y,t t t t t t t t t t y,t+1 t t t t t
j̸=i j̸=i
Using(52)and(53)wecandeduce
J˜i (K˜i,K−i∗)−J˜i (Ki∗,K−i∗)=−(J˜i (Ki∗,K−i∗)−J˜i (K˜i,K−i∗))
y,t y,t y,t y,t
N N
=−E(cid:2) y⊤(cid:0) Qi+(Ki∗)⊤RiKi∗+(cid:0) A −(cid:88) BjKj∗(cid:1)⊤ Pi∗ (cid:0) A −(cid:88) BjKj∗(cid:1) −Pi′ (cid:1) y (cid:3)
t t t t t t t t y,t+1 t t t y,t t
j=1 j=1
=−E(cid:2) y⊤(cid:0) Qi+(Ki∗−Ki+Ki)⊤Ri(Ki∗−Ki+Ki)
t t t t t t t t t
+(cid:0) A −(cid:88) BjKj∗−Bi(Ki∗−Ki+Ki)(cid:1)⊤ Pi∗
t t t t t t t y,t+1
j̸=i
(cid:0)
A
−(cid:88) BjKj∗−Bi(Ki∗−Ki+Ki)(cid:1) −Pi′ (cid:1)
y
(cid:3)
t t t t t t t y,t t
j̸=i
=−E(cid:2) y⊤(cid:0) (Ki∗−Ki)⊤(Ri+(Bi)⊤Pi∗ Bi)(Ki∗−Ki)
t t t t t y,t+1 t t t
+2(Ki∗−Ki)⊤(cid:0) RiKi−(Bi)⊤Pi∗ (A −(cid:88) BjKj∗−BiKi)(cid:1)(cid:1) y (cid:3)
t t t t t y,t+1 t t t t t t
j̸=i
Letusdefinethenaturalgradient(Fazeletal.,2018;Maliketal.,2019)as
Ei∗ =∇i (K˜i,K−i∗)Σ−1 =RiKi−(Bi)⊤Pi∗ (A −(cid:88) BjKj∗−BiKi).
t y,t y t t t y,t+1 t t t t t
j̸=i
Then,usingcompletionofsquaresweget
J˜i (K˜i,K−i∗)−J˜i (Ki∗,K−i∗)
y,t y,t
=−E(cid:2) y⊤(cid:0) (Ki∗−Ki)⊤(Ri+(Bi)⊤Pi∗ Bi)(Ki∗−Ki)+2(Ki∗−Ki)⊤Ei∗(cid:1) y (cid:3) (54)
t t t t t y,t+1 t t t t t t t
=−E(cid:2) y⊤(cid:0) (Ki∗−Ki)⊤(Ri+(Bi)⊤Pi∗ Bi)(Ki∗−Ki)+2(Ki∗−Ki)⊤Ei∗
t t t t t y,t+1 t t t t t t
+(Ei∗)⊤(Ri+(Bi)⊤Pi∗ Bi)−1Ei∗−(Ei∗)⊤(Ri+(Bi)⊤Pi∗ Bi)−1Ei∗(cid:1) y (cid:3)
t t t y,t+1 t t t t t y,t+1 t t t
=−E(cid:2) y⊤(cid:0) (Ki∗−Ki+(Ri+(Bi)⊤Pi∗ Bi)−1Ei∗)⊤(Ri+(Bi)⊤Pi∗ Bi)
t t t t t y,t+1 t t t t y,t+1 t
(Ki∗−Ki+(Ri+(Bi)⊤Pi∗ Bi)−1Ei∗)(cid:1) y (cid:3)
t t t t y,t+1 t t t
+E(cid:2) y⊤(cid:0) (Ei∗)⊤(Ri+(Bi)⊤Pi∗ Bi)−1Ei∗(cid:1) y (cid:3)
t t t t y,t+1 t t t
≤E(cid:2) y⊤(cid:0) (Ei∗)⊤(Ri+(Bi)⊤Pi∗ Bi)−1Ei∗(cid:1) y (cid:3)
t t t t y,t+1 t t t
=E(cid:2) Tr(cid:0) y y⊤(cid:0) (Ei∗)⊤(Ri+(Bi)⊤Pi∗ Bi)−1Ei∗(cid:1)(cid:1)(cid:3)
t t t t t y,t+1 t t
∥Σ ∥
≤ y ∥Ei∗∥2 (55)
σ t F
R
∥Σ ∥
≤ y ∥∇i (K˜i,K−i∗)∥2. (56)
σ σ2 y,t F
R y
Thisconcludestheproof.
F.GeneralityofthediagonaldominanceconditionandProofofTheorem4.4
BeforeprovingTheorem4.4weshowthatAssumption4.3(henceforthreferredtoastheSystemNoise(SN)condition)
generalizesAssumption3.3in(Hamblyetal.,2023)fortheone-shotgameT =1. Thefirstthingtonoticeisthattheterm
intheSNcondition(cid:0)ρ¯2T−1(cid:1)2
>1becauseρ¯>1(bydefinition)hencecanbediscarded. Furthermoretheψtermcanalso
ρ¯2−1
bediscardedasitisstrictlypositive. NowsincewehaveT =1andσ =∥Σ ∥=σ duetotherecedinghorizonsetting
X K∗ y
25IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
wecanrewriteAssumption3.3in(Hamblyetal.,2023)as
√ γ2 (max (J˜i (Ki∗,K−i∗))2
σ2 > 20m(N −1) B,0 i y,0
y σ σ
Q R
Thiscanbeequivalentlywrittenas
√ γ2 (max (J˜i (Ki∗,K−i∗))2
σ > 20m(N −1) B,0 i y,0
R σ σ2
Q y
√ γ2 σ2(γi )2
≥ 20m(N −1) B,0 y P,1
σ σ2
Q y
√
≥ 20m(N −1)γ2 γi
B,0 P,1
wherethesecondinequalityisobtainedusingLemma3.11in(Hamblyetal.,2023)andthelastoneisobtainedusingthe
√
(cid:112)
factthatPi =Qi foranyT. Byobservation 20m(N −1)γ2 γi > 2m(N −1)γ2 γi henceifAssumption3.3
T T B,0 P,1 B,0 P,1
in(Hamblyetal.,2023)istruethenAssumption4.3isalsotrueandsoitisaloosercondition.
F.1.PROOFOFTHEOREM4.4
Proof. Weproveforagivent∈{0,...,T −1}thelinearrateofconvergenceofnaturalpolicygradient(16)forthecontrol
policies(Ki) withthesetoffuturecontrollers(Ki) fixed. Thetechniquesforshowingthesameresultfor
t i∈[N] s s>t,i∈[N]
(K¯i) isquitesimilarandhenceisomitted. ThelinearconvergenceresultfollowsduetothePLcondition(Lemma4.1)
t i∈[N]
foragivent∈{0,...,T −1}. Wefirstintroducethesetoflocal-NEpolicies(K˜i∗) whichsatisfytheequation(11)
t i∈[N]
foralli∈[N]. Theseareessentiallythetargetsthatwewant(Ki) toachieve.
t i∈[N]
Firstwedefinethenaturalpolicygradientforagentiifallagentsarefollowingpolicies((K1) ,...,(KN) )
s t≤s≤T−1 s t≤s≤T−1
N
(cid:88)
Ei :=∇i (Ki,K−i)Σ−1 =RiKi−(Bi)⊤Pi (A − BjKj).
t y,t y t t t t+1 t t t
j=1
Thepolicyupdate(16)inAlgorithm1uses∇˜i (Ki,K−i)Σ−1whichisastochasticapproximationofEi. Nowwedefine
y,t y t
anothersetofcontrolsforagiveni∈[N],(Ki,K˜−i∗)wheretheplayeri∈[N]hascontrolpolicy(Ki) butthe
s t≤s≤T−1
otheragentsj ̸= ifollowthelocal-NEonlyattimet,(K˜j∗,Kj ,...,Kj ). Thissetofpolicies(Ki,K˜−i∗)isifall
t t+1 T−1
playersj ̸=isomehowachievedtheirrespectivelocal-NEpoliciesbutplayeriisfollowingpolicyKi. Thissetofpolicies
t
willbeusefulintheanalysisofthealgorithm. Thenaturalpolicygradientunderthissetofpoliciesisgivenas
Ei∗ =∇i (Ki,K˜−i∗)Σ−1 =RiKi−(Bi)⊤Pi (A −BiKi−(cid:88) BjK˜j∗).
t y,t y t t t t+1 t t t t t
j̸=i
Noticethatthepolicyupdate(16)inAlgorithm1isnotanestimateofEi∗. Nowweintroducesomepropertiesofthecost
t
J˜i forthesetofcontrollers(Ki,K˜−i∗)includingthePLcondition
y,t
Lemma6.7. GiventhatKi′ =(Ki′,Ki ,...),thecostfunctionJ˜i satisfiesthefollowingsmoothnessproperty:
t t+1 y,t
J˜i (Ki′,K˜−i∗)−J˜i (Ki,K˜−i∗)
y,t y,t
=Tr[((Ki′−Ki)⊤(Ri+(Bi)⊤Pi Bi)(Ki′−Ki)+2(Ki′−Ki)⊤Ei∗)Σ ]. (57)
t t t t t+1 t t t t t t y
Furthermore, the cost function J˜i (K˜i,K−i∗) also satisfies the PL growth condition with respect to policy gradient
y,t
∇i (K˜i,K−i∗)andnaturalpolicygradientEi∗:
y,t t
σ
J˜i (Ki,K˜−i∗)−J˜i (K˜i∗,K˜−i∗)≤ y∥Ei∗∥2
y,t y,t σ t F
R
1
≤ ∥∇i (Ki,K˜−i∗)∥2
σ σ y,t F
R y
26IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
wherethevaluefunctionmatrixPi isdefinedrecursivelyby
y,t
N N
(cid:88) (cid:88)
Pi =Qi+(Ki)TRiKi+(A − BjKj)⊤Pi (A − BjKj),Pi =Qi . (58)
y,t t t t t t t t y,t+1 t t t y,T T
j=1 j=1
andσ istheminimumeigenvalueofΣ .
y y
TheproofofthisLemmaissimilartothatofLemma4.1andthusomitted. BeforestartingtheproofofTheorem4.4we
statetheboundonlearningratesηiandsmallestsingularvalueσ ofmatrixΣ ,
y y
(cid:18) (cid:19)
1 1
ηi ≤min , (59)
4(cid:13) (cid:13)Ri+(Bi)⊤Pi Bi(cid:13) (cid:13)2 +3(cid:13) (cid:13)Ri+(Bi)⊤Pi Bi(cid:13) (cid:13) 2(cid:13) (cid:13)Ri+(Bi)⊤Pi Bi(cid:13) (cid:13)2 +1
t t t+1 t t t t+1 t t t t+1 t
whereσ istheLSVofmatrices(Ri) . ForsimplicityofanalysiswewillsetΣ =σ I. Theboundonlearning
R t t∈{0,...,T−1} y y
rate ηi is standard in Policy Gradient literature (Fazel et al., 2018; Malik et al., 2019). The update (16) in Algorithm
1 employs stochastic natural policy gradient E˜i := ∇˜i (Ki,K−i)Σ−1 where ∇˜i (Ki,K−i) is the stochastic policy
t y,t y y,t
gradient(15). UsingLemma4.2ifthesmoothingradiusr =O(ϵ)andmini-batchsizeN =Θ(log(1/δ)/ϵ2),wecanobtain
b
theapproximationerrorinstochasticgradientδi :=∥∇˜i (Ki,K−i)−∇i (Ki,K−i)∥=O(ϵ)withprobability1−δ.
t y,t y,t
WedenotetheupdatedcontrollerafteronestepofstochasticnaturalpolicygradientasKi′whichisdefinedasfollows:
Ki′ =Ki−ηiE˜i,
t t t
=Ki−ηi(Ei+∆Ei), where∆Ei =E˜i−Ei,
t t t t t t
=Ki−ηi(Ei∗+∆Ei+∆Ei∗), where∆Ei∗ =Ei−Ei∗ (60)
t t t t t t t
Now we characterize the difference in the costs produced by the update of the controller for agent i from Ki to Ki′
t t
(giventhattheotherplayersfollowtheNEcontrollers(Kj∗) )atthegiventimet. Thefuturecontrollers(Ki)
t j̸=i s s>t,i∈[N]
areassumedtobefixedandthesetofvaluematrices(Pi ) isafunctionofthesetoffuturecontrollers(58). To
t+1 i∈[N]
conservespaceinthefollowinganalysisweusethenotation|||A|||2 :=A⊤Aand|||A|||2 :=A⊤BAformatricesAandB
B
ofappropriatedimensions. UsingLemma6.7weget
J˜i (Ki′,K˜−i∗)−J˜i (Ki,K˜−i∗)
y,t y,t
=σ Tr[(Ki′−Ki)⊤(Ri+(Bi)⊤Pi Bi)(Ki′−Ki)+2(Ki′−Ki)⊤Ei∗)],
y t t t t t+1 t t t t t t
=σ Tr[(ηi)2(Ei∗+∆Ei+∆Ei∗)⊤(Ri+(Bi)⊤Pi Bi)(Ei∗+∆Ei+∆Ei∗)]
y t t t t t t+1 t t t t
−2ηiTr[(Ei∗+∆Ei+∆Ei∗)⊤Ei∗Σ ]
t t t t y
Simplifyingtheexpressionusingthenotations|||A|||2 :=A⊤Aand|||A|||2 :=A⊤BA,
B
J˜i (Ki′,K˜−i∗)−J˜i (Ki,K˜−i∗)
y,t y,t
=σ yTr(cid:2) (ηi)2(cid:0)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)E ti∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2
(Ri+(Bi)⊤Pi
Bi)+2(∆E ti∗+∆E ti)⊤(R ti+(B ti)⊤P ti +1B ti)E ti∗
t t t+1 t
+(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)∆Ei∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2 +2(∆Ei)⊤(Ri+(Bi)⊤Pi Bi)∆Ei∗
t (Ri+(Bi)⊤Pi Bi) t t t t+1 t t
t t t+1 t
+(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)∆E ti(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2
(Ri+(Bi)⊤Pi
Bi)(cid:1)(cid:3) −2ηiσ yTr(cid:2)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)E ti∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2(cid:3) −2ηiTr(cid:2) Σ y(∆E ti+∆E ti∗)⊤E ti∗(cid:3)
t t t+1 t
≤σ
y(cid:2) (ηi)2Tr(cid:0)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)E
ti∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2
(Ri+(Bi)⊤Pi
Bi)(cid:1)
+
(η 2i)2 Tr(cid:0)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)E
ti∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2(cid:1)
t t t+1 t
+2(ηi)2Tr(cid:0)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(∆Ei∗+∆Ei)⊤(Ri+(Bi)⊤Pi Bi)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2(cid:1)
t t t t t+1 t
+
3 (ηi)2Tr(cid:0)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)∆Ei∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2 (cid:1) +3(ηi)2Tr(cid:0)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)∆Ei(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2 (cid:1)(cid:3)
2 t (Ri+(Bi)⊤Pi Bi) t (Ri+(Bi)⊤Pi Bi)
t t t+1 t t t t+1 t
−2ηiσ yTr(cid:0)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)E ti∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2(cid:1) +ηiσ yTr(cid:0)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)E ti∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2(cid:1) +
σηi
Tr(cid:0)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Σ y(∆E ti+∆E ti∗)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2(cid:1)
y
27IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
wherewehaveusedthefactthat2Tr(A⊤B) ≤ Tr(A⊤A)+Tr(B⊤B)toobtaintheinequality. Furtheranalyzingthe
expression
J˜i (Ki′,K˜−i∗)−J˜i (Ki,K˜−i∗)
y,t y,t
≤σ
y(cid:2) (ηi)2Tr((cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)E
ti∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2
(Ri+(Bi)⊤Pi
Bi))
t t t+1 t
+
(ηi)2
Tr((cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ei∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2 )+2(ηi)2Tr((cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(∆Ei∗+∆Ei)⊤(Ri+(Bi)⊤Pi Bi)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2 )
2 t t t t t t+1 t
+
3 (ηi)2Tr((cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)∆Ei∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2 )+3(ηi)2Tr((cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)∆Ei(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2 )(cid:3)
2 t (Ri+(Bi)⊤Pi Bi) t (Ri+(Bi)⊤Pi Bi)
t t t+1 t t t t+1 t
−ηiσ yTr((cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)E ti∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2 )+ηiσ yTr((cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)∆E ti(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2 +(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)∆E ti∗(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)2 )
≤σ y(cid:18) (ηi)2(cid:13) (cid:13)R ti+(B ti)⊤P ti +1B ti(cid:13) (cid:13)2 + (η 2i)2 −ηi(cid:19) Tr(cid:0) (E ti∗)⊤E ti∗(cid:1) (61)
+σ y(cid:16) 4(ηi)2(cid:13) (cid:13)R ti+(B ti)⊤P ti +1B ti(cid:13) (cid:13)2 +3(ηi)2(cid:13) (cid:13)R ti+(B ti)⊤P ti +1B ti(cid:13) (cid:13)+ηi(cid:17)
(cid:0) ∥∆Ei∥2 +∥∆Ei∗∥2(cid:1)
t F t F
Bydefinitionweknowthat
(cid:18) (cid:19)
1 1
ηi ≤min , .
4(cid:13) (cid:13)Ri+(Bi)⊤Pi Bi(cid:13) (cid:13)2 +3(cid:13) (cid:13)Ri+(Bi)⊤Pi Bi(cid:13) (cid:13) 2(cid:13) (cid:13)Ri+(Bi)⊤Pi Bi(cid:13) (cid:13)2 +1
t t t+1 t t t t+1 t t t t+1 t
Thususingthisdefinition(61)canbewrittenas
J˜i (Ki′,K˜−i∗)−J˜i (Ki,K˜−i∗)≤−ηiσ Tr(cid:0) (Ei∗)⊤Ei∗(cid:1) +2ηiσ (cid:0) ∥∆Ei∥2 +∥∆Ei∗∥2(cid:1) (62)
y,t y,t y t t y t F t F
Usingthedefinitionof∆Ei∗wecandeducethat
t
(cid:13) (cid:13)
∥∆E ti∗∥ F =(cid:13) (cid:13) (cid:13)(B ti)⊤P ti +1(cid:88) B tj(K tj −K tj∗)(cid:13) (cid:13)
(cid:13)
≤∥B ti∥ F∥P ti +1∥ F (cid:88) ∥B tj∥ F∥K tj −K tj∗∥ F
j̸=i F j̸=i
(cid:88)
≤γ2γ ∥Kj −Kj∗∥
B P,t+1 t t F
j̸=i
whereγ :=max ∥Bi∥ andγ :=max ∥Pi∥ . Usingtheseboundwecanre-write(62)as
B i∈[N],t>0 t F P,t i∈[N] t F
J˜i (Ki′,K˜−i∗)−J˜i (Ki,K˜−i∗)
y,t y,t
(cid:18) (cid:19)2
≤−ηiσ Tr(cid:0) (Ei∗)⊤Ei∗(cid:1) +2ηiσ ∥∆Ei∥2 +2ηiγ4γ2 σ (cid:88) ∥Kj −Kj∗∥2
y t t y t F B P,t+1 y t t F
j̸=i
(cid:18) (cid:19)2
≤−ηiσ (J˜i (Ki,K˜−i∗)−J˜i (K˜i∗,K˜−i∗))+2ηiσ ∥∆Ei∥2 +2ηiγ4γ2 σ (cid:88) ∥Kj −Kj∗∥
R y,t y,t y t F B P,t+1 y t t F
j̸=i
andthesecondinequalityisduetothegradientdominationconditioninLemma6.7. Nowwecharacterizethecostdifference
J˜i (Ki′,K˜−i∗)−J˜i (K˜i∗,K˜−i∗):
y,t y,t
J˜i (Ki′,K˜−i∗)−J˜i (K˜i∗,K˜−i∗)=J˜i (Ki′,K˜−i∗)−J˜i (Ki,K˜−i∗)+J˜i (Ki,K˜−i∗)−J˜i (K˜i∗,K˜−i∗)
y,t y,t y,t y,t y,t y,t
(cid:18) (cid:19)
≤ 1−ηiσ (cid:0) J˜i (Ki,K˜−i∗)−J˜i (K˜i∗,K˜−i∗)(cid:1) +2ηiσ ∥∆Ei∥2
R y,t y,t y t F
(cid:18) (cid:19)2
+2ηiγ4γ2 σ (cid:88) ∥Kj −K˜j∗∥
B P,t+1 y t t F
j̸=i
(cid:18) (cid:19)
≤ 1−ηiσ (J˜i (Ki,K˜−i∗)−J˜i (K˜i∗,K˜−i∗))+2ηiσ ∥∆Ei∥2
R y,t y,t y t F
+2ηiγ B4γ P2 ,t+1(N −1)(cid:88)
J˜j (Kj,K˜−j∗)−J˜j (K˜j∗,K˜−j∗)
σ y,t y,t
R
j̸=i
28IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
wherethelastinequalityfollowsbyutilizingthesmoothnesscondition(57)inLemma6.7asshownbelow.
(cid:88) J˜j (Kj,K˜−j∗)−J˜j (K˜j∗,K˜−j∗)=(cid:88) Tr[((Kj −K˜j∗)⊤(Rj +(Bj)⊤Pj Bj)(Kj −K˜j∗))Σ ]
y,t y,t t t t t t+1 t t t y
j̸=i j̸=i
≥σ σ (cid:88) ∥Kj −K˜j∗∥2 ≥ σ yσ R
(cid:18)
(cid:88) ∥Kj −K˜j∗∥
(cid:19)2
(63)
y R t t F N −1 t t F
j̸=i j̸=i
Assumethat∥∆Ei∥2 ≤ δ andletusdefineη¯ := max ηi. Nowifwecharacterizethesumofdifferenceofcosts
t F t i∈[N]
(cid:80)N (cid:0) J˜i (Ki′,K˜−i∗)−J˜i (K˜i∗,K˜−i∗)(cid:1) weget
i=1 y,t y,t
N
(cid:88)(cid:0) J˜i (Ki′,K˜−i∗)−J˜i (K˜i∗,K˜−i∗)(cid:1)
y,t y,t
i=1
N (cid:18) (cid:18) (cid:19)(cid:19)
≤(cid:88) 1−ηi σ −2m(N −1) γ4γ2 (cid:0) J˜i (Ki,K˜−i∗)−J˜i (K˜i∗,K˜−i∗)(cid:1) +2ηiσ ∥∆Ei∥2
R σ B P,t+1 y,t y,t y t F
R
i=1
(cid:18) (cid:19) N N
≤ 1−ησ R (cid:88) (J˜i (Ki,K˜−i∗)−J˜i (K˜i∗,K˜−i∗))+2(cid:88) ηiσ ∥∆Ei∥2
2 y,t y,t y t F
i=1 i=1
(cid:18) (cid:19) N
≤ 1−ησ R (cid:88) (J˜i (Ki,K˜−i∗)−J˜i (K˜i∗,K˜−i∗))+2Nη¯2σ δ (64)
2 y,t y,t y t
i=1
where the second inequality is obtained by the diagonal dominance condition σ2 ≥ 2m(N −1)γ4γ2 and η :=
R B P,t+1
min ηi.Nowletusdefineasequenceofcontrollers(Ki,k) fork ={0,1,2,...}suchthatKi,k+1 =Ki,k−ηiE˜i,k
i∈[N] t i∈[N] t t t
whereE˜i,k isthestochasticpolicygradientatiterationk. Using(64)wecanwrite
t
N
(cid:88) J˜i (Ki,k,K˜−i∗)−J˜i (K˜i∗,K˜−i∗)
y,t y,t
i=1
≤(cid:18)
1−ησ
R(cid:19)k (cid:88)N
(J˜i (Ki,0,K˜−i∗)−J˜i (K˜i∗,K˜−i∗))+2Nη¯2σ δ
(cid:88)k (cid:18)
1−η
σ
R
(cid:19)j
2 y,t y,t y t 2σ
y
i=1 j=0
≤(cid:18) 1−ησ R(cid:19)k (cid:88)N
(J˜i (Ki,0,K˜−i∗)−J˜i (K˜i∗,K˜−i∗))+
4Nη¯2σ y2
δ (65)
2 y,t y,t ησ t
R
i=1
Wecanuse(54)and(63)towritedownthefollowinginequalities,
J˜i (Ki,K˜−i∗)−J˜i (K˜i∗,K˜−i∗)≥σ σ ∥Ki−K˜i∗∥2,
y,t y,t y R t t F
J˜i (Ki,K˜−i∗)−J˜i (K˜i∗,K˜−i∗)≤σ (cid:0) (γ +γ2γ )∥Ki−Ki∗∥2 +e¯∥Ki−Ki∗∥ (cid:1) (66)
y,t y,t y R B P,t+1 t t F t t F
TheseinequalitiescanbeutilizedtoupperboundthedifferencebetweenoutputofthealgorithmandtheNashequilibrium
controllers. Using(66)and(65)weget
N
(cid:88) ∥Ki,k−K˜i∗∥2 (67)
t t F
i=1
≤(cid:18) 1−ησ R(cid:19)k (cid:88)N (γ R+γ B2γ P,t+1)∥K ti,0−K ti∗∥2
F
+e¯∥K ti,0−K ti∗∥
F +
4Nη¯2σ
yδ
2 σ ησ2 t
i=1 R R
Henceifk =Θ(cid:0) log(cid:0)1(cid:1)(cid:1) andδ =O(ϵ),then(cid:80)N J˜i (Ki,k,K˜−i∗)−J˜i (K˜i∗,K˜−i∗)=O(ϵ)andusing(57)wecan
ϵ t i=1 y,t y,t
deducethat∥Ki,k−K˜i∗∥2 =O(ϵ)fori∈[N]. ThislinearrateofconvergencecanalsobeprovedforthecostJ˜i using
t t x¯,t
similartechniques,andhenceweomititsproof.
29IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
G.ProofofTheorem4.5
Proof. Definitions: ThroughouttheproofwerefertotheoutputoftheinnerloopofAlgorithm1asthesetofoutputcon-
trollers(Ki) .Intheproofweusetwoothersetsofcontrollersaswell.Thefirstset(Ki∗)
t i∈[N],t∈{0,...,T−1} t i∈[N],t∈{0,...,T−1}
whichdenotestheNEascharacterizedinTheorem2.3. Thesecondsetiscalledthelocal-NE(asinproofofTheorem
4.4)andisdenotedby(K˜i∗) . Theproofquantifiestheerrorbetweentheoutputcontrollers(Ki)
t i∈[N],t∈{0,...,T−1} t i∈[N]
andthecorrespondingNEcontrollers(Ki∗) byutilizingtheintermediatelocal-NEcontrollers(K˜i∗) foreach
t i∈[N] t i∈[N]
time t ∈ {T −1,...,0}. For each t the error is shown to depend on error in future controllers (Ki) and the
s s≥t,i∈[N]
approximationerror∆ introducedbytheNaturalPolicyGradientupdate. If∆ =O(ϵ),thentheerrorbetweentheoutput
t t
andNEcontrollersisshowntobeO(ϵ).
LetusstartbydenotingtheNEvaluefunctionmatricesforagenti∈[N]attimet∈{0,1,...,T},undertheNEcontrol
matrices(Ki∗) byPi∗. TheNEcontrolmatricescanbecharacterizedas:
s i∈[N],s∈{t+1,...,T−1} t
Ki∗ :=argminJ˜i ((Ki,Ki∗ ,...,Ki∗ ),K−i∗)
t y,t t t+1 T−1
Ki
t
n=−(Ri+(Bi)⊤Pi∗ Bi)−1(Bi)⊤Pi∗ Ai∗ (68)
t t t+1 t t t+1 t
whereAi∗ :=A +(cid:80) BjKj∗. TheNEvaluefunctionmatricesarebedefinedrecursivelyusingtheNEcontrollersas
t t j̸=i t t
Pi∗ =Qi+(Ai∗)⊤Pi∗ Ai∗−(Ai∗)⊤Pi∗ Bi(Ri+(Bi)⊤Pi∗ Bi)−1(Bi)⊤Pi∗ Ai∗
t t t t+1 t t t+1 t t t t+1 t t t+1 t
=Qi+(Ai∗)⊤Pi∗ (Ai∗+BiKi∗),
t t t+1 t t t
Pi∗ =Qi (69)
T T
ThesufficientconditionforexistenceanduniquenessofthesetofmatricesKi∗andPi∗isshowninTheorem2.3. Letus
t t
nowdefinetheperturbedvaluesmatricesPi(resultingfromthecontrolmatrices(Ki) ). Attimetletus
t s i∈[N],s∈{t+1,...,T−1}
assumetheexistenceanduniquenessofthetargetcontrolmatrices(K˜i∗) suchthat
t i∈[N]
K˜i∗ :=argminJ˜i (Ki,K˜−i∗)
t y,t
Ki
t
=−(Ri+(Bi)⊤Pi Bi)−1(Bi)⊤Pi A˜i (70)
t t t+1 t t t+1 t
whereK˜j∗ =(K˜j∗,Kj ,...,Kj )andA˜i :=A +(cid:80) BjK˜j∗. Wewillshowtheexistenceanduniquenessofthe
t t+1 T−1 t t j̸=i t t
targetcontrolmatricesgiventhattheinner-loopconvergenceinAlgorithm1(Theorem4.4)iscloseenough. Usingthese
matriceswedefinethevaluefunctionmatrices(P˜i) asfollows
t i∈[N]
(cid:18) N (cid:19)⊤ (cid:18) N (cid:19)
P˜i =Qi+(K˜i∗)⊤RiK˜i∗+ A +(cid:88) BjK˜i∗ Pi A +(cid:88) BiK˜j∗
t t t t t t t t t+1 t t t
j=1 j=1
=Qi+(A˜i)⊤Pi A˜i−(A˜i)⊤Pi Bi(Ri+(Bi)⊤Pi Bi)−1(Bi)⊤Pi A˜i
t t t+1 t t t+1 t t t t+1 t t t+1 t
=Qi+(A˜i)⊤Pi (A˜i+BiK˜i∗)
t t t+1 t t t
P˜i =Qi (71)
T T
Finally we define the perturbed value function matrices Pi which result from the perturbed matrices
t
(Ki) obtainedusingtheNaturalPolicyGradient(16)inAlgorithm1:
s i∈[N],s∈{t+1,...,T−1}
(cid:18) N (cid:19)⊤ (cid:18) N (cid:19)
(cid:88) (cid:88)
Pi =Qi+(Ki)⊤RiKi+ A + BjKj Pi A + BjKj (72)
t t t t t t t t t+1 t t t
j=1 j=1
ThroughoutthisproofweassumethattheoutputoftheinnerloopinAlgorithm1,alsocalledtheoutputmatricesKi,are
t
∆ awayfromthetargetmatricesK˜i∗,suchthat∥Ki−K˜i∗∥≤∆ . Weknowthat,
t t t t t
∥Ki−Ki∗∥≤∥Ki−K˜i∗∥+∥K˜i∗−Ki∗∥≤∆ +∥K˜i∗−Ki∗∥.
t t t t t t t t t
30IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Motivationforproof: Figure4ashowstheflowoftheMRPGalgorithm. ThealgorithmutilizesNaturalPolicyGradient
(NPG)toconvergetotheNEπi∗,∀ifort=T −1,thent=T −2andcontinuesinarecedinghorizonmanner(backwards-
t
in-time). Theorem4.4provesthattheNPGconvergestothelocalNEunderthediagonaldominancecondition4.3. Figure4b
showstwodistinctcostfunctionsofagivenagenti∈[N]inagamewithtimehorizonT =2. LetthecostC˜i(·)bethecost
underMRPGalgorithmattimestept=0,aftertheNEhasbeenapproximatedandfixedattimestept=1i.e.K˜i∗ ≈Ki∗,∀i.
1 1
HencethiscostC˜i(Ki,K−i)=J˜i (cid:0) (Ki,K˜i∗),(K−i,K˜−i∗)(cid:1) ≈J˜i (cid:0) (Ki,Ki∗),(K−i,K−i∗)(cid:1) =:Ci∗(Ki,K−i).On
0 0 y,0 0 1 0 1 y,0 0 1 0 1 0 0
theotherhand,letcostCi(·)bethecostofagentiunderVanillaPGmethodfortimestept = 0,wheretheNEhasnot
been approximated for timestep t = 1, hence Ci(Ki,K−i) = J˜i (cid:0) (Ki,Ki),(K−i,K−i)(cid:1) ̸= Ci∗(Ki,K−i) since
0 0 y,0 0 1 0 1 0 0
Ki ̸= Ki∗,∀i. ForeachcostC˜i orCi NPG((16)Algorithm1)convergestothetarget/minimumofthecost(Theorem
1 1
4.4). Inthefigure4bwedenotetheminimumofCibyKi′andtheminimumofC˜ibyK˜i∗. TheHamilton-Jacobi-Isaacs
0 0
equations(Bas¸ar&Olsder,1998)inthecontextofthistwo-shotLQgamestatesthatKi∗ =argmin Ci∗(K,K−i∗),∀i
0 K 0
whichcoupledwithC˜i ≈Ci∗ =⇒ K˜i ≈Ki∗. HenceMRPGleadstoacloserapproximationoftheNEthanVanillaPG
0 0
algorithms.
(a) (b)
Figure4.(a)MRPGAlgorithmconvergestotheNEusingNaturalPolicyGradient(NPG)startingfromt = T −1andmovingina
recedinghorizonmanner(backwards-in-time).(b)Costfunctions(CiandC˜i)oftheithagentinatwo-shotgame(T =2).Thearrows
denotehowNPGintheprevioustimestept=1convergestothetrueNE.
Nowweobtainanupperboundontheterm∥K˜i∗−Ki∗∥using(68)and(70):
t t
K˜i∗−Ki∗
t t
=−(Ri+(Bi)⊤Pi Bi)−1(Bi)⊤Pi A˜i+(Ri+(Bi)⊤Pi∗ Bi)−1(Bi)⊤Pi∗ Ai∗
t t t+1 t t t+1 t t t t+1 t t t+1 t
=−(Ri+(Bi)⊤Pi Bi)−1(Bi)⊤Pi (A˜i−Ai∗)
t t t+1 t t t+1 t t
−(Ri+(Bi)⊤Pi Bi)−1(Bi)⊤(Pi −Pi∗ )Ai∗
t t t+1 t t t+1 t+1 t
−(cid:0) (Ri+(Bi)⊤Pi Bi)−1−(Ri+(Bi)⊤Pi∗ Bi)−1(cid:1) (Bi)⊤Pi∗ Ai∗
t t t+1 t t t t+1 t t t+1 t
=−(Ri+(Bi)⊤Pi Bi)−1(Bi)⊤Pi (cid:88) Bj(K˜j∗−Kj∗)
t t t+1 t t t+1 t t t
j̸=i
−(Ri+(Bi)⊤Pi Bi)−1(Bi)⊤(Pi −Pi∗ )Ai∗
t t t+1 t t t+1 t+1 t
−(cid:0) (Ri+(Bi)⊤Pi Bi)−1−(Ri+(Bi)⊤Pi∗ Bi)−1(cid:1) (Bi)⊤Pi∗ Ai∗
t t t+1 t t t t+1 t t t+1 t
=−(Ri+(Bi)⊤Pi Bi)−1(Bi)⊤Pi (cid:88) Bj(K˜j∗−Kj∗)
t t t+1 t t t+1 t t t
j̸=i
−(Ri+(Bi)⊤Pi Bi)−1(Bi)⊤(Pi −Pi∗ )Ai∗
t t t+1 t t t+1 t+1 t
+(Ri+(Bi)⊤Pi Bi)−1(Bi)⊤(cid:0) Pi −Pi∗ (cid:1)
t t t+1 t t t+1 t+1
Bi(Ri+(Bi)⊤Pi∗ Bi)−1(Bi)−1(Bi)⊤Pi∗ Ai∗ (73)
t t t t+1 t t t t+1 t
31IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
wherethelastequalityisdueto
−(Ri+(Bi)⊤Pi Bi)−1+(Ri+(Bi)⊤Pi∗ Bi)−1
t t t+1 t t t t+1 t
=−(Ri+(Bi)⊤Pi Bi)−1(Ri+(Bi)⊤Pi∗ Bi)(Ri+(Bi)⊤Pi∗ Bi)−1
t t t+1 t t t t+1 t t t t+1 t
+(Ri+(Bi)⊤Pi Bi)−1(Ri+(Bi)⊤Pi Bi)(Ri+(Bi)⊤Pi∗ Bi)−1
t t t+1 t t t t+1 t t t t+1 t
=(Ri+(Bi)⊤Pi Bi)−1(Bi)⊤(cid:0) Pi −Pi∗ (cid:1) Bi(Ri+(Bi)⊤Pi∗ Bi)−1(Bi)−1
t t t+1 t t t+1 t+1 t t t t+1 t t
Furtheranalyzing(73)weget
(Ri+(Bi)⊤Pi Bi)(K˜i∗−Ki∗)+(Bi)⊤Pi (cid:88) Bj(K˜j∗−Kj∗)
t t t+1 t t t t t+1 t t t
j̸=i
=−(Bi)⊤(Pi −Pi∗ )(Ai∗+BiKi∗)
t t+1 t+1 t t t
 R1+(B1)⊤P1 B1 (B1)⊤P1 B2 ... K˜1∗−K1∗ 
t t t+1 t t t+1 t t t


(B t2)⊤P
.
.
.t2 +1B t1 R t2+(B t2
.
.
.)⊤P t2 +1B t2 . .. ... 

K˜N∗−. .
.
KN∗

t t
(cid:124) (cid:123)(cid:122) (cid:125)
Φ˜
t+1
 (B1)⊤(P1∗ −P1 )(A1∗+B1K1∗) 
t t+1 t+1 t t t
.
= .  (74)
 . 
(BN)⊤(PN∗ −PN )(AN∗+BNKN∗)
t t+1 t+1 t t t
whereΦ˜−1 isguaranteedtoexistasshownbelow. Using(74)wenowobtainanupperboundonmax ∥K˜i∗−Ki∗∥:
t+1 i∈[N] t t
max∥K˜i∗−Ki∗∥≤∥Φ˜−1 ∥∥A∗∥max∥Bj∥ ∥Pj∗ −Pj ∥ (75)
t t t+1 t t ∞ t+1 t+1 ∞
j∈[N] j∈[N]
Wealsodefine
 (B1)⊤(P1 −P1∗ )
t t+1 t+1
Φˆ :=Φ˜ −Φ∗ = . . (cid:0) B1 ... BN(cid:1)
t+1 t+1 t+1  .  t t
(BN)⊤(PN −PN∗)
t t+1 t+1
Nowwecharacterize∥Φ˜−1 ∥:
t+1
∥Φ˜−1 ∥=∥(I+(Φ∗ )−1Φˆ )−1(Φ∗ )−1∥
t+1 t+1 t+1 t+1
∞
≤∥(Φ∗ )−1∥(cid:88) ∥∥(Φ∗ )−1∥k(cid:0) max∥Bj∥2∥Pi −Pj∗ ∥(cid:1)k
t+1 t+1 t t+1 t+1
j∈[N]
k=0
≤2∥(Φ∗ )−1∥≤2c(−1) (76)
t+1 Φ
where the last inequality is possible due to the fact max ∥Pj − Pj∗ ∥ ≤ 1/(2c(−1)c2) where c(−1) =
j∈[N] t+1 t+1 Φ B Φ
max ∥(Φ∗ )−1∥andc =max ∥Bi∥. Hencecombining(75)-(76),
t∈{0,...,T−1} t+1 B i∈[N],t∈{0,...,T−1} t
max∥K˜i∗−Ki∗∥≤c¯ max∥Pj∗ −Pj ∥ (77)
t t 1 t+1 t+1
j∈[N] j∈[N]
where c¯ :=2c(−1)c c∗ andc∗ =∥A∗∥.
1 Φ B A A t
NowwecharacterizethedifferencePi−Pi∗ =Pi−P˜i+P˜i−Pi∗. FirstwecancharacterizeP˜i−Pi∗using(69)and
t t t t t t t t
32IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
(71):
P˜i−Pi∗ =(A˜i)⊤Pi (A˜i+BiK˜i∗)−(Ai∗)⊤Pi∗ (Ai∗+BiKi∗)
t t t t+1 t t t t t+1 t t t
=(A˜i−Ai∗)⊤Pi (A˜i+BiK˜i∗)+(Ai∗)⊤(Pi −Pi∗ )(A˜i+BiK˜i∗)
t t t+1 t t t t t+1 t+1 t t t
+(Ai∗)⊤Pi∗ (A˜i+BiK˜i∗−Ai∗−BiKi∗)
t t+1 t t t t t t
(cid:18) (cid:19)⊤
= (cid:88) Bj(K˜j∗−Kj∗) Pi (A˜i+BiK˜i∗)+(Ai∗)⊤(Pi −Pi∗ )(A˜i+BiK˜i∗)
t t t t+1 t t t t t+1 t+1 t t t
j̸=i
N
+(Ai∗)⊤Pi∗ (cid:88) Bj(K˜j∗−Kj∗) (78)
t t+1 t t t
j=1
Usingthischaracterizationwebound∥P˜i−Pi∗∥usingtheAM-GMinequality
t t
∥P˜i−Pi∗∥
t t
N
≤2∥A∗∥∥Pi∗ ∥c (cid:88) ∥K˜j∗−Kj∗∥
t t+1 B t t
j=1
+(cid:0) ∥A∗∥/2+∥Pi∗ ∥c +∥Ai∗∥/2(cid:1) c
(cid:18) (cid:88)N ∥K˜j∗−Kj∗∥(cid:19)2
+
c4 B(cid:18) (cid:88)N ∥K˜j∗−Kj∗∥(cid:19)4
t t+1 B t B t t 2 t t
j=1 j=1
+(cid:0) ∥A∗∥/2+1/2+∥Ai∗∥/2(cid:1) ∥Pi −Pi∗ ∥2+∥Ai∗∥∥A∗∥∥Pi −Pi∗ ∥
t t t+1 t+1 t t t+1 t+1
N
≤(cid:0) 2c∗c∗c +(c∗/2+c∗c +ci∗/2)c +c4/2(cid:1)(cid:88) ∥K˜j∗−Kj∗∥
A P B A P B A B B t t
(cid:124) (cid:123)(cid:122) (cid:125)j=1
c¯2
+c∗/2+1/2+ci∗/2+ci∗c∗∥Pi −Pi∗ ∥
A A A A t+1 t+1
(cid:124) (cid:123)(cid:122) (cid:125)
c¯3
≤c¯ N max∥K˜j∗−Kj∗∥+c¯ ∥Pi −Pi∗ ∥ (79)
2 t t 3 t+1 t+1
j∈[N]
where ci∗ := max ∥Ai∗∥,c∗ := max ∥Pi∗ ∥,andthelastinequalityispossibleduetothe
A t∈{0,...,T−1} t P i∈[N],t∈{0,...,T−1} t+1
factthat ∥Pi −Pi∗ ∥,∥K˜j∗−Kj∗∥≤1/N. SimilarlyPi−P˜icanbedecomposedusing(71)and(72):
t+1 t+1 t t t t
(cid:18) N (cid:19)⊤ (cid:18) N (cid:19)
Pi−P˜i =(Ki)⊤RiKi+ A +(cid:88) BjKj Pi A +(cid:88) BjKj
t t t t t t t t t+1 t t t
j=1 j=1
(cid:20) (cid:18) N (cid:19)⊤ (cid:18) N (cid:19)(cid:21)
− (K˜i∗)⊤RiK˜i∗+ A +(cid:88) BjK˜i∗ Pi A +(cid:88) BiK˜j∗ .
t t t t t t t+1 t t t
j=1 j=1
33IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Westartbyanalyzingthequadraticformx⊤Pix:
t
(cid:34) (cid:18) N (cid:19)⊤ (cid:18) N (cid:19)(cid:21)
(cid:88) (cid:88)
x⊤Pix=x⊤ Qi+(Ki)⊤RiKi+ A + BjKj Pi A + BjKj x
t t t t t t t t t+1 t t t
j=1 j=1
(cid:20) (cid:18) (cid:19)
=x⊤ (Ki)⊤(cid:0) Ri+(Bi)⊤Pi Bi(cid:1) Ki+2(BiKi)⊤Pi A +(cid:88) BjKj +Qi
t t t t+1 t t t t t+1 t t t t
j̸=i
(cid:18) (cid:19)⊤ (cid:18) (cid:19)(cid:21)
(cid:88) (cid:88)
+ A + BjKj Pi A + BjKj x
t t t t+1 t t t
j̸=i j̸=i
(cid:20) (cid:18) (cid:19)
=x⊤ (Ki)⊤(cid:0) Ri+(Bi)⊤Pi Bi(cid:1) Ki+2(BiKi)⊤Pi A +(cid:88) BjK˜j∗ +Qi
t t t t+1 t t t t t+1 t t t t
j̸=i
(cid:18) (cid:19)⊤ (cid:18) (cid:19)
+2(BiKi)⊤Pi (cid:88) Bj(Kj −K˜j∗)+ A +(cid:88) BjK˜j∗ Pi A +(cid:88) BjK˜j∗
t t t+1 t t t t t t t+1 t t t
j̸=i j̸=i j̸=i
(cid:18) (cid:19)⊤ (cid:18) (cid:19)
+ (cid:88) Bj(Kj −K˜j∗) Pi (cid:88) Bj(Kj −K˜j∗)
t t t t+1 t t t
j̸=i j̸=i
(cid:18) (cid:19) (cid:18) (cid:19)(cid:21)
+2 A +(cid:88) BjK˜j∗ Pi (cid:88) Bj(Kj −K˜j∗) x
t t t t+1 t t t
j̸=i j̸=i
Completingsquaresweget
x⊤Pix (80)
t
(cid:34)
=x⊤ (Ki−K˜i∗)⊤(cid:0) Ri+(Bi)⊤Pi Bi(cid:1) (Ki−K˜i∗)
t t t t t+1 t t t
(cid:18) (cid:19)⊤ (cid:18) (cid:19)
+ A +(cid:88) BjK˜j∗ Pi A +(cid:88) BjK˜j∗ +Qi
t t t t+1 t t t t
j̸=i j̸=i
(cid:18) (cid:19)⊤ (cid:18) (cid:19)
− A +(cid:88) BjK˜j∗ Pi Bi(cid:0) Ri+(Bi)⊤Pi Bi(cid:1)−1 (Bi)⊤Pi A +(cid:88) BjK˜j∗
t t t t+1 t t t t+1 t t t+1 t t t
j̸=i j̸=i
(cid:18) (cid:18) N (cid:19) N (cid:19)⊤ (cid:18) (cid:19)(cid:21)
+ 2 A +(cid:88) BjK˜j∗ +(cid:88) Bj(Kj −K˜j∗) Pi (cid:88) Bj(Kj −K˜j∗ x.
t t t t t t t+1 t t t
j=1 j=1 j̸=i
Nowwetakealookatthequadraticx⊤P˜ix:
t
x⊤P˜ix
t
(cid:20) (cid:18) N (cid:19)⊤ (cid:18) N (cid:19)(cid:21)
=x⊤ Qi+(K˜i∗)⊤RiK˜i∗+ A +(cid:88) BjK˜j∗ Pi A +(cid:88) BjK˜j∗ x
t t t t t t t t+1 t t t
j=1 j=1
(cid:20) (cid:18) (cid:19)
=x⊤ (K˜i∗)⊤(cid:0) Ri+(Bi)⊤Pi Bi(cid:1) K˜i∗+2(BiK˜i∗)⊤Pi A +(cid:88) BjK˜j∗ +Qi
t t t t+1 t t t t t+1 t t t t
j̸=i
(cid:18) (cid:19)⊤ (cid:18) N (cid:19)(cid:21)
+ A +(cid:88) BjK˜j∗ Pi A +(cid:88) BjK˜j∗ x
t t t t+1 t t t
j̸=i j̸=i
(cid:20)(cid:18) (cid:19)⊤ (cid:18) N (cid:19)
=x⊤ A +(cid:88) BjK˜j∗ Pi A +(cid:88) BjK˜j∗ +Qi (81)
t t t t+1 t t t t
j̸=i j̸=i
(cid:18) (cid:19)⊤ (cid:18) (cid:19)(cid:21)
− A +(cid:88) BjK˜j∗ Pi Bi(cid:0) Ri+(Bi)⊤Pi Bi(cid:1)−1 (Bi)⊤Pi A +(cid:88) BjK˜j∗ x.
t t t t+1 t t t t+1 t t t+1 t t t
j̸=i j̸=i
34IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Using(80)and(81),weget
x⊤(Pi−P˜i)x
t t
(cid:34)
=x⊤ (Ki−K˜i∗)⊤(cid:0) Ri+(Bi)⊤Pi Bi(cid:1) (Ki−K˜i∗)
t t t t t+1 t t t
(cid:18) (cid:18) N (cid:19) N (cid:19)⊤ (cid:18) (cid:19)(cid:21)
+ 2 A +(cid:88) BjK˜j∗ +(cid:88) Bj(Kj −K˜j∗) Pi (cid:88) Bj(Kj −K˜j∗ x
t t t t t t t+1 t t t
j=1 j=1 j̸=i
(cid:34)
=x⊤ (Ki−K˜i∗)⊤(cid:0) Ri+(Bi)⊤Pi Bi(cid:1) (Ki−K˜i∗)
t t t t t+1 t t t
(cid:18) (cid:18) N (cid:19) N N (cid:19)⊤
+ 2 A +(cid:88) BjKj∗ +2(cid:88) Bj(K˜j∗−Kj∗)+(cid:88) Bj(Kj −K˜j∗)
t t t t t t t t t
j=1 j=1 j=1
(cid:18) (cid:19)(cid:21)
Pi (cid:88) Bj(Kj −K˜j∗ x
t+1 t t t
j̸=i
Usingthischaracterizationwebound∥Pi−P˜i∥:
t t
∥Pi−P˜i∥
t t
≤(cid:0) ∥Ri+(Bi)⊤Pi∗ Bi∥+∥(Bi)⊤(cid:0) Pi∗ −Pi∗ (cid:1) Bi∥(cid:1) ∥Ki−K˜i∗∥
t t t+1 t t t+1 t+1 t t t
N
+(2ci∗+2c (cid:88)(cid:0) ∥K˜j∗−K∗∥+∥Kj −K˜j∗∥(cid:1)(cid:0) ∥Pi∗ ∥+∥Pi −Pi∗ ∥(cid:1)
A B t t t t t+1 t+1 t+1
j=1
N
c (cid:88) ∥Kj −K˜j∗∥
B t t
j=1
Asbeforeassuming ∥Pi −Pi∗ ∥,∥K˜j∗−Kj∗∥≤1/N,
t+1 t+1 t t
N
∥Pi−P˜i∥≤(cid:0) c¯∗+c2/2+2c2(c∗ +1)+2ci∗(cid:1)(cid:88) ∥Kj −K˜j∗∥
t t B B P A t t
(cid:124) (cid:123)(cid:122) (cid:125)j=1
c¯4
N
+(cid:0) c2/2+ci∗+c (cid:1) ∥Pi −Pi∗ ∥+c (c∗ +1)(cid:88) ∥K˜j∗−Kj∗∥
B A B t+1 t+1 B P t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) j=1
c¯5 c¯6
N
≤c¯ N∆ +c¯ ∥Pi −Pi∗ ∥+c¯ (cid:88) ∥K˜j∗−Kj∗∥ (82)
4 t 5 t+1 t+1 6 t t
j=1
where c¯∗ :=max ∥Ri+(Bi)⊤Pi∗ Bi∥.
i∈[N],t∈{0,...,T−1} t t t+1 t
LetusdefineeK :=max ∥Kj −Kj∗∥,eP :=max ∥Pj −Pj∗∥. Using(77),(79)and(82)weget
t j∈[N] t t t j∈[N] t t
eK ≤c¯ eP +∆
t 1 t+1 t
eP ≤(c¯ +c¯ )NeK +(c¯ +c¯ )eP +c¯ N∆
t 2 6 t 3 5 t+1 4
≤(c¯ (c¯ +c¯ )N +c¯ +c¯ )eP +(c¯ +c¯ +c¯ )N∆
1 2 6 3 5 t+1 2 4 6 t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
c¯7 c¯8
Usingthisrecursivedefinitionwededuce
T−1 T−1
(cid:88) (cid:88)
eP ≤c¯T−teP +c¯ c¯s∆ =c¯ c¯s∆
t 7 T 8 7 t+s 8 7 t+s
s=0 s=0
Henceif∆=O(ϵ),inparticular∆ ≤ϵ/(2c¯ c¯tc¯ T)theneP ≤ϵ/2c¯ andeK ≤ϵfort∈{0,...,T −1}.
t 1 7 8 t 1 t
35IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
H.CostaugmentationtechniqueandO(γ)-Nashequilibrium
Inthissectionwewillprovehoweachagentcanindependentlyuseacost-augmentationtechniquetoensurethesatisfaction
ofthediagonaldominationcondition(Assumption4.3)henceensuringconvergenceoftheMRPGalgorithm. Wealsoprove
thatthiscostaugmentationresultsinanO(γ)-Nashequilibriumwhereγ isthemaxoverthecostaugmentationparameters.
Firstweintroducetheγi-augmentedcostfunctionsasfollows.
T
J¨i (Ki,K−i,γi)=E(cid:104)(cid:88) (1+γi)y⊤(cid:0) Qi +(Ki)⊤RiKi(cid:1) y (cid:105) ,
y,t s s s s s s s
s=t
T
J¨i (Ki,K−i,γi)=E(cid:104)(cid:88) (1+γi)x¯⊤(cid:0) Q¯i +K¯i)⊤R¯iK¯i(cid:1) x¯ (cid:105)
x¯,t s s s s s s s
s=t
Theγi ≥0arechosensuchthattheysatisfythegradientdominationconditionwhichcanbewrittenasfollows.
t
(cid:112)
2m(N −1)γ2 γi
γi ≥ B,t P¨,t+1 −1 (83)
t σ(Ri)
t
Thestochasticgradientoftheaugmentedcostfunctionis,
∇¨i (Ki,K−i)=
m
(cid:88)Nb
J¨i (Kˆi(e ,t),K−i)e
y,t N r2 y,t j j
b
j=1
∇¨i (K¯i,K¯−i)= m
(cid:88)Nb
J¨i (Kˆ¯i(e ,t),K¯−i)e
x˜,t N r2 x˜,t j j
b
j=1
respectively,wheree ∼SpN×mN(r)istheperturbationandKˆi(e,t):=(Ki+e,...,Ki )istheperturbedcontroller
j t T−1
setattimestept. N denotesthemini-batchsizeandrthesmoothingradiusofthestochasticgradient. TheMRPGalgorithm
b
forthisaugmentedcostisverysimilartoAlgorithm1butitutilizestheaugmentedstochasticgradients∇¨i and∇¨i and
y,t x˜,t
alsoaprojectionoperatorProj whereD >0islargeenough.
D
Algorithm2MRPGforaugmentedcostGS-MFTG
1: InitializeKi =0,K¯i =0foralli∈[N],t∈{0,...,T −1}
t t
2: fort=T −1,...,1,0,do
3: fork =1,...,K do
4: NaturalPolicyGradientfori∈[N]
(cid:18) Ki(cid:19) (cid:18)(cid:18) Ki(cid:19) (cid:18) ∇¨i (Ki,K−i)Σ−1(cid:19)(cid:19)
t ←Proj t −ηi y,t y (84)
K¯i D K¯i k ∇¨i (K¯i,K¯−i)Σ−1
t t x˜,t x˜
5: endfor
6: endfor
Sincethediagonaldominanceconditionissatisfieddueto(83),Algorithm2willconvergelinearlytotheNEofthecost
augmentedGS-MFTGgame. NowweprovethattheNEundertheaugmentedcoststructureisO(γ¯)awayfromtheNE
oftheoriginalgame. Inthissectionweonlydealwiththedeviationprocess(y ) (5)-(6)andsimilarguaranteescanbe
t ∀t
obtainedforthemean-fieldprocess(x¯ ) .
t ∀t
Theorem6.8. TheNEundertheaugmentedcoststructuredenotedassetofcontrollers(K¨i∗) suchthat
t ∀i,t
J¨i(K¨i∗,K¨−i∗,γi)≤J¨i(Ki,K¨−i∗,γi), ∀i∈[N],K ∈Rp×m (85)
y y
isanO(γ)-NashequilibriumfortheGS-MFTG(3)-(4).
Ji(K¨i∗,K¨−i∗)≤Ji(K,K¨−i∗)+O(γ), ∀i∈[N],K ∈Rp×m (86)
y y
36IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
whereγ =max γi. AlsothedifferencebetweenNEcostandaugmentedNEcostare
0≤t≤T−1,i∈[N] t
|J¨i(K¨i∗,K¨−i∗,γi)−Ji(Ki∗,K−i∗)|=O(γ).
y y
Proof. Letusfirstdefinetheaugmentedcostfunction
T
J¨i (Ki,K−i,γi)=E(cid:104)(cid:88) (1+γi)y⊤(cid:0) Qi +(Ki)⊤RiKi(cid:1) y (cid:105)
y,t t s s s s s s
s=t
withγi =0withi∈[N]. Usingasimilarargumentthisaugmentedcostcanbewrittendownas,
T
J¨i (Ki,K−i,γi)=E[y⊤P¨iy ]+N¨i
y,t t t t t
whereP¨iandN¨icanbewrittenrecursivelyas,
t t
N N
P¨i =(1+γi)(Qi+(Ki)⊤RiKi)+(A +(cid:88) BjKj)⊤P¨i (A +(cid:88) BjKj), P¨i =Qi
t t t t t t t t t t+1 t t t T T
j=1 j=1
N¨i =N¨i +Tr(ΣP¨i ), N¨i =0 (87)
t t+1 t+1 T
Recallingthenon-augmentedcostcanbewrittendownas,
Ji (Ki,K−i)=E[y⊤Piy ]+Ni
y,t t t t t
wherePiandNicanbewrittenrecursivelyas,
t t
N N
(cid:88) (cid:88)
Pi =(Qi+(Ki)⊤RiKi)+(A + BjKj)⊤Pi (A + BjKj), Pi =Qi
t t t t t t t t t+1 t t t T T
j=1 j=1
Ni =Ni +Tr(ΣPi ), N¨i =0 (88)
t t+1 t+1 T
Thedifferencebetweentheaugmentedandnon-augmentedcostsisasfollows.
|J¨i (Ki,K−i,γi)−Ji (Ki,K−i)|=E[y⊤(P¨i−Pi)y ]+N¨i−Ni =Tr(Σ (P¨i−Pi))+N¨i−Ni (89)
y,t y,t t t t t t t y t t t t
SotoquantifythedifferenceJ¨i (Ki,K−i,γi)−Ji (Ki,K−i)weneedtoupperbound∥P¨i−Pi∥and|N¨i−Ni|. Using
y,t y,t t t t t
(87)and(88)thequantity|N¨i−Ni|is
t t
T−1
|N¨i−Ni|= (cid:88) Tr(Σ(P¨i−Pi)) (90)
t t s s
s=t+1
Thequantity∥P¨i−Pi∥canbeboundedasfollows.
t t
N N
∥P¨i−Pi∥=∥γi(Qi+(Ki)⊤RiKi)+(A +(cid:88) BjKj)⊤(P¨i −Pi )(A +(cid:88) BjKj)∥
t t t t t t t t t t t+1 t+1 t t t
j=1 j=1
N
≤γi(γ +γ ∥Ki∥2)+(γ +(cid:88) γ ∥Kj∥)2∥P¨i −Pi ∥≤γi(γ +γ D2)+(γ +Nγ D)2∥P¨i −Pi ∥
t Q R t A B t t+1 t+1 t Q R A B t+1 t+1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
j=1
c1 c2
(91)
Using(91)wecanrecursivelysay
T−1
∥P¨i−Pi∥≤c (cid:88) cs−tγi +cT−t∥P¨i −Pi∥=O(cid:16) max γi(cid:17) =O(γ) (92)
t t 1 2 s 2 T T s
t≤s≤T−1
s=t
37IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Similarly
|N¨i−Ni|=O(γ) (93)
t t
using(90)andusing(89)witht=0
|J¨i(Ki,K−i,γi)−Ji(Ki,K−i)|=O(γ) (94)
y y
Using(85)and(94)foranyKi ∈R(p×m)×T,
Ji(K¨i∗,K¨−i∗)−J¨i(Ki,K¨−i∗,γi)≤Ji(K¨i∗,K¨−i∗)−J¨i(K¨i∗,K¨−i∗,γi)=O(γ) (95)
y y y y
Nowletusfixi∈[N]anddefineasetofcontrollers(Ki,K¨−i∗)whereKi ∈R(p×m)×T. LetusdefineP˜i,P˜¨i,N˜iandN˜¨i
t t t t
asfollows.
N N
P˜¨i =(1+γi)(Qi+(Ki)⊤RiKi)+(A +BiKi+(cid:88) BjK¨j∗)⊤P˜¨i (A +BiKi+(cid:88) BjK¨j∗), P˜¨i =Qi
t t t t t t t t t t t t+1 t t t t t T T
j̸=i j̸=i
N˜¨i =N˜¨i +Tr(ΣP˜¨i
),
N˜¨i
=0
t t+1 t+1 T
N N
P˜i =(Qi+(Ki)⊤RiKi)+(A +BiKi+(cid:88) BjK¨j∗)⊤P˜i (A +BiKi+(cid:88) BjK¨j∗), P˜i =Qi
t t t t t t t t t t t+1 t t t t t T T
j̸=i j̸=i
N˜i =N˜i +Tr(ΣP˜i ), N˜i =0
t t+1 t+1 T
Usingtheseexpressionswecandeduce
Ji(Ki,K¨−i∗)=J¨i(Ki,K¨−i∗,γi)−Tr(Σ (P˜¨i−P˜i))−N˜¨i+N˜i
y y y 0 0 0 0
≥J¨i(K¨i∗,K¨−i∗,γi)−Tr(Σ (P˜¨i−P˜i))−N˜¨i+N˜i (96)
y y 0 0 0 0
Usinganalysissimilarto(87)-(93)wecandeducethat∥P˜¨i−P˜i∥=O(γ)and|N˜¨i−N˜i|=O(γ).
Using(95)-(96)wecan
0 0 0 0
write
Ji(K¨i∗,K¨−i∗)−Ji(Ki,K¨−i∗)=Ji(K¨i∗,K¨−i∗)−J¨i(Ki,K¨−i∗,γi)+J¨i(K¨i∗,K¨−i∗,γi)−Ji(Ki,K¨−i∗)=O(γ)
y y y y y y
whichresultsin(86).
SimilarlywealsoboundthedifferencebetweenJ¨i (K¨i∗,K¨−i∗,γi)−Ji (Ki∗,K−i∗)where(Ki∗) istheNEcon-
y,t y,t t ∀i,t
trollersand(K¨i∗) aretheNEcontrollersundertheaugmentedcost. Thesematricesaredefinedasfollows,
t ∀i,t
T
J¨i (K¨i∗,K¨−i∗,γi)=E(cid:104)(cid:88) (1+γi)y⊤(cid:0) Qi +(K¨i∗)⊤RiK¨i∗(cid:1) y (cid:105)
y,t t s s s s s s
s=t
withγi =0withi∈[N]. Usingasimilarargumentthisaugmentedcostcanbewrittendownas,
T
J¨i (K¨i∗,K¨−i∗,γi)=E[y⊤P¨i∗y ]+N¨i∗
y,t t t t t
whereP¨iandN¨icanbewrittenrecursivelyas,
t t
P¨i∗ =(1+γi)(Qi+(K¨i∗)⊤RiK¨i∗)+(A¨∗)⊤P¨i∗ A¨∗, P¨i∗ =Qi
t t t t t t t t+1 t T T
N¨i∗ =N¨i∗ +Tr(ΣP¨i∗ ), N¨i∗ =0 (97)
t t+1 t+1 T
whereA¨∗ =A +(cid:80)N BjK¨j∗andK¨i∗istheNEaccordingtotheaugmentedcostdefinedasfollows.
t t j=1 t t t
K¨i∗ =−((1+γi)Ri+(Bi)⊤P¨i∗ Bi)−1(Bi)⊤P¨i∗ A¨i∗
t t t t t+1 t t t+1 t
38IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
whereA¨i∗ =A +(cid:80) BjK¨j∗. Recallingthenon-augmentedcostcanbewrittendownas,
t t j̸=i t t
Ji (Ki∗,K−i∗)=E[y⊤Pi∗y ]+Ni∗
y,t t t t t
wherePi∗andNi∗canbewrittenrecursivelyas,
t t
Pi∗ =(Qi+(Ki∗)⊤RiKi∗)+(A∗)⊤Pi∗ A∗, Pi∗ =Qi
t t t t t t t+1 t T T
Ni∗ =Ni∗ +Tr(ΣPi∗ ), N¨i∗ =0 (98)
t t+1 t+1 T
whereA∗ =A +(cid:80)N BjKj∗andKi∗istheNEaccordingtotheaugmentedcostdefinedasfollows.
t t j=1 t t t
Ki∗ =−(Ri+(Bi)⊤Pi∗ Bi)−1(Bi)⊤Pi∗ Ai∗
t t t t+1 t t t+1 t
where Ai∗ = A +(cid:80) BjKj∗. First we characterize the difference between the NE controllers under the two cost
t t j̸=i t t
functions.
K¨i∗−Ki∗ =−((1+γi)Ri+(Bi)⊤P¨i∗ Bi)−1(Bi)⊤P¨i∗ A¨i∗+(Ri+(Bi)⊤Pi∗ Bi)−1(Bi)⊤Pi∗ Ai∗
t t t t t t+1 t t t+1 t t t t+1 t t t+1 t
=∆¨Ki∗ −∆¨Ki∗
t,1 t,2
where
∆¨Ki∗ =−(cid:2) ((1+γi)Ri+(Bi)⊤P¨i Bi)−1−(Ri+(Bi)⊤P¨i Bi)−1(cid:3) (Bi)⊤P¨i∗ A¨i∗
t,1 t t t t+1 t t t t+1 t t t+1 t
∆¨Ki∗ =−(Ri+(Bi)⊤P¨i Bi)−1(Bi)⊤P¨i∗ A¨i∗+(Ri+(Bi)⊤Pi Bi)−1(Bi)⊤Pi∗ Ai∗
t,2 t t t+1 t t t+1 t t t t+1 t t t+1 t
UsingtheequalityA−1−B−1 =A−1BB−1−A−1AB−1 =A−1(B−A)B−1 foranysetofmatricesAandB whose
inversesexistwecansimplifytheexpression∆¨Ki∗ asfollows.
t,1
∆¨Ki∗ =−(cid:2) ((1+γi)Ri+(Bi)⊤P¨i Bi)−1(γiRi)(Ri+(Bi)⊤P¨i Bi)−1(cid:3) (Bi)⊤P¨i∗ A¨i∗
t,1 t t t t+1 t t t t t t+1 t t t+1 t
Hencethenormofthisexpressioncanbeupperboundedby
∥∆¨Ki∗∥≤γi∥Ri∥∥(Ri+(Bi)⊤P¨i∗ Bi)−1∥∥(Ri+(Bi)⊤Pi∗ Bi)−1∥∥(Bi)⊤P¨i∗ A¨i∗∥
t,1 t t t t t+1 t t t t+1 t t t+1 t
(cid:124) (cid:123)(cid:122) (cid:125)
c¯0
Thenormofthesecondexpression∆¨Ki∗ canbeboundedusingtechniquesfromproofofTheorem4.5.
t,2
∆¨Ki∗ ≤∥Φ¨−1 ∥∥A∗∥γ ∥P¨i∗ −Pi∗ ∥
t,2 t+1 t B t+1 t+1
(cid:124) (cid:123)(cid:122) (cid:125)
c¯1
where
 (1+γ1)R1+(B1)⊤P¨1∗ B1 (B1)⊤P¨1∗ B2 ... (B1)⊤P¨1∗ BN 
t t t t+1 t t t+1 t t t+1 t
 (B2)⊤P¨2∗ B1 (1+γ2)R2+(B2)⊤P¨2∗ B2 ... (B2)⊤P¨2∗ BN 
Φ¨
t+1
=


t
. .
.t+1 t t t
. .
.
t t+1 t
...
t
. .
.t+1 t 


(BN)⊤P¨N∗B1 (BN)⊤P¨N∗B2 ... (1+γN)RN +(BN)⊤P¨N∗BN
t t+1 t t t+1 t t t t t+1 t
Thismatrixisinvertibleifγi arechosensoastosatisfythediagonaldominancecondition. Thedifferencebetweenthe
t
augmentedandnon-augmentedcostsattheircorrespondingNEisasfollows.
J¨i (K¨i∗,K¨−i∗,γi)−Ji (Ki∗,K−i∗)=E[y⊤(P¨i∗−Pi∗)y ]+N¨i∗−Ni∗ =Tr(Σ (P¨i∗−Pi∗))+N¨i∗−Ni∗
y,t y,t t t t t t t y t t t t
(99)
SonowwecharacterizethedifferenceP¨i∗−Pi∗usingthealternateexpressionsforboththematrices.
t t
P¨i∗−Pi∗ =(A¨i∗)⊤P¨i∗ (A¨i∗+BiK¨i∗)−(Ai∗)⊤Pi∗ (Ai∗+BiKi∗)
t t t t+1 t t t t t+1 t t t
39IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
UsingthisexpressionweboundthenormofP¨i∗−Pi∗asfollows.
t t
N
∥P¨i∗−Pi∗∥≤(2γ∗γ∗γ +(γ∗/2+γ∗γ +γi∗/2)γ +γ4/2)(cid:88) ∥K¨j∗−Kj∗∥
t t A P B A P B A B B t t
(cid:124) (cid:123)(cid:122) (cid:125)
j=1
c¯2
+(γ∗/2+1/2+(γi∗)2γ∗/2)∥P¨i∗ −Pi∗ ∥
A A A t+1 t+1
(cid:124) (cid:123)(cid:122) (cid:125)
c¯3
≤(c¯ c¯ N +c¯ )∥P¨i∗ −Pi∗ ∥+c¯ c¯ Nγi (100)
1 2 3 t+1 t+1 0 2 t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
c¯4 c¯5
Using(100)recursivelyweget
T−t−1 (cid:18) (cid:19)
∥P¨i∗−Pi∗∥≤c¯ (cid:88) c¯sγi +c¯T−t∥P¨i∗−Pi∗∥≤c¯ c¯T−t(T −t) max γi =O max γi (101)
t t 5 4 t+s 4 T T 5 4 s+j s
0≤s≤T−t−1 t≤s≤T−1
s=0
Similarly(97)and(98)
|N¨i∗−Ni∗|=O(γ) (102)
t t
using(99)witht=0,(101)and(102)wearriveat
|J¨i(Ki,K−i,γi)−Ji(Ki,K−i)|=O(γ)
y y
I.MRPGalgorithm: Analysiswithsample-pathgradients
Inthissectionwewillshowhowtoutilizethesample-pathsofN teamscomprisingM agentseachtoapproximatethe
expectedcostinstochasticgradientcomputation(15). Thisempiricalcostisreferredtoasthesample-pathcost,andthe
sample-pathcostsforthestochasticprocessesy andx˜ ((10)and(9))aredefinedasbelow.
t t
M T
Jˇi (Ki,K−i)= 1 (cid:88)(cid:88) (yj)⊤(Qi+(Ki)⊤RiKi)yj, (103)
y,t M s t s t s s
j=1s=t
T
Jˇi (K¯i,K¯−i)=(cid:88) (x˜ )⊤(Q¯i+(K¯i)⊤R¯iK¯i)x˜ , (104)
x˜,t s t s t s s
s=t
whereKi,K¯i = 0. Mostliterature(Lietal.,2021;Fazeletal.,2018;Maliketal.,2019)usesthesample-pathcostas
T T
astand-infortheexpectedcosttobeusedinstochasticgradientcomputationasin(15). (Maliketal.,2019)mentions
thatthesample-pathcostisanunbiasedestimatoroftheexpectedcost(underastabilizingcontrolpolicy)andisδclose
to the expected cost, if the length of the sample path isO(log(1/δ)). We show that this intuition although true for the
infinite-horizonergodic-costsetting,doesnotcarryovertothefinite-horizonoreveninfinite-horizondiscounted-costsetting.
Thefinitehorizonsettingpreventsusfromobtainingsamplepathsofarbitrarylengths,whichresultsinanapproximation
error. WeshowinLemma6.9thatthesample-pathcostisanunbiasedbuthighvarianceestimatoroftheexpectedcost. And
usingtheMarkovinequalityweshowhowtousemini-batchsizeN togetagoodestimateofthestochasticgradient.
b
Similaranalysiscanbecarriedoutfortheinfinite-horizondiscounted-costsettingandweprovideanoutlineofthisargument
below. Intheinfinite-horizonergodic-costsettingtheexpectedcostessentiallydependsontheinfinitetailoftheMarkov
chain which achieves a stationary distribution since all stabilizing controllers in stochastic LQ settings ensure unique
stationary distributions. As a result the sample path cost is bound to approach the expected cost as the Markov chain
distributionapproachesthestationarydistribution. Ontheotherhand,intheinfinite-horizondiscountedcostsetting,the
expectedcostdependsontheheadoftheMarkovchain(withaneffectivetimehorizonof1/(1−γ))
Intheinfinite-horizondiscounted-costsettingthecostcannotbewrittendowncleanlyintermsofthestationarydistribution.
Thisisduetothefactthatinthediscountedcostsettingthecostdependsessentiallyontheheadofthesamplepathand
40IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
intheergodiccostsettingthecostdependsonthetailofthesamplepath. Asthetailofthesamplepathsapproachesthe
stationarydistribution(forstabilizingcontrollers),theergodiccostapproachestheexpectedcost.
Nowwepresenttheresultprovingthatinthefinite-horizonsettingthesample-pathisanunbiasedestimatoroftheexpected
costandthesecondmomentofthedifferenceisbounded.
Lemma6.9.
E[Jˇi (Ki,K−i)]=J˜i (Ki,K−i), E[Jˇi (K¯i,K¯−i)]=J˜i (K¯i,K¯−i)
y,t y,t x˜,t x˜,t
E[(Jˇi (Ki,K−i)−J˜i (Ki,K−i))2]≤2Tr(Φ)2
y,t y,t
Proof. ForeaseofexpositioninthisproofwewillconsideransingleagentstochasticLQsystemundersetofcontrollers
K =(K )
t 0≤t≤T−1
(dynamics) x =(A −B K )x +ω =A (t)x +ω ,
t+1 t t t t t K t t
T
(sample-pathcost)
Jˇ(K)=(cid:88)
(x )⊤(Q +(K )⊤R K )x ,
t s s s s s s
s=t
T
(expectedcost)
J˜(K)=E(cid:104)(cid:88)
(x )⊤(Q +(K )⊤R K )x
(cid:105)
t s s s s s s
s=t
wherex ,ω ∼N(0,Σ)andarei.i.d. andK =0. Theresultscanbegeneralizedtothesystems(9)-(12)byconcatenating
0 t T
thecontrollersforallN playersintoajointcontroller. Proofoftheunbiasednessfollowstrivially.
T
E[Jˇ(K)]=E(cid:2)(cid:88) (x )⊤Q (τ)x (cid:3) =J˜(K).
t τ K τ t
τ=t
WefirstrecallstandardresultsfromliteratecharacterizingtheexpectationandvarianceofthequadraticformsofGaussian
randomvariables.
Lemma6.10((Seber&Lee,2012)). Letz ∼N(0,I )andM ∈Rp×pbeasymmetricmatrixthenE[z⊤Mz]=Tr(M)
p
andVar(z⊤Mz)=2∥M∥2.
F
NowweshowthatJˇ isanunbiasedestimatorforJ˜ andalsoboundthesecondmomentofthedifferencebetweenthetwo.
t t
Fromtheaboveequationwecanwrite
t
(cid:88)
x = At−1ω whereAs =A (s)A (s−1)...A (t),∀s≥t≥0andAs =I,∀s<t
t τ τ−1 t K K K t
τ=1
Letusintroduce
 
Σ21 0 0 ··· 0
  A1 1Σ1 2 Σ1 2 0 ··· 0    Σ ω− 21 ω 0 
Ψ=  A2 1Σ1 2 A2 2Σ21 Σ1 2 ··· 0  , ϖ =  . . .  ,
   . . . . . . . . . ... . . .     Σ− ω1 2ω t−1
At 1−1Σ21 At 2−1Σ1 2 At 3−1Σ1 2 ··· Σ1 2
Wecandeducethat(Ψϖ) =x where(Ψϖ) isthet-thblockofΨϖ. WecanusethesequantitiestoexpressthecostJˇ as
t t t t
aquadraticfunctionasfollows
T T
Jˇ(K)=(cid:88)
(x )⊤Q (τ)x
=(cid:88)
(Ψϖ)⊤Q (τ)(Ψϖ) =ϖ⊤Φϖ
t τ K τ τ K τ
τ=t τ=t
41IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
whereΦ=Ψ⊤diag((Q (τ))t )ΨandQ (τ)=Q +(K )⊤R K . Proofofboundedsecondmomentfollowsfrom,
K τ=1 K τ τ τ τ
E[(J˜(K)−J˜(K))2]≤(cid:0)E[Jˇ(K)−J˜(K)](cid:1)2 +Var(Jˇ(K))
t t t t t
=Var(ϖ⊤Φϖ)=2∥Φ∥2 ≤2Tr(Φ)2,
F
usingLemma6.10andthefactthat∥·∥ ≤Tr(·).
F
ThelemmastatesthatthesamplepathcostsJˇi andJˇi areunbiasedestimatesofexpectedcostsJ˜i andJ˜i ,respectively
y,t x˜,t y,t x˜,t
andthesecondmomentsofJˇi andJˇi arebounded. Noticethattheboundonsecondmomentcanbeconvertedintoa
y,t x˜,t
uniformboundbyensuringauniformboundonthecostofcontrollers(Ki,K−i). Thesample-pathcostcanbeusedto
obtainsample-pathpolicygradientsasfollows. Wedenotethesesample-pathgradientswithrespecttocontrollerK as
i,t
∇˜ Jˇi (T)suchthat
Ki,t K
∇ˇi (Ki,K−i)=
m
(cid:88)Nb
Jˇi (Kˆi(e ,t),K−i)e ,e ∼SpN×mN(r),
y,t N r2 y,t j j j
b
j=1
∇ˇi (K¯i,K¯−i)= m
(cid:88)Nb
Jˇi (Kˆ¯i(e ,t),K¯−i)e ,e ∼SpN×mN(r),
x˜,t N r2 x˜,t j j j
b
j=1
respectively, whereKˆi(e,t) := (Ki +e,...,Ki )andKˆ¯i(e,t) := (K¯i +e,...,K¯i )arethecontrollersetswith
t T−1 t T−1
perturbationseattimestepst.
Algorithm3SP-MRPGforGS-MFTG
1: InitializeKi =0,K¯i =0foralli∈[N]
t t
2: fort=T,T −1,...,1,0,do
3: fork =1,...,K do
4: GradientDescent
(cid:18) Ki(cid:19) (cid:18) Ki(cid:19) (cid:18) ∇ˇi (Ki,K−i)(cid:19)
t ← t −ηi y,t (105)
K¯i K¯i k ∇ˇi (K¯i,K¯−i)
t t x˜,t
5: endfor
6: endfor
NowweintroducetheMRPGalgorithmwhichusessample-pathpolicygradientsinsteadofstochasticpolicygradients(15).
ThisalgorithmwillbecalledSP-MRPGinshort. UsingLemma6.9wecanboundthedifferencebetweenthestochastic
gradient∇˜ J˜i(K)andthesample-pathgradient∇˜ Jˇi(K ). Thefollowinglemmastatesthefactthat∇˜ Jˇi(K )is
Ki,t Ki,t t t Ki,t t t
anunbiasedestimatorof∇˜ J˜i(K)andboundsthesecondmomentofthedifferencebetweenthetwo. Thetechniques
Ki,t
usedtoprovethisLemmaaresimilarto(Fazeletal.,2018;Maliketal.,2019)henceareomitted.
Lemma6.11.
E[∇ˇi (Ki,K−i)]=∇˜i (Ki,K−i),
y,t y,t
2m2
E[|∇ˇi (Ki,K−i)−∇˜i (Ki,K−i)|]≤ Tr(Φ)2
y,t y,t N r4
b
Theprooffollowsfromlemma6.9. Noticethatalthoughwedonothaveahighconfidenceboundbetweenthesample
pathpolicygradientandstochasticgradientasinLemma4.2, weinsteadhaveaboundonthesecondmomentoftheir
difference. NowusingthetheMarkovinequalitywecanconvertthesecondmomentboundintoahighconfidenceboundin
thefollowinglemma.
Lemma6.12. UsingMarkovinequalityandLemma6.11,
2m2
|∇ˇi (Ki,K−i)−∇˜i (Ki,K−i)|≤ Tr(Φ)2
y,t y,t N r4δ
b
withprobabilityatleast1−δ.
42IndependentRLforCooperative-CompetitiveGames:AMean-FieldPerspective
Henceforagivenϵ>0,ifr =O(ϵ)andN =Θ˜(ϵ−4δ−1,thentheapproximationerrorbetweenthesample-pathgradient
b
andthepolicygradient∥∇ˇi (Ki,K−i)−∇i (Ki,K−i)∥=O(ϵ). Noticethatthemini-batchsizeforthesample-path
y,t y,t
gradientΘ˜(ϵ−4δ−1ismuchhighercomparedtothemini-batchsizeneededforthestochasticgradientΘ˜(ϵ−2).
Wenowshowthismini-batchsizedependenceusingempiricalresultsbelow. InFigure5wecomparethedifferencebetween
theMRPGalgorithm(Algorithm1)whichutilizesstochasticgradientsvstheSP-MRPGalgorithm(Algorithm3)which
utilizesthesample-pathgradients.
Figure5. ComparisonbetweenMRPG(N =2,000),SP-MRPG(N =2,000)andSP-MRPG(N =10,000).
b b b
ThefigureshowsconvergenceofnormoferrorovernumberofiterationswithnumberofteamsN =1,timehorizonT =1,
numberofagentsperteamM =1000andagentshavescalardynamics. Thenumberofinner-loopiterationsK =1000and
learningrateηi =0.001. WesimulatetheMRPGalgorithmwithmini-batchsizeN =2000,SP-MRPGwithmini-batch
k b
sizeN =2000andSP-MRPGwithmini-batchsizeN =10,000. Itisshownthatifmini-batchsizeisthesameMRPG
b b
hasbetterconvergencecomparedtoSP-MRPGwhichisplaguedwithhighvariance. Ifweincreasethemini-batchsizein
SP-MRPGwecanreducethevarianceduetolowervarianceofthesample-pathgradientestimates.
43