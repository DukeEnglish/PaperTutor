JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 1
A Spatial-Temporal Progressive Fusion Network for
Breast Lesion Segmentation in Ultrasound Videos
Zhengzheng Tu, Zigang Zhu, Yayang Duan, Bo Jiang, Qishun Wang and Chaoxue Zhang
Abstract—Ultrasound video-based breast lesion segmentation
provides a valuable assistance in early breast lesion detection
and treatment. However, existing works mainly focus on lesion
segmentation based on ultrasound breast images which usually
cannotbeadaptedwelltoobtaindesirableresultsonultrasound
videos. The main challenge for ultrasound video-based breast
lesionsegmentationishowtoexploitthelesioncuesofbothintra-
frame and inter-frame simultaneously. To address this problem,
weproposeanovelSpatial-TemporalProgressiveFusionNetwork
(STPFNet) for video based breast lesion segmentation problem.
The main aspects of the proposed STPFNet are threefold. First,
we propose to adopt a unified network architecture to capture
both spatial dependences within each ultrasound frame and
temporal correlations between different frames together for
ultrasounddatarepresentation.Second,weproposeanewfusion
module, termed Multi-Scale Feature Fusion (MSFF), to fuse
spatial and temporal cues together for lesion detection. MSFF
Fig. 1. In the figure the first two rows represent benign lesions and their
can help to determine the boundary contour of lesion region
ground truth, and the last two rows represent malignant lesions and their
to overcome the issue of lesion boundary blurring. Third, we
groundtruth.Fromlefttorightarethethreetypesofchallengesinourdataset,
propose to exploit the segmentation result of previous frame as
which are the significant change in shape of the lesions, the blurring of the
thepriorknowledgetosuppressthenoisybackgroundandlearn boundaries,andthepresenceofregionssimilartothelesions,respectively.
more robust representation. In particular, we introduce a new
publicly available ultrasound video breast lesion segmentation
dataset, termed UVBLS200, which is specifically dedicated to
breast lesion segmentation. It contains 200 videos, including 80 usually low resolution, noisy, and similar in foreground and
videosofbenignlesionsand120videosofmalignantlesions.Ex- background [7], [8], [9]. These bring great challenges to the
perimentsontheproposeddatasetdemonstratethattheproposed lesion segmentation tasks in ultrasound data, such as blurred
STPFNet achieves better breast lesion detection performance
tumor boundaries, and irregular shapes [10].
than state-of-the-art methods.
To address the issues of boundary blurring of the myotonic
Index Terms—Breast lesion segmentation, Deep learning net-
junction(MTJ)inultrasoundimages,Zhouetal.[11]proposed
work, Ultrasound video.
a Region-Adaptive Network (RAN) to locate MTJ region
and segment it. It first employs a region-based multi-task
I. INTRODUCTION learning module to explore regions containing MTJ and then
BREASTcancercontinuestobeoneofthemostprevalent extracts MTJ structures from the adaptively selected region
via U-shaped paths to remove boundary ambiguities. Also,
diseases for women. [1]. Many exploratory techniques
to address the issues of low resolution with thyroid nodules
have emerged in the field of breast cancer diagnosis and
and shadow interference, Ouahabi et al. [12] proposed a fully
treatment[2],[3].Amongthem,ultrasoundhasemergedasan
convolutional dense dilation network which combines low-
important technique for breast lesion detection and diagnosis
levelfinesegmentationwithhigh-levelcoarsesegmentationto
due to its superior ability to distinguish early indications [4],
extract more features from the ultrasound images. In addition
[5]. Segmentation of breast ultrasound images and detection
to the above studies in the field of ultrasound, there have
of lesion areas are important steps to help physicians identify
been some studies on addressing ultrasound breast segmen-
different functional tissues and guide tumour localisation and
tation problems. Some researchers consider to conduct both
clinical treatment [6]. However, different from natural images
tumour classification and segmentation together to achieve
and other medical image segmentation, ultrasound data is
robust segmentation purpose. Zhou et al. [13] demonstrated
Correspondingauthors:BoJiangandChaoxueZhang that joint learning of these two tasks could help to improve
Zhengzheng Tu, Zigang Zhu, Bo Jiang and Qishun Wang are with the theirperformancerespectively.Theyproposeanewmulti-task
Anhui Provincial Key Laboratory of Multimodal Cognitive Computing,
learning scheme for accurate segmentation and classification
School of Computer Science and Technology, Anhui University, Hefei,
230601, China (e-mail: zhengzhengahu@163.com, 18355421540@163.com, of breast tumours. Also, some works improve the blurred
jiangbo@ahu.edu.cn,qishunahu@163.com). boundary in segmentation by enhancing the learning of the
YayangDuanandChaoxueZhangarewiththeFirstAffiliatedHospitalof
blurred boundaries. For example, Wang et al. [14] designed a
AnhuiMedicalUniversity,Hefei,230022,China(drduan yayang@163.com,
zcxay@163.com). supervised residual representation module to learn ambiguous
4202
raM
81
]VI.ssee[
1v99611.3042:viXraJOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 2
boundaries and confused regions by employing a residual Progressive Fusion Network (STPFNet), for breast lesion
feedbacktransmissionstrategy.However,thisresidualnetwork segmentation tasks. It makes full use of spatial-temporal in-
relies heavily on searching ambiguous boundaries when ex- formationforultrasoundvideoviatemporalandspatialfusion
tractingfeatures.Someotherstrategiesalsoconsidertoobtain modules.
more detailed features to reduce the problems associated with (2) We design a new Multi-Scale Feature Fusion module,
ultrasound imaging. For example, Xue et al. [15] generated termed MSFF, to extract multi-scale (coarse-to-fine) informa-
a multilayer integrated feature map to learn remote non- tion effectively for ultrasound video data representation.
local dependencies by integrating features from all CNN [16] (3)Weconstructapubliclyavailableultrasoundvideobreast
layers, and achieved accurate segmentation of breast lesion lesion segmentation dataset (UVBLS200) which contains 200
boundaries by learning additional boundary information. videosequencestotalling10,666imageswithgroundtruthan-
Incontrasttoultrasoundimages,ultrasoundvideoscanpro- notations.Thedatasetwillfacilitatebreastlesionsegmentation
vide richer spatial and temporal information for lesion detec- studies and will be released to the public.
tion.Lietal.[17]presentedadatasetandbenchmarkdedicated (4) We evaluate our STPFNet method on UVBLS200
tosegmentationofbreastlesionsinultrasoundvideos.Intheir dataset on both image and video levels. The experimental
work, the temporal transformer module focuses on motion results show that our proposed STPFNet achieves better per-
informationwithinthesameregionamongconsecutiveframes, formance compared to existing methods.
while the spatial transformer module makes full use of the
informationfromthepreviousframetocapturetheinformation II. RELATEDWORK
spatially. Although the spatial-temporal information in the A. Segmentation of breast lesions
video is exploited in work [17], the above mentioned issues
Ultrasound breast lesion segmentation plays a crucial role
of ultrasound imaging, such as noise and blurred boundary,
in the detection and treatment of breast lesions. Formerly,
haven’t been fully addressed. In addition, Lin et al. [44]
ultrasound breast lesion segmentation relied primarily on the
proposed a larger dataset of ultrasound video breast lesions
experience and observation of physicians. However, this way
and a new Frequency and Localisation Feature Aggregation
lacks reliability due to its subjectivity and inconsistency.
Network (FLA-Net) from a frequency domain perspective.
Then, some attentions have been given to traditional image
To overcome the above issues, in this paper, we pro-
processingtechniquesandmanymethods[18],[19],[20]have
pose a novel Spatial-Temporal Progressive Fusion Network
been developed in this field. These techniques to some extent
(STPFNet) for video based breast lesion segmentation prob-
facilitate the precise segmentation of ultrasound lesions. With
lem. The proposed STPFNet contrains three main aspects.
the booming development of machine learning technology,
First,STPFNetadoptsaunifiednetworkarchitecturetocapture
machine learning based breast lesion segmentation methods
both spatial dependences within each ultrasound frame and
have emerged. By exploiting the rich ultrasound image data
temporal correlations between different frames together for
and the autonomous learning capability of the algorithms,
ultrasound data representation. Thus, it can fully exploit the
these methods [21], [22], [23] learn a variety range of tumour
cuesofintra-frameandinter-framesimultaneouslyforaccurate
featuresandachieveaccuratetumoursegmentation.Incontrast
lesion segmentation. Second, in STPFNet, we design a new
to traditional methods, these approaches have the ability to
fusionmodule,termedMulti-ScaleFeatureFusion(MSFF),to
handlediverseshapesoflesion,therebyyieldingmoreprecise
fuse spatial and temporal cues with different scales together
and stable segmentation results. In recent years, along with
forlesiondetection.MSFFcanhelptodeterminetheboundary
deep learning being developed well, many deep neural net-
contour of lesion region to overcome the issue of lesion
work methods [24], [25], [26], [27] have been proposed to
boundary blurring in ultrasound videos. Third, STPFNet fully
segment breast lesions. These methods usually involve multi-
exploitsthesegmentationresultofpreviousframeastheprior
levelfeatureextractionandincorporatecontextualinformation
knowledge to suppress the noisy background and highlight
to improve the accuracy of breast lesion segmentation. Of
the foreground to learn more accurate visual representation
these, U-Net [28] plays a significant role in medical image
for lesion objects. In particular, we construct a novel dataset,
segmentation. It is composed of an encoder and a decoder
termed UVBLS200, specifically designed for breast lesion
interconnected through hopping connections, enabling better
segmentationtasks.Thedatasetconsistsof80videosequences
segmentation performance with less data. In addition, Soltani
depictingbenignlesionand120videosequencesdepictingma-
etal.[29]proposedaMaskR-CNNbasedmethodforaccurate
lignantlesion.Eachvideosequencecontainsthecorresponding
segmentation of multiple instances in an image by predicting
image frames along with the ground truth of the lesion. In
boththeboundingboxandmaskoftheobjectssimultaneously.
Fig. 1, we show our proposed dataset UVBLS200, which
Applying the benefits of deep learning to breast lesion seg-
includesbothbenignandmalignantlesionsandtheirannotated
mentationisexpectedtoprovidephysicianswithmorereliable
ground truth. In addition, we briefly show some challenges in
assistance and achieve more accurate lesion segmentation.
the UVBLS200 dataset, such as significant variation in lesion
shape, blurred boundaries, and the presence of areas that are
B. Video Object Segmentation
similar to the lesions.
Overall,themaincontributionsofthispaperaresummarized Initially, the background and development of video object
as follows, segmentation can be traced back to the early still image
(1) We propose a new approach, called Spatial-Temporal segmentation techniques. Subsequently, with the rise of videoJOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 3
Fig.2. Inourproposednetwork,wedenoteframes1tot-1inthevideoaspastframes.Theterm”pred-t”denotesthemultiplicationofthepredictionmask
from the previous frame with the current frame. ”t-1” represents the previous frame. Subsequently, the ”Pred-t” and the previous frame ”t-1” are fed into
the network backbone, thus generating Vp and Kp respectively. These obtained features are then fused together by the MSFF module, and the final
t−1 t−1
predictionisproducedbythedecoder.
processing techniques, researchers began to propose various (SFM), the Multi-Scale Feature Fusion Module (MSFF), and
algorithms and methods for the dynamic characteristics of the loss function.
video. Wei et al. [30] proposed an automatic video seg-
mentation algorithm based on k-median clustering algorithm
A. Overall Architecture
and 2D binary model. However, these methods almost lie on
Fig. 2 illustrates the overall framework of the proposed
low-level feature representation, and rarely utilize high-level
Spatial-Temporal Progressive Fusion Network in this paper.
feature representation. As a result, the object segmentation in
The Temporal Fusion Module, similar to STM [31], fuses
complex scenes is relatively unsatisfactory. Semi-supervised
the current frame with the past frames to find similar regions
learning has been incorporated into video object segmenta-
betweenthecurrentandthepastframesinthevideosequence.
tion recently. In semi-supervised video object segmentation,
The Spatial Fusion Module fuses the current frame with the
instead of labelling the whole video in detail, only a part
previous frame. It takes the predicted lesion region from the
of the video sequence needs to be labelled. The model then
previous frame as a priori knowledge to suppress the noisy
learns the mapping relationship from labelled frames to un-
background and highlight the foreground to achieve the local-
labelled frames. Semi-supervised video object segmentation
isationofthelesionregion.Thentoimprovetheperformance,
not only uses labelled training data, but also makes full
we further use a multi-scale feature fusion approach. This
use of unlabelled data to improve the performance of the
involvesthefusionoftheultrasoundbreastlesioninformation
model. STM [31] employed space-time fusion to integrate
obtainedbytheencoderwiththefusioninformationofthetwo
the information of past and current frames, and decoded it
branchesdescribedabove.Subsequently,thefeaturesobtained
using skip connections, resulting in a considerable improve-
from the fusion are up-sampled by a decoder, resulting in a
ment in performance. KMN [32] presented an enhanced STM
prediction that matches the size of the input image.
approach, for addressing the issue of object missegmentation
by introducing memory-to-query matching. Nevertheless, this
solutionreliesonlyonpixelsimilaritymatchingandislimited B. Temporal Fusion Module
in its ability to handle appearance changes and deformations.
In this work, we draw on video object segmentation meth-
Furthermore, RMNet [33] mitigated the limitations of STM
ods developed for natural images to improve our approach.
to some extent, by incorporating the mask predicted by the
Specifically, the space-time memory network skillfully uses
current frame with optical flow calculation.
the temporal information within a video sequence.
InspiredbySTM[31],weuseResNet50[34]astheencoder,
and define the feature of the last layer of the current frame
III. METHOD
through the encoder as r ∈ RH×W×C, where H stands for
This section provides a comprehensive overview of the the height of the feature, W for the width of the feature, and
Spatial-Temporal Progressive Fusion Network, including the C represents the number of channels of the feature. Without
Temporal Fusion Module (TFM), the Spatial Fusion Module reducingtheresolution,thenumberofchannelsofrisreducedJOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 4
to 1/2 and 1/8 of the original number of channels as the value frameinthetemporaldimension.Thenweperformasimilarity
and key of the current frame, defined as VC ∈ RH×W×C/2 calculationf betweentheKp ofthepreviousframeandthe
t−1
and KC ∈ RH×W×C/8, respectively. For past frames, the KC ofthecurrentframe.Thecalculationprocessisasfollows:
sameoperationasaboveisperformedtogetthevalueandkey
f(KC,Kp )=softmax(exp(KC ⊗Kp )) (3)
for each past frame. Then all past frames are concatenated to t−1 t−1
form the value and key of all past frames, defined as VP ∈ where ⊗ represents matrix multiplication. Furthermore, con-
RT×H×W×C/2, KP ∈ RT×H×W×C/8. Then the similarity sidering the overlap of lesion regions between frames, we
weight between the KC of the current frame and the KP of multiplythelesionregionpredictedinthepreviousframeasa
the past frames is calculated. Finally the obtained similarity prior knowledge with the current frame, which can make the
weightsaremappedonthepastframestofindthemostsimilar network to pay more attention to the lesion region predicted
position between the current frame and the past frames. The in the past and suppress the background noise. The formula
similarity calculation function f is shown below: is as follows:
Vp =θ(P ∗C ) (4)
f(KC,KP)=softmax(exp(KC ⊗KP)) (1) t−1 m f
whereP indicatesthepredictedmaskofthepreviousframe,
where ⊗ represents matrix multiplication. By mapping the m
C indicatesthecurrentframe,θmeanssubsequentoperations
similarityweightstopastframes,thefinalfeatureyisobtained f
that are encoding and convolution, and Vp denotes the
as follows: t−1
value to which a priori knowledge is added. By mapping the
y =f(KC,KP)VP. (2)
similarity weights onto the current frame, the final feature z
Like the STM, the temporal fusion module has similar lim- is obtained as follows:
itations. Due to the noise and blurring in ultrasound video
z =f(KC,Kp )Vp . (5)
frames, the temporal fusion module still incorrectly segments t−1 t−1
similar regions as the lesion.
D. Multi-Scale Feature Fusion Module
Inultrasoundlesionsegmentation,itisimportanttoachieve
C. Spatial Fusion Module
finesegmentationoflesionboundaries.However,itisdifficult
Inspiredbythiswork[35],wefindthattheobjectregionin
to solve the problem of noise and blurred boundaries in
adjacent frames of video exhibits a certain level of continuity.
ultrasound videos simply using the approach of video object
Thisimpliesthattheapproximatepositionoftheobjectregion
segmentation. To address these issues, we propose a Multi-
can be inferred from the frames with a certain degree of
Scale Feature Fusion method to interactively fuse temporal,
overlap.Therefore,byutilizingthepositionalinformationfrom
spatial,andcoarse-grainedfeaturestorealizethesegmentation
thepreviousframe,moredetailsofthelesioncanbeobtained.
of boundaries. As shown in Fig. 4. Specifically, we take
The SFM can encode the previous neighbouring frame and
the features from two different scales as the fine-grained
add the lesion prediction results from the previous frame as
features, including the global similarity region features from
a prior knowledge to locate the lesion more accurately. As
thetemporalfusionmoduleandthelesionregionfeaturesfrom
the spatial fusion module. Furthermore, we take the features
from the encoder as coarse-grained information and introduce
other fine-grained information at multiple scales, making the
network to learn more detailed features and achieve better
contourinformationofthelesion.First,wedenotethefeatures
obtained by the temporal fusion module, the spatial fusion
moduleandtheencoderas y ∈RH×W×C/2, z ∈RH×W×C/2
and w ∈ RH×W×C/8, respectively. Since, average pooling is
to obtain the average value in the local window, which has a
certain smoothing effect and helps to reduce the influence of
noise. Secondly, max pooling has certain invariance for small
translationofobjectininputimages.Evenifthelesionmoves
slightly in the window, max pooling can still extract the same
characteristic. In the multi-scale feature fusion module, we
Fig. 3. This is our spatial fusion module. The prediction of the previous perform max pooling and average pooling on z, y, and w,
frame(Ho×Wo×1)ismultipliedwiththecurrentframe(Ho×Wo×Co)
respectively. And the results of the pooling will be concated
asaprioriknowledgetosuppressthebackgroundnoise.
and fed into the FC to obtain the weight maps denoted as Z,
showninFig.3,weconvolvethepreviousframetoobtainthe Y, W, respectively. The above process can be described as
corresponding key, defined as Kp ∈ RH×W×C/8, where follows:
t−1
t−1 denotes the previous frame of the current frame in the Y =Φ(avg(y)⊕max(y)) (6)
same video sequence. The previous frame has not only most
Z =Φ(avg(z)⊕max(z)) (7)
similar texture information as the current frame in the spatial
dimension, but also most close lesion location as the current W =Φ(avg(w)⊕max(w)) (8)JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 5
A. Data Acquisition
Our ultrasound video breast lesion segmentation dataset
(UVBLS200) was collected from the total of 528 patients,
using Resona 7 and Toshiba 660a ultrasound systems and
including 200 video sequences with a total of 10,666 frames.
The video sequences have varying resolutions, with a max-
imum of 1072 × 756 and a minimum of 256 × 256. The
data used in the dataset presented in this paper have been
approved by the Clinical Medical Research Ethics Committee
of the First Affiliated Hospital of Anhui Medical University.
The UVBLS200 dataset does not contain any personal patient
information.
Fig.4. Themulti-scalefeaturefusionmoduleisproposed.Featuresobtained B. Dataset Description
byspatial-temporalfusionarecategorisedasfine-grainedinformation,while
We describe UVBLS200 dataset in detail, which is for-
features obtained with convolution only are categorised as coarse-grained
information. The features are fused using a weighted summation performed matted in DAVIS 2017 and consists of 200 video sequences
bytwoLinearlayers. with a total of 10,666 frames, where there are 80 benign and
120 malignant lesions in 200 video sequences. For the video
sequences, the ground truth of each frame was annotated by
where ⊕ stands for concat operation, Φ represents the FC
threeradiologists(atleast3yearsofmedicalexperience)anda
which mainly consists of two Linear layers, avg stands for
seniordepartmentdirectorwasresponsibleforsupervisingand
averagepooling,andmaxstandsformaxpooling.wemutiply
refining the quality of the ground truth to ensure the accuracy
the weight maps Z, Y, and W with z, y, and w to get the
of our dataset. Two aspects of the main contribution of our
weighted features and directly add them to obtain the mulit-
dataset are as follows:
scale fusion feature denoted as X. And the poccess can be
(1) We provide a publicly available dataset of ultrasound
described as:
video breast lesion segmentation, annotated by specialized
X =Y ·y+Z·z+W ·w. (9) physicians to ensure the accuracy of our dataset.
(2) Our dataset is more realistic and comprehensive than
Finally, we fed the mulit-scale fusion feature into the decoder the available published ultrasound breast datasets. Our data
to produce the prediction maps. contains not only typical lesion samples, but also atypical
samples that appear to be caused by a variety of factors,
making the dataset more diverse.
E. Loss Function Calculation
Forexample,someimageshavemoreblurredlesionbound-
In this paper, we use three consecutive frames with their ariesandmorepronouncedchangesintumourshapethansome
ground truth during the training process, and provide the typicalimages.Inaddition,sinceourdatasetconsistsofvideo
ground truth of the first frame to predict the result of segmen- sequences,itispossibletoobservethesamelesioninmultiple
tation for the subsequent frames during the testing process. In frames, thus obtaining more detailed information about the
the training phase, we use the cross-entropy loss function to lesion.
calculate the distance between the prediction of our network
andthegroundtruth.Herewedenotethegroundtruthasg2,g3
V. EXPERIMENT
and the predictions as p2, p3, respectively. The CELoss is
In this section, we will introduce the set up of the exper-
described as:
iment and the evaluation metrics. In addition, we compare
(cid:88)
CELoss(g2,p2)=− g2∗log(p2) (10) our method with the state-of-the-art methods. Finally, we
conduct ablation experiments to verify the effectiveness of
CELoss(g3,p3)=−(cid:88) g3∗log(p3). (11) each proposed module.
The total loss is formulated as:
A. Experimental Detail
Loss=CELoss(g2,p2)+CELoss(g3,p3). (12) We use 90% of the video sequences in the UVBLS200
dataset for training and 10% for testing. Our network is
implemented on an NVIDIA A100 GPU, utilizing Python 3.6
IV. UVBLS200:ULTRASOUNDVIDEOBREASTLESION and PyTorch development environment 1.6.0.
SEGMENTATIONDATASET
B. Evaluation Metrics
For the development of ultrasound video lesion segmen-
tation, we construct a new ultrasound video breast lesion Toensureasystematicandcomprehensiveevaluationofthe
segmentation dataset, and we will introduce the dataset in experimental results, we employ four indicators to assess the
detail in this section. segmentation results. We utilize the Dice coefficient whichJOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 6
TABLEI
COMPARISONWITHTHESTATE-OF-THE-ARTMETHODS.THEDATASETISEVALUATEDUSINGBOTHIMAGE-LEVELANDVIDEO-LEVELMETHODS,AND
THEOPTIMALRESULTSAREHIGHLIGHTEDINBOLD
Method Venue Year Type Dice Iou Recall MAE
Unet++ TMI 2019 Image 0.723 0.576 0.664 0.054
HarDNet ICCV 2019 Image 0.823 0.727 0.823 0.035
MSNet MICCAI 2021 Image 0.800 0.700 0.769 0.041
TRUNet arxiv 2022 Image 0.819 0.724 0.828 0.038
UCTNet AAAI 2022 Image 0.825 0.721 0.846 0.037
STM ICCV 2019 Video 0.821 0.729 0.857 0.039
AFB-URR NeurIPS 2020 Video 0.811 0.713 0.794 0.037
DCF-Net ICCV 2021 Video 0.804 0.707 0.794 0.035
UFO TMM 2022 Video 0.789 0.680 0.813 0.040
TMFF(Ours) - - Video 0.841 0.752 0.888 0.035
quantifies the proportion of overlapping region to measure further exploits the uncertainty region to refine the boundary
the similarity between the segmentation result and the ground basedonSTM.DCF-Netexploitsspatial-temporalinformation
truth. To assess the degree of overlap between the segmenta- by extracting the position-related affinities between consec-
tion results and the ground truth, we use the IoU to calculate utive frames. In addition, UFO changes the last two layers
the ratio of the intersection to the union of both. In addition, of the skip connection to a transformer block, making it
we exploit Recall to evaluate the capability of model for easiertocapturelong-termdependenciesbetweenfeatures.To
detecting true instance. Finally, we use MAE to measure the ensurethefairnessofexperiments,wetrainallmethodsonthe
error between the predicted value and the actual value. The UVBLS200 dataset. The experimental results in Table I, we
smallerthevalue,thebetterthepredictivepowerofthemodel. canseethatourmethodoutperformsothermethodsinallthree
The following formulas represent the specific implementation metrics of dice, iou and recall, and achieves the best perfor-
of four metrics: mance.Specifically,comparedtothebaseline[31],ourmethod
improves 2.0%, 2.3%, and 3.1% on the Dice, Iou, and Recall
Dice=(2|SR∩GT|)/(|SR|+|GT|) (13)
metrics, respectively, while decreasing 0.4% on the MAE.
IoU =(|SR∩GT|)/(|SR∪GT|) (14) In addition, compared to the UCTNet network, we improve
1.6%, 3.1%, and 4.2%, respectively, while decreasing 0.2%
where |SR| represents the set of pixels in the segmentation
in MAE. The methods for images also achieves good results
result,and|GT|representsthesetofpixelsinthegroundtruth.
on our dataset, indirectly proving the validity of our dataset.
|SR∩GT| denotes the number of pixels in the intersection of
The analysis for the results shows that our method achieves
SR and GT, |SR∪GT| denotes the number of pixels in the
satisfactory segmentation results for ultrasonic videos.
concatenation of SR and GT.
Recall=TP/(TP +FN) (15) TABLEII
THEIMPACTOFTWOMODULESINTHENETWORKONTHEDATASET,ON
where TP denotes the count of true positive examples, while DICE,IOU,RECALLANDMAEMETRICS
FN represents the count of false negative examples.
Baseline MSFF SFM Dice Iou Recall MAE
n
1 (cid:88)
MAE =
n
|yˆ i−y i| (16) √ 0.821 0.729 0.857 0.039
i=1 √ √
0.826 0.732 0.842 0.036
where, n represents the number of samples, yˆ represents the
i
√ √
actual value of the i sample, and y i represents the predicted 0.832 0.744 0.888 0.040
value of the i sample. √ √ √
0.841 0.752 0.888 0.035
C. Comparison with State-of-the-Art Methods
2) Analysis for Visual Results: We focus on comparing the
1) Comparison of Different Methods: In this section, we visualisation results of the methods for videos, as shown in
compare our method with state-of-the-art methods separately. Fig.5.Wecanseeourmethodperformsasignificantimprove-
The methods of images include: UNet++ [36], HarDNet [37], ment for segmentation of lesion boundaries. In addition, due
MSNet [38], TRUNet [39], and UCTNet [40]. The methods to the high noise in the ultrasound image, it is hard to locate
of videos include: STM [31], AFB [41], DCF-Net [42], and the lesion region correctly, and to segment the region similar
UFO[43].STMlocatesobjectsbyfindingthesimilarityregion to the lesion (e.g., STM and AFB in the first and fifth rows)
between past and current frames in a video. And AFB-URR accurately. We use the previous frame as a prior knowledge,JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 7
Fig.5. Visualpresentationofcomparativeexperiments.Thefirstcolumncorrespondstothegroundtruth,thesecondcolumnshowsthebaselineprediction
results,thethirdcolumnshowstheAFBpredictionresults,thefourthrowshowstheDCFpredictionresults,thefifthrowshowstheUFOpredictionresults,
andthesixthcolumnrepresentsthepredictionresultsofourmethod.Wherethebreastlesionareaishighlightedinred.
thus reducing the incorrect segmentation of the region similar onlyreducesinformationlossbutalsobettercomplementsthe
to the lesion to a certain extent. For boundary blurring of lesionboundaries.Duringtheablationexperiments,wesimply
ultrasound breast lesion, we use multi-scale feature fusion to merge the features obtained from the two fusion branches
extractmoredetailedinformationfromdifferentscales,which and compare the performance with the MSFF module to
can locate the boundary contour more accurately than other demonstratetheeffectivenessoftheMSFFmodule.Asshown
methods. in Table II, the multi-scale feature fusion module leads to a
1.5% increase in the Dice coefficient, a 2.0% increase in the
IoU, a 4.6% increase in Recall, and a 0.1% decrease in MAE
D. Ablation Study
when compared to the baseline.
1) Architecture Ablation: We conduct a series of ablation
experiments to demonstrate the effectiveness of the designed 2) ParameterAblation: Wealsocomparesomekeyparam-
module. In the spatial fusion module, considering that the eters and analyze the differences between them. As shown in
previous frame and the current frame in a video usually have Table III. Specifically, we investigate the effectiveness of the
close location for the lesion, we apply the segmentation result strategy of mapping predictions from previous frames to the
of the previous frame as a prior knowledge to locate the current frame in the spatial fusion module, and explore the
lesion region of the current frame accurately. In addition, effectiveness of adding coded information as coarse-grained
we calculate the similarity between the current frame and information as well as the optimal coding layer. Finally, we
the previous frame with the aim of minimising the cases compare the advantages of max and average pooling layers in
of incorrect segmentation of the lesion region. As shown in multiscale feature fusion. Based on the experimental results,
TableII,thismodificationresultedinimprovementscompared simple multiplication of the previous frame prediction results
to the baseline, including a 0.9% increase on Dice, a 0.8% asaprioriknowledgewiththecurrentframehelpstosuppress
increase on IoU, and a 0.5% decrease on MAE. background noise and locate the lesion area. In addition, the
Inthemultiscalefeaturefusionmodule,wefusethefeatures features extracted from the different layers in the encoder
obtainedfromthetimefusionbranch,thespatialfusionbranch contribute to the lesion segmentation, especially from the last
and the encoder to obtain richer detail information, which not layer. The simultaneous use of combined max pooling andJOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 8
TABLEIII
ABLATIONEXPERIMENTS.”+2”,”+3”,AND”+4”REPRESENTTHEFEATURESENCODEDINTHETHIRDLAST,SECONDLAST,ANDLASTENCODER
LAYERS,RESPECTIVELY.AND”∗MASK”INDICATESTHEMAPPINGMETHODTHATMAPSTHEPREDICTIONRESULTOFTHEPREVIOUSFRAMETOTHE
CURRENTFRAME
Baseline +Maxpool +Avgpool +2 +3 +4 ∗mask Dice Iou Recall MAE
√ √ √ √
0.837 0.747 0.861 0.037
√ √ √ √
0.834 0.743 0.889 0.039
√ √ √ √ √
0.836 0.745 0.881 0.038
√ √ √ √ √
0.834 0.742 0.852 0.040
√ √ √ √
0.821 0.726 0.810 0.037
√ √ √ √ √
0.841 0.752 0.888 0.035
average pooling helps to smooth out feature variations and space. In contrast, we map the mask of the previous frame
makes the network focus on the most critical regions. onto the current frame as a prior knowledge for suppressing
cluttered information in the blurred background, especially
those regions that are similar to the lesion. In addition, we
consider further fusion of more detailed information in the
processofrefiningtheboundariestoobtainmorecleardetailed
information and to make the boundaries more accurately.
VII. CONCLUSION
In this paper, we propose a Spatial-Temporal Progressive
Fusion network (STPFNet) to solve the problems of blurred
boundary and irregular shape of ultrasound breast lesions. We
first make full use of spatial-temporal information through
the temporal fusion module and the spatial fusion module.
Fig. 6. Challenging samples. The table consists of three columns: the first Then, we perform multi-scale fusion to fuse temporal and
columndisplaysframesusedforprediction,thesecondcolumnshowsground spatial features as well as features from the encoder to get
truth,andthethirdcolumnrepresentsthepredictionresults.
more detailed information. We also use the previous frame
as a prior knowledge to locate the lesion area. Finally, we
construct a new UVBLS200 dataset for breast lesion segmen-
VI. DISCUSSION
tation. We perform a comparative evaluation of our proposed
We introduce a novel Spatial-Temporal Progressive Fusion method against several other state-of-the-art techniques on
Network,whicheffectivelyaddressestheproblemofincorrect the UVBLS200 dataset. The results not only demonstrate the
segmentation of non-lesion regions and accurately segments effectiveness of our approach compared to other methods, but
blurred boundaries. However, some misjudgements still occur also indirectly show our challenging dataset. In the future, we
in some cases. For example, when there are shadows around intend to dig deeper into the UVBLS200 dataset to address
a lesion, the network may incorrectly segment some of the more challenges in our dataset.
shadows as lesion areas, resulting in inaccurate boundary, as
shown in the first row of Fig. 6. In addition, introducing ACKNOWLEDGMENT
prior knowledge from the previous frame successfully mit-
Thanks to the First Affiliated Hospital of Anhui Medi-
igates misjudgements for non-lesion regions, still producing
cal University for collecting and annotating the UVBLS200
incorrect segmentation when the lesion is very similar to the
dataset.
surrounding background and close to the lesion, as shown in
the second row of Fig. 6. Thus, in our future studies, we will
REFERENCES
go on deeper insights into the pathological characteristics of
breast lesions and try to achieve more precise segmentation. [1] Y. Shu, H. Li, B. Xiao, X. Bi, and W. Li, “Cross-Mix Moni-
toring for Medical Image Segmentation With Limited Supervision,”
Wewillexplorenewapproachesinfeatureextractionandnew
IEEE Transactions on Multimedia, pp. 1700–1712, Jan. 2023, doi:
architectures in enhancing differentiation between the lesion 10.1109/tmm.2022.3154159.
and the surrounding background regions. [2] AhmedM.Alaa,KyeongH.Moon,W.Hsu,andM.Schaar,“Confident-
Care:AClinicalDecisionSupportSystemforPersonalizedBreastCan-
Incomparisontothework[17],weadoptSTMasthebase-
cerScreening,”IEEETransactionsonMultimedia,IEEETransactionson
line and consider utilising the information from the previous Multimedia,Feb.2016.
frame. However, in this study [17], the main point is on the [3] O.M.Bucci,L.Crocco,andR.Scapaticci,“OntheOptimalMeasure-
mentConfigurationforMagneticNanoparticles-EnhancedBreastCancer
fusion of the previous frame and the mask of the previous
Microwave Imaging,” IEEE Transactions on Biomedical Engineering,
frame, thus fully exploiting the role of the previous frame in pp.407–414,Feb.2015,DOI:10.1109/tbme.2014.2355411.JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 9
[4] H.Hille,“AdvancesinBreastUltrasound,”inSonography,2012.DOI: [23] H.Dhahri,E.AlMaghayreh,A.Mahmood,W.Elkilani,andM.Faisal
10.5772/30078. Nagi,“AutomatedBreastCancerDiagnosisBasedonMachineLearning
[5] A.Jalalian,S.B.T.Mashohor,H.R.Mahmud,M.I.B.Saripan,A.R. Algorithms,” Journal of Healthcare Engineering, vol. 2019, pp. 1-11,
B.Ramli,andB.Karasfi,“Computer-aideddetection/diagnosisofbreast Nov.2019,DOI:10.1155/2019/4253641.
cancer in mammography and ultrasound: a review,” Clinical Imaging, [24] R. Almajalid, J. Shan, Y. Du, and M. Zhang, “Development of a
pp.420–426,May2013,DOI:10.1016/j.clinimag.2012.09.024. Deep-Learning-Based Method for Breast Ultrasound Image Segmen-
[6] Y.Xu,Y.Wang,J.Yuan,Q.Cheng,X.Wang,andP.L.Carson,“Medical tation,” in 2018 17th IEEE International Conference on Machine
breastultrasoundimagesegmentationbymachinelearning,”Ultrasonics, Learning and Applications (ICMLA), Orlando, FL, Dec. 2018. DOI:
pp.1–9,Jan.2019,DOI:10.1016/j.ultras.2018.07.006. 10.1109/icmla.2018.00179.
[7] Y. Shu, H. Li, B. Xiao, X. Bi, and W. Li, “Cross-Mix Moni- [25] Y. Hu et al., “Automatic tumor segmentation in breast ultrasound
toring for Medical Image Segmentation With Limited Supervision,” images using a dilated fully convolutional network combined with an
IEEE Transactions on Multimedia, pp. 1700–1712, Jan. 2023, doi: activecontourmodel,”MedicalPhysics,pp.215-228,Jan.2019,DOI:
10.1109/tmm.2022.3154159. 10.1002/mp.13268.
[8] S.PradeepandP.Nirmaladevi,“AReviewonSpeckleNoiseReduction [26] Z. Ning, S. Zhong, Q. Feng, W. Chen, and Y. Zhang, “SMU-Net:
Techniques in Ultrasound Medical images based on Spatial Domain, Saliency-GuidedMorphology-AwareU-NetforBreastLesionSegmen-
TransformDomainandCNNMethods,”IOPConferenceSeries:Materi- tation in Ultrasound Image,” IEEE Transactions on Medical Imaging,
alsScienceandEngineering,p.012116,Feb.2021,DOI:10.1088/1757- pp.476–490,Feb.2022,doi:10.1109/tmi.2021.3116087.
899x/1055/1/012116. [27] S. W. Cho, N. R. Baek, and K. R. Park, “Deep Learning-based
[9] J. Liu et al., “Speckle noise reduction for medical ultrasound images Multi-stageSegmentationMethodUsingUltrasoundImagesforBreast
based on cycle-consistent generative adversarial network,” Biomedical Cancer Diagnosis,” Journal of King Saud University - Computer and
Signal Processing and Control, vol. 86, p. 105150, Sep. 2023, DOI: Information Sciences, vol. 34, no. 10, pp. 10273–10292, Nov. 2022,
10.1016/j.bspc.2023.105150. DOI:10.1016/j.jksuci.2022.10.020.1.
[10] H. Wu, X. Huang, X. Guo, Z. Wen and J. Qin, “Cross-Image De- [28] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional Net-
pendency Modeling for Breast Ultrasound Segmentation,” in IEEE worksforBiomedicalImageSegmentation,”inLectureNotesinCom-
Transactions on Medical Imaging, vol. 42, no. 6, pp. 1619-1631, June puterScience,MedicalImageComputingandComputer-AssistedInter-
2023,doi:10.1109/TMI.2022.3233648. vention-MICCAI2015,2015,pp.234-241.
[11] G.-Q. Zhou et al., “A Single-Shot Region-Adaptive Network for [29] H. Soltani, M. Amroune, I. Bendib, and M. Y. Haouam, “Breast
Myotendinous Junction Segmentation in Muscular Ultrasound Im- Cancer Lesion Detection and Segmentation Based On Mask R-CNN,”
ages,” IEEE Transactions on Ultrasonics, Ferroelectrics, and Fre- in 2021 International Conference on Recent Advances in Mathemat-
quency Control, vol. 67, no. 12, pp. 2531-2542, Dec. 2020, DOI: ics and Informatics (ICRAMI), Tebessa, Algeria, Sep. 2021. DOI:
10.1109/tuffc.2020.2979481. 10.1109/icrami52622.2021.9585913.
[12] A. Ouahabi and A. Taleb-Ahmed, “RETRACTED: Deep learn- [30] Wei Wei, K. N. Ngan, and A. Habili, “Multiple feature clustering
ing for real-time semantic segmentation: Application in ultrasound algorithm for automatic video object segmentation,” in 2004 IEEE
imaging,” Pattern Recognition Letters, pp. 27-34, Apr. 2021, DOI: InternationalConferenceonAcoustics,Speech,andSignalProcessing,
10.1016/j.patrec.2021.01.010. Montreal,Que.,Canada,Sep.2004.DOI:10.1109/icassp.2004.1326622.
[13] Y.Zhouetal.,“Multi-tasklearningforsegmentationandclassification [31] S.W.Oh,J.-Y.Lee,N.Xu,andS.J.Kim,“VideoObjectSegmentation
of tumors in 3D automated breast ultrasound images,” Medical Image UsingSpace-TimeMemoryNetworks,”in2019IEEE/CVFInternational
Analysis,pp.101918,May2021,DOI:10.1016/j.media.2020.101918. Conference on Computer Vision (ICCV), Seoul, Korea (South), Oct.
[14] K. Wang, S. Liang, and Y. Zhang, “Residual Feedback Network for 2019.DOI:10.1109/iccv.2019.00932.
Breast Lesion Segmentation in Ultrasound Image,” in Medical Image [32] H. Seong, J. Hyun, and E. Kim, “Kernelized Memory Network for
ComputingandComputerAssistedIntervention-MICCAI2021,Lecture VideoObjectSegmentation,”inComputerVision-ECCV2020,Lecture
NotesinComputerScience,2021,pp.471-481. Notes in Computer Science, 2020, pp. 629–645. DOI: 10.1007/978-3-
[15] C.Xueetal.,“Globalguidancenetworkforbreastlesionsegmentation 030-58542-6 38.
inultrasoundimages,”MedicalImageAnalysis,pp.101989,May2021, [33] H. Xie, H. Yao, S. Zhou, S. Zhang, and W. Sun, “Efficient Regional
DIO:10.1016/j.media.2021.101989. MemoryNetworkforVideoObjectSegmentation,”in2021IEEE/CVF
[16] Y. Kim, “Convolutional Neural Networks for Sentence Classification,” Conference on Computer Vision and Pattern Recognition (CVPR),
in Proceedings of the 2014 Conference on Empirical Methods in Nashville,TN,USA,Jun.2021.DOI:10.1109/cvpr46437.2021.00134.
NaturalLanguageProcessing(EMNLP),Doha,Qatar,Jan.2014.DOI: [34] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for
10.3115/v1/d14-1181. ImageRecognition,”in2016IEEEConferenceonComputerVisionand
[17] Li,J.,Zheng,Q.,Li,M.,Liu,P.,Wang,Q.,Sun,L.,Zhu,L.,“Rethinking Pattern Recognition (CVPR), Las Vegas, NV, USA, Jun. 2016. DOI:
BreastLesionSegmentationinUltrasound:ANewVideoDatasetandA 10.1109/cvpr.2016.90.
BaselineNetwork,”inMedicalImageComputingandComputerAssisted [35] S.Zhao,Y.Wu,S.Wang,W.Ke,andH.Sheng,“MaskGuidedSpatial-
Intervention-MICCAI2022.pp.391-400. Temporal Fusion Network for Multiple Object Tracking,” in 2022
[18] P. Jiang, J. Peng, G. Zhang, E. Cheng, V. Megalooikonomou, and H. IEEEInternationalConferenceonImageProcessing(ICIP),Bordeaux,
Ling,“Learning-basedautomaticbreasttumordetectionandsegmenta- France,Oct.2022.DOI:10.1109/icip46576.2022.9898054.
tioninultrasoundimages,”in20129thIEEEInternationalSymposium [36] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang,
on Biomedical Imaging (ISBI), Barcelona, Spain, May 2012. DOI: “Unet++:Anestedu-netarchitectureformedicalimagesegmentation,”
10.1109/isbi.2012.6235878. inDeepLearninginMedicalImageAnalysisandMultimodalLearning
[19] L.CaiandY.Wang,“Aphase-basedactivecontourmodelforsegmenta- forClinicalDecisionSupport,LectureNotesinComputerScience,2018,
tionofbreastultrasoundimages,”in20136thInternationalConference pp.3-11.DOI:10.1007/978-3-030-00889-5 1.
on Biomedical Engineering and Informatics, Hangzhou, China, Dec. [37] P. Chao, C.-Y. Kao, Y. Ruan, C.-H. Huang, and Y.-L. Lin, “HarDNet:
2013.DOI:10.1109/bmei.2013.6746913. A Low Memory Traffic Network,” in 2019 IEEE/CVF International
[20] F. Kharajinezhadian, F. Yazdani, P. P. Isfahani, and M. Kavousi, Conference on Computer Vision (ICCV), Seoul, Korea (South), Oct.
“Automatic Breast Tumor Classification in Ultrasound Images Using 2019.DOI:10.1109/iccv.2019.00365.
Morphological Features and New Texture Analysis Based on Image [38] X. Zhao, L. Zhang, and H. Lu, “Automatic Polyp Segmentation via
Visibility Graph and Gabor Filters,” SN Computer Science, Oct. 2022, Multi-scale Subtraction Network,” in Medical Image Computing and
DOI:10.1007/s42979-022-01431-3. ComputerAssistedIntervention-MICCAI2021,LectureNotesinCom-
[21] Q.Huang,Y.Luo,andQ.Zhang,“Breastultrasoundimagesegmenta- puterScience,2021,pp.120-130.DOI:10.1007/978-3-030-87193-2 12.
tion: a survey,” International Journal of Computer Assisted Radiology [39] N. K. Tomar, A. Shergill, B. Rieders, U. Bagci, and D. Jha, “Tran-
andSurgery,pp.493-507,Mar.2017,DOI:10.1007/s11548-016-1513- sresunet: Transformer based resu-net for real-time colonoscopy polyp
1. segmentation,”arXivpreprintarXiv:2206.08985,2022.
[22] S.U.Khan,N.Islam,Z.Jan,K.Haseeb,S.I.A.Shah,andM.Hanif, [40] H. Wang, P. Cao, J. Wang, and O. R. Zaiane, “UCTransNet: Rethink-
“Amachinelearning-basedapproachforthesegmentationandclassifi- ing the Skip Connections in U-Net from a Channel-wise Perspective
cation of malignant cells in breast cytology images using gray level with Transformer,” Proceedings of the AAAI Conference on Artificial
co-occurrence matrix (GLCM) and support vector machine (SVM),” Intelligence,pp.2441-2449,Jul.2022,DOI:10.1609/aaai.v36i3.20144.
Neural Computing and Applications, pp. 8365-8372, Jun. 2022, DOI: [41] Y. Liang, X. Li, NavidH. Jafari, and J. Chen, “Video Object Segmen-
10.1007/s00521-021-05697-1. tationwithAdaptiveFeatureBankandUncertain-RegionRefinement,”JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 10
Neural Information Processing Systems,Neural Information Processing
Systems,Jan.2020.
[42] M. Zhang et al., “Dynamic Context-Sensitive Filtering Network for
Video Salient Object Detection,” in 2021 IEEE/CVF International
Conference on Computer Vision (ICCV), Montreal, QC, Canada, Oct.
2021.DOI:10.1109/iccv48922.2021.00158.
[43] Y. Su, J. Deng, R. Sun, G. Lin, H. Su and Q. Wu, “A Uni-
fied Transformer Framework for Group-based Segmentation: Co-
Segmentation, Co-Saliency Detection and Video Salient Object Detec-
tion,”inIEEETransactionsonMultimedia,vol.26,pp.313-325,2024,
doi:10.1109/TMM.2023.3264883.
[44] Lin,Junhao,etal.”ShiftingMoreAttentiontoBreastLesionSegmenta-
tioninUltrasoundVideos,”inMedicalImageComputingandComputer
AssistedIntervention-MICCAI2023.pp.497-507.