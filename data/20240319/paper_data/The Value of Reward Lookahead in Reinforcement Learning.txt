The Value of Reward Lookahead in Reinforcement Learning
NadavMerlis
FairPlayJointTeam,CREST,ENSAEParis
DorianBaudry
FairPlayJointTeam,CREST,ENSAEParis
InstitutPolytechniquedeParis
VianneyPerchet
FairPlayJointTeam,CREST,ENSAEParis
CriteoAILab
Abstract
In reinforcement learning (RL), agents sequentially interact with changing environments while
aimingtomaximizetheobtainedrewards. Usually,rewardsareobservedonlyafteracting,andso
thegoalistomaximizetheexpected cumulativereward. Yet,inmanypracticalsettings,reward
informationisobservedinadvance–pricesareobservedbeforeperformingtransactions;nearby
trafficinformationispartiallyknown;andgoalsareoftentimesgiventoagentspriortotheinteraction.
Inthiswork,weaimtoquantifiablyanalyzethevalueofsuchfuturerewardinformationthrough
thelensofcompetitiveanalysis. Inparticular,wemeasuretheratiobetweenthevalueofstandard
RLagentsandthatofagentswithpartialfuture-rewardlookahead. Wecharacterizetheworst-case
rewarddistributionandderiveexactratiosfortheworst-caserewardexpectations. Surprisingly,the
resultingratiosrelatetoknownquantitiesinofflineRLandreward-freeexploration. Wefurther
providetightboundsfortheratiogiventheworst-casedynamics. Ourresultscoverthefullspectrum
between observing the immediate rewards before acting to observing all the rewards before the
interactionstarts.
Keywords: ReinforcementLearning,RewardLookahead,CompetitiveRatio
1. Introduction
ReinforcementLearning(RL,SuttonandBarto,2018)istheproblemoflearninghowtointeract
withachangingenvironment. Thesettingusuallyconsistsoftwomajorelements: atransitionkernel,
whichgovernshowthestateoftheenvironmentevolvesduetotheactionsofanagent,andareward
giventotheagentforperforminganactionatagivenenvironmentstate. Agentsmustdecidewhich
actionstoperforminordertocollectasmuchrewardaspossible,takingintoaccountnotonlythe
immediaterewardgain,butalsothelong-termeffectsofactionsonthestatedynamics.
InthestandardRLframework,rewardinformationisusuallyobservedafterplayinganaction,
andagentsonlyaimtomaximizetheircumulativeexpected reward,alsoknownasthevalue(Jaksch
etal.,2010;Azaretal.,2017;Jinetal.,2018;Dannetal.,2019;ZanetteandBrunskill,2019;Efroni
etal.,2019b;SimchowitzandJamieson,2019;Zhangetal.,2021). Yet,inmanyreal-worldscenarios,
partialinformationaboutthefuturerewardisaccessibleinadvance. Forexample,whenperforming
transactions,pricesareusuallyknown. Innavigationsettings,rewardsaresometimesassociatedwith
traffic, which can be accurately estimated for the near future. In goal-oriented problems (Schaul
etal.,2015;Andrychowiczetal.,2017),thelocationofthegoalisoftentimesrevealedinadvance.
© N.Merlis,D.Baudry&V.Perchet.
4202
raM
81
]GL.sc[
1v73611.3042:viXraMERLISBAUDRYPERCHET
Thisinformationiscompletelyignoredbyagentsthatmaximizetheexpectedreward,eventhough
usingthisfutureinformationontherewardshouldgreatlyincreasetherewardcollectedbytheagent.
As an illustration, consider a driving problem where an agent travels between two locations,
aimingtocollectasmuchrewardaspossible. Inonesuchscenario,rewardsaregivenonlywhen
travelingfreeroads. Itwouldthenbereasonabletoassumethatagentsseewhetherthereistraffic
beforedecidinginwhichwaytoturnateveryintersection(‘one-steplookahead’). Inanalternative
scenario,theagentparticipatesinride-sharingandgainsarewardwhenpickingupapassenger. In
thiscase,agentsgaininformationonnearbypassengersalongthepath,notnecessarilyjustinthe
closestintersection(‘multi-steplookahead’). Finally,thedestinationmightberevealedonlyatthe
beginningoftheinteraction,andrewardisonlygainedwhenreachingit(‘fulllookahead’). Inall
examples,theadditionalinformationshouldbeutilizedbytheagenttoincreaseitscollectedreward.
Inthispaper,Weanalyzethevalueoffuture(lookahead)informationontherewardthatcould
be obtained by the agent through the lens of competitive analysis. More precisely, we study the
competitiveratio(CR)betweenthevalueofanagentthatonlyhasaccesstorewarddistributionsand
thatofalookaheadagentwhoseestheactualrewardrealizationsforseveralfuturetimestepsbefore
choosingeachaction. Ourcontributionsarethefollowing: (i)Givenanenvironmentanditsexpected
rewards,wecharacterizethedistributionthatmaximizesthevalueoflookaheadagents,forallranges
oflookaheadfromonesteptofulllookahead;thisdistributionthereforeminimizestheCR.(ii)We
derivetheworst-caseCRasafunctionofthedynamicsoftheenvironment(thatis,fortheworst-case
rewardexpectations). Surprisingly,theCRthatemergesiscloselyrelatedtofundamentalquantities
inreward-freeexplorationandofflineRL(Xieetal.,2022;Al-Marjanietal.,2023). (iii)Weanalyze
theCRfortheworst-possibleenvironment. Inparticular,tree-likeenvironmentsthatrequiredeciding
both when and where to navigate exhibit near-worst-case CR. (iv) Lastly, we complement these
resultsbypresentingdifferentenvironmentsandtheirCR,providingmoreintuitiontoourresults.
1.1. RelatedWork
Theideaofutilizinglookaheadinformationtoupdatetheplayedpolicyisrelatedtoacontrolconcept
called Model Predictive control (MPC, Camacho et al., 2007), also known as receding horizon
control. In complex control problems, it could be challenging to predict the system behavior in
longhorizonsduetoerrorsinthemodelornonlineardynamics. Tomitigatethis,MPCdesignsa
controlschemeformuchshorterhorizons,wherethemodelisapproximatelyaccurate,oftentimeson
asimplified(e.g.,linearized)model. Then,tocorrectthedeviationsduetomodelingerrors,MPC
continuouslyupdatesthecontrolleraccordingtotheactualsystemstate. Inourcontext,thelocalized
system estimates could be seen as lookahead information. Similar ideas have also been used for
planninginreinforcementlearningsettings(Tamaretal.,2017;Efronietal.,2019a,2020). Yet,these
conceptsaremainlyusedtoimproveplanningefficiencyandaccountfornonlinearities/disturbances
inthemodel. Incontrast,wefocusonnewinformationthatisrevealedtotheagentintheformof
futurerewardrealization;toourknowledge,suchamodelwasneverstudiedinthecontextofMPC.
Thespecialcaseofone-steplookahead,whereimmediaterewardsareobservedbeforemakinga
decision,hasbeenstudiedinvariousproblems. Possiblythemostfamousinstanceofsuchaproblem
istheprophetinequality. There,asetofknowndistributionsissequentiallyobserved,andagents
choose whether to either take a reward and end the interaction or discard it and move to the next
distribution(Correaetal.,2019b). Thiscouldbeformulatedasachainenvironmentwithtwoactions
–arewardingactionthatmovestoanabsorbingstateandanon-rewardingonethatmovesforward
2THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
in the chain. A generalization of the prophet problem to resource allocation over Markov chains
was studied in (Jia et al., 2023). To obtain a CR that is independent of the interaction length, the
authorsallowboththeonlineandofflinealgorithmstochoosetheirinitialstate. Inbothcases(and
manyotherproblems),theCRismeasuredbetweenaone-steplookaheadandafulllookaheadagent,
whichobservesallrewardsinadvance. Incontrast,wemeasuretheCRbetweenno-lookaheadagents
andallpossiblelookaheads,soourresultsarecomplementary.
Finally,Gargetal.(2013)studiedanotherrelatedresourceallocationmodel. Intheirwork,the
competitiveratioforMarkovDecisionProcessesismeasuredbetweenanonlineagentwithaccess
totheL-futurerewarddistributionsandtransitionprobabilities,versusanagentwhoobservesall
statistical information in advance. A similar adversarial notion is also presented specifically for
resourceallocation. Incontrast,weassumethatthedistributionsareknowntobothagentsandonly
theoracleobservesrewardrealizations.
Whiletheseresearchlinesdemonstratetheapplicabilityoflookaheadinformation,theyallfocus
ondifferentobjectivesthanours.
2. Preliminaries
2.1. MarkovDecisionProcesses
Weworkundertheepisodictabularreinforcementlearningmodel. Theenvironmentismodeledasa
MarkovDecisionProcess(MDP),definedbythetuple(S,A,H,P,R,µ),whereS isthestatespace
(|S| = S),Aistheactionspace(|A| = A),H ∈ Nisthehorizon,P isthetransitionkernel,Risthe
stochasticrewardandµ ∈ ∆ istheinitialstatedistribution. Atthefirsttimestep,aninitialstateis
S
generateds ∼ µ. Then,ateverytimesteph ∈ [H] ≜ {1,...,H},givenenvironmentstates ∈ S,
1 h
theagentperformsanactiona ∈ A,obtainsastochasticrewardR (s ,a )andtransitionstoastate
h h h h
s ∈ S withprobabilityP (s |s ,a ). Forbrevity,weusethenotationX = [H]×S ×A.
h+1 h h+1 h h
Weassumethatrewardsatdifferenttimestepsareindependent,butallowthemtobearbitrarily
correlatedbetweenstate-actionsatthesamestep. Wedenotetheexpectedrewardbyr (s,a)and
h
assume that the rewards are non-negative.1 Rewards and transitions are always assumed to be
mutually independent, and transitions are independent between rounds. While we focus on non-
stationary models, where the reward and transition distributions could depend on the timestep h,
our analysis techniques could be easily adapted to stationary models, where the distributions are
timestep-independent,andalltheproofsintheappendixalsostatetheresultsforstationarymodels.2
2.2. LookaheadPoliciesandValues
We assume w.l.o.g. that all rewards are generated before the interaction starts. We denote by
R = {R (s,a)} , the set of all rewards at timestep h and by RL = {R }h+L−1, the
h h s∈S,a∈A h t t=h
L-lookaheadrewardinformation,containingallrewardinformationforL-timestepsstartingfromh.
Byconvention,R0 istheemptyset. Alookaheadpolicyisdefinedasfollows.
h
Definition1 AlookaheadpolicyπL : [H]×S ×RSAL (cid:55)→ ∆ isapolicythatforeachtimesteph,
A
observesthestates andthelookaheadrewardinformationRL andgeneratesanactiona with
h h h
probabilityπL(a |s ,RL). ThesetofalllookaheadpoliciesisdenotedbyΠL.
h h h h
1.Thisassumptionisstandardwhentheperformanceismeasuredbyaratio,sinceotherwise,ratiosarenotwell-defined.
2.Instationaryenvironments,rewardexpectationshavetobeidenticalforalltimesteps;thisadditionalconstraintaffects
theresultswhenlookingattheworst-caserewardexpectations.
3MERLISBAUDRYPERCHET
Forexample,aone-steplookaheadpolicyobservestheimmediaterewardsatthecurrentstatebefore
acting,whileafulllookaheadpolicyhasaccesstoallrewardrealizationsbeforetheinteractionstarts.
WhenL = 0,thepolicyonlydependsonthestateandisMarkovian;wethereforedenoteΠM = Π0.
Thegoalofanyagentistomaximizeitscumulativereward,alsoknownasthevalue,VL,π =
(cid:104) (cid:105)
E (cid:80)H R (s ,a )|s ∼ µ,π . Forbrevity,weomittheconditioningontheinitialstatedistribution.
h=1 h h h 1
TheoptimalvaluegivenalookaheadLisVL,∗ = sup VL,πL . Ifwewanttoemphasizethat
πL∈ΠL
anenvironmentparameter(say,thetransitionkernelP)isfixed,weshallspecifyit,e.g.,VL,π(P,r).
Weanalyzetherelationbetweenthe‘standardvalue’ofanagentthatplaysoptimallyusingno
futureinformation(V0,∗)andalookaheadagentthatobservestheL-futurerewardsbeforeacting
(VL,∗). Formally, let D(r) be the set of all non-negative distributions with rewards expectations
r (s,a). TheL-lookaheadcompetitiveratio(CR)isdefinedas
h
V0,∗(P,r)
CRL(P,r) = inf . (1)
RH∼D(r)
VL,∗(P,r)
Thatis,thecompetitiveratioistheworst-possiblemultiplicativelossofthestandard(no-lookahead)
policy,comparedtoanL-lookaheadpolicy,givenfixedtransitionkernelandexpectedrewards. For
ratiostobewell-defined,wefollowtheconventionthatanydivisionbyzeroequals+∞.
Remark2 We emphasize that the reward distributions are known in advance to both the no-
lookahead and the L-step lookahead agents, in striking contrast to adversarial settings. In the
latter,therewardcouldbearbitraryandisonlygiventoanoracleagent. Inparticular,anyupper
boundonCRL(P,r)willalsoapplytoadversarialsettings.
Remark3 Withoutlookaheadinformation,P andr sufficetocalculatetheoptimalvalue(Sutton
andBarto,2018),soonecouldalsowriteCRL(P,r) = V0,∗(P,r) .
sup RH∼D(r)VL,∗(P,r)
WesimilarlystudytheL-lookaheadCRfortheworst-caserewardexpectations,definedas3
CRL(P) = inf CRL(P,r).
r ∈[0,1]SA
h
Finally,westudytheCRfortheworst-caseenvironmentP andinitialstatedistributionµ,denoted
byCRL. Inparticular,weshowthatstationaryenvironmentsachievenear-worst-caseCR.
2.3. OccupancyMeasures
Occupancy measures are the visitation probabilities of an agent in different state-actions. In par-
ticular, for any (potentially lookahead) policy, we define dπ(s) = Pr{s = s} and dπ(s,a) =
h h h
Pr{s = s,a = a},whererandomnessisw.r.t. bothtransitions,rewardsandinternalpolicyran-
h h
domization,giventhatactionsaregeneratedfromthepolicyπ ∈ ΠL. Forh = 1,thestatedistribution
onlydependsontheinitialstatedistributionµ,andweusedπ(s),d (s)andµ(s)interchangeably.
1 1
Wealsodefinetheconditionaloccupancymeasureasdπ(s|s = s′) = Pr{s = s|s = s′}forsome
h t h t
t ≤ handsimilarlyusedπ(s,a|s = s′). Intuitively,thisisthereachingprobabilityfromstates′ at
h t
timettoastatesattimehwhenplayingapolicyπ. Withoutlookaheadinformation,itiswell-known
3.Whilewelimittheexpectationsto[0,1],thesameresultswouldholdforr ∈RSA(seeRemark13intheappendix).
h +
4THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
thatthesetofoccupancymeasuresinducedbyMarkovianpoliciesisaconvexcompactpolytope
(Altman,2021),andthevalueofanyMarkovianpolicycouldbeexpressedusingoccupanciesby
 
(cid:34) H (cid:35)
(cid:88) (cid:88)
V0,π = E R h(s h,a h) = E  1{s h = s,a h = a}R h(s,a)
h=1 (h,s,a)∈X
(cid:88) (cid:88)
= Pr{s = s,a = a}E[R (s,a)] = dπ(s,a)r (s,a) = dπTr. (2)
h h h h h
(h,s,a)∈X (h,s,a)∈X
Finally, denote the optimal reaching probability to a state s ∈ S as d∗(s) = max dπ(s).
h π∈ΠM h
Notice that rewards and transitions are independent, so reward information does not affect the
optimal reaching probability and it is sufficient to look at Markovian policies. Moreover, after
reaching a state s, an agent could always deterministically choose an action a, so d∗(s,a) =
h
max dπ(s,a) = d∗(s). Similarly,wedefinetheoptimalconditionalreachingprobabilityas
π∈ΠM h h
d∗(s|s = s′) = max dπ(s|s = s′),andasthefornon-conditionaloccupancymeasures,we
h t π∈ΠM h t
havethatd∗(s,a|s = s′) = d∗(s|s = s′).
h t h t
3. CompetitivenessVersusFullLookaheadAgents
BeforeanalyzingtheCRforthefullrangeoflookaheadvalues,westartbystudyingthefulllookahead
case, where all rewards are observed before the interaction starts. This regime is applicable, for
example, in goal-oriented problems, where goals are given to the agent before an episode starts
(Andrychowicz et al., 2017). Notably, we show a link between the CR for the worst-case reward
expectations,CRH(P),andexistingcomplexitymeasuresinofflineRLandreward-freeexploration.
Whiletheresultsofthissectionwilllaterbecoveredbythemoregeneralmulti-steplookahead,this
casegivesvaluableinsightsontheworst-casedistributions. Moreover,muchoftheprooftechniques
presentedinthissectionwilllaterbeusedtoprovetheresultsforthemulti-steplookahead.
Whenallrewardsareobservedbeforetheinteractionstarts,eachinstantiationoftherewardis
equivalent to an RL problem with known deterministic rewards. In particular, the optimal policy
giventherewardisMarkovian,andusingthevalueformulationinEquation(2),wehave
   
(cid:88) (cid:88)
VH,∗(P,r) = E max dπ h(s,a)R h(s,a) ≤ E  max dπ h(s,a)R h(s,a)
π∈ΠM π∈ΠM
(h,s,a)∈X (h,s,a)∈X
(cid:88) (cid:88)
= d∗(s)E[R (s,a)] = d∗(s)r (s,a). (3)
h h h h
(h,s,a)∈X (h,s,a)∈X
Atfirstglance,thisboundseemsextremelycrude–theagentoptimallynavigatestoeachstateand
collects its expected reward. Yet, at a second glance, it gives a clear intuition on the worst-case
distribution; a situation where only one reward is realized in every episode, and a full lookahead
agentcouldoptimallynavigatetocollectit. Whilewecannotfullyenforceasinglerewardrealization
(duetotheindependenceofrewardsindifferenttimesteps),wecanapproximatethisbehaviorby
focusingonlong-shotdistributions(HillandKertz,1981).
Definition4 Rewardshavelong-shotdistributionswithparameterϵ ∈ (0,1)andexpectationr if
(cid:26)
r (s,a)/ϵ w.p. ϵ
∀h ∈ [H],s ∈ S,a ∈ A : R (s,a) = h
h 0 w.p. 1−ϵ
5MERLISBAUDRYPERCHET
independentlyforallh,s,a. WealsousethenotationR ∼ LS (r).
ϵ
Noticethatforanygivenϵ,long-shotdistributionsarebounded;thus,long-shotrewardscouldalways
bescaledtobesupportedby[0,1]withoutaffectingtheCR.Moreover, whenϵ ≪ 1/SAH, with
highprobability,atmostasinglerewardwillberealized,andtheboundinEquation(3)isachieved
inequalityasϵ → 0. Formally,theCRversusafulllookaheadagentischaracterizedasfollows:
Theorem5 [CRversusFullLookaheadAgents]
(cid:80) dπ(s,a)r (s,a)
1. Worst-casedistributions:CRH(P,r) = max (h,s,a)∈X h h .
π∈ΠM (cid:80) d∗(s)r (s,a)
(h,s,a)∈X h h
2. Worst-caserewardexpectations:CRH(P) = max min dπ h(s,a) .
π∈ΠM (h,s,a)∈X d∗(s)
h
3. Worst-caseenvironments: For all environments, CRH ≥ max(cid:8) 1 , 1 (cid:9) . Also, for any
SAH AH
δ ∈ (0,1)thereexiststationaryenvironmentswithrewardsover[0,1]s.t. ifS = An+1for
n ∈ {0,...,H −1},thenCRH(P,r) ≤ 1+δ ,andifS ≥ AH−1,then
(H−log (S−1))·(A−1)(S−1)
A
CRH(P,r) ≤ 1+δ.
AH
ProofSketch (seeAppendixAforthefullproof).
PartI. RecallingRemark3andEquation(2),toprovethefirstpartoftheproposition,oneonly
needstocalculatethefulllookaheadvaluefortheworst-casedistribution. Anupperboundforthis
value is already given in Equation (3); we directly calculate the value for long-shot distributions
LS (r)andshowthatthisboundisachievedatthelimitofϵ → 0.
ϵ
PartII. TheproofofthesecondpartofthetheoremutilizesthepreviouslycalculatedCRH(P,r)
tooptimizefortheworst-caseexpectations. Thisisdoneusingtheminimaxtheorem,exchanging
therewardminimizationandthepolicymaximization. Tomaketheinternalmaximizationproblem
concave,wemovefromthespaceofMarkovianpoliciestothesetofoccupancymeasuresinducedby
Markovianpolicies,whichisconvex(Altman,2021). Tomaketherewardminimizationconvex,we
showthatthedenominatorcanbeconvertedtotheconstraint(cid:80) d∗(s)r (s,a) = 1. Then,
(h,s,a)∈X h h
theminimaxtheoremcanbeapplied,andweexplicitlysolvetheresultingoptimizationproblem. The
formalapplicationoftheminimaxtheoremanditssolutionisdoneinLemma12intheappendix.
PartIII. Theproofofthefinalstatementisfurtherdividedintotwoparts.
Lower bounding CRH. The lower bound CRH ≥ 1/AH is inductively achieved from the
dynamicprogrammingequationsforboththeno-lookaheadandfulllookaheadvalues. Thebound
CRH ≥ 1/SAH isobtainedbychoosingaspecificpolicyπ ∈ ΠM andsubstitutinginCRH(P):
the Markovian policy whose occupancy is dπu(s,a) = 1 (cid:80) dπ h∗ ′,s′,a′(s,a), where
h SAH (h′,s′,a′)∈X h
π∗ ∈ ΠM isapolicythatmaximizesthereachingprobabilityto(h,s,a) ∈ X.
h,s,a
Upper bounding CRH – designing a worst-case environment. We show that a modified tree
graphachievesanear-worst-casecompetitiveratio. Intree-basedMDPs,eachstaterepresentsanode
inatree,withtheinitialstateasitsroot,andactionstaketheagentdownwardsthroughthetree. In
ourexample,rewardsarelong-shotslocatedattheleavesofsuchtrees. However,thisstructure,by
itself,doesnotleadtotheworst-casebound. Intuitively,astandardRLagentwouldnavigatetothe
leafwiththemaximalexpectedreward,whileanagentwithafulllookaheadwouldnavigatetothe
leaf with the highest reward realization. Since there are at most S leaves with A actions in each,
6THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
thiswouldleadtoCRH(P) ≈ 1 . Thisisimprovedbyasimplemodification: attherootofthe
SA
tree,weallocateoneactionto‘delay’theentrancetothetreeandstayintheroot(asillustratedin
Figure2intheappendix). Whileagentswithoutlookaheadhavenoincentivetousethisaction,afull
lookaheadagentcouldpredictwhenarewardwillberealizedandenterthetreeatatimingthatallows
itscollection. WhenH islargeenough(comparedtothetreedepth),thisallowsthefulllookahead
agenttohaveapproximatelyH attemptstocollectarewardandleadtotheadditionalH-factor(up
tologfactors). TheproofcouldbeextendedtoanyvalueofS byallowingthetreetobeincomplete–
wereferthereaderstotheremarkattheendofProposition10intheappendixformoredetails.
Surprisingly,theCRfortheworst-caserewardexpectationCRH(P)istheinverseofaconcen-
trabilitycoefficientthatappearsinmanydifferentRLsettings,calledthecoverabilitycoefficient. In
particular, it affects the learning complexity in both online and offline RL settings, where agents
must learn to act optimally either based on logged date or interaction with the environment (Xie
et al., 2022).4 It also has a central role in reward-free exploration, where agents aim to learn the
environmentsothattheycanperformwellforanygivenrewardfunction(Al-Marjanietal.,2023).
Weemphasizethatthelookaheadsettingisfundamentallydifferent–weassumethatallagentshave
exactinformationonboththedynamicsandrewarddistributionsandaskaboutthemultiplicative
performanceimprovementduetoadditionalknowledgeonrewardrealization. Incontrast,inlearning
settings,themaincomplexityisusuallyinlearningthedynamics,andtherewardsareoftentimes
assumedtobedeterministic. Moreover,theanalyzedquantitiesareeitherregretmeasuresorsample
complexity,whichcannotbedirectlylinkedtothecompetitiveratio.
The last part of Theorem 5 shows that tree-like environments with a delaying action at their
root exhibit worst-case CR. Similar delay mechanisms were previously used to prove regret and
PAClowerboundsfornonstationaryMDPs(Dominguesetal.,2021;Tirinzonietal.,2022),though
with a major difference – in previous works, a nonstationary reward distribution is used to force
theagenttolearnwhentotraversethetreeandwheretonavigate,andtherewardistime-extended
(obtainedforΩ(H)rounds). Incontrast,ourformulationisfullystationaryandarewardcanonlybe
collectedonce. Still,thelookaheadagentcanusethedelaytolinearlyincreasethereward-collection
probability,withoutanyneedtocreatetime-extendedrewards.
4. CompetitivenessVersusMulti-StepLookaheadAgents
WenowgeneralizetheresultsofSection3andanalyzethecompetitiveratiocomparedtoL-lookahead
agents, for any possible lookahead range L ∈ [H]. We also give special attention to the case of
one-steplookahead,wheretheimmediaterewardsarerevealedbeforetakinganaction.
Inspiredbythefulllookaheadcase,wefocusonlong-shotrewards. Forsuchrewards,anagent
wouldexpecttoseenomorethanasinglerewardduringanepisode,whichwouldonlybediscovered
L-stepsinadvance. Assuch,areasonablestrategywouldplayaMarkovianpolicythatmaintainsa
‘favorable’statedistribution,suchthatwheneverandwhereverafuturerewardisrealized,theagent
couldoptimallynavigatetoit. Lettingt (h)bethetimestepwheretheh-steprewardsarerevealed
L
toanL-lookaheadagent,thiscorrespondswiththefollowingworst-casevalue:
4.AsubtledifferencebetweenthecoefficientsiswhethertheoutermaximumisoverallvalidMarkovianoccupancy
measuresorallpossiblestate-actiondistributions;see(Al-Marjanietal.,2023,Section2.3)forfurtherdiscussion.
7MERLISBAUDRYPERCHET
Proposition6 ForanyL ∈ [H],lett (h) = max{h−L+1,1}. Then,itholdsthat
L
(cid:88) (cid:88)
sup VL,∗(P,r) = max r (s,a) dπ (s′)d∗(s|s = s′)
RH∼D(r) π∈ΠM
(h,s,a)∈X
h
s′∈S
tL(h) h tL(h)
The proof can be found at Appendix B. It is comprised of calculating the value of long-shot
rewardsR ∼ LS (r)atthelimitwhenϵ → 0andthenshowingthatthesamequantityalsoservesas
ϵ
anupperboundofthevalueforallrewarddistributions.
Forfulllookahead,wehavet (h) = 1,anddπ becomestheinitialstatedistributionµ. This
H tH(h)
leadstothesamevalueasinEquation(3). ThesecondextremityiswhenL = 1andt (h) = h. Then,
1
theconditionaloccupancyisd∗(s|s = s′) = 1{s = s′}andwegetthesimplifiedexpression
h tL(h)
(cid:88)
sup V1,∗(P,r) = max r (s,a)dπ(s). (4)
h h
RH∼D(r) π∈ΠM
(h,s,a)∈X
Notably, this is the value of an agent that collects the rewards of all the actions in visited states
(regardlessoftheactionitactuallyplayed)buthasnolookaheadinformation.
RecallingRemark3,onecoulduseProposition6todirectlycalculateCRL(P,r). This,inturn,
allowsanalyzingtheworst-caserewardexpectationsandenvironment,asstatedinthefollowing:
Theorem7 [CRversusMulti-StepLookaheadAgents]
ForanyL ∈ [H],lett (h) = max{h−L+1,1}. Then,itholdsthat
L
1. Worst-casedistributions:CRL(P,r) =
max π∈ΠM(cid:80) (h,s,a)∈Xr h(s,a)dπ h(s,a)
.
max π∈ΠM(cid:80) (h,s,a)∈Xr h(s,a)(cid:80) s′∈Sdπ tL(h)(s′)d∗ h(s|s tL(h)=s′)
2. Worst-caserewardexpectations:
dπ(s,a)
CRL(P) = min max min h .
π∗∈ΠMπ∈ΠM(h,s,a)∈X (cid:80) s′∈Sdπ tL∗ (h)(s′)d∗ h(s|s tL(h) = s′)
(cid:110) (cid:111)
3. Worst-caseenvironments:Forallenvironments,CRL ≥ max 1 , 1 . Also,for
SAH (H−L+1)AL
anyδ ∈ (0,1)thereexiststationaryenvironmentswithrewardsover[0,1]s.t. ifS = An+1
for n ∈ {0,...,L−1}, then CRL(P,r) ≤ 1+δ , and if S ≥ AL +1,
(H−log (S−1))·(A−1)(S−1)
A
thenCRL(P,r) ≤ 1+δ .
(H−L+1)(AL−1)
ProofSketch (seeAppendixBforthefullproof). Thefirstpartofthetheoremisadirectresultof
Proposition6andRemark3. Forthesecondpart,wefirstrewrite
(cid:80) r (s,a)dπ(s,a)
CRL(P,r) = min max (h,s,a)∈X h h ,
π∗∈ΠMπ∈ΠM (cid:80) (h,s,a)∈X r h(s,a)(cid:80) s′∈Sdπ tL∗ (h)(s′)d∗ h(s|s tL(h) = s′)
andasinthefulllookaheadcase,weapplytheminimaxtheoremusingLemma12. However,direct
application would require calculating the infimum over π∗ ∈ ΠM, and not a minimum. Thus,
comparedtothefulllookahead,wealsoneedtoprovethattheminimumisobtainedinthisset. We
dosoinLemma14,relyingonthesetofoccupancymeasuresbeingaconvexcompactpolytope.
Inthelastpart,weusethesametreeexampletoupperboundCRL(P,r). Thelowerboundis
provenusingareductionfromthefulllookaheadbound. Inparticular,theboundof1/SAH trivially
8THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
holdsfromthefulllookaheadcase. Forthesecondlowerbound,wedeviseaMarkovianpolicyπ
u
suchthatfortheappropriatechoiceofrewardfunctionsri,weprovethat
V0,πu(P,r) 1 (cid:40) max π∈ΠM(cid:80) (s,a)∈S×A(cid:80)i h+ =L i−1r hi(s,a)dπ h(s,a|s i = s′)(cid:41)
≥ min .
VL,∗(P,r) H −L+1 i∈[H−L+1], (cid:80) (cid:80)i+L−1ri(s,a)d∗(s|s = s′)
s′∈S (s,a)∈S×A h=i h h i
EachofthetermsisthecompetitiveratioversusafulllookaheadagentwithhorizonLthatstarts
actingats = s′. Hence,byTheorem5,alltermsareboundedby 1 . Toelaborate,therewardri
i AL
limitstherewardonlytothenewtimestepsthelookaheadagentgetstoobservewhenitreachesstepi.
Thepolicyπ isamixture(intheoccupancyspace)ofpoliciesπ thatstartbyplayingtheMarkovian
u i
policythatmaximizesthevalueofProposition6,uptotimestepi,andthenmaximizesri.
Theorem 7 extends the full lookahead results of Theorem 5 and tightly characterizes the CR for
the full spectrum of lookaheads, both as a function of the environment and for the worst-case
environments. Noticethateventhoughlookaheadpoliciesarehighlynon-Markovian,allboundsare
expressedusingMarkovianpolicies.
One-steplookahead. Inthecasewheretheimmediaterewardisobservedbeforeacting,Theorem7
provesthatevenfortheworst-caseenvironment,CR1 = Θ(cid:0) 1 (cid:1) ,namely,independentofthesize
HA
ofthestate-space. Moreover,foranytransitionkernelP,theCRisgivenby
dπ(s,a)
CR1(P) = min max min h . (5)
π∗∈ΠMπ∈ΠM(h,s,a)∈X
dπ h∗(s)
WhilethecoverabilitycoefficientofCRH(P)requiresapolicyπ tocoverallstatessimultaneously
inproportiontotheiroptimalreachingprobability,CR1(P)providesaweakercoverabilitynotion;it
requiresbeingabletocoveranypre-knownstate-distributioninducedbyaMarkovpolicyπ∗. We
emphasizethatπ mustcoverthisdistributionusingallactions,soimitatingthebehaviorofπ∗ might
bechallenging–witharatioof1/AH astheworstcase.
Thus,CR1(P)couldbeseenasanintermediatepointbetweenthecoverabilitycoefficientand
single-policycoverability(Xieetal.,2022),definedbytheratiobetweenthestate-actionoccupancy
oftheoptimalpolicyandasingledatadistribution. Yet,Xieetal.(2022)arguethatthisnotionis
tooweaktoallowanyguarantees. Itisofinteresttoinvestigatewhetherourrefinednotion,which
requirescoveringallvalidstatedistributions,mitigatestheissuestheypresentandallowsderiving
meaningfulresultsinofflineandonlineRL.
In general, one could interpret the ratios CRL(P) as a class of decreasing5 (inverse) concen-
trabilitycoefficients,startingfromthecoverabilityofallpre-knownstatedistributions(CR1(P))
andendingwiththecoverabilitycoefficient(CRH(P)). Thus,itisintriguingtofurtherstudythe
connectionofthesevaluestootherdomainsinwhichconcentrabilitynaturallyarises.
5. Examples
WenowpresentseveralMDPstructuresandanalyzetheircompetitiveratioforvariouslookaheads.
5.Thesequenceisdecreasingbydefinitionbecauseincreasingthelookaheadonlyextendsthepolicyclass.
9MERLISBAUDRYPERCHET
(a)ChainMDP:agentsstartattheheadofachain (b)GridMDP:agentsstartatthebottom-
andcaneithermoveforwardinthechainor left corner of an n×n grid and can
transitiontoanabsorbingterminalstate. moveeitheruporright,untilendingat
thetop-rightcornerafter2n−1steps.
Figure1: Examples: CRforgridandchainenvironments.
Disguisedcontextualbandit(Al-Marjanietal.,2023). Maybethemostbasicscenarioiswhen
actionsdonotaffectthetransitions,i.e.,P (s′|s,a) = P (s′|s)forallpossible(h,s,a,s′). Specifi-
h h
cally,thestatedistributionisindependentoftheplayedpolicy–thereexistsanoccupancymeasure
d suchthatforallpolicies,dπ(s) = d (s). Thus,italsoholdsthatd∗(s) = d (s),and
h h h h h
dπ(s,a) d (s)π (a|s) 1
CRH(P) = max min h = max min h h = max minπ (a|s) = .
π∈ΠMh,s,a d∗ h(s) π∈ΠMh,s,a d h(s) π∈ΠMh,s,a h A
Thelastequalityholdssinceπ (a|s) ∈ ∆ . Usingthesamearguments,onecouldalsoobtainthis
h A
CRforone-steplookahead,sobythemonotonicityoftheCRinthelookahead,CRL(P) = 1 forall
A
L ∈ [H]. Thisistobeexpected–withoutcontroloverthedynamics,thebestlookaheadagentscould
doistomaximizeimmediaterewards,andanyadditionallookaheadinformationisuseless. Then,in
eachstate,knowingtherealizationcanonlyincreasetherewardbyafactorof
E[maxaR h(s,a)]
≤ A.
maxaE[R h(s,a)]
Delayed trees. This is the example described in the proofs of the main results, also detailed
in Proposition 10 and depicted in Figure 2. In such environments, we get a worst-case CR of
(cid:16) (cid:110) (cid:111)(cid:17)
CRL(P,r)=Θ max 1 , 1 . Thesetreesareanextremecasewherelookaheadinfor-
(H−L)AL SAH
mationisnotonlyusedtocollectimmediaterewardsbutrathertonavigatetolong-termrewards.
ChainMDPs. Wegobacktoabandit-likescenarioandaddlimitedcontrolonthedynamics,inthe
formofachain. Theagentstartsattheheadofthechain(s ), andateachnodek ofthechain, it
1
couldchoosetoadvancetothenextnodebytakingtheactiona = a ortomovetoanabsorbing
1
terminalstates bytakinganyotheraction. TheenvironmentisdepictedinFigure1(a).
T
One special problem that falls into this structure is the prophet inequality problem. In partic-
ular, assume that reward can only be obtained when moving from the chain to the terminal state
(∀k,a, r (s ,a ) = r (s ,a) = 0). Thus, at each node of the chain, the agent chooses whether
h k 1 h T
tocollectarewardandeffectivelyendtheinteractionordiscarditandmoveforwardinthechain.
In other words, the problem becomes an optimal-stopping problem. As such, it is reasonable to
10THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
allowtheagenttoseetheinstantaneousrewardsbeforedecidingwhethertostop,leadingtoone-step
lookaheadagents. Thisproblemhasnumerousapplications,especiallyinthecontextofposted-price
mechanisms(Correaetal.,2017,2019a). AclassicalresultisthattheCRbetweenone-steplookahead
andfulllookaheadagentsisalwaysboundedby1/2(HillandKertz,1981).
Assumingthisrewardstructurewiththeworst-caserewarddistribution,thefulllookaheadagent
couldreachallrewardsandcollectthem,thuscollectingVH,∗(P,r) = (cid:80)H (cid:80) r (s ,a)(asin
k=1 a∈A k k
Equation(3)). Similarly,aone-steplookaheadagentcouldmoveforwardinthechainusingthepolicy
π (s ) = a whileeffectivelycollectingallrewardsandachievingthesamevalue(seeEquation(4)).
k k 1
Incontrast,ano-lookaheadagentwouldhavetochooseasinglerewardtocollect,obtainingavalue
ofV0,∗(P,r) = max r (s ,a). TheresultingCRforthisrewardstructurewouldbe
k∈[H],a∈A k k
max r (s ,a) 1
CRH(P,r) = CR1(P,r) = k∈[H],a∈A k k ≥ ,
(cid:80)H (cid:80) r (s ,a) (A−1)H
k=1 a∈A k k
wheretheinequalityissincethereareonlyA−1rewardingactions,andequalityisachievedwhenall
expectedrewardsareequal. Notably,therewardstructureintheprophetproblemisnear-worst-case;
one could verify that for chain MDPs, it holds that CRH(P) ≥ (cid:0) 1− 1(cid:1) 1 . This is due to the
e AH
secondpartofTheorem5,usingthefollowingpolicy: forallchainstatesk ∈ [H],moveforwardw.p.
π (a |s ) = 1− 1 and play any other action i > 1 w.p. π (a |s ) = 1 . At the absorbing
k 1 k H k i k (A−1)H
states ,playuniformlyπ (a |s ) = 1/A. Thissimpleexampleprovidestwoimportantinsights.
T h i T
1. Hardnessversusone-steplookahead: chainMDPsexhibittheworst-caseCRversusone-step
lookaheadagents. Acentralreasonisthattomovetowardsrewardingstates(forwardinthe
chain),agentsmusttakenon-rewardingactions(a )–thereisatradeoffbetweengathering
1
instantaneousrewardsandmovingtofuturerewardingstates.
2. Easinessversusfulllookahead: aspreviouslymentioned,theCRbetweenone-stepandfull
lookaheadagentsisthewell-knownprophetinequalityandisatleast1/2;Inotherwords,for
chainMDPs,theinformation-gainfromone-step-tofulllookaheadismarginalcomparedto
thevalueofone-stepversusno-lookahead. Thisismainlybecausenavigatingtorewarding
states is especially easy in chain MDPs – the agent only has to move forward. In contrast,
inenvironmentswherenavigatingtorewardingstatesisdifficult(e.g.,thetreeenvironment
describedinthemainresults),thereisasubstantialgaintothefulllookahead.
TheseinsightsmotivatetwonaturalassumptionsthatreducetheCR.
Denserewards. Assumethatinallstateswithnon-zerorewards,itholdsthat
maxar h(s,a)
≤ C.
minar h(s,a)
Then,agentscouldnavigatetorewardingfuturestatesandstillcollectrewards,mitigatingtheissue
observedinthechainMDPs. Lettingπ∗ ∈ argmax (cid:80) r (s,a)dπ(s),wehave
π∈ΠM (h,s,a)∈X h h
max (cid:80) r (s,a)dπ(s,a) (cid:80) r (s,a)dπ∗(s,a)
CR1(P,r) = π∈ΠM (h,s,a)∈X h h ≥ (h,s,a)∈X h h
max (cid:80) r (s,a)dπ(s) (cid:80) r (s,a)dπ∗(s)
π∈ΠM (h,s,a)∈X h h (h,s,a)∈X h h
(cid:80)
(h,s,a)∈X
A1
C
(cid:80) a′∈Ar h(s,a′)dπ h∗(s,a) (∗) 1
≥ ≥ ,
(cid:80) r (s,a)dπ∗(s) AC
(h,s,a)∈X h h
where(∗)issince(cid:80) dπ∗(s,a) = dπ∗(s). Thus,denserewardsremovethehorizondependencein
a h h
theCR,andforsmallC,wegetasimilarCRasinthedisguisedcontextualbanditproblem.
11MERLISBAUDRYPERCHET
ErgodicMDPs. Onewaytomakethenavigationtaskeasieristolimitthecontroloftheagenton
thestate. In(Al-Marjanietal.,2023),theauthorssuggestlookingatMDPswhosetransitionkernels
arenear-uniform. Formally,for0 < β < α < 1,theydefinedthefamilyoftransitions
(cid:40) (cid:88)S 1−Sβ−1(cid:41)
P = q ∈ RS : q = 1,maxq ≤ Sα−1,minq ≥ ,
α,β + i i i i i S −1
i=1
and assumed that P (·|s,a) ∈ P for all h,s,a. As α goes to zero, the transition distribution
h α,β
becomes uniform, while at the limit of α,β → 1, this becomes the set of all possible transition
kernels. Underthisassumption,theyprovethatthecoverabilitycoefficientisboundedbySαAH (see
theendoftheproofofLemma38ofAl-Marjanietal.2023),whichimpliesthatCRH(P) ≥ 1 .
SαAH
(cid:104) (cid:105)
In particular, if for all h,s,a, P (s′|s,a) ∈ 1−C/S , C , then CRH(P) ≥ 1 : independent of
h S−1 S CAH
thesizeofthestate-space. Finally,intheirproof,Al-Marjanietal.2023showthatdπ(·) ∈ P for
h α,β
allpoliciesandtimesteps. SubstitutingtoEquation(5)(andusingtheuniformpolicyforπ)directly
leadstoCR1(P) ≥ 1−Sβ−1 ,potentiallyimprovingtheworst-caseenvironmentwhenSα ≤ H.
ASα
Grid MDPs We end this section by analyzing a navigation example, where an agent navigates
fromonecornerofann×ngridtotheoppositecorner("NavigatinginManhattan",alsoillustrated
in Figure 1(b)). Due to space limits, we briefly describe the results while fully proving them in
AppendixC.2. Thisexampledirectlygeneralizesthechainexamplewithaddednavigationdifficulty;
byenforcingzerorewardsforallstatesabovethebottomrow,weeffectivelygetachainMDPof
horizonn. Asadirectresult, weimmediatelygetthatCR1(P) = Θ(1)andCRH(P) = O(1).
H H
Surprisingly,thisboundistight–addingoneadditionaldimensiontotheproblemisjustasdifficult
asachain. Likechains,someofthedifficultycomesfromsparsityinthereward,butevenwhenall
rewardshaveunitexpectations,weshowthatCRL(P) = Θ(1). Thisimpliesthattheproblemhas
L
additionalhardnessduetotheneedfornavigation,whichisthesameorderofmagnitudeastheone
duetosparsereward. Asafinalremark,weshowthattheratiobetweenone-steplookaheadandfull
lookaheadingridMDPsisatmostO(1). Thismightbecounter-intuitiveatfirst,astheworst-case
H
CRversuseitherofthemisΘ(1). Infact, thisispossiblesincetheworst-caseenvironmentsare
H
different;whencompetingwithone-steplookaheadagent,thehardnesscomesfromrewardsparsity,
whileversusfulllookahead,itisalsoduetonavigationissues. Theone-steplookaheadagentcannot
useitsinformationtonavigate,soithasthesameCRof1/H astheno-lookahead.
6. ConclusionsandFutureWork
We studied the value of future reward lookahead information in tabular reinforcement learning
throughthelensofcompetitiveanalysis. WecharacterizedtheCRfortheworst-casedistributions,
rewardexpectationsandtransitionkernelsforthefullrangeofpossiblelookahead. Wealsoshowed
the connection between the resulting CR and concentrability coefficients from the literature of
offlineandreward-freeRL.Wefindtheappearanceofthesamecoefficientsinseeminglycompletely
differentRLproblemsintriguingandwarrantsfurtherstudy.
While we took the first step in analyzing competitiveness in RL, various other competitive
measurescouldbestudied. Onenaturalalternativewouldbetostudytransitionlookahead,where
agentsobservefuturetransitionrealizations. Webelievethattheresultswouldgreatlydifferfrom
ours; indeed, even with one-step lookahead, the CR can be exponentially small (as we prove in
12THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
AppendixC.3). Anotherrelevantcompetitivitymeasureistocompareanagentwithpredictionsofthe
futurerewardstoagentswithexactlookaheadinformation. Thismodelstherealisticscenariowhere
agentsgetapproximateinformationonfuturerewardsandwanttoutilizeittoimproveperformance.
Finally,asintheprophetproblem,onecouldanalyzetheCRbetweenmulti-steplookaheadtofull
lookaheadagents. Weleaveallthesedirectionsforfuturework.
Finally,wefocusontheCRfortheworst-casedistribution,whichallowsustoderivetheexact
valueoflookaheadagents. However,planningwithlookaheadforgeneralrewarddistributioncanbe
challenging. Forfulllookahead,onecanperformstandardplanningusingrewardrealization,making
planningtractable. Withone-steplookahead,itispossibletowriteBellmanequationsforthevalue,
buteachcalculationdependsonthefulldistributionofthereward,makingithardtocalculate. For
multi-step lookahead, there is no clear way to perform planning without incorporating the future
rewardsintothestate,renderingtheplanningexponential. Whileexactplanningmightbeintractable,
itcouldbepossibletodevisemethodsforapproximateplanning,butweleavethisforfuturework.
Acknowledgments
We thank Simon Mauras and Jose Correa for the helpful discussions. This project has received
fundingfromtheEuropeanUnion’sHorizon2020researchandinnovationprogrammeunderthe
Marie Skłodowska-Curie grant agreement No 101034255. Dorian Baudry thanks the support of
ANR-19-CHIA-02SCAI.
References
AymenAl-Marjani,AndreaTirinzoni,andEmilieKaufmann. Activecoverageforpacreinforcement
learning. In Proceedings of Thirty Sixth Conference on Learning Theory, volume 195, pages
5044–5109.PMLR,2023.
EitanAltman. ConstrainedMarkovdecisionprocesses. Routledge,2021.
MarcinAndrychowicz,FilipWolski,AlexRay,JonasSchneider,RachelFong,PeterWelinder,Bob
McGrew,JoshTobin,OpenAIPieterAbbeel,andWojciechZaremba. Hindsightexperiencereplay.
Advancesinneuralinformationprocessingsystems,30,2017.
Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for rein-
forcementlearning. InInternationalConferenceonMachineLearning,pages263–272.PMLR,
2017.
EduardoFCamacho,CarlosBordons,EduardoFCamacho,andCarlosBordons. Modelpredictive
control. Springer,2007.
JoséCorrea,PatricioFoncea,RubenHoeksma,TimOosterwijk,andTjarkVredeveld. Postedprice
mechanismsforarandomstreamofcustomers. InProceedingsofthe2017ACMConferenceon
EconomicsandComputation,pages169–186,2017.
JoséCorrea,PaulDütting,FelixFischer,andKevinSchewior. Prophetinequalitiesforiidrandom
variablesfromanunknowndistribution.InProceedingsofthe2019ACMConferenceonEconomics
andComputation,pages3–17,2019a.
13MERLISBAUDRYPERCHET
Jose Correa, Patricio Foncea, Ruben Hoeksma, Tim Oosterwijk, and Tjark Vredeveld. Recent
developmentsinprophetinequalities. ACMSIGecomExchanges,17(1):61–70,2019b.
ChristophDann,LihongLi,WeiWei,andEmmaBrunskill. Policycertificates: Towardsaccountable
reinforcement learning. In International Conference on Machine Learning, pages 1507–1516,
2019.
OmarDarwicheDomingues,PierreMénard,EmilieKaufmann,andMichalValko. Episodicrein-
forcement learning in finite mdps: Minimax lower bounds revisited. In Algorithmic Learning
Theory,pages578–598.PMLR,2021.
YonathanEfroni,GalDalal,BrunoScherrer,andShieMannor. Howtocombinetree-searchmethods
in reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume33,pages3494–3501,2019a.
YonathanEfroni,NadavMerlis,MohammadGhavamzadeh,andShieMannor. Tightregretbounds
formodel-basedreinforcementlearningwithgreedypolicies. InAdvancesinNeuralInformation
ProcessingSystems,pages12224–12234,2019b.
YonathanEfroni,MohammadGhavamzadeh,andShieMannor. Onlineplanningwithlookahead
policies. AdvancesinNeuralInformationProcessingSystems,33:14024–14033,2020.
Vikas Garg, TS Jayram, and Balakrishnan Narayanaswamy. Online optimization with dynamic
temporaluncertainty: Incorporatingshorttermpredictionsforrenewableintegrationinintelligent
energy systems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 27,
pages1291–1297,2013.
TheodorePHillandRobertPKertz. Ratiocomparisonsofsupremumandstopruleexpectations.
ZeitschriftfürWahrscheinlichkeitstheorieundVerwandteGebiete,56:283–285,1981.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. JournalofMachineLearningResearch,11(Apr):1563–1600,2010.
JianhaoJia,HaoLi,KaiLiu,ZiqiLiu,JunZhou,NikolaiGravin,andZhihaoGavinTang. Online
resourceallocationinmarkovchains. InProceedingsoftheACMWebConference2023,pages
3498–3507,2023.
ChiJin,ZeyuanAllen-Zhu,SebastienBubeck,andMichaelIJordan. Isq-learningprovablyefficient?
Advancesinneuralinformationprocessingsystems,31,2018.
TomSchaul,DanielHorgan,KarolGregor,andDavidSilver. Universalvaluefunctionapproximators.
InInternationalconferenceonmachinelearning,pages1312–1320.PMLR,2015.
MaxSimchowitzandKevinGJamieson. Non-asymptoticgap-dependentregretboundsfortabular
mdps. InAdvancesinNeuralInformationProcessingSystems,pages1153–1162,2019.
RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.
AvivTamar,GarrettThomas,TianhaoZhang,SergeyLevine,andPieterAbbeel. Learningfromthe
hindsightplan—episodicmpcimprovement. In2017IEEEInternationalConferenceonRobotics
andAutomation(ICRA),pages336–343.IEEE,2017.
14THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
AndreaTirinzoni,AymenAlMarjani,andEmilieKaufmann. Nearinstance-optimalpacreinforce-
mentlearningfordeterministicmdps. AdvancesinNeuralInformationProcessingSystems,35:
8785–8798,2022.
TengyangXie,DylanJFoster,YuBai,NanJiang,andShamMKakade.Theroleofcoverageinonline
reinforcementlearning. InTheEleventhInternationalConferenceonLearningRepresentations,
2022.
AndreaZanetteandEmmaBrunskill. Tighterproblem-dependentregretboundsinreinforcement
learningwithoutdomainknowledgeusingvaluefunctionbounds. InInternationalConferenceon
MachineLearning,pages7304–7312.PMLR,2019.
ZihanZhang,XiangyangJi,andSimonDu. Isreinforcementlearningmoredifficultthanbandits? a
near-optimalalgorithmescapingthecurseofhorizon. InConferenceonLearningTheory,pages
4528–4531.PMLR,2021.
15MERLISBAUDRYPERCHET
AppendixA. ProofsforFullLookaheadAgents
Theorem8(CRversusFullLookaheadAgents)
(cid:80) dπ(s,a)r (s,a)
1. Worst-casedistributions:CRH(P,r) = max (h,s,a)∈X h h .
π∈ΠM (cid:80) d∗(s)r (s,a)
(h,s,a)∈X h h
2. Worst-caserewardexpectations:Fornon-stationaryrewardexpectations,
dπ(s,a)
CRH(P) = max min h .
π∈ΠM(h,s,a)∈X d∗ h(s)
Iftherewardexpectationsarestationary(r (s,a) = r(s,a)),then
h
(cid:80)H dπ(s,a)
CRH(P) = max min h=1 h .
π∈ΠM(s,a)∈S×A (cid:80)H d∗(s)
h=1 h
3. Worst-caseenvironments: For all environments, CRH ≥ max(cid:8) 1 , 1 (cid:9) . Also, for any
SAH AH
δ ∈ (0,1)thereexiststationaryenvironmentswithrewardsover[0,1]s.t. ifS = An+1for
n ∈ {0,...,H −1},thenCRH(P,r) ≤ 1+δ ,andifS ≥ AH−1,then
(H−log (S−1))·(A−1)(S−1)
A
CRH(P,r) ≤ 1+δ.
AH
Proof
Worst-casedistribution: WealreadysawinEquation(3)thatforallrewarddistributions
(cid:88)
VH,∗(P,r) ≤ d∗(s)r (s,a).
h h
(h,s,a)∈X
Wenowshowthatforanyδ > 0,thereexistsadistributionsuchthat
(cid:88)
VH,∗(P,r) ≥ (1−δ) d∗(s)r (s,a).
h h
(h,s,a)∈X
Thiswouldimplythat
(cid:88)
sup VH,∗(P,r) = d∗(s)r (s,a)
h h
RH∼D(r)
(h,s,a)∈X
andconcludethispartoftheproof(byRemark3andEquation(2)).
Let ϵ ∈ (0,1) and assume long-shot reward distribution R ∼ LS (r). For any (h,s,a) ∈ X,
ϵ
definetheeventthatapositiverewardwasrealizedjustin(h,s,a):
(cid:26) (cid:27)
r (s,a)
G = R (s,a) = h ,∀(h′,s′,a′) ̸= (h,s,a) : R (s′,a′) = 0 .
h,s,a h h′
ϵ
Underanyoftheseevents,thevalueoftheoptimalfulllookaheadagentis
 
(cid:20) (cid:21)
E  πm ∈Πa Mx (cid:88) dπ h(s′,a′)R h′(s′,a′)(cid:12) (cid:12)G h,s,a = E πm ∈Πa Mx dπ h(s,a)r h(s ϵ,a)(cid:12) (cid:12)G h,s,a = r h(s ϵ,a) d∗ h(s).
(h′,s′,a′)∈X
16THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
Now,noticethateachofthesemutuallyexclusiveeventsG occurw.p. ϵ(1−ϵ)SAH−1,andthat
h,s,a
thevalueisnon-negativewhennoneofthemoccur. Hence,forthisrewarddistribution,
 
VH,∗(P,r) ≥ (cid:88) Pr{G h,s,a}E max (cid:88) dπ h(s′,a′)R h′(s′,a′)(cid:12) (cid:12)G h,s,a
π∈ΠM
(h,s,a)∈X (h′,s′,a′)∈X
=
(cid:88) ϵ(1−ϵ)SAH−1r h(s,a)
d∗(s)
ϵ h
(h,s,a)∈X
(cid:88)
= (1−ϵ)SAH−1 d∗(s)r (s,a). (6)
h h
(h,s,a)∈X
Setting ϵ = 1−(1−δ)SAH1 −1 ≈ δ leads to the desired bound and concludes this part of the
SAH
proof.
Worst-caserewardexpectations: Beforeprovingtheresults,weremarkonthechoicetolimit
rewardexpectationsto[0,1]. Themainmotivationfordoingsoistheubiquityofthisboundedness
assumptionintheliteratureofRL,butinfact,itisonlyamatterofconventionandhasnorealimpact.
Indeed,sinceCRisinvarianttoscaling,thesameresultwoulddirectlyholdforanyboundedinterval
[0,C]. Furthermore,asexplainedinRemark13,theresultwouldalsoholdunderthelessrestrictive
assumptionsthatrewardexpectationsarejustnon-negative.
WeproofbothresultsusingLemma12. Asafirststep,wehighlightthattheonlydependenceof
theoptimizationproblemintheMarkovianpolicyπisthroughtheoccupancymeasuredπ. Therefore,
h
denotingthesetofalloccupancymeasuresinducedbyaMarkovianpolicywithtransitionkernelP
byD = DM(P),theproblemcanbereformulatedas
CRH(P) = inf CRH(P,r)
r ∈[0,1]SA
h
(cid:80) dπ(s,a)r (s,a)
(h,s,a)∈X h h
= inf max
r h∈[0,1]SAπ∈ΠM (cid:80) (h,s,a)∈X d∗ h(s)r h(s,a)
(cid:80)
d (s,a)r (s,a)
(h,s,a)∈X h h
= inf max . (7)
r h∈[0,1]SAd∈DM(P) (cid:80) (h,s,a)∈X d∗ h(s)r h(s,a)
ThesetofthepossibleoccupancymeasuresisconvexandcompactinRSAH (Altman,2021),sowe
+
canapplyLemma12withα = d∗(s),y = d (s,a)andx = r (s,a),resultingwith
h,s,a h h,s,a h h,s,a h
d (s,a) dπ(s,a)
CRH(P) = max min h = max min h ,
d∈DM(P)(h,s,a)∈X d∗ h(s) π∈ΠM(h,s,a)∈X d∗ h(s)
whereweagainusedtheequivalencebetweenoptimizingoverMarkovianpoliciesandtheiroccu-
pancymeasures.
Forstationaryrewards,wherer (s,a) = r(s,a)forall(h,s,a) ∈ X,werewriteEquation(7)as
h
(cid:16) (cid:17)
(cid:80) (cid:80)H
d (s,a) r(s,a)
(s,a)∈S×A h=1 h
CRH(P) = inf max .
(cid:16) (cid:17)
r h∈[0,1]SAd∈DM(P) (cid:80) (cid:80)H d∗(s) r(s,a)
(s,a)∈S×A h=1 h
17MERLISBAUDRYPERCHET
Now, another application of Lemma 12 with α = (cid:80)H d∗(s), y = (cid:80)H d (s,a) and
s,a h=1 h s,a h=1 h
x = r(s,a)yields
s,a
(cid:80)H d (s,a) (cid:80)H dπ(s,a)
CRH(P) = max min h=1 h = max min h=1 h ,
d∈DM(P)(s,a)∈S×A (cid:80)H d∗(s) π∈ΠM(s,a)∈S×A (cid:80)H d∗(s)
h=1 h h=1 h
whichisthedesiredresultforstationaryenvironments.
Worst-caseenvironment–lowerbound: WenowderivethelowerboundCRH ≥ max(cid:8) 1 , 1 (cid:9) .
SAH AH
Weproveitfornonstationaryenvironments,soinparticular,italsoholdsforstationaryones.
Recallthatbydefinition,forany(h,s,a) ∈ X,d∗(s,a)istheoccupancymeasureofaMarkovian
h
policythatmaximizesthevisitationprobabilityin(h,s,a),andletπ∗ beaMarkovianpolicythat
h,s,a
achieves this occupancy. Since the set of occupancy measures induced by Markovian policies is
convex(Altman,2021),thereexistsaMarkovianpolicyπ ∈ ΠM suchthatitsoccupancymeasure
u
istheaverageofalltheseoccupancies,namely,forall(h,s,a) ∈ X,
1 (cid:88) π∗
dπu(s,a) = d h′,s′,a′(s,a).
h SAH h
(h′,s′,a′)∈X
Usingthepreviouspartofthetheorem,forallenvironmentsP,itholdsthat
dπ(s,a)
CRH(P) = max min h
π∈ΠM(h,s,a)∈X d∗ h(s)
dπu(s,a)
≥ min h
(h,s,a)∈X
d∗(s)
h
1 (cid:80) dπ h∗ ′,s′,a′(s,a)
SAH (h′,s′,a′)∈X h
= min
(h,s,a)∈X
d∗(s)
h
π∗
(∗) 1 d h,s,a(s,a)
≥ min SAH h
(h,s,a)∈X
d∗(s)
h
1 d∗(s,a)
= min h
SAH (h,s,a)∈X d∗(s)
h
1
= .
SAH
In(∗),wediscardallthe(non-negative)termsinthesummationwhere(h′,s′,a′) ̸= (h,s,a),while
inthefollowingequalities,weusethedefinitionofπ∗ andthefactthatd∗(s,a) = d∗(s). Asthis
h,s,a h h
inequalityholdsforallenvironments,italsoimpliesthatCRH ≥ 1 .
SAH
ToprovethatCRH ≥ 1 ,wetakeadifferentapproachandgobacktotheBellmanequations.
AH
DenotebyV¯∗(s|R),theoptimalvalueofafulllookaheadpolicy,startingfromtimesteph ∈ [H]and
h
states ∈ S,andgivenrewardrealizationR. Therefore,thevalueofthefulllookaheadagentisgiven
byVH,∗ = E (cid:2) V¯∗(s|R)(cid:3) . Similarly,denotethestandardvaluewithnolookaheadinformation
R,s∼µ 1
startingfromtimesteph ∈ [H]andstates ∈ S byV0,∗(s). Aspreviouslyexplained,givenreward
h
realizations,theoptimalfulllookaheadpolicyisMarkovian,sobothvaluescanbecalculatedusing
18THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
thefollowingBellmanequationsforalls ∈ S andh ∈ [H]:
(cid:40) (cid:41)
(cid:88)
V¯∗(s|R) = max R (s,a)+ P (s′|s,a)V¯∗ (s′|R) , V¯∗ (s|R) = 0
h h h h+1 H+1
a∈A
s′∈S
(cid:40) (cid:41)
V0,∗(s) = max r (s,a)+ (cid:88) P (s′|s,a)V0,∗(s′) , V0,∗ (s) = 0.
h h h h+1 H+1
a∈A
s′∈S
Weprovebybackwardinductionthatforallh ∈ [H+1]ands ∈ S,E(cid:2) V¯∗(s|R)(cid:3) ≤ AH+1−hV0,∗(s).
h h
Specifically,usingthisrelationforh = 1andtakingtheexpectationovertheinitialstatedistribution
wouldimplythatVH,∗ ≤ AHV0,∗,regardlessoftheenvironment,andthusCRH ≥ 1 .
AH
Asthebaseoftheinduction,seethattheclaimtriviallyholdsforh = H+1,whereallvaluesare
0. Next,foranyh ∈ [H]ands ∈ S,giventhattheclaimholdsforallstatesinsteph+1,wehave
(cid:34) (cid:40) (cid:41)(cid:35)
E(cid:2) V¯∗(s|R)(cid:3) = E max R (s,a)+ (cid:88) P (s′|s,a)V¯∗ (s′|R)
h h h h+1
a∈A
s′∈S
(cid:32) (cid:40) (cid:41)(cid:33)
(cid:88) (cid:88)
≤ E R (s,a)+ P (s′|s,a)V¯∗ (s′|R)
h h h+1
a∈A s′∈S
(cid:32) (cid:33)
= (cid:88) r (s,a)+ (cid:88) P (s′|s,a)E(cid:2) V¯∗ (s′|R)(cid:3)
h h h+1
a∈A s′∈S
(cid:32) (cid:33)
( ≤∗) (cid:88) r (s,a)+AH−h (cid:88) P (s′|s,a)V0,∗(s′)
h h h+1
a∈A s′∈S
(cid:32) (cid:33)
≤ AH−h(cid:88) r (s,a)+ (cid:88) P (s′|s,a)V0,∗(s′)
h h h+1
a∈A s′∈S
(cid:40) (cid:41)
≤ AH−h·Amax r (s,a)+ (cid:88) P (s′|s,a)V0,∗(s′)
h h h+1
a∈A
s′∈S
= AH+1−hV0,∗(s),
h
where throughout the derivation, we use the fact that all rewards (and thus the values) are non-
negativeand(∗)isduetotheinductionhypothesis. Thisconcludestheproofofthelowerboundin
thestatement,namely,thatforalldynamicsandrewards,itholdsthatCRH ≥ max(cid:8) 1 , 1 (cid:9) .
SAH AH
Worst-case environment – upper bound: see Proposition 10, where we present a tree-like
stationaryenvironmentforwhichtheaforementionedboundsarenear-tight.
19MERLISBAUDRYPERCHET
AppendixB. ProofsforMulti-StepLookaheadAgents
Proposition6 ForanyL ∈ [H],lett (h) = max{h−L+1,1}. Then,itholdsthat
L
(cid:88) (cid:88)
sup VL,∗(P,r) = max r (s,a) dπ (s′)d∗(s|s = s′)
RH∼D(r) π∈ΠM
(h,s,a)∈X
h
s′∈S
tL(h) h tL(h)
ProofWestartbylower-boundingtheoptimalvalueinthepresenceoflong-shotrewards. Then,we
proveamatchinguppervalueforallrewardsandL-steplookaheadpolicies
Lowerboundonthevalueoflong-shots. Letϵ > 0andassumethatR ∼ LS (r),namely,that
ϵ
forany(h,s,a) ∈ X,arewardofr (s,a)/ϵisgeneratedwithprobabilityϵ;otherwise,thereward
h
wouldbezero. Letπ ∈ ΠM beanyMarkovianpolicythatdoesnotobservefuturerewardsandlet
π˜ ∈ ΠL beapolicythatplaysπ ifalltheL-stepfuturerewardsarezeroandotherwiseoptimally
navigatestoonestrictlypositivereward(tiesbrokenarbitrarily). Inparticular,ifonlyonelong-shot
rewardisrealizedat(h,s,a),thispolicywouldplayπ untiltimestept (h) = max{h−L+1,1}
L
andthenmaximizethereachingprobabilityfroms tos = s. Iftheagentsuccessfullyreaches
tL(h) h
s = s,itwillplaya = aandcollectthereward.
h h
Thevalueofπ˜canbelower-boundedbythevaluethatatmostonelong-shotisrealized;Denoting
(cid:26) (cid:27)
r (s,a)
G = R (s,a = h ,∀(h′,s′,a′) ̸= (h,s,a) : R (s,a) = 0 ,
h,s,a h h
ϵ
theeventthatarewardwasrealizedonlyin(h,s,a) ∈ X,webound
VL,∗(P,r) ≥ VL,π˜(P,r)
(cid:34) H (cid:35)
= E (cid:88) R h′(s h′,a h′)(cid:12) (cid:12)π˜
h′=1
(cid:34) H (cid:35)
( ≥1) (cid:88) E (cid:88) R h′(s h′,a h′)(cid:12) (cid:12)π˜,G
h,s,a
Pr{G h,s,a}
(h,s,a)∈X h′=1
(cid:20) (cid:21)
= (cid:88) E r h(s,a) 1{s
h
= s,a
h
= a}(cid:12) (cid:12)π˜,G
h,s,a
Pr{G h,s,a}
ϵ
(h,s,a)∈X
( =2) (cid:88) (cid:88) Pr(cid:8) s = s′|π(cid:9) max Pr(cid:8) s = s,a = a|s = s′,π′(cid:9)r h(s,a) Pr{G }
tL(h) π′∈ΠM h h tL(h) ϵ h,s,a
(h,s,a)∈Xs′∈S
( =3) (cid:88) (cid:88) dπ (s′)d∗(s|s = s′)r h(s,a) Pr{G }
tL(h) h tL(h) ϵ h,s,a
(h,s,a)∈Xs′∈S
( =4) (cid:88) (cid:88) dπ (s′)d∗(s|s = s′)r h(s,a) ·ϵ(1−ϵ)SAH−1
tL(h) h tL(h) ϵ
(h,s,a)∈Xs′∈S
(cid:88) (cid:88)
≥ e−ϵSAH r (s,a) dπ (s′)d∗(s|s = s′).
h tL(h) h tL(h)
(h,s,a)∈X s′∈S
In(1),weusethefactsthattheeventsG aredisjointandtherewardsarenon-negative. Next,in
h,s,a
(2),wedecomposetostepsuntilt (h),whereweplayπ,andstepsfromt (h)toh,wherewetryto
L L
20THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
maximizereachingprobabilityto(s,a)attimesteph. Noticethattherewardisindependentofthe
transition,sotheoptimalreachingpolicyisMarkovian. Relation(3)replacestheprobabilitynotation
toconditionaloccupancymeasureand(4)substitutestheprobabilityoftheevents. Maximizingover
π andtakingthelimitofsmallϵ,wegetalowerboundof
(cid:88) (cid:88)
sup VL,∗(P,r) ≥ sup max e−ϵSAH r (s,a) dπ (s′)d∗(s|s = s′)
RH∼D(r) ϵ>0π∈ΠM
(h,s,a)∈X
h
s′∈S
tL(h) h tL(h)
(cid:88) (cid:88)
= max r (s,a) dπ (s′)d∗(s|s = s′).
π∈ΠM
h tL(h) h tL(h)
(h,s,a)∈X s′∈S
Upperboundonthevalueofallrewarddistributions. Foranyfixedlookaheadpolicyπ ∈ ΠL
andanyrewarddistribution,webound
(cid:34) H (cid:35)
(cid:88)
VL,π(P,r) = E R (s ,a )|π
h h h
h=1
(cid:88)
= E[R (s,a)1{s = s,a = a}|π]
h h h
(h,s,a)∈X
= (cid:88) (cid:88) Pr(cid:8) s = s′|π(cid:9)E(cid:2) R (s,a)1{s = s,a = a}|π,s = s′(cid:3)
tL(h) h h h tL(h)
(h,s,a)∈Xs′∈S
= (cid:88) (cid:88) dπ (s′)E(cid:2) R (s,a)Pr(cid:8) s = s,a = a|π,s = s′,R (s,a)(cid:9) |π,s = s′(cid:3)
tL(h) h h h tL(h) h tL(h)
(h,s,a)∈Xs′∈S
(cid:20) (cid:21)
≤ (cid:88) (cid:88) dπ tL(h)(s′)E R h(s,a) πm ∗∈a Πx LPr(cid:8) s
h
= s,a
h
= a|π∗,s
tL(h)
= s′,R h(s,a)(cid:9)(cid:12) (cid:12)π,s
tL(h)
= s′
(h,s,a)∈Xs′∈S
( =1) (cid:88) (cid:88) dπ (s′)d∗(s|s = s′)E(cid:2) R (s,a)|π,s = s′(cid:3)
tL(h) h tL(h) h tL(h)
(h,s,a)∈Xs′∈S
( =2) (cid:88) (cid:88) dπ (s′)d∗(s|s = s′)r (s,a)
tL(h) h tL(h) h
(h,s,a)∈Xs′∈S
≤ max (cid:88) (cid:88) dπ∗ (s′)d∗(s|s = s′)r (s,a)
π∗∈ΠL
tL(h) h tL(h) h
(h,s,a)∈Xs′∈S
( =3) max (cid:88) (cid:88) dπ∗ (s′)d∗(s|s = s′)r (s,a)
π∗∈ΠM
tL(h) h tL(h) h
(h,s,a)∈Xs′∈S
Relation (1) holds since the state dynamics are independent of the rewards realization and the
maximalreachingprobabilityisd∗(s|s = s′). Relation(2)holdsbecausewereachthestate
h tL(h)
attimestept (h)justbeforeseeingR (s,a);therefore,thetwovariablesareindependent. Finally,
L h
relation(3)holdssincewecanrewritethevalueas
H
max (cid:88)(cid:88) dπ∗ (s′) (cid:88) 1{t (h) = i}d∗(s|s = s′)r (s,a).
π∗∈ΠL
i L h tL(h) h
i=1s′∈S (h,s,a)∈X
Thisexpressionisequivalenttotheoptimalvalueofano-lookaheadagentwhoseexpectedrewardat
any(i,s′,a′) ∈ X is(cid:80) 1{t (h) = i}d∗(s|s = s′)r (s,a),sothereexistsaMarkovian
(h,s,a)∈X L h tL(h) h
policythatmaximizesthisvalue.
21MERLISBAUDRYPERCHET
Theorem9 [CRversusMulti-StepLookaheadAgents]ForanyL ∈ [H],lett (h) = max{h−L+1,1}.
L
Then,itholdsthat
1. Worst-casedistributions:CRL(P,r) =
max π∈ΠM(cid:80) (h,s,a)∈Xr h(s,a)dπ h(s,a)
.
max π∈ΠM(cid:80) (h,s,a)∈Xr h(s,a)(cid:80) s′∈Sdπ tL(h)(s′)d∗ h(s|s tL(h)=s′)
2. Worst-caserewardexpectations:
dπ(s,a)
CRL(P) = min max min h .
π∗∈ΠMπ∈ΠM(h,s,a)∈X (cid:80) s′∈Sdπ tL∗ (h)(s′)d∗ h(s|s tL(h) = s′)
Iftherewardexpectationsarestationary(r (s,a) = r(s,a)),then
h
(cid:80)H dπ(s,a)
CRL(P) = min max min h=1 h .
π∗∈ΠMπ∈ΠM(s,a)∈S×A (cid:80)H (cid:80) dπ∗ (s′)d∗(s|s = s′)
h=1 s′∈S tL(h) h tL(h)
(cid:110) (cid:111)
3. Worst-caseenvironments:Forallenvironments,CRL ≥ max 1 , 1 . Also,for
SAH (H−L+1)AL
anyδ ∈ (0,1)thereexiststationaryenvironmentswithrewardsover[0,1]s.t. ifS = An+1
for n ∈ {0,...,L−1}, then CRL(P,r) ≤ 1+δ , and if S ≥ AL +1,
(H−log (S−1))·(A−1)(S−1)
A
thenCRL(P,r) ≤ 1+δ .
(H−L+1)(AL−1)
Proof
Worst-casedistribution: ThispartofthetheoremisadirectlycorollaryofProposition6,applied
with Remark 3 and Equation (2). We remark that we assume w.l.o.g. that r (s,a) > 0 for at
h
leastonereachable(h,s,a) ∈ X (i.e.,d∗(s,a) > 0). Otherwise,bothvaluesinthenumeratorand
h
denominatorequalzeroandtheratioisdefinedas+∞.
Worst-caserewardexpectations: AsintheproofofTheorem8,westartbyrewritingthemaxi-
mizationproblemsinthecompetitiveratiousing,DM(P)thesetofoccupancymeasuresinducedby
thetransitionkernelP andallMarkovianpolicies:
CRL(P) = inf CRL(P,r)
r ∈[0,1]SA
h
max (cid:80) r (s,a)dπ(s,a)
π∈ΠM (h,s,a)∈X h h
= inf
r h∈[0,1]SA max π∗∈ΠM(cid:80) (h,s,a)∈X r h(s,a)(cid:80) s′∈Sdπ tL∗ (h)(s′)d∗ h(s|s tL(h) = s′)
(cid:80) r (s,a)dπ(s,a)
(h,s,a)∈X h h
= inf min max
r h∈[0,1]SAπ∗∈ΠMπ∈ΠM (cid:80) (h,s,a)∈X r h(s,a)(cid:80) s′∈Sdπ tL∗ (h)(s′)d∗ h(s|s tL(h) = s′)
(cid:80)
r (s,a)d (s,a)
(h,s,a)∈X h h
= inf min max
r h∈[0,1]SAd′∈DM(P)d∈DM(P) (cid:80) (h,s,a)∈X r h(s,a)(cid:80) s′∈Sd′ tL(h)(s′)d∗ h(s|s tL(h) = s′)
(cid:80)
r (s,a)d (s,a)
(h,s,a)∈X h h
= inf inf max .
d′∈DM(P)r h∈[0,1]SAd∈DM(P) (cid:80) (h,s,a)∈X r h(s,a)(cid:80) s′∈Sd′ tL(h)(s′)d∗ h(s|s tL(h) = s′)
(8)
22THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
Continuing following the proof of Theorem 8, we use the convexity and compactness of the set
ofoccupancymeasurestoapplyLemma12onthetwointernalproblems,thistimewithα =
h,s,a
(cid:80) d′ (s′)d∗(s|s = s′). Doingsoresultswith
s′∈S tL(h) h tL(h)
d (s,a)
CRL(P) = inf max min h .
d′∈DM(P)d∈DM(P)(h,s,a)∈X (cid:80) s′∈Sd′ tL(h)(s′)d∗ h(s|s tL(h) = s′)
Atthispoint,wedeviatefromthepreviousproofandanalyzetheexternaloptimizationproblem. In
particular,wewanttoshowthattheminimumisobtainedinthesetofMarkovianpolicies. Weprove
itusingLemma14. Foritsapplication,noticethatDM(P)isaconvexandcompactpolytope,and
(cid:110) (cid:111)
thereforesodoesitslineartransformationP = (cid:80) d′ (s′)d∗(s|s = s′)|d ∈ DM(P) ,
s′∈S tL(h) h tL(h)
sotheconditionsofthelemmahold: theinfimumisobtainedataminimizerintheset. Substitutingit
backintoCRL(P)andusingtheequivalencebetweenoccupancymeasuresandpoliciesleadstothe
desiredresult:
d (s,a)
CRL(P) = min max min h (Lemma14)
d′∈DM(P)d∈DM(P)(h,s,a)∈X (cid:80) s′∈Sd′ tL(h)(s′)d∗ h(s|s tL(h) = s′)
dπ(s,a)
= min max min h .
π∗∈ΠMπ∈ΠM(h,s,a)∈X (cid:80) s′∈Sdπ tL∗ (h)(s′)d∗ h(s|s tL(h) = s′)
Forstationaryrewards,wherer (s,a) = r(s,a),werewriteEquation(8)as
h
(cid:16) (cid:17)
(cid:80) (cid:80)H
d (s,a) r(s,a)
(s,a)∈S×A h=1 h
CRL(P) = inf inf max .
(cid:16) (cid:17)
d′∈DM(P)r h∈[0,1]SAd∈DM(P) (cid:80)
(s,a)∈S×A
(cid:80)H h=1(cid:80) s′∈Sd′ tL(h)(s′)d∗ h(s|s
tL(h)
= s′) r(s,a)
WecannowreapplyLemma12withtheappropriateα =(cid:80)H (cid:80) d′ (s′)d∗(s|s = s′),
s,a h=1 s′∈S tL(h) h tL(h)
followedbyapplyingLemma14,toget
(cid:80)H
d (s,a)
CRL(P) = inf max min h=1 h (Lemma12)
d′∈DM(P)d∈DM(P)(s,a)∈S×A (cid:80)H (cid:80) d′ (s′)d∗(s|s = s′)
h=1 s′∈S tL(h) h tL(h)
(cid:80)H dπ(s,a)
= min max min h=1 h . (Lemma14)
π∗∈ΠMπ∈ΠM(s,a)∈S×A (cid:80)H (cid:80) dπ∗ (s′)d∗(s|s = s′)
h=1 s′∈S tL(h) h tL(h)
Worst-caseenvironment–lowerbound: Firstnoticethatbydefinition,anyL-steplookahead
policyisalsoafulllookaheadpolicy. Inparticular,forallenvironments,VL,∗(P,r) ≤ VH,∗(P,r),
andthereverserelationwouldholdfortheCR.Thus, fromTheorem5, wedirectlygetthelower
boundCRL ≥ CRH ≥ 1 . WefurtherproofthatCRL ≥ 1 usingareductiontothe
SAH (H−L+1)AL
fulllookaheadcase.
23MERLISBAUDRYPERCHET
Tothisend,westartbydecomposingtheno-lookaheadvalueofanyπ ∈ ΠM asfollows
(cid:88)
V0,π(P,r) = r (s,a)dπ(s,a)
h h
(h,s,a)∈X
(cid:88)
= r (s,a)Pr{s = s,a = a}
h h h
(h,s,a)∈X
=
(cid:88)
r
(s,a)(cid:88) Pr(cid:8)
s =
s′(cid:9) Pr(cid:8)
s = s,a = a|s =
s′(cid:9)
h tL(h) h h tL(h)
(h,s,a)∈X s′∈S
(cid:88) (cid:88)
= r (s,a) dπ (s′)dπ(s,a|s = s′)
h tL(h) h tL(h)
(h,s,a)∈X s′∈S
L
(cid:88) (cid:88) (cid:88)
= dπ(s′) r (s,a)dπ(s,a|s = s′)
1 h h 1
s′∈S h=1(s,a)∈S×A
H−L+1
(cid:88) (cid:88) (cid:88)
+ dπ(s′) r (s,a)dπ (s,a|s = s′).
i i+L−1 i+L−1 h
i=2 s′∈S (s,a)∈S×A
Inthelastinequality,wedecomposethesummationintotwotermsdependingonwhethert (h) ≜
L
i = 1ort (h) > 1.
Forbrevity,let(cid:8) ri(cid:9)H−L+1
besuchthatforall(h,s,a) ∈ X,
L i=1
r1(s,a) = r (s,a)1{h ∈ [L]},
h h
ri(s,a) = r (s,a)1{h = i+L−1}, ∀i ∈ {2,...H −L+1}.
h h
Usingthisnotation,onecouldrewritethevalueas
H−L+1 i+L−1
(cid:88) (cid:88) (cid:88) (cid:88)
V0,π(P,r) = dπ(s′) ri(s,a)dπ(s,a|s = s′). (9)
i h h i
i=1 s′∈S (s,a)∈S×A h=i
Noticethatri aretheexpectedrewardsoftimestepsobservedbythelookaheadagentatstepi. We
h
nowdefinethefollowingsetofpolicies
• AMarkovianpolicythatmaximizestheL-lookaheadvalueisdenotedby
(cid:88) (cid:88)
π∗ ∈ argmax r (s,a) dπ (s′)d∗(s|s = s′).
h tL(h) h tL(h)
π∈ΠM
(h,s,a)∈X s′∈S
• Foranyi ∈ [H −L+1],letπ beaMarkovianpolicythatplaysπ∗ untilreachingsomestate
i
s andthencontinuesbyapolicythatmaximizestherewardfunctionri forthefollowingL
i
timesteps:
i+L−1
(cid:88) (cid:88)
π ∈ argmax ri(s,a)dπ(s,a|s ).
i h h i
π∈ΠM
(s,a)∈S×A h=i
Fori = 1,thestates wouldbetheinitialstate,generatedfromtheinitialstatedistribution.
1
Notice that starting for the ith timestep, π is an optimal policy given rewards ri in the
i
24THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
standardMDPmodel,sothereexistsanoptimalMarkovianpolicythatmaximizesitsvalue
simultaneously for all s ∈ S. By ignoring all but the ith term in Equation (9), one could
i
clearlyseethat
i+L−1
V0,πi ≥ (cid:88) dπ∗ (s′) (cid:88) (cid:88) ri(s,a)dπi(s,a|s = s′)
i h h i
s′∈S (s,a)∈S×A h=i
i+L−1
= (cid:88) dπ∗ (s′) max (cid:88) (cid:88) ri(s,a)dπ(s,a|s = s′).
i h h i
π∈ΠM
s′∈S (s,a)∈S×A h=i
• AllaforementionedpoliciesareMarkovian,sobytheconvexityoftheoccupanciesinducedby
Markovianpolicies(Altman,2021),thereexistsπ ∈ ΠM suchthatforall(h,s,a) ∈ X,
u
H−L+1
1 (cid:88)
dπu(s,a) = dπi(s,a).
h H −L+1 h
i=1
Sincevaluesarelinearintheoccupancymeasure,wecanboundtheoptimalno-lookaheadvalueby
V0,∗(P,r) ≥ V0,πu(P,r)
H−L+1
1 (cid:88)
= V0,πi
H −L+1
i=1
H−L+1 i+L−1
≥ 1 (cid:88) (cid:88) dπ∗ (s′) max (cid:88) (cid:88) ri(s,a)dπ(s,a|s = s′). (10)
H −L+1 i π∈ΠM h h i
i=1 s′∈S (s,a)∈S×A h=i
Movingforwards,weuseasimilardecompositiontotheL-lookaheadvalueusingProposition6:
(cid:88) (cid:88)
VL,∗(P,r) ≤ max r (s,a) dπ (s′)d∗(s|s = s′)
π∈ΠM
h tL(h) h tL(h)
(h,s,a)∈X s′∈S
= (cid:88) r (s,a)(cid:88) dπ∗ (s′)d∗(s|s = s′)
h tL(h) h tL(h)
(h,s,a)∈X s′∈S
L
(cid:88) (cid:88) (cid:88)
= d (s′) r (s,a)d∗(s|s = s′)
1 h h 1
s′∈S h=1(s,a)∈S×A
H−L+1
+ (cid:88) (cid:88) dπ∗ (s′) (cid:88) r (s,a)d∗ (s|s = s′)
i i+L−1 i+L−1 i
i=2 s′∈S (s,a)∈S×A
H−L+1 i+L−1
= (cid:88) (cid:88) dπ∗ (s′) (cid:88) (cid:88) ri(s,a)d∗(s|s = s′). (11)
i h h i
i=1 s′∈S (s,a)∈S×A h=i
Toconcludethere (cid:80)duction,recalltheinequality (cid:80) (cid:80)i iα αi ix yii ≥ min i(cid:110) x yii(cid:111) ,whichholdsforallvaluesof
x ,y ,α ≥ 0s.t. α > 0, duetothequasiconcavityoftheratiooflinearfunctions(giventhe
i i i i i
25MERLISBAUDRYPERCHET
convention that x/0 = +∞). Applying this on the CR with the coefficients α = dπ∗(s′) and
i,s′ i
usingEquations(10)and(11),wegetforallenvironmentsthat
V0,∗(P,r)
CRL(P,r) =
VL,∗(P,r)
1 (cid:40) max π∈ΠM(cid:80) (s,a)∈S×A(cid:80)i h+ =L i−1r hi(s,a)dπ h(s,a|s i = s′)(cid:41)
≥ min
H −L+1 i∈[H−L+1], (cid:80) (cid:80)i+L−1ri(s,a)d∗(s|s = s′)
s′∈S (s,a)∈S×A h=i h h i
(∗) 1
≥ .
(H −L+1)AL
Thelastinequalityisthereductiontothefulllookahead: eachofthetermsisexactlytheCRversusa
fulllookaheadagentwithhorizonLandrewardexpectationsri (seeTheorem5,part1). Thus,each
ofthetermsislower-boundedbytheboundfortheworst-caseenvironmentgivenhorizonL(see
Theorem5,part3)–by 1 .
AL
Worst-caseenvironment–upperbound: asinTheorem7,thispartoftheproofiscoveredin
Proposition10,wherewepresentatree-likestationaryenvironmentwiththestatedbehavior.
26THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
Figure2: Anear-worst-caseenvironment: tree-likeMDP.Anagentcandecidetostayattherootof
thetree,butonceitstartstotraversethetree,itmustnavigatetooneofitsleaves,from
whichitmovestoanon-rewardingterminalstate. Allleaveshavelong-shotrewards,while
allothernodesyieldnoreward.
AppendixC. AnalyzingtheCompetitiveRatioofSpecificEnvironments
C.1. Upper-BoundsforRewardLookahead–DelayedTrees
Proposition10 ForanyL ∈ [H]andanyδ ∈ (0,1),thereexiststationaryenvironmentswithre-
wardsover[0,1]s.t. ifS = An+1forn ∈ {0,...,L−1},thenCRL ≤ 1+δ ,
(H−log (S−1))·(A−1)(S−1)
A
and if S ≥ AL +1, then CRL ≤ 1+δ . Moreover, if L = H and S ≥ AH −1, there
(H−L+1)(AL−1)
existsanenvironments.t. CRH ≤ 1+δ
AH
ProofAssumethatS = An+1forsomen ∈ N. Wedividetheproofintodifferentcases,depending
onthevaluesofnandL.
27MERLISBAUDRYPERCHET
Case1: L ∈ [H]andn ∈ [1,L−1]. Toprovethisbound,wedesignatreeMDPwithanadditional
optiontodecidewhentotraverseit,asillustratedinFigure2. Inparticular,assumethatthefixed
initialstates istherootofatreeofdepthn+1suchthattheroothasA−1descendantsandall
1
othernodeshaveAdescendants. Thus,thenumberofnodesinthistreeis
(cid:88)n An−1
1+(A−1) Ai−1 = 1+(A−1) = An,
A−1
i=1
andthenumberofleavesis(A−1)An−1. Assumingthataftertraversingthetree,theenvironment
movestoaterminalstates ,thisenvironmentcouldindeedberepresentedusingS = An+1states.
T
WedenotethedynamicsofthistreebyP.
Forthedynamics,weallocateoneactionintherootofthetreethatkeepstheagentattheroot,
whiletherestoftheactionsallowtraversingthroughthetree. Attheleaves,allactionstransition
toaterminalstates . Weemphasizethatonceanagenthasdecidedtostarttraversingthetree,it
T
hastocontinueallthewayuntiltheleaves(andterminalstate),sothedecisionwhentotraversethe
treeistakenatitsroot. Finally,therewardofanyactionatanyleafisalong-shotLS (ϵ),namely
ϵ
Bernoulli-distributedwithprobabilityϵ. Inparticular,thisdistributionisboundedin[0,1].
Inthisexample,anyagentwithnolookaheadinformationwillperformatmostoneactionata
singleleaf,independentlyoftherewardrealization,thuscollectinginexpectationnomorethanthe
expectedrewardofasingleleafV0,∗ ≤ E[LS (ϵ)] = ϵ.
ϵ
Ontheotherhand,anL-lookaheadagentcouldstarttraversingthetreeonlywhenarewardwill
berealizeduponitsarrivaltotheleaf. Toreachaleafattimesteph,theagenthastostarttraversing
thetreeattimesteph−n. Thus,thisagentwillwaitattheroottoseewhetherarewardisrealizedin
anyleafattimesteps{n+1,...,H},andonlyifso,willtraverseit.
Sincethereare(A−1)An−1leaveswithAactionseach,theprobabilitythatnorewardisrealized
inanyleafatthesetimestepsis(1−ϵ)(H−n)·(A−1)An−1·A,andtheoptimallookaheadagentwould
collectanexpectedrewardofatleast
VL,∗ ≥ 1−(1−ϵ)(H−n)·(A−1)An
≥ (H −n)·(A−1)Anϵ−((H −n)·(A−1)An)2ϵ2,
wherethelastinequalityissince(1−x)n ≤ 1−nx+n2x2.
Combiningbothinequalities,forthisenvironmentwehavethat
V0,∗
CRL(P,r) =
VL,∗
ϵ
≤
(H −n)·(A−1)Anϵ−((H −n)·(A−1)An)2ϵ2
1
= .
(H −n)·(A−1)An−((H −n)·(A−1)An)2ϵ
Inparticular,foranyδ > 0,wecouldfixϵsmallenoughsuchthat
1+δ
CRL(P,r) ≤ ,
(H −log (S −1))·(A−1)(S −1)
A
whereweusedtherelationS = An+1.
28THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
Case2: L ∈ [H]andn = 0. ThisisthecaseofS = 2. Weseparateitfortheclarityofpresentation,
buttheexampleremainsthesame: thefirststates istheinitialstateandtheseconds = s isa
1 2 T
terminalnon-rewardingstate. Whenins ,asingleactiondoesnotchangethestatebutyieldsno
1
reward,whileallotherA−1actionstransitiontheenvironmenttostates ,givingalong-shotreward
2
LS (ϵ). Asinthefirstcase,withoutanylookaheadinformation,theagentcouldcollectarewardat
ϵ
mostonceandobtaininexpectationatmostV0,∗ ≤ E[LS (ϵ)] = ϵ.
ϵ
Ontheotherhand,anylookaheadagentwouldmovefroms tos onlywhenarewardisrealized.
1 2
SincethereareA−1rewardingactionsandH opportunitiestocollectrewards,alookaheadagent
couldcollectatleast
VL,∗ ≥ 1−(1−ϵ)H(A−1) ≥ H(A−1)ϵ−H2(A−1)2ϵ2,
wherethelastinequalityisagainduetotheinequality(1−x)n ≤ 1−nx+n2x2.
Combiningbothbounds,wenowget
V0,∗ ϵ 1
CRL(P,r) = ≤ = .
VL,∗ H(A−1)ϵ−H2(A−1)2ϵ2 H(A−1)−H2(A−1)2ϵ
Thus,foranyδ > 0,thereexistsmallenoughϵsuchthat
1+δ
CRL(P,r) ≤ .
H(A−1)
Case 3: L ∈ [H − 1] and S ≥ AL−1 + 1. We use the same example as in the first case and
n = L−1,ignoringallextrastates. Directsubstitutiontothatboundresultswith
1+δ
CRL(P,r) ≤ ,
(H −L+1)·(A−1)(S −1)
Case4: Finally,ifL = H andS ≥ AH−1,wediscardtheloopattherootandjustbuildafulltree
ofdepthH,leadingtoAH−1 leaves(withAactionseach). Fromtheroot,thefulllookaheadagent
canreachanyleafwitharealizedreward,whichexistswithprobability1−(1−ϵ)AH
. Following
theexactsameanalysiswouldnowyieldCRH(P,r) ≤ 1+δ foranyδ > 0,concludingtheproof.
AH
ModificationwhenS ∈ [An+2,An+1]: Inthiscase,wecannotbuildacompletetreeofdepth
log (S −1)+1. Instead, we start from the complete tree of depth ⌊log (S −1)⌋+1 and use
A A
anyextrastatestocreateadditionalleavesofdepth⌈log (S −1)⌉+1. Thenumberofleavesfor
A
S = An+1wasN = (A−1)An−1. Therefore,(cid:4)S−N0(cid:5) oftheseleaveswillhaveAdescendants
0 A
inthenewtree,increasingthenumberofleavesbyA−1each,whileoneadditional‘old’leafwill
taketherestofthestates. Forthisreason,thetotalnumberofleavesN wouldbe
(cid:22) (cid:23) (cid:18) (cid:22) (cid:23) (cid:19)
S −N S −N
0 0
N ≥ N +(A−1) + S −N −A −1
0 0
A A
(cid:22) (cid:23)
S −N
0
= S − −1
A
(cid:18) (cid:19)
1
≥ S 1− −2.
A
29MERLISBAUDRYPERCHET
Recallingthatineachleaf,wehaveApossibleactions,sorewardscouldberealizedinNAlocations,
andincreasingthedepthby1(sothatthelookaheadagenthasonelessattempt),wecanfollowthe
exactsameanalysisandgetamoregeneralboundof
(cid:18) (cid:19)
1+δ 1+δ
CRL(P,r) ≤ = Θ
(H −⌈log (S −1)⌉)·(S(A−1)−2A) (H −log (S −1))·AS
A A
foranyA+2 ≤ S ≤ AL+1.
C.2. AnalysisofGridMDPs
InthegridMDP,anagentstartsatthebottom-leftcornerofann×ngridandcaneithermoveupor
rightuntilgettingtothetop-rightcorner(‘Manhattannavigation’,seeFigure1(b)). Aftertakingone
lastaction,theinteractionends. Wedenotethestatesontheith column(startingfromtheleft)by
s ,...,s (withs asthebottomstate)andthestatesonthejth row(startingfromthebottom)
i,1 i,n i,1
bys ,...,s (withs astheleftmoststate). Atthetopedgeofthegrid,theagentmustmove
1,j n,j 1,j
right,andattherightedge,itmustmoveup. ThesizeofthestatespaceisS = n2,theactionspace
isofsize(atmost)A = 2andthehorizonisH = 2n−1.
ThisMDPgeneralizesthechainMDPwithA = 2,analyzedinSection5;indeed,bysettingthe
rewardtobenon-zeroonlywhengoingupfromthebottomrow(s ),weeffectivelygetachain
1,j
of length n and a corresponding CR of CRH = CR1 = 1 = 2 . In particular, the reduction
n H+1
immediatelyleadstoanupperboundofO(cid:0)1(cid:1) forCR1 (andCRH),wheretheboundforone-stepis
H
almostworst-case,sinceCR1 ≥ 1 = 1 . Interestingly,thisisanear-worst-caserewardplacement
AH 2H
alsoversusfulllookaheadforthegrid-MDP,aswenowprove.
Onewaytoprovethisistoanalyzeaflowonthegridgraph,whichisequivalenttooccupancyin
deterministicMDPs. Thevalueofthefulllookaheadagentcorrespondswiththemaximalpossible
flowthroughanyedgeinthegraph,whichistheunitflow(d∗ (s ) = 1). Hence,thegoalofthe
i+j−1 i,j
no-lookaheadagentistomakesurethatthereisaminimalflowinalltheedgesofthegraph,and
thisminimumwouldbetheCR.Thiscouldbeachievedbydistributingaflowonthebottomand
leftmoststatesandsendingitinstraightlinestotheothersideofthegrid,asexplainedinFigure3.
Theresultingflowensuresaminimalflowof 1 throughalltheedges. Evenmore,lookingatthe
2(n−1)
flowdescription,wecouldexplicitlywritethestochasticpolicythatachievesthisflowbylookingat
theratiooftheflowineachdirection:

1 i = j = 1 (start)
  2
  n−i i = 1,j ∈ {2,...,n−1} (bottom)
  n−i+1
  1 j = 1,i ∈ {2,...,n−1} (leftmost)
  n−j+1





 1 i,j ∈ {2,...,n−1} (middle)
π(Move-Right|s ) = 2
i,j



 1 i = n,j ∈ {2,...,n−1} (top)







  0 j = n,i ∈ {2,...,n−1} (rightmost)

  1 i = j = n (end)
2
Forthispolicy,itiseasytoprovethattheminimaloccupancydπ (s ,a)islower-boundedby
i+j−1 i,j
1 bydirectlyverifyingontheedgesofthegrid(startingfromthebottomandleftedgesandthen
2(n−1)
30THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
Figure3: Illustration of a possible flow on a grid graph, starting from the bottom-left corner and
ending at the top-right corner. The first step is to distribute the flow on the bottom and
leftmoststates,suchthatthereisexcessflowof 1 flowineachofthesestates(green).
2(n−1)
Attheleftmoststate,thisexcessflowissentatadirectlinetowardstheright(blue),while
in the bottom row, this flow is sent up (red). Such flow ensures that all edges have a
minimalflowof 1 .
2(n−1)
continuingtothetopandrightones),andthenprovingwithasimpleinductionthatstrictlyinsidethe
grid,dπ (s ) = 1 . Thisimpliesthat
i+j−1 i,j n−1
dπ(s,a) 1 1
CRH(P) = max min h = max min dπ (s ,a) ≥ = .
π∈ΠM(h,s,a)∈X d∗ h(s) π∈ΠM(i.j)∈[n]2,a∈A i+j−1 i,j 2(n−1) H −1
Inparticular,forthegridMDP,theworst-caseCRforfulllookaheadisatmostworsebyafactor
of2comparedtotheCRversusone-steplookahead,similartothechainMDP.However,incontrast
tochains,wheretheprophetinequalityensuresaconstantratiobetweenone-stepandfulllookahead,
in grids, this ratio could depend on H. For example, assume long-shot rewards R ∼ LS (1) for
ϵ
arbitrarilysmallϵ. Aswealreadycalculatedthevalueforlong-shotrewards,weknowthatone-step
lookaheadagentseffectivelycollectallexpectedrewardsalongtheirtrajectory(Equation(4))–at
most2H rewards–whilethefulllookaheadagentscollectallreachablerewards(Equation(6))–a
totalofΩ(H2)rewards. Atfirstglance,itmightbeseenasacontradiction,followingalogicthat
no-lookahead no-lookahead one-steplookahead
” = · ”,
fulllookahead one-steplookahead fulllookahead
31MERLISBAUDRYPERCHET
butthecarefulreaderwouldnoticethattheCRsarederivedforverydifferentrewardexpectations;
oneCRiscalculatedforsparsechain-likerewardswhiletheotheriscalculatedfordenserewards
whereallexpectationsareequal.
Denserewards. WeendthisexamplebyanalyzingtheCRwhenrewardsaredense–allrewards
are of unit expectation. Since all reward expectations are equal to 1, regardless of the policy, the
valueofallno-lookaheadagentswouldtriviallybeH. ForthevalueofL-lookaheadagentsweuse
Proposition6andrewritethevaluebydecomposingtodifferentvaluesoft (h)asfollows:
L
(cid:88) (cid:88)
sup VL,∗(P,1) = max 1· dπ (s′)d∗(s|s = s′)
RH∼D(1) π∈ΠM
(h,s,a)∈X s′∈S
tL(h) h tL(h)
 
L H−L+1
(cid:88) (cid:88)  (cid:88) (cid:88) (cid:88) 
= d∗(s)+ max dπ(s′) d∗ (s|s = s′)
h t t+L−1 t
π∈ΠM 
(s,a)∈S×Ah=1 t=2 s′∈S (s,a)∈S×A
Sincetheenvironmentisdeterministic,alloccupanciesd∗ arebinary: oneifastateisreachableand
zerootherwise. Fromtheinitialstate,thereareL2 reachablestatessothefirsttermisequaltoL2.
Forthesecondterm,weboundthenumberofreachablestatesafterexactlyLstepsbyL+1(allthe
possiblenumberof’up’movesbetween0and+L). Thisyieldsthebound
 
 
 
 
H−L+1 
 (cid:88) (cid:88) 
sup VL,∗(P,1) ≤ L2+ max dπ(s′)(L+1)
t
RH∼D(1) π∈ΠM   t=2 s′∈S   
  (cid:124) (cid:123)(cid:122) (cid:125)  
=1
= L2+(H −L)(L+1)
≤ H(L+1)
andresultwithaCRofCRL(P,1) ≥ 1 .
L+1
Thisboundisnear-tight,againusingProposition6. Fortheproof,wefocusonapolicyπ that
iteratesbetweenmovingupandright. Aspreviouslyexplained,thenumberofreachablestateswhen
lookingLstepsforwardisL+1ifwecouldperformallcombinationsofmovingupandright. In
particular, this is the case as long as we are not too close to the top-right border of the grid. By
iteratingthemovementsupwardsandrightwards,foranyh ≤ H −2L,wearrivetoastates such
i,j
thatmax{i,j} ≤
(cid:6)h(cid:7)
≤
(cid:6)2n−1−2L(cid:7)
≤ n−L,whichensuresweareadistanceofatleastLfrom
2 2
theborder. Therefore,wecanbound
H−2L
(cid:88) (cid:88)
sup VL,∗(P,1) ≥ L2+ dπ(s′)(L+1)
t
RH∼D(1)
t=2 s′∈S
(cid:124) (cid:123)(cid:122) (cid:125)
=1
= L2+max{(H −2L−1)(L+1),0}
= Ω(HL),
wherethelastrelationisimmediatelyobtainedbylookingateitherL < H/4orL ≥ H/4. Thus,we
alsohavethatCRL(P,1) = O(1/L),andwecanconcludethatCRL(P,1) = Θ(1/L).
32THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
C.3. Upper-BoundforTransitionLookahead
In this appendix, we analyze the competitive ratio versus one-step transition lookahead agents.
Formally,ateachtimestephandstates ,suchagentsobservewhatthenextstates wouldbe
h h+1
uponplayinganyoftheactionsa ∈ A. Weassumethatthisistheonlyinformationavailabletothe
agent(namely,theagenthasnorewardlookahead). Wealsoassumethattransitionsaregenerated
independentlyatdifferenttimestepsandareindependentoftherewards. Notably,evenwithone-step
information,theCRisexponentiallysmall,asstatedinthefollowingproposition:
Proposition11 ForanyA ≥ 2,H ≥ 5andS ≥
A(1−1 e)H,thereexistsanenvironmentsuchthat
theCRversusone-steptransitionlookaheadagentsisCR ≤ 2 .
(A−1)(1−1 e)H−3
ProofTheenvironmentwebuildisacompletetreeofdepthd(tobedetermined),whereeachnode
hasA−1descendants. Theagentalwaysstartsattherootofthetree. Ateachnode,theagentcan
playA = 1tostayatthesamenode,whiletherestoftheA−1actionsmovetheagenttooneofthe
descendantsofthenodeuniformlyatrandom. OnlyoneleafhasadeterministicunitrewardofR = 1
forallactions,whileallotherleavesyieldnoreward. Aftertraversingthetree,theagentmovestoa
terminalnon-rewardingstates . Thetotalnumberofstatesrequiredtocreatethisenvironmentis
T
d−1 d−2
(cid:88) (cid:88)
1+ (A−1)i ≤ 2+(A−1) Ai = Ad−1+1 ≤ Ad,
i=0 i=0
andthenumberofleavesinthetreeisN = (A−1)d−1. Ano-lookaheadagentcouldnotdobetter
thanrandomlytraversingthetreeandwouldobtainanexpectedrewardofatmostV0,∗ ≤ 1/N.
Ontheotherhand,one-steptransitionlookaheadagentscouldchoosethefollowingpolicy: ifan
actionleadsinthedirectionoftherewardingleaf,takeit;otherwise,waitinthecurrentnode. To
obtainthereward,therehavetobeatleastd−1timestepswhereanactionleadsintherightdirection,
over the span of H −1 attempts (one additional round is required to collect the reward. Letting
(cid:16) (cid:17)A−1
p = 1− 1− 1 ≥ 1− 1 be the probability that such an action exists at a certain node
s A−1 e
(‘success’),thevalueoftheone-steplookaheadagentwouldbeatleasttheprobabilitythatabinomial
distributionBin(n = H −1,p = p )hasatleastd−1successes. Settingd =
(cid:4)(cid:0)
1−
1(cid:1) H(cid:5)
−1,so
s e
thatd−1 ≤
(cid:0)
1−
1(cid:1)
(H −1)−1,weuseHoeffding’sinequalitytoget
e
(cid:18) (cid:18) (cid:19) (cid:19)
1
V1,∗ ≥ Pr Bin H −1,1− ≥ d−1
e
(cid:32) (cid:33)
(cid:18)(cid:18) 1(cid:19) (cid:19)2
≥ 1−exp −2 1− (H −1)−(d−1)
e
1
≥ 1− .
e2
Therefore,thecompetitiveratioisupper-boundedfor
1/(A−1)d−1 2
CR ≤ ≤ .
1− e1
2
(A−1)(1−1 e)H−3
WeremarkthattheconstraintA ≥ 2allowsbuildingsuchatree,whileH ≥ 5ensuresadepthofat
leastd ≥ 2.
33MERLISBAUDRYPERCHET
AppendixD. AuxiliaryLemmas
Lemma12 Letd ∈ Nandα ∈ Rd. Also,letD ⊂ Rd beaconvexcompactnonemptyset. Then,
+ +
yTx y
i
inf max = maxmin ,
x∈[0,1]d y∈D αTx y∈D i∈[d] α i
wherewedefineallratiostobe+∞ifthedenominatorequalszero.
ProofWefirstremarkthatifα = 0foralli ∈ [d], thenbythe definitionofthedivisionbyzero,
i
bothsidesaretriviallyequalto+∞,andtheresultholds. Thus,fromthispointonwards,weassume
w.l.o.g. thatforsomei ∈ [d],itholdsthatα > 0.
0 i0
StepI: Westartfromanalyzingthel.h.s. problemandshowingthat
yTx
inf max = inf maxyTz.
x∈[0,1]d y∈D αTx z∈Rd y∈D
+
αTz=1
Noticethatchoosingx = 1{i = i }leadstoaboundedvalueof maxy∈Dyi0 < ∞,sothevalueis
i 0 αi0
finite–therecannotbeasolutionsuchthatαTx = 0(andthevalueis+∞),andwecanw.l.o.gadd
the constraint αTx > 0. We further remark that both the numerator and denominator are always
non-negative,sotheinfimumisboundedfrombelowby0. Giventhat,theinternalproblemisalways
well-defined,andthemaximizerisgivenbyy ∈ argmax yTx.
x y∈D
We next show that the constraints x ∈ [0,1]d,αTx > 0 can be replaced by the constraints
x ∈ Rd,αTx = 1. First, for any x ∈ [0,1]d s.t. αTx > 0, define z = x ∈ Rd, for which
+ x αTx +
αTz = 1and
yTx x
max = maxyT = maxyTz ≥ inf maxyTz.
y∈D αTx y∈D αTx y∈D x z∈Rd y∈D
+
αTz=1
Thus,wehavetheinequality
yTx yTx
inf max = inf max ≥ inf maxyTz.
x∈[0,1]d y∈D αTx x∈[0,1]d y∈D αTx z∈Rd y∈D
+
αTx>0 αTz=1
On the other hand, for any z ∈ Rd s.t. αTz = 1, define x = z (which is well defined due
+ z maxizi
totheconstraints). Forthischoice,wegetthatx ∈ [0,1]d andαTx = αTz = 1 > 0. In
particular,onecanwritez = xz
,whichimpliesz
that
z maxizi maxizi
αTxz
x yTx yTx
maxyTz = maxyT z = max z ≥ inf max .
y∈D y∈D αTx z y∈D αTx z x∈[0,1]d y∈D αTx
Therefore,wealsohavetheotherinequality
yTx
inf maxyTz ≥ inf max ,
z∈Rd y∈D x∈[0,1]d y∈D αTx
+
αTz=1
34THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
whichimpliesequality
yTx
inf max = inf maxyTz.
x∈[0,1]d y∈D αTx z∈Rd y∈D
+
αTz=1
StepII: Applyingtheminimaxtheorem.
Theobjectiveislinearinz,y (andthusconvexandconcaveinthevariables,respectively),and
thesetD isconvexandcompact. Theconstraintonz isalsoconvex,thoughnotcompact,butthisis
easilyfixable;noticethatforallisuchthatα = 0,z doesnotaffecttheconstraint. Ontheother
i i
hand, setting z > 0 can only increase the objective since y ,z ≥ 0. Indeed, for any z ∈ Rd s.t.
i i i +
αTz = 1, letting z˜ = z 1{α > 0}, we have αTz˜ = 1 and yTz ≤ yTz˜. Hence, w.l.o.g., we can
i i i
always add the constraint that z = 0 for all i ∈ [d] with α = 0. With this additional constraint,
i i
thesetZ = (cid:8) z ∈ Rd|αTz = 1,∀is.t. α = 0 : z = 0(cid:9) isconvexandcompact,sotheinfimumis
+ i i
actuallyaminimumandwecanapplytheminimaxtheoremtoobtain
inf maxyTz = maxminyTz
z∈Rd y∈D y∈D z∈Z
+
αTz=1
StepIII: Solvingtheinternalproblemforfixedvaluesofy.
Atthispoint,wenotethatcomponentswhereα = 0donotaffecteitherthevalueorthesolution.
i
Therefore,fromthispointonwards,weassumew.l.o.gthatα > 0foralli;wewillthenapplyour
i
results only on the subset of components with α > 0. Given that, we also assume w.l.o.g. that
i
y > 0foralli–otherwise,theconstraintcouldbemetbylettingz > 0forcomponentswithy = 0,
i i i
whichwouldleadtotheoptimalvalueof0(weverifythiscaseattheendoftheproof).
Thus,wefocusonsolvingthefollowingproblem: foranyfixedy ∈ Rd s.t. y > 0foralli,solve
i
min yTz
z
s.t. z ≥ 0, ∀i ∈ [d],
i
αTz = 1.
Duetothelinearityofboththeobjectiveandconstraints(inz),KKTconditionsarebothnecessary
andsufficientforthesolutionofthisproblem. Lettingµandλbethedualvariablesfortheconstraints
z ∈ Rd andαTz = 1,respectively,theKKTrequiresthatforalli ∈ [d],
+
y −µ −λα = 0 (stationarity)
i i i
µ z = 0 (complementaryslackness)
i i
µ ≥ 0, z ≥ 0 (feasibility1)
i i
αTz = 1. (feasibility2)
Forthestationaritytoholdwiththenon-negativityofµ ,wemusthavethatλ ≤ min yi. More-
i i∈[d] αi
over,ifthisisastrictinequality,allµ arestrictlypositive,whichleadstotheinfeasiblezero-reward
i
vector(duetothecomplementaryslackness). Therefore,wecanconcludethatλ = min yi,and
i∈[d] αi
soµ = 0onlyincoordinateswherethisminimalratioinachieved. Bycomplementaryslackness,
i
z = 0fortherestofthecoordinates.
i
35MERLISBAUDRYPERCHET
Substitutingintheequalityconstraint,weget
d d
(1) (cid:88) (2) (cid:88) (cid:88) y i (2) 1 (cid:88)
1 = α z = α z = z = y z .
i i i i i i i
λ λ
i=1 i:yi=λ i:yi=λ i=1
αi αi
Explicitly,(1)isbytheconstraintand(2)issincez = 0when yi > λ. Reorganizing,wegetthat
i αi
thevalueoftheinternalproblemis
d
(cid:88) y i
y z = λ = min .
i i
i∈[d] α i
i=1
Weendbyremarkingthatwheny = 0forsomei ∈ [d],thevaluebecomes0sothattheresultalso
i
holdsinthiscase.
Summary: Combiningallpartsoftheproof,wegot
yTx y
i
inf max = max min
x∈[0,1]d y∈D αTx y∈D i:αi>0 α i
Ifwedefinetheinternalvaluetobe+∞whenα = 0,wecanfurtherwrite
i
yTx y
i
inf max = maxmin ,
x∈[0,1]d y∈D αTx y∈D i∈[d] α i
whichconcludestheproof.
Remark13 Followingalmostidenticalproof,wecouldsimilarlyprovethat
yTx y
i
inf max = maxmin .
x∈Rd y∈D αTx y∈D i∈[d] α i
+
The only change would be in the first step; using the same rescaling idea (z = x ∈ Rd), one
x αTx +
couldprovethat
yTx
inf max ≥ inf maxyTz,
x∈Rd y∈D αTx z∈Rd y∈D
+ +
αTz=1
whilethereverseinequalitytriviallyholdssince(cid:8) z ∈ Rd|αTz = 1(cid:9) ⊂ Rd. Therestoftheproof
+ +
followswithoutanychange.
Notably, since this lemma is used in all our proofs to calculate the CR for the worst-case
reward expectations, it implies that we would get the same results were we to define the CR as
CRL(P) = inf CRL(P,r).
r ∈RSA
h +
36THEVALUEOFREWARDLOOKAHEADINREINFORCEMENTLEARNING
Lemma14 Let d ∈ N. Also, let D ∈ Rd be a convex compact set and P ∈ Rd be a convex
+ +
compactpolytope,bothassumedtobenonempty. Then
y y
i i
inf maxmin = minmaxmin ,
α∈P y∈D i∈[d] α i α∈P y∈D i∈[d] α i
wherewedefineallratiostobe+∞ifthedenominatorequalszero.
ProofWeassumew.l.o.g. thatP ̸= {0},sincetheinfimumoverasingletonisalwaysequaltothe
minimum(inthiscase,bothequal+∞),andtheresulttriviallyholds.
Next,forallα ∈ P,definef(α) = max min yi. Noticethatforanyα ∈ P s.t. α ̸= 0,
y∈D i∈[d] αi
thereexistsi ∈ [d]suchthatα > 0,andso
0 i0
y y
f(α) = maxmin
i
≤ max
i0
< ∞,
y∈D i∈[d] α i y∈D α i0
wherethelastinequalityfollowsfromthecompactnessofD. Inparticular,sinceP ̸= {0}andis
nonempty,suchα¯ ̸= 0exists,andthusinf f(α) ≤ f(α¯) < ∞,sothevalueattheoptimization
α∈P
probleminthel.h.s. isfinite.
Wenextprovethatf(α)isquasi-concaveoverP,namely
∀α ̸= β ∈ P,λ ∈ (0,1) : f(λα+(1−λ)β) ≥ min{f(α),f(β)}.
First,ifα ̸= 0andβ = 0(ortheopposite),foranyλ ∈ (0,1)wehavethat
1
f(λα+(1−λ)β) = f(λα) = f(α) ≥ f(α) = min{f(α),f(β)},
λ
whereweusedthenon-negativityoff(α)andtheconventionthatf(0) = +∞. Next,assumethat
bothα,β ̸= 0. Also,letyα suchthat
y
yα ∈ argmaxmin i
y∈D i∈[d] α i
andsimilarlydefineyβ. Suchy mustexist,sincewecouldalwayswrite
y y
i i
f(α) = maxmin = max min .
y∈D i∈[d] α i y∈D i∈[d]:αi>0 α i
ThemaximumoverafinitenumberoflinearfunctionsiscontinuousandthesetD iscompact,soa
maximizerinD isalwaysattainable. Usingthesedefinitions,wehave,
y
i
f(λα+(1−λ)β) = maxmin
y∈D i∈[d] λα i+(1−λ)β i
λyα+(1−λ)yβ
≥ min i i (D isconvex)
i∈[d] λα i+(1−λ)β i
(cid:40) (cid:41)
(∗) yα yβ
≥ minmin i , i
i∈[d] α i β i
(cid:40) (cid:41)
yα yβ
= min min i ,min i
i∈[d] α i i∈[d] β i
= min{f(α),f(β)}.
37MERLISBAUDRYPERCHET
Relation(∗)isduetotheinequality a+c ≥ min(cid:8)a, c(cid:9) fora,b,c,d ≥ 0,andonecouldeasilyverify
b+d b d
thattheinequalityisstillvalidwheneitherb = 0ord = 0.
Finally, recall that P is a compact convex polytope; in particular, each interior point could
be represented as a convex combination of one of its finite extreme points ext(P). Then, by the
quasi-concavity,thevalueofeachinteriorpointislower-boundedbythevalueofatleastoneofthese
extremepointssothat
inf f(α) = min f(α).
α∈P α∈ext(P)
ThisprovesthattheinfimumisattainablebyapointinP,thusconcludingtheproof.
38