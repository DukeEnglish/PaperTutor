A Systematic Review of XR-based Remote Human-Robot
Interaction Systems
XIANWANG,
TheHongKongPolytechnicUniversity,HongKongSAR
LUYAOSHEN,
TheHongKongUniversityofScienceandTechnology(Guangzhou),China
LIK-HANGLEE∗,
TheHongKongPolytechnicUniversity,HongKongSAR
Brief visual summary of XR-based remote HRI 3 | Virtual interface
1 | XR technologies 360° 360°
Digital twin Direct Dreicgoitnasl ttrwucinti o+n 3D Dreinreccotn +st 3ruDction 3D reconstruction Vroirotmual control CAoRntext-aware
Virtual interface 4 | User’s perspective
VR HMD AR HMD
Local environment
360 MR C°ave D we itc ho ru op bli on tg s C wo itu hp rl oin bg ots D pey rn sa pm ecic tive Bird's-eye view
5 | Robot and task classification
Mobile AR i
Robot classification
2 | Interaction modalities
Robotic arm Mobile robot Drones/UVA Humanoid robot D roo bu ob tle-armed Medical robotics RMoobboilteic r aorbmo+t
Task classification
Controller Gesture
G/prlaabcbemingen/pticking Navigation IMnadnuustfraicatlu/ring Escnavnironmental Surgery Search G Ena tm ere ts a/ inment
i
Joystick Haptic devices 6 | Enhancement locations and types
Enhancement locations
Motion capture Walk Remote environment
Enhancement locations
VE User Robot (virtual) Robot (real) RE Object (virtual)
2D screen Voice Enhancement types
Voice Video Text Ray Highlight Haptic Graphic Avatar 3D object
Glove Head Gaze
Fig.1. BriefvisualsummaryofthecurrentstateofthedomainconcerningXR-basedremoteHRI,summarizes
thesixkeydimensionsofcurrentsystemdesign.
Thissurveyprovidesanexhaustivereviewoftheapplicationsofextendedreality(XR)technologiesinthe
fieldofremotehuman-computerinteraction(HRI).Wedevelopedasystematicsearchstrategybasedonthe
PRISMAmethodology.Fromtheinitial2,561articlesselected,100researchpapersthatmetourinclusion
criteriawereincluded.Wecategorizedandsummarizedthedomainindetail,delvingintoXRtechnologies,
includingaugmentedreality(AR),virtualreality(VR),andmixedreality(MR),andtheirapplicationsin
facilitatingintuitiveandeffectiveremotecontrolandinteractionwithroboticsystems.Thesurveyhighlights
existingarticlesontheapplicationofXRtechnologies,userexperienceenhancement,andvariousinteraction
∗L.-H.Leeisthecorrespondingauthor.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfee
providedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeand
thefullcitationonthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACMmustbehonored.
Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires
priorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
Conferenceacronym’XX,June03–05,2018,Woodstock,NY
©2018AssociationforComputingMachinery.
ACMISBN978-1-4503-XXXX-X/18/06...$15.00
https://doi.org/XXXXXXX.XXXXXXX
1
4202
raM
81
]CH.sc[
1v48311.3042:viXraConferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
designsforXRinremoteHRI,providinginsightsintocurrenttrendsandfuturedirections.Wealsoidentified
potentialgapsandopportunitiesforfutureresearchtoimproveremoteHRIsystemsthroughXRtechnology
toguideandinformfutureXRandroboticsresearch.
CCSConcepts:•Human-centeredcomputing→Virtualreality;Mixed/augmentedreality;Collabo-
rativeinteraction;Ubiquitousandmobiledevices.
AdditionalKeyWordsandPhrases:Human-robotinteraction,ExtendedReality,VirtualReality,Augmented
Reality,Teleoperation,Remotecollaboration
ACMReferenceFormat:
XianWang,LuyaoShen,andLik-HangLee.2018.ASystematicReviewofXR-basedRemoteHuman-Robot
InteractionSystems.In.ACM,NewYork,NY,USA,35pages.https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
ThefieldofHuman-RobotInteraction(HRI),originallyperceivedasthestudyofhumanengagement
withrobots,hasevolvedtoinvestigatethedesignofrobotsforsociallymeaningfulinteractions
withhumansandtheenhancementoftheseinteractions[105].HRIaimstoengineerrobotscapable
ofeffectivelycollaboratingwithhumansacrossdiversecontexts,suchasindustrialenvironments,
domesticsettings,andeducationalinstitutions.Inourincreasinglyglobalizedanddigitalizedworld,
whereadvancesincommunicationandmobiletechnologiesaredrivingtherapiddevelopmentof
collaborationconcepts,traditionalface-to-faceinteractionsarebeingcomplemented,and,incertain
instances,replacedbyremotecollaborationmechanismsthatovercomegeographicalandtemporal
constraints.Remotehuman-robotcollaborationholdsconsiderablepotentialtorevolutionizevarious
fields.Forinstance,itcanfacilitatetasksinhazardousenvironmentsbyallowinghumanstocontrol
robotsfromasafedistance,oritcanenhancecomplextasksbyenablingrobotstorelayinformation
from remote locations to humans. The implications of this form of collaboration include safer
workplaces,improvedefficiency,andgreateraccessibility.
Despitevideoconferencingandteleconferencingservingaseffectivetoolsforremotecollabora-
tion,thesetechnologiesexhibitlimitationswhenappliedtocomplextaskswithinthecontextof
remotehuman-robotcollaboration.Particularlyinscenariosinvolvingroboticremotecontrolor
teaching,thelackofintuitiveandcontextualunderstandingofferedbysimplevideointerfacescan
posesubstantialchallenges.Toovercometheselimitations,ExtendedReality(XR)–encompassing
AugmentedReality(AR),VirtualReality(VR),andMixedReality(MR)–offersapromisingsolution.
ARoverlaysdigitalinformationontotherealworld,VRimmersesusersinacompletelydigital
environment,andMRblendsrealandvirtualworlds.Thesetechnologiesprovideafusionofthe
digitalandphysicalworlds,allowingphysicalanddigitalobjectstocoexistandinteractinreal
time.XRtechnologiesareincreasinglysophisticatedandportable,presentingnewopportunities
forHRI.DevicessuchastheMetaQuest2andHololens2exemplifythepotentialofXRtoenhance
remoteHRI.Forexample,withtheassistanceofXRtechnology,novicescanperformriskytasks
(e.g.,welding)inasafespaceandinamoreintuitiveway(e.g.,withthefirstviewoftherobot).In
addition,operatorscanuseXRtoswitchbetweendifferentlocationstocontroltherobotwithout
needingtomoveinphysicalspace.XRempowersremoteHRItobemoreimmersive,intuitive,and
effective.
However,numerouschallengesneedtobeaddressedtounlockXR’sfullpotentialinthecon-
textofremotehuman-robotinteraction.Thesechallengesincludedesigningintuitiveinteraction
techniques,reducingremotecontrollatency,andevaluatingremoteHRIsystems.Giventhatthis
remainsanunder-researchedtopic,thispaperseekstocontributetothisemergingfieldofstudy.We
provideasystematicliteraturereviewofXRtechnology-basedremotehuman-robotcollaboration
2XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
applications,highlightkeyadvancements,andidentifyareasthatdeservetobestudiedinfurther
research.Thecontributionsofthissurveyareasfollows:
(1) ProvideacomprehensivereviewofHRIsystemdesignbasedonXRtechnologyforremote
controlscenarios.
(2) Identifygeneralconvergencesanddivergencesinsystemdesignwithintheexistingliterature.
(3) ProposearesearchagendaforfutureXR-basedremoteHRI.
1.1 OverviewofXR-basedRemoteHuman-RobotInteraction
Figure1providesabriefpictorialsummaryofthecurrentstateofthedomainconcerningXR-based
remoteHRI.Inourproposedmodel,weestablishtwodistinctspaces:the‘localspace’andthe
‘remote space’, The survey scope is highly relevant to these two spaces. Thus, we give their
definitionsasfollows.
1.1.1 LocalSpace: ‘Localspace’referstothephysicalenvironmentinwhichtheuserislocated.It
isoftenequippedwithXRtechnologies,suchasARorVRheadsets,thatcanoverlayorimmerse
theuserinavirtualenvironment(seeFig.1leftofthecenterschematic).Thisspaceiscrucialfor
theuser’sinteractionwiththerobot,asithoststhevirtualinterfaceforcontrollingtherobot.For
example,inthelocalspace,auserwearingaVRheadsetmayseearoboticdigitaltwin(seeFig.1
toprightofthecenterschematic)inthevirtualspace.Thisvirtualinterfaceallowstheusertoissue
commandstotherobotlocatedinanotherspacethroughvariousinteractionmodalities,suchas
gesturecontrol.Thelocalspaceisdesignedtobeintuitiveanduser-friendly,allowingtheuserto
manipulatetherobottoperformcomplextaskswithouthavingtobephysicallypresentinthesame
spaceastherobot.
1.1.2 RemoteSpace: Conversely,‘remotespace’isthephysicalenvironmentwhererobotsoperate
andperformtheirtasks.Thiscouldbeafactoryhandlinghazardousmaterials,adistantplanet,or
acomplexsurgicalfieldwherethedirectpresenceofhumansisimpossibleorundesirable.The
robotactsasanagentfortheuser,performingtasksfromthelocalspaceviaavirtualinterface.
For example, in a manufacturing facility, a robotic arm is responsible for handling hazardous
chemicalsonaconveyorbeltundertheguidanceofanoperatorinlocalspace(seeFig.1lower
rightofthecenterschematic).Theremotespaceischaracterizedbyitstask-orientedproperty,
utilizingthephysicalcapabilitiesoftherobottoperformspecificactionsthatbenefitfromorrequire
teleoperation.
Toprovideacomprehensiveunderstandingofthetwospacetypesmentionedabove,wehave
conducted a detailed synthesis and analysis of the relevant literature. Based on this, we have
categorizedthesystemdesignwithinthisdomainalongseveraldimensions:1)theXRtechnologies
approachusedinremoteHRI;thisdimensionexplorestheuseofvariousXRtechnologiessupported
inlocalspace–suchasVR,AR,andMR–tobridgethegapbetweenlocalandremotespaces,
providingvirtualmanipulationinterfacesforlocalusers.Ourfocusisonhowthesetechnologies
can be utilized to create immersive and intuitive interfaces to control robots in remote space
locations.2)theinteractionmodalitiesbetweentheuserandthevirtualinterface;interaction
modalities are methods by which users communicate with and control virtual interfaces. This
includes gesture control, controllercontrol,and motion capture, etc. The choice ofinteraction
method influences the efficiency of the user’s manipulation of the virtual interface, which in
turn affects the robot’s ability to accurately and efficiently perform tasks in the remote space,
emphasizingthedesignrequirementsofcateringtothecontextofthetaskandtheuser’sphysical
environmentinthelocalspace.3)thedesignofthevirtualinterface;thisdimensionexamines
howinformationinremotespaceispresentedinlocalspace(e.g.,adigitaltwinofarobotina
3Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
remotespaceora3Dreconstructionofaremoteenvironment),andtheinterfacedesignmayneed
totakeintoaccountthecognitiveloadoftheuserandstrikeabalancebetweencomplexityand
usability.4)the user’sperspectiveofobservingtherobot’sactionsintheremotespace;This
dimension is concerned with how the user perceives and understands the robot’s movements
andremoteenvironmentthroughthevirtualinterface.Itdealswiththeperspectivefromwhich
theuserobservestherobotorremotespace(coupledtotherobot,decoupledfromtherobot,or
bird’s-eyeview,etc.)andhowthisperspectiveenhancesorhinderstaskexecution.Thechoiceof
perspectiveaffectstheuser’ssituationalawareness,thedesignofthevirtualinterface,andthe
intuitivenessofcontrollingtherobot.5)therobotandspecifictasksclassificationintheremote
space;thisdimensioninvolvescategorizingrobotsbasedontheirphysicalcapabilitiesandthetasks
theyperform.Understandingthisdimensionhelpsdeveloperscustomizethevirtualinterfacesand
userperspectivesofXRtechnologiestomeetthespecificneedsofrobotsandtasks.and6)the
enhancementlocationsandtypesofthemultimodalelements;thisdimensionexamineshow
multimodalelements(e.g.,visual,auditory,andhapticfeedback)canbeaugmentedandintegrated
intolocalspaceandremotespacetoimproveusercontrolandperceptionoftherobot.Enhancement
typesmayincludehighlightingimportantcontrolorfeedbackcues(e.g.,text,video,3Dgraphic
overlays)andprovidingsimulatedhapticfeedbacktoemulatethephysicalinteractionofobjectsin
theremotespace.
1.2 ExistingSurveys
PreviousresearchhasconductedseparateinvestigationsintoXRtechnology,remotecollaboration,
andHRI.However,theintersectionofthesethreedimensions,particularlyremoteHRI,hasremained
largelyunexplored.Notably,Schaferetal.[101],andWangetal.[130]haveconductedreviewsof
remotecollaborationsystemsusingXRtechnology.Schaferetal.emphasizedsynchronousremote
collaborationsystems,whereasWangetal.concentratedonphysicaltasks.However,theirmain
focus is human-to-human remote collaboration instead of human-robot interaction. Moreover,
whenanalyzingHRIsystemsorcollaborativerobots,themajorityofresearchershaveprimarily
exploredtheapplicationofARtechnology[9,27,31,47,76,92,112].ThestudybyDianatfaret
al.[32]encompassesVRtechnologybutonlysynthesizesVRsimulationapplicationsforsurgical
robotsanddoesnotadequatelyconsiderinteractionscenariosbetweenhumansandtangiblerobots.
Walkeretal.[126]proposedataxonomyforHRIsystemsusingXRtechnology,buttheirprimary
focuswasnotHRIinremotecontexts.
Inreviewingtheexistingliterature,wenoticethatnoneoftheexistingsurveyarticlessystemati-
callycategorizeandsynthesizetheusageofXRtechnologiesinremoteHRIsettings.Incontrast,
ourarticleaddressesthegapsinthecurrentresearch,includingallXRtechnologies,anddivesinto
theissueofHRIinremotecontexts.Thissystematicreviewcanserveasthefirstcomprehensive
guideforresearcherstosituatetheirworkwithinabroaderframeworkandexploreinnovative
systemsforXR-basedremoteHRI.
1.3 StructureoftheSurvey
Theremainderofthispaperisstructuredasfollows:Section2thoroughlyexplainsthemethodology
employed for this survey. Section 3 offers an in-depth discussion and analysis of the included
articles,specificallyfocusingontheutilizedtechniques,typesandtasksofremoterobots,task
evaluation,theroleofXRtechniques,multiplayer/robotsupport,andsystemlatency.Thisanalysis
isbasedonourdevelopedtaxonomyanddataextractionrules.InSection4,adetaileddiscussion
andanalysisconcerningthedevelopmentandrecentadvancementsinXR-basedremoteHRIare
presented.Subsequently,Section5delvesintothechallengestotheresearchandsuggestspotential
4XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
future research directions. Finally, Section 6 provides a comprehensive summary of the entire
paper.
2 METHODOLOGY
Toensuremethodologicalrigourandtransparencyinourliteraturereviewprocess,weutilizedthe
PreferredReportingItemsforSystematicReviewsandMeta-Analyses(PRISMA)framework,as
recommendedbyTakkoucheetal.[115].Thereviewwasconductedcollaborativelybymultiple
authors,usinganonlinetoolnamedCovidence1.ThecompletePRISMAresultscanbefoundin
Figure2.Uponcompletingoursearch,webeganastructuralprocessoffilteringthrough2,588
articles.Weinitiallyremoved27duplicatearticles,leavinguswithapoolof2,561articlestoexamine.
Wescreenedthesearticlesbasedontheirtitlesandabstracts,leadingtotheexclusionof2,216
articlesthatdidnotmeetourcriteria.Followingafull-textevaluation,anadditional245articles
werefurtherexcluded.ThespecificinclusionandexclusioncriteriaaredescribedinSection2.2.Our
literaturereviewprocesswasconductedintwophasestocapturethemostcurrentresearch.The
firstroundofdataextractiontookplaceinMay2022,followedbyasecondroundinDecember2023.
Thistwo-phaseapproachallowedustoincorporatethelateststudiesandensureacomprehensive
reviewoftheliteratureuptoDecember2023.Ultimately,weselected100articlesfordataextraction
andfurtheranalysisinoursurvey.
Fig.2. SystematicreviewprocessusingPRISMA.
2.1 SearchStrategy
2.1.1 Keywords. Thekeywordsprimarilyreflectthethreedimensionsofremoteoperations,human-
robotinteractionandVAMtechnology.Consideringtheterminologicalexpressionsindifferent
contexts,weselectedthekeywords“distributed”,“remote”,“teleoperation”,“telerobotics”,“telep-
resence”and“spatial”fortheremoteenvironment.Themainkeywordsinvolvedinhuman-robot
interactionare“robotic”,“robot”,“machine”,“human-robotinteraction”,“human-robotcollabora-
tion”,“interaction”,“collaboration”,“cooperation”,“collaborate”andtheabbreviations“HRI”and
“HRC”,thekeywordsandabbreviationsrelatedtoVAMtechnologywehavechosen“virtualreality”,
“augmentedreality”,“mixedreality”,“vr”,“ar”and“mr”.
1https://www.covidence.org/
5Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
2.1.2 Databases. Toensureacomprehensiveliteraturereview,wesearchedforthemostrelevant
articlesavailableonseveralpublicationdatabases,includingACMDigitalLibrary,IEEEXplore,
and ScienceDirect (Elsevier). To supplement this search, we also conducted a snowball search
onGoogleScholar,whichinvolvedreviewingthereferencelistsofrelevantarticlestoidentify
additionalpublications.Additionally,weutilizedtheConnectedPapers2onlinetooltoidentify
relatedarticlesandbroadenoursearchuntilnonewrelevantarticlesappeared.Thisapproach
allowedustothoroughlyexploretherelevantliteratureandidentifykeypublicationsrelatedto
ourresearchquestion.
2.2 InclusionandExclusionCriteria
DuringtheScreeningandEligibilitystagesofourPRISMAreview,wecarefullydevelopedappro-
priateinclusionandexclusioncriteriatoguideourselectionprocess.Wehavesummarizedthese
criteriainTable1,whichoutlinesthefactorsthatweusedtoidentifyrelatedworkofinterest.Our
analysisfocusedonarticlesthatmetoneormoreoftheinclusioncriteria,whilealsoensuringthat
anyarticlesthatmetatleastoneoftheexclusioncriteriawereremovedfromconsideration.By
adheringtotheseguidelines,wedidourutmosttoensurearigorousandthoroughreviewofthe
relevantliterature.
Table1. InclusionandExclusionCriteria
Criterion Description
𝐼 1 Theresearchproposesthedesign,developmentorsystemforremotecontrolofrobotsbasedonaugmented,virtualormixed
reality.
𝐼 2 Theresearchusedaugmented,virtualormixedrealityasamethodofremotecontroloftherobot.
𝐸 1 Thesystemproposedinthestudydoesnotsupportremotecontrol.
𝐸 2 Thestudydoesnotinvolverealrobots.
𝐸 3 Thetargetofresearchcontrolisnotarobot.
𝐸 4 Thestudydidnotuseaugmented,virtualormixedrealitytechnologies.
𝐸 5 Thisisnotatechnicalarticle.
2.3 DataExtractionandAnalysis
Toextractrelevantinformationfromtheincludedarticles,wedevelopedadataextractionrubric
thatenabledustosystematicallycapturethekeyaspectsofaugmented,virtualormixedreality
technologies,Human-robotinteraction,andremotecontrol.Initially,thefirstauthorofthissurvey
selected10articlesinapseudo-randommanner([7,12,48,58,66,119,127,129,142,156]),and
developeditemsbasedontherelevantaspectsidentifiedinthearticles.Theinitialdataextraction
rubricwasthenevaluatedbyallauthorsandrefinedintothefinalversiondescribedinTable2.The
firstandsecondauthorsindependentlyextracteddatafromeacharticleusingthisfinalisedrubric.
Incasesofconflictingdata,consensuswasreachedthroughdiscussionsamongtheauthors.
DataextractionitemsDE1-DE3pertaintogeneraldescriptorsofthepaper,includingstudy
number(authoranddate),title,andkeywords.XRtechnologies(DE4)encompasscommontypes
of augmented, virtual, and mixed reality technologies, such as VR HMD, VR video, AR HMD,
MobileAR,projectors,MR,CAVE,andothers.Interactionmodalities(DE5)highlightthevarietyof
hardwareandtheirapplicationsinvirtualenvironments.Thetelepresenceinterface(DE6)allows
userstooperatetherobotmoreintuitivelywithavirtualinterface.Itmayincludeadirectinterface,
adigitaltwinoftheremoterobot,avirtualcontrolroom,oradigitaltwinoftheremoterobot
combinedwitha3Dreconstructionoftheremoteenvironment.Theuser’sperspective(DE7)refers
totheuser’sviewpoint,suchasbeingboundtotherobot’sperspective,observingtherobotfrom
2https://www.connectedpapers.com/
6XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
adetachedthirdvieworatop-down"God’sview,"orhavingtheabilitytochangeperspectives
basedonneed.GenerictypesofrobotsaredescribedunderDE8,whilespecifictasks(DE9)that
canbeperformedbythehuman-robotcollaborationsystemareofinterest,asdifferenttasksmight
requirevariousrobotsortelepresencedesigns.RealizingthefullpotentialoftheXRalsointendsto
improvetheefficiencyandintuitivenessofremotecontrol.Thesystemmayhavebeenenhanced
indifferentlocations(DE13)withmultimodalenhancements(DE14),e.g.,haptic,video,2Dor
3Doverlays, avatars,andmore. Inaddition,human-robotinteractionmay includemultiplayer
or robot collaboration (DE12), necessitating distinctions between one-to-one, one-to-multi, or
multi-to-multicollaboration,whichcouldinfluencestudydesign.Theevaluationmethodofthe
study(DE10)isalsoessential;itcouldbequantitative,qualitative(e.g.,questionnaires),orfocused
ontherobotratherthantheuser,includingthepotentialpresenceofdelays(DE11)inremote
controls.
Table2. DataExtractionRubricfortheSelected100Articles
ID DataExtraction Type
DE1 StudyID Opentext
DE2 Title Opentext
DE3 Keywords Opentext
DE4 UsedXRtechnologies VRHMD,ARHMD,MR,MobileAR,CAVE,Other
DE5 Interactionmodalities Gesture,Controller,Joystick,Gaze,Head,Hapticdevices,Motioncapture,Walk,2Dscreen,
Voice,Glove,Other
DE6 Virtualinterfaces Direct,Digitaltwin,Virtualcontrolroom,Digitaltwin+3Dreconstruction,Direct+3Drecon-
struction,3Dreconstruction,Context-awareAR,Multiple,Other
DE7 Userperspective Couplingwithrobots,Decouplingfromrobots,Dynamicperspective,Bird’s-eyeview,Other
DE8 Typeofrobots Mobilerobot,Drones/UAV,HumanoidRobot,Roboticarm,Mobilerobot+roboticarm,Double-
armedrobot,MedicalRobotics,Other
DE9 Specifictasksinvolved Navigation, Grabbing/Picking/Placement, Surgery, Game/Entertainment, Indus-
trial/Manufacturing,Search,Environmentscan,No,Multiple,Other
DE10 Howwasitmeasuredorevaluated? Time/accuracyofthetask,Interviews,Questionnaire,AR/VRperformance,Comparison,N/A,
Other
DE11 Wasthereadiscussionaboutdelays? No,Yes(Times),Other
DE12 Supportmultiplayercollaboration? No,Multi-user-onerobot,Oneuser-multi-robot,Multi-Multi,Other
DE13 Wherearetheenhancementslocated? User,Robot(real),Robot(virtual),Object(virtual),Realenvironment(RE),Virtualenviron-
ment(VE)
DE14 Enhancementtypes Haptic,Voice,Graphic,Text,3DObject,Highlight,Ray,Avatar
3 RESULTSANDDESCRIPTIVESTATISTICS
3.1 OverviewofIncludedArticles
Wemeticulouslyextractedpertinentinformationfromthe100articlesidentifiedduringourscreen-
ing.Theselectedarticlesspanfrom2013to2023,andtheannualpublicationcountisillustratedin
Figure3.Thesearticlesarefromwell-knownvenuesforHRIandhuman-computerinteraction(HCI),
includingtheIEEEInternationalConferenceonIntelligentRobotsandSystems(IROS),ACM/IEEE
InternationalConferenceonHuman-RobotInteraction(HRI),IEEEInternationalSymposiumon
Robot and Human Interactive Communication (RO-MAN), and the ACM Symposium on User
InterfaceSoftwareandTechnology(UIST).Thesedataweresubsequentlyanalyzedandsummarized
bothstatisticallyandgraphically,withadditionalqualitativeinsightsemergingduringtheiterative
analysis.AcomprehensivelistofdataextractsfortheincludedarticlescanbefoundinAppendixA.
Overthepastdecade,therehasbeenageneralincreaseinthenumberofarticlespublishedonthe
investigatedtopics,indicatinggrowinginterestandsignificanceinrecentyears.Thistrendcould
beattributedtothelandscapeofXRtechnologymaturing,anditisworthnotingthatthevolume
ofpublicationsinthisfieldreacheditszenithin2023.
7Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
Fig.3. Articlesperyear(N=100)
Thearticlesoriginateprimarilyfromtheroboticsandmanufacturingdomains,andouruniqueness
istoextendthedomainstoremoteinteraction.Keywordssuchas“reality”and“virtual”frequently
appear, with “augmented” being a high frequency term, although less so than “virtual”. This
observationmaysuggestapreferenceforvirtualrealityoveraugmentedrealityinthisresearch
area,butamoredetailedanalysisisneededtoconfirmthis.Keywordssuchas“robot”,“robots”,and
“robotics”arealsoprevalentinstatistics.Thehighfrequencyoftheterm“teleoperation”indicates
thatthesearticlespredominantlyfocusonteleoperation,whilethekeyword“human-robot”often
appearsinthecontextofcollaborationbetweenhumansandrobots.Thesetwokeywordsco-existin
severalarticles,suggestingthattheyexploretheintersectionofthesetwothemes,i.e.,teleoperated
controlinhuman-robotcollaboration.Somearticlesrefertothiscooperationas“collaboration”,
whileothersusetheterm“interaction”.Figure4presentsthetop100keywordsinawordcloud,
providing a more precise visualization of the frequency distribution of these terms within the
includedarticles.
Fig.4. Frequently-usedkeywordsintheincludedarticles
3.2 Technologies
Figure5depictstheemploymentofassortedXRtechnologiesanddiverseinteractionmodalitiesin
theanalyzedstudies.ItisevidentthatVirtualRealityHead-MountedDisplay(VRHMD)dominates
8XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
thelandscape,constituting67%ofadoption,andsignificantlysurpassingotherapparatuses.Aug-
mentedRealityHead-MountedDisplays(ARHMDs)followwith19%prevalence,whilealimited
numberofstudiesutilizeMR(5%),CAVE(2%),andMobileAR(1%)technologies.
AmajorityofstudiesfavorVRtechnology,whichcanlikelybeattributedtothenecessityfor
usersoperatingremotelytoreceiveinformationabouttheremoteroboticsystemorenvironment.
VRtechnologydeliversthisdatainamoreimmersive,intuitivemanner,fosteringaheightened
sense of presence – a key advantage of VR technology. In contrast, AR technology excels at
superimposingvirtualinformationontorealenvironments.Nonetheless,inthecontextofremote
operation,ARfaceschallengesinsatisfyingthedemandforinformationoverlayontherealrobot
anditsworkspace(wheretheuserisabsent),possiblyaccountingforthehigherprevalenceofVR
technologyinresearch.AnothercontributingfactorcouldbethelowercostofcommercialVR
HMDscomparedtoARHMDs[145].Furthermore,VRHMDscanintegratesupplementarydepth
camerastoachievefunctionalitysimilartoARHMDs,asexemplifiedbythestudyconductedby
Yewetal.[150].TheirresearchprototypeusedtheattachedcameratotracktheposeoftheOculus
RiftHMDandtheroboticarmtogenerateanddisplaytheARenvironmentintheHMD.Thisfactor
mayexplaintheincreasedusageofVRHMDsinresearch.Additionally,ahandfulofstudiesemploy
acombinationofdevices,suchasARandVR[6],oftenwithinthecontextofmulti-personremote
collaboration.Inthesescenarios,alocaloperatorutilizesanARHMDtomanipulatetherobot,
whilearemoteexpertwearsaVRHMDtoobtainimmersivethree-dimensionalguidanceofthe
localrobotandworkenvironment.Subsequently,thisguidanceinformationistransmittedtothe
localworker’sARHMD.
Fig.5. UseofdifferentXRdevicesandinteractiontypesintheincludedarticles.(a)Statisticsonthetypeof
XRtechnologyusedinthestudies;(b)Statisticsontheinteractionmodalityusedinthestudies.
Themostprevalentinteractiontypeinvolvesusingbuilt-incontrollersprovidedwiththedevices
(27%). This trend is reasonable, as the most commonly utilized XR devices are VR HMDs and
commercialVRHMDsgenerallyincludetheirproprietarycontrollers,regardlessofformfactors.
Gesturalinteractionisanotherfrequentmethod(20%),primarilybecausecommercialARHMDs
predominantlyusehandorheadgesturesforinteractionwithvirtualcontent.Additionally,many
studiesemployVRHMDswithsupplementarydepthcameras,suchasLeapMotion3,mounted
ontheheadsettodetectusergestures,asremoterobotoperationthroughgesturesisoftenmore
intuitivethanusingcontrollers.Thejoystickinteraction,typicallyassociatedwithgamepadslike
3https://www.ultraleap.com/product/leap-motion-controller/
9Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
Xbox4,featuresprominentlyintheresearch(9%)andisoftenconsideredwhenthesubjectrobots
aredronesormobilerobots[3,11,51,109,127,138,153],onlythestudybyVuetal.[124]usedthe
joysticktomanipulatetheroboticarm.Theuseofvirtualfixturehapticdevicesisalsorelatively
high(7%).Hapticdevicescanoverlayenhancedsensoryinformationonusers’perceptionofthe
realenvironmenttoimprovehumanperformanceinbothdirectandteleoperatedtasks[37].Motion
captureinteractionsoftennecessitateuserswearingsensors(6%)andmappingroboticarmsto
armorshouldercoordinates,renderingoperationsmoreintuitiveandreducinglearningcosts.The
remaining interaction types – such as actual walking in remote environments (4%), 2D screen
(3%),voice(2%),glove(3%),headmovement(1%),andgaze(1%)–constituteaminorpercentage
overall.Theselesscommoninteractionoptionsarefrequentlylinkedwithspecificrobotoperation
tasks.Forexample,Monirietal.studieduservisualattentioninHRI,andgazewaschosenasthe
interactionmethodsincethehumaneyegazeisanimportantindicatorofthedirectionofvisual
attentionfocus[79].
Fig.6. PercentagebarstackingchartfordifferentXRtechnologiescorrespondingtotheinteractiontypes.
BuildingonthepreviouslydiscussedXRtechniquesandinteractionmodalities,Figure6provides
acomprehensiveoverviewoftheseelementsintheexaminedresearch.Interactionreferstohow
usersmanipulatevirtualenvironmentsorobjectsthroughtheiractions.Ourfindingsindicatethat
thechoiceofXRtechnologypartiallydeterminestheinteractionparadigm.VRemergesasthemost
popularoptionforremotelycontrollingrobots,withmoststudiesoptingforcontrollersorgestures
asinteractionmethods.VRdevicesappearcompatiblewithadiverserangeofcontroltechniques,
except for 2D screens (i.e., touchscreen devices such as tablets or smartphones), typically not
employedbyVRHMDs.Thisexclusionislogical,giventhatVRdevicesobstructtheuser’slineof
sighttotherealworld,makingitimpossibleforuserstoviewcontentona2Dscreenwhilewearing
aVRHMD[18].Incontrast,mostsystemsutilizingARHMDsrelyongesturesforinteraction[41].
Thispreferencemaystemfromthenatureofconsumer-gradeARHMDs,ingeneral,notincluding
proprietarycontrollers,makinggesturesaconvenient,self-containedinteractionsolution.
3.3 RobotTypesandTasks
Ouranalysisoffersasummaryofthevariousrobottypes(seeFig.8withexamples)featuredinthe
includedstudies,aswellasthespecifictasksdiscussedortestedinthepapers.Thisinformationis
illustratedinFigure7.AsshowninFigure7(a),theroboticarmisthetypeofrobotmostextensively
researched,accountingfor41%ofthestudies.Mobilerobotsanddronesfollow,withrespective
sharesof20%and10%.Wecategorizeaspecialtypeofrobot–roboticarm+mobilerobot.This
typeofrobothasthecharacteristicsofbothroboticarmsandmobilerobots,anditsmainstructure
isamovablebaseonwhichthereisaroboticarm.Itcanbeinterpretedasamovableroboticarm.
Sucharobottypewas3%oftheincludedarticles.Othertypesofrobots,suchashumanoidrobots
(with human facial features, 8%), two-armed robots (7%), and medical robots (3%), constitute a
smallerportionoftheoverallrobottypes.Regardingthespecifictasksperformedbytherobots,
4https://www.xbox.com/en-SG/accessories/controllers/xbox-wireless-controller
10XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Fig.7. Differentrobotsanddifferenttypesoftasksincludedinthestudy.(a)Statisticsofthevariousrobot
typesusedinthereviewedstudies;(b)Statisticsontasksperformedbyrobots.
Fig.8. Examplesofdifferentrobottypes:a)Roboticarm[136];b)Mobilerobot[106];c)Drones/UAV[123];d)
Humaniodrobot[21];e)Double-armedrobot[157];f)Medicalrobot[159];f)Roboticarm+mobilerobot[52].
Figure7(b)revealsthatthethreemostdominanttasktypesareobjectgrasping/picking/placement,
robotnavigation,andspecializedoperationsinindustry/manufacturing,representing26%,18%,
and18%ofthetasktypes,respectively.Anotablefractionoftasktypesismultiple(8%),while
somestudiesdonotspecifytheexacttasksthattherobotsintheresearchcanexecute(7%).The
remainingtypesoftasks,suchasremoteenvironmentalscanning(3%),surgery/healthcare(3%),
search(2%),andgaming/entertainment(3%),compriseaminimalpercentage.
Fig.9. Percentagestackedbarchartsfordifferentrobotsandcorrespondingtasktypes.
Wealsoobservedpotentialcorrelationsbetweenvariousrobottypesandtasktypes,asdepicted
inFigure9.Ouranalysisindicatesthatroboticarms,two-armedrobots,andotherspecificrobot
types,suchasindustrialmachines[80,81],maintenancerobots[150],andminingrobots[142],are
predominantlyemployedinindustrialorproductiontasks.Theserobotstypicallylackmobility
11Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
capabilities(i.e.,theydonotpossessachassisthatenablesmovement),sotheirtasktypesdonot
involvenavigation.Conversely,mobilerobots,drones,andasubsetofhumanoidandothermore
mobilerobotsareresponsiblefortaskssuchasnavigation,search,andenvironmentalscanningthat
relyonmobility.Roboticarmsandtwo-armedrobotsprimarilyperformthegrasping,picking,and
placementtasks,asthesefunctionsconnecttotheservicesorfundamentalfunctionsofroboticarms.
Thetasksarealsoakeycomponentoftheindustrialorproductionchainforwhichroboticarmsare
originallyresponsible.Tosomeextent,two-armedrobotscanbeconsideredacombinationoftwo
roboticarms.Medicalrobotsrepresentthemosthomogeneousrobottype,astheirsoleresponsibility
istoassistinsurgicalprocedures[119,159].Furthermore,wediscoveredthathumanoidrobots
appeartopossessuniquesocialcharacteristics.Thetaskstheyareassigned,suchasintervening
withchildrendiagnosedwithautismspectrumdisorders(ASD)[65],engaginginchessgameswith
remoteusers[104],assistingremoteuserswithdressing[104],expressingemotions[114],receiving
andguidingusers[48],andmaintainingroadtrafficsecurity[46],areinherentlylinkedtohuman
orsocialactivities.Thissuggeststhathumanoidrobots,duetotheiranthropomorphicformand
capabilities,areparticularlysuitedforrolesthatrequiresocialinteractionorhuman-liketasks.
3.4 EvaluationofTasks
Fig.10. Statisticsofdifferentevaluationmethods.
We examined the evaluation methods used in the included studies. We discovered that 38%
ofthepapersdidnotconductanyformofevaluation,while36%employedasingleevaluation
method,and36%utilizedahybridapproach.Amongthesemethods,69%ofthepapersemployed
aquantitativeevaluation,whichincludedassessingtime/accuracytocompletethetask(48.91%),
administeringstandardizedquestionnairessuchastheNASA-TLX[50],oremployingtask-based
designquestionnaires(26.09%).Amere5.43%ofthepapersfocusedonqualitativeuserevaluations,
suchasinterviews.Additionally,asmallsubsetofpapers,amountingto8.70%,concentratedon
comparingtheperformanceofrobotsandtheirdigitaltwins.Thistypeofstudyistypicallyevaluated
bycomparingthetrajectorycoordinatesofthesystem’sinputandtherobot’soutput.Forinstance,
studiesbyYunetal.[151],Cousinsetal.[28],andBianetal.[12]evaluatedthecoordinatesofthe
user’shandinputandtherobotarm’soutput.Betancourtetal.’sstudycomparedthe3Dspatial
coordinates of a virtual drone and a real flying vehicle [11]. A few studies, constituting 3.26%,
evaluatedtheperformanceorimpactofAR/VRitself.Forexample,Kuoetal.comparedtheaccuracy
ofmanipulatingobjectsthroughVR,video,andtherealworld[66].Similarly,Chenetal.evaluated
differentmethodsof3DreconstructioninVR[22].
3.5 XRTechnologiesFacilitateEffectiveRemoteRobotCollaboration
3.5.1 VirtualInterfaceDesign. WeadoptthetaxonomyproposedbyWalkeretal.[126]toanalyze
theuserinterfacedesigninXR,dividingitintotwocomponents:userperspectiveanduserinterface
(seeFigure11).Theuserperspectivecomprisesfivecategories:robot-coupled,robot-decoupled,
12XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Bird’s-eyeview,dynamicperspective,andothers.Robot-coupledperspectivesinvolveusersviewing
thescenethroughthe“eyes”oftherobot.ThisapproachwasexemplifiedintheworksofVempatiet
al.[123],Chackoetal.[19],andBrizzietal.[16],whoconductedtheirstudiesintheremoteoperating
systems of UAVs, humanoid robots, and double-armed robots, respectively, with perspectives
coupledtotherobots.Inthisapproach,theuser’sviewpointislinkedtotherobotandchanges
astherobotmoves.Conversely,robot-decoupledperspectivesenableuserstoobservetherobot’s
actionsfromadetachedviewpoint,unboundfromtherobot’smovements.Thisperspectivewas
demonstratedintheworkofKuoetal.[66]andZinchenkoetal.[159],whodevelopedsystems
thatmanipulatearemoteroboticarminVRwithaperspectivedecoupledfromtherobot.Similarly,
Stedmanetal.[106]employedadecoupledperspectiveintheirworkwitharemotemobilerobot.
The Bird’s-eye view offers an overhead, top-down view, as illustrated by Jang et al. [59], who
utilized this perspective to control swarm robots. while the dynamic perspective allows users
toswitchbetweendifferentviewpoints,thisperspectivewasexemplifiedintheworksofWeiet
al.[136]andXuetal.[143],whobothemployedacombinationoftwoperspectivesintheirstudies.
Wecategorizetheuserinterfaceintoseveraltypes(seeexamplesinFig.14).Oneoftheseis
thedirectinterface,wherethecameraontheremoterobotsidetransmitstoa360-degreevideo
interfaceinthevirtualenvironment.Thistypeofinterface,asexemplifiedbytheworkofZhao
etal.[156],allowsuserstodirectlyobservetheremoteworkspace.Anothervariantofthedirect
interfaceisaugmentedbya3Dreconstructionoftheremoteenvironment,asdemonstratedinthe
workofChenetal.[21].Thisapproachenhancestheuser’sperceptionoftheremoteworkspaceby
providingamoreimmersiveandspatiallyaccuraterepresentation.Adifferentapproachinvolves
theuseofadigitaltwinoftherobot.Inthissetup,adigitalreplicaoftheremoterobotexistsinthe
virtualenvironment,andtheusercontrolstheremoterobotbymanipulatingthedigitaltwin,as
illustratedintheworkofZinchenkoetal.[159].Otherinterfaceoptionsincludethedigitaltwin
combinedwitha3DreconstructionoftheremoteenvironmentasanexamplebyKuoetal.[66],
virtualcontrolroom–rebuildsavirtualconsoleinthevirtualenvironment,asdemonstratedby
Kalinovetal.[61],3DreconstructionoftheremoteenvironmentasshownintheworkofZeinet
al.[153],oracombinationoftheaforementionedinterfaces.Thesevariousinterfacedesignsoffer
differentlevelsofimmersion,controlprecision,andspatialawareness,cateringtodifferentuser
needsandtaskrequirements.
Fig.11. (a)Categorizationandstatisticsofuserperspectives;(b)Statisticsonthenumberofdifferenttypes
ofuserinterfacesinthereview.
Wediscoveredthattheuser’sperspective(seeFig.12withexamples)predominantlyinvolves
robot-coupled(33%)orrobot-decoupled(54%)views,whileotherperspectives,suchasdynamic
13Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
Fig.12. Examplesofdifferentuserperspectives:a)Decouplingwithrobot[86];b)Couplingwithrobot[117];
c)Dynamicperspective[157];d)Bird’s-eyeview[125].
(6%), bird’s-eye view (2%), and others (5%), constitute a relatively small proportion. Regarding
userinterfaces,thedigitaltwin(32%)anddirectinterfaces(24%)aremostprevalent,followedbya
considerableshareofdigitaltwincombinedwith3Dreconstructionoftheremoteenvironment
(16%).Theremaininginterfaces,includingcombinationsofmultipleinterfaces(7%),overlaying
virtual interfaces on real environments (i.e., context-aware AR interface, 6%), direct interfaces
augmentedwith3Dreconstructionsoftheremoteenvironment(5%),standalone3Dreconstruction
oftheremoteenvironment(3%),virtualcontrolrooms(3%),andotherinterfaces(4%),representa
relativelyminorportion.
Fig.13. Percentagestackedbarchartoftherelationshipbetweentheuserperspectiveandtheuserinterface.
Consistentwithourpreviousanalysis,weinvestigatedtherelationshipbetweenuserperspective
and user interface using a percentage stacked bar chart, as illustrated in Figure 13. The most
significant observation is that the direct user view within the user interface tends to be robot-
coupled.Thisisprimarilybecauseusersneedtoobservetheremoteenvironment’sworkspace
directlyfromtherobot’sviewpoint.Moreover,wefoundthatwhentheuserperspectiveisdecoupled
fromtherobot,itisfrequentlynecessarytoincorporateadigitaltwinoftherobotwithintheXR
environment.Thisisunderstandablesinceusersneedtobeawareoftheremoterobot’smotionstate,
necessitatingthecreationofacorrespondingdigitaltwininXRtofacilitatebettercomprehension
of the robot’s operational state. Notably, most studies have opted for the digital twin solution,
whileonlyafew,suchasXuetal.[146],haveemployedacamerapositionednexttotheremote
robottoconveytherobot’sworkstatusvialivevideo.Thevirtualcontrolroomuserinterfacealso
requirestheuser’sperspectivetoberobot-coupled.AlthoughonlytwostudiesemployedBird’s-eye
perspectives,weobservedthatbothexecute3Dreconstructionsoftheremoteenvironment.Lastly,
wealsodiscoveredthatuserinterfacedesignsfordynamicperspectivestendtobemoreintricate,
suchasZhouetal.[157],oftenincorporatingmultipleuserinterfaces.
14XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Fig.14. Examplesofdifferentvirtualuserinterfaces:a)Digitaltwin[90];b)Directinterface[100];c)Digital
twin+3Dreconstruction[106];d)Multipleinterface[157];e)Direct+3Dreconstructioninterface[21];f)3D
reconstructioninterface[125];g)Virtualcontrolroom[61];h)Context-awareARinterface[57].
Fig.15. Relationshipsbetweentypesandlocationsofmultimodalenhancements.
3.5.2 Enhancement. Inouranalysisoftheincludedstudies,weevaluatedthelocationandtypeof
informationenhancedbythemultimodalityofthesystems.Wefoundthat39%ofthestudiesemploy
onlyasinglemodalapproachtoimproveremoteoperations.Mostofthesesystemsonlysupported
userstoviewremoterobots,workspaces,orenvironmentsimmersivelyviaXR.Suchsystemsdo
nothavemultimodalenhancementsinanylocation,andtheimprovementtotheuserissimply
theimmersionofXR.Ontheotherhand,62%ofthestudiesoptedformultimodalenhancement
duringspecificpartsofremoterobotoperation.Theprimaryareasofenhancementwerethevirtual
environment(43.43%)andtheuser(21.21%).Asmallernumberofstudieschosetoenhancevirtual
objectsmanipulatedwithinthevirtualworkspace(10.10%),virtual(8.08%)andrealrobots(6.06%)
orinrealenvironments(11.11%).Thiscouldbeattributedtothefactthatfewerremoteoperating
systemsareusingARtechnologycomparedtoVRtechnology.ARtechnologyisthepredominant
technologyapplicabletoaugmentationinrealrobotsandrealenvironments.Augmentationon
topofrealrobotsoftennecessitatescollaborationamongmultipleindividuals.Forinstance,the
workofMourtzisetal.[81]andSchwarzetal.[104]involvedacooperativesystemwithmultiple
users,withoneuserataremotelocationandanotheruserwiththerobot.However,thistypeof
systemrepresentsaverysmallpercentageofourcollectedpapers(formoredetails,seeSection3.6).
Additionally,weobservedthatthehighlightenhancementmodewasonlyappliedtothevirtual
representationsoftherobotandtheobjectsbeingmanipulated.Thisenhancementtypeaccentuates
specificpartsofthedigitaltwinorthevirtualobjectforclearerinteraction.Avatarenhancement
alsoappearedexclusivelyinthevirtualenvironment.Ontheuserside,enhancementprimarily
involvedhapticfeedback,utilizingtoolssuchashapticglovesorvirtualfixtures.Theresearch
15Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
conductedbyDuetal.[34],Aschenbrenneretal.[6],andHormazaetal.[53]alsoexploredtheuse
ofvoiceenhancementontheuserside.Intheenvironment,whetherrealorvirtual,afewstudies
employedlivevideoenhancement,openingalivevideowindowwithintheenvironment,suchas
Zinchenkoetal.’swork[159].
3.6 Multi-playerandMulti-RobotInteractionSupport
Fig.16. Examplesofsystemsthatsupportremotemultiplayerormulti-robotinteraction:a)Multi-userremote
operateonerobot[43];b)Multi-userremoteoperatemulti-robot,1:Multi-usercontrolofmultipledrones[93],
2:Twouserscontroltwodrones[56];c)Oneuserremoteoperatemulti-robot[58];d)Alocaluserandaremote
usercooperatetogethertocontrolarobot,1:Localandremoteuserscollaboratewithdesktoprobots[57],
2:CoachandlearnerremotelyexercisetabletennisinVRviaroboticarm[40],3:Expertsremotelyguide
novicesinusingmedicalequipment[14],4:Thelocalusercontrolstherobottoplaychesswiththeremote
user[104],5:Theconductorremotelydirectstheusertomaneuvertheexcavator[73].
Inouranalysis,81%oftheincludedpapersdidnotsupportmulti-playerormulti-robotinteraction,
andalloftheircollaborationinvolvedoneuseroroperatorcollaboratingwithasinglerobot.Only
19%oftheinteractionparadigmsdescribedinthepaperssupportedmulti-playerormulti-robot
interaction.Oneofthemostcommoninteractionparadigmsinvolvedoneuserinteractingremotely
withasinglerobot,whileanotheruserwassituatednexttotheremoterobot(N=12).Forinstance,
papersbyMourtzisetal.[80,81],Blacketal.[14],Fuchinoetal.[40]andMonirietal.[79]propose
thatanoviceoperatororworkerontherobotsidereceivesinstructionfromaremoteexpertor
commanderusingXRforenvironmentalawarenessandcommunication.Thecollaborationmodel
inbothJangetal.[58]andGongetal.’s[46]studiesinvolvedoneusercollaboratingwithmultiple
robots(N=2).Theremainingmodesincludemultipleusersinteractingwithmultiplerobots,such
astheworkofPhanetal.[93]andHoningetal.[56](N=2),multipleusersoperatingasingle
robot,asexemplifiedbyGalambosetal.[43](N=1),andoneusercollaboratingwithmultiple
robotswhilemultipleusersarepresentontherobotside,asdemonstratedbyAschenbrenneret
al.[6]andWalkeretal.[125](N=2).
16XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
3.7 ExplorationofSystemLatencyIssues
Fig.17. Densitydistributionofsystemlatencytimes(unit:ms)intheincludedstudies.(a)Latencytimes
werereportedbyallincludedstudies;(b)Latencywasscaledto0–500mstoshowmoredetail.
Inourdataextraction,wediscoveredthatsystemlatencyeffectsarenotconsideredinnearly
halfofthestudies(49%),despitethefactthatremotecontrollatencydurationisacriticalfactor
influencinguserexperienceandtaskaccuracy.Asignificantnumberofourincludedstudiessimply
acknowledgedthepresenceofsystemlatencyorclaimedthattheirsystemsexperienceddelays,
withoutprovidingspecificmeasurementsorquantifyingthedurationoftheirsystemlatency(23%).
Only28%ofthestudiesexplicitlyanalyzedthelatencyofthesystem,andwecompiledthereported
studytimesinFigure17.Figure17(a)displaysthelatencytimesreportedbyallstudies,revealing
thatthemajorityofstudieshadlatencytimeswithintherangeoflessthan500𝑚𝑠,andonlyafew
studieshadlatencytimesintherangeofgreaterthan1000𝑚𝑠.Focusingonthelatencytimesless
than500𝑚𝑠,asseeninFigure17(b),wefoundthatmoststudieshaddelaytimeswithintherange
of200𝑚𝑠 to400𝑚𝑠.Amongtheincludedstudies,severalstandoutfortheiruniqueapproaches.Le
etal.’s[68]researchfocusedoncontrollingsystemlatencyandcomparingtheimpactofdifferent
latenciesonuserexperience.Incontrast,McHenryetal.’s[77]studyemployedanasynchronous
systemoperationtoovercomethehighlatencyassociatedwithEarth-Moontransmission.These
studieshighlightdistinctstrategiesforaddressinglatencyinextendedrealitysystemsforremote
roboticcontrol.
4 DISCUSSION
Based on our data extraction results, Figure 18 provides a visual summary of the number of
papersassociatedwitheachdimensionexploredinthissurveyarticle.Itisimportanttonotethat
thepriorsectiondepictsthefactualaspectsofthecollectedarticles.Next,wehavetointerpret
thesefactsandhighlighttheinsights.Thus,thissectiondelvesintotheprevalentstrategiesand
discrepanciesobservedacrosstheseselecteddimensionalfeatures.Ourdiscussionaimstoshed
light on the critical aspects of human-robot interaction and XR technologies, highlighting the
areasofconvergenceanddivergenceamongthestudiesreviewed.Byexaminingthesepatterns,we
seektoidentifyopportunitiesforfutureresearchanddevelopmentinthisrapidlyevolvingfield,
ultimatelycontributingtomoreeffectiveandseamlesscollaborationamonghumansandrobots.
4.1 ImpactofdifferentrobottypesonremoteHRI
4.1.1 Howdifferentrobottypesinfluenceuserinterfaceanduserperspective. Thewidevarietyof
robots,eachwithitsdistinctcharacteristicsandcapabilities,inherentlyaffectsthedesignchoices
17Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
Fig.18. SankeyDiagram,avisualizationwithoverallcountsofcharacteristicsacrossalldimensions.Different
stages(nodes)colorsareusedtodistinguishdifferentdimensions; a :XRtechnologiesandinteraction
modalities; a : Robot and specifictasks classification; a : User’s Perspective and virtual interface; a :
Enhancementlocationsandtypes; a :Supportformulti-user/multi-robot; a :Evaluationoftasks.Where
thewidthoftheflow(links)isproportionaltothetotalnumberofarticleswereviewed(N=100).
18
tnemecnahne
oN
toN
denoitnem
niwt
latigiD
tcejbo D3
tnemecalP/gnikciP/gnibbarG
rellortnoC
resu enO
mra citoboR
txeT
EV
secived
citpaH
tceriD
tobor
htiw
gnipuoceD
gnitcafunaM/lairtsudnI
ee vz oa lGG
yca/
re um ci cT a
ratavA
erutpac
noitoM
sksat
elpitluM
tobor demra-elbuoD
DMH RV
cihparG
)lautriv(tcej
nb oO itcurtsnocer
D3+
niwt
latigiD
oN
erutseG
tobor eiboM
eranoitseuQ
citpaH
resU
RA erawa-txetnoC
noitagavaN
noitcaretni
rehtO
itlum+
1 it -l -u im tlu--
m1
tobor
htiw
gnipuoC
tobor
doinamuH
RV/RA
itlum--itlum
thgilhgiH
)laer(toboR
ecafretni
elpitluM
erac
htlaeH/yregruS
mra
tobor
+ tobor eiboM
noitcaretni
elpitluM
weivretnI
1+1--1
oediV
)lautriv(toboR
mo
no or i l to curt
rn tso
nc o l ca eu rt r DiV
3
ksat
rehtO
tob too br ola rc ri ed he tM O
ek dcl aia o eW V H
DMH RA
dohtem
rehtO
evitcepsrep
cimanyD
noitcurtsnocer
D3
+ tceriD
nacs
tnemnorivnE
kcitsyoJ
nosirapmoC
itlum--1
tnemecnahne
ecy ioa VR
ER
ecafretni
rehtO
weivV
eO yeP
- sr 'e dh rt iO BlaicoS/tnemniatretnE
h/e cm raa eG S
VAU/senorD
neercs D2
seciveh
Dc e eT lp r ite lh ut MO RM EVAC RA eiboMXR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Fig.19. Bubblediagramoftherelationshipbetweentypesofrobotsanduserinterfaceandperspective.
Differentcoloredbubblesrepresentdifferentrobottypes,theverticalaxisindicatesdifferentuserperspectives,
andthehorizontalaxesAtoIindicatedifferentuserinterfaces.Thesizeofthebubblesisproportionaltothe
numberofarticles;thebiggerthebubble,themorearticlesinthecorrespondingcategory,orviceversa.e.g.,
amongthe100papersreviewed,onestudyutilizedaroboticarm,adoptingaperspectiveofcouplingwiththe
robotthroughavirtualcontrolroominterface,while22articlesalsofocusingonroboticarms,employeda
decouplingperspectiveusingadigitaltwinuserinterface.
foruserinterfacesandperspectivesinremotehuman-robotinteraction(HRI)systems.Ouranalysis
hasidentifiedspecificassociationsbetweenvarioustypesofrobotsandtheircorrespondingtasks
(Section3.3),whicharecriticaltoshapingtheuserinterfaceandperspectivedesign(SeeFigure19).
AsanalyzedinSection3.3,industrialrobots,suchasroboticarmsanddouble-armedrobots,are
primarilydeployedforproductiontasks.Giventheirlimitedmobility[5],theserobotsnecessitatea
userinterfaceandperspectivethatfocusesprimarilyonprecisioncontrolandmanipulation[30],
ratherthannavigation.Forinstance,userinterfacesdesignedfordouble-armedrobotsandrobotic
arms often facilitate multi-viewpoint together with dynamic perspective observation to assist
users [144, 157]. A key distinguishing factor between these two robot types is their preferred
controlmethodintheiruserinterfacedesigns:two-armedrobotstypicallyfavourdirectoperation,
whereasroboticarmsgenerallyreflectadigitaltwin[88,110,122](i.e.,thedatamappingbetween
thevirtualityandphysicalworlds).Thisdistinctioncouldstemfromthefactthatthedouble-armed
configurationoftherobotsalignscloselywiththehumandual-armanatomy[45].Thisalignment
facilitatesdirectcontroloftheremoterobotandcouplestheuser’sperspectivewiththerobot,
potentiallyreducingtheuser’slearningcurvewhilemakingtheoperationmoreintuitive.
Onthecontrary,mobilerobots,drones,andcertainhumanoidrobotswithadvancedmobilityare
designedfortasksthatrequirenavigation,search,andenvironmentalscanning.Thesetasksrequire
userinterfacesandperspectivesthatenhancespatialawarenessandpromotedynamicmovement
withintheremoteenvironment[149].Forinstance,mobilerobotinterfacesuniquelysupportabird’s-
eyeview,assistingusersincomprehendingtheremotethree-dimensionalenvironment[6,125].
Inadditiontotheaboveinterfacedesignsrelatedtothetypeoftasktherobotisassigned,itcan
benoticedthroughthebubblediagramthatoverall,thereareincreasingnumbersofdesignsusing
directinterfacesinallrobottypesexceptmedicalrobots.Perhapsonereasonisthatthecostofsuch
directinterfacesisminimalanddoesnotrequire3Dreconstructionorthecreationofadigitaltwin
oftherealrobot[2,54].Moreover,wefindthatsuchdirectinterfacesareoftencoupledwiththe
robotperspective,exceptforbeingmoreintuitive,probablyalsoforcostreductionconsiderations.
Thisdesignoftenrequiresonlyoneortwocamerasmountedontherobot[7,42,100,154].
19Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
Finally,humanoidrobots,knownfortheirdistinctsocialattributes,aretypicallyengagedin
tasksrelatedtohumanorsocialactivities[39].Insuchscenarios,userinterfaceandperspective
designsshouldbegearedtowardsintuitivecontrolandinteraction,allowinguserstointeractwith
therobotmorehuman-likeandsocially.
4.1.2 Typesofrobotsaffectvirtualenhancements. Inourreview,medicalrobotsextensivelyutilize
virtualenhancementelements,withallexaminedmedicalrobotsincorporatingsuchenhancements
intheirinteractions[14,119,159].Weacknowledgethatthisobservationmaybeinfluencedbythe
limitednumberofmedicalrobotstudies(only3)includedinourreview,whichmaynotprovidea
sufficientrepresentationofthefield.Alternatively,thishighutilizationofvirtualenhancements
couldbeattributedtotheuniquerequirementsofmedicaltaskscenarios,whichrequirefulluseof
theXRcapabilitiestoassistoperators[116],suchasphysicians.
Roboticarms,anindustrialrobottypecharacterizedbyitsrisingpopularity,alsodemonstrateda
highuseofvirtualaugmentationelements,with61%oftheexaminedroboticarmsincorporating
oneormoresuchelements.ThisprevalencemaybeduetotheextensiveresearchfocusonXR-
basedremotecontrolofroboticarms,leadingtoagreaterexplorationofXR’suniqueaugmentation
characteristics.Inparticular,roboticarmsweretheonlyrobottypethataugmentedvirtualobjects,
whichcouldbeassociatedwiththeircommontaskofpickingandplacingobjects[140].Thistask
maynecessitateadditionalaugmentationonobjectstoenhanceuser-manipulationcapabilities.
Furthermore,roboticarmsandmobilerobotsweretheonlytypesthatutilizedtextoverlaysinthe
virtualenvironment.Thisfeaturemayalsoberelatedtothetasksperformedbyroboticarmsand
mobilerobots.Roboticarmsareoftenusedinprofessionalcontextsandindustrialenvironments,
whereremoteoperatorsmaybenefitfromtextpromptsorremindersforthenextoperationaltask,
asexemplifiedinthedesignbyWangetal.[132–134].Theremoteoperatorofthemobilerobotcan
alsogetinformationabouttheorientationoftherobotmovementfromthetextprompts[62,113].
Theuniquemobilitypropertiesofmobilerobotsalsoinfluencedthechoiceofvirtualenhance-
ments.Mobilerobotsmakeextensiveuseofenhanceddesignintheirenvironments(64.29%),both
virtualandreal.Manydesignsused3Dobjectenhancementsintheenvironment[29,58,125].This
featuremaybenecessarytoprovidespatiallocationcuesforusersnavigatingremotemobilerobots
usingXR,arequirementthatdoesnotapplytoothernon-mobilerobots[8].Trinitatovaetal.’s
designusesaroboticarmbutstilluses3Dobjectenhancementintheenvironment[120].Their
purposewastouse3Dspherestoindicatethecenterofthemanipulatedpartoftheroboticarm,
stilltoindicatepositionalinformationinspace.Thissuggeststhattheenhancementof3Dobjects
intheenvironmentisoftenassociatedwithpositionalinformation.
4.2 DesigningremoteHRIsystemwithusersandscenarios
TheinfluenceofusersandscenariosonremoteHRIsystemdesignisacrucialaspecttoconsider
whendevelopingeffectivehuman-robotcollaborationexperiences.Dependingontheusers’ex-
pertise,background,andpreferences,aswellasspecifictaskscenarios,thedesignofremoteHRI
systemsmayneedtobeadaptedaccordinglytoensureoptimalperformanceandusability.For
example,adjusttheuserperspectiveandvirtualinterfaceaccordingly,orselectdifferentenhanced
designelementsappropriately(seeSection3.5).
4.2.1 Expertiselevelofusersanditsimpactoninteractiondesign. Expertusers,suchasengineersor
technicians,mayhavemoreadvancedskillsandfamiliaritywithroboticsystems,allowingthemto
handlemorecomplexinterfacesandcontrolmechanisms.Ontheotherhand,noviceusers,suchas
non-specialistworkersorfirst-timeusers,mayrequiremoreintuitiveanduser-friendlyinterfaces
thatprioritizeeaseofuseandlearningoveradvancedfunctionality[108,137].Inaddition,some
20XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
ofthesystemsdesignedforbeginnerswillsupportthefunctionofmultipleoperatorstosupport
remoteexpertguidance[14].
4.2.2 Designing adaptive systems to cater to diversified scenarios. Different task scenarios may
necessitatevaryinglevelsofdetailandcontrolintheinterfacedesign.Inscenarioswherehigh
precisionandaccuracyarerequired,suchasremotesurgeryordelicatemanipulationtasks,the
interfacemayneedtoemphasizefine-grainedcontrolandprovidemorecomprehensivefeedbackto
theuser[155].Conversely,inlessdemandingtasks,suchassimplenavigationorobjecttransport,
the interface could be designed with a more streamlined approach, focusing on usability and
efficiency[15].Inaddition,specificscenariosmayrequireuniqueinterfaceelementsorfeatures
thatcatertotheparticularchallengesorrequirementsofthetask.Forexample,insearchandrescue
missions,theinterfacemaybenefitfromincorporatingreal-timemappingandtrackingcapabilities
toaidinrobotnavigationandlocalizationwithintheenvironment[95].
4.3 TheroleofXRinfacilitatingremoteHRI
Byprovidingimmersiveandinteractiveexperiences,XRtechnologieshavethepotentialtobridge
thegapbetweenusersandremoteroboticsystems,enablingmoreefficientandnaturalcollaboration.
4.3.1 Enhancinguserperspectiveandunderstandingofremoteenvironments. Onekeyadvantage
of XR in remote HRI is creating highly realistic and accurate digital twins of both the remote
environmentandtherobotitself(seeSection3.5.1,digitaltwinsarethemostcommonlyuseduser
interface).Userscanbetterunderstandtheremoteworkspacebysimulatingtherobot’smovements
andactionswithinavirtualenvironment,makingplanningandexecutingtaskseasier.Furthermore,
the digital twin approach allows for more intuitive control mechanisms, as users can directly
interact with the virtual representation of the robot, which in turn is mapped to the physical
robot’sbehavior[102].AnotherimportantaspectofXRinremoteHRIistheprovisionofenhanced
userperspectives(Section3.5.1).XRtechnologiesenableawiderangeofviewingoptions,suchas
couplingtheuser’sviewwiththerobot,decouplingtheviewfromtherobot,orprovidingdynamic
andflexibleperspectives.Thesedifferentperspectivesallowuserstoadapttheirviewingexperience
accordingtotheirpreferencesandspecifictaskrequirements,improvingsituationalawarenessand
overalltaskperformance[25].
4.3.2 Supportingmulti-playerormulti-robotinteractionsthroughXR.. OneofthekeybenefitsofXR
insupportingmulti-playerormulti-robotinteractionsistheabilitytocreatesharedvirtualspaces
whereuserscancollaborateandcommunicatemoreeffectively[103].Bysimulatingmultipleusers
orrobotswithinavirtualenvironment,XRallowsforbettersituationalawarenessandcoordination,
whichiscrucialforcomplextasksthatrequireteamworkandcooperation.Thissharedvirtualspace
enablesuserstovisualizetheactionsandintentionsofotherusersorrobots,improvingoverall
task performance and efficiency. Moreover, XR technologies can be used to develop advanced
userinterfacesthatcatertotheuniquerequirementsofmulti-playerormulti-robotscenarios.For
instance,XRinterfacescanprovidereal-timestatusupdates,taskassignments,andperformance
metricsforeachuserorrobot,enablingbettermonitoringanddecision-making.Additionally,XR
interfacescanfacilitatemoreintuitivecontrolmechanisms,allowinguserstoseamlesslyswitch
betweencontrollingmultiplerobotsorcollaboratingwithotherusers.
Despite the potential benefits of XR in multi-player or multi-robot interactions, our survey
indicates that a significant portion of the included studies did not support such interactions
(Section3.6),withmostfocusingononeusercollaboratingwithonerobot.However,severalstudies
haveexploredvariousinteractionparadigms,suchascollaborationbetweenalocaluseranda
remote user[14, 40, 57, 73, 104], one user interacting with multiple robots [58], multiple users
21Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
operatingasinglerobot[43],ormultipleuserscollaboratingwithmultiplerobots[56,93].These
studies demonstrate the feasibility and potential advantages of leveraging XR technologies in
multi-playerormulti-robotHRIscenarios.
Inadditiontothosediscussedabove,animportantconsiderationinXR-enabledmulti-player
ormulti-robotinteractionsystemsisthechallengeoflatency,asemphasizedbyJayetal.[60],
latencycanseverelyimpacttheeffectivenessofcollaborationandtheuser’spresenceinthevirtual
environment,affectingtheuser’sexperience.Highlatencycandisruptcoordinationbetweenusers
androbots,leadingtoerrorsanddecreasedtaskperformance.Minimizinglatencytoenhanceuser
experienceandensuresmoothreal-timeinteractionsisimportant.Ourreviewwaspleasedtofind
thatmanystudieshavenotedtheproblemoflatency(Section3.7);however,addressinglatencyis
especiallyimportantforsystemsthatrequiretheparticipationofmultipleusersandrobots.As
thenumberofparticipatingusersandrobotsincreases,thelatencyproblemmaybecomemore
pronounced(SeeSection5.2.2forfurtherdetails).
4.3.3 TheuseofmultimodalenhancementinXRtoimproveremoteoperations. Multimodalenhance-
mentsinXR,suchasvisual,auditory,andhapticfeedback[69],cansignificantlyimprovetheuser
experienceandtaskperformanceinremoteoperations[64].Ourresults(Section3.5.2)providea
detailedsummaryofthelocationsandkindsofmultimodalenhancements.Theseenhancements
provide users with more intuitive and immersive ways of perceiving and interacting with the
remoteenvironmentandtherobotsinvolved.Byleveragingmultimodalfeedback,XRcanhelp
bridgethegapbetweentheuserandtheremoteworkspace,leadingtomoreefficientandaccurate
taskexecution[75].
Forexample,visualenhancements,suchashighlightingspecificpartsofarobotorvirtualobject,
candrawtheuser’sattentiontoimportantelementsandprovidecontext-awareinformation[13].
Auditoryfeedback,suchasspatialaudioorvoicecommands,candelivercrucialinformationto
theuserandfacilitatenaturalcommunicationwithotherusersortheroboticsystemitself[98].
Hapticfeedback,enabledthroughdevicessuchashapticglovesorvirtualfixtures,canprovide
userswithamoretangiblesenseoftouch,enhancingtheirperceptionoftheremoteenvironment
andimprovingtheirabilitytoperformcomplextasks[87].
Ourresultsindicatethatasignificantportionofthestudiesincorporatedmultimodalenhance-
mentsintheirXRsystems,withafocusonaugmentingthevirtualenvironment,user,andvirtual
objects.However,thereremainsroomforfurtherexplorationintermsofaugmentingrealrobots,
realenvironments,andotheraspectsofremoteoperations.
5 CHALLENGESANDFUTUREDIRECTIONS
Afterreviewingourcomprehensivesurvey,wefoundthatsignificantprogresshasbeenmadein
thedevelopmentofremoteHRIbasedonXRtechnologies,butmanybarriersanddevelopment
opportunitiesstillexist.Thefollowingsectionsdiscussourreflectionsindetail,providingideasfor
futureresearchersandsuggestionsforfuturedevelopers.Wewillorganizethemintofourmain
sections–challengesintheselectionofevaluationmethods,unleashingthepotentialofXRin
remoteHRI,user-centeredsystemdesign,andlongitudinalstudiesandreal-worlddeployment.We
hopetoworktogetherwithfutureresearcherstoprovideinnovativesolutionstothesechallenges.
5.1 Challengesintheselectionofevaluationmethods
SelectingeffectiveevaluationmethodsisakeychallengeforfutureresearchersexploringXR-based
remote human-computer interaction. Our review found that a small half (38%) of the relevant
studiesdidnotindicateanyevaluationmethodstheyused(seeSection3.4).Byanalyzingexisting
22XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
studies,evaluationmethodscanbebroadlyclassifiedintotwomaincategories:1)evaluatingthe
systemefficiencyand2)evaluatingtheuser’sexperience.
Toevaluatesystemefficiency,includingmetricssuchassystemlatencyandtaskcompletion
efficiency, the selection of evaluation methods should take into account the specific types of
problemsthatthesesystemsneedtosolve.Forexample,ifhelpisneededforindustrialproduction
and assembly tasks, it may be more important to test the latency of the system as well as the
accuracyandthetimetocompletespecifictasks[49].Incontrast,digitaltwinoperatorinterfaces
(XR as a teleoperation interface extension) may be more concerned with whether the digital
twin corresponds to the behavior and actions of the real robot, usually comparing trajectory
coordinates[70,74].XRasasolutionmaysignificantlyaffectthesystemlatencyandthuschange
theefficiencyofteleoperation[4],whichneedstobeconsideredbyfutureresearchers.
Incontrast,theevaluationoftheuserexperienceinXR-basedHRIinvolvesmoresubjectiveuser
factorssuchasusersatisfactionandeaseofuseofthesystem.Assessingthesefactorscangreatly
benefitfromqualitativemethods,includingstructuredinterviewsandtheuseofvariousscalesto
measureuserexperience.Futureresearchersshouldconsiderthespecificcontextofuse,thetarget
usergroup,andthepurposeoftheevaluationtoselecttheappropriatestandardizedscale.For
example,researcherscanchoosethescaleNASA-TLX [50]tomeasureusers’subjectiveworkloadof
thesystem,theSystemUsabilityScale[17]canusetomeasuresystemusability,andUserExperience
Questionnaire[67]canmeasuretheuserexperienceofinteractiveproducts.Inaddition,interviews
canprovidedeeperinsightintouserevaluationsorientedtouserperceptions,thatmaynotbe
capturedbyseparatequantitativemeasures[89].
5.2 UnleashingthePotentialofXRinRemoteHRI
IntherealmofremoteHRI,thesignificanceofXRisunmistakable.Itpresentsburgeoningopportu-
nitiestotransformtheremoteHRIparadigm,fosteringmoreimmersive,intuitive,andefficient
interactionsystems.TheseXR-enabledsystemsarepoisedtocatertotheintricatedemandsof
remoteinteractionswithadiversityofrobottypes.Nevertheless,aconsiderableportionofXR’s
potential remains unexplored within the current research context, thus delineating promising
pathwaysforforthcomingscholarlyexploration.
5.2.1 Multi-modalRemoteEnhancementinXR. XRoffersadistinctiveplatformformulti-modal
engagement,fosteringaricherandmoreintuitiveuserinteractionexperience.Althoughourreview
hashighlightedseveralinstanceswheremulti-modalenhancementshavebeenusedeffectively,
considerablescoperemainsforexploringinnovativeandintegrativeapproachestomulti-modal
remoteinteractionwithintheXRframework.Futureresearchshouldactivelypursuetheunder-
standingofthesynergyofmulti-modalcues,suchasvisual,auditory,andhapticcuesandtheir
rolesinenhancingusercomprehensionandcontrolinrobotteleoperation.
Inthevisualdomain,considerationscouldrevolvearoundtheoptimalselectionofvisualen-
hancementsfordistinctlocations.Forinstance,questionsmightariseabouttheefficiencyofa
3Dobjectoverlayinvirtualenvironmentsversushighlightedcueswithintheuser’sfieldofview
formoreintuitivenavigationofmobilerobots.Similarly,objectmanipulationcanbeguidedby
highlightingorbyusingpictorialmarkers,suchasarrows,forquickerresponsesfromremote
operators. Inaddition,differentrobotsand taskscanaffect thedesignof visualenhancements.
Futureresearchcouldfocusonexploringandevaluatinginterfacedesignsfordifferentcategories
ofrobotsandtasks.
Thepossibilityoftailoredremoteenhancementsfordifferentrobottypesandtasktypespresents
anintriguingopportunityforresearch.Itisplausiblethatauditorycuesmayneedtobedesigned
differentlyforvariousrobotsandtasks.Regardinghapticfeedback,theremayberequirementsfor
23Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
additionalhapticdevices,andpseudo-hapticscouldserveasanalternative[121].Introducingnew
hapticdevicesshouldbecarefullyweighedagainsttheriskofinadvertentlyincreasingthephysical
burdenonremoteoperators[135].
The reviewed literature did not sufficiently cover the comprehensive consideration of these
multimodaldesignspecifics,especiallytheintegrationofmultiplemodalities.Futureresearchshould
thoroughlyexaminethesemultichannelinteractions,providinginsightintooptimalstrategiesto
reducecognitiveload,enhancetaskperformance,andheightenuserimmersion.
5.2.2 Multi-playerandmulti-robotinteractionsandsystemlatency. XRisbecomingmorepopular
inindustrial,collaborative,andsocialdomains,andtheneedforsystemsthatsupportmultiple
usersandmultiplerobotsisprobablygoingtoincreaseinthefuture[158].Futureresearchersand
developersarerequiredtobeawareoftheimpactofsystemlatency,inadditiontodesigningefficient
andintuitiveXRsystemstocopewiththesepotentialdemands(seeSection4.3.2).Waltemateet
al.[128]indicatedthatalthoughlatencyisunavoidableinVRapplications,latencyabove75𝑚𝑠
affectstheperceptionofmotorperformanceandsimultaneity.Whereasalatencygreaterthan
125𝑚𝑠 decreasestheuser’ssenseofagencyandbodyownership,whichisworsenedbymorethan
300𝑚𝑠.Althoughoursurveyfoundthatthelatencyformostincludedstudieswasinareasonable
range–200𝑚𝑠to400𝑚𝑠,withinthatrange,usersmayperceivedelaysinthesystem,butnotenough
tocompletelycrashtheuserexperience.Withtheadditionofmorepeopleorrobots,thelatency
effectmaygetworse.Thisisanimportantissueforfutureresearcherstobeawareof.However,
Waltemateetal.alsopointedoutthatwhetherparticipantsnoticelatencyinvirtualenvironments
maydependonthemotortaskanditsperformanceratherthanthephysicallatency[128].This
suggeststhatauser-friendlyoperatingdesignmaylargelycompensateforthenegativeeffectsof
latency.Foroperationaltasksthatrequireprecisecontrol,itmaybenecessarytocontrolthedelay
towithin75𝑚𝑠.
5.2.3 NavigatingComplexEnvironmentswithXRinRemoteHRI. InthedomainofremoteHRI,
operating in complex environments presents significant challenges that could be substantially
mitigatedwiththejudiciousapplicationofXRtechnologies[71].Robotsmayneedtonavigate
throughareaswithvaryingterrain,unpredictableobstacles,ordynamicconditions,allofwhich
posedifferentsetsofcomplexitiesforremoteoperators.Thefidelitywithwhichtheseenvironments
canbereproducedinanXRinterfacecouldhaveasubstantialimpactontheeffectivenessofthe
HRI.
At present, the majority of systems employ a static third-person perspective or are coupled
witharobottorelayenvironmentaldatatoremoteusers.However,thisapproach,whetherit’sa
singularviewpointormerelyareplicationoftherobot’sperspective,maynotprovidetheuserwith
sufficientcontexttomakeoptimaldecisions.AdvancedXRtechniqueshavethepotentialtooffera
comprehensiveenvironmentaloverview,suchasthedynamicorbird’s-eyeview,asinvestigatedin
ahandfulofstudieswithinourreview.However,theseinvestigationsremainsparse.
Futureresearchcouldconcentrateontheintegrationofmultipleperspectives[118],suchasa
primaryviewlinkedtotherobot,athird-personperspectiveforobservingtherobot,andatop-down
bird’s-eye view. This multi-perspective approach could enhance remote users’ comprehension
ofthedistantenvironment.Itmightevenbefeasibletoestablishavirtualenvironmentcamera
to monitor the user avatar’s operational state and the robot’s digital twin from a third-person
perspectiveinthevirtualenvironment[10],potentiallymitigatingrisksassociatedwithcertain
tasks.
24XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Anotherpromisingavenueforfutureresearchistheaugmentationoftheenvironmentthrough
multimodality,suchasincorporatinghapticfeedbackandauditorycuestoenrichtheuser’sper-
ceptionoftheremoteenvironment.Additionally,theconceptofanadaptiveenvironmentrecon-
structionsystempresentsapotentialresearchdirection.Differentrobotsandtasksmaynecessitate
varyingdegreesofenvironmentalreconstructionfidelity.Forinstance,pick-and-placetasksmay
onlyrequirelow-fidelityreconstructionoftheoperator’stable,whilegeologicalexplorationtasks
maydemandhigh-fidelityenvironmentalreconstruction.Implementingadaptiveenvironmental
reconstructiontailoredtospecifictasksandrobotscouldpotentiallyreducesystemlatencyand
preventbandwidthwastage[139].
LeveragingXR’scapacitytoreconstructremoteenvironmentscouldenhanceremoteoperators’
spatialawareness,therebyimprovingnavigationandtaskperformance.Futureresearchshould
prioritizethedevelopmentofadvancedenvironmentalreconstructiontechniquestoprovideamore
comprehensiveandreal-timedepictionofcomplexenvironments.
5.2.4 DigitalTwininXR-basedRemoteHuman-RobotInteraction. Thedevelopmentandutilization
ofdigitaltwinsinXR-basedremotehuman-robotinteraction(HRI)presentsarangeofopportunities
for future research. In the studies we reviewed, digital twins have emerged as an important
componentinenhancingtheinteractionandintegrationbetweenphysicalandvirtualenvironments.
DigitaltwinscanbeusedasmoreintuitiveinterfacesinXR-basedhuman-computerinteractionto
improvesystemefficiencyanduserexperience,andaremorewidelyusedinavarietyofindustrial
andsocialscenarios[102].
Futureresearchinthisareashouldfocusonimprovingthefidelityofdigitaltwinsandtheir
real-timesynchronizationwiththeirphysicalcounterparts.Thisincludesimprovingtheaccuracy
withwhichdigitaltwinssimulatecomplexphysicalprocessesanddynamics,whichiscriticalfor
applicationsinindustriessuchasmanufacturing,healthcare,andurbanplanning.Additionally,
theintegrationofadvancedmachinelearningandartificialintelligencetechniquescanprovide
smarterandmoreautonomousdigitaltwinscapableofpredictivemaintenance,adaptivelearning,
anddecisionsupport[55].
Digitaltwinsnotonlyenhancetheinteractionbetweenphysicalandvirtualenvironments,but
alsoprovidepossibilitiesforthestudyofscenariosthatareimpossibleorimpracticaltotestin
reality.Forexample,thedesignbySuetal.[110]displaysboththezoomed-inpartsofthetask’s
operational details and a scaled-down model of the robotic arm’s digital twin from the user’s
perspective.Suchadesignallowstheoperatortoobserveboththelocaldetailsandtheoverall
motionoftheroboticarmatthesametime,whichisnotpossibleinreality.Thisdesigntakesfull
advantageofthepotentialofXRanddigitaltwins,andfuturedesignscouldmakeeffortstoexplore
XRanddigitaltwincapabilitiesthatcannotberealizedinreality.
5.3 User-centeredSystemDesign
AsthedomainofXR-basedremoteroboticsoperatingsystemscontinuestomature,thenecessity
foruser-centeredsystemdesignbecomesincreasinglyapparent.Thisdesignphilosophyensures
thedevelopmentofsystemsthatstrikeabalancebetweentechnologicalsophisticationanduser
accessibility,fosteringanintuitiveuserexperience.
Acriticalfacetofuser-centereddesignistheaccommodationofdiverseuserproficiencylevels.
Future research should strive to engineer systems that are universally accessible and efficient,
cateringtousersacrosstheproficiencyspectrum.Thiscouldentailthedesignofadaptiveinterfaces
thatcalibrateinresponsetoauser’sskilllevel,ortheprovisionofcomprehensivetrainingmodules
tofacilitateuserfamiliarizationwiththesystem.Furthermore,theuniqueneedsandpreferences
ofdifferentusergroupswarrantconsideration.Forinstance,asystemintendedforprofessional
25Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
engineersmightnecessitateafocusonprecisionandadvancedfunctionalities,whereasasystem
tailoredforthegeneralpopulacemightemphasizeeaseofuseandintuitivecontrols.Usagesce-
nariosareanotherimportantconsideration,forinstance,asystemdeployedforremotesurgical
procedureswouldhavedifferentspecificationscomparedtooneutilizedforremoteassemblytasks.
Futureresearchshoulddiscernthedistinctneedsofvarioususergroupsandalsogainadeeper
understandingofthesediverseusagescenariosandtheiruniquerequirements.
Lastly,theinnovativepotentialofXRneedstobeconsideredinsystemdesign.XRtechnologies
harborthepotentialtocraftimmersiveandengagingexperiencesthatcansignificantlyenhance
user satisfaction and efficiency. For example, with the rising popularity of home robots, users
operatingahomerobotviaXRmightinteractwithanavatarthatcouldbeasmallanimaloreven
ahuman,ratherthanatraditionalrobot.Futureresearchshouldprobeintoinnovativemethods
ofharnessingXR’spotentialtoelevatetheuserexperience.Thiscouldinvolvetheexplorationof
novelinteractiontechniques,thedevelopmentofimmersivefeedbacksystems,ortheinventionof
uniquevisualizationmethods.
5.4 LongitudinalStudiesandReal-worldDeployment
Oursurveyprovidesevidenceofresearchers’explorationofXR-basedremotehuman-robotinterac-
tionsystemsacrossvariousdomainssuchasindustrialmanufacturingandmedicine.However,these
workshavepredominantlyfocusedonevaluatingsystemperformancewithincontrolledlaboratory
settings,ratherthanreal-worldpracticalapplications.Realenvironmentsarecharacterizedbyin-
creasedcomplexityandamultitudeoffactorsthatcanpotentiallyimpacttaskperformance,factors
thatarenotpresentinlaboratorysettings.Forinstance,theperformanceofasystemoperatingin
arealfieldenvironmentmaybeinfluencedbynaturalelementslikestrongwinds,whichcanbe
mitigatedoreliminatedaltogetherinlaboratorysettings.Moreover,laboratoryevaluationstypically
entailshortdurations,whereasreal-worldindustrialapplicationsdemandlong-termperformance.
Neglectingconsistentusageovertimecanescalatemaintenancecostsanddiminishoperational
efficiency.Insummary,theefficacyobservedinXR-basedremotehuman-robotinteractionsystems
withinlaboratorysettingsmaynotnecessarilytranslatetopracticalindustrialapplications.
Futureresearchshouldfocusonimprovingtheassessmentofsystemsinreal-worldsettings.
Initially,itisnecessarytoassessthesystem’sadaptabilitybyevaluatingjobcompletionperformance
in uncontrolled industrial environments. Furthermore, it is recommended that the experiment
lengthbeextendedtodaysandmonths,inordertoevaluatethesystem’slong-termperformance
anduserbehaviorsinthewild,thereforeguaranteeingitsreliability,usabilityandconsistency.
6 CONCLUSION
ThispaperexplorestheapplicationofExtendedReality(XR)technologiesintheemergingfieldof
remoteHuman-RobotInteraction(HRI),throughacomprehensivereviewandin-depthanalysisof
relevant100literature.Thepurposeofthisresearchistoshedlightonthepotential,obstacles,and
futureresearchdirectionsofXRasappliedtoremoteHRI.
OurfindingsemphasizethetransformativeroleXRhasassumedinreconceptualizingtheremote
HRI paradigm, thereby enabling interaction systems that are simultaneously more engaging,
intuitive,andefficient.Wehavetakenananalyticalapproachtodelineatetheinfluenceexertedby
differentrobotictypesonuserinterfacesandperspectivesandhighlightthenecessityofcustom-
tailoreddesignsthatproperlyconsidertheuniquefeaturesoftherobotandtaskrequirements.
Furthermore,wescrutinizetheemploymentofmultichannelenhancementsasapotentialstrategy
forsupportingteleoperation,whileemphasizingthesignificanceofauser-centereddesigninsystem
development.Ourstudyalsouncoversseveralpertinentareasthatwarrantfurtherinvestigation,
26XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
inclusive but not limited to facilitating multi-user or multi-robot interactions and addressing
challengesassociatedwithlatencyandreal-timeperformance.
Asitstands,theprevailingsystemdesignappearsdeficientinfullyleveragingthecapabilities
offeredbyXRtechnology.Thereexistsanurgentneedforadditionalresearchdedicatedtoadvancing
user-friendly XR system design for remote HRI. The insights presented in this paper not only
contributetotheongoingevolutionofremoteHRIbutalsoserveasanindispensableresource
forresearchers,practitioners,andsystemdesignerswhoaimtouseXRtechnologytooptimize
human-computerinteraction.
REFERENCES
[1] MahmoudAbdulsalamandNabilAouf.2023. VitRobPipeline:ASeamlessTeleoperationPipelineforAdvanced
VirtualReality-RobotInterfaceAppliedforPrecisionAgriculture.In2023IEEEInternationalConferenceonRobotics
andBiomimetics(ROBIO).IEEE,1–6.
[2] DmytroAdamenko,SteffenKunnen,RobinPluhnau,AndréLoibl,andArunNagarajah.2020.Reviewandcomparison
ofthemethodsofdesigningtheDigitalTwin.ProcediaCIRP91(2020),27–32.
[3] ZhumingAi,MarkA.Livingston,andIraS.Moskowitz.2016.Real-timeunmannedaerialvehicle3Denvironment
explorationinamixedrealityenvironment.664–670. https://doi.org/10.1109/ICUAS.2016.7502588
[4] IanFAkyildizandHongzhiGuo.2022.Wirelesscommunicationresearchchallengesforextendedreality(XR).ITU
JournalonFutureandEvolvingTechnologies3,1(2022),1–15.
[5] HaiderAFAlmurib,HaidarFadhilAl-Qrimli,andNandhaKumar.2012.Areviewofapplicationindustrialrobotic
design.In2011NinthInternationalConferenceonICTandKnowledgeEngineering.IEEE,105–112.
[6] DorisAschenbrenner,MengLi,RadoslawDukalski,JoukeVerlinden,andStephanLukosch.2018. Collaborative
ProductionLinePlanningwithAugmentedFabrication.509–510. https://doi.org/10.1109/VR.2018.8446533
[7] XueBai,ChangqiangLi,KeyanChen,YongjieFeng,ZhaoweiYu,andMingXu.2018.Kinect-BasedHandTracking
forFirst-Person-PerspectiveRoboticArmTeleoperation.684–691. https://doi.org/10.1109/ICInfA.2018.8812561
[8] KarlinBark,CuongTran,KikuoFujimura,andVictorNg-Thow-Hing.2014.Personalnavi:Benefitsofanaugmented
realitynavigationalaidusingasee-thru3dvolumetrichud.InProceedingsofthe6thInternationalConferenceon
AutomotiveUserInterfacesandInteractiveVehicularApplications.1–8.
[9] ZahraaBassyouniandImadHElhajj.2021.Augmentedrealitymeetsartificialintelligenceinrobotics:Asystematic
review.FrontiersinRoboticsandAI(2021),296.
[10] SteveBenford,ChrisGreenhalgh,TomRodden,andJamesPycock.2001.Collaborativevirtualenvironments.Commun.
ACM44,7(2001),79–85.
[11] JulioBetancourt,BaptisteWojtkowski,PedroCastillo,andIndiraThouvenin.2022.Exocentriccontrolschemefor
robotapplications:Animmersivevirtualrealityapproach.IEEETransactionsonVisualizationandComputerGraphics
(2022),1–1. https://doi.org/10.1109/TVCG.2022.3160389
[12] FeifeiBian,RuifengLi,LijunZhao,YihuanLiu,andPeidongLiang.2018. InterfaceDesignofaHuman-Robot
InteractionSystemforDual-ManipulatorsTeleoperationBasedonVirtualReality.1361–1366. https://doi.org/10.
1109/ICInfA.2018.8812457
[13] FrankBiocca,ArthurTang,CharlesOwen,andFanXiao.2006. Attentionfunnel:omnidirectional3Dcursorfor
mobileaugmentedrealityplatforms.InProceedingsoftheSIGCHIconferenceonHumanFactorsincomputingsystems.
1115–1122.
[14] DavidBlack,YasOloumiYazdi,AmirHosseinHadiHosseinabadi,andSeptimiuSalcudean.2023.Humanteleoperation-
ahapticallyenabledmixedrealitysystemforteleultrasound.Human–ComputerInteraction(2023),1–24.
[15] DougABowman,JosephLGabbard,andDeborahHix.2002.Asurveyofusabilityevaluationinvirtualenvironments:
classificationandcomparisonofmethods.Presence:Teleoperators&VirtualEnvironments11,4(2002),404–424.
[16] FilippoBrizzi,LorenzoPeppoloni,AlessandroGraziano,ErikaDiStefano,CarloAlbertoAvizzano,andEmanuele
Ruffaldi.2018. EffectsofAugmentedRealityonthePerformanceofTeleoperatedIndustrialAssemblyTasksina
RoboticEmbodiment.IEEETransactionsonHuman-MachineSystems48,2(2018),197–206. https://doi.org/10.1109/
THMS.2017.2782490
[17] JohnBrooke.1996.Sus:a“quickanddirty’usability.Usabilityevaluationinindustry189,3(1996),189–194.
[18] GrigoreCBurdeaandPhilippeCoiffet.2003.Virtualrealitytechnology.JohnWiley&Sons.
[19] SoniaMaryChacko,ArmandoGranado,AshwinRajKumar,andVikramKapila.2020.AnAugmentedRealitySpatial
ReferencingSystemforMobileRobots.4446–4452. https://doi.org/10.1109/IROS45743.2020.9340742
[20] JunshenChen,MarcGlover,ChenguangYang,ChunxuLi,ZhijunLi,andAngeloCangelosi.2017.Developmentofan
immersiveinterfaceforrobotteleoperation.Springer,1–15.
27Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
[21] YangChen,LeyuanSun,MehdiBenallegue,RafaelCisneros-Limón,RohanP.Singh,KenjiKaneko,ArnaudTanguy,
GuillaumeCaron,KenjiSuzuki,andAbderrahmaneKheddar.2022. EnhancedVisualFeedbackwithDecoupled
ViewpointControlinImmersiveHumanoidRobotTeleoperationusingSLAM.IEEE,306–313.
[22] YiChen,BaohuaZhang,JunZhou,andKaiWang.2020. Real-time3Dunstructuredenvironmentreconstruction
utilizingVRandKinect-basedimmersiveteleoperationforagriculturalfieldrobots. ComputersandElectronicsin
Agriculture175(2020),105579. https://doi.org/10.1016/j.compag.2020.105579
[23] SungHoChoi,Kyeong-BeomPark,DongHyeonRoh,JaeYeolLee,YaldaGhasemi,andHeejinJeong.2022. An
XR-basedApproachtoSafeHuman-RobotCollaboration.481–482. https://doi.org/10.1109/VRW55335.2022.00106
[24] TorbenCichonandJürgenRoßmann.2018.DigitalTwins:AssistingandSupportingCooperationinHuman-Robot
Teams.486–491. https://doi.org/10.1109/ICARCV.2018.8580634
[25] AndyCockburn,AmyKarlson,andBenjaminBBederson.2009.Areviewofoverview+detail,zooming,andfocus+
contextinterfaces.ACMComputingSurveys(CSUR)41,1(2009),1–31.
[26] R.Codd-Downey,P.MojiriForooshani,A.Speers,H.Wang,andM.Jenkin.2014. FromROStounity:Leveraging
robotandvirtualenvironmentmiddlewareforimmersiveteleoperation.932–936. https://doi.org/10.1109/ICInfA.
2014.6932785
[27] GabrieldeMouraCosta,MarceloRobertoPetry,andAntónioPauloMoreira.2022.AugmentedRealityforHuman–
RobotCollaborationandCooperationinIndustrialApplications:ASystematicLiteratureReview.Sensors22,7(2022),
2725.
[28] MatthewCousins,ChenguangYang,JunshenChen,WeiHe,andZhaojieJu.2017.Developmentofamixedreality
basedinterfaceforhumanrobotinteraciotn,Vol.1.27–34. https://doi.org/10.1109/ICMLC.2017.8107738
[29] ChristyanCruzUlloa,DavidDomínguez,JaimeDelCerro,andAntonioBarrientos.2022. AMixed-RealityTele-
OperationMethodforHigh-LevelControlofaLegged-ManipulatorRobot.Sensors22,21(2022),8146.
[30] MarkRCutkosky.2012.Roboticgraspingandfinemanipulation.Vol.6.SpringerScience&BusinessMedia.
[31] FrancescoDePace,FedericoManuri,AndreaSanna,andClaudioFornaro.2020.AsystematicreviewofAugmented
Realityinterfacesforcollaborativeindustrialrobots.Computers&IndustrialEngineering149(2020),106806.
[32] MortezaDianatfar,JyrkiLatokartano,andMinnaLanz.2021.ReviewonexistingVR/ARsolutionsinhuman–robot
collaboration.ProcediaCIRP97(2021),407–411.
[33] GuanglongDu,YongdaDeng,WingWYNg,andDiLi.2022.AnIntelligentInteractionFrameworkforTeleoperation
BasedonHuman-MachineCooperation.IEEETransactionsonHuman-MachineSystems52,5(2022),963–972.
[34] GuanglongDu,RuiguangHan,GengchengYao,WingW.Y.Ng,andDiLi.2022.AGesture-andSpeech-GuidedRobot
TeleoperationMethodBasedonMobileInteractionWithUnrestrictedForceFeedback.IEEE/ASMETransactionson
Mechatronics27,1(2022),360–371. https://doi.org/10.1109/TMECH.2021.3064581
[35] GuanglongDu,BoZhang,ChunquanLi,andHuaYuan.2019.ANovelNaturalMobileHuman-MachineInteraction
MethodWithAugmentedReality.IEEEAccess7(2019),154317–154330. https://doi.org/10.1109/ACCESS.2019.2948880
[36] WenFan,XiaoqingGuo,EnyangFeng,JialinLin,YuanyiWang,JiamingLiang,MartinGarrad,JonathanRossiter,
ZhengyouZhang,NathanLepora,etal.2023.DigitalTwin-DrivenMixedRealityFrameworkforImmersiveTeleoper-
ationWithHapticRendering.IEEERoboticsandAutomationLetters(2023).
[37] SimoneFani,SimoneCiotti,ManuelGCatalano,GiorgioGrioli,AlessandroTognetti,GaetanoValenza,Arash
Ajoudani,andMatteoBianchi.2018.Simplifyingtelerobotics:Wearabilityandteleimpedanceimproveshuman-robot
interactionsinteleoperation.IEEERobotics&AutomationMagazine25,1(2018),77–88.
[38] SimoneFani,SimoneCiotti,ManuelG.Catalano,GiorgioGrioli,AlessandroTognetti,GaetanoValenza,Arash
Ajoudani,andMatteoBianchi.2018. SimplifyingTelerobotics:WearabilityandTeleimpedanceImprovesHuman-
RobotInteractionsinTeleoperation.IEEERobotics&AutomationMagazine25,1(2018),77–88. https://doi.org/10.
1109/MRA.2017.2741579
[39] JesseFoxandAndrewGambino.2021.Relationshipdevelopmentwithhumanoidsocialrobots:Applyinginterpersonal
theoriestohuman–robotinteraction.Cyberpsychology,Behavior,andSocialNetworking24,5(2021),294–299.
[40] KodaiFuchino,MohammedAl-Sada,andTatsuoNakajima.2023.T2Remoter:aRemoteTableTennisCoachingSystem
CombiningVRandRobotics.In2023IEEE29thInternationalConferenceonEmbeddedandReal-TimeComputing
SystemsandApplications(RTCSA).IEEE,275–276.
[41] MarkusFunk,MareikeKritzler,andFlorianMichahelles.2017.HoloLensismorethanairTap:naturalandintuitive
interactionwithholograms.InProceedingsoftheseventhinternationalconferenceontheinternetofthings.1–2.
[42] WeiGai,HuiyuLi,YanshuaiZhao,MaiwangShi,WenfeiWang,YunchuanSun,andChengleiYang.2020. ANew
NavigationMethodforVR-basedTeleroboticSystemviaSupervisor’sRealWalkinginaLimitedPhysicalSpace.
494–498. https://doi.org/10.1109/IWCMC48107.2020.9148474
[43] PéterGalambos,ÁdámCsapó,PéterZentay,IstvánMarcellFülöp,TamásHaidegger,PéterBaranyi,andImreJ.Rudas.
2015.Design,programmingandorchestrationofheterogeneousmanufacturingsystemsthroughVR-poweredremote
collaboration.RoboticsandComputer-IntegratedManufacturing33(2015),68–77. https://doi.org/10.1016/j.rcim.2014.
28XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
08.012
[44] LuigiGammieri,MarcoSchumann,LuigiPelliccia,GiuseppeDiGironimo,andPhilippKlimant.2017.Couplingofa
RedundantManipulatorwithaVirtualRealityEnvironmenttoEnhanceHuman-robotCooperation.ProcediaCIRP62
(2017),618–623. https://doi.org/10.1016/j.procir.2016.06.056
[45] VicentGirbes-Juan,ViniciusSchettino,YiannisDemiris,andJosepTornero.2020. Hapticandvisualfeedback
assistancefordual-armrobotteleoperationinsurfaceconditioningtasks.IEEETransactionsonHaptics14,1(2020),
44–56.
[46] LiangGong,ChangyangGong,ZhaoMa,LujieZhao,ZhenyuWang,XudongLi,XiaolongJing,HaozheYang,and
ChengliangLiu.2017.Real-timehuman-in-the-loopremotecontrolforalife-sizetrafficpolicerobotwithmultiple
augmentedrealityaideddisplayterminals.420–425. https://doi.org/10.1109/ICARM.2017.8273199
[47] ScottAGreen,MarkBillinghurst,XiaoQiChen,andJGeoffreyChase.2007.Humanrobotcollaboration:Anaugmented
realityapproach—aliteraturereviewandanalysis.InInternationalDesignEngineeringTechnicalConferencesand
ComputersandInformationinEngineeringConference,Vol.48051.117–126.
[48] FabienGrzeskowiak,MarieBabel,JulienBruneau,andJulienPettre.2020.TowardVirtualReality-basedEvaluation
ofRobotNavigationamongPeople.766–774. https://doi.org/10.1109/VR46266.2020.00100
[49] VehbiCGungorandGerhardPHancke.2009.Industrialwirelesssensornetworks:Challenges,designprinciples,and
technicalapproaches.IEEETransactionsonindustrialelectronics56,10(2009),4258–4265.
[50] SandraGHartandLowellEStaveland.1988.DevelopmentofNASA-TLX(TaskLoadIndex):Resultsofempiricaland
theoreticalresearch.InAdvancesinpsychology.Vol.52.Elsevier,139–183.
[51] HoomanHedayati,MichaelWalker,andDanielSzafir.2018.Improvingcollocatedrobotteleoperationwithaugmented
reality.78–86.
[52] JuanDavidHernández,ShlokSobti,AnthonySciola,MarkMoll,andLydiaE.Kavraki.2020. IncreasingRobot
AutonomyviaMotionPlanningandanAugmentedRealityInterface.IEEERoboticsandAutomationLetters5,2(2020),
1017–1023. https://doi.org/10.1109/LRA.2020.2967280
[53] LeireAmezuaHormaza,WaelM.Mohammed,BorjaRamisFerrer,RonalBejarano,andJoseL.MartinezLastra.2019.
On-lineTrainingandMonitoringofRobotTasksthroughVirtualReality,Vol.1.841–846. https://doi.org/10.1109/
INDIN41052.2019.8971967
[54] JiHou,BenjaminGraham,MatthiasNießner,andSainingXie.2021.Exploringdata-efficient3dsceneunderstanding
withcontrastivescenecontexts.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
15587–15597.
[55] ZiqiHuang,YangShen,JiayiLi,MarcelFey,andChristianBrecher.2021. AsurveyonAI-drivendigitaltwinsin
industry4.0:Smartmanufacturingandadvancedrobotics.Sensors21,19(2021),6340.
[56] WolfgangHönig,ChristinaMilanes,LisaScaria,ThaiPhan,MarkBolas,andNoraAyanian.2015.Mixedrealityfor
robotics.5382–5387. https://doi.org/10.1109/IROS.2015.7354138
[57] KeiichiIhara,MehradFaridan,AyumiIchikawa,IkkakuKawaguchi,andRyoSuzuki.2023.HoloBots:Augmenting
HolographicTelepresencewithMobileRobotsforTangibleRemoteCollaborationinMixedReality.InProceedingsof
the36thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology.1–12.
[58] InmoJang,JunyanHu,FarshadArvin,JoaquinCarrasco,andBarryLennox.2021. OmnipotentVirtualGiantfor
RemoteHuman–SwarmInteraction.488–494. https://doi.org/10.1109/RO-MAN50785.2021.9515542
[59] InmoJang,HanlinNiu,EmilyC.Collins,AndrewWeightman,JoaquinCarrasco,andBarryLennox.2021.Virtual
KinestheticTeachingforBimanualTelemanipulation.120–125. https://doi.org/10.1109/IEEECONF49454.2021.9382763
[60] CarolineJay,MashhudaGlencross,andRogerHubbold.2007. Modelingtheeffectsofdelayedhapticandvisual
feedbackinacollaborativevirtualenvironment.ACMTransactionsonComputer-HumanInteraction(TOCHI)14,2
(2007),8–es.
[61] IvanKalinov,DariaTrinitatova,andDzmitryTsetserukou.2021.WareVR:VirtualRealityInterfaceforSupervisionof
AutonomousRoboticSystemAimedatWarehouseStocktaking.2139–2145. https://doi.org/10.1109/SMC52423.2021.
9659133
[62] KotaroKanazawa,NoritakaSato,andYoshifumiMorita.2023.ConsiderationsonInteractionwithManipulatorin
VirtualRealityTeleoperationInterfaceforRescueRobots.In202332ndIEEEInternationalConferenceonRobotand
HumanInteractiveCommunication(RO-MAN).IEEE,386–391.
[63] YaesolKim,MyrnaCitlaliCastilloSilva,SaraAnastasi,andNikhilDeshpande.2023.TowardsImmersiveBilateral
TeleoperationUsingEncountered-TypeHapticInterface.In2023IEEEInternationalConferenceonSystems,Man,and
Cybernetics(SMC).IEEE,1354–1359.
[64] AkemiKobayashi,RyosukeAoki,NorimichiKitagawa,ToshitakaKimura,YouichiTakashima,andTomohiroYamada.
2016. Towardsenhancingforce-inputinteractionbyvisual-auditoryfeedbackasanintroductionoffirstuse.In
Human-ComputerInteraction.InteractionPlatformsandTechniques:18thInternationalConference,HCIInternational
2016,Toronto,ON,Canada,July17-22,2016.Proceedings,PartII18.Springer,180–191.
29Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
[65] RomanKulikovskiy,MeganSochanski,MattesonEaton,JessicaKorneder,andWing-YueGeoffreyLouie.2021.Can
therapistsdesignrobot-mediatedinterventionsandteleoperaterobotsusingVRtodeliverinterventionsforASD?
IEEE,3669–3676.
[66] Chen-YuKuo,Chun-ChiHuang,Chih-HsuanTsai,Yun-ShuoShi,andShanaSmith.2021. Developmentofan
immersiveSLAM-basedVRsystemforteleoperationofamobilemanipulatorinanunknownenvironment.Computers
inIndustry132(2021),103502. https://doi.org/10.1016/j.compind.2021.103502
[67] BettinaLaugwitz,TheoHeld,andMartinSchrepp.2008.Constructionandevaluationofauserexperiencequestion-
naire.InHCIandUsabilityforEducationandWork:4thSymposiumoftheWorkgroupHuman-ComputerInteractionand
UsabilityEngineeringoftheAustrianComputerSociety,USAB2008,Graz,Austria,November20-21,2008.Proceedings4.
Springer,63–76.
[68] DinhTungLe,SheilaSutjipto,YujunLai,andGavinPaul.2020.IntuitiveVirtualRealitybasedControlofaReal-world
MobileManipulator.767–772. https://doi.org/10.1109/ICARCV50220.2020.9305517
[69] Ju-HwanLeeandCharlesSpence.2008.Assessingthebenefitsofmultimodalfeedbackondual-taskperformance
underdemandingconditions.PeopleandComputersXXIICulture,Creativity,Interaction22(2008),185–192.
[70] ChengxiLi,PaiZheng,ShufeiLi,YatmingPang,andCarmanKMLee.2022.AR-assisteddigitaltwin-enabledrobot
collaborativemanufacturingsystemwithhuman-in-the-loop.RoboticsandComputer-IntegratedManufacturing76
(2022),102321.
[71] ZhijunLi,CuichaoXu,QiangWei,ChaoShi,andChun-YiSu.2018.Human-inspiredcontrolofdual-armexoskeleton
robotswithforceandimpedanceadaptation. IEEETransactionsonSystems,Man,andCybernetics:Systems50,12
(2018),5296–5305.
[72] JeffreyI.Lipton,AidanJ.Fay,andDanielaRus.2017.Baxter’shomunculus:Virtualrealityspacesforteleoperationin
manufacturing.IEEERoboticsandAutomationLetters3,1(2017),179–186.
[73] DiLiu,JeongheeKim,andYoungjibHam.2023.Multi-userimmersiveenvironmentforexcavatorteleoperationin
construction.AutomationinConstruction156(2023),105143.
[74] XinLiu,DuJiang,BoTao,GuozhangJiang,YingSun,JianyiKong,XiliangTong,GuojunZhao,andBaojiaChen.2022.
Geneticalgorithm-basedtrajectoryoptimizationfordigitaltwinrobots.FrontiersinBioengineeringandBiotechnology
9(2022),793782.
[75] PraveenKumarReddyMaddikunta,Quoc-VietPham,BPrabadevi,NatarajanDeepa,KapalDev,ThippaReddy
Gadekallu,RukhsanaRuby,andMadhusankaLiyanage.2022.Industry5.0:Asurveyonenablingtechnologiesand
potentialapplications.JournalofIndustrialInformationIntegration26(2022),100257.
[76] ZhanatMakhataevaandHuseyinAtakanVarol.2020.Augmentedrealityforrobotics:Areview.Robotics9,2(2020),
21.
[77] NeilMcHenry,JasonSpencer,PatrickZhong,JeremyCox,MichaelAmiscaray,K.C.Wong,andGregoryChamitoff.2021.
PredictiveXRTelepresenceforRoboticOperationsinSpace.1–10. https://doi.org/10.1109/AERO50100.2021.9438161
[78] LingxiaoMeng,JiangshanLiu,WeiChai,JiankunWang,andMaxQ-HMeng.2023. VirtualRealityBasedRobot
TeleoperationviaHuman-SceneInteraction.ProcediaComputerScience226(2023),141–148.
[79] MohammadMehdiMoniri,FabioAndresEspinosaValcarcel,DieterMerkel,andDanielSonntag.2016.HumanGaze
andFocus-of-AttentioninDualRealityHuman-RobotCollaboration.238–241. https://doi.org/10.1109/IE.2016.54
[80] D.Mourtzis,V.Zogopoulos,andE.Vlachou.2017.AugmentedRealityApplicationtoSupportRemoteMaintenance
asaServiceintheRoboticsIndustry.ProcediaCIRP63(2017),46–51. https://doi.org/10.1016/j.procir.2017.03.154
[81] DimitrisMourtzis,VasiliosZogopoulos,andFotiniXanthi.2019. Augmentedrealityapplicationtosupportthe
assemblyofhighlycustomizedproductsandtoadapttoproductionre-scheduling. TheInternationalJournalof
AdvancedManufacturingTechnology105(2019),3899–3910.
[82] HengyangMu,YifeiLi,DianshengChen,JitingLi,andMinWang.2021.DesignofTankInspectionRobotNavigation
SystemBasedonVirtualReality.1773–1778. https://doi.org/10.1109/ROBIO54168.2021.9739389
[83] BálintGyörgyNagy,JánosDóka,SáandorRácz,GézaSzabó,IstvánPelle,JánosCzentye,LászlóToka,andBalázs
Sonkoly.2019.TowardsHuman-RobotCollaboration:AnIndustry4.0VRPlatformwithCloudsUndertheHood.1–2.
https://doi.org/10.1109/ICNP.2019.8888107
[84] PNandhini,PChellammal,JSJaslin,SHarthyRubyPriya,MUma,andRKaviyaraj.2023.TeleoperationintheAgeof
MixedReality:VR,AR,andROSIntegrationforHuman-RobotDirectInteraction.In20234thInternationalConference
onElectronicsandSustainableCommunicationSystems(ICESC).IEEE,240–245.
[85] FedericaNenna,DavideZanardi,andLucianoGamberini.2023.EnhancedInteractivityinVR-basedTelerobotics:An
Eye-trackingInvestigationofHumanPerformanceandWorkload.InternationalJournalofHuman-ComputerStudies
177(2023),103079.
[86] BukeikhanOmarali,BriceDenoun,KasparAlthoefer,LorenzoJamone,MaurizioValle,andIldarFarkhatdinov.
2020.VirtualRealitybasedTeleroboticsFrameworkwithDepthCameras.1217–1222. https://doi.org/10.1109/RO-
MAN47096.2020.9223445
30XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
[87] ClaudioPacchierotti,AsadTirmizi,andDomenicoPrattichizzo.2014.Improvingtransparencyinteleoperationby
meansofcutaneoustactileforcefeedback.ACMTransactionsonAppliedPerception(TAP)11,1(2014),1–16.
[88] SungmanPark,YeongtaeJung,andJoonbumBae.2018.Aninteractiveandintuitivecontrolinterfaceforatele-operated
robot(AVATAR)system.Mechatronics55(2018),54–62. https://doi.org/10.1016/j.mechatronics.2018.08.011
[89] MichaelQuinnPatton.2002.Qualitativeresearch&evaluationmethods.sage.
[90] LorenzoPeppoloni,FilippoBrizzi,CarloAlbertoAvizzano,andEmanueleRuffaldi.2015.ImmersiveROS-integrated
frameworkforrobotteleoperation.177–178. https://doi.org/10.1109/3DUI.2015.7131758
[91] LorenzoPeppoloni,FilippoBrizzi,EmanueleRuffaldi,andCarloAlbertoAvizzano.2015.Augmentedreality-aided
tele-presencesystemforrobotmanipulationinindustrialmanufacturing.237–240.
[92] OrnnalinPhaijit,MohammadObaid,ClaudeSammut,andWafaJohal.2022.ATaxonomyofFunctionalAugmented
RealityforHuman-RobotInteraction.InProceedingsofthe2022ACM/IEEEInternationalConferenceonHuman-Robot
Interaction.294–303.
[93] ThaiPhan,WolfgangHönig,andNoraAyanian.2018.MixedRealityCollaborationBetweenHuman-AgentTeams.
659–660. https://doi.org/10.1109/VR.2018.8446542
[94] PolinaPonomareva,DariaTrinitatova,AlekseyFedoseev,IvanKalinov,andDzmitryTsetserukou.2021.GraspLook:a
VR-basedTelemanipulationSystemwithR-CNN-drivenAugmentationofVirtualEnvironment.IEEE,166–171.
[95] JorgePenaQueralta,JussiTaipalmaa,BilgeCanPullinen,VictorKathanSarker,TuanNguyenGia,HannuTenhunen,
MoncefGabbouj,JenniRaitoharju,andTomiWesterlund.2020.Collaborativemulti-robotsystemsforsearchand
rescue:Coordinationandperception.arXivpreprintarXiv:2008.12610(2020).
[96] NaveenRastogiandAmitKumarSrivastava.2019.Controlsystemdesignfortokamakremotemaintenanceoperations
usingassistedvirtualrealityandhapticfeedback.FusionEngineeringandDesign139(2019),47–54. https://doi.org/10.
1016/j.fusengdes.2018.12.094
[97] AndoniRivera-Pinto,JohanKildal,andElenaLazkano.2023. TowardProgrammingaCollaborativeRobotby
InteractingwithItsDigitalTwininaMixedRealityEnvironment. InternationalJournalofHuman–Computer
Interaction(2023),1–13.
[98] GiulioRosati,AntonioRodà,FedericoAvanzini,andStefanoMasiero.2013. Ontheroleofauditoryfeedbackin
robot-assistedmovementtrainingafterstroke:reviewoftheliterature.Computationalintelligenceandneuroscience
2013(2013),11–11.
[99] MoseSakashita,HyunjuKim,BrandonWoodard,RuidongZhang,andFrançoisGuimbretière.2023.VRoxy:Wide-Area
CollaborationFromanOfficeUsingaVR-DrivenRoboticProxy.InProceedingsofthe36thAnnualACMSymposium
onUserInterfaceSoftwareandTechnology.1–13.
[100] FilippoSanfilippo,JesperSmith,SylvainBertrand,andTorHalvardSkarbergSvendsen.2022. Mixedreality(MR)
EnabledProprioandTeleoperationofaHumanoidRobotforParaplegicPatients.In20225thInternationalConference
onInformationandComputerTechnologies(ICICT).IEEE,153–158.
[101] AlexanderSchäfer,GerdReis,andDidierStricker.2021.ASurveyonSynchronousAugmented,VirtualandMixed
RealityRemoteCollaborationSystems.ACMComputingSurveys(CSUR)(2021).
[102] Michael Schluse, Marc Priggemeyer, Linus Atorf, and Juergen Rossmann. 2018. Experimentable digital
twins—Streamliningsimulation-basedsystemsengineeringforindustry4.0. IEEETransactionsonindustrialin-
formatics14,4(2018),1722–1731.
[103] RalphSchroeder.2010. BeingThereTogether:Socialinteractioninsharedvirtualenvironments. OxfordUniversity
Press.
[104] MaxSchwarz,ChristianLenz,AndreRochow,MichaelSchreiber,andSvenBehnke.2021.NimbRoAvatar:Interactive
ImmersiveTelepresencewithForce-FeedbackTelemanipulation.5312–5319. https://doi.org/10.1109/IROS51168.2021.
9636191
[105] ThomasBSheridan.2016.Human–robotinteraction:statusandchallenges.Humanfactors58,4(2016),525–532.
[106] HarveyStedman,BasaranBahadirKocer,MirkoKovac,andVijayM.Pawar.2022. VRTAB-Map:AConfigurable
ImmersiveTeleoperationFrameworkwithOnline3DReconstruction.IEEE,104–110.
[107] JannekSteinke,JustusRischke,PeterSossalla,JohannesHofer,ChristianLVielhaus,NicoVomHofe,andHPFrank
Fitzek.2023.TheFutureofDogWalking–Four-LeggedRobotsandAugmentedReality.In2023IEEE24thInternational
SymposiumonaWorldofWireless,MobileandMultimediaNetworks(WoWMoM).IEEE,352–354.
[108] FranzSteinmetz,AnnikaWollschläger,andRomanWeitschat.2018.Razer—ahriforvisualtask-levelprogramming
andintuitiveskillparameterization.IEEERoboticsandAutomationLetters3,3(2018),1362–1369.
[109] PatrickStotko,StefanKrumpen,MaxSchwarz,ChristianLenz,SvenBehnke,ReinhardKlein,andMichaelWeinmann.
2019. AVRSystemforImmersiveTeleoperationandLiveExplorationwithaMobileRobot.3630–3637. https:
//doi.org/10.1109/IROS40897.2019.8968598
[110] Yun-PengSu,Xiao-QiChen,TonyZhou,ChristopherPretty,andGeoffreyChase.2021. Mixedreality-enhanced
intuitiveteleoperationwithhybridvirtualfixturesforintelligentroboticwelding. AppliedSciences11,23(2021),
31Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
11280.
[111] ZhongdaSun,MingluZhu,ZhaocongChen,XuechuanShan,andChengkuoLee.2021.Haptic-FeedbackRingEnabled
Human-MachineInterface(HMI)AimingatImmersiveVirtualRealityExperience.333–336. https://doi.org/10.1109/
Transducers50396.2021.9495698
[112] RyoSuzuki,AdnanKarim,TianXia,HoomanHedayati,andNicolaiMarquardt.2022.AugmentedRealityandRobotics:
ASurveyandTaxonomyforAR-enhancedHuman-RobotInteractionandRoboticInterfaces.InCHIConferenceon
HumanFactorsinComputingSystems.1–33.
[113] KrzysztofAdamSzczurek,RaulMarinPrades,EloiseMatheson,JoseRodriguez-Nogueira,andMarioDiCastro.2023.
Multimodalmulti-usermixedrealityhuman–robotinterfaceforremoteoperationsinhazardousenvironments.IEEE
Access11(2023),17305–17333.
[114] BarnabasTakacs,GergelyRichter,KlaraCsizinszky,DanieleMazzei,andLajosSimon.2015.Towardsaunifiedcontrol
frameworkforhumanoidrobotsandtheirvirtualavatarsinphysicalandvirtualreality-basedinteractions.1905–1909.
https://doi.org/10.1109/ICCAS.2015.7364676
[115] BahiTakkoucheandGuyNorman.2011.PRISMAstatement.Epidemiology22,1(2011),128.
[116] RussellHTaylor,AriannaMenciassi,GaborFichtinger,PaoloFiorini,andPaoloDario.2016.Medicalroboticsand
computer-integratedsurgery.Springerhandbookofrobotics(2016),1657–1684.
[117] KonstantinosTheofilis,JasonOrlosky,YukieNagai,andKiyoshiKiyokawa.2016.Panoramicviewreconstructionfor
stereoscopicteleoperationofahumanoidrobot.242–248. https://doi.org/10.1109/HUMANOIDS.2016.7803284
[118] JGregoryTrafton,NicholasLCassimatis,MagdalenaDBugajska,DerekPBrock,FarileeEMintz,andAlanCSchultz.
2005.Enablingeffectivehuman-robotinteractionusingperspective-takinginrobots.IEEETransactionsonSystems,
Man,andCybernetics-PartA:SystemsandHumans35,4(2005),460–470.
[119] FernandoTrejoandYaopingHu.2018. UserPerformanceofVR-BasedDissection:DirectMappingandMotion
CouplingofaSurgicalTool.3039–3044. https://doi.org/10.1109/SMC.2018.00516
[120] DariaTrinitatovaandDzmitryTsetserukou.2023.StudyoftheEffectivenessofaWearableHapticInterfaceWith
CutaneousandVibrotactileFeedbackforVR-BasedTeleoperation.IEEETransactionsonHaptics(2023).
[121] YusukeUjitokoandYukiBan.2021. Surveyofpseudo-haptics:Hapticfeedbackdesignandapplicationproposals.
IEEETransactionsonHaptics14,4(2021),699–711.
[122] BalazsP.Vagvolgyi,WillPryor,RyanReedy,WenlongNiu,AntonDeguet,LouisL.Whitcomb,SimonLeonard,and
PeterKazanzides.2018.Scenemodelingandaugmentedvirtualityinterfaceforteleroboticsatelliteservicing.IEEE
RoboticsandAutomationLetters3,4(2018),4241–4248.
[123] AnuragSaiVempati,HarshitKhurana,VojtechKabelka,SimonFlueckiger,RolandSiegwart,andPaulBeardsley.2019.
AVirtualRealityInterfaceforanAutonomousSprayPaintingUAV.IEEERoboticsandAutomationLetters4,3(2019),
2870–2877. https://doi.org/10.1109/LRA.2019.2922588
[124] ThanhLongVu,DacDangKhoaNguyen,SheilaSutjipto,DinhTungLe,andGavinPaul.2022.InvestigationofUser
PerformanceinVirtualReality-basedAnnotation-assistedRemoteRobotControl.1–2.
[125] MichaelWalker,ZhaozhongChen,MatthewWhitlock,DavidBlair,DanielleAlbersSzafir,ChristofferHeckman,
andDanielSzafir.2021. Amixedrealitysupervisionandtelepresenceinterfaceforoutdoorfieldrobotics.IEEE,
2345–2352.
[126] MichaelWalker,ThaoPhung,TathagataChakraborti,TomWilliams,andDanielSzafir.2022.Virtual,augmented,
andmixedrealityforhuman-robotinteraction:Asurveyandvirtualdesignelementtaxonomy. arXivpreprint
arXiv:2202.11249(2022).
[127] MichaelE.Walker,HoomanHedayati,andDanielSzafir.2019.RobotTeleoperationwithAugmentedRealityVirtual
Surrogates.202–210. https://doi.org/10.1109/HRI.2019.8673306
[128] ThomasWaltemate,IreneSenna,FelixHülsmann,MariekeRohde,StefanKopp,MarcErnst,andMarioBotsch.2016.
Theimpactoflatencyonperceptualjudgmentsandmotorperformanceinclosed-loopinteractioninvirtualreality.
InProceedingsofthe22ndACMconferenceonvirtualrealitysoftwareandtechnology.27–35.
[129] Ker-JiunWang,CarolineYanZheng,andZhi-HongMao.2019. Human-Centered,ErgonomicWearableDevice
withComputerVisionAugmentedIntelligenceforVRMultimodalHuman-SmartHomeObjectInteraction.767–768.
https://doi.org/10.1109/HRI.2019.8673156
[130] PengWang,XiaoliangBai,MarkBillinghurst,ShushengZhang,XiangyuZhang,ShuxiaWang,WeipingHe,Yuxiang
Yan,andHongyuJi.2021.AR/MRremotecollaborationonphysicaltasks:Areview.RoboticsandComputer-Integrated
Manufacturing72(2021),102071.
[131] PanWang,JunhaoXiao,HuiminLu,HuiZhang,RuoyiYan,andShaozunHong.2017.Anovelhuman-robotinteraction
systembasedon3Dmappingandvirtualreality.5888–5894. https://doi.org/10.1109/CAC.2017.8243836
[132] QiyueWang,YongchaoCheng,WenhuaJiao,MichaelT.Johnson,andYuMingZhang.2019.Virtualrealityhuman-
robotcollaborativewelding:Acasestudyofweavinggastungstenarcwelding.JournalofManufacturingProcesses48
(2019),210–217. https://doi.org/10.1016/j.jmapro.2019.10.016
32XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
[133] QiyueWang,WenhuaJiao,PengWang,andYuMingZhang.2021. DigitalTwinforHuman-RobotInteractive
WeldingandWelderBehaviorAnalysis. IEEE/CAAJournalofAutomaticaSinica8,2(2021),334–343. https:
//doi.org/10.1109/JAS.2020.1003518
[134] QiyueWang,WenhuaJiao,RuiYu,MichaelT.Johnson,andYuMingZhang.2019. ModelingofHumanWelders’
OperationsinVirtualRealityHuman–RobotInteraction.IEEERoboticsandAutomationLetters4,3(2019),2958–2964.
https://doi.org/10.1109/LRA.2019.2921928
[135] XianWang,DiegoMonteiro,Lik-HangLee,PanHui,andHai-NingLiang.2022.Vibroweight:Simulatingweightand
centerofgravitychangesofobjectsinvirtualrealityforenhancedrealism.In2022IEEEhapticssymposium(HAPTICS).
IEEE,1–7.
[136] DongWei,BidanHuang,andQiangLi.2021.Multi-viewmergingforrobotteleoperationwithvirtualreality.IEEE
RoboticsandAutomationLetters6,4(2021),8537–8544.
[137] DavidWeintrop,AfsoonAfzal,JeanSalac,PatrickFrancis,BoyangLi,DavidCShepherd,andDianaFranklin.2018.
EvaluatingCoBlox:Acomparativestudyofroboticsprogrammingenvironmentsforadultnovices.InProceedingsof
the2018CHIConferenceonHumanFactorsinComputingSystems.1–12.
[138] S.Wibowo,I.Siradjuddin,F.Ronilaya,andM.N.Hidayat.2021. Improvingteleoperationrobotsperformanceby
eliminatingviewlimitusing360cameraandenhancingtheimmersiveexperienceutilizingVRheadset,Vol.1073.
IOPPublishing,012037.
[139] MathiasWien,RenaudCazoulat,AndreasGraffunder,AndreasHutter,andPeterAmon.2007.Real-timesystemfor
adaptivevideostreamingbasedonSVC.IEEETransactionsonCircuitsandSystemsforVideoTechnology17,9(2007),
1227–1237.
[140] MahisornWongphati,YushiMatsuda,HirokataOsawa,andMichitaImai.2012.Wheredoyouwanttousearobotic
arm?Andwhatdoyouwantfromtherobot?.In2012IEEERO-MAN:The21stIEEEInternationalSymposiumonRobot
andHumanInteractiveCommunication.IEEE,322–327.
[141] PengxiangXia,HengxuYou,andJingDu.2023.Visual-hapticfeedbackforROVsubseanavigationcontrol.Automation
inConstruction154(2023),104987.
[142] JiachengXie,ShuguangLiu,andXuewenWang.2022. Frameworkforaclosed-loopcooperativehumanCyber-
PhysicalSystemfortheminingindustrydrivenbyVRandAR:MHCPS. Computers&IndustrialEngineering168
(2022),108050. https://doi.org/10.1016/j.cie.2022.108050
[143] RuishuoXu,WeijunWang,WeiFeng,ZhaokunZhou,BoyoungAn,RuizhenGao,andKaichenZhou.2022.Designof
ahuman-robotinteractionsystemforrobotteleoperationbasedondigitaltwinning.IEEE,720–726.
[144] ShiyuXu,ScottMoore,andAkanselCosgun.2022. Shared-ControlRoboticManipulationinVirtualReality.1–6.
https://doi.org/10.1109/HORA55278.2022.9800046
[145] XuanhuiXu,EleniMangina,andAbrahamGCampbell.2021.Hmd-basedvirtualandaugmentedrealityinmedical
education:Asystematicreview.FrontiersinVirtualReality2(2021),692103.
[146] YangXu,ChenguangYang,XiaofengLiu,andZhijunLi.2018. ANovelRobotTeachingSystemBasedonMixed
Reality.250–255. https://doi.org/10.1109/ICARM.2018.8610861
[147] ChungXue,YuansongQiao,andNiallMurray.2020.EnablingHuman-Robot-InteractionforRemoteRoboticOperation
viaAugmentedReality.194–196. https://doi.org/10.1109/WoWMoM49955.2020.00046
[148] ChungXueErShamaine,YuansongQiao,JohnHenry,KenMcNevin,andNiallMurray.2020.RoSTAR:ROS-based
TeleroboticControlviaAugmentedReality.1–6. https://doi.org/10.1109/MMSP48831.2020.9287100
[149] YuriDVYasuda,LuizEduardoGMartins,andFabioAMCappabianco.2020. Autonomousvisualnavigationfor
mobilerobots:Asystematicliteraturereview.ACMComputingSurveys(CSUR)53,1(2020),1–34.
[150] A.W.W.Yew,S.K.Ong,andA.Y.C.Nee.2017.ImmersiveAugmentedRealityEnvironmentfortheTeleoperationof
MaintenanceRobots.ProcediaCIRP61(2017),305–310. https://doi.org/10.1016/j.procir.2016.11.183
[151] HuitaekYunandMartinB.G.Jun.2022. Immersiveandinteractivecyber-physicalsystem(I2CPS)andvirtual
realityinterfaceforhumaninvolvedroboticmanufacturing.JournalofManufacturingSystems62(2022),234–248.
https://doi.org/10.1016/j.jmsy.2021.11.018
[152] FaisalZaman,CraigAnslow,andTaehyunJamesRhee.2023. Vicarious:Context-awareViewpointsSelectionfor
MixedRealityCollaboration.InProceedingsofthe29thACMSymposiumonVirtualRealitySoftwareandTechnology.
1–11.
[153] MohammadKassemZein,MajdAlAawar,DanielAsmar,andImadH.Elhajj.2021.Deeplearningandmixedreality
toautocompleteteleoperation.IEEE,4523–4529.
[154] JingxinZhang.2018.ExtendedAbstract:NaturalHuman-RobotInteractioninVirtualRealityTelepresenceSystems.
812–813. https://doi.org/10.1109/VR.2018.8446521
[155] JiachenZhang,OnaizahOnaizah,KevinMiddleton,LidanYou,andEricDiller.2017. Reliablegraspingofthree-
dimensionaluntetheredmobilemagneticmicrogripperforautonomouspick-and-place.IEEERoboticsandAutomation
Letters2,2(2017),835–840.
33Conferenceacronym’XX,June03–05,2018,Woodstock,NY XianWang,LuyaoShen,andLik-HangLee
[156] LijunZhao,XiaoyuLi,ZhenyeSun,KeWang,andChenguangYang.2017. Arobotnavigationmethodbasedon
human-robotinteractionfor3Denvironmentmapping.409–414. https://doi.org/10.1109/RCAR.2017.8311896
[157] TianyuZhou,QiZhu,andJingDu.2020.Intuitiverobotteleoperationforcivilengineeringoperationswithvirtual
realityanddeeplearningscenereconstruction.AdvancedEngineeringInformatics46(2020),101170.
[158] CindyZiker,BarbaraTruman,andHeatherDodds.2021.Crossreality(XR):Challengesandopportunitiesacrossthe
spectrum.InnovativelearningenvironmentsinSTEMhighereducation:Opportunities,challenges,andlookingforward
(2021),55–77.
[159] KaterynaZinchenkoandKai-TaiSong.2021.AutonomousEndoscopeRobotPositioningUsingInstrumentSegmenta-
tionWithVirtualRealityVisualization.IEEEAccess9(2021),72614–72623. https://doi.org/10.1109/ACCESS.2021.
3079427
A DATAEXTRACTIONLISTFORINCLUDEDPUBLICATION
Table3. XRTechnologiesandInteractionModalities(DE4andDE5)
Category Citations
XRtechnologies
VRHMD [21][143][94][136][124][72][110][125][138][153][122][65][157][20][88][109][91][132][22][43][66][151][142][104][24][123][38][159][26][7][90]
[114][93][86][144][58][59][129][61][119][117][156][79][146][68][111][28][133][53][16][48][131][12][134][154][83][77][6][99][84][63][36][1]
[62][40][120][85][141][73][100]
ARHMD [81][51][80][142][52][42][34][127][148][35][23][46][147][6][113][107][78][97][14][33]
MR [29][3][56][57][152]
CAVE [43][44][11]
MobieAR [19]
Other [106][96][150][82]
Interactionmodalities
Controller [21][136][72][125][65][157][132][22][44][66][123][26][114][86][144][61][52][68][131][83][84][1][62][40][85][100][151][152][36][73]
Gesture [29][80][142][91][58][59][156][146][28][148][12][23][147][57][113][78][14][34][99][152][97]
Joystick [124][138][153][51][109][3][11][127][141]
Hapticdevices [94][20][110][96][104][119][63][151][36]
Motioncapture [143][91][38][7][16]
Walk [93][42][48][35][99]
2Dscreen [82][19][46]
Voice [53][6][97]
Glove [88][111][120]
Head [117][99]
Gaze [79]
Other [106][81][122][43][24][159][129][56][133][35][77][73]
Table4. VirtualInterfaceandtheUser’sPerspective(DE6andDE7)
Category Citations
XRtechnologies
Digitaltwin [29][110][122][132][150][44][151][142][159][91][114][59][119][11][68][133][34][127][16][148][48][134][83][23][77][147][84][36][1][120][78]
[85][97]
Direct [143][138][51][51][20][88][109][91][22][3][104][38][7][129][82][117][156][42][28][19][16][12][154][46][57][152][141][14][100]
Digitaltwin+3Dreconstruction [94][106][96][43][66][24][26][93][86][58][56][131][113][34][6]
Multiple [124][157][144][35][62][73]
Direct+3Dreconstruction [21][153][123][99][63]
3Dreconstruction [136][125][79]
Virtualcontrolroom [72][65][107]
Other [81][80][52][111][146][53][40]
User’sperspective
Decouplingwithrobots [94][124][106][29][81][110][51][122][132][96][150][43][44][66][151][142][24][159][26][90][86][59][119][79][146][68][56][133][34][53][127]
[148][131][134][35][83][23][77][147][57][152][84][113][36][1][62][78][85][97][33]
Couplingwithrobots [143][72][138][153][65][20][88][109][91][80][22][3][104][123][38][7][129][82][61][117][156][42][111][28][19][16][12][154][46][99][63][107]
[141][100]
Dynamicperspective [157][144][52][11][73]
Godperspective [125][58]
Other [21][124][114][93][48][6][40][14]
34XR-basedRemoteHRI Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Table5. RobotTypesandSpecificTasksClassification(DE8andDE9)
Category Citations
Robottypes
Roboticarm [143][94][136][124][110][122][20][132][96][43][44][66][151][38][7][90][86][144][59][146][68][28][133][53][148][134][35][83][23][77]
[147][6][84][63][36][107][40][120][78][85][97]
Mobilerobot [106][29][125][138][109][24][26][58][52][156][42][34][19][131][154][57][99][113][62][141]
Drones/UAV [153][51][3][123][114][129][61][11][56][127]
Humanoidrobot [21][65][104][114][117][48][46][100]
Double-armedrobot [72][157][88][91][79][16][12]
MedicalRobotics [159][119][14]
Other [81][80][150][142][82][111][152][73]
Roboticarm+mobilerobot [22][1][33]
Specifictasks
Grabbing/Picking/Placement [143][94][136][124][29][20][91][66][151][7][86][144][59][52][79][56][16][12][77][147][152][36][107][62][78][85][100]
Navigation [21][106][138][109][3][26][93][58][82][117][11][42][19][127][48][131][154][141]
Industrial/Manufacturing [81][72][110][122][157][80][132][150][43][142][38][146][133][134][83][6][97][33]
Multiple [88][96][24][90][34][35]
No [44][129][28][148][23][84][120]
Environmentscan [125][22][156]
Surgery/Healthcare [159][119][14]
Search [61]
Game/Entertainment/Social [104][111][40]
Other [153][65][123][114][68][46][57][99][63][113][1][73]
Table6. Enhancementlocationsandtypes(DE13andDE14)
Category Citations
Enhancementlocation
VE(Virtualenvironment) [29][125][153][132][43][123][114][119][28][56][133][48][134][46][107][62][78][73][66][159][144][58][79][148][16][23][99][152][113][36][40]
[120][97][14]
User [94][20][88][96][104][38][111][34][63][141][110][151][79][23][36][40][120][97][14]
Robot(Virtual) [11][83][77][66][144][58][16][113]
Robot(Real) [81][80][51][23][99]
RE(Realenvironment) [52][19][127][57][51][152]
Object(Virtual) [124][122][44][147][110][151][159][148][16]
No [21][143][136][106][72][138][65][157][109][91][22][150][142][3][24][26][7][90][93][86][59][129][82][61][117][156][42][146][68][53][131]
[12][35][154][6][84][1][85][33][100]
Enhancementtypes
Voice [34][97]
Video [107][78][51][159][144][152][113][36][62]
Text [132][133][134][81][66][151][144][19][148][113][62]
Ray [66][79][16][23][152]
Highlight [11][147][144][58][148][16][113]
Haptic [94][20][88][96][104][38][111][63][141][110][151][34][36][40][120][14]
Graphic [122][44][123][119][46][124][153][159][148][16][57][99][152][120]
Avatar [43][114][28][56][48][73][58][79][23][57][99][152]
3DObject [29][125][80][52][127][83][77][124][81][110][153][51][58][19][23][40][120][97][14]
No [21][143][136][106][72][138][65][157][109][91][22][150][142][3][24][26][7][90][93][86][59][129][82][61][117][156][42][146][68][53][131]
[12][35][154][6][84][1][85][33][100]
Table7. Systemsevaluationmethods(DE10)
Category Citations
N/A [106][125][157][80][96][150][43][44][142][24][159][26][7][114][93][144][59][129][82][117][52][156][79][146][111][56][53][148][131][154]
[83][23][46][77][147][6][84][107]
Time/accuracyofthetask [143][94][29][81][153][51][109][38][86][119][68][127][152][63][62][120][78][85][141][73][14][136][124][72][110][138][122][65][20][88][91]
[132][123][90][133][34][19][16][48][134][35][36][33]
Questionnaire [94][29][81][153][51][109][104][38][86][61][119][68][127][57][152][63][62][120][78][85][141][73][97]
Comparison [143][151][11][28][1][120][141]
Interview [51][127][57][100]
AR/VR [22][14]
Other [22][151][3][42]
Table8. Whetherthesystemsupportsmultiplayer/multi-bot(onlyincludedsupportedcategories,DE12)
Category Citations
Multi-user-Onerobot [43]
Oneuser-Multi-robot [58][46]
Multi-user-Multi-robot [93][56][6]
One-user-One-user+Onerobot [81][80][104][79][99][152][113][40][73][14]
One-user-Multi-user+Onerobot [125]
Includedarticlesthatarenotlistedonthetablei.e.donotsupportmultiplayer/multi-botoperations.
35