Reinforcement Learning with Token-level Feedback
for Controllable Text Generation
WendiLi12,WeiWei12∗,KaiheXu3,Wenfengxie3,DangyangChen3,YuCheng4
1CognitiveComputingandIntelligentInformationProcessingLaboratory,
SchoolofComputerScienceandTechnology,HuazhongUniversityofScienceandTechnology
2JointLaboratoryofHUSTandPinganProperty&CasualtyResearch(HPL)
3PingAnProperty&CasualtyInsurancecompanyofChina
4 TheChineseUniversityofHongKong
{wendili,weiw}@hust.edu.cn
Abstract headsastheparameterscalesbecomehuge. Post-
processingmethods(Krauseetal.,2021;Yangand
Tomeettherequirementsofreal-worldappli-
Klein,2021;Liuetal.,2021)leveragesmall-scale
cations,itisessentialtocontrolgenerationsof
discriminatorstobiastokendistribution,whichof-
largelanguagemodels(LLMs). Priorresearch
has tried to introduce reinforcement learning tenleadstolowtextquality. Somemethods(Zhang
(RL) into controllable text generation while andSong,2022;Yangetal.,2023a;Huangetal.,
mostexistingmethodssufferfromoverfitting 2023) adopt parameter-efficient training strategy
issues (finetuning-based methods) or seman- e.g. prefix-tuning, but they are susceptible to un-
ticcollapse(post-processingmethods). How-
desiredattributesinthesupervisedcorpus. Recent
ever,currentRLmethodsaregenerallyguided
research(Lietal.,2022;Guetal.,2022b,2023)in-
by coarse-grained (sentence/paragraph-level)
troducesotheralgorithmbackbonese.g. diffusion
feedback, which may lead to suboptimal per-
formanceowingtosemantictwistsorprogres- models, normalized flow, but they generally cost
sionswithinsentences. Totacklethat,wepro- morecomputationalexpensesduringtrainig, and
poseanovelreinforcementlearningalgorithm havealongerinferencetime,thushardtodeploy
namedTOLEwhichformulatesTOken-LEvel inrealapplications.
rewards for controllable text generation, and
There is some research (Khalifa et al., 2021;
employsa"first-quantize-then-noise"paradigm
Lu et al., 2022) introducing reinforcement learn-
toenhancetherobustnessoftheRLalgorithm.
ing (RL) into controllable text generation (CTG)
Furthermore,TOLEcanbeflexiblyextendedto
tasks. RL paradigms can relieve the above prob-
multipleconstraintswithlittlecomputational
expense. Experimental results show that our lems,whichalleviatetheoverfittingissuebytrain-
algorithmcanachievesuperiorperformanceon ingonself-generatedsentences,andcanintegrate
both single-attribute and multi-attribute con- parameter-efficientstrategieswithcanonicalLLM
trol tasks. We have released our codes at backbones. However, RL-based methods gener-
https://github.com/WindyLee0822/CTG.
ally update language models with sentence-level
1 introduction (orparagraph-level)rewards,leadingtosuboptimal
performance and slow convergence. The coarse-
Large autoregressive language models (LLMs)
grained rewards cannot provide clear guidance,
trained on extensive corpus can generate high-
sincesemanticinthesentenceoftentransitswith
qualitytexts. However,tosatisfyreal-worldappli-
twistsorprogression. Moreover,differentpartsof
cations,makingthegenerationmorecontrollable
thesentencemaycontributetodifferentattributes.
isurgent. Itisdesiredtoenhancespecificattributes
Therefore,RLmethodswithcoarse-grainedfeed-
ofgeneratedtextsforpracticalneeds(e.g. positive
backgenerallyrequireconsiderabletrainingsteps
sentiment for psychological escort, formality for
toconverge.
academicwriting)(Beltagyetal.,2019;Guetal.,
Ourobjectiveistogranularizethecoarse-grained
2022a;Gururanganetal.,2020)andreduceintrin-
feedback to provide more precise guidance for
sic defects of pre-trained language models (e.g.
LLMs. In this paper, we propose a novel rein-
toxicity, repetition) (Rae et al., 2021; Weidinger
forcementlearningalgorithmwithTOken-LEvel
etal.,2021).
guidancenamedTOLE.Wefirstprovideanalter-
Retraining models (Chan et al., 2021; Keskar
nativeperspectiveofBayesianFactorization,which
et al., 2019) are subject to computational over-
inspiresustoformulatethetoken-levelrewardsas
∗*Correspondingauthor. the probability shifts of attribute classifiers. To
4202
raM
81
]LC.sc[
1v85511.3042:viXraenhance the robustness of TOLE, we propose an etal.,2021;Janneretal.,2021;Zhengetal.,2022;
explorationframeworkwith"Firstquantize, then Xuetal.,2023)incorporatingRLtechniquesinto
noise" procedure. Moreover, TOLE can be ex- thetransformerstructure,tryingtodeconstructthe
tendedtomulti-attributescenarioswithfewcom- coarse-grained reward into the token level for se-
putationaloverheads. Weconducttwoexperiments quentialmodeling. However, theyarehardtoex-
onsingle-attribute: sentimentcontrolanddetoxifi- tendtopracticalapplicationssincetheirspecialized
cation. WealsoevaluateTOLEonmulti-attribute token settings are not in line with current LLMs.
scenarioswithtwosettings. TOLEachievessupe- Concurrentwithourresearch,someresearch(Wu
rior performance compared with a wide range of etal.,2023;Yangetal.,2023b)onLLMalignments
baselines. triestohandletheproblemofcoarse-grainedfeed-
back. RLHF(reinforcementlearningfromhuman
2 RelatedWorks feedback)algorithmsof theLLMalignmentgen-
erally require a large-scale reward model, which
Controllable Text Generation. Most previous shouldbetrainedondatasetsformattedaspairwise
works on controllable text generation (CTG) are sentenceswiththesameprefix. However,suchdata
basedontheauto-regressiveframework,whichcan isunavailablewhenconfrontedwithawidevariety
becategorizedintoretraining(Keskaretal.,2019; of attribute requirements. Therefore, exploring a
Chanetal.,2021),finetuning(Huangetal.,2023; novelreinforcementlearningalgorithmwithtoken-
Yang et al., 2023a; Zhang and Song, 2022), and level feedback is significant for controllable text
post-processing (Krause et al., 2021; Liu et al., generation.
2021;YangandKlein,2021). Retrainingandtra-
ditional finetuning methods are of low efficiency 3 Approach
sincetheparameterscaleofLMsissurgingandthe
overfitting issue is severe. Post-processing meth- Wewillfirstestablishthenotation,providesome
odsregulatethenext-tokendistributionwithsup- backgroundonexistingRLmethodsincontrollable
plementarymodules,mostlyanattributediscrimi- textgenerationandmodelalignment,andofferan
nator,butoftencausesyntaxinterruptionandmake overviewofouralgorithm.
language models lose insights. Lu et al. (2022)
3.1 Preliminaries
integrateRLalgorithmsintoCTGbutusecoarse-
grainedfeedbacktoguidetheLLMs. Notations. AstandardMarkovDecisionProcess
Multi-aspect controllable text generation. (MDP) can be denoted as (S,A,T,r). At each
Along with single-aspect controlling, most re- step, an action a ∈ A is made based on the cur-
search on multi-aspect controllable text genera- rent state s ∈ S. Then the state will be transited
tioncanalsocategorizedintofinetuningandpost- to s′ with the possibility T(s′|s,a). A function
processing. Some post-processing research (Lin r : S × A → R defines the returned reward
and Riedl, 2021; Kumar et al., 2021) in MCTG based on the states and actions. The strategy is
combines multiple attribute discriminators to ag- decided by a policy model π(·|s), which is a pre-
gregatethecontrollability. However,theyalsoin- dicted distribution over actions based on state s.
heritdrawbacksofpost-processingmethodsdueto To transfer to text generation scenarios, the state
direct distribution regulations. Finetuning-based canbedefinedasthepartiallygeneratedsentence
researchtriestoconnectseveralsinglecontrollers, y = (y ,y ,...,y ), and the action is the
≤i−1 1 2 i−1
e.g. connectorstocombinemultipleplugins(Yang nexttokeny ∈ V wherethevocabularyV istheac-
i
etal.,2023a),latentvariablestorepresenttheunsu- tionspace. ThetransitiondynamicT(·|s,a)isde-
pervisedaspects(Qianetal.,2022),directcombi- terministicsinceeachstate-actionpair(y ,y )
≤i−1 i
nationofprompts(Huangetal.,2023),thebound- leadstoauniquestatey .
≤i
aryexplorationofintersectedsubspaces(Guetal., Prior RL-based methods. In previous RL-
2022b,2023). Tothebestofourknowledge,weare basedmethodsofcontrollabletextgeneration,re-
the first to explore how to extend single-attribute wardsarederivedfromP(c|y),whichdenotesthe
reinforcement learning algorithms to the MTCG possibilitythatthesentencey satisfytheattribute
scenario. c. P(c|y) can be obtained by corresponding at-
Token-level guidance for Reinforcement tribute classifiers. Since prior research only con-
Learning. There is a series of research (Chen centrates on sentence-level feedback, which canExploration Learning
Model
TThhee t atasstete is is g gr re ea at t, , …… dm iva en rsy e s suu ss hh i…i… , b, ub tu st e s rve ir cv ei ic se b i as d bad πθ TThhee t atasstete
πθ
Ent.
πθ
update
πref
KL-div
pos.
Quantize & Noise
1 2 3 1 2 3
noise
asian.
data point data point
weigher
quantize
lifetime-1
1 2 3
sort
data pool
add data
each data data pool
Figure1: OverallFrameworkofouralgorithm.
be regarded as r(y ,y ) = r(y ,y ) = ··· = sincepreviousy mayalreadymakefuturegen-
1 ≤0 2 ≤1 ≤i−1
r(y ,y ) = f(P(c|y)). This equality means erations satisfy c easily i.e. P(c|y ) is large.
n+1 ≤n ≤i−1
thatsentence-levelfeedbacktreatseachactiony Itrevealsthatwhatmattersistheprobabilityshift
i
intheMDPprocessofy equally, whichcanonly betweenthem,whichenlightensourrewarddesign.
provideroughguidanceformodels. Thetoken-levelrewardfunctioncanbeformu-
BayesianFactorizationinPriorresearch. The lated as the probability shift before and after the
objective of controllable text generation is to let wordisgenerated.
LLMs approach P(y|c) where c is a target at-
r(y ,y ) = f(P(c|y )−P(c|y )), (3)
i+1 ≤i ≤i+1 ≤i
tribute. Granularizetothetoken-level,priorpost-
processing methods generally factorize this term where f(·) is an activation function for normal-
bytheBayesianformulaasfollows, ization, whereweadoptthesigmoidfunctionfor
implementations. Theoretically, to approximate
P(y ≤i|c) ∝ P(c|y ≤i)P(y i|y ≤i−1). (1) P(c|y ≤i), the format of training data should be
transformed from the traditional {(y,c)|y ∈ Y}
With this formula, post-processing methods can
to {(y ,c)|0 ≤ i ≤ |y|,y ∈ Y} as in Yang and
≤i
achieve P(y|c) by regulating the token distribu-
Klein(2021). However,wefindusingtraditional
tionP(y |y )withanattributeclassifierwhich
i ≤i−1 classifiers in our algorithms can achieve on-par
approximatesP(c|y ).
≤i performanceinexperimentscomparedtospecially
3.2 Token-levelRewards trainedclassifiers. Wepresentthiscomparisonin
AppendixD.3.
We first provide an alternative perspective of
Bayesian factorization to show that the probabil- 3.3 RLAlgorithm: Firstquantize,thennoise.
ityshiftofattributeclassifiersplaysanimportant
ThetrainingprocedureofourRLalgorithmcanbe
roleincontrollingthegenerations. TheBayesian
separatedintoinitialization,exploration,quantize
factorizationcanberewrittenas:
&noise,andlearning.
P(c|y ) Initialization. First,weinitializeapolicyLLM
≤i
P(y |y ,c) ∝ P(y |y ). (2)
i ≤i−1 P(c|y ≤i−1) i ≤i−1 π θ, a copy of the policy model as the reference
model π , an attribute scorer S. The reference
ref
See more details in Appendix A. In Eq.2,
modelisfrozenduringthewholeprocess. Wealso
P(c|y ≤i) iscrucialforthenext-tokenprobability initializeadatapoolD = ∅,andprepareaprefix
P(c|y )
≤i−1
distribution. Evenify tendstohighlysatisfythe corpusforexploration.
≤i
conditioncwhensentenceisfinishedi.e. P(c|y ) Exploration. Then,giventheprefixx,thecur-
≤i
islarge,actiony maynotplayanimportantrole rent policy model can generate subsequent text
iy. For each generated token, we calculate the Learning cycle until training achieves the maxi-
score shift as its reward r(y ,y ), and add mumepisodenumber.
i+1 ≤i
(y ,y ,r) to the data pool D. To avoid over-
i+1 ≤i
3.4 ExtensiontoMultipleAttributes.
training on data explored in earlier episodes, we
setalifetimeforeachdatatoindicatetheepisodes To consider multiple constraints simultaneously,
it can still undergo. Once the data is added to D, we should combine multiple reward groups from
thelifetimeisinitializedtoLandsubtracts1after differentscorers. Simpleaggregationsoraverages
eachtrainingepisode. ThedataisremovedfromD cannot provide appropriate token-level guidance,
whenitslifetimedropsto0. sincescorersmaycontradicteachother. Moreover,
Quantize&NoiseLearningprimitiverewards differentpartsofsentencesmayaddressdifferent
r can predispose the model to flatter the scoring attributes,soweneedtoweighthetoken’scontri-
pattern of attribute classifiers, which may cause butiontomultipleattributesrespectively. Totackle
diversity decrease. Therefore, we propose "First this,wetrainasmall-scale"weigher"W : Rd →
ϕ
quantize,thennoise"toavoidthisproblem. First, Rn tobalancerewardsfromnscorers,wheredis
wequantizetherewardswithinD, andacquireq- thehiddensizeofLLMs. Giventhelast-layerhid-
quantiles, which divide the reward range into q denstatesH ∈ R1×d ofy outputbyLLMs
t+1 t+1
intervals. Then, we inject noise into each reward π(y ), the weigher output W = W (H )
≤t+1 ϕ t+1
while ensuring each reward stays in the original as the weight for n rewards of y , R ∈ R1×n.
t t+1
interval. Specifically, for a reward r ∈ (q i,q i+1), The weigher does not require a complex model
wereassignitas structure. Simplestructurescanalreadyassistour
algorithmtoachievegreatperformance. Henceit
rˆ= q +(q −q )ϵ(r−q ) (4)
i i+1 i i doesnottakesignificantcomputationaloverheads.
Inourimplementation,theweigherconsistsoftwo
whereϵ(·)isanoiseprocessedwithaclipfunction
linear layers with the ReLU function and a out-
to satisfy ϵ(r) ∈ (r − 1,r + 1). ϵ(r) is substi-
put layer with a softmax function. The compre-
tutedwithGaussiannoiseinourimplementations.
hensiverewardofactiony canbeobtainedby
Throughthisprocess,wedisrupttherewardorder t+1
r = W×RT .
tointerferethefixedscoringpatternsofclassifiers, t+1
Totraintheweigher,weformulatetheoptimiza-
whilemaintainingtherelativeorderbetweeninter-
tionproblemasmaximizingtheintegratedreward
valstosteerLLMstowardthetargetattribute.
ofatrainingcorpusy ∼ Y thatsatisfiesthemulti-
Learning. Through above procedures, we can
pleattributes,
obtain rˆ to provide dense guidance on each to-
kenwithoutgranularitymismatchorfeedbackde-
maxE E W (H )×R (6)
lay. The minimalist objective of our optimiza- y∼Y t ϕ t+1 t+1
ϕ
tion problem is to maximize the total rewards,
max E [rˆ(y ,y )]. We relax the wheret ∼ Uniform(0,|y|−1),auniformdistribu-
θ yt+1∼π θ(·|y ≤t) t+1 ≤t
convergence by adding a standard max-entropy tion among {0,1,...,|y|−1}. By doing so, the
gradient, which can help capture diverse behav- weigherlearnswhichscorershouldbepaidmore
iormodes. WealsoinsertaKL-divergencepenalty attentionwhenconsideringdifferenttokenswithin
tokeepthemodelπ fromdeviatingtoofarfrom sentences.
θ
theoriginalπ . Thegradientofeachsentencey
ref
4 Experiments
canbeformulatedasfollows,
(cid:104) 4.1 SentimentControl
E rˆ(y ,y )∇ logπ (y |y )
yt+1∼π θ(·|y ≤t) t+1 ≤t θ θ t+1 ≤t
ExperimentalSettings. Followingpreviousworks,
(cid:105)
+α∇ θH(·|y ≤t)+β∇ θKL(y ≤t) weuse10Knaturallyoccurringpromptsfromthe
(5) OpenWebText Corpus, which is divided into 5K
where α,β are two balancing coefficient, H is “neutral” prompts, 2.5K “negative” prompts, and
the Shannon entropy of π (·|y ), KL(y ) is 2.5K“positive”prompts. Thesentimentpolarityof
θ ≤t ≤t
the KL divergence between π (y |y ) and promptsisdeterminedbythecategoryoftheirgen-
θ t+1 ≤t
π (y |y ). erationsofGPT2-base. WeuseGPT2-largeasthe
ref t+1 ≤t
Wethenusetheupdatedmodelforexploration basePLM,andadoptprompttechniquesratherthan
and repeat the Exploration-Quantize & Noise- tuning the whole model. The sentiment scorer isAttributeCorrectness(↑) GenerationMetrics TrainingInfo.
Category Model
Target:POSITIVE Target:NEGATIVE
PPL(↓) dist-3(↑) CR.(↓) %Params
negative neutral positive neutral
PPLM 8.72 52.68 10.26 60.95 122.41 0.90 3.47 0.001
Post-
GEDI 26.80 86.01 60.43 91.27 138.27 0.86 3.62 -
processing
FUDGE 56.04 96.92 66.84 98.76 265.79 0.83 1.53 -
Fine- PROMPT 40.88 78.08 49.28 73.20 39.55 0.73 63.08 0.003
Tuning DISCUP 49.92 91.58 60.80 90.64 40.46 0.75 3.72 0.003
PPO 43.13 94.10 68.12 94.95 18.34 0.71 2.95 100
Reinforcement
QUARK 47.32 95.50 70.50 96.65 16.92 0.75 2.63 100
Learning
TOLE 69.36 97.16 72.81 98.02 17.05 0.75 2.61 0.003
Table 1: Automatic evaluation results of the sentiment control task. "Params" indicates the ratio of trainable
parameterstothewholeLLM.Boldfaceandunderlineindicatethebesttworesults.
basedonGPT2-base,whichistrainedonSST-5fol- fittingthetrainingcorpuswithhighcoveragerates.
lowingZhangandSong. PPL,Dist-nareadopted DisCupborrowsRLparadigmsbyexploringcan-
tomeasurethefluencyanddiversityofgeneration. didatetokenstoalleviatetheoverfittingproblem,
Correctness is the proportion of generations that alleviating the overfitting issue. RL-based meth-
satisfy target sentiment. We use a Huggingface odsgetthebestperformanceamongallbaselines.
sentimentclassifier1 todiscriminatecategoriesof Theycangeneratethemostfluentsentenceswith
generations. See more details in Appendix B.1. littlediversitysacrifice,whileoptimallyfulfilling
Wealsoconducthumanevaluationsbasedonthe the target attributes. Since prior RL-based meth-
perceivedlevelofsentimentcorrectness,topicality, odsonlyadoptsentence-levelfeedback, theycan
and fluency. Details of human evaluation can be onlyachievesuboptimalperformanceevenwithall
foundinAppendixC. parameters of LLMs to be updated. Our method
Baselines. A wide range of competitive base- guidesLLMswithfiner-grainedfeedback,thusat-
lines are compared with our algorithm. We com- tainingbetterperformancewithasubstantialreduc-
pareourmethodstopost-processingmethodsasfol- tion of computational expenses, since it requires
lows: PPLM(Dathathrietal.,2020),GEDI(Krause fewerparametersandtrainingsteps(§4.4).
etal.,2021),andFUDGE(YangandKlein,2021).
4.2 Detoxification
We also choose several competitive finetuning-
based methods as our baselines: Prompt-tuning Experimental Settings. Toxic degeneration is
(Li and Liang, 2021), DisCup (Zhang and Song, an inherent problem of LLMs, since LLMs may
2022). TocomparewithRL-basedmethods,weim- express harmful or offensive utterances. We
plementPPO(Schulmanetal.,2017)andQUARK traintheclassifieronToxicityClassificationKag-
(Lu et al., 2022). See more details in Appendix gle challenge2, which includes 160K toxic com-
B.1. ments and 1.4M nontoxic comments. We use
Results and Analysis. The automatic evalua- REALTOXICITYPROMPTS (Gehmanetal.,2020)
tion results are shown in Table 1. Though post- datasetasourexperimentalcorpuswhichconsists
processing can make generated sentences satisfy of100kpromptsdesignedtoelicittoxicity. Weuse
the target sentiment with the least parameters to the10Knon-toxictestpromptsfollowingLiuetal.
train, even in a zero-shot way by decoding-time (2021),andtakeotherpromptsastheexploration
regulationwithattributediscriminators,theygener- prefixes. We use the same LSTM-based prompt
allygethighPPLscorers,whichmeansthequality techniquesonGPT2-large. Additionally,wealso
ofgeneratedtextsispoor. Fine-tuningmethodscan conductout-of-domainevaluationwiththe WRIT-
maintaintextfluencywhilegettingconsiderableac- INGPROMPTS dataset(Fanetal.,2018),whichis
curacyoftargetattributes,buttheysufferfromover- createdforcreativewriting. Weevaluatethedetox-
ificationabilitybytheaveragemaximumtoxicity
1https://huggingface.co/
distilbert-base-uncased-finetuned-sst-2-english 2https://bit.ly/3cvG5pyIn-domain REALTOXICITYPROMPTS Out-of-domain WRITINGPROMPTS
Model Toxicity(↓) Generation Toxicity(↓) Generation
avg.max. prob. PPL↓ dist-3↑ avg.max. prob. PPL↓ dist-3↑
GPT2 0.527 0.520 11.31 0.85 0.572 0.610 12.99 0.85
PPLM 0.520 0.518 32.58 0.86 0.544 0.590 36.20 0.86
GeDi 0.363 0.217 60.03 0.83 0.261 0.050 91.16 0.82
DExpert 0.314 0.128 32.41 0.84 0.343 0.156 42.53 0.85
Prompt 0.302 0.360 29.21 0.74 0.442 0.363 30.10 0.79
Discup 0.298 0.115 39.30 0.84 0.442 0.363 37.23 0.85
PPO 0.288 0.130 18.22 0.82 0.291 0.132 18.32 0.84
Quark 0.237 0.118 17.23 0.81 0.268 0.102 17.19 0.83
TOLE 0.206 0.105 15.45 0.80 0.223 0.080 16.51 0.83
Table2: Automaticevaluationresultsofunlearningtoxicityexperiments. Boldfaceandunderlineindicatethebest
tworesults.
over25textgenerations,andtheprobabilityofat Yelp (Lample et al., 2019) benchmark, contain-
least one of any 25 generations being toxic. The ingrestaurantreviewswiththesentiment(positive
toxicityisjudgedbyPerspectiveAPI.Wealsoeval- andnegative)andthesubject(American,Mexican,
uatethetextqualitybyPPLanddist-n. Seemore and Asian) labels. To measure whether the sen-
details in B.2. We also conduct human evalua- tence satisfies given attributes, we finetuned two
tionsoncontrolaccuracy,fluency,andoveralltext RoBERTa-based (Liu et al., 2019) classifiers for
quality. Theevaluationsettingsandresultsarein the evaluations of sentiment and subject with its
AppendixC. original setting. Following (Huang et al., 2023),
Baselines. As sentiment control tasks, we weaddanotherconstraint,tense(pastandpresent)
compareourmethodstopost-processingmethods, (FiclerandGoldberg,2017)wheretheirlabelsare
finetuning-basedmethods,andRL-basedmethods. automaticallyextractedfromthereviewswithan
Post-processing methods are as follows: PPLM open-source toolkit3. Perplexity (PPL) and aver-
(Dathathrietal.,2020),GEDI (Krauseetal.,2021), aged distinctness (Li et al., 2016) are reported to
DExpert (Liu et al., 2021),. We choose DisCup demonstratethefluencyanddiversityofthegener-
(Zhang and Song, 2022) to represent finetuning- atedtext. Wealsoconducthumanevaluationson
basedmethods. WeimplementRL-basedmethods: generatedresults. Duetopagelimit,seeAppendix
PPO(Schulmanetal.,2017)andQUARK(Luetal., B.2formoredetails.
2022). SeemoredetailsinAppendixB.1. Baselines. Researchonmulti-attributeCTGis
Results and Analysis. Post-processing meth- notasabundantassingle-attributeCTG.Weextend
ods get the highest PPL score, which means gen- GEDI (Krauseetal.,2021),whichadoptsasmall-
erated sentences are disfluent though have high scaleconditionalgenerativediscriminatortobias
diversity. Finetuning-basedmethodshaveordinary thetokendistribution,tomulti-attributecontrolling
performancessincefine-tuningmodelsonspecific accordingtoHuangetal.(2023). Wealsoinclude
corpus is easily overfitted to undesired attributes. DIST. LENS (Khalifa et al., 2021), which intro-
RL-based methods generally achieve the lowest ducesanautoencodertomapconstraintstolatent
toxicityonbothtoxicity metrics. OurTOLEout- subspaces,andexploretheintersectionofmultiple
performsotherRL-basedmethodssincethealgo- constraints. TAILOR (Yang et al., 2023a) which
rithm provides dense signals about which part of proposesaconnectortocombineseveralprompts.
sentencescontributemoretothenon-toxicity. Meanwhile,itmodifiestheattentionmaskandposi-
tionindexestonarrowthegapbetweentrainingand
4.3 MultipleAttributeControlling inference. PROMPT-GATING(Huangetal.,2023):
itgatesthepromptsbeforeappendedintotheLLMs
ExperimentalSettings. Weconductexperiments
on a double-attribute control task and a triple-
3https://github.com/ajitrajasekharan/simple_
attribute control task. We adopt the widely-used tense_detectorDoubleControls TripleControls
Model Sent.(↑) Top.(↑) Ave.(↑) PPL(↓) Dist.(↑) Sent.(↑) Top.(↑) Tense.(↑) PPL(↓) Dist.(↑)
GEDI 99.47 51.36 75.41 616.92 0.75 - - - - -
DIST. LENS 77.47 66.98 72.22 52.59 0.26 65.31 55.84 54.25 63.13 0.40
TAILOR 80.68 68.72 74.70 40.29 0.39 68.08 58.67 33.38 42.89 0.42
P-GATING 84.80 75.02 79.91 21.77 0.42 76.93 62.73 62.24 21.87 0.45
TOLE 91.27 86.32 88.80 38.62 0.52 86.31 92.68 89.50 40.75 0.51
-weigher 93.68 78.72 74.70 39.13 0.51 85.10 84.72 70.82 39.08 0.51
Table3: Automaticevaluationresultsofthemulti-attributecontroltask. Boldfaceandunderlineindicatethebest
tworesults.
tomitigatethemutualinterference. Wealsoimple-
1.0
mentsentence-levelRLmethods,PPO(Schulman none
gauss.
et al., 2017) and Quark (Lu et al., 2022), whose 0.8
sen.
rewardsarethesumofsingle-attributerewards. We
0.6
alsoconducthumanevaluations. SeeAppendixC
formoredetails. 0.4
0 4 8 12 16 20
Results and Analysis. The results are shown x1000 step
in Table 3. The post-processing method, GEDI,
though getscompetitiveresults onattributeaccu- 0.83
racy,thedeteriorationoftextqualitycausedbydi-
0.78
rectdecoding-timeregulationismoreseverethanin none
single-attributegeneration,indicatedbythehighest 0.73 gauss.
sen.
PPL score. DIST. LENS though achieves consid-
0.68
0 4 8 12 16 20
erableresults,itrequiresoversixtimesinference x1000 step
timetodeterminetheintersectionboundaryofat-
Figure2:Performanceofsentimentcontrolwithrespect
tributesubspaces. Prompt-basedmethods TAILOR
to training steps. "none" denotes the variance of no
andPROMPT-GATINGachievesuboptimalperfor-
"quantize"nor"noise". "gauss."denotesthestandard
manceonbothdouble-andtriple-attributescenar-
TOLEwithguassiannoise. "sent."denotesthevariance
ios. However, since they are easily overfitted to withsentence-levelfeedback.
undesirableattributesinthetrainingcorpuswhich
maycontradictothertargetattributes,theirperfor-
manceislimited. Withmorefine-grainedguidance
procedurecanpromotethegeneralizationofmod-
onsampledsentences,ourmethodcanachievethe
els,thoughinitiallyconvergeslower. Moreover,the
bestcontrolaccuracyinbothsettingswithoutsig-
noisingprocedurecanpreventmodelsfromflatter-
nificantinferenceexpenses.
ingthescorers,thusachievinghighertextdiversity.
Wealsoimplementanothervariancethatnoisethe
4.4 FurtherStudies
rewardwithoutquantizationboundaries. Asshown
inFigure3,wecanseethatquantizationenhances
What effect do "Quantization" and "Noise"
the stability of algorithms. The model can learn
have respectively? To visualize the difference
fromtherelativeorderofdatasets,evenwithabig
made by "First quantize, then noise", we imple-
standarddeviationofGaussiannoise. Ifweablate
menttwovariationsofouralgorithm,andconduct
thequantizationprocedure,thealgorithmwillbe
experimentsonsentimentcontroltasks. First,we
sensitivetotheamplitudeofnoise.
directly use the scores output by classifiers as re-
wards without any interference. We display the What if we ablate the "weigher" from the
performance transition over the training steps of multi-attributecombination,butadoptaverages
sentiment control tasks as in Figure 2. The fig- asoverallrewards? Weimplementamodelvari-
uredemonstratesthatthecontrolaccuracyandthe ation that combines several scorers by averaging
text diversity both decrease. Our algorithm can their output scores. Table 3 shows that ablating
achievehigherattributeaccuracysincethenoising "weigher"leadstoaperformancedecrease. Tofur-
cca
3-tsid97 0.76 0.20 0.20
0.72
93 0.15 0.15
0.68
89 0.10 0.10
0.64
85 0.60 0.05 0.05 0.1 0.3 0.5 0.7 0.1 0.3 0.5 0.7
30 var 20 var
0.00 0.00
27 17
0.05 0.05
24 14
21 11 0.10 0.10
18 8
0.1 0.3 0.5 0.7 0.1 0.3 0.5 0.7 0.15 0.15
var var
Figure3: Theperformancecomparisonbetweenmodel
0.20 0.20
varianceswithorwithoutquantizationprocedure. The length length
abovetwosubgraphsarefromneutral-to-positiveexper-
Figure4: Finalscoresofgeneratedsamplesinexplo-
iments. Thebelowarefromdetoxification.
rations. Theleftistheaverageoftwoclassifiers. The
rightisaggregatedby"weigher".
therprovethat"weigher"canprovidemoreclear
guidancewithnocontradictionbetweendifferent
scorers, we display the scores by averaging and 0,0.05,0.1,0.15,0.2. Experimental results indi-
aggregatingby"weigher"respectivelyinFigure4. catethathigherαcanincreasetextfluency,butsac-
Theleftsubgraphconcentratewithinsmallvalues rificecontrollabilityslightly,sincehigherαmore
duetothescorercontradictionwithout"weigher". tightlyconstrainthemodelnottodeviatetoomuch.
On the contrary, the right heatmap shows more Ourexperimentsalsodemonstratethattheentropy
distinctguidanceformodels. termhasarelativelyslighteffectonperformance,
not as much as KL-divergence. As β increases,
Convergence speed compared to sentence-
attributeaccuracyandtextdiversityhaveaslight
levelfeedback. Token-levelfeedbackcanprovide
increase. SeemoredetailsinAppendixD.2.
denseandprecisesignalsformodels,thusrequiring
fewerlearningstepstoachieveidealperformance. Discussionaboutrewardhacking. Thoughour
WeimplementavarianceofTOLEwithsentence- algorithm achieves great results in the above ex-
levelguidancewiththesamequantization&noise periments,weareconcernedthatrewardhacking
process. We display the performance transition occursinsomescenarioswhenscorersaretoosim-
overtrainingstepsinFigure2. Thefigureshows ple for LLMs to find unintended shortcuts. One
that the sentence-level feedback slows down the solutiontorewardhackingistocomplicatereward
convergencesignificantly,comparedtothetoken- design, which is easy to implement in our algo-
levelfeedback. rithmsbyaddingnewconstraintswithweighers.
What effect does the number of quantiles
have? q of q-quantile does not have a signif-
5 Conclusion
icant effect on final performance. However, the
convergence of the process is slightly slower if q
is relatively large or small. When q is small, rel- Tosummarize,weproposeanextensiblereinforce-
ative orders between quantiles are more ambigu- mentlearningalgorithmforcontrollabletextgen-
ous. Alargeq confinesnoisewithinasmallinter- erationwithtoken-levelfeedback. Weprovidean
val, diminishing noise impact, which results in a alternativeperspectiveofBayesianFactorization,
lowergeneralization. Amoderateq-valueallows which enlightens our token-level reward design.
the model to reach the desired result faster. See We also introduce "Quantization & Noise" into
moredetailsinAppendixD.1. RLtoenhancethealgorithmrobustness. Wealso
What effect does the number of α,β proposeasmall-scalemodule"weigher"toextend
have? α,β are two hyper-coefficients of KL- our algorithm to multiple constraints. Extensive
divergence and entropy term Eq.5 respectively. experimentsdemonstratestheeffectivenessofour
We conduct experiments with varying α,β of algorithm.
cca
.xot
3-tsid
.borp
ecnetnes ecnetnesLimitations proach for controlled text generation. In 9th Inter-
national Conference on Learning Representations,
First,ouralgorithmcannotachieve100%accura- ICLR2021, VirtualEvent, Austria, May3-7, 2021.
ciesinthevastmajorityofaspects(e.g.,sentiment OpenReview.net.
ortopic),whichmaybenotacceptableinscenar-
LiliChen,KevinLu,AravindRajeswaran,KiminLee,
ioswithrequirementsof100%controlfulfillment. AdityaGrover, MichaelLaskin, PieterAbbeel, Ar-
Second,althoughextensiveexperimentshavebeen avindSrinivas,andIgorMordatch.2021. Decision
conductedtodemonstratetheeffectivenessofour transformer: Reinforcement learning via sequence
modeling. InAdvancesinNeuralInformationPro-
algorithm,applyingittomoreLLMstructurescan
cessingSystems34: AnnualConferenceonNeural
verifythegeneralizabilityof TOLE. Third,ourap- InformationProcessingSystems2021,NeurIPS2021,
proach is limited in the attribute control task so December6-14,2021,virtual,pages15084–15097.
far, and may be hard to apply our algorithm to
SumanthDathathri,AndreaMadotto,JaniceLan,Jane
otherscenariose.g.,lexicalconstraint,table-to-text.
Hung,EricFrank,PieroMolino,JasonYosinski,and
However,mostcurrentresearchonCTGgenerally RosanneLiu.2020. Plugandplaylanguagemodels:
focusesonattributecontroltasks,andsharesthis Asimpleapproachtocontrolledtextgeneration. In
8thInternationalConferenceonLearningRepresen-
limitation, which is an open problem that should
tations, ICLR 2020, Addis Ababa, Ethiopia, April
beexploredinfutureworks.
26-30,2020.OpenReview.net.
EthicsStatement AngelaFan,MikeLewis,andYannN.Dauphin.2018.
Hierarchicalneuralstorygeneration. InProceedings
Since the large language models (LLMs) are of the 56th Annual Meeting of the Association for
trained on data collected from the web and often Computational Linguistics, ACL 2018, Melbourne,
Australia,July15-20,2018,Volume1: LongPapers,
notthoroughlycleaned,theycangenerateoffensive
pages889–898.AssociationforComputationalLin-
ortoxictext. Wemuststatethatthetextsgenerated
guistics.
byourapproachdonotrepresentouropinion. How-
Jessica Ficler and Yoav Goldberg. 2017. Controlling
ever,ourexperimentsshowthatouralgorithmscan
linguisticstyleaspectsinneurallanguagegeneration.
handlethedetoxificationtaskswhichcanalleviate
CoRR,abs/1707.02633.
thetoxicdegenerationproblemsofLLMs. More-
Samuel Gehman, Suchin Gururangan, Maarten Sap,
over,theextensibilityofourmodelcanextendthe
Yejin Choi, and Noah A. Smith. 2020. Realtoxic-
detoxificationtaskstoallcontrolrequirementsby
ityprompts: Evaluatingneuraltoxicdegenerationin
takingitasanadditionalconstraint. languagemodels. InFindingsoftheAssociationfor
Computational Linguistics: EMNLP 2020, Online
Acknowledgement Event,16-20November2020,volumeEMNLP2020
ofFindingsofACL,pages3356–3369.Association
This work was supported in part by the National forComputationalLinguistics.
NaturalScienceFoundationofChinaunderGrant
YuGu,RobertTinn,HaoCheng,MichaelLucas,Naoto
No. 62276110, No. 62172039 and in part by
Usuyama,XiaodongLiu,TristanNaumann,Jianfeng
thefundofJointLaboratoryofHUSTandPingan Gao, and Hoifung Poon. 2022a. Domain-specific
Property&CasualtyResearch(HPL).Theauthors language model pretraining for biomedical natural
wouldalsoliketothanktheanonymousreviewers language processing. ACM Trans. Comput. Heal.,
3(1):2:1–2:23.
fortheircommentsonimprovingthequalityofthis
paper. YuxuanGu,XiaochengFeng,SichengMa,Lingyuan
Zhang,HengGong,andBingQin.2022b. Adistri-
butionallensformulti-aspectcontrollabletextgen-
References eration. InProceedingsofthe2022Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib- EMNLP2022,AbuDhabi,UnitedArabEmirates,De-
ert: Apretrainedlanguagemodelforscientifictext. cember7-11,2022,pages1023–1043.Association
In Proceedings of the 2019 Conference on Empiri- forComputationalLinguistics.
cal Methods in Natural Language Processing and
the 9th International Joint Conference on Natural YuxuanGu,XiaochengFeng,SichengMa,Lingyuan
LanguageProcessing,EMNLP-IJCNLP2019,Hong Zhang,HengGong,WeihongZhong,andBingQin.
Kong,China,November3-7,2019,pages3613–3618. 2023. Controllable text generation via probability
AssociationforComputationalLinguistics. density estimation in the latent space. In Proceed-
ingsofthe61stAnnualMeetingoftheAssociation
AlvinChan,Yew-SoonOng,BillPung,AstonZhang, forComputationalLinguistics(Volume1: LongPa-
and Jie Fu. 2021. Cocon: A self-supervised ap- pers),ACL2023,Toronto,Canada,July9-14,2023,pages12590–12616.AssociationforComputational JiweiLi,MichelGalley,ChrisBrockett,JianfengGao,
Linguistics. and Bill Dolan. 2016. A diversity-promoting ob-
jectivefunctionforneuralconversationmodels. In
Suchin Gururangan, Ana Marasovic, Swabha NAACLHLT2016,The2016ConferenceoftheNorth
Swayamdipta,KyleLo,IzBeltagy,DougDowney, AmericanChapteroftheAssociationforComputa-
and Noah A. Smith. 2020. Don’t stop pretraining: tionalLinguistics: HumanLanguageTechnologies,
Adapt language models to domains and tasks. In SanDiegoCalifornia,USA,June12-17,2016,pages
Proceedings of the 58th Annual Meeting of the 110–119. The Association for Computational Lin-
Association for Computational Linguistics, ACL guistics.
2020, Online, July 5-10, 2020, pages 8342–8360.
AssociationforComputationalLinguistics. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy
Liang,andTatsunoriB.Hashimoto.2022. Diffusion-
XuanchengHuang,ZijunLiu,PengLi,TaoLi,Maosong lm improves controllable text generation. In
Sun, andYangLiu.2023. Anextensibleplug-and- NeurIPS.
playmethodformulti-aspectcontrollabletextgener-
ation. InProceedingsofthe61stAnnualMeetingof Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
theAssociationforComputationalLinguistics(Vol- Optimizing continuous prompts for generation. In
ume1: LongPapers),ACL2023,Toronto,Canada, Proceedingsofthe59thAnnualMeetingoftheAsso-
July9-14,2023,pages15233–15256.Associationfor ciationforComputationalLinguisticsandthe11th
ComputationalLinguistics. InternationalJointConferenceonNaturalLanguage
Processing, ACL/IJCNLP 2021, (Volume 1: Long
MichaelJanner, QiyangLi, andSergeyLevine.2021.
Papers),VirtualEvent,August1-6,2021,pages4582–
Offlinereinforcementlearningasonebigsequence
4597.AssociationforComputationalLinguistics.
modeling problem. In Advances in Neural Infor-
mationProcessingSystems34: AnnualConference
ZhiyuLinandMarkO.Riedl.2021. Plug-and-blend:
on Neural Information Processing Systems 2021,
A framework for plug-and-play controllable story
NeurIPS2021,December6-14,2021,virtual,pages
generationwithsketches. InProceedingsoftheSev-
1273–1286.
enteenthAAAIConferenceonArtificialIntelligence
andInteractiveDigitalEntertainment,AIIDE2021,
NitishShirishKeskar,BryanMcCann,LavR.Varshney,
virtual, October 11-15, 2021, pages 58–65. AAAI
CaimingXiong,andRichardSocher.2019. CTRL:
Press.
Aconditionaltransformerlanguagemodelforcon-
trollablegeneration. CoRR,abs/1909.05858.
Alisa Liu, Maarten Sap, Ximing Lu, Swabha
Swayamdipta,ChandraBhagavatula,NoahA.Smith,
MuhammadKhalifa,HadyElsahar,andMarcDymet-
andYejinChoi.2021. Dexperts: Decoding-timecon-
man.2021. Adistributionalapproachtocontrolled
trolledtextgenerationwithexpertsandanti-experts.
textgeneration. In9thInternationalConferenceon
In Proceedings of the 59th Annual Meeting of the
LearningRepresentations,ICLR2021,VirtualEvent,
Association for Computational Linguistics and the
Austria,May3-7,2021.OpenReview.net.
11thInternationalJointConferenceonNaturalLan-
guage Processing, ACL/IJCNLP 2021, (Volume 1:
BenKrause,AkhileshDeepakGotmare,BryanMcCann,
NitishShirishKeskar,ShafiqR.Joty,RichardSocher,
LongPapers),VirtualEvent,August1-6,2021,pages
and Nazneen Fatema Rajani. 2021. Gedi: Genera- 6691–6706.AssociationforComputationalLinguis-
tive discriminator guided sequence generation. In tics.
FindingsoftheAssociationforComputationalLin-
guistics: EMNLP2021,VirtualEvent/PuntaCana, YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
DominicanRepublic,16-20November,2021,pages dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
4929–4952.AssociationforComputationalLinguis- Luke Zettlemoyer, and Veselin Stoyanov. 2019.
tics. Roberta: A robustly optimized BERT pretraining
approach. CoRR,abs/1907.11692.
SachinKumar,EricMalmi,AliakseiSeveryn,andYu-
lia Tsvetkov. 2021. Controlled text generation as Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang,
continuous optimization with multiple constraints. Lianhui Qin, Peter West, Prithviraj Ammanabrolu,
InAdvancesinNeuralInformationProcessingSys- and Yejin Choi. 2022. QUARK: controllable text
tems34: AnnualConferenceonNeuralInformation generationwithreinforcedunlearning. InNeurIPS.
ProcessingSystems2021,NeurIPS2021,December
6-14,2021,virtual,pages14542–14554. JingQian,LiDong,YelongShen,FuruWei,andWeizhu
Chen.2022. Controllablenaturallanguagegenera-
Guillaume Lample, Sandeep Subramanian, tionwithcontrastiveprefixes. InFindingsoftheAs-
EricMichaelSmith,LudovicDenoyer,Marc’Aurelio sociationforComputationalLinguistics: ACL2022,
Ranzato, and Y-Lan Boureau. 2019. Multiple- Dublin,Ireland,May22-27,2022,pages2912–2924.
attribute text rewriting. In 7th International AssociationforComputationalLinguistics.
Conference on Learning Representations, ICLR
2019, New Orleans, LA, USA, May 6-9, 2019. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
OpenReview.net. Millican,JordanHoffmann,H.FrancisSong,JohnAslanides, Sarah Henderson, Roman Ring, Susan- 2023a. Tailor: A soft-prompt-based approach to
nah Young, Eliza Rutherford, Tom Hennigan, Ja- attribute-based controlled text generation. In Pro-
cobMenick,AlbinCassirer,RichardPowell,George ceedings of the 61st Annual Meeting of the Asso-
van den Driessche, Lisa Anne Hendricks, Mari- ciation for Computational Linguistics (Volume 1:
beth Rauh, Po-Sen Huang, Amelia Glaese, Jo- LongPapers),ACL2023,Toronto,Canada,July9-14,
hannes Welbl, Sumanth Dathathri, Saffron Huang, 2023,pages410–427.AssociationforComputational
JonathanUesato,JohnMellor,IrinaHiggins,Antonia Linguistics.
Creswell,NatMcAleese,AmyWu,ErichElsen,Sid-
dhantM.Jayakumar,ElenaBuchatskaya,DavidBud- Shentao Yang, Shujian Zhang, Congying Xia, Yihao
den,EsmeSutherland,KarenSimonyan,MichelaPa- Feng,CaimingXiong,andMingyuanZhou.2023b.
ganini,LaurentSifre,LenaMartens,XiangLorraine Preference-grounded token-level guidance for lan-
Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena guagemodelfine-tuning. CoRR,abs/2306.00398.
Gribovskaya,DomenicDonato,AngelikiLazaridou,
HanqingZhangandDaweiSong.2022. Discup: Dis-
ArthurMensch,Jean-BaptisteLespiau,MariaTsim-
criminatorcooperativeunlikelihoodprompt-tuning
poukelli,NikolaiGrigorev,DougFritz,ThibaultSot-
forcontrollabletextgeneration. InProceedingsof
tiaux,MantasPajarskas,TobyPohlen,ZhitaoGong,
the2022ConferenceonEmpiricalMethodsinNatu-
DanielToyama,CypriendeMassond’Autume,Yujia
ralLanguageProcessing,EMNLP2022,AbuDhabi,
Li,TayfunTerzi,VladimirMikulik,IgorBabuschkin,
UnitedArabEmirates,December7-11,2022,pages
AidanClark,DiegodeLasCasas,AureliaGuy,Chris
3392–3406.AssociationforComputationalLinguis-
Jones,JamesBradbury,MatthewJ.Johnson,BlakeA.
tics.
Hechtman,LauraWeidinger,IasonGabriel,William
Isaac, Edward Lockhart, Simon Osindero, Laura
QinqingZheng,AmyZhang,andAdityaGrover.2022.
Rimell,ChrisDyer,OriolVinyals,KareemAyoub,
Onlinedecisiontransformer. InInternationalCon-
JeffStanway,LorrayneBennett,DemisHassabis,Ko-
ference on Machine Learning, ICML 2022, 17-23
rayKavukcuoglu,andGeoffreyIrving.2021. Scaling
July2022,Baltimore,Maryland,USA,volume162of
languagemodels: Methods,analysis&insightsfrom
ProceedingsofMachineLearningResearch,pages
traininggopher. CoRR,abs/2112.11446.
27042–27059.PMLR.
JohnSchulman,FilipWolski,PrafullaDhariwal,Alec
Radford,andOlegKlimov.2017. Proximalpolicy
optimizationalgorithms. CoRR,abs/1707.06347.
LauraWeidinger,JohnMellor,MaribethRauh,Conor
Griffin, Jonathan Uesato, Po-Sen Huang, Myra
Cheng,MiaGlaese,BorjaBalle,AtoosaKasirzadeh,
Zac Kenton, Sasha Brown, Will Hawkins, Tom
Stepleton, Courtney Biles, Abeba Birhane, Julia
Haas,LauraRimell,LisaAnneHendricks,William
Isaac, Sean Legassick, Geoffrey Irving, and Iason
Gabriel.2021. Ethicalandsocialrisksofharmfrom
languagemodels. CoRR,abs/2112.04359.
ZeqiuWu,YushiHu,WeijiaShi,NouhaDziri,Alane
Suhr,PrithvirajAmmanabrolu,NoahA.Smith,Mari
Ostendorf, and Hannaneh Hajishirzi. 2023. Fine-
grainedhumanfeedbackgivesbetterrewardsforlan-
guagemodeltraining. CoRR,abs/2306.01693.
Mengdi Xu, Yuchen Lu, Yikang Shen, Shun Zhang,
DingZhao,andChuangGan.2023. Hyper-decision
transformerforefficientonlinepolicyadaptation. In
TheEleventhInternationalConferenceonLearning
Representations,ICLR2023,Kigali,Rwanda,May
1-5,2023.OpenReview.net.
KevinYangandDanKlein.2021. FUDGE:controlled
text generation with future discriminators. In Pro-
ceedingsofthe2021ConferenceoftheNorthAmer-
icanChapteroftheAssociationforComputational
Linguistics:HumanLanguageTechnologies,NAACL-
HLT 2021, Online, June 6-11, 2021, pages 3511–
3535.AssociationforComputationalLinguistics.
Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong
Yang, Mingfeng Xue, Boxing Chen, and Jun Xie.A BayesianFactorization LLMsandtrainscontinuousvectorsasprefixeson
attribute-specificdata. DisCup(ZhangandSong,
TheBayesianfactorizationiswidelyusedincon-
2022)adoptsLSTM-basedpromptstotrainLLMs
trollabletextgenerationasthefollowingformula-
toapproachare-rankedtokendistribution,rather
tion:
thantakingthenext-tokenasthelabel. PPO(Schul-
P(y |y ,c) ∝ P(y |y )P(c|y ) (7) manetal.,2017)learnstomaximizetheexpected
t ≤t−1 t ≤t−1 ≤t
rewards, while avoiding deviating too far. Quark
wherey tisthet-thtokenofasentenceyincorpora. (Lu et al., 2022) is the SOTA RL-based method
Post-processingmethodsregulatethedistribution for controllable text generation. It trains LLMs
ofthenexttokenwithattributeclassifiersthrough conditioningonrewardtokens.
Eq.7,whereP(y |y )isapproximatedwithlog-
t ≤t−1
itsoutputbyLLMs,andP(c|y )isscoredbythe B.2 Multipleattributecontrolling
≤t
attribute classifier. Finetune-based methods train Experimental Settings. The model structure
language models on attribute-specific corpora. c and scorer structure are the same as in Ap-
in P(y t|y ≤t−1,c) is represented through continu- pendix B.1. We use an Adam optimizer and
ouspromptsorcontrolcodes(Yangetal.,2023a; a linear scheduler with a warm-up ratio of 0.1,
Keskaretal.,2019). and a learning rate of 5e-5. For identical-
ComparedtothetraditionalBayesianfactoriza- domain settings, We use the textual prefixes
tionformasinEq.7,thedifferenceofourderiva- as in Huang et al. (2023), which are: “Once
tion is that we reserve a term P(c,y ≤i−1) during upon a time”,“The book”,“The chicken”,“The
the derivation. This term is usually ignored con- city”,“Thecountry”,“Thelake”,“Themovie”,“The
sidering its invariance to y i. The novel Bayesian painting”,“Theweather”,“Thefood”,“Whilethisis
factorizationcanbetransformedinto: happening”,“Thepizza”,“Thepotato”,“Thepresi-
P(c|y )P(y ) dentofthecountry”,“Theyearis1910.”. Forcross-
≤t ≤t
P(y |y ,c) ∝ (8)
t ≤t−1 domainsettings,weincrementtheaboveprefixset
P(c,y )
≤t−1
with“Insummary”,“Thisessaydiscusses”,“Views
P(c|y )
≤t
∝ P(y |y ) (9) on”, “The connection”, “Foundational to this is”,
t ≤t−1
P(c|y )
≤t−1
“Toreview,”,“Inbrief,”,“Anillustrationof”,“Fur-
P(c|y ) thermore,”, “The central theme”, “To conclude,”,
where ≤t indicatesthetheprobabilityshift.
P(c|y ≤t−1) “Thekeyaspect”,“Priortothis”,“Emphasisedare”,
B ExperimentalDetails “Tosummarise”,“Therelationship”,“Moreimpor-
tantly,”,“Ithasbeenshown”,“Theissuefocused
B.1 Single-attributeControl
on”, “In this essay” as in Gu et al. (2022b). The
ExperimentalSettings. WeusethesameLSTM weighersconsistoftwolinearlayers,aReLUac-
continuouspromptsasZhangandSong(2022)to tivationlayer,andaregressionlayer. Weannotate
steer rather than tuning the whole LLMs. The topicdatawithsentimentclassifiersasinYangetal.
scorer is implemented based on GPT2-base with (2023a)toobtainmulti-annotateddatasets. Since
the same LSTM-based prompts, which is trained exploration from the base GPT2 cannot generate
onSST-5. WeuseanAdamoptimizerandalinear topicalsentences,weconductawarm-upfinetun-
schedulerwithawarm-upratioof0.1,alearning ingonthesamemulti-annotateddatasets.
rateof5e-5. BaselineBrief. GEDI (Krauseetal.,2021)is
Baseline Brief. PPLM (Dathathri et al., 2020) extendedbyaveragingnormalizedscoresofgener-
updatesparametersofshallowlayersofLLMswith ativediscriminators. Thesescoresarethenusedto
theguidanceofattributeclassifiers. GEDI(Krause biasthetokendistributionformulti-attributecon-
et al., 2021) finetunes a class-conditional LM as trolling. We also include DIST. LENS (Khalifa
a generative discriminator to control the genera- etal.,2021), whichintroducesanautoencoderto
tion. DExpert (Liu et al., 2021) fine-tunes two map constraints to latent subspaces, and explore
PLMsasanexpertandananti-experttosteertext the intersection of multiple constraints. TAILOR
generation. FUDGE(YangandKlein,2021)trans- (Yangetal.,2023a)combinesseveralpromptsby
formsthedataformulationofthetrainingcorpus further training on pseudo multiple annotations.
to make the attribute discriminators get prospec- PROMPT-GATING(Huangetal.,2023)improvethe
tives. Prompt-tuning(LiandLiang,2021)freezes combinationabilityofpromptsbyintroducingad-ditionalgating/addingparameters. PPO(Schulman Model Cor. Top. Flu. Kappa
etal.,2017)andQuark(Luetal.,2022)havebeen
GEDI 7.8 5.2 4.9 0.65
introducedintheabovesubsections.
P.T. 7.6 5.4 6.7 0.71
C HumanEvaluation QUARK 8.0 6.6 7.0 0.66
TOLE 8.2 6.7 7.0 0.68
C.1 EvaluationSettings
Table4: Humanevaluationresultsofsentimentcontrol
Weconducthumanevaluationsonallthreeexper-
tasks. Cor., Top., Flu. denotes Correctness, Topical-
imental settings. We sample 50 random prompts
ity, andFluencyrespectively. P.T.denotesthevallina
forunlearnrepetition,100promptsforsentiment
prompt-tuningmethods. KappadenotesFleiss’skappa
control(50/50forneutral/oppositesentiment)and value.
100promptsformulti-attributecontrolling(50/50
foridentical-/cross-domain). Wesamplefivegen-
erationsforeachprompt. Weinvitefivestudentsto
Model Tox. Top. Flu. Kappa
scorethesamples. Eachstudentisproventohave
GeDi 7.5 5.9 5.1 0.73
sufficientEnglishskillsthroughpre-tests. Theyare
P.T. 7.0 6.3 6.8 0.68
askedtogiveascoreintherangeof0-10fromthe
followingquestions.
QUARK 7.9 7.3 7.0 0.63
TOLE 8.2 7.3 7.0 0.71
Inthesentimentcontroltask,questionsare
Table 5: Human evaluation results of detoxification.
• Correctness: Does the generated sentence
Tox.,Top.,Flu. denoteLess-Toxicity,Topicality,and
matchthetargetemotion?
Fluencyrespectively. P.T.denotesthevallinaprompt-
tuningmethods. KappadenotesFleiss’skappavalue.
• Topicality: Isthegenerationnatural,relevant,
followslogicallyfromtheprompt,andmain-
tainsaconsistenttone,wordchoice,andstruc-
Model Acc. Flu. OA Kappa
ture?
GEDI 6.6 4.8 4.9 0.79
• Fluency: Isthegenerationgrammaticallycor-
DIST. LENS 7.5 6.6 6.3 0.66
rectandcoherent?
TAILOR 7.2 6.4 6.5 0.68
TOLE 8.0 6.6 6.8 0.71
Inthedetoxificationtask,questionsare
Table6: Humanevaluationresultsofmulti-aspectcon-
• Non-Toxicity: Is the generated sentence po-
trolling. Acc., Flu., OA denote Accuracy, Fluency,
lite,respectfulandreasonable?
andOverallrespectively. KappadenotesFleiss’skappa
value.
• Topicality: which one is more natural, rele-
vant,followslogicallyfromtheprompt,and
maintainsaconsistenttone,wordchoice,and
structure? C.2 ResultsandAnalysis
• Fluency: which one is more grammatically
ResultsofthehumanevaluationareshowninTa-
correctandcoherent?
ble4,Table5,Table6,correspondingtosentiment
control,detoxification,multi-attributecontrolling
Inthemulti-attributecontrollingtasks,questions
respectively. Theresultsofhumanevaluationgen-
are
erallysupporttheanalysisofautomaticevalutions
in §4. The post-processing method can achieve
• Accuracy: Does the generation match both
greatattributeaccuracybutremainslowtextqual-
targetattributes?
ityaccordingtoGEDI. Finetuning-basedmethods
• Fluency: Isthesystem’sgenerationgrammat- achievesuboptimalperformanceduetooverfitting
ical,easy-to-read? issuesofsupervisedlearning. RL-basedmethods
performbestamongbaselineswithhighattribute
• Overall: Isthisgenerationhuman-like? accuracyandtextquality.Model neg-pos neu-pos pos-neg neu-neg
q-3 q-5 q-7 q-9
1.0
normal+TOLE 69.36 97.16 72.81 98.02
special+TOLE 69.13 97.56 72.85 98.24
0.8
0.6 Table 7: Experiments on different classifier settings.
"normal"and"special"denotesthecanonicaltraining
0.4
0 3 6 9 12 15 18 method and the decomposed training method respec-
x1000 step
tively. "a"-"b"meansthatthegoalistosteerpromptof
"a"tothetarget"b".
Figure5: Caption
D.2 Effectofthenumberα,β
98.7 98.2
pos pos α is a hyper-coefficient of KL-divergence in the
98.2
neg 97.9 neg
trainingobjective(Eq. 5). Figure6indicatesthatas
97.7
97.6 thecoefficientsincrease,themodelhasadecrease
97.2
inattributecorrectnessandanincreaseintextflu-
96.7 97.3
0 0.050.10.150.2 0 0.050.10.150.2 ency. ThisisbecausetheKL-divergenceconstrains
kl coefficient Ent. coefficient
theexistingmodelfromdeviatingtoofarfromthe
0.78 0.79
original,mitigatingtheperturbationofthesemantic
pos pos
0.76
neg 0.76 neg space,butlimitingthemodel’scontrollabilityover
0.74 the attributes. β is a hyper-coefficient of entropy
0.73
0.72 term. Figure6demonstratesthattheentropyterm
0.70 0.70 hasarelativelyslighteffectonperformance,notas
0 0.050.10.150.2 0 0.050.10.150.2
kl coefficient Ent. coefficient muchasKL-divergence. Asβ increases,attribute
17.2 17.4 accuracyandtextdiversityhaveaslightincrease.
pos pos
17.1
neg 17.1 neg D.3 TrainingmethodsofClassifiers
17.0
16.8 Theoretically, the training corpus for attribute
16.9
classifiers should be organized as (y ,c),
16.8 16.5 ≤t
0 0.050.10.150.2 0 0.050.10.150.2
which means a desired sentence y should
kl coefficient Ent. coefficient
be decomposed into |y| training samples
Figure 6: Performance (y-axis) on sentiment control (y ,c),(y ,c),(y ,c). We conduct experi-
≤0 ≤1 ≤|y|
tasktogeneratepositive/negativesentencesfromneutral
ments on both the sentiment control task and the
sentences,withvaryingKL/Entropycoefficient(x-axis).
detoxification task, which shows that this setting
doesnotmakeasignificantperformancegain.
D FurtherStudies E QuanlitativeResults
D.1 Effectofthequantilenumberq
We conduct experiments on q = 3,5,7,9 respec-
tively. Performancesvaryingwithstepsareshown
inFigure5. Wecanseethatalllinesachievesim-
ilarfinalperformance. However,theconvergence
of the process is slightly slower when q = 3 and
q = 9. Wehaveanalyzedin4.4thatsmallq makes
relativeordersbetweenquantilesmoreambiguous
sinceeachquantilehasalargerintervalfornoise,
whilealargeq confinesnoisewithinasmallinter-
val, diminishing noise impact, which results in a
lowergeneralization. Convergenceisfasterwhen
q = 5,7,whichvalidatesthatamoderateq-value
allowsthemodeltoreachthedesiredresultfaster.
cca
cca
3-tsid
lpp
cca
3-tsid
lppPrompts: Soon,itbecomesclear:
GPT2 Soon,itbecomesclear: ifafemaleisintherightplaceandtimeofyearat
Quark Soon,itbecomesclear: westillhavetimetosolvetheproblem,hopeit
TOLE Soon,itbecomesclear: herworkherewillbeasuccess. shewillbeagreat
Prompts:ForFrance,Germany’sprimarypartner
GPT2 ForFrance,Germany’sprimarypartnerintheG8,andaleadingmemberofEurope’s"G
Quark ForFrance,Germany’sprimarypartnerintheEurozone,thisisamomentofgreatchallenge,
and
TOLE For France, Germany’s primary partner in the fight against terrorism. We share a long
historyoffriendshipand
Prompts:EvenVladimirLeninhimselfcouldn’t
GPT2 EvenVladimirLeninhimselfcouldn’thavebeenagoodhusband,sohehadtotakeon
Quark EvenVladimirLeninhimselfcouldn’tdowhathe’sdoingrightnow. Wemusttakeaction
TOLE EvenVladimirLeninhimselfcouldn’tdomore. Weareblessedwithgreatleadersinour
country
Prompts:I’mnotsaying
GPT2 I’mnotsayingitwasmyfault. Iamnottheonewhodidthis
Quark I’mnotsayingI’maChristian.’ ’Thisismyfirsttime. We
TOLE I’mnotsayingthat. Wealwaysbackyouupandbeatyourside. Believe
Prompts:Those,too,arethe
GPT2 Those,too,arethefirstandforemostinthe"Rough"categoryofthe"
Quark Those,too,arethemenandwomenoftheNavy."Thestatementalsoreferredtothe
TOLE Those,too,arethemostbeautifulpeopleIknow. Wehavemuchtolearntogether"
Table8: Generationexamplesofthesentimentcontroltask. SincebothQuarkandTolecanachieve>90%final
results. Toexplicitlyshowthatourmethodsconvergesquicker. Wechoosetopresentinterimresultsfrommodels
trainedfor5ksteps.
Prompts: Theprisonersarethebadguys,and
PPLM Theprisonersarethebadguys,andhersonisPeter,theevilone. Andtosoothehispain,
heisgoingto
GeDi Theprisonersarethebadguys,andIwasintothestrugglefortheirfreedom. Atschoolin
Thailand’sNarathiwatprison
DExpert Theprisonersarethebadguys,andbysettingtheircagesupsoclosetothestreet,they
weretheleastoftheirproblems. But
Prompt Theprisonersarethebadguys,andthegoodguysareinjail. Thebadguyswillalways
getafreepass. Theprisoners
DisCup Theprisonersarethebadguys,andtheprisonerscangetawaywithit,butit’snottheend.
Wecangetmorejusticeif
TOLE Theprisonersarethebadguys,andpeoplemaylookdownuponthem,butmanyofthem
behaviorwellafterbeingreleasedfromprison.
Table9: Baselinesresultsforthedetoxificationtask. Thistablepresentthefinalmodelforeachmethod.Positive,Asian
Dist. Lens Onceuponatime,greatfood. theserviceisgreatandveryfriendly. Advertisements
-yummy! ***Helpful!! :)
Tailor Onceuponatime,iwasabigfanofthisestablishment. ihadbeentothemacouple
timesalreadywithfriendsandtheplacewasalwayspackedwithpeople. thisplace
wasahit,somuchsothatnowIamgoingtogothereeverytime
Prompt-gating Once upon a time i was at the restaurant and had dinner at a friend’s place, the
food was great. the chef is very knowledgeable (the owner’s daughter is a sushi
professional),buticanonlysaythattherestaurantwasverycleanandthefoodwas
veryflavorful.
TOLE Onceuponatime,chuanisanawesomerestaurantinastripmall. authenticchinese
comfortfoodwithgreatservice. ijustlovebeingabletofindmoremoderndecor
whilediningandanexcellentdiningdiningarea. whilei’vebeenheretwiceand
stilltheleasthalfatthesamediningexperienceisverynice. andifieata
Positive,American
Dist. Lens Theyearis1910. greatfoodandtheservicewasgreat. willbeback! Advertisements
forgoodsushi!!! Followme@wendie
Tailor Theyearis1910. thisisnotyouraveragesushibar. theatmospherewasverycasual
andcozy. thefoodwasgood. theservicewaspromptandfast. thefoodwasfresh
anddelicious.
Prompt-gating Theyearis1910. wewenttothisplaceforthefirstyear. itlooksnice. thereare
acoupleofbooths,atablefor4(andacounter)togetseatedin. wehadanearly
reservation. i ordered the steak. the steak was great. my boyfriend enjoyed the
chickenwings. iwasveryimpressedbythesteak.
TOLE Theyearis1910. acoffeeculture(notonlyforcoffee)oraliquortospendafew
dollarstothesugaritinapapercupcakeforanafteralongvegasstrip’sworth. you
canwalktomr. wynn... that’showcrowdedandyoucouldwaitforagoodcheap
mealonthe
Positive,Mexican
Dist. Lens Whilethisishappening,greatfood! themargaritasarefantasticandfriendlystaff.
Advertisements=goodmexicanrestaurant! Related
Tailor While this is happening to be my new favorite restaurant, and my new favorite
location,ican’tsaythati’veeverbeentothisplacewithalltheotherreviews. there
isalotofgoodfoodatthetable,butthewaitstaffandfoodisjustasgreat! ilove
thattherearenowaiterstotakemyorders,butthatdoesn’tmeanthefoodisbad.
thefoodherewasgood.
Prompt-gating Whilethisishappeningi’mgoingherefordinnerforthefirsttime. thefoodhere
was very, very good and very tasty!!we ordered a couple of different salads and
sometacos. igotavegobeeftacowithaspicysauce(itisverygood). ialsogotan
onionrings(itdoesnothaveanyonions,noraretheremanyonionsinthisrecipe),
anditwasdelicious!
TOLE Whilethisishappening,youcanselectitemsandchoosethesaucesandhotsalsa;
you’llrealizeyourfacetrembformakingaburritoplaceandaburritoyouhave
to be glad it goes!!! i would highly recommend this joint!!!!!!! you get there at
lunchtime,it’sattheplaza
Table10: Baselinescomparsionformulti-controltasks. Thistablepresentthefinalmodelforeachmethod.