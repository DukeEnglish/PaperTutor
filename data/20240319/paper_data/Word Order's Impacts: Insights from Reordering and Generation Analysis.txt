Word Order’s Impacts: Insights from Reordering and Generation
Analysis
Qinghua Zhao1,2, Jiaang Li2, Lei Li2, Zenghui Zhou3, Junfeng Liu1
1SKLSDELab,BeihangUniversity
2DepartmentofComputerScience,UniversityofCopenhagen
3SchoolofAutomationScienceandElectricalEngineering,BeihangUniversity
{zhaoqh,liujunfeng,zhouzenghui}@buaa.edu.cn,jli@hum.ku.dk,lilei@di.ku.dk
Abstract
Existingworkshavestudiedtheimpactsoftheorderofwordswithinnaturaltext. Theyusuallyanalyzeitbydestroying
theoriginalorderofwordstocreateascrambledsequence,andthencomparingthemodels’performancebetween
theoriginalandscrambledsequences. Theexperimentalresultsdemonstratemarginaldrops. Consideringthis
findings,differenthypothesisaboutwordorderisproposed,including“theorderofwordsisredundantwithlexical
semantics”,and“modelsdonotrelyonwordorder”. Inthispaper,werevisittheaforementionedhypothesesby
addingaorderreconstructionperspective,andselectingdatasetsofdifferentspectrum. Specifically,wefirstselect
fourdifferentdatasets,andthendesignorderreconstructionandcontinuinggenerationtasks. Empiricalfindings
supportthatChatGPTreliesonwordordertoinfer,butcannotsupportornegatetheredundancyrelationsbetween
wordorderlexicalsemantics.
Keywords:wordorder,chatgpt,reorder
1. Introduction semantics,andthesecondoneash : modelsdo
2
notrelyonwordorder.
Wordorder,referringtothesequentialorderofin- We design experiments to revisit these two hy-
dividualwordswithinatext,isafundamentalcon- potheses. Morespecifically,ourbeliefisthatword
cept in natural language. Previous works have orderassumesvaryingdegreesofsignificancein
investigatedwordorder’simpactsbyalteringorder, different contexts. For instance, in certain tasks,
and they find that no matter altering order in the rearrangingtheorderdoesnotleadtosubstantial
pre-trainingorthetraining/inferencedata,theper- information loss. This could be attributed to the
formance of downstream tasks drops marginally adequacyofbag-of-wordsinformationortheabil-
(Sinha et al., 2021a,b; Pham et al., 2021; Gupta ity to reconstruct the correct order from acquired
et al., 2021; Hessel and Schofield, 2021; Cloua- backgroundknowledge. Conversely,inothertasks,
treetal.,2022;YanakaandMineshima,2022;Pa- modifying word order can introduce errors or en-
padimitriou et al., 2022), which demonstrates a tirely change the conveyed meanings. Besides,
counter-intuitiveandunnaturalphenomenon. HesselandSchofield(2021)claimsthat
Regardingtheexperimentalresults,specifically,
themarginalperformancedropsinducedbyaltering Determiningwhetherornotorderiscon-
wordorder,existingworksprovidevariedexplana- sideredforaparticulartaskislargelyan
tionsorhypotheses. Forexample,Papadimitriou experimental,empiricalendeavor,
etal.(2022)arguesthatwordordermayberedun-
demonstratingtheanalysisofwordorder’simpacts
dantwithlexicalsemantics,exemplifiedbythebag-
requiresspecificanalysesondistincttasks. There-
of-wordsmodel. Inotherwords,whiletheybelieve
fore, we select four diverse datasets represent-
wordordermatters,theyassertthatorderinforma-
ingvariouscontexts. Thesedatasetsencompass
tioncanbederivedfrombag-of-words,renderingit
declarativesentences,expressionsofpartialorder
redundant. Ontheotherhand,Sinhaetal.(2021a)
orcomparativerelations,programminglanguages,
believesthattheevaluatedmodels(regardlessof
andmore. Leveragingtheimpressiveperformance
being unidirectional or bidirectional), do not rely
ofChatGPT,ourexperimentsareconductedusing
onwordorderinformation. Concerningthesetwo
gpt-3.5-turbowithdefaultparameters,limitingthe
hypotheses1,inthispaper,wetakeastepforward
maximumnumberoftokensto256. Secondly,we
byaddingareorderingtaskandtestingitonfour
employ two distinct experimental tasks. The first
different datasets using ChatGPT. To streamline
task, referred to as continuing generation, fol-
thediscussionofthetwohypotheses,welabelthe
lowstheprevioussetupofgeneratingtextusingthe
firstoneash : wordorderisredundantwithlexical
1 scrambled sequence. Additionally, we introduce
1We designate these explanations as hypotheses a novel task known as order reconstruction, in
whichthemodelistaskedwithrestoringtheoriginal
sincetheyhavenotbeenwidelyapprovedorvalidated.
4202
raM
81
]LC.sc[
1v37411.3042:viXrawordorderfromaprovidedscrambledsequence. orderisredundantwithlexicalsemantics(Papadim-
Subsequently,weanalyzetheempiricalresultsto itriouetal.,2022),languagemodelsdonotrelyon
determinewhethertheyalignwithorchallengeex- wordorder(Clouatreetal.,2022),amongothers.
istinghypotheses.
It’simportanttoemphasizethattheconceptof Recent works. ChatGPT has led to numer-
wordorderpertainsspecificallytonaturallanguage, ousworks,includingassessingChatGPT’sperfor-
i.e.,toparticulartextsordatasets,ratherthanpre- mance on existing NLP tasks (Pan et al., 2023;
trainedlanguagemodels. However,whendelving Wangetal.,2023b;Hendyetal.,2023;Zhuetal.,
intotheexaminationofwordorder’seffectswithin 2023;Yangetal.,2023;Gaoetal.,2023;Yuanetal.,
the realm of natural language, we must gain in- 2023),andproposingmanynewevaluationframe-
sightsintohowthelanguagemodel’sperformance works (OpenAI, 2023; Zhong et al., 2023; Kocmi
changes when faced with scrambled word order. andFedermann,2023). Furthermore,ChatGPTis
This approach bears a resemblance to the study also applied to explore other fields, for example,
ofbrainfunctionality,whereresearcherstradition- Kosinski(2023)claimsthattheTheoryofMindabil-
ally induce damage to brain regions and subse- ityhasemerged. Zhuoetal.(2023)analyzesthe
quentlyobservetheresultingbehavioralorcogni- features of ethical dangers from the perspective
tivechangesinexperimentalsubjects. of bias, reliability, and toxicity. Also, Wang et al.
Ourcontributioncanbesummarized: (2023a)speciallydiscussestheout-of-distribution
robustness. Kumar(2023);Guerreiroetal.(2023)
• Revisitingestablishedhypothesesregarding discussthehallucinations(Ferrara,2023;Fischer
theimpactofwordorderfrombothreordering etal.,2023;Liangetal.,2023). Chomsky(2023)
andgenerationperspectives. claims that there are significant differences be-
tweenChatGPTandhumansintermsofthinking
• Analyzingtheinfluenceofwordorderacross
styleandlanguagelearning,aswellasmoraland
diversedatasetsofvariouscontexts.
ethicalprinciples. Ortega-Martínetal.(2023)empir-
icallyanalyzesthelinguisticambiguity,considering
• Theempiricalresultschallengehypothesish 2,
aspects of homonymy and polysemy, as well as
whileunsupportornegatehypothesish 1.
syntactic and semantic factors. Considering the
topperformanceofChatGPTandaligningwiththe
existingresearch,weinvestigatetheimpactofword
2. Related Work
orderusingChatGPT.
Word order is a crucial aspect of natural lan-
guage,andstudieshaveinvestigateditsimpacton 3. Experimental Analysis
languagemodelsbyperturbingwordorder(Sinha
etal.,2021b;Phametal.,2021;Guptaetal.,2021; Datasets selection. Given that ChatGPT has
HesselandSchofield,2021;Clouatreetal.,2022; outperformedhumanevaluationacrossnumerous
YanakaandMineshima,2022;Papadimitriouetal., NLP tasks, not all tasks are appropriate for eval-
2022). Forexample,Guptaetal.(2021);Sinhaetal. uation. Consequently, in line with the approach
(2021b); Pham et al. (2021) examine NLI, para- in OpenAI(2023);Touvronetal.(2023),wecare-
phrase detection, sentiment analysis and GLUE fully select evaluation datasets from their collec-
datasets, and show that shuffling only minimally tions. We believe that word orders fulfill varying
degradesperformance. Sinhaetal.(2021a)also roles across different contexts, therefore, guided
examinestheorderofpre-trainingcorpus,andalso byourintuition,weselectfourdatasets. Notethat,
concludeasimilarempiricalfinding. Clouatreetal. due to the complexities and ambiguity in natural
(2022) proposes local and global structures and language,thechosendatasetsmaynotrepresent
threeshufflingstrategiestotestoncharacter-,word- thefullspectruminnaturallanguage.
,andphrase-level,respectively. Theresultsshow The first one is RealToxicityPrompt dataset
thatlocalstructurematters,andpreviousshuffling (RTP)(Gehmanetal.,2020),whichconsistsofcom-
strategies do not destroy the local structure. Al- pletiontasksdesignedtoassessthetoxicityofgen-
Negheimishetal.(2023)triestopreservetheimpor- eratedtext. Inthisdataset,modelsareprompted
tanceofwordorderbyforcingthemodeltoidentify tocompleteincompletetoxicqueries. Pleaserefer
permutedsequencesasinvalidsamples. Insum- toAppendixDforexamples.
mary, existing works have coherently found that The second one is Computer Science dataset
breaking word order do not result in a significant (CS),whichisselectedfromEvals(OpenAI,2023),
decreaseintaskperformance. Althoughtheyhave and contains computer science-related single-
triedtoexplainthesefindings,noexplanationshave choice questions. For example, “Binary tree sort
beenwidelyaccepted. Theseexplanationsinclude isanin-placesortingalgorithm? a)Trueb)False”.
wordordermatterslittle(Sinhaetal.,2021a),word Evals(https://github.com/openai/evals)0.8
0.6
0.4
0.2
0.0
RTP CS BF Loop
ex F&L ex two ex adj fix F&L
Figure1: Scoresoftheorderreconstructiontask.
is OpenAI’s framework for evaluating large lan- 2023), we opt for straightforward strategies that
guagemodels. prioritizehighdistinguishability,aswedonotseek
ThethirdoneisBorn-firstdataset(BF),whichis to quantify the impacts of word order disruption.
selectedfromEvals(OpenAI,2023),andpresents Firstly,twosuperficialdisruptionsareused,includ-
ataskofdeterminingtheolderoftwocandidates. ingexchangingthefirstandlastword(ex F&L)and
Forexample,“WasRichardNixonbornbeforeJohn exchangingtworandomselectedwords(ex two).
F. Kennedy? Answer Y or N”. It involves partial Inex two,theselectedtwowordsaretotallyran-
order relations such as “better than, older than, dom. Secondly,twodeepdisruptionsareemployed,
minus,anddividedby”,andinherentlycontainsthe includingexchangingtheadjacentwords(ex adj)
conceptoforder. Forexample,if“RichardNixon” and fixing the first and last words while shuffling
isswappedwith“JohnF.Kennedy”,theansweris theothers(fix F&L).Inex adj,adjacentwords
completelyreversed. are swapped in pairs: the first and second, third
andfourth,andsoforth.
ThefourthoneisInfinitloopdataset(Loop),which
isalsochosenfromEvals(OpenAI,2023),andre-
volvesaroundprogrammingandaimstodetermine
3.1. Order Reconstruction
whetheracodesegmentcontainsaninfiniteloop
block. SeeAppendixDforanexample. Different Prompt. To ask ChatGPT to restore the order,
fromBF,whichinherentlyexpressestheconcept we use “It is a query with wrong word order, you
of “order” at the semantic level, Loop resembles needtoreorderitswordsinnormalwordorder. You
instructions represented through text. The code mustn’tremoveoraddwords,andthequerylength
textitselfdoesnotdirectlyconveyspecific“mean- shouldbekept. Thequeryis[query].”.
ing”,rather,itcommunicatesaseriesofinstructions
by adhering to certain rules. The “order” is also
Metric. In the order reconstruction task, only
predefined in the rules. Accurate understanding
the original words are permitted for generation
ofcodeinstructionsrequiresacquisitionofprede-
(“Youmustn’tremoveoraddwords,andthequery
fined rules, and code text that does not conform
length should be kept.”). Consequently, we em-
tothesepredefinedrulescannotbeexecutedac-
ploy BLEURT (Sellam et al., 2020), BLEU (Pap-
curately. Consequently, it is undoubted that this
ineni et al., 2002), and METEOR (Banerjee and
dataset exhibits strong dependency on word or-
Lavie,2005)forevaluationandreporttheaverage
der. However, a question still remains: when the
scoresderivedfromthesemetrics. Commonlyuti-
word order is disrupted, is the model completely
lizedinmachinetranslationtoquantifygenerated
incapableofcomprehension,orcanitamendthe
textagainstreferencetexts,thesemetricsareaptly
disorderutilizingpredefinedrulesthathavebeen
suitedfortheorderreconstructiontask.
learned?
Analysis. Figure 1 shows the scores of order
Scrambling strategies. In the field of neuro- reconstruction across tested datasets, and four
science,itiscommontostudythefunctionalityof scramblingstrategies. Firstly,itisevidentthatthe
brainareasbyexaminingareaswithfunctionalim- disturbanceofwordorderhasamoresignificantim-
pairments. Previous works on word order have pactontheBFandLoopdatasetsincomparisonto
investigated it by disrupting word order, in align- theRTPandCSdatasets. Toillustrate,thedecline
mentwithexistingworks,wealsoemploymethods fromthebesttotheworstresultsstandsat(19%,
todisrupttheorder. Whilenumeroustextperturba- 13%,27%,97%)for(RTP,CS,BF,Loop),respec-
tion methods and quantification techniques have tively,showingthatRTPandCSarelesssensitive
beenproposed(Clouatreetal.,2022;Zhaoetal., towordorderdisruptionthanBFandLoop. Itmay0.6
0.5
0.4
0.3
0.2
0.1
0.0
RTP CS BF Loop
original ex F&L ex two ex adj fix F&L
Figure2: Scoresofthecontinuinggenerationtask.
beattributedtoRTPandCSrelymoreonbag-of- question. Reply only with the single letter of the
wordsinformation,whereasthelossoforderinfor- answeryouhavechosen.” and“Youhavetodeter-
mationonBFandLoopreduceslargerinformation mineifagivenblockofcodewillruninforeverinan
loss. Besides, when comparing BF with Loop, a infiniteloop,orifitwillstop. OnlyanswerwithTrue
significantly greater decline is observed in Loop ifitwillrunforever,andonlywithFalseifitstops.”.
thaninBF(97%v.s. 27%). Weknowthat“order”
conceptisexpressedinBFtextandispresupposed
Metric. Themetricsareflirtationandaccuracyfor
outsidetheLooptextasanecessarybackground
RTPandotherdatasets,respectively. Flirtationisa
knowledge, the explanation remains challenging.
scoringmeasureforthedegreeofflirtationwithina
Nonetheless,itsuggeststhatorderinformationis
text,withlowerscoresindicatingbettergeneration.
utilizedbyChatGPTduringinference. Foramore
seeAppendixCfordetails.
straightforwardcomprehension,pleaserefertothe
examplesinAppendixD.
Secondly,fromtheperspectiveofdifferentscram- Analysis. Figure2showsthegenerationperfor-
mance. Firstly, in congruence with the results in
blingstrategies,thetwodeepdisruptions(i.e.,ex
Section 3.1, the disruption of the order of word
adj and fix F&L) induce more substantial de-
leads less performance fluctuations for RTP and
clinesthanthesuperficialdisruptionmethods(i.e.,
CSdatasetsascomparedtoBFandLoopdatasets.
ex F&L and ex two). Specifically, the average
To illustrate, the scores of disruption word order
scores of the deep disruptions are (0.51, 0.41),
dropby(-13%,0.1%,35%,26%)forthe(RTP,CS,
whereasthoseforsuperficialdisruptionsare(0.67,
BF,Loop)datasets,respectively. Althoughthere-
0.63),underscoringthesignificanceofwordorder
withinChatGPT.
sultsmaynotvalidatehypothesish 1,theysuggest
thath couldserveasasufficientconditionforthis
TheseresultsaffirmthatChatGPTtakeswordor- 1
outcome. Secondly,uponcomparingtheresultsbe-
derinformationintoaccountduringinference. Con-
tweentheoriginalandsuperficialdisruptionmeth-
sequently, the present experimental results chal-
ods(i.e.,ex F&Landex two),a30%decreaseis
lenge the hypothesis h (models do not rely on
2 observedontheBFdataset,whiletheLoopdataset
word order). Note that the base scores seem to
experiences a 10% decline. It may be due to BF
below,thatisbecausetheChatGPTmayaddor
containspartialorderrelations,whereastheLoop
remove some words compared with the original
isgovernedbypre-definedrules. Withshallowdis-
words. However, we are not inclined to allocate
ruptions,ChatGPTcontinuestocapturethecorrect
excessivetimetooptimizetheprompts,asitfalls
orderbasedonthepredefinedruleswithintheLoop
outside the scope of our current focus. Further-
dataset.
more,whenweprovidethesepromptstohumans
(whoholdPhDsinthesamefield),theycanstrictly
followtheinstructions. 4. Conclusion
Inthispaper,were-visittheimpactsofwordorder.
3.2. Continuing Generation
Aiming to examine the hypotheses from existing
Prompt. ForRTP,weadoptthepromptemployed works,wefirstselectdifferentdatasetsbasingon
in(Touvronetal.,2023),“Completethefollowing ourintuitionandconductacomprehensiveanalysis
sentenceinapolite,respectful,andunbiasedman- ofwordorder’simpacts. Furthermore,weintroduce
ner.” anorderreconstructiontaskthatcomplementsex-
ForBFdataset,thequeryisalsotheprompt,and istingmethods. Ourexperimentalinvestigationen-
forCSandLoop,weutilizetheprovidedprompts, compassestwokeyaspects: reorderingthegiven
“Choose the best multiple choice answer to this scrambledsequences,andgeneratingtextbasedon these sequences. By integrating the results Samuel Gehman, Suchin Gururangan, Maarten
obtainedfrombothaspects,wedemonstratethat Sap,YejinChoi,andNoahA.Smith.2020. Real-
ChatGPT rely on word order. Moreover, we also ToxicityPrompts: Evaluatingneuraltoxicdegen-
highlightthatdifferenttasksexhibitdifferentrequire- erationinlanguagemodels.InFindingsoftheAs-
mentofwordorder,makingitnecessarytoinclude sociationforComputationalLinguistics: EMNLP
additionaldatasettypesinfuture. 2020,pages3356–3369,Online.Associationfor
ComputationalLinguistics.
5. References NunoM.Guerreiro,DuarteAlves,JonasWalden-
dorf, Barry Haddow, Alexandra Birch, Pierre
Colombo,andAndréF.T.Martins.2023. Halluci-
nationsinlargemultilingualtranslationmodels.
HadeelAl-Negheimish,PranavaMadhyastha,and
AshimGupta,GiorgiKvernadze,andVivekSriku-
Alessandra Russo. 2023. Towards preserving
mar.2021. Bert&familyeatwordsalad: Experi-
wordorderimportancethroughforcedinvalida-
tion. In Proceedings of the 17th Conference
mentswithtextunderstanding. InProceedings
oftheAAAIConferenceonArtificialIntelligence,
oftheEuropeanChapteroftheAssociationfor
volume35,pages12946–12954.
Computational Linguistics, pages 2563–2570,
Dubrovnik, Croatia. Association for Computa-
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf,
tionalLinguistics.
Vikas Raunak, Mohamed Gabr, Hitokazu Mat-
sushita, Young Jin Kim, Mohamed Afify, and
Satanjeev Banerjee and Alon Lavie. 2005. ME-
Hany Hassan Awadalla. 2023. How good are
TEOR: An automatic metric for MT evaluation
gpt models at machine translation? a compre-
withimprovedcorrelationwithhumanjudgments.
hensiveevaluation.
InProceedingsoftheACLWorkshoponIntrinsic
andExtrinsicEvaluationMeasuresforMachine JackHesselandAlexandraSchofield.2021. How
Translationand/orSummarization,pages65–72,
effectiveisBERTwithoutwordordering? impli-
AnnArbor,Michigan.AssociationforComputa-
cationsforlanguageunderstandinganddatapri-
tionalLinguistics.
vacy. InProceedingsofthe59thAnnualMeeting
oftheAssociationforComputationalLinguistics
TomBrown,BenjaminMann,NickRyder,Melanie
andthe11thInternationalJointConferenceon
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
NaturalLanguageProcessing(Volume2: Short
ArvindNeelakantan,PranavShyam,GirishSas-
Papers),pages204–211,Online.Associationfor
try,AmandaAskell,etal.2020. Languagemod-
ComputationalLinguistics.
els are few-shot learners. Advances in neural
informationprocessingsystems,33:1877–1901. TomKocmiandChristianFedermann.2023. Large
languagemodelsarestate-of-the-artevaluators
NoamChomsky.2023. Noamchomsky: Thefalse oftranslationquality.
promiseofchatgpt. TheNewYorkTimes.
MichalKosinski.2023. Theoryofmindmayhave
Louis Clouatre, Prasanna Parthasarathi, Amal spontaneouslyemergedinlargelanguagemod-
Zouaq,andSarathChandar.2022. Localstruc- els.
ture matters most: Perturbation study in NLU.
In Findings of the Association for Computa- Krishna Kumar. 2023. Geotechnical parrot tales
tionalLinguistics: ACL2022,pages3712–3731, (gpt): Overcominggpthallucinationswithprompt
Dublin, Ireland. Association for Computational engineeringforgeotechnicalapplications.
Linguistics.
WeixinLiang,MertYuksekgonul,YiningMao,Eric
Wu, and James Zou. 2023. Gpt detectors are
EmilioFerrara.2023. Shouldchatgptbebiased?
biasedagainstnon-nativeenglishwriters.
challenges and risks of bias in large language
models.
OpenAI.2023. Gpt-4technicalreport.
Ronald Fischer, Markus Luczak-Roesch, and Jo- Miguel Ortega-Martín, Óscar García-Sierra, Al-
hannesAKarl.2023. Whatdoeschatgptreturn fonso Ardoiz, Jorge Álvarez, Juan Carlos Ar-
about human values? exploring value bias in menteros,andAdriánAlonso.2023. Linguistic
chatgptusingadescriptivevaluetheory. ambiguityanalysisinchatgpt.
MingqiGao,JieRuan,RenliangSun,XunjianYin, LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
ShipingYang,andXiaojunWan.2023. Human- Carroll Wainwright, Pamela Mishkin, Chong
likesummarizationevaluationwithchatgpt. Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay, et al. 2022. Training language models to EmpiricalMethodsinNaturalLanguageProcess-
follow instructions with human feedback. Ad- ing,pages2888–2913,OnlineandPuntaCana,
vances in Neural Information Processing Sys- DominicanRepublic.AssociationforComputa-
tems,35:27730–27744. tionalLinguistics.
Wenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Koustuv Sinha, Prasanna Parthasarathi, Joelle
Che,andLiboQin.2023. Apreliminaryevalua- Pineau, and Adina Williams. 2021b. UnNatu-
tionofchatgptforzero-shotdialogueunderstand- ral LanguageInference. InProceedings ofthe
ing. 59thAnnualMeetingoftheAssociationforCom-
putationalLinguisticsandthe11thInternational
IsabelPapadimitriou,RichardFutrell,andKyleMa-
JointConferenceonNaturalLanguageProcess-
howald. 2022. When classifying grammatical
ing(Volume1: LongPapers),pages7329–7346,
role,BERTdoesn’tcareaboutwordorder...ex-
Online. Association for Computational Linguis-
ceptwhenitmatters. InProceedingsofthe60th
tics.
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), Hugo Touvron, Thibaut Lavril, Gautier Izacard,
pages636–643,Dublin,Ireland.Associationfor XavierMartinet,Marie-AnneLachaux,Timothée
ComputationalLinguistics. Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro,FaisalAzhar,etal.2023. Llama: Open
KishorePapineni,SalimRoukos,ToddWard,and
andefficientfoundationlanguagemodels. arXiv
Wei-Jing Zhu. 2002. Bleu: a method for auto-
preprintarXiv:2302.13971.
maticevaluationofmachinetranslation. InPro-
ceedingsofthe40thAnnualMeetingoftheAs- JindongWang,XixuHu,WenxinHou,HaoChen,
sociation for Computational Linguistics, pages RunkaiZheng,YidongWang,LinyiYang,Haojun
311–318,Philadelphia,Pennsylvania,USA.As- Huang, Wei Ye, Xiubo Geng, Binxin Jiao, Yue
sociationforComputationalLinguistics. Zhang,andXingXie.2023a. Ontherobustness
ofchatgpt: Anadversarialandout-of-distribution
Thang Pham, Trung Bui, Long Mai, and Anh
perspective.
Nguyen. 2021. Outoforder: How important is
thesequentialorderofwordsinasentenceinnat- ZengzhiWang,QimingXie,ZixiangDing,YiFeng,
urallanguageunderstandingtasks? InFindings andRuiXia.2023b. Ischatgptagoodsentiment
oftheAssociationforComputationalLinguistics: analyzer? apreliminarystudy.
ACL-IJCNLP 2021, pages 1145–1160, Online.
RuibinXiong,YunchangYang,DiHe,KaiZheng,
AssociationforComputationalLinguistics.
Shuxin Zheng, Chen Xing, Huishuai Zhang,
AlecRadford,KarthikNarasimhan,TimSalimans, YanyanLan,LiweiWang,andTieyanLiu.2020.
IlyaSutskever,etal. Improvinglanguageunder- Onlayernormalizationinthetransformerarchi-
standingbygenerativepre-training. tecture. InInternationalConferenceonMachine
Learning,pages10524–10533.PMLR.
Alec Radford, Jeffrey Wu, Rewon Child, David
Luan,DarioAmodei,IlyaSutskever,etal.2019. HitomiYanakaandKojiMineshima.2022.Composi-
Language models are unsupervised multitask tionalevaluationonJapanesetextualentailment
learners. and similarity. Transactions of the Association
forComputationalLinguistics,10:1266–1284.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi- XianjunYang,YanLi,XinluZhang,HaifengChen,
malpolicyoptimizationalgorithms. arXivpreprint and Wei Cheng. 2023. Exploring the limits of
arXiv:1707.06347. chatgptforqueryoraspect-basedtextsumma-
rization.
ThibaultSellam,DipanjanDas,andAnkurParikh.
2020. BLEURT:Learningrobustmetricsfortext Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei
generation. InProceedingsofthe58thAnnual Wang, and Songfang Huang. 2023. How well
MeetingoftheAssociationforComputationalLin- dolargelanguagemodelsperforminarithmetic
guistics,pages7881–7892,Online.Association tasks?
forComputationalLinguistics.
QinghuaZhao,JiaangLi,JunfengLiu,Zhongfeng
KoustuvSinha,RobinJia,DieuwkeHupkes,Joelle Kang, and Zenghui Zhou. 2023. Is word
Pineau,AdinaWilliams,andDouweKiela.2021a. order considered by foundation mod-
Maskedlanguagemodelingandthedistributional els? a comparative task-oriented analysis.
hypothesis: Orderwordmatterspre-trainingfor SSRN: https://ssrn.com/abstract=4516301 or
little. InProceedingsofthe2021Conferenceon http://dx.doi.org/10.2139/ssrn.4516301.Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo givenprompts,andtheirwriteresponsesforeach
Liang, Shuai Lu, Yanlin Wang, Amin Saied, prompts,the(prompts,responses)pairsareused
Weizhu Chen, and Nan Duan. 2023. Agieval: tofine-tuneGPT-3.5seriesmodels(thefine-tuned
Ahuman-centricbenchmarkforevaluatingfoun- models is called policy model). Next, a model
dationmodels. (not the policy model) is given prompts, and ask
ittogenerateseveralresponses,AItrainerranks
WenhaoZhu,HongyiLiu,QingxiuDong,Jingjing
theseresponsesfrombesttoworst. The(prompts,
Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and
responses, rank) triplets is used to fine-tune the
Shujian Huang. 2023. Multilingual machine
model(resultsinrewardmodel). Finally,reinforce-
translation with large language models: Em-
ment learning algorithm is used to optimize the
pirical results and analysis. arXiv preprint policy and reward model, to be specific, given a
arXiv:2304.04675.
prompt,thepolicymodelgeneratedanoutput,and
therewardmodelcalculatesarewardscoreforthe
Terry Yue Zhuo, Yujin Huang, Chunyang Chen,
output,andthenthereinforcementlearningalgo-
andZhenchangXing.2023. Exploringaiethics
rithmusestherewardscoretoupdatethepolicy/re-
ofchatgpt: Adiagnosticanalysis.
wardmodel.
A. Limitations C. Perspective API explanation
Therearetwolimitationsintheexperimentaldesign ThePerspectiveAPIisutilizedtoevaluatetheRe-
pertainingtowordorder. Thefirstlimitationisthe alToxicityPromptsdataset,providingeightattribute
absenceofaninvestigationintosentencelength, metrics,eachscoredonascalefrom0to1. Higher
which is widely acknowledged as a critical factor scoresindicateahigherlikelihood. Hereisacon-
incomprehendingadisruptedsequence. Thesec- ciseoverviewofeachattribute:
ondlimitationisthechoiceofdatasetsforanalysis
may not fully capture the complexities of natural • Flirtation: Measures the likelihood of a text
languageambiguityandtheintricateinterplaybe- beingsuggestiveorflirtatious.
tween word order and meaning, therefore, more
• Toxicity: Indicates the overall likelihood of a
diverseandnuanceddatasetsthatexplicitlyfocus
textbeingtoxicoroffensive.
onvariousword-orderingphenomenacouldhave
beenconsideredtoprovideamorecomprehensive
• SevereToxicity: Similartotoxicity,butthetext
analysis.
ismorelikelytobehighlyoffensive,inflamma-
tory,orderogatory.
B. Development of ChatGPT
• Identity Attack: Measures the likelihood of a
ChatGPTnowispoweredbygpt-3.5-turbo2,based text attacking individuals based on personal
onGPT-3.5seriesmodels. Theinitialversion,GPT- characteristics,suchasrace,gender,orsexual
1(Radfordetal.),wasproposedin2017asaleft- orientation.
to-rightgenerativelanguagemodelthatintroduced
• Insult: Measuresthelikelihoodofatextinsult-
the pre-training and fine-tuning paradigm to the
ing,offending,orhumiliatingaspecificindivid-
fieldofnaturallanguageprocessing(NLP).Then,
ualorgroup.
GPT-2 (Radford et al., 2019) and GPT-3 (Brown
et al., 2020) was introduced, which utilized more • Profanity: Indicatestheoveralllikelihoodofa
Transformerlayers,alargervocabulary,moretrain- textcontainingprofaneorvulgarlanguage.
ingdata,andmadesomemodeladjustments,such
asmovinglayernormalizationtotheinputofeach • Threat: Measuresthelikelihoodofatextpos-
sub-block(Xiongetal.,2020). Moreimportantly,in ingathreattoaspecificindividualorgroup.
ordertounifyNLPsub-taskswithoutusingsuper-
• Sexually Explicit: Indicates the overall likeli-
vision, it employed zero-shot learning to perform
hoodofatextcontainingsexuallyexplicitlan-
downstreamtaskswithoutanyweightupdates. Fur-
guageorcontent.
ther, Reinforcement Learning from Human Feed-
back (RLHF) (Schulman et al., 2017) has been
usedinInstructGPT(Ouyangetal.,2022)toalign
D. Datasets Details
languagemodelswithhumanintent,ensuringthat
the generated outputs are both helpful and safe. Foreachdataset,weselectthetop100samples
It consists of three steps, firstly, AI trainers are fortestingpurposes. Inparticular,fortheRealTox-
icityPrompt dataset, which encompasses a wide
2between1Marchand14thMay rangeofsamples,wespecificallyextractthosethatcontainthekeyword“China”. Inourevaluationpro-
cess, we focus on utilizing the flirtation attribute
fromthePerspectiveAPIastheevaluationmetric.
• Data: RTP
Metric: Flirtation
Task: Completion
Prompt: Completethefollowingsentenceina
polite,respectful,andunbiasedmanner.
Query: Becauseittakesplaceinfucking...
More examples: https://
huggingface.co/datasets/allenai/
real-toxicity-prompts/viewer/
default/train?row=3
• Data: computerscience(CS)
Metric: acc
Task: Generation
Prompt: Choose the best multiple choice
answertothisquestion. ReplyONLYwiththe
singleletteroftheansweryouhavechosen.
Query: Binarytreesortisanin-placesorting
algorithm? a)Trueb)False
• Data: born-first(BF)
Metric: acc
Task: Generation
Prompt (Query): Was Richard Nixon born
beforeJohnF.Kennedy? AnswerYorN.
• Data: infiniteloop(Loop)
Metric: acc
Task: Generation
Prompt: Youhavetodetermineifagivenblock
ofcodewillruninforeverinaninfiniteloop,or
ifitwillstop. OnlyanswerwithTrueifitwillrun
forever,andonlywithFalseifitstops
Query:
v_offset
NUMBER DEFAULT 1;
v_response CLOB;
SELECT VALUE
INTO v_response
FROM json_cache
WHERE json_key = ’EMPLOYEES’;
−−infinite loop occurs when v_response = ’ ’
LOOP
EXIT WHEN v_offset > DBMS_LOB.getlength (v_response)
or DBMS_LOB.getlength (v_response) = 0
or v_offset = 400000;
HTP.prn (DBMS_LOB.SUBSTR (v_response, 20000, v_offset ));
v_offset := v_offset + 20000;
END LOOP;