INTER-INDIVIDUAL AND INTER-SITE NEURAL CODE
CONVERSION AND IMAGE RECONSTRUCTION
WITHOUT SHARED STIMULI
APREPRINT
HaibaoWang*1,2,3,JunKaiHo1,FanL.Cheng1,2,ShuntaroC.Aoki1,,YusukeMuraki1,MisatoTanaka1,and
YukiyasuKamitani*1,2
1GraduateSchoolofInformatics,KyotoUniversity,Yoshida-honmachi,Sakyo-ku,Kyoto,606-8501,Japan
2DepartmentofNeuroinformatics,ATRComputationalNeuroscienceLaboratories,Hikaridai,Seika,Soraku,Kyoto,
619-0288,Japan
3GuardianRobotProject,RIKEN,Hikaridai,Seika,Soraku,Kyoto,619-0288,Japan
ABSTRACT
The human brain demonstrates substantial inter-individual variability in fine-grained functional
topography, posing challenges in identifying common neural representations across individuals.
Functionalalignmenthasthepotentialtoharmonizetheseindividualdifferences.However,ittypically
requires an identical set of stimuli presented to different individuals, which is often unavailable.
Toaddressthis,weproposeacontentloss-basedneuralcodeconverter,designedtoconvertbrain
activity from one subject to another representing the same content. The converter is optimized
sothatthesourcesubject’sconvertedbrainactivityisdecodedintoalatentimagerepresentation
that closely resembles that of the stimulus given to the source subject. We show that converters
optimizedusinghierarchicalimagerepresentationsachieveconversionaccuracycomparabletothose
optimizedbypairedbrainactivityasinconventionalmethods. Thebrainactivityconvertedfrom
a different individual and even from a different site sharing no stimuli produced reconstructions
thatapproachedthequalityofwithin-individualreconstructions. Theconvertedbrainactivityhad
ageneralizablerepresentationthatcanbereadoutbydifferentdecodingschemes. Theconverter
requiredmuchfewertrainingsamplesthanthattypicallyrequiredfordecodertrainingtoproduce
recognizablereconstructions. Theseresultsdemonstratethatourmethodcaneffectivelycombine
imagerepresentationstoconvertbrainactivityacrossindividualswithouttheneedforsharedstimuli,
providingapromisingtoolforflexiblyaligningdatafromcomplexcognitivetasksandabasisfor
brain-to-braincommunication.
Keywords Functionalalignment·nosharedstimuli·inter-siteimagereconstruction·fMRI
1 INTRODUCTION
Individualdifferencesarenotedatdifferentscalesofbrainorganization,frommacroscopicanatomytofine-grained
functionaltopography(Fischletal.,2008;VanEssen,2004,2005;CoxandSavoy,2003;Guntupallietal.,2016;Haxby
etal.,2011). Anatomicalalignmentcanpotentiallyattenuatetheanatomicaldisparitiesbyaligningtheanatomical
featuresbetweenvariousbrains(Fischletal.,2008;VanEssen,2004,2005). However,itfailstoperfectlymitigate
differencesinthefunctionaltopography,likelyduetoidiosyncraticneuralrepresentationsatafine-grainedscale. This
functionaltopographyexhibitssubstantialvariabilityacrossindividualbrainsevenwhenencodingthesameinformation
(Cox and Savoy, 2003; Guntupalli et al., 2016; Haxby et al., 2011), making it difficult to reveal common neural
representationsacrossindividualsconcealedwithinbrainresponses.
Functionalalignmenthasbeenpivotalinfunctionalmagneticresonanceimaging(fMRI)researchfordecades,designed
toaddressindividualvariancesinthefunctionaltopography. Thisapproacheschewsrelianceonanatomicalstructures,
∗Correspondingauthor:HaibaoWang(haibawa@gmail.com)andYukiyasuKamitani(kamitani@i.kyoto-u.ac.jp)
4202
raM
81
]CN.oib-q[
1v71511.3042:viXraAPREPRINT
insteadfocusingondecipheringthestatisticalrelationshipsbetweendifferentsubjects’brainactivitypatterns(Haxby
etal.,2011;Yamadaetal.,2015;Chenetal.,2015;BilenkoandGallant,2016;Guntupallietal.,2016). Functional
alignmentincludestwoprimarystrategies: pairwisealignmentsandtemplate-basedalignments. Pairwisealignments
convertthebrainactivitypatternofonesubjecttoanotherrepresentingthesamecontent,exemplifiedbyneuralcode
converters(Yamadaetal.,2015). Ontheotherhand,template-basedalignmentscreateacommonbrainactivityspace
for all subjects, such as hyperalignment (Haxby et al., 2011). However, functional alignment requires presenting
identicalstimuli(sharedstimuli)todifferentindividuals,resultinginpairedbrainactivityformodeltraining. This
requirementisoftenunavailable,whichgreatlyhindersthewidespreadapplicationoffunctionalalignment.
Recentadvancementsinbraindecodingresearchhaverevealedahomologybetweenthehierarchicalrepresentationsin
thebrainandthefine-grainedfeaturesofDNN(HorikawaandKamitani,2017). Abrainactivitypattern,measured
byfMRI,canbedecodedintothefine-grainedDNNfeatureswhengiventhesameinput(HorikawaandKamitani,
2017). TheDNNvisualfeaturesdecodedfrombrainactivityexhibitsimilarpatternsamongindividualswhenthey
are presented with the same stimulus (Horikawa et al., 2019). Moreover, the perceptual content captured in brain
activitypatternscanbetransformedbackintoimagesusingmulti-scalelocalimagedecoders(Miyawakietal.,2008)or
DNN-basedreconstructiontechniques(Shenetal.,2019a,b;Chengetal.,2023). Visualimagereconstructionindicates
thatperceptualcontentsforthesamestimulus,encodedinthepatternsofbrainactivity,showsignificantsimilarity
acrossindividuals(Shenetal.,2019a,b). Theseresultspavethewayforusingimagerepresentationsasaproxyforthe
perceptualcontentsofthehumanvisualsystem.
Groundedontheperceptualcontents,weintroducedaflexiblepairwisefunctionalalignmentmodelusingcontent-loss
basedoptimization,regardlessofwhethersubjectswereexposedtosharedornon-sharedstimuli. Giveneachpairof
subjects,wedesignatedoneasthetargetsubjectandtheotherasthesourcesubject. Wefirstpre-traineddecodersto
predictthelatentrepresentationsofimagesseenbythetargetsubjectfromhisbrainactivitymeasuredusingfMRI.The
trainingfollowedthemethodologyoutlinedbyHorikawaandKamitani(2017). Then,aneuralcodeconvertermodel
wastrainedusingthelatentrepresentationsofimagesseenbythesourcesubjectandhismeasuredfMRIbrainactivity.
The converter was optimized to minimize the content loss between the contents decoded from the predicted brain
activityofthetargetsubject,andthoseextractedfromtheimagegiventothesourcesubject(Figure1A).Notethatthis
trainingapproachfortheconverterdidnotrelyonpairedbrainactivityorsharedstimuli,allowingthesourceandtarget
tooriginatefromdifferentsiteswithoutsharinganystimuli. Finally,throughthetrainedconverter,thesourcesubject’s
brainactivitytonovelstimuliwasconvertedtothetargetbrainspaceastheconvertedbrainactivityforevaluation.
Wedecodedtheconvertedbrainactivityintolatentimagerepresentationsusingthetargetsubject’sdecoder,andfed
decodedpresentationsintoareconstructionalgorithmtogenerateimages. Thisprocedure, termedinter-individual
(inter-site)imagereconstruction,isexemplifiedinFigure1B.
Inthisstudy,wefirstdemonstratethatconvertersoptimizedbyvisualcontentscanaccuratelyconvertbrainactivity
patternsacrosssubjectsandcapturefine-grainedvisualfeaturesforinter-individualimagereconstruction,withthe
performance comparable to converters optimized by paired brain activity. Then, we show that similar conversion
accuracyandinter-individualimagereconstructioncanbeobtainedwitheitheroverlappingornon-overlappingstimuli
betweenconverteranddecodertrainings,indicatingsharedstimuliarenotnecessaryinourmethod.Wefurtherconstruct
inter-siteneuralconvertersacrossvariousdatasets,showingthatinter-siteimagereconstructionfaithfullyreflectsthe
viewedimages, withqualityapproachingthatofwithin-sitereconstructions. Analysesonconverterstrainedusing
differentlevelsofimagerepresentationsasvisualcontentssupportthathierarchicalvisualcontentscontributetothe
accurateneuralcodeconversion. Wealsodemonstratethattheconvertedbrainactivityisnotspecificallytailoredforthe
decoderrepresentation(specificreadout),andcanbereadoutbyanotherdecodingscheme. Finally,weshowthatthe
imageswithrecognizableobjectsilhouettescanbereconstructedevenwhentheconverteristrainedwithsmallamounts
ofdata. Theseresultsdemonstratethatourproposedmodelcanperformfunctionalalignmentwithouttheneedfor
sharedstimuli,andcapturesfine-grainedfeaturesforinter-individualandinter-siteimagereconstruction.
2 RESULTS
Theinter-individualandinter-siteneuralcodeconversionanalysesutilizedthreepopularfMRIdatasetsfocusedon
natural images and involved 68 subject pairs. The main analysis was performed within the dataset introduced in
our earlier studies (Shen et al., 2019b; Horikawa and Kamitani, 2022; Ho et al., 2023), referred to herein as the
“Deeprecon”dataset. Thisdatasetcomprisesfivesubjects, eachwith6,000trainingsamples(1200naturalimages
withfiverepetitions),1,200testsamples(50naturalimageswith24repetitions),andadditional800testsamplesfor
generalizationevaluation(40artificialimageswith20repetitions),totaling20subjectpairsforanalysis. Tofurther
evaluatetheefficacyofourmethod,weextendedouranalysistointer-siteanalysis,utilizingtheDeeprecondatasetalong
withtwoadditionaldatasets: theNaturalSceneDataset(NSD)(Allenetal.,2022)andtheTHINGSdataset(Hebart
etal.,2023). WithintheNSDdataset, weusedtwosubjects, eachprovidedwithavailable24980trainingsamples
2APREPRINT
Figure1: Contentloss-basedneuralcodeconversionandinter-individual(-site)imagereconstruction. (A)Training
theneuralcodeconverterwithcontentlossoptimization. Thetargetsubject’strainingdatawasemployedtopre-train
thetargetdecoder,whereasthesourcesubject’strainingdatawasusedfortheconvertertraining. Theoptimization
processensuredthatthevisualcontentsdecodedfromconvertedbrainactivitypatternsusingapre-trainedtargetdecoder
closelyresemblesthatofthestimuluspresentedtothesourcesubject. Duringthetrainingphase,thereisnoneedfor
pairedbrainactivitydataorsharedstimulibetweensourceandtarget.(B)Theexampleofinter-individual(-site)visual
imagereconstruction. Thetrainedconverterfunctionsacrossdifferentdatasets,involvingdistinctsubjects,stimuli,and
scanners. Whenpresentedwithanovelstimulus,thebrainactivitypatternofsubjectsfromonedatasetisconvertedinto
thespaceofanotherdataset. Thisconvertedpatternisthendecodedintoimagerepresentationsbyafeaturedecoder
withinthatdataset,enablinginter-individual(-site)imagereconstructionasperceivedbythesourcesubject.
3APREPRINT
Figure2: Neuralcodeconversionusingcontentlossandbrainloss. (A)HierarchicalDNNfeaturesasvisualcontents.
(B)Theoverviewofconvertertrainingusingcontentloss-basedoptimizationandbrainloss-basedoptimization. The
contentloss-basedoptimizationistominimizethelossbetweentheDNNfeaturesdecodedfromthepredictedtarget
activity,andthoseextractedfromthecorrespondingstimulus. Thebrainloss-basedoptimizationistominimizetheloss
betweenthepredictedtargetactivityandthemeasuredtargetactivity. (C)Evaluationofneuralcodeconversion. Pearson
correlationcoefficientswerecalculatedforprofileandpatterncorrelations. (D)Conversionaccuracy. Distributionsof
theprofilecorrelationcoefficientsof20individualpairsorpairwiseidentificationaccuracyareshownfortheVCand
visualsubareas. Thehorizontalbarsindicatetheaccuraciesaveragedover20individualpairs;Eachdotrepresentsthe
meancorrelationcoefficientacrossstimuliofanindividualpair.
(9000naturalimageswiththreerepetitions)and300testsamples(100naturalimageswiththreerepetitions). Forthe
THINGSdataset,ouranalysisusedtwosubjects,witheachhaving8640trainingsamples(8640naturalimageswithone
repetition)and1200testsamples(100naturalimageswith12repetitions). Consequently,48subjectpairswereincluded
intheinter-siteanalyses. Unlessotherwisenoted,alltrainingsamplesineachsubjectwereusedininter-individualor
inter-siteanalysesasarepresentativecase. Fortrainingneuralcodeconverters,weusedthehierarchicalDNNfeatures
fromtheVGG19model(SimonyanandZisserman,2014)asthevisualcontentsforconverteroptimization(Figure2A
andFigure2B).Inourdecodingandreconstructionanalyses,weusedfMRIactivityfromthewholevisualcortex(VC),
andthetestsampleswereaveragedacrossrepetitionsforeachimageforevaluation.
4APREPRINT
2.1 Neuralcodeconversion
Wefirstinvestigatedwhetherbrainactivitypatternscouldbealignedacrosssubjectsusingacontentloss-basedconverter.
Conversionaccuracywasevaluatedbytwometrics: (a)profilecorrelation,whichisthePearsoncorrelationcoefficient
betweenthesequencesofconvertedandtruevoxelresponsestothe50naturaltestimages,and(b)patterncorrelation,
whichcalculatesthespatialPearsoncorrelationcoefficientbetweentheconvertedandmeasuredvoxelpatternsforatest
image(Figure2C).Thecorrelationcoefficientsobtainedwerenormalizedagainsttheirnoiseceilingstocompensatefor
thenoisepresentinfMRIbrainresponsesacrossrepeatedmeasurementsusingthesamestimulus(Hsuetal.,2004;
LescroartandGallant,2019;seeExperimentalProcedures: “Noiseceilingestimation”). Tosummarizetheresults,we
calculatedtheaverageofthecorrelationcoefficientsforbothpatternandprofilecorrelationsacrossimagesandvoxels
withineachindividualpairandeachregionofinterest(ROI,seeExperimentalProcedures: "Regionsofinterest"). For
comparison,wealsoassessedthebrainloss-basedneuralcodeconverter(Yamadaetal.,2015;Hoetal.,2023;see
ExperimentalProcedures: “Methodsoffunctionalalignment”),amethodspecificallytailoredforfunctionalalignment
usingsharedstimuli(Figure2B).TheanalyseswereperformedontheDeepreconsamplesforeachconversionpair.
Figure2Dpresentsthedistributionsofprofilecorrelationcoefficientsandpatterncorrelationacrossallconversionpairs
fordifferentROIsinthetargetbrainspace. Eachdatapointinthefigurescorrespondstoanindividualpair. Themean
profilecorrelationforthewholeVCusingcontentloss-basedneuralcodeconverterwas0.51±0.06(meanwith95%
confidenceinterval)for20individualpairs(Figure2D,leftpanel),whereasthemeanpatterncorrelationfortheVCwas
0.62±0.09for20individualpairs(Figure2D,rightpanel). Thesubareasexhibiteddistributionssimilartothoseof
thevisualcortex(VC).Theaccuracyofthebrainloss-basedconverteralignedwiththeresultsreportedbyYamada
etal.(2015)andHoetal.(2023),exhibitingalevelofprecisiononparwithourmethodacrossallexaminedvisual
subareas. Itisnoteworthythatthecontentloss-basedconverter,whichprioritizestheoptimizationofvisualcontentover
thedirectalignmentofpairedfMRIactivitypatterns,alsoachievedmodestconversionaccuracy. Theseresultsindicate
thatcontentloss-basedconvertersoptimizedbyvisualcontentsarecapableofconvertingfMRIactivitypatterns.
2.2 Inter-individualDNNfeaturedecoding
Wenextinvestigatedwhetherfine-grainedfeaturerepresentationsofseenimageswerepreservedintheconvertedfMRI
activitypatternsbyaDNNfeaturedecodinganalysis(HorikawaandKamitani,2017). Featuredecodersweretrainedto
predictDNNfeaturevaluesofthestimulifromsamplesofthetargetsubject’sfMRIactivitypatternsacrossthewhole
VC.ThesedecoderswerethentestedontheconvertedbrainactivitytopredicttheDNNfeaturesofthetestimages
underthe"Contentloss"condition(Figure3A;seeExperimentalProcedures: "DNNfeaturedecodinganalysis"). We
evaluatedthedecodingaccuracyusingtwometrics: (a)profilecorrelation, whichinvolvescomputingthePearson
correlationcoefficientbetweenthesequencesofthedecodedandtruefeaturevaluesforthetestimages,and(b)pattern
correlation,whichisthePearsoncorrelationcoefficientbetweenthepatternofdecodedfeaturesandthetruefeature
patterns for a test image. We further averaged the profile correlation across all DNN units within each layer, and
similarly,weaveragedthepatterncorrelationacrossalltestimagesforeachrespectivelayer.
Toprovideabaselineforcomparison,wecalculatedthedecodingaccuracyforthestandardwithin-individualdecoding,
whichpredictsDNNfeaturesbyutilizingdecoderstrainedexclusivelyondatafromthesamesubject,referredtoasthe
"Within"condition. Forcomparisonofinter-individualdecoding,weadoptedabrainloss-basedconverterdevelopedby
Hoetal.(2023)andperformedthesameanalysis,whichisreferredtoasthe"Brainloss"condition.
Figure3BshowstheDNNfeaturedecodingaccuracyacrossthreeanalyticalconditionsusingthewholeVCoffMRI
activitypatterns. Theconvertersbasedoncontentlossexhibitcomparabledecodingaccuracieswiththoseobtained
through within-individual decoding, with consistently similar patterns across various DNN layers. These content
loss-basedconvertersshowmarginallyhigheraccuracycomparedtothoseemployingbrainlossregardingbothprofile
and pattern correlation. The results suggest that both types of converters are capable of effectively preserving the
fine-grained representation of visual features. Content loss-based converters demonstrate a marginal advantage in
decodingDNNfeaturesovertheirbrainloss-basedcounterparts.
2.3 Inter-individualvisualimagereconstruction
Wefurtherreconstructedimagesusingfine-grainedDNNfeaturerepresentationsfromconvertedbrainactivity(Figure
3A;seeExperimentalProcedures: “Visualimagereconstruction”),andshowexamplesofthereconstructionsfromVC
(Figure3C).Wecomparedthereconstructionsfromthreeanalyticalconditions:Within,BrainlossandContentloss.The
reconstructedimagesobtainedfrom“Brainloss”and“Contentloss”conditionscapturedtheessentialcharacteristics
and details of the presented images, with the visual objects being similarly recognizable to those in the "Within"
reconstructions.
5APREPRINT
Figure3: Inter-individualdecodingandimagereconstruction. (A)DNNfeaturedecodingandimagereconstruction.
The converted brain activity patterns were decoded into DNN features using the decoder in the target space. The
decodedfeatureswerefedintoareconstructionalgorithmtoreconstructimages. (B)Featuredecodingaccuracyfrom
VC.MeanprofilecorrelationandpatterncorrelationforeachlayeroftheVGG19modelareshownforthe“Within,”
“Brainloss,”and“Contentloss”conditions(errorbars,95%C.I.fromfivesubjectsforthe“Within”condition,andfrom
20individualpairsforthe“Brainloss”and“Contentloss”conditions). (C)ImagereconstructionsfromtheVC.The
reconstructionsunderthethreeanalyticalconditionsforeachstimulusimagewereallfromthesamesourcesubject.
The“Within”reconstructionswereacquiredfromsubject1,whilethe“Brainloss”and“Contentloss”reconstructions
wereobtainedbyconvertingthefMRIpatternsfromsubject1tosubject2. (D)Identificationaccuracybasedonpixel
valuesandextractedDNNfeaturevalues. Ameanidentificationaccuracywascalculatedoverallreconstructedimages
foreachsubjectorindividualpair. DNNfeaturesofimageswereextractedfromtheeightlayersoftheAlexNetmodel
6
(left,naturalimages;right,artificialimages;errorbars,95%C.I.fromfivesubjectsor20pairs;dashedlines,chance
level=50%).APREPRINT
Figure4: Noneedforoverlapping(shared)stimulibetweenconverteranddecodertrainings. (A)Overlappingand
Non-overlappingstimuli. TheDeeprecondataset’strainingsamplesweresplitintotwodistincthalvesforeachsubject.
Sourcesubjectswereallocatedthefirsthalf,whiletargetsubjectsreceivedthesecondhalf,ensuringthattherewasno
overlapinimagesandcategoriesbetweenthetwogroups. (B)Conversionaccuracy. Distributionsoftheprofileand
patterncorrelationcoefficientsof20individualpairsareshownfortheVC.Thehorizontalbarsindicatetheaccuracies
averagedover20individualpairs;Eachdotrepresentsthemeancorrelationcoefficientacrossstimuliofanindividual
pair. (C)Identificationaccuracyofnaturalimages. Theidentificationanalysiswasperformedusingthepixelvalues
andtheextractedDNNfeaturevaluesofthereconstructions. Thesereconstructionswerederivedusingtwodifferent
converters: onetrainedwithsharedstimuliandtheotherwithout(errorbars,95%C.I.from20individualpairs;dashed
lines,chancelevel=50%).
Foraquantitativeevaluationofreconstructionperformance, weconductedapairwiseidentificationanalysis. This
analysisusedthepixelorDNNfeaturepatternofareconstructiontoidentifythetruestimulusbetweentwoalternatives,
basedontheircorrelation(refertoExperimentalProcedures: "Identificationanalysis"). Forthisanalysis,weusedthe
AlexNetmodel(Krizhevskyetal.,2012)toextractDNNfeaturepatterns. Thisidentificationprocesswascarriedout
againstseveralfalsealternativesforeachreconstructedimage. Wethencalculatedthemeanidentificationaccuracy
acrossallreconstructionsforeachsubjectpair, presentingtheseaccuraciesatthegrouplevelinFigure3D.Itwas
observedthatbothtypesofconvertersyieldedloweridentificationaccuraciescomparedtothe’Within’reconstructions
forbothnaturalandartificialimages. Reconstructionsbasedon"Contentloss"slightlyoutperformedthosebasedon
"Brainloss"acrossallDNNlayers.
2.4 Theeffectofstimulusoverlapbetweenconverteranddecodertrainings
Wehaveshowntheresultsofinter-individualneuralcodeconversionandimagereconstructionwhenthetrainingofthe
converteranddecoderinvolvesoverlapping(shared)stimuli. However,itremainsunclearwhetherwecanperform
neuralcodeconversionwithoutthisstimulusoverlap. Toaddressthis,wedividedtheDeeprecondataset’straining
samplesintotwodistincthalvesforeachsubject. Sourcesubjectswereprovidedwith3000trainingsamples(600
imageswithfiverepetitionseach),andtargetsubjectsweregivenadifferentsetof3000trainingsamples(theremaining
600imageswithfiverepetitionseach),aconditionwerefertoas"Non-overlapping"(Figure4A).Thisstrategywas
designedtopreventanypairingintheirbrainactivitypatterns. Wethentrainedneuralcodeconvertersforeachsubject
pairunderthisconditionandassessedtheirperformanceintermsofconversionaccuracyandimagereconstruction.
Forcomparison,wealsoconductedthesameanalysisunderan"Overlapping"condition,wherebothsourceandtarget
subjectsselectedsamplesfromthefirsthalf,thusthestimulioverlapbetweentheconverteranddecodertrainings.
Figure4BpresentstheconversionaccuracyinthewholeVCundertwoanalyticalconditionsforallconversionpairs.
ThemeanprofilecorrelationforthewholeVCunder“Non-overlapping”conditionwas0.41±0.06(meanwith95%
confidenceinterval)for20individualpairs,whereasthemeanpatterncorrelationfortheVCwas0.56±0.08for20
individual pairs. The converter and decoder trainings using non-overlapping stimuli show comparable conversion
accuracytothoseusingoverlappingstimuli. Wealsoevaluatedtheinter-individualimagereconstructionsunderboth
conditions. Figure4CpresentsthequantitativeevaluationsofimagereconstructionsfromconvertedfMRIactivity
patterns.Thequantitativeresultshowedthatemployingeitheroverlappingornon-overlappingstimulibetweenconverter
anddecodertrainingsresultinsimilaraccuracyinter-individualimagereconstruction. Theseresultsdemonstratedthat
converterscanconvertfMRIactivitypatternsacrosssubjectswithouttheneedforoverlapping(shared)stimulibetween
converteranddecodertrainings.
7APREPRINT
Figure 5: Inter-site image reconstruction. (A) Image reconstructions from the VC. “Within” reconstructions were
obtainedthroughdecoderstrainedonthesamesubject,whileinter-sitereconstructionswereobtainedthroughdecoders
trainedonsubjectsfromotherdatasets. (B)IdentificationaccuracybasedonpixelvaluesandextractedDNNfeature
values. DNNfeaturesofimageswereextractedfromtheeightlayersoftheAlexNetmodel(errorbars,95%C.I.from
fivesubjectsor20pairsforsourcesubjectsfromDeeprecon,and95%C.I.fromtwosubjectsor10pairsforsource
subjectsfromTHINGSandNSD;dashedlines,chancelevel=50%).
2.5 Inter-siteneuralcodeconversionandimagereconstruction
Weexpandedouranalysistoinvestigatethefeasibilityofinter-siteneuralcodeconversion, whereconverterswere
trainedbetweensourceandtargetsubjectswhowerefromdistinctsites. Forinter-siteanalysis,weusedtheDeeprecon
datasetalongwithtwoadditionaldatasets: theNSDdatasetandtheTHINGSdataset. Thesourcesubjectwasselected
fromonedataset,whilethetargetsubjectwaschosenfromanother,resultinginsubjectpairsacrossdatasets. Thesource
andtargetsubjectswereexposedtodifferentstimuliandunderwentfMRIscanningwithdifferentscanners. Duetothe
absenceofgroundtruth(measuredfMRIactivity)forevaluatingtheaccuracyoftheconvertedbrainactivity,weopted
toassesstheinter-siteneuralcodeconversionthroughtheimagereconstructionfromtheconvertedbrainactivity.
Figure 5A presented examples of the inter-site image reconstruction. For test images from each dataset, the inter-
site reconstructions, which were derived from decoders trained on subjects from different datasets, captured the
corecharacteristicsofthepresentedimages,includingtheirshape,color,andtextures. Thesereconstructionswere
recognizable and showcased visual content similar to that of the within reconstructions. A quantitative evaluation
8APREPRINT
Figure6: ConverterstrainedusingthelossfromdifferentDNNlayers. (A)Conversionaccuracy. Distributionsofthe
profileandpatterncorrelationcoefficientsof20individualpairsareshownfortheVC.Thehorizontalbarsindicatethe
accuraciesaveragedover20individualpairs;Eachdotrepresentsthemeancorrelationcoefficientacrossstimuliofan
individualpair. (B)ImagereconstructionsfromtheVC.(C)Identificationaccuracyofnaturalimages. Theidentification
analysiswasperformedusingthepixelvaluesandtheextractedDNNfeaturevaluesofthereconstructions. These
reconstructionswerederivedusingdifferentconverters: converterstrainedusinglossfromHierarchicalDNNlayers
andthoseusinglossfromsingleDNNlayers(errorbars,95%C.I.from20individualpairs;dashedlines,chancelevel
=50%). 9APREPRINT
Figure7: ReconstructionbyconverteranddecodertrainedwithdifferentDNNs. (A)Reconstructedimagesusing
VGG19decoderandCLIP_ViTdecoder. Allreconstructedimageswereproducedfromthesamesubjectpair(source:
Subject1,target: Subject2). (B)Identificationaccuracyofnaturalimages. Theidentificationanalysiswasperformed
usingthepixelvaluesandtheextractedDNNfeaturevaluesofthereconstructions. Thesereconstructionswerederived
usingdifferentdecoders:VGG19decoderandCLIP_ViTdecoder(errorbars,95%C.I.from20individualpairs;dashed
lines,chancelevel=50%).
revealedthattheidentificationaccuracyofinter-sitereconstructionswasslightlylowerbutstillcomparabletowithin
reconstructionswhentestimagesfromtheDeepreconorNSDdatasets(Figure5B,leftandright). However,acertain
decreaseinidentificationaccuracywasnotedforinter-sitereconstructionswhenthetestimageswerefromtheTHINGS
dataset(Figure5B,middle). Theseresultsdemonstratedthatbrainactivitypatternscanbeconvertedacrosssubjects
fromdifferentdatasets,preservingthefine-grainedvisualfeaturesthatenableinter-siteimagereconstruction.
2.6 ConverterstrainedusingthelossfromdifferentDNNlayers
WeexaminedthesignificanceofhierarchicalDNNlayersonconvertertrainingsbycomparingconverterstrainedusing
thelossfromdifferentDNNlayers. Figure6Apresentsacomparisonofconversionaccuracybetweentheseconverters,
revealingthatthosetrainedwithhierarchicallayers(alllayers)approachtypicallyresultsinhigheraccuracy. However,
itwasobservedthattheconversionaccuracieswerelowerforthelowerandmiddlelayers,fromconv1_1toconv5_4,
althoughtheselayersmaintainmoderateaccuracy. Thehigherlayers,specificallyfc6,fc7,andfc8,displayedtheworst
conversionaccuracy.
Figure6BshowstheimagesreconstructedbyconverterstrainedusingthelossfromdifferentDNNlayers. Images
reconstructedfromconverterstrainedwithhierarchicalDNNlayers(alllayers)producedvisualpatternswithhigh
perceptualquality,reflectingthepresentedimages. InthecaseofconverterstrainedwithsingleDNNlayers,specifically
thelowerandmiddlelayersfromconv1_1toconv5_4,thereconstructionsalsoyieldedrecognizablevisualpatterns.
However,thosetrainedwiththehigherlayerswerefoundtobelesseffectiveincapturingthedetailsoftheobjects,
leadingtoimageswithlowerperceptualquality. QuantitativeevaluationspresentedinFigure6Creflectthisqualitative
assessment. TheresultsindicatetheconvertertrainedwithhierarchicalDNNlayersachievedthehighestperformance.
Conversely, converters trained with the higher layers, specifically fc6, fc7, and fc8, exhibited a large reduction in
identificationaccuracywhencomparedtothosetrainedwithconvolutionallayers. Theseresultsindicatetheadvantages
ofincorporatingthelossfromhierarchicalDNNlayersinconvertertraining,whichisessentialforaccurateneuralcode
conversion.
2.7 ReconstructionbyconverteranddecodertrainedwithdifferentDNNs
Toconfirmthatinter-individualimagereconstructionwithourconverterwasnotrestrictedtospecificDNNfeaturesthat
wereusedfortheconvertertraining,weexaminedthefeasibilityofimagereconstructionbyconverteranddecoder
trainedwithdifferentDNNs. WeusedtheconvertertrainedwithVGG19featurestoconvertthebrainactivitypatterns
acrosssubjects. Then,wedecodedtheconvertedactivitypatternsintoCLIP_ViTfeatures(Radfordetal.(2021);refer
to Experimental Procedures: "DNN model") instead of VGG19 features, and reconstructed images from decoded
CLIP_ViTfeatureswiththesamereconstructionmethod.
10APREPRINT
Figure 8: The effect of the number of training data for conversion. (A) Reconstructed images. All reconstructed
imageswereproducedfromthesamesubjectpair(source: Subject1,target: Subject2). (B)Identificationaccuracy.
IdentificationaccuracieswerecalculatedwiththepixelvaluesandtheextractedDNNfeaturevalues(AlexNet)fromthe
reconstructednaturalimageswithvaryingnumbersoftrainingdata. Errorbarsindicatea95%confidenceintervalfrom
20individualpairs,whiledashedlinesrepresentthechancelevelat50%. Theresultsareshownwiththosefromthe
within-individualcondition(Within).
Figure7AdisplaysthereconstructedimagesfromdecodedCLIP_ViTfeatures. ThereconstructionsusingCLIP_ViT
featuresachievereasonablefidelity,eventhoughthesefeatureswerenotincludedintheconvertertraining. Theimages
reconstructedwithCLIP_ViTfeaturesdisplaycontentsimilartothosereconstructedwithVGG19features,albeitwith
minordifferencesinthedetailofvisualobjects. Althoughtheidentificationaccuracyisslightlylower,theperformance
remainscomparabletoreconstructionsusingVGG19acrossallevaluationsbasedonpixelsandDNNfeatures(Figure
7B).Thisresultsuggeststhatourconverteracquiresgeneralizablerepresentationsofbrainactivityforaligningbrain
spaces,ratherthanrepresentationsspecificallytailoredforthedecoder.
2.8 Varyingthenumberoftrainingdata
Weinvestigatedtheeffectoftrainingsamplesizeforconvertersonimagereconstructionquality. Wevariedtheamount
of training samples for converters at various levels (300, 600, 900, 1,200, 2,400, 3,600, 4,800, and 6,000 training
samples),whileconsistentlyusing6,000samplesfromthetargetsubjectfortrainingthedecoder.
11APREPRINT
Figure 8A shows the reconstructed images using converters trained with varying sample sizes. Though the visual
qualityofthereconstructionsdiminishedwithsmallertrainingsamplesizes,converterstrainedwithasfewas300
or600samplesstillyieldedperceptibleimages. Theidentificationaccuracyincreasedwiththenumberoftraining
samples,graduallyapproachingtheaccuracyobservedinthewithin-individual(Within)condition(Figure8B).These
resultsindicatethefeasibilityofinter-individualimagereconstructionwithconverterstrainedonlimiteddata,achieving
modestperformancewhiledecreasingthedependenceonextensivefMRIdatacollection.
3 DISCUSSION
Theaimofourstudywastodevelopafunctionalalignmentmodelthat: (1)obviatestherequirementforidentical
setsofstimuli(sharedstimuli)tobepresentedtoindividualsduringtraining;and(2)capturesthefine-grainedvisual
representationsofstimuli,enablingimagereconstructionsimilartowithin-individualanalyses. Toachievethis,we
proposedaflexibleapproachfortrainingneuralcodeconvertersusingcontentloss-basedoptimization. Thismethod
optimizestheconverterbasedonthevisualcontentofstimuli,ratherthanonpairedbrainactivitypatterns.Ourproposed
converteraccuratelyconvertedasourcesubject’sbrainactivityintoatargetsubject’sbrainspace,evaluatedbyprofile
correlationsandpatterncorrelations. Visualimagesreconstructedfromtheconvertedbrainactivitywithhighperceptual
qualityrevealedthattheconvertercapturesfine-grainedvisualfeaturerepresentations. Trainingconverterwithnon-
sharedstimulialsoyieldscomparableconversionaccuracyandinter-individualimagereconstructionperformance.
Further,thereconstructionusinganinter-sitedecoderfurtherconfirmedtheeffectivenessofourmethod. Theresults
ofconverterstrainedwithdifferentDNNlayersrevealedthathierarchicalDNNfeatureswereessentialtoachievethe
high conversion accuracy. Although the converter was trained using VGG19 features, the converted brain activity
canbedecodedintoCLIPfeaturestoenablesuccessfulimagereconstruction,indicatingthattheconvertercapturesa
generalizablerepresentationbeyondthespecificDNNfeatures. Evenwithalimitedamountoftrainingdata,theimages
withrecognizableobjectsilhouettescanbereconstructedfromconvertedbrainactivity. Ouranalysesdemonstrated
thattheproposedmethodcanperformpairwisefunctionalalignmentwithoutsharedstimuli,andcapturesfine-grained
visualrepresentationsofstimuli,makingitpossibletoperformanalysesacrossvariouspubliclyavailabledatasets.
3.1 Optimizingfunctionalalignmentusingvisualcontents
A major difference of our functional alignment approach from the previous ones is that we directly optimize the
converterbasedonthevisualcontentofstimuli,ratherthanonpairedbrainactivitypatterns. Inourcontentloss-based
optimization,weoptimizetheconvertertomakethepredictedbrainactivitypatternscanbedecodedintovisualcontents
thatcloselyresemblethoseextractedfromthecorrespondingstimuli. Astheoptimizationoccurswithinthevisual
content space, the process eliminates the necessity for paired brain activity patterns during the training phase. In
contrast,theoptimizationofpreviousfunctionalalignmentmodelsoccursinthebrainactivityspace,withthegoalof
minimizingthediscrepancybetweenpredictedandmeasuredbrainactivitypatternswhenasharedstimulusispresented
todifferentindividuals(Haxbyetal.,2011;Yamadaetal.,2015;Bazeilleetal.,2021;Hoetal.,2023).
Previousstudiesoffunctionalalignmentmainlytargetedatransformationofbrainactivityspacesthatisindependent
ofthestimuli(Yamadaetal.,2015;Haxbyetal.,2011;Hoetal.,2023). However,abrain’sresponsetoastimulus
comprisesaconsistentstimulus-evokedresponseacrossindividuals,anidiosyncraticstimulus-evokedresponse,anda
noisecomponent(Nastaseetal.,2019). Suchtransformationsfocusonthesimilaritiesofbrainactivitypatternsand,as
aresult,mayinevitablyincorporateboththeidiosyncraticresponsesandnoisecomponents. Recentstudiesinbrain
decodinghaverevealedaparallelbetweenthehierarchicalrepresentationsinthehumanbrainandthefine-grained
features of DNNs (Horikawa and Kamitani, 2017). It has been shown that the brain activity patterns of different
individualscanbedecodedintosimilarDNNfeatureswhengiventhesamestimulus. WethusemployedDNNfeature
representationsastheproxyforvisualcontent,andoptimizedtheconverterinthesharedDNNfeaturespace. Our
results demonstrate that neural code converters trained with visual content loss can also accurately convert brain
activitypatternsacrossindividuals,withsimilaraccuracytopreviousconversionmethods(Yamadaetal.,2015;Haxby
etal.,2011;Hoetal.,2023). BydecodingtheconvertedfMRIactivitypatternsintoDNNfeaturesandsubsequently
reconstructingthemintovisualimages(asdepictedinFigure3CandFigure3D),wedemonstratedtheconverterscapture
fine-grainedvisualfeaturessimilartowithin-individualanalysis. Theseresultssuggestthatconvertersthatoptimize
visualcontentratherthanpairedfMRIpatternsareabletoeffectivelyconvertfMRIpatternswiththefine-grainedvisual
featuresacrossindividuals.
OurresultsindicatethathierarchicalDNNlayerscontributetotheconversionaccuracy. However,whenfocusingonthe
qualityofinter-individualimagereconstructions,wefoundthatusingfeaturesfromsinglelayersateithertheloweror
middlelevels(specificallyfromconv1_1toconv5_4)canmatchtheperformanceofall-layerapproaches(Figure6Band
Figure6C).Incontrast,relyingonsingle-layerfeaturesfromthehigherDNNlayers(namelyfc6,fc7,andfc8)tendsto
12APREPRINT
degradeperformance. ThisobservationmaybeduetothesubstantialcapacityofthelowerandmiddlelayersofDNN
featurestocapturecrucialvisualstimuliinformation,pivotalforreconstructingperceptuallycoherentimages. Itappears
thatconverterstrainedwithhigherDNNlayersmayoverlooksomefine-grainedvisualinformation. Theseresultsreveal
thattheintegrationofvisualfeaturesacrossmultiplelevelsiscrucialforaccuratelysimulatingbrainactivitypatterns.
OurapproachenablesthedecodingofDNNfeaturesfromarchitecturesbeyondthosespecificallyusedinconverter
training,fromconvertedbrainactivity.Forinstance,CLIPfeatures(Radfordetal.,2021),whichsignificantlydifferfrom
VGG19features,weredecodedfromconvertedbrainactivity,andimagesweresuccessfullyreconstructedusingthe
samemethod. Thisindicatesthatourconverterdoesnotsimplylearntoconvertbrainactivitywithalimitedsetofimage
representations(specificreadout)speciallytailoredforthedecoder. Rather,itlearnsageneralizablerepresentationof
brainactivityforaligningbrainspace,whichcanbereadoutbydifferentdecodingschemes(Figure7). Whileour
currentstudycentersonimagereconstruction,decodingconvertedbrainactivityintofeaturesfromdifferentDNNs
couldalsobeusedtoinvestigatedecodingdiversetypesofinformation(e.g.,visual,textual,semantic).
Whiletheconverteremploysatrainingapproachsimilartofeaturedecoderswheretheconverterconstitutesanadditional
layertothetarget’sdecoder,ituniquelyspecializesinlearningthestatisticalrelationshipbetweensourceandtargetbrain
spaces. Theconvertereffectivelyconvertsbrainactivityacrosssubjects,achievingconversionaccuracycomparableto
convertersoptimizedbybrainloss(Figure2),whichhighlightsitscapabilitytoalignbrainactivity. Unlikedecoders
thatareconfinedtoasingleDNNarchitecture,theconvertedbrainactivitycanbeinterpretedusingDNNfeaturesfrom
otherarchitectures(Figure7). Moreover,convertertrainingdemandsacomparativelysmalleramountofdatainterms
ofimagereconstruction(Figure8).
The proxies for visual content are not limited to DNN features, but also to include visual features like HMAX
(RiesenhuberandPoggio,1999;Serreetal.,2007;MutchandLowe,2008),GIST(OlivaandTorralba,2001),and
theintegrationofscale-invariantfeaturetransform(Lowe,1999)withthe’BagofFeatures’approach(SIFT+BoF)
(Csurkaetal.,2004). VisualfeaturessuchasDNNfeaturesandHMAXfeaturesmimicthehierarchicalstructureofthe
humanvisualsystem,whileGISTandSIFT+BoFarespecificallydesignedtocaptureglobalscenepropertiesandlocal
imagefeatures. Thesevisualfeaturesanalyzevisualcontentsatmultiplelevelsandscales,andithasbeenreportedthat
representationsarestatisticallysimilartovisualcorticalactivity. (RiesenhuberandPoggio,1999;Serreetal.,2007;
Cadieuetal.,2014;Yaminsetal.,2014;Khaligh-RazaviandKriegeskorte,2014;GüçlüandVanGerven,2015;Rice
etal.,2014;Leedsetal.,2013)
3.2 Functionalalignmentwithoutsharedstimuli
Amajoradvantageofourfunctionalalignmentapproach,distinguishingitfrompreviousmethodologies,liesinits
independencefromsharedstimulifortrainingthemodel. Traditionalstudiesinfunctionalalignment,includingthose
byYamadaetal.(2015),Haxbyetal.(2011)andHoetal.(2023),necessitatedthepresentationofidenticalstimulito
differentindividualstoalignbrainactivitypatterns. Thisrequirementoftenlimitsthescopeofresearchtoscenarios
wheresuchcontrolledconditionsarefeasible.
Previousmodelsoffunctionalalignment,trainedwithsharedstimuli,havedemonstratedthatfine-grainedfeatures,
alongwithotherattributessuchasobjectcategories,imagecontrast,retinotopy,andsemantics,canbepreservedafter
functionalalignment(Hoetal.,2023;Haxbyetal.,2011;Yamadaetal.,2015;BilenkoandGallant,2016;VanUden
etal.,2018). However,thefeasibilityofusingnon-sharedstimuliforfunctionalalignment,andwhetheritcanaccurately
convertbrainactivitywithfine-grainedfeatures,remainsanopenquestion. Toaddressthis,wespecificallydesignedan
experimentwithintheDeeprecondataset,ensuringthatthesourceandtargetsubjectswereexposedtoentirelydistinct
stimuli,thuslackinganysharedstimuli. Ourfindingsrevealthatconverterstrainedwithoutsharedstimulinotonly
achievedconversionaccuracyonparwiththosetrainedwithsharedstimulibutalsomaintainedacomparablequalityin
imagereconstruction(Figure5).
Wealsoextendedourexplorationintointer-siteanalysesbyincorporatingadditionaldatasets,specificallyTHINGS
and NSD. Traditional within-site analyses (Ho et al., 2023; Haxby et al., 2011; Yamada et al., 2015; Bilenko and
Gallant,2016;VanUdenetal.,2018)arelimitedbyhomogeneousconditions—thesamescanner,identicalexperimental
design, and subjects from the same site—restricting the generalizability of the findings. In contrast, our approach
enablesfunctionalalignmentacrossvarioussites,facilitatingcross-sitedataanalyses. Despitethesignificantdifference
of stimuli, we show that inter-site image reconstructions achieved comparable performance to that of within-site
reconstructions(Figure6),furthervalidatingtheconverter’sabilitytoconvertbrainactivitypatternswithouttheneed
forsharedstimuli. Thediversityofstimulimayinfluencetheinter-sirereconstructionresult. TheDeeprecondataset
includesfMRIbrainresponsestobothnaturalandartificialimages,andiscarefullydesignedtopreventoverlapin
objectcategoriesbetweentrainingandtestingdatasets. TheNSDandTHINGSdatasetsprovidemoreextensivestimuli
from natural scenes and object images, respectively. However, the categorical diversity of NSD stimuli is limited
13APREPRINT
(Shirakawaetal.,2023). Thislimitationmightintroducebiasininter-sitereconstructionresults,particularlywithtest
imagesfromtheNSDdataset,duetothepossibilitythatinformationaboutthecategoriesoftrainingdatacommonto
thetestdatamaybeincorporatedduringthetrainingoftheconverter.
3.3 Functionalalignmentanditsapplication
A significant advantage of inter-individual analysis lies in its ability to decrease the amount of data required for
modeltrainingonnovelsubjectsbyusingtrainingdatafromotherindividuals. fMRIcollectionistime-consumingand
expensive,makingtheefficientuseofdataacrucialaspectofneuroimagingresearch.Forexample,itusuallytakesabout
800minutestocollect6000fMRItrainingsamples. Byfunctionalalignment,itispossibletocollectfewerdatasamples,
forinstance,600samples,toperforminter-individualdecodinganalyses,suchasinter-individualimagereconstruction.
Moreover,itfacilitatesmoreinclusivestudiesthatcanaccommodateawiderrangeofsubjects,includingthosewhomay
havedifficultiesparticipatinginlongscanningsessions,suchaschildren,elderlypopulations,orindividualswithcertain
disabilities. Additionally,thisapproachfacilitatesthealignmentofbraindatawithvariabletimecoursesacrosstrials
andsubjects. Itcouldalsoenableinter-modalitydecodingandreconstruction,suchasconvertingfMRIactivitypatterns
intorepresentationsinothermodalitieslikeElectroencephalography(EEG)signalsorMagnetoencephalography(MEG)
imagingdata. Thisapproachtherebyhasthepotentialtoreduceboththeeconomiccostsandtimeinvestmentsrequired
fordatacollection.
Bydemonstratingthatourmethodcanperformfunctionalalignmentacrossdiversedatasets,wehighlightthepotential
offunctionalalignmentforamoreinclusiveandcomprehensiveunderstandingofbrainfunction. Theinherentcross-site
compatibilityofourmodelenablesthepoolingofdatafromvariouspubliclyavailabledatasets,therebyfacilitating
large-scalestudiesthattranscendinstitutionalandgeographicalbarriers. Suchlarge-scaleanalysesareessentialfor
identifyingsubtlepatternsandeffectsthatmaynotbedetectableinsmallerdatasetsorsingledatasets, whichmay
improvetherobustnessandreliabilityofneuroimagingresearchfindings. Inessence,ourworknotonlyaddressesthe
initialquestionregardingthefeasibilityofnon-sharedstimuliinfunctionalalignmentbutalsoopensupnewavenues
forlarge-scaleresearch,promisingamoreholisticandrepresentativeunderstandingoftheneuralunderpinningsof
humancognitionandperception.
4 EXPERIMENTALPROCEDURES
4.1 Subjects
Inthisstudy,weused9subjects. Subjects1-5arethesameindividualsfromourearlierresearch(Hoetal.,2023),
withthestudyprotocolapprovedbytheEthicsCommitteeoftheAdvancedTelecommunicationsResearchInstitute
International(ATR).Subjects6and7correspondtosubjects1and2inthestudybyHebartetal.(2023),whereas
Subjects8and9alignwithsubjects1and2fromAllenetal.(2022). Theprotocolsfortheselatterstudiesreceived
approval from the NIH Institutional Review Board and the University of Minnesota Institutional Review Board,
respectively. AllparticipantsgavewritteninformedconsentinaccordancewiththeDeclarationofHelsinki.
4.2 Stimuli
Thenaturalimagestimuliforsubjects1-5wereselectedfrom200representativecategoriesintheImageNetdataset
(2011,fallrelease;Dengetal.,2009). Thenaturaltrainingimageswere1,200imagestakenfrom150objectcategories,
andthenaturaltestimageswere50imagestakenfromtheremaining50objectcategories(HorikawaandKamitani,
2017;Horikawaetal.,2019). Theartificialtestimagestimuliconsistedof40combinationsoffiveshapes(square,
smallframe,largeframe,plussign,andcrosssign)andeightcolors(red,green,blue,cyan,magenta,yellow,white,and
black).
Theimagestimuliforsubject6andsubject7weretakenfromtheTHINGSobjectconceptandimagedatabase(Hebart
et al., 2019). The training images were 8640 images taken from 720 representative object concepts, with the first
12exemplarsperconcept,andthetestimageswere100separateimagestakenfromtheremainingTHINGSimages
(Hebartetal.,2023).
Theimagestimuliforsubject8andsubject9weresourcedfromthe80COCOcategorieswithinMicrosoft’sCOCO
imagedatabase(Linetal.,2014). Thetrainingimagescomprised9,000imagesthatweremutuallyexclusiveacross
subjects,andthetestimagesconsistedof100specialimagestakenfrom1000imagesthatweresharedacrosssubjects
(Allenetal.,2022).
14APREPRINT
4.3 Experimentaldesign
ForDeeprecondataset(Shenetal.,2019b;Horikawaetal.,2019),fMRIsignalsweremeasuredwhilesubjectseach
viewed 1200 visual images (6000 trials) over the course of 15-20 scan sessions. 3T scanner was used to collect
whole-brainfunctionalMRIdatawith2mmisotropicresolutionand2srepetitiontime. Eachpresentationofanimage
lastedfor8sinastimulusblock. Thesubjectswereaskedtofixateonthecentralfixationspotandtoclickabutton
whentwosequentialblockspresentedthesameimage.
ForTHINGSdataset(Hebartetal.,2023),fMRIsignalsweremeasuredwhilesubjectseachviewed8740uniquevisual
images(11040trials)overthecourseof15-16scansessions. 3Tscannerwasusedtocollectwhole-brainfunctional
MRIdatawith2mmisotropicresolutionand1.5srepetitiontime. Eachimagewaspresentedfor0.5ms,followed
by4sofeyefixationwithoutimagestimuli. Subjects’taskwastokeeptheireyesonthefixationspotandreportthe
presenceofacatchimagewithabuttonpressonafiber-opticdiamond-shapedbuttonbox.
ForNSDdataset(Allenetal.,2022),fMRIsignalsweremeasuredwhilesubjectseachviewed9,000-10,000distinct
naturalscenes(22,000-30,000trials)overthecourseof30-40scansessions. Scanningwasconductedat7Tusing
whole-braingradient-echoEPIat1.8mmresolutionand1.6srepetitiontime. Imageswerepresentedfor3swith1
sgapsinbetweenimages. Subjectsfixatedcentrallyandperformedalong-termcontinuousrecognitiontaskonthe
images.
4.4 fMRIdatapreprocessing
Weusedpre-processedfMRIdatareleasedbypreviousstudies(Shenetal.,2019b;HorikawaandKamitani,2022;
Hebartetal.,2023;Allenetal.,2022). ThefMRIdataforsubjects1-5wereobtainedfromtheDeeprecondataset
(Shenetal.,2019b;HorikawaandKamitani,2022). Afterpre-processingwiththeFMRIPREPpipeline(Estebanet
al.,2019),theBOLDtimeserieswereadjustedforhemodynamicdelaysbyapplyingatemporalshiftof4seconds
(equivalentto2volumes). Then,nuisancevariableswereremovedfromthevoxel-wisetimeseriesforeachrun. This
involvedregressingoutseveralparameters: aconstantbaseline,alineartrend,andtemporalcomponentsrelatedtothe
sixmotionparameterscalculatedduringthemotioncorrectionphase. Thedatasampleswerefinallydespikedtoreduce
extremevalues(beyond±3SDforeachrun)inthetimeseriesandaveragedwithineach8-secondtrial(fourvolumes).
ThefMRIdataforsubjects6and7weresourcedfromtheTHINGSdataset. Briefly,preprocessingwasconducted
usingFMRIPREP,ICAdenoisingwasthenapplied,andresponseswereestimatedbyasingle-trialGLMapproach. We
alsoreusedfMRIdatafromtheNSDdatasetforsubjects8and9. Insummary,thefMRIdatawerepre-processedby
performinginterpolationtocorrectforslicetimedifferencesandheadmotion. Agenerallinearmodelwasthenusedto
estimatesingle-trialbetaweights. CorticalsurfacereconstructionsweregeneratedusingFreeSurfer,andbothvolume-
andsurface-basedversionsofthebetaweightswerecreated.
4.5 Regionsofinterest(ROIs)
Forsubjects1-5,thebrainregionsV1,V2,V3,andV4weredemarcatedusingastandardretinotopyexperiment(Engel
etal.,1994;Serenoetal.,1995)ineachsubject’snativebrainspace. Thehighervisualcortex(HVC)wasdesignatedas
acontinuousareaencompassingtheLOC,FFA,andPPA,identifiedbyconventionalfunctionallocalizers(Kourtziand
Kanwisher,2000;Kanwisheretal.,1997;EpsteinandKanwisher,1998). Thewholevisualcortex(VC)wasdefinedas
thecombinationoftheregionsV1,V2,V3,V4,andHVC.Forsubject6andsubject7,theVCisthecombinationof
allvisualareasdefinedbasedonretinotopicmappingandfunctionallocalizerexperiments(Hebartetal.,2023). For
subject8andsubject9,theVCwasgeneratedfromsurface-basedrepresentationsofthedatausingtheHCP_MMP1
atlas(Glasseretal.,2016).
4.6 Methodsoffunctionalalignment
Contentloss-basedneuralcodeconverter. Theneuralcodeconvertermodelforeachpairofsubjectsconsistsofa
threefullyconnectedDNNlayerwithnonlinearfunctions,predictingthebrainactivitypatternsofonesubject(target)
fromthebrainactivitypatternsofanothersubject(source). Theconverterisoptimizedbasedonthetargetdecoderand
contentloss. TheconverterΦtakesasourcesubject’sbrainactivitypatternx ∈Rmconsistingofmvoxels’values,
i
predictsthetargetsubject’sbrainactivitypatternasΦ(x )∈Rn,consistingofnvoxels’values. Then,apre-trained
i
targetdecodertakesthepredictedbrainactivitypatternΦ(x ),andpredictsthedecodedfeaturepatternoftheimage
i
stimulusasd
il
=W lΦ(x i)+b l,whered
il
∈Rdl isthedecodedfeaturepatternconsistingofd
l
units’valuesinthe
l-thDNNlayerforthei-thimagestimulus. W
l
∈Rdl×nisthedecodingmatrixandb
l
∈Rdl isthebiasvectorofa
pre-traineddecoder. Theconverterisoptimizedtomakedecodedfeaturesfromconvertedbrainactivitysimilartothe
15APREPRINT
featuresofthestimulusofthesourcesubject. Itistrainedtominimizetheobjectivefunction
N L
(cid:88) (cid:88)
L(Φ)= η ||v −(W Φ(x )+b )||2 (1)
l il l i l
i l
wherev
il
∈ Rdl representstrueunits’valuesinthel-thDNNlayerforthei-thimagestimulus,N isthenumberof
trainingsamples,ListhenumberofDNNlayers. η istheparameterthatweighsthecontributionofthel-thlayer,
l
whichwassettobe1/||v ||2.
il
In our approach, the converter was resolved through an iterative process. Each iteration was characterized by the
implementationofastochasticdecodingstrategyusingallDNNlayers. Specifically,withineachconvolutionallayer,
afeaturemapwaschosenatrandom,andallitsunitsweresubjectedtothedecodingprocess. Incontrast,duetothe
comparativelyfewerunitspresentinthefullyconnectedlayers,allunitsweredecodedineachiteration. 1024iterations
wereusedtoensurethecomprehensiveinvolvementofallDNNunitsfromeverylayeroftheimagestimulus.
Brainloss-basedneuralcodeconverter. Thebrainloss-basedneuralcodeconverterconsistsofasetofregularized
linearregressionmodels(Hoetal.,2023). Ittakesasourcesubject’sbrainactivitypatternx ∈ Rm consistingof
i
m voxels’ values, and predicts the target brain activity pattern as yˆ = Mx +c, consisting of n voxels’ values.
i i
M∈Rn×mistheconversionmatrixandc∈Rnisthebiasvector. Theconverteristrainedtominimizetheobjective
function
N
(cid:88)
L(M,c)= ||y −(Mx +c)||2+λ||M||2 (2)
i i F
i
wherey isthemeasuredtargetsubject’sbrainactivitypatternforthei-thsample,N isthenumberoftrainingsamples,
i
λistheregularizationparameter,and||·|| representstheFrobeniusnorm.
F
4.7 DNNmodel
WeusedtheVGG19DNNmodel(SimonyanandZisserman,2014)implementedusingtheCaffelibrary(Jiaetal.,
2014) in the converter training and DNN feature decoding analysis. This model is pre-trained for the 1,000-class
objectrecognitiontaskusingtheimagesfromImageNet(Dengetal.,2009;thepre-trainedmodelisavailablefrom
https://github.com/BVLC/caffe/wiki/Model-Zoo). The model consists of 16 convolutional layers and three fully
connectedlayers. Alltheinputimagestothemodelwererescaledto224×224pixels. Outputsfromindividualunits
beforerectificationwereused. Thenumberofunitsineachlayerisasfollows: conv1_1andconv1_2, 3,211,264;
conv2_1andconv2_2,1,605,632;conv3_1,conv3_2,conv3_3,andconv3_4,802,816;conv4_1,conv4_2,conv4_3,
andconv4_4,401,408;conv5_1,conv5_2,conv5_3,andconv5_4,100,352;fc6andfc7,4,096;andfc8,1,000.
WeusedtheAlexNetDNNmodel(Krizhevskyetal.,2012)implementedusingtheCaffelibrarytoextractDNNfeatures
fromthereconstructedimagesandthepresentedimage. Thismodelisalsopre-trainedwithimagesinImageNetto
classify1000objectcategories(availablefromhttps://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet). The
modelconsistsoffiveconvolutionallayersandthreefullyconnectedlayers. Alltheinputimagestothemodelwere
rescaledto224×224pixels. Thenumberofunitsineachlayerisasfollows: conv1,290,400;conv2,186,624;conv3
andconv4,64,896;conv5,43,264;fc6andfc7,4,096;andfc8,1,000.
We used the CLIP_ViT model (Radford et al., 2021) implemented using the pytorch library (Paszke et al., 2019)
for the analysis of generalization beyond trained DNN features. This model is pre-trained on diverse image-text
pairs to link images with text descriptions for a variety of visual tasks without task-specific training (available
from https://github.com/openai/CLIP). All the input images to the model were rescaled to 224 × 224 pixels. The
numberofunitsineachlayerfortheimageencoderisasfollows: conv1, 37,632; fromtransformer_resblocks0to
transformer_resblocks11,boththeattn_outputandmlplayersineachblock,38,400;ln_post,768;andmodel_output,
512.
4.8 Noiseceilingestimation
ToaddressmeasurementnoiseinfMRIdatawhenrepeatedbrainresponsemeasurementstothesamestimulus,we
appliedanoiseceilingestimationtechniquefollowingLescroartandGallant(2019)andHsuetal.(2004). Thismethod
calculatesthenoiseceilingbyaveragingthecorrelationcoefficientsofrepeatedstimuliresponseswithinanindividual,
basedontheprinciplethatamodel’spredictionscannotsurpassthesubject’sresponses. Therawpredictionaccuracies
oftheconvertermodelswerenormalizedbydividingtherawaccuraciesbythenoiseceilings. Weexcludedsamplesor
voxelsfromperformanceassessmentsiftheirnoiseceilingsfellbelowaspecifiedthreshold(the99thpercentileofthe
16APREPRINT
distributionfromrandompairs),duetounreliablemeasurements. However,weretainedallvoxelsinthesubsequent
analysisofDNNfeaturedecodingtoavoidpotentialdataleakage.
4.9 DNNfeaturedecodinganalysis
WeusedaridgelinearregressionmodelasaDNNfeaturedecoder. Thismodelpredictsfeaturevaluesofthestimulus,
given an fMRI activity pattern that is evoked by the stimulus. We standardized both the feature values and voxel
responses before model training and utilized a voxel selection procedure. This procedure involved calculating the
Pearsoncorrelationcoefficientsbetweensequencesofvoxelresponsesandfeaturevaluesforallvoxels. The500voxels
exhibiting the highest correlations were selected for training. We set the ridge regularization parameter to 100 to
enhancemodelrobustness. Fortestingthetraineddecoders,weusedtheaveragefMRIpatternoverrepetitions,which
improvedthesignal-to-noiseratioofthefMRIsignal. ThefeaturedecodingisdetailedinthestudiesbyHorikawaand
Kamitani(2017,2022),andShenetal.(2019b).
4.10 Visualimagereconstruction
Thereconstructionmethodusedinthisstudywasextendedfromouroriginaldeepimagereconstructionstudy(Shen
etal.,2019b). Thepixelvaluesofaninputimagewereoptimizedtomakeitsimagefeaturesmatchthedecodedfeatures
frombrainactivity. FollowingShenetal.(2019b),weusedthefeaturevaluesbeforetherectificationoperationfrom
eightlayers(conv1-5andallfullyconnectedlayers). Weappliedthesamenaturalimagepriorandextendedtheloss
functionbyaddingaDISTS(DeepImageStructureandTextureSimilarity)losscomponent(Muraki,2024),which
leveragesspatialcharacteristicsoffeaturemapstoimprovethedetailofimagereconstructions. Giventhedecoded
featuresfrommultiplelayers,animagewasreconstructedbysolvingthefollowingoptimizationproblem:
z∗ =argmin(L (z)+λ L (z)+λ L (z)) (3)
mse tex tex str str
z
wherezisthelatentvector,L (z)isthelossoriginallyusedinShenetal.(2019b):
mse
L
L (z)=(cid:88) γ ∥Ψ (G(z))−u ∥2 (4)
mse l l l
l
G is the deep generator network (DGN) to enhance the naturalness of the image (Nguyen et al., 2016), and the
reconstruedimageisobtainedasG(z∗). Ψ isthefunctionthatmapstheimagetotheDNNfeaturevectorofthel-th
l
layer. u
l
∈RPl×Ql×Kl representsthedecodedDNNfeaturevectoroftheimageatthel-thlayer,whereP l,Q l,andK
l
denotethewidth,height,andnumberofchannelsofthefeaturemapsinthel-thlayer,respectively. γ istheparameter
l
thatweightsthecontributionofthel-thlayer,whichwassettobe1/||u ||2. ListhenumberofDNNlayers. L (z)is
l tex
thetexturesimilaritylossandL (z)isthestructureloss;together,theyconstitutetheDISTSloss. λ andλ serve
str tex str
asthecoefficientsofweightsforthetextureandstructuralsimilaritylosses,respectively.
Ifwedenoteuˆ
l
=Ψ l(G(z))∈RPl×Ql×Kl,thetexturesimilarityL tex(z)andL str(z)arethendefinedasfollows:
L
L
(z)=−(cid:88)
α
1 (cid:88) µ k(u l)µ k(uˆ l)+ϵ
(5)
tex lK µ (u )2+µ (uˆ )2+ϵ
l k l k l
l k
L
L
(z)=−(cid:88)
β
1 (cid:88) δ k(u l,uˆ l)+ϵ
(6)
str lK δ (u )2+δ (uˆ )2+ϵ
l k l k l
l k
where
1 (cid:88)
µ (u )= u (7)
k l P Q pl,ql,kl
l l
pl,ql
1 (cid:88)
δ (u )= (u −µ (u ))2 (8)
k l P Q pl,ql,kl k l
l l
pl,ql
1 (cid:88)
δ (u ,uˆ )= u uˆ −µ (u )µ (uˆ ) (9)
k l l P Q pl,ql,kl pl,ql,kl k l k l
l l
pl,ql
Here,u ∈Risanotationmeaningtheactivityvalueofthe(p,q,k)-thelementforthel-layer. Theα andβ are
pl,ql,kl l l
hyperparametersthatsignifytheweightassignedtoeachlayer,whichweretunedusingdatafromasubjectexcluded
fromtheresultanalyses. Asmallpositiveconstantϵisincludedtoavoidnumericalinstabilitywhenthedenominator
is close to zero. We solved the optimization problem using stochastic gradient descent with momentum with 200
iterations.
17APREPRINT
4.11 Identificationanalysis
We employed identification analysis to assess the quality of image reconstruction. This approach involved the
identificationofpresentedimagesbasedonthesimilarityofimagefeatures,includingpixelsorDNNfeatures. We
reshapedfeaturesintoaone-dimensionalfeaturevectorandusedthefeaturevectorofthereconstructedimagefor
comparisonagainstthetruefeaturevectorofthepresentedimage,aswellasthefalsealternativeofanotherimage.
Anidentificationwasconsideredcorrectwhenthecorrelationcoefficientofthereconstructedimage’sfeaturevector
washigherforthetruefeaturevectorthanforthefalsealternative. Thisprocedurewasrepeatedformultiplefalse
alternativesperreconstruction. Wethendefinedtheidentificationaccuracyforareconstructedimageastheratioof
correctidentifications.
4.12 Statistics
Weperformedstatisticalanalysesatthegrouplevelusingmeanvaluesfromallsubjectpairs(withinDeeprecon: 20
subjectpairs;inter-siteanalysis: 10individualpairsorfoursubjectpairsineachinter-siteconversion). Intheevaluation
ofconversionaccuracy,foreachpairofsubjects,theprofilecorrelationcoefficientsforallvoxelswereusedtocalculate
themeanconversionaccuracy(profile),whilethepatterncorrelationcoefficientsfor50visualstimuliwereusedto
calculate the mean conversion accuracy (pattern). Then, the mean conversion accuracies (pattern/profile) from 20
individualpairswereusedtocalculatethegroupmeanandits95%confidenceinterval. IntheDNNfeaturedecoding
analysis,foreachconversionoreachsubject(within),thedecodingaccuraciesforallDNNunitswereusedtocalculate
themeandecodingaccuracy(profile),whilethedecodingaccuraciesfor50visualstimuliwereusedtocalculatethe
meandecodingaccuracy(pattern). Then,themeandecodingaccuraciesfrom20individualpairsorfivesubjects(within)
wereusedtocalculatethegroupmeanandits95%confidenceinterval. Intheidentificationanalysisofreconstructed
images,foreachconversionoreachsubject(within),theidentificationaccuraciesforindividualreconstructedimages
wereusedtocalculatethemeanidentificationaccuracy. FortheanalyseswithinDeeprecon,themeanidentification
accuraciesfrom20subjectpairsorfivesubjects(Within)wereusedtocalculatethegroupmeanandits95%confidence
interval. Fortheanalysesofinter-siteconversion,themeanidentificationaccuraciesfromallconversions(10subject
pairsorfoursubjectpairsineachinter-siteconversion)orsubjects(Within,fivesubjectsortwosubjects)wereusedto
calculatethegroupmeanandits95%confidenceinterval.
5 ACKNOWLEDGMENTS
Wethankourlaboratoryteam,especiallyYoshihiroNagano,KenShirakawa,EizaburoDoi,andHidekiIzumi,fortheir
invaluablecommentsandsuggestionsonthemanuscript. FundingwassupportedbyJapanSocietyforthePromotion
of Science (KAKENHI grant JP20H05705 and JP20H05954 to Y.K.), the New Energy and Industrial Technology
DevelopmentOrganization(GrantNumberJPNP20006toYK),GuardianRobotProject,RIKEN,andJapanScience
andTechnologyAgency(CRESTgrantJPMJCR18A5andJPMJCR22P3toY.K.).
References
EmilyJAllen,GhislainSt-Yves,YihanWu,JesseLBreedlove,JacobSPrince,LoganTDowdle,MatthiasNau,Brad
Caron,FrancoPestilli,IanCharest,etal. Amassive7tfmridatasettobridgecognitiveneuroscienceandartificial
intelligence. Natureneuroscience,25(1):116–126,2022.
ThomasBazeille,ElizabethDupre,HugoRichard,Jean-BaptistePoline,andBertrandThirion. Anempiricalevaluation
offunctionalalignmentusinginter-subjectdecoding. NeuroImage,245:118683,2021.
NataliaYBilenkoandJackLGallant. Pyrcca: regularizedkernelcanonicalcorrelationanalysisinpythonandits
applicationstoneuroimaging. Frontiersinneuroinformatics,10:49,2016.
CharlesFCadieu,HaHong,DanielLKYamins,NicolasPinto,DiegoArdila,EthanASolomon,NajibJMajaj,and
JamesJDiCarlo. Deepneuralnetworksrivaltherepresentationofprimateitcortexforcorevisualobjectrecognition.
PLoScomputationalbiology,10(12):e1003963,2014.
Po-HsuanCameronChen,JaniceChen,YaaraYeshurun,UriHasson,JamesHaxby,andPeterJRamadge. Areduced-
dimensionfmrisharedresponsemodel. Advancesinneuralinformationprocessingsystems,28,2015.
FanLCheng,TomoyasuHorikawa,KeiMajima,MisatoTanaka,MohamedAbdelhack,ShuntaroCAoki,JinHirano,
andYukiyasuKamitani. Reconstructingvisualillusoryexperiencesfromhumanbrainactivity. ScienceAdvances,9
(46):eadj3906,2023.
David D Cox and Robert L Savoy. Functional magnetic resonance imaging (fmri)“brain reading”: detecting and
classifyingdistributedpatternsoffmriactivityinhumanvisualcortex. Neuroimage,19(2):261–270,2003.
18APREPRINT
GabriellaCsurka,ChristopherDance,LixinFan,JuttaWillamowski,andCédricBray. Visualcategorizationwithbags
ofkeypoints. InWorkshoponstatisticallearningincomputervision,ECCV,volume1,pages1–2.Prague,2004.
JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage
database. In2009IEEEconferenceoncomputervisionandpatternrecognition,pages248–255.Ieee,2009.
StephenAEngel,DavidERumelhart,BrianAWandell,AdrianTLee,GaryHGlover,Eduardo-JoseChichilnisky,
MichaelNShadlen,etal. fmriofhumanvisualcortex. Nature,369(6481):525–525,1994.
RussellEpsteinandNancyKanwisher. Acorticalrepresentationofthelocalvisualenvironment. Nature,392(6676):
598–601,1998.
BruceFischl,NiranjiniRajendran,EvelinaBusa,JeanAugustinack,OliverHinds,BTThomasYeo,HartmutMohlberg,
KatrinAmunts,andKarlZilles. Corticalfoldingpatternsandpredictingcytoarchitecture. Cerebralcortex,18(8):
1973–1980,2008.
MatthewFGlasser,TimothySCoalson,EmmaCRobinson,CarlDHacker,JohnHarwell,EssaYacoub,KamilUgurbil,
JesperAndersson,ChristianFBeckmann,MarkJenkinson,etal. Amulti-modalparcellationofhumancerebral
cortex. Nature,536(7615):171–178,2016.
Umut Güçlü and Marcel AJ Van Gerven. Deep neural networks reveal a gradient in the complexity of neural
representationsacrosstheventralstream. JournalofNeuroscience,35(27):10005–10014,2015.
JSwaroopGuntupalli,MichaelHanke,YaroslavOHalchenko,AndrewCConnolly,PeterJRamadge,andJamesV
Haxby. Amodelofrepresentationalspacesinhumancortex. Cerebralcortex,26(6):2919–2934,2016.
JamesVHaxby,JSwaroopGuntupalli,AndrewCConnolly,YaroslavOHalchenko,BryanRConroy,MIdaGobbini,
MichaelHanke,andPeterJRamadge. Acommon,high-dimensionalmodeloftherepresentationalspaceinhuman
ventraltemporalcortex. Neuron,72(2):404–416,2011.
MartinNHebart,AdamHDickter,AlexisKidder,WanYKwok,AnnaCorriveau,CaitlinVanWicklin,andChrisI
Baker. Things: Adatabaseof1,854objectconceptsandmorethan26,000naturalisticobjectimages. PloSone,14
(10):e0223792,2019.
MartinNHebart,OliverContier,LinaTeichmann,AdamHRockter,CharlesYZheng,AlexisKidder,AnnaCorriveau,
Maryam Vaziri-Pashkam, and Chris I Baker. Things-data, a multimodal collection of large-scale datasets for
investigatingobjectrepresentationsinhumanbrainandbehavior. Elife,12:e82580,2023.
Jun Kai Ho, Tomoyasu Horikawa, Kei Majima, Fan Cheng, and Yukiyasu Kamitani. Inter-individual deep image
reconstructionviahierarchicalneuralcodeconversion. NeuroImage,271:120007,2023.
TomoyasuHorikawaandYukiyasuKamitani. Genericdecodingofseenandimaginedobjectsusinghierarchicalvisual
features. Naturecommunications,8(1):15037,2017.
TomoyasuHorikawaandYukiyasuKamitani. Attentionmodulatesneuralrepresentationtorenderreconstructions
accordingtosubjectiveappearance. CommunicationsBiology,5(1):34,2022.
TomoyasuHorikawa,ShuntaroCAoki,MitsuakiTsukamoto,andYukiyasuKamitani. Characterizationofdeepneural
networkfeaturesbydecodabilityfromhumanbrainactivity. Scientificdata,6(1):1–12,2019.
AnneHsu,AlexanderBorst,andFrédéricETheunissen. Quantifyingvariabilityinneuralresponsesanditsapplication
forthevalidationofmodelpredictions. Network: ComputationinNeuralSystems,15(2):91–109,2004.
YangqingJia,EvanShelhamer,JeffDonahue,SergeyKarayev,JonathanLong,RossGirshick,SergioGuadarrama,
andTrevorDarrell. Caffe: Convolutionalarchitectureforfastfeatureembedding. InProceedingsofthe22ndACM
internationalconferenceonMultimedia,pages675–678,2014.
NancyKanwisher,JoshMcDermott,andMarvinMChun. Thefusiformfacearea: amoduleinhumanextrastriate
cortexspecializedforfaceperception. JournalofNeuroscience,17(11):4302–4311,1997.
Seyed-MahdiKhaligh-RazaviandNikolausKriegeskorte. Deepsupervised,butnotunsupervised,modelsmayexplain
itcorticalrepresentation. PLoScomputationalbiology,10(11):e1003915,2014.
ZoeKourtziandNancyKanwisher. Corticalregionsinvolvedinperceivingobjectshape. JournalofNeuroscience,20
(9):3310–3318,2000.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. Advancesinneuralinformationprocessingsystems,25,2012.
DanielDLeeds,DarrenASeibert,JohnAPyles,andMichaelJTarr. Comparingvisualrepresentationsacrosshuman
fmriandcomputationalvision. Journalofvision,13(13):25–25,2013.
MarkDLescroartandJackLGallant. Humanscene-selectiveareasrepresent3dconfigurationsofsurfaces. Neuron,
101(1):178–192,2019.
19APREPRINT
Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,andCLawrence
Zitnick. Microsoftcoco: Commonobjectsincontext. InComputerVision–ECCV2014: 13thEuropeanConference,
Zurich,Switzerland,September6-12,2014,Proceedings,PartV13,pages740–755.Springer,2014.
DavidGLowe. Objectrecognitionfromlocalscale-invariantfeatures. InProceedingsoftheseventhIEEEinternational
conferenceoncomputervision,volume2,pages1150–1157.Ieee,1999.
YoichiMiyawaki,HajimeUchida,OkitoYamashita,Masa-akiSato,YusukeMorito,HirokiCTanabe,NorihiroSadato,
andYukiyasuKamitani. Visualimagereconstructionfromhumanbrainactivityusingacombinationofmultiscale
localimagedecoders. Neuron,60(5):915–929,2008.
YusukeMuraki. Improvingvisualimagereconstructionfrombrainactivityusingtextureandstructuresimilaritylosses.
Master’sthesis,KyotoUniversity,2024.
JimMutchandDavidGLowe. Objectclassrecognitionandlocalizationusingsparsefeatureswithlimitedreceptive
fields. InternationalJournalofComputerVision,80:45–57,2008.
SamuelANastase,ValeriaGazzola,UriHasson,andChristianKeysers. Measuringsharedresponsesacrosssubjects
usingintersubjectcorrelation,2019.
AnhNguyen,AlexeyDosovitskiy,JasonYosinski,ThomasBrox,andJeffClune. Synthesizingthepreferredinputsfor
neuronsinneuralnetworksviadeepgeneratornetworks. Advancesinneuralinformationprocessingsystems,29,
2016.
AudeOlivaandAntonioTorralba. Modelingtheshapeofthescene: Aholisticrepresentationofthespatialenvelope.
Internationaljournalofcomputervision,42:145–175,2001.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,Zeming
Lin,NataliaGimelshein,LucaAntiga,etal. Pytorch: Animperativestyle,high-performancedeeplearninglibrary.
Advancesinneuralinformationprocessingsystems,32,2019.
AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,Amanda
Askell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfromnaturallanguagesupervision. In
Internationalconferenceonmachinelearning,pages8748–8763.PMLR,2021.
GraceERice,DavidMWatson,TomHartley,andTimothyJAndrews. Low-levelimagepropertiesofvisualobjects
predict patterns of neural response across category-selective regions of the ventral visual pathway. Journal of
Neuroscience,34(26):8837–8844,2014.
MaximilianRiesenhuberandTomasoPoggio. Hierarchicalmodelsofobjectrecognitionincortex. Natureneuroscience,
2(11):1019–1025,1999.
MartinISereno,AMDale,JBReppas,KKKwong,JWBelliveau,TJBrady,BRRosen,andRBHTootell. Bordersof
multiplevisualareasinhumansrevealedbyfunctionalmagneticresonanceimaging. Science,268(5212):889–893,
1995.
ThomasSerre,LiorWolf,StanleyBileschi,MaximilianRiesenhuber,andTomasoPoggio. Robustobjectrecognition
withcortex-likemechanisms. IEEEtransactionsonpatternanalysisandmachineintelligence,29(3):411–426,2007.
GuohuaShen,KshitijDwivedi,KeiMajima,TomoyasuHorikawa,andYukiyasuKamitani. End-to-enddeepimage
reconstructionfromhumanbrainactivity. Frontiersincomputationalneuroscience,13:432276,2019a.
GuohuaShen,TomoyasuHorikawa,KeiMajima,andYukiyasuKamitani. Deepimagereconstructionfromhuman
brainactivity. PLoScomputationalbiology,15(1):e1006633,2019b.
KShirakawa,MTanaka,SAoki,KMajima,andYKamitani. Criticalassessmentofgenerativeaimethodsandnatural
imagedatasetsforvisualimagereconstructionfrombrainactivity. Retrievedfromosf.io/nmfc5,2023.
KarenSimonyanandAndrewZisserman. Verydeepconvolutionalnetworksforlarge-scaleimagerecognition. arXiv
preprintarXiv:1409.1556,2014.
David C Van Essen. Surface-based approaches to spatial localization and registration in primate cerebral cortex.
Neuroimage,23:S97–S107,2004.
DavidCVanEssen. Apopulation-average,landmark-andsurface-based(pals)atlasofhumancerebralcortex. Neuroim-
age,28(3):635–662,2005.
CaraEVanUden,SamuelANastase,AndrewCConnolly,MaFeilong,IsabellaHansen,MIdaGobbini,andJamesV
Haxby. Modelingsemanticencodinginacommonneuralrepresentationalspace. Frontiersinneuroscience,12:
378029,2018.
Kentaro Yamada, Yoichi Miyawaki, and Yukiyasu Kamitani. Inter-subject neural code converter for visual image
representation. NeuroImage,113:289–297,2015.
20APREPRINT
DanielLKYamins,HaHong,CharlesFCadieu,EthanASolomon,DarrenSeibert,andJamesJDiCarlo. Performance-
optimizedhierarchicalmodelspredictneuralresponsesinhighervisualcortex. Proceedingsofthenationalacademy
ofsciences,111(23):8619–8624,2014.
21