QEAN: Quaternion-Enhanced Attention Network
for Visual Dance Generation
Zhizhen Zhou1, Yejing Huo1, Guoheng Huang1*, An Zeng1,
Xuhang Chen2*, Lian Huang3, Zinuo Li4
1Guangdong University of Technology, Guangdong, China.
2Huizhou University, Guangdong, China.
3Guangdong Mechanical and Electrical College, Guangdong, China.
4University of Western Australia, WA, Australia.
*Corresponding author(s). E-mail(s): kevinwong@gdut.edu.cn;
xuhangc@hzu.edu.cn;
Contributing authors: 3121005415@mail2.gdut.edu.cn;
2112305121@mail2.gdut.edu.cn; zengan@gdut.edu.cn;
mrhuanglian@gmail.com; zinuo.li@research.uwa.edu.au;
Abstract
The study of music-generated dance is a novel and challenging Image genera-
tion task. It aims to input a piece of music and seed motions, then generate
naturaldancemovementsforthesubsequentmusic.Transformer-basedmethods
face challenges in time series prediction tasks related to human movements and
musicduetotheirstruggleincapturingthenonlinearrelationshipandtemporal
aspects. This can lead to issues like joint deformation, role deviation, floating,
and inconsistencies in dance movements generated in response to the music. In
this paper, we propose a Quaternion-Enhanced Attention Network (QEAN) for
visual dance synthesis from a quaternion perspective, which consists of a Spin
Position Embedding (SPE) module and a Quaternion Rotary Attention (QRA)
module. First, SPE embeds position information into self-attention in a rota-
tionalmanner,leadingtobetterlearningoffeaturesofmovementsequencesand
audio sequences, and improved understanding of the connection between music
anddance.Second,QRArepresentsandfuses3Dmotionfeaturesandaudiofea-
tures in the form of a series of quaternions, enabling the model to better learn
thetemporalcoordinationofmusicanddanceunderthecomplextemporalcycle
conditionsofdancegeneration.Finally,weconductedexperimentsonthedataset
AIST++,andtheresultsshowthatourapproachachievesbetterandmorerobust
performance in generating accurate, high-quality dance movements. Our source
1
4202
raM
81
]RG.sc[
1v62611.3042:viXracode and dataset can be available from https://github.com/MarasyZZ/QEAN
and https://google.github.io/aistplusplus dataset respectively.
Keywords:Dancegeneration,Multi-modaltask,Quaternionnetwork,Time-series
predictiontask,Animationgenerationtask
1 Introduction
Other Methods Other Methods
Other Methods
P S Q
Our method
seed motion P : Pre-quanternion parameterizaiton Our method lets each prediction learn the
S : Spin Position Embedding
relationship between dance and music
Q : Quaternion Attention
Fig. 1 The motivation of our method. We compare the effectiveness of our method is compared
with other approaches in generating dance movements from seed motions. In the top row labeled
“othermethods”,twosetsofimagesshowcasethetransformationofseedmovementsintounnatural
final poses characterized by joint deformation and character drift. Conversely, in the bottom row
labeled“ourmethod”,wedemonstratehowtheapplicationofPre-quaternionparameterization(P),
SpinPositionEmbedding(S),andQuaternionAttention(Q)yieldsnatural-lookingfinalposes.Each
predictionproducedbyourmethodsuccessfullylearnsthecorrelationsbetweendanceandmusic.
Dancing is a universal language across all cultures [1, 2] and is used by many as
a powerful means of self-expression on online media platforms, becoming a dynamic
tool for disseminating information on the Internet. Although dance is an art form,
it requires professional practice and training to give dancers a rich expressive voice
[3]. Therefore, from a computational point of view, music-conditioned 3D dance gen-
eration [4, 5, 6, 7] has become a critical task that promises to open up a variety
of practical applications. However, creating satisfying dance sequences that harmo-
nize with specific music and body structures faces challenges because of our lack of
understanding of the timing of human movements and the connection between music
and dance. Overcoming these challenges is essential to achieve fluid movements with
a high degree of kinematic complexity while ensuring consistency with the complex
non-linear relationships of the accompanying music.
2Later, with the continuous development and advancement of deep learning, many
deep learning methods [6, 7, 8, 9, 10, 11] were started to be applied to dance gener-
ation. Firstly, RNN [8, 10] based methods were used to simulate human dances, but
RNN approaches would face the challenges of static poses and error accumulation,
especiallywhentheinputdatavaried.Subsequently,someresearchershaveusedVari-
ational Auto-Encoders (VAE) and Generative Adversarial Networks (GAN) to model
2D dance movements [12], and then LSTM-Auto-Encoders were used to model 3D
dancemovementsdirectlyfrommusicalfeatures[13],althoughsuchanapproachsolves
the shortcomings of error accumulation that exist in the RNN approach. However,
such an approach suffers from the disadvantage of instability and is prone to regress
to non-standard poses.
Inrecentyears,Transformer[12,13,14,15,16,17,18]hasbeenfavouredbymany
in natural language processing as well as visual processing, and some scholars have
madegreatprogressintheirresearch[19,20]tobeabletogeneratehigh-qualitydance
movements given a piece of music. However, due to the existence of Transformer’s
inadequatemodelingofthetemporaldependenceofsequenceswhendealingwithtime-
series data, the generated dances will suffer from problems such as drifting and foot
slipping (As shown in Fig.1). Given the non-linear relationship between music and
dance, existing approaches to Transformer do not fully model this relationship.
Quaternions are widely used as a mathematical tool for rotational expression and
gesturecontrol[21].Inviewofthis,webelievethattheintroductionofquaternionsin
the field of dance generation may be a promising approach. Compared to traditional
Euler angles, quaternions are more effective in avoiding the “Gimbal Lock” problem
andimprovingthestabilityofgesturerepresentations.Weexpecttousequaternionsto
moreaccurately adaptdancemovementstothe rhythmandemotion ofthemusic. By
combiningmusicalfeatureswiththecorrelationofquaternions,wecanmoreaccurately
capture the complex relationship between music and dance.
In this paper, to address these challenges, we propose a Quaternion-Enhanced
Attention Network for multi-modal dance synthesis (shown in Fig.2). The network
mainlyconsistsofaSpinPositionEmbedding(SPE)moduleandaQuaternionRotary
Attention (QRA) module. The SPE module is mainly used in the Transformer struc-
ture of the network, which embeds information in the form of relative positions into
the self-attention. The audio and motion features are extracted by the Transformer
structure in the network, respectively, and the SPE module combines the advantages
ofrelativepositioncodingandabsolutepositioncodingtomaximizethemodel’srepre-
sentationofsequencefeatures.Theextractedaudioandmotionfeaturesareexpanded
to four dimensions by quaternion parameterization dimension, and then the splicing
operation is performed through the Quaternion Rotary Attention module. Compared
to the work of Li [19], the proposed SPE better increases the model’s representation
and utilisation of positional information. Besides, the QRA module better learns the
representation of the link between audio and movement relationships, improving the
quality of the generated dances with good robustness.
The contribution of the proposed network can be summarized as follows:
1.Inthispaper,weintroduceaQuaternion-EnhancedAttentionNetwork(QEAN)
for generating multimodal dances. This addresses challenges seen in current methods,
3like awkward joint movements and character inconsistency. QEAN uses quaternion
operations to better capture the complex relationship between music and dance,
improving the modeling of temporal dependencies.
2. We introduce the Spin Position Embedding (SPE) module, which computes
query and key vectors for features, applies rotational operations, and embeds results
into self-attention. SPE addresses limitations of traditional position encoding by
introducing relative position encoding based on rotations, enhancing modeling for
variable-length sequences while solving length consistency and overfitting issues.
Additionally,relativepositioninformationimprovesmodelingofintrinsicfeatureasso-
ciations,significantlyenhancingthemodel’srepresentationandutilizationoftemporal
order in human motion.
3. We introduce the quaternion perspective and propose the Quaternion Rotary
Attention (QRA) module. The QRA module maps audio and motion features to the
quaternionspaceandexplorestheintrinsiccorrelationbetweenthetwousingHamilto-
nianmultiplication,whichenablesthemodeltobetterlearnthetemporalcoordination
between music and dance, and generate smooth and natural dances coordinated with
the music tempo.
4. Experimental results on the AIST++ dataset demonstrate that our proposed
networkiscapableofeffectivelylearningtheconnectionbetweenaudioandmovement,
leading to the generation of higher quality dance movements. It outperforms other
current state-of-the-art methods in terms of dance quality.
2 Related Work
3D Human Motion SynthesisTheresearchongeneratingrealisticandcontrol-
lable 3D human motion sequences, as discussed in [8, 22, 23, 24], has seen significant
advancements in recent years. Initial efforts utilized statistical models like kernel-
based probability distributions [25] to synthesize motion, but these methods tended
to oversimplify motion details. A subsequent breakthrough came with the introduc-
tionofthemotiongraphapproach[26],whichaddressedthislimitationbyadoptinga
non-parametric method. This technique involved constructing a directed graph using
motioncapturedatasets,whereeachnoderepresentedapose,andedgesdenotedtran-
sitions between poses. Motion generation was achieved through random walks on this
graph. However, a notable challenge in motion graphs was the generation of plausible
transitions, and certain methods sought to overcome this by introducing parameter-
izations for transitions [27]. As deep learning gained prominence, several approaches
explored the use of neural networks trained on extensive motion capture datasets to
generate 3D motion. Various network architectures, including CNNs [23, 28], GANs
[29], VAE [30], RNNs [6, 20], and Transformers [4, 20] have been investigated. While
auto-regressive models like RNNs and pure Transformers [31] theoretically have the
capacitytogenerateinfinitemotion,practicalchallengessuchasmeanregressionarise.
This phenomenon leads to motion “freezing” or drifting into unnatural movements
afterseveraliterations.Toaddressthis,somestudies[31,32]proposeperiodicusageof
the network’s output as input during the training process. Additionally, Phase Func-
tion Neural Networks and their variants have been introduced [33, 34] to tackle the
4mean regression issue by conditioning network weights on the phase. However, their
scalability in representing diverse movements is limited.
Music-Driven Dance Generation In recent years, data-driven deep learning
has become the dominant technique for dance generation. Joao [35] used graph con-
volutional networks to learn from a variety of dance datasets and generate new dance
sequences that are smooth and continuous. This deep learning approach significantly
improvesthequalityandcontinuityofthegenerateddances.Holden[36],Qiu[37]and
Starke [38] built on deep learning by de-augmenting long-term dependency modeling
as a means of generating more coherent long dance sequences. Common approaches
include integrating skeleton information and employing attention mechanisms. Li [19]
builtonthatpreviousworkbyproposingtheFullAttentionCross-ModalTransformer
model (FACT), which can generate non-freezing, high-quality 3D motion sequences
conditioned on music by learning audio-motion correspondences sequences.
Quaternion Networks In various domains of deep learning such as few-shot
segmentation [39], human motion synthesis [21], and multi-sensor signal processing,
Quaternion Neural Networks have made significant strides. Similar to the task dis-
cussed in this paper, Quaternion representations are employed in neural network
architecturesasaparameterizationforrotations.Quaternionnetworks,exemplifiedby
QuaterNet[21],utilizequaternionstorepresentjointrotationsinbothRecurrentNeu-
ral Networks (RNNs) and Convolutional Neural Networks (CNNs). This approach
addresses the discontinuity issues associated with Euler angles, achieving outstand-
ing performance in long-term prediction tasks. In the context of our work, focused on
music-drivendancegeneration,weproposeconstructingalearningprocessforthecor-
relation between music and dance. This is essential as it requires consideration of the
non-linear characteristics of both motion and music. Therefore, our method involves
exploring the relationship between audio and motion features using quaternions. By
leveraging quaternions, we aim to enhance the correlation between audio and motion,
facilitating the generation of high-quality dance sequences.
3 Methods
3.1 Overview of QEAN
In this paper, we propose a Quaternion-Enhanced Attention Network (QEAN) for
generating high-quality dances under musical conditions, as illustrated in Fig. 2.
We are given random motion seeds Y of length 120 frames and audio features Z
of length 240 frames, where Y can be denoted as Y = {y ,y ,...,y } and Z can be
1 2 t
denoted as Z = {z ,z ,...,z }. Our objective is to generate a sequence of future
1 2 t′
motion from t+1 to t′, Y′ ={y ,y ...y }, where t′ ≫t. QEAN first utilizes the
t+1 t+2 t′
twoinputtransformers,themotiontransformerf andtheaudiotransformerf ,
mot audio
to encode features and generate motion and audio embeddings, represented as hy
1:T
andhz ,respectively.Next,thesetwoembeddedfeaturesarecombinedandsubjected
1:T′
to a quaternion parameterization operation (see 3.2 for details). This operation maps
the features to four dimensions and embeds the information into the self-attention in
a rotational manner using Spin Position Embedding (see 3.3 for details). Finally, the
5Quaternion Attention Transformer
Dimension-based
x'2 θ
Encoded
Dropout
quaternion Query/Key
x2
Query/Key Feed Forward x16
Parameterization x' x
(x1,x2) 1 1
Add & Nomalize
Quaternion Rotary Attention
Quaternion Parameterization Spin Position Embedding
(i) (ii) (iii)
p
M Ao ut dio ion FF ee aa tt uu rr ee E n c o d in g& E x tra c tio n F e a tu re a ra m e te riz a tioq u a te rn io n E m b e d d in gP o s itio nS p in Qua Tte rr an nio sn fo A rmtt ee rntion
n
Quaternion-Enhanced Attention Network
(a)
Fig. 2 The overview of our method. (a) describes the basic process, which contains three modules
(i),(ii),and(iii).Whentheinputsareamotionsequencewithalengthof120framesandanaudio
sequencewithalengthof240frames,featuresareextractedbythemotiontransformerandtheaudio
transformer,respectively.Theextractedfeaturesareparameterizedbyaquadraticparameterization
operation, and the dimension is changed to 4 dimensions. Through the Spin Position Embedding
(SPE) module, the corresponding 4-dimensional features are rotated to embed the information into
the self-attention in a rotational manner. The information processed by the SPE is used to explore
thecoordinationbetweenthemusicandthedancethroughthequaternionicattentionaltransformer,
andfinally,thecorrespondingdanceisgenerated.(i),(ii)and(iii)describetheprocessingofquater-
nionparameterization,spinpositionembeddingandthebasicstructureofthequaternionattention
transformer,respectively.SpecificdetailsaregivenintheMethodssection.
fused features are learned by a Quaternion Rotary Attention Transformer (see 3.4 for
details) to generate the corresponding dance movements.
3.2 Quaternion Algorithms and Quaternion Parameterization
Webeginbyelucidatingthefundamentalconceptsofquaternionscrucialforunder-
standing the context of this paper. Quaternions, classified as hyper-complex numbers
of rank 4, stand out as a direct and non-commutative extension of complex-valued
numbers. In our proposed methodologies, the intricate interplay between Hamilton
products and quaternion algebra emerges as the linchpin, forming the cornerstone
of our innovative approaches. This exploration of quaternion principles lays the
groundwork for the subsequent discussions and applications detailed in this study.
A quaternion Q in quaternion domain D, Q ∈ D, can be represented as:
Q=e+fi+gj+hk (1)
6Where e,f,g and h are real numbers,and i,j and k are the quaternion unit basis.In
a quaternion, e is the real part, where fi+gj+hk, with i2=j2=k2=ijk=-1 is the
imaginary part.
A pure quaternion is a quaternion whose real part is 0, resulting in the vector
Q=fi+gj+hk. Operations on quaternions are defined as follows.
The addition of two Quaternions is defined as:
Q+R=Q +R +(Q +R )i+(Q +R )j+(Q +R )k (2)
e e f f g g h h
WhereQandPwithsubscriptsdenotetherealandimaginarypartsofthequaternions
Q and P.
The Multiplication with scalar γ can be defined as:
γQ=γe+γfi+γgj+γhk (3)
The conjugate complex Q⋆ of Q can be defined as:
Q⋆ =e−fi−gj−hk (4)
The multiplication of quaternions Q and R can be defined as follows:
(cid:79)
Q R=(Q R −Q R −Q R −Q R )
e e f f g g h h
+(Q R +Q R −Q R +Q R )i
f e e f h g g h (5)
+(Q R +Q R +Q R −Q R )j
g e h e e g f h
+(Q R −Q R +Q R +Q R )k
h e g f f g e h
The equation above clearly describes the exchange between quaternions Q and R,
indicating that Hamiltonian product is essential in quaternion neural networks. In
this study, we extensively employ the Hamiltonian product to learn the correlations
between music and dance, which forms the foundation of QEAN and enhances its
generalization ability.
To implement our approach, we combine music and motion features to create a
featurevector.Specifically,35-dimensionalmusicfeaturesand219-dimensionalmotion
featurescanbecombinedintoa254-dimensionalfeaturevectorthroughconcatenation,
based on dimension and time correspondence. Subsequently, we convert this concate-
natedfeaturevectorintoasequenceofquaternions.Inthisprocess,eachmusicfeature
and three-dimensional dance motion feature are broken down into four components,
representing a quaternion with a real part and three imaginary parts. As a result, the
original254-dimensionalfeaturevectoristransformedintoaquaternionsequencewith
a length of 63 (disregarding the last two dimensions as they are insufficient to form a
complete quaternion). Finally, we input this quaternion sequence into the Spin Posi-
tion Embedding for further processing. In this model, a position encoding is assigned
to each quaternion, enabling the capture of position information within the sequence.
By incorporating position information, the model gains a better understanding of
the sequence and improves its performance accordingly.
7θ
x'2
Encoded Encoded Encoded Encoded
Query/KeyQuery/KeyQuery/KeyQuery/Key Query/Key x2 Query/KeyQuery/KeyQuery/KeyQuery/Key
·········
x' x
Feature (x1,x2) 1 1 Position Embedding Feature
Spin Position Embedding
Fig. 3 The generalsituation of Spin Position Embedding. Specifically, theinput action sequences
andaudiosequencesinthispaperaregivenfeaturevectorrepresentationsafterbeingencodedbytheir
respectiveTransformers.Thefeaturevectorsoftheactionsequencesarexm,andthefeaturevectors
oftheaudiosequencesarexn.Thesewordvectorsarethenmultipliedbydifferentrotationmatrices
Rm,Rn accordingtotheirpositionsmandnintheirrespectivesequencestoachievethepositional
informationoffusion.Finally,theencodedvectorsoftheactionsequencesaretransformedintoquery
vectors qm,and the rotationally transformed key vectors kn of the encoded audio sequences are run
on a click to compute the correlation between the two modal sequences. With this Spin Position
Embedding,themodalitycanbettermodelthepositionalinformationofthetwosequences,aswell
asthecorrelationbetweenthem,thusincreasingthelearningofcross-modalrepresentations.
3.3 Spin Position Embedding
The main types of position embedding methods are relative position embedding
and absolute position embedding methods. In 2017, the Transformer [14] model was
proposed.Theconceptofpositionalembeddingwasintroducedinthismodeltoprovide
information about the position of each word or token in this input sequence. This
is crucial for NLP tasks that heavily rely on the relative position of words. In the
Transformermodel,positionalembeddingisusedtoencodeinformationaboutdifferent
positions using sine and cosine functions, and the positional embedding is updated at
differentfrequenciesfordifferentdimensions.Inthisway,themodelisabletolearnthe
relative positions of the tokens in the sequence. This way of position coding with sine
and cosine, which is also known as absolute position coding, is easy to implement and
relies directly on the absolute position without position loss, but this type of coding
has poor generalisation ability, the model only adapts to a specific length of absolute
position coding, and is prone to overfitting when the length varies, and performs
poorly on tasks in long sequences. Relative positional embedding, on the other hand,
is a method of obtaining positional embedding by using the relative distance or order
relationship between lexical elements to rely on. This embedding method can provide
relative position information between lexical elements instead of relying completely
on absolute position. Such an embedding approach highlights the relevance of lexical
elementsintermsofcontent,whichisconducivetocontentcomprehension,andavoids
the excessive computation caused by the exponential growth of absolute positional
embedding with position. Consequently, it improves the generalisation ability.
Motivated by the work of Jianlin Su [40], who proposed the Rotary Position
Embedding, a positional embedding method designed to enhance the performance of
the Transformer architecture by integrating relative positional information into self-
attention.ThepopularLLama2[10]modelcurrentlyemploysthispositionembedding
approach.Therefore, we borrowed from Su and embedded the extracted audio and
8motion features into self-attention in the form of rotated positions to better learn the
features in it and improve the computational efficiency. The basic idea can be seen in
Fig.3 .
First,wedefineasequenceoffeaturesoflengthN(sincemotionandaudiofeatures
operatesimilarlyintheprocessofSPE,TheOinthenextequationrepresentsdifferent
operations for different eigenvectors in different situations): F = {W }N . Where
N i i=1
w represents the i-th token in the input sequence, and the embedding corresponding
i
to the input sequence F is denoted as E = {x }N , where x represents the d-
N N i i=1 i
dimensional embedding vector of the i-th token w .
i
Before performing self-attention operations, we use the feature embedding vectors
to calculate the q, k, and v vectors and incorporate the corresponding positional
information. The function expressions are as follows:
q =O (x ,s) (6)
s q s
k =O (x ,t) (7)
t k t
v =O (x ,t) (8)
t v t
Here, q represents the query vector for the s-th token with positional information
s
s integrated into the feature vector x , while k and v represent the key and value
s t t
vectors for the t-th token with positional information t integrated into the feature
vector x .
t
Then, we need to compute the output of self-attention for the s-th feature embed-
ding vector x . This involves calculating an attention score between q and other
s s
k , and then multiplying the attention score by the corresponding v , followed by
t t
summation to obtain the output vector o :
s
exp(q √sTkt)
a = d (9)
s,t (cid:80)N exp(q √skj)
j=1 d
N
(cid:88)
o = a v (10)
s s,t n
n=1
Next, in order to leverage the relative positional relationships between the men-
tionedtokens,let’sassumethatthedotproductoperationbetweenthequeryvectorq
s
andthekeyvectork isrepresentedbyafunctiong.Theinputtofunctiongincludes
t
the word embedding vectors x , x and their relative position s-t:
s t
<O (x ,s),O (x ,t)>=g(x ,x ,s−t) (11)
q s k t s t
We then discover an alternative approach to position embedding that upholds the
aforementioned relationship.
9O (x ,s)=(W x )eisθ (12)
q s q s
O (x ,t)=(W x )eitθ (13)
k t k t
O(x ,x ,s−t)=Re[(W x )(W x )∗ei(s−t)θ] (14)
s t q s k t
Here, x represents any real number, e is the base of the natural logarithm, and i
is the imaginary unit in complex numbers.
WecancleverlyuseEuler’sformulaeix =cosx+isinx,wheretherealpartiscosx
and the imaginary part sinx is of a complex number.
After transformation, formulas O and g can be changed to:
eisθ =cos(sθ)+isin(sθ) (15)
eitθ =cos(tθ)+isin(tθ) (16)
ei(s−t)θ =cos((s−t)θ)+isin((s−t)θ) (17)
O (x ,s)=(W x )eisθ (18)
q s q s
Then, according to linear algebra, we can represent q using a matrix:
s
(cid:32) (cid:33) (cid:32) (cid:33)(cid:32) (cid:33)
q(1) W(11) W(12) x(1)
q = s =(W x )= q q s (19)
s q(2) q s W(21) W(22) x(2)
s q q s
O (x ,s)=(W x )eisθ =q eisθ (20)
q s q s s
Therefore, multiplying these two complex numbers, we get the following result:
q eisθ =[q(1)cos(sθ)−q(2)sin(sθ),q(2)cos(sθ)+q1sin(sθ) (21)
s s s s s
Then,wemagicallydiscoverthattheaboveexpressionisequaltothequeryvector
multiplied by a rotation matrix:
O (x ,s)=(W x )eisθ =q eisθ
q s q s s
(cid:18)
cos(sθ)
−sin(sθ)(cid:19)(cid:32) q(1)(cid:33)
= s (22)
sin(sθ) cos(sθ) q(2)
s
Similarly, the key vector k can be represented as follows:
t
O (x ,t)=(W x )einθ =k eitθ
k t k t t
10(cid:32) (cid:33)
=(cid:0)
cos(tθ)
−sin(tθ)(cid:1) k t(1)
k(2)
t
+(cid:0)
sin(tθ)
cos(tθ)(cid:1)(cid:18)
k
t2(cid:19)
(23)
k1
t
By rearranging the above formulas, we can simplify the following expression:
<O (x ,s),O (x ,t)>
q s k t
(cid:32)(cid:18)
cos(sθ)
−sin(sθ)(cid:19)T (cid:32) q(1)(cid:33)(cid:33)T
= s
sin(sθ) cos(sθ) q(2)
s
(cid:18)
cos(tθ)
−sin(tθ)(cid:19)(cid:32) k(1)(cid:33)
t
sin(tθ) cos(tθ) k(2)
t
(cid:16) (cid:17)(cid:18) cos((s−t)θ) −sin((s−t)θ)(cid:19)(cid:32) k(1)(cid:33)
= q(1) q(2) t (24)
s s sin((s−t)θ) cos((s−t)θ) k(2)
t
With the above formulas, we can summarize the following calculation process: In
simple terms, theprocess of self-attention withSpin Position Embedding involves, for
eachfeatureembeddingvectorinthetokensequence,firstcalculatingitscorresponding
query and key vectors. Then, for each token position, calculate the corresponding
rotatedpositionembeddinginformation.Afterthat,applytherotationtransformation
to the elements of the query and key vectors for each token position in pairs, and
finally, calculate the dot product between the query and key to obtain the result of
self-attention.
3.4 Quaternion Rotary Attention
For the features after the rotated attention module, we assume that there are N-
lengthqueryseriesX andanM-lengthkey-valuesseriesγ.Firstlytheoriginalχandγ
are projected onto the representation space,and a series of operations are performed:
Q=χWQ ∈RN×d, K =γWK ∈RM×d and V =γWV ∈RM×d.
Here, Q represents the query vector, K represents the key, V represents the value,
d represents the number of channels in the attention layer and W represents the
trainable weights. Then, QRA will calculate H =Attn(X,γ) to the map query series
to output H using key-value series.
11Transformer Network Quaternion Rotary Attention
Output
Linear Matmul
16x
Linear
Rotate Rotate
Multi-Head Attention
Q ωQ θQ K ωK θK V
Spin Position
Embedding
Input
Fig. 4 The overall of our Transformer structure.Our Transformer structure enhances the generali-
sation ability of the model by adding regularisation means such as Dropout in multiple places and
adjustingthenumberofAttentionheadstoexpandthemodelcapacityonthebasisoftheoriginal.
The absolute position information of the input sequence is converted into a polar coordinate repre-
sentationoftherelativepositionusingSpinPositionEmbedding,(ρ,θ)whereρdenotesthedistance
from the centre point, θ denotes the relative angle. This Spin Position Embedding module provides
betterlocalrelativepositionswithsomerotationalinvariance.Inthisway,ourmodelcanbettersup-
port some tasks that are sensitive to position information, such as behavioural sequence modelling
and3Dshapeanalysis.
Frequency/phase-Generation:
 ωQ   θQ
1 1
··· ,···=Conv(Q;W ωQ),Conv(Q;W θQ),
ωQ θQ
P P (25)
 ωK   θK
1 1
··· ,···=Conv(K;W ωK),Conv(K;W θK),
ωK θK
P P
Series-Rotation
Φ p(Q,posQ)=Q˜ei(2πω pQposQ+θ pQ), p=1,2,··· ,P
(26)
Ψ p(K,posK)=K˜ej(2πω pKposK+θ pK), p=1,2,··· ,P
12Series-Attention with softmax-kernel (shown in Fig.5)
(cid:32) P (cid:33)
S =softmax √1 (cid:88) Re[Φ (Q,posQ )Ψ (K,posK )H] (27)
p p
P d
p=1
Series-Aggregation:
H =SV (28)
Fig.5 Athree-dimensionalillustrationofarotatedsoftmax-kernel.Therotatedsoftmax-kernelrep-
resents the embeddings in quaternion form and rotates them using the angular frequency ω .Thus,
embeddingswithdifferentphasescanbedistinguished.Finally,thesimilarityoftherotatedembed-
dingsismeasuredbymeasuringtheexponentialdotproductbetweenthem.
Here, we hypothesize that the series have P periods, and P is a hyper-parameter .
In frequency/phase-generation step, we utilize 1D convolutions with activation ReLU
to generate P latent frequencies ωQ ∈ [0,+∞)N×1(ωK is similar). Convolutions
1∼P 1∼P
can effectively capture local contexts of each time step to generate reliable latent
frequencies, and these latent frequencies are not identical at each time step imply-
ing variable periods. Moreover, to account for phase shifts, we additionally generate
P latent phases θQ ∈ (−π,π)N×1 using 1D convolutions with activation π ·tanh
1∼P
(θK is similar).In series-rotation step, we rotate the representations at each time
1∼P
step according to the learned latent frequencies and phases in the previous step.Each
row vector of Q˜,K˜ is in the quaternion form of the corresponding row vector of Q,
k, and posQ = [0,1,2,··· ,N −1]T/N,posK = [0,1,2,··· ,M −1]T/M are position
vectors of series Q and K, respectively. In the series-attention step, to integratedly
capture position-wise similarity under multiple periods, the unnormalized similarity
is the mean of quaternion dot-product under multiple rotations. Finally, in the series-
aggregation step, the outputs H ∈ RN×d is generated using the softmax-normalized
similarity.Inpractice,weemploythemulti-headvariantofQRA,andwillnotgointo
details here, as it can be derived quite directly. Notice that, QRA is more expressive
thancanonicaldot-productattention.WhenP=1,ω=0andθ=0,QRAdegenerates
into canonical attention.
134 Experiments
4.1 Datasets
TheAIST++[41]dancemovementdatasetwasconstructedfromtheAISTdance[19]
videodatabase.Awell-developedprocesswasdesignedforestimatingcameraparame-
ters,3Dhumankeypointsand3Dhumandancemovementsequencesfrommulti-view
videos. The dataset provides 3D human keypoint annotations and camera parameters
for 10.1 million images covering 30 different subjects in 9 viewpoints. These features
makeitthelargestandrichestdatasetcontaining3Dhumankeypointannotationscur-
rently available. Additionaly, the dataset contains 1,408 3D human dance movement
sequencesrepresentedasjointrotationsandroottrajectories.Thesedancemovements
areevenlydistributedacross10dancegenresandcontainhundredsofchoreographies.
Thedurationofthemovementsrangesfrom7.4to48.0seconds,andeachdancemove-
ment is accompanied by corresponding music. Based on these annotations, AIST++
is designed to support multiple tasks including multi-view human keypoint estima-
tion, human motion prediction/generation, and cross-modal analysis between human
motion and music.
4.2 Implementation Details
In our primary experiments, the model takes a seed motion sequence spanning 120
frames (2 seconds) and a music sequence covering 240 frames (4 seconds) as input.
These two sequences are aligned at the initial frame, and the model’s output consists
ofafuturemotionsequencewithN=20framessupervisedbyL2loss.Duringtheinfer-
ence process, future motions are continuously generated in an auto-regressive manner
at 60 FPS, with only the first predicted motion retained at each step.For music fea-
ture extraction, we employ the publicly available audio processing toolbox, Librosa
[42], which includes 1-dimensional envelope, 20-dimensional MFCC, 12-dimensional
chroma, 1-dimensional one-hot peaks, and 1-dimensional one-hot beats, resulting in
a 35-dimensional music feature. The motion features combine a 9-dimensional repre-
sentation of rotation matrices for all 24 joints with a 3-dimensional global translation
vector, resulting in a 219-dimensional motion feature. These raw audio and motion
features are initiallyembedded into 800-dimensional hidden representations using lin-
ear layers, with learnable position embeddings added before inputting them into the
transformer layers. All three transformers (audio, motion, cross-modal) feature 16
attentionheadswithahiddensizeof800.Intermsoftrainingdetails,allexperiments
aretrainedusingtheAdamoptimizerwithabatchsizeof16.Thelearningratestarts
at1e-4anddecaysto{1e-5,1e-6}at{90k,150k}steps,respectively.Trainingconcludes
after 500k steps, taking approximately 2 days on one RTX 3090. The baseline com-
parison includesthe latest works on3D dance generationwith music and seed motion
asinput,suchasLi[19]andLietal[4].Foramorecomprehensiveevaluation,wealso
compare it with the recent state-of-the-art 2D dance generation method, DanceRevo-
lution [5]. We adapt this work to output 3D joint positions for a direct quantitative
comparison with our results, even though joint positions do not allow for immediate
repositioning.Theofficialcodeprovidedbytheauthorsisusedtotrainandtestthese
baselines on the same dataset as ours.
144.3 Quanitative Evalutation
In this section, we assess the performance of our proposed Multi-modal Roformer
across three key dimensions: (1) motion quality (2) generation diversity and (3)
motion-music correlation. The results presented in Table 1 demonstrate that, under
identical experimental conditions, our model surpasses state-of-the-art methods [2, 6,
7] in these aspects.
Motion Quality: Similar to previous studies, we assess the quality of generated
motion by computing the Frechet Inception distance (FID) [43], which measures the
dissimilarity between the distribution of generated motion and ground-truth motion.
Tocapturemotionfeatures,weutilizetwometiculouslycraftedmotionfeatureextrac-
tors, as undisclosed motion encoders were employed in earlier works [44]. These
extractorsinclude:(1)ageometricfeatureextractor,generatingabooleanvectorthat
representsgeometricrelationshipsamongspecificbodypointsinthemotionsequence,
and(2)adynamicfeatureextractor,mappingthemotionsequencetocapturedynamic
aspects such as velocity and acceleration.We designate FID based on these geometric
and dynamic features as FID and FID , respectively. The metrics are computed
g d
by comparing real dance motion sequences in the AIST++ test set with 40 gener-
ated motion sequences, each comprising T = 1200 frames (20 seconds). As depicted
in Table 1, our generated motion sequences exhibit distributions that are closer to
ground-truth motion compared to the three methods.
Generation Diversity:We also assess the model’s capacity to generate diverse
dance movements in response to different input music, comparing its performance
to baseline methods. Following a methodology similar to previous research [45], we
compute the average Euclidean distance in the feature space of 40 generated motions
fromtheAIST++testsettoquantifydiversity.Themotiondiversityingeometricand
dynamic feature spaces is denoted as Dist and Dist , respectively.Table1 illustrates
g k
thatourmethodexcelsingeneratingmorediversedancemovementsincomparisonto
the baselines, with the exception of Li [29]. The latter discretizes motions, resulting
in discontinuous outputs and elevated Dist .
k
Motion-MusicCorrelation:Moreover,wegaugethecorrelationbetweenthegen-
erated 3D motion and input music by introducing a novel metric known as the Beat
Alignment Score. This metric evaluates the motion-music correlation by measuring
the similarity between the beats in the motion and music. Librosa [42] is employed to
extract music beats, while motion beats are computed as local minima in the motion
velocity.TheBeatAlignmentScoreisarticulatedastheaveragedistancebetweeneach
motion beat and its nearest music beat. To be specific, our Beat Alignment Score is
defined as:
BeatAlign=
1(cid:88)z exp(−min∀td
j
∈Bd||tc
i
−td j||2
) (29)
z 2α2
i=1
where Bc = {tc} is the set of motion beats, Bd = td is the music beats, and α is a
i j
parameter for normalizing sequences with different FPS.
15Wesetα=3forallexperimentssincetheFPSforallourexperimentalsequencesis
60.AsimilarmetriccalledBeatHitRateisintroducedin,butitreliesonmanuallyset
thresholds for alignment (“hits”) depending on the dataset, while our metric directly
measures distances. This metric is explicitly designed to be unidirectional, as dance
movements do not necessarily need to match every music beat. On the other hand,
each dynamic beat should have a corresponding music beat. To calibrate the results,
we compute correlation metrics for the entire AIST++ dataset (upper bound) and
randomlypaireddata(lowerbound).AsshowninTable1,ourgeneratedmotionshows
better correlation with input music compared to the baselines. However, there is still
considerableroomforimprovementforallmethods,includingours,whencomparedto
actualdata.Thisreflectsthatmusic-motioncorrelationremainsachallengingproblem.
Motion Quality Motion Diversity Motion-Music Corr
Methods FID k↓ FIDg↓ Dist k↑ Distg↑ BeatAlign↑
AIST++ - - 9.057 7.556 0.292
AIST++(random) - - - - 0.213
Lietal[5]. 86.43 20.58 6.85 4.93 0.232
Dancenet[6] 69.18 17.76 2.86 2.72 0.232
DanceRevolution[7] 73.42 31.01 3.52 2.46 0.22
FACT(baseline)[19] 48.95 28.1 4.9 6.69 0.232
our 30.1 11.5 7.82 9.37 0.239
Table 1 Conditional Motion Generation Evaluation on AIST++ dataset.
Comparingtothethreerecentstate-of-artmethods,ourgeneratedmotionsaremore
realistic,bettercorrelatedwithinputmusicandmorediversified.
4.4 Ablation Study
We conducted ablation studies on the Spin Position Embedding and Multi-modal
Quaternion parameterization, respectively. The quantitative scores are shown in 2.
Position Embedding In the ablation experiments focused on position coding,
we explore two distinct approaches and conduct experiments based on the follow-
ing configurations: (1) a learnable coding approach for absolute positions (baseline),
and (2) a rotary coding approach for relative positions. Method 2 was selected
to introduce explicit relative position dependence in the self-attention formulation.
This choice offers increased flexibility in sequence length, a potential reduction in
dependencies between tokens, and the capacity to encode relative positions for lin-
ear self-attention.As illustrated in Table 2, we observe that the rotational embedding
method of relative position leads to a significant reduction in the FID values com-
g
pared to the original method. This indicates that dances generated using rotary
position embedding are notably closer to reality.
Quaternion parameterizationHere,weperformedablationexperimentsonthe
original baseline as well as with the addition of the QRA module. Through Table 2,
we can observe that Quaternion Rotary Attention (QRA), by introducing quaternion
operations, is able to fully explore the relationship between audio and motion, and
achieves more significant enhancement results.
16FID ↓ FID ↓ BeatAlign ↑
k g
baseline 48.95 28.1 0.232
baseline+Spin Position Embedding 30.1 11.5 0.239
baseline+QRA 46.33 26.2 0.236
Table 2 Ablation Study on Spin Position Embedding and Quaternion Rotary
Attention.Asillustratedinthetable,theexperimentalresultsclearlydemonstratetheeffectiveness
ofourproposedmethod.Thegraphshowsasignificantimprovementinperformancemetricswhen
comparedtothebaselineapproach.
Fig. 6 Frameextraction.Thevisualrepresentationclearlyemphasizestheeffectivenessofourpro-
posedmethod.
Fig.7 Thisimageillustratestheframeextractioneffectofadancegeneratedbyalternativemethods.
During the post-production phase, the generated dance movements exhibit phenomena of dance
collapseandunscientificlimbfolding.
5 Conclusion
We propose a network called QEAN for generating 3D dance movements. QEAN
employs Spin Position Embedding (SPE) the position encoding part to embed the
position information in a rotational manner in the self-attention, which improves the
model’s representation of the sequences and enhances the model’s understanding of
the human movements in terms of their temporal order. Additionally, we propose
QuaternionRotaryAttention(QRA),aquaternion-valuedrelationallearningnetwork,
which uses quaternion values to explore the temporal coordination between music
and dance. To demonstrate the superiority of QEAN, we conducted experiments on
the AIST++ dataset. The results of the relevant experimental data demonstrate the
superiority of our approach in the 3D dance generation task. Furthermore, the results
of our ablation experiments illustrate the importance of SPE and QRA in this task.
6 Acknowledgement
This work was supported in part by National Natural Science Foundation under
Grant 92267107, the Science and Technology Planning Project of Guangdong under
Grant2021B0101220006,ScienceandTechnologyProjectsinGuangzhouunderGrant
202201011706, Key Areas Research and Development Program of Guangzhou under
Grant 2023B01J0029, Science and technology research in key areas in Foshan under
17Grant 2020001006832, Key Area Research and Development Program of Guangdong
Province under Grant 2018B010109007 and 2019B010153002, Science and technology
projects of Guangzhou under Grant 202007040006, and Guangdong Provin-cial Key
Laboratory of Cyber-Physical System under Grant 2020B1212060069.
7 Declarations
Conflict of interest We declare that we do not have any commercial or associative
interest that represents a conflict of interest in connection with the work submitted.
References
[1] Yue Yang and Ensi Zhang. “Cultural thought and philosophical elements of
singing and dancing in Indian films”. In: Trans/Form/A¸c˜ao 46 (Dec. 2023),
pp. 315–328. doi: 10.1590/0101-3173.2023.v46n4.p315.
[2] Mark Siciliano. “A citation analysis of business librarianship: Examining the
Journal of Business and Finance Librarianship from 1990–2014”. In: Journal
of Business & Finance Librarianship 22 (2017), pp. 81–96. url: https://api.
semanticscholar.org/CorpusID:63474056.
[3] Andreas Aristidou, Efstathios Stavrakis, Margarita Papaefthimiou, George
Papagiannakis, and Yiorgos Chrysanthou. “Style-based motion analysis for
dance composition”. In: The Visual Computer 34 (2018), pp. 1725–1737. url:
https://api.semanticscholar.org/CorpusID:27531229.
[4] Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler, and
Hao Li. “Learning to Generate Diverse Dance Motions with Transformer”. In:
ArXiv abs/2008.08171 (2020). url: https://api.semanticscholar.org/CorpusID:
221173065.
[5] Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, and Daxin Jiang.
“Dance Revolution: Long-Term Dance Generation with Music via Curriculum
Learning”.In:InternationalConferenceonLearningRepresentations.2020.url:
https://api.semanticscholar.org/CorpusID:235614403.
[6] Xinjian Zhang, Yi Xu, Su Yang, Longwen Gao, and Huyang Sun. “Dance
Generation with Style Embedding: Learning and Transferring Latent Represen-
tations of Dance Styles”. In: ArXiv abs/2104.14802 (2021). url: https://api.
semanticscholar.org/CorpusID:233476346.
[7] Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, and Daxin Jiang.
“Dance Revolution: Long-Term Dance Generation with Music via Curriculum
Learning”.In:InternationalConferenceonLearningRepresentations.2020.url:
https://api.semanticscholar.org/CorpusID:235614403.
[8] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam M. Shazeer. “Sched-
uled Sampling for Sequence Prediction with Recurrent Neural Networks”. In:
ArXiv abs/1506.03099 (2015). url: https://api.semanticscholar.org/CorpusID:
1820089.
[9] Zhifeng Xie, Wenling Zhang, Bin Sheng, Ping Li, and C. L. Philip Chen.
“BaGFN:BroadAttentiveGraphFusionNetworkforHigh-OrderFeatureInter-
actions”. In: IEEE Transactions on Neural Networks and Learning Systems
1834 (2021), pp. 4499–4513. url: https://api.semanticscholar.org/CorpusID:
238476689.
[10] Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, and
JitendraMalik.“LearningIndividualStylesofConversationalGesture”.In:2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
(2019), pp. 3492–3501. url: https://api.semanticscholar.org/CorpusID:
182952539.
[11] BinSheng,PingLi,RiazAli,andC.L.PhilipChen.“ImprovingVideoTemporal
ConsistencyviaBroadLearningSystem”.In:IEEETransactionsonCybernetics
52.7 (2022), pp. 6662–6675. doi: 10.1109/TCYB.2021.3079311.
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,
Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. “An Image is
Worth 16x16 Words: Transformers for Image Recognition at Scale”. In: ArXiv
abs/2010.11929 (2020). url: https://api.semanticscholar.org/CorpusID:
225039882.
[13] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,
and Baining Guo. “Swin Transformer: Hierarchical Vision Transformer using
Shifted Windows”. In: 2021 IEEE/CVF International Conference on Computer
Vision (ICCV) (2021), pp. 9992–10002. url: https://api.semanticscholar.org/
CorpusID:232352874.
[14] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention is All you
Need”. In: Neural Information Processing Systems. 2017. url: https://api.
semanticscholar.org/CorpusID:13756489.
[15] Xiao Lin, Shuzhou Sun, Wei Huang, Bin Sheng, Ping Li, and David Dagan
Feng. “EAPT: Efficient Attention Pyramid Transformer for Image Processing”.
In: IEEE Transactions on Multimedia 25 (2021), pp. 50–61. url: https://api.
semanticscholar.org/CorpusID:245536278.
[16] Zinuo Li, Xuhang Chen, Chi-Man Pun, and Xiaodong Cun. “High-Resolution
Document Shadow Removal via A Large-Scale Real-World Dataset and A
Frequency-Aware Shadow Erasing Net”. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV). Oct. 2023, pp. 12449–
12458.
[17] Zinuo Li, Xuhang Chen, Shuqiang Wang, and Chi-Man Pun. “A Large-Scale
Film Style Dataset for Learning Multi-frequency Driven Film Enhancement”.
In:ProceedingsoftheThirty-SecondInternationalJointConferenceonArtificial
Intelligence, IJCAI-23. Ed. by Edith Elkind. Main Track. International Joint
Conferences on Artificial Intelligence Organization, Aug. 2023, pp. 1160–1168.
doi: 10.24963/ijcai.2023/129. url: https://doi.org/10.24963/ijcai.2023/129.
[18] Shenghong Luo, Xuhang Chen, Weiwen Chen, Zinuo Li, Shuqiang Wang, and
Chi-Man Pun. “Devignet: High-Resolution Vignetting Removal via a Dual
Aggregated Fusion Transformer With Adaptive Channel Expansion”. In: arXiv
preprint arXiv:2308.13739 (2023).
19[19] Ruilong Li, Sha Yang, David A. Ross, and Angjoo Kanazawa. “AI Chore-
ographer: Music Conditioned 3D Dance Generation with AIST++”. In: 2021
IEEE/CVF International Conference on Computer Vision (ICCV) (2021),
pp. 13381–13392. url: https://api.semanticscholar.org/CorpusID:236882798.
[20] Lian Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian,
Chen Change Loy, and Ziwei Liu. “Bailando: 3D Dance Generation by Actor-
Critic GPT with Choreographic Memory”. In: 2022 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (2022), pp. 11040–11049.
url: https://api.semanticscholar.org/CorpusID:247627867.
[21] Dario Pavllo, Christoph Feichtenhofer, Michael Auli, and David Grangier.
“Modeling Human Motion with Quaternion-Based Neural Networks”. In: Inter-
national Journal of Computer Vision 128 (2019), pp. 855–872. url: https://
api.semanticscholar.org/CorpusID:59158790.
[22] Weizhao Ma, Mengxiao Yin, Guiqing Li, Feng Yang, and Kan Chang.
“PCMG:3D point cloud human motion generation based on self-attention
and transformer”. In: The Visual Computer (2023). url: https : / / api .
semanticscholar.org/CorpusID:261566852.
[23] David Greenwood, Stephen D. Laycock, and Iain Matthews. “Predicting Head
PosefromSpeechwithaConditionalVariationalAutoencoder”.In:Interspeech.
2017. url: https://api.semanticscholar.org/CorpusID:11113871.
[24] YuhangHuang,JunjieZhang,ShuyanLiu,QianBao,DanZeng,ZhinengChen,
and Wu Liu. “Genre-Conditioned Long-Term 3D Dance Generation Driven by
Music”. In: ICASSP 2022 - 2022 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) (2022), pp. 4858–4862. url: https://
api.semanticscholar.org/CorpusID:249437513.
[25] Sepp Hochreiter and Ju¨rgen Schmidhuber. “Long Short-Term Memory”. In:
Neural Computation 9(1997),pp.1735–1780.url:https://api.semanticscholar.
org/CorpusID:1915014.
[26] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen.
“Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen
Convolutional CLIP”. In: ArXiv abs/2308.02487 (2023). url: https://api.
semanticscholar.org/CorpusID:260611350.
[27] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-
Philippe Morency, and Ruslan Salakhutdinov. “Multimodal Transformer for
Unaligned Multimodal Language Sequences”. In: Proceedings of the conference.
AssociationforComputationalLinguistics.Meeting 2019(2019),pp.6558–6569.
url: https://api.semanticscholar.org/CorpusID:173990158.
[28] Ziheng Wu, Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Xing Shi, and Jun Huang.
“EasyPhoto: Your Smart AI Photo Generator”. In: 2023. url: https://api.
semanticscholar.org/CorpusID:263829612.
[29] Purva Tendulkar, Abhishek Das, Aniruddha Kembhavi, and Devi Parikh. “Feel
The Music: Automatically Generating A Dance For An Input Song”. In: ArXiv
abs/2006.11905 (2020). url: https://api.semanticscholar.org/CorpusID:
219572850.
20[30] Jogendra Nath Kundu, Himanshu Buckchash, Priyanka Mandikal, and Rahul.
“Cross-Conditioned Recurrent Networks for Long-Term Synthesis of Inter-
Person Human Motion Interactions”. In: 2020 IEEE Winter Conference on
Applications of Computer Vision (WACV) (2020), pp. 2713–2722. url: https:
//api.semanticscholar.org/CorpusID:214675800.
[31] Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohith Krishnan Pil-
lai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, Tamara L.
Berg, Mohit Bansal, Jingjing Liu, Lijuan Wang, and Zicheng Liu. “VALUE: A
Multi-TaskBenchmarkforVideo-and-LanguageUnderstandingEvaluation”.In:
ArXiv abs/2106.04632 (2021). url: https://api.semanticscholar.org/CorpusID:
235377363.
[32] Partha Ghosh, Jie Song, Emre Aksan, and Otmar Hilliges. “Learning Human
Motion Models for Long-Term Predictions”. In: 2017 International Conference
on 3D Vision (3DV) (2017), pp. 458–466. url: https://api.semanticscholar.
org/CorpusID:13549534.
[33] Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang,
and Nan Duan. “Visual ChatGPT: Talking, Drawing and Editing with Visual
Foundation Models”. In: ArXiv abs/2303.04671 (2023). url: https://api.
semanticscholar.org/CorpusID:257404891.
[34] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang,
and Jie Tang. “GLM: General Language Model Pretraining with Autoregres-
sive Blank Infilling”. In: Annual Meeting of the Association for Computational
Linguistics. 2021. url: https://api.semanticscholar.org/CorpusID:247519241.
[35] Zongwen Bai, Xiaohuan Chen, Meili Zhou, Tingting Yi, and Wei-Che Chien.
“Low-rankmultimodalfusionalgorithmbasedoncontextmodeling”.In:Journal
of Internet Technology 22.4 (2021), pp. 913–921.
[36] Daniel Holden, Jun Saito, and Taku Komura. “A deep learning framework for
character motion synthesis and editing”. In: ACM Transactions on Graphics
(TOG) 35 (2016), pp. 1–11. url: https://api.semanticscholar.org/CorpusID:
18149328.
[37] Haibo Qiu, Chunyu Wang, Jingdong Wang, Naiyan Wang, and Wenjun Zeng.
“CrossViewFusionfor3DHumanPoseEstimation”.In:2019IEEE/CVFInter-
national Conference on Computer Vision (ICCV) (2019), pp. 4341–4350. url:
https://api.semanticscholar.org/CorpusID:201891326.
[38] YeZhu,KyleOlszewski,YuehuaWu,PanosAchlioptas,MengleiChai,YanYan,
and S. Tulyakov. “Quantized GAN for Complex Music Generation from Dance
Videos”. In: ArXiv abs/2204.00604 (2022). url: https://api.semanticscholar.
org/CorpusID:247922422.
[39] Zewen Zheng, Guoheng Huang, Xiaochen Yuan, Chi-Man Pun, Hongzhi Liu,
and Wing-Kuen Ling. “Quaternion-Valued Correlation Learning for Few-Shot
Semantic Segmentation”. In: IEEE Transactions on Circuits and Systems for
Video Technology 33 (2023), pp. 2102–2115. url: https://api.semanticscholar.
org/CorpusID:253661872.
[40] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. “RoFormer:
Enhanced Transformer with Rotary Position Embedding”. In: ArXiv
21abs/2104.09864 (2021). url: https://api.semanticscholar.org/CorpusID:
233307138.
[41] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto.
“AIST Dance Video Database: Multi-Genre, Multi-Dancer, and Multi-Camera
DatabaseforDanceInformationProcessing”.In:InternationalSocietyforMusic
Information Retrieval Conference. 2019. url: https://api.semanticscholar.org/
CorpusID:208334750.
[42] Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W. Ellis, Matt McVicar,
Eric Battenberg, and Oriol Nieto. “librosa: Audio and Music Signal Analysis
in Python”. In: SciPy. 2015. url: https://api.semanticscholar.org/CorpusID:
33504.
[43] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and
Sepp Hochreiter. “GANs Trained by a Two Time-Scale Update Rule Converge
toaLocalNashEquilibrium”.In:Neural Information Processing Systems.2017.
url: https://api.semanticscholar.org/CorpusID:326772.
[44] Kensuke Onuma, Christos Faloutsos, and Jessica K. Hodgins. “FMDistance: A
FastandEffective DistanceFunctionforMotionCapture Data”.In: Eurograph-
ics. 2008. url: https://api.semanticscholar.org/CorpusID:8323054.
[45] Hao Hao Tan and Mohit Bansal. “LXMERT: Learning Cross-Modality Encoder
Representations from Transformers”. In: Conference on Empirical Methods in
Natural Language Processing. 2019. url: https://api.semanticscholar.org/
CorpusID:201103729.
22