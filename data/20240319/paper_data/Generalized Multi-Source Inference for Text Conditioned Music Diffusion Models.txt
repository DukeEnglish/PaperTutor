GENERALIZEDMULTI-SOURCEINFERENCEFORTEXTCONDITIONEDMUSIC
DIFFUSIONMODELS
EmilianPostolache1,GiorgioMariani1,LucaCosmo2,EmmanouilBenetos3,EmanueleRodola`1
1SapienzaUniversityofRome 2Ca’FoscariUniversityofVenice
3QueenMaryUniversityofLondon
ABSTRACT separation.Despitebeingaversatilecompositionalmodelformusic,
MSDMhasthreelimitations: (i)Itrequiresknowledgeofseparated
Multi-Source Diffusion Models (MSDM) allow for compositional
coherent sources, which are hard to acquire. (ii) It architecturally
musicalgenerationtasks: generatingasetofcoherentsources,cre-
assumes a fixed number of sources and their respective class type
atingaccompaniments, andperformingsourceseparation. Despite
(e.g.,Bass,Drums,Guitar,Piano). (iii)Itisimpossibletocondition
their versatility, they require estimating the joint distribution over
the sources on rich semantic information, as commonly done with
thesources,necessitatingpre-separatedmusicaldata,whichisrarely
text-conditionedmusicmodels. Thesecondapproach,basedonsu-
available,andfixingthenumberandtypeofsourcesattrainingtime.
pervisedinstructionprompting[15,16],fine-tunesalatentdiffusion
This paper generalizes MSDM to arbitrary time-domain diffusion
modelwithinstructionsthatallowadding,removing,andextracting
models conditioned on text embeddings. These models do not re-
sourcespresentinamusicaltrack.Althoughthisapproachaddresses
quire separated data as they are trained on mixtures, can parame-
theissues(ii)and(iii)ofMSDM,itdoesnotsolvetheproblem(i),
terize an arbitrary number of sources, and allow for rich semantic
necessitatingpre-separateddata. Astrategyforscalingbothmodels
control. Weproposeaninferenceprocedureenablingthecoherent
istrainingwithdataobtainedbyseparatingsourcesfrommixtures
generationofsourcesandaccompaniments. Additionally,weadapt
using a pre-trained separator [17]. This approach, though, is not
the Dirac separator of MSDM to perform source separation. We
flexible because such separated data contains artifacts, and we are
experimentwithdiffusionmodelstrainedonSlakh2100andMTG-
limitedtothenumberandtypeofsourcestheseparatorcanhandle.
Jamendo,showcasingcompetitivegenerationandseparationresults
Wedevelopanovelinferenceprocedureforthetask,calledGen-
inarelaxeddatasetting.
eralizedMulti-SourceDiffusionInference(GMSDI),thatcanbeused
Index Terms— Music Generation, Diffusion Models, Source in combination with any text-conditioned (time-domain) diffusion
Separation modelformusic.Suchamethod:(i)Requiresonlymixturedatafor
training,resultinginanunsupervisedalgorithmwhenpairedwitha
contrastiveencoder.(ii)Parameterizesanarbitrarynumberandtype
1. INTRODUCTION
ofsources.(iii)Allowsforrichsemanticcontrol.Toourknowledge,
thisisthefirstgeneralalgorithmforunsupervisedcompositionalmu-
The task of musical generation has seen significant advancements
sicgeneration.Afterdevelopingtherequiredbackgroundnotionsin
recently,thankstodevelopmentsingenerativemodels.Thefamilies
Section2,wedeveloptheinferencetechniquesinSection3.Wede-
of generative models showcasing state-of-the-art results are latent
tailtheexperimentalsetupinSection4andshowempiricalresults
language models [1] and (score-based) diffusion models [2, 3, 4].
inSection5.WeconcludethepaperinSection6.
Latentlanguagemodelsmapacontinuous-domain(timeorspectral)
signaltoasequenceofdiscretetokensandestimateadensityover
suchsequencesautoregessively[5,6]orviamask-modeling[7].Dif-
2. BACKGROUND
fusionmodels[8,9],ontheotherhand,operateoncontinuousrepre-
sentations(time,spectral,orlatentdomains),capturingthegradient
AmusicaltrackyisamixtureofKinstrumentalandvocalsources
ofthelog-densityperturbedbyanosingprocess(Gaussian).Despite
x = {x } . Therefore, wehavey =
(cid:80)K
x , withK de-
differences between these generative models, they typically share k k∈[K] k=1 k
pendingonthemixture. Fixingasourcex ,wedenotethecomple-
somemechanismsforconditioningonrichtextualembeddings,ob- k
tainedeitherusingtext-onlyencoders[10]oraudio-textcontrastive mentarysetwithx k¯ = {x l} l∈[K]−{x k}. Whilewetypicallydo
nothavedirectaccesstotheaudioconstituents{x } ,weare
encoders[11,12,13]. Suchamechanismallowsgeneratingamusi- k k∈[K]
usuallyequippedwithatextembeddingzwhichprovidesinforma-
caltrackfollowinganaturallanguageprompt.
tion about the sources. We can obtain z = Etext(q) by encoding
Generativemodelsformusictypicallyoutputonlyafinalmix- ϕ
a text description q with a text-only encoder Etext, or use a pre-
ture. As such, generating the constituent sources is challenging. ϕ
trainedcontrastiveaudio-textencoderEcontr toextractembeddings
Thisimpliesthatmusicalgenerativemodelsarehardtoemployin ϕ
bothfromtheaudiomixturesz = Econtr(y)andfromtextdescrip-
musicproductiontasks,wherethesubsequentmanipulationofsub- ϕ
tionsz=Econtr(q).
tracks, creationofaccompaniments, andsourceseparationisoften ϕ
required. Two existing approaches aim to address this issue. The
firstapproach,calledMulti-SourceDiffusionModels(MSDM)[14], 2.1. Text-conditionedScore-basedDiffusionModels
trainsadiffusionmodelintimedomainon(supervised)setsofco-
herentsourcesviewedasdifferentchannelswithoutconditioningon Weworkwithcontinuous-timescore-based[4]diffusionmodels. A
textual information. Such a model allows for generating a set of text-conditionedscore-baseddiffusionmodelS parameterizesthe
θ
coherentsources,creatingaccompaniments,andperformingsource logarithmoftheperturbedaudiomixturedensity,conditionedonthe
4202
raM
81
]DS.sc[
1v60711.3042:viXraFig.1.DiagramforunconditionalgenerationprocedurewithGMSDI,samplingtwocoherentsources.
textualembedding: With the model, it is possible to perform music generation and
source separation. Total (unconditional) generation integrates Eq.
∇ y(t)logp(y(t)|z)≈S θ(y(t),z,σ(t)), (1) (3)directly, generatingallcoherentsources{x k}
k∈[K]
composing
atrack. Partial(conditional)generation(i.e.,accompanimentgen-
(cid:82)
wherep(y(t)|z)= y(0)p(y(t)|y(0))p(y(0)|z),with eration)fixesaknownsubsetofsourcesx
I
= {x i}
i∈I
(I ⊂ [K])
and generates the complementary subset x I¯ (I¯ = [K] − I) co-
p(y(t)|y(0))=N(y(t)|y(0),σ2(t)I) (2) herently. Sourceseparationextractsallsourcesfromanobservable
mixture y(0), integrating, for all k, the approximate posteriors
a Gaussian perturbation kernel depending on a noise schedule ∇ log(x(t)|y(0)),modeledwithDiracdeltalikelihoodfunc-
{σ(t)} t∈[0,T].WetrainS θminimizing: tiox nk s( .t) They propose a contextual separator using SMSDM and a
θ
weakly supervised separator, using a model S for each source
E E E [L ] , θ,k
t∼U[0,T] y(0)∼p(y(0)|z) y(t)∼p(y(t)|y(0)) SM type. When constraining the last source, the weakly supervised
separatorsamplesfrom:
whereL isthedenoisingscore-matchingloss[2,3]:
SM
L
SM
=∥S θ(y(t),z,σ(t))−∇ y(t)logp(y(t)|y(0))∥2 2.
S (x (t),σ(t))−S
(y(0)−K (cid:88)−1
x (t),σ(t)). (4)
θ,k k θ,K k
Atinferencetime,weuseclassifier-freeguidance[18],integrating k=1
S∗(y(t),z,σ(t))
θ 3. GENERALIZEDMULTI-SOURCEDIFFUSION
= S θ(y(t),z,σ(t))+w(S θ(y(t),z,σ(t))−S θ(y(t),z∗,σ(t))), INFERENCE
where z∗ is a fixed learned embedding modeling the uncondi-
We train (or use) a text-conditioned diffusion model (Eq. (1))
tional∇ logp(y(t)),andw ∈ Ristheembeddingscalehyper-
y(t) S (y(t),z,σ(t)),withpairsofaudiomixturesy(t)andassociated
parameter. Wecanuseanegativeembedding[19]insteadofz∗ to θ
textembeddingsz,containinginformationaboutthesourcespresent
betterguideinference. Withanabuseofnotation, wewillreferto
inthemixture.Weassumethateachtextembeddingzisoftheform
S θ∗asS θ.
z ⊗···⊗z
(morecompactly(cid:78)K
z ),whereeachz describes
1 K k=1 k k
asourcex presentinyand⊗denotesanencodingofconcatenated
k
2.2. Multi-SourceDiffusionModels textualinformation(e.g.,z ⊗···⊗z =Etext(q ,...,q ),with
1 K ϕ 1 K
Etext(q ) = z ). Theideaistoleveragesuchtextembeddingsfor
In [14], authors assume a fixed number K of coherent sources of ϕ k k
parameterizingtheindividualsourcescorefunctions:
known type {x } contained in the mixture y. They train a
k k∈[K]
Multi-Source Diffusion Model (MSDM), an unconditional score-
∇ logp(x (t)|z )≈S (x (t),z ,σ(t)), (5)
baseddiffusionmodelS θMSDM thatcapturesthejointdistributionof xk(t) k k θ k k
coherentsources:
even if the model is trained only on mixtures. We devise a set of
∇ (x1(t),...,xK(t))logp(x 1(t),...,x K(t)) i sn iofe nre Inn fc ee rep nro cec ,ed au br lees tofo sr oS lvθ e,c tha elle td asG ksen oe fr Sal Miz Se Dd MM inul tt hi- eS ro eu lr ac xe edD dif afu ta-
≈ SMSDM((x (t),...,x (t)),σ(t)). (3) θ
θ 1 K setting.3.1. Totalgeneration 25
WeaklySupervisedMSDM[14]
Inordertogenerateacoherentsetofsources{x } ,described GMSDI
k k∈[K]
bytextembeddings{z k} k∈[K],wecansamplefromtheconditionals 20 GMSDI(NegPrompt)
p(x k(t)|x k¯(t),y(t),z 1,...,z K,z 1⊗···⊗z K):
p(x(t),y(t)|z ,...,z ,z ⊗···⊗z )
1 K 1 K . (6) 15
p(x k¯(t),y(t)|z k¯,z 1⊗···⊗z K)
First,wedevelopthenumeratorinEq.(6)usingthechainrule:
10
p(x(t),y(t)|z ,...,z ,z ⊗···⊗z )
1 K 1 K
= p(x k(t)|z k)p(y(t),x k¯(t)|x k(t),z k¯,z 1⊗···⊗z K)
5
= p(x k(t)|z k)p(y(t)|x(t))p(x k¯(t)|x k(t),z k¯)
≈ p(x (t)|z )p(y(t)|x(t)). (7)
k k
0
Weassumeindependenceofthelikelihoodp(y(t)|x(t))fromem-
Bass Drums Guitar Piano
beddings and approximate the last equality dropping the unknown
Generatedsources
termp(x k¯(t) | x(t),z k¯). WesubstituteEq. (7)inEq. (6),takethe
gradientofthelogarithmwithrespecttox (t)andmodelthelikeli-
hoodwithisotropicGaussians[20]dependk ingonavarianceγ2 : Fig. 2. FAD (lower is better) between generated sources and
xk Slakh100testdata(200chunks,∼12seach). NegPromptindicates
logp(x (t)|z )p(y(t)|x(t)) thepresenceofnegativeprompting.
∇ k k
xk(t)logp(x k¯(t),y(t)|z k¯,z 1⊗···⊗z K)
=∇ logp(x (t)|z )+∇ logp(y(t)|x(t))
xk(t) k k xk(t)
withx (t)(i ∈ I)sampledfromtheperturbationkernelinEq. (2)
i
=∇ xk(t)logp(x k(t)|z k)+∇
xk(t)logN(y(t)|(cid:88)K
x l(t),γ x2 kI)
c co annd gi eti no en re ad teo tn hex
ai
ca cn od mα p, aβ nim∈ eR nts mca ixli tn ug ref sac (cid:80)to jr ∈s. JU xsi jn dg irE eq c. tl( y1 .1),we
l=1
K
=∇ logp(x (t)|z )+ 1 (y(t)−(cid:88) x(t)). (8) 3.3. Sourceseparation
xk(t) k k γ2 l
xk
l=1 SourceseparationcanbeperformedbyadaptingEq. (4)tothetext-
Applying similar steps we obtain the score of the density on y(t) conditioned model. Let an observable mixture y(0) be composed
conditionedonx(t)(noticetheoppositelikelihoodgradient): bysourcesdescribedby{z } .Wecanseparatethesourcesby
k k∈[K]
choosingaconstrainedsource(w.l.o.g. theK-th)andsampling,for
p(y(t)|x(t),z ,...,z ,z ⊗···⊗z )
1 K 1 K k∈[K−1],with:
K K
(cid:79) 1 (cid:88)
≈ ∇ y(t)logp(y(t)| z l)+ γ2( x l(t)−y(t)). (9) K (cid:88)−1
y S (x (t),z ,σ(t))−S (y(0)− x(t),z ,σ(t)). (13)
l=1 l=1 θ k k θ l K
During inference, we sample from Eqs. (8) and (9) in parallel, l=1
replacingthegradientsofthelog-densitieswithscoremodels(Eq. We call this method GMSDI Separator. We also define a GMSDI
(5)): Extractor,whereweextractthek-thsourcex with:
k
  S Sθ θ( (x yk (t( )t ,) (cid:78),z k
K
l=,σ 1( zt l) ,) σ+ (t)γ )x21 +k(y
γ1
y2(t () (cid:80)−
K
l=(cid:80) 1xK l= l1 (tx )l −(t) y)
(t)).
(10)
S θ(x k(t),z k,σ(t))−S θ(y(0)−x k(t),(cid:79) l̸=kz l,σ(t)), (14)
(cid:80)
AdiagramofthemethodisillustratedinFigure1.Givenapartition constrainingthemixture l̸=kx l(t),complementarytox k(t).
{J } of[K]containingMsubsets(i.e.,∪ J =[K]),
m m∈[M] m∈[M] m
wecanperforminferencemoregenerallywith:
4. EXPERIMENTALSETUP
  SS θθ (( y(cid:80) (tj )∈ ,J (cid:78)m
K
lx =j 1( zt) l, ,(cid:78) σ(tj )∈ )J +m γz
1
y2j, (σ (cid:80)(t
K
l) =) 1+
x
lγ (tJ21 )m −(y y( (t t) ))−
.
(cid:80)K l=1x l(t)) T Mo ouˆv sa al ii -d lia kte
e
[o 8u ]r dt ih ffe uo sr ie ot nica ml odcl ea li sm
.
s, Thw ee firtr sa tin mot dw eo
l
it sim tre a- id no em da oin
n
(11) Slakh2100 [21]. Slakh2100 is a dataset used in source separation,
containing 2100 multi-source waveform music tracks obtained by
3.2. Partialgeneration synthesizingMIDItrackswithhigh-qualityvirtualinstruments. We
train the diffusion model on mixtures containing the stems Bass,
Wecangenerateaccompanimentsx J foragivensetofsourcesx I, Drums, Guitar, and Piano (the most abundant classes). To condi-
describedby{z i} i∈I,byselectingasetofaccompanimenttextem- tion the diffusion model, we use the t5-small pre-trained T5
beddings{z j} j∈J.WeintegrateEqs.(10)forj ∈J: text-onlyencoder[10], whichinputstheconcatenationofthestem
  S Sθ θ( (x yj (t( )t ,), (cid:78)zj
K
l( =t 1), zσ l,( σt) () t)+ )+γx1 2
j γ1
y2(cid:2)y (cid:2)(cid:0)( αt) (cid:80)− i(cid:0) ∈α I(cid:80) xii (∈ t)I +xi β(t (cid:80))+ l∈β J(cid:80) xll (∈ tJ )(cid:1)x −l( yt) ((cid:1) t(cid:3)
)(cid:3) (1,
2)
l t
t
wa a hb i een ae s
s
kls
o
lB yup a
r
sr cse us es pse a en n
i
rnt vd
s
ii sin D
d ee
dt rh u .ae m Tm ms h) ei i.x
x
wt tGu
u
inr rive
e
de o( ane wt. tg th sr. ia,
a
zt i“ enB w
i in
sa e gs 2s k 1t, n
i
8mD o awr
e
tu
,
2m t
s
2h us ke c” Hhlai zf b
a
(t e
n
∼h lse
a
1pdt 2r pe sa rs )c
o
.ck ari cbc ho inn ig s-
DAF
32.9
50.51
24.6
61.5
44.51
55.4
35.8 65.9
25.7
47.8
91.7
85.58
Table1.Gridsearchoverembeddingscalewon100chunks(∼12s
MSDM[14] each)ofSlakh2100testset. ResultsinSI-SDR (dB–higherisbet-
GMSDI i
ter).Thesourceinparenthesisistheconstrainedsource.
6 Model w=3.0 w=7.5 w=15.0 w=24.0
GMSDIExtractor 7.66 9.61 6.00 -0.62
GMSDISeparator(Bass) 8.10 6.72 -1.09 -20.60
GMSDISeparator(Drums) 9.44 8.69 -1.48 -21.62
4
GMSDISeparator(Guitar) 5.82 4.37 -2.27 -17.49
GMSDISeparator(Piano) 7.60 6.41 -2.68 -16.90
2
Table2.QuantitativeresultsforsourceseparationontheSlakh2100
testset.ResultsinSI-SDR (dB–higherisbetter).
i
Model Bass Drums Guitar Piano All
0 Demucs+Gibbs(512steps)[27] 17.16 19.61 17.82 16.32 17.73
Bass Drums Guitar Piano All WeaklySupervisedMSDM[14] 19.36 20.90 14.70 14.13 17.27
Generatedsources MSDM[14] 17.12 18.68 15.38 14.73 16.48
GMSDISeparator 9.76 15.57 9.13 9.57 11.01
GMSDIExtractor 11.00 10.55 9.52 10.13 10.30
Fig.3. FAD(lowerisbetter)resultsontotalandpartialgeneration,
Ensamble 11.00 15.57 9.52 10.13 11.56
withrespecttoSlakh2100testmixtures(200chunks,∼12seach).
tracks(conditional). Ontotalgeneration(All),wesetγ = ∞and
Thesecondmodelistrainedonamorerealisticdataset,namely y
reach∼1lowerFADpoint, using600samplingsteps. Onpartial
MTG-Jamendo [22]. MTG-Jamendo is a music tagging dataset
generation, we sample using 300 steps, setting γ ≪ ∞, to in-
containing over 55000 musical mixtures and 195 tag categories. y
formthegeneratedmixtureabouttheconditioningsources. Inthis
Wetrainourdiffusionmodelontheraw_30s/audio-lowver-
scenario, MSDM tends to generate silence. To enforce non-silent
sion of the dataset, using the first 98 shards for training and the
resultswithMSDM,wesample100examplesforeachconditioning
last2forvalidation. Themodelwindowisof219 samples(∼24s)
chunkandselectthesamplewiththehighestL norm.
at 22kHz. We condition the model with the pre-trained check- 2
point music_audioset_epoch_15_esc_90.14.pt1 of the Forsourceseparation,weemploytheSI-SDRimprovement(SI-
SDR)[25]asanevaluationmetricandfollowtheevaluationproto-
LAIONCLAPcontrastiveencoder[13].Attrainingtime,wecondi- i
tionthediffusionmodelwithembeddingsEcontr(y)obtainedfrom colof[14]. First,weperformagridsearch(Table1)tofindagood
ϕ embeddingscalew. FortheGMSDISeparator,wedonotuseneg-
the training mixtures y themselves, resulting in an unsupervised
model. At inference time, we use ADPM22 [23] with ρ = 1 for ativeprompting,whilefortheGMSDIExtractor,weonlyusenega-
generationandAEuler2withs =20forseparation. tivepromptsforBassandDrums.WeevaluateonthefullSlakh2100
churn testsetwithw = 3andconstrainedDrumsforGMSDISeparator
andw = 7.5forGMSDIExtractor,showcasingresultsinTable2.
5. EXPERIMENTALRESULTS Training onlywith mixtures (plusassociated labels), theensemble
ofthetwoseparatorsreaches11.56dB,beingzero-shot,i.e.,wedo
First,wewanttounderstandwhetherthemodeltrainedonSlakh2100 nottargetsourceseparationduringtraining[26].
mixturescanparameterizesinglesourceswell.Wesample,foreach We release qualitative examples for the Slakh2100 and MTG-
stem,200chunksof∼12s,conditioningwithembeddingsofsingle Jamendomodelsonourdemopage3.
stem labels (e.g., “Bass”). Then, we compute the Fre´chet Audio
Distance(FAD)[24]withVGGishembeddingsbetweensuchsam-
6. CONCLUSIONS
ples and 200 random Slakh2100 test chunks of the same source.
InFigure2, wecompareourmodelagainsttheweaklysupervised
We have proposed GMSDI, a compositional music generation
version of MSDM [14], where a model learns the score function
methodworkingwithanytime-domaintext-guideddiffusionmodel.
for each stem class (a setting requiring access to clean sources).
Themethodobtainsreasonablegenerationandseparationmetricson
We notice that single-stem prompting is insufficient for obtaining
Slakh2100, enablingunsupervisedcompositionalmusicgeneration
goodFADresults,especiallyforBassandDrums,causingsilenceto
forthefirsttime.Infuturework,wewanttoextendthetechniqueto
begenerated. Wefindnegativeprompts(Section2.1)essentialfor
latentdiffusionmodelsandnarrowthegapwithsupervisedmethods.
obtaining non-silent results using “Drums, Guitar, Piano” (Bass),
“Bass” (Drums), “Bass, Drums” (Guitar), “Bass, Drums” (Piano).
Inallsettingsabove,weuse150samplingsteps. 7. ACKNOWLEDGEMENTS
Following, we ask how well the model can perform coherent
synthesiswithGMSDI.InFigure3,wecomputetheFADbetween ThisworkissupportedbytheERCGrantno.802554(SPECGEO)
200randomSlakh2100testmixturechunks(∼12seach)andmix- and PRIN 2020 project no.2020TA3K9N (LEGO.AI). L.C. is sup-
turechunksobtainedbysummingthemodel’sgeneratedstems(un- ported by the IRIDE grant from DAIS, Ca’ Foscari University of
conditional) or the generated stems together with the conditioning Venice. E.B.issupportedbyaRAEng/LeverhulmeTrustResearch
Fellowship[grantno.LTRF2223-19-106].
1https://github.com/LAION-AI/CLAP
2https://github.com/crowsonkb/k-diffusion 3https://github.com/gladia-research-group/gmsdi
DAF
85.0
72.1 54.1
57.3
56.0
90.3
11.1
69.1
55.6
85.58. REFERENCES [14] Giorgio Mariani, Irene Tallini, Emilian Postolache, Michele
Mancusi,LucaCosmo,andEmanueleRodola`, “Multi-source
[1] AaronvandenOord,OriolVinyals,andKorayKavukcuoglu, diffusionmodelsforsimultaneousmusicgenerationandsepa-
“Neuraldiscreterepresentationlearning,” inAdvancesinNeu- ration,” arXivpreprintarXiv:2302.02257,2023.
ralInformationProcessingSystems,2017,vol.30.
[15] YuanchengWang, ZeqianJu, XuTan, LeiHe, ZhizhengWu,
[2] YangSongandStefanoErmon, “Generativemodelingbyesti- JiangBian,andShengZhao, “Audit:Audioeditingbyfollow-
matinggradientsofthedatadistribution,” Advancesinneural inginstructionswithlatentdiffusionmodels,” arXivpreprint
informationprocessingsystems,vol.32,2019. arXiv:2304.00830,2023.
[3] Jonathan Ho, Ajay Jain, and Pieter Abbeel, “Denoising dif- [16] Bing Han, Junyu Dai, Xuchen Song, Weituo Hao, Xinyan
fusionprobabilisticmodels,” Advancesinneuralinformation He, Dong Guo, Jitong Chen, Yuxuan Wang, and Yanmin
processingsystems,vol.33,pp.6840–6851,2020. Qian, “Instructme: An instruction guided music edit and
remixframeworkwithlatentdiffusionmodels,”arXivpreprint
[4] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab- arXiv:2308.14360,2023.
hishekKumar,StefanoErmon,andBenPoole, “Score-based
[17] Chris Donahue, Antoine Caillon, Adam Roberts, Ethan
generativemodelingthroughstochasticdifferentialequations,”
Manilow, et al., “Singsong: Generating musical accompani-
in International Conference on Learning Representations,
mentsfromsinging,” arXivpreprintarXiv:2301.12662,2023.
2020.
[18] JonathanHoandTimSalimans,“Classifier-freediffusionguid-
[5] Andrea Agostinelli, Timo I. Denk, Zala´n Borsos, Jesse
ance,”inNeurIPS2021WorkshoponDeepGenerativeModels
Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang,
andDownstreamApplications,2021.
Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al.,
“Musiclm: Generating music from text,” arXiv preprint [19] Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad
arXiv:2301.11325,2023. Levi, Pawan Sasanka Ammanamanchi, and Stella Biderman,
“Stay on topic with classifier-free guidance,” arXiv preprint
[6] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, arXiv:2306.17806,2023.
GabrielSynnaeve,YossiAdi,andAlexandreDe´fossez, “Sim-
[20] VivekJayaramandJohnThickstun, “Sourceseparationwith
ple and controllable music generation,” arXiv preprint
deep generative priors,” in International Conference on Ma-
arXiv:2306.05284,2023.
chineLearning.PMLR,2020,pp.4724–4735.
[7] HugoFloresGarcia, PremSeetharaman, RitheshKumar, and
[21] Ethan Manilow, Gordon Wichern, Prem Seetharaman, and
BryanPardo, “Vampnet: Musicgenerationviamaskedacous-
Jonathan Le Roux, “Cutting music source separation some
tictokenmodeling,” arXivpreprintarXiv:2307.04686,2023.
slakh: A dataset to study the impact of training data quality
[8] Flavio Schneider, Zhijing Jin, and Bernhard Scho¨lkopf, andquantity,” in2019IEEEWorkshoponApplicationsofSig-
“Mouˆsai: Text-to-music generation with long-context latent nalProcessingtoAudioandAcoustics(WASPAA).IEEE,2019,
diffusion,” arXivpreprintarXiv:2301.11757,2023. pp.45–49.
[9] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qi- [22] Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair
uqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, Porter,andXavierSerra, “Themtg-jamendodatasetforauto-
andMarkDPlumbley, “Audioldm2: Learningholisticaudio maticmusictagging,”inInternationalConferenceonMachine
generation with self-supervised pretraining,” arXiv preprint Learning,2019.
arXiv:2308.05734,2023. [23] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li,andJunZhu, “Dpm-solver: Afastodesolverfordiffusion
[10] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
probabilistic model sampling in around 10 steps,” Advances
SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPe-
inNeuralInformationProcessingSystems,vol.35,pp.5775–
terJLiu, “Exploringthelimitsoftransferlearningwithauni-
fiedtext-to-texttransformer,” TheJournalofMachineLearn- 5787,2022.
ingResearch,vol.21,no.1,pp.5485–5551,2020. [24] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and
Matthew Sharifi, “Fre´chet audio distance: A reference-free
[11] IlariaManco,EmmanouilBenetos,ElioQuinton,andGyo¨rgy
metricforevaluatingmusicenhancementalgorithms,” inIn-
Fazekas, “Learning music audio representations via weak
terspeech,2019.
language supervision,” in ICASSP 2022-2022 IEEE Interna-
tionalConferenceonAcoustics,SpeechandSignalProcessing [25] JonathanLeRoux,ScottWisdom,HakanErdogan,andJohnR.
(ICASSP).IEEE,2022,pp.456–460. Hershey, “Sdr–half-baked or well done?,” in ICASSP 2019-
2019IEEEInternationalConferenceonAcoustics,Speechand
[12] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail,
SignalProcessing(ICASSP).IEEE,2019,pp.626–630.
andHuamingWang, “Claplearningaudioconceptsfromnat-
urallanguagesupervision,” inICASSP2023-2023IEEEInter- [26] Jordi Pons, Xiaoyu Liu, Santiago Pascual, and Joan Serra`,
nationalConferenceonAcoustics,SpeechandSignalProcess- “Gass: Generalizingaudiosourceseparationwithlarge-scale
ing(ICASSP).IEEE,2023,pp.1–5. data,” arXivpreprintarXiv:2310.00140,2023.
[27] Ethan Manilow, Curtis Hawthorne, Cheng-Zhi Anna Huang,
[13] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor
Bryan Pardo, and Jesse Engel, “Improving source separa-
Berg-Kirkpatrick, and Shlomo Dubnov, “Large-scale con-
tionbyexplicitlymodelingdependenciesbetweensources,”in
trastive language-audio pretraining with feature fusion and
ICASSP2022-2022IEEEInternationalConferenceonAcous-
keyword-to-caption augmentation,” in ICASSP 2023-2023
tics,SpeechandSignalProcessing(ICASSP).IEEE,2022,pp.
IEEEInternationalConferenceonAcoustics,SpeechandSig-
291–295.
nalProcessing(ICASSP).IEEE,2023,pp.1–5.