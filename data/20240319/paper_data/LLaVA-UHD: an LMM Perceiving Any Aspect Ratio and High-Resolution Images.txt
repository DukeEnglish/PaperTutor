LLaVA-UHD: an LMM Perceiving Any Aspect Ratio
and High-Resolution Images
RuyiXu1 YuanYao2∗ ZonghaoGuo3 JunboCui1 ZanlinNi1 ChunjiangGe1
Tat-SengChua2 ZhiyuanLiu1 MaosongSun1 GaoHuang1∗
1TsinghuaUniversity 2NationalUniversityofSingapore
3UniversityofChineseAcademyofSciences
xrorrim@gmail.com yaoyuanthu@gmail.com gaohuang@tsinghua.edu.cn
https://github.com/thunlp/LLaVA-UHD
Abstract
Visualencodingconstitutesthebasisoflargemultimodalmodels(LMMs)inun-
derstandingthevisualworld. ConventionalLMMsprocessimagesinfixedsizes
andlimitedresolutions,whilerecentexplorationsinthisdirectionarelimitedin
adaptivity,efficiency,andevencorrectness. Inthiswork,wefirsttakeGPT-4Vand
LLaVA-1.5asrepresentativeexamplesandexposesystematicflawsrootedintheir
visualencodingstrategy. Toaddressthechallenges,wepresentLLaVA-UHD,a
largemultimodalmodelthatcanefficientlyperceiveimagesinanyaspectratioand
highresolution. LLaVA-UHDincludesthreekeycomponents: (1)Animagemodu-
larizationstrategythatdividesnative-resolutionimagesintosmallervariable-sized
slicesforefficientandextensibleencoding,(2)acompressionmodulethatfurther
condensesimagetokensfromvisualencoders,and(3)aspatialschematoorganize
slicetokensforLLMs. ComprehensiveexperimentsshowthatLLaVA-UHDout-
performsestablishedLMMstrainedwith2-3ordersofmagnitudemoredataon
9benchmarks. Notably,ourmodelbuiltonLLaVA-1.5 supports6times
336×336
larger(i.e.,672×1088)resolutionimagesusingonly94%inferencecomputation,
andachieves6.4accuracyimprovementonTextVQA.Moreover,themodelcanbe
efficientlytrainedinacademicsettings,within23hourson8A100GPUs(vs. 26
hoursofLLaVA-1.5).
1 Introduction
RecentprogressinLargeMultimodalModels(LMMs)[26,11,23,28,5]haswitnessedasignificant
surgeinvision-languageunderstanding,reasoning,andinteractioncapabilities. Thisisachievedby
projectingvisualsignalsintoLargeLanguageModels(LLMs)toenabletheirvisualperceptionofthe
world,wherevisualencodingstrategyplaysafundamentalrole[21,3,28]. Real-worldimagesare
knowntoresideinawiderangeofaspectratiosandresolutions,presentingsignificantchallengesfor
LMMsinvariousapplications.
However,mostexistingLMMs[8,11,28]perceiveimagesinafixedaspectratio(i.e.,1:1)andalow
resolution(i.e.,224×224). Thecompromisetothissimplifiedsettingtypicallyleadstosevereshape
distortionandblurofimagecontents. TheproblemsignificantlyhurtsthecapabilitiesofLMMs,
especiallyforfine-grainedcapabilities,suchassmallobjectunderstanding[20]andopticalcharacter
recognition[42,5,17]. Moreover,theissuealsoexacerbateshallucinationproblems(i.e.,producing
textualresponsesnotfactuallygroundedinimages),sincemodelscanonlylearntomakebestguesses
toblurredimages[37,44].
To achieve image perception in varied aspect ratios and high resolutions for LMMs, there are
two main challenges: (1) Adaptivity. Since visual encoders (e.g., CLIP-ViT [34]) are pretrained
∗CorrespondingAuthors
4202
raM
81
]VC.sc[
1v30711.3042:viXrain fixed resolutions, it can be difficult to deal with images in a wide range of aspect ratios and
resolutions. Simpleimageinterpolationthatdeviatesfarfromthepretrainingscenarioscanresult
inout-of-distributionissues. (2)Efficiency. Directlyencodinghigh-resolutionimagesusingvision
Transformers[13]requiresquadraticcomputationcostwithrespecttoimagesizes. Inaddition,it
canbeevenmorecostlyforLLMstoprocessthelargenumberofvisualtokensfromhigh-resolution
images(e.g.,4096tokensfor896×896imagesinViT-L/14).
Moreover,carelessvisualencodingstrategiescanevenresultinsystematicflawsincorrectness. For
example,despiteitspowerfulcapabilitiesinvariousaspects,ithasbeencommonlyreportedthat
GPT-4V [2] can surprisingly struggle in some basic capabilities, such as identifying the number
ofobjects[41]. Themechanisticcauseforsuchembarrassmentremainslargelyunknown. Inthis
work,weperformthefirstmechanisticinvestigationofGPT-4Vflawsfromtheperspectiveofvisual
encodingstrategy. OurcontrolledexperimentsinprobingGPT-4Vshowthattheproblemcanbe
partiallyrootedinitsvisualencodingstrategyindealingwithhigh-resolutionimages. Investigation
onLLaVA-1.5[27],arepresentativeopen-sourceLMMalsoshowssystematicissuesincorrectness,
indicatingtheirpotentialvulnerabilityforadversarialattacks.
To address the challenges, we present LLaVA-UHD, a large multimodal model that efficiently
perceivesanyaspectratioandhigh-resolutionimages. Themodelhasthreekeycomponents: (1)At
thecoreofLLaVA-UHDisanimagemodularizationstrategythatdividesnative-resolutionimages
intosmallervariable-sizedslicesforefficientandextensibleencoding. Incomparisontorecentworks
thatfitimagesintoseveralfixedaspectratiosandresolutions[24,23],thevariable-sizedslicesin
LLaVA-UHDenablefulladaptivitytonative-resolutionimageswithoutpaddingorshape-distorting
resizing. Thisisinanalogytothebetteradaptivityofusingwaterdropsvs. icecubesinfull-filling
variable-sizedglasses. Wealsoshowthatthestrategyguaranteesminordeviationfromthepretraining
settingofvisualencoderstomaximallyretaintheircapabilities. (2)Thevisualtokensarecondensed
byacompressionlayertomodestlengths,largelyreducingthecomputationforLLMs.(3)Finally,the
compressedslicetokensareorganizedinaspatialschematoinformLLMsabouttheslicepositions
intheimage.
Comprehensiveexperimentson9benchmarksshowthatLLaVA-UHDsignificantlyimprovesthe
capabilitiesofLMMs,outperformingestablishedcounterpartstrainedwith2-3ordersofmagnitude
moredata. Notably,ourmodelbuiltonLLaVA-1.5 supports672×1088resolutionimages
336×336
usingonly94%inferencecomputation,andachieves6.4accuracyimprovementonTextVQAand3.2
accuracyimprovementonPOPE.Theadvantageenlargeswithmoreextremeaspectratios. Wealso
showthatinstructiontuningonViTparametersissufficientforadaptationtoabroadrangeofimages.
Moreover,themodelcanbeefficientlytrainedinacademicsettings,within23hours(vs. 26hoursof
LLaVA-1.5)on8A100GPUs.
Thecontributionofthisworkcanbesummarizedasthreefold: (1)Weperformthefirstmechanistic
investigationofGPT-4Vfromtheperspectiveofvisualencodingstrategyandexposesystematic
flaws. (2) We present LLaVA-UHD, a large multimodal model that can efficiently perceive any
aspectratioandhigh-resolutionimages. (3)Weconductcomprehensiveexperimentstodemonstrate
theeffectivenessofLLaVA-UHDon9popularbenchmarks,andalsoprovideanalysisfordeeper
understandingofthemodel.
2 PilotExperiments
WestartwithapilotexperimentonthevisualencodingstrategiesofexistingLMMs,takingGPT-
4V[2]andLLaVA-1.5[27]asrepresentativeexamples. GPT-4Visapowerfulandmostrecognized
proprietary LMM, while LLaVA-1.5 is one of the most influential open-source LMMs. Despite
their strong performance in many aspects, it has been commonly reported that dilemmas can be
encounteredinsomebasiccapabilities[41]. Forexample,GPT-4Vispronetomiscountingtheobject
numbersinimages,whereasthecausesremainlargelyunknown.
Inthiswork,weperformthefirstmechanisticinvestigationofGPT-4Vflawsfromtheperspectiveof
visualencodingstrategy. Thekeyideaisthatbyusingsyntheticimagesascontinuousprobes,wecan
evaluatethebehaviorsofGPT-4Vinahighlycontrolledmanner,therebyidentifyingtheunderlying
causes. Ourexperimentalresultsindicatethat,somesystematicflawsofGPT-4Varelikelytobe
rootedinitsvisualencodingstrategy,whichcanbepotentiallyexploitedforadversarialattacks.
2(a) Input image (b) Average output (c) Correct answer (4) (d) 2N answer (8)
1N 2N 4N
1N 2N 4N
1N 2N 4N
(e) Output pattern (f) 4N answer (16) (g) Close error (5 ) (h) Close error ( 3)
Figure1: ExperimentalresultsofGPT-4Vinidentifyingnumbersofobjects. Notethatthedashed
linesin(a)areforillustrationpurposesonly,andnotpresentedtoGPT-4V.
2.1 GPT-4VExperiments
Preliminary. AccordingtothepubliclyavailableinformationfromOpenAI,2GPT-4Vemploystwo
imageprocessingmodes: lowresolutionandhighresolution. (1)Inlow-resolutionmode, foran
originalimagewithdimensionsWandH,themodelprocessesonlyalow-resolutionoverviewimage.
(2)Inhigh-resolutionmode,besidestheoverviewimage,GPT-4Vprocessesadditionalslicesofthe
originalhigh-resolutionimage,whereeachslicehas512×512resolution,resultingin⌈W ⌉×⌈ H ⌉
512 512
slicesintotal. InourexperimentsonGPT-4V’snewhigh-resolutionmode,interestingerrorpatterns
areobserved,promptinganexplorationintoGPT-4V’sunderlyingvisualencodinglogic.
HowdopositionsinimagesinfluenceGPT-4V’sbehavior? Ourexperimentsstartwithasimple
instance: GiventheimageasshowninFig.1(a),weaskGPT-4V:“Howmanycirclesarethereinthe
image?” Wesynthesizeaseriesofimagevariantsbychangingthepositionsofcirclesintheimage,
andkeepthetextpromptunchanged. Forbetterreliability,wealsosynthesizeimagesusingother
colorsandshapesaswell,in{red,green,white}×{circle,triangle,square}. Foreachinstance,we
query15timestobetterapproximatethetrueresponsedistribution.
WecalculatetheaveragenumberansweredbyGPT-4Vforeachpositionintheimage,andreport
theheatmapinFig.1(b). Wecanobservethattheresultishighlycorrelatedwithobjectpositionsin
images. Specifically,thepatternsaresplitby256×256squares,andthreeinterestingpatternscanbe
identified: (1)Thecentralsquareexhibitsthehighestresponsenumber,(2)themiddleedgesshowa
lowernumber,and(3)thecornersaretheclosesttogroundtruth.
Toinvestigatethecause,wefurtherseparatethemodelresponsesbynumber,andreportthedistri-
butionacrosspositionsforeachresponseinFig.1(c),(d),(f),(g)and(h). Interestingly,besidesthe
correctanswers(4: 66.1%)andcloseanswers(5: 16.6%,3: 10.2%),itturnsoutthattheremaining
twoabnormalanswers(8: 5.2%,16: 1.9%),whichdoublesandquadruplesthegroundtruth,account
fortheerrorpatterninFig.1(b). CombiningtheresultswiththepublicinformationfromOpenAI,
wehypothesizethemostlikelycauseisthat,thereareoverlapsintheslicesofGPT-4Vwhenthe
imageresolutionisnotdivisibleby512.3 AsillustratedinFig.1(e),theoverlappingareasbetween
twosliceswilldoublethenumber,andtheoverlappingareasbetweenfoursliceswillquadruplethe
number.4
2https://platform.openai.com/docs/guides/vision
3NotethattheissueisdifferentfromtheoverlappingslidingwindowsinCNNs,sincetheoverlapsinGPT-4V
isinconsistentacrossdifferentresolutionimages.
4Note that besides visual encoding strategies, model behaviors are also influenced by the accumulated
trainingdynamicsandRLHF.Thereforethedouble/quadrupleeffectdoesnotdominatetheresults.Allresults
arefromGPT-4Von03-05-2024.
3256px 256p x 256px Phase 1 Phase 2 Phase 3
Pixel
(a) Input image (b) Answer distribution
Figure2: ResultsonprobingGPT-4Vviacontinuouslychangingimageresolutions.
Attack success rate 1:9 1:2 2:3 3:2 2:1 9:1
Left Right Top Bottom
<1:1 87.1% 100% 100% 100%
>1:1 99.8% 99.9% 99.9% 96.7%
Figure3: ExperimentalresultsofadversariallyattackingLLaVA-1.5usinginputimagescontaining
paddingpixels. Left: AttacksuccessrateswhereLLaVA-1.5ignoresthegreyareaandanswersthe
colorofthecentralrectangle(e.g.,green). Right: Synthesizedinputimagescontaining(1)arectangle
invariedaspectratios,and(2)paddingpixels.
HowdoimageresolutionsinfluenceGPT-4V’sbehavior? Toverifythehypothesis,wefurther
probeGPT-4Vthroughcontinuouslychangingimageresolutions. Specifically,weproportionally
resizetheimageinFig.2(a)intodifferentresolutions,andqueryabouttheobjectnumberinthesame
way. Foreachresolution,werepeatedlyquery30timesforbetterreliability.
WereporttheexperimentalresultsinFig.2(b).Weobservethatthemodelresponsesshowasignificant
phasechangewithimageresolutions: (1)Inphase1,sincetherearenoimageslices,mostanswers
arecorrect;(2)Inphase2,answer12dominatestheresponsespossiblyduetotheincompletecircles
ineachslice. (3)Phase3showsmixedanswersof9,12and16. Notethat16canbewellexplained
bytheerrorpatterninFig.1(e). WereferreaderstoSectionAforamoredetailedillustrationofeach
phase. Besides,wealsonoticethatmanyabnormalphenomenonsinFig.2(b)cannotbeperfectly
explainedyet,whichweleaveforfuturework.
Inconclusion,theseexperimentalfindingsshedlightonGPT-4V’spotentialvulnerabilitiesinhigh-
resolutionimageprocessing,warrantingfurtherinvestigationintotheimplicationsoftheseweaknesses
andthedevelopmentofstrategiestocounterpotentialadversarialattacksonLMMs.
2.2 LLaVA-1.5Experiments
Todealwithimageswithvariedaspectratios,LLaVA-1.5padstheinputimagesintosquaresbefore
feedingthemintothevisualencoder. Thisencodingmethodresultsinawasteofcomputationfor
non-squareimages. Forexample,a1:4imagehasonly25%effectivecomputationafterpaddinginto
squares. Toquantifytheinfluence,wetrainanunpaddedversionofLLaVA-1.5,byfittingtheViT
positionembeddingintotheaspectratioofinputimagesusing2Dinterpolation. Theresultantimage
tokensremainnomorethan576asinLLaVA-1.5(seeSection3.1). Fromtheexperimentalresultsin
Table2,weobservethatadaptiveaspectratioencodingwithoutpaddingconsistentlyimprovesthe
performanceofLLaVA-1.5.
Anotherissueofpaddingisthat,themodelessentiallycannotknowwhetherthepadding-likepixels
comefromimagepre-processingoranactualpartoftheoriginalinputimage. Todemonstratethis
issue,wesynthesizeaseriesofinputimagesasinFig.3(right),whereblue/green/redrectanglesin
variousaspectratiosaresurroundedbygrey(i.e.,thecolorofLLaVA-1.5’spaddingRGBvalue).
Giventheinputimage,weprompt: “Whatisthecoloroftheleft/right/top/bottommostarea?” From
4
768
px
noitoporPA man is
Input Image
Large Language Model (Vicuna-13B)
Standard Slice Area
, , \n , , \n A man
Ideal Slice NumberN = ⌈6.5⌉ =7 Shared Compression Layer
6 7 8
Factorization & Partition Shared Vision Transformer (CLIP-ViT-L/14)
A B C D E F 0
1×6 2×3 3×2 6×1 1×7
7×1 1×8 2×4 4×2 8×1
2D Positional Embedding Interpolation
Best Partition
A B C Positional
Score Function S(·)
D E F Embedding
2×3
Figure4: TheLLaVA-UHDframework. Left: Givenahigh-resolutionimage,LLaVA-UHDfirst
calculates the ideal slice nu形m变bepra,ratintiodntshen selects the best partition from possible factorization,
splittingthehigh-resolutionimageintovaried-sizedslices. Right: Slicesareencodedinnativeaspect
ratiosby2Dinterpolationonpositionembeddings,andthencompressedandarrangedinaspatial
schemaforLLMprocessing.
theresultsinFig.3(left),weobservethatLLaVA-1.5neglectsthegreyinputareas(consideringthem
aspadding),andfaithfullyrespondswiththecolorofthecentralrectangle.
2.3 ConclusionsonPilotExperiments
Insummary,bothpowerfulproprietaryLMMssuchasGPT-4Vandopen-sourceLLaVA-1.5have
systematicissuesintheirunderlyingvisualencodingstrategies.Theresultsshowthatvisualstrategies
must be designed with caution. Common practices such as padding, shape-distorting resizing,
and repetitive slicing can result in a waste of computation, a loss of model capability, and even
vulnerabilitytoadversarialattacks. Therefore,thereisanurgentneedformoreadaptiveandefficient
visualencodingmethods.
3 Method
Based on the principles learned from the pilot experiments, we propose LLaVA-UHD, a large
multimodal model that can efficiently perceive any aspect ratio and high-resolution images. As
showninFig.4,themodelincludesthreekeycomponents: (1)Animagemodularizationstrategy
thatdividesnative-resolutionimagesintosmallervariable-sizedslicesforefficientandextensible
encoding,(2)acompressionmodulethatfurthercondensesimagetokensfromvisualencoders,and
(3)aspatialdecorationschematoorganizeslicetokensforLLMs.
3.1 ModularizedVisualEncoding
To deal with high-resolution images with varied aspect ratios, a naive approach is to interpolate
thepositionembeddingsofViTtothetargetshapefordirectencodingasawhole. However,this
approach is sub-optimal due to the quadratic computation cost and the performance degradation
fromout-of-distributionissues. Toaddressthechallenge,wepresentamodularizedvisualencoding
strategy. Thebasicideaistodividenative-resolutionimagesintosmallervariable-sizedsliceslices,
wheretheshapeofeachslicedoesnotdeviatetoofarfromthestandardpretrainingsettingofViT.
Withvariable-sizedsliceslices,LLaVA-UHDcanachievefulladaptivitytonative-resolutionimages
withoutpaddingorshape-distortingreshaping.
5High-ResolutionImagePartitionStrategy. Thegoalofimageslicingstrategyistodetermineasplit
ofhigh-resolutionimages,withminimalchangestotheresolutionsofeachslice. Givenanimagein
resolution(W ,H )andaViTpretrainedinresolution(W ,H ),wefirstdeterminethenumberof
I I v v
slices(i.e.,theidealcomputation)neededtoprocesstheimage: N =⌈WI×HI⌉. Thenwefactorize
theslicenumberN intomcolumnsandnrows: C ={(m,n)|m×nWv =×H Nv ,m∈N,n∈N}. To
N
selectthemostappropriatepartition,wedefineascorefunctiontomeasurethedeviationfromthe
standardpretrainingsettingofViT:
(cid:12) (cid:12)
S(W I,H I,W v,H v,m,n)=−(cid:12) (cid:12) (cid:12)log HW II ×× mn −logW
H
vv(cid:12) (cid:12) (cid:12), (1)
wherehigherscoreS(·)indicatesasmallerdeviationfromthestandardsettingofViT,andisthus
preferred. Thereforethepartitioncanbeobtainedasfollows:
m∗,n∗ =argmaxS(W ,H ,W ,H ,m,n), (2)
I I v v
(m,n)∈C¯
wherethecandidatesetC¯ = C N. Inpractice,wenoticethatinsomecases,theremightbeonlya
fewpossiblefactorizationschemesforN,especiallyforprimenumbers,whichcanleadtolimited
choices and therefore extreme partitions of images. For example, N = 7 has only two extreme
partitionchoices,1:7and7:1. Toaddresstheissue,inadditiontotheidealslicenumberN,wealso
allowamodestchangeofslicenumbersN−1,N+1toincorporatemoreplausiblepartitionchoices.
Therefore,thefinalpartitionisgivenbyEquation2,whereC¯ =C ∪C ∪C .
N−1 N N+1
Theoretically,weshowthatthepartitionstrategyguaranteesminorexpectedchangesandmodest
worst-casechangeswithrespecttostandardpretrainingresolution(W ,H )foreachslice. Specifi-
v v
cally,weshowthatforinputimageswhereN ≤20andaspectratioin[1:6,6:1],theaspectratioof
eachsliceresideswithin[1:2,2:1],andtheareaofeachsliceresideswithin[0.33W H ,1.5W H ].
I I I I
WereferreaderstoSectionBforfullproofdetails.
ArbitraryAspectRatioSliceEncoding. MostexistingLMMsutilizeastaticresolutionforimage
sliceencoding[5,27,11]. Thisessentiallypreventsfulladaptivitytonativeresolutions,sinceonly
severalpredefinedfixed-shapeslicesareavailable. Moreover,thestaticsliceresolutioninevitably
incurs padding or shape-distorting resizing, which hurts the performance, efficiency, and even
correctnessasdiscussedinSection2.
Toaddresstheproblem,weproposetoencodeimageslicesinaspectratiosgivenbythepartition
strategyasis. Specifically,weproportionallyresizetheoriginalimagefollowingtheaspectratio,
suchthatthenumberofpatchesmaximallyfitswithinthepretrainingbudgetM (i.e.,thenumber
ofpositionembeddingsinViT).Thenwereshapethepretrained1Dpositionembeddingsequence
ofViTinto2DformatP ∈Rq×q×l followingitspretrainingsetting,whereM =q×q,andlisthe
dimensionofpositionembeddings. Afterthat,we2D-interpolateP tofitthesliceresolutiongiven
bythepartitionstrategyforvisualencoding. Inourexperiments, weshowthatViTandposition
embeddingparameterscanbekeptfrozenduringpretraining,andupdatingtheseparametersduring
theinstruction-tuningstageissufficientforgoodperformance. Inadditiontoslices,wealsoprovidea
low-resolutionoverviewimageinnativeaspectratio. Theoverviewimagecanprovidecoarse-grained
informationandglobalsemanticconnectionsinimages.
3.2 CompressionLayer
High-resolutionimagesrequireLLMstoprocesssignificantlymorevisualtokens,whichaccountsfor
amajorpartofthecomputation. Forexample,a672×1008resolutionimagewillproduce3,456
visual tokens for LLaVA-1.5 [27]. To address the issue, we compress the visual tokens of each
imagesliceusingasharedperceiverresamplerlayer[3]. Specifically,imagetokensoutputbythe
visualencodersareresampledtoalowernumberusingasetofqueryvectorsviacross-attention
(from576to64inourexperiments). ComparedwiththeprevalentMLP-basedvisualprojection
approaches [27, 26, 39], perceiver resampler maintains a fixed and affordable number of visual
tokensregardlessofimageresolutions,andisthereforemorecompatiblewithhigh-resolutionimage
understanding. Asaresult,LLaVA-UHDcanencode672×1008resolutionimagesusinganeven
lowercomputationcostthanLLaVA-1.5inencoding336×336resolutionimages.
63.3 SpatialSchemaforImageSlices
Sincetheimagepartitionisdynamicacrossdifferentimages,itisnecessarytoinformLLMofthe
spatial organizations of image slices. Inspired by [6], we design a spatial schema to inform the
relativepositionsofimageslicesusingtwospecialtokens. Specifically,weuse“,”toseparatethe
slicerepresentationsinarow,anduse“\n”toseparatedifferentrows. Inourexperiments,wefind
thatthesimpleschemacaneffectivelyinformthedynamicpartitiontoyieldgoodperformance.
4 Experiments
Inthissection,weempiricallyinvestigatetheeffectivenessofLLaVA-UHD.Wefirstprovidethe
implementationdetails,andreporttheevaluationresultson9commonbenchmarkscomparedwith
strongbaselines. Thenweprovideanalyticresultsforbetterunderstandingofthemodel.
4.1 ImplementationDetails
ModelConfiguration. Inthiswork,webuiltLLaVA-UHDfollowingtheimplementationofLLaVA-
1.5 [27]. Specially, we use the CLIP-ViT-L/14 as visual encoder (default resolution 336×336),
Vicuna-13B[9]asLLM,andasharedvisualresampler[5]astheprojectortoconnectthevisual
encoder and LLM. During the encoding of image slices, a minor reshape within half patches
(maximum 7-8 pixels) could be performed to fit the slice into patches. The number of learnable
queriesinresamplerissetto64. FortheimagepartitionedasN sub-patches,thenumberofvisual
tokensfedintoLLMis64×(N +1),withtokensofthelow-resolutionoverviewimage. Wesetthe
maximumN tobe6inexperiments,whichsupportsamaximumof672×1008resolutionimages.
FollowingLLaVA-1.5,weperformatwo-stagetrainingasfollows.
Stage 1: Pretraining details. During this stage, only the perceiver resampler is tuned, with the
CC-595Kdataset[28]for1epoch,usingAdamWoptimizerwithalearningrateof1e−3 andthe
cosinelearningrateschedule. Theglobalbatchsizeissetto256. Thetrainingcostofthisstageis∼5
hoursusing8×A100GPUs.
Stage2: Instruction-tuningdetails. Duringthisstage,thevisualencoderisfrozenandwefine-tune
thevisualresamplerandLLM,witha656Kmixturedataset[27]whichcontainsLLaVA-Instruct[28],
TextVQA[36],GQA[18],OCR-VQA[32],andVisualGenome[19]. Thelearningrateis2e−5and
batchsizeis128. Othersettingsarethesameasstage1. Thetrainingcostofthisstageis∼18hours
using8×A100GPUs.
4.2 ExperimentalSetting
Weintroduceexperimentalsettings,includingthebenchmarks,evaluationmetrics,andbaselines.
Benchmarks. We adopt 9 popular benchmarks to evaluate our model, including: (1) General
visual question answering benchmarks such as VQA-V2 [4], GQA [18], ScienceQA [30], and
VizWiz[15];(2)OpticalcharacterbasedvisualquestionansweringbenchmarksuchasTextVQA[36];
(3)HallucinationbenchmarksuchasPOPE[22];(4)ComprehensivebenchmarkssuchasMME[14],
MMBench[29],andMMBench-CN[29].
Evaluation Metrics. In addition to the performance on popular benchmarks, we also report the
computation cost (TFLOPs) in processing an image in the maximum supported resolution. The
computation cost is aggregated from the visual encoder, projector, and LLM. We also report the
accumulatedmultimodaltrainingdatavolumeforreference,whichincludesimage-textpairsused
duringpertainingandinstructiontuning. Formodelspost-trainedonexistingmultimodalmodelsas
backbones,thisalsoincludesthetrainingdataofthebackbones.
Baselines. Wecompareourmodelwithstrongbaselines. (1)Generalbaselines. WeadoptQwen-
VL[5], LLaVA-1.5[27], MiniGPT-v2[7], Shikra[8], BLIP-2[21]andInstructBLIP[11]asrep-
resentative general baselines. Since the implementation of LLaVA-UHD is highly aligned with
LLaVA-1.5, itservesasthemostdirectbaseline. (2)High-resolutionLMMs. SPHINX[24]and
mPLUG-Owl2[43]encodeimagesinfixedresolutions;Ureader[42]andMonkey[23]supportenu-
meratedresolutiontypes(severalpredefinedfixed-shapeslices);Fuyu-8B[6]andOtterHD-8B[20]
canencodeimagesinanyresolutions.
7Table1:Mainresultson9popularbenchmarks.#Data:accumulatedmultimodaltrainingdatavolume,
MaxRes.: maximum resolution supported, TFLOPs: computation cost of processing maximum
resolutionimages,AR.: aspectratiosupported,∆: improvementsoverLLaVA-1.5backbone.
Model #Data MaxRes. AR. TFLOPs VQAv2 GQA VQAT POPE SQA VizWiz MME MMB MMBCN
BLIP-2[21] 129M 224×224 Fix 1.0 41.0 41.0 42.5 85.3 61.0 19.6 1293.8 - -
InstructBLIP[11] 130M 224×224 Fix 1.0 - 49.5 50.7 78.9 63.1 33.4 1212.8 - -
Shikra [8] 6M 224×224 Fix 8.0 77.4 - - - - - - 58.8 -
Qwen-VL[5] 1.4B 448×448 Fix 9.2 78.8 59.3 63.8 - 67.1 35.2 - 38.2 7.4
SPHINX[24] 1.0B 448×448 Fix 39.7 78.1 62.6 51.6 80.7 69.3 39.9 1476.1 66.9 56.2
SPHINX-2k[24] 1.0B 762×762 Fix 69.4 80.7 63.1 61.2 87.2 70.6 44.9 1470.7 65.9 57.9
MiniGPT-v2[7] 326M 448×448 Fix 4.3 - 60.1 - - - 53.6 - - -
Fuyu-8B[6] - 1024×1024 Any 21.3 74.2 - - 74.1 - - 728.6 10.7 -
OtterHD-8B[20] - 1024×1024 Any 21.3 - - - 86.0 - - 1223.4 58.3 -
mPLUG-Owl2[43] 401M 448×448 Fix 1.7 79.4 56.1 58.2 86.2 68.7 54.5 1450.2 64.5 -
UReader[42] 86M 896×1120 Enum 26.0 - - 57.6 - - - - - -
Monkey[23] 1.0B 896×1344 Enum 65.3 80.3 60.7 - 67.6 69.4 61.2 - - -
LLaVA-1.5[27] 1.2M 336×336 Fix 15.5 80.0 63.3 61.3 85.9 71.6 53.6 1531.3 67.7 63.6
LLaVA-UHD(ours) 1.2M 672×1008 Any 14.6 81.7 65.2 67.7 89.1 72.0 56.1 1535.0 68.0 64.8
∆ - ×6times - -0.9 +1.7 +1.9 +6.4 +3.2 +0.4 +2.5 +3.7 +0.3 +1.2
4.3 MainResults
WereportthemainexperimentalresultsinTable1,fromwhichwehavethefollowingobservations:
(1)LLaVA-UHDoutperformsstrongbaselinesonpopularbenchmarks. Thisincludesstronggeneral
baselinestrainedon2-3ordersofmagnitudemoredatasuchasQwen-VLandInstructBLIP,andalso
high-resolutionLMMsthatrequiresignificantlymorecomputationsuchasFuyu-8B,OtterHD-8B,
MonkeyandSPHINX-2k.TheresultsshowthatLLaVA-UHDcanproperlydealwithnative-resolution
imagesforstrongperformance,aswellasgooddataandcomputationefficiency. (2)LLaVA-UHD
achievessignificantimprovementsovertheLLaVA-1.5backbone. Notably,bysimplyperceiving
imagesinnativehigh-resolution,LLaVA-UHDachieves6.4accuracyimprovementonTextVQA
and3.2accuracyimprovementonPOPE.Thereasonisthattheblurredcontentinlow-resolution
images can prevent LMMs from accurately identifying the challenging fine-grained objects and
opticalcharacters. Theresultsdemonstratethefundamentalroleofperceivingnativehigh-resolution
imagesinvariousmultimodaltasks,andtheeffectivenessofLLaVA-UHDinaddressingtheproblem.
(3) In terms of resolution and efficiency, compared with LLaVA-1.5 associated fixed 336×336
resolution,LLaVA-UHDsupports672×1088resolutionimagesinanyaspectratiousingonly94%
inference computation. The results indicate promising scalability of LLaVA-UHD to potentially
largerresolutionsinfuture.
4.4 AnalyticResults
We provide further analytic results, including ablation on alternative components, evaluation on
imageswithmoreextremeaspectratios,bestpracticeforfrozen/trainableparameters,andcasestudy.
AblationStudy. InTable2,weconductablationstudiesonalternativecomponents. (1)Wereplace
thepaddingstrategyofLLaVA-1.5withtheadaptiveencodingstrategyofLLaVA-UHD,supporting
arbitraryaspectratioswhilemaintainingidenticalmaximumresolutions. Wecanobserveconsistent
improvement since wasted computation from padding is avoided. (2) We replace the perceiver
resamplerofLLaVA-UHDwiththe2-layerMLPofLLaVA-1.5. Weobservethatperceiverresampler
achievescomparableorbetterperformancethanMLP,usingonly12.9%computationcost. (3)We
furtherreplacetheLLaVA-UHDimagepartitionstrategywiththenaivepartitionstrategy[24](i.e.,
fixed 2×2 slices). Resultsshowthat LLaVA-UHD canmore properlydivideimagesinto slices
Table2: AblationResults. FP:Fixedimagepartitionstrategy.
Model #TFLOPs VQAv2 GQA VQAT POPE SQA VizWiz
LLaVA-1.5 15.50 80.0 63.3 61.3 85.9 71.6 53.6
w/adaptiveenc. 15.50 80.1 64.1 61.8 86.7 72.0 54.2
LLaVA-UHD 14.63 81.7 65.2 67.7 89.1 72.0 56.1
w/MLP 113.65 81.6 65.4 66.9 89.2 71.5 56.3
w/MLP&FP.[24] 80.10 81.2 64.3 66.1 89.1 71.1 54.3
w/ospatialschema 80.07 79.3 63.9 65.4 88.3 70.3 53.1
8Table 3: Experimental results on extreme aspect ratio images. Absolute performance and the
degradationcomparedwiththestandardbenchmarksinTable2arereported.
Model #TFLOPs VQAv2 GQA VQAT POPE SQA VizWiz
LLaVA-1.5 15.50 74.6(-5.4) 57.9(-5.4) 58.4(-3.9) 81.1(-4.8) 66.3(-5.3) 50.1(-3.5)
w/adaptiveenc. 15.50 74.9(-5.2) 62.5(-1.6) 60.7(-1.1) 82.3(-4.4) 66.9(-5.1) 50.9(-3.3)
LLaVA-UHD 14.63 81.4(-0.3) 61.8(-3.4) 64.5(-3.2) 85.1(-4.0) 71.5(-0.5) 54.0(-2.1)
w/MLP 113.65 81.3(-0.3) 62.0(-3.4) 63.9(-3.0) 85.2(-4.0) 70.9(-0.6) 54.3(-2.0)
w/MLP&FP.[24] 80.10 79.6(-1.6) 61.9(-2.4) 58.5(-7.6) 84.4(-4.7) 69.4(-1.7) 52.2(-2.1)
Table4: Theeffectoftuningvisualencoderatdifferenttrainingstages.
UpdateViT
VQAv2 GQA VQAT POPE SQA VizWiz
pre-training Fine-tuning
✓ 81.7 65.2 67.7 89.1 72.0 56.1
✓ 78.2 61.1 58.9 83.9 68.6 51.4
79.4 64.5 65.7 87.3 71.9 55.4
✓ ✓ 80.2 63.7 62.6 87.2 71.6 55.1
LLaVA-1.5[27] 80.0 63.3 61.3 85.9 71.6 53.6
forbetterperformance. (4)WeremovethespatialschemafromLLaVA-UHD.Theperformance
degradationdemonstratestheeffectivenessandnecessityofspatialschemaininformingthedynamic
slicepositionsforLMMs.
LLaVA-UHD generalizes to images with extreme aspect ratios. We further investigate the
generalizationcapabilityofLLaVA-UHDbyconstructinganextendedversionofexistingbenchmarks.
Specifically,weexpandtheaspectratioofanimagebydoublingthelengthofitslongersidethrough
padding. FromtheresultsinTable3,wecanseethattheadvantageofLLaVA-UHDincreasesas
comparedwithLLaVA-1.5andalternatives. ThereasonisthatLLaVA-UHDperceivesimagesin
nativeaspectratios. Incomparison,LMMsthatencodeimagesinfixedaspectratioswillsufferfrom
significantdistortioninthecontentshapes. Moreover,thisalsocausesthecomputationtobeunevenly
distributedalongthewidthandheightoftheimagecontent.
Instruction-tuningViTparametersissufficientforadaptation. Weinvestigatetheeffectoftuning
ViTparametersatdifferenttrainingstages,includingpretrainingandinstruction-tuning. Fromthe
resultsinTable4,weobservethat: (1)UpdatingViTduringinstruction-tuningissufficienttoachieve
goodperformance. Infact,wefindthatLLaVA-UHDcanimproveoverLLaVA-1.5evenwhenViT
parametersarefrozeninbothpretrainingandinstructiontuning. (2)FurtherupdatingViTduring
pretrainingdoesnotleadtobetterresults. WehypothesizethereasonisthatjointlytrainingViTand
resampler(fromscratch)onlimitedpretrainingdatacanleadtoinstabilityissues.
CaseStudy. ToprovideamoreintuitiveunderstandingofthecapabilitiesofLMMsindealingwith
high-resolutionimages,weprovidequalitativeresultsforLLaVA-UHDandLLaVA-1.5inFig.5. We
canseethatLLaVA-UHDcancorrectlyidentifythedensecontentinthetimetable(Case1),thetext
onthesmallposter(Case2),andiconsandtextonthephone(Case3)forfine-grainedrecognitionand
reasoning. Incomparison,LLaVA-1.5canonlyperceivecoarse-grainedinformation,andtherefore
tendstoprovideeitheruninformative(Cases1and2)orincorrect/hallucinatedanswers(Case3)in
thesechallengingscenarios. TheresultsdemonstratetheeffectivenessandadvantageofLLaVA-UHD
inperceivingnativeaspectratioandhigh-resolutionimagesforfine-grainedmultimodalcapabilities.
5 RelatedWork
VisualEncodinginLMMs. TheadventofChatGPT[1]andGPT-4[2]hasspurredthedevelopment
ofnumerousopen-sourcelargelanguagemodels(LLMs)[9,38,10]. UtilizinganLLMasalanguage
encoderanddecoder,therespringsupplentyofLMMs[21,11,3,28,5,17],withaimatunderstanding
visualimage. Therefore, howtoencodevisionfeaturesintoLLMsbecomesthecoreproblemin
thecommunity. Fortunately,CLIP[34]proposestorespectivelyextractlanguageembeddingsusing
languagemodelslikeBERT[12]andvisualfeaturesusingvisionmodelslikeViT[13]andCNN[16],
9LLaVA-UHD (Ours)
The image shows a sign with the opening times for
the restaurant. It appears to be open from
11:30AM to 1:30 PM and from 4:00 PM to 7:00
PM from Monday to Saturday. The restaurant is
closed on Sundays.
LLaVA-1.5
The image shows a sign with the opening times for
the restaurant. It appears to be open from 12:00 to
What is the opening time of 2:00 PM and from 5:00 PM to 7:00 PM.
this restaurant ?
LLaVA-UHD (Ours)
The poster in the image displays the text "SAVER
MENU." This suggests that the restaurant is
advertising a menu with lower-priced items, which is a
common marketing strategy for fast-food restaurants to
attract customers looking for affordable options.
1
LLaVA-1.5
The poster in the image shows a menu with various
food items, but the text is too small to read clearly. It
appears to be a promotional poster for the McDonald's
restaurant, likely displaying their menu offerings or
What is written on the poster ?
special deals.
LLaVA-UHD (Ours)
The weather report on your phone indicates that it is 19
degrees Celsius outside. It also shows that there are
changes in temperature expected throughout the day.
The weather appears to be partly cloudy, suggesting a
mix of sun and clouds.
LLaVA-1.5
The weather report on the phone screen indicates that
How is the weather today the temperature is 19 degrees Celsius. However, the
according to this weather image does not provide enough information to
report on my phone ? determine the specific weather conditions. In this case,
only the temperature is visible.
Figure5: QualitativecomparisonofLLaVA-UHDandLLaVA-1.5infine-grainedrecognitionand
reasoningcapabilities.
andalignthemincontrastivelearningfashionusingconsiderableimage-textpairs[35],sothatvisual
embeddingsarewellalignedtowardsthelanguage.
Existing visual projection approaches towards LLMs can be divided into three categories. (1)
Flamingo [3] proposes perceiver resampler, which utilizes a fixed number of queries to capture
visualfeaturesbycross-attentionoperationandfeedsthemintoLLMsforimage/videounderstanding.
(2)BLIP-2[21]pretrainsQ-FormertobridgetheimageencoderandLLMs. (3)LLaVA[28]just
leveragesanMLPmoduletoconnectlanguageandvisionfeaturespace. Beyondthem,SPHINX[24]
mixesmanykindsofvisualfeatures,includingDINO-V2[33],CLIP-ViT[34]andCLIP-CNN[34],
and Q-Former to augment visual representation. Vary [40] pretrains a visual model tailored for
document/chartrecognitionandunderstanding,andintegratesitwithvisualfeaturesofLLaVA[28]
forfurtherfeatureenhancement.
However,sincetheseLMMsrelyonCLIP-ViTthatrequiresfixedresolutionimageasinput,ithinders
LMMsfromhandlingimageswithhigherresolutionoranyaspectratio,andunderminesfine-grained
downstreamtaskslikeopticalcharacterrecognitionorsmallobjectunderstanding.
High-resolutionLMMs. Toperceiveimageswithhigherresolutions,recentworkcanbedivided
intofourcategories. (1)Up-Resize. Qwen-VL[5]interpolatesthepositionalembeddingofViTfrom
224×224to448×448andadditionallyexecutesatrainingstagetofine-tunetheViT.CogAgent[17]
and LLaVA-HR [31] marries a large low-resolution encoder with a small high-resolution image.
MiniGPT-v2[7]onlyresizesthepositionalembeddingswithoutfine-tuningthevisualencoderduring
10instruction tuning. These methods dramatically change the original visual position encoding of
CLIP-ViT[34],whichcancausesub-optimalvisualrepresentation. (2)Fix+Crop. Toaddressthe
aboveissue,SPHINX[24]utilizesafixedwindowsize(224×224)tocropapaddedimage(448×448)
into four slices, and concatenates them with a down-sampled 224×224 image as visual inputs.
Monkey[23]followsthisideayetincreasestheaccessibleimagesizeto896×1344,andconverts
eachsliceusingasharedresampler. (3)Fix+Enumerated-Crop. UReader[42],LLaVA-1.6[26]and
infiMM-HD[25]enumerateasimilaraspectratiotoresize,ratherthanusingafixedsquareratio
(e.g.,2×2asinSPHINX[24]). Theunavoidableimageresizingandpaddingoperationmightcause
imagedeformationandwasteofcomputation,respectively. (4)Any. Fuyu-8B[6]andOtter-HD[20]
directlyutilizeLLMstoencodevisualfeaturesinsteadofvisiontransformers. Theyjustsplitimages
intopatchesandprojectthemusinglinearlayersbeforefeedingintotheLLM.Regardingimage
patchesasasequenceenablesitselftoprocessimageswithcontinuousresolution. However, the
removalofanimageencodermeansinsufficientvisualrepresentation,whichmakesthesemethods
limitedinunsatisfactoryperformance.
Incomparison,LLaVA-UHDsupportsimagesinanyaspectratiosandhighresolutions.Byintegrating
theadvantagesofmodularizedandadaptiveimageencoding,aswellasperceiverresampler,LLaVA-
UHDcanachievestrongperformancewithimprovedcomputationefficiency.
6 Conclusion
Inthiswork,wepresentLLaVA-UHD,alargemultimodalmodelthatefficientlyperceivesanyaspect
ratioandhigh-resolutionimages. Comprehensiveexperimentalresultson9popularbenchmarks
demonstratetheeffectivenessofLLaVA-UHD,especiallyinfine-grainedmultimodalcapabilities.
Analyticalevaluationresultsareprovidedfordeeperunderstandingofthemodel. Inthiswork,we
limittheresolutionofLLaVA-UHDtomaximum672×1008. Infuture,consideringthepromising
efficiency and scalability, we will explore higher-resolution images and more challenging tasks
suchassmallobjectdetectionandsegmentation. Besides,imageslicesarecurrentlyindependently
encoded,withinteractionsonlyinLLMs. Weplantoestablishefficientconnectionsbetweenimage
slicesviaimprovedvisualencodingstrategiesforfine-grainedglobalinformationinteraction.
References
[1] IntroducingChatGPT.https://openai.com/blog/chatgpt. 2022.
[2] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. GPT-4technicalreport. arXiv
preprintarXiv:2303.08774,2023.
[3] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,RomanRing,ElizaRutherford,SerkanCabi,
TengdaHan,ZhitaoGong,SinaSamangooei,MarianneMonteiro,JacobL.Menick,SebastianBorgeaud,
AndyBrock,AidaNematzadeh,SahandSharifzadeh,MikolajBinkowski,RicardoBarreira,OriolVinyals,
AndrewZisserman,andKarénSimonyan. Flamingo:avisuallanguagemodelforfew-shotlearning. In
NeurIPS,2022.
[4] StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,CLawrenceZitnick,
andDeviParikh. VQA:Visualquestionanswering. InIEEEICCV,pages2425–2433,2015.
[5] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou. Qwen-VL:Afrontierlargevision-languagemodelwithversatileabilities. arXivpreprint
arXiv:2308.12966,2023.
[6] RohanBavishi,ErichElsen,CurtisHawthorne,MaxwellNye,AugustusOdena,ArushiSomani,,and
SagnakTasırlar. Introducingourmultimodalmodels. adept.ai/blog/fuyu-8b. 2023.
[7] JunChen,DeyaoZhu,XiaoqianShen,XiangLi,ZechunLiu,PengchuanZhang,RaghuramanKrish-
namoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny. MiniGPT-v2:largelanguagemodel
asaunifiedinterfaceforvision-languagemulti-tasklearning. arXivpreprintarXiv:2310.09478,2023.
[8] KeqinChen,ZhaoZhang,WeiliZeng,RichongZhang,FengZhu,andRuiZhao. Shikra: Unleashing
multimodalllm’sreferentialdialoguemagic. arXivpreprintarXiv:2306.15195,2023.
11[9] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephEGonzalez,etal. Vicuna:Anopen-sourcechatbotimpressingGPT-4
with90%*ChatGPTquality. 2023.
[10] HyungWonChung, LeHou, ShayneLongpre, BarretZoph, YiTay, WilliamFedus, EricLi, Xuezhi
Wang,MostafaDehghani,SiddharthaBrahma,AlbertWebson,ShixiangShaneGu,ZhuyunDai,Mirac
Suzgun,XinyunChen,AakankshaChowdhery,SharanNarang,GauravMishra,AdamsYu,VincentY.
Zhao,YanpingHuang,AndrewM.Dai,HongkunYu,SlavPetrov,EdH.Chi,JeffDean,JacobDevlin,
AdamRoberts,DennyZhou,QuocV.Le,andJasonWei. Scalinginstruction-finetunedlanguagemodels.
arXivpreprintarXiv:2210.11416,2022.
[11] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleFung,andStevenHoi. InstructBLIP:Towardsgeneral-purposevision-languagemodelswith
instructiontuning. arXivpreprintarXiv:2305.06500,2023.
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectionaltransformersforlanguageunderstanding.InJillBurstein,ChristyDoran,andThamarSolorio,
editors,NAACL,pages4171–4186,2019.
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.Animageisworth
16x16words:Transformersforimagerecognitionatscale. ICLR,2021.
[14] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,XiawuZheng,
KeLi,XingSun,etal. MME:Acomprehensiveevaluationbenchmarkformultimodallargelanguage
models. arXivpreprintarXiv:2306.13394,2023.
[15] DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,andJeffreyP
Bigham. VizWizgrandchallenge:Answeringvisualquestionsfromblindpeople. InIEEECVPR,pages
3608–3617,2018.
[16] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition.
InIEEECVPR,pages770–778,2016.
[17] WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,ZihanWang,
YuxiaoDong, MingDing, etal. CogAgent: Avisuallanguagemodelforguiagents. arXivpreprint
arXiv:2312.08914,2023.
[18] DrewAHudsonandChristopherDManning. GQA:Anewdatasetforreal-worldvisualreasoningand
compositionalquestionanswering. InIEEECVPR,pages6700–6709,2019.
[19] RanjayKrishna,YukeZhu,OliverGroth,JustinJohnson,KenjiHata,JoshuaKravitz,StephanieChen,
YannisKalantidis,Li-JiaLi,DavidAShamma,etal. VisualGenome:Connectinglanguageandvision
usingcrowdsourceddenseimageannotations. IJCV,123:32–73,2017.
[20] BoLi, PeiyuanZhang, JingkangYang, YuanhanZhang, FanyiPu, andZiweiLiu. OtterHD:Ahigh-
resolutionmulti-modalitymodel. arXivpreprintarXiv:2311.04219,2023.
[21] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. BLIP-2: Bootstrappinglanguage-imagepre-
trainingwithfrozenimageencodersandlargelanguagemodels. ICML,2023.
[22] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluatingobject
hallucinationinlargevision-languagemodels. arXivpreprintarXiv:2305.10355,2023.
[23] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,JingxuYang,YaboSun,YuliangLiu,andXiang
Bai. Monkey:Imageresolutionandtextlabelareimportantthingsforlargemulti-modalmodels. arXiv
preprintarXiv:2311.06607,2023.
[24] ZiyiLin,ChrisLiu,RenruiZhang,PengGao,LongtianQiu,HanXiao,HanQiu,ChenLin,WenqiShao,
KeqinChen,JiamingHan,SiyuanHuang,YichiZhang,XumingHe,HongshengLi,andYuQiao.SPHINX:
thejointmixingofweights,tasks,andvisualembeddingsformulti-modallargelanguagemodels. arXiv
preprintarXiv:2311.07575,2023.
[25] HaogengLiu,QuanzengYou,XiaotianHan,YiqiWang,BohanZhai,YongfeiLiu,YunzheTao,Huaibo
Huang,RanHe,andHongxiaYang. InfiMM-HD:Aleapforwardinhigh-resolutionmultimodalunder-
standing. arXivpreprintarXiv:2403.01487,2024.
12[26] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
LLaVA-NeXT:Improvedreasoning,ocr,andworldknowledge.https://llava-vl.github.io/blog/
2024-01-30-llava-next/. 2024.
[27] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. arXivpreprintarXiv:2310.03744,2023.
[28] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. NeurIPS,36,2024.
[29] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,Jiaqi
Wang,ConghuiHe,ZiweiLiu,etal. MMBench:Isyourmulti-modalmodelanall-aroundplayer? arXiv
preprintarXiv:2307.06281,2023.
[30] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,Peter
Clark,andAshwinKalyan.Learntoexplain:Multimodalreasoningviathoughtchainsforsciencequestion
answering. AdvancesinNeuralInformationProcessingSystems,35:2507–2521,2022.
[31] GenLuo,YiyiZhou,YuxinZhang,XiawuZheng,XiaoshuaiSun,andRongrongJi. Feastyoureyes:
Mixture-of-resolutionadaptationformultimodallargelanguagemodels. arXivpreprintarXiv:2403.03003,
2024.
[32] AnandMishra,ShashankShekhar,AjeetKumarSingh,andAnirbanChakraborty. OCR-VQA:Visual
questionansweringbyreadingtextinimages. InIEEEICDAR,pages947–952,2019.
[33] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,Pierre
Fernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. DINOv2:Learningrobustvisual
featureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023.
[34] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InICML,pages8748–8763,2021.
[35] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. LAION-5B:Anopenlarge-scale
datasetfortrainingnextgenerationimage-textmodels. NeurIPS,pages25278–25294,2022.
[36] AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,DeviParikh,and
MarcusRohrbach. TowardsVQAmodelsthatcanread. InIEEECVPR,pages8317–8326,2019.
[37] ZhiqingSun,ShengShen,ShengcaoCao,HaotianLiu,ChunyuanLi,YikangShen,ChuangGan,Liang-
YanGui,Yu-XiongWang,YimingYang,etal.Aligninglargemultimodalmodelswithfactuallyaugmented
RLHF. arXivpreprintarXiv:2309.14525,2023.
[38] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. LLaMA:Openandefficientfoundation
languagemodels. arXivpreprintarXiv:2302.13971,2023.
[39] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,ZhuoyiYang,Lei
Zhao,SongXiXuan,etal. CogVLM:Visualexpertforlargelanguagemodels. 2023.
[40] HaoranWei,LingyuKong,JinyueChen,LiangZhao,ZhengGe,JinrongYang,JianjianSun,Chunrui
Han,andXiangyuZhang. Vary:Scalingupthevisionvocabularyforlargevision-languagemodels. arXiv
preprintarXiv:2312.06109,2023.
[41] ZhengyuanYang,LinjieLi,KevinLin,JianfengWang,Chung-ChingLin,ZichengLiu,andLijuanWang.
ThedawnofLMMs:Preliminaryexplorationswithgpt-4v(ision). arXivpreprintarXiv:2309.17421,9(1):
1,2023.
[42] JiaboYe, AnwenHu, HaiyangXu, QinghaoYe, MingYan, GuohaiXu, ChenliangLi, JunfengTian,
QiQian,JiZhang,etal. UReader: UniversalOCR-freevisually-situatedlanguageunderstandingwith
multimodallargelanguagemodel. arXivpreprintarXiv:2310.05126,2023.
[43] QinghaoYe,HaiyangXu,JiaboYe,MingYan,HaoweiLiu,QiQian,JiZhang,FeiHuang,andJingren
Zhou. mplug-owl2:Revolutionizingmulti-modallargelanguagemodelwithmodalitycollaboration. arXiv
preprintarXiv:2311.04257,2023.
[44] TianyuYu,YuanYao,HaoyeZhang,TaiwenHe,YifengHan,GanquCui,JinyiHu,ZhiyuanLiu,Hai-
TaoZheng,MaosongSun,etal. RLHF-V:TowardstrustworthyMLLMsviabehavioralignmentfrom
fine-grainedcorrectionalhumanfeedback. InCVPR,2024.
13256px 256p x 256px Phase 1 Phase 2 Phase 3
Pixel
(a) Input image (b) Answer distribution
Figure6: ResultsonprobingGPT-4Vviacontinuouslychangingimageresolutions.
512px 512px 512px
256px 600px 768px
Phase 1 (1 slice) Phase 2 (4 slices) Phase 3 (4 slices)
Figure7: IllustrationonGPT-4Vphases(hypothesized). Redsquareindicatesaslice.
A DetailedIllustrationonGPT-4VPhases
FromthepilotexperimentalresultsinFig.6,weobservethattheGPT-4Vresponsesshowasignificant
phasechangewithimageresolutions. Hereweprovidedetailedillustrationsofthehypothesized
causefromtheperspectiveofvisualencoding:
(1)Inphase1,sincethereisonlyoneimageslice,mostanswersarecorrect. Morespecifically,when
dealingwithinputimagesunder512resolution,iftheimagesareresizedto512,thebehaviorwill
bethesamewithinphase1. However,sincethebehaviorchangessignificantlywithinphase1,we
suspectthattheinputimagesaremostlikelytobepaddedinto512resolutions,asshowninFig.7(a).
(2)Inphase2,answer12dominatestheresponsespossiblyduetotheincompletecirclesineachslice,
asshowninFig.7(b).
(3)Phase3showsmixedanswersof9,12and16. Amongtheseresponses,answer16canbewell
explainedbytheslicestrategyinFig.7(c). Besides,wealsonoticethatmanyabnormalphenomenons
inFig.2(b)cannotbeperfectlyexplainedyet,whichweleaveforfuturework.
B Proofs
Inthissection,weprovideproofsfortheimagepartitionstrategy. Weshowthatthesliceresolution
exhibitsmodestchangestotheoriginalresolutionofViT.
RangeofSliceAspectRatios. Theaspectratiooftheslicecanberepresentedby:
W W H
v = I : I,
H m n
v
whereW ,H arethewidthandheightoftheslice,W ,H arethesizesoftheoriginalimage,and(m,
v v I I
n)isthebestpartition. Restrictingtheaspectratior = Wv ∈[1,2]isequivalentto|log(r)|≤|log2|,
Hv 2
14
768
px
noitoporP(cid:12) (cid:16) (cid:17) (cid:12)
whichisalsoequivalentto(cid:12)log WI −log(n)(cid:12)≤|log(2)|. Weneedtoprove:
(cid:12) HI m (cid:12)
W 1
∀ I ∈[ ,6],N ≤20
H 6
I
(cid:12) (cid:18) (cid:19) (cid:12)
∃(m,n)∈C¯,(cid:12) (cid:12)log W I −log(n )(cid:12) (cid:12)≤|log(2)|,
(cid:12) H m (cid:12)
I
whichisequivalentto
∀N ≤20,(n ,m )∈C¯
i i
(cid:12)(cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)(cid:12)
∃(n j,m j)∈C¯,(cid:12) (cid:12)
(cid:12)
log mn i −log mn j (cid:12) (cid:12) (cid:12)≤2·|log(2)|,
i j
whichcanbeverifiedbyenumeratingallpossiblefactorizationsofC¯ = C ∪C ∪C for
N−1 N N+1
N ≤20. Theresultsshowthattheaspectratioofeachsliceresideswithin[1,2].
2
Expected Aspect Ratio. We assume that the ratio of the original image is greater than 1 (i.e.,
H > W ). The situation is the same for H < W . Assuming that the sizes of the images are
I I I I
uniformlydistributedforN ∈[0,20],whiletheaspectratiooftheoriginalimages WI ∈[1,6],we
HI
haveP(W ,W ,n,m)= 1 · 1. Theexpectedaspectratiocanbeobtainedby:
I H 20 5
m×W (cid:90)(cid:90) m×W
E( n×HI)=
WI ∈[1,6]
( n×HI)·P(W I,H I,n,m)dW IdH I,
I HI I
W ·H ∈[0,20s]
I I
n,m=argmaxS(·)
where s is the area of a standard resolution of ViT. After calculation, we obtain E(r) = 1.258,
Var(r) = 0.048. Theresultsshowthattheexpectedaspectratiooftheslicesis1:1.258,whichis
closetothestandardpertainingsettingofViT.Morecommonlyassumingthatimagesareuniformly
distributed between [1,3], and the aspect ratio is uniformly distributed between [1,2], we have
E(r)=1.147,Var(r)=0.011,indicatingevensmallerchanges.
RangeofSliceArea. Letn= WI × HI,whichleadstoN =⌈n⌉. Weconsiderdividingtheimage
into{N −1,N,N +1}slicesW . Tv hereH fv ore,themaximumvalueofeachsliceS = n (when
max N−1
N ̸= 2),andS = n (whenN = 2). TheminimumvalueS = n . Asnapproaches3−,
max N min N+1
where N = 3, S achieves the maximum value of 1.5. Similarly, as n approaches 1+, where
max
N =2,S achievestheminimumvalueof0.33.
min
ExpectedSliceArea. Stillassumingthatthesizesoftheimagesareuniformlydistributedwithin
N ∈ [0,20], whiletheaspectratiooftheimages WI ∈ [1,6]. Theexpectedareaofslicecanbe
HI 6
obtainedby:
W ×H (cid:90)(cid:90) W ×H
E( I I)= ( I I)·P(W ,H ,n,m)dW dH .
n×m WI ∈[1,6] n×m I I I I
HI
W ·H ∈[0,20s]
I I
n,m=argmaxS(·)
Aftercalculation,weobtainE(WI×HI)=1.057,Var(WI×HI)=0.016. Thisshowsthatourslice
n×m n×m
areasarerelativelyconcentrated,similartotheoriginalresolutionofViT.
C Discussions
Weprovidediscussionsonlimitationsandpotentialnegativeimpactofthiswork.
Limitations and Future Work. (1) Higher resolutions. In this work, we limit the resolution of
LLaVA-UHDtomaximum672×1008. AlthoughthisresolutionincreasesthestandardLLaVA-1.5
resolutionby6times,higher-resolutionimagessuchas4Kimagesandremotesensingimagesare
stilloutofreach. Infuture, consideringthepromisingefficiencyandscalability, wewillexplore
higher-resolutionimagesandmorechallengingtaskssuchassmallobjectdetectionandsegmentation.
(2)Jointsliceencoding.Currentlyimageslicesarecurrentlyindependentlyencoded,withinteractions
15onlyinLLMs. Weplantoestablishefficientconnectionsbetweenimageslicesviaimprovedvisual
encodingstrategiesforfine-grainedglobalinformationinteraction.
PotentialNegativeImpact. Inthiswork,weinvestigatethefailurepatternandtheunderlyingcause
forGPT-4VandLLaVA-1.5. Themechanismcanbepotentiallyusedforadversarialattacksonthese
models. ItisworthnotingthatthegoalofthisworkistoraiseattentiontothevulnerabilityofLMMs
andprovideadeeperunderstandingoftheimportanceofvisualencodingstrategies. Thisworkcalls
forfurthereffortstomitigatetherevealedissuestoensuretherobustnessandsafetyofLMMs.
16