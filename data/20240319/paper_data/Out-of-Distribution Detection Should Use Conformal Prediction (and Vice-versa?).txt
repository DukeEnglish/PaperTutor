Out-of-Distribution Detection Should Use Conformal Prediction
(and Vice-versa?)
PaulNovello*12 JosebaDalmau*12 LeoAndeol34
Abstract criticalMachineLearningapplications. Moregenerally,we
areinterestedinOut-of-Distribution(OOD)detection,i.e.
Research on Out-Of-Distribution (OOD) detec-
in identifying when an example comes from a particular
tionfocusesmainlyonbuildingscoresthateffi-
datadistributionforwhich,inmostpracticalcases,weonly
ciently distinguish OOD data from In Distribu-
haveaccessthroughadatasetofexamplesdrawnfromit.
tion (ID) data. On the other hand, Conformal
Prediction (CP) uses non-conformity scores to CurrentOODdetectionstrategiesrelyonconstructingan
construct prediction sets with probabilistic cov- OODscores ,afunctionthatassignsascalartoeachinput
ood
erage guarantees. In this work, we propose to example. Thisscorediscriminatesbetweenin-distribution
use CP to better assess the efficiency of OOD (ID)dataandOODdatabyassigninglowerscorestothe
scores. Specifically, we emphasize that in stan- formerwhileassigninghigherscorestothelatter.
dardOODbenchmarksettings,evaluationmetrics
WhenOODdetectionisusedinamachinelearningpipeline
canbeoverlyoptimisticduetothefinitesample
toidentifyexamplesthatdifferfromthedatathemodelhas
size of the test dataset. Based on the work of
beentrainedon,thereisanaturalqualitativeinterpretation
(Batesetal.,2022),wedefinenewconformalAU-
ofOODdetectionintermsofuncertainty. Forinstance,an
ROCandconformalFRP@TPR95metrics,which
example with a low OOD score should be one for which
arecorrectionsthatprovideprobabilisticconser-
the model can predict with low uncertainty, while an ex-
vativenessguaranteesonthevariabilityofthese
amplewithahighOODscoreshouldbelinkedtoahighly
metrics.Weshowtheeffectofthesecorrections
uncertainprediction.
on two reference OOD and anomaly detection
benchmarks,OpenOOD(Yangetal.,2022)and ConformalPrediction(CP)isafamilyofpost-hocmethods
ADBench(Hanetal.,2022). Wealsoshowthat forUncertaintyQuantificationthatworkaswrappersover
thebenefitsofusingOODtogetherwithCPapply machine learning models, transforming point predictions
the other way around by using OOD scores as intopredictionsetswithrigorousprobabilisticguarantees
non-conformityscores,whichresultsinimprov- based on so-called nonconformity scores. The user pre-
ing upon current CP methods. One of the key specifiesarisklevelα,andtheconstructedpredictionsetis
messagesofthesecontributionsisthatsinceOOD guaranteedtocontainthegroundtruthvaluewithaprobabil-
isconcernedwithdesigningscoresandCPwith ityofatleast1−α.SinceCPisawayofprovidingrigorous
interpreting these scores, the two fields may be uncertaintyquantificationguaranteesbuiltuponscores,itis
inherentlyintertwined. naturaltoapplyittothescoresusedinOODdetection. The
mainpurposeofourworkistodigintotheConformal
PredictioninterpretationofOODdetectionscoresand
1.Introduction showsomeofitsadvantages.
Machine Learning and Deep Learning models are being Tothatend,wefirstfollowtheworkof(Batesetal.,2022)
increasinglydeployedinreal-worldapplications,wherethey onoutlierdetectionandapplytheirideastoOODdetection.
arelikelytobeconfrontedwithdatathatisdifferentfrom (Batesetal.,2022),casttheOODdetectionproblemintothe
thedatatheyweretrainedorvalidatedon. Wewouldlike statisticalframeworkofhypothesistesting. Theyshowthat
to be able to identify this situation, especially for safety- thep-values,builtwithacalibrationdataset,areprovably
marginallyvalidbutdependonthechoiceofthecalibration
*Equalcontribution 1IRTSaintExupery,France2Artificialand
dataset,andsodoallthemetricsderivedfromthesep-values:
Natural Intelligence Toulouse Institute, France 3SNCF, France
FPR,AUROC... Oneofthemaincontributionsofourwork
4MathematicalInstituteofToulouse,France.Correspondenceto:
is to explore the consequences of this effect for the task
PaulNovello<paul.novello@irt-saintexupery.com>.
ofOODdetectionandtoproposeanalternativeconformal
1
4202
raM
81
]LM.tats[
1v23511.3042:viXraConformalOOD
AUROCandconformalFPRmetrics. outperformstheclassicalnonconformityscore.
Therelevanceofthenewmetricsweproposeisbestappre- • WepointoutthatOODandCParetwodomainsthat
ciatedinthecontextofsafety-criticalapplications,orinan havemuchtocontributetoeachother, andadvocate
eventualcertificationprocessofanOODdetectioncompo- forfurtherresearchexploringthislink.
nent. ThetrueAUROCorFPRmetricsareinaccessiblefor
agivenOODscore, andwecanonlyprovideanapproxi-
2.Background
mation obtained from a finite dataset. However, this can
introducefluctuationsinourapproximation,thusoveresti- 2.1.Out-of-DistributionDetection
matingorunderestimatingthetruemetrics.Inacertification
Givennexamples,{x ,...,x }sampledfromaprobability
process,wearemainlyinterestedinguaranteeingthatour 1 n
distributionP onaspaceX,andanewdatapointx ,
estimationsareconservativewithhighprobability,attheex- id n+1
thetaskofOut-of-Distribution(OOD)detectionconsistsin
penseoflosingsomeapproximationprecision. Conformal
assessingifx wassampledfromP -inwhichcaseit
AUROCandConformalFPRdoexactlythat. Weshowthe n+1 id
isconsideredIn-Distribution(ID)-ornot-thusconsidered
effectofthesenewmetricsontworeferencebenchmarks,
OOD.
theOODbenchmarkOpenOOD(Yangetal.,2022),andthe
anomalydetectionbenchmark(Hanetal.,2022). ThemostcommonprocedureforOODdetectionistocon-
structascores :X →Randathresholdτ suchthat:
Second,weshowthatnotonlycanCPcontributetoOOD ood
detection,butresearchinOODdetectioncanhelpCPtoo. (cid:40)
x isdeclaredOODif s (x )>τ
Indeed, CP has traditionally focused on constructing pre- n+1 ood n+1 (1)
diction sets from nonconformity scores. Still, the scores x n+1 isdeclaredIDif s ood(x n+1)≤τ
usedareusuallysimplefunctionsofthesoftmaxscoresfor
classificationtasksorclassicaldistancesinEuclideanspace Wecalls oodanOODscoreoranon-conformityscore.
for regression tasks. Here, we draw inspiration from the
OODdetectionliteraturetobuildmoreinvolvednonconfor- Task-basedOOD Thisisthemostcommonapproachin
mityscoresandcomparetheirperformancetothetraditional theliteraturewhenitcomestoOODdetectionforneural
nonconformityscoresofCP.Forthetaskofclassification, networks. ItalsoencompassesOpen-SetRecognition. Let’s
webuildpredictionsetsbasedonmultipledifferentOOD considerthatx i canbeassignedalabely i sothatwecan
scoresandfindthatascorebasedonsomeofthem,notably construct a dataset {(x 1,y 1),...,(x n,y n)} defining some
Mahalanobis(Leysetal.,2018)orKNN(Sunetal.,2022) superviseddeeplearningtask. Inthatcase,P id :=P train.
aregoodcandidatesasnonconformityscores. Task-based OOD uses representations built by the neural
network f throughout its training to design s . Many
ood
Ultimately,oneofthekeymessagesofthesecontributions
sophisticated methods follow this approach (Yang et al.,
isthatsinceOODisconcernedwithdesigningscoresand
2021). Asimpleexampleistotakethenegativemaximum
conformal prediction with interpreting these scores, the
of the output softmax of f (Hendrycks & Gimpel, 2018)
two fields may be inherently intertwined. Highlighting (cid:0) (cid:1)
asanOODscore(s (x )=−max f(x ) where
ood n+1 n+1
thisrelationshipmightoffersignificantpotentialforcross-
max(x)isthehighestcomponentofthevectorx. Another
fertilization.
simpleideaistofindthedistancetothenearestneighborin
someintermediatelayeroff (Sunetal.,2022).
SummaryofContributions
• WecasttheOODdetectionproblemintotheframework Task-agnostic OOD This approach encompasses One-
of statistical hypothesis testing and apply the ideas ClassClassificationandAnomaly/OutlierDetection. Let’s
of(Batesetal.,2022)tocorrectOODscores.
consideradataset{x 1,...,x n}inafullyunsupervisedway.
Thereisnonotionoflabels,sowehavetoapproximateP
id
• We propose new conformal AUROC and conformal somehoworsomerelatedquantitiesfromscratch.Examples
FPR metrics, which are provably conservative with areGANsorVAEswiths definedasreconstructionerror.
ood
highprobability. See(Yangetal.,2021)forathoroughreview.
• WeshowtheeffectofconformalAUROCandconfor- The validation procedure is the same for both ap-
malFPRinthereferencebechmarksOpenOOD(Yang proaches. We consider p additional ID samples
etal.,2022)andADBench(Hanetal.,2022). {x n+1,...,x n+p} sampled from P id (typically, the
testsetofthecorrespondingdataset),andpOODsamples
• WebuildnewnonconformityscoresforCPbasedon {x¯ ,...,x¯ } from another distribution P ̸= P
1 p ood id
OODscoresandperformanexperimentalcomparison (typically, another dataset). We apply s to obtain
ood
betweenthescores.WefindthattheMahalanobisscore {s (x¯ ),...,s (x¯ ),s (x ),...,s (x )}.
ood 1 ood p ood n+1 ood n+p
2ConformalOOD
Then, we assess the discriminative power of s by 3.RelatedWorks
ood
evaluating metrics depending on the threshold τ. By
3.1.StatisticsFrameworksforOOD
consideringIDsamplesasnegativeandOODaspositive,
wecancompute: In this work, we study the potential of using Conformal
PredictionasastatisticalframeworkforinterpretingOOD
scores. ThisideaofcastingOODinastatisticalframework
• TheAreaUndertheReceiverOperatingCharacteristic
hasalreadybeenattemptedindifferentsettings.
(AUROC):wecomputetheFalsePositiveRate(FPR)
andtheTruePositiveRate(TPR)forτ =s (x ),
i ood n+i
Selective Inference and Testing Selective Inference
i ∈ {1,...,p},andcomputetheareaunderthecurve
worksontopofaMLpredictorbyusinganadditionaldeci-
withFPRasx-axisandTPRasy-axis.
sionfunctioninordertodecideforeachexamplewhether
the original model’s prediction should be considered. A
score equivalent to an OOD score is used to define this
• FPR@TPR95: The value of the False Positive Rate
decision function. Several approaches exist, for instance,
(FPR) when τ is selected so that the True Positive
throughbuildingastatisticaltest(Haroushetal.,2022)or
Rate (TPR) is 0.95, i.e. the FPR with τ such that
(cid:80) bytraininganeuralnetworkwithanappropriateloss(Geif-
1/p 1 = 0.95. It can be generalized to
i sood(x¯i)>τ man & El-Yaniv, 2017; 2019). However, the framework
FPR@TPRβ,foranyβ ∈(0,1).
ofConformalPredictionappearsbettersuitedtoourgoal
sinceitappliestoscoresinapost-processingmanner,does
notrequireassumptionsormodificationsonthemodel,and
2.2.ConformalPrediction
benefitsfromdynamicdevelopmentintheMLcommunity.
FewMachineLearningandDeepLearningmodelsprovide
a notion of uncertainty related to their predictions. Even
ConformalOODandAD ConformalPredictionhasbeen
themodelstrainedforclassificationtasksprovidingsoftmax
previouslyappliedtoOut-of-DistributionandAnomalyDe-
outputs, which can be interpreted as the probabilities for
tection. For instance, (Liang et al., 2022) have proposed
theinputbelongingtothedifferentclasses,areusuallyill-
amethodbasedonCPforOODwithlabeledoutliers,and
calibrated and overconfident, making the softmax output
(Kauretal.,2022)proposetouseconformalp-values. CP
anincorrectproxyofthetrueuncertaintyoftheprediction.
isoneofseveralframeworksthatallowobtainingstatistical
(Pearce et al., 2021). Conformal Prediction (CP) (Vovk
guaranteesforOODdetection. Oneofthefirstmethodsfor
et al., 2005; Angelopoulos & Bates, 2022) is a series of
AnomalyDetectionwasintroducedby(Vovketal.,2003).
post-processinguncertaintyquantificationtechniquesthat
Sincethen,severalothermethodshavebeenproposedby
aremodel-agnosticandprovidefinite-sampleguaranteeson
(Laxhammar&Falkman,2011;Laxhammar,2014;Balasub-
themodelpredictions. ThesimplestCPtechnique,thesplit
ramanianetal.,2014),aswellasmorerecently(Angelopou-
CP,worksasawrapperonatrainedmodelf. Itrequires
los & Bates, 2022; Guan & Tibshirani, 2022), where the
acalibrationdataset{(x ,y ),...,(x ,y )}
n+1 n+1 n+ncal n+ncal lengthsofthepredictionsetsasOODscores. Theseworks
independentofthetrainingdata,andarisk(orerrorrate)
all use the standard CP setting, in which basic marginal
α that the user is willing to tolerate. Based on so-called
guaranteesareobtained. Wegofurtheronthisapproachby
nonconformityscorescomputedonthecalibrationdataset,it
usingCPasaprobabilistictooltorefinetheinterpretation
buildsapredictionsetC (x )foranewtestsample
α n+ncal+1 and,hence,theusefulnessofanyOODscore.
x withthefollowingfinitesampleguarantee
n+ncal+1
3.2.FindingEfficientScoresforConformalPrediction
P (y ∈C (x ))≥1−α. (2)
n+ncal+1 α n+ncal+1 WealsoinvestigatethebenefitsofusingOODscoresasnon-
conformityscoresinCP.Commonwaystobuildprediction
Inordertoobtaintheguaranteeequation(2),theonlyas- sets for classification, such as LAC (Sadinle et al., 2019)
sumptionrequiredisthatthecalibrationandtestdataform or APS (Romano et al., 2020) and RAPS (Angelopoulos
an exchangeable sequence (a condition weaker than, and etal.,2020)arebasedonthesoftmaxoutputofclassifiers.
thereforeautomaticallysatisfiedbyindependenceandiden- However, non-conformity scores also exist for other pre-
ticaldistribution)(Shafer&Vovk,2008)andthattheyare dictors(Vovketal.,2005),forinstance,basedonnearest
independentofthetrainingdata. Itisessentialtoknowthat neighbordistance(Shafer&Vovk,2008). Inthiswork,we
theguaranteeequation(2)ismarginal,i.e. holdsinaverage suggestinterpretinganyOODscoreasapotentialgeneral
overboththechoiceofthecalibrationdatasetandthetest replacementforscoresinCP,openingalargeavenueforCP
sample. Asweshallemphasize,theremightbefluctuations scorecrafting. ThisideacouldapplytoanyMLtask,but
duetothefinitesamplesizeofthecalibrationdataset. wedemonstratethatonaclassificationtasktobeconsistent
3ConformalOOD
withthestandardOODbenchmarksettingsthatwefollow 4.2.Fluctuationsofthep-value
inthepresentpaper.
Infact,(Batesetal.,2022)pointsoutthatthequantity
4.OODScoresThroughtheLensofCP F(τ;Dcal):=P (cid:0) umarg(x)≤τ|Dcal(cid:1) .
id x∼Pid (cid:98) id
Let us begin by describing the typical benchmark setup
for the evaluation of an OOD score. First, two datasets
withnon-overlappinglabelsarechosen,oneofthemtobe
usedastheIDdataset,andtheotherone,D astheOOD
ood
dataset. TheIDdatasetissplitintoatrainingsetDtrainand
id
atestsetDtest. AnOODdetectoristrainedusingtheID
id
trainingsetalone,whichwilloutputanOODscoreforeach
inputprovided. Finally,theIDtestsetandtheOODdataset
areusedtoevaluatetheperformanceoftheobtainedOOD
scorewithcommonevaluationmetrics: F-scores,AUROC
orFPR@TPRβ.
4.1.OODviaHypothesisTesting
LetusfirstrewritetheproblemofOODdetectionusingthe
frameworkofstatisticalhypothesistesting. Theframework
ofhypothesistestingallowsustoreasonintermsofp-values,
which have multiple benefits: they have a rigorous math-
ematical definition, and probabilistic interpretation, they
Figure1.HistogramofF(0.1;Dcal)fordifferentcalibrationsets.
canbeinterpretedequivalentlyforanyscore,andusedfor id
Thehistogramisobtainedbysplittingthedatasetsvhn extrainto
comparisonofdifferentscores. Givenatestexamplex ,
test disjointcalibrationsetsof10000pointseach,andapproximating
wewishtotestforx ∼P ,i.e. wewishtotestthenull
test id the value of F for each calibration set by integrating over the
hypothesisH :x ∼P againstthealternatehypothesis
0 test id remaining521131examples.
H :x ̸∼P . ThevalueP (s (x)≥s (x ))
1 test id x∼Pid ood ood test
is an exact p-value for the null hypothesis H . Since we is a random variable that depends on the calibration set
0
don’t have access to the distribution P , a first naive ap- Dcal. Asapracticalconsequence,thesameOODscores
id id ood
proach would be to approximate P with the empirical correctedusingtwodifferentcalibrationdatasetswillgive
id
distributionobtainedfromasampleofi.i.d. randomvari- risetodifferentp-values. TherandomvariableF(τ;Dcal)
id
ablesx ∼P ,wecallthissamplethecalibrationset,and followsadistributionthatisknown: itisaBetadistribution
i id
wedenoteitbyDcal. However,fluctuationsinthissample thatdependsontheparametersn andτ:
id cal
can lead to over-confident estimations of the p-value. In
F(τ;Dcal)∼Beta(ℓ,n +1−ℓ), (5)
ordertoavoidobtainingover-confidentestimationsofthe id cal
p-value, we use the correction proposed by (Bates et al.,
whereℓ=⌊(n +1)τ⌋(cf. (Batesetal.,2022)or(Vovk,
cal
2022) (which can be originally traced to (Papadopoulos
2012)foraproofoftheresult). Inordertoillustratethis
etal.,2002)):
phenomenon,weleveragethefactthatSVHNdatasetpro-
videsanadditionalsetof530000extratestimages.Itallows
u (cid:98)marg(x)= 1+|{i∈D ic dal 1: +s o nod c( ax
l
)≥s ood(x i)}| . (3) s spim litu tl ia nt gin tg he5 o3 vd er ra 5w 3s 0o 00f 0th ee xr aa mnd plo em siv na tr hia eb sl ve hF n( eτ x; trD aic dda al) ta, sb ey
t
into53differentfoldsof10000exampleseach. Foreach
Withthiscorrection,weobtainmarginallyvalidp-values, fold, the 10000 examples are used to constitute the cali-
thatis,p-valuesthatsatisfy brationdatasetDcal,whereastheremainingover520000
id
examples are used to approximate the computation of F,
P (cid:0) umarg(x)≤t(cid:1) ≤t, forall 0≤t≤1. (4) i.e.,givenacalibrationdatasetDcal,
x∼Pid (cid:98) id
520000
Bymarginally,wearepointingoutthattheprobabilityin F(τ;Dcal)≈Fˆ(τ;Dcal)= 1 (cid:88) 1 .
theaboveformulaintegratesoverboththecalibrationset id id 520000 u(cid:98)marg(xk)≤τ
k=1
Dcal andthetestpointx. However,foragivencalibration (6)
id
set,thecorrectedp-valuemaystillbeoverlyconfident,as Duetothelargenumberofpointsusedintheapproximating
weshowinthenextsection. sum,the53valuesobtainedarefaithfulapproximationsof
4ConformalOOD
Figure2.Cumulativehistogramofthemarginalp-values(blue)andthecalibration-conditionalp-values(brown)obtainedbyperforming
theSimesadjustmentmethod.Fourzoomsofthesameplotareshown,obtainedwithacalibrationdatasetof10000pointsfromtheSVHN
dataset.Theapproximationofthemarginalp-valuesbecomespoorforsmallervaluesofα,anditcanbeoverlyoptimistic.Thecorrection
isconservativeforallvaluesofαsimultaneously,asshowninthefigure,whichhappenswithprobabilityδ=0.1overthechoiceofthe
calibrationdataset.
therandomvariablesF(τ;Dcal). Weperformthissimula- n ))–soasforthep-values,theempiricalapproximation
id cal
tionwithτ = 0.1andplotthe53valuesintoahistogram. F(cid:100)PR(τ) also depends on the calibration dataset Dcal and
id
Additionally,wefitaBetadistributiontothehistogramus- cangiverisetooverlyconfidentestimationsofthetrueFPR.
ingthescikit-learnlibrary. Theseplotsarefoundinfigure1. Thiseffectcanimpactothermetricsdefinedbasedonthis
Aswecansee,theestimatedparametersofthefittedbeta empiricalFPR.Inordertoobtainhigh-confidenceconser-
distribution are very close to those predicted by the theo- vativeestimatesoftheFPR,(Batesetal.,2022)proposea
reticalresultofequation(5). Ifthevalueu (cid:98)marg(x)werea correctionoftheempiricalFPRthatsatisfiesthefollowing:
truep-value,thevalueofF(τ;Dcal)wouldbeequaltoτ,
id (cid:104) + (cid:105)
butaswecanseefromthetheoreticalresultandtheexperi- P FPR(τ)≤F(cid:100)PR (τ),∀τ ∈R ≥1−δ, (10)
mentabove,F(τ;Dcal)isarandomvariablethatfluctuates
id
+
arounditsmeanvalueτ. where F(cid:100)PR (τ) is a correction version of the empirical
F(cid:100)PR(τ). ThecorrectedFPRisobtainedbyapplyingacor-
4.3.ProbabilisticGuaranteesforthep-valuesandFPR +
rectionfunctionhtotheempiricalFPR,i.e. F(cid:100)PR (τ) =
Inordertohaveguaranteedconservativeestimatesforthe h◦F(cid:100)PR(τ).
p-values, we follow (Bates et al., 2022) in further cor-
The calibration-conditional p-values are obtained by ap-
recting the marginal p-values thus obtaining calibration-
plying a correction function h the marginal p-values, i.e.
conditionalp-values. Givenauser-predefinedrisklevelδ,
ucc(x)=h◦umarg(x).
thecalibration-conditionalp-valuesuccwillsatisfy (cid:98) (cid:98)
(cid:98)
P(cid:16) P(cid:0) ucc(x)≤t|Dcal(cid:1)
≤t,
∀t∈(0,1)(cid:17)
≥1−δ, (7)
Fourdifferentcorrectionfunctionshareproposedby(Bates
(cid:98) id etal.,2022), theSimes, DKWM,AsymptoticandMonte
Carlo corrections. The Simes, DKWM and Monte Carlo
wheretheprobabilityinsideistakenoverx∼P ,andthe
id
probabilityoutsideoverthechoiceofDcal. Thus, witha correctionsallprovidethefinitesampleguaranteesofequa-
id tion(7)andequation(10),whiletheAsymptoticcorrection
probabilityofatleast1−δ, wecanbeconfidentthatwe
provides only an asymptotic guarantee, that is, when the
haveagoodcalibrationset,meaningthatourp-valueswill
numberofcalibrationpointsgoestoinfinity. Betweenthe
beconservative. Anillustrationofthedifferencebetween
threecorrectionsprovidingthefinitesampleguarantee,we
marginal p-values and calibration-conditional p-values is
find the Monte Carlo one to give tighter bounds (cf. the
givenin4.2,wherethecumulativehistogramsofbothare
appendixAformoredetailsonhowtheSimesandMonte
comparedusingtheso-calledSimescorrectionforobtaining
Carlocorrectionsaredefined).
thecalibration-conditionalp-values.
Likewise,wecancorrecttheFPRdirectly. Wedenoteby
4.4.ConformalMetricsforOOD
FPR(τ)thetrueFPRandbyF(cid:100)PR n(τ)theempiricalapprox-
imationoftheFPRobtainedusingnsamples,i.e., Basedontheabovediscussion,wedefinethenotionsofcon-
formalFPRandthenconformalAUROCandFPR@TPR95
FPR(τ)=P (s (x)≥τ), (8)
x∼Pid ood that are built upon conformal FPR. The conformal FPR
1 (cid:88) is obtained by applying the correction function h to the
F(cid:100)PR(τ)=
n
1 sood(xi)≥τ. (9)
empiricalFPRasexplainedintheprevioussection,while
cal
xi∈D ic dal
theconformalAUROCisobtainedbyplottingtheAUROC
NotethattheFPRandthep-valuesareintrinsicallylinked curvehavingconformalFPRasthex-axis. Similarly,con-
–wehavethatuˆmarg(x) = (n calF(cid:100)PR n(s ood(x))+1)/(1+ formalFPR@TPR95isobtainedbycomputingtheclassical
5ConformalOOD
Figure3.DifferentzoomlevelsoftheROCcurves.TheTPRiscalculatedbyusingallthepointsinthe”Cifar10”datasetforthethree
curves.AsfortheTPR,thebluecurveisobtainedbyusingalldatapointsinthe”svhn extra”dataset,theorangecurveisanapproximation
ofthebluecurveusing1000calibrationpoints,whereasthegreencurveisobtainedbycorrectingtheFPRviatheconformalAUROC
method.
FPR@TPR95andthenapplyingthecorrection. Thecom- correction for the AUROC, with δ = 0.01. The results
putationsareperformedbyconsideringtheIDvalidation are displayed in Table 1. We also run the benchmark for
datasetasthecalibrationdataset. ConformalFPR,AUROC, δ =0.05andFPR-95,whichwedefertoAppendixC.
andFPR@TPR95arenotnecessarilybetterapproximations
Table1showsthatafterthecorrection,theconformalAU-
oftherealFPR,AUROCandFPR@TPR95values.Nonethe-
ROCislowerthantheclassicalAUROC,byoftenmorethan
less,theyareguaranteedtouseconservativeestimatesofthe
1percent. Ontheonehand,thisissignificant,especiallyfor
FPRwithauser-definedmiscalibrationtoleranceδ,which
suchbenchmarkswheretheState-of-the-artoftenholdsbya
isanessentialpropertyinmanysafety-criticalapplications
fractionofapercentage.Ontheotherhand,thecorrectionis
orcertificationprocesses(Sellkeetal.,2001). Theeffectof
notsosevere,andthebestbaselinesstillgetverygoodAU-
thecorrectionontheROCcurveisillustratedinFigure4.3
ROCdespitethecorrection. Inotherwords,thecorrection
usingtheSVHNdatasetasIDandCifar-10asOOD.
islargeenoughtomanifestitsimportancebutlowenough
tostillbeuseableinpractice: itcostsonlyroughly1or2
4.5.SaferBenchmarksforOOD
percentinAUROCtobe99%surethattheFPRinvolvedin
AUROCand(toalesserextent)FPR@TPR95aretwomet- theAUROCcalculationisnotoverestimated.
ricsthatOODandADpractitionersintensivelyusetobench-
markandevaluatetheperformancesofdifferentOODde- 4.5.2.ADBENCH
tection algorithms. However, as we saw in the previous
We perform the same procedure as OpenOOD with AD-
sections, the evaluation can be overly optimistic, which
Bench(Hanetal.,2022),whichgathersmanytask-agnostic
canbedetrimentaltoalgorithmsdesignedforsafety-critical
OOD baselines – considered Anomaly Detection (AD),
applications. In this section, we reevaluate various OOD
hencethebenchmark’sname. Weconducttheexperiments
baselinesincludedintheveryfurnishedOpenOOD(Yang
with ”unsupervised AD” baselines, i.e. baselines that do
etal.,2022),andADBench(Hanetal.,2022)benchmarks
not leverage labeled anomalies. We apply the correction
andillustratethetrade-offbetweenperformancesandprob-
withδ =0.05andsummarizetheresultsinFigure4. The
abilisticguarantees.
completeresultsaredeferredtoAppendixD.
4.5.1.OPENOOD Figure4(left)showsascatterplotwithmeanclassicalAU-
ROCandmeanAUROCcorrectionoverdifferentmethods
OpenOOD(Yangetal.,2022)isanextensivebenchmark
foreachdatasetasy-axisandx-axis,respectively. Thevari-
fortask-basedOOD,i.e. forOODmethodsthatassessif
abilityandmagnitudeofthecorrectionarehigherthanfor
sometestdataresemblessometrainedbackbone’straining
OpenOODsincethenumberofpointsinthetestsetchanges
data. Usually, backbones trained on CIFAR-10, CIFAR-
dependingonthedatasetandisgenerallywaylower. This
100, Imagenet200, and Imagenet are considered. In our
observationisimportantbecauseitillustratesthebrittleness
case, we consider a ResNet18 trained on the first three
oftheconclusionsthatcanbedrawnfromADbenchmarks
datasets only since we are not evaluating a new baseline
andsupportstheincreasinglycommonlyacceptedfactthat
butonlyinvestigatinganewmetricforthebenchmark. We
no method is provably better than others in AD – one of
evaluatetheAUROCofseveralbaselineswithvariousOOD
thekeyconclusionsofADBench’spaperitself(Hanetal.,
datasetsgatheredintotwogroups,NearOODandFarOOD,
2022). Figure4(right)showsthemeanclassicalandcon-
following OpenOOD’s guidelines. We then compute the
formal AUROC for each baseline over the datasets. The
6ConformalOOD
CIFAR-10 CIFAR-100 ImageNet-200
OODtype NearOOD FarOOD NearOOD FarOOD NearOOD FarOOD
class. conf. class. conf. class. conf. class. conf. class. conf. class. conf.
OpenMax(Bendale&Boult,2015) 87.2 85.95 89.53 88.3 76.66 74.95 79.12 77.52 80.4 78.82 90.41 88.77
MSP(Hendrycks&Gimpel,2018) 87.68 86.56 91.0 89.98 80.42 78.93 77.58 76.0 83.3 81.85 90.2 88.83
TempScale(Guoetal.,2017) 87.65 86.55 91.27 90.3 80.98 79.51 78.51 76.95 83.66 82.21 90.91 89.53
ODIN(Liangetal.,2018) 80.25 79.04 87.21 86.26 79.8 78.3 79.44 77.92 80.32 78.85 91.89 90.59
MDS(Leeetal.,2018) 86.72 85.49 90.2 89.09 58.79 56.85 70.06 68.31 62.51 60.68 74.94 73.09
MDSEns(Leeetal.,2018) 60.46 58.69 74.07 72.72 45.98 43.97 66.03 64.43 54.58 52.76 70.08 68.35
Gram(Sastry&Oore,2020) 52.63 50.69 69.74 68.11 50.69 48.69 73.97 72.63 68.36 66.74 70.94 69.3
EBO(Liuetal.,2020) 86.93 85.9 91.74 90.9 80.84 79.36 79.71 78.19 82.57 81.1 91.12 89.71
GradNorm(Huangetal.,2021) 53.77 51.92 58.55 56.76 69.73 68.11 68.82 67.19 73.33 71.85 85.29 83.99
ReAct(Sunetal.,2021) 86.47 85.41 91.02 90.12 80.7 79.23 79.84 78.32 80.48 79.0 93.1 91.79
MLS(Hendrycksetal.,2022) 86.86 85.81 91.61 90.74 81.04 79.58 79.6 78.07 82.96 81.5 91.34 89.94
KLM(Hendrycksetal.,2022) 78.8 77.58 82.76 81.63 76.9 75.38 76.03 74.52 80.69 79.14 88.41 86.74
VIM(Wangetal.,2022) 88.51 87.42 93.14 92.25 74.83 73.17 82.11 80.69 78.81 77.2 91.52 90.05
KNN(Sunetal.,2022) 90.7 89.69 93.1 92.19 80.25 78.79 82.32 80.93 81.75 80.27 93.47 92.25
DICE(Sun&Li,2022) 77.79 76.44 85.41 84.37 79.15 77.61 79.84 78.33 81.97 80.5 91.19 89.84
RankFeat(Songetal.,2022) 76.33 74.76 70.15 68.39 62.22 60.33 67.74 65.9 58.57 57.0 38.97 37.09
ASH(Djurisicetal.,2022) 74.11 72.71 78.36 77.02 78.39 76.89 79.7 78.23 82.12 80.72 94.23 93.11
SHE(Zhangetal.,2023) 80.84 79.64 86.55 85.55 78.72 77.18 77.35 75.8 80.46 79.0 90.48 89.17
Table1.ClassicalAUROC(class.) vsConformalAUROC(conf.) obtainedwiththeMonteCarlomethodandδ = 0.01forseveral
baselinesfromOpenOODbenchmark.
0.9 0.7 0.69 0.64 0.68 0.63 0.66 0.67 0.66 0.67 0.64 0.67 0.65 C Cl oa ns fs oi rc mal a A l U AR UO RC OC
0.61
0.8 0.6 0.57 0.54 0.55 0.53 0.56 0.56 0.56 0.56 0.53 0.56 0.54 0.53
0.50
0.5
0.41
0.7 0.4
0.6 0.3
0.2
0.5
0.1
0.4
0.0
0.05 0.10
Mean
AU0 R.1 O5
C
correct0 io.2 n0 0.25 IForest OCSVM CBLOF COF COPOD ECOD HBOS KNN LOF PCA SOD DeepSVDD DAGMM
variable
Figure4.ResultsvisualizationforADBenchbenchmark.(left)ScatterplotwithmeanclassicalAUROCandmeanAUROCcorrection
overdifferentmethodsforeachdatasetasy-axisandx-axis,respectively.(right)MeanAUROCandAUROCcorrectionoverdifferent
datasetsforeachADmethod.
correctionismorestable,demonstratingthatthecorrection Sofar,wehaveshownhowOODcanuseCP,butweargue
affectsallbaselinessimilarly. thatCPcouldalsouseOOD.Indeed,CPisaboutinterpret-
ingscorestoprovideprobabilisticresults. ButCPworks
regardlessofthegivenscore.Indeed,allscoreswillhavethe
5.ADeeperLinkbetweenCPandOOD
sameguarantee,butbetterscoreswillgivetighterprediction
Intheprevioussections,wehavemostlyemphasizedthat sets,andworsescoreswillgiveverylargeanduninformative
practitionersofOODdetectionshouldlookatCPasanad- predictionsets. ForCPtoprovidepowerfulprobabilistic
ditionalbuildingblockforcorrectlyinterpretingthescores guarantees, the scores have to be informative, hence the
thatalltheOODmethodsrelyon. Inthissection,weadvo- commonpracticeofrelyingonscoresderivedfromthesoft-
catethatthelinkbetweenOODandCPgoesevendeeper max values of a neural network (Sadinle et al., 2019). It
andthatbothfieldscouldbenefitfromeachother. turnsoutthatthesoftmaxisalsoascoreusedinOODde-
7
CORUA
lacissalc
naeM
CORUA
lamrofnoc
dna
lacissalc
naeMConformalOOD
LAC APS RAPS
α 0.005 0.01 0.05 0.005 0.01 0.05 0.005 0.01 0.05
Gram 9.57 8.34 1.89 9.60±0.10 1.93±0.06 8.66±0.13 9.56±0.13 8.7±0.16 1.89±0.03
ReAct 3.75 1.98 1.03 4.47±0.16 1.97±0.09 3.62±0.17 4.46±0.15 3.67±0.19 2.02±0.09
ODIN 7.15 5.82 1.14 7.42±0.17 1.53±0.06 5.14±0.08 7.45±0.16 5.1±0.10 1.57±0.08
KNN 2.57 1.48 1.01 3.62±0.15 1.09±0.03 2.71±0.11 3.69±0.11 2.77±0.09 1.08±0.02
Mahalanobis 1.85 1.47 1.04 1.89±0.07 1.04±0.01 1.49±0.04 1.92±0.05 1.49±0.05 1.04±0.01
Softmax 2.44 1.73 1.03 3.92±0.26 1.1±0.01 2.16±0.13 3.81±0.24 2.17±0.11 1.09±0.01
Table2.Efficiency(mean±standarddeviationforAPSandRAPS)ofthepredictionsetsfordifferentscoresforConformalclassification
onCifar10.Thebestisbolded,andthesecondisunderlined.
tection(Hendrycks&Gimpel,2018),whichsuggeststhat WeusedalimitednumberofOODscoresinourexperiment,
OODscoresandCPscoresmightberelatedinsomeway. but many more are available. This section highlights the
Inthissection,weexploreusingdifferentOODscoresto needtoexplorenonconformityscorecraftinginthisresearch
performCP.WeconsideraResNet18trainedonCIFAR-10 arearatherthandemonstratethesuperiorityofMahalanobis
andbuildconformalpredictionsetsfollowingtheprocedure orKNNovernonconformityscores.
describedinsection2.2. Tobuildthesepredictionsets,we
usescoresbasedonReAct(Sunetal.,2021),Gram(Sastry 6.Conclusion&Discussion
&Oore,2020),KNN(Sunetal.,2022),Mahalanobis(Lee
Inconclusion,ourworkhighlightstheinherentrandomness
etal.,2018),andODIN(Liangetal.,2018). Notethatwe
ofOODmetricsanddemonstrateshowConformalPredic-
had to adapt those scores to make them class-dependent
tion(CP)caneffectivelycorrectthesemetrics.Wehavealso
sincethescoreusedinCPisdefinedass (x,y). Wedid
cp shownthatrecentadvancementsinCPallowforuniform
sousingtheOODlibraryoodeel1, followingaprocedure
conservativeness guarantees on OOD metrics, providing
thatwedescribeindetailinAppendixB.Then,giventhe
morereliableevaluations. Furthermore,ouranalysisreveals
OOD score s (x,y ), we construct softmax-like scores
ood i (cid:80) thatthecorrectionintroducedbyCPdoesnotsignificantly
sˆ(x,y )=exps (x,y )/ exps (x,y ),anduseit
i ood i j ood j impact the performance of the best OOD baselines. On
forCP.
theotherhand,wealsoshowedthatwecoulduseOODto
For each defined score, we perform the calibration step improve existing CP techniques by using OOD scores as
on n = 2000 points following the classical Least- nonconformityscores. Wefoundthatsomeofthem,espe-
cal
Ambiguoussetclassifiers(LAC)procedure(Sadinleetal., ciallyMahalanobisandKNN,aregoodcandidatesfornon-
2019),andthemorerecentAdaptivePredictionSet(APS) conformityscores,unlockingawholeavenueforcrafting
(Romanoetal.,2020)andRegularizedAdaptivePrediction CPnonconformityscoresbasedontheplethoraofexisting
Set(RAPS)(Angelopoulosetal.,2020)methods. Forall post-hocOODscores.
methods, we construct the prediction sets for each of the
By integrating CP with OOD, we have demonstrated the
remaining n −n = 8000 points, and for coverages
val cal fruitful synergy between the two fields. OOD detection
1 − α ∈ {0.005,0.01,0.05}. The prediction sets calcu-
focusesondevelopingscoresthataccuratelydiscriminate
lations are implemented using the library PUNCC2. We
betweenOODandID,whileCPspecializesininterpreting
assess the mean efficiency of the prediction sets for each
scorestoprovideprobabilisticguarantees. Thisinterplay
score,includingLAC,APS,andRAPSbasedonsoftmax,
betweenOODandCPpresentsopportunitiesformutualad-
asclassicallydoneinCPinTable5). SinceAPSandRAPS
vancement:advancementsinCPresearchcanenhanceOOD
involvesamplingauniformrandomvariable,wereportthe
by offering more refined probabilistic interpretations of
meanandthestandarddeviationofthemeanefficiencyfor
OODscores,whichisparticularlycrucialinsafety-critical
10evaluations.
applications. Conversely, progress in OOD research can
Table5showsthatallOODscoresareinefficientforCP.For benefitCPbyprovidingscoresthatimprovetheefficiency
example, Gram performsvery poorly. However, in some ofpredictionsets. Thissuggeststhatfurtherexplorationand
instances,somescores,likeKNNorMahalanobis,perform collaborationbetweenthetwofieldsholdgreatpotential.
better than classical CP scores. This suggests that OOD
Insummary,ourfindingsunderscoretheintertwinednature
scoresmaybegoodcandidatesasnonconformityscores.
ofOODandCP,emphasizingtheneedforcontinuedinves-
1https://github.com/deel-ai/oodeel tigationandcross-fertilizationtoadvancebothdisciplines.
2https://github.com/deel-ai/puncc
8ConformalOOD
Impactstatement Han, S., Hu, X., Huang, H., Jiang, M., andZhao, Y. Ad-
bench: Anomalydetectionbenchmark. AdvancesinNeu-
Thispaperpresentsworkwhosegoalistoadvancethefield
ral Information Processing Systems, 35:32142–32159,
of Machine Learning. There are many potential societal
2022.
consequences of our work, none which we feel must be
specificallyhighlightedhere. Haroush, M., Frostig, T., Heller, R., and Soudry, D. A
statisticalframeworkforefficientoutofdistributionde-
References tection in deep neural networks, March 2022. URL
http://arxiv.org/abs/2102.12967.
Angelopoulos, A., Bates, S., Malik, J., and Jordan, M. I.
Uncertainty sets for image classifiers using conformal Hendrycks, D.andGimpel, K. ABaselineforDetecting
prediction. arXivpreprintarXiv:2009.14193,2020. MisclassifiedandOut-of-distributionExamplesinNeural
Networks. CoRR,abs/1610.02136,2016. URLhttp:
Angelopoulos,A.N.andBates,S. Agentleintroduction //arxiv.org/abs/1610.02136.
toconformalpredictionanddistribution-freeuncertainty
quantification,2022. Hendrycks, D.andGimpel, K. ABaselineforDetecting
MisclassifiedandOut-of-DistributionExamplesinNeu-
Balasubramanian,V.,Ho,S.-S.,andVovk,V. Conformal ral Networks, October 2018. URL http://arxiv.
predictionforreliablemachinelearning: theory,adapta- org/abs/1610.02136.
tionsandapplications. Newnes,2014.
Hendrycks, D., Basart, S., Mazeika, M., Zou, A., Kwon,
Bates, S., Cande`s, E., Lei, L., Romano, Y., andSesia, M. J., Mostajabi, M., Steinhardt, J., andSong, D. Scaling
TestingforOutlierswithConformalp-values,May2022. Out-of-distributionDetectionforReal-worldSettings. In
URLhttp://arxiv.org/abs/2104.08279. Chaudhuri,K.,Jegelka,S.,Song,L.,Szepesva´ri,C.,Niu,
G., and Sabato, S. (eds.), International Conference on
Bendale, A. and Boult, T. E. Towards Open Set Deep
MachineLearning,ICML2022,17-23July2022,Balti-
Networks. CoRR,abs/1511.06233,2015. URLhttp:
more, Maryland, USA, volume 162 of Proceedings of
//arxiv.org/abs/1511.06233.
Machine Learning Research, pp. 8759–8773. PMLR,
2022.URLhttps://proceedings.mlr.press/
Djurisic,A.,Bozanic,N.,Ashok,A.,andLiu,R. Extremely
v162/hendrycks22a.html.
Simple Activation Shaping for Out-of-distribution De-
tection. CoRR, abs/2209.09858, 2022. doi: 10.48550/ Huang, R., Geng, A., and Li, Y. On the Importance of
ARXIV.2209.09858. URL https://doi.org/10. GradientsforDetectingDistributionalShiftsintheWild.
48550/arXiv.2209.09858. CoRR,abs/2110.00218,2021. URLhttps://arxiv.
org/abs/2110.00218.
Geifman, Y. and El-Yaniv, R. Selective classification for
deepneuralnetworks. Advancesinneuralinformation Kaur, R., Jha, S., Roy, A., Park, S., Dobriban, E.,
processingsystems,30,2017. Sokolsky, O., and Lee, I. iDECODe: In-Distribution
Equivariance for Conformal Out-of-Distribution De-
Geifman, Y. and El-Yaniv, R. SelectiveNet: A deep
tection. Proceedings of the AAAI Conference on
neural network with an integrated reject option. In
Artificial Intelligence, 36(7):7104–7114, June 2022.
Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceed-
ISSN2374-3468,2159-5399. doi: 10.1609/aaai.v36i7.
ings of the 36th International Conference on Machine
20670. URL https://ojs.aaai.org/index.
Learning,volume97ofProceedingsofMachineLearn-
php/AAAI/article/view/20670.
ingResearch,pp.2151–2159.PMLR,09–15Jun2019.
URLhttps://proceedings.mlr.press/v97/ Laxhammar, R. Conformal anomaly detection. Sko¨vde,
geifman19a.html. Sweden: UniversityofSko¨vde,2,2014.
Guan,L.andTibshirani,R. Predictionandoutlierdetection Laxhammar, R. and Falkman, G. Sequential conformal
inclassificationproblems.JournaloftheRoyalStatistical anomalydetectionintrajectoriesbasedonhausdorffdis-
SocietySeriesB:StatisticalMethodology,84(2):524–546, tance. In14thinternationalconferenceoninformation
2022. fusion,pp.1–8.IEEE,2011.
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. Lee,K.,Lee,K.,Lee,H.,andShin,J. ASimpleUnified
On Calibration of Modern Neural Networks. CoRR, Framework for Detecting Out-of-distribution Samples
abs/1706.04599, 2017. URL http://arxiv.org/ andAdversarialAttacks. CoRR,abs/1807.03888,2018.
abs/1706.04599. URLhttp://arxiv.org/abs/1807.03888.
9ConformalOOD
Leys,C.,Klein,O.,Dominicy,Y.,andLey,C. Detecting Song, Y., Sebe, N., and Wang, W. RankFeat: Rank-
multivariateoutliers: UsearobustvariantoftheMaha- 1 Feature Removal for Out-of-distribution Detection.
lanobisdistance. JournalofExperimentalSocialPsychol- CoRR, abs/2209.08590, 2022. doi: 10.48550/ARXIV.
ogy,2018. 2209.08590. URLhttps://doi.org/10.48550/
arXiv.2209.08590.
Liang, S., Li, Y., and Srikant, R. Enhancing The Relia-
bilityofOut-of-distributionImageDetectioninNeural Sun, Y. and Li, Y. DICE: Leveraging Sparsification for
Networks. In6thInternationalConferenceonLearning Out-of-distribution Detection. In Avidan, S., Brostow,
Representations, ICLR 2018, Vancouver, BC, Canada, G.J.,Cisse´,M.,Farinella,G.M.,andHassner,T.(eds.),
April30-May3,2018,ConferenceTrackProceedings. ComputerVision-ECCV2022: 17thEuropeanConfer-
OpenReview.net,2018.URLhttps://openreview. ence,TelAviv,Israel,October23-27,2022,Proceedings,
net/forum?id=H1VGkIxRZ. PartXXIV,volume13684ofLectureNotesinComputer
Science, pp. 691–708. Springer, 2022. doi: 10.1007/
Liang, Z., Sesia, M., and Sun, W. Integrative conformal 978-3-031-20053-3\ 40. URLhttps://doi.org/
p-valuesforpowerfulout-of-distributiontestingwithla- 10.1007/978-3-031-20053-3_40.
beled outliers, August 2022. URL http://arxiv.
org/abs/2208.11111. Sun, Y., Guo, C., and Li, Y. ReAct: Out-of-
distributionDetectionWithRectifiedActivations. CoRR,
Liu,W.,Wang,X.,Owens,J.D.,andLi,Y. Energy-based abs/2111.12797,2021. URLhttps://arxiv.org/
Out-of-distribution Detection. CoRR, abs/2010.03759, abs/2111.12797.
2020. URL https://arxiv.org/abs/2010.
03759. Sun, Y., Ming, Y., Zhu, X., and Li, Y. Out-of-
distribution Detection with Deep Nearest Neighbors.
Papadopoulos,H.,Proedrou,K.,Vovk,V.,andGammerman, CoRR, abs/2204.06507, 2022. doi: 10.48550/ARXIV.
A. Inductiveconfidencemachinesforregression. InMa- 2204.06507. URLhttps://doi.org/10.48550/
chineLearning:ECML2002:13thEuropeanConference arXiv.2204.06507.
onMachineLearningHelsinki,Finland,August19–23,
2002Proceedings13,pp.345–356.Springer,2002. Vovk,V. ConditionalValidityofInductiveConformalPre-
dictors. InProceedingsoftheAsianConferenceonMa-
Pearce, T., Brintrup, A., and Zhu, J. Understanding chineLearning, pp.475–490.PMLR,November2012.
softmax confidence and uncertainty. arXiv preprint URLhttps://proceedings.mlr.press/v25/
arXiv:2106.04972,2021. vovk12.html.
Romano,Y.,Sesia,M.,andCandes,E. Classificationwith Vovk, V., Nouretdinov, I., and Gammerman, A. Testing
validandadaptivecoverage. AdvancesinNeuralInfor- exchangeabilityon-line. InProceedingsofthe20thIn-
mationProcessingSystems,33:3581–3591,2020. ternationalConferenceonMachineLearning(ICML-03),
pp.768–775,2003.
Sadinle,M.,Lei,J.,andWasserman,L. Leastambiguous
set-valuedclassifierswithboundederrorlevels. Journal Vovk, V., Gammerman, A., and Shafer, G. Algorithmic
oftheAmericanStatisticalAssociation, 114(525):223– learninginarandomworld,volume29. Springer,2005.
234,2019.
Wang, H., Li, Z., Feng, L., and Zhang, W. ViM: Out-
Sastry, C. S. and Oore, S. Detecting Out-of-distribution Of-distribution with Virtual-logit Matching. CoRR,
ExampleswithGramMatrices.InProceedingsofthe37th abs/2203.10807, 2022. doi: 10.48550/ARXIV.2203.
InternationalConferenceonMachineLearning,ICML 10807. URL https://doi.org/10.48550/
2020, 13-18 July 2020, Virtual Event, volume 119 of arXiv.2203.10807.
ProceedingsofMachineLearningResearch,pp.8491–
8501. PMLR, 2020. URL http://proceedings. Yang, J., Zhou, K., Li, Y., and Liu, Z. General-
mlr.press/v119/sastry20a.html. ized Out-of-distribution Detection: A Survey. CoRR,
abs/2110.11334,2021. URLhttps://arxiv.org/
Sellke,T.M.,Bayarri,M.J.,andBerger,J.O. Calibration abs/2110.11334.
ofρvaluesfortestingprecisenullhypotheses.TheAmeri-
canStatistician,55:62–71,2001.URLhttps://api. Yang,J.,Wang,P.,Zou,D.,Zhou,Z.,Ding,K.,Peng,W.,
semanticscholar.org/CorpusID:396772. Wang, H., Chen, G., Li, B., Sun, Y., et al. Openood:
Benchmarkinggeneralizedout-of-distributiondetection.
Shafer,G.andVovk,V. Atutorialonconformalprediction. AdvancesinNeuralInformationProcessingSystems,35:
JournalofMachineLearningResearch,9(3),2008. 32598–32611,2022.
10ConformalOOD
Zhang,J.,Fu,Q.,Chen,X.,Du,L.,Li,Z.,Wang,G.,Liu,
X., Han, S., and Zhang, D. Out-of-distribution Detec-
tion based on In-distribution Data Patterns Memoriza-
tion with Modern Hopfield Energy. In The Eleventh
InternationalConferenceonLearningRepresentations,
ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenRe-
view.net,2023. URLhttps://openreview.net/
pdf?id=KkazG4lgKL.
11ConformalOOD
A.Appendix: SimesandMonteCarlocorrections
Inourworkweusetwoofthecorrectionsproposedby(Batesetal.,2022),SimesandMonteCarloCorrection. Inthis
section,weintroducethesecorrectionsforthesakeofcompleteness,aswellastwoothercorrectionsthatwedonotusefor
reasonstobedetailed.
SimesCorrection Generally,weareinterestedinsmallp-valuesandtheSimescorrectionfocusesonthose,thatisby
addingasmallercorrectiontothesmallerp-valuesthanthelargerones.
(cid:18)
i···(i−n/2+1)
(cid:19)2/n
bs =1−δ2/n , i=1,...,n (11)
n+1−i n···(n−n/2+1)
DKWM TheformerapproachmaybecomparedtotheclassicaluniformconcentrationDKWMresult,wherethebare
definedas
(cid:112)
bd =min{(i/n)+ log(2/δ)/2n,1}; (12)
i
However,DKWMtendstoprovidemuchlargerboundsthanSimes.
AsymptoticCorrection Thepreviousapproachbroughtfinitesampleguaranteesbutatthecostofalargecorrection. In
ordertoproduceatighterbound,foramorepowerfultest,welookintoacorrectionthatiscorrectasymptotically.
(cid:16)(cid:112) (cid:17)−1
c (δ):= 2loglogn (−log[−log(1−δ)]
n
(13)
+2loglogn+(1/2)logloglogn−(1/2)logπ).
(cid:40) (cid:112) (cid:41)
i i(n−i)
ba =min +c (δ) √ ,1 , i=1,...,n (14)
i n n n n
ThisboundisquitesimilartoSimesforsmallvalues,butquitetighterfortheremainingones.
MonteCarloCorrection TheMonterCarloCorrectionoffersadvantagesofboththeSimesandAsymptoticmethods. It
providesafinite-sampleguarantee,mimicsSimesforsmallp-valuesandremainsclosertotheasymptoticcorrectionfor
largerones.
(cid:110) (cid:111)
hm,δˆ
(t)=min
hs(t),ha,δˆ
(t) , t∈[0,1]. (15)
B.Appendix: Designingclass-dependentOODscoresforCP
Let’sconsideraclassificationtaskwithaclassifierf trainedtofitadataset{(x ,y ),...,(x ,y )},wherex ∈ X and
1 1 n n i
y ∈{1,...C}foralli∈{1,...,n}. InOOD,thescorefunctions :X →R,whereasinCP,thenon-conformityscore
i ood
s :X ×R→R. Hence,inordertoconstructanon-conformityscoreoutofs ,wehavetomakeitclass-dependent. In
cp ood
thissection,wedescribehowtoconstructclass-dependentOODscoresoutofclassicalOODscoresforappropriateusagein
CP.
B.1.ReAct
ReActmethod(Sunetal.,2021)getsthequantilesoff’spenultimatelayer’sactivationvaluesandthenclipstheactivation
values for a new input data point. The output softmax are then used for OOD scoring. Therefore, making the score
class-dependentisstraightforward: oneonlyhastogettheclasssoftmax.
B.2.ODIN
TheideaofODIN(Liangetal.,2018)isalsototweakthenetworksothatthesoftmaxbecomesmoreinformativeforOOD
detection. SimilarlytoReAct,oneonlyhastogeteachclass’ssoftmaxtomakethescoreclass-dependent.
12ConformalOOD
B.3.KNN
Foreach{x ,...,x }fromthetrainingset,considerH = {h(x ),...,h(x )}whereh : X → Rp isdefinedsuchthat
1 n 1 n
h(x )istheactivationvectorofx off’spenultimatelayer. LetN : Rp → Rp bethenearestneighbormapsuchthat
i i H
N(h)isthenearestneighborofhamongH. KNN(Sunetal.,2022)buildsthescores as
ood
s (x )=∥h(x )−N (h(x )∥.
ood n+1 n+1 H n+1
Tomakethisscoreclass-dependent,onecanbuildC maps{N ,...,N }whereH = {h(x )|f(x ) = k}andthen
H1 HC k i i
defineanewscore
s (x ,y)=∥h(x )−N (h(x )∥
ood n+1 n+1 Hy n+1
B.4.Mahalanobis
LetconsiderthemaphasinKNN.Foreachk ∈{1,...,C},Mahalanobisdistancemethod(Leeetal.,2018)computesΣ
k
andµ ,whicharetheempiricalcovariancematrixandmeanvectorsofeachsetofpoints{h(x )} . Then,thescore
k i i|f(xi)=k
s iscomputedas:
ood
(cid:113)
s (x )= (x −µ )TΣ−1(x −µ ),
ood n+1 n+1 f(xn+1) n+1 f(xn+1)
whereΣ= 1 (cid:80) Σ . Tomakethescoreclass-dependent,onesimplyhastodefine
C k∈{1,...,C} k
(cid:113)
s (x ,y)= (x −µ )TΣ−1(x −µ ).
ood n+1 n+1 y y n+1 y
B.5.Gram
Letf beaclassifierofdepthL. Grammethod(Sastry&Oore,2020)buildsastatisticδ :X →RLthatoutputsthechannel-
wisecorrelationoftheactivationmapsforeachlayer. First,{δ(x ),...,δ(x )}arecomputed. Then,amulti-dimensional
1 n
statistic{d } iscomputedforeachlayerafteraclass-wiseaggregation.
l,k l∈{1,...,L},k∈{1,...,C}
Foranewtestpointx ,δ(x )iscomputed,alongwithf(x ). Thescoreisbuiltoutofaweightedmeanofthe
n+1 n+1 n+1
layer-wisedeviation:
(cid:88)
s (x )= w |δ(x ) −d |,
ood n+1 l n+1 l l,f(xn+1)
l∈{1,...,L}
where{w } aresomenormalizationweightscomputedwiththetrainingdata. Itisquitestraightforwardtomake
l l∈{1,...,L}
thisOODscoreclass-dependentbydefining
(cid:88)
s (x ,y)= w |δ(x ) −d |.
ood n+1 l n+1 l l,y
l∈{1,...,L}
C.Appendix: ComplementaryresultsonOpenOODbenchmark
Inthissection,wepresentthefullresultsofbenchmarksonOpenOOD.TheresultsdisplayedareAUROCwithδ =0.05in
Table3,FPR@TPR95withδ =0.05inTable5andFPR@TPR95withδ =0.01inTable4.
13ConformalOOD
CIFAR-10 CIFAR-100 ImageNet-200
OODtype Near Far Near Far Near Far
marg. conf. marg. conf. marg. conf. marg. conf. marg. conf. marg. conf.
OpenMax(Bendale&Boult,2015) 87.2 86.18 89.53 88.52 76.66 75.26 79.12 77.81 80.4 79.2 90.41 89.49
MSP(Hendrycks&Gimpel,2016) 87.68 86.76 91.0 90.17 80.42 79.2 77.58 76.29 83.3 82.2 90.2 89.36
TempScale(Guoetal.,2017) 87.65 86.75 91.27 90.48 80.98 79.78 78.51 77.24 83.66 82.58 90.91 90.11
ODIN(Liangetal.,2018) 80.25 79.26 87.21 86.43 79.8 78.57 79.44 78.2 80.32 79.19 91.89 91.17
MDS(Leeetal.,2018) 86.72 85.71 90.2 89.29 58.79 57.2 70.06 68.63 62.51 60.96 74.94 73.6
MDSEns(Leeetal.,2018) 60.46 59.01 74.07 72.96 45.98 44.34 66.03 64.72 54.58 52.99 70.08 68.76
Gram(Sastry&Oore,2020) 52.63 51.04 69.74 68.41 50.69 49.06 73.97 72.87 68.36 67.0 70.94 69.69
EBO(Liuetal.,2020) 86.93 86.08 91.74 91.05 80.84 79.63 79.71 78.47 82.57 81.47 91.12 90.33
GradNorm(Huangetal.,2021) 53.77 52.26 58.55 57.09 69.73 68.41 68.82 67.48 73.33 72.12 85.29 84.45
ReAct(Sunetal.,2021) 86.47 85.6 91.02 90.28 80.7 79.5 79.84 78.6 80.48 79.35 93.1 92.4
MLS(Hendrycksetal.,2022) 86.86 86.0 91.61 90.9 81.04 79.84 79.6 78.35 82.96 81.88 91.34 90.56
KLM(Hendrycksetal.,2022) 78.8 77.8 82.76 81.83 76.9 75.65 76.03 74.8 80.69 79.54 88.41 87.44
VIM(Wangetal.,2022) 88.51 87.62 93.14 92.41 74.83 73.47 82.11 80.95 78.81 77.57 91.52 90.7
KNN(Sunetal.,2022) 90.7 89.87 93.1 92.35 80.25 79.05 82.32 81.19 81.75 80.63 93.47 92.83
DICE(Sun&Li,2022) 77.79 76.68 85.41 84.56 79.15 77.89 79.84 78.61 81.97 80.86 91.19 90.43
RankFeat(Songetal.,2022) 76.33 75.05 70.15 68.71 62.22 60.67 67.74 66.24 58.57 57.06 38.97 37.43
ASH(Djurisicetal.,2022) 74.11 72.96 78.36 77.27 78.39 77.16 79.7 78.5 82.12 81.07 94.23 93.66
SHE(Zhangetal.,2023) 80.84 79.86 86.55 85.73 78.72 77.46 77.35 76.08 80.46 79.34 90.48 89.72
Table3.ClassicalAUROC(marg.) vsConformalAUROC(conf.) obtainedwiththeMonteCarlomethodandδ = 0.05forseveral
baselinesfromOpenOODbenchmark.
D.Appendix: FullresultsforADBench
Inthissection,wepresentthefullresultsoftheADBenchbenchmark. Table6displaysclassicalAUROC,Table7displays
conformalAUROC,andTable8displaysthedifferencebetweenthetwo(AUROCcorrection),allwithδ =0.05.
14ConformalOOD
CIFAR-10 CIFAR-100 ImageNet-200
OODtype Near Far Near Far Near Far
marg. conf. marg. conf. marg. conf. marg. conf. marg. conf. marg. conf.
OpenMax(Bendale&Boult,2015) 46.77 48.98 29.48 31.48 55.57 57.8 54.77 57.0 63.32 65.75 32.29 35.35
MSP(Hendrycks&Gimpel,2016) 53.57 55.8 31.44 33.45 54.73 56.96 59.08 61.31 55.25 57.69 35.44 38.29
TempScale(Guoetal.,2017) 56.85 59.08 33.36 35.38 54.77 56.99 58.24 60.47 55.03 57.5 34.11 37.06
ODIN(Liangetal.,2018) 84.55 86.78 60.9 62.97 58.44 60.67 57.75 59.98 66.38 68.8 33.66 36.75
MDS(Leeetal.,2018) 46.22 48.44 30.3 32.3 82.75 84.98 70.46 72.68 79.34 81.52 61.26 63.81
MDSEns(Leeetal.,2018) 92.06 94.29 61.09 62.87 95.84 98.07 66.97 68.85 91.69 93.8 80.43 82.89
Gram(Sastry&Oore,2020) 93.52 95.75 69.29 71.48 92.48 94.71 63.1 65.2 85.43 87.63 84.95 87.44
EBO(Liuetal.,2020) 67.54 69.77 40.55 42.58 55.49 57.72 56.41 58.64 59.46 61.93 34.0 37.07
GradNorm(Huangetal.,2021) 95.37 97.6 89.34 91.52 86.13 88.36 82.79 85.02 83.07 85.33 66.78 69.67
ReAct(Sunetal.,2021) 71.56 73.78 42.43 44.52 56.74 58.97 56.32 58.55 65.37 67.8 27.21 30.28
MLS(Hendrycksetal.,2022) 67.54 69.77 40.53 42.56 55.48 57.71 56.53 58.76 58.94 61.44 33.59 36.68
KLM(Hendrycksetal.,2022) 86.41 88.63 76.42 78.65 79.52 81.75 70.16 72.39 69.42 71.91 39.57 42.56
VIM(Wangetal.,2022) 48.07 50.29 25.77 27.65 62.96 65.19 49.72 51.95 59.91 62.32 26.86 29.81
KNN(Sunetal.,2022) 34.54 36.65 23.88 25.77 61.32 63.54 54.04 56.27 60.42 62.9 26.49 29.66
DICE(Sun&Li,2022) 80.15 82.38 53.93 56.06 58.1 60.33 55.95 58.17 60.98 63.46 35.93 39.04
RankFeat(Songetal.,2022) 67.38 69.61 68.24 70.47 79.94 82.17 68.89 71.11 92.02 93.91 98.48 99.58
ASH(Djurisicetal.,2022) 89.03 91.26 76.66 78.89 66.14 68.37 62.67 64.89 65.95 68.44 26.26 29.46
SHE(Zhangetal.,2023) 84.49 86.72 63.26 65.41 59.32 61.54 62.74 64.97 65.92 68.31 41.5 44.62
Table4.ClassicalFPR@TPR95(marg.)vsConformalFPR@TPR95(conf.)obtainedwiththeMonteCarlomethodandδ=0.01for
severalbaselinesfromOpenOODbenchmark.
CIFAR-10 CIFAR-100 ImageNet-200
OODtype Near Far Near Far Near Far
marg. conf. marg. conf. marg. conf. marg. conf. marg. conf. marg. conf.
OpenMax(Bendale&Boult,2015) 46.77 48.58 29.48 31.11 55.57 57.39 54.77 56.59 63.32 65.14 32.29 33.98
MSP(Hendrycks&Gimpel,2016) 53.57 55.39 31.44 33.08 54.73 56.55 59.08 60.9 55.25 57.06 35.44 37.16
TempScale(Guoetal.,2017) 56.85 58.67 33.36 35.01 54.77 56.59 58.24 60.06 55.03 56.85 34.11 35.8
ODIN(Liangetal.,2018) 84.55 86.37 60.9 62.59 58.44 60.26 57.75 59.57 66.38 68.2 33.66 35.34
MDS(Leeetal.,2018) 46.22 48.03 30.3 31.94 82.75 84.57 70.46 72.28 79.34 81.16 61.26 63.08
MDSEns(Leeetal.,2018) 92.06 93.88 61.09 62.54 95.84 97.66 66.97 68.5 91.69 93.51 80.43 82.25
Gram(Sastry&Oore,2020) 93.52 95.34 69.29 71.08 92.48 94.3 63.1 64.81 85.43 87.25 84.95 86.77
EBO(Liuetal.,2020) 67.54 69.36 40.55 42.21 55.49 57.31 56.41 58.23 59.46 61.28 34.0 35.7
GradNorm(Huangetal.,2021) 95.37 97.19 89.34 91.16 86.13 87.95 82.79 84.61 83.07 84.89 66.78 68.6
ReAct(Sunetal.,2021) 71.56 73.38 42.43 44.14 56.74 58.56 56.32 58.14 65.37 67.19 27.21 28.81
MLS(Hendrycksetal.,2022) 67.54 69.36 40.53 42.19 55.48 57.3 56.53 58.35 58.94 60.76 33.59 35.28
KLM(Hendrycksetal.,2022) 86.41 88.23 76.42 78.24 79.52 81.34 70.16 71.98 69.42 71.24 39.57 41.3
VIM(Wangetal.,2022) 48.07 49.88 25.77 27.3 62.96 64.78 49.72 51.54 59.91 61.72 26.86 28.46
KNN(Sunetal.,2022) 34.54 36.27 23.88 25.42 61.32 63.14 54.04 55.86 60.42 62.23 26.49 28.09
DICE(Sun&Li,2022) 80.15 81.97 53.93 55.67 58.1 59.92 55.95 57.77 60.98 62.8 35.93 37.66
RankFeat(Songetal.,2022) 67.38 69.2 68.24 70.06 79.94 81.76 68.89 70.71 92.02 93.84 98.48 99.55
ASH(Djurisicetal.,2022) 89.03 90.85 76.66 78.48 66.14 67.96 62.67 64.49 65.95 67.77 26.26 27.85
SHE(Zhangetal.,2023) 84.49 86.31 63.26 65.02 59.32 61.14 62.74 64.56 65.92 67.74 41.5 43.27
Table5.ClassicalFPR@TPR95(marg.)vsConformalFPR@TPR95(conf.)obtainedwiththeMonteCarlomethodandδ=0.05for
severalbaselinesfromOpenOODbenchmark.
15ConformalOOD
IForest OCSVM CBLOF COF COPOD ECOD HBOS KNN LOF PCA SOD DeepSVDD DAGMM
cover 0.87 0.93 0.89 0.77 0.89 0.92 0.80 0.86 0.85 0.94 0.74 0.46 0.90
donors 0.78 0.72 0.62 0.71 0.82 0.89 0.78 0.82 0.59 0.83 0.56 0.36 0.71
fault 0.57 0.48 0.64 0.62 0.44 0.45 0.51 0.73 0.59 0.46 0.68 0.52 0.46
fraud 0.90 0.91 0.88 0.96 0.88 0.89 0.90 0.93 0.96 0.90 0.95 0.73 0.90
glass 0.77 0.35 0.83 0.72 0.72 0.66 0.77 0.82 0.69 0.66 0.73 0.47 0.76
Hepatitis 0.70 0.68 0.66 0.41 0.82 0.75 0.80 0.53 0.38 0.76 0.68 0.52 0.55
Ionosphere 0.84 0.76 0.91 0.87 0.79 0.73 0.62 0.88 0.91 0.79 0.86 0.51 0.73
landsat 0.48 0.36 0.64 0.53 0.42 0.36 0.55 0.58 0.54 0.36 0.60 0.63 0.44
ALOI 0.57 0.56 0.55 0.65 0.54 0.56 0.53 0.61 0.67 0.57 0.61 0.51 0.52
letter 0.61 0.46 0.76 0.80 0.54 0.56 0.60 0.86 0.84 0.50 0.84 0.56 0.50
20news0 0.64 0.63 0.71 0.71 0.61 0.61 0.62 0.73 0.80 0.64 0.73 0.50 0.63
20news1 0.51 0.53 0.52 0.58 0.52 0.54 0.53 0.57 0.61 0.54 0.58 0.48 0.54
20news2 0.50 0.51 0.47 0.53 0.50 0.52 0.51 0.51 0.54 0.51 0.50 0.49 0.53
20news3 0.75 0.72 0.83 0.81 0.75 0.75 0.74 0.79 0.71 0.73 0.70 0.67 0.54
20news4 0.48 0.51 0.45 0.57 0.48 0.51 0.50 0.48 0.51 0.51 0.53 0.53 0.48
20news5 0.52 0.49 0.47 0.50 0.48 0.46 0.49 0.48 0.55 0.48 0.48 0.49 0.54
Lymphography 1.00 1.00 1.00 0.91 0.99 1.00 0.99 0.56 0.90 1.00 0.73 0.34 0.72
magic.gamma 0.73 0.61 0.75 0.67 0.68 0.64 0.71 0.82 0.69 0.67 0.75 0.60 0.59
musk 1.00 0.81 1.00 0.39 0.94 0.95 1.00 0.70 0.41 1.00 0.74 0.56 0.77
PageBlocks 0.90 0.89 0.85 0.73 0.88 0.92 0.81 0.82 0.76 0.91 0.78 0.59 0.90
pendigits 0.95 0.94 0.90 0.45 0.91 0.93 0.93 0.73 0.48 0.94 0.66 0.42 0.64
Pima 0.73 0.67 0.71 0.61 0.69 0.63 0.71 0.73 0.66 0.71 0.61 0.51 0.56
annthyroid 0.82 0.57 0.62 0.66 0.77 0.79 0.60 0.72 0.70 0.66 0.77 0.77 0.57
satellite 0.70 0.59 0.71 0.55 0.63 0.58 0.75 0.65 0.56 0.60 0.64 0.55 0.62
satimage-2 0.99 0.97 1.00 0.57 0.97 0.96 0.98 0.93 0.47 0.98 0.83 0.49 0.96
shuttle 1.00 0.97 0.83 0.52 0.99 0.99 0.99 0.70 0.57 0.99 0.70 0.49 0.98
smtp 0.86 0.72 0.70 0.69 0.70 0.78 0.56 0.84 0.58 0.83 0.40 0.72 0.71
speech 0.51 0.50 0.51 0.56 0.53 0.51 0.51 0.51 0.52 0.51 0.56 0.54 0.53
Stamps 0.91 0.84 0.68 0.54 0.93 0.88 0.91 0.69 0.51 0.91 0.73 0.56 0.89
thyroid 0.98 0.88 0.95 0.91 0.94 0.98 0.96 0.96 0.87 0.96 0.93 0.49 0.80
vertebral 0.37 0.38 0.41 0.49 0.26 0.41 0.29 0.34 0.49 0.37 0.40 0.37 0.53
vowels 0.75 0.63 0.90 0.95 0.55 0.62 0.73 0.97 0.93 0.67 0.92 0.56 0.61
Waveform 0.71 0.56 0.72 0.73 0.75 0.62 0.69 0.74 0.73 0.65 0.69 0.56 0.49
WDBC 0.99 0.99 0.99 0.96 0.99 0.97 0.99 0.92 0.89 0.99 0.92 0.62 0.77
Wilt 0.42 0.31 0.33 0.50 0.33 0.36 0.32 0.48 0.51 0.20 0.53 0.46 0.37
wine 0.80 0.73 0.26 0.44 0.89 0.77 0.91 0.45 0.38 0.84 0.46 0.60 0.62
WPBC 0.47 0.45 0.45 0.46 0.49 0.47 0.51 0.47 0.41 0.46 0.51 0.50 0.48
yeast 0.38 0.41 0.45 0.44 0.37 0.44 0.40 0.39 0.45 0.41 0.42 0.48 0.41
campaign 0.73 0.67 0.64 0.58 0.78 0.77 0.79 0.73 0.59 0.73 0.69 0.53 0.58
cardio 0.93 0.94 0.90 0.71 0.92 0.94 0.85 0.77 0.66 0.96 0.73 0.58 0.75
Cardiotocography 0.68 0.78 0.65 0.54 0.67 0.78 0.61 0.56 0.60 0.75 0.52 0.53 0.62
celeba 0.70 0.71 0.74 0.39 0.76 0.76 0.76 0.60 0.39 0.79 0.48 0.54 0.45
CIFAR100 0.73 0.68 0.70 0.70 0.69 0.70 0.70 0.74 0.74 0.70 0.71 0.56 0.53
CIFAR101 0.55 0.59 0.61 0.63 0.46 0.51 0.44 0.60 0.72 0.60 0.62 0.50 0.58
CIFAR102 0.56 0.58 0.58 0.61 0.56 0.57 0.54 0.60 0.65 0.58 0.59 0.58 0.51
CIFAR103 0.55 0.58 0.59 0.56 0.51 0.53 0.50 0.56 0.60 0.56 0.56 0.60 0.56
CIFAR105 0.50 0.58 0.58 0.57 0.47 0.52 0.47 0.54 0.60 0.57 0.54 0.46 0.59
CIFAR106 0.64 0.65 0.68 0.69 0.65 0.66 0.65 0.72 0.72 0.68 0.69 0.57 0.50
CIFAR107 0.54 0.59 0.56 0.57 0.52 0.55 0.50 0.54 0.60 0.57 0.56 0.62 0.61
agnews0 0.50 0.47 0.54 0.61 0.49 0.47 0.48 0.58 0.63 0.47 0.56 0.35 0.48
agnews1 0.58 0.54 0.58 0.71 0.51 0.54 0.55 0.62 0.74 0.55 0.61 0.37 0.56
agnews2 0.65 0.61 0.71 0.73 0.61 0.59 0.61 0.75 0.79 0.61 0.73 0.50 0.53
agnews3 0.54 0.55 0.57 0.70 0.51 0.53 0.51 0.62 0.70 0.55 0.61 0.50 0.51
amazon 0.56 0.54 0.58 0.58 0.57 0.54 0.56 0.59 0.56 0.54 0.58 0.45 0.51
imdb 0.50 0.45 0.50 0.49 0.50 0.45 0.48 0.48 0.49 0.46 0.50 0.52 0.42
yelp 0.61 0.59 0.64 0.68 0.60 0.57 0.59 0.68 0.66 0.59 0.66 0.50 0.55
Table6. FullresultsforADBench:classicalAUROC.
16ConformalOOD
IForest OCSVM CBLOF COF COPOD ECOD HBOS KNN LOF PCA SOD DeepSVDD DAGMM
cover 0.75 0.83 0.79 0.64 0.78 0.82 0.74 0.74 0.73 0.84 0.60 0.30 0.79
donors 0.72 0.66 0.54 0.64 0.77 0.85 0.72 0.76 0.53 0.78 0.50 0.31 0.65
fault 0.48 0.40 0.55 0.53 0.35 0.37 0.43 0.65 0.50 0.37 0.59 0.43 0.37
fraud 0.77 0.63 0.63 0.82 0.62 0.64 0.86 0.68 0.79 0.66 0.69 0.51 0.64
glass 0.65 0.31 0.73 0.60 0.60 0.53 0.64 0.72 0.58 0.54 0.63 0.38 0.66
Hepatitis 0.54 0.52 0.52 0.23 0.70 0.61 0.66 0.26 0.22 0.62 0.54 0.38 0.40
Ionosphere 0.77 0.68 0.85 0.80 0.70 0.63 0.50 0.83 0.85 0.71 0.80 0.38 0.64
landsat 0.42 0.30 0.58 0.48 0.35 0.30 0.49 0.52 0.48 0.30 0.54 0.58 0.39
ALOI 0.49 0.46 0.45 0.55 0.43 0.46 0.46 0.52 0.58 0.47 0.52 0.41 0.42
letter 0.44 0.30 0.61 0.66 0.36 0.39 0.42 0.74 0.72 0.33 0.71 0.40 0.35
20news0 0.51 0.49 0.57 0.60 0.48 0.46 0.47 0.62 0.69 0.50 0.61 0.36 0.49
20news1 0.36 0.39 0.37 0.44 0.37 0.39 0.37 0.44 0.47 0.39 0.44 0.33 0.39
20news2 0.34 0.36 0.34 0.39 0.34 0.36 0.35 0.36 0.38 0.36 0.34 0.32 0.38
20news3 0.65 0.61 0.73 0.71 0.63 0.64 0.64 0.69 0.58 0.62 0.59 0.54 0.45
20news4 0.28 0.31 0.27 0.39 0.27 0.32 0.30 0.30 0.34 0.31 0.35 0.35 0.30
20news5 0.34 0.32 0.29 0.30 0.30 0.31 0.32 0.29 0.35 0.32 0.28 0.31 0.35
Lymphography 0.95 0.95 0.95 0.83 0.95 0.95 0.95 0.45 0.81 0.96 0.62 0.25 0.62
magic.gamma 0.70 0.57 0.72 0.63 0.65 0.60 0.68 0.79 0.65 0.64 0.72 0.56 0.55
musk 0.57 0.65 0.22 0.21 0.83 0.85 0.58 0.53 0.22 0.32 0.59 0.42 0.64
PageBlocks 0.84 0.83 0.79 0.67 0.82 0.86 0.74 0.76 0.70 0.85 0.71 0.52 0.84
pendigits 0.87 0.86 0.82 0.33 0.82 0.85 0.85 0.60 0.35 0.86 0.53 0.29 0.52
Pima 0.63 0.57 0.62 0.51 0.59 0.54 0.62 0.64 0.55 0.61 0.52 0.40 0.45
annthyroid 0.76 0.49 0.55 0.59 0.70 0.72 0.53 0.65 0.63 0.59 0.71 0.71 0.49
satellite 0.67 0.55 0.67 0.50 0.59 0.54 0.71 0.61 0.51 0.56 0.59 0.50 0.58
satimage-2 0.51 0.71 0.45 0.41 0.74 0.80 0.77 0.79 0.34 0.70 0.68 0.31 0.84
shuttle 0.94 0.87 0.74 0.46 0.89 0.94 0.94 0.65 0.52 0.85 0.63 0.42 0.91
smtp 0.81 0.60 0.58 0.59 0.58 0.68 0.45 0.76 0.48 0.72 0.32 0.60 0.60
speech 0.31 0.31 0.31 0.37 0.34 0.32 0.31 0.31 0.33 0.32 0.35 0.34 0.34
Stamps 0.84 0.75 0.57 0.42 0.87 0.80 0.83 0.57 0.39 0.85 0.62 0.42 0.81
thyroid 0.90 0.77 0.86 0.81 0.86 0.90 0.92 0.87 0.77 0.88 0.84 0.33 0.69
vertebral 0.23 0.26 0.28 0.37 0.14 0.29 0.17 0.20 0.37 0.25 0.29 0.24 0.40
vowels 0.57 0.45 0.73 0.76 0.35 0.45 0.54 0.77 0.74 0.49 0.75 0.35 0.43
Waveform 0.56 0.42 0.59 0.58 0.60 0.47 0.53 0.59 0.59 0.50 0.54 0.39 0.34
WDBC 0.95 0.95 0.95 0.91 0.95 0.92 0.96 0.86 0.81 0.95 0.85 0.50 0.67
Wilt 0.29 0.20 0.21 0.37 0.20 0.24 0.19 0.35 0.38 0.12 0.42 0.34 0.26
wine 0.70 0.63 0.12 0.31 0.80 0.67 0.84 0.33 0.26 0.75 0.32 0.48 0.48
WPBC 0.33 0.32 0.32 0.33 0.36 0.33 0.38 0.32 0.29 0.33 0.39 0.38 0.35
yeast 0.29 0.31 0.35 0.35 0.28 0.34 0.31 0.30 0.36 0.31 0.32 0.38 0.31
campaign 0.68 0.62 0.59 0.52 0.74 0.72 0.75 0.68 0.53 0.68 0.64 0.48 0.52
cardio 0.84 0.85 0.80 0.60 0.83 0.84 0.75 0.67 0.53 0.86 0.62 0.47 0.64
Cardiotocography 0.59 0.70 0.57 0.45 0.58 0.70 0.52 0.48 0.51 0.66 0.43 0.45 0.53
celeba 0.64 0.65 0.69 0.30 0.70 0.71 0.71 0.28 0.30 0.74 0.38 0.48 0.37
CIFAR100 0.63 0.59 0.60 0.60 0.59 0.60 0.60 0.65 0.64 0.61 0.61 0.46 0.42
CIFAR101 0.43 0.48 0.50 0.52 0.35 0.39 0.33 0.49 0.62 0.49 0.51 0.39 0.48
CIFAR102 0.45 0.48 0.47 0.50 0.44 0.45 0.43 0.49 0.55 0.47 0.48 0.48 0.40
CIFAR103 0.44 0.48 0.49 0.46 0.40 0.42 0.39 0.47 0.50 0.46 0.46 0.49 0.45
CIFAR105 0.38 0.48 0.47 0.46 0.35 0.40 0.34 0.42 0.49 0.46 0.42 0.34 0.49
CIFAR106 0.54 0.55 0.58 0.59 0.54 0.55 0.54 0.61 0.62 0.58 0.59 0.47 0.39
CIFAR107 0.43 0.48 0.45 0.46 0.41 0.44 0.39 0.44 0.50 0.46 0.46 0.51 0.50
agnews0 0.41 0.39 0.45 0.53 0.40 0.38 0.39 0.49 0.56 0.39 0.48 0.27 0.40
agnews1 0.50 0.46 0.50 0.64 0.42 0.45 0.46 0.54 0.67 0.47 0.53 0.28 0.48
agnews2 0.57 0.53 0.63 0.66 0.52 0.51 0.52 0.68 0.73 0.53 0.66 0.42 0.45
agnews3 0.45 0.46 0.49 0.63 0.43 0.44 0.43 0.54 0.64 0.46 0.53 0.42 0.42
amazon 0.48 0.45 0.50 0.49 0.48 0.46 0.47 0.50 0.48 0.46 0.50 0.37 0.43
imdb 0.41 0.36 0.41 0.40 0.42 0.36 0.40 0.39 0.40 0.37 0.41 0.44 0.34
yelp 0.52 0.50 0.55 0.60 0.52 0.49 0.51 0.60 0.59 0.51 0.58 0.42 0.47
Table7. FullresultsforADBench:conformalAUROC.
17ConformalOOD
IForest OCSVM CBLOF COF COPOD ECOD HBOS KNN LOF PCA SOD DeepSVDD DAGMM
cover 0.12 0.10 0.11 0.13 0.11 0.10 0.07 0.12 0.12 0.10 0.14 0.15 0.11
donors 0.06 0.07 0.08 0.07 0.05 0.04 0.06 0.06 0.06 0.05 0.06 0.05 0.06
fault 0.09 0.08 0.09 0.09 0.09 0.09 0.09 0.08 0.09 0.09 0.09 0.08 0.09
fraud 0.13 0.28 0.25 0.14 0.26 0.25 0.04 0.26 0.16 0.25 0.26 0.22 0.26
glass 0.12 0.05 0.10 0.12 0.12 0.13 0.13 0.10 0.11 0.13 0.10 0.09 0.10
Hepatitis 0.16 0.15 0.14 0.18 0.12 0.14 0.13 0.27 0.16 0.14 0.14 0.14 0.14
Ionosphere 0.08 0.08 0.06 0.07 0.09 0.10 0.12 0.05 0.05 0.08 0.06 0.13 0.09
landsat 0.06 0.06 0.05 0.05 0.06 0.06 0.06 0.06 0.05 0.06 0.05 0.05 0.05
ALOI 0.07 0.10 0.10 0.09 0.10 0.10 0.06 0.09 0.08 0.10 0.09 0.10 0.10
letter 0.17 0.16 0.15 0.14 0.19 0.17 0.18 0.13 0.13 0.17 0.13 0.15 0.15
20news0 0.14 0.14 0.13 0.11 0.14 0.15 0.14 0.11 0.11 0.14 0.12 0.13 0.14
20news1 0.15 0.15 0.15 0.14 0.15 0.15 0.16 0.13 0.14 0.15 0.14 0.14 0.15
20news2 0.16 0.15 0.14 0.15 0.16 0.16 0.16 0.15 0.16 0.15 0.15 0.16 0.15
20news3 0.10 0.11 0.10 0.10 0.11 0.11 0.10 0.10 0.13 0.11 0.12 0.13 0.09
20news4 0.20 0.20 0.17 0.18 0.21 0.19 0.20 0.18 0.16 0.20 0.17 0.18 0.18
20news5 0.17 0.17 0.18 0.20 0.18 0.15 0.17 0.19 0.20 0.16 0.20 0.18 0.19
Lymphography 0.05 0.05 0.05 0.08 0.04 0.05 0.05 0.11 0.09 0.04 0.11 0.09 0.11
magic.gamma 0.03 0.04 0.03 0.04 0.03 0.04 0.03 0.03 0.04 0.03 0.03 0.04 0.04
musk 0.43 0.15 0.78 0.18 0.11 0.11 0.42 0.17 0.19 0.68 0.16 0.14 0.13
PageBlocks 0.06 0.06 0.06 0.06 0.06 0.06 0.07 0.06 0.06 0.06 0.07 0.06 0.05
pendigits 0.08 0.08 0.09 0.12 0.08 0.08 0.08 0.13 0.13 0.08 0.13 0.13 0.12
Pima 0.10 0.10 0.10 0.10 0.10 0.09 0.09 0.10 0.10 0.09 0.10 0.11 0.11
annthyroid 0.06 0.08 0.07 0.07 0.07 0.06 0.07 0.07 0.07 0.07 0.06 0.06 0.08
satellite 0.04 0.04 0.04 0.05 0.04 0.04 0.04 0.05 0.05 0.04 0.05 0.05 0.04
satimage-2 0.48 0.26 0.55 0.16 0.23 0.16 0.21 0.14 0.13 0.27 0.15 0.18 0.12
shuttle 0.06 0.10 0.09 0.06 0.10 0.05 0.05 0.04 0.05 0.13 0.06 0.07 0.07
smtp 0.05 0.12 0.12 0.10 0.12 0.11 0.10 0.09 0.09 0.11 0.08 0.12 0.11
speech 0.19 0.19 0.19 0.19 0.19 0.19 0.19 0.20 0.19 0.19 0.21 0.21 0.18
Stamps 0.07 0.09 0.11 0.11 0.06 0.08 0.07 0.12 0.12 0.07 0.11 0.14 0.07
thyroid 0.08 0.10 0.09 0.10 0.08 0.08 0.04 0.09 0.10 0.08 0.09 0.16 0.11
vertebral 0.14 0.12 0.13 0.12 0.12 0.12 0.12 0.14 0.13 0.12 0.11 0.12 0.13
vowels 0.19 0.18 0.17 0.20 0.20 0.17 0.20 0.20 0.19 0.18 0.17 0.21 0.19
Waveform 0.15 0.15 0.14 0.15 0.15 0.15 0.15 0.14 0.14 0.16 0.15 0.16 0.16
WDBC 0.04 0.04 0.04 0.06 0.04 0.05 0.04 0.06 0.08 0.04 0.07 0.12 0.10
Wilt 0.13 0.11 0.12 0.12 0.13 0.12 0.14 0.13 0.13 0.08 0.12 0.12 0.11
wine 0.10 0.10 0.14 0.14 0.09 0.11 0.08 0.12 0.11 0.10 0.15 0.11 0.13
WPBC 0.13 0.13 0.13 0.13 0.13 0.13 0.14 0.14 0.12 0.13 0.12 0.12 0.13
yeast 0.09 0.10 0.10 0.10 0.09 0.09 0.09 0.09 0.10 0.10 0.11 0.10 0.10
campaign 0.05 0.05 0.05 0.06 0.05 0.05 0.04 0.05 0.06 0.05 0.06 0.05 0.06
cardio 0.09 0.09 0.10 0.12 0.09 0.09 0.10 0.10 0.14 0.10 0.11 0.11 0.11
Cardiotocography 0.09 0.08 0.08 0.09 0.09 0.08 0.08 0.08 0.09 0.09 0.08 0.08 0.09
celeba 0.06 0.06 0.05 0.09 0.05 0.05 0.05 0.31 0.09 0.05 0.10 0.06 0.08
CIFAR100 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.11
CIFAR101 0.12 0.11 0.11 0.11 0.11 0.12 0.11 0.11 0.10 0.11 0.10 0.10 0.11
CIFAR102 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.10 0.11
CIFAR103 0.11 0.11 0.10 0.10 0.11 0.11 0.11 0.10 0.10 0.11 0.10 0.11 0.11
CIFAR105 0.12 0.10 0.11 0.11 0.12 0.12 0.12 0.12 0.11 0.11 0.12 0.12 0.10
CIFAR106 0.11 0.10 0.11 0.10 0.11 0.11 0.11 0.10 0.10 0.10 0.10 0.10 0.11
CIFAR107 0.11 0.11 0.10 0.10 0.11 0.11 0.11 0.10 0.10 0.10 0.10 0.11 0.11
agnews0 0.09 0.09 0.08 0.08 0.09 0.09 0.09 0.08 0.08 0.09 0.08 0.09 0.09
agnews1 0.09 0.09 0.08 0.07 0.09 0.09 0.09 0.08 0.07 0.09 0.08 0.09 0.09
agnews2 0.08 0.08 0.08 0.07 0.08 0.09 0.08 0.07 0.06 0.08 0.07 0.08 0.08
agnews3 0.09 0.09 0.08 0.07 0.09 0.09 0.09 0.08 0.07 0.09 0.08 0.08 0.08
amazon 0.08 0.08 0.08 0.08 0.08 0.08 0.08 0.09 0.09 0.08 0.09 0.08 0.08
imdb 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.08 0.08
yelp 0.08 0.08 0.08 0.08 0.08 0.09 0.08 0.08 0.08 0.08 0.08 0.08 0.08
Table8. FullresultsforADBench:AUROCcorrection(differencebetweenconformalandclassicalAUROC).
18