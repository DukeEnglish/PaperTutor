Urban Scene Diffusion through Semantic
Occupancy Map
Junge Zhang1,4 ‚ãÜ Qihang Zhang2 Li Zhang1 Ramana Rao Kompella3
Gaowen Liu3 Bolei Zhou4
1Fudan University 2The Chinese University of Hong Kong 3Cisco
4University of California, Los Angeles
Abstract. Generating unbounded 3D scenes is crucial for large-scale
scene understanding and simulation. Urban scenes, unlike natural land-
scapes,consistofvariouscomplexman-madeobjectsandstructuressuch
as roads, traffic signs, vehicles, and buildings. To create a realistic and
detailed urban scene, it is crucial to accurately represent the geome-
try and semantics of the underlying objects, going beyond their visual
appearance. In this work, we propose UrbanDiffusion, a 3D diffusion
model that is conditioned on a Bird‚Äôs-Eye View (BEV) map and gener-
atesanurbanscenewithgeometryandsemanticsintheformofsemantic
occupancymap.Ourmodelintroducesanovelparadigmthatlearnsthe
data distribution of scene-level structures within a latent space and fur-
ther enables the expansion of the synthesized scene into an arbitrary
scale. After training on real-world driving datasets, our model can gen-
erateawiderangeofdiverseurbanscenesgiventheBEVmapsfromthe
held-out set and also generalize to the synthesized maps from a driving
simulator. We further demonstrate its application to scene image syn-
thesiswithapretrainedimagegeneratorasaprior.Theprojectwebsite
is https://metadriverse.github.io/urbandiff/.
1 Introduction
Significant progress has been made in image generation and 3D-aware object
generation [6,31‚Äì33,36]. Most of the works focus on generating individual ob-
jects or scenes in a single image. However, large-scale urban scene generation,
with its application to interactive scene simulation and autonomous driving,
remains much less explored. Different from natural scene generation [5,7,27],
generatingurbanscenesrequirescomposingaccurategeometryandsemanticsof
the man-made objects and structures such as roads, lanes, buildings, and traffic
signs. Many recent attempts have achieved success in generating realistic visual
appearances of urban scenes at the pixel level, including BEVGen [42], BEV-
Control[49]andMagicDrive[10],aswellasthosevideogenerationmethodssuch
as StyleSV [54], GAIA-1 [16] and Drivedreamer [46].
1 The work was done when Junge Zhang was a visiting graduate student at UCLA.
‚ãÜ
4202
raM
81
]VC.sc[
1v79611.3042:viXra2 Zhang et al.
Scene Generation
Scene Expansion
50m
Fig.1: Diverseindividualscenesandlarge-scalescenesgeneratedbyUrbanDiffusion.
A scene is represented by the semantic occupancy map and the color labels indicate
different semantic categories. The input BEV layout is also attached as a reference.
Despitethevisuallyappealingresultsproducedbytheaforementionedmeth-
ods, a fundamental limitation remains - the lack of accurate geometry and se-
mantic information for the generated scene structure. As a result, it becomes
challenging to manipulate the camera pose and viewpoint of the generated im-
ages,aswellastransformthevisualsceneintoaninteractivesimulationenviron-
ment.RecentworkslikeDiscoScene [48]andUrbanGIRAFFE[50]haveachieved
controllable3D-awareimagegenerationviamanipulatingthegivenscenelayout,
revealing the importance of grounding image generation in the underlying 3D
structure for scene generation tasks.
Sincea3Dscenecontainsamuchricherstructurethana2Dimagesnapshot,
representing 3D scenes beyond the pixels is essential for large-scale scene gener-
ation. In recent two years, the semantic occupancy map has become an effective
scene representation that incorporates both geometric structure and semantics,
leadingtomuchprogressinthe3Dperceptionofautonomousdriving[22,43,47].
Occupancymapasscenerepresentationallowsthedrivingsystemtoextractnec-
essary scene information for planning and navigation [17,44]. Therefore, gener-
ating3D scenesin termsof occupancymaps cannot onlyincorporategeometric
structure and semantics but also facilitate the future integration of the gener-
atedscenesfordownstreamtaskssuchasinteractiveenvironmentsimulationand
scene image rendering [50].
Inthiswork,weproposeUrbanDiffusionthatconsidersoccupancymapsas
thescenerepresentationandlearnstogeneratelarge-scaleurbanscenesthrough
a 3D diffusion model. Fig. 1 shows diverse single-frame scenes and large-scale
scenesgeneratedbytheproposedmethod.UrbanDiffusiontakestheBirds‚ÄôEye
View(BEV)layoutastheinputcondition,whichdescribesatrafficscenario,and
then generates the corresponding 3D semantic occupancy maps as the output.
We first embed the collected semantic occupancy map from the real-world
datasets and learn a denoiser in the latent space through the diffusion process.
We then design an extensible representation to aggregate multiple frames for
large-scale scene generation. The experiments show that the proposed modelUrban Scene Diffusion through Semantic Occupancy Map 3
can generate diverse and realistic urban scenes from complex BEV layouts sam-
pled from the validation set of the NuScenes dataset [2] and generalize to the
BEV layouts synthesized by a driving simulator MetaDrive [21]. We further
demonstrate that the trained model can work as an effective generative prior
fordownstreamtaskslikescenecompletion,sceneout-painting,andsceneimage
rendering.
We summarize our contributions as follows: (i) We introduce a new task
of generating unbounded urban scenes at the occupancy map level to preserve
the scene geometryand semantic information. (ii) Wepropose a novel 3Ddiffu-
sion model conditioned on BEV map, which can generate large-scale 3D urban
scenes through semantic voxels with temporal consistency. (iii) After training
the proposed model can generate diverse and realistic urban scenes based on
theinputBEVconditionandbenefitsdownstreamgenerationtasksuchaspoint
cloud segmentation and scene image synthesis.
2 Related Work
Unbounded Scene Generation Many image generation works have explored
generating 2D images with infinite resolution [25,37]. Other works have aimed
tosynthesizenaturalsceneswithtemporalconsistency,includingcamera-flytra-
jectories, by applying geometric constraints during the generation of large-scale
natural scenes [4,5,7,23,27,34]. However, urban scenes are very different from
naturalscenesbecausetherearemanyman-madeobjectslikeroads,lanes,build-
ings,andtrafficsigns,whichareinrigidstructureandarrangement.Generating
realistic urban scenes requires accurate geometry and semantic information for
the underlying scene structure. Another line of works explores generating 3D
scenesfromascenelayout.Notableworksinthisdomain,suchasGANCraft[12]
andUrbanGIRAFFE[50],havedemonstratedthecapabilitytomanipulatecam-
era poses for rendering novel view images. However, it involves the prerequisite
ofa3Dlayout,aconditionthatcanbechallengingtofulfillinreal-worldscenes.
Recent works Infinicity [26] and SGAM [35] create 3D scenes by generating
satelliteimagesandthenmappingtheminto3Dworlds.BerfScene[53]proposes
equivarient scene representation that conditions on BEV maps for unbounded
scene generation. While these previous studies have offered valuable perspec-
tives on utilizing satellite images to reconstruct scene structures, the task of ac-
curately recovering intricate and slender structures, such as poles, traffic signs,
andotherman-madefeaturesatstreet-level,solelyfromasatelliteview,remains
a formidable challenge. Furthermore, the ability to easily control the generated
results to fit the requirements of various simulations is a critical need for the
generation process. Our proposed method allows user to generate and expand
urban scenes using the easily accessible Bird‚Äôs Eye View (BEV) maps as input
condition.
3D Generation Generating 3D data has been a hot topic in recent years [8,9,
20,29,51,56]. Some works try to learn 3D data distribution directly. PVD [56],
DPM [29], and LION [51] learn point diffusion, while SDF fusion works [8,8,9]4 Zhang et al.
learndiffusioninSDFspace.OtherworkslikeDreamfusion[32]andMagic3D[24]
recover potential 3D structures from image space with the prior of the genera-
tive model. However, these prior works are restricted to object-level generation.
Some other works have attempted to perform scene-level generation, including
indoor scene generation [1,15,39,52] and outdoor scene generation [19]. The
mostrelatedwork,NeuralField-LDM[19],hasattemptedtogenerate3Dfeature
volume from image space through a diffusion process. Nevertheless, as demon-
strated in their paper, the complexity and ambiguity of feature volume prevent
the method from depicting good geometry. The generation is also constrained
within a defined range, a limitation particularly evident when attempting to
generate large-scale scenes.
3 Method
Stage 1: 3D VQVAE
ùíõùüé Codebook ùì©
‚ãÖ‚ãÖ‚ãÖ üî•
3D CNN 3D CNN
Encoder Decoder
üî• üî•
Stage 2: BEV-conditioned denoising ‚àºùìù(ùëß%$; ùõº$ùëß%,1‚àíùõº$ùêº)
BEV 3 DD e cC oN deN r‚ùÑ
Attn Attn
ùê∏!"# ‚äó
√ó ùëª
3DU-Net Denoiser ùùêùúΩ
Fig.2: Framework of UrbanDiffusion. An autoencoder with 3D VQVAE architec-
tureistrainedtoembedsemanticoccupancymapsintoalatentspace(top).Arandom
latent code is gradually diffused by a BEV-conditioned denoising procedure and then
decoded into a semantic occupancy map (bottom).
UrbanDiffusion consists of two key components for large-scale urban scene
generation: (1) BEV-conditioned 3D semantic occupancy map generation, and
(2) scene extension for unbounded generation. Sec. 3.1 briefly introduces the
diffusion process and guidance. Sec. 3.2 then presents the design of a diffusion
modelthatincorporatestheBEVmapasaconditioningfeatureandgenerates3D
semanticoccupancymaps.Sec.3.3finallygivesthedetailofthesceneextension,
where we leverage the capability of the trained model to expand a single local
3D scene to a large-scale scene.
3.1 Preliminary
DiffusionprocessDiffusionprocess[13,40],whichisdefinedasaMarkovchain,
can be divided into a forward process and a reverse process. In the forward pro-
cess,thegoalistograduallyaddnoisetotheinputdatax suchthatitbecomes
0
a random sample from a Gaussian distribution. The probability distribution ofUrban Scene Diffusion through Semantic Occupancy Map 5
theforwardprocessisdeterminedbyŒ± ,whichisaseriesofparameterstoensure
t
the sequence converges to the Gaussian distribution:
‚àö ‚àö ‚àö
xÀÜ = Œ± xÀÜ + 1‚àíŒ± œµ‚àó ‚àºN(xÀÜ ; Œ±¬Ø x ,(1‚àíŒ±¬Ø )I), (1)
t t t‚àí1 t t‚àí1 t t 0 t
whenŒ±¬Ø
=(cid:81)t
Œ± .Theoptimizationobjectistopredictthenoiseœµappliedto
t s=1 s
the input x with the denoising model œµ , i.e.,
0 Œ∏
E ‚à•œµ‚àíœµ (x ,t)‚à•, (2)
œµ,t Œ∏ t
in a uniform sampled interval t‚àà[0,1].
Classifier-free Guidance The classifier-free guidance [14] allows the model
to generate samples from a conditional distribution with high diversity. Given
condition c, the conditional probability distribution can be denoted as:
œµÀú (x ,c)=(1+w)œµ (x ,c)‚àíwœµ (x ).
Œ∏ t Œ∏ t Œ∏ t
3.2 Latent Diffusion for Semantic Occupancy Map
We utilize 3D semantic data represented as x ‚àà {0,1}H√óW√óZ√óC, where H,
W, and Z denote the length, width, and height of the voxel grids, respectively,
and C represents the number of semantic label categories. Inspired by the la-
tent diffusion model (LDM) [33], we aim to train the diffusion model in a fast
andmemory-efficientmanner,consideringthehighmemorycostassociatedwith
representing 3D data. To achieve this, we embed the 3D semantic data x into
a lower-dimensional latent space, reducing memory usage and computational
requirements. The LDM then utilizes this latent representation to conduct the
diffusion process with a classifier-free guidance.
LatentrepresentationToembedthe3Dsemanticdataintoalower-dimensional
space, we adopt VQVAE [45] with 3D convolution operators to regularize the
real-world data that are collected from the noisy sensors. The goal is to embed
the data into a latent space while maintaining the data geometry and semantics
andimprovingtheefficiencyofdiffusion.Wedenotetheoperatoroftheencoder,
the vector quantization, and the decoder respectively as E, Q, and D. Z is the
correspondingcodebookforthevectorquantization.ThetraininglossofVQVAE
is:
L =‚à•x‚àíxÀú‚à•+‚à•sg[E(x)]‚àíz ‚à•2+Œ≤‚à•sg[z ]‚àíE(x)‚à•2, (3)
VQVAE q 2 q 2
where sg[¬∑] denotes stopping gradient operation in VQVAE and
z =Q(E(x)):=argmin‚à•z ‚àíE(x)‚à•,xÀú =D(Q(E(x))), (4)
q k
zk‚ààZ
where z denotes the latent from the codebook closest to the embedded vector
q
E(z) and xÀú means the reconstructed vector. To better align the attribute of
semanticlogitsspace,wetakecrossentropyasthereconstructionlossbetweenx
andxÀú.Thequantizationlossregularizesthepotentialnoiseofthedata,benefiting
the following diffusion process. Later we conduct an ablation study for a proper
choice of the hyper-parameters for the autoencoder in Tab. 2.6 Zhang et al.
Diffusion processToaccommodatethecomplexityofthe3Ddataandcapture
the spatial feature of the 3D structure, we adopt a standard 3D U-Net œµ to
Œ∏
predict the noise for our 3D diffusion model. For the given semantic occupancy
map x , we denote the learning process of the diffusion model as:
0
(cid:104) (cid:105) ‚àö
E ‚à•œµÀÜ (z )‚àíœµ‚à•2 , z = Œ±¬Ø E(x )+(1‚àíŒ±¬Ø )I. (5)
œµ,t Œ∏ t 2 t t 0 t
Incorporating condition into the diffusion processBEVlayout,whichde-
scribesthetrafficactivitiesandsurroundings,iscrucialtotheurbanscenegener-
ation.Italsoallowstheusertocontrol3Dscenegeneration.Takingintoaccount
the easy accessibility of BEV maps from current simulator like MetaDrive [21]
andrecognizingtheirsignificanceingeneratingrealisticurbanenvironments,we
proposetheincorporationofBEVmapsasaguidingconditioninourgeneration
process, enhancing the quality and practicality of the generated content. To fig-
ureouthowtoeffectivelyinjecttheconditionalinformationofBEVmaps,weex-
plore several different conditioning strategies: Cross-attention, Modulation, and
Concatenate.Withtheconditionembeddingfroma2DBEVmapb‚ààRH√óW√óC,
c =E (b)‚ààRh√ów√óc, the results of different ways of injecting conditions in
bev bev
Fig. 3 into the generation model are shown in Tab. 3. We find that concatenat-
ing the 2D layout BEV feature leads to better control for the large-scale scene
generation shown in Fig. 7, which implies better feature alignment.
1. Cross-attention
ùëßùíèùíêùíäùíîùíÜ
Attn Attn
2. Modulation
Attn Attn
ùê∏!"#
3. Concat
Attn Attn
cat
broadcasting √ó ùëª
Fig.3: Different ways of BEV condition injection.
BEV-conditional scene generation Given a BEV layout, the trained model
can generate diverse and realistic samples that contain the scene geometry and
semantic information. We adopt classifier-free guidance [14] to better conduct
the conditional sampling. The noise prediction could be formulated as:
œµÀÜ (z |c )=œµ (z |œï)+w¬∑(œµ (z |c )‚àíœµ (z |œï)). (6)
Œ∏ t bev Œ∏ t Œ∏ t bev Œ∏ t
œµ (z |œï) means the prediction of unconditional generation.
Œ∏ tUrban Scene Diffusion through Semantic Occupancy Map 7
3.3 Scene Extension Module
It is intractable for a generative model to have a single-shot generation for a
large-scale complicated 3D scene. We thus follow a divide-and-conquer strategy
totacklelarge-scalescenegenerationbydesigningasceneextensionmodule.We
firstdividealarge-scalesceneintoseveralsingle-framepartsandthenseparately
generate the whole representation. After that, we have a scene extension step
to extend and aggregate several single-frame scenes together. It is crucial to
maintain the temporal consistency for the whole scene representation during
the generation to ensure smooth expansion. The process of scene expansion is
illustratedinFig.4. GiventheobservationG ofthesceneattimet,weformulate
t
Sample ùë•/ ùë•*+,-". ùê∫/
ùë•$%&‚Äô"(=ùë∑(ùë•),ùëÉ),ùëÉ)*+) E keep updategenerate
ùëß*+,-". Merge
Time ùë° Time ùë°+1 ‚äó
ùê∏!"# ‚äó Sampling
ùëê!"#
Sample ùë•/01 ùê∫/01=ùêåùêûùê´ùê†ùêû(ùê∫/,ùë•/01)
Fig.4: Illustration of the scene expansion.Afterprojectingthegeneratedsample
x to the next frame via the ego poses P and P at time t and t+1, and the
t t t+1
BEV maps, we could get the overlap part and further encode both the x and
masked
BEV to guide the generation process for the output sample x with high temporal
t+1
consistency. Finally, we merge the sample x into the global scene G with ‚Äôkeep‚Äô
t+1 t
for the original scene , ‚Äôupdate‚Äô for the intersection part by re-registering the labels of
occupancy grids and ‚Äôgenerate‚Äô for the new part.
the generation x at the time t as:
t
x ‚àºP(¬∑|G ), G =Merge(G ,x ). (7)
t+1 t t+1 t t+1
The overlap between the current observation and the generated part is denoted
as:
x =P(x ,P ,P ), (8)
masked t t t+1
where P means the projection through the current pose P and the previous
t+1
poseP .Onenaiveout-paintingwayistodirectlyutilizethegenerativecapability
t
of the diffusion model [28]. Given a fixed mask M ‚àà {0,1}h√ów√ód, the painting
process becomes:
z =(1‚àíM)‚äôzknown+M ‚äôzunknown, (9)
t‚àí1 t‚àí1 t‚àí1
‚àö
wherezknownissampledfromthedistributionN( Œ±¬Ø z ,(1‚àíŒ±¬Ø )I)andzunknown
t‚àí1 t 0 t t‚àí1
is sampled from distribution N(¬µ (z ,t,c),Œ£ (z ,t,c)). However, we found that
Œ∏ t Œ∏ t
duringtheprogressivegeneration,itcanaccumulateerrorsandgenerateartifact8 Zhang et al.
results. So we take the mask image as another condition and fine-tune the pre-
traineddiffusionmodelŒ∏ toallowourmodeltogenerateabetterresultforscene
expansion. The target distribution is formulated as:
E ‚à•œµ‚àíœµ (z ,t,c ,c )‚à•. (10)
z,t,œµ‚àºN(0,I) Œ∏‚Ä≤ t bev mask
The progressive generation process thus can extend a single scene into a large-
scale scene seamlessly in an unbounded way.
3.4 Scene Image Synthesis
The proposed 3D scene diffusion model not only generates diverse and realistic
scenesbasedontheinputconditionbutalsocanbeusedasagenerativepriorfor
many downstream applications. As follows, we describe how the learned model
canbeincorporatedwithpre-trainedimagegeneratorforsceneimagegeneration.
Recently, there are many attempts at generating 3D scenes conditioned on
3D voxels [7,12,26]. These works have shown that explicit 3D structure can
work effectively as a geometry prior to benefit 3D-aware scene image synthesis.
Following this line of works, we also showcase the utility of semantic occupancy
gridsgeneratedby UrbanDiffusionforsceneimagesynthesis.Thesegridsserve
as an additional source of information, enabling improved synthesis of scene
images.
To generate a scene image based on the generated semantic occupancy grid,
our approach involves several steps. Firstly, we position a camera inside each
scene. Next, we calculate the grids that intersect with the rays originating from
the camera, and assign appropriate encodings to these grids. The encoding con-
sists of two components: positional encoding of the grid‚Äôs world coordinate, and
semantic embedding. To determine the colors, we employ an MLP that decodes
the encoding information.
To optimize our scene synthesis, we adopt the Score Distillation Sampling
(SDS) technique, as introduced in DreamFusion [32]. This involves distilling
knowledge from a 2D diffusion model to refine our scene generation process.
Additionally,weachievecontrollablegenerationbyfine-tuningadiffusionmodel
that is conditioned on semantic and depth maps. To accomplish this, we gather
a small set of driving images paired with perspective view images, semantic
maps, and depth maps. Using this dataset, we fine-tune a pre-trained diffusion
model[33]toimprovethequalityandcontrollabilityofourscenesynthesis.More
details could be found in the supplementary.
4 Experiments
Dataset We evaluate our proposed method on the nuScenes Dataset [2] that
containsmulti-sensorydata,High-precisionmaps,and3Dannotatedlabels.Two
important data resources in our methods are 3D semantic occupancy grids with
2D synchronized BEV maps. We construct BEV maps following the instructionUrban Scene Diffusion through Semantic Occupancy Map 9
of [55] by projecting the annotated objects onto the 2D HD maps and then
transferring all the representations to the ego-frame coordinate system. To get
3Dsemanticoccupancygridsforthewholescenes,wenoticethattherearesome
available datasets [43,44,47] aimed at occupancy prediction, which provides
occupancy grids with annotated labels, achieved by accumulating all the col-
lected LiDAR points, building meshes from these points and assigning semantic
labels through calculating chamfer distance to the unlabeled points. The data
of semantic occupancy map are collected from the dataset [43], which allows us
to train and evaluate our generative model.
Implementation details At the training stage of the 3D VQVAE, we apply
weighted cross entropy loss on the raw data and update the model weights in
the mode of Exponential Moving Average. We crop the input data to the size
of 192√ó192√ó16 at the center and compress the data to a latent space with a
dimension of 48√ó48√ó4. For the BEV map, an encoder is trained with unet
to encode the BEV representation at the same resolution as the latent feature
in the horizontal dimension. The diffusion model œµ is then trained in the latent
Œ∏
spacefor100epochs.Another50epochsareutilizedtofine-tunethemodelœµ to
Œ∏
getmodelweightsofcompletionmodelœµ‚Ä≤.WeadoptDDIM[38]tofacilitatethe
Œ∏
sampling stage with 100 steps. We use the L norm for all distance calculations
2
in noise prediction. Detailed ablation is shown in the supplementary.
4.1 BEV-conditional Generation
TheBEVmap,whichdescribesatrafficsituationataparticularlocation,isread-
ilyavailableinmanydrivingsimulatorssuchasMetaDrive[21]andWaymax[11].
With the designed model and pipeline, we get easily generate the corresponding
urban scene with geometry and semantic information via the BEV map.
Fig. 5 shows several examples of the large-scale scenes generated based on
the BEV maps randomly selected from the nuScenes validation set, the BEV
maps synthesized from a driving simulator MetaDrive [21] and the BEV maps
from Waymo Motion Dataset [41]. By randomly sampling the model multiple
times, we can also generate different diverse scenes from the same BEV map, as
shown in Fig. 5d.
ItisinterestingtoseethateventhoughwithoutthedatafromMetaDrive[21]
for training, the trained model can generalize to the fake BEV map synthesized
bythedrivingsimulatorandgeneraterealisticsceneaccordingly.Itdemonstrates
that the proposed UrbanDiffusion is applicable to create new driving environ-
ments for interactive simulation. In particular, the generated scene already con-
tainsgeometryandsemantics,whichiseasytoincorporateinadrivingsimulator
like MetaDrive.
4.2 Quantitative Evaluation
We build several evaluation metrics for our methods and baselines, since the
taskweproposetoworkonhasnotbeenwidelyexploredbefore.Theevaluation10 Zhang et al.
(a)
(b)
(c)
(d)
(e)
(f)
(g)
Fig.5: Scenes generated from BEV maps sampled from nuScenes validation set (a),
from Waymo Motion Dataset [41](b), procedurally generated by MetaDrive simula-
tor [21] (c), and from nuPlan Dataset [3]. Large-scale scenes are generated from bev
mapsextractedfromdrivinglogsinnuScenes(e)andMetaDrviesimulator(f).Wealso
demonstrate diverse scenes generated conditioned on same BEV maps in (g).Urban Scene Diffusion through Semantic Occupancy Map 11
metrics consider the data quality, the diversity of the generated data, and the
consistency to the input condition.
‚Äì Quality To measure the data distribution distance, like P-FID in [30],
we individually train a specific network to get latent representation for all
the scenes. Based on this, we could calculate a voxel-wise Fr√©chet Inception
Distance, namely V-FID to measure the distribution distance between the
generated data and the real data. We utilize an autoencoder network to
compress the scene representation into a lower dimensional space and then
calculate the distance of distribution in this compressed space. This metric
reflectsbothgeometryandsemanticsqualityinthefeaturespace.Wealsoin-
cludeMaximumMeanDiscrepancy(MMD),whichisanon-parametric
distance between two sets of samples. It compares the distance between two
setsofsamplesbymeasuringthemeansquareddifferenceofthestatisticsof
the two.
‚Äì Accuracy We calculate the Intersection over Union (IoU) to determine
whether a voxel is occupied or not, and the mean Intersection over Union
(mIoU) to handle different categories of data. The mIoU includes all the 16
categories from the dataset [43].
‚Äì ConditionConsistencyItisessentialtoassessiftheconditionedgenerated
samples are in alignment with the specified condition. We achieve this by
extracting the height of the ground level from the generated voxel. The
accuracy is then calculated relative to the given Bird‚Äôs Eye View (BEV)
map. This process enables us to determine the degree of alignment between
the generated samples and the provided condition.
We‚Äôve gathered a variety of related methods as baselines for comparison. Our
Bird‚Äôs Eye View (BEV)-conditioned model is compared with two 3D generative
models:adiffusionmodel-based3DU-Net,anda2D-3Dliftingmethodthatsep-
arately generates a top-down view semantic image, a corresponding height im-
age,andliftstheserepresentationsinto3Dspace.Suchgenerationstrategieshave
beenpreviouslyexploredeitherparametrically[26]ornon-parametrically[7].For
each method, we generate 5k samples and evaluate these against data from the
nuScenesvalidationset.OurevaluationincorporatestheVariationofFr√©chetIn-
ception Distance (V-FID) to measure the distance of feature distribution, along
with other metrics for scene-level evaluation. The human evaluation was con-
ducted by asking 8 users to choose the better one when comparing visualiza-
tion results. Each person evaluated 32 examples. The results demonstrated in
Table 1, reveal that our proposed method perfors better in terms of both quan-
titative indicators (including V-FID and Maximum Mean Discrepancy (MMD))
and qualitative indicators (human evaluation).
Methods V-FID(‚Üì) MMD(‚Üì) Comparison Preference
2D-3Dlifting 567.0 1.292
Oursvs2D-3Dlifting 100%
Diffw.3DU-Net [33] 445.6 0.273
Ours 291.4 0.106 OursvsDiffw.3DU-Net 96.9%
Table 1: Quantitative comparisons with different baselines(left). V-FID and
MMD are reported as evaluation metrics. and Human Evaluation(right).12 Zhang et al.
(a) 2D-3D lifting (b) Diff w. U-Net (c) Ours (d) Ground Truth
Fig.6: Qualitative examples for different methods and ground truth data.
4.3 Ablation study
Autoencoder The selection of the initial stage model is a critical factor. For
this reason, we have conducted an ablation study on different architectural de-
signs for the Vector Quantized Variational AutoEncoder (VQ-VAE) model. As
demonstrated in Table 2, certain types of VQ-VAE show superior performance
compared to those applying a KL divergence penalty, in terms of Intersection
over Union (IoU) and mean IoU (mIoU). This suggests that the discrete variant
of VQ-VAE is better suited for 3D occupancy data. Additionally, an increase in
theembeddingdimensionresultsinanenhancementofIoUandmIoU.Downsam-
pling to 1/8 significantly impairs performance. Therefore, based on the findings
outlined in Table 2, we opt to downsample the raw data to a 1/4 resolution and
use a codebook with 2048 embeddings.
Methods IoU(‚Üë) mIoU(‚Üë) Methods V-FID(‚Üì) CC(‚Üë)
VAE,KL 0.982 0.682
Crossattn 311.3 0.252
VQVAE,embed512 0.987 0.764
VQVAE,embed1024with1/4resolution 0.988 0.798 Modulation 317.4 0.674
V VQ QV VA AE E, ,e em mb be ed d2 20 04 48 8w wi it th h1 1/ /8 4r re es so ol lu ut ti io on n 00 .. 997 83 8 00 .. 851 03 0 Concat 291.4 0.789
Table 2: Comparisons with different Table 3: Comparisons with dif-
autoencoders. IoU and mIoU are utlized ferent condition options. V-FID
to evluate the reconstruction results. andCC(conditionconsistency)arere-
ported as evaluation metrics.
Different conditioning options In our study, we conduct an experiment to
determine the most effective method for integrating the Bird‚Äôs Eye View (BEV)
feature into the diffusion process. We consider three potential options: modu-
lation, cross-attention, and concatenation. The modulation approach is similar
to Adaptive Instance Normalization (AdaIN) [18], the cross-attention method
borrows from the text-to-image synthesis technique [33], and the concatenation
strategy aligns the 2D BEV feature with the 3D latent space by pooling along
the height axis.Urban Scene Diffusion through Semantic Occupancy Map 13
To measure the efficacy of these conditional generation strategies, we mea-
sure the V-FID scores, which serve as an indicator of both the quality of the
generated samples and the consistency with the given condition. As shown in
Fig.7 and Table 3, the concatenation strategy, which combines the BEV feature
with the latent feature, outperforms both the cross-attention and modulation
methods. This finding implies that concatenation more effectively aligns the 2D
BEV features with the 3D representation, thereby ensuring a more accurate
and reliable transformation from the 2D BEV map to the 3D generated output.
Consequently, our results suggest that the concatenation strategy is an optimal
choice for integrating BEV features into the diffusion process when generating
3D data from 2D conditions.
(a) BEV (b) Crossattn (c) Modulation (d) Concat
Fig.7: Comparison of different methods of incorporating conditions. Compared to
the cross-attention and adaIn methods, concatenating the BEV (Bird‚Äôs Eye View)
condition can achieve superior controllability during the generation process.
4.4 Point cloud segmentation
Wetaketheexperimentofpointcloudsegmentationasadownstreamtaskforour
generation to verify the effectiveness of our generated data. We convert all the
occupancy grids into point clouds, and then consider 3D semantic segmentation
andverifytheaugmentationusingoursyntheticdataviaCylinder3D[57].Tab.4
shows generated data could boost the performance of the perception model and
verifies the effectiveness of our data.
Method
Real 42.57 7.70 58.04 79.40 27.95 30.46 52.11 11.12 36.87 40.47 81.47 29.57 46.17 52.45 83.5988.09 48.00
Realw.aug.43.797.3270.8279.1529.4830.6753.3210.2141.49 53.2982.47 32.1347.1854.2283.8188.3550.48
Table 4: 3D point cloud segmentation experiment. We show the IoU results for all
the categories on the nuScenes validation set. The segmentation experiment verfy the
effectiveness of our generated data which could boost the performance of the down-
stream task. Those results that differ by more than one percentage point are marked.
reirrab elcycib
sub rac
heV.snoC elcycrotom nairtsedep enoccffiart
reliart kcurt
ruS.irD taflrehto klawedis
niarret
edamnam noitategev
UoIm14 Zhang et al.
4.5 Scene Image Synthesis
We further verify that the generated semantic occupancy grids could be used
as a generative prior and benefits the scene image synthesis. Specifically, we
use Score Distillation Sampling to distill knowledge from a 2D diffusion model
to optimize the rendered urban scene. By parameterizing the scene using an
MLP as the renderer and optimizing it using SDS, we are able to transform the
semantic occupancy grid into a 3D environment with visual appearance. The
result is shown in Fig. 8. We can see that the synthesized images exhibit visual
diversity and content consistency with the semantic labels.
Fig.8: Scene image synthesis based on semantic occupancy grid. The left
column illustrates the generated semantic occupancy grid observed in overhead view.
The right columns show the synthesized scene images from different angles. The text
descriptions for each scene are ‚Äôa driving scene image at Boston, daytime.‚Äô and ‚Äôa
driving scene image at Singapore, daytime.‚Äô
5 Conclusion
WepresentUrbanDiffusion,a3Ddiffusionapproachforgeneratingunbounded
urban scenes. The proposed model accurately represents the geometry and se-
mantics of objects by conditioning on BEV layout, producing diverse and re-
alistic urban scenes in the form of semantic occupancy maps. The model also
demonstratesthecapabilitytogeneraterealisticurbanscenesbasedontheBEV
layouts synthesized from a driving simulator.
Limitations. The visual appearance of the synthesized scene image is not ideal,
as the image synthesis is not the focus of this work and is independent of theUrban Scene Diffusion through Semantic Occupancy Map 15
3D diffusion model training. In the future, we will train a multi-modal diffusion
modelthatcangenerategeometry,semantics,andvisualappearanceatthesame
time. On the other hand, there is no object instance in the generated scene and
the moving objects in the scene also bring some artifacts. We will incorporate
instance information in the diffusion process in the future.
References
1. Bautista, M.A., Guo, P., Abnar, S., Talbott, W., Toshev, A., Chen, Z., Dinh, L.,
Zhai, S., Goh, H., Ulbricht, D., et al.: Gaudi: A neural architect for immersive 3d
scene generation. Advances in Neural Information Processing Systems 35, 25102‚Äì
25116 (2022) 4
2. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A.,
Pan,Y.,Baldan,G.,Beijbom,O.:nuscenes:Amultimodaldatasetforautonomous
driving. In: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. pp. 11621‚Äì11631 (2020) 3, 8
3. Caesar, H., Kabzan, J., Tan, K.S., Fong, W.K., Wolff, E., Lang, A., Fletcher, L.,
Beijbom, O., Omari, S.: nuplan: A closed-loop ml-based planning benchmark for
autonomous vehicles. arXiv preprint arXiv:2106.11810 (2021) 10
4. Cai,S.,Chan,E.R.,Peng,S.,Shahbazi,M.,Obukhov,A.,VanGool,L.,Wetzstein,
G.: Diffdreamer: Towards consistent unsupervised single-view scene extrapolation
withconditionaldiffusionmodels.In:ProceedingsoftheIEEE/CVFInternational
Conference on Computer Vision. pp. 2139‚Äì2150 (2023) 3
5. Chai, L., Tucker, R., Li, Z., Isola, P., Snavely, N.: Persistent nature: A generative
model of unbounded 3d worlds. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 20863‚Äì20874 (2023) 1, 3
6. Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo,
O., Guibas, L.J., Tremblay, J., Khamis, S., et al.: Efficient geometry-aware 3d
generativeadversarialnetworks.In:ProceedingsoftheIEEE/CVFConferenceon
Computer Vision and Pattern Recognition. pp. 16123‚Äì16133 (2022) 1
7. Chen, Z., Wang, G., Liu, Z.: Scenedreamer: Unbounded 3d scene generation from
2d image collections. arXiv preprint arXiv:2302.01330 (2023) 1, 3, 8, 11
8. Cheng, Y.C., Lee, H.Y., Tulyakov, S., Schwing, A.G., Gui, L.Y.: Sdfusion: Multi-
modal3dshapecompletion,reconstruction,andgeneration.In:Proceedingsofthe
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4456‚Äì
4465 (2023) 3
9. Chou, G., Bahat, Y., Heide, F.: Diffusion-sdf: Conditional generative modeling of
signeddistancefunctions.In:ProceedingsoftheIEEE/CVFInternationalConfer-
ence on Computer Vision. pp. 2262‚Äì2272 (2023) 3
10. Gao,R.,Chen,K.,Xie,E.,Hong,L.,Li,Z.,Yeung,D.Y.,Xu,Q.:Magicdrive:Street
viewgenerationwithdiverse3dgeometrycontrol.arXivpreprintarXiv:2310.02601
(2023) 1
11. Gulino, C., Fu, J., Luo, W., Tucker, G., Bronstein, E., Lu, Y., Harb, J., Pan, X.,
Wang, Y., Chen, X., et al.: Waymax: An accelerated, data-driven simulator for
large-scale autonomous driving research. arXiv preprint arXiv:2310.08710 (2023)
9
12. Hao, Z., Mallya, A., Belongie, S., Liu, M.Y.: Gancraft: Unsupervised 3d neural
rendering of minecraft worlds. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 14072‚Äì14082 (2021) 3, 816 Zhang et al.
13. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
neural information processing systems 33, 6840‚Äì6851 (2020) 4
14. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598 (2022) 5, 6
15. H√∂llein, L., Cao, A., Owens, A., Johnson, J., Nie√üner, M.: Text2room: Extracting
textured3dmeshesfrom2dtext-to-imagemodels.arXivpreprintarXiv:2303.11989
(2023) 4
16. Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J.,
Corrado, G.: Gaia-1: A generative world model for autonomous driving. arXiv
preprint arXiv:2309.17080 (2023) 1
17. Hu, S., Chen, L., Wu, P., Li, H., Yan, J., Tao, D.: St-p3: End-to-end vision-based
autonomousdrivingviaspatial-temporalfeaturelearning.In:EuropeanConference
on Computer Vision. pp. 533‚Äì549. Springer (2022) 2
18. Huang,X.,Belongie,S.:Arbitrarystyletransferinreal-timewithadaptiveinstance
normalization. In: Proceedings of the IEEE international conference on computer
vision. pp. 1501‚Äì1510 (2017) 12
19. Kim,S.W.,Brown,B.,Yin,K.,Kreis,K.,Schwarz,K.,Li,D.,Rombach,R.,Tor-
ralba,A.,Fidler,S.:Neuralfield-ldm:Scenegenerationwithhierarchicallatentdif-
fusionmodels.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition. pp. 8496‚Äì8506 (2023) 4
20. Li, M., Duan, Y., Zhou, J., Lu, J.: Diffusion-sdf: Text-to-shape via voxelized dif-
fusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 12642‚Äì12651 (2023) 3
21. Li, Q., Peng, Z., Feng, L., Zhang, Q., Xue, Z., Zhou, B.: Metadrive: Composing
diverse driving scenarios for generalizable reinforcement learning. TPAMI (2022)
3, 6, 9, 10
22. Li,Y.,Yu,Z.,Choy,C.,Xiao,C.,Alvarez,J.M.,Fidler,S.,Feng,C.,Anandkumar,
A.:Voxformer:Sparsevoxeltransformerforcamera-based3dsemanticscenecom-
pletion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 9087‚Äì9098 (2023) 2
23. Li,Z.,Wang,Q.,Snavely,N.,Kanazawa,A.:Infinitenature-zero:Learningperpet-
ualviewgenerationofnaturalscenesfromsingleimages.In:EuropeanConference
on Computer Vision. pp. 515‚Äì534. Springer (2022) 3
24. Lin,C.H.,Gao,J.,Tang,L.,Takikawa,T.,Zeng,X.,Huang,X.,Kreis,K.,Fidler,
S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 300‚Äì309 (2023) 4
25. Lin,C.H.,Lee,H.Y.,Cheng,Y.C.,Tulyakov,S.,Yang,M.H.:Infinitygan:Towards
infinite-pixel image synthesis. arXiv preprint arXiv:2104.03963 (2021) 3
26. Lin,C.H.,Lee,H.Y.,Menapace,W.,Chai,M.,Siarohin,A.,Yang,M.H.,Tulyakov,
S.: Infinicity: Infinite-scale city synthesis. arXiv preprint arXiv:2301.09637 (2023)
3, 8, 11
27. Liu, A., Tucker, R., Jampani, V., Makadia, A., Snavely, N., Kanazawa, A.: Infi-
nite nature: Perpetual view generation of natural scenes from a single image. In:
Proceedingsof the IEEE/CVFInternationalConference on ComputerVision. pp.
14458‚Äì14467 (2021) 1, 3
28. Lugmayr, A., Danelljan, M., Romero, A., Yu, F., Timofte, R., Van Gool, L.: Re-
paint: Inpainting using denoising diffusion probabilistic models. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
11461‚Äì11471 (2022) 7Urban Scene Diffusion through Semantic Occupancy Map 17
29. Luo, S., Hu, W.: Diffusion probabilistic models for 3d point cloud generation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 2837‚Äì2845 (2021) 3
30. Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for
generating3dpointcloudsfromcomplexprompts.arXivpreprintarXiv:2212.08751
(2022) 11
31. Niemeyer,M.,Geiger,A.:Giraffe:Representingscenesascompositionalgenerative
neural feature fields. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 11453‚Äì11464 (2021) 1
32. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using
2d diffusion. arXiv preprint arXiv:2209.14988 (2022) 1, 4, 8
33. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition.pp.10684‚Äì10695(2022) 1,
5, 8, 11, 12
34. Rombach, R., Esser, P., Ommer, B.: Geometry-free view synthesis: Transformers
and no 3d priors. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 14356‚Äì14366 (2021) 3
35. Shen, Y., Ma, W.C., Wang, S.: Sgam: Building a virtual 3d world through si-
multaneous generation and mapping. Advances in Neural Information Processing
Systems 35, 22090‚Äì22102 (2022) 3
36. Shi,Z.,Zhou,X.,Qiu,X.,Zhu,X.:Improvingimagecaptioningwithbetteruseof
captions. arXiv preprint arXiv:2006.11807 (2020) 1
37. Skorokhodov, I., Sotnikov, G., Elhoseiny, M.: Aligning latent and image spaces
to connect the unconnectable. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 14144‚Äì14153 (2021) 3
38. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.arXivpreprint
arXiv:2010.02502 (2020) 9
39. Song, L., Cao, L., Xu, H., Kang, K., Tang, F., Yuan, J., Zhao, Y.: Roomdreamer:
Text-driven 3d indoor scene synthesis with coherent geometry and texture. arXiv
preprint arXiv:2305.11337 (2023) 4
40. Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Ermon,S.,Poole,B.:Score-
basedgenerativemodelingthroughstochasticdifferentialequations.arXivpreprint
arXiv:2011.13456 (2020) 4
41. Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo,
J., Zhou, Y., Chai, Y., Caine, B., et al.: Scalability in perception for autonomous
driving: Waymo open dataset. In: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition. pp. 2446‚Äì2454 (2020) 9, 10
42. Swerdlow, A., Xu, R., Zhou, B.: Street-view image generation from a bird‚Äôs-eye
view layout. arXiv preprint arXiv:2301.04634 (2023) 1
43. Tian, X., Jiang, T., Yun, L., Wang, Y., Wang, Y., Zhao, H.: Occ3d: A large-
scale 3d occupancy prediction benchmark for autonomous driving. arXiv preprint
arXiv:2304.14365 (2023) 2, 9, 11
44. Tong,W.,Sima,C.,Wang,T.,Chen,L.,Wu,S.,Deng,H.,Gu,Y.,Lu,L.,Luo,P.,
Lin,D.,etal.:Sceneasoccupancy.In:ProceedingsoftheIEEE/CVFInternational
Conference on Computer Vision. pp. 8406‚Äì8415 (2023) 2, 9
45. Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning.
Advances in neural information processing systems 30 (2017) 5
46. Wang,X.,Zhu,Z.,Huang,G.,Chen,X.,Lu,J.:Drivedreamer:Towardsreal-world-
driven world models for autonomous driving. arXiv preprint arXiv:2309.09777
(2023) 118 Zhang et al.
47. Wei,Y.,Zhao,L.,Zheng,W.,Zhu,Z.,Zhou,J.,Lu,J.:Surroundocc:Multi-camera
3doccupancypredictionforautonomousdriving.In:ProceedingsoftheIEEE/CVF
International Conference on Computer Vision. pp. 21729‚Äì21740 (2023) 2, 9
48. Xu, Y., Chai, M., Shi, Z., Peng, S., Skorokhodov, I., Siarohin, A., Yang, C.,
Shen,Y.,Lee,H.Y.,Zhou,B.,etal.:Discoscene:Spatiallydisentangledgenerative
radiance fields for controllable 3d-aware scene synthesis. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4402‚Äì
4412 (2023) 2
49. Yang, K., Ma, E., Peng, J., Guo, Q., Lin, D., Yu, K.: Bevcontrol: Accurately
controllingstreet-viewelementswithmulti-perspectiveconsistencyviabevsketch
layout. arXiv preprint arXiv:2308.01661 (2023) 1
50. Yang, Y., Yang, Y., Guo, H., Xiong, R., Wang, Y., Liao, Y.: Urbangiraffe: Rep-
resenting urban scenes as compositional generative neural feature fields. arXiv
preprint arXiv:2303.14167 (2023) 2, 3
51. Zeng, X., Vahdat, A., Williams, F., Gojcic, Z., Litany, O., Fidler, S., Kreis,
K.: Lion: Latent point diffusion models for 3d shape generation. arXiv preprint
arXiv:2210.06978 (2022) 3
52. Zhang,Q.,Wang,C.,Siarohin,A.,Zhuang,P.,Xu,Y.,Yang,C.,Lin,D.,Zhou,B.,
Tulyakov, S., Lee, H.Y.: Scenewiz3d: Towards text-guided 3d scene composition.
In: CVPR (2024) 4
53. Zhang, Q., Xu, Y., Shen, Y., Dai, B., Zhou, B., Yang, C.: Berfscene: Bev-
conditioned equivariant radiance fields for infinite 3d scene generation. In: CVPR
(2024) 3
54. Zhang, Q., Yang, C., Shen, Y., Xu, Y., Zhou, B.: Towards smooth video composi-
tion. In: ICLR (2023) 1
55. Zhou,B.,Kr√§henb√ºhl,P.:Cross-viewtransformersforreal-timemap-viewsemantic
segmentation. In: Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition. pp. 13760‚Äì13769 (2022) 9
56. Zhou,L.,Du,Y.,Wu,J.:3dshapegenerationandcompletionthroughpoint-voxel
diffusion.In:ProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision. pp. 5826‚Äì5835 (2021) 3
57. Zhu,X.,Zhou,H.,Wang,T.,Hong,F.,Ma,Y.,Li,W.,Li,H.,Lin,D.:Cylindrical
andasymmetrical3dconvolutionnetworksforlidarsegmentation.In:CVPR(2021)
13