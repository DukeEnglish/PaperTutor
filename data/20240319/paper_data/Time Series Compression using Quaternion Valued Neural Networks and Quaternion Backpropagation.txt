PREPRINT 1
Time Series Compression using Quaternion Valued
Neural Networks and Quaternion Backpropagation
Johannes Po¨ppelbaum and Andreas Schwung
Abstract—We propose a novel quaternionic time-series com- usedapproachispiecewiseaggregateapproximation(PAA)[5]
pressionmethodologywherewedividealongtime-seriesintoseg- whichdividesatimeseriesinN segmentsandtakesthemean
mentsofdata,extractthemin,max,meanandstandarddeviation
of the n datapoints included in N to form the compressed
of these chunks as representative features and encapsulate them
time series. It is popular for being very intuitive and easy
in a quaternion, yielding a quaternion valued time-series. This
time-series is processed using quaternion valued neural network and cheap to compute. However, this does not account for
layers, where we aim to preserve the relation between these data deviations, as a sine wave with a high amplitude may
featuresthroughtheusageoftheHamiltonproduct.Totrainthis later appear similar to a constant value. To overcome this
quaternion neural network, we derive quaternion backpropaga-
downside and enrich the downsampled or compressed time-
tion employing the GHR calculus, which is required for a valid
series with additional descriptive properties, we propose a
product and chain rule in quaternion space. Furthermore, we
investigate the connection between the derived update rules and novel, quaternion based time-compression method. In partic-
automatic differentiation. We apply our proposed compression ular, we propose to divide a longer time-series into smaller
method on the Tennessee Eastman Dataset, where we perform chunks from which we extract four representative features,
fault classification using the compressed data in two settings:
namely the min, max, mean and standard deviation. As this
a fully supervised one and in a semi supervised, contrastive
fourpropertiesarestronglyrelated,weencapsulatetheminthe
learning setting. Both times, we were able to outperform real
valued counterparts as well as two baseline models: one with mathematicalobjectofaquaternion.ByutilizingtheHamilton
the uncompressed time-series as the input and the other with a product in quaternion valued one dimensional convolution
regulardownsamplingusingthemean.Further,wecouldimprove layer, we aim to preserve their relationship during the feature
theclassificationbenchmarksetbySimCLR-TSfrom81.43%to
extraction. While similar approaches have been proposed in
83.90%.
the image domain [6], [7] where the r-, g- and b-channel of
Index Terms—Quaternion Neural Network, Quaternion Back- an image have been encoded in a quaternion, our approach
propagation, Time-Series Compression, Fault Classification.
is novel to the time-series domain, especially in combination
with the quaternionic compression methodology. It can serve
I. INTRODUCTION asaneasytoimplementimprovementtoalreadyexisting,PAA
based implementations and pipelines without major refac-
NOWADAYS an extensive amount of time-series data
toring, leveraging the strengths of quaternion valued neural
is widely available, be it from industrial applications,
network(QNN),namelyrelationshippreservationandsparsity
Internet-of-Things (IoT) devices or wearables. Due to high
in comparison to a real valued equivalent.
sampling frequencies of sensors or long recording times, long
Following the big success and broad range of applications
sequences of data are the consequence. Often, the aim is
of real-valued neural networks, in recent years along complex
to process them using machine learning models. However,
valued models [8]–[13] also quaternion valued models [6],
this gets difficult as recurrent architectures might struggle
[14]–[18] appealed the interest of researchers and gain more
with longer sequences as it is difficult to squeeze all seen
popularity over time. Applications are e.g. image tasks, 3D
information into a fixed length representation [1], [2].
point cloud processing, speech / language tasks or sensor
Also, with more data to process, training times of mod-
fusion. Quaternion valued MLPs using elementwise operating
els increase drastically. Likewise, storage requirements grow
activation functions are further proven to be universal inter-
and potentially transmission times and energy consumption
polators in H [19]. To train these quaternion valued neural
increases which is especially relevant for IoT devices. The
networks by means of gradient descent, [20]–[22] introduced
authors of [3] show that a higher sampling rate as such
quaternion backpropagation variants, using partial derivatives
did not increase the models accuracy while requiring sig-
withrespecttothefourquaternioncomponents.However,they
nificantly more training time when using a 1d-cnn based
do not use the GHR calculus [23], which is required for the
model architecture. Similarly, in [4] downsampling yielded
validity of the product and chain rule in quaternion space
performance increasements in a decoding task for multiple
(compare III-B1).
network architecture types.
Inthispaper,afterintroducingtherequiredquaternionalge-
Naturally,thequestionwhetherwecanreducethenumberof
bra,weproposeanovelquaternionbackpropagation,basedon
datapoints required for the machine learning task by applying
the GHR calculus introduced in [23], considerably extending
a cheap to compute data compression arises. One common
[24]. We provide implementation ready formulas for all parts
of the computational chain including intermediate results.
Johannes Po¨ppelbaum and Andreas Schwung are with South Westphalia
UniversityofAppliedSciences Furthermore,weproposeanovelquaternionbasedtime-series
4202
raM
81
]GL.sc[
1v22711.3042:viXraPREPRINT 2
compression and test it using the Tennessee Eastman (TE) subsequent reconstruction rather than a fault classification
dataset. using the compressed data.
The main contributions of our work are the following: The authors of [38] investigate the effect on the classifica-
• We propose a novel time-series compression method tion performance for univariate and multivariate time-series
yielding a shorter, quaternion valued time-series. This data when applying compressed sensing, discrete wavelet
compressed time-series is subsequently used for fault transform(DWT),theSZalgorithmfrom[36]andacombina-
classification using a quaternion valued neural network tion of DWT, lifting scheme and SZ as compression methods.
model. In [39], the authors could show that by applying compressive
• We show that for the approach used in initial work on sensing,theclassificationperformanceusingahiddenMarkov
quaternion backpropagation, the product rule as well as model for human activity recognition on video data could
the chain rule does not hold. This imposes the require- often be increased. Similarly, the authors of [40] were able to
ment to extend these approaches. show that the use of a lossy compression does not reduce the
• We derive quaternion backpropagation using the GHR prediction quality when using medical data to predict human
calculus which defines proper product and chain rules. stresslevelsusingamultilayerperceptron(MLP)architecture.
We employ detailed calculations for each intermediate We differ from these works as we propose a quaternion based
result,givingproperinsightsontheunderlyingquaternion compression,intendedfortime-seriesdataandtobeprocessed
mathematics. with the Hamilton product within quaternion valued neural
• We investigate the relation of the derived quaternion network (NN), for the first time. Furthermore, contrary to
backpropagation and automatic differentiation, revealing [38] and [40] we directly feed the compressed time-series
insights on scenarios where it can be used to train QNN. data to the fault classification model instead of performing
• We experimentally prove the effectiveness of the pro- a reconstruction first and waive on utilizing the frequency
posed compression and provide detailed comparisons domain as in the former.
with real-valued counterparts as well as two baseline Finally,[41]showedthatPAAbasedapproachescanbenefit
models in fully supervised learning setups. We further from additionally using the standard deviation rather than
use a state of the art self-supervised learning setup to just the mean. Likewise, in [42] additionally to the mean,
demonstratethatourapproachoutperformspreviouslyset the minimum and maximum values are used. Our proposed
classification benchmarks. methodology can be seen as the fusion of the two approaches
within the quaternion space.
Thepaperisstructuredasfollows:SectionIIpresentsthere-
Backpropagation: The breakthrough of training neural
lated work and Section III the required fundamentals. Section
networks using backpropagation started with the well known
IV continues with the derived quaternion valued backpropa-
paper[43]andwasextendedandmodifiedinanumberofways
gation algorithm, Section V with the relation of quaternion
in the following. As neural networks emerged from the real
backpropagation and automatic differentiation and Section
numbers to the complex models, equivalent complex variants
VI with the proposed time-series compression methodology.
were introduced in [44]–[46]. A detailed description of the
Section VII provides the experimental evaluation. Finally, the
usage of the CR / Wirtinger Calculus, which comes in handy
paper is concluded by Section VIII.
for complex backpropagation, can be found in [47]. None
of them target quaternion valued models though. Related are
II. RELATEDWORK
also gradient based optimization techniques in the quaternion
The related work can mainly be divided into the fol- domain as described in [23], [48]. However, they differ from
lowing categories: time-series compression, backpropagation our work as they don’t target multi-layered, perceptron based
techniques,especiallyinthecomplexorhypercomplexdomain architectures.
and finally applications of quaternion valued neural networks Although,incontrast,thesemulti-layered,perceptronbased
which are described in the following. architectures are targeted in [20]–[22], the respective used
Time-series Compression: In the field of time-series data approaches suffer from the fact that the chain rule does not
compression, it has to be separated between lossless and holdthere.WeovercomethisbyemployingtheGHR-Calculus
lossy compression techniques. The former is characterized and derive the quaternion backpropagation based on this, as
by the fact that they allow for an error free reconstruction. initially proposed by [24], and significantly extend it.
Examples are SPRINTZ [25], DACTS [26], RAKE [27], Application of Quaternion Neural Networks: With re-
RLBE [28] or DRH [29]. In contrast, the latter does not gardstoquaternionneuralnetworkapplications,[6]composes
enableaperfectreconstruction,butinsteadallowsforahigher fully quaternion valued models by combining quaternion con-
compression rate as lossless techniques are limited in that volutionandquaternionfully-connectedlayerstousethemon
regard. An often used lossy approach is to approximate the classification and denoising tasks of color images. Similarly,
time-seriesusingpiece-wiselinears[30]–[32]orpolynominals [49] uses quaternion valued variational autoencoders (VAEs)
[33]–[35]. Further approaches are e.g. SZ [36] or LFZip [37]. for image reconstruction and generation tasks, outperforming
However,thecompressionapproachisfundamentallydifferent real valued VAEs while simultaneously reducing the number
as it is not aiming for extracting representative features but of parameters. The same observation was made by [50] when
instead uses approximations of the original data in a function using quaternion generative adversarial networks (GAN) for
representation. Furthermore, the focus here is usually on image generation. The authors of [14] achieve rotationalPREPRINT 3
invarianceforpoint-cloudinputsbyutilizingquaternionvalued and multiplication as
inputs and features, the used weights and biases however are
x⊗y =x y −x·y+x y+y x+x×y
realvaluedmatrices.Inthecontextofspeech/languageunder- 0 0 0 0
standing,[15]employsaquaternionMLPapproach,wherethe =(x 0y 0−x 1y 1−x 2y 2−x 3y 3)
quaternion model could outperform real valued counterparts +(x y +x y +x y −x y )i (1)
0 1 1 0 2 3 3 2
while requiring fewer epochs to be trained. Similarly, [16]
+(x y −x y +x y +x y )j
0 2 1 3 2 0 3 1
utilizes quaternion convolutional neural networks (CNNs) and
+(x y +x y −x y +x y )k.
quaternion recurrent neural networks (RNNs) for this task. 0 3 1 2 2 1 3 0
Again, these models were able to outperform their real- We further define the quaternion Hadamard-product of two
valued counterparts while requiring approximately four times quaternions x,y as
fewer trainable parameters. In a related task, [51] employs
quaternion CNN for emotion recognition in speech. In [17], x◦y=x y +x y i+x y j+x y k.
0 0 1 1 2 2 3 3
a novel quaternion weight initialization strategies as well as
quaternion batch-normalization is proposed to perform image 2) QuaternionInvolutions: Ofparticularimportanceforthe
classificationandsegmentationwithquaternionvaluedmodels. quaternionderivativesarethequaternionself-inversemappings
A further application of recurrent quaternion models is [18] or involutions, defined as [54]
where quaternion RNNs, specifically Quaternion Gated Re-
current Units are applied on sensor fusion for navigation and ϕ(η)=qη =ηqη−1 =ηqη−1 =−ηqη (2)
human activity recognition. Finally, [52] proposes quaternion
where η is a pure unit quaternion. Using Equation (2), we
product units and uses them to human action and hand action
can create the involutions as
recognition. Furthermore, they utilize them for point cloud
classification. All these mentioned approaches differ from our
q=q +q i+q j+q k
0 1 2 3
work as none is concerned with time series compression of
qi =−iqi=q +q i−q j−q k
industrial data in the quaternion domain, in particular with 0 1 2 3
a fault classification downstream task where the compressed qj =−jqj =q −q i+q j−q k
0 1 2 3
data serves as the models input. qk =−kqk =q −q i−q j+q k.
0 1 2 3
The corresponding conjugate involutions are
III. FUNDAMENTALS
This section introduces the required fundamentals, starting qi∗ =q 0−q 1i+q 2j+q 3k
with the mathematics of quaternions, followed by quaternion qj∗ =q +q i−q j+q k
0 1 2 3
derivatives and the regular backpropagation algorithm for real
qk∗ =q +q i+q j−q k.
valued neural networks. 0 1 2 3
With their help, various relations can be created which
come in handy in the following quaternion valued derivation
A. Quaternions
calculations, as they often help in simplifying the calculations
1) Quaternion Algebra: Quaternions were discovered by
and to avoid elaborate term sorting [23], [55]:
Hamilton in 1843 [53] as a method to extend the complex
numberstothethreedimensionalspace.Aquaternionq,q∈H
consists of three parts, one real and three imaginary q = 1(cid:0) q+qi+qj+qk(cid:1) , q =−i (cid:0) q+qi−qj−qk(cid:1)
0 4 1 4
(3)
q=q 0+q 1i+q 2j+q 3k =q 0+q q
2
=− 4j (cid:0) q−qi+qj−qk(cid:1) , q
3
=−k
4
(cid:0) q−qi−qj+qk(cid:1)
where q ,q ,q ,q ∈R. Often, q is referred to as the real
0 1 2 3 0
partandqasthevectorpart.Theimaginarycomponentsi,j,k
q =
1(cid:0) q∗+qi∗+qj∗+qk∗(cid:1)
, q =
i (cid:0) q∗+qi∗−qj∗−qk∗(cid:1)
have the properties 0 4 1 4
i2 =j2 =k2 =ijk =−1 q 2 =
4j (cid:0) q∗−qi∗+qj∗−qk∗(cid:1)
, q 3 =
k
4
(cid:0) q∗−qi∗−qj∗+qk∗(cid:1)
(4)
ij =+k, jk =+i, ki=+j
ji=−k, kj =−i, ik =−j.
Similar to complex numbers, quaternions have a conjugate:
q∗ = 1(cid:0) −q+qi+qj+qk(cid:1) , qi∗ = 1(cid:0) q−qi+qj+qk(cid:1)
2 2
(5)
q∗ =q 0−q 1i−q 2j−q 3k =q 0−q qj∗ = 1(cid:0) q+qi−qj+qk(cid:1) , qk∗ = 1(cid:0) q+qi+qj−qk(cid:1)
2 2
√
A quaternion q fulfilling ∥q∥ = qq∗ =
(cid:112)
q 02+q 12+q 22+q 32 = 1 is called a unit quaternion q= 1(cid:0) −q∗+qi∗+qj∗+qk∗(cid:1) , qi = 1(cid:0) q∗−qi∗+qj∗+qk∗(cid:1)
and a quaternion q with q 0 =0 is called a pure quaternion. 2 2
The addition of two quaternions x,y is defined as qj = 1(cid:0) q∗+qi∗−qj∗+qk∗(cid:1) , qk = 1(cid:0) q∗+qi∗+qj∗−qk∗(cid:1)
2 2
x+y=(x +y )+(x +y )i+(x +y )j+(x +y )k (6)
0 0 1 1 2 2 3 3PREPRINT 4
B. Quaternion Derivatives Using the chain rule ∂f = ∂f ∂z, the outer derivative yields
∂x ∂z ∂x
1) Partial derivatives: For a quaternion valued function ∂f ∂ (cid:16) (cid:17) ∂ (cid:16) (cid:17)
f(q),q = q
0
+q 1i+q 2j +q 3k, [20] and [21] calculate the
∂z
=
∂z
zz∗ =
∂z
z 02+z 12+z 22+z 32
derivatives as follows:
=2(z +z i+z j+z k)=2z
0 1 2 3
(cid:18) (cid:19)
∂f ∂f ∂f ∂f ∂f and the inner derivative is
= + i+ j+ k (7)
∂q ∂q 0 ∂q 1 ∂q 2 ∂q 3 ∂z ∂ (cid:16) (cid:17) ∂ (cid:16)
= xy = x y +x y i+x y j+x y k
However, as we will show now, neither the product rule nor ∂x ∂x ∂x 0 0 0 1 0 2 0 3
the chain rule hold for this approach. +x 1y 0i−x 1y 1+x 1y 2k−x 1y 3j+x 2y 0j−x 2y 1k
(cid:17)
Proposition 1. For the derivative of a quaternion valued −x 2y 2+x 2y 3i+x 3y 0k+x 3y 1j−x 3y 2i−x 3y 3
function f(q),q∈H following (7), the product rule does not =2(−y +y i+y j+y )k =−2y∗.
0 1 2 3
hold.
Combining inner and outer derivative yields
Proof. Considerf(q)=qq∗ =q2+q2+q2+q2asthefunction
0 1 2 3 ∂f ∂f ∂z
of choice. The direct derivation following (7) yields = =2z(−2y∗)=−4(xyy∗)
∂x ∂z∂x
(cid:18) (cid:19)
∂f ∂f ∂f ∂f ∂f
= + i+ j+ k =−4x 0y 02−4x 0y 12−4x 0y 22−4x 0y 32
∂q ∂q 0 ∂q 1 ∂q 2 ∂q 3 +(cid:0) −4x y2−4x y2−4x y2−4x y2(cid:1) i
=(2q +2q i+2q j+2q k)=2q 1 0 1 1 1 2 1 3
0 1 2 3 +(cid:0) −4x y2−4x y2−4x y2−4x y2(cid:1) j
2 0 2 1 2 2 2 3
Using the product rule, we can calculate the same derivation +(cid:0) −4x y2−4x y2−4x y2−4x y2(cid:1) k
3 0 3 1 3 2 3 3
using
Obviously the results don’t match up and hence chain
rule does not hold for this way of calculating quaternion
∂ ∂q∗ ∂q
(qq∗)=q + q∗ derivatives.
∂q ∂q ∂q
∂ 2) GHR-Calculus: Initial improvements on Quaternion
=q (q −q i−q j−q k)
∂q 0 1 2 3 DerivativesyieldedtheHR-Calculus,howeveritstilllacksthe
∂ validity of the product- and chain-rule (Compare Appendix
+ ∂q(q 0+q 1i+q 2j+q 3k)q∗
A). Hence, [23] extends it to the GHR-Calculus, enabling the
definition of a novel quaternion product- and chain-rule. The
=q(1−ii−jj−kk)+(1+ii+jj+kk)q∗
derivative and conjugate derivative are defined as
=4q−2q∗ ̸=2q.
(cid:18) (cid:19)
∂f 1 ∂f ∂f ∂f ∂f
= − iµ− jµ− kµ (8)
∂qµ 4 ∂q ∂q ∂q ∂q
0 1 2 3
Proposition 2. For the derivative of a quaternion valued
function f(z(x,y)); x,y,z ∈H following (7), the chain rule ∂f 1(cid:18) ∂f ∂f ∂f ∂f (cid:19)
= + iµ+ jµ+ kµ (9)
does not hold. ∂qµ∗ 4 ∂q ∂q ∂q ∂q
0 1 2 3
Proof. Consider f(q) = zz∗; z = xy as the function of whereby µ̸=0, µ∈H.
choice. a) The Conjugate Rule: For a real valued function f,
Westartbycalculatingthederivativewithoutthechainrule: [23] define the conjugate rule as
(cid:18)
∂f
(cid:19)∗
∂f
(cid:18)
∂f
(cid:19)∗
∂f
∂f ∂ (cid:104) (cid:105) = , = (10)
= (xy)(xy)∗ ∂qµ ∂qµ∗ ∂qµ∗ ∂qµ
∂x ∂x
∂ (cid:104) b) The Product Rule: Furthermore, they define the
= (x +x i+x j+x k)(y +y i+y j+y k)
∂x 0 1 2 3 0 1 2 3 quaternion product rule as
(cid:105)
(y −y i−y j−y k)(x −x i−x j−x k)
0 1 2 3 0 1 2 3
∂(fg) ∂(g) ∂(f) ∂(fg) ∂(g) ∂(f)
∂ (cid:104) =f + g, =f + g.
= x2y2+x2y2+x2y2+x2y2 ∂qµ ∂qµ ∂qgµ ∂qµ∗ ∂qµ∗ ∂qgµ∗
∂x 0 0 0 1 0 2 0 3
+x2y2+x2y2+x2y2+x2y2+x2y2+x2y2 c) The Chain Rule: Finally, [23] also defines a quater-
1 0 1 1 1 2 1 3 2 0 2 1
(cid:105) niary chain rule as
+x2y2+x2y2+x2y2+x2y2+x2y2+x2y2
2 2 2 3 3 0 3 1 3 2 3 3
=2x 0y 02+2x 0y 12+2x 0y 22+2x 0y 32 ∂f(g(q)) ∂f ∂gν ∂f ∂gνi ∂f ∂gνj ∂f ∂gνk
+(cid:0) 2x y2+2x y2+2x y2+2x y2(cid:1) i ∂qµ = ∂gν ∂qµ+ ∂gνi ∂qµ + ∂gνj ∂qµ + ∂gνk ∂qµ
1 0 1 1 1 2 1 3
+(cid:0) 2x 2y 02+2x 2y 12+2x 2y 22+2x 2y 32(cid:1) j ∂f(g(q)) ∂f ∂gν ∂f ∂gνi ∂f ∂gνj ∂f ∂gνk
+(cid:0) 2x 3y 02+2x 3y 12+2x 3y 22+2x 3y 32(cid:1) k ∂qµ∗ = ∂gν ∂qµ∗+ ∂gνi∂qµ∗+ ∂gνj ∂qµ∗+ ∂gνk ∂qµ∗PREPRINT 5
with µ,ν ∈H, µν ̸=0. apply the strategy from the following Subsection IV-C. Re-
Note that unless otherwise stated, in the following we al- calling Equation (11), the output y of a QNN is calculated
waysuseµ=ν =1+0i+0j+0kasthissimplifiesthenotation as
throughoutthecalculations.Forfurtherunderstanding,detailed
example calculations can be found in the Appendix B. y=W(L)a(L−1)+b(L)
where a(L−1) is the output of the previous layer. One output
IV. QUATERNIONBACKPROPAGATION quaternion y can be obtained using
i
In this section, we derive the backpropagation for QNNs n
based on the GHR-Calculus. We start by defining the forward y
=(cid:88) w(L) a(L−1)+b(L).
i i,j j i
phase and the quaterniary loss function, then the derivation j=1
of the actual backpropagation follows. Specifically, we start
For deriving L with respect to the weights W(L), biases
with the derivatives of the final layer of a QNN and work
b(L) and activation outputs of the previous layer a(L−1), we
ourwaybackwardsthroughthearchitectureandcontinuewith
utilize the chain rule where we first derive ∂L and then ∂y ,
derivingthederivativesforanarbitraryhiddenlayer,following ∂y ∂W∗
∂y and ∂y.
the usual backpropagation order of sequence. ∂b∗ ∂a
Note that we use the conjugate W∗ and b∗ as this is
the direction of the steepest descent [23], [48]. For better
A. Forward phase and Loss function readability,wewaiveonthesubscripts□ indicatingthema-
i,j
We can formulate the forward phase of a regular feed- trix/vectorelementsaswellasthesuperscript□(L) throughout
forwardQNNlayer(l)withninputsandmoutputsasfollows: the following calculations.
1) Derivative with respect to the weights:
n
a(l) =σ(z(l)), z(l) =(cid:88) w(l) a(l−1)+b(l) Theorem 1. The derivative of L with respect to the weights
i i i i,j j i
j=1 w i( ,L j) of a QNN is
where i ∈ {1,...,m}, j ∈ {1,...,n} and w,y,b,a,z ∈ H. ∂L(y(w,b) 1 1
=− ea∗ =− (d−y)a∗.
The corresponding matrix-vector formulation is ∂w∗ 2 2
a(l) =σ(z(l)), z(l) =W(l)a(l−1)+b(l) (11)
Proof. Thederivativewithrespecttotheweightsw(L) follow-
i,j
where W∈Hm×n and b∈Hm .The final output y∈Hm of ing the GHR-Calculus is calculated as follows:
the last layer L and hence the overall model is a(L).
WedefinethelossfunctionL(·)betweenthedesiredoutput ∂L(y(w,b) ∂L ∂y ∂L ∂yi ∂L ∂yj ∂L ∂yk
= + + + .
d i and the actual output y i as ∂w∗ ∂y ∂w∗ ∂yi∂w∗ ∂yj ∂w∗ ∂yk ∂w∗
L=e∗T e We start by calculating the respective left partial derivatives
whereby e ∈ {1,...,m} and e = d −y. By doing so, we
obtain thei real valued loss i i i ∂ ∂L
y
= ∂∂ y(cid:2) (d 0−y 0)2+(d 1−y 1)2+(d 2−y 2)2+(d 3−y 3)2(cid:3)
L=e 1∗e 1+e 2∗e 2+···+e m∗ e m =1
[−2(d −y )+2(d −y )i+2(d −y )j+2(d −y )k]
=(e 2+e 2+e 2+e 2)+(e 2+e 2+e 2+e 2) 4 0 0 1 1 2 2 3 3
01 11 21 31 02 12 22 32
1
+···+(e 2 +e 2 +e 2 +e 2 ) =− [(d −y )−(d −y )i−(d −y )j−(d −y )k]
0m 1m 2m 3m 2 0 0 1 1 2 2 3 3
=(d 01−y 01)2+(d 11−y 11)2 =−1
(d−y)∗
=−1
e∗
+(d −y )2+(d −y )2 2 2
21 21 31 31 (13)
+(d 02−y 02)2+(d 12−y 12)2 For the derivatives with respect to the involutions yi, yj and
+(d −y )2+(d −y )2 yk wejuststatethefinalresultshere,thedetailedcalculations
22 22 32 32
+... can be found in the Appendix C-A.
+(d −y )2+(d −y )2 ∂L 1 1
0m 0m 1m 1m =− (d−y)i∗ =− ei∗
+(d −y )2+(d −y )2. ∂yi 2 2
2m 2m 3m 3m
(12)
∂L 1 1
=− (d−y)j∗ =− ej∗
∂yj 2 2
B. Final layer
∂L 1 1
We start deriving the quaternion backpropagation algorithm =− (d−y)k∗ =− ek∗ (14)
∂yk 2 2
by first considering the output of the final layer of a QNN
without the usage of an activation function. If an activation Now we calculate the right partial derivatives of
function shall be used, one can use intermediate results and y,yi,yj and yk with respect to w∗:PREPRINT 6
∂yi ∂yj ∂yk
∂y ∂(wa+b) ∂(wa) = = =0.5 (19)
= = ∂b∗ ∂b∗ ∂b∗
∂w∗ ∂w∗ ∂w∗
∂ Combining the left and right partial derivatives is related to
= [(a w −a w −a w −a w )
∂w∗ 0 0 1 1 2 2 3 3 Equation (17) with the exception that the right derivatives are
missing the a∗. Consequently, the final derivative is
+(a w +a w −a w +a w )i
0 1 1 0 2 3 3 2
+(a 0w 2+a 1w 3+a 2w 0−a 3w 1)j ∂L(e(w,b) 1
=− e.
+(a w −a w +a w +a w )k] ∂b∗ 2
0 3 1 2 2 1 3 0
1
= [a +a i+a j+a k+(a i−a +a k−a j)i
4 0 1 2 3 0 1 2 3
3) Derivative with respect to activations: Finally, we need
+(a j−a k−a +a i)j+(a k+a j−a i−a )k]
0 1 2 3 0 1 2 3
to derive the loss with respect to the activations / outputs of
1 1
= [−a +a +a +a ]=− a∗ the previous layer a(L−1).
2 0 1 2 3 2 j
(15)
Theorem3. ThederivativeofLwithrespecttotheactivations
For the involutions, again we just state the final results, the a(L−1) of a QNN yields
detailed calculations can be found in the Appendix C-B. j
(cid:16) (cid:17)
∂yi = ∂yj = ∂yk = 1 a∗ (16) ∂L a j(L−1) = (cid:88) −1 e∗w .
∂w∗ ∂w∗ ∂w∗ 2 ∂a(L−1) 2 i i,j
j i∈K
Combining both parts to form the overall derivative yields
Proof. As multiple output neurons i ∈ K are connected to
∂L(y(w,b) = −1 e∗−1 a∗+ −1 ei∗1 a∗ a j(L−1), we have to take the sum of the respective derivatives:
∂w∗ 2 2 2 2
−1 1 −1 1
+
=
12
(cid:2)
ee ∗j∗ −2a ei∗ ∗+ −e2 j∗e −k∗ e2 k∗a (cid:3)∗
a∗ ( =6) −1 ea∗
(17) ∂L ∂(cid:16) aa (j L(L −− 11 ))(cid:17) =(cid:88)∂L(y ∂i( al) (( La −j(L 1)−1)))
4 2 j i∈K j
which concludes the proof
=(cid:88)∂L ∂y
i
+∂L ∂y ii
+
∂L ∂y ij
+
∂L ∂y ik
i∈K∂y i∂a j ∂y ii∂a j ∂y ij ∂a j ∂y ik ∂a j
2) Derivative with respect to the bias: Likewise, we need
to derive L with respect to the bias b(L). Thecalculationsfortheindividualpartsofthesumareanalog
i
to the ones when deriving with respect to the weights, hence
Theorem 2. The derivative of L with respect to the bias b(L) we will not list the detailed calculations here but report the
i
of a QNN is results.Thedetailed derivativescanbefound intheAppendix
C-D.
∂L(y(w,b) 1 1
=− e=− (d−y).
∂b∗ 2 2
∂y ∂(wa+b) ∂yi ∂(wa+b)i
= =w, = =0
∂a ∂a ∂a ∂a
(20)
Proof. Similarly to the weights, the derivative is defined as ∂yj ∂(wa+b)j ∂yk ∂(wa+b)k
follows: = =0, = =0
∂a ∂a ∂a ∂a
Finally, the derivative formulates as
∂L(y(w,b) ∂L ∂y ∂L ∂yi ∂L ∂yj ∂L ∂yk
= + + + .
∂b∗ ∂y ∂b∗ ∂yi∂b∗ ∂yj ∂b∗ ∂yk ∂b∗ ∂L(a j) = (cid:88) −1 e∗w .
∂a 2 i i,j
The left partial derivatives are already known from Equations j i∈K
(13) - (14) in IV-B1, hence we just have to calculate the right which concludes the proof.
partial derivatives:
In the same manner, we can obtain the derivatives
∂y ∂(wa+b) ∂(b)
∂b∗ = ∂b∗ = ∂b∗ ∂L ∂( aa ij) = (cid:88) − 21 (e i∗w i,j)i
∂ j i∈K
= (b +b i+b j+b k) (18)
=
∂ 1b (∗ 1+0 ii+1 jj+k2 k)=3
−0.5
∂L ∂( aa jjj) = i(cid:88) ∈K− 21 (e i∗w i,j)j (21)
4
For the three involutions yi, yj and yk, the detailed calcula- ∂L ∂a(a kj) = (cid:88) −1 2(e i∗w i,j)k
tions can be found in the Appendix C-C and we just state the j i∈K
results here: which we need for applying the chain rule.PREPRINT 7
4) Update rules for the last layer: Based on the previous ai(z), aj(z)and ak(z). Using an element-wise operating
calculations,theupdaterulesofweightsandbiasesattimestep activation function σ(·) to calculate a(l) = σ(z) = σ(z )+
0
n for the last layer are σ(z )i+σ(z )j+σ(z )k these are
1 2 3
1
w(L)(n+1)=w(L)(n)−λ e (n)a∗(L−1)(n) ∂a ∂
i,j i,j 2 i j
∂z
=
∂z
[σ(z 0)+σ(z 1)i+σ(z 2)j+σ(z 3)k]
1
b(L)(n+1)=b(L)(n)−λ e (n) 1
i i 2 i = [σ′(z )+σ′(z )+σ′(z )+σ′(z )]
4 0 1 2 3
whereby λ indicates the learning rate.
∂ai ∂
= [σ(z )+σ(z )i−σ(z )j−σ(z )k]
C. Hidden layer ∂z ∂z 0 1 2 3
For the hidden layers, usually activation functions are used. 1
= [σ′(z )+σ′(z )−σ′(z )−σ′(z )]
Thus, we obtain three parts we have to derive for. As we 4 0 1 2 3
know already from the quaternion chain rule, we cannot
simply combine them multiplicatively, especially for the three ∂aj ∂
= [σ(z )−σ(z )i+σ(z )j−σ(z )k]
components. Instead, we first start with deriving with respect ∂z ∂z 0 1 2 3
to the activation input, where the loss is a function 1
= [σ′(z )−σ′(z )+σ′(z )−σ′(z )]
4 0 1 2 3
(cid:16) (cid:16) (cid:17)(cid:17) (cid:16) (cid:17)
L(a(l))=L a(l) z(l) ;a(l) =σ z(l) .
∂ak ∂
conT th ine un e, ww ie thc dan eric vr ie na gte wt ih the ri en sv po el cu tti to on ws (lo )f ,bth (li )s ad ne dri av (a lt −iv 1e ).a Fn od
r
∂z
=
∂z
[σ(z 0)−σ(z 1)i−σ(z 2)j+σ(z 3)k]
i,j i i 1
simplicity and better readability, we will avoid the superscript = [σ′(z )−σ′(z )−σ′(z )+σ′(z )]
□(l) indicating the layer when we deal with the involutions to 4 0 1 2 3
prevent double superscripts. Combining the respective partial derivatives yields
1) Derivative with respect to the activation input:
Theorem4. ThederivativeofLwithrespecttotheactivation
∂L
∂L(cid:0) a(l)(z(l))(cid:1)
input z(l) of a QNN is =
i ∂z(l) ∂z(l)
∂∂ zL
(l)
=p 0σ′(z 0)+p 1σ′(z 1)i+p 2σ′(z 2)j+p 3σ′(z 3)k =p1 4[σ′(z 0)+σ′(z 1)+σ′(z 2)+σ′(z 3)]
i 1
=p◦σ′(z) +pi 4[σ′(z 0)+σ′(z 1)−σ′(z 2)−σ′(z 3)]
1
whereby p is the derivative of the next layer and σ(·) is an +pj [σ′(z )−σ′(z )+σ′(z )−σ′(z )]
elementwise activation function. 4 0 1 2 3
1
Proof. For deriving with respect to the activation input z i(l) +pk 4[σ′(z 0)−σ′(z 1)−σ′(z 2)+σ′(z 3)]
we have to calculate = 1(cid:2) p+pi+pj +pk(cid:3) σ′(z )
4 0
∂L(cid:0) a(l)(z(l))(cid:1) ∂L∂a ∂L ∂ai ∂L ∂aj ∂L ∂ak + 1(cid:2) p+pi−pj −pk(cid:3) σ′(z )
= + + + . 4 1
∂z(l) ∂a ∂z ∂ai ∂z ∂aj ∂z ∂ak ∂z
+ 1(cid:2) p−pi+pj −pk(cid:3) σ′(z )
We can derive the outer equation with respect to both, the 4 2
regular quaternion or it’s conjugate, and here it’s more conve- + 1(cid:2) p−pi−pj +pk(cid:3) σ′(z )
nient and simultaneously also computationally more efficient 4 3
to choose the regular quaternion as z is a direct result of the =p σ′(z )+p σ′(z )i+p σ′(z )j+p σ′(z )k
0 0 1 1 2 2 3 3
forward phase while z∗ is not. =p◦σ′(z)
To obtain the derivatives with respect to the involutions
ai, aj and ak we can simply take the known result and flip which concludes the proof.
the signs of the imaginary parts according to Equation (24).
Forthelastlayerweknow ∂L already.Foranarbitraryhidden Note that in the last step we used the alternative represen-
∂a
layer (l) we don’t know it yet. Hence, for better readability tations
and generalization, in the following we will call the result
simply p(l+1). As this is coming from the following layer we q = 1(cid:0) q+qi+qj +qk(cid:1) , q i= 1(cid:0) q+qi−qj −qk(cid:1)
assignthesuperscript□(l+1) Furthermore,thisnamingisalso 0 4 1 4
convenient since the result will change from the last layer to q 2j = 1 4(cid:0) q−qi+qj −qk(cid:1) , q 3k = 1 4(cid:0) q−qi−qj +qk(cid:1)
the hidden layer, but we can always refer to it as p.
For the right part of the partial derivatives, we first of (3) which can be obtained by multiplying with {1,i,j,k}
need to obtain the derivatives of a(z) and it’s involutions respectively. Furthermore, we can safely assume ∂L = pi,
∂aiPREPRINT 8
∂L =pj and ∂L =pk due to Equations (21) and following Proof. In the same manner, we can also compute the deriva-
∂aj ∂ak
(22). As required for the chain rule, we can further obtain tion with respect to the bias b(l)
i
∂L
=(p◦σ′(z))i
∂zi ∂L(z(l)(b(l))) ∂L ∂z ∂L ∂zi ∂L ∂zj ∂L ∂zk
i = + + + .
∂L
=(p◦σ′(z))k
∂b∗(l) ∂z ∂b∗ ∂zi∂b∗ ∂zj ∂b∗ ∂zk ∂b∗
∂zj
Using the known results
i
∂L
=(p◦σ′(z))k ∂z 1 ∂zi 1 ∂zj 1 ∂zk 1
∂zk =− , = , = , =
i ∂b∗ 2 ∂b∗ 2 ∂b∗ 2 ∂b∗ 2
following the same strategy.
from Equations (18) and (19) the combined derivative yields
2) Derivative with respect to the weights: Now we can
continue with deriving the loss function with respect to the
∂L(b)
=
∂L(z(b)) =q−1 +qi1 +qj1 +qk1
weights of a hidden layer. ∂b∗ ∂b∗ 2 2 2 2
Theorem 5. The derivative of L with respect to the weights
= 1(cid:0) −q+qi+qj +qk(cid:1)( =5) q∗.
2
of a hidden layer w(l) of a QNN yields
i,j which concludes the proof.
∂L(w)
=q∗a∗ 4) Derivativewithrespecttotheactivationsoftheprevious
∂w∗(l)
layer: Finally,weneedtoderivewithrespecttotheactivation
whereby q is the derivative of L with respect to the following ofthepreviouslayera(l−1).Again,therightpartialderivatives
j
activation function. are already known from Equation (20). As in the regular
backpropagation, in this case we need to consider all neurons
Proof. For the derivative with respect to the hidden layers
weights w(l), we need to calculate K where a respective activation a j(l−1) is input to.
i,j
Theorem 7. The derivative of a hidden layer of a QNN with
∂L(z(l)(w(l))) ∂L ∂z ∂L ∂zi ∂L ∂zj ∂L ∂zk respecttoitsinputs,theactivationsa(l−1) ofapreviouslayer,
= + + + . j
∂w∗(l) ∂z ∂w∗ ∂zi∂w∗ ∂zj ∂w∗ ∂zk ∂w∗ is (cid:16) (cid:17)
∂L a(l−1)
Just like in the case above, we assign the name ∂L = q j = (cid:88) qw
for the result. Then, we get the following partial
de∂ rz i( vl)
atives
∂a(l−1) i i,j
j i∈K
for the involutions:
whereby q is the derivative of L with respect to the following
∂y ∂y ∂y activation function
=qi, =qj, =qk
∂zi ∂zj ∂zk
Proof. Just as in Theorem 3 we have to take the sum of the
The right partial derivatives with respect to the weights are respective derivatives:
already known from Equations (15) and (16) in Subsection
IV ∂- zB, name 1ly
∂zi 1 ∂zj 1 ∂zk 1
∂L(cid:16)
a
j(l−1)(cid:17)
=(cid:88) ∂L(z i(l)(a j(l−1)))
=− a∗, = a∗, = a∗, = a∗ ∂a(l−1) ∂a(l−1)
∂w∗ 2 ∂w∗ 2 ∂w∗ 2 ∂w∗ 2 j i∈K j
whereby z = z(l), w = w(l) and a = a(l−1). Consequently, =(cid:88) ∂L ∂z i +∂L ∂z ii + ∂L ∂z ij + ∂L ∂z ik
the full derivatioi n is i,j i i∈K ∂z i∂a j ∂z ii∂a j ∂z ij ∂a j ∂z ik ∂a j
∂L(w) ∂L(z(w)) −1 1 1 1 By using the known derivatives from Equation (20) we obtain
= =q a∗+qi a∗+qj a∗+qk a∗
∂w∗ ∂w∗ 2 2 2 2 the final derivation
= 1(cid:0) −q+qi+qj +qk(cid:1) a∗ ( =5) q∗a∗ ∂L(cid:16) a(l−1)(cid:17)
2 j (cid:88)
= qw
which concludes the proof. ∂a(l−1) i i,j
j i∈K
3) Derivativewithrespecttothebiases: Likewise,weneed which concludes the proof.
the derivative of the loss function with respect to the biases
Likewise, we can obtain
of a hidden layer
Theorem 6. The derivative of L with respect to the bias of a ∂L(a j) = (cid:88) (a w )i
hidden layer b i(l) of a QNN yields ∂a ji i∈K i i,j
∂ ∂L b∗( (b l)) =q∗ ∂L ∂( aa jjj) = i(cid:88) ∈K(q iw i,j)j (22)
whereby q is the derivative of L with respect to the following ∂L(a j) = (cid:88) (qw )k.
activation function. ∂ak i i,j
j i∈KPREPRINT 9
Note that the superscripts □i,j,k indicate the quaternion applying the chain rule on the individual quaternion elements
involutions and the subscripts □ the indices within the y , y , y and y is
i,j 0 1 2 3
neural network. This result for a hidden layer (l) is then the
∂f (cid:18) ∂y ∂yi ∂yj ∂yk(cid:19)∗
starting point when calculating the derivatives for optimizing = s∗ +si∗ +sj∗ +sk∗ .
the previous layer (l−1) (compare Theorem 4). ∂x ∂x ∂x ∂x ∂x
5) Update rules for the hidden layer: Using the derived
results, we can formulate the update rules for the parameters
Proof. Asallinputelementscanbeinvolvedinthecalculation
in the hidden layers as
of the respective output elements y , y , y and y , when
0 1 2 3
calculating the derivative with respect to x , x , x and x
(cid:16) 0 1 2 3
w(l)(n+1)=w(l)(n)−λ p (l+1)σ′(z (l))+p (l+1)σ′(z (l))i all output elements have to be linked with the derivative
i,j i,j 0i 0i 1i 1i
components s , s , s and s to apply the chain rule on a
(cid:17)∗ 0 1 2 3
+p (l+1)σ′(z (l))j+p (l+1)σ′(z (l))k a∗(l−1) component level. Consequently, the derivative formulates as
2i 2i 3i 3i j
(cid:16) (cid:17)
=w(l)(n)−λ p(l)◦σ′(z(l)) a∗(l−1)
i,j i i j ∂f ∂f(y(x))
(cid:16) =
b(l)(n+1)=b(l)(n)−λ p (l+1)σ′(z (l))+p (l+1)σ′(z (l))i ∂x ∂x
i i 0i 0i 1i 1i
∂y ∂y ∂y ∂y
+p 2( il+1)σ′(z 2( il))j+p 3( il+1)σ′(z 3( il))k(cid:17)∗ =s 0∂x0
0
+s 1∂x1
0
+s 2∂x2
0
+s 3∂x3
0
(cid:18) (cid:19)
(cid:16) (cid:17)∗ ∂y ∂y ∂y ∂y
=b(l)(n)−λ p(l)◦σ′(z(l)) . + s 0 +s 1 +s 2 +s 3 i
i i i 0∂x 1∂x 2∂x 3∂x
1 1 1 1
(cid:18) (cid:19)
Furthermore, the new p j(l) becomes + s 0∂∂ xy 0 +s 1∂∂ xy 1 +s 2∂∂ xy 2 +s 3∂∂ xy 3 j
2 2 2 2
(cid:18) (cid:19)
p j(l) =
(cid:88)(cid:16)
p 0( il+1)σ′(z 0( il))+p 1( il+1)σ′(z 1( il))i + s
0∂∂ xy
0 +s
1∂∂ xy
1 +s
2∂∂ xy
2 +s
3∂∂ xy
3 k
3 3 3 3
i∈K (cid:18) (cid:19)
+p (l+1)σ′(z (l))j+p (l+1)σ′(z (l))k(cid:17) w(l) ( =4) 1(cid:0) s∗+si∗+sj∗+sk∗(cid:1) ∂y 0 + ∂y 0i+ ∂y 0j+ ∂y 0k
2i 2i 3i 3i i,j 4 ∂x ∂x ∂x ∂x
0 1 2 3
=
(cid:88)(cid:16)
p i(l+1)◦σ′(z
i(l))(cid:17)
w i( ,l j). + i(cid:0)
s∗+si∗−sj∗−sk∗(cid:1)(cid:18)
∂y 1 + ∂y 1i+ ∂y 1j+ ∂y
1k(cid:19)
4 ∂x ∂x ∂x ∂x
i∈K 0 1 2 3
(cid:18) (cid:19)
Thus, all required derivatives to optimize a QNN beginning + j(cid:0) s∗−si∗+sj∗−sk∗(cid:1) ∂y 2 + ∂y 2i+ ∂y 2j+ ∂y 2k
with the last layer backward to the first layer are derived and 4 ∂x 0 ∂x 1 ∂x 2 ∂x 3
(cid:18) (cid:19)
the quaternion backpropagation is complete. + k(cid:0) s∗−si∗−sj∗+sk∗(cid:1) ∂y 3 + ∂y 3i+ ∂y 3j+ ∂y 3k
4 ∂x ∂x ∂x ∂x
0 1 2 3
V. QUATERNIONBACKPROPAGATIONANDAUTOMATIC =s∗1(cid:20)(cid:18) ∂y
0 +
∂y
0i+
∂y
0j+
∂y 0k(cid:19)
DIFFERENTIATION 4 ∂x ∂x ∂x ∂x
0 1 2 3
(cid:18) (cid:19)
As shown by [56], current deep learning libraries can be ∂y ∂y ∂y ∂y
+i 1 + 1i+ 1j+ 1k
used to implement hypercomplex neural networks when using ∂x ∂x ∂x ∂x
0 1 2 3
elementwise activation functions, e.g. by utilizing the Kro- (cid:18) (cid:19)
∂y ∂y ∂y ∂y
necker product. These libraries usually make use of automatic +j 2 + 2i+ 2j+ 2k
∂x ∂x ∂x ∂x
0 1 2 3
differentiation, allowing to train the implemented hypercom- (cid:18) (cid:19)(cid:21)
∂y ∂y ∂y ∂y
plex models without the necessity to define the respective +k 3 + 3i+ 3j+ 3k
∂x ∂x ∂x ∂x
derivatives. Naturally, the question arises how the relation is 0 1 2 3
(cid:20)(cid:18) (cid:19)
between automatic differentiation derivatives and the GHR +si∗1 ∂y 0 + ∂y 0i+ ∂y 0j+ ∂y 0k
derivatives. Consequently, this subsection investigates if auto- 4 ∂x 0 ∂x 1 ∂x 2 ∂x 3
(cid:18) (cid:19)
maticdifferentiationcanbeusedtoimplementandtrainQNN ∂y ∂y ∂y ∂y
+i 1 + 1i+ 1j+ 1k
following the derived optimization. We specifically target the ∂x ∂x ∂x ∂x
0 1 2 3
automatic differentiation in PyTorch [57], although similar (cid:18) (cid:19)
∂y ∂y ∂y ∂y
considerations could be made for other packages. For details −j 2 + 2i+ 2j+ 2k
∂x ∂x ∂x ∂x
0 1 2 3
on the automatic differentiation in PyTorch, we refer to [58]. (cid:18) (cid:19)(cid:21)
∂y ∂y ∂y ∂y
−k 3 + 3i+ 3j+ 3k
∂x ∂x ∂x ∂x
0 1 2 3
A. The relation of the GHR chain rule and chain rule in (cid:20)(cid:18) (cid:19)
1 ∂y ∂y ∂y ∂y
automatic differentiation +sj∗ 0 + 0i+ 0j+ 0k
4 ∂x ∂x ∂x ∂x
0 1 2 3
Westartbymakingageneralstatementaboutthechainrule (cid:18) (cid:19)
∂y ∂y ∂y ∂y
in automatic differentiation: −i 1 + 1i+ 1j+ 1k
∂x ∂x ∂x ∂x
0 1 2 3
Theorem 8. Assume a function f(y(x)) and the derivative (cid:18) ∂y ∂y ∂y ∂y (cid:19)
∂f to be s=s +s i+s j+s k. Then the derivative when +j 2 + 2i+ 2j+ 2k
∂y 0 1 2 3 ∂x 0 ∂x 1 ∂x 2 ∂x 3PREPRINT 10
(cid:18) (cid:19)(cid:21)
∂y ∂y ∂y ∂y
−k 3 + 3i+ 3j+ 3k
∂x ∂x ∂x ∂x
0 1 2 3
(cid:20)(cid:18) (cid:19) Proof. The proof directly follows from Theorem 8 when
1 ∂y ∂y ∂y ∂y
+sk∗ 0 + 0i+ 0j+ 0k choosing s=q∗ and applying (10).
4 ∂x ∂x ∂x ∂x
0 1 2 3
(cid:18) (cid:19)
∂y ∂y ∂y ∂y
−i 1 + 1i+ 1j+ 1k ToputthatCorollaryinsimplewords,onecanconcludethat
∂x 0 ∂x 1 ∂x 2 ∂x 3 theoutputderivativewhenapplyingthechainruleinautomatic
(cid:18) (cid:19)
∂y ∂y ∂y ∂y differentiation is the conjugate of the GHR derivative, as long
−j 2 + 2i+ 2j+ 2k
∂x ∂x ∂x ∂x as the input derivative is also the conjugate.
0 1 2 3
(cid:18) (cid:19)(cid:21)
+k ∂y 3 + ∂y 3i+ ∂y 3j+ ∂y 3k Corollary 3. Consider the function f(y(x)) with ∂ ∂f y = q,
∂x 0 ∂x 1 ∂x 2 ∂x 3 ∂f = qi, ∂f = qj and ∂f = qk where the derivative ∂f
=s∗1(cid:20) ∂y
+
∂y
i+
∂y
j+
∂y k(cid:21) f∂ oy li lowing th∂ eyj chain rule o∂ fy tk he GHR calculus is ∂x∗
4 ∂x ∂x ∂x ∂x
0 1 2 3
+si∗1(cid:20) ∂yi
+
∂yi
i+
∂yi
j+
∂yi k(cid:21) ∂∂ xf
∗
=q ∂∂ xy
∗
+qi ∂∂ xy ∗i +qj ∂∂ xyj
∗
+qk ∂∂ xyk ∗.
4 ∂x ∂x ∂x ∂x
0 1 2 3
1(cid:20) ∂yj ∂yj ∂yj ∂yj (cid:21) Then, the relation
+sj∗ + i+ j+ k
4 ∂x 0 ∂x 1 ∂x 2 ∂x 3 ∂f ∂f
=
+sk∗1(cid:20) ∂yk
+
∂yk
i+
∂yk
j+
∂yk k(cid:21) ∂xAD ∂x∗ GHR
4 ∂x ∂x ∂x ∂x
0 1 2 3 holds, when
∂y ∂yi ∂yj ∂yk ∂f (cid:18) ∂f (cid:19)∗
=s∗ +si∗ +sj∗ +sk∗ . = .
∂x∗ ∂x∗ ∂x∗ ∂x∗ ∂y ∂y
AD GHR
In addition to this Theorem, we can further establish the
Proof. The proof also directly follows from Theorem 8 when
following useful Corollaries: choosing s=q∗.
Corollary 1. For a real valued function f(y(x)) and µ,ν ∈
H=1 where ∂f =q, the GHR chain rule From this, we can state that the derivatives for GHR and
∂y automatic differentiation are the same, when the variable
∂f ∂f ∂y ∂f ∂yi ∂f ∂yj ∂f ∂yk derivingforinautomaticdifferentiationistheconjugateofthe
= + + +
∂x ∂y∂x ∂yi ∂x ∂yj ∂x ∂yk ∂x one in GHR calculus and the derivative connected using the
chain rule is also the conjugate. This is of particular interest
simplifies to
as for the direction of steepest descent in the GHR calculus
∂f =q∂y +qi∂yi +qj∂yj +qk∂yk
.
one needs to derive with respect to W∗ and b∗ [23], [48] but
∂x ∂x ∂x ∂x ∂x when deriving for the individual components, we derive for
Proof. The relation w , w , w , w , b , b , b and w respectively, and thus not
0 1 2 3 0 1 2 3
(cid:18) ∂f(cid:19)η ∂fη the conjugate.
= In the following, we will continue with deriving the loss
∂q ∂qη
function as well as the derivatives with respect to w , b and
i,j i
holds ∀η ∈{i,j,k} [23]. For a real valued f, a for the automatic differentiation. Specifically, Corollary 2
j
fi =fj =fk =−ifi=−jfj =−kfk =f will be used together with the derivative with respect to the
loss function for the gradients flowing backward through the
and the corollary follows. model, and Corollary 3 for the derivatives with respect to the
parameters.
This simplification is used from now on.
Corollary 2. Consider the function f(y(x)) with ∂f = q,
∂y
∂f = qi, ∂f = qj and ∂f = qk where the derivative ∂f B. Forward phase and Loss function
∂yi ∂yj ∂yk ∂x
following the chain rule of the GHR calculus is Following [56], the real valued emulation of a quaternion
∂f ∂y ∂yi ∂yj ∂yk linear layer with n inputs and m outputs is calculated as
=q +qi +qj +qk .
∂x ∂x ∂x ∂x ∂x 
w w ··· w

a
 
b
 
z

1,1 1,2 1,n 1 1 1
Then, when applying the chain rule on the quaternion com- w 2,1 w 2,2 ··· w 2,na 2 b 2 z 2
ponents, the derivat ∂iv fe in aut (cid:18)om ∂a ftic dif (cid:19)fe ∗rentiation (AD) is  

. .
.
. .
.
... . .
.
   

. .
.
  + 

. .
.
  = 

. .
.
 

= w m,1 w m,2 ··· w m,n a n b m z m
∂xAD ∂xGHR
where w is the real valued matrix representation of the
when i,j
∂f (cid:18) ∂f (cid:19)∗ quaternion w i,j and a j, b i, z i the respective vector represen-
= .
tations of a , b and z.
∂y ∂y i i i
AD GHRPREPRINT 11
x 0 b
0
∂∂ zL 0∂ ∂z b00
z
0
× + σ y 0
∂∂ zL 0∂w∂z 00 x0 ... ∂∂ yL 0∂ ∂y z00 ∂∂ yL 0
x
1 ∂w∂ 0L x0∂ ∂w w0x 00 b
1
∂∂ zL 1∂ ∂z b11
z
1
× + σ y 1
w
0
∂w∂ 0L x1∂ ∂w w0x 01 ∂∂ zL 0∂w∂z 00 x1 ... ∂∂ yL 1∂ ∂y z11 ∂∂ yL 1
∂w∂ 0L x2∂ ∂w w0x 02 b
2
∂∂ zL 2∂ ∂z b22
z
2
× + σ y 2
x 2 ∂w∂ 0L x3∂ ∂w w0x 03 ∂∂ zL 0∂w∂z 00 x2 ... ∂∂ yL 2∂ ∂y z22 ∂∂ yL 2
b
3
∂∂ zL 3∂ ∂z b33
z
3
× + σ y 3
x 3 ∂∂ zL 0∂w∂z 00 x3 ... ∂∂ yL 3∂ ∂y z33 ∂∂ yL 3
Fig.1. Visualizationofthegradientflow
Accordingly, one partial calculation is carried out as fol- avoid confusion with the subscript indicating the quaternion
lows: component. This is also what we are going to do in the
     following whenever possible.
w −w −w −w a b
0 1 2 3 0 0
z i =w i,ja j+b i =  w w1
2
w w0
3
− ww 03 −w w2 1    a a1 2  +  b b1 2 
 C. Derivative with respect to the inputs of the activation
w −w w w a b
3 2 1 0 3 3 functions
The loss calculation as shown in Equation (12) remains When considering elementwise activation functions a(l) =
unchanged. σ(z)=σ(z )+σ(z )i+σ(z )j+σ(z )k, the derivatives in
0 1 2 3
Figure1furthershowstheinformationandgradientflowfor automatic differentiation are straight forward
anarbitraryhiddenlayer(l).Itisbasedonavisualizationofa
quaternion linear layer implementation in PyTorch [57] which ∂a0  ∂ σ(z ) σ′(z )
tw A he p ep pg e re n in nde cixr ipat D le ed . sF tu ao ys ri sn s tg i hm eP p sy l aiT c mo itr eyc ,h fowV reiz reju m[5 s at9 i] s n, h inos gwho ww ta en r ig ge hin t tswF w0ig 1au ,lr wte h 2o5 u angin dh      ∂ ∂∂ ∂ ∂a az z z0 1 21 2     =     ∂ ∂ ∂∂ ∂z z z0 1 2σ σ( (z z0 1 2) )     =    σ σ′ ′( (z z0 1 2) )    .
w 3 W. F eo ar gt ah ie nfi sn taa rl tl ba yye dr, erσ ivi is ngsim thp ely lot sh se wid ite hnt ri et sy pf eu cn tc tt oio yn.
and
∂ ∂a z33 ∂∂ z3σ(z 3) σ′(z 3)
i
specifically to its components y 0,y 1,y 2 and y 3. However, this Thus, as in Theorem 4, the derivatives with respect to
time we do not compose them back in a quaternion but keep the activations are an elementwise multiplication with the
themasavector,astheautomaticdifferentiationdoestreatthe derivative coming from the following layer (l + 1) during
outputs as completely independent of each other. Hence, we the backward phase. They don’t change anything in terms of
don’t have derivatives as in Equations (7), (8) or (9). Instead, conjugation and scaling.
we get the simple derivatives
 
∂L D. Derivative with respect to the activation output of the
∂ ∂y L0   (d 0−y 0)  e 0 previous layer
    ∂ ∂∂ ∂y yL L1 2    =−2   ((( ddd 321 −−− yyy 321 )))  =−2  e e e1 2 3   (23) oP ur to pp uo ts zit (i lo )n of3. aA qs us au tm ere nit oh ne d lie nr ei ava rti lv ae ye∂ r∂L z (lw )i tt oh bre esp qe ∗c =t to
q
0th −e
∂y3 q 1i−q 2j−q 3k. Then, the derivative with respect to its input
a(l−1) in automatic differentiation is
which is the vector representation of −2e . We can observe j
i
that the derivative with respect to the models output y is the (cid:16) (cid:17)
i ∂L a(l−1)
conjugate of the derivative obtained with the GHR calculus j = (cid:88) (qw )∗
and greater by a factor of 4. ∂a(l−1) i i,j
j i∈K
Note that in the calculations above we omitted the su-
perscript □(L) indicating the layer and the subscripts □ andhencetheconjugateasfortheGHRderivativeswith ∂L =
i,j ∂z
indicating the row and column for better readability and to q=q +q i+q j+q k.
0 1 2 3PREPRINT 12
Proof. TheprooffollowsdirectlyfromTheorem7andCorol- G. Implications
lary 2.
As we have shown, emulating QNN utilizing matrix-vector
Based on this, and the fact that an elementwise activation calculations in R4 and elementwise activation functions as
does not change anything in terms of conjugation, we can wellasusingautomaticdifferentiationyieldsproperparameter
conclude that the gradient flow backwards through the whole updates according to the derived quaternion backpropagation.
model is the conjugate of the GHR gradients as long as This can also be unterstood such that one has to apply the
the derivative with respect to the loss-function in automatic chain rule on the quaternion components instead of the whole
differentiation is the conjugate of the GHR derivative. From quaternion when not using the GHR calculus. The user just
Equation (23) we know that this is the case. Furthermore, this has to be aware of two facts: the derivative with respect to
result has the consequence that the gradient is also bigger by the loss in automatic differentiation needs to be real valued
a factor of four. The detailed calculations are further shown and the conjugate of the GHR derivative and it can introduce
in the Appendix E-A. a factor in the gradients, hence a learning rate correction
might be desired, and the gradient flowing backwards through
the model, the gradient with respect to the activations of
E. Derivative with respect to the weights
the previous layers, is the conjugate of the GHR derivatives.
Proposition 4. Assume the derivative ∂L with respect to the
∂z Hence, one can benefit from the advantages of automatic
output z i(l) of a quaternion linear layer (l) to be q∗ = q 0 − differentiation as long as the limitations are known. Due to
q 1i−q 2j−q 3k.Then,thederivativewithrespecttoitsweights Theorem8,arbitraryfunctionscanbeintroducedinthemodel
w(l) in automatic differentiation is as long as the gradient flowing backwards through the model
i,j
is the conjugate of the GHR gradient. Especially the usage of
∂L(w)
=q∗a∗ nonelementwiseoperatingactivationfunctionswithinQNNis
∂w(l)
ofparticularinteresthereandsomethingweaimtoinvestigate
and hence the same as for the GHR derivatives with ∂L = in future work.
∂z
q=q +q i+q j+q k.
0 1 2 3
Proof. TheproofdirectlyfollowsfromTheorem5andCorol-
VI. QUATERNIONICTIME-SERIESCOMPRESSION
lary 3. Having introduced the required theoretical fundamentals,
namely the quaternion backpropagation required to optimize
For the detailed calculations, compare with Appendix E-B.
QNN by means of gradient descent, we now proceed to de-
If we now insert the results from Equation (23), we obtain
scribe the quaternionic time-series compression methodology
∂L(w) where an overview is depicted in Figure 2.
=−2ea∗.
∂w(L) The sensor readings are processed using the compression
As we can see, the weight updates for the final layer are four algorithm which we describe in more detail in the following
times as big as in the GHR Calculus (compare Theorem 1). subsection.Thisyieldsashorter,quaternionvaluedtime-series
This also holds for arbitrary hidden layers due to Proposition which is then fed into the QNN architecture
3. Asafeatureextractor,weuseDropout-Conv1d-Activation
- Pooling blocks, followed by a flatten operation and Dropout
F. Derivative with respect to the bias - Linear - Activation blocks as a classifier. All of these layers
are quaternion valued which will be described in more detail
Proposition 5. Assume the derivative ∂L with respect to the
∂z later.Thefinallayer,however,isaconventionallinearlayeras
output z 1(l) of a quaternion linear layer (l) to be q∗ = q 0 − there is no natural way to express a probability distribution or
q 1i−q 2j−q 3k. Then, the derivative with respect to its bias calculateacross-entropyinquaternionspace.Hence,itserves
b(l) in automatic differentiation is the purpose of a learnable mapping back from H→R.
i
∂L(b) For comparison withreal valuedNNs, wecreate anequiva-
=q∗
lent by replacing all quaternion valued layers with real valued
∂b(l)
ones. As these are lacking the capability to deal with the
and hence the same as for the GHR derivatives with ∂L =
∂z quaternionic input data, we convert the input data of shape
q=q 0+q 1i+q 2j+q 3k. Hm×k into R4m×k, i.e. for each quaternion channel of length
Proof. TheproofdirectlyfollowsfromTheorem6andCorol- k we create 4 channels of length k, carrying the information
lary 3. about min, max, mean and std. Consequently, that yields the
relation of one quaternionic channel to four real valued ones.
Again,thedetailedcalculationsareshowninAppendixE-C.
Also here, if we insert the results from Equation (23), we
A. Compression Algorithm
obtain
∂L(b)
As an attempt to shrink time-series data, an intuitive and
=−2e
∂b(l) often used approach is to downsample the data, e.g. by using
and thus four times the derivative of Theorem 2. Likewise, the mean of the considered data segment. However, while
gradientsfourtimesasbigastheGHRgradientsholdsforthe doing so, there is an inherent information loss. For example,
hidden layers due to Proposition 3. the mean does not reflect if there was large deviation in thePREPRINT 13
Sensor 1
Sensor 2 Target d
Hm×k
y L(y,d)
.
.
.
Sensor m
n times m times
Fig.2. Illustrationofthequaternionictime-seriescompressionmethodology.
underlying data or if all values were relatively close to each By employing the Hamilton product throughout the NN
other. To overcome this issue, multiple characteristics instead architecture, we ensure that the four features are used in a
ofjust onecan beused inanattempt tolower theinformation contiguous manner and emphasize the combined nature as
lossandcreatemorerepresentative,compresseddata.Asthese wellastheirinterrelationshipwithintheproposedcompression
statistics are related to each other, we propose to compose representation
and gather them in a quaternion, specifically in the real and
threeimaginaryparts.Hence,weobtainamathematicalobject
carrying the compressed information. Naturally, this yields B. Quaternion valued Layer
a limit of four statistical values, for extensions one might
In addition to the already described quaternion linear layer,
consider octonion [60] or dual-quaternion [61], [62] based
for our model we use the following quaternion valued layers:
architectures. We will elaborate more on this in future work.
1) 1D Quaternion Convolution Layer: To extract features
Inthisstudy,wechosethemin,max,meanandstandarddevi-
from a quaternionic time-series, analog to the real valued
ationforthedescriptivepropertiesastheyareveryintuitivein
equivalent,aone-dimensionalconvolutioninquaternionspace
terms of representativeness and easy to obtain. Furthermore,
is required. Note that these layer can also be seen as a
this combination has showed good results in [63].
special case of the quaternion linear layer when treating the
Assumeamultivariatetime-seriesconsistingofmchannels
convolutionasamatrixmultiplicationbyatoeplitzmatrix.For
and n samples. Then, for all channels m, we divide the n
aquaternionvaluedinputofsize(B,C ,L )whereB isthe
in in
samples into k parts of equal length l. In case n is not
batchsize,C aretheinputchannelsandL isthelengthof
in in
divisible by l without remainder, then the last chunk is of a the multivariate input sequence, the ith element of the output
shorter length. Subsequently, for all chunks, we determine the
channel j using a kernel size K is calculated applying
minimum, maximum, mean and standard deviation, which we
storeinthefourcomponentsofaquaternion,Hence,wecreate
a mapping from Rm×n into Hm×k whereby the compression y =b
+C (cid:88)in−1K (cid:88)−1
x ⊗w
j,i j c,i+k j,c,k
rate is determined by the ratio l/4. The whole process is
c=0 k=0
further illustrated by Figure 3.
whereby the overall shape of the output is (B,C ,L ).
out out
Here, the weights are of size HCout,Cin,K and the bias is of
Chunk 1 Chunk 2 ··· Chunk k size HCout. For ease of notation, we omit the dilation and the
stride.
R 3 5 -1 7 4 9 3 1 -2 -2 0 5 ··· 7 3 2 1 2 6
2) Activations: We employ element-wise working activa-
−1.00 −2.00 1.00
tion functions as proposed in [6], [15], [16]. Assuming an
+9.00i +5.00i +7.00i
H ··· already known and tested activation ψ(·) like ReLU or Tanh,
+4.50j +0.83j +3.67j
+3.45k +2.79k +2.42k the application on a quaternion input is
R 0 -1 2 4 3 6 1 0 -4 3 1 -1 ··· 0 -1 -3 -5 -4 -1
−1.00 −4.00 −5.00 ψ(q)=ψ(q )+ψ(q )i+ψ(q )j+ψ(q )k.
0 1 2 3
+6.00i +3.00i +0.00i
H ···
+2.33j +0.00j −2.33j The design and usage of activation functions specifically
+2.58k +2.37k +1.97k
designed for the quaternion space provides an interesting
Fig.3. Exampleillustratingthecompressionalgorithm. direction for future research.
1
rosneS
2
rosneS
noisserpmoC
mhtiroglA tuoporD-Q d1vnoC-Q noitavitcA looP-Q nettalF tuoporD-Q raeniL-Q noitavitcA raeniLPREPRINT 14
3) 1D Quaternion Max-Pooling: Maximum-Pooling in A. Experimental setup
quaternionspacedirectlyraisesthequestion”Whatactuallyis
For our experimental evaluation, we use the TE dataset
the maximum of a set of multiple quaternions?” which has no
which consists of 21 fault cases and a regular operation case.
distinct answer. This is due to the fact that it is composed out
In total, 52 variables are considered. For the training split,
of four elements which can have their individual maximums,
480 samples are collected for the fault cases and 500 for the
however they still have a combined meaning. In [64], an
regular operation case. For the testing split, 960 samples are
approach is proposed based on the amplitude or magnitude
collected, whereby the fault was introduced after eight hours
of a quaternion: it is used to construct a guidance matrix
or 160 samples. We just consider the last 800 samples after
which selects the pooled quaternions based on the maximum
the introduction of the fault in this work. Furthermore, we
magnitude out of a quaternion valued matrix. Contrary, [65]
apply a sliding window of length 320 on the data. For our
opts to consider and pool the real and three imaginary parts
compression algorithm, we chose 40 chunks of length eight,
separately. In the following, we will describe both approaches
yielding a compression-rate of factor two. Hence, we obtain
in detail, as they tackle the problem from different views: data in the shape (N,52,4,40) and (N,208,40).
the 1D Component Max-Pooling puts more emphasis on the
To incorporate different architectural choices, we use a
individualmaximumswhereasthe1DMagnitudeMax-Pooling
combinationofoneconvolutionblockwiththreelinearblocks,
emphasizes the combined relation of a quaternion.
two convolution blocks with four linear blocks and three con-
a) 1D Quaternion Component Max-Pooling: This ap- volutionblockswithfourlinearblocksfromwhichwedesigna
proach considers the real part q 0 and the imaginary parts q 1, narrowvariantwithalowamountoftrainableparametersanda
q 2 and q 3 separately. Specifically, using a kernel size K, the widevariantwithahighamountoftrainableparameterseach.
output is composed of the maximum of the K individual real For the quaternion models, we use both presented pooling
parts and the respective K individual imaginary parts i, j and methods:thecomponentpoolingandthemagnitudepoolingto
k. This is described using compare them in different learning setups. For the real valued
comparison models, we also opt for two versions: The first
out(B ,C ,k)=
i j is a model with an equal amount of trainable parameters,
max R(in(N i,C j,stride×k+m)) the second is a version which creates the same number
m=0,...,kernel size−1
of features as the layer outputs. However, this comes with
+ max I(in(N ,C ,stride×k+m))i
m=0,...,kernel size−1 i j the effect of having approximately four times the trainable
+ max J(in(N ,C ,stride×k+m))j parameters in the non-quaternion variant in comparison to the
i j
m=0,...,kernel size−1 quaternion version. Furthermore, we add two baseline models
+ max K(in(N i,C j,stride×k+m))k. to showcase the effect of the proposed compression: one for
m=0,...,kernel size−1
the uncompressed input and one using just a simple mean-
b) 1D Quaternion Magnitude Max-Pooling: Contrary to resampling.Bothutilizethesameconfigastherealmodelwith
thecomponent-wiseapproach,hereweconsiderthequaternion equalparameters,howeverduetothedifferentinputshape,the
asawholeanduseitsnorm∥q∥asacomparisonmeasurement. first convolution layer and consequently the first linear layer
Then, out of K considered quaternions, the one with the after the flatten operation is different. Hence, we end up with
greatest norm is selected as the pooling output. We can a total of 36 different model architectures to compare for our
formulate this process as experiments. A detailed overview of the model configurations
can be found in Table I.
out(B i,C j,k)=in(N i,C j,l) where For each model, we run a tuning with 50 trials from which
l= argmax in(N ,C ,∥stride×k+m∥). 20arewarmuptrialsusingthePythonpackageOptuna[66]to
i j
m=0,...,kernel size−1 obtain an optimized set of hyperparameters. Specifically, we
tune for the following:
As the norm operation is not affected by the sign of the
quaternion components, using this approach it is more likely • Activation: ReLU, Tanh, Tanhshrink
that negative values remain after pooling in comparison to the • Learning Rate: 0.25×10−6 - 0.25×10−1
component max-pooling approach. • Batch-size: 24 - 28
• Dropout ratio: 0, 0.1, 0.2, 0.3, 0.4
VII. EXPERIMENTS
B. Supervised Learning
Thissectionprovidestheexperimentalevaluationofthepro- In the fully supervised learning experiments, we trained 36
posedmethodology.Westartwitharegularsupervisedtraining models in total. All of them were trained using the cross-
including a test of the robustness against random parameter entropy loss for 50 epochs each. This yielded the results
initialization. This is intended to prove the effectiveness of provided in Table II.
the proposed compression and to provide detailed comparison As we can see, in all cases, the quaternion model out-
of quaternion valued with real valued architectures. This is performed its real valued counterparts. This is the case even
followed by using the proposed approach in a self-supervised whenbeinginadisadvantageparameter-wisewhencomparing
learning setup to highlight state-of-the-art performance. Fi- with the real eq. features model configuration. Thus, we can
nally, a detailed comparison with the literature is provided. conclude that the quaternionic compression in combinationPREPRINT 15
TABLEI
USEDNEURALNETWORKCONFIGURATIONS
params convoutputchannel linearoutputsizes
quat 327030 32 128 8 22
low realequalparams 327535 25x4 133 30 22
realequalfeaturespace 1303926 32x4 512 32 22
1c3l
quat 1859702 96 256 8 22
high realequalparams 1859146 85×4 256 32 22
realequalfeaturespace 7432310 96×4 1024 32 22
quat 229366 32 32 128 128 8 22
low realequalparams 229342 25×4 25×4 128 96 32 22
realequalfeaturespace 911350 32×4 32×4 512 512 32 22
2c4l
quat 1189366 96 96 256 256 8 22
high realequalparams 1189749 73×4 73×4 259 256 32 22
realequalfeaturespace 4746742 96×4 96×4 1024 1024 32 22
quat 163958 32 32 32 128 128 8 22
low realequalparams 164998 23×4 22×4 22×4 96 60 24 22
realequalfeaturespace 649334 32×4 32×4 32×4 512 512 32 22
3c4l
quat 845686 96 96 96 256 256 8 22
high realequalparams 853118 66×4 66×4 66×4 124 60 24 22
realequalfeaturespace 3370870 96×4 96×4 96×4 1024 1024 32 22
TABLEII valued model with equal parameter count. Except for the
ACCURACIESIN%OBTAINEDUSINGTHESUPERVISEDLEARNINGSETUP. 1c3l low configuration, both quaternion architectures obtained
MAXIMUMVALUESAREHIGHLIGHTEDBOLD.
a higher mean and median accuracy than their real valued
counterparts. Additionally, the quaternion variants show the
quat real baseline
tendency to have smaller variation and confidence intervals.
comp. mag. equal equal uncom-
mean Hence, especially with a limited budget on training runs, they
pooling pooling params features pressed
are more likely to produce higher accuracies.
low 68.97 70.82 69.85 66.57 66.71 55.46
high 68.58 71.59 66.95 68.59 70.07 52.39 Thisoverallhighlightstheadvantageofusingthequaternion
low 71.16 70.41 69.54 68.69 61.77 59.71 valued models in this scenario. Furthermore, the magnitude
high 71.41 68.90 67.08 69.72 60.94 52.66
pooling seems to be favorable in this fully supervised training
low 69.87 69.03 67.42 66.96 57.82 56.49
high 69.70 69.98 66.90 68.99 53.13 56.82 setup, which we explain with its more regularizing behavior
in comparison to the component pooling.
with the quaternion valued model architecture outperforms C. Self-Supervised Learning
the real valued counterparts being fed the same data in these To further test state-of-the-art performance, we use the
experiments. In terms of quaternion pooling, no tendency is contrastive learning approach SimCLR-TS proposed by [67]
observable at this point as both versions achieved the respec- which established a new baseline on the TE dataset. To
tive highest accuracy three times. With the one exception of prove the superiority of quaternion valued architectures in
the 1c3l high configuration for the uncompressed baseline, in the proposed application, we replicate the contrastive learning
all baseline runs a significant drop in performance is existent. setup and transfer it to our use case. For detailed information
This highlights the advantages of compressing a long time- on SimCLR-TS, we refer to the original work.
series for a fault classification task as proposed. We change the models in such a way that we only use the
To investigate the effect of random initialization on the convolutionalblockswithaflattenoperationattheendforthe
respective architectures, for each model we take the best contrastive learning. The following linear evaluation is done
set of hyperparameter, obtained from the experiments above, with a single linear layer, similar to SimCLR-TS. The tuning
and train it again for 50 times using this specific set of setupremainsthesame,howeverweaddasecondlearningrate
hyperparameter. This yields the mean accuracies reported in as SimCLR-TS is a two-staged methodology where a shared
Table III and maximum accuracies reported in Table IV. learning rate does not necessary make sense. The contrastive
Furthermore, the accuracy distribution is shown in Figure 4. learning part is again trained for 50 epochs, for the following
It can be observed that one of the quaternion variants linear evaluation however it is sufficient to use 20 training
achieves the highest overall accuracies over all layer and epochs.Furthermore,inthisexperimentweusethetwomodel
parameterconfigurations,wherebytheresultsarefivetoonein configurations which obtained the highest accuracies in the
favor of the quaternion magnitude pooling. Similar outcomes supervisedlearningsetup,namely2c4lhighand3c4llow,with
can be identified in terms of the mean performance as four which we could obtain the results displayed in Table V.
times the quaternion magnitude pooling performed best, one As we can see, again the quaternion valued models out-
time the quaternion component pooling and one time the real perform the real valued counterpart. The real-valued models
l3c1
l4c2
l4c3PREPRINT 16
70 70
60 60
50 50
40 40
Quat. Quat. RealEq. RealEq. Baseline Baseline Quat. Quat. RealEq. RealEq. Baseline Baseline
Comp. Mag. Params Features Unc. Mean Comp. Mag. Params Features Unc. Mean
(a)1c3llow (b)1c3lhigh
70 70
60 60
50 50
40 40
Quat. Quat. RealEq. RealEq. Baseline Baseline Quat. Quat. RealEq. RealEq. Baseline Baseline
Comp. Mag. Params Features Unc. Mean Comp. Mag. Params Features Unc. Mean
(c)2c4llow (d)2c4lhigh
70 70
60 60
50 50
40 40
Quat. Quat. RealEq. RealEq. Baseline Baseline Quat. Quat. RealEq. RealEq. Baseline Baseline
Comp. Mag. Params Features Unc. Mean Comp. Mag. Params Features Unc. Mean
(e)3c4llow (f)3c4lhigh
Fig.4. Obtainedresultswhentrainingfor50timeswiththebestdeterminedhyperparameteranddifferentrandomparameterinitialization.Notethatin(f)
oneoutlierat4.55%fortherealeq.featuresboxplotisnotdisplayed.
benefit from the higher number of trainable parameters, but D. Literature comparison
stillremaininferior.Hence,alsoinamoreadvanced,state-of-
the-arttrainingsetup,theusageoftheHamiltonproductwithin TheTEdatasetiswidelyusedinmachinelearningresearch,
the convolutions allows for performance benefits and enables however it also bears some problems in terms of compara-
models with fewer trainable parameters. In these experiments, bility: Not always all of the 21 fault cases are used, and
the component pooling outperformed the magnitude pooling. also sometimes not all sensor channels are used, hurting the
We attribute that to the fact that most of the training work is comparability.Further,alsovaryingwindowingapproachesare
done in the self-supervised part, where the more regularizing used to create the samples for training and testing. Finally,
effect of the magnitude pooling is not as beneficial as in a different evaluation metrics exist. Nevertheless, we want to
fullysupervisedtrainingsetup.Instead,themodelperformance compare our results with other works to put our proposed
benefits from the higher degree of flexibility offered by the approach into context. Therefore, we provide a detailed set of
component pooling. results, obtained using a variety of approaches, in Table VI.
Thefirstworkusingallfaultcaseswewanttocomparewith
%
ni
ycaruccA
%
ni
ycaruccA
%
ni
ycaruccA
%
ni
ycaruccA
%
ni
ycaruccA
%
ni
ycaruccAPREPRINT 17
TABLEIII
MEANACCURACIESIN%INCLUDINGTHE95%CONFIDENCEINTERVALOBTAINEDINTHERANDOMINITIALIZATIONTEST.MAXIMUMVALUESARE
HIGHLIGHTEDBOLD.
quat real baseline
comp. mag. equal equal uncom-
mean
pooling pooling params features pressed
low 66.57±3.09 67.20±3.13 67.92±3.15 64.16±3.13 65.69±3.29 53.28±3.23
high 66.16±3.19 67.91±2.42 65.00±3.23 64.79±3.43 60.31±5.94 46.85±3.98
low 66.61±3.48 67.03±4.39 63.90±6.15 63.27±5.93 56.81±4.80 54.28±4.28
high 68.34±2.35 66.88±4.14 65.53±4.31 64.44±4.18 57.97±6.08 45.15±5.29
low 66.14±5.19 66.26±4.11 61.87±7.26 63.51±5.09 51.35±6.43 52.37±6.50
high 65.98±3.46 66.72±4.60 63.96±4.84 62.51±24.25 52.45±5.96 47.43±4.95
TABLEIV the regular operation case were omitted when applying a
MAXACCURACIESIN%OBTAINEDINTHERANDOMINITIALIZATION randomforestmodel.[70]proposesamultiscaleconvolutional
TEST.MAXIMUMVALUESAREHIGHLIGHTEDBOLD.
neuralnetworkandlongshort-termmemory(MCNN-LSTM),
however they only apply it on cases 0, 1, 2, 4, 8, 10, 12, 13,
quat real baseline
14,17and18oftheTE-Process.Similarly,thereis[72]which
comp. mag. equal equal uncom-
mean proposesamultichannelone-dimensionalconvolutionalneural
pooling pooling params features pressed
network (MC1-DCNN), however they omit the regular oper-
low 69.93 71.77 70.92 68.21 69.13 56.28
high 69.80 70.21 68.48 68.12 67.85 51.41 ation case 0 and also incorporate features from the frequency
low 71.18 71.91 71.30 69.19 62.31 57.88 domain whereas we only work with compressed data in the
high 71.23 72.79 70.55 68.27 63.17 49.52
time domain.
low 72.02 71.03 67.89 67.50 58.26 61.06
high 69.41 70.73 69.85 68.92 58.08 52.69 In contrast to leaving out fault cases, [73] leaves out sensor
channels and only utilizes 33 channels of the dataset when
doingprocessmonitoringbasedonunstableneuronoutputsin
TABLEV
deep belief networks, and hence lacks comparability. Finally,
ACCURACIESIN%OBTAINEDUSINGTHESELF-SUPERVISEDLEARNING
SETUP.MAXIMUMVALUESAREHIGHLIGHTEDBOLD. [74] utilizes a CNN-pooling based architecture, however they
do binary classification instead of having one model for all
quat real baseline cases.EventhoughtheyreportF1-scoresinsteadofaccuracies,
comp. mag. equal equal uncom- we can state that our multi-class model is able to outperform
mean
pooling pooling params features pressed the multiple binary ones.
2c4lhigh 83.90 78.26 75.69 78.79 75.92 73.47
3c4llow 78.38 76.52 76.25 77.51 72.57 65.68
VIII. CONCLUSION
In this paper, we proposed a novel time-series compression
is [68] where Support Vector Machines (SVM) and Principal approach for time-series data where we encode the mini-
Component Analysis (PCA) is used for fault classification mum, maximum, mean and standard deviation of time-series
on the TE dataset. Another relevant work that uses all error extracts in a quaternion, yielding a quaternion valued time-
cases is [67] from which we adapted the semi-supervised series. Through the usage of the Hamilton product as the
learning. All of them, we could outperform with our best fundamentalmultiplicationwithinQNNs,wewereabletolink
accuracyof83.9%obtainedusingthe2c4lhighconfiguration. these features in the quaternion convolution layer. Further, we
Compared to SimCLR-TS, the biggest performance gains developed a novel quaternion backpropagation utilizing the
could be achieved on the difficult cases 3, 9 and 15 at the GHR-Calculus. After introducing the required fundamentals
cost of performance losses in cases 0, 13 and 18. and quaternion mathematics, we showed that by using plain
Further, [69] employed stacked supervised auto-encoder, partial derivatives with respect to the quaternion components
however they left out the most difficult cases 3, 9 15 which as in other approaches to quaternion backpropagation, the
is a practice not untypical on TE. Since SimCLR-TS also product and more critical, the chain rule, does not hold. By
reported a result for that setup, we likewise performed a applyingtheGHRcalculus,we endupwithderivativeswhich
training on this reduced cases setup. This yielded an accuracy do, to create our novel quaternion backpropagation algorithm.
of 93.58%, which is on par with SSAE. In almost all fault We further provided insights on the relation of automatic
cases improvements over SimCLR-TS could be achieved, the differentiation and quaternion backpropagation, and pointed
only two exceptions are the regular operation case 0 with a out a scenario where automatic differentiation can be used to
slight drop of about 0.03%, the fault case 13 with a drop of train QNN.
45,51% and case 16 with a drop of 0,61%, preventing an In the conducted experiments, quaternion valued models
even bigger improvement. were able to outperform real valued counterparts in a fully
Inaddition,thereareotherconfigurationsusingjustasubset supervisedlearningsetup,utilizingthesamecompresseddata,
of the 22 cases of TE. In [71], Cases 16 - 21 as well as and also a baseline without any compression and a simple
l3c1
l4c2
l4c3
l3c1
l4c2
l4c3PREPRINT 18
TABLEVI
COMPARISONOFTHEACHIEVEDACCURACIESUSINGCONTRASTIVELEARNINGANDTHEPROPOSEDCOMPRESSIONMETHODWITHOTHERAPPROACHES
Allcases Without3,9,15 other
SVM PCA SimCLR- 2c4l SSAE SimCLR- 2c4l MCNN- Enhanced MC1- UN-DBN CNN
Case
[68] [68] TS[67] high [69] TS[67] high LSTM[70] RF[71] DCNN[72] [73]‡ [74]†
Case0 19.27 19.69 55.12 37.42 96.48 98.37 98.34 93.5 - - - -
Case1 88.44 88.54 96.29 100.0 100.0 99.43 100.0 99.9 99 100.0 1 0.9139
Case2 86.04 89.06 100.0 100.0 100.0 99.86 100.0 99.3 98 100.0 0.99 0.8796
Case3 15.73 21.25 25.57 73.18 - - - - 35 83.48 0.08 0.5059
Case4 58.02 81.35 96.57 100.0 100.0 87.71 100.0 100 97 99.22 1 0.9973
Case5 64.79 87.81 83.86 100.0 100.0 79.71 100.0 - 100 90.40 1 0.9035
Case6 71.88 89.58 99.14 100.0 100.0 98.86 100.0 - 100 93.10 1 0.9150
Case7 88.13 88.75 100.0 100.0 100.0 100.0 100.0 - 100 100.0 1 0.9155
Case8 45.10 83.96 86.71 96.47 96.32 95.29 100.0 98.8 76 99.27 0.98 0.8295
Case9 12.92 22.60 29.43 53.85 - - - - 23 74.60 0.02 0.4953
Case10 27.08 76.98 91.71 100.0 62.16 97.14 98.54 99.9 81 93.75 0.84 0.7005
Case11 14.90 70.21 99.29 100.0 95.93 98.57 100.0 - 76 95.45 0.91 0.6016
Case12 52.40 87.08 95.14 100.0 99.87 100.0 100.0 97.0 89 100.0 1 0.8666
Case13 35.10 69.58 57.86 32.02 89.88 71.29 25.78 98.2 30 99.14 0.95 0.4692
Case14 62.19 88.23 99.57 100.0 100.0 100.0 100.0 100 100 100.0 1 0.8868
Case15 22.19 26.98 02.71 19.13 - - - - 28 73.50 0.16 0.4354
Case16 16.77 73.65 98.57 98.96 54.66 99.57 98.96 - - 100.0 0.68 0.6684
Case17 53.65 75.94 100.0 100.0 100.0 99.86 100.0 100 - 100.0 0.98 0.7711
Case18 30.94 73.54 99.00 64.24 93.56 47.29 56.34 99.7 - 98.43 0.89 0.8274
Case19 51.25 85.73 100.0 100.0 79.37 99.57 100.0 - - 100.0 0.98 0.7087
Case20 44.90 79.69 99.14 100.0 92.51 100.0 100.0 - - 94.12 0.86 0.7288
Case21 8.65 85.63 81.71 70.48 78.06 93.29 100.0 - 6 70.59 0.50 0.3126
Overall 44.11* 71.17* 81.43 83.90 91.52 93.00 93.58 98.8* 71.46 94.08 80.10* 0.7301*
*Selfcalculatedsinceitisnotstatedinoriginalwork
‡HereaDetectionRate(DR)isstated
†HeretheF1-scoreisstatedinsteadofthefaultdetectionaccuracy
mean-resampling baseline. In a random initialization test, we in Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and
could further show that these results hold for 50 different Structure in Statistical Translation. Doha, Qatar: Association for
ComputationalLinguistics,2014,pp.103–111.
randommodelinitialization.Furthermore,thequaternionvari-
[2] D.Bahdanau,K.Cho,andY.Bengio,“NeuralMachineTranslationby
ants showed the tendency to have lower variation and smaller JointlyLearningtoAlignandTranslate,”2014,publisher:arXivVersion
confidence intervals. Also, in a self-supervised learning setup Number:7.
[3] L.B.HinkleandV.Metsis,“ModelEvaluationApproachesforHuman
based on SimCLR-TS we could prove the superiority of
Activity Recognition from Time-Series Data,” in Artificial Intelligence
the quaternion valued architectures, even outperforming the inMedicine,A.Tucker,P.HenriquesAbreu,J.Cardoso,P.PereiraRo-
results from SimCLR-TS. Simultaneously, we compared two drigues,andD.Rian˜o,Eds. Cham:SpringerInternationalPublishing,
2021,vol.12721,pp.209–215,seriesTitle:LectureNotesinComputer
quaternionmax-poolingvariants,wherethemagnitudepooling
Science.
showed an overall performance advantage in the fully super-
[4] S.Nakagome,T.P.Luu,Y.He,A.S.Ravindran,andJ.L.Contreras-
visedlearningsetupduetoitsmoreregularizingbehavior,and Vidal, “An empirical comparison of neural networks and machine
the component pooling in the self supervised setup due to its learningalgorithmsforEEGgaitdecoding,”ScientificReports,vol.10,
no.1,p.4372,Mar.2020.
higher degrees of flexibility.
[5] E.Keogh,K.Chakrabarti,M.Pazzani,andS.Mehrotra,“Dimensionality
Hence,wecanconcludethatquaternionvaluedarchitectures ReductionforFastSimilaritySearchinLargeTimeSeriesDatabases,”
offer a performance advantage in this compression methodol- Knowledge and Information Systems, vol. 3, no. 3, pp. 263–286, Aug.
2001.
ogy over their real valued counterparts throughout all used
[6] X.Zhu,Y.Xu,H.Xu,andC.Chen,“QuaternionConvolutionalNeural
model configurations and training setups. Networks,”Mar.2019,arXiv:1903.00658[cs].
In future work, we plan to investigate the impact or contri- [7] T. Parcollet, M. Morchid, and G. Linares, “Quaternion Convolutional
NeuralNetworksforHeterogeneousImageProcessing,”inICASSP2019
butionoftheindividualpropertiesusedtoformthequaternion
-2019IEEEInternationalConferenceonAcoustics,SpeechandSignal
for the final classification result. Based on these outcomes, Processing (ICASSP). Brighton, United Kingdom: IEEE, May 2019,
other features representing the time-series extracts shall be pp.8514–8518.
[8] A. Hirose, Ed., Complex-valued neural networks: advances and appli-
observedtofindtheidealcombinationoffourproperties.Also,
cations,ser.IEEEPressseriesoncomputationalintelligence. Hoboken,
we aim to investigate activation functions specifically tailored N.J:JohnWiley&SonsInc,2013,oCLC:ocn812254892.
for quaternion models instead of using elementwise operating [9] C. Trabelsi, O. Bilaniuk, Y. Zhang, D. Serdyuk, S. Subramanian, J. F.
functions, with a particular focus on their derivatives and how Santos, S. Mehri, N. Rostamzadeh, Y. Bengio, and C. J. Pal, “Deep
ComplexNetworks,”2017,publisher:arXivVersionNumber:4.
the relation with automatic differentiation is in this scenario.
[10] N. Benvenuto and F. Piazza, “On the complex backpropagation algo-
rithm,” IEEE Transactions on Signal Processing, vol. 40, no. 4, pp.
REFERENCES 967–969,Apr.1992.
[11] Y. Ishizuka, S. Murai, Y. Takahashi, M. Kawai, Y. Taniai, and
[1] K.Cho,B.VanMerrienboer,D.Bahdanau,andY.Bengio,“OntheProp- T.Naniwa,“ModelingWalkingBehaviorofPoweredExoskeletonBased
erties of Neural Machine Translation: Encoder–Decoder Approaches,” on Complex-Valued Neural Network,” in 2018 IEEE InternationalPREPRINT 19
ConferenceonSystems,Man,andCybernetics(SMC). Miyazaki,Japan: [32] X.Kitsios,P.Liakos,K.Papakonstantinopoulou,andY.Kotidis,“Sim-
IEEE,Oct.2018,pp.1927–1932. Piece:HighlyAccuratePiecewiseLinearApproximationthroughSimilar
[12] C.-A. Popa, “Complex-valued convolutional neural networks for real- Segment Merging,” Proc. {VLDB} Endow., vol. 16, no. 8, pp. 1910–
valuedimageclassification,”in2017InternationalJointConferenceon 1922,2023.
Neural Networks (IJCNN). Anchorage, AK, USA: IEEE, May 2017, [33] F. Eichinger, P. Efros, S. Karnouskos, and K. Bo¨hm, “A time-series
pp.816–822. compressiontechniqueanditsapplicationtothesmartgrid,”TheVLDB
[13] D.Hayakawa,T.Masuko,andH.Fujimura,“ApplyingComplex-Valued Journal,vol.24,no.2,pp.193–218,Apr.2015.
Neural Networks to Acoustic Modeling for Speech Recognition,” in [34] D.Lemire,“ABetterAlternativetoPiecewiseLinearTimeSeriesSeg-
2018Asia-PacificSignalandInformationProcessingAssociationAnnual mentation,”inProceedingsofthe2007SIAMInternationalConference
Summit and Conference (APSIPA ASC). Honolulu, HI, USA: IEEE, onDataMining. SocietyforIndustrialandAppliedMathematics,Apr.
Nov.2018,pp.1725–1731. 2007,pp.545–550.
[14] W. Shen, B. Zhang, S. Huang, Z. Wei, and Q. Zhang, “3D-Rotation- [35] A.Ukil,S.Bandyopadhyay,andA.Pal,“IoTDataCompression:Sensor-
EquivariantQuaternionNeuralNetworks,”inComputerVision–ECCV AgnosticApproach,”in2015DataCompressionConference. Snowbird,
2020,A.Vedaldi,H.Bischof,T.Brox,andJ.-M.Frahm,Eds. Cham: UT,USA:IEEE,Apr.2015,pp.303–312.
SpringerInternationalPublishing,2020,vol.12365,pp.531–547,series [36] S. Di and F. Cappello, “Fast Error-Bounded Lossy HPC Data Com-
Title:LectureNotesinComputerScience. pressionwithSZ,”in2016IEEEInternationalParallelandDistributed
[15] T. Parcollet, M. Morchid, P.-M. Bousquet, R. Dufour, G. Linares, ProcessingSymposium(IPDPS). Chicago,IL,USA:IEEE,May2016,
and R. De Mori, “Quaternion Neural Networks for Spoken Language pp.730–739.
Understanding,”in2016IEEESpokenLanguageTechnologyWorkshop [37] S. Chandak, K. Tatwawadi, C. Wen, L. Wang, J. Aparicio Ojea, and
(SLT). SanDiego,CA:IEEE,Dec.2016,pp.362–368. T.Weissman,“LFZip:LossyCompressionofMultivariateFloating-Point
[16] T. Parcollet, M. Ravanelli, M. Morchid, G. Linare`s, and R. De Mori, TimeSeriesDataviaImprovedPrediction,”in2020DataCompression
“Speechrecognitionwithquaternionneuralnetworks,”2018,publisher: Conference(DCC). Snowbird,UT,USA:IEEE,Mar.2020,pp.342–
arXivVersionNumber:1. 351.
[17] C. J. Gaudet and A. S. Maida, “Deep Quaternion Networks,” in 2018 [38] J. Azar, A. Makhoul, R. Couturier, and J. Demerjian, “Robust IoT
International Joint Conference on Neural Networks (IJCNN). Rio de time series classification with data compression and deep learning,”
Janeiro:IEEE,Jul.2018,pp.1–8. Neurocomputing,vol.398,pp.222–234,Jul.2020.
[39] O. P. Concha, R. Y. D. Xu, and M. Piccardi, “Compressive Sensing
[18] U. Onyekpe, V. Palade, S. Kanarachos, and S.-R. Christopoulos, “A
of Time Series for Human Action Recognition,” in 2010 International
Quaternion Gated Recurrent Unit Neural Network for Sensor Fusion,”
Information,vol.12,no.3,p.117,Mar.2021.
ConferenceonDigitalImageComputing:TechniquesandApplications.
Sydney,Australia:IEEE,Dec.2010,pp.454–461.
[19] P.Arena,L.Fortuna,G.Muscato,andM.Xibilia,“MultilayerPercep-
[40] J. Azar, A. Makhoul, M. Barhamgi, and R. Couturier, “An energy
tronstoApproximateQuaternionValuedFunctions,”NeuralNetworks,
efficient IoT data compression approach for edge machine learning,”
vol.10,no.2,pp.335–342,Mar.1997.
FutureGenerationComputerSystems,vol.96,pp.168–175,Jul.2019.
[20] T.Nitta,“Aquaternaryversionoftheback-propagationalgorithm,”Pro-
[41] L.KaramitopoulosandG.Evangelidis,“ADispersion-BasedPAARep-
ceedings of ICNN’95 - International Conference on Neural Networks,
resentationforTimeSeries,”in2009WRIWorldCongressonComputer
vol.5,pp.2753–2756,1995.
ScienceandInformationEngineering. LosAngeles,CaliforniaUSA:
[21] T. Parcollet, M. Ravanelli, M. Morchid, G. Linare`s, C. Trabelsi,
IEEE,2009,pp.490–494.
R.DeMori,andY.Bengio,“QuaternionRecurrentNeuralNetworks,”
[42] B. Lkhagva, Yu Suzuki, and K. Kawagoe, “New Time Series Data
2018,publisher:arXivVersionNumber:3.
RepresentationESAXforFinancialApplications,”in22ndInternational
[22] N.Matsui,T.Isokawa,H.Kusamichi,andF.Peper,“QuaternionNeural
ConferenceonDataEngineeringWorkshops(ICDEW’06). Atlanta,GA,
NetworkwithGeometricalOperators,”J.Intell.FuzzySyst.,no.15,pp.
USA:IEEE,2006,pp.x115–x115.
149–164,2004.
[43] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning repre-
[23] D. Xu, C. Jahanchahi, C. C. Took, and D. P. Mandic, “Enabling
sentationsbyback-propagatingerrors,”Nature,vol.323,no.6088,pp.
quaternionderivatives:thegeneralizedHRcalculus,”RoyalSocietyOpen
533–536,Oct.1986.
Science,vol.2,no.8,p.150255,Aug.2015.
[44] H. Leung and S. Haykin, “The complex backpropagation algorithm,”
[24] D.Xu,Y.Xia,andD.P.Mandic,“OptimizationinQuaternionDynamic IEEETransactionsonSignalProcessing,vol.39,no.9,pp.2101–2104,
Systems: Gradient, Hessian, and Learning Algorithms,” IEEE transac-
Sep.1991.
tionsonneuralnetworksandlearningsystems,vol.27,no.2,pp.249–
[45] T.Nitta,“AnExtensionoftheBack-PropagationAlgorithmtoComplex
261,Feb.2016. Numbers,”NeuralNetworks,vol.10,no.8,pp.1391–1415,Nov.1997.
[25] D.Blalock,S.Madden,andJ.Guttag,“Sprintz:TimeSeriesCompres- [46] D.T.LaCorteandY.M.Zou,“Newton’sMethodBackpropagationfor
sionfortheInternetofThings,”ProceedingsoftheACMonInteractive, Complex-ValuedHolomorphicMultilayerPerceptrons,”2014,publisher:
Mobile,WearableandUbiquitousTechnologies,vol.2,no.3,pp.1–23, arXivVersionNumber:1.
Sep.2018. [47] K. Kreutz-Delgado, “The Complex Gradient Operator and the CR-
[26] A. Go´mez-Brando´n, J. R. Parama´, K. Villalobos, A. Illarramendi, and Calculus,”2009,publisher:arXivVersionNumber:1.
N. R. Brisaboa, “Lossless compression of industrial time series with [48] D.P.Mandic,C.Jahanchahi,andC.C.Took,“AQuaternionGradient
directaccess,”ComputersinIndustry,vol.132,p.103503,Nov.2021. OperatorandItsApplications,”IEEESignalProcessingLetters,vol.18,
[27] G.Campobello,A.Segreto,S.Zanafi,andS.Serrano,“RAKE:Asimple no.1,pp.47–50,Jan.2011.
andefficientlosslesscompressionalgorithmfortheInternetofThings,” [49] E. Grassucci, D. Comminiello, and A. Uncini, “A Quaternion-Valued
in201725thEuropeanSignalProcessingConference(EUSIPCO). Kos, Variational Autoencoder,” in ICASSP 2021 - 2021 IEEE International
Greece:IEEE,Aug.2017,pp.2581–2585. Conference on Acoustics, Speech and Signal Processing (ICASSP).
[28] J. Spiegel, P. Wira, and G. Hermann, “A Comparative Experimental Toronto,ON,Canada:IEEE,Jun.2021,pp.3310–3314.
StudyofLosslessCompressionAlgorithmsforEnhancingEnergyEffi- [50] E. Grassucci, E. Cicero, and D. Comminiello, “Quaternion Generative
ciencyinSmartMeters,”in2018IEEE16thInternationalConferenceon Adversarial Networks,” in Generative Adversarial Learning: Architec-
IndustrialInformatics(INDIN). Porto:IEEE,Jul.2018,pp.447–452. tures and Applications, R. Razavi-Far, A. Ruiz-Garcia, V. Palade, and
[29] H. S. Mogahed and A. G. Yakunin, “Development of a Lossless Data J.Schmidhuber,Eds. Cham:SpringerInternationalPublishing,2022,
Compression Algorithm for Multichannel Environmental Monitoring vol.217,pp.57–86,seriesTitle:IntelligentSystemsReferenceLibrary.
Systems,” in 2018 XIV International Scientific-Technical Conference [51] A.MuppidiandM.Radfar,“SpeechEmotionRecognitionUsingQuater-
on Actual Problems of Electronics Instrument Engineering (APEIE). nion Convolutional Neural Networks,” in ICASSP 2021 - 2021 IEEE
Novosibirsk:IEEE,Oct.2018,pp.483–486. International Conference on Acoustics, Speech and Signal Processing
[30] G. Luo, K. Yi, S.-W. Cheng, Z. Li, W. Fan, C. He, and Y. Mu, (ICASSP). Toronto,ON,Canada:IEEE,Jun.2021,pp.6309–6313.
“Piecewiselinearapproximationofstreamingtimeseriesdatawithmax- [52] S.Qin,X.Zhang,H.Xu,andY.Xu,“FastQuaternionProductUnitsfor
errorguarantees,”in2015IEEE31stInternationalConferenceonData Learning Disentangled Representations in $\mathbb {SO}(3)$,” IEEE
Engineering. Seoul,SouthKorea:IEEE,Apr.2015,pp.173–184. Transactions on Pattern Analysis and Machine Intelligence, pp. 1–17,
[31] Yongwei Ding, Xiaohu Yang, A. J. Kavs, and Juefeng Li, “A novel 2022.
piecewiselinearsegmentationfortimeseries,”in2010The2ndInterna- [53] W. R. Hamilton, “II. On quaternions; or on a new system of imagi-
tionalConferenceonComputerandAutomationEngineering(ICCAE). naries in algebra,” The London, Edinburgh, and Dublin Philosophical
Singapore:IEEE,Feb.2010,pp.52–55. MagazineandJournalofScience,vol.25,no.163,pp.10–13,Jul.1844.PREPRINT 20
[54] T. A. Ell and S. J. Sangwine, “Quaternion involutions and anti- APPENDIXA
involutions,” Computers & Mathematics with Applications, vol. 53, HR-CALCULUS
no.1,pp.137–143,Jan.2007.
[55] A. Sudbery, “Quaternionic analysis,” Mathematical Proceedings of the Similar to the CR-Calculus [47], [48] introduces the HR-
Cambridge Philosophical Society, vol. 85, no. 2, pp. 199–225, Mar. Calculus as a method to derive quaternion valued func-
1979.
tions.This enables deriving holomorphic quaternionic func-
[56] M.E.Valle,“UnderstandingVector-ValuedNeuralNetworksandTheir
Relationship with Real and Hypercomplex-Valued Neural Networks,” tions as well as nonholomorphic real functions of quaternion
2023,publisher:arXivVersionNumber:1. variables. The quaternion derivatives are derived as
[57] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T.Killeen,Z.Lin,N.Gimelshein,L.Antiga,A.Desmaison,A.Ko¨pf, ∂f 1(cid:18) ∂f ∂f ∂f ∂f (cid:19)
E.Yang,Z.DeVito,M.Raison,A.Tejani,S.Chilamkurthy,B.Steiner, = − i− j− k
∂q 4 ∂q ∂q ∂q ∂q
L. Fang, J. Bai, and S. Chintala, PyTorch: an imperative style, high- 0 1 2 3
performance deep learning library. Red Hook, NY, USA: Curran ∂f 1(cid:18) ∂f ∂f ∂f ∂f (cid:19)
AssociatesInc.,2019. = − i+ j+ k
∂qi 4 ∂q ∂q ∂q ∂q
[58] A.Paszke,S.Gross,S.Chintala,G.Chanan,E.Yang,Z.DeVito,Z.Lin, 0 1 2 3 (24)
A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in ∂f 1(cid:18) ∂f ∂f ∂f ∂f (cid:19)
pytorch,”inNIPS-W,2017. = + i− j+ k
∂qj 4 ∂q ∂q ∂q ∂q
[59] S. Zagoruyko, “Pytorchviz,” https://github.com/szagoruyko/pytorchviz, 0 1 2 3
2021,v0.0.2. ∂f 1(cid:18) ∂f ∂f ∂f ∂f (cid:19)
[60] C.-A. Popa, “Octonion-Valued Neural Networks,” in Artificial Neural = + i+ j− k .
∂qk 4 ∂q ∂q ∂q ∂q
NetworksandMachineLearning–ICANN2016,A.E.Villa,P.Masulli, 0 1 2 3
andA.J.PonsRivero,Eds. Cham:SpringerInternationalPublishing,
The corresponding conjugate derivatives are defined as
2016,vol.9886,pp.435–443,seriesTitle:LectureNotesinComputer
Science. (cid:18) (cid:19)
[61] J. Po¨ppelbaum and A. Schwung, “Predicting Rigid Body Dynamics ∂f 1 ∂f ∂f ∂f ∂f
= + i+ j+ k
Using Dual Quaternion Recurrent Neural Networks With Quaternion ∂q∗ 4 ∂q ∂q ∂q ∂q
0 1 2 3
Attention,”IEEEAccess,vol.10,pp.82923–82943,2022.
(cid:18) (cid:19)
[62] A.Schwung,J.Po¨ppelbaum,andP.C.Nutakki,“RigidBodyMovement ∂f 1 ∂f ∂f ∂f ∂f
= + i− j− k
PredictionUsingDualQuaternionRecurrentNeuralNetworks,”in2021 ∂qi∗ 4 ∂q ∂q ∂q ∂q
0 1 2 3
22nd IEEE International Conference on Industrial Technology (ICIT).
(cid:18) (cid:19)
Valencia,Spain:IEEE,Mar.2021,pp.756–761. ∂f 1 ∂f ∂f ∂f ∂f
= − i+ j− k
[63] C. Guo, M. Lu, and J. Chen, “An evaluation of time series summary ∂qj∗ 4 ∂q ∂q ∂q ∂q
0 1 2 3
statisticsasfeaturesforclinicalpredictiontasks,”BMCMedicalInfor-
(cid:18) (cid:19)
maticsandDecisionMaking,vol.20,no.1,p.48,Dec.2020. ∂f 1 ∂f ∂f ∂f ∂f
= − i− j+ k .
[64] Q. Yin, J. Wang, X. Luo, J. Zhai, S. K. Jha, and Y.-Q. Shi, “Quater- ∂qk∗ 4 ∂q ∂q ∂q ∂q
0 1 2 3
nionConvolutionalNeuralNetworkforColorImageClassificationand
Forensics,”IEEEAccess,vol.7,pp.20293–20301,2019. Proposition 6. When deriving a quaternion valued function
[65] X.Zhu,Y.Xu,H.Xu,andC.Chen,“QuaternionConvolutionalNeural f(q),q ∈ H using (24) and the known product rule from R,
Networks,” in Proceedings of the European Conference on Computer
Vision(ECCV),Sep.2018. the product rule also does not apply.
[66] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, “Optuna: A
Next-generationHyperparameterOptimizationFramework,”inProceed- Proof. Again consider f(q)=qq∗ =q 02+q 12+q 22+q 32 as the
ingsofthe25thACMSIGKDDInternationalConferenceonKnowledge function of choice. Then the direct derivation is
Discovery&DataMining. AnchorageAKUSA:ACM,Jul.2019,pp.
(cid:18) (cid:19)
2623–2631. ∂f 1 ∂f ∂f ∂f ∂f
= − i− j− k
[67] J. Po¨ppelbaum, G. S. Chadha, and A. Schwung, “Contrastive learning ∂q 4 ∂q ∂q ∂q ∂q
basedself-supervisedtime-seriesanalysis,”AppliedSoftComputing,vol. 0 1 2 3
1
117,p.108397,Mar.2022. = (2q −2q i−2q j−2q k)
[68] C.Jing,X.Gao,X.Zhu,andS.Lang,“FaultclassificationonTennessee 4 0 1 2 3
Eastmanprocess:PCAandSVM,”in2014InternationalConferenceon 1
MechatronicsandControl(ICMC). Jinzhou,China:IEEE,Jul.2014, = q∗
2
pp.2194–2197.
[69] Y.Wang,H.Yang,X.Yuan,Y.A.Shardt,C.Yang,andW.Gui,“Deep Usingtheproductrule,wecancalculatethesamederivation
learningforfault-relevantfeatureextractionandfaultclassificationwith
using
stacked supervised auto-encoder,” Journal of Process Control, vol. 92,
∂ ∂q∗ ∂q
pp.79–89,Aug.2020. qq∗ =q + q∗ (25)
[70] J.YuanandY.Tian,“AMultiscaleFeatureLearningSchemeBasedon ∂q ∂q ∂q
DeepLearningforIndustrialProcessMonitoringandFaultDiagnosis,”
IEEEAccess,vol.7,pp.151189–151202,2019. Calculating the partial results
[71] Z.ChaiandC.Zhao,“EnhancedRandomForestWithConcurrentAnal-
ysis of Static and Dynamic Nodes for Industrial Fault Classification,” ∂q∗ 1 1
= (1+ii+jj+kk)=−
IEEETransactionsonIndustrialInformatics,vol.16,no.1,pp.54–66, ∂q 4 2
Jan.2020.
[72] J. Yu, C. Zhang, and S. Wang, “Multichannel one-dimensional con- and
volutional neural network-based feature learning for fault diagnosis of ∂q 1
= (1−ii−jj−kk)=1
industrialprocesses,”NeuralComputingandApplications,vol.33,no.8,
∂q 4
pp.3085–3104,Apr.2021.
[73] J. Yu and X. Yan, “Whole Process Monitoring Based on Unstable and inserting back into (25) yields
NeuronOutputInformationinHiddenLayersofDeepBeliefNetwork,”
IEEETransactionsonCybernetics,vol.50,no.9,pp.3998–4007,Sep. ∂ ∂q∗ ∂q
2020. qq∗ =q + q∗
∂q ∂q ∂q
[74] G.SinghChadha,M.Krishnamoorthy,andA.Schwung,“TimeSeries
basedFaultDetectioninIndustrialProcessesusingConvolutionalNeural = −1 q+1q∗ ̸= 1 q∗
Networks,” in IECON 2019 - 45th Annual Conference of the IEEE 2 2
IndustrialElectronicsSociety. Lisbon,Portugal:IEEE,Oct.2019,pp.
173–178.PREPRINT 21
APPENDIXB APPENDIXC
GHR-CALCULUS DETAILEDDERIVATIVECALCULATIONS
A. Detailed calculations for the derivatives of L with respect
Example 1. When using the GHR-Calculus and definitions,
to the invoutions yi, yj and yk
product rule can be used as follows:
Solution: Againconsiderf(q)=qq∗ =q2+q2+q2+q2
0 1 2 3
as the function of choice. Then the direct derivation is
∂L = ∂ (cid:2) (d −y )2+(d −y )2+(d −y )2+(d −y )2(cid:3)
(cid:18) (cid:19) ∂yi ∂yi 0 0 1 1 2 2 3 3
∂f 1 ∂f ∂f ∂f ∂f
= − i− j− k 1
∂q 4 ∂q ∂q ∂q ∂q = [−2(d −y )+2(d −y )i−2(d −y )j−2(d −y )k]
0 1 2 3 4 0 0 1 1 2 2 3 3
1
= (2q −2q i−2q j−2q k) 1
4 0 1 2 3 =− 2[(d 0−y 0)−(d 1−y 1)i+(d 2−y 2)j+(d 3−y 3)k]
1
= 2q∗ =−1 (d−y)i∗ =−1 ei∗
2 2
Using the product rule, we can calculate the same deriva-
tion using ∂L = ∂ (cid:2) (d −y )2+(d −y )2+(d −y )2+(d −y )2(cid:3)
∂yj ∂yj 0 0 1 1 2 2 3 3
∂(qq∗) ∂(q∗) ∂(q) ∂(q∗) ∂(q) 1
=q + q∗ =q + q∗ withµ=1 = [−2(d −y )−2(d −y )i+2(d −y )j−2(d −y )k]
∂qµ ∂qµ ∂qq∗µ ∂q ∂qq∗ 4 0 0 1 1 2 2 3 3
(26) 1
=− [(d −y )+(d −y )i−(d −y )j+(d −y )k]
Calculating the partial results 2 0 0 1 1 2 2 3 3
1 1
=− (d−y)j∗ =− ej∗
2 2
∂(q∗) −1
= (same as for HR with µ=1)
∂qµ 2
∂L = ∂ (cid:2) (d −y )2+(d −y )2+(d −y )2+(d −y )2(cid:3)
and ∂yk ∂yk 0 0 1 1 2 2 3 3
1
= [−2(d −y )−2(d −y )i−2(d −y )j+2(d −y )k]
4 0 0 1 1 2 2 3 3
1
∂(q)
=
1(cid:18) ∂f
−
∂f
iq∗
−
∂f
jq∗
−
∂f kq∗(cid:19) =− 2[(d 0−y 0)+(d 1−y 1)i+(d 2−y 2)j−(d 3−y 3)k]
∂qq∗ 4 ∂q 0 ∂q 1 ∂q 2 ∂q 3 =−1 (d−y)k∗ =−1 ek∗
=
1(cid:16)
1−iiq∗ −jjq∗
−kkq∗(cid:17)
2 2
4
1(cid:16) (cid:17)
=
q∗q∗−1 −iq∗iq∗−1 −jq∗jq∗−1 −kq∗kq∗−1
4
B. Detailed calculations for the derivatives of the involutions
1
= (q∗−iq∗i−jq∗j−kq∗k)q∗−1 yi, yj and yk with respect to w∗
4
=
1(cid:0) q∗+qi∗+qj∗+qk∗(cid:1) q∗−1
4
=q
0q∗−1
∂yi ∂(wa+b)i ∂(wa)i
= =
∂w∗ ∂w∗ ∂w∗
∂
and inserting back into (26) yields = [(a w −a w −a w −a w )
∂w∗ 0 0 1 1 2 2 3 3
+(a w +a w −a w +a w )i
0 1 1 0 2 3 3 2
∂(qq∗) ∂(q∗) ∂(q)
=q + q∗ −(a 0w 2+a 1w 3+a 2w 0−a 3w 1)j
∂qµ ∂qµ ∂qqµ
−(a w −a w +a w +a w )k]
−1 0 3 1 2 2 1 3 0
=
2
q+q 0q∗−1 q∗
=
1
[a +a i−a j−a k+(a i−a −a k+a j)i
−1 4 0 1 2 3 0 1 2 3
= (q +q i+q j+q k)+q
2 0 1 2 3 0 +(−a 0j+a 1k−a 2+a 3i)j+(−a 0k−a 1j−a 2i−a 3)k]
1 1
= (q −q i−q j−q k) = [a −a i−a j−a k]
2 0 1 2 3 2 0 1 2 3
1 1
= q∗. = a∗
2 2PREPRINT 22
∂yj ∂(wa+b)j ∂(wa)j
= =
∂w∗ ∂w∗ ∂w∗
∂yk ∂(wa+b)k ∂(b)k
∂ = =
= ∂w∗[(a 0w 0−a 1w 1−a 2w 2−a 3w 3) ∂b∗ ∂b∗ ∂b∗
∂
−(a w +a w −a w +a w )i = (b −b i−b j+b k)
0 1 1 0 2 3 3 2 ∂b∗ 0 1 2 3
+(a 0w 2+a 1w 3+a 2w 0−a 3w 1)j 1
= (1−ii−jj+kk)
−(a w −a w +a w +a w )k] 4
0 3 1 2 2 1 3 0
1
1 = (1+1+1−1)
= 4[a 0−a 1i+a 2j−a 3k+(−a 0i−a 1−a 2k−a 3j)i 4
=0.5
+(a j+a k−a −a i)j+(−a k+a j+a i−a )k]
0 1 2 3 0 1 2 3
1
= [a −a i−a j−a k]
2 0 1 2 3
1 D. Detailed calculations for the derivatives of y, yi, yj and
= 2a∗ yk with respect to a
∂yk ∂(wa+b)k ∂(wa)k
∂y ∂(wa+b) ∂(wa)
= = = =
∂w∗ ∂w∗ ∂w∗ ∂a ∂a ∂a
∂ ∂
= ∂w∗[(a 0w 0−a 1w 1−a 2w 2−a 3w 3) = ∂a[(a 0w 0−a 1w 1−a 2w 2−a 3w 3)
−(a 0w 1+a 1w 0−a 2w 3+a 3w 2)i +(a 0w 1+a 1w 0−a 2w 3+a 3w 2)i
−(a 0w 2+a 1w 3+a 2w 0−a 3w 1)j +(a 0w 2+a 1w 3+a 2w 0−a 3w 1)j
+(a 0w 3−a 1w 2+a 2w 1+a 3w 0)k] +(a 0w 3−a 1w 2+a 2w 1+a 3w 0)k]
1 1
= 4[a 0−a 1i−a 2j+a 3k+(−a 0i−a 1+a 2k+a 3j)i = 4[(w 0+w 1i+w 2j+w 3k)−(w 0i−w 1−w 2k+w 3j)i
+(−a 0j−a 1k−a 2−a 3i)j+(a 0k−a 1j+a 2i−a 3)k] −(w 0j+w 1k−w 2−w 3i)j−(w 0k−w 1j+w 2i−w 3)k]
1
= 2[a 0−a 1i−a 2j−a 3k] =w 0+w 1i+w 2j+w 3k
=w
1
= a∗
2
∂yi ∂(wa+b)i ∂(wa)i
= =
∂a ∂a ∂a
C. Detailed calculations for the derivatives of the involutions
∂
yi, yj and yk with respect to b∗ = ∂a[(a 0w 0−a 1w 1−a 2w 2−a 3w 3)
+(a w +a w −a w +a w )i
0 1 1 0 2 3 3 2
−(a w +a w +a w −a w )j
0 2 1 3 2 0 3 1
∂yi ∂(wa+b)i ∂(b)i
= = −(a w −a w +a w +a w )k]
∂b∗ ∂b∗ ∂b∗ 0 3 1 2 2 1 3 0
1
∂ = [(w +w i−w j−w k)−(w i−w +w k−w j)i
=
∂b∗
(b 0+b 1i−b 2j−b 3k) 4 0 1 2 3 0 1 2 3
1 −(−w 0j−w 1k−w 2−w 3i)j−(−w 0k+w 1j+w 2i−w 3)k]
= (1+ii−jj−kk)
4 =0
1
= (1−1+1+1)
4
=0.5 ∂yj ∂(wa+b)j ∂(wa)j
= =
∂a ∂a ∂a
∂
= [(a w −a w −a w −a w )
∂yj ∂(wa+b)j ∂(b)j ∂a 0 0 1 1 2 2 3 3
= =
∂b∗ ∂b∗ ∂b∗ −(a 0w 1+a 1w 0−a 2w 3+a 3w 2)i
∂
+(a w +a w +a w −a w )j
= (b −b i+b j−b k) 0 2 1 3 2 0 3 1
∂b∗ 0 1 2 3
−(a w −a w +a w +a w )k]
1 0 3 1 2 2 1 3 0
= (1−ii+jj−kk) 1
4 = [(w −w i+w j−w k)−(−w i−w +w k+w j)i
1 4 0 1 2 3 0 1 2 3
= (1+1−1+1)
−(w j−w k−w +w i)j−(−w k−w j−w i−w )k]
4 0 1 2 3 0 1 2 3
=0.5 =0PREPRINT 23
APPENDIXE
∂yk ∂(wa+b)k ∂(wa)k AUTOMATICDIFFERENTIATIONCALCULATIONS
= =
∂a ∂a ∂a A. Derivative with respect to the activation output of the
∂ previous layer
= [(a w −a w −a w −a w )
∂a 0 0 1 1 2 2 3 3
The derivative with respect to the layers input a(l−1) is
−(a w +a w −a w +a w )i
0 1 1 0 2 3 3 2 calculated as
−(a w +a w +a w −a w )j
0 2 1 3 2 0 3 1  ∂L  ∂L ∂zi0 + ∂L ∂zi1 + ∂L ∂zi2 + ∂L ∂zi3
+(a 0w 3−a 1w 2+a 2w 1+a 3w 0)k] ∂a0 ∂zi0 ∂a0 ∂zi1 ∂a0 ∂zi2 ∂a0 ∂zi3 ∂a0
−= (−1 4[ w(w j0 +− ww 1 ki− −w w2j ++ ww 3 ik )j) −− ((− ww k0 +i− ww j1− −w w2k i−− ww 3 )j k) ]i      ∂ ∂∂ ∂a aL L1 2     = i(cid:88) ∈K     ∂ ∂∂ ∂z zL Li i0 0∂ ∂∂ ∂z za ai i1 20 0 + + ∂ ∂∂ ∂z zL Li i1 1∂ ∂∂ ∂z za ai i1 21 1 + + ∂ ∂∂ ∂z zL Li i2 2∂ ∂∂ ∂z za ai i1 22 2 + + ∂ ∂∂ ∂z zL Li i3 3∂ ∂∂ ∂z za ai i1 23 3    

0 1 2 3 0 1 2 3 ∂L ∂L ∂zi0 + ∂L ∂zi1 + ∂L ∂zi2 + ∂L ∂zi3
=0 ∂a3 ∂zi0 ∂a3 ∂zi1 ∂a3 ∂zi2 ∂a3 ∂zi3 ∂a3
 
q w −q w −q w −q w
i0 i,j0 i1 i,j1 i2 i,j2 i3 i,j3
APPENDIXD =(cid:88) −q i0w i,j1−q i1w i,j0−q i2w i,j3+q i3w i,j2

LAYERVISUALIZATION i∈K−q i0w i,j2+q i1w i,j3−q i2w i,j0−q i3w i,j1
−q w −q w +q w −q w
i0 i,j3 i1 i,j2 i2 i,j1 i3 i,j0
 
q w −q w −q w −q w
i0 i,j0 i1 i,j1 i2 i,j2 i3 i,j3
=
(cid:88) −(q i0w i,j1+q i1w i,j0+q i2w i,j3−q i3w i,j2)
.
i∈K−(q i0w i,j2−q i1w i,j3+q i2w i,j0+q i3w i,j1)
−(q w +q w −q w +q w ) i0 i,j3 i1 i,j2 i2 i,j1 i3 i,j0
Taking the rows of the output vector for the real part and
the imaginary parts respectively, and using Equation (1) we
obtain
(cid:16) (cid:17)
∂L a(l−1)
j = (cid:88) (qw )∗
∂a(l−1) i i,j
j i∈K
B. Derivative with respect to the weights
The derivative with respect to the layers weights w(l) is
calculated following
 ∂L  ∂L ∂z0 + ∂L ∂z1 + ∂L ∂z2 + ∂L ∂z3
∂w0 ∂z0∂w0 ∂z1∂w0 ∂z2∂w0 ∂z3∂w0
  ∂L    ∂L ∂z0 + ∂L ∂z1 + ∂L ∂z2 + ∂L ∂z3 
∂w1=∂z0∂w1 ∂z1∂w1 ∂z2∂w1 ∂z3∂w1
 ∂L  ∂L ∂z0 + ∂L ∂z1 + ∂L ∂z2 + ∂L ∂z3
 ∂w2

 ∂z0∂w2 ∂z1∂w2 ∂z2∂w2 ∂z3∂w2

∂L ∂L ∂z0 + ∂L ∂z1 + ∂L ∂z2 + ∂L ∂z3
∂w3 ∂z0∂w3 ∂z1∂w3 ∂z2∂w3 ∂z3∂w3
 
q a −q a −q a −q a 0 0 1 1 2 2 3 3
= −q 0a 1−q 1a 0+q 2a 3−q 3a 2

−q 0a 2−q 1a 3−q 2a 0+q 3a 1
−q a +q a −q a −q a
0 3 1 2 2 1 3 0  
q a −(−q )(−a )−(−q )(−a )−(−q )(−a )
0 0 1 1 2 2 3 3
= q 0(−a 1)+(−q 1)a 0+(−q 2)(−a 3)−(−q 3)(−a 2)
.
q 0(−a 2)−(−q 1)(−a 3)+(−q 2)a 0+(−q 3)(−a 1)
q (−a )+(−q )(−a )−(−q )(−a )+(−q )a
0 3 1 2 2 1 3 0
Composing the rows back to a quaternion and using Equation
(1) yields
∂L(w)
=q∗a∗. ∂w(l)
C. Derivative with respect to the bias
As the respective bias component is not involved in the
Fig. 5. Visiualization of the gradient flow for parameter w0 obtained with
calculation of all output components, but just b for z , b for
TorchViz 0 0 1
z , b for z and b for z , the derivatives simplify to
1 2 2 3 3
3_x
2_x
0_w
1_x
0_x
)1(
)1(
)1(
)1(
)1(
)3 1_ (b
darGetalumuccA
)2 1_ (b
darGetalumuccA
darGetalumuccA
darGetalumuccA
)1 1_ (b
darGetalumuccA
)0 1_ (b
darGetalumuccA
0drawkcaBluM
darGetalumuccA
0drawkcaBluM
0drawkcaBluM
darGetalumuccA
0drawkcaBluM
darGetalumuccA
0drawkcaBddA
0drawkcaBddA
0drawkcaBddA
0drawkcaBddA
0drawkcaBuleR
0drawkcaBuleR
0drawkcaBuleR
0drawkcaBuleR
3_y
2_y
1_y
0_y
)1(
)1(
)1(
)1(PREPRINT 24
 ∂L ∂L ∂z0
∂b0 ∂z0 ∂b0  q 
   

∂ ∂∂ ∂b bL L1 2   

=   

∂ ∂∂ ∂z zL L1 2∂ ∂∂ ∂z zb b1 21 2   

=  − − −q q q0 1 2  .
∂L ∂L ∂z3 3
∂b3 ∂z3 ∂b3
Consequently, the conversion back to a quaternion is
∂L(b)
=q∗.
∂b(l)