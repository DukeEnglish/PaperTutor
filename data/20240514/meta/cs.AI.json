[
    {
        "title": "MambaOut: Do We Really Need Mamba for Vision?",
        "authors": "Weihao YuXinchao Wang",
        "links": "http://arxiv.org/abs/2405.07992v1",
        "entry_id": "http://arxiv.org/abs/2405.07992v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07992v1",
        "summary": "Mamba, an architecture with RNN-like token mixer of state space model (SSM),\nwas recently introduced to address the quadratic complexity of the attention\nmechanism and subsequently applied to vision tasks. Nevertheless, the\nperformance of Mamba for vision is often underwhelming when compared with\nconvolutional and attention-based models. In this paper, we delve into the\nessence of Mamba, and conceptually conclude that Mamba is ideally suited for\ntasks with long-sequence and autoregressive characteristics. For vision tasks,\nas image classification does not align with either characteristic, we\nhypothesize that Mamba is not necessary for this task; Detection and\nsegmentation tasks are also not autoregressive, yet they adhere to the\nlong-sequence characteristic, so we believe it is still worthwhile to explore\nMamba's potential for these tasks. To empirically verify our hypotheses, we\nconstruct a series of models named \\emph{MambaOut} through stacking Mamba\nblocks while removing their core token mixer, SSM. Experimental results\nstrongly support our hypotheses. Specifically, our MambaOut model surpasses all\nvisual Mamba models on ImageNet image classification, indicating that Mamba is\nindeed unnecessary for this task. As for detection and segmentation, MambaOut\ncannot match the performance of state-of-the-art visual Mamba models,\ndemonstrating the potential of Mamba for long-sequence visual tasks. The code\nis available at https://github.com/yuweihao/MambaOut",
        "updated": "2024-05-13 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07992v1"
    },
    {
        "title": "SPIN: Simultaneous Perception, Interaction and Navigation",
        "authors": "Shagun UppalAnanye AgarwalHaoyu XiongKenneth ShawDeepak Pathak",
        "links": "http://arxiv.org/abs/2405.07991v1",
        "entry_id": "http://arxiv.org/abs/2405.07991v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07991v1",
        "summary": "While there has been remarkable progress recently in the fields of\nmanipulation and locomotion, mobile manipulation remains a long-standing\nchallenge. Compared to locomotion or static manipulation, a mobile system must\nmake a diverse range of long-horizon tasks feasible in unstructured and dynamic\nenvironments. While the applications are broad and interesting, there are a\nplethora of challenges in developing these systems such as coordination between\nthe base and arm, reliance on onboard perception for perceiving and interacting\nwith the environment, and most importantly, simultaneously integrating all\nthese parts together. Prior works approach the problem using disentangled\nmodular skills for mobility and manipulation that are trivially tied together.\nThis causes several limitations such as compounding errors, delays in\ndecision-making, and no whole-body coordination. In this work, we present a\nreactive mobile manipulation framework that uses an active visual system to\nconsciously perceive and react to its environment. Similar to how humans\nleverage whole-body and hand-eye coordination, we develop a mobile manipulator\nthat exploits its ability to move and see, more specifically -- to move in\norder to see and to see in order to move. This allows it to not only move\naround and interact with its environment but also, choose \"when\" to perceive\n\"what\" using an active visual system. We observe that such an agent learns to\nnavigate around complex cluttered scenarios while displaying agile whole-body\ncoordination using only ego-vision without needing to create environment maps.\nResults visualizations and videos at https://spin-robot.github.io/",
        "updated": "2024-05-13 17:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07991v1"
    },
    {
        "title": "The Platonic Representation Hypothesis",
        "authors": "Minyoung HuhBrian CheungTongzhou WangPhillip Isola",
        "links": "http://arxiv.org/abs/2405.07987v1",
        "entry_id": "http://arxiv.org/abs/2405.07987v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07987v1",
        "summary": "We argue that representations in AI models, particularly deep networks, are\nconverging. First, we survey many examples of convergence in the literature:\nover time and across multiple domains, the ways by which different neural\nnetworks represent data are becoming more aligned. Next, we demonstrate\nconvergence across data modalities: as vision models and language models get\nlarger, they measure distance between datapoints in a more and more alike way.\nWe hypothesize that this convergence is driving toward a shared statistical\nmodel of reality, akin to Plato's concept of an ideal reality. We term such a\nrepresentation the platonic representation and discuss several possible\nselective pressures toward it. Finally, we discuss the implications of these\ntrends, their limitations, and counterexamples to our analysis.",
        "updated": "2024-05-13 17:58:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07987v1"
    },
    {
        "title": "Localized Adaptive Risk Control",
        "authors": "Matteo ZecchinOsvaldo Simeone",
        "links": "http://arxiv.org/abs/2405.07976v1",
        "entry_id": "http://arxiv.org/abs/2405.07976v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07976v1",
        "summary": "Adaptive Risk Control (ARC) is an online calibration strategy based on set\nprediction that offers worst-case deterministic long-term risk control, as well\nas statistical marginal coverage guarantees. ARC adjusts the size of the\nprediction set by varying a single scalar threshold based on feedback from past\ndecisions. In this work, we introduce Localized Adaptive Risk Control (L-ARC),\nan online calibration scheme that targets statistical localized risk guarantees\nranging from conditional risk to marginal risk, while preserving the worst-case\nperformance of ARC. L-ARC updates a threshold function within a reproducing\nkernel Hilbert space (RKHS), with the kernel determining the level of\nlocalization of the statistical risk guarantee. The theoretical results\nhighlight a trade-off between localization of the statistical risk and\nconvergence speed to the long-term risk target. Thanks to localization, L-ARC\nis demonstrated via experiments to produce prediction sets with risk guarantees\nacross different data subpopulations, significantly improving the fairness of\nthe calibrated model for tasks such as image segmentation and beam selection in\nwireless networks.",
        "updated": "2024-05-13 17:48:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07976v1"
    },
    {
        "title": "Investigating the Semantic Robustness of CLIP-based Zero-Shot Anomaly Segmentation",
        "authors": "Kevin StanglMarius ArvinteWeilin XuCory Cornelius",
        "links": "http://arxiv.org/abs/2405.07969v1",
        "entry_id": "http://arxiv.org/abs/2405.07969v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07969v1",
        "summary": "Zero-shot anomaly segmentation using pre-trained foundation models is a\npromising approach that enables effective algorithms without expensive,\ndomain-specific training or fine-tuning. Ensuring that these methods work\nacross various environmental conditions and are robust to distribution shifts\nis an open problem. We investigate the performance of WinCLIP [14] zero-shot\nanomaly segmentation algorithm by perturbing test data using three semantic\ntransformations: bounded angular rotations, bounded saturation shifts, and hue\nshifts. We empirically measure a lower performance bound by aggregating across\nper-sample worst-case perturbations and find that average performance drops by\nup to 20% in area under the ROC curve and 40% in area under the per-region\noverlap curve. We find that performance is consistently lowered on three CLIP\nbackbones, regardless of model architecture or learning objective,\ndemonstrating a need for careful performance evaluation.",
        "updated": "2024-05-13 17:47:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07969v1"
    }
]