[
    {
        "title": "MambaOut: Do We Really Need Mamba for Vision?",
        "authors": "Weihao YuXinchao Wang",
        "links": "http://arxiv.org/abs/2405.07992v1",
        "entry_id": "http://arxiv.org/abs/2405.07992v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07992v1",
        "summary": "Mamba, an architecture with RNN-like token mixer of state space model (SSM),\nwas recently introduced to address the quadratic complexity of the attention\nmechanism and subsequently applied to vision tasks. Nevertheless, the\nperformance of Mamba for vision is often underwhelming when compared with\nconvolutional and attention-based models. In this paper, we delve into the\nessence of Mamba, and conceptually conclude that Mamba is ideally suited for\ntasks with long-sequence and autoregressive characteristics. For vision tasks,\nas image classification does not align with either characteristic, we\nhypothesize that Mamba is not necessary for this task; Detection and\nsegmentation tasks are also not autoregressive, yet they adhere to the\nlong-sequence characteristic, so we believe it is still worthwhile to explore\nMamba's potential for these tasks. To empirically verify our hypotheses, we\nconstruct a series of models named \\emph{MambaOut} through stacking Mamba\nblocks while removing their core token mixer, SSM. Experimental results\nstrongly support our hypotheses. Specifically, our MambaOut model surpasses all\nvisual Mamba models on ImageNet image classification, indicating that Mamba is\nindeed unnecessary for this task. As for detection and segmentation, MambaOut\ncannot match the performance of state-of-the-art visual Mamba models,\ndemonstrating the potential of Mamba for long-sequence visual tasks. The code\nis available at https://github.com/yuweihao/MambaOut",
        "updated": "2024-05-13 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07992v1"
    },
    {
        "title": "SPIN: Simultaneous Perception, Interaction and Navigation",
        "authors": "Shagun UppalAnanye AgarwalHaoyu XiongKenneth ShawDeepak Pathak",
        "links": "http://arxiv.org/abs/2405.07991v1",
        "entry_id": "http://arxiv.org/abs/2405.07991v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07991v1",
        "summary": "While there has been remarkable progress recently in the fields of\nmanipulation and locomotion, mobile manipulation remains a long-standing\nchallenge. Compared to locomotion or static manipulation, a mobile system must\nmake a diverse range of long-horizon tasks feasible in unstructured and dynamic\nenvironments. While the applications are broad and interesting, there are a\nplethora of challenges in developing these systems such as coordination between\nthe base and arm, reliance on onboard perception for perceiving and interacting\nwith the environment, and most importantly, simultaneously integrating all\nthese parts together. Prior works approach the problem using disentangled\nmodular skills for mobility and manipulation that are trivially tied together.\nThis causes several limitations such as compounding errors, delays in\ndecision-making, and no whole-body coordination. In this work, we present a\nreactive mobile manipulation framework that uses an active visual system to\nconsciously perceive and react to its environment. Similar to how humans\nleverage whole-body and hand-eye coordination, we develop a mobile manipulator\nthat exploits its ability to move and see, more specifically -- to move in\norder to see and to see in order to move. This allows it to not only move\naround and interact with its environment but also, choose \"when\" to perceive\n\"what\" using an active visual system. We observe that such an agent learns to\nnavigate around complex cluttered scenarios while displaying agile whole-body\ncoordination using only ego-vision without needing to create environment maps.\nResults visualizations and videos at https://spin-robot.github.io/",
        "updated": "2024-05-13 17:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07991v1"
    },
    {
        "title": "Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots",
        "authors": "Chengyue WuYixiao GeQiushan GuoJiahao WangZhixuan LiangZeyu LuYing ShanPing Luo",
        "links": "http://arxiv.org/abs/2405.07990v1",
        "entry_id": "http://arxiv.org/abs/2405.07990v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07990v1",
        "summary": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has\nattracted significant attention due to their superior performance in visual\ncontexts. However, their capabilities in turning visual figure to executable\ncode, have not been evaluated thoroughly. To address this, we introduce\nPlot2Code, a comprehensive visual coding benchmark designed for a fair and\nin-depth assessment of MLLMs. We carefully collect 132 manually selected\nhigh-quality matplotlib plots across six plot types from publicly available\nmatplotlib galleries. For each plot, we carefully offer its source code, and an\ndescriptive instruction summarized by GPT-4. This approach enables Plot2Code to\nextensively evaluate MLLMs' code capabilities across various input modalities.\nFurthermore, we propose three automatic evaluation metrics, including code pass\nrate, text-match ratio, and GPT-4V overall rating, for a fine-grained\nassessment of the output code and rendered images. Instead of simply judging\npass or fail, we employ GPT-4V to make an overall judgement between the\ngenerated and reference images, which has been shown to be consistent with\nhuman evaluation. The evaluation results, which include analyses of 14 MLLMs\nsuch as the proprietary GPT-4V, Gemini-Pro, and the open-sourced Mini-Gemini,\nhighlight the substantial challenges presented by Plot2Code. With Plot2Code, we\nreveal that most existing MLLMs struggle with visual coding for text-dense\nplots, heavily relying on textual instruction. We hope that the evaluation\nresults from Plot2Code on visual coding will guide the future development of\nMLLMs. All data involved with Plot2Code are available at\nhttps://huggingface.co/datasets/TencentARC/Plot2Code.",
        "updated": "2024-05-13 17:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07990v1"
    },
    {
        "title": "A Generalist Learner for Multifaceted Medical Image Interpretation",
        "authors": "Hong-Yu ZhouSubathra AdithanJulián Nicolás AcostaEric J. TopolPranav Rajpurkar",
        "links": "http://arxiv.org/abs/2405.07988v1",
        "entry_id": "http://arxiv.org/abs/2405.07988v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07988v1",
        "summary": "Current medical artificial intelligence systems are often limited to narrow\napplications, hindering their widespread adoption in clinical practice. To\naddress this limitation, we propose MedVersa, a generalist learner that enables\nflexible learning and tasking for medical image interpretation. By leveraging a\nlarge language model as a learnable orchestrator, MedVersa can learn from both\nvisual and linguistic supervision, support multimodal inputs, and perform\nreal-time task specification. This versatility allows MedVersa to adapt to\nvarious clinical scenarios and perform multifaceted medical image analysis. We\nintroduce MedInterp, the largest multimodal dataset to date for medical image\ninterpretation, consisting of over 13 million annotated instances spanning 11\ntasks across 3 modalities, to support the development of MedVersa. Our\nexperiments demonstrate that MedVersa achieves state-of-the-art performance in\n9 tasks, sometimes outperforming specialist counterparts by over 10%. MedVersa\nis the first to showcase the viability of multimodal generative medical AI in\nimplementing multimodal outputs, inputs, and dynamic task specification,\nhighlighting its potential as a multifunctional system for comprehensive\nmedical image analysis. This generalist approach to medical image\ninterpretation paves the way for more adaptable and efficient AI-assisted\nclinical decision-making.",
        "updated": "2024-05-13 17:58:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07988v1"
    },
    {
        "title": "The Platonic Representation Hypothesis",
        "authors": "Minyoung HuhBrian CheungTongzhou WangPhillip Isola",
        "links": "http://arxiv.org/abs/2405.07987v1",
        "entry_id": "http://arxiv.org/abs/2405.07987v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07987v1",
        "summary": "We argue that representations in AI models, particularly deep networks, are\nconverging. First, we survey many examples of convergence in the literature:\nover time and across multiple domains, the ways by which different neural\nnetworks represent data are becoming more aligned. Next, we demonstrate\nconvergence across data modalities: as vision models and language models get\nlarger, they measure distance between datapoints in a more and more alike way.\nWe hypothesize that this convergence is driving toward a shared statistical\nmodel of reality, akin to Plato's concept of an ideal reality. We term such a\nrepresentation the platonic representation and discuss several possible\nselective pressures toward it. Finally, we discuss the implications of these\ntrends, their limitations, and counterexamples to our analysis.",
        "updated": "2024-05-13 17:58:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07987v1"
    }
]