[
    {
        "title": "Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots",
        "authors": "Chengyue WuYixiao GeQiushan GuoJiahao WangZhixuan LiangZeyu LuYing ShanPing Luo",
        "links": "http://arxiv.org/abs/2405.07990v1",
        "entry_id": "http://arxiv.org/abs/2405.07990v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07990v1",
        "summary": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has\nattracted significant attention due to their superior performance in visual\ncontexts. However, their capabilities in turning visual figure to executable\ncode, have not been evaluated thoroughly. To address this, we introduce\nPlot2Code, a comprehensive visual coding benchmark designed for a fair and\nin-depth assessment of MLLMs. We carefully collect 132 manually selected\nhigh-quality matplotlib plots across six plot types from publicly available\nmatplotlib galleries. For each plot, we carefully offer its source code, and an\ndescriptive instruction summarized by GPT-4. This approach enables Plot2Code to\nextensively evaluate MLLMs' code capabilities across various input modalities.\nFurthermore, we propose three automatic evaluation metrics, including code pass\nrate, text-match ratio, and GPT-4V overall rating, for a fine-grained\nassessment of the output code and rendered images. Instead of simply judging\npass or fail, we employ GPT-4V to make an overall judgement between the\ngenerated and reference images, which has been shown to be consistent with\nhuman evaluation. The evaluation results, which include analyses of 14 MLLMs\nsuch as the proprietary GPT-4V, Gemini-Pro, and the open-sourced Mini-Gemini,\nhighlight the substantial challenges presented by Plot2Code. With Plot2Code, we\nreveal that most existing MLLMs struggle with visual coding for text-dense\nplots, heavily relying on textual instruction. We hope that the evaluation\nresults from Plot2Code on visual coding will guide the future development of\nMLLMs. All data involved with Plot2Code are available at\nhttps://huggingface.co/datasets/TencentARC/Plot2Code.",
        "updated": "2024-05-13 17:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07990v1"
    },
    {
        "title": "AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments",
        "authors": "Samuel SchmidgallRojin ZiaeiCarl HarrisEduardo ReisJeffrey JoplingMichael Moor",
        "links": "http://arxiv.org/abs/2405.07960v1",
        "entry_id": "http://arxiv.org/abs/2405.07960v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07960v1",
        "summary": "Diagnosing and managing a patient is a complex, sequential decision making\nprocess that requires physicians to obtain information -- such as which tests\nto perform -- and to act upon it. Recent advances in artificial intelligence\n(AI) and large language models (LLMs) promise to profoundly impact clinical\ncare. However, current evaluation schemes overrely on static medical\nquestion-answering benchmarks, falling short on interactive decision-making\nthat is required in real-life clinical work. Here, we present AgentClinic: a\nmultimodal benchmark to evaluate LLMs in their ability to operate as agents in\nsimulated clinical environments. In our benchmark, the doctor agent must\nuncover the patient's diagnosis through dialogue and active data collection. We\npresent two open benchmarks: a multimodal image and dialogue environment,\nAgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA. We embed\ncognitive and implicit biases both in patient and doctor agents to emulate\nrealistic interactions between biased agents. We find that introducing bias\nleads to large reductions in diagnostic accuracy of the doctor agents, as well\nas reduced compliance, confidence, and follow-up consultation willingness in\npatient agents. Evaluating a suite of state-of-the-art LLMs, we find that\nseveral models that excel in benchmarks like MedQA are performing poorly in\nAgentClinic-MedQA. We find that the LLM used in the patient agent is an\nimportant factor for performance in the AgentClinic benchmark. We show that\nboth having limited interactions as well as too many interaction reduces\ndiagnostic accuracy in doctor agents. The code and data for this work is\npublicly available at https://AgentClinic.github.io.",
        "updated": "2024-05-13 17:38:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07960v1"
    },
    {
        "title": "RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors",
        "authors": "Liam DuganAlyssa HwangFilip TrhlikJosh Magnus LudanAndrew ZhuHainiu XuDaphne IppolitoChris Callison-Burch",
        "links": "http://arxiv.org/abs/2405.07940v1",
        "entry_id": "http://arxiv.org/abs/2405.07940v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07940v1",
        "summary": "Many commercial and open-source models claim to detect machine-generated text\nwith very high accuracy (99\\% or higher). However, very few of these detectors\nare evaluated on shared benchmark datasets and even when they are, the datasets\nused for evaluation are insufficiently challenging -- lacking variations in\nsampling strategy, adversarial attacks, and open-source generative models. In\nthis work we present RAID: the largest and most challenging benchmark dataset\nfor machine-generated text detection. RAID includes over 6 million generations\nspanning 11 models, 8 domains, 11 adversarial attacks and 4 decoding\nstrategies. Using RAID, we evaluate the out-of-domain and adversarial\nrobustness of 8 open- and 4 closed-source detectors and find that current\ndetectors are easily fooled by adversarial attacks, variations in sampling\nstrategies, repetition penalties, and unseen generative models. We release our\ndataset and tools to encourage further exploration into detector robustness.",
        "updated": "2024-05-13 17:15:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07940v1"
    },
    {
        "title": "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning",
        "authors": "Yinzhu QuanZefang Liu",
        "links": "http://arxiv.org/abs/2405.07938v1",
        "entry_id": "http://arxiv.org/abs/2405.07938v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07938v1",
        "summary": "In this paper, we introduce EconLogicQA, a rigorous benchmark designed to\nassess the sequential reasoning capabilities of large language models (LLMs)\nwithin the intricate realms of economics, business, and supply chain\nmanagement. Diverging from traditional benchmarks that predict subsequent\nevents individually, EconLogicQA poses a more challenging task: it requires\nmodels to discern and sequence multiple interconnected events, capturing the\ncomplexity of economic logics. EconLogicQA comprises an array of multi-event\nscenarios derived from economic articles, which necessitate an insightful\nunderstanding of both temporal and logical event relationships. Through\ncomprehensive evaluations, we exhibit that EconLogicQA effectively gauges a\nLLM's proficiency in navigating the sequential complexities inherent in\neconomic contexts. We provide a detailed description of EconLogicQA dataset and\nshows the outcomes from evaluating the benchmark across various leading-edge\nLLMs, thereby offering a thorough perspective on their sequential reasoning\npotential in economic contexts. Our benchmark dataset is available at\nhttps://huggingface.co/datasets/yinzhu-quan/econ_logic_qa.",
        "updated": "2024-05-13 17:13:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07938v1"
    },
    {
        "title": "PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition",
        "authors": "Ziyang ZhangQizhen ZhangJakob Foerster",
        "links": "http://arxiv.org/abs/2405.07932v1",
        "entry_id": "http://arxiv.org/abs/2405.07932v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07932v1",
        "summary": "Large language models (LLMs) have shown success in many natural language\nprocessing tasks. Despite rigorous safety alignment processes, supposedly\nsafety-aligned LLMs like Llama 2 and Claude 2 are still susceptible to\njailbreaks, leading to security risks and abuse of the models. One option to\nmitigate such risks is to augment the LLM with a dedicated \"safeguard\", which\nchecks the LLM's inputs or outputs for undesired behaviour. A promising\napproach is to use the LLM itself as the safeguard. Nonetheless, baseline\nmethods, such as prompting the LLM to self-classify toxic content, demonstrate\nlimited efficacy. We hypothesise that this is due to domain shift: the\nalignment training imparts a self-censoring behaviour to the model (\"Sorry I\ncan't do that\"), while the self-classify approach shifts it to a classification\nformat (\"Is this prompt malicious\"). In this work, we propose PARDEN, which\navoids this domain shift by simply asking the model to repeat its own outputs.\nPARDEN neither requires finetuning nor white box access to the model. We\nempirically verify the effectiveness of our method and show that PARDEN\nsignificantly outperforms existing jailbreak detection baselines for Llama-2\nand Claude-2. Code and data are available at https://github.com/Ed-Zh/PARDEN.\n  We find that PARDEN is particularly powerful in the relevant regime of high\nTrue Positive Rate (TPR) and low False Positive Rate (FPR). For instance, for\nLlama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction in\nthe FPR from 24.8% to 2.0% on the harmful behaviours dataset.",
        "updated": "2024-05-13 17:08:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07932v1"
    }
]