MAxPrototyper: A Multi-Agent Generation System for Interactive
User Interface Prototyping
MingyueYuan JieshanChen AaronQuigley
UniversityofNewSouthWales CSIRO’sData61 CSIRO’sData61
Australia Australia Australia
mingyue.yuan@unsw.edu.au Jieshan.Chen@data61.csiro.au Aaron.Quigley@data61.csiro.au
ABSTRACT UIdesigntodayhasadvancedsignificantly,forexamplewiththe
Inautomateduserinteractivedesign,designersfacekeychallenges, useoflow-fidelityvisualmodelssuchaswireframestohelpcreate
including accurate representation of user intent, crafting high- high-fidelityfunctionalprototypes[8].Inaddition,techniquessuch
quality components, and ensuring both aesthetic and semantic asSwire[16],Screen2Vec[20],andVINS[6]incorporatesketches
consistency.Addressingthesechallenges,weintroduceMAxPro- orscreenshotstoretrieveGUIs,whileGuigle[4]andRaWi[17]
totyper,ourhuman-centered,multi-agentsystemforinteractive furtherenhanceretrievability,thussimplifyingtheprototyping
designgeneration.ThecoreofMAxPrototyperisathemedesign process.However,theseretrievedGUIimagesoftensufferfrom
agent.Itcoordinateswithspecializedsub-agents,eachresponsible limitationsintermsofvisualfidelity,creativityandreusability.
forgeneratingspecificpartsofthedesign.Throughanintuitiveon- WidelyusedGUIprototypingtools,suchasSketch[28],Adobe
lineinterface,userscancontrolthedesignprocessbyprovidingtext XD[1],Figma[11],typicallyofferacombinationoffundamental
descriptionsandlayout.Enhancedbyimprovedlanguageandim- GUIcomponentsandtemplates.However,theirinabilitytogen-
agegenerationmodels,MAxPrototypergenerateseachcomponent eratecustomizedresultsalsoaffectstheirsupportforthecreative
withcarefuldetailandcontextualunderstanding.Itsmulti-agent processandefficiencyoftheoveralldesignprocess.Additionally,
architectureenablesamulti-roundinteractioncapabilitybetween whilegenerativemethodssuchaslayout2image[5],VAE[25],Mid-
thesystemandusers,facilitatingpreciseandcustomizeddesign Journey[22]andstablediffusion[27]showcaseremarkablecreative
adjustmentsthroughoutthecreationprocess. potential,theirpracticalimplementationishinderedbydifficultto
control.resultinginunstructuredandnon-editableoutcomes.The
CCSCONCEPTS needformanualreconstructionandthevaguenessofcomponents
occupyingsmallareaswithintheoverallimagegeneration,suchas
•Human-centeredcomputing→SystemsandtoolsforInter-
“text”,“textbutton”,and“icon”,furthercomplicatetheseissues.
actiondesign;Computingmethodologies.
Toaddressthelimitationsofnon-editableoutputs,enhancethe
KEYWORDS creativepotentialofretrieval-basedmethods,andguaranteehigh-
quality prototype generation, we present our novel interactive
Interactivedesign,userinterface,conversationalagents,largelan-
designsystemMAxPrototyper—Multi-AgentCollaborationfor
guagemodels,imagegeneration
ExplainableUIPrototypeGeneration.Thissystemempowersde-
ACMReferenceFormat: signerstocrafthigh-fidelityprototypesthatarebothcustomizable
MingyueYuan,JieshanChen,andAaronQuigley.2024.MAxPrototyper:A andcomprehensibleateachstageofthegenerationprocess.
Multi-AgentGenerationSystemforInteractiveUserInterfacePrototyping. MAxPrototyperinitiatesthedesignjourneywithuser-provided
InExtendedAbstractsoftheCHIConferenceonHumanFactorsinComputing
inputssuchastextdescriptionsandawireframelayout.Thetext
Systems(CHIEA’24).ACM,NewYork,NY,USA,5pages.
providesthegeneraldesignrequirement(e.g.,“Startingpagefor
1 INTRODUCTION MAxPrototyper:aintelligentdesignassistant.”),andthewireframe
givesapreliminarylayoutoftheintendedUIdesign.
User interface (UI) design is an essential aspect of the modern
Specifically, this process draws inspiration from a top-down
softwareindustry,asitplaysasignificantroleinshapingtheoverall
designphilosophy.Itallowsoursystem,MAxPrototyper,tofirst
userexperience.WhiletraditionalUIdesignapproachesrequire
generateanoverallthemedesign,followedbytheiterativecreation
extensiveeffortandexpertise,numerousstudies[8,20,29,30,35]
ofeachindividualcomponents.Thisprocessensuresaestheticcon-
haveexploredandvalidatedthattheuseoftechniques,suchas
sistency throughout the design generation, thus maintaining a
deeplearning,canalleviatetheworkloadassociatedwithUIdesign.
coherentvisualstyleacrossallelementsoftheprototype.
Despitetheseadvancements,thereisstilladisconnectbetweenthe
Acorefeatureofoursystemistheabilityofacentralagent
toolsavailableandthedailypracticeofdesigners.
toutilizeacachepool,whichblendsaccumulatedresultstoeffec-
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor tivelyguidethesub-agents’actions,thusmaintainingtheintegrity
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed ofthedesigncontext.Thisstrategyenablesaccuratemulti-round
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
interaction capabilities. Designers are thus provided with an it-
onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish, erativeframeworkthatsupportsanin-depthunderstandingand
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora customizableadjustmentsateachgenerationphase.
fee.Requestpermissionsfrompermissions@acm.org.
Bybalancingautomatedgenerationandcustomization,weaim
CHIEA’24,May11–16,2024,Honolulu,HI,USA
©2024AssociationforComputingMachinery. tosupportastreamlineddesignprocessandtoolthatempowers
4202
yaM
21
]CH.sc[
1v13170.5042:viXraCHIEA’24,May11–16,2024,Honolulu,HI,USA MingyueYuan,JieshanChen,andAaronQuigley
designerswithdeeperinsightsandgreatercontrol.Theresulting thisdatathroughScreen2Words[32],whichaugmentsRicotopro-
prototypescanbeconvenientlysavedinSVGorJSONformat,fur- vide112khigh-leveltextualdescriptionsforits22kUIscreenshots.
therempoweringdesignerswithpracticalefficiency. Throughthisdataset,weobtained<UIdescription>.
BeyondUIfunctionalitysemantics,designgenerationnecessi-
2 METHOD tatesthoughtfulconsiderationofthemes,colors,andthetarget
audience. We adopt Blip2 [19], which is a zero-shot visual lan-
guagemodel,togeneratethemedescriptionsviaavisualquestion-
answering approach. We identify four key attributes for theme
design: theme color, primary color, theme description, and app
category,andcreateaspecificquestion,pairingitwithaUIim-
age,andtheninputitintoBlip2toderivetheanswer,whichis
showninTable1.Finally,thesethreedescriptionsareconcatenated
together,andformUISemanticKnowledge:<textcontent/iconde-
scriptions><highleveldescription><themedesigndescription>.
2.1.2 IconKnowledgeBase. WecollectedthedatafromGoogle
Figure1:Overview:usinguserpromptandUILayoutasin- MaterialDesignIcons[14],ahigh-qualityrepositorythatstores
put,MAxPrototyperutilizesamulti-agentmethodforUI over900diverseicons(inSVGformatandwithtextdescription).
prototypecreation.Itintegratesfourprimaryagents—Theme WeobtainourIconKnowledgeBase:<iconSVGcode><semantic
Design,TextContent,ImageContent,andIcon—withTheme description>.
Designasthecentralguidingthesub-agents. Table1:Blip2’sVQAinstructiontemplatesforscreenshots
AsillustratedinFig.1,oursystemconsistsofknowledgebases
andfourkeyagents:ThemeDesignAgent𝐴 𝑡ℎ𝑒𝑚𝑒,TextContent Attributes Blip2InstructionTemplates
Agent𝐴 𝑡𝑒𝑥𝑡,ImageContentAgent𝐴 𝑖𝑚𝑔andIconAgent𝐴 𝑖𝑐𝑜𝑛.We T Prh ie mm ae ryC Col oo lr
or
“ “W Beh sia dt ei ss tt hh ee bb aa cc kk gg rr oo uu nn dd ,wco hlo ar t’o sf thth eis dosc mre inen as nh to ct o? l”
orinthisimage?”
collectknowledgebasestoimportrelevantdesignknowledge.Build- ThemeDescription “Canyoudescribethisscreenshotindetail?”
AppCategory “Whichcategorydoesthisappbelongto?”
inguponthisfoundation,MAxPrototyperprocessesuserinputs,
comprisingbothapromptandaUIlayout.Thesysteminitiates
2.2 ThemeDesignAgent
itsworkflowwiththeThemeDesignAgent,whichformulatesa
high-levelthemedescriptionandgeneratesacorrespondingtheme 𝐴 𝑡ℎ𝑒𝑚𝑒 actsasthesupervisor,directingtheUIdesignprocess.Pow-
imagetoestablishacoherentandconsistentdesignnarrative. eredwithUIknowledgebasecollectedinSection2.1,itsetsthe
Servedasthemaindirector,𝐴 𝑡ℎ𝑒𝑚𝑒subsequentlyexecutespecial- generalstylebygeneratingtheglobalthemedescriptionandtheme
izedtasksthrough𝐴 𝑡𝑒𝑥𝑡,𝐴 𝑖𝑚𝑔and𝐴 𝑖𝑐𝑜𝑛forthedetailedcreation image.TheseinformationcanmakesurethegeneratedUIdesign
ofeachcomponent.Seamlesscoordinationbetweenthemainagent isofhighquality,coherentandconsistent.
andsupportingagentsisvitalforeffectiveness.Utilizingablend
2.2.1 KnowledgeRetrieval. Domain-specificknowledgeenhances
ofpreviousresultsandacachepool,thecentralagenteffectively
theaccuracyofLLM-generatedcontent.RecognizingtheLLM’s
informsthesub-agents,ensuringthedesigncontextismaintained.
token input limitations and the complexities of fine-tuning, we
introduceaknowledgeretrievalphase,infusingdomain-specific
2.1 KnowledgeBaseConstruction
knowledgeintoourgenerationprocess.
Employingdomain-specificknowledgeenhancesthecreativityand Basedontheuserprompt(𝐼𝑛 𝑝)andUIlayout(𝐼𝑛 𝑙),wewantto
outputqualityofLargeLanguageModels(LLMs).[21,33].Wecol- retrievethemostrelevantknowledgefromourlargeknowledge
lecttwoknowledgebases,namelyUIknowledge(pairsoftheme base. To do so, we concatenate these two information together,
descriptionswithlocalcomponentdescriptions)andIconKnowl- andencodethemintoavectorEmb(𝐼𝑛)asthequeryvector,where
edge(PairsofSVGcodewithdescriptions)for𝐴 𝑡ℎ𝑒𝑚𝑒 and𝐴 𝑖𝑐𝑜𝑛. 𝐼𝑛=𝐼𝑛 𝑝+𝐼𝑛 𝑙.WeusetheTEXT-EMBEDDING-ADA-002embedding
model[23]).Similarly,wealsoembedeachpieceofUIknowledge
2.1.1 UIKnowledgeBase.
relatedtoUICompositionandSemanticintoavector(Emb(𝑘𝑏 𝑗))
1)UICompositionKnowledge.RicoDataset[10]isoneofthe
aswell.Thenwecomputethecosinedistancebetweenqueryand
most comprehensive open-source UI datasets, including the UI
eachknowledge,andretrievethetop-kresultstoinstructourmulti-
screenshots,theirviewhierarchyinformationandtheirattributes
agentsystem1.Wedenotetheretrievedknowledgeas𝑟𝑒𝑓𝑒𝑟 𝑖.
liketext,boundsandclass,andthecompositionoftheseUIelements.
Ourpreliminaryexperiments,alongsidefindingsbyBryan[31],
Weextractedclassandboundsfromthesemetadata,andformthe
indicatethatwhenemployingrelatedknowledgeasfew-shotprompt-
UIcompositionknowledge(<componenttypes><boundingboxes>)
ing,theinitialexampletendstobethemostinfluential.Subsequent
foreachUI.
examplesoftenprovidediminishingreturnsinfocusingthemodel’s
2)UISemanticKnowledge.Thefine-grainedcomponentde-
output.Furthermore,giventheinputlengthlimitationsoflanguage
scriptioncanbeobtainedbyparsingcontent-descriptionfromthe
models,whichrestrictthenumberofexemplarsintheprompt,we
Ricodatasetmetadata.Weobtain<textcontent/icondescriptions>.
limitthenumberofreferencesto2.
WhileRicocontainsthemetadataofthecompositionandtext
contentsofUI,itlacksthehigh-levelUIdescription.Weobtain 1Weset𝑘=2inourexperiments.MAxPrototyper:AMulti-AgentGenerationSystemforInteractiveUserInterfacePrototyping CHIEA’24,May11–16,2024,Honolulu,HI,USA
2.2.2 Theme Description Generation. Given a user prompt thedesignatedpositionat[bbox].” Inalignmentwithequation2,the
𝐼𝑛 𝑝, a UI layout 𝐼𝑛 𝑙, and the top 𝑘 retrieved knowledge items executionpromptofthe𝐴 𝑡𝑒𝑥𝑡 obtainsitsvaluefromthecentral
w{(cid:205) it𝑘 𝑖 h=0 th𝑟 e𝑒𝑓 sy𝑒𝑟 s𝑖 te} m(w ’sh te hr ee m,𝑘 e= de2 s) c, rt iph te is oe nc po rm ompo pn te 𝑃n 𝑡ℎts 𝑒𝑚ar 𝑒e tc oo fn oc ra mte un laa tt eed
a
a ng ae ten dt’ wsc ita hch 𝑝e 𝑡𝑒, 𝑥d 𝑡e ,n toot ee nd sa us re𝐶 c𝑎 o𝑐 nℎ𝑒 si𝑡 s− t1 e, na cn yd inis ss yu stb es meq .uentlyconcate-
comprehensiveinputforouragent.
Uponcompletingthethemedescriptiongeneration,theresultant 2.4 ImageContentAgent
themedescriptionisdenotedby𝑅𝑒𝑠 𝑡ℎ𝑒𝑚𝑒. Toenhancethegenerationqualityoflocalimage-associatedcompo-
nentsandmaintaintheconsistency,wealsodeployouradaptively
2.2.3 ThemeImageGeneration. Theobjectiveofgeneratinga
themeimageistovisuallyguidetheoveralldesign,allowingthe
fine-tunedstablediffusionmodelin𝐴 𝑖𝑚𝑔,butdisablingControlNet
module.weusetheimagedescriptionfromthegeneratedthemede-
sub-agentstogeneratecoherentandconsistentdesign.
scriptionastheuserprompt(a).Ratherthanusingthelatentimage
WeconsiderStableDiffusionmodel[26],astate-of-the-arttext-
generatedfromGaussiannoise,weextracttheareaoftheimage
to-imagegenerationasourmainmodel.However,asthismodel
componentfromthethemeimageastheinput(b).Byharmonizing
onlyconsidersthetextcondition,itsuffersfromlimitedcontrol
bothtextualandvisualsignals,wecanguaranteethattheproduced
overthespatialcompositionoftheimage,acrucialaspectforour
contentalignsseamlesslywiththeprimarythemedesignintent.
UIdesigngeneration.Toaddressthis,weintegrateControlNet[34],
whichaugmentsthediffusionmodelbyprovidingenhancedspatial
2.5 IconAgent
controlovereachmodule.ControlNetcontrolsthegenerationby
manipulatingthedenoisingmodulebyimportingadditionalspatial 𝐴 𝑖𝑐𝑜𝑛 is crucial for selecting appropriate icons and integrating
conditioninUNet.WeemployUIlayoutasthespatialcondition. them into the graphical user interface components. In addition
Inaddition,asthestablediffusionmodelfacesobstacleswhen toactingasintuitivevisualcues,well-designediconscanimprove
generatingUIimages,whichrequiresadifferentdomainknowledge comprehensionandtheoveralluserexperience.Thesystemprompt
fromgeneralimages[9].Wefurtherfinetunethemodelusingthe for𝑝 𝑖𝑐𝑜𝑛,is:“Inreferencetorelevantinformationandtakinginto
datasets of UI screenshots and their complementary high-level accountitspositioningat[bbox],andbasedonthethemedescription,
descriptionscollectedinSection2.1. proposeanindicativephraselike“msg”forthe“Icon”..Asshown
inequation(2),executionpromptisbasedonthecentralagent’s
2.2.4 Sub-agentExecution. Duringsub-agentexecutionphase,
cache,𝐶𝑎𝑐ℎ𝑒 𝑡−1,combinedwith𝑝 𝑖𝑐𝑜𝑛.Thisapproachensuresthe
ThemeDesignAgentidentifiestheoptimalsub-agentcorrespond-
iconsselectedmatchtheGUIdesignsemanticallyandvisually.
ingtothecomponenttype.Weconsidered13componenttypes,as
detailedbytheRicodataset.Toelaborate,𝐴 𝑡𝑒𝑥𝑡 handles“TextBut-
3 EXPERIMENT
ton”and“Text”.𝐴 𝑖𝑚𝑔isentrustedwith“Image”and“Background
Image”,andthe𝐴 𝑖𝑐𝑜𝑛 focuseson“Icon”components.Forother 3.1 ExperimentsSetup
componenttypes,weseektorenderthemeditable,drawingin-
FromSection2.1.1,weintotalcollectedasetof3,738UItextual
sightsfromRaWi[18].Thecolorforanycomponentisdetermined
descriptions,theircorrespondingwireframesandUIscreenshots.
byidentifyingthedominantRGBcolorfromtheimageregion’s
Forvalidationpurpose,theUIscreenshotsaretreatedastheground
histogramandthenrepresentingitinHTMLcode.
truth.Itisessentialtonotethatthisdataisreservedwithinthe
Thedynamicbetweenthecentralagentandthesub-agentsises-
testsetfolderofRicoandhasnotbeenutilizedduringthemodel’s
sentialtooursystem’sfunctionality.Whenacentralagentengages fine-tuningphase.Imagesareresizedtodimensionsof512×512.
asub-agent,itusesacombinationofpastresultsandacachepool
Toassessthequalityanddiversityofgeneratedresults,weutilize
toinformthesub-agent’sprompts:
twometrics:FréchetInceptionDistance(FID)[15]andGeneration
𝐶𝑎𝑐ℎ𝑒 𝑡 =𝑅𝑒𝑠 𝑡−1+𝐶𝑎𝑐ℎ𝑒 𝑡−1 (1) Diversity(GD)[7],whichareusedintheimagegenerationtask[7,
12].FIDevaluatesthesimilaritybetweengeneratedresultsandreal
𝑝 𝑡+1=𝑝 𝑠𝑢𝑏+𝐶𝑎𝑐ℎ𝑒 𝑡 (2)
ones,whileGDassessesthediversityofthegeneratedprototypes.
Inthiscontext,𝑅𝑒𝑠 𝑡−1istheoutputfromthesub-agentforthe
𝑡−1𝑡ℎ component.The𝐶𝑎𝑐ℎ𝑒 𝑡 representsacachepoolthatinte- 3.2 ResultsandDiscussion
gratesthepreviousresultwithaccumulatedknowledgefromearlier
3.2.1 RQ1:Howdoesourfine-tunedmodelperformagainst
iterations.Thiscachepoolservesasanessentialmemoryfunction,
baselinemodelsinbothqualityanddiversity?
retainingthedesigncontextandfacilitatingmulti-turninteractions.
Baselines.ForRQ1,weconsidertwostate-of-the-artimagegen-
Meanwhile,𝑝 𝑡+1functionsasthepromptforthe𝑡−1𝑡ℎ component’s erationmodels:stable-diffusion-1-5[2]andstable-diffusion-2-
sub-agentinteraction,incorporatingboththespecificprompt𝑝
𝑠𝑢𝑏 1[3]asthesetwobaselinesdonotincorporateControlNetmodule,
forthecurrentsub-agentandthecumulativeknowledgein𝐶𝑎𝑐ℎ𝑒 𝑡. weconsidervariantsofbothbyintegratingControlNet,denotedas
stable-diffusion-1-5(withControlNet),stable-diffusion-2-1
2.3 TextContentAgent
(withControlNet).Inaddition,wealsoemployanablatedversion
Theprimaryroleof𝐴 𝑡𝑒𝑥𝑡istogeneratetextualinformationtailored ofourapproach,MAxProtytper(w/oControlNet)asabaseline.
tospecificGUIcomponents.WeuseGPT-4[24].Thesystemprompt Results.AsseeninTable2,Forgenerationquality,weconsis-
forthisagent,representedas𝑝 𝑡𝑒𝑥𝑡,is:“Basedonthethemedescrip- tentlyoutperformsbothbaselinemodelsintermsofFIDscores,
tionandrelevantdetails,provideatextcontentrecommendationfor regardlessofwhetherControlNetisutilized.Specifically,alowerCHIEA’24,May11–16,2024,Honolulu,HI,USA MingyueYuan,JieshanChen,andAaronQuigley
Table2:ComparativeanalysisofFIDandGDscoresamong “KnowledgeRetrieval”moduleisessentialfordataimportation,
variousmodelswithandwithouttheControlNet withitsremovalincreasingtheFIDscorefrom23.76to42.56,sig-
nificantlyimpactingdesignquality.“ThemeImageGeneration”is
Model FID GD vitalforlinkingtextandvisuals,seestheFIDscoreriseto33.08
whenomitted,underscoringitssignificanceinvisualintegration.
stable-diffusion-1-5(w/oControlNet) 69.48 15.93
The“Sub-agentExecution”phase,withspecializedagentsforeach
stable-diffusion-2-1(w/oControlNet) 67.15 15.42
designelement,demonstratesthevalueofadetailedgeneration
MAxPrototyper(w/oControlNet) 33.08 15.95
approachthroughminorFIDscorevariations.
stable-diffusion-1-5(withControlNet) 54.42 11.48 Inconclusion,“KnowledgeRetrieval”and“ThemeImageGen-
stable-diffusion-2-1(withControlNet) 57.23 11.14 eration”arethemostinfluential,butallmodulescollaboratively
MAxPrototyper 23.76 13.98 enhancethefinaloutput.
Table3:Resultsofablationstudyfordifferentmodules 4 DISCUSSIONANDFUTUREWORK
Ourcurrentwork,MAxPrototyper,focusesonimprovingthedesign
Method FID GD generation process and enhancing the quality of prototypes. It
MAxPrototyper 23.76 13.98 drawsinspirationfromarichknowledgebaseandprimarilyrelies
onhigh-leveluserdescriptionsandwireframelayoutstotranslate
-RetrievedKnowledgeItems 42.56 12.14
userintentintotangibledesignprototypes.Lookingahead,there
-ThemeDescriptionGeneration 28.43 11.77
areseveraldirectionsofimprovementsandexpansionsthatwecan
-ThemeImageGeneration 33.08 12.95
dointhefuture:
-TextContentAgent 24.06 13.78
1)AutomatedWireframeGeneration. Tofurtherstreamline
-ImageContentAgent 24.71 13.38
theuserexperience,wecouldofferautomaticgenerationofrelevant
-IconContentAgent 24.32 13.41
wireframesbasedontheuser’shigh-leveldescriptions,thusremov-
ingoneextrastepfromtheuser’sside.Inthecurrentiterationof
ourwork,wechosenottoincludethisfeature,becausewewantto
FIDscoresuggeststhatthedistributionofgeneratedimagesmore
focusonrefiningthegenerationprocessofthedesignprototypes
closelymatchesthatofrealimages.ThisindicatesthatMAxPro-
fromprovidedwireframesandhigh-leveldescriptions.Integrating
totyper’soutputsaremorerealistic,evidentfromitssignificantly
automatedwireframecreationpresentsitsowndistinctchallenges,
reducedFIDscores:33.08withoutControlNetandanevenlower
particularlyinensuringthatthecreatedwireframesaccuratelyand
23.76withControlNet.
satisfactorilyreflecttheuser’sdesignintent.However,recognizing
Fordiversity,Notably,GDscoresreflectthemodel’sabilityto
itspotentialbenefits,weregarditasapotentialaspectforfuture
producevariedyetdetailedUIdesigns.AhigherGDindicatesmore
enhancement.
detail,suggestingthatthedesignsarediverseandintricateintheir
2)DynamicComponentIntegration.Currently,oursystem
presentation.MAxPrototyper’sGDscores,bothwithandwithout
mainlydealswithstaticcomponents,whichwesimplyclassifyinto
ControlNet,surpassthoseofthebaselinemodels.Thisincreasein
text,images,andicons.Ourfutureworkcandiscussaboutexpand-
GDvaluesemphasizesourmodel’ssuperiorcapabilityinproducing
ingthistoincludemoredynamicandinteractiveelements,such
designsthatarevariedandenrichedwithdetailscomparedtoits
ascheckboxesandpickers,thusmakingourgeneratedprototypes
peers.
morefunctionalandinteractive.
ForControlNet,IncorporatingControlNetresultsinnoticeable
3)ToolIntegrationandDesign-to-Code.Withanaimtoseam-
improvementsinFIDscoresforallmodels.ForourMAxPrototyper,
lesslyincorporateoursystemintodesigners’workflow,weimagine
theFIDshowsanenhancementof10.68,representingasubstantial
MAxPrototyper could be developed as a plugin that integrates
32%improvement.
smoothly with popular wireframe sketching software, such as
Inconclusion,OurMAxPrototyperoutperformsthebaseline
Figma[11]andSketch[28].Thiswouldwidenoursystem’sfunc-
modelsintermsofbothqualityanddiversity.TheadditionofCon-
tionalitybyprovidingdesignerswithrobusteditingcapabilities
trolNetoptimizesthisperformancefurtherbyintroducinglayout
directlywithintheseplatforms.Additionally,futureworkcouldalso
constraints,reinforcingitspotentialforgeneratingrealisticand
explorewaystoautomatedesign-to-codeconversiontoprovide
detail-orientedUIdesigns.
designersanddeveloperswithaend-to-endsolution.
3.2.2 RQ2:HowdoMAxPrototyper’sindividualmodules 4) Usability and Accessibility. As we continue to improve
influenceitsperformanceinqualityanddiversity? MAxPrototyper,wearecommittedtoaugmentingoursystemwith
BaselinesandAblationStrategy.Tobettercomprehendthe standardmobiledeviceuserinterfaceguidelineslikematerialde-
interactionandindividualimpactofMAxPrototyper’scomponents sign[13].Theseimprovementswillfurtherenhanceoursystem’s
ontheendresult,weperformanablationstudy,sequentiallyre- usabilityandaccessibility,ensuringourprototypesnotonlylook
movingeachmoduleandevaluatingtheeffect.AsseeninTable3, appealingbutalsoofferintuitiveanduser-friendlyinteractions.
wecarefullycraftedsixablations. Byfocusingonthesepotentialdirectionsandcontinuallyrefin-
Results.MAxPrototyper’sanalysishighlightseachmodule’s ingourprocessesbasedonuserfeedback,wecanbettercombine
critical role in achieving high-quality, diverse UI designs. The automationefficiencywithcreativefreedom.MAxPrototyper:AMulti-AgentGenerationSystemforInteractiveUserInterfacePrototyping CHIEA’24,May11–16,2024,Honolulu,HI,USA
REFERENCES
[31] BryanWang,GangLi,andYangLi.2023.Enablingconversationalinteractionwith
[1] Adobe.2023.AdobeXD.https://adobexdplatform.com/. mobileuiusinglargelanguagemodels.InProceedingsofthe2023CHIConference
[2] StabilityAI.2022.stable-diffusion-v1-5.https://huggingface.co/runwayml/stable- onHumanFactorsinComputingSystems.1–17.
diffusion-v1-5. [32] B.Wang,G.Li,X.Zhou,Z.Chen,andY.Li.2021. Screen2Words:Automatic
[3] StabilityAI.2023.stable-diffusion-2-1.https://huggingface.co/stabilityai/stable- MobileUISummarizationwithMultimodalLearning.(2021).
diffusion-2-1. [33] GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,
[4] C.Bernal-Cardenas,K.Moran,M.Tufano,Z.Liu,L.Nan,Z.Shi,andD.Poshy- LinxiFan,andAnimaAnandkumar.2023.Voyager:Anopen-endedembodied
vanyk.2019.Guigle:AGUISearchEngineforAndroidApps.ACM(2019). agentwithlargelanguagemodels.arXivpreprintarXiv:2305.16291(2023).
[5] SanketBiswas,PauRiba,JosepLladós,andUmapadaPal.2021. Docsynth:a [34] LvminZhangandManeeshAgrawala.2023. AddingConditionalControlto
layoutguidedapproachforcontrollabledocumentimagesynthesis.InDocument Text-to-ImageDiffusionModels. arXiv:2302.05543[cs.CV]
AnalysisandRecognition–ICDAR2021:16thInternationalConference,Lausanne, [35] Tianming Zhao, Chunyang Chen, Yuanning Liu, and Xiaodong Zhu. 2021.
Switzerland,September5–10,2021,Proceedings,PartIII.Springer,555–568. GUIGAN:LearningtoGenerateGUIDesignsUsingGenerativeAdversarialNet-
[6] S.Bunian,K.Li,C.Jemmali,C.Harteveld,andM.S.El-Nasr.2021.VINS:Visual works. arXiv:2101.09978[cs.HC]
SearchforMobileUserInterfaceDesign.(2021).
[7] N.Cao,X.Yan,Y.Shi,andC.Chen.2019. AI-Sketcher:ADeepGenerative
ModelforProducingHigh-QualitySketches.AssociationfortheAdvancementof
ArtificialIntelligence(AAAI)(2019).
[8] JieshanChen,ChunyangChen,ZhenchangXing,XinXia,LimingZhu,John
Grundy,andJinshuiWang.2020.Wireframe-BasedUIDesignSearchthrough
ImageAutoencoder. 29,3,Article19(jun2020),31pages. https://doi.org/10.
1145/3391613
[9] JieshanChen,MulongXie,ZhenchangXing,ChunyangChen,XiweiXu,Liming
Zhu,andGuoqiangLi.2020.ObjectDetectionforGraphicalUserInterface:Old
FashionedorDeepLearningoraCombination? CoRRabs/2008.05132(2020).
arXiv:2008.05132 https://arxiv.org/abs/2008.05132
[10] BiplabDeka,ZifengHuang,ChadFranzen,JoshuaHibschman,DanielAfergan,
YangLi,JeffreyNichols,andRanjithaKumar.2017.Rico:AMobileAppDataset
forBuildingData-DrivenDesignApplications.InProceedingsofthe30thAnnual
SymposiumonUserInterfaceSoftwareandTechnology(UIST’17).
[11] Figma.2023.Figma.https://www.figma.com/.
[12] S.Ge,V.Goswami,C.L.Zitnick,andD.Parikh.2020.CreativeSketchGeneration.
(2020).
[13] Google.2023.MaterialDesign.https://material.io/.
[14] Google.2023.MaterialIcons.https://github.com/google/material-design-icons.
[15] M.Heusel,H.Ramsauer,T.Unterthiner,B.Nessler,andS.Hochreiter.2017.GANs
TrainedbyaTwoTime-ScaleUpdateRuleConvergetoaLocalNashEquilibrium.
(2017).
[16] F.Huang,J.F.Canny,andJ.Nichols.2019.Swire:Sketch-basedUserInterface
Retrieval.Inthe2019CHIConference.
[17] KristianKolthoff,ChristianBartelt,andSimonePaoloPonzetto.2023.Correction
to:Data-drivenprototypingvianatural-language-basedGUIretrieval.Automated
SoftwareEngineering30,1(2023).
[18] KristianKolthoff,ChristianBartelt,andSimonePaoloPonzetto.2023. Data-
drivenprototypingvianatural-language-basedGUIretrieval.AutomatedSoftware
Engineering30,1(2023),13.
[19] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.2023.Blip-2:Bootstrapping
language-imagepre-trainingwithfrozenimageencodersandlargelanguage
models.arXivpreprintarXiv:2301.12597(2023).
[20] J.J.Li,L.Popowski,T.Mitchell,andB.A.Myers.2021.Screen2Vec:Semantic
EmbeddingofGUIScreensandGUIComponents.(2021).
[21] PanLu,BaolinPeng,HaoCheng,MichelGalley,Kai-WeiChang,YingNianWu,
Song-ChunZhu,andJianfengGao.2023.Chameleon:Plug-and-playcomposi-
tionalreasoningwithlargelanguagemodels. arXivpreprintarXiv:2304.09842
(2023).
[22] MidJourney.2023.MidJourney.https://www.midjourney.com.
[23] OpenAi.2023. Embeddings-OpenAIAPI. https://platform.openai.com/docs/
guides/embeddings.
[24] OpenAi.2023.GPT4-OpenAIAPI.https://platform.openai.com/docs/models/gpt-
4.
[25] AliRazavi,AaronVandenOord,andOriolVinyals.2019.Generatingdiverse
high-fidelityimageswithvq-vae-2.Advancesinneuralinformationprocessing
systems32(2019).
[26] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörn
Ommer.2022.High-resolutionimagesynthesiswithlatentdiffusionmodels.In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.
10684–10695.
[27] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörn
Ommer.2022. High-resolutionimagesynthesiswithlatentdiffusionmodels.
2022IEEE.InCVFConferenceonComputerVisionandPatternRecognition(CVPR).
10674–10685.
[28] Sketch.2023.Sketch.https://sketch.io/.
[29] Y.Su,Z.Liu,C.Chen,J.Wang,andQ.Wang.2021. OwlEyes-Online:AFully
AutomatedPlatformforDetectingandLocalizingUIDisplayIssues.
[30] AmandaSwearnginandYangLi.2019. Modelingmobileinterfacetappability
usingcrowdsourcinganddeeplearning.InProceedingsofthe2019CHIConference
onHumanFactorsinComputingSystems.1–11.