A Generalist Learner for Multifaceted Medical Image Interpretation
Authors:Hong-YuZhouPhD1,SubathraAdithanMD2,JuliánNicolásAcostaMD1, EricJ.TopolMD3,
PranavRajpurkarPhD1
Affiliations:
1. DepartmentofBiomedicalInformatics,HarvardMedicalSchool,Boston,USA.
2. JawaharlalInstituteofPostgraduateMedicalEducationandResearch,Puducherry,IN.
3. ScrippsResearchTranslationalInstitute,ScrippsResearch,LaJolla,CA,USA.
Correspondingauthor:
PranavRajpurkar,PhD
pranav_rajpurkar@hms.harvard.edu
Abstract
Currentmedicalartificialintelligencesystemsareoftenlimitedtonarrowapplications,hinderingtheir
widespreadadoptioninclinicalpractice.Toaddressthislimitation,weproposeMedVersa,a
generalistlearnerthatenablesflexiblelearningandtaskingformedicalimageinterpretation.By
leveragingalargelanguagemodelasalearnableorchestrator,MedVersacanlearnfrombothvisual
andlinguisticsupervision,supportmultimodalinputs,andperformreal-timetaskspecification.This
versatilityallowsMedVersatoadapttovariousclinicalscenariosandperformmultifacetedmedical
imageanalysis.WeintroduceMedInterp,thelargestmultimodaldatasettodateformedicalimage
interpretation,consistingofover13millionannotatedinstancesspanning11tasksacross3
modalities,tosupportthedevelopmentofMedVersa.OurexperimentsdemonstratethatMedVersa
achievesstate-of-the-artperformancein9tasks,sometimesoutperformingspecialistcounterparts
byover10%.MedVersaisthefirsttoshowcasetheviabilityofmultimodalgenerativemedicalAIin
implementingmultimodaloutputs,inputs,anddynamictaskspecification,highlightingitspotentialas
amultifunctionalsystemforcomprehensivemedicalimageanalysis.Thisgeneralistapproachto
medicalimageinterpretationpavesthewayformoreadaptableandefficientAI-assistedclinical
decision-making.
1Figure1|Overviewofthestudy.MedVersahasbeendevelopedandvalidatedusingacurated
datasetcomprisingradiographs,dermoscopyimages,computedtomographyscans,andmedicaltext
data.Theresultingmodeliscapableofperforming11vision-languageandvision-centrictasks.In
practicalapplications,MedVersacanacceptawiderangeofinputs,includingimagesofvarioustypes
(e.g.,multimodal,multiview,ormultiperiod)andnaturallanguagerequests.Themodel'slearnable
orchestrator,poweredbyalargelanguagemodel(LLM),independentlyassesseswhethertoexecute
thetaskonitsownortointegratevisualmodelingmodules.Bycombininglanguagemodelsand
visionmodules,MedVersacanlearnfrombothvisualandlinguisticsupervision,aswellasgenerate
multimodaloutputs,showcasingitsversatilityandpotentialforreal-worldapplicationsinmedical
imageanalysis.
Introduction
Thefieldofmedicalartificialintelligence(AI)hasbeenadvancingatarapidpace,usheringinanew
eraofdiagnosticaccuracyandpatientcare.Withinthisdynamiclandscape,researchershavebeen
focusingtheireffortsondevelopingsolutionsforspecifictasks,suchasidentifyingchestpathologies
1–5andclassifyingskindiseases.6–8Similarly,themajorityofmedicalAIproductsapprovedbytheUS
FoodandDrugAdministrationforclinicalusehavebeendesignedtoaddressoneortwospecific
tasks.9However,thistask-specificapproachmaylimitthereal-worldclinicalapplicationsoftheseAI
systems,astheymaynotbeabletoadapttothediverseandcomplexneedsofhealthcaresettings.
10,11
Addressingthisconcern,generalistmedicalartificialintelligence(GMAI)wasproposedto
utilizerecentadvancesinfoundationmodels12formoreflexibleproblemsolving.10However,
contemporaryGMAImodelshavebeendesignedtolearnfromnaturallanguagesupervision.13–17
2Althoughthesemodelsworkwellinvision-languagetasks,itdoesnotreadilyapplytoamajorityof
vision-centricproblems,suchasdetectionandsegmentation,whichareindispensabletomedical
imageinterpretation.18–20
InspiredbytheGMAIparadigm,weproposeMedVersa,ageneralistlearnercapableof
multifacetedmedicalimageinterpretation.AtthecoreofMedVersaistofunctionthelargelanguage
modelasalearnableorchestrator,whichlearnstoorchestratethemultimodalinputsandexecute
tasksusinglanguage/visionmodules.ThisarchitecturaldesignequipsMedVersatoovercomethe
limitationsoftraditionalapproaches13–17byintegratingbothvisualandlinguisticsupervisionwithinits
learningprocesses,whileatthesametimesupportingon-the-flytaskspecificationwithlanguage.
MedVersaisaversatilemodelthatexcelsinbothvision-languagetasks,suchasgeneratingradiology
reportsandansweringvisualquestions,andvision-centricchallenges,includingdetectinganatomical
structuresandsegmentingmedicalimages(Fig.1).ThisdualcapabilityenablesMedVersatotrainon
diversemedicaldataacrossmultiplemodalitiesandtasks,resultingingeneral,shared
representations.
TofacilitatethedevelopmentofMedVersa,wehavecuratedadiverseandmultimodal
datasetcalledMedInterp,whichisspecificallydesignedformultifacetedmedicalimage
interpretation(Fig.2).MedInterpisanextensivedataset,containingover13millionannotated
instancesandcoveringawiderangeofvision-languageandvision-centrictasks.Bytrainingand
assessingMedVersaontheMedInterpdataset,wehavedemonstratedthatitsurpasses
state-of-the-artspecialistcounterpartsinninetasks,oftenbynotablemargins(Fig.2).Forinstance,in
thetaskofradiologyreportgeneration,MedVersaoutperformsbothMAIRA-121,aspecialistlarge
multimodalmodelfromMicrosoft,andMed-PaLMM13,ageneralistbiomedicalfoundationmodel
fromGooglethatis10timeslargerthanMedVersa.Moreover,MedVersaalsoexcelsinvisual
localizationtasks,surpassingthewell-establishedobjectdetector22intwolocalizationtasks.
Additionally,MedVersademonstratessuperiorperformancecomparedtostate-of-the-artspecialist
methodsinvariousothertasks,includinglongitudinalstudycomparisons,region-of-interest
captioning,open-endedVQA,andchestpathologyclassification.Themodel'sconsistentandsuperior
performancehasbeenfurthervalidatedonsixexternalcohorts,highlightingitsrobustnessand
generalizability.
3Figure2|Dataandperformance.a,Relativeimprovements(overthespecialistcounterpart)in11
tasks.MedVersaachievesover5%relativeimprovementsinseventasks.b,Numberofinstances
includedbytheMedInterp.Wepresentthedatadistributionsacrossdifferenttasksettings.
4Results
Reportgeneration
Table2presentstheevaluationresultsonthreesections:findings,impression,andtarget
(concatenationoffindingsandimpression)usingfiveevaluationmetrics:BLEU-423,BertScore24,
CheXbert25,RadGraph26,andRadCliQ.27Forthefindingssection,amongthebaselines,MAIRA-121
achievesahigherBLEU-4scoreof14.2,whileMed-PaLMM13producesabetterRadGraphscoreof
26.7,bothofwhicharethecurrentstate-of-the-art.SinceMAIRA-1andMed-PaLMMarenotpublicly
accessible,weaddedanothercompetitivebaseline,ClsGen28,forconsistentcomparisonsacross
differentsections.ClsGenachievesahigherBLEU-4scorethanMed-PaLMM.
TheproposedMedVersawasevaluatedacrossallsectionsandmetrics.Itoutperformsall
baselinesinthefindingssectionwithaBLEU-4scoreof17.8(vs.14.2ofMAIRA-1),aCheXBertscore
of46.4(vs.44.0ofMAIRA-1),andaRadGraphscoreof28.0(vs.26.7ofMed-PaLMM),establishing
itssuperiorityandsettingthenewstate-of-the-artincapturingboththelinguisticandclinicalaspects
ofradiologyreporting.ParticularlynoteworthyisthattheresultsofMed-PaLMMwereobtainedfrom
asignificantlylargermodel,withtentimesmoreparametersthanthoseofMedVersa.Thisimplies
thatthelattermodelismoreadvantageousintermsoftrainingandinferenceefficiency.Forthe
impressionsection,MedVersasurpassesClsGeninallevaluationmetrics,andthesamesuperiorityis
maintainedwhenallsectionsarecombined.WeperformedexternalvalidationontheIUX-raydataset
29(Table2),whereMedVersakeepsmaintaininganotableadvantageoverClsGen.
Vision-centrictasks
Fordetectiontasks,MedVersaexhibitscompetitiveperformance,surpassingYOLOv522by
noticeable,consistentmarginsinthedetectionofavarietyofanatomicalstructures(Fig.3a),with
mostIoUscoresoncertainstructuressurpassing0.6.Itshowsparticularlyhigheffectivenessinthe
detectionoflungzones.Whenidentifyingchestpathologies,MedVersa'scapabilitiesexceedthoseof
YOLOv5,notablyinthedetectionof27outof33conditions(Fig.3b).Italsomaintainsahigher
averageperformancecomparedtoYOLOv5(0.303vs.0.278).OntheexternalcohortNIHChestXray,
MedVersaalsooutperformsYOLOv5byanaverageofnearlytwopercentindetectingcommonchest
pathologies(Table2).
Regardingsegmentationtasks,MedVersademonstratescompetitiveresults,performing
competitivelytonnUNet30andnnSAM.31Allthreeapproachesperformfairlywellinsegmenting
majorchestorgans(Fig.3c)andskinlesions(Fig.3d).Nonetheless,MedVersaoutperformsnnUNet
andnnSAMbysignificantmarginsinchestmajororgansegmentation.Inthetaskofabdominalorgan
segmentation(Fig.3e),MedVersaalsoshowscompetitiveperformancetonnUNet3Dwhichuses
complexandtime-consumingdataaugmentationtechniques.Table3showcasesthesegmentation
5resultsofskinlesionsandabdominalorgans.
Figure3|Experimentalresultsofvision-centrictasks.WecomparedMedVersaagainstYOLOv5on
2detectiontasks:a,anatomicalstructureandb,chestpathologydetection.Formedicalimage
segmentation,wemainlycomparedMedVersatonnUNetonc,chestmajororgan,d,skinlesion,and
e,abdominalorgansegmentation.Besides,nnSAMwasincludedasabaselinefor2Dsegmentation
tasks(candd).TheevaluationmetricsofdetectionandsegmentationtasksareIoU(Intersectionover
Union)andDICEsimilarityscores,respectively.Weprovided95%confidenceintervalsineach
subfigure.
6Figure4|Experimentalresultsofclassificationtasks.MedVersawascomparedtodifferent
specialistmodels-DAM(DeepAUCMaximization)andCRCKD(CategoricalRelation-preserving
ContrastiveKnowledgeDistillation)-ona,chestpathologyandb,skinlesionclassification,
respectively.95%confidenceintervalswereprovidedalongwithF1scores.
Longitudinalstudycomparisons,open-endedVQA,andregion-of-interest
captioning
Inlongitudinalstudycomparisons,themodelistypicallytaskedwithdrawingacomparative
conclusionbetweentwogroupsofimagescollectedatdifferentperiods.Thispresentsasignificant
challengeforimageinterpretation,asmodelsmustworkwithmultipleimagestoanalyzevarious
anatomicalstructures,extractingfeaturesandidentifyingsubtledisease-relatedchanges.Asshown
inTable1,thebaselinemethodEKAIDbuildscomplexanatomicalstructure-awaregraphstoencode
anatomicalanddiseasefeaturesforrecognizingthedifferencesbetweenCXRstudies.Incontrast,
MedVersaadoptsastraightforwardyeteffectivewaytoprocesslongitudinalimages(see.Fig.6e).
Moreover,MedVersalargelyoutperformsEKAIDacrossdifferentmetrics(BLEU-4:44.7vs.40.4,
BertScore:71.4vs.69.1,CheXbert:50.0vs.49.1,RadGraph:23.7vs.20.4,RadCliQ:2.05vs.2.19).
AsTable1displays,MedVersaoutperformsPTLM32,astate-of-the-artmodelforopen-ended
medicalVQA,byanaverageofsixpercentinBLEU-4,BertScore,CheXbert,andRadGraphscores.
MedVersaalsoachievesa30%lowerRadCliQscorecomparedtoPTLM.The95%confidence
intervalsindicatethattheimprovementbroughtbyMedVersacanbestatisticallysignificant.The
resultontheexternalcohortalsovalidatestheadvantageofMedVersa(Table2).
Inthetaskofregion-of-interestcaptioning,MedVersashowsanobviousadvantageover
MiniGPT-v2acrossvariousmetrics.TheRadCliQscore,whichcomprehensivelyevaluatesthelexical
andclinicalsignificanceofgeneratedtext, issubstantiallylowerforMedVersaat2.70versus3.08for
MiniGPT-v2,suggestingcaptionsofMedVersaaresemanticallymorealignedwithreference
standards.TheresultfromanexternalcohortfurtherconfirmsthebenefitofMedVersa,asshownin
Table2.
7Figure5|Comparativeanalysesoflearningfrommultimodalsupervision.Westudytheimpactof
trainingMedVersawithdifferenttypesoftasks(i.e.,withdifferentformsofsupervision).VCdenotes
trainingwithvision-centrictasks,whileVLstandsfortrainingwithvision-languagedata.
Classificationtasksinradiologyanddermatology
Fig.4presentsperformancecomparisonsofMedVersaagainstDAM33inchestpathology
classificationandagainstCRCKD34inskinlesionclassification,bothofwhicharetopperforming
modelsintheirrespectivefields.MedVersademonstratessuperiorperformanceoverDAMwithan
8averageF1scoreof0.615,notablyhigherthanDAM's0.580inchestpathologyclassification(Fig.4a).
Thispatternofoutperformanceextendsin29out33pathologies,includingbothcommon(e.g.,lung
opacity,pulmonaryedema,spinalfracture)andlesscommonones(e.g.,hydropneumothorax,
bronchiectasis),whichindicatesMedVersa'sstrongdiagnosticaccuracyacrossvariousconditions.In
skinlesionclassification,MedVersa'sadvantageisalsonoticeable.TheaverageF1scoreofMedVersa
is0.772,appreciablyaboveCRCKD's0.750,underscoringMedVersa'seffectivenessinclassifying
skinconditions(Fig.4b).ItisworthnotingthatMedVersaoutperformsCRCKDbysignificantmargins
inbenignkeratosis-likelesions(bkl),whichhasadiverserangeofsubtypes.Thisfurther
demonstratesthegeneralizationabilityofMedVersa.Forexternalvalidation(Table2),MedVersa
againsurpassesDAMbyalargemarginonCheXpert3,whichalsoexceedsthemeanperformanceof
radiologists(F1score:0.734vs.0.610).5
Enhancingmedicalimageunderstandingwithvisualandlinguisticsupervision
ThecomparativeanalysesinFig.5demonstratethatintegratingvision-centricandvision-language
trainingparadigmscanlargelyenhanceAIperformanceacrossvarioustasks.Incorporating
vision-centrictrainingalongsidevision-languagetrainingleadstoanaverageimprovementof4.1%
comparedtomodelstrainedsolelyonvision-languagedata.Thissuggeststhatvision-centrictraining
allowsthemodeltodevelopamorecomprehensiveunderstandingofvisualinformation,enabling
betterinterpretationandutilizationofvisualcuesindifferentcontexts.
Similarly,combiningvision-centricandvision-languagetasksduringtrainingyieldsanaverage
improvementof3.3%overmodelstrainedexclusivelyonvision-centrictasks.Thisfindinghighlights
thecrucialrolethatlinguisticsupervisionplaysinenhancingthemodel'scapabilitytocomprehend
andreasonaboutvisualinformation.Theobservedimprovementsinskinandabdomensegmentation
taskssuggestthatvision-languagetrainingmayhavestrengthenedthemodel'sabilitytofollow
instructionsmoreeffectively.Byincorporatingbothvisualandlinguisticsupervisionintothetraining
process,MedVersaisabletolearnmoregeneralandcomprehensiverepresentationsthatcapturea
widerarrayoffeatures,relationships,andsemanticmeanings.Theseresultsemphasizethe
significanceofdevelopingGMAImodelsthatcanlearnfromacombinationofvisualandlinguistic
supervisioninordertoachieveoptimalperformanceandgeneralization.
9Discussion
Tothebestofourknowledge,MedVersaisthefirstGMAImodelthatsupportsmultimodaloutputs,
inputs,andon-the-flytaskspecification.TrainedonMedInterp,amedicaldatasetencompassing11
differenttasksacrossthreeimagingmodalities,MedVersasetsthenewstate-of-the-artinreport
generationandoutperformshighlycompetitivespecialistmodelsinbothvision-languageand
vision-centrictasks.ThedevelopmentofMedVersapotentiallyunlocksnewopportunitiestobuild
moreversatileGMAImodels.Moredetailedperspectivesareprovidedinthefollowing.
MedVersaintegratesvisualandlinguisticsupervisionthroughitsmultimodal-outputdesign.
MedVersadistinguishesitselffrompreviousendeavorsby seamlesslyincorporatingbothvisualand
textualguidanceinitstrainingprocess.ThisuniqueapproachallowsMedVersatotackleawiderange
ofmedicaltasks,fromgeneratingradiologyreportstosegmentingmedicalimages.Themodel's
abilitytoassimilateknowledgefromvariousinputtypesandgeneratemultimodaloutputsresultsin
thedevelopmentofgeneral androbustsharedrepresentations,whichhelpsboostthemodel
accuracyonthetasksandalleviatepotentialbiasesinthedata.Theincorporationofmultimodal
outputsinMedVersa'salsoalignswiththelatestprogressingenerativeAI,wheretheuseofvaried
andall-encompassingtrainingdatahasyieldedpromisingresults.Bygaininginsightsfrombothvisual
andtextualcues,MedVersaconstructsamorecomprehensivegraspofmedicalinformation,paving
thewayformorepreciseanddependablediagnoses.Itscapacitytoadapttoimpromptutask
specificationsrendersMedVersaamultifacetedandflexibleinstrumentfordiverseclinical
applications,establishingitsplaceasausefulresourceinmedicalAIforthoroughdiagnostics.
Largelanguagemodelsactaslearnableorchestrators.Unlikepreviousendeavorsthatusedlarge
languagemodelsasstandalonelanguagepredictors,thelargelanguagemodelinMedVersa
transcendsitstraditionalrolebyactingasalearnableorchestratorcapableofinterpretingmedical
vision-languagedataandcoordinatingwithvisionmodules.ThisdesignallowsMedVersatoleverage
thestrengthsofboththelargelanguagemodelandspecialistcomponents,resultinginamore
comprehensiveandeffectivesystemformedicalimageinterpretation.Theintegrationofvision
moduleswithinMedVersa enhancesitscapabilityinareaswherelanguage-basedmodels
traditionallyfalter,suchasdetailedimageanalysisrequiredinchestabnormalitydetectionandskin
lesionsegmentation.Thecomprehensiveapproach,combiningthecontextualdecision-makingofthe
largelanguagemodelwiththeprecisionofvisionmodules,offersamorerobustandversatile
diagnostictool.Thisneworchestrationrepresentsanewstepbeyondthelimitationsofprevious
medicalfoundationmodels,offeringanewperspectiveofintegratinglargelanguagemodelsinto
generativemultimodalmedicalAI.
Impactofdatasetcomposition.WhilethemajorityofthedatausedtotrainMedVersaconsistsof
X-rayimages,withasmallerproportionofdermatology(derm)andcomputedtomography(CT)data,
thisimbalancedoesnotfundamentallyimpactthevalidityofourgeneralistmodeltrainingapproach.
Ourprimaryfocusisoninvestigatinghowtoeffectivelylearnfromvisualandlinguisticsupervision
andhowtointegratemultipletaskswithinasinglemodel.ThechoicetopredominantlyuseX-raydata
wasdrivenbyitswideavailabilityandtheprevalenceofassociatedtextreports,whichfacilitatethe
explorationofourresearchquestions.Nevertheless,weacknowledgethattheinclusionofamore
diverserangeofimagingmodalitiescouldpotentiallyenhancethemodel'sgeneralizationcapabilities.
Futureworkcouldexploretheimpactofincorporatingamorebalanceddatasetwithahigher
10proportionofotherimagingmodalities,suchasderm,CT,andmagneticresonanceimaging(MRI).
Despitethislimitation,webelieveourcurrentstudyprovidesvaluableinsightsintotheeffectiveness
oflearningfromvisualandlinguisticsupervisionandthefeasibilityofmulti-taskintegrationina
generalistmodel.
ExtensibleGMAIandbeyond.Thesystemdesignfeaturesanotablelevelofextensibility,allowingfor
thepracticalintegrationofnewvisionmodulesintoitsexistingframework.ThisaspectofMedVersa
enablesittoadaptandgrowinresponsetoevolvingmedicalimagingtechniquesanddiagnostic
requirements.DifferingfromtraditionalmedicalAImodels,MedVersaintegratesthelargelanguage
modelinthewaythatprovidesaextensibleplatformfortheadditionofnewspecialistmodelsas
advancementsinmedicaltechnologyoccur.Thisfeatureensuresthattheoverallsystemremains
up-to-dateandeffectiveinafieldcharacterizedbyrapidtechnologicalchangesandemerging
diagnosticchallenges.Asnovelmedicalimagingmethodsareintroduced,MedVersacanbeupdated
tomaintainitsrelevanceinthedynamiclandscapeofmedicaldiagnostics.Thismodulardesignnot
onlypreparesitforfutureadvancementsbutalsoencouragesongoingimprovementandinnovation
withinthesystem.IthighlightsthepotentialofMedVersaasanextensibleandadaptablesolutionin
medicalAI,equippedtoaddressthevariedandchangingrequirementsofhealthcarepractitioners
andpatientsinacontinuouslyevolvingmedicalenvironment.
Potentialtostreamlineclinicalworkflows.Theimpactofourworkisprominentinofferingaunified
solutionthatcanhelpstreamlinetheclinicalworkflowswithmedicalAIproducts.Incontrast,
task-specificmodels,designedforindividualtasks,maycomplicateorfragmentworkflows,
necessitatingthemedicalprofessionalstoswitchbetweenmultiplesystems.Forinstance,inabusy
metropolitanhospital,theradiologydepartmentfaceschallengesmanagingahighvolumeofdiverse
imagingtasksdaily,fromurgentchestX-rayinterpretationstoCTscansrequiringdetailedanalysis.
TheintroductionofMedVersaallowsforaseamlesstransitionbetweenthesetaskswithinasingle,
integratedplatform.Previously,radiologistshadtoswitchbetweenmultiplespecialistAImodules,
eachwithitsowninterfaceanddiagnosticfocus,leadingtoinefficienciesanddelaysinpatientcare.
ThecomprehensivecapabilityofMedVersatointerpretvarioustypesofmedicalimagesmeansthat
radiologistscouldefficientlyworkthroughtheircaseloads,significantlyreducingtheturnaroundtime
fordiagnosticreports.Thisstreamlinedprocessnotonlyimprovesoperationalefficiencybutalso
ensuresthatpatientsreceivefasterdiagnoses,leadingtoquickertreatmentdecisionsandbetter
outcomes,finallyincreasingtheadoptionrateofAIproductsinreal-worldclinicalsettings.
Roadtofullorchestration.AchievingfullorchestrationofMedVersainvolvesseveralstrategic
advancements.First,broadeningitsdatasetscopetoencompassawiderarrayofmedicaldatatypes,
suchasdetailedelectronichealthrecords,comprehensivegeneticinformation,andreal-timepatient
monitoringdata,iscrucial.ThisdiversificationwillenhancethediagnosticaccuracyofMedVersaby
providingamoreholisticviewofpatienthealth.Second,incorporatingcutting-edgeAIandmachine
learningmodules,particularlyinevolvingareasofnaturallanguageprocessingandcomputervision,
willrefineitscapabilitytointerpretandanalyzecomplexmedicaldatasetsaccurately.The
developmentandintegrationofadvancedmodulesforeffectivedatasynthesisandnuanced
interpretationareessentialforprovidingcomprehensivemedicalinsights.Thispathalsoincludes
rigorousattentiontoethical,privacy,andsecurityissues,ensuringMedVersa'soperationwithina
frameworkthatprioritizespatientconfidentialityanddataintegrity.Ultimately,thefullorchestration
ofMedVersaaimstotransformhealthcaredeliverythroughpersonalized,efficient,and
11broad-spectrummedicalanalyses,leadingtosuperiorpatientcareandoptimizedhealthcare
processes.
Limitations.Despitetheadvancements,thereareinherentlimitationsthatwarrantconsideration.
OneprimaryconcernliesinthedependencyofMedVersaonthequalityanddiversityofthedataused
fortrainingthemodels.Ifthedatasetisnotsufficientlyvariedorrepresentativeoftheglobal
population,thereisariskofbiasintheAI-generateddiagnostics,potentiallyleadingtolessaccurate
outcomesforcertaindemographicgroups.Additionally,thecomplexityofintegratingvariousvision
moduleswiththelargelanguagemodelposeschallengesinensuringseamlessinteroperabilityand
maintainingtheconsistencyofthesystem'soverallperformance.ThedynamicnatureofMedVersa,
whileadvantageousforadaptability,alsoraisesquestionsaboutthelong-termmanageabilityand
scalabilityofthesystem,especiallyasitcontinuouslyevolvestoincludenewmodalitiesandnetwork
modules.Moreover,theinterpretabilityandexplainabilityofthedecision-makingprocessof
MedVersaremainsacriticalarea.ThecomplexinteractionsbetweendifferentAImodelscanobscure
thereasoningbehindspecificdiagnosticconclusions,makingitchallengingformedicalprofessionals
tofullyunderstandandtrusttherecommendations.Theselimitationsunderscoretheneedfor
ongoingresearchanddevelopmentinenhancingtherobustness,transparency,andethical
considerations,ensuringitalignswiththehigheststandardsofclinicalpracticeandpatientcare.
12Methods
Datasetsanddatapreprocessing
WecuratedMedInterptomorecomprehensivelytrainandevaluatemedicalFMsforthepurposeof
medicalimageinterpretation.AnoverviewofMedInterpwaspresentedinTable4.Specifically,
MedInterpconsistsof10publiclyavailabledatasets,someofwhichareassociatedwithmorethan
onetask.
MIMIC-CXR.Thisisalarge,publiclyaccessibledatasetcomprising377,110chestX-rays(CXRs)
correspondingto227,835radiographicstudiesperformedattheBethIsraelDeaconessMedical
CenterinBoston,MA.4Thedatasetwasfullydeidentified,andtheprotectedhealthinformationwas
alsoremoved.Wereferredtotheofficialsplit35andcombinedstudieswith‘train’and‘validate’tags
intothetrainingset,whiletherestwereincludedinthetestset(forinternalvalidation).Thefree-text
radiologyreportpreprocessingfollowedthestepsinCXR-RePair.36Specifically,weextracted
sectionsofindication,comparison,findings,andimpressionfromfree-textradiologyreportsvia
keywordsmatching.Then,wefilteredoutstudieswithemptyfindingsandimpressionsections.After
thesesteps,wecanobtain149,711(2,144)findingssectionsand189,411(2,212)impressionsections.
Numbersinparenthesesdenotethesamplesizeofthetestset.Besides,wealsoextractedcomplete
radiologyreports,i.e.,reportsthathavefindingsandimpressionsections.Thisresultedin122,702
(1,437)completereports,whichwerealsoinvolvedintrainingandinternalvalidationstagesalong
withsectionsoffindingsandimpressions.NotethatsomestudiesmayhavemorethanoneCXR,and
imagesofMIMIC-CXRwerealsousedinothertasks.
ChestImaGenome.Thisdatasetaugmentedthefree-textreportsofMIMIC-CXR4,35withlocal
annotationsderivedfrombothrule-basednaturallanguageprocessing(NLP)andatlas-based
boundingboxdetection.37TheseannotationsareintricatelylinkedthroughCXRontologies
developedbyradiologists,forminganatomy-centeredscenegraphs.Wefollowedthedatasplitof
MIMIC-CXRtoavoidtrainingandtestsetsleakage.Thechestpathologyclassificationtaskincluded
235,721CXRswithannotationsof33pathologies(Fig.4a).AvastmajorityofCXRshavebounding
boxannotationsof36anatomicalstructures(seeFig.4a),leadingto8,425,163boxesintotal.Wealso
exploitedtheanatomy-centeredgraph-structuredannotationofChestImaGenome.Forchest
pathologydetection,wefirstidentifiedconnectionsbetweenpathologiesandanatomies.Next,we
canassignboundingboxesofanatomiestoassociatedpathologiesthatweremarkedpositive.A
similarstrategywasalsoadoptedforregion-of-interestcaptioning,whereconnectionsbetween
sentencesfromfree-textreportsandanatomieswereextractedusingNLPtechniques.38Afterthis,
wehadtextualcaptionsgroundedonanatomies.Sothetaskinputwouldbetheboxcoordinatesof
anatomies,andtheoutputwouldbetheassociatedcaptions.
Medical-Diff-VQA.Thisisapubliclyavailabledatasetcontainingavastnumberofquestion-answer
pairsbasedonCXRs.39Toconstructthisdataset,keywordsofabnormalityandtheirattributeswere
firstcollected.Then,regularexpressionswereutilizedtodetectabnormality/diseasekeywordswithin
thefree-textreportsofeachpatientvisitinMIMIC-CXR.4,35Theseidentifiedkeywordsservedas
anchortermstosegmentthesentences,andnearbytextsectionswerethenscannedfortherelevant
attributekeywords.Theaccuracyandcompletenessoftheextractedinformationhavebeencarefully
checkedbyhumansandadvancedNLPtools.39Inpractice,weleveragethecodeinanopensource
13repositorytogeneratethedatasets.39Sinceopen-endedvisualquestionansweringisourmainfocus,
wereducedthenumberofyes/noquestion-answerpairsbysettingthe‘less_yes_no’variableinthe
codetoTrue.Thisresultsin383,683normalquestion-answerpairs,whereeachpairisassociated
withonefrontalCXR,and147,269longitudinalcomparisons,whereeachcomparisonencompasses2
studies,andeachstudymaycontainmorethanoneCXR.WeusedthesamedatasplitasinChest
ImaGenomeandMIMIC-CXRtoavoidtrainingandtestinformationleakageacrossdifferentdatasets.
Tobuildacohortforexternalvalidation,weappliedthedatasetconstructioncodetothefree-text
reportsofIUX-ray29toextract2,883normalquestion-answerpairs.
HAM10000.Thisisabulkcollectionofmulti-sourcedermatoscopicimagesofcommonpigmented
skinlesions.40Thedatasetcomprises10,015imagecasesandencompassesadiversecollectionof
significantdiagnosticcategorieswithinthedomainofpigmentedlesions.Thesecategoriesinclude
Actinickeratosesandintraepithelialcarcinoma/Bowen'sdisease(akiec),basalcellcarcinoma(bcc),
benignkeratosis-likelesions(bkl),dermatofibroma(df),melanoma(mel),melanocyticnevi(nv),and
vascularlesions(vasc).Fortheskinlesionclassificationtask,weused1,511imagesfromISIC2018
taskthreeasthetestsetforinternalvalidation.41Forskinlesionsegmentation,werandomlysplitthe
datasetintotrainingandtestsets.Theratioofthetrainingsettothetestsetis9:1.Wetrainedskin
classificationmodelsontherawdatasetsdirectlywithoutanyclassbalancingskills.42
AbdomenCT-1K.ThisisacollectionofabdominalCTscans,constructedtoenhanceexisting
single-organdatasetswithannotationsoffourabdominalorgansannotations:liver,kidney,spleen,
andpancreas.43Weused991scanswithpubliclyavailablesegmentationmasksformodeltraining
andvalidation.Theywererandomlysplitintotrainingandvalidationsets,theratiobetweenwhichis
9:1.ConsideringthelargeamountofGPUmemorycostfortraining3Dsegmentationnetworks,we
resizedeachscantoa3Dvolumesized192(width)×192(height)×64(depth)pixels.Wealsoclipped
theHounsfiledUnitvaluesat[-200,300].
CheXmask.Theisalarge-scaledatasetofanatomicalsegmentationmasksformulti-centerchest
radiographs.Inpractice,weincorporatedCXRs(239,931images)fromMIMIC-CXR4,35formodel
trainingandinternalvalidation,whilethevalidationset(200images)ofCheXpert3wasusedfor
externalvalidation.EachCXRwasassociatedwithsegmentationmasksofthreemajorchestorgans:
leftlung,rightlung,andheart.
CheXpert.Thisisanotherlargepublicdatasetforchestradiographinterpretation,which
retrospectivelycollectedthechestradiographicexaminationsfromStanfordHospital,performed
betweenOctober2002andJuly2017.3Inourcase,weuseditstestset(500studies,668images)
withstronggroundtruthforexternallyvalidatingtheresultsofthechestpathologyclassification
task.Thetestsetlabelswereestablishedthroughthemajorityvoteofannotationsfromfive
radiologists,withthreeofthembeingthesameasthosewhoannotatedthevalidationset,whilethe
othertwowererandomlyselected.Besides,200imagesinthevalidationsetofCheXpertwerealso
usedinthetaskofchestmajororgansegmentation.
14Figure6|OverviewofMedVersa.a,ProcessingworkflowofMedVersa.Dashedarrowsindicatethat
theassociatedproceduresarecontingentuponthedecisionregardingtheutilizationofthevision
module.Redarrowsrepresenttheoperationsundertakenwhenemployingthevisionmodule.There
arethreekindsof<Task>inMedVersa:<DET>,<2DSEG>,and<3DSEG>.b,Architectureofthe
vision-languageadapter.c,Illustrationoftheworkflowforchestpathologydetection.d,Illustrationof
theworkflowforabdominalorgansegmentation.e,Illustrationoftheworkflowforlongitudinalstudy
comparisons.
IUX-ray.Thedatasetcontains7,470pairsofCXRsandradiologyreports.29Itservedastheexternal
validationsetforthereportgenerationtask.Tomaintaintheconsistencyofcross-datasetvalidation,
wefilteredoutreportsthatdonotcontainsectionsoffindingsandimpressionsimultaneously,
resultingin3,323studies.EachstudyhasonefrontalandonelateralCXRs,associatedwithone
radiologyreport.NotethatimagesofIUX-raywerealsousedintheexternalvalidationofthe
open-endedvisualquestionansweringtask.
NIHChestX-ray.Thisdatasetincludesover100,000anonymizedCXRsofmorethan30,000
individualsfromtheNIHClinicalCenter.2Apartfromimage-levelpathologylabels,NIHChestX-ray
alsoprovidesasmallnumberofboundingboxannotations.Inpractice,weincorporatedthebox
annotations(577boxes)offourcommonchestpathologies-atelectasis,cardiomegaly,effusion,and
pneumothorax-intotheexternalvalidationsetforchestpathologydetection.
MS-CXR.Thedatasetoffersphrasegroundingannotationsthatarelocallyalignedbyboard-certified
radiologists,aimingtosupportresearchinthedomainofcomplexsemanticmodelingforbiomedical
vision-languagetasks.44,45Eachphraseisassociatedwithatleastoneboundingboxannotatedon
oneCXR.Fortheregion-of-interestcaptioningtask,weusedMS-CXRastheexternalvalidation
cohort,whereboxcoordinateswerepassedtothemodeltogeneratedescriptivetext.
15Pipeline
AsshowninFig.6a,MedVersaiscomposedofthreecomponents:themultimodalinputcoordinator,
thelargelanguagemodelbasedlearnableorchestrator,andavarietyoflearnablevisionmodules.In
practicalusage,MedVersaexpectsinputsintheformofimage-requestpairs.Notethatthevision
inputmayconsistofmorethanoneimage,whichcanbemultimodal,multiviewormultiperiod(see
Fig.1).MedVersaautonomouslydecideswhethertousea2Dora3Dvisionencodertoprocessthe
visioninputsbasedonananalysisoftheinputmodality.Afterreceivingtheprocessedinputs,the
largelanguagemodelcandecidewhethertoindependentlyperformthetaskorutilizeasetofvisual
modelingmodulesforassistance.Thisdynamicdecision-makingprocessensuresthattasksare
handledwiththeappropriatelevelofexpertiseandefficiency.
MedVersa,anorchestratedGMAIsystem
Multimodalinputcoordinator.AsFig.6adisplays,themultimodalinputcoordinatorcomprisesthe
generalvisionencoders,thevision-languageadapters,andthetokenizer.Wedesignthisarchitecture
bytakinginspirationsfromMiniGPT-446,47,LLaVA-Med48,andMed-PaLMM13butkeepthe
architectureeasyforimplementation.Thegeneralvisionencodersaretheprimarygateforvision
inputs.Specifically,weexploitdistinctencodersfor2Dand3Dimagingdata,respectively.The2D
visionencoderutilizesthetransformerarchitecture49toextractvisualtokensfromtheimages.For
the3Dencoder,werefertotheencoderfromthe3DUNet.50Theextractedvisualtokensare
concatenatedandpassedtotheadaptertogetmappedtothelanguagespace.Here,wepresentan
efficientdesignofthevision-languageadapter,whichonlycontainsastackofthreelayers(Fig.6b).
ThefirstlayerisresponsibleforreducingthenumberofvisualtokenstocontroltheGPUmemory
cost,whichcanbeachievedwithanadaptivepoolingfunction.51Next,thelayernormalization52is
appliedtothepooledvisualtokens,followedbyalinearprojectionlayertomapthevisual
representationstothelanguagespace.Toalignwiththe2Dand3Dvisionencoders,wealsoemploy
twoindependentadapterstoprocesstheextractedvisualtokensaccordingly.Meanwhile,thepaired
requestisprocessedwiththeLlamatokenizer53,whichisabyte-pairencodingmodelbasedon
sentencepiece.54Therequestistransformedintoaseriesoftextualtokens,whicharethen
contextualizedbythefollowinglargelanguagemodelalongwiththemappedvisualtokens.This
enablesthesystemtounderstandandcorrelatethevisualdatawiththerelevantrequests.
Orchestratedmodeling.Unlikepriorresearchthatdependedexclusivelyonlargelanguagemodels
(LLMs)fortaskexecution,MedVersaleveragestheplanningcapabilitiesofthelargelanguagemodel
toactasalearnableorchestratorofsystemoperations.Specifically,theorchestratorhastodecide
whethertocarryoutthetaskindependentlyoruseaspecificvisionmoduleforsupportbasedonthe
analysisofvisualandlinguisticdata.Thisdecision-makingprocesscanbeformulatedas:
𝑙𝑙𝑚 (𝐼, 𝑇)→(𝑙𝑙𝑚 , 𝑠 ).𝐼and𝑇denote theextractedvisualandtextualtokens,respectively.𝑙𝑙𝑚
θ 𝑜 𝑜𝑘 θ
standsforthelargelanguagemodel.𝑙𝑙𝑚 and𝑠 representtheoutputsofthelanguagemodelandthe
𝑜 𝑜𝑘
𝑘thvisionmodule,respectively.Forvision-languagetasks,MedVersaonlyadoptsthe𝑙𝑙𝑚 asthe
𝑜
16Figure7|Visionmodules.a,Visualdetectionmodule.b,2Dvisualsegmentationmodule.c,3Dvisual
segmentationmodule.Conv2dandConv3dstandforthe2Dand3Dconvolution,respectively.GN
denotesthegroupnormalizationlayer,andLReLUrepresentstheleakyReLUactivationfunction.We
initializedtheencoderofthespecialistmodulefor2Dsegmentationusingthepretrainedweightsof
ResNet-18onImageNet.Theredarrowsdenotetheskipconnections.
finallanguageresponse.Forvision-centrictasks,thechoiceof𝑘isdeterminedbasedonthe𝑙𝑙𝑚 .
𝑜
Specifically,the𝑙𝑙𝑚 determinesthetasktypeandgeneratestherelevant<Task>inthe𝑙𝑙𝑚 .There
θ 𝑜
arethreekindsof<Task>includedinMedVersa:<DET>,<2DSEG>,and<3DSEG>.Thepredicted
<Task>guidesthesysteminselectingthekthvisualmodelingmodulefromthepool,tailoredfor
executingthetaskdescribedby<Task>(seeFig.6aformoredetails).Meanwhile,weindexthe
correspondinglatentembeddingsof<Task>fromtheoutputlogitsofthe𝑙𝑙𝑚 .Theseembeddings
θ
gathertheinformationfromtheinputdataandhelppromptthevisualmodelingmoduletocomplete
thedesiredtask.Toaccomplishthis,theindexedlatentembeddingscanbeeitherpasseddirectlyto
17thevisualdetectionmoduleorintegratedwiththeintermediatefeaturesofthevisualsegmentation
modules.WeillustratetheorchestrationprocessonthreetasksinFig.6,includingthechest
pathologydetection(Fig.6c),theabdominalorgansegmentation(Fig.6d),andthelongitudinalstudy
comparisons(Fig.6e).
Visionmodules.InMedVersa,wehaveincorporatedthreemodulesdesignedforvision-focused
tasks,andthesecanbereadilyexpandedorreplacedifadditionalornewdedicatedmodulesbecome
necessary.AsshowninFig.7a,wedevelopalightweightvisualdetectionmodulethatcanbe
integratedwiththeorchestrator.Forthevisualsegmentationmodules,weemploy2D55and3D
UNets50for2Dand3Dimagesegmentationtasks,respectively(Fig.7band6c).Weinitializethe
encoderofthe2DUNetusingthepretrainedweightsofResNet-1856onImageNet.57Wehave
attemptedseveraldifferentapproachestoincorporatetheindexedembeddingsfromthelarge
languagemodelintothevisionmodules.Ourobservationisthatthefeatureconcatenationoraddition
outperformsthemorecomplexoperation,suchascrossattention.58Basedonthis,weaddthe
indexedembeddingstotheintermediatefeaturemapsinsegmentationmodules,whilefeedingthese
embeddingstothedetectionmoduledirectly.Notethatalldedicatedmodulesarelearnableandneed
tobetrainedwiththeotherpartsofMedVersa.
Modeltrainingandtestingwithmeticulous,referringimageinstructions.ThesuccessofAlpaca,
alongwithrecentadvancementsinlargelanguagemodels59–62,haveunderscoredtheimportanceof
incorporatingdiverseinstructionstoconsolidatemultipletasksandenhancegeneralization
capabilitiesduringsupervisedfine-tuning.Thiscompellingevidencepromptedustoembracethis
conceptwithinMedVersa.
Weproposereferringimageinstructiontuning,whereimageidentifiersareaddedto
instructionstospecifydifferentimages.Thistechniqueenhancesthemodel’scapabilitytoperform
complexcomparativeanalyses ,suchasthelongitudinalstudycomparisons,whereweneedtoassign
imagestodifferentstudiesandcomparestudiesinsteadofimages.Forexample,inFig.6e,theexact
inputtoMedVersaforlongitudinalstudycomparisonsislike:
‘<img0>𝑣 </img0><img1>𝑣 </img1><img2>𝑣 </img2><img3>𝑣 </img3>Highlightanydifferencein
0 1 2 3
<img0><img1>comparedtothepriorstudy<img2><img3>.’
𝑣 standsforthevisualtokensofthe𝑖thinputimage.Weshowcaseallinstructionsusedinthemodel
𝑖
traininginTable5.Foreachtaskinourstudy,weaskedChatGPTtogenerateamaximumof20
prompts,eachadheringtoapredefinedtemplate.Thistemplateconsistsoftheinitialinstructionfor
eachtask,providingastructuredstartingpointfortheprompts.Afterthis,weconductedamanual
review,carefullysiftingthroughthegeneratedpromptstoeliminateanythatweresimilarinnature,
ensuringthatweretainedonlythemostdiverseanddistinctpromptsforouranalysis.Duringthe
trainingandtestphases,foragivensamplecorrespondingtoaspecifictask,wechoosean
instructionatrandomfromthesetofinstructionslinkedtothetask.
Here,weoutlinethemethodforcreatinggroundtruthlabelsforvarioustasksandsamples.
Forvision-languagetasks,naturallanguageanswersaredirectlyutilizedasthetargetfortrainingthe
model.Inparticular,forclassificationtasks,themodelisinstructedtoproducethenamesof
diagnoses.Whendealingwithvision-centrictasks,weemploydistinctlabelingtechniquesfor
detectionandsegmentation.Indetectiontasks,duringeachtrainingiteration,weinitiallyselectupto
nineclassesatrandomandconveytheirnames(togetherwitharandomlychoseninstruction)tothe
model.Themodelistrainedtoappendeither<N/A>or<DET>tagsfollowingeachclassname.<N/A>
indicatestheabsenceofthecorrespondingclassintheinputimage,whereas<DET>signifiesits
18presence.Subsequently,weidentifythelatentembeddingsof<DET>tagsandusetheminthevisual
detectionmodulefordeterminingboundingboxcoordinates.Forsegmentationtasks,asingleclass
nameisrandomlychosenfromtheset,andthisname,alongwiththeinstruction,isfedintothe
model.Themodelisthentrainedtogenerateeither‘Thesegmentationmaskof[classname]is
<2DSEG>’or‘Thesegmentationmaskof[classname]is<3DSEG>,’dependingonwhetherthetaskis
2Dor3Dsegmentation.Asindetection,therelevantembeddingsfor<2DSEG>or<3DSEG>arethen
passedtotheappropriate2Dor3Dvisualsegmentationmodulestocreatethesegmentationmasks.
Domain-awareminibatchgradientdescentformultimodalmultitasktraining.Unlikecurrent
medicalFMs13–15thatonlyprobedthevision-languagecapability,MedVersaneedstobetrainedon
bothvision-languageandvision-centrictasks,whichbringschallengestoclassicminibatchgradient
descentoptimization.63Toaddressthis,weproposedomain-awareminibatchgradientdescent,
wherethecoreideaisconstructingminibatchesusingtrainingsamplesfromthesametaskandthe
sameimagingmodality.Practically,weinitiallydividethetrainingdataintosevengroupsbasedon
theirtaskattributes:reportgeneration,classification,detection,segmentation,VQA,region
captioning,andlongitudinalcomparisons.Subsequently,wedynamicallygenerateminibatchesfor
eachgroupbyrandomlysamplingtrainingdatawithmatchinginputimagingmodalities.Thismeans
thateachminibatchshouldconsistofhomogeneousdatapertainingtoaspecifictaskandasingle
imagingmodality.Forexample,oneminibatchcouldcompriseexclusivelyofsamplesfeaturingthe
segmentationtaskonCTscans,whileanothermaycontainonlysampleswithdetectionannotations
onCXRs.
Duringeachtrainingiteration,webeginbyrandomlyselectinganimagingmodality.
Subsequently,fromthetaskpoollinkedtothismodality,werandomlychooseataskandsampledata
pertainingtothatspecifictask.Gradientdescentisappliedseparatelyforeachminibatch.Thisallows
themodeltooptimizespecificallyforthetaskandtheimagingtypesinthatbatch,leadingtomore
efficientlearningandbetteroverallperformance.Wealsoutilizedifferentlossfunctionstailoredto
differenttasktypes.Forexample,acrossentropylossisusedforvision-languagetasks,whilea
combinationofthecrossentropyandregressionlossesisappliedtothedetectiontask.Forthe
segmentationtask,weemployboththefocalloss64andtheDICEloss65,withequalweightsassigned
toeach.
Implementationdetails
Forthe2Dvisionencoderinthemultimodalinputcoordinator,weusethebaseversionofSwin
Transformer66pretrainedonImageNet.57Thisencoderischaracterizedbyitsfour-stagestructure,a
windowsizeofseven,apatchsizeoffour,andaninitialfeaturedimensionof128.Forthe3Dvision
encoder,weadopttheencoderarchitecturefromthe3DUNet.50Forspecifictaskslikereport
generation,classification,open-endedVQA,andlongitudinalstudycomparisons,theencoder
processestheinputimagesthrougharandomcroppingtechnique,wherethecroppedarearanges
from50%to100%oftheoriginalimage.Thesecroppedimagesarethenresizedtoastandard
dimensionof224×224pixelswiththreechannels.Differentaugmentationtechniquesareapplied
basedonthenatureofthetask.Forchestorganandskinlesionsegmentationtasks,arandom
horizontalflipisappliedtoeachimage.InthecaseofabdomenCTscans,amorecomplex
manipulationisperformedbyflippingeach3Dvolumeoverarandomaxis.Toefficientlymanagethe
volumeofvisualtokens,MedVersautilizesanadaptiveaveragepoolingstrategy,standardizingthe
outputlengthtonine.Additionally,thesystemimplementstwodistinctlinearprojectorsfor2Dand
193Ddata.Eachprojectorcomprisesafullyconnectedlayer,transformingeachpooledvisualtokeninto
a1Dvectorof4,096elements.
WeinitializedtheLLM-basedorchestratorusingthemodelweightsofLlama-2-Chat.53The
trainingoftheorchestratorinMedVersaemploystheLow-RankAdaptation(LoRA)strategy67aswe
foundthatitoutperformedfull-parametertraining.LoRAutilizestheconceptoflow-rankmatrix
decompositiontoapproximatealargeweightmatrixinneuralnetworklayers.Bysettingtherankand
alphavaluesofLoRAto16,themethodensuresefficienttrainingwhilemodifyingonlyafractionof
themodelparameters. TheAdamWoptimizer68,incombinationwithacosinelearningrate
scheduler,isusedforoptimization.Trainingparametersaremeticulouslyset,withaninitiallearning
rateof3e-4andaminimumof3e-6,over500,000trainingiterations.Thefirst3,000iterations
involvealinearwarm-upphase,startingwithalearningrateof1e-7.Finally,thetraininginfrastructure
comprises24NVIDIAA100GPUs(80G).Thissetupallowsthetrainingstagetobecompletedwithin
a72-hourwindow.
WeuseF1scoreinsteadofAUC(AreaUndertheCurve)forevaluatingclassificationtasks
becauseF1scorebettercapturesthemodel'sperformanceonbothrecallandprecision.Incontrast,
AUCsummarizesperformanceacrossallpossiblethresholdsforclassifyinganinstanceaspositive.It
doesn'temphasizethemodel'sperformanceatthehigh-precision,high-recallregion.Indisease
classificationtasks,itiscrucialtohaveahighF1scorebecausefalsenegatives(failingtoidentifya
patientwiththedisease)andfalsepositives(incorrectlydiagnosingahealthypatientwiththe
disease)canbothhaveseriousconsequences.Therefore,theF1scoreisusedinourexperimentsasit
balancesbothprecisionandrecall,ensuringthatthemodelcorrectlyidentifiesahighproportionof
truediseasecaseswhileminimizingmisdiagnoses.Inpractice,wefoundadefaultthreshold0.5is
sufficient.Consideringtherobustnessandgeneralizationtonew,unseendata,weusethisthreshold
whencomputingprecisionandrecallscores.
Baselines
● ClsGen.Thisisadifferentiableend-to-endmethodwiththreeparts:aclassifier,agenerator,
andaninterpreter.28Theclassifierlearnsdiseasefeaturesthroughcontextmodelinganda
disease-stateawaremechanism.Thegeneratorturnsthediseaseinformationintoamedical
report.Theinterpreterthenreviewsandrefinesthesereports,ensuringtheyalignwiththe
classifier'sfindings.WeempiricallyfoundClsGenshowedmoreconsistentperformance
comparedtopopularreportgenerationapproaches,suchasR2Gen69andM2Trans.70
● DAM.Thismethodisparticularlyrelevantforaddressingcomplexclassificationproblems,
especiallywhendealingwithimbalanceddatasets.33WeincludedtheDAMsupervised
methodasabaselineforchestpathologyclassification,whichcurrentlyisstate-of-the-arton
theCheXpertdataset.5
● MAIRA-1.ThisisaspecialistlargemultimodalmodelforreportgenerationfromMicrosoft.21
ItadoptedtheLLaVA-1.5architecture.71,72MAIRA-1alsobenefitsfromtheuseofGPT-3.5for
dataaugmentation,adding131,558reportswithparaphrasedfindingsandindicationsections
tothetrainingset.MAIRA-1producesreportswithstate-of-the-artquality.
● Med-PaLMM.ThisisalargegeneralistbiomedicalAIsystemfromGoogle.13Med-PaLMM
wasbuiltbyfinetuningwithbiomedicaldataontopofPaLM-E73,ageneralistmultimodalFM
trainedonnon-medicalimagesandtext.Here,wecomparedtoitsbestvariantthathas84
billionparameters,whichmaintainsthestate-of-the-artinthetaskofreportgeneration.
20● PTLM.Itisthestate-of-the-artapproachonopen-endedmedicalvisualquestionanswering.
32PTLMmapstheextractedvisualfeaturestoasetoflearnabletokens,whichcandirectly
promptthelanguagemodelforparameter-efficientfinetuning.
● EKAID.EKAIDintegratestheexpertknowledgegraphsintorepresentationlearning.39Thisis
animage-differencemodelthatissensitivetoanatomicalstructures,allowingittoextract
image-differencefeaturesthatarepertinenttotheprogressionofdiseasesand
interventions.EKAIDpresentsstate-of-the-artresultsinthetaskoflongitudinalstudy
comparisons.
● MiniGPT-v2.Thisisanewmultimodalfoundationmodelthatcancaptionboundingboxeson
naturalimages.47Specifically,itacceptsboxcoordinatesasinputsandoutputsacaptionthat
describestheobjectswithinthebox.WethereforefinetunedMiniGPT-v2onthe
region-of-interestcaptioningtask.
● CRCKD.Thisapproachaimstobringsimilarimagepairsfromthesameskinclasscloser
togetherinbothteacherandstudentmodelswhilepushingapartdissimilarimagepairsfrom
differentskinclasses.34Itisawidelyadoptedbaselineandshowscompetitiveperformance
forcategorizingskinlesions.NotethatbothCRCKDandMedVersaweretraineddirectlyon
theraw,imbalancedHAM10000dataset.
● YOLOv5.YOLOv5isastate-of-the-art,real-timeobjectdetectionalgorithmthatispartofthe
YOLO(YouOnlyLookOnce)family.22Followingitspredecessorsinprovidingfastand
accurateobjectdetectioncapabilities,YOLOv5hasbeenwidelyusedfordetecting
abnormalitiesinmedicalimages.74–76
● nnUNet.nnUNetisaself-configuringmethodfordeeplearning-basedbiomedicalimage
segmentation.30Thisframeworkisversatileinhandlingvariousmedicalimagingdatasets,
employingdifferentconfigurationsandpreprocessingstepsdependingonthedataset
characteristics.Itadaptsthenetworktopologies,suchas2DUNetand3DUNet,accordingto
thespecificrequirementsofmedicalsegmentationtasks.WeadoptednnUNET's
automaticallyconfigurednetworksonthedatasetsasbaselinemodelsfor2Dand3D
segmentation.
● nnSAM.ThennSAMarchitectureintegratestherobustandeffectivefeatureextraction
abilitiesofSegmentAnythingModel77withtheadaptiveconfigurationstrengthsofnnUNet.31
Thiscombinationmaximizesthepotentialofeachmodel,withSAMprovidinghigh-quality
featureextractionandnnUNetenablingthesystemtoautomaticallyadjusttotheunique
demandsofeachdataset.nnSAMshowsstate-of-the-artresultsinthedata-efficient
segmentationtask.
Confidenceintervals
Fortheestimationof95%confidenceintervals,non-parametricbootstrapsamplingisutilized.This
processincludescreating1,000bootstrapsamplesfromtheunseenvalidationsetthroughrandom
samplingwithreplacement,witheachsamplehavingthesamesizeasthevalidationset.Wethen
computetheevaluationmetricscoresforeachofthesesamples.Upongathering1,000scoresfor
themetrics,weorganizethesescoressequentially.Theperformancemetricsatthe2.5thand97.5th
percentilesareidentifiedandpresentedastheperformanceindicators.
21Data availability
Alltrainingandvalidationdataispubliclyavailable,linksofwhichareprovidedinTable4.
Author contributions
P.R.andH.-Y.Z.conceivedthestudy.H.-Y.Z.plannedandexecutedtheexperimentsand
dataanalysis.S.A.interpretedmedicalreportgenerationresults.H.-Y.Z.,S.A.,J.N.A.,andP.R.drafted
themanuscript.Allauthorsprovidedcriticalfeedbackandsubstantiallycontributedtotherevisionof
themanuscript.Allauthorsreadandapprovedthemanuscript.
Competing interests
P.R.isco-founderofa2zRadiologyAI.Theotherauthorsdeclarenocompetinginterests.
22Tables
Table1|Experimentalresultsof4vision-languagetasks:radiologyreportgeneration,longitudinal
studycomparisons,open-endedvisualquestionanswering,andregion-of-interestcaptioning.
Specifically,theevaluationofradiologyreportswasconductedonthreedifferentsections:findings,
impression,andtarget(concatenation).ResultsofMAIRA-1andMed-PaLMMarecitedfromtheir
papersastheirmodelshavenotbeenreleased.Numbersinbracketsarethe95%confidence
intervals.↓indicatesthatthelowerresultsarebetter. VQAstandsforvisualquestionanswering.
Tasks Models Eval.section BLEU-4 BertScore CheXbert RadGraph RadCliQ(↓)
Radiology ClsGen Findings 11.9 40.5 42.6 23.5 3.28
report [11.4,12.3] [39.8,41.1] [41.9,43.4] [22.8,24.2] [3.24,3.33]
generation
MAIRA-1 Findings 14.2 - 44.0 24.3 3.10
[13.7,14.7] [43.1,44.9] [23.7,24.8] [3.07,3.14]
Med-PaLMM Findings 11.5 - - 26.7 -
(85B) [-,-] [-,-]
MedVersa Findings 17.8 49.7 46.4 28.0 2.71
[17.2,18.4] [49.0,50.4] [45.5,47.4] [27.3,28.7] [2.66.2.75]
ClsGen Impression 8.5 38.0 48.7 18.8 3.25
[7.6,9.3] [37.3,38.6] [48.0,49.5] [18.0,19.7] [3.18,3.33]
MedVersa Impression 13.7 48.9 52.4 25.7 2.66
[12.7,14.7] [48.0,49.8] [51.3,53.5] [24.6,26.9] [2.60,2.71]
ClsGen Target 13.7 42.4 44.3 25.2 3.20
[13.0,14.3] [41.6,43.1] [43.2,45.4] [24.4,26.0] [3.14,3.25]
MedVersa Target 16.0 47.4 46.6 30.0 2.74
[15.3,16.7] [46.6,48.2] [45.3,47.8] [29.1,30.8] [2.69,2.79]
Longitudinal EKAID All 40.4 69.1 49.1 20.4 2.19
study [39.9,41.0] [68.7,69.5] [48.7,49.4] [19.9,20.9] [2.14,2.23]
comparisons
MedVersa All 44.7 71.4 50.0 23.7 2.05
[43.7,45.6] [70.6,72.2] [49.5,50.6] [22.6,24.9] [2.01,2.10]
Open-ended PTLM All 25.2 64.7 78.3 30.4 1.64
VQA [24.4,26.0] [64.1,65.5] [77.3,79.2] [29.7,31.0] [1.57,1.71]
MedVersa All 31.2 76.5 85.1 33.4 1.09
[30.7,31.8] [75.9,77.1] [84.6,85.6] [32.7,34.2] [1.06,1.12]
Region-of- MiniGPT-v2 All 5.1 36.6 55.3 18.3 3.08
interest [4.6,5.5] [36.3,37.0] [54.9,55.8] [17.9,18.6] [3.05,3.13]
captioning
MedVersa All 8.4 43.8 60.7 22.8 2.70
[8.2,8.7] [43.6,44.1] [60.4,61.1] [22.5,23.1] [2.68,2.71]
23Table2|Externalvalidationresults.Weevaluatedsevencapabilities(i.e.,reportgeneration,
classification,detection,segmentation,open-endedvisualquestionanswering,and
region-of-interestcaptioning)onsixunseenexternalcohorts(i.e.,IUX-ray,CheXpert,NIHChestX-ray,
andMS-CXR).Foreachcapability,wecomparedMedVersaagainstastate-of-the-artspecialistmodel.
Forclassification,detection,andsegmentationtasks,weusedthemeanF1score,meanIoU
(IntersectionoverUnion),andmeanDICEscoreastheevaluationmetrics,respectively.Forother
tasks,wereportedtheresultsofRadCliQ.Numbersinbracketsarethe95%confidenceintervals.↓
indicatesthatthelowerresultsarebetter.
Capabilities Datasets Models Metrics Results 95%CIs
Report IUX-ray ClsGen RadCliQ(↓) 3.07 [3.00,3.12]
generation
MedVersa 2.57 [2.54,2.60]
Classification CheXpert DAM MeanF1score 0.653 [0.633,0.669]
MedVersa 0.734 [0.712,0.756]
Detection NIH YOLOv5 MeanIoU 0.223 [0.210,0.235]
ChestX-ray
MedVersa 0.239 [0.225,0.254]
Segmentation CheXmask nnSAM MeanDICE 0.923 [0.917,0.928]
score
MedVersa 0.955 [0.952,0.957]
Open-ended IUX-ray PTLM RadCliQ(↓) 1.75 [1.69,1.82]
VQA
MedVersa 1.12 [1.07,1.17]
Region-of- MS-CXR MiniGPT-v2 RadCliQ(↓) 3.43 [3.38,3.48]
interest
captioning MedVersa 3.29 [3.23,3.35]
24Table3|Segmentationresultsofskinlesionsandabdominalorgans.Forabdominalorgan
segmentation,thered,blue,green,andaquacolorsrepresentthepancreas,liver,kidney,andspleen,
respectively.
Skinlesionsegmentation Abdominalorgansegmentation
Groundtruth MedVersa Groundtruth MedVersa
25Table4|OverviewofMedInterp.AlldatasetsincludedinMedInterpcanbeaccessedand
downloadedviatheprovidedURLs.Wereportedthedatasetsizeafterpreprocessing.Foreach
dataset,wealsodenotedtheassociatedtask(s)andthestage(s)involved.VQAdenotesvisual
questionanswering.1,2,3inthestagescolumnstandforthetraining,internalvalidation,andexternal
validationstages,respectively.
Datasets Size Tasks Stages URL
MIMIC-CXR 216,420studies Radiologyreportgeneration 1,2 https://physionet.org/content
/mimic-cxr/2.0.0/
ChestImaGenome 235,721images Chestpathologyclassification 1,2 https://physionet.org/content
/chest-imagenome/1.0.0/
8,425,163boxes Anatomicalstructuredetection 1,2
2,922,665boxes Chestpathologydetection 1,2
2,104,211captions Region-of-interestcaptioning 1,2
Medical-Diff-VQA 383,683QApairs Open-endedVQA 1,2 https://github.com/Holipori/
MIMIC-Diff-VQA
147,269 Longitudinalstudycomparisons 1,2
comparisons
2,883QApairs Open-endedVQA 3
HAM10000 11,526images Skinlesionclassification 1,2 https://dataverse.harvard.edu
/dataset.xhtml?persistentId=
doi:10.7910/DVN/DBW86T
10,015masks Skinlesionsegmentation 1,2
AbdomenCT-1K 3,964masks Abdominalorgansegmentation 1,2 https://github.com/JunMa11/
AbdomenCT-1K
CheXmask 719,793masks Chestmajororgansegmentation 1,2 https://physionet.org/content
/chexmask-cxr-segmentation
-data/0.3/
600masks 3
CheXpert 668images Chestpathologyclassification 3 https://stanfordmlgroup.gith
ub.io/competitions/chexpert/
IUX-ray 3,323studies Radiologyreportgeneration 3 https://openi.nlm.nih.gov/
NIHChestX-ray 577boxes Chestpathologydetection 3 https://nihcc.app.box.com/v/
ChestXray-NIHCC
MS-CXR 1,448captions Region-of-interestcaptioning 3 https://physionet.org/content
/ms-cxr/0.1/
26Table5|InstructionsusedbyMedVersaindifferenttasks._*_istheplaceholderforthemainimage
identifier(e.g.,<img0>).-*-denotestheplaceholderfortheabnormalities,boundingboxcoordinates,
referenceimageidentifier,detection,andsegmentationtargetsinbinaryclassification,
region-of-interestcaptioning,longitudinalstudycomparisons,detection,andsegmentationtasks,
respectively.Foreachtask,weaskedChatGPTtogenerateatmost20promptsbasedona
predefinedtemplate(i.e.,thefirstinstructionforeachtask.Then,wemanuallyfilteredoutsimilar
promptsandkeptthosediverseones.
Tasks Instructions
Reportgeneration(findings 1.Canyoudetailthefindingsobservedin_*_?
section) 2.Kindlyenumeratethefindingsfrom_*_.
3.I'dlikeabreakdownofthefindingsfrom_*_.
4.I'dlikeasectiononthefindingsderivedfrom_*_.
5.Pleasewriteafindingsectionfor_*_.
6.Wouldyoupleasewriteafindingsectionfor_*_?
7.Pleasewriteasectionoffindingsfor_*_.
8.Wouldyoupleasewriteasectionoffindingsfor_*_?
9.Howwouldyoucharacterizethefindingsfrom_*_?
10.Pleaselistthediscerniblefindingsfrom_*_?
11.Canyoucompilealistofallthenotablefindingspresentin_*_?
12.Pleasedocumentanyfindingsyouseein_*_.
Reportgeneration(impression 1.Canyoupleaseprovideyouroverallimpressionof_*_?
section) 2.What'syourmainimpressionfrom_*_?
3.Pleasedraftaconciseimpressionon_*_.
4.Wouldyougiveacomprehensiveimpressionbasedon_*_?
5.I'mlookingforanimpressionfor_*_.
6.Provideyourdiagnosticimpressionbasedonthe_*_.
7.Draftanimpressionfor_*_.
8.Wouldyoupleasewriteanimpressionsectionfor_*_?
9.Summarizetheimpressionfor_*_.
Reportgeneration 1.Canyouprovidearadiologyreportfor_*_?
(completereport) 2.Pleasereport_*_.
3.Canyouprovideareportof_*_withfindingsandimpression?
4.Report_*_withfindingsandimpression.
5.Pleasewritearadiologyreportfor_*_.
6.Pleasegeneratearadiologyreportfor_*_.
7.Pleaseprovideadetailedreportfor_*_.
8.Canyouprovideacomprehensivereportof_*_?
9.Pleasewritearadiologyreportfor_*_.
10.Canyougiveathoroughreportof_*_?
11.Couldyoupleasereport_*_?
12.Canyouprovideacomprehensivereportfor_*_?
Chestpathologyclassification,skin 1.Whatisthediagnosisfor_*_?
lesionclassification 2.Basedon_*_,whattypeoflungdiseaseissuspected?
3.Canyouidentifyanyabnormalityin_*_?
4.Whatpathologyisindicatedby_*_?
5.Whatlungdiseaseislikelypresentin_*_?
6.Whatareyourconclusionsfrom_*_?
7.Whatisyourinterpretationresultof_*_?
8.Whatabnormalitiesarepresentin_*_?
9.Whatisthedifferentialdiagnosisforthefindingsin_*_?
Region-of-interest 1.Describeregion-*-in_*_.
captioning 2.Detailanyabnormalitiesin-*-of_*_.
3.Canyoucharacterizethefeatureswithin-*-on_*_?
4.Pleaseprovideananalysisoftheanomaliesseenin-*-within_*_.
5.Describeanypathologicalfindingswithin-*-of_*_.
6.Highlightandexplainanyabnormalitiesyoudetectin-*-of_*_.
7.Identifyanddescribeanyabnormalityin-*-of_*_.
8.Couldyoupleasedescribetheregion-*-in_*_?
9.Wouldyoupleasedescribetheregion-*-in_*_?
10.Giveadescriptionoftheregion-*-in_*_.
Longitudinalstudycomparisons 1.Highlightanydifferencein_*_comparedtothepriorstudy-*-.
2.Identifyanyprogressionin_*_sincethelaststudy-*-.
3.Comparethecurrentstudy_*_withthepastone-*-andidentifyanydifferencebetweenthem.
274.Presentanychangesin_*_sincethelaststudy-*-.
5.Detailanyprogressionorregressionin_*_incomparisontotheolderstudy-*-.
6.Detectchangesin_*_comparedtothepaststudy-*-.
7.Compare_*_withthepriorstudy-*-andtellmeanydifference.
Anatomicalstructuredetection, 1.Detectanysignsof-*-in_*_.
chestpathologydetection 2.Highlighttheareasthatindicate-*-in_*_.
3.Showmetheregionsin_*_where-*-mightbepresent.
4.Assess_*_andmarkareasconsistentwith-*-findings.
5.Locateandcircleanyfeaturesof-*-in_*_.
6.Compare_*_totypical-*-patternsandhighlightanymatches.
7.Detectanddisplaypotentialsymptomsof-*-within_*_.
8.Isthereanytraceof-*-in_*_?Pointitout.
9.Helpmespot-*-byilluminatingitsmarkersin_*_.
10.Searchforanycharacteristicsignsof-*-in_*_.
11.Examineandunderscorethepresenceof-*-in_*_.
12.Wouldyoupleasehelpmelocate-*-in_*_?
13.Couldyoupleasehelpmelocate-*-in_*_?
14.Pleasehelpmelocate-*-in_*_?
Chestmajororgansegmentation, 1.Segment-*-in_*_.
skinlesionsegmentation, 2.Highlighttheboundariesof-*-in_*_.
abdominalorgansegmentation 3.Isolateandshowonly-*-from_*_.
4.Canyoudelineate-*-in_*_?
5.Segment-*-fromthegiven_*_.
6.Ineedaclearsegmentationof-*-in_*_,please.
7.Outlinethecontoursof-*-in_*_.
8.Showaclearboundaryaround-*-in_*_.
9.Separate-*-fromthesurroundinganatomyin_*_.
10.Provideasegmentedviewof-*-in_*_.
11.Pleaseidentifyandsegment-*-fromtherestin_*_.
12.Givemeaclearcutoutof-*-in_*_.
13.Pleasemaskeverythingexceptfor-*-in_*_.
14.Drawaboundaryaround-*-in_*_.
15.Wouldyoupleasehelpmesegment-*-in_*_?
16.Couldyoupleasehelpmesegment-*-in_*_?
17.Pleasehelpmesegment-*-in_*_?
28References
1. Rajpurkar,P.etal.CheXNet:Radiologist-LevelPneumoniaDetectiononChestX-RayswithDeep
Learning.arXiv[cs.CV](2017).
2. Wang,X.etal.ChestX-Ray8:Hospital-scalechestX-raydatabaseandbenchmarkson
weakly-supervisedclassificationandlocalizationofcommonthoraxdiseases.in2017IEEE
ConferenceonComputerVisionandPatternRecognition(CVPR)2097–2106(IEEE,2017).
3. Irvin,J.etal.CheXpert:ALargeChestRadiographDatasetwithUncertaintyLabelsandExpert
Comparison.AAAI33,590–597(2019).
4. Johnson,A.E.W.etal.MIMIC-CXR,ade-identifiedpubliclyavailabledatabaseofchest
radiographswithfree-textreports.SciData6,317(2019).
5. Tiu,E.etal.Expert-leveldetectionofpathologiesfromunannotatedchestX-rayimagesvia
self-supervisedlearning.NatBiomedEng6,1399–1406(2022).
6. Liu,Y.etal.Adeeplearningsystemfordifferentialdiagnosisofskindiseases.Nat.Med.26,
900–908(2020).
7. Esteva,A.etal.Dermatologist-levelclassificationofskincancerwithdeepneuralnetworks.
Nature542,115–118(2017).
8. Daneshjou,R.etal.DisparitiesindermatologyAIperformanceonadiverse,curatedclinicalimage
set.SciAdv8,eabq6147(2022).
9. Joshi,G.etal.FDAapprovedArtificialIntelligenceandMachineLearning(AI/ML)-Enabled
MedicalDevices:Anupdatedlandscape.bioRxiv(2022)doi:10.1101/2022.12.07.22283216.
10. Moor,M.etal.Foundationmodelsforgeneralistmedicalartificialintelligence.Nature616,
259–265(2023).
11. Rajpurkar,P.&Lungren,M.P.TheCurrentandFutureStateofAIInterpretationofMedical
Images.N.Engl.J.Med.388,1981–1990(2023).
12. Bommasani,R.etal.OntheOpportunitiesandRisksofFoundationModels.arXiv[cs.LG](2021).
13. Tu,T.etal.TowardsGeneralistBiomedicalAI.arXiv[cs.CL](2023).
2914. Wu,C.,Zhang,X.,Zhang,Y.,Wang,Y.&Xie,W.Towardsgeneralistfoundationmodelforradiology
byleveragingweb-scale2D&3Dmedicaldata.arXiv[cs.CV](2023).
15. Moor,M.etal.Med-Flamingo:aMultimodalMedicalFew-shotLearner.arXiv[cs.CV](2023).
16. Lu,M.Y.etal.AFoundationalMultimodalVisionLanguageAIAssistantforHumanPathology.
arXiv[cs.CV](2023).
17. Huang,Z.,Bianchi,F.,Yuksekgonul,M.,Montine,T.J.&Zou,J.Avisual–languagefoundationmodel
forpathologyimageanalysisusingmedicalTwitter.Nat.Med.29,2307–2316(2023).
18. Chen,T.etal.Aunifiedsequenceinterfaceforvisiontasks.arXiv[cs.CV]31333–31346(2022).
19. Wang,W.etal.VisionLLM:LargeLanguageModelisalsoanOpen-EndedDecoderfor
Vision-CentricTasks.arXiv[cs.CV](2023).
20. Zhang,H.etal.GLIPv2:UnifyinglocalizationandVision-languageunderstanding.arXiv[cs.CV]
36067–36080(2022).
21. Hyland,S.L.etal.MAIRA-1:Aspecialisedlargemultimodalmodelforradiologyreportgeneration.
arXiv[cs.CL](2023).
22. Jocher,G.,Nishimura,K.,Mineeva,T.&Vilariño,R.yolov5.Coderepository(2020).
23. Papineni,K.,Roukos,S.,Ward,T.&Zhu,W.-J.Bleu:aMethodforAutomaticEvaluationofMachine
Translation.inProceedingsofthe40thAnnualMeetingoftheAssociationforComputational
Linguistics(eds.Isabelle,P.,Charniak,E.&Lin,D.)311–318(AssociationforComputational
Linguistics,Philadelphia,Pennsylvania,USA,2002).
24. Zhang,T.,Kishore,V.,Wu,F.,Weinberger,K.Q.&Artzi,Y.BERTScore:EvaluatingTextGeneration
withBERT.arXiv[cs.CL](2019).
25. Smit,A.etal.CheXbert:CombiningAutomaticLabelersandExpertAnnotationsforAccurate
RadiologyReportLabelingUsingBERT.arXiv[cs.CL](2020).
26. Jain,S.etal.RadGraph:ExtractingClinicalEntitiesandRelationsfromRadiologyReports.arXiv
[cs.CL](2021).
27. Yu,F.etal.EvaluatingprogressinautomaticchestX-rayradiologyreportgeneration.Patterns(N
Y)4,100802(2023).
3028. Nguyen,H.T.N.etal.AutomatedGenerationofAccurate\&FluentMedicalX-rayReports.arXiv
[cs.CL](2021).
29. Demner-Fushman,D.etal.Preparingacollectionofradiologyexaminationsfordistributionand
retrieval.J.Am.Med.Inform.Assoc.23,304–310(2016).
30. Isensee,F.,Jaeger,P.F.,Kohl,S.A.A.,Petersen,J.&Maier-Hein,K.H.nnU-Net:aself-configuring
methodfordeeplearning-basedbiomedicalimagesegmentation.Nat.Methods18,203–211
(2021).
31. Li,Y.,Jing,B.,Li,Z.,Wang,J.&Zhang,Y.nnSAM:Plug-and-playSegmentAnythingModel
ImprovesnnUNetPerformance.arXiv[cs.CV](2023).
32. vanSonsbeek,T.,Derakhshani,M.M.,Najdenkoska,I.,Snoek,C.G.M.&Worring,M.Open-Ended
MedicalVisualQuestionAnsweringThroughPrefixTuningofLanguageModels.arXiv[cs.CV]
(2023).
33. Yuan,Z.,Yan,Y.,Sonka,M.&Yang,T.Large-scalerobustdeepAUCmaximization:Anewsurrogate
lossandempiricalstudiesonmedicalimageclassification.in2021IEEE/CVFInternational
ConferenceonComputerVision(ICCV)3040–3049(IEEE,2021).
34. Xing,X.etal.CategoricalRelation-PreservingContrastiveKnowledgeDistillationforMedical
ImageClassification.inMedicalImageComputingandComputerAssistedIntervention–MICCAI
2021163–173(SpringerInternationalPublishing,2021).
35. Johnson,A.etal.MIMIC-CXR-JPG-chestradiographswithstructuredlabels.physionet.org
https://doi.org/10.13026/8360-t248(2019).
36. Endo,M.,Krishnan,R.,Krishna,V.,Ng,A.Y.&Rajpurkar,P.Retrieval-BasedChestX-RayReport
GenerationUsingaPre-trainedContrastiveLanguage-ImageModel.inProceedingsofMachine
LearningforHealth(eds.Roy,S.etal.)vol.158209–219(PMLR,2021).
37. Wu,J.T.etal.ChestImaGenomeDatasetforClinicalReasoning.arXiv[cs.CV](2021).
38. Wu,J.T.etal.ChestImaGenomedatasetforclinicalreasoning.arXiv[cs.CV](2021).
39. Hu,X.etal.ExpertKnowledge-AwareImageDifferenceGraphRepresentationLearningfor
Difference-AwareMedicalVisualQuestionAnswering.inProceedingsofthe29thACMSIGKDD
31ConferenceonKnowledgeDiscoveryandDataMining4156–4165(AssociationforComputing
Machinery,NewYork,NY,USA,2023).
40. Tschandl,P.,Rosendahl,C.&Kittler,H.TheHAM10000dataset,alargecollectionofmulti-source
dermatoscopicimagesofcommonpigmentedskinlesions.SciData5,180161(2018).
41. Codella,N.etal.SkinLesionAnalysisTowardMelanomaDetection2018:AChallengeHostedby
theInternationalSkinImagingCollaboration(ISIC).arXiv[cs.CV](2019).
42. Alam,T.M.etal.AnEfficientDeepLearning-BasedSkinCancerClassifierforanImbalanced
Dataset.Diagnostics(Basel)12,(2022).
43. Ma,J.etal.AbdomenCT-1K:IsAbdominalOrganSegmentationaSolvedProblem?IEEETrans.
PatternAnal.Mach.Intell.44,6695–6714(2022).
44. Boecking,B.etal.MakingtheMostofTextSemanticstoImproveBiomedicalVision–Language
Processing.inComputerVision–ECCV20221–21(SpringerNatureSwitzerland,2022).
45. Boecking,B.etal.MS-CXR:Makingthemostoftextsemanticstoimprovebiomedical
vision-languageprocessing.PhysioNethttps://doi.org/10.13026/B90J-VB87(2022).
46. Zhu,D.,Chen,J.,Shen,X.,Li,X.&Elhoseiny,M.MiniGPT-4:EnhancingVision-Language
UnderstandingwithAdvancedLargeLanguageModels.arXiv[cs.CV](2023).
47. Chen,J.etal.MiniGPT-v2:largelanguagemodelasaunifiedinterfaceforvision-language
multi-tasklearning.arXiv[cs.CV](2023).
48. Li,C.etal.LLaVA-Med:TrainingaLargeLanguage-and-VisionAssistantforBiomedicineinOne
Day.arXiv[cs.CV](2023).
49. Vaswani,A.etal.Attentionisallyouneed.Adv.NeuralInf.Process.Syst.30,(2017).
50. Çiçek,Ö.,Abdulkadir,A.,Lienkamp,S.S.,Brox,T.&Ronneberger,O.3DU-Net:LearningDense
VolumetricSegmentationfromSparseAnnotation.inMedicalImageComputingand
Computer-AssistedIntervention–MICCAI2016424–432(SpringerInternationalPublishing,
2016).
51. He,K.,Zhang,X.,Ren,S.&Sun,J.SpatialPyramidPoolinginDeepConvolutionalNetworksfor
VisualRecognition.IEEETrans.PatternAnal.Mach.Intell.37,1904–1916(2015).
3252. Ba,J.L.,Kiros,J.R.&Hinton,G.E.LayerNormalization.arXiv[stat.ML](2016).
53. Touvron,H.etal.Llama2:OpenFoundationandFine-TunedChatModels.arXiv[cs.CL](2023).
54. Kudo,T.&Richardson,J.SentencePiece:Asimpleandlanguageindependentsubwordtokenizer
anddetokenizerforNeuralTextProcessing.arXiv[cs.CL](2018).
55. Isensee,F.etal.nnU-Net:Self-adaptingFrameworkforU-Net-BasedMedicalImage
Segmentation.arXiv[cs.CV](2018).
56. He,K.,Zhang,X.,Ren,S.&Sun,J.Deepresiduallearningforimagerecognition.arXiv[cs.CV]
770–778(2015).
57. Deng,J.etal.ImageNet:Alarge-scalehierarchicalimagedatabase.in2009IEEEConferenceon
ComputerVisionandPatternRecognition248–255(IEEE,2009).
58. Jaegle,A.etal.Perceiver:GeneralPerceptionwithIterativeAttention.inProceedingsofthe38th
InternationalConferenceonMachineLearning(eds.Meila,M.&Zhang,T.)vol.1394651–4664
(PMLR,18--24Jul2021).
59. Taori,R.etal.Stanfordalpaca:Aninstruction-followingllamamodel.Preprintat(2023).
60. Wei,J.etal.FinetunedLanguageModelsAreZero-ShotLearners.arXiv[cs.CL](2021).
61. Chung,H.W.etal.ScalingInstruction-FinetunedLanguageModels.arXiv[cs.LG](2022).
62. Singhal,K.etal.Largelanguagemodelsencodeclinicalknowledge.Nature620,172–180(2023).
63. Hinton,G.,Srivastava,S.&Swersky,K.NeuralNetworksforMachineLearningLecture6a
Overviewofmini--batchgradientdescent.
http://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf(2012).
64. Lin,T.-Y.,Goyal,P.,Girshick,R.,He,K.&Dollar,P.FocalLossforDenseObjectDetection.IEEE
Trans.PatternAnal.Mach.Intell.42,318–327(2020).
65. Sudre,C.H.,Li,W.,Vercauteren,T.,Ourselin,S.&JorgeCardoso,M.GeneralisedDiceOverlapasa
DeepLearningLossFunctionforHighlyUnbalancedSegmentations.DeepLearnMedImageAnal
MultimodalLearnClinDecisSupport(2017)2017,240–248(2017).
66. Liu,Z.etal.SwinTransformer:HierarchicalVisionTransformerusingShiftedWindows.in2021
IEEE/CVFInternationalConferenceonComputerVision(ICCV)9992–10002(IEEE,2021).
3367. Hu,E.J.etal.LoRA:Low-RankAdaptationofLargeLanguageModels.arXiv[cs.CL](2021).
68. Loshchilov,I.&Hutter,F.DecoupledWeightDecayRegularization.arXiv[cs.LG](2017).
69. Chen,Z.,Song,Y.,Chang,T.-H.&Wan,X.GeneratingRadiologyReportsviaMemory-driven
Transformer.arXiv[cs.CL](2020).
70. Miura,Y.,Zhang,Y.,Tsai,E.B.,Langlotz,C.P.&Jurafsky,D.ImprovingFactualCompletenessand
ConsistencyofImage-to-TextRadiologyReportGeneration.arXiv[cs.CL](2020).
71. Liu,H.,Li,C.,Wu,Q.&Lee,Y.J.VisualInstructionTuning.arXiv[cs.CV](2023).
72. Liu,H.,Li,C.,Li,Y.&Lee,Y.J.ImprovedBaselineswithVisualInstructionTuning.arXiv[cs.CV]
(2023).
73. Driess,D.etal.PaLM-E:AnEmbodiedMultimodalLanguageModel.arXiv[cs.LG](2023).
74. Mohiyuddin,A.etal.BreastTumorDetectionandClassificationinMammogramImagesUsing
ModifiedYOLOv5Network.Comput.Math.MethodsMed.2022,1359019(2022).
75. Wan,J.,Chen,B.&Yu,Y.PolypDetectionfromColorectumImagesbyUsingAttentiveYOLOv5.
Diagnostics(Basel)11,(2021).
76. Luo,Y.,Zhang,Y.,Sun,X.,Dai,H.&Chen,X.IntelligentSolutionsinChestAbnormalityDetection
BasedonYOLOv5andResNet50.J.Healthc.Eng.2021,2267635(2021).
77. Kirillov,A.etal.SegmentAnything.arXiv[cs.CV](2023).
34