EconLogicQA: A Question-Answering Benchmark for Evaluating
Large Language Models in Economic Sequential Reasoning
YinzhuQuan∗ ZefangLiu∗
GeorgiaInstituteofTechnology GeorgiaInstituteofTechnology
Atlanta,GA30332,USA Atlanta,GA30332,USA
yquan9@gatech.edu liuzefang@gatech.edu
Abstract newbenchmarkdesignedtorigorouslyassessthe
logicalreasoningcapabilitiesofLLMsspecifically
Inthispaper,weintroduceEconLogicQA,arig-
within the contexts of economics, business, and
orousbenchmarkdesignedtoassessthesequen-
supplychainmanagement.
tial reasoning capabilities of large language
models(LLMs)withintheintricaterealmsof EconLogicQAdistinguishesitselffromexisting
economics, business, and supply chain man- benchmarksbychallengingLLMstonotonlyiden-
agement. Diverging from traditional bench- tifybutalsologicallysequencemultipleintercon-
marksthatpredictsubsequenteventsindividu- nected events from realistic economic scenarios.
ally,EconLogicQAposesamorechallenging
Thisapproachaimstoreflecttheintricatedecision-
task:itrequiresmodelstodiscernandsequence
making processes required in these fields, going
multipleinterconnectedevents,capturingthe
beyondmerefactrecallorsimpleeventprediction.
complexityofeconomiclogics. EconLogicQA
By focusing on the sequencing of events based comprisesanarrayofmulti-eventscenariosde-
rivedfromeconomicarticles,whichnecessitate onlogicalratherthansimplychronologicalorder,
an insightful understanding of both temporal EconLogicQA inspects the LLMs’ ability to en-
andlogicaleventrelationships. Throughcom- gagewithandunderstandtheunderlyingmechan-
prehensiveevaluations,weexhibitthatEcon- icsofeconomicphenomena.
LogicQA effectively gauges a LLM’s profi-
The benchmark utilizes a curated dataset de-
ciency in navigating the sequential complex-
rivedfromawiderangeofbusinessnewsarticles,
ities inherent in economic contexts. We pro-
guidingGPT-4togeneratemulti-choicequestions
vide a detailed description of EconLogicQA
datasetandshowstheoutcomesfromevaluat- that demand an intelligent understanding of logi-
ingthebenchmarkacrossvariousleading-edge calconnections. Arigoroushumanreviewprocess
LLMs, thereby offering a thorough perspec- ensures the accuracy and appropriateness of the
tiveontheirsequentialreasoningpotentialin content, refiningthedatasettoenhanceitspracti-
economic contexts. Our benchmark dataset
calvalue. Throughcomprehensivetestingacross
is available at https://huggingface.co/
variousstate-of-the-artLLMs,thispapernotonly
datasets/yinzhu-quan/econ_logic_qa.
demonstratesEconLogicQA’seffectivenessineval-
1 Introduction uatinglogicalreasoningbutalsoprovidesinsights
into the potential improvements and applications
Logicalreasoningisapivotalskillinmanyprofes-
ofLLMsincomplexreasoningtasks.
sionalandacademicdomains,enablingindividuals
Ourcontributionsofthispaperareasfollows:
to make informed decisions by understanding re-
1. We propose a novel benchmark, EconLog-
lationshipsbetweensequentialeventsorpiecesof
icQA,whichrigorouslyassessesLLMs’log-
information. Inpractice,thereasoningcapabilities
icalreasoningcapabilitieswithineconomics,
oflargelanguagemodels(LLMs)arefrequentlyuti-
business,andsupplychainmanagement.
lizedacrossvariousapplications,yettheireffective-
2. We utilize GPT-4 to generate questions and
ness in logical reasoning remains underexplored.
answersfrombusinessarticles,ensuringhigh-
Despiteitsimportance,thereisaevidentgapinthe
quality, well-crafted multiple-choice ques-
literatureregardingthecapabilityofLLMstoper-
tionsthroughmeticuloushumanreview.
formlogicalreasoningatahighlevel. Thispaper
3. We conduct a comprehensive evaluation of
addressesthisgapbyintroducingEconLogicQA,a
both open and proprietary LLMs to assess
*Theseauthorscontributedequallytothiswork. theirperformanceonthisbenchmark.
1
4202
yaM
31
]LC.sc[
1v83970.5042:viXra2 RelatedWork theBBT-CFLEBbenchmark,supportingadvanced
understanding and generation tasks in the finan-
SequentialReasoningBenchmarks. Intherealm cialdomainandfosteringsignificantresearchand
ofassessingcomplexreasoningabilities,Jinetal. developmentinthisspecializedarea. Zhangetal.
(2023)introducetheCLADDERdataset,exploring (2023) present the FinEval, a specialized bench-
capacities of large language models (LLMs) for markforassessingfinancialknowledgeinLLMs,
causalreasoning,differentiatingitselfbyfocusing demonstrating significant potential through GPT-
onformalrules-basedcausalinferenceinsteadof 4’shighperformanceacrossdiverseprompttypes.
the typical evaluation of commonsense causality VanPatten(2023)introducetheEconQA,anovel
(Talmoretal.,2018;Huangetal.,2019)inNatural datasetforassessingLLMsonmultiple-choiceeco-
Language Processing (NLP). Wang et al. (2023) nomics questions, reveals that Chain-of-Thought
presentSTEPS,arigorousbenchmarkdesignedto reasoning improves performance, particularly in
assessmodels’understandingofactionsequence mathematicalqueries,whilepromptvariationshave
orderinsequentialtaskssuchascookingandman- amoderateeffectonaccuracy.
ufacturing,whichhighlightschallengesofcurrent
LLMsinperformingorderreasoningwithoutspe- 3 EconLogicQA
cifictuning. Inexaminingadjacentdomains,Guha
Inthissection,wedetailthedatasetgenerationand
et al. (2024) launch LEGALBENCH, emerging as
humanreviewprocessesforcreatingtheEconLog-
a notable benchmark that evaluates LLMs in le-
icQAbenchmarkandprovideillustrativeexamples
gal reasoning, having been developed collabora-
fromit.
tivelywithlegalexpertstocovervariousfacetsof
practicalandtheoreticallegalanalysis. Yangetal.
3.1 DatasetGeneration
(2024)establishAQA-Bench,servingasaninter-
activebenchmarkthatevaluatesLLMs’sequential Tostreamlinethequestion-generationprocessand
reasoningabilitiesacrossvariousalgorithmictasks, reduce the subjectivity, labor-intensiveness, and
includingDepth-FirstSearch(DFS),Breadth-First randomnessofmanualcreation,weutilizetheGPT-
Search(BFS),andbinarysearch,byrequiringmod- 4toautomaticallygeneratequestionsbyextracting
els to dynamically interact with the task environ- keypointsfromnewsarticles. Wespecificallyse-
ment,andtherebyuncoveringnotableperformance lectedeconomics-relatedarticlesfromthe2011to
disparities among different LLMs. Valmeekam 2022 news dataset available on Kaggle*, which
et al. (2024) create PlanBench as an extensible is under the CC0 Public Domain license. This
benchmarkfocusedonevaluatingLLMs’planning cleaneddatasetprovidesacomprehensiverangeof
and reasoning capabilities, particularly in action economicnewsarticles,andwefurthernarrowour
andchange,wherediversescenariosareusedfrom focustothosecategorizedunderbusinesstoalign
theautomatedplanningcommunitytodiscernbe- withourresearchscopeineconomics.
tweengenuineplanningabilitiesandmereretrieval In the data generation process, instructional
frompre-trainedknowledge. prompts are developed to guide GPT-4 in creat-
ingmulti-choicequestionsthatchallengemodels
Economic Benchmarks. In the finance domain,
tologicallysequenceeventswithintheframework
Shah et al. (2022) launch the Financial Lan-
ofbusiness-relatedscenarios. Thesequestionsstart
guage Understanding Evaluation (FLUE) bench-
withabriefscenariodescriptionandinvolvefour
markalongsidetheFinancialLANGuage(FLANG)
eventsthatmustbeorderedbasedontheirlogical
model, offering a comprehensive suite of evalua-
orchronologicalsequenceratherthantheirappear-
tionsfocusedoneconomicandfinancialdomains,
ances in the source articles. The selected events
significantlyoutperformingexistingmodelsonvar-
pertaintotypicalbusinessoreconomicsituations,
iousNLPtasks. Hendrycksetal.(2020)compile
necessitatingadeepunderstandingofbusinessprac-
the Massive Multitask Language Understanding
ticesandeconomicprinciplesforaccuratesequenc-
(MMLU) benchmark of 57 diverse tasks, includ-
ing. Thepromptsspecifythatthegeneratedcontent
ing the economics subject, designed to evaluate
shouldbeoriginal,concise,andcraftedwithoutref-
themultitaskaccuracyoflanguagemodels,reveal-
erencingtheoriginalnewsarticlesandunnecessary
ingthateventhelargestmodelsstillstrugglewith
expert-levelperformanceandhaveinconsistentac-
*https://www.kaggle.com/datasets/hadasu92/
curacy across subjects. Lu et al. (2023) propose cnn-articles-after-basic-cleaning/
2detail. Eachquestionisdesignedtobecompleted EconLogicQAExample1
independently, making it suitable for evaluation.
Question: Costco,amajorretailer,istakingsteps
Theformattedoutputincludesascenariodescrip-
tocontrolitschickensupplychaintomaintainthe
tionfollowedbyfourchoiceslabeledA,B,C,and
priceofitspopularrotisseriechickens. Arrangethe
D,concludingwiththecorrectsequenceandabrief
followingeventsinthelogicalsequencebasedon
explanationtoensurethatthereasoningbehindthe
Costco’sstrategy.
sequence is clear and deducible solely from the
Choices:
informationpresentedinthequestionandchoices.
A. Costco recruits farmers to grow chickens for
Thisstructureisintendedtoenhancecomprehen-
them.
sionandapplicationofbusinessconcepts. SeeAp-
B.Costcoopensa$450millionpoultrycomplexin
pendixFigure1foranexampleofGPT-4response
Nebraska.
withtheprompt.
C. Costco faces supply challenges and cost pres-
suresinthepoultryindustry.
3.2 ReviewProcess
D.Costcodecidestokeepitsrotisseriechickensat
In order to maintain the integrity and quality of
$4.99despitecompetitorsincreasingtheirprices.
thedataset,humanverificationisincorporatedinto
CorrectOrder: D,C,A,B
the workflow. This manual review is essential as
EconLogicQAExample2
somegeneratedresponsesexhibiterrorsinthecor-
rect sequencing of events. Each question under- Question: Arrange the following events related
goesmeticulousexamination,andadjustmentsare tothe FederalReserve’s monetarypolicyintheir
madetoensureaccuracyandclarityinthelogical correctorder.
sequence provided. Furthermore, the dataset un- Choices:
dergoesarigorousreviewtoidentifyandexclude A. The Federal Reserve announces the end of its
sensitivenewsarticlesthatcouldbeinappropriate. pandemic-era stimulus and accelerates the roll-
Intotal,204questionsareremovedfromtheinitial backofitsassetpurchases.
poolof854questions. Thecriteriaforremovalin- B.TheFederalReservesignalsitwouldhikeinter-
cludescenariosthatyieldmultiplevalidsequences estratesmultipletimesthroughout2022.
andinstanceswherealogicalsequencecannotbe C.TheFederalReservereducesitsmassivebalance
clearly established. This comprehensive vetting sheet.
processsignificantlyenhancestheevaluationqual- D.TheFederalReserveraisesinterestratesdueto
ity. inflationwellabove2%andastronglabormarket.
CorrectOrder: A,B,D,C
3.3 DatasetExamples
ToprovideacleardepictionofEconLogicQA’scon- Table1: QuestionexamplesfromEconLogicQA.
tents,wepresenttwoexamplesfromthedatasetin
theTable1. Thefirstexampledetailsasequenceof
ingofmultipleopenandproprietarylargelanguage
decisionsbyCostcotomanageitschickensupply
models(LLMs)ineconomicscenarios.
chain effectively, while the second outlines steps
takenbytheFederalReservetonavigatefiscalchal- 4.1 ExperimentSetup
lenges. Theseexamplesillustratethedataset’spri-
WeimplementexperimentsonvariousLLMsusing
maryobjective: toevaluatethecapabilityoflarge
theEconLogicQAdatasettoassessitssequential
language models in sequencing economic events
reasoning capabilities within the intricate realms
logically,notjustchronologically. Eachquestion
ofeconomics,business,andsupplychainmanage-
is meticulously designed to challenge models to
ment. Weselectthecurrentmainstreamopenand
demonstrate their understanding of complex eco-
proprietary LLMs in our study, including Llama-
nomicinteractionsandtoapplylogicalreasoning
2 (Touvron et al., 2023a,b), Llama 3 (Meta AI,
withinreal-worldbusinesscontexts.
2024),Gemma(Teametal.,2024),Mistral(Jiang
etal.,2023),Yi(Youngetal.,2024),Zephyr(Tun-
4 Experiments
stalletal.,2023), GPT-3.5(Ouyangetal.,2022),
This section outlines experiments with the Econ- and GPT-4 (Achiam et al., 2023). Each model is
LogicQAdataset,assessingthesequentialreason- evaluated in both 1-shot and 5-shot settings. We
3donotinclude0-shotinourselectionbecausethe v0.2exhibitedpromisingresults,withtheaccuracy
resultsareunsatisfactoryduetothetask’scomplex- of31.54%inthe1-shotsettingand32.31%inthe
ity. Therefore, we recommend using a few-shot 5-shotsetting,underscoringitsadaptabilitytocom-
approach for sorting problems. Accuracy is the plex reasoning tasks, though it still lags behind
primarymetricused,offeringadirectmeasureof GPT-4’soverallperformance.
eachmodel’sunderstandingoftheconceptswithin
theEconLogicQAdataset. Model 1-Shot 5-Shot
All experiments are conducted on NVIDIA
Llama-2-7B 0.77% 1.54%
A100 GPUs. Each open LLM use in this paper
Llama-2-7B-Chat 9.23% 10.00%
issourcedfromtheHuggingfaceTransformersli-
Llama-2-13B 9.23% 1.54%
brary(Wolfetal.,2019). LanguageModelEvalua-
Llama-2-13B-Chat 14.62% 8.46%
tionHarness(Gaoetal.,2023)isusedtotestopen
Llama-3-8B 23.85% 23.85%
LLMs on sequential reasoning evaluation tasks.
Llama-3-8B-Instruct 34.62% 37.69%
The YAML configuration file is used to set key
Gemma-2B-IT 7.69% 7.69%
parametersinourscenario,suchasterminatingout-
Gemma-1.1-2B-IT 8.46% 6.92%
puts, extracting answers, evaluating results, and
Gemma-7B-IT 2.31% 4.62%
soon. LLMsareconfiguredtostopgeneratingre-
Gemma-1.1-7B-IT 0.77% 3.85%
sponsesonceitbeginstoposenewquestionsafter
Mistral-7B-v0.1 26.15% 30.00%
answeringthequestionintheprompt, settingthe
Mistral-7B-v0.2 26.15% 32.31%
temperature to 0.0 without any sampling. Then,
Mistral-7B-Instruct-v0.1 15.38% 20.77%
weextracttheanswerfromtheresponsegenerated
Mistral-7B-Instruct-v0.2 31.54% 32.31%
by LLMs using regular expression. Finally, we
Yi-6B 3.85% 29.23%
verifytheanswerthroughexactmatchinganduse
Yi-6B-Chat 20.77% 30.77%
accuracyastheevaluationmetric.
Zephyr-7B-Alpha 23.08% 23.08%
Zephyr-7B-Beta 17.69% 14.62%
4.2 ExperimentResults
GPT-3.5-Turbo 37.69% 38.46%
TheresultsfromourexperimentsshowninTable2
GPT-4 55.38% 53.85%
indicatediverseperformancesamongthemodels,
GPT-4-Turbo 56.92% 56.15%
withsignificantdisparitiesobservedintheirability
tohandlethesequencingofeconomicevents. GPT-
Table2: Comparisonoftheaccuracyofmultiplelarge
4-Turbo exhibits the highest accuracy, achieving language models on the EconLogicQA dataset under
56.92%inthe1-shotscenarioand56.15%inthe5- 1-shotand5-shotlearningscenarios.
shotscenario,makingitthebest-performingmodel
in our tests. GPT-4 follows closely, demonstrat- Theseexperimentscollectivelydemonstratethe
ingthesecond-highestperformancewith55.38% varyingdegreesofproficiencyinapplyingLLMs
in the 1-shot and 53.85% in the 5-shot settings. to economic sequential reasoning, reflecting the
Remarkably,the1-shotscenariogenerallyresults currentlandscapeofLLMscapabilitiesinthisdo-
inbetterperformancethanthe5-shotscenariofor main. There is still a clear gap in the ability of
thesetwomodels,whichcouldbeattributedtothe currentLLMstoaccuratelyhandlemanyeconomic
models’abilitytoleveragetheirpre-trainedknowl- scenariosandcorrectlysequenceevents,especially
edge effectively without the potential confusion open LLMs. This limitation points to significant
introducedbyadditionalcontextinthe5-shotsce- challengesthatpersistinthefield,emphasizingthe
nario. need for targeted improvements and innovations
OpenLLMswereevaluated,showingvariedper- infutureresearch. Addressingtheseshortcomings
formancesasoutlinedinTable2. Notably,Llama- could lead to more robust models that are better
3-8B-Instruct demonstrated significant improve- equippedtonavigatethecomplexityofeconomic
mentswhenfine-tunedwithinstructionscompared reasoning.
with Llama-3-8B, achieving 34.62% accuracy in
5 Conclusion
the1-shotsettingand37.69%inthe5-shotsetting,
whichhighlightsthesubstantialimpactofinstruc- This study introduces EconLogicQA, a novel
tiontuningonenhancingperformanceforquestion benchmarkspecificallydesignedtoassessthelogi-
answering tasks. Similarly, Mistral-7B-Instruct- calreasoningcapabilitiesoflargelanguagemodels
4(LLMs)inthedomainsofeconomics,business,and LifuHuang,RonanLeBras,ChandraBhagavatula,and
supply chain management. The benchmark chal- YejinChoi.2019. Cosmosqa: Machinereadingcom-
prehensionwithcontextualcommonsensereasoning.
lengesLLMswithcomplexrealisticeconomicsce-
arXivpreprintarXiv:1909.00277.
narios. UtilizingGPT-4,high-quality,well-crafted
multiple-choicequestionsaregeneratedfrombusi- Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch,ChrisBamford,DevendraSinghChaplot,Diego
ness articles and refined through meticulous hu-
delasCasas,FlorianBressand,GiannaLengyel,Guil-
manreview. Acomprehensiveevaluationofboth
laumeLample,LucileSaulnier,etal.2023. Mistral
openandproprietaryLLMsisconducted,providing 7b. arXivpreprintarXiv:2310.06825.
deepinsightsintotheircapabilitiesandlimitations
Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele,
withinthisspecializedcontext.
Ojasv Kamal, LYU Zhiheng, Kevin Blin, Fer-
Inthefuture,variousenhancementscanbemade nando Gonzalez Adauto, Max Kleiman-Weiner,
toimprovetheperformanceofLLMsineconomic Mrinmaya Sachan, et al. 2023. Cladder: Assess-
ingcausalreasoninginlanguagemodels. InThirty-
reasoning. Prompt engineering could be refined
seventhConferenceonNeuralInformationProcess-
tobetterguidemodelsthroughcomplexeconomic
ingSystems.
scenarios,thusenhancingtheiraccuracyinunder-
Dakuan Lu, Hengkui Wu, Jiaqing Liang, Yipei Xu,
standingandprocessingcomplexlogicalrelation-
Qianyu He, Yipeng Geng, Mengkun Han, Yingsi
ships. Additionally,theapplicationofparameter-
Xin, and Yanghua Xiao. 2023. Bbt-fin: Compre-
efficient fine-tuning (PEFT) using the EconLog- hensive construction of chinese financial domain
icQAtrainingdatasetoffersapromisingapproach pre-trainedlanguagemodel,corpusandbenchmark.
arXivpreprintarXiv:2302.09432.
to customize models efficiently, optimizing their
responses. Moreover,thereisasignificantoppor- MetaAI.2024. Metallama3. Accessed: 2024-05-09.
tunitytodevelopspecializedLLMsthatarespecif-
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
ically designed to address the unique challenges
CarrollWainwright,PamelaMishkin,ChongZhang,
ineconomics,business,andsupplychainmanage- SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
ment. 2022. Training languagemodelsto followinstruc-
tionswithhumanfeedback. Advancesinneuralin-
formationprocessingsystems,35:27730–27744.
References Raj Sanjay Shah, Kunal Chawla, Dheeraj Eidnani,
AgamShah,WendiDu,SudheerChava,NatrajRa-
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama man, Charese Smiley, Jiaao Chen, and Diyi Yang.
Ahmad, Ilge Akkaya, Florencia Leoni Aleman, 2022. When flue meets flang: Benchmarks and
DiogoAlmeida,JankoAltenschmidt,SamAltman, large pre-trained language model for financial do-
ShyamalAnadkat,etal.2023. Gpt-4technicalreport. main. arXivpreprintarXiv:2211.00083.
arXivpreprintarXiv:2303.08774.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
LeoGao,JonathanTow,BaberAbbasi,StellaBiderman, JonathanBerant.2018. Commonsenseqa:Aquestion
SidBlack,AnthonyDiPofi,CharlesFoster,Laurence answeringchallengetargetingcommonsenseknowl-
Golding,JeffreyHsu,AlainLeNoac’h,HaonanLi, edge. arXivpreprintarXiv:1811.00937.
KyleMcDonell,NiklasMuennighoff,ChrisOciepa,
Gemma Team, Thomas Mesnard, Cassidy Hardin,
Jason Phang, Laria Reynolds, Hailey Schoelkopf,
RobertDadashi,SuryaBhupatiraju,ShreyaPathak,
Aviya Skowron, Lintang Sutawika, Eric Tang, An-
LaurentSifre,MorganeRivière,MihirSanjayKale,
ishThite, BenWang, KevinWang, andAndyZou.
Juliette Love, et al. 2024. Gemma: Open models
2023. A framework for few-shot language model
based on gemini research and technology. arXiv
evaluation.
preprintarXiv:2403.08295.
NeelGuha,JulianNyarko,DanielHo,ChristopherRé,
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
AdamChilton,AlexChohlas-Wood,AustinPeters,
Martinet,Marie-AnneLachaux,TimothéeLacroix,
Brandon Waldon, Daniel Rockmore, Diego Zam-
BaptisteRozière,NamanGoyal,EricHambro,Faisal
brano, et al. 2024. Legalbench: A collaboratively
Azhar, et al. 2023a. Llama: Open and effi-
built benchmark for measuring legal reasoning in
cient foundation language models. arXiv preprint
largelanguagemodels. AdvancesinNeuralInforma-
arXiv:2302.13971.
tionProcessingSystems,36.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
DanHendrycks,CollinBurns,StevenBasart,AndyZou, bert, Amjad Almahairi, Yasmine Babaei, Nikolay
MantasMazeika,DawnSong,andJacobSteinhardt. Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
2020. Measuringmassivemultitasklanguageunder- Bhosale, et al. 2023b. Llama 2: Open founda-
standing. InInternationalConferenceonLearning tion and fine-tuned chat models. arXiv preprint
Representations. arXiv:2307.09288.
5Lewis Tunstall, Edward Beeching, Nathan Lambert, A DataGenerationPromptandResponse
Nazneen Rajani, Kashif Rasul, Younes Belkada,
Shengyi Huang, Leandro von Werra, Clémentine
Fourrier, Nathan Habib, et al. 2023. Zephyr: Di-
rect distillation of lm alignment. arXiv preprint
arXiv:2310.16944.
KarthikValmeekam,MatthewMarquez,AlbertoOlmo,
Sarath Sreedharan, and Subbarao Kambhampati.
2024. Planbench: Anextensiblebenchmarkforeval-
uatinglargelanguagemodelsonplanningandreason-
ingaboutchange. AdvancesinNeuralInformation
ProcessingSystems,36.
Tate Van Patten. 2023. Evaluating domain specific
llmperformancewithineconomicsusingthenovel
econqadataset.
Weizhi Wang, Hong Wang, and Xifeng Yan. 2023.
Steps:Abenchmarkfororderreasoninginsequential
tasks. arXivpreprintarXiv:2306.04441.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier-
ricCistac,TimRault,RémiLouf,MorganFuntowicz,
et al. 2019. Huggingface’s transformers: State-of-
the-artnaturallanguageprocessing. arXivpreprint
arXiv:1910.03771.
Siwei Yang, Bingchen Zhao, and Cihang Xie. 2024.
Aqa-bench: Aninteractivebenchmarkforevaluating
llms’ sequential reasoning ability. arXiv preprint
arXiv:2402.09404.
Alex Young, Bei Chen, Chao Li, Chengen Huang,
Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng
Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi:
Open foundation models by 01. ai. arXiv preprint
arXiv:2403.04652.
Liwen Zhang, Weige Cai, Zhaowei Liu, Zhi Yang,
WeiDai,YujieLiao,QianruQin,YifeiLi,Xingyu
Liu, Zhiqiang Liu, et al. 2023. Fineval: A chi-
nesefinancialdomainknowledgeevaluationbench-
mark for large language models. arXiv preprint
arXiv:2308.09975.
6Prompt:
Generateaconcisemulti-choicequestionwithoutreferencingtheoriginalnewsarticle,starting
withabriefscenariodescription. Includefoureventsfromthisscenariothatneedsequencingbased
ontheirlogicalconnection,notjusttheorderofappearanceinthenews. Theseeventsshouldbe
relevanttotypicalbusinessoreconomicsituations,requiringknowledgeofbusinesspracticesand
economicsforcorrectsequencing. Ensureasinglecorrectsequence, freefromambiguity. The
question should be crafted for independent completion, suitable for student evaluation. Create
original, concise content for both the question and choices, avoiding plagiarism and limiting
excessivedetailsornumericaldata. Formattheoutputwiththescenariodescription,followedby
choiceslabeledA,B,C,D,andfinishwiththecorrectsequenceandabriefexplanation,ensuring
thesequencelogicisdeduciblesolelyfromthequestionandchoicespresented.
Hereistheoutputformat:
```
Question: ...
Choices:
A....
B....
C....
D....
CorrectOrder: ...
Explanation: ...
```
Hereisthebusinessnewsarticle:
```
{news_article}
```
GPT-4Response:
Question: Arrange the following events related to the development and launch of Apple’s Siri
voiceassistantintheircorrectchronologicalorder.
Choices:
A.AppleacquiredSiriinApril2010.
B.Siriwasspunoutasastandalonebusinessin2008.
C.SiriwasintegratedintotheiPhone4S,whichwasannouncedtobesoldinstoresonOctober14.
D.WorkonSiribeganinthesummerof2007atSRIInternational.
CorrectOrder: D,B,A,C
Explanation: The development of Siri started in the summer of 2007 at SRI International. The
project was then spun out as a standalone business in 2008. Apple acquired Siri in April 2010.
Finally, Siri was integrated into the iPhone 4S, which was announced to be sold in stores on
October14.
Figure1: ThepromptstructureforgeneratingsortingquestionsinEconLogicQA.Itinvolvesfillingthecontentsof
newsarticlesintoprompttemplates. WealsogiveanexampleofGPT-4responsetoaspecificpromptconstructed
fromthisinformation.
7