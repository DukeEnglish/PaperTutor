MambaOut: Do We Really Need Mamba for Vision?
WeihaoYu XinchaoWang
NationalUniversityofSingapore
weihaoyu@u.nus.edu xinchao@nus.edu.sg
Code:https://github.com/yuweihao/MambaOut
InmemoryofKobeBryant
“WhatcanIsay,Mambaout.” —KobeBryant’sNBAfarewellspeechin2016.
Accuracy vs. MACs vs. Model Size
Linear Linear 86
MambaOut
84
VMamba
SSM PlainMamba
82
σ σ σ
80
Conv Conv
78
Linear Linear Linear Linear Model Size
76 Vision Mamba 20M 40M 80M
Gated CNN block Mamba block 0 5 10 15 20 25
(e.g. Our MambaOut) (e.g. Vision Mamba) MACs (G)
(a) (b)
Figure1: (a)ArchitectureofGatedCNN[18]andMamba[25]blocks(omittingNormalizationand
shortcut). TheMambablockextendstheGatedCNNwithanadditionalstatespacemodel(SSM).
AswillbeconceptuallydiscussedinSection3,SSMisnotnecessaryforimageclassificationon
ImageNet[19,65]. Toempiricallyverifythisclaim,westackGatedCNNblockstobuildaseriesof
modelsnamedMambaOut. (b)MambaOutoutperformsvisualMambamodels,e.g.,VisionMamhba
[102],VMamba[49]andPlainMamba[86],onImageNetimageclassification.
Abstract—Mamba,anarchitecturewithRNN-liketokenmixerofstatespace
model (SSM), was recently introduced to address the quadratic complexity of
theattentionmechanismandsubsequentlyappliedtovisiontasks1. Nevertheless,
the performance of Mamba for vision is often underwhelming when compared
withconvolutionalandattention-basedmodels. Inthispaper,wedelveintothe
essenceofMamba,andconceptuallyconcludethatMambaisideallysuitedfor
tasks with long-sequence and autoregressive characteristics. For vision tasks,
asimageclassificationdoesnotalignwitheithercharacteristic,wehypothesize
thatMambaisnotnecessaryforthistask;Detectionandsegmentationtasksare
also not autoregressive, yet they adhere to the long-sequence characteristic, so
we believe it is still worthwhile to explore Mamba’s potential for these tasks.
To empirically verify our hypotheses, we construct a series of models named
MambaOut through stacking Mamba blocks while removing their core token
mixer,SSM.Experimentalresultsstronglysupportourhypotheses. Specifically,
our MambaOut model surpasses all visual Mamba models on ImageNet image
classification, indicating that Mamba is indeed unnecessary for this task. As
for detection and segmentation, MambaOut cannot match the performance of
state-of-the-artvisualMambamodels,demonstratingthepotentialofMambafor
long-sequencevisualtasks.
1ThevisiontaskswediscussinthispaperincludeimageclassificationonImageNet[19,65],objectdetection
&instancesegmentationonCOCO[47]andsemanticsegmentationonADE20K[101].
Preprint.Underreview.
4202
yaM
31
]VC.sc[
1v29970.5042:viXra
)%(
ycaruccA
1-poT
teNegamI1 Introduction
Inrecentyears, Transformer[75]hasbecomethemainstreambackboneforvarioustasks, under-
pinningnumerousprominentmodelssuchasBERT[20], GPTseries[59,60,6,1]andViT[23].
However,thetokenmixerofTransformer,attention[3],incursaquadraticcomplexitywithrespect
tosequencelength,posingmajorchallengesforlongsequences. Toaddressthisissue,avarietyof
tokenmixerswithlinearcomplexitytotokenlengthhavebeenintroduced[71],suchasdynamic
convolution[81,83,39],Linformer[77],Longformer[5],BigBird[95],andPerformer[12]. More
recently, a new wave of RNN-like models has emerged [40, 96, 26, 58, 25], drawing significant
interestfromthecommunityfortheircapabilityofparallelizabletrainingandperformingefficient
inferenceonlongsequences. Notably,modelslikeRWKV[58]andMamba[25]areproventobe
effectiveasthebackboneforlargelanguagemodels(LLMs)[58,46].
Motivated by the promising capabilities of RNN-like models, various research endeavors have
attempted to introduce Mamba [25] into visual recognition tasks, exemplified by the pioneering
worksofVisionMamba[102],VMamba[49],LocalMamba[37],andPlainMamba[86],etc. The
tokenmixerofMambaisthestructuredstatespacemodels(SSM)[27,26,25],underthespiritof
RNN.Nevertheless,theirexperimentsshowthattheSSMbasedmodelsforvision,inreality,lead
to underwhelming performance compared with state-of-the-art convolutional [51, 21, 28, 63, 87,
48,90,35,78,91]andattention-basedmodels[16,73,22,93,45,74,69,90]. Thisgivesrisetoa
compellingresearchquestion: DowereallyneedMambaforVision?
In this paper, we investigate the nature of Mamba, and conceptually summarizes that Mamba is
ideallysuitedfortaskswithtwokeycharacteristics: long-sequenceandautoregressive,becauseof
thetheinherentRNNmechanismofSSM[27,26,25](seeexplanationofFigure2andFigure3).
Unfortunately,notmanyvisiontaskspossessbothcharacteristics. ImageclassificationonImageNet,
forexample,conformstoneither,whileobjectdetection&instancesegmentationonCOCOand
semanticsegmentationonADE20Kconformtoonlylong-sequence. Autoregressivecharacteristic,
ontheotherhand,demandsthateachtokenaggregateinformationsolelyfromprecedingandcurrent
tokens,aconceptdenotedascausalmodefortokenmixing.[62](seeFigure3(a)). Infact,allvisual
recognitiontasksfallwithintheunderstandingdomainratherthanthegenerativeone,meaningthat
themodelcanseetheentireimageatonce. Assuch,imposingadditionalcausalconstraintsontoken
mixinginvisualrecognitionmodelscouldleadtoaperformancedrop(seeFigure3(b)). Although
thisissuecanbemitigatedviabidirectionalbranches[67],itisinevitablethattheissuepersistswithin
eachbranch.
Basedontheconceptualdiscussionabove,weproposethetwohypothesesasfollows:
• Hypothesis1: SSMisnotnecessaryforimageclassification,sincethistaskconformsto
neitherthelong-sequenceorautoregressivecharacteristic.
• Hypothesis2: SSMmaybepotentiallybeneficialforobjectdetection&instancesegmenta-
tionandsemanticsegmentation,sincetheyfollowthelong-sequencecharacteristic,though
theyarenotautoregressive.
To experimentally validate our hypotheses, we developed a series of models termed MambaOut
throughstackingGatedCNN[18]blocks. ThekeydistinctionbetweenGatedCNNandMamba
blocksliesintheexistenceofSSM,asillustratedinFigure1(a). Experimentalresultsdemonstrate
thatthesimplerMambaOutmodel,inreality,alreadysurpassestheperformanceofvisualMamba
models[102,49,37,86],whichinturnverifiesourHypothesis1. Wealsoshowempiricalresultsthat
MambaOutfallsshortofmatchingtheperformanceofstate-of-the-artvisualMambamodels[49,37]
indetectionandsegmentationtasks(seeTables2and3),whichunderscoresthepotentialofSSMon
thesetasksandeffectivelyvalidatesourHypothesis2.
Thecontributionsofourpaperarethreefold. Firstly,weanalyzetheRNN-likemechanismofSSM
andconceptuallyconcludethatMambaissuitedfortaskswithlong-sequenceandautoregressive
characteristics. Secondly,weexaminethecharacteristicsofvisualtasksandhypothesizethatSSMis
unnecessaryforimageclassificationonImageNetsincethistaskdoesnotmeeteithercharacteristic,
yet exploring the potential of SSM for detection and segmentation tasks remains valuable since
thesetasksconformtolong-sequencecharacteristic, thoughtheyarenotautoregressive. Thirdly,
wedevelopaseriesofmodelsnamedMambaOutbasedonGatedCNNblocksbutwithoutSSM.
ExperimentsshowthatMambaOuteffectivelysurpassesvisualMambamodelsinImageNetimage
2classificationbutdoesnotreachtheperformanceofstate-of-the-artvisualMambamodelsindetection
andsegmentationtasks. Theseobservations,inturn,validateourhypotheses. Assuch,MambaOut,
becauseofitsOccam’srazornature,mayreadilyserveasanaturalbaselineforfutureresearchon
visualMambamodels.
2 Relatedwork
Transformerhasbeenwidelyutilizedinvariousdomains,likeBERT[20]andGPTseries[59,60,6,1]
in NLP and ViT [23] in computer vision. However, the attention module in Transformers scales
quadraticallywithsequencelength, presentingasignificantcomputationalchallenge. Numerous
studies[71]haveexploredvariousstrategiestomitigatethisissue,includinglow-rankapproaches
[77], kernelization [40, 12], token mixing range limitation [5, 95, 50, 29], and history memory
compression [61]. More recently, RNN-like methods [17, 40, 96], particularly RWKV [58] and
Mamba[25],havegarneredattentionfortheirpromisingresultsinlargelanguagemodels[58,46].
EagerexploratoryresearchershavequicklymovedtoincorporateSSMandMamba[25]intovisual
recognitiontasks[102,49,37,86,44,56,57,97,85]. Forinstance,VisionMamba[102]integrates
Mamba[25]todevelopisotropicvisionmodelsakintoViT[23];VMamba[49]employsMamba
toconstructhierarchicalvisionmodelssimilartoAlexNet[42]andResNet[32];LocalMamba[37]
enhancesvisualMambamodels[102,49]byincorporatinglocalinductivebiases;PlainMamba[86]
aimstofurtherenhancetheperformanceofisotropicMambamodels;EfficientVMamba[57]focuses
onefficiencythroughtheintroductionofatrousselectivescanforlightweightvisualMambamodels.
Unlike these initiatives, our work does not aim to design new visual Mamba models. Instead,
we explore a pertinent research question about the necessity of Mamba [25] in traditional visual
recognitioncontexts[19,65,47,101]. Wehopethispapercanprovideinsightsforfutureresearchon
visualMambamodels.
3 Conceptualdiscussion
Inthissection,wefirstdiscusswhatcharacteristicsoftaskstheMambamodelissuitedfor. Next,we
examinewhethervisualrecognitiontasksconformtothesecharacteristics. Basedontheexamination
results,weproposehypothesesregardingthenecessityofMambaforvision.
3.1 WhattasksisMambasuitablefor?
ThetokenmixerofMambaisselectiveSSM[26,25]whichdefinesfourinput-dependentparameters
(∆,A,B,C)andtransformsthemto(A,B,C)by
A=exp(∆A), B=(∆A)−1(exp(∆A)−I)·∆B. (1)
Thenthesequence-to-sequencetransformationofSSMcanbeexpressedby
h =Ah +Bx , (2)
t t−1 t
y =Ch , (3)
t t
wheretdenotesthetimestep,x representstheinput,h signifiesthehiddenstate,andy indicates
t t t
the output. The recurrent property [34] of Equation 2 distinguishes RNN-like SSM from causal
attention. Thehiddenstatehcanbeseenasafixed-sizememorythatstoresallhistoricalinformation.
ThroughEquation2,thismemoryisupdatedwhileitssizeremainsconstant. Thefixedsizemeans
thememoryisinevitablylossy,butitensuresthatthecomputationalcomplexityofintegratingthe
memorywiththecurrentinputremainsconstant. Conversely,causalattentionstoresallkeysand
valuesfromprevioustokensasitsmemory,whichexpandsbyaddingthecurrenttoken’skeyand
value with each new input. This memory is theoretically lossless. However, as more tokens are
inputted,thememorysizegrows,therebyincreasingthecomplexityofintegratingthememorywith
thecurrentinput. ThedifferencesinmemorymechanismsbetweenRNN-likemodelsandcausal
attentionarefurtherillustratedinFigure2.
BecauseSSM’smemoryisinherentlylossy,itlogicallyfallsshortofthelosslessmemoryofattention.
Consequently, Mambacannotshowcaseitsstrengthsinhandlingshortsequences, anareawhere
3Causal attention RNN-like
Old memory Old memory
New memory New memory
None
Step 1 Fuse Fuse
New memory
Old memory Old memory
New memory
Step 2 Fuse Fuse
New memory
Old memory
Old memory
New memory
Step 3 Fuse Fuse
...
(a) (b)
Figure 2: The mechanism illustration of causal attention and RNN-like models from memory
perspective,wherex denotestheinputtokenofi-thstep. (a)Causalattentionstoresallprevious
i
tokens’ keys k and values v as memory. The memory is updated by continuously adding the
currenttoken’skeyandvalue,sothememoryislossless,butthedownsideisthatthecomputational
complexity of integrating old memory and current tokens increases as the sequence lengthens.
Therefore, attention can effectively manage short sequences but may encounter difficulties with
longer ones. (b) In contrast, RNN-like models compress previous tokens into fixed-size hidden
stateh,whichservesasthememory. ThisfixedsizemeansthatRNNmemoryisinherentlylossy,
whichcannotdirectlycompetewiththelosslessmemorycapacityofattentionmodels. Nonetheless,
RNN-like models can demonstrate distinct advantages in processing long sequences, as the
complexityofmergingoldmemorywithcurrentinputremainsconstant,regardlessofsequence
length.
attentionperformswellwithease. However,inscenariosinvolvinglongsequences,attentionwill
falterduetoitsquadraticcomplexity. Inthiscase,Mambacandistinctlyhighlightitsefficiencyin
mergingmemorywiththecurrentinput,thusmanaginglongsequencessmoothly. Therefore,Mamba
isparticularlywell-suitedforprocessinglongsequences.
AlthoughtherecurrentnatureofSSM(Equation2)allowsMambatohandlelongsequencesefficiently,
itintroducesasignificantlimitation: h canonlyaccessinformationfromthepreviousandcurrent
t
timesteps. AsillustratedinFigure3,thistypeoftokenmixingistermedcausalmode,whichcanbe
formulatedas:
y =f(x ,x ,...,x ), (4)
t 1 2 t
wherex andy representtheinputandoutputofthet-thtoken,respectively. Duetoitscausalnature,
t t
thismodeiswell-suitedforautoregressivegenerationtasks.
Anothermodeiscalledfully-visiblemode,whereeachtokencanaggregateinformationfromall
precedingandsubsequenttokens. Thismeanstheoutputofeachtokendependsontheinputsfromall
tokens:
y =f(x ,x ,...,x ,...,x ), (5)
t 1 2 t T
whereT representsthetotalnumberoftokens. Thefully-visiblemodeissuitableforunderstanding
tasks,whereallinputscanbeaccessedbythemodelatonce.
Attentionisinfully-visiblemodebydefault, butitcaneasilyturnintocausalmodebyapplying
causal masks to the attention maps. RNN-like models inherently operate in causal mode due to
theirrecurrentproperties,asillustratedbyMamba’sEquation2. Duetothisinherentcharacteristic,
RNN-likemodelscannotbetransformedintofully-visiblemode. AlthoughRNNscanapproximate
afully-visiblemodeusingbidirectionalbranches,eachbranchstillindividuallyremainsincausal
4Fully-visible mode Causal mode
86
Fully-visible mode (default)
𝑦 ! 𝑦 ! 84
Causal mode
𝑦 " 𝑦 " 82
𝑦 # 𝑦 # 80 79.8 78.9
𝑦 𝑦 78
$ $
𝑦 𝑦 76
% %
𝑥 𝑥 𝑥 𝑥 𝑥 𝑥 𝑥 𝑥 𝑥 𝑥 74
! " # $ % ! " # $ %
72.2
Input Input 72
70.6
e.g., BERT and ViT’s e.g., GPT’s attention and 70
attention Mamba’s SSM 68
ViT-Ti ViT-S
(a) (b)
Figure3: (a)Twomodesoftokenmixing[62]. oratotalofT tokens,thefully-visiblemodeallows
tokenttoaggregateinputsfromalltokens,i.e.,{xi}T ,tocomputeitsoutputy . Incontrast,the
i=1 t
causalmoderestrictstokenttoonlyaggregateinputsfromprecedingandcurrenttokens{x }t .
i i=1
Bydefault,attentionoperatesinfully-visiblemodebutcanbeadjustedtocausalmodewithcausal
attentionmasks. RNN-likemodels,suchasMamba’sSSM[25,26],inherentlyoperateincausal
modeduetotheirrecurrentnature. (b)WemodifytheViT’sattention[23,72]fromfully-visible
tocausalmodeandobserveperformancedroponImageNet,whichindicatescausalmixingis
unnecessaryforunderstandingtasks.
mode. Therefore,Mambaiswell-suitedfortasksthatrequirecausaltokenmixing,duetotheinherent
limitationsofitsrecurrentproperties.
Insummary,Mambaisideallysuitedfortasksthatdisplaythefollowingcharacteristics:
• Characteristic1: Thetaskinvolvesprocessinglongsequences.
• Characteristic2: Thetaskrequirescausaltokenmixingmode.
Next,wewilldiscusswhethervisualrecognitiontasksexhibitthesetwocharacteristics.
3.2 Dovisualrecognitiontaskshaveverylongsequences?
Inthissubsection,weexplorewhethervisualrecognitiontasksnecessitatelongsequencemodeling.
WeusetheTransformermodel[75]asacasestudytofacilitateouranalysis. ConsideraTransformer
blockwithacommonMLPratioof4;assumingitsinputX ∈RL×D hasatokenlengthofLand
channel(embedding)dimensionsofD,theFLOPsfortheblockcanbecalculatedas:
FLOPs=24D2L+4DL2. (6)
Fromthis,wederivetheratioofthequadratictermtothelinearterminLas:
4DL2 L
r = = . (7)
L 24D2L 6D
If L > 6D, the computational load of the quadratic term in L surpasses that of the linear term.
Thisprovidesasimplemetrictodetermineifthetaskinvolveslongsequences. Forinstance,with
384 channels in ViT-S, the threshold τ = 6×384 = 2304, and for 768 channels in ViT-B,
small
τ =6×768=4608.
base
ForimageclassificationonImageNet,thetypicalinputimagesizeis2242,resultingin142 =196
tokenswithpatchsizeof162. Clearly,196ismuchlessthanbothτ andτ ,indicatingthat
small base
imageclassificationonImageNetdoesnotqualifyasalong-sequencetask.
Forobjectdetection&instancesegmentationonCOCO,withaninferenceimagesizeof800×1280,
andforsemanticsegmentationonADE20K,withaninferenceimagesizeof512×2048,thenumber
oftokensisapproximately4K,givenpatchsizeof162. Since4K > τ and4K ≈ τ ,both
small base
detectiononCOCOandsegmentationonADE20Kcanbeconsideredlong-sequencetasks.
5
tuptuO
)%(
ycaruccA
1-poT
teNegamIIntput Stage 1 Stage 2 Stage 3 Stage 4
Conv σ
Gated CNN Gated CNN Gated CNN Gated CNN Linear Linear
Blocks Blocks Blocks Blocks
Norm
(a) Overall framework of MambaOut (b) Gated CNN block
Figure4: (a)TheoverallframeworkofMambaOutforvisualrecognition. SimilartoResNet[32],
MambaOutadoptshierarchicalarchitecturewithfourstages. D representsthechanneldimensionsat
i
thei-thstage. (b)ThearchitectureofGatedCNNblock. ThedifferencebetweentheGatedCNN
block[18]andtheMambablock[25]liesintheabsenceoftheSSM(statespacemodel)intheGated
CNNblock.
3.3 Dovisualrecognitiontasksneedcausaltokenmixingmode?
AsdiscussedinSection3.1andillustratedinFigure3,thefully-visibletokenmixingmodeallows
unrestricted range of mixing, whereas the causal mode limits the current token to only access
information from preceding tokens. Visual recognition is categorized as an understanding task,
whereinthemodelcanseetheentireimageatonce,eliminatingtheneedforrestrictionsontoken
mixing. Imposingadditionalconstraintsontokenmixingcanpotentiallydegrademodelperformance.
AsdemonstratedinFigure3(b),whencausalrestrictionsareappliedtoVisionTransformers(ViT)
[23, 72], a noticeable decline in performance is observed. Generally, the fully-visible mode is
appropriateforunderstandingtasks,whilethecausalmodeisbettersuitedforautoregressivetasks.
This claim can also be substantiated by the observation that BERT [20] and ViT [23] (BEiT [4]
andMAE[30])areusedmoreforunderstandingtasksthanGPT-1/2[59,60]andimageGPT[9].
Therefore,visualrecognitiontasksdonotneedcausaltokenmixingmode.
3.4 HypothesesregardingthenecessityofMambaforvision
Basedonourprecedingdiscussion,wesummarizeourhypothesesregardingthenecessityofintro-
ducingMambaforvisualrecognitiontasksasfollows:
• Hypothesis1: ItisnotnecessarytointroduceSSMforimageclassificationonImageNet,as
thistaskdoesnotmeetCharacteristic1orCharacteristic2.
• Hypothesis 2: It is still worthwhile to further explore the potential of SSM for visual
detectionandsegmentationsincethesetasksalignwithCharacteristic2,despitenotfulfilling
Characteristic1.
4 Experimentalverification
4.1 GatedCNNandMambaOut
Next,weaimtovalidateourhypothesesempirically. AsdepictedinFigure1(a),Mambablockis
based[25]ontheGatedCNNblock[18]. BoththeGatedCNNandMamba’smeta-architecturecan
beconsideredasasimplifiedintegrationoftheMetaFormer’s[89]tokenmixerandanMLP,akinto
MetaNeXt[91]. Formally,giventheinputX ∈RN×D,themeta-architectureisformulatedas:
X′ =Norm(X), (8)
Y =(TokenMixer(X′W )⊙σ(X′W ))W +X, (9)
1 2 3
whereNorm(·)representsnormalization[38,2,82];TokenMixer(·)TokenMixer(·)referstothe
moduletoconducttokenmixing[90];W ∈RD×rD,W ∈RD×rDandW ∈RrD×Darelearnable
1 2 3
6
Stem
Downsampling Downsampling DownsamplingAlgorithm1PyTorchcodeofGatedCNNblock
import torch
import torch.nn as nn
class GatedCNNBlock(nn.Module):
def __init__(self, dim, expension_ratio=8/3, kernel_size=7, conv_ratio=1.0,
norm_layer=partial(nn.LayerNorm,eps=1e-6),
act_layer=nn.GELU,
drop_path=0.):
super().__init__()
self.norm = norm_layer(dim)
hidden = int(expension_ratio * dim)
self.fc1 = nn.Linear(dim, hidden * 2)
self.act = act_layer()
conv_channels = int(conv_ratio * dim)
self.split_indices = (hidden, hidden - conv_channels, conv_channels)
self.conv = nn.Conv2d(conv_channels, conv_channels, kernel_size=kernel_size, padding=kernel_size//2,
groups=conv_channels)
self.fc2 = nn.Linear(hidden, dim)
def forward(self, x):
shortcut = x # [B, H, W, C] = x.shape
x = self.norm(x)
g, i, c = torch.split(self.fc1(x), self.split_indices, dim=-1)
c = c.permute(0, 3, 1, 2) # [B, H, W, C] -> [B, C, H, W]
c = self.conv(c)
c = c.permute(0, 2, 3, 1) # [B, C, H, W] -> [B, H, W, C]
x = self.fc2(self.act(g) * torch.cat((i, c), dim=-1))
return x + shortcut
parameterswithMLPexpansionr;σisactivationfunction[24,33]. TokenmixersofGatedCNN
andMambaare:
TokenMixer (Z)=Conv(Z) (10)
GatedCNN
TokenMixer (Z)=SSM(σ(Conv(Z))) (11)
Mamba
ComparingEquations10and11,andreferencingFigure1(a),theprimarydistinctionbetweenthe
GatedCNN[59]andtheMambablock[25]liesinthepresenceofSSM.Thispromptsustodevelop
a series of models, termed MambaOut, which are based on the Gated CNN block without SSM.
MambaOutwillhelpusassessthenecessityofMambaforvisualrecognitiontasks.
Specifically, we specify the token mixer of Gated CNN as depthwise convolution [11] of 7×7
kernelsize,followingConvNeXt[51,54]. Besides,toimprovethepracticalspeed,weonlyconduct
depthwiseconvolutiononpartialchannels[53,91,7],followingInceptionNeXt[91]. Asshownin
Algorithm 1, the implementation of Gated CNN block is simple and elegant. Similar to ResNet,
weadopt4-stageframeworktobuildMambaOutbystackingGatedCNNblocksateachstage,as
depicted in Figure 4. The configuration details of each model size are shown in Table 4 in the
appendix.
4.2 ImageclassificationonImageNet
Setup. ImageNet[19,65]servesasthegoldstandardbenchmarkforimageclassification,encom-
passing a wide array of 1,000 common classes. It comprises approximately 1.3 million training
imagesand50,000validationimages. ThetrainingschemefollowsDeiT[72]withoutdistillation.
Specifically,theuseddataaugmentationcontainsrandomresizedcrop(inputimagesizeof2242),
horizontalflip,RandAugment[15],Mixup[98],CutMix[94],RandomErasing[100]andcolorjitter,
andregularizationtechniquesincludeweightdecay,stochasticdepth[36]andlabelsmoothing[70].
AllourmodelsaretrainedbyAdamW[52,41].Thelearningratescalingruleislr= batchsize×10−3.
1024
Inthispaper,wesetthebatchsizeto4096,sothelearningrateis0.004. OurMambaOutmodels
areimplementedwithPyTorch[55]andtimm[80]librariesandtrainedonTPUv3. Moretraining
hyper-parametersareshowninTable5intheappendix.
Results. The performance of our MambaOut models, visual Mamba models, and various other
convolutionandattention-basedmodelsonImageNet[19,65]ispresentedinTable1. Notably,our
MambaOutmodels,whichdonotincorporateSSM,consistentlyoutperformvisualMambamodels
[102,49,37,57,86]thatincludeSSMacrossallmodelsizes. Forinstance,theMambaOut-Small
7Table1: PerformanceofmodelsonImageNetattheresolutionof2242. OurMambaOutmodel
employstheGatedCNNblock[59]. TheMambablock[25],derivedfromtheGatedCNNblock,
incorporatesanadditionalSSM(statespacemodel). ItisevidentthatvisualMambamodelsfall
shortofMambaOut’sperformance,letalonesurpassingstate-of-the-artconvolutionalorconvolution-
attention-hybridmodels. *NotethatVMambaV9modifiesthemeta-architectureoftheMambablock
toMetaFormer[90],differentfromothervisualMambamodelsandMambaOut.
Token
Test@2242
Token
Test@2242
Param Param
Model Mixing (M) MAC Acc Model Mixing (M) MAC Acc
Type (G) (%) Type (G) (%)
VAN-B0[28] Conv 4 0.9 75.4 ConvNeXt-S[51] Conv 50 8.7 83.1
FasterNet-T1[7] Conv 8 0.9 76.2 VAN-B3[28] Conv 45 9.0 83.9
InceptionNeXt-A[91] Conv 4 0.5 75.3 ConvFormer-S36[90] Conv 40 7.6 84.1
DeiT-Ti[72] Attn 6 1.3 72.2 InternImage-S[78] Conv 50 8 84.2
T2T-ViT-7[92] Attn 4 1.1 71.7 T2T-ViT-19[92] Attn 39 8.5 81.9
PVTv2-B0[79] Conv+Attn 3 0.6 70.5 Swin-S[50] Attn 50 8.7 83.0
MobileViTv3-XS[76] Conv+Attn 3 0.9 76.7 Focal-Small[88] Attn 51 9.1 83.5
EMO-6M[99] Conv+Attn 6 1.0 79.0 CSWin-S[22] Attn 35 6.9 83.6
Vim-Ti[102] Conv+SSM 7 1.5 76.1 MViTv2-S[45] Attn 35 7.0 83.6
LocalVim-T[37] Conv+SSM 8 1.5 76.2 CoAtNet-1[16] Conv+Attn 42 8.4 83.3
EfficientVMamba-T[57] Conv+SSM 6 0.8 76.5 UniFormer-B[43] Conv+Attn 50 8.3 83.9
EfficientVMamba-S[57] Conv+SSM 11 1.3 78.7 CAFormer-S36[90] Conv+Attn 39 8.0 84.5
MambaOut-Femto Conv 7 1.2 78.9 SG-Former-M[64] Conv+Attn 39 7.5 84.1
PoolFormer-S24[89] Pool 21 3.4 80.3 TransNeXt-Small[68] Conv+Attn 50 10.3 84.7
ConvNeXt-T[51] Conv 29 4.5 82.1 VMamba-S[49] Conv+SSM 44 11.2 83.5
VAN-B2[28] Conv 27 5.0 82.8 LocalVMamba-S[37] Conv+SSM 50 11.4 83.7
ConvFormer-S18[90] Conv 27 3.9 83.0 PlainMamba-L2[86] Conv+SSM 25 8.1 81.6
InternImage-T[78] Conv 30 5 83.5 VMambaV9-S[49] Conv+SSM 50 8.7 83.6
InceptionNeXt-T[91] Conv 28 4.2 82.3 MambaOut-Small Conv 48 9.0 84.1
DeiT-S[72] Attn 22 4.6 79.8 ConvNeXt-B[51] Conv 89 15.4 83.8
T2T-ViT-14[92] Attn 22 4.8 81.5 RepLKNet-31B[21] Conv 79 15.3 83.5
Swin-T[50] Attn 29 4.5 81.3 ConvFormer-M36[90] Conv 57 12.8 84.5
Focal-Tiny[88] Attn 29 4.9 82.2 HorNet-B[63] Conv 88 15.5 84.3
CSWin-T[22] Attn 23 4.3 82.7 InternImage-B[78] Conv 97 16 84.9
CoAtNet-0[16] Conv+Attn 25 4.2 81.6 DeiT-B[72] Attn 86 17.5 81.8
iFormer-S[69] Conv+Attn 20 4.8 83.4 T2T-ViT-24[92] Attn 64 13.8 82.3
CAFormer-S18[90] Conv+Attn 26 4.1 83.6 Swin-B[50] Attn 88 15.4 83.5
SG-Former-S[64] Conv+Attn 23 4.8 83.2 CSwin-B[22] Attn 78 15.0 84.2
TransNeXt-Tiny[68] Conv+Attn 28 5.7 84.0 MViTv2-B[45] Attn 52 10.2 84.4
Vim-S[102] Conv+SSM 26 5.1 80.5 CoAtNet-2[16] Conv+Attn 75 15.7 84.1
VMamba-T[49] Conv+SSM 22 5.6 82.2 iFormer-L[69] Conv+Attn 87 14.0 84.8
Mamba-2D-S[44] Conv+SSM 24 – 81.7 CAFormer-M36[90] Conv+Attn 56 13.2 85.2
LocalVim-S[37] Conv+SSM 28 4.8 81.2 TransNeXt-Base[68] Conv+Attn 90 18.4 84.8
LocalVMamba-T[37] Conv+SSM 26 5.7 82.7 VMamba-B[49] Conv+SSM 75 18.0 83.7
EfficientVMamba-B[57] Conv+SSM 33 4.0 81.8 Mamba-2D-B[44] Conv+SSM 92 – 83.0
PlainMamba-L1[86] Conv+SSM 7 3.0 77.9 PlainMamba-L3[86] Conv+SSM 50 14.4 82.3
VMambaV9-T*[49] Conv+SSM 31 4.9 82.5 VMambaV9-B[49] Conv+SSM 89 15.4 83.9
MambaOut-Tiny Conv 27 4.5 82.7 MambaOut-Base Conv 85 15.8 84.2
model achieves top-1 accuracy of 84.1%, 0.4% higher than that of LocalVMamba-S [37], while
requiringonly79%oftheMACs. TheseresultsstronglysupportourHypothesis1,whichpositsthat
introducingSSMforimageclassificationonImageNetisunnecessary,aligningwiththeprincipleof
Occam’srazor.
Additionally,visualMambamodelscurrentlyexhibitasignificantperformancegapwhencompared
tostate-of-the-artconvolutionandattentionmodels. Forinstance,theCAFormer-M36[90],which
employs traditional token mixers like simple separable convolutions [66] and standard attention
mechanisms [75], outperforms all visual Mamba models of comparable size by more than 1%
in accuracy. Should future research aim to challenge our Hypothesis 1, it will be necessary to
developvisualMambamodelswithtokenmixersofconvolutionandSSMtoachievestate-of-the-art
performanceonImageNet.
8Table 2: Performance of object detection and instance segmentation on COCO with Mask
R-CNN. TheMACsaremeasuredwithinputsizeof800×1280.
Token Param MAC MaskR-CNN1×schedule
Backbone
MixingType (M) (G) APb APb APb APm APm APm
50 75 50 75
ConvNeXt-T[48] Conv 48 262 44.2 66.6 48.3 40.1 63.3 42.8
FocalNet-T[87] Conv 49 268 46.1 68.2 50.6 41.5 65.1 44.5
Swin-T[50] Attn 48 267 42.7 65.2 46.8 39.3 62.2 42.2
ViT-Adapter-S[10] Attn 48 403 44.7 65.8 48.3 39.9 62.5 42.8
CSWin-T[22] Attn 42 279 46.7 68.6 51.3 42.2 65.6 45.4
PVTv2-B2[79] Conv+Attn 45 309 45.3 67.1 49.6 41.2 64.2 44.4
SG-Former-S[64] Conv+Attn 41 – 47.4 69.0 52.0 42.6 65.9 46.0
TransNeXt-Tiny[68] Conv+Attn 48 – 49.9 71.5 54.9 44.6 68.6 48.1
VMamba-T[49] Conv+SSM 42 286 46.5 68.5 50.7 42.1 65.5 45.3
LocalVMamba-T[37] Conv+SSM 45 291 46.7 68.7 50.8 42.2 65.7 45.5
EfficientVMamba-B[57] Conv+SSM 53 252 43.7 66.2 47.9 40.2 63.3 42.9
VMambaV9-T[49] Conv+SSM 50 270 47.4 69.5 52.0 42.7 66.3 46.0
MambaOut-Tiny Conv 43 262 45.1 67.3 49.6 41.0 64.1 44.1
ConvNeXt-S[48] Conv 70 348 45.4 67.9 50.0 41.8 65.2 45.1
FocalNet-S[87] Conv 72 365 48.3 70.5 53.1 43.1 67.4 46.2
Swin-S[50] Attn 69 354 44.8 66.6 48.9 40.9 63.2 44.2
CSWin-S[22] Attn 54 342 47.9 70.1 52.6 43.2 67.1 46.2
PVTv2-B3[79] Conv+Attn 65 397 47.0 68.1 51.7 42.5 65.7 45.7
SG-Former-M[64] Conv+Attn 51 – 48.2 70.3 53.1 43.6 66.9 47.0
TransNeXt-Small[68] Conv+Attn 69 – 51.1 72.6 56.2 45.5 69.8 49.1
VMamba-S[49] Conv+SSM 64 400 48.2 69.7 52.5 43.0 66.6 46.4
LocalVMamba-S[37] Conv+SSM 69 414 48.4 69.9 52.7 43.2 66.7 46.5
PlainMamba-L1[86] Conv+SSM 31 388 44.1 64.8 47.9 39.1 61.6 41.9
VMambaV9-S[49] Conv+SSM 64 357 48.7 70.0 53.4 43.7 67.3 47.0
MambaOut-Small Conv 65 354 47.4 69.1 52.4 42.7 66.1 46.2
ConvNeXt-B[48] Conv 108 486 47.0 69.4 51.7 42.7 66.3 46.0
FocalNet-B[87] Conv 111 507 49.0 70.9 53.9 43.5 67.9 46.7
Swin-B[50] Attn 107 496 46.9 – – 42.3 – –
ViT-Adapter-B[10] Attn 102 557 47.0 68.2 51.4 41.8 65.1 44.9
CSWin-B[22] Attn 97 526 48.7 70.4 53.9 43.9 67.8 47.3
PVTv2-B5[79] Conv+Attn 102 557 47.4 68.6 51.9 42.5 65.7 46.0
TransNeXt-Base[68] Conv+Attn 109 – 51.7 73.2 56.9 45.9 70.5 49.7
VMamba-B[49] Conv+SSM 96 540 48.5 69.6 53.0 43.1 67.0 46.4
PlainMamba-L2[86] Conv+SSM 53 542 46.0 66.9 50.1 40.6 63.8 43.6
VMambaV9-B[49] Conv+SSM 108 485 49.2 70.9 53.9 43.9 67.7 47.6
MambaOut-Base Conv 100 495 47.4 69.3 52.2 43.0 66.4 46.3
4.3 Objectdetection&instancesegmentationonCOCO
Setup. COCO2017[47]servesasawidelyrecognizedbenchmarkforobjectdetectionandinstance
segmentation.Inourexperiments,MambaOutisemployedasthebackbonewithinMaskR-CNN[31],
initializedwithweightspre-trainedonImageNet. Weadheretothestandard1×trainingschedule
of12epochs. Thetrainingimagesareresizedsuchthattheshortersidemeasures800pixels,while
thelongersidedoesnotexceed1333pixels. TheAdamWoptimizer[52,41]isusedwithalearning
rate of 0.0001 and a total batch size of 16. Our implementation leverages the PyTorch [55] and
mmdetection[8]libraries. WeutilizeFP16precisiontosavetrainingcosts. Theexperimentsare
conductedon4GPUsofNVIDIA4090.
Results. AlthoughMambaOutcansurpasssomevisualMambamodels[57,86]inobjectdetection
andinstancesegmentationonCOCO[47], itstilllagsbehindthestate-of-the-artvisualMambas,
suchasVMamba[49]andLocalVMamba[49]. Forinstance,theperformanceofMambaOut-Tinyas
thebackboneforMaskR-CNNtrailsVMamba-T[49]by1.4APband1.1APm. Thisperformance
disparityunderscoresthebenefitsofintegratingMambainlong-sequencevisualtasks,reinforcingour
Hypothesis2. However,visualMambastillexhibitsasignificantperformancegapwhencomparedto
thestate-of-the-artconvolution-attention-hybridmodels,TransNeXt[68]. VisualMambaneedsto
furthervalidateitseffectivenessbyoutperformingotherstate-of-the-artmodelsinthevisualdetection
task.
4.4 SemanticsegmentationonADE20K
Setup. ADE20K[101],awidely-usedbenchmarkforthesemanticsegmentationtask,encompasses
150 semantic categories. It includes 20,000 images in the training set and 2,000 images in the
validation set. In our experiments, Mamba is employed as the backbone for UperNet [84], with
9Table3: PerformanceofSemanticsegmentationwithUperNet[84]onADE20K[101]validation
set. TheMACsaremeasuredwithinputsizeof512×2048.
Token UperNet
Backbone
MixingType Param(M) MAC(G) mIoU(SS) mIoU(MS)
ConvNeXt-T [48] Conv 60 939 46.0 46.7
HorNet-T[63] Conv 55 924 49.2 49.3
ConvFormer-S18[90] Conv 54 925 47.5 48.6
InternImage-T[78] Conv 59 944 47.9 48.1
Swin-T[50] Attn 60 945 44.4 45.8
Twins-S[13] Attn 54 901 46.2 47.1
Focal-T[88] Attn 62 998 45.8 47.0
CSWin-T[22] Attn 60 959 49.3 50.7
UniFormer-S[43] Conv+Attn 52 955 47.0 48.5
CAFormer-S18[90] Conv+Attn 54 1024 48.1 48.9
SG-Former-S[64] Conv+Attn 53 989 49.9 51.5
TransNeXt-Tiny[68] Conv+Attn 59 – 51.1 51.7
VMamba-T[49] Conv+SSM 55 964 47.3 48.3
LocalVMamba-T[37] Conv+SSM 57 970 47.9 49.1
EfficientVMamba-B[57] Conv+SSM 65 930 46.5 47.3
PlainMamba-L2[86] Conv+SSM 55 285 – 46.8
PlainMamba-L3[86] Conv+SSM 81 419 – 49.1
VMambaV9-T[49] Conv+SSM 62 948 48.3 48.6
MambaOut-Tiny Conv 54 938 47.4 48.6
ConvNeXt-S[48] Conv 82 1027 48.7 49.6
HorNet-S[63] Conv 85 1027 50.0 50.5
ConvFormer-S36[90] Conv 67 1003 49.6 50.7
InternImage-S[78] Conv 80 1017 50.1 50.9
Swin-S[50] Attn 81 1038 47.6 49.5
Twins-B[13] Attn 89 1020 47.7 48.9
Focal-S[88] Attn 85 1130 48.0 50.0
CSWin-S[22] Attn 65 1027 50.4 51.5
CAFormer-S36[90] Conv+Attn 67 1197 50.6 50.8
SG-Former-M[64] Conv+Attn 68 1114 51.2 52.1
TransNeXt-Small[68] Conv+Attn 80 – 52.2 52.8
VMamba-S[49] Conv+SSM 76 1081 49.5 50.5
LocalVMamba-S[37] Conv+SSM 81 1095 50.0 51.0
VMambaV9-S[49] Conv+SSM 82 1039 50.6 51.2
MambaOut-Small Conv 76 1032 49.5 50.6
ConvNeXt-B [48] Conv 122 1170 49.1 49.9
HorNet-B[63] Conv 126 1171 50.5 50.9
ConvFormer-M36[90] Conv 85 1113 50.4 51.3
InternImage-B[78] Conv 128 1185 50.8 51.3
Swin-B[50] Attn 121 1188 48.1 49.7
Twins-L[13] Attn 133 1164 48.8 50.2
Focal-B[88] Attn 126 1354 49.0 50.5
CSWin-B[22] Attn 110 1222 51.1 52.2
UniFormer-B[43] Conv+Attn 80 1106 49.5 50.7
CAFormer-M36[90] Conv+Attn 84 1346 51.7 51.7
SG-Former-B[64] Conv+Attn 109 1304 52.0 52.7
TransNeXt-Base[68] Conv+Attn 90 – 53.0 53.7
VMamba-B[49] Conv+SSM 110 1226 50.0 51.3
VMambaV9-B[49] Conv+SSM 122 1170 51.0 51.6
MambaOut-Base Conv 112 1178 49.6 51.0
initialization from ImageNet pre-trained weights. The training is conducted using the AdamW
optimizer [41, 52] with learning rate of 0.0001 and batch size of 16 for 160,000 iterations. Our
implementation utilizes the PyTorch [55] and mmsegmentation [14] libraries. Experiments are
performedonfourGPUsofNVIDIA4090,withFP16precisiontoenhancethetrainingspeed.
Results. TheperformancetrendforsemanticsegmentationonADE20Kissimilartoobjectdetection
on COCO. MambaOut can outperform some visual Mamba models but cannot match the results
ofstate-of-the-artMambamodels. Forinstance,LocalVMamba-T[37]surpassesMambaOut-Tiny
by0.5mIoUinbothsinglescale(SS)andmulti-scale(MS)evaluations,furthercorroboratingour
Hypothesis2empirically.Additionally,visualMambamodelscontinuetoexhibitnotableperformance
deficitswhencomparedtothemoreadvancedhybridmodelsthatintegrateconvolutionandattention
mechanisms,suchasSG-Former[64]andTransNeXt[68]. VisualMambaneedstofurthershowcase
itslong-sequencemodelingstrengthsbydeliveringstrongerperformanceinvisualsegmentationtask.
105 Conclusion
Inthispaper,wediscusstheMambamechanismconceptuallyandconcludethatitisideallysuitedfor
taskswithlong-sequenceandautoregressivecharacteristics. Weanalyzecommonvisualtasksagainst
thesecriteriaandarguethatintroducingMambaforImageNetimageclassificationisunnecessary,as
itmeetsneithercharacteristic.However,thepotentialofMambaforvisualdetectionandsegmentation
tasks,whichalignwiththelong-sequencecharacteristic,meritsfurtherexploration. Tosubstantiate
ourclaimsempirically,wedevelopMambaOutmodelsthatemployMambablockswithouttheircore
tokenmixer,SSM.MambaOutsurpassesallvisualMambamodelsonImageNet,yetitexhibitsa
notableperformancegapcomparedtostate-of-the-artvisualMambamodels,therebyvalidatingour
assertions. Duetocomputationalresourcelimitations,thispaperonlyverifiestheMambaconcept
forvisualtasks. Inthefuture, wemayfurtherexploreMambaandRNNconceptsaswellasthe
integration of RNN and Transformers for large language models (LLMs) and large multimodal
models(LMMs).
Acknowledgement
WeihaowaspartlysupportedbySnapResearchFellowship,GoogleTPUResearchCloud(TRC),
andGoogleCloudResearchCreditsprogram. WethankDongzeLian,QiuhongShen,XingyiYang,
andGongfanFangforvaluablediscussions.
References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport. arXiv
preprintarXiv:2303.08774,2023.
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450,2016.
[3] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointlylearning
toalignandtranslate. arXivpreprintarXiv:1409.0473,2014.
[4] HangboBao,LiDong,SonghaoPiao,andFuruWei. Beit:Bertpre-trainingofimagetransformers. In
InternationalConferenceonLearningRepresentations,2021.
[5] IzBeltagy,MatthewEPeters,andArmanCohan. Longformer:Thelong-documenttransformer. arXiv
preprintarXiv:2004.05150,2020.
[6] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
[7] JierunChen,Shiu-hongKao,HaoHe,WeipengZhuo,SongWen,Chul-HoLee,andS-HGaryChan.Run,
don’twalk:Chasinghigherflopsforfasterneuralnetworks. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages12021–12031,2023.
[8] KaiChen,JiaqiWang,JiangmiaoPang,YuhangCao,YuXiong,XiaoxiaoLi,ShuyangSun,WansenFeng,
ZiweiLiu,JiaruiXu,ZhengZhang,DazhiCheng,ChenchenZhu,TianhengCheng,QijieZhao,Buyu
Li,XinLu,RuiZhu,YueWu,JifengDai,JingdongWang,JianpingShi,WanliOuyang,ChenChange
Loy,andDahuaLin. MMDetection: Openmmlabdetectiontoolboxandbenchmark. arXivpreprint
arXiv:1906.07155,2019.
[9] MarkChen,AlecRadford,RewonChild,JeffreyWu,HeewooJun,DavidLuan,andIlyaSutskever.
Generativepretrainingfrompixels. InInternationalconferenceonmachinelearning,pages1691–1703.
PMLR,2020.
[10] ZheChen,YuchenDuan,WenhaiWang,JunjunHe,TongLu,JifengDai,andYuQiao.Visiontransformer
adapterfordensepredictions. InTheEleventhInternationalConferenceonLearningRepresentations,
2022.
[11] FrançoisChollet. Xception:Deeplearningwithdepthwiseseparableconvolutions. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pages1251–1258,2017.
11[12] KrzysztofChoromanski,ValeriiLikhosherstov,DavidDohan,XingyouSong,AndreeaGane,Tamas
Sarlos,PeterHawkins,JaredDavis,DavidBelanger,LucyColwell,etal. Maskedlanguagemodelingfor
proteinsvialinearlyscalablelong-contexttransformers. arXivpreprintarXiv:2006.03555,2020.
[13] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and
ChunhuaShen. Twins: Revisitingthedesignofspatialattentioninvisiontransformers. Advancesin
neuralinformationprocessingsystems,34:9355–9366,2021.
[14] MMSegmentationContributors. MMSegmentation: Openmmlabsemanticsegmentationtoolboxand
benchmark. https://github.com/open-mmlab/mmsegmentation,2020.
[15] EkinDCubuk,BarretZoph,JonathonShlens,andQuocVLe. Randaugment:Practicalautomateddata
augmentationwithareducedsearchspace. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognitionworkshops,pages702–703,2020.
[16] ZihangDai,HanxiaoLiu,QuocVLe,andMingxingTan. Coatnet:Marryingconvolutionandattention
foralldatasizes. Advancesinneuralinformationprocessingsystems,34:3965–3977,2021.
[17] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-xl:Attentivelanguagemodelsbeyondafixed-lengthcontext. InProceedingsofthe57th
AnnualMeetingoftheAssociationforComputationalLinguistics,pages2978–2988,2019.
[18] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutionalnetworks. InInternationalconferenceonmachinelearning,pages933–941.PMLR,2017.
[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,pages
248–255.Ieee,2009.
[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
[21] XiaohanDing,XiangyuZhang,JungongHan,andGuiguangDing. Scalingupyourkernelsto31x31:
Revisitinglargekerneldesignincnns. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages11963–11975,2022.
[22] XiaoyiDong,JianminBao,DongdongChen,WeimingZhang,NenghaiYu,LuYuan,DongChen,and
BainingGuo. Cswintransformer:Ageneralvisiontransformerbackbonewithcross-shapedwindows. In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages12124–12134,
2022.
[23] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal. Animageis
worth16x16words:Transformersforimagerecognitionatscale. arXivpreprintarXiv:2010.11929,2020.
[24] KunihikoFukushima. Visualfeatureextractionbyamultilayerednetworkofanalogthresholdelements.
IEEETransactionsonSystemsScienceandCybernetics,5(4):322–333,1969.
[25] AlbertGuandTriDao. Mamba: Linear-timesequencemodelingwithselectivestatespaces. arXiv
preprintarXiv:2312.00752,2023.
[26] AlbertGu,KaranGoel,andChristopherRé. Efficientlymodelinglongsequenceswithstructuredstate
spaces. arXivpreprintarXiv:2111.00396,2021.
[27] AlbertGu,IsysJohnson,KaranGoel,KhaledSaab,TriDao,AtriRudra,andChristopherRé. Combining
recurrent,convolutional,andcontinuous-timemodelswithlinearstatespacelayers. Advancesinneural
informationprocessingsystems,34:572–585,2021.
[28] Meng-HaoGuo,Cheng-ZeLu,Zheng-NingLiu,Ming-MingCheng,andShi-MinHu. Visualattention
network. ComputationalVisualMedia,9(4):733–752,2023.
[29] AliHassani,StevenWalton,JiachenLi,ShenLi,andHumphreyShi. Neighborhoodattentiontransformer.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages6185–
6194,2023.
[30] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick.Maskedautoencoders
arescalablevisionlearners. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages16000–16009,2022.
12[31] KaimingHe,GeorgiaGkioxari,PiotrDollár,andRossGirshick. Maskr-cnn. InProceedingsoftheIEEE
internationalconferenceoncomputervision,pages2961–2969,2017.
[32] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages770–778,2016.
[33] DanHendrycksandKevinGimpel. Gaussianerrorlinearunits(gelus). arXivpreprintarXiv:1606.08415,
2016.
[34] SeppHochreiterandJürgenSchmidhuber. Longshort-termmemory. Neuralcomputation,9(8):1735–
1780,1997.
[35] QibinHou,Cheng-ZeLu,Ming-MingCheng,andJiashiFeng. Conv2former:Asimpletransformer-style
convnetforvisualrecognition. arXivpreprintarXiv:2211.11943,2022.
[36] GaoHuang,YuSun,ZhuangLiu,DanielSedra,andKilianQWeinberger. Deepnetworkswithstochastic
depth. InComputerVision–ECCV2016: 14thEuropeanConference, Amsterdam, TheNetherlands,
October11–14,2016,Proceedings,PartIV14,pages646–661.Springer,2016.
[37] TaoHuang,XiaohuanPei,ShanYou,FeiWang,ChenQian,andChangXu. Localmamba:Visualstate
spacemodelwithwindowedselectivescan. arXivpreprintarXiv:2403.09338,2024.
[38] SergeyIoffeandChristianSzegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreducing
internalcovariateshift. InInternationalconferenceonmachinelearning,pages448–456.pmlr,2015.
[39] Zi-HangJiang,WeihaoYu,DaquanZhou,YunpengChen,JiashiFeng,andShuichengYan. Convbert:
Improving bert with span-based dynamic convolution. Advances in Neural Information Processing
Systems,33:12837–12848,2020.
[40] AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFrançoisFleuret. Transformersarernns:
Fastautoregressivetransformerswithlinearattention. InInternationalconferenceonmachinelearning,
pages5156–5165.PMLR,2020.
[41] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014.
[42] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetclassificationwithdeepconvolutional
neuralnetworks. Advancesinneuralinformationprocessingsystems,25,2012.
[43] KunchangLi,YaliWang,JunhaoZhang,PengGao,GuangluSong,YuLiu,HongshengLi,andYuQiao.
Uniformer:Unifyingconvolutionandself-attentionforvisualrecognition. IEEETransactionsonPattern
AnalysisandMachineIntelligence,2023.
[44] Shufan Li, Harkanwar Singh, and Aditya Grover. Mamba-nd: Selective state space modeling for
multi-dimensionaldata. arXivpreprintarXiv:2402.05892,2024.
[45] YanghaoLi,Chao-YuanWu,HaoqiFan,KarttikeyaMangalam,BoXiong,JitendraMalik,andChristoph
Feichtenhofer. Mvitv2: Improvedmultiscalevisiontransformersforclassificationanddetection. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages4804–4814,
2022.
[46] OpherLieber,BarakLenz,HofitBata,GalCohen,JhonathanOsin,ItayDalmedigos,ErezSafahi,Shaked
Meirom,YonatanBelinkov,ShaiShalev-Shwartz,etal. Jamba:Ahybridtransformer-mambalanguage
model. arXivpreprintarXiv:2403.19887,2024.
[47] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,
andCLawrenceZitnick. Microsoftcoco:Commonobjectsincontext. InComputerVision–ECCV2014:
13thEuropeanConference,Zurich,Switzerland,September6-12,2014,Proceedings,PartV13,pages
740–755.Springer,2014.
[48] ShiweiLiu,TianlongChen,XiaohanChen,XuxiChen,QiaoXiao,BoqianWu,TommiKärkkäinen,
MykolaPechenizkiy,DecebalMocanu,andZhangyangWang. Moreconvnetsinthe2020s:Scalingup
kernelsbeyond51x51usingsparsity. arXivpreprintarXiv:2207.03620,2022.
[49] YueLiu,YunjieTian,YuzhongZhao,HongtianYu,LingxiXie,YaoweiWang,QixiangYe,andYunfan
Liu. Vmamba:Visualstatespacemodel. arXivpreprintarXiv:2401.10166,2024.
[50] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo. Swin
transformer:Hierarchicalvisiontransformerusingshiftedwindows. InProceedingsoftheIEEE/CVF
internationalconferenceoncomputervision,pages10012–10022,2021.
13[51] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSainingXie. A
convnetforthe2020s. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages11976–11986,2022.
[52] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
[53] NingningMa,XiangyuZhang,Hai-TaoZheng,andJianSun. Shufflenetv2: Practicalguidelinesfor
efficientcnnarchitecturedesign. InProceedingsoftheEuropeanconferenceoncomputervision(ECCV),
pages116–131,2018.
[54] Franck Mamalet and Christophe Garcia. Simplifying convnets for fast learning. In International
ConferenceonArtificialNeuralNetworks,pages58–65.Springer,2012.
[55] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performancedeeplearninglibrary. Advancesinneuralinformationprocessingsystems,32,2019.
[56] BadriNPatroandVijaySAgneeswaran. Simba:Simplifiedmamba-basedarchitectureforvisionand
multivariatetimeseries. arXivpreprintarXiv:2403.15360,2024.
[57] XiaohuanPei,TaoHuang,andChangXu. Efficientvmamba:Atrousselectivescanforlightweightvisual
mamba. arXivpreprintarXiv:2403.09977,2024.
[58] BoPeng,EricAlcaide,QuentinAnthony,AlonAlbalak,SamuelArcadinho,HuanqiCao,XinCheng,
MichaelChung,MatteoGrella,KranthiKiranGV,etal. Rwkv:Reinventingrnnsforthetransformerera.
arXivpreprintarXiv:2305.13048,2023.
[59] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguageunderstand-
ingbygenerativepre-training. 2018.
[60] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[61] JackWRae,AnnaPotapenko,SiddhantMJayakumar,andTimothyPLillicrap.Compressivetransformers
forlong-rangesequencemodelling. arXivpreprintarXiv:1911.05507,2019.
[62] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.
Journalofmachinelearningresearch,21(140):1–67,2020.
[63] YongmingRao,WenliangZhao,YansongTang,JieZhou,SerNamLim,andJiwenLu. Hornet:Efficient
high-order spatial interactions with recursive gated convolutions. Advances in Neural Information
ProcessingSystems,35:10353–10366,2022.
[64] SuchengRen,XingyiYang,SonghuaLiu,andXinchaoWang. Sg-former:Self-guidedtransformerwith
evolvingtokenreallocation. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages6003–6014,2023.
[65] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. Internationaljournalofcomputervision,115:211–252,2015.
[66] MarkSandler,AndrewHoward,MenglongZhu,AndreyZhmoginov,andLiang-ChiehChen.Mobilenetv2:
Invertedresidualsandlinearbottlenecks. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages4510–4520,2018.
[67] MikeSchusterandKuldipKPaliwal. Bidirectionalrecurrentneuralnetworks. IEEEtransactionson
SignalProcessing,45(11):2673–2681,1997.
[68] Dai Shi. Transnext: Robust foveal visual perception for vision transformers. arXiv preprint
arXiv:2311.17132,2023.
[69] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, and Shuicheng Yan. Inception
transformer. AdvancesinNeuralInformationProcessingSystems,35:23495–23509,2022.
[70] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniewWojna. Rethinkingthe
inceptionarchitectureforcomputervision. InProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition,pages2818–2826,2016.
14[71] YiTay,MostafaDehghani,DaraBahri,andDonaldMetzler. Efficienttransformers: Asurvey. ACM
ComputingSurveys,55(6):1–28,2022.
[72] HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,andHervé
Jégou. Training data-efficient image transformers & distillation through attention. In International
conferenceonmachinelearning,pages10347–10357.PMLR,2021.
[73] HugoTouvron,MatthieuCord,AlexandreSablayrolles,GabrielSynnaeve,andHervéJégou. Going
deeperwithimagetransformers. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision,pages32–42,2021.
[74] ZhengzhongTu,HosseinTalebi,HanZhang,FengYang,PeymanMilanfar,AlanBovik,andYinxiao
Li. Maxvit:Multi-axisvisiontransformer. InEuropeanconferenceoncomputervision,pages459–479.
Springer,2022.
[75] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser, andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessing
systems,30,2017.
[76] ShaktiNWadekarandAbhishekChaurasia.Mobilevitv3:Mobile-friendlyvisiontransformerwithsimple
andeffectivefusionoflocal,globalandinputfeatures. arXivpreprintarXiv:2209.15159,2022.
[77] SinongWang,BelindaZLi,MadianKhabsa,HanFang,andHaoMa. Linformer: Self-attentionwith
linearcomplexity. arXivpreprintarXiv:2006.04768,2020.
[78] WenhaiWang,JifengDai,ZheChen,ZhenhangHuang,ZhiqiLi,XizhouZhu,XiaoweiHu,TongLu,
Lewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with
deformableconvolutions. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages14408–14419,2023.
[79] WenhaiWang,EnzeXie,XiangLi,Deng-PingFan,KaitaoSong,DingLiang,TongLu,PingLuo,and
LingShao. Pvtv2:Improvedbaselineswithpyramidvisiontransformer. ComputationalVisualMedia,
8(3):415–424,2022.
[80] Ross Wightman. Pytorch image models. https://github.com/rwightman/
pytorch-image-models,2019.
[81] FelixWu,AngelaFan,AlexeiBaevski,YannNDauphin,andMichaelAuli. Paylessattentionwith
lightweightanddynamicconvolutions. arXivpreprintarXiv:1901.10430,2019.
[82] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
computervision(ECCV),pages3–19,2018.
[83] ZhanghaoWu,ZhijianLiu,JiLin,YujunLin,andSongHan. Litetransformerwithlong-shortrange
attention. arXivpreprintarXiv:2004.11886,2020.
[84] TeteXiao,YingchengLiu,BoleiZhou,YuningJiang,andJianSun. Unifiedperceptualparsingforscene
understanding. InProceedingsoftheEuropeanconferenceoncomputervision(ECCV),pages418–434,
2018.
[85] RuiXu,ShuYang,YihuiWang,BoDu,andHaoChen. Asurveyonvisionmamba:Models,applications
andchallenges. arXivpreprintarXiv:2404.18861,2024.
[86] Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, and
ElliotJCrowley. Plainmamba:Improvingnon-hierarchicalmambainvisualrecognition. arXivpreprint
arXiv:2403.17695,2024.
[87] JianweiYang,ChunyuanLi,XiyangDai,andJianfengGao. Focalmodulationnetworks. Advancesin
NeuralInformationProcessingSystems,35:4203–4217,2022.
[88] JianweiYang,ChunyuanLi,PengchuanZhang,XiyangDai,BinXiao,LuYuan,andJianfengGao. Focal
attentionforlong-rangeinteractionsinvisiontransformers. AdvancesinNeuralInformationProcessing
Systems,34:30008–30022,2021.
[89] WeihaoYu,MiLuo,PanZhou,ChenyangSi,YichenZhou,XinchaoWang,JiashiFeng,andShuicheng
Yan. Metaformerisactuallywhatyouneedforvision. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages10819–10829,2022.
15[90] WeihaoYu,ChenyangSi,PanZhou,MiLuo,YichenZhou,JiashiFeng,ShuichengYan,andXinchao
Wang.Metaformerbaselinesforvision.IEEETransactionsonPatternAnalysisandMachineIntelligence,
2024.
[91] Weihao Yu, Pan Zhou, Shuicheng Yan, and Xinchao Wang. Inceptionnext: When inception meets
convnext. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,2024.
[92] LiYuan,YunpengChen,TaoWang,WeihaoYu,YujunShi,Zi-HangJiang,FrancisEHTay,JiashiFeng,
andShuichengYan. Tokens-to-tokenvit: Trainingvisiontransformersfromscratchonimagenet. In
ProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages558–567,2021.
[93] LiYuan,QibinHou,ZihangJiang,JiashiFeng,andShuichengYan. Volo:Visionoutlookerforvisual
recognition. IEEEtransactionsonpatternanalysisandmachineintelligence,45(5):6575–6586,2022.
[94] SangdooYun, DongyoonHan, SeongJoonOh, SanghyukChun, JunsukChoe, andYoungjoonYoo.
Cutmix:Regularizationstrategytotrainstrongclassifierswithlocalizablefeatures. InProceedingsofthe
IEEE/CVFinternationalconferenceoncomputervision,pages6023–6032,2019.
[95] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontanon,PhilipPham,AnirudhRavula,QifanWang,LiYang,etal. Bigbird:Transformersforlonger
sequences. Advancesinneuralinformationprocessingsystems,33:17283–17297,2020.
[96] ShuangfeiZhai,WalterTalbott,NitishSrivastava,ChenHuang,HanlinGoh,RuixiangZhang,andJosh
Susskind. Anattentionfreetransformer. arXivpreprintarXiv:2105.14103,2021.
[97] HanweiZhang,YingZhu,DanWang,LijunZhang,TianxiangChen,andZiYe. Asurveyonvisual
mamba. arXivpreprintarXiv:2404.15956,2024.
[98] HongyiZhang,MoustaphaCisse,YannNDauphin,andDavidLopez-Paz. mixup:Beyondempiricalrisk
minimization. InInternationalConferenceonLearningRepresentations,2018.
[99] JiangningZhang,XiangtaiLi,JianLi,LiangLiu,ZhucunXue,BoshenZhang,ZhengkaiJiang,Tianxin
Huang,YabiaoWang,andChengjieWang. Rethinkingmobileblockforefficientattention-basedmodels.
In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1389–1400. IEEE
ComputerSociety,2023.
[100] ZhunZhong,LiangZheng,GuoliangKang,ShaoziLi,andYiYang. Randomerasingdataaugmentation.
InProceedingsoftheAAAIconferenceonartificialintelligence,volume34,pages13001–13008,2020.
[101] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene
parsingthroughade20kdataset. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages633–641,2017.
[102] LianghuiZhu,BenchengLiao,QianZhang,XinlongWang,WenyuLiu,andXinggangWang. Vision
mamba: Efficientvisualrepresentationlearningwithbidirectionalstatespacemodel. arXivpreprint
arXiv:2401.09417,2024.
A MoredetailsofMambaOutmodels
TheMambaOutmodelconfigurationsareshowninTable4andthehyper-parameterstotrainMam-
baOutonImageNetareshowninTable5.
Table4: ConfigurationsofMambaOutmodels. Thecontentsinthetuplesrepresenttheconfigurations
inthefourstagesofthemodels.
Size Femto Tiny Small Base
Stem 3×3convwithstride2;Norm;GELU;3×3convwithstride2,Norm
Downsamplinglayers 3×3convwithstride2
Tokenmixer 7×7depthwiseconv
MLPratio 8/3
ClassifierHead Globalaveragepooling,Norm,MLP
#Blocks (3,3,9,3) (3,3,9,3) (3,4,27,3) (3,4,27,3)
#Channel (48,96,192,288) (96,192,384,576) (96,192,384,576) (128,256,512,768)
Parameters(M) 7.3 26.5 48.5 84.8
MACs(G) 1.2 4.5 9.0 15.8
16Table5: Hyper-parametersofMambaOutonImageNetimageclassification.
MambaOut
Femto Tiny Small Base
Inputresolution 2242
Epochs 300
Batchsize 4096
Optimizer AdamW
Adamϵ 1e-8
Adam(β ,β ) (0.9,0.999)
1 2
Learningrate 4e-3
Learningratedecay Cosine
Gradientclipping None
Warmupepochs 20
Weightdecay 0.05
RandAugment 9/0.5
RepeatedAugmentation off
Cutmix 1.0
Mixup 0.8
Cutmix-Mixupswitchprob 0.5
Randomerasingprob 0.25
Labelsmoothing 0.1
Peakstochasticdepthrate 0.025 0.2 0.4 0.6
Randomerasingprob 0.25
EMAdecayrate None
17