Localized Adaptive Risk Control
MatteoZecchin OsvaldoSimeone
CentreforIntelligentInformationProcessingSystems
DepartmentofEngineering
King’sCollegeLondon
London,UnitedKingdom
{matteo.1.zecchin,osvaldo.simeone}@kcl.ac.uk
Abstract
AdaptiveRiskControl(ARC)isanonlinecalibrationstrategybasedonsetpredic-
tionthatoffersworst-casedeterministiclong-termriskcontrol,aswellasstatistical
marginalcoverageguarantees. ARCadjuststhesizeofthepredictionsetbyvary-
ingasinglescalarthresholdbasedonfeedbackfrompastdecisions. Inthiswork,
we introduce Localized Adaptive Risk Control (L-ARC), an online calibration
schemethattargetsstatisticallocalizedriskguaranteesrangingfromconditional
risktomarginalrisk,whilepreservingtheworst-caseperformanceofARC.L-ARC
updatesathresholdfunctionwithinareproducingkernelHilbertspace(RKHS),
withthekerneldeterminingtheleveloflocalizationofthestatisticalriskguarantee.
Thetheoreticalresultshighlightatrade-offbetweenlocalizationofthestatistical
riskandconvergencespeedtothelong-termrisktarget. Thankstolocalization,
L-ARCisdemonstratedviaexperimentstoproducepredictionsetswithriskguar-
anteesacrossdifferentdatasubpopulations,significantlyimprovingthefairnessof
thecalibratedmodelfortaskssuchasimagesegmentationandbeamselectionin
wirelessnetworks.
1 Introduction
Adaptive risk control (ARC), also known as online risk control, is a powerful tool for reliable
decision-makinginonlinesettingswherefeedbackisobtainedaftereachdecision[GibbsandCandes,
2021, Feldman et al., 2022]. ARC finds applications in domains, such as finance, robotics, and
health, in which it is important to ensure reliability in forecasting, optimization, or control of
complexsystems[Wisniewskietal.,2020,Lekeufacketal.,2023,Zhangetal.,2023,Zecchinetal.,
2024]. Whileprovidingworst-casedeterministicguaranteesofreliability,ARCmaydistributesuch
guaranteesunevenlyintheinputspace,favoringasubpopulationofinputsatthedetrimentofanother
subpopulation.
As an example, consider the tumor segmentation task illustrated in Figure 1. In this setting, the
objectiveistocalibrateapre-trainedsegmentationmodeltogeneratemasksthataccuratelyidentify
tumorareasaccordingtoauser-definedreliabilitylevel[Yuetal.,2016]. Thecalibrationprocess
typically involves combining data from various datasets, such as those collected from different
hospitals. For an online setting, as visualized in the figure, ARC achieves the desired long-term
reliabilityintermsoffalsenegativeratio.However,itdoessobyprioritizingcertaindatasets,resulting
inunsatisfactoryperformanceonotherdatasources. Suchbehaviorisparticularlydangerous,asit
mayresultinsomesubpopulationsbeingpoorlydiagnosed. Thispaperaddressesthisshortcomingof
ARCbyproposinganovellocalizedvariantofARC.
Preprint.Underreview.
4202
yaM
31
]LM.tats[
1v67970.5042:viXraFigure1: CalibrationofatumorsegmentationmodelviaARC[Angelopoulosetal.,2024a]andthe
proposedlocalizedARC,L-ARC.Calibrationdatacomprisesimagesfrommultiplesources,namely,
theKvasirdataset[Jhaetal.,2020]andtheETIS-LaribPolypDBdataset[Silvaetal.,2014]. Both
ARCandL-ARCachieveworst-casedeterministiclong-termriskcontrolintermsoffalsenegative
rate(FNR).However,ARCdoessobyprioritizingKvasirsamplesatthedetrimentoftheLaribdata
source,forwhichthemodelhaspoorFNRperformance. Incontrast,L-ARCcanyielduniformly
satisfactoryperformanceforbothdatasubpopulations.
1.1 AdaptiveRiskControl
Toelaborate,consideranonlinedecision-makingscenarioinwhichinputsareprovidedsequentially
toapre-trainedmodel. Ateachtimestept 1,themodelobservesafeaturevectorX ,andbasedon
t
aboundednon-conformityscoringfunction≥ s: [0,S ]andathresholdλ R,itoutputs
max t
X ×Y → ∈
apredictionset
C =C(X ,λ )= y :s(X ,y) λ , (1)
t t t t t
{ ∈Y ≤ }
where isthedomainofthetargetvariableY. Aftereachtimestept,themodelreceivesfeedback
Y
intheformofalossfunction
L = (C ,Y ) (2)
t t t
L
thatisassumedtobenon-negative,upperboundedbyB < andnon-increasinginthepredicted
∞
setsize C . Anotableexampleisthemiscoverageloss
t
| |
(C,y)=1 y / C . (3)
L { ∈ }
Accordingly, for an input-output sequence (X ,Y ) T the performance of the set predictions
C T in(1)canbegaugedviathecumula{ tivet riskt }t=1
{ t }t=1
T T
L¯(T)= 1 (cid:88) (C ,Y )= 1 (cid:88) L . (4)
T L t t T t
t=1 t=1
Forauser-specifiedlosslevelαandalearningratesequence η T ,ARCupdatesthethresholdλ
{ t }t=1 t
in(1)as[Feldmanetal.,2022]
λ =λ +η (L α), (5)
t+1 t t t
−
whereL αmeasuresthediscrepancybetweenthecurrentloss(2)andthetargetα. Forstepsize
t
−
decreasingasη =η t−1/2fora (0,1)andanarbitraryη >0,theresultsin[Angelopoulosetal.,
t 1 1
∈
2024b]implythattheupdaterule(5)guaranteesthatthecumulativerisk(4)forthemiscoverageloss
(3)convergestotargetlevelαforanydatasequence (X ,Y ) as
t t t≥1
{ }
(cid:12) (cid:12)L¯(T) α(cid:12) (cid:12) S max+η 1B , (6)
− ≤ √T
thusofferingaworst-casedeterministiclong-termguarantee. Furthermorewhendataaregenerated
i.i.d.as(X ,Y ) P forallt 1,inthespecialcaseofthemiscoverageloss(3),thesetpredictor
t t XY
∼ ≥
producedby(5)enjoystheasymptoticmarginalcoverageguarantee
p
lim Pr[Y / C ]=α, (7)
T
T→∞ ∈
wheretheprobabilityiscomputedwithrespecttothetestsample(X,Y) P ,whichisindepen-
XY
dentofthesequenceofsamples (X ,Y ) T ,andtheconvergenceisin∼ probabilitywithrespectto
{ t t }t=1
thesequence (X ,Y ) . Notethatin[Angelopoulosetal.,2024b],astrongerversionof(7)is
t t t≥1
{ }
provided,inwhichthelimitholdsalmostsurely.
2Figure2: ThedegreeoflocalizationinL-ARCisdictatedbythechoiceofthereweightingfunction
class viathemarginal-to-conditionalguarantee(9). Attheleftmostextreme,weillustrateconstant
W
reweighting functions, for which marginal guarantees are recovered. At the rightmost extreme,
reweighting with maximal localization given by Dirac delta functions for which the criterion (9)
correspondstoaconditionalguarantee. Inbetweenthetwoextremesliefunctionsets withan
W
intermediateleveloflocalizationyieldinglocalizedguarantees.
1.2 ConditionalandLocalizedRisk
The convergence guarantee (7) for ARC is marginalized over the covariate X. Therefore, there
is no guarantee that the conditional miscoverage Pr[Y / C X =x] is smaller than the target
T
∈ |
α. This problem is particularly relevant for high-stakes applications in which it is important to
ensureahomogeneouslevelofreliabilityacrossdifferentregionsoftheinputspace,suchasacross
subpopulations. Thatsaid,evenwhenthesetpredictorC(X )isobtainedbasedonanoffline
cal
|D
calibration data set with i.i.d. data (X,Y) P , it is generally impossible to control the
cal XY
D ∼
conditionalmiscoverageprobabilityas
Pr[Y / C(X )X =x] α forallx (8)
cal
∈ |D | ≤ ∈X
withoutmakingfurtherassumptionsaboutthedistributionP orproducinguninformativeprediction
XY
sets[Vovk,2012,FoygelBarberetal.,2021].
Arelaxedmarginal-to-conditionalguaranteewasconsideredbyGibbsetal.[2023],whichrelaxed
themarginalmiscoveragerequirement(8)as
(cid:20) (cid:21)
w(X)
E 1 Y / C(X ) αforallw() , (9)
X,Y,Dcal E [w(X)] { ∈ |Dcal } ≤ · ∈W
X
where isasetofnon-negativereweightingfunctions,andtheexpectationistakenoverthejoint
W
distribution of the calibration data and the test pair (X,Y). Note that with a singleton set
cal
D
encompassingasingleconstantfunction,e.g.,w(x) = 1,thecriterion(9)reducestomarginal
W
coverage. Furthermore, as illustratedin Figure 2, depending on the degree of localizationof the
functionsinset ,thecriterion(9)interpolatesbetweenmarginalandconditionalguarantees.
W
At the one extreme, a marginal guarantee like (7) is recovered when the reweighting functions
areconstant. Conversely,attheotherextreme, conditionalguaranteesasin(8)emergewhenthe
reweightingfunctionsaremaximallylocalized,i.e.,when = w(x)=δ(x µ):µ ,where
W { − ∈X}
δ(x)denotestheDiracdeltafunction. Inbetweenthesetwoextremes,oneobtainsanintermediate
degreeoflocalization. Forexample,thiscanbedonebyconsideringreweightingfunctionssuchas
=(cid:40) w(x)=(cid:88)∞
β
(cid:32) κexp(cid:32)
∥x −µ
i
∥2(cid:33) +1(cid:33)
:E [w(X)]>0, andw(x) 0 x
(cid:41)
,
W i − l X ≥ ∀ ∈X
i=1
(10)
wherel 0isafixedlengthscale,κ 0isafixedscalingparameter,and denotestheEuclidean
≥ ≥ ∥·∥
norm.Furthermore,functionw(x)mayalsodependontheoutputofthepre-trainedmodel,supporting
calibrationrequirementsviaconstraintsoftheform(9)[Zhangetal.,2024].
InGibbsetal.[2023], theauthorsdemonstratedthatitispossibletodesignofflinesetpredictors
C(X )thatapproximatelycontrolrisk(9),withanapproximationgapthatdependsonthedegree
cal
|D
oflocalizationofthefamily ofweightingfunctions.
W
31.3 LocalizedRiskControl
Motivatedbytheimportanceofconditionalriskguarantees,weproposeLocalizedARC(L-ARC),a
novelonlinecalibrationalgorithmthatproducespredictionsetswithlocalizedstatisticalriskcontrol
guaranteesasin(9),whilealsoretainingtheworst-casedeterministiclong-termguarantees(6)of
ARC.UnlikeGibbsetal.[2023],ourworkfocusesononlinesettingsinwhichcalibrationiscarried
outsequentiallybasedonfeedbackreceivedonpastdecisions.
ThekeytechnicalinnovationofL-ARCliesinthewaysetpredictionsareconstructed. Asdetailedin
Section2,L-ARCpredictionsetsreplacethesinglethresholdin(1)withathresholdfunctiong()
·
mappingcovariateX toalocalizedthresholdvalueg(X). Thethresholdfunctionisadaptedinan
onlinefashionwithinareproducingkernelHilbertspace(RKHS)family basedonaninputdata
G
streamandlossfeedback. ThechoiceoftheRKHSfamilydeterminesthefamily ofweighting
W
functionsinthestatisticalguaranteeoftheform(9),thusdictatingthedesiredleveloflocalization.
Themaintechnicalresults,presentedinSection2.3,areasfollows.
• Inthecaseofi.i.d. sequences,(X ,Y ) P forallt 1,L-ARCprovideslocalized
t t XY
∼ ≥
statisticalriskguaranteeswherethereweightingclass correspondstoallnon-negative
W
functions w with a positive mean under distribution P . More precisely, given a
XY
∈ G
targetlossvalueα,thetime-averagedthresholdfunction
T
1 (cid:88)
g¯ ()= g (), (11)
T · T t ·
t=1
ensuresthatforanyfunctionw ,thelimit
∈W
(cid:20) (cid:21)
w(X) p
limsupE (C(X,g¯ ),Y) α+A( ,w) (12)
X,Y E [w(X)]L T ≤ G
T→∞ X
holds,whereconvergenceisinprobabilitywithrespecttothesequence (X ,Y ) and
t t t≥1
{ }
theaverageisoverthetestpair(X,Y). ThegapA( ,w)dependsonboththeRKHS and
G G
functionw;itincreaseswiththeleveloflocalizationofthefunctionsintheRKHS ;andit
G
equalszerointhecaseofconstantthresholdfunctions,recovering(7)forthespecialcaseof
themiscoverageloss.
• Furthermore,foranarbitrarysequence (X ,Y ) L-ARChasacumulativelossthat
t t t≥1
{ }
convergestoaneighborhoodofthenominalreliabilitylevelαas
(cid:12) (cid:12)
(cid:12)1 (cid:88)T (cid:12) B( )
(cid:12) (C(X ,g ),Y ) α(cid:12) G +C( ), (13)
(cid:12) (cid:12)T L t t t − (cid:12) (cid:12)≤ √T G
t=1
whereB( )andC( )aretermsthatincreasewiththeleveloflocalizationofthefunction
G G
intheRKHS . ThequantityC( )equalszerointhecaseofconstantthresholdfunctions,
G G
recoveringtheguarantee(6)ofARC.
InSection3weshowcasethesuperiorconditionalriskcontrolpropertiesofL-ARCascompared
toARCforthetaskofelectricitydemandforecasting,tumorsegmentation,andbeamselectionin
wirelessnetworks.
2 LocalizedAdaptiveRiskControl
2.1 Setting
UnliketheARCpredictionset(1),L-ARCadoptspredictionsetsthataredefinedbasedonathreshold
functiong : R. Specifically,ateachtimet 1theL-ARCpredictionsetisobtainedbasedon
t
anon-conformX it→
yscoringfunctions:
≥Ras
X ×Y →
C =C(X ,g ):= y :s(X ,y) g (X ) . (14)
t t t t t t
{ ∈Y ≤ }
By(14),thethresholdg (X )islocalized,i.e.,itisselectedasafunctionofthecurrentinputX . In
t t t
thispaper,weconsiderthresholdfunctionsoftheform
g ()=f ()+c , (15)
t t t
· ·
4wherec Risaconstantandfunctionf ()belongstoareproducingkernelHilbertspace(RKHS)
t t
associa∈
tedtoakernelk(, ):
·Rwithinnerproduct
, andnorm . Notethat
H thethresholdfunctiong ()· be· lonX gs× toX the→ RKHS determinedby⟨ t· he·⟩H kernelk′(, )∥· =∥H k(, )+1.
t
· G · · · ·
Wefocusontheonlinelearningsetting,inwhichateverytimesett 1,themodelobservesaninput
≥
featureX ,producesasetC ,andreceivesasfeedbackthelossL = (C ,Y ). NotethatlabelY
t t t t t t
L
maynotbedirectlyobserved,andonlytheloss (C ,Y )mayberecorded. Basedontheobserved
t t
L
sequenceoffeaturesX andfeedbackL ,weareinterestedinproducingpredictionsetsasin(14)
t t
thatsatisfythereliabilityguarantees(12)and(13),withareweightingfunctionset encompassing
allnon-negativefunctionsw() withapositivemeanE [w(X)]underdistribuW tionP ,i.e.,
X X
· ∈G
= w() : E [w(X)]>0, andw(x) 0forallx . (16)
X
W { · ∈G ≥ ∈X}
Importantly,asdetailedbelow,theleveloflocalizationinguarantee(12)dependsonthechoiceofthe
kernelk(, ).
· ·
2.2 L-ARC
Givenaregularizationparameterλ>0andalearningrateη 1/λ,L-ARCupdatesthethreshold
t
≤
functiong ()=f ()+c in(14)basedontherecursiveformulas
t t t
· ·
c =c η (α L ), (17)
t+1 t t t
− −
f ()=(1 λη )f () η (α L )k(X , ), (18)
t+1 t t t t t
· − · − − ·
withf () = 0andc = 0. Inordertoimplementtheupdate(17)-(18),itisusefultorewritethe
1 1
·
functiong ()as
t+1
·
t
(cid:88)
g ()= ai k(X , )+c , (19)
t+1 · t+1 i · t+1
i=1
wherethecoefficients ai t arerecursivelydefinedas
{ t+1}i=1
at = η (α L ), (20)
t+1 − t − t
ai =(1 η λ)ai, fori=1,2,...,t 1. (21)
t+1 − t t −
Accordingly,ifthelossL islargerthanthelong-termtargetα,theupdaterule(20)-(21)increases
t
the function g () around the current input X , while decreasing it around the previous inputs
t+1 t
·
X ,...,X . Intuitively,thischangeenhancesthereliabilityforinputsintheneighborhoodofX .
1 t−1 t
2.3 TheoreticalGuarantees
Inthissection,weformalizethetheoreticalguaranteesofL-ARC,whichwereinformallystatedin
Section1.3as(12)and(13).
Assumption1(Stationaryandboundedkernel). Thekernelfunctionisstationary,i.e.,k(x,x′)=
k˜( x x′ ), for some non-negative function k˜(), which is ρ-Lipschitz for some ρ > 0, upper
bo∥ und− edb∥
yκ< ,andcoercive,i.e.,lim
k˜(·
z)=0.
z→∞
∞
Manywell-knownstationarykernels,suchastheradialbasisfunction(RBF),Cauchy,andtriangular
kernels,satisfyAssumption1. Thesmoothnessparameterρandthemaximumvalueofthekernel
functionκdeterminethelocalizationofthethresholdfunctiong () . Forexample,thesetof
t
· ∈ G
functions definedin(10)correspondstothefunctionclass(16)associatedwiththeRKHSdefined
bytheraiW sedRBFkernelk(x,x′) = κexp( x x′ 2/l)+1, withlengthscalel = 2e(κ/ρ)2.
−∥ − ∥
AsillustratedinFigure2, byincreasingκandρ, weobtainfunctionswithanincreasinglevelof
localization,rangingfromconstantfunctionstomaximallylocalizedfunctions.
Assumption 2 (Bounded non-conformity scores). The non-conformity scoring function is non-
negativeandbounded,i.e.,s(x,y) S < foranypair(x,y) .
max
≤ ∞ ∈X ×Y
Assumption 3 (Bounded and monotone loss). The loss function is non-negative; bounded, i.e.,
(C,Y) B < foranyC andY ;andmonotonic,inthesensethatforpredictionsets
L C′andC≤ suchth∞ atC′ C,th⊆ eiY nequality∈ (Y C,Y) (C′,Y)holdsforanyY .
⊆ L ≤L ∈Y
52.3.1 StatisticalLocalizedRiskControl
Toprovethelocalizedstatisticalguarantee(12)wewillmakethefollowingassumption.
Assumption 4 (Strictly decreasing loss). For any fixed threshold function g() , the loss
E [ (C(X,g),Y)X =x]isstrictlydecreasinginthethresholdg(x)foranyx · ∈ . G
Y
L | ∈X
Assumption5(Left-continuousloss). Foranyfixedthresholdfunctiong() ,theloss (C(x,g+
h),y)isleft-continuousinh Rforany(x,y) . · ∈G L
∈ ∈X ×Y
Theorem1. Fixauser-definedtargetreliabilityα. Foranyregularizationparameterλ>0andany
learningratesequenceη = η t−1/2 < 1/λ,forsomeη > 0,givenasequence (X ,Y ) T of
t 1 1 { t t }t=1
i.i.d. samplesfromP ,thetime-averagedthresholdfunction(11)satisfiesthelimit
XY
(cid:20) (cid:21)
w(X) p f
limsupE (C(X,g¯ ),Y) α+κB ∥ w ∥H , (22)
X,Y E [w(X)]L T ≤ E [w(X)]
T→∞ X X
foranyweightingfunctionw()=f ()+c wheretheexpectationiswithrespecttothetest
w w
· · ∈W
sample(X,Y).
Proof. SeeAppendixA.
By(22),theaveragelocalizedlossconvergesinprobabilitytoaquantitythatcanbeboundedbythe
targetαwithagapA( ,w)thatincreaseswiththeleveloflocalizationκ.
G
2.3.2 Worst-CaseDeterministicLong-TermRiskControl
Theorem2. Fixauser-definedtargetreliabilityα. Foranyregularizationparameterλ>0andany
learningratesequenceη = η t−1/2 < 1/λwithη > 0,givenanysequence (X ,Y ) T with
boundedinput X Dt < 1 ,L-ARCproducesa1 sequenceofthresholdfunc{ tiont s gt (}t )=1 T in
∥ t ∥ ≤ ∞ { t · }t=1
(19)thatsatisfytheinequality
(cid:12) (cid:12)
(cid:12) (cid:12)1 (cid:88)T
(C(X ,g ),Y )
α(cid:12)
(cid:12)
1 (cid:18) S
max +
4B√ρκD +2B(2κ+1)(cid:19)
+κB. (23)
(cid:12) (cid:12)T L t t t − (cid:12) (cid:12)≤ √T η
0
η 0λ
t=1
Proof. WedefertheprooftoAppendixB.
Formalizingtheupperboundin(13),Theorem2statesthatthedifferencebetweenthelong-term
cumulativeriskandthetargetreliabilitylevelαdecreaseswitharateB( )T−1/2toavalueC( )=
G G
κB thatisincreasingwiththemaximumvalueofthekernelκ. Inthespecialcase, κ = 0which
correspondstonolocalization,theright-handsideof(23)vanishesinT,recoveringARClong-term
guarantee(6).
3 Experiments
Inthissection,weexploretheworst-caselong-termandstatisticallocalizedriskcontrolperformance
ofL-ARCascomparedtoARC.Firstly,weaddressthetaskofelectricitydemandforecasting,utilizing
datafromtheElec2dataset[Harriesetal.,1999]. Next,wepresentanexperimentfocusingontumor
segmentation,wherethedatacomprisesi.i.d. samplesdrawnfromvariousimagedatasets[Jhaetal.,
2020,Bernaletal.,2015,2012,Silvaetal.,2014,Vázquezetal.,2017]. Finally,westudyaproblem
inthedomainofcommunicationengineeringbyfocusingonbeamselection,akeytaskinwireless
systems[Alietal.,2017]. Afurtherexampleconcerningapplicationswithcalibrationconstraints
canbefoundinAppendixC.2. Unlessstatedotherwise,weinstantiateL-ARCwiththeRBFkernel
k(x,x′) = κexp( x x′ 2/l) with κ = 1, length scale l = 1 and regularization parameter
λ=10−4. Withas− m∥ alle− rlen∥ gthscalel,weobtainincreasinglylocalizedweightingfunctions. All
theexperimentsareconductedonaconsumer-gradeMacMiniwithanM1chip. Thesimulationcode
isavailableathttps://github.com/kclip/localized-adaptive-risk-control.git.
60.92 0.25
ARC L-ARCl=1.0 Marginal
L-ARCl=2.0 L-ARCl=0.5 0.20 Mon-Fri
0.91
0.15 Sat-Sun α
0.90 0.10
0.05
0.89
0 10000 20000 30000 40000
0.00
Time-step(t) ARC l=2 l=1 l=0.5
Figure3: Long-termcoverage(left)andaveragemiscoverageerror(right),marginalizedandcon-
ditionedonweekdaysandweekendsforARCandL-ARCwithvaryingvaluesofthelocalization
parameterlontheElec2dataset.
0.125 0.5 0.5
ARC L-ARCl=1.0 Marginal CVC-ClinicDB Marginal CVC-ClinicDB
L-ARCl=2.0 L-ARCl=0.5 Kvasir CVC-ColonDB Kvasir CVC-ColonDB
0.4 CVC-300 ETIS-LaribPolypDB 0.4 CVC-300 ETIS-LaribPolypDB
0.100
0.3 0.3
α
0.075 0.2 0.2
0.1 0.1
0.050
0 500 1000 1500 2000 0.0 0.0
Time-step(t) ARC l=2 l=1 l=0.5 ARC l=2 l=1 l=0.5
Figure4:Long-termFNR(left),averageFNRacrossdifferentdatasources(center),andaveragemask
sizeacrossdifferentdatasources(right)forARCandL-ARCwithvaryingvaluesofthelocalization
parameterlforthetaskoftumorsegmentation[Fanetal.,2020].
3.1 ElectricityDemand
TheElec2datasetcomprisesT = 45312hourlyrecordingsofelectricitydemandsinNewSouth
Wales,Australia. Thedatasequence Y T issubjecttodistributionshiftsduetofluctuationsin
{ t }t=1
demandovertime,suchasbetweendayandnightorbetweenweekdaysandweekends. Weadopt
asetupakintothatofAngelopoulosetal.[2024b],whereintheeven-timedatasamplesareused
foronlinecalibrationwhileodd-timedatasamplesareusedtoevaluatecoverageaftercalibration.
Attimet,theobservedcovariateX correspondstothepasttimeseriesY ,andtheforecasted
t 1:t−1
electricitydemandYˆ isobtainedbasedonamovingaveragecomputedfromdemanddatacollected
t
withinthepreceding24to48hours. WeproducepredictionsetsC basedonthenon-conformity
t
score s(X ,Y ) = Yˆ Y and we target a miscoverage rate α = 0.1 using the miscoverage
t t t t
| − |
loss(3). BothARCandL-ARCusethelearningrateη = t−1/2. L-ARCisinstantiatedwiththe
t
RBFkernelk(x,x′)=κexp( ϕ(x) ϕ(x′) 2/l),whereϕ(x)isa7-dimensionalfeaturevector
−∥ − ∥
correspondingtothedailyaverageelectricitydemandduringthepast7days.
IntheleftpanelofFigure3,wereportthecumulativemiscoverageerrorofARCandL-ARCfor
differentvaluesofthelocalizationparameterl. Allalgorithmsconvergetothedesiredcoveragelevel
of0.9inthelong-term. TherightpanelofFigure3,displaystheaveragemiscoverageerroronthe
hold-outdatasetatconvergence. Wespecificallyevaluateboththemarginalizedmiscoveragerateand
theconditionalmiscoveragerateseparatelyoverweekdaysandweekends. L-ARCisshowntoreduce
theweekendcoverageerrorrateascomparedtoARCprovidingbalancedcoverageasthelengthscale
ldecreases.
3.2 TumorImageSegmentation
Inthissection,wefocusonthetaskofcalibratingapredictivemodelfortumorsegmentation. Here,
thefeaturevectorX representsad d image,whilethelabelY identifiesasubsetofthe
t H W t
× ⊆P
imagepixels = (1,1),...,(d ,d ) thatencompassesthetumorregion. AsinAngelopoulos
H W
P { }
etal.[2022],thedatasetisacompilationofsamplesfromseveralopen-sourceonlinerepositories:
Kvasir,CVC-300,CVC-ColonDB,CVC-ClinicDB,andETIS-LaribDB.Wereserve50samplesfrom
eachrepositoryfortestingtheperformancepost-calibration,whiletheremainingT =2098samples
7
RNFmret-gnoL
egarevoCmret-gnoL
rorrERNFegarevA
rorrEegarevocsiMegarevA
eziSksaMegarevAare used for online calibration. Predicted sets are obtained by applying a threshold g(X ) to the
t
pixel-wiselogitsf(p ,p )generatedbythePraNetsegmentationmodel[Fanetal.,2020],withthe
H W
objectiveofcontrollingthefalsenegativeratio(FNR) (C ,Y )=1 C Y /Y . BothARCand
t t t t t
L −| ∩ | | |
L-ARCarerunusingthesamedecayinglearningrateη =0.1t−1/2. L-ARCisinstantiatedwiththe
t
RBFkernelk(x,x′)=κexp( ϕ(x) ϕ(x′) 2/l),whereϕ(x)isa5-dimensionalfeaturevector
−∥ − ∥
obtainedviatheprincipalcomponentanalysis(PCA)fromthelasthiddenlayeroftheResNetmodel
usedinPraNet.
IntheleftmostpanelofFigure4,wereportthelong-termFNRforvaryingvaluesofthelocalization
parameter l, targeting an FNR level α = 0.1. All methods converge rapidly to the desired FNR
level,ensuringlong-termriskcontrol. Thecalibratedmodelsarethentestedonthehold-outdata,
andtheFNRandaveragepredictedsetsizeareseparatelyevaluatedacrossdifferentrepositories. In
themiddleandrightpanelsofFigure4,wereporttheaverageFNRandaveragepredictionsetsize
averagedover10trials.
ThemodelcalibratedviaARChasamarginalizedFNRerrorlargerthanthetargetvalueα. Moreover,
theFNRerrorisunevenlydistributedacrossthedifferentdatarepositories,rangingfromFNR=0.08
forCVC-300toFNR=0.32forETIS-LaribPolypDB.Incontrast,L-ARCcanequalizeperformance
across repositories, while also achieving a test FNR closer to the target level. In particular, as
illustratedintherightmostpanel,L-ARCimprovestheFNRforthemostchallengingsubpopulation
in the data by increasing the associated prediction set size, while maintaining a similar size for
subpopulationsthatalreadyhavesatisfactoryperformance.
3.3 BeamSelection
0.100 ARC MondrianARC L-ARC
ARC
0.095 L-ARC 60 −25
0.090 Mondr.ARC 40 −30
0 5000 10000 15000 20000 35 Timestep(t) 20 − 2.0
40
0 −
1.5 45
1.0
−20
50
Trans 0mitter
50 50
Trans 0mitter
50 50
Trans 0mitter
50
−−
50
0.1 0.2 0.3 0.4
− px − px − px
TargetRisk(L∗)
Figure5: Long-termrisk(left-top),averagebeamsetsize(left-bottom),andSNRlevelacrossthe
deploymentarea(right)forARC,MondrianARC,andL-ARC.Thetransmitterisdenotedasagreen
circleandobstaclestopropagationareshownasgreyrectangles.
Motivatedbytheimportanceofreliableuncertaintyquantificationinengineeringapplications,we
addressthetaskofselectinglocation-specificbeamsfortheinitialaccessprocedureinsixth-generation
wirelessnetworks[Alietal.,2017]. Furtherdetailsregardingtheengineeringaspectsoftheproblem
and the simulation scenario are provided in Appendix C.1.1. In the beam selection task, at each
time t, the observed covariate corresponds to the location X = [p ,p ] of a receiver within the
t x y
networkdeployment,wherep andp representthegeographicalcoordinates. Basedontheobserved
x y
covariate,thetransmitterchoosesaset,denotedasC [1, ,B ],consistingofasubsetofthe
t max
⊆ ···
B availablecommunicationbeams.
max
Eachcommunicationbeamiisassociatedwithawirelesslinkcharacterizedbyasignal-to-noise
ratioY ,whichfollowsanunknowndistributiondependingontheuser’slocationX . Werepresent
t,i t
thevectorofsignal-to-noiseratiosasY
t
=[Y t,1,...,Y t,Bmax] ∈RBmax. ForasetC t,thetransmitter
sweepsoverthebeamsetC ,andtheperformanceismeasuredbytheratiobetweentheSNRobtained
t
onthebestbeaminsetC andthebestSNRonallthebeams,i.e.,
t
max Y
(C ,Y )=L =1 i∈Ct t,i . (24)
L t t t − max Y
i∈{1,...,Bmax} t,i
GivenanSNRpredictorYˆ =f (X )forallbeamsatlocationX ,weconsidersetsthatinclude
t SNR t t
onlybeamswithapredictedSNRexceedingathresholdg (X )as
t t
C(X ,g )= i [1,...,B ]:Yˆ >g (X ) . (25)
t t max t,i t t
{ ∈ }
8
ksiRmret-gnoL
eziSteSmaeB.gvA yp RNSInthissetting,localizationreferstothefairprovisionofserviceacrosstheentiredeploymentarea.
Asabenchmark,wethusalsoconsideranadditionalcalibrationstrategythatdividesthedeployment
areaintotworegions: oneencompassingalllocationsnearthetransmitter,whicharemorelikelyto
experiencehighSNRlevels,andtheotherincludinglocationsfarfromthetransmitter. Foreachof
theseregions,weruntwoseparateinstancesofARCalgorithms. Inspiredbythemethodintroduced
inBoströmetal.[2021]forofflinesettings,werefertothisbaselineapproachasMondrianARC.
IntheleftpanelsofFigure5,wecomparetheperformanceofARC,MondrianARC,andL-ARC
with an RBF kernel with l = 10, using a calibration data sequence of length T = 25000. All
methodsachievethetargetlong-termSNRregret,butL-ARCachievesthisresultwhileselecting
setswithsmallersizes,thusrequiringlesstimeforbeamsweeping. Additionally,asillustratedon
therightpanel,thankstothelocalizationofthethresholdfunction,L-ARCensuresasatisfactory
communicationSNRlevelacrosstheentiredeploymentarea. Incontrast,bothARCandMondrian
ARCproducebeam-sweepsetswithunevenguaranteesoverthenetworkdeploymentarea.
4 RelatedWork
Our work contributes to the field of adaptive conformal prediction (CP), originally introduced
by Gibbs and Candes [2021]. Adaptive CP extends traditional CP [Vovk et al., 2005] to online
settings,wheredataisnon-exchangeableandmaybeaffectedbydistributionshifts. Thisextension
hasfoundapplicationsinreliabletime-seriesforecasting[XuandXie,2021,Zaffranetal.,2022],
control[Lekeufacketal.,2023,Angelopoulosetal.,2024a],andoptimization[Zhangetal.,2023,
Deshpandeetal.,2024]. AdaptiveCPensuresthatpredictionsetsgeneratedbythealgorithmcontain
theresponsevariablewithauser-definedcoveragelevelonaverageacrosstheentiretimehorizon.
Recently,Bhatnagaretal.[2023]proposedavariantofadaptiveCPbasedonstronglyadaptiveonline
learning,providingcoverageguaranteesforanysubsequenceofthedatastream. Whiletheirapproach
offers localized guarantees in time, L-ARC provides localized guarantees in the covariate space.
Moresimilartoourworkis[Bastanietal.,2022],whichstudiesgroup-conditionalcoverage. Our
workextendsbeyondcoverageguaranteestoamoregeneralriskdefinition,akintoFeldmanetal.
[2022]. Angelopoulosetal.[2024a]studiedtheasymptoticcoveragepropertiesofadaptiveconformal
predictionsinthei.i.d. setting; andourworkextendstheseresultstoencompasscovariateshifts.
Finally,theguaranteeprovidedbyL-ARCissimilartothatofGibbsetal.[2023],albeitforanoffline
conformalpredictionsetting.
5 ConclusionandLimitations
WehavepresentedandanalyzedL-ARC,avariantofadaptiveriskcontrolthatproducesprediction
sets based on a threshold function mapping covariate information to localized threshold values.
L-ARCcanguaranteebothworst-casedeterministiclong-termriskcontrolandstatisticallocalized
riskcontrol. EmpiricalanalysisdemonstratesL-ARC’sabilitytoeffectivelycontrolriskfordifferent
taskswhileprovidingpredictionsetsthatexhibitconsistentperformanceacrossvariousdatasub-
populations. TheeffectivenessofL-ARCiscontingentuponselectinganappropriatekernelfunction.
Furthermore,L-ARChasmemoryrequirementsthatgrowwithtimeduetotheneedtostoretheinput
data X andcoefficients(20)-(21). TheselimitationsofL-ARCmotivatefutureworkaimed
t t>1
{ }
atoptimizingonlinethekernelfunctionbasedonhold-outdata[Kiyanietal.,2024]orinanonline
manner[Angelopoulosetal.,2024a],andatstudyingthestatisticalguaranteesofmemory-efficient
variantsofL-ARC[Kivinenetal.,2004].
6 Acknowledgments
ThisworkwassupportedbytheEuropeanUnion’sHorizonEuropeprojectCENTRIC(101096379).
The work of Osvaldo Simeone was also supported by the Open Fellowships of the EPSRC
(EP/W024101/1) by the EPSRC project (EP/X011852/1), and by Project REASON, a UK Gov-
ernmentfundedprojectundertheFutureOpenNetworksResearchChallenge(FONRC)sponsored
bytheDepartmentofScienceInnovationandTechnology(DSIT).
9References
AnumAli,NuriaGonzález-Prelcic,andRobertWHeath. Millimeterwavebeam-selectionusingout-
of-bandspatialinformation. IEEETransactionsonWirelessCommunications,17(2):1038–1052,
2017.
AnastasiosAngelopoulos,EmmanuelCandes,andRyanJTibshirani. ConformalPIDcontrolfor
timeseriesprediction. AdvancesinNeuralInformationProcessingSystems,36,2024a.
AnastasiosNAngelopoulos,StephenBates,AdamFisch,LihuaLei,andTalSchuster. Conformal
riskcontrol. arXivpreprintarXiv:2208.02814,2022.
AnastasiosNAngelopoulos,RinaFoygelBarber,andStephenBates. Onlineconformalprediction
withdecayingstepsizes. arXivpreprintarXiv:2402.01139,2024b.
OsbertBastani,VarunGupta,ChristopherJung,GeorgyNoarov,RamyaRamalingam,andAaron
Roth. Practical adversarial multivalid conformal prediction. Advances in Neural Information
ProcessingSystems,35:29362–29373,2022.
JorgeBernal, JavierSánchez, andFernandoVilarino. Towardsautomaticpolypdetectionwitha
polypappearancemodel. PatternRecognition,45(9):3166–3182,2012.
JorgeBernal,FJavierSánchez,GloriaFernández-Esparrach,DeboraGil,CristinaRodríguez,and
FernandoVilariño. WM-DOVAmapsforaccuratepolyphighlightingincolonoscopy: Validation
vs.saliencymapsfromphysicians. Computerizedmedicalimagingandgraphics,43:99–111,2015.
AadyotBhatnagar,HuanWang,CaimingXiong,andYuBai. Improvedonlineconformalprediction
viastronglyadaptiveonlinelearning. InInternationalConferenceonMachineLearning,pages
2337–2363.PMLR,2023.
HenrikBoström,UlfJohansson,andTuweLöfström. Mondrianconformalpredictivedistributions.
InConformalandProbabilisticPredictionandApplications,pages24–38.PMLR,2021.
NicoloCesa-Bianchi,AlexConconi,andClaudioGentile. Onthegeneralizationabilityofon-line
learningalgorithms. IEEETransactionsonInformationTheory,50(9):2050–2057,2004.
Shachi Deshpande, Charles Marx, and Volodymyr Kuleshov. Online calibrated and conformal
predictionimprovesbayesianoptimization. InInternationalConferenceonArtificialIntelligence
andStatistics,pages1450–1458.PMLR,2024.
Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu Fu, Jianbing Shen, and Ling Shao.
Pranet: Parallelreverseattentionnetworkforpolypsegmentation. InInternationalconferenceon
medicalimagecomputingandcomputer-assistedintervention,pages263–273.Springer,2020.
ShaiFeldman,LiranRingel,StephenBates,andYanivRomano. Achievingriskcontrolinonline
learningsettings. arXivpreprintarXiv:2205.09095,2022.
RinaFoygelBarber,EmmanuelJCandes,AadityaRamdas,andRyanJTibshirani. Thelimitsof
distribution-freeconditionalpredictiveinference. InformationandInference: AJournalofthe
IMA,10(2):455–482,2021.
IsaacGibbsandEmmanuelCandes. Adaptiveconformalinferenceunderdistributionshift. Advances
inNeuralInformationProcessingSystems,34:1660–1672,2021.
Isaac Gibbs, John J Cherian, and Emmanuel J Candès. Conformal prediction with conditional
guarantees. arXivpreprintarXiv:2305.12616,2023.
AndreaGoldsmith. Wirelesscommunications. Cambridgeuniversitypress,2005.
MichaelHarries,NewSouthWales,etal. Splice-2comparativeevaluation: Electricitypricing. 1999.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages770–778,2016.
10Jakob Hoydis, Fayçal Aït Aoudia, Sebastian Cammerer, Merlin Nimier-David, Nikolaus Binder,
GuillermoMarcus,andAlexanderKeller. SionnaRT:Differentiableraytracingforradiopropaga-
tionmodeling. arXivpreprintarXiv:2303.11103,2023.
DebeshJha,PiaHSmedsrud,MichaelARiegler,PålHalvorsen,ThomasDeLange,DagJohansen,
andHåvardDJohansen. Kvasir-seg: Asegmentedpolypdataset. InMultiMediaModeling: 26th
InternationalConference,MMM2020,Daejeon,SouthKorea,January5–8,2020,Proceedings,
PartII26,pages451–462.Springer,2020.
JyrkiKivinen,AlexanderJSmola,andRobertCWilliamson. Onlinelearningwithkernels. IEEE
transactionsonsignalprocessing,52(8):2165–2176,2004.
ShayanKiyani,GeorgePappas,andHamedHassani. Conformalpredictionwithlearnedfeatures.
arXivpreprintarXiv:2404.17487,2024.
JordanLekeufack,AnastasiosAAngelopoulos,AndreaBajcsy,MichaelIJordan,andJitendraMalik.
Conformaldecisiontheory: Safeautonomousdecisionsfromimperfectpredictions. arXivpreprint
arXiv:2310.05921,2023.
HoreaMuresanandMihaiOltean. Fruitrecognitionfromimagesusingdeeplearning. ActaUniversi-
tatisSapientiae,Informatica,10(1):26–42,2018.
JuanSilva,AymericHistace,OlivierRomain,XavierDray,andBertrandGranado. Towardembedded
detectionofpolypsinwceimagesforearlydiagnosisofcolorectalcancer. Internationaljournalof
computerassistedradiologyandsurgery,9:283–293,2014.
DavidVázquez,JorgeBernal,FJavierSánchez,GloriaFernández-Esparrach,AntonioMLópez,
AdrianaRomero,MichalDrozdzal,andAaronCourville. Abenchmarkforendoluminalscene
segmentationofcolonoscopyimages. Journalofhealthcareengineering,2017,2017.
Vladimir Vovk. Conditional validity of inductive conformal predictors. In Asian conference on
machinelearning,pages475–490.PMLR,2012.
VladimirVovk,AlexanderGammerman,andGlennShafer. Algorithmiclearninginarandomworld,
volume29. Springer,2005.
WojciechWisniewski,DavidLindsay,andSianLindsay. Applicationofconformalpredictioninterval
estimations to market makers’ net positions. In Conformal and probabilistic prediction and
applications,pages285–301.PMLR,2020.
Chen Xu and Yao Xie. Conformal prediction interval for dynamic time-series. In International
ConferenceonMachineLearning,pages11559–11569.PMLR,2021.
Lequan Yu, Hao Chen, Qi Dou, Jing Qin, and Pheng Ann Heng. Integrating online and offline
three-dimensional deep learning for automated polyp detection in colonoscopy videos. IEEE
journalofbiomedicalandhealthinformatics,21(1):65–75,2016.
Margaux Zaffran, Olivier Féron, Yannig Goude, Julie Josse, and Aymeric Dieuleveut. Adaptive
conformalpredictionsfortimeseries. InInternationalConferenceonMachineLearning,pages
25834–25866.PMLR,2022.
MatteoZecchin,SangwooPark,andOsvaldoSimeone. Forkinguncertainties: Reliableprediction
andmodelpredictivecontrolwithsequencemodelsviaconformalriskcontrol. IEEEJournalon
SelectedAreasinInformationTheory,2024.
Lujing Zhang, Aaron Roth, and Linjun Zhang. Fair risk control: A generalized framework for
calibratingmulti-groupfairnessrisks. arXivpreprintarXiv:2405.02225,2024.
YunchuanZhang,SangwooPark,andOsvaldoSimeone. Bayesianoptimizationwithformalsafety
guaranteesviaonlineconformalprediction. arXivpreprintarXiv:2306.17815,2023.
11A ProofofTheorem1
Weareinterestedinboundingthelocalizedriskin(22)ofthethresholdfunction(11)forallweighting
functionsinset definedin(16). Tostudythelimitin(22)wefirstnotethatL-ARCupdaterule(18)
W
correspondstoanonlinegradientdescentstepforalossfunctionℓ(g,x,y),withrespecttofunction
f()andconstantcinfunctiong()=f()+casin(15). Inparticular,interpretingtheupdaterule
· · ·
(17)-(18)asagradientdescentstep,weobtainthatthepartialderivativesofthelossfunctionℓ(g,x,y)
evaluatedatg()=f()+care
· ·
∂ℓ(g,x,y)
ℓ(g)= ()=(α (C(x,g),y))k(x, )+λf() , (26)
∇f ∂f · −L · · ∈H
∂ℓ(g,x,y)
ℓ(g)= =(α (C(x,g),y)) R, (27)
∇c ∂c −L ∈
sothatthefirstorderapproximationofthelossℓ(g,x,y)aroundg()isgivenby
·
ℓ(g+ϵδ ,x,y) ℓ(g,x,y)+ϵ(α (C(x,g),y)) K ,δ +ϵ f,δ (28)
f x f f
≈ −L ⟨ ⟩ ⟨ ⟩
ℓ(g+ϵδ ,x,y) ℓ(g,x,y)+ϵ(α (C(x,g),y))δ . (29)
c c
≈ −L
Inordertostudytheconvexityofthelossℓ(g,x,y)ing(),wecomputethethederivativesof(26)-
·
(27)withrespecttof()andc. Thederivativeof(26)withrespecttof istheoperatorA:
· H→H
satisfying
ℓ(g+ϵδ ) ℓ(g)
Aδ = lim ∇f f −∇f
f ϵ→0 ϵ
(C(x,g),y) (C(x,g+ϵδ ),y)
= lim L −L f K +λδ
−ϵ→0 ϵ x f
(cid:68)∂ (C(x,g),y)∂g(x) (cid:69)
= L ,δ K +λδ
− ∂g(x) ∂f f x f
∂ (C(x,g),y)
= L K ,δ K +λδ . (30)
− ∂g(x) ⟨ x f ⟩ x f
Itfollowsthat
∂ (C(x,g),y)
f,Af = L f(x)2+λ f 2 . (31)
⟨ ⟩ − ∂g(x) ∥ ∥H
Similarly,thederivativeof(26)withrespecttocistheoperatorB :R isgivenby
→H
∂ (C(x,g),y)
Bc= L K c, (32)
− ∂g(x) x
whichsatisfies
∂ (C(x,g),y)
f,Bc = L f(x)c. (33)
⟨ ⟩ − ∂g(x)
Thederivativeof(27)withrespecttocisgivenby
∂ (C(x,g),y)
Dc= L c, (34)
− ∂g(x)
andthederivativewithrespecttotof istheoperatorC : givenby
H→R
∂ (C(x,g),y)
Cδ = L K ,δ , (35)
f − ∂g(x) ⟨ x f ⟩
sothat
∂ (C(x,g),y)
c,Cf = L f(x)c. (36)
⟨ ⟩ − ∂g(x)
12(cid:104) (cid:105)
FromAssumption4, theinequalityL′ = E ∂L(C(x,g),Y) X =x γ > 0holds. Thus, the
− Y ∂g(x) | ≥
second-ordertermoftheapproximationofE [ℓ(g,X,Y)X =x]aroundg()=0satisfies
Y
| · ̸
(cid:34) (cid:20) A B(cid:21)(cid:20) f(cid:21)(cid:12) (cid:12) (cid:35)
E [f c] (cid:12)X =x =E [ f,Af + f,Bc + c,Cf + c,Dc X =x]
Y C D c (cid:12) (cid:12) Y ⟨ ⟩ ⟨ ⟩ ⟨ ⟩ ⟨ ⟩|
=L′f(x)2+2L′f(x)c+L′c2+λ f 2
∥ ∥H
=L′(f(x)+c)2+λ f 2 >0. (37)
∥ ∥H
WethenconcludethatthelossfunctionE [ℓ(g,X,Y)X =x]isstronglyconvexing(),andthat
Y
| ·
thepopulationlossminimizer
g∗()=f∗()+c∗ =argminE [ℓ(g,X,Y)] (38)
X,Y
· ·
g∈G
isunique. Foranycovariateshiftw() denoteitscomponentsf () andc Rsuchthat
w w
· ∈W · ∈H ∈
w()=f ()+c . Fromthefirstorderoptimalityconditions,itholdsthatthedirectionalderivatives
w w
· ·
withrespecttof ()andc mustsatisfy
w w
·
E [ ℓ(g∗+ϵf ,X,Y) ]=E [(α (C (X,g∗),Y))f (X)+λ f ,f∗ ]=0,
X,Y ϵ w ϵ=0 X,Y t w w H
∇ | −L ⟨ ⟩
(39)
E [ ℓ(g∗+ϵc ,X,Y) ]=E [(α (C (X,g∗),Y))c ]=0, (40)
X,Y ϵ w ϵ=0 X,Y t w
∇ | −L
whichimpliesthatfortheoptimalsolutiong∗()
·
(cid:20) (cid:21) (cid:28) (cid:29)
w(X) f
E (C(X,g∗),Y) =α+λ f∗, w . (41)
X,Y E [w(X)]L E [w(X)]
X X H
Equality(41)amountstoalocalizedriskcontrolguaranteeforthethresholdg∗()forcovariateshift
·
inw() . Thefollowinglemmastatesthatthetime-averageL-ARCthresholdfunctiong¯ ()
T
define· di∈ n(W 11)convergestothepopulationriskminimizerg∗(). ·
·
Lemma1. Foranyregularizationparameterλ>0andanylearningratesequenceη =η t−1/2 <
t 1
1/λ,forsomeη >0,givenasequence (X ,Y ) T ofi.i.d. samplesfromP ,thetime-averaged
1 { t t }t=1 XY
thresholdfunction(11)satisfiesforanyϵ>0
lim Pr[ g∗ g¯ ϵ]=0 (42)
T→∞ ∥ − T ∥∞ ≥
Proof. Toproveconvergenceinprobability, weneedtoshowthatthelossfunctionℓ(g,X,Y)is
bounded. Tothisend,wefirstshowthatℓ(g,X,Y)isLipschitzing()bystudyingthenormofthe
·
derivatives(26)-(27). Forg ()=f ()+c returnedbytheupdaterule(18),thegradientwithrespect
t t t
· ·
tof ()satisfies
t
·
(cid:13) (cid:13)
(cid:13)
(cid:13)
(cid:13)∂ℓ(g ∂t, fx,y)
(
·)(cid:13)
(cid:13)
(cid:13)
= ∥(α −L(C(x,g t),y)k(x, ·)+λf t( ·)
∥H
H
B√κ+λ f () 2B√κ, (43)
≤ ∥ t · ∥H ≤
where the first inequality follows from the boundedness on the kernel (Assumption 1) and the
boundednessontheloss(Assumption3),whilethelastfollowsfromProposition1. Thegradient
withrespecttoccanbesimilarlyboundedas
(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)∂ℓ(g ∂t, cx,y)
(
·)(cid:12)
(cid:12) (cid:12)= |(α −L(C(x,g t),y) |≤B. (44)
Fromthemeanvaluetheoremitfollowsthatforg()=f()+candg′()=f′()+c′
· · · ·
(cid:16) (cid:17)
ℓ(g,X,Y) ℓ(g′,X,Y) 2B√k+B f f′ +B (c c′). (45)
| − |≤ ∥ − ∥H | − |
SinceL-ARCreturnsfunctionsf ()withboundedRKHSnormandinfinitynorm(Proposition1),
t
·
andthresholdsfunctiong ()withboundedininfinitynorm(Proposition3),weconcludethatthere
t
·
existsafiniteℓ < suchthat ℓ(g,X,Y) ℓ . Giventhatthelossisboundedwecanapply
max max
∞ | |≤
13[Kivinenetal.,2004,Theorem4]andobtainthatforthethreshold g () returnedbyL-ARC
t t≥1
andthepopulationlossminimizerg∗()itholds { · }
·
T T (cid:18) (cid:18) (cid:19) (cid:19)
1 (cid:88) 1 (cid:88) 2 1 1
ℓ(g ,X ,Y ) ℓ(g∗,X ,Y )+B2κ2(2κ2+1)2 2η + + .
T t t t ≤ T t t √T 0 η 0λ2 2η 0λ2T
t=1 t=1
(46)
By Hoeffding’s inequality the empirical average on the right-hand side of (46) converges to its
expectedvalue. Formally,wehavethatwithprobabilityatleast1 δwithrespecttothesequence
(X ,Y ) T −
{ t t }t=1
(cid:12) (cid:12) (cid:115)
(cid:12)1 (cid:88)T (cid:12) 2 (cid:18) 1(cid:19)
(cid:12) ℓ(g∗,X ,Y ) E [ℓ(g∗,X,Y)](cid:12) ℓ log . (47)
(cid:12) (cid:12)T t t − X,Y (cid:12) (cid:12)≤ max T δ
t=1
Similarly,by[Cesa-Bianchietal.,2004,Theorem2]theempiricalriskontheleft-handsideof(46)
convergestothepopulationriskofthetime-averagedsolution(11). Withprobabilityatleast1 δ
withrespecttothesequenceofsamples (X ,Y ) T ,itholds −
{ t t }t=1
(cid:115)
T (cid:18) (cid:19)
1 (cid:88) 2 1
E [ℓ(g¯ ,X,Y)] ℓ(g ,X ,Y )+ℓ log (48)
X,Y T ≤ T t t t max T δ
t=1
Combiningthetwoinequalities,withprobabilityatleast1 2δwithrespectto (X ,Y ) T ,
− { t t }t=1
(cid:18) (cid:18) (cid:19) (cid:19)
2 1 1
E [ℓ(g¯ ,X,Y) ℓ(g∗,X,Y)] B2κ2(2κ2+1)2 2η + +
X,Y T − ≤ √T 0 η 0λ2 2η 0λ2T
(cid:115)
(cid:18) (cid:19)
2 1
+2ℓ log (49)
max T δ
SincetheE [ℓ(g,X,Y)]isstronglyconvexthereexistsavalueγ >0suchthatthesecondorder
X,Y
approximationofE [ℓ(g,X,Y)]atg∗()satisfies
X,Y
·
γ
2
(cid:0)(cid:13) (cid:13)f∗ −f¯ T(cid:13) (cid:13) H+(c∗ −c¯ T)(cid:1)2 ≤E X,Y[ℓ(g¯ T,X,Y) −ℓ(g∗,X,Y)] (50)
Combining(50)and(49),andleveraging f √κ f ,whichfollowsfromtheAssumption1,
∥ ∥∞ ≤ ∥ ∥H
weconcludethatwithprobability1 2δ
−
(cid:118)
(cid:117) (cid:117)2B2κ2(2κ2+1)2(cid:18)
2
(cid:18)
1
(cid:19)
1
(cid:19)
4ℓ
(cid:115)
2
(cid:18) 1(cid:19)
g∗ g¯ (cid:116) 2η + + + max log .
∥ − T ∥∞ ≤ γ(κ+1) √T 0 η 0λ2 2η 0λ2T γ(κ+1) T δ
(51)
Choosingδ = 1,foranyϵ>0,itholds
T
lim Pr[ g∗ g¯ ϵ]=0. (52)
T→∞ ∥ − T ∥∞ ≥
Byitself,theconvergenceofthethresholdfunctiong¯ ()tothepopulationriskminimizerg∗()isnot
T
· ·
sufficienttoprovidelocalizedriskcontrolguaranteesforL-ARCtime-averagedsolution. However,
under the additional loss regularity assumption in Assumption 5, we can show that set predictor
C(X,g¯ )enjoysconditionalriskcontrolforT .
T
→∞
Havingassumedthattheloss (C(x,g),y)isleft-continuousanddecreasingforlargerpredictionsets
(Assumption3and5),foranyL δ′ >0thereexistsϵ>0suchthatforg()suchthat g∗ g ϵit
· ∥ − ∥∞ ≤
holds
(C(X,g),Y) (C(X,g∗),Y)+δ′. (53)
L ≤L
Forsuchg()thefollowinginequalityholds
·
(cid:20) (cid:21) (cid:20) (cid:21)
w(X) w(X)
maxE (C(X,g),Y) maxE (C(X,g∗),Y) +δ′. (54)
w∈W E[w(X)]L ≤w∈W E[w(X)]L
14AsstatedinLemma1,wecanalwaysfindT largeenough,suchthat g∗ g¯ ϵwitharbitrary
largeprobability. Thisimplies,thatforanyδ′ >0, ∥ − T ∥∞ ≤
(cid:20) (cid:21) (cid:20) (cid:21)
w(X) w(X)
lim maxE (C(X,g¯ ),Y) maxE (C(X,g∗),Y) +δ′ (55)
T→∞w∈W E[w(X)]L T ≤w∈W E[w(X)]L
(cid:28) (cid:29)
f
α+λ f∗, w +δ′ (56)
≤ E [w(X)]
X H
f
α+κB ∥ w ∥H +δ′, (57)
≤ E [w(X)]
X
wheretheinequality(55)followsfrom(54),theinequality(56)followsfrom(41)andtheinequality
(57)fromProposition1.
B ProofofTheorem2
Weareinterestedinboundingtheabsolutedifferencebetweenthecumulativelossvalueincurredby
thesetpredictors C(g ,X ) T producedbyL-ARC(18)andthetargetreliabilitylevelα,i.e.,
{ t t }t=1
(cid:12) (cid:12)
(cid:12) T (cid:12)
(cid:12)1 (cid:88) (cid:12)
(cid:12)
(cid:12)T
( L(C t,Y t) −α)(cid:12) (cid:12). (58)
(cid:12) t=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:12)
Lt
FromAssumption1andhavingassumed X Dfort 1,itfollowsthat
t
∥ ∥≤ ≥
lim k(X ,x)=0. (59)
t
∥x∥→∞
Aboundonthecumulativeriskcanthenbeobtainedbybounding
(cid:13) (cid:13)
(cid:13)1 (cid:88)T (cid:13)
(cid:13) (L α)(k(X , )+1)(cid:13) , (60)
(cid:13) (cid:13)T t − t · (cid:13) (cid:13)
t=1 ∞
whereforafunctionf : R,theinfinitynorm f isdefinedasmax f(x). Infact,from
X → ∥ ∥∞ x∈X | |
(60)wedirectlyobtainaboundonthecumulativerisk
(cid:12) (cid:12) (cid:12) (cid:12) (cid:13) (cid:13)
(cid:12)1 (cid:88)T (cid:12) (cid:12)1 (cid:88)T (cid:12) (cid:13)1 (cid:88)T (cid:13)
(cid:12) (L α)(cid:12)= lim (cid:12) (L α)(k(X ,x)+1)(cid:12) (cid:13) (L α)(k(X , )+1)(cid:13) .
(cid:12) (cid:12)T t − (cid:12) (cid:12) ∥x∥→∞(cid:12) (cid:12)T t − t (cid:12) (cid:12)≤(cid:13) (cid:13)T t − t · (cid:13) (cid:13)
t=1 t=1 t=1 ∞
(61)
Tothisend,wefirstnotethatfunctions f t() t∈Ngeneratedby(18)haveboundedRKHSnormand
{ · }
aresmooth.
√
Proposition1. Foreveryt 1,wehavetheinequalities f B κ and f κB.
≥ ∥ t ∥H ≤ λ ∥ t ∥∞ ≤ λ
Proof. Theproofisbyinduction,withthebasecase f () B√κ/λbeingsatisfiedasf ()=0.
∥ 1 · ∥H ≤ 1 ·
Theinductionstepisgivenas
f = (1 λη )f η (α L )k(x , ) (62)
∥ t+1 ∥H ∥ − t t − t − t t · ∥H
(1 λη )f + η (α L )k(x , ) (63)
≤∥ − t t ∥H ∥ t − t t · ∥H
(1 λη ) f +η B√κ (64)
≤ − t ∥ t ∥H t
B√κ
, (65)
≤ λ
wheretheequality(62)followsfromtheupdaterule(18);theinequality(63)fromthepropertiesof
thenorm,theinequality(64)fromAssumption1and3,andtheinequality(65)fromtheinduction
√
hypothesis f B κ.
∥ t ∥H ≤ λ
Proposition2. Fort 1andany(x,x′) wehave
≥ ∈X ×X
B√2ρκD
f(x) f(x′) . (66)
| − |≤ λ
15Proof. DenotetheevaluationfunctionatxasK =k(x, ). FromProposition1andtheLipschitz
x
·
continuityassumedinAssumption1,itfollowsthat
f(x) f(y) = f,K f,K
x H y H
| − | |⟨ ⟩ −⟨ ⟩ |
= f,K K
x y H
|⟨ − ⟩ |
f K K
≤∥ ∥H∥ x − y ∥H
(cid:112)
= f k(x,x)+k(y,y) 2k(x,y)
∥ ∥H −
(cid:112)
f 2ρ x y
≤∥ ∥H ∥ − ∥
2B√ρκD
(67)
≤ λ
wherethefirstinequalityfollowsfromCauchy–Schwarzinequality,thesecondfromtheLipschitz
continuityofthekernel,andthelastonefromProposition1togetherwith x Dforx .
∥ ∥≤ ∈X
Leveragingtheabovecharacterizationofthefunctionf ()returnedbyL-ARC,wenowshowthat
t
·
thethresholdfunctiong ()hasmaximumandminimumvaluesthatareuniformlybounded.
t
·
Proposition3. Foreveryt 1andx wehaveg (x) [G ,G ]with
t min max
≥ ∈X ∈
2B√ρκD
G =S + +η B(2κ+1) (68)
max max λ 0
and
2B√ρκD
G = η B(2κ+1). (69)
min − λ − 0
Proof. Wenowprovetheupperbound(68). Theproofisbycontradictionanditstartbyassuming
thatthereexistsat>1andx suchthatg (x) G whileg ()<G forallt′ <t. From
t max t′ max
∈X ≥ ·
theupdaterule(18)wehavethat
g (x)=g (x)+η (α L )(k(X ,x)+1) λη f (x)
t−1 t t−1 t t−1 t−1 t−1
− −
G η B(κ+1) λη f (x)
max 0 0 t−1
≥ − − | |
G η B(2κ+1). (70)
max 0
≥ −
FromProposition2wealsohave
2B√ρκD 2B√ρκD
g (X ) g (x) G η B(2κ+1) S , (71)
t−1 t−1 ≥ t−1 − λ ≥ max − 0 − λ ≥ max
wherethelastinequalityfollowsfromG beingdefinedas(68). FromAssumption2,forallx ,
max
∈X
g (X ) S = α L = g (x) (1 λη )g (x) G , (72)
t−1 t−1 max t−1 t t−1 t−1 max
≥ ⇒ ≥ ⇒ ≤ − ≤
whichcontradictswiththeoriginalassumptionthatthereexistsxsuchthatg (x) G .
t max
≥
Theproofofthelowerbound(69)followssimilarly. Assumethereexistst>1andx suchthat
g (x) G whileg ()>G fort′ <t. Fromtheupdaterule(18)wehavethat ∈X
t min t′ min
≤ ·
g (x)=g (x)+η (α L )(k(X ,x)+1) λη f (x)
t−1 t t−1 t t−1 t−1 t−1
− −
G +η B(κ+1)+λη f (x)
min 0 0 t−1
≤ | |
G +η B(2κ+1) (73)
min 0
≤
FromProposition2wealsohave
2B√ρκD 2B√ρκD
g (X ) g (x)+ G +η B(2κ+1)+ 0 (74)
t−1 t−1 ≤ t−1 λ ≤ min 0 λ ≤
wherethelastinequalityfollowsfromG beingdefinedas(69). FromAssumption2,forallx ,
min
∈X
g (X ) 0 = L α = g (x) (1 λη )g (x) G (75)
t−1 t−1 t−1 t t−1 t−1 min
≤ ⇒ ≥ ⇒ ≥ − ≥
whichcontradictstheassumptionthatthereexistsmin g (x) G .
x∈X t min
≤
16Havingestablishedanupperandlowerboundonthemaximumvalueofthefunctiong ()generated
t
by(18)wecannowbound(60). Define∆ =η−1 η−1 and∆ =η−1andnotethat·
t t − t−1 1 1
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)1 (cid:88)T (cid:12) (cid:12)1 (cid:88)T (cid:12)
(cid:12) (L α)(cid:12) max(cid:12) (L α)(k(X ,x)+1)(cid:12)
(cid:12) (cid:12)T t − (cid:12) (cid:12)≤x∈X (cid:12) (cid:12)T t − t (cid:12) (cid:12)
t=1 t=1
(cid:13) (cid:13)1 (cid:88)T (cid:32) (cid:88)t (cid:33) (cid:13) (cid:13)
=(cid:13) ∆ η (L α)(k(X , )+1)(cid:13)
(cid:13) (cid:13)T r t t − t · (cid:13) (cid:13)
t=1 r=1 ∞
(cid:13) (cid:13)1 (cid:88)T (cid:32) (cid:88)T (cid:33)(cid:13) (cid:13)
=(cid:13) ∆ η (L α)(k(X , )+1) (cid:13)
(cid:13) (cid:13)T r t t − t · (cid:13) (cid:13)
r=1 t=r ∞
(cid:13) (cid:13)1 (cid:88)T (cid:32) (cid:88)T (cid:33)(cid:13) (cid:13)
=(cid:13) ∆ f +c (1 λη )f c (cid:13)
(cid:13) (cid:13)T r t+1 t+1 − − t t − t (cid:13) (cid:13)
r=1 t=r ∞
(cid:13) (cid:13)1 (cid:88)T (cid:32) (cid:88)T (cid:33)(cid:13) (cid:13)
=(cid:13) ∆ g g +λ η f (cid:13)
(cid:13) (cid:13)T r T+1 − r t t (cid:13) (cid:13)
r=1 t=r ∞
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)1 (cid:88)T (cid:13) (cid:13)λ (cid:88)T (cid:88)T (cid:13)
(cid:13) ∆ (g g )(cid:13) +(cid:13) ∆ η f (cid:13)
≤(cid:13) (cid:13)T r T+1 − r (cid:13) (cid:13) (cid:13) (cid:13)T r t t(cid:13) (cid:13)
r=1 ∞ r=1 t=r ∞
T T
1 (cid:88) λ (cid:88)
∆ g g + f . (76)
≤T r ∥ T+1 − r ∥∞ T ∥ t ∥∞
r=1 t=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
:=E1 :=E2
ThefirsttermcanbeboundedbasedonProposition(3)as
T (cid:18) (cid:19)
1 (cid:88) 1 4B√ρκD
E max g g ∆ = S + +2η B(2κ+1) , (77)
1 ≤ T r ∥ T+1 − r ∥∞ r η TT max λ 0
r=1
andsimilarly,forthesecondterm,wehave
T
λ (cid:88)κB
E =κB. (78)
2 ≤ T λ
t=1
Fixadecreasinglearningrateη =η t−ω andaregularizationparameterλ=λ T−ξ,thentheE
t 0 0 1
becomes
S 4B√ρκD 2B(2κ+1)
E = max + + (79)
1 η T1−ω η λT1−ω T1−ω
0 0
Foranyω <1,itfollows
(cid:12) (cid:12)
(cid:12)1 (cid:88)T (cid:12)
lim (cid:12) (L α)(cid:12)=κB. (80)
T→∞(cid:12) (cid:12)T t − (cid:12) (cid:12)
t=1
17C AdditionalExperiments
C.1 BeamSelection
C.1.1 SimulationDetails
60
40
20
0
20 Transmitter
−
50 0 50
− px
Figure 6: Network deployment assumed in the simulations. A single transmitter (green circle)
communicateswithreceiversthatareuniformlydistributedinascenecontainingmultiplebuildings
(greyrectangles).
Forthebeamselectionexperiment,weconsiderthenetworkdeploymentdepictedinFigure6,in
whichatransmitter(greencircle)communicateswithusersinanurbanenvironmentwithmultiple
buildings(greyrectangles).Weassumethatcommunicationoccursatafrequencyf =2.14GHzand
c
thatthetransmitterisequippedwithN =8transmittingantennaswhilereceivingusershavesingle-
t
antennaequipment. ThetransmitteradoptsadiscreteFouriertransformbeamformingcodebookof
sizeB =11,witheachbeamb givenby
max i
1
b
i
= [1,ej2π B2 mπ ai x,...,ej(Nt−1) B2 mπ ai x] CNt, fori 0,...,B
max
1 , (81)
√N ∈ ∈{ − }
t
wherej =√ 1. ThewirelesschannelresponsehR CNt betweenthetransmitterandareceiver
locatedatX− = [p ,p ] R2, ismodeledusingSio∈ nnaray-tracer[Hoydisetal.,2023], andwe
t x y
∈
accountforsmallscalefadingusingaRayleighnoisemodel[Goldsmith,2005].Theresultingchannel
vectorisdistributedas
h hR(X )+Rayleigh(σ). (82)
t t
∼
wherehR(X )istheraytraceroutputandRayleigh(σ)isaRayleighdistributedrandomvariable
t
withparameterσ =10−4. Assumingunitpowertransmitsymbolsandreceivernoise,forachannel
vectorh thecommunicationsignal-to-noiseratio(SNR)obtainedusingthebeamformerb isgiven
t i
by
Y =hTb . (83)
t,i t i
BeamsetsareobtainedcalibratinganSNRpredictorYˆ =f (X )realizedusinga3-layerfully
t SNR t
connectedneuralnetworkthatistrainedon2500sampleswiththeuserlocationgenerateduniformly
atrandomwithinthedeploymentarea.
C.1.2 EffectoftheLengthScale
InFigure7, westudytheeffectofthelengthscalel ofthekernelfunctiononthetime-averaged
thresholdfunctiong¯ (X)returnedbyL-ARC.WereportthevalueoftheL-ARCtime-averaged
T
threshold, g¯ (X), in(11), forthesameexperimentalset-upasinSection3.3, andforincreasing
T
localizationofthekernelfunction. Asthelengthscaleparameterl decreases,correspondingtoa
morelocalizedkernel,thevalueofthethresholdisallowedtovarymoreacrossthedeploymentarea.
Inparticular,thethresholdfunctionreducesitsvaluearoundareaswherethebeamselectionproblem
becomesmorechallenging,suchasbuildingclusters,inordertocreatelargerbeamselectionsets.
18
ypL-ARCl=50 L-ARCl=25 L-ARCl=10 0.30
60 0.25
40 0.20
20 0.15
0 0.10
−20 Transmitter Transmitter Transmitter 0.05
50 0 50 50 0 50 50 0 50 0.00
− px − px − px
Figure7: Time-averagedthresholdfunctiong¯ fordifferentvaluesoflocalizationparameterl.
T
ARCκ=0 L-ARCl=0.5 1.0 15 ARC
0.76 L-ARCl=1 L-ARCl=0.25 0.8 TargetCoverageLevel L-ARCl=1
0.6 10 L L- -A AR RC Cl l= =0 0. .5 25
ARC
0.74 0.4
L-ARCl=1 5
0.2 L-ARCl=0.5
L-ARCl=0.25
0.72 0.0 0
0 2000 4000 6000 8000 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8
Time-step(t) Model’sConfidence(Conf(X)) Model’sConfidence(Conf(X))
Figure8: Long-termcoverage(left),coveragerate(center),andpredictionsetsize(right)versus
model’sconfidenceforARCandL-ARCfordifferentvaluesofthelocalizationparameterl.
C.2 ImageClassificationwithCalibrationRequirements
Inthissection,weconsideranimageclassificationtaskundercalibrationrequirementsbasedonthe
fruit-360dataset[MuresanandOltean,2018]. Forthisproblem,thefeaturevectorX isanimage
t
ofsize100 100,andthecorrespondinglabelY = 1,...,130 isoneof130typesoffruit,
t
× ∈Y { }
vegetable, or nut in image X . We study the online calibration of a pre-trained ResNet18 model
t
[Heetal.,2016]. ForaninputimageX ,thepredictionsetisobtainedfromthemodel’spredictive
t
distributionpˆ(y X )as
t
|
C(X ,g )= y :pˆ(y X )>g (X ) , (84)
t t t t t
{ ∈Y | }
andwetargetthemiscoverageloss(3)withatargetmiscoveragerateα=0.25. Inordertocapture
calibrationrequirements,weimposecoverageconstraintsthatarelocalizedinthemodel’sconfidence.
Themodel’sconfidenceindicatorisgivenbythemaximumvalueofthemodel’spredictivedistribution
pˆ(y X ),i.e.,
t
|
Conf(X )=maxpˆ(y X ). (85)
t t
y∈Y |
Accordingly,werunARCandL-ARCcalibrationwithasequenceofT = 8000samplesandwe
instantiateL-ARCusingtheexponentialkernelk(x,x′)=κexp( ϕ(x) ϕ(x′) 2/l),wherethe
−∥ − ∥
featurevectorisgivenbythemodel’suncertainty,i.e.,ϕ(x)=Conf(x).
In the leftmost panel of Figure 8 we report the long-term coverage of ARC and L-ARC for an
increasingleveloflocalizationobtainedbydecreasingthelengthscalel. Allmethodsguarantee
long-term coverage. In the middle panel, we use hold-out data to evaluate the coverage of the
calibratedmodelconditionedonthemodel’sconfidencelevel. Forsmalllengthscalel,L-ARCyields
predictionsetsthatsatisfythecoveragerequirementacrossdifferentlevelsofthemodel’sconfidence.
Incontrast,ARC,duetoitsinabilitytoadaptthethresholdfunction,hasalargemiscoverageratefor
smallmodelconfidencelevels. Asillustratedintherightpanel,thisisachievedbyproducingalarger
setsizewhenthemodel’sconfidenceislow.
19
yp
egarevoCmret-gnoL etaRegarevoC
eziSteS
)xp,xp(T¯g