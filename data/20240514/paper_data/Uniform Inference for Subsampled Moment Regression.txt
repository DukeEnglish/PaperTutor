Uniform Inference for Subsampled Moment Regression∗
DavidM.Ritzwoller VasilisSyrgkanis
StanfordUniversity StanfordUniversity
ABSTRACT. We propose a method for constructing a confidence region for the solution to a
conditionalmomentequation. Themethodisbuiltaroundaclassofalgorithmsfornonparametric
regression based on subsampled kernels. This class includes random forest regression. We
boundtheerrorintheconfidenceregion’snominalcoverageprobability,undertherestriction
thattheconditionalmomentequationofinterestsatisfiesalocalorthogonalitycondition. The
methodisapplicabletotheconstructionofconfidenceregionsforconditionalaveragetreatment
effectsinrandomizedexperiments,amongmanyothersimilarproblemsencounteredinapplied
economicsandcausalinference. Asaby-product,weobtainseveralneworder-explicitresults
ontheconcentrationandnormalapproximationofhigh-dimensionalU-statistics.
Keywords: RandomForestRegression,Half-SampleBootstrap,U-statistics
JEL:C01,C14,C12
1. INTRODUCTION
ConsideranindependentandidenticallydistributedsampleD = (D )n ,whereeachobservationD
n i i=1 i
canbepartitionedD =(A ,X ). Westudyamethodforconstructingauniformconfidenceregionforthe
i i i
parametervectorθ (x(d))=(θ (x(j)))d ,whereeachθ (x)istheuniquescalarsolutiontotheconditional
0 0 j=1 0
momentequation
M(x;θ,g )=E[m(D ;θ,g )|X =x]=0 (1.1)
0 i 0 i
in θ and x(d) = (x(j))d is a specified d-vector in the domain of X . Here, g is an unknown nuisance
j=1 i 0
parameter,identifiedviaanauxiliarystatisticalproblem,andm(·;θ,g)isaknownmomentfunction. Many
problems in applied economics and causal inference can be formulated as instances of (1.1), including
nonparametricregression,quantileregression,andestimationofconditionalaveragetreatmenteffects.
To fix ideas, consider Banerjee et al. (2015), who study the effects of a poverty alleviation program
implementedinGhana.1 Foreachindividualintheirsample,theyobservethedataD =(Y ,W ,Z ),where
i i i i
Y isameasurementoftotalassetstakentwoyearsaftertheimplementationoftheprogram,W isanindicator
i i
denotingassignmenttotheprogram,andZ isavectorofcovariates. Abroadaimofthestudyistodetermine
i
Date:May14,2024
∗Email:ritzwoll@stanford.edu,vsyrgk@stanford.edu.WethankJiafengChen,GuidoImbens,JosephRomano,andBradRossfor
helpfulcommentsandconversations.RitzwollergratefullyacknowledgessupportfromtheNationalScienceFoundationunderthe
GraduateResearchFellowship. ComputationalsupportwasprovidedbytheData,Analytics,andResearchComputing(DARC)
groupattheStanfordGraduateSchoolofBusiness(RRI:SCR 022938).
1Banerjeeetal.(2015)studydatacollectedfromseveralsimilargraduationprograms.Wefocusonthedatafromtheirevaluationof
theprogramimplementedinGhana.AppendixEgivesfurtherdetailsonourtreatmentoftheBanerjeeetal.(2015)data.
4202
yaM
31
]ME.noce[
1v06870.5042:viXra2
theconditionsunderwhichrecipientsofaidexperiencelastingimprovementsinwelfare. Onequantitythat
caninformthisdeterminationistheconditionalaveragetreatmenteffect(CATE)
θ (x)=E [Y (1)−Y (0)|X =x] , (1.2)
0 P i i i
whereY (1)andY (0)arethepotentialoutcomesgeneratedbytheinterventionW andX issomechosen
i i i i
subvectorofZ . Acanonicalapproachtoestimating(1.2)ispremisedontheobservationthatθ (x)isthe
i 0
solutiontothemomentequation
M(x;θ,g )=E[(µ (Z )−µ (Z ))+β(W ,Z )(Y −µ (Z ))−θ |X =x]=0, (1.3)
0 1 i 0 i i i i Wi i i
ofRobinsetal.(1994)andHahn(1998),wherethenuisanceparameterg collectstheconditionaloutcome
0
regressionandHorvitz-Thompsonweight
w 1−w
µ (z)=E [Y |W =w,Z =z] and β(w,z)= − , (1.4)
w P i i i
π(z) 1−π(z)
forthepropensityscoreπ(z)=P{W =1|Z =z}. Seee.g.,NieandWager(2021),FosterandSyrgkanis
i i
(2023),andreferencesthereinforfurtherdiscussion.
Often,estimatesofsolutionstoconditionalmomentequationsoftheform(1.1)or(1.3)areobtainedby
solvingtheempiricalconditionalmomentequation
n
(cid:88)
M (x;θ,gˆ ,D )= K(x,X )m(D ;θ,gˆ )=0 (1.5)
n n n i i n
i=1
inθ,wheregˆ issomefirst-stageestimatorofthenuisanceparameterg andK(x,x′)issome,potentially
n 0
randomanddata-dependent,kernelfunctionmeasuringthedistancebetweenxandx′. Subsampledorbagged
kernels,introducedbyBreiman(1996),andpopularizedbyWagerandAthey(2018)andAtheyetal.(2019),
areparticularlyconvenient,dueinparttotheircomputationalefficiencyandrobustnesstotuningparameter
choice. Popularexamplesofsubsampledkernelestimatorsincludek-NNregression(FixandHodges,1989)
andrandomforestregression(Breiman,2001). Solutionstoconditionalmomentequations(1.5)constructed
withrandomforestregressionarereferredtoasGeneralizedRandomForests(GRF)(Atheyetal.,2019).
Figure1displaysGRFestimatesoftheCATE(1.2)fortheexperimentstudiedinBanerjeeetal.(2015),
wherethechosenconditioningcovariatesX arepretreatmentmeasurementsofmonthlyconsumptionand
i
totalassets.2 Thegraduationprogramappearstobemosteffectiveforindividualswithhighlevelofbaseline
consumptionandalowlevelofbaselineassets.3 Thatis,individualswithanopportunitytoincreasetheir
assetsareabletodosoonlyiftheyhaveahighlevelofbaselineconsumption. Theeffectoftheprogramfor
individualswithlowbaselineconsumptionorhighbaselineassetsappearsmoremuted. Theseresultsare
suggestiveofapovertytrap: individualswithoutastablesourceofconsumptionmaybeincentivizedtosell
productiveassets(KraayandMcKenzie,2014;Balbonietal.,2022).
2Theseestimatesareconstructedbyapproximatingthesolutiontotheconditionalmoment(1.3)withasubsampledkernelestimator
oftheform(1.5)ateachpointxonagrid.Boththenuisanceparameterestimatorgˆ andthekernelK(x,x′)areconstructedwith
n
theimplementationofrandomforestregressionmadeavailablethroughthe“GRF”Rpackage(Atheyetal.,2019).
3Thequartilesofbaselinelogconsumptionare3.33,3.76,and4.20.Thequartilesofbaselineassetsare-0.45,-0.71,and0.03.Panel
AofFigureE.4displaysofscatterplotofthejointdistributionofbaselinelogconsumptionandassets.3
FIGURE 1. CATEEstimates
5.0
CATE
0.3
4.5
0.2
0.1
4.0
3.5
3.0
−0.6 −0.3 0.0 0.3
Baseline Assets
Notes:Figure1displaysaheatmapgivingCATEestimatesfortheinterventionstudiedinBanerjeeetal.(2015)onpost-treatment
assets.ThecolorofeachrectangleindicatestheestimateoftheCATEqueriedattherectangle’scentralpoint.Thehorizontaland
verticalaxesdisplaythebaselinemonthlyconsumption,normalizedtodollarsandmeasuredinlogsbase10,andthebaselinevalueof
anindexfortotalassets.CATEestimatesareobtainedbysolvingtheempiricalmomentequation(1.5)foreachvaluexonanevenly
spacedgridonbothaxes.Thenuisanceparameterestimategˆ andthekernelK(x,x′)areconstructedwiththeimplementationof
n
randomforestregressionmadeavailablethroughthe“GRF”Rpackage(Atheyetal.,2019).SeeAppendixEforfurtherdetails.
The information communicated by Figure 1 is rich and granular. This contrasts with the more widely
encounteredpracticeofreportingregressioncoefficientsonlinearinteractionsofpretreatmentcovariateswith
treatmentindicators. Infact,aswewillsee,thelatterapproachgivesasubstantivelydifferentpictureofthe
heterogeneityintheeffectoftheBanerjeeetal.(2015)graduationprogram.
We contribute a method for assessing the statistical significance of estimates typified by Figure 1. In
particular, we propose a computationally simple procedure for constructing uniform upper and lower
confidenceboundsforsolutionstoconditionalmomentequations(1.1)centeredaroundsubsampledkernel
estimatorsoftheform(1.5). Formally,weconstructafamilyofrandomintervals
(cid:110) (cid:111)
Cˆ(x(d))= Cˆ(x(j))=[c (x(j)),c (x(j))]:j ∈[d] , (1.6)
L U
onthebasisoftheobserveddata,suchthat
sup
(cid:12)
(cid:12)P
(cid:110)
θ
0(x(d))∈Cˆ(x(d))(cid:111) −(1−α)(cid:12)
(cid:12)≤r
n,d
(1.7)
P∈P
forsomesequencer ,wherePissomestatisticalfamilythatcontainsthedistributionP ofthedataD . We
n,d i
saythataregion(1.6)satisfying(1.7)isuniformlyasymptoticallyvalidattherater . Here, uniformity
n,d
operatesoverboththed-dimensionalquery-vectorx(d) andthestatisticalfamilyP.4 Themaintheoretical
contributionofthispaperistheconstructionofuniformasymptoticallyvalidconfidenceregionswhoseerror
4FollowingLi(1989),uniformvalidityofaconfidenceregionfornonparametricregressionisoftenreferredtoas“Honesty”(see.,
e.g.,Chernozhukovetal.(2014);ArmstrongandKolesa´r(2020);Kuchibhotlaetal.(2023)).Weusethelabel“uniformvalidity,”
followinge.g.,RomanoandShaikh(2012),todistinguishthispropertyfrom“Honest”constructionofsubsampledkernels(Athey
andImbens,2016;WagerandAthey,2018),whichwillbestudiedindetailinSection3.
noitpmusnoC
enilesaB4
rater convergestozeroinasymptoticregimeswherethenumberofpointsdinthequery-vectorx(d) may
n,d
increasemuchmorequicklythanthesamplesizen.5
Webegin,inSection2,bydefiningtheproposedconfidenceregionandillustratingitsapplicationtothe
Banerjeeetal.(2015)experiment. Ourconstructioncanbeseenasaninstanceofsubsampling(Politisetal.,
1999;PolitisandRomano,1994),althoughourformalanalysisismoredirectlyconnectedtotheexchangeably
weightedbootstrap(PræstgaardandWellner,1993;Chernozhuokovetal.,2022).
In Section 3, we give a bound on the accuracy of the proposed confidence region. We require that the
confidenceregionbebuiltaroundthesolutiontoaNeymanorthogonalmoment. Thisrestrictionmitigatesthe
errorinducedbyestimationofnuisanceparameters. Ourresultiscomparabletothegenericboundsonthe
accuracyofGaussianmultiplierbootstrapconfidenceregionsfornonparametricregressionandZ-estimation
giveninChernozhukovetal.(2014)andBellonietal.(2018),respectively. Wegeneralizetheseresults,inthe
sensethatwetreatinferenceforconditionalZ-estimatorswhosescorefunctionispotentiallyunknownto
theresearcher. Wedocumentthattheproposedconfidenceregionisaccurateandinformativeatempirically
relevantsamplesizeswithasimulationcalibratedtotheBanerjeeetal.(2015)data.
Asaby-productofthisanalysis,wegiveseveralnewresultsontheconcentrationandnormalapproximation
oflargeorder,high-dimensional,U-statistics. Inparticular,wegiveaconcentrationinequalityandcentral
limittheoremforhigh-dimensionalU-statisticswithexplicitorder-dependence. Theseboundsareapplicable
to non-degenerate U-statistics whose order b satisfies b = o(n), up to a dimension dependent logarithmic
factor. Thisgeneralityrepresentsasubstantialimprovementoverexistingresults(Songetal.,2019;Minsker,
2023), that apply to the regime b = o(n1/3), and is essential for our application. Our results hinge on a
newconcentrationinequalityforthedifferencebetweenaU-statisticanditsHa´jekprojection(Ha´jek,1968),
obtainedthroughaHoffman-JørgensentypeargumentenabledbyasymmetrizationinequalityduetoSherman
(1994). WecollecttheseresultsinSection4. Section5concludes.
1.1 RelatedLiterature. Thereisanextensiveliteratureonestimationofsolutionstoconditionalmoment
equations. See,forexample,Newey(1993),AiandChen(2003),ChenandPouzo(2012),andChernozhukov
etal.(2023). ChenandChristensen(2018)andChenetal.(2024)proposerelatedmethodsforconstructing
uniformconfidencebandsforparametersidentifiedbyconditionalmoments,emphasizingachievingminimax
ratesinHo¨lderclassesbybuildingconfidenceregionsaroundcarefullyconstructedsieveestimatorswith
Lepski’s method (Chernozhukov et al., 2014). See also Singh and Vijaykumar (2023) for an analysis of
inference for kernel ridge regression. By contrast, our aim is to provide a simple procedure for uniform
inferencebasedonestimatorswhoseprecisestructuremaybeunknowntotheuser.
WecontributetoalargeliteratureontheroleofNeymanorthogonalityinestimationofsolutionstomoment
equationswithnuisanceparameters. Chernozhukovetal.(2018),Chernozhukovetal.(2022),andIchimura
and Newey (2022) provide extensive discussion and guidance on the derivation of orthogonal moments.
NieandWager(2021),FosterandSyrgkanis(2023),andKennedy(2023)applyaspectsofthisanalysisto
conditional moment estimation. The closest paper, in this literature, is Belloni et al. (2018), who give a
5Theleadingexamplesforchoicesofthequery-vectorx(d)arecaseswherex(d)istakentobetheobservedvaluesofthecovariates
X ,...,X orwherex(d)givesafinegridoverthedomainofX .Inbothcasesthedimensiondofthequery-vectorx(d)iseither
1 n i
equaltoorpotentiallylargerelativetothesamplesizen.5
related,general,treatmentofZ-estimation. WebuildonthisanalysisbystudyingconditionalZ-estimators
withunknownscorefunctions.
OurpaperismotivatedbyAtheyandImbens(2016),WagerandAthey(2018),andAtheyetal.(2019),
whopopularizedestimationofsolutionstoconditionalmomentequationswithsubsampledkernelregression.
TheestimatorsconsideredinSection3arecloselyrelatedtothe“OrthogonalRandomForests”estimator
proposedinOprescuetal.(2019). Methodsforconstructingconfidenceregionsforrandomforestsregression
are studiedin Sextonand Laake (2009), Wager et al. (2014), Mentchand Hooker (2016) and Athey etal.
(2019). Wecontributetothisliteraturebyprovidingmethodsforconstructinguniformlyasymptoticallyvalid
confidenceregions.
Ourformalanalysisbuildsonagroundbreakingsequenceofpapersoncentrallimittheoremsformaxima
of sums initiated by Chernozhukov et al. (2013). Extensions and refinements of these results are given
in Chernozhukov et al. (2017a) and Chernozhuokov et al. (2022). Similar approaches for applying these
resultstotheconstructionofuniformconfidenceregionsaregiveninChernozhukovetal.(2014)andBelloni
etal.(2018). Otheraspectsofouranalysisdrawontheconsiderationofexchangeablyweightedbootstrap
approximationstoasymptoticallylinearstatisticsgiveninPræstgaardandWellner(1993),ChungandRomano
(2013),andYadlowskyetal.(2023).
TheasymptoticanalysisofU-statisticshasalongandinvolvedhistory(seee.g.,Lee,1990foratextbook
introduction). We provide a more detailed literature review in Section 4. Recently, several papers have
demonstratedthatcentrallimittheoremsfornon-degenerate,real-valued,U-statisticsoforderbcanhold,
evenifbisincreasingatsomeratethatsatisfiesb=o(n)asnincreasestoinfinity(WagerandAthey,2018;
DiCiccioandRomano,2022;Pengetal.,2022;Minsker,2023). However,tothebestofourknowledge,there
arenogeneral,order-explicit,exponentialmomentinequalitiesorhigh-dimensionalcentrallimittheorems
thatobtaininthisregime. Inparticular,Songetal.(2019)andMinsker(2023)establishhigh-dimensional
momentinequalitiesandcentrallimittheoremsthatapplytotheregimeb=o(n1/3).6 Aswewillsee,thisis
insufficientforourmainapplicationtosubsampledkernelregression. Wegeneralizetheseresults,obtaining
exponentialmomentinequalitiesandnon-asymptotic,high-dimensional,centrallimittheorems,withexplicit
orderdependence,thatholdintheregimeb=o(n).
Finally,wecontributetoalargeliteratureonthestatisticalanalysisofsubsampledkernelregressionand
randomforestregression. Awidevarietyofconsistencyresultsaregivenin,e.g.,Bu¨hlmannandYu(2002),
Lin and Jeon (2006), Biau et al. (2008), Mentch and Hooker (2014), Scornet et al. (2015), and Cattaneo
etal.(2024). HighdimensionalconsistencyresultsaregiveninSyrgkanisandZampetakis(2020),Chietal.
(2022),andHuoetal.(2023).
1.2 Notation. ThedataZ andX takevaluesinthespacesZ andX,respectively. Definethegenericnorm
i i
∥·∥ onX. Thenuisanceparameterg isafinitecollectionofhreal-valuedfunctionsg =(g(k) )h ,each
∞ 0 0 0 k=1
6Minsker(2023)additionallygivesanexponentialmomentinequalitythatholdsintheregimeb = o(n)byplacingstringent
restrictions on the smoothness of the kernel of the U-statistic of interest. These smoothness restrictions will not hold in our
applicationtosubsampledkernelregression.6
havingdomainZ. Definethenorm
(cid:16) (cid:104) (cid:105)(cid:17)1/2
∥g−g ∥ = sup sup E (g(k)(Z )−g(k) (Z ))2 |X =x(j) (1.8)
0 2,∞ i 0 i i
k∈[h]j∈[d]
foranyg =(g(k))h . Thenuisanceparameterg takesvaluesinthespaceG.
k=1 0
ThequantitiescandC denoteuniversalpositiveconstants,whosevaluesareallowedtodependonlyon
thefamilyofdistributionsP. Fortworeal-valuedfunctionsf andg onadomainX,wesayg(x)≲f(x)if
g(x) ≤ Cf(x)foreachxinX. ThesetS collectsallofthesubsetsof[n] = {1,...,n}ofsizebandD
n,b s
denotesthesubsetoftheobserveddataD withindicesinthesets. ForafunctionalF onF, weusethe
n
notation
d (cid:12) d2 (cid:12)
∂ fF(f)[h]= dtF(f +th)(cid:12)
t=0
and ∂ f,fF(f)[h]= dt2F(f +th)(cid:12)
t=0
todenotefirstandsecondorderdirectionalderivatives,respectively. Throughout,foranyfunctionf(x)and
vectorx(d) =(x(j))d ,weletf(x(d))denotethevector(f(x(1)),··· ,f(x(d))).
j=1
2. IMPLEMENTATION
Webuildconfidenceregionsaroundsolutionstotheempiricalconditionalmomentequation
M (x;θ,gˆ ,D )=0 (2.1)
n n n
inthevariableθ,evaluatedateachx(j) inx(d). Letθˆ (x)denotethesolutionto(2.1)evaluatedatx. Our
n
construction is agnostic to the structure of the empirical moment (2.1). This property is essential for the
applicationtosubsampledkernelregressionconsideredinSection3.
2.1 Construction. Foranysin[n],letθˆ(x(d))denotethevectorofsolutionsto(2.1)evaluatedateach
s
x(j) inx(d) withthedataD replacedbythesubsampleD . Ourproposalispremisedonapproximatingthe
n s
samplingdistributionoftheroot
R (x(d))=θˆ (x(d))−θ (x(d)) (2.2)
n n 0
withtheconditionaldistributionofthehalf-samplebootstraproot
R∗(x(d))=θˆ (x(d))−θˆ (x(d)), (2.3)
n h n
where h denotes a random element of S , i.e., a random half-sample of [n]. The nuisance parameter
n,n/2
estimatorgˆ doesnotneedtobere-estimatedwhencomputing(2.3). Aversionofthehalf-samplebootstrap
n
is implemented by default in the GRF R package (Athey et al., 2019).7 The half-sample bootstrap is an
instanceofsubsampling(PolitisandRomano,1994;Politisetal.,1999).
√
Letλˆ2 denotethevarianceof nR∗(x(j)),conditionedonthedataD ,andletcv (α)denotethe1−α
n,j n n n
quantileofthedistributionofthestudentizedprocess
√
Sˆ∗(x(d))= n∥Λˆ−1/2 R∗(x(d))∥ , (2.4)
n n n ∞
7Theversionofthehalf-samplebootstrapconsideredinAtheyetal.(2019)isbasedoncombiningestimatesofthevariances
ofcomponentsofalinearizationofthemomentM(x;θ,g)withaDeltamethodtypeargument. Bycontrast,thebootstraproot
Equation(2.3)isagnostictothestructureoftheconditionalmomentunderconsideration.7
againconditionedonthedataD . Here,Λˆ denotesthediagonalmatrixwithelementsλˆ2 .8 Theconfidence
n n n,j
regionconsideredinthispaperhasthefollowingstructure.
Definition2.1(UniformConfidenceRegion). Definetheintervals
Cˆ(x(j))=θˆ (x(j))±n−1/2λˆ cv (α) foreach j in[d]. (2.5)
n n,j n
Thelevel-αuniformconfidenceregionforθ (x(d))isgivenbyCˆ(x(d)).
0
Confidence regions with the same structure, based on different choices of bootstrap root, are studied in,
e.g.,Chernozhukovetal.(2014)andBellonietal.(2018). Theessentialfeatureofthebootstraproot(2.3)
is that it can be computed without knowing anything about the structure of the estimator θˆ (x(d)). In
n
particular,approachesbasedontheRademacherorGaussianmultiplierbootstraprelyonknowledgeofa
linearapproximationtoθˆ (x(d)).
n
Togainintuition,supposethattheestimatorθˆ (x(d))satisfiesalinearrepresentation
n
n
1 (cid:88)
θˆ (x(d))= u(x(d),D ) (2.6)
n i
n
i=1
forsomefunctionu(·,·). LetV ,...,V denoteacollectionofrandomvariables,whereV takesthevalue1
1 n i
iftheindexiisanelementoftherandomsethusedtodefinethehalf-samplebootstrap,andtakesthevalue
−1otherwise. Observethat
n n
2 (cid:88) 1 (cid:88)
R∗(x(d))=θˆ (x(d))−θˆ (x(d))= I{i∈h}u(x(d),D )− u(x(d),D )
n h n n i n i
i=1 i=1
n
1 (cid:88) (cid:16) (cid:17)
= V u(x(d),D )−θ (x(d)) . (2.7)
i i 0
n
i=1
Therepresentation(2.7)isduetoYadlowskyetal.(2023),whodrawonasimilarobservationmadeinthe
contextoftwo-sampletestinginChungandRomano(2013). TheweightsV areexchangeableRademacher
i
randomvariables,i.e.,theyareuniformlydistributedon{1,−1}. Iftheweightswerefullyindependent,the
representation(2.7)reducestotheRademacherbootstrap.9 Chernozhuokovetal.(2022)giveacentrallimit
theoremforstatisticsoftheform(2.6). Weobtainacentrallimittheoremforthestatistic(2.7)byadaptinga
couplingargumentduetoYadlowskyetal.(2023). Inparticular,weshow(inasuitablesense)thatlimiting
conditionaldistributionofthebootstraproot(2.3)isthesameasthelimitingdistributionoftheroot(2.2).
In practice, many widely applied estimators are not perfectly linearly decomposable. Often, however,
estimatorsdosatisfyanapproximatelineardecomposition,inthesensethattheequality(2.6)holdswitha
remaindertermoforder,say,o (n−γ)forsomepositiveconstantγ. Aswewillsee,estimatorsconstructed
p
with subsampled kernels are approximately linear around some unknown function u(·,·). If an estimator
8Thequantitiesλˆ2 andcv (α)areeasilyapproximatedbyresamplingthebootstraproot(2.3).Tosimplifyexposition,weomit
n,j n
explicitconsiderationofresidualrandomnessinducedbythisapproximation.
9InAppendixD.2,weconsideravariantofthebootstraproot(2.3)basedonre-estimatingandre-scalingθˆ (x(d))onasubsample
n
ofarandomsizeBin(n,1/2). Forthisconstruction,theequivalentobjectstotheweightsV arefullyindependent. Thatis,this
i
bootstraprootisequivalenttotheRademacherbootstraprootforlinearstatistics. Weshowthattheresultingconfidenceregions
obtainthesameerrorratesoncoverageaccuracy.8
θˆ (x(d))isapproximatelylinear,thenthesubsampledestimateθˆ (x(d))isimmediatelyalsoapproximately
n h
linear.10 Thus, for approximately linear statistics, the representation (2.7) continues to hold, now with a
remainder term of order o (n−γ). As a consequence, the validity of the confidence region formulated in
p
Definition2.1,forapproximatelylinearestimators,followsfromthecentrallimittheoremsdiscussedinthe
precedingparagraphandanappropriategeneralizationofSlutsky’stheorem.
2.2 Application. WenowreturntotheapplicationtothedatastudiedinBanerjeeetal.(2015),considered
inSection1. Figure2displaysupperandlowerconfidenceboundsfortheCATE(1.2)onpost-treatment
assets. TheseboundsarebuiltwiththeconfidenceregionformulatedinDefinition2.1.
ConsistentwiththequalitativefeaturesoftheestimatesdisplayedinFigure1,thenullhypothesisthatthe
CATEisequaltozeroisonlyrejectedforindividualswithlowbaselineassetsandhighbaselineconsumption.
Thatis,thegraduationprogramhasapositiveimpactonindividualswhodonothavemanyassetstobegin
with,butwhodohaveaccesstoastablesourceofconsumption. Ontheotherhand,theconfidenceregions
containzeroforindividualswhohaveeitherlowbaselineconsumptionorhighbaselineassets.
ItisillustrativetocontrasttheestimatesandconfidenceboundsdisplayedinFigures1and2withamore
frequentlyencounteredmethodforassessingtreatmenteffectheterogeneity—interactedlinearregression.
Table 1 reports estimates and standard errors associated with several linear regression specifications,
constructedwiththesamedata. Thefirstcolumnreportsthecoefficientfromaregressionofpost-treatment
assetsonatreatmentindicator. ConsistentwithresultsreportedinBanerjeeetal.(2015),theaverageeffect
oftheprogramispositiveandsignificant. Thesecondandthirdspecificationsinteractthetreatmentindicator
with pre-treatment assets and consumption, respectively. In both cases, the estimate of the coefficient on
theinteractionisstatisticallyinsignificant. Thefourthspecificationinteractsbothpre-treatmentassetsand
consumptionwithatreatmentindicator. Here,allcoefficientslosestatisticalsignificance.
Theimplicitviewofmuchofappliedeconomicsappearstobethattheflexibilityaffordedbynonparametric
methods is not worth sacrificing the statistical precision of more parsimonious, linear, alternatives.11 The
exercise here suggests otherwise. The linearity imposed by interacted regression masks the structure in
theeffectheterogeneityrecoveredbytheGRFestimator. Theresultingbiasissosubstantialthatstatistical
significance is lost. The half-sample confidence regions developed in this paper enable the recovery of
statistically significant measurements of effect heterogeneity. The remainder of the paper is devoted to
developingtheoreticalguaranteesontheaccuracyofconfidenceboundstypifiedbyFigure2.
3. SUBSAMPLED KERNEL REGRESSION
We establish a bound on the accuracy of the nominal coverage probability for the confidence region
introduced in Definition 2.1. We begin in Section 3.1 by discussing subsampled kernel regression and
introducing several quantities that take a prominent role in our analysis. Our results apply to conditional
10Approximatelinearitydoesnotimmediatelyimplyarepresentationanalogousto(2.7)forarootconstructedwithanempirical
bootstrap,asapproximatelinearitywouldnotnecessarilyholdundertheempiricaldistribution.
11WeconductasmallsurveyofpaperspublishedintheAmericanEconomicReviewinthefirstsixmonthsof2023.Of38empirical
papers,30assesstreatmenteffectheterogeneityinsomeway.Asbestaswecantell,onlytwopapersdisplaynonparametricestimates
ofeffectheterogeneity.Bycontrast,10displaytheresultsofaninteractedlinearregressiontypifiedbyTable1.Therestareeither
structuralpapers,orareonlyinterestedininteractionswithbinarycovariates.SeeAppendixD.1formoredetails.9
FIGURE 2. Half-SampleConfidenceRegion
PanelA:UpperBound
5.0
CATE
0.5
4.5
0.4
0.3
4.0
3.5
3.0
−0.6 −0.3 0.0 0.3
Baseline Assets
PanelB:LowerBound
5.0
CATE
0.1
4.5
0.0
−0.1
4.0
3.5
3.0
−0.6 −0.3 0.0 0.3
Baseline Assets
Notes: Figure2displaysheatmapsgivinghalf-sampleupperandlowerconfidenceboundsfortheCATEoftheintervention
studiedinBanerjeeetal.(2015)onpost-treatmenttotalassets.Theconfidenceboundsareconstructedatlevelα=0.1.Theupper
andlowerboundsaredisplayedwithdifferentcolorpalettestoemphasizetheuseofdifferentscales. Acontourlinehasbeen
superimposedoverthelowerboundtodemarcatewheretheboundcrosseszero.TheaxesandestimatorarethesameasinFigure1.
momentsthatsatisfyaNeymanorthogonalitycondition,inadditiontoseveralsimpleregularityconditions.
We overview these restrictions in Section 3.2. Our main result is stated in Section 3.3. The results of a
simulationcalibratedtotheBanerjeeetal.(2015)dataarereportedinSection3.4.
3.1 Subsampled Kernel Regression. Subsampled kernel regression is a broad class of algorithms for
solvingregressionproblemsoftheform(1.5),basedonconstructingadata-drivenkernelfunctionK(x,x′)
withsubsampling. Formally,fixsomepositiveintegerr andlet(s )r collectasequenceofsubsetsof[n]
q q=1
noitpmusnoC
enilesaB
noitpmusnoC
enilesaB10
TABLE 1. InteractedLinearRegression
DependentVariable: Post-TreatmentAssets
(1) (2) (3) (4)
Treatment 0.218(0.031) 0.194(0.032) 0.102(0.172) 0.111(0.201)
Assets 0.770(0.051) 1.229(0.288)
Consumption 0.06(0.022) -0.026(0.026)
Assets×Consumption -0.119(0.076)
Treatment×Assets 0.047(0.093) -0.065(0.552)
Treatment×Consumption 0.031(0.045) 0.021(0.050)
Treatment×Assets×Consumption 0.027(0.141)
Observations: 2,438 2,438 2,438 2,438
Notes:Table1reportsestimatesofthecoefficientsoffourlinearregressionspecifications,constructedwiththeBanerjeeetal.
(2015)data.Robuststandarderrorsaredisplayedinparentheses.Coefficientsoninterceptsarenotdisplayed.
drawnindependentlyanduniformlyfromS . Letξ denotesomeauxiliarysourceofrandomnessandlet
n,b
(ξ )r collectasetofindependentrandomvariableswiththesamedistributionasξ. Westudyconditional
sq q=1
empiricalmomentestimatorsoftheform(1.5),wherethekernelfunctionK(x,x′)admitsthedecomposition
r
(cid:88)
K(x,X )= I{i∈s }κ(x,X ,D ,ξ ) (3.1)
i q i sq sq
q=1
forsomeknownkernelκ(·,·,D ,ξ ). Severalwidelyappliedinstancesofsubsampledkernelsareasfollows.
s s
Example 3.1 (Subsampled k-Nearest Neighbor Regression). Nearest-neighbors regression is a simple
example of a kernel with the structure (3.1) (Fix and Hodges, 1989). Here, the kernel κ(x,X ,D ,ξ ) is
i′ s s
non-zeroifandonlyifX isoneofthek closestpointstoxamongthepointsinthesubsampleD . ■
i′ s
Example 3.2 (Random Forest Regression). Random forest regression, introduced by Breiman (2001), is
anotherexampleofakernelwiththestructure(3.1). Inthiscase,eachpair(D ,ξ )generatessomepartition
s s
ofthedomainofX . Thekernelκ(x,x′,D ,ξ )isnon-zeroifandonlyifxandx′ areinthesameelementof
i s s
thepartitiongeneratedby(D ,ξ ). Often,suchpartitionsareconstructedwithrecursivealgorithms,e.g.,the
s s
“CART”algorithmofBreimanetal.(1984). ■
Weimposethefollowingrestrictionsonthekernelsunderconsideration.
Assumption3.1(HonestyandSymmetry).
(i)Thekernelκ(·,·,D ,ξ)isHonestinthesensethat
s
κ(x,X ,D ,ξ )⊥⊥m(D ;θ,g)|X ,D , (3.2)
i s s i i s−i
where⊥⊥denotesconditionalindependenceands denotesthesets\{i}.
−i
(cid:80)
(ii) The kernel κ(·,·,D ,ξ) is positive and satisfies the restriction κ(·,X ,D ,ξ ) = 1 almost surely.
s i∈s i s s
Moreover,theconditionalexpectationE[κ(·,X ,D ,ξ )|D ]isinvarianttopermutationsofthedataD .
i s s s s
The “Honesty” condition stipulated in Part (i) of Assumption 3.1 imposes the restriction that any part
of the data D that can affect the value of the moment m(D ;θ,g) cannot affect the value of the kernel
i i11
κ(x,X ,D ,ξ ). This condition was introduced in Athey and Imbens (2016). Honesty is often achieved
i s s
throughkernelconstructionschemesbasedonsample-splitting;seeWagerandAthey(2018)andAtheyetal.
(2019)forfurtherdiscussion. Part(i)ofAssumption3.1imposesseveralweakregularityconditions.
Thefollowingtwoquantitiesrestrictthe“size”and“variability”ofthechosenkernel.
Definition3.1(ShrinkageandIncrementality).
(i)Wesaythatthekernelκ(·,·,D ,ξ )hasauniformshrinkagerateε if
s s b
(cid:104) (cid:110) (cid:111)(cid:105)
sup sup E max ∥X −x(j)∥ :κ(x(j),X ,D ,ξ )>0 ≤ε . (3.3)
i ∞ i s s b
P∈Pj∈[d]
(ii)Wesaythatakernelκ(·,·,D ,ξ )isuniformlyincrementalif
s s
(cid:32) (cid:34) (cid:35)(cid:33)
(cid:88)
inf sup Var E κ(x,X ,D ,ξ )m(D ;θ,g)|l ∈s,D =D ≳b−1 (3.4)
i s s i l
P∈Pj∈[d]
i∈s
whereD isanindependentrandomvariablewithdistributionP.
Theshrinkagerateofakernelκ(·,·,D ,ξ )isanalogoustothebandwidthofaclassical,deterministic,kernel.
s s
Theincrementalityrestrictionensuresthatthechosenkernelisnotoverlydependentonasingledatapoint.
BothnotionswereintroducedbyWagerandAthey(2018)andhavebeencharacterizedexplicitlyforvarious
widelyappliedsubsampledkernelestimators.12
Example3.1(Continued). Inthecaseofhonestsubsampledk-NNregression,Khosravietal.(2019)show
thatε ≲b−1/p,wherepisthe“intrinsicdimension”ofthemeasureofthecovariatesX . Roughlyspeaking,
b i
adistributionhasanintrinsicdimensionofpifitis(locally)wellapproximatedbyameasuresupportedona
subspaceofX ofdimensionp. Seee.g.,Kpotufe(2011)forfurtherdiscussion. Inturn,Khosravietal.(2019)
andPengetal.(2022)showthatthekernelsassociatedwithbothhonestandnon-honestvariantsofk-NN
regressionareincremental,uptologarithmicfactorsthatdependonthedimensionofthecovariates. ■
Example 3.2 (Continued). Analogously, for honest random forest regression, Wager and Athey (2018)
establishthatε ≲b−c/p,wherepisthedimensionofthedomainofX . Seee.g.,WagerandAthey(2018)
b i
andOprescuetal.(2019)forfurtherdiscussion. Boundsadaptivetotheintrinsicdimensionofthemeasureof
X aregiveninHuoetal.(2023)underfurtherrestrictions. WagerandAthey(2018)andPengetal.(2022)
i
givesimpleconditionsunderwhichthekernelassociatedwithsubsampled,honest,randomforestregression
isuniformlyincremental,againuptodimensiondependentlogarithmicfactors. ■
3.2 Moment Restrictions. The uniform confidence region introduced in Definition 2.1 is based on
undersmoothing,inthesensethattheconstructionmakesnoexplicitcorrectionforbias. Inotherwords,the
confidenceregionsthatweconsiderarereliantontheuseofestimatorsθˆ (x(d))whosebiasisofasmaller
n
stochastic order than the sampling variance. To this end, we emphasize the use of conditional moments
M(·;θ ,g ),thatsatisfyalocalNeymanOrthogonalitycondition.
0 0
12Theterminology“shrinkage”wasintroducedinOprescuetal.(2019),andisnotintendedtoconnote(explicit)regularization.12
Definition3.2(LocalNeymanOrthogonality). WesaythataconditionalmomentM(·;θ ,g )isuniformly
0 0
locallyNeymanorthogonalif
∂ M(x(j);θ ,g )[g−g ]=0 (3.5)
g 0 0 0
forallP inPandx(j) inx(d).
The use of Neyman orthogonal moments ensures that the bias induced by the estimation of the nuisance
parameterg withgˆ issmall(Newey,1994).
0 n
InadditiontoNeymanorthogonality,werequireseveralsmoothnessrestrictionsonthefunctionm(·;θ,g).
Inthemaintext,toeaseexposition,weimposethefollowinglinearityandboundednessrestriction.
Assumption3.2(MomentLinearityandBoundedness).Themomentfunctionm(·;θ,g)satisfiesthelinear
representation
m(D ;θ,g)=m(1)(D ;θ,g)·θ+m(2)(D ;g) (3.6)
i i i
forsomeknownfunctionsm(1)(·;θ,g)andm(2)(·;g). Moreover,theabsolutevalueofthefunctionm(·;θ,g)
isboundedbytheconstant(θ+1)ϕalmostsurely.
ThelinearityrestrictionentailedinAssumption3.2isinessentialandisimposedforthesakeofsimplicity.13
Theboundednessrestrictioniseasilyweakenedtoaslightlymoreinvolvedassumptionstatedintermsofthe
sub-exponentialnorm. Again,weimposeboundednesstoeaseexposition.
WemaintainthefollowingmildsmoothnessrestrictionsonthemomentfunctionM(·;θ ,g ).
0 0
Assumption3.3(MomentSmoothness).
(i)ThemomentM(·;θ,g )issecondordersmooth,inthesensethat
0
sup sup (cid:12) (cid:12)∂ g,gM(x(j);θ 0,g 0)[g−g 0](cid:12) (cid:12)≲∥g−g 0∥2
2,∞
(3.7)
P∈Pj∈[d]
foreachg inG.
(ii)Thevariogram
V(x;g)=E(cid:2) (m(Z ;θ (x),g)−m(Z ;θ (x),g ))2 |X =x(cid:3)
i 0 i 0 0
isuniformlyLipschitzinbothofitscomponents,inthesensethat
sup sup(cid:12) (cid:12)V(x;g)−V(x′;g)(cid:12) (cid:12)≲∥x−x′∥
∞
(3.8)
P∈Pg∈G
holdsforallxandx′ inX and
sup sup |V(x(j),g)−V(x(j),g′)|≲∥g−g′∥2 (3.9)
2,∞
P∈Pj∈[d]
holdsforeachg andg′ inG.
(iii)Definethemoments
(cid:104) (cid:105) (cid:104) (cid:105)
M(1)(x;θ,g)=E m(1)(D ;θ,g)|X =x and M(2)(x;g)=E m(2)(D ;g)|X =x ,
i i i i
13InAppendixA,weshowthatmomentlinearitycanbereplacedbythehigh-levelassumptionthatθˆ (x(d))isconsistentfor
n
θ (x(d)). ThisstateofaffairsisstandardinM-estimationproblems(seee.g., NeweyandMcFadden,1994). Asourrunning
0
examplesuselinearmoments,andsufficientconditionsfortheconsistencyofθˆ (x(d))havebeenestablished(seee.g.,Assumption
n
4.1andTheorem4.3of Oprescuetal.,2019),weomitadetailedconsiderationofnonlinearmoments.13
associatedwiththefunctionsm(1)(·;θ,g)andm(2)(·;g)introducedinAssumption3.2. Bothmomentsare
uniformlyLipschitzintheirfirstcomponent,inthesensethat
sup sup(cid:12) (cid:12)M(j)(x;g)−M(j)(x′;g)(cid:12) (cid:12)≲∥x−x′∥
∞
(3.10)
P∈Pg∈G
for each j in {1,2} and all x and x′ in X. Moreover, the first moment is uniformly Lipschitz in its third
componentandboundedfrombelowinthesensethat
sup sup (cid:12) (cid:12)M(1)(x(j);g)−M(1)(x(j);g 0)(cid:12) (cid:12)≲∥g−g 0∥
2,∞
and (3.11)
P∈Pj∈[d]
inf inf
(cid:12) (cid:12)M(1)(x(j);g)(cid:12)
(cid:12)≥c (3.12)
P∈Pj∈[d]
foreachg inG andsomepositiveconstantc.
NeymanorthogonalmomentssatisfyingthesmoothnessrestrictionsspecifiedinAssumptions3.2and3.3
areavailableformanywidelyconsideredstatisticalproblems.
Example3.3(NonparametricRegression). SupposethatD =(Y ,Z )containsameasurementofanoutcome
i i i
Y and a vector of covariates Z and that the data X are some function, e.g., a sub-vector, of Z . In this
i i i i
setting,weareofteninterestedinestimatingtheconditionalexpectationfunction
θ (x)=E [Y |X =x] .
0 P i i
Here,theparameterθ (x)isidentifiedviathelinearmomentfunctionm(D ,θ) = Y −θ. Asthereareno
0 i i
nuisanceparameters,localNeymanorthogonalityisimmediate. ■
Example3.4(ConditionalAverageTreatmentEffects). Now,supposethatD additionallycontainsabinary
i
valuedvariableW thatindicateswhetherunitihasbeenrandomlyassignedtoanintervention. Inthiscase,
i
interest may be in estimating a CATE (1.2). Under strong ignorability, the CATE is identified by several
momentfunctions,e.g.,thoseimpliedbyinversepropensityweightingoroutcomeregression(Imbensand
Rubin,2015). Themoment(1.3)istheuniqueNeymanorthogonalidentifyingmomentfortheparameter
(1.2)(seee.g.,Hahn,1998;Chernozhukovetal.,2018). Notethat,inthiscase,Part(i)ofAssumption3.3is
impliedbythemorerefinedbound
sup sup (cid:12) (cid:12)∂ g,gM(x(j);θ 0,g 0)[g−g 0](cid:12) (cid:12)≲∥µ−µ 0∥ 2,∞∥β−β 0∥
2,∞
, (3.13)
P∈Pj∈[d]
where µ and β are defined in (1.4). This structure yields the celebrated “Double Robustness” result for
estimationofaveragetreatmenteffectsandconditionalaveragetreatmenteffects;seeChernozhukovetal.
(2018)andKennedy(2023)forfurtherdiscussion. Weimposethemoregeneralcondition(3.7),asthisbound
exhibitsmanyofthesamefeaturesandwillholdforawidervarietyofproblems. ■
Example3.5(ConditionalLocalAverageTreatmentEffects). Inturn,supposethatD additionallycontains
i
ameasurementofabinaryinstrumentB andthatinterestisnowinestimatingtheconditionallocalaverage
i
treatmenteffect
θ (x)=E [Y (1)−Y (0)|W (1)>W (0),X =x] , (3.14)
0 P i i i i i14
where W (1) and W (0) are the potential outcomes for the treatment W generated by the instrument B .
i i i i
Under standard assumptions (Angrist et al., 1996), Tan (2006) and Fro¨lich (2007) show that the unique
Neymanorthogonalmomentfunctionfortheparameter(3.14)isgivenby
(cid:18) (cid:19)⊤(cid:18) (cid:18)(cid:18) (cid:19) (cid:19)(cid:19)
1 Y
m(D ;θ,g)= (γ (Z )−γ (Z ))+βϱ(B ,Z ) i −γ (Z ) , (3.15)
i
−θ
1 i 0 i i i
W
Bi i
i
wherethenuisanceparameterg nowcollectsthenuisancefunctions
0
(cid:20)(cid:18) (cid:19) (cid:21)
Y b 1−b
γ (z)=E i |B =b,Z =z and βϱ(b,z)= − (3.16)
b P i i
W ϱ(z) ϱ(z)
i
andϱ(z)=P{B =1|Z =z}denotestheinstrumentalpropensityscore. ■
i i
AdditionalexamplesofproblemswheresmoothNeymanorthogonalidentifyingmomentsareavailableinclude
estimationofpartiallylinearregressionandpartiallylinearinstrumentalvariableregression(Chernozhukov
et al., 2018), dynamic treatment effects (Lewis and Syrgkanis, 2021), and long-term treatment effects
identified by surrogate outcomes (Athey et al., 2020; Chen and Ritzwoller, 2023). Ichimura and Newey
(2022) and Belloni et al. (2018) provide extensive discussion on the derivation of Neyman orthogonal
moments. FurtherdiscussionoftheroleofNeymanorthogonalityinsemiparametricestimationisgivenin
Chernozhukovetal.(2018)andFosterandSyrgkanis(2023).
3.3 Coverage. Thefollowingtheoremgivesaboundontheerrorinthenominalcoverageprobabilityof
theconfidenceregionsintroducedinDefinition2.1. Weemphasizethattheresultisapplicabletoasymptotic
regimeswherethedimensionofthequery-vectorx(d) canbeexponentiallylargerthanthesamplesizen.
Theorem3.1(CoverageErrorBound).Supposethatthekernelκ(·,·,D ,ξ )satisfiesAssumption3.1,has
s s
uniformshrinkagerateε ,andisuniformlyincrementalandthattheNeymanorthogonalmomentfunction
b
M(·;θ ,g )satisfiesAssumptions3.2and3.3. Moreover,supposethatquantity∥θ (x(d))∥ isuniformly
0 0 0 ∞
√
bounded as P varies over P and that r has been chosen to satisfy n ≤ b r. If the nuisance parameter
estimatorgˆ satisfiestheprobabilitybound
n
(cid:40) (cid:114) (cid:41)
b 1
sup P ∥gˆ −g ∥2 ≥ δ2 ≲ (3.17)
n 0 2,∞ n n,g n
P∈P
forsomesequenceδ ,thentheconfidenceregionformulatedinDefinition2.1satisfiesthebound
n,g
sup
(cid:12)
(cid:12)P
(cid:110)
θ
0(x(d))∈Cˆ(x(d))(cid:111) −(1−α)(cid:12)
(cid:12)
P∈P
(cid:18) blog5(dn)(cid:19)1/4 (cid:18) (cid:114)
n
(cid:19)
(cid:112)
≲ + δ2 + ε log(d), (3.18)
n n,g b b
forallsufficientlylargenandb.14
Remark3.1. Theorem3.1followsfromanapplicationofamoregeneralresultstatedinAppendixA.This
resultappliestoconditionalmomentestimatorsthatarenotnecessarilyconstructedwithsubsampledkernels.
14The statistical family P is defined implicitly by the omitted constants in the uniform bounds stated in Definition 3.1 and
Assumption3.3,inadditiontotherestrictionthat∥θ (x(d))∥ isboundedasP variesoverP.
0 ∞15
Rather,theresultholdsunderthehigh-levelassumptionthattheestimatorθˆ (x(d))isapproximatelylinear,
n
withasufficientlysmallremainderterm,andhassufficientlysmallbiasandstochasticequicontinuity. These
conditionsareverifiedforsubsampledkernelregressioninAppendixB.
Severalaspectsofthisargumentarenew. Inparticular,throughastandardseriesofexpansions(seee.g.,
Chernozhukovetal.,2018),weshowthattherootR (x(d))isapproximatedby
n
1 (cid:88) (cid:104) (cid:105)
U (x(d))= E u(x(d);D ,ξ ,θ ,g )|D , where (3.19)
n,b s s 0 0 s
N
b
s∈S
n,b
u(x;D ,ξ ,θ,g)=M(1)(x;g )−1(cid:88)(cid:0) κ(x,X ,D ,ξ )m(D ;θ,g)−E(cid:2) κ(x,X ,D ,ξ )m(D ;θ,g)(cid:3)(cid:1) ,
s s 0 i s s i i s s i
i∈s
with a remainder term given by the second term in (3.18). The quantity (3.19) can be recognized as a
complete,deterministic,U-statisticoforderb.15 WethenapplyanewresultthatdemonstratesthatU-statistics
oforderbareapproximatelylinearwitharemaindertermoforder(b/n)b/2,uptologarithmicfactors. This
isadramaticimprovementoveranalogousresultsgiveninSongetal.(2019)andMinsker(2023),whose
remaindertermsexhibitpolynomialdecayasbandngrowandonlyapplytoregimeb=o(n1/3). Thisregime
isunsuitableforourapplication. ThisresulthasotherapplicationsandisdiscussedindetailinSection4.
In other words, we establish that the root R (x(d)) satisfies a linear representation of the form (2.6)
n
up to a small remainder term. It immediately follows that the bootstrap root R∗(x(d)) satisfies the linear
n
representation(2.7),uptoasmallremainderterm,asitisconstructedwithsubsampling. Weconcludeby
applyingsuitablehigh-dimensionalcentrallimittheorems(Chernozhuokovetal.,2022)tothelinearterms
(2.6)and(2.7). Tohandlethehalf-samplebootstrap,weapplyanargument,basedonacouplingproposedin
Yadlowskyetal.(2023),similartothePoissonizationtrickusedinPræstgaardandWellner(1993). ■
Remark3.2. Thebound(3.18)canbeinterpretedasabias-variancedecomposition. Thefirsttermin(3.18)
resultsfromaboundontheaccuracyofanormalapproximationto(3.19). Theterminvolvingthekernel
shrinkageε isaremnantofaboundonthesupremumofthebiasoftheestimatorθˆ (x(d)). ■
b n
Remark 3.3. It is worth emphasizing that the coverage bound given in Theorem 3.1 is achieved without
assumingthattheestimatorθˆ (x(d))isconstructedwithsample-splitting. Thatis,thenuisanceparameter
n
estimatorgˆ canbecomputedusingthesamedatausedtoevaluatetheconditionalmomentM (·;θ,gˆ ,D ).
n n n n
Often,sample-splittingisnecessarytoensurethatstochasticequicontinuityissufficientlysmall(Chernozhukov
etal.,2018). Avoidingsample-splittingcanbepracticallyimportant;RitzwollerandRomano(2023)show
thattherandomnessinducedbysample-splittingcanbelarge. ■
Theorem3.1statesaboundoncoverageerrorintermsoftwogenericsequences: δ ,expressingtherate
n,g
of convergence of the nuisance parameter estimator gˆ , and ε , measuring the effective bandwidth of the
n b
kernel. Thebound(3.18)exhibitsaninterestingtradeoffbetweentheseobjectsandthechoiceofsubsample
sizeb. Toseethis,supposethat
b=nγ b, ε
b
≲b−γε, and ∥gˆ n−g 0∥
2,∞
≲n−γg , (3.20)
15TheincrementalityconditionspecifiedinDefinition3.1ensuresthatthisU-statisticisnon-degenerate.16
withprobabilitygreaterthan1−1/n,forsomeconstantsγ ,γ ,andγ between0and1. Inthiscase,ignoring
b ε g
logarithmicfactorsandotherconstants,thebound(3.18)canbere-expressedas
γb−1 1−γb−4γg 1−γb(1+2γε)
n 4 +n 2 +n 2 . (3.21)
Inotherwords,theconfidenceregiondefinedinDefinition2.1isconsistentif
1≤γ +4γ and 1≤γ (1+2γ ), (3.22)
b g b ε
respectively. That is, we are able to accommodate larger values of the shrinkage rate ε and nuisance
b
parameterestimationerrorδ ifthesubsamplesizebislarger,relativetothesamplesizen. However,asthe
n,g
subsamplesizebincreases,thenormalapproximationerror(i.e.,thefirsttermin(3.18))increases.
RecallfromSection3.1that,formanypopular,honest,subsampledkernelestimators,theshrinkagerate
ε satisfies a bound ε ≲ b−c/p for some small constant c and some integer p measuring the (potentially,
b b
intrinsic)dimensionofthecovariatesX . Thus,inordertoensurethatthesecondinequalityinconsistency
i
condition(3.22)issatisfied,itisessentialtoaccommodatesubsamplesizesγ closetoone,i.e.,theregime
b
b=o(n). ThisisenabledbythegeneralresultsontheasymptoticlinearityofU-statisticsgiveninSection4.
Ontheotherhand,whenthesubsamplesizescalingfactorγ isclosetoone,therestrictionimposedbythe
b
consistencycondition(3.22)ontherateofconvergenceofthenuisanceparameterestimatorgˆ isveryweak.
n
Observethatthecaseγ =0impliesthefamiliarconditionthatnuisanceparameterscanbeestimatedatthe
b
raten−1/4 inrootmeansquarederror(Chernozhukovetal.,2018). Ifthenuisanceparameterestimatorgˆ is
n
itselfestimatedwithrandomforestregression,SyrgkanisandZampetakis(2020),Chietal.(2022),andHuo
etal.(2023),amongothers,giveconditionsunderwhichsufficientratesofconvergencecanbeachieved.
3.4 Performance. WenowmeasuretheperformanceoftheconfidenceregionformulatedinDefinition2.1.
We apply a method for simulation design proposed by Athey et al. (2021). In particular, we calibrate a
simulationtotheBanerjeeetal.(2015)datausingaGenerativeAdversarialNetwork(GAN)(Goodfellow
et al., 2014). Further details on this calibration are given in Appendix E. In effect, we construct a data
generatingprocessthatapproximatestheBanerjeeetal.(2015)data,whereweknowthetruevalueofthe
CATEθ (x)queriedateachvaluexusedtoconstructthegridsdisplayedinFigures1and2.
0
Measurementsaretakenastwoparametersvary. First,weconsiderseveralvaluesofthesamplesizen. In
particular,weconsidersettingswithn=h·n ,forhin{1,2.5,5,7.5,10},wheren isthesamplesizeofthe
0 0
Banerjeeetal.(2015)data. Second, wevarytheproportionb/n. Weconsiderthreeregimes: b/n = 0.05,
b/n=(2/(h+1))0.05,andb/n=(1/h)0.05. Observethatbincreasesinproportiontoninthefirstregime
andthatbisconstantasnvariesinthethirdregime.16 Thesecondregimeresidesbetweenthesetwoextremes.
Figure 3 displays measurements of the coverage and width of the confidence region formulated in
Definition 2.1, in addition to measurements of the bias of the estimator θˆ (x(d)). The first row displays
n
measurementsofthecoverageoftheconfidenceregion,thecoverageofthelowerbound(i.e.,PanelBof
Figure 2), and the coverage of the upper bound (i.e., Panel A of Figure 2). The nominal level is α = 0.1.
At the observed sample size n = 2438, i.e., h = 1, the confidence region is somewhat anti-conservative.
0
16WechoosethesamevalueofbforconstructingtheempiricalmomentM (·;θ,g,D )andthenuisanceparameterestimatorgˆ .
n n n17
FIGURE 3. Performance
Coverage Lower Coverage Upper Coverage
0.98 0.99
0.95
0.96
0.96
0.90 0.94
0.92 0.93
0.85
0.90
0.80 0.90
0.88
0.75 0.86 0.87
Mean Length Max Bias Mean Bias
0.0040
0.050
0.0035
0.35 0.045
0.0030
0.040
0.30 0.0025
0.035
0.0020
0.030
0.25
1.0 2.5 5.0 7.5 10.0 1.0 2.5 5.0 7.5 10.0 1.0 2.5 5.0 7.5 10.0
( )
Sample Multiplier h
b 1 b 2 b
=0.05(cid:215) =0.05(cid:215) =0.05
n h n h+1 n
Notes:Figure3displaysseveralmeasurementsoftheperformanceoftheconfidenceintervalsformulatedinDefinition2.1ina
simulationcalibratedtotheBanerjeeetal.(2015)data.Thenominallevelisα=0.1;ahorizontaldottedlineisdisplayedatthe
nominalcoverage1−αinthefirstpanel.Theconfidenceboundsconsideredareconstructedanalogouslytotheconfidencebounds
displayedinFigure2.Thex-axisofeachpanelisthesamplemultiplierh.Thecolorofeachmeasurementvarieswiththechoiceof
b/n.FurtherdetailsonthisdesignandimplementationofthissimulationaregiveninAppendixE.
Consistentwiththestructureofthebound(3.18),coveragedoesnotimproveasthesamplesizeincreases
unlesstheproportionb/nalsodecreases. Intheregimeswheretheproportionb/ndecreasesasthesample
multiplierhincreases,coveragebecomesmoderatelyconservative.
ThesecondrowofFigure3illustratesabias-variancetrade-offwiththesubsamplesizeb. Thefirstpanel
displaysmeasurementsoftheaveragewidthoftheconfidenceregion. Here,theaverageistakenoverboth
simulationdrawsandthequery-vectorx(d). Thewidthoftheconfidenceregionisincreasingintheproportion
b/nandisessentiallyconstantifb/nisconstantasnincreases. Bycontrast,thesecondtwopanelsdisplay
themaximumandaveragebiasoftheestimatorθˆ (x(d)),againtakenoverthequery-vectorx(d). Thebiasis
n
decreasingintheproportionb/nandisessentiallyconstantifbisconstantasnvaries.17
17Thedefaultvaluefortheproportionb/nintheGRFRpackageis0.5.Theresultsofthissimulationsuggestthatthisproportion
shouldbereduced,atleastinsettingswherethedimensionofthecovariatevectorX issmall.
i18
4. GENERAL RESULTS FOR HIGH-DIMENSIONALU-STATISTICS
AnessentialstepintheproofofTheorem3.1followsfromaneworder-explicitboundontheremainderina
linearapproximationtoahigh-dimensionalU-statistic. Inthissection,wepresentthisresultandstateseveral
corollaries. Inparticular,wegiveneworder-explicitresultsontheconcentrationandnormalapproximation
ofhigh-dimensionalU-statistics. Thatis,weconsidertheasymptoticbehavioroftheborderU-statistic
1 (cid:88)
U (x(d))= u(x(d);D ), (4.1)
n,b s
N
b
i∈S
n,b
wherethevectoru(x(d);·)collectsthedeterministic,symmetric,real-valuedkernelfunctionu(x;·),evaluated
at the d-vector of points x(d) = (x(j))d in the space X. We assume that each component of the kernel
j=1
functionu(x(d);D )hasmeanzero. ProofsforresultsstatedinthissectionaregiveninAppendixC.
s
4.1 Context. TheasymptoticanalysisofU-statisticswasinitiatedbyHoeffding(1948),whoestablished
a central limit theorem in the regime where the order b is fixed and the sample size n is increasing. The
Hoeffdingcentrallimittheoremhasbeenextendedonlyrecentlytotheregimewheretheorderbincreaseswith
thesamplesizen. DiCiccioandRomano(2022)givearesultwiththisflavorintheregimewhereb=o(n1/2).
Wager and Athey (2018), Peng et al. (2022), and Minsker (2023) are able to strengthen this result to the
regimewhereb=o(n). Westateandprovethismoregeneralresultforthesakeofcompleteness,andbecause
itsmainideaswillserveasusefultouchpointsinthemoreinvolvedanalysistofollow. Weuseanargument
similartotheproofofTheorem3.1ofMinsker(2023). Definethekernelvarianceν2 =Var(u(x(j);D )),
j [b]
theHa´jekprojection
(cid:104) (cid:105)
u(1)(x(j);D)=E u(x(j);D )|D =D , (4.2)
[b] 1
andtheHa´jekprojectionvarianceσ2 =Var(u˜(1)(x(j);D )).
b,j i
Theorem4.1(HoeffdingCentralLimitTheorem).Foranysequenceofkernelordersb=b ,where
n
1
ν2
j
→0 (4.3)
nσ2
b,j
asn→0,wehavethat
(cid:115)
n 1 (cid:88) u(x(j);D )→d N(0,1), (4.4)
σ2 b2N s
b,j b s∈S
n,b
d
asn→∞,where→denotesconvergenceindistribution.
Remark4.1. Theorem4.1isestablishedbyconsideringthedecomposition
 
(cid:115) n (cid:115) n
1 (cid:88) n 1 (cid:88) b (cid:88)
σ2 n
u(1)(x(j);D s)−
σ2 b2

N
u(x(j);D s)−
n
u(1)(x(j);D s) . (4.5)
b,j i=1 b,j b s∈S i=1
n,b
Inparticular,wegiveahigh-probabilityboundforthesecondtermandapplyastandardcentrallimittheorem
tothefirstterm. Thecondition(4.3)isneededtoboundthesecondterm. Asaby-productoftheproof,we
showthatbσ2 ≤ν2. Thus,thenormalization(4.3)impliesthatb=o(n). ■
b,j j19
Large deviation bounds for high-dimensional U-statistics, i.e., U-statistics with vector-valued kernels,
werenotgivenuntilHoeffding(1963). Thisresultisnowmorestandard;amodernversionisstatedasfollows.
AproofisgiveninSongetal.(2019). Thenorm∥·∥ denotestheψ -Orlicznorm.18
ψ1 1
Lemma4.1(LemmaA.5,Songetal.(2019)).Iftheψ -Orlicznormbound
1
∥u(x(j);D )∥ ≤ϕ (4.6)
s ψ1
issatisfiedforeachj in[d],then
(cid:114)
bν2log(dn) bϕlog2(dn)
∥U (x(d))∥ ≲ + (4.7)
n,b ∞
n n
withprobabilitygreaterthan1−C/n,whereν2 =max ν2.
j∈[d] j
Again,Lemma4.1demonstratesthatU (x(d))concentratesintheregimethatb=o(n),uptoalogarithmic
n,b
factorthatdependsonthedimensiond. Here,however,concentrationisexpressedintermsofthequantity
n−1bν2, rather than the more appropriate, and potentially substantively smaller, normalizing quantity
j
n−1b2σ2 usedinTheorem4.1. Inpartmotivatedbythisincongruity,ArconesandGine´ (1993),Arcones
b,j
(1995),Gine´etal.(2000),establishaseriesofrefinedlargedeviationboundsforhigh-dimensionalU-statistics
thatusetheappropriatenormalizingfactor(amongmanyotherrelatedresults). SeeDelaPenaandGine´
(1999)foratextbooktreatment. However,theconstantsusedtoexpresstheseboundsdependimplicitlyon
theorderb,andsoarenotapplicabletoasymptoticregimeswherebmaybegrowingwiththesamplesizen.
More recently, Chen (2018), Chen and Kato (2019), and Song et al. (2019) have studied central limit
theoremsforhigh-dimensionalU-statistics. Ofthesepapers,onlySongetal.(2019)givesresultswithexplicit
dependenceontheorderb. Theirresultsareonlyapplicabletotheregimewhereb=o(n1/3). Minsker(2023)
gives a large deviation bound with the correct normalizing factor and explicit order dependence, but this
resultisagainonlyapplicabletotheregimeb=o(n1/3).
4.2 ConcentrationoftheHa´jekResidual. Weobtainalargedeviationboundonthedifference
n
b (cid:88)
U (x(d))− u(1)(x(d);D ). (4.8)
n,b i
n
i=1
We refer to the quantity (4.8) as the Ha´jek residual (Ha´jek, 1968). This bound is used in the proof of
Theorem 3.1 and implies a new large deviation bound and central limit theorem for high-dimensional
U-statistics,statedinthefollowingsubsection.
Theorem4.2.Definetheterms
ψ2 =max(cid:8) ν2−bσ2 (cid:9) and σ2 = minσ2 . (4.9)
b j b,j b b,j
j∈[d] j∈[d]
Ifthekernelfunctionu(x(j);D )satisfiesthebound (4.6)foreachj in[d],then
s
(cid:114) n (cid:13) b (cid:88)n (cid:13)
(cid:13)U (x(d))− u(1)(x(d);D )(cid:13) ≲ξ , where (4.10)
b2σ2(cid:13) n,b n i (cid:13) ∞ n,b
b i=1
18Randomvariablesaresub-Exponentialifandonlyiftheyhaveafiniteψ -Orlicznorm(Section2.7, Vershynin,2018).
120
 
(cid:18) Cblog(dn)(cid:19)b/2
(cid:32) nψ2(cid:33)1/2
(cid:18) ϕ2blog4(dn)(cid:19)1/2
ξ n,b = n  b2σb 2 + σ2  ,
b b
withprobabilitygreaterthan1−C/n.
Remark 4.2. Roughly speaking, the bound (4.10) follows by first demonstrating that the Ha´jek residual
canbeexpressedasadegenerateU-statisticoforderb. ThisallowsustoderiveHoffman-Jørgensentype
bounds on higher moments of (4.8) with a symmetrization argument. Here, we make essential use of a
symmetrizationinequalityforcompletelydegeneratekernels,withexplicitdependenceontheorder,dueto
Sherman(1994). ThissymmetrizationinequalitywasalsousedinSongetal.(2019)andMinsker(2023). ■
Remark 4.3. The bound (4.10) implies that if b−C ≤ σ2 for some positive constant C, then the Ha´jek
b
residualis(roughly)ofstochasticorder(b/n)b/2 forallsufficientlylargeb. Thisisadramaticimprovement
over existing bounds (Song et al., 2019; Minsker, 2023), which decay polynomially as b and n increase
andonlyconvergetozerointheregimeb=o(n1/3). Inourview,thisresulthintsatanexplanationforthe
widespreadsuccessofsubsamplinginmachinelearningandstatisticalinference. Subsampledstatistics,ofa
largeorder,areessentiallylinear. ■
4.3 ConcentrationandNormalApproximation. Wenowstatealargedeviationboundandcentrallimit
theoremforthehigh-dimensionalU-statistic(4.1). BothresultsarecorollariesofTheorem4.2,applytothe
regimeb=o(n),anddependonthecorrectnormalizingfactor.
Corollary4.1.LetΣbethediagonalmatrixwithcomponentsσ2 .
b,j
(i)UnderthesameconditionsasTheorem4.2,wehavethat
(cid:114) n ϕlog2(dn)
Σ−1/2U (x(d))≲log1/2(dn)+ +ξ (4.11)
b2 n,b σ n1/2 n,b
b
withprobabilitygreaterthan1−C/n.
(ii)LetZ denoteacenteredGaussianrandomvectorwithcovariancematrixVar(u˜(1)(x(d),D )). Underthe
i
sameconditionsasTheorem4.2,wehavethat
(cid:12) (cid:26)(cid:114) (cid:27) (cid:12)
sup (cid:12) (cid:12) (cid:12)P bn 2Σ−1/2U n,b(x(d))∈R −P (cid:110) Σ−1/2Z ∈R(cid:111)(cid:12) (cid:12)
(cid:12)
R∈R
(cid:18) ϕ2log5(dn)(cid:19)1/4
(cid:112)
≲ +ξ log(d), (4.12)
σ2n n,b
b
whereRdenotesthesetofhyper-rectanglesinRd.
Remark4.4. DelaPenaandGine´ (1999)statearesultanalogoustoPart(i)ofCorollary4.1,inthesense
thattheU-statisticU (x(d))isnormalizedbythecorrectquantityn1/2b−2Σ−1/2. Theconstantsusedin
n,b
theirbounddependimplicitlyonb. Part(i)ofCorollary4.1improvessubstantiallyonLemma4.1incontexts
wheretheHa´jekprojectionvariancesσ2 aresmallerthanb−1.19 Undertherestrictionσ2 ≳b−1,imposed
b,j b,j
in the application to subsampled kernel regression considered in Section 3, Part (i) of Corollary 4.1 only
19Section4ofSongetal.(2019)givesseveralexamplesofstatisticswheretheHa´jekprojectionvarianceσ2 issmallerthanb−1.
b,j21
improvesonLemma4.1byaconstantfactor. Part(ii)ofCorollary4.1givesahigh-dimensionalequivalentto
Theorem4.1. Ananalogoushalf-samplebootstrapcentrallimittheoremfollowsfromargumentsverysimilar
topartsoftheproofofTheorem3.1. ■
5. CONCLUSION
Weproposeaconfidenceregionforsolutionstoconditionalmomentequations. Theconfidenceregionis
builtaroundanestimatorbasedonsubsampledkernelregression. Asarunningexample,weconsiderthe
constructionofconfidenceregionsforconditionalaveragetreatmenteffectsaroundaGeneralizedRandom
Forest(Atheyetal.,2019). Empirically,wedocumentthattheproposedconfidenceregionisabletorecover
treatmenteffectheterogeneityundetectedbyinteractedlinearregression. Theoretically,weestablishabound
oncoverageaccuracythatillustratesabias-variancetradeoffintheuser-chosensubsamplesize. Inorderto
dothis,weobtainseveralnewresultsontheasymptoticsofhigh-dimensionalU-statistics.
Wegiveconditionssufficientfortheasymptoticvalidityoftheproposedconfidenceregions. However,the
confidenceregionisnotnecessarilyoptimal,inanyparticularsense,underthemaintainedassumptions. Itis
likelytobethecaseanoptimalconfidenceregionwouldneedtoincorporateabiasestimateandprocedure
for choosing tuning parameters to balance bias and variance (see e.g., Chernozhukov et al. (2014), Chen
etal.(2024)forinstantiationsoftheseideas). Adaptingthisapproachtosubsampledmomentregressionisan
interestingdirectionforfurtherresearch.22
REFERENCES
Adler,R.J.andTaylor,J.E.(2009). Randomfieldsandgeometry. SpringerScience&BusinessMedia.
Ai,C.andChen,X.(2003). Efficientestimationofmodelswithconditionalmomentrestrictionscontaining
unknownfunctions. Econometrica,71(6):1795–1843.
Angrist,J.D.,Imbens,G.W.,andRubin,D.B.(1996). Identificationofcausaleffectsusinginstrumental
variables. JournaloftheAmericanstatisticalAssociation,91(434):444–455.
Arcones,M.A.(1995). Abernstein-typeinequalityforu-statisticsandu-processes. Statistics&probability
letters,22(3):239–247.
Arcones, M. A. and Gine´, E. (1993). Limit theorems for u-processes. The Annals of Probability, pages
1494–1542.
Armstrong,T.B.andKolesa´r,M.(2020). Simpleandhonestconfidenceintervalsinnonparametricregression.
QuantitativeEconomics,11(1):1–39.
Athey,S.,Chetty,R.,Imbens,G.,andKang,H.(2020). Estimatingtreatmenteffectsusingmultiplesurrogates:
Theroleofthesurrogatescoreandthesurrogateindex.
Athey,S.andImbens,G.(2016). Recursivepartitioningforheterogeneouscausaleffects. Proceedingsofthe
NationalAcademyofSciences,113(27):7353–7360.
Athey, S., Imbens, G. W., Metzger, J., and Munro, E. (2021). Using wasserstein generative adversarial
networksforthedesignofmontecarlosimulations. JournalofEconometrics.
Athey, S., Tibshirani, J., and Wager, S. (2019). Generalized random forests. The Annals of Statistics,
47(2):1148–1178.
Balboni, C., Bandiera, O., Burgess, R., Ghatak, M.,andHeil, A.(2022). Whydopeoplestaypoor? The
QuarterlyJournalofEconomics,137(2):785–844.
Banerjee,A.,Duflo,E.,Goldberg,N.,Karlan,D.,Osei,R.,Pariente´,W.,Shapiro,J.,Thuysbaert,B.,and
Udry,C.(2015). Amultifacetedprogramcauseslastingprogressfortheverypoor: Evidencefromsix
countries. Science,348(6236):1260799.
Belloni, A., Chernozhukov, V., Chetverikov, D., andWei, Y.(2018). Uniformlyvalidpost-regularization
confidence regions for many functional parameters in z-estimation framework. Annals of statistics,
46(6B):3643.
Biau,G.,Devroye,L.,andLugosi,G.(2008). Consistencyofrandomforestsandotheraveragingclassifiers.
JournalofMachineLearningResearch,9(9).
Boucheron,S.,Lugosi,G.,andMassart,P.(2013). Concentrationinequalities: Anonasymptotictheoryof
independence,(2013).
Breiman,L.(1996). Baggingpredictors. Machinelearning,24:123–140.
Breiman,L.(2001). Randomforests. Machinelearning,45:5–32.
Breiman,L.,Friedman,J.,Olshen,R.,andStone,C.J.(1984). Classificationandregressiontrees. Routledge.
Bu¨hlmann,P.andYu,B.(2002). Analyzingbagging. TheannalsofStatistics,30(4):927–961.
Cattaneo,M.D.,Klusowski,J.M.,andTian,P.M.(2024). Onthepointwisebehaviorofrecursivepartitioning
anditsimplicationsforheterogeneouscausaleffectestimation.
Chen,J.andRitzwoller,D.M.(2023). Semiparametricestimationoflong-termtreatmenteffects. Journalof
Econometrics,237(2):105545.23
Chen, X. (2018). Gaussian and bootstrap approximations for high-dimensional u-statistics and their
applications.
Chen,X.,Christensen,T.,andKankanala,S.(2024). Adaptiveestimationanduniformconfidencebandsfor
nonparametricstructuralfunctionsandelasticities. TheReviewofEconomicStudies.
Chen, X. and Christensen, T. M. (2018). Optimal sup-norm rates and uniform inference on nonlinear
functionals of nonparametric iv regression: Nonlinear functionals of nonparametric iv. Quantitative
Economics,9(1):39–84.
Chen, X. and Kato, K. (2019). Randomized incomplete U-statistics in high dimensions. The Annals of
Statistics,47(6):3127–3156.
Chen, X. and Pouzo, D. (2012). Estimation of nonparametric conditional moment models with possibly
nonsmoothgeneralizedresiduals. Econometrica,80(1):277–321.
Chernozhukov,V.,Chetverikov,D.,Demirer,M.,Duflo,E.,Hansen,C.,Newey,W.,andRobins,J.(2018).
Double/debiased machine learning for treatment and structural parameters: Double/debiased machine
learning. TheEconometricsJournal,21(1).
Chernozhukov,V.,Chetverikov,D.,andKato,K.(2013). Gaussianapproximationsandmultiplierbootstrap
formaximaofsumsofhigh-dimensionalrandomvectors.
Chernozhukov,V.,Chetverikov,D.,andKato,K.(2014). Anti-concentrationandhonest,adaptiveconfidence
bands. TheAnnalsofStatistics,42(5):1787–1818.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2017a). Central limit theorems and bootstrap in high
dimensions. TheAnnalsofProbability,45(4):2309–2352.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2017b). Detailed proof of nazarov’s inequality. arXiv
preprintarXiv:1711.10696.
Chernozhukov,V.,Escanciano,J.C.,Ichimura,H.,Newey,W.K.,andRobins,J.M.(2022). Locallyrobust
semiparametricestimation. Econometrica,90(4):1501–1535.
Chernozhukov,V.,Newey,W.K.,andSantos,A.(2023). Constrainedconditionalmomentrestrictionmodels.
Econometrica,91(2):709–736.
Chernozhuokov,V.,Chetverikov,D.,Kato,K.,andKoike,Y.(2022). Improvedcentrallimittheoremand
bootstrapapproximationsinhighdimensions. TheAnnalsofStatistics,50(5):2562–2586.
Chi,C.-M.,Vossler,P.,Fan,Y.,andLv,J.(2022). Asymptoticpropertiesofhigh-dimensionalrandomforests.
TheAnnalsofStatistics,50(6):3415–3438.
Chung, E. and Romano, J. P. (2013). Exact and asymptotically robust permutation tests. The Annals of
Statistics,41(2):484–507.
DelaPena,V.andGine´,E.(1999). Decoupling: fromdependencetoindependence. Springer.
DiCiccio,C.andRomano,J.(2022). Cltforu-statisticswithgrowingdimension. StatisticaSinica,32(1).
Efron,B.andStein,C.(1981). Thejackknifeestimateofvariance. TheAnnalsofStatistics,pages586–596.
Fix, E. and Hodges, J. L. (1989). Discriminatory analysis. nonparametric discrimination: Consistency
properties. InternationalStatisticalReview/RevueInternationaledeStatistique,57(3):238–247.
Foster,D.J.andSyrgkanis,V.(2023). Orthogonalstatisticallearning. TheAnnalsofStatistics,51(3):879–908.
Fro¨lich,M.(2007). Nonparametricivestimationoflocalaveragetreatmenteffectswithcovariates. Journal
ofEconometrics,139(1):35–75.
Gine´, E., Latała, R., and Zinn, J. (2000). Exponential and moment inequalities for u-statistics. In High
DimensionalProbabilityII,pages13–38.Springer.24
Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,Courville,A.,andBengio,
Y.(2014). Generativeadversarialnets. Advancesinneuralinformationprocessingsystems,27.
Go¨tze, F., Sambale, H., and Sinulis, A. (2021). Concentration inequalities for polynomials in α-sub-
exponentialrandomvariables. ElectronicJournalofProbability,26(none):1–22.
Hahn, J. (1998). On the role of the propensity score in efficient semiparametric estimation of average
treatmenteffects. Econometrica,pages315–331.
Ha´jek, J. (1968). Asymptotic normality of simple linear rank statistics under alternatives. The Annals of
MathematicalStatistics,pages325–346.
Hanson,D.L.andWright,F.T.(1971). Aboundontailprobabilitiesforquadraticformsinindependent
randomvariables. TheAnnalsofMathematicalStatistics,42(3):1079–1083.
Hoeffding, W. (1948). A class of statistics with asymptotically normal distribution. Ann. Math. Statist.,
19(4):293–325.
Hoeffding, W. (1963). Probability inequalities for sums of bounded random variables. Journal of the
AmericanStatisticalAssociation,pages13–30.
Huo,Y.,Fan,Y.,andHan,F.(2023). Ontheadaptationofcausalforeststomanifolddata. arXivpreprint
arXiv:2311.16486.
Ichimura,H.andNewey,W.K.(2022). Theinfluencefunctionofsemiparametricestimators. Quantitative
Economics,13(1):29–61.
Imbens, G. W. and Rubin, D. B. (2015). Causal inference in statistics, social, and biomedical sciences.
CambridgeUniversityPress.
Kennedy,E.H.(2023). Towardsoptimaldoublyrobustestimationofheterogeneouscausaleffects. Electronic
JournalofStatistics,17(2):3008–3049.
Khosravi,K.,Lewis,G.,andSyrgkanis,V.(2019). Non-parametricinferenceadaptivetointrinsicdimension.
arXivpreprintarXiv:1901.03719.
Kpotufe, S. (2011). k-nn regression adapts to local intrinsic dimension. Advances in neural information
processingsystems,24.
Kraay,A.andMcKenzie,D.(2014). Dopovertytrapsexist? assessingtheevidence. JournalofEconomic
Perspectives,28(3):127–148.
Kuchibhotla,A.K.,Balakrishnan,S.,andWasserman,L.(2023). Medianregularityandhonestinference.
Biometrika,110(3):831–838.
Lee,A.(1990). U-statistics: theoryandpractice. Taylor&Francis.
Lewis,G.andSyrgkanis,V.(2021). Double/debiasedmachinelearningfordynamictreatmenteffects. In
NeurIPS,pages22695–22707.
Li, K.-C. (1989). Honest confidence regions for nonparametric regression. The Annals of Statistics,
17(3):1001–1008.
Lin, Y. and Jeon, Y. (2006). Random forests and adaptive nearest neighbors. Journal of the American
StatisticalAssociation,101(474):578–590.
Mentch, L.andHooker, G.(2014). Ensembletreesandclts: Statisticalinferenceforsupervisedlearning.
stat,1050:25.
Mentch,L.andHooker,G.(2016). Quantifyinguncertaintyinrandomforestsviaconfidenceintervalsand
hypothesistests. TheJournalofMachineLearningResearch,17(1):841–881.25
Minsker,S.(2023). U-statisticsofgrowingorderandsub-gaussianmeanestimatorswithsharpconstants.
MathematicalStatisticsandLearning.
Montgomery-Smith,S.J.(1993).Comparisonofsumsofindependentidenticallydistributedrandomvariables.
ProbabilityandMathematicalStatistics,14:281–285.
Newey,W.K.(1993). Efficientestimationofmodelswithconditionalmomentrestrictions. InEconometrics,
volume11ofHandbookofStatistics,pages419–454.Elsevier.
Newey,W.K.(1994). Theasymptoticvarianceofsemiparametricestimators. Econometrica: Journalofthe
EconometricSociety,pages1349–1382.
Newey, W. K. and McFadden, D. (1994). Large sample estimation and hypothesis testing. Handbook of
econometrics,4:2111–2245.
Nie, X. and Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika,
108(2):299–319.
Oprescu, M., Syrgkanis, V., and Wu, Z. S. (2019). Orthogonal random forest for causal inference. In
InternationalConferenceonMachineLearning,pages4932–4941.PMLR.
Peng, W., Coleman, T., and Mentch, L. (2022). Rates of convergence for random forests via generalized
u-statistics. ElectronicJournalofStatistics,16(1):232–292.
Politis,D.N.andRomano,J.P.(1994). Largesampleconfidenceregionsbasedonsubsamplesunderminimal
assumptions. TheAnnalsofStatistics,pages2031–2050.
Politis,D.N.,Romano,J.P.,andWolf,M.(1999). Subsampling. SpringerScience&BusinessMedia.
Præstgaard,J.andWellner,J.A.(1993). Exchangeablyweightedbootstrapsofthegeneralempiricalprocess.
TheAnnalsofProbability,pages2053–2086.
Ritzwoller, D. M. and Romano, J. P. (2023). Reproducible aggregation of sample-split statistics. arXiv
preprintarXiv:2311.14204.
Robins, J. M., Rotnitzky, A., and Zhao, L. P. (1994). Estimation of regression coefficients when some
regressorsarenotalwaysobserved. JournaloftheAmericanstatisticalAssociation,89(427):846–866.
Romano,J.P.andShaikh,A.M.(2012). Ontheuniformasymptoticvalidityofsubsamplingandthebootstrap.
TheAnnalsofStatistics,40(6).
Scornet,E.,Biau,G.,andVert,J.-P.(2015). Consistencyofrandomforests. AnnalsofStatistics,43(4):1716–
1741.
Sexton,J.andLaake,P.(2009). Standarderrorsforbaggedandrandomforestestimators. Computational
Statistics&DataAnalysis,53(3):801–811.
Sherman,R.P.(1994). Maximalinequalitiesfordegenerateu-processeswithapplicationstooptimization
estimators. TheAnnalsofStatistics,22(1):439–459.
Singh,R.andVijaykumar,S.(2023). Kernelridgeregressioninference. arXivpreprintarXiv:2302.06578.
Song, Y., Chen, X., and Kato, K. (2019). Approximating high-dimensional infinite-order u-statistics:
Statisticalandcomputationalguarantees. ElectronicJournalofStatistics,13(2).
Syrgkanis,V.andZampetakis,M.(2020). Estimationandinferencewithtreesandforestsinhighdimensions.
InConferenceonlearningtheory,pages3453–3454.PMLR.
Tan,Z.(2006). Regressionandweightingmethodsforcausalinferenceusinginstrumentalvariables. Journal
oftheAmericanStatisticalAssociation,101(476):1607–1618.
vanderVaart,A.W.andWellner,J.A.(2013). Weakconvergenceandempiricalprocesses: withapplications
tostatistics. Springer.26
Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data science,
volume47. Cambridgeuniversitypress.
Wager,S.andAthey,S.(2018). Estimationandinferenceofheterogeneoustreatmenteffectsusingrandom
forests. JournaloftheAmericanStatisticalAssociation,113(523):1228–1242.
Wager,S.,Hastie,T.,andEfron,B.(2014). Confidenceintervalsforrandomforests: Thejackknifeandthe
infinitesimaljackknife. TheJournalofMachineLearningResearch,15(1):1625–1651.
Yadlowsky,S.,Fleming,S.,Shah,N.,Brunskill,E.,andWager,S.(2023). Evaluatingtreatmentprioritization
rulesviarank-weightedaveragetreatmenteffects.SupplementalAppendixto:
Uniform Inference for Subsampled Moment Regression*
DavidM.Ritzwoller VasilisSyrgkanis
StanfordUniversity StanfordUniversity
Contents
AppendixA. AnAbstractBoundonCoverageError 1
A.1. Assumptions 1
A.2. Coverage 3
A.3. ProofofTheoremA.1 3
A.4. ProofsforSupportingLemmas 6
AppendixB. ProofofTheorem3.1 17
B.1. ProofsforSupportingLemmas 20
AppendixC. ProofsforResultsStatedinSection4 28
C.1. ProofofTheorem4.1 28
C.2. ProofofTheorem4.2 29
C.3. ProofofCorollary4.1,Part(i) 34
C.4. ProofofCorollary4.1,Part(ii) 34
AppendixD. AdditionalResultsandDiscussion 35
D.1. ASmallSurveyofHeterogeneityAssessment 35
D.2. Binomial-SampleBootstrap 36
AppendixE. DetailsConcerningDataandSimulations 40
E.1. Data 40
E.2. ParameterChoices 41
E.3. SimulationCalibration 41
*Date:May14,20241
APPENDIX A. AN ABSTRACT BOUND ON COVERAGE ERROR
Inthisappendix,wegiveanabstractboundontheaccuracyofthenominalcoverageprobabilityforthe
confidenceregionintroducedinDefinition2.1. Thatis,wemakenouseofthekernelstructureexpressed
in (1.5). We specify a set of high-level assumptions in Appendix A.1. The abstract bound is stated in
AppendixA.2andprovedinAppendixA.3. ProofsforsupportingLemmasaregiveninAppendixA.4.
A.1 Assumptions. WerequireseveralmildsmoothnessrestrictionsonthemomentfunctionM(·;θ,g). In
contrasttothesetofassumptionsspecifiedinSection3.2,wedonotrequiremomentlinearity. Instead,we
imposethefollowinggeneralizationofPart(iii)ofAssumption3.3.
AssumptionA.1(MomentRestrictions).ThemomentfunctionM(x;θ ,g )istwicecontinuouslydifferentiable
0 0
initssecondargument. Let
∂ ∂2
M(1)(x;θ,g)= M(x;θ′,g )| and H(x;θ,g)= M(x;θ′,g )| (A.1)
∂θ′ 0 θ′=θ ∂2θ′ 0 θ′=θ
denotetheJacobianandHessianofM(·;θ ,g )inθ,respectively. TheJacobianM(1)(x;θ,g)isuniformly
0 0
Lipschitzinitssecondargumentandboundedfrombelowinthesensethat
sup sup (cid:12) (cid:12)M(1)(x(j);g)−M(1)(x(j);g 0)(cid:12) (cid:12)≲∥g−g 0∥
2,∞
and (A.2)
P∈Pj∈[d]
inf inf
(cid:12) (cid:12)M(1)(x(j);g)(cid:12)
(cid:12)≥c (A.3)
P∈Pj∈[d]
for each g in G and some positive constant c. The Hessian H(x;θ,g) is uniformly bounded as x, θ, and g
varyovertheirrespectivedomains.
WeadditionallyrequireNeymanorthogonalityandsecondordersmoothness,i.e.,Part(i)ofAssumption3.3.
Moreover,weimposeananalogoussmoothnessrestrictiononthecenteredempiricalmoment
M (x;θ,g)=M (x;θ,g,D )−E[M (x;θ,g,D )] , (A.4)
n n n n n
wherewehavemadethedependenceonD implicittoeasenotation.
n
AssumptionA.2(EmpiricalSmoothness).ThecenteredempiricalmomentM (x;θ,g)istwicecontinuously
n
differentiableinitssecondargument. Let
∂ M (x;θ′,g )| =M(1) (x;θ,g) and ∂2 M (x;θ′,g )| =H (x;θ,g) (A.5)
∂θ′ n 0 θ′=θ n ∂2θ′ n 0 θ′=θ n
denote the Jacobian and Hessian of M (·;θ,g) in θ, respectively. The Hessian H (x;θ,g) is uniformly
n n
boundedalmostsurelyasx,θ,andg varyovertheirrespectivedomains.
Next,weimposeasetofhigh-levelrestrictionsonthestructureoftheempiricalconditionalmoment(2.1).
Attimeswerefertothenormalizedstatistic
U (x)=−(M(1)(x;g ))−1M (x;θ ,g ). (A.6)
n 0 n 0 02
First, we impose a condition that ensures that (A.6) is approximately linear. Recall that the norm ∥·∥
ψ1
denotestheψ -Orlicznorm.
1
AssumptionA.3(ApproximateLinearity).Thereexistsafunctionu(·,·),aconstantφ≥1,andreal-valued
sequencesδ andρ suchthatE(cid:2) u(x(j),D )(cid:3) =0,
n,u n,u i
(cid:104) (cid:105)
∥u(x(j),D )∥ ≤φ, and E u4(x(j),D ) ≤Var(u(x(j),D ))φ2 (A.7)
i ψ1 i i
holdforallj in[d]andP inP. Moreover,ifλ2 denotesVar(u(x(j),D ))andλ2 =min λ2,then
j i j∈[d] j
(cid:40) (cid:114) n (cid:41)
sup P λn 2(cid:13) (cid:13)U n(x(d))− n1 (cid:88) u(x(d),D i)(cid:13) (cid:13) ∞ ≥δ n,u ≤ρ n,u . (A.8)
P∈P
i=1
RemarkA.1. Condition(A.8)enablestheapplicationoftherepresentation(2.7). Thebounds(A.7)hold,for
example,ifthequantitiesu (x(j),D )areboundedbyφ. ■
n i
Second,weimposeseveralrestrictionsrelatingtotheestimatorsθˆ (x(d))andgˆ . Throughout,wemeasure
n n
theerrorintheestimatorofθˆ (x(d))intermsofthenorm
n
∥θˆ (x(d))−θ (x(d))∥ = sup |θˆ (x(j))−θ (x(j))|. (A.9)
n 0 ∞ n 0
j∈[d]
A closely related collection of conditions, in the context of estimation of the solution to unconditional
orthogonalmoments,isstatedasAssumption3.2ofChernozhukovetal.(2018).
AssumptionA.4(Bias,Consistency,andStochasticEquicontinuity).Recallthedefinitionoftheobjectλ2
introducedinAssumptionA.3.
(i)Definethequantity
Bias (x;θ,g)=M(x;θ,g)−E[M (x;θ,g,D )] . (A.10)
n n n
Thereexistsasequenceδ suchthat
n,B
(cid:114)
n
sup sup ∥Bias (x(d);θ(x(d)),g)∥ ≲(1+∥θ(x(d))∥ )δ (A.11)
λ2 n ∞ ∞ n,B
P∈Pg∈G
uniformlyoveranyvectorθ(x(d))={θ(x(j))}d .
j=1
(ii)Thereexistsequencesδ ,δ ,δ ,ρ ,ρ ,andρ suchthat
n,m n,g n,θ n,m n,g n,θ
(cid:26)(cid:114) (cid:27)
sup P λn 2(cid:13) (cid:13)M( n1) (x(d);θ 0(x(d)),g 0)(cid:13) (cid:13)2
∞
≥δ
n,m
≤ρ
n,m
, (A.12)
P∈P
(cid:26)(cid:114) (cid:27)
n
sup P ∥gˆ −g ∥2 ≥δ2 ≤ρ , and (A.13)
λ2 n 0 2,∞ n,g n,g
P∈P
(cid:26)(cid:114) (cid:27)
n
sup P ∥θˆ (x(d))−θ (x(d))∥2 ≥δ2 ≤ρ . (A.14)
λ2 n 0 ∞ n,θ n,θ
P∈P3
(iii)Thereexistsequencesδ ,δ ,ρ ,andρ suchthat
n,S n,J n,S n,J
(cid:26)(cid:114) (cid:27)
sup P λn 2(cid:13) (cid:13)M n(x(d);θ 0(x(d)),gˆ n)−M n(x(d);θ 0(x(d)),g 0)(cid:13) (cid:13) ∞ ≥δ n,S ≤ρ n,S . (A.15)
P∈P
and
(cid:26)(cid:114) (cid:27)
sup P λn 2(cid:13) (cid:13)M( n1) (x(d);θ 0(x(d)),gˆ n)−M( n1) (x(d);θ 0(x(d)),g 0)(cid:13) (cid:13) ∞ ≥δ n,J ≤ρ n,J . (A.16)
P∈P
respectively.
A.2 Coverage. Thefollowingtheoremgivesanon-asymptoticboundontheerrorinthenominalcoverage
probabilityoftheconfidenceregionsintroducedinDefinition2.1.
TheoremA.1(GenericCoverageErrorDecomposition).Collecttheerrorsequences
(cid:16) (cid:17)
δ =δ2 +δ2 +δ +δ +δ +δ δ +δ +λ1/2n−1/4(cid:0) δ +δ (cid:1) and
n n,g n,θ n,B n,S n,u n,θ n,m n,g n,B n,J
ρ =ρ +ρ +ρ +ρ +ρ +ρ
n n,m n,g n,θ n,S n,J n,u
and assume that δ ≤ C δ and ρ ≤ C ρ for any 0 < ε < 1. Suppose that the Neyman orthogonal
εn ε n εn ε n
momentfunctionM(x;θ ,g )satisfiesAssumptionA.1andPart(i)ofAssumption3.3andthatthecentered
0 0
empiricalmomentfunctionM (x;θ ,g )satisfiesAssumptionA.2. IfAssumptionsA.3andA.4hold,then
n 0 0
theconfidenceregiondefinedinDefinition2.1satisfies
sup
(cid:12)
(cid:12)P
(cid:110)
θ
0(x(d))∈Cˆ(x(d))(cid:111) −(1−α)(cid:12) (cid:12)≲(cid:18) φ2lo λg 25 n(dn)(cid:19)1/4
+δ
n(cid:112)
logd+ρ
n
. (A.17)
P∈P
RemarkA.2. TheoremA.1isverifiedbyconsideringthedecomposition
(cid:32) n (cid:33) n
√ √ 1 (cid:88) 1 (cid:88)
nR (x(d))= n R (x(d))− u(x(d),D ) + √ u(x(d),D ). (A.18)
n n i i
n n
i=1 i=1
First,throughastandardseriesofexpansions(seee.g.,Chernozhukovetal.,2018),weshowthatthefirst
termin(A.18)isboundedabovebyδ withprobabilitygreaterthan1−ρ . Ananalogousboundholdsfor
n n
thebootstraprootR∗(x(d)),asthehalf-samplebootstraprootisobtainedwithsubsampling. Thesebounds
n
producethelattertermsontheright-handsideof(A.17).
The first term on the right-hand side of (A.17) is obtained through the application of suitable of high-
dimensionalcentrallimittheoremsforthesecondtermin(A.18)(Chernozhuokovetal.,2022). Inthecaseof
thebootstraproot,thecentrallimittheoremthatweapplyleveragestherepresentations(2.7). Inparticular,
weapplyanargumentbasedonacouplingproposedinYadlowskyetal.(2023). Basically,theweightsinthe
sums(2.7)arecoupledwithasequenceofindependentandidenticallydistributedweightsandthedifference
betweenthesumsusingthetwotypesofweightsarebounded. ThisboundusesaLe´vy-typegeneralizationof
anappropriateBernstein-typebound. ■4
A.3 ProofofTheoremA.1. Throughout,weletRdenotethesetofhyper-rectanglesinRd. Fixarectangle
R=[a ,a ]inR,wherea anda arevectorsinRdwitha ≤a ,interpretedcomponentwise. Foreacht>0,
l u l u l u
definetheenlargedrectangleR =[a −t1 ,a +t1 ]. LetZ denoteacenteredGaussianrandomvectorwith
t l d u d
covariancematrixVar(u(x(d),D )). LetΛbethediagonalmatrixwithcomponentsλ2 =Var(u(x(j),D )).
i j i
Wehavethat
(cid:12) (cid:12)P (cid:110)√ nΛˆ− n1/2 R n(x(d))∈R(cid:111) −P (cid:110) Λ−1/2Z ∈R(cid:111)(cid:12) (cid:12)
≤(cid:12) (cid:12)P (cid:110)√ nΛ−1/2R n(x(d))∈R t(cid:111) −P (cid:110) Λ−1/2Z ∈R t(cid:111)(cid:12) (cid:12) (A.19)
+(cid:12) (cid:12)P (cid:110) Λ−1/2Z ∈R t(cid:111) −P (cid:110) Λ−1/2Z ∈R(cid:111)(cid:12) (cid:12) (A.20)
(cid:110)√ (cid:111)
+P n∥(Λ−1/2−Λˆ−1/2 )R (x(d))∥ ≥t , (A.21)
n n ∞
andthatsimilarly
(cid:12) (cid:12)P (cid:110)√ nΛˆ− n1/2 R n∗(x(d))∈R|D n(cid:111) −P (cid:110) Λ−1/2Z ∈R(cid:111)(cid:12) (cid:12)
(cid:110)√ (cid:111) (cid:110) (cid:111)
≤|P nΛ−1/2R∗(x(d))∈R |D −P Λ−1/2Z ∈R | (A.22)
n t n t
(cid:110) (cid:111) (cid:110) (cid:111)
+|P Λ−1/2Z ∈R −P Λ−1/2Z ∈R | (A.23)
t
(cid:110)√ (cid:111)
+P n∥(Λ−1/2−Λˆ−1/2 )R∗(x(d))∥ ≥t|D , (A.24)
n n ∞ n
foranyt>0. Weprovideboundsforeachterm,(A.19)through(A.24).
ToboundtheGaussianapproximationerrors(A.19)and(A.22),weapplythefollowingTheorem,which
establishesagenericquantitativecentrallimitforthestatisticR inadditiontogenericquantitativeconditional
n
centrallimittheoremsforbothbootstrapprocedures.
Theorem A.2.Suppose that the moment function M(x;θ ,g ) satisfies Assumption A.1 and Part (i) of
0 0
Assumption3.3andthatAssumptionsA.3andA.4hold.
(i)Theinequality
(cid:12) (cid:110)√ (cid:111) (cid:12) φ1/2 (cid:18) log5(dn)(cid:19)1/4 (cid:112)
sup sup (cid:12)P nR (x(d))∈R −P {Z ∈R}(cid:12)≲ +δ logd+ρ (A.25)
R∈RP∈P(cid:12) n (cid:12) λ1/2 n n n
holds.
(ii)IfthebootstraprootisconstructedwiththeHalf-Samplebootstrap,thentheinequality
(cid:12) (cid:110)√ (cid:111) (cid:12) φ1/2 (cid:18) log5(dn)(cid:19)1/4 (cid:112)
sup sup (cid:12)P nR∗(x(d))∈R|D −P {Z ∈R}(cid:12)≲ +δ logd (A.26)
R∈RP∈P(cid:12) n n n (cid:12) λ1/2 n n
holdswithprobabilitygreaterthan1−Cn−1/2φλ−1log3/2(dn)−ρ .
n
ToboundthedifferencesintheGaussianprobabilities(A.20)and(A.23), weapplythefollowinganti-
concentrationinequality,statedinChernozhukovetal.(2017b)andoftenreferredtoasNazarov’sinequality.5
LemmaA.1(Theorem1, Chernozhukovetal.,2017b).LetZ = (Z )d beacenteredGaussianrandom
j j=1
vector in Rd such that E[Z2] ≥ c for all j in [d] and some constant c. For every z ∈ Rd and t > 0, the
j
inequality
t(cid:112)
P {Z ≤z+t}−P {Z ≤z}≲ logd
c
holds.
Inparticular,wehavethat
(cid:110) (cid:111) (cid:110) (cid:111) (cid:112)
|P Λ−1/2Z ∈R −P Λ−1/2Z ∈R |≤t logd
t
forallt>0.
Finally,toboundtheterms(A.21)and(A.24)resultingfromvarianceestimation,weapplythefollowing
boundontheaccuracyofbootstrapvarianceestimateλˆ2 .
n,j
LemmaA.2.SupposethatAssumptionA.3holds. IfthebootstraprootisconstructedwiththeHalf-Sample
bootstrap,then
P (cid:40) sup (cid:12) (cid:12) (cid:12) (cid:12)λˆ λ2 n 2,j −1(cid:12) (cid:12) (cid:12) (cid:12)≥C λφ 22 nlog2(dn)+C n1 δ n2(cid:41) ≲1−C(ρ n+n−1). (A.27)
j∈[d] j
ToapplyLemmaA.2,observethattheBorell-TISinequality(e.g.,Theorem2.1.1ofAdlerandTaylor,2009)
impliesthat
(cid:110) (cid:112) (cid:111)
P ∥Λ−1/2Z∥ ≥C logdn ≤n−1 .
∞
Thus,TheoremA.2impliesthat
(cid:110) (cid:112) (cid:111)
(cid:18) φ2log5(dn)(cid:19)1/4
(cid:112)
P ∥Λ−1/2R (x(dn))∥ ≥C logdn ≲ +δ logd+ρ .
n ∞ λ2n n n
andthat
(cid:110) (cid:112) (cid:111)
(cid:18) φ2log5(dn)(cid:19)1/4
(cid:112)
P ∥Λ−1/2R∗(x(d))∥ ≥C logdn|D ≲ +δ logd
n ∞ n λ2n n
withprobabilitygreaterthan1−Cn−1/2φλ−1log3/2(dn)−ρ . Hence,LemmaA.2impliesthat
n
sup (cid:12) (cid:12)λˆ n,j/λ
j
−1(cid:12) (cid:12)≤ sup (cid:12) (cid:12)λˆ2 n,j/λ2
j
−1(cid:12) (cid:12)≲ λφ 22 nlog2(dn)+ n1 δ n2
j∈[d] j∈[d]
withprobabilityatleast1−C(ρ +n−1). Thus,wehavethat
n
P (cid:26) ∥(Λ−1/2−Λˆ−1/2 )R (x(d))∥ ≳(cid:18) φ2 log2(dn)+ 1 δ2(cid:19) (cid:112) logdn(cid:27)
n n ∞ λ2n n n
(cid:18) φ2log5(dn)(cid:19)1/4
(cid:112)
≲ +δ logd+ρ
λ2n n n6
andthat
P (cid:26) ∥(Λ−1/2−Λˆ−1/2 )R∗(x(d))∥ ≳(cid:18) φ2 log2(dn)+ 1 δ2(cid:19) (cid:112) logdn|D (cid:27)
n n ∞ λ2n n n n
(cid:18) φ2log5(dn)(cid:19)1/4
(cid:112)
≲ +δ logd
λ2n n
withprobabilitygreaterthan1−Cn−1/2φλ−1log3/2(dn)−ρ .
n
Puttingthepiecestogether,bysetting
(cid:18) φ2 1 (cid:19) (cid:112)
t= log2(dn)+ δ2 logdn,
λ2n n n
wehavethat
(cid:110) (cid:111)
sup sup |P Λˆ−1/2 R (x(d))∈R −P {Z ∈R}|
n n
R∈RP∈P
(cid:18) φ2log5(dn)(cid:19)1/4 (cid:112) (cid:18) φ2log2(dn 1 (cid:19)
≲ +δ log(d)+ )+ δ2 log(dn)+ρ (A.28)
λ2n n λ2n n n n
andthat
(cid:110) (cid:111)
sup sup |P Λˆ−1/2 R∗(x(d))∈R|D −P {Z ∈R}|
n n n
R∈RP∈P
(cid:18) φ2log5(dn)(cid:19)1/4 (cid:18) φ2log2(dn) 1 (cid:19)
≲ + + δ2 log(dn) (A.29)
λ2n λ2n n n
withprobabilitygreaterthan1−Cn−1/2φλ−1log3/2(dn)−ρ . Tocompletetheproof,observethat
n
(cid:12) (cid:12)P (cid:110) θ 0(x(d))∈Cˆ(x(d),D n)(cid:111) −(1−α)(cid:12) (cid:12)
=(cid:12) (cid:12)P (cid:110)√ n∥Λˆ− n1/2 R n(x(d))∥ ∞ ≤cv n(α)(cid:111) −P (cid:110)√ n∥Λˆ− n1/2 R n∗(x(d))∥ ∞ ≤cv n(α)(cid:111)(cid:12) (cid:12)
(cid:18) φ2log5(dn)(cid:19)1/4
(cid:112)
≲ +δ logdn+ρ
λ2n n n
by(A.28)and(A.29),asrequired.
A.4 ProofsforSupportingLemmas.
A.4.1 ProofofTheoremA.2,Part(i). Throughout,wetakeθ (x)=0forallxwithoutlossofgenerality.
0
WebeginbyshowingthattherootR (x(d))iswell-approximatedbythestatisticU (x(d)). Takextobeany
n n
componentofthevectorx(d). ByaTaylorexpansionaboutθ (x),wehavethat
0
M(x;θˆ (x),gˆ )−M(x;θ (x),gˆ )=(θˆ (x)−θ (x))M(1)(x;θ (x),gˆ )
n n 0 n n 0 0 n
+(θˆ (x)−θ (x))2H(x;θ˜ (x),gˆ ) (A.30)
n 0 0 n7
forsomeθ˜ (x)betweenθˆ (x)andθ (x). Moreover,wecanwrite
0 n 0
(θˆ (x)−θ (x))M(1)(x;θ (x),gˆ )
n 0 0 n
=(θˆ (x)−θ (x))M(1)(x;θ (x),g )
n 0 0 0
(cid:16) (cid:17)
+(θˆ (x)−θ (x)) M(1)(x;θ (x),gˆ )−M(1)(x;θ (x),g ) (A.31)
n 0 0 n 0 0
and
M(x;θˆ (x),gˆ )−M(x;θ (x),gˆ )=(M(x;θˆ (x),gˆ )−M (x;θˆ (x),gˆ ,D ))
n n 0 n n n n n n n
+(M(x;θ (x),g )−M(x;θ (x),gˆ ))
0 0 0 n
(cid:16) (cid:104) (cid:105)(cid:17)
= M(x;θˆ (x),gˆ )−E M (x;θˆ (x),gˆ ,D )
n n n n n n
(cid:16) (cid:104) (cid:105) (cid:17)
+ E M (x;θˆ (x),gˆ ,D ) −M (x;θˆ (x),gˆ ,D )
n n n n n n n n
+(M(x;θ (x),g )−M(x;θ (x),gˆ )). (A.32)
0 0 0 n
Thus,bytheidentity
M (x;θˆ (x),gˆ )=M (x;θˆ (x),gˆ )−M (x;θ (x),gˆ )
n n n n n n n 0 n
+M (x;θ (x),gˆ )−M (x;θ (x),g )+M (x;θ (x),g ), (A.33)
n 0 n n 0 0 n 0 0
theequalities(A.30),(A.31),and(A.32)implythat
M(1)(x;θ (x),g )(θˆ (x)−θ (x))
0 0 n 0
=M(1)(x;θ (x),gˆ )(θˆ (x)−θ (x))
0 n n 0
(cid:16) (cid:17)
− M(1)(x;θ (x),gˆ )−M(1)(x;θ ,g ) (θˆ (x)−θ (x))
0 n 0 0 n 0
=−M (x;θ (x),g ) (A.34)
n 0 0
+Bias(x;θˆ (x),gˆ )+Nuis(x;θ (x),gˆ ) (A.35)
n n 0 n
+Stoch(1)(x;θˆ (x),gˆ )+Stoch(2)(x;gˆ ) (A.36)
n n n
−(θˆ (x)−θ (x))2H(x;θ˜ (x),gˆ ) (A.37)
n 0 0 n
(cid:16) (cid:17)
−(θˆ (x)−θ (x)) M(1)(x;θ (x),gˆ )−M(1)(x;θ (x),g ) (A.38)
n 0 0 n 0 0
where
(cid:104) (cid:105)
Bias(x;θˆ (x),gˆ )=M(x;θˆ (x),gˆ )−E M (x;θˆ (x),gˆ ) , (A.39)
n n n n n n n
Nuis(x;θ (x),gˆ )=M(x;θ (x),g )−M(x;θ (x),gˆ ), (A.40)
0 n 0 0 0 n
Stoch(1)(x;θˆ (x),gˆ )=M (x;θˆ (x),gˆ )−M (x;θ (x),gˆ ), and (A.41)
n n n n n n 0 n8
Stoch(2)(x;gˆ )=M(x;θ (x),g )−M(x;θ (x),gˆ ), (A.42)
n 0 0 0 n
respectively.
We now give bounds for the terms (A.35), (A.36), (A.37), and (A.38). To handle (A.35), observe that
Parts(i)and(ii)ofAssumptionA.4implythat
(cid:114) n (cid:18) λ1/2 (cid:19)
|Bias(x(d);θˆ (x),gˆ )|≲(1+∥θˆ (x(d))∥ )δ ≲δ 1+ δ (A.43)
λ2 n n n ∞ n,B n,B n1/4 n,θ
with probability greater than 1 − ρ . Moreover, a Taylor expansion, second-order smoothness, i.e.,
n,θ
AssumptionA.1,andAssumptionA.4,Part(ii),givethat
(cid:114) (cid:114)
n n
Nuis(x(d);θ (x),gˆ )= ∂ M(x(d);θ (x),g )[g−g ]
λ2 0 n λ2 g 0 0 0
(cid:114) (cid:114)
n n
+ ∂ M(x(d);θ (x),g )[g−g ]≲ ∥g−g ∥2 ≲δ2 . (A.44)
λ2 g,g 0 0 0 λ2 0 2,∞ n,g
withprobabilitygreaterthan1−ρ .
n,g
Next,wehandletheterm(A.36). ByAssumptionA.2,aTaylorexpansiongives
Stoch(1)(x;θˆ (x),gˆ )=(θˆ (x)−θ (x))M(1) (x;θ (x),gˆ )
n n n 0 n 0 n
+(θˆ (x)−θ (x))2H (x;θ˜ (x),gˆ ) (A.45)
n 0 n 0 n
forsome,potentiallydifferent,θ˜ (x)betweenθˆ (x)andθ (x). Toboundthisterm,observethat
0 n 0
(1) (1) (1) (1)
|M (x;θ (x),gˆ )|≤|M (x;θ (x),g )|+|M (x;θ (x),gˆ )−M (x;θ (x),g )|. (A.46)
n 0 n n 0 0 n 0 n n 0 0
Hence,AssumptionA.2andAssumptionA.4,Parts(ii)and(iii)implythat
(cid:114) n λ1/2
∥Stoch(1)(x(d);θˆ (x(d)),gˆ )∥ ≤δ δ + δ δ +δ2 (A.47)
λ2 n n ∞ n,m n,θ n1/4 n,θ n,J n,θ
withprobabilitygreaterthan1−ρ −ρ −ρ . Moreover,wehavethat
n,m n,θ n,J
(cid:114)
n
∥Stoch(2)(x(d);gˆ )∥ ≤δ (A.48)
λ2 n ∞ n,S
withprobabilitygreaterthan1−ρ .
n,S
Finally,wehandletheterms(A.37)and(A.38). AssumptionA.4,Part(ii),andAssumptionA.1implythat
(cid:114)
n
∥(θˆ (x(d))−θ (x(d)))2H(x(d);θ˜ (x(d)),gˆ )∥ ≲δ2 and (A.49)
λ2 n 0 0 n ∞ n,θ
(cid:114)
n
∥(θˆ (x(d))−θ (x(d)))(M(1)(x(d);θ (x),gˆ )−M(1)(x(d);θ (x),g ))∥ ≲δ δ (A.50)
λ2 n 0 0 n 0 0 ∞ n,θ n,g9
withprobabilitiesgreaterthan1−ρ and1−ρ ,respectively. Puttingthepiecestogether,thedecomposition
n,θ n,g
(A.34)andthelower-boundednessoftheJacobianM(1)(·;θ,g)implythat
(cid:114)
n
∥R (x(d))−U(x(d))∥ (A.51)
λ2 n ∞
(cid:16) (cid:17)
≲δ2 +δ2 +δ +δ +δ δ +δ +λ1/2n−1/4(cid:0) δ +δ (cid:1)
n,g n,θ n,B n,S n,θ n,m n,g n,B n,J
with probability greater than 1−ρ +ρ +ρ +ρ +ρ , by the bounds (A.43), (A.44), (A.47),
n,m n,g n,θ n,S n,J
(A.48),(A.49),and(A.50).
Withthisinplace,considerthedecomposition
(cid:32) n (cid:33) n
1 (cid:88) 1 (cid:88)
R (x)= u(x,D )−U (x) − u(x,D )+∆ (x), where (A.52)
n i n i n
n n
i=1 i=1
∆ (x)=R (x)−U(x), (A.53)
n n
andwerecallthatthefunctionu(·,·)isdefinedinAssumptionA.3. FixarectangleR=[a ,a ]inR. Define
l u
thenormalizedfunctions
uˆ(x(d),D )=Λ−1/2u(x(d),D ) and Uˆ(x(d))=Λ−1/2U(x(d))
i i
and the analogously normalized rectangle R˜ = [Λ−1/2a ,Λ−1/2a ] and the enlarged rectangle R˜ =
l u t
[Λ−1/2a −1 t,Λ−1/2a +1 t]. Observethatthedecomposition(A.52)impliesthat
l d u d
(cid:12) (cid:110)√ (cid:111) (cid:12)
(cid:12)P nR (x(d))∈R −P {Z ∈R}(cid:12)
(cid:12) n (cid:12)
(cid:12) (cid:110)√ (cid:111) (cid:110) (cid:111)(cid:12)
=(cid:12)P nΛ−1/2R (x(d))∈R˜ −P Λ−1/2Z ∈R˜ (cid:12)
(cid:12) n (cid:12)
(cid:40) n (cid:41)
(cid:12) 1 (cid:88) (cid:110) (cid:111)(cid:12)
≤(cid:12)P √ uˆ(x(d),D )∈R˜ −P Λ−1/2Z ∈R˜ (cid:12) (A.54)
(cid:12) n i t t (cid:12)
i=1
(cid:12) (cid:110) (cid:111) (cid:110) (cid:111)(cid:12)
+(cid:12)P Λ−1/2Z ∈R˜ −P Λ−1/2Z ∈R˜ (cid:12) (A.55)
(cid:12) t (cid:12)
(cid:40) n (cid:41)
√ 1 (cid:88) 1
+P n∥Uˆ (x(d))− uˆ(x(d),D )∥ ≥ t (A.56)
n i ∞
n 2
i=1
(cid:26) (cid:27)
√ 1
+P n∥Λ−1/2∆ (x(d))∥ ≥ t (A.57)
n ∞
2
foreacht>0. Theproofiscompletebyprovidingappropriateboundsfortheterms(A.54)through(A.57).
Weboundthenormalapproximationterm(A.54)throughtheapplicationofthefollowingquantitative
centrallimittheorem,statedasTheorem2.1ofChernozhuokovetal.(2022).10
LemmaA.3(Theorem2.1, Chernozhuokovetal.,2022).LetX ,...,X beacollectionofindependent,
1 n
centered,randomvectorsinRd andletZ beacenteredGaussianrandomvectorwithcovariancematrix
n
1 (cid:88) E(cid:2)
X
X⊤(cid:3)
. (A.58)
n i i
i=1
Ifthereexistabsoluteconstantsc,C ,andφsuchthatthebounds
1
n n
1 (cid:88) E(cid:2) X2 (cid:3) ≥c, 1 (cid:88) E(cid:2) X4 (cid:3) ≤C φ2 , and ∥X ∥ ≤φ (A.59)
n i,j n i,j 1 i,j ψ1
i=1 i=1
hold,thentheinequality
sup
(cid:12)
(cid:12)P
(cid:40) √1 (cid:88)n
X
∈R(cid:41)
−P
(cid:8)√
nZ
∈R(cid:9)(cid:12)
(cid:12)≤C
(cid:18) φ2log5(dn)(cid:19)1/4
(A.60)
(cid:12) n i (cid:12) 2 n
R∈R
i=1
holdsforsomeconstantC thatdependsonlyoncandC .
2 1
Inparticular,observethat
n
1 (cid:88) (cid:104) (cid:105)
E uˆ2(x(j),D ) =1 (A.61)
i
n
i=1
bydefinitionandthat
n
1 (cid:88) (cid:104) (cid:105)
E uˆ4(x(j),D ) ≤(φ/λ)2 (A.62)
i
n
i=1
byAssumptionA.3. Similarlywehavethat
∥uˆ(cid:0) xj),D (cid:1) ∥ ≤(φ/λ) (A.63)
i ψ1
byAssumptionA.3. Consequently,as
Var(uˆ(x(j),D ))=Λ−1/2Var(Z)Λ−1/2, (A.64)
i
bydefinition,LemmaA.3impliesthat
(cid:12) (cid:40) 1 (cid:88)n (cid:41) (cid:110) (cid:111)(cid:12) (cid:18) φ2log5(dn)(cid:19)1/4
(cid:12)P √ uˆ(D )∈R˜ −P Λ−1/2Z ∈R˜ (cid:12)≲ . (A.65)
(cid:12) n i t t (cid:12) λ2n
i=1
Inturn,toboundtheterm(A.55),wehavethat
(cid:12) (cid:110) (cid:111) (cid:110) (cid:111)(cid:12) (cid:112)
(cid:12)P Λ−1/2Z ∈R −P Λ−1/2Z ∈R (cid:12)≲t logd. (A.66)
(cid:12) t (cid:12)
byLemmaA.1. Moreover,toboundtheterm(A.56),recallthat
(cid:40) n (cid:41)
√ 1 (cid:88)
P n∥Uˆ (x(d))− uˆ(x(d),D )∥ ≥δ ≤ρ (A.67)
n i ∞ n,u n,u
n
i=1
byAssumptionA.3. Thus,bychoosingt=Cδ ,thebound(A.51)impliesthatthesumofterm(A.56)and
n
term(A.57)isupperboundedbyCρ . Hence,bypluggingthischoiceoftinto(A.66)and(A.67),wecan
n11
concludethat
(cid:12) (cid:110)√ (cid:111) (cid:12) (cid:18) φ2log5(dn)(cid:19)1/4 (cid:112)
(cid:12)P nR (x(d))∈R −P {Z ∈R}(cid:12)≲ +δ log(d)+ρ , (A.68)
(cid:12) n n (cid:12) λ2n n n
asrequired.
A.4.2 ProofofTheoremA.2,Part(ii). Theresultfollowsfromanargumentwhosestructureissimilarto
theproofofTheoremA.2,Part(i). Again,wetakeθ (x) = 0forallx,withoutlossofgenerality. Weare
0
interestedinstudyingthediscrepancy
R∗(x)=θˆ (x)−θˆ (x)=(θˆ (x)−θ (x))−R (x).
n h n h 0 n
Weknowfrom(A.52)that
(cid:32) n (cid:33) n
1 (cid:88) 1 (cid:88)
R (x)= u(x,D )−U (x) − u(x,D )+∆ (x), (A.69)
n i n i n
n n
i=1 i=1
where∆ (x)isdefinedin(A.53). Ontheotherhand,asθˆ (x)isconstructedwitharandomhalf-samplehof
n h
thedataD ,wehavethat
n
(cid:32) n (cid:33) n
2 (cid:88) 2 (cid:88)
θˆ (x)−θ (x)= u(x,D )−U (x) − u(x,D )+∆ (x), (A.70)
h 0 i h i h
n n
i∈h i∈h
whereU (x)and∆ (x)areconstructedwiththehalf-sampleh.
h h
TheproofofTheoremA.2,Part(i),workedbygivingahighprobabilityboundforthefirstandthirdterm
in(A.69)andshowingthatthethesecondtermsatisfiesacentrallimittheorem. Here,asweareinterested
in giving a bound conditioned on the data D , we show that the difference between the second terms in
n
(A.69)and(A.70)satisfiesacentrallimittheoremontheeventthatthefirstandthirdtermsin(A.69)and
(A.70) satisfy a specified bound, which we show holds with high probability. In particular, let F (t) and
n
F (t)denotetheeventsthat
h
(cid:114) (cid:114)
n n
∥∆ (x(d))∥ ≤t/4 and ∥∆ (x(d))∥ ≤t/4, (A.71)
λ2 n ∞ λ2 h ∞
respectively. Similarly,letH (t)andH (t)denotetheeventsthat
n h
n
√ 1 (cid:88)
n∥ uˆ(x(d),D )−Uˆ (x(d))∥ ≤t/4 and (A.72)
i n ∞
n
i=1
n
√ 2 (cid:88)
n∥ uˆ(x(d),D )−Uˆ (x(d))∥ ≤t/4
i h ∞
n
i∈h
respectively,whereUˆ (x(d))isdefinedanalogouslytoUˆ (x(d)). DefinetheeventE (t)=F (t)∩F (t)∩
h n n n h
H (t)∩H (t). Fixahyper-rectangleRinR. RecallthedefinitionsofthetransformedrectangleR˜ andthe
n h12
enlargedtransformedrectangleR˜ . OntheeventE (t),wehave
t n
(cid:110)√ (cid:111)
|P nR∗(x(d))∈R|D −P {Z ∈R}|
n n
(cid:110)√ (cid:111) (cid:110) (cid:111)
=|P nΛ−1/2R∗(x(d))∈R˜ |D −P Λ−1/2Z ∈R˜ |
n n
(cid:40) n n (cid:41)
2 (cid:88) 1 (cid:88) (cid:110) (cid:111)
≤|P √ uˆ(x(d),D )− √ uˆ(x(d),D )∈R˜ |D −P Λ−1/2Z ∈R˜ |
i i t n t
n n
i∈h i=1
(cid:110) (cid:111) (cid:110) (cid:111)
+|P Λ−1/2Z ∈R˜ −P Λ−1/2Z ∈R˜ |
t
foreacht>0. AsthedataD aredrawnindependentlyandidenticallywithdistributionP inPandwehave
h
assumedthatδ ≲ δ andρ ≲ ρ , bysettingt = Cδ , AssumptionA.3andthebound(A.51)imply
n/2 n n/2 n n
thattheeventE (t)occurswithprobabilitygreaterthan1−Cρ . Thus,LemmaA.1impliesthat
n n
(cid:110)√ (cid:111)
|P nR∗(x(d))∈R|D −P {Z ∈R}| (A.73)
n n
(cid:12) (cid:40) n n (cid:41) (cid:12)
≤(cid:12) (cid:12)P √2 (cid:88) uˆ(x(d),D i)− √1 (cid:88) uˆ(x(d),D i)∈R˜ t |D n −P (cid:110) Λ1/2Z ∈R˜ t(cid:111)(cid:12) (cid:12) (A.74)
(cid:12) n n (cid:12)
i∈h i=1
(cid:112)
+δ log(d)
n
withprobabilitygreaterthan1−Cρ . Hence,itsufficestoboundtheterm(A.74).
n
Tothisend,weapplyacouplingargumentintroducedinYadlowskyetal.(2023),whichissimilartoa
PoissonizationtechniquestudiedinPræstgaardandWellner(1993)(seealsoSection3.6.2ofvanderVaart
andWellner(2013)). Inparticular,letV bearandomvariabletakingthevalue1wheniisanelementofthe
i
subsethandtakingthevalue−1otherwise. Observethat
n n
2 (cid:88) 1 (cid:88) 1 (cid:88)
uˆ(x(d),D )− uˆ(x(d),D )= V uˆ(x(d),D ). (A.75)
i i i i
n n n
i∈h i=1 i=1
LetV˜ ,...,V˜ denoteacollectionofrandomvariablesvaluedon{−1,1}. Wedefinetheirjointdistributionas
1 n
follows. LetQ denotearandomvariablewithdistributionBin(n,1/2). IfQ ≥n/2,thenchooseQ −n/2
n n n
indicesiin[n]withV = −1andsetV˜ = 1. IfQ < n/2,thenchoosen/2−Q indiceswithV = 1and
i i n n i
setV˜ =−1. SetV˜ =V forallotherunits. ObservethatthecollectionV˜ areindependentandidentically
i i i i
distributedRademacherrandomvariables. Withthisinplace,weobtainthedecomposition
n n n
1 (cid:88) V uˆ(x(d),D )= 1 (cid:88) V˜uˆ(x(d),D )+ 1 (cid:88)(cid:0) V −V˜(cid:1) uˆ(x(d),D ). (A.76)
i i i i i i i
n n n
i=1 i=1 i=1
LetV(t)denotetheeventthat
n
√ n(cid:13) (cid:13)1 (cid:88)(cid:0) V −V˜(cid:1) uˆ(x(d),D )(cid:13) (cid:13) >t. (A.77)
(cid:13)n i i i (cid:13)
∞
i=113
FixanyrectangleR′ =[a′,a′ ]anddefinetheenlargedrectangleR′ =[a −t1 ,a +t1 ]. OntheeventV(t),
l u t l d u d
thedecomposition(A.76)impliesthat
(cid:40) n (cid:41)
(cid:12) (cid:12)P √1 (cid:88) V iuˆ(x(d),D i)∈R′ |D n −P (cid:110) Λ−1/2Z ∈R′(cid:111)(cid:12) (cid:12)
n
i=1
(cid:40) n (cid:41)
≤(cid:12) (cid:12)P √1 n(cid:88) V˜ iuˆ(x(d),D i)∈R˜ t |D n −P (cid:110) Λ−1/2Z ∈R′ t(cid:111)(cid:12) (cid:12) (A.78)
i=1
(cid:110) (cid:111) (cid:110) (cid:111)
+|P Λ−1/2Z ∈R′ −P Λ−1/2Z ∈R′ | (A.79)
t
(cid:112)
forallt>0. LemmaA.1impliesthat(A.79)islessthant log(d). Tohandletheterm(A.78),weapplythe
followingquantitativecentrallimittheorem,statedasLemma4.6inChernozhuokovetal.(2022).
LemmaA.4(Lemma4.6, Chernozhuokovetal.,2022).ConsiderthesettingandassumptionsofLemmaA.3.
Let X = (X ,...,X ) collect the observed data and let V˜ ,...,V˜ be a collection of independent
n 1 n 1 n
Rademacherrandomvariables. Wehavethat
(cid:12) (cid:40) (cid:88)n (cid:41) (cid:40) (cid:88)n (cid:41) (cid:12) (cid:18) φ2log5(dn)(cid:19)1/4
sup (cid:12)P n−1/2 X ∈R −P n−1/2 V˜X ∈R|X (cid:12)≤C . (A.80)
(cid:12) i i i n (cid:12) 2 n
R∈R
i=1 i=1
withprobabilitygreaterthan
φlog3/2(dn)
1−C (A.81)
n1/2
forsomeconstantC thatdependsonlyontheconstantsC andcdefinedinthestatementofLemmaA.3.
2 2
Thus,ontheeventV(t),LemmaA.3andLemmaA.4implythat
(cid:12) (cid:12)P (cid:40) n1 (cid:88)n V iuˆ(x(d);D i)∈R′ |D n(cid:41) −P (cid:8)√ nZ ∈R′(cid:9)(cid:12) (cid:12)≲(cid:18) φ2lo λg 25 n(dn)(cid:19)1/4 +t(cid:112) log(d),
i=1
withprobabilitygreaterthan1−Cn−1/2λ−1φlog3/2(dn). Hence,itsufficestogiveahighprobabilitybound
onV(t)forasuitablechoiceoft.
Tothisend,observethat
n
G =(cid:13) (cid:13)√1 (cid:88)(cid:0) V −V˜(cid:1) uˆ(x(d);D )(cid:13) (cid:13) isequidistributedwith (A.82)
n (cid:13) n i i i (cid:13) ∞
i=1
(cid:13) 2
|Qn (cid:88)−n/2|
(cid:13)
(cid:13)√ uˆ(x(d);D )(cid:13) . (A.83)
(cid:13) n i (cid:13) ∞
i=1
Considerthedecomposition
(cid:110) n(cid:111) (cid:110) n(cid:111)
P {G ≥t}≤P G ≥t,|Q −n/2|≤δ +P |Q −n/2|≥δ
n n n n
2 2
(cid:40) k (cid:41)
(cid:13) 2 (cid:88) (cid:13) (cid:110) n(cid:111)
≤P max (cid:13)√ uˆ(D )(cid:13) ≥t +P |Q −n/2|≥δ , (A.84)
1≤k≤δn(cid:13) n i (cid:13) ∞ n 2
2 i=114
forsomeδ >0tobechosen. Observethat
(cid:26) δn(cid:27) (cid:18) δ2n(cid:19)
P |Q −n/2|≥ ≤2exp − (A.85)
n
2 6
bythemultiplicativeChernoffbound. Toboundthefirsttermin(A.84),wecombinetwoinequalities. The
firstinequalityisthefollowingstandardBernstein-typebound,statedinSongetal.(2019).
LemmaA.5(LemmaA.2,Songetal.(2019)).LetZ ,...,Z beindependent,centered,randomvectorsin
1 n
Rd. Definethequantity
n
σ2 =max(cid:88) E(cid:2) Z2 (cid:3) (A.86)
i,j
j∈[d]
i=1
andassumethat∥Z ∥ ≤ϕforalli∈[n]andj ∈[d]. Theinequality
ij ψ1
(cid:40) n (cid:41)
(cid:13)(cid:88) (cid:13) (cid:16) (cid:17) 1
P (cid:13) Z (cid:13) ≥C σlog1/2(dg)+ϕlog(dn)(log(dn)+log(g)) ≲
(cid:13) i(cid:13) ∞ g
i=1
holdsforanyconstantg >0.
ThesecondinequalityisaLe´vytypeinequalityforindependentrandomvectors,duetoMontgomery-Smith
(1993). SeeChapter1ofDelaPenaandGine´ (1999)foratextbooktreatment.
LemmaA.6(Theorem1.1.5,DelaPenaandGine´ (1999)).LetZ ,...,Z beindependentrandomvectorsin
1 n
Rd. ThereexistsauniversalconstantsC andC suchthat
1 2
(cid:40) k (cid:41) (cid:40) k (cid:41)
(cid:88) (cid:88) t
P max ∥ Z ∥ >t ≤C P ∥ Z ∥ > (A.87)
i ∞ 1 i ∞
1≤k≤n C 2
i=1 i=1
forallt>0.
Inparticular,as
2 2 φ
∥√ uˆ(x(j);D )∥ ≤ √ ,
n
i ψ1
nλ
and
δn/2 (cid:20) (cid:21)
(cid:88) 2
max E √ uˆ(x(j);D ) =2δ , (A.88)
i
j∈[d] n
k=1
LemmaA.5andLemmaA.6implythat
(cid:40) (cid:13) 2 (cid:88)k (cid:16) (cid:17)(cid:13) (cid:18) 2 φ (cid:19)(cid:41) 1
P max (cid:13)√ uˆ x(d);D (cid:13) ≥C δlog1/2(dn)+ √ log2(dn) ≲ . (A.89)
1≤k≤δn(cid:13) n i (cid:13) ∞ nλ n
2 i=1
Now,thechoice
(cid:114)
logn
δ =C (A.90)
n
gives
(cid:26) (cid:27)
δn 1
P Q ≥ ≲ (A.91)
n
2 n15
by(A.85). Pluggingthischoiceinto(A.89)yields
(cid:40) k (cid:41)
(cid:13) 2 (cid:88) (cid:16) (cid:17)(cid:13) 1 φ 1
P max (cid:13)√ uˆ x(d);D (cid:13) ≥C√ log2(dn) ≲ . (A.92)
1≤k≤δn(cid:13) n i (cid:13) ∞ nλ n
2 i=1
Hence,wefindthattheinequality
P (cid:40) (cid:13) (cid:13)√1 (cid:88)n (cid:0) V −V˜(cid:1) uˆ(x(d),D )(cid:13) (cid:13) ≥C(cid:18) φ2log4(dn)(cid:19)1/2(cid:41) ≲ 1
(cid:13) n i i i (cid:13) ∞ λ2n n
i=1
holdsforallnsufficientlylarge.
Thus,bysetting
(cid:18) φ2log4(dn)(cid:19)1/2
t=C ,
λ2n
wefindthat
(cid:40) n (cid:41)
(cid:12) (cid:12)P 1 (cid:88) V iuˆ(x(d),D i)∈R′ |D n −P (cid:110) Λ−1/2Z ∈R′(cid:111)(cid:12) (cid:12)
n
i=1
(cid:18) φ2log5(dn)(cid:19)1/4
(cid:112)
(cid:18) φ2log4(dn)(cid:19)1/2
≲ + log(d) (A.93)
λ2n λ2n
(cid:18) φ2log5(dn)(cid:19)1/4
≲ , (A.94)
λ2n
withprobabilitygreaterthan1−Cn−1/2λ−1φlog3/2(dn). Puttingthepiecestogether,theinequalities(A.73)
and(A.94)implythat
(cid:110)√ (cid:111)
(cid:18) φ2log5(dn)(cid:19)1/4
(cid:112)
|P nR∗(x(d))∈R|D −P {Z ∈R}|≲ +δ log(d)
n n λ2n n
withprobabilitygreaterthan1−C(n−1/2φλ−1log3/2(dn)+ρ ),asrequired.
n
A.4.3 ProofofLemmaA.2,Part(i). Again,wetakeθ (x)=0forallx,withoutlossofgenerality. Recall
0
fromtheproofofTheoremA.2,Part(ii),thatV isarandomvariabletakingthevalue1wheniisanelement
i
ofthesubseth,andtakingthevalue−1otherwise,andthat
n n n
2 (cid:88) 1 (cid:88) 1 (cid:88)
u(x,D )− u(x,D )= V u(x,D ). (A.95)
i i i i
n n n
i∈h i=1 i=1
Toeasenotation,definetheobjects
n
1 (cid:88) 2 (cid:88)
T (x)= u(x,D )−U (x) and T (x)= u(x,D )−U (x).
n i n h i h
n n
i=1 i∈h
Weareinterestedinstudying
(cid:20) (cid:21)
(cid:16) (cid:17)2
λˆ2 =nE R∗(x(j))
n,j V n16
 
(cid:32) n (cid:33)2
1 (cid:88)
=nE V  V iu(x(j),D i)+T h(x(j))−T n(x(j))+∆ h(x(j))−∆ n(x(j))  ,
n
i=1
wherethenotationE [·]denotesthattheexpectationisevaluatedonlyovertherandomvariablesV ,...,V .
V 1 n
OntheeventE(t),definedastheintersectionoftheevents(A.71)and(A.72),wehavethat
 
(cid:32) n (cid:33)2
sup (cid:12) (cid:12)λˆ2 n,j −λ2 n,j(cid:12) (cid:12)≤t2 , where λ2 n,j =nE V  n1 (cid:88) V iu(x(j),D i)  .
j∈[d]
i=1
Wecanevaluate
n n
λ2 = 1 (cid:88) u2(x(j),D )+ 1 (cid:88)(cid:88) E[V V ]u(x(j),D )u(x(j),D ).
n,j n i n i i′ i i′
i=1 i=1 i′̸=i
Observethat
1 1
E[V V ]= E[V |V =1]− E[V |V =−1]
i i′ i i′ i′ i
2 2
(cid:18) (cid:19) (cid:18) (cid:19)
1 n/2−1 n/2 1 n/2 n/2−1 1
= − − − =−
2 n−1 n−1 2 n−1 n−1 n−1
andthereby
n n
λ2 = 1 (cid:88) u2(x(j),D )− 1 1 (cid:88)(cid:88) u(x(j),D )u(x(j),D ). (A.96)
n,j n i nn−1 i i′
i=1 i=1 i′̸=i
Now,observethatthefirsttermin(A.96)satisfies
n
(cid:12)1 (cid:88) (cid:12) φ
sup (cid:12) u2(x(j),D )−λ2(cid:12)≲ log(dn)
(cid:12)n i j(cid:12) n
j∈[d]
i=1
withprobabilitygreaterthan1−n−1,byBernstein’sinequality(seee.g.,Theorem2.8.1ofVershynin(2018)).
Tohandlethesecondtermin(A.96),weapplythefollowingsub-exponentialformulationoftheHansonand
Wright(1971)exponentialconcentrationinequalityforquadraticforms,duetoGo¨tzeetal.(2021).
Lemma A.7 (Proposition 1.1, Go¨tze et al. (2021)).Let X ,...,X be independent, centered, random
1 n
variablessatisfyingE(cid:2) X2(cid:3) = σ2 and∥X ∥ ≤ φ. IfA = (a )isanysymmetricn×nmatrix,thenthe
i i i ψ1 i,i′
inequality
P (cid:40) (cid:12) (cid:12)(cid:88) i=n 1i(cid:88) ′=n 1a i,i′X iX
i′
−(cid:88) i=n 1σ i2a i,i(cid:12) (cid:12)≥t(cid:41) ≤2exp(cid:32) C1 min(cid:32) φ4∥t A2
∥2
F, φ∥t A1/ ∥2
o1/
p2(cid:33)(cid:33) (A.97)
holdsforanyt≥0,where∥·∥ and∥·∥ denotetheFrobeniusandℓ operatornorms,respectively.
F op 2
Inparticular,ifAdenotesthen×nmatrixwithzeroesonthediagonaland(n(n−1))−1 ineveryotherentry,
then
1 1 1
∥A∥2 = and ∥A∥1/2 =
F nn−1 op n17
andsoLemmaA.7impliesthat
(cid:12)1 1 (cid:88)n (cid:88) (cid:12) φ2
sup (cid:12) u(x(j),D )u(x(j),D )(cid:12)≲ log2(dn)
(cid:12)nn−1 i i′ (cid:12) n
j∈[d]
i=1 i′̸=i
withprobabilitygreaterthan1−n−1. Thus,ontheeventE (t),wehavethat
n
sup (cid:12) (cid:12)λˆ2 −λ2(cid:12) (cid:12)≲
φ2
log2(dn)+t2
n,j j n
j∈[d]
withprobabilitygreaterthan1−Cn−1,asφ≥1. Bysetting
(cid:114)
λ2
t=C δ ,
n
n
AssumptionA.3andthebound(A.51)implythattheeventE (t)occurswithprobabilitygreaterthan1−ρ .
n n
Thus,wecanconcludethat
sup (cid:12) (cid:12)λˆ2 /λ2 −1(cid:12) (cid:12)≲ φ2 log2(dn)+ 1 δ2
n,j n,j λ2n n n
j∈[d]
withprobabilitygreaterthan1−C(ρ +n−1),asrequired.
n
APPENDIX B. PROOF OF THEOREM 3.1
TheresultfollowsfromanapplicationofTheoremA.1. Webeginbyverifyingtherequisiteassumptions.
To this end, observe that Assumption A.1 follows immediately from Assumption 3.2 and Part (iii) of
Assumption3.3. NeymanorthogonalityandPart(i)ofAssumption3.3holdbyassumption. Likewise,by
definition,Assumption3.2impliesthatAssumptionA.2holdswith
n
M(1)
(x;θ,g)=(cid:88)(cid:16)
K(x,X )m(1)(D
;θ,g)−E(cid:104)
K(x,X )m(1)(D
;θ,g)(cid:105)(cid:17)
(B.1)
n i i i i
i=1
andH (x;θ,g)=0,respectively.
n
We now quantify the generic sequences δu and ρu defined in Assumption A.3. Recall the normalized
n n
statisticU (x)definedin(A.6). Bythedefinition(3.1),thisquantitycanbewritten
n
r
1(cid:88)
U (x)=− u(x;D ,ξ ,θ ,g ), where
n
r
sq s 0 0
q=1
u(x;D ,ξ ,θ,g)=M(1)(x;g )−1(cid:88)(cid:0) κ(x,X ,D ,ξ )m(D ;θ,g)−E(cid:2) κ(x,X ,D ,ξ )m(D ;θ,g)(cid:3)(cid:1) .
s s 0 i s s i i s s i
i∈s
Definethede-randomizedkernelfunctionandHa´jekprojection
u˜(x;D)=E[u(x;D ,ξ ,θ (x),g )|D =D] and (B.2)
s s 0 0 s
u˜(1)(x;D)=E[u(x;D ,ξ ,θ (x),g )|i∈s,D =D] , (B.3)
s s 0 0 i18
respectively,inadditiontothequantities
σ2 =Var(u˜(x(j),D )) and σ2 = minσ2 . (B.4)
b,j i b b,j
j∈[d]
TheresultbelowfollowsfromanapplicationofTheorem4.2.
LemmaB.1(AsymptoticLinearity).Supposethatthede-randomizedkernelfunction (B.2)isboundedbyϕ
√
almostsurelyandisinvarianttopermutationsofitssecondargument. Ifbandrarechosentosatisfyn≤ rb
andthereexistsaconstantC suchthat
1
blog(dn)
≤C <1 and b−C2 ≤σ2 (B.5)
n 1 b
forsomepositiveconstantC ,then
2
(cid:114) n (cid:13)
(cid:13)U (x(d))−
b (cid:88)n
u˜(1)(x(d);D
)(cid:13)
(cid:13)
≲(cid:18) (1+∥θ 0(x(d))∥)2ϕ2log2(dn)(cid:19)1/4
(B.6)
b2σ2(cid:13) n n i (cid:13) ∞ nσ2
b i=1 b
withprobabilitygreaterthan1−Cn−1 forallblargerthanaconstantb thatdependsonlyonC .
0 2
Inparticular,observethatAssumption3.2impliesthatthede-randomizedkernelfunction(B.2)isbounded
byϕ,uptoaconstantthatdependsonlyonP. Permutationinvarianceagainfollowsbyassumption. Wemay
assumethatthefirstconditionin(B.5)holds,asotherwisethedesiredboundisvacuouslytrue. Thesecond
conditionin(B.5)follows,withC =1,fromincrementality. Consequently,wecanchoose
2
(cid:18)
(1+∥θ
(x(d))∥)2ϕ2log2(dn)(cid:19)1/4
0
δ =C (B.7)
n,u nσ2
b
andρ =Cn−1,respectively.
n,u
Next,wequantifythesequencesintroducedinAssumptionA.4. TheLemmabelowfollowsfromarguments
similartothoseusedinWagerandAthey(2018)andOprescuetal.(2019). Thenoveltyisthateachboundis
uniformoverthequery-vectorx(d).
Lemma B.2 (Bias, Consistency, and Stochastic Equicontinuity).Suppose that the kernel κ(·,·,D ,ξ ) is
s s
honestandhasauniformshrinkagerateε . Supposethattheratecondition (3.17),statedinTheoremA.1,
n
√
holdsforsomesequencesδ andρ andthatr andbarechosentosatisfyn≤b r.
n,g n,g
(i)IftheboundednesspartofAssumption3.2andAssumption3.3,Part(iii),hold,then
∥Bias (x(d);θ(x(d)),gˆ )∥ ≲(1+∥θ(x(d))∥ )ε and (B.8)
n n ∞ ∞ n
P
(cid:26)
∥M(1) (x(d);θ (x(d)),g
)∥≥Cϕb1/2log1/2(dn)(cid:27)
≲ 1 . (B.9)
n 0 0 n1/2 n
(ii)SupposethattheNeymanorthogonalmomentfunctionM(·;θ,g)satisfiesAssumption3.2andAssumption3.3,
Part(i). Ifthereexistssomeconstantcsuchthatε ≤c,
n
(cid:18) b2σ2(cid:19)1/4 ϕ2blog(dn)
b δ ≤c, and ≤c
n,g
n n19
forallsufficientlylargen,then
(cid:110) (cid:111) 1
P ∥θˆ (x(d))−θ (x(d))∥ ≥Cτ ≲ρ + , where
n 0 ∞ n,θ n,g
n
(cid:18) b2σ2(cid:19)1/4 (cid:114)
blog(dn)
τ = b δ2 +ε + (1+∥θ (x(d))∥ )ϕ
n,θ n n,g n n 0 ∞
forallsufficientlylargen.
(iii)IftheNeymanorthogonalmomentfunctionM(·;θ,g)satisfiesAssumption3.2andAssumption3.3,then
P (cid:110)(cid:13) (cid:13)M n(x(d);θ 0(x(d)),gˆ n)−M n(x(d);θ 0(x(d)),g 0)(cid:13) (cid:13) ∞ ≥Cτ n,S(cid:111) ≲ρ n,g + n1 and
P (cid:110)(cid:13) (cid:13)M( n1) (x(d);θ 0(x(d)),gˆ n)−M( n1) (x(d);θ 0(x(d)),g 0)(cid:13) (cid:13) ∞ ≥Cτ n,S(cid:111) ≲ρ n,g + n1 , where
(cid:114) blog(dn)(cid:32) (cid:18) b2σ2(cid:19)1/4 (cid:33) blog2(dn)
τ = b δ +ε1/2 + (1+∥θ (x(d))∥ )ϕ.
n,S n,g n 0 ∞
n n n
Withtheseresultsinplace,weapplyTheoremA.1. Wemayassumethatthereexistssomesmallconstant
csuchthat
ϕ2log5(dn)
ε ≤c, δ ≤c, and ≤c (B.10)
n n,g σ2n
b
forsufficientlylargen,asotherwisetheboundisvacuouslytrue. Recallthesequence
λ1/2
δ =δ2 +δ2 +δ +δ +δ +(δ +δ + (δ +δ ))δ (B.11)
n n,g n,θ n,B n,S n,u n,m n,g n1/4 n,B n,J n,θ
introducedinthestatementofTheoremA.1. Observethatthechoice
u(x,D )=b·u˜(x,D ),
i i
suggestedbyLemmaB.1impliesthat
λ2 =b2σ2 .
b
Thus,LemmasB.1andB.2implythat
(cid:18)
n
(cid:19)1/2 (cid:18) ϕ2log3(dn)(cid:19)1/4
δ ≲δ2 + ε + (B.12)
n n,g b2σ2 n σ2n
b b
(cid:32) (cid:33)(cid:32) (cid:33)
(cid:18) n (cid:19)1/4 (cid:18) b2σ2(cid:19)1/4 (cid:18) n (cid:19)1/4 ϕlog1/2(dn)
+ δ + ε b δ2 + ε + (B.13)
n,g b2σ2 b n n n,g b2σ2 b n σ b1/2 n1/4
(cid:40) (cid:41)
(cid:18) log2(dn)(cid:19)1/4 (cid:18) n (cid:19)1/2 log(dn) ϕlog2(dn)
+ δ +max ε , + (B.14)
σ2n n,g b2σ2 n σ n1/2 σ n1/2
b b b b
as∥θ (x(j))∥ isuniformlybounded,wherewehaveusedthefactsthat
0 ∞
(cid:18)
n
(cid:19)1/2(cid:18) blog(dn)(cid:19)1/2(cid:18) b2σ2(cid:19)1/4 (cid:18) log2(dn)(cid:19)1/4
b δ ≤ δ and
b2σ2 n n n,g σ2n n,g
b b20
(cid:40) (cid:41)
(cid:18) (cid:19)1/2(cid:18) (cid:19)1/2 (cid:18) (cid:19)1/2
n blog(dn) n log(dn)
1/2
ε ≤max ε ,
b2σ2 n n b2σ2 n σ n1/2
b b b
inwriting(B.14)aswellasthefactthatthetermsδ andδ aresmallerthanothertermsappearinginthe
n,m n,J
bound. Observethatthenormalizations(B.10)implythat
(cid:18) b2σ2(cid:19)1/4
b δ3 ≲δ2 ,
n n,g n,g
(cid:18) (cid:19)1/4 (cid:18) (cid:19)1/2
n n
ε δ ≲ ε ,
b2σ2 n n,g b2σ2 n
b b
ϕlog1/2(dn) (cid:26) ϕ2log(dn)(cid:27)
δ ≲max δ2 , ,
n,g σ1 b/2 n1/4 n,g σ2 bn
ε δ2 ≲δ2 ,
n n,g n,g
(cid:114) n (cid:114) n
ε2 ≲ ε , and
b2σ2 n b2σ2 n
b b
(cid:112)
ϕ log(dn) (cid:114) n
ε ≲ ε .
b1/2σ1 b/2 n b2σ2
b
n
Thus,theterm(B.13)isboundedfromaboveby(B.12). Similarly,asthenormalizations(B.10)implythat
(cid:40) (cid:41)
(cid:18) log2(dn)(cid:19)1/4 (cid:18) log2(dn)(cid:19)1/2
δ ≲max δ2 , and
σ2n n,g n,g σ2n
b b
(cid:40) (cid:41) (cid:40) (cid:41)
(cid:18)
n
(cid:19)1/2
log(dn)
(cid:18)
n
(cid:19)1/2 (cid:18) ϕ2log3(dn)(cid:19)1/4
max ε , ≲max ε ,
b2σ2 n σ n1/2 b2σ2 n σ2n
b b b b
theterm(B.14)isboundedfromaboveby(B.12). Hence,wefindthat
(cid:114) n (cid:18) ϕ2log3(dn)(cid:19)1/4
δ ≲δ2 + ε + , (B.15)
n n,g b2σ2 n σ2n
b b
as∥θ (x(j))∥ isuniformlybounded. Similarly,LemmasB.1andB.2implythat
0 ∞
(cid:18) ϕ2log4(dn)(cid:19)1/4
ρ ≲ +ρ (B.16)
n σ2n n,g
b
as∥θ (x(j))∥ isuniformlybounded. Moreover,as
0 ∞
∥u(x(j),D)∥ ≤b·(1+|θ(x(j))|)ϕ
ψ1
foreachj in[d],wehavethat
(cid:18) ϕ(1+∥θ (x(j))∥ )(cid:19)1/2(cid:18) log5(dn)(cid:19)1/4 (cid:18) ϕ2log5(dn)(cid:19)1/4
0 ∞ ≲ . (B.17)
σ n σ2n
b b21
Theresultfollowsbycombiningthebounds(B.15),(B.16),and(B.17)throughTheoremA.1andapplying
incrementality.
B.1 ProofsforSupportingLemmas.
B.1.1 ProofofLemmaB.1. Toeasenotation,wedefinetheparameter
ϕ(θ )=(1+∥θ (x(d))∥)ϕ
0 0
and drop dependence on θ(x) and g when writing u(x;D ,ξ ,θ(x),g), as these values will be taken to be
s s
θ (x)andg throughout. Weareinterestedinstudyingthediscrepancy
0 0
n
b (cid:88)
U (x(d))− u˜(1)(x(d);D ). (B.18)
n i
n
i=1
Definethequantities
1 (cid:88) 1 (cid:88)
Uˆ (x(d))= u(x(d);D ,ξ ) and U (x(d))= u˜(x(d);D ) (B.19)
n s s n s
N N
b b
s∈S s∈S
n,b n,b
forsomecollectionofindependentrandomvariablesξ =(ξ ) havingthesamedistributionasξ. The
s s∈S
n,b
statisticsUˆ (x(d))andU (x(d))arethecomplete,randomizedandde-randomized,U-statisticsassociated
n n
withtherandomizedd-dimensionalkernelu(x(d);·,·),respectively. Considerthedecomposition
(cid:114) n b (cid:88)n
U (x(d))− u˜(1)(x(d);D )
b2σ2 n n i
b i=1
(cid:114) n (cid:114) n
= (U (x(d))−Uˆ (x(d)))+ (Uˆ (x(d))−U (x(d)))
b2σ2 n n b2σ2 n n
b b
(cid:114) n b (cid:88)n
+ (U (x(d))− u˜(1)(x(d);D )). (B.20)
b2σ2 n n i
b i=1
Ahigh-probabilityboundforthethirdtermin(B.20)isobtainedfromTheorem4.2,statedinSection4.2. In
particular,Theorem4.2andtheconditions
blog(dn)
≤C <1 and b−C2 ≤σ2
n 1 b
implythat
(cid:114) n (cid:13) b (cid:88)n (cid:13)
(cid:13)U (x(d))− u˜(1)(x(d);D )(cid:13)
b2σ2(cid:13) n n i (cid:13) ∞
b i=1
(cid:32) (cid:33)
(cid:18) Cblog(dn)(cid:19)b/2 (cid:18)
n
(cid:19)1/2 (cid:18) blog4(dn)(cid:19)1/2
≲ + ϕ(θ )
n b2σ2 σ2 0
b b
(cid:18)
ϕ(θ
)2log2(dn)(cid:19)1/4
≲ 0 (B.21)
nσ2
b22
withprobabilitygreaterthan1−1/nforallblargerthansomeconstantb thatdependsonlyonC .
0 2
Boundsforthefirsttwotermsin(B.20)areobtainedfromLemmaA.5. Inparticular,observethat
Uˆ = 1 (cid:88) u(D ,ξ )=E(cid:2) u(D ,ξ )|D ,ξ(cid:3)
n,b
N
s s sg sg n
b
s∈S
n,b
andthatthereforewecanwrite
r
U −Uˆ = 1(cid:88) Z , with Z =u(D ,ξ )−E(cid:2) u(D ,ξ )|D ,ξ(cid:3) .
n n
r
1 q sq sq sq sq n
q=1
Conditioned on the data D and the residual randomness ξ, the observations Z , q ∈ [r], are centered,
n q
mutuallyindependent,andsatisfy
∥Z ∥ ≲ϕ(θ )
q,j ψ1 0
byassumption. Consequently,LemmaA.5impliesthat
(cid:114) n (cid:114) n (cid:18) ϕ(θ )log1/2(dn) ϕ(θ )2log(dn)(cid:19)
(U (x(d))−Uˆ (x(d)))≲ 0 + 0 (B.22)
b2σ2 n n b2σ2 r1/2 r
b b
withprobabilitygreaterthan1−n−1. Inturn,wecansimilarlywrite
1 (cid:88)
Uˆ −U = Z with Z =u(D ,ξ )−E[u(D ,ξ )|D ] .
n n s s s s s s s
N
b
s∈S
n,b
ConditionedonthedataD ,theobservationsZ ,s∈S ,arecentered,mutuallyindependent,andsatisfy
n s n,b
∥Z ∥ ≲ϕ(θ )
s ψ1 0
byassumption. Thus,LemmaA.5impliesthat
(cid:32) (cid:33)
(cid:114) n (cid:114) n ϕ(θ )log1/2(dn) ϕ(θ )2log(dn)
(Uˆ (x(d))−U (x(d)))≲ 0 + 0 (B.23)
b2σ2
b
n n b2σ2
b N
b1/2 N
b
Puttingthepiecestogether,thebounds(B.21),(B.22),and(B.23)implythat
(cid:114) n (cid:13) b (cid:88)n (cid:13)
(cid:13)U (x(d))− u˜(1)(x(d);D )(cid:13)
b2σ2(cid:13) n n i (cid:13) ∞
b i=1
(cid:32)
(cid:18) bϕ(θ )2log3(dn)(cid:19)1/4 (cid:114) n (cid:18) ϕ(θ )log1/2(dn) ϕ(θ )2log(dn)(cid:19)
≲ 0 + 0 + 0
n b2σ2 r1/2 r
b
(cid:32) (cid:33)(cid:33)
ϕ(θ )log1/2(dn) ϕ(θ )2log(dn)
0 0
+ + , (B.24)
1/2 N
N b
b23
with probability greater than 1−C/n. In the proof of Theorem 4.2, we show that σ2 ≲ b−1. Thus, the
b
√ √
restrictionn≤ rbimpliesthatn3/4 ≲ rbσ1/2 . Bycombiningthisinequalitywithn2b2 ≲N ,wefindthat
b b
(cid:114) n (cid:13)
(cid:13)U (x(d))−
b (cid:88)n
u˜(1)(x(d);D
)(cid:13)
(cid:13)
≲(cid:18) ϕ(θ 0)2log2(dn)(cid:19)1/4
b2σ2(cid:13) n n i (cid:13) ∞ nσ2
b i=1 b
withprobabilitygreaterthan1−C/n,asrequired.
B.1.2 ProofofLemmaB.2,Part(i). Webeginbyverifyingtheinequality(B.8). Observethat
(cid:34) n (cid:35)
(cid:88)
E[M (x;θ,g)]=E K(x,X )m(D ;θ,g)
n i i
i=1
1 (cid:88) (cid:88)
= E[κ(x,X ,s,ξ)m(D ;θ,g)]
i i
N
b
s∈S i∈s
n,b
=
1 (cid:88) (cid:88) E(cid:2)E(cid:2)
κ(x,X ,s,ξ)|X ,D
(cid:3)E(cid:2)
m(D ;θ,g)|X ,D
(cid:3)(cid:3)
(Honesty)
N
i i s−i i i s−i
b
s∈S i∈s
n,b
1 (cid:88) (cid:88)
= E[κ(x,X ,s,ξ)E[m(D ;θ,g)|X ]]
i i i
N
b
s∈S i∈s
n,b
1 (cid:88) (cid:88)
= E[κ(x,X ,s,ξ)M(X ;θ,g)] .
i i
N
b
s∈S i∈s
n,b
Therefore,thenormalization
(cid:88)
κ(x,X ,s,ξ)=1 (B.25)
i
i∈s
impliesthat
1 (cid:88) (cid:88)
Bias (x;θ,g)= E[κ(x,X ,s,ξ)(M(X ;θ,g)−m(x;θ,g))] .
n i i
N
b
s∈S i∈s
n,b
BytheboundednesspartofAssumption3.2andAssumption3.3,Part(iii),wefindthat
Bias (x;θ,g)≲(1+|θ|)E[κ(x,X ,s,ξ)∥X −x∥ ]≤(1+|θ|)ε , (B.26)
n i i ∞ n
wherefinalinequalityfollowsfromthedefinitionoftheshrinkagerateε andthenormalization(B.25).
n
Next,weverifytheinequality(B.9). Definethefunction
(cid:88)(cid:16) (cid:104) (cid:105)(cid:17)
J(x;D ,ξ )= κ(x,X ,s,ξ )m(1)(D ;θ,g)−E κ(x,X s,ξ )m(1)(D ;θ,g)
s s i s i i s i
i∈s
andobservethat
n
(1) 1(cid:88)
M (x;θ ,g )= J(x;D ,ξ ). (B.27)
n 0 0 r sq sq
q=1
Considerthedecomposition
M(1) (x;θ ,g )=A˜(x)+Aˆ(x)+A(x), (B.28)
n 0 024
where
n
A˜(x)= 1(cid:88)(cid:0) J(x;D ,ξ )−E(cid:2) J(x;D ,ξ )|D ,ξ(cid:3)(cid:1) , (B.29)
r
sq sq sq sq n
q=1
1 (cid:88)
Aˆ(x)= (J(x;D ,ξ )−E[J(x;D ,ξ |D ]) , and (B.30)
s s s s s
N
b
s∈S
n,b
1 (cid:88)
A(x)= E[J(x;D ,ξ )|D ] , (B.31)
s s s
N
b
s∈S
n,b
respectively. WeagainapplyLemmaA.5tobound(B.29)and(B.30). Inparticular,byboundednesspartof
Assumption3.2,LemmaA.5conditionedonD andξ andconditionedonD impliesthat
n n
(cid:26) ϕlog1/2(dr)(cid:27)
1
P ∥A˜(x(d)))∥ ≥C ≲ and (B.32)
∞
r1/2 n
(cid:40) (cid:41)
ϕlog1/2(dN ) 1
P ∥Aˆ(x(d))∥ ≥C b ≲ (B.33)
∞
1/2 n
N
b
respectively. Inturn,weapplyLemma4.1,statedinSection4.1,toboundtheterm(B.45). Inthiscase,by
boundednesspartofAssumption3.2,Lemma4.1impliesthat
(cid:26) ϕb1/2log1/2(dn)(cid:27)
1
P ∥A (x(d))∥ ≥C ≲ . (B.34)
n ∞
n1/2 n
Thus,thedecomposition(B.42),Lemma4.1,andthebounds(B.46)and(B.47)imply
P
(cid:26)
∥M(1) (x;θ ,g )∥
≥Cϕb1/2log1/2(dn)(cid:27)
≲ 1 , (B.35)
n 0 0 ∞ n1/2 n
asn≤br1/2.
B.1.3 ProofofLemmaB.2,Part(ii). Bymomentlinearity,i.e.,Assumption3.2,wehavethat
(cid:16) (cid:17)−1(cid:16) (cid:17)
θˆ (x)−θ (x)= M(1)(x;g ) M(x;θˆ (x),g )−M(x;θ ,g ) (B.36)
n 0 0 n 0 0 0
(cid:16) (cid:17)−1(cid:16) (cid:17)
= M(1)(x;g ) M(x;θˆ (x),g )−M (x;θˆ (x),gˆ ,D )
0 n 0 n n n n
(cid:16) (cid:17)−1
=− M(1)(x;g ) M (x;θˆ (x),gˆ ) (B.37)
0 n n n
(cid:16) (cid:17)−1(cid:16) (cid:17)
+ M(1)(x;g ) Bias(x;θˆ (x),gˆ )+Nuis(x;θˆ (x),gˆ ) (B.38)
0 n n n n
wherewerecallthat
Bias(x;θ,g)=M(x;θ,g)−E[M (x;θ,g,D )] and (B.39)
n n
Nuis(x;θ,g)=M(x;θ,g )−M(x;θ,g). (B.40)
025
Webeginbystudyingtheterm(B.37),i.e.,
(cid:16) (cid:17)−1
Q (x;θ,g)= M(1)(x;g ) M (x;θ,g,D ), (B.41)
n 0 n n
Considerthedecomposition
Q (x;θ,g)=Q˜ (x;θ,g)+Qˆ (x;θ,g)+Q (x;θ,g), (B.42)
n n n n
where
r
Q˜ (x;θ,g)= 1(cid:88)(cid:0) u(x;D ,ξ ,θ,g)−E(cid:2) u(x;D ,ξ ,θ,g)|D ,ξ(cid:3)(cid:1) , (B.43)
n
r
sq sq sq sq n
q=1
1 (cid:88)
Qˆ (x;θ,g)= (u(x;D ,ξ ,θ,g)−E[u(x;D ,ξ ,θ,g)|D ]) , and (B.44)
n s s s s s
N
b
s∈S
n,b
1 (cid:88)
Q (x;θ,g)= E[u(x;D ,ξ ,θ,g)|D ] . (B.45)
n N s s s
b
s∈S
n,b
We again apply Lemma A.5 to bound (B.43) and (B.44). In particular, by the boundedness part of
Assumption3.2,LemmaA.5conditionedonD andξ andconditionedonD impliesthat
n n
(cid:26) (1+∥θ(x(d))∥ )ϕlog1/2(dr)(cid:27) 1
P ∥Q˜(x(d),θ(x(d)),g)∥ ≥C ∞ ≲ and (B.46)
∞
r1/2 n
(cid:40) (cid:41)
(1+∥θ(x(d))∥ )ϕlog1/2(dN ) 1
P ∥Qˆ(x(d),θ(x(d)),g)∥ ≥C ∞ b ≲ (B.47)
∞
1/2 n
N
b
respectively. Inturn,weapplyLemma4.1,statedinSection4.1,toboundtheterm(B.45). Inthiscase,by
theboundednesspartofAssumption3.2,Lemma4.1impliesthat
(cid:26) (1+∥θ(x(d))∥ )ϕb1/2log1/2(dn)(cid:27) 1
P ∥Q (x(d);θ(x(d)),g)∥ ≥C ∞ ≲ . (B.48)
n ∞ n1/2 n
Thus,thedecomposition(B.42),Lemma4.1,andthebounds(B.46)and(B.47)imply
(cid:26) (1+∥θ(x(d))∥ )ϕb1/2log1/2(dn)(cid:27) 1
P ∥Q (x(d);θ(x(d)),g)∥ ≥ ∞ ≲ , (B.49)
n ∞
n1/2 n
asn≤br1/2.
Now,turningtotheterm
(cid:16) (cid:17)−1(cid:16) (cid:17)
B (x;θˆ (x),gˆ )= M(1)(x;g ) Bias(x;θˆ (x),gˆ )+Nuis(x;θˆ (x),gˆ ) , (B.50)
n n n 0 n n n n
combiningthebounds(A.43)and(A.44)givenintheproofofTheoremA.2,Part(ii),withthebound(B.26)
derivedinPart(i)ofthisLemmaimpliesthat
(cid:110) (cid:111)
P ∥B (x(d);θ(x(d)),g)∥ ≥Cτ′ ≲ρ , (B.51)
n ∞ n n,g26
where
(cid:32) (cid:33)
(cid:18) b2σ2(cid:19)1/2 (cid:18) b2σ2(cid:19)1/4
τ′ = b δ2 +ε +∥θ(x(d))−θ (x(d))∥ b δ +ε . (B.52)
n n n,g n 0 ∞ n n,g n
Consequently,thedecomposition(B.36),impliesthat
∥θˆ (x(d))−θ (x(d))∥
n 0 ∞
(cid:18) b2σ2(cid:19)1/2 (cid:18) ϕb1/2log1/2(dn)(cid:19)
≲ b δ2 +ε +(1+∥θˆ (x(d))∥ )
n n,g n n ∞ n1/2
(cid:32) (cid:33)
(cid:18) b2σ2(cid:19)1/4
+∥θˆ (x(d))−θ (x(d))∥ b δ +ε
n 0 ∞ n,g n
n
(cid:18) b2σ2(cid:19)1/2 (cid:18) ϕb1/2log1/2(dn)(cid:19)
≲ b δ2 +ε +(1+∥θ (x(d))∥ )
n n,g n 0 ∞ n1/2
(cid:32) (cid:33)
(cid:18) b2σ2(cid:19)1/4 ϕb1/2log1/2(dn)
+∥θˆ (x(d))−θ (x(d))∥ b δ +ε +
n 0 ∞ n,g n
n n1/2
withprobabilitygreaterthan1−ρ −C/n. Thus,bytherestrictionsthatε ≤c,
n,g n
(cid:18) b2σ2(cid:19)1/4 ϕ2blog(dn)
b δ ≤c, and ≤c
n,g
n n
forallsufficientlylargen,wefindthat
(cid:110) (cid:111) 1
P ∥θˆ (x(d))−θ (x(d))∥ ≥Cτ ≲ρ + , where
n 0 ∞ n n,g
n
(cid:18) b2σ2(cid:19)1/2 ϕb1/2log1/2(dn)
τ = b δ2 +ε +(1+∥θ (x(d))∥ ) ,
n n n,g n 0 ∞ n1/2
asrequired.
B.1.4 ProofofLemmaB.2,Part(iii). Wegivethedetailsoftheproofofthestatedprobabilityboundonthe
discrepancy
(cid:13) (cid:13)M n(x(d);θ 0(x(d)),gˆ n)−M n(x(d);θ 0(x(d)),g 0)(cid:13) (cid:13)
∞
. (B.53)
only. TheargumentgivingtheanalogousboundassociatedwiththetermM(1) (x(d);θ,g)isidentical. Define
n
thefunctions
(cid:88)(cid:16)
W(x;D ,ξ ,g)= κ(x,X ,D ,ξ )(m(D ;θ ,g)−m(D ;θ ,g )) (B.54)
s s i s s i 0 i 0 0
i∈s
(cid:17)
−E[κ(x,X ,D ,ξ )(m(D ;θ ,g)−m(D ;θ ,g ))] and
i s s i 0 i 0 0
W(x;D )=E [f(x;D ,ξ )] , (B.55)
s ξs s s27
wherethenotationE [·]indicatesthatweareevaluatingtheexpectationovertherandomnessinA. Define
A
thequantities
r
W˜ (x;g)= 1(cid:88)(cid:0) W(x;D ,ξ ,g)−E [W(x;D ,ξ ),g](cid:1) , (B.56)
n
r
sq sq s s s
q=1
Wˆ (x;g)= 1 (cid:88) (cid:0) W(x;D ,ξ ,g)−W(x;D ,g)(cid:1) , and (B.57)
n s s s
N
b
s∈S
n,b
1 (cid:88)
W (x;g)= W(x;D ,g). (B.58)
n s
N
b
s∈S
n,b
andconsiderthedecomposition
M (x;θ (x),g)−M (x;θ (x),g )=W˜ (x,g)+Wˆ (x,g)+W (x,g). (B.59)
n 0 n 0 0 n n n
Toboundtheterm(B.56),weapplyLemmaA.5conditionedonallrandomnessexcepttherandomsubsets
s ,...,s . Inparticular,as
1 q
E (cid:2) g(x;D ,ξ )−E [g(x;D ,ξ )](cid:3) =0, and
sq sq sq s s s
(cid:104) (cid:105)
∥g(x(d);D ,ξ )−E g(x(d);D ,ξ ) ∥ ≲(1+∥θ (x(d))∥ )ϕ
sq sq s s s ∞ 0 ∞
bytheboundednesspartofAssumption3.2,wehavethat
(cid:26) (1+∥θ (x(d))∥ )ϕlog1/2(dr)(cid:27) 1
P ∥G˜ (x(d))∥≥C 0 ∞ ≲ . (B.60)
n
r1/2 n
Toboundtheterm(B.57),weagainapplyLemmaA.5conditionedonallrandomnessexceptξ. As
E (cid:2) W(x;D ,ξ ,g)−W(x;D ,g)(cid:3) =0, and
ξs s s s
∥W(x(d);D ,ξ ,g)−W(x(d);D ,g)∥ ≲(1+∥θ (x(d))∥ )ϕ
s s s ∞ 0 ∞
bytheboundednesspartofAssumption3.2,wehavethat
(cid:40) (cid:41)
(1+∥θ (x(d))∥ )ϕlog1/2(dN ) 1
P ∥Wˆ (x(d))∥≥C 0 ∞ b ≲ . (B.61)
n
1/2 n
N
b
Toboundtheterm(B.58),weapplyLemma4.1. Inparticular,observethat
E[W(x(d);D ,g)]=0 and ∥W(x(d);D ,g)∥ ≲(1+∥θ(x(d))∥ )ϕ
s s ψ1 ∞
bytheboundednesspartofAssumption3.2. Inturn,observethat
(cid:34) (cid:35)
E(cid:104) (cid:0) W(x;D ,g)(cid:1)2(cid:105) ≤E (cid:88) κ(x,X ,D ,ξ )E(cid:2) (m(D ;θ ,g)−m(D ;θ ,g ))2 |X (cid:3)
s i s s i 0 i 0 0 i
i∈s
(cid:34) (cid:35)
(cid:88)
≲E κ(x,X ,D ,ξ )V(x,g) +ε
i s s n
i∈s28
≲∥g−g ∥2 +ε
0 ∞ n
wherethefirstinequalityfollowsfromHonestyandJensen’sinequality,thesecondinequalityfollowsfrom
Assumption3.3,Part(ii),andthedefinitionoftheshrinkagerateε ,andthethirdinequalityfollowsfrom
n
(cid:80)
Assumption3.3,Part(ii),andthenormalizationthat κ(x,X ,D ,ξ)=1almostsurely. Thus,weobtain
i∈s i s
(cid:110) (cid:111) 1
P ∥W (x(d);g)∥≥Cξ′ ≲ , where (B.62)
n n n
(cid:115)
ξ′ =
b(∥g−g 0∥2 2,∞+ε n)log(dn)
+
b(1+∥θ 0(x(d))∥ ∞)ϕlog2(dn)
.
n n n
Now,observethat
(cid:115)
b(∥gˆ n−g 0∥2 2,∞+ε n)log(dn) ≲(cid:114) blog(dn)
∥gˆ −g ∥
+(cid:114) blog(dn)
ε1/2
n 0 2,∞ n
n n n
(cid:114) blog(dn)(cid:32) (cid:18) b2σ2(cid:19)1/4 (cid:33)
≲ b δ +ε1/2
n,g n
n n
withprobabilitygreaterthan1−ρ . Puttingthepiecestogether,asn ≤ br1/2,thedecomposition(B.59)
n,g
andthebounds(B.60),(B.61),and(B.62)implythat
P (cid:110)(cid:13) (cid:13)M n(x(d);θ 0(x(d)),gˆ n)−M n(x(d);θ 0(x(d)),g 0)(cid:13) (cid:13)
∞
≥Cτ n,S(cid:111) ≲ n1 where
(cid:114) blog(dn)(cid:32) (cid:18) b2σ2(cid:19)1/4 (cid:33)
b
τ = b δ +ε1/2 + (1+∥θ (x(d))∥ )ϕlog2(dn),
n,S n,g n 0 ∞
n n n
asrequired. ■
APPENDIX C. PROOFS FOR RESULTS STATED IN SECTION 4
C.1 ProofofTheorem4.1. Wedropthedependenceonx(j) toeasenotation. Theargumentwillfollowby
firstre-expressingtheU-statisticofinterestintermsofitsHoeffdingexpansion,statedasfollows(seee.g.,
EfronandStein(1981)).
LemmaC.1.LetZ ,...,Z beacollectionofbindependentandidenticallydistributedreal-valuedrandom
1 b
variables and f : Rb → R denote some symmetric function satisfying Var(f(Z )) < ∞. There exist
[b]
functionsf ,...,f suchthat
1 b
b
(cid:88) (cid:88)
f(Z ,...,Z )=E[f(Z ,...,Z )]+ f (Z ), (C.1)
1 b 1 b l s
l=1s∈S
b,l
andall2b−1randomtermsontheright-handsideof (C.1)aremean-zeroanduncorrelated. Moreover,the
functionf (·)isgivenbytheHa´jekprojectionf (z)=E[f(Z ,...,Z )|Z =z].
1 1 1 n 129
Inparticular,LemmaC.1impliesthatthereexistfunctionsu˜(1)(·),...,u˜(b)(·)suchthat
b
(cid:88) (cid:88)
u˜(x(j);D )= u˜(l)(D ) (C.2)
[b] s
l=1s∈S
b,l
andthatall2b−1termsontheright-handsideof(C.2)aremean-zeroanduncorrelated. Thus,wehavethat
b (cid:18) (cid:19)
(cid:88) b
Var(u˜(D ))= Var(u˜(l)(D )) (C.3)
s s
l
l=1
andthatthereby
(cid:18) (cid:19)
b
Var(u˜(l)(D ))≤Var(u˜(D )). (C.4)
s s
l
Moreover,wecanwrite
b (cid:18) (cid:19)(cid:18) (cid:19)−1
1 (cid:88) (cid:88) b n (cid:88)
u˜(D )= u˜(l)(D ), (C.5)
s s
N l l
b
s∈S n,b l=1 s∈S n,l
andthereby
 
b (cid:18) (cid:19)−2(cid:18) (cid:19)
1 (cid:88) (cid:88) n b
Var u˜(D s)= Var(u˜(l)(D s)), (C.6)
N l l
b
s∈S l=1
n,b
throughasimplecountingargument. Consequently,wehavethat
 
n b (cid:18) (cid:19)−2(cid:18) (cid:19)
1 (cid:88) b (cid:88) (cid:88) n b
Var u˜(D s)− u˜(1)(D s)= Var(u˜(l)(D s))
N n l l
b
s∈S i=1 l=2
n,b
b (cid:18) (cid:19)−2
(cid:88) n
≤Var(u˜(D ))
s
l
l=2
(cid:18) (cid:19)2
b
≤Var(u˜(D )) , (C.7)
s
n
bytheinequality(C.4). Now,considerthedecomposition
 
(cid:115) (cid:115) n
n 1 (cid:88) 1 (cid:88)
σ2 b2

N
u˜(D s)=
σ2 n
u˜(1)(D s) (C.8)
b,j b s∈S b,j i=1
n,b
 
(cid:115) n
n 1 (cid:88) b (cid:88)
−
σ2 b2

N
u˜(D s)−
n
u˜(1)(D s) . (C.9)
b,j b s∈S i=1
n,b
Observethatthenormalization(4.3)andChebychev’sinequalityimplythattheterm(C.9)iso (1)asn→∞.
p
Theresultthenfollowsbyapplyingthecentrallimittheoremtotheterm(C.8).30
C.2 ProofofTheorem4.2. Weareinterestedinstudyingthequantity
(cid:114) n (cid:32) b (cid:88)n (cid:33)
U (x(d))− u˜(1)(x(d);D ) . (C.10)
b2σ2 n,b n i
b i=1
Observethat
(cid:18) (cid:19)(cid:18) (cid:19)−1
b n−1 n
u˜(1)(x(d);D )= u˜(1)(x(d);D )
i i
n b−1 b
1 (cid:88)
= u˜(1)(x(d);D )I{i∈s} (C.11)
i
N
b
s∈S
n,b
andthatconsequentlythedifference(C.10)canbewritten
 
(cid:32) (cid:33)
(cid:114) n 1 (cid:88) (cid:88)
b2σ2

N
u˜(x(d);D s)− u˜(1)(x(d);D i)  . (C.12)
b b s∈S i∈s
n,b
Inotherwords,thedifference(C.10)canbere-expressedasascaled,complete,U-statisticoforderbwiththe
kernelfunction
(cid:88)
h(x(d);D )=u˜(x(d);D )− u˜(1)(x(d);D ). (C.13)
s s i
i∈s
Moreover,thekernelfunction(C.13)iscompletelydegenerate,inthestandardsensethat
(cid:104) (cid:105)
E h(x(d);D )|i∈s,D =0 (C.14)
s i
almostsurely.
Togiveahigh-probabilityboundonthedifference(C.12),weinvertaboundonthehigher-ordermoment
 
(cid:12) (cid:12)q
E (cid:12) (cid:12) 1 (cid:88) h(x;D s)(cid:12) (cid:12) 
(cid:12)N (cid:12)
b
s∈S
n,b
forarbitraryq ≥2,wherexisanarbitraryelementofthequery-vectorx(d). Weexpressthisproblemmore
tractablythroughasymmetrizationargument. Inparticular,weapplythefollowingsymmetrizationinequality,
duetoSherman(1994). SeeTheorem5.2ofSongetal.(2019)foranexpeditedproof.
Lemma C.2 (Theorem 5.2, Song et al. (2019)).Let Z ,...,Z denote a collection of independent and
1 n
identicallydistributedreal-valuedrandomvariables. Considerareal-valuedsymmetrickernelfunctionf of
orderbthatsatisfies
E[f(Z ,...,Z )|Z ]=0 (C.15)
1 b 1
almostsurely. LetV ,...,V denoteanindependentcollectionofRademacherrandomvariables. IfΦ(·)is
1 n
anyconvex,non-negative,andnon-decreasingfunction,thenthesymmetrizationinequality
     
(cid:88) (cid:88)
E Φ f(Z s)≤E Φ2b V sf(Z s) (C.16)
s∈S s∈S
n,b n,b31
holds,whereV =Π V foreachsubsetsin[n].
s i∈s i
The application of Lemma C.2 is facilitated by the following moment bound for higher moments of
Rademacherchaos,oftenreferredtoastheBonamiinequality.
Lemma C.3 (Theorem 3.2.2, De la Pena and Gine´ (1999)).Fix a collection of real-valued quantities
{z : s ∈ S } and let V ,...,V denote an independent collection of Rademacher random variables.
s n,b 1 n
ConsiderthehomogeneousRademacherchaosoforderb,givenby
(cid:88)
Z = V z , (C.17)
b s s
s∈S
n,b
whereV =Π V foreachsubsetsin[n]. Themomentinequality
s i∈s i
(cid:88)
E[|Z |q]≤qbq/2(∆ )q/2 , where ∆ = (z )2 , (C.18)
b b b s
s∈S
n,b
holdsforeveryq >2.
LemmaC.3impliesthat
   q/2
(cid:12) (cid:12)q (cid:18) (cid:19)−1
E (cid:12) (cid:12) 1 (cid:88) V sh(x;D s)(cid:12) (cid:12) |D n≤qbq/2  1 (cid:88) n (h(x;D s))2  . (C.19)
(cid:12)N (cid:12) N b
b b
s∈S s∈S
n,b n,b
Consequently,LemmaC.2impliesthat
    
(cid:12) (cid:12)q (cid:12) (cid:12)q
E (cid:12) (cid:12) 1 (cid:88) h(x;D s)(cid:12) (cid:12) ≤2bqE E (cid:12) (cid:12) 1 (cid:88) V sh(x;D s)(cid:12) (cid:12) |D n
(cid:12)N (cid:12) (cid:12)N (cid:12)
b b
s∈S s∈S
n,b n,b
 
 q/2
(cid:18) (cid:19)−1
1 (cid:88) n
≤2bqqbq/2E 
N b
(h(x;D s))2  

(C.20)
b
s∈S
n,b
To simplify the expression (C.20), we apply the following representation of complete U-statistics, due to
Hoeffding (1948). To express this result, we require some additional notation. Let P denote the set of
n
permutationsof[n],treatingeachpermutationπ inP asabijectionfrom[n]to[n]. Foreachpermutationπ,
n
definetheset
s ={π((l−1)b),...,π(lb)}. (C.21)
π,l
Observethatifnisdivisiblebyb,thecollections ,...,s isamutuallyexclusivepartitionoftheset[n]
π,1 π,n/b
foreachpermutationπ.
LemmaC.4(Hoeffding(1948)).ThecompleteU-statisticoforderbwithkernelfunctionu(·)admitsthe
alternativerepresentations
(cid:22) (cid:23)⌊n/b⌋
1 (cid:88) 1 (cid:88) b (cid:88)
U = u(D )= u(D ),
n s s
N n! n π,l
b
s∈S n,b π∈P l=132
where⌊x⌋denotesthelargestintegersmallerthanorequaltox.
Inparticular,byLemmaC.4,Jensen’sinequality,andthebound
(cid:18) (cid:19)
(cid:16)n(cid:17)b n
≤
b b
wehavethat
 
 q/2
(cid:18) (cid:19)−1
1 (cid:88) n
2bqqbq/2E 
N b
(h(x;D s))2  

b
s∈S
n,b
 
 q/2
(cid:22) (cid:23)−1⌊n/b⌋(cid:18) (cid:19)−1
=2bqqbq/2E  n1 ! (cid:88) n b (cid:88) n b (cid:0) h(x;D s π,l)(cid:1)2   
π∈Pn l=1
 
 q/2
(cid:22) (cid:23)−1⌊n/b⌋(cid:18) (cid:19)b
≤2bqqbq/2E  n b (cid:88) nb (cid:0) h(x;D s π,l)(cid:1)2    , (C.22)
l=1
where π is an arbitrary element of P . We note that the summands in (C.22) are now independent and
n
identicallydistributed.
Toboundtheexpectation(C.22),weapplythefollowingversionofRosenthal’sinequalityfornon-negative
randomvariables.
LemmaC.5(Theorem15.13,Boucheronetal.(2013)).LetZ ,...,Z denoteacollectionofindependent
1 n
real-valuedandnon-negativerandomvariables. Forallq ≥1,themomentinequality
(cid:34) n (cid:35)1/q

(cid:34) n (cid:35)1/2 (cid:20)
(cid:21)1/2q2
E (cid:12) (cid:12)(cid:88) Z i(cid:12) (cid:12)q ≤E (cid:88) Z i +Cq1/2E max|Z i|q 
i∈[n]
i=1 i=1
holds.
Inparticular,LemmaC.5andtheBinomialTheoremimplythat
 
 q/2
(cid:22) (cid:23)−1⌊n/b⌋(cid:18) (cid:19)b
2bqqbq/2E  n b (cid:88) nb (cid:0) h(x;D s π,l)(cid:1)2   
l=1
(cid:32)

(cid:22) (cid:23)−1⌊n/b⌋(cid:18) (cid:19)b
1/2
≤Cbqqbq/2 E  n (cid:88) b (cid:0) h(x;D s )(cid:1)2 
b n π,l
l=1
+(cid:16)q(cid:17)1/2
E
 max
(cid:32)
(cid:18) b(cid:19)b+1
(cid:0)
h(x;D s
)(cid:1)2(cid:33)q/2 1/q(cid:33)q
2 l∈[n/b] n π,l
(cid:32) (cid:33)
(cid:18) (cid:19)bq/2 (cid:18) (cid:19)q/2 (cid:20) (cid:21)
≤Cbqqbq/2 b E(cid:2) (h(x;D ))2(cid:3)q/2 +qq/2 b E max (cid:0) h(x;D )(cid:1)q . (C.23)
s s
n n l∈[n/b] π,l33
Itremainstoboundthetwomomentsin(C.23). Toboundthevarianceterm,weapplyaHoeffdingexpansion
to the symmetric statistic u˜(x;D ), stated as Lemma C.1 in the proof of Theorem 4.1. In particular,
s
π,l
LemmaC.1impliesthatthereexistfunctionsu˜(1),...,u˜(b) suchthat
b
(cid:88) (cid:88)
u˜(x;D )= u˜(l)(x;D ), (C.24)
s s
[b]
l=1s∈s
b,l
whereall2b−1randomtermsontheright-handsideof(C.24)aremean-zeroanduncorrelated. Consequently,
wefindthat
(cid:32) b (cid:33)
(cid:16) (cid:17) (cid:88)
Var h(x;D ) =Var u˜(x;D )− u˜(1)(x;D )
s s s
[b] [b] [b]
i=1
 
b
(cid:88) (cid:88)
=Var u˜(l)(x;D s)
l=2s∈s
b,l
=Var(u˜(x;D ))−bVar(u˜(1)(x;D ))≤γ2 . (C.25)
s [b] i b
Toboundthehigherordermomentin(C.23),weapplythefollowingstandardmaximalinequality.
LemmaC.6.LetZ ,...,Z denoteacollectionofcenteredreal-valuedrandomvariables. If∥Z ∥ ≤φfor
1 k j ψ1
allj in[k],then
(cid:20) (cid:21)
E max|Z |q ≲(2qφlog(2k))q . (C.26)
j
j∈[k]
Proof. Weapplythefollowingstandardmaximalinequality.
LemmaC.7(Lemma5.5,Songetal.(2019)).LetZ ,...,Z denoteacollectionofcentered,real-valued,
1 k
randomvariables. Foreachβ in(0,1),definethestandardconvexifiedYoungfunction
(cid:110) (cid:111)
ψ˜ (z)=(βe)1/βzI z <(1/β)1/β +exp(zβ)I{z ≥β}. (C.27)
β
Ifthereexistssomeconstantφ>0suchthatE(cid:2) ψ˜ (|Z |/φ)(cid:3) ≤2foreachj in[k],thenthemomentbound
β j
(cid:20) (cid:21)
E max|Z | ≤φ21/β(1/β)1/βlog1/β(2d) (C.28)
j
j∈[k]
holds.
Observethat
E(cid:2) ψ˜ (|Z |q/φq)(cid:3) ≤E(cid:2) exp(|Z |/φ)(cid:3) ≤2 (C.29)
(1/q) j j
by the fact that ψ˜ (z) ≤ exp(z1/q) and the assumption that ∥Z ∥ = φ. Consequently, Lemma C.7
(1/q) j ψ1
impliesthat
(cid:20) (cid:21)
E max|X |q ≤(2qlog(2k)φ)q , (C.30)
j
j∈[k]
asrequired.34
Ifisclearthat∥h(x;D )∥ ≤(b+1)ϕ,byassumption,andsoLemmaC.6impliesthat
s [b] ψ1
(cid:20) (cid:21)
E max (cid:0) h(x;D )(cid:1)q ≲(4qbϕlog(2n))q . (C.31)
s
π,l
l∈[n/b]
Puttingthepiecestogether,thebounds(C.25)and(C.31)implythat
 
(cid:12) (cid:12)q
E (cid:12) (cid:12) 1 (cid:88) h(x;D s)(cid:12) (cid:12) 
(cid:12)N (cid:12)
b
s∈S
n,b
(cid:32) (cid:33)
(cid:18) (cid:19)bq/2 (cid:18) (cid:19)q/2 (cid:20) (cid:21)
≤Cbqqbq/2 b E(cid:2) (h(x;D ))2(cid:3)q/2 +qq/2 b E max (cid:0) h(x;D )(cid:1)q
s s
n n l∈[n/b] π,l
(cid:18) b(cid:19)bq/2(cid:32) (cid:18) b3/2(cid:19)q (cid:33)
≤Cbqqbq/2 γq +q3q/2 ϕqlogq(n)
n b n1/2
(cid:32)
(cid:18) b(cid:19)b/2(cid:18) (cid:18) b3/2(cid:19)
(cid:19)(cid:33)q
= Cbqb/2 γ +q3/2 ϕlog(n) , (C.32)
n b n1/2
Hence,anapplicationofMarkov’sinequalityandaunionboundimpliesthat
P(cid:26)(cid:114)
b2n
σ2(cid:13)
(cid:13) (cid:13)
(cid:13)(cid:32)
U n(x(d))− nb
(cid:88)n
u˜(1)(x(d);D
i)(cid:33)(cid:13)
(cid:13) (cid:13)
(cid:13)
b i=1 ∞
(cid:32) (cid:33)
(cid:18) (cid:19)b/2 (cid:18) (cid:19)1/2 (cid:18) (cid:19)1/2 (cid:27)
b n b
≥ Cq γ +q3/2 ϕlog(n) ≤dexp(−q). (C.33)
n b2σ2 b σ2
b b
Throughthechoiceq =log(dn),wefindthat
(cid:114) b2n σ2(cid:13) (cid:13) (cid:13) (cid:13)(cid:32) U n(x(d))− nb (cid:88)n u˜(1)(x(d);D i)(cid:33)(cid:13) (cid:13) (cid:13)
(cid:13)
b i=1 ∞
(cid:32) (cid:33)
(cid:18) Cblog(dn)(cid:19)b/2 (cid:18) nγ2 (cid:19)1/2 (cid:18) bϕ2log4(dn)(cid:19)1/2
≤ b + (C.34)
n b2σ2 σ2
b b
withprobabilitygreaterthan1−1/n,asrequired.
C.3 ProofofCorollary4.1,Part(i). Observethat
(cid:114) (cid:114) n
n 1 (cid:88)
Σ−1/2U (x(d))= Σ−1/2u(1)(x(d);D ) (C.35)
b2 n,b n i
i=1
(cid:114) (cid:32) n (cid:33)
n b (cid:88)
+ Σ−1/2 U (x(d))− u(1)(x(d);D ) .
b2 n,b n i
i=1
LemmaA.5impliesthat
(cid:13)(cid:114) 1 (cid:88)n (cid:13) ϕlog2(dn)
(cid:13) Σ−1/2u˜(1)(x(d),D )(cid:13) ≲log1/2(dn)+ (C.36)
(cid:13) n
i=1
i (cid:13) ∞ σ bn1/2
withprobabilitygreaterthan1−C/n. Theresultthenfollowsfrom(C.35)andTheorem4.2.35
C.4 ProofofCorollary4.1,Part(ii). FixarectangleR=[a ,a ]inR,wherea anda arevectorsinRd
l u l u
witha ≤a ,interpretedcomponentwise. DefinetheenlargedrectangleR =[a −t1 ,a +t1 ]foreach
l u t l d u d
t>0. Definethenormalizedquantity
uˆ(1)(x(d);D )=Σ−1/2u˜(1)(x(d);D ). (C.37)
i i
Observethatthedecomposition(C.35)impliesthat
(cid:12) (cid:26)(cid:114) (cid:27) (cid:12)
(cid:12) (cid:12) (cid:12)P bn 2Σ−1/2U n,b(x(d))∈R −P (cid:110) Σ−1/2Z ∈R(cid:111)(cid:12) (cid:12)
(cid:12)
(cid:12) (cid:40) n (cid:41) (cid:12)
≤(cid:12) (cid:12)P √1 (cid:88) uˆ(1)(x(d);D i)∈R t −P (cid:110) Σ−1/2Z ∈R t(cid:111)(cid:12) (cid:12) (C.38)
(cid:12) n (cid:12)
i=1
(cid:12) (cid:12)
+(cid:12) (cid:12)P (cid:110) Σ−1/2Z ∈R t(cid:111) −P (cid:110) Σ−1/2Z ∈R t(cid:111)(cid:12) (cid:12) (C.39)
(cid:12) (cid:12)
(cid:12) (cid:40) (cid:114) n (cid:41)(cid:12)
+(cid:12) (cid:12) (cid:12)P (cid:13) (cid:13) bn 2Σ−1U n,b(x(d))− √1 n(cid:88) uˆ(1)(x(d);D i)(cid:13) (cid:13)
∞
>t (cid:12) (cid:12)
(cid:12)
(C.40)
i=1
We bound the normal approximation term (C.38) through the application of Lemma A.3. In particular,
observethat
n
1 (cid:88) (cid:104) (cid:105)
E uˆ2(x(j);D ) =1 (C.41)
i
n
i=1
bydefinition. Additionally,wehavethat
n
1 (cid:88) (cid:104) (cid:105)
E uˆ4(x(j),D ) ≤(ϕ/σ )2 (C.42)
n i b
i=1
andthat
∥uˆ(cid:0) xj),D (cid:1) ∥ ≤(ϕ/σ ) (C.43)
i ψ1 b
byassumption. Consequently,as
Var(uˆ(x(j),D ))=Σ−1/2Var(Z)Σ−1/2, (C.44)
i
bydefinition,LemmaA.3impliesthat
(cid:12) (cid:12) (cid:12) (cid:12)P (cid:26)(cid:114) bn 2Σ−1/2U n,b(x(d))∈R(cid:27) −P (cid:110) Σ−1/2Z ∈R(cid:111)(cid:12) (cid:12) (cid:12) (cid:12)≲(cid:18) ϕ2lo σg 25 n(dn)(cid:19)1/4 . (C.45)
b
Now, toboundthedifferencesintheGaussianprobabilities(A.20)and(A.23), weapplyLemmaA.1. In
particular,wehavethat
(cid:12) (cid:110) (cid:111) (cid:110) (cid:111)(cid:12) (cid:112)
(cid:12)P Σ−1/2Z ∈R −P Σ−1/2Z ∈R (cid:12)≤t log(d). (C.46)
(cid:12) t t (cid:12)
Theresultthenfollowsfromthedecomposition(C.40)andTheorem4.2.36
APPENDIX D. ADDITIONAL RESULTS AND DISCUSSION
D.1 ASmallSurveyofHeterogeneityAssessment. Weconductasmall-scalesurveyoftreatmenteffect
heterogeneityestimationinappliedeconomics. Wereviewthe45paperspublishedintheAmericanEconomic
ReviewbetweenJanuaryandJuneof2023. Weconsideronlythemaintextofeacharticle.
First, we categorize each paper according to whether it was empirical. Of the empirical papers, we
determine whether any of the figures or tables display estimates of treatment effect heterogeneity. (We
excludeintertemporaleffectheterogeneity,e.g.,event-studies). Wethencategorizeeachofthepapersthat
displayestimatesoftreatmenteffectheterogeneityaccordingtowhethertheirreportisnonparametric,based
oninteractedlinearregression,basedontheinteractionoftreatmentwithbinarycovariates,orinvolvesa
structuralmodel.
We categorize 38 papers as empirical. Of these, 30 report treatment effect heterogeneity. Two papers
reporttreatmenteffectheterogeneitynonparametrically. Ninepapersuseinteractedlinearregression. Seven
papersusestructuralmodeling. Twelvepapersreportcoefficientoninteractionsofbinarycovariates. Many
ofthesepapersdiscretizetheacontinuouscovariateintoabinarycovariate,e.g.,ageintoindicatorsforage
aboveandbelow50.
D.2 Binomial-SampleBootstrap. Recallthatthehalf-samplebootstraprootisgivenby
R∗(x(d))=θˆ (x(d))−θˆ (x(d)) (D.1)
n h n
wherehdenotesarandomelementofS andθˆ (x(d))denotesaversionoftheestimatorθˆ (x(d))evaluated
n,n/2 n n
onthedataD . Ourtheoreticalanalysisofthehalf-samplebootstrapisbasedontheobservationthatifthe
h
estimatorθˆ (x(d))admitsalinearrepresentation
n
n
1 (cid:88)
θˆ (x(d))= u(x(d);D ) (D.2)
n i
n
i=1
forsomefunctionu(·;·),thentherootR∗(x(d))admitstherepresentation
n
n
1 (cid:88)
R∗(x(d))= V (u(x(d),D )−θ (x(d))) (D.3)
n n i i 0
i=1
where V is equal to 1 if i is in h and is equal to −1 otherwise. The weights V ,...,V are exchangeable
i 1 n
Rademacherrandomvariables.
Inthisappendix,wediscussanalternativesubsamplingprocedurethatinducesanalogousweightsthatare
fullyindependent. Thatis,forstatisticsthatadmitthelinearrepresentation(D.2),thesamplingprocedure
considered here is equivalent to the Rademacher bootstrap. We refer to this procedure as the ”Binomial-
Sample”bootstrap. TheBinomial-Samplebootstraprootisgivenby
2Q
R∗(x(d))= n (θˆ(x(d))−θˆ (x(d))), (D.4)
n n s n37
whereQ isanindependentrandomvariablewithaBin(n,1/2)distributionandsdenotesarandomelement
n
of S . That is, s is a random set in [n] of cardinality Q . If the estimator θˆ (x(d)) admits a linear
n,Qn n n
representation(D.2),thentherootR∗(x(d))admitstherepresentation
n
n
1 (cid:88)
R∗(x(d))= V˜(u(x(d),D )−θ (x(d))) (D.5)
n n i i 0
i=1
whereV˜ isequalto1ifiisinsandisequalto−1otherwise. TheweightsV˜ ,...,V˜ arefullyindependent.
i 1 n
AboundexactlyanalogoustoTheorem3.1holdsiftheconfidenceregionformulatedinDefinition2.1is
constructedwiththeBinomial-Samplebootstrap. ThisfollowsimmediatelyfromthefollowingTheorem,
whichgivesresultsanalogoustoTheoremA.2,Part(ii),andLemmaA.2,statedintheproofofTheoremA.1.
Theorem D.1.Suppose that the moment function M(x;θ ,g ) satisfies Assumption A.1 and Part (i) of
0 0
Assumption3.3andthatAssumptionsA.3andA.4hold.
(i)IfthebootstraprootisconstructedwiththeBinomial-Samplebootstrap,thentheinequality
(cid:12) (cid:110)√ (cid:111) (cid:12) φ1/2 (cid:18) log5(dn)(cid:19)1/4 (cid:112)
sup sup (cid:12)P nR∗(x(d))∈R|D −P {Z ∈R}(cid:12)≲ +δ logd (D.6)
R∈RP∈P(cid:12) n n n (cid:12) λ1/2 n n
holdswithprobabilitygreaterthan1−Cn−1/2φλ−1log3/2(dn)−ρ .
n
(ii)Moreover,inthiscase,wehavethat
P (cid:40) sup (cid:12) (cid:12) (cid:12) (cid:12)λˆ λ2 n 2,j −1(cid:12) (cid:12) (cid:12) (cid:12)≥C λφ 22 nlog2(dn)+C n1 δ n2(cid:41) ≲1−C(ρ n+n−1). (D.7)
j∈[d] j
RemarkD.1. FigureD.1displaysupperandlowerconfidenceboundsfortheCATE(1.2)onpost-treatment
assets. TheseboundsarebuiltwiththeconfidenceregionformulatedinDefinition2.1,implementedwiththe
binomial-samplebootstrapgiven. Thequalitativeandquantitativefeaturesofthisfigureareverysimilarto
thefeaturesofFigure2. ■
D.2.1 ProofofTheoremD.1, Part(i). Theresultfollowsfromanargumentverysimilartotheproofof
TheoremA.2,Part(ii). Again,wetakeθ (x)=0forallx,withoutlossofgenerality. Here,weareinterested
0
instudyingthediscrepancy
(cid:18) (cid:19) (cid:18) (cid:19)
2Q 2Q (cid:16) (cid:17)
R∗(x)= n θˆ(x)−θˆ (x)= n θˆ(x)−θ (x) −R (x).
n n s n n s 0 n
Byanalogyto(A.70),wecanwrite
(cid:32) (cid:33)
(cid:18) (cid:19)
2Q n (cid:16) θˆ(x)−θ (x)(cid:17) = 2Q n 1 (cid:88) u(x,D )−U (x) (D.8)
s 0 i s
n n Q
n
i∈s
2 (cid:88) 2Q n
− u(x,D )+ ∆ (x), (D.9)
i s
n n
i∈s38
FIGURE D.1. Binomial-SampleConfidenceRegion
PanelA:UpperBound
5.0
CATE
0.5
4.5 0.4
0.3
4.0
3.5
3.0
−0.6 −0.3 0.0 0.3
Baseline Assets
PanelB:LowerBound
5.0
CATE
0.1
4.5
0.0
−0.1
4.0
3.5
3.0
−0.6 −0.3 0.0 0.3
Baseline Assets
Notes:FigureD.1displaysheatmapsgivingbinomial-sampleupperandlowerconfidenceboundsfortheCATEoftheintervention
studiedinBanerjeeetal.(2015)onpost-treatmenttotalassets.Theconfidenceboundsareconstructedatlevelα=0.1.Theupper
andlowerboundsaredisplayedwithdifferentcolorpalettestoemphasizetheuseofdifferentscales. Acontourlinehasbeen
superimposedoverthelowerboundtodemarcatewheretheboundcrosseszero.TheaxesandestimatorarethesameasinFigure1.
whereU (x)and∆ (x)areconstructedwiththesubsamples. LetQ (t )denotetheeventthat
s s n 0
(cid:12) (cid:12)
(cid:12)2Q n (cid:12)
(cid:12) −1(cid:12)≤t 0 . (D.10)
(cid:12) n (cid:12)
Similarly,letF′(t)andH′ (t)denotetheeventsthat
n n
√
n∥Λ−1/2∆ (x(d))∥ ≤t/4, and (D.11)
s ∞
noitpmusnoC
enilesaB
noitpmusnoC
enilesaB39
n
√ 2 (cid:88)
n∥ uˆ(x(d),D )−Uˆ (x(d))∥ ≤t/4, (D.12)
i s ∞
n
i∈s
respectively,whereUˆ (x(d))isagaindefinedanalogouslytoUˆ (x(d)). DefinetheeventE′(t,t )=F (t)∩
s n n 0 n
F (t)∩H (t)∩H (t)∩Q (t ). Fixahyper-rectangleRinD,andagainrecallthenormalizedandenlarged
h n h n 0
hyper-rectanglesR˜ andR˜ . OntheeventE′(t,t ),wehave
t n 0
(cid:110)√ (cid:111)
|P nR∗(x(d))∈R|D −P {Z ∈R}|
n n
(cid:110)√ (cid:111) (cid:110) (cid:111)
=|P nΛ−1/2R∗(x(d))∈R˜ |D −P Λ−1/2Z ∈R˜ |
n n
(cid:40) n n (cid:41)
2 (cid:88) 1 (cid:88) (cid:110) (cid:111)
≤|P √ uˆ(x(d),D )− √ uˆ(x(d),D )∈R˜ |D −P Λ1/2Z ∈R˜ |
n i n i t(1+t0) n t(1+t0)
i∈s i=1
(cid:110) (cid:111) (cid:110) (cid:111)
+|P ΛZ ∈R˜ −P Λ1/2Z ∈R˜ |
t(1+t0)
foreacht>0. LetV˜ bearandomvariabletakingthevalue1withiisanelementofthesubsetsandtaking
i
thevalue−1otherwise. Observethat
n n
2 (cid:88) 1 (cid:88) 1 (cid:88)
uˆ(x(d),D )− uˆ(x(d),D )= V˜uˆ(x(d),D ). (D.13)
i i i i
n n n
i∈s i=1 i=1
andthattheweightsV˜ areindependentandidenticallydistributedRademacherrandomvariables. Thus,on
i
theeventE′(t,t ),wehave
n 0
(cid:110)√ (cid:111)
|P nR∗(x(d))∈R|D −P {Z ∈R}|
n n
(cid:18) φ2log5(dn)(cid:19)1/4
(cid:112)
≲ +t(1+t ) log(d) (D.14)
λ2n 0
withprobabilitygreaterthan1−Cn−1/2λ−1φlog3/2(dn)byLemmasA.1andA.4. Hence,itsufficetogive
ahighprobabilityboundonE′(t,t )forsuitablechoicesoftandt .
n 0 0
Tothisend,observethatamultiplicativeChernoffboundimpliesthat
(cid:12) (cid:12) (cid:114)
(cid:12) (cid:12)2Q n −1(cid:12) (cid:12)≲ log(n) (D.15)
(cid:12) n (cid:12) n
withprobabilitygreaterthan1−n−1. Thus,asthedataD aredrawnindependentlyandidenticallywith
s
distributionP inPandwehaveassumedthatδ ≲ δ andρ ≲ ρ foranyfixed0 < ε < 1, bysetting
εn n εn n
t=δ andt =(log(n)/n)1/2,AssumptionA.3andtheboundEquation(A.51)implythattheeventE′(t,t )
n 0 n 0
occurswithprobabilitygreaterthan1−C(ρ +n−1),asrequired.
n
D.2.2 ProofofTheoremD.1,Part(ii). Again,wetakeθ (x)=0forallx,withoutlossofgenerality. Recall
0
fromtheproofofTheoremA.2,Part(iii),thatV˜ isarandomvariabletakingthevalue1wheniisanelement
i40
ofthesubsets,andtakingthevalue−1otherwise,andthat
n n
2 (cid:88) 1 (cid:88) 1 (cid:88)
u(x(d),D )− u(x(d),D )= V˜u(x(d),D ). (D.16)
i i i i
n n n
i∈s i=1 i=1
Definetheobject
1 (cid:88)
T (x)= u (x,D )−U (x).
s n i s
Q
n
i∈h
Weareinterestedinstudying
(cid:20) (cid:21)
(cid:16) (cid:17)2
λˆ2 =nE R∗(x(j))
n,j V˜ n
 
(cid:32) n (cid:33)2
=nE V˜  n1 (cid:88) V˜ iu(x(j),D i)+ 2Q nn T s(x(j))−T n(x(j))+ 2Q nn ∆ s(x(j))−∆ n(x(j))  ,
i=1
wherethenotationE [·]denotesthattheexpectationisevaluatedonlyovertherandomvariablesV˜ ,...,V˜ .
V˜ 1 n
OntheeventE′(t ,t),definedintheproofofTheoremA.2,Part(iii),wehavethat
n 0
 
(cid:32) n (cid:33)2
sup (cid:12) (cid:12)λˆ2 n,j −λ2 n,j(cid:12) (cid:12)≤(t(1+t 0))2 , where λ2 n,j =nE V  n1 (cid:88) V˜ iu(x(j),D i)  .
j∈[d]
i=1
Wecanevaluate
n
λ2 = 1 (cid:88) u2(x(j),D ).
n,j n i
i=1
astheweightsV˜ aremutuallyindependent.
i
Now,observethat
n
(cid:12)1 (cid:88) (cid:12) φ
sup (cid:12) u2(x(j),D )−λ2(cid:12)≲ log(dn)
(cid:12)n i j(cid:12) n
j∈[d]
i=1
withprobabilitygreaterthan1−n−1 byBernstein’sinequality. Thus,ontheeventE′(t ,t),wefindthat
n 0
sup (cid:12) (cid:12)λˆ2
n,j
−λ2 n,j(cid:12) (cid:12)≲ φ
n
log(dn)+(t(1+t 0))2 ,
j∈[d]
withprobabilitygreaterthan1−n−1. Bysetting
(cid:114)
λ2
t=C δ ,
n
n
and t = (log(n)/n)1/2, Assumption A.3 and the bound (A.51) imply that the event E′(t,t ) occurs with
0 n 0
probabilitygreaterthan1−Cρ −n−1. Consequently,wefindthat
n
sup (cid:12) (cid:12)λˆ2 −λ2 (cid:12) (cid:12)≲ φ log(dn)+ λ2 δ2 ,
n,j n,j n n n
j∈[d]
withprobabilitygreaterthan1−C(ρ +n−1),asrequired. ■.
n41
APPENDIX E. DETAILS CONCERNING DATA AND SIMULATIONS
Inthisappendix,wedocumentourtreatmentoftheBanerjeeetal.(2015)data. AppendixE.1detailsour
acquisitionandcleaningofthesedata. InAppendixE.2,wegivefurtherdetailsconcerningtheconstruction
ofFigure1andFigure2. AppendixE.3discussesoursimulationcalibration.
E.1 Data. ThedatafromBanerjeeetal.(2015)wereacquiredfromhttps://dataverse.harvard.
edu/dataset.xhtml?persistentId=doi:10.7910/DVN/NHIXNTonSeptember10,2021. The
datafromthegraduationprogramimplementedinPakistanareconsideredinChenandRitzwoller(2023)and
RitzwollerandRomano(2023). Here,weconsiderthedatafromthegraduationprogramimplementedin
Ghana,asithasalargersamplesize.
Thedatarecordmeasurementsofmanypre-treatmentandpost-treatmentoutcomesfor2,606individualsin
thenorthernregionofGhana. Baselinesurveymeasurementsweremadepriortotheallocationoftreatment. A
multifacetedtreatment,composedofproductiveassets,consumptionsupport,healthandnutritionaleducation,
andsavingsrequirementswasrandomlyallocatedto632oftheindividuals. Banerjeeetal.(2015)consider
datafromtwoendlinesurveys,madetwoyearsandthreeyearsaftertheinitialassettransfer,respectively. For
thepurposeofthispaper,weconsideronlydatafromthebaselinesurvey,recordsofthetreatmentallocation,
andmeasurementsfromthefirstendlinesurvey. Weomitdatafrom164attritedindividuals,noneofwhom
wereassignedtothetreatment.
The covariate vector Z is composed of measurements of 16 pre-treatment outcomes. Five of these
i
outcomesareassociatedwithconsumption: totalmonthlyconsumptionandtotalmonthlyconsumptionon
food,non-food,anddurablecommodities. Eachconsumptionvariableismeasuredin2014USdollars. We
transform total monthly consumption to logs, base 10. Three of the variables are associated with assets,
eachofwhichisanindexconstructedfromsurveydatameasuringtotalassets,totalproductiveassets,and
totalhouseholdassets. Wetransformthetotalassetsmeasurementtologs,base10. Fiveoftheoutcomes
areassociatedwithfoodsecurity. Theseconsistoffourbinaryvariablesindicatingdifferentaspectsoffood
security,e.g. didachildskipameal,inadditiontoanindexaggregatingthesemeasurements.20 Thefinalfour
variablesareassociatedwithfinanceandincome: thetotalamountofoutstandingloans,thetotalamountof
savings,incomefromagriculture,andtotalincomefrombusiness. ThecovariatevectorX collectsthetotal
i
monthlyconsumptionandassetsforeachindividual. TheoutcomeY measuresthetotalassetstwoyears
i
aftertheinitialassettransfer. Again,wetransformthesemeasurementstologs,base10.
E.2 ParameterChoices. InFigure1andFigure2,wesetthesubsampleproportionb/nequalto0.05. We
user = 200bootstrapreplicatesthroughout. Weuse20,000treestoconstructFigure1and2,000treesin
eachbootstrapreplicatetoconstructFigure2andthroughoutthesimulation.
20Thereare2individualswithmissingvaluesforthefoodsecurityindex.Weimputethesevalueswiththemedianvaluesofthefood
securityindex.42
E.3 SimulationCalibration. WecalibrateasimulationtotheBanerjeeetal.(2015)datausingacollection
ofGeneralizedAdversarialNetworks(GAN)(Goodfellowetal.,2014). Thisapproachtosimulationdesign
wasproposedbyAtheyetal.(2021). Roughlyspeaking,aGANisapairofneuralnetworks. Theobjective
of the first network, the generator, is to generate data that looks like the Banerjee et al. (2015) data. The
objectiveofthesecondnetwork,thediscriminator,istodiscriminatebetweentherealBanerjeeetal.(2015)
dataandthedatageneratedbythegenerator. Thesenetworkscompeteiterativelyuntilconvergence. Theidea
isthat,afterconvergence,thegeneratorisagoodproxyforthetrueprocessthatgeneratedtheBanerjeeetal.
(2015)data. Weusethe“WGAN”packageassociatedwithAtheyetal.(2021).
To calibrate our simulation, we estimate three GANs. The first GAN estimates the distribution of the
covariatesX ,i.e.,baselineconsumptionandbaselinetotalassets. ThesecondGANestimatesthedistribution
i
ofZ conditionalonX . RecallthatZ collectsallbaselinecovariates,otherthanthecovariatesinX . The
i i i i
thirdGANestimatesthedistributionofY conditionedonZ ,X ,andW . TogenerateanobservationD ,we
i i i i i
generateX ,generateZ conditionedonX ,andgeneratethepotentialoutcomesY (1)andY (0)fromthe
i i i i i
estimateddistributionsofY conditionedonZ , X , andW = 1andW = 0, respectively. Thetreatment
i i i i i
indicator is W is drawn i.i.d., Bernoulli with the observed probability in the Banerjee et al. (2015) data
i
andwesetY = Y (W ). Inthisway, weknowthetruetreatmenteffectY (1)−Y (0)foreachunitinour
i i i i i
simulation. Wedraw10millionobservationsD withthisprocess. Inthesimulation,datasetsofvarioussizes
i
aresampledfromthiscollection.
WeusearelatedproceduretodeterminethetrueCATEθ (x)ateachvaluexinthequery-vectorx(d) (i.e.,
0
thecentersofeachoftherectanglesdisplayedinFigure1). Specifically,foreachxinx(d),wedraw100,000
observationsfromthedistributionofZ conditionedonX =x. WethendrawobservationsY (1)andY (0)
i i i i
foreachofthesereplicates,andcomputetheaverageofthetruetreatmenteffectsY (1)−Y (0). FigureE.2
i i
displaysthesepseudo-truevaluesoftheCATEθ (x),alongsideareproductionoftheestimatesofthetrue
0
CATEconstructedwithGRF.Oursimulationdesigncapturesmanyofthesamefeaturesofthedatarecovered
byGRF,butgivesasomewhatsmootherpictureoftheCATE.
FigureE.3displaysascatterplotcomparingthemomentsofthedatafromtheBanerjeeetal.(2015)datato
thedatageneratedbyourcalibratedsimulation. Thedistributionsmatchquiteclosely. FigureE.4comparesa
scatterplotoftheobservedvaluesofbaselineconsumptionandbaselineassetsintheBanerjeeetal.(2015)
datawithaheat-mapofthedistributionofthesecovariatesinoursimulation. Thelimitsofthehorizontaland
verticalaxesinthisFigurematchFigures1and2displayedinthemaintext. Someobservationsfalloutside
ofthelimitsofthisfigure. Thequartilesofbaselinelogconsumptionare3.33,3.76,and4.20. Thequartiles
ofbaselineassetsare-0.45,-0.71,and0.03.43
FIGURE E.2. ComparisonofEstimatedandCalibratedCATEs
PanelA:CATEEstimates
5.0
CATE
0.3
4.5
0.2
0.1
4.0
3.5
3.0
−0.6 −0.3 0.0 0.3
Baseline Assets
PanelB:CalibratedCATEvalues
5.0
CATE
0.25
4.5 0.20
0.15
0.10
0.05
4.0
3.5
3.0
−0.6 −0.3 0.0 0.3
Baseline Assets
Notes:FigureE.2displaysheat-mapscomparingGRFestimatesoftheCATEoftheprogramconsideredinBanerjeeetal.(2015)
tothe“true”valueofCATEusedinourcalibratedsimulation.PanelArecreatesFigure2.PanelBisconstructedanalogously.
noitpmusnoC
enilesaB
noitpmusnoC
enilesaB44
FIGURE E.3. Validation
X Y
1e+04
1e+02
1e+00
1e−02
1e−02 1e+00 1e+02 1e+04 1e−02 1e+00 1e+02 1e+04
X Y
1e+07
1e+03
1e−01
1e−01 1e+03 1e+07 1e−01 1e+03 1e+07
X Y X and Y
1.0
0.5
0.0
−0.5
−1.0
−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0
True Data
Unconditional Treated Untreated
Notes:FigureE.3displaysscatterplotscomparingthemomentsofthedatafromBanerjeeetal.(2015)totheGANgenerated
simulationdata.Columnsdifferentiatebetweendifferenttypesofvariables.Rowsdifferentiatebetweendifferenttypesofmoments.
Thex-axisofeachsub-panelmeasuresthemomentsofthetruedata. They-axisofeachsub-panelmeasuresthemomentsof
thegenerateddata. Thexandyaxesinthefirsttworowsaredisplayedinlog-scale. Aforty-fivedegreelineisdisplayedinall
sub-panels.Blueandgreendotsdenotemomentsconditionedontreatmentbeingsettooneandzero,respectively.Blackdotsdenote
unconditionedmoments.
ataD
detareneG
ataD
detareneG
ataD
detareneG
Mean
Variance
Correlation45
FIGURE E.4. CovariateDensity
PanelA:ObservedCovariates
5.0
4.5
4.0
3.5
3.0
−0.6 −0.3 0.0 0.3
Baseline Assets
PanelB:SimulationCovariateDensity
5.0
Density
0.0125
0.0100
4.5
0.0075
0.0050
0.0025
4.0
3.5
3.0
−0.6 −0.3 0.0 0.3
Baseline Assets
Notes:PanelAofFigureE.4displaysascatterplotoftheobservedvaluesofbaselineconsumptionandbaselineassetsinthe
Banerjeeetal.(2015)data.Thehorizontalandverticalaxesdisplaythebaselinemonthlyconsumption,normalizedto2014dollars
onalogarithmicscalebase10,andanindexforbaselineassets,respectively.PanelBdisplaysaheat-mapgivingthedensityofthe
jointdistributionofbaselineconsumptionandbaselineassetsassociatedwithourcalibratedsimulation.
noitpmusnoC
enilesaB
noitpmusnoC
enilesaB