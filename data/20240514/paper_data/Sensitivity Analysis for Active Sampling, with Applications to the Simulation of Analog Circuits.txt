Sensitivity Analysis for Active Sampling, with
Applications to the Simulation of Analog Circuits
CHHAIBI Reda GAMBOA Fabrice OGER Christophe
Universite´ Toulouse III - Paul Sabatier Universite´ Toulouse III - Paul Sabatier NXP Semiconductors
Toulouse, France Toulouse, France Toulouse, France
reda.chhaibi@math.univ-toulouse.fr fabrice.gamboa@math.univ-toulouse.fr christophe.oger@nxp.com
ORCID: 0000-0002-0085-0086 ORCID: 0000-0001-9779-4393
OLIVEIRA Vinicius PELLEGRINI Cle´ment REMOT Damien
NXP Semiconductors Universite´ Toulouse III - Paul Sabatier Universite´ Toulouse III - Paul Sabatier
Toulouse, France Toulouse, France Toulouse, France
vinicius.alvesdeoliveira@nxp.com clement.pellegrini@math.univ-toulouse.fr damien.remot@math.univ-toulouse.fr
ORCID: 0000-0003-2122-7733 ORCID: 0000-0001-8072-4284
Authors are in alphabetical order.
CONTENTS I. Introduction
A. Context and literature
I Introduction 1
I-A Context and literature . . . . . . . . . 1 Context: Performances of integrated circuit are knowingly
I-B Description of Sampling Flow . . . . 2 sensitive to a plurality of effects linked to the fabrication
process or electrical stress via their impact on each device’s
II Datasets 3 characteristics (process and mismatch variations, temperature,
II-A The classical Sobol’ G-function . . . 3 voltage, aging, etc). As such, today’s analog design flows are
II-B Simulation of analog circuits . . . . 3 proposingvarioustoolstosimulatetheimpactoftheseeffects.
This has conducted the design flow to usually focus first on
III Features selection 4 the impact of statistical variations of devices on the circuit
III-A Chatterjee’s estimator of Crame`r- performances using dedicated tools, and then once the circuit
von-Mises indices . . . . . . . . . . . . 4 is tuned with respect to these, a next step can be to evaluate
III-B Conjecture for detection of noisy fea- aging with other tools to verify that the impact of degraded
tures . . . . . . . . . . . . . . . . . . . 4 devices stays within the remaining performance margins left
after statistical simulation. If this sequential approach may
IV Experiments 5 be fine for circuits designed with mature nodes where ag-
ing’s impact on devices is normally orders of magnitude less
V Conclusion 6 compared to statistical variations, this becomes impractical
in advanced nodes where it not rare to see the impact of
References 6
aging more comparable to the one of statistical variations, as
Abstract—We propose an active sampling flow, with the use- for instance mentioned in [1]. In such cases, the only way
case of simulating the impact of combined variations on analog would be to simulate the combined effect of these different
circuits.Insuchacontext,giventhelargenumberofparameters, causes, which leads to an explosion in number of simulations
itisdifficulttofitasurrogatemodelandtoefficientlyexplorethe
to run if for example at each statistical variations sample we
spaceofdesignfeatures.Bycombiningadrasticdimensionreduc-
executetheoftenmulti-stepstressconditionsexecutiontoadd
tion using sensitivity analysis and Bayesian surrogate modeling,
we obtain a flexible active sampling flow. On synthetic and real agingdegradation.Methodshelpingtoreducethetotalnumber
datasets, this flow outperforms the usual Monte-Carlo sampling of simulations in such multi-variation domains scenarios are
which often forms the foundation of design space exploration. therefore highly desired to limit the exploding CPU cost they
IndexTerms—SensitivityAnalysis,UncertaintyQuantification,
imply.
Cramer-Von-Mises index, Surrogate models, Active Sampling,
Indeed, Monte Carlo (MC) based analysis is necessary to
Simulation of Analog circuits.
provide a robust estimation assessment of the circuit’s design
performanceduetofabricationprocessvariations.MCconsists
TheauthorsR.C.,F.G.andC.PacknowledgethesupportoftheANR-3IA
ANITI(ArtificialandNaturalIntelligenceToulouseInstitute). in the standard approach for routine design validation [2]. On
1
4202
yaM
31
]LM.tats[
1v17970.5042:viXrathe other hand, MC based analysis requires a large number method will be the tools of dimensionality reduction on the
of computationally expensive circuit simulations, making it one hand, and surrogate modeling on the other hand.
unfeasible for modern and more complex circuit technolo-
gies. To overcome this limitation, Importance sampling (IS) B. Description of Sampling Flow
methods [2], [3] can be used instead to draw samples from
artificially modified distributions shifted to the targeted oper- Within the context of integrated circuit design, the effects
ating zones of interest, normally towards fail regions. Thus, mentioned in Introduction are modelled by a random vector
the convergence of the estimated robustness assessment can X = (cid:0) X(1),...,X(D)(cid:1) , where each X(i), i = 1,...,D
be accelerated since the ”rare” events are prone to be drawn is a real bounded random variable valued in B(i). These
aroundthesetargetedregionsofinterest.Thedownsideisthat random variables are also called explanatory variables and D
thesemethodsnormallyfavorlimitedspecificzonesofinterest, issupposedtobeverylarge.Theperformanceofanintegrated
therefore disfavoring the exploration and discovery of other circuit is described by a random variable Y = F(X), where
important operating zones that end up being ignored [4]. thefunctionF isnoteasilyaccessibleinpractice.Thefunction
F describes namely an expensive simulation and it is referred
Surrogate Modeling: Still to avoid the large number of to as the ”black-box” or the ”computer code”. Here, we shall
expensivecircuitsimulations,data-drivensurrogatemodels[4] suppose that F : B → R. Also, it will be convenient to
can be constructed and used to approximate the circuit’s re- designate, for any subset I ⊂ 1,D , the associated product
sponse.Thankstothat,thecircuit’sdesignperformancecanbe space as (cid:74) (cid:75)
assessed in a faster manner. However, the main disadvantage
(cid:89)
is that these models require sufficient training samples, i.e., B(I):= B(i) . (1)
samples that are representative of the circuit performance
i∈I
dispersion. Moreover, the data-driven surrogate models suffer
from the “curse of dimensionality” [4], [5]. More specifically, Naturally X ∈B :=B( 1,D )=(cid:81)D B(i).
i=1
(cid:74) (cid:75)
the high dimensionality (which is quite common in modern
analog circuits) makes it challenging to train the surrogate
models. BEGIN
Fortunately,somepreviousresearch[6]indicatesthatnotall
variationsininputparametersareequallyimportant.Inreality,
Buildaninitialbatchofdata,
any given circuit performance measure is normally dominated asEquation(2),realisations
offeaturesandcorresponding
by a group of critical devices, whereas the other devices have
performancesatstake.
very little to no influence, especially in the case where the
circuithasasymmetricdesignintendedtoalleviatetheprocess
UseFeatureSelection
variation [4].
dfeaturesselectedas Algorithm2,toselect
themostinfluential thedfeaturesestimated
Active Learning: This knowledge allows to consider “di- asthemostrelevant
mension reduction” capable to reduce the input dimension
Overtherealisations
such that only the key and relevant parameters are isolated.
forthesesdfeatures, D−dfeaturesselected
One possibility is to combine the efficiency of faster evalua- callAlgorithm1,to asthelessinfluential
obtainanewrealisation
tions of surrogate models with active learning to sequentially
ofthesedfeatures.
reduce the prediction error [5]. [7] adopts a Gaussian process
Drawarealisationofthese
(GP) to approximate the underlying circuit performance func- D−dfeatures,withthe
usualdistributionlaw.
tion and an entropy reduction criteria for active learning. One
limitation is that entropy-based criterion is known for being
computationally expensive. [4] adds a nonlinear-correlated Considerthenewrealisation
overallthefeaturesandcall
deep kernel method with feature selection to isolate the key theexpensivesimulatoron
determinant features to focus on and to consider in the surro- ittogetthecorresponding
performance.Then,addthenew
gate modeling approach that also uses a GP. These previously sampletothepreviousones
proposed approaches have been assessed to tackle up to 1k
input dimensions, which can be considered a ”small” analog
Dowereachthe
circuit. A robust feature selection method is thus necessary finalbudgetNf? No
to allow the use of surrogate modeling approaches in this
application.
Yes
Goalsandcontributions:Havingatourdisposalananalog
circuit simulator, we aim for an efficient exploration of the
END
parameter space in order to limit the number of samples to
effectively simulate. To that end, the corner stones of our Fig.1. Diagramofouractivesamplingflow.
2Let us now describe our active sampling flow, which is In order to obtain a sample x ∈ B, it only re-
N+1
illustrated in Figure 1. We assume that we have a first batch mains to sample the remaining (non-relevant) coordinates
of N ∈N∗ realizations z ∈ B( 1,D \I ). Then the new sample is obtained by
0 N+1 d
concatenation(cid:74) x (cid:75) := (x ,z ) ∈ B. The associated
S :={(x ,y )∈B×R, 1≤j ≤N } , (2) N+1 (cid:101)N+1 N+1
N0 j j 0 output is obtained by invoking the black-box function F to
compute y = F (x ) and finally obtain S :=
of the random variables (X,Y). The goal is to successively N+1 N+1 N+1
choosenewsamples,untilwereachedafinalbudgetN ∈N∗, S N ∪{(x N+1,y N+1)} .
f
with the best characterization of circuit performance.
II. Datasets
Surrogate modeling: Assuming we have chosen a set of The datasets we use to showcase our active sampling flow
samplesS N forN 0 ≤N <N f,weconstructsurrogatemodel are a standard Sobol G-function and simulations from actual
of F, denoted F(cid:98)N based on the observations S N. Then this analogcircuits.TableIsummarizesthecontentofthisSection.
surrogatemodelF(cid:98)N,isusedinordertochoosethenextsample
point in B for which the prediction through F(cid:98)N is the most
A. The classical Sobol’ G-function
uncertain.
Inthecommunityofsensitivityanalysis,itiscommontouse
Among the multiple possibilities of surrogate models, we
thefollowingSobol’G-functionforsyntheticbenchmarks[9].
haveoptedforGaussianProcesses(GPs)[8],fortheirflexibil-
Givend≤Danda∈(R\{−1})d,suchfunctionF =G(d)
ity and because of their natural Bayesian framework. Indeed, Sobol′
is defined as
given a GP F(cid:98)N, that has been trained to interpolate between
the values of S N, we have access to both predictions on the G(d) : [0,1]D →R
entire space Sobol′ x(cid:55)→(cid:81)d |4x(i)−2|+ai ,
(cid:104) (cid:105) i=1 1+ai
m (cid:98)N :x(cid:55)→E F(cid:98)N(x) , (3) which will be only considered for a=0 Rd, meaning that
and well as the variance d
Y =G(d) (X):=(cid:89) |4X(i)−2| . (6)
(cid:104) (cid:105) Sobol′
v
(cid:98)N
:x(cid:55)→Var F(cid:98)N(x) . (4)
i=1
The first d variables, (X(1),...,X(d)), will be called the
Dimensionality reduction: It is well known that GPs do significantfeatures,andtheD−dothers,(X(d+1),...,X(D)),
not scale with the dimension. Therefore, it is absolutely nec- will be called the noisy features. We add these fictitious
essary to identify d ≪ D relevant indices I d = (i 1,...i d) ⊂ attributes to the dataset given by the Sobol’ G function, in
1,D which better represent the variations at hand. Accord- order to artificially reproduce the same dimensionality found
(cid:74)ingly(cid:75)thetrainedGaussianProcessRegressorF(cid:98)N istakenasa in the non-synthetic datasets (See Table I).
(random) map defined over B(I ). Likewise for m and v .
d (cid:98)N (cid:98)N
The statistical procedure for finding I is the Feature
d B. Simulation of analog circuits
Selection Algorithm 2, which is detailed in Section III along
with the necessary statistical theory. The analog circuits at hand are a High-Speed Operational
TransconductanceAmplifier(HSOTA)andaFastInternalRef-
Optimizedsampling: Thuswecanchoosethenextsample erenceClock(FIRC)generatorbothdesignedin16nmFinFET
x (cid:101)N+1 ∈ B(I d) as the one maximizing the variance, meaning technology and composed each of few thousands of devices.
that See for example [10], [11] for general information about
(cid:16) (cid:17) HSOTAamplifiersandclockcircuits.Wehaveatourdisposal
x = x(1) ,...,x(d) =argmax v (x) , (5)
(cid:101)N+1 (cid:101)N+1 (cid:101)N+1 (cid:98)N simulationsperformedthankstoSPICE-likeanalogsimulators,
x∈B(Id)
whichareindustrystandards.ForexampleCadence’sSpectre®
which we depict in Algorithm 1. Circuit Simulator [12]. Furthermore
• for the HSOTA device, the outputs are two slew rate
Algorithm 1: Optimized Sampling
measures (up and down) and an offset voltage, which
Require: S N : the training dataset of size N; all depend on D =450 explanatory variables ;
I d =(i 1,...,i d) : labels of relevant features; • for the FIRC device, the outputs are measures of the
m : the number of candidate points ; generated clock frequency and its duty cycle, which all
F(cid:98)N ← Gaussian Process Regressor trained on S N ; depend on D =44 959 explanatory variables.
S ← Sample m i.i.d. points in B(I ) according to the law
d Notice that simulations take into account circuit aging, which
of (cid:0) X(i),i∈I (cid:1) ;
d depends on variables describing usage scenarios. Here, we do
x ← Point in S maximizing variance (Eq. (5)) ;
(cid:101)N+1 (cid:16) (cid:17) notincludesuchagingvariables,andbyexplanatoryvariables,
return x (cid:101)N+1 = x (cid:101)( N1 +) 1,...,x (cid:101)( Nd +) 1 we only mean process and mismatch variations, temperature
and supply voltages in the design of circuits.
3Dataset Samplesize Outputs/dimY Attributes FictitiousAttributes TotalAttributes(D)
G(d=4) 30000 1 4 446 450
Sobol′
HSOTA 30000 3 450 0 450
G(d=10) 30000 1 10 9990 10000
Sobol′
FIRC 9982 2 44959 0 44959
TABLEI
CHARACTERISTICSOFDATASETS.THESOBOL’GFUNCTIONHASEXTRANOISY(FICTITIOUS)ATTRIBUTESADDED,INORDERTOBECOMECOMPARABLE
TOTHEREALDATASETSFIRCANDHSOTA.
Of course, there is no intrinsic dimension d in these cases. Algorithm 2: Feature Selection
Andnaturally,thechosendimensionsdependonwhichoutput Require: S : the training dataset of size N ;
N
is considered. In practice, dimension reduction to the values d∈N∗ : the number of features to select ;
d HSOTA ∈{10,13} and d FIRC ∈{11,15} is satisfactory. for i∈ 1,D do
III. Features selection
Comp(cid:74)
ute
ξ(cid:98)(cid:75)(i)
thanks to Equation (7);
N
end for
A. Chatterjee’s estimator of Crame`r-von-Mises indices (cid:16) (cid:17)
Our method for dimensionality reduction relies on the
Take the d features (i 1,...,i d) maximizing ξ(cid:98) N(i)
1≤i≤D
computation of sensitivity indices, which are powerful tools return (i 1,...,i d), most relevant d feature labels
from the field of Global Sensitivity Analysis, as described in
[13]. Thanks to such indices, one can assess, for 1≤i≤D,
how sensitive is Y to a variable X(i). Throughout the paper, B. Conjecture for detection of noisy features
wewillonlyconsiderthefirstorderCrame`r-von-Misesindices
For statistically testing whether a fixed feature is relevant,
(CvM indices) which are given by
we have at our disposal the following Central Limit Theorem
ξ(cid:16) X(i),Y(cid:17)
:=
(cid:82) t∈R (cid:82)Var V(cid:2)E ar(cid:2) [1 PY [≤ Yt ≤| X t]](i P)(cid:3)(cid:3) (P dY t)(dt)
.
for one estimator ξ(cid:98) N(i).
t∈R Y Theorem III.1 (2.2 from [14]). For all 1≤i≤D, under the
When there is no ambiguity, we will note ξ(i) :=ξ(cid:0) X(i),Y(cid:1) hypothesis
and we have that ξ(i) ∈[0,1].
The remarkable property of the CvM index ξ(i) is that it H 0(i) :“X(i) is a noisy feature′′ ,
vanishes if only if Y and X(i) are independent. And it is
equal to 1 if and only if Y is a function of X(i). As such,
thecorrespondingChatterjeeestimator,ξ(cid:98) N(i),fluctuatesaccord-
ing the following Central Limit Theorem (CLT)
the CvM index ξ(i) quantifies the dependence, allowing us to
decide whether or not the variable X(i) is relevant. √ (cid:18) 2(cid:19)
We will estimate these CvM indices empirically using
Nξ(cid:98) N(i) →N 0,
5
.
Chatterjee’s method based on ranks. The estimator ξ(cid:98)(i0) is
N
However,forafeatureselectionwithahighnumberofnoisy
definedasfollows.Givenafeature1≤i ≤D,letusassume
0
that we have at our disposal N ∈ N∗ i.i.d. realisations of variables, we need a joint test, in the sense that we need to
(cid:0) X(i0),Y(cid:1) test the following hypothesis
S N(i0) =(cid:110)(cid:16) x( ji0),y j(cid:17) ∈B(i0)×R, 1≤j ≤N(cid:111) . H 0 =H 0(i1,...,ik) :“X(i1),...,X(ik) are the noisy features” ,
Let us sort the sample as (cid:16) x(i0) ,y (cid:17) such that where (i 1,...,i k)∈P k( 1,D ). In other words,
N,(j) N,(j) (cid:74) (cid:75)
1≤j≤N
H =H(i1,...,ik) =H(i1)∩···∩H(ik) .
x(i0) ≤···≤x(i0) . 0 0 0 0
N,(1) N,(N)
Thus, under such H , it is natural to track the variable
Then we define this rank-based estimator for ξ(i0) by 0
ξ(cid:98)(i0) :=1−
3(cid:80)N j=− 11|r j+1−r j|
, (7)
ξ(cid:98)max =ξ(cid:98)max(N,(i 1,...i k)):= i∈(m i1a ,..x .ik)ξ(cid:98) N(i) ,
N N2−1
given a sample size N ∈ N. Notice that maxima of random
where the ranks r ’s are given by
j variablesarenotalwaystractable.Thecasethatiswellstudied
N is that of the maxima of independent variables with Gaussian
(cid:88)
∀1≤j ≤N, r j := 1 {yN,(k)≤yN,(j)} . tails. This is the topic of classical books such as [15]. In
k=1 fact, under H 0, we expect the ξ(cid:98) N(i) ’s to decorrelate from the
The almost sure convergence lim N→∞ξ(cid:98) N(i0) = ξ(i0) is given samples of Y, so that ξ(cid:98)max truly behaves like a maximum
by [14, Theorem 1.1]. of i.i.d. random variables. Because Gaussian tails are a given
Thus, because of the simplicity of this estimator, it can be thanks to the concentration inequalities in the proof of [14,
incorporated into a feature selection method as Algorithm 2. Theorem 1.1], theorems describing extremal statistics such as
4Fig. 2. Given Nmax = 10000, k = 100 and P = 500, we generate Fig. 3. Given N = 10 000, kmax and P = 500, we gener-
Nmax × P i.i.d. realisations of mutually independent random variables ate N × P i.i.d. realisations of mutually independent random variables
(cid:0) X(1),...,X(k),Y(cid:1) ,sothatwehaveP batchesofNmaxsamples.Oneach (cid:0) X(1),...,X(kmax),Y(cid:1) ,sothatwehaveP batchesofN samples.Oneach
ofthesebatches,weusetheNmaxsamplestocomputeξ(cid:98)max(N,(1,...,k)) ofthesebatches,weusetheN samplestocomputeξ(cid:98)max(N,(1,...,k))for
for N ≤ Nmax. Thus, we obtain P independent realisations of γ(N,k) k≤kmax.Thus,weobtainP independentrealisationsofγ(N,k)depending
dependingonN <Nmax,whatweplotinblue. onk<kmax,whatweplotinblue.
[15, Theorem 1.5.3] should still hold. Hence, we make the
following conjecture
Conjecture III.2. Under H , we have:
0
(cid:114)
4log(k)
ξ(cid:98)max(N,(i 1,...,i k))=
5N
×γ(N,k) , (8)
where the family
(cid:0)
γ(N,k) ;
(N,k)∈(N∗)2(cid:1)
is tight, mean-
ing uniformly bounded with high probability .
In order to verify empirically such a Conjecture, we start
by dealing with tightness of γ by generating a huge number
of realisations of ξ(cid:98)max, which can be observed on Figures 2
and 3.
Wealsomakesomeadditionalnumericalexperimentstotest Fig.4. Weperforman(OLS)linearregressiontoestimatetheslope−α1 in
therelevanceofEquation(8).Indeed,takingthelogarithm,we Equation(9).Weobtainaconfidenceintervalforα1of[0.498, 0.501]with
risklevelat95%.
obtain
logξ(cid:98)max(N,(i 1,...,i k)) (9)
5N
=−α log +β log(2log(k))+logγ(N,k) ,
1 2 1
where
1
α =β = .
1 1 2
As logγ(N,k) is bounded with high probability, it can be
treatedasaresidue.Andthevaluesofα andβ canbetested
1 1
thankstoaclassicalOrdinaryLeastSquares(OLS)regression.
The results in Figures 4 and 5 tend to confirm the conjecture
as well.
IV. Experiments
Variations of the flow: In order to assess the quality of Fig. 5. We perform an (OLS) linear regression to estimate the slope β1 in
our active sampling flow as described in Fig. 1, we consider Equation (9). We obtain a confidence interval of [0.497 ; 0.504] with risk
levelat95%.
different variations, in the feature selection and the sample
5Featureselection Choiceofnextsample
Method1 GSA(Algorithm2) Maximalvariance(Algorithm1) Evolution of R2(N) for different sampling strategies
Method2 GSA(Algorithm2) RandomChoice
Method3 OracleSelection Maximalvariance(Algorithm1) 1.0
Method4 OracleSelection RandomChoice
Method5 RandomSelection Maximalvariance(Algorithm1)
Method6 RandomSelection RandomChoice
TABLEII 0.5
VARIATIONSOFTHEACTIVESAMPLINGFLOWOFFIG.1DURINGOUR
EXPERIMENTS.
0.0
choice. The different methods are listed in Table II and are
motivated as follows.
0.5
In the spirit of ablation studies, the choice of next sample
Features selection : oracle ; Sample choice : max_variance
can be done using the maximal variance criterion (Algorithm Features selection : oracle ; Sample choice : random
Features selection : chatterjee ; Sample choice : max_variance
1) or using a random choice. Regarding the feature selection, 1.0 F Fe ea at tu ur re es s s se el le ec ct ti io on n : : c rah na dtt oe mrj e e ; ; S Sa am mp pl le e c ch ho oi ic ce e : : r ma an xd _o vm a r i a n ce
more variations are possible. A priori, the worst choice is a Features selection : random ; Sample choice : random
random selection. Then there is our feature selection based 0 100 200 300 400 500
Sample size N
on sensitivity analysis (Algorithm 2). And finally there is the
ideal case where we know of most relevant features thanks to Fig.6. WecomputethemeanR2(N)over20runsforeachmethod,asN
an oracle. grows,asdescribedthroughSectionIV,forthedatasetbuiltfromGd S= ob4 ol′.
For synthetic dataset obtained from the Sobol’ G-function,
the oracle is simply the knowledge of the truly relevant vari-
ables during the dataset’s generation. For the HSOTA and the V. Conclusion
FIRC, the oracle is given by a sensitivity analysis performed
In this paper, we have shown the relevance of the statistical
on the entire dataset before launching the exploration.
tools from sensitivity analysis, in order to design an active
Design of experiment and performance metric: We first sampling flow for the characterization of analog circuits’
start by separating our datasets into a training subset and a performancesagainstalargenumberofvariablesrepresenting
testing subset, according to a 75%-25% split ratio. Then, we sources of parametric variation. This active sampling flow
perform the same following procedure for each method in outperformsMonte-Carlosamplingandwehaveillustratedthe
Table II. role of the different ingredients.
ForanyN 0 ≤N ≤N f,the(variantofthe)activesampling The framework is flexible enough to include surrogate
flow provides a sample set models other than GPs, or to seek sampling criteria other
S ={(x ,y )∈B×R, 1≤j ≤N} , than maximum variance. As such, one could include expert
N j j
knowledge in the surrogate models or one could optimize for
which is a subset of the training dataset. The sample S
N other criteria such as failure rates. This could be the topic of
is used to train a Gaussian Process Regressor F(cid:98)N over the
future research.
selected features.
We are thus in position of computing an R2 score on
the testing dataset
(cid:0) xtest,ytest(cid:1)
. For convenience, we
REFERENCES
j j 1≤j≤NT
recall the definition of the R2 coefficient, also known as [1] B.Bailey,“Addingcircuitagingtovariability.”https://semiengineering.
determination score com/adding-aging-to-variability,2021.
[2] S. Jallepalli, R. Mooraka, S. Parihar, E. Hunter, and E. Maalouf,
R2(N)=1−
(cid:80)N j=T 1(cid:0) y jtest−E(cid:2) F N(cid:0) xt jest(cid:1)(cid:3)(cid:1)2
,
“ evE em np tlo py roin bg abis lc ita il ee sd insig tm hea as ba sm enp cl eing offo inr puef tfi dci oe mnt ainest mim apat pi io nn g,”of IEr Ear Ee
(cid:80)NT (cid:0) ytest−y(cid:1)2
Transactions on Computer-Aided Design of Integrated Circuits and
j=1 j
Systems,vol.35,no.6,pp.943–956,2016.
where y is the mean
[3] X. Shi, F. Liu, J. Yang, and L. He, “A fast and robust failure analysis
of memory circuits using adaptive importance sampling method,” in
y = 1
(cid:88)NT
ytest . Proceedingsofthe55thAnnualDesignAutomationConference,pp.1–6,
N j 2018.
T
j=1 [4] S. Yin, G. Dai, and W. W. Xing, “High-dimensional yield estimation
using shrinkage deep features and maximization of integral entropy
In Figure 6, we illustrate the result graphically in the case
reduction,” in Proceedings of the 28th Asia and South Pacific Design
ofF =G( Sd o= bo4 l) ′.Amorecomprehensiveaccountoftheresults AutomationConference,pp.283–289,2023.
will be given in a table. [5] Y. Liu, G. Dai, and W. W. Xing, “Seeking the yield barrier: High-
dimensional sram evaluation through optimal manifold,” in 2023 60th
Discussion:Itseemsthatourflow(Method1-Theoriginal ACM/IEEEDesignAutomationConference(DAC),pp.1–6,IEEE,2023.
flowofFig.1)outperformsallothersamplingmethods,except [6] J.Zhai,C.Yan,S.-G.Wang,andD.Zhou,“Anefficientbayesianyield
estimationmethodforhighdimensionalandhighsigmasramcircuits,”in
for the Method 3, which is best case scenario using oracle a
Proceedingsofthe55thAnnualDesignAutomationConference,pp.1–6,
priori information. 2018.
6
)N(2R
erocS[7] S. Yin, X. Jin, L. Shi, K. Wang, and W. W. Xing, “Efficient bayesian
yield analysis and optimization with active learning,” in Proceedings
ofthe59thACM/IEEEDesignAutomationConference,pp.1195–1200,
2022.
[8] C.K.WilliamsandC.E.Rasmussen,Gaussianprocessesformachine
learning,vol.2. MITpressCambridge,MA,2006.
[9] I. Azzini and R. Rosati, “A function dataset for benchmarking in
sensitivityanalysis,”DatainBrief,vol.42,p.108071,2022.
[10] Wikipedia contributors, “Operational transconductance amplifier —
Wikipedia, the free encyclopedia.” https://en.wikipedia.org/w/index.
php?title=Operational transconductance amplifier&oldid=1214653053,
2024. [Online;accessed23-April-2024].
[11] Wikipedia contributors, “Clock signal — Wikipedia, the free encyclo-
pedia.” https://en.wikipedia.org/w/index.php?title=Clock signal&oldid=
1217842343,2024. [Online;accessed23-April-2024].
[12] Cadence, “Circuit simulation.” https://www.cadence.com/en US/home/
tools/custom-ic-analog-rf-design/circuit-simulation.html, 2024. [On-
line;accessed23-April-2024].
[13] F.Gamboa,T.Klein,A.Lagnoux,andL.Moreno,“Sensitivityanalysis
in general metric spaces,” Reliability Engineering & System Safety,
vol.212,p.107611,2021.
[14] S.Chatterjee,“Anewcoefficientofcorrelation,”JournaloftheAmerican
StatisticalAssociation,vol.116,no.536,pp.2009–2022,2021.
[15] M. R. Leadbetter, G. Lindgren, and H. Rootze´n, Extremes and related
properties of random sequences and processes. Springer Science &
BusinessMedia,2012.
7