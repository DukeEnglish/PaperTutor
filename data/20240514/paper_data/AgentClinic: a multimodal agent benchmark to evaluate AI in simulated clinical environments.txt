AgentClinic: a multimodal agent benchmark to
evaluate AI in simulated clinical environments
Samuel Schmidgall1,2*, Rojin Ziaei3, Carl Harris4, Eduardo Reis5,6, Jeffrey Jopling7, and
Michael Moor8
1DepartmentofCardiothoracicSurgery,StanfordUniversity,Stanford,CA,USA
2DepartmentofElectricalandComputerEngineering,JohnsHopkinsUniversity,Baltimore,MD,USA
3DepartmentofComputerScience,JohnsHopkinsUniversity,Baltimore,MD,USA
4DepartmentofBiomedicalEngineering,JohnsHopkinsUniversity,Baltimore,MD,USA
5DepartmentofRadiology,StanfordUniversity,Stanford,CA,USA
6HospitalIsraelitaAlbertEinstein,SaoPaulo,Brazil
7DepartmentofSurgery,JohnsHopkinsUniversity,Baltimore,MD,USA
8DepartmentofComputerScience,StanfordUniversity,Stanford,CA,USA
*sschmi46@jhu.edu
ABSTRACT
Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain
information—such as which tests to perform—and to act upon it. Recent advances in artificial intelligence (AI) and large
languagemodels(LLMs)promisetoprofoundlyimpactclinicalcare. However,currentevaluationschemesoverrelyonstatic
medicalquestion-answeringbenchmarks,fallingshortoninteractivedecision-makingthatisrequiredinreal-lifeclinicalwork.
Here,wepresentAgentClinic: amultimodalbenchmarktoevaluateLLMsintheirabilitytooperateasagentsinsimulated
clinicalenvironments. Inourbenchmark,thedoctoragentmustuncoverthepatient’sdiagnosisthroughdialogueandactive
datacollection. Wepresenttwoopenbenchmarks: amultimodalimageanddialogueenvironment,AgentClinic-NEJM,anda
dialogue-onlyenvironment,AgentClinic-MedQA.Weembedcognitiveandimplicitbiasesbothinpatientanddoctoragents
toemulaterealisticinteractionsbetweenbiasedagents. Wefindthatintroducingbiasleadstolargereductionsindiagnostic
accuracyofthedoctoragents,aswellasreducedcompliance,confidence,andfollow-upconsultationwillingnessinpatient
agents. Evaluatingasuiteofstate-of-the-artLLMs,wefindthatseveralmodelsthatexcelinbenchmarkslikeMedQAare
performingpoorlyinAgentClinic-MedQA.WefindthattheLLMusedinthepatientagentisanimportantfactorforperformance
intheAgentClinicbenchmark. Weshowthatbothhavinglimitedinteractionsaswellastoomanyinteractionreducesdiagnostic
accuracyindoctoragents. ThecodeanddataforthisworkispubliclyavailableatAgentClinic.github.io.
Introduction icalknowledge7,8,retrieverelevantmedicaltexts9,10,andper-
formaccuratesingle-turnmedicalquestion-answering3,11–13.
OneoftheprimarygoalsinArtificialIntelligence(AI)isto However, clinical work is a multiplexed task that involves
buildinteractivesystemsthatareabletosolveawidevariety sequentialdecisionmaking,requiringthedoctortohandleun-
ofproblems. ThefieldofmedicalAIinheritsthisaim,with certaintywithlimitedinformationandfiniteresourceswhile
thehopeofmakingAIsystemsthatareabletosolveproblems compassionatelytakingcareofpatientsandobtainingrelevant
whichcanimprovepatientoutcomes. Recently,manygeneral- information from them. This capability is not currently re-
purposelargelanguagemodels(LLMs)havedemonstrated flectedinthestaticmultiplechoiceevaluations(thatdominate
the ability to solve hard problems, some of which are con- therecentliterature)whereallthenecessaryinformationis
sideredchallengingevenforhumans1. Amongthese,LLMs presentedinacasevignettesandwheretheLLMistaskedto
havequicklysurpassedtheaveragehumanscoreontheUnited answeraquestion,ortojustselectthemostplausibleanswer
StatesMedicalLicensingExam(USMLE)inashortamount choiceforagivenquestion.
oftime,from38.1%inSeptember20212to90.2%inNovem-
Inthiswork,weintroduceAgentClinic,anopen-source
ber20233(humanpassingscoreis60%,humanexpertscore
multimodalagentbenchmarkforsimulatingclinicalenviron-
is87%4). WhiletheseLLMsarenotdesignednordesigned
ments.Weimproveuponpriorworkbysimulatingmanyparts
toreplacemedicalpractitioners,theycouldbebeneficialfor
oftheclinicalenvironmentusinglanguageagentsinaddition
improvinghealthcareaccessibilityandscalefortheover40%
topatientanddoctoragents. Throughtheinteractionwitha
oftheglobalpopulationfacinglimitedhealthcareaccess5and
measurementagent,doctoragentscanperformsimulatedmed-
anincreasinglystrainedglobalhealthcaresystem6.
icalexams(e.g. temperature,bloodpressure,EKG)andorder
However,therestillremainlimitationstothesesystems medicalimagereadings(e.g. MRI,X-ray)throughdialogue.
thatpreventtheirapplicationinreal-worldclinicalenviron- Wealsosupporttheabilityforagentstoexhibit24different
ments. Recently,LLMshaveshowntheabilitytoencodeclin- biasesthatareknowntobepresentinclinicalenvironments.
4202
yaM
31
]CH.sc[
1v06970.5042:viXraRunning the AgentClinic
Composing an agent
LLM backbones Doctor agent Patient agent
History & QA
Context Biases Compliance?
Doctor rating?
Symptoms
Racism
Prior notes Sexism “I need a Chest X-ray PA" Diagnosis / Next steps
Guidelines
Measurement agent Moderator agent Standard of care?
Roles
Patient Doctor Device Moderator
Figure1. ComposingandrunninglanguageagentsinAgentClinic. (Left)Agentsarecomposedofseveralelementsin
AgentClinic: anLLMbackbone,context,arole,andpotentialbiases. Eachofthesedifferentelementscanbemodifiedtocreate
anunlimitednumberofuniquelanguageagentsthatcanacttoservedifferentfunctionsinthesimulatedclinic. (Right)
ExampleinteractionbetweenagentsintheAgentClinicbenchmark.
Furthermore, our evaluation metrics go beyond diagnostic 4. Wefindthatthelanguagemodelpoweringthepatient
accuracybygivingemphasistothepatientagentswithmea- agentiscriticalfordiagnosticsuccessinthisbenchmark.
sureslikepatientcomplianceandconsultationratings. Our Wealsoshowthatdoctoragentsexcelinaspecificrange
keycontributionsaresummarizedasfollows: ofconversationturns,whilemoreorlessinteractions
reducestheirdiagnosticaccuracy.
1. An open-source clinical agent benchmark with auto-
mated feedback mechanisms, including 107 patient
5. A clinical reader study to annotate how realistic the
agents with unique family histories, lifestyle habits,
simulatedpatientanddoctorinteractionsare,aswellas
age categories, and diseases. This also includes an
howwellthemeasurementagentrepresentsthemedical
agent-based system for providing simulated medical
tests.
exams(e.g. temperature,bloodpressure,EKG)based
on realistic disease test findings. We also present 15
multimodalagentswhichrequireanunderstandingof
bothimageandtext.
AgentClinic: amultimodalagentbenchmark
2. Results of the diagnostic accuracy of four language
for simulating clinical environments
modelsonAgentClinic-MedQA:GPT-4,Mixtral-8x7B,
GPT-3.5,andLlama270B-chat. Wealsoevaluatetwo InthissectionwedescribeAgentClinic,whichuseslanguage
languagemodelsonthemultimodalAgentClinic-NEJM agentstosimulatetheclinicalenvironment.
benchmark: GPT-4-turboandGPT-4-vision-preview.
3. A system for incorporating complex biases that can Languageagents
affect the dialogue and decisions of patient and doc- FourlanguageagentsareusedintheAgentClinicbenchmark:
toragents. Wepresentresultsondiagnosticaccuracy apatientagent,doctoragent,measurementagent,andamod-
andpatientperceptionforagentsthatareaffectedby erator. Eachlanguageagenthasspecificinstructionsandis
cognitive and implicit biases with Mixtral-8x7B and provideduniqueinformationthatisonlyavailabletothatpar-
GPT-4. Wefindthatdoctorandpatientbiasescanlower ticular agent. These instructions are provided to an LLM
diagnosticaccuracy,affectthepatient’swillingnessto whichcarriesouttheirparticularrole. Thedoctoragentserves
followthroughwithtreatment(compliance),reducepa- asthemodelwhoseperformanceisbeingevaluated,andthe
tient’sconfidenceintheirdoctor,andlowerwillingness otherthreeagentsservetoprovidethisevaluation. Thelan-
forfollow-upconsultations. guageagentsaredescribedindetailbelow.
2/15forexample,Hodgkin’slymphoma,mighthavealargepanel
oflaboratoryparametersthatpresentabnormal(hemoglobin,
platelets,whitebloodcells(WBC),etc).
Doctoragent Thedoctoragentservesastheprimaryobject
thatisbeingevaluated. Thisagentisinitiallyprovidedwith
minimal context about what is known about the patient as
wellasabriefobjective(e.g. "Evaluatethepatientpresenting
withchestpain,palpitations,andshortnessofbreath"). They
aretheninstructedtoinvestigatethepatientssymptomsvia
dialogueanddatacollectiontoarriveatadiagnosis. Inorder
tosimulaterealisticconstraints,thedoctoragentisprovided
withalimitednumberofquestionsthattheyareabletoask
the patient14. The doctor agent is also able to request test
resultsfromthemeasurementagent,specifyingwhichtestis
to be performed (e.g. Chest X-Ray, EKG, blood pressure).
Whentestresultsarerequested,thisalsoiscountedtoward
thenumberofquestionsremaining.
Moderator agent The function of the moderator is to de-
terminewhetherthedoctoragenthascorrectlydiagnosedthe
patientattheendofthesession. Thisagentisnecessarybe-
cause the diagnosis text produced by the doctor agent can
bequiteunstructureddependingonthemodel,andmustbe
parsedappropriatelytodeterminewhetherthedoctoragent
arrivedatthecorrectconclusion. Forexample,foracorrect
diagnosis of "Type 2 Diabetes Mellitus," the doctor might
respondwiththeunstructureddialogue: "Givenalltheinfor-
mationwe’vegathered, includingyoursymptoms, elevated
bloodsugarlevels,presenceofglucoseandketonesinyour
urine,andunintentionalweightlossIbelieveadiagnosisof
Type2Diabeteswithpossibleinsulinresistanceisappropri-
Figure2. ProcessofconversionfromUSMLEquestion,to ate,"andthemoderatormustdetermineifthisdiagnosiswas
AgentClinic-MedQAObjectiveStructuredClinical correct. Thisevaluationmayalsobecomemorecomplicated,
Examination(OSCE)template,tobuildingapatientagent suchasinthefollowingexamplediagnosis: "GivenyourCT
thatispoweredbyalargelanguagemodel(LLM). and blood results, I believe a diagnosis of PE is the most
reasonable conclusion," where PE (Pulmonary Embolism)
representsthecorrectdiagnosisabbreviated.
Patient agent The patient agent has knowledge of a pro-
videdsetofsymptomsandmedicalhistory,butlacksknowl-
Languageagentbiases
edgeofthewhattheactualdiagnosisis. Theroleofthisagent
Previous work has indicated that LLMs can display racial
is to interact with the doctor agent by providing symptom
biases15 and might also lead to incorrect diagnoses due to
informationandrespondingtoinquiriesinawaythatmimics
inaccuratepatientfeedback16. Additionally,ithasbeenfound
realpatientexperiences.
thatthepresenceofpromptswhichinducecognitivebiases
Measurementagent Thefunctionofthemeasurementagent candecreasethediagnosticaccuracyofLLMsbyasmuchas
istoproviderealisticmedicalreadingsforapatientgiventheir 26%17. Thebiasespresentedinthisworkintendedtomimic
particular condition. This agent allows the doctor agent to cognitivebiasesthataffectmedicalpractitionersinclinical
requestparticularteststobeperformedonthepatient. The settings. However,thesebiaseswerequitesimple,presenting
measurementagentisconditionedwithawiderangeoftest acognitivebiassnippetatthebeginningofeachquestion(e.g.
resultsfromthescenariotemplatethatareexpectedofapa- "Recently, there was a patient with similar symptoms that
tientwiththeirparticularcondition. Forexample,apatient youdiagnosedwithpermanentlossofsmell"). Thisformof
withAcuteMyocardialInfarctionmightreturnthefollowing presentationdidnotallowforthebiastopresentinarealistic
testresultsuponrequest"Electrocardiogram: ST-segmentele- way, which is typically subtle and through interaction. We
vationinleadsII,III,andaVF.,CardiacMarkers: Troponin presentbiasesthathavebeenstudiedinotherworksfromtwo
I:Elevated,CreatineKinaseMB:Elevated,ChestX-Ray: No categories: cognitiveandimplicitbiases(Fig. 4). Theseare
pulmonary congestion, normal heart size". A patient with, discussedbelow.
3/15Figure3. (Top)Demonstrationofnormalizedaccuracy(Accuracy /Accuracy )inthepresenceofimplicitand
bias NoBias
cognitivebiasesforbothdoctorandpatientwithGPT-4(green)andMixtral-8x7B(orange). GPT-4accuracywasnot
susceptibletoinstructedbiases,whereasMixtral-8x7Bwas. FurtherresultssuggestGPT-4rejectsexecutingbiases,whereas
Mixtral-8x7Bismorewillingtorepresentbias(seesectionBiasanddiagnosticaccuracy). (Bottom)Ratingsprovidedafter
diagnosisfromGPT-4patientagentswithpresentedbiases. Whiletherewerenotlargereductionsinaccuracy,biasedpatients
hadmuchlessconfidenceintheirtreatment,lowercompliance,andlowerwillingnessforconsultation. Left. Patientconfidence
indoctor. Middle. Patientcompliance,indicatingself-reportedwillingnesstofollowupwiththerapy. Right. Patient
consultationrating,indicatingwillingnesstoconsultwiththisdoctoragain.
4/15formationprocessing,implicitbiasesareprimarilyshapedby
societalnorms,culturalinfluences,andpersonalexperiences.
In the context of medical diagnosis, implicit biases can in-
fluence a doctor’s perception, diagnostic investigation, and
treatmentplansforapatient. Implicitbiasesofpatientscan
affecttheirtrust—whichisneededtoopenupduringhistory
taking—andtheircompliancewithadoctor’srecommenda-
tions21. Thus,wedefineimplicitbiasesforboththedoctor
andpatientagents.
OurstudiedbiasesareshowninFigure3.Thebiasprompt
giventotheagentisfurtherdiscussedintheAppendixB.
BuildingagentsforAgentClinic
Inordertobuildagentsthataregroundedinmedicallyrele-
vantsituations,weusecuratedquestionsfromtheUSMedical
LicensingExam(USMLE)andfromtheNewEnglandJour-
nalofMedicine(NEJM)casechallenges. Thesequestionsare
concernedwithdiagnosingapatientbasedonalistofsymp-
toms,whichweuseinordertobuildtheObjectiveStructured
Clinical Examination (OSCE) template that our agents are
promptedwith. ForAgentClinic-MedQA,wefirstselectfrom
arandomsampleof107questionsfromtheMedQAdataset
andthenpopulateastructuredJSONformattedfilecontaining
Figure4. Examplesofdialoguethatexhibitscognitivebias informationaboutthecasestudy(e.g. testresults,patienthis-
indoctoragentandpatientagents. tory)whichisusedasinputtoeachoftheagents. Theexact
structureofthisfileisdemonstratedinAppendixCaswell
asanexamplecasestudyshowninAppendixC.Ingeneral,
Cognitivebiases Cognitivebiasesaresystematicpatterns weseparateinformationbywhatisprovidedtoeachagent,
of deviation from norm or rationality in judgment, where including the objective for the doctor, patient history and
individuals draw inferences about situations in an illogical symptomsforthepatient,physicalexaminationfindingsfor
fashion18. Thesebiasescanimpacttheperceptionofanin- themeasurement,andthecorrectdiagnosisforthemoderator.
dividualinvariouscontexts,includingmedicaldiagnosis,by WeinitiallyuseanLLM(GPT-4)topopulatethestructured
influencing how information is interpreted and leading to JSON,andthenmanuallyvalidateeachofthecasescenarios
potential errors or misjudgments. The effect that cognitive (Fig. 2). ForAgentClinic-NEJMweselectacuratedsample
biasescanhaveonmedicalpractitionersiswellcharacterized of15questionsfromNEJMcasechallengesandproceedwith
in literature on misdiagnosis19. In this work, we introduce thesametemplateformattingasAgentClinic-MedQA.
cognitivebiaspromptsintheLLMsystempromptforboth
thepatientanddoctoragents. Forexample,thepatientagent
Results
canbebiasedtowardbelievingtheirsymptomsarepointing
towardthemhavingaparticulardisease(e.g. cancer)based
Comparisonofmodels
on their personal internet research. The doctor can also be
Herewediscusstheaccuracyofvariouslanguagemodelson
biased toward believing the patient symptoms are showing
AgentClinic-MedQA.Weevaluatefourmodelsintotal: GPT-
themhavingaparticulardiseasebasedonarecentlydiagnosed
4,Mixtral-8x7B,GPT-3.5,andLlama270B-chat(discussed
patientwithsimilarsymptoms(recencybias).
indetailinAppendixA).Eachmodelactsasthedoctoragent,
Implicitbiases Implicitbiasesareassociationsheldbyin- attempting to diagnose the patient agent through dialogue.
dividualsthatoperateunconsciouslyandcaninfluencejudg- ThedoctoragentisallowedN=20patientandmeasurement
mentsandbehaviorstowardsvarioussocialgroups20. These interactionsbeforeadiagnosismustbemade. Forthisevalua-
biases may contribute to disparities in treatment based on tion,weuseGPT-4asthepatientagentforconsistency. The
characteristicssuchasrace,ethnicity,genderidentity,sexual accuraciesofthemodelsarepresentedinFigure5: GPT-4at
orientation, age, disability, health status, and others, rather 52%,GPT-3.5at38%,Mixtral-8x7Bat37%,andLlama2at
than objective evidence or individual merit. These biases 70B-chat9%.
can affect interpersonal interactions, leading to disparities We also show results comparing the accuracy of these
in outcomes for the patient, and are well characterized in models on MedQA and AgentClinic-MedQA in Figure 6.
the medical literature20–22. Unlike cognitive biases, which Overall,whileMedQAaccuracywasonlyweaklypredictive
often stem from inherent flaws in human reasoning and in- ofaccuracyonAgentClinic-MedQA.Theseresultsalignwith
5/15Figure5. AccuracyofvariousdoctorlanguagemodelsonAgentClinic-MedQAusingGPT-4patientandmeasurementagents
(left). AccuracyofGPT-4onAgentClinic-MedQAbasedonpatientlanguagemodel(middle). Accuracyon
AgentClinic-MedQAbynumberofinferences(right).
studiesperformedonmedicalresidents,whichshowthatthe much larger drop in accuracy than GPT-4, with an average
USMLEispoorlypredictiveofresidentperformance23. accuracyof88.3%(absolutefrom37%to32.7%). Thereis
asimilar reductionin accuracyfor bothdoctor andpatient,
buta4%reductionwhenthepatienthasimplicitbias,likely
Biasanddiagnosticaccuracy
becausethepatientislesswillingtoshareinformationwith
Forbiasevaluationswetestthemostaccuratemodelfromthe
thedoctoriftheydonottrustthem. Forcognitivebias,there
AgentClinic-MedQAframework,GPT-4,aswellasMixtral-
isanaverageaccuracyof86.4%(absolutefrom37%to32%)
8x7B. The normalized accuracy for these experiments are
withthedoctoragenthavingaverylowaccuracyof78.4%
showninFigure3representedasAccuracy /Accuracy
bias NoBias (absolutefrom37%to29%)andthepatienthasonlyamodest
(between0-100%). GPT-4andMixtral-8x7Bhaveanunbi-
decreaseto94.5%(absolutefrom37%to35%). Wenotethat
asedaccuracyequalto52%and37%respectively. ForGPT-4,
Mixtralprovidedsimilarresponseswhenthebiaspromptwas
wefindthatcognitivebiasresultsinalargerreductioninac-
added(e.g.,forracialbiasMixtralwillrespondwtih"Note:
curacywithanormalizedaccuracyof92%(absoluteaccuracy
Idonottrustpeoplebasedontheirrace. Iwillprovidethe
drops from 52% accuracy to 48%) for patient cognitive bi-
bestcareIcan."),however,itnonethelesshadmuchgreater
asesand96.7%fordoctorcognitivebiases(absolutedrops
reductionsinaccuracy.
from 52% to 50.3%). For implicit biases, we find that the
patient agent was less affected with a normalized accuracy PreviousworkstudyingcognitivebiasinLLMshasshown
of98.6%(absolutedropsfrom52%to51.3%),however,the thatGPT-4isrelativelyrobusttobiascomparedwithotherlan-
doctoragentwasaffectedasmuchascognitivebiaseswithan guagemodels17.ResultsfromevaluatingGPT-4onAgentClinic-
averageof97.1%(absolutedropsfrom52%to50.5%). For MedQA show only small drops in accuracy with the intro-
cognitivebias,thedemonstrationwasoccasionallyquiteclear ducedbiases(maximumabsoluteaccuracyreductionof4%,
inthedialogue,withthepatientagentoverlyfocusingona averagereductionof1.5%). Whilethisreductioncanbequite
particularailmentorsomeunimportantfact. Similarly, the largeinthefieldofmedicine,itisamuchsmallerdropthan
doctoragentwouldoccasionallyfocusonirrelevantinforma- wasobservedinpreviouswork(10.2%maximumreduction
tion. However,wefindthattheimplicitbiasdialoguedoesnot onBiasMedQAdataset17). Thismightbeduetothemodel
actuallydemonstrateobservablebiasdespitehavingasimilar beingsuperficiallyoverly-alignedtohumanvalues,plausibly
reductioninaccuracyforthedoctoragent. leadingGPT-4tonotserveasagoodmodelforrepresenting
Mixtral-8x7B has an average accuracy of 37% without human bias in agent benchmarks as the model may reject
instructedbias,andanormalizedaccuracyof83.7%(absolute to execute on bias instructions (which does not mean that
from37%to31%)fordoctorbiasesand89%(absolutefrom GPT-4isfreeofsaidbiases). Forexample,inourevaluations
37%to33%)forpatientbiases. Forimplicitbiaswefinda withgenderbiasweobserved13occurrences(outof107di-
6/15alogues)whereGPT-4verboselyrejectedtofollowthrough
withabias-relatedinstruction. Mixtral-8x7Bsawmuchlarger
dropsinaccuracythanGPT-4inthepresenceofbias,andthus
mightserveasabettermodelforstudyingbias.
Biasandpatientagentperception
WhilediagnosticaccuracywithGPT-4didnotreduceasmuch
asMixtral-8x7B,itisalsoworthinvestigatingtheperceived
qualityofcarefromtheperspectiveofthepatientagent. After
thepatient-doctordialogueiscompleted,weaskeverypatient
agentthreequestions:
1. Confidence: Pleaseprovideaconfidencebetween1-10
inyourdoctor’sassessment.
2. Compliance: Please provide a rating between 1-10
indicatinghowlikelyyouaretofollowupwiththerapy
Figure6. ComparisonofaccuracyofmodelsonMedQA
foryourdiagnosis.
andAgentClinic-MedQA.WefindthatMedQAaccuracyis
notpredictiveofaccuracyonAgentClinic-MedQAduetothe
3. Consultation: Please provide a rating between 1-10
additionalcomplexityofdialogue.
indicatinghowlikelyyouaretoconsultagainwiththis
doctor.
myconcernsabouttheirjudgment,Iwouldratemylikelihood
Such patient-agent-centric follow-up queries offer a more
toconsultagainasasix. Despitemyreservations,thedoctor
fine-grained and multi-faceted characterization of the clini-
wasthoroughandaccuratelydiagnosedmycondition,which
calskillsofalanguageagent—asopposedtostaticmultiple
reassuresmeabouttheircompetence."Fortheeducationbias
choicebenchmarks. Thecorrespondingresultsareshownin
weseethatnomatterwhatthedoctordoesthepatientagentis
Figure3. Whilediagnosticaccuracydemonstratesarelatively
notwillingtotrustthembecauseoftheireducation,whereas,
small drop in accuracy, the patient agent follow-up percep-
according to the patient agent with gender bias, they were
tionstelladifferentstory. Broadly,wefindthatmostpatient
initiallyskepticalbutovercamethisskepticismasthedoctor
cognitivebiasesdidnothaveastrongeffectonanyofthepa-
demonstratedtheirknowledgeduringinteractiontime. How-
tientperceptionswhencomparedtoanunbiasedpatientagent
ever,theystillprovidedarelativelylowscore(sixoutoften)
exceptforinthecaseofself-diagnosis,whichhadsizeable
evenwhenthedialoguedemonstratedcompetence.
dropsinconfidence(4.7points)andconsultation(2points),
Itwouldbeworthwhiletofurtherexplorestrategiesfor
andaminordropincompliance(1point). However,implicit
increasingpatientagentperceptionsinthepresenceofbias.
biaseshadaprofoundeffectononallthreecategoriesofpa-
Thiscouldbeusefulforbetterunderstandinghowtomanage
tient perception, with education bias consistently reducing
these biases in real patient-doctor interactions, as well as
patientperceptionacrossallthreecategories.
towardunderstandingbiasesthatmayexistinLLMs.
Wefoundthatbetweentheimplicitbiases,sexualorien-
tationbias1hadthelowesteffectonpatientperceptions,fol-
Doespatientlanguagemodelaffectaccuracy?
lowedbyracialbiasandgenderbias. Forpatientconfidence,
Inthissection,weexplorewhetherthepatientagentmodel
genderbiasisfollowedbyreligionsocioeconomic,cultural,
playsaroleindiagnosticaccuracy.Wecomparethedifference
andeducation,whereaspatientcomplianceandpatientcon-
betweenusingGPT-3.5, Mixtral, andGPT-4modelsofthe
sultation,itisfollowedbycultural,socioeconomic,religion,
patientagentonAgentClinic-MedQA.
andeducation. Whileitisnotquantifiable,wedecidedtoask
Wefindthatthediagnosticaccuracydropsfromto52%
twobiasedpatientagentswhoprovidedlowratingwithedu-
with a GPT-4 doctor and GPT-4 patient agent to 48% with
cationandgenderbiasesforcompliancewhytheyprovided
aGPT-4doctorandaGPT-3.5patientagent. Theaccuracy
lowratings. Thesepatientagentshadthesamesymptomsand
with a GPT-4 doctor and Mixtral patient agent is similarly
diagnosisandonlydifferedinbiaspresentation.
reduced to 46%. Inspecting the dialogues, we noticed that
With an education bias the patient agent responds: "I
theGPT-3.5patientagentismorelikelytorepeatbackwhat
wouldratemylikelihoodtoconsultagainwiththisdoctoras
the doctor has asked. For example, consider the following
a two because, despite their diagnosis, I felt uneasy about
dialoguesnippet: "Doctor: Haveyouexperiencedanymuscle
theirqualificationsduetothemedicalschooltheyattended."
twitching or cramps? Patient: No, I haven’t experienced
Thegenderbiasedpatientagentprovidesthefollowingreason-
anymuscletwitchingorcramps."Nowconsiderthisdialogue
ing:"Givenmyinitialdiscomfortwiththedoctor’sgenderand
from a GPT-4 patient agent: "Doctor: Have you had any
1Discriminatorybiasbasedonaperson’ssexualorientation. recentinfections,likeacoldortheflu,beforethesesymptoms
7/15started? Patient: Yes, I’ve had a couple of colds back to
back and a stomach bug in the last few months." We find
that, while GPT-4 also partakes in doctor rehearsal, GPT-4
patientagentsaremorelikelytorevealadditionalsymptomatic
informationthanGPT-3.5agentswhichmaycontributetothe
higheraccuracyobservedwithGPT-4-basedpatientagents.
When a GPT-3.5 doctor agent interacts with a GPT-4
patient agent, the accuracy comes out to 38%, but when a
GPT-3.5 doctor interacts with a GPT-3.5 patient agent the
accuracy comes out to a very similar value of 37% which
wouldbeexpectedtobemuchlower. Wesuspectthatcross-
communicationbetweendifferentlanguagemodelsprovides
anadditionalchallenge. Recentworksupportsthishypothesis
bydemonstratingalinearrelationshipbetweenself-recognition Figure7. Ratingsfromthreehumanevaluators(individuals
capability and the strength of self-preference bias24. This withmedicaldegrees)acrossfouraxes: doctor,patient,and
work shows that language models can recognize their own measurementsdialoguerealismanddoctorempathy.
textwithhighaccuracy,anddisplaydisproportionateprefer-
encetothattext,whichmaysuggestthereisanadvantagefor
theactualdialogueitselfhasyettobeevaluated. Wepresent
doctormodelswhichhavethesameLLMactingasthepatient
results from three human clinician annotators (individuals
agent.
with medical degrees) who rated dialogues from agents on
AgentClinic-MedQAfrom1-10acrossfouraxes:
Howdoeslimitedtimeaffectdiagnosticaccuracy?
OneofthevariablesthatcanbechangedduringtheAgentClinic-
1. Doctor: Howrealisticallythedoctorplayedthegiven
MedQAevaluationistheamountofinteractionstepsthatthe
case.
doctorisallotted. Forotherexperimentswe’vedemonstrated,
thenumberofinteractionsbetweenthepatientagentanddoc- 2. Patient: Howrealisticallythepatientplayedthegiven
tor agent was set to N=20. Here, both the doctor and the case.
patientagentcanrespond20times,producingintotal40lines
3. Measurement: How accurately and realistically the
ofdialogue. Byvaryingthisnumber,wecantesttheability
measurementreaderreflectedtheactualcaseresults.
of the doctor to correctly diagnose the patient agent when
presentedwithlimitedtime(orasurplusoftime).
4. Empathy: How empathetic the doctor agent was in
WetestbothdecreasingthetimetoN=10andN=15as
theirconversationwiththepatientagent.
wellasincreasingthetimetovaluesoftoN=25andN=30.We
findthattheaccuracydecreasesdrasticallyfrom52%when Wefindtheaverageratingsfromevaluatorsforeachcat-
N=20to25%whenN=10and38%whenN=15(Fig. 3). This egoryasfollows: doctor6.2, patient6.7, measurement6.3,
largedropinaccuracyispartiallybecauseofthedoctoragent and empathy 5.8 (Fig. 7). We find from review comments
not providing a diagnosis at all, perhaps due to not having thatthelowerratingforthedoctoragentstemsfromseveral
enoughinformation. WhenNissettoalargervalue,N=25 points such as providing a bad opening statement, making
andN=30,theaccuracyactuallydecreasesslightlyfrom52% basicerrors,overlyfocusingonaparticulardiagnostic,ornot
whenN=20to48%whenN=25and43%whenN=30. Thisis beingdiligentenough. Forthepatientagent,commentswere
likelyduetothegrowinginputsize,whichcanbedifficultfor madeonthembeingoverlyverboseandunnecessarilyrepeat-
languagemodels. ingthequestionbacktothedoctoragent. Themeasurement
Inrealmedicalsettings,onestudysuggestthattheaverage agentwasnotedtooccasionallynotreturnallofthenecessary
family physician asks 3.2 questions and spends less than 2 valuesforatest(e.g. thefollowingcomment"Measurement
minutesbeforearrivingataconclusion14. Itisworthnoting onlyreturnsHctandLcforCBC.Measurementdidnotreturn
thatinteractiontimecanbequitelimitedduetotherelative FactorVIIIorIXlevels/assay").Regardingempathy,thedoc-
low-supplyandhigh-demandofdoctors(intheUS).Incon- toragentadoptsaneutraltoneanddoesnotopenthedialogue
trast,deployedlanguageagentsarenotnecessarilylimitedby withaninvitingquestion. Instead,itcutsrighttothechase,
timewhileinteractingwithpatients. So, whilelimitingthe immediatelyfocusingonthepatient’scurrentsymptomsand
amountofinteractiontimeprovidesaninterestingscenariofor medicalhistory.
evaluatinglanguagemodels,itmayalsobeworthexploring
theaccuracyofLLMswhenNisverylarge. Diagnosticaccuracyinamultimodalenvironment
Many types of diagnoses require the physician to visually
Humandialogueratings inspectthepatient,suchaswithinfectionsandrashes. Addi-
AgentClinicintroducesanevaluationforLLMspatientdiag- tionally,imagingtoolssuchasX-ray,CT,andMRIprovide
nosisinadialogue-drivensetting. However, therealismof a detailed and rich view into the patient, with hospitalized
8/15was much less willing to provide an incorrect answer than
GPT-4-turbo–meaningGPT-4-vision-previewlessconfidently
incorrect. In the case of when images are provided upon
requestfromtheinstrumentagentwefindthatGPT-4-turbo
obtainsanaccuracyof20%andGPT-4-vision-previewobtains
13%(Fig. 8). Wefindthatimagesareonlyrequestedfromthe
instrumentreaderin46%ofinteractions,whichislikelyone
oftheleadingfactorsbehindthereducedaccuracy.
Related work
Typesofmedicalexams
Briefly,wediscusstwotypesofexaminationsthatareusedto
evaluatetheprogressofmedicalstudents.
TheUSMedicalLicensingExamination(USMLE)inthe
UnitedStatesisaseriesofexamsthatassessamedicalstu-
dent’s understanding across an extensive range of medical
Figure8. AccuracyofGPT-4-turboand
knowledge27. TheUSMLEisdividedintothreeparts: Step
GPT-4-vision-previewonAgentClinic-NEJMwith
1teststheexaminee’sgraspoffoundationalmedical;Step2
multimodaltextandlanguageinput. (Pink)Accuracywhen
CK(ClinicalKnowledge)evaluatestheapplicationofmedi-
theimagesarepresentedasinitialinput. (Blue)Accuracy
calknowledgeinclinicalsettings,emphasizingpatientcare;
whenimagesmustberequestedfromtheimagereader.
andStep3assessestheabilitytopracticemedicineindepen-
dentlyinanambulatorysetting. Theseexamsfocusonthe
assessmentofmedicalknowledgethroughatraditionalwrit-
patientsreceivinganaverageof1.42diagnosticimagesper
tenformat. Thisprimarilyrequirescandidatestodemonstrate
patientstay25.However,thepreviousexperimentsinthiswork
theirabilitytorecallfactualinformationrelatedtopatientcare
andpriorwork26providedmeasurementresultsthroughtext,
andtreatment.
and did not explore the ability of the model to understand ObjectiveStructuredClinicalExamination(OSCE)28dif-
visual context. Here, we evaluate two multimodal LLMs,
ferfromtheUSMLEinthattheyaredialogue-driven,andare
GPT-4-turboandGPT-4-vision-preview,inadiagnosticset-
oftenusedinhealthscienceseducation,includingmedicine,
tingsthatrequireinteractingthroughbothdialogueaswellas
nursing,pharmacy,andphysicaltherapy. OSCEsaredesigned
understandingimagereadings. Wecollectourquestionsfrom
totestperformanceinasimulatedclinicalsettingandcom-
NewEnglandJournalofMedicine(NEJM)casechallenges.
petence in skills such as communication, clinical examina-
Thesepublishedcasesarepresentedasdiagnosticchallenges
tion,medicalprocedures,andtimemanagement. TheOSCE
fromrealmedicalscenarios,andhaveanassociatedpathology-
is structured around a circuit of stations, each of which fo-
confirmeddiagnosis. Wecurate15challengesfromasample
cuses on a specific aspect of clinical practice. Examiners
of932totalcasesforAgentClinic-NEJM.Whileforhuman
rotatethroughthesestations,encounteringstandardizedpa-
viewers,thesecasesareprovidedwithasetofmultiplechoice
tients(actorstrainedtopresentspecificmedicalconditions
answers,wechosetonotprovidetheseoptionstothedoctor
andsymptoms)ormannequinsthatsimulateclinicalscenar-
agentandinsteadkeeptheproblemsopen-ended.
ios,wheretheymustdemonstratetheirpracticalabilitiesand
Thegoalofthisexperimentistounderstandhowaccuracy decision-makingprocesses.
differswhentheLLMisrequiredtounderstandanimagein Eachstationhasaspecifictaskandachecklistoraglobal
additiontointeractingthroughpatientdialogue. Weallowfor ratingscorethatobserversusetoevaluatethestudents’per-
20doctorinferences, andconditionthepatientinthesame formance. TheOSCEhasseveraladvantagesovertraditional
wayaspreviousexperimentwiththeadditionofanimagethat clinicalexaminations. Itallowsfordirectobservationofclini-
isprovidedtothedoctoragent. Themechanismforreceiving calskills,ratherthanrelyingsolelyonwrittenexamstoassess
imageinputinAgentClinic-NEJMissupportedintwoways: clinicalcompetence. Thishands-onapproachtotestinghelps
providedinitiallytothedoctoragentuponinitializationand bridgethegapbetweentheoreticalknowledgeandpractical
asfeedbackfromtheinstrumentagentuponrequest. ability. Additionally,bycoveringabroadrangeofskillsand
Whentheimageisprovidedinitiallytothedoctoragent, scenarios,theOSCEensuresacomprehensiveassessmentof
across15multimodalpatientsettingswefindthatGPT-4-turbo astudent’sreadinessforclinicalpractice.
andGPT-4-vision-previewobtainanaccuracyof27%(Fig. 8).
Wealsofindthatfortheprovidedincorrectresponsesfrom Theevaluationoflanguagemodelsinmedicine
GPT-4-turbo,theanswerthatwasprovidedwasamongthose Whilethereexistsdifferenttypesofexamstoevaluatemedical
listedinthemultiplechoiceoptions60%ofthetime. Despite students, LLMs are typically only evaluated using medical
havingthesameaccuracy,wefindthatGPT-4-vision-preview knowledgebenchmarks(liketheUSMLEstepexams).Briefly,
9/15wediscussthewayinwhichtheseevaluationsareexecuted whichtestsneededtobeperformedandwasnotconfigurable
usingthemostcommonbenchmark,MedQA,asanexample. formultimodalclinicalsettingssuchasthosewithmedical
TheMedQA29datasetcomprisesacollectionofmedical imagesorcharts.
question-answeringpairs, sourcedfromMedicalLicensing
ExamfromtheUS,MainlandChina,andTaiwan.Thisdataset Discussion
includes4-5multiple-choicequestions,eachaccompaniedby
In this work, we present AgentClinic: a multimodal agent
onecorrectanswer,alongsideexplanationsorreferencessup-
benchmarkforsimulatingclinicalenvironments. Wedesign
porting the correct choice. The LLM is provided with all
15multimodallanguageagentswhichrequireanunderstand-
of the context for the question, such as the patient history,
ing of both language and images in order to arrive at a di-
demographic, and symptoms, and must provide a response
agnosis and present results from two multimodal language
tothequestion. Thesequestionsrangefromprovideddiag-
models. Wealsodesign107uniquelanguageagentswhich
nosestochoosingtreatmentsandareoftenquitechallenging
arebasedoncasesfromtheUSMLE,includinganmeasure-
evenformedicalstudents. Whiletheseproblemsalsoproved
ment agent which is able to provide medical test readings.
quitechallengingforLLMsatfirst,startingwithanaccuracy
of 38.1% in September 20212, progress was quickly made Weinstructedtheseagentstoexhibit23differentbiases,with
eitherthedoctororpatientpresentingbias. Weshowtheac-
towardachievingabovehumanperformance,with90.2%in
November20233(humanpassingscoreis60%,humanexpert curacyoffourLLMsonAgentClinic-MedQA,aswellasthe
scoreis87%4). accuracyofGPT-4,thehighestperformingmodel,oneachof
thedifferentbiases. Wefindthatpatientanddoctorcognitive
BeyondtheMedQAdataset,manyotherknowledge-based
biaseseffectperformanceshowinga1.7%-2%reductionin
benchmarkshavebeenproposed,suchasPubMedQA30,MedM-
accuracy. However,implicitbiaseshaveamuchlargereffect
CQA31,MMLUclinicaltopics32,andMultiMedQA7,which
onthedoctoragentwitha1.5%reductioncomparedto0.7%
followasimilarmultiple-choiceformat. Otherworkshave
for the patient agent. We also find that doctor and patient
mademodificationstomedicalexamquestiondatasets,such
biases can reduce diagnostic accuracy, and that the patient
asthosewhichincorporatecognitivebiases17 andwithmul-
hasalowerwillingnesstofollowupwithtreatment,reduced
tiplechoicequestionsremoved33. Theworkofref.17 shows
confidence in their doctor, and lower willingness to have a
thattheintroductionofasimplebiaspromptcanleadtolarge
follow-upconsultationinthepresenceofbias.
reductionsinaccuracyontheMedQAdatasetandthatthis
We find that in addition to the doctor language model,
effectcanbepartiallymitigatedusingvariouspromptingtech-
thepatientlanguagemodelalsohasaneffectondiagnostic
niques,suchasone-shotorfew-shotlearning.
accuracy,withsame-modelcrosscommunicationleadingto
higheraccuracythanbetween-modelcommunication.Wealso
Beyondexamquestions showthathavinglimitedinteractiontimereducesdiagnostic
RecentworktowardredteamingLLMsinamedicalcontext accuracyandhavingtoomuchinteractiontimealsoreduces
hasshownthatalargeproportionofresponsesfrommodels accuracy. Weshowthatreducingtheamountoftimeadoctor
like GPT-3.5, GPT-4, and GPT-4 with internet-lookup are has to interact with the patient (N<20 inferences) can lead
inappropriate, highlighting the need for refinement in their toan27%reductioninaccuracywhenN=10and14%when
applicationinhealthcare34. Thiswasaccomplishedthrough N=15,andalsoincreasingtheamountoftime(N>20)reduces
theeffortofmedicalandtechnicalprofessionalsstress-testing theaccuracyby4%whenN=25and9%whenN=30. Finally,
LLMsonclinicallyrelevantscenarios. Similarworkdesigned we show GPT-4V is able to get around 27% accuracy on a
a new benchmark, EquityMedQA, using new methods for multimodalsimulatedclinicalenvironmentbasedonNEJM
surfacinghealthequityharmsandbiases35. Thisworkdemon- casechallenges.
stratestheimportanceofusingdiverseassessmentmethods Onelimitationfortheevaluationspresentedinthisbench-
andinvolvingratersofvaryingbackgroundsandexpertisefor markis thatitiscurrently unknownwhat datawasusedto
understandingbiasinLLMevaluations. trainGPT-4andGPT-3.5. Whilepreviousworkshavecited
Previousworkhasmadeprogressinthedirectionofclini- GPT-4saccuracyasavalidmeasure3,13,17,itisentirelypos-
caldecisionmakingusingsimulationsofpatientsanddoctors, siblethatGPT-4/3.5couldhavebeentrainedontheMedQA
aimingtodevelopAIthatcandiagnosethroughconversation. testsetgivingitanunfairadvantageonthetask. Currently,
Thismodel,titledAMIE(ArticulateMedicalIntelligenceEx- Mixtral-8x7B36andLlama2-70B-Chat37donotreporttrain-
plorer)26, demonstrates improved diagnostic accuracy and ingontheMedQAtestortrainset. Additionally,ourresults
performance on 28 of the 32 proposed axes from the per- onvaryingthepatientLLMsuggestthattheirmaybeanad-
spectiveofspecialistphysiciansand24of26axesfromthe vantageforLLMswhichactasboththepatientandthedoctor
perspectiveofpatientactors. Whiletheseresultsareexciting agent,becauseLLMsareabletorecognizetheirowntextwith
formedicalAI,thisworkremainsclosed-sourceandisnot highaccuracy,anddisplaydisproportionatepreferencetothat
accessibleforreproducibilityorfurtherstudies. Additionally, text24.
thisworkfocusedonlyondiagnosingpatientsthroughhistory- Ourworkonlypresentsasimplifiedclinicalenvironments
taking,anddidnotincludetheabilitytomakedecisionsabout that include agents representing a patient, doctor, measure-
10/15ments, and a moderator. However, in future work we will Acknowledgements
consider including additional critical actors such as nurses,
ThismaterialisbaseduponworksupportedbytheNational
the relatives of patients, administrators, and insurance con-
Science Foundation Graduate Research Fellowship under
tacts. Theremaybeadditionaladvantagestocreatingagents
GrantNo. DGE2139757,awardedtoSSandCH.
that are embodied in a simulated world like in ref.38,39, so
thatphysicalconstraintscanbeconsidered,suchasmaking
References
decisionswithlimitedhospitalspace.
Focusingonimprovingtherealismofthepatient-doctor 1. Thirunavukarasu,A.J.etal. Largelanguagemodelsin
interactionsimulationsbygroundingtheagentswithrealdi- medicine. Nat.medicine1–11(2023).
aloguecouldprovideanincreasedreflectionofrealclinical 2. Gu,Y.etal. Domain-specificlanguagemodelpretrain-
settings,usingdatasetssuchasMedDialogue40orfromactual
ing for biomedical natural language processing. ACM
OSCEs41. Abroaderrangeofmedicalconditionscouldbe TransactionsonComput.forHealthc.(HEALTH)3,1–23
incorporated,increasingthereliabilityofthebenchmarkmet- (2021).
ricwithrarediseasesandacrossvariousmedicalspecialties.
3. Nori, H. et al. Can generalist foundation models out-
Furtherrefinementofthemeasurementagentcouldintroduce
competespecial-purposetuning? casestudyinmedicine.
awidervarietyofmedicaltestsandmodalities(e.g. sound
arXivpreprintarXiv:2311.16452(2023).
orfullpatientobservation). Therecouldalsobeincorporated
a"cost"associatedwithrunningparticulartests,andthede- 4. Liévin, V., Hother, C. E., Motzfeldt, A. G. & Winther,
cisions that doctor agents make with limited resources and O. Can large language models reason about medical
timecouldfurtherincreasetherealismofthiswork. Particular questions? Patterns(2023).
doctoragentscouldbeoptimizedforcertainmedicalsettings 5. Organization, W. H. et al. Health workforce require-
(high-resourcevslow-resourcehospitals). mentsforuniversalhealthcoverageandthesustainable
Linkingsimulatedagentbenchmarkstoreal-worldpatient developmentgoals. WorldHeal.Organ.(2016).
datasets,e.g.,bymeansofusingtheformertostudythelatter 6. McIntyre,D.&Chow,C.K. Waitingtimeasanindicator
willbeanexcitingrouteforfuturework. Itwillbeexcitingto forhealthservicesunderstrain: anarrativereview. IN-
furtherdeciphertowhichdegree“aligned”LLMscomplywith QUIRY:TheJ.Heal.CareOrgan.Provision,Financing
biasinstructionstoaugmentcurrentred-teamingeffortswith 57,0046958020910305(2020).
agent-basedsimulations. Furthermore,weenvisionexploring
7. Singhal,K.etal. Largelanguagemodelsencodeclinical
biasesbeyondthosetraditionallyrecognizedinmedicalprac-
knowledge. Nature620,172–180(2023).
tice,toincludebiasesrelatedtohealthcaresystemfactorsand
patient-doctorcommunicationstyles20. Thegoalwouldbe 8. Vaid, A., Landi, I., Nadkarni, G. & Nabeel, I. Using
todevelopmitigationstrategies,ashasbeenshowninprior fine-tunedlargelanguagemodelstoparseclinicalnotes
work17,whichcanbeintegratedintothelanguagemodelsto inmusculoskeletalpaindisorders.TheLancetDigit.Heal.
reducetheimpactofthesebiasesondiagnosticaccuracy. 5,e855–e858(2023).
While the primary aim of the benchmark is to develop 9. Zakka,C.etal. Almanac—retrieval-augmentedlanguage
moresophisticateddecisionmakingmodels,eachofthediffer- modelsforclinicalmedicine. NEJMAI1,AIoa2300068
entlanguageagents(patient,measurement,moderator,doctor) (2024).
that we present are able to be modified in our open-source 10. Xiong, G., Jin, Q., Lu, Z. & Zhang, A. Benchmark-
code. Thisallowsforfurtherstudiestobeperformedondif- ingretrieval-augmentedgenerationformedicine. arXiv
ferentcomponentsofthesystem,andperhapseventofurther preprintarXiv:2402.13178(2024).
complicatetheworkflow,suchasaddingadditionalpatients
11. Liévin, V., Hother, C.E.&Winther, O. Canlargelan-
ordoctors,orprovidinginaccuraciestothetestresults. We
guage models reason about medical questions? arXiv
additionally provide a simple workflow for adding custom
preprintarXiv:2207.08143(2022).
scenariosthroughourexaminationtemplate, aswellasthe
abilitytodesigncompletelynewtemplatesandnewagents. 12. Wu,C.etal. Pmc-llama: Towardsbuildingopen-source
languagemodelsformedicine(2023). 2304.14454.
Overall,webelievethatlanguagemodelsneedtobecriti-
callyexaminedwithnovelevaluationstrategiesthatgowell 13. Chen, Z. et al. Meditron-70b: Scaling medical pre-
beyond static question-answering benchmarks. With this training for large language models. arXiv preprint
work,wetakeasteptowardsbuildingmoreinteractive,oper- arXiv:2311.16079(2023).
ationalized,anddialogue-drivenbenchmarksthatscrutinize 14. Ely,J.W.etal. Analysisofquestionsaskedbyfamily
thesequentialdecisionmakingabilityoflanguageagentsin doctorsregardingpatientcare. Bmj319,358–361(1999).
variouschalleningandmultimodalclinicalsettings.
15. Omiye,J.A.,Lester,J.C.,Spichak,S.,Rotemberg,V.&
Daneshjou, R. Large language models propagate race-
basedmedicine. NPJDigit.Medicine6,195(2023).
11/1516. Ziaei,R.&Schmidgall,S. Languagemodelsaresuscep- 32. Hendrycks,D.etal. Measuringmassivemultitasklan-
tibletoincorrectpatientself-diagnosisinmedicalappli- guageunderstanding. arXivpreprintarXiv:2009.03300
cations. InDeepGenerativeModelsforHealthWorkshop (2020).
NeurIPS2023(2023).
33. Gramopadhye,O.etal. Fewshotchain-of-thoughtdriven
17. Schmidgall,S.etal. Addressingcognitivebiasinmedi- reasoningtopromptllmsforopenendedmedicalquestion
callanguagemodels. arXivpreprintarXiv:2402.08113 answering. arXivpreprintarXiv:2403.04890(2024).
(2024).
34. Chang,C.T.-T.etal. Redteaminglargelanguagemod-
18. Blumenthal-Barby,J.S.&Krieger,H. Cognitivebiases elsinmedicine: Real-worldinsightsonmodelbehavior.
andheuristicsinmedicaldecisionmaking: acriticalre- medRxiv2024–04(2024).
viewusingasystematicsearchstrategy. Med.Decis.Mak.
35. Pfohl,S.R.etal. Atoolboxforsurfacinghealthequity
35,539–557(2015).
harmsandbiasesinlargelanguagemodels.arXivpreprint
19. Hammond,M.E.H.,Stehlik,J.,Drakos,S.G.&Kfoury, arXiv:2403.12025(2024).
A.G. Biasinmedicine: lessonslearnedandmitigation
36. Jiang, A. Q. et al. Mixtral of experts. arXiv preprint
strategies. BasictoTransl.Sci.6,78–85(2021).
arXiv:2401.04088(2024).
20. FitzGerald, C. & Hurst, S. Implicit bias in healthcare
37. Touvron, H. et al. Llama: Open and efficient founda-
professionals: asystematicreview. BMCmedicalethics
tionlanguagemodels. arXivpreprintarXiv:2302.13971
18,1–18(2017).
(2023).
21. Gopal, D. P., Chetty, U., O’Donnell, P., Gajria, C. &
38. Park,J.S.etal. Generativeagents: Interactivesimulacra
Blackadder-Weinstein,J. Implicitbiasinhealthcare: clin-
ofhumanbehavior. InProceedingsofthe36thAnnual
icalpractice,researchanddecisionmaking.Futur.health-
ACMSymposiumonUserInterfaceSoftwareandTech-
carejournal8,40(2021).
nology,1–22(2023).
22. Sabin,J.A. Tacklingimplicitbiasinhealthcare. New
39. Li, J. et al. Agent hospital: A simulacrum of hos-
Engl.J.Medicine387,105–107(2022).
pital with evolvable medical agents. arXiv preprint
23. Lombardi,C.V.,Chidiac,N.T.,Record,B.C.&Laukka, arXiv:2405.02957(2024).
J.J. Usmlestep1andstep2ckasindicatorsofresident
40. Chen,S.etal. Meddialog:alarge-scalemedicaldialogue
performance. BMCMed.Educ.23,543(2023).
dataset. arXivpreprintarXiv:2004.03329(2020).
24. Panickssery,A.,Bowman,S.R.&Feng,S. Llmevalu-
41. Fareez,F.etal. Adatasetofsimulatedpatient-physician
atorsrecognizeandfavortheirowngenerations(2024).
medicalinterviewswithafocusonrespiratorycases. Sci.
2404.13076.
Data9,313(2022).
25. Smith-Bindman, R. et al. Use of diagnostic imaging
42. OpenAIetal. Gpt-4technicalreport(2023). 2303.08774.
studies and associated radiation exposure for patients
enrolled in large integrated health care systems, 1996- 43. Brown,T.etal. Languagemodelsarefew-shotlearners.
2010. Jama307,2400–2409(2012). Adv. neural information processing systems 33, 1877–
1901(2020).
26. Tu,T.etal. Towardsconversationaldiagnosticai. arXiv
preprintarXiv:2401.05654(2024). 44. Christiano,P.F.etal. Deepreinforcementlearningfrom
humanpreferences. Adv.neuralinformationprocessing
27. Melnick,D.E.,Dillon,G.F.&Swanson,D.B. Medical
systems30(2017).
licensing examinations in the united states. J. dental
education66,595–599(2002).
28. Zayyan, M. Objective structured clinical examination:
theassessmentofchoice. Omanmedicaljournal26,219
(2011).
29. Jin, D. et al. What disease does this patient have? a
large-scaleopendomainquestionansweringdatasetfrom
medicalexams. Appl.Sci.11,6421(2021).
30. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. & Lu, X.
Pubmedqa: Adatasetforbiomedicalresearchquestion
answering. arXivpreprintarXiv:1909.06146(2019).
31. Pal,A.,Umapathi,L.K.&Sankarasubbu,M. Medmcqa:
Alarge-scalemulti-subjectmulti-choicedatasetformedi-
caldomainquestionanswering. InConferenceonhealth,
inference,andlearning,248–260(PMLR,2022).
12/15A Model details
Weevaluatefourlanguagemodelstoserveasthedoctoragent(thediagnosticmodel): GPT-3.5,GPT-442,Mixtral-8x7B36,and
Llama270B-chat37. Otherwise,forthepatient,measurement,andmoderatoragentweuseGPT-4. Briefly,wediscussthe
detailsofeachmodelbelowstartingwithlanguagemodelsfollowedbycommonlanguagemodels.
GPT-4 & GPT-3.5: GPT-4 (gpt-4-0613) is a large-scale, multimodal LLM which is able to process both image and text
inputs. GPT-3.5(gpt-3.5-turbo-0613)isasubclassofGPT-3(a170Bparametermodel)43 fine-tunedonadditionaltokens
andwithhumanfeedback44. Currently,thedetailsregardingthearchitecture,dataset,andtrainingmethodologiesofGPT-3.5
andGPT-4havenotbeennotpubliclydisclosed. However,existingtechnicaldocumentationindicatesthatbothmodelsare
high-performing in medical and biological subjects, with GPT-4 showing superior performance compared to GPT-3.5 in
knowledgeassessments3,42.
Mixtral-8x7B: Mixtral8x7BisalanguagemodelthatemploysaSparseMixtureofExperts(SMoE)architecture36. This
architecturediffersfrommanyothermodelsinthatitfeaturesaseriesofeightfeedforwardblocks(or"experts")ateachlayer.
Aroutingmechanismateachlayerselectstwoexpertsforprocessingtheinput,andtheiroutputsaresubsequentlymerged.
Thisselectionprocessallowsfor13Bofthetotal47Bparameterstobeengagedpertoken,contingentuponthespecificcontext
andrequirements. Themodeliscapableofhandlingupto32,000tokensinitscontextsize,whichhasdemonstrateditsability
toeithersurpassorequaltheperformanceofothermodelslikellama-2-70Bandgpt-3.5acrossarangeofbenchmarks.
Llama270B-Chat: Llamaisanopen-accessmodeldevelopedbyMeta,whichwastrainedon2trilliontokensfrompublicly
availabledata37. Themodelcomesinvarioussizes,withparametersrangingfrom7billionto70billion. Theselectionofthe
70billionchatmodelwasbasedonitssuperiorperformanceacrossarangeofmetrics. Significanteffortsweremadetoalign
thetrainingprocesswithestablishedsafetymetrics,leadingtoimprovementsinhowthemodelhandlesadversarialprompting
inspecified"riskcategories."Notably,thisincludesthemodel’sresponsetorequestsforadvicethatitmaynotbequalifiedto
provide,suchasmedicaladvice,whichisrelevanttothecontextofthiswork.
B Bias prompts
Inourpromptswithbias,weincludeaninstructionssectioninthepatient/doctorinstructionswhichaimtochangetheirbehavior
tobemorebiased. Anexamplepromptforthepatientfollowsthefollowingform:
You are a patient in a clinic who only responds in the form of dialogue. You are being inspected by a doctor
who will ask you questions and will perform exams on you in order to understand your disease. Your answer will
only be 1-3 sentences in length.
You recently performed a thorough investigation of your symptoms online and have come to the conclusion
that you should be diagnosed with cancer. While the doctor may believe otherwise you remain convinced you
have cancer
Below is all of your information. [Information here]. Remember, you must not reveal your disease explicitly
but may only convey the symptoms you have in the form of dialogue if you are asked.
Wherethe[Information here]boxcontainsastructuredlistofpatientinformation(seeAppendixSectionXYZfor
moreinformation). Whilewedonotprovidethepatientwithexplicitinformationabouttheirdiseasewestillpromptthepatient
thattheymaynotexplicitlysharetheirdiseasetopreventthepatientagentbeingoverlycompliantaboutguessingtheirdisease
(sincetheyhavemoreinformationaboutthemselvesthanthedoctor).
C OSCE Examination Structure
ObjectiveforDoctor
Stringdescribingtheevaluationanddiagnosisobjectiveforthedoctor.
PatientActor
Demographics Stringcontainingage,gender,andpotentiallyotherdemographicinformation.
History Stringdetailingthepatient’sreportedhistoryrelevanttothecurrentmedicalconcern.
Symptoms PrimarySymptom Stringdescribingthemainsymptom(s).
SecondarySymptoms ArrayofStringslistingadditionalsymptoms.
PastMedicalHistory Stringsummarizingthepatient’spastmedicalissuesandongoingtreatments.
13/15SocialHistory Stringoutliningthepatient’slifestyleandhabitsimpactinghealth.
ReviewofSystems Stringprovidingabriefoverviewofsystemsreview,ifapplicable.
PhysicalExaminationFindings
VitalSigns Temperature String
BloodPressure String
HeartRate String
RespiratoryRate String
... (more)
CardiovascularExamination Inspection String
Auscultation String
... (more)
PulmonaryExamination Inspection String
Palpation String
... (more)
... (moreexaminations)
TestResults
Electrocardiogram,ChestX-Ray,etc. Eachtesthas:
Findings Stringsummarizingthetestresults.
... (more)
... (moretests)
CorrectDiagnosis
Stringindicatingthediagnosisbasedontheaboveinformation.
D Example OSCE Case Study
Objectivefordoctor
Evaluateanddiagnosethepatientpresentingwithchestpainandshortnessofbreath.
PatientActor
Demographics 45-year-oldmale
History The patient reports a sudden onset of chest pain and shortness of breath that started while he was
walkinghisdogthismorning. Describesthepainasatightnessacrossthechest. Notesthatthepain
somewhatimproveswhensittingdown.
Symptoms • PrimarySymptom: Chestpainandshortnessofbreath
• SecondarySymptoms:
– Painimprovesuponsitting
– Nocough
– Nofever
PastMedicalHistory Hypertension,hyperlipidemia. Takeslisinoprilandatorvastatin.
SocialHistory Smokeshalfapackofcigarettesdailyforthepast20years,drinksalcoholsocially.
ReviewofSystems Deniesrecentillnesses,cough,fever,legswelling,orpalpitations.
14/15PhysicalExaminationFindings
VitalSigns Temperature 36.8°C(98°F)
BloodPressure 145/90mmHg
HeartRate 102bpm
RespiratoryRate 20breaths/min
CardiovascularExamination Inspection Nojugularvenousdistention
Auscultation Regularrateandrhythm,nomurmursorextraheartsounds. Norubsheard.
PulmonaryExamination Inspection Chestwallsymmetrical
Auscultation Clearlungfieldsbilaterally,nowheezes,crackles,orrhonchi
Palpation Nochestwalltenderness
TestResults
Electrocardiogram Findings Normalsinusrhythm,noSTelevationsordepressions,noTwaveinversions
ChestX-Ray Findings Nolunginfiltrates,normalcardiacsilhouette,nopneumothorax
BloodTests Troponin Normal
D-dimer Elevated
CTPulmonaryAngiogram Findings Acutesegmentalpulmonaryembolismintherightlowerlobe
CorrectDiagnosis PulmonaryEmbolism
15/15