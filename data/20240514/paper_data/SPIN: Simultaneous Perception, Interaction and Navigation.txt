SPIN: Simultaneous Perception, Interaction and Navigation
ShagunUppal AnanyeAgarwal HaoyuXiong KennethShaw DeepakPathak
CarnegieMellonUniversity
Figure1. LearningtoSPIN:Ourrobotlearnstosimultaneouslyperceive,manipulate,andnavigateclutteredunstructuredenvironments
inawhole-bodyfashion. Therobothasanactuatedcamerawithalimitedfieldofviewthatitmustcontroltogetinformationaboutits
environment.Themotionandperceptionproblemaretightlycoupledsincewhattherobotknowsabouttheenvironmentinfluenceshowit
canmoveandviceversa.Weshowresultsinalargevarietyofscenariosbothindoorsandoutdoorswithdifferentobstacleslikeboxesand
furniture.Ourrobotcanpickupdifferentobjectslikecups,andutensils.Videodemosathttps://spin-robot.github.io
Abstract poundingerrors,delaysindecision-making,andnowhole-
bodycoordination. Inthiswork,wepresentareactivemo-
Whiletherehasbeenremarkableprogressrecentlyinthe
bilemanipulationframeworkthatusesanactivevisualsys-
fieldsofmanipulationandlocomotion,mobilemanipulation
tem to consciously perceive and react to its environment.
remains a long-standing challenge. Compared to locomo-
Similartohowhumansleveragewhole-bodyandhand-eye
tion or static manipulation, a mobile system must make a
coordination, we develop a mobile manipulator that ex-
diverserangeoflong-horizontasksfeasibleinunstructured
ploitsitsabilitytomoveandsee,morespecifically–tomove
and dynamic environments. While the applications are
in order to see and to see in order to move. This allows it
broadandinteresting,thereareaplethoraofchallengesin
to not only move around and interact with its environment
developingthesesystemssuchascoordinationbetweenthe
butalso,choose“when”toperceive“what”usinganactive
baseandarm,relianceononboardperceptionforperceiv-
visualsystem. Weobservethatsuchanagentlearnstonav-
ingandinteractingwiththeenvironment, andmostimpor-
igate around complex cluttered scenarios while displaying
tantly, simultaneously integrating all these parts together.
agile whole-body coordination using only ego-vision with-
Priorworksapproachtheproblemusingdisentangledmod-
outneedingtocreateenvironmentmaps. Videosareavail-
ular skills for mobility and manipulation that are trivially
ableathttps://spin-robot.github.io
tiedtogether. Thiscausesseverallimitationssuchascom-
4202
yaM
31
]OR.sc[
1v19970.5042:viXra1.Introduction
ConsidertheexampleshowninFigure2. Apersonistrying
tocarryacoffeecupthroughclutter. Thisnotonlyrequires
navigationalplanningfromstarttogoalbutplanningofthe
wholebodytoavoidobstaclesalongtheway. Furthermore,
duetoego-centricvision,thepersonneedstoactivelylook
around to gather the presence of obstacles. This general
formofmobilemanipulationtasknecessitatesacoupledun-
derstanding of whole-body control with active perception.
Thiscapabilityisoneofthefundamentalandfrequentlyen-
counteredtasksinembodiedcognition. Figure2. Humanandrobotillustrationofwhole-bodynavigation
throughtheclutter.
Thedominantparadigmtotacklethisproblemisthrough
classical planning-based control which requires apriori
knowledge about the precise location of all the obstacles robotdidnotseeduringtrainingtime.
along with a detailed map of the environment. In most Ourapproachpresentsaradicalhypothesisthatthetradi-
real-world scenarios, this assumption is impractical due to tionallynon-reactiveplanningapproachtowhole-bodycon-
computationalreasons,butmoreimportantly,becauseenvi- trolcanindeedbecastintoareactivemodel–i.e. –single
ronments are dynamic and objects keep moving around in end-to-end policy trained by RL. Despite a big departure
general. Furthermore, relying on precise measurement of fromoptimalcontrolliterature,thishypothesisisnotassur-
scenes for control does not allow agents to reactively im- prisingsinceagilewhole-bodycoordinationandfastobsta-
provise to changes in their environment. Practically, even cleavoidanceinhumansaredevelopedintomusclememory
whenthecompleteenvironmentmapisknownapriori,joint overtime. Wenowdiscussourapproachindetail.
planning for a system with high degrees of freedom, say a
mobilebasewithanarm,isoftenintractableandtooexpen- 2.Method
sivetobedeployedinreal-time.
Humans,ontheotherhand,donotrelyonpreciseknown We want our mobile manipulator (Fig. 5) to navigate and
estimatesofobjectlocationsandinsteaduseego-centricvi- manipulateobjectswhileavoidingobstaclesincluttereden-
sion to navigate around obstacles in real-time. In an un- vironments. Itsharesanatomicalsimilaritieswithahuman,
familiar environment, where to look is informed by where bringingwithitmanyofthesamechallenges. First,ithasa
they want to move (called ‘active perception’), and how limbintheformofanarmthatcanberaisedandlowered,
they move in return determines what all they can see im- sotherobotmustconstantlymovethearmtoavoidanyob-
mediately afterward. This integrated mobility and percep- stacles. Second,ithasanactuatedcamerawithaverylim-
tionallowsustosee,adapt,andreacttomaneuverthrough itedfieldofview(87◦ horizontal,58◦ vertical),soitneeds
unseenheavilyclutteredenvironments. toconstantlylookaroundtosimultaneouslyplanaheadand
This paper presents SPIN, an end-to-end approach to lookoutforunexpectedobstacles. Imagineyourselfwalk-
SimultaneousPerception,Interaction,andNavigation. We ing through a cluttered cabinet, there are too many obsta-
trainasinglemodelthatnotonlyoutputslow-levelcontrols clesaroundtokeeptrackof, andyoucan’tseeallofthem
for the robot body and arm but also predicts where should at once, so you must keep looking all around your body
therobot’sego-centriccameralookateachtimestepwhile to plan a path through the clutter but also make sure you
movingitswholebodybyavoidingobstacles. Wetrainour don’thitanythingyoumissedalongtheway.Unlikeregular
approachviareinforcementlearning(RL),andtogetaround walkingwhereoureyesmostlypointstraightaheadandthe
thecomputationalbottleneckofrenderingdepthimages,we pathisclear,hereyoumustactivelychoosewhattoperceive
use a teacher-student training framework where robot be- for simultaneously planning ahead and also doing reactive
haviorisfirstlearnedusingRLwithaccesstovisibleobject fixestoyourplannedpath. Sincealltheobstaclescannotbe
scandotsandthendistilledintoapolicythatoperatesfrom perceived at a single glance, you must have spatial aware-
ego-depthusingsupervisedlearning. Weevaluateacross6 ness and know where the obstacle you saw some time ago
benchmarksinsimulationrangingfromeasy,medium,and is right now in relation to your body. Note that this entire
harddifficulty,andtworeal-worldenvironmentswithasim- processisverydifferentfromtheclassicalapproach,where
ilar level of clutter as the hard environments in simulation perception, planning, and obstacle avoidance are separate
and also add dynamic, adversarial obstacles. We find that processes executed separately and in sequence. Further, it
our method outperforms classical methods and baselines isassumedthattheoutputofeachisperfect,whereasthisis
whichdonotuseactivevision. Wealsoobservedemergent rarelythecaseinpracticeinourunstructuredworld.
behaviors,includingdynamicobstacleavoidancewhichthe Todealwiththischallenging, entangledproblemsetup,Coupled visuomotor optimization (CVO)
Phase 1: Learning to see and move Phase 2: Supervised Learning
L2 Regression
LSTM LSTM
Proprioception Proprioception
arobot ârobot
π π
Visible acamera âcamera
Scandots
zt ẑt
Pointnet Depth image CNN
Pre-initialized weights
Decoupled visuomotor optimization (DVO)
Phase 1a: Learning to move Phase 1b: Learning to see
Pre-initialized weights
LSTM LSTM
Proprioception Proprioception
π
robot
árobot
π
robot
arobot
Privileged Visible LSTM
Scandots
zt
Scandots
źt
π
camera
ácamera
Pointnet Pointnet
Phase 2: Supervised Learning (same as above)
Figure3. Welearnapolicythatusesego-visiontosimultaneouslyperceive,interact,andnavigateinclutteredenvironments. Wepropose
two methods: (1) Coupled Visuomotor Optimization (CVO) learns robot and camera actions at the same time. We train an RL policy
topredictthese. Weonlyprovidescandotsiftheyarevisibleintheagent’sfield-of-viewallowingtheagenttolearntomoveitscamera
andaggregateinformationaboutitsenvironment. Thisisfollowedbyaphase-2supervisedtrainingwherethisbehaviorisdistilledinto
astudentnetworkthatoperateswithego-centricdepthimages(2)DecoupledVisuomotorOptimization(DVO)decouplestheactionand
perceptionlearningintotwoparts: firsttheagentlearnstonavigateacrossclutterassumingaccesstoallobstacles. Inphase1b,therobot
learnstomoveitscameratoestimatetherelevantinformation.Thisisfollowedbysupervisedlearningsameasabove.
wetakeadata-drivenapproach. Wetrainourrobottonavi- tainsthesameinformationandischeaptocompute. Wedo
gateinsideprocedurallygeneratedclutterinsimulationus- so using scandots s which are the xyz coordinates of the
t
ingRL.Therobotisonlyallowedtoperceivethepartofits boundingboxofeachobstacle. Tospecifywhichobjectto
environment that is visible to the camera and learns to co- pick, we give the initial location of the object (before it is
ordinate its arm, base, and camera motion such that it can touched by the robot) o . In lieu of the object image, we
i
planaheadandreactivelyadjusttoobstacles. give the current location of the object o . Here, scandots
t
In practice, since training with RL requires many sam- s and object location o are privileged information which
t t
ples and rendering depth is inefficient (see supp. Section must later be estimated from depth images. Given this in-
10),wedividetrainingintotwophases. Inthefirstone,we formation, we train two separate LSTM policies π and
nav
learnmobilemanipulationbehaviorsviaRLusingacheap- π . Attesttime,thenavpolicyisactivatedtoreachatar-
pick
to-computevariantofdepthandinphase2wetrainaCNN get location and we switch to the pick one once the robot
forperceptionfromdepthimagesasillustratedinFigure3. getsclosetotheobject.
2.1.1 PickPolicy
2.1. Phase 1 - Learning Simultaneous Perception,
InteractionandNavigation Thisaccessesproprioceptionx consistingofrobotjointan-
t
gles and velocities q ,q˙ , base linear and angular velocity
Inthisstage,weuseRLtolearntocontrolallthejointsof t t
v ,ω . Forperception,itgetstheobject’sinitialandcurrent
the robot to navigate clutter and pick target objects. Since t t
locationo ,o andpredictsrobotandcameraactions.
rendering depth images directly from the robot camera is i t
expensive, we must instead use an ersatz version that con- [a ,a ]=π (x ,F(o ,x ),o ) (1)
robot cam pick t t t iFigure 4. We illustrate one scenario of the simulation benchmark here with many obstacles in a narrow passage. The agent learns to
developwhole-bodycoordinationsuchastherobot’sarmmovementinthelasttwoframes,toreactivelyadaptandnavigatethroughsuch
clutteredscenesbyactivelymovingaroundthecameraandaggregatinginformationforefficientnavigationwithoutcollisions.
where F is a masking function that masks object position cameraandrobotmotionswithaccesstoonlyvisiblescan-
o if it is not in the field of view of the camera. This is dots ˆz = P(F(s ,x )). This policy is trained via RL to
t t t t
required since object position can only be estimated from predict the robot actions from phase 1 policy a . This
robot
depthinphase2ifitisvisible. optimization forces the student policy to learn camera be-
haviorsthatcaptureinformationabouttheenvironmentthat
2.1.2 NavigationPolicy is needed to move optimally. We initialize π n1 ab v from the
weightsofπ1a
nav
Trainingthispolicyrequiresacomplexjointvisuomotorop-
timizationsincerobotmotionisdependentonitsknowledge min ∥aˆ −a ∥
robot robot
π1b
oftheenvironmentwhichinturndependsonhowtherobot nav
moves. Wepresenttwoapproachestotacklethisproblem. s.t. [ˆa robot,ˆa cam]=π n1 ab v(x t,ˆz t,g t) (4)
Coupled Visuomotor Optimization (CVO) Here, we
Thisdecoupledapproachlearnstomoveandseeinseparate
setupapartiallyobservableenvironmentfortherobotand
phases which eases the optimization burden. In principle,
let the RL algorithm do the joint optimization using large-
the coupled optimization is better since it is possible that
scale data. In particular, the policy gets proprioception x
t
the1apolicymaylearntoexploitprivilegedinformationin
and only visible scandots ˜s = F(s ,x ) as observation
t t t
a way that the 1b policy cannot estimate it for any set of
and has to predict both the camera and the robot actions.
camera movements. However, in our setting, this did not
Sincethescandotsarepermutationinvariant,wepassthem
turnouttobethecase.
throughatrainablepoint-netarchitectureP toobtaincom-
We train using PPO [34] with backpropagation through
pressedlatentz =P(˜s )thatwepasstothepolicy
t t
time[39]inprocedurallygeneratedenvironments.
[a ,a ]=π (x ,z ) (2) Rewards: For the navigation task, we use distance to
robot cam nav t t
goal reward ∥g ∥ along with a forward progress reward
t
This presents a tough optimization landscape because the |(v t) g| where (v t)
g
is velocity along the direction of the
observations at each step are strongly dependent on a . goal.
cam
Forinstance,ifthecameraswivelsaroundtheobservations r
nav
=0.1·∥g t∥+0.1·|(v t) g| (5)
atthenexttimestepmaylookcompletelydifferent. Indeed,
For the pick task, we provide an object reaching reward,
we observe that this requires billions of samples inside a
i.e., the distance between the gripper and the object. This
GPU-accelerated simulator to optimize which may not al-
isfollowedbyaliftrewardifasuccessfulgraspisdetected
waysbefeasibleinpractice.
(basedonwhethercontactforcescrossathreshold).
Decoupled Visuomotor Optimization (DVO) To ease
theoptimizationprocess,welearntherobotandcameraac- r =0.5·∥o −p ∥+0.5·r (6)
pick t t lift
tionsseparately. First,welearnhowtomovebygivingthe
robotaccesstoallavailablescandotsz = P(s )inalocal where
t t
vicinity.Sincetherobotseeseverything,thecameramotion (cid:34) (cid:35)
isirrelevantandwejustpredicttherobot’smotion r =(cid:0) 1−tanh(cid:0) 15·[(o ) ] (cid:1)(cid:1)I (cid:88) f >10 (7)
lift t z + i
i
a =π1a (x ,z ,g ) (3)
robot nav t t t
where [x] = max(x,0) and I is the indicator function
+
whereg isthegoalwithrespecttothebase.Usingthispol- whichforcestherewardtobeactiveonlywhenobjectcon-
t
icy as supervision, we train another policy to predict both tactforcesf exceed10N.
iTrainingenvironments:Weprocedurallygeneratelong Arm Extend
Wrist roll,
corridorswithobstaclesplacedinbetweentherobotandthe pitch, yaw Camera
pan, tilt
goal. Theinitialjointsandorientationoftherobotareran-
domized. Near the edges of the corridors, we place ran- Gripper
domized obstacles and walls to simulate distractors in the
Arm Lift
depthimage. Forthepicktask,objectsarespawnedonta-
Visible scandots in
blesofvaryingdimensions. Weusefivedifferentobjects- camera’s field-of-view
a banana, mug, can, foambrick, and a bottle. The episode
isterminatediftherobotreachesthegoalorhitsanobsta- Base
cle/table.
Figure5. (Left)Wecomputevisiblescandotsbyprojectingthem
2.2.Phase2-FromScandotstoDepth
tothecameraframeandcheckingiftheyliewithintheimageplane
(Right)ofthestretchRE1robotthatweuseexperiments. Ithas
Scandots are not directly observable in the real world and
twoDoFsinthebase,oneeachforarmliftandextension,twofor
must instead be estimated from the depth image. We train
thecamera,threeforthewrist,andoneforthegripper.
aconvolutionnetworkC toconvertrendereddepthimages
d toperceptionlatents˜z . Thislatentispassedtoastudent
t t • Classical: Thisusesaclassicalstacktocontrolthebase
policyπ′ topredicttheactions[˜a ,˜a ]. Thisissuper-
robot cam motion. We first teleoperate the robot for 3-5 minutes
visedusingL2lossfromthephase1actions. Theweights
to construct a map using the onboard 2D RPLidar using
for π′ are initialized using π. We train this policy using
gmapping.Next,move baseisusedtoplanapaththrough
DAgger[31]. Forthenavigationpolicy,weoptimize
theenvironment. Finally,wemovetherobottothestart,
min ∥π′ (C (d ),x ,g )−π (z ,x ,g )∥ (8) useaMonteCarlomethod[22]tolocalize,andthenexe-
nav nav t t t nav t t t
Cnav,π n′ av cutetheplan.Notethatthisbaselinegetsaneasierversion
Notethattheteacherpolicyπ canbetrainedusingeither oftheproblemsinceitassumesthatthemapisknownin
nav
thecoupledordecoupledapproach. Similarly,forthepick advanceanddoesnotconsiderarmmotionduetothe2D
policy,weestimatecurrentobjectpositiono fromdepth Lidar. Thisisusedtotestwhetherreactivenavigationis
t
superiortoplanning.
min (cid:13) (cid:13)π p′ ick(C pick(d t),x t,o i)−π pick(z t,x t,o t,o i)(cid:13) (cid:13) • NoPointNet: Insteadofpassingobjectscandotsthrough
Cpick,π p′
ick a permutation-invariant PointNet architecture, we con-
(9)
catenatethemanduseanMLPtoestimatealatent.
3.ExperimentalSetup
4.ResultsandAnalysis
WeusetheHelloRobotStretch[1]forallourexperiments
(Fig. 5). The robot has 10 actuated joints which include 2 Weevaluateourapproachbothinsimulationaswellasreal-
degreesoffreedomforthecamera, 2forbaserotationand world. Since doing a lot of in-the-wild real-world experi-
translation, 2 for the arm, 1 for the gripper fingers, and 3 mentsismoretime-consumingandcumbersomeduetovar-
for the dexterous wrist. An Intel D435i depth camera is iouspracticalreasons,wethoroughlyevaluateourapproach
mountedonthetopoftherobotheadwhichisactuatedus- on 6 simulation benchmarks with multiple scenarios. We
ingtwomotors.Thelearnedpolicyoperatesat10Hzandwe explaineachofthesebenchmarksindetailinSection4.3.
do velocity control for the robot base and position control While simulation benchmarks are useful for fair com-
foralltheotherjoints.Velocitycontrolfortherobotbaseal- parisonwithbaselinesaswellasreproducibility,real-world
lowsustoperformsimultaneousrobottranslationandrota- experimenting is essential for determining the efficacy of
tionformoreagilebehavior. WetrainusingIsaacGymEnvs oursystemintrulyunstructuredanddynamicenvironments.
[28]using8192environmentswhichtakes6hoursoftrain- Forthis,wetestoursystemonvariousreal-worldenviron-
ingforphase1and10hoursoftrainingtimeforphase2on mentsasshowninFigure1andbenchmarkitsperformance
aRTX3090. Wecompareagainstthefollowingbaselines: on2real-worldsetupsasdescribedinSection4.2.
• FixCam: Thecamerajointsarefrozenandthecamerais Through simulation experiments, we aim to answer the
forcedtolookforward. Thisbaselineshowswhetherac- following questions: (1) For a mobile agent, is active per-
tivevisionisusefulforthemobilemanipulationproblem ception with an actuated camera necessary, or is a fixed
andafixedviewpointisnotenough. viewpoint enough? (2) Can an active visual agent outper-
• Mapping:Insteadofusingamovingdepthcameratoget form a classical agent that relies on pre-built maps? What
a series of frames this baseline assumes exteroception is arethelimitationsofthelatter? (3)Whataresomepracti-
providedintheformofamap. Wesimulateexteroceptive calarchitecturaldesignchoicesforoptimizingmobilityand
noiseasin[2,29]. perception together? We empirically answer each of these(a)Whileoursimulationlacksdynamicobstacles,therobotcanstillevadethembecausethepolicycontinuouslyadjustsitsplan.
(b)Ifthereisanoverhangingobstacle,therobotlowersitsarmtoavoiditinsteadofturningaround,therebydisplayingagilewhole-bodycoordination.
(c)Therobotadaptsitsplanontheflytoobstaclesbyturningaroundoncethecameraseesit.
Figure6.TypesofemergentbehaviorexhibitedbySPIN(a)dynamicobstacleavoidance(b)whole-bodymovement(c)adaptivererouting.
questions in Section 4.3. Our real-world experiments pri- cedural environments seen during training. We illustrate
marilyfocusoncomparingthecapabilitiesofourreactive, three such scenarios in Figure 6. As highlighted in sev-
learnedsystemtoaclassicalmappingandthenplanningap- eral frames, Figure 7a depicts robustness to adversarially
proach. For our method, we observe interesting scenarios placeddynamicobstaclesthatconstantlyblockthepathof
demonstratingemergentbehaviorsduringreal-worldexper- therobot. Itneedstocontinuouslyperceiveitsenvironment
imentsdetailedinSection4.1and4.2. in multiple directions and quickly react to those changes.
Weobservethatincaseswhenthereisnofeasiblepathfor
4.1.EmergentBehavior
therobottonavigatethrough,italsolearnstostopandlook
Large-scale simulation pre-training allows our robot to aroundinordertoreplanitspathandavoidcollisions. Sim-
learnemergentbehaviorstoavoidobstaclesinclutteredsce- ilarly,in7bweseethatassoonasafloatingobstacleissud-
narios, even in the presence of dynamic obstacles. We see denlyplacedinfrontoftherobot,itshowsspatialawareness
several such behaviors during real-world experimentation and whole-body coordination and lowers its arm to navi-
which were neither planned nor specifically trained for in gate through, instead of turning and replanning the entire
simulationbutemergeasaresultofalargediversityofpro- basemovementwhichwouldtakemoretime. InFigure7c,Reach Pick Place
Scenario1 Scenario2 Scenario1 Scenario2
Easy Medium Hard Easy Medium Hard Easy Medium Hard Easy Medium Hard
SuccessRate:
FixCam 1.00 0.53 0.20 1.00 0.50 0.26 0.86 1.00 0.53 0.16 0.97 0.50 0.20
NoPointNet 1.00 0.87 0.57 1.00 0.77 0.63 0.93 1.00 0.83 0.57 1.00 0.77 0.60
Mapping 1.00 1.00 1.00 0.86 1.00 0.97 0.97 1.00 1.00 1.00 1.00 0.90 0.97
SPIN(DVO) 1.00 1.00 0.96 1.00 1.00 0.90 0.97 1.00 1.00 0.90 1.00 0.90 0.90
SPIN(CVO) 1.00 0.97 0.93 1.00 1.00 0.93 0.97 1.00 0.97 0.90 1.00 0.97 0.93
AverageEpisodeDuration(s):
FixCam 6.86 23.48 38.94 6.54 27.24 42.36 11.00 7.25 17.06 41.07 8.02 20.24 45.98
NoPointNet 6.30 14.87 33.25 7.22 15.04 34.09 9.25 4.88 15.22 32.00 7.89 18.49 37.42
Mapping 6.20 14.02 26.24 6.55 12.28 28.05 4.98 6.77 9.85 22.29 4.86 12.62 26.12
SPIN(DVO) 5.92 16.25 28.44 7.32 17.12 32.04 9.24 7.22 18.45 34.24 9.40 15.98 41.24
SPIN(CVO) 6.24 14.00 23.57 6.55 15.81 29.31 5.74 6.51 13.39 27.25 8.03 12.79 31.25
Table1. Weevaluatethesuccessrateon10randomenvironmentswithanaverageof3fixedseedsacrossalldifficultyscenariosbasedon
theobstaclecourse.Wereportthesuccessrateofeachpartofthetaskincludingreaching(Reach),picking(Pick),andplacing(Place)the
targetobjectinthedesiredlocation.Theplacetaskrequirestheagenttobringbacktheobjectacrosstheobstaclesnearitsstartlocation.
we see an adaptive rerouting mechanism where the agent namicscenario(Table 2)wheretheclassicalhasanearzero
changes its straight-line motion as soon as a person kicks successratewhileourmethodcansucceed. Ithastheemer-
inaboxinfrontofit. Thesebehaviorsemergeinreal-time gent ability to avoid a new obstacle in space, whereas the
andshowtheabilityofoursystemtocontinuouslyperceive, classical baseline relies on the pre-built map and fails en-
adapt,andreacttochangesinitsenvironmentwhichisvery tirely. Note that, we do not train our policy with dynamic
hardforaclassicalplanner. obstacles in simulations, but this behavior comes out as a
by-productoflotsofdiverseexperienceinsimulation. We
4.2.Real-worldresults
designtheobservationspacesuchthateverythingisrelative
We test on two real-world scenes - an academic lab and totherobot. Thisallowstheagenttoperceivetheenviron-
an open study area with couches and a kitchenette next ment as moving within its local reference frame, allowing
to it with both static as well as dynamic obstacles. Both generalizationtodynamicobstacles.
these environments have unstructured clutter and humans
4.3.Simulationresults
asdynamicobstaclesthatmakeitchallengingfortheagent
to navigate through these spaces. For each environment, The simulation benchmarks have 6 scenes, 2 of each easy,
we have 4 static obstacles and at most 1 dynamic obstacle medium, and hard environments. Easy environments have
thrownadversarially. Wecompareagainstaclassicalbase- 0-1 obstacles within a 5m goal range. Medium environ-
linethatusesanA1RPLidarwithgmappingandmove base mentshave2-3obstacleswithin5mandthehardoneshave
for planning. We first teleoperate the robot for 3-5min to heavilyclutteredsceneswith5obstacleswithin5m.Ineach
create a map. Note that this provides the added advantage ofthesecases, onescene(Scenario1)comprisesofatight
that this baseline knows the entire map in advance. Since 1m wide long corridor which bounds the agent to not take
the Lidar cannot see objects above the plane we only test shortcutsandreachthegoalonlybynavigatingthroughob-
on ground obstacles and ignore floating ones that require stacles. The second (Scenario 2) is an L-shaped corridor
whole-bodycoordination. Weruntheplannertoonlyplan withthegoalattheend.Theevaluationmetricsarereported
thebasemotion. InTab.2wecomparethesuccessrateand asanaverageof10episodeswithrandomagentandobsta-
average number of collisions. An episode succeeds when cleinitializationacross3seeds.
the robot reaches within 15cm of the specified goal posi- We compare against various baselines to study the im-
tion. Overall, our method succeeds 20-40% more than the pact of our design decisions in Tab. 1. For each scenario,
classicalbaseline. Thisisbecausetheclassicalmethodsuf- wereportthesuccessrateandaverageepisodelengthacross
fersfromnoiseandisnotabletorecoverfromanoisymap, 10 rollouts. Our method achieves ≈ 33% higher success
and gets stuck, whereas the learned policy learns to look ratethantheNoPointNetbaselinesincepermutationinvari-
at the obstacles again and again to improve its uncertainty ant scandots latent makes the optimization problem easier
estimates and constantly updates its knowledge of where and also generalizes better at test time. Ours achieves ≈
obstaclesare. Thisabilityisevenmoreapparentinthedy- 68%highersuccessratethantheFixCambaselinewiththeStaticObstacles DynamicObstacles indynamicenvironmentsduetotheconfoundingmotionof
other agents [13, 33, 41, 44]. Once a map is built, a path
Scenario1
canbeplannedoverit. Exactpathscanbecomputedusing
Ours Classical Ours Classical
graph search algorithms [17], probabilistic methods which
AverageSuccess 0.8 0.6 0.6 0.0
arefasterbutyieldapproximatelyoptimalsolutions[25]or
Average#Collisions 1.0 0.4 1.6 1.2
potential-fieldbasedmethods[21].Alloftheseassumeper-
Scenario2
fectperceptionandre-planningisusuallyexpensivemaking
Ours Classical Ours Classical themsusceptibletonoiseandprecludingreactivebehavior.
AverageSuccess 0.8 0.4 0.6 0.2
Average#Collisions 0.8 0.6 1.6 1.0 Learning-basednavigation Inrecentyears,learninghas
been used toimprove the classical navigationstack. Mod-
Table 2. We compare our method against a classical mapping
ular approaches [8, 9, 15, 27] still leverage SLAM-based
andplanningbaselinefornavigationinclutteredsceneswithboth
methodstobuildamapbutuselearningorheuristicchanges
staticaswellasdynamicobstacles.Theclassicalperformsreason-
to get priors for the best possible route to a goal. End-to-
ablyinstaticenvironments,itquicklybreakswithdynamicobsta-
endapproachesforgomapsentirelyandtrainapolicytogo
cleslikehumanswalkingaround,whereasourmethodshowsmore
from images to robot commands to go to a goal location
robustreactivitytosuchobstaclesevenwithoutbeingtrainedwith
dynamicobstaclesinsimulation.Wereportthesuccessrateofour [10,11,40]. Wealsotaketheend-to-endapproachbutun-
methodcomparedwiththebaseline.Fortheclassicalbaseline,we likepriorworkwherewhattherobotseesisfixedbasedon
teleoperatetherobotfor2-3min. its position, in our case it must move its head and actively
choosewhatitseesmakingoptimizationmorechallenging.
camera pointing straight ahead. This is because in some
Mobile Manipulation A mobile base and arm together
cases the robot encounters obstacles in its peripheral vi-
cancompleteusefulin-the-wildmanipulationbutpresenta
sion and our policy can change the camera angle to avoid
morechallengingcontrolproblem. Imitationlearningtech-
them. Active vision is necessary for the robot to move ef-
niquesfocusoncollectinglargedatasetsinavarietyofset-
fectively through a cluttered environment. Our method is
tingswithadexterous6-dofarmandawheeledmobilebase
significantly better than the Mapping baseline because the
using teleoperation [3, 4, 6, 12, 18, 42]. Because of the
systematic noise in the object locations makes it hard for
high-dimensionality of mobile manipulation, there is also
the robot to avoid them, especially in cluttered environ-
control methods that leverage synergies between both the
ments, whereas our method can continuously estimate the
baseandthearmandplanstogether. [14,18,19,43].
position of obstacles while it is moving and adapt the mo-
tiononline. Finally,wecomparethedecoupled(DVO)op-
6.DiscussionandLimitations
timization against coupled (CVO) optimization variant of
ourmethodandfindthattheyachievesimilarperformance. We present SPIN, an approach to train robots that can si-
Wehypothesizethatthepartialobservabilityandjointopti- multaneously perceive, interact, and navigate cluttered en-
mization for camera and robot actions in CVO training al- vironmentsusingadata-drivenapproach. Weshowthatour
lowstheagenttoquicklydiscoveroptimalshortcutsthatare RL-based reactive approach is effective for active whole-
otherwisehardertodistillfromaprivilegedteacherpolicy. body control-perception problem, traditionally addressed
vianon-reactiveplanningmethods. With recentinterestin
5.RelatedWorks humanoid and other mobile robots with actuated cameras,
on neck for instance, SPIN is a cost-effective agile whole-
ClassicalApproaches Theproblemofnavigatingrobots
bodycontrolsolutionwithlimitedsensingandcompute.
around obstacles has been studied for decades. Classical
Althoughourrobotcanperceivegeometryandavoidob-
methods solve the motion and perception problem sepa-
staclesusingdepth,itstilloperatesonstereo-matcheddepth
rately. First these methods build a map of the environ-
instead of raw RGB. This leads to scenarios where it can
ment using the robot’s onboard sensors such as cameras,
bump into glass obstacles or shiny surfaces. In the future,
proprioceptionandLidarorinfrared[7,16]. Kalman-filter-
wewouldliketouseRGBforperception.
based[38]techniquesareoftenusedtotrackpositions,but
theycan’trepresentmulti-modalambiguitiesorrecoveraf- Acknowledgements We thank Jared Mejia and Mihir
tertrackingfailure[32]. Grid-basedmethodssolvethisbut Prabhudesaiforhelpingwithstress-testinginreal-worldex-
suffer from high memory usage [5]. Modern SLAM ap- periments. We are also grateful to Zackory Erickson and
proachesORBSLAM3[30],OpenVSLAM[35]andRTAB theHelloRobotteamfortheirsupportwiththerobothard-
[24] use variations of a method that relies on particle fil- ware. Thisworkwassupportedinpartbygrantsincluding
ters [37] to hold a multi-modal belief of the robot’s loca- ONRN00014-22-1-2096,AFOSRFA9550-23-1-0747,and
tion in the map [26, 36]. SLAM is especially challenging theGoogleresearchawardtoDP.References [14] ZipengFu,XuxinCheng,andDeepakPathak. Deepwhole-
bodycontrol:learningaunifiedpolicyformanipulationand
[1] Stretchbyhellorobot. https://hello-robot.com/.
locomotion. InConferenceonRobotLearning,pages138–
5
149.PMLR,2023. 8
[2] Ananye Agarwal, Ashish Kumar, Jitendra Malik, and
[15] SaurabhGupta,JamesDavidson,SergeyLevine,RahulSuk-
DeepakPathak. Leggedlocomotioninchallengingterrains
thankar, and Jitendra Malik. Cognitive mapping and plan-
usingegocentricvision. InConferenceonRobotLearning,
ningforvisualnavigation. InProceedingsoftheIEEEcon-
pages403–415.PMLR,2023. 5
ference on computer vision and pattern recognition, pages
[3] MichaelAhn,AnthonyBrohan,NoahBrown,YevgenCheb- 2616–2625,2017. 8
otar,OmarCortes,ByronDavid,ChelseaFinn,ChuyuanFu, [16] J-SGutmann,WolframBurgard,DieterFox,andKurtKono-
Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i lige. Anexperimentalcomparisonoflocalizationmethods.
can,notasisay:Groundinglanguageinroboticaffordances. InProceedings.1998IEEE/RSJInternationalConferenceon
arXivpreprintarXiv:2204.01691,2022. 8 IntelligentRobotsandSystems.InnovationsinTheory,Prac-
[4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen tice and Applications (Cat. No. 98CH36190), pages 736–
Chebotar,JosephDabis,ChelseaFinn,KeerthanaGopalakr- 743.IEEE,1998. 8
ishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. [17] PeterEHart,NilsJNilsson,andBertramRaphael.Aformal
Rt-1: Robotics transformer for real-world control at scale. basisfortheheuristicdeterminationofminimumcostpaths.
arXivpreprintarXiv:2212.06817,2022. 8 IEEE transactions on Systems Science and Cybernetics, 4
[5] Wolfram Burgard, Dieter Fox, Daniel Hennig, and Timo (2):100–107,1968. 8
Schmidt. Estimatingtheabsolutepositionofamobilerobot [18] JesseHaviland,NikoSu¨nderhauf,andPeterCorke. Aholis-
using position probability grids. In Proceedings of the na- ticapproachtoreactivemobilemanipulation.IEEERobotics
tional conference on artificial intelligence, pages 896–901, andAutomationLetters,7(2):3122–3129,2022. 8
1996. 8 [19] JiahengHu,PeterStone,andRobertoMart´ın-Mart´ın.Causal
[6] Ben Burgess-Limerick, Chris Lehnert Jurgen Leitner, and policygradientforwhole-bodymobilemanipulation. arXiv
PeterCorke. Enablingfailurerecoveryforon-the-movemo- preprintarXiv:2305.04866,2023. 8
bilemanipulation. arXivpreprintarXiv:2305.08351,2023. [20] GlennJocher. YOLOv5byUltralytics,2020. 2
8
[21] Oussama Khatib. The potential field approach and opera-
[7] AnthonyRCassandra,LesliePackKaelbling,andJamesA tional space formulation in robot control. In Adaptive and
Kurien. Actingunderuncertainty:Discretebayesianmodels LearningSystems:TheoryandApplications,pages367–377.
for mobile-robot navigation. In Proceedings of IEEE/RSJ Springer,1986. 8
InternationalConferenceonIntelligentRobotsandSystems.
[22] N. Koenig and A. Howard. Design and use paradigms
IROS’96,pages963–972.IEEE,1996. 8 for gazebo, an open-source multi-robot simulator. In 2004
[8] MatthewChang, TheophileGervet, MukulKhanna, Sriram IEEE/RSJ International Conference on Intelligent Robots
Yenamandra,DhruvShah,SoYeonMin,KavitShah,Chris and Systems (IROS) (IEEE Cat. No.04CH37566), pages
Paxton,SaurabhGupta,DhruvBatra,etal. Goat:Gotoany 2149–2154vol.3,2004. 5
thing. arXivpreprintarXiv:2311.06430,2023. 8 [23] JasonKu,AliHarakeh,andStevenLWaslander. Indefense
[9] Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, ofclassicalimageprocessing: Fastdepthcompletiononthe
Abhinav Gupta, and Ruslan Salakhutdinov. Learning cpu.In201815thConferenceonComputerandRobotVision
to explore using active neural slam. arXiv preprint (CRV),pages16–22.IEEE,2018. 2
arXiv:2004.05155,2020. 8 [24] MathieuLabbe´andFranc¸oisMichaud.Rtab-mapasanopen-
[10] PrithvijitChattopadhyay,JudyHoffman,RoozbehMottaghi, source lidar and visual simultaneous localization and map-
andAniruddhaKembhavi. Robustnav:Towardsbenchmark- pinglibraryforlarge-scaleandlong-termonlineoperation.
ing robustness in embodied navigation. In Proceedings of Journaloffieldrobotics,36(2):416–446,2019. 8
the IEEE/CVF International Conference on Computer Vi- [25] Steven M LaValle, James J Kuffner, BR Donald, et al.
sion,pages15691–15700,2021. 8 Rapidly-exploringrandomtrees:Progressandprospects.Al-
[11] Tao Chen, Saurabh Gupta, and Abhinav Gupta. Learning gorithmic and computational robotics: new directions, 5:
explorationpoliciesfornavigation. InInternationalConfer- 293–308,2001. 8
enceonLearningRepresentations,2019. 8 [26] JunSLiuandRongChen. Sequentialmontecarlomethods
[12] Yuqing Du, Daniel Ho, Alex Alemi, Eric Jang, and Mohi for dynamic systems. Journal of the American statistical
Khansari. Bayesian imitation learning for end-to-end mo- association,93(443):1032–1044,1998. 8
bilemanipulation. InProceedingsofthe39thInternational [27] Haokuan Luo, Albert Yue, Zhang-Wei Hong, and Pulkit
ConferenceonMachineLearning,pages5531–5546.PMLR, Agrawal.Stubborn:Astrongbaselineforindoorobjectnavi-
2022. 8 gation.In2022IEEE/RSJInternationalConferenceonIntel-
[13] DieterFox,WolframBurgard,andSebastianThrun.Markov ligentRobotsandSystems(IROS),pages3287–3293.IEEE,
localization for mobile robots in dynamic environments. 2022. 8
Journalofartificialintelligenceresearch,11:391–427,1999. [28] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo,
8 Michelle Lu, Kier Storey, Miles Macklin, David Hoeller,Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel [43] Naoki Yokoyama, Alexander William Clegg, Eric Under-
State. Isaacgym:Highperformancegpu-basedphysicssim- sander,SehoonHa,DhruvBatra,andAksharaRai.Adaptive
ulationforrobotlearning,2021. 5 skill coordination for robotic mobile manipulation. arXiv
[29] TakahiroMiki,JoonhoLee,JeminHwangbo,LorenzWell- preprintarXiv:2304.00410,2023. 8
hausen,VladlenKoltun,andMarcoHutter. Learningrobust [44] Chao Yu, Zuxin Liu, Xin-Jun Liu, Fugui Xie, Yi Yang, Qi
perceptive locomotion for quadrupedal robots in the wild. Wei, and Qiao Fei. Ds-slam: A semantic visual slam to-
ScienceRobotics,7(62):eabk2822,2022. 5 wards dynamic environments. In 2018 IEEE/RSJ interna-
[30] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D tionalconferenceonintelligentrobotsandsystems(IROS),
Tardos. Orb-slam: aversatileandaccuratemonocularslam pages1168–1174.IEEE,2018. 8
system. IEEE transactions on robotics, 31(5):1147–1163,
2015. 8
[31] Ste´phaneRoss,GeoffreyGordon,andDrewBagnell. Are-
ductionofimitationlearningandstructuredpredictiontono-
regretonlinelearning.InProceedingsofthefourteenthinter-
national conference on artificial intelligence and statistics,
pages627–635.JMLRWorkshopandConferenceProceed-
ings,2011. 5
[32] StergiosIRoumeliotisandGeorgeABekey. Bayesianesti-
mationandkalmanfiltering:Aunifiedframeworkformobile
robotlocalization. InProceedings2000ICRA.Millennium
conference. IEEE international conference on robotics and
automation. Symposia proceedings (Cat. No. 00CH37065),
pages2985–2992.IEEE,2000. 8
[33] Muhamad Risqi U Saputra, Andrew Markham, and Niki
Trigoni. Visualslamandstructurefrommotionindynamic
environments: Asurvey. ACMComputingSurveys(CSUR),
51(2):1–36,2018. 8
[34] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRad-
ford,andOlegKlimov. Proximalpolicyoptimizationalgo-
rithms. arXivpreprintarXiv:1707.06347,2017. 4
[35] Shinya Sumikura, Mikiya Shibuya, and Ken Sakurada.
Openvslam:Aversatilevisualslamframework. InProceed-
ingsofthe27thACMInternationalConferenceonMultime-
dia,pages2292–2295,2019. 8
[36] SebastianThrun. Particlefiltersinrobotics. InUAI,pages
511–518.Citeseer,2002. 8
[37] Sebastian Thrun, Dieter Fox, Wolfram Burgard, and Frank
Dellaert. Robustmontecarlolocalizationformobilerobots.
ArtificialIntelligence,128(1):99–141,2001. 8
[38] Greg Welch, Gary Bishop, et al. An introduction to the
kalmanfilter. 1995. 8
[39] PaulJWerbos. Backpropagationthroughtime: whatitdoes
and how to do it. Proceedings of the IEEE, 78(10):1550–
1560,1990. 4
[40] Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee,
Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra.
Dd-ppo:Learningnear-perfectpointgoalnavigatorsfrom2.5
billionframes. arXivpreprintarXiv:1911.00357,2019. 8
[41] DenisFWolfandGauravSSukhatme. Mobilerobotsimul-
taneouslocalizationandmappingindynamicenvironments.
AutonomousRobots,19:53–65,2005. 8
[42] Josiah Wong, Albert Tung, Andrey Kurenkov, Ajay Man-
dlekar, Li Fei-Fei, Silvio Savarese, and Roberto Mart´ın-
Mart´ın. Error-aware imitation learning from teleoperation
data for mobile manipulation. In Conference on Robot
Learning,pages1367–1378.PMLR,2022. 8SPIN: Simultaneous Perception, Interaction and Navigation
Supplementary Material
7.QualitativeResults does not learn camera movement in a relevant fashion and
alsobecomeshardertodistillintoadepth-conditionedstu-
We test our framework in several in-the-wild sce-
dentpolicythroughonlyego-centricview.Similarly,forthe
narios, some of which are illustrated in Fig-
3-phasedecoupledvisuo-motoroptimization,weinducein-
ure 1. Qualitative video results are available at
formation bottleneck with a low-dimensional latent space
https://spin-robot.github.io/
ofsize16forthescandotslatentwhiletrainingtheteacher
Weseeemergentbehaviorwheretherobotcontinuously
policy, which again helps it in attending to most relevant
avoidsdynamicobstacleswithoutseeingthemduringtrain-
information at any time instant t in order to make student
ing.Wealsoobservegeneralizationheavilyclutteredindoor
policydistillationfeasible.
to dim-lit outdoor environments. The agent also demon-
stratesreactivewhole-bodycoordinationwhereitmovesits
armupordowntoefficientlynavigateacrossfloatingobsta- ObservationSpace. Theobservationspacefortherobot
cles instead of re-routing and re-planning base movement, comprisesofjointpositions(q),jointvelocities(q vel),end-
demonstrating3Dspatialawareness. effectorpositionp eef,goalposition(p goal)anddepthlatent
(zˆ) containing visual information about the environment.
8.ImplementationDetails Note that during phase 1 training in simulation, scandots
(z)areusedasaproxyforfasterdepthrenderingandlater
We make several design choices for the working of our distilledintoaegocentricdepth-conditionedpolicy. During
framework. Firstly, the robot is only allowed to have lo- real-world deployment, p is obtained via forward kine-
eef
calvisibilityinordertodevelophighlyreactiveandinstant maticsandotherproprioceptioninformationisobtaineddi-
behaviors. Atanytimeinstantt,theagentcanperceiveit’s rectlyfromtherobot.
environmentwithinarangeof2minall4direction–front,
back,leftandrightbasedonthecamera’sviewingdirection
Action Space. The action space of the robot consists of
andit’sfieldofview.Wealsoempiricallyobservethatgiven
thevelocityforbaserotationaswellastranslationandjoint
largerviewingrange,say>5mwhichcontainsinformation
positions for all the other joints including arm, camera as
of more than 4-5 nearest obstacles to the agent makes it a
well as gripper actions. The gripper action is a continu-
sub-globalpathplanningproblemwhichbecomesharderto
ouslyvaryingscalarwhichcanactuatethegrippertodiffer-
optimize,leadingtodegradedbehaviorandperformanceas
entextents,unlikeabinaryactionindicatingopenorclosed
reportedinTable3.
gripper.
VisibilityRange SuccessRate↑ DistancetoGoal(m)↓
≤1m 0.96 0.28 Reward Scales. We use a distance and forward progress
≤2m 0.96 0.26 goalforreaching,abinaryrewardforgraspingandacontin-
≤3m 0.93 0.63 uousshapedrewardforliftingtheobjecttoacertainheight
≤5m 0.86 1.21 above the table. We also add a small penalty for the joint
velocitiestothearmstretchandcamerajointsforatempo-
Table3.Wereporttheaveragesuccessrateandaveragedistanceto rallysmoothgaitwhichpermitseasiersim2realtransferas
goalfor10episodesacross3seedseachwithdifferentmaximum
wellasmoreappropriatebehaviorsleadingtolessjitterand
visibilityrangefortheagentatanytimeinstant.Asreported,with
moreconsistentmovementsontherealhardware.
broadervisibility,theagentshowsmorefrequentstallingleading
The reward scales used for goal reaching, grasping and
tohigheraveragedistancetogoal.
lift rewards are reported in Table 4. Detailed formulations
oftherewardfunctionsaredescribedinSection2.1.
Secondly, contraryto standard teacher-student architec-
tures which use privileged system information for training
the teacher, we restrict the privileged information to only Network Architecture and Training Details. The actor
elements in the robot’s field-of-view that can be retrieved and critic for teacher policy are LSTM with 256 hidden
fromtheego-viewinthesamestate.Forthis,weprojectthe units,withinputasprorpioception,goalandscandotslatent.
obstaclescandotsontotheimageplaneandpassonlythose The scandots are compressed using a pointnet architecture
scandotstotherobotobservationwhichlieinthecamera’s forpermutationinvariance. Thedepthnetworkforthestu-
field-of-view. Notethat,ifthiswerenotthecase,therobot dent policy takes as input a low-resolution depth image of(a)PoseI:Facingforward (b)PoseII:Facingdownward (c) Pose III: Facing downward (d)Pose IV: Facing forward and
andslightlyforward slightlydownward
Figure7.DifferentcameraposesfortheFixCambaseline.
RewardScale Scenario1 Scenario2
ReachReward 0.1 E M H E M H
GraspReward 0.5 I 0.93 0.40 0.20 0.97 0.40 0.20
LiftReward 0.8 II 0.70 0.30 0.10 0.67 0.47 0.10
JointVelocityPenalty -0.03 III 0.86 0.33 0.10 0.77 0.30 0.10
IV 1.00 0.53 0.20 1.00 0.50 0.26
Table4.Wereporttheaveragesuccessrateandaveragedistanceto
goalfor10episodesacross3seedseachwithdifferentmaximum Table 5. Success rate for 4 FixCam poses in easy (E), medium
visibilityrangefortheagentatanytimeinstant.Asreported,with (M),hard(H)envs.
broadervisibility,theagentshowsmorefrequentstallingleading
tohigheraveragedistancetogoal.
ObjectDetectionforPickPolicy Oncetherobotreaches
nearthegoal,werandomlyselectanobjectwithinitsfield
size 58 × 87 and comprises of 3-layer convolution back- ofviewinordertobegraspedandfetchedtoatargetloca-
bone followed by 3 fully-connected layers. We use Adam tion. For getting the target object location, we run YOLO
Optimizer with an initial learning rate of 1e−3, entropy [20],areal-timeobjectdetectionmodelwithanaveragein-
coefficientof5e−4andγ as0.99. ferencespeedof20ms.Weusethecorrespondingdepthim-
agetodeprojectthepixelpointintoa3D-coordinatewhich
ispassedasthenewgoalpositiontothemanipulationpol-
Asynchronous DAgger Training. Since depth render- icy.
ingonsimulatorsisacomputationalbottleneck,weimple-
mentanasynchronousversionofDAggeralgorithmwhich 9.Analysingcameraandbasemotion
simultaneously collects data in a buffer and trains the stu-
dent policy with batches sampled from the collected data FixedCameraBaseline WerunFixCambaselinewith4
using2parallelprocesses. Thisprovidesa2.5×computa- camera poses (Figure 7) – I: Front, II: Down, III: Down
tionalspeedupoverthenon-parallelizedversionofthealgo- andslightlyfront,IV:Frontandslightlydownoneasy(E),
rithm, allowing faster convergence of the student network. medium (M), hard (H) environments. I, IV with max fov
We also find that freezing the weights of the student actor havemuchlowersuccessthanSPIN,implyingactivevision
pre-initialized from the teacher policy for first 1000 itera- isrequiredinclutter. Pose(IVdepictstheFixCambaseline
tionshelpsaswarm-upstepstothedepthconvolutionback- referredinthepaper.
boneforstabletraining.
Camera Movement and Camera Observations: We
Post-processingforcleandepthimages. Tomitigatethe show camera trajectory in Figure 8. When navigating
issues due to noisy depth, we post-process the depth ob- through clutter (frames 1, 2, 4), it tilts downward to max-
tained from the Intel RealSense Camera using a real-time imizefovnearthebase,butwithnonearbyobstacle(frame
fast hole-filling algorithm for depth images [23]. With the 3),itfacesfront. Detailedmovementofthecameracanbe
camera constantly in motion, there are additional artefacts seen on the website along with paired RGBD images for
with depth images. For this, we additionally use temporal rollouts. RGBframesareonlyforanalysis,thepolicyonly
filteringoverthestreamofdepthimages. observesdepthimages.Figure8. Cameramovementanalysisinatrajectory. Theagentfacesthecameradownwardwhennavigatingthroughtightlycluttered
vicinityascanbeseeninthefirst,secondandfourthframe,whereasthecamerapointsmoretowardsthefrontwhentherearenoimmediate
obstaclesinthedirectionofmovement,asillustratedinthethirdframe.
Figure 9. Scenarios where whole-body coordination is essential under heavy obstructions. In the above cases where the obstacles are
tightlypacked,itisnotpossiblefortherobottonavigatethroughthemavoidingcollisionswithoutliftingthearmtoanappropriateheight.
Necessity for Active Vision and Whole-body coordina-
tion Active Vision: In principle, a multi-camera system
should be equivalently adequate, however most views will
contain insignificant information and require large models
to process. With limited onboard compute on most robots
and requirement for real-time reactivity (< 0.1s), it be-
comes infeasible to deploy them with larger vision back-
bones.
Whole-body coordination (WBC): Under heavy obstruc-
tions, the robot cannot move without collision if the base
& arm control are decoupled. Figure 9 shows such a sce-
nariowhereafixedarmandgripperclosetobasewouldfail Figure 10. Success rate for scandots (blue) vs depth (red). The
depth-basedpolicyattainscloseto0performanceevenafter400k
withoutWBC,whichalsoallowsittousetheextradegree
envstepsoftraining,whereasthepolicytrainedwithscandotsin-
offreedomtofindshorterandmoreefficientpaths.
creasinglyimprovesovertime.
10.Directlytrainingfromdepthimages.
We compare training from depth (red) and our 2-phase
method with scandots (blue) in a medium difficulty envi-
ronmentasillustratedinFigure10. Depthpolicyhas<1% can fit on a single GPU), causing 61× slow-down bottle-
successafter22htraining,whereastotal(phase1+2)wall- neck. This shows the necessity and efficiency of our pro-
clock time for SPIN is 16h (6+10). The simulator gives posed2-phasedcoupledvisuomotoroptimizationapproach
≈50kfpsforscandots(8192envs)and≈820fpsfordepth using scandots over naively training an RL policy from
(256 environments – maximum parallel environments that depthobservations.Figure11.Visualizationsofenvironmentmapbuiltusing2DLidar.Therobotislocalizedasperitsinitialpositionandorientation.
11.ClassicalNavigationBaseline
As discussed in Section 3, we compare our method with
aclassicalmap-basedbaselinewhichusesthe2DRPLidar
tobuildanenvironmentmapandthencreatesaplanusing
Monte Carlo method. We observe that for > 90% of the
cases, the robot is able to build a map and find a feasible
path,howeveritisnotabletoexecutetheplannedpathfor
>85%cases.Thisissuearisesduetonoisycontrolorunex-
pectedwheelmotionduetoterraindifferences.Ourmethod
isabletoovercomesuchfailuresduetoaconstantfeedback
and reactive improvisation through proprioception as well
asdepth,whichallowsittodealwithuncertaintieswithout
requiringapre-builtenvironmentmap.Moreover,duetolo-
calizationinaccuracies,thebaselinemethodisoftenunable
toreachtheintendedgoalifnotinitializedinthesameori-
entation as was used before building the map. Contrary to
that,weheavilyrandomizealldegreesoffreedomaswellas
the robot orientation at the beginning of every rollout dur-
ing test time. Moreover, since the robot has a 2D Lidar
installedonit,wedonottestitinenvironmentswithfloat-
ing obstacles which would require 3D understanding and
whole-body coordination to navigate through clutter. We
showsomevisualizationsofthemapbuiltandplanscreated
bytherobotinFigure11.