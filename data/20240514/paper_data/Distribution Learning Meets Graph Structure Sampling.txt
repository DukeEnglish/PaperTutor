Distribution Learning Meets Graph Structure Sampling
Arnab Bhattacharyya Sutanu Gayen
National University of Singapore IIT Kanpur
arnabb@nus.edu.sg sutanugayen@gmail.com
Philips George John Sayantan Sen
CNRS-CREATE & NUS Centre for Quantum Technologies,
philips.george.john@u.nus.edu National University of Singapore
sayantan789@gmail.com
N. V. Vinodchandran
University of Nebraska-Lincoln
vinod@cse.unl.edu
Abstract
This work establishes a novel link between the problem of PAC-learning high-dimensional
graphicalmodels andthe task of(efficient) countingandsamplingofgraphstructures,usingan
online learning framework. The problem of efficiently counting and sampling graphical struc-
tures, such as spanning trees and acyclic orientations, has been a vibrant area of research in
algorithms. We show that this rich algorithmic foundation can be leveraged to develop new
algorithms for learning high-dimensional graphical models.
We observe that if we apply the exponentially weighted average (EWA) or randomized
weighted majority (RWM) forecasters on a sequence of samples from a distribution P∗ using
the log loss function, the average regret incurred by the forecaster’s predictions can be used to
∗
bound the expected KL divergence between P and the predictions. Known regret bounds for
EWA and RWM then yield new sample complexity bounds for learning Bayes nets. Moreover,
these algorithms can be made computationally efficient for several interesting classes of Bayes
nets.
(i) For the class of tree-structured distributions over [k]n, we design an algorithm that, for
any ε>0, uses O(nk2/ε) samples from some distribution P∗ and returns P, an efficiently
∗
samplable mixture of trees, whose KL-divergence from P is at most 3 times the optimal
∗
KL-divergence beetween P and any tree-structured distribution plus ε, wbith probability
at least2/3. This guaranteeis not knownto be achievable with the well-knownChow-Liu
algorithm.
(ii) Foranyconstantd>0,givenanundirectedchordalgraphG,wedesignanalgorithmthat
uses O(n3kd+1/ε2) samples from some distribution P∗ over [k]n, runs in polynomial time,
∗
and returns a Bayesnet P with skeleton G and indegree d, whose KL-divergencefrom P
is at emost ε more than the optimal KL-divergence between P∗ and any Bayes net with
skeleton G and indegree db, with probability at least 2/3. The only previous result in this
spirit was for G being a tree and in the realizable setting.
1
4202
yaM
31
]GL.sc[
1v41970.5042:viXraContents
1 Introduction 3
1.1 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2 Our Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.3.1 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2 Preliminaries 11
2.1 Probability Distributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.2 Clipping and Bucketing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.3 PAC Distribution Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.4 Online Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3 Agnostic Learning of Bayesian Networks 16
3.1 Connection between Regret and KL divergence . . . . . . . . . . . . . . . . . . . . . 16
3.2 Discretization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.3 Sample Complexity for learning Bayes Nets . . . . . . . . . . . . . . . . . . . . . . . 21
4 Learning Tree-structured distributions 24
4.1 Sampling from the EWA/RWM Distribution . . . . . . . . . . . . . . . . . . . . . . 25
4.2 Learning Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
5 Learning Chordal-structured distributions 27
5.1 Preliminaries about Chordal Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
5.2 EWA and RWM with Chordal Experts . . . . . . . . . . . . . . . . . . . . . . . . . . 30
A Lower bound for learning tree-structured distributions 40
B Learning Bayesian networks with bounded vertex cover 42
C Learning Tree-structured distributions 43
21 Introduction
High-dimensional distributions are foundational to contemporary machine learning, with ap-
plications that span across various domains including gene regulation networks [FLNP00, CR06,
CR09, EDAL10], protein signaling networks [DEKM98, SM03, TRBK05], brain connectivity net-
works [HLS+10, VGPT10], and psychiatric symptom networks [BvBR+15, PGBL+20, XMW22]
etc. In particular, probabilistic graphical models, including Bayesian networks, Ising models, and
Gaussian graphical models, are extensively utilized to model a wide range of data generation pro-
cesses in practice (see [Lau96, WJ+08, KF09] and the references therein). These models serve as
powerful tools for understanding complex relationships and dependencies within high-dimensional
datasets,makingthemindispensableinboththeoretical andappliedmachinelearning. Agraphical
model is a succinct representation of a high-dimensional distribution over an (exponentially) large
sample space such as Rn or 0,1 n. Due to the limited dependence between component random
{ }
variables encoded by a dependence graph, these models are particularly powerful for describing
the joint probability distribution over a large set of variables in a succinct and interpretable way.
Learning distributions represented by graphical models is a central problem with both theoretical
and practical significance.
ABayesiannetwork(Bayesnet)withnvariablesandalphabetsizekisaprobabilitydistribution
over the sample space [k]n defined with respect to a directed acyclic dependency graph (DAG) G
over the vertex set [n]. Each node of G defines a component random variable over the sample
space [k]. Every component i is independent of all the components that are not descendants of i
conditioned on the components that are the parents of i. Using Bayes rule, it can be shown that
such a distribution then factorizes as a product of n conditional probability values. Thus if G has
indegree at most d, such a distribution can be encoded by at most nkd+1 parameters instead of kn.
Learning Bayesian network distributions from samples generally involves two steps: structure
learning, where the underlying dependency graph is identified, and parameter learning, which
involves determining the conditional probability table when the structure is known. Structure
learning methods fall into two categories: constraint-based and score-based. Constraint-based
approachesbeginwithafulldirectedacyclicgraph(DAG)andeliminateedgesbytestingconditional
independences, requiring significant conditional dependencies for accurate graph recovery. Score-
based methods, on the other hand, assign scores to each DAG and formulate structure recovery
as an optimization problem, often tackled with local heuristics like greedy hill climbing due to the
complexity of the optimization.
The current work reprises the score-based approach towards Bayes net learning, but instead of
optimizing the score directly, we use the framework of online learning to reduce the problem to
sampling from a family of high-dimensional structures. In the setup of online learning, the goal is
to design a forecaster that observes a sequence of examples x(1),x(2),...,x(T), and at each time t,
outputs a prediction p based only on x(1),...,x(t 1). After predicting p , it observes x(t), and it
t − t
incurs a loss ℓ(p ,x(t)) for a loss function ℓ. The cumulative loss of the forecaster is benchmarked
t
against that of a fixedband known setof experts. Thegoal of thealgorithmbis to minimize the regret,
defined as the dbifference between the cumulative loss over all rounds and the loss it would incur
if it were to follow the best expert in hindsight. Online learning is a mature field with numerous
applicationstotheoreticalcomputerscience,suchasingametheory,approximationalgorithms, and
complexity theory (see [FS96, FS99, BHK+23, DGP09, Rub17, BBD+19, KR12] and the references
therein).
Inthecontextofdistributionlearning,wesetthesetofexpertsasallpossiblecandidateBayesian
3networks (up to a sufficient discretization). The inputs will be random samples from the input
distribution, and we set the loss function to be the negative log-likelihood ℓ(p,x) = logp(x).
−
The primary obstacle in applying the online approach to distribution learning lies in ensuring
computational efficiency. All the standard forecasting algorithms have running tbime at least linbear
inthenumberofexperts;inourcase,theexpertswillconsistofallthediscretization ofallcandidate
Bayes nets, whichisexponentially many. Akey insightofourworkis theclose relationshipbetween
this challenge and thetask of efficient countingand samplingof DAGs. Recognizing this link allows
us to transfer techniques and algorithms from the counting and sampling literature to the realm of
distribution learning, leading to significant new results in learning Bayes net distributions.
1.1 Our Results
WefirstsetuptheframeworkofPAC-learning[Val84]fordistributions;formaldefinitionsappear
in Section 2. We use KL-divergence as the notion of similarity between probability distributions,
and we will throughout work with distributions on [k]n = 1,...,k n. Let be a class of such
{ } C
distributions; in our applications, will correspond to some family of Bayes nets.
C
For ε> 0,A 1 and two distributions P and P over [k]n, we say P is an (ε,A)-approximation
≥
for P with respect to if D (P P) A min D (P Q)+ε. When A = 1, we simply say P
KL Q KL
is an ε-approximationC of P. Ank algo≤ rithm· is sai∈ dCbto be ak n agnostic PbAC-learner for if for any
C
ε,δ > 0 and access to i.i.d. sampbles from an input distribution P , it outputs a distribution Pb
∗
which is an ε-approximation for P with probability at least 1 δ. If P is not necessarily in , the
∗
− C
algorithm is called improper; otherwise, it is called proper. Also, the realizable setting correspondbs
to when the input P is guaranteed to be in . b
∗
C
General Framework for Learning Bayes Nets It is well-known (e.g., [BGP+23]) that given
a DAG G, the minimum KL divergence between P and a Bayes net over G can be written as
∗
J I(X ;X ), where X P and J is a constant independent of G. Hence, if
P i∗ s−
the
i c∈laV s( sG)
of
Bai yesPa nG e( ti s)
over
DAGs∼
of
in∗ -degreeP∗
d, a natural strategy for designing agnostic
C P
learning for is to approximate the mutual information between any node and any set of d other
C
nodes up to a suitable additive error. Next, one maximizes over all possible DAGs with in-degree d,
the sum of mutual informations between a node and its d parents. Iterating over all possible DAG
structures would lead to an algorithm with sample complexity O(n2kd+1ε 2) and exponential time
−
complexity.
Our proposed approach immediately gives an improper learneing guarantee with a quadratically
better sample complexity in the realizable setting (but again with exponential time complexity).
Theorem 1.1. Let be the family of distributions over [k]n that can be defined as Bayes nets over
C
DAGs with n nodes and maximum in-degree d. There exists a PAC-learner for in the realizable
C
setting using O(nkd+1ε 1log(n/ε)) samples that returns a mixture of Bayes nets from which is
−
C
an ε-approximation of the input distribution with probability at least 2/3.
The sample complexity in Theorem 1.1 is nearly optimal1 for constant error probability. The
same approach also yields polynomial sample complexity for agnostic learning in the non-realizable
1Note that for learning with respect to total variation distance, the corresponding sample complexity of
O(nkd+1ε−2) is known [BCD20], but theproof uses thetournament method which does not apply to KL divergence
as it is not a metric.
e
4setting with exponentially small error probability and for proper learning with polynomially small
error probability. While these bounds are no longer optimal, we will use them later to design the
first polynomial time algorithms for learning Bayes nets over large classes of DAGs.
Learning Tree-structured Distributions By tree-structured distributions (or simply, trees
when the meaning is clear), we mean Bayes nets whose underlying DAG has in-degree 1. They
can be equivalently defined as undirected Markov models over trees. Prior works have established
tight sample complexity bounds for learning trees. It is known [DP21, BGP+23] that for proper
PAC-learning in the realizable setting, Θ(nε 1) samples are necessary, that for agnostic proper
−
PAC-learning Θ(n2ε 2) samples are necessary, and that both bounds are nearly achieved by the
−
classic Chow-Liu algorithm [CL68]. e
In our propeosed framework, we surprisingly get a learning algorithm for (ε,3)-approximating
trees using Θ(nk2ε 1) samples in the agnostic (non-realizable) setting. Note that this is quadrati-
−
cally better than the aforesaid optimal bound for agnostic proper learning of trees.
e
Theorem 1.2. Let be the family of tree-structured distributions over [k]n. There exists an algo-
C
rithm that for any ε > 0, given sample access to a distribution P , returns an (ε,3)-approximation
∗
P of P with probability at least 2/3 and uses m = O(nk2ε 1) samples and poly(m) running time.
∗ −
Moreover, there is a polynomial time algorithm to generate samples from P.
b e
Acuriouscorollary isthatintherealizablesetting, wegetaPAC-learninbgalgorithmforlearning
trees with sample complexity O(nk2ε 1) that is very different from Chow-Liu. In fact, the current
−
best upper bound on the sample complexity of Chow-Liu from [BGP+23] is O(nk3ε 1), which is
−
worse by a factor of k. The outeput distribution P is a mixture of exponentially many trees but can
nevertheless be sampled in polynomial time by using the matrix-tree theorem,eas we explain later.
Learning with mixtures of trees has been studiedbbefore [MJ00, KK09, AHHK12, SZVdB23] but in
other contexts.
Wealsonotethatgoingbeyondtrees,thesameapproachallowsustodeveloppolynomialsample
and time algorithms for learning Bayes nets on an unknown DAG whose moralization is promised
to have constant vertex cover size. Here, instead of sampling using the matrix-tree theorem, we
can utilize a recent result of Harviainen and Koivisto [HK23] to sample such DAGs. Details appear
in Appendix B.
Learning with Known Chordal Skeleton The skeleton of a DAG refers to its underlying
undirected graph. A well-investigated class of Bayes nets that go beyond trees are polytrees, i.e.,
BayesnetswhoseDAGshavetreeskeletons. Forexample,aBayesnetonaninwarddirectedstarisa
polytree but not a tree. Polytrees are especially interesting because they admit fast exact inference
[Pea14]. Dasgupta [Das99] looked at the problem of learning polytree distributions in terms of
the negative log-likelihood cost. He showed that it is NP-hard to get a 2-polytree (Bayes nets of
indegree 2 whose skeleton is acyclic) whose cost is at most c times that of the optimal 2-polytree
for some constant c > 1, even if we have oracle access to the true entropies (equivalently, infinite
samples). Our reverse-KL cost differs from Dasgupta’s cost by an additive term that corresponds
to the entropy of the true unknown distribution [BGP+23, Lemma 3.3]. Moreover, our distribution
learning algorithms achieve an additive approximation in the reverse-KL cost.
More generally, we consider Bayes nets having a chordal skeleton with bounded indegree; poly-
trees with bounded indegree are a special case.
5Theorem 1.3. Let G be an undirected chordal graph, and suppose k and d are fixed constants. Let
G be the family of distributions over [k]n that can be defined as Bayes nets over DAGs having
Ld
skeleton G and with indegree at most d. There exist (i) an agnostic PAC-learner for G using
Ld
O(n4k2d+2ε 4logδ 1) samples that is improper and returns an efficiently-samplable mixture of dis-
− −
tributions from G, and (ii) an agnostic PAC-learner for G using O(n3kd+1ε 2log(n/εδ)) samples
Ld Ld −
aend polynomial running time that is proper and returns a distribution from G.
Ld
e
The closest related result is that of [CYBC24] who designed a PAC-learner in the realizable
setting for polytrees with optimal sample complexity O(nkd+1ε 1logδ 1). However, their analysis
− −
crucially uses the realizability assumption, and it was left as an open question in that work to find
an efficient agnostic learner for polytrees. The samplee complexity bounds in Theorem 1.3 follow
from the general bounds for learning Bayes nets; the crux of the theorem is the computational
efficiency. As we explain below, this is accomplished using a dynamic programming approach,
motivated by prior work on counting the number of acyclic orientations in chordal graphs [SSW93,
BS22, CHMM23].
1.2 Our Techniques
Online Learning Framework to Learning in reverse KL Given i.i.d. samples from a distri-
bution P , we are trying to learn it. Roughly, for a random x P , a good approximate Bayes
∗ ∗
∼
net P should maximize the probability P(x), or equivalently, minimize the expected log-likelihood
E log 1 . Keeping this in mind, we define the loss of any Bayes net P predicted by the
x P∗ P(x)
∼
algorithhm to beilog 1 for a sample x.
P(x) b
We follow the online learning framework. Here the algorithm observes a set of samples
A
x(1),...,x(T) P
∗
bover T rounds from an unknown Bayes net P ∗. The goal of the algorithm
∼
is to learn a distribution P which is close to P . After observing each sample x(t), predicts
∗
A
a Bayes net P and incurs a loss of log 1 for this round. However, there is a set of experts
t
b
Pt(x(t))
to help . For simplicity, we can assume each expert E is simply one Bayes net among
E A b ∈ E
all possible Bayes nets. Had stuck tobany particular expert E , it would incur a total loss
A ∈ E
T log 1 over all theT rounds. Thealgorithm can change theexperts in between or do some
t=1 E(x(t))
rPandomized strategy for choosing the expert among . Let P
t
be its prediction after round t. The
E
regret is defined to be the difference between the loss of the algorithm and that of the best expert:
T log 1 min T log 1 . b
t=1 E(x(t)) − E ∈E t=1 E(x(t))
We design a set of finitely many Bayesian networks that “covers” all Bayes nets to get our set
P P
of experts. This discretization incurs a loss that we can control (see Section 3.2 for the details).
Importantly, we relate the regret mentioned above to the expected average of the reverse KL di-
vergence over the rounds: E[1D (P P )]. Learning in reverse KL divergence D (P ) has
T KL ∗ || t KL ∗ ||·
implications for maximum likelihood and learning in metric distance functions such as the total
variation and Hellinger distance. Thereforbe, learning in the reverse KL divergence has received con-
siderable attention in machine learning. Once we control the average distance, using the convexity
of KL, we can show that the mixturedistribution 1 T P is also close to P . Finally, we manage
T t=1 t ∗
to translate the above bounds from expectation to high probability using the bounded difference
P
inequality due to McDiarmid. b
In particular, we use two different online algorithms: the Exponential Weighted Average (EWA)
algorithm and the Randomized Weighted Majority (RWM) algorithm. EWA returns us the mixture
61 T P which improperly learns P in (reverse) KL. RWM returns a random Bayes net P which
T t=1 t ∗
properly learns P in expected KL.
P ∗
b b
Efficient Learning of Restricted Classes of Bayes Nets Our learning algorithm for Bayes
nets mentioned above is sample-optimal but not time-efficient in general, since the number of
experts to be maintained is of exponential size. However, we observe that for special cases of
Bayes nets, we can efficiently sample from the experts according to the randomized strategy of
the algorithm. As a remark, the idea that the computational barrier of RWM or EWA may be
side-stepped by developing efficient sampling schemes which were also used in a recent work on fast
equilibrium computation in structured games [BHK+23] and partly motivated our work.
To see the simplest example of this idea, suppose = P ,...,P is a set of distributions
1 N
P { }
over [k], and let n = be a set of product distributions over [k]n. Each element
⊗
P P ×P ×···×P
of P⊗n is indexed as Pi for i = (i 1,...,i n), so that Pi(x) = n j=1P ij(x j). The size of P⊗n is clearly
Nn, so it is infeasible to work with it directly. The RWM algorithm maintains a distribution over
Q
⊗n, so that the probability that RWM picks Pi for its prediction P
t
at time t is proportional to
P
t s−=1 1Pi(x(s))η, where x(s) is the observed sample at time s and η > 0 is a parameter. Therefore:
b
Q
RP Wr M[P
t
= Pi]=
iQ′
t s−=
t
s1 1 −=P
1
1i P(x i′((s x) () sη
))η
=
j
Y=n
1 Q i′
jt s−=1
t
s1 −=P
1
1i Pj( ix
′
j(( js x)
(
j) sη
)
)η.
b
P Q P Q
The crucial observation is that RWM maintains a product distribution over product distributions,
and so we can sample each P from independently.
ij
P
When the underlying Bayes net is a tree, i.e. of indegree 1, we show that RWM samples a
random rooted spanning arborescence from a weighted complete graph. The probability to output
a particular arborescence A is proportional to w where each w is a weight that can be
e A e e
explicitly computed in terms of the observed samp∈les and the parameters of the algorithm. It is
Q
well-known that the matrix-tree theorem (more precisely, Tutte’s theorem) for counting weighted
arborescences can be used for this purpose, and hence, we obtain an alternative to the Chow-Liu
algorithm for approximately learning a tree Bayes net efficiently and sample-optimally.
Next, we generalize our algorithm to polytree-structured Bayes nets where the underlying skele-
ton is acyclic. Here, we are assuming that the skeleton is given, so that the goal of the algorithm
is to learn an acyclic orientation of the skeleton. For simplicity, suppose the skeleton is known
to be the path. Given a particular orientation of the edges, we obtain a particular Bayes net
structure. Once the structure is fixed, we can fix the conditional probability distributions for each
parent child. This will completely specify a Bayes net P, which can assign probability P(x(t))
→
for the sample x(t). Therefore, we can also compute the total loss ℓ = T logP(x(t)) 1. Then,
P t=1 −
each structure P will be chosen proportional to e −ηℓP in the RWM algorit Phm. In order to sample a
particularBayes netamongtheentireclass usingRWM,weneedtofirstcomputethenormalization
c po an thsta Bn at yo esf t nh ee tsR .W AM ps aa rm ticp ule lar r’s pd ai ts htri Bb au yt eio sn n: eZ
t
P:=
w
PilP l∈bPee c− hη oℓ sP enov wer ithth pe rc ol bas as biP lito yf ea −ll ηℓ( Pd /is Zcr bet yiz te hd e)
RWM’s sampler. We first show how to compute Z efficiently using dynamic programming.
For simplicity, let us assume Boolean alphabet, i.e. k = 2. As discussed before, due to the
factorization property of Bayes nets, given a fixed orientation T of the path skeleton, iterating over
all possible (discretized) conditional probabilities for each of the n variables is as good as iterating
over T-structured Bayes nets. For path graphs, the indegree is either 0, 1, or 2. With a slight
7abuse of notation, let be the set of all discretized distributions P(x) for a binary variable x; let
0
B
be the set of all discretized distributions P(x y) for binary variables x,y; let be the set of
1 2
B | B
all discretized distributions P(x y,z) for binary variables x,y,z.
|
We now show how to compute Z by induction on the set of vertices of the path. Suppose Z is
j
the normalization constant obtained by only restricting to the first j +1 nodes in the path. That
is, if is the class of all discretized Bayes nets on j + 1 variables with a path skeleton, then
j
P
iZ
nj
du=
ctionP ,∈Pwj
ee
−
mη aℓP in, taw inhe mre oℓ
rP
e
ro en fil ny ec dom inp fou rt mes att ih oe nl fo os rs eb aa cs hed j.on Let the first aj n+
d
1 vari ba ebl te hs.
e
F clo ar ssth oe
f
P Pj,
←
Pj,
→
all discretized Bayes nets on j +1 variables with a path skeleton and the last edge pointing left
and right, respectively. Correspondingly, define Z and Z ; clearly, Z = Z +Z . As the
j, j, j j, j,
← → ← →
inductive hypothesis, assume that Z and Z are already computed. We now need to compute
j, j,
← →
Z and Z .
j+1, j+1,
← →
If the (j +1)-th edge orients rightward, then the parents of nodes 1,...,j +1 do not change,
while the new node j +2 has parent j +1. We can accommodate this new edge by simply adding
the negative log of the conditional probabilities due to this new edge to the loss restricted to the
first j+1 variables. Computing
T
∆ = logP(B)(x(t) x(t) ) 1,
j+2 | j+1 −
Xt=1B X∈B1
we will have
Z = (Z +Z )e η∆.
j+1, j, j, −
→ ← →
If the (j +1)-th edge orients leftward, the adjustment is slightly trickier as node j +1 will get
a new parent j+2, while the new node j+2 has no parent. In that case, we need to first subtract
out the previous sum of negative log conditional probabilities at j +1. Let us define:
T T
∆ = logP(B)(x(t) x(t) ) 1, ∆ = logP(B)(x(t) ) 1,
1 j+1 | j − 2 j+1 −
Xt=1B X∈B1 Xt=1B X∈B0
T T
∆ = logP(B)(x(t) x(t) ,x(t) ) 1, ∆ = logP(B)(x(t) x(t) ) 1,
3 j+1 | j j+2 − 4 j+1 | j+2 −
Xt=1B X∈B2 Xt=1B X∈B1
T
∆ = logP(B)(x(t) ) 1.
5 j+2 −
Xt=1B X∈B0
If node j is not a parent of node j +1, then node j +1 contributed ∆ loss to Z while now it
2 j,
←
contributes ∆ loss to Z . Otherwise, it contributed ∆ loss to Z while now it contributes
4 j+1, 1 j,
← →
∆ loss to Z . The new node j+2 contributes ∆ loss to Z independentof what happens
3 j+1, 5 j+1,
← ←
to the other variables. Summarizing:
Z = Z e η(∆4 ∆2+∆5)+Z e η(∆3 ∆1+∆5).
j+1, j, − − j, − −
← ←· →·
It is easy to see that these updates can be performed efficiently using an appropriate dynamic
programming table. Once we have computed the total sum Z = Z +Z , sampling a structure
n, n,
← →
according to the sampler’s distribution can simply be done by suitably unrolling the DP table.
The argument described above extends to learning bounded indegree polytrees and bounded
indegree chordal graphs. For polytrees, the idea is illustrated in Figure 1. For chordal graphs, the
8A A A A
B C B C B C B C
D E F D E F D E F D E F
Figure 1: Givena rootedpolytree skeleton,for eachnode v, andforeachfixed orientationof edgesincident
to v, we maintain the total weight of all consistent orientations of the subtree rooted at v. Above, the
orientations of edges incident to B and C are fixed. This is needed when computing the weight for the
subtree rootedat A, since in the first two panels, the in-degreeof C change from 1 to 2, while in the second
two panels, C’s in-degree does not change.
algorithm firstbuildsaclique tree decomposition anduses this structurefor dynamicprogramming.
The obvious issue with chordal graphs is that some orientations may lead to cycles, unlike the case
for polytrees. However, chordal graphs enjoy certain nice property (see Lemma 5.6) that allows us
to independently perform weighted counting/sampling of acyclic orientations in each subtreeof the
clique tree.
1.3 Related Works
Hoffgen [Höf93] gave the first sample complexity bounds for agnostic learning of a Bayes net
withknownstructurefromsamples inKL divergence. Thiswork alsogave an efficient algorithm for
special cases such as trees using the classical Chow-Liu algorithm. Subsequently, Dasgupta [Das97]
gave an efficient algorithm for learning an unknown Bayes net (discrete and Gaussian) on a fixed
structure. This result was improved to a sample-optimal learning of fixed-structure Bayes nets in
[BGMV20, BGP+23].
The general problem of distribution learning of Bayes networks with unknown DAG structure
has remained elusive so far. It has not been shown to be NP-hard, although some related problems
and specific approaches are NP-hard [Chi95, CHM04, DL97, KS01]. Many of the early approaches
requiredfaithfulness,aconditionwhichpermitslearningoftheMarkovequivalenceclass,e.g. [SG91,
Chi02, FNP13]. Finite sample complexity of such algorithms has also been studied, e.g. [FY96].
Specifically for polytrees, [RP88, GPP90] studied recovery of the DAG for polytrees under the
infinite sample regime and in the realizable setting, while [CYBC24] gave finite sample complexity
for this problem. [GA21] studied the more general problem of learning Bayes nets, and their
sufficient conditions simplified in the setting of polytrees.
A notable prior work in the context of the current paper is the work by Abbeel, Koller, and
Ng [AKN06], which also explores the improper learning of Bayesian networks with polynomial
sample and time complexities. However, our research diverges from theirs in three critical ways:
firstly, their studydoesnotoffer any properlearningalgorithms; secondly, itlacks agnostic learning
guarantees; andthirdly,theirapproachdoesnotachieveoptimalsamplecomplexityintherealizable
setting. While they do demonstrate a “graceful degradation” as inputs deviate from the hypothesis
class, this does not equate to a definitive agnostic learning guarantee as provided in our work. On
a positive note, their research does attain polynomial sample and time complexities for learning
any Bayesian network with a bounded total degree in the realizable setting. It is worth noting
that our results for distributions with chordal skeleton are applicable even when the total degree is
9unbounded, provided that the indegree remains bounded, a scenario where the findings of Abbeel,
Koller, and Ng would not be applicable.
Online Learning of Structured Distributions To the best of our knowledge, our proposed
approach of learning high-dimensional distributions through online learning has not been reported
intheliteraturebefore. TheclosestwegetistheSparsitron algorithmbyKlivansandMeka[KM17]
which learns an unknown Ising model from samples. However, Sparsitron is typical to Ising models
where the conditional distribution at any component follows a logistic regression model which the
Sparsitron algorithm learns.
Robust Learning In the field of distribution learning, it is commonly assumed that all sam-
ples are consistently coming from an unknown distribution. However, real-world conditions often
challenge this assumption, as samples may become corrupted—either removed or substituted by
samples from an alternate distribution. Under such circumstances, the theoretical assurances of
traditional algorithms may no longer apply. This discrepancy has spurred interest in developing
robustlearning algorithms capable of tolerating sample corruption. Recent years have seen notable
advancements in this area, including the development of algorithms for robustly learning Bernoulli
product distributions [DKK+19], and enhancing the robustness of learning Bayes nets [CDKS18].
See [LRV16, DKK+17, DKK+18, BDLS17, HL18, KSS18, DKK+18, DKS18, CL21, CHL+23] and
the references therein for a sample of current works in this area. These works primarily focus on
guarantees with respect to the total variation distance.
Of particular relevance to us is the TV-contamination model. Here, if the distribution to be
learnt is P, one gets samples from a ‘contaminated’ distribution Q such that d (P,Q) η. Note
TV
≤
that this is a stronger model of contamination than Huber contamination [Hub92], where the noise
is restrictive to be additive, meaning that an adversary adds a limited number of noisy points to a
set of uncontaminated samples from P.
OnecaninterpretourresultsusingaKL-contamination model. Ifthedistributiontobelearntis
anunknownP promisedtobelongtoaclass ,thecontaminated distributionQissomedistribution
C
satisfying D (Q P) η. The noise is again non-additive, but the model is weaker than TV-
KL
k ≤
contamination. Any (η,A) approximation for Q with respect to yields a distribution P such that
C
D (Q P) (A+1)η. Therefore, we get that for Hellinger distance:
KL
k ≤
b
b H(P,P) H(P,Q)+H(P,Q) √η+ (A+1)η (2A+3)η.
≤ ≤ ≤
q q
Similarly, one can alsobbound d TV(P,P)b= O(√η) for constant A. To the best of our knowledge,
the KL-contamination model has not been explicitly considered before, but if one were to directly
apply the results of [CDKS18] with tbhe assumption that D (Q P) η, one would obtain a
KL
k ≤
distribution P such that d (P,P) = O( ηlog1/η), worse than ours by a log1/η factor which
TV
seems unavoidable using their approach [DKS22]. Moreover, their results require that be a class
p p C
of balanced Bbayes nets, a technicabl condition which is not needed for our analysis 2.
1.3.1 Open Problems
Our work opens up several interesting research avenues.
2ABayesnetissaid tobec-balanced forsomec>0ifallcoordinatesofthecorrespondingconditionalprobability
table are between c and 1−c.
10• An intriguing question is whether we can extend our result for chordal graphs of bounded
indegreetogeneralgraphsofboundedtreewidthandboundedindegree. Interestingly, Stanley
[Sta73] showed that counting the number of acyclic orientations reduces to the evaluation of
theTutte polynomialatthe point(2,0), and theTuttepolynomialcan beevaluated efficiently
for bounded treewidth graphs [Nob98, And98]. This is relevant because the weights that
EWA/RWMmaintain areinsomesenseaweighted countofthenumberof acyclic orientations
of the skeleton. However, we did not find a deletion-contraction recurrence for these weights,
and so their connection to the Tutte polynomial is unclear.
• Anotherimportantfollow-updirectionforlearningBayesnetswouldbetosearchover Markov
equivalence classes rather than DAG’s. A Markov equivalence class corresponds to the set of
DAGs that represent the same class of Bayes nets, and they can be represented as partially
directed graphs (essential graphs) that satisfy some special graphical properties. It would be
interesting to explore if the structure of essential graphs can be used to speed up weighted
counting and sampling; indeed, a very recent work [Sha24] gives a polynomial time algorithm
for uniformly sampling an essential graph that is consistent with a given skeleton.
• What is the role of approximate sampling in the context of distribution learning? So far,
in this work, we have only used exact sampling algorithms for spanning arborescences and
acyclic orientations. Can Markov chain techniques be brought to good use here? Our work
further motivates settling the complexity status of approximately counting the number of
acyclic orientations of an undirected graph; this question is a long-standing open problem in
the counting/sampling literature.
• Finally, while we have restricted ourselves to learning Bayes nets here, our framework is quite
general and also applies to learning other classes of distributions, such as Ising models and
factor models. We leave these questions for future work.
Organization of the paper The rest of the paper is organized as follows. In Section 2, we
present the preliminaries required for this work. Section 3 establishes the connection between
regret in online learning to KL divergence in the scenario of agnostic learning of distributions. It
also presents several necessary techniques from online learning along with the EWA and RWM
algorithms that will be used later in our work. In Section 4, we discuss our results on learning
tree-structured distributions, and present our alternative proper learning algorithm in AppendixC.
InSection 5, wepresentourresults onlearningchordal-structureddistributionsandin AppendixA,
wegivethelower boundoflearningtree-structureddistributions. Finally, inAppendixB,wedesign
efficient learning algorithms for Bayes nets over graphs with bounded vertex cover.
2 Preliminaries
For integers 0 < m n, let [n] denote the set 1,...,n , and let [m,n] denote the set m,m+
≤ { } {
1,...,n . For any η > 0, let exp (u) denote e ηu. For concise expressions and readability, we
} η −
use the asymptotic complexity notion of O(), where we hide poly-logarithmic dependencies of the
·
parameters. By stating i.i.d samples from a distribution P, we mean independently and identically
distributed samples from P. For a positivee integer ℓ, Unif([ℓ]) denotes the uniform distribution on
the set [ℓ], where each i [ℓ] is chosen with equal probability of 1/ℓ.
∈
112.1 Probability Distributions
We let ∆( ) denote the set of probability distributions over the elements of a set . Let P and
D D
Q be two such distributions over :
D
– The KL-divergence between P and Q is defined as: D (P,Q) = P(x)log P(x) .
KL x Q(x)
∈D
P
– The total variation (TV) distance between P and Q is defined as: d (P,Q) = P(x)
Q(x).
TV x ∈D| −
| P
Lemma 2.1 (Pinkser’s inequality). Let P and Q be two probability distributions defined over the
same sample space . Then the following holds:
D
D (P Q)
KL
d (P,Q) ||
TV
≤ s 2
We study distributions over high-dimensional spaces, which can require an exponential amount
of space to represent in general. So, for computational efficiency, we focus on distributions from
which we can generate samples efficiently. This notion is formally defined below.
Definition 2.2 (Efficiently samplable distribution). A distribution P is said to be efficiently sam-
plable if there exists a probabilistic Turing machine M which on input 0k produces a string x such
that Pr[M(0k) = x] P(x) 2 k and M runs in time poly(x +k).
−
| − | ≤ | |
Next we definegraphical models of interest in this work. For a directed graph G on n nodes, we
will identify its set of nodes with [n]. The underlying undirected graph of G is called the skeleton.
For any i [n], let pa (i) denote the set of the parents of node i and nd (i) denote the set of its
∈ G G
non-descendants. The subscript G may be removed if it is clear from the context.
Let us start with the definition of Bayesian networks.
Definition 2.3 (Bayesian networks). A probability distribution P over n variables X ,...,X is
1 n
said to be a Bayesian network (Bayes net in short) on a directed acyclic graph G with n nodes if3
for every i [n], X is conditionally independent of X given X . Equivalently, P admits the
i nd(i) pa(i)
∈
factorization:
n
P(x) = Pr [X = x]= Pr [X = x j pa(i),X = x ] for all x. (1)
i i j j
X P X P | ∀ ∈
∼ i=1 ∼
Y
It is well-known that Bayesian networks are efficiently sampleable.
Now we define tree-structured distributions, a subclass of Bayesian networks defined above.
Definition 2.4 (Tree-structured distribution). Let T be a tree, and G be any rooted orientation
of T. A probability distribution P is said to be T-structured if it is a Bayesian network on G. A
distribution P is said to be tree-structured if it is T-structured for some tree T.
Now we definethenotion of polytree-structured distributions, which generalizes tree-structured
distributions defined before.
3We usethe notation X to denote {X :i∈S} for a set S ⊆[n].
S i
12Definition 2.5 (Polytree-structured distribution). A directed acyclic graph (DAG) G is said to be
a polytree if the skeleton of G is a forest. For a positive integer k N, G is said to be a k-polytree if
∈
the in-degree of each nodein G is at most k. A distribution P is said to bea (k-)polytree-structured
if it is a Bayes net defined over some (k-)polytree G.
Finally, we define the notion of Chordal-structured distributions, a class of Bayes nets that
generalizes polytree-structured distributions and will be crucially used in this work.
Definition 2.6 (Chordal-structured distribution). An undirected graph G is said to be chordal if
every cycle of length at least 4 has a chord, that is, an edge that connects two non-adjacent vertices
on the cycle. A distribution P is said to be chordal-structured if it is a Bayes net defined over a
DAG whose skeleton is chordal.
Clearly, any tree-structured distribution is polytree-structured, and any polytree-structured
distribution is chordal-structured.
2.2 Clipping and Bucketing
We define two operations on probability distributions that will be useful in what follows. Let
P ∆([k]) for some positive integer k, and without loss of generality P(1) P(2) P(k).
∈ ≤ ≤ ··· ≤
Note that P(k 1) 1/2.
− ≤
Now we define the notion of Clipping below.
Definition 2.7. Given 0 < τ < 1/k2, if P(i) τ for 1 i < k, define Clip (P) = P. Otherwise,
≥ ≤ τ
let m such that P(m) < τ but P(m+1) τ, and define Clip (P) = Q where:
≥ τ
τ if 1 i m
≤ ≤
Q(i) = P(i) if m+1 i< k
1
k 1Q(j) if i = k
≤
−
j=−1
Note that Clip (P)(k) = P(k)
m
(τ
P
P(i)) 1 (k 1)τ τ for τ 1 .
τ − i=1 − ≥ k − − ≥ ≤ 2k2
Lemma 2.8. For any P ,P ∆P([k]), if τ 1/2k2 and Q = Clip (P):
∗ τ
∈ ≤
D (P Q) D (P P)+2k2τ.
KL ∗ KL ∗
k ≤ k
Proof. By definition, Q(i) P(i) for i < k and Q(k) P(k) kτ. Hence,
≥ ≥ −
k
P(i) P(k) P(k) 1/k
D (P Clip (P)) D (P P) = P (i)log log log log
KL ∗ k τ − KL ∗ k ∗ Q(i) ≤ Q(k) ≤ P(k) kτ ≤ 1/k kτ
Xi=1 − −
2k2τ
≤
where we used the facts that P(k) 1/k, log(1 x) 2x for x 1/2, and k2τ 1/2.
≥ − − ≤ ≤ ≤
Now we define the bucketing operation. Below, let Q = Clip (P) so that τ Q(1) Q(2)
τ
≤ ≤ ≤
Q(k 1) 1/2.
··· ≤ − ≤
13Definition 2.9. Given Q as above, and a parameter γ > 0, define Bucket (Q) to be a distribution
γ
R such that:
τ (1+γ) ⌊log 1+γ(Q(i)/τ)
⌋
if 1 i< k
R(i) = · ≤
(1
−
jk =−11R(j) if i = k
Lemma 2.10. Suppose 0 < τ < 1/2k2P and 0 γ < 1. For any P ,P ∆([k]), if Q = Clip (P),
≤ ∗ ∈ τ
and R = Bucket (Q), then:
γ
D (P R) D (P Q)+γ D (P P)+2k2τ +γ.
KL ∗ KL ∗ KL ∗
k ≤ k ≤ k
Proof. The second inequality follows from Lemma 2.8; so we focus on the first. Note that R(i)
≤
Q(i) for i < k, and so, R(k) Q(k).
≥
k k 1
Q(i) − Q(i)
D (P R) D (P Q) = P (i)log P (i)log log(1+γ) γ.
KL ∗ KL ∗ ∗ ∗
k − k R(i) ≤ R(i) ≤ ≤
i=1 i=1
X X
For future reference, note that using the notation above, R(k) Q(k) τ (for τ 1 ).
≥ ≥ ≤ 2k2
2.3 PAC Distribution Learning
A distribution learning algorithm takes as input a sequence of i.i.d. samples generated from a
probability distribution P and outputs a description Θ of a distribution P as an estimate for P.
Θ
In the following, is a family of probability distributions over [k]n, and P is a distribution over
C
[k]n not necessarily in . Let us first define the notion of approximation ofba distribution that will
C
be used in this work.
Definition 2.11 ((ε,A)-approximation of a distribution). A distribution P is said to be an (ε,A)-
approximation for P with respect to if:
C
b
D (P P) A inf D (P Q)+ε.
KL KL
k ≤ ·Q k
∈C
b
When A = 1, P is said to be an ε-approximation for P. Note that when P , if P is an
∈ C
ε-approximation for P, then d (P,P) √2ε using Pinsker’s inequality (Lemma 2.1).
TV
≤
Now we proceedbto define the notion of PAC-learning with respect to KL divergence. b
b
Definition 2.12 (PAC-learning inKLdivergence). Adistributionlearningalgorithm is said to4 be
an agnostic PAC-learner for with sample complexity m (n,k,ε,δ) and runningtime t (n,k,ε,δ),
if for all distributions P oveC r [k]n and all ε,δ (0,1), gC iven ε,δ, and a sample set oC f size m =
∈
m (n,k,ε,δ) drawn i.i.d. from P, the algorithm runs for time t t (n,k,ε,δ) and outputs the
C ≤ C
descriptionΘofadistributionP suchthatwithprobabilityatleast1 δ, P isanε-approximation
Θ Θ
−
for P with respect to (where the probability is taken over the samples as well as the algorithm’s
C
randomness). b b
If P is restricted to be in , the algorithm is said to be a realizable PAC-learner for . If
C C
the output P is guaranteed to be in , the algorithm is said to be a proper PAC-learner for ;
Θ
C C
otherwise the learner is called an improper PAC-learner.
b
4In the learning theory literature, when A>1, such a guarantee is also sometimes called semi-agnostic learning.
142.4 Online Learning
The framework of prediction with experts is setup as follows; the formulation we follow here is
based on Chapter 2 of [CBL06]. The goal is to design an algorithm that predicts an unknown
A
sequence x(1),x(2),... of elements of an outcome space . The algorithm’s predictions P ,P ,...
1 2
X
belongtoadecision space (whichinourcasewillbetheprobabilitysimplexon ). Thealgorithm
D X
makes its predictions sequentially, and the quality of its predictions is benchmarkedbagabinst a
A
set of reference predictions called experts. At each time instant t, has access to the set of expert
A
predictions , where is a fixed subset of . makes its own prediction P based on the expert
t
E E D A
predictions. Finally, after the prediction P is made, the true outcome x(t) is revealed, and the
t
algorithm incurs a loss ℓ(P ,x(t)), where ℓ : R is a non-negativebloss function.
t
A D×X →
Note that x(1),x(2),... form an arbitrabry sequence. Hence, naturally, the loss incurred by the
algorithm depends on how wbell the experts fit these outcomes.
Now we are ready to define the notion of regret.
Definition 2.13 (Regret). Given the notation above, the cumulative regret (or simply regret) over
T steps is defined as:
T T
Reg ( ; ) = ℓ(P ,x(t)) min ℓ(E,x(t))
T A E t −E
t=1 ∈E t=1
X X
Since is usually fixed, we often use the shb orthand Reg ( ). The average regret is defined as
T
E A
1Reg ( ). The total T steps are also sometimes referred to as the horizon of the algorithm.
T T A
Our work utilizes two standard online learning algorithms: Exponentially Weighted Average
(EWA) and Randomized Weighted Majority (RWM). Both algorithms assume that the expert set is
finite; the first algorithm is deterministic, while the second one is probabilistic.
Fact 2.14. In both EWA and RWM algorithms, when run with a hyperparameter η > 0 and a set
of experts = E ,...,E , the weight of the i-th expert (i [N]) after time step t (t [T]) is
1 N
E { } ∈ ∈
denoted by w defined as follows:
i,t
t
w , exp η ℓ(E ,x(s)) .
i,t i
− !
s=1
X
Algorithm 2: RWM forecaster
Algorithm 1: EWA forecaster
Input: Experts = E ,...,E ,
Input: Experts = E 1,...,E N , E { 1 N }
E { } parameter η, horizon T.
parameter η, horizon T.
1 w 1 for each i [N].
1 w i,0 1 for each i [N]. i,0 ← ∈
← ∈ 2 for t 1 to T do
2 for t 1 to T do ←
3 P t← ← PN i= N j1 =w 1wi,t j− ,t1 −E 1i. 3 S Pam r[Ppl te =P bt E∈ i]E =wh
N
je w =r i 1e ,t w− j1 ,t−1.
4 Obutpu Pt P t.
4 Outpbut P t.
P
5 Observe outcome x(t).
5 Observe outcome x(t).
b
6 for i [N] do b
∈ 6 for i [N] do
7 w ∈
i,t ← 7 w i,t
w exp η ℓ(E ,x(t)) . ←
i,t −1 · − · i w i,t 1 exp η ℓ(E i,x(t)) .
(cid:16) (cid:17) − · − ·
(cid:16) (cid:17)
15Before proceeding to give the regret bounds for EWA and RWM algorithms, we need the notion
of exp-concave loss functions.
Definition 2.15 (Exp-concave loss function). Let be a convex decision space, be a sample
D X
space, and let α R be a parameter. A loss function ℓ : R is said to be α-exp-concave if
∈ D×X →
the function f : R, f (P) = exp( αℓ(P,x)) is concave for all x .
x x
D → − ∈ X
The following regret bound holds for EWA with exp-concave loss functions. Importantly for us,
ℓ(P,x) = log(P(x)) is exp-concave for 0 α 1.
− ≤ ≤
Lemma 2.16 ([CBL06], Theorem 3.2). With N = experts and an α-exp-concave loss func-
|E|
tion ℓ(P,x), for any sequence of outcomes x(1),...,x(T), the regret of the EWA forecaster with the
parameter η = α is bounded as follows:
logN
Reg (EWA; ) .
T E ≤ η
The regret bound below for RWM assumes the loss function is bounded.
Lemma 2.17 ([CBL06], Lemma 4.2). With N = experts and a loss function bounded in
|E|
[ L ,L ], for any sequence of outcomes x(1),...,x(T), the expected regret of the RWM fore-
max max
−
caster with the parameter η = 8(logN)/T is bounded as follows:
p
T logN
E[Reg (RWM; )] L .
T E ≤ max s 2
Moreover, with probability at least 1 δ, the following holds:
−
T logN T 1
Reg (RWM; )] L + log .
T E ≤ max s 2 s2 δ
 
3 Agnostic Learning of Bayesian Networks
In this section, we study the notions and our results on the agnostic learning of Bayesian
networks. WestartwithdescribingtherelationbetweenregretandKLdivergence,whichiscrucially
used throughout this work.
3.1 Connection between Regret and KL divergence
In the prediction with experts framework, let us fix the decision space to be the probability
D
simplexontheoutcomespace andthelossfunctiontobeℓ(P,x) = logP(x)forthedistribution
X −
P definedovertheprobabilitysimplexon . Thenwehavethefollowinggeneralconnectionbetween
X
the expected average regret and the expected KL divergence when the outcomes x(1),x(2), are
···
i.i.d samples from a distribution P .
∗
Lemma 3.1. If is a (possibly randomized) online learning algorithm that receives i.i.d samples
A
x(1),...,x(T) from aprobability distribution P on and makespredictions P ,...,P ∆( )
∗ 1 T
X ∈ D ⊆ X
with respect to an expert set ,
C ⊆ D
b b
1
E E D (P P ) E [Reg ( ; )]+minD (P P).
x(1),...,x(T)t Unif([T]) KL ∗ k t ≤ T x(1),...,x(T) T A C P KL ∗ k
∼ h i ∈C
b
16Note that if is a randomized algorithm, both the left and right hand side of the inequality in
A
Lemma 3.1 are random variables.
Proof. From the definition of regret (Definition 2.13), we have:
T T
1 1
Reg ( ; ) , log min log .
T A C P (x(t)) − P P(x(t))
t=1 t ∈C t=1
X X
Thus, the expectation over the samples of thebaverage regret for T rounds would be the following:
T T
1 1 1 1
E [Reg ( ; )] = E log min log
T x(1),...,x(T) ∼P∗ T A C x(1),...,x(T) T ·
Xt=1
P t(x(t)) − P
∈C Xt=1
P(x(t))!
T
1 1 1
E log b min E log (2)
≥ T x(1),...,x(T)"
Xt=1
P t(x(t))#−P ∈Cx ∼P∗
(cid:20)
P(x)
(cid:21)
T
1 1
= E log b minD (P P) H(P )
KL ∗ ∗
T x(1),...,x(T)"
t=1
P t(x(t))#−P
∈C
k −
X
where the second-last step is by applying Jensen’s inequbality, as min is a concave function, and
also uses the fact that x(1),...,x(T) P . Also, H(P ) denotes the entropy of the distribution P .
∗ ∗ ∗
∼
Thus we have the following:
T
1 1 1
E [Reg ( ; )] E log minD (P P) H(P )
T x(1),...,x(T) T A C ≥ T x(1),...,x(T)"
t=1
P t(x(t))#− P
∈C
KL ∗ k − ∗
X
T
1 1
= E E b log x(1),...,x(t 1) H(P )
− ∗
T Xt=1x(1),...,x(t−1)x ∼P∗" P t(x)
(cid:12)
#−
minD KL(P ∗ P)(cid:12) (cid:12)
−P bk
∈C
1
= E D (P P ) minD (P P)
KL ∗ t KL ∗
T
Xt
x(1),...,x(t−1)
h
k i− P
∈C
k
T b
1
= E D (P P ) minD (P P)
KL ∗ t KL ∗
x(1),...,x(T−1)"T
Xt=1
k #− P
∈C
k
b
In the second line, inside the summation over t, the expectation does not include x(t) because the
prediction P does not depend upon x(t), as P is predicted before x(t) is revealed .
t t
We immbediately have the following corollabry:
Corollary 3.2. In the same setup as Lemma 3.1 above, if 1Reg ( ; ) ρ,
T T A C ≤
T
1
E D P P ρ+minD (P P).
KL ∗ t KL ∗
x(1),...,x(T)" kT
t=1
!# ≤ P
∈C
k
X
b
Proof. Thisfollows fromapplyingthefactthatKL-divergenceisconvex initssecondargument.
Now, we apply these results to the EWA and RWM algorithms. Let us start with the results for
the EWA algorithm.
17Lemma 3.3. In the same setup as Lemma 3.1 above, suppose that for every P , min P(x)
x
∈ C ∈X ≥
τ holds.
• If is the EWA algorithm run with parameter η =1, then we have the following:
A
T
1 log
E D
KL
P
∗
P
t
minD KL(P
∗
P)+ |C|.
x(1),...,x(T)" kT
t=1
!# ≤ P
∈C
k T
X
b
• For any ε,δ (0,1), let be the EWA algorithm run with parameter η = ε .
∈ A 2√Tlog(1/τ)√log(1/δ)
Then with probability at least 1 δ, the following holds:
−
T
1 2log log(1/τ) log(1/δ)
D KL P ∗ P t minD KL(P ∗ P)+ |C| +ε.
kT t=1 ! ≤ P ∈C k ε√T p
X
b
Proof. Recall that our loss function ℓ(P,x) = logP(x) is η-exp-concave for all 0 η 1. Hence,
− ≤ ≤
we can apply Lemma2.16 for any such η. Thefirstitem follows from Corollary 3.2 by setting η = 1.
For the second item, we use the method of bounded differences. Define the function f:
T
1
f(X(1),...,X(T)) = D P P
KL ∗ t
kT !
t=1
X
b
Claim 3.4. For any X(1),X(2),...,X(T),X(1)′ ,
∈ X
1
f(X(1),X(2),...,X(T)) f(X(1)′,X(2),...,X(T)) 2ηlog .
| − | ≤ τ
Proof. Let P and P be the outputs from the EWA algorithm after t rounds using the samples
t t
X(1),X(2),...,X(T) and X(1)′,X(2),...,X(T) respectively. Then, P
t
=
Q
w t,QQ and P
t′
=
w Qb. b ∈C
Q t′,Q P
∈C b b η
P Q(X(s))
BydefinitionoftheEWAalgorithmandthechoiceofourlossfunction,w = s≤t
t,Q (cid:16) (cid:17) η
Q R(X(s))
R∈C s≤t
Q(X(1)′ ) Q(X(s)) η P (cid:16) Q (cid:17)
and w = 2≤s≤t . Since for every Q , τ Q(X(1)) 1, τ2η wt,Q
t′,Q (cid:16) R(X(1)Q′) R(X((cid:17) s)) η ∈ C ≤ Q(X(1)′) ≤ τ ≤ w t′ ,Q ≤
R∈C 2≤s≤t
τ 2η and henPce for
every(cid:16)
yQ , τ2η
(cid:17)P t′(y)
τ 2η. Therefore:
− −
∈ X ≤ Pt(y) ≤
b
f(X(1)′,...,X(T))
f(X(1),...,b
X(T)) = P ∗(y)log
T t=1P t′(y)
2ηlog
1
.
(cid:12) (cid:12) (cid:12)
−
(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)y X∈X P
PT t=1P
b
bt(y)!(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
≤ τ
(cid:12) (cid:12)
Applying the McDiarmid’s inequality,
2ε2
Pr D (P P) E D (P P) ε 2exp δ,
KL ∗ || − KL ∗ || ≥ ≤ −4Tη2log2τ −1 ! ≤
h(cid:12) h i(cid:12) i
(cid:12) b b (cid:12)
byourchoiceofη(cid:12). Theseconditemthenfollows(cid:12)fromLemma2.16,Corollary3.2,andtheabove.
18Now let us discuss the guarantees of the RWM algorithm.
Lemma 3.5. In the same setup as Lemma 3.1 above, suppose that = P ,...,P and that for
1 N
C { }
every P , min P (x) τ holds.
i x i
∈ C ∈X ≥
• If is the RWM algorithm run with the parameter η = (8logN)/T — giving proper
A
predictions P ,...,P — and P P where t is sampled uniformly from [T], then
1 T ∈ C ← t ∈ C p
we have:
b b b b
log(1/τ) T log
E D
KL
P
∗
P minD KL(P
∗
P)+ |C|.
,x(1),...,x(T) k ≤ P k pT
A h (cid:16) (cid:17)i ∈C
b
• With the same algorithm as above, for any δ (0,1), the following bound holds with
A ∈
probability 1 δ over the randomness of RWM, the random choice of t from [T], and the
≥ −
randomness of the samples x(1),...,x(T):
log(1/τ) log + log(2/δ)
D (P P) minD (P P)+ |C| .
KL ∗ k ≤ P KL ∗ k (cid:16)p δ√T p (cid:17)
∈C
b
Proof. ThefirstitemfollowsfromLemma3.1,takingtheexpectation ofbothsidesovertherandom-
ness of , and the expected regret bound in Lemma 2.17. Note that we can take L = log(1/τ)
max
A
since P(x) τ for all x and P , by the assumption of the lemma.
≥ ∈X ∈ C
TheRWMalgorithm(seeAlgorithm2)canequivalentlybeviewedasfollows. SampleU ,...,U
1 T
∼
Uniform((0,1]) independently. At each t [T], predict P P , where i [N] is the index
∈
t
←
it
∈ C
t
∈
such that U
t ∈
Pji
N
kt = =− 1 11 ww kj ,, tt −− 11 ,
PN
ki jt == 11 ww kj, ,t t− −1 1#. Note that mobdulo the choice of U = (U 1,...,U T), the
RWM algorithm is deterministic.
P P
From Lemma 2.17 (high probability bound), with probability 1 δ/2 over the random choice
≥ −
of U, Regret T( A; C)
≤
log(1/τ) Tlo 2g |C| + T
2
log 2
δ
. Conditioning on this event E, applying
Lemma 3.1, we get (cid:18)q q (cid:19)
log(1/τ) log + log(2/δ)
E E , D (P P ) minD (P P) |C|
x(1),...,x(T)t ∼Unif([T])(cid:20)ZX,t KL ∗ k t − P ∈C KL ∗ k (cid:12) (cid:12)E (cid:21) ≤ (cid:16)p 2√T p (cid:17)
b (cid:12)
Note that, since P for all t [T], 0 for all
x(cid:12)(1),...,x(T)
and t [T]. Thus, we
t X,t
∈ C ∈ Z ≥ ∈ X ∈
can use Markov’s inequality to show that, conditioned on the event , with probability 1 δ/2
E ≥ −
over the randomnessbof x(1),...,x(T) and the random choice of t Unif([T]),
∼
log(1/τ) log + log(2/δ)
D (P P) minD (P P) |C| . (3)
KL ∗ k − P KL ∗ k ≤ (cid:16)p δ√T p (cid:17)
∈C
b
Taking a union bound with event and the failure event of the above bound (3), we can see that
E
(3) holds with probability 1 δ over the randomness of algorithm , the random choice of t, and
≥ − A
the randomness of the samples x(1),...,x(T). This completes the proof of the lemma.
193.2 Discretization
We will typically want to design agnostic PAC-learners for classes of distributions that are
C
infinite,e.g.,tree-structureddistributions,etc. However, toapplytheresultsoftheprevioussection,
we first need to finitize . In the current section, we show how to accomplish this when is the
C C
class of Bayes nets over a fixed DAG G.
Definition 3.6. Given ε (0,1) and variables Y and X 1,...,X
d
over [k], let usdefine
εY |X1,...,Xd:
∈ N
the ε-discretization of Y (X ,...,X ). Let τ = ε and γ = ε. For each σ [k]d, let
| 1 d 4k2 4 ∈
εY |(X1,...,Xd)=σ
equal the following family of distributions over Y:
N
1
P ∆(Y) P (j) τ, ℓ ,...,ℓ 0, log , i= j,P (i) = τ(1+γ)ℓi ,
σ ∈ | σ ≥ ∀ 1 k ∈ 1+γ 2τ ∀ 6 σ
j[∈[k](cid:26) (cid:20) (cid:22) (cid:23)(cid:21) (cid:27)
and let εY |X1,...,Xd be the Cartestian product of εY |(X1,...,Xd)=σ over all σ [k]d.
N N ∈
Definition 3.7. Let G be a DAG with n nodes. For an alphabet size k 2 and parameter
≥
ε (0,1), given variables X ,...,X , we define the following family of probability distributions
1 n
∈
over (X ,...,X ):
1 n
G = Xi|Xpa(i)
Nε Nε/n
iY∈[n]
where the product is a Cartesian product. Each element of G can identified as a Bayes net on G.
Nε
Now we have the following lemma which bounds the size of log G .
Nε
(cid:12) (cid:12)
Lemma 3.8. Suppose G has in-degree at most d. Then: (cid:12) (cid:12)
(cid:12) (cid:12)
nk
log G = O nkd+1log .
Nε ε
(cid:12) (cid:12) (cid:18) (cid:19)
(cid:12) (cid:12)
Proof. Using the fact that log(1+γ)(cid:12) γ/(cid:12)2 for 0 γ 1, we can say the following:
≥ ≤ ≤
G = n Xi|Xpa(i)=σ 8nk log 8nk2
nkd+1
64n2k3
nkd+1
.
(cid:12) (cid:12)Nε (cid:12)
(cid:12)
i Y=1σ ∈[kY]|pa(i)|(cid:12) (cid:12)Nε/n (cid:12)
(cid:12)
≤ ε ε ! ≤ ε2 !
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)
Taking log on the above expression, we have the result.
n
Lemma 3.9. Any P G satisfies min P(x) ε .
∈ Nε x ∈[k]n ≥ 4nk2
(cid:16) (cid:17)
Proof. Since the conditional probability distribution at each node satisfies that its minimum prob-
ability value is at least ε/(4nk2) by definition of G, and the probability mass function of a Bayes
Nε
net is defined as the product of the conditional probabilities at each node, the result follows.
Lemma 3.10. Let G ∆([k]n) be the set of all Bayes nets defined on a given DAG G with n
C ⊆
nodes and maximum in-degree d. For any P ∆([k]n), the following holds:
∗
∈
min D (P P) min D (P P)+ε.
KL ∗ KL ∗
P G k ≤ P G k
∈Nε ∈C
20Proof. Given the DAG G, for a distribution Q ∆([k]n), a node i [n], and an assignment
∈ ∈
σ [k]pa(i) to the parents of the node i, let Q ∆([k]pa(i)) be the marginal distribution of Q
| | pa(i) | |
∈ ∈
on pa(i), and let Q ∆([k]) denote the conditional distribution of Q on node i, conditioned on
i,σ
∈
its parents having value σ. Note that here, Q itself need not be a Bayes net.
Suppose P G minimizes D (P P). Let R = Bucket Clip (P ), where τ and γ are as in
∈ C KL ∗ k i,σ γ τ i,a
Definition 3.7, and let R be the Bayes net defined by having the conditional distributions at each
node be R :
i,σ
n
Pr [X = x] = R (x ) for all x.
X R
i,xpa(i) i
∼ i Y=1
We first claim that R belongs to G. This is because by the definition of the clipping and
Nε
bucketing operations, each R satisfies that for all values j in [k] except one, R (j) is in the
i,σ i,σ
interval [τ,1/2] and equals τ times a non-negative power of (1 + γ) and at the exceptional j,
R (j) τ (by the remark after Lemma 2.10). So, G must contain R.
i,σ ≥ Nε
Using Lemma 2.10, D (P R ) D (P P )+ε/n. We now use the following fact:
KL i∗,σk i,σ
≤
KL i∗,σk i,σ
Fact 3.11 (Lemma 3.3 in [BGP+23]). Suppose P,Q ∆([k]n), and Q is a Bayes net on a DAG G.
∈
Then the following holds:
D (P Q) = J + P (σ) D (P Q ), (4)
KL P,G pa(i) KL i,σ i,σ
k · k
iX∈[n]σ ∈[Xk]|pa(i)
where J is a constant that depends on P and G but not on Q.
P,G
Therefore, we can say that:
D (P R) = J + P (σ) D (P R )
KL ∗
k
P∗,G p∗a(i)
·
KL i∗,σk i,σ
iX∈[n]σ ∈[Xk]|pa(i)
J + P (σ) (D (P Q )+ε/n)
≤
P∗,G p∗a(i)
·
KL i∗,σk i,σ
iX∈[n]σ ∈[Xk]|pa(i)
= D (P Q)+ε.
KL ∗
k
3.3 Sample Complexity for learning Bayes Nets
We can now collect the tools developed in Sections 3.1 and 3.2 to prove Theorem 1.1 from
the introduction. The algorithm that yields the first part of Theorem 1.1 (improper learning) is
quite simple to describe (Algorithm 3 below), corresponding to EWA forecaster with η = 1. In the
following theorem, we specify the and T to take so as to obtain desired guarantees.
N
21Algorithm 3: EWA-based learn-
ing for Bayes nets
Input : = P ,...,P ,T, Algorithm 4: RWM-based learning
1 N
N { }
hyperparameter η > 0. for Bayes nets
Output: Sampler for P. Input : = P ,...,P ,T,
1 N
N { }
1 w 1 for each i [N]. hyperparameter η > 0.
i,0
← ∈
2 for t 1 to T do b Output: P .
← ∈ N
3 Observe sample x(t) P . 1 w 1 for each i [N].
∗ i,0
∼ ← ∈
4 for i [N] do 2 for t 1 tobT do
∈ ←
5 w i,t w i,t 1 P i(x(t))η. 3 Sample i t from [N] with
6 function E← WA-S−am· pler() Pr(i t = i) = wi,t−1 .
j∈[N]wj,t−1
7 Sample t [T] uniformly at
random.← 4 Observe samp Ple x(t)
∼
P ∗.
5 for i [N] do
8 Sample i [N] with ∈
probabili∼
ty
wi,t−1
.
6 w i,t
←
w i,t −1 ·P i(x(t))η.
j∈[N]wj,t−1 7 Sample t uniformly from [T].
109 reture rt nu Ern Wx
A∼
-SP aPi m.
pler
8 return P
←
P it.
b
/* This is a sampler for P.
*/
b
Theorem 3.12 (Bayes net improper learning). Let be the class of distributions over [k]n that
C
can be defined as Bayes nets over an unknown DAG with n nodes and in-degree d.
• Thereisanalgorithm thatforallP ∆([k]n)andallε (0,1), receivesm = O nkd+1 log nk
∗ ∈ ∈ ε ε
i.i.d. samples from P and returns a distribution P that is with probability at(cid:16)least 2/3, an(cid:17)
∗
(ε,3)-approximation for P with respect to .
∗
C
b
• There is a realizable PAC-learner for with sample complexity O nkd+1 log nk .
C εδ εδ
(cid:16) (cid:17)
• There is an agnostic PAC-learner for with sample complexity O(n4k2d+2ε 4log(1/δ)).
−
C
Proof. Let be the collection of all DAGs on n nodes that have in-deegree d, then we can say that
G
(n!)d+1. Let = G , using the notation of Definition 3.7. Then, using Lemma 3.8, we
|G| ≤ N Nε/6
G
have: S∈G
nk nk
log = O log +nkd+1log = O nkd+1log . (5)
|N| |G| ε ε
(cid:18) (cid:19) (cid:18) (cid:19)
By Lemma 3.10 and the definition of ,
N
minD (P P) minD (P P)+ε/6. (6)
KL ∗ KL ∗
P k ≤ P k
∈N ∈C
We begin with the first item. We run EWA-based learning (Algorithm 3) using as the expert
N
set, η = 1, and loss function ℓ(P,x) = logP(x). By the first part of Lemma 3.3 and Markov’s
−
inequality, with probability at least 2/3:
T
1 log
D KL P ∗ P t 3 minD KL(P ∗ P)+ |N| . (7)
kT
Xt=1
! ≤ (cid:18)P
∈N
k T
(cid:19)
b
22Using(5),(6)andourchoiceofT nowensuresthatright-hand-sidein(7)isatmost3min D (P P)+
P KL ∗
∈C k
ε.
Fortheseconditem,let = G . ByLemma3.10,ifP ,min D (P P) εδ/6.
N′ G Nεδ/6 ∗ ∈ C P ∈N KL ∗ k ≤
We can now run the same argumeSn∈tGas above with ε being εδ to obtain the result.
We now prove the third item. Suppose ε,δ (0,1). By Lemma 3.9, for every P ,
∈ ∈ N
min
x
∈[k]nP(x)
≥
24nε
k2
n . WerunEWAusing
N
astheexpertset, η = 4nlog |N|log(2 ε4 √n Tk2/ε)√log(1/δ)
and the same loss(cid:16)functi(cid:17)on. By the second part of Lemma 3.3, we get that with probability at least
1 δ:
−
1 T 4nlog log(24nk2/ε) log(1/δ) ε
D KL P ∗ P t minD KL(P ∗ P)+ |N| + (8)
kT t=1 ! ≤ P ∈N k ε√T p 2
X
b
Using (5) and (6), if T = Θ n4k2d+2ε 4log4(nk/ε)log(1/δ) , then the right-hand-side of (8) is at
−
most min P D KL(P ∗ P)+ε(cid:16). (cid:17)
∈C k
TheabovetheoremisinterestinginthecontextofimproperlearningofBayes netswithconstant
in-degreed, becauseitgives anearly-optimal samplecomplexity (optimal uptologarithmic factors)
for realizable PAC learning with constant success probability (second part) and for getting a (ε,3)-
approximation with constant success probability (first part).
While the next theorem does not have significant advantages for general in-degree d Bayesian
networks due to the less efficient nature of Algorithm 4 and its higher sample complexity compared
toestablishedalgorithmssuchasChow-Liuforproperlearning,weproveittolaythegroundworkfor
subclassesofBayesian networks (suchas trees, polytrees, andchordalgraphs). For thesesubclasses,
the sampling method employed in Algorithm 4 can be optimized to yield efficient algorithms for
proper learning.
Theorem 3.13. Let be the class of distributions over [k]n that can be defined as Bayes nets over
C
an unknown DAG with n nodes and in-degree d. There is a proper agnostic (ε,δ)-PAC-learner for
with sample complexity O n3kd+1 log3 nk for any δ (0,1) and ε > 0.
C δ2ε2 ε ∈
(cid:16) (cid:16) (cid:17)(cid:17)
Proof. Define = Ge — where is the collection of all DAGs on n nodes with in-degree
N Nε/2 G
G
d — the same way asS∈Gin the proof of Theorem 3.12. We run the RWM-based learning algorithm
(Algorithm 4) with as the expert set, η = (8log )/T, and the log loss function. By the
N |N|
second part of Lemma 3.5, and noting that:
p
(i) ThemaximumlossatanyroundisatmostL = log(1/τ) = nlog(8nk2/ε)(usingLemma3.9).
max
(ii) the size of the cover satisfies log O nkd+1log nk (using Lemma 3.8 and the fact
N |N| ≤ ε
that log (d+1)nlogn). (cid:16) (cid:17)
|G| ≤
we have that, for any δ > 0, with probability 1 δ over the random samples as well as the
≥ −
randomness of the algorithm, we have:
nlog 8nk2 C nkd+1log(nk/ε)+ log(2/δ)
ε ·
D KL(P ∗ kP) ≤ PminD KL(P ∗ kP)+ (cid:16) (cid:17)(cid:18)q δ√T p (cid:19),
∈N
b
23where C is some constant 1.
≥
Using Lemma 3.10, the definition of , and the fact that = G , we have that
C N G ∈GNε/2
S
nlog 8nk2 C nkd+1log(nk/ε)+ log(2/δ)
ε ·
D KL(P ∗ kP) ≤ m P inD KL(P ∗ kP)+ (cid:16) (cid:17)(cid:18)q δ√T p (cid:19) +ε/2.
∈C
b
8Cn2log2 8nk2 (nkd+1log(nk/ε)+log(2/δ))
Now if we choose any T ε , we get D (P P) ε/2 +
≥ (cid:16) (cid:17) δ2ε2 KL ∗ k ≤
ε/2 = ε with probability 1 δ (this bound for T uses the fact that (√x+√y)2 2(x+y) by the
≥ − ≤
AM-GM inequality). This gives us the O n3kd+1 log3 nk sample complexity for propb er agnostic
δ2ε2 ε
(ε,δ)-PAC learning. (cid:16) (cid:16) (cid:17)(cid:17)
e
4 Learning Tree-structured distributions
Let us start with the definition of arborescence, which will be used throughout this section.
Definition 4.1 (Arborescence). A directed graph G = (V,E) is an out-arborescence rooted at
v V if its skeleton (underlying undirected graph) is a tree (acyclic and connected) and there is a
∈
unique directed path from v to w for every w V v . An in-arborescence rooted at v is similar,
∈ \{ }
but with unique directed paths from all w V v to v.
∈ \{ }
GivenavertexsetV = [n],let denotethesetofout-arborescencesrootedatnode1.Notethat
T
G
each G will have m = n 1 edges. Any tree-structured distribution P on [k]n is G-structured
T
for
som∈
e
GG
. Let
Tree−
denote the set of all tree-structured distributions on [k]n. Now we
T
∈ G C
have the following lemma.
Lemma 4.2. If we consider the finite set Tree = G of distributions on [k]n (see Defini-
tion 3.7), then for any distribution P
∆N ([ε
k]n),
G ∈GTNε
∗ ∈ S
min D (P Q) min D (P Q)+ε,
KL ∗ KL ∗
Q Tree k ≤ Q Tree k
∈Nε ∈C
The size is bounded by log Tree O(nk2log(nk/ε)).
|Nε | ≤
Proof. From Lemma3.8andLemma3.10, foranyG , wecan discretize thesetof G-structured
T
∈ G
distributions to a finite set G of size log G O(nk2log(nk/ε)) such that, for any distribution
Nε |Nε | ≤
P on [k]n, min D (P Q) min D (P Q)+ε.
∗ Q ∈NεG KL ∗ k ≤ G-structured distributions Q KL ∗ k
Thus, if we take Tree G, then the following holds:
Nε ← G ∈GTNε
log Tree maxlog GS +log O(nk2log(nk/ε))+O(nlogn) O(nk2log(nk/ε)),
|Nε |≤ G |Nε | |GT | ≤ ≤
and the guarantee on the minimum KL divergence from P comes from the definition of tree-
∗
Tree
structured distributions and the construction of .
Nε
244.1 Sampling from the EWA/RWM Distribution
In this section, we describe our approach for sampling tree-structured distribution using EWA
and RWM algorithms.
Tree
FollowingDefinition3.7,eachtree-structureddistributionP in canbefactoredas(G,V )
Nε P
whereGistheunderlyingrootedarborescenceandV = (p : e E(G),p )isthevectoroffunctions
P e 1
∈
where p (x ,x ) for e = (i,j) corresponds to Pr (X = x X = x ) and p (x ) corresponds
e i j X P j j i i 1 1
∼ |
to Pr (X = x ), where X denotes the root node of G. Using the notation of Definition 3.6,
X P 1 1 1
p
1
∈
N∼ εX /n1 and for e = (i,j), p
e
∈
NεX /nj|Xi. For simplicity, we will call these sets N1 and Ne
respectively in this section. Let us start with our result for EWA algorithm.
Tree
Lemma 4.3. Let P be the output of the EWA algorithm run with the expert set , parameter
t Nε
η, and horizon T t for time step t. For any sequence of observed samples x(1),...,x(T) [k]n,
≥ ∈
one can generate absample from P in polynomial time for bounded k.
t
Tree
Proof. In the EWA algorithm, P is a mixture of the distributions in . For a distribution
Tree
tb Nε
P that factors into (G,(p : e E(G),p )) as above, let the weight of P in P be ω(P).
∈ Nε e ∈ 1 t
We have the following: b
b
ω(P)
(s) (s)
exp log p (x ) p (x )
= s<t η 1 1 · e ∈E(G) e e
Q (cid:16) exp Qlog q (x(s) ) (cid:17) q (x(s) )
H ∈GT qe ∈Ne:e ∈E(H),q1 ∈N1 s<t η 1 1 · e ∈E(G) e e
P P exp logp (xQ(s) ) (cid:16) expQlogp (x(s) ) (cid:17)
= s<t η 1 1 · e ∈E(G) s<t η e e
(s) (s)
q1 ∈N1 s<Q texp ηlogq 1(x 1 ) · H ∈Q GT e ∈EQ (H) qe ∈Ne s<texp ηlogq e(x e )
=
P s<Q texp ηlogp 1(x( 1s) ) P
e
∈Q
E(G)
qeP
∈Ne
s<Q texp ηlogq e(x( es) )
q1 ∈Q N1 s<texp ηlogq 1(x( 1s) ) · H ∈Q GT e ∈EP (H) qe ∈Q Ne s<texp ηlogq e(x( es) )·
(s)
P Q exp logpP(x ) Q P Q
s<t η e e
. (9)
(s)
e ∈YE(G) qe ∈Q Ne s<texp ηlogq e(x e )
We can interpret (9) P as followQ s: the first term gives the probability of sampling p 1, the
1
∈ N
second term gives the probability of sampling G T, and the third term gives the probability of
∈ G
sampling p e for each e E(G).
e
∈ N ∈
• p is sampled from a product distribution over 1. So, it can be sampled in time O( 1 )
1
N |N | ≤
(nk/ε)O(k).
• G is sampled from a weighted mixture of spanning arborescences of the complete directed
graphonnnodes,wheretheweightofaspanningarborescenceH isproportionalto w(e)
e E(H)
and w(e) = exp logq (x(s) ) for any e = (i,j) [n]2. Sampling weighte∈ dQ arbores-
s<t η e e ∈
qe e
cencesisknowP∈ nN toQbeinpolynomialtime[Bor60,Cha82,Tut01,DL20],usingTutte’stheorem;
for completeness, we describe the algorithm in the Appendix C.
• Each p is sampled from a product distribution over e. So, each of them can be sampled in
e
time O( e ) (nk/ε)O(k2). N
|N | ≤
25Once P is sampled, a sample from P can be generated in polynomial time as P is tree-structured
and k is bounded. This completes the proof of the lemma.
Now let us state our result for RWM algorithm.
Tree
Lemma 4.4. If the RWM algorithm is run with the expert set , parameter η, and horizon
Nε
T, the prediction P at time step t T can be computed in polynomial time, for any sequence of
t
≤
observed samples x(1),...,x(T) [k]n and bounded k.
∈
b
Proof. This follows directly from the argument used to prove Lemma 4.3, since in RWM, P is
t
sampled during the execution of the learning algorithm rather than during the sampling stage as
in EWA. b
4.2 Learning Guarantees
In this section, we will state the learning guarantees for tree-structured distributions for both
the improper and proper settings. Let us start by describing our result for the improper setting.
Theorem 4.5. Let P be an unknown discrete distribution defined over [k]n, and ε (0,1) be
∗
∈
a parameter. Given sample access to P , there exist algorithms outputting efficiently-sampleable
∗
distributions for each of the following guarantees:
(i) fortheoutputdistributionQ, withprobability atleast2/3, D (P Q)isan(ε,3)-approximation
KL ∗
k
nk2log(nk/ε)
of P and the sample complexity is O ;
∗ ε
(cid:16) (cid:17)
Tree nk2log(nk/δε)
(ii) the algorithm is a realizable PAC learner for with sample complexity O .
C δε
(cid:16) (cid:17)
(iii) thealgorithmisanagnosticPAClearnerfor Tree withsample complexityO(n4k4ε 4log(1/δ)).
−
C
Proof. The algorithm is the EWA-based improper learning algorithm (Algorithm 3), and the cor-
rectness and sample complexity follow from Theorem 3.12. The fact that the output distribution
is efficiently sampleable is shown in Lemma 4.3.
Intherealizablecase(whereP isatree-structureddistribution),thissamplecomplexity bound
∗
matches the following lower-bound in [CDKS17, Appendix A.2] up to log-factors for constant error
probability.
Lemma 4.6 (Restatement of lower bound result in Appendix A.2 of [CDKS17]). Let P be an
∗
unknown tree-structured distribution defined over 0,1 n, and ε (0,1) be a parameter. Any
{ } ∈
algorithm that outputs a distribution Q which is an ε-approximation of P with constant probability
∗
requires Ω(n) samples from P 5.
ε ∗
Now we will state the guarantees for proper learning of the tree-structured distributions.
Theorem 4.7. Let P be an unknown discrete distribution defined over [k]n, and ε > 0,δ (0,1)
∗
∈ Tree
be parameters. Given sample access to P , there exists a proper, agnostic PAC-learner for
∗
C
that takes O n3k2 log3 nk samples from P and runs in time O(poly(n,1/ε,1/δ)) for constant
ε2δ2 ε ∗
k. (cid:16) (cid:16) (cid:17)(cid:17)
e
5Thelowerboundstatedin[CDKS17]holdsforgeneralBayesnetdistributionsofindegree≤d. Fortree-structured
distributions, we have d = 1. Moreover, their lower bound is with respect to TV-distance, which translates to KL-
divergence dueto Pinkser’s inequality.
26Proof. The algorithm is the RWM-based proper learning algorithm (Algorithm 4) applied with
Trees
= , η = (8log )/T and the log loss function. The algorithm, by construction, will
N Nε/2 |N|
output a tree-structured distribution in . The correctness (agnostic PAC guarantee) and sample
p
N
complexity follow from exactly the same argument as in Theorem 3.13, taking d = 1 since tree-
structured distributions are Bayes nets with in-degree 1. The fact that the RWM-computation can
be done in poly(n,k, 1, 1) time, in spite of having exponentially many distributions in , follows
ε δ N
from the fact that the sampling from the weight distribution, for each time step t, can be done
Tree
efficiently when = (see Lemma 4.4).
N Nε/2
5 Learning Chordal-structured distributions
In this section, we show that chordal-structured distributions can be learnt efficiently. Before
proceeding to describe our results, we first define some notions that will be used in our results and
proofs.
5.1 Preliminaries about Chordal Graphs
Given an undirected graph G = (V,E) and subsets S,T V, we let G[S] denote the induced
⊆
subgraph of G on S, E(S) denote the edge set of G[S], and E(S,T) denote the set of edges with
one endpoint in S and the other in T. For a vertex v of G, let Nbr (v) denote the set of adjacent
G
vertices of v in G (vertices u such that u,v E(G)).
{ } ∈
An undirected graph is chordal if every cycle of length at least 4 contains a chord, that is an
edge connecting two vertices of the cycle which is not part of the cycle.
Definition 5.1 (Clique tree). Let G = (V,E) be a graph with vertex set V and edge set E. The
clique tree, denoted by G, of G is a tree that has the maximal cliques of G as its vertices and for
T
every two maximal cliques C and C , each clique on the path from C to C in G contains C C .
′ ′ ′
T ∩
G has the induced subtree property: for every vertex v V, the set of nodes in G that contains
T ∈ T
v forms a connected subtree of G. The treewidth of G equals one less than the size of the largest
T
maximal clique in G.
Let G be a chordal graph, and let G be a clique tree of G. Fix an arbitrary maximal clique
T
C V( G) and we view G as a tree rooted at C . We denote this rooted clique tree as G and
r ∈ T T r TCr
as if G is clear from context. For any maximal clique C, let Pa (C) denote the parent of C
TCr TCr
in the rooted tree , and let denote the subtree of rooted at C. Let V[ ] denote the
TCr TC TCr TC
vertex set of G[ ], that is, C . For notational convenience, we will use to denote
both the subtreeTC of G as wellC th′ ∈ eV v( T eC rt) ex′ set V[ ] when the usage will be clear fromTC the context.
T S TC
Thus, G[ ] denotes the subgraph of G induced by the the vertices in V[ ].
C C
T T
Definition 5.2 (Separator set). Let G be a chordal graph, and let be a rooted clique tree of
TCr
G for a maximal clique C . For C V( ), the separator of C with respect to is defined as
r
∈
TCr TCr
follows:
Sep(C) = C Pa (C)
∩ TCr
Next we define the notion of link set, which is crucially used in our proofs.
27Definition 5.3 (Link set). Let G = (V,E) be a chordal graph, and be a rooted clique tree of
TCr
G for a maximal clique C . For C V( ), the link set of C is defined as follows:
r
∈
TCr
Link(C)= E(C,V[ ])
C
T
An orientation of an edge set F assigns a direction to each edge in F; an orientation is acyclic
if it does not give rise to a directed cycle. The indegree of an orientation is the maximum number
of incident edges which are oriented inward at any vertex. Two acyclic orientations on edge sets F
and F are said to be consistent with each other if they agree on the edges in F F .
′ ′
∩
Definition 5.4 (Indegree-Bounded Acyclic Orientations). Let G = (V,E) be an undirected graph.
For any subset of edges F E and integer d 0, AO (F) denotes the set of all acyclic orientations
d
⊆ ≥
of F with indegree at most d. Given an orientation that assigns orientations to a subset of E,
O
we let AO (F; ) denote the subset of AO (F) that is consistent with . If F corresponds to the
d d
O O
edge set of a subgraph H, then we also use the notations AO (H) and AO (H, ) respectively.
d d
O
B
ACDE
A C
D E DEG ACB
F G H
DGF EGH
(a) A chordal graph and a clique tree decomposition with reference clique C =DEG. In the left panel, the
edges of Link(C) are in red and the vertices V[ C] are in bold. In the right panel, the nodes of C are in
T T
green, and the separator vertices in each node of the clique tree are colored in blue.
B
A C
ABC
D E ACDE
DEG
F G H
DGF EGH
(b) The samechordalgraphandadifferentclique tree decompositionwithreferenceclique C =ACDE. The
colors have the same meaning as in (a).
We will use below some standard observations about chordal graphs and acyclic orientations;
for the sake of completeness, we give their proofs.
Lemma 5.5 (Lemma 1 of [BS22], restated). Let G = (V,E) be a chordal graph and consider a
clique tree of G rooted at a node C . Let C be a node in and C ,...,C be its children in
TCr r TCr 1 ℓ
. Then the edge sets of the graphs G[ Sep(C )],...,G[ Sep(C )] are mutually disjoint.
TCℓ TC1
\
1 TCℓ
\
ℓ
28Proof. We willprove this by contradiction. Let us assumethat thereexists i = j with i,j [ℓ] such
6 ∈
that G[ Sep(C )] and G[ Sep(C )] share an edge e= u,v . This implies that both G[ ]
TCi\ i TCj
\
j
{ }
TCi
and G[ ] contain the edge e. However, this would imply C u,v by the inducted subtree
TCj
⊇ { }
property (since any pair of nodes in and can only be connected in G through C), and
TCi TCj
T
thus that e will be present in both E(Sep(C )) and E(Sep(C )). This contradicts our assumption
i j
on e.
Lemma 5.6(SeeTheorem11inChapter4of[Sun22]). LetGbeachordal graph whichisacyclically
orientable with indegree d, and consider a rooted clique tree of G. Consider a non-leaf node
≤
TCr
C andC ,...,C bethe setof children ofC inthe rooted tree. Consideranacyclic orientation
∈
TCr 1 ℓ
of Link(C)withindegree d, andlet be the orientation restricted toE(Sep(C ), )
OC
≤
OSep(Ci) OC i TCi
for every i [ℓ]. Then there exists a bijection between AO ( , ) and AO ( , ) ...
∈
d TC OC d TC1 OSep(C1)
× ×
AO ( , ).
d TCℓ OSep(Cℓ)
Proof. Consider any acyclic orientation of with indegree d consistent with a given acyclic
C
O T ≤
orientation of Link(C). Any subset of an acyclic orientation will be acyclic as well, and
C
O O
indegree bounds will be preserved. Hence restricted to each will give acyclic orientations
O
TCi
of with indegree d. Given these consistent orientations ,..., of the subtrees, and
Oi TCi
≤
O1 Oℓ
knowing that is consistent with , we can reconstruct . This suffices to argue the injectivity.
C
O O O
In order to prove surjectivity, let us consider acyclic orientations AO ( , ) for
Oi
∈
d TCi OSep(Ci)
every i [ℓ]. Now let us consider the following orientation by taking the union of and
C
∈ O O
ℓ . Following Lemma 5.5, we know that is consistent with . Now we would like to prove
i=1Oi
O
OC
that is an acyclic orientation and has indegree d.
S O ≤
Wewillproveacyclicity bycontradiction. Assumethatthereisatleastonecyclein . Consider
O
the shortest cycle among all those cycles in . The cycle must contain at least two edges in E(C),
O
because if it contains one or no edges of E(C), then it is contained inside some (C ) which is
i
T
impossiblesince isacyclic. Considertwoverticesu,v C whichareinthecyclebutnotadjacent
i
O ∈
in the cycle. In C, there exists an edge between u and v, with orienting it either (u,v) or (v,u).
C
O
Whichever the case, we can shorten the cycle by taking a shortcut on the edge, which contradicts
the assumption that we are considering the shortest cycle. This implies that is acyclic.
O
To argue that the indegree of is d, note that any v which is common to and for
O ≤
TCi TCj
i = j will belong to C as well. Hence the orientation of all incident edges of v will be fixed by
C
6 O
itself, andcombining ,..., willnotincreasetheindegreebeyonddsincetheyareallconsistent
1 ℓ
O O
with . Thus the mapping is surjective as well. Combining the above, we have the proof of the
C
O
lemma.
Lemma 5.7. Let C and C be two cliques of G so that C is a node in G and C is a child of C
i i
T
in G. Then there are no edges between V[ ] C and C C .
T
TCi
\ \
i
Proof. Assume for the sake of contradiction that u,v is an edge between u V[ ] C and
{ } ∈
TCi
\
v C C . In particular, say u K where K is a clique in . Then, by the definition of the
∈ \
i
∈
TCi
clique tree decomposition, (i) the edge is contained neither in C nor in any of the cliques in the
subtree rooted at C (since u C and v C C ), but (ii) the edge is part of some maximal-clique
i i
6∈ ∈ \
C . Hence, C and K must be separated by C in the clique tree. But this is a contradiction, since
′ ′
u C and u K but u C.
′
∈ ∈ 6∈
295.2 EWA and RWM with Chordal Experts
In this section, we consider distributions on chordal graphs which can be oriented acyclically
with indegree d. This assumption is sufficient to bound the size of each maximal clique.
≤
Remark 5.8. Let G be an undirected chordal graph. If there exists an acyclic orientation of G
with indegree d, then for any clique tree decomposition of G (rooted at a maximal clique
≤
TCr
C ), all the nodes of (maximal cliques of G) have cardinality d+1.
r TCr
≤
Proof. Byassumption, wehaveanacyclic orientation withindegree dforeach cliqueC V( ).
≤ ∈
TCr
If we topologically order the nodes of C, the last node will have indegree C 1 d, which implies
| |− ≤
C d+1.
| |≤
We will establish the following theorem.
Theorem 5.9. Let G be an undirected chordal graph, and suppose k and d are fixed constants. Let
G be the family of distributions over [k]n that can be defined as Bayes nets over DAGs having
Ld
skeleton G and with indegree d. Given sample access from P ∆([k]n), and a parameter ε> 0,
∗
≤ ∈
there exist the following:
(i) An agnostic PAC-learner for G using O(n4k2d+2ε 4log(1/δ)) samples that is improper and
Ld −
returns an efficiently-samplable mixture of distributions from G.
Ld
e
(ii) An agnostic PAC-learner LearnChordalDist for G using O(n3kd+1ε 2log(n/εδ)) samples and
Ld −
poly(n) running time that is proper and returns a distribution from G.
Ld
e
The sample complexity guarantees follow from Theorem 3.12 and from Theorem 3.13. What
remains to be justified is that the algorithms run efficiently and the distribution output by the
improper learning algorithm can be sampled efficiently. Below, let x(1),...,x(T) be the samples
drawn from P .
∗
Let G be a clique tree decomposition of G rooted at a maximal clique C . We can assume
TCr r
without loss of generality that G has at least one acyclic orientation with indegree d; otherwise,
≤
G is empty and the problem is trivial. Hence, by Remark 5.8, every node in G has at most d+1
Ld TCr
vertices of G.
Claim 5.10. For any C V( ), AO (Link(C)) n+d d+1 .
∈ TCr | d | ≤ d
(cid:0) (cid:1)
Proof. Note that there are at most d+1 vertices in C and each such vertex has at most n incident
edges, of which at most d can be incoming. Combining the above, we have the claim.
For an orientation ofG[ ]andanodev V[ ], letin (v, ) denotethesetofin-neighbors
C C C
O T ∈ T O
of v in G[ ] with respect to . We define the weight, with respect to the clique C, of a node
C
T O
v V[ ], acyclic orientation of G[ ] and time-step t [T] as follows:
C C
∈ T O T ∈
t
wt (v, ,t) , exp logq x(s) x(s) . (10)
C O
q vX|inC(v,O)s Y=1
η
(cid:16)
v | inC(v, O)
(cid:17)
∈Nε/n
For all C V( G) and for all AO (Link(C)), and for all t [T], we will store an entry in a
C d
∈ T O ∈ ∈
table Table[C, ,t].
C
O
30Table[C, ,t] , wt (v, ,t). (11)
C C
O O
O∈AO d(XG[ TC], OC)v ∈YV[ TC]
Note that, according to this definition, Table[C , ,t] gives the total weight of all indegree d
r
∅ ≤
acyclic orientations of G after observing t samples. The sum is over an exponential-sized set;
nevertheless, we will be able to use dynamic programming to compute it efficiently.
Let us start with the case that C is a leaf node of G. Note that, if C is a leaf node of G,
T T
AO (G[ ], ) = , where is an acyclic orientation of C. So, the corresponding table
d C C C C
T O {O } O
entry can be directly computed for any in poly(n,t,1/ε) time for constant d and k. This
C
O
completes the base case of our dynamic programming.
Now let us assume that C has ℓ children C ,...,C in G. For AO (Link(C)), we show
1 ℓ C d
T O ∈
that Table[C, ,t] can be inductively computed in terms of the table entries Table[C , ,t] for
OC i OCi
1 i ℓ, where each AO (Link(C )), so that we obtain a bottom-up dynamic programming
≤ ≤
OCi
∈
d i
algorithm for counting weighted acyclic orientations with indegree d starting from the leaf nodes
≤
of G. By Lemma 5.6, we first break each acyclic orientation of into ,..., (acyclic
C 1 ℓ
T O T O O
orientations of the child subtrees ) which can be combined consistently with an orientation
TCi
AO (Link(C)).
C d
O ∈
The table entry Table[C, ,t] can be expressed as follows:
C
O
Table[C, ,t] , wt (v, ,t)
C C
O O
O∈AO d(XG[ TC], OC)v ∈YV[ TC]
= wt (v, ℓ ,t)
C ∪j=1Oj
O1 ∈AO d(XG .[ TC1], OC),v ∈VY[ TC]
.
.
Oℓ∈AO d(G[ TCℓ], OC)
ℓ
= wt (v, ,t) wt (v, ,t) , (12)
C OC

Ci Oi

v Y∈C O1 ∈AO d(XG .[ TC1], OC), i Y=1v ∈VY[ TCi] \C
.  
.
Oℓ∈AO d(G[ TCℓ], OC)
where in the last line, we used the fact that V[ ] C are disjoint for distinct i, and also that,
TCi
\
there are no edges between V[ ] C and C C by Lemma 5.7.
TCi
\ \
i
For each i, let Cons ( ) AO (Link(C )) be the set of acyclic orientations of Link(C ) of
i c d i i
O ⊆
indegree at most d that are consistent with . From (12), we can write Table[C, ,t] as:
C C
O O
wt (v, ,t)
C C
O ×
v ∈C \(CY1 ∪... ∪Cℓ)
ℓ
wt (v, ,t) wt (v, ,t) .
 Ci Oi · C OC 
∀i, OCi∈XCons i( OC) ∀i, Oi∈AO dX(G[ TCi], OCi) i Y=1v ∈V[ TCYi] \Sep(Ci) v ∈∪ℓ i=Y1Sep(Ci)

   (13) 
We can now state a recurrence for Table[C, ,t] using the notation above.
C
O
31Lemma 5.11. For any C V( ), AO (Link(C)),t [T], if C ,...,C are the children of
∈
TCr OC
∈
d
∈
1 ℓ
C in , then:
TCr
ℓ
Table[C, ,t] = wt (v, ,t) Table[C , ,t] Ξ( , ,..., ),
OC C OC i OCi
!
OC OC1 OCℓ
v ∈CY\∪iCi ∀i, OCi∈XCons i( OC) i Y=1
where:
wt (v, ,t)
C C
Ξ( , ,..., )= O .
OC OC1 OCℓ
wt (v, ,t)
v ∈∪iYSep(Ci)
j ∈[ℓ Q]:Cj∋v
Cj OCj
Proof. Wearguebyinduction. Forthebasecase,whenC isaleaf,Table[C, ,t] = wt (v, ,t)
which agrees with (11). Otherwise, we inductively use (11) on each
Table[COC
, ,t]
av n∈dC cheC
ck
thO aC
t
i OCi Q
(13), and therefore (11), holds for Table[C, ,t]. The only fact we need here is that if v Sep(C ),
C i
O ∈
all its incident edges belong to Link(C ) and hence their orientations are fixed by .
i OCi
The pseudocode for counting the number of acyclic orientations with maximum indegree d is
described in Algorithm 5.
Algorithm 5: CountChordalDist (G, ,η,SampleList = [x(1),...,x(t)],t)
TCr
Input: A known chordal skeleton G which is acyclically orientable with indegree d, a
rooted clique tree of G, hyperparameter η, a list of samples [x(1),...,x(t)] from
TCr
P , time step t.
∗
Output: 3-dimensional table Table.
1 Table .
← ∅
2 NumberofLevels the number of levels in G.
← T
3 for every leaf node C V( ) do
C
∈ T
4 for every AO (C) do
C d
O ∈
5 for every v V(G ) do
C
∈
6 Compute wt (v, ,t) from the samples according to Equation (10).
C C
O
7 Table[C, ,t] = wt (v, ,t) according to Equation (11).
OC v ∈C C OC
8 // The root is at level 0, and the lowest leaf is at level NumberOfLevels.
Q
9 for j = NumberOfLevels 1 to 0 do
−
10 the nodes at level j in G.
S ← T
11 for every non-leaf C do
∈ S
12 Let the children of C be C ,...,C , which are at level j+1 by definition.
1 ℓ
13 for every AO (Link(C)) do
C d
O ∈
14 Compute Table[C, ,t] according to Lemma 5.11.
C
O
15 Return Table.
Once Table[, ,t] has been constructed, we can sample an orientation of the chordal graph G
· ·
at time step t. The idea is to go down from the root of to its leaves, level-by-level, with each
TCr
iteration orientingthelinkof amaximalcliquewhilebeingconsistentwiththeorientations sampled
so far. For each clique C V( ) with parent C , we suppose that the edges in Link(C ) have
∈
TCr p p
been oriented fully. We use the table Table[, ,t] to sample in a consistent way with the already
· ·
oriented edges. To argue correctness, we again appeal to Lemma 5.6 to see that once a parent
32Algorithm 6: SamplingChordalDist G,d,η,SampleList = [x(1),...,x(t)],t)
Input: A chordal skeleton G which is acyclically orientable with indegree d,
hyperparameter η > 0, a list of samples x(1),...,x(t) from distribution P on [k]n,
∗
time step t [T]
∈
Output: A DAG with skeleton G.
1 Construct a clique tree G of G using the algorithm of [RTL76].
T
2 r root( G).
← T
3 L Leaves( G).
← T
4 Stack .
← ∅
5 Call the counting algorithm CountChordalDist (G, G,η,SampleList) to obtain a 3-d DP
T
table Table.
6 Push (r, ) to the Stack.
∅
7 while Stack is not empty do
8 Pop (B, ) from Stack, where B is a node in G and AO (Link(Pa (B))) if
p p d G
O T O ∈ T
B = r.
6
9 for every orientation AO(Link(B), ) do
p
O ∈ O
10
Table[B, ,t]
P ( ) O
B
O ← Table[B, ,t]
′
O
′ AO(Link(B), p)
O∈ P O
11 Sample AO(Link(B)) from the distribution P .
B B
O ∈
12 Fix the orientation of Link(B) to be .
B
O
13 for each child C of B in G do
T
14 Push (C, ) to Stack.
B
O
15 Return all the orientations of the edges in G.
Algorithm 7: LearnChordalDist (RWM-based proper learning of chordal-structured distri-
butions)
Input : hyC ph eo rr pd aa rl amsk ee tl ee rto ηn >G, 0,in Td .egree parameter d,
N
=
i ∈[n] S
⊆NbrG(i)NεX /ni|XS,
Q Q
Output: A chordal-structured distribution P.
1 Sample t 1,...,T uniformly at random.
← { }
2 SampleList . b
← ∅
3 for s 1 to t do
←
4 Observe sample x(s) P .
∗
∼
5 SampleList SampleList x(s) .
← ∪{ }
6 Call SamplingChordalDist (G,d,η,SampleList,t) to obtain a DAG D.
7 For each i [n], sample p as described in text.
i
∈
8 return the Bayes net P = (p ,...,p ) on the DAG D.
1 n
b
b b b
33clique has been oriented, its children can be oriented independently as long as they are consistent
with the parent’s orientation. Algorithm 6 gives the pseudocode.
SupposeD istheorientation ofGthatissampledbySamplingChordalDistforauniformlychosen
value of t [T]. Then, we can readily sample a Bayes net on D by following the same strategy
∈
described in the proof of Lemma 4.3. Namely, for each i [n], we sample p (y ,y ) from
∈
i i Pa D(i)
Xi|XPaD(i)
independently, where:
Nε/n b
t
(s) (s)
Pr[p = p] exp logp x ,x .
i ∝ η i Pa D(i)
s Y=1 (cid:16) (cid:17)
b
Finally, the sampled Bayes net P is obtained by letting the conditional probability distribution at
each node i be p . The proper learning algorithm is summarized as pseudocode in Algorithm 7.
i
The improper learning algorithmb is similar, but it uses EWA in place of RWM, and so, it samples
a random P whebn generating samples instead of during the learning phase.
b
Acknowledgements
AB and PGJ’s research were supported by the National Research Foundation, Prime Minister’s
Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CRE-
ATE) programme. AB and SS were supported by National Research Foundation Singapore under
its NRF Fellowship Programme (NRF-NRFFAI1-2019-0002). AB was additionally supported by
an Amazon Research Award and a Google South/Southeast Asia Research Award. SG’s work is
partially supported by the SERB CRG Award CRG/2022/007985. NVV’s work was supported in
partby NSFCCFgrants 2130608 and2342244 andaUNL GrandChallenges Catalyst Competition
Grant.
WewouldliketothankDebojyotiDey, aPh.D.studentatIITKanpur,fordiscussionsregarding
robust learning algorithms in high dimensions. AB would also like to thank Daniel Beaglehole for
a short meeting which seeded the idea for this work.
34References
[AHHK12] Anima Anandkumar, Daniel J Hsu, Furong Huang, and Sham M Kakade. Learning
mixtures of tree graphical models. NeurIPS, 2012. 5
[AKN06] PieterAbbeel,DaphneKoller,andAndrewYNg. Learningfactorgraphsinpolynomial
time and samplecomplexity. The Journal of Machine Learning Research, 7:1743–1788,
2006. 9
[And98] Artur Andrzejak. An algorithm for the tutte polynomials of graphs of bounded
treewidth. Discrete mathematics, 190(1-3):39–54, 1998. 11
[BBD+19] Soheil Behnezhad, Avrim Blum, Mahsa Derakhshan, MohammadTaghi Hajiaghayi,
Christos H Papadimitriou, and Saeed Seddighin. Optimal strategies of blotto games:
Beyond convexity. In Proceedings of the 2019 ACM Conference on Economics and
Computation, pages 597–616, 2019. 3
[BCD20] Johannes Brustle, Yang Cai, and Constantinos Daskalakis. Multi-item mechanisms
without item-independence: Learnability via robustness. In Proceedings of the 21st
ACM Conference on Economics and Computation, pages 715–761, 2020. 4
[BDLS17] Sivaraman Balakrishnan, Simon S Du, Jerry Li, and Aarti Singh. Computationally ef-
ficient robustsparseestimation inhigh dimensions. InConference on Learning Theory,
pages 169–212. PMLR, 2017. 10
[BGMV20] Arnab Bhattacharyya, Sutanu Gayen, Kuldeep S Meel, and NV Vinodchandran. Effi-
cientdistanceapproximationforstructuredhigh-dimensionaldistributionsvialearning.
Advances in Neural Information Processing Systems, 33:14699–14711, 2020. 9
[BGP+23] ArnabBhattacharyya, SutanuGayen, EricPrice,VincentYFTan,andNVVinodchan-
dran. Near-optimal learning of tree-structured distributions by chow and liu. SIAM
Journal on Computing, 52(3):761–793, 2023. 4, 5, 9, 21, 40, 41
[BHK+23] Daniel Beaglehole, Max Hopkins, Daniel Kane, Sihan Liu, and Shachar Lovett. Sam-
pling equilibria: Fast no-regret learning in structured games. In Nikhil Bansal and
Viswanath Nagarajan, editors, Proceedings of the 2023 ACM-SIAM Symposium on
Discrete Algorithms, SODA 2023, Florence, Italy, January 22-25, 2023, pages 3817–
3855. SIAM, 2023. 3, 7
[Bor60] Carl Wilhelm Borchardt. Ueber eine der interpolation entsprechende darstellung der
eliminations-resultante. Journal fürdiereine undangewandte Mathematik,57:111–121,
1860. 25, 44
[BS22] Ivona Bezáková and Wenbo Sun. Counting and sampling orientations on chordal
graphs. In International Conference and Workshops on Algorithms and Computation,
pages 352–364. Springer, 2022. 6, 28
[BvBR+15] Lynn Boschloo, Claudia D van Borkulo, Mijke Rhemtulla, Katherine M Keyes, Denny
Borsboom, and Robert A Schoevers. The network structure of symptoms of the di-
agnostic and statistical manual of mental disorders. PloS one, 10(9):e0137621, 2015.
3
35[CBL06] Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games. Cambridge
university press, 2006. 15, 16
[CDKS17] Clément L Canonne, Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Testing
bayesian networks. In Conference on Learning Theory, pages 370–448. PMLR, 2017.
26
[CDKS18] Yu Cheng, Ilias Diakonikolas, Daniel Kane, and Alistair Stewart. Robust learning of
fixed-structurebayesiannetworks. AdvancesinNeuralInformation Processing Systems,
31, 2018. 10
[Cha82] Seth Chaiken. A combinatorial proof of the all minors matrix tree theorem. SIAM
Journal on Algebraic Discrete Methods, 3(3):319–329, 1982. 25, 44
[Chi95] DavidMaxwellChickering. Learningbayesiannetworksisnp-complete. InDougFisher
and Hans-Joachim Lenz, editors, Learning from Data - Fifth International Workshop
on Artificial Intelligence and Statistics, AISTATS, 1995. 9
[Chi02] David Maxwell Chickering. Optimal structure identification with greedy search. Jour-
nal of machine learning research, 3(Nov):507–554, 2002. 9
[CHL+23] Clément Canonne, Samuel B Hopkins, Jerry Li, Allen Liu, and Shyam Narayanan.
The full landscape of robust mean testing: Sharp separations between oblivious and
adaptive contamination. In 2023 IEEE 64th Annual Symposium on Foundations of
Computer Science (FOCS), pages 2159–2168. IEEE, 2023. 10
[CHM04] MaxChickering,DavidHeckerman,andChrisMeek. Large-samplelearningofbayesian
networks is np-hard. Journal of Machine Learning Research, 5:1287–1330, 2004. 9
[CHMM23] JeanCardinal,HungP.Hoang,ArturoMerino,andTorstenMütze. Zigzaggingthrough
acyclic orientations of chordal graphs and hypergraphs. In Proceedings of the 2023
Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 3029–3042,
2023. 6
[CL68] C. K. Chow and C. N. Liu. Approximating discrete probability distributions with
dependence trees. IEEE Trans. Inf. Theory, 14(3):462–467, 1968. 5
[CL21] Yu Cheng and Honghao Lin. Robust learning of fixed-structure bayesian networks
in nearly-linear time. In 9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 10
[CR06] Robert Castelo and Alberto Roverato. A robust procedure for gaussian graphical
model search from microarray data with p larger than n. Journal of Machine Learning
Research, 7(12), 2006. 3
[CR09] Robert Castelo and Alberto Roverato. Reverse engineering molecular regulatory
networks from microarray data with qp-graphs. Journal of Computational Biology,
16(2):213–227, 2009. 3
[CYBC24] Davin Choo, Joy QipingYang, ArnabBhattacharyya, andClément L Canonne. Learn-
ing bounded-degree polytrees with known skeleton. ALT, 2024. 6, 9
36[Das97] SanjoyDasgupta. Thesamplecomplexityoflearningfixed-structurebayesiannetworks.
Machine Learning, 29:165–180, 1997. 9
[Das99] Sanjoy Dasgupta. Learning polytrees. In Proceedings of the Fifteenth conference on
Uncertainty in artificial intelligence, pages 134–141, 1999. 5
[DEKM98] Richard Durbin, Sean R Eddy, Anders Krogh, and Graeme Mitchison. Biological
sequence analysis: probabilistic models of proteins and nucleic acids. Cambridge uni-
versity press, 1998. 3
[DGP09] Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. The com-
plexity of computing a nash equilibrium. Communications of the ACM, 52(2):89–97,
2009. 3
[DKK+17] Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and
Alistair Stewart. Being robust (in high dimensions) can be practical. In International
Conference on Machine Learning, pages 999–1008. PMLR, 2017. 10
[DKK+18] Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and
Alistair Stewart. Robustly learning a gaussian: Getting optimal error, efficiently. In
Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algo-
rithms, pages 2683–2702. SIAM, 2018. 10
[DKK+19] IliasDiakonikolas, GautamKamath,DanielKane,JerryLi,AnkurMoitra,andAlistair
Stewart. Robust estimators in high-dimensions without the computational intractabil-
ity. SIAM Journal on Computing, 48(2):742–864, 2019. 10
[DKS18] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts
with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on
Theory of Computing, pages 1061–1073, 2018. 10
[DKS22] Ilias Diakonikolas, Daniel M Kane, and Yuxin Sun. Optimal sq lower bounds for
robustly learning discrete product distributions and ising models. In Conference on
Learning Theory, pages 3936–3978. PMLR, 2022. 10
[DL97] Paul Dagum and Michael Luby. An optimal approximation algorithm for bayesian
inference. Artificial Intelligence, 93(1):1–28, 1997. 9
[DL20] Patrick DeLeenheer. Anelementary proofofamatrixtreetheoremfordirectedgraphs.
SIAM Review, 62(3):716–726, 2020. 25, 44, 45, 46
[DP21] Constantinos Daskalakis and Qinxuan Pan. Sample-optimal and efficient learning of
tree ising models. In STOC, 2021. 5
[EDAL10] David Edwards, Gabriel CG De Abreu, and Rodrigo Labouriau. Selecting high-
dimensional mixed graphical models using minimal aic or bic forests. BMC bioin-
formatics, 11:1–13, 2010. 3
[FLNP00] Nir Friedman, Michal Linial, Iftach Nachman, and Dana Pe’er. Using bayesian net-
works to analyze expression data. In Proceedings of the fourth annual international
conference on Computational molecular biology, pages 127–135, 2000. 3
37[FNP13] Nir Friedman, Iftach Nachman, and Dana Pe’er. Learning bayesian network struc-
ture from massive datasets: The “sparse candidate” algorithm. arXiv preprint
arXiv:1301.6696, 2013. 9
[FS96] Yoav Freund and Robert E Schapire. Game theory, on-line prediction and boosting.
In Proceedings of the ninth annual conference on Computational learning theory, pages
325–332, 1996. 3
[FS99] Yoav Freund and Robert E Schapire. Adaptive game playing using multiplicative
weights. Games and Economic Behavior, 29(1-2):79–103, 1999. 3
[FY96] Nir Friedman and Zohar Yakhini. On the sample complexity of learning bayesian
networks. In Proceedings of the Twelfth Annual Conference on Uncertainty in Articial
Intelligence (UAI 96 ), San Francisco, CA, pages 274–282, 1996. 9
{ }
[GA21] Ming Gao and Bryon Aragam. Efficient bayesian network structure learning via lo-
cal markov boundary search. Advances in Neural Information Processing Systems,
34:4301–4313, 2021. 9
[GPP90] Dan Geiger, Azaria Paz, and Judea Pearl. Learning causal trees from dependence
information. In AAAI, pages 770–776. AAAI Press / The MIT Press, 1990. 9
[HK23] Juha Harviainen and Mikko Koivisto. Revisiting bayesian network learning with small
vertex cover. In Uncertainty in Artificial Intelligence, pages 819–828. PMLR, 2023. 5,
42
[HL18] Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares
proofs. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of
Computing, pages 1021–1034, 2018. 10
[HLS+10] Shuai Huang, Jing Li, Liang Sun, Jieping Ye, Adam Fleisher, Teresa Wu, Kewei Chen,
Eric Reiman, Alzheimer’s Disease NeuroImaging Initiative, et al. Learning brain con-
nectivity of alzheimer’s disease by sparse inverse covariance estimation. NeuroImage,
50(3):935–949, 2010. 3
[Höf93] Klaus-UHöffgen. Learningandrobustlearningofproductdistributions. InProceedings
of the sixth annual conference on Computational learning theory, pages 77–83, 1993. 9
[Hub92] Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in statis-
tics: Methodology and distribution, pages 492–518. Springer, 1992. 10
[KF09] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and tech-
niques. MIT press, 2009. 3
[KK09] M Kumar and Daphne Koller. Learning a small mixture of trees. Advances in neural
information processing systems, 22, 2009. 5
[KM17] Adam R. Klivans and Raghu Meka. Learning graphical models using multiplicative
weights. In Chris Umans, editor, Symposium on Foundations of Computer Science,
FOCS, pages 343–354, 2017. 10
38[KR12] Dan Kovenock and Brian Roberson. Coalitional colonel blotto games with application
totheeconomics ofalliances. Journal of Public Economic Theory, 14(4):653–676, 2012.
3
[KS01] David R. Karger and Nathan Srebro. Learning markov networks: maximum bounded
tree-width graphs. In S. Rao Kosaraju, editor, Proceedings of the Twelfth Annual
Symposium on Discrete Algorithms, January 7-9, 2001, Washington, DC, USA, pages
392–401. ACM/SIAM, 2001. 9
[KSS18] Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation
and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM
SIGACT Symposium on Theory of Computing, pages 1035–1046, 2018. 10
[Lau96] Steffen L Lauritzen. Graphical models, volume 17. Clarendon Press, 1996. 3
[LRV16] Kevin ALai, AnupB Rao, andSantosh Vempala. Agnostic estimation of mean andco-
variance. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science
(FOCS), pages 665–674. IEEE, 2016. 10
[MJ00] Marina Meila and Michael I Jordan. Learning with mixtures of trees. Journal of
Machine Learning Research, 2000. 5
[Nob98] Steven D Noble. Evaluating the tutte polynomial for graphs of bounded tree-width.
Combinatorics, probability and computing, 7(3):307–321, 1998. 11
[Pea14] Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible infer-
ence. Elsevier, 2014. 5
[PGBL+20] Victor Peralta, Gustavo J Gil-Berrozpe, Julián Librero, Ana Sánchez-Torres, and
Manuel J Cuesta. The symptom and domain structure of psychotic disorders: a net-
work analysis approach. Schizophrenia Bulletin Open, 1(1):sgaa008, 2020. 3
[RP88] George Rebane and Judea Pearl. The recovery of causal poly-trees from statistical
data. Int. J. Approx. Reason., 2(3):341, 1988. 9
[RTL76] Donald J Rose, R Endre Tarjan, and George S Lueker. Algorithmic aspects of vertex
elimination on graphs. SIAM Journal on computing, 5(2):266–283, 1976. 33
[Rub17] Aviad Rubinstein. Settling the complexity of computing approximate two-player nash
equilibria. ACM SIGecom Exchanges, 15(2):45–49, 2017. 3
[SG91] Peter Spirtes and Clark Glymour. An algorithm for fast recovery of sparse causal
graphs. Social science computer review, 9(1):62–72, 1991. 9
[Sha24] Vidya Sagar Sharma. A fixed-parameter tractable algorithm for counting markov
equivalence classes with the same skeleton. In AAAI, 2024. 11
[SM03] Victor Spirin and Leonid A Mirny. Protein complexes and functional modules in
molecular networks. Proceedings of the national Academy of sciences, 100(21):12123–
12128, 2003. 3
39[SSW93] Carla D Savage, Matthew B Squire, and Douglas B West. Gray code results for acyclic
orientations. Congressus Numerantium, pages 185–185, 1993. 6
[Sta73] RichardPStanley. Acyclic orientations ofgraphs. Discrete Mathematics, 5(2):171–178,
1973. 11
[Sun22] Wenbo Sun. Efficient Sampling and Counting of Graph Structures Related to Chordal
Graphs. Rochester Institute of Technology, 2022. 29
[SZVdB23] NikilRoashanSelvam,HonghuaZhang,andGuyVandenBroeck. Mixturesofalltrees.
InInternational ConferenceonArtificialIntelligenceandStatistics,pages11043–11058.
PMLR, 2023. 5
[TRBK05] John Thomas, Naren Ramakrishnan, and Chris Bailey-Kellogg. Graphical models of
residue coupling in protein families. In Proceedings of the 5th international workshop
on Bioinformatics, pages 12–20, 2005. 3
[Tut01] William Thomas Tutte. Graph theory, volume 21. Cambridge university press, 2001.
25, 44
[Val84] LeslieGValiant. Atheoryofthelearnable. Communications of the ACM,27(11):1134–
1142, 1984. 4
[VGPT10] Gaël Varoquaux, Alexandre Gramfort, Jean-Baptiste Poline, and Bertrand Thirion.
Brain covariance selection: better individualfunctional connectivity models usingpop-
ulation prior. Advances in neural information processing systems, 23, 2010. 3
[WJ+08] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families,
and variational inference. Foundations and Trends® in Machine Learning, 1(1–2):1–
305, 2008. 3
[XMW22] Shanghong Xie, Erin McDonnell, and Yuanjia Wang. Conditional gaussian graphical
model for estimating personalized disease symptom networks. Statistics in medicine,
41(3):543–553, 2022. 3
A Lower bound for learning tree-structured distributions
In this section, we show a lower bound of Ω(n) holds for our tree structure learning results.
ε
This follows from [BGP+23]. We are including it here for completeness.
Theorem A.1. Given sample access to an unknown tree-structured discrete distribution defined
over [k]n, and ε > 0 be a parameter. With probability at least 9/10, Ω(n) samples are necessary for
ε
Tree
any realizable PAC learner for learning .
C
We will prove the above result for k = 2, that is, for distributions defined over 0,1 n. The
{ }
lower bound immediately extends to distributions over [k]n.
We will first prove the above result for n = 3 nodes. Then we will extend the result for n = 3ℓ,
for some positive integer ℓ.
40Definition A.2 (Hellinger distance). Let P and Q be two probability distributions defined over
the same sample space . The Hellinger distance between P and Q is defined as follows:
D
2
1
H(P,Q) = P(x) Q(x)
v u u2 x X∈D r − q !
t
Fact A.3. Let P and Q be two probability distributions defined over the same sample space D.
Then the following holds:
1
H(P,Q) D (P Q)
KL
≤ 2 ||
r
We will also need the following folklore result.
FactA.4. LetP beanunknowndiscretedistributiondefinedover 0,1 3,andε> 0beaparameter.
{ }
In order to output a distribution P defined over 0,1 3 such that H(P,P) √ε, Ω(1) samples
{ } ≤ ε
from P are necessary.
b b
Lemma A.5. There exist three tree-structured distributions P ,P ,P defined over 0,1 3, such
1 2 3
{ }
that any algorithm that that can learn any distribution P in up to ε-KL-distance, that is, can output
i
a distribution P such that D (P P ) ε, requires Ω(1) samples from P , for any i [3].
i KL i || i ≤ ε i ∈
P has three nodes X ,Y ,Z , P has three nodes X ,Y ,Z and P has three nodes X ,Y ,Z .
1 b 1 1 1 2b 2 2 2 3 3 3 3
In P , (Y ,Z ) takes values uniformly at random between (0,0) and (1,1) and X copies Y and Z
1 1 1 1 1 1
with probability (1 ε), and with the remaining probability, takes a value from Ber(1). P and P
− 2 2 3
are defined in a similar fashion where we set X = Z and X = Y , respectively.
2 2 3 3
We will use the following results from [BGP+23].
Claim A.6 (See Fact 7.4 (i) and Lemma 7.5 (i) of [BGP+23]). (i) H(P ,P ) 10√εforanyi =
i j
≥ 6
j [3].
∈
(ii) Every tree on 3 vertices is not Θ(εlog 1)-approximate for any of P ,P and P .
T ε 1 2 3
Proof of Lemma A.5. We will prove this by contradiction. Suppose there exists an algorithm
A
that can learn any distribution up to ε-KL-distance using o(1) samples, by outputting the pmf of
ε
P. In that case, we will argue that using , we can distinguish between P ,P and P .
1 2 3
A
Let us denote the unknown distribution be P, and suppose given sample access to P, takes
A
ob(1/ε) samples from P and outputs a distribution P such that H(P,P) √ε. This implies that
≤
given sample access to either of P with i [3], can output such a P such that H(P ,P) √ε,
i i
∈ A ≤
which contradicts Fact A.4. Using Fact A.3, we arebdone with the probof of the lemma.
b b
Now we are ready to prove Theorem A.1.
Proof of Theorem A.1. Let us assume that n = 3ℓ for some positive integer ℓ. We will divide the
n variables into ℓ blocks, each of size 3 nodes. We will prove this by contradiction. We will define
a distribution P over 0,1 n, where the i-th block is chosen uniformly to be either P or P .
1 2
{ }
Suppose there exists an algorithm that can take o(n) samples and outputs an ε-approximate
ε
tree T of P, with probability at least 9/10. Since we have chosen each block independently, T is a
disjoint union of T ,...,T . Using Claim A.6, and setting ε as ε/ℓ, we can say that each T is not
1 ℓ i
Θ(ε/ℓ)-approximate with probability at least 2/3. Thus, using Chernoff bound, we can say that
at least ℓ trees are not Cε approximate. Thus, D (P P) ε for a suitable constant C for any
100 ℓ KL || ≥
tree-structured distribution P. This completes the proof of the theorem.
b
b
41B Learning Bayesian networks with bounded vertex cover
Inthissection, weshow thatbayesian networks can belearnedefficiently if thesize ofthevertex
cover of the associated moralized graph is bounded. Before proceeding to present our result, we
need some notations.
Let G = (V,E) be DAG with the vertex set V and the edge set E. Here each vertex v V
∈
correspondstoavariableandtheedgesinE encodetheconditionalindependencerelations between
the nodes of V. Let denote the set of all possible DAG on the vertex set V. Moreover, for every
D
DAG G, let us associate it with a non-negative weight function f : G R+ 0 which encodes
→ ∪{ }
how well G fits a given dataset. Now we are ready to define the notion of modular weight function,
which will be crucially used in our proofs.
Definition B.1 (Modular weight function). Let G(V,E) be a DAG with vertex set V and edge
set E. Moreover, let f :G R+ 0 denotes the weight function associated with G. The weight
→ ∪{ }
function f is said to be a modular weight function if it has the following form:
f(G) = f (G ) (14)
v v
v V
Y∈
Now we define the notion of the vertex cover number of a DAG below.
Definition B.2 (Vertex cover number of a DAG). Given a DAG G, let G denote the undirected
M
moralized graph corresponding to G. The vertex cover number τ(G) of G is the size of the smallest
vertex cover of G . Moreover, for some integer ℓ, the set of DAGs G such that τ(G) ℓ is denoted
M
≤
as (ℓ).
DAG
Now we are ready to state the main result that we will be proving in this section.
Theorem B.3. Let P be an unknown discrete distribution on [k]n defined over a DAG G such
∗
that the size of the vertex cover of the moralized graph G corresponding to G is bounded by an
M
integer ℓ. Moreover, let (ℓ) be the family of distributions over [k]n that can be defined as Bayes
DAG
nets over DAGs whose vertex cover size of the associated moralized graph is bounded by ℓ. Given
sample access from P , and a parameter ε > 0, there exist the following:
∗
(i) AnagnosticPAC-learnerfor (ℓ) usingO(n4k2ℓ+2ε 4log(1/δ)) samplesandO(exp(ℓ)poly(n))
−
DAG
running time that is improper and returns an efficiently samplable mixture of distributions
from (ℓ).
DAG
(ii) An agnostic PAC-learner for (ℓ) using O(n3kℓ+1δ 2ε 2log(nk/ε)) samples and runs in
− −
DAG
time O(exp(ℓ)poly(n)) that is proper and returns a distribution from (ℓ).
DAG
In order to prove the above theorem, we will be using the following result from [HK23] which
states that (ℓ) can be sampled efficiently.
DAG
Theorem B.4 (Theorem 13 of [HK23] restated). There exists a randomized algorithm that can
sample from (ℓ) with weight proportional to f(G) efficiently in expected sampling time 4ℓnO(1).
DAG
Note that the above result gives a polynomial time sampling algorithm for (ℓ) when ℓ is
DAG
bounded by a constant.
Interestingly, our loss function is also a modular weight function.
42Claim B.5. Our loss function exp log t P(x(i)) is a modular weight function.
β s=1
(cid:16) (cid:17)
Q
This follows from the fact that the distribution P defined over the DAG G factorizes: see
∗
Equation (1) in Definition 2.3. Now we are ready to present the proof of our main result.
Proof of Theorem B.3. (i) We will be using the EWA algorithm (Algorithm 1) for this purpose.
Ineachroundofthealgorithm, wewillbeusingthealgorithmfromTheoremB.4. Thesample
complexity (aka. number of rounds of EWA algorithm) follows from the guarantee of EWA
algorithm for learning Bayesian networks (see Theorem 3.12). Since in each round, we will
be calling the algorithm corresponding to Theorem B.4, the running time of our algorithm
will be O(exp(ℓ)poly(n)).
(ii) We will be calling RWM algorithm (Algorithm 2) here. Similar to the above, in each round,
we will be using the algorithm from Theorem B.4. The sample complexity (aka. number
of rounds of RWM algorithm) follows from the guarantee of RWM algorithm for learning
Bayesian networks (see Theorem 3.13). Since in each round, we will be calling the algorithm
corresponding to Theorem B.4, the running time of our algorithm will be O(exp(ℓ)poly(n)).
C Learning Tree-structured distributions
Full proof of Lemma 4.3
As in Section 4, given a vertex set V = [n], denotes the set of all spanningout-arborescences
T
rooted at node 1. Tree denotes the set of all tG ree-structured distributions on [k]n. Tree denotes
C Tree Nε
the finite set of distributions (see Lemma 4.2) that discretize with error ε in D . Each tree-
KL
Tree C
structured distribution P in can be factored as (G,V ) where G is the underlying rooted
Nε P
arborescence and V = (p : e E(G),p ) is the vector of functions where p (x ,x ) for e = (i,j)
P e 1 e i j
∈
corresponds to Pr (X = x X = x ) and p (x ) corresponds to Pr (X = x ). Using
X P j j i i 1 1 X P 1 1
the notation of Defi∼ nition 3.6, p 1|
∈
NεX /n1 and for e = (i,j), p
e
∈
NεX /nj|Xi. F∼ or simplicity, we will
k2
call these sets 1 and e respectively in this section. Node that 1 , e nk , which is
N N |N | |N | ≤ ε
polynomial in n and 1/ε for constant k. (cid:16) (cid:17)
IntheproofofLemma4.3,wederivethefollowingexpressionforω (P),theweightassignedtoa
t
tree-structureddistributionP =(G,(p :e E(G),p ))inthediscretized set bytheEWA/RWM
∼ e 1
∈ N
algorithms at step t.
ω t(P) =
q1
∈Q
Ns
1<te sx <p
tη
el xo pg ηp
l1
o( gx q( 1s 1) ()
x( 1s) ) ·
H
∈Q
Ge T∈E( eG ∈)
EP
(Hq )e ∈N qe
e
∈Q
Ns
e<te sx <p
tη
el xo pg ηq
le
o( gx q( es e) ()
x( es) )·
(s)
P Q exp logpP(x ) Q P Q
s<t η e e
. (15)
(s)
e ∈YE(G) qe ∈Q Ne s<texp ηlogq e(x e )
P Q
This gives the following high-level algorithm for sampling from the EWA/RWM distribution.
43(i) Sample p from the distribution over 1 with the following probability:
1
N
(s) η
p (x )
Pr(p ), s<t 1 1 .
1 (cid:16)
Q q
(x(cid:17)(s)
)
η
q1 1 s<t 1 1
∈N
(cid:16) (cid:17)
P Q
(ii) Sample a spanning arboresence G (rooted at node 1), where Pr(G) w(e) and
T
∈ G ∝
e E(G)
w(e) , q (x(s) ) η for any e = (i,j) [n]2. This can be done in p∈ olQ ynomial time
s<t e e ∈
(even thq oe uP∈ gN he (cid:16) thQere are expo(cid:17) nentially many such arborescences) by applying Tutte’s matrix-
tree theorem to weighted digraphs [Bor60, Cha82, Tut01, DL20]. We describe this approach
in the algorithm SamplingArborescence,which we describe and analyze later. We invoke
G SamplingArborescence(G K ,w w,r 1),
0 n
← ← ← ←
where K is the complete graph on the vertex set [n] viewed as a digraph, which returns
n
G (rooted at 1) with the required sampling probability.
T
∈ G
(iii) For each e E(G) (fixed in the preceding step), p is sampled from a product distribution
e
∈
over e, with the following probability:
N
(s) η
p (x )
Pr(p ), s<t e e .
e (cid:16)
Q q
(x(cid:17)(s)
)
η
qe e s<t e e
∈N
(cid:16) (cid:17)
P Q
OnceP issampled,asamplefromP canbegeneratedinpolynomialtimeasP istree-structured.
Correctness The correctness of the above algorithm follows from considering each factor of
Equation (15) and noting that the steps (i)-(iii) described above are independent.
Running Time In Step (i), using memoization between time steps appropriately, p can be
1
sampled in time O( 1 ) (nk/ε)O(k). The idea is to keep DPT[p] = p(x(s) ) η for each
|N | ≤ s<t −1 1
p 1 from the sampling procedure at step (t 1), and multiply DPT(cid:16)[p] with p(x (cid:17) )η before
Q t 1,1
∈ N (s) η − −
step t to get p(x ) . A similar idea can be used in step (iii) to sample each p (n 1 of
s<t 1 e −
them) in time(cid:16) QO( e ) (cid:17)(nk/ε)O(k2). The SamplingArborescence call in this case (invoked
|N | ≤
with K ) requires O(n5) time, which gives an overall time complexity which is poly(n,1/ε) for
n
constant alphabet size k.
The SamplingArborescence algorithm
Preliminaries LetG= ([n],E,w)beaconnectedweighteddirectedgraphonnvertices 1,...,n
{ }
with m edges e ,...,e . w : E R is a positive weight function. Let A (G) denote the n n
1 m >0 w
→ ×
weighted vertex adjacency matrix of G. For every i,j [n], A (G) is defined as follows:
w
∈
w(e ), if e is the directed edge from i to j
k k
[A (G)] =
w i,j
(0, if there is no directed edge from i to j
44Let N (G) be the weighted inward incidence matrix of order m n defined as follows: For
in,w
×
every i [n] and k [m], we have:
∈ ∈
w(e ), if directed edge e points to vertex i
k k
[N (G)] =
in,w k,i
(0p, otherwise
Similarly, we can define the weighted outward incidence matrix M (G) of order n m as
out,w
×
follows, for i [n] and k [m]:
∈ ∈
w(e ), if directed edge e points from vertex i
k k
[M (G)] =
out,w i,k
(0p, otherwise
The indegree matrix D (G) is a n n diagonal matrix such that, for all i [n], [D (G)]
in,w in,w i,i
× ∈
is equal to the sum of the weights of all incoming edges to vertex i. Similarly, the outdegree matrix
D (G) is a n n diagonal matrix such that for all i [n], [D (G)] is equal to the sum of
out,w out,w i,i
× ∈
the weights of all outgoing edges from vertex i.
Now we can define the Laplacian matrices L (G) and L (G) associated with digraph G as
1,w 2,w
follows:
L (G) , D (G) A (G), and L (G) = D (G) A (G). (16)
1,w in,w
−
w 2,w out,w
−
⊤w
For all these associated matrices, we omit G and denote the matrix A (G) as A etc. when
w w
G is clear from the context. Now we have the following relationships between the Laplacian and
incidence matrices:
Claim C.1 (Equation (10) of [DL20]). (i) L = (N M )N .
1,w i⊤n,w
−
out,w in,w
(ii) L = (M N )M .
2,w out,w
−
i⊤n,w o⊤ut,w
For a vertex r [n], let Lr be the (n 1) (n 1) matrix obtained by removing the r-th row
∈ 1,w − × −
and r-th column of L . Similarly, we also define Nr and Mr . Then we have the following:
1,w in,w out,w
Claim C.2 (Equation (11) of [DL20]). (i) Lr = ((Nr ) Mr )Nr .
1,w in,w ⊤ − out,w in,w
(ii) Lr = ((Mr (Nr ) ))(Mr ) .
2,w out,w − in,w ⊤ out,w ⊤
Definition C.3 (Weight of a graph). Let G = (V,E,w) be a weighted (directed or undirected)
graph, where w :E R . Let H be any subgraph of G. The weight of graph H is defined as the
>0
→
product of the weights of its edges, i.e.
w(H) = w(e).
e ∈YE(H)
Definition C.4 (Contraction and deletion). If G = (V,E,w) is a weighted undirected graph (not
necessarily simple) and e = i,j E, the graph G/e obtained by contracting edge e is a weighted
{ } ∈
graph on V i,j i,j where
\{ }∪{h i}
• Vertices i and j are removed, and a new vertex i,j is added.
h i
• All edges i,j (in case of parallel edges) are removed in G/e.
{ }
45• Edges u,v where u,v V i,j are preserved in G/e with the same weight.
{ } ∈ \{ }
• Every edge u,v E where u i,j and v V i,j becomes an edge i,j ,v in G/e
{ } ∈ ∈ { } ∈ \{ } {h i }
with the same weight.
Similarly the graph G e obtained by deleting edge e is just (G e) = (V,E e,w ) where
E e
\ \ \ | \
the edge e is deleted (including all parallel edges) from G.
Definition C.5 (Contraction edge mapping). With the above definition of contraction (with par-
allel edges kept), if G is a graph and G = G e for e = i,j E(G), we can map each edge
′
\ { } ∈
e E(G) injectively to an edge e E(G) that caused its inclusion in G. We denote this mapping
′ ′
∈ ∈
by f : E(G) E(G).
G′:G ′
→
The following theorem is a generalization of Tutte’s matrix-tree theorem to weighted graphs.
Lemma C.6 (Theorem 3 of [DL20]). Let G = (V,E,w) be a weighted directed graph on n vertices
[n] with positive weight function w : E R . Then, for any r [n], the sum of the weights of
>0
→ ∈
the weighted outgoing (incoming) spanning arborescences rooted at vertex r is equal to det(Lr )
1,w
(det(Lr ), respectively).
2,w
Now the next corollary follows as any undirected graph can be viewed as a directed graph with
edge i,j corresponding to a pair of directed edges (i,j) and (j,i).
{ }
Corollary C.7. If G = K (the complete graph on n vertices) and w : [n]2 R is any positive
n >0
→
weight function,
det(L1 )= w(e),
1,w
G X∈GTe ∈YE(G)
where , as defined earlier, is the set of all spanning out-arborescences on [n] rooted at vertex 1.
T
G
The algorithm If G is a graph and v V(G) be a vertex of G, let denote the set of out-
G,v
∈ A
arborescences on V(G) which are subgraphs of G (viewed as a digraph), rooted at v, and span the
set of nodes reachable from v. Note that this set will not be empty unless v is an isolated vertex.
Definition C.8 (Productarborescencedistribution). LetG= (V,E,w) beaweighted graphwhere
w :E R is a weight function on the edges. For any v V(G), a distribution µ on is said
0 G,v
→ ≥ ∈ A
to be the product arborescence distribution if
Pr (G) w(e) for all G .
G,v
G µ ∝ ∈ A
∼ e G
Y∈
Claim C.9. Suppose G = (V,E,w) is a weighted digraph with non-negative weights, r V(G) and
∈
µ is a product arborescence distribution on . Then
G,r
A
det(Lr (G e))
Pr (e T)= 1 1,w \ .
T µ ∈ − det(Lr (G))
∼ 1,w
Proof. Since Pr (T) w(e), we have
µ ∝ e ∈T
Q w(T) w(T) det(Lr (G e))
Pr (e T)= T ∈AG,r:T ∋e = 1 T ∈AG,r:T 6∋e = 1 1,w \ ,
T ∼µ ∈ P T ∈AG,r w(T) − P T ∈AG,r w(T) − det(Lr 1,w(G))
wherethefirstequalityfoPllowsfromDefinitionC.8PandDefinitionC.3,andthethirdequalityfollows
from applying Lemma C.6 to G e and G.
\
46Algorithm 8: The SamplingArborescence algorithm
Input: G a directed graph, w :E(G ) R weight function, r V(G ) root node.
0 0 0 0
→ ≥ ∈
Output: Arborescence G with Pr(output G ) w(e).
T ∈ AG0,r T ∝ e ∈E(GT)
1 Let G .
T ← ∅ Q
2 Choose an arbitrary ordering of the m edges of G , and let EdgeList [e ,...,e ].
0 1 m
←
3 Let ContractionMapping[e ] e for each i [m].
i i
← ∈
4 Let j 0.
←
5 while r has an outgoing edge in G do
j
6 Let e =(r,x) be the first remaining outgoing edge.
7 Compute p
det(Lr 1,w(Gj\e))
.
e ← det(Lr 1,w(Gj))
8 Sample u Unif((0,1]).
∼
9 if u p (with probability p ) then
e e
≤
10 G G e (delete edge e).
j+1 j
← \
11 j j +1.
←
12 else
13 G G /e (contract edge e).
j+1 j
←
14 Add ContractionMapping[e] to G (this will be an edge of G ).
T 0
/* Reconstruct ContractionMapping for mapping E(G ) to E(G ). */
j+1 0
15 CM empty dictionary.
′
←
16 for each edge e E(G ) do
j+1
∈
17 CM[e] ContractionMapping[f (e)] (see Definition C.5).
′
←
Gj+1:Gj
18 ContractionMapping CM
′
←
19 r r,x V(G ) (the newly-contracted vertex).
j+1
← h i ∈
20 j j +1.
←
21 return G .
T
Now we are ready to prove the correctness of the algorithm SamplingArborescence.
Claim C.10. There is an algorithm SamplingArborescencethat, if invoked with input (G,w,r)
for a digraph G, w :E(G) R , and r V(G), returns a random arborescence G sampled from
0 T
the product arborescence di→ strib≥ ution on ∈ . The algorithm runs in time O(E V 3) in the real
G,r
A | || |
RAM model.
Proof. Thepseudocodefor the algorithm is given as Algorithm 8. Observe that, if G , then
T G,r
∈A
it is actually sampled from the product arborescence distribution (Definition C.8) by Claim C.9,
since each edge e is added to G with the correct probability 1 p .
T e
−
It remains to ensure that G is indeed a spanning arborescence. Let r denote the value of r
T j
used in iteration j of the while loop (lines 5-20). r V(G ) by construction. Let (r ) V(G )
j j j 0
∈ S ⊆
denote r if V(G ) = V(G ) (no contractions) and the set of nodes in G corresponding to the
j j 0 0
{ }
contracted-vertex in G otherwise.
j
By construction, when (r ) > 1, the algorithm maintains (in G ) an out-arborescence rooted
j T
|S |
at r = r that spans (r ). This is because when the algorithm selects an edge e = (r ,x) E(G )
0 j j j
S ∈
and takes G = G /(r ,x), this edge will correspond to an edge (v,x) in the original graph G
j+1 j j 0
where v (r ) and x (r ). This will ensure that there are no (undirected) cycles formed when
j j
∈ S 6∈ S
adding (v,x) to G and the rooted arborescence invariant will be maintained in iteration j + 1
T
47with (r ) = (r ) x . Note that when an edge e is selected for contraction in iteration j,
j+1 j
S S ⊔{ }
the algorithm maintains a mapping ContractionMapping from the edges in G to the edges in G
j 0
and adds the original edge (in G ) to G . The other property of the algorithm is that it does not
0 T
disconnect the graph — an edge e which is a cut edge will be present in all arborescences and hence
will have p = 0, ensuring that it will never be selected for deletion (in line 9). So, the algorithm
e
will terminate when (r ) is the set of nodes reachable from r = r , and G will have the required
j 0 T
S
spanning arborescence.
For the time complexity bound, note that the while loop (lines 5-20) will run at most E
| |
times, while the expensive step inside the while loop will be the two determinant computations in
line 7 (to compute p ). These can be done in O(V 3) time in the real RAM model, by Gaussian
e
| |
elimination etc. Contraction, deletion etc. require only O(V + E ) O(V 2) time. This gives us
| | | | ≤ | |
the O(E V 3) bound.
| || |
48