Open-vocabulary Auditory Neural Decoding Using
fMRI-prompted LLM
XiaoyuChen∗ ChangdeDu∗ LiuChe
LaboratoryofBrainAtlasand LaboratoryofBrainAtlasand LaboratoryofBrainAtlasand
Brain-InspiredIntelligence,StateKey Brain-InspiredIntelligence,StateKey Brain-InspiredIntelligence,StateKey
LaboratoryofMultimodalArtificial LaboratoryofMultimodalArtificial LaboratoryofMultimodalArtificial
IntelligenceSystems,CASIA IntelligenceSystems,CASIA IntelligenceSystems,CASIA
SchoolofArtificialIntelligence, Beijing,China UniversityofChineseAcademyof
UniversityofChineseAcademyof changde.du@ia.ac.cn Science
Science Beijing,China
Beijing,China liuche2022@ia.ac.cn
chenxiaoyu2022@ia.ac.cn
YizheWang HuiguangHe†
CASIA LaboratoryofBrainAtlasand
Beijing,China Brain-InspiredIntelligence,StateKey
yizhe.wang@ia.ac.cn LaboratoryofMultimodalArtificial
IntelligenceSystems,CASIA
SchoolofArtificialIntelligence,
UniversityofChineseAcademyof
Science
Beijing,China
huiguang.he@ia.ac.cn
ABSTRACT method.Theexperimentalresultsdemonstratethatusingbrain
Decodinglanguageinformationfrombrainsignalsrepresentsa representationasaprompttofurtherdriveLLMforauditoryneural
vitalresearchareawithinbrain-computerinterfaces,particularlyin decodingisfeasibleandeffective.
thecontextofdecipheringthesemanticinformationfromthefMRI
CCSCONCEPTS
signal.However,manyexistingeffortsconcentrateondecoding
smallvocabularysets,leavingspacefortheexplorationofopen •DoNotUseThisCode→GeneratetheCorrectTermsfor
vocabularycontinuoustextdecoding.Inthispaper,weintroducea YourPaper;GeneratetheCorrectTermsforYourPaper;Generate
novelmethod,theBrainPromptGPT(BP-GPT).Byusingthe theCorrectTermsforYourPaper;GeneratetheCorrectTermsfor
brainrepresentationthatisextractedfromthefMRIasaprompt, YourPaper.
ourmethodcanutilizeGPT-2todecodefMRIsignalsintostimulus
text.Further,weintroduceatext-to-textbaselineandalignthe KEYWORDS
fMRIprompttothetextprompt.Byintroducingthetext-to-text
Do,Not,Us,This,Code,Put,the,Correct,Terms,for,Your,Paper
baseline,ourBP-GPTcanextractamorerobustbrainpromptand
promotethedecodingofpre-trainedLLM.WeevaluateourBP-GPT
1 INTRODUCTION
ontheopen-sourceauditorysemanticdecodingdatasetandachieve
asignificantimprovementupto4.61%onMETEORand2.43%on “Thelimitsofmylanguagemeanthelimitsofmyworld”-Ludwig
BERTScoreacrossallthesubjectscomparedtothestate-of-the-art Wittgenstein. Wittgenstein’s statement refers to the standpoint
thataperson’sentireunderstandingoftheworldisreflectedinthe
∗Bothauthorscontributedequallytothisresearch. thingstheycandescribeinlanguage.Soitisimportantforhuman-
†correspondingauthor. centricartificialintelligence,forexample,thebrain-computerin-
terface,tounderstandthelanguageinformationfromthehuman
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
brain.
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation In recent years, due to the rapid development of deep learn-
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe inganditswidespreadapplicationinbrain-computerinterfaces,
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
especiallyinneuralencodinganddecoding[7,11,12,26,30],sig-
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. nificantprogresshasbeenmadeintheresearchfieldofextracting
ACMMM,2024,Melbourne,Australia languageinformationfrombrainsignals.Forexample,reconstruct-
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ingaudioofauditorystimulifrombrainsignals[4,18,25,33],or
ACMISBN978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnn reconstructingcorrespondingtext[2,4,20,28,32].Inthisarticle,
4202
yaM
31
]CH.sc[
1v04870.5042:viXraACMMM,2024,Melbourne,Australia XiaoyuChen,ChangdeDu,LiuChe,YizheWang,andHuiguangHe
prompt.Subsequently,wereconstructedthetextusingGPT-2in
someone who has a temper a very very bad temper amannersimilartothebrain-to-text.Sincetheinputandoutput
ofthetext-to-textbaselineareidentical,eliminatingmodaldiffer-
ences,wedesignatethetextpromptastheoptimalpromptforthe
ground-truthtext.Consequently,weincorporatedacontrastiveloss
toalignthefMRIpromptwiththetextprompt,enhancingdecoding
performance.
Insummary,ourmaincontributionsareasfollows:
• WeproposeBrainPrompt-GPT(BP-GPT),whichisanovel
structurethatcanusethefMRIprompttodecodethetextof
Prompt speechstimuliinanend-to-endstructure.
• Weintroducedalanguagepriorbyapre-trainedLLM(GPT-
2)tocompensateforthelowtemporalresolutionoffMRI;
Figure1:Wefocusondecodingsemanticinformationfrom
throughcontrastivelearning,weencouragefMRIprompts
fMRIintheauditoryneuraldecodingscenarioandusefMRI
toalignwithtextprompts,therebyreducingtheimpactof
signalsaspromptstoguideapre-trainedGPT-2toachieve
modaldifferencesondecodingperformance.
decoding.
• WeevaluatedourBP-GPTmodelonanopen-sourceaudi-
torysemanticdecodingdatasetandachievedasignificant
wefocusonreconstructinglanguageinformationbydecodingthe improvement of up to 4.61% on METEOR and 2.43% on
originaltext(seeFigure1foranillustration).Specifically,wefocus BERTScoreacrossallsubjectscomparedtothestate-of-the-
ontextdecodinginauditoryneuraldecodingscenarios.Forthe artmethod.Theseresultsdemonstratethefeasibilityand
typesofbrain-computerinterfaces,wefocusondecodingtextfrom advantagesofourapproach.
functionalmagneticresonanceimaging(fMRI),awidelyusednon-
invasivebrain-computerinterfaceinbothresearchandpractical 2 RELATEDWORK
applications.
2.1 Largelanguagemodels
Toaccomplishthisgoal,twoprimarychallengesmustbead-
dressed.Firstly,thetemporalresolutionoffMRIsignalspresents Since the introduction of the transformer architecture [29], nu-
asignificantobstacle.Despiteitscommendablespatialresolution meroustransformer-basedLargeLanguageModels(LLMs)have
andnon-invasivecharacteristics,fMRIsignalsexhibitsignificantly emerged.Whileallthesemodelsutilizestackedattentionlayers
lowtemporalresolution.Forinstance,inthecontextofcommonly toconstructtheirnetworks,eachexhibitsuniquecharacteristics.
spokenEnglish,wheretheaveragespeakingspeedexceeds2words Broadly speaking, existing LLMs can be categorized into three
persecond[28],theBOLDresponsetoneuralactivityrisesand groups:encoder-only,decoder-only,andencoder-decoder.
fallsoverapproximately10secondswhichisextremelyslowerthan Theencoder-onlyLLMsaremostlyusedtoextractfeaturesof
thespeechstimuli[14].Secondly,anotheressentialchallengeisthe inputtextandareusedfordifferentdownstreamtasks.Themost
significantdifferencebetweenfMRImodalityandtextmodality.In representativeoneistheBERT[6]model.BERTadoptsabidirec-
fact,inauditoryinformationdecodingscenarios,thetextdoesnot tionalarchitectureandtrainsusingamaskedlanguagemodeland
presentasthestimulussignalreceivedbythesubject.Rather,it next-sensepredictiontask,allowingittoextractrepresentationsof
containsthesemanticsofthestimulussignal.Therefore,thequality textbasedonthepreviousandfollowingtext.Apopularoptimized
ofmodalalignmentsignificantlyinfluencestheeffectivenessofthe versionofBERTisRoBERTa[13],whichbuildsuponBERT’sarchi-
decodingmodel. tectureandpre-trainingobjectives,refiningthetrainingprocess
ThelowtemporalresolutioninfMRInecessitatesourmodelto toachieveimprovedperformance.Itremovesthenextsentence
decodemultiplewordsfromasinglefMRIsignalduringthedecod- predictionobjectiveandtrainsonmoredataandforlongerepochs,
ingprocess.Totacklethisill-posedinverseproblem,weproposea leadingtobetterrepresentations.
solutionbyincorporatingalanguagepriorthroughapre-trained Decoder-onlyLLMsstemfromthedecodercomponentofthe
LargeLanguageModel(LLM)-GenerativePre-trainedTransformer transformer architecture. Their key feature lies in masking fu-
2(GPT-2)[22].AsillustratedinthebottompartofFigure2,wees- turepositionstoensurethatpredictionsofthecurrenttokenare
tablishamappingfromfMRItopromptsandtrainGPT-2toemploy basedsolelyonprecedingtokens.Themostprominentexample
autoregressivemethods,generatingcorrespondingtextbasedon ofdecoder-onlyLLMsistheGPT(GenerativePre-trainedTrans-
theprovidedfMRIprompts.Byapplyingthecross-entropylossto former)series[1,3,21,22],whichintroducesalarge-scaleautore-
theoutputlogitofGPT-2,thefMRIencodercanlearnthesuitable gressivelanguagemodelbasedontheTransformerarchitecture.
promptsforthetargettext. Thesemodelshavedemonstratedtheeffectivenessofunsupervised
TomitigatethesubstantialmodaldisparitybetweenthefMRI pre-trainingfollowedbyfine-tuningacrossvariousnaturallan-
signalandthetextdata,weintroducedanothertext-to-textbase- guageprocessingtasks.Beyondtheirtechnicalaspects,theyhave
lineduringthetrainingstage.Asdepictedintheuppersectionof significantlyadvancednaturallanguageunderstandingandgener-
Figure2,weutilizedBidirectionalEncoderRepresentationsfrom ationtasks,includingtranslation,questionanswering,codegen-
Transformers(BERT)[6]asthetextencodertoextractthetext eration,andevencreativewriting.Notably,withtheemergenceOpen-vocabularyAuditoryNeuralDecodingUsingfMRI-promptedLLM ACMMM,2024,Melbourne,Australia
Text Text Prompt Text
BERT
Text
text tokens
Text-to-Text
baseline
CE
Loss
Text Prompt
Contrastive
Loss
fMRI Prompt
fMRI
fMRI Encoder
Figure2:Thetrainingstagesofourmethod.Theupperpart:weusetheBERTandGPT-2fortheencoderanddecoderof
ourtext-to-textbaseline.Inthisbaseline,theBERTrepresentationwillbemappedintoatextpromptwhichisusedfor
reconstructingtheoriginaltextusingGPT-2.Thelowerpart:weuseatransformerfMRIencodertoextractthefMRIprompt
andaddacontrastivelosstoalignthefMRIprompttothetextprompt.Then,theGPT-2willdecodethetextaccordingtothe
fMRIprompt.
ofChatGPT,theGPTserieshasbecomethemostpopularchoice 2.2 DecodingtheBrainSignalsintoText
amongLLMs. Intheearlystagesoftextdecoding,modeloutputsoftencomprised
Inadditiontotheaforementionedarchitectures,therearemod- asinglevocabularyandonlysupportedalimitedvocabularyset.
elsthatutilizetheencoder-decoderarchitecturetoleveragethe Forinstance,intheBrain2word[2],researchersemployedaclassi-
strengthsofbothcomponents.ExamplesincludeBART[10]and ficationmodeltodecodeparticipants’brainsignalswhentheyread
T5[23].Whilebothmodelsarebasedontheencoder-decoderar- words.InthestudybyDefossezetal.[4],contrastivelearningwas
chitecture,eachadoptsauniqueapproachandpossessesdistinct appliedtodecodewordsorphrasesfromauditorybrainsignals.
capabilitiesfornaturallanguageprocessingandgenerationtasks. Additionally,Pereiraetal.[20]andSunetal.[27]exploredthe
T5emphasizesatext-to-textframeworkforunifiedprocessingof decodingofsentencesfrombrainsignalsintheirrespectiveworks.
varioustasks,whereasBARTfocusesonbidirectionalandauto- Inrecentyears,duetothesuccessofLLMs,someworkhasbegun
regressivetrainingforsequence-to-sequencetaskssuchassumma- toattempttouseLLMstodecodecompletetextsfromvarioustypes
rizationandtranslation. ofbraindata.Forexample,Wangetal.[31]proposedusingBART
[10]todecodetextfromtheEEGandeyetrackingsignalsoftheACMMM,2024,Melbourne,Australia XiaoyuChen,ChangdeDu,LiuChe,YizheWang,andHuiguangHe
subjectsduringreading,achievingthedecodingofopenvocabular-
ies.InDeWave[8],Duanetal.proposedusingaquantizedvariant 𝑁
encoderandBARTtoimprovethedecodingofEEG2text,achieving L𝑡𝑒𝑥𝑡 =−∑︁ log𝑝 𝜃(W|P𝑖𝑇 ) (2)
decodingwithoutexternaleventmarkerslikehandlingoreyetrack- 𝑖=1
ing.UnlikedecodingusingEEG,inUniCoRN[32],Xietal.alsoused 𝑁 L
BARTtodecodefMRIsignalsintotext.Tangetal[28].usedlinear =−∑︁∑︁ log𝑝 𝜃(𝑤 𝑗|𝑝𝑇 1,...,𝑝𝑇 𝑘,𝑤 1,...,𝑤 𝑗−1), (3)
regressionandGPT,basedontheneuralencodingarchitecture,to 𝑖=1 𝑗=1
completetextdecodingusingsimilaritymeasurement.Ourwork, wheretheW =(𝑤 1,...,𝑤 𝑗)isthetokensofthetext,and𝑘isthe
alongwithUniCoRNandTangetal.’smostrelevant,isbasedon
lengthofprompt.
fMRIdecoding.DifferentfromUniCoRNwhichtreatsfMRIasa
foreignlanguageandusesmachinetranslationstructurefortext 3.2 fMRItoTextDecoding
decoding,weadoptedapromptapproach.ComparedtoTangetal.,
There are two main challenges in decoding text from the fMRI
whichadoptsaneuralencodingarchitecture,ourmethodadoptsa
signals:(1)thelowtemporalresolutionoffMRIand(2)thesig-
moredirectneuraldecodingarchitecture,whichisanend-to-end
nificantmodaldifferencebetweenfMRIandtext.Inthispart,we
approach.
willintroduceourmethodwhichcouldaddressthetwochallenges
above.Ourmethodconsistsoftwoessentialcomponents.Thefirst
3 METHOD isthefMRI-promptedtextdecodingmethod,whichintroducesthe
promptparadigmintofMRIdecodingandaddressestheproblemof
3.1 ATexttoTextBaseline
lowtemporalresolutionoffMRI.ThesecondisaligningthefMRI
Thefundamentalconceptofourmethoddrawsinspirationfrom prompttothetextpromptandthereforereducetheimpactofmodal
theencodinganddecodingprocessesappliedtotextanditscorre- differencesandaddressthelastchallenge.
spondingmodalities.InthefieldofNaturalLanguageProcessing
(NLP)orComputerVision(CV),varioustasksdemandtheencod- 3.2.1 fMRI-promptedtextdecoding. Duetothelowtemporalreso-
inganddecodingoftext,suchastext-to-imagegeneration[24] lutionoffMRI,weneedtodecodemultiplewordsfromeachfMRI
imagecaptioning[16,36].Inthispart,wealignboththeencoding samplepoint.Weadoptedasimilarstructureasthetext-to-text
inputanddecodingtargetwiththetextcorrespondingtoauditory baselinetosolvethisproblem.Specifically,weuseafMRIencoder
stimuli.DuetotheremarkableprogressmadebyLLMsrecently modeltoencodethefMRIintotherepresentationspaceanduse
[3,6,22]anditscontinuedevolutionatanastonishingspeed[1],we thisrepresentationasthepromptofthetextgenerationprocessof
haveestablishedtheencoderanddecoderofourtext-to-textmodel GPT-2:
entirelybasedonthepre-trainedLLMs,minimizingthevolume
ofparametersthatneedtobetrained.Thisdesignfacilitatesthe P𝑖𝐵 =E𝜂(𝑥 𝑖𝐵 ), (4)
seamlessintegrationofnewLLMs,enablingeasyupgradesforour wheretheE𝜂 isthefMRIencoder,and𝑥 𝑖𝐵 denotethefMRI.P𝑖𝐵 =
method. (𝑝𝐵,···,𝑝𝐵)
denotethefMRIpromptwhichisextractedbythe
WechoseBERTasthetextencoderandGPT-2asthetextdecoder 1 𝑘
fMRIencoder.
inthisworkanddrewinspirationfromtherecentimagecaption
Sincewedidnotintroduceapre-trainedfMRIencoderhere,the
works[16,36]whichuseencodedrepresentationasthepromptof
fMRI-to-textpartdidnotrequireamappingnetwork.Furthermore,
targettextinthedecoding.Specifically,wefirstencodethetextinto
thefMRIencodercanbetrainedusingthecross-entropylossonthe
therepresentationspaceusingthelasthiddenstateofBERT.Then
outputoftheGPT-2.Here,fine-tuningtheGPT-2isnotamandatory
amappingnetworkisappliedtomaptheBERTrepresentationinto
optioneither.Whenwechoosenottofine-tunetheGPT-2,theloss
theprompt: iscompletelydependentontheparametersofthefMRIencoder𝜂
andhasthefollowingform:
𝑁
P𝑖𝑇 =M𝜃(BERT(𝑥 𝑖)), (1) L𝑏𝑟𝑎𝑖𝑛 =−∑︁ log𝑝 𝜂(W|P𝑖𝐵 ) (5)
𝑖=1
𝑁 L
wheretheM(·)denotethemappingnetwork,andP𝑖𝑇 =(𝑝𝑇 1,...,𝑝𝑇 𝑘) =−∑︁∑︁ log𝑝 𝜂(𝑤 𝑗|𝑝 1𝐵,...,𝑝 𝑘𝐵,𝑤 1,...,𝑤 𝑗−1). (6)
istheextractedpromptsequence. 𝑖=1 𝑗=1
Toreconstructthetext,wefeedthepromptintoGPT-2togener-
However, fine-tuning the GPT-2 may bring performance im-
atetheoriginaltext.Thetextwasgeneratedusingthenext-token-
provementsindownstreamtasks.Meanwhile,inthetextdecoding
predictionparadigmandthepromptisconsideredasthepreceding
taskofthiswork,thefine-tuningoftheGPT-2canbringmorespe-
textofthetargettext.Inthetraining,theparametersofBERTare
cificbenefits.WewillintroducethisspecificbenefitsinSection3.4
fixed.FortheGPT-2,fine-tuningisnotamandatoryoptionheredue
andcomparethesetwooptionsinthelaterexperimentalsection.
tothepresenceofthemappingnetwork.Soonlytheparametersof
themappingnetworkhavetobeoptimized.Weuseacross-entropy 3.2.2 AlignwiththeOptimalPrompt. Giventheconsiderablesig-
lossbetweentheoutputofGPT-2andthetokensofthetargettext nificantmodaldifferencesbetweenfMRIandtext,extractingef-
fortheoptimizationofthemappingnetwork: fectivefMRIpromptsthroughthefMRIencoderposeschallenges.Open-vocabularyAuditoryNeuralDecodingUsingfMRI-promptedLLM ACMMM,2024,Melbourne,Australia
Figure3:Anillustrationoftheinferencestageisprovidedhere.Duringthisstage,thefMRIpromptisconsideredasthe
precedingtextforthetargettextgeneration.Subsequently,GPT-2generatesthetextinanautoregressivemanner,relyingon
boththefMRIpromptandthegeneratedtext.Fordecidingthelengthofdecodingtext,wecomparedtwostrategiesinthis
work.Thefirstoneistousethewordratemodeltopredictthelengthoftext;Thesecondoneistousespecialtokensand
fine-tunetheGPT-2,thedecodingprocesswillendwhenGPT-2generatesenoughspecialtokens($inourimplementation).
Conversely,thetextbaselineinherentlycircumventsthismodaldif- formula:
ference.Sowearguethatthepromptderivedfromthetextbaseline
canbedeemedastheoptimalpromptforthetexttobegenerated. 𝐿=𝐿 𝑏𝑟𝑎𝑖𝑛+𝛼𝐿 𝐶, (10)
Toutilizetheoptimalprompt,weemploycontrastivelearningto
alignthefMRIpromptwiththetextprompt. wherethe𝛼 isahyperparameterforthecontrastiveloss.
Specifically,wesetthefMRIpromptandtextpromptofthesame
textasthepositivepairandcalculatethesimilaritybetweenthe
3.4 Inference
positivepairusingthefollowingformula:
As is shown in Figure 3, in the inference stage, we extract the
S𝑝 =exp(𝑐𝑜𝑠(P𝐵𝑖 ·P𝑇𝑖 )/𝜏), (7) f oM nR thI ep fr Mom RIp pt rfi or mst pa t.n Td hg ee tn ee xr ta wte illth be et ge ex nt eu ras tin edg wG oP rT d-2 byde wp oe rn dd ii nng
a
wherethe𝜏 referstothetemperaturehyperparameter. next-token-predictionparadigm.
Forthenegativepair,weusethefMRIpromptandtextprompt Asourfocusisontextdecodingwithinanauditoryneuralde-
fromdifferenttextsandthefMRIpromptofdifferenttexts.The codingscenario,theauditorystimulireceivedbythesubjectsexclu-
similaritybetweenthenegativepairscanbeformulatedas: sivelyconsistofwordswithoutpunctuation.Thiswillcausetrouble
forthemodelduringtheinferencephase,aswecannotdetermine
theendofgenerationthroughthestoptoken(usually,periodsare
S𝑛 =exp(𝑐𝑜𝑠(P𝐵𝑖 ·P 𝐵𝑗 )/𝜏)+exp(𝑐𝑜𝑠(P𝐵𝑖 ·P 𝑇𝑗 )/𝜏),𝑖 ≠ 𝑗. (8) used).Althoughpunctuationcanbemanuallyaddedduringtext
annotationforaudiostimuli,thisintroducestwoconcerns.Firstly,
Basedonthedefinitionabove,thecontrastivelosshasthefol-
thespeakermighthavedeliveredaspontaneousspeechwithout
lowingform:
adheringtotheground-truthspeechdraft.Secondly,sincethede-
codingoftenusesthefMRIsignalswithinafixed-lengthwindow,
(cid:20) 𝑆 (cid:21)
𝐿 C =−E log 𝑆𝑝 . (9) andtheendofthiswindowdoesnotalwaysmatchtheendofthe
𝑛 sentence,evenifweaddthepunctuationmanually,theyarevery
likelyunabletoindicatetheendofthegeneration.
3.3 Training
Forthereasonsoutlinedabove,wechosenottoincludepunctua-
Wewilldividethetrainingprocessintotwostages.Intheinitial tioninthemodeltrainingprocess.Tofindtheendofthegeneration
stage, the text-to-text baseline is trained for text encoding and intheinferencestage,weoffertwostrategieshere,anillustration
decoding.Thisprocessenablesthemodeltolearntoextractthe ofthesetwostrategiesisshowninFigure3.
optimalpromptforthetargettext,whichisthenutilizedasthe Inthefirstone,weadoptanapproachfromrecentwork[28],uti-
targetofcontrastivelearninginthesubsequentstage.Then,we lizingawordratemodeltopredictthenumberofwordsperceived
trainourdecodingmodelinthesecondstageusingthefollowing byparticipants.ThetextgenerationprocesswillbestoppedwhenACMMM,2024,Melbourne,Australia XiaoyuChen,ChangdeDu,LiuChe,YizheWang,andHuiguangHe
thelengthofthegeneratedtextmeetsthewordcountpredictedby
thewordratemodel.
Thesecondstrategyistousethespecialtokentosegmenttext
basedontherepetitiontime(TR)offMRI.Inthiswork,weadd$to
thegound-truthtextduringthetraining,andstopthegeneration
whenwemeetenough$intheinferencetext.Wealsoaddanequal
marktopointoutthebeginningofthegourd-truthtext.
3.5 MappingNetworkandfMRIEncoderModel
ThecoreofourmodelliesinmappingtheoriginalfMRIsignalor
representationofthetextencoderintotheprompt.Giventhecom- (a)UTS01
plexitiesinthesetransformations,especiallyforthefMRI-to-text
part,weemployedthetransformerstructures[29]toaccomplish
themappingoforiginalfeaturestotheprompt.Thetransformer’s
inherentcapabilityforglobalattentionenablestheextractionof
semanticrelationshipsfromtheinputfMRIsequence,yieldinga
fMRIpromptbettersuitedforthetargettext.
4 EXPERIMENT
4.1 Dataset
We evaluate our method on an fMRI dataset obtained during a
passivenaturallanguagelisteningtask[9]alongwithitsextended
data[28].ThisdatasetcomprisesfMRIdatafrom8subjectsrecorded (b)UTS02
whiletheypassivelylistenedtonaturallyspokenEnglishstories.
The stories were sourced from The Month and New York Times
ModernLovepodcasts.Specifically,thefirst3subjects(UTS01to
UTS03)inthedatasethadaccesstostoriesfrombothTheMonthand
NewYorkTimesModernLove,expandingthetotalnumberofstories
to84forthesesubjects.Fortheremaining5subjects(UTS04to
UTS08),all27storiesarefromTheMonth.Tomaintainconsistency
withexistingworks[9,28],weselectedthestory"WhereThere’s
Smoke",sharedbyallsubjects,asthetestset.
Inthevanillasectionofthedataset,thestoriesweredivided
into5sections,eachcontaining5storiesandanadditionaltest
story("WhereThere’sSmoke").Thisconfigurationresultedina
(c)UTS03
totaldurationofstimulatingaudioexceeding6hoursforallsub-
jects.Intheextendedsectionofthedataset,thefirstthreesub-
jectsunderwentanadditional10sessions,increasingtheoverall Figure4:Thecorticalflatmapsfortheauditorycortex(inred
datasetdurationto81hoursacrossallsubjects.Allstoriesfeature color)ofthedifferentsubjectsweused.
asinglespeakernarratinganautobiographicaltalewithoutapre-
paredscript.Thetextsweremanuallytranscribedbyonelistener
andautomaticallyalignedtotheaudiousingthePennPhonetics multiple20-secondwindows,withzerooverlaps.Fortheprompt
LabForcedAligner(P2FA)[34].Consequently,thetranscriptions length,weuse𝑘 =30.
lackpunctuationmarks,andsomewordsmayberepeatedasthe Forthemappingnetwork,theBERTrepresentationwillbefirst
speakeristhinkingandorganizingtheirlanguage.Additionally, mappedtoa512-dimensionalvectorbeforepassingforwardtothe
certainsounds,suchas"cough,""laugh,""lipsmack,""miscnoise," transformer.Weusean8-layertransformerhere,with8attention
and"silence,"wereannotated. headsineachlayer.ThefMRIencoderhasthesamearchitectureas
themappingnetwork,exceptforalinearlayerfortheinput.
4.2 ImplementingDetails AllthecodesareimplementedusingPyTorch[19],andtraining
onanNvidiaA-100GPUwithAdamWoptimizer[15].Thebatch
Inourwork,wechoosethefirstthreesubjectsthathavetheex-
sizeissetto32.
tendeddataforallofourexperiments.Fortheregionofinterest
(ROI),weusethevoxelintheauditorycortexforourexperiment
4.3 BaselineandEvaluationMetrics
(seeFigure4forthecorticalflatmaps).Thetemperatureofcon-
trastivelossissetto𝜏 =0.1,andtheweightofcontrastivelossisset We compare our method to Tang et al. [28], which is the state-
to𝛼 =1.WesplitthefMRIsequenceandcorrespondingtextinto of-the-artmethodinthedatasetweused.Intheirmethod,theyOpen-vocabularyAuditoryNeuralDecodingUsingfMRI-promptedLLM ACMMM,2024,Melbourne,Australia
BLEU-1↑ METEOR↑ BERTScore↑
UTS01 UTS02 UTS03 UTS01 UTS02 UTS03 UTS01 UTS02 UTS03
T2T+WR 0.1968 0.2085 0.1862 0.1414 0.1466 0.1266 0.8192 0.8205 0.8163
T2T+WR+fine-tune 0.2296 0.2421 0.241 0.1692 0.1699 0.176 0.8242 0.8278 0.8259
T2T+Spe 0.2189 0.2044 0.2187 0.2032 0.204 0.2082 0.8325 0.8328 0.8343
T2T+Spe+fine-tune 0.2621 0.2613 0.2554 0.2627 0.2597 0.2549 0.8432 0.8451 0.8417
Table1:Text-to-textperformance.’WR’referstousewordratemodelintheinference,and’Spe’referstothespecialtokens.In
theresultswithoutannotation’fine-tune’,theparameterofGPT-2isfixed.
useaneuronencodingstructure,whichusestheGPTtogenerate 4.5 EvaluationoffMRItoTextDecoding
proposalwordsandencodetheproposalwordstofMRItofindthe Inthispart,weevaluatethedecodingperformanceofourmodel.To
mostmatchingword.Forafaircomparison,weusethesamestory alignwithexistingwork[28],wereporttheperformanceofthefirst
("WhereThere’sSmoke")inthedatasetasthetestsetanddividethe threesubjects:UTS01,UTS02,andUTS03,whohaveexperienced
entirestoryinto20-secondwindows,calculateevaluationmetrics all84storiesintheexperiment.Forthesettingofourmethod,we
withineachwindow,andtaketheaverageofallwindowsasthe addspecialtokensintheground-truthtextforthetraining.The
metricscorefortheentiretestset. GPT-2isfine-tunedinthetraining,andtheinferenceisstopped
SimilartoTangetal.[28],weuseidenticallanguagesimilarity whentheGPT-2generatesenough$.Werefertothissettingasthe
metricstoevaluateourmethodinseveralaspects.BLEU[17]indi- BP-GPTinalltheexperiments.
catesthenumberofindividualtranslatedsegmentsthatappearin As is shown in Table 2, our method achieves comparable or
theground-truthtext.METEOR[5]computestheharmonicmean evenbetterperformance.Specifically,ontheMETEOR,ourmethod
ofunigramprecisionandrecall.AndBERTScore[35]computesa canachieveanimprovementrangingfrom2.99%to4.61%onall
similarityscoreforeachtokeninthecandidatesentencewitheach thesubjects.FortheBERTScore,wealsoachieveanimprovement
tokeninthereferencesentenceusingcontextualembeddings. rangingfrom1.87%to2.43%onallthesubjects.
4.6 AblationStudy
Inthispart,wemakeablationstudiesthatevaluatethecontributions
4.4 EvaluationtheText-to-textBaseline
ofcontrastivelearninganddifferentinferencestrategiesforthe
Sincewetreatthetextpromptastheoptimalpromptfordecoding performance.WereporttheseexperimentresultsinTable3and
inourmethod,wefirstevaluatetheperformanceofthetext-to-text Table4.Fortheinferencestrategies,wemark’WR’fortheresults
baselineinourwork.Weconsider4settingsinthispart.Forthe thatusethewordratemodeltodistinguishitfromtheresultsthat
firsttwosettings,weusethewordratemodelintheinferenceand usethespecialtokensintheinference.
fine-tuneorfixtheparametersofGPT-2inthetraining.Forthe
lasttwosettings,weusethespecialtokensandalsocomparetwo 4.6.1 ContrastiveLearning. Todemonstratetheeffectivenessof
optionsforfine-tuningorfixingtheparametersofGPT-2. contrastivelearning,wecomparetheperformanceofourmethod
TheexperimentresultsarelistedinTable1.Fromtheexperimen- withorwithoutcontrastivelearning.Weincludetheexperiment
talresults,wecanfindthatourtext-to-textbaselinecaneffectively resultsonbothinferencestrategiessincethechallengeofthesig-
encodeanddecodetextundervariousexperimentalsettings.This nificantmodaldifferencebetweenthefMRIandtextexistsboth
resultsupportsourfurtherapplicationofthispromptparadigmin inthesesettings.Wewanttoexplorethroughtheseexperiments
fMRI-to-textdecoding.Also,wefindthatfine-tuningtheGPT-2in whetheraligningthefMRIpromptwiththetextpromptcanbring
thetrainingcanalwaysbringimprovementsinperformance,regard- performanceimprovementsinvariousinferencestrategies.Here,
lessoftheinferencestrategyweused.Fortheinferencestrategies, differentinferencestrategieswillleadtodifferenttext-to-textbase-
ratherthanusingawordratemodeltoinferthetextlength,wefind lines.Asthetargetoftheconservativelearning,thetextprompt
thataddingspecialtokensintheground-truthtextinthetraining isextractedusingthetext-to-textbaselinethathasthesameinfer-
stagecaneffectivelyimprovethedecodingperformance.Moreover, encestrategyasthefMRI-to-textmodel.Wewouldliketoreferthe
wefindthatfine-tuningtheGPT-2forthespecialtokenscanbring Section4.4formoredetails.
asignificantimprovementintheresults.Specifically,thissetting WereporttheexperimentresultsinTable3.BycomparingTable
canbringupto5.69%onBLEU-1,5.95%onMETEOR,and1.23% 3withTable1,wecanfindthatthereisabiggapbetweenthe
onBERTScoreamongallthesubjects.Basedontheseexperiment performanceoffMRI-to-textdecodingandtheperformanceofthe
results,wechoosethesettingthatusesspecialtokensandfine-tune text-to-textdecoding,indicatingthemodaldifferencesbetweentext
theGPT-2asthetext-to-textbaselineforallthefollowingexperi- andfMRIimpactthedecodingperformanceseriously.Also,through
mentswhichusethespecialtokensasinferencestrategies.Also, theresults,wefindthataligningthefMRIpromptwiththetext
thetext-to-textbaselineforthewordratemodelstrategiesinthe promptalwaysbringsperformanceimprovements,nomatterwhat
followingexperimentsisalsofine-tuned. inferencestrategieshavebeenchosen.Specifically,whenusingaACMMM,2024,Melbourne,Australia XiaoyuChen,ChangdeDu,LiuChe,YizheWang,andHuiguangHe
BLEU-1↑ METEOR↑ BERTScore↑
UTS01 UTS02 UTS03 UTS01 UTS02 UTS03 UTS01 UTS02 UTS03
Tang et al. 0.2331 0.2426 0.2470 0.1621 0.1677 0.1703 0.8077 0.8104 0.8116
BP-GPT 0.2159 0.2111 0.2113 0.2082 0.1976 0.2034 0.832 0.8322 0.8332
Table2:Compareourmethodwiththeexistingwork.TheBP-GPTreferstousingthespecialtokensfortheinferenceand
fine-tuningtheGPT-2.
BLEU-1↑ METEOR↑ BERTScore↑
UTS01 UTS02 UTS03 UTS01 UTS02 UTS03 UTS01 UTS02 UTS03
BP-GPT-wo-contras(WR) 0.1855 0.1851 0.1979 0.1372 0.1358 0.1442 0.8134 0.8131 0.8172
BP-GPT(WR) 0.2052 0.1944 0.2017 0.1451 0.1476 0.1497 0.8192 0.8164 0.8198
BP-GPT-wo-contras 0.2027 0.2041 0.2043 0.1943 0.1958 0.1946 0.829 0.8278 0.8281
BP-GPT 0.2159 0.2111 0.2113 0.2082 0.1976 0.2034 0.832 0.8322 0.8332
Table3:Abalationstudy:contrastivelearning.Experimentsthatusethewordratemodelintheinferencearemarkedwith
’WR’.
BLEU-1↑ METEOR↑ BERTScore↑
UTS01 UTS02 UTS03 UTS01 UTS02 UTS03 UTS01 UTS02 UTS03
BP-GPT-wo-fine-tune(WR) 0.198 0.1936 0.1997 0.1343 0.1409 0.139 0.8157 0.8145 0.8129
BP-GPT(WR) 0.2052 0.1944 0.2017 0.1451 0.1476 0.1497 0.8192 0.8164 0.8198
BP-GPT-wo-fine-tune 0.2083 0.1949 0.2065 0.206 0.1924 0.2022 0.8318 0.8295 0.8298
BP-GPT 0.2159 0.2111 0.2113 0.2082 0.1976 0.2034 0.832 0.8322 0.8332
Table4:Abalationstudy:inferencestrategy.Experimentsthatusethewordratemodelintheinferencearemarkedwith’WR’.
wordratemodelattheinferencestage,aligningthefMRIprompt thatcorrespondstoafMRITR,bringinganegativeinfluenceon
tothetextpromptcanbringanimprovementupto1.97%onBLEU- theperformance.
1,1.18%onMETEOR,and0.58%onBERTScore.Whilechoosing
thespecialtokensforinference,contrastivelearningcanbringan 4.7 PromptLength
improvementupto1.32%onBLEU-1,1.39%onMETEOR,and0.44%
Inourmethod,fMRIdrivesGPT-2togeneratedecodingtargets
onBERTScore.Thisresultprovesthatourapproachofaligning throughtheprompt.Therefore,thelengthofthepromptiscrucial
fMRIpromptswithtextpromptsisfeasibleandeffective. for the decoding performance. In this part, we investigated the
relationshipbetweenpromptlengthanddecodingperformance
4.6.2 InferenceStrategy. Duetothecharacteristicsofdecoding andreportedtheresultinFigure5.WeuseourBP-GPTsetting,that
tasksinauditorydecodingscenarios,thechoiceinferencestrategy isusespecialtokensininferencewithafine-tunedGPT-2model.
hasbecomeparticularlyimportant.Wefurthertaketheablation Asisshowninthefigure,thedecodingperformanceincreases
studyonit.Specifically,wecomparedfourexperimentalsettings,in- withthelengthoftheprompt.Althoughalongerpromptlengthcan
cludingtheperformanceoftwoinferenceschemeswithafine-tuned bringbetterresults,italsoincursgreaterhardwarecosts,whetherit
andnotfine-tunedGPT-2.TheresultisreportedinTable4.Also, isforthemappingnetworkorthefMRIencoder.DuetoGPUmem-
sameasinSection4.6.1,thealigningtargetisthecorresponding orylimitations,themaximumpromptlimitduringourexperiment
textpromptunderthesameinferencestrategy. was30.However,basedontheexperimentalresults,weexpecta
AsisshowninTable4,wefindthatusingspecialtokenstoindi- betterperformancewithalongerpromptlength.
catetheendofdecodingcanalwaysbringaperformanceimprove-
4.8 ConclutionandFutureWorks
ment.Also,fine-tuningtheGPT-2canbringmoreimprovement
withthespecialtokens.Webelievethatfine-tuningtheGPT-2in Inthiswork,weproposeadecodingmethodcapableofextracting
thetrainingcanmaketheparametersofboththefMRIencoder textfromfMRIsignalswithintheauditoryneuraldecodingsce-
andGPT-2adapttothespecialtoken.However,iffine-tuningis nario.Thecentralconceptofourmethodinvolvesemployingan
notperformed,onlythefMRIencoderwilllearntoadjustthefMRI fMRI-promptedLargeLanguageModel(LLM)fordecoding.Specif-
prompttoenableGPT-2tooutput$attheendofeachtextfragment ically,weutilizeanfMRIencodertoextractfMRIrepresentations,Open-vocabularyAuditoryNeuralDecodingUsingfMRI-promptedLLM ACMMM,2024,Melbourne,Australia
whichserveaspromptsforthepre-trainedGPT-2model.Through REFERENCES
theapplicationofcross-entropyloss,ourfMRIencoderlearnsthe [1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,Floren-
appropriatepromptforGPT-2togeneratethetargettext.Inorderto ciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,Shyamal
Anadkat,etal.2023. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774
reducemodaldifferencesbetweenthetextandfMRI,weintroduce
(2023).
acontrastivelosstoalignthefMRIpromptwiththetextprompt. [2] NicolasAffolter,BeniEgressy,DamianPascual,andRogerWattenhofer.2020.
Experimentalresultsdemonstratetheeffectivenessandadvantage Brain2word:decodingbrainactivityforlanguagegeneration. arXivpreprint
arXiv:2009.04765(2020).
ofourapproach. [3] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,
Insummary,ourprompt-basedLLMdecodingmethodoffers PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda
Askell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneural
easeofimplementationandextendstovarioustext-basedneural
informationprocessingsystems33(2020),1877–1901.
decodingtasks.Furthermore,withtheongoingadvancementof [4] AlexandreDéfossez,CharlotteCaucheteux,JérémyRapin,OriKabeli,andJean-
LLMs,ourmethodremainsreadilycompatiblewithupdatedand RémiKing.2023.Decodingspeechperceptionfromnon-invasivebrainrecordings.
NatureMachineIntelligence5,10(2023),1097–1107.
superiorLLMs,facilitatingperformanceimprovementseffortlessly.
[5] MichaelDenkowskiandAlonLavie.2014. Meteoruniversal:Languagespe-
Movingforward,ourfocuswillbeonapplyingourpromptedLLM cifictranslationevaluationforanytargetlanguage.InProceedingsoftheninth
decodingparadigmtoabroaderrangeofneuraldecodingfields workshoponstatisticalmachinetranslation.376–380.
[6] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert:
andintegratingitwithadditionalLLMs. Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.arXiv
preprintarXiv:1810.04805(2018).
[7] ChangdeDu,KaichengFu,JinpengLi,andHuiguangHe.2023.Decodingvisual
neuralrepresentationsbymultimodallearningofbrain-visual-linguisticfeatures.
IEEETransactionsonPatternAnalysisandMachineIntelligence(2023).
[8] YiqunDuan,JinzhaoZhou,ZhenWang,Yu-KaiWang,andChin-TengLin.2023.
Dewave:Discreteeegwavesencodingforbraindynamicstotexttranslation.
arXivpreprintarXiv:2309.14030(2023).
[9] AmandaLeBel,LaurenWagner,ShaileeJain,AneeshAdhikari-Desai,Bhavin
Gupta,AllysonMorgenthal,JerryTang,LixiangXu,andAlexanderGHuth.2023.
AnaturallanguagefMRIdatasetforvoxelwiseencodingmodels.ScientificData
10,1(2023),555.
[10] MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,Abdelrahman
Mohamed,OmerLevy,VesStoyanov,andLukeZettlemoyer.2019.Bart:Denoising
sequence-to-sequencepre-trainingfornaturallanguagegeneration,translation,
andcomprehension.arXivpreprintarXiv:1910.13461(2019).
[11] RuiLi,YitingWang,Wei-LongZheng,andBao-LiangLu.2022. Amulti-view
spectral-spatial-temporalmaskedautoencoderfordecodingemotionswithself-
supervisedlearning.InProceedingsofthe30thACMInternationalConferenceon
Multimedia.6–14.
[12] SikunLin,ThomasSprague,andAmbujKSingh.2022. Mindreader:Recon-
structingcompleximagesfrombrainactivities.AdvancesinNeuralInformation
ProcessingSystems35(2022),29624–29636.
[13] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer
Levy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019. Roberta:A
robustlyoptimizedbertpretrainingapproach.arXivpreprintarXiv:1907.11692
(2019).
[14] NikosKLogothetis.2003.TheunderpinningsoftheBOLDfunctionalmagnetic
resonanceimagingsignal.JournalofNeuroscience23,10(2003),3963–3971.
[15] IlyaLoshchilovandFrankHutter.2017.Decoupledweightdecayregularization.
(a)BLEU-1andMETEOR arXivpreprintarXiv:1711.05101(2017).
[16] RonMokady,AmirHertz,andAmitHBermano.2021.Clipcap:Clipprefixfor
imagecaptioning.arXivpreprintarXiv:2111.09734(2021).
[17] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. Bleu:a
methodforautomaticevaluationofmachinetranslation.InProceedingsofthe
40thannualmeetingoftheAssociationforComputationalLinguistics.311–318.
[18] BrianNPasley,StephenVDavid,NimaMesgarani,AdeenFlinker,ShihabA
Shamma,NathanECrone,RobertTKnight,andEdwardFChang.2012. Re-
constructingspeechfromhumanauditorycortex. PLoSbiology10,1(2012),
e1001251.
[19] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.
Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advances
inneuralinformationprocessingsystems32(2019).
[20] FranciscoPereira,BinLou,BriannaPritchett,SamuelRitter,SamuelJGershman,
NancyKanwisher,MatthewBotvinick,andEvelinaFedorenko.2018.Towarda
universaldecoderoflinguisticmeaningfrombrainactivation.Naturecommuni-
cations9,1(2018),963.
[21] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal.2018.
Improvinglanguageunderstandingbygenerativepre-training.(2018).
[22] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,
etal.2019.Languagemodelsareunsupervisedmultitasklearners.OpenAIblog
1,8(2019),9.
[23] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,
(b)BERTScore MichaelMatena,YanqiZhou,WeiLi,andPeterJLiu.2020.Exploringthelimits
oftransferlearningwithaunifiedtext-to-texttransformer.Journalofmachine
learningresearch21,140(2020),1–67.
Figure5:Theperformanceunderdifferentpromptlength.ACMMM,2024,Melbourne,Australia XiaoyuChen,ChangdeDu,LiuChe,YizheWang,andHuiguangHe
[24] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,Alec [30] YixinWang,ShuangQiu,DanLi,ChangdeDu,Bao-LiangLu,andHuiguang
Radford,MarkChen,andIlyaSutskever.2021.Zero-shottext-to-imagegeneration. He.2022. Multi-modaldomainadaptationvariationalautoencoderforEEG-
InInternationalconferenceonmachinelearning.Pmlr,8821–8831. basedemotionrecognition.IEEE/CAAJournalofAutomaticaSinica9,9(2022),
[25] RobertaSantoro,MichelleMoerel,FedericoDeMartino,GiancarloValente,Kamil 1612–1626.
Ugurbil,EssaYacoub,andEliaFormisano.2017.Reconstructingthespectrotem- [31] ZhenhailongWangandHengJi.2022.Openvocabularyelectroencephalography-
poralmodulationsofreal-lifesoundsfromfMRIresponsepatterns.Proceedings to-textdecodingandzero-shotsentimentclassification.InProceedingsofthe
oftheNationalAcademyofSciences114,18(2017),4799–4804. AAAIConferenceonArtificialIntelligence,Vol.36.5350–5358.
[26] PaulScotti,AtmadeepBanerjee,JimmieGoode,StepanShabalin,AlexNguyen, [32] NuwaXi,SendongZhao,HaochunWang,ChiLiu,BingQin,andTingLiu.2023.
AidanDempster,NathalieVerlinde,EladYundler,DavidWeisberg,KennethNor- UniCoRN:UnifiedCognitiveSignalReconstructioNbridgingcognitivesignals
man,etal.2024.Reconstructingthemind’seye:fMRI-to-imagewithcontrastive andhumanlanguage.arXivpreprintarXiv:2307.05355(2023).
learninganddiffusionpriors.AdvancesinNeuralInformationProcessingSystems [33] MindaYang,SameerASheth,CatherineASchevon,GuyMMckhannIi,and
36(2024). NimaMesgarani.2015.Speechreconstructionfromhumanauditorycortexwith
[27] JingyuanSun,ShaonanWang,JiajunZhang,andChengqingZong.2019.Towards deepneuralnetworks.InSixteenthAnnualConferenceoftheInternationalSpeech
sentence-levelbraindecodingwithdistributedrepresentations.InProceedingsof CommunicationAssociation.
theAAAIConferenceonArtificialIntelligence,Vol.33.7047–7054. [34] JiahongYuan,MarkLiberman,etal.2008.SpeakeridentificationontheSCOTUS
[28] JerryTang,AmandaLeBel,ShaileeJain,andAlexanderGHuth.2023.Seman- corpus.JournaloftheAcousticalSocietyofAmerica123,5(2008),3878.
ticreconstructionofcontinuouslanguagefromnon-invasivebrainrecordings. [35] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav
NatureNeuroscience(2023),1–9. Artzi.2019. Bertscore:Evaluatingtextgenerationwithbert. arXivpreprint
[29] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones, arXiv:1904.09675(2019).
AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017. Attentionisall [36] LuoweiZhou,HamidPalangi,LeiZhang,HoudongHu,JasonCorso,andJianfeng
youneed.Advancesinneuralinformationprocessingsystems30(2017). Gao.2020.Unifiedvision-languagepre-trainingforimagecaptioningandvqa.In
ProceedingsoftheAAAIconferenceonartificialintelligence,Vol.34.13041–13049.