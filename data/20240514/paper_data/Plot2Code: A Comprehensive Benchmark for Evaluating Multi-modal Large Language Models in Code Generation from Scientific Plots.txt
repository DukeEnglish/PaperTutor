Plot2Code: A Comprehensive Benchmark for
Evaluating Multi-modal Large Language Models in
Code Generation from Scientific Plots
ChengyueWu YixiaoGe QiushanGuo
TheUniversityofHongKong ARCLab,TencentPCG TheUniversityofHongKong
yixiaoge@tencent.com
JiahaoWang ZhixuanLiang
TheUniversityofHongKong TheUniversityofHongKong
ZeyuLu YingShan PingLuo
ShanghaiJiaoTongUniversity ARCLab,TencentPCG TheUniversityofHongKong
Abstract
TheremarkableprogressofMulti-modalLargeLanguageModels(MLLMs)has
attractedsignificantattentionduetotheirsuperiorperformanceinvisualcontexts.
However,theircapabilitiesinturningvisualfiguretoexecutablecode,havenot
beenevaluatedthoroughly. Toaddressthis,weintroducePlot2Code,acompre-
hensive visual coding benchmark designed for a fair and in-depth assessment
ofMLLMs. Wecarefullycollect132manuallyselectedhigh-qualitymatplotlib
plotsacrosssixplottypesfrompubliclyavailablematplotlibgalleries. Foreach
plot,wecarefullyofferitssourcecode,andandescriptiveinstructionsummarized
by GPT-4. This approach enables Plot2Code to extensively evaluate MLLMsâ€™
codecapabilitiesacrossvariousinputmodalities. Furthermore,weproposethree
automaticevaluationmetrics,includingcodepassrate,text-matchratio,andGPT-
4Voverallrating,forafine-grainedassessmentoftheoutputcodeandrendered
images. Insteadofsimplyjudgingpassorfail,weemployGPT-4Vtomakean
overalljudgementbetweenthegeneratedandreferenceimages,whichhasbeen
shown to be consistent with human evaluation. The evaluation results, which
includeanalysesof14MLLMssuchastheproprietaryGPT-4V,Gemini-Pro,and
theopen-sourcedMini-Gemini,highlightthesubstantialchallengespresentedby
Plot2Code. WithPlot2Code,werevealthatmostexistingMLLMsstrugglewith
visualcodingfortext-denseplots,heavilyrelyingontextualinstruction. Wehope
that the evaluation results from Plot2Code on visual coding will guide the fu-
turedevelopmentofMLLMs. AlldatainvolvedwithPlot2Codeareavailableat
https://huggingface.co/datasets/TencentARC/Plot2Code.
1 Introduction
In the wake of significant advancements in big data and computational power, Large Language
Models(LLMs)[33,4,12,15],suchasChatGPT[27]andGPT-4[28],havebecomefocalpoints
of interest in both commercial and academic spheres. To extend their versatility across various
contexts, Multi-modal Large Language Models (MLLMs) [8, 23, 29] have rapidly evolved, as
exemplified by the latest models such as GPT-4V [29], Gemini [9], Claude-3 [1], and the open-
sourcemodelsLLaVA[21,22],Mini-GPT[44,5]andsoon[8,7]. Concurrently,adiversearrayof
Preprint.Underreview.
4202
yaM
31
]LC.sc[
1v09970.5042:viXra(a) Ground truth plot samples (c) Overall pipeline
Requirement
Generate the matplotlib code to
create a plot that looks like the
given image. The generated code
Input should be surrounded by
```python and ```
Instruction
(Optional)
GPT-4v Judgement
The AI-generated image appears to be a The figure is a 3x2 grid of
very close match to the reference image, subplots, each with a different
with only minor differences that â€¦ variation of a streamplot. The
"rating": 9 streamplotsare created using a
grid of X and Y values ranging
(b) Generated plot samples from -3 to 3 â€¦
Response
``` python
ðŸ¤– import matplotlib.pyplotas plt
import numpyas np
Execute â€¦
plt.show()
```
Figure1: OverviewofPlot2Code. Left: (a)Representativesamplesfromthegroundtruthplotsin
ourPlot2Codedataset. (b)Plotsamplesgeneratedbymulti-modalLLMsusingthereferenceimage.
Right: (c)Thecomprehensivepipelineemployedtoassessthecodegenerationabilityofmulti-modal
LLMs. Weconsidertwodistinctsettings: DirectAskingandConditionalAsking.
evaluationbenchmarks [17,16,41,39]arecuratedtoassesstheirvisualcomprehensionperformance
across different domains. However, there remains a notable gap in attention towards diagrams
withintext-denseimages,whicharecrucialforassessingthemulti-modalreasoningproficiencyof
MLLMs[24,25].
InlinewithRichardFeynmanâ€™sphilosophy,"WhatIcannotcreate,Idonotunderstand,"evaluating
thecapabilityofMLLMstogeneratecodethatrendersaprovidedimageeffectivelyfurthershowcases
theirmulti-modalunderstandingandreasoningprowess. ThisparticularchallengedemandsMLLMs
to accurately interpret the visual elements present in input diagrams, correlate them with textual
contextprovided,andfinallyderiveexecutablecodetogeneratetheplots. Althoughthedevelopment
ofcodegenerationfromuni-modalnaturallanguagehasexperiencedrapidprogressinrecentyears[30,
10,37],theexplorationofcodegenerationusingmulti-modalinputsremainsanactiveareaofresearch.
Previousefforts,e.g. HumanEval[6]andMBPP[2],haveconcentratedonuni-modalcodeproblems,
whilethemorerecentDesign2Code[31]hasexpandedthescopetoincludeuser-interface(UI)design,
particularlyHTMLfiles,forevaluatingMLLMs. However,thesestudiesfocusonunimodalscenarios
(e.g.,text-only[6,2]orimage-only[31]inputs)andhavelimitedcapabilitieswhenevaluatingmodels
withmultimodalinputs.
Tothisend,ourworkunderscoresthecriticalimportanceandmotivationbyaddressingthreekey
challengesinevaluatingMLLMsâ€™codingcapabilities:
1. Dotheevaluationsettingsaccommodateallmodalities,includingtextandimages,for
bothinputandoutput? Thisfundamentalquestionpertainstothescopeofvisualcoding.
Byemployingextensiveevaluationsettings,wecanconductthoroughablationanalysesof
MLLMsâ€™performanceacrossvariousinputmodalitiesortheircombinations, whilealso
assessingtheoutputacrossdifferentmodalities.
2. Are the evaluation metrics accurate, straightforward, and comprehensive? Most
existingcodebenchmarksrelyonunitteststoobtainbinaryevaluationresults. Whilethis
approachmaysufficeforuni-modalcodetasks,itfallsshortforvisualcodingtasksthat
requirenotonlythecodepassratesbutalsoassessmentsofimagefidelity.
3. Aretheevaluationsforvisualcodingtasksrelevanttoreal-worldapplications? Itis
imperative that benchmarks align with real-world uses and applications, particularly in
codingtasks. Employingthecommonlyusedmultiple-choiceformatforevaluatingcode
taskswouldbeinadequateandincongruous.
2Hence, in response to the aforementioned challenges, we present Plot2Code, a comprehensive
and specialized multi-modal code benchmark crafted to evaluate the multi-modal understanding,
reasoning,andcodingcapabilitiesofMLLMs. Thisbenchmarkcomprisesacarefullycurateddataset
comprising132matplotlibplotsacross6plottypes,incorporatingatotalof293subplotssourced
frommatplotlibgalleries. Eachplotispairedwithitscorrespondingcodeandadetaileddescription
generatedbyGPT-4. Tocatertodiverseinputandoutputformats,Plot2Codeincludestwoevaluation
settings,DirectAskingandConditionalAsking,supportingautomaticmetric-basedevaluations
forbothtextandimageoutputs. MLLMscanbeevaluatedusingtext,images,orablendofbothas
inputs,whilethetextandimageoutputscanbeassessedbasedonthecodepassrateandGPT-4V
overallrating,whichconsistentlyalignswithhumanevaluations.
Weevaluate14publiclyaccessibleMLLMsacrossvariousevaluationsettingstodetermineoptimal
performance. OurfindingsunderscorethesignificantchallengesposedbyPlot2Code,withGPT-4V
onlyachievinganoverallscoreof7.68/10,indicatingconsiderableroomforenhancementinvisual
codingtasks.
Thecontributionsofthisstudycanbesummarizedasfollows:
â€¢ Weconstructanovelevaluationbenchmark,Plot2Code,tailoredformulti-modalcodetasks,
enablingtheassessmentofadvancementsinmulti-modalunderstandingandreasoning.
â€¢ DevelopmentofadiversearrayofevaluationsettingsforPlot2Code,accommodatingvaried
modalitiesforinputandoutputthroughimage-codepairsandautomaticevaluationmetrics.
â€¢ Evaluations of various publicly available MLLMs on Plot2Code, revealing that current
MLLMs like GPT-4V, Gemini-Pro, and Claude-3, demonstrate modest performance in
visualcodingtasks.
WeanticipatethatPlot2Codewillstimulatetheresearchcommunitytofurtherexploreandadvance
therealmofMLLMs,propellingustowardstherealizationoftrulyintelligentmulti-modalsystems.
2 RelatedWork
2.1 Multi-modalLargeLanguageModels
WiththeremarkableprogressachievedbyLargeLanguageModels(LLMs)[27,33,28],incorporating
multi-modal input signals into the backbone LLMs has garnered significant interest from both
academiaandindustry[21,9,23,29,9]. Theprimaryfocusofmulti-modalLLMresearchinvolves
developingadditionalencodersthatenablecompatibilityandprocessingofmulti-modalinputsby
thebackboneLLMs. Toenhancepracticalapplications,severalstudies[11,23,9]concentrateon
processingtext-denseimages,suchasdocumentsandcharts,usinghigh-resolutionvisionencoders.
OurgoalistoconductathoroughandcomprehensiveinvestigationofMLLMsandtheirpotential
by assessing their ability to generate code from reference plots, showcasing their visual coding
proficiency.
2.2 Multi-modalCodeBenchmark
BuildinguponthegrowingproficiencyofLLMs,asubsetofspecializedmodels,knownasCode
LLMs[30,19,10],hasemerged. Thesemodelsfocusspecificallyonprogrammingcode,offering
numerousappealingapplicationssuchascodecompletionandinfilling. Codetaskseffectivelyreflect
thein-depthreasoningabilitiesof(M)LLMs. Uni-modalcodebenchmarks,likeHumanEvaland
MBPP[6,2],testthegeneratedcodeusingsingle-roundunittestswiththePass@kmetric. More
recently,LLMagentshavebeenevaluatedinmorecomplexmulti-turninteractivecodesettings[35,
38]. Extendingbeyondtheuni-modalcontext,MMCode[18]incorporatesimageinputintocode
tasks,whileDesign2Code[31]evaluatesMLLMsâ€™generatedHTMLfilesthroughCLIPscoresand
HTMLblocks. Ourworkproposesacomprehensivebenchmark,Plot2Code,whichsupportsawide
rangeofevaluationscenariosandaccommodatesbothuni-modalandmulti-modalinputs.Themetrics
encompass text-based measures such as code pass rate and generated plot similarity, serving as
anall-encompassingevaluationsuiteforassessingMLLMsâ€™in-depthunderstandingandreasoning
capabilities. SeeTable2fordetailedcomparisonswithrelatedbenchmarks.
33 DatasetCollection
Subplots, Axes & Figures Pie & Polar Statistics
Instruction: The first subplot is located at the bottom of the Instruction: The chart has four sections, each labeled as 'Frogs', Instruction: The figure is a scatter plot with error bars. The x-
figure. It displays the function with default margins, meaning 'Hogs', 'Dogs', and 'Logs'. The sizes of these sections are 15, 30, values range from 0.1 to 4 with increments of 0.5. The y-values
the plot fits exactly within the axes. The second subplot is 45, and 10respectively. These sizes represent the proportion of are calculated as the exponential of the negative x-values. The
located at the top left of the figure. It is zoomed out with each section in the pie chart. error in the x-values is 0.2 and the error in the y-values is 0.4.
margins set to 2 on both the x and y axes. The third subplot is
located at the top right of the figure. It is zoomed in towards the
center with the x margin set to 0 and the y margin set to -0.25.
Contours & Fields Lines, Bars & Markers Text, Labels & Annotations
Instruction: The plot is created using a grid of 100 points in the Instruction: The figure size is 9x3. The first subplot is a bar Instruction: first text, "eggs", is positioned at the coordinates
x-direction ranging from -3.0 to 3.0 and 100 points in the y- chart, the second is a scatter plot, and the third is a line plot. All (0.6, 0.7) on the plot. It has a font size of 50 and is rotated 30
direction ranging from -2.0 to 2.0. The z-values for the plot are three plots use the same data, which is a dictionary with four degrees. It is enclosed in a rounded box with an edge color of
calculated using two exponential functions. The z-values are set items: 'apple' with a value of 10, 'orange' with a value of 15, (1., 0.5, 0.5) and a face color of (1., 0.8, 0.8). The second text,
to be negative in the lower left corner of the plot. The color map 'lemon' with a value of 5, and 'lime' with a value of 20. The keys "spam", is positioned at the coordinates (0.55, 0.6) on the plot.
used for the plot is 'PuBu_r'. of the dictionary ('apple', 'orange', 'lemon', 'lime') are used as the It also has a font size of 50 but is rotated -25 degrees. The text
x-axis labels, and the values of the dictionary (10, 15, 5, 20) are is aligned to the right horizontally and to the top vertically.
used as the y-axis values.
Figure2: ExamplesofPlot2Codebenchmark. Weshowdifferent-typeplotswiththeircorrespond-
inginstructions.
Inthissection,wedelineatetheprocessofcuratingandprocessingourbenchmarkdata. Initially,we
crawleverywebsitelinkenumeratedinthematplotlibgallery1. Subsequently,weextractthecode
blockfromeachcorrespondingHTMLfile. Thisprocedureyieldsatotalof841distinctcodeblocks,
whicharesubjectedtofurtherfilteringandprocessingasexplicatedinthesubsequentsections.
3.1 TestSetCuration
Theprimaryobjectiveistoacquirewell-structuredplot-codepairsthateffectivelyevaluatethecode
generationcapabilitiesofMLLM.ItisimportanttonotethattheinitiallycrawledPythoncodemay
notalwaysbesuitableforgeneratinghigh-qualityplotsforevaluationpurposes. Toaddressthis,we
employacombinationofautomaticprocessingandmanualfiltering,asdetailedbelow.
GenerationFiltering Duringouranalysis,weobservethatasingleHTMLfilemayencompass
multiplecodesegments,withsomesegmentsbeingunabletoproduceplotsduetotheirfocuson
importlinesandinitializationfunctions. Toovercomethislimitation,weexclusivelyextractcode
fromHTMLfilescontainingasinglecodeblock. Thisensuresthattheextractedcodeencompasses
allessentialcomponentsandcangenerateaplotwithoutnecessitatingadditionaldependencies. We
thenfilteroutallthecodethatcannotgeneratetheplotsandobtain529plot-codepairs.
TypeFiltering Ouranalysisoperatesundertheassumptionthattheplotsaresimple,staticfigures
devoid of animation and interaction. Consequently, each plot can be regarded as an image file
renderedbythematplotlibengine. Tomaintainthissimplicity,wefilteroutanyplotsassociatedwith
specifictags,suchasanimation,widget,andeventhandling,foundintheircorrespondingURLs. A
detailedbreakdownoftheplot-codepairtypesinourdatasetisprovidedinFigure3.
Manual Curation Subsequent to the aforementioned processing, we conduct a final round of
manualcurationtofilterexamplesbasedonthecriteriaoutlinedbelow:
â€¢ Theplotisdevoidofanyexternalfiledependenciesandcanbedirectlyrenderedusingthe
correspondingcode.
1https://matplotlib.org/stable/gallery/index.html
4Table1: KeyStatisticsofPlot2Code. To-
kensarecountedbyLLaMA-2tokenizer.
Figure3: TypeDistributionofPlot2Code.
Statistic Number
TotalSamples 132 Contours & Fields
-Contours&Fields 30(22.7%)
-Lines,Bars&Markers 37(28.0%)
-Texts,Labels&Annotations 14(10.6%) Pie & Polar 22.8%
-Statistics 17(12.9%)
6.9%
-Subplots,Axes&Figures 25(18.9%)
-Pie&Polar 9(6.8%)
TotalSubplotCount 293 Subplots, Axes 18.8% 27.7% L &i n me as r, kB ea rr ss
& Figures
CodeLength(tokens) 401Â±281
-MinimumLength 60
12.9% 10.9%
-MaximumLength 1823
InstructionLength(tokens) 279Â±115
Statistics Texts, Labels
-MinimumLength 72 & Annotations
-MaximumLength 628
TextCount 23Â±13
â€¢ Theplotsexhibitawidearrayofdiversityintermsofsize,text,colors,andtypes,thereby
servingasacomprehensivebenchmarkforevaluationthatencompassesawidearrayof
commonlyusedchartsandplots.
â€¢ Theplotsareuniformlydistributedacrossvariousdifficultylevels,rangingfrombeginnerto
specializedlevels.
Duringthemanualfilteringprocess,weadoptamorestringentapproachtoretainonlyhigh-quality
plots. Ultimately,weprocure132testexamplesthatserveasourbenchmark.
3.2 EvaluationSetting
Weassessthetestset,curatedinthepreviousstep,undertwodistinctevaluationscenarios: direct
askingandconditionalasking. TofacilitateconvenientextractionofcodefromtheMLLM-generated
responses,werequestthecodetobeenclosedbetweenspecificmarkers,enablingtheuseofregular
expressionsforextraction.
DirectAsking ThissettingmeansgivingtheMLLManimageasinputandrequiringittogenerate
executablecodethatproducesagraphcloselyresemblingtheinputimage. Thespecificpromptcan
befoundinAppendix A.1. Figure7illustratesanexampleinthiscase.
Conditional Asking For MLLMs, this setup means receiving an image and conditions (text
instructions)asinputandgeneratingexecutablecodethatproducesresultsinlinewiththespecified
conditions. ForLLMs,theinputincludesconditionsonly,withotherrequirementsbeingconsistent
withthoseforMLLMs. WeemployGPT-4toextracttheseinstructionsfromthegroundtruthcode,
instructingittoretainallessentialinformationforreproductionwhileavoidingexposureofcode
implementationdetails. ThepromptusedtoconstructtheseinstructionscanbefoundinAppendix
A.2. Figure8illustratesanexampleinthiscase.
3.3 DataStatistics
KeyStatistics Togaugethedifficultylevelsofthetestexamples,wepresentseveralstatisticsin
Table 1. We enumerate the total number of subplots present in our test samples, as a single plot
maycomprisemultiplesubplots. Intotal,thereare293subplots(min=1,max=18). Wetokenizethe
scrapedcodefilesusingtheLLaMA-2tokenizer[33]. Thegroundtruthcodeexhibitsanaverage
token count of 409, with a standard deviation of 291. Additionally, we tokenize the instructions
accompanying our test samples, yielding an average of 242 tokens and a standard deviation of
558. UtilizingPaddleOCR2,wecalculatethetextcountforeachplot,withanaverageof23anda
standarddeviationof13. Collectively,thesemetricsindicatethatourbenchmarkexamplesareindeed
challengingandencompassabroadspectrumofcomplexitiesfoundinscientificplots.
TypeDistribution Togaininsightintothevarietyofplottypesencompassedbyourtestset,we
depictthetypedistributionviaapiechartinFigure3. Foreachsample,wedetermineitstypebased
onthetagpresentinitsURL,sourcedfromthematplotlibgallery. Thesecategoriesaredefinedby
thematplotlibgalleryitself. Themostprevalenttypesincludelines,bars,andmarkers,whileother
categoriescomprisecontours,fields,piecharts,polarplots,subplotsaxes,statisticalrepresentations,
andtextlabelsandannotations.
3.4 EvaluationMetrics
Unlike unimodal code generation tasks, which can be assessed simply by conducting unit tests
andrecordingthecodepassrate,apieceofcodemightexecuteflawlesslywithoutanyerrorsbut
failtorenderanimagesimilartotheprovidedreferenceimage. Thisdiscrepancynecessitatesthe
development of precise and automatic evaluation metrics for multimodal code generation tasks.
Consequently,weproposeasuiteofevaluationmetricsencompassingvariousaspectstoconductthis
evaluation. Ourevaluationmetricsincludethecodepassrate,low-levelmetrics,textmatchrate,and
GPT-4vjudgementscore. Wewillelaborateonthesemetricsindetailinthesubsequentparagraphs.
Code Pass Rate The MLLM is expected to generate code that can be rendered into an image
usingmatplotlib. Consequently,wecomputethecodepassratetodeterminewhethertheMLLMcan
generateexecutablecodegiventheinputreferenceimageandinstruction. Thesubsequentmetrics
willonlybeappliedtoimagesthathavebeencorrectlygenerated.
GPT-4VJudgement WedeviseanevaluationpipelineleveragingtheGPT-4vmodeltoassessthe
high-levelsimilaritybetweengeneratedplotsandgroundtruthplots. Thispipelineassignsaratingto
testsamplesonascaleof1-10,takingintoaccountvariousaspectssuchasoverallappearance,colors,
shapes,positions,andothervisualelementsoftheimages. Thecomprehensivepromptutilizedfor
evaluationcanbefoundintheAppendix A.3.
Text-MatchRatio Whilethehigh-levelsimilarityevaluationprovidedbytheGPT-4vjudgement
isvaluable,itdoesnotaccountfordetailedplotcomponentssuchastext,whichiscrucialforplot
interpretation. Tocompensateforthis,weintroducethetext-matchratio,aimingtoassessthefine-
grainedsimilaritybetweenthegeneratedandgroundtruthplots.Thismetricevaluatestheprecisionof
thetextpresentinthegroundtruthsample,ensuringthatalltextelementsareaccuratelyreproduced
intheplotunderassessmentandtherearenoextratextsinthegeneratedimage.
3.5 ComparisonwithOtherDatasets
AsdepictedinTable2,ourdatasetencompassesthemostextensiverangeofevaluationsettingsand
metricscomparedtoallotheruni-modalandmulti-modalcodebenchmarks.
Table2: Comparisonwithotheruni-modalandmulti-modalcodebenchmarks.â€œI"represents
images,â€œT"representstext,andâ€œI+T"standsforthemulti-modalinformationwithimagesandtext.
InputFormat OutputEvalFormat EvaluationMetric
Dataset TaskType
T I+T I T I+T PassRate ComponentMatch Rating
HumanEval[6] Programming âœ” âœ˜ âœ˜ âœ” âœ˜ âœ” âœ˜ âœ˜
SVGEditBench[26] SVG âœ” âœ˜ âœ” âœ˜ âœ˜ âœ˜ âœ˜ âœ˜
MMcode[18] Algorithm âœ” âœ” âœ˜ âœ” âœ˜ âœ” âœ˜ âœ˜
Design2Code[31] Websites âœ˜ âœ” âœ” âœ˜ âœ˜ âœ˜ âœ” âœ”
Plot2Code Plots âœ” âœ” âœ” âœ” âœ” âœ” âœ” âœ”
2https://github.com/PaddlePaddle/PaddleOCR
6Table3: Quantitativeresultsfor14MLLMsacrosstwosettings,DirectAskingandConditional
Asking. ThemaximumvalueofGPT-4Voverallratingisbolded.
DirectAsking ConditionalAsking
Model BackboneLLM
PassRate Text-Match Rating PassRate Text-Match Rating
LLMs
ChatGPT[27] ChatGPT[27] - - - 80.3 56.7 6.59
GPT-4[28] GPT-4[28] - - - 80.3 68.0 7.36
GPT-4(CoT) [28] GPT-4[28] - - - 78.8 66.0 7.09
GPT-4(PS+) [28] GPT-4[28] - - - 77.3 66.8 7.26
Closed-sourceMLLMs
Claude-3-Opus [1] Claude-3 [1] 84.1 57.5 4.37 78.0 69.7 7.68
Claude-3-Sonnet [1] Claude-3 [1] 75.8 46.7 5.38 65.9 57.0 7.20
Gemini-Pro[9] Gemini [9] 68.2 53.6 5.06 55.3 66.9 7.10
GPT-4V[29] GPT-4[28] 84.1 57.7 6.48 81.8 70.7 7.68
GPT-4V(CoT)[29] GPT-4[28] 89.4 56.3 6.30 81.8 69.7 7.75
GPT-4V(PS+) [29] GPT-4[28] 86.4 55.3 6.25 85.6 71.4 7.83
Open-sourceMLLMs(Lowresolutionsetting)
Mini-Gemini-2B[20] Gemma-2B [32] 39.4 21.4 1.96 22.7 31.8 2.80
Mini-Gemini-8x7B[20] Mixtral-8x7B [14] 75.8 33.9 3.76 62.1 52.3 5.74
Mini-Gemini-34B[20] Yi-34B[40] 67.4 30.5 2.78 50.0 51.2 4.79
Open-sourceMLLMs(Highresolutionsetting)
DeepSeek-VL-7B [23] DeepSeek-7B [3] 72.0 38.7 3.69 56.8 50.1 5.19
LLaVA-1.6-Mistral-7B[21] Mistral-7B [13] 64.4 32.6 3.06 42.4 45.1 4.48
LLaVA-1.6-34B[21] Yi-34B [40] 72.0 34.6 3.18 53.0 50.7 5.60
Mini-Gemini-8x7B-HD[20] Mixtral-8x7B [14] 73.5 40.7 3.87 58.4 53.7 6.08
Mini-Gemini-34B-HD[20] Yi-34B[40] 55.8 34.0 3.06 43.4 46.1 5.35
4 Experiments
In this section, we evaluate a variety of multi-modal large language models and methods on our
Plot2Codebenchmarktocomparetheirperformance. Thisincludesbothclosed-sourcecommercial
APImodelsandstate-of-the-artopen-sourcemodels.
4.1 EvaluationDetails
Evaluated(M)LLMs Toensureacomprehensiveevaluation,weassess14representativeclosed-
sourceandopen-source(M)LLMsthatvaryinparameters,resolutionsettings,andbackboneLLMs,
suchasGPT[27],DeepSeek[3],Mistral[13],Mixtral[14],andYi[40]. Thequantitativeevaluation
isprovidedinSec. Wealsoexploredifferentpromptstrategies,includingChain-of-Thought[36]
andPlan-and-Solve[34]. 4.2. WeinvestigatetheinfluenceofdifferentdesignsofMLLMsonthe
performanceofourbenchmarkinSec. 4.3.
EvaluationMethods AsmentionedinSec.3.2,weemploytwodistinctevaluationsettings: Direct
AskingandConditionalAsking. ForLLMslackingvisioncapabilities,weevaluatethemsolelyinthe
ConditionalAskingsettingwithinstructioninput. Furthermore,weextendtheGPT-4Vjudgement
settingtoconductpairwiseevaluationsbetweentwo(M)LLMsandperformacorrelativeanalysis
betweenGPT-4Vjudgementandhumanevaluation. MoredetailsareprovidedinSec.4.4.
4.2 OverallEvaluation
We showcase the quantitative results of (M)LLMs on our Plot2Code benchmark here. The code
passrate,text-matchratio,andGPT-4Voverallratingforbothdirectaskingandconditionalasking
scenariosarereportedinTable 3.
ThecomprehensivechallengeofPlot2Code. Thebenchmarkposesconsiderablechallenges,as
evenadvancedmodelslikeClaude-3-Opus,Gemini-Pro,andGPT-4Vachieveonly7.68,7.10,and
7.68,respectively,intheoverallassessmentfortheconditionalaskingscenario,indicatingsubstantial
roomforimprovement. Inadditiontotheoverallrating,thepassratealsopresentschallengesfor
MLLMs,particularlywheninstructionsareadded. Forexample,Gemini-Proâ€™spassratedecreases
7Figure4: Pairevaluationresultsinthecon- Figure 5: Ablation experiments involving
ditionalaskingsetting.WeuseGPT-4Vwith- the addition of OCR tokens or not. The
out prompt strategies as the baseline (this basemodelisMini-Gemini-8x7B-HD.OCR
methodisnotshowninthetableasitserves tokensareextractedusingPaddleOCR,which
asthebasisforpairwisecomparison). issupportedbytheofficialimplementationof
Mini-Gemini.
Win Tie Loss
100 11110000
GPT-4 34% 49% 17% w w/ /o O O CC RR ( D(D iri ere ctc t A A sks )k)
80 w/o OCR (Con. Ask) 8888
w/ OCR (Con. Ask)
GPT-4V (CoT) 18% 69% 13%
60 6666
Gemini-Pro 48% 30% 22% 40 4444
Claude-3-Opus 40% 37% 23% 20 2222
0 20 40 60 80 100 0 0000
Percentage (%) Code Pass Rate Text-Match Ratio GPT-4V Judgement
from 68.2% to 55.3% after incorporating the instruction, as the added requirements will make it
hardertogeneratethecorrespondingcode. IncontrasttowidelyusedbenchmarkslikeMT-benchand
HumanEval,whererecentadvancedmodelsattainratingsabove9.00andcodepassratesexceeding
80%,Plot2Codenecessitatesbothvisualunderstandingandreasoningabilitiestoanalyzetheplot,
generateexecutablecode,andcreateaplotresemblingthereferenceplot. Thisheightenedchallenge
for(M)LLMsservesasamorerigorousexaminationoftheirvisualreasoningcomprehensionand
reasoningcapabilities.
The gap between closed-source and open-source models. The performance of open-source
models lags considerably behind that of closed-source models. We evaluate recently advanced
open-sourceMLLMs,includingDeepSeek-VL[23],Mini-Gemini[9],andLLaVA-Next[21]. The
bestperformanceamongtheopen-sourceMLLMsisachievedbyMini-Gemini-8x7B-HD,which
scores6.08intheGPT-4Vjudgementwitha58.4%codepassrate. However,thisperformanceis
stillnotonparwiththatofcommercialclosed-sourceMLLMs. Thereisaneedfortheopen-source
communitytodevelopmorepowerfulmodelsthatcancompetewith,orevensurpass,thecapabilities
ofadvancedproprietarymodels.
4.3 Influenceofdifferencesettings.
Weanalyzetheresultsfromvariousperspectives,encompassingpromptstrategies,backboneLLMs,
andtheresolutionsettings. Thekeyfindingsaresummarizedasfollows.
The influence of LLMs. As depicted in Table 3, there is a strong correlation between model
performanceandthebackboneLLMused,evidentinbothMini-GeminiandLLava. Thissuggests
thatthePlot2CodetaskmayrequirepowerfulbackboneLLMstofacilitatethereasoningprocessand
generateexecutablecode.
Differentevaluationsettings. AsmentionedinSec. 3.2,therearetwodistinctevaluationsettings.
InTable3,itcanbeobservedthatintheconditionalaskingsetting,MLLMsgenerallyachievealower
passrateandhighersimilaritycomparedtothedirectaskingsetting. Weattributethistothefactthat
theaddedinstructionimposesstricterrestrictionsorrequirementsonthegeneratedcode,makingit
morechallengingformodelstogenerateexecutablecode. However,theadditionalinstructioncan
enhancethesimilarityofthegeneratedimagetothereferenceimage. Beyondthesetwoevaluation
settings,wealsoinvestigatetheinfluenceofdifferentpromptstrategies,includingChain-of-Thought
andPlan-and-Solve. WefindthatpromptsencouragingMLLMstoengageindeeperthinkingdo
notshowaclearadvantageoverourdefaultprompt,indicatingthattheexplorationofmulti-modal
reasoningpromptsisstillinprogress.
Differentimageresolutionsettings. InadditiontothebackboneLLMs,weexaminethevision
encodersettings,specificallyfocusingonimageresolutionsettings. ManyMLLMsutilizevision
8
)%(
egatnecreP gnitaRFigure6: Illustrationofinaccuracyof
traditionallow-levelmetrics.
Table4: Comparisonofdifferentindicators
indistinguishingimagegroupswithandwith- ðŸ¤” Traditionallow-level metricscannotprecisely
reflectthequalityofthetestsample.
outsignificantdifferences.
Test Sample A
Indicators t-statistic p-value RejectH0
MSE: 0.074
MSE -1.24 0.22 âœ˜ Ground Truth Plot SSIM: 0.781
SSIM 1.28 0.21 âœ˜ Rating:9
CLIP-Score 4.23 1.24Ã—10âˆ’4 âœ”
Text-MatchRatio 5.69 9.62Ã—10âˆ’7 âœ” Test Sample B
GPT-4VJudgement 9.07 1.22Ã—10âˆ’11 âœ”
MSE: 0.089
SSIM: 0.776
Rating:3
encoderswith higherresolutioncapabilities toprovide moredetailedinformation fromtheinput
images. WeobservethatMLLMswithhigherresolutionconsistentlyimproveperformance,atrend
similar to that seen in ChartQA [24] and DocQA [25]. We also conduct an experiment wherein
weaddOCRtokensextractedbytheofficialPaddleOCRimplementationusedinMini-Gemini,as
showninFigure5. Thisapproachyieldsperformanceimprovementsakintothoseachievedwith
high-resolutionsettings,suggestingcurrentMLLMsmaystillrequiremorepowerfulvisionencoders
tocapturedetailedinformationfromimages.
4.4 PairwiseModelComparison
Inaccordancewiththeconventionalpracticeofpairwisemodelevaluation[42,43],weexpandthe
GPT-4Vjudgementsettingtoperformpairwiseevaluationsbetweentwo(M)LLMs. Thedetailed
prompt employed for pairwise model comparison can be found in the Appendix A.3. For each
referencesample,werequestGPT-4Vtodeterminewhichgeneratedimageismoresimilarwhen
comparing a pair of MLLMs. To mitigate the influence of differing positions, we swap the two
generatedimagesforanadditionalevaluation. Amodelisconsideredvictoriousonlyifitwinsboth
rounds;otherwise,theresultisdeemedatie. WeutilizeGPT-4Vasthebaselineforcomparison. The
resultsareillustratedinFigure4,fromwhichwecaninferthat: (i)ComparedtoGPT-4,theinclusion
ofimageinputforGPT-4Visbeneficialingeneratinghigherqualityplots. (ii)Thecommonlyused
promptstrategy,Chain-of-Thought,doesnotyieldadditionaladvantagesinourbenchmark.
5 StatisticalAnalysis
Inthissection,weperformseveralstatisticalanalysestojustifythedesignofourbenchmark. Since
ourbenchmarkinvolvescomparingthegeneratedimagewiththereferenceimage,wefirstinvestigate
whethertheproposedmetricscaneffectivelyindicateimagesimilarity. Additionally,weconducta
correlativeanalysistodeterminetherelationshipbetweenGPT-4vjudgementandhumanevaluation,
furthersubstantiatingthebasicintuitionbehindourproposedmetricandbenchmark.
Hypothesistestsforimagesimilaritymetrics. Sinceplotimagestypicallyexhibitsimilaritydue
totheircommonwhitebackgroundandoverallstructure,hypothesistestsareconductedtoevaluate
theeffectivenessofdifferentindicatorsindistinguishingbetweenimagegroupswithandwithout
visualdifferences. Wefirst select twoimagegroups: onegroupconsistsofimagesgeneratedby
GPT-4Vwithhighvisualsimilarity,whiletheothergroupcontainsimagesfromMini-Gemini-2B
withconsiderablylowersimilarity. Wethenperformahypothesistestusingseveralmetrics,including
traditionallyusedMeanSquaredLoss(MSE),StructuralSimilarity(SSIM),aswellasthemetricswe
proposed,namelytext-matchratioandGPT-4Vrating,todeterminewhetherthemetriccaneffectively
differentiatebetweenthetwoimagegroupswithvaryinglevelsofvisualsimilarity.
Specifically,weconducttwo-samplet-test(independentt-test).
XÂ¯ âˆ’XÂ¯
t= 1 2 (1)
(cid:113)
s2
1 +
s2
2
n1 n2
9Table5: CorrelationcoefficientcomparisonbetweenGPTevaluationsandhumanevaluations.
Kendallâ€™sTau Pearson Spearman
Coefficient p-value Coefficient p-value Coefficient p-value
0.437 8.68Ã—10âˆ’49 0.479 6.89Ã—10âˆ’54 0.469 1.57Ã—10âˆ’51
whereXÂ¯ andXÂ¯ arethesamplemeansofthetwogroups,s2ands2arethesamplevariances,and
1 2 1 2
n and n are the sample sizes. The null hypothesis (H ) assumed that there was no significant
1 2 0
differenceinthescoresoftheindicatorsbetweenthetwoimagegroups. Thealternativehypothesis
(H ) assumed that there was a significant difference. The p-value, a measure of the strength of
1
evidence in support of a null hypothesis, was calculated for each indicator. A smaller p-value
indicatesstrongerevidenceagainstthenullhypothesis. Inthiscase,asignificancelevelof0.05was
chosen,meaningthatifthep-valuewaslessthan0.05,thenullhypothesiswouldberejectedinfavor
ofthealternativehypothesis,witha95%probabilityofcorrectlyrejectingthenullhypothesis. The
resultsofourhypothesistestcanbefoundinTable 6.
FortheMSEandSSIMindicators,thep-valueswere0.22and0.21,respectively,bothofwhichwere
greaterthanthesignificancelevel. Therefore,wecouldnotrejectthenullhypothesisforthesetwo
indicators,suggestingthattheymightnoteffectivelydistinguishbetweenthetwoimagegroups.
Ontheotherhand,fortheGPT-4V-JudgementandText-MatchRatioindicators,thep-valueswere
1.22Ã—10âˆ’11 and 9.62Ã—10âˆ’7, respectively, both of which were less than the significance level
andCLIP-scoreâ€™sp-value(1.24Ã—10âˆ’4). Therefore,werejectedthenullhypothesisforthesetwo
indicators,suggestingthattheycouldeffectivelydistinguishbetweenthetwoimagegroups.
Itshouldbenotedthathypothesistestingonlyprovidesstatisticalevidencebasedonthecollected
data,anddoesnotprovewhetherthenullhypothesisorthealternativehypothesisisabsolutelytrue.
CorrelativeanalysisbetweenGPT-4Vjudgementandhumanevaluation. Toinvestigatethe
similaritybetweentheGPT-4Vjudgementandhumanevaluation,acorrelativeanalysiswasperformed
usingdifferentcorrelationcoefficients,includingKendallâ€™sTau,Pearsoncorrelationcoefficient,and
Spearmanâ€™srankcorrelationcoefficient. Thesamplesizeforthisanalysiswas920. Detailscanbe
foundintheAppendix C.
AsshowninTable5,Allthreecorrelationcoefficientsindicatedamoderatepositiverelationship
betweentheGPT-4Vjudgementandhumanevaluation. Moreover,thep-valueswereallsmallerthan
thesignificancelevelof0.05,suggestingthatthecorrelationswerestatisticallysignificant.
ThesefindingsimplythattheGPT-4Vjudgementisingeneralagreementwithhumanevaluation,
demonstratingitseffectivenessinassessingthesimilaritybetweengeneratedimagesandrealimages.
6 Conclusion
In this study, we have presented a comprehensive benchmark, Plot2Code, for the evaluation of
multi-modallanguagemodelâ€™scodegenerationability. Thisbenchmarkencompassesawiderange
ofcomplexitiesandtypesofscientificplots,makingitarobusttoolforassessingtheperformance
ofdifferentmodels. Wehaveproposedasuiteofevaluationmetrics,includingcodepassrate,text-
matchratio,andGPT-4vjudgementscore,whichtogetherprovideaholisticevaluationofamodelâ€™s
performance.
OurevaluationofvariousmodelsonthePlot2Codebenchmarkhasrevealedsignificantdifferences
inperformance, highlightingthechallengesposedbythistaskandtheroomforimprovementin
currentmodels. Wehavefoundthatwhilesomemodelscangenerateexecutablecodeandproduce
plotssimilartothereferenceimage,accuratelyreproducingalltextelementsandfine-graineddetails
remainsachallenge.
Infuturework,webelieveourPlot2Codebenchmarkcanstimulatefurtherexplorationofmulti-modal
reasoning,text-denseimageunderstanding,andcomplexcodegenerationcapabilitiesofMLLMs.
There are numerous aspects that are worth exploring, including multi-modal reasoning prompt
strategies,thedesignofvisionencodersthatarecompatiblewithtext-denseimages. Infuturework,
webelieveourPlot2Codebenchmarkcanstimulatefurtherexplorationofmulti-modalreasoning,
10text-denseimageunderstanding,andcomplexcodegenerationcapabilitiesofMLLMs. Thereare
numerousaspectsthatareworthexploring,includingmulti-modalreasoningpromptstrategies,the
design of vision encoders that are compatible with text-dense images. We hope more research
relatedtothisaspectscanfurtherreducethegapbetweentheopen-sourcecommunityMLLMsand
closed-sourcecommercialAPIs.
References
[1] Anthropic. Claude3haiku:ourfastestmodelyet. 2024. Availableat:https://www.anthropic.com/
news/claude-3-haiku.
[2] JacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,DavidDohan,Ellen
Jiang,CarrieCai,MichaelTerry,QuocLe,etal. Programsynthesiswithlargelanguagemodels. arXiv
preprintarXiv:2108.07732,2021.
[3] XiaoBi,DeliChen,GuantingChen,ShanhuangChen,DamaiDai,ChengqiDeng,HonghuiDing,Kai
Dong,QiushiDu,ZheFu,etal. Deepseekllm:Scalingopen-sourcelanguagemodelswithlongtermism.
arXivpreprintarXiv:2401.02954,2024.
[4] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
Advancesinneuralinformationprocessingsystems,33:1877â€“1901,2020.
[5] JunChen,DeyaoZhu,XiaoqianShen,XiangLi,ZechunLiu,PengchuanZhang,RaghuramanKrish-
namoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny. Minigpt-v2:largelanguagemodel
asaunifiedinterfaceforvision-languagemulti-tasklearning. arXivpreprintarXiv:2310.09478,2023.
[6] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,JaredKaplan,
HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman,etal. Evaluatinglargelanguagemodels
trainedoncode. arXivpreprintarXiv:2107.03374,2021.
[7] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,ZhongMuyan,QinglongZhang,
XizhouZhu,LeweiLu,etal. Internvl: Scalingupvisionfoundationmodelsandaligningforgeneric
visual-linguistictasks. arXivpreprintarXiv:2312.14238,2023.
[8] YuyingGe,SijieZhao,JinguoZhu,YixiaoGe,KunYi,LinSong,ChenLi,XiaohanDing,andYingShan.
Seed-x:Multimodalmodelswithunifiedmulti-granularitycomprehensionandgeneration. arXivpreprint
arXiv:2404.14396,2024.
[9] Google Gemini Team. Gemini: a family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805,2023.
[10] DayaGuo,QihaoZhu,DejianYang,ZhendaXie,KaiDong,WentaoZhang,GuantingChen,XiaoBi,
YWu,YKLi,etal. Deepseek-coder:Whenthelargelanguagemodelmeetsprogrammingâ€“theriseofcode
intelligence. arXivpreprintarXiv:2401.14196,2024.
[11] WeiHaoran,KongLingyu,ChenJinyue,ZhaoLiang,GeZheng,YangJinrong,SunJianjian,HanChunrui,
andZhangXiangyu. Vary: Scalingupthevisionvocabularyforlargevision-languagemodels. arXiv
preprintarXiv:2312.06109,2023.
[12] JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,
DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal. Trainingcompute-optimal
largelanguagemodels. arXivpreprintarXiv:2203.15556,2022.
[13] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot,Diego
delasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,LucileSaulnier,etal. Mistral7b.
arXivpreprintarXiv:2310.06825,2023.
[14] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBamford,
DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,etal. Mixtralof
experts. arXivpreprintarXiv:2401.04088,2024.
[15] JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,RewonChild,ScottGray,
AlecRadford,JeffreyWu,andDarioAmodei. Scalinglawsforneurallanguagemodels. arXivpreprint
arXiv:2001.08361,2020.
[16] BohaoLi,YuyingGe,YixiaoGe,GuangzhiWang,RuiWang,RuimaoZhang,andYingShan. Seed-bench-
2:Benchmarkingmultimodallargelanguagemodels. arXivpreprintarXiv:2311.17092,2023.
[17] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench:Benchmarking
multimodalllmswithgenerativecomprehension. arXivpreprintarXiv:2307.16125,2023.
[18] KaixinLi,YuchenTian,QishengHu,ZiyangLuo,andJingMa. Mmcode:Evaluatingmulti-modalcode
largelanguagemodelswithvisuallyrichprogrammingproblems. arXivpreprintarXiv:2404.09486,2024.
11[19] RaymondLi,LoubnaBenAllal,YangtianZi,NiklasMuennighoff,DenisKocetkov,ChenghaoMou,Marc
Marone,ChristopherAkiki,JiaLi,JennyChim,etal. Starcoder: maythesourcebewithyou! arXiv
preprintarXiv:2305.06161,2023.
[20] YanweiLi, YuechenZhang, ChengyaoWang, ZhishengZhong, YixinChen, RuihangChu, Shaoteng
Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models.
arXiv:2403.18814,2023.
[21] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee. Llava-next:
Improvedreasoning,ocr,andworldknowledge,January2024.
[22] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.Advancesinneural
informationprocessingsystems,36,2024.
[23] HaoyuLu,WenLiu,BoZhang,BingxuanWang,KaiDong,BoLiu,JingxiangSun,TongzhengRen,
ZhuoshuLi,HaoYang,YaofengSun,ChengqiDeng,HanweiXu,ZhendaXie,andChongRuan.Deepseek-
vl:Towardsreal-worldvision-languageunderstanding,2024.
[24] AhmedMasry,DoXuanLong,JiaQingTan,ShafiqJoty,andEnamulHoque. Chartqa:Abenchmarkfor
questionansweringaboutchartswithvisualandlogicalreasoning. arXivpreprintarXiv:2203.10244,2022.
[25] MineshMathew,DimosthenisKaratzas,andCVJawahar. Docvqa:Adatasetforvqaondocumentimages.
InProceedingsoftheIEEE/CVFwinterconferenceonapplicationsofcomputervision,pages2200â€“2209,
2021.
[26] KunatoNishinaandYusukeMatsui. Svgeditbench:Abenchmarkdatasetforquantitativeassessmentof
llmâ€™ssvgeditingcapabilities. arXivpreprintarXiv:2404.13710,2024.
[27] OpenAI. Chatgpt. https://chat.openai.com,2023.
[28] OpenAI. Gpt-4technicalreport. ArXiv,abs/2303.08774,2023.
[29] OpenAI. GPT-4V(ision)systemcard,2023.
[30] BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,YossiAdi,
JingyuLiu,TalRemez,JÃ©rÃ©myRapin,etal. Codellama:Openfoundationmodelsforcode. arXivpreprint
arXiv:2308.12950,2023.
[31] ChengleiSi,YanzheZhang,ZhengyuanYang,RuiboLiu,andDiyiYang. Design2code:Howfararewe
fromautomatingfront-endengineering? arXivpreprintarXiv:2403.03163,2024.
[32] GemmaTeam,ThomasMesnard,CassidyHardin,RobertDadashi,SuryaBhupatiraju,ShreyaPathak,
LaurentSifre,MorganeRiviÃ¨re,MihirSanjayKale,JulietteLove,etal. Gemma:Openmodelsbasedon
geminiresearchandtechnology. arXivpreprintarXiv:2403.08295,2024.
[33] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov, SoumyaBatra, PrajjwalBhargava, ShrutiBhosale, etal. Llama2: Openfoundationand
fine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[34] LeiWang,WanyuXu,YihuaiLan,ZhiqiangHu,YunshiLan,RoyKa-WeiLee,andEe-PengLim. Plan-
and-solveprompting:Improvingzero-shotchain-of-thoughtreasoningbylargelanguagemodels. arXiv
preprintarXiv:2305.04091,2023.
[35] XingyaoWang,ZihanWang,JiatengLiu,YangyiChen,LifanYuan,HaoPeng,andHengJi. Mint:Evalu-
atingllmsinmulti-turninteractionwithtoolsandlanguagefeedback. arXivpreprintarXiv:2309.10691,
2023.
[36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesinneural
informationprocessingsystems,35:24824â€“24837,2022.
[37] ChengyueWu,YukangGan,YixiaoGe,ZeyuLu,JiahaoWang,YeFeng,PingLuo,andYingShan. Llama
pro:Progressivellamawithblockexpansion. arXivpreprintarXiv:2401.02415,2024.
[38] JohnYang,AksharaPrabhakar,KarthikNarasimhan,andShunyuYao. Intercode: Standardizingand
benchmarkinginteractivecodingwithexecutionfeedback. AdvancesinNeuralInformationProcessing
Systems,36,2024.
[39] KainingYing,FanqingMeng,JinWang,ZhiqianLi,HanLin,YueYang,HaoZhang,WenboZhang,
YuqiLin,ShuoLiu,etal. Mmt-bench: Acomprehensivemultimodalbenchmarkforevaluatinglarge
vision-languagemodelstowardsmultitaskagi. arXivpreprintarXiv:2404.16006,2024.
[40] AlexYoung,BeiChen,ChaoLi,ChengenHuang,GeZhang,GuanweiZhang,HengLi,JiangchengZhu,
JianqunChen,JingChang,etal. Yi:Openfoundationmodelsby01.ai. arXivpreprintarXiv:2403.04652,
2024.
[41] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,Dongfu
Jiang,WeimingRen,YuxuanSun,etal. Mmmu:Amassivemulti-disciplinemultimodalunderstanding
andreasoningbenchmarkforexpertagi. arXivpreprintarXiv:2311.16502,2023.
12[42] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,ZiLin,
ZhuohanLi, DachengLi, EricXing, etal. Judgingllm-as-a-judgewithmt-benchandchatbotarena.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[43] ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,JiaoSun,YuningMao,XuezheMa,AviaEfrat,
PingYu,LiliYu,etal. Lima: Lessismoreforalignment. AdvancesinNeuralInformationProcessing
Systems,36,2024.
[44] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing
vision-languageunderstandingwithadvancedlargelanguagemodels. arXivpreprintarXiv:2304.10592,
2023.
13A PromptTemplate
Inthissection,weintroducetheprompttemplateusedfortheexperiment.
A.1 PromptforCodeGeneration
Weusethefollowingforthedirectaskingsetting.
You are a helpful assistant that can generate Python code using matplotlib. Generate the
matplotlibcodetocreateaplotthatlookslikethegivenimage,assimilaraspossible. The
generatedcodeshouldbesurroundedby```pythonand```
<image_token><image_token><image_token>...
Fortheconditionalaskingsetting,weaddtheinstructionofthereferenceplotatthefrontofthedirect
askingprompt.
<instruction>
You are a helpful assistant that can generate Python code using matplotlib. Generate the
matplotlibcodetocreateaplotthatlookslikethegivenimage,assimilaraspossible. The
generatedcodeshouldbesurroundedby```pythonand```
<image_token><image_token><image_token>...
A.2 PromptforInstructionGeneration
Hereisthepromptforgeneratingeachplotâ€™scorrespondinginstruction. WerequitetheGPT-4to
examinethecodeforeachplotandsummarizethekeyinformationinitwithoutanyimplementation
details.
Please review the Python code provided below, which uses matplotlib.pyplot to generate
figures. Yourjobistoidentifykeydetails,liketype,texts,etc.,requiredtorecreateafigure
fromthegivencode: <code>
Remember,yourresponseshouldnotincludeanycodeandavoidimplementationdetails. Do
notdescribeanydetailedvariablesorfunctionsinthecode. Instead,useeverydaylanguage
todescribethenecessaryinformation. Ifthecodeusesrandomseed,youshouldextractitfor
reproduction. Revealthedatausedinthefigureforrecreation. Strictlyfollowtherulethatdo
notexposeanyvariablesorfunctionsusedinthecode. Summarizethecrucialinformationas
follows:
A.3 PromptforEvaluation
WeusethefollowingpromptforGPT-4Voverallrating. Wewillprovideboththegroundtruthimage
andthetestimagegeneratedbytheMLLMassistantforGPT-4Vtoratethesimilarity.
14Youareahelpfulassistant. Pleaseevaluatethesimilaritybetweenareferenceimagecreated
using matplotlib and an image generated by code provided by an AI assistant. Consider
factorssuchastheoverallappearance,colors,shapes,positions,andothervisualelements
oftheimages. Beginyourevaluationbyprovidingashortexplanation. Beasobjectiveas
possible. AfterprovidingyourexplanationAfterprovidingyourexplanation,youmustrate
theresponseonascaleof1to10bystrictlyfollowingthisformat: "[[rating]]",forexample:
"Rating: [[5]]",
<gt_image_token><gt_image_token><gt_image_token>...
<test_image_token><test_image_token><test_image_token>...
Inthepairevaluation,weutilizethefollowingprompttodeterminewhichgeneratedimage,either
fromAssistantAorAssistantB,ismoresimilartothegroundtruthimage.
Youareahelpfulassistant. Pleaseactasanimpartialjudgeandevaluatethequalityofthe
generatedimagesprovidedbytwoAIassistantsgiventhegroundtruthimagedisplayedbelow.
Youshouldchoosetheassistantthatgeneratethemoresimilarimage. Yourevaluationshould
considerfactorssuchastheoverallappearance,colors,shapes,positions,andothervisual
elementsoftheimages.
Hereisthegroundtruthimage.
<gt_image_token><gt_image_token><gt_image_token>...
HereistheimagegeneratedbytheassistantA.
<test_image_A_token><test_image_A_token><test_image_A_token>...
HereistheimagegeneratedbytheassistantB.
<test_image_B_token><test_image_B_token><test_image_B_token>...
Beginyourevaluationbycomparingthetworesponsesandprovideashortexplanation.Avoid
anybiasesandensurethattheorderinwhichtheresponseswerepresenteddoesnotinfluence
yourdecision.
A.4 PromptStrategy
Additionally, we experiment with alternative prompt strategies that promote more reasoning by
MLLMassistants,suchasChain-of-Thought (CoT)[36]andPlan-and-Solve(PS) [34].
TheChain-of-Thought(CoT)promptisdemonstratedbelow,whereinaspecificsentenceisaddedat
thecommencementoftheassistantâ€™sresponse.
<USER>
YouareahelpfulassistantthatcangeneratePythoncodeusingmatplotlib...
<Assistant>
Letusthinkstepbystep. ...
15WemodifyPlan-and-Solve(PS)strategytomakeitcompatiblewithourvisualcodingtaskandcallit
asPS+inTable 3. ItfirstencourageMLLMassistantstomakeadetailedplan.
<USER>
YouareahelpfulassistantthatcangeneratePythoncodeusingmatplotlib...
<Assistant>
Letusfirstdescribetheplotandmakeadetailedplanstepbystep...
If the assistant outputs the code during the first step, the strategy will terminate. Otherwise, the
secondstepwillbeemployed,promptingtheassistanttoproducethefinalanswerbasedontheplan
describedinthefirststage.
Previousmessages...
<Assistant>
Basedontheabovedescription,nowwearepreparedtogeneratethecode.Thegeneratedcode
issurroundedby```pythonand```tomakeiteasiertobeextractedbyregularexpressions.
Therefore,thecodeis:
B CaseStudy
In this section, we present several examples using GPT-4V as the model under evaluation. We
showcasecasesfromthedirectaskingsetting(Figure7),theconditionalaskingsetting(Figure8),
andthepair-evaluationsettingcomparedtoGemini-Pro(Figure9),respectively. Allthesamplesare
drawnwiththedefaultpromptstrategy.
C CorrelationAnalysis
In this section, we discuss the details of the correlation analysis. We select 20 pair evaluation
samples with GPT-4v as the baseline in the conditional asking setting (10 compared to Gemini-
Pro,10comparedtoClaude-3-Opus). Subsequently,weusethese20samplestocreateanonline
questionnaireandinvitecolleaguesfromthelab,whoholdatleastabachelorâ€™sdegree,toparticipate.
Each question in the questionnaire presents the ground truth image, the generated image from
AssistantA,andthegeneratedimagefromAssistantB.Participantsareaskedtochooseoneofthe
followingthreeoptions:
â€¢ AssistantAâ€™sgeneratedimageismoresimilartothegroundtruthimage
â€¢ AssistantBâ€™sgeneratedimageismoresimilartothegroundtruthimage
â€¢ thelevelofsimilarityisclose
Intheend,wereceive46completedquestionnaires,resultingin46x20=920samplesforconducting
thecorrelationanalysis.
16<USER>
DirectAskingPrompt(illustrated in Appendix A.1)
<GPT-4V>
import matplotlib.pyplot as plt
import numpy as np
# Data for plotting
x = np.linspace(0, 2 * np.pi, 500)
y1 = np.sin(x)
y2 = np.sin(3 * x)
fig, axs = plt.subplots(1, 3, figsize=(9, 6), sharey=True)
# Plot with fill between y1 and 0
axs[0].fill_between(x, y1, 0, color='blue')
axs[0].set_title('between (x1, 0)')
# Plot with fill between y1 and 1
axs[1].fill_between(x, y1, 1, color='blue')
axs[1].set_title('between (x1, 1)')
# Plot with fill between y1 and y2
axs[2].fill_between(x, y1, y2, color='blue')
axs[2].set_title('between (x1, x2)')
# Set the x label only on the last subplot
for ax in axs:
ax.label_outer()
ax.set_xlim([0, 2*np.pi])
ax.set_xticks([]) # Remove x-axis labels
plt.tight_layout()
plt.show()
<Evaluation>
To evaluate the similarity between the reference image and the generated image, we will
consider several factors:
1. Overall Appearance: The layout of both images includes three separate plots aligned
horizontally. However, the reference image has a square aspect ratio for each plot, while the
generated image has a rectangular aspect ratio, making the plots wider.
2. Colors: Both images use a similar shade of blue for the filled areas within the plots.
3. Shapes: The shapes within the plots are significantly different. The reference image
contains smooth, wave-like shapes that are filled between certain bounds. The generated
image also has wave-like shapes, but they are more sinusoidal and regular, with sharper
peaks and troughs.
4. Positions: The positioning of the plots relative to each other is consistent in both images,
with each plot directly adjacent to the next.
5. Visual Elements: The axes, labels, and titles are present in both images, but the font size
and the exact positioning may vary slightly due to the different aspect ratios of the plots.
Considering these factors, the images share some similarities in layout and color but differ
significantly in the shapes and aspect ratios of the plots. The overall concept seems to be
similar, but the execution and the details of the plots are quite different.
Rating: [[3]]
Figure7: AcaseofDirectAsking,showcasingthegeneratedcode,plot,andevaluationresult.
17<USER>
The figure created by this code consists of three subplots arranged horizontally, all sharing
the same y-axis. The figure size is 6 by 6. The first subplot is a filled area plot between the y-
axis and a curve defined by the sine of 2 times pi times y. The title of this subplot is 'between
(x1, 0)â€™. The second subplot is a filled area plot between a vertical line at x=1 and the same
sine curve as in the first subplot. The title of this subplot is 'between (x1, 1)', and it also has
an x-axis label 'xâ€™. The third subplot is a filled area plot between the sine curve and another
curve defined by 1.2 times the sine of 4 times pi times y. The title of this subplot is 'between
(x1, x2)â€™. The y values range from 0 to 2 with a step of 0.01. The x values for the first curve
are the sine of 2 times pi times y, and for the second curve are 1.2 times the sine of 4 times pi
times y.
DirectAskingPrompt(illustrated in Appendix A.1)
<GPT-4V>
import matplotlib.pyplot as plt
import numpy as np
# Define the y values
y = np.arange(0, 2, 0.01)
# Define the x values for the curves
x1 = np.sin(2 * np.pi * y)
x2 = 1.2 * np.sin(4 * np.pi * y)
# Create the figure and subplots
fig, axs = plt.subplots(1, 3, figsize=(6, 6), sharey=True)
# First subplot
axs[0].fill_betweenx(y, x1, 0, color='blue')
axs[0].set_title('between (x1, 0)')
# Second subplot
axs[1].fill_betweenx(y, x1, 1, color='blue')
axs[1].set_title('between (x1, 1)')
axs[1].set_xlabel('x')
# Third subplot
axs[2].fill_betweenx(y, x1, x2, color='blue')
axs[2].set_title('between (x1, x2)')
# Display the plot
plt.tight_layout()
plt.show()
<Evaluation>
To evaluate the similarity between the reference image and the AI-generated image, we will
compare the following elements:
1. Overall Appearance: We will look at the layout and composition of the images.
2. Colors: We will compare the color schemes used in both images.
3. Shapes: We will assess the shapes present in the images and their accuracy.
4. Positions: We will examine the positioning of the elements within the images.
5. Other Visual Elements: We will consider any additional visual elements that contribute to
the likeness or differences between the images.
Upon comparing the two images:
- The overall appearance of both images is quite similar, with three panels displaying what
appears to be filled curves.
- The colors used in both images are shades of blue, although the reference image has a
lighter shade compared to the AI-generated image's deeper blue.
- The shapes of the filled curves are very similar, with slight variations in the curvature that
could be attributed to the randomness in the data generation or rendering process.
- The positions of the curves within the panels are consistent, with each panel showing a
progression of the shape's complexity or the interaction between two variables.
- Other visual elements such as the axes, labels, and titles appear to be consistent across both
images.
Considering these factors, the images are quite similar, with only minor differences in the
shade of blue and slight variations in the shapes' curvature.
Rating: [[9]]
Figure8: AcaseofConditionalAsking,showcasingthegeneratedcode,plot,andevaluationresult.
18Ground Truth Image
Assistant A Assistant B
GPT-4V Judgment (when A is placed in the first position):
Upon comparing the two responses to the ground truth image, it is clear that the image generated by
Assistant A is more similar to the ground truth image than the image generated by Assistant B.
Assistant A's image closely matches the ground truth in terms of overall appearance, colors, shapes,
and positions. The colors of the box plots (red, blue, green) are identical to the ground truth, and the
layout of the plots, including the titles and axis labels, are accurately reproduced.
Assistant B's image, while maintaining the correct structure of a box plot, does not replicate the
colors of the box plots accurately. All the boxes are blue, which differs from the ground truth image
that has distinct colors for each box plot. Additionally, the titles and axis labels are missing, which
are present in the ground truth.
Therefore, based on the evaluation criteria of overall appearance, colors, shapes, positions, and other
visual elements, Assistant A has generated an image that is more similar to the ground truth image.
GPT-4V Judgment (when B is placed in the first position):
Upon comparing the images generated by Assistant A and Assistant B with the ground truth image, it
is clear that Assistant B has produced an image that is more similar to the ground truth.
Assistant A's image differs significantly in color, using a uniform blue for all box plots, whereas the
ground truth image uses distinct colors for each box plot (red, blue, green). Additionally, Assistant
A's image does not replicate the notched feature in the right-hand side box plots, which is a key
characteristic of the ground truth image.
Assistant B's image, on the other hand, closely matches the ground truth in terms of color, with each
box plot accurately reflecting the colors in the ground truth (red, blue, green). Furthermore, Assistant
B has successfully replicated the notched feature in the box plots on the right-hand side, just as in the
ground truth image.
Therefore, based on the evaluation criteria of overall appearance, colors, shapes, and positions,
Assistant B's image is more similar to the ground truth image.
Figure9: Acaseofpairevaluation. Weinterchangetheorderofresponsesfromthetwoassistants
andconducttheevaluationtwice. Bothresultsarepresentedhere.
19