Investigating the Semantic Robustness of CLIP-based Zero-Shot Anomaly
Segmentation
KevinStangl* MariusArvinte WeilinXu CoryCornelius
TTIC,USA IntelLabs,USA
kevin@ttic.edu firstname.lastname@intel.com
Abstract
Zero-shotanomalysegmentationusingpre-trainedfoun-
dation models is a promising approach that enables effec-
tivealgorithmswithoutexpensive,domain-specifictraining
or fine-tuning. Ensuring that these methods work across
various environmental conditions and are robust to distri-
bution shifts is an open problem. We investigate the per-
formance of WinCLIP [14] zero-shot anomaly segmenta-
tion algorithm by perturbing test data using three seman-
tic transformations: bounded angular rotations, bounded
saturation shifts, and hue shifts. We empirically measure
a lower performance bound by aggregating across per-
sampleworst-caseperturbationsandfindthataverageper-
formancedropsbyupto20%inareaundertheROCcurve
and 40% in area under the per-region overlap curve. We Figure1. Effectsofthethreeaugmentationsappliedtothesame
findthatperformanceisconsistentlyloweredonthreeCLIP anomalous(alargecrackintheshellofahazelnut)MVTecsam-
ple.Thethirdcolumnrepresentstheoriginalsample.
backbones,regardlessofmodelarchitectureorlearningob-
jective,demonstratinganeedforcarefulperformanceeval-
uation.
Recentwork[14,31]leveragesthezero-shotgeneraliza-
tion ability of foundation models [21] to address anomaly
segmentation across a varied number of objects and envi-
1.Introduction
ronments. Amodernlineofworkincomputervisiondeals
with test-time robustness of discriminative models against
Visual anomaly segmentation is a challenging and impor-
naturaldistributionshifts[7,10],whereitisfoundthat,gen-
tant task, especially in manufacturing line applications,
erally, even large-scale models are fragile to well-chosen,
whereobjectshavetobeinspectedinafast,automatedway,
per-sample perturbations. This has been recently demon-
thatisrobustunderavariednumberofenvironmentalcon-
strated for CLIP-based models both for the case of noise-
ditions [3, 32]. Anomalies manifest as fabrication defects
like perturbations [23] and for natural perturbations (such
(scratches, chips) and localizing them using fine-grained
ascameraposechanges)[26].
segmentation allows for interpretability of different types
ofdefects[3]. Giventhevarietyofitemsthatareinspected, In this work, we investigate the robustness of down-
collecting large, annotated training sets for each object is streamanomalysegmentationusingCLIPwhenfacedwith
prohibitive. Even if data would be available, training and three perturbations (rotations, hue shift, saturation shift)
deployingseparatemodelsforeachobjectiscostly. that simulate naturally occurring distribution shifts. Im-
portantly, we bound these shifts in a controlled way such
*WorkdonewhileatIntelLabs,USA. that anomalies remain clearly present when inspected by
ThispublicationincludesworkthatwasfundedbytheGovernmentun- humans – the question we investigate is if CLIP-based
dertheDefenseAdvancedResearchProjectsAgency(DARPA)Guaran- anomalysegmentationisinvarianttotheseshifts?
teeing AI Robustness against Deception (GARD) Program, Agreement
#HR00112030001. Our results demonstrate that average segmentation per-
4202
yaM
31
]VC.sc[
1v96970.5042:viXra
noitatoR
tfihs
euH
tfihs
noitarutaSformanceisreducedacrossmostobjectsacrossCLIPback- The modified sample x is input to zero-shot anomaly
aug
bones, with drops of up to 40% in the per-sample worst- detection using WinCLIP [14], yielding the estimated soft
case setting. This performance drop points to the need of (continuous)anomalymapsy˜ as:
aug
conducting lower bound performance evaluations of foun-
dationmodelsusedinzero-shotanomalysegmentation,and y˜ aug =w(x aug). (2)
isinlinewitharecentlineofworkthatinvestigatesthere-
Finally,theoutputsensitivitymapsareadditionallyrotated
silience of foundation models to natural distribution shifts
counter-clockwise using the angle θ (hue and saturation
andadversarialexamples[15,24].
shiftsdonotspatiallychangethesample)toyield:
1.1.RelatedWork
y˜=f(y˜ ;−θ,0,0), (3)
aug
Alargenumberofrecentworkshaveintroducedzero-shot
anomalyclassificationandsegmentationalgorithmsthatuse wherethechoiceofδ h = δ s = 0impliesthereisnoHSV
foundation models [5, 6, 9, 12, 30]. Among these, we fo- spaceconversionperformed.Thisallowsforfairend-to-end
cus on WinCLIP [14] because of its good performance, comparison between y˜ and the ground-truth segmentation
fast inference speed, and open-source implementation in mapsy,withoutrequiringanyinterventionony.
the anomalib library [1]. WinCLIP for anomaly seg- Theentirepre-processingpipelineisdifferentiable,thus
mentation has two components: a template-based prompt- the parameters θ,δ h,δ s can be optimized using gradient-
ing strategy, and a multi-scale pooling strategy to aggre- based methods. However, doing so on a per-sample basis
gateinformationacrossmultiplespatialscales.Otherrecent requires defining a differentiable loss function that mea-
methodshaveusedthepropertiesofdiffusionprocessesfor sures anomaly segmentation loss. We introduce the fol-
anomaly detection [19], pre-trained diffusion models [30], lowing simplified version of the Dice loss [25] (using the
andvision-languagemodels[9]. ℓ 1-norm)frommedicalimagesegmentation:
Recentwork[24,27]investigatestherobustnessofCLIP
(cid:80) (cid:80)
y˜(1−y ) y˜y
andhowpreviouslyreportedresultslikelyover-statetheob- l(y˜,y)= i i i − i i i , (4)
(cid:80) (cid:80)
(1−y )+ϵ y +ϵ
servedeffectiverobustnessduetoevaluatingonastatictest i i i i
set from one distribution. The work in Idrissi et al. [13]
where ϵ = 1e-8 is a small constant added for numerical
characterizessubsetsoftheImageNetdatasetandidentifies
stability.
colorshiftsasacommonsubsetandpotentialaugmentation.
Notethatthisisavalidsegmentationloss: thefirstterm
Our proposed work is similar in spirit to both previous
encourageslowvaluesfory˜,whereisatisfiesy = 0(i.e.,
i i
idea, given that we propose to evaluate anomaly segmen-
truenegativepixels).Similarly,thesecondtermencourages
tation on worst-case augmented samples from the original
higher values of y˜ for the true positive pixels. Unlike the
i
testset,andouraugmentationsincludecolorshifts[4,11].
Diceloss,ourproposedlossdoesnotcontaintheestimated
Another line of work [17, 26, 28] questions whether CLIP
outputy˜inthedenominator,whichimprovesnumericalsta-
suffers from inaccurate physical grounding, showing that
bilitywhenback-propagatinggradientstotheinputs.
semanticpropertiesofimagesarenotunderstoodbyvision-
Unlikesupervisedlearning(wherethegoalislossmini-
languagemodelssuchasLLaVA[18]orGPT-4V[20].
mization),thegoalofourworkistomaximizethislossby
optimizingthethreeparametersθ,δ ,δ as:
h s
2.Methods
θ∗,δ∗,δ∗ = argmax l(y˜(θ,δ ,δ ),y),
h s h s
To investigate the performance of anomaly segmentation θ,δh,δs (5)
underworst-casedistributionshifts, weusethreesemantic s.t.θ ∈[−90◦,90◦],δ ∈[−0.5,0.5].
s
preserving augmentations: rotation, hue shift, and satura-
tion shift. Their effects on the same sample are shown in Toobtainthetightestlowerperformancebounds,thisopti-
Figure 1, where it can be seen that the anomaly remains mizationisdoneseparatelyforeachsampleinthetestset.
detectableacrosstheentireaugmentationrange. We used the Adam optimizer [16] with at most 200 opti-
Thequerysamplexisclockwiserotatedusinganangle mization steps per sample to approximate the solution to
θ,followedbyanRGBtoHSVconversion. Additiveshifts (5). Detailsareprovidedinthesupplementarymaterial.
δ andδ areindependentlyappliedtothehueandsatura- Theaboveisanapproximationoftheworst-casesetting
h s
tion channels [11], respectively, after which the sample is encounteredinpracticeacrossanextendedperiodoftime:
convertedbacktoitsRGBrepresentation,yieldingtheaug- everyindividualtestsampleismanipulatedinitsownworst
mentedsamplex . Thepre-processingissummarizedas: way(byaccidentorill-intended)withrespecttothedown-
aug
streamtask,whilestillensuringthatthemanipulationisre-
x =f(x;θ,δ ,δ ). (1) alisticandpreservesthesemanticinformationofthesample
aug h s1.0 looselowerbound, butwefindthateveninthiscase, con-
Hazelnut Screw Bottle clusionsarenon-trivial.
Capsule Pill Metal nut
Figure2showszero-shotsegmentationpAUROCforsix
0.9
objectsintheMVTecdatasetwhenthesamerotationangle
θ is applied to all test samples using the protocol in Sec-
0.8 tion 2. Only objects that are placed on matte backgrounds
werechosenforevaluation,toensurethatrotationdoesnot
0.7 produce artificial anomalies in the corners of x aug. Fig-
ure2revealsthatrotatingsampleshasanon-trivialandnon-
smootheffectondownstreamtaskperformance:smallrota-
0.6
tionsatarbitraryanglescancauseaperformancedifference
ofupto3%inpAUROCtoappearinmostobjects. Perfor-
0.5 mancecanvarybyupto10%betweenthelowerandupper
boundsofacertainobject(e.g.,forcapsule).Similarresults
90° 45° 0° 45° 90°
fortheVisAdatasetareinthesupplementarymaterial.
Rotation angle [degrees]
Figure 3 shows zero-shot segmentation pAUROC the
Figure2. Zero-shotanomalysegmentationperformancewhenthe same six objects in Figure 2 when the same hue shift δ
h
samerotationangleθisappliedtotheMVTectestsetforrotation- is applied to the test samples. In this case, variations are
invariantobjects. smoother, but effects are still non-trivial: certain objects
arelesssensitive(e.g.,allscrewimagesaregray-scale,and
1.0 completelyunaffectedbyhue),whileothersshowasignif-
Hazelnut Screw Bottle
icantperformancevariation(e.g., pillandmetalnut)ofup
Capsule Pill Metal nut
to15%. Giventhathueshiftsshouldnotaffectphysicalde-
0.9
fects in objects, this indicates that the backbone used here
(ViT-B/16+) is not sufficiently robust to these shifts, even
0.8 whentheshiftsarenotsample-specific.
3.2.Per-samplelowerperformancebounds
0.7
We report per-sample lower performance bounds where
theworst-casetransformationisindependentlyobtainedfor
0.6
each test sample, followed by aggregating all augmented
samplesandreportingperformance.
0.5 AsexpectedfromFigure2,thedistributionofworst-case
/2 0 /2 anglesacrossindividualsamplesisapproximatelyuniform:
Hue shift h [radians] given that only rotation-invariant objects were evaluated,
there is no reason for specific angles to be systematically
Figure3. Zero-shotanomalysegmentationperformancewhenthe
worst-caseforallsamples. Thedistributionisshowninthe
sameadditive(modulo2π)hueshiftδ isappliedtotheMVTec
h
testsetforrotation-invariantobjects. supplementarymaterial.
Figure 4 shows the distribution of performance drops
across objects for three CLIP backbones tested on both
(e.g.,usingaboundedsaturationshift). Samplesareaggre- the augmented MVTec and VisA datasets. Four per-
gated and final performance is measured using three met- sample worst-case augmentations were used: three one-
rics, following the protocol described in Jeong et al. [14]: dimensionalones–inangleθ,hueshiftδ ,saturationshift
h
pixel-levelareaundertheROCcurve(pAUROC),areaun- δ –andtheworst-caselowerperformanceboundobtained
s
dertheper-regionoverlapcurve(AUPRO),andtheF 1-max by jointly optimizing all three variables for each sample
score. and aggregating results. In general, we notice that the
lower bound is looser (performance suffers a larger drop)
3.ExperimentalResultsandDiscussion for the VisA dataset compared to MVTec in the case of
non-adversariallyfine-tunedbackbones. Whenconsidering
3.1.Uniformlowerperformancebounds
thatVisAisamoredifficulttestsetthatMVTec(basedon
Wefirstevaluatetheperformanceofzero-shotanomalyseg- theloweroriginalperformanceinTable1),theevenhigher
mentationwhenonlyoneofthethreeaugmentationsisap- worst-caseperformancedropindicatesthatasignificantro-
plied using the same value to the entire test set. This is a bustnessgapexistsinhardersegmentationproblems.
CORUAp
CORUApMVTec VisA
0.0
0.1
0.2
0.3
0.4 Angle-only
Hue-only
Saturation-only
0.5 Three-dimensional
ViT-B/16+ ViT-L/14 ViT-L/14-FARE² ViT-B/16+ ViT-L/14 ViT-L/14-FARE²
Figure4. Zero-shotanomalysegmentationperformancefortwodatasets(MVTecandVisA,leftandright,respectively)usingthreeCLIP
backbones(ViT-B/16+, ViT-L/14, andadversariallyfine-tunedViT-L/14-FARE2 forWinCLIP.Thethreetest-time, worst-casesemantic
perturbations(angle,saturation,hue)areconsideredeitherseparately,orsimultaneously(3D).Thebarsshowthedifferencebetweenthe
originaltestsetsandtheconsideredlowerbounds.
AnomalySegmentation MVTec-AD VisA
Eval. data Backbone pAUROC AUPRO F -max Loss pAUROC AUPRO F -max Loss
1 1
ViT-B/16+ 80.1 64.2 24.1 0.075 76.4 55.4 10.9 0.165
Original ViT-L/14 59.1 21.9 9.9 0.108 59.3 26.3 2.89 0.199
ViT-L/14-FARE2 37.7 10.6 8.3 0.108 30.6 4.81 2.10 0.203
Per-sample ViT-B/16+ 59.8 23.6 12.8 0.085 40.3 16.0 2.28 0.180
3Dlower ViT-L/14 26.4 4.25 7.68 0.115 19.9 3.59 1.37 0.210
bound ViT-L/14-FARE2 24.2 3.54 7.80 0.111 16.5 2.12 1.37 0.208
Table1.Anomalysegmentationperformanceundertheoriginaltestdataandtheper-sampleworst-caselowerbound.
In terms of specific one-dimensional perturbations, hue beseenthatthevaluesareinverselycorrelatedwiththeper-
shifts affect VisA the most on average (across all back- formancemetrics,andthat(4)isavalidsegmentationloss.
bones), while the results for MVTec are less conclusive,
and more backbone-dependent. The adversarially fine- 4.Conclusion
tunedFARE2 backbone[23]consistentlyshowslowerper-
Our work demonstrates the generalization challenges of
formance drops, but the nominal performance also suffers
zero-shot anomaly segmentation algorithms when faced
a significant drop, indicating a sharp trade-off between ro-
with relatively simple test-time augmentations (rotation,
bustnesstodistributionshiftsandabsoluteperformance.
hue, saturation). By formulating an optimization problem
Table1showsresultsforanomalysegmentationmetrics, that determines the per-sample worst-case augmentations,
evaluated on both datasets, using three models. We com- we show performance drops for CLIP-based anomaly seg-
paretheoriginal(un-augmented)testsetperformancewith mentationofupto40%inpAUROCcomparedtotheorigi-
that of the worst-case per-sample lower bound. Given the naltestset.
imbalancednatureofanomalysegmentation(fewerpositive Standard practice for CLIP pre-trained models is to not
pixelsthannegatives), theAUPROscoressuffersaperfor- useheavyaugmentationpipelines[21]–somesampleshave
manceofupto40%usingtheViT-B/16+architecture.Sim- textlabelinformationabouttheircolor,whichwouldmake
ilarly, we reproduce the already-low F1-max score on the naivecoloraugmentationsnotcompatiblewiththelearning
original test sets, and this value drop to as low as 2% for objective. Future research could consider how to general-
theVisAdatasetusingtheViT-B/16+architecture. Table1 izesemanticaugmentationstomulti-modaldataandinclude
shows the numerical values of the optimization objective this in the training of foundation models. Finally, all our
definedin(4)usedtofindworst-caseaugmentations: itcan work targeted zero-shot anomaly segmentation. Few-shot
CORUAp
)lanigiro
detnemgua(anomaly segmentation is common in practice and signifi- Imagenet-x: Understanding model mistakes with factor of
cantimprovementscouldbeachievediftest-timeaugmen- variation annotations. arXiv preprint arXiv:2211.01866,
tations were optimized to maximize performance for each 2022. 2
objecttypeinsteadofminimizingit,aswedidinthiswork. [14] JongheonJeong,YangZou,TaewanKim,DongqingZhang,
AvinashRavichandran, andOnkarDabeer. Winclip: Zero-
References /few-shotanomalyclassificationandsegmentation,2023. 1,
2,3
[1] SametAkcay, DickAmeln, AshwinVaidya, BarathLaksh- [15] Nikhil Kandpal, Matthew Jagielski, Florian Trame`r, and
manan, Nilesh Ahuja, and Utku Genc. Anomalib: A deep Nicholas Carlini. Backdoor attacks for in-context learning
learning library for anomaly detection. In 2022 IEEE In- withlanguagemodels,2023. 2
ternational Conference on Image Processing (ICIP), pages [16] Diederik P Kingma and Jimmy Ba. Adam: A method for
1706–1710.IEEE,2022. 2 stochastic optimization. arXiv preprint arXiv:1412.6980,
[2] Yoshua Bengio, Nicholas Le´onard, and Aaron Courville. 2014. 2,1
Estimating or propagating gradients through stochastic [17] Martha Lewis, Nihal V. Nayak, Peilin Yu, Qinan Yu, Jack
neurons for conditional computation. arXiv preprint Merullo,StephenH.Bach,andElliePavlick. Doesclipbind
arXiv:1308.3432,2013. 1 concepts? probingcompositionalityinlargeimagemodels,
[3] Paul Bergmann, Michael Fauser, David Sattlegger, and 2023. 2
Carsten Steger. Mvtec ad–a comprehensive real-world [18] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
datasetforunsupervisedanomalydetection. InProceedings Visual instruction tuning. In Thirty-seventh Conference on
oftheIEEE/CVFconferenceoncomputervisionandpattern NeuralInformationProcessingSystems,2023. 2
recognition,pages9592–9600,2019. 1 [19] VictorLivernoche,VineetJain,YasharHezaveh,andSiamak
[4] AnandBhattad,MinJinChong,KaizhaoLiang,BoLi,and Ravanbakhsh.Ondiffusionmodelingforanomalydetection.
D.A.Forsyth.Unrestrictedadversarialexamplesviaseman- arXivpreprintarXiv:2305.18593,2023. 2
ticmanipulation,2020. 2 [20] OpenAI. Gpt-4technicalreport,2024. 2
[5] YunkangCao,XiaohaoXu,ChenSun,YuqiCheng,Zongwei [21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Du,LiangGao,andWeimingShen. Segmentanyanomaly Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
without training via hybrid prompt regularization. arXiv Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
preprintarXiv:2305.10724,2023. 2 Krueger, and Ilya Sutskever. Learning transferable visual
[6] YunkangCao,XiaohaoXu,ChenSun,XiaonanHuang,and modelsfromnaturallanguagesupervision,2021. 1,4
Weiming Shen. Towards generic anomaly detection and [22] E. Riba, D. Mishkin, D. Ponsa, E. Rublee, and G. Brad-
understanding: Large-scalevisual-linguisticmodel(gpt-4v) ski. Kornia: anopensourcedifferentiablecomputervision
takesthelead. arXivpreprintarXiv:2311.02782,2023. 2 libraryforpytorch. InWinterConferenceonApplicationsof
[7] XinquanChen,XitongGao,JuanjuanZhao,KejiangYe,and ComputerVision,2020. 1
Cheng-Zhong Xu. Advdiffuser: Natural adversarial exam- [23] Christian Schlarmann, Naman Deep Singh, Francesco
plesynthesiswithdiffusionmodels. InProceedingsofthe Croce, and Matthias Hein. Robust clip: Unsupervised ad-
IEEE/CVF International Conference on Computer Vision, versarial fine-tuning of vision embeddings for robust large
pages4562–4572,2023. 1 vision-languagemodels,2024. 1,4
[8] ThomasEboli. High-qualitypytorchimageandvolumero- [24] ZhouxingShi, NicholasCarlini, AnanthBalashankar, Lud-
tation. AccessedonApril24,2024. 1 wig Schmidt, Cho-Jui Hsieh, Alex Beutel, and Yao Qin.
[9] Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Effective robustness against natural distribution shifts for
MingTang,andJinqiaoWang. Anomalygpt: Detectingin- models with different training data. arXiv preprint
dustrial anomalies using large vision-language models. In arXiv:2302.01381,2023. 2
Proceedings of the AAAI Conference on Artificial Intelli- [25] Carole H. Sudre, Wenqi Li, Tom Vercauteren, Sebastien
gence,pages1932–1940,2024. 2 Ourselin,andM.JorgeCardoso. GeneralisedDiceOverlap
[10] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein- as a Deep Learning Loss Function for Highly Unbalanced
hardt, and Dawn Song. Natural adversarial examples. In Segmentations, page 240–248. Springer International Pub-
Proceedings of the IEEE/CVF conference on computer vi- lishing,2017. 2
sionandpatternrecognition,pages15262–15271,2021. 1 [26] ShengbangTong,ZhuangLiu,YuexiangZhai,YiMa,Yann
[11] HosseinHosseiniandRadhaPoovendran. Semanticadver- LeCun, and Saining Xie. Eyes wide shut? exploring the
sarialexamples. InProceedingsoftheIEEEConferenceon visual shortcomings of multimodal llms. arXiv preprint
ComputerVisionandPatternRecognitionWorkshops,pages arXiv:2401.06209,2024. 1,2
1614–1619,2018. 2 [27] WeijieTu, WeijianDeng, andTomGedeon. Acloserlook
[12] Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, attherobustnessofcontrastivelanguage-imagepre-training
MichaelSpratling,andYan-FengWang. Registrationbased (clip). AdvancesinNeuralInformationProcessingSystems,
few-shotanomalydetection,2022. 2 36,2024. 2
[13] BadrYoubiIdrissi,DianeBouchacourt,RandallBalestriero, [28] Vishaal Udandarao, Max F. Burg, Samuel Albanie, and
Ivan Evtimov, Caner Hazirbas, Nicolas Ballas, Pascal Vin- Matthias Bethge. Visual data-type understanding does not
cent,MichalDrozdzal,DavidLopez-Paz,andMarkIbrahim. emergefromscalingvision-languagemodels,2023. 2[29] MichaelUnser,PhilippeThevenaz,andLeonidYaroslavsky.
Convolution-based interpolation for fast, high-quality rota-
tionofimages. IEEETransactionsonimageprocessing, 4
(10):1371–1381,1995. 1
[30] Julian Wyatt, Adam Leach, Sebastian M Schmon, and
ChrisGWillcocks. Anoddpm: Anomalydetectionwithde-
noising diffusion probabilistic models using simplex noise.
In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition,pages650–656,2022. 2
[31] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and
JimingChen. Anomalyclip: Object-agnosticpromptlearn-
ing for zero-shot anomaly detection. arXiv preprint
arXiv:2310.18961,2023. 1
[32] YangZou,JongheonJeong,LathaPemula,DongqingZhang,
andOnkarDabeer. Spot-the-differenceself-supervisedpre-
trainingforanomalydetectionandsegmentation,2022. 1Investigating the Semantic Robustness of CLIP-based Zero-Shot Anomaly
Segmentation
Supplementary Material
A.ImplementationDetails alli(noanomalouspixels),wemonitorthelossfunctionL
in (4) and select the specific perturbations (θ∗,δ∗,δ∗) that
A.1.Augmentations h s
maximizethisvalue.
Toimplementtherotationaugmentation,weusedthethree-
A.3.Selectingrotation-invariantobjects
pass convolutional approach from Unser et al. [29]. Com-
pared to bilinear interpolation, this method preserves the To select rotation-invariant objects, we used the per-pixel
high-frequencycontentofthesamplesandensuresthatmin- average mean squared error between the extrapolated cor-
imal distortions (at least when evaluated by humans) are nerpixelsusingourrotationmethodandtheoriginalcorner
introduced via the rotation. To extrapolate pixels in the pixelsas:
corners of the rotated sample, we used the replicate
paddingmode: foreachextrapolatedpixel,thevalueofthe
e(x)=
(cid:80) im i·(x i−f(x i;θ =45◦))2
, (6)
(cid:80)
nearest pixel from the rotated sample itself is copied. We m
i i
used the torch-rotation library [8] to implement the
wherem isabinarymaskthatindicateswhetheraspecific
differentiablethree-passrotation. i
pixel was extrapolated or not and is obtained by rotating
Hue and saturation shifts were implemented using the
a monochrome image with maximum pixel intensity. We
RGB-to-HSV conversion function from the kornia li-
evaluatethiserrorusingthespecificangleθ =45◦,withthe
brary [22], with the shifts themselves being otherwise
reasoningthatitiswherethemostextrapolationoccurs,and
straightforwardtoimplement.
rotation-invariantobjectsshouldnotbecut-offbythisrota-
Giventhatsaturationisclippedto[0,1]aftertheshiftis
tion. If a cut-off would occur, the object texture would be
applied, we used a straight-through estimator (STE) [2] to
reflected in the extrapolated corner, thus producing a high
back-propagatethroughthisoperationevenoutsideitsvalid
errorcomparedtotheoriginalcornerregion.
range. Similarly, we used the STE estimator to clip and
The above is followed by a manual thresholding of the
back-propagatethevaluesoftherotationangleθ andsatu-
error,aswellasfinalcarefulvisualinspectionofthedown-
rationshiftδ whenevertheyexceedtheirpermittedranges.
s
selected classes. Tables 2 and 3 show the selection results
A.2.Solvingtheoptimizationproblem forobjectsintheMVTecandVisAdatasets,respectively.
We used the Adam optimizer [16] with learning rates of 5 A.4.Optimizationoutcomes
forθand0.1forδ andδ ,respectively. Thelearningrates
h s Figure5showsthehistogramofper-sampleworst-casero-
werechosenbasedontheℓ -normofthegradientwithre-
∞ tation angles θ∗ when optimizing only for rotation. The
specttoeachvariableatinitialization, andwerenotexten-
higher mode around the origin is due to the intentional ar-
sivelyfine-tunedorsearchedfor.
tifacts introduced when de-rotating the estimated anomaly
We found that using random restarts and for the opti-
mapy˜ : fordifficultsamples,thismaycauseaslightper-
aug
mizationproblemiscrucialforsuccessfullydecreasingthe
formanceboostatanyrotationangle. Thiscausestheopti-
per-samplelowerperformancebound.Inparticular,theloss
mizationin (5)toreturnanear-zeroangleastheworst-case
surfaceishighlynon-convexfortheangleθ,andoptimiza-
one.
tionoftengetstrappedinlocalminima. Giventherelatively
low budget of 200 steps, we used 5 restarts: the first opti-
B.Additionaluniformlowerboundresults
mizationrunstartswiththedefaultvaluesθ =δ =δ =0,
h s
while the subsequent four runs restart these values using Figure6showstheresultsforapplyingthesamesaturation
uniformsamplingfromthevalidintervalofeachvariable. shift δ to all samples in the MVTec test set. Similar to
s
Additionally, we also monitor the per-sample segmen- the result for the hue shift δ in Figure 3, there are signif-
h
tation pAUROC for samples that have at least one anoma- icantperformancevariationsformostobjectsexceptmetal
louslabeledpixeliny andselectthespecificperturbations nut,whichalreadycontainsgray-scalesamplesthatareless
(θ∗,δ∗,δ∗) that minimize this value. Note that this is still sensitivetocolorshifts.
h s
sub-optimalwhenitcomestominimizingthepAUROCon Figures 7, 8, and 9 show the uniform lower bounds ob-
pixels from all samples of a specific object, which is gen- tainedfromapplyingthesamerotation,hueshift,orsatura-
erallyintractable. Finally,forsamplesthathavey = 0for tionshifttoalltestsamplesintheVisAdataset,respectively.
iObjectname CornerMSE Used
30
Metalnut 0.000255 True
Pill 0.000304 True
25
Hazelnut 0.000391 True
Bottle 0.002179 True
20
Screw 0.005964 True
Leather 0.013949 False
15
Wood 0.021015 False
Capsule 0.02621 True
10
Cable 0.027923 False
Transistor 0.077574 False
5
Tile 0.09312 False
Carpet 0.104486 False
0
Grid 0.126188 False 90° 45° 0° 45° 90°
Toothbrush 0.17937 False Per-sample worst-case rotation angle * [degrees]
Zipper 0.92926 False
Figure5.Empiricaldistributionofper-sampleworst-caserotation
anglesforall rotationallyinvariantobjectin MVTec. Theslight
Table 2. Average corner extrapolation MSE for objects in the
peak around the origin is caused by the optimization being sub-
MVTec dataset. Leather and wood were excluded from evalua-
optimalduetothede-rotationofy˜ usingzerovalueextrapola-
tionaftermanualinspection,giventhatthetexturesspantheentire aug
tioninthecorners.
sampleandsimpleextrapolationwouldleadtoqualitativeartifacts
inthesamplesthatcouldbemistakenforanomalies.
1.0
Hazelnut Screw Bottle
Objectname CornerMSE Used
Capsule Pill Metal nut
Pipefryum 0.000668 True 0.9
Pcb3 0.001449 True
Macaroni1 0.004837 True
0.8
Pcb4 0.005264 True
Fryum 0.00995 True
0.7
Pcb2 0.010521 True
Pcb1 0.013953 True
Chewinggum 0.024117 True 0.6
Macaroni2 0.035739 True
Cashew 0.049281 True
0.5
Candle 0.051116 False
Capsules 0.082838 False
0.4 0.2 0.0 0.2 0.4
Saturation shift [intensity]
Table3.AveragecornerextrapolationMSEforobjectsintheVisA
dataset. Figure 6. Zero-shot anomaly segmentation performance when
the same saturation shift δ is applied to the MVTec test set for
s
rotation-invariantobjects.
Whilethesameconclusionsrelatedtohowsmoothperfor-
mancevaries witheachaugmentation type(noisyfor rota-
onlyinvolvecolours(hueandsaturation)aremoreeffective
tions, smooth for color shifts), the dynamic ranges of the
(leads to a higher performance drop) than the ones involv-
per-objectperformancearemuchlargerforVisA,whichis
ingtherotationaugmentation. Weattributethistothenon-
generallyamoredifficultdataset. Forcertainobjects,such
as Pcb1 or Macaroni1, performance can vary by up to smoothoptimizationsurfacefortherotationangleθ,which
holdsevenunderourconsideredsegmentationloss.
0.4pointsinpAUROCwhenrotationsorhueshiftsareap-
plied,respectively.
C.Additionalworst-caselowerboundresults
Figure10showstheresultsforworst-caselowerboundsob-
tainedbyjointlyoptimizinganycombinationofconsidered
augmentations. Ingeneral,wenotethataugmentationsthat
tnuoC
CORUApPipe fryum Pcb2 Macaroni2 Cashew
1.0
Chewinggum Macaroni1 Pcb1 Fryum
0.9
0.8
0.7
0.6
0.5
0.4
0.3
80 60 40 20 0 20 40 60 80 1.0
Rotation angle [degrees] Pipe fryum Pcb2 Macaroni2 Cashew
Chewinggum Macaroni1 Pcb1 Fryum
0.9
Figure7. Zero-shotanomalysegmentationperformancewhenthe
samerotationangleθ isappliedtotheVisAtestsetforrotation- 0.8
invariantobjects.
0.7
0.6
0.5
0.4
0.3
0.2
1.0 0.4 0.2 0.0 0.2 0.4
Pipe fryum Pcb2 Macaroni2 Cashew Saturation shift [intensity]
Chewinggum Macaroni1 Pcb1 Fryum
0.9
Figure9. Zero-shotanomalysegmentationperformancewhenthe
0.8 samesaturationshiftδ sisappliedtotheVisAtestsetforrotation-
invariantobjects.
0.7
0.6
0.5
0.4
0.3
3 2 1 0 1 2 3
Hue shift [radians]
Figure8. Zero-shotanomalysegmentationperformancewhenthe
same hue shift δ is applied to the VisA test set for rotation-
h
invariantobjects.
CORUAp
CORUAp
CORUApMVTec VisA
0.0
0.1
Angle-only
0.2 Hue-only
Saturation-only
Angle+Hue
0.3 Angle+Saturation
Hue+Saturation
Three-dimensional
0.4
0.5
ViT-B/16+ ViT-L/14 ViT-L/14-FARE² ViT-B/16+ ViT-L/14 ViT-L/14-FARE²
Figure10.Zero-shotanomalysegmentationperformancefortwodatasets(MVTecandVisA,leftandright,respectively)usingthreeCLIP
backbones(ViT-B/16+,ViT-L/14,andadversariallyfine-tunedViT-L/14-FARE2forWinCLIP.Eachpossiblecombinationofthethreetest-
time,worst-casesemanticperturbations(angle,saturation,hue)isconsidered,toyieldasetofeighttotaltest-timeaugmentationstrategies.
Thebarsshowthedifferencebetweentheoriginaltestsetsandtheconsideredlowerbounds.ThisfigurecomplementsFigure4inthemain
body.
CORUAp
)lanigiro
detnemgua(