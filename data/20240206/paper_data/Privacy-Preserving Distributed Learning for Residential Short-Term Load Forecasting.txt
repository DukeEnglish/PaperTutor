IEEEINTERNETOFTHINGSJOURNAL,VOL.XX,NO.X,FEBRUARY2024 1
Privacy-Preserving Distributed Learning for
Residential Short-Term Load Forecasting
Yi Dong∗§, Yingjie Wang∗†, Mariana Gama‡, Mustafa A. Mustafa¶‡, Geert Deconinck†, and Xiaowei Huang§
Abstract—In the realm of power systems, the increasing LoadForecasting(STLF)hasbeenwidelystudiedtofacilitate
involvement of residential users in load forecasting applications the power system operations [3], [4]. However, it is evident
hasheightenedconcernsaboutdataprivacy.Specifically,theload
that residential-user privacy is at risk when residential-user
data can inadvertently reveal the daily routines of residential
load data is collected and mined [5]. For example, the res-
users, thereby posing a risk to their property security. While
federated learning (FL) has been employed to safeguard user idential user’s daily routines and presence at home can be
privacybyenablingmodeltrainingwithouttheexchangeofraw detected with a high probability from its electric load data,
data, these FL models have shown vulnerabilities to emerging whichwilldirectlyaffecttheresidentialuser’spropertysafety.
attack techniques, such as Deep Leakage from Gradients and
Therefore, how to accurately forecast residential power load
poisoning attacks. To counteract these, we initially employ a
while ensuring data privacy becomes an open challenge.
Secure-Aggregation(SecAgg)algorithmthatleveragesmultiparty
computationcryptographictechniquestomitigatetheriskofgra- The Federated Learning (FL) method has been introduced
dient leakage. However, the introduction of SecAgg necessitates in recent years to overcome the challenges of residential-user
the deployment of additional sub-center servers for executing
privacy. It can decouple the data storage from the training
the multiparty computation protocol, thereby escalating compu-
process [6], while reaching a desirable accuracy compared
tational complexity and reducing system robustness, especially
in scenarios where one or more sub-centers are unavailable. To to the centrally trained model [7], [8]. There are already
address these challenges, we introduce a Markovian Switching- effortstoapplyFLtopowersystemforecastingtopreservethe
based distributed training framework, the convergence of which sensitive individual consumption profiles [9]–[14]. Yang et al.
is substantiated through rigorous theoretical analysis. The Dis-
integrate variational mode decomposition, federated k-means
tributed Markovian Switching (DMS) topology shows strong
clusteringalgorithm,andSecureBoosttogetherforSTLFwith
robustness towards the poisoning attacks as well. Case studies
employingreal-worldpowersystemloaddatavalidatetheefficacy data privacy protection [9]. Jun et al. propose a novel method
of our proposed algorithm. It not only significantly minimizes for disaggregating community-level behind-the-meters solar
communication complexity but also maintains accuracy levels generation using a federated learning-based Bayesian neural
comparable to traditional FL methods, thereby enhancing the
network, which can preserve the confidentiality of utilities’
scalability of our load forecasting algorithm.
data[11].Yongetal.proposeaverifiableandoblivioussecure
Index Terms—Load Forecasting, Data Privacy, Distributed aggregation for FL [15]. Their algorithm could tolerate the
Learning, Federated Learning, Secure Aggregation, Collabora-
high drop-up rate of clients during large-scale FL training.
tive Work
Asad et al. highlight recent FL algorithms and evaluate their
communicationefficiencyindetailedcomparisons.Theexper-
I. INTRODUCTION imental results indicate that training data-driven models using
ELECTRIC load forecasting plays an essential role in FL not only enhances security and privacy, but also reduces
power scheduling, planning, operating and management communication costs [16].
[1], [2]. The stability of the power system is under threat Theabove-mentionedFLbasedalgorithmsclaimtheadvan-
due to the intermittence of renewable energy generations tage of privacy in terms of not sharing the original data. With
and the complex nature of utility-customer interactions and the gradual success of various privacy attack technologies in
dynamic behaviors. To overcome this, residential Short-term different applications, the load data can also be reverted if the
attacker can access the gradients from agents [17]. Here, we
*Bothauthorscontributedequallytothisresearch.
useasimpleexampletoshowthethreatoftheattackalgorithm
§DepartmentofComputerScience,UniversityofLiverpool,UK,{yi.dong,
xiaowei.huang}@liverpool.ac.uk to the artificial intelligence-based load forecasting algorithms,
† Electa, Department of Electrical Engineering (ESAT), KU Leuven and shown in Fig. 1.
EnergyVille,Belgium,{tony.wang,geert.deconinck}@kuleuven.be
‡ COSIC, Department of Electrical Engineering (ESAT), KU Leuven, In Fig. 1, the two figures on the left are the original
Belgium,mariana.botelhodagama@kuleuven.be consumption data and the original image after conversion.
¶ Department of Computer Science, The University of Manchester, UK, The sub-figures on the right are the regenerated images from
mustafa.mustafa@manchester.ac.uk
the attacker after they obtained the original gradients. As we
This work is supported by the UK EPSRC (End-to-End Conceptual
GuardingofNeuralArchitectures[EP/T026995/1])andtheFWOSBOproject can see, the attacker can accurately regenerate the original
SNIPPET (Secure and Privacy-Friendly Peer-to-Peer Electricity Trading data only after 10 iterations. Therefore, any leakage of the
[S007619N]). This project has received funding from the European Union’s
gradient can cause a significant threat to customers’ data. To
Horizon 2020 (FOCETA: grant agreement No 956123) and UKRI (SPACE:
projectNo10046257). prevent information leakage from agents’ gradients, Bonawitz
Copyright©2024IEEE.Personaluseofthismaterialispermitted.
However,permissiontousethismaterialforanyotherpurposesmustbeobtainedfromtheIEEEbysendingarequesttopubs-permissions@ieee.org.
4202
beF
2
]GL.sc[
1v64510.2042:viXraIEEEINTERNETOFTHINGSJOURNAL,VOL.XX,NO.X,FEBRUARY2024 2
Comparison between Recovered and Real Load Deep Leakage from Gradient Attack Process a Byzantine-robust federated learning model against the poi-
1.0 soning attacks in various ways, such as detecting malicious
Iter = 0 Iter = 10
0.8
clients based on their impact on the model performance
0.6
and constructing new loss functions [26]–[29]. However,
0.4
the common Byzantine-robust models cannot always defend
0.2
the poisoning attacks, particularly when some of the clients
0.0
0 5 10 15 20 conspire together [30]. Among the more advanced defenses,
24 hour load after conversion Iter = 20 Iter = 30 there are two significant approaches. “Romoa” is a robust
0
model aggregation solution to both solo and collusive attacks.
5
With a hybrid similarity measurement method, it can detect
10
the attacks precisely whether it is targeted or untargeted
15
attacks [31]. “FLTrust” reserved a small clean dataset as a
20
reference at the server. By cross-validation, the central server
0 5 10 15 20 gives a trust score to each collected update. It provides a
Fig. 1: Deep leakage from gradient attack. more precise and less computationally costly way to detect
malicious clients [32].
To overcome the above-mentioned challenges, we borrow
et al. [18] proposed using Multi-Party Computation (MPC) to the idea from consensus-based distributed optimization [33]–
aggregate the gradients privately. Multiparty computation is [35], which has flexibility in connected network graphs and
a cryptographic technique that allows a group of mistrustful initialization-freeadvantages.Inthispaper,weproposeafully
parties to perform a computation over secret input data. By decentralizedMarkovianSwitchingshort-termresidential-user
usingMPC,agroupofagentscanobtaintheaggregatedvalue loadforecastingalgorithm.Theproposeddistributedalgorithm
of all their gradients without ever revealing any individual employs Markovian Switching topologies [36], which can
gradient. The ideas presented in [18] were extensively used randomly select sets of agents to co-train the model, and
and developed in further work related to secure aggregation therefore reduce the communication complexity in theory
for privacy-preserving federated learning, such as [19]–[24]. and accelerate encryption speed. Furthermore, the Distributed
Since these works focus on the federated learning setting, the Markovian Switching (DMS) topology is instinct-resilient
secureaggregationmechanismsrelyontheexistenceofatleast against poisoning attacks. In the centralized FL training, the
one central server. For the works using additive masking [18], central server updates the global model with the average
[22],[23],asingleserverissufficient.In[21],[24],thecentral updatesacrossthewholeclientgroupeachround.Eventhough
aggregator is distributed, meaning that at least two servers are the malicious clients take a small part of the whole group, the
required. accumulated strewing effect can significantly undermine the
Multiparty computation achieves privacy by distributing global model over multiple rounds. Unlike the centralized FL
the computation between different parties, which then need setting, the DMS takes a random set of clients each round,
to communicate with each other according to the chosen whichreducesthechanceofmaliciousclientspoisoningattack
MPC protocol to obtain the result of the desired calculation for every round. The above-mentioned cumulative effect will
without compromising private input data. The parties running not happen in the DMS setting.
the MPC protocol can either be the agents themselves, or The major contributions of this work are summarized as
external servers that receive the data from the agents already follows:
in encrypted form. However, there are two possible issues: 1) A secure and safe distributed algorithm is proposed for
1) Scalability: performing computations over private data short-term load forecasting and its convergence has been
leads to an increase in communication. Setting the MPC theoreticallyproved.Differentfrommostexistingstudies,
parties as external servers helps diminish this overhead, but e.g.,[5],[9]andreferencestherein,whichareextensively
still results in an expanded network, with each agent needing concentrated on federated algorithms, the proposed algo-
to connect to every MPC party. rithm in this paper is based on fully Peer-to-Peer (P2P)
2) Single-point failures: if the central server in a federated distributed consensus learning without a central server.
learning system experiences a single-point failure, then the 2) The DMS is developed for the proposed framework.
entire system may become unavailable until the server is Different from the traditional FL, the DMS has inherited
restored. The MPC protocol we use does not solve the issue, robustness towards both poisoning attacks and DLG at-
as it needs all connected parties. However, due to the nature tacks. Besides that, the DMS topology shows advantages
of our proposed network topology, no single party (or server) in efficiency compared with other distributed learning
will need to remain online during the full training process. topologies.
Besides the Deep Leakage from Gradients (DLG) attacks, 3) We apply the Secure Aggregation (SecAgg) to the
the poisoning attack is another common threat towards FL proposed distributed learning topologies. The SecAgg
models. A poisoning attack involves one or a few malicious ensures no original gradients are shared. Therefore it
clients injecting extreme outliers or tailored data into the guarantees data privacy from DLG attacks by design.
trainingdatasetormodelupdatestoundermineormisleadthe Additionally, we analyzed its impact on the original
global model performance [25]. Some researchers developed complexity as an extra add-up layer.IEEEINTERNETOFTHINGSJOURNAL,VOL.XX,NO.X,FEBRUARY2024 3
4) The proposed algorithm has been successfully validated thedistributedloadforecastingconformstothecharacteristics
on a real power system load forecasting dataset. Further- of the undirected graph, which can enhance the stability of
more,thereproducibilityandreplicabilityofourworkare communicated graphs and reduce the convergence time [37].
guaranteed since all source code is available on GitHub LetV ={1,...,N}beasetofN localagents.Assumethat,
for open access1. at any time t, the agents constitute a graph Gt = (Vt,Et),
The rest of this paper is organized as follows. The mathe- whereVt ⊆V isasetofagentsandEt ⊆Vt×Vt isasetof
matical preliminaries used in this paper are summarized and edges. Whenever (i,j)∈Et, we say that they are neighbors.
the researched problem is formulated in Section II. A secure- We write Lt as the Laplace matrix of Gt.
aggregated and Markovian switching distributed framework (cid:26)
−a , i̸=j
for STLF is proposed and analyzed in Section III. Simulation Lt
ij
= (cid:80)ij
a , i=j
(1)
resultsandcorrespondinganalysisarepresentedinSectionIV. i̸=j ij
Finally, Section V concludes this paper. To enhance privacy and safety during the training process,
the communication graph in this paper is considered as a
II. PRELIMINARIES time-variance topology, called Markovian switching network
topology, which means that the graph Gt might change over
In this section, we recall some basic concepts (federated
time due to the addition and deletion of agents, which forms
learning and distributed learning) and preliminaries related to
a temporal graph G ={Gt} . The Markovian switching
the proposed STLF model, including graph theory and secure t=0,1,...
network has q substructures. The transition probability matrix
aggregation.
is T = [T ] ∈ Rq×q, where T = P{Gt+1 = j|Gt =
ij ij
i},∀i,j ∈Q with Q being the graph structure topology set.
A. Federated Learning
FL is a distributed training framework for neural networks. C. Distributed Learning
It decouples the training process and the requirement for
Different from the FL framework, the distributed learning
centralized data storage. Instead of collecting raw data from
algorithm in this paper is designed in a fully decentralized
the individual agent, the raw data stays locally with agents.
setting, where the agents can only communicate with their
The agents use their own data to train the global model,
adjacent neighbors.
M . Then, the central server collects the weights updates
j From [38], the training process of the distributed learning
fromtheagents.Basedontheselectedalgorithm,forexample
algorithm can be summarized as:
the Federated Average (FedAvg), the server will average the
submittedweightsandusethemtoupdatetheglobalmodelto ϕ (k)=θ (k)−γ∇ L(D ,M (θ (k))) (2)
i i θi i PF,i i
M .Inthisway,agentscanparticipateinthemodeltraining (cid:88)
j+1 θ (k+1)=α a ϕ (k) (3)
withoutrevealingtheirrawdata.Figure2showstheprocedure i ij j
j∈N
of FL.
where θ (k) and ϕ (k) denote the weights of the neural
i i
Global model network before and after the kth training iteration, γ,α > 0
Central server
Mj
are the learning rate, and ∇(·) is the gradient term regarding
Initialize Weightsn
empirical risk L, which is a measure of how well a machine
Update with
collected Weights Average Weights1 learning model performs on a given dataset. It is defined as
the average of the loss function (i.e., the discrepancy between
Global model Extract
predicted values and actual values) over the entire dataset. D
Mj+1 i
Agentn and M PF,i denote the dataset and local model of ith agent,
Convergent? No Local respectively.
Broadcast Agent1
Private data1
train
Notice that the equations (2) and (3) are also called an
Yes
Private datan Learning-Then-Consensus (LTC) algorithm, where we can
Ready to use
easily swap the equations (2) and (3) to get the twinned
Fig. 2: Training procedure of federated learning. Consensus-Then-Learning (CTL) algorithm.
IntheFL framework,thesubmittedlocal modelupdatecan D. Secure Aggregation
be“gradients”or“weights”.Itdependsonthecommunication
Secure aggregation is achieved through a cryptographic
bandwidth, but they are practically the same. In the privacy
technique, called MPC, which allows us to perform calcu-
analysis, we use the term “weights” for consistency.
lations over encrypted data. There are different types of MPC
protocols, with different security guarantees. In this work,
B. Graph Theory we use SCALE-MAMBA [39], a software framework that
implementsMPCprotocolsbasedonsecretsharingwhichhave
In this paper, it is assumed that the information can be
active security, meaning that the protocol remains secure even
communicatedbetweendifferentcomputingagents.Therefore,
if up to a certain percentage of the parties performing the
1https://github.com/YingjieWangTony/FL-DL.git computationbehavesmaliciouslyanddeviatesarbitrarilyfromIEEEINTERNETOFTHINGSJOURNAL,VOL.XX,NO.X,FEBRUARY2024 4
the protocol. Additionally, the protocols in SCALE-MAMBA secret shared result can be reached without revealing the
are in the preprocessing model. In this model, there is an inputs. Indeed, communication is generally the bottleneck for
offline phase, where input-independent data is generated. This computationswithMPC.However,forsecureaggregationonly
can be done at any time, including before the inputs for additionisneeded,andsocommunicationisonlyrequiredfor
the desired computation are even available. Later, during the the secret sharing of the inputs and reconstructing the final
onlinephase,wecanperformourcomputationfasterbyusing aggregated results.
the preprocessed data. AsillustratedinFig.3,itispossibletohaveagentsalsoact
In this paper, we consider an MPC protocol based on the as an MPC party, in which case each agent needs to secretly
Shamirsecretsharingscheme.Inthisscheme,inordertoshare share its own gradients with the other agents (who are also
a secret value s with a set of ν parties, the person holding s actingasMPCparties).Inordertoimprovescalability,wecan
must generate a secret polynomial f(X) of degree h such instead have external servers play the role of MPC parties. In
that f(0) = s. By randomly generating the other polynomial this case, we can have as few as three MPC parties, to whom
coefficients f ,...,f , the following polynomial is obtained: the agents will send secret shares of their gradients. These
1 h
external servers should be run by entities with conflicting
f(X)=s+f ·X+...+f ·Xh.
1 h interests,inordertoavoidcollusionandthereforemaintainthe
Then, each party i is given a secret share s =f(i). Note that privacyofthegradients.Notethatweshouldalwaysaggregate
i
each secret share by itself looks random, but when at least morethantwogradientsineachround,sinceotherwisethetwo
h+1partiescombinetheirsharesthentheoriginalsecretvalue agents whose weights were aggregated will be able to derive
can be retrieved via Lagrange interpolation. The choice of each other’s gradients from the revealed aggregated value.
degreehwilldeterminehowmanydishonestpartiesweallow Remark 1: We note that our use of Secure Aggregation is
without compromising the secret since any set of h or fewer not intended to be an improvement upon previous work in the
partiescannotreconstructthepolynomial.However,tobeable federatedlearningsetting.Indeed,thisworkisinafundamen-
to perform all the operations correctly with this scheme, it is tally different setting due to our proposed network topology.
also required that a majority of parties is honest. Although some existing papers already analyze the use of
We write ⟨x⟩ when we refer to a secret shared value x, and different topologies for improving the efficiency of secure
each secret share i is represented as ⟨x⟩ . All these values are aggregation [22], [23], they always assume the existence of
i
elements of a finite field F , where p is a prime number, and a central aggregation server. Additionally, Differential Privacy
p
operations between secret shared values also take place in F . (DP)hasalsobeenconsideredinpreviousworkstoensurethat
p
even the final aggregated result does not leak any information
[19],[20].WedonotanalyzetheuseDPsinceitisnotdirectly
Agents/MPC parties affected by the choice of topology. Nonetheless, it would be
⟨𝜙⟩1 relativelystraightforwardtoadaptprevioustechniques[40]for
combining MPC and DP to our use case.
⟨𝜙⟩2 ⟨𝜙⟩5
III. DISTRIBUTEDMARKOVIANSWITCHINGALGORITHM
A. Problem Formulation
Inthispaper,wefocusontheprivacy-preservingresidential
⟨𝜙⟩3 ⟨𝜙⟩4 short-termloadforecastingproblem,whereeveryagentiholds
aprivatedatasetD ,andwedefineD =D ∪...∪D .TheN
(a)MPCpartiesplayedbytheagents. i 1 N
agents will communicate and train a model M where DL
Agents MPC parties DL
⟨𝜙⟩1 stands for distributed learning. This will be compared with a
federated learning model M (i.e., the server collects and
FL
⟨𝜙⟩2
modifiestheweightsfromalllocalagentsduringthetraining)
⟨𝜙⟩3 and a model M
C
trained by centralized learning (i.e., the
server collects all raw data from the agents before training).
B. DMS Learning
(b)MPCpartiesplayedbyexternalservers.
Fig. 3: Agent secret sharing a gradient ϕ with the MPC parties. The proposed short-term load forecasting algorithm is to
utilize decentralized distributed learning to collectively aggre-
To perform linear operations, i.e., adding two secret shared gate the local gradients. In a fully decentralized setting, the
values or multiplying a secret shared value by a public scalar agents can only send/collect messages to/from their neighbors
inF ,itisenoughforeachpartytolocallyperformtheseoper- but have a collective objective of training a global model
p
ationsontheirownshares.E.g.,tocalculate⟨z⟩=⟨x⟩+a·⟨y⟩, M , which is required to be high-performing and privacy-
DL
eachpartyishouldcompute⟨z⟩ =⟨x⟩ +a·⟨y⟩ .Multiplying preserving.
i i i
two secret shared values or performing more complex opera- Every distributed agent is regarded as a self-training agent
tions such as comparisons requires communication between and cooperates with each other using distributed neural net-
the parties running the MPC protocol so that the correct works with a neurodynamic consensus-based approach, as
…IEEEINTERNETOFTHINGSJOURNAL,VOL.XX,NO.X,FEBRUARY2024 5
User 1 User 2 [41]–[43].Inthispaper,agradientdescentalgorithmforagent
𝜃1
i is formulated as
𝜃2
𝜃n 𝜃1 …𝜃i
…
𝜃3 𝜃2 ϕ i(k)=θ i(k)−γ∇ θiL(D i,M PF,i(θ i(k))) (8)
User n 𝜃j User 3 Neural Network Model (cid:88)
⟨θ (k+1)⟩=α a ⟨ϕ (k)⟩ (9)
Input Layer Hidden Layer Output Layer i ij j
j∈N
Fig. 4: An illustrative framework of distributed learning.
where 0 ≤ α ≤ 1 is positive constant; a denotes the
ij
communication link between agent i and agent j; a > 0
ij
shown in Fig. 4. The framework consists of a group of neural if they can communicate, otherwise, a = 0. Note that
ij
networksdistributedoveraconnectedgraph,wheretheupdate for each agent i, the consensus only requires the updated
of coefficients includes two stages: in the first stage, the weights ϕ from its adjacent neighbors j ∈ N with N
j i i
ith agent trains its neural network locally using the dataset denoting the neighbors of agent i. Recall also that we add
D , and in the second stage, the consensus of their locally theupdatedweightsusingsecureaggregation,hencethesecret
i
trained weights is performed. With this setting, all the agents share notation used in Equation 9. The shares of ⟨θ (k+1)⟩
i
are able to obtain the same neural network if every sub- will then be sent by the MPC parties to agent i, who can
dataset is available to at least one local agent, which will be finally reconstruct the value θ (k+1).
i
demonstrated by the convergence analysis. Based on the above discussion, the overall structure of the
Minimizing the empirical risk allows the model to gen- fully decentralized distributed algorithm for a region can be
eralize well to new, unseen data. The idea is that if the summarized in Algorithm 1.
model is trained on a dataset that is representative of the real-
world data it will encounter in the future, then minimizing Algorithm 1 Distributed Markovian Switching (DMS)
the empirical risk should result in a model that can make Initialization:
accurate predictions on new data. Therefore, the objective of for each agent i∈V:
the network is to minimize the empirical risk over the entire 1. initialize the weights of local neural network θ ;
i
dataset, given by 2. set the required consensus speed by changing the coeffi-
cients in Equation (9).
N
(cid:88) Iteration:
minL(D,M (θ))= L(D ,M (θ )) (4)
PF i PF,i i
3. set k :=k+1, for i∈V
i=1
4. train the local neural network using the gradient descent
where D and D denote the entire dataset and the sub-dataset
i method (8) with locally stored dataset D to obtain ϕ (k);
of agent i, respectively; θ and θ are the weights of the i i
i 5. apply secure aggregation to encrypt original weights
global and local neural networks; M and M represent
PF PF,i ⟨ϕ (k)⟩;
the models of the global neural network and the local ones, i
6. run the consensus operation (9) to drive the globally aver-
respectively. It is worth noting that the local learning process,
aged weights θ (k+1) by communicating with its neighbors
i.e., i
N ;
i
minL(D ,M (θ )) (5) Termination: termination condition is satisfied or iteration
i PF,i i
budget is reached.
does not necessarily yield the minimization of the global
empirical risk in (4), since the models M are different.
PF,i
Consequently, this necessitates the employment of consensus
C. Correctness Analysis on Performance/Convergence
tools in cooperation with the training process.
Note that the sub-dataset of each agent is assumed to be We need to show that Algorithm 1 is able to converge with
stationary.Fortheithagentwithn samplesinD ,thegradient all agents having the same model. Considering the generality,
i i
can be calculated by we choose Mean Squared Error (MSE) as the loss function.
MSE is a widely used measure in statistics and machine
1
(cid:88)ni
∇ L(D ,M (θ ))= ∇ ℓ(sk,θ ) (6) learning for quantifying the difference between estimated
θi i PF,i i n θi i
i valuesandtheactualvalues[44].Itiscalculatedastheaverage
k=1
ofthesquaresoftheerrorsordeviations,thatis,thedifference
where sk represents the kth data sample in D , and ℓ(sk,θ )
i i between the estimator and what is estimated. Mathematically,
is the empirical risk of sample sk on a model with weights
MSE is defined as
θ . Similarly, the global gradient is given by
i
1
(cid:88)ns
∇ θL(D,M PF(θ))=
n1 (cid:88)n
∇ θℓ(sk,θ) (7)
MSE =
n s
i=1(y i−yˆ)2 (10)
k=1
where n is the number of samples, y and yˆ are the true and
s i i
where
n=(cid:80)N
n is the total number of samples in D. predicted values, respectively.
i=1 i
Gradient-based methods have been widely used in training First of all, two assumptions are needed. With these two
neural networks, due to their high efficiency and flexibility assumptions and the mean value theorem, we can obtain theIEEEINTERNETOFTHINGSJOURNAL,VOL.XX,NO.X,FEBRUARY2024 6
closed-loop MSE dynamics, and therefore the convergence of To simplify the notations in the analysis process below,
the MSE can be proved with a designed learning rate. we rewrite the weights in a compact form by integrating all
Assumption 1: The Hessian matrix of local empirical risk weights into a vector form as:
is bounded, i.e.,:
Φ(k)=col(ϕ (k),··· ,ϕ (k)) (15)
1 N
p I ≤∇2L(D ,M (θ ))≤p I (11) Θ(k)=col(θ (k),··· ,θ (k)) (16)
i n i PF,i i i n 1 N
Then, we can rewrite the proposed distributed algorithm (8)
where 0≤p ≤p are non-negative constants.
i i and (9) in the following form:
Assumption 2: The gradient noise, w , satisfies the follow-
i
ing properties i.e.,: Φ(k)=Θ(k)−(Γ⊗I )∇L(Θ(k))+Ω(k) (17)
n
E[w ]=0 (12) Θ(k+1)=α(A⊗I n)Φ(k) (18)
i
E[∥w ∥2]≤ξ2 (13) where Γ = diag(η ,··· ,η ), ∇L(Θ) =
i i 1 N
col(∇L (Θ),··· ,∇L (Θ)), Ω = col(ω ,··· ,ω ). A
1 N 1 N
where ξ i is non-negative constant, ∥·∥ denotes the L2-norm is the Laplacian matrix, which is defined as A = [a ij] N×N.
of the argument. a > 0 if the agent i is communicating with agent j. Note
ij
Remark 2: Assumption 1 indicates that the network ob- that each row of the A sums to 1, which is also called
jective,
(cid:80)N
i=1L(D i,M PF,i(θ i)), is strongly convex, which row-stochastic matrix [37].
means that the neural network has a unique and optimal so- ToconfirmthattheMSEconverges,wetransfertheweights
lution/weights θ∗. In distributed optimization problems, some to an error recursion formation:
assumptions have been made, including the use of bounded
Φˆ(k)=Φ(k)−θ∗1 (19)
Hessian matrices for convergence analysis. Other works [45]– N
[47] have used bounded gradients with set constraints, which Θˆ(k)=Θ(k)−θ∗1 (20)
N
is not feasible for unconstrained problems. Our assumption is
Furthermore, to relate the gradient term ∇L(Θ(k)) with
lessrestrictivebecausewecansolveproblemswithunbounded
error term Θˆ(k), we have the mean value theorem from [49]:
gradients.
Assumption 2 implies that the noise, w i, is unbiased and (cid:90) 1
has uniformly bounded variance ξ i2, which is a stronger ∇ zg(y)=∇ zg(x)+[ ∇2 zg[x+τ(y−x)]dτ](y−x) (21)
assumption and has been considered in many studies [42], 0
[48]. SubstitutingtheΘ(k)andθ∗1 N intoequation(21)asy and
Theorem 1: Under Assumptions 1 and 2, if the learning
x:
rates satisfy 0 ≤ γ ≤ 2, the worst MSE, ∥N∥∞ in the (cid:90) 1
distributed learning ni etworp ki converges to ∇ θL(Θ(k))=∇ θL(θ∗1N)+[ ∇ θ∇L[θ∗1N+τ(Θˆ(k))]dτ](Θˆ(k))
0
(22)
lim ∥N(k)∥∞ ⪯αΞ (14) Since ∇ θL(θ∗1 N) = 0, and we let P(k) =
k→∞ (cid:82)1 ∇ ∇L[θ∗1 + τ(Θˆ(k))]dτ. The equation (17) can be
0 θ N
rewritten as:
where γ is the locallearning rate for ith agent. p is the local
i i
upper bound of the Hessian matrix as shown in equation (11). Φˆ(k)=Θˆ(k)−(Γ⊗I )P(k)Θˆ(k)+Ω(k) (23)
N
NistheMSEvectorsofthedistributedneuralnetworks,which
is defined later in equation (25). Ξ=diag(ξ2,··· ,ξ2 ) is the Combining(18)and(23)together,wecangettheerrorterm
1 N
matrix format of the uniformly bounded variances. update law as:
uppR ee rm ba or uk nd3 f: orIn deT sih ge no inre gm the1, le0 arn≤ ingγ ri at≤
es,
bp2 iasep dro ov nid wes hica hn Θˆ(k+1)=α(A⊗I n)[I nN−(Γ⊗I N)P(k)]Θˆ(k)+α(A⊗I n)Ω (( 2k 4)
)
we can prove the convergence of the proposed algorithms. We define the MSE vectors of the distributed neural net-
Equation (14) reveals how the learning rates, the bounds of works as:
the Hessian matrix, and the noise variance will affect the
M(k)=col(E∥ϕˆ (k)∥2,··· ,E∥ϕˆ (k)∥2) (25)
performance of the algorithms. It means that minimizing the 1 N
learning rates can lead to a reduction in the optimality error. N(k)=col(E∥θˆ (k)∥2,··· ,E∥θˆ (k)∥2) (26)
1 N
The designer can balance between the convergence speed
By taking the Euclidean norm for each agent’s error dy-
and the optimality error according to the real applications by
namics, we have:
adjustingtheheterogeneouslearningrates.Inotherwords,this
theorem serves as a reference to select learning rates during ∥θˆ(k+1)∥2 =∥α(cid:88) a ϕˆ (k)∥2 (27)
i ij j
our training process in both DL and FL frameworks. In (14),
j∈N
wehave3parameters,p ,αandξ .p canbeobtainedfromthe
i i i
Since the norm function ∥·∥ is a convex function, applying
loss function, α ≤ 1 is a fixed parameter, and the coefficient
Jensen’s inequality to (27):
of gradient noises ξ should be approximated according to the
i
practical applications. ∥θˆ(k+1)∥2 ≤α(cid:88) a ∥ϕˆ (k)∥2 (28)
i ij j
Proof:
j∈NIEEEINTERNETOFTHINGSJOURNAL,VOL.XX,NO.X,FEBRUARY2024 7
Takingtheexpectationofbothsidesof(28),wecanobtain: to exchange information with the agents that are in its direct
vicinity.Thislimitedcommunicationrequirementisbeneficial
N(k+1)⪯αAM(k) (29)
asitreducestheoverallcomplexityofthealgorithmandmakes
Taking the Euclidean norm and expectation of Φˆ (k) leads it more suitable for distributed implementation. Moreover, the
i
to: proposed algorithm removes the requirement of the central
server, which can be a bottleneck in some systems.
M(k)=E∥[I −(Γ⊗I )P(k)]Θˆ(k)∥2+E∥Ω(k)∥2 (30)
nN N Inthissetting,thecommunicationcomplexityforeachagent
Based on Assumption 1, the bound of the symmetrical is O(n|E|), where E is the set of communication edges and
matrix P(k) could be obtained: n is the number of iterations, because in every iteration the
agent only needs to send one message to their neighbors and
p iI n ≤P(k)≤p iI n (31) every message is of constant size O(1). The computational
complexity for every agent is also O(n|E|), since every
and therefore:
iteration the agent can process the received messages and
2
0≤[I nN −(Γ⊗I N)P(k)]≤λ iI n (32) update the local state in a linear way. It is noticed that due to
therandomlyconnectedcharacteristicofMarkovianswitching
where λ =max(1−γ p )2,(1−γ p )2.
i i i i i topology, the communication edges in each round are not a
Based on Assumption 2, we have:
constant value, e.g. E ∈[1,30],n=30.
E∥Ω(k)∥2 ≤Ξ2 (33)
IV. CASESTUDIES
Until now, the closed-loop MSE error can be derived as:
In our study, we propose a new training strategy called
N(k+1)⪯αAΛN(k)+αAΞ (34) DMSlearning,whichutilizesadynamicswitchingmechanism
between different topologies during the learning process. To
where Λ=diag(λ 1,··· ,λ i) evaluate the performance of DMS, we compare it with four
Considering the worst case MSE, ∥N(k+1)∥∞, other popular training strategies: Centralized Learning, Fe-
dAvg, Distributed Learning with Fully-Connected (DFC), and
∥N(k+1)∥∞ ⪯α∥AΛ∥∞∥N(k)∥∞+α∥AΞ∥∞ (35) Distributed Learning with Ring Topology (DRING). Here, we
useFig.5toillustratethecommunicationdifferencesbetween
To guarantee the convergence of (35), we must have different learning strategies. These five training strategies are
∥AΛ∥∞ ≤ 1. We also have ∥A∥∞ = 1 and α ≤ 1, so we implemented on four state-of-the-art deep learning neural
have network models: DNN [50], CNN [51], Wavenet [52], and
λ i ≤1 (36) LSTM[3].Theexperimentsareconductedonadatasetwitha
large number of samples, and the results are evaluated based
which means:
on the accuracy, convergence rate, and communication cost of
(1−γ p )2 ≤1 (37) the trained models.
i i
(1−γ p )2 ≤1 (38)
i i 2 1 2 1 2 1
Because of p ≤ p , we should choose the learning rate be-
i i 3 S 6 3 6 3 6
tween the range to guarantee the convergence of the proposed
distributed learning algorithm: 2 F Te od pe or4 la ote gd y A (Fv ee dr5 a Ag vi gn )g Ring To4 pology (D5 RING) TF ou pl4 l oy l oC go yn n (Dec5 Ft Ced )
0≤γ ≤ (39)
i p 2 1 2 1 2 1 2 1
i
■ 3 6 3 6 3 6 … 3 6
Aftertheconvergenceanalysis,wealsoprovideatheoretical 4 5 4 5 4 5 4 5
t = 0 t = 1 t = 2 t = n
analysis of the proposed algorithm due to the complexity
Markovian Switching Topologies (DMS)
of the algorithm is not easy to visualize. We evaluate the …
S Central Server 1 2 3 4 5 6 Clients
complexitybasedonthecommunicatedcomponents’sizewith
Fig. 5: Communication topologies of different strategies.
a normalized factor, O(n|E|). The algorithm is a gradient-
based algorithm, which is a widespread technique in ma-
chine learning and optimization. Gradient-based algorithms
A. Data Sources
are efficient in terms of computation, as they only require
thecalculationofthegradientoftheobjectivefunction,which The Smart Metering Electricity Customer Behavior Trials
canbedoneusingsimplemathematicaloperations.Thismakes are used as our case study [53]. It contains over 5000 Irish
thealgorithmeasytoimplementandcomputationallyefficient, home and business participants during 2009 and 2010. Their
which is a desirable feature in many real-world applications. electricity consumption is recorded by smart meters with 30
In terms of communication complexity, the proposed al- minutesintervals.ThelongestrecordisfromJanuary1st,2009
gorithm only requires the agents to communicate with their to June 30th, 2010. After data cleaning and clustering, we
immediate neighbors. In other words, each agent only needs selected30householdstopresentavirtualenergycommunity.IEEEINTERNETOFTHINGSJOURNAL,VOL.XX,NO.X,FEBRUARY2024 8
The selection was made among 30 houses that were clustered moreefficientuseofresourcesandmakesitpossibletousethe
together with all methods and obtained a high score in each. algorithm in a distributed setting, where the number of agents
More specifically, in this case, we use a typical non- can be large. To demonstrate the scalability of the proposed
Independently Identically Distributed (non-IID) dataset with algorithm, we first analyze the converge steps in the slowest
a large group of agents. It is not realistic and practical to DistributedLearningwithRingTopology(DRING).Then,we
directly apply FL to the raw dataset. Therefore, we performed conducted an experiment to work with up to 200 distributed
theK-meansalgorithmtoclusterthedatasetintosmallgroups learning agents and 3 different communication topologies
[54]. The clustering result can be seen as Fig. 6. (DMS, DFC [38] and DRING [55]). The experimental results
are shown in Fig. 7.
Cluster Distribution for K-Means
1600
1400 25000 0.60 DMS
20000
1200 0.55
15000
1000 10000 0.50
800 5000 0.45
0 0.40
6 40 00 0 10 N u m 2 b0 e r o f3 A0 g e 4 n0 ts 50 0 C o1 n0 s e n s u s S 2 te0 p s 30
(a) Convergence Performance. (b) DMS Performance.
200
0 0.60 DFC 0.8 DRING 1 2 3 4 5 6 7 8 0.7
Clusters 0.55
0.6
0.50 0.5 Fig. 6: The clustering result with K-means. 0.45 0.4
0.3
0.40
0.2
As we can see from Fig. 6, Cluster 1 contains most of 0 C o n1 s0 e n s u s S t e p 2 s0 30 0 10 C0 o0 n s e n s u s 2 S0 t0 e0 p s 3000
the agents. It indicates this cluster represents the most com-
(c) DFC Performance. (d) DRING Performance.
mon consumption behavior. Therefore,we randomly select 30
Fig. 7: Scalability of different decentralized learning topologies.
agents from Cluster to perform our further study in this
1
paper.
The experiment was designed to evaluate the effect of
increasing the number of agents on the algorithm’s conver-
B. Experiment Setup
gence performance. Figure 7(a) shows that the convergence
The evaluation is based on simulations. We programmed performance increases almost linearly with the number of
the forecasting code in Python and based on the machine agents under the DRING topology, while the other three
learning framework provided by FLOWER2(flwr), where the figures show the convergence of a 200-agent system with dif-
neural network models are written in PyTorch3. ferenttopologies.Thisindicatesthattheproposedalgorithmis
For the secure aggregation experiments, we use four ma- scalable and can easily adapt to large-scale federated learning
chines with an Intel i-9900 CPU and 128GB of RAM, and systems.Actually,boththecommunicationandcomputational
four machines with an Intel Core i7-770 CPU and 32GB of complexity of an agent do not depend on the set of agents,
RAM. For experiments with up to 4 MPC parties, only the but rather the set of neighbors. This means that as long as
firstgroupofmachinesisused(onemachineperMPCparty). the number of neighbors is kept unchanged, the algorithm
For experiments with more than 8 parties, each machine runs can scale to as many agents as possible. This is a significant
more than one party. The ping time between the machines is advantage over other federated learning algorithms that have
1.003 ms. a high computational and communication complexity with
respect to the number of agents. From the simulation results,
it can be observed that due to the random communication
C. Experiment Results and Analysis
approach, the DMS topology significantly improves conver-
Inthissection,weevaluatetheperformanceoftheproposed
gence speed compared to the DRING topology, and achieves
algorithm by assessing it against four key aspects: scalability,
convergence speed almost equivalent to that of the DFC
privacy,accuracy,andcomplexity.Byevaluatingthealgorithm
topology. Theoretically, the DMS topology reduces 50% of
from these four key perspectives, we can ensure that it meets
the communication edges compared to the DFC topology.
the standards required for practical use.
b) Privacy: For secure aggregation, we used an MPC
a) Scalability: We measure the algorithm’s ability to
protocolbasedonShamirsecretsharingwhichremainssecure
handle increasing amounts of data and agents, ensuring that
as long as the majority of the parties are honest (i.e., when
it can scale to meet the needs of large-scale applications. The
there are three parties, at most one can be malicious). If this
evaluation method for scalability is the convergence speed.
is satisfied and a dishonest party deviates from the protocol,
The proposed algorithm can handle a large number of agents
the honest parties will detect it and abort the protocol with
with a relatively small number of neighbors. This allows for
probability 1 − 1/p, where p is the order of the field. We
2https://flower.dev/ choose p such that log 2p=128, so that dishonest behavior is
3https://pytorch.org/ caught with overwhelming probability.
eziS
sthgieW
lamitpO
)GNIRD(
spetS
sthgieW
lamitpO
sthgieW
lamitpOIEEEINTERNETOFTHINGSJOURNAL,VOL.XX,NO.X,FEBRUARY2024 9
We implemented secure aggregation using SCALE- which in turn presents lower complexities than other previous
MAMBA and performed experiments for different commu- works (as is shown in [24, Table 2]).
nication topologies. For the FedAvg topology, we consider 3
external MPC parties, receiving secret shared gradients from TABLEII:Computationandcommunicationcomplexitypertraining
iterationofthedifferenttopologiescomparedwithSAFELearn[21].
the 30 agents in every round. For the DRING topology, each
misthelengthofthemodelupdatesandnisthenumberofclients.
agent acts as an MPC party and performs the secure aggrega-
tion protocol together with the two connected neighbors. For Computation Communication
Approach
the DFC topology, there are 30 MPC parties played by the Server Client Server Client
agents themselves, who secretly share their gradients with all SAFELearn [21] O(mn) O(m) O(mn) O(m)
other29agentsineveryround.IntheDMStopology,theMPC
FedAvg O(mn) O(m) O(mn) O(m)
parties are played by the subset of agents chosen to aggregate
DRING - O(m) - O(m)
their gradients in that round.
DFC - O(mn) - O(mn)
The runtimes obtained for one round of the different
DMS - O(m) - O(m)
topologies are summarized in Table I. DFC clearly results
TABLEI:Runtimesinsecondsforcomputingsecureaggregationof The setting of the protocol in [21] is similar to the FedAvg
gradients for one round of training. topology, hence the identical complexities. With the DFC
topology, the absence of a distributed central server results
DMS
FedAvg DRING DFC
3parties 4parties 5parties in high communication and computation complexities for the
1.08s 0.16s 488.41s 0.16s 0.25s 0.39s clients,whichcanbecomeunpracticalasthenumberofclients
increases. The DRING and DMS topology allow removing
in the most expensive secure aggregation computation, taking the central server while maintaining low complexities on the
488.41s(≈8min). This was to be expected, since using more clients’ side. Note that the number of MPC parties also
MPC parties quickly increases the communication overhead. influences the complexities but is not considered in this table.
On the other hand, for this experiment, we ran more than one This is because in [21], FedAvg and DRING the number of
party on a single machine, with some machines running up parties is small and constant throughout the protocol. In the
to six parties, which also contributed to the slow runtime. In DMS topology, as the complexity analysis in Section III-C,
FedAvg,thefactthatthereareonly3MPCpartiessignificantly the worst-case scenario for DMS involves the number of
improves the performance. However, DRING and DMS allow interacting clients in each round equal to the total number
evenfastercomputationtimes.InDRING,eachgroupofthree of clients. When using DFC, the MPC parties are the clients,
neighboring agents is aggregating 3 gradients in each round and hence their impact is already accounted for.
(instead of the 3 MPC parties aggregating 30 gradients in Fe- c) Robustness to attacks: In this section, we conducted
dAvg).Ifallthegroupsof3agentsperformtheircomputations experiments to evaluate the robustness of our proposed
in parallel, a runtime of 0.16s is achieved. Regarding DMS, method, DMS, to two different attack algorithms, poisoning
thenumberofagentsaggregatingtheirgradientsineachround attacks, and DLG. With the poisoning attack scenario, we
can be adjusted according to the desired runtimes. Naturally, arbitrarily and randomly attacked 10% of the agents (3 out of
increasingthisnumberwillalsoresultinasloweraggregation, 30) and attackers introduced a poisoning attack by injecting a
especially since the number of MPC parties running the deliberate malicious perturbation of 0.2 into their broadcasted
protocolwillincrease.Nonetheless,aggregationwith5parties model weights during each training round. We monitored the
in DMS still achieves runtimes more than twice as fast as resulting mismatch between the model’s parameters and the
FedAvg. optimal value throughout the training process. The outcomes
Remark 4: Due to a limitation of the SCALE-MAMBA of these experiments are presented in Fig. 8, which illustrates
software, each gradient to be aggregated must initially be a comprehensive comparison between our DMS approach and
entered as three separate values (one for the sign and two for FedAvg.
the value itself). These values are then put together to obtain From the observations in Fig. 8, it is evident that our
the original gradient value before performing the aggregation DMS exhibits superior robustness compared to the FedAvg
itself.However,thismeansthatweneedtosecretshare3times algorithm under poisoning attacks. Even under a sustained
asmanyvalues,resultinginruntimes2to3timesslowerthan onslaught of over 1000 rounds of attacks, our method dis-
if only the value itself was secretly shared. The runtimes in played only a 9% increase in error, unlike FedAvg. This
Table I assume that the gradients can be read as one single robustness, which refers to maintaining accuracy under at-
secret value. tack, can be attributed to our novel approach that adapts to
Due to the use of different network topologies, models and changingcommunicationtopologies.Inourimplementationof
experimental setups, comparing the numerical benchmarks in the Markovian switching mechanism, only a subset of agents
Table I with those of previous related work is not straightfor- participate in weight updates each round, hindering attackers
ward.Instead,welookatthecomputationandcommunication from consistently propagating their malicious weights. This
complexities for performing secure aggregation for the com- inconsistency significantly enhances the system’s robustness
munication topologies considered in this work. We compare againstadversarialinfluences.Theseresultsunderscoretheef-
them to corresponding complexities for the protocol in [21], fectivenessofourDMSmethodindefendingagainstpoisoningIEEEINTERNETOFTHINGSJOURNAL,VOL.XX,NO.X,FEBRUARY2024 10
Fig. 8: Weights update under attacks.
Fig. 10: Comparison between recovered and real converted Load
under FedAvg setting.
not unified initialized. Second, the communicating clients in
each round of training are randomly selected. Therefore, it
is way more difficult for the attacker to hijack weights in
different stages for a specific client.
d) Accuracy: Here, we measure the accuracy of the al-
gorithm’s predictions. The evaluation metric we used is MSE.
We conduct an experiment to forecast short-term residential
load using four different state-of-the-art models: DNN, CNN,
LSTM, and WaveNet. For each model, we implement four
differentalgorithms:FedAvg,DRING,DFCandDMS.Firstly,
the forecasting results of three randomly selected residentials
are shown in Fig. 11, for the full forecasting results, please
Fig. 9: Recovered data from DLG attack.
refer to our GitHub4.
attacks, suggesting its potential to significantly enhance the
security of learning systems.
The second scenario is under the attack of DLG. DLG is
a strong attack against gradients-sharing-based NN training.
Once it hijacks the real gradients, the attacker can recover the
original training set. In this experiment, an attacker randomly
selects a communication line and gains continuous access to
the transmitted information. By doing so, the attacker can
obtain the change in weights between two communications,
effectively capturing gradient information. This data is then
used to recover load data. For benchmarking purposes, we
compare our results against the FedAvg algorithm in this
experiment. The experiment results are shown in Fig. 9.
Fig. 9 shows the recovered data by employing the DLG
algorithm. It can be seen that our DMS algorithm performs
better than the FedAvg algorithm in terms of protecting the
information leakage from the gradient. In FedAvg, the central
server initializes and broadcasts the global model to clients.
Therefore, each client holds an identical NN model with
Fig. 11: Forecasting performance of different algorithms on DNN
the same randomly initialized weights. Meanwhile, for every
models.
roundoftraining,eachclientneedstosharetheirupdateswith
the central server. In this case, the attacker can succeed if
FromFig.11,wecanseethattheproposedDMSalgorithm
they have access to different stages of weights, for example,
performswellandisabletocapturethetrendsandfluctuations
the initialized weights and any weights from later updates.
in the actual electricity consumption values quite well. It is
The comparison of recovered data under FedAvg setting can
slightly better than other algorithms in most time slots. It is
be clearly seen as 10. By contrast, the attacker faces more
difficulties in the DMS setting. First, the clients’ models are 4https://github.com/YingjieWangTony/FL-DL.gitIEEEINTERNETOFTHINGSJOURNAL,VOL.XX,NO.X,FEBRUARY2024 11
noticed that there are some deviations between the actual and evenafter100trainingepochs.Thesefluctuationsareprimarily
every predicted peak value in Fig. 11. That is largely due to caused by the heterogeneity in local data distributions. Each
the loss function, MSE. Using MSE in regression problems node trains on a distinct dataset, leading to models that
is common, but it has some limitations when the data is are finely tuned to their specific data characteristics. When
imbalanced. First, MSE treats all errors equally, regardless if these models are aggregated, as in Federated Learning, the
their values are high or low. In other words, MSE will not process attempts to reconcile these divergent updates, caus-
prioritize peak value accuracy. It leads to an underestimation ing fluctuations in the global model. This phenomenon is
ofpeakvalues.Second,whenthedataisimbalanced,thepeak exacerbated in environments where the local datasets are not
values are often considered as outliers during optimization. independently and identically distributed. While theoretically,
While MSE is sensitive to outliers, it aims to minimize the a centralized training approach using a powerful computing
error across all samples uniformly. This can be problematic center and all available data would yield an optimal model,
whenpeaksareoutliersbutareveryimportant.Thirdly,global practical constraints like geographical limitations and privacy
optimization could be a reason, too. The optimization process concerns make this approach infeasible. Thus, distributed
aims to minimize the global error. Peak values are rare in training,despiteitsinherentfluctuations,becomesanecessary
normal consumption profiles, the optimization process may compromise, offering a balance between model performance
choose to “sacrifice” accuracy on these points in favor of andadherencetologisticalandprivacyconstraints.Overtime,
better global accuracy. We propose the following solutions to asmoreroundsofaggregationoccur,theglobalmodeltendsto
this situation, custom loss function, data resampling, feature stabilize, gradually adapting to the diversity of local updates.
engineering, and ensemble methods. These solutions could be
a good help towards peak value prediction. However, this is V. CONCLUSION
beyond the current scope of the paper.
In this paper, we developed a Markovian Switching dis-
tributed learning framework for residential short-term load
forecasting. Moreover, a secure aggregation approach, MPC
has been employed to address the threat of deep leakage from
the gradient. We analyzed DMS from several perspectives,
such as accuracy, scalability, complexity and privacy. The
DMSiscomparedwithtraditionalcentralized,FL,DRINGand
DFC models. The simulation shows that the DMS model not
onlysecuresresidential-userprivacybutalsoshowsequivalent
orevensuperioraccuracythantheothermodels.Particularly,it
significantly reduces computational complexity and enhances
scalability compared to FL models.
REFERENCES
Fig. 12: Training performance of different algorithms on DNN [1] O. Abedinia, N. Amjady, and H. Zareipour, “A new feature selection
models. techniqueforloadandpriceforecastofelectricalpowersystems,”IEEE
Trans.PowerSyst.,vol.32,no.1,pp.62–74,2016.
[2] M.Ali,M.Adnan,andM.Tariq,“Optimumcontrolstrategiesforshort
In Fig. 12, we show the training performance among four termloadforecastinginsmartgrids,”Int.J.Electr.PowerEnergySyst.,
different algorithms. Mean squared error is applied to eval- vol.113,pp.792–806,2019.
[3] W.Kong,Z.Y.Dong,Y.Jia,D.J.Hill,Y.Xu,andY.Zhang,“Short-term
uate the performance of the models. To further illustrate the
residentialloadforecastingbasedonLSTMrecurrentneuralnetwork,”
comparisonresultsamongdifferentmodelsandalgorithms,we IEEETrans.SmartGrid,vol.10,no.1,pp.841–851,2017.
summarizetheoverallmeansquareerrorvaluesforeachmodel [4] W.Lin,D.Wu,andB.Boulet,“Spatial-temporalresidentialshort-term
load forecasting via graph neural networks,” IEEE Trans. Smart Grid,
and algorithm on the testing dataset, as shown in Table. III.
vol.12,no.6,pp.5373–5384,2021.
[5] J. S. Nightingale, Y. Wang, F. Zobiri, and M. A. Mustafa, “Effect
TABLE III: Mean square error (kWh) of different algorithms and of clustering in federated learning on non-iid electricity consumption
models. prediction,”in2022IEEEPESInnov.SmartGridTechnol.Conf.Eur.,
2022.
DNN CNN LSTM WAVENET
[6] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
FedAvg 0.05263 0.06495 0.0537 0.4965 Challenges,methods,andfuturedirections,”IEEEsignalprocess.mag.,
DRING 0.05289 0.06498 0.0556 0.3659 vol.37,no.3,pp.50–60,2020.
DFC 0.05305 0.06493 0.0552 0.5043 [7] N.GholizadehandP.Musilek,“Federatedlearningwithhyperparameter-
DMS 0.05236 0.06388 0.0550 0.5156 basedclusteringforelectricalloadforecasting,”InternetThings,vol.17,
p.100470,2022.
From the results given in Fig. 12 and Table III, the [8] J. D. Ferna´ndez, S. P. Menci, C. M. Lee, A. Rieger, and G. Fridgen,
“Privacy-preserving federated learning for residential short-term load
proposed DMS algorithm meets the requirements and has
forecasting,”Appl.Energy,vol.326,p.119915,2022.
better accuracy than other models. For the dataset used in [9] Y.Yang,Z.Wang,S.Zhao,andJ.Wu,“Anintegratedfederatedlearning
this case, the DMS algorithm proposed obtains better per- algorithmforshort-termloadforecasting,”Electr.PowerSyst.Res.,vol.
214,p.108830,2023.
formance and fewer prediction errors in most cases. It is
[10] A.Z.Tan,H.Yu,L.Cui,andQ.Yang,“Towardspersonalizedfederated
noticed that some fluctuations during the training process, learning,”IEEETrans.NeuralNetw.Learn.Syst.,2022.IEEEINTERNETOFTHINGSJOURNAL,VOL.XX,NO.X,FEBRUARY2024 12
[11] J. Lin, J. Ma, and J. Zhu, “A privacy-preserving federated learning [32] X.Cao,M.Fang,J.Liu,andN.Z.Gong,“Fltrust:Byzantine-robustfed-
method for probabilistic community-level behind-the-meter solar gen- eratedlearningviatrustbootstrapping,”arXivprepr.arXiv:2012,13995,
eration disaggregation,” IEEE Trans. Smart Grid, vol. 13, no. 1, pp. 2020.
268–279,2021. [33] Z. Ding, “Consensus output regulation of a class of heterogeneous
[12] J. Gao, W. Wang, Z. Liu, M. F. R. M. Billah, and B. Campbell, nonlinear systems,” IEEE Trans. Autom. Control, vol. 58, no. 10, pp.
“Decentralized federated learning framework for the neighborhood: a 2648–2653,2013.
casestudyonresidentialbuildingloadforecasting,”inProc.19thACM [34] T.ZhaoandZ.Ding,“Distributedinitialization-freecost-optimalcharg-
conf.embed.networkedsens.syst.,2021,pp.453–459. ingcontrolofplug-inelectricvehiclesfordemandmanagement,”IEEE
[13] M.SaviandF.Olivadese,“Short-termenergyconsumptionforecasting Trans.Ind.Inform.,vol.13,no.6,pp.2791–2801,2017.
at the edge: A federated learning approach,” IEEE Access, vol. 9, pp. [35] Y. Dong, T. Zhao, and Z. Ding, “Demand-side management using a
95949–95969,2021. distributed initialisation-free optimisation in a smart grid,” IET renew.
[14] M. N. Fekri, K. Grolinger, and S. Mir, “Distributed load forecasting powergener.,vol.13,no.9,pp.1533–1543,2019.
using smart meter data: Federated learning with Recurrent Neural [36] Y.Wang,L.Cheng,W.Ren,Z.-G.Hou,andM.Tan,“Seekingconsensus
Networks,”Int.J.Electr.PowerEnergySyst.,vol.137,p.107669,2022. in networks of linear agents: Communication noises and Markovian
[15] Y.Wang,A.Zhang,S.Wu,andS.Yu,“VOSA:Verifiableandoblivious switchingtopologies,”IEEETrans.Autom.Control,vol.60,no.5,pp.
secure aggregation for privacy-preserving federated learning,” IEEE 1374–1379,2014.
Trans.DependableSecureComput.,pp.1–17,2022. [37] W. Ren and R. W. Beard, Distributed Consensus in Multi-Vehicle
[16] M.Asad,A.Moustafa,T.Ito,andM.Aslam,“Evaluatingthecommu- CooperativeControl. Springer,2008,vol.27.
nicationefficiencyinfederatedlearningalgorithms,”in2021IEEE24th [38] Z. Li, B. Liu, and Z. Ding, “Consensus-based cooperative algorithms
int.conf.comput.support.coop.workdes.,2021,pp.552–557. fortrainingoverdistributeddatasetsusingstochasticgradients,”IEEE
[17] L.Zhu,Z.Liu,andS.Han,“Deepleakagefromgradients,”Adv.neural Trans.NeuralNetw.Learn.Syst.,2021.
inf.process.syst.,vol.32,2019. [39] A. Aly, M. Keller, E. Orsini, D. Rotaru, P. Scholl, N. P. Smart, and
T.Wood,“SCALEandMAMBAdocumentation,”2018.
[18] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan,
[40] J. Bo¨hler and F. Kerschbaum, “Secure multi-party computation of
S.Patel,D.Ramage,A.Segal,andK.Seth,“Practicalsecureaggregation
forprivacy-preservingmachinelearning,”inProc.2017ACMSIGSAC differentially private heavy hitters,” in Proc. 2021 ACM SIGSAC
conf. comput. commun. secur., ser. CCS ’17. New York, NY, USA: conf. comput. commun. secur., ser. CCS ’21. New York, NY, USA:
Association for Computing Machinery, 2021, p. 2361–2377. [Online].
AssociationforComputingMachinery,2017,p.1175–1191.
Available:https://doi.org/10.1145/3460120.3484557
[19] S.Truex,N.Baracaldo,A.Anwar,T.Steinke,H.Ludwig,R.Zhang,and
[41] L.BottouandO.Bousquet,“Thetradeoffsoflargescalelearning,”Adv.
Y.Zhou,“Ahybridapproachtoprivacy-preservingfederatedlearning,”
neuralinf.process.syst.,vol.20,2007.
inProc.12thACMworkshopartif.intell.secur.,ser.AISec’19. New
[42] H. Tang, X. Lian, M. Yan, C. Zhang, and J. Liu, “d2: Decentralized
York,NY,USA:AssociationforComputingMachinery,2019,p.1–11.
trainingoverdecentralizeddata,”inInt.Conf.Mach.Learn.,2018,pp.
[20] D. Byrd and A. Polychroniadou, “Differentially private secure multi-
4848–4856.
party computation for federated learning in financial applications,” in
[43] D. Saad, “Online algorithms and stochastic approximations,” Online
Proc.firstACMint.conf.AIfinance,ser.ICAIF’20. NewYork,NY,
Learn.,vol.5,no.3,p.6,1998.
USA:AssociationforComputingMachinery,2021.
[44] P.J.BickelandK.A.Doksum,Mathematicalstatistics:basicideasand
[21] H. Fereidooni, S. Marchal, M. Miettinen, A. Mirhoseini, H. Mol-
selectedtopics,volumesI-IIpackage. CRCPress,2015.
lering, T. Nguyen, P. Rieger, A. Sadeghi, T. Schneider, H. Yalame,
[45] A.NedicandA.Ozdaglar,“Distributedsubgradientmethodsformulti-
and S. Zeitouni, “Safelearn: Secure aggregation for private federated
agent optimization,” IEEE Trans. Autom. Control, vol. 54, no. 1, pp.
learning,” in 2021 IEEE secur. priv. workshops. Los Alamitos, CA,
48–61,2009.
USA:IEEEComputerSociety,may2021,pp.56–62.
[46] Z.Deng,S.Liang,andY.Hong,“Distributedcontinuous-timealgorithms
[22] J. H. Bell, K. A. Bonawitz, A. Gasco´n, T. Lepoint, and M. Raykova,
forresourceallocationproblemsoverweight-balanceddigraphs,”IEEE
“Secure single-server aggregation with (poly)logarithmic overhead,” in
trans.cybern.,vol.48,no.11,pp.3116–3125,2017.
Proc.2020ACMSIGSACconf.comput.commun.secur.,ser.CCS’20.
[47] Y. Zhu, W. Yu, G. Wen, G. Chen, and W. Ren, “Continuous-time
New York, NY, USA: Association for Computing Machinery, 2020,
distributedsubgradient algorithmfor convexoptimization withgeneral
p. 1253–1269. [Online]. Available: https://doi.org/10.1145/3372297.
constraints,”IEEETrans.Autom.Control,vol.64,no.4,pp.1694–1701,
3417885
2018.
[23] J. So, B. Guler, and A. Avestimehr, “Turbo-aggregate: Breaking the
[48] J. C. Duchi, A. Agarwal, and M. J. Wainwright, “Dual averaging for
quadraticaggregationbarrierinsecurefederatedlearning,”IEEEJ.Sel.
distributed optimization: Convergence analysis and network scaling,”
AreasInf.Theory,vol.PP,pp.1–1,Jan.2021.
IEEETrans.Autom.control,vol.57,no.3,pp.592–606,2011.
[24] T. Gehlhar, F. Marx, T. Schneider, A. Suresh, T. Wehrle, and [49] W. Rudin et al., Principles of Mathematical Analysis. McGraw-hill
H. Yalame, “Safefl: Mpc-friendly framework for private and robust
NewYork,1976,vol.3.
federated learning,” in 2023 IEEE secur. priv. workshops. Los
[50] H. S. Hippert, C. E. Pedreira, and R. C. Souza, “Neural networks
Alamitos, CA, USA: IEEE Computer Society, may 2023, pp. forshort-termloadforecasting:Areviewandevaluation,”IEEETrans.
69–76.[Online].Available:https://doi.ieeecomputersociety.org/10.1109/ powersyst.,vol.16,no.1,pp.44–55,2001.
SPW59333.2023.00012
[51] C.-L. Liu, W.-H. Hsaio, and Y.-C. Tu, “Time series classification with
[25] V.Tolpegin,S.Truex,M.E.Gursoy,andL.Liu,“Datapoisoningattacks multivariate convolutional neural network,” IEEE Trans. ind. electron.,
againstfederatedlearningsystems,”in25theur.symp.res.comput.secur. vol.66,no.6,pp.4788–4797,2018.
Springer,2020,pp.480–501. [52] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,
[26] G.F.Cretu,A.Stavrou,M.E.Locasto,S.J.Stolfo,andA.D.Keromytis, A.Graves,N.Kalchbrenner,A.Senior,andK.Kavukcuoglu,“Wavenet:
“Castingoutdemons:Sanitizingtrainingdataforanomalysensors,”in A generative model for raw audio,” arXiv prepr. arXiv:1609,03499,
2008IEEESymp.Secur.Priv. IEEE,2008,pp.81–95. 2016.
[27] J.Feng,H.Xu,S.Mannor,andS.Yan,“Robustlogisticregressionand [53] I. Commission for Energy Regulation (CER), “CER smart metering
classification,”Adv.neuralinf.process.syst.,vol.27,2014. project-gascustomerbehaviourTrial,2009-2010,”2012.
[28] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li, [54] Y. Mansour, M. Mohri, J. Ro, and A. T. Suresh, “Three approaches
“Manipulatingmachinelearning:Poisoningattacksandcountermeasures forpersonalizationwithapplicationstofederatedlearning,”arXivprepr.
forregressionlearning,”in2018IEEEsymp.secur.priv. IEEE,2018, arXiv:2002,10619,2020.
pp.19–35. [55] T. Zhao, Z. Li, and Z. Ding, “Consensus-based distributed optimal
[29] J.Steinhardt,P.W.W.Koh,andP.S.Liang,“Certifieddefensesfordata energy management with less communication in a microgrid,” IEEE
poisoningattacks,”Adv.neuralinf.process.syst.,vol.30,2017. Trans,Ind.Inform.,vol.15,no.6,pp.3356–3367,2018.
[30] M.Fang,X.Cao,J.Jia,andN.Gong,“Localmodelpoisoningattacksto
{Byzantine-Robust}federatedlearning,”in29thUSENIXsecur.symp.,
2020,pp.1605–1622.
[31] Y.Mao,X.Yuan,X.Zhao,andS.Zhong,“Romoa:Robustmodela
ggregation for the resistance of federated learning to model poisoning
attacks,” in 26th eur. symp. res. comput. secur. Springer, 2021, pp.
476–496.