KB-Plugin: A Plug-and-play Framework for Large Language Models to
Induce Programs over Low-resourced Knowledge Bases
JiajieZhang1 ,ShulinCao1,LinmeiHu2,LingFeng1,LeiHou1*,JuanziLi1
1TsinghuaUniversity 2BeijingInstituteofTechnology
zhangjj23@mails.tsinghua.edu.cn
Abstract
Programinduction(PI)hasbecomeapromis- ùëö& #$ Q Le: B W roh no Ji as m ta el sle r, ùëö #% $ Q ra: il wSe am y a lip nh eo ir se on ùëö #‚Äô $ Q co: u C nti t oa fti Yon u ta
Jr. or his father? the rail network Saito at Cornell
ingparadigmforusingknowledgebases(KBs) named what? University
to help large language models (LLMs) an-
swercomplexknowledge-intensivequestions. ùëö #( $ Schema Plugin
LLM
Nonetheless, PI typically relies on a large ùëö Program Induction
!"
numberofparallelquestion-programpairsto Plugin
make the LLM aware of the schema of the
Program: Find(Cornell
Program: Find (LeBron Program: Find (Semaphore
given KB, and is thus challenging for many James Jr.) Find (LeBron railway line) Relate(part of University)
ReverseRelate(organization)
James Jr.) Relate (father) network) FilterConcept(rail
low-resourced KBs that lack annotated data. Or() Argmax(height) network) Find(Yuta Saito) And()
Relate(citation count)
To this end, we propose KB-Plugin, a plug- Answer: LeBron James Answer: TransAdelaide Answer: 464
and-playframeworkthatenablesLLMstoin-
Figure1:IllustrationofKB-Plugin.Bysimplyplugging
duce programs over any low-resourced KB.
theschemapluginofaKBandthePIplugin,theLLM
Firstly,KB-Pluginadoptsself-supervisedlearn-
isinjectedwiththeschemainformationofthisKBand
ing to encode the detailed schema informa-
theabilitytoinduceprogramsoverit.
tion of a given KB into a pluggable module,
namelyschemaplugin. Secondly,KB-Plugin
utilizes abundant annotated data from a rich-
resourcedKBtotrainanotherpluggablemod- andcapacitytosupportcomplexreasoningopera-
ule, namely PI plugin, which can help the tions(Caoetal.,2022a;Guetal.,2023;Lietal.,
LLM extract question-relevant schema infor- 2023b). GivenaKB,PImethodsemployLLMsto
mation from the schema plugin of any KB
convertaquestionintoamulti-stepprogram(e.g.,
andutilizethisinformationtoinduceprograms
KoPL (Cao et al., 2022a) and S-expression (Su
over this KB. Experiments on five heteroge-
etal.,2016)),whoseexecutionagainsttheKBpro-
neous KBQA datasets show that KB-Plugin
achieves better or comparable performance ducestheanswer. Despitestrongcapacity,mostPI
with25√ósmallerbackboneLLMcomparedto methodsrelyonindividualtrainingforeachKBus-
SoTAPImethodsforlow-resourcedKBs,and ingalargenumberofmanuallyannotatedquestion-
evenapproachestheperformanceofsupervised program pairs (Xie et al., 2022; Li et al., 2023b;
methods. Our code and data are available at
Luoetal.,2023). Asformanylow-resourcedKBs
https://github.com/THU-KEG/KB-Plugin.
thatlackprogramannotations,howtoenableLLMs
toutilizetheirknowledgeviaPIremainsachalleng-
1 Introduction ingproblem.
Recent studies (Cao et al., 2022b; Li et al.,
Recently,theusageofknowledgebases(KBs)as
2023a)haveindicatedthatthemappingfromques-
externalresourcestoassistlargelanguagemodels
tions to program sketches (i.e., composed func-
(LLMs)(Brownetal.,2020;Zhaoetal.,2023)in
tionswithoutarguments,suchasFind‚ÜíRelate‚Üí
answeringcomplexknowledge-intensivequestions
FilterConcept) primarily correlates with lan-
has gained increasing study (Pan et al., 2023; Li
guagecompositionalstructuresandisthustransfer-
et al., 2023b; Jiang et al., 2023). Among various
ableacrossKBs. HencethemainchallengeforPI
methods,programinduction(PI)hasemergedasa
overlow-resourcedKBsistodeterminetheargu-
promisingparadigmduetoitsgoodinterpretability
mentforeachfunction(GuandSu,2022),whichre-
* Correspondingauthor. quiresLLMstolinknaturallanguageinaquestion
1
4202
beF
2
]LC.sc[
1v91610.2042:viXra
ùëö
"!to corresponding schema items (i.e., pre-defined plugin for each of them. Given a training ques-
relations and concepts) in the KB (e.g., in Fig 1, tion,weplugtheseschemapluginsalonewiththe
the relation ‚Äúpart of network‚Äù and the concept PI plugin into the LLM in turn and train the PI
‚Äúrail network‚Äù are arguments of function Relate plugintomaketheLLMgeneratethecorrectpro-
andFilterConcept,respectively),soitisimpor- gram whose arguments conform to the currently
tanttoprovideLLMsadequateinformationofeach pluggedschemaplugin. Inthisway,thePIplugin
schemaitem. Astraightforwardapproachistodi- isforcedtolearntheskillsofextractingandutiliz-
rectlyfeedalltheschemainformationtotheLLM ingquestion-relevantschemainformationfromthe
viaaprompt. However,thebroadschemaofKBs pluggedschemapluginforPIoverthecorrespond-
and limitedcontextwindowsof LLMs make this ingKB.Besides,sincethePIpluginistrainedtobe
infeasible(Lietal.,2023a). compatiblewithdifferentschemaplugins,itcanbe
Regardingtheabovechallenges,weareinspired directlytransferredtootherlow-resourcedKBsand
byrecentstudiesthatclaimthattheparametersof generalizewellwiththeirschemaplugins,evenif
LLMs can encode task-specific knowledge (Sax- mostschemaitemsintheseKBsareunseenduring
enaetal.,2022;Moiseevetal.,2022;Wangetal., itstraining.
2022). Our basic idea is to encode schema infor- In experiments, we take Wikidata-based KQA
mationofaKBintotheparametersofapluggable Pro as the rich-resourced KB to train the PI plu-
module (e.g., LoRA (Hu et al., 2022)), namely ginandevaluateourframeworkonthreeFreebase-
schema plugin, and use another pluggable mod- based datasets (WebQSP, GraphQ, and GrailQA)
ule, namely PI plugin, to help the LLM capture and two domain-specific datasets (MetaQA for
question-relevant schema information from the movie domain and SoAyBench for academic do-
schema plugin and utilize this information to in- main). TheresultsshowthatKB-Pluginachieves
duce programs. As illustrated in Fig. 1, by sim- better or comparable performance with 25√ó
ply plugging the schema plugin of a KB and the smallerbackboneLLMcomparedtoSoTAPImeth-
PI plugin, the LLM is injected with the schema odsforlow-resourceKBs. OnGraphQ,GrailQA,
information of this KB and the ability to induce andMetaQA,KB-Pluginevensurpassestheperfor-
programs over it. We name this framework KB- manceofseveralsupervisedmethods.
Plugin. To implement KB-Plugin, there remain Ourcontributionsinclude: (1)proposingKB-
twokeyproblems: (1)Bywhattaskcansufficient Plugin,anovelplug-and-playframeworkthaten-
information about each schema item in a KB be ables LLMs to induce programs over any low-
encoded into its schema plugin? (2) Without an- resourcedKB;(2)empiricalvalidationoftheeffi-
notateddatafromthelow-resourceKBs,howcan cacyofKB-Pluginthroughcomprehensiveexperi-
thePIpluginlearntoextractandutilizequestion- mentsonfiveheterogeneousKBQAdatasets.
relevant schema information from their schema
pluginstoinduceprogramsovertheseKBs? 2 RelatedWork
To address the above problems, we propose a
novelpluginlearningandtransferframework. First, Low-resourced Program Induction. Recently,
inspiredbypriorstudies(Bordesetal.,2013;Lin therehaveemergedthreetypesofPImethodsfor
et al., 2015) which show that schema items in a low-resourcedKBsthatlackprogramannotations,
KBcanbewellrepresentedbyfacttriplesinvolv- buteachofthemhaslimitations: (1)Few-shotpro-
ingthem,weproposetolearnschemapluginsvia gramgenerationmethods(Guetal.,2023;Lietal.,
a self-supervised triple completion task. Specifi- 2023a)utilizein-contextlearningabilityofLLMs
cally, given a KB, we plug a schema plugin into to induce programs with a handful of demonstra-
the LLM and tune the plugin to enable the LLM tions. However,theycanonlydeterminefunction
tocompleterelevanttriplesforeachschemaitem argumentsbasedontheschemaitemnamesdueto
intheKB.Inthisway,thedetailedschemainfor- limitedcontextwindows,sotheyfacechallenges
mation can be encoded into this schema plugin. indistinguishingsimilarschemaitems. Theyalso
As for PI plugin learning, inspired by Cao et al. suffer from long inference time due to excessive
(2022b),weutilizeabundantprogramannotations LLM calls or executing a vast number of poten-
from a rich-resourced KB. Specifically, we use tialprograms;(2)Few-shotdatagenerationmeth-
this KB to generate multiple KBs with different ods(Lietal.,2023c)alsoemployin-contextlearn-
schemasviaaliasreplacementandtrainaschema ingwithLLMstoconvertautomaticallysampled
2programs into questions, and train a smaller PI type of arguments, and can be serialized as y =
(cid:10) (cid:11)
modelusingthegeneratedquestion-programpairs. f (arg ),¬∑¬∑¬∑ ,f (arg ),¬∑¬∑¬∑ ,f (arg ) ,f ‚àà
1 1 t t |y| |y| t
Nonetheless,thegeneratedquestionsmaynotalign F,arg ‚àà E ‚à™C ‚à™R‚à™{‚àÖ}. Here, F is a set of
t
withprogramsandoftenlackdiversityduetothe pre-definedfunctionsthatcoverbasicreasoningop-
limited number of program templates; (3) Simi- erationsonKBs. Inthiswork,weuseKoPL(Cao
lar to us, program transfer methods (Cao et al., etal.,2022a)asourprogramminglanguage.
2022b)alsoleverageprogramannotationsfroma Task Formulation. Suppose we have access
rich-resourcedKBtoaidPIforlow-resourcedKBs. to (1) source KB KBS and source domain data
However, they mainly focus on program sketch DS = {(xS,yS)}nS ,whicharequestion-program
i i i=1
transfer and perform poorly without fine-tuning pairsforKBS; (2)targetKBKBT,whichislow-
using annotated question-answer pairs from low- resourcedandhasnoannotateddata. Thegoalisto
resourced KBs to adapt to their schemas. While learnaPImodelMT thatcantranslateaquestion
PI
KB-pluginobviatestherelianceonanyannotated xT forKBT intoprogramyT,whoseexecutionon
data from low-resourced KBs, thereby enabling KBT producesthecorrectanswer.
LLMstoeasilyutilizetheirknowledge.
4 Methodology
Plug-and-Play Modules for LLMs. In recent
years, various parameter-efficient modules have
Asmentionedintheintroduction,toenableaLLM
been proposed to adapt LLMs to different down- M to induce programs over low-resourced KBT,
stream tasks (Lester et al., 2021; Hu et al., 2022;
KB-Pluginlearnstwotypesofpluggablemodules
LiandLiang,2021;Pfeifferetal.,2021). These
forM: (1)KB-specificschemapluginm ,which
sc
modules show plug-and-play characteristics and
stores information of schema items of a given
caninjecttask-specificknowledgeandskillsinto
KBwithinitsparameters;(2)KB-transferablePI
LLMs(Xiaoetal.,2023;Zhangetal.,2023). Some
pluginm ,whichencodestheskillofinducing
PI
researchersalsofoundthatpluggablemodulesfor
programs over any KB by extracting and utiliz-
similartasksencodeknowledgeandskillsintothe
ingquestion-relevantschemainformationfromthe
parametricspaceinsimilarways(Qinetal.,2021; schemapluginofthisKB.ItistrainedwithKBS
Suetal.,2022),providingbasicrationalityforthe and DS but can be directly transferred to KBT.
transferabilityofourPIplugin. ThefinalPImodelforKBT canbeformulatedas
3 ProblemFormulation MT = plug(M,{mT ,m }), (1)
PI sc PI
In this section, we first provide some necessary where mT is the schema plugin of KBT and
sc
definitionsandthenformulateourtask. plug(M,{¬∑}) means plugging the plugins in {¬∑}
Knowledge Base. A knowledge base (KB) can intoM.
beformalizedasKB = {C,E,R,T},whereC,E, In the following, we will first introduce the ar-
R and T represent the sets of concepts, entities, chitectureoftwotypesofplugins,thenpresentour
relationsandfacttriples,respectively. Specifically, pluginlearningandtransferframework.
R = {r ,r }‚à™R ,wherer is‚Äúinstanceof‚Äù,r
e c l e c
is ‚Äúsubclass of‚Äù, and R is the set of other gen- 4.1 PluginArchitecture
l
eralrelations. Correspondingly,T canbedivided A host of studies have demonstrated that knowl-
intotheredisjointsubsets: (1)‚Äúinstanceof‚Äùtriples edgeandskillscanbeencapsulatedwithinthepa-
T e = {(e,r e,c)|e ‚àà E,c ‚àà C}; (2) ‚Äúsubclass of‚Äù rameters of LLMs (Saxena et al., 2022; Moiseev
triplesT c = {(c i,r c,c j)|c i,c j ‚àà C};(3)relational et al., 2022; Wang et al., 2022). Inspired by this,
triples T l = {(e i,r,e j)|e i,e j ‚àà E,r ‚àà R l}. Ele- we implement both schema plugin and PI plugin
mentsinC andRarealsocalledtheschemaitems with LoRA (Hu et al., 2022), a popular type of
ofKB. pluggablemoduleforLLMswithafewtrainable
Program Induction. Given a KB KB and a nat- parameters.
(cid:10) (cid:11)
ural language question x = w ,w ,¬∑¬∑¬∑ ,w , Specifically,letL bethesetofweightmatrices
1 2 |x| M
program induction (PI) aims to convert x into a intheself-attentionmodulesandMLPmodulesof
program y, which would return the correct an- a LLM M. For each W ‚àà Rd√ók in L , LoRA
i M
swer when executed against KB. Formally, y modifies its forward pass from h = W x to h =
i
is composed of functions that take a specific (W +A B )x,whereA ‚àà Rd√ór andB ‚àà Rr√ók
i i i i i
3ùêæùêµ# ùêæùêµ##
basketb ia nl hl s t ut ae mna cm ae n ofsub Lcl .a As .s Lo af kers sm pos etp m re to a sbr m e tt ers a o mf Alias Replacement bask ie nt s pc ta elu n rsb c oe n ofsub Lcl .a As .s Lo af kera s pt lh al ye st i fc o t ream bas bk aeL stb k.A a el t. l b L atea llak m te er as || m | c| oi ||n n ss t ut aa bin n cc s l ae i s no ssf t oa fnce ùëö!" ba L ss pk .A oe rt . tb L sa a tl el k at ee mra sm
sports team || contains subclass basketball team
instance ofLebron James instance ofLebron James Lebron James | human || member of sports team | forward LLM basketball team | L.A. Lakers
Program: Find(Lebron James) Program: Find(Lebron James) L.A. Lakers | basketball team || member of sports team | backward human | Lebron James
Relate(member of sports team) Relate(plays for) Lebron James | human || what relation || basketball team | L.A. Lakers member of sports team
FilterConcept(basketball team) FilterConcept(basket club)
(a) KB Generation and Data Augmentation (b) Learning of Schema Pluginvia Schema-relevant Triple Completion
(c) Learning of PI Plugin (d) Plugin Transfer
ùêæùêµ#! Q: Which basketball team
does Lebron James play for? F Fi in ltd e( rL Ce ob nr co en p tJ (a bm ase ks e) tR bae ll la tt ee a(m me ) m ber of sports team) Q: Se rm aila p nh eto wre o rr kai nlw amay e dli n we h i as t ?on the
ùêæùêµ##
ùêæùêµ# ùëö$% F Fi in ltd e( rL Ce ob nr co en p tJ (a bm ase ks e) tR ce lula bt )e (plays for) ùêæùêµ& ùëö$%
‚Ä¶ ‚Ä¶ LLM ‚Ä¶ LLM
ùêæùêµ#" Constrained Decoding
F Fi in ltd e( rL Ce ob nr co en p tJ (a bm ase ks e) tR bae ll la tt ee a(p mla ) yer of) F nein twd( oS re km ) Fap ilh teo rr Ce or nai cl ew pa t(y r ali in l e n) e tR we ola rkte )(part of
Figure2: Overviewofourpluginlearningandtransferframework: (a)GeneratemultiplesourceKBswithdifferent
schemasandaugmentedsourcedomaindataviaaliasreplacement;(b)Learnanindividualschemapluginforeach
sourceKBandthetargetKBviaself-supervisedschema-relevanttriplecompletiontask;(c)TrainthePIplugin
byinducingprogramforeachsourceKBwhenpluggingitintotheLLMalongwiththecorrespondingschema
plugin. (d)TransferthePIpluginbypluggingitintotheLLMwiththeschemapluginofthetargetKBandinducing
programsoverthetargetKBwithconstraineddecoding.
are two matrices with rank r ‚â™ min(d,k). A which is illustrated in Fig. 2 and contains
LoRApluginm isthusdefinedas four steps: (1) Generate multiple source KBs
j
m =
{(Amj,Bmj)|W
‚àà L }, (2)
KBS1,...,KBSN withdifferentschemasandaug-
j i i i M mented data DS = {(xS,yS1,...,ySN)}nS
a j j j j=1
and plug(M,{m 1,...,m N}) means re- based on KBS and DS via alias replacement,
placing all W i ‚àà L M with W i + where ySi is the golden program for question
(cid:80)N j=1Am i jB imj. If we train M‚Ä≤ = xS on Kj BSi; (2) Learn individual schema plugin
j
plug(fz(M),{fz(m ),...,fz(m ),m })
1 N‚àí1 N mSi for each KBSi via self-supervised schema-
onacertaintask,wherefz(¬∑)representsparameter sc
relevant triple-completion task; (3) Train PI plu-
freezing, knowledge and skills related to this
gin m by requiring MS1,...,MSN to gener-
taskwillbeencodedwithinm . Althoughother PI PI PI
N ate yS1,...,ySN given xS, respectively, where
parameter-efficient pluggable modules such as j j j
prefix-tuning(LiandLiang,2021)canalsoserve M PSi I = Plug(fz(M),{fz(mS sci),m PI}), so that
asourpluginmodules,theadvantagesofLoRAare m PI isforcedtoextractandutilizeschemainfor-
thatitdoesnotincreaseinputlengthorinference mation from each mS sci; (4) Learn schema plugin
latency. mT sc for KBT using the same method in (2) and
takeMT = plug(M,{mT ,m })asthefinalPI
PI sc PI
4.2 PluginLearningandTransferFramework model for KBT. We will introduce each step in
There are two primary challenges for learning detailinthefollowing.
schemapluginsandthePIplugin: (1)Howtoen-
4.2.1 KBGenerationandDataAugmentation
codesufficientinformationabouteachschemaitem
ofaKBintoaschemaplugin? (2)Howtoensure Weutilizethealiasesofeachschemaitemtogener-
that the PI plugin can extract and utilize useful atemultipleKBswithdifferentschemasbasedon
schema information for program induction from KBS = {CS,ES,RS,TS}. AsshowninFig.2(a),
schemapluginsofdifferentKBs,insteadofignor- foreachschemaitemv ‚àà CS ‚à™RS,wereplacev
ingtheschemapluginentirely,directlylearningto withv ,arandomlychosenaliasofv,andrecord
i
induce program over source KB during training, a (v) = v . Forexample,theconcept‚Äúbasketball
i i
andconsequentlylosingtransferability? team‚Äùcanbereplacedwith‚Äúbasketclub‚Äùandthe
To handle these challenges, we propose a relation‚Äúmemberofsportsteam‚Äùcanbereplaced
novel plugin learning and transfer framework, with ‚Äúplays for‚Äù. Relevant triples in TS are also
4
!#ùëö
##ùëö
"#ùëö
"!
"!
"!
"& !ùëömodified with the same alias. In this way, KBSi (e ,instance_of,c ),(e ,instance_of,c ) ‚àà T ,
i i j j e
thathasadifferentschemathanKBS iscreated. In and use each (e ,c ,r,e ,c ) to construct three
i i j j
practice, we let KBS1 = KBS and repeat above querieswithanswers:
processN‚àí1timestogenerateKBS2,...,KBSN.
‚Ä¢ ‚Äú‚ü®e ‚ü©|‚ü®c ‚ü©||‚ü®r‚ü©|forward‚Äù‚Üí‚Äú‚ü®c ‚ü©|‚ü®e ‚ü©‚Äù;
i i j j
Similarly, for each question-program
‚Ä¢ ‚Äú‚ü®e ‚ü©|‚ü®c ‚ü©||‚ü®r‚ü©|backward‚Äù‚Üí‚Äú‚ü®c ‚ü©|‚ü®e ‚ü©‚Äù;
j j i i
pair (xS,yS) ‚àà DS, suppose yS =
j j j ‚Ä¢ ‚Äú‚ü®e ‚ü© | ‚ü®c ‚ü© || what relation || ‚ü®c ‚ü© | ‚ü®e ‚ü©‚Äù ‚Üí
(cid:68) (cid:69) i i j j
f 1(arg 1),¬∑¬∑¬∑ ,f t(arg t),¬∑¬∑¬∑ ,f |yS|(arg |yS|) , ‚Äú‚ü®r‚ü©‚Äù.
j j
we replace every arg t ‚àà CS ‚à™RS with a i(arg t) We empirically find that including c i, c j benefits
to obtain ySi, which is the correct program for the information encoding for both concepts and
j
xS executable on KBSi. We repeat the process relations.
j
for KBS1,...,KBSN to obtain augmented data Letthesetofallgeneratedqueriesandanswers
DS = {(xS,yS1,...,ySN)}nS . beD sc = {(q i,a i)}l i=1,thenm sc istrainedtomin-
a j j j j=1
imize
4.2.2 LearningofSchemaPlugin (cid:88)
L = ‚àí logP(a |q ), (3)
sc i i
Manystudiesaboutknowledgegraphembedding
(qi,ai)‚ààDsc
showthattheinformationofschemaitemsinaKB
whereP(a |q )isthelikelihoodofM generating
canberepresentedbynotonlytheirnamesbutalso i i sc
a given q , defined by token-level cross entropy.
triples containing them (Bordes et al., 2013; Lv i i
Notethatthelearningofm doesnotrelyonany
et al., 2018). Inspired by this, we propose to en- sc
additionaldataexcepttheKBitself,sowecantrain
codeschemainformationintoschemapluginsvia
aschemapluginforanyKB.
aself-supervisedtriplecompletiontask. Asillus-
tratedinFig.2(b),tolearntheschemapluginm sc 4.2.3 LearningofPIPlugin
for a given KB KB = {C,E,R,T}, where T =
As illustrated in Fig. 2(c), to learn the PI plu-
T ‚à™T ‚à™T , we train M = Plug(fz(M),m )
e c l sc sc gin m , we first train individual schema plu-
PI
to complete relevant triples for each concept and gin mSi for each KBSi. After that, given
sc
relation in KB in sequence-to-sequence form as (xS,yS1,...,ySN) ‚àà DS, where xS is a ques-
j j j a i
follows.
tion and ySi is the golden program for xS
First, for each concept c ‚àà C, we require j j
on KBSi, we train m by feeding xS to
M sc to complete relevant ‚Äúinstance of‚Äù triples PI i
MS1,...,MSN and requiring them to gener-
to aggregate the semantic features of entities be- PI PI
longing to c. Specifically, we sample K triples ate yS1,...,ySN, respectively. Here, MSi =
j j PI
(e k,instanceof,c)fromT e(seeAppendixBforde- Plug(fz(M),{fz(mS sci),m PI}). Theoverallobjec-
tailed sampling strategy), and use each sampled tivecanbeformulatedas:
tripletoconstructtwopairsofverbalizedqueries N
(cid:88) (cid:88)
andanswerastheinputsandexpectedoutputsfor L = ‚àí logP (ySi|xS),
PI i j j
M sc: (xS,yS1,...,ySN)‚ààDS i=1
j j j a
‚Ä¢ ‚Äú‚ü®e k‚ü©||instanceof‚Äù‚Üí‚Äú‚ü®c‚ü©‚Äù; (4)
‚Ä¢ ‚Äú‚ü®c‚ü©||containsinstance‚Äù‚Üí‚Äú‚ü®e k‚ü©‚Äù. whereP i(y jSi|xS j)isthelikelihoodofM PSi
I
gener-
Here,‚ü®e ‚ü©and‚ü®c‚ü©meansfillinginthenamesofe ating ySi given xS, defined by token-level cross
k k j j
andc,respectively. entropy. Togenerateprogramsconformingtodif-
Besides, the information of a concept is also ferentschemasgiventhesamequestion,m PI must
relatedtoitssub-andsuper-concepts. Therefore, learnto(1)choosecorrectfunctionsaccordingto
for each triple (c ,subclassof,c ) ‚àà T , we also the compositional structure of the question; (2)
i j c
constructtwoquerieswithanswersforM : extractandutilizequestion-relevantschemainfor-
sc
mation for argument determination from the cor-
‚Ä¢ ‚Äú‚ü®c ‚ü©||subclassof‚Äù‚Üí‚Äú‚ü®c ‚ü©‚Äù;
i j
responding schema plugin, because it is the only
‚Ä¢ ‚Äú‚ü®c ‚ü©||containssubclass‚Äù‚Üí‚Äú‚ü®c ‚ü©‚Äù.
j i differenceamongMS1,...,MSN.
PI PI
Finally, the information of a relation can be
learnedfromitsnameandtheelementsconnected 4.2.4 PluginTransfer
by it. Therefore, for each r ‚àà R , we sample K Once the PI plugin m is trained, we directly
l PI
triples (e ,r,e ) from T , choose c , c such that transferittoKBT asinFig2(d),andletMT =
i j l i j PI
5plug(M,{mT sc,m PI}) be the PI model for KBT. Dataset |R| |R u| |C| |C u| |Dtest| |D utest|
Here, mT is the trained schema plugin for KBT
sc KQAPro 1209 - 794 - - -
usingthemethodinSec.4.2.2. SincemT andmSi WebQSP 412 296 446 363 1639 1083
sc sc
GraphQ 9569 8931 7298 7004 2395 2340
aretrainedwiththesametasks,weexpectthatthey
GrailQA(dev) 3938 3524 2018 1868 6763 6578
encodeschemainformationintotheirparametersin
GrailQA(test) 3938 3524 2018 1868 13231 -
similarways(Qinetal.,2021;Suetal.,2022),so MetaQA 9 9 9 3 39093 39093
m canalsoextractschemainformationfrommT SoAyBench 17 11 5 3 792 756
PI sc
tohelpPIoverKBT. Besides,toguaranteeMT
PI Table1: Statisticsforsourceandtargetdomaindatasets
generatingvalidprogramswhichdonotcauseexe-
andtheiroverlapswith16sourceKBsgeneratedfrom
cutionerrororreturnanemptyanswer,weadopt KQAPro. |R|/|C|denotesthenumberofrelations/
constrained decoding, i.e., after MT generates conceptsintheirKBs. |R |/|C |denotesthenumber
PI u u
f (arg ),...,f (arg ),weenumerateallthevalid ofrelations/conceptsunseeninthesourceKBs. |Dtest|
1 1 t t
f t+1(arg t+1) following the method of Gu et al. and |D utest| denotes the numbers of test cases and test
(2023) and restrict MT to only generate one of casesthatinvolveunseenschemaitems,respectively.
PI
them. More details are in Appendix C. We also
use beam search to retain top-k programs during
5.2 Baselines
decodingtoprovideMT withamoreglobalview.
PI
ForWebQSP,GraphQ,GrailQA,andMetaQA,we
mainlycompareKB-Pluginwithlow-resourcedPI
methods including (1) few-shot program genera-
5 Experiments
tion methods Pangu (Gu et al., 2023) and KB-
BINDER(Lietal.,2023a);(2)few-shotdatagen-
5.1 Datasets erationmethodAPS(Lietal.,2023c);(3)program
transfermethodProgramTrans(Caoetal.,2022b),
Source Domain. We use KQA Pro (Cao et al., whereweadoptitsresultswithoutfine-tuningon
2022a)asthesourcedomaindatasets. Itprovides target KBs for fair comparison. In addition, we
117,970questionswithdiversecompositionalstruc- also provide the results of several representative
turesandcorrespondingprogramsbasedonasub- supervisedmodelsforcomparison.
setofWikidata(VrandecicandKr√∂tzsch,2014). ForSoAyBench,wechoosetool-usingmethods
that were evaluated on it as baselines, including
Target Domain. We use WebQSP (Yih et al.,
DFSDT(Qinetal.,2023)andSoAy(Anonymous,
2016), GraphQ (Su et al., 2016), GrailQA (Gu
2024). Thesemethodssolvequestionsbyprompt-
et al., 2021), MetaQA (Zhang et al., 2018) and
ingLLMstocallAminerAPIsinspecificordersvia
SoAyBench(Anonymous,2024)asthetargetdo-
in-contextlearning. Theirprocessesofdetermining
main datasets. Among them, WebQSP, GraphQ,
thecompositionofAPIsandfillinginarguments
and GrailQA are based on Freebase (Bollacker
foreachAPIcanalsobeviewedasprograminduc-
etal.,2008). TheirKBscontainalargenumberof
tion.
schemaitemsandcanevaluatetheeffectivenessof
KB-Pluginforlarge-scaleKBs. MetaQAandSoAy- Weprovidedetaileddescriptionsofallthebase-
Bencharetwodatasetsinmovieandacademicdo- linesandourevaluationmetricsinAppendixD.1.
mains,respectively,andcanevaluatetheeffective-
5.3 ImplementationDetails
nessforspecificdomains. ForMetaQA,sincemost
of the relations in its KB have been covered by Inexperiments,weuseLlama2-7B(Touvronetal.,
KQAPro,weremovetheserelationsandrelevant 2023)asthebackboneLLMofKB-Pluginandset
question-program pairs from KQA Pro to avoid therankr ofLoRAto16. Thenumberofparame-
dataleakage. ForSoAyBenchwhichisoriginally tersofeachpluginisconsequently40M,whichis
atool-usingdatasetbasedonAminer(Tangetal., extremely lightweight. The number of generated
2008)APIs,weconstructitsKBbycollectingrele- source KBs is set to 16 to balance performance
vantdatafromtheseAPIs. Table1showsthestatis- and training efficiency. The sampling number K
ticsofthesedatasetsandtheiroverlapwithsource in schema plugin learning is set to be 500, 500,
KBsgeneratedfromKQAPro. Mostschemaitems 50, 100, 3000, and 1000 for KQA Pro, WebQSP,
in the target KBs are unseen in source KBs and GraphQ,GrailQA,MetaQA,andSoAyBench,re-
mosttestcasesalsoinvolveunseenschemaitems. spectively,tolimitthesizeoftheconstructeddata
6GrailQA Method Acc
Method WebQSP GraphQ
Test Dev
DFSDT(gpt-3.5-turbo) 45.7
Supervised DFSDT(gpt-4) 59.7
SoAy(gpt-3.5-turbo) 67.7
QGG 74.0 - 36.7 -
SoAy(gpt-4) 88.7
BERT+Ranking - 25.0 58.0 -
KB-Plugin 90.8
ArcaneQA 75.6 31.8 73.7 76.8
w/oschemaplugin 70.8
RnG-KBQA 75.6 - 74.4 76.9
w/mS0 64.0
sc
Low-resourced
ProgramTrans 53.8‚àó - - - Table4: AccuracyresultsonSoAyBench.
APS 51.1 - 57.7 62.1
KB-BINDER 53.2 39.5 56.0 -
Pangu 54.5 43.3 62.7 - Dataset Method D ste es et n D ute ns st een
KB-Plugin 57.2/61.1‚àó 49.5 62.7 65.0 KB-Plugin 64.9 53.3
w/oschemaplugin 41.0 42.8 - 57.5 WebQSP w/oschemaplugin 47.6 37.6
w/mS sc0 48.0 37.9 - 51.0 Gain +17.4 +15.7
KB-Plugin 40.0‚àó 49.7
Table2: F1resultsonWebQSP,GraphQ,andGrailQA. GraphQ w/oschemaplugin 70.9‚àó 42.2
‚àómeansusingoracletopicentities. Gain -30.9‚àó +7.5
KB-Plugin 69.0 64.8
GrailQA-dev w/oschemaplugin 64.9 57.3
Method 1-hop 2-hop 3-hop Gain +4.1 +7.5
Supervised
Table 5: F1 Results of KB-Plugin with and without
KV-Mem 96.2 82.7 48.9 schema plugin. Dtest and Dtest denote the sets of
PullNet 97.0 99.9 91.4 unseen seen
testcasesthatinvolveanddonotinvolveschemaitems
EmbedKGQA 97.5 98.8 94.8
TransferNet 97.5 100.0 100.0 unseen in the source KBs, respectively. ‚àó means the
results may not be indicative since there are only 55
Low-resourced
casesinDtest ofGraphQ.
seen
KB-BINDER 93.5 99.6 96.4
KB-Plugin 97.1 100.0 99.3
w/oschemaplugin 92.6 99.0 98.9
w/mS0 90.4 93.6 88.6 ing. KB-Pluginevensurpassesseveralsupervised
sc
models on GraphQ and GrailQA, which demand
Table3: Hit@1resultsonMetaQA. trainingusingthousandsofannotatedsamplesfrom
targetKBs,showingtheeffectivenessoftransfer-
ringpriorknowledgefromrich-resourcedKBs.
for schema plugin learning. We use beam size 5
OnMetaQAandSoAyBench,KB-Pluginoutper-
forallexperiments. Moredetailscanbefoundin
formsallthelow-resourcedbaselineseventhough
AppendixD.2.
theyusemorepowerfulLLMs(i.e.,Codex,gpt-3.5-
5.4 MainResults turbo, and gpt-4), indicating that our framework
also performs well for domain-specific KBs. In
TheresultsarepresentedinTable2,3and4. Com-
particular,KB-Pluginachievesstrongperformance
pared with Pangu, the SoTA PI method for low-
onparwithsupervisedSoTAsonMetaQAevenif
resourcedKBs,KB-PluginimprovestheF1score
itdoesnotseeanytargetrelationsfromthesource
by 2.7% and 6.2% on WebQSP and GraphQ, re-
domain.
spectively,andachievescomparableperformance
onGrailQA,despitePanguusing25√ólargermodel
5.5 AblationStudy
(175B Codex) and 100 annotated examples from
eachdataset. Moreover,PanguneedstocallCodex To demonstrate the effect of schema plugins, we
hundredsoftimesforaquestiontoscoreeachcan- remove them from our framework, i.e., we di-
didate program, while our model selects the op- rectly train a PI plugin using the source domain
timal program via beam search, which is signifi- dataandtransferittothetargetKBswithouttrain-
cantly faster and less costly. Besides, since Pro- inganyschemaplugins. AccordingtoTable2,3,
gramTrans,KB-BINDER,andPangualllinkques- 4, and 5, the performance of KB-Plugin without
tionstoschemaitemsaccordingtotheirnamesonly, schemapluginsisseverelydegraded,especiallyon
thesuperiorityofKB-Pluginalsodemonstratesthe thetestcasesthatinvolveschemaitemsunseenin
benefitsofaggregatingadditionalschemainforma- thesourceKBs. Theexperimentalresultsillustrate
tionfromrelevanttriplesviaschemapluginlearn- that(1)directPItransferisdifficultduetothesub-
7QuestionI WhichairporttoflyintoRome?
Pangu Find(Rome)Relate(touristattractions) ((cid:37))
KB-Pluginw/oschemaplugin Find(Rome)Relate(country)FilterConcept(sovereignstate) ((cid:37))
KB-Plugin Find(Rome)Relate(transportterminus)FilterConcept(airport) ((cid:33))
(London,transportterminus,Lutonairport),(London,instanceof,citytown),
RelevantTriples
(Lutonairport,instanceof,airport)
QuestionII WhatroledidPaulMccartneyplayintheBeatles?
Pangu Find(PaulMccartney)Relate(instrumentsplayed)((cid:37))
KB-Plugin Find(Beatles)Relate(member)Find(PaulMccartney)ReverseRelate(member)And()Relate(role) ((cid:33))
WhatisJaneLynch‚ÄôsroleinGlee?
SourceDomainDataPair
Find(Glee)Relate(starring)Find(JaneLynch)ReverseRelate(starring)And()Relate(characterrole)
Table 6: Two typical questions from the test set of WebQSP that KB-Plugin succeeds while Pangu fails. The
incorrectfunctionsandargumentsaremarkedasred,whilethecorrectonesaremarkedasgreen.
WebQSP GraphQ GrailQA-dev succeeds in forcing the PI plugin to learn to ex-
70
63.7 64.2 65 tractandutilizeschemainformationfromdifferent
65 60.7
60 57.2 schemaplugins,andthelearnedskillcanbetrans-
54.9
55 05 47.2 48.2 51 49.5 ferredtotargetKBs.
45
40.6 45 46.8 46.4 5.6 CaseStudy
40
35 36.7 To better showcase the advantages of KB-Plugin
30
over in-context learning PI methods, we present
0 2 4 6 8 10 12 14 16
Number of Generated Source KBs acasecomparisonbetweenKB-PluginandPangu
inTable6. QuestionIshowstheeffectofschema
Figure3: KB-Pluginperformancewithdifferentnum-
plugin learning and utilization. Both Pangu and
bersofgeneratedsourceKBs.
KB-Pluginwithoutschemapluginstruggletopre-
dict the correct relation ‚Äútransport terminus‚Äù be-
stantialdifferencebetweentheschemasofsource causeitisunseeninthedemoexamplesorsource
andtargetKBs;(2)schemapluginsoftargetKBs KBs. The complete KB-Plugin, however, effec-
effectively encode adequate schema information tivelyencodestheinformationthat‚Äútransportter-
via the triple completion task, and the PI plugin minus‚Äùisapossiblerelationbetween‚Äúcitytown‚Äù
canextractandutilizequestion-relevantschemain- and‚Äúairport‚Äùintotheschemapluginviacomplet-
formationfromtheseschemapluginseventhough ingrelevanttriples,andsucceedsinpredictingthis
it is never trained with them. In addition, if we relationbyutilizingaboveinformation. Question
adopttheschemapluginofasourceKB,e.g.,mS0, II demonstrates the benefits of harnessing abun-
sc
forthetargetKBs,theperformanceofKB-Plugin dantprogramannotationsfromthesourcedomain,
alsodropsheavily,showingthenecessityofusing where Pangu produces a program with incorrect
matchedschemaplugin. functioncompositionbecausenoneofitsdemoex-
ampleshasasimilarcompositionalstructure,while
ToshowtherationalityofourPIpluginlearning
KB-Plugininducesthecorrectprogrambyutilizing
method, we evaluate the performance of PI plu-
priorknowledgelearnedfromthesourcedomain.
gins trained with different numbers of generated
FurtheranalysiscanbefoundinAppendixEandF.
source KBs on WebQSP, GraphQ, and GrailQA,
and present the results in Fig. 3. The PI plugin
6 Conclusion
trainedwithonlyonesourceKBperformspoorly,
implyingthatitignorestheschemapluginentirely WeproposeKB-Plugin,aplug-and-playframework
and directly learns PI over this source KB. Once that enables LLMs to induce programs over any
there emerges a new source KB with a different low-resourcedKBbylearningtwotypesofplug-
schema,theperformanceofthetrainedPIplugin gable modules: KB-specific schema plugin and
increases substantially, and there is an apparent KB-transferable PI plugin. KB-Plugin achieves
trendthattheperformancewillincreasewithmore better or comparable performance on five hetero-
generated source KBs. These results prove that geneousKBQAdatasetswithmuchsmallerback-
training the PI plugin over multiple source KBs boneLLMscomparedtoSoTAPImethodsforlow-
8
1FresourcedKBs,demonstratingitseffectivenessfor References
both large-scale and domain-specific KBs. Abla-
Anonymous.2024. Soay: Aservice-orientedapisap-
tionstudyandcasestudyalsoprovetherationality plyingframeworkoflargelanguagemodels.
andfurthershowcasetheadvantageofKB-plugin.
Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh,
Tim Sturge, and Jamie Taylor. 2008. Freebase: a
7 Limitations
collaborativelycreatedgraphdatabaseforstructuring
humanknowledge. InProceedingsoftheACMSIG-
WediscussseverallimitationsofKB-Plugininthis
MODInternationalConferenceonManagementof
section: (1) In the experiments, we only adopt
Data,SIGMOD2008,Vancouver,BC,Canada,June
Llama2-7B as our backbone model due to lim- 10-12,2008,pages1247‚Äì1250.ACM.
ited computing resources. Actually, KB-Plugin
Antoine Bordes, Nicolas Usunier, Alberto Garc√≠a-
ismodel-agnosticandcanalsobeappliedtomore
Dur√°n, Jason Weston, and Oksana Yakhnenko.
language models with various sizes and architec-
2013. Translatingembeddingsformodelingmulti-
tures. (2) KB-Plugin requires that the source do- relationaldata. InAdvancesinNeuralInformation
main dataset covers questions with diverse vari- ProcessingSystems26: 27thAnnualConferenceon
NeuralInformationProcessingSystems2013.Pro-
ous compositional structures, and performs rela-
ceedingsofameetingheldDecember5-8,2013,Lake
tively poorly for questions whose compositional
Tahoe,Nevada,UnitedStates,pages2787‚Äì2795.
structuresareunseeninthesourcedomaindataset
thoughtheyarerare(seeAppendixEfordetails). TomB.Brown,BenjaminMann,NickRyder,Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Futureresearchcanfocusonimprovingthetrans-
Neelakantan,PranavShyam,GirishSastry,Amanda
ferabilityofKB-Pluginacrosscompositionalstruc-
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
tures. Inpractice,wecanalsocontinuetotrainthe Gretchen Krueger, Tom Henighan, Rewon Child,
PIpluginusingsomeself-trainingmethodssuchas Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
ClemensWinter,ChristopherHesse,MarkChen,Eric
EGST(Lietal.,2023c)toadapttothesequestions.
Sigler,MateuszLitwin,ScottGray,BenjaminChess,
(3)Inthiswork,sincebothtrainingandevaluation
Jack Clark, Christopher Berner, Sam McCandlish,
of KB-Plugin require annotated KBQA datasets, Alec Radford, Ilya Sutskever, and Dario Amodei.
wecanonlytakeasingledatasetKQAProasthe 2020. Languagemodelsarefew-shotlearners. InAd-
vancesinNeuralInformationProcessingSystems33:
source dataset and take other datasets as the tar-
AnnualConferenceonNeuralInformationProcess-
getdatasets,whichmaylimittheupperboundsof
ing Systems 2020, NeurIPS 2020, December 6-12,
KB-Plugin. Intherealisticscenariowhereweneed 2020,virtual.
toapplyKB-PluginforanewKB,wecantakeall
Shulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie,
theseKBQAdatasetsasthesourcedomaindatasets
YutongXiang,LeiHou,JuanziLi,BinHe,andHan-
sothatthetrainedsourceschemapluginswouldbe
wangZhang.2022a. KQApro: Adatasetwithex-
morediverseandthetrainedPIpluginwouldalso plicitcompositionalprogramsforcomplexquestion
havestrongertransferabilityandgeneralizability. answeringoverknowledgebase. InProceedingsof
the60thAnnualMeetingoftheAssociationforCom-
putationalLinguistics(Volume1:LongPapers),ACL
8 EthicalConsiderations
2022,Dublin,Ireland,May22-27,2022,pages6101‚Äì
6119.AssociationforComputationalLinguistics.
Thoughourframework(aswellasotherPImeth-
ods)caneffectivelyreducetheprobabilityofLLMs
Shulin Cao, Jiaxin Shi, Zijun Yao, Xin Lv, Jifan Yu,
generating inaccurate answers when faced with LeiHou,JuanziLi,ZhiyuanLiu,andJinghuiXiao.
questionsinvolvinguncommonknowledge,itmay 2022b. Program transfer for answering complex
questions over knowledge bases. In Proceedings
stillmakemistakesiftheinducedprogramsarein-
of the 60th Annual Meeting of the Association for
correct. Inaddition,thereisariskofbeinghacked
ComputationalLinguistics(Volume1: LongPapers),
throughtargetedmeanssuchasinjectingharmful ACL2022,Dublin,Ireland,May22-27,2022,pages
ornonfactualknowledgeintotheKBs. Hencead- 8128‚Äì8140.AssociationforComputationalLinguis-
tics.
ditional care and protective measures should be
takenifourframeworkisdeployedinuser-facing
MarkChen,JerryTworek,HeewooJun,QimingYuan,
applications. Henrique Pond√© de Oliveira Pinto, Jared Kaplan,
Allthedatasetsandencyclopediasusedinthis Harrison Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
work are publicly published with permissible li-
Krueger,MichaelPetrov,HeidyKhlaaf,GirishSas-
censes.
try, Pamela Mishkin, Brooke Chan, Scott Gray,
NickRyder,MikhailPavlov,AletheaPower,Lukasz
9Kaiser, Mohammad Bavarian, Clemens Winter, BrianLester,RamiAl-Rfou,andNoahConstant.2021.
Philippe Tillet, Felipe Petroski Such, Dave Cum- The power of scale for parameter-efficient prompt
mings, Matthias Plappert, Fotios Chantzis, Eliza- tuning. InProceedingsofthe2021Conferenceon
beth Barnes, Ariel Herbert-Voss, William Hebgen EmpiricalMethodsinNaturalLanguageProcessing,
Guss,AlexNichol,AlexPaino,NikolasTezak,Jie EMNLP2021,VirtualEvent/PuntaCana,Domini-
Tang,IgorBabuschkin,SuchirBalaji,ShantanuJain, can Republic, 7-11 November, 2021, pages 3045‚Äì
William Saunders, Christopher Hesse, Andrew N. 3059.AssociationforComputationalLinguistics.
Carr,JanLeike,JoshuaAchiam,VedantMisra,Evan
Morikawa, Alec Radford, Matthew Knight, Miles TianleLi,XueguangMa,AlexZhuang,YuGu,YuSu,
Brundage,MiraMurati,KatieMayer,PeterWelinder, andWenhuChen.2023a. Few-shotin-contextlearn-
BobMcGrew,DarioAmodei,SamMcCandlish,Ilya ingforknowledgebasequestionanswering. CoRR,
Sutskever, and Wojciech Zaremba. 2021. Evaluat- abs/2305.01750.
inglargelanguagemodelstrainedoncode. CoRR,
abs/2107.03374. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Yu Gu, Xiang Deng, and Yu Su. 2023. Don‚Äôt gener- Proceedingsofthe59thAnnualMeetingoftheAsso-
ate,discriminate: Aproposalforgroundinglanguage ciationforComputationalLinguisticsandthe11th
modelstoreal-worldenvironments. InProceedings InternationalJointConferenceonNaturalLanguage
of the 61st Annual Meeting of the Association for Processing, ACL/IJCNLP 2021, (Volume 1: Long
ComputationalLinguistics(Volume1: LongPapers), Papers),VirtualEvent,August1-6,2021,pages4582‚Äì
ACL2023,Toronto,Canada,July9-14,2023,pages 4597.AssociationforComputationalLinguistics.
4928‚Äì4949.AssociationforComputationalLinguis-
tics. XingxuanLi,RuochenZhao,YewKenChia,Bosheng
Ding, Lidong Bing, Shafiq R. Joty, and Soujanya
Yu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler,
Poria. 2023b. Chain of knowledge: A framework
PercyLiang,XifengYan,andYuSu.2021. Beyond
forgroundinglargelanguagemodelswithstructured
I.I.D.: threelevelsofgeneralizationforquestionan- knowledgebases. CoRR,abs/2305.13269.
sweringonknowledgebases. InWWW‚Äô21: TheWeb
Conference2021,VirtualEvent/Ljubljana,Slovenia,
Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, Zhichao
April19-23,2021,pages3477‚Äì3488.ACM/IW3C2.
Duan,BowenDong,NingLiu,andJianyongWang.
2023c. Flexkbqa: A flexible llm-powered frame-
YuGuandYuSu.2022. Arcaneqa: Dynamicprogram
workforfew-shotknowledgebasequestionanswer-
inductionandcontextualizedencodingforknowledge
ing. CoRR,abs/2308.12060.
basequestionanswering. InProceedingsofthe29th
InternationalConferenceonComputationalLinguis-
YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and
tics, COLING 2022, Gyeongju, Republic of Korea,
Xuan Zhu. 2015. Learning entity and relation em-
October12-17,2022,pages1718‚Äì1731.International
beddingsforknowledgegraphcompletion. InPro-
CommitteeonComputationalLinguistics.
ceedings of the Twenty-Ninth AAAI Conference on
ArtificialIntelligence,January25-30,2015,Austin,
Matthew Honnibal, Ines Montani, Sofie Van Lan-
Texas,USA,pages2181‚Äì2187.AAAIPress.
deghem,andAdrianeBoyd.2020. spacy: Industrial-
strengthnaturallanguageprocessinginpython.
Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng,
YikaiGuo,WentaiZhang,ChenghaoMa,Guanting
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Dong,MeinaSong,andWeiLin.2023. Chatkbqa: A
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
generate-then-retrieveframeworkforknowledgebase
WeizhuChen.2022. Lora: Low-rankadaptationof
largelanguagemodels. InTheTenthInternational question answering with fine-tuned large language
ConferenceonLearningRepresentations,ICLR2022, models. CoRR,abs/2310.08975.
VirtualEvent,April25-29,2022.OpenReview.net.
Xin Lv, Lei Hou, Juanzi Li, and Zhiyuan Liu. 2018.
JinhaoJiang,KunZhou,ZicanDong,KemingYe,Xin Differentiatingconceptsandinstancesforknowledge
Zhao,andJi-RongWen.2023. Structgpt: Ageneral graphembedding. InProceedingsofthe2018Con-
frameworkforlargelanguagemodeltoreasonover ferenceonEmpiricalMethodsinNaturalLanguage
structureddata. InProceedingsofthe2023Confer- Processing,Brussels,Belgium,October31-Novem-
enceonEmpiricalMethodsinNaturalLanguagePro- ber4,2018,pages1971‚Äì1979.AssociationforCom-
cessing,EMNLP2023,Singapore,December6-10, putationalLinguistics.
2023,pages9237‚Äì9251.AssociationforComputa-
tionalLinguistics. AlexanderH.Miller,AdamFisch,JesseDodge,Amir-
HosseinKarimi,AntoineBordes,andJasonWeston.
YunshiLanandJingJiang.2020. Querygraphgenera- 2016. Key-valuememorynetworksfordirectlyread-
tionforansweringmulti-hopcomplexquestionsfrom ingdocuments. InProceedingsofthe2016Confer-
knowledgebases. InProceedingsofthe58thAnnual enceonEmpiricalMethodsinNaturalLanguagePro-
Meeting of the Association for Computational Lin- cessing,EMNLP2016,Austin,Texas,USA,Novem-
guistics,ACL2020,Online,July5-10,2020,pages ber1-4,2016,pages1400‚Äì1409.TheAssociationfor
969‚Äì974.AssociationforComputationalLinguistics. ComputationalLinguistics.
10FedorMoiseev,ZheDong,EnriqueAlfonseca,andMar- transparent framework for multi-hop question an-
tinJaggi.2022. SKILL:structuredknowledgeinfu- sweringoverrelationgraph. InProceedingsofthe
sionforlargelanguagemodels. InProceedingsofthe 2021ConferenceonEmpiricalMethodsinNatural
2022ConferenceoftheNorthAmericanChapterof LanguageProcessing,EMNLP2021,VirtualEvent
theAssociationforComputationalLinguistics: Hu- /PuntaCana,DominicanRepublic,7-11November,
manLanguageTechnologies,NAACL2022,Seattle, 2021,pages4149‚Äì4158.AssociationforComputa-
WA, United States, July 10-15, 2022, pages 1581‚Äì tionalLinguistics.
1588.AssociationforComputationalLinguistics.
YuSu,HuanSun,BrianM.Sadler,MudhakarSrivatsa,
Lunyiu Nie, Shulin Cao, Jiaxin Shi, Jiuding Sun, IzzeddinGur,ZenghuiYan,andXifengYan.2016.
QiTian,LeiHou,JuanziLi,andJidongZhai.2022. On generating characteristic-rich question sets for
GraphqIR:unifyingthesemanticparsingofgraph QAevaluation. InProceedingsofthe2016Confer-
querylanguageswithoneintermediaterepresentation. enceonEmpiricalMethodsinNaturalLanguagePro-
InProceedingsofthe2022ConferenceonEmpirical cessing,EMNLP2016,Austin,Texas,USA,Novem-
MethodsinNaturalLanguageProcessing,EMNLP ber1-4,2016,pages562‚Äì572.TheAssociationfor
2022,AbuDhabi,UnitedArabEmirates,December ComputationalLinguistics.
7-11,2022,pages5848‚Äì5865.AssociationforCom-
putationalLinguistics. YushengSu,XiaozhiWang,YujiaQin,Chi-MinChan,
YankaiLin,HuadongWang,KaiyueWen,Zhiyuan
Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Ji- Liu,PengLi,JuanziLi,LeiHou,MaosongSun,and
apuWang,andXindongWu.2023. Unifyinglarge JieZhou.2022. Ontransferabilityofprompttuning
languagemodelsandknowledgegraphs: Aroadmap. fornaturallanguageprocessing. InProceedingsof
CoRR,abs/2306.08302. the2022ConferenceoftheNorthAmericanChapter
oftheAssociationforComputationalLinguistics:Hu-
Jonas Pfeiffer, Aishwarya Kamath, Andreas R√ºckl√©, manLanguageTechnologies,NAACL2022,Seattle,
Kyunghyun Cho, and Iryna Gurevych. 2021. WA, United States, July 10-15, 2022, pages 3949‚Äì
Adapterfusion: Non-destructivetaskcompositionfor 3969.AssociationforComputationalLinguistics.
transfer learning. In Proceedings of the 16th Con-
ferenceoftheEuropeanChapteroftheAssociation HaitianSun,TaniaBedrax-Weiss,andWilliamW.Co-
forComputationalLinguistics: MainVolume,EACL hen.2019. Pullnet:Opendomainquestionanswering
2021, Online, April 19 - 23, 2021, pages 487‚Äì503. withiterativeretrievalonknowledgebasesandtext.
AssociationforComputationalLinguistics. In Proceedings of the 2019 Conference on Empiri-
cal Methods in Natural Language Processing and
YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,Lan the 9th International Joint Conference on Natural
Yan,YaxiLu,YankaiLin,XinCong,XiangruTang, LanguageProcessing,EMNLP-IJCNLP2019,Hong
BillQian,SihanZhao,RunchuTian,RuobingXie, Kong,China,November3-7,2019,pages2380‚Äì2390.
Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, AssociationforComputationalLinguistics.
andMaosongSun.2023. Toolllm: Facilitatinglarge
JieTang,JingZhang,LiminYao,JuanziLi,LiZhang,
languagemodelstomaster16000+real-worldapis.
CoRR,abs/2307.16789. and Zhong Su. 2008. Arnetminer: extraction and
mining of academic social networks. In Proceed-
Yujia Qin, Xiaozhi Wang, YuSheng Su, Yankai Lin, ings of the 14th ACM SIGKDD International Con-
NingDing,ZhiyuanLiu,JuanziLi,LeiHou,Peng ferenceonKnowledgeDiscoveryandDataMining,
Li, Maosong Sun, and Jie Zhou. 2021. Exploring LasVegas,Nevada,USA,August24-27,2008,pages
low-dimensionalintrinsictasksubspaceviaprompt 990‚Äì998.ACM.
tuning. CoRR,abs/2110.07867.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
ApoorvSaxena,AdrianKochsiek,andRainerGemulla. bert, Amjad Almahairi, Yasmine Babaei, Nikolay
2022. Sequence-to-sequenceknowledgegraphcom- Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
pletionandquestionanswering. InProceedingsof Bhosale,DanBikel,LukasBlecher,CristianCanton-
the60thAnnualMeetingoftheAssociationforCom- Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
putationalLinguistics(Volume1:LongPapers),ACL JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
2022,Dublin,Ireland,May22-27,2022,pages2814‚Äì CynthiaGao,VedanujGoswami,NamanGoyal,An-
2828.AssociationforComputationalLinguistics. thonyHartshorn,SagharHosseini,RuiHou,Hakan
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
ApoorvSaxena, AditayTripathi, andParthaP.Taluk- IsabelKloumann,ArtemKorenev,PunitSinghKoura,
dar.2020. Improvingmulti-hopquestionanswering Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
overknowledgegraphsusingknowledgebaseembed- anaLiskovich,YinghaiLu,YuningMao,XavierMar-
dings. InProceedingsofthe58thAnnualMeetingof tinet,TodorMihaylov,PushkarMishra,IgorMoly-
theAssociationforComputationalLinguistics,ACL bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
2020, Online, July 5-10, 2020, pages 4498‚Äì4507. stein,RashiRungta,KalyanSaladi,AlanSchelten,
AssociationforComputationalLinguistics. Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
Jiaxin Shi, Shulin Cao, Lei Hou, Juanzi Li, and Han- lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
wang Zhang. 2021. Transfernet: An effective and ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
11Melanie Kambadur, Sharan Narang, Aur√©lien Ro- ACL2016,August7-12,2016,Berlin,Germany,Vol-
driguez,RobertStojnic,SergeyEdunov,andThomas ume2: ShortPapers.TheAssociationforComputer
Scialom.2023. Llama2: Openfoundationandfine- Linguistics.
tunedchatmodels. CoRR,abs/2307.09288.
YuyuZhang,HanjunDai,ZornitsaKozareva,Alexan-
DennyVrandecicandMarkusKr√∂tzsch.2014. Wiki- derJ.Smola,andLeSong.2018. Variationalreason-
data: afreecollaborativeknowledgebase. Commun. ingforquestionansweringwithknowledgegraph. In
ACM,57(10):78‚Äì85. ProceedingsoftheThirty-SecondAAAIConference
onArtificialIntelligence,(AAAI-18),the30thinno-
XiaozhiWang,KaiyueWen,ZhengyanZhang,LeiHou, vative Applications of Artificial Intelligence (IAAI-
Zhiyuan Liu, and Juanzi Li. 2022. Finding skill 18), and the 8th AAAI Symposium on Educational
neurons in pre-trained transformer-based language AdvancesinArtificialIntelligence(EAAI-18),New
models. InProceedingsofthe2022Conferenceon Orleans,Louisiana,USA,February2-7,2018,pages
EmpiricalMethodsinNaturalLanguageProcessing, 6069‚Äì6076.AAAIPress.
EMNLP2022,AbuDhabi,UnitedArabEmirates,De-
cember7-11,2022,pages11132‚Äì11152.Association ZhengyanZhang,ZhiyuanZeng,YankaiLin,Huadong
forComputationalLinguistics. Wang,DemingYe,ChaojunXiao,XuHan,Zhiyuan
Liu, Peng Li, Maosong Sun, and Jie Zhou. 2023.
Chaojun Xiao, Zhengyan Zhang, Xu Han, Chi-Min Plug-and-play knowledge injection for pre-trained
Chan, Yankai Lin, Zhiyuan Liu, Xiangyang Li, languagemodels. InProceedingsofthe61stAnnual
Zhonghua Li, Zhao Cao, and Maosong Sun. 2023. Meeting of the Association for Computational Lin-
Plug-and-play document modules for pre-trained guistics(Volume1:LongPapers),ACL2023,Toronto,
models. InProceedingsofthe61stAnnualMeeting Canada,July9-14,2023,pages10641‚Äì10658.Asso-
oftheAssociationforComputationalLinguistics(Vol- ciationforComputationalLinguistics.
ume1: LongPapers),ACL2023,Toronto,Canada,
July9-14,2023,pages15713‚Äì15729.Associationfor Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
ComputationalLinguistics. Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-
ichenZhang,JunjieZhang,ZicanDong,YifanDu,
TianbaoXie,ChenHenryWu,PengShi,RuiqiZhong, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao
TorstenScholak,MichihiroYasunaga,Chien-Sheng Jiang,RuiyangRen,YifanLi,XinyuTang,Zikang
Wu,MingZhong,PengchengYin,SidaI.Wang,Vic- Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
torZhong,BailinWang,ChengzuLi,ConnorBoyle, 2023. A survey of large language models. CoRR,
Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming abs/2303.18223.
Xiong,LingpengKong,RuiZhang,NoahA.Smith,
Luke Zettlemoyer, and Tao Yu. 2022. Unifiedskg:
Unifying and multi-tasking structured knowledge
groundingwithtext-to-textlanguagemodels. InPro-
ceedingsofthe2022ConferenceonEmpiricalMeth-
odsinNaturalLanguageProcessing,EMNLP2022,
AbuDhabi,UnitedArabEmirates,December7-11,
2022,pages602‚Äì631.AssociationforComputational
Linguistics.
Xuchen Yao. 2015. Lean question answering over
freebase from scratch. In NAACL HLT 2015, The
2015ConferenceoftheNorthAmericanChapterof
theAssociationforComputationalLinguistics: Hu-
manLanguageTechnologies,Denver,Colorado,USA,
May31-June5,2015,pages66‚Äì70.TheAssociation
forComputationalLinguistics.
XiYe,SemihYavuz,KazumaHashimoto,YingboZhou,
andCaimingXiong.2022. RNG-KBQA:generation
augmentediterativerankingforknowledgebaseques-
tionanswering. InProceedingsofthe60thAnnual
Meeting of the Association for Computational Lin-
guistics(Volume1: LongPapers),ACL2022,Dublin,
Ireland,May22-27,2022,pages6032‚Äì6043.Associ-
ationforComputationalLinguistics.
Wen-tauYih,MatthewRichardson,ChristopherMeek,
Ming-WeiChang,andJinaSuh.2016. Thevalueof
semanticparselabelingforknowledgebasequestion
answering. InProceedingsofthe54thAnnualMeet-
ingoftheAssociationforComputationalLinguistics,
12Function Input√óArgs‚ÜíOutput Description (e.g., ‚ÄúWho is the heaviest film director?" from
Find E√ó‚àÖ‚ÜíE findanentityfromtheKB GrailQA, whose target program is FindAll()
FindAll ‚àÖ√ó‚àÖ‚ÜíE‚Ä≤ returnallentitiesintheKB
Relate (E‚à™E‚Ä≤)√óR‚ÜíE‚Ä≤ asinglehopalongarelation FilterConcept(director)SelectAmong(weight
ReverseRelate (E‚à™E‚Ä≤)√óR‚ÜíE‚Ä≤ areversehopalongarelation
FilterConcept E‚Ä≤√óC‚ÜíE‚Ä≤ returnentitiesinaconcept kg). For these questions, we follow Pangu (Gu
And/Or (E‚Ä≤,E‚Ä≤)√ó‚àÖ‚ÜíE‚Ä≤ intersection/unionoftwosets
Argmax/Argmin E‚Ä≤√óR‚ÜíE‚Ä≤ superlativeaggregations et al., 2023) to start constrained decoding from
LT/LE/GT/GE E√óR‚ÜíE‚Ä≤ </‚â§/>/‚â• FindAll()FilterConcept(c), where c is a topic
Count E‚Ä≤√ó‚àÖ‚ÜíN setcardinality
conceptprovidedbyGuandSu(2022).
Table7: KoPLfunctionsusedinthiswork. E: entity; When t > 0, we execute the current program
E‚Ä≤:asetofentities;R:relation;C:concept;N:integer.
p = ‚ü®f (arg ),...,f (arg )‚ü© to get its denota-
t 1 1 t t
tion (i.e., a set of entities) and also the concepts,
A DetailsofKoPLFunctions forwardrelations,andbackwardrelationsthatare
reachable from the denotation. For each concept
WelistKoPLfunctionsusedinthisworkinTable7.
c,weenumerateFilterConcept(c)asacandidate
Wemakesomemodificationstotheoriginal(Cao
inP . Foreachforwardrelationr,weenumer-
t+1
etal.,2022a)forconciseness. ExceptFindtaking
ateRelate(r)asacandidate. Foreachbackward
topic entities as the argument, other functions ei-
relation r, we enumerate ReverseRelate(r) as a
therhavenoargumentsortakeschemaitems(i.e.,
candidate,andalsoincludeLT(r),LE(r),GT(r),and
conceptsorrelations)astheirarguments.
GE(r) in P if the denotation of p is a numeri-
t+1 t
calvaluesuchasaquantityoradate. Inaddition,
B TripleSamplingStrategy
candidateswithsuperlativescanbeenumeratedas
Let the given KB be KB = {C,E,R,T}, where Argmax(r) and Argmin(r). Also, Count() can al-
T = T e ‚à™T c ‚à™T l. For each e ‚àà E, let cnt(e) be ways be included to P t+1. If there are multiple
itspopularity(i.e.,thenumberofitsoccurrencesin topic entities, we enumerate Find(e‚Ä≤) as a candi-
KB). date to add a new branch, where e‚Ä≤ ‚àà E is a
topic
Whensampling‚Äúinstanceof‚Äùtriplesforacon- topic entity not in p . When p contains multiple
t t
cept c ‚àà C, we hope the sampled triples contain branches, we enumerate Or() and And() as candi-
representative entities belonging to c, so we sort datestomergethelasttwobranches.
all(e ,instanceof,c) ‚àà T indescendingorderof
k e
cnt(e )andselectthefirstK triples. D ExperimentalSetup
k
When sampling relational triples for a rela-
D.1 DetailsofBaselinesandEvaluation
tion r ‚àà R , we take both representative-
l Metrics
ness and diversity into account. Therefore, we
Thedetailsofourbaselinesareasfollows:
sort all (e ,r,e ) ‚àà T in descending order of
i j l
Pangu (Gu et al., 2023) utilizes potent LLM
min(cnt(e ),cnt(e ))andselectthefirstK triples.
i j
Codex (Chen et al., 2021) to produce programs
C DetailsofConstrainedDecoding inastep-wisefashionviain-contextlearning. At
each step, it first extends existing programs into
In constrained decoding, after MT generates t
PI newvalidcandidatesbyenumeratingallpossible
function chunks f (arg ),...,f (arg ), we enu-
1 1 t t next functions with arguments, then scores each
merate all admissible f (arg ) as the candi-
t+1 t+1 candidateusingCodexwithseveraldemonstrations
date set P following the definition of KoPL
t+1 andretainsthetop-kcandidates.
functionsinTable7,andconstrainMT tocontinue
PI KB-BINDER(Lietal.,2023a)firstletsCodexgen-
generatingoneofthesecandidateorgeneratingthe
erateseveral"draft"programsforagivenquestion
‚ü®EOS‚ü©tokentoendthedecodingprocess.
byimitatingafewexamples,thengroundstheargu-
Specifically, let E be the set of topic
topic mentsinthedraftstothetargetKBusingsimilarity
entities in the question obtained using off-the-
search to produce hundreds of refined programs.
shelf entity linkers 1. At t = 0, we enumerate
The final answer is decided by the majority vote
Find(e) for each e ‚àà E as a candidate
topic afterexecutingalltheserefinedprograms.
in P . Specially, around 5% of questions in
1 Automatic Program Sampling (APS) (Li et al.,
GraphQ and GrailQA do not have a topic entity
2023c) utilizes gpt-3.5-turbo2 to translate auto-
1EntitylinkingisnotamajorchallengeforPI,andexhaus- matically sampled programs based on a handful
tivefuzzystringmatching(Yao,2015)sufficestoachievea
reasonableperformance. 2https://platform.openai.com/docs/models/gpt-3-5
13of templates into corresponding questions via in- Seen Unseen
Dataset
contextlearning,andsubsequentlyfine-tuneaRnG- Num EM F1 Num EM F1
KBQA(Yeetal.,2022)PImodelusingthegener- GraphQ 2148 71.0 52.8 247 15.4 20.4
GrailQA 6433 79.9 67.4 330 10.0 16.4
atedquestion-programpairs.
ProgramTrans (Cao et al., 2022b) is a program Table8: PerformanceofKB-Pluginontestcaseswhose
transfer method that first uses a seq2seq sketch compositional structures are seen and unseen in the
parser to translate the question into a program sourcedatasetKQAPro. EMmeanstheexactmatchof
sketch,thenusesanargumentparsertosearchsuit- programsketch.
ableargumentfromtheKBforeachfunction. We
adopt its results without fine-tuning on the target
E AnalysisaboutQuestionCompositional
KBsforfaircomparison.
Structures
DFSDT(Qinetal.,2023)istheSoTAmethodfor
generaltoolusing. Tosolveaquestion,itemploys For GraphQ and GrailQA, we translate their
aLLMtocallsuitabletoolAPIsindepth-firstorder. SPARQL programs to KoPL programs using
Ateachstep,theLLMcaneither(1)callthenext GraphQ Trans (Nie et al., 2022) and analyze
APItoproceedalongapromisingpathor(2)undo the performance of KB-Plugin on the test cases
the current call and call another API to expand a whose question compositional structures (identi-
newpath. fiedbyprogramsketches)areseenandunseenin
thesourcedomaindatasetKQAPro,respectively.
SoAy(Anonymous,2024)istheSoTAmethodon
FromtheresultsinTable8wecanseethat(1)KQA
SoAyBench. Givenaquestion,itemploysaLLMto
Pro covers most of question compositional struc-
firstselectthemostsuitableplan(i.e.,APIcombi-
turesinthetargetdataset;(2)KB-Plugincorrectly
nation)fromacandidatepool,thenwriteaPython
predictstheprogramsketchesforover70%ques-
programwithbranchingandloopingstructurefol-
tions whose compositional structures are seen in
lowingtheplantocallAPIstogettheanswer.
thesourcedomaindataset,implyingthatthemap-
Supervised Methods. For WebQSP, GraphQ,
pingfromquestionstoprogramsketchesislargely
GrailQA,andMetaQA,wealsoprovidethefully
independentofKBschemasandtransferableacross
supervisedresultsofseveralrepresentativemodels
KBs,whichisconsistentwiththefindingsofCao
for comparison, including QGG (Lan and Jiang,
etal.(2022b)andLietal.(2023a);(3)KB-Plugin
2020), BERT+Ranking (Gu et al., 2021), Arc-
performspoorlyonthequestionswithunseencom-
naeQA(GuandSu,2022),RnG-KBQA(Yeetal.,
positionalstructuresthoughtheyarerelativelyrare,
2022),KV-Mem(Milleretal.,2016),PullNet(Sun
indicatingthatmoreadvancedtransfertechniques
et al., 2019), EmbedKGQA (Saxena et al., 2020)
acrosscompositionalstructuresremainstobeex-
andTransferNetShietal.(2021).
plored.
EvalutionMetrics. Followingthesebaselines,we
use F1 for WebQSP, GraphQ, and GrailQA, use F ErrorAnalysis
Hit@1forMetaQA, anduseAccuracyfor SoAy-
Weanalyze100incorrectpredictions(i.e., F1<1)
Bench.
randomly sampled from the dev set of GrailQA.
The major errors are predicting wrong schema
D.2 ImplementationDetails items (36%). Specially, when facing several
schema items with only subtle differences, e.g.,
Wetraintheschemapluginsofthesourceandtarget ‚Äúpublisher‚Äù(reverse)v.s. ‚Äúgameversionpublished‚Äù,
KBs for 3 epochs and 1 epoch, respectively. The KB-plugintendstoprefertochoosetheshorterone
batchsizeandlearningratearesettobe128and1e- due to the inherent defects of beam search. Be-
5,respectively. Besides,wetrainthePIpluginfor1 sides, 21%errorsareduetoawrongtermination
epochwithbatchsize16andlearningrate1e-5. For checkwherethemodelmissesthelastrelationor
WebQSP,GraphQ,andGrailQA,weusethesame predictsanadditionalfunction. Therearealso5%
off-the-shelf entity-linker as Pangu to find topic wrongfunctionpredictions. Apartfromtheabove
entities;ForMetaQA,wefollowourbaselinesto errorscausedbyourmodel,27%errorsarecaused
useoracletopicentities;ForSoAyBench,wefind byunidentifiedorwronglyidentifiedtopicentities
topicentitiesusingspaCy(Honnibaletal.,2020). duringentitylinking,9%errorsareduetoambigu-
14ous or wrong annotations, and the remaining 2%
errorsareduetotheincompletionofKBs.
15