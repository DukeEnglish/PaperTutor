L2G2G: a Scalable Local-to-Global Network
Embedding with Graph Autoencoders
Ruikang Ouyang1, Andrew Elliott5,6 Stratis Limnios5, Mihai Cucuringu2,3,4,5,
and Gesine Reinert2,5
1 Department of Engineering, University of Cambridge, Cambridge, UK
2 Department of Statistics, University of Oxford, Oxford, UK
3 Mathematical Institute, University of Oxford, Oxford, UK
4 Oxford-Man Institute of Quantitative Finance, University of Oxford, Oxford, UK
5 The Alan Turing Institute, London, UK
6 School of Mathematics and Statistics, University of Glasgow, Glasgow, UK
Abstract. Foranalysingreal-worldnetworks,graphrepresentationlearn-
ing is a popular tool. These methods, such as a graph autoencoder
(GAE),typicallyrelyonlow-dimensionalrepresentations,alsocalledem-
beddings, which are obtained through minimising a loss function; these
embeddings are used with a decoder for downstream tasks such as node
classificationandedgeprediction.WhileGAEstendtobefairlyaccurate,
they suffer from scalability issues. For improved speed, a Local2Global
approach,whichcombinesgraphpatchembeddingsbasedoneigenvector
synchronisation,wasshowntobefastandachievegoodaccuracy.Herewe
proposeL2G2G,aLocal2GlobalmethodwhichimprovesGAEaccuracy
without sacrificing scalability. This improvement is achieved by dynam-
ically synchronising the latent node representations, while training the
GAEs. It also benefits from the decoder computing an only local patch
loss. Hence, aligning the local embeddings in each epoch utilises more
information from the graph than a single post-training alignment does,
while maintaining scalability. We illustrate on synthetic benchmarks, as
well as real-world examples, that L2G2G achieves higher accuracy than
the standard Local2Global approach and scales efficiently on the larger
datasets.Wefindthatforlargeanddensenetworks,itevenoutperforms
the slow, but assumed more accurate, GAEs.
Keywords: GraphAutoencoder,Local2Global,NodeEmbedding,Group
Synchronisation
1 Introduction
Graph representation learning has been a core component in graph based real
world applications, for an introduction see [13]. As graphs have become ubiqui-
tous in a wide array of applications, low-dimensional representations are needed
to tackle the curse of dimensionality inherited by the graph structure. In prac-
tice, low-dimensional node embeddings are used as efficient representations to
4202
beF
2
]GL.sc[
1v41610.2042:viXra2 R. Ouyang et al.
address tasks such as graph clustering [25], node classification [2], and link pre-
diction [27], or to protect private data in federated learning settings [20,14].
Graph Autoencoders (GAEs) [23,19] emerged as a powerful Graph Neural
Network (GNN) [5] tool to produce such node representations. GAEs adapt au-
toencoders and variational autoencoders [1,19] to graph structure data using a
Graph Convolutional Neural Network (GCN) [28] as the encoder and for node
embeddings. Although a GAE can achieve high accuracy in graph reconstruc-
tion, it suffers from a high computational cost. Several solutions for reducing
computationalworkloadhavebeenproposed.Linearmodels,likePPRGo[4]and
SGC [8], remove the non-linearities between layers. Layer-wise sampling meth-
ods such as GraphSage [12], FastGCN [6] and LADIES [29] sample a subset of
neighbors of the nodes in each layer, while subgraph-sampling based methods
such as GraphSaint [26] and Cluster-GCN [9] carry out message passing only
through a sampled subgraph.
In this paper, we use FastGAE [22] as a starting point, which computes ap-
proximate reconstruction losses by evaluating their values only from suitably
selected random subgraphs of the original graph. While FastGAE reduces the
computationalcostofatraditionalGAE,itsoverallperformancecanbesubstan-
tiallyinferiortoaGAEwhenthesampleusedtoapproximatethelossisnotlarge
enough. For improved performance, the general Local2Global (L2G) framework
by [15] leverages the eigenvector synchronization of [10,11], to align indepen-
dently created embeddings in order to produce a globally consistent structure
(here we employ GAEs for embeddings and denote the resulting method by
GAE+L2G). However, this architecture is data inefficient and suffers from a
lossofperformanceindownstreamtaskssinceitlearnsmultipleseparateGAEs.
Moreoveraggregatingthelocalembeddingsafterthetrainingprocessmightlead
to a loss of useful information learned during training.
Instead, we propose the Local to GAE to Global (L2G2G) model, which op-
timizes the Local2Global aligned embeddings directly, reducing the amount of
information loss and allowing us to train a single global modified GAE. This
structure leverages the scalable approach of FastGAE by only considering small
sections of the graph when updating the weights. Figure 1 shows the model
pipeline. Our main contributions are:
1. We introduce L2G2G as a new fast network embedding method.
2. We provide a theoretical complexity analysis for GAE+L2G and an experi-
mental comparison of the runtimes showing that the runtime sacrifice for per-
formance in L2G2G is minimal.
3. We test L2G2G and the baseline methods on real and synthetic data sets,
demonstratingthatL2G2GcanboosttheperformanceofGAE+L2G,especially
on medium scale data sets, while achieving comparable training speed.
The paper is structured as follows. Section 2 introduces notation and dis-
cusses GAEs and the Local2Global framework by [15], including GAE+L2G.
Section 3 presents our method, L2G2G, as well as a time complexity analysis,
comparing it to GAE, FastGAE,and GAE+L2G. Section 4 provides experimen-
tal results on synthetic and real data sets, on networks of up to about 700,000Local-2-GAE-2-Global 3
Fig.1. L2G2L pipeline for two patches. The two patches are in blue and yellow, the
overlappingnodesbetweenthemingreen.Separatenodeembeddingsforeachpatchare
obtainedviaasingleGCN.ThedecoderalignstheembeddingsusingtheLocal2Global
synchronisationalgorithmtoyieldaglobalembeddingandthenusesastandardsigmoid
function. The GCN is then iteratively optimised using the training loss.
nodes.InSection5wediscusstheresultsandindicatedirectionsforfuturework.
The code is available at https://github.com/tonyauyeung/Local2GAE2Global.
2 Preliminaries
Notations: An undirected attributed graph G = (V,E,X) consists of a set of
nodes V of size N, a set of unweighted, undirected edges E of size M, and a
N ×F matrix X of real-valued node attributes (features). The edge set is also
represented by the N ×N adjacency matrix A. Moreover, based on the L2G
framework, we define a patch P to be a subgraph of G which is induced by a
subsetofthenodesetV;henceapatchP withthefeaturematrixcorresponding
i
to its nodes is denoted as (V(i),E(i),X(i)) Node embeddings are denoted as a
N ×e matrix Z, where e is the embedding size and σ is the sigmoid function.
Graph Autoencoders (GAEs): Given a graph G = (V,E,X), a GCN is used
to obtain an N × e embedding matrix Z = GCN(X,A), and a sparse ap-
proximation of the adjacency matrix through Aˆ = σ(ZZT). The GAE ob-
tains the embedding through minimising the cross-entropy reconstruction loss,
L (Aˆ,A)=Loss(Aˆ,A):=−(cid:80)N A logAˆ withrespecttotheparameters
GAE i,j=1 ij ij
of the GCN; this minimisation is also called training. Here a recursive method
called message passing is used. The decoder then computes an inner product
between each pair of node embeddings in the graph as proxy for the edge prob-
abilities. Even though GAEs outperform traditional node embedding methods,
such as spectral clustering [24] and DeepWalk [21], they usually scale poorly to
largegraphs.Thisisduetohavingtovisitalltheneighborsofanoderecursively
duringthemessagepassingphaseintheencodingGCN,andthedecoderscaling
as O(N2) in complexity. We highlight two approaches for improving scalability:4 R. Ouyang et al.
1.FastGAE[22]:Thismodeladdressesthescalabilityissuesbyreconstructing
the adjacency matrix of a sampled subgraph. This is achieved by evaluating an
approximatereconstructionloss(Loss )foreverysubgraphandaggregating
approx
theminonelosstobeoptimizedbythemodel.Thissamplingprocedurereduces
thecomputationcomplexityofdecodingduringeachtrainingepochfromO(N2)
to O(N2), where N is the number of nodes in the subgraph.
S S
2. Local2Global (L2G) [15]: This framework is a generic method to align
embeddings computed on different parts of the graph (potentially on different
machines and by different entities with different privacy constraints) into a sin-
gle global embedding, regardless of the embedding method, as follows. Suppose
that P ,...,P are k patches, which pairwise overlap on at least d nodes and at
1 k
most l nodes. It is assumed that the graph union of all patches gives the initial
graph G. The pattern of overlapping patches is captured in a so-called patch
graph, denoted G = (V ,E ), whose node set V = {P ,...,P } denote the
P P P P 1 k
patches. An edge between two nodes in G indicates that there is an overlap of
P
atleastdnodesintheinitialgraphGbetweenthosetwopatches.Then,foreach
patch P a node embedding matrix Z is computed using an embedding method
i i
of choice. When the embedding is obtained through a GAE, we refer to the
method as GAE+L2G. Local2Global then leverages the overlap of the patches
to compute an optimal alignment based on a set of affine transforms which
synchronizes all the local patch embeddings into a single and globally consis-
tent embedding, as follows. First, we estimate rotation matrices Sˆ ∈RF×F,j =
j
1,...,k, one for each patch. With M = (cid:80) X(i)(X(i))T we first esti-
ij u∈Pi∩Pj u u
mate the rotations between each pair of overlapping patches (P ,P ) ∈ E by
i j p
R =M (MTM )−1/2.NextwebuildR˜ =w R /(cid:80) |V(P )∩V(P )|toap-
ij ij ij ij ij ij ij j i j
proximately solve the eigen problem S =R˜S, obtaining Sˆ=[Sˆ ,...Sˆ ]. We also
1 k
find a translation matrix Tˆ = [Tˆ ,...,Tˆ ] by solving Tˆ = argmin||BT −C||2,
1 k 2
T∈Rk×F
where B ∈{−1,1}|Ep|×k is the incidence matrix of the patch graph with entries
B
(Pi,Pj),t
= δ
it
−δ jt, δ is the Kronecker Delta, and C ∈ R|Ep|×F has entries
C =
(cid:80) (cid:0) Zˆ(i)−Zˆ(j)(cid:1)
/|P ∩P |. This solution yields the estimated
(Pi,Pj) t∈Pi∩Pj t t i j
coordinates of all the nodes up to a global rigid transformation. Next, we apply
the appropriate rotation transform to each patch individually, Zˆ(j) = Z(j)SˆT,
j
thenapplythecorrespondingtranslationtoeachpatch(henceperformingtrans-
lation synchronisation), and finally average in order to obtain the final aligned
node embedding Z¯ =(cid:80) (Zˆ(j)+Tˆ )/|{j : i∈P }|.
i j i j j
3 Methodology
Local-2-GAE-2-Global Combining Local2Global and GAEs produces a scalable
GAE extension for node embeddings using autoencoders; using separate GAEs
for each of the patches allows specialization to the unique structure in each of
thepatches.OurLocal-2-GAE-2-Global(L2G2G)frameworkleveragesthesame
divide-and-conquer technique Local2Global capitalises on, but is designed andLocal-2-GAE-2-Global 5
adapted to the traditional GAE pipeline to benefit from its accuracy. The core
ideaofL2G2Gistoevaluateembeddingslocallyonthepatchesbutsynchronizing
thepatchembeddingsusingtheL2GframeworkwhiletrainingaGAE.Thisleads
to k GCNs encoding the k patches: Z = GCN(X(i),A(i)), for i = 1,...,k. To
i
accountforthedynamicupdateduringtrainingandadapttothelocaloptimiza-
tion scheme, we modify the GAE decoder to adjust the embeddings using the
Local2Global framework; hence the patch-wise decoder in L2G2G estimates the
edgeprobabilitiesbetweennodesinpatchesiandjbyσ((S Z +T )T(S Z +T )),
i i i j j j
where S =S (Z) and T =T (Z) are the Local2Global transformations of each
i i i i
of the patch embeddings.
In contrast to GAE+L2G, L2G2G synchronizes the embeddings before the
decoderstepandalsoperformssynchronizationsduringthemodeltraining,thus
taking full advantage of the patch graph structure during training. The cross-
entropy losses of each patch are aggregated to give a global loss function:
k
L
=(cid:88)
N L
(cid:16) Aˆ(j),A(j)(cid:17)
/N.
L2G2G j GAE
j=1
Similarly to the FastGAE algorithm, L2G2G reduces computation by only con-
sidering local structure. However, rather than training the network using only
the local information, L2G2G aggregates the local embeddings to reconstruct
the global information, thus boosting performance.
A schematic diagram of L2G2G is shown in figure 1, and pseudo-code for
L2G2G is given in algorithm 1. As computing the alignment step can be costly,
assuming that the Local2Global alignment would not change too quickly during
training, we update the rotation and translation matrices only every 10 epochs.
Algorithm 1 Local-2-GAE-2-Global (L2G2G): An overview
Require: P ,...,P , where P =(X(j),A(j))
1 k j
for e in [1,...,T] do
for j in [1,...k] do
Z ←GCN(X(j),A(j))
j
end for
Zˆ ,...,Zˆ ←Sync(Z ,...,Z )
1 k 1 k
L←0
for j in [1,...,k] do
Aˆ ←σ(Zˆ ZˆT)
j j j
L←L+ NjL (Aˆ(j),A(j))
N GAE
end for
Optimize encoder using L
end for
Complexity Analysis Following the computations in [7] and [15], we derive the
complexity of GAE, FastGAE, GAE+L2G and L2G2G. We assume that the6 R. Ouyang et al.
numberofnodes,edgesandfeaturessatisfyN,M,F ≥1,and,following[7],that
the dimensions of the hidden layers in the GCN are all F. Then, the complexity
of a L-layer GCN scales like O(LNF2 +LMF) and that of the inner product
decoderscaleslikeO(N2F).AE:maybeaddsomethinghereaboutfulldecoder?
Thus, for as shown in [7], for T epochs the time complexity of the decoder and
the encoder of a GAE scales like O(T(LNF2 + LMF + N2F)). In contrast,
√
as stated in [22], the complexity of per-epoch of FastGAE with a N down-
sampling size is O(LNF2+LMF +NF), and hence for T epochs the FastGAE
complexity scales like O(T(LNF2+LMF +NF)).
To simplify the complexity analysis of both Local2Global approaches we
assume that the overlap size of two overlapping patches in the patch graph
is fixed to d ∼ F. Following [15], finding the rotation matrix S scales like
O(|E |dF2)=O(|E |F3).Thetranslationproblemcanbesolvedbyat-iteration
p p
solverwithacomplexityperiterationofO(|E |F),wheretisfixed.Toalignthe
p
local embeddings, one has to perform matrix multiplications, which requires
O(N F2) computations, where N is the number of nodes in the jth patch. The
j j
complexityoffindingtherotationmatrix(O(|E |F3))dominatesthecomplexity
p
of the computing the translation (O(|E |F)). Thus, the complexity of the L2G
p
algorithm with k patches is
O(cid:0)
|E
|F3+F2(cid:80)k
N
(cid:1)
.
p j=1 j
The GAE+L2G algorithm uses a GAE for every patch, and for the jth
patch,forT trainingepochstheGAEscaleslikeO(T(LN F2+LM F+N2F)),
j j j
with M number of edges in the jth patch. Summing over all patches and
j
(cid:80)
ignoring the overlap between patches as lower order term, so that N =
j j
O(N), (cid:80) N2 ≈ N2/k, and (cid:80) M ≈ M, the GAE+L2G algorithm scales like
j j j j
O(cid:0)
TF (LNF +LM
+N/k)+kF3(cid:1)
. For the complexity of L2G2G, as L2G2G
aligns the local embeddings in each epoch rather than after training, we re-
place kF3 + NF2 with T(kF3 + NF2), and thus the algorithm scales like
(cid:16) (cid:16) (cid:17)(cid:17)
O T LNF2+LMF + N2F +kF3 . In the PyTorch implementation of Fast-
k
GAE,thereconstructionerrorisapproximatedbycreatingtheinducedsubgraph
√
fromsampling⌊ N⌋proportionaltodegree,withanexpectednumberofatleast
O(M/N)edgesbetweenthem.Then,thecomputationofthedecoderis(atleast)
O(M/N) instead of O(N2). Table 1 summarises the complexity results.
Model GeneralTimeComplexity PyTorchimplementation
GAE O(cid:0)TF(LNF+LM+N2)(cid:1) O(TF(LNF+LM+M))
FastGAE ≥O(TF(LNF+LM+N)) O(TF(LNF+LM+M/N))
GAE+L2GO(cid:16) TF(L(NF+M)+N2)+kF3(cid:17) O(cid:0)TF(LNF+LM+M)+kF3(cid:1)
k
L2G2G O(cid:16) TF(L(NF+M)+N2 +kF3)(cid:17) O(cid:0) TF(LNF+LM+M+kF3)(cid:1)
k
Table 1. Complexity comparison in the general and in the sparse case .
Thus, in the standard case, increasing the number of patches k reduces the
complexityofthecomputationoftheGAEdecoders.InthePyTorchimplemen-
tation,ifkscaleslinearlywithN,theexpressionislinearinN.Incontrast,when
thenumberofnodesN isnotverylarge,thenumberoffeaturesF becomesmoreLocal-2-GAE-2-Global 7
prominent,sothatthetrainingspeedmaynotnecessarilyincreasewithincreas-
ing number of patches. Table 1 shows that L2G2G sacrifices
O(cid:0) TkF3(cid:1)
training
time to obtain better performance; with an increase in the number of patches,
the training speed gap between L2G2G and GAE+L2G increases linearly.
4 Experimental Evaluation
Datasets To measure the performance of our method, we compare the ability of
L2G2Gtolearnnodeembeddingsforgraphreconstructionagainstthefollowing
benchmark datasets Cora ML, Cora [3], Reddit [26] and Yelp [26].
Inaddition,wetestedtheperformanceofL2G2Gonfoursyntheticdatasets,
generatedusingaStochasticBlockModel(SBM)whichassignsnodestoblocks;
edges are placed independently between nodes in a block with probability p
in
andbetweenblockswithprobabilityp [17].Weencodetheblockmembership
out
as node features; with L blocks, v being in block l is encoded as unit vector
e ∈ {0,1}L. To test the performance across multiple scales we fix the number
l
of blocks at 100, and vary the block size, p and p , as follows:
in out
1. ‘SBM-Small’ with block sizes 102 and (p ,p )=(0.02,10−4),
in out
2. ‘SBM-Large-Sparse’ with block sizes 103 and (p ,p )=(10−3,10−4),
in out
3. ‘SBM-Large’ with blocks of sizes 103 and (p ,p )=(0.02,10−4),
in out
4. ‘SBM-Large-Dense’ with block sizes 103 and (p ,p )=(0.1,0.002).
in out
Table 2 gives some summary statistics of these real and synthetic data sets.
Stochastic block model Real Data
Small Large-Sparse Large Large-DenseCora ML Cora Reddit Yelp
N 10,000 100,000 100,000 100,000 2,995 19,793 232,965 716,847
M 104,485 99,231 1,493,135 14,897,099 16,316 126,84223,213,83813,954,819
F 100 100 100 100 2,879 8,710 602 300
Table 2. Network data statistics: N = no.nodes, M = no.edges, F =no.features
Experimental setup and Results To assess whether L2G2G is a scalable alterna-
tive for the use of a GAE without having to sacrifice accuracy in downstream
tasks, we compare it against the standard GAE [19], GAE+L2G [15] and Fast-
GAE[22].Wetrainthemodelsoneachdatasetfor200epochs,withlearningrate
0.001 and the Adam optimizer [18], and two layers in the GCN. The dimension
of the first hidden layer is 32 and the dimension of the last layer is 16. We run
each experiment 10 times with different random seeds for each model on each
dataset.AlltheexperimentswereconductedonaV100GPU.Wethencompare
the methods using the Area Under the Curve (AUC) and the Average Precision
(AP). Following [15], we test our algorithm with fixed patch size 10.
Table 3 shows that L2G2G outperforms both FastGAE and GAE+L2G
on most experiments. Having established the theoretical training speed gain
of L2G2G, these results illustrate that L2G2G can perform better than the
GAE+L2G, as well as achieve comparable training speed. AE: do we need to8 R. Ouyang et al.
Average Performance On Different Datasets (AUC in %)
GAE FastGAE GAE+L2G L2G2G
Cora ml 95.95 ± 0.42 83.90 ± 1.10 90.25 ± 0.19 92.58 ± 0.35
SBM-small 95.32 ± 0.18 76.34 ± 0.57 93.84 ± 0.14 95.39 ± 0.21
Cora 96.07 ± 0.09 81.78 ± 0.76 90.59 ± 0.11 94.96 ± 0.26
SBM-Large-sparse 94.88 ± 0.23 80.89 ± 0.84 94.73 ± 0.07 95.02 ± 0.23
SBM-Large 86.84 ± 0.11 70.90 ± 1.29 84.60 ± 0.10 86.62 ± 0.25
SBM-Large-dense 64.07 ± 0.12 65.20 ± 0.94 65.45 ± 0.05 65.88 ± 0.03
Reddit 88.69 ± 0.57 79.99 ± 1.31 88.50 ± 0.23 88.37 ± 0.39
Yelp 86.55 ± 0.28 73.79 ± 6.54 85.82 ± 0.17 84.01 ± 0.11
Average Performance On Different Datasets (AP in %)
GAE FastGAE GAE+L2G L2G2G
Cora ml 95.37 ± 0.57 83.90 ± 1.10 90.57 ± 0.19 92.41 ± 0.39
SBM-small 95.23 ± 0.12 76.34 ± 0.57 95.22 ± 0.11 95.71 ± 0.24
Cora 95.76 ± 0.14 81.78 ± 0.76 90.50 ± 0.15 94.67 ± 0.29
SBM-Large-sparse 95.26 ± 0.24 80.89 ± 0.84 95.88 ± 0.07 95.44 ± 0.26
SBM-Large 89.64 ± 0.21 70.90 ± 1.29 87.35 ± 0.11 89.34 ± 0.33
SBM-Large-dense 67.64 ± 0.30 65.20 ± 0.94 71.25 ± 0.06 72.08 ± 0.05
Reddit 88.16 ± 0.60 79.99 ± 1.31 88.40 ± 0.18 88.57 ± 0.40
Yelp 86.73 ± 0.29 73.79 ± 6.54 85.26 ± 0.12 83.56 ± 0.11
Table 3.Experimentsondifferentdatasetswithpatchsize10.Bold:thebestamong
the fast methods, underlined: the model outperforms the GAE.
clarifywithnewpytorchstuff?Furthermore,incontrasttoFastGAEweobserve
that the performance of L2G2G and of the GAE are very close to each other
on the medium and large-scale data sets, indicating that L2G2G does not lose
much performance compared to the much slower but assumed more accurate
classic GAE. Furthermore, L2G2G even outperforms the GAE when the data
set is large and dense, such as SBM-Large-dense and Reddit.
Figure 2 shows a comparison of the training time of the models, as well as
the changes of training speed as the data set size increases (on the log scale). It
is worth mentioning that we are not accounting for the run time of the graph
clustering. The results show that the training speed of L2G2G and GAE+L2G
areverycloseonbothsmallandlargescaledatasets.Althoughthegapbetween
the training speed of L2G2G and that of GAE+L2G increases for large-scale
datasets,L2G2Gstillachieveshightrainingspeed,andisevennotmuchslower
that FastGAE while achieving much better performance. In almost all cases,
L2G2GisfasterthanthestandardGAE,exceptforthetwosmallerdatasets.Its
training time is around an order of magnitude smaller per epoch for the larger
models.Asanaside,GAEssufferfrommemoryissuesastheyneedtostorevery
large matrices during the decoding step.
Ablation Study Here we vary the number of patches, ranging from 2 to 10.
Figure 3 shows the performance changes with different number of patches forLocal-2-GAE-2-Global 9
Fig.2. Training time of the baseline models(GAE, FastGAE and GAE+L2G) and
L2G2G on benchmark data sets (excluding partitioning time). Note that the y-axis is
on a log-scale, and thus the faster methods are at least an order of magnitude faster.
each model on each data set. When the patch size increases, the performance
of L2G2G decreases less than GAE+L2G. This shows that updating the node
embeddings dynamically during the training and keeping the local information
with the agglomerating loss actually brings stability to L2G2G. Moreover, we
Cora ML Cora SBM-small
SBM-Large Reddit Yelp
Fig.3.LineplotsoftheROCscoreandaccuracyofL2G2GandGAE+L2G,trainedon
each dataset, with different patch sizes. For each subplot, the blue lines represent the
metricsforL2G2G,whiletheorangeonesrepresentthoseforGAE+L2G.Theshadows
in each subplot indicate the standard deviations of each metric.10 R. Ouyang et al.
have explored the behaviour of training time for L2G2G when patch size in-
creasesfrom2to30,onbothasmall(Cora)andalarge(Yelp)dataset.Figure4
shows that on the small-scale data set Cora, the gap in training speed between
L2G2G and GAE+L2G remains almost unchanged, while on Yelp, the gap be-
tween L2G2G and GAE+L2G becomes smaller. However, the construction of
the overlapping patches in the Local2Global library can create patches that are
much larger than N/k, potentially resulting in a large number of nodes in each
patch.Hence,thetrainingtimeinourtestsincreaseswiththenumberofpatches.
CPU GPU
Fig.4. Training time (excluding partitioning) of L2G2G and GAE+L2G on Cora
(Top) and Yelp (Bottom), while varying patch size with CPU results presented on
the left and GPU results presented on the right. The x axis is shown in log scale.
Since all the computations in Local2Global library built by [15] are carried
out on the CPU, the GPU training can be slowed down by the memory swap
betweenCPUandGPU.Thus,tofurtherexplorethebehaviourofouralgorithm
when the number of patches increases, we ran the test on both CPU and GPU.
The results are given by Figure 4. This plot illustrates that the GPU training
time of L2G2G increases moderately with increasing patch size, mimicking the
behaviourofGAE+L2G.Incontrast,theCPUtrainingtimeforthesmallerdata
set(Cora)decreaseswithincreasingpatchsize.ThelargerbutmuchsparserYelp
data set may not lend itself naturally to a partition into overlapping patches.
Summarising, L2G2G performs better than the baseline models across most
settings, while sacrificing a tolerable amount of training speed.
aroC
pleYLocal-2-GAE-2-Global 11
5 Conclusion and Future Work
In this paper, we have introduced L2G2G, a fast yet accurate method for ob-
taining node embeddings for large-scale networks. In our experiments, L2G2G
outperforms FastGAE and GAE+L2G, while the amount of training speed sac-
rificed is tolerable We also find that L2G2G is not as sensitive to patch size
change as GAE+L2G.
Future work will investigate embedding the synchronization step in the net-
work instead of performing the Local2Global algorithm to align the local em-
beddings. This change would potentially avoid matrix inversion, speeding up
the calculations. We shall also investigate the performance on stochastic block
models with more heterogeneity.
To improve accuracy, one could add a small number of between–patch losses
into the L2G2G loss function, to account for edges which do not fall within
a patch. The additional complexity of this change would be relatively limited
when restricting the number of between–patches included. Additionally, the Lo-
cal2Globallibraryfrom[16]isimplementedonCPU,losingspeedduetomoving
memory between the CPU and the GPU. We will investigate re-implementing
the Local2Global algorithm on a GPU.
References
1. Baldi, P.: Autoencoders, unsupervised learning, and deep architectures. In:
I. Guyon, G. Dror, V. Lemaire, G. Taylor, D. Silver (eds.) Proceedings of ICML
WorkshoponUnsupervisedandTransferLearning,ProceedingsofMachineLearn-
ing Research, vol. 27, pp. 37–49. PMLR, Bellevue, Washington, USA (2012)
2. Bayer, A., Chowdhury, A., Segarra, S.: Label propagation across graphs: Node
classification using graph neural tangent kernels. In: ICASSP 2022-2022 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 5483–5487. IEEE (2022)
3. Bojchevski, A., Gu¨nnemann, S.: Deep gaussian embedding of graphs: Unsuper-
vised inductive learning via ranking. In: International Conference on Learning
Representations (2018). URL https://openreview.net/forum?id=r1ZdKJ-0W
4. Bojchevski, A., Klicpera, J., Perozzi, B., Kapoor, A., Blais, M., Ro´zemberczki,
B.,Lukasik,M.,Gu¨nnemann,S.:Scalinggraphneuralnetworkswithapproximate
PageRank. In: Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining. ACM (2020)
5. Bruna, J., Zaremba, W., Szlam, A., LeCun, Y.: Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203 (2013)
6. Chen,J.,Ma,T.,Xiao,C.:Fastgcn:fastlearningwithgraphconvolutionalnetworks
via importance sampling. arXiv preprint arXiv:1801.10247 (2018)
7. Chen, M., Wei, Z., Ding, B., Li, Y., Yuan, Y., Du, X., Wen, J.: Scalable graph
neural networks via bidirectional propagation. CoRR abs/2010.15421 (2020).
URL https://arxiv.org/abs/2010.15421
8. Chen, M., Wei, Z., Huang, Z., Ding, B., Li, Y.: Simple and deep graph convolu-
tionalnetworks.In:InternationalConferenceonMachineLearning,pp.1725–1735.
PMLR (2020)12 R. Ouyang et al.
9. Chiang, W.L., Liu, X., Si, S., Li, Y., Bengio, S., Hsieh, C.J.: Cluster-GCN. In:
Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. ACM (2019)
10. Cucuringu,M.,Lipman,Y.,Singer,A.:Sensornetworklocalizationbyeigenvector
synchronization over the euclidean group. ACM Trans. Sen. Netw. 8(3) (2012)
11. Cucuringu,M.,Singer,A.,Cowburn,D.:Eigenvectorsynchronization,graphrigid-
ity and the molecule problem. Information and Inference 1(1), 21–67 (2012)
12. Hamilton, W., Ying, Z., Leskovec, J.: Inductive representation learning on large
graphs. Advances in Neural Information Processing Systems 30 (2017)
13. Hamilton, W.L.: Graph representation learning. Morgan & Claypool Publishers
(2020)
14. He, C., Balasubramanian, K., Ceyani, E., Rong, Y., Zhao, P., Huang, J., An-
navaram, M., Avestimehr, S.: Fedgraphnn: A federated learning system and
benchmark for graph neural networks. CoRR abs/2104.07145 (2021). URL
https://arxiv.org/abs/2104.07145
15. Jeub,L.G.,Colavizza,G.,Dong,X.,Bazzi,M.,Cucuringu,M.:Local2global:adis-
tributedapproachforscalingrepresentationlearningongraphs. MachineLearning
112(5), 1663–1692 (2023)
16. Jeub, L.G.S.: Local2global github package. Github https://github.com/LJeub/
Local2Global (2021)
17. Karrer,B.,Newman,M.E.J.:Stochasticblockmodelsandcommunitystructurein
networks. Physical Review E 83(1) (2011)
18. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR
(Poster) (2015)
19. Kipf, T.N., Welling, M.: Variational graph auto-encoders. arXiv preprint
arXiv:1611.07308 (2016)
20. Pan, Q., Zhu, Y.: Fedwalk: Communication efficient federated unsupervised node
embedding with differential privacy. In: Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining, KDD ’22, p. 1317–1326.
AssociationforComputingMachinery,NewYork,NY,USA(2022). DOI10.1145/
3534678.3539308. URL https://doi.org/10.1145/3534678.3539308
21. Perozzi, B., Al-Rfou, R., Skiena, S.: DeepWalk. In: Proceedings of the 20th ACM
SIGKDDinternationalconferenceonKnowledgediscoveryanddatamining.ACM
(2014)
22. Salha,G.,Hennequin,R.,Remy,J.B.,Moussallam,M.,Vazirgiannis,M.:Fastgae:
Scalablegraphautoencoderswithstochasticsubgraphdecoding. NeuralNetworks
142, 1–19 (2021)
23. Simonovsky,M.,Komodakis,N.:Graphvae:Towardsgenerationofsmallgraphsus-
ingvariationalautoencoders.In:ArtificialNeuralNetworksandMachineLearning–
ICANN2018:27thInternationalConferenceonArtificialNeuralNetworks,Rhodes,
Greece, October 4-7, 2018, Proceedings, Part I 27, pp. 412–422. Springer (2018)
24. Tang, L., Liu, H.: Leveraging social media networks for classification. Data Min.
Knowl. Discov. 23(3), 447–478 (2011)
25. Tsitsulin, A., Palowitch, J., Perozzi, B., Mu¨ller, E.: Graph clustering with graph
neural networks. Journal of Machine Learning Research 24(127), 1–21 (2023)
26. Zeng, H., Zhou, H., Srivastava, A., Kannan, R., Prasanna, V.: Graphsaint: Graph
samplingbasedinductivelearningmethod. In:InternationalConferenceonLearn-
ing Representations (2020)
27. Zhang, M., Chen, Y.: Link prediction based on graph neural networks. Advances
in Neural Information Processing Systems 31 (2018)Local-2-GAE-2-Global 13
28. Zhang, S., Tong, H., Xu, J., Maciejewski, R.: Graph convolutional networks: a
comprehensive review. Computational Social Networks 6(1), 1–23 (2019)
29. Zou,D.,Hu,Z.,Wang,Y.,Jiang,S.,Sun,Y.,Gu,Q.:Layer-dependentimportance
sampling for training deep and large graph convolutional networks. Advances in
neural information processing systems 32 (2019)