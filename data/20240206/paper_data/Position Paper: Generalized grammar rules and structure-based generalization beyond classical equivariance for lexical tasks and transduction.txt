Position Paper: Generalized grammar rules and structure-based generalization
beyond classical equivariance for lexical tasks and transduction
MirceaPetrache1 ShubhenduTrivedi2
Abstract tional generalization (Chomsky, 1957; Montague, 1970;
Compositionalgeneralizationisoneofthemain Fodor&Pylyshyn,1993;Partee,1995;Lakeetal.,2019)—
propertieswhichdifferentiateslexicallearningin theabilitytoprocessanovelsentenceandassignitoverall
humans from state-of-art neural networks. We meaning(s) by composing the meanings of its individual
propose a general framework for building mod- parts—which,asthenamesuggests,reliesonthecomposi-
elsthatcangeneralizecompositionallyusingthe tionalityoflanguageitself. Aconsequenceistheabilityto
conceptofGeneralizedGrammarRules(GGRs), comprehendaninfinitearrayofnewsentencesfromonlya
a class of symmetry-based compositional con- finitevocabulary(Chomsky,1957;2016).
straints for transduction tasks, which we view
Whetherneuralnetworkmodelscanexhibitcompositional
as a transduction analogue of equivariance con- generalization has been a (sometimes heated) topic of de-
straints in physics-inspired tasks. Besides for- bate for decades, acquiring a somewhat prominent place
malizing generalized notions of symmetry for
in the history of AI and cognitive science in the pro-
languagetransduction,ourframeworkisgeneral cess (Pollack, 1990; Fodor&Pylyshyn, 1993; Marcus,
enough to contain many existing works as spe-
2001;Johnson, 2004;Lake&Baroni, 2018; Hupkesetal.,
cialcases. WepresentideasonhowGGRsmight 2019). It is only fairly recently that deep learning meth-
beimplemented,andintheprocessdrawconnec- odshavebeenabletodeliverunprecedentedresultsinlan-
tionstoreinforcementlearningandotherareasof
guage modeling. However, such performanceis provided
research. by extremely large, sample inefficient models. Further,
suchmodelsdonotusuallygeneralizetoexamplesthatre-
quire the use of language compositionality (Hupkesetal.,
1. Introduction
2019;Dzirietal.,2023). Ithasbeenarguedthattheinabil-
Humansareabletoeffortlesslyprocesssentencesthatthey ity of neural networks to leverage compositionality limits
have never encountered before. This is true not just for themfromtruenaturallanguageunderstanding(Westetal.,
meaningful sentences which contain words whose mean- 2023;Donatelli&Koller,2023;Marcusetal.,2023). Itis
ings are not known, but also for nonsense sentences pep- thusnaturalthatwewouldwantneuralnetworkmodelsto
peredwith noncewords. Suchsentencesare semantically exhibitcompositionalgeneralization,bringingthemcloser
meaningless, but can still be evocative from their syntax. totrueunderstandingthanmostlymemorization.
Consider the example provided by (Ingraham, 1903): the
Recent work has started to focus on the idea of building
gostakdistims the doshes. On the surface, the sentence is
compositional generalization into neural networks using
meaningless,butas(Ingraham,1903)pointsout: “Youdo
ideasfromsymmetry,morespecifically,usinggroupequiv-
notknowwhatthismeans;nordoI.Butifweassumethat
ariance (Gordonetal., 2019; White&Cotterell, 2022;
it is English, we know that the doshes are distimmed by
Lietal.,2022;Akyürek&Andreas,2023).
the gostak. We know too that one distimmer of doshes is
a gostak. If, moreover,the doshesare galloons, we know We frame our paper as a position piece, arguing that the
thatsomegalloonsaredistimmedbythegostak.Andsowe notionofgroupequivarianceisnotsufficientforcomposi-
maygoon.” tional generalization in language; we must examine sym-
metrynotionsbeyondthecaseofactionsbygroups. More
This remarkable cognitive ability is a result of composi-
specifically, in the direction of “imposing grammar-like
1Department of Mathematics and Institute for Math. and
rules”fortransduction,whichwecallGeneralizedGram-
Comp. Engineering,PUCChile2shubhendu@csail.mit.edu. Cor- mar Rules (GGR). We elucidate our position by consid-
respondenceto: MirceaPetrache<mpetrache@mat.uc.cl>,Shub- eringthetaskoftransductionbetween(formal)languages,
henduTrivedi<shubhendu@csail.mit.edu>. such that a formal notion of “meaning” remains invari-
ant. We propose a general framework where generalized
Preprint.
1
4202
beF
2
]LC.sc[
1v92610.2042:viXraGeneralizedgrammarrulesandstructure-basedgeneralizationbeyondclassicalequivariance
grammar rules serve the same role vis-à-vis symmetry in 2019). Themajorityofsuchworks(Lake&Baroni,2018;
transductionasgroupactionsdo inequivariantneuralnet- Gordonetal., 2019; Hewittetal., 2020; Hupkesetal.,
works(Cohen&Welling,2016). We alsoshowthatmany 2020;Dankersetal.,2022;White&Cotterell,2021;2022)
worksinlanguagecompositionalityalreadyexploitthisun- have focused on analyzing sequence-to-sequence model
derlying notion in restricted settings. We argue that for- performanceontoydatasets.Inparticular,(Lake&Baroni,
malizingsuchgeneralizednotionsofsymmetrywillpermit 2018) (followed by (Ruisetal., 2020)), presented a
a more structured and principled framework to facilitate dataset with Simplified version of the CommAI Naviga-
buildingneuralnetworksthatexhibitbetter compositional tion (SCAN) tasks. While the dataset has proven to
generalization. The situation is analogousto groupequiv- be influential for benchmarking compositional general-
ariant networks (Cohen&Welling, 2016), where present- ization of sequence-to-sequence models, (Bastingsetal.,
ing the right language and framework (Kondor&Trivedi, 2018) has shown that an off-the-shelf model can per-
2018; Bekkers, 2020) led to a noticeable acceleration in form perfectly on SCAN without compositional general-
the development of high performance networks. Further, ization. (Bastingsetal., 2018) also proposed a comple-
as(Akyürek&Andreas, 2023) pointout, formalizationof mentary dataset to remedy issues with SCAN. On the
general notions of symmetries will also give a better han- otherhand,(Valvodaetal.,2022)arguedagainstusingtoy
dle on principled augmentation procedures (via automata datasets, instead proposing the use of artificial string-to-
ratherthansymmetrygroups). stringdatasets,generatedbysamplingsubsequentialfinite-
state transducers (SFSTs). The contention is that by con-
However, unlike in the case of group equivariance, we
trollingtheformalpropertiesofSFSTsusedtogeneratethe
cannot usually assume that the underlying symmetries
datasets, one could get a better measure of compositional
in the language case are known, because outside a few
generalization.
specific tasks such as formal language translation or toy
language transduction, rules are not explicit and may In an influential work (Gordonetal., 2019) considered
have many exceptions. Thus, we propose an error quan- the hypothesis that language compositionality can be un-
tification for specific grammar rules, akin to approxi- derstood as a form of group equivariance, construct-
mateequivarianceerrorquantificationinclassicalequivari- ing a group-equivariant network exploiting local permu-
ance (Petrache&Trivedi, 2023; Huangetal., 2023). We tation symmetries that could solve most SCAN tasks.
presentour positionunderthe hypothesisthatsuch an un- (White&Cotterell, 2022) go a step further, and build a
derlyingsymmetrycanbeapproximatedandthatageneral more expressive group-equivariant model that also incor-
modelforlanguagetransducersareautomata-liketransduc- poratesagroup-invarianthardalignmentmechanism.Both
tionmodels. oftheseworksacknowledgethelimitationsofusinggroup
equivariance in SCAN-like tasks and conjecture that ex-
While“learningtheunderlyinggrammarruleequivariance"
ploitingmore generalsymmetrieswill bringaboutfurther
(learningGGRs)iscomputationallydemanding,wepresent
gains in real world NLP scenarios. A similar message is
ideasonhowitcouldbelearntinprincipleusingreinforce-
echoed by (Akyürek&Andreas, 2023) who use a compo-
mentlearning(RL)viaMarkovdecisionprocesses(MDPs)
sitional adaptation of group symmetries, as a principled
and generalizations. In this sense, we show that incorpo-
mechanismfordataaugmentation.
ratingsymmetryin grammar-basednaturallanguagetasks
faces the same computational challenges as general RL Additionally, there are several works that take the route
tasks, indicating strong links between learning the under- of program induction, meta-learning etc. for composi-
lyingautomataequivariance,andRLtheory. However,we tional generalization (Nyeetal., 2020; Qiuetal., 2021;
alsonotethatwhileautomatainhabitalargeuniverse,nat- Akyüreketal., 2020; Conklinetal., 2021; Chenetal.,
urallanguagegrammarismuchmorerestricted(Chomsky, 2020; Csordásetal., 2021), without highlightingthe anal-
1957;Montague,1970),evenifitmaynotseemsoatfirst ogytogroup-equivariance.Itmustbenoted,however,that
sight. "Learning automata-symmetries" applied in NLP grammaticalinference(Angluin&Smith, 1983) is known
willallowtodetectahierarchyofsymmetries,thatcanbe tohavenouppercomplexitybound(Daley&Smith,1986),
usedtostructuremorecomplexandrealisticlanguagecom- thusweneedtorestricttorealisticscenarios.
positionality/systematicityattemptsinthefuture.
Motivated by the message of (Gordonetal.,
2019; White&Cotterell, 2022; Lietal., 2022;
1.1.RelatedWorks
Akyürek&Andreas, 2023), as well as the successes
Several groups have recently probed the ability of ex- of incorporating symmetry in computer vision and the
isting neural networks to exhibit compositional general- physical sciences via group equivariant networks, we
ization. Relevant works include (Johnsonetal., 2016; presentthisworkasaninvitationtoextendtheconceptof
Bahdanauetal., 2018; Loulaetal., 2018; deVriesetal., symmetriestolanguageinaunifiedframework.
2Generalizedgrammarrulesandstructure-basedgeneralizationbeyondclassicalequivariance
2. Transduction between Languages Recall that, by definition, a formal language L is
recognized by a Turing machine M if the strings
AbasicnotioninNLPisthatoftransductionwhich,ata
on which M halts form L i.e. L = {x ∈
high level, involves two (formal or natural) languages L
1 Σ∗|Mhaltswhen < x > isgivenasaninput}. Classes
andL andconsistsof learninga mappingT : L → L
2 1 2 of formal grammars across the Chomsky-Schützenberger
that preserves some property of interest, such as mean-
hierarchy (Chomsky&Schützenberger, 1963) can be de-
ing. Transductive NLP tasks are very diverse, including
finedthroughclasses ofTMsguaranteedto recognizeany
transliteration,translation,speechrecognition,spellingcor-
formal language. Such languagesinclude the larger class
rection, temporal phrase normalization, prediction of sec-
ofType-0grammars. LanguagesnotdefinableusingTMs
ondaryproteinstructurefromaninputsequenceetc. Anin-
are uncomputable. Thereexistinterpretableand wellun-
timatelyrelatednotionisthatofafinite-statetransducer,
derstood bijections as below,whose restrictions to classes
which is an abstract automaton that takes an input string
fromtheChomskyhierarchyfiltrationofformalgrammars
and instead of simply accepting it, convertsit into an out-
arewellcharacterized(Hopcroft&Ullman,1979):
putstring accordingto some prescribedrules. Before dis-
cussing transducers more formally, we discuss more ba- {Turingmachines} a −cc →epts {FormalLanguages},
sic notionspertainingto formallanguagesandTuringma-
generates
{FormalLanguages} ←− {Formalgrammars}.
chines.
Formaltransducerscouldbethoughtofasmoreflexible
2.1.TuringMachinesandFiniteStateAutomata extensions of TMs in which the condition Σ ⊆ Γ is re-
laxed. We can still represent transducers by graphs as in
Recall that a Turing Machine (TM) is defined by a 6-
thecaseofTMsasdiscussedearlier,butwiththelabelsof
tupleM = (S,Σ,Γ,s ,H,δ), withthefollowingrequire-
0 edgesenrichedviaoutputalphabetsymbols.IfT isatrans-
ments. S is a finite set of internal states; Σ is a finite
input alphabet,notcontainingthepointersymbol⊲, nor ducer with input/output languages L 1,L 2, the associated
the blanksymbol(cid:3), nor left/rightshiftsymbols←,→; Γ transductionfunctioncanstillbedenotedbyT :L 1 →L 2,
withaslightabuseofnotation.
is a finite tape (or output) alphabet, not containing ←
and →, and such that Σ∪{⊲,(cid:3)} ⊆ Γ; s ∈ Σ is a spe- Asstatedearlier,transductioninvolvesamappingbetween
0
cial state called the start state; H ⊆ S are the halting twolanguageswhichpreservesa propertyofinterestsuch
states;finally,δ denotestheso-calledtransitionfunction as meaning. One way of formalizing such properties is
δ : (S \ H) × Γ → S × (Σ ∪ {←,→}) such that the via Semantics. Traditionalsemantics (Carnap, 1959) con-
tape head never erases the ⊲ symbol. The transition ta- siders predicates P(~x) depending on a set of variables ~x
ble of a TM can be encodedin a directed graph over ver- whose"interpretations"aremappingsI(~x)ofthevariables
tex set S, which has one edge (s,s′) for every choice of tocounterpartsinworldmodels,andthemeaningofP(~x)
γ,σ′,a ∈ Γ×(Σ ∪{←,→}) so that δ(s,γ) = (s′,σ′), isdefinedasthesetofinterpretationsI(~x)forwhichP(~x)
andinthiscasetheedge(s,s′)haslabel(γ,σ′). Notethat holds true. In a "dual" view, more suitable to our presen-
finite state automata (FSA) such as deterministic finite tation(andwithcomparableexpressivepower),"meaning"
automata,arespecialcasesofTMs,beingarestrictedtype canbeformalizedastheassignmentL→L ofsemantic
M
of Turing machine without access to a “scratch” memory. values (SV) to sentencesfrom a formallanguage L. In a
That is, the tape headcan onlyperform“read”operations universalistviewL wouldencodeallsetsofpossiblein-
M
andalwaysmovesfromlefttoright. The“memory”thata terpretations, but in more concrete cases it can boil down
finitestatemachinepossessesisentirelyaccountedbywhat topropertiesrelevanttoatasksuchase.g.syntacticproper-
stateitisin. Finally,notethattheterminology“finitestate tiesofasentence.Itisnaturaltoconsiderthatthesemantic
automata” can cause some confusion since TMs can also value language L also has a formal language structure,
M
only have a finite number of states. However, due to the linkedtothatofthesourcelanguageLviaatransducer.
presenceoftapememory,TMscanhaveaninfinitenumber
Whenintroducingcompositionalsymmetrieslater,theidea
ofpossibleconfigurations,whichisnotthecaseforFSAs.
that "transduction symmetries preserve meaning" (or se-
mantic values) is an importantprinciple, even if this does
2.2.Transductionbetweenformallanguages
not appear in the implementation. This is why we spend
We willmainlyconcentrateontransductiontasksbetween sometimetoformalizetheabovenotionfurther.
formal languages, i.e. languages generated by formal
Grammars over semantic values. As mentioned above,
grammars, which by definition are finite collections of
westartbypostulatingtheexistenceofa"grammarofpossi-
production rules. In particular, we are most interested in
blemeanings"G ,generatinglanguageL ,whosealpha-
M M
the class of languagesgeneratedby the so-called context-
betΣ correspondsto"atomic"SVvalues. Then,atrans-
M
sensitive(a.k.a.Type-1)grammars.
ductiontaskbetweenlanguagesL andL canbethought
1 2
3Generalizedgrammarrulesandstructure-basedgeneralizationbeyondclassicalequivariance
ofasaccompaniedby"interpretation"(orsemanticassign- A particular case is that ρ (g) is the identity of Y for all
Y
ment)transducersI ,I withinputlanguagesL ,L , g ∈ G,inwhichcasef isG-invariant: f ◦(ρ (g)) = f
1,M 2,M 1 2 X
and outputlanguageL . Such a transducerby definition forallg ∈G.
M
assignsSVstosentencesfromL . ThenatransducerT :
i 12 The notion of equivariance has led to the development
L → L "preserves meaning" if T ◦ I = I .
1 2 12 2,M 1,M of equivariantneural networks, a fruitful area of research
ThismeansthatforeachsentencefromL if we assign it
1 within machine learning, which has been especially suc-
aninterpretationbyI orfirstapplyT andthenassign
1,M 12 cessfulin applicationsin the physicalsciences. The main
theresultaninterpretationbyI ,wegetthesameresult.
2,M benefitsofequivariantneuralnetworksstem fromthefact
that they encode a clear symmetry of maps from T ,T
X Y
Remark 2.1. "Universal"interpretationfunctionsI
i,M
are
explicitlyintothemodellayerstructure. Thus,theyenable
usually notexplicitly known, and hard to encodeusefully
encoding label functions more efficiently. Crucially, eqs.
for realistic scenarios. Thus, it must be emphasized that
(3.1),(3.2)arethusnotthemainreasonofinterestinequiv-
the described setup does not postulate the absolutist view
ariance,asisthefactthatstructuralresultsfromgrouprep-
that "there exists a universal meaning", instead we take
resentationtheoryandthealgebraofconvolutions,furnish
a relativist/pragmatiststand, andconsiderad-hocversions
atoolboxforconstructingmapsfromT ,T efficiently.
X Y
L ,I foreachgivenscenario.
M i,M
3.2.GeneralizedGrammarRulesasSymmetriesin
3. Symmetries inLanguage Transduction Transduction
Intheprevioussection,wediscussedtransductionbetween The notion of "language symmetries" that we propose
formallanguages, alongwith a formalnotionof meaning, hasthefollowingcrucialpropertyincommonwithgroup-
which stays invariant during the transductive process. In equivariancesymmetries: The structure of the underlying
this section, we use this meaning invariance as a start- taskpermitsonetoencode,inashortandformallydefined
ing off point to describe general “language symmetries,” form,alargesetoftransformationsthatpreservethetruth
while also comparing them to symmetries discussed in valuesof the task beinginvestigated,which in turnare in-
classical group action contexts, which is the bread and terpretableasstructuralpropertiesofthetask. Wepropose
butter of equivariant networks (Cohen&Welling, 2016; twowaysofformalizinglanguagesymmetries: Usingquo-
Kondor&Trivedi, 2018). As the reader is more likely to tient operations over automata, and using predicates over
befamiliarwiththeclassicalcase,wediscussitfirst. grammars. While we describe both, it is the latter that is
operationallymorerelevant.
3.1.BasicframeworkforclassicalEquivariance
3.2.1.QUOTIENT OPERATIONS ON TURING MACHINES
For the sake of completeness, we begin by recalling that
a groupisa setG, endowedwitha binaryoperationG× Let T = (S,Σ,Γ,s 0,H,δ) be a (finite) transducer. If ∼
G → G satisfying the associative property, and such that denotesanequivalencerelationoverthesetofstatesS,then
anidentitye ∈ Gexists,andsuchthatallelementsg ∈ G wemaydefinethecorrespondingquotienttransducer
haveaninverseinG.
T/∼=(S/∼,Σ,Γ,[s ],H,δ)
0
Remark 3.1. In order to define equivariance, we only re-
quire a binary operation, and not the remaining require- wherethenewinternalstatespaceS/ ∼comprisestheset
mentsfromthedefinitionofagroup. of∼-equivalenceclassesfromS,Hareequivalenceclasses
thatcontainan elementof H, and the transitiontable δ is
Equivariance. Consider two spaces X,Y and a function definedbysettingδ([s],γ) = ([s′],σ′)wheneverthereex-
f : X → Y. Fordefiningequivariancewe needto fix G- istss ∈ [s],s′ ∈ [s′]suchthatδ(s,γ) = (s′,σ′). Thiscan
actionsonX andY,i.e. mapsρ : G → T ,ρ : G → be also obtained via a suitable quotient of the underlying
X X Y
T withT ⊆ {T : X → X},T ⊆ {T : Y → Y}such computationalgraph, which identifies nodes according to
Y X Y
thatforallg,h∈G,thefollowingholds thequotientS 7→S/∼anddefineshaltingstatesH.
Quotient-symmetricTMs. Incasewe addtherestriction
ρ (gh)=ρ (g)◦ρ (h), ρ (gh)=ρ (g)◦ρ (h).
X X X Y Y Y Σ ⊂ Γ, we obtaina definitionofquotientTM’s asaspe-
(3.1)
cial case. Note that if a TM M recognizes L(M), then
AssumeGisaspacewithabinaryoperationG×G →G. ingeneral,aquotientofitM′ = M/ ∼willrecognizea
Givenafunctionf :X →Y andactionsρ ,ρ asabove, languageequalto,orlargerthanthatrecognizedbyMi.e.
X Y
wesaythatf isG-equivariantifforallg ∈Gwehave L(M′) ⊇ L(M),asaconsequenceofcollapsingstatese-
quences. WesaythatL = L(M)hassymmetryinduced
f ◦(ρ (g))=(ρ (g))◦f. (3.2) by ∼ quotient(or∼ symmetric)if L(M/ ∼) = L(M).
X Y
4Generalizedgrammarrulesandstructure-basedgeneralizationbeyondclassicalequivariance
Inthiscase,M/∼isa"simplerTMequivalenttoM". ∀x ,x ∈Σ∗,∀y ∈Λ∗,T(x a a )b y T(x a )=
1 2 1 1 1 2 1 1 2 3
If by abuse of notation we think of M as a functionM : b T(x )T(x )T(a a )y T(a x a )b y .
1 1 1 1 3 1 1 2 2 3 1
Σ∗ →{0,1}whichassigns"1"toacceptedstringsand"0"
totheothers,thenthepassagetothequotientM7→M/∼
WhileourtreatmentcouldinprincipleapplyforGTRs,we
is a Turing Machine analogue to the case of improving a
willactuallyrestrictto a specialcase. We say thata GTR
group-invariantclassificationtaskwithlabels{0,1}byre-
oftheform(3.3)isaGeneralizedGrammarRule(GGR)
placing a model by an group-invariant model with fewer
if
parameters.
The extension to transducers, of which TMs are a special • itinvolvesnovariabley overtheoutputlanguage,
j
case,isdirect: wesaythattrasducerT is∼symmetricif
• we have k = 1 and the left hand side is composed
T/∼inducesthesamemapΣ∗ 7→Λ∗asT.Intheanalogy 1
onlyofT(A );
to group-equivariance,quotienttransducers correspondto 1
passingtoasuitablequotientingroup-equivarianttasks. • thelengthofeachA isshorterorequaltothelength
i
Quotient symmetries are natural and parallel familiar
ofA 1.
group-equivariantsetups, however it seems hard to make
thempracticallyuseful,asagoodunderstanding/discussion ThustheformofaGGRis(takingmodificationsof(3.3)as
ofspecifictransducerdynamicsseemsnecessaryforimple- above,andnotationchangesh=h 1,A=A 1,k 2 =k)
menting a useful quotient symmetry. We present a more
directapproachinthenextsection. (R): ∀1≤i≤h,∀x i ∈C i,
T(A)=B T(A )B ···T(A )B . (3.4)
0 1 1 k k
3.2.2.GENERALIZEDGRAMMAR RULES
OurfirstexamplewasaGGRwhilethesecondwasnot.
Perhaps the most straightforward way for fixing symme-
triesinlanguagetransductionisintermsofpredicatesover If a GTR or a GGR holds for a transducer T, this con-
(transduction)grammars,i.e. rulesfortransducersT with strainsthetransducerbehavior,andatthesametime,dueto
input/outputlanguagesL ,L withalphabetsΣ,Λ. Thena theuniversalquantifiersin(3.3),itencodesinformationof
1 2
GeneralizedTransductionRule (GTR) for T is a predi- possibly extensive sets of relationsaboutthe graph of the
cateoftheform: transductionfunctioninducedbyT. GGRsplayaspecial
role, because they can be considered as production rules
∀1≤i≤h ,∀1≤j ≤h ,∀x ∈C ,∀y ∈D , for transducer T, i.e. they prescribe how to transduce a
1 2 i i j j
string of the form A , once strings of shorter length have
B T(A )B ···T(A )B = 1
0 1 1 k1 k1
beentransduced. ThusGGRsaredirectlysuitablefordata
B T(A )B ···T(A )B , (3.3)
0 1 1 k2 k2 augmentation,whereasgeneralGTRsarenot.
inwhichh ,h ,k ,k arenonnegativeintegerswithk > Wenowintroduceanerrorfunctionthatmeasures"howfar
1 2 1 2 1
0andwehaveused: from holding" for a given transducer T a given GGR is.
Withthesamenotationasin(3.4),forafixedvalueof"tem-
• two finite sets of symbols for variable strings X = perature"parameterβ >0weset
{x ,...,x },Y = {y ,...,y },andacorrespond-
1 h1 1 h2
ing set of subclasses of the input/output languages Err β(T,R):=
C 1,...,C h1 ⊆L 1andD 1,...,D h2 ⊆L 2; h
exp (−β−log(♯Σ)) ℓ(a ) dist(A,B), (3.5)
j
• two collections of strings
A 1,...,A k1,A 1,...,A
k2
∈(X∪Σ)∗andtwocollec- 1a ≤Xi∈ i≤C hi (cid:0) Xj=1 (cid:1)
b b
tionsofstringsB ,...,B ,B ,...,B ∈(Y∪Λ)∗,
0 k1 0 k2
in whichℓ(α) is thelengthof a stringα, anddist(α ,α )
suchthatforeach1 ≤ i ≤ h thesymbolx appears 1 2
1 i
istheLevenshteindistancebetweenstringsα ,α (wemay
inatleastoneoftheA ,A ,andforeach1≤i ≤h 1 2
j j 2
replaceitbyatask-dependentcombinatorialdistanceinap-
thesymboly appearsinatleastoneoftheB ,B .
i j j
plications), A ∈ Σ+ is the string obtained by replacing
Example3.2. To make the abovemoreconcrete,consider x = a for1 ≤ i ≤ h in A, andB ∈ Λ∗ is obtainedby
i i
twoexamplesofGTRs. Fora i ∈Σ∗,b i ∈Λ∗: thesamereplbacementintherighthandsideof(3.4),i.e. in
B 0T(A 1)···B k. b
∀x ,x ∈Σ∗,T(x a a x a )=
1 2 1 1 2 2 3 The factor exp(···) in (3.5) is a natural normalization,
b T(x )T(x )T(a a )T(x a )b .
1 1 1 1 3 2 2 2 which is useful in order to have the finitenes guarantee
5Generalizedgrammarrulesandstructure-basedgeneralizationbeyondclassicalequivariance
Err (T,R) provedin below Prop. 3.3. This result, valid generalizationin naturallanguageprocessingcontexts. In
β
even if the classes C ,D contain infinite sequences of particular, they work with a "local" permutation action
i i
strings of unbounded length, works under the following T : Σ → Σ,g ∈ G, suchthatan extensionto Σ∗ → Σ∗
g
mild assumption on T: we say that T has at most D- canbedefinedby
power growth1 if there exist C > 0 and D ∈ N such
that for all input strings α ∈ L there holds ℓ(T(α)) ≤ ∀k∀x ,...,x ∈Σ, T (x ...x )=
1 1 k g 1 k
C(ℓ(α))D.
T (x )...T (x ) (4.1)
g 1 g k
Proposition3.3 (Provedin App. A). Assume that T has
based on this, an implementation of an equivariant se-
D-power growth. Let (R) be a GRR with notation as in
quencetosequencemodelbasedongroupconvolutionsis
(3.4). ThenthereexistsaconstantCdependingonlyonthe
proposed.
growthboundforT andonh,A,B ,...,B ,A ,...,A
0 k 1 k
andβ >0,suchthatErr β(T,R)≤C. In our setting, given a set of T g,g ∈ G as above, we can
gettheGGRs(oneforeachelementg ∈G)definedas
The idea of the proof is that dist(A,B) only grows
(R ):∀x ,x ∈Σ∗, T (x x )→T (x )T (x ),
polynomially in the lengths ℓ(a ), whereas the factor g 1 2 g 1 2 g 1 g 2
i
exp (−β−log(♯Σ)) jℓ(a j) decaybsexbponentially,al- whichpermitthegenerationofalltherulesin(4.1)bycom-
lowi(cid:16)ng the convergence of the(cid:17)sum. The reason why the position.
P
aboveconvergenceguaranteeisimportant,isthatonlyhav- Example 4.2 (Context sensitivity). Another scenario de-
inga finite valueof the errorwill allowto set upgradient scribed in (Gordonetal., 2019) but left as an open direc-
descentproceduresbasedonthiserrorfunction,for"fitting tionisthatofconjunctionreplacement. Thetaskinvolves,
asetofGGRstoagiventransducer"or"learningapproxi- forexample,rulesallowingtoencodethefollowing:
mateGGRsunderlyingthedata".
T(RUNLEFTANDWALK) = LTURNRUNWALK,
Remark 3.4. As a consequenceof the form of (3.5), note
also that simpler rules have lower length growth of Aˆ,Bˆ T(RUNLEFTAFTERWALK) = WALKLTURNRUN.
(andthusofdist(A,B))intermsoftheℓ(a ),andasacon-
i
sequence,inasetupwherewelearnsymmetries,learning (Gordonetal.,2019)notethatforthistask,compositional
simplerrulesfirsbtisbfavouredbyourerrorfunction,asthe generalizationcan notbe achievedvia their frameworkof
exponential decay will "kick in" at lower values of ℓ(a ), localequivariance.Toseethis,noticethatchangesinduced
i
themorecomplicatedourrules. by the replacement of AND by AFTER in the input lan-
guagecannotbeimplementedbyanylocalgroupoperation
intheoutputlanguage.
4. Examples UsesofLanguage Symmetries in
Transduction Whilepermutationsymmetriesarenotabletoexpresssuch
rules,inoursettingwecanproducerulesthatcan. Forin-
Intheprevioussections,weelucidatedageneralnotionof stance,wecanexpressthedifferencebetween"AND"and
language symmetry via GGRs and connected it to the no- "AFTER"as:
tionofclassicalequivariance.Inthissection,weshowthat
severalrecentworksthat exploitsimilar ideas, in fact can ∀x 1,x 2 ∈Σ∗, T(x 1ANDx 2) → T(x 1)T(x 2),
berecoveredasspecialcasesofourtreatment,alsoserving ∀x ,x ∈Σ∗, T(x AFTERx ) → T(x )T(x ).
1 2 1 2 2 1
tofurtherclarifyit.
More generally, these rules can encode context sensitive
4.1.StringEquivarianceof(Gordonetal.,2019) transductionrules,whicharethuslexicalsymmetries.
Example 4.1 (String equivariance). Gordon et al.
4.2.EquivariantTransductionviaInvariantAlignment
(Gordonetal.,2019)isoneofthefirstandmostinfluential
of(White&Cotterell,2022)
works for the use of equivariance towards compositional
We will show here how we can use GGRs to extend the
1By inspecting the proof of Prop. 3.3, it can be
settingof(White&Cotterell,2022).
verified that if T has higher than polynomial growth
and log(ℓ(T(A))/log(ℓ(A)) ≤ F(ℓ(A)) for some Example 4.3 (Grammar tagging and annotations).
F : N → [0,+∞), then a variant of the result The work of (White&Cotterell, 2022) generalizes
may still hold, if we replace normalization factor by (Gordonetal.,2019)inthefollowingsense: Thepermuta-
exp (−βF(ℓ(A))−log(♯Σ)) jℓ(aj) . However this tions T g are applied but conditioned over lexical classes.
seem(cid:16)slike atechnical point and isnot de(cid:17)veloped further inthis In other words, symmetries are only allowed to exchange
P
setting. words inside a given fixed lexical class (examples of
6Generalizedgrammarrulesandstructure-basedgeneralizationbeyondclassicalequivariance
lexicalclassesaresubsetsofverbs/adjectives/nouns/etc.of same form T(α) in (Lake&Baroni, 2023). There is yet
interchangeablesemanticroles),afterwhichtheextension another difference that x ,x may vary over elements of
1 2
to Σ∗ is performed as shown in Example 4.1. This can Σ ratherthanoverΣ∗ asinoursetting. Thetranslationto
be achieved by allowing part-of-speech taggers. But of oursettingcanbedonebyusingasingletaggerforwords,
course, we can even include conditioning over a wide andreplacingwordswbylwl′ ontheleftofthequantified
range of annotations, beyond the (local) level of single equations. Thusrules1-4staythesamewhilee.g. the5th
wordtagging,viamoregeneralgrammaticaltaggers.We ruleabovebecomes
can condition the switch of a whole substring by another,
in the presence of a suitable tagging of the former. In ∀x ,x ∈Σ+, T(lx l′llugl′lx l)=
1 2 1 2
order to implement conditioning on annotations of N
T(x )T(u )T(x )T(u )T(u ).
possible classes, we formalize annotations, i.e. extend 1 1 1 1 1
Σ via new letters {l ,l′ : i = 1...,N} (to be thought
i i Then we may stipulate that the input language L comes
as class-dependent brackets), and assume that the input 1
withsuitablewell-formedtagging,fittingtheabovesetup.
sentences include annotations, in which we interpret
substring l Al′ as the annotationthatA is in grammatical
i i
5. Learning Transduction Symmetries
class i. Then, in order to apply T to exchange x with
g
y = T (x) onlyrestrictedto substringsfromgrammatical
g In the previous section we showed that several recent
classi,weincludetherules∀x ∈ Σ∗, T (l xl′) = l yl .
g i i i i workson languagecompositionalitycouldbe seen asspe-
Notice that this is only imposing T -action to strings
g cial cases of our framework. Here, we consider the ques-
taggedbyl ,l′ delimiters.
i i tionsof“learningsymmetries”intransduction,inthesense
of“learningtransducerswithapproximate2symmetrycon-
4.3.ToyTransducerRulesin(Lake&Baroni,2023)
straints”. Presentingafullapproachtosymmetrylearning
Example 4.4 (Toy transducer rules are GGRs). In is a challenging open problem even for classical (group)
(Lake&Baroni, 2023), toy model languages were gener- equivariance. Therefore,ourgoalisnottoproposeasolu-
ated in order to test systematicity, which can be thought tion,butrathertopointouttherelevanceofthesequestions
of as a weakening of compositionality. Notably, system- withinourframework. Weexpectthatthiswillhelporient
aticgeneralizationhasremainedanopenchallengeinneu- the reader and, as we will see, it will permit reinterpreta-
ralnetworksforoverthreedecades, andhasbeenthesub- tionsofseveralimportantopenquestions.
ject of a long standing debate. (Lake&Baroni, 2023)
showedthatstandardneuralnetworkmodels,ifoptimized 5.1.Learningapproximatesymmetries: motivation
for their compositional skills, can do a reasonable job at andconnectionwithReinforcementLearning
mimickinghumansystematicgeneralization. Interestingly,
Howdoweinferwhatsymmetriesarethebesttoimposeon
(Lake&Baroni,2023)testedforsystematicityviainterpre-
atransducerforagiventask? Beyondcertainapplications,
tation grammars, which in fact are GGRs. In their setup,
suchastransductionbetweenprogramminglanguages(say
theinputalphabetΣconsistedofafinitesetofwordssuch
from a high-resource language such as Python, to a low-
as"zup","fep","tufa",etc. andtheoutputalphabetΛwasa
resource one such as Verilog), or grammars pertaining to
finitesetofcolors. Anexampleofinterpretationgrammar
toy models, as the transduction tasks become harder, the
is
symmetriesarenotexplicitlyknown. Wemayhaveaccess
to only a handful of generalized grammar rules, whereas
T(zup) = [green],
theremainingoneshavetobelearned.Thefinitenessresult
T(fep) = [rose],
of Prop. 3.3, allows to formulate approximateGGRs and
T(gazzer) = [red], perform optimization over Err or related loss functions,
β
T(tufa) = [bourbon], allowingtolearnGGRsadaptedtoatask.
Previous attempts towards this task include “neural pro-
∀x ,x ∈Σ, T(x lugx )=
1 2 1 2 gram interpreters” (Reed&DeFreitas, 2015) and “neural
T(x )T(x )T(x )T(x )T(x ),
2 1 2 1 1 turing machines” (Zaremba&Sutskever, 2015), which fit
within the purview of “program induction”. For more re-
∀x ,x ∈Σ, T(x kikix ) = T(x )T(x ),
1 2 1 2 1 2
∀x ∈Σ, T(x blicket) = T(x )T(x ), 2Note that allowing approximate symmetries has al-
1 1 1 1
ways been part of the definitions of compositionality: e.g.
∀x ,x ∈Σ, T(x x ) = T(x )T(x ).
1 2 1 2 1 2 (Fodor&Pylyshyn,1993)says"insofarasalanguageissystem-
atic,alexicalitemmustmakeapproximatelythesamesemantic
Eachlineaboveisa GGRasinoursetup, butwiththe re- contributiontoeachexpressioninwhichitoccurs"(withempha-
striction that left hand sides of all the equations have the sisaddedhere).
7Generalizedgrammarrulesandstructure-basedgeneralizationbeyondclassicalequivariance
cent examples, neural network models such as transform- 5.2.Furtherexamplesoflinkstoconceptualthemesin
ers(Strobletal.,2023)orRNNs(Orvietoetal.,2023)have languageprocessing
been evaluated on tasks formulated as formal language
There are several directions that seem closely related and
recognition.
may benefit from the parallel to learning transducer sym-
Putative GGRs R over a transduction task T can be en- metries,inconjunctiontotheRLsetupsketchedabove.
codedasanotcompletelydeterminedtransducer,givenas
Structuralformsearch. Afirstdirectionisworkon“dis-
coveryofstructuralform”(Kemp&Tenenbaum,2008),in
which a framework for building graph grammars underly-
T =(S ,Σ∪X ,Γ∪X ,s ,H,δ ).
R R R R 0 R ingdata isindicated. ApplyingtheaboveRL approachto
automata search can be of interest for searching amongst
geometricformsunderlyingdata.
A simple way to formalize learning R is to think of
Language based on agent models. (Andreas, 2022) pro-
S ,X ,δ probabilistically: weconsiderT ’scomputa-
R R R R
posestotreatlanguagemodelsas"agentmodels"inwhich
tionalgraphasa randomgraphwhoseedgesit isourtask
interpretationisbasedonmodellingofanagent’sunderly-
to learn. In practice we may fix S ,X , and determine
R R
inginternalstates. Inourframework,wemaydirectlyfor-
δ ’sdistributionfromdata. ThissettingisakintoMarkov
R
mulate these internal states at the level of the transducers
Decision Processes (MDP) from classical Reinforcement
learningthetask,andlearningcanproceedasanRLagent,
Learningtheory(e.g. asappearingin(Abateetal.,2022)).
adaptingitsGGRtotheunderlyingtask. Learnablehidden
Note that in this formalization, we would allow errors in internal states then play a similar role as learnable GGR
GGRfitnesstothemodel,inordertoformulateatractable constraints. Note that this formalization is quite different
differentiable learning task. On the other hand, similar fromthe one of (Andreas, 2022), althoughthe underlying
tothecase ofapproximateequivariance,allowingsymme- taskinterpretationcoincides.
try errors in prescribed-symmetry models, may also help
Words as cues. It has been argued (Lupyan&Lewis,
to allow for exceptionsin real-world modelsand to make
2019) that interpreting human lexical processing in terms
themodelsmorerobust. Thisconsiderationdirectlytrans-
of tokens and semantic interpretation can run into prob-
latestoourproposedtransducersymmetries,andcanbeim-
lems, which become evident when treating transduction
plementedviatheabove"probabilistictransducers"frame-
betweenunrelated(natural)languages. A"words-as-cues"
work.
viewthenproposesalessstructuredapproach,inwhichlan-
Thenoveltyofourmainpositionappliedhere,besidesthe guage/semanticsjustgiveinductivebiasesonthe interpre-
superficialchangeofworkingwithtransducersratherthan tations of multimodalinputs. This view can be construed
theclassicalRLsetting,isthatwereinterprettheRLthe- asconceptuallysimilartoours,inwhichGGRerrorscould
ory as a learning task for transducer symmetries. Be- be quantifyingthese biases. We emphasize the parallelto
yondthealreadyexploredEquivariance-GGRlink,Some classicalequivariance,inwhichthesourceofinductivebias
consequences of this viewpoint come from the link from weremoreclassical/physicalsymmetriesofthetasks.
GGRsandEquivariance,toRLareasfollows:
Reformulating binding and disentanglement. The im-
plementationofGGRsallowsattackingtheso-calledbind-
ing problem (see (Greffetal., 2020)) in transduction and
• Consequence1: ageometricviewoftransductionand relatedtasks, fromthepointofview ofgeneralizedequiv-
language processing, saying that e.g. transduction ariance. Thiscanbringpossiblebenefits: forexample,the
tasksarelikeexplorationbyanagent,butinlanguage reinterpretation of GGRs as symmetries allows to extend
space,justifiesthatspatialRLstrategiesshouldberel- thedefinitionofdisentanglement(Higginsetal.,2018),ini-
evantintransductiontasks. tiallybasedonclassicalequivariance,totransductionprob-
• Consequence2:usingthebridgefromRLtoequivari- lems.
ancetheory,wecanstatethatlearning(group)symme-
triescouldbenefitfromRLapproaches.
6. Conclusions
• Consequence3: RL can be thoughtofas a transduc-
tion task with symmetries: the link fromRL to trans- Based on a theory of meaning modeled via formal lan-
ductionis well accepted, andthe parallelto equivari- guagesandtransducers,wehaveproposedaframeworkin
ance theory indicates that our framework may allow which Generalized Grammar Rules allow to impose com-
e.g. to compare numbers of degrees of freedom un- positionalgeneralizationconstraintsonlearningtasks,and
der differentRL structures as in (Petrache&Trivedi, givea directmethodforcompositionaldataaugmentation.
2023).
8Generalizedgrammarrulesandstructure-basedgeneralizationbeyondclassicalequivariance
We have argued that GGRs can be imposed in transduc- left and right. In Linzen, T., Chrupała, G., and Al-
tion tasks, bearing parallels to the imposition of group- ishahi, A. (eds.), Proceedings of the 2018 EMNLP
equivarianceinphysics-inspiredmodelswithnaturalgroup Workshop BlackboxNLP: Analyzing and Interpreting
actions. Thishasbeendonebyformulatinganaturalquan- Neural Networks for NLP, pp. 47–55, Brussels, Bel-
tification for the validity error of GGRs, analogousto ap- gium, November 2018. Association for Computational
proximate equivariance error quantification. We then de- Linguistics. doi: 10.18653/v1/W18-5407. URL
scribed how a series of recent works related to composi- https://aclanthology.org/W18-5407.
tionalityintransductionfitwithinourframework,andthen
Bekkers, E. J. B-spline cnns on lie groups. In
described how our frameworkcan be used in tasks where
8th International Conference on Learning Rep-
GGRs can be learned. HereGGR learningcanbe viewed
resentations, ICLR 2020, Addis Ababa, Ethiopia,
as the learning of compositional symmetries, with direct
April 26-30, 2020. OpenReview.net, 2020. URL
relations to RL strategies, also bearing interesting conse-
https://openreview.net/forum?id=H1gBhkBFDH.
quences upon RL theory. Finally, we indicated how our
framework connects to adjacent research topics such as
Carnap,R. Introductiontosemanticsandformalizationof
structural form search, words-as-cuesmodels, agent mod-
logic. HarvardUniversityPress,1959.
els, and in problems related to compositionality, such as
bindinganddisentanglement. Chen,X.,Liang,C.,Yu,A.W.,Song,D.X.,andZhou,D.
Compositionalgeneralizationvianeural-symbolicstack
7. PotentialBroader Impact machines. ArXiv,abs/2008.06662,2020.
Chomsky, N. Syntactic Structures. De Gruyter Mou-
Thisworkistheoretical,presentinganewpointofviewto-
ton, Berlin, Boston, 1957. ISBN 9783112316009.
wards compositionality. The underlying views may have
doi: doi:10.1515/9783112316009. URL
consequences if implemented, however potential impact
https://doi.org/10.1515/9783112316009.
doesnotfollowdirectlyfromourpaper.
Chomsky,N. WhatKindofCreaturesAreWe? Columbia
References University Press, 2016. ISBN 9780231175968. URL
http://www.jstor.org/stable/10.7312/chom17596.
Abate, A., Almulla, Y., Fox, J., Hyland, D., and
Wooldridge, M. Learning task automata for reinforce- Chomsky,N.andSchützenberger,M.P. Thealgebraicthe-
ment learning using hidden markov models. arXiv oryofcontext-freelanguages*. Studiesinlogicandthe
preprintarXiv:2208.11838,2022. foundationsofmathematics,35:118–161,1963.
Akyürek, E. and Andreas, J. Lexsym: Compositionality Cohen, T. and Welling, M. Group equivariant convolu-
aslexicalsymmetry. InProceedingsofthe61stAnnual tional networks. ArXiv, abs/1602.07576, 2016. URL
Meeting of the Association for Computational Linguis- https://api.semanticscholar.org/CorpusID:609898.
tics(Volume1: LongPapers),pp.639–657,2023.
Conklin, H., Wang, B., Smith, K., and Titov, I.
Akyürek,E., Akyurek,A.F., andAndreas,J. Learningto
Meta-learning to compositionally generalize. ArXiv,
recombineandresampledataforcompositionalgeneral-
abs/2106.04252,2021.
ization. ArXiv,abs/2010.03706,2020.
Csordás, R., Irie, K., andSchmidhuber,J. The devilis in
Andreas, J. Language models as agent models. In Find-
thedetail: Simpletricksimprovesystematicgeneraliza-
ings of the Association for Computational Linguistics:
tionoftransformers. InConferenceonEmpiricalMeth-
EMNLP2022,pp.5769–5779,2022.
odsinNaturalLanguageProcessing,2021.
Angluin,D.andSmith,C.H. Inductiveinference: Theory
Daley,R.P.andSmith,C.H. Onthecomplexityofinduc-
and methods. ACM computing surveys (CSUR), 15(3):
tiveinference. InformationandControl,69(1-3):12–40,
237–269,1983.
1986.
Bahdanau, D., Murty, S., Noukhovitch, M., Nguyen,
Dankers,V.,Bruni,E.,andHupkes,D. Theparadoxofthe
T. H., de Vries, H., and Courville, A. C. Sys-
compositionalityofnaturallanguage:Aneuralmachine
tematic generalization: What is required and can it
translation case study. In Muresan, S., Nakov, P., and
be learned? CoRR, abs/1811.12889, 2018. URL
Villavicencio,A.(eds.),Proceedingsofthe60thAnnual
http://arxiv.org/abs/1811.12889.
Meeting of the Association for Computational Linguis-
Bastings, J., Baroni, M., Weston, J., Cho, K., and tics (Volume 1: Long Papers), pp. 4154–4175, Dublin,
Kiela, D. Jump to better conclusions: SCAN both Ireland, May 2022. Association for Computational
9Generalizedgrammarrulesandstructure-basedgeneralizationbeyondclassicalequivariance
Linguistics. doi: 10.18653/v1/2022.acl-long.286. URL Hupkes, D., Dankers, V., Mul, M., and Bruni, E. Com-
https://aclanthology.org/2022.acl-long.286.positionality decomposed: How do neural networks
generalise? J.Artif.Intell.Res.,67:757–795,2019.URL
deVries,H.,Bahdanau,D.,Murty,S.,Courville,A.C.,and https://api.semanticscholar.org/CorpusID:211259383
Beaudoin, P. CLOSURE: assessing systematic general-
izationof CLEVR models. In VisuallyGroundedInter- Hupkes, D., Dankers, V., Mul, M., and Bruni, E.
actionandLanguage(ViGIL),NeurIPS2019Workshop, Compositionality decomposed: How do neural
Vancouver, Canada, December 13, 2019, 2019. URL networks generalise? (extended abstract). In
https://vigilworkshop.github.io/static/papeBresss/ie2re8,.pCd.f(.ed.), Proceedings of the Twenty-Ninth
International Joint Conference on Artificial Intel-
Donatelli, L. and Koller, A. Compositional- ligence, IJCAI-20, pp. 5065–5069. International
ity in computational linguistics. Annual Re- Joint Conferences on Artificial Intelligence Organi-
view of Linguistics, 9(1):463–481, 2023. doi: zation, 7 2020. doi: 10.24963/ijcai.2020/708. URL
10.1146/annurev-linguistics-030521-044439. URL https://doi.org/10.24963/ijcai.2020/708.
https://doi.org/10.1146/annurev-linguisticJso-u0rn3a0l5tr2ac1k-.044439.
Dziri, N., Lu, X., Sclar, M., Li, X. L., Jian, L., Lin, B. Y., Ingraham, A. Swain School Lectures. The Open Court
West, P., Bhagavatula, C., Bras, R. L., Hwang, J. D., PublishingCompany,Chicago„1903.
Sanyal,S.,Welleck,S.,Ren,X.,Ettinger,A.,Harchaoui,
Johnson,J.,Hariharan,B.,vanderMaaten,L.,Fei-Fei,L.,
Z., andChoi, Y. Faith and fate: Limits oftransformers
Zitnick, C. L., and Girshick, R. B. Clevr: A diagnos-
oncompositionality. ArXiv,abs/2305.18654,2023.
tic dataset for compositional language and elementary
Fodor,J.A.andPylyshyn,Z.W. Connectionismandcog- visual reasoning. 2017 IEEE Conference on Computer
nitivearchitecture.InReadingsinphilosophyandcogni- VisionandPatternRecognition(CVPR),pp.1988–1997,
tivescience,pp.801–818.1993. 2016.
Johnson,K. Onthesystematicityoflanguageandthought.
Gordon, J., Lopez-Paz, D., Baroni, M., and Bouchacourt,
Journalof Philosophy, 101(3):111–139,2004. doi: 10.
D. Permutation equivariant models for compositional
5840/jphil2004101321.
generalizationinlanguage. InInternationalConference
onLearningRepresentations,2019.
Kemp,C.andTenenbaum,J.B.Thediscoveryofstructural
form. ProceedingsoftheNationalAcademyofSciences,
Greff, K., Van Steenkiste, S., and Schmidhuber, J. On
105(31):10687–10692,2008.
thebindingprobleminartificialneuralnetworks. arXiv
preprintarXiv:2012.05208,2020. Kondor, R. and Trivedi, S. On the generalization of
equivariance and convolution in neural networks
Hewitt, J., Hahn, M., Ganguli, S., Liang, P., and Man-
to the action of compact groups. In Dy, J. G. and
ning, C. D. RNNs can generate bounded hierarchical
Krause, A. (eds.), Proceedings of the 35th Inter-
languageswith optimalmemory. In Webber, B., Cohn,
national Conference on Machine Learning, ICML
T., He, Y., and Liu, Y. (eds.), Proceedings of the 2020
2018, Stockholmsmässan, Stockholm, Sweden, July
Conference on Empirical Methods in Natural Lan-
10-15, 2018, volume 80 of Proceedings of Machine
guage Processing (EMNLP), pp. 1978–2010, Online,
LearningResearch,pp.2752–2760.PMLR,2018. URL
November 2020. Association for Computational Lin-
http://proceedings.mlr.press/v80/kondor18a.html.
guistics. doi: 10.18653/v1/2020.emnlp-main.156. URL
https://aclanthology.org/2020.emnlp-main.L1a5ke6,.B. M. and Baroni, M. Generalization without sys-
tematicity: On the compositional skills of sequence-to-
Higgins, I., Amos, D., Pfau, D., Racaniere, S., Matthey,
sequence recurrent networks. In International Confer-
L., Rezende, D., and Lerchner, A. Towards a defi- enceonMachineLearning,2018.
nition of disentangled representations. arXiv preprint
arXiv:1812.02230,2018. Lake, B. M. and Baroni, M. Human-like systematic gen-
eralizationthroughameta-learningneuralnetwork. Na-
Hopcroft,J.E.andUllman,J.D. IntroductiontoAutomata ture.2023Nov;623(7985):115–21.,2023.
Theory, LanguagesandComputation. Addison-Wesley,
1979. ISBN0-201-02988-X. Lake,B. M.,Linzen,T.,andBaroni,M. Humanfew-shot
learning of compositional instructions. In Annual
Huang,N.,Levie,R.,andVillar,S. Approximatelyequiv- Meeting of the Cognitive Science Society, 2019. URL
ariantgraphnetworks. ArXiv,abs/2308.10436,2023. https://api.semanticscholar.org/CorpusID:58006558.
10Generalizedgrammarrulesandstructure-basedgeneralizationbeyondclassicalequivariance
Li, Q., Zhu, Y., Liang, Y., Wu, Y. N., Zhu, S., and Pollack, J. B. Recursive distributed repre-
Huang, S. Neural-symbolic recursive machine for sentations. Artificial Intelligence, 46(1):77–
systematic generalization. CoRR, abs/2210.01603, 105, 1990. ISSN 0004-3702. doi: https:
2022. doi: 10.48550/ARXIV.2210.01603. URL //doi.org/10.1016/0004-3702(90)90005-K. URL
https://doi.org/10.48550/arXiv.2210.01603. https://www.sciencedirect.com/science/article/pii/0
Qiu, L., Shaw, P., Pasupat, P., Nowak, P. K., Linzen, T.,
Loula, J., Baroni, M., and Lake, B. Rearranging the fa-
Sha, F., and Toutanova, K. Improving compositional
miliar: Testing compositional generalization in recur-
generalization with latent structure and data augmenta-
rent networks. In Linzen, T., Chrupała, G., and Al-
tion. ArXiv,abs/2112.07610,2021.
ishahi,A.(eds.),Proceedingsofthe2018EMNLPWork-
shop BlackboxNLP: Analyzing and Interpreting Neu- Reed, S. and De Freitas, N. Neural programmer-
ral Networks for NLP, pp. 108–114, Brussels, Bel- interpreters. arXivpreprintarXiv:1511.06279,2015.
gium, November 2018. Association for Computational
Ruis, L., Andreas, J., Baroni, M., Bouchacourt, D.,
Linguistics. doi: 10.18653/v1/W18-5413. URL
and Lake, B. M. A benchmark for systematic
https://aclanthology.org/W18-5413.
generalization in grounded language understand-
Lupyan, G. and Lewis, M. From words-as-mappings to ing. In Larochelle, H., Ranzato, M., Hadsell, R.,
words-as-cues: Theroleoflanguageinsemanticknowl- Balcan, M., and Lin, H. (eds.), Advances in Neu-
edge. Language, Cognition and Neuroscience, 34(10): ral Information Processing Systems, volume 33,
1319–1337,2019. pp. 19861–19872.Curran Associates, Inc., 2020. URL
https://proceedings.neurips.cc/paper_files/paper/20
Marcus, G., Leivada, E., and Murphy, E. A
Strobl, L., Merrill, W., Weiss, G., Chiang, D., and An-
sentence is worth a thousand pictures: Can
gluin, D. Transformers as recognizers of formal lan-
large language models understand human lan-
guages: A survey on expressivity. arXiv preprint
guage? ArXiv, abs/2308.00109, 2023. URL
arXiv:2311.00208,2023.
https://api.semanticscholar.org/CorpusID:260351531.
Valvoda, J., Saphra, N., Rawski, J., Williams, A., and
Marcus, G. F. The Algebraic Mind: Integrat- Cotterell, R. Benchmarking compositionality with
ing Connectionism and Cognitive Science. The formal languages. In Calzolari, N., Huang, C.-R.,
MIT Press, 04 2001. ISBN 9780262279086. Kim, H., Pustejovsky, J., Wanner, L., Choi, K.-S.,
doi: 10.7551/mitpress/1187.001.0001. URL Ryu, P.-M., Chen, H.-H., Donatelli, L., Ji, H., Kuro-
https://doi.org/10.7551/mitpress/1187.001.0ha0s0h1i,.S., Paggio, P., Xue, N., Kim, S., Hahm, Y., He,
Z., Lee, T. K., Santus, E., Bond, F., and Na, S.-H.
Montague,R.Universalgrammar.Theoria,36(3):373–398, (eds.), Proceedings of the 29th International Confer-
1970. doi:10.1111/j.1755-2567.1970.tb00434.x. ence on Computational Linguistics, pp. 6007–6018,
Gyeongju, Republic of Korea, October 2022. Interna-
Nye, M., Solar-Lezama, A., Tenenbaum, J., and Lake, tional Committee on Computational Linguistics. URL
B.M. Learningcompositionalrulesvianeuralprogram https://aclanthology.org/2022.coling-1.525.
synthesis. Advances in Neural Information Processing
West, P., Lu, X., Dziri, N., Brahman, F., Li, L., Hwang,
Systems,33:10832–10842,2020.
J. D., Jiang, L., Fisher, J. R., Ravichander, A., Chandu,
Orvieto, A., Smith, S. L., Gu, A., Fernando, A., Gul- K. R., Newman, B., Koh, P. W., Ettinger, A., andChoi,
cehre, C., Pascanu, R., and De, S. Resurrecting recur- Y. Thegenerativeaiparadox:"whatitcancreate,itmay
rentneuralnetworksforlongsequences. arXivpreprint not understand". ArXiv, abs/2311.00059, 2023. URL
https://api.semanticscholar.org/CorpusID:264832736
arXiv:2303.06349,2023.
White, J. C. and Cotterell, R. Examining the induc-
Partee, B. H. Lexical semantics and
tive bias of neural language models with artificial
compositionality. 1995. URL
languages. In Annual Meeting of the Associa-
https://api.semanticscholar.org/CorpusID:116976482.
tion for Computational Linguistics, 2021. URL
https://api.semanticscholar.org/CorpusID:235293810
Petrache,M.andTrivedi,S.Approximation-generalization
trade-offs under (approximate) group equivari- White, J. C. and Cotterell, R. Equivariant transduction
ance. In Thirty-seventh Conference on Neural throughinvariantalignment. InProceedingsofthe29th
Information Processing Systems, 2023. URL InternationalConferenceonComputationalLinguistics,
https://openreview.net/forum?id=DnO6LTQ77U.pp.4651–4663,2022.
11Generalizedgrammarrulesandstructure-basedgeneralizationbeyondclassicalequivariance
Zaremba, W. and Sutskever, I. Reinforcement learn-
ing neural turing machines-revised. arXiv preprint
arXiv:1505.00521,2015.
12Generalizedgrammarrulesandstructure-basedgeneralizationbeyondclassicalequivariance
A. ProofofProposition3.3
We refer to the notation of (3.4) and (3.3) here, and the goal is to study the convergence of the sum from (3.5) (also
reproduced in (A.5) below, for the reader’s convenience). We start with a few auxiliary bounds. First, we bound the
numberofsummandsineachtermin(3.5).
♯Cℓ :=♯(C ∩{x∈L : ℓ(x)=ℓ})≤♯{x∈Σ∗ : ℓ(x)=ℓ}=(♯Σ)ℓ. (A.1)
i i 1
Next,considerthetermdist(A,B). AsdistisLevenshteindistance(equallingtheminimumnumberofinsertion/deletions
requiredtopassfromAtoB,itfollowsthat
b b
dist(A,B)≤ℓ(A)+ℓ(B). (A.2)
b b
We now control from above ℓ(A),ℓ(B). The power-D growth hypothesis on T states that for all α ∈ L we have
b b b b 1
ℓ(T(α))≤C (ℓ(α))D. Forthecaseofℓ(A)thisimpliesthat,foranyA∈(X ∪Σ)∗,
T
b b
D
ℓ(A) = ℓ T bA|
xi=ai,1≤i≤h
≤C
T
ℓ A|
xi=ai,1≤i≤h
(cid:16) (cid:16) (cid:17)(cid:17) D (cid:16) (cid:16) (cid:17)(cid:17)
h
b
D
≤ C ℓ(A)+ d ℓ(a ) ≤C (ℓ(A)+d ℓ(~a))
T i i T max
!
i=1
X
≤ C C (ℓ(~a)+1)D, (A.3)
T R
whereintheabove
• d denotesthetotalnumberofinstancesofvariablex inA
i i
• wedenoted :=max d
max 1≤i≤h i
• wedenoteℓ(~a):= h ℓ(a ).
j=1 j
P
We now proceed to a similar estimate for ℓ(B). Note that B is obtained from the right hand side of (3.4) by replacing
x =a forall1≤i≤handbycomputingT(A )forall1≤i≤k. Wemayapplyaboundsimilarto(A.3)toeachsuch
i i i
term,andwefind b b
k k k k
D
ℓ(B) = ℓ(B )+ ℓ(T(A ))≤ ℓ(B )+C ℓ(A )+d ℓ(~a)
i i i T i max
i=0 i=1 i=0 i=1
X X X X(cid:0) (cid:1)
b ≤ C (C(1)+C )(ℓ(~a)+1)D ≤C C (ℓ(~a)+1)D. (A.4)
T R R T R
Nowwe mayplug(A.3), (A.4)into(A.2)andthenplugthe outcometogetherwith(A.1) into(3.5) tofindthefollowing
e
bounds(thefirstlineisacopyof(3.5),fortheconvenienceofthereader:
h
Err (T,R) := exp (−β−log(♯Σ)) ℓ(a ) dist(A,B) (A.5)
β j
 
a1∈C1X,...,ah∈Ch Xj=1
≤ C (C +C )  exp((−β−log( ♯Σ))ℓ(~ab ))b (ℓ(~a)+1)D
T R R
a1∈C1X,...,ah∈Ch
= C (Ce +C ) ··· exp (−β−log(♯Σ))k~ℓk (k~ℓk +1)D
T R R 1 1
~ℓX∈Nha1X∈C1ℓ1 ahX∈C hℓh (cid:16) (cid:17)
e
(A ≤.1) C (C +C ) (♯Σ)k~ℓk1exp (−β−log(♯Σ))k~ℓk (k~ℓk +1)D
T R R 1 1
~ℓX∈Nh (cid:16) (cid:17)
= C e e−βk~ℓk1(k~ℓk +1)D, (A.6)
T,R 1
~ℓX∈Nh
wherewedenotedC := C (C +C ), andthesum in(A.5)convergesforanyβ > 0, andisboundeddepending
T,R T R R
onlyonh,β,D,inwhichh,DinturndependonlyonR,T.
e
13