Nomic Embed: Training a Reproducible Long Context Text Embedder
ZachNussbaum JohnX.Morris
zach@nomic.ai jack@nomic.ai
jxm3@cornell.edu
BrandonDuderstadt AndriyMulyar
brandon@nomic.ai andriy@nomic.ai
Abstract NomicEmbed
62.39
JinaBaseV2
60.39
This technical report describes the training MTEB 62.26 text-embedding-3-small
of nomic-embed-text-v1, the first fully re- 60.99 text-embedding-ada
producible, open-source, open-weights, open- 85.53
data, 8192 context length English text em- 85.45 LoCo
beddingmodelthatoutperformsbothOpenAI 82.4
52.7
Ada-002andOpenAItext-embedding-3-small
on short and long-context tasks. We release 54.16
51.9
the training code and model weights under JinaLC
58.2
an Apache 2 license. In contrast with other
55.25
open-sourcemodels,wereleaseatrainingdata
50 55 60 65 70 75 80 85
loaderwith235millioncuratedtextpairsthat
allowsforthefullreplicationofnomic-embed- Figure1: TextEmbeddingModelBenchmarks. Ag-
text-v1. You can find code and data to repli- gregateperformanceofnomic-embed-text-v1,OpenAI
cate the model at https://github.com/nomic- text-embedding-ada, OpenAI text-embedding-3-small
ai/contrastors. and jina-embedding-base-v2 on short and long con-
text benchmarks. Nomic Embed is the only fully au-
1 Introduction
ditable long-context model that exceeds OpenAI text-
embedding-ada, OpenAI text-embedding-3-small, and
Text embeddings are an integral component of
Jina performance across both short and long context
modern NLP applications powering retrieval-
benchmarks. X-axisunitsvaryperbenchmarksuite.
augmented-generation (RAG) for LLMs and se-
mantic search (Lewis et al., 2021a; Izacard et al.,
2022b; Ram et al., 2023). These embeddings en- base-en Gu¨nther et al. (2024) and E5-Mistral-7b-
codesemanticinformationaboutsentencesordoc- instructWangetal.(2023b).
uments as low-dimensional vectors that are used Unfortunately, jina-embedding-v2-base does
indownstreamapplications,suchasclusteringfor not surpass OpenAI’s text-embedding-ada-002
data visualization, classification, and information Neelakantan et al. (2022) (see Table 1). Further,
retrieval. E5-Mistral Wang et al. (2023b) is not feasible to
The majority of the top open-source models use in many engineering applications due to the
on the MTEB benchmark (Muennighoff et al., large inference requirements of a 7B parameter
2023) are limited to context lengths of 512, such transformer, and is not recommended for use be-
as E5 Wang et al. (2022), GTE Li et al. (2023), yond4096tokens.
and BGE Xiao et al. (2023). This short context This report describes how we trained nomic-
length reduces model utility in domains where embed-text-v1, a 137M parameter, open-source,
overall document semantics are not localized to open-weights, open-data, 8192 sequence length
sentences or paragraphs. Most top embedding modelthatsurpassesOpenAItext-embedding-ada
models with a context length longer than 2048 and text-embedding-3-small performance on both
are closed-source, such as Voyage-lite-01-instruct short and long context benchmarks (Table 1). We
Voyage (2023) and text-embedding-ada-002 Nee- release the model weights and codebase under an
lakantanetal.(2022). Apache-2 license. We additionally release our
The top two performing open-source long con- curated training dataset to enable end-to-end au-
text embedding models are jina-embedding-v2- ditabilityandreplicationofthemodel.
4202
beF
2
]LC.sc[
1v31610.2042:viXraModel Params Seq MTEB LoCo JinaLC Weights Code Data
nomic-embed-text-v1 137M 8192 62.39 85.53 54.16 Yes Yes Yes
nomic-embed-text-v1-ablated 137M 8192 61.36 86.89 53.53 Yes Yes Yes
jina-embeddings-base-v2-en 137M 8192 60.39 85.45 51.90 Yes No No
text-embedding-ada-002 N/A 8192 60.99 52.70 55.25 No No No
text-embedding-3-small N/A 8192 62.26 82.4 58.21 No No No
E5-Mistral-7b-instruct 7B 4096 66.6 87.8 N/A Yes No No
text-embedding-3-large N/A 8192 64.59 79.4 58.69 No No No
Table 1: Benchmarking nomic-embed-text-v1 against OpenAI models and other top long context open-source
models. Nomic-embed-text-v1 is the only 100M parameter class open-source model that outperforms OpenAI
text-embedding-ada and text-embedding-3-small on both short and long-context tasks. Nomic-embed-text-v1-
ablatedreferstothetrainingsetupdescribedinSection5.4, whichomitstheHotpotQAandFEVERdata. ‘Seq’
referstothecontextlengthofthemodel,andJinaLCisanaverageovertasksintheJinaLongContextbenchmark.
2 RelatedWork context evaluation. Additionally, the LoCo Saad-
Falcon et al. (2024) benchmark was recently re-
State-of-the-arttextembeddingmodelsaretrained leasedtoevaluatetheperformanceoflongcontext
by initializing a pre-trained transformer and then retrievalmodels.
fine-tuning with a contrastive loss objective. Tra-
As AI applications mature, auditability and
ditionally, fine-tuning involved leveraging labeled
compliance of models and their training data will
datasets such as MSMarco and SNLI (Bowman
beacriticalcomponentofsafemodeldeployments
et al., 2015) to generate paired training data for
inhigh-impactdomains. Forexample,recentwork
the contrastive signal. Examples include SBERT
by Anthropic on sleeper agents (Hubinger et al.,
(Reimers and Gurevych, 2019), SimCSE (Gao
2024) demonstrates the risk of deploying models
etal.,2022),andSGPT(Muennighoff,2022). Re-
without end-to-end auditability. Top-performing
centsystemssuchasE5(Wangetal.,2022),GTE
text embedding models currently do not have au-
(Lietal.,2023),BGE(Xiaoetal.,2023),Instruc-
ditable training stacks (i.e. a fully reproducible
tOR (Su et al., 2023a), and Jina (Gu¨nther et al.,
trainingpipelinewithavailableweights,data,and
2023, 2024) utilize a multi-stage regime in which
code).
a pretrained transformer is first contrastively fine-
tuned using a large corpus of weakly paired data 3 TrainingData
(e.g. Quora, Reddit Comments) and then addi-
tionally fine-tuned on small, higher quality la- In this section, we describe our data mix across
beled datasets such as MSMarco. The two-stage each training stage. You can access the train-
paradigm significantly improves model quality as ing data of nomic-embed-text-v1 by visiting the
weakly paired data is available in much greater nomic-ai/contrastors code repository. You can
quantity. explore a 5M sample of our contrastive train-
ingpairsathttps://atlas.nomic.ai/map/nomic-text-
Evaluating text embedding models is challeng-
embed-v1-5m-sample.
ing. The BEIR benchmark Thakur et al. (2021)
evaluatesdenseretrieverson15zero-shotretrieval
3.1 MaskedLanguageModelingPretraining
datasets. Early transformer-based text embedding
models such as SBERT (Reimers and Gurevych, Following(Devlinetal.,2019),weuseBooksCor-
2019) were only evaluated on semantic textual pus(Zhuetal.,2015)andaWikipediadumpfrom
similarity (STS) datasets. More recently, MTEB 2023 to train a long-context BERT model, here-
Muennighoffetal.(2023)hasbecomethedefacto inafter called nomic-bert-2048. Each document
benchmark for quantitatively evaluating embed- fromBooksCorpusandWikipediaistokenizedus-
ding models across many tasks, but has limited ing the bert-base-uncased tokenizer from Devlin
evaluations over long context lengths (>512 to- etal.(2019)andpackedtochunksof2048tokens.
kens). Jina Gu¨nther et al. (2024) developed a If a document is shorter than 2048 tokens, we ap-
benchmark of four datasets specialized for long pendanotherdocumentuntilitfits2048tokens. Ifadocumentisgreaterthan2048tokens,wesplitit trainoverthereleasedtrainingsetsfromtheBEIR
acrossmultipledocuments. benchmark(Thakuretal.,2021). Fortheretrieval
datasets (MSMarco, NQ, HotpotQA, and Fever),
3.2 UnsupervisedContrastivePretraining
weminenegatives,ifnotalreadyminedusinggte-
Similar to Wang et al. (2022); Li et al. (2023); baseLi et al. (2023). For every (q,d) pair, we get
Xiao et al. (2023); Ni et al. (2022), we use large thetop-ksimilardocumentsashardnegatives. For
collectionsofpubliclyavailabledatatoformpairs. all other datasets, we randomly sample negatives
These datasets span various objectives and do- inplaceofhardnegativesaswefoundthatmining
mains, from web retrieval to clustering of scien- negativesdidnotimproveperformance.
tificarticles. Intotal,wecurated 470millionpairs Similar to the unsupervised contrastive stage,
across29datasets1.
wesampleadatasetandfillabatchwithallpoints
However, since these datasets can contain fromthatchosendataset.
noisy examples, we employ consistency filtering
(Gu¨ntheretal.,2023;Wangetal.,2022). 4 ExperimentalSetup
Insteadofusingall-MiniLM-L6-v2model2,we
use the gte-base model3. For each pair, described 4.1 ModelArchitecture
as(query,document),weembedboththequeries One of the main drawbacks of existing text en-
anddocumentsofa1millionpointsub-sampleof coders is their limited sequence length, which is
the dataset. For each query, we find the top-k (in predominately capped at 512 tokens. To train a
this case 2) neighbors using cosine similarity. If long sequence length model, we first begin by
document is not in the top-k neighbors, we dis- adapting BERT so it can accommodate a long se-
card the example. After filtering, we end up with quence length. In this work, we target an 8192
∼235Mpairs. Thefulldatasetdistributioncanbe sequencelength. Todoso,weapplythefollowing
seeninTable5. architecture changes and optimizations to BERT
As the majority of these datasets are composed base(Devlinetal.,2019):
of sequences shorter than 2048 tokens we addi-
tionally curate long context datasets to allow for • Substituting absolute positional embeddings
thelearningoflong-rangedependencies. Namely, for rotary positional embeddings (Su et al.,
weusefullWikipediaarticlespairedwiththeirti- 2023b)
tlesaswellasabstractsandfullpaperbodiesfrom
asinglepaperfromS2ORC(Loetal.,2020). • Using SwiGLU activation instead of GeLU
Duringtraining,wesamplepairsfromonedata (Shazeer,2020)
source at a time and fill the entire batch with
samples from that single source to discourage the • UsingFlashAttention(Daoetal.,2022)
modelfromlearningsource-specificshortcuts.
• SettingDropoutto0(GeipingandGoldstein,
3.3 SupervisedContrastiveFine-tuning 2022)
SupervisedfinetuningisperformedonMSMarco
• Vocab size as a multiple of 64 (Portes et al.,
(Bajaj et al., 2018; Wang et al., 2023a), NQ
2023)(Shoeybietal.,2020)
(Karpukhin et al., 2020; Gao and Callan, 2021),
NLI (Gao et al., 2022), HotpotQA (Yang et al.,
resultingina137Mparameterencoder.
2018), FEVER (Thorne et al., 2018), portions of
We train all stages with a max sequence length
MEDI (Su et al., 2023a), WikiAnswers (Fader
of 2048 and employ Dynamic NTK interpola-
et al., 2014), and Reddit4. For the datasets MS-
tion at inference to scale to 8192 sequence length
Marco, NQ, NLI, FEVER, and HotpotQA, we
(Peng et al., 2023; emozilla, 2023). Addition-
1https://huggingface.co/ ally, weoptforSwiGLUversusGeGLUlikepro-
datasets/sentence-transformers/
posedin(Portesetal.,2023)asruntimeisroughly
embedding-training-data
2all-MiniLM-L6-v2 model https://huggingface.co/ 25%fasterforSwiGLUusingtheFlashAttention
thenlper/gte-base) repository5.
3gte-base model (https://huggingface.co/thenlper/
gte-base)
4https://github.com/PolyAI-LDN/conversational- 5https://github.com/Dao-AILab/
datasets/tree/master/reddit flash-attention/tree/mainModel Bsz Steps Seq Cola SST2 MRPCSTSB QQP MNLIQNLI RTE Avg
nomic-bert-2048 4k 100k 2k 0.50 0.93 0.88 0.90 0.92 0.86 0.92 0.82 0.84
MosaicBERT 4k 70k 2k 0.54 0.93 0.87 0.90 0.92 0.86 0.92 0.82 0.85
RobertaBase 8k 500k 512 0.64 0.95 0.90 0.91 0.92 0.88 0.93 0.79 0.86
JinaBERTBase 4k 100k 512 0.51 0.95 0.88 0.90 0.81 0.86 0.92 0.79 0.83
MosaicBERT 4k 178k 128 0.59 0.94 0.89 0.90 0.92 0.86 0.91 0.83 0.85
Table2: GLUEDevSetResults. RobertanumberstakenfromTable8in(Liuetal.,2019). MosaicBertnumbers
takenfromTableS1inPortesetal.(2023)exceptforthe2048modelwhichweevaluatedinthesamemanneras
nomic-bert-2048. JinaBertBaseGlueTestnumbersreportedinTable2from(Gu¨ntheretal.,2024).
4.2 MaskedLanguageModeling 0.01. Gradient clipping is set to 1.0. We use an
linear warmup schedule of 700 steps and an in-
During training, we use a 30% masking rate in-
versesquarerootdecayschedule. Wetrainwitha
stead of 15% following (Portes et al., 2023) and
maxsequencelengthof2048for1fullepochover
weremovetheNextSentencePredictiontask(Liu
thedata.
et al., 2019). We use the AdamW optimizer
Due to GPU memory constraints, we could
(LoshchilovandHutter,2019)withalearningrate
not fit the full model, optimizer, states, and data
of5e-4withβ =0.9β =0.98. Weemployalin-
1 2
into memory. As a workaround, we employ
ear warmup of 6% of the total training steps and
GradCache (Luyu Gao and Callan, 2021) as well
a linear decay to 0. We use a global batch size of
as mixed precision training (Micikevicius et al.,
4096 with gradient accumulation over 8 batches.
2018).
We utilize DeepSpeed (Rajbhandari et al., 2020)
Finally, we use task specific prefixes to break
stage 2 to fit bigger batches into memory. Ad-
the symmetry of the biencoder as in (Wang et al.,
ditionally, we use bfloat16 precision for matrix
2022). Without prefixes, the model receives con-
multiplicationandfp32forgradientaccumulation
flicting reward signal. Consider the case of deter-
dtype. We disable gradient clipping (Liu et al.,
mining which response is closest to the question
2019)andsetweightdecayto1e-5. Wetriedtrain-
”WhatisthecapitalofFrance?”:
ing with a learning rate of 1e-3, but found insta-
bilities during training. We call our final model
1. “What is the name of the capital city of
nomic-bert-2048andalsoreleaseitsweights.
France?
4.3 UnsupervisedContrastivePretraining
2. “ParisisthecapitalofFrance.”
Unsupervisedcontrastivepretrainingaimstoteach
A semantic similarity task would consider the
a model to distinguish the most similar doc-
first closest, while a question answering task
uments from other irrelevant documents. To
would consider the second closest. Prefixes en-
do so, we employ the InfoNCE contrastive loss
able the model to distinguish between the behav-
(van den Oord et al., 2019). For a given batch
iorsspecifiedbyeachofthesetasks.
B = (q ,d ),(q ,d ),...,(q ,d ), we minimize
0 0 1 1 n n
Weusethefollowingtask-specificprefixes:
thelossfunction:
1 (cid:88) es(qi,di)/τ • search query
L = − log
C n es(qi,di)/τ +(cid:80)n es(qi,dj)/τ
i j̸=i • search document
wheres(q,d)isthecosinesimilarityof(q,d)
• classification
We initialize the model for unsupervised con-
trastive training with the weights of nomic-bert-
• clustering
2048. Weuseabatchsizeof16,384soeachbatch
hasalargenumberofin-batchnegatives. Ourop- inspired by Reimers et al. (2023). We first break
timizations for the encoder architecture and train- prefixesintotwocategories: symmetric,wherethe
ing strategy centered around achieving this batch query and document have a similar structure, and
size. We use AdamW with a learning rate of 2e- asymmetric, where the query is usually a single
5, β = 0.9, β = 0.999, and weight decay of sentenceandthedocumentcanbemanysentences.
1 2(Su et al., 2023a) The first two prefixes are used methodolgy presented in (Liu et al., 2019). The
forretrievaltasks: wheresearch queryistypi- GLUEbenchmarkconsistsof9tasks,butweeval-
callyforthequestionandsearch documentis uateon8similarto(Liuetal.,2019).
for the response. classification is used for Foreachtask,wetrainfor10epochswithbatch
STS-related tasks like rephrasals. clustering sizes16,32andlearningrate1e-5,2e-5,3e-5with
isusedfortaskswheretoobjectiveistogroupse- a linear warmup of 6% across 5 seeds. The me-
mantically similar texts close together, like Arxiv dian score per task at the end of the 10 epochs is
title-abstractpairs. Forsymmetrictasks,thesame presentedinTable2. Notewereportaccuracyfor
prefix is appended to both the query and docu- MRPC and QQP and Pearson for STSB 6. We re-
ment. port our results in Table 2. Similar to (Liu et al.,
2019),weinitializefromanMNLIcheckpointfor
4.4 SupervisedContrastiveFine-tuning RTE,STSB,andMRPC.
The last stage of training aims to boost perfor- MosaicBERT (Portes et al., 2023) performs
mance by utilizing human-labeled datasets. Sev- slightly better but is trained for slightly longer
eral papers including (Ni et al., 2021a,b; Wang and on C4 (Raffel et al., 2019). Across all tasks,
et al., 2022; Li et al., 2023) have shown that fine- nomic-bert-2048scoressimilarlytoMosaicBERT
tuning on these datasets leads to improvements in except on Cola. However, we used a longer se-
downstreamperformance. quencelengthmodelandineffecthaveseenmore
We adapt the paired contrastive loss to include tokens during pretraining. JinaBERT also scores
hard negatives in each batch. We train for one similarly, although they report test scores ver-
epoch using seven hard negatives per pair and a sus dev scores and is trained similarly to Mo-
batch size of 256. We employ a learning rate of saicBERT.
2e-5, β = 0.9, β = 0.999, and weight decay
1 2 5.2 MTEBResults
of 0.01. Gradient clipping is set to 1.0. We use
MTEB (Muennighoff et al., 2023) has become
a linear warmup schedule of 400 steps and a lin-
the standard benchmark for evaluating embed-
ear cooldown to 0 and train with prefixes as de-
ding models due to its diverse coverage of 8
scribedabove. Wefoundthatincreasingthenum-
tasks spanning 56 datasets. MTEB evaluated em-
ber of negatives above 7 to not meaningfully im-
bedding models across Classification, Clustering,
prove performance. We also found that training
Pair Classification, Reranking, Retrieval, Seman-
formultipleepochshurtsperformance.
tic Textual Similarity, and Summarization. The
5 Results MTEBscoreisaweightedaverageoftheper-task
scores.
We evaluate nomic-bert-2048 on the GLUE
benchmark (Wang et al., 2019) and find that it 5.3 LongContextResults
is competitive with similarly sized and trained However, as noted in (Gu¨nther et al., 2024),
models. We evaluate nomic-embed-text-v1 on MTEBhasveryfewdatasetsthatincludelongse-
MTEB (Muennighoff et al., 2023), Jina’s Long quences. To evaluate nomic-embed-text-v1’s per-
Context Benchmark (Gu¨nther et al., 2024), and formance on longer sequences, we consider two
LoCo (Saad-Falcon et al., 2024). nomic-embed- additional benchmarks: (Gu¨nther et al., 2024)
text-v1exceedstext-embedding-ada-002andjina- LongContextDatasetaswellastheLoCobench-
embeddings-v2-base-en. On the long con- markfrom(Saad-Falconetal.,2024).
text benchmarks, LoCo and Jina Long Context
5.3.1 JinaAILongContextBenchmark
Benchmark, nomic-embed-text-v1 uniformly out-
performs jina-embeddings-v2-base-en. nomic- TheJinaLongContextBenchmark(Gu¨ntheretal.,
embed-text-v1 outperforms text-embedding-ada- 2024)evaluateson4datasetsacrossRetrievaland
002onLoCoandontwooffourdatasetsinJina’s Clustering; namely, NarrativeQA (Gu¨nther et al.,
LongContextBenchmark.
2024),WikiCites7,SciFact(Waddenetal.,2020),
6https://github.com/
5.1 nomic-bert-2048GLUEResults facebookresearch/fairseq/issues/1561#
issuecomment-571729519
We evaluate nomic-bert-2048 on the GLUE
7https://huggingface.co/datasets/
benchmark (Wang et al., 2019) following the jinaai/cities_wiki_clusteringTable3:ResultsontheMTEBbenchmark(Muennighoffetal.,2023).Thenumbersareaveragedforeachcategory.
Pleaserefertohttps://huggingface.co/spaces/mteb/leaderboardforthescoresperdatasetand
themostuptodateresults.
Category→ Cls. Clust. PairCls.Rerank Retr. STS Summ. Avg
Numberofdatasets→ 12 11 3 4 15 10 1 56
UnsupervisedModels
Glove(Penningtonetal.,2014) 57.3 27.7 70.9 43.3 21.6 61.9 28.9 42.0
SimCSE(Gaoetal.,2022) 62.5 29.0 70.3 46.5 20.3 74.3 31.2 45.5
nomic-embed-text-v1 71.2 42.5 83.7 55.0 48.0 80.8 30.7 59.9
unsup
SupervisedModels
SimCSE (Gaoetal.,2022) 67.3 33.4 73.7 47.5 21.8 79.1 23.3 48.7
bert-sup
Contriever(Izacardetal.,2022a) 66.7 41.1 82.5 53.1 41.9 76.5 30.4 56.0
GTR (Nietal.,2021a) 67.4 42.4 86.1 56.7 48.5 78.4 30.6 59.0
xxl
Sentence-T5 (Nietal.,2021b) 73.4 43.7 85.1 56.4 42.2 82.6 30.1 59.5
xxl
E5 (Wangetal.,2022) 75.2 44.5 86.0 56.6 50.6 82.1 30.2 62.3
large-v2
E5 (Wangetal.,2023b) 78.5 50.3 88.3 60.2 56.9 84.6 31.4 66.6
mistral
GTE (Lietal.,2023) 73.0 46.2 84.6 58.6 51.1 82.3 31.2 62.4
base
GTE (Lietal.,2023) 73.3 46.8 85.0 59.1 52.2 83.4 31.7 63.1
large
BGE (Xiaoetal.,2023) 75.5 45.8 86.6 58.9 53.3 82.4 31.1 63.6
base
BGE (Xiaoetal.,2023) 76.0 46.1 87.1 60.0 54.3 83.1 31.6 64.2
large
Jina (Gu¨ntheretal.,2024) 73.5 41.7 85.4 57.0 47.9 80.7 31.6 60.4
v2
text-embedding-ada-002 70.9 45.9 84.9 56.3 49.3 81.0 30.8 61.0
text-embedding-3-small 73.2 46.7 85.0 56.7 51.1 81.6 31.1 62.3
text-embedding-3-large 75.5 49.0 85.7 59.2 55.4 81.7 29.9 64.6
nomic-embed-text-v1-ablated 73.6 43.7 84.6 53.3 51.4 80.2 31.3 61.4
nomic-embed-text-v1 74.1 43.9 85.2 55.7 52.8 82.1 30.1 62.4
andBigPatent8 (Sharmaetal.,2019). Resultsare TVepisodetranscripts,andscientificresearchpa-
presented in Table 4. Similar to (Gu¨nther et al., pers. We include the QASPER Abstract Articles
2024),wereporttheV-scoresandNDCG@10for dataset for completeness, but would like to high-
the clustering and retrieval datasets respectively. light that many models seem to oversaturate the
Acrosssequencelengthsandtasks,nomic-embed- benchmark and approach 1.0 NDCG@10. Re-
text-v1 beats or ties jina-embeddings-v2-base on sultsarepresentedinTable6. nomic-embed-text-
all datasets at 8k context length. Additionally, v1 beats jina-embeddings-v2-base-en across se-
nomic-embed-text-v1 beats text-embedding-ada- quence lengths. nomic-embed-text-v1 beats M2-
002 on two of the four datasets. We also report Bert at 2048 and is competitive at 8192. At se-
similar results to (Gu¨nther et al., 2024) on Wi- quencelength4096,nomic-embed-text-v1iscom-
kiCitiesClustering that sequence length hurts per- petitive with E5 Mistral while being significantly
formance,suggestingthatlongersequencelengths smaller.
arenotnecessarytoperformwellonthetest.
5.4 Few-ShotEvaluationofBEIR
5.3.2 LoCoBenchmark
While the BEIR component of MTEB was origi-
The LoCo Benchmark consists of 5 retrieval
nally purposed as a zero-shot benchmark, several
datasets,3from(Shahametal.,2022)and2from
top open-source models, including BGE (Xiao
(Dasigietal.,2021). Thebenchmarktestsretrieval
etal.,2023),GTE(Lietal.,2023),andE5-Mistral
acrossmeetingtranscripts,nationalpolicyreports,
(Wangetal.,2023b)reporttrainingontrainsplits
of BEIR benchmark datasets such as FEVER and
8https://huggingface.co/datasets/
jinaai/big-patent-clustering HotpotQA.TounderstandtheimpactofthisonourModel Seq NarrativeQA WikiCities SciFact BigPatent Avg
nomic-embed-text-v1 128 20.1 90.0 65.4 18.5 48.5
nomic-embed-text-v1-ablated 128 20.8 86.8 65.2 17.5 47.6
jina-embeddings-base-v2 128 19.6 79.9 62.1 14.4 44.0
text-embedding-ada-002 128 25.4 84.9 68.8 16.6 48.9
text-embedding-3-small 128 29.5 87.5 68.8 15.0 50.2
text-embedding-3-large 128 45.6 87.9 74.8 16.5 56.2
nomic-embed-text-v1 512 23.9 88.7 70.5 25.3 52.1
nomic-embed-text-v1-ablated 512 25.7 81.9 71.5 23.7 50.7
jina-embeddings-base-v2 512 21.3 79.3 66.7 21.9 47.3
text-embedding-ada-002 512 25.5 84.8 72.6 23.0 51.5
text-embedding-3-small 512 32.2 89.0 73.2 23.6 54.5
text-embedding-3-large 512 48.1 89.9 77.6 23.6 59.6
nomic-embed-text-v1 8191 37.8 84.3 70.2 24.5 54.2
nomic-embed-text-v1-ablated 8191 44.0 77.4 69.1 23.6 53.5
jina-embeddings-base-v2 8191 39.4 75.7 69.4 23.1 51.9
text-embedding-ada-002 8191 41.1 84.7 72.7 22.5 55.3
text-embedding-3-small 8191 47.1 89.9 73.3 22.5 58.3
text-embedding-3-large 8191 51.6 86.2 77.7 19.3 58.7
Table 4: Jina Long Context Evaluation Benchmark. Numbers for text-embedding-ada-002 and
jina-embeddings-base-v2takenfrom(Gu¨ntheretal.,2024).
downstreamscores,wealsotrainanomic-embed- 7 Conclusion
text-v1-ablatedmodelthatomitstheFEVER,Hot-
potQA, and MEDI datasets. As reported in Ta-
We release the first fully open-source long con-
ble 1, this decreases our overall MTEB score by
texttextembeddingmodelthatsurpassesOpenAI
about one point. To maintain an apples-to-apples
Ada-002 performance on both sort and long con-
comparison with top open-source models, we opt
text benchmarks. We release the model weights
to train on the FEVER, HotpotQA, and MEDI
and training code under a permissible license as
datasetsforthereleasedversionofnomic-embed-
wellastherecipe,includingdata,toreproducethe
text-v1. Unfortunately,duetothenatureofclosed-
model.
source models, we have no indication regarding
whether closed-source models trained on these
datasets. 7.1 Contributions
Zach Nussbaum lead the project, including the
majority of the implementation, training and data
6 TrainingResources
decisions present in the final version, as well as
making several design decisions at all levels of
the stack. Jack Morris made several design con-
Full training of nomic-embed-text-v1 can be con- tributions regarding dataset curation and model
ducted in a single week on one 8xH100 node. architecture. Brandon Duderstadt made several
Masked language modeling of nomic-bert-2048 design contributions across the entire stack and
takesroughly4days. Contrastivepretraininglasts wrotethebaseimplementationofthedatacuration
3 and a half days. Contrastive fine-tuning takes pipeline. Andriy Mulyar set early project direc-
one hour. We encourage the reader to initialize tion, reviewed code implementations, and made
from our nomic-bert-2048 or Unsupervised Con- several model design and dataset curation contri-
strastive checkpoints, released under the same li- butions.
censeasnomic-embed-text-v1.References Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2022.
Simcse:Simplecontrastivelearningofsentenceem-
PayalBajaj, DanielCampos, NickCraswell, LiDeng,
beddings.
Jianfeng Gao, Xiaodong Liu, Rangan Majumder,
Andrew McNamara, Bhaskar Mitra, Tri Nguyen,
Jonas Geiping and Tom Goldstein. 2022. Cramming:
MirRosenberg,XiaSong,AlinaStoica,SaurabhTi-
Training a language model on a single gpu in one
wary, and Tong Wang. 2018. Ms marco: A human
day.
generatedmachinereadingcomprehensiondataset.
Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda,
SamuelR.Bowman,GaborAngeli,ChristopherPotts,
Anirudha Rayasam, and Zachary C Lipton. 2019.
and Christopher D. Manning. 2015. A large an-
Amazonqa: A review-based question answering
notated corpus for learning natural language infer-
task.
ence. In Proceedings of the 2015 Conference on
EmpiricalMethodsinNaturalLanguageProcessing
Michael Gu¨nther, Louis Milliken, Jonathan Geuter,
(EMNLP). Association for Computational Linguis-
GeorgiosMastrapas,BoWang,andHanXiao.2023.
tics.
Jina embeddings: A novel set of high-performance
sentenceembeddingmodels.
WilliamCosterandDavidKauchak.2011. SimpleEn-
glishWikipedia: Anewtextsimplificationtask. In
MichaelGu¨nther,JackminOng,IsabelleMohr,Alaed-
Proceedingsofthe49thAnnualMeetingoftheAsso-
dineAbdessalem,TanguyAbel,MohammadKalim
ciationforComputationalLinguistics: HumanLan-
Akram,SusanaGuzman,GeorgiosMastrapas,Saba
guageTechnologies,pages665–669,Portland,Ore-
Sturua,BoWang,MaximilianWerk,NanWang,and
gon, USA. Association for Computational Linguis-
Han Xiao. 2024. Jina embeddings 2: 8192-token
tics.
general-purpose text embeddings for long docu-
ments.
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,
andChristopherRe´.2022. Flashattention: Fastand
Felix Hamborg, Norman Meuschke, Corinna Bre-
memory-efficientexactattentionwithio-awareness.
itinger, and Bela Gipp. 2017. news-please: A
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, genericnewscrawlerandextractor. InProceedings
NoahA.Smith,andMattGardner.2021. Adataset ofthe15thInternationalSymposiumofInformation
of information-seeking questions and answers an- Science,pages218–223.
choredinresearchpapers.
ChristopherHideyandKathyMcKeown.2016. Identi-
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and fying causal relations using parallel Wikipedia ar-
KristinaToutanova.2019. Bert:Pre-trainingofdeep ticles. In Proceedings of the 54th Annual Meet-
bidirectional transformers for language understand- ingoftheAssociationforComputationalLinguistics
ing. (Volume1: LongPapers),pages1424–1433,Berlin,
Germany. Association for Computational Linguis-
emozilla. 2023. Dynamically scaled rope further in- tics.
creasesperformanceoflongcontextllamawithzero
fine-tuning. EvanHubinger,CarsonDenison,JesseMu,MikeLam-
bert, Meg Tong, Monte MacDiarmid, Tamera Lan-
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. ham, Daniel M. Ziegler, Tim Maxwell, Newton
2014. OpenQuestionAnsweringOverCuratedand Cheng, Adam Jermyn, Amanda Askell, Ansh Rad-
ExtractedKnowledgeBases. InKDD. hakrishnan,CemAnil,DavidDuvenaud,DeepGan-
guli,FazlBarez,JackClark,KamalNdousse,Kshi-
AngelaFan,YacineJernite,EthanPerez,DavidGrang-
tijSachan,MichaelSellitto,MrinankSharma,Nova
ier, Jason Weston, and Michael Auli. 2019. ELI5:
DasSarma, Roger Grosse, Shauna Kravec, Yuntao
long form question answering. In Proceedings of
Bai, Zachary Witten, Marina Favaro, Jan Brauner,
the 57th Conference of the Association for Compu-
HoldenKarnofsky,PaulChristiano,SamuelR.Bow-
tationalLinguistics,ACL2019,Florence,Italy,July
man, Logan Graham, Jared Kaplan, So¨ren Minder-
28- August 2, 2019, Volume 1: Long Papers, pages
mann, Ryan Greenblatt, Buck Shlegeris, Nicholas
3558–3567.AssociationforComputationalLinguis-
Schiefer, and Ethan Perez. 2024. Sleeper agents:
tics.
Training deceptive llms that persist through safety
Katja Filippova and Yasemin Altun. 2013. Overcom- training.
ingthelackofparalleldatainsentencecompression.
HamelHusain,Ho-HsiangWu,TiferetGazit,Miltiadis
In Proceedings of the 2013 Conference on Empiri-
Allamanis, and Marc Brockschmidt. 2019. Code-
calMethodsinNaturalLanguageProcessing,pages
SearchNetchallenge:Evaluatingthestateofseman-
1481–1491,Seattle,Washington,USA.Association
ticcodesearch. arXivpreprintarXiv:1909.09436.
forComputationalLinguistics.
WikimediaFoundation. Wikimediadownloads. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-
bastian Riedel, Piotr Bojanowski, Armand Joulin,
LuyuGaoandJamieCallan.2021. Condenser: apre- andEdouardGrave.2022a. Unsuperviseddensein-
trainingarchitecturefordenseretrieval. formationretrievalwithcontrastivelearning.Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu- ArvindNeelakantan,TaoXu,RaulPuri,AlecRadford,
cas Hosseini, Fabio Petroni, Timo Schick, Jane Jesse Michael Han, Jerry Tworek, Qiming Yuan,
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Nikolas Tezak, Jong Wook Kim, Chris Hallacy,
Edouard Grave. 2022b. Atlas: Few-shot learning Johannes Heidecke, Pranav Shyam, Boris Power,
withretrievalaugmentedlanguagemodels. Tyna Eloundou Nekoul, Girish Sastry, Gretchen
Krueger, David Schnurr, Felipe Petroski Such,
VladimirKarpukhin,BarlasOg˘uz,SewonMin,Patrick Kenny Hsu, Madeleine Thompson, Tabarak Khan,
Lewis,LedellWu,SergeyEdunov,DanqiChen,and Toki Sherbakov, Joanne Jang, Peter Welinder, and
Wen tau Yih. 2020. Dense passage retrieval for Lilian Weng. 2022. Text and code embeddings by
open-domainquestionanswering. contrastivepre-training.
DanielKhashabi,AmosNg,TusharKhot,AshishSab- Jianmo Ni, Gustavo Hernandez Abrego, Noah Con-
harwal, Hannaneh Hajishirzi, and Chris Callison- stant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei
Burch.2021. Gooaq:Openquestionansweringwith Yang. 2022. Sentence-t5: Scalable sentence en-
diverseanswertypes. coders from pre-trained text-to-text models. In
FindingsoftheAssociationforComputationalLin-
MahnazKoupaeeandWilliamYangWang.2018. Wik-
guistics: ACL2022,pages1864–1874,Dublin,Ire-
ihow: Alargescaletextsummarizationdataset.
land.AssociationforComputationalLinguistics.
Patrick Lewis, Ethan Perez, Aleksandra Piktus,
Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019.
Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Justifying recommendations using distantly-labeled
Heinrich Ku¨ttler, Mike Lewis, Wen tau Yih,
reviews and fine-grained aspects. In Proceedings
Tim Rockta¨schel, Sebastian Riedel, and Douwe
of the 2019 Conference on Empirical Methods in
Kiela. 2021a. Retrieval-augmented generation for
Natural Language Processing and the 9th Interna-
knowledge-intensivenlptasks.
tional Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 188–197, Hong
Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale
Kong, China. Association for Computational Lin-
Minervini, Heinrich Ku¨ttler, Aleksandra Piktus,
guistics.
PontusStenetorp,andSebastianRiedel.2021b. Paq:
65 million probably-asked questions and what you
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-
candowiththem.
tavo Herna´ndez A´brego, Ji Ma, Vincent Y. Zhao,
ZehanLi,XinZhang,YanzhaoZhang,DingkunLong, YiLuan,KeithB.Hall,Ming-WeiChang,andYinfei
Pengjun Xie, and Meishan Zhang. 2023. To- Yang.2021a. Largedualencodersaregeneralizable
wards general text embeddings with multi-stage retrievers.
contrastivelearning.
Jianmo Ni, Gustavo Herna´ndez A´brego, Noah Con-
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man- stant, Ji Ma, Keith B. Hall, Daniel Cer, and Yinfei
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Yang. 2021b. Sentence-t5: Scalable sentence en-
Luke Zettlemoyer, and Veselin Stoyanov. 2019. codersfrompre-trainedtext-to-textmodels.
Roberta: A robustly optimized bert pretraining ap-
Aaron van den Oord, Yazhe Li, and Oriol Vinyals.
proach.
2019. Representationlearningwithcontrastivepre-
KyleLo,LucyLuWang,MarkNeumann,RodneyKin- dictivecoding.
ney, and Dan S. Weld. 2020. S2orc: The semantic
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-
scholaropenresearchcorpus.
ricoShippole.2023. Yarn:Efficientcontextwindow
Ilya Loshchilov and Frank Hutter. 2019. Decoupled extensionoflargelanguagemodels.
weightdecayregularization.
Jeffrey Pennington, Richard Socher, and Christopher
JiaweiHanLuyuGao,YunyiZhangandJamieCallan. Manning. 2014. GloVe: Global vectors for word
2021. Scaling deep contrastive learning batch size representation. In Proceedings of the 2014 Con-
undermemorylimitedsetup. InProceedingsofthe ferenceonEmpiricalMethodsinNaturalLanguage
6thWorkshoponRepresentationLearningforNLP. Processing (EMNLP), pages 1532–1543, Doha,
Qatar.AssociationforComputationalLinguistics.
Paulius Micikevicius, Sharan Narang, Jonah Alben,
Gregory Diamos, Erich Elsen, David Garcia, Boris Jacob Portes, Alex Trott, Sam Havens, Daniel King,
Ginsburg, Michael Houston, Oleksii Kuchaiev, Abhinav Venigalla, Moin Nadeem, Nikhil Sardana,
GaneshVenkatesh,andHaoWu.2018. Mixedpre- Daya Khudia, and Jonathan Frankle. 2023. Mo-
cisiontraining. saicbert: Abidirectionalencoderoptimizedforfast
pretraining.
NiklasMuennighoff.2022. Sgpt:Gptsentenceembed-
dingsforsemanticsearch. ColinRaffel,NoamShazeer,AdamRoberts,Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
NiklasMuennighoff,NouamaneTazi,Lo¨ıcMagne,and WeiLi,andPeterJ.Liu.2019. Exploringthelimits
NilsReimers.2023. Mteb: Massivetextembedding oftransferlearningwithaunifiedtext-to-texttrans-
benchmark. former. arXive-prints.Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Nandan Thakur, Nils Reimers, Andreas Ru¨ckle´, Ab-
and Yuxiong He. 2020. Zero: Memory optimiza- hishekSrivastava,andIrynaGurevych.2021. Beir:
tionstowardtrainingtrillionparametermodels. Aheterogenousbenchmarkforzero-shotevaluation
ofinformationretrievalmodels.
PranavRajpurkar,JianZhang,KonstantinLopyrev,and
Percy Liang. 2016. SQuAD: 100,000+ Questions James Thorne, Andreas Vlachos, Christos
forMachineComprehensionofText. arXive-prints, Christodoulopoulos, and Arpit Mittal. 2018.
pagearXiv:1606.05250. FEVER:alarge-scaledatasetforfactextractionand
VERification. InNAACL-HLT.
OriRam,YoavLevine,ItayDalmedigos,DorMuhlgay,
Voyage. 2023. Excited to announce voyage embed-
Amnon Shashua, Kevin Leyton-Brown, and Yoav
dings!
Shoham.2023. In-contextretrieval-augmentedlan-
guagemodels.
David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu
Wang, Madeleine van Zuylen, Arman Cohan, and
Nils Reimers, Elliot Choi, Amr Kayid, Alekhya Nan-
Hannaneh Hajishirzi. 2020. Fact or fiction: Veri-
dula, Manoj Govindassamy, and Abdullah Elkady.
fying scientific claims. In Proceedings of the 2020
2023. Introducingembedv3.
Conference on Empirical Methods in Natural Lan-
guageProcessing(EMNLP),pages7534–7550,On-
Nils Reimers and Iryna Gurevych. 2019. Sentence-
line.AssociationforComputationalLinguistics.
bert: Sentence embeddings using siamese bert-
networks.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
Jon Saad-Falcon, Dan Fu, and Simran Arora. 2024.
GLUE: A multi-task benchmark and analysis plat-
Long-contextretrievalmodelswithmonarchmixer.
formfornaturallanguageunderstanding. InthePro-
ceedingsofICLR.
AbigailSee,PeterJ.Liu,andChristopherD.Manning.
2017. Gettothepoint:Summarizationwithpointer- Liang Wang, Nan Yang, Xiaolong Huang, Binxing
generatornetworks. InProceedingsofthe55thAn- Jiao,LinjunYang,DaxinJiang,RanganMajumder,
nual Meeting of the Association for Computational and Furu Wei. 2022. Text embeddings by weakly-
Linguistics (Volume 1: Long Papers), pages 1073– supervisedcontrastivepre-training.
1083, Vancouver, Canada. Association for Compu-
tationalLinguistics. Liang Wang, Nan Yang, Xiaolong Huang, Binxing
Jiao,LinjunYang,DaxinJiang,RanganMajumder,
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori andFuruWei.2023a. Simlm: Pre-trainingwithrep-
Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, resentationbottleneckfordensepassageretrieval.
MorGeva, JonathanBerant, andOmerLevy.2022.
SCROLLS: Standardized CompaRison over long LiangWang,NanYang,XiaolongHuang,LinjunYang,
language sequences. In Proceedings of the 2022 Rangan Majumder, and Furu Wei. 2023b. Improv-
Conference on Empirical Methods in Natural Lan- ingtextembeddingswithlargelanguagemodels.
guageProcessing,pages12007–12021,AbuDhabi,
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas
United Arab Emirates. Association for Computa-
Muennighoff.2023. C-pack: Packagedresourcesto
tionalLinguistics.
advancegeneralchineseembedding.
Eva Sharma, Chen Li, and Lu Wang. 2019. BIG-
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
PATENT: A large-scale dataset for abstractive and
gio, William W. Cohen, Ruslan Salakhutdinov, and
coherentsummarization. CoRR,abs/1906.03741.
Christopher D. Manning. 2018. HotpotQA: A
dataset for diverse, explainable multi-hop question
Noam Shazeer. 2020. Glu variants improve trans-
answering. InConferenceonEmpiricalMethodsin
former.
NaturalLanguageProcessing(EMNLP).
Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2016.
Patrick LeGresley, Jared Casper, and Bryan Catan-
Character-levelconvolutionalnetworksfortextclas-
zaro.2020. Megatron-lm:Trainingmulti-billionpa-
sification.
rameterlanguagemodelsusingmodelparallelism.
Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan
Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Salakhutdinov, Raquel Urtasun, Antonio Torralba,
Yushi Hu, Mari Ostendorf, Wen tau Yih, Noah A. andSanjaFidler.2015. Aligningbooksandmovies:
Smith, Luke Zettlemoyer, and Tao Yu. 2023a. One Towards story-like visual explanations by watching
embedder, any task: Instruction-finetuned text em- moviesandreadingbooks.
beddings.
Appendix
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. 2023b. Roformer: En-
hancedtransformerwithrotarypositionembedding.Table5: PretrainingDatasetDistribution
Dataset Datapoints %Dataset
Reddita 64,978,944 0.28
PAQ(Lewisetal.,2021b) 52,953,088 0.23
AmazonReviews(Nietal.,2019) 38,682,624 0.16
S2ORCTitleAbstract(Loetal.,2020) 35438592 0.15
WikiAnswers(Faderetal.,2014) 9,912,320 0.04
S2ORCCitationTitles(Loetal.,2020) 7,585,792 0.03
S2ORCAbstractCitation(Loetal.,2020) 7,503,872 0.03
S2ORCAbstractBody(Loetal.,2020) 6,389,760 0.03
WikipediaTitleBody(Foundation) 6,078,464 0.03
Gooaq(Khashabietal.,2021) 1,245,184 0.01
Codesearch(Husainetal.,2019) 835,584 <.01
AGNews(Zhangetal.,2016) 409,600 <.01
CCNews(Hamborgetal.,2017) 344,064 <.01
NPRb 344,064 <.01
CNN(Seeetal.,2017) 278,528 <.01
YahooTitle-Answerc 262,144 <.01
AmazonQA(Guptaetal.,2019) 212,992 <.01
YahooTitle-Questiond 196,608 <.01
SentenceCompression(FilippovaandAltun,2013) 163,840 <.01
YahooQAe 131,072 <.01
ELI5(Fanetal.,2019) 98,304 <.01
Altlex(HideyandMcKeown,2016) 98,304 <.01
Wikihow(KoupaeeandWang,2018) 81,920 <.01
SimpleWiki(CosterandKauchak,2011) 81,920 <.01
StackExchangeDuplicateQuestionsf 65,536 <.01
StackExchangeTitleBodyg 65,536 <.01
StackExchangeBodyBodyh 65,536 <.01
QuoraDuplicateQuestionsi 32,768 <.01
SQuAD(Rajpurkaretal.,2016) 16,384 <.01
Total 234,553,344 1
ahttps://huggingface.co/datasets/sentence-transformers/reddit-title-body
bhttps://files.pushshift.io/news/
chttps://www.kaggle.com/soumikrakshit/yahoo-answers-dataset
dhttps://www.kaggle.com/soumikrakshit/yahoo-answers-dataset
ehttps://www.kaggle.com/soumikrakshit/yahoo-answers-dataset
fhttps://data.stackexchange.com/apple/query/fork/1456963
ghttps://data.stackexchange.com/apple/query/fork/1456963
hhttps://data.stackexchange.com/apple/query/fork/1456963
ihttps://quoradata.quora.com/First-Quora-Dataset-Release-Question-PairsModel Seq Param. Tau Tau Tau QASP. QASP. Avg
Scr. Gov. QMS. Tit. Abs.
Art. Art.
UnsupervisedModels
Jina (Gu¨ntheretal.,2024) 2048 137M 87.2 97.7 35.1 95.3 99.7 83.0
base-v2
Jina (Gu¨ntheretal.,2023) 8192 137M 93.3 98.6 40.8 95.1 99.3 85.5
base-v2
nomic-embed-text-v1-ablated 2048 137M 83.1 97.3 49.4 97.4 99.9 85.4
nomic-embed-text-v1-ablated 4096 137M 89.1 97.6 49.6 97.5 99.9 86.7
nomic-embed-text-v1-ablated 8192 137M 92.5 97.8 47.6 96.5 99.9 86.9
nomic-embed-text-v1 2048 137M 86.1 96.9 47.8 96.1 99.7 85.3
nomic-embed-text-v1 4096 137M 89.0 97.4 45.7 95.8 99.9 85.6
nomic-embed-text-v1 8192 137M 90.9 97.8 44.2 94.9 99.9 85.5
text-embedding-ada-002 8192 N/A 37.3 44.3 7.30 85.1 89.7 52.7
text-embedding-3-small 8192 N/A 92.2 97.7 27.4 95.9 98.9 82.4
text-embedding-3-large 8192 N/A 88.0 93.6 25.5 93.2 96.8 79.4
E5 (Wangetal.,2023b) 4096 7B 95.9 98.3 46.8 98.4 99.8 87.8
mistral
SupervisedModels
M2-Bert(Saad-Falconetal.,2024) 2048 80M 81.8 94.7 58.5 87.3 95.5 83.6
M2-Bert(Saad-Falconetal.,2024) 8192 80M 94.7 96.5 64.1 86.8 97.5 87.9
Table 6: Results on the LoCo benchmark (Saad-Falcon et al., 2024). NCDG@10 is reported for each dataset.
Wesplitevaluationsintoparameterclassandwhethertheevaluationisperformedinasupervisedorunsupervised
setting. We bold the top-performing model in each split. Nomic-embed-text-v1 is the best-performing 100M
parameterclassunsupervisedmodel. Nomic-embed-text-v1iscompetitivewiththetop-performingmodelsinboth
the7BparameterclassandwithmodelstrainedinasupervisedsettingspecificallyfortheLoCobenchmark.