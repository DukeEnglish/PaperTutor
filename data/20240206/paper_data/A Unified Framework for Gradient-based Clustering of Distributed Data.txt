A Unified Framework for Gradient-based Clustering of
Distributed Data
Aleksandar Armacki1, Dragana Bajović2, Dušan Jakovetić3, and Soummya Kar1
1Carnegie Mellon University, Pittsburgh, PA, USA,
{aarmacki,soummyak}@andrew.cmu.edu
2Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia,
dbajovic@uns.ac.rs
3Faculty of Sciences, University of Novi Sad, Novi Sad, Serbia,
dusan.jakovetic@dmi.uns.ac.rs
Abstract
We develop a family of distributed clustering algorithms that work over networks of
users. In the proposed scenario, users contain a local dataset and communicate only with
their immediate neighbours, with the aim of finding a clustering of the full, joint data.
Theproposedfamily,termedDistributedGradientClustering(DGC-F ),isparametrized
ρ
by ρ ≥ 1, controling the proximity of users’ center estimates, with F determining the
clustering loss. Specialized to popular clustering losses like K-means and Huber loss,
DGC-F gives rise to novel distributed clustering algorithms DGC-KM and DGC-HL ,
ρ ρ ρ
whileanovelclusteringlossbasedonthelogisticfunctionleadstoDGC-LL . Weprovide
ρ
a unified analysis and establish several strong results, under mild assumptions. First, the
sequence of centers generated by the methods converges to a well-defined notion of fixed
point, under any center initialization and value of ρ. Second, as ρ increases, the family
of fixed points produced by DGC-F converges to a notion of consensus fixed points. We
ρ
show that consensus fixed points of DGC-F are equivalent to fixed points of gradient
ρ
clustering over the full data, guaranteeing a clustering of the full data is produced. For
the special case of Bregman losses, we show that our fixed points converge to the set of
Lloyd points. Numerical experiments on real data confirm our theoretical findings and
demonstrate strong performance of the methods.
1 Introduction
Clustering is an unsupervized learning problem, where the goal is to group the data based
on a similarity criteria, without having any prior knowledge of the underlying distribution or
the true number of groups, e.g., Xu and Wunsch (2005); Jain (2010). Clustering is applied
across a wide range of problems and domains, such as marketing research, text classification,
anomalydetection, andbiomedicalapplications, e.g., ArabieandHubert(1996);Dhillonetal.
(2003); Chandola et al. (2009); Xu and Wunsch (2010); Pediredla and Seelamantula (2011).
Clustering can be hard or soft, with hard clustering assigning each sample to only one cluster,
while soft clustering outputs the probability of a point belonging to each cluster. We are
1
4202
beF
2
]GL.sc[
1v20310.2042:viXra(a)Client-server (b)Peer-to-peer
Figure1: Anexampleofclient-serverandpeer-to-peersetupsindistributedlearning. Verticesrepresentsusers
and edges represent communication links.
interested in hard clustering. Formally, for a given dataset D = {y ,...,y } ⊂ Rd, the hard
1 N
clustering problem of partitioning D into K disjoint clusters is given by
(cid:88) (cid:88)
min H(x,C) = w f(x(k),y ), (1)
r r
x∈RKd,C∈CK,D
k∈[K]r∈C(k)
where x = (cid:2) x(1)⊤ ... x(K)⊤(cid:3)⊤ is the vector stacking the K centers x(k) ∈ Rd, C is the
K,D
set of all K-partitions of D, i.e., C ∈ C is a K-tuple C = (cid:0) C(1),...,C(K)(cid:1), with each
K,D
C(k) being a subset of D1, such that C(k)∩C(l) = ∅ and ∪ C(k) = D, w ∈ (0,1) is a
k∈[K] r
fixed weight associated with sample r, such that (cid:80) w = 1, and f : Rd ×Rd (cid:55)→ [0,∞)
r∈[N] r
a loss function. For example, setting f to be the squared Euclidean norm, one recovers the
renowned K-means clustering problem, e.g., Lloyd (1982); Awasthi and Balcan (2016). In
general, the problem (1) is highly non-convex and NP-hard, see, e.g., Selim and Ismail (1984);
Megiddo and Supowit (1984); Vattani (2009); Awasthi et al. (2015). As such, the best one can
hope for is reaching a stationary point of (1), with various schemes guaranteeing convergence
to stationary points, e.g., MacQueen (1967); Lloyd (1982); Banerjee et al. (2005); Pediredla
and Seelamantula (2011); Armacki et al. (2022a).
Distributed learning is a paradigm attracting great interest recently, as it offers many
benefits, like privacy, as users’ data is stored locally, only exchanging model parameters, e.g.,
McMahan et al. (2017). Another benefit is decreased computation and storage burden, as
data is handled locally, making it easier to parse smaller chunks of data, e.g., Yang (2013);
Jakovetić et al. (2020). For example, it is estimated that in 2024 around 147 zetabytes of data
will be produced worldwide, a growth of over a 100% compared to the amount in 2020 Taylor
(2021). Such rapid increase in the amount of data often renders storing and processing the
dataatasinglelocationimpossible, makingdistributedalgorithmsessential. Communication-
wise, distributed algorithms are client-server, e.g., McMahan et al. (2017); Li et al. (2020);
Kairouz et al. (2021) or peer-to-peer, e.g., Kar et al. (2012); Sayed (2014); Vlaski et al. (2023).
Compared to client-server, peer-to-peer methods have no coordinator and users communicate
directly. Since no single point of failure exists, peer-to-peer methods are more resilient to
unreliable or adversarial users, e.g., Tsianos et al. (2012); Sundaram and Gharesifard (2019);
Yu and Kar (2023). Figure 1 visualizes the client-server and peer-to-peer communication
setups. We study the peer-to-peer setup and will use the term “distributed” to refer to it.
The distributed setting presents a unique challenge for clustering, as users store their data
locally, making it difficult to produce a clustering of the full, joint dataset. Many recent
1In a slight abuse of notation, we will also use D to denote the set of indices of the data, i.e., D=[N].
2Table 1: Comparison of clustering algorithms. ✔ means a condition is satisfied and ✗ otherwise. Peer-to-peer
and Beyond KM refer to whether an algorithm works in peer-to-peer networks and provides methods beyond
K-meansclustering. Storageandcommunicationcostsarecomputedperuser. D andN arethedatasetand
i i
neighbourhood of user i, D = ∪ D the full data, m the number of users, d the data dimension, T the
i∈[m] i
numberofiterationsforwhichanalgorithmisrun,B thenumberofcenterupdatesperiteration(seeSections
2, 3 for definitions).
Method Communication Cost Storage Cost Peer-to-peer Beyond KM
Forero et al. (2011) TKd ((|N |+1)K+|D |)d+|D | ✔ ✗†
i i i
Kar and Swenson (2019) TKd (K+|D |)d+|D | ✔ ✗
i i
Armacki et al. (2022a) −‡ (K+|D|)d+|D| ✗ ✔
DGC-F (This paper) BTKd (K+|D |)d+|D | ✔ ✔
i i
† While Forero et al. (2011) provide methods for soft clustering, only a K-means version is made for hard clustering.
‡ The method from Armacki et al. (2022a) is centralized, hence communication cost measurements do not apply.
works focus on the client-server setting, e.g., Li and Guo (2018); Alguliyev et al. (2021); Dafir
et al. (2021); Dennis et al. (2021); Qiao et al. (2023); Huang et al. (2023). In comparison,
peer-to-peer clustering has been studied in proportionally much smaller body of work. In
this paper we develop a unified approach for peer-to-peer (hard) clustering. We do so by first
proposing a general clustering problem that can be solved in a distributed manner and gives
rise to distributed versions of popular (centralized) clustering problems, such as K-means,
e.g., Lloyd (1982) and Huber loss clustering, e.g., Pediredla and Seelamantula (2011). Next,
we develop a method that solves the general problem, provably converges, results in novel
clustering algorithms when applied to specific losses and produces a clustering of the full
data in peer-to-peer networks. Our method is general, easy to implement and exhibits strong
theoretical and practical performance (see Sections 3-5).
Literature review. Peer-to-peer clustering methods have been proposed in Datta et al.
(2009); Forero et al. (2011); Balcan et al. (2013); Oliva et al. (2013); Qin et al. (2017); Kar
and Swenson (2019); Yu et al. (2021). Datta et al. (2009) propose approximate K-means
algorithms for both peer-to-peer and client-server networks, with a theoretical study limited
to the client-server case. Forero et al. (2011); Qin et al. (2017); Yu et al. (2021) study soft and
hard K-means. Only the method in Forero et al. (2011), built on the Alternating Directions
Method of Multipliers (ADMM) framework, provably converges. Balcan et al. (2013) study
the K-means and K-medians problems and propose methods with provable guarantees. Oliva
et al. (2013) study K-means in the special case were users have a single sample. Kar and
Swenson (2019) propose a parametric family of K-means methods with provable guarantees.
It is worth mentioning Armacki et al. (2022a), who propose a general framework for gradient-
based clustering in centralized setting. Our work can be seen as the distributed counterpart
of their method, with important differences discussed in Section 3. Note that the literature of
distributed hard clustering is lacking in methods beyond K-means, as the efforts almost ex-
clusively focus on designing variants of K-means. It is well known in the centralized literature
that, in applications where different properties, such as robustness to outliers, are desired,
methods beyond K-means are required, e.g., Pediredla and Seelamantula (2011); Arora et al.
(1998); Banerjee et al. (2005). While Balcan et al. (2013) provide a distributed K-median
method, theirapproachisbuiltontheideaofdesigningacoreset, e.g., Har-PeledandMazum-
dar (2004), i.e., creating a set that approximates the full data and use it for training. The
construction of coreset is costly, as it requires running involved approximation algorithms and
3users communicating subsets of data. As such, the methods in Balcan et al. (2013) can incur
high communication and storage costs, as the size of the coreset scales with the number of
users m and desired approximation quality2. Moreover, their approach requires users to share
the local data, which can make them reluctant to participate, due to privacy concerns. Works
Forero et al. (2011); Kar and Swenson (2019); Armacki et al. (2022a) are closest to ours, as
they provide iterative methods with strong theoretical guarantees, that only exchange local
center estimates. We provide a comparison with these methods in Table 1, highlighting key3
algorithmic features, i.e., communication and storage cost, if they work in distributed setting
and if they provide clustering beyond K-means. We can see from Table 1 that our proposed
method has the better or comparable communication and storage cost (B can be set to 1),
while being theonly method thatsimultaneously works in peer-to-peer networksand supports
costs beyond K-means.
Another line of work related to ours is that of first-order methods for distributed opti-
mization, e.g., Nedić and Ozdaglar (2009); Nedić et al. (2017); Shi et al. (2015); Lorenzo and
Scutari (2016); Jakovetić et al. (2014, 2018); Xin et al. (2020, 2022a,b); Swenson et al. (2022).
As the framework proposed in this paper is gradient-based and used to solve a distributed
optimization problem (see Sections 2, 3), it is related to first-order distributed optimization
methods, with some key differences. First, we consider the specific problem of clustering,
which, apart from Kar and Swenson (2019)4, has not been studied in the context of first-order
methods. Second, due to optimizing over both centers and clusters (see Sections 2, 3), the
problem considered in this paper is a combined distributed continuous (centers) and discrete
(clusters) problem and the analysis techniques used in classical distributed optimization are
not applicable, with a novel convergence analysis derived.
Contributions. Our contributions are as follows.
• Weproposeageneralapproachforclusteringdataoverpeer-to-peernetworks,dubbedDGC-
F . Our approach encompasses smooth, convex loss functions and general distance metrics,
ρ
such as K-means, Huber or logistic loss and Euclidean and Mahalanobis distance. DGC-F
ρ
works over any connected communication graph and users only exchange center estimates,
with data staying private.
• We establish convergence guarantees in the following regimes. For fixed ρ, we show that
DGC-F converges to aptly defined fixed points under any center initialization, making it
ρ
amenable to initialization schemes like K-means++, while the clusters converge in finite
time. As ρ grows, we show that fixed points of DGC-F converge to the set of aptly defined
ρ
consensus fixed points.
• We show that as ρ → ∞, the cluster center estimates attain consensus, thus guaranteeing
that clusters converge to a clustering of the full data, for ρ large enough. In the case of
Bregman losses, we show that these limiting consensus fixed points reduce to the classical
notion of Lloyd fixed points associated with hard clustering. No assumptions are made on
users’ data similarity, with data across users possibly highly heterogeneous.
2In particular, the size of coreset is O(cid:0)1(Kd+log 1)+mKlogmK(cid:1) , where γ ∈ (0,1) and ϵ > 0 are the
ϵ4 γ γ
success probability (i.e., with probability 1−γ) and approximation quality.
3While computation cost is a key feature, all the methods from Table 1 perform similar operations and
incur similar computation costs. For that reason, we chose to omit computation from Table 1.
4The analysis in Kar and Swenson (2019) is tailored to the method proposed therein, oblivious to the fact
that it is a first-order method. As we discuss in Section 3, their method is a special case of ours, applied to
the K-means cost, with varying step-sizes.
4• We verify the performance of our methods on real data, with our Huber loss based method
outperforming the K-means based one on many tasks, demonstrating the benefits of a
general family. Our methods also outperform the two baselines, centralized and local clus-
tering5 showing the benefits of clustering over distributed data, such as less sensitivity to
initialization, as well as the benefits of collaboration over local clustering in isolation.
Paper organization. The rest of the paper is organized as follows. Section 2 introduces
theproblemconsiderinthepaper. Section3outlinestheproposedfamilyofmethods. Section
4 presents the main results. Section 5 provides numerical results. Section 6 concludes the
paper. Appendix contains additional details and proofs. We introduce the notation in the
remainder of this section.
Notation. The spaces of real numbers and d-dimensional vectors are denoted by R and
Rd. The set of non-negative integers is denoted by N. The set of integers up to and including
M is denoted by [M] = {1,...,M}. For a set A, A denotes the closure of A, while |A| denotes
the number of elements of A. We use ⟨·,·⟩ and ∥·∥ to denote the Euclidean inner product and
the induced vector/matrix norm. ∇ f(x,y) denotes the gradient of f with respect to x. We
x
use 1 and I to denote the d-dimensional vector of ones and d×d identity matrix. ⊗ denotes
d d
the Kronecker product, A⊤ denotes transposition and λ (A) denotes the largest eigenvalue
max
of A. O(·) is the “big O”, i.e., a = O(b ) implies there exist C > 0 and n ∈ N, such that
n n 0
a ≤ Cb , for all n ≥ n , for a ,b ≥ 0. Superscripts denote the iteration counter, subscripts
n n 0 n n
denote the user, while the value in the brackets corresponds to the particular center/cluster,
e.g., xt(k) stands for the k-th center of user i, at iteration t.
i
2 Problem formulation
Consider a network of m > 1 users connected over a graph G = (V,E), where V = [m] is
the set of vertices (i.e., users), E is the set of (undirected) edges connecting them, such that
{i,j} ∈ E if and only if users i and j can communicate. Let D = {y ,...,y } be the local
data of user i, with (cid:80) N = N, and w ∈ (0,1) the
weighti associ, i1
ated
wii t, hNi
the r-th point
i∈[m] i i,r
of the i-th user, with (cid:80) (cid:80) w = 1. In this setup, (1) is equivalent to
i∈[m] r∈[Ni] i,r
(cid:88) (cid:88) (cid:88)
min w f(x (k),y ), (2)
i,r i i,r
sux bi∈ jeR ctK td o,C x1i∈ =C ..K .=,D xi mi∈[m]k∈[K]r∈Ci(k)
where C is the set of K-partitions of D . Formulation (2) ensures that a clustering of the
K,Di i
jointdatasetisproduced, bysynchronizingthecenterestimatesacrossusersviatheconstraint
x = ... = x , i.e., enforcing that the centers are the same across all users. For (2) to be well
1 m
defined and solvable in distributed fashion, we assume the following.
Assumption 1. The dataset D has at least K distinct samples.
Remark 1. Assumption 1 requires the full data to have at least K distinct points, which is
natural, as we aim to find K clusters. No assumptions are made on users’ local data.
Assumption 2. The graph G = (V,E) is connected.
5Centralizedandlocalclusteringrefertomethodsworkingonfulldataandonlyonlocaldatainisolation,
see Section 5.
5Remark 2. Assumption 2 ensures that the problem (2) can be solved in a distributed fashion,
by peer-to-peer communication only. It is a standard in distributed literature, see, e.g., Vlaski
et al. (2023) and references therein.
Note that (2) is a constrained problem, requiring either global synchronization of users’
center estimates, or an involved primal-dual scheme, e.g., Forero et al. (2011), to be solved.
To make it amenable to a simple first-order approach and local communication, consider the
relaxation
(cid:88) (cid:88) (cid:104)1 (cid:88)
min J (x,C) = ∥x (k)−x (k)∥2
ρ i j
x∈RKmd, 2
C∈Cm,K,D
i∈[m]k∈[K] j∈Ni
1 (cid:88) (cid:105)
+ w f(x (k),y ) , (3)
i,r i i,r
ρ
r∈Ci(k)
where C is the set of all clusterings of the data, i.e., for C ∈ C , we have C =
m,K,D m,K,D
(C ,...,C ), with C ∈ C , N = {j ∈ V : {i,j} ∈ E} is the set of neighbours of user i
1 m i K,Di i
(not including i), while ρ ≥ 1 is a tunable parameter. The formulation (3) relaxes (2), by
considering an unconstrained problem that penalizes the difference of centers among neigh-
bouring users and controls the trade-off between center estimation and center proximity via
ρ. A similar relaxation was considered in Kar and Swenson (2019), for the special case of
f(x,y) = ∥x−y∥2. The formulation (3) is very general and includes a myriad of clustering
loss functions, with some examples below.
Example 1. Distributed Bregman clustering: if the loss in (3) is a Bregman distance Breg-
man (1967), f(x,y) = ψ(y)−ψ(x)−⟨∇ψ(x),y −x⟩, with ψ : Rd (cid:55)→ R strictly convex and
differentiable, we get a novel problem formulation of distributed Bregman clustering. For the
special case f(x,y) = ∥x−y∥2, we recover the distributed K-means formulation from Kar
and Swenson (2019).
Example 2. Distributed Huber loss clustering: if the loss in (3) is the Huber loss Huber
(1964), f(x,y) = ϕ (∥x−y∥), where ϕ : R (cid:55)→ [0,∞), for some δ > 0, is given by
δ δ
(cid:40)
x2
, |x| ≤ δ
ϕ (x) = 2 , (4)
δ
δ|x|−
δ2
, |x| > δ
2
we get a novel formulation of distributed Huber clustering.
Example 3. Distributed logistic clustering: if the loss in (3) is the logistic loss, i.e., f(x,y) =
log(1+exp(∥x−y∥2)), we get a novel problem of distributed logistic clustering.
Example4. Distributedfairclustering: ifthelossin(3)isthe“fair” lossf(x,y) = h (∥x−y∥),
γ
where h γ(x) = 2γ2(x2/γ −log(1+x2/γ), for γ > 0, e.g., Rabbat et al. (2005), we get a novel
problem of distributed fair clustering.
Note that all formulations are novel, from the perspective of distributed clustering. While
Bregman and Huber loss clustering are popular clustering methods in the centralized setting,
e.g., Banerjee et al. (2005); Pediredla and Seelamantula (2011); Armacki et al. (2022a), to
6the best of our knowledge, losses like logistic and fair loss have not been considered previously,
even in the centralized literature.
Next, using the graph Laplacian matrix L = D−A, where D,A ∈ Rm×m are the degree
and adjacency matrices, see, e.g., Chung (1997); Cvetkovic et al. (1997), and letting x =
(cid:2) x⊤ ... x⊤(cid:3)⊤ ∈ RKmd be the vector stacking users’ center estimates x ∈ RKd, we can
1 m i
represent (3) as
min J ρ(x,C) = 1/ρJ(x,C)+1/2⟨x,Lx⟩, (5)
x∈RKmd,C∈Cm,K,D
where J(x,C) = (cid:80) H(x ,C ), with L = L ⊗ I . In order to solve (5), we make the
i∈[m] i i Kd
following assumptions.
Assumption3. Thelossf iscoercive, convexandβ-smoothwithrespecttothefirstargument,
i.e., for all x,y,z ∈ Rd, we have lim f(x,y) = ∞, and
∥x∥→∞
β
f(z,y)+⟨∇f(z,y),x−z⟩ ≤ f(x,y) ≤ f(z,y)+⟨∇f(z,y),x−z⟩+ ∥x−z∥2.
2
Remark 3. Assumption 3 ensures the loss function is well-behaved. Coercivity ensures center
estimates stay close to the dataset D, by not allowing them to grow arbitrarily large. Convex-
ity and smoothness are standard assumptions for gradient-based methods, see, e.g., Nesterov
(2018).
3 Proposed family of methods
In this section we describe the DGC-F family of methods proposed to solve the general
ρ
distributed clustering problem (5). During training users maintain their current center and
cluster estimates. The algorithm starts with users choosing their initial center estimates
x0 ∈ RKd, i ∈ [m]. At iteration t ≥ 0, each user i ∈ [m] first forms the clusters, by finding a
i
k ∈ [K] for each r ∈ D , such that
i
g(xt(k),y ) ≤ g(xt(l),y ), for all l ̸= k, (6)
i i,r i i,r
and assigns y to Ct+1(k). Here, g : Rd ×Rd (cid:55)→ [0,∞) is a distance function, related to the
i,r i
loss f (see Assumptions 4, 5 ahead). Next, the centers are updated, by performing B ≥ 1
updates, i.e., for b = 0,...,B−1
(cid:18) (cid:19)
xt,b+1(k) = xt,b(k)−α (cid:88) (cid:104) xt,b(k)−xt,b(k)(cid:105) + 1 (cid:88) w ∇ f(cid:16) xt,b(k),y (cid:17) , (7)
i i i j ρ i,r x i i,r
j∈Ni r∈Ct+1(k)
i
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
consensus
innovation
where xt,0(k) = xt(k) and α > 0 is a sufficiently small fixed step-size. Finally, the new center
i i
is xt+1(k) = xt,B(k), and the steps are repeated. The procedure is summarized in Algorithm
i i
1. The center initialization performed at the outset of training can be done in an arbitrary
manner, i.e., each user is allowed to initialize their own centers and no synchronization is
required. This allows for significant flexibility and implementing initialization algorithms,
like distributed K-means++, e.g., Yu et al. (2021). Steps 2-5 in Algorithm 1 outline the
cluster update steps, while Steps 7-10 outline the center update steps, using the consensus
+ innovation framework Kar et al. (2012); Kar and Moura (2013). Note that DGC-F uses
ρ
different functions for cluster assignment and center updates. We assume the following on the
relationship between functions g and f.
7Algorithm 1 DGC-F
ρ
Require: Step-size α > 0, penalty parameter ρ ≥ 1, number of rounds T ≥ 1, number of
local iterations B ≥ 1, initial centers x0 = (cid:2) x0(1)⊤ ... x0(K)⊤(cid:3)⊤ ∈ RKd, i ∈ [m].
i i i
1: for each user i ∈ [m] in parallel, in round t = 0,1,...,T-1 do:
2: Set Ct+1(k) ← ∅, for all k ∈ [K];
i
3: for each r ∈ [N i] do:
4: Find k ∈ [K] such that, for all l ∈ [K]: g(xt i(k),y i,r) ≤ g(xt i(l),y i,r);
5: Update the cluster: Ct+1(k) ← Ct+1(k)∪{r};
i i
6: Set xt,0(k) ← xt(k);
i i
7: for all clusters k ∈ [K] in parallel and local updates b = 0,...,B−1 do:
8: Exchange the current center estimates xt i,b(k) and xt j,b(k) with neighbours j ∈ N i;
(cid:16) (cid:17)
9: xt i,b+1(k) ← xt i,b(k)−α ρ1 (cid:80)
r∈C
it+1(k)∇ xf(xt i,b(k),y i,r)+(cid:80) j∈Ni[xt i,b(k)−xt j,b(k)] ;
10: Set xt+1(k) ← xt,B(k), for all k ∈ [K];
i i
11: Return (xT,CT), i ∈ [m].
i i
Assumption 4. The distance function g is a metric, i.e., for all x,y,z ∈ Rd:
1. g(x,y) ≥ 0 and g(x,y) = 0 if and only if x = y;
2. g(x,y) = g(y,x);
3. g(x,y) ≤ g(x,z)+g(z,y).
Assumption 5. The loss f preserves the ordering with respect to g, i.e., for all x,y,z ∈ Rd
f(x,y) < f(z,y) if g(x,y) < g(z,y) and f(x,y) = f(z,y) if g(x,y) = g(z,y).
Remark 4. Assumption 4 requires g to be a well-behaved distance function, while Assumption
5 ensures that the cluster update step (6) does not increase the cost J .
ρ
Example 5. For g(x,y) = ∥x−y∥ being the Euclidean distance, losses f 1(x,y) = 1/2g(x,y)2,
f (x,y) = ϕ (g(x,y)), f (x,y) = log(1+exp(g(x,y)2), f (x,y) = h (g(x,y)) satisfy Assump-
2 δ 3 4 γ
tions 4, 5 and recover Examples 1-4. For g(x,y) = (cid:112) ⟨x−y,A(x−y)⟩ being a Mahalanobis
distance, with A positive definite, the losses f -f again satisfy Assumptions 4, 5 and give rise
1 4
to novel Mahalanobis distance distributed clustering methods.
By specializing f and g, we get instances of DGC-F . For example, for g(x,y) = ∥x−y∥,
ρ
f(x,y) = 1∥x−y∥2, we get the DGC-KM algorithm, with center update given by
2 ρ
(cid:18) (cid:19)
xt i,b+1(k) = 1−α(cid:104) 1/ρ (cid:88) w
i,r
+|N i|(cid:105) xt i,b(k)+ α
ρ
(cid:88) w i,ry
i,r
+α (cid:88) xt j,b(k).
r∈Ct+1(k) r∈Ct+1(k) j∈Ni
i i
(8)
For g(x,y) = ∥x−y∥, with f(x,y) = ϕ (∥x−y∥), we get the DGC-HL algorithm, with the
δ ρ
8center update equation
(cid:18) (cid:19)
xt i,b+1(k) = 1−α(cid:104) 1/ρ (cid:88) w
i,r
+1/ρ (cid:88) ∥xt,b(δ kw )i −,r
y ∥
+|N i|(cid:105) xt i,b(k)
r∈Ct+1(k) r∈Ct+1(k) i i,r
i,n i,f (9)
(cid:18) (cid:19)
+ α (cid:88) w y + (cid:88) δw i,ry i,r +α (cid:88) xt,b(k),
ρ i,r i,r ∥xt,b(k)−y ∥ j
r∈Ct+1(k) r∈Ct+1(k) i i,r j∈Ni
i,n i,f
where Ct+1(k) = {r ∈ Ct+1(k) : ∥xt,b(k)−y ∥ ≤ δ} the set of points near the current center,
i,n i i i,r
with Ct+1(k) = {r ∈ Ct+1(k) : ∥xt,b(k)−y ∥ > δ} the set of points far from the current
i,f i i i,r
center. For g(x,y) = ∥x−y∥, with f(x,y) = log(1+exp(∥x−y∥2)), we get the DGC-LL
ρ
algorithm, with the center update equation
(cid:18) (cid:19)
xt i,b+1(k) = 1−α(cid:104) 1/ρ (cid:88) 1+exp(−∥2 xw t,i b, (r
k)−y ∥2)
+|N i|(cid:105) xt i,b(k)
r∈Ct+1(k) i i,r
i (10)
+ α (cid:88) 2w i,r y +α (cid:88) xt,b(k).
ρ 1+exp(−∥xt,b(k)−y ∥2) i,r j
r∈Ct+1(k) i i,r j∈Ni
i
DGC-KM , DGC-HL and DGC-LL represents distributed algorithms for K-means, Huber
ρ ρ ρ
and logistic clustering, respectively. If w = 1 for all i ∈ [m], r ∈ [N ], we can see from (8)
i,r N i
that DGC-KM assigns uniform weight to each sample, while DGC-HL and DGC-LL assign
ρ ρ ρ
non-uniform weights. As such, DGC-KM is well suited to applications where each sample
ρ
is important and caries equal weight. On the other hand, we can see from (9) that DGC-
HL separates the data from cluster Ct+1(k) into two groups: nearby points in Ct+1(k), and
ρ i i,n
faraway points in Ct+1(k). Nearby points are again assigned uniformed weight, with faraway
i,f
points assigned the weight δ/∥xt i,b(k)−yi,r∥ ∈ (0,1), that decays as y
i,r
gets farther away from
the center. As such, DGC-HL is well-suited to applications where robustness to outliers is
ρ
desired, as it assigns a decreasing weight to points farther away from the center, i.e., the
perceived outliers. Next, we can see from (10) that DGC-LL assigns non-uniform weights to
ρ
all points, given by 2/(1+exp(−∥xt i,b(k)−yi,r∥2)) ∈ [1,2), which increases as y
i,r
gets farther away
from the center. As such, DGC-LL can be seen as a fairness promoting algorithm, that aims
ρ
to exploit information from the whole system by giving higher weight to faraway points, and
is well-suited to applications where faraway points, i.e., outliers, carry rare and important
information.
Remark 5. Communication takes place in step 8 of Algorithm 1 and is performed B ≥ 1
times per iteration, taking the total number of communication rounds to BT. Compared to
Forero et al. (2011); Kar and Swenson (2019), where the number of communication rounds is
T, our algorithm communicates more by a factor of B, if B > 1.
Remark 6. Each user is required to store its center estimates, local data and labels, taking
the storage cost to (K+|D |)d+|D |, with Kar and Swenson (2019) incurring the same cost.
i i
Forero et al. (2011) additionally require storing neighbours’ center estimates, increasing the
storage cost by |N |Kd, which can be a significant, e.g., in dense networks (i.e., large |N |) or
i i
high dimensions (i.e., large d).
9Remark 7. The main computation cost is incurred in cluster and center update steps 2-5 and
7-10. Cluster update requires finding the closest center for each sample and the cost of it is
the same as in any clustering algorithm. The cost in center update comes from computing the
gradient of f in (7). Compared to Forero et al. (2011); Kar and Swenson (2019), our cost is
again higher by a factor of B, if B > 1.
Remark 8. Increasing B results in the center update more closely approximating the argmin
step, i.e., xt+1 ≈ argmin J (x,Ct+1). As we show in Appendix B (see proofs of Lem-
x∈RKmd ρ
mas 2, 7), this results in faster decrease in the cost J and faster convergence of centers.
ρ
However, increasing B incurs higher computation and communication cost. This trade-off is
explored numerically in Section 5.
There are three major differences between DGC-F and centralized gradient clustering
ρ
from Armacki et al. (2022a). First, the center update (7) incorporates the consensus part, as
otherwise, users would be performing the centralized gradient clustering on their local data,
without collaboration. Second, users can perform multiple center updates per iteration, while
the centralized method performs only one. This is important, as more updates allow DGC-F
ρ
to mimic the argmin step, akin to, e.g., Lloyd’s algorithm. Finally, centers are updated even
when the corresponding clusters are empty, which is not the case for the centralized method.
These differences result in distinct algorithm dynamics, additional challenges in convergence
analysis in Section 4.2, as well as a novel consensus analysis in Section 4.4.
The method in Kar and Swenson (2019) is a special case of DGC-F , designed for the
ρ
squaredEuclideanlossandauser,clusterandtimevaryingstep-sizeαt(k) = α/(|Ct+1(k)|/ρ+
i i
|N |), with α < min |N |/(max N /ρ + λ (L)). DGC-F is much more general,
i i∈[m] i i∈[m] i max ρ
encompassing a variety of clustering algorithms outlined in this section and allowing multiple
center updates per round. Additionally, DGC-F is easier to implement, as it uses the same
ρ
fixed step-size for all users6.
4 Main results
Inthissectionwepresentthemainresults. Section4.1definesfixed points,akeyconceptinour
analysis. Section 4.2 presents convergence guarantees of DGC-F , establishing convergence of
ρ
centerstoafixedpointandconvergenceofclustersin finite time. Section4.3presentsaclosed-
form expressions for fixed points, when the loss f is a Bregman distance. Section 4.4 studies
the behaviour of fixed points as ρ → ∞, establishing consensus of centers and convergence of
clusters to a clustering of the full data for finite ρ. We drop the subscript in gradients, e.g.,
∇J(x,C) ≡ ∇ J(x,C). The proofs from this section can be found in Appendix B.
x
4.1 Setting up the analysis
In this section we define key concepts used in the analysis.
Definition 1. Let x ∈ RKmd be cluster centers. We say that U ⊂ C is the set of optimal
x m,K,D
clusterings with respect to x, if for all clusterings C ∈ U , (6) is satisfied.
x
6Our step-size requires knowledge of the penalty ρ, smoothness β and λ (L), see Section 4. If the cost
max
is K-means, with weights w = 1, it can be shown that β = max N , which is similar to α used in the
i,r i∈[m] i
step-sizeinKarandSwenson(2019). Inthatsense,thestep-sizeinouralgorithmrequiressharedknowledgeof
thesameparametersasinKarandSwenson(2019)andcanbeachievedbyanygossipalgorithm,e.g.,Dimakis
et al. (2010), at the expense of a few extra communication rounds.
10Definition 2. The pair (x⋆,C⋆) is a fixed point of DGC-F , if:
ρ
1. C⋆ ∈ U ;
x
2. ∇J (x⋆,C⋆) = 0.
ρ
Remark 9. Definition 2 requires (x⋆,C⋆) to be a stationary point of J , i.e., the clusters C⋆
ρ
to be optimal with respect to x⋆, and x⋆ to be optimal with respect to J (x,C⋆). As such, it is
ρ
not possible to further improve the clustering, nor the centers at a fixed point. Recalling that
J is non-convex, reaching a fixed point is the best we can do.
ρ
Definition 3. The set U ⊂ C is the set of clusterings, such that:
x m,K,D
1. U ⊆ U ;
x x
2. ∇J (x,C) = 0, for all C ∈ U .
ρ x
By Definitions 2 and 3, x can be a fixed point if and only if U ̸= ∅. As such, we will call
x
x a fixed point if U ̸= ∅.
x
4.2 Convergence to fixed points
In this section we show that centers produced by DGC-F are guaranteed to converge to a
ρ
fixed point. Recalling Remark 9, this is, in general, the best one can achieve.
Theorem 1. Let Assumptions 1-5 hold. For the step-size α < 1/(β/ρ+λ max(L)), any initial-
ization x0 ∈ RKmd and ρ ≥ 1, the sequence of centers {xt} t∈N generated by DGC-F ρ converges
to a fixed point, i.e., a x⋆ such that U ̸= ∅. Moreover, the clusters converge in finite time,
x⋆
i.e., there exists a t > 0 such that U ⊂ U , for all t ≥ t .
0 xt x⋆ 0
Theorem 1 states that the sequence of centers generated by DGC-F is guaranteed to
ρ
converge to a fixed point, for any center initialization. This is a strong result, which pro-
vides great flexibility, in the sense that no synchronization of users’ initial centers is needed
beforehand. While the convergence of centers is asymptotic in nature, Theorem 1 guarantees
that clusters converge in finite time. This result is of great importance, as in practice, we are
often interested only in the clustering of the data. In that sense, this result guarantees that a
solution will be provided in finite time.
In order to prove Theorem 1, a series of lemmas are introduced, similarly to Armacki
et al. (2022a). However, due to the problem being distributed and the algorithmic differences
discussed in Section 3, the majority of proofs from Armacki et al. (2022a) are not directly
applicable. The next result characterizes the behaviour of the distributed cost J .
ρ
Lemma 1. For each fixed clustering C ∈ C , the function J is convex and β -smooth,
m,K,D ρ L,ρ
with β
L,ρ
= β/ρ+λ max(L).
Prior to stating the next result, note that the center update (7) can be represented com-
pactly as
(cid:18) (cid:19)
1
xt,b+1 = xt,b−α∇J (xt,b,Ct+1) = xt,b−α ∇J(xt,b,Ct+1)+Lxt,b , (11)
ρ
ρ
11where ∇J(xt,b,Ct+1) ∈ RKmd is the vector stacking of the gradients of H with respect to x ,
i
whose i-th block, for any i ∈ [m], is given by
(cid:104) (cid:105)
∇J(xt,b,Ct+1) = ∇H(xt,b,Ct+1) ∈ RKd. (12)
i i
i
We next prove that DGC-F generates a non-increasing sequence of values of J .
ρ ρ
Lemma 2. For the sequence {(xt,Ct)} t∈N, generated by Algorithm 1, with α < βL1 ,ρ, the
resulting sequence of costs {J ρ(xt,Ct)} t∈N is non-increasing.
The next result states that if two cluster centers are sufficiently close, their set of optimal
clusteringsmatch. TheproofcanbefoundinKarandSwenson(2019);Armackietal.(2022a).
Lemma 3. There exists an ϵ > 0, such that, for any x,x′ ∈ RKmd satisfying
∗
max g(x (k),x′(k)) < ϵ , we have U ⊂ U .
i∈[m],k∈[K] i i ∗ x′ x
The next result shows that any convergent subsequence of {xt}
t∈N
converges to a fixed
point.
Lemma 4. Let {xt} t∈N be a sequence generated by DGC-F ρ, with α < βL1 ,ρ. Then, any
convergent subsequence converges to a fixed point.
The next lemma shows that the clusters converge in finite time. The proof follows the
same reasoning as the one in Armacki et al. (2022a), and is omitted for brevity.
Lemma 5. For any convergent subsequence {xts} s∈N of {xt} t∈N, generated by DGC-F ρ, there
exists a s
0
∈ N, such that, for all s ≥ s 0, Cts+1 ∈ U x⋆, where x⋆ = lim s→∞xts.
The following lemma shows that the generated sequence of cluster centers stays bounded.
Lemma 6. The sequence of centers {xt} t∈N generated by DGC-F ρ stays bounded, i.e., there
exists a M > 0, such that ∥xt∥ ≤ M , for all t ∈ N.
ρ ρ
The next lemma shows that, if a point in the sequence of centers is sufficiently close to a
fixed point, then all the subsequent points remain in the neighborhood of the fixed point.
Lemma 7. Let {xt} t∈N be the sequence of centers generated by DGC-F ρ, with the step-size
satisfying α < 1 . Let x⋆ ∈ RKmd be a fixed point, in the sense of Definition 2. Then, there
βL,ρ
exists an ϵ⋆ > 0, for which, for all ϵ ∈ (0,ϵ⋆), there exists a t
ϵ
∈ N, such that, if ∥xt0−x⋆∥ ≤ ϵ,
for some t > t , then ∥xt−x⋆∥ ≤ ϵ, for all t ≥ t .
0 ϵ 0
We are now ready to prove our main result.
Proof of Theorem 1. By Lemma 2 and the fact that the corresponding sequence of costs
{J ρ(xt,Ct)} t∈N is nonnegative, the monotone convergence theorem states that this sequence
converges to some J⋆ ≥ 0. On the other hand, by Bolzano-Weierstrass theorem and Lemma
ρ
6, the sequence {xt} t∈N has a convergent subsequence, {xts} s∈N, with some x⋆ ∈ RKmd
as its limit. From the continuity of J
ρ
and convergence of xts, we can then conclude that
J ρ⋆ = lim s→∞J ρ(xts,Cts) = J(x⋆,C⋆). Lemma 4 then implies that x⋆ is a fixed point. Fi-
nally, Lemmas 5 and 7 imply the convergence of the full sequence, i.e., lim xt = x⋆.
t→∞
Convergence of clusters in finite time is implied by Lemmas 3,5.
124.3 Fixed point analysis - Bregman clustering
In this section we provide closed-form expressions for fixed points of DGC-F , when the loss
ρ
is Bregman, i.e., f(x,y) = ψ(y)−ψ(x)−⟨∇ψ(x),y −x⟩, with ψ strictly convex. To apply
DGC-F , we require a distance g satisfying Assumptions 4, 5. As noted in Armacki et al.
ρ
(2022a), it is possible to find such g for many Bregman distances, via Chen et al. (2008);
Acharyya et al. (2013), which state that a class of Bregman distances are squares of distance
metris. Some examples of such Bregman distances are given in Appendix B. We make the
following assumption on the Bregman inducing function ψ.
Assumption 6. ψ is strictly convex, twice differentiable and induces a Bregman distance that
is a square of a metric.
Lemma 8. Let f(x,y) be a Bregman distance, with ψ satisfying Assumption 6. Then, any
fixed point (x⋆,C⋆) of DGC-F satisfies, for all i ∈ [m], k ∈ [K]
ρ
(cid:16) (cid:88) (cid:88) (cid:17)
x⋆ i(k) = P i− ,k1 1/ρ∇2ψ(x⋆ i(k)) w i,ry
i,r
+ x⋆ j(k) ,
r∈C i⋆(k) j∈Ni
where P = 1∇2ψ(x⋆(k))(cid:80) w +|N |I .
i,k ρ i r∈C⋆(k) i,r i d×d
i
Remark 10. If f(x,y) = 1∥x−y∥2 (i.e., K-means clustering), it can be shown that ψ(x) =
2
1∥x∥2, e.g., Banerjee et al. (2005). In that case ∇2ψ(x) ≡ I , hence fixed points of DGC-
2 d×d
KM satisfy, for any i ∈ [m], k ∈ [K]
x⋆(k) =
1/ρ(cid:80)
r∈C
i⋆(k)w i,ry
i,r
+(cid:80) j∈Nix⋆ j(k)
.
i 1/ρ(cid:80) r∈C⋆(k)w
i,r
+|N i|
i
This is consistent with the fixed points of the method from Kar and Swenson (2019), termed
generalized Lloyd minima.
4.4 Consensus and fixed points
In this section we study the behaviour of the sequences of fixed points generated by DGC-F .
ρ
NotethatfixedpointsofDGC-F dependonρ, i.e., x ≡ x , foreachfixedpointxofDGC-F .
ρ ρ ρ
As such, we are interested in the behaviour of the family of fixed points {x } of DGC-F ,
ρ ρ≥1 ρ
as ρ → ∞. To start, we state the assumption used in this section.
Assumption 7. The distance function g is the Euclidean distance and the gradient of f is of
the form ∇f(x,y) = γ(x−y), where γ ≡ γ(x,y) > 0 may depend on x and y.
We show in Appendix B that Assumption 7 is satisfied for K-means, Huber and logistic
losses. The next result shows fixed points of DGC-F remain in co(D,x0)7, for each ρ.
ρ
Lemma 9. Let Assumption 7 hold. Then for any ρ ≥ 1, fixed points x of DGC-F satisfy
ρ ρ
x (k) ∈ co(D,x0), for all i,k, where x0 ∈ RKmd is the center initialization.
i,ρ
Next, we define an important concepts used in the analysis.
7Weusetheshorthandnotationco(D,x0)todenotetheconvexhullofthedataDandinitialcentersx0(k),
i
i∈[m], k∈[K].
13Definition 4. The point x ∈ RKmd is a consensus fixed point, if:
1. x = ... = x ;
1 m
2. there exists a C ∈ U , such that 1⊤∇J(x,C) = 0, where 1 = 1 ⊗I .
x m Kd
Remark 11. Consensus fixed points are not fixed points per Definition 2, as they do not have
to satisfy ∇J(x,C) = 0.
We can write any consensus fixed point as x = 1 ⊗ x, where x ∈ RKd is the vector
m
stacking of K centers. From point 2) in Definition 4 and the definition of ∇J, we get
(cid:88)
1⊤∇J(x,C) = ∇H(x,C ) = 0. (13)
i
i∈[m]
If we define C = (C(1),...,C(K)), where C(k) = ∪ C (k), it follows from (13) that, for
i∈[m] i
all k ∈ [K]
(cid:88) (cid:88) (cid:88)
w ∇f(x(k),y ) = w ∇f(x(k),y )
i,r i,r i,r i,r
r∈C(k) i∈[m]r∈Ci(k)
= [1⊤∇J(x,C)] = 0, (14)
k
i.e., (x,C)isastationarypointofthecentralizedproblem(1). Theconverseholdsaswell, i.e.,
any stationary point (x,C) of (1) induces a consensus fixed point (x,C), where x = 1 ⊗x
m
and C = (C ,...,C ), with C (k) = C(k)∩D . As such, our aim is to find consensus fixed
1 m i i
points, ensuring that a clustering of the full data is produced.
Remark 12. Specializing (14) to K-means cost, any consensus fixed point (x,C), with x =
1 ⊗x satisfies
m
1 (cid:88) (cid:88) 1 (cid:88)
x(k) = w y = w y ,
i,r i,r i,r i,r
W W
k k
i∈[m]r∈Ci(k) r∈C(k)
where W = (cid:80) (cid:80) w and C(k) = ∪ C (k), i.e., consensus fixed points corre-
spond
tok meansi∈ o[ fm g] lobar∈ lC cil( uk s) teri s,r
, also known
asi∈ L[ lm o] ydi
points.
The next result shows that centers of consensus fixed points, whose joint clusters are
non-empty, belong to co(D).
Lemma 10. Let (x,C) be a consensus fixed point, x = 1⊗x. Then x(k) ∈ co(D), for all k
that ∪ C (k) ̸= ∅.
i∈[m] i
Centers whose corresponding clusters are empty do not contribute to the cost or the
gradient. As such, they can be freely assigned and we can always choose a center that belongs
to co(D). Define
X = (cid:8) x ∈ RKmd : x = 1 ⊗x is a consensus fixed point and x(k) ∈ co(D), for all k ∈ [K](cid:9) .
m
It then follows from Lemma 10 that X ̸= ∅. The next result shows that consensus is achieved
(cid:16) (cid:17)
at a rate O 1 .
ρ
Lemma 11. Let Assumption 7 be satisfied, {x } be a family of fixed points generated by
ρ ρ≥1
DGC-F
ρ
and denote by R = max x∈co(D,x0)∥x∥. Then, we have ∥Lx ρ∥ ≤ 2mβR/ρ.
14We are now ready to state the main result of this section.
Theorem2. LetAssumption7holdand{x } beafamilyoffixedpointsgeneratedbyDGC-
ρ ρ≥1
F . Then, the sequence converges to the set X, i.e., lim d(x ,X) = 0, where d(x,X) =
ρ ρ→∞ ρ
inf ∥x−y∥. Moreover, the clusters converge for finite ρ, i.e., there exists a ρ > 0 such
y∈X 0
that U ⊂ U , for all ρ ≥ ρ , and some x ∈ X.
xρ x 0
Theorem 2 states that center consensus is achieved asymptotically, while Lemma 11 shows
that consensus is achieved at a rate ∥Lx ρ∥ = O(1/ρ). We verify it numerically in Section 5.
Moreover, clusters converge for a finite ρ, which implies that we are guaranteed to produce a
clustering of the full data for a finite value of ρ. While higher values of ρ guarantee consensus,
increasing ρ results in slower convergence of DGC-F , as it takes more time to optimize the
ρ
clustering cost J, which we verify in Appendix C. Similar observations were made in Kar
and Swenson (2019). As such, ρ offers an inherent trade-off between convergence speed and
producingaclusteringofthefulldata. Inapplicationswhereconvergencespeedisparamount,
moderate values of ρ are apt, otherwise Theorem 2 and Lemma 11 suggest choosing large ρ
to guarantee a clustering of the joint data.
5 Numerical results
Inthissectionwepresentnumericalresults. Experimentsareimplementedinpython,averaged
across 10 runs. We assume uniform weights, i.e., w = 1. For Huber loss we set δ = 5.
i,r N
We use Iris Fisher (1936), MNIST Lecun et al. (1998) and CIFAR10 Krizhevsky (2009) data.
In particular, we use the full Iris data, the first seven digits of MNIST and three and eight
classes of CIFAR, dubbed CIFAR3 and CIFAR8, for a total of K = {3,7,3,8} classes for
Iris, MNIST, CIFAR3 and CIFAR8. In all experiments we use a network of m = 10 users,
communicating over a ring graph. Each user is assigned N = {15,700,300,800} samples for
i
Iris, MNIST, CIFAR3 and CIFAR8, split equally among each class. For a full description of
the data and additional results, see Appendix C.
Figure2: BehaviourofJ fordifferentρandB =1. ThefigurespresentDGC-KM ,DGC-HL andDGC-LL ,
ρ ρ ρ ρ
left to right.
The first set of experiments, using Iris data, aim to verify our theory. We evaluate the
cost J for different values of ρ and B, and consensus for different values of ρ. Consensus is
ρ
measuredviamaximumcenterdistanceamongusers, i.e., max ∥x −x ∥. Usersinitialize
i,j∈[m] i j
their centers by choosing a random sample from each class of their local data. The results are
presented in Figures 2, 3 and Table 2.
Figures 2 and 3 present the cost J per iteration. Figure 2 shows the cost for different
ρ
values of ρ and B = 1, while Figure 3 shows the cost for different values of B and ρ = 10. The
15Table 2: Effect of ρ on maximum center distance, with B =1.
ρ=1 ρ=10 ρ=102 ρ=103
DGC-KM 1.16 3.3×10−1 4.7×10−2 5×10−3
ρ
DGC-HL 1.17 3.1×10−1 4.8×10−2 5×10−3
ρ
DGC-LL 1.23 4.3×10−1 6.1×10−2 8×10−3
ρ
Figure 3: Behaviour of J for different B and ρ=10. The figures present DGC-KM , DGC-HL and DGC-
ρ 10 10
LL , left to right.
10
Table 3: Clustering accuracy. We use B = 1, with ρ = 10 on MNIST and ρ = 100 on both CIFAR3 and
CIFAR8 data.
Sklearn KM++ CGC-KM LGC-KM DGC-KM DGC-HL DGC-LL
ρ ρ ρ
MNIST 73.68±0.16% 73.57±0.53% 62.98±3.11% 73.32±1.08% 74.36±1.30% 70.98±1.64%
CIFAR3 50.55±0.43% 50.06±0.41% 41.41±4.44% 51.21±1.99% 49.14±4.23% 46.02±3.61%
CIFAR8 21.19±0.09% 21.42±0.32% 16.71±0.46% 20.92±0.41% 21.54±0.88% 20.62±0.45%
16solid line represents the average performance, with the shaded region showing the standard
deviation. As predicted in Lemma 2, the cost is decreasing in each iteration (Figure 2) and
largerB leadstofasterconvergence(Figure3). Table2presentsthemaximumcenterdistance
across users after T = 500 iterations, with B = 1. As predicted in Lemma 11, the maximum
center distance for all three algorithms is of the order O(1/ρ).
Table 4: Clustering accuracy on Iris data.
K-means Huber Logistic
CGC 88.8±0.3% 84.7±13.3% 88.9±0.3%
LGC 89.6±3.3% 89.7±2.1% 89.0±1.8%
DGC 91.1±0.8% 91.2±0.8% 91.0±0.3%
Next, we study the clustering quality of our methods on Iris data. We set ρ = 10 and
B = 1 for all methods. We measure the clustering quality by comparing the labels produced
by clustering with the true labels, i.e., accuracy. We benchmark the performance of DGC
with: 1) CGC - centralized gradient clustering from Armacki et al. (2022a) that has access
to the full dataset; 2) LGC - gradient clustering from Armacki et al. (2022a) performed on
users’ local data in isolation. For DGC and LGC, we report the average accuracy across all
users. We use the same loss function for all methods, i.e., if we evaluate the performance
of DGC-KM, we benchmark it with CGC and LGC using the K-means cost. All methods
run for T = 500 iterations. DGC and LGC are given the same initialization, as was done
in the previous experiments. CGC initializes by choosing a point from each class uniformly
at random, from the full data. The results are presented in Table 4. We can see that DGC
outperforms both CGC and LGC. Compared to LGC, our approach shows the benefits of
collaboration and access to more data. Next, both DGC and LGC outperform CGC, which
can be explained by the fact that for Iris, local datasets are representative of the full data,
and, moreover it is easier to find good initialization on smaller datasets. This is in line with
observation from Forero et al. (2011), who noted that distributed algorithms are less sensitive
to initialization.
Finally, to test the performance of DGC on larger datasets, we perform experiments on
MNIST and CIFAR data. We again measure the performance using accuracy, and evaluate
DGC-KM , DGC-HL and DGC-LL , with B = 1 and ρ = 10 for MNIST, while ρ = 100
ρ ρ ρ
for CIFAR. To benchmark the performance, we use CGC and LGC with K-means cost, as
well as python’s centralized scikit-learn Pedregosa et al. (2011) implementation of K-means.
DGC, LGC and CGC are again initialized by randomly assigning a sample from each class,
while scikit-learn implementation uses the more powerful K-means++ initialization. We run
DGC, LGC and CGC for T = 4000 iterations, while scikit-learn K-means is used with de-
fault settings. The results are presented in Table 3. DGC-HL performs the best on MNIST
and CIFAR8 data, showing the benefit of distributed clustering beyond K-means. DGC-KM
performs the best on CIFAR3, outperforming both centralized K-means methods, even the
scikit-learn version with the strong K-means++ initialization, further demonstrating the su-
perior robustness to initialization of distributed methods. Finally, the purely local method
LGC consistently performs the poorest, highlighting the necessity for collaboration.
176 Conclusion
We study clustering over distributed data, where users have access to their local dataset, with
the goal of obtaining a clustering of the full data. We design a family of clustering algorithms
applicable to a wide range of clustering problems. Theoretical studies show that the sequence
of centers generated by DGC-F converges to fixed points and that, as ρ increases, we are
ρ
guaranteed to produce a clustering of the full data. We confirm our results numerically and
demonstrate strong performance across a set of scenarios. Future ideas include using group
lassoregularizer, knowntoachieveconsensusforfiniteρHallacetal.(2015);Sunetal.(2021);
Armacki et al. (2022b), studying statistical consistency and cluster recovery Pollard (1981);
Kumar and Kannan (2010); Awasthi and Sheffet (2012); Dennis et al. (2021); Ghosh et al.
(2020); Armacki et al. (2024).
Bibliography
Acharyya,S.,Banerjee,A.,andBoley,D.(2013). Bregmandivergencesandtriangleinequality.
In Proceedings of the 2013 SIAM International Conference on Data Mining, pages 476–484.
SIAM. (Cited on page 13.)
Alguliyev, R. M., Aliguliyev, R. M., and Sukhostat, L. V. (2021). Parallel batch k-means for
big data clustering. Computers & Industrial Engineering, 152:107023. (Cited on page 3.)
Arabie, P. and Hubert, L. (1996). Advances in cluster analysis relevant to marketing re-
search. In Gaul, W. and Pfeifer, D., editors, From Data to Knowledge, pages 3–19, Berlin,
Heidelberg. Springer Berlin Heidelberg. (Cited on page 1.)
Armacki, A., Bajovic, D., Jakovetic, D., and Kar, S. (2022a). Gradient based clustering.
In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S., editors,
Proceedings of the 39th International Conference on Machine Learning, volume 162 of Pro-
ceedings of Machine Learning Research, pages 929–947. PMLR. (Cited on pages 2, 3, 4, 6,
10, 11, 12, 13, 17, and 30.)
Armacki, A., Bajovic, D., Jakovetic, D., and Kar, S. (2022b). Personalized federated learning
via convex clustering. In 2022 IEEE International Smart Cities Conference (ISC2), pages
1–7. (Cited on page 18.)
Armacki, A., Bajović, D., Jakovetić, D., and Kar, S. (2024). A one-shot framework for
distributed clustered learning in heterogeneous environments. IEEE Transactions on Signal
Processing, 72:636–651. (Cited on page 18.)
Arora, S., Raghavan, P., and Rao, S. (1998). Approximation schemes for euclidean k-medians
and related problems. In Proceedings of the Thirtieth Annual ACM Symposium on Theory
of Computing, STOC ’98, page 106–113, Dallas, Texas, USA. Association for Computing
Machinery. (Cited on page 3.)
Awasthi, P. and Balcan, M.-F. (2016). Foundations for center-based clustering: worst-case
approximations and modern developments. In Henning, C., Meila, M., Murtagh, F., and
Rocci, R., editors, Handbook of cluster analysis, pages 67–100. Chapman and Hall/CRC,
1st edition. (Cited on page 2.)
18Awasthi, P., Charikar, M., Krishnaswamy, R., and Sinop, A. K. (2015). The hardness of
approximation of euclidean k-means. arXiv preprint arXiv:1502.03316. (Cited on page 2.)
Awasthi, P. and Sheffet, O. (2012). Improved spectral-norm bounds for clustering. In Gupta,
A., Jansen, K., Rolim, J., and Servedio, R., editors, Approximation, Randomization, and
Combinatorial Optimization. Algorithms and Techniques, pages 37–49, Berlin, Heidelberg.
Springer Berlin Heidelberg. (Cited on page 18.)
Balcan, M.-F. F., Ehrlich, S., and Liang, Y. (2013). Distributed k-means and k-median
clusteringongeneraltopologies.InBurges,C.,Bottou,L.,Welling,M.,Ghahramani,Z.,and
Weinberger, K., editors, Advances in Neural Information Processing Systems, volume 26.
Curran Associates, Inc. (Cited on pages 3 and 4.)
Banerjee, A., Merugu, S., Dhillon, I. S., and Ghosh, J. (2005). Clustering with bregman
divergences. Journal of Machine Learning Research, 6(58):1705–1749. (Cited on pages 2, 3,
6, and 13.)
Bregman, L. (1967). The relaxation method of finding the common point of convex sets and
its application to the solution of problems in convex programming. USSR Computational
Mathematics and Mathematical Physics, 7(3):200–217. (Cited on page 6.)
Chandola, V., Banerjee, A., and Kumar, V. (2009). Anomaly detection: A survey. ACM
Comput. Surv., 41(3). (Cited on page 1.)
Chen, P., Chen, Y., and Rao, M. (2008). Metrics defined by Bregman Divergences. Commu-
nications in Mathematical Sciences, 6(4):915 – 926. (Cited on page 13.)
Chung, F. R. (1997). Spectral graph theory, volume 92. American Mathematical Soc. (Cited
on pages 7 and 28.)
Cvetkovic, D., Rowlinson, P., and Simic, S. (1997). Eigenspaces of Graphs. Encyclopedia of
Mathematics and its Applications. Cambridge University Press. (Cited on pages 7 and 28.)
Dafir, Z., Lamari, Y., and Slaoui, S. C. (2021). A survey on parallel clustering algorithms for
big data. Artificial Intelligence Review, 54:2411–2443. (Cited on page 3.)
Datta, S., Giannella, C., and Kargupta, H. (2009). Approximate distributed k-means cluster-
ing over a peer-to-peer network. IEEE Transactions on Knowledge and Data Engineering,
21(10):1372–1388. (Cited on page 3.)
Dennis, D. K., Li, T., and Smith, V. (2021). Heterogeneity for the win: One-shot federated
clustering. In Meila, M. and Zhang, T., editors, Proceedings of the 38th International
Conference on Machine Learning, volume139ofProceedings of Machine Learning Research,
pages 2611–2620. PMLR. (Cited on pages 3 and 18.)
Dhillon, I. S., Mallela, S., and Kumar, R. (2003). A divisive information-theoretic feature
clustering algorithm for text classification. Journal of Machine Learning Research (JMLR),
3:1265–1287. (Cited on page 1.)
Dimakis, A. G., Kar, S., Moura, J. M. F., Rabbat, M. G., and Scaglione, A. (2010). Gossip
algorithms for distributed signal processing. Proceedings of the IEEE, 98(11):1847–1864.
(Cited on page 10.)
19Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of
Eugenics, 7(2):179–188. (Cited on pages 15 and 34.)
Forero, P. A., Cano, A., and Giannakis, G. B. (2011). Distributed clustering using wireless
sensornetworks. IEEE Journal of Selected Topics in Signal Processing,5(4):707–724. (Cited
on pages 3, 4, 6, 9, 10, and 17.)
Ghosh, A., Chung, J., Yin, D., and Ramchandran, K. (2020). An efficient framework for
clustered federated learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and
Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages
19586–19597. Curran Associates, Inc. (Cited on page 18.)
Hallac, D., Leskovec, J., and Boyd, S. (2015). Network lasso: Clustering and optimization
in large graphs. In Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’15, page 387–396, New York, NY, USA.
Association for Computing Machinery. (Cited on page 18.)
Har-Peled, S. and Mazumdar, S. (2004). On coresets for k-means and k-median clustering. In
Proceedings of the Thirty-Sixth Annual ACM Symposium on Theory of Computing, STOC
’04, page 291–300, New York, NY, USA. Association for Computing Machinery. (Cited on
page 3.)
Huang, J., Feng, Q., Huang, Z., Xu, J., and Wang, J. (2023). Fast algorithms for distributed
k-clustering with outliers. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato,
S., and Scarlett, J., editors, Proceedings of the 40th International Conference on Machine
Learning, volume 202 of Proceedings of Machine Learning Research, pages 13845–13868.
PMLR. (Cited on page 3.)
Huber, P.J.(1964). RobustEstimationofaLocationParameter. The Annals of Mathematical
Statistics, 35(1):73 – 101. (Cited on page 6.)
Jain, A. K. (2010). Data clustering: 50 years beyond k-means. Pattern Recognition Letters,
31(8):651–666. Award winning papers from the 19th International Conference on Pattern
Recognition (ICPR). (Cited on page 1.)
Jakovetić, D., Bajović, D., Sahu, A. K., and Kar, S. (2018). Convergence rates for distributed
stochastic optimization over random networks. In 2018 IEEE Conference on Decision and
Control (CDC), pages 4238–4245. (Cited on page 4.)
Jakovetić, D., Bajović, D., Xavier, J., and Moura, J. M. F. (2020). Primal–dual methods
for large-scale and distributed convex optimization and data analytics. Proceedings of the
IEEE, 108(11):1923–1938. (Cited on page 2.)
Jakovetić, D., Xavier, J., and Moura, J. M. F. (2014). Fast distributed gradient methods.
IEEE Transactions on Automatic Control, 59(5):1131–1146. (Cited on page 4.)
Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz,
K., Charles, Z., Cormode, G., Cummings, R., D’Oliveira, R. G. L., Eichner, H., Rouayheb,
S. E., Evans, D., Gardner, J., Garrett, Z., Gascón, A., Ghazi, B., Gibbons, P. B., Gruteser,
M., Harchaoui, Z., He, C., He, L., Huo, Z., Hutchinson, B., Hsu, J., Jaggi, M., Javidi, T.,
Joshi, G., Khodak, M., Konecný, J., Korolova, A., Koushanfar, F., Koyejo, S., Lepoint, T.,
20Liu, Y., Mittal, P., Mohri, M., Nock, R., Özgür, A., Pagh, R., Qi, H., Ramage, D., Raskar,
R., Raykova, M., Song, D., Song, W., Stich, S. U., Sun, Z., Suresh, A. T., Tramèr, F.,
Vepakomma, P., Wang, J., Xiong, L., Xu, Z., Yang, Q., Yu, F. X., Yu, H., and Zhao, S.
(2021). Advances and open problems in federated learning. Foundations and Trends® in
Machine Learning, 14(1–2):1–210. (Cited on page 2.)
Kar,S.andMoura,J.M.(2013). Consensus+innovationsdistributedinferenceovernetworks:
cooperationandsensinginnetworkedsystems. IEEE Signal Processing Magazine, 30(3):99–
109. (Cited on page 7.)
Kar,S.,Moura,J.M.F.,andRamanan,K.(2012). Distributedparameterestimationinsensor
networks: Nonlinear observation models and imperfect communication. IEEE Transactions
on Information Theory, 58(6):3575–3605. (Cited on pages 2 and 7.)
Kar, S. and Swenson, B. (2019). Clustering with distributed data. arXiv preprint
arXiv:1901.00214. (Cited on pages 3, 4, 6, 9, 10, 12, 13, 15, and 35.)
Krizhevsky, A.(2009). Learningmultiplelayersoffeaturesfromtinyimages. Technical Report
TR-2009, University of Toronto. (Cited on pages 15 and 34.)
Kumar, A.andKannan, R.(2010). Clusteringwithspectralnormandthek-meansalgorithm.
In2010 IEEE 51st Annual Symposium on Foundations of Computer Science,pages299–308.
(Cited on page 18.)
Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278–2324. (Cited on pages 15
and 34.)
Li, S. and Guo, X. (2018). Distributed k-clustering for data with heavy noise. In Bengio,
S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors,
Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.
(Cited on page 3.)
Li, T., Sahu, A. K., Talwalkar, A., and Smith, V. (2020). Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50–60. (Cited on
page 2.)
Lloyd, S. (1982). Least squares quantization in PCM. IEEE Transactions on Information
Theory, 28(2):129–137. (Cited on pages 2 and 3.)
Lorenzo, P. D. and Scutari, G. (2016). Next: In-network nonconvex optimization. IEEE
Transactions on Signal and Information Processing over Networks, 2(2):120–136. (Cited on
page 4.)
MacQueen, J. (1967). Some methods for classification and analysis of multivariate observa-
tions. In 5-th Berkeley Symposium on Mathematical Statistics and Probability, volume 1,
pages 281–297. University of California Press. (Cited on page 2.)
McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A. y. (2017).
Communication-Efficient Learning of Deep Networks from Decentralized Data. In Singh,
21A. and Zhu, J., editors, Proceedings of the 20th International Conference on Artificial In-
telligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages
1273–1282. PMLR. (Cited on page 2.)
Megiddo,N.andSupowit,K.J.(1984). Onthecomplexityofsomecommongeometriclocation
problems. SIAM Journal on Computing, 13(1):182–196. (Cited on page 2.)
Nedić, A., Olshevsky, A., andShi, W.(2017). Achievinggeometricconvergencefordistributed
optimization over time-varying graphs. SIAM Journal on Optimization, 27(4):2597–2633.
(Cited on page 4.)
Nedić, A. and Ozdaglar, A. (2009). Distributed subgradient methods for multi-agent opti-
mization. IEEE Transactions on Automatic Control, 54(1):48–61. (Cited on page 4.)
Nesterov, Y. (2018). Lectures on Convex Optimization. Springer Publishing Company, Incor-
porated, 2nd edition. (Cited on pages 7, 25, and 30.)
Oliva, G., Setola, R., and Hadjicostis, C. N. (2013). Distributed k-means algorithm. arXiv
preprint arXiv:1312.4176. (Cited on page 3.)
Pediredla, A. K. and Seelamantula, C. S. (2011). A Huber-loss-driven clustering technique
and its application to robust cell detection in confocal microscopy images. In 2011 7th
International Symposium on Image and Signal Processing and Analysis (ISPA), pages 501–
506. (Cited on pages 1, 2, 3, and 6.)
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,
M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau,
D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12:2825–2830. (Cited on page 17.)
Pollard, D. (1981). Strong consistency of k-means clustering. The Annals of Statistics,
9(1):135–140. (Cited on page 18.)
Qiao, D., Ding, C., and Fan, J. (2023). Federated spectral clustering via secure similarity
reconstruction. In Thirty-seventh Conference on Neural Information Processing Systems.
(Cited on page 3.)
Qin, J., Fu, W., Gao, H., and Zheng, W. X. (2017). Distributed k -means algorithm and
fuzzy c -means algorithm for sensor networks based on multiagent consensus theory. IEEE
Transactions on Cybernetics, 47(3):772–783. (Cited on page 3.)
Rabbat, M., Nowak, R., and Bucklew, J. (2005). Generalized consensus computation in
networkedsystemswitherasurelinks. InIEEE 6th Workshop on Signal Processing Advances
in Wireless Communications, 2005., pages 1088–1092. (Cited on page 6.)
Sayed, A. H. (2014). Adaptive networks. Proceedings of the IEEE, 102(4):460–497. (Cited on
page 2.)
Selim, S. Z. and Ismail, M. A. (1984). K-means-type algorithms: A generalized convergence
theorem and characterization of local optimality. IEEE Transactions on Pattern Analysis
and Machine Intelligence, PAMI-6(1):81–87. (Cited on page 2.)
22Shi, W., Ling, Q., Wu, G., and Yin, W. (2015). Extra: An exact first-order algorithm
for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944–966.
(Cited on page 4.)
Sun, D., Toh, K.-C., and Yuan, Y. (2021). Convex clustering: Model, theoretical guarantee
and efficient algorithm. J. Mach. Learn. Res., 22(1). (Cited on page 18.)
Sundaram, S. and Gharesifard, B. (2019). Distributed optimization under adversarial nodes.
IEEE Transactions on Automatic Control, 64(3):1063–1076. (Cited on page 2.)
Swenson, B., Murray, R., Poor, H. V., and Kar, S. (2022). Distributed stochastic gradient
descent: Nonconvexity, nonsmoothness, and convergence to local minima. J. Mach. Learn.
Res., 23(1). (Cited on page 4.)
Taylor, P. (2021). Volume of data/information created, captured, copied, and consumed
worldwide from 2010 to 2020, with forecasts from 2021 to 2025 (in zettabytes). Technical
report, IDC & Statista. (Cited on page 2.)
Tsianos, K. I., Lawlor, S., and Rabbat, M. G. (2012). Consensus-based distributed opti-
mization: Practical issues and applications in large-scale machine learning. In 2012 50th
Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages
1543–1550. (Cited on page 2.)
Vattani, A. (2009). The hardness of k-means clustering in the plane.
https://cseweb.ucsd.edu/ avattani/papers/kmeans_hardness.pdf. (Cited on page 2.)
Vlaski, S., Kar, S., Sayed, A. H., and Moura, J. M. (2023). Networked signal and information
processing: Learning by multiagent systems. IEEE Signal Processing Magazine, 40(5):92–
105. (Cited on pages 2 and 6.)
Xin, R., Khan, U. A., and Kar, S. (2022a). Fast decentralized nonconvex finite-sum optimiza-
tion with recursive variance reduction. SIAM Journal on Optimization, 32(1):1–28. (Cited
on page 4.)
Xin, R., Khan, U. A., and Kar, S. (2022b). A fast randomized incremental gradient
method for decentralized nonconvex optimization. IEEE Transactions on Automatic Con-
trol, 67(10):5150–5165. (Cited on page 4.)
Xin, R., Pu, S., Nedić, A., and Khan, U. A. (2020). A general framework for decentralized
optimization with first-order methods. Proceedings of the IEEE, 108(11):1869–1889. (Cited
on page 4.)
Xu, R.andWunsch, D.(2005). Surveyofclusteringalgorithms. IEEE Transactions on Neural
Networks, 16(3):645–678. (Cited on page 1.)
Xu, R. and Wunsch, D. C. (2010). Clustering algorithms in biomedical research: A review.
IEEE Reviews in Biomedical Engineering, 3:120–154. (Cited on page 1.)
Yang, T. (2013). Trading computation for communication: Distributed stochastic dual coor-
dinate ascent. In Burges, C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K.,
editors, Advances in Neural Information Processing Systems, volume26.CurranAssociates,
Inc. (Cited on page 2.)
23Yu, H., Chen, H., Zhao, S., and Shi, Q. (2021). Distributed soft clustering algorithm for
iot based on finite time average consensus. IEEE Internet of Things Journal, 8(21):16096–
16107. (Cited on pages 3 and 7.)
Yu, S. and Kar, S. (2023). Secure distributed optimization under gradient attacks. IEEE
Transactions on Signal Processing, 71:1802–1816. (Cited on page 2.)
24A Introduction
The Appendix provides additional materials and results omitted from the main body of the
paper. Appendix B contains the proofs omitted from the main body. Appendix C provides
additional numerical results.
B Missing proofs
In this section we provide the proofs omitted from the main body. Subsection B.1 provides
proofsfromSection4.2, SubsectionB.2providesproofsfromSection4.3, whileSubsectionB.3
provides proofs omitted from Section 4.4.
B.1 Proofs from Section 4.2
Proof of Lemma 1. Recall the clustering cost J from (3). Since at least one cluster is non-
ρ
empty at each user, it readily follows that J is convex, as a sum of convex functions.
ρ
Next, we know from (5) that we can represent the gradient of J compactly as ∇J (x,C) =
ρ ρ
1/ρ∇J(x,C)+Lx. Therefore, for each x,z ∈ RKmd, we have
∥∇J ρ(x,C)−∇J ρ(z,C)∥ ≤ 1/ρ∥∇J(x,C)−∇J(z,C)∥+∥L(x−z)∥
(15)
≤ 1/ρ∥∇J(x,C)−∇J(z,C)∥+λ max(L)∥x−z∥,
where λ (L) is the largest eigenvalue of L. We now look at ∥∇J(x,C)−∇J(z,C)∥. First
max
note that, for each i ∈ [m] and k ∈ [K], the i,k-th component of ∇J(x,C) ∈ RKmd is given
by [∇J(x,C)] = (cid:80) w ∇f(x (k),y ). It then follows that
i,k r∈Ci(k) i,r i i,r
(cid:88) (cid:88)
∥∇J(x,C)−∇J(z,C)∥2 = ∥[∇J(x,C)−∇J(z,C)] ∥2
i,k
i∈[m]k∈[K]
= (cid:88) (cid:88) (cid:13) (cid:13) (cid:88) w (cid:2) ∇f(x (k),y )−∇f(z (k),y )(cid:3)(cid:13) (cid:13)2
(cid:13) i,r i i,r i i,r (cid:13)
i∈[m]k∈[K] r∈Ci(k)
( ≤a) (cid:88) (cid:88) (cid:88) w i,r(cid:13) (cid:13)w (cid:2) ∇f(x (k),y )−∇f(z (k),y )(cid:3)(cid:13) (cid:13)2
w (cid:13)(cid:101)i,k i i,r i i,r (cid:13)
(cid:101)i,k
i∈[m]k∈[K]r∈Ci(k)
(b) (cid:88) (cid:88) (c)
≤ β2 w2 ∥x (k)−z (k)∥2 ≤ β2∥x−z∥2,
(cid:101)i,k i i
i∈[m]k∈[K]
wherew = (cid:80) w ,(a)followsfromJensen’sinequality,(b)followsfromβ-smoothness
(cid:101)i,k r∈Ci(k) i,r
of f, while (c) follows from the fact that w < 1. Using the properties of the Kronecker prod-
(cid:101)i,k
uct, it can be shown that λ (L) = λ (L), i.e., the largest eigenvalue of L ∈ RKmd×Kmd
max max
corresponds to the largest eigenvalue of L ∈ Rm×m. Plugging everything back in (15) yields
∥∇J ρ(x,C)−∇J ρ(z,C)∥ ≤ β/ρ∥x−z∥+λ max(L)∥x−z∥ = β L,ρ∥x−z∥.
It can be shown that convexity and β -Lipschitz continuous gradients together imply β -
L,ρ L,ρ
smoothness, see, e.g., Nesterov (2018). This completes the proof.
25Proof of Lemma 2. First,notethat(6)togetherwithAssumption5impliesthattheclustering
reassignment step decreases the cost since, at every time t ≥ 1 and user i ∈ [m],
H(xt,Ct+1) = (cid:88) (cid:88) w f(cid:0) xt(k),y(cid:1) ≤ (cid:88) (cid:88) w f(cid:0) xt(k),y(cid:1) = H(xt,Ct),
i i y i y i i i
k∈[K]y∈Ct+1(k) k∈[K]y∈Ct(k)
i i
while the consensus part, ⟨xt,Lxt⟩, remains unchanged. This readily implies that
1 1 1 1
J (xt,Ct+1) = J(xt,Ct+1)+ ⟨xt,Lxt⟩ ≤ J(xt,Ct)+ ⟨xt,Lxt⟩ = J (xt,Ct). (16)
ρ ρ
ρ 2 ρ 2
Next, from Lemma 1 we have that, for any t ≥ 08 and b = 0,...,B−1,
(cid:68) (cid:69) β
J (xt,b+1,Ct+1) ≤ J (xt,b,Ct+1)+ ∇J (xt,b,Ct+1),xt,b+1−xt,b + L,ρ ∥xt,b+1−xt,b∥2.
ρ ρ ρ
2
Using (11), we get
J (xt,b+1,Ct+1) ≤ J (xt,b,Ct+1)−c(α)∥∇J (xt,b,Ct+1)∥2, (17)
ρ ρ ρ
(cid:16) (cid:17)
where c(α) = α 1− αβL,ρ . Applying (17) recursively, and recalling that xt+1 = xt,B,
2
xt = xt,0, we get that, for any time t ≥ 0
B−1
(cid:88)
J (xt+1,Ct+1) ≤ J (xt,Ct+1)−c(α) ∥∇J (xt,b,Ct+1)∥2. (18)
ρ ρ ρ
b=0
Choosing α < 1 guarantees that c(α) > 0, which readily implies
βL,ρ
J (xt+1,Ct+1) ≤ J (xt,Ct+1). (19)
ρ ρ
Finally, combining (16) and (19), we get that, for any t ≥ 1,
J (xt+1,Ct+1) ≤ J (xt,Ct+1) ≤ J (xt,Ct),
ρ ρ ρ
which completes the proof.
Remark 13. We can see the benefit of performing B rounds of center update in equation (18),
where a higher value of B leads to a stronger decrease in the cost function J .
ρ
Proof of Lemma 4. Let {xts} s∈N be a convergent subsequence of {xt} t∈N. Let x⋆ ∈ RKmd be
its limit point and assume the contrary, that x⋆ is not a fixed point. By Definition 2, this
implies ∥∇J (x⋆,C)∥ > 0, for all C ∈ U As the number of possible clusterings is finite, we
ρ x⋆
define
ϵ = min ∥∇J (x⋆,C)∥ > 0. (20)
1 ρ
C∈U x⋆
Next, from the continuity of g, we know that, for any ϵ > 0, there exists a δ > 0, such that,
for any x,x′ ∈ Rd, for which ∥x−x′∥ ≤ δ, we have g(x,x′) < ϵ. Choose ϵ > 0 from Lemma 3.
∗
8Notethat,startingfromthecenterinitializationx0,wefirstupdatetheclusterstoobtainC1. Fromthere,
we perform E center updates to obtain x1, hence for the center update step, the counter starts from t = 0.
Thisisdifferentfortheclusterupdatestep,aswedonothaveaclusteringattimet=0,socostdecreasewith
respect to cluster update only starts from iteration t=1.
26From lim s→∞xts = x⋆, we know that, for any fixed δ > 0, there exists a sufficiently large
s ∈ N, such that, for all i ∈ [m], k ∈ [K] and all s ≥ s , ∥xts(k)−x⋆(k)∥ < δ. From the
0 0 i i
previous discussion, it then readily follows that there exists a δ > 0 and a sufficiently large
∗
s ∈ N, such that g(xts(k),x⋆(k)) < ϵ , for all i ∈ [m], k ∈ [K] and s ≥ s . Per Lemma 3, we
0 i i ∗ 0
then have C ∈ U ⊂ U , for all s ≥ s . From (20), it follows that, for all s ≥ s ,
xts+1 xts x⋆ 0 0
∥∇J (x⋆,Cts+1)∥ ≥ ϵ . (21)
ρ 1
Next, using (18), we have
B−1
(cid:88)
J (xt+1,Ct+1) ≤ J (xt,Ct)−c(α) ∥∇J (xt,b,Ct+1)∥2
ρ ρ ρ
b=0
t B−1
(cid:88)(cid:88)
≤ ... ≤ J (x0,C1)−c(α) ∥∇J (xs,e,Cs+1)∥2.
ρ ρ
s=0 b=0
Rearranging, we get
t B−1
(cid:88)(cid:88)
c(α) ∥∇J (xs,e,C )∥2 ≤ J (x0,C1). (22)
ρ s+1 ρ
s=0 b=0
Additionally, note that
s(t) s(t)B−1 t B−1
(cid:88) (cid:88)(cid:88) (cid:88)(cid:88)
∥∇J (xtj,Ctj+1)∥2 ≤ ∥∇J (xtj,e,Ctj+1)∥2 ≤ ∥∇J (xj,Cj+1)∥2, (23)
ρ ρ ρ
j=0 j=0 b=0 j=0 b=0
where s(t) = sup{j : t ≤ t}. Combining (22) and (23), we get
j
s(t)
(cid:88)
c(α) ∥∇J (xtj,Ctj+1)∥2 ≤ J (x0,C1). (24)
ρ ρ
j=0
Since the term on the right hand side of (24) is finite and independent of t, we can take the
limit as t goes to infinity, to obtain
∞
(cid:88)
c(α) ∥∇J (xtj,Ctj+1)∥2 < ∞, (25)
ρ
j=0
where we use the fact that lim s(t) = ∞. The inequality (25) implies that
t→∞
lim s→∞∥∇J ρ(xts,Cts+1)∥2 = 0. Next, fix an ϵ > 0. By the definition of limits, there ex-
ists a s
1
∈ N, such that, for all s ≥ s 1, ∥∇J ρ(xts,Cts+1)∥ < ϵ. On the other hand, from
lim s→∞xts = x⋆, we know that there exists a s
2
∈ N, such that ∥xts−x⋆∥ < ϵ, for all s ≥ s 2.
As C ∈ U ⊂ U , for all s ≥ s , we then have, for any s ≥ max{s ,s ,s },
xts+1 xts x⋆ 0 0 1 2
∥∇J (x⋆,Cts+1)∥ ≤ ∥∇J (x⋆,Cts+1)−∇J (xts,Cts+1)∥+∥∇J (xts,Cts+1)∥
ρ ρ ρ ρ
≤ β ∥x⋆−xts∥+ϵ < (β +1)ϵ,
L,ρ L,ρ
where we used the Lipschitz continuity of gradients of J in the second inequality. As ϵ > 0
ρ
was arbitrarily chosen, we can conclude that
lim ∥∇J (x⋆,Cts+1)∥ = 0, (26)
ρ
s→∞
which clearly contradicts (21). Hence, it follows that x⋆ is a fixed point, i.e., there exists a
clustering C ∈ U , such that ∇J (x⋆,C) = 0.
x⋆ ρ
27Proof of Lemma 6. By Lemma 2, we know that
... ≤ J (xt,Ct+1) ≤ ... ≤ J (x1,C1) ≤ J (x0,C1) < ∞. (27)
ρ ρ ρ
Next, suppose the contrary, that the sequence of centers {xt}
t∈N
is unbounded. This im-
plies the existence of a user i ∈ [m], cluster k ∈ [K], and a subsequence t , s ∈ N, such
s
that lim ∥xts(k)∥ = ∞. From Assumption 2, if consensus for the k-th cluster is not
s→∞ i
reached, i.e., if there exists a j ∈ [m] such that lim ∥xts(k)−xts(k)∥ > 0, then clearly
s→∞ i j
lim s→∞⟨xts,Lxts⟩ → ∞, which implies J ρ(xts,Cts) → ∞, contradicting (27).
Therefore,itmustbethatlim ∥xts(k)−xts(k)∥ = 0,forallj ∈ [m]. Foreachs ∈ N,let
s→∞ i j
t = max(cid:8) t ≤ t : there exists a j ∈ [m] such that Ct(k) ̸= ∅(cid:9), i.e., t is the largest element
s s j s
in the sequence prior to t , such that the k-th cluster of at least one user is non-empty. We
s
then have the following possibilities:
1. If t = t , then xts(k) = xts(k), for all j ∈ [m].
s s j j
2. If t < t , then, recalling the update rule (7), for all j ∈ [m], we have
s s
xts(k) = xts−1,B−1(k)−α (cid:88) (cid:16) xts−1,B−1(k)−xts−1,B−1(k)(cid:17)
j j j l
l∈Nj
= xts−1,B−2(k)−α (cid:88) (cid:16) xts−1,B−1(k)−xts−1,B−1(k)+xts−1,B−2(k)−xts−1,B−2(k)(cid:17)
j j l j l
l∈Nj
B−1
= ... =
xts−1(k)−α(cid:88) (cid:88) (cid:16) xts−1,b(k)−xts−1,b(k)(cid:17)
j j l
b=0 l∈Nj
= ... =
xts(k)−αt (cid:88)s−tsB (cid:88)−1
(cid:88) (cid:16) xts−r,b(k)−xts−r,b(k)(cid:17)
.
j j l
r=1 b=0 l∈Nj
By the definition of t , if t < t , it follows that, for the k-th center, the algorithm only
s s s
performs B(t −t ) consensus steps between times t and t , i.e., only diffuses the k-th
s s s s
center estimates xts(k), j ∈ [m], across the network9.
j
From 1. and 2. we can readily conclude that xts(k) only depends on the k-th center esti-
j
mates at time t , xts(k), l ∈ [m], for all users. From the preceding discussion, the facts that
s l
lim ∥xts(k)∥ = ∞ and lim ∥xts(k) − xts(k)∥ = 0, for all j ∈ [m], we can readily
s→∞ i s→∞ i j
conclude that ∥xts(k)∥ → ∞, for all j ∈ [m]. For a center x ∈ RKmd and clustering C, define
j
(cid:88) (cid:88)
H (x,C) = w f(x (k),y ),
k j,r j j,r
j∈[m]r∈Cj(k)
i.e., for a given center and clustering, H defines the cost associated with the k-th cluster
k
across all the users. Combining the facts that ∥xts(k)∥ → ∞, for all j ∈ [m], and that for
j
9To be more precise, since the algorithm performs E(t −t ) consensus steps, the diffusion is performed
s s
acrosstheE(t −t )-hopneighbours. Foravertexi,thesetofK-hopneighboursofiisthesetofverticesthat
s s
can be reached from i by traversing at most K edges, see, e.g., Chung (1997); Cvetkovic et al. (1997).
28every s ∈ N, there exists a j ∈ [m], such that Cts(k) ̸= ∅, with Assumption 3, we get
j
(cid:16) (cid:17) (cid:88) (cid:88)
lim H xts,Cts = lim w f(xts(k),y ) = ∞.
k j,r j j,r
s→∞ s→∞
j∈[m]r∈Cts(k)
j
It is easy to see that unboundness of H implies unboundedness of J , i.e., we have
k ρ
lim s→∞J ρ(xts,Cts) = ∞, clearly contradicting (27). Therefore, the desired claim follows.
Proof of Lemma 7. Recall that, by Lemma 2, the sequence of costs {J ρ(xt,Ct)} t∈N is non-
increasing. Moreover, since J ≥ 0, we know that the limit of the sequence of costs exists and
ρ
is finite. Let
J⋆ = lim J (xt,Ct). (28)
ρ ρ
t→∞
By assumption, U ̸= ∅. From the definition of U , for all C ∈ U \U we have
x⋆ x⋆ x⋆ x⋆
∥∇J (x⋆,C)∥ > 0. (29)
ρ
As U is a finite set, we can define ϵ = min ∥∇J (x⋆,C)∥ > 0. Let ϵ > 0 be such
that
Lx⋆
emma 3 holds. From the
contin1
uity
ofC g,∈U wxe⋆\ kU nxo⋆
w
thaρ
t there exists a δ
>∗
0, such that,
∗
for all x ∈ RKmd, i ∈ [m], k ∈ [K],
∥x−x⋆∥ < δ =⇒ g(x (k),x⋆(k)) < ϵ . (30)
∗ i i ∗
Define
(cid:26) (cid:27)
ϵ
ϵ⋆ = min δ , 1 . (31)
∗
β
L,ρ
For an arbitrary ϵ ∈ (0,ϵ⋆), let t ∈ N be such that, for all t ≥ t ,
0 0
c(α)
J (xt,Ct) ≤ J⋆+ (ϵ −β ϵ)2, (32)
ρ ρ 2 1 L,ρ
with c(α) defined as in Lemma 2. Note that such a choice of t is possible, from (28) and the
0
fact that (ϵ −β ϵ)2 > 0. Our goal now is to show that, for a fixed ϵ ∈ (0,ϵ⋆), if for some t
1 L,ρ
such that t ≥ t and ∥xt−x⋆∥ < ϵ, then ∥xt+1−x⋆∥ < ϵ.
0
First note that, if t ≥ t and ∥xt − x⋆∥ < ϵ, it holds that Ct+1 ∈ U . To see this,
0 x⋆
assume the contrary, ∥xt−x⋆∥ < ϵ and Ct+1 ∈/ U . It follows from (31) that ∥xt−x⋆∥ < δ .
x⋆ ∗
From (30) and Lemma 3, we then have U ⊂ U , and hence, Ct+1 ∈ U . Using Lipschitz
xt x⋆ x⋆
continuity of gradients of J , we get
ρ
∥∇J (xt,Ct+1)−∇J (x⋆,Ct+1)∥ ≤ β ∥xt−x⋆∥ ≤ β ϵ. (33)
ρ ρ L,ρ L,ρ
As Ct+1 ∈/ U , from (29), we have
x⋆
∥∇J (x⋆,Ct+1)∥ ≥ ϵ . (34)
ρ 1
Applying the triangle inequality, (33) and (34), we get
∥∇J (xt,Ct+1)∥ ≥ ϵ −β ϵ. (35)
ρ 1 L,ρ
29Note that by (31), the right-hand side of (35) is positive. Combining (18), (32), (35), we get
c(α)
J (xt+1,Ct+1) ≤ J (xt,Ct)−c(α)∥∇J (xt,Ct+1)∥2 ≤ J⋆+ (ϵ −β ϵ)2−c(α)∥∇J (xt,Ct+1)∥2
ρ ρ ρ ρ 2 1 L,ρ ρ
c(α)
≤ J⋆+ (ϵ −β ϵ)2−c(α)(ϵ −β ϵ)2 < J⋆,
ρ 2 1 L,ρ 1 L,ρ ρ
which is a contradiction. Hence, Ct+1 ∈ U . Next, it can be shown that convexity and
x⋆
β -smoothness of J together imply β -co-coercivity of J , i.e., for each fixed clustering
L,ρ ρ L,ρ ρ
C ∈ C and x,z ∈ RKmd, we have
m,K,D
1
⟨∇J (x,C)−∇J (z,C),x−z⟩ ≥ ∥∇J (x,C)−∇J (z,C)∥2.
ρ ρ ρ ρ
β
L,ρ
For a formal account of this result, see, e.g., Nesterov (2018). Combining this fact with the
update rule (11), and the fact that Ct+1 ∈ U , we have
x⋆
∥xt+1−x⋆∥2 = ∥xt,B−1−α∇J (xt,B−1,Ct+1)−x⋆∥2
ρ
= ∥xt,B−1−x⋆∥2+α2∥∇J (xt,B−1,Ct+1)∥2−2α⟨∇J (xt,B−1,Ct+1),xt−x⋆⟩
ρ ρ
(cid:18) (cid:19)
1
≤ ∥xt,B−1−x⋆∥2−α −α ∥∇J (xt,B−1,Ct+1)∥2.
ρ
β
L,ρ
Repeating the argument recursively and recalling that xt,0 = xt, we get
(cid:18) (cid:19)B−1
1 (cid:88)
∥xt+1−x⋆∥2 ≤ ∥xt−x⋆∥2−α −α ∥∇J (xt,b,Ct+1)∥ ≤ ∥xt−x⋆∥2 < ϵ2. (36)
ρ
β
L,ρ
b=0
where second inequality follows from the step-size choice α < 1 . Therefore, we have shown
βL,ρ
that∥xt−x⋆∥ < ϵimplies∥xt+1−x⋆∥ < ϵ. Thesameresultholdsforalls > tinductively.
Remark 14. We can again see the benefit of performing B rounds of center update in (36),
where higher values of B lead to stronger decrease in distance of centers from a fixed point.
B.2 Proofs from Section 4.3
Example of Bregman distances that are squares of metrics. ForthesquaredEuclidean
normf(x,y) = ∥x−y∥2,wehaveg(x,y) = (cid:112) f(x,y) = ∥x−y∥. Similarly,fortheMahalanobis-
like Bregman distance f(x,y) = ∥x−y∥2, for a positive definite matrix A, we have g(x,y) =
A
(cid:112) f(x,y) = ∥x−y∥ . For further examples, see Armacki et al. (2022a).
A
Proof of Lemma 4. By Definition 2, we know that (x⋆,C⋆) must satisfy
1
0 = ∇J (x⋆,C⋆) = J(x⋆,C⋆)+Lx⋆. (37)
ρ
ρ
By the definition of J(x⋆,C⋆) and L, it is not hard to see that, for each i ∈ [m], the k-th
component of the gradient of J (x⋆,C⋆), for any k ∈ [K], is given by
ρ
1 (cid:88) (cid:88)
[∇J(x⋆,C⋆)] = w ∇f(x⋆(k),y )+ (x⋆(k)−x⋆(k)). (38)
i,k ρ i,r i i,r i j
r∈C i⋆(k) j∈Ni
30Using the definition of Bregman divergence, we have
∇ f(x,y) = −∇ψ(x)+∇ψ(x)+∇2ψ(x)(x−y) = ∇2ψ(x)(x−y). (39)
x
Combining (37), (38) and (39), for any i ∈ [m] and k ∈ [K], we get
1 (cid:88) (cid:88)
w ∇2ψ(x⋆(k))(x⋆(k)−y )+ (x⋆(k)−x⋆(k)) = 0 ⇐⇒
ρ i,r i i i,r i j
r∈C i⋆(k) j∈Ni
 
1 (cid:88) 1 (cid:88) (cid:88)
 ρ∇2ψ(x⋆ i(k)) w i,r +|N i|I d×dx⋆ i(k) =
ρ
w i,ry i,r + x⋆ j(k). (40)
r∈C i⋆(k) r∈C i⋆(k) j∈Ni
AccordingtoAssumption6,thematrixP = 1∇2ψ(x⋆(k))(cid:80) w +|N |I ispositive
i,k ρ i r∈C⋆(k) i,r i d×d
i
definite, and hence invertible. Multiplying both sides of (40) with P−1 completes the proof.
i,k
B.3 Proofs from Section 4.4
Lemma 12. Assumption 7 is satisfied for K-means, Huber and logistic loss functions.
Proof. For f(x,y) = 1∥x−y∥2, we have ∇f(x,y) = x−y, i.e., γ(x,y) = 1. For f(x,y) =
2
ϕ (∥x−y∥), we have
δ
(cid:40)
x−y, ∥x−y∥ ≤ δ
∇f(x,y) =
δ(x−y)
, ∥x−y∥ > δ
∥x−y∥
i.e.,
(cid:40)
1, ∥x−y∥ ≤ δ
γ(x,y) = .
δ , ∥x−y∥ > δ
∥x−y∥
Finally, for f(x,y) = log(1+exp(g(x,y)2)), we have ∇f(x,y) = 2(x−y) , i.e., γ(x,y) =
1+exp(−∥x−y∥2)
2 , which completes the proof.
1+exp(−∥x−y∥2)
Proof of Lemma 9. We will show a stronger result, namely, that for each fixed ρ, the sequence
of centers {xt}
t∈N
generated by DGC-F
ρ
stays in co(D,x0). We prove the claim by induction.
Clearly, for each i ∈ [m] and k ∈ [K], we have x0(k) ∈ co(D,x0).
i
Next,assumethat,forsomet > 0andeachi ∈ [m]andk ∈ [K],wehavext(k) ∈ co(D,x0).
i
Recalling the update equation (7), using Assumption 7 and the fact that xt,0(k) = xt(k), it
i i
follows that, for each i ∈ [m] and k ∈ [K]
(cid:18) (cid:19)
xt,1(k) = xt(k)−α (cid:88) (cid:2) xt(k)−xt(k)(cid:3) + 1 (cid:88) w γt (k)(xt(k)−y )
i i i j ρ i,r i,r i i,r
j∈Ni r∈Ct+1(k)
i
(cid:18) (cid:19)
= 1−α(cid:0) |N i|+1/ρ (cid:88) w i,rγ it ,r(k)(cid:1) xt i(k)+α (cid:88) xt j(k)+α/ρ (cid:88) w i,rγ it ,r(k)y i,r,
r∈Ct+1(k) j∈Ni r∈Ct+1(k)
i i
(41)
whereweuseγt (k)asashorthandnotationforγ(xt(k),y ). Itcanbereadilyseenthat(41)is
i,r i i,r
aconvexcombinationofxt(k),xt(k)andy ,j ∈ N ,r ∈ Ct+1(k),forα < 1 .
i j i,r i i 1/ρ(cid:80) r∈Ct+1wi,rγ it ,r(k)+|Ni|
i
31It is easy to see that |N | ≤ λ (L) and, using the results from Lemma 12, it can be
i max
shown that γ(x,y) ≤ β, for all three loss functions (K-means, Huber and logistic), where
we recall that β is the smoothness parameter. Finally, recalling that our step-size satisfies
α < 1 ≤ 1 , itreadilyfollowsfrom(41)thatxt,1(k)isaconvex
β/ρ+λmax(L) 1/ρ(cid:80) r∈Ct+1wi,rγ it ,r(k)+|Ni| i
i
combination of elements from co(D,x0), by induction hypothesis. Using the same arguments,
we can easily show that xt,b(k) ∈ co(D,x0), for all i ∈ [m], k ∈ [K] and b = 2,...,B −1.
i
Since xt+1 = xt,B, it follows that xt+1(k) = xt,B(k) ∈ co(D,x0), which completes the induc-
i i
tion proof.
Note that the set co(D,x0) is closed, it readily follows that the limit points of sequences
generated by DGC-F stay in co(D,x0). According to Theorem 1, the limit points are the
ρ
fixed points, which completes the proof.
Proof of Lemma 10. Assumethecontrary,thatthereexistsaclusterk ∈ [K]suchthatC(k) ̸=
∅andx(k) ∈/ co(D). Definex(k)tobetheprojectionofx(k)ontoco(C(k)),withrespecttothe
(cid:101)
Euclidean distance, i.e., x(k) = argmin ∥x(k)−y∥. Note that the projection is well
(cid:101) y∈co(C(k))
defined, as the distance metric is induced by an inner product and co(C(k)) is a non-empty,
closed, convex set. By Assumption 5, for all r ∈ C(k), we have
g(x(k),y ) ≤ g(x(k),y ) =⇒ f(x(k),y ) ≤ f(x(k),y ),
(cid:101) i,r i,r (cid:101) i,r i,r
where we used the fact that x(k) is the projection of x(k) onto co(C(k)).
(cid:101)
If forall r ∈ C(k)we have∥x(k)−y ∥ = ∥x(k)−y ∥, it readilyfollows that∥x(k)−y∥ =
(cid:101) i,r i,r (cid:101) (cid:101)
∥x(k)−y∥, for all y ∈ co(C(k)). If x(k) ∈ co(C(k)), it then follows that ∥x(k)−x(k)∥ =
(cid:101) (cid:101) (cid:101) (cid:101)
∥x(k)−x(k)∥ = 0, implying that x(k) ∈ co(C(k)), which can not be, as co(C(k)) ⊆ co(D)
(cid:101) (cid:101)
and x(k) ∈/ co(D). Similarly, if x(k) ∈ ∂co(C(k)), where ∂co(C(k)) denotes the boundary
(cid:101)
of co(C(k)), by definition of boundary, there exists a sequence {y (cid:101)n} n∈N ⊂ co(C(k)) that
converges to x(k). From the fact that ∥x(k) − y ∥ = ∥x(k) − y ∥, for all n ∈ N, we have
(cid:101) (cid:101) (cid:101)n (cid:101)n
lim ∥x(k)−y ∥ = lim ∥x(k)−y ∥ = 0, resulting in x(k) ∈ ∂co(C(k)), which again
n→∞ (cid:101)n n→∞ (cid:101) (cid:101)n
can not be.
Therefore, there must exist a r ∈ C(k) such that ∥x(k)−y ∥ < ∥x(k)−y ∥, implying
(cid:101) i,r i,r
that f(x(k),y ) < f(x(k),y ). As such, we have
(cid:101) i,r i,r
(cid:88) (cid:88)
w f(x(k),y ) < w f(x(k),y ).
i,r (cid:101) i,r i,r i,r
r∈C(k) r∈C(k)
Defining x = (cid:2) x(1)⊤ ... x(K)⊤(cid:3)⊤, where x(l) = x(l) for all l ̸= k and x(k) = x(k), it can
(cid:98) (cid:98) (cid:98) (cid:98) (cid:98) (cid:101)
be readily observed that
H(x,C) < H(x,C), (42)
(cid:98)
where H is the centralized cost from (1). From the convexity of f (Assumption 3) and Defini-
tion 4, it readily follows that a consensus fixed point must satisfy x ∈ argmin H(y,C),
y∈RKd
which is clearly violated in (42). Therefore, the claim follows.
Prior to proving Lemma 11, consider the function F (x) = J(x,C), where C is a fixed
c
clustering of the dataset D into K clusters. From the finiteness of D, we know that there is
a finite number of distinct partitions and hence a finite number of distinct functions F (x),
c
while from Assumption 3 it follows that all functions F (x) are coercive. This in turns implies
c
the existence of a global minimizer of F (x), i.e., a point z ∈ RKmd, such that F (z ) =
c c c c
32min F (x). It can readily be seen that, if some of the partitions of the clustering C are
x∈RKmd c
empty, we can set the corresponding centers to be any value without changing the minimum,
e.g., if C (k) = ∅, the point z′ ∈ RKmd, given by z′ (l) = z (l), for all l ∈ [K] and j ̸= i, and
i c c,j c,j
(cid:40)
z (l), l ̸= k
z′ (l) = c,i ,
c,i
0, l = k
then z′ = argmin F (x). Recall that C denotes the set of all K-partitions of the
c x∈RKmd c K,D
full dataset D. Combining this with the coercivity of F and the finiteness of D, it readily
follows that, for each C ∈ C , we can find a minimizer z′ = argmin F (x), such
K,D c x∈RKmd c
that max ∥z′∥ < ∞. Moreover, Lemma 10 guarantees an even stronger result, that we
C∈CK,D c
can find z′ such that z′ = argmin F (x) and z′(k) ∈ co(D), for each k ∈ [K]. Since
c c x∈RKmd c
co(D) ⊂ co(D,x0) and co(D,x0) is a compact set (follows from finiteness of D), there exits a
finite R, such that R = max ∥x∥.
x∈co(D,x0)
Proof of Lemma 11. Recall that the cost function (5). By definition, we know that, for any
C ∈ U
ρ xρ
1
∇J (x ,C ) = 0 ⇐⇒ Lx = − ∇J(x ,C ). (43)
ρ ρ ρ ρ ρ ρ
ρ
Consider the function F (x) = J(x,C ). From the discussion preceding Lemma 11 and the
ρ ρ
fact that U ⊂ C is a finite set, it readily follows that, for each ρ ≥ 1, we can obtain
xρ m,K,D
a sequence of global minima z of the functions F , such that z ∈ co(D). From (43) and
ρ ρ ρ
Assumption 7, it follows that
1 1 mβ 2mβR
∥Lx ∥ = ∥∇F (x )∥ = ∥∇F (x )−∇F (z )∥ ≤ ∥x −z ∥ ≤ ,
ρ ρ ρ ρ ρ ρ ρ ρ ρ
ρ ρ ρ ρ
which is what we wanted to show.
We are now ready to prove Theorem 2.
Proof of Theorem 2. From Lemma 11, we know that {x } achieves consensus as ρ → ∞, i.e.,
ρ
there exists a point x ∈ RKd such that lim x = x, for all i ∈ [m]. Define x = 1 ⊗x ∈
ρ→∞ i,ρ m
RKmd and note that it satisfies the point 1) in Definition 4. To prove point 2), it remains to
show that there exists a clustering C ∈ U , such that 1⊤∇J(x,C) = 0. To that end, assume
x
the contrary, that for any C ∈ U
x
∥1⊤∇J(x,C)∥ > 0.
As the number of possible clusterings is finite, we know that there exists an ϵ > 0, such that
min ∥1⊤∇J(x,C)∥ = ϵ > 0. (44)
C∈Ux
Next,notethatfromx = lim x ,andLemma3,thereexistsaρ ≥ 1,suchthatU ⊆ U ,
ρ→∞ ρ 0 xρ x
for all ρ ≥ ρ . As x is a fixed point of J , we know that, for some C ∈ U
0 ρ ρ ρ xρ
0 = ρ∇J (x ,C ) = ∇J(x ,C )+ρLx ,
ρ ρ ρ ρ ρ ρ
which readily implies
∇J(x,C ) = ∇J(x,C )−∇J(x ,C )−ρLx .
ρ ρ ρ ρ ρ
33Using the fact that 1⊤L = 0, we get
1⊤∇J(x,C ) = 1⊤(∇J(x,C )−∇J(x ,C )).
ρ ρ ρ ρ
Taking the norm and using the smoothness of J with respect to the first variable, we get
√
∥1⊤∇J(x,C )∥ ≤ m∥∇J(x,C )−∇J(x ,C )∥ ≤ m3/2β∥x−x ∥.
ρ ρ ρ ρ ρ
Since ∥x − x∥ → 0 and C ∈ U , for all ρ ≥ ρ , it follows that ∥1⊤∇J(x,C )∥ → 0,
ρ ρ x 0 ρ
contradicting (44). Hence, x is a consensus fixed point. Convergence of clusters for a finite
value of ρ is now a direct consequence of the fact that x → x and Lemma 3.
ρ
C Additional experiments
In this section we provide additional numerical experiments. To begin, we provide a detailed
description of the datasets.
We use Iris Fisher (1936), MNIST Lecun et al. (1998) and CIFAR10 Krizhevsky (2009)
data. Iris consists of K = 3 classes, with N = 150 samples evenly split among the classes
and d = 4 features. MNIST and CIFAR10 consist of ten classes each, from which we create
smaller subsets. In particular, we use the first seven digits of MNIST and create two CIFAR
datasets with three (CIFAR3) and eight (CIFAR8) classes, for a total K = {7,3,8} classes for
MNIST, CIFAR3 and CIFAR8. We randomly select a thousand samples per class, for a total
ofN = {7000,3000,8000}samplesforMNIST,CIFAR3andCIFAR8. Thenumberoffeatures
of MNIST and CIFAR data, corresponding to pixels of the images, is d = {784,3072}. We
normalize the data, dividing all the pixels by the highest value, so that each pixel belongs to
[0,1]. In all experiments we use a network of m = 10 users, communicating over a ring graph,
which is visualized in Figure 4. Each user is assigned an equal number of samples from each
class, so that each user contains N = {15,700,300,800} samples for Iris, MNIST, CIFAR3
i
and CIFAR8, respectively.
Figure 4: Ten users communicating over a ring graph, with each user having exactly two neighbours.
Next, we study the effects of B and ρ on accuracy. The setup is the same as in Section 5
and we run the methods for T = 500 iterations. The results are presented in Tables 5 and 6.
Table 5 shows the accuracy of our methods for different values of ρ, with B = 1. We can see
that the accuracy is typically the largest for ρ = 1, which can be explained by the fact that
for Iris data local datasets seem to be representative of the global data and for ρ = 1 the users
strike the best balance between finding good clusters and collaborating. Table 6 shows the
accuracy of our methods for different values of B, with ρ = 10. We can see that the accuracy
is unaffected by different values of B. Combined with the experiments from Section 5, this
34implies that larger B should be chosen when convergence speed is a priority, while smaller B
should be chosen when communication or computation are constrained. In either case, the
accuracy shouldn’t be affected significantly.
Table 5: Effect of ρ on accuracy, with B =1.
ρ=1 ρ=10 ρ=100 ρ=1000
DGC-KM 91.53±2.17% 91.13±0.85% 89.73±0.53% 91.93±0.96%
ρ
DGC-HL 91.86±1.26% 91.20±0.78% 90.00±0.89% 91.80±1.39%
ρ
DGC-LL 90.73±2.12% 91.00±0.45% 89.00±0.33% 90.67±1.23%
ρ
Table 6: Effect of B on accuracy, with ρ=10.
B =1 B =10 B =100
DGC-KM 90.8±0.9% 90.8±0.9% 90.6±0.6%
10
DGC-HL 90.6±0.5% 90.8±0.3% 90.7±0.2%
10
DGC-LL 90.6±0.6% 90.7±0.5% 90.6±0.3%
10
The last set of experiments studies the effects of ρ on convergence speed. We run our
algorithms for T = 1000 iterations, computing the cost J in each iteration, denoted by Jt.
ρ ρ
To normalize the data, we then subtract the final value, i.e., we plot J hot−JT, as we know
r ρ
that the cost is decreased in each iteration. We call the quantity J hot−JT normalized cost,
r ρ
and use it as the performance metric. The results are presented in Figure 5. We can see that
the normalized cost converges slower for larger values of ρ, as discussed in Section 4 and as
noted by Kar and Swenson (2019).
Figure 5: Behaviour of Jt −JT for different ρ and B = 1. The figures present DGC-KM , DGC-HL and
ρ ρ ρ ρ
DGC-LL , left to right.
ρ
35