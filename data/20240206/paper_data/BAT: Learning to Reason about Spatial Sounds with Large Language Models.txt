BAT: Learning to Reason about Spatial Sounds with Large Language Models
ZhishengZheng12 PuyuanPeng1 ZiyangMa2 XieChen2 EunsolChoi1 DavidHarwath1
Abstract understandingtaskssuchasvisualquestionansweringand
imagecaptioning(Lietal.,2023;Liuetal.,2023;OpenAI,
Spatialsoundreasoningisafundamentalhuman
2023;Pengetal.,2024). Otherworkhasintegratedsound
skill, enabling us to navigate and interpret our
perception ability into LLMs, enabling them to perform
surroundings based on sound. In this paper we
tasks such as audio question answering, speech recogni-
present BAT,whichcombinesthespatialsound
tion,speechtranslation,andspeechsynthesis(Deshmukh
perception ability of a binaural acoustic scene
etal.,2023;Wangetal.,2023a;Chuetal.,2023;Borsos
analysismodelwiththenaturallanguagereason-
et al., 2023; Gong et al., 2024). However, despite these
ingcapabilitiesofalargelanguagemodel(LLM)
LLM-basedmodels’abilitytohandlenumerousvisualand
toreplicatethisinnateability. Toaddressthelack
audio tasks with impressive performance, so far none of
ofexistingdatasetsofin-the-wildspatialsounds,
themcanhandlein-the-wildspatialaudioinput. Humans
wesynthesizedabinauralaudiodatasetusingAu-
areequippedwithbinauralhearingability,allowingusto
dioSet and SoundSpaces 2.0. Next, we devel-
notonlyidentifythetypeofsoundwearehearing,butalso
opedSPATIALSOUNDQA,aspatialsound-based
inferhowfarawaythesoundis,whatdirectionitiscom-
question-answering dataset, offering a range of
ingfrom,andwhethermultiplesoundsourcesatdifferent
QAtasksthattrainBATinvariousaspectsofspa-
locationsarepresentatonce. Considerscenariossuchas
tialsoundperceptionandreasoning. Theacoustic
locatingsomeonecallingfromupstairs,hearingyourphone
front end encoder of BAT is a novel spatial au-
ringingfrombehindasofa,orsensingfootstepsapproach-
dio encoder named Spatial Audio Spectrogram
ingfrombehind;thesesituationsalldemandspatialaudio
Transformer,or SPATIAL-AST,whichbyitself
perceptionandreasoning,acapabilitynotyetpresentinthe
achievesstrongperformanceacrosssoundevent
aforementionedmodels.
detection, spatial localization, and distance es-
timation. By integrating SPATIAL-AST with Tobridgethisgap,weintroduceBAT,thefirstspatialaudio-
LLaMA-2 7B model, BAT transcends standard basedLLMdesignedtoreasonaboutsoundsina3-Den-
SoundEventLocalizationandDetection(SELD) vironment. Recognizing the current lack of large-scale
tasks,enablingthemodeltoreasonabouttherela- datasetsforin-the-wildspatialaudioandspatialaudio-based
tionshipsbetweenthesoundsinitsenvironment. questionanswering,wesynthesizedalarge-scalebinaural
OurexperimentsdemonstrateBAT’ssuperiorper- audiodatasetusingAudioset(Gemmekeetal.,2017)clips
formance on both spatial sound perception and assoundsourcesandSoundspaces2.0(Chenetal.,2022)for
reasoning,showcasingtheimmensepotentialof simulatingdiverse,3-D,reverberantacousticenvironments.
LLMsinnavigatingandinterpretingcomplexspa- In conjunction with this dataset, we developed SPATIAL-
tialaudioenvironments. 1 SOUNDQA,adiversecollectionofquestionansweringtasks
designedtotrainandevaluatespatialsoundunderstanding
acrossvaryinglevelsofcomplexity.
1.Introduction
The successful training and evaluation of BAT hinge on
the ability to encode rich spatial audio information accu-
TheevolutionofLargeLanguageModels(LLMs)hasen-
rately. However,existingencodersoftenspecializeineither
abledtheirapplicationbeyondtextualdata,givingriseto
monauraleventdetection(Baadeetal.,2022;Huangetal.,
multimodallanguagemodelscapableofperformingimage
2022;Chenetal.,2023;2024b)orarelimitedtofirst-order
1Department of Computer Science, University of Texas at ambisonics(Shimadaetal.,2021;Wangetal.,2023b;Kim
Austin, USA 2Department of Computer Science and Engineer- etal.,2023),focusingonalimitedrangeofsoundevents.
ing,ShanghaiJiaoTongUniversity,China. Correspondenceto:
Otherapproachesaresolelydedicatedtodirection-of-arrival
ZhishengZheng<zszheng@utexas.edu>,DavidHarwath<har-
wath@utexas.edu>. (DoA)estimationinspecificmulti-channelformats(Yang
1Ourdataset,code,andmodelweightswillbereleasedupon etal.,2022a;Wangetal.,2023c),lackingthebreadthneeded
publication.
1
4202
beF
2
]SA.ssee[
1v19510.2042:viXraBAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
for comprehensive spatial audio understanding tasks. To 2022). Toaddressthesechallenges,researchershavedevel-
overcometheselimitations,wedevelopedSPATIAL-AST,a opedacousticsimulationtechniques(Scheibleretal.,2018;
novelmulti-taskspatialencoderthatisnotonlycapableof Chenetal.,2022)andalgorithmsthatleveragespatialau-
soundeventdetectionandspatiallocalizationbutalsoexcels dio(Yangetal.,2022b;Wangetal.,2023c).Existingspatial
indistanceperception,therebyprovidingacomprehensive audiodatasets,suchasYouTube-360(Morgadoetal.,2020),
solutionforspatialaudiotasks. YouTube-ASMR (Yang et al., 2020), Pano-AVQA (Yun
etal.,2021),andSTARSS23(Shimadaetal.,2023),offer
In our experiments, we found that for SPATIAL-AST is
varyinglevelsofspatialaudioinformation. However,they
both effective and parameter-efficient. Using multi-step
oftensufferfrominconsistentquality,lackcrucialground
training curriculum, the SPATIAL-AST encoder by itself
truthlabelslikesoundsourcedistanceanddirection, and
can achieve strong performance across multiple tasks: a
arelimitedinscope,hinderingthedevelopmentofspatial
meanAveragePrecision(mAP)of50.03%foraudioevent
audio-basedmachinelearningmodels.
classification,aMeanAngularError(MAE)of17.94°for
directionofarrivalestimation, andaDistanceErrorRate
2.2.SoundEventLocalizationandDetection
(DER)within0.5metersoftheactuallocationat32.54%for
distanceestimation. ByfusingtheSPATIAL-ASTencoder The Sound Event Localization and Detection (SELD)
totheLLaMA-2(Touvronetal.,2023b)andadoptingan task (Adavanne et al., 2018) merges sound source local-
appropriate training curriculum, our BAT not only excels ization with sound event detection (SED), a task which
intasksinvolvingperceptionbutalsodemonstratesspatial theDCASEcommunity2laterincorporatedintotheirchal-
reasoningabilitiesinscenarioswithmixedsoundsources lenges as Task 3. Subsequent works (Wang et al., 2020;
(e.g. “Isthestereosystemtotheleftofthebarkingdog?”), Zhang et al., 2021; Wang et al., 2023b) adapted architec-
achieving a 76.89% accuracy in answering spatial sound tures like GRU (Cho et al., 2014), TDNNF (Povey et al.,
reasoningquestions. 2018),andConformer(Gulatietal.,2020)forSELDtasks,
achievingstrongperformance. However,thesemodelshave
Ourkeycontributionsaresummarizedasfollows:
primarilyfocusedonshallowspatialaudioperception,and
thepotentialofleveraginglargelanguagemodelstoenable
• We present the first spatial audio-based question an-
spatialreasoningremainsunexploredinpriorwork,which
sweringdatasetSPATIALSOUNDQA,offeringarange
isthemainfocusofourwork.
of3-Daudiounderstandingtasksfromperceptionto
reasoning,whichprovidesaplatformfortrainingmod-
2.3.MultimodalLargeLanguageModels
elstoperformspatialsoundunderstandingtasks.
Recent models (OpenAI,2023; Li et al.,2023; Liu et al.,
• We propose SPATIAL-AST, a binaural spatial audio
2023;Pengetal.,2024)haveequippedLLMswiththeabil-
encoderarchitecturethatjointlyperformssoundevent
itytoreasonaboutandcreateimagesandvideos. Forthe
detection,spatiallocalization,anddistanceestimation
audiomodality,AudioGPT(Huangetal.,2023)integrates
thatachievesstrongperformanceacrossallthreetasks.
ChatGPTasaversatileinterfaceforawidearrayofaudio
• We introduce BAT, which integrates SPATIAL-AST and speech applications. Pengi (Deshmukh et al., 2023)
withtheLLaMA-2LLM,resultinginamodelcapable proposes the use of transfer learning to frame all audio
ofansweringcomplexreasoningquestionsaboutmul- tasksastext-generationchallenges,utilizingbothaudioand
tiplesoundsourcessituatedwithina3-Denvironment. textinputstoproduceresultswithoutadditionalfine-tuning.
LTU (Gong et al., 2024) proposes a monaural audio QA
dataset,andcombinesanAudioSpectrogramTransformer
2.RelatedWork
(AST)(Gongetal.,2021)featureextractorwithaLoRA-
adapted(Huetal.,2022)LLaMA(Touvronetal.,2023a)
2.1.SpatialAudio
LLM to train the model to reason and answer questions
Spatial audio is a family of techniques used to create the aboutthesoundsinaclip. Crucially,LTUdiffersfromour
illusionofsoundsourcesexistingwithina3-Dspacearound workinthatLTUistrainedonlyonmonauralsounddata,
thelistener. Encapsulatingtraditionalstereoandsurround whereasweconsidermulti-channelspatialsoundrendered
soundsystemstomorerecentmethodssuchasambisonicau- byareverberant,3-Denvironment.SALMONN(Tangetal.,
dio,spatialaudioisusedinapplicationsrangingfromvirtual 2024)usesOpenAI’sWhispermodel(Radfordetal.,2023)
reality(Mystakidis,2022)toadvancedtheatersystems. In andBEATs(Chenetal.,2023)asdualauditoryencoders
therealmofartificialintelligenceandmachinelearning,spa- toextractspeechandaudiorepresentations,andthencon-
tialaudiopresentsuniquechallengesforintelligentagents catenates them to LLMs to enable the generation of re-
situatedin3-Dphysicalspacesinaccuratelylocalizingand
2https://dcase.community/community_info
interpretingsoundsources(Eversetal.,2020;Guizzoetal.,
2BAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
Table1.SPATIALSOUNDQAOverview: Thefirstfourtypesfocusonperception,whilethelastemphasizesreasoning. DP:Distance
Prediction;DoA:Direction-of-Arrival.Numbers(e.g.,139K,15.9%)indicatetheQAsamplecountandtheirpercentagesinthedataset.
Type #SourcesExample
A:Detection Q:Identifythesoundeventsintheaudioclip./A:babylaughter;laughter;speech
1
(139K,15.9%) Q:Whatarethedistinctsoundspresentinthisaudioclip?/A:heartsounds,heartbeat
B:DP&DoA Q:Howwouldyoudescribethelocationofthisaudioclip?/A:right,front,below;2.5m
1
(139K,15.9%) Q:Atwhatdistanceandinwhichdirection,isthemusic’ssoundoriginating?/A:left,behind,below;5m
Q:Identifythesoundeventsintheaudioclipcomingfromtheright,front,below,approximately
C:Detection 3metersaway./A:slosh;speech
2
(118K,13.5%) Q:Whatsoundeventscanyoudetectintheaudiorecordingemanatingfromtheleft,behind,above,
roughly0.5metersaway?/A:music;musicalinstrument;steelpan
Q:Inwhichdirectionandhowfarawayisthesourceoftheheartsounds,heartbeat’ssound?
D:DP&DoA
2 A:left,behind,below;1m
(118K,13.5%)
Q:Whereisthesoundofthemusiccomingfrom?/A:left,behind,below;3m
Q:Whenmeasuringthedirectlinedistance,isthesoundproducedbywheezeclosertoyouthan
thesoundfrombirdflight,flappingwings?/A:No
Q:Isthesourceofbothexplosionandspeech’ssoundsfromyourleftside?/A:Yes
E:Reasoning Q:Isthesoundfromthemusiconthefrontandthesoundfromtherattleonthebehind?/A:No
2
(358K,41.2%) Q:Doesthesoundofelectricshaver,electricrazorappearonthebehindofthesoundofwaterfall?/A:Yes
Q:Canyouestimatethedistancefromthesoundofthespeechtothesoundofthedog?/A:1.5m
Q:Whatisthesoundontheabovesideofthesoundofthevibration?/A:croak;frog
Q:Couldyoudeterminewhetherthesinging’ssoundistotheleftorrightofthesteam’ssound?/A:left
sponses. Qwen-audio(Chuetal.,2023)focusessolelyon SpatialAudioSimulator. Weusethestate-of-the-artau-
trainingtheWhisper-basedaudioencoderusinglarge-scale diosimulator,SoundSpaces2.0(Chenetal.,2022). This
datasets, achieving proficiency in universal audio under- platformperformson-the-flygeometry-basedsoundrender-
standing across a diverse range of tasks and audio types. ing,enablingrealisticacousticreverberationwitharbitrary
However,despitetheirimpressiveperformanceintheaudio source-receiverlocations. Manysimulationparameterscan
domain, none of these models have the capability to per- be easily modified, such as the material properties of a
ceiveandreasoningaboutspatialaudiothatissituatedin room’s walls and the objects within the room, as well as
diverse,reverberant,andcomplex3-Denvironments. thegeometryofthereceiver’smicrophonearray. Thisen-
ablesustocurateadiverseandhighlyrealisticdatasetthat
also offers easy access to the ground-truth sound genera-
3. SPATIALSOUNDQA
tionparameters,suchasthespatiallocationandorientation
ThetrainingofBATrequiresaspatialaudio-basedquestion of each source within the environment. Specifically, we
answeringdataset. CurrentdatasetslikeSpatialVLM(Chen leverage Matterport3D (Chang et al., 2017) for our envi-
etal.,2024a)andPano-AVQA(Yunetal.,2021)arefocused ronmental meshes, which includes highly detailed mesh
onvisionandlackspatialaudioinformation(oraudioalto- renderingsof90completebuildings,eachfeaturinganav-
gether). Tofillthisvoid,weintroduceSPATIALSOUNDQA, erage of 24.5 rooms across 2.61 floors with an average
the first spatial audio-based question answering dataset, floorspaceof517.34m2. Foragivenarbitrarysourcelo-
which is designed to emphasize the complexities of the cations,amonauralsoundsourceAs,receiverlocationr,
spatialaudio,freefromvisualinfluences,andcaterstothe and the receiver’s heading direction θ in a specific mesh
uniquedemandsofspatialaudioperceptionandreasoning. environment,theaudiosignalAr receivedbyamicrophone
isgivenbytheconvolutionoftheroomimpulseresponse
3.1.SpatialAudioGeneration withthesoundsource,asshowninthefollowingequation:
Collectingandannotatingspatialaudiointherealworldis Ar (t)=R (t,s,r,θ)∗As(t) (1)
m m
atimeconsumingtaskthatismademorecomplexbyenvi-
ronmentalacousticvariabilityandlimitationsinrecording where t ∈ [1,T] represents the time sample index,
equipment. In order to efficiently create a dataset with a m ∈ [1,2,...,n] represent the microphone index and
largeamountofvariabilityacrossenvironmentsandtypes
R m(t,s,r,θ)istheimpulseresponsebetweenthesource
ofsoundsourcesthatisalsorichinground-truthmetadata andreceiver.
aboutthesesources, wehaveadoptedasimulation-based
Tominimizeperceptualdissonanceinauditorylocalization
approachforgeneratingspatialaudiodataforthedataset.
whenvisualcuesareabsent,weensurethatboththesound
3BAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
sourceandthereceiverarelocatedwithinthesameroom. SoundEventDetection(TypeA&C).Forthistask,the
We position the receiver at coordinates that allow for an questionis“Whatsoundeventscanyoudetectintheaudio
uprightstance. Asforthesoundsources,theyareplaced recording?”anditsparaphrases. Theanswermayconsist
atrandomlocationsthroughouttheroom. Intotal,21,131 of a sorted list of sound classes present in the audio clip,
reverberationsaregenerated,averagingapproximately9.58 arranged in alphabetical order. Question type A utilizes
reverberationsperroom. audiowith1soundsource,whileCuses2.
SoundSources. Previousspatialaudiodatasets(Yangetal., DirectionandDistanceEstimation(TypeB&D).This
2020;Shimadaetal.,2023)oftenencompassalimitedrange taskaimstoidentifythedirectionanddistanceofasound
ofaudioevents,typicallyrestrictedtocategorieslikemusic, source. Forestablishinggroundtruth,thethree-dimensional
speech,anddomesticsounds. Tobroadenourscopeandbet- space issegmentedinto eight regions. Eachregionis de-
terrepresentreal-worldacousticscenarios,wesamplefrom finedbyatuplerepresentingthedirectionalaxis(left/right,
AudioSet(Gemmekeetal.,2017)tospecifyourmonaural front/behind,above/below)withrespecttothereceiver. Ad-
soundsourceAs. AudioSetconsistsofroughly2million ditionally,distanceisquantifiedinincrementsof0.5meters,
10-secondin-the-wildYouTubeclipsusedforaudioclassifi- spanningarangefrom0to10meters. Atypicalquestion
cation,witheachclipweaklyannotatedwith527typesof mightbe,“Whereisthesoundofthemusiccomingfrom?”
audioevents, oftenfeaturingmultipleoverlappingevents accompaniedbyGPT-generatedparaphrasedversions. The
within a single clip. However, certain categories among formatforanswersisconstrainedtoexpressionslike“left,
these 527 types are discernible only through visual cues front,above;2.5m”. QuestiontypeButilizesaudiowith1
(e.g., “Single-lens reflex camera” vs. “Camera”). To en- soundsource,whileDuses2.
hancethedataset’sreliability,weexcludelabelsthatrequire
Spatial Reasoning (Type E). This task differs from the
visualinformationbymanualinspection. Additionally,to
previous perception-focused tasks by introducing scenar-
mitigatetheimpactofpoor-qualitytrainingsamples,were-
ioswheretwosoundeventsoccurconcurrently,originating
movelabelswithlessthan50%quality.3 Ultimately,given
from distinct distances and directions. When generating
thattheinputaudioundergoesconvolutionwithreverbera-
thesescenarios,weensurethateachmixcontainstwosound
tion,wefurtherexcludemostnoise-relatedlabels,suchas
sourceswithentirelydifferentsoundeventsandorientations.
“Noise”,“Echo”and“Outside,urbanormanmade”. This
Thechallengehereistodiscernthespatialdifferencesbe-
curationresultsinasetof355audioeventlabelsthatare
tweentheseoverlappingsounds. Forexample,aquestion
appropriateforoursetting,identifiablesolelythroughaudio
mightbe,“Whatisthesoundontheleftsideofthesound
cues. After filtering by these categories, we are left with
ofthedogbarking?”Thistaskdemandsbothperceptionand
1,861,750clipsintheAudioSet-2Msplit,whileourfiltered
complexreasoning. Themodelmustimplicitlyseparatethe
AudioSet-20Ksplitcontains18,373clips,andourevalua-
soundsourcesbasedontheiruniqueclasses,spatiallylocal-
tionsetconsistsof17,148clips. Lastly,tocounteractany
izeeachsource,andthenanalyzetherelationshipbetween
volumevariabilityinthesourceaudio,weappliedloudness
thesourcesinthecontextofthequestion. However,theen-
normalizationacrossallclips.
coderSPATIAL-ASTwasnotbeenexposedtosuchmixed
datatypesduringitspre-trainingstage,implyingalackof
3.2.Question-AnswerPairGeneration
audioseparationability. Thistaskisthusdesignedtotest
WecuratedSPATIALSOUNDQAtoincludediversequestion- themodel’sin-contextlearningcapabilitiesandeffectively
answerpairs,allcenteredaroundthechallengesofspatial utilizethelatentrepresentationsfromtheencodertoaddress
audio perception and reasoning. Similar to LTU (Gong thiscomplexmulti-sourceaudioreasoningchallenge. To
et al., 2024), each sample in the dataset is structured as makeevaluationstraightforward,wehavechosenabinary
atupleof(audio,question,answer),wheretheaudioand (Yes/No)responseformatforthesequestions. Byadopting
questionserveasinputstothemodel,andtheansweracts binaryanswers,wecanmoreeffectivelygaugethemodel’s
as the target label. Table 1 illustrates the array of tasks reasoningperformanceandmaintainconsistentassessment
in SPATIALSOUNDQA,rangingfrombasicperceptionto standardsacrossdifferentquestiontypes.
complex spatial reasoning. To enhance variety, the ques-
tionsinthesetasksareparaphrasedusingGPT-4,ensuringa 4.Method
broadspectrumofqueries. Theanswers,ontheotherhand,
are generated uniformly through a systematic rule-based Figure 1 illustrates our overall architecture for the
approach,maintainingconsistencyacrossthedataset. We BAT model. We will first introduce SPATIAL-AST, our
enumerateeachquestiontypeinthefollowingparagraphs. novelspatialaudioencoderthatactsasafrontendforBAT.
We then discuss how it is integrated with the LLaMA-2
3Forexample,theestimatedqualityofthe“clatter”labelis20%.
LLM in order to build spatial reasoning ability on top of
Source:https://research.google.com/audioset/dataset/clatter.html.
SPATIAL-AST’sauditoryperceptionability.
4BAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
excitingmusic… <EOS>
…
🔥
LLaMA 2 (Adapter v2)
Transformer Block 12x
CLS tokens Audio tokens
Projection 🔥
Patch-Embed
n music
Co sn tav
c
3 kx3
cos
CLS tokens Audio tokens
Embedding ❄
Reflectio
Direct
sou
Dnd
irect
sound
dog
sin
Spatial-AST❄ Receiver
IPD
What is the sound on the left side of
L R
the sound of the dog barking?
Mel-spectrogram Phase-spectrogram
Spatial Audio
Figure1. IllustrationoftheBATmodelstructure.Therightmostpartshowsthebinauralaudiogenerationprocess.
4.1.SpatialAudioEncoder: SPATIAL-AST tions,whicharethenweightedbymelWtoalignwiththe
dimensionoftheMel-spectrogram. Thefinalfront-endout-
WeproposeanovelarchitectureSPATIAL-ASTtocapture
put,Z,isaconcatenationoftheseprocessedcomponents.
spatialaudioinformation. Themodelisshownontheleft-
ItincludesboththeMel-Spectrogramsoftheleftandright
handsideofFigure1.
channels,aswellasthecosineandsinetransformationsof
theIPD,allcombinedasoutlinedinEquation5:
Front-End. Our encoder integrates both Mel-
Spectrogram and Interaural Phase Difference (IPD).
Asdetailedinthe SPATIAL-AST sectionofFigure1, we
Z =[S ;S ;cos(IPD)×melW;
initially transform the time-domain signal x(n) into the 1 2 (5)
frequency domain X(t,f) using the short-time Fourier sin(IPD)×melW], Z ∈R4×T×M
transform(STFT),asrepresentedbyEquation2:
Backbone. TheinitialinputZ isfirstpassedtoa3x32D
N−1
X i(t,f)= (cid:88) x i(n)w(n−t)e−j2π Nfn , i∈{1,2} (2) convolution followed by a batch normalization (Ioffe &
Szegedy,2015)layerandaGELU(Hendrycks&Gimpel,
n=0
2016) layer to fuse the inter-channel information. Subse-
Herew(n)denotesanN-pointwindowfunction,andiindi-
quently,weemployaPatch-EmbedCNNwithakernelsize
catestheleftorrightchannelinthebinauralrecording.
of (16×16) and corresponding strides in both time and
Subsequently,wecomputeboththeMel-Spectrogramand frequencydimensions,ensuringnon-overlappingsegmen-
the Interaural Phase Difference (IPD) based on these fre- tationofthefeaturesintodistinctpatchtokens. Toextract
quencydomainsignalsX (t,f). TheMel-Spectrogramfor spatialcues,weconcatenatethree[CLS]tokensatthebe-
i
eachchanneliscalculatedasperEquation3,andtheIPD ginningoftheaudiotokens,eachspecificallydesignatedfor
isderivedfromthephasespectrogramsoftheleftandright extractinginformationabouttheaudio’scategory,distance,
channels,asindicatedinEquation4: anddirectionrespectively. Alltokensarethenfedinto12-
S
(t,m)=log(cid:0)
|X
(t,f)|2×melW(cid:1)
(3)
layerTransformerencoderblocks. Finally,weprocessthe
i i three[CLS]tokensusingthreeseparatelinearlayerstoget
wheremelWisaM-binMelfilterbank. predictionsfortheircorrespondingtasks.
IPD(t,f)=∠X 2(t,f)
(4)
Pre-TrainingObjective. Thespatialencoderispre-trained
X (t,f) to handle three tasks, namely sound event detection, dis-
1
tanceprediction,anddirectionalprediction,withthelatter
To address the numerical instabilities due to phase beingfurtherdividedintoazimuthandelevationanglepre-
wraparoundinIPD,weapplycosineandsinetransforma- dictions. Weemploycross-entropylossforallthreetasks.
5BAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
Table2. Theperception-to-reasoningcurriculum.
asinterpretingspatialrelationshipsanddistancesbetween
Stage QestionType #Tr. Samples Percentages multiplesoundsources. Theimportanceofthiscurriculum
isevidencedinTable10inAppendixH.
1 A,B 278K 31.8%
2 A,B,C,D 514K 58.8%
3 A,B,C,D,E 872K 100% 5.Experiments
5.1.ImplementationDetails
Todiscretizepredictiontargetsfordistanceanddirectional
prediction,wecategorizedistanceintointervalsof0.5me- Input Audio Processing. The 10-second audio sources
tersandangleintointervalsof1degreeforbothazimuth arefirstloudnessnormalizedbyscalingthemsothateach
andelevation. clip has the same total sound energy. After convolution
with the room impulse response from SoundSpaces, we
Thefinalpre-trainingobjectiveofthespatialencoderis:
trim or pad these clips to 10 seconds to align with Au-
dioMAE(Huangetal.,2022). Theresultingwaveformsare
L=λ L +λ L +λ L (6) binauralwith2channelsata32kHzsamplingrate. Weuse
1 cls 2 dis 3 doa
awindowsizeof1024,ahopsizeof320,and128mel-bins
where L , L , L represent the losses for detection,
cls dis doa tocomputetheShort-TimeFourierTransforms(STFTs)and
distance, and direction respectively, while λ , λ , λ are
1 2 3 mel-spectrograms. Asaresult,fora10-secondrecording
hyperparameters.
fromAudioSet,theconcatenatedMel-spectrogramandIPD
featuredimensionis(4,1024,128).
4.2. BAT:AModelforReasoningaboutSpatialSounds
Spatial Audio Encoder. We implement a patch mask-
On its own, SPATIAL-AST is capable of predicting the
ing ratio of0.25 in both timeand frequency during train-
type,direction,anddistanceforaninputspatialsound. To
ing,andtheunmaskedtokensarethenconcatenatedwith
extend this ability to encompass spatial reasoning about
three learnable [CLS] tokens. We initialize the weights
multiplesoundsourcespresentwithinanenvironment,we
ofthetransformerblocksusingtheofficialpretrainedAu-
fuse SPATIAL-AST to the LLaMA-2 7B LLM (Touvron
dioMAE(Huangetal.,2022)checkpoint. Theencoderis
etal.,2023b). Inputspatialaudioisfirstprocessedbythe
trainedon8RTX3090GPUs,witheachepochtakingap-
SPATIAL-AST encoder, and then a projection module is
proximately10minutes. Recognizingthegreaterchallenge
usedtomapitssetofoutputtokensintoLLaMA-2’sinput
posedbysoundeventdetectioncomparedtodistanceand
textembeddingspace. Forefficientfine-tuning,weusethe
directionalprediction,wetrainourencoderintwostages.
LLaMA-adapterv2(Gaoetal.,2023).
In the first stage, we focus exclusively on detection loss,
Thefine-tuningobjectiveof BAT istopredicttheanswer settingtheweightsλ 2andλ 3tozeroasperEquation6. In
textconditionedonitspairedquestiontextandthecorre- thesecondstage,wereintroducethelossesfordistanceand
sponding audio input. The model seeks to maximize the direction,adjustingthelossweightstoλ 1 =1250,λ 2 =1,
probabilityofpredictingthenextanswertoken, applying andλ 3 =2tobroadenthecapabilitiesofthemodel. This
cross-entropylossforeachtokenposition1≤t≤T inthe two-stagetrainingapproachiscrucialforachievingabal-
sequence,whichcanbemathematicallyrepresentedas: anced performance across different tasks. The effects of
varying these weights on model performance for various
P (y |y ,··· ,y ;a ,a ,··· ,a ) (7) tasksaredetailedinAppendixE.
θ t t−1 1 l l−1 1
LLM.Divergingfromtheoriginaltwo-stagelearnablepa-
wherea isthel-thtokenofaudioembedding, andy de-
l t
rameters design of LLaMA-adapter v2, We concurrently
notesthet-thtokenoftextoutput.
trainparametersincludingzero-initializedattention,projec-
For the training of BAT, we devised a perception-to- tion,norm,bias,andscaletosimplifythetrainingprocess.
reasoningcurriculum,asdetailedinTable2. Theprocess Thetrainingiscompletedon8V100GPUs. Forgeneration,
beginswitha“warm-up”phasefocusingonsingle-source weemployagreedydecodingandsetthetemperatureto0.1
perception tasks (question types A and B), enabling the andnucleussampling(Holtzmanetal.,2020)toppto0.75.
model to adapt to audio modality inputs and infer basic
Baseline. For our encoder baseline comparisons, we se-
propertiesofasingleaudiosourceincludingitsclass,dis-
lectestablishedmodelssuchasAudioMAE(Huangetal.,
tance, and direction. In the second stage, we introduce
2022)andSELDnet(Adavanneetal.,2018). Toensurea
multi-sourcescenarios(questiontypesCandD),wherethe
faircomparison,SELDnet’sfeatureextractionnetworkis
modelistaughttoimplicitlyperformsoundsourcesepara-
augmentedwitha12-layertransformerblock,aligningthe
tionbyansweringquestionsaboutspecificsourceswithin
modelparametersofthecomparedarchitecturestoapproxi-
mixedaudio. Thefinalstageunlocksthefullcapabilities
ofBAT,integratingreasoningtasks(questiontypeE),such
mately90M.Additionally,forBAT,weuseaversionwith
6BAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
Table3.Thistablepresentsacomparativeanalysisofvariousmodelsusingbothmonauralandbinauraldata,focusingonkeyperformance
metrics:meanAveragePrecision(mAP,↑),ErrorRateat20°(ER ,↓),MeanAngularError(MAE,↓),andDistanceErrorRate(DER,
20°
↓).ResultsforAudioMAEmodelswerederivedthroughinferenceusingtheirofficialcheckpoints,withoutfurthertraining.Modelsthat
utilizeanechoicaudiodata(noreverberation)aregrayed-outforclarity.
Data PerformanceMetrics
AudioType InputFeatures mAP(%)↑ER (%)↓MAE(°)↓DER(%)↓
20°
AudioMAE(Huangetal.,2022) Anechoic Mel-spectrograms 53.56 - - -
AudioMAE(Huangetal.,2022) 47.18 - - -
Joint-Training 51.39 95.85 88.52 26.87
Monaural Mel-spectrograms
SPATIAL-AST(ours) Stage-1 52.26 - - -
Stage-2 50.69 95.35 89.42 28.25
SELDnet(Adavanneetal.,2018) Mel-spectrograms,IPD 42.66 25.19 19.21 38.46
SPATIAL-AST(ours) Binaural Mel-spectrograms 49.94 25.50 18.57 34.85
Joint-Training
SPATIAL-AST(ours) Mel-spectrograms,IPD 50.57 24.81 18.16 35.29
Stage-1 51.70 - - -
SPATIAL-AST(ours) Mel-spectrograms
Stage-2 50.86 30.94 21.66 28.60
Binaural
Stage-1 52.26 - - -
SPATIAL-AST(ours) Mel-spectrograms,IPD
Stage-2 50.03 23.89 17.94 32.54
monauralinputforcomparison,whichhasanarchitecture modifiedthe3x3convolutionmodule,settingitsinputchan-
similartoLTU(Gongetal.,2024),servingasabaseline. nelto1,andonlyfeedamonauralmelspectrogramasinput.
Despitebinauraldatatypicallyofferingricherspatialinfor-
EvaluationMetrics. Intheevaluationofourspatialaudio
mation,ourencoderdemonstratesrobustperformancewith
encoder, outlined in Table 3, we use the mean Average
monauralinputs,especiallyinsoundeventdetection(SED)
Precision(mAP)forsoundeventdetection; ErrorRateat
anddistanceprediction(DP)tasks. Theencoderachieves
20° (ER ), as referenced in (Mesaros et al., 2019), and
20° a mAP of 51.39 for monaural input, surpassing both its
MeanAngularError(MAE)forDirectionofArrival(DoA)
binauralcounterpart,whichscores49.94,andAudioMAE,
accuracy,evaluatingwhetherpredictionsdeviatemorethan
which achieves 47.18 in mAP. Additionally, our encoder
20°fromthereferencepointandquantifyingtheaverage
with monaural input even attains the best Distance Error
angulardeviationbetweenpredictionsandgroundtruth;and
Rate(DER)comparedtomodelsusingbinauralinput. How-
DistanceErrorRate(DER)formeasuringtheaccuracyof
ever,asmonauralaudiocannotcapturespatialinformation,
distancepredictionswithin0.5metersoftheactuallocation.
itshowslowperformanceinthedirection-of-arrival(DoA)
FortheevaluationofBAT,asshowninTable4,weapply
task, with an ER of 95.82 and an MAE of 88.52. The
similarmetrics,includingmAPandDER.FortheDoAtask, 20°
performanceofmonauraldatasuggestsitissufficientlyef-
wherethethree-dimensionalspaceissegmentedintoeight
fectiveforSEDandDPtasks.Theeffectivenessofmonaural
distinctsections,weuseaccuracy(Acc)astheperformance
inputcanbeattributedtothefactthatbinauralaudiodoes
metric. Additionally,forreasoningtasksinquestiontypeE,
notnecessarilyprovideadditionalbenefitsforsoundcate-
BinaryAccuracy(BA)servesastheevaluationmetric.
gory perception. The results from the single-task setting
detailedinAppendixDcorroboratethisconclusion.
5.2.PerformanceofSPATIAL-ASTonSELDTasks
Joint-training vs. Two-stage training. Joint-training
Table3presentstheexperimentalresultsforourproposed
emergesasthemoreeffectiveapproachformonauraldata,
SPATIAL-AST audio encoder. We compare SPATIAL-
outperformingtwo-stagetraininginbothdetectionanddis-
AST against current state-of-the-art models such as Au-
tancepredictiontasks,achievingamAPof51.39andDER
dioMAE (Huang et al., 2022) and SELDnet (Adavanne
of 26.87. However, for binaural data, two-stage training
et al., 2018). We also compare spatial cue extraction be-
consistentlydemonstratesimprovementsinitssecondstage,
tween monaural and binaural data. Additionally, we ex-
regardless of whether IPD is used. The enhancement is
ploretheimpactoftwotrainingmethodologies,including
particularlysignificantwiththeusageofIPD,achievingthe
joint-trainingandtwo-stageapproaches,andhighlightthe
lowestMAEof23.89andDERof17.94,demonstratingthe
importanceofIPD.
importanceofincludingmulti-microphonephaseinforma-
Monauralvs. Binaural. Tosupportmonauralinput, we tionforspatialaudioperception. Thisalsounderscoresthe
7BAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
Table4.PerformanceevaluationonSPATIALSOUNDQA.ThemetricsincludemAPfordetectionaccuracy,representedbytwocolumns
correspondingtoquestiontypesAandC,respectively;Accuracy(Acc)forassessingtheaccuracyofidentifyingdirectionsandDistance
ErrorRate(DER)fordistanceprediction,withtwocolumnseachforquestiontypesBandD;andBinaryAccuracy(BA)fortheaccuracy
ofbinary(Yes/No)responsesinreasoning(questiontypeE).Inputtypesincludemonauralaudio(M),binauralaudio(B),prompt(P),or
groundtruthsoundsourceparameters(GT).The“Random”and“Oracle”rowsserveasbaselineandidealperformancebenchmarks.
Reasoning(BA↑)
Detection Direction-of-Arrival Distance
Model Input
(mAP↑)* (Acc↑) (DER↓) Direction Distance Avg.
Random P 0.61|0.59 12.57|12.41 67.33|67.46 50.00 50.00 50.00
MonauralBAT(ours)1 M+P 24.15|6.42 14.31|11.93 34.17|56.26 57.69 51.36 54.53
One-stageBAT1 26.23|9.06 76.49|37.09 29.28|51.62 65.12 81.98 73.55
B+P
One-stageBAT1,2 25.50|5.67 72.74|35.63 31.51|73.68 59.59 49.76 54.68
P 0.69|0.65 12.88|13.14 51.65|71.73 58.28 50.67 54.48
BAT
B+P 26.34|9.89 75.54|37.65 29.16|47.90 69.77 84.01 76.89
Oracle1 GT+P 94.16|94.26 99.20|99.81 0.00|0.00 98.21 100.00 99.10
* LikeLTU(Gongetal.,2024),weuseatextencoder(text-embedding-3-small)toencodeBAT’soutputandthelabelsfromthe
evaluationdataset,thencomputethecosinesimilaritybetweenthesetextembeddingsforevaluation.mAPpotentiallyunderestimates
BAT’sperformance.
1 Onlytrainstage3. 2WeremovedquestiontypesCandDfromSPATIALSOUNDQA.
importanceofemployingatwo-stagetrainingapproachfor in training BAT to effectively handle complex reasoning,
enhancedperformanceinbinauralsettings. underscoringthenecessityofamulti-stagecurriculumthat
graduallyintroducesthemodeltoincreasinglysophisticated
ModelArchitecture. WhencomparedtoAudioMAEand
spatialaudioperceptionandreasoningtasks.
SELDnet, our encoder exhibits enhanced performance in
reverberant environments. As AudioMAE is tailored for The “Monaural BAT” row shows the performance of
monauralinput,wefocusourcomparisononmonauralsce- BAT trained with monaural audio. The architecture of
narios.Inthesecontexts,ourproposedmodelsurpassesboth monauralBATissimilartothatofLTU(Gongetal.,2024),
joint-trainingandtwo-stagetrainingmethods. Thisachieve- soweuseitasabaselineforcomparison. Theresultsre-
mentisespeciallysignificantgiventhatourmodelhasbeen vealthatmonauralBATfacessignificantchallengesinrea-
finetunedformuchfewertraininghours. Forsimplicity,we soning tasks, achieving a Binary Accuracy (BA) of only
compareourmodelwithSELDnetinjoint-training,where 54.53%. This suggests that monaural audio input alone
itsurpassesSELDnetonallmetrics. cannotprovidesufficientspatialinformationforcomplex
reasoningchallenges,highlightingtheusefulnessof SPA-
5.3.PerformanceofBATonSPATIALSOUNDQA TIALSOUNDQAforlearningspatialaudioreasoningcapa-
bilities. MoreablationscanbefoundinAppendixH.
Table 4 shows the performance of various configurations
of our BAT model on SPATIALSOUNDQA, broken down
6.Conclusion
acrossthedifferentquestiontypes. BATperformsstrongly
acrossallquestiontypes,outperformingtherandombase-
Inthiswork,wehavepresentedBAT,thefirstLLMcapa-
linebyaverylargemargin. Weobservethattheone-stage
bleofprocessingspatialaudioinput. Totrainandevaluate
BAT generally lags behind the three-stage BAT in perfor-
BAT,weintroducedSPATIALSOUNDQA,thefirstextensive
mance across various metrics. Intriguingly, we observed
spatial audio-based question answering dataset. We also
a significant drop in the model’s reasoning ability when
proposedSPATIAL-AST,anovelspatialaudioencoderca-
questiontypesCandD,relatedtosoundsourceseparation,
pableofefficientlyhandlingsoundeventdetection,spatial
wereremovedfromthetrainingset. Thisisevidentfromthe
localization, and distance estimation. BAT and SPATIAL-
BinaryAccuracy(BA)results,whichapproximaterandom
SOUNDQAshowcasetheimmensepotentialofLLMsfor
chance in the reasoning tasks. Analysis of the one-stage
reasoning about spatial sound. SPATIALSOUNDQA also
BAT’soutputsrevealsthatthemodelstrugglestodifferen-
enablesrichfuturework,suchasreasoningabouttheenvi-
tiatebetweensoundsourcesandtheirrespectivepositions
ronmentitself(materials,shape,layout),modelingmoving
when confronted with reasoning questions. This finding
soundstofortrackingproblems,orincorporatingthevisual
highlightstheimportanceofsoundsourceseparationtasks
modalitywhichSoundSpaces2.0nativelysupports.
8BAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
BroaderImpact References
Our research on BAT, the first spatial audio-based large Adavanne, S., Politis, A., Nikunen, J., and Virtanen, T.
languagemodel,alongsidetheSPATIALSOUNDQAdataset, Sound event localization and detection of overlapping
holdssignificantpotentialforbroadimpactacrossvarious sources using convolutional recurrent neural networks.
fields: Proc.JSTSP,2018.
Advancements in Spatial Audio: Our work addresses a Baade,A.,Peng,P.,andHarwath,D.F. Mae-ast: Masked
keygapinspatialaudioperceptionandreasoning. Byde- autoencodingaudiospectrogramtransformer. Proc.In-
velopinganLLMspecificallyforspatialaudio,weopenup terspeech,2022.
newpossibilitiesforapplicationswherespatialsoundcues
Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E.,
playacrucialrole,suchasvirtualreality,gaming,andaudio
Pietquin, O., et al. AudioLM: A language modeling
engineering. Thiscanleadtomoreimmersiveandrealistic
approachtoaudiogeneration. Proc.TASLP,2023.
experiencesinthesedomains.
Chang,A.,Dai,A.,Funkhouser,T.,Halber,M.,Niessner,
EnhancementsinMultimodalLargeLanguageModels:
M.,etal. Matterport3D:LearningfromRGB-Ddatain
The integration of spatial audio into LLMs represents a
indoorenvironments. Proc.3DV,2017.
significantsteptowardstrulymultimodalAIsystems. Our
model not only demonstrates the capability of LLMs in
Chen, B., Xu, Z., Kirmani, S., Ichter, B., Driess, D.,
processingcomplexspatialaudioinformationbutalsopaves
et al. SpatialVLM: Endowing vision-language mod-
thewayforfutureresearchincombiningauditoryandother
els with spatial reasoning capabilities. arXiv preprint
sensorymodalities,suchasvisualortactile,tocreateeven
arXiv:2401.12168,2024a.
moresophisticatedAImodels.
Chen,C.,Schissler,C.,Garg,S.,Kobernik,P.,Clegg,A.,
EmpoweringEmbodiedAI:Theabilitytointerpretand
etal. SoundSpaces2.0: Asimulationplatformforvisual-
reasonaboutspatialsoundscansignificantlyenhanceem-
acousticlearning. Proc.NeurIPS,2022.
bodied AI systems, like robots or autonomous vehicles.
Thesesystemscanutilizespatialaudiocuesforbetternav- Chen,S.,Wu,Y.,Wang,C.,Liu,S.,Tompkins,D.,Chen,
igationandinteractionwiththeirenvironment,leadingto Z.,andWei,F. BEATs: Audiopre-trainingwithacoustic
moreeffectiveandsaferapplicationsinreal-worldscenar- tokenizers. Proc.ICML,2023.
ios.
Chen,W.,Liang,Y.,Ma,Z.,Zheng,Z.,andChen,X. EAT:
Self-supervised pre-training with efficient audio trans-
former. arXivpreprintarXiv:2401.03497,2024b.
Cho,K.,VanMerrie¨nboer,B.,Bahdanau,D.,andBengio,Y.
Onthepropertiesofneuralmachinetranslation: Encoder-
decoderapproaches. Proc.SSST,2014.
Chu,Y.,Xu,J.,Zhou,X.,Yang,Q.,Zhang,S.,etal. Qwen-
Audio: Advancinguniversalaudiounderstandingviauni-
fiedlarge-scaleaudio-languagemodels. arXivpreprint
arXiv:2311.07919,2023.
Deshmukh,S.,Elizalde,B.,Singh,R.,andWang,H. Pengi:
Anaudiolanguagemodelforaudiotasks. Proc.NeurIPS,
2023.
Evers, C., Lo¨llmann, H. W., Mellmann, H., Schmidt, A.,
Barfuss, H., et al. The LOCATA challenge: Acoustic
sourcelocalizationandtracking. Proc.TASLP,2020.
Gao,P.,Han,J.,Zhang,R.,Lin,Z.,Geng,S.,etal. LLaMA-
Adapterv2: Parameter-efficientvisualinstructionmodel.
arXivpreprintarXiv:2304.15010,2023.
Gemmeke,J.F.,Ellis,D.P.W.,Freedman,D.,Jansen,A.,
Lawrence,W.,etal. AudioSet: Anontologyandhuman-
labeleddatasetforaudioevents. Proc.ICASSP,2017.
9BAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
Gong,Y.,Chung,Y.-A.,andGlass,J. AST:Audiospectro- Mesaros,A.,Adavanne,S.,Politis,A.,Heittola,T.,andVir-
gramtransformer. Proc.Interspeech,2021. tanen,T. Jointmeasurementoflocalizationanddetection
ofsoundevents. Proc.WASPAA,2019.
Gong,Y.,Luo,H.,Liu,A.H.,Karlinsky,L.,andGlass,J.
Listen,think,andunderstand. Proc.ICLR,2024. Morgado, P., Li, Y., and Nvasconcelos, N. Learning rep-
resentationsfromaudio-visualspatialalignment. Proc.
Guizzo,E.,Marinoni,C.,Pennese,M.,Ren,X.,Zheng,X., NeurIPS,2020.
etal. L3DAS22challenge: Learning3daudiosourcesin
arealofficeenvironment. Proc.ICASSP,2022. Mystakidis,S. Metaverse. Encyclopedia,2022.
OpenAI. GPT-4V(ision)systemcard. 2023.
Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y.,
etal. Conformer: Convolution-augmentedtransformer
Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., et al.
forspeechrecognition. Proc.Interspeech,2020.
Grounding multimodal large language models to the
world. Proc.ICLR,2024.
Hak, C. C., Wenmaekers, R. H., and Van Luxemburg, L.
Measuringroomimpulseresponses: Impactofthedecay
Povey,D.,Cheng,G.,Wang,Y.,Li,K.,Xu,H.,etal. Semi-
rangeonderivedroomacousticparameters. Proc.Acta
orthogonallow-rankmatrixfactorizationfordeepneural
Acustica,2012.
networks. Proc.Interspeech,2018.
Hendrycks,D.andGimpel,K. Gaussianerrorlinearunits Radford,A.,Kim,J.W.,Xu,T.,Brockman,G.,McLeavey,
(GELUs). arXivpreprintarXiv:1606.08415,2016. C.,andSutskever,I. Robustspeechrecognitionvialarge-
scaleweaksupervision. Proc.ICML,2023.
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.
Thecuriouscaseofneuraltextdegeneration. Proc.ICLR, Scheibler,R.,Bezzam,E.,andDokmanic,I. Pyroomacous-
2020. tics: Apythonpackageforaudioroomsimulationand
arrayprocessingalgorithms. Proc.ICASSP,2018.
Hu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,etal.
LoRA: Low-rank adaptation of large language models. Shimada,K.,Koyama,Y.,Takahashi,N.,Takahashi,S.,and
Proc.ICLR,2022. Mitsufuji,Y. ACCDOA:Activity-coupledcartesiandirec-
tionofarrivalrepresentationforsoundeventlocalization
Huang, P.-Y., Xu, H., Li, J., Baevski, A., Auli, M., et al.
anddetection. Proc.ICASSP,2021.
Maskedautoencodersthatlisten. Proc.NeurIPS,2022.
Shimada, K., Politis, A., Sudarsanam, P., Krause, D.,
Huang, R., Li, M., Yang, D., Shi, J., Chang, X., Uchida,K.,etal. STARSS23: Anaudio-visualdataset
etal. AudioGPT:Understandingandgeneratingspeech, of spatial recordings of real scenes with spatiotem-
music, sound, and talking head. arXiv preprint poral annotations of sound events. arXiv preprint
arXiv:2304.12995,2023. arXiv:2306.09126,2023.
Ioffe,S.andSzegedy,C. Batchnormalization:Accelerating Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., et al.
deepnetworktrainingbyreducinginternalcovariateshift. SALMONN:Towardsgenerichearingabilitiesforlarge
Proc.ICML,2015. languagemodels. Proc.ICLR,2024.
Kim, J. S., Park, H. J., Shin, W., and Han, S. W. AD- Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
YOLO: You look only once in training multiple sound M.-A.,etal. LLaMA:Openandefficientfoundationlan-
eventlocalizationanddetection. Proc.ICASSP,2023. guagemodels. arXivpreprintarXiv:2302.13971,2023a.
Li, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: Boot- Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
strappinglanguage-imagepre-trainingwithfrozenimage A.,etal. LLaMA2: Openfoundationandfine-tunedchat
encodersandlargelanguagemodels. Proc.ICML,2023. models. arXivpreprintarXiv:2307.09288,2023b.
Liu, H., Li, C., Wu, Q., andLee, Y.J. Visualinstruction Wang,J.,Du,Z.,Chen,Q.,Chu,Y.,etal.LauraGPT:Listen,
tuning. Proc.NeurIPS,2023. attend,understand,andregenerateaudiowithGPT.arXiv
preprintarXiv:2310.04673,2023a.
Loshchilov, I. and Hutter, F. SGDR: Stochastic gradient
descentwithwarmrestarts. Proc.ICLR,2017. Wang, Q., Wu, H., Jing, Z., Ma, F., Fang, Y., Wang, Y.,
Chen, T., Pan, J., Du, J., and Lee, C.-H. The USTC-
Loshchilov,I.andHutter,F. Decoupledweightdecayregu- Iflyteksystemforsoundeventlocalizationanddetection
larization. Proc.ICLR,2019. ofdcase2020challenge. Proc.DCASE2020,2020.
10BAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
Wang, Q., Jiang, Y., Cheng, S., Hu, M., Nian, Z., et al.
The nerc-slip system for sound event localization and
detectionofdcase2023challenge. Proc.DCASE2023,
2023b.
Wang, Y., Yang, B., and Li, X. FN-SSL: Full-band and
narrow-bandfusionforsoundsourcelocalization. Proc.
Interspeech,2023c.
Yang,B.,Ding,R.,Ban,Y.,Li,X.,andLiu,H. Enhancing
direct-path relative transfer function using deep neural
networkforrobustsoundsourcelocalization. Proc.TIT,
2022a.
Yang,B.,Liu,H.,andLi,X. SRP-DNN:Learningdirect-
pathphasedifferenceformultiplemovingsoundsource
localization. Proc.ICASSP,2022b.
Yang, K., Russell, B., and Salamon, J. Telling left from
right:Learningspatialcorrespondenceofsightandsound.
Proc.CVPR,2020.
Yun, H., Yu, Y., Yang, W., Lee, K., and Kim, G. Pano-
AVQA: Grounded audio-visual question answering on
360degvideos. Proc.ICCV,2021.
Zhang,Y.,Wang,S.,Li,Z.,Guo,K.,Chen,S.,andPang,
Y. Data augmentation and class-based ensembled cnn-
conformernetworksforsoundeventlocalizationandde-
tection. Proc.DCASE2021,2021.
11BAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
A.WhyDoWeUseLargeLanguageModels?
TheuseofLargeLanguageModels(LLMs)inourspatialaudioreasoningframeworkisnecessaryduetoseveralcritical
reasons. First, LLMs enable complex questions to be posed using natural language, allowing for a more flexible and
intuitive query format, such as “Is the dog further away from you than the stereo?” or “Is the dog to the left or to the
rightofthestereo?”Thiscapabilityeliminatestheneedforacascadeofseparateprocessingstepslikesourceseparation,
classification,DoA,distanceclassifiers,andrule-basedtemplates,whichwouldsignificantlylimitthetypesofquestionsthat
couldbeaddressed. Additionally,LLMsfacilitatethejointtrainingofthespatialaudioencoderandreasoningmoduleinan
end-to-endmanner,reducingtheriskoferrorpropagationinherentinsegmentedprocessingapproaches. Moreover,LLMs
offertheabilitytogeneralizeacrossdifferentquestionswithoverlappingexpressions,enhancingthemodel’sadaptability
andversatilityinrespondingtoawiderangeofspatialaudio-relatedqueries. Lastbutnotleast,thistaskalsointendsto
furtherexpandtheapplicationboundariesofLLMs,demonstratingtheirversatilityandeffectivenessindomainsbeyond
traditionaltextandimageprocessing.
B.Hyperparameters
WepresentthespecifictraininghyperparameterconfigurationsforSPATIAL-AST&BATinTable5.
Table5. TraininghyperparametersofSPATIAL-AST&BAT
SPATIAL-AST BAT
Configuration
Stage-1 Stage-2 Stage-1 Stage-2 Stage-3
SoundSource AudioSet-2M AudioSet-20K
AudioNormalization ✓ ✓
Augmentation ✓ ✗
Weightedsampling ✓ ✗
Optimizer AdamW(Loshchilov&Hutter,2019)
Optimizermomentum β =0.9,β =0.95
1 2
Weightdecay 0.0001 0.05
Baselearningrate 0.001 0.001
Learningrateschedule half-cyclecosinedecay(Loshchilov&Hutter,2017)
Warm-upepochs 10 5 2
Epochpartitioningfactor 10 10
Epochs 60 40 20 20 25
Batchsize 64 2
GPUs 8RTX3090 8TeslaV100
C.RoomAcousticsCharacteristics
Table6presentstheaverageRT60valuesacrosssomecommonroomtypesintheMatterport3D(Changetal.,2017)training
settings. RT60isastandardacousticmeasurementthatisdefinedasthetimeittakesforthesoundpressureleveltoreduce
by60dB(Haketal.,2012). Thismetricvariesacrossdifferentroomsduetofactorssuchasroomsize,objectarrangement,
andconstructionmaterials,eachinfluencingtheroom’sacousticcharacteristics.
Table6. Averagereverberationtime(RT60)fordifferentrooms.
LivingRoom Bedroom Closet Bathroom Toilet Garage Lounge Kitchen DiningRoom
RT60 0.2076 0.1917 0.1950 0.1906 0.1706 0.2190 0.2224 0.2061 0.2201
12BAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
D.Performanceof SPATIAL-AST inaSingle-TaskSetting
We evaluate the performance of our encoder in single-task scenarios, specifically focusing on classification, distance
prediction,anddirection-of-arrival(DoA).AsshowninTable7,weanalyzetheimpactofincludingoromittingInteraural
PhaseDifference(IPD),aswellascomparingmonauralversusbinauralformats,ontheperformanceofeachtask. Our
findings reveal that IPD generally enhances performance across all tasks, indicating the value of spatial information.
Moreover,weobservethatmonauraldataprovidessufficientinformationfortaskslikesoundeventdetectionanddistance
prediction,achievingthebestperformanceinDistancePredictionandcomparableresultstobinauraldatainSoundEvent
Detection. However,binauraldatasignificantlyimprovesperformanceintasksinvolvingdirectionperception,underscoring
theencoder’spotentialtohandlemultipletaskseffectivelyandtheimportanceofaudioformatinspatialaudioperception.
Table7.Performanceoftheencoderinsingle-taskscenarios: classification,distanceprediction,andDirection-of-Arrival(DoA).We
comparetheimpactofincludingorexcludingInterauralPhaseDifference(IPD)ontheperformanceofeachtask.Keyevaluationmetrics
includemeanAveragePrecision(mAP,↑),ErrorRateat20°(ER ,↓),MeanAngularError(MAE,↓),andDistanceErrorRate(DER,↓).
20°
Data PerformanceMetrics
Task
AudioFormat InputFeatures mAP(%)↑ ER (%)↓ MAE(°)↓ DER(%)↓
20°
Detection 52.13 - - -
Direction-of-Arrival Monaural Mel-spectrogram - 96.80 85.47 -
DistancePrediction - - - 21.44
Mel-spectrogram 51.70 - - -
Detection
Mel-spectrogram,IPD 52.26 - - -
Mel-spectrogram - 21.42 15.98 -
Direction-of-Arrival Binaural
Mel-spectrogram,IPD - 20.11 15.36 -
Mel-spectrogram - - - 25.55
DistancePrediction
Mel-spectrogram,IPD - - - 22.60
Generalist Binaural Mel-spectrogram,IPD 50.03 23.89 17.94 32.54
E.AblationsoftheSpatialAudioEncoder SPATIAL-AST
LossWeights. AsdetailedinEquation6,wefixλ =1andλ =2,andbyadjustingthevalueofλ ,wemonitorchanges
2 3 1
inthemodel’sperformance. AsdepictedinFigure2,withtheincreaseofλ ,themAPconsistentlyimproves. However,for
1
theMAEmetricindirection-of-arrival(DoA),theperformancegraduallydeteriorates. Thispresentsatrade-offbetweenthe
twometrics. Prioritizingclassification,weoptforsettingsthatyieldthehighestmAP.Insubsequentexperiments,wefixthe
hyperparametersatλ =1250,λ =1,λ =2.
1 2 3
10
50
12
40
30 14
20 16
10 18
0
20
0 200 400 600 800 1000 1200
1
Figure2. Classification(mAP)andDirection-of-Arrival(MAE)fordifferentλ values.
1
13
)%(
PAm
)°(
EAMBAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
3000
Table8.Ablation study on monaural audio input: We employ
2500
our proposedarchitecture totrain andinfer underfour distinct
2000 scenarios,withandwithoutaudionormalizationandreverberation.
1500 Normalization Reverberation mAP(%)↑ DER(%)↓
1000 ✗ ✗ 52.64 49.49
✓ ✗ 52.87 49.51
500
✗ ✓ 51.13 39.57
0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.510.0 ✓ ✓ 51.39 26.87
Distance (meters)
Figure3.Frequency distribution of training reverberations’ dis-
tancesrangingfrom0to10meters.
Audio Normalization and Reverberation. In this ablation study, we assess the effects of audio normalization and
reverberationontheencoder’sperformance,usingmonauralandfixingλ ,λ ,λ =1250,1,2forsimplicity,asoutlined
1 2 3
inTable8. Theadditionofreverberationindeedimpactsthemodel’sabilitytodetectsoundevents,asitmakesthetask
morechallengingwithoutaddingextraeventinformation. Acomparisonbetweenrows1and2withrows3and4indicates
thatremovingreverberationsignificantlyenhancesmeanAveragePrecision(mAP),increasingfrom51.13%to52.64%.
However,it’snoteworthythataudiowithoutreverberationlacksspatialinformation,leadingtoaDistanceErrorRate(DER)
fordistanceperceptionthatisnearlyequivalenttorandomguessing. BasedonthedistancedistributiondescribedinFigure3,
randomsamplingyieldsapproximately67%DER,andwhensamplingislimitedtospecificdistanceslike1.0,1.5,and
2.0 meters, it results in around 53% DER. The implementation of audio normalization further improves the encoder’s
performanceinbothclassificationanddistancetasks. Comparingrows3and4,themAPincreasesslightlyfrom51.13%to
51.39%,whileDERsignificantlydropsfrom39.57%to26.87%,underscoringthebenefitsofaudionormalizationinour
model.
F. SPATIALSOUNDQA Generation
Figure4. SoundeventsAandBoccurindifferentdirectionsanddistancessimultaneously.
IntheprocessofgeneratingSPATIALSOUNDQA,wesimulatespatialaudioscenarioswheremultiplesoundeventsoccur
simultaneously,eachatdistinctdirectionsanddistancesfromthereceiver. Forexample,asdepictedinFigure4,considera
situationwheretwosoundsources,AandB,areactiveconcurrently. SoundeventAoccursatanazimuthof115degrees
andanelevationof40degrees,situated2.5metersawayfromthereceiver. Incontrast,soundsourceBoccursatcoordinates
of-40degreesazimuthand-25degreeselevation,positioned6metersdistant. It’simportanttonotethatintheactualdataset
construction,thedirectionanddistanceparametersareoftennotasdistinctasinthisexample. Withthesegroundtruth
spatialparameters, wecanthenconstructadiversearrayofquestionsandanswersconcerningthespatialrelationships
14
)secnerruccO
fo
rebmuN(
ycneuqerFBAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
betweenthetwosoundsources,asshowninTable1.
InsupplementtoTable1,weprovidedetailsforeachquestiontype,includingtheinitialpromptusedandthecorresponding
answertemplate.
Table9. PromptsandanswertemplatesforSPATIALSOUNDQA
Type Prompt Answertemplate
A:Detection Identifythesoundeventsintheaudioclip. soundevent1;soundevent2
Whereistheaudioclipcomingfrom?
B:DP&DoA {d1},{d2},{d3};{distance}
Howwouldyoudescribethelocationofthe{soundevent}’ssound?
C:Detection Identifythesoundeventsintheaudioclipcomingfromthe{d1},{d2},{d3}, soundevent1;soundevent2
approximately{distance}away.
D:DP&DoA Whereisthesoundofthe{soundevent}comingfrom? {d1},{d2},{d3};{distance}
Dothesoundof{soundevent1}andthesoundof{soundevent2}bothcomefrom
your{d}side?
Canyouhearthe{soundevent1}’ssoundfromthe{d1}andthe{soundevent2}’s
fromthe{d2}?
E:Reasoning Yes/No
Wouldyoufindthesoundof{soundevent1}onthe{d}sideofthesoundof
{soundevent2}?
Intermsofstraight-linedistance,doesthesoundof{soundevent1}reachyou
fromacloserpointcomparedtothesoundof{soundevent2}?
G.PromptDetails
Forourspatialaudio-basedlargelanguagemodel BAT,wedesignedaspecificprompttoguideitsresponsestovarious
spatialaudiotasks. Thepromptstructureisasfollows:
"Based on the audio you’ve heard, refer to the instruction and provide a response.\n\n
### Instruction:\n{instruction}\n\n### Response:"
Toformtheinput,wereplace“{instruction}”withourspecificquestion,andthenconcatenateafixedlengthoflearnable
querytokensatthebeginning.
H.Ablationsof BAT
InTable10,wethoroughlyexaminetheinfluenceofdifferenttrainingcurriculumon BAT’sreasoningability. Initially,
themodelencountersdifficultieswithreasoningtasks,particularlywhenfacingunfamiliarreasoning-stylequestions,as
evidencedbyitsinabilitytoproduceaccuratebinary(Yes/No)responses. Aone-stageversionofBATsurprisinglyachievesa
respectableoverallBinaryAccuracy(BA)of73.55%inreasoningtasks. However,whenweremovequestiontypesCandD,
whicharerelatedtosoundsourceseparation,fromtheone-stagetraining,themodel’sreasoningabilitydropssignificantly,
exhibitingnear-randomperformancewithaBAofonly54.68%. Intriguingly,whenBATisfirsttrainedonsoundsource
separationtasksbeforeproceedingtothefinalreasoningstage,itretainsitsabilitytoreasoneffectively,evenintheabsence
ofseparation-relatedquestionsduringthefinalstage. ThelastrowofTable10showcasestheoptimalresultsachieved
through our three-stage curriculum, demonstrating the importance of a comprehensive training approach in equipping
BATwithrobustreasoningskills.
Table11demonstratesthatthecombinationofbothCLSandaudiotokensyieldsthebestperformanceinreasoningtasks.
However,itisnoteworthythatusingonlythreeCLStokensasarepresentationforspatialaudioalsoresultsinimpressive
performance. AsperTable12,alargerlearnablequerysizeintheprojectionmodulecorrelateswithimprovedreasoning
efficacy.
I.LimitationsandFutureWork
WeareconfidentthatBATwillsignificantlycontributetothedevelopmentofspatialaudioperceptionandreasoningaswell
asmultimodalLLM.However,aswithanyresearchtool,itiscrucialtoacknowledgecertainlimitationsandassumptions
15BAT:LearningtoReasonaboutSpatialSoundswithLargeLanguageModels
Table10. TheinfluenceofdifferenttrainingcurriculumonBAT’sreasoningability.
Stage Reasoning(BA↑)
Model #Sources
1 2 3 Direction Distance Avg.
Random - - - - 50 50 50
✓ ✗ ✗ 0 0 0
✓ ✓ ✗ 0 0 0
✗ ✗ ✓ 65.12 81.98 73.55
BAT 2 ✗ ✗ ✓* 59.59 49.76 54.68
✗ ✓ ✓* 64.69 79.16 71.93
✓ ✓ ✓* 69.56 83.51 76.54
✓ ✓ ✓ 69.77 84.01 76.89
* WeremovedquestiontypesCandDfromSPATIALSOUNDQA.
Table11.ImpactofdifferenttokensfromSPATIAL-ASTon
One-stageBAT’sreasoningability. Table12.Impactoflearnablequerysizeonreasoningaccuracy.
Reasoning(BA↑) Reasoning(BA↑)
Learnable
CLStokens AudioTokens
Direction Distance Avg. Query Direction Distance Avg.
✓ ✗ 60.32 72.92 66.62 32 61.42 83.49 72.45
✗ ✓ 61.70 78.95 70.33 64 69.77 84.01 76.89
✓ ✓ 65.12 81.98 73.55
inherentinourapproach.
Oneoftheprimarychallengesweencounterisextractingasmuchinformationaspossiblefromspatialaudio,withaspecific
focusonhowtobetterutilizephaseinformation. Currently,ourmodelhandlesamaximumoftwosoundsources,withan
emphasisprimarilyonaudio. Lookingahead,thereispotentialtoexpandintomulti-sourcescenariosandtointegrateboth
audioandspeechprocessingforamoreholisticapproach. Additionally,whileourcurrentframeworkislimitedtobinaural
audio,exploringambisonicscouldprovideamoreimmersiveandrealisticspatialaudioexperience. Moreover,expanding
SPATIALSOUNDQAtobemoreopen-endedwouldbetteralignwithhumanusagepatternsandpreferences,allowingfora
widerrangeofqueriesandresponses.
Anotherimportantconsiderationistheintegrationofadditionalmodalities, suchasvisualinformation, tocomplement
auditorycues. Thismultimodalapproachmightenhancethemodel’sunderstandingandinterpretationofcomplexenviron-
ments. Lastly,thesim2realgapremainsanaspectthatrequiresfurtherinvestigation. Observinghowourmodelperforms
inreal-worldscenarios,asopposedtosimulatedenvironments,willbecrucialinassessingitspracticalapplicabilityand
effectiveness.
16