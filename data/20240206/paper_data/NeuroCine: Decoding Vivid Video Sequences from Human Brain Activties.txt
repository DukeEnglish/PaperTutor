NeuroCine: DecodingVividVideoSequencesfromHumanBrainActivties
JingyuanSun**1 MingxiaoLi**1 ZijiaoChen2 Marie-FrancineMoens1
Abstract measures(Kupershmidtetal.,2022;Chenetal.,2023;Wang
etal.,2022).Successfullyachievingthiscouldnotonlyaid
In the pursuit to understand the intricacies of
peoplewithdisabilitiesbutalsodeepenourunderstandingof
humanbrain‚Äôsvisualprocessing,reconstructing
dynamicvisionprocessingmechanismsinthehumanbrain.
dynamicvisualexperiencesfrombrainactivities
emergesasachallengingyetfascinatingendeavor. Functional Magnetic Resonance Imaging (fMRI), noted
Whilerecentadvancementshaveachievedsuccess for its high spatial resolution, is pivotal in capturing
inreconstructingstaticimagesfromnon-invasive detailed activations within the visual cortex and related
brainrecordings, thedomainoftranslatingcon- brainareas(He¬¥naffetal.,2021).Thisaspectisparticularly
tinuousbrainactivitiesintovideoformatremains advantageous for reconstructing visual content from
underexplored. Inthiswork,weintroduceNeu- brain activity. While there has been significant progress
roCine,anoveldual-phaseframeworktotargeting in reconstructing static images (Chen et al., 2022; Sun
the inherent challenges of decoding fMRI data, etal.,2023a),videoreconstructionremainsanareaunder
suchasnoises,spatialredundancyandtemporal activeexploration. ChallengesarisefromfMRI‚Äôsinherent
lags. Thisframeworkproposesspatialmasking characteristics: it primarily measures changes in the
and temporal interpolation-based augmentation blood-oxygen-level-dependent (BOLD) signal, showing
forcontrastivelearningfMRIrepresentationsanda spatialredundancyinadjacentvoxels(UgÀòurbiletal.,2013).
diffusionmodelenhancedbydependentpriornoise Additionally,atemporallagexistsbetweenneuralactivity
forvideogeneration. Testedonapubliclyavail- andBOLDsignalsduetohemodynamicresponse,potentially
ablefMRIdataset,ourmethodshowspromising causingdiscrepanciesbetweenvisualstimuliandtheirfMRI
results,outperformingthepreviousstate-of-the-art representations(deZwartetal.,2009). Furthermore, the
modelsbyanotablemarginof20.97%,31.00% non-invasivenatureoffMRIintroducesnoisefromvarious
and 12.30% respectively on decoding the brain physiological and scanner-related sources (Parrish et al.,
activitiesofthreesubjectsinthefMRIdataset,as 2000),complicatinghigh-fidelityvideoreconstruction.
measured by SSIM. Additionally, our attention
Inresponsetothesechallenges, weproposeadual-phase
analysis suggests that the model aligns with
frameworknamelyNeuroCinetoreconstructhigh-resolution
existingbrainstructuresandfunctions,indicating
videos from fMRI data. Targeting the special spatial-
itsbiologicalplausibilityandinterpretability.
temporalcharacteristicsoffMRI,weproposetoaugment
fMRI with spatial masking and temporal interpolation in
the first phase. An fMRI encoder is optimized to learn
1.Introduction representationsrobustagainstthedisturbancescausedbythe
augmentation.Buildingonthis,thesecondphaseemploys
Human experiences are inherently dynamic, unfolding
thetrainedfMRIencodertoguideavideodiffusionmodelin
momentbymomentmuchlikeframesinamovie(Varela
generatingvideos.Weenhancethisphasebyincorporating
et al., 2017). The human brain, with its extraordinary
dependentpriornoisetoaddressfMRI‚Äôslowsignal-to-noise
complexity, continuously interprets these visual stimuli,
ratio. With these two key innovations, our double-phase
weavingarichtapestryofexperiences(Bartelsetal.,2008;
approach aims to effectively translate complex and noisy
Nishimoto et al., 2011). Unraveling the intricacies of
fMRIdataintoaccurateandmeaningfulvisualreconstruc-
this complex system to understand the neural activities
tions,demonstratingthesynergybetweenadvancedneural
beneathremainsaremarkablechallenge. Oneofthemost
imagingandmachinelearningindecodingbrainactivity.
ambitious endeavors in this field is the reconstruction of
thesedynamicvisualexperiencesusingnon-invasivebrain WeevaluateourmethodusingapubliclyavailablefMRI-
video pair dataset from three human subjects watching
*Equal contribution 1Department of Computer Science, videos.Ourresultssignificantlyoutperformpreviousmodels
KU Leuven, Leuven, Belgium 2School of Medicine, National inbothpixel-levelandsemantic-levelmetrics.Specifically,
UniversityofSingapore,Singapore.Correspondenceto:Jingyuan
ourmethodsurpassesthepreviousSOTAmodel(Chenetal.,
Sun<jingyuan.sun@kuleuven.be>.
2023) by a notable margin of 20.97% in decoding brain
activitiesofSubject1,31.00%inSubject2,and12.30%
1
4202
beF
2
]VC.sc[
1v09510.2042:viXraNeuroCine:DecodingVividVideoSequencesfromHumanBrainActivties
inSubject3.Additionally,attentionanalysisofourmodelre- offMRIwithsuchattention, theydidnottrainthemodel
vealscorrelationswiththevisualcortexandhighercognitive itselftoberobusttospatialandtemporaldisturbancesaswe
networks,indicatingbiologicalplausibilityandinterpretabil- do.Theyalsoonlyusedthenaivediffusionsetupwhilewe
ity.Thisunderlinesthepotentialofourmethodinadvancing introducedependentnoiseconsideringthenoisynatureof
thefieldofneuraldecodingandvisualreconstruction. fMRI.Methodsfrom(Kupershmidtetal.,2022;Chenetal.,
2023;Wangetal.,2022)willserveasbaselinesinthispaper
andbecomparedagainstourproposedmethod.
2.RelatedWorks
2.1.DecodingVisualContentsfromBrainActivities 2.2.DiffusionModels
ReconstructingImagesfromBrainActivities DiffusionModels(Hoetal.,2020),drawinginspirationfrom
nonequilibriumthermodynamics,areprobabilisticmodels
The advancement of deep generative models (Fang et al.,
that transform data into Gaussian noise and then back to
2021)hascatalyzedresearchinreconstructingvisualcontent
theoriginaldata, showcasingexceptionalperformancein
frombrainactivities.Thisareahaspredominantlyfocused
contentgenerationtaskssuchastexttoimage(Rombach
ondecodingbothviewedandimaginedimages. Initially,
et al., 2022), 3D object (Poole et al., 2022), and audio
manystudiesconvertedfMRIsignalsintoimagefeatures,
generation(Chenetal.,2020).Theiterativenatureofthese
whichwerethenusedbyfine-tunedGANstogenerateimages
models, requiring hundreds of steps for generation, has
(Mozafarietal.,2020;Seeligeretal.,2017). Forinstance,
been addressed by Song et al. (2020) with the denoising
(Shenetal.,2019)utilizedapre-trainedVGGnetworktode-
diffusionimplicitmodel(DDIM),reducingthenumberof
codefMRIdataintohierarchicalimagefeatures,whichwere
stepsforhigh-qualityoutputs.Enhancementslikeordinary
subsequentlyinputintoaGANforimagesynthesis.Morere-
differentialequationsolvers(Luetal.,2022a;b;Liuetal.,
centworksoverthepasttwoyearshaveshiftedtowardsusing
2022),varianceoptimization(Baoetal.,2022),exposurebias
DiffusionModels,whichhaveproducedimagesthataremore
reduction(Lietal.,2024;Ningetal.,2024;2023),andim-
semanticallycoherentandvisuallyrealistic(Qianetal.,2023;
provednoiseschedulers(Nichol&Dhariwal,2021)further
Linetal.,2022;Chenetal.,2022;Sunetal.,2023a). For
accelerateinferenceandrefinegenerativecapabilities.
example,(Sunetal.,2023b)enhancedfMRIrepresentation
learningwithdenoisingtechniquesandintegratedpixel-level
2.3.VideoGenerationwithDiffusionModels
guidancefromimageauto-encoders,effectivelyseparating
vision-relatedneuralactivitiesfrombackgroundnoise.
TheinitialadvancementsofusingDiffusionModelstogen-
Reconstructing Videos from Brain Activities Though eratevideosareprimarilyattributedtotheworkofVDM(Ho
studies reconstructing static images from brain activities etal.,2022b)whichintroducedthe3DdiffusionUNet. A
keep emerging, decoding videos remains a challenging subsequentdevelopmentbyHoetal.(2022a)usedacascaded
task under exploration. Conventional methods for video samplingframeworkwithasuper-resolutionmethodtogener-
reconstructionfromfMRIdatatypicallyapproachedthetask atehigh-resolutionvideos.Furthercontributionsinthisrealm
asasequenceofseparateimagereconstructions.Asnotedby includetheintroductionofatemporalattentionmechanism
(Wenetal.,2018),thiscouldleadtolowerframeratesanda overframesbySingeretal.(2022)inMake-A-Video.Mag-
lackofconsistencyacrossframes.Despitetheselimitations, icVideobyZhouetal.(2022)and(Heetal.,2022)LVDMin-
theirresearchdemonstratedthefeasibilityofdecodingboth corporatethismechanismwithinthelatentDiffusionModels
low-levelimagefeaturesandcategoricalinformationfrom togeneratevideos.Inthiswork,weadapttheimagediffusion
fMRIdatainresponsetovideostimuli. Buildingonthis, modeltovideodiffusionmodelbyinsertingatemporallayer
(Wangetal.,2022)employedalinearlayertoencodefMRI aftereachspatiallayerandincorporatingdependentnoises.
representations and utilized a conditional video GAN to
enhancethequalityandframerateofthegeneratedvideos. 3.Method
Nonetheless,theeffectivenessofthismethodislimitedbya
limiteddataset,whichisasignificantchallengeconsidering Our method consists of fMRI feature leaning and video
thesubstantialdatademandsofGANtraining. decodingtwo-phaseframeworkforreconstructingvideos
from fMRI-recorded brain activities. Phase 1 involves
Similartowhat(Beliyetal.,2019)haddone,(Kupershmidt
tuningapre-trainedfMRIencoderwithspatialandtemporal
etal.,2022)appliedaseparableautoencoderconduciveto
augmented contrastive learning to align fMRI data with
self-supervisedlearning,managingtosurpasstheoutcomes
CLIP‚Äôstextandimagefeatures, enhancingtheextraction
of(Wangetal.,2022). However,thevisualqualityandse-
ofsemanticinformationfromfMRIsignals. Phase2uses
manticaccuracyoftheresultingvideosremainedsuboptimal.
thetrainedfMRIencodertoguideavideodiffusionmodel,
Advancingthefield,(Chenetal.,2023)introducedtheuse
incorporating dependent prior noise to compensate for
ofcontrastivelearningalongsidespatial-temporalattention
fMRI‚Äôslowsignal-to-noiseratio.
mechanisms to effectively learn the fMRI representation.
Though they also targeted the spatial-temporal features InSection3.1,wedelveintolearningfMRIrepresentation
withspatialandtemporalaugmentedcontrastivelearning.In
2NeuroCine:DecodingVividVideoSequencesfromHumanBrainActivties
[a] [b]
‚Ä¶ ùë£ *‚Äô()
‚Ä¶ Temporal ùêø ‚Äô()
3D to 1D Interpolation
Patchify
Tokenize ùë£ fMRI Encoder
*
E f ùêø +,-
‚Ä¶
Spatial ùêø
A dog is being petted Masking *+,
CLIP model ùë£*+,
*
Original Vectors
‚Ä¶
Shared Noise ùúñ! Dependnet Noise ùúñ"
Diffusion
UNet copy
VAE
DecoderD
Spatial Masked Vectors G
‚Ä¶ Generated Video ùëß" ùëß% ùëß%&$ ùëß!#$ ùëß! ‚®Ç ùõΩ ‚äï1‚àíùõΩ‚®Ç
‚®Ç
f EM ncR oI
d er ùê∏
‚®Çùõº%#
‚äï1‚àíùõº%#
! Cross Attention
Temporal Interpolated Vectors [c] [d] ùëß" ùëß%
Figure1: Theproposeddouble-phaseframeworktoreconstructseenvideosfromfMRI.[a]: Usingspatialmaskingand
temporalinterpolationtoproduceaugmentedsamples[b]:Phase1trainsanfMRIencodertomapfromfMRItoCLIPtext
andimageembeddingswithcontrastivelearning. [c]: Phase2conditionsgenerationofadiffusionmodelwithPhase1‚Äôs
fMRIencoderincorporatingdependentnoises.[d]:Dependentnoisegeneration.
Section3.2,weelaborateonthedesignofpriornoiseforthe viewfMRIdataasasequenceof3Dtensors,butduetothe
videodiffusionmodeltodecodemorecoherentvideosfrom limitedscaleoffMRI-videopairs,weavoidcomplex,heavily
brainactivity.InSection3.3,wedescribeourexperimental parameterizedmodelsforcapturingthe3DstructureoffMRI.
approach for analyzing our results, aiming to clarify the Consideringpreviousworkwithsatisfactorydecodingper-
contributionofeachbrainregionthroughoutdifferentstages formance(Sunetal.,2023b;Chenetal.,2022),thefMRIdata
oflearning. fromthevisualcortexisrearrangedfrom3Dto1D,aligned
withthevisualprocessinghierarchy,segmentedintouniform
3.1.FMRIFeatureLearning patches,andthenconvertedintotokens.Traditionalmethods
of neural decoding often oversimplify the translation of
3.1.1.PRE-TRAININGANDFMRIINPUTFORMAT fMRIdataintovideoframesbymappingfMRIframesto
afixednumberofframes,creatingan‚ÄúfMRIframewindow‚Äù.
Pre-training
However,thisapproachdoesnotadequatelyaddressthetem-
ThescarcityoffMRI-videopairdatacompelsustoleverage poraldelayinfMRIdata.Toovercomethis,weimplementa
apre-trainedfMRIrepresentationspace,astrategywidelyac- slidingwindow,definedasv ={v ,v ,...,v },with
t t t+1 t+w‚àí1
knowledgedbypriorresearch(Sunetal.,2023b;Chenetal., v ‚ààRn√óp√óbrepresentingtokenembeddingsattimet,where
t
2023;2022;Sunetal.,2023a). Weutilizeamodelshared n, p, and b denote batch size, patch size, and embedding
bySunetal.(2023b), whichhasbeeneffectivelyapplied dimension. Thisresultsinv ‚ààRn√ów√óp√ób,wherewisthe
t
in image reconstruction from fMRI. This model benefits windowsize.Wethenapplyouralgorithmstothesewindows,
fromadouble-contrastivemaskedautoencoding(DC-MAE) considering the unique spatiotemporal characteristics of
technique,optimizedontheHCPdataset(VanEssenetal., fMRI,aselaboratedinsubsequentsubsections.
2013).ItincorporatesaVisionTransformer-basedencoder
toprocessmaskedfMRIsignalsandadecoderdedicated 3.1.2.SPATIALANDTEMPORALAUGMENTATION
to restoring the original, unmasked signals. The term
‚ÄúDouble-Contrastive‚Äù underlines the model‚Äôs strategy of To address the challenges of limited fMRI-video pair
employingtwodistinctcontrastingoperationstooptimize availabilityandthelowsignal-to-noiseratioinfMRIdata,
contrastive losses during fMRI representation learning. weproposeanoise-robustfMRIencodertrainingmethod.
We adopt this fMRI encoder to establish a pre-trained Akeyaspectofourmethodisthenovelcognitiveplausible
representationspace.Forfurtherdetailsonthismethodology, augmentationofsamplesforcontrastivelearning,adapted
werecommendconsultingtheoriginalpublication. to the unique spatial and temporal features of fMRI data.
Traditionalcomputervisionaugmentationtechniqueslike
FMRIInputFormatInourvideodecodingapproach,we croppingandrotationarenotideallysuitableforfMRIdata.
3NeuroCine:DecodingVividVideoSequencesfromHumanBrainActivties
FMRIimagescapturebrainactivitywithspatialspecificity embeddings. Weaimtooptimizetheselossesjointly,with
tiedtoneurologicalfunction.Rotatingorcroppingtheseim- theoveralllossfunctionbeingdefinedas:
agescoulddisrupttheinherentfunctionaltopology.Aftera L =¬µ L +¬µ L +L (3)
Ef spa spa tem tem emb
thoroughliteraturereview(Glover,2011;Buxton,2009)and Inthis equation, ¬µ and ¬µ arehyperparametersthat
spa tem
expertconsultations,weemploytwoeffectiveaugmentation adjusttheweightofthecorrespondinglosses. Thesetting
methods:spatialmaskingandtemporalinterpolation. andeffectsof¬µ and¬µ willbedetailedinSection5.2.
spa tem
For spatial masking, we randomly select a portion of the
tokensandsetthemtozero. Formally,inv ‚ààRn√ów√óp√ób, 3.2.GenerationwithDiffusionModel
t
wesetŒ≥ bvaluesinthefourthdimensionofshapebtozero,
spa 3.2.1.PRELIMIARIES
withŒ≥ asahyperparameter.Thepositionstobemasked
spa
areconsistentwithinthesamewindow.Butwedonotforce Diffusion Models (Sohl-Dickstein et al., 2015) show
themaskedpositionstobeuniformacrossthesamebatch. significantpotentialingeneratingbothimagesandvideos.
In this work, we adopt the widely used Stable Diffusion
Temporalinterpolationreplacesrandomlyselectedframes
(SD)(Rombachetal.,2022)asthebaselinemodel,knownfor
within a window with interpolations of other frames,
itsefficientdenoisingcapabilitiesintheimage‚Äôslatentspace,
weightedbytheirtemporaldistance‚Äîthefarthertheframe,
whichrequiresconsiderablyfewercomputationalresources.
theloweritsweight.Mathematically,forawindowwithw
Duringtraining,theSDbeginsbyusingaKL-VAE(Kingma
fMRIframesv ,andtheselectedithframev ,thejittered
t ti &Welling,2013)encodertoconvertimagex tolatentspace:
framevÀÜ iscalculatedas: 0
ti z = E(x ). It then progressively transforms this latent
n (cid:18) (cid:19) 0 0
(cid:88) |i‚àíj| representationintoaGaussiannoise,followingtheequation:
vÀÜ = 1‚àí v (1)
ti n tj
j=1,jÃ∏=i
‚àö ‚àö
The original frame v ti will then be replaced by vÀÜ ti. The z t= Œ±¬Ø tz 0+ 1‚àíŒ±¬Ø tœµ (4)
extentofinterpolationiscontrolledbythetemporalinterpola-
tionratioŒ≥ ,ahyperparameter.Thesettingandeffectsof Here, the œµ represents a noise sampled from a normal
tem
Œ≥ temandŒ≥ spawillbedetailedinSection5.2AblationStudy. distribution:œµ‚àºN(0,1).Œ±¬Ø tisthepredefinednoiseschedule.
Themodelistrainedtopredicttheaddednoiseateachstep,
3.1.3.CONTRASTIVEMAPPING andthelossfunctioncouldbeformulatedas:
Inourmethod,weemployavision-Transformer-basedfMRI
encodertoprocessfMRItokenvectors,aligningthemwith Ls timple=E t,x0,œµt‚àºN(0,1)[‚à•œµ t‚àíœµ Œ∏(z t,t,c)‚à•2 2] (5)
theCLIPmodel‚Äôstextandimageembeddings.Thisprocess
tisthediffusiontimestep,andcisthetextpromptcondition.
is enhanced by incorporating contrastive learning using
Duringinference,theSDgraduallyreconstructsanimage
augmentedexamplesasdescribedintheprevioussubsection.
alignedwiththeprovidedtextpromptfromGaussiannoise.
Initially, we utilize the pre-trained CLIP model (Radford Thedenoisedresultsarethenprocessedthroughthedecoder
etal.,2021)toencodethestimuliusedinthecollectionof oftheKL-VAEtoreconstructthecoloredimagesfromtheir
fMRIdata.Foreachvideo,captionsaregeneratedusingthe latentrepresentation:x =D(z ).
0 0
pre-trainedBLIPmodel(Lietal.,2022)andthenencoded
withtheCLIPmodeltoproducetextembeddings.Similarly, 3.2.2.DEPENDENTPRIORVIDEODIFFUSION.
we process each video frame to generate corresponding
Following previous studies (Wu et al., 2023; Chen et al.,
imageembeddings.ThefMRIencoderisfedwiththetoken
2023),weutilizeapre-trainedtext-to-imageStableDiffusion
vectorsandtrainedtomapfromfMRItoCLIPembeddings.
(SD) model as our foundational video generator. While
Additionally,thefMRIencoderisfedwithboththespatially
adeptatcreatinghigh-qualityindividualframes,theoriginal
andtemporallyaugmentedexamplesandisoptimizedwith
SDmodellackstemporalcoherenceforvideogeneration.To
contrastivelossestolearnfMRIfeaturesrobusttothespatial
addressthis,wemodifyitbyconverting2Dconvolutionsto
andtemporaldisturbances.
pseudo3Dandaddingatemporalattentionlayeraftereach
Theformalrepresentationofourlossfunctions,considering spatial self-attention layer. This modification introduces
theoriginalfMRItokenvectorsv t,theirspatiallyaugmented temporalawareness,allowingeachvisualtokentoattendto
versionvspa, andtemporallyaugmentedversionvtem, is tokensfromtheprevioustwoframes.Thetemporalattention
t t
asfollows: layeroperatesas:
L spa=L CE[E f(v t),E f(vt tem)] QKT
L tem=L CE[E f(v t),E f(vs tpa)] (2) Attention(Q,K,V)=Softmax( ‚àö d
k
)V (6)
L =L [E (v ),etxt]+L [E (v ),eimg],
emb CE f t t CE f t t withQ,K,V beingthequery,key,andvaluematrices,and
WQ,WK,WV aslearnableparameters.
where L is the cross-entropy loss and E denotes the
CE f
fMRIencoder.etxtandeimgmeantheCLIPtextandimage Fordecodingbrainactivityintovideo,westartbysampling
t t
4NeuroCine:DecodingVividVideoSequencesfromHumanBrainActivties
m latent codes from Gaussian noise and progressively 4.ExperimentalSetup
refinethemusingthefMRIrepresentation. Giventhelow
4.1.Dataset
signal-to-noise ratio in fMRI signals, enhancing video
qualityischallenging. Previousresearch(Geetal.,2023) Our study uses a publicly accessible fMRI-video dataset,
demonstrates that employing a deterministic ODE solver citedasdataset(Wenetal.,2018),whichincludesbothfMRI
inthegenerativeprocessoftheSDmodelresultsinahigh andvideoclipdata. ThefMRIdatawerecapturedusinga
correlationofinitialnoiseinframesfromthesamevideo. 3TMRIscannerwitha2-secondrepetitiontime(TR)and
Similarly, it has been observed that fMRI signals from involvedthreeparticipants.Thedatasetcomprisesatraining
similarvisualstimuliexhibitahighdegreeofcorrelation. setof18videoclips,each8minuteslong,totaling2.4hours
Based on these observations, we utilize correlated noise andproviding4,320pairedtrainingexamples. Thetesting
asaformofpriorknowledgewithinthegenerativemodel setincludes5sectionsof8-minutevideoclips,totaling40
andthefMRIdecodingprocess. Tocreateasequenceof minutes,resultingin1,200testfMRIscans. Thesevideos,
dependentnoise,whereeachnoiseissampledfromGaussian displayed at 30 FPS, feature diverse content including
Distributionwithameanofzeroandavarianceofone,we animals,humans,andnaturallandscapes.
divideeachnoiseintotwocomponents: œµ andœµj,andthe
s i
dependentnoiseisobtainedbyfollowingformula:
4.2.EvaluationMetric
Our evaluation metrics are divided into pixel-level and
œµj=(cid:112) Œ≤¬∑œµ +(cid:112) 1‚àíŒ≤¬∑œµj (7) semantics-level assessments. For pixel-level, we use the
s i StructuralSimilarityIndexMeasure(SSIM)(Wangetal.,
2004). For semantics-level, we employ an N-way top-K
‚àö accuracyclassificationtest. BothSSIMandclassification
whereœµ s‚àºN(0,1)andœµ i‚àºN(0,1). Œ≤isthehyperparame- accuracyarecomputedforeachframeagainstitsgroundtruth
terconditioningthenoiseratio,whosesettingandeffectsare frame.TheclassificationtestinvolvesanImageNetclassifier
discussedinSection5.2‚ÄôsAblationStudy. Avisualization comparing the classifications of the groundtruth and
ofgeneratingdependentnoiseispresentedinFigure1[d]. predictedframes.Successisdefinedwhenthegroundtruth
Duringtraining,wesubstitutetheoriginalnoiseinSDmodel classrankswithinthetop-Kprobabilitiesofthepredicted
withourcustomizeddependentnoisetogeneratenoisylatent frame‚Äôs classification from N randomly chosen classes,
codesateachtimestep.Conversely,inthegenerativephase, includingthegroundtruthitself. Thistestisrepeated100
theprocessbeginswiththeintroductionofourdependent timestocalculateanaveragesuccessrate.
noise.
Forvideo-basedmetrics,asimilarclassificationapproach
is used but with a video classifier. This classifier, based
3.3.InterpretationofBrainActivity
on VideoMAE (Tong et al., 2022) and trained on the
Understandingtheactivatedbrainregionsisanothersignif- Kinetics-400 dataset (Kay et al., 2017), assesses video
icantobjectiveinbrainencodinganddecodingwork.Inthis semanticsacross400categories,includingvariousmotions
study,weanalyzetheresultsusingtheself-attentionmecha- andhumaninteractions.
nismwithinthefMRIencoderforeachfMRI-videopair.The
attentionmapsarevisualizedatvariousstagesofmodellearn- 4.3.ImplementationDetails
ing,acrossdifferentlayers,andinresponsetodifferentvisual
ForafaircomparisonwithpreviousSOTA(Chenetal.,2023),
stimuli.Thisapproachhelpsinelucidatingthecontribution
theoriginalvideosinourstudyarereducedinframeratefrom
ofeachbrainregionacrossvariouslearningstages.
30framespersecond(FPS)to3FPSandthewindowsizeis
Initially, a sample-level interpretation is conducted. For setas2.ThisresultsinaconversionwhereeachfMRIframe
eachfMRI-videosamplepair,weextractself-attentionmaps correspondstosixvideoframes.Inourpracticalapplication,
fromthreekeylayersinthefMRIencoder: theearlylayer thisapproachenablesthereconstructionofa2-secondvideo
(thefirstlayer),themiddlelayer(the12thlayer),andthe segment(consistingofsixframes)fromasinglefMRIframe.
finallayer(the24thlayer). Theseattentionmapsarethen Our method is capable of generating the reconstruction
averagedacrossallvideosamplestofacilitateagroup-level of longer video sequences from multiple fMRI frames,
analysis. To correlate the attention values with specific contingentontheavailabilityofadditionalGPUmemory.
brainregions,wereverse-engineerthepatchifiedfMRIdata
For the first phase, our implementation utilizes a Vision
back into fMRI voxels using the inverse computation of
Transformer(ViT)-basedfMRIencoder.WeemployafMRI
theFClayer.Subsequently,thesefMRIvoxelsaremapped
encoder pre-trained on large-scale public available fMRI
ontotheselectedRegionsofInterest(ROIs)ontheCIFITI
databypreviouswork(Sunetal.,2023b). Thisencoderis
den-91ksurface,asreferencedin(VanEssenetal.,2013).
characterizedbythefollowingparameters: apatchsizeof
Forvisualizationpurposes,weemployNilearn(Abraham
16, adepthof24layers, andanembeddingdimensionof
etal.,2014)toplotthebrainvoxelsbacktothebrainsurface
1024. Initially,theencoderundergoespre-trainingwitha
toenhanceourunderstandingofcomplexbrainactivities.
5NeuroCine:DecodingVividVideoSequencesfromHumanBrainActivties
maskratioof0.75. Subsequently,itisenhancedwiththe 5.Results
additionofaprojectionhead. Thisprojectionheadserves
5.1.VideoReconstructionPerformance
totransformthetokenembeddingsintoadimensionalityof
77√ó768. Settingsofhyperparametersrelatedwithspatial
andtemporalaugmentationarediscussedinSection5.2.
GT
Inthesecondphase,ourapproachinvolvesleveragingStable
DiffusionV1-5(Rombachetal.,2022).Althoughtheinitial NeuroCine
ÔºàOursÔºâ
versionofStableDiffusionistrainedforprocessingimages
ataresolutionof512√ó512,ourexperimentalmethodology Chen,
involvesfine-tuningthemodel.Thisfine-tuningincorporates 2023
anaugmentedtemporallayer,adaptingthemodelforvideo
processing at a reduced resolution of 256 √ó 256 and a Kupershmit
2022
frame rate of 3 FPS. In this phase, we specifically focus
onfine-tuningtheparameterswithinthespatialattention, Wang,
2022
cross-attention,andtemporalattentionheadsofthediffusion
model. Thetrainingprocessiscarriedoutwithtextcondi-
Wen,
tioningforadurationof1000steps,utilizingalearningrate 2018
of2√ó10‚àí5andabatchsizeof14.Afteracquiringthevideo
diffusionmodel,wereplaceitstextencoderwithanfMRI Figure3: Comparisonofthedecodedresultsconsidering
encoder,whichhasbeenpreviouslytrained.Subsequently, ourframeworkNeuroCineandbaselines.
wefine-tunethelayersresponsibleforspatialself-attention,
visual-fMRIcross-attention,andtemporalattentionwithin
the video generation model and the fMRI encoder. This
GT
fine-tuningisconductedusingthefMRI-Videopairdataset.
Weusealearningrateof2√ó10‚àí5andabatchsizeof24.All
experimentswereconductedusingasingleA100GPU. NeuroCine
(Ours)
0.25
0.225 0.224 [a] C 2h 0e 2n 3,
0.211
0.2 0.187
GT
0.171 0.171
0.15 0.146
0.135
NeuroCine
0.118 0.119 0.1110.117 (Ours)
0.1
Chen,
2023
0.05
Figure 4: Additional comparisons of decoded results
0
betweenourmethodNeuroCineandthepreviousstate-of-
Subject1 Subject 2 Subject 3
[b] the-art(SOTA)model.
Ours Chen Kupershmidt Wang
image [b] In this section, we compare our methodology with prior
classification 50- 0.172
way 0.186 works,includingstudiesfromChenetal.(2023),Wenetal.
video (2018),Wangetal.(2022),andKupershmidtetal.(2022),
classification 50- 0.202
way 0.217 focusing on Structural Similarity Index Measure (SSIM)
scoresandclassificationaccuracy.OurSSIMresults,shown
0.11 0.13 0.15 0.17 0.19 0.21 0.23
inFigure2[a],indicateourmethodsetsanewstate-of-the-art
(SOTA)withscoresof0.225,0.224,and0.211forSubject
Chen Ours
1,2,and3.Specifically,ourmethodsurpassestheprevious
Figure2:ComparisonsofStructuralSimilarityIndexMea- ( Chen et al. (2023)) by a significant margin of 20.97%
sure(SSIM)Scoresand50-wayImage/VideoClassification inSubject1,31.00%inSubject2and12.30%inSubject
Accuracy. [a]ComparingtheSSIMscoresofourmethod 3. (You mayreferto hyperparamtersettings inTable1‚Äôs
withotherthreebenchmarksonSubject1,2and3.(b)Com- experiment 10 for reproduction of Subject 1‚Äôs decoding
paringourmethod‚Äôs50-wayImageandVideoClassification performance.) Figure2[b]furthershowsthatourmethod
AccuracywithpreviousSOTAmodelonSubject1. achievessuperioraccuracyinboth50-wayImageandVideo
ClassificationAccuracy.
6NeuroCine:DecodingVividVideoSequencesfromHumanBrainActivties
Ablated ID Spatial Mask InterT pe om lap tio or na l R atio Spatial Loss Tem Wpo er iga hl tLoss Dependent Dependent SSIM
Parameter Ratio (ùú∏ ùíîùíëùíÇ)
(ùú∏ùíïùíÜùíé)
Weight (ùùÅùíîùíëùíÇ)
(ùùÅùíïùíÜùíé)
Noise Use Noise Ratio (ùõÉ)
0 \ \ \ \ \ \ 0.171
Spatial Mask 1 0.2 1/3 1 1 No \ 0.204
Ratio 2 0.4 1/3 1 1 No \ 0.184
3 0.6 1/3 1 1 No \ 0.184
Temporal 1 0.2 1/3 1 1 No \ 0.204
Jittering Ratio 4 0.2 1/2 1 1 No \ 0.186
1 0.2 1/3 1 1 No \ 0.204
Spatial and 5 0.2 1/3 0.5 0.5 No \ 0.191
Temporal Loss
Weight 6 0.2 1/3 0.25 0.75 No \ 0.186
7 0.2 1/3 0.75 0.25 No \ 0.185
1 0.2 1/3 1 1 No \ 0.204
Dependent 8 0.2 1/3 1 1 Yes 0.25 0.242
Noise Ratio
9 0.2 1/3 1 1 Yes 0.5 0.230
10 0.2 1/3 1 1 Yes 0.75 0.225
Table1:AblationstudyaboutNeuroCine‚Äôsimportanthyperparameters‚Äôeffectsonfinalvideodecodingperformancemeasured
bySSIM,includingspatialmaskratio, temporalinterpolationratio, spatiallossweight, temporallossweight, usingof
dependentnoiseanddependentnoiseratio.
ourmodel‚Äôsreconstructedresultsonallsubjects,whereit
GT consistently decodes high-quality, semantically accurate
videos from different subjects. We present more video
Subject 1 framesgeneratedbyourmodelinFigure7intheappendix.
Subject 2 5.2.AblationStudy
Inthissubsection,weperformanablationstudytoassess
Subject 3 the impact of each component within our model and the
significanceofhyperparametersettingsonvideodecoding
performance.
GT
SpatialandTemporalAugmentation:Theratiosforspatial
Subject 1 maskingandtemporalinterpolationdictatetheextentofmod-
ificationtothefMRItokenvectorstogenerateaugmented
samples.Resultsfromexperiments[0-3]inTable1indicate
Subject 2
thatspatialaugmentationsignificantlyenhancesdecoding
performance,withaspatialmaskratioof0.2yieldingthebest
Subject 3
results. However,excessivemaskingdetrimentallyaffects
theSSIMofthereconstructedsamples,underscoringthebal-
Figure 5: Decoded results of our framework NeuroCine anceneededtoutilizefMRI‚Äôsspatialredundancyeffectively.
fromallthreesubjectsinthedataset. Asimilarbalanceiscrucialforthetemporalinterpolation
ratio,wherearatioof1/3enhancesperformancewithoutthe
adverseeffectsseenwhentheratioisincreasedfurther.
Qualitative comparisons in Figure 3 show that, unlike
Augmentation Loss Weight: The augmentation loss
other models that yield blurry or unrecognizable outputs,
weights determine their contribution to the total loss in
our method and Chen et al. (2023)‚Äôs approach produce
optimizingthefMRIencoder.Experiments[1,5-7]inTable
high-quality, semantically accurate videos. A detailed
1demonstratethatabalancedaugmentationlossbetween
comparison with the previous SOTA method (Chen et al.
spatialandtemporalaspectsimprovesperformance, with
(2023))inFigure4revealsoursuperiorsemanticalignment
settingbothweightsto1achievingthehighestSSIM.This
withgroundtruthvideos. Forinstance,whereourmethod
highlightstheimportanceofevenlycapturingspatialand
accurately generates a video of a swimming turtle, Chen
temporalfeaturesoffMRIdataforaccuratevideodecoding.
etal.(2023)‚Äôsoutputshowsagroupoffish.Theversatilityof
ourmodelisfurtherdemonstratedinFigure5whichshows Dependent Noise Ratio: Introducing dependent noise
7NeuroCine:DecodingVividVideoSequencesfromHumanBrainActivties
[a]
BeforeContrastive AfterContrastive FullModel
[b]
FirstLayer MiddleLayer LastLayer
[c] [d]
BrainNetwork
Figure6: Self-AttentionAnalysisforSubject1. Thegradationfrombluetoredonthevoxelmapsignifiestheascending
importanceofeachvoxelinthegenerationprocess,asdeterminedbytheattentionanalysis. Panels[a]and[c]emphasize
theevolutionofattentionmapsthroughoutthelearningstages.Panel[b]highlightsthedifferentialattentionpatternsacross
varioustransformerlayers. Panel[d]presentsacomparativeviewofattentionmapscorrespondingtoafewvideoframe
examplesunderdifferentcategories.
targets the inherent noise in fMRI data, enhancing the acomprehensionofhigh-levelsemanticfeatures.Thispro-
model‚Äôsabilitytodecodevideos,asshowninexperiments gressionisaccompaniedbyadecreaseinthesumofattention
[1,8-10]inTable1.Thiscouldconfirmourhypothesisthat values, suggestingamoreefficientallocationofattention
incorporating dependent noise as a prior enhances video towardsdiscriminativefeaturesandarefinedrepresentation
reconstructionwithDiffusionModels. However,wealso thatcapturesmoreinformationwithlessattention,partic-
‚àö
observethatincreasingthenoiseratio( Œ≤)toomuchcan ularlyaftercontrastivelearning,whichpointstothemodel‚Äôs
negativelyaffectperformance.Thisoutcomeisanticipated, increasedefficiencyandabilityastrainingprogresses.
sinceahighernoiseratioresultsineachframebecoming
Ontopoftheseobservations,weexploretheattentionmaps
moresimilartotheothers,leadingtothecreationofvideos
generated in response to individual video stimuli. The
withframesthataretoocloselyaligned,lackingvariationand
proficiency of our model to differentiate among various
dynamism,andpotentiallyresultinginnearlystaticvideos.
visualcategoriesandtheirassociatedcerebralpatternswas
substantiated,asevidencedinFigure6[d].Acaseinpointis
5.3.InterpretationoffMRIEncoder
theheightenedattentionvaluesinthefusiformgyrusregion
The results shown in Figure 6 align with the established whensubjectsviewedimagesofacatcomparedtowhenthey
evidencethatthevisualcortexplaysapivotalroleindecod- wereexposedtoimagesrelatedtofood. Anotherexample
ing visual spatiotemporal information (Zhou et al., 2013; isthedistinctattentionpatternsintheextrastriatebodyarea
Vetteretal.,2014).OurfMRIencoderexhibitsanenhanced (EBA)whensubjectsviewimagesofhumans,incontrastto
capacityforsemanticinformationprocessingasitundergoes whentheyseeimagesofairplanes,highlightingtheregion‚Äôs
contrastivelearningandfulltraining,asindicatedinFigure6 specificityinprocessingbiologicalforms.
panels[a]and[c]. Itshowsanincreasingtrendtoextract
moresemanticinformationfromcontrastivetrainingandfull 6.Conclusion
modeltraining,withamarkedshiftinfocustowardshigher
visualregionsfollowingcontrastivelearning.Thisdemon- Inconclusion,thisstudyintroducesanoveldual-phaseframe-
strateshowthefMRIencoderevolvesinitslearningfrom workfordecodinghigh-qualityvideosfromfMRIdata,ef-
thebrain,transitioningfromafocusonlow-levelfeaturesto fectively tackling challenges like spatial redundancy and
8NeuroCine:DecodingVividVideoSequencesfromHumanBrainActivties
temporallagsinfMRIsignals.Ourmethod,whichcombines Chen,N.,Zhang,Y.,Zen,H.,Weiss,R.J.,Norouzi,M.,and
spatial-temporalcontrastivelearningwithanenhancedvideo Chan,W. Wavegrad:Estimatinggradientsforwaveform
diffusionmodel,showsnotableimprovementsoverexisting generation. arXivpreprintarXiv:2009.00713,2020.
modelsinbothSSIMandsemanticaccuracy.Theempirical
Chen, Z., Qing, J., Xiang, T., Yue, W. L., and Zhou,
results,benchmarkedagainstpriorworks,demonstratethe
J. H. Seeing beyond the brain: Masked mod-
efficacyofourapproach.Thisresearchnotonlyadvancesthe
eling conditioned diffusion model for human vi-
fieldofneuraldecodingbutalsoopensnewavenuesforex-
sion decoding. In arXiv, November 2022. URL
plorationinneuralimagingandcognitiveneuroscience,with
https://arxiv.org/abs/2211.06956.
potentialapplicationsinunderstandinghumancognitionand
developingassistivetechnologiesforpeoplewithdisabilities.
Chen,Z.,Qing,J.,andZhou,J.H. Cinematicmindscapes:
High-quality video reconstruction from brain activity.
BroaderImpact arXivpreprintarXiv:2305.11675,2023.
This study‚Äôs advancements in neural decoding have deZwart,J.A.,vanGelderen,P.,Jansma,J.M.,Fukunaga,
promisingimplicationsforvariousfields,rangingfromneu- M., Bianciardi, M., and Duyn, J. H. Hemodynamic
rosciencetothedevelopmentofbrain-computerinterfaces, nonlinearities affect bold fmri response timing and
as large-scale models continue to evolve. However, the amplitude. Neuroimage,47(4):1649‚Äì1658,2009.
widespreadapplicationofthistechnologyunderscoresthe
Fang, T., Qi, Y., and Pan, G. Reconstructing perceptive
needforstringentgovernmentalregulationsandproactive
imagesfrombrainactivitybyshape-semanticgan. ArXiv,
measures by the research community to safeguard the
abs/2101.12083,2021.
privacyandsecurityofbiologicaldata,therebypreventing
anypotentialmisuse. Regardingdataethics,ourresearch
Ge,S.,Nah,S.,Liu,G.,Poon,T.,Tao,A.,Catanzaro,B.,Ja-
utilizedpreprocesseddatafrompubliclyaccessibledatasets.
cobs,D.,Huang,J.-B.,Liu,M.-Y.,andBalaji,Y. Preserve
The fMRI data employed in our training have undergone
yourowncorrelation: Anoisepriorforvideodiffusion
processingtoensurethattheydonotcontainanyinformation
models. InProceedingsoftheIEEE/CVFInternational
thatcouldbedirectlytracedbacktoindividualparticipants.
ConferenceonComputerVision,pp.22930‚Äì22941,2023.
Furthermore, the original collection of this fMRI data
adheredtostrictethicalguidelinesandreviews,asdetailed Glover,G.H. Overviewoffunctionalmagneticresonance
intherespectivesourcepublications.Thisapproachreflects imaging. NeurosurgeryClinics,22(2):133‚Äì139,2011.
ourcommitmenttoethicalresearchpractices,prioritizing
participantanonymityanddataintegrity. He,Y.,Yang,T.,Zhang,Y.,Shan,Y.,andChen,Q. Latent
videodiffusionmodelsforhigh-fidelityvideogeneration
witharbitrarylengths. arXivpreprintarXiv:2211.13221,
References
2022.
Abraham, A., Pedregosa, F., Eickenberg, M., Gervais, P.,
He¬¥naff,O.J.,Bai,Y.,Charlton,J.A.,Nauhaus,I.,Simoncelli,
Mueller,A.,Kossaifi,J.,Gramfort,A.,Thirion,B.,and
E.P.,andGoris,R.L. Primaryvisualcortexstraightens
Varoquaux,G. Machinelearningforneuroimagingwith
naturalvideotrajectories. Naturecommunications,12(1):
scikit-learn. Frontiersinneuroinformatics,8:14,2014.
5982,2021.
Bao,F.,Li,C.,Zhu,J.,andZhang,B. Analytic-dpm:anan- Ho, J., Jain, A., and Abbeel, P. Denoising diffusion
alyticestimateoftheoptimalreversevarianceindiffusion probabilisticmodels. AdvancesinNeuralInformation
probabilisticmodels. arXivpreprintarXiv:2201.06503, ProcessingSystems,33:6840‚Äì6851,2020.
2022.
Ho,J.,Chan,W.,Saharia,C.,Whang,J.,Gao,R.,Gritsenko,
Bartels,A.,Zeki,S.,andLogothetis,N.K. Naturalvision A.,Kingma,D.P.,Poole,B.,Norouzi,M.,Fleet,D.J.,etal.
reveals regional specialization to local motion and Imagenvideo:Highdefinitionvideogenerationwithdif-
to contrast-invariant, global flow in the human brain. fusionmodels. arXivpreprintarXiv:2210.02303,2022a.
Cerebralcortex,18(3):705‚Äì717,2008.
Ho,J.,Salimans,T.,Gritsenko,A.,Chan,W.,Norouzi,M.,
andFleet,D.J. Videodiffusionmodels,2022b.
Beliy,R.,Gaziv,G.,Hoogi,A.,Strappini,F.,Golan,T.,and
Irani,M.Fromvoxelstopixelsandback:Self-supervision
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier,
in natural-image reconstruction from fmri. ArXiv,
C.,Vijayanarasimhan,S.,Viola,F.,Green,T.,Back,T.,
abs/1907.02431,2019.
Natsev,P.,etal. Thekineticshumanactionvideodataset.
arXivpreprintarXiv:1705.06950,2017.
Buxton,R.B.Introductiontofunctionalmagneticresonance
imaging:principlesandtechniques.Cambridgeuniversity Kingma,D.P.andWelling,M. Auto-encodingvariational
press,2009. bayes. arXivpreprintarXiv:1312.6114,2013.
9NeuroCine:DecodingVividVideoSequencesfromHumanBrainActivties
Kupershmidt,G.,Beliy,R.,Gaziv,G.,andIrani,M. Apenny Poole, B., Jain, A., Barron, J. T., and Mildenhall, B.
foryour(visual)thoughts:Self-supervisedreconstruction Dreamfusion: Text-to-3d using 2d diffusion. arXiv
of natural movies from brain activity. arXiv preprint preprintarXiv:2209.14988,2022.
arXiv:2206.03544,2022.
Qian,X.,Wang,Y.,Fu,Y.,Xue,X.,andFeng,J. Semantic
Li,J.,Li,D.,Xiong,C.,andHoi,S. Blip: Bootstrapping neural decoding via cross-modal generation. arXiv
language-imagepre-trainingforunifiedvision-language preprintarXiv:2303.14730,2023.
understandingandgeneration. InInternationalConfer-
ence on Machine Learning, pp. 12888‚Äì12900. PMLR, Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,
2022. Agarwal,S.,Sastry,G.,Askell,A.,Mishkin,P.,Clark,J.,
etal. Learningtransferablevisualmodelsfromnatural
Li,M.,Qu,T.,Yao,R.,Sun,W.,andMoens,M.-F. Allevi- language supervision. In International conference on
atingexposurebiasindiffusionmodelsthroughsampling machinelearning,pp.8748‚Äì8763.PMLR,2021.
with shifted time steps. International Conference on
LearningRepresentations,2024. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer,B. High-resolutionimagesynthesiswithlatent
Lin, S., Sprague, T., and Singh, A. K. Mind reader: diffusion models. In Proceedings of the IEEE/CVF
Reconstructing complex images from brain activities. ConferenceonComputerVisionandPatternRecognition,
arXivpreprintarXiv:2210.01769,2022. pp.10684‚Äì10695,2022.
Liu,L.,Ren,Y.,Lin,Z.,andZhao,Z. Pseudonumerical Seeliger, K., Gu¬®c¬∏lu¬®, U., Ambrogioni, L., Gu¬®c¬∏lu¬®tu¬®rk, Y.,
methods for diffusion models on manifolds. arXiv and van Gerven, M. Generative adversarial networks
preprintarXiv:2202.09778,2022.
for reconstructing natural images from brain activity.
NeuroImage,181:775‚Äì785,2017.
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J.
Dpm-solver:Afastodesolverfordiffusionprobabilistic
Shen, G., Dwivedi, K., Majima, K., Horikawa, T., and
modelsamplinginaround10steps. AdvancesinNeural
Kamitani, Y. End-to-end deep image reconstruction
InformationProcessingSystems,35:5775‚Äì5787,2022a.
fromhumanbrainactivity. Frontiersincomputational
neuroscience,13:21,2019.
Lu,C.,Zhou,Y.,Bao,F.,Chen,J.,Li,C.,andZhu,J. Dpm-
solver++: Fastsolverforguidedsamplingofdiffusion
Singer,U.,Polyak,A.,Hayes,T.,Yin,X.,An,J.,Zhang,S.,
probabilisticmodels. arXivpreprintarXiv:2211.01095,
Hu,Q.,Yang,H.,Ashual,O.,Gafni,O.,etal. Make-a-
2022b.
video:Text-to-videogenerationwithouttext-videodata.
arXivpreprintarXiv:2209.14792,2022.
Mozafari,M.,Reddy,L.,andvanRullen,R. Reconstructing
naturalscenesfromfmripatternsusingbigbigan. 2020
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and
International Joint Conference on Neural Networks
Ganguli,S. Deepunsupervisedlearningusingnonequi-
(IJCNN),pp.1‚Äì8,2020.
libriumthermodynamics. InInternationalConference
Nichol,A.Q.andDhariwal,P.Improveddenoisingdiffusion
onMachineLearning,pp.2256‚Äì2265.PMLR,2015.
probabilistic models. In International Conference on
Song, J., Meng, C., and Ermon, S. Denoising diffusion
MachineLearning,pp.8162‚Äì8171.PMLR,2021.
implicitmodels. arXiv:2010.02502,October2020. URL
Ning, M., Sangineto, E., Porrello, A., Calderara, S., and https://arxiv.org/abs/2010.02502.
Cucchiara,R. Inputperturbationreducesexposurebiasin
Sun,J.,Li,M.,andMoens,M.-F. Decodingrealisticimages
diffusionmodels. arXivpreprintarXiv:2301.11706,2023.
frombrainactivitywithcontrastiveself-supervisionandla-
Ning, M., Li, M., Su, J., Salah, A.A., andErtugrul, I.O. tentdiffusion.InProceedingsofthe26thEuropeanConfer-
Elucidatingtheexposurebiasindiffusionmodels. Inter- enceonArtificialIntelligence(ECAI2023),2023,2023a.
nationalConferenceonLearningRepresentations,2024.
Sun, J., Li, M., Zhang, Y., Moens, M.-F., Chen, Z.,
Nishimoto,S.,Vu,A.T.,Naselaris,T.,Benjamini,Y.,Yu, and Wang, S. Contrast, attend and diffuse to de-
B.,andGallant,J.L. Reconstructingvisualexperiences code high-resolution images from brain activities.
frombrainactivityevokedbynaturalmovies. Current In Thirty-seventh Conference on Neural Informa-
biology,21(19):1641‚Äì1646,2011. tion Processing Systems, 2023b. URL https:
//openreview.net/forum?id=YZSLDEE0mw.
Parrish,T.B.,Gitelman,D.R.,LaBar,K.S.,andMesulam,
M.-M. Impact of signal-to-noise on functional mri. Tong, Z., Song, Y., Wang, J., and Wang, L. Videomae:
Magnetic Resonance in Medicine: An Official Journal Masked autoencoders are data-efficient learners for
oftheInternationalSocietyforMagneticResonancein self-supervisedvideopre-training. Advancesinneural
Medicine,44(6):925‚Äì932,2000. informationprocessingsystems,35:10078‚Äì10093,2022.
10NeuroCine:DecodingVividVideoSequencesfromHumanBrainActivties
UgÀòurbil,K.,Xu,J.,Auerbach,E.J.,Moeller,S.,Vu,A.T.,
Duarte-Carvajalino,J.M.,Lenglet,C.,Wu,X.,Schmitter,
S.,VandeMoortele,P.F.,etal.Pushingspatialandtempo-
ralresolutionforfunctionalanddiffusionmriinthehuman
connectomeproject. Neuroimage,80:80‚Äì104,2013.
VanEssen,D.C.,Smith,S.M.,Barch,D.M.,Behrens,T.E.,
Yacoub, E., Ugurbil, K., Consortium, W.-M. H., et al.
Thewu-minnhumanconnectomeproject: anoverview.
Neuroimage,80:62‚Äì79,2013.
Varela,F.J.,Thompson,E.,andRosch,E. Theembodied
mind, revised edition: Cognitive science and human
experience. MITpress,2017.
Vetter,P.,Smith,F.W.,andMuckli,L. Decodingsoundand
imagerycontentinearlyvisualcortex. CurrentBiology,
24(11):1256‚Äì1262,2014.
Wang,C.,Yan,H.,Huang,W.,Li,J.,Wang,Y.,Fan,Y.-S.,
Sheng,W.,Liu,T.,Li,R.,andChen,H. Reconstructing
rapidnaturalvisionwithfmri-conditionalvideogenerative
adversarialnetwork. CerebralCortex,32(20):4502‚Äì4511,
2022.
Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli,
E. P. Image quality assessment: from error visibility
to structural similarity. IEEE transactions on image
processing,13(4):600‚Äì612,2004.
Wen, H., Shi, J., Zhang, Y., Lu, K.-H., Cao, J., and Liu,
Z. Neural encoding and decoding with deep learning
for dynamic natural vision. Cerebral cortex, 28(12):
4136‚Äì4160,2018.
Wu,J.Z.,Ge,Y.,Wang,X.,Lei,S.W.,Gu,Y.,Shi,Y.,Hsu,
W.,Shan,Y.,Qie,X.,andShou,M.Z. Tune-a-video:One-
shottuningofimagediffusionmodelsfortext-to-video
generation.InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pp.7623‚Äì7633,2023.
Zhou,D.,Rangan,A.V.,McLaughlin,D.W.,andCai,D.
Spatiotemporaldynamicsofneuronalpopulationresponse
intheprimaryvisualcortex. ProceedingsoftheNational
AcademyofSciences,110(23):9517‚Äì9522,2013.
Zhou, D., Wang, W., Yan, H., Lv, W., Zhu, Y., andFeng,
J. Magicvideo: Efficient video generation with latent
diffusionmodels. arXivpreprintarXiv:2211.11018,2022.
11NeuroCine:DecodingVividVideoSequencesfromHumanBrainActivties
A.VisualizationofDecodingOutcomes
InFigure7,weshowcaseadditionaldecodingvideosforallthreesubjectsexaminedinourstudy.Thesefindingsillustratethe
capabilityofourproposedmodeltodecodehigh-qualityvideoswithprecisesemantics.Furthermore,thediversecollection
ofvideosdisplayedbelowhighlightsthemodel‚Äôsabilityindecodingabroadrangeofsemanticcontentfromhumanbrain
activity.Thisincludesvideosrelatedtoobjects,humanmotion,scenes,animals,etal.
GT
Subject 1
GT
Subject 1
[a]
GT
Subject 2
GT
Subject 2
[b]
GT
Subject 3
GT
Subject 3
[c]
Figure7:Videosgeneratedbyourmodels:[a]DecodingoutcomesforSubject1,[b]DecodingoutcomesforSubject2,[c]
DecodingoutcomesforSubject3.
12