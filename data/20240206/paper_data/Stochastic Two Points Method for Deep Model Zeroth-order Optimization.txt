Stochastic Two Points Method for Deep Model Zeroth-order Optimization
YijiangPang1 JiayuZhou1
Abstract mance. Among these approaches, zeroth-order methods
havebecomeespeciallyattractiverecentlysincetheyonly
Largefoundationmodels,suchaslargelanguage
rely on function values, often referred to as zeroth-order
models,haveperformedexceptionallywellinvar-
information,tooptimizemodels,avoidmemory-intensive
iousapplicationscenarios. Buildingorfullyfine-
back-propagation,andenablefullorpartialfine-tuningwith
tuning such large models is usually prohibitive
minimum computing resources. For instance, MeZO, a
duetoeitherhardwarebudgetorlackofaccess
recently proposed zeroth-order method, exhibits memory
to backpropagation. The zeroth-order methods
savings up to 12 times compared with standard full fine-
offerapromisingdirectionfortacklingthischal-
tuningandshowscompetitiveperformancewithfullfine-
lenge,whereonlyforwardpassesareneededto
tuning&parameter-efficienttuningmethods(Malladietal.,
updatethemodel. Thispaperintroducesaneffi-
2023). The line of research is broadly investigated and
cientStochasticTwo-Point(S2P)approachwithin
generallyanalyzedwithintheframeworkofoptimizingthe
the gradient-free regime. We present the theo-
reticalconvergencepropertiesofS2Punderthe
non-convexoptimizationproblemmin x∈Rdf(x),wherethe
f :Rd →Risdifferentiableandthederivativesarenotdi-
generalandrelaxedsmoothnessassumptions.The
rectlyaccessible. Thecomplexityofthisproblemisstudied
theoreticalpropertiesalsoshedlightonafaster
overfunctionquerycomplexity,namelythecomplexityin
and more stable S2P variant, Accelerated S2P
termsofthenumberoffunctionevaluations.
(AS2P),throughexploitingournewconvergence
properties that better represent the dynamics of Existing analyses of zeroth-order approaches mainly fo-
deepmodelsintraining. Ourcomprehensiveem- cus on convergence to ϵ-first-order stationary points un-
pirical results show that AS2P is highly effec- dertheL−smoothnessassumption(Nesterov&Spokoiny,
tiveinoptimizingobjectivesforlargedeepmod- 2017;Bergouetal.,2020). Zeroth-orderoptimizationcan
els,includinglanguagemodels,andoutperforms becategorizedintotwotypesbywhetherornotitexplic-
standardmethodsacrossvariousmodeltypesand itly approximates gradient: gradient estimator and direct
scales, with 2× speed-up in training over most search (Ghadimi & Lan, 2013; Chen et al., 2020; Lewis
conductedtasks. etal.,2000;Connetal.,2009). Gradientestimatormethods
computeanestimateofthegradientthroughzeroth-order
informationtooptimizef,i.e.,random(gradient-free)or-
1.Introduction acles. Random oracles are analyzed in the framework of
StochasticApproximation(SA),e.g.,random-directionsSA
Utilizingpre-trainedlargemodelsforvariousdownstream
(RDSA). Gaussian smoothing is a gradient estimator al-
tasks has emerged as a prominent trend, particularly in
gorithmthatinitiallyusesRDSAasarandomoracle,and
thecontextofLargeLanguageModels(LLMs),whichde-
theirworkestablishestheframeworkofanalyzingthecon-
mandsubstantialcomputationalresourcesanddataduring
vergence properties of f once explicitly obtaining mean
theirinitialtrainingphase(Devlinetal.,2018;Bommasani
squarederrorbetweentheapproximatedgradientandtrue
etal.,2021). Differentfromsmallerdeepmodels,fullfine-
gradient(Nesterov&Spokoiny,2017). Ontheotherhand,
tuning these models is often prohibitive due to the mas-
thedirectsearchgenerallyoptimizesf byupdatingtheob-
sive computing resources needed. Therefore, techniques
jectivefunctionalongfixedorrandomizeddirectionswith
such as parameter-efficient tuning, including prompt tun-
fixedoradaptivestepsize(e.g.,reducestepsizewhenthe
ing(Lesteretal.,2021)andLoRA(Huetal.,2021),aswell
selecteddirectionsgetrejected)(Vicente,2013). Stochas-
aszeroth-ordermethods(Malladietal.,2023;Prasadetal.,
tic Three Points (STP) (Bergou et al., 2020) is a repre-
2022),aredevelopedanddemonstratedsatisfactoryperfor-
sentative approach in this category. With the condition
E|sT∇f(x)| ≥ C||∇f(x)|| where s is a random vector
1MichiganStateUniversity. Correspondenceto: JiayuZhou
<jiayuz@msu.edu>. sampledfromspecificdistributionsandCisasmallpositive
constant,oneoftheSTPdirections±swithanappropriate
Preprint.
1
4202
beF
2
]GL.sc[
1v12610.2042:viXraStochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
step size consistently decreases the objective function in plexityundertherelaxedsmoothnessassumptionis
expectation. novel.
• Based on our theoretical analysis, we proposed a
Inpractice,itisoftenusefultosampleasymmetric(two-
faster variant, Accelerated S2P (AS2P), which ex-
sided) random perturbation per update for optimization
ploitsournewconvergencepropertiesandincorpo-
problems. Thisapproachfindspracticalutilityinscenarios
ratesourtheoreticalfindings.
likeLLMfine-tuning(Malladietal.,2023;Zelikmanetal.,
• We conduct extensive experiments on large deep
2023)andprovidestheoreticalenhancementwhenexploit-
models,includinglanguagemodels,thatshowAS2P
ing multiple random perturbations per update (Salimans
significantly outperforms competing methods on
et al., 2017; Mania et al., 2018). Examples include STP
gradient-freeadaptation,with2×speed-upintrain-
fromdirectsearchandtwo-sidedGradientApproximation
ingovermostconductedtasks.
Gaussiansmoothing(basicrandomsearch),abbreviatedas
GA.Whenusingsymmetricperturbations,theirrespective
updatesaregivenby: 2.Relatedwork
Extensiveexistingliteraturestudiedthezeroth-orderopti-
STP:x =argmin{f(x +αs ),f(x −αs ),f(x )},
k+1 k k k k k
mization under convex and non-convex settings (Shamir,
GA:x =x −αg
k+1 k k 2017;Jamiesonetal.,2012;Agarwaletal.,2009;Raginsky
whereg = f(xk+ρsk)−f(xk−ρsk)s ,s ∼N(0,I), &Rakhlin,2011;Duchietal.,2015). Boundstoreachfirst-
k 2ρ k k
orderstationarypointsundergeneralsmoothnessassump-
whereαdenotesstepsizeandρdenotessmoothingparam- tionhavebeenderived,whichgenerallydependonmodelpa-
eter. Note that f(x k) in STP cannot be reused from the rameterdimensiond(Nesterov&Spokoiny,2017;Bergou
previousiterationwhenusingbatchdatasettings. GAand etal.,2020). Alineofworkinvestigatestheeffectiveness
STPhavesimilarconvergenceandbehavior,andweshow of noise perturbation to various tasks, e.g., generalizing
laterinourpaperthattheycanbelinkedorinterconnected GaussianSmoothingtoBernoulli(±1)distribution(Gao&
underspecificsettings.Theconvergenceofbothapproaches Sener,2022),orthonormalizationofnoiseperturbationover
reliesonthe(general)L-smoothnessassumption,awidely Gram–Schmidt process (Choromanski et al., 2018; Mah-
employedconceptinnon-convexoptimizationbutlimitsthe eswaranathanetal.,2019). Moreover, practicalandtheo-
analysistofunctionsboundedbyquadratic,forinstance,a retical results showed the advantages of the zeroth-order
simplefunctionx4isnotgloballyL-smooth.
methodmeetinglow-rankstructuresoftheunderlyingprob-
lem(Caietal.,2022;Malladietal.,2023;Wangetal.,2018;
Recently,(Zhangetal.,2019)proposedthe(relaxed)L ,L -
0 1
Sener&Koltun,2020). Someapproachesalsoguarantee
smoothnessassumptionandshowthattherelaxedsmooth-
second-orderconvergence(Lucchietal.,2021;Zhang&Gu,
ness is a more realistic assumption for many real-world
2022;Renetal.,2023). However,theproblemhasrarely
tasks, especially for deep models and contains, e.g., uni-
beenstudiedunderthepopularrelaxedsmoothnessassump-
variatepolynomialandexponentialfunctions(Zhangetal.,
tion(Zhangetal.,2019). Basedonthetheories,manywork
2019; 2020; Danilova et al., 2022). There is a growing
proposedpracticalmethodstoadapttovariousdeepmodel
trend to adapt theories of existing popular techniques to
scenariossuchashyper-parameteroptimization(Bergstra&
therelaxedsmoothnessassumption(Wangetal.,2023;Li
Bengio,2012;Yang&Shami,2020),black-boxadversarial
etal.,2023),andgaininsightsintothedevelopmentofnew
attackondeepmodels(Ilyasetal.,2018;Guoetal.,2019;
algorithms.
Liu et al., 2018). Moreover, several methods have been
In this paper, distinct from existing work, we build upon developed for and adapted to deep models gradient-free
relaxedsmoothnessandadvancetheefficiencyofthezeroth adaptation(Malladietal.,2023;Prasadetal.,2022;Deng
order optimization by proposing a new approach called etal.,2022).
Stochastic Two-Point (S2P), which eliminates the non-
updatingcomponentf(x )ofSTPandthuseffectivelysav-
k 3.StochasticTwo-PointSearch
ingoneforwardpassinabatchdataforwardpass.Thepaper
hasthefollowingcontributionstozeroth-ordermethodsfor Inthissection,wefirstintroduceaprototypeofStochastic
largedeepmodels: Two-pointSearch(S2P)andanalyzeitsconvergenceusing
thegeneralsmoothnessassumption. Wethenimproveour
• We analyze the convergence properties of S2P un-
analysisofS2Pusingtherelaxedsmoothnessassumption,
der general and relaxed smoothness assumptions.
ThebasicformofS2PhasquerycomplexityO(d) whichleadstotheAcceleratedTwo-PointSearch(AS2P).
ϵ2
undergeneralsmoothnessassumption,whichisthe Throughoutthispaper,weuseboldlowercaselettersx,yto
samewithNesterov&Spokoiny,2017;Bergouetal., denotevectors. Forvectors,weuse||·||todenotetheℓ -
2
2020.Toourknowledge,theanalysisofquerycom-
2StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
norm. Forafunctionf :Rd →R,weuse∇f todenotethe Algorithm1StochasticTwo-Pointsearch(S2P).
gradientandf⋆todenotetheglobalminimumoffunction Inputs: EpochsK,objectivefunctionf parameterizedwith
f. WeuseO(·),Ω(·)tohideabsoluteconstantsthatdonot x∈Rd,stoppingcriterionϵ.
dependonanyproblemparameter. Weneedthefollowing Parameter: x
standarddefinitionsandassumptions(Nesterov&Spokoiny, 1: for k =0,...,K do
2017;Bergouetal.,2020;Zhangetal.,2019). 2: s k ∼R{Rademacherdistribution. Thenormaland
Definition3.1. Foradifferentiablefunctionf,xisaϵ-first- uniformdistributionalsoapply.}
orderstationarypointif||∇f(x)||≤ϵ. 3: ChoosingonefromOpt √ion1-4: Updateα
Definition 3.2. A differentiable function f is L- 4: Option1. α k =α 0/ Kd{Theorem3.8}
gradient Lipschitz if ||∇f(x 1)−∇f(x 2)|| ≤ L||x 1 − 5: Option2. α k = | Lγk d| where
x 2|| ∀x 1,x 2. |γ |= |f(x+ρsk)−f(x−ρsk)| {Theorem3.8}
k √2ρ √
Definition 3.3. A differentiable function f is (L ,L )-
smoothnessif||∇2f(x)||≤L +L ||∇f(x)||. 0 1 6: Option3. α k = 2/BL 1 dK {Theorem3.10}
0 1 7: Option4. α k = √|γk| where
Assumption3.4. Thefunctionf isL-gradientLipschitz. (AL0+ 2BL1|γk|)d
|γ |= |f(x+ρsk)−f(x−ρsk)|{Theorem3.10}
Assumption 3.5. The function f satisfies (L ,L )- k 2ρ
0 1
smoothness 8: x k+1 =argmin{f(x k+α ks k),f(x k−α ks k)}
9: endfor
Unless otherwise specified, we assume function f is 10: returnx
boundedbelowbyf⋆.
3.1.StochasticTwo-PointSearch(S2P) assumption. SimilartoSTP,weinitiateouranalysisfrom
Lemma 3.6, which shows the absolute value of the inner
We first propose a prototype algorithm, Stochastic Two-
productbetweengradientg andrandomperturbationsis
PointSearch(S2P),whichimprovesSTPbyremovingthe
larger than a positive value in expectation, which forms
non-updating component, f(x ). This seemingly minor
k thefoundationofdescent. Buildinguponthisfoundation,
changeeliminatestheneedforanadditionalforwardpass
Lemma3.7introducesaprogressiveboundandidentifies
ateachiterationwhencomparedtomethodslikeGA.The
the optimal step size at each iteration. This optimal step
changeisalsonon-trivialbecausethecomputationoff(x )
k sizeinspiresouralgorithmdevelopment,particularlyOption
inSTPcannotbereusedfromthepreviousiterationunder
2inTheorem3.8. Thecentralresultinthissubsectionis
thebatchdatacondition,andiscriticaltotheconvergence
Theorem3.8,whichestablishesthatAlg.1canemployboth
ofSTP.IfasimilarconvergenceismaintainedinS2P,such
stationaryanddynamicstepsizes(Option1andOption2,
aneliminationcangreatlyreducethecomputationneeded
respectively)toreachanϵ-first-orderstationarypointwitha
tooptimizelargedeepmodels,includinglanguagemodels. querycomplexityofO(d).
TheS2PalgorithmissummarizedinAlg.1.
ϵ2
Especially, the strategy of dynamic step size aims to ap-
Specifically, thechoiceofthedistributionofrandomper-
proximate the optimal step size at each iteration, i.e., ap-
turbationswithinthreecommonlyusedprobabilitydistri- proximating αopt = |∇f(xk)Tsk| with α = |γk| where
butions, normal, uniform, and Rademacher distribution k Ld k Ld
|γ | = |f(x+ρsk)−f(x−ρsk)|. Simultaneously, the error
(Bernoulli±1distribution),doesnotalterouranalysisre- k 2ρ
sults within our proof framework. However, we use the |δ | := |α −αopt| ≤ ρ iscontrolled. Pleaserefertoin-
k k k 2
randomperturbationsfromtheRademacherdistributionfor equality(12)inAppendixB.1formoredetails. Theabove
our analysis since STP originally utilizes the normal dis- findingsunderlinethefundamentalcorrelationbetweenthe
tributionanduniformdistribution. WealsonotethatS2P step-wisestepsizeα and|γ |,specifically,α ∝|γ |with
k k k k
involvestwodifferentsymmetricperturbationsineachiter- asufficientsmallρ. Anothercrucialobservationisthein-
ation,whichareutilizedfordynamicstepsizeadaptation. terplaybetweenstepsizeα andsmoothingparameterρ
k k
Thisapproachnecessitatestwicethecomputationalcostin (Notethetransitionfromρ → ρ whenadoptingspecific
k
eachupdatecomparedtoGAinpracticaldeployment. Ulti- step-wise strategies). Results presented in Appendix B.1
mately,ourgoalistoachieveonesymmetricperturbation (proof of Lemma 3.7) and Appendix B.1 (proof of Theo-
ineachiterationinourproposedacceleratedvariantofS2P, rem3.8)showtherequirementofbothα andρ fallwithin
√ k k
i.e.,AS2PinAlg.2. therangeof(0, 2||∇f(xk)||]toensurestep-wiseprogress.
Ld
Thisobservationhintsatasignificantconnectioninmagni-
3.2.S2PunderGeneralSmoothnessAssumption tudebetweenα andρ ,whichinspiresthedevelopmentof
k k
Alg.2.
We first analyze the convergence properties of f running
theproposedS2Palgorithmunderthegeneralsmoothness Wewanttoemphasizethatourresultsalsorevealtheinher-
3StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
entconnectionbetweenS2PandGA:S2PforOption2has linearlydependentonthestepsizeα,whichdistinguishes
almostthesameupdatingformulawithGA,whenthesign thisresultfromtheoneinLemma3.7. Intuitively,alarge
trickdescribedinSection3.4isapplied. ϵ indicates a large gradient, which may, in turn, imply
g
Lemma 3.6. For all g ∈ Rd, and random vector a large top eigenvalue of Hessian under Assumption 3.5.
Consequently,alargestepsizeisnolongerthebestchoice.
s ∼ R where R is the Rademacher distribution, then
E s∼R|⟨g,s⟩|≥ √1 ||g|| 2. ThisconceptispivotalinfurtherimprovementsuponS2P.
2
ThemainresultinthissubsectionisTheorem3.10,which
TheresultcanbedirectlyderivedbyapplyingKhintchine showsthatAlg.1canemploybothstationaryanddynamic
inequality(Khintchine,1923),andtheproofispresentedin step sizes (Option 3 and Option 4, respectively) to reach
theappendixA.PleaserefertoLemma3.4inBergouetal., an ϵ-first-order stationary point with a query complexity
2020forsimilarresultswithnormal&unifromdistributions. ofO(d). Importantly,Theorem3.10showsthestructured
ϵ2
Notethattherandomperturbationcanbenormalizedasdone naturewithinthelearningprocesswhentakingdynamicstep
inSTP,sowehaveE s∼R|⟨g, ||s s||⟩| = √1 dE s∼R|⟨g,s⟩| ≥ size. Forinstance,inregionswherethefunctionissmooth
√1 ||g|| 2. The formula trick can be easily applied to the andthegradientnormislarge,wecananticipateareduced
2d
followinganalysis,andtheconclusionremainsthesame. querycomplexity. Conversely,underthefourthcondition
outlined in Table 1, we encounter situations where it is
Lemma3.7(Progressivebound). Supposeobjectivefunc-
impossibletodecrease||∇f(x)||duetohighlevelsofnon-
tionf(·)satisfiesAssumption3.4and||∇f(x )|| ≥ϵ . If
√ k 2 g smoothness. Fortunately, our proposed step size strategy
werunalgorithm1withstepsizeα= 2ϵg,wehavefollow- allowsustosafelytraversethesehighlynon-smoothregions.
2Ld
ingprogressiveboundE[f(x )−f(x )|x
]≤−Ω(ϵ2
g),
k+1 k k Ld Importantly, theorem 3.11 shows that the gradient norm
whereE[·|x ]denotestheconditionalexpectationw.r.t. x .
k k ||∇f(x)|| is bounded by |γ| in expectation. This implies
that the statistical information of γ is highly likely to re-
TheproofispresentedintheappendixB.
veal characteristics of gradient norm information, which
Theorem3.8(Querycomplexity). Supposeobjectivefunc- isfurthercorrelatedwithsecond-orderinformationunder
tion f(·) satisfies Assumption 3.4. If we run algorithm 1 Assumption3.5. Toillustrate,letusdefineτ := ησ :=
k γ
withstepsizestrategyoptions1or2,thealgorithmreturns ηStdDev(γ ), i.e., τ at k-th iteration represents η×
recent k
in expectation an ϵ-first-order stationary point in O(d) standarddeviationofrecentobservations(e.g.,themostre-
ϵ2
functionevaluations. Specifically, cent10%iterations)ofγ.Then,withsufficientsmallρ,The-
orem 3.11 suggests that ||∇f(x )|| ≤ η StdDev(γ )
k a recent
2d (f(x )−f⋆) Lα
Option1 K ≥ ( 0 + 0)2, al √most for sure with appropriate choice of η a, such as
ϵ2 α 0 2 3 2. Moreover,underAssumption3.5,itcanestablishthat
Option2 K ≥
4Ld(f(x 0)−f⋆)
,
||∇2f(x)||isboundedbyη bStdDev(γ recent)undercertain
ϵ2− ρ2 confidenceintervalwithcarefulselectionofη b.
2
Lemma3.9(Progressivebound). Supposeobjectivefunc-
whereα 0 >0forOption1stationary √stepsize;ForOption tion f(·) satisfies Assumption 3.5 and ||∇f(x k)|| ≥ ϵ g.
2dynamicstepsize,scalarρ k ∈(0, 2||∇ Lf d(xk)||]forρ k in Alg.1withstepsizeα = √ 2ϵg givesthefollow-
eachiteration. Generally,itcanbesettoasmallvalue,e.g., 2(AL0+BL1ϵg)d
√ ingfollowingprogressiveboundE[f(x )−f(x )|x ]≤
ρ= L2 dϵ.
−Ω(
ϵ2
g ),whereE[·|x
]denk o+ t1 esthecok nditik
onal
(AL0+BL1ϵg)d k
TheproofispresentedintheappendixB. expectationw.r.t. x k,andconstantsA=1.01,B =1.01.
TheproofispresentedintheappendixC.1.
3.3.S2PunderRelaxedSmoothnessAssumption
Theorem3.10(Querycomplexity). Supposeobjectivefunc-
Wenowanalyzetheconvergencepropertiesoftheproposed tionf(·)satisfiesAssumption3.5. Withstepsizestrategy
S2P algorithm under the relaxed smoothness assumption. options3or4,Alg.1returnsinexpectationanϵ-first-order
The assumption posits that f may behave like a smooth stationarypointinO(d)functionevaluations. Specifically,
ϵ2
functionincertainlocalregionsofthelosslandscape,but
√ √
therecanalsoexistsomehighlynon-smoothregionswhere √ AL d+BL (f(x )−f⋆) d
Option3 K ≥( d+ 0 1 0 )2
thetopeigenvalueofHessianmaybelarge,necessitating ϵ
specialconsiderations(Zhangetal.,2019;Kunstneretal., Option4 TheresultissummarizedinTable1.
2023).
whereconstantsA=1.01,B =1.01.
Lemma3.9providestheprogressiveboundandtheoptimal
stepsizeateachiteration. Wehighlightthatϵ isnolonger TheproofispresentedintheappendixC.2.
g
4StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
Conditions[b] requirementoverρ[a] Querycomplexity
L 1 ≤ √1 2B,||∇f(x)||≥ 1−√AL 2B0 L1 ρ≤ d√ 21 ξ√ d 8d(f(x ϵ0)−f⋆)
L L1
1
≤
≥
√ √1 12 2B B, ,| || |∇ ∇f f( (x x) )| || |≤
≤
1 √− 2√ BA AL L2 LB0
0
1−L1
1
ρ ρ≤
≤
d d1 1(cid:113) (cid:113)2 2ξ ξ( (A AL L0 0+ +√ √ϵ ϵ2 2B BL L1 1ϵ ϵ) )√ √d
d
8AL0d8 (fA ( ((L √1 x−0
0
2d )√ B−(f 2 LfB( 1⋆x −L )0 (1 1) 2− )) √ϵ ϵf 2 22⋆ B)
L1−1)
L 1 ≥ √1 2B,||∇f(x)||≥ √ 2BAL L0 1−1 ρ≤ d1(cid:113) 2ξ(AL0+√ϵ 2BL1ϵ)√ d 8(2√ 2BL1−1)(√ 2B AL L1 0−1)(f(x0)−f⋆−ϵ)d
[a]ξisaconstantassociatedwiththird-orderpropertyoff,detailedinappendixinequality(27).
[b]Forthefourthcondition,decreasinglossvalueinsteadofgradientnorm,detailedinappendixinequality(36).
Table1. Withdynamicstepsizestrategy,theconvergencepropertyoff underrelaxedsmoothness.
Theorem 3.11. Suppose objective function f(·) satisfies intoafastervariantofS2P,AcceleratedS2P(AS2P).AS2P
Assumption3.5. Thenthegradientnorm||∇f(x)||canbe augments stochastic two-point search with dynamic step
boundedinexpectationas sizesandincorporatesstatisticalinformationrelatedtoγ to
k
potentiallycapturebothfirst-orderandsecond-orderdynam-
|γ|−ρd(AL +BL ||∇f(x)||)≤||∇f(x)||
0 1 icsoftheobjectivefunctionf. AS2Palgorithmisdescribed
√ √
||∇f(x)||≤ 2|γ|+ 2ρd(AL +BL ||∇f(x)||) inAlg.2andhastwohighlightedimprovements.
0 1
where|γ|= |f(x+ρs)−f(x+ρs)|. ConstantsA=1.01,B = Progressiveγ-clipping. Theimmediateobservationstem-
2ρ mingfromtheconvergencepropertiesoff runningS2Pun-
1.01whenρ≤ 1 .
2L1d derrelaxedsmoothnessassumptionisthenon-lineardepen-
dencebetweentheapproximatedoptimalstepsizeα and
TheproofispresentedinAppendix C.3. k
|γ k|, i.e., α
k
= √|γk| = 1√ .
(AL0+ 2BL1|γk|)d (AL0/|γk|+ 2BL1)d
Algorithm 2 Accelerated Stochastic Two-Point search Specifically, the step size is almost linearly incremental
(AS2P). when|γ k|issmall,buttheincrementdecreasesfastwhen
Inputs: EpochsK,datasetD ,objectivefunctionf(·)pa- |γ k|isrelativelylarge. Wethusproposeastrategytomimic
rameterizedwithx∼Rd,scalarρ 0andρ endassmoothing similar behavior, i.e., α k ∝ γ k′ where γ k′ = 1/|γk|1 +1/τ ka.
parameters,andscalarsη ,η . Decaystrategy,e.g.,cosine τa = η StdDev(γ )practicallyactasthethresholdto
a b k a recent
decay. estimatetheinhibitionstrengthto|γ |. Moreover,inspired
k
Parameter: x bythestructureofoptimizingf showingbyTable1along
1: for k =0,...,K do withempiricalinvestigations,wefoundthatf behavesmore
2: s k ∼R{Rademacherdistribution. likesatisfyingsmoothfunctionduringtheinitialstagesof
Normalanduniformdistributionalsoapply.} trainingandenteringnon-smoothregionsastrainingpro-
3: ρ k=fullD:ρ 0,batchB ⊂D:Decaystrategy(init= gresses. So, we propose a progressive adjustment of the
ρ 0,end=ρ end,k) thresholdτ ka overiterations. Thecompletestrategyiseluci-
4: ρ|γ k s|= )−|f f(x (xk+ρ −ksk ρ) 2− ρ skf( )x (cid:1)k−ρksk)|,β k =sign(cid:0) f(x k+ Ada ut te od min atl ii cne L-6 eao rf nA inl gg. R2 a.
te. Havingα ∝γ′,thenweana-
k k k k k k k
5: σ ρ =StdDev(γ recent) lyzethemagnitudeofstepsize. Fromstep-wisedescentas-
6: γ k′ = 1/(1/|γ k| + 1/τ k′) where τ k′ = pectdiscussedinsec √tion3.2,bothα kandρ karerequiredto
Decaystrategy(init=2η a,end=η a,k)∗σ ρ bewithinrange(0, 2||∇f(xk)||]toguaranteeconvergence
7: α k = full D: β kρ kγ τk′ b, batch B ⊂ D: β k∗Decay undergeneralsmoothnesL sd assumption. Meanwhile,under
8:
xst kra +t 1eg =y( xin kit += αρ
kk
s, kk)∗k ηγ bσk′
ρ
t
t
fh
h
oe
a rt
are
α
gla
k
ox oe
=
dd as
O
pm p(o
rd1
oo
)
xt ,h in mae
n
as
d
ts ioTa ns hs
e
ou
o
fm
r
gep rmt ai do 3n ie.,
1
nT
1
th ne
r
oeo rvr me em
a .l
Ss3 oρ.1 ,k0 if=s wug eOg oe
(
ns
d1
lt ys
)
9: endfor
tuneonehyper-parameter,sayρ andapproximateitwellin
10: returnx k
practice,thenasafecriterionforstepsizeisα ≤ρ . Be-
k k
sidesthat,accordingtoouranalysis,thealgorithmapplying
thedynamicstepsizestrategyhasthepotentialtooutper-
3.4.AcceleratedStochasticTwo-PointSearch(AS2P)
formthealgorithmwithstationarystepsize. However,the
Ourconvergenceanalysisoff runningS2Punderthegen- dynamic step size strategy requires twice symmetric per-
eral and relaxed smoothness assumptions yields insights
5StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
turbations forward passes at each iteration k. In order to the random perturbations sampled from the Rademacher
reduce the number of forward passes, we propose to as- distribution. Thischoiceisinfluencedbysomestudiesthat
signβ := sign(f(x+ρ s )−f(x−ρ s ))asthesign suggestsignvariantsoftenexhibitadvantages(Gao&Sener,
k k k k k
off(x+α s )−f(x−α s ),whichwecallsigntrick. 2022;Kunstneretal.,2023). Thedetailsofthesetupare
k k k k
Thenthecalculationofargmin{f(x+α s ),f(x−α s )} summarizedinAppendixD.1.
k k k k
isunnecessarysincex+β abs(α )s = argmin{f(x+
k k k
α ks k),f(x−α ks k)}whereabs(α k)isknowninpractical. 4.1.Performancecomparisonwithstandardmethods
However, for a safe sign assignment, principally, it is re-
Performance over common deep models and datasets.
quired α ≤ ρ at least supposing ρ is small enough to
k k k
Each row of Figure 1(b) demonstrates the conver-
guaranteetheconsistencyofsignofdirectionalgradientin
gence rate under pre-trained ResNet18&CIFAR10,
localregions. Basedontheaboveintuitions, wepropose
astrategyα =β ρ γ′/τbwhereτb =η StdDev(γ ) ResNet50&CIFAR10, ResNet101&CIFAR100, and
since
γ′/τbk
≤
1k alk mk ostk
for sure
sk uppob
sing η is
r le ac re gnt
e
ResNet152&CIFAR100 respectively. Accordingly, each
enough.k Mk eanwhile,thestrategygivesα ∝1/τb b,which row of Figure 1(a), which is derived from Figure 1(b),
k k demonstrates the training cost ratio (calculating through
potentiallyfitstheviewofclippinggradientdescent,improv-
numberoffunctionqueries)ofreachingspecificlossvalues
ingthetrainingprocessbyputtingconstraintsonthestep
(epochs) where the ratio {1, 0.8, 0.6, 0.4, 0.2} are costs
sizeaccordingtotheupperboundontheHessian(Zhang
ofGAreaching{500, 400, 300, 200, 100}epochs. Note
etal.,2019;Kunstneretal.,2023).
that STP requires three function queries at each iteration
Finally, we have α k = ηbσγ/|β γk kρ |+k ηb/ηa in short, which whereas other methods need two, so Figure 1(a) simply
emphasizes(a)Thenon-lineardependencebetweenα k and counts the ratio of STP as 1.5× original values when
|γ k|; (b) The interaction between the absolute value and derivingfromFigure1(b). WeconcludefromFigure1that
standarddeviationofγ k. the proposed AS2P outperforms all the baselines, which
generallyrequires0.5×trainingcostofothermethodsto
4.Experiments reach some specific loss values under most settings. See
Figure 5 in Appendix D.2 for additional results under
Inthissection,weconductanevaluationoftheproposed similar settings. We also notice that the performances
AS2PwithcommonlyuseddeepmodelssuchasResNet18, have no obvious difference between applying random
ResNet50, ResNet101, ResNet152 (He et al., 2016) and perturbationssampledfromnormaldistributionandrandom
datasetssuchasCIFAR-10,CIFAR-100(Krizhevsky&Hin- perturbations sampled from Rademacher distribution.
ton, 2009). In addition to these evaluations, we also fol- However,bothoutperformthemethodsapplyinguniform
low the experimental settings described in Malladi et al., noise, referring to Figure 4 in Appendix D.2. However,
2023tofullfine-tuneLLMmodelOPT-13B(Zhangetal., the performance gap between applying different random
2022)onclassificationtasks{SST-2,RTE}(Socheretal., distributions is not the main focus of this work, so in
2013; Dagan et al., 2005), multiple choice tasks {Copa, the following experiment, we only consider random
ReCoRD}(Roemmeleetal.,2011;Zhangetal.,2018),and perturbationssampledfromnormaldistribution.
generationtasks{SQuAD,DROP}(Rajpurkaretal.,2016;
Performance over fully fine-tuning LLM. Figure 2(b)
Duaetal.,2019).
and Figure 2(c) show the convergence rate of fully fine-
Specifically,wecomparetheperformanceofAS2Pagainst tuningOPT-13Boversixlanguagetasks. Figure2(a)shows
several standard methods, including GA (Nesterov & the corresponding training cost ratio of reaching specific
Spokoiny, 2017; Malladi et al., 2023), GA sign (Gao & loss values (epochs) where the ratio {1, 0.75, 0.5, 0.25}
Sener,2022),STP(Bergouetal.,2020),andutilizenormal are costs of GA with cosine decay LR reaching {20000,
distributionasthedefaultsourceofrandomperturbations. 15000,10000,5000}mini-batchiterations. Besides,extra
For clarity, GA represents the two-side random direction baselineGAwithaconstantlearningrate(LR)isaddedin
stochasticapproximationastherandomoraclewithtunable this experiment, suggested in Malladi et al., 2023. Over-
hyper-parameters learning rate and smoothing parameter. all, Figure 2 shows a large performance improvement of
Notably,GAisprincipallyequivalenttoMeZOaspresented theproposedmethodAS2Pagainstothermethods. Gener-
inMalladietal.,2023,whichreducesmemoryconsumption ally,formosttasks,AS2Prequireslessthan0.5×training
withimplementationtrickbydoingthetwiceforwardpasses costsofothermethodstoreachsomespecificlossvalues.
sequentiallyinsteadofinparallel. Itisworthmentioning Specifically,thelosscuresbetweenAS2PandSTPontask
that this trick is also applied in our implementations and SQuADlargelyoverlap,however,theactualtrainingcost
hardwarebudgetreferstoTable 4inMalladietal.,2023. ratio between AS2P and STP on task SQuAD is around
Furthermore,weintroduceAS2P signandGA signasvari- 1:1.5demonstratedbyFigure2(a).
antsofAS2PandGA,respectively. Thesevariantsutilize
6StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
ResNet18&CIFAR10 ResNet50&CIFAR10 ResNet101&CIFAR100 ResNet152&CIFAR100
1.00 1.00 1.001.00 0.981.00 0.931.00 1.00
000 ... 257 505
0.100.00 9.107.108.20
0.204.20 2.4 03 .30 3.40
0.309.40 1.8 00 .50 3.60 0.502.570.60 6.80 0.50 6.620.70
0.005.005.00 6.200.20
0.104.104.10 8.308.40
0.207.20 4.504.509.60 0.306.30 3.8 07 .705.80 0.400.360.86
0.007.00 7.10 4.200.20
0.107.10 5.307.400.40
0.208.20 4.601.600.60 0.306.30 2.8 05 .706.80 0.309.340.82
0.006.006.10 0.200.20 0.104.10
3.20 5.400.40 0.204.20
2.40 5.508.60 0.301.20 9.50 9.705.80 0.302.30 0.60 3.81
0.00
2.00 1.80 1.68 1.62 1.60 2.16 1.88 1.76 1.70 1.68 4.58 4.54 4.49 4.46 4.45 4.59 4.56 4.52 4.48 4.48
Loss values AS2P AS2P_sign STP GA_sign GA
(a) Trainingcostratioofreachingspecificlossvalues.TableversioninAppendixD.2.
ResNet18&CIFAR10 ResNet50&CIFAR10 ResNet101&CIFAR100 ResNet152&CIFAR100
4.6 4.6
2.2 2.2
2.0 2.0 4.5 4.5
1.8 1.8 4.4
4.4
1.6 1.6 4.3
4.3
0 100 200 300 400 500
AS2P AS2P_sign STP GA_sign GA
Number of epochs
(b) Trainingloss.
Figure1. Performancecomparisonwithvariousbaselinesundercommondeepmodels&datasets.
AS2P cosine decay LR STP cosine decay LR GA constant LR GA cosine decay LR
1.00 1.00 1.00 1.00 1.00 1.00 1.00
0.75 0.75 0.75 0.75 0.75 0.75 0.75
0.65 0.65
000 ... 025 050
0.100.190.230.25
0.150.300.380.50 0.170.380.50 0.170.380.47
0.030.030.120.25
0.030.030.300.50
0.030.030.35 0.030.030.35 0.050.110.230.25
0.070.230.400.50 0.100.260.47 0.100.260.47
0.030.070.230.25
0.050.190.420.50 0.050.230.42 0.050.230.42
0.070.150.300.25
0.120.230.530.50
0.150.23 0.150.23 0.100.190.230.25
0.150.300.400.50 0.170.340.47 0.150.340.42
0.46 0.41 0.38 0.39 0.68 0.67 0.67 0.67 2.13 1.99 1.94 1.94 2.61 2.58 2.58 2.57 0.71 0.60 0.57 0.56 1.78 1.67 1.63 1.65
SST2 RTE Copa ReCoRD SQuAD DROP
Loss values of specific tasks
(a) Trainingcostratioofreachingspecificlossvalues.TableversioninAppendixD.2.
GA constant LR GA cosine decay LR AS2P cosine decay LR STP cosine decay LR GA constant LR GA cosine decay LR AS2P cosine decay LR STP cosine decay LR
00 .. 89 SST2 22 .. 23 Copa 1.2 SQuAD 00 .. 78 SST2 22 .. 23 Copa 1.2 SQuAD
000 ... 567 1122 .... 8901 01 .. 80 000 ... 456 122 ... 901 01 .. 80
00 .. 34 11 .. 67 0.6 0.3 11 .. 78 0.6
0.2 1.5 0.4 0.2 1.6 0.4
RTE ReCoRD DROP RTE ReCoRD DROP
00 .. 66 68 22 .. 66 25 50 22 .. 46
0.66
22 .. 55 46 2.6
0.64 2.600 2.2 2.52 2.4
00 .. 66 02 222 ... 555 257 505 12 .. 80 00 .. 66 24 222 ... 445 680 22 .. 02
0.58 2.500 1.6 0.60 2.44 1.8
2.475 1.4 2.42
0 5000 10000 15000 20000 0 5000 10000 15000 20000
number of mini-batch iterations number of mini-batch iterations
(b) Trainingloss (c) Evaluationloss
Figure2.Convergencerateoffullfine-tuningOPT-13Bmodelwithmethods{GAwithconstantLR},{GA,STP,andAS2Pwithcosine
decayLR}onclassificationtasks{SST-2,RTE},multiplechoicetasks{Copa,ReCoRD},andgenerationtasks{SQuAD,DROP}.
4.2.EffectivenessofcomponentsinAS2P rateandAS2Pwithoutprogressiveγ-clipping. Compared
withtheGAmethod,theprogressiveγ-clippingstrategy,i.e.
Automaticlearningrateandprogressiveγ-clipping. Fur-
AS2PW.O.AutoLR,appearstodecreasetheconvergence
ther,weverifytheeffectivenessoftwostrategiesunderpre-
rateduringtheinitialstagesoftrainingbutbringasmoother
trainedResNet18andCIFAR10. Figure7(a)showsthecon-
cure. Incontrast,theautomaticlearningratestrategy,i.e.,
vergencerateofAS2Pwithout(W.O.)automaticlearning
AS2PW.O.γ-clipping,increasestheconvergencerateatthe
7
oitar tsoc
gniniarT
seulav
ssoL
oitar
tsoc
gniniarT
seulav
ssol
seulav
ssol
seulav
ssol
seulav
ssol
seulav
ssoL
seulav
ssol
seulav
ssol
seulav
ssoL
seulav
ssol
seulav
ssol
seulav
ssol
seulav
ssol
seulav
ssoL
seulav
ssol
seulav
ssolStochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
GA 0.0010 50 b=3
AS2P W.O. Auto LR 0.0008 0 2.3 b=5 - Default
2.2 A ASS2 2PP W.O. -clipping 0.0006 50 2.2 b b= =7 10
0.0004 |L R
-
A| -
S
2A PS2P
2.1
b=20
0.0002
2.0 2.0
0.0000
0.0010 50 1.9
1.8 0.0008 0 1.8
0.0006 50 1.7
1.6 0.0004 |L R - G| - A GA 1.6
0.0002
1.5
0 100 N2 u0 m0 ber of epo3 c0 h0 s 400 500 0.0000 0 2500 5000 Num7 b5 e0 r0 of min1 i0 -b0 a0 t0 ch ite1 r2 a5 ti0 o0 ns 15000 17500 20000 0 100 N2 u0 m0 ber of epo3 c0 h0 s 400 500
(a) TheconvergencerateofAS2Pwithout (b) Thedynamicsoflearningrateandγ. (c) Performancecomparisonwithvariousη .
b
automaticlearningrateandwithoutprogres-
siveγ-clipping.
Figure3. VerificationofeffectivenessofcomponentsinAS2P
beginningoftraining. However,itdoesnotshowclearad- large, it is necessary to constrain the step size according
vantagesinthelaterstagesoftraining. TheproposedAS2P to the upper bound of the top eigenvalue of Hessian, the
combinedthetwostrategiesmanagestostrikeabalanceand gradientnorm. Inthiscontext,Theorem3.11suggeststhat
convergesfastthroughouttheentiretrainingphase,referring ηStdDev(γ )canserveasawell-estimatedupperbound
recent
toFigure3(b)forthedynamicsoflearningrateandγ. onthetopeigenvalueofHessian. Consequently,itcanbe
usedtolimitthestepsizeduringoursymmetrictwo-point
Hyper-parameters. Further,AS2Pintroducesextrahyper-
descent,i.e.,α ∝ 1 . Whatmakesthisfinding
parameters,i.e.,ρ end,StdDev(γ recent),η aandη b. Weempir- k ηStdDev(γrecent)
intriguingisthephenomenonthatitintroducesapractical
icallyverifiedthatρ =ρ /10,η =5,andutilizingthe
end 0 a
training acceleration. Existing studies suggest using the
mostrecent10%γ forStdDev(γ )tendtoworkwell
recent
upper bound on the top eigenvalue of Hessian (the gradi-
acrossvariousnetworkarchitecturesanddatasetsincluding
entnorm)tolimitstepsizetosafelytraversenon-smooth
commondeepmodelsandLLMs,asdetailedinTable3and
regions,whilenaivegradientdescentmaybetooaggressive
Table4inAppendixD.1.Moreover,theresearchdelvesinto
under the relaxed smoothness assumption. Our work ad-
investigatingtheimpactofthehyper-parameterη onthe
b vancesthisperspectivebyshowingthatα ∝ 1
convergencerate. Figure3(c)showstheconvergencerateof k ηStdDev(γrecent)
cannotonlyimposeconstraintsonthestepsizealongwith
AS2Papplyingvaryingη underpre-trainedResNet18and
b α ∝ 1 but also accelerate training process in spe-
CIFAR10. Wenotethatη bhasarelativelysignificantinflu- k 1/γk+C
enceontheconvergencerate,whichimpliestheimportance cificregions. Intuitively,alargerstepsizeissafelyexpected
oftheinteractionbetweentheabsolutevalueandstandard whenthelargestHessianissmall.
deviationofγ k. Further,thisworkemphasizestheintegrationbetweenα ,
k
|γ |andStdDev(γ ). Itisnon-trivialtoconsidercap-
k recent
5.ConclusionandDiscussions turingtheinteractionsthroughthelearningprocesstoavoid
tuning hyper-parameters, especially η , for various f or
b
Inthiswork,westudythecomplexityoftheproposedS2P building a more complex relationship instead of such as
methodunderbothgeneralandrelaxedsmoothnessassump- α ∝ 1 only. Thisinvestigationwillleadusto
tionsforgradient-freeoptimization.Ourtheoreticalanalysis
k ηStdDev(γrecent)
ourfuturework.
inducesavariantofS2P,AcceleratedS2P,whichexploits
ournewconvergencepropertiesandincorporatesourtheo-
reticalfindings,thatthestandarddeviationofγmayinclude
second-order information about the objective function f.
EmpiricalexperimentsshowedthattheproposedAS2Pout-
performsallbaselinemethodsbyalargemargin. Wenote
thefollowingimportantpointsofdiscussion.
Justification for ηStdDev(γ ). According to studies
recent
ofclippedgradientdescent(Zhangetal.,2019;Kunstner
etal.,2023),whencrossingthenon-smoothregionsofloss
landscape where the top eigenvalue of Hessian might be
8
seulav
ssoL
etar
gninrael
fo
sulaV
etar
gninrael
fo sulaV
seulav
seulav
seulav
ssoLStochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
ImpactStatements Deng,M.,Wang,J.,Hsieh,C.-P.,Wang,Y.,Guo,H.,Shu,T.,
Song,M.,Xing,E.P.,andHu,Z. Rlprompt: Optimizing
Thispaperpresentsworkwhosegoalistoadvancethefield
discretetextpromptswithreinforcementlearning. arXiv
of Machine Learning. There are many potential societal
preprintarXiv:2205.12548,2022.
consequencesofourwork,noneofwhichwefeelmustbe
specificallyhighlightedhere. Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K. Bert:
Pre-training of deep bidirectional transformers for lan-
References guageunderstanding. arXivpreprintarXiv:1810.04805,
2018.
Agarwal, A., Wainwright, M. J., Bartlett, P., and Raviku-
mar,P. Information-theoreticlowerboundsontheoracle Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S.,
complexityofconvexoptimization. AdvancesinNeural andGardner,M. Drop: Areadingcomprehensionbench-
InformationProcessingSystems,22,2009. markrequiringdiscretereasoningoverparagraphs. arXiv
preprintarXiv:1903.00161,2019.
Bergou,E.H.,Gorbunov,E.,andRichta´rik,P. Stochastic
Duchi,J.C.,Jordan,M.I.,Wainwright,M.J.,andWibisono,
threepointsmethodforunconstrainedsmoothminimiza-
A. Optimalratesforzero-orderconvexoptimization:The
tion. SIAMJournalonOptimization,30(4):2726–2749,
poweroftwofunctionevaluations. IEEETransactions
2020.
onInformationTheory,61(5):2788–2806,2015.
Bergstra, J. and Bengio, Y. Random search for hyper-
Gao,K.andSener,O. Generalizinggaussiansmoothingfor
parameter optimization. Journal of machine learning
randomsearch. InInternationalConferenceonMachine
research,13(2),2012.
Learning,pp.7077–7101.PMLR,2022.
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.,
Ghadimi,S.andLan,G. Stochasticfirst-andzeroth-order
Arora,S.,vonArx,S.,Bernstein,M.S.,Bohg,J.,Bosse-
methodsfornonconvexstochasticprogramming. SIAM
lut,A.,Brunskill,E.,etal. Ontheopportunitiesandrisks
JournalonOptimization,23(4):2341–2368,2013.
offoundationmodels. arXivpreprintarXiv:2108.07258,
2021. Guo,C.,Gardner,J.,You,Y.,Wilson,A.G.,andWeinberger,
K. Simple black-box adversarial attacks. In Interna-
Cai, H., Mckenzie, D., Yin, W., and Zhang, Z. Zeroth- tionalConferenceonMachineLearning,pp.2484–2493.
order regularized optimization (zoro): Approximately PMLR,2019.
sparsegradientsandadaptivesampling. SIAMJournal
onOptimization,32(2):687–714,2022. He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearn-
ingforimagerecognition. InProceedingsoftheIEEE
Chen,Y.,Orvieto,A.,andLucchi,A. Anaccelerateddfo conferenceoncomputervisionandpatternrecognition,
algorithmforfinite-sumconvexfunctions. arXivpreprint pp.770–778,2016.
arXiv:2007.03311,2020.
Hu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,
Choromanski,K.,Rowland,M.,Sindhwani,V.,Turner,R., S.,Wang,L.,andChen,W. Lora:Low-rankadaptationof
andWeller,A. Structuredevolutionwithcompactarchi- largelanguagemodels. arXivpreprintarXiv:2106.09685,
tecturesforscalablepolicyoptimization. InInternational 2021.
ConferenceonMachineLearning,pp.970–978.PMLR,
Ilyas, A., Engstrom, L., Athalye, A., and Lin, J. Black-
2018.
boxadversarialattackswithlimitedqueriesandinforma-
tion. InInternationalconferenceonmachinelearning,
Conn,A.R.,Scheinberg,K.,andVicente,L.N.Introduction
pp.2137–2146.PMLR,2018.
toderivative-freeoptimization. SIAM,2009.
Jamieson,K.G.,Nowak,R.,andRecht,B. Querycomplex-
Dagan,I.,Glickman,O.,andMagnini,B. Thepascalrecog-
ityofderivative-freeoptimization. AdvancesinNeural
nisingtextualentailmentchallenge. InMachinelearning
InformationProcessingSystems,25,2012.
challengesworkshop,pp.177–190.Springer,2005.
Khintchine, A. U¨ber dyadische bru¨che. Mathematische
Danilova,M.,Dvurechensky,P.,Gasnikov,A.,Gorbunov,
Zeitschrift,18(1):109–116,1923.
E.,Guminov,S.,Kamzolov,D.,andShibaev,I. Recent
theoreticaladvancesinnon-convexoptimization.InHigh- Krizhevsky,A.andHinton,G. Learningmultiplelayersof
DimensionalOptimizationandProbability: WithaView featuresfromtinyimages. Technicalreport,University
TowardsDataScience,pp.79–163.Springer,2022. ofToronto,Toronto,Ontario,2009.
9StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
Kunstner,F.,Chen,J.,Lavington,J.W.,andSchmidt,M. Ren, Z., Tang, Y., and Li, N. Escaping saddle points in
Noiseisnotthemainfactorbehindthegapbetweensgd zeroth-orderoptimization: thepoweroftwo-pointesti-
and adam on transformers, but sign descent might be. mators. arXivpreprintarXiv:2209.13555,2023.
arXivpreprintarXiv:2304.13960,2023.
Roemmele, M., Bejan, C. A., and Gordon, A. S. Choice
Lester,B.,Al-Rfou,R.,andConstant,N.Thepowerofscale ofplausiblealternatives: Anevaluationofcommonsense
for parameter-efficient prompt tuning. arXiv preprint causalreasoning.In2011AAAISpringSymposiumSeries,
arXiv:2104.08691,2021. 2011.
Lewis,R.M.,Torczon,V.,andTrosset,M.W. Directsearch
Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever,
methods: thenandnow. Journalofcomputationaland
I. Evolution strategies as a scalable alternative to rein-
AppliedMathematics,124(1-2):191–207,2000.
forcement learning. arXiv preprint arXiv:1703.03864,
2017.
Li, H., Jadbabaie, A., and Rakhlin, A. Convergence
of adam under relaxed assumptions. arXiv preprint
Sener,O.andKoltun,V. Learningtoguiderandomsearch.
arXiv:2304.13972,2023.
arXivpreprintarXiv:2004.12214,2020.
Liu, S., Chen, P.-Y., Chen, X., and Hong, M. signsgd
Shamir,O. Anoptimalalgorithmforbanditandzero-order
viazeroth-orderoracle. InInternationalConferenceon
convexoptimizationwithtwo-pointfeedback. TheJour-
LearningRepresentations,2018.
nal of Machine Learning Research, 18(1):1703–1713,
Lucchi,A.,Orvieto,A.,andSolomou,A. Onthesecond- 2017.
orderconvergencepropertiesofrandomsearchmethods.
AdvancesinNeuralInformationProcessingSystems,34: Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
C.D.,Ng,A.Y.,andPotts,C. Recursivedeepmodelsfor
25633–25645,2021.
semanticcompositionalityoverasentimenttreebank. In
Maheswaranathan,N.,Metz,L.,Tucker,G.,Choi,D.,and Proceedingsofthe2013conferenceonempiricalmethods
Sohl-Dickstein,J. Guidedevolutionarystrategies: Aug- innaturallanguageprocessing,pp.1631–1642,2013.
mentingrandomsearchwithsurrogategradients. InIn-
ternationalConferenceonMachineLearning,pp.4264– Vicente, L. N. Worst case complexity of direct search.
4273.PMLR,2019. EUROJournalonComputationalOptimization,1(1-2):
143–153,2013.
Malladi, S., Gao, T., Nichani, E., Damian, A., Lee,
J. D., Chen, D., and Arora, S. Fine-tuning lan- Wang,B.,Zhang,H.,Ma,Z.,andChen,W. Convergence
guagemodelswithjustforwardpasses. arXivpreprint of adagrad for non-convex objectives: Simple proofs
arXiv:2305.17333,2023. and relaxed assumptions. In The Thirty Sixth Annual
Conference on Learning Theory, pp. 161–190. PMLR,
Mania,H.,Guy,A.,andRecht,B. Simplerandomsearch
2023.
ofstaticlinearpoliciesiscompetitiveforreinforcement
learning. Advances in Neural Information Processing Wang,Y.,Du,S.,Balakrishnan,S.,andSingh,A.Stochastic
Systems,31,2018. zeroth-orderoptimizationinhighdimensions. InInterna-
tionalconferenceonartificialintelligenceandstatistics,
Nesterov,Y.andSpokoiny,V. Randomgradient-freemini-
pp.1356–1365.PMLR,2018.
mizationofconvexfunctions. FoundationsofComputa-
tionalMathematics,17:527–566,2017.
Yang,L.andShami,A. Onhyperparameteroptimization
of machine learning algorithms: Theory and practice.
Prasad, A., Hase, P., Zhou, X., and Bansal, M. Grips:
Neurocomputing,415:295–316,2020.
Gradient-free,edit-basedinstructionsearchforprompting
largelanguagemodels. arXivpreprintarXiv:2203.07281,
Zelikman,E.,Huang,Q.,Liang,P.,Haber,N.,andGood-
2022.
man,N.D. Justonebyte(pergradient): Anoteonlow-
Raginsky, M. and Rakhlin, A. Information-based com- bandwidthdecentralizedlanguagemodelfinetuningusing
plexity,feedbackanddynamicsinconvexprogramming. shared randomness. arXiv preprint arXiv:2306.10015,
IEEETransactionsonInformationTheory,57(10):7036– 2023.
7056,2011.
Zhang,B.,Jin,J.,Fang,C.,andWang,L. Improvedanal-
Rajpurkar,P.,Zhang,J.,Lopyrev,K.,andLiang,P. Squad: ysisofclippingalgorithmsfornon-convexoptimization.
100,000+questionsformachinecomprehensionoftext. AdvancesinNeuralInformationProcessingSystems,33:
arXivpreprintarXiv:1606.05250,2016. 15511–15521,2020.
10StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
Zhang, H. and Gu, B. Faster gradient-free methods for A.Technicallemmas
escaping saddle points. In The Eleventh International
LemmaA.1. ((Zhangetal.,2020)DescentInequality)Sup-
ConferenceonLearningRepresentations,2022.
poseobjectivefunctionf(·)satisfiesAssumption3.5,and
Zhang,J.,He,T.,Sra,S.,andJadbabaie,A. Whygradient c > 0 be a constant. For any x k and x k+1, as long as
clippingacceleratestraining:atheoreticaljustificationfor ||x k−x k+1||≤ Lc 1,wehave
adaptivity. arXivpreprintarXiv:1905.11881,2019.
f(x )≤f(x )+(x −x )T∇f(x ) (1)
k+1 k k+1 k k
Zhang,S.,Liu,X.,Liu,J.,Gao,J.,Duh,K.,andVanDurme,
AL +BL ||∇f(x )||
B.Record:Bridgingthegapbetweenhumanandmachine + 0 1 k ||x −x ||2 (2)
2 k+1 k
commonsense reading comprehension. arXiv preprint
arXiv:1810.12885,2018. whereA = 1+ec− ec−1,B = ec−1. NotethatAandB
c c
aremonotonicallyincreasingfunctionsw.r.t. c>0.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,
Lemma A.2. (Lemma 3.6) For all g ∈ Rd, and random
etal.Opt:Openpre-trainedtransformerlanguagemodels.
vector s ∼ R where R is the Rademacher distribution,
arXivpreprintarXiv:2205.01068,2022.
i.e., each element s ∼ {+1,−1} with equal chances and
E s∼R||s||2
2
=d,thenE s∼R|⟨g,s⟩|≥ √1 2||g|| 2.
Proof.
d
(cid:88)
|⟨g,s⟩|=| g s | (3)
i i
i=1
AccordingtoKhintchineinequality(Khintchine,1923),i.e.,
d d d
A p((cid:88) |g i|2)1 2 ≤(E|(cid:88) g is i|p)p1 ≤B p((cid:88) |g i|2)21
i=1 i=1 i=1
where


√21 2− p1 0<p<p
0
A p = 221(Γ((p+1)/2)/ π)p1 p 0 <p<2

1 2≤p<∞.
(cid:26) 1 0<p≤2
B = √
p 221(Γ((p+1)/2)/ π)p1 2<p<∞.
wherep ≈1.847andΓistheGammafunction,wehave
0
d
1 (cid:88)
√ ||g|| ≤E| g s |≤||g|| ,
2 2 i i 2
i=1
Combinedwithequation3,wehave
1
√ ||g|| ≤E |⟨g,s⟩|≤||g|| .
2 s∼R 2
2
Thiscompletestheproof.
11StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
B.Convergenceanalysisunderthegeneral Option1. Stationarystepsize
smoothnessassumption
Lemma 3 shows that E |∇f(x )Ts | ≥
sk∼R k k
B.1.ProgressiveboundofS2P
√1 ||∇f(x k)|| 2, then inequality (6) can be reformu-
2
latedas
Lemma B.1. (Lemma 3.7) (Progressive bound) Suppose
objective function f(·) satisfies Assumption 3.4 and α Lα2d
E[f(x )|x ]≤f(x )− √ ||∇f(x )|| +
||∇f(x k)|| 2 √≥ ϵ g. If we run algorithm 1 with step k+1 k k 2 k 2 2
size α = 2ϵg, we have following progressive bound
2Ld
E[f(x k+1)−f(x k)|x k]≤−Ω(
Lϵ2
g d),whereE[·|x k]denotes Takingexpectationsintheaboveinequalityw.r.t. s
k
con-
theconditionalexpectationw.r.t. x k. ditional on x k, and denoting θ
k
= E[f(x k+1)] and g
k
=
E[||∇f(x )|| ],wehave
k 2
Proof. Using L-gradient Lipschitz, we have (descent
lemma) αg Lα2d
θ ≤θ − √k +
E[f(x )−f(x )|x ] k+1 k 2 2
k+1 k k
√ θ −θ Lαd
≤E[∇f(x k)T(x k+1−x k)|x k]+ L 2E[||x k+1−x k||2] g k ≤ 2( k αk+1 + 2 )
=−αE|∇f(x )Ts |+
Lα2
E||s ||2 Takeupdatingstep
(cid:88)K
g
≤√ 2(θ 0−θ
k+1 +
KLαd
)
k k 2 k 2 k α 4
k=0
Lα2d
=−αE|∇f(x )Ts |+
k k 2 We can conclude that there exists an iteration j ∼ [0,K]
Lemma 3 shows that E |∇f(x )Ts | ≥ suchthat
sk∼R k k
√1 ||∇f(x k)|| 2,then
2 √ θ −θ Lαd
g ≤ 2( 0 k+1 + )
E[f(x )−f(x )|x ]≤−√α ||∇f(x )|| + Lα2d j αK √2 √
k+1 k k 2 k 2 2 √ (f(x )−f⋆) Kd Lα d α
g ≤ 2( 0 + √0 ) (α= √ 0 )
≤−√α 2ϵ g+ Lα 22d j √
2d
(f(xα )0 −K
f⋆) Lα
2 K Kd
g ≤ √ ( 0 + 0)
To guarantee convergence, α ∼ [0,√ 2ϵg], then suppose j K α 0 2
√ Ld
α= 2L2ϵ dg,wehaveE[f(x k+1)−f(x k)|x k]≤− 4ϵ L2 g d which Thenlet √ √2d((f(x0)−f⋆) + Lα0)≤ϵ,wehave
completestheproof. K α0 2
2d (f(x )−f⋆) Lα
B.2.QuerycomplexityofS2P K ≥ ( 0 + 0)2,
ϵ2 α 2
0
TheoremB.1. (Theorem3.8)(Querycomplexity)Suppose
objectivefunctionf(·)satisfiesAssumption3.4. Ifwerun ,whichcompletestheproofforoption1.
algorithm1withstepsizestrategyoptions1or2,thealgo-
Option2. Dynamicstepsize
rithmreturnsinexpectationanϵ-first-orderstationarypoint
inO( ϵd 2)functionevaluations. Taking expectations in the above inequality (6) w.r.t. s
k
conditionalonx ,anddenotingθ =E[f(x )],wehave
k k k+1
Proof. Using L-gradient Lipschitz, we have (descent
lemma) Lα2d
θ ≤θ −α|∇f(x )Ts |+ (7)
E[f(x )|x ] (4) k+1 k k k 2
k+1 k
≤f(x )+E[∇f(x )T(x −x )|x ] (5)
k k k+1 k k Weknowthatthebestαopt = |∇f(xk)Tsk|,andwecanap-
L k Ld
+ E[||x −x ||2] proximatethebeststepsizewithα = |f(x+ρsk)−f(x−ρsk)|
2 k+1 k k 2ρLd
=f(x k)−αE|∇f(x k)Ts k|+
L 2α2
E||s k||2 2
( isor aα sck a= larα
.
0|f(x+ρsk) 2− ρf(x−ρsk)| whereα 0 = L1 d)whereρ
=f(x )−αE|∇f(x )Ts |+
Lα2d
(6) Beforecontinuingworkingontheinequality(7), weesti-
k k k 2 matetheerrorbetweenthebeststepsizeandtheapproxi-
12StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
matedstepsize,|δ |:=|α −αopt|,firstly. whichfurtherconcludesthatweneed
k k k
|δ k| (8)
K ≥
4Ld(f(x 0)−f⋆)
, (15)
= 2ρ1 Ld(cid:12) (cid:12)|f(x+ρs k)−f(x−ρs k)|−2ρ|∇f(x k)Ts k|(cid:12) (cid:12) ϵ2− ρ 22
iterationstoreachϵ-first-orderstationarypoint(||f(x )||≤
1 j
≤ |f(x+ρs )−f(x−ρs )−2ρ∇f(x )Ts | ϵ).
2ρLd k k k k
√
(9) Meanwhile,werequirethat0<ρ ≤ 2||∇f(xk)|| forρ in
k Ld k
1 eachiterations,anditcanbesettoasmallvalueuniversally.
= 2ρLd|(f(x+ρs k)−f(x)−ρ∇f(x k)Ts k) (10) E.g.,0<ρ≤ √ 2ϵ,thenwehaveK ≥ 4Ld(f(x0)−f⋆).
Ld ϵ2(1− 1 )
−(f(x−ρs )−f(x)+ρ∇f(x )Ts )|
L2d2
k k k Then, we can safely conclude that the algorithm returns
≤ 2ρ1 Ld(L 2ρ2||s k||2+ L 2ρ2||s k||2) (11) i fn une cx tip oe nct ea vt aio lun ata ion nϵ s- ,fi wrs ht i- co hrd ce or ms pt la et ti eo sn ta hr ey pp ro oi on ft fi on roO p( tiϵ od 2 n)
ρ
≤ (12) 2.
2
Notethatinequality(9)appliedreversetriangleinequality C.Convergenceanalysisundertherelaxed
andinequality(11)appliedtheequivalentdefinitionsofL-
smoothnessassumption
smooth function |f(x+ρs )−f(x)−ρ∇f(x )Ts | ≤
k k k
L||ρs ||2. C.1.ProgressiveboundofS2P
2 k
Supposewedotakeα = |f(x+ρsk)−f(x−ρsk)| andsubsti- Lemma C.1. (Lemma 3.9) (Progressive bound) Suppose
k 2ρLd
tute α = αopt +δ , inequality (7) can be reformulated objective function f(·) satisfies Assumption 3.5 and
k k k ||∇f(x )|| ≥ ϵ . If we run algorithm 1 with step size
as k √2 g
α= 2ϵg ,wehavefollowingprogressivebound
θ k+1 ≤θ k−(α kopt+δ k)|∇f(x k)Ts k|+
L(α kopt 2+δ k)2d E[f(x2( kA +L 10 )+ −BL f1ϵ (g x)d
k)|x k] ≤ −Ω( (AL0+ϵ B2 g L1ϵg)d), where
|∇f(x )Ts |2 E[·|x ] denotes the conditional expectation w.r.t. x , and
=θ − k k −δ |∇f(x )Ts | (13) k k
k Ld k k k constantsA=1.01,B =1.01.
|∇f(x )Ts |2 Ldδ2
+ k k +δ |∇f(x )Ts |+ k
2Ld k k k 2 Proof. Givethedecentlemmainequality(1),wehave
|∇f(x )Ts |2 Ldδ2
=θ k− 2Lk
d
k +
2
k E[f(x k+1)] (16)
≤θ −
|∇f(x k)Ts k|2
+
Ldρ2
Applyinequality(12)
≤f(x k)−αE[g kT∇f(x k)] (17)
k 2Ld 8 + AL 0+BL 1||∇f(x k)|| E[α2||g ||2]
||∇f(x )||2 Ldρ2 2 k
≤θ k− 4Ldk + 8 ApplyLemma3 =f(x k)−αE[|sT k∇f(x k)|] (18)
(14)
AL +BL ||∇f(x )||
+ 0 1 k E[α2||s ||2] Updatingstep
Notethatitactuallyputrequirementonρtoguaranteecon- 2 k
α
v √ergence,i.e.,forρ k ineachiterations,weneed0 < ρ ≤ ≤f(x k)− √ 2||∇f(x k)|| (19)
2||∇f(xk)||.
Ld AL +BL ||∇f(x )||
+α2 0 1 k d Lemma3 (20)
Continually,inequality(14)furthercanbere-formulatedas 2
ρ2 Suppose ||∇f(x )|| ≥ ϵ , and to guarantee convergence
||∇f(x k)||2 ≤4Ld(θ k−θ k+1)+
2 α∈[0,
√ 2ϵgk
].
Lg
etα=
√
2ϵg ,wehave
(cid:88)K Kρ2
(AL0+BL1ϵg)d 2(AL0+BL1ϵg)d
||f(x )||2 ≤4Ld(θ −θ )+
k=0 k 0 k+1 2 E[f(x k+1)]≤f(x k)−
4(AL
+ϵ2 g
BL ϵ
)d.
0 1 g
We can conclude that there exists an iteration j ∼ [0,K]
suchthat whichcompletestheproof.
4Ld(θ −θ ) ρ2 4Ld(f(x )−f⋆) ρ2NotethatforthespecificvalueofAandB,wehaveA =
||f(x j)||2 ≤ 0
K
k+1 +
2
≤ K0 + 21+ec− ec c−1,B = ec c−1 and||x k+1−x k|| = ||αs k|| =
13StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
√ √
2ϵg √ ≤ c → c ≥ 2L1ϵg √ → c ≥ suchthat
2(AL0+BL1ϵg) d L1 2(AL0+BL1ϵg) d
√ 21
dB
→ec ≥1+ √1 2d. Itiseasytoseethatsuchcexists,
g j ≤
√2(θ 0−θ K+1)
+ √
Aα2L 0d
wecansafelyconsiderA=1.01,B =1.01forsimplicity ( 2α−Bα2L d)K 2α−Bα2L d
1 1
(under large d) since A and B are expected to be small 2(f(x )−f⋆) Aα2L d
values. ≤ √ 0 + √ 0 (25)
( 2α−Bα2L d)K 2α−Bα2L d
1 1
√
C.2.QuerycomplexityofS2P Supposeα= √2 ,inequality(25)canbereformulated
as
BL1 dK
TheoremC.1. (Theorem3.10)(Querycomplexity)Suppose
√ √
objectivefunctionf(·)satisfiesAssumption3.5. Ifwerun B(f(x )−f⋆)L d AL d
g ≤ √0 √ 1 + √0 √ .
algorithm1withstepsizestrategyoptions3or4,thealgo- j
K− d BL ( K− d)
1
rithmreturnsinexpectationanϵ-first-orderstationarypoint
inO( ϵd 2)functionevaluations. Underthissetting,wecanseethattheg j canbecontinually
decreasedwithatleastK >d,whichfurthershowsthatit
need
√ √
Proof. Givethedecentlemmainequality(1),wehave √ AL d+BL (f(x )−f⋆) d
K ≥( d+ 0 1 0 )2
ϵ
E[f(x )] (21)
k+1
iterationstoreachϵ-first-orderstationarypoint. Then,we
≤f(x )−αE[gT∇f(x )] (22)
k k k cansafelyconcludethatthealgorithmreturnsinexpectation
+
AL 0+BL 1||∇f(x k)||
E[α2||g ||2]
anϵ-first-orderstationarypointinO( ϵd 2)functionevalua-
2 k tions,whichcompletestheproofforoption1.
=f(x )−αE[|sT∇f(x )|] (23)
k k k NotethatforthespecificvalueofAandB,wehaveA =
+α2AL 0+BL 21||∇f(x k)|| E[||s k||2] Updatingstep 1+ √ec− ec c−1,B = ec c−1 √and||x k+1−x k|| = (cid:113)||αs k|| =
√2 ≤ c → c ≥ √2 → ec ≥ 1 + 2. It is
(24) BL1 K L1 B K K
easytoseethatsuchcexists,wecansafelyconsiderA=
1.01,B = 1.01forsimplicity(underlarged)sinceAand
Option1. Stationarystepsize
Bareexpectedtobesmallvalues.
Lemma 3 shows that E |∇f(x )Ts | ≥
sk∼R k k Option2. Dynamicstepsize
√1 ||∇f(x k)|| 2, then inequality (24) can be reformu-
2 Takingexpectationsintheaboveinequality(24)w.r.t. s
latedas k
conditionalonx ,anddenotingθ =E[f(x )],wehave
k k k+1
α
E[f(x k+1)]≤f(x k)− √ 2||∇f(x k)|| θ
k+1
(26)
+α2AL 0+BL 21||∇f(x k)|| d ≤θ k−α|sT k∇f(x k)|+α2AL 0+B √L 21||∇f(x k)|| d
AL + 2BL |sT∇f(x )|
≤θ −α|sT∇f(x )|+α2 0 1 k k d.
Takingexpectationsintheaboveinequalityw.r.t. s con- k k k 2
k (27)
ditional on x , and denoting θ = E[f(x )] and g =
k k k+1 k
E[||∇f(x k)||],wehave It is easy to know that αopt = √|sT∇f(xk)| .
k (AL0+ 2BL1|sT∇f(xk)|)d
α AL +BL g Let|γ |= |f(xk+ρsk)−f(xk−ρsk)|,andweapproximatethe
θ ≤θ − √ g +α2 0 1 kd k 2ρ
g
k(√ 2k α+1 − 2Bαk 2L 1d )2 ≤k
θ k−θ
k+1+2 Aα2 2L 0d b apes pt rost xe ip ms ai tz ie onw eit rh roα rk as=
|δ
k(A |L :=0+ |√ α| 2γ kBk −| L1 α|γ kok p| t)d |.anddenotethe
2(θ −θ ) Aα2L d Before we continue working on the inequality (27), we
g k ≤ √ 2αk −Bαk+ 2L1
d
+ √ 2α−Bα0
2L d
derive the upper bound of |δ k| f (cid:12)or our following an (cid:12)aly-
1 1 sis. Firstly, we denote |ϵ ρ| := (cid:12)|sT∇f(x k)| − |γ k|(cid:12) =
(cid:88)K
g
k
≤
√2(θ 0−θ k+1)
+ √
KAα2L 0d (cid:12) (cid:12)|sT∇f(x k)|−|f(xk+ρsk) 2− ρf(xk−ρsk)|(cid:12) (cid:12)=O(ρ2d3/2)(Tay-
k=0
2α−Bα2L 1d 2α−Bα2L 1d lor expansion). So that, we can define |ϵ ρ| ≤ ξρ2d3/2
whereξ isaconstantassociatedwiththird-orderproperty
We can conclude that there exists an iteration j ∼ [0,K] off. Noted3/2isthecompensationofnormalizings.
14StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
Specifically, we try to prove |δ | ≤ |ϵ |. We define a whichconcludesthatweneed
k ρ
newfunctiong(x) = √x , thentoprove|δ k| ≤
AL0+ 2BL1x 8d(f(x )−f⋆)
|ϵ ρ| is equivalent to prove |g(|sT∇f(x k)|) − g(|γ k|)| ≤ K ≥ 0 (34)
(cid:12) (cid:12) ϵ
d(cid:12)|sT∇f(x k)| − |γ k|(cid:12), further it is equivalent to prove
g′(x) = A√L0 ≤ d when x ≥ 0, which is iterationstoreachϵ-first-orderstationarypoint.
(AL0+ 2BL1x)
obviously true. Overall, we have approximation error
Condition2
|δ |≤ξρ2d3/2.
k √
Suppose 1 − 2BL ≥ 0 and ||∇f(x )|| ≤
Then,wecontinueouranalysis. Supposewedotakestep √ 1 k
AL + 2BL ||∇f(x )||, we have ||∇f(x )|| ≤
s thiz ee nα ink e= qua(A litL y0+ (2√ 7| 2γ )Bk c| L a1 n|γ bk e|) rd e-a fn od rmsu ub ls at ti etu at seα k =α kopt+δ k, 1−√A0 L 2B0 L1. Me1 anwhile,k suppose |δ k| ≤ ξρ2dk 3/2 ≤
√||∇f(xk)|| , then inequality (33) can be re-
θ (28)
2(AL0+ 2BL1||∇f(xk)||)d
k+1 formulatedas
≤θ −(αopt+δ )|sT∇f(x )| (29)
k k k k √k ||∇f(x )||2
AL + 2BL |sT∇f(x )| θ ≤θ − √ k
+(α kopt+δ k)2 0 21 k k d k+1 k 8(AL 0+ 2BL 1 1−√AL 2B0 L1)d
=θ k−
(AL
0+| √|s 2T B∇ Lf 1( |x sk T) ∇||2
f(x k)|)d
(30) ||∇f(x k)||2 ≤(θ k−θ k+1) 1−8A √L 20 Bd
L 1
−|sT∇f(x )|δ +
|| √sT∇f(x k)||2 (cid:88)K
||∇f(x )||2 ≤(θ −θ )
8A √L 0d
k k k 0 k+1
2(AL + 2BL |sT∇f(x )|)d 1− 2BL
√ 0 1 k k=0 1
AL + 2BL |sT∇f(x )|
+ 0 1 k k dδ2+|sT∇f(x )|δ
2 k k k We can conclude that there exists an iteration j ∼ [0,K]
||sT∇f(x )||2 suchthat
≤θ − √ k (31)
k
2(AL 0+
√
2BL 1|sT∇f(x k)|)d
||∇f(x )||2 ≤
8AL 0d √(θ 0−θ k+1)
+
(AL 0+ 2BL 1|sT k∇f(x k)|)d
δ2
j (1− 2BL 1)K
2 k (cid:115)
8AL d(f(x )−f⋆)
||∇f(x )||2 ||∇f(x )||≤ 0 √ 0 ,
≤θ k−
4(AL
+√ 2BLk
||∇f(x )||)d
(32) j (1− 2BL 1)K
0 1 k
√
+ (AL 0+ 2BL 1||∇f(x k)||)d δ2 Lemma3 whichconcludesthatweneed
2 k
(33) 8AL d(f(x )−f⋆)
K ≥ 0 √ 0
(1− 2BL )ϵ2
1
Condition1
√
iterationstoreachϵ-first-orderstationarypoint.
Suppose 1 − 2BL ≥ 0 and ||∇f(x )|| ≥ AL +
√ 1 k 0
2BL 1||∇f(x k)||, inequality (33) can be reformulated Condition3
as √
Suppose 1 − 2BL ≤ 0 and ||∇f(x )||2 ≤
||∇f(x )|| ||∇f(x )||d 1 k
θ
k+1
≤θ k− 4dk + 2k δ k2 ( 1−√AL 2B0 L1)2. Meanwhile, suppose |δ k| ≤ ξρ2d3/2 ≤
√||∇f(xk)|| , then inequality (33) can be re-
Meanwhile,suppose|δ k|≤ξρ2d3/2 ≤ 21 d,wehave f2 o(A rmL u0+ late2 dB aL s1||∇f(xk)||)d
||∇f(x )||≤8d(θ −θ )
k k k+1
||∇f(x )||2
(cid:88)K
||∇f(x k)||≤8d(θ 0−θ k+1)
θ
k+1
≤θ k−
8(AL
0+√
2BL
1k
| √AL0 |)d
√1− 2BL1
k=0
8AL d(2 2BL −1)
We can conclude that there exists an iteration j ∼ [0,K] ||∇f(x )||2 ≤(θ −θ ) 0√ 1
k k k+1
2BL −1
suchthat 1
√
K
||∇f(x j)||≤
8d(θ
0
K−θ k+1) (cid:88)
||∇f(x k)||2 ≤(θ 0−θ
k+1)8AL 0√d( 22 BL2B −L 11−1)
k=0 1
8d(f(x )−f⋆)
||∇f(x )||≤ 0
j K We can conclude that there exists an iteration j ∼ [0,K]
15StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
suchthat C.3.BoundofgradientnormofS2P
√
||∇f(x )||2 ≤ 8AL 0d(θ 0√−θ k+1)(2 2BL 1−1) TheoremC.2. (Theorem3.11)Supposeobjectivefunction
j
( 2BL −1)K f(·) satisfies Assumption 3.5. Then the gradient norm
1
(cid:115) √ ||∇f(x )||canbeboundedinexpectationas
8AL d(f(x )−f⋆)(2 2BL −1) k
||∇f(x )||≤ 0 √0 1 , √
j
( 2BL 1−1)K |γ|−ρd(AL 0+BL 1||∇f(x)||)≤||∇f(x)||≤ 2|γ|
√
whichconcludesthatweneed + 2ρd(AL +BL ||∇f(x)||)
0 1
√
8AL d(f(x )−f⋆)(2 2BL −1)
K ≥ 0 √0 1 (35) where|γ| = |f(x+ρs)−f(x+ρs)|. ConstantsA = 1.01,B =
( 2BL −1)ϵ2 2ρ
1 1.01whenρ≤ 0.00√1
iterationstoreachϵ-first-orderstationarypoint.
2L1 d
Condition4 Proof.
√
Suppose 1 − 2BL ≤ 0 and ||∇f(x )||2 ≥ √
( √AL0 )2. Meanwh1 ile, suppose δ
k
≤ ξρk 2d3/2 ≤ ||∇f(x)||≤E[ 2|sT∇f(x)|] (37)
1− 2BL1
1
√||∇f(xk)|| , then inequality (33) can be re- =E[√ |2ρsT∇f(x)|] (38)
2(AL0+ 2BL1||∇f(xk)||)d 2ρ
formulatedas
( √AL0 )2
=E[√1 2ρ|(cid:0) f(x+ρs)−f(x−ρs)(cid:1)
(39)
θ ≤θ − √1− 2BL1 (36)
k+1 k 8(AL 0+ 2BL 1||∇f(x k)||)d −(cid:0) f(x+ρs)−f(x−ρs)−2ρsT∇f(x)(cid:1) |]
Since
8(AL0+(
√1−
2B√AL L2B0
1L ||1
∇) f2
(xk)||)d
is a monotone decreasing
≤E[√ 2|f(x+ρs) 2− ρf(x−ρs)|
(40)
function w.r.t. ||∇f(x k)||, then we can conclude thatthe
+
√1 |f(x+ρs)−f(x−ρs)−2ρsT∇f(x)(cid:1)
|]
lossfunctioncannotbeindicatorofreachingϵ-first-order
2ρ
stationarypoints. However,withanappropriateselectionof √ 1
parameters,thelossfunctioncanbeminimized. I.e., = 2|γ|+ √ E[|f(x+ρs)−f(x−ρs) (41)
2ρ
(√ AL0 )2 −2ρsT∇f(x)(cid:1)
|]
θ ≤θ − √2BL1−1
k+1 k 8(AL 0+ 2BL 1√ 2BAL L0 1−1)d ≤√
2|γ|+
√1 AL 0+BL 1||∇f(x)||
E[||2ρs||2] (42)
AL 2ρ 2
θ ≤θ − √ 0√ √ √
k+1 k
8(2 2BL −1)( 2BL −1)d = 2|γ|+ 2ρd(AL +BL ||∇f(x)||).
1 1 0 1
AL
θ ≤θ −(K+1) √ 0√
k+1 0
8(2 2BL 1−1)( 2BL 1−1)d Noteinequality(38)appliesLemmaA.2, inequality(42)
AL appliesLemmaA.1. Andthesamewiththefollowingproof.
θ −f⋆ ≤θ −f⋆−K √ 0√ ,
k 0
8(2 2BL −1)( 2BL −1)d
1 1
1
||∇f(x)||≥E[|sT∇f(x)|]=E[ |2ρsT∇f(x)|]
whichconcludesthatweneed 2ρ
√ √
K ≥
8(2 2BL 1−1)( 2BL 1−1)(f(x 0)−f⋆−ϵ)d =E[ 21 ρ|(cid:0) f(x+ρs)−f(x−ρs)(cid:1)
AL
0 −(cid:0) f(x+ρs)−f(x−ρs)−2ρsT∇f(x)(cid:1)
|]
iterationstoreach“localϵ-optimalpoint”(localminimum
|f(x+ρs)−f(x−ρs)| 1
orsaddlepoint). ≥E[ − |f(x+ρs)
2ρ 2ρ
WesummarizetheresultsoverallconditionsinTable2.
−f(x−ρs)−2ρsT∇f(x)(cid:1)
|]
NotethatforthespecificvalueofAandB,wehaveA =
1+ec− ec−1,B = ec−1 and||x −x || = ||αs || = =|γ|− 1 E[|f(x+ρs)−f(x−ρs)−2ρsT∇f(x)(cid:1) |]
c c k+1 k k 2ρ
√γk √ ≤ c → c ≥ √1 → ec ≥ 1+ √1 .
I( tA iL s0 e+ asy2B toL1 sγ ek e) thd atsuL c1 hcexists,B we2d cansafelyconsid2 ed r ≥|γ|− 1 AL 0+BL 1||∇f(x)|| E[||2ρs||2]
2ρ 2
A=1.01,B =1.01forsimplicity(underlarged)sinceA
≥|γ|−ρd(AL +BL ||∇f(x)||).
andBareexpectedtobesmallvalues. 0 1
16StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
Conditions[b] requirementoverρ[a] Querycomplexity
L 1 ≤ √1 2B,||∇f(x)||≥ 1−√AL 2B0 L1 ρ≤ d√ 21 ξ√ d 8d(f(x ϵ0)−f⋆)
L L1
1
≤
≥
√ √1 12 2B B, ,| || |∇ ∇f f( (x x) )| || |≤
≤
1 √− 2√ BA AL L2 LB0
0
1−L1
1
ρ ρ≤
≤
d d1 1(cid:113) (cid:113)2 2ξ ξ( (A AL L0 0+ +√ √ϵ ϵ2 2B BL L1 1ϵ ϵ) )√ √d
d
8AL0d8 (fA ( ((L √1 x−0
0
2d )√ B−(f 2 LfB( 1⋆x −L )0 (1 1) 2− )) √ϵ ϵf 2 22⋆ B)
L1−1)
L 1 ≥ √1 2B,||∇f(x)||≥ √ 2BAL L0 1−1 ρ≤ d1(cid:113) 2ξ(AL0+√ϵ 2BL1ϵ)√ d 8(2√ 2BL1−1)(√ 2B AL L1 0−1)(f(x0)−f⋆−ϵ)d
[a]ξisaconstantassociatedwiththird-orderpropertyoff,detailedinappendixinequality(27).
[b]Forthefourthcondition,decreasinglossvalueinsteadofgradientnorm,detailedinappendixinequality(36).
Table2. Withdynamicstepsizestrategy,theconvergencepropertyoff underrelaxedsmoothness.
NotethatforthespecificvalueofAandB,wehaveA = For the experiment over LLM, the six text tasks follow
1+ec−ec−1,B = ec−1 and||x −x ||=||(x+ρs)− theoriginalsettingsexactly(Malladietal.,2023), which
c c √ k+1 k √
(x−ρs)|| = ||2ρs|| = 2ρ d ≤ c → c ≥ 2ρL d. It randomly samples 1,000 examples and 500 examples for
is easy to see that such c exists,L w1 e can safely c1 onsider trainingandvalidationrespectivelyforeachtask. Weget
ρ ≤ 1 , thenwehavec ≥ √1 . Itiseasytoseesuchc theresultswithafixedrandomseed. Specifically,forthe
exists2 ,L w1d
esetA=1.01,B
=1.0d
1forsimplicity. learningrateandsmoothingparameter,weapplythebest
values mentioned in Malladi et al., 2023 for GA. Then,
AS2Pdirectlyappliesthevalueofsmoothingparameterρ
D.Experiments 0
fromGAandonlyneedstotuneonehyper-parameter η .
b
ForSTPmethod,wesearchthebestα fromlist{5e-5,2e-
D.1.Setup 0
5,1e-5,5e-6,1e-6,1e-7}. Thedetailsofhyper-parameters
Forexperimentovercommondeepmodelsanddatasets,we are summarized in Table 4, which shows that only η is
b
do grid search for initial learning rate α 0 over list {2e-4, necessarytoupdateamongallfourextrahyper-parameters
1e-4,8e-5,5e-5,2e-5,1e-5}andforsmoothingparameter ρ , η , η , andStdDev(γ ) ofAS2Pcompared with
end a b recent
ρ 0overlist{1e-3,5e-4,1e-4,5e-5,1e-5}withallmethods. experimentsaboutcommondeepmodels&datasets.
Weaveragetheresultsacross5randomseeds.
Notetheselectedhyper-parametersdirectlyapplytosign Hyper-parameter Task Method
GA GAconstant AS2P STP
variants. Thetunablehyper-parametersaresummarizedin
SST-2
Table3. RTE
Copa
α0
ReCoRD
1e-7 1e-7 - 2e-5
SQuAD
DROP
Hyper-parameter Arc.&Dataset Method LRscheduler All Cosinedecay Constantvalue Cosinedecay Cosinedecay
GA AS2P STP
ρ0 All 1e-3 1e-3 1e-3 -
α0
R
RR R
e es
se e
N
Ns sN N
e et
te e
1
1t t
0
51 5
1
28 0
&
&& &
C
CC C
I
II I
F
FF F
A
AA A
R
RR R
1
11 1
0
00 0
0
0
2 1
2
2. .
.
.0 0
0
0e e
e
e- -
-
-5 5
5
5
- -
-
-
2 2
1
1. .
.
.0 0
0
0e e
e
e- -
-
-4 4
4
4
StdDeρ vη ηe (a bn γd
recent)
A A
A
Al l
l
ll l
l
l
- -
-
-
- -
-
-
ρ0
15
0/ 5
0
%10 - -
-
-
LRscheduler All Cosinedecay
ResNet18&CIFAR10 1e-3 1e-3 - Table4.Summaryofhyper-parametersusedinexperimentsover
ρ0 RR ese NsN ete 1t 05 10 && CC II FF AA RR 11 00
0
1 5e e- -3
4
5 5e e- -4
4
-
-
LLM.Basically,AS2Pneedstotuneη b,andtheselectedvalues
ResNet152&CIFAR100 5e-4 5e-4 - arerobustacrossvaryingtasks.
ρend All - ρ0/10 -
ηa All - 5 -
D.2.AdditionalExperiments
ResNet18&CIFAR10 - 5 -
ResNet50&CIFAR10 - 5 -
ηb ResNet101&CIFAR100 - 3 - TableversionofFigure1(a)andFigure2(a). Thebaseof
ResNet152&CIFAR100 - 5 - trainingcostratio,e.g.,{1,0.8,0.6,0.4,0.2},normalizesthe
StdDev(γrecent) All - 10% -
numberoffunctionquerieswhenbasemethodGAreaches
{500,400,300,200,100}epochswithsomespecificloss
Table3.Summaryofhyper-parametersusedinexperimentsover
values. Then, the training cost ratio aligns with the ratio
commondeepmodelsanddatasets. ItshowsthatAS2Phasex-
trahyper-parametersρ ,η ,η ,andStdDev(γ ).Basically, betweenthenumberoffunctionqueriesofthebasemethod
end a b recent
thosehyper-parametersareunnecessarytotunewithinabovedeep andothermethodsreachingthesamelossvalues.
modelsanddatasets.
17StochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
Figure4.PerformancecomparisonbetweenapplyingdifferentnoisedistributionssuchasNormaldistribution,Rademacherdistribution,
andUniformdistribution.
2.2
A A AS S S2 2 2P P P_ _ _s G ui nag iun fos rs mian 22 .. 23 A A AS S S2 2 2P P P_ _ _s G ui nag iun fos rs mian 4.60 A A AS S S2 2 2P P P_ _ _s G ui nag iun fos rs mian 4.60 A A AS S S2 2 2P P P_ _ _s G ui nag iun fos rs mian
2.1 4.55 4.55
2.0 2.0 4.50 4.50
1.9 4.45 4.45
1.8 1.8 4.40
4.40
1.7 4.35
1.6 1.6 4.35 4.30
1.5 4.30
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Number of epochs Number of epochs Number of epochs Number of epochs
(a) Under pre-trained(b) Under pre-trained(c) Under pre-trained(d) Under pre-trained
ResNet18&CIFAR10 ResNet50&CIFAR10 ResNet101&CIFAR100 ResNet152&CIFAR100
Figure5. Convergencerateofpre-trainedResNet18&CIFAR100andpre-trainedResNet50&CIFAR100.
AS2P 4.65 AS2P
4.65 GA GA
STP 4.60 STP
4.60
4.55
4.55
4.50
4.50
4.45
4.45
4.40
4.40
4.35 4.35
4.30 4.30
0 100 200 300 400 500 0 100 200 300 400 500
Number of epochs Number of epochs
(a) pre-trainedResNet18&CIFAR100 (b) pre-trainedResNet50&CIFAR100
Figure6.Verificationofeffectivenessofproposedmethodunderpre-trainedResNet101&CIFAR100.Left-sidefiguredemonstratedthe
convergencerateofAS2Pwithout(W.O.)automaticlearningrateandwithoutprogressiveγ-clipping.Right-sidetwofiguresdemonstrate
thedynamicsoflearningrateandγ;
GA 0.0010 50
4.60 v vS S2 2P P W W.. OO .. Au-c tl oip Lp Ring 0.0008 0
vS2P 0.0006 50
4.55 0.0004 |LR| - AS2P
- AS2P
0.0002
4.50
0.0000
0.0010 50
4.45
0.0008 0
0.0006 50
4.40
0.0004 |LR| - GA
- GA
4.35 0.0002
0.0000
0 100 200 300 400 500 0 2500 5000 7500 10000 12500 15000 17500 20000
Number of epochs Number of mini-batch iterations
(a) Convergenceratecomparison (b) Learningratecomparison
18
seulav
ssoL
seulav
ssoL
seulav
ssoL
seulav
ssoL
seulav
ssoL
seulav
ssoL
etar gninrael
fo
sulaV
etar
gninrael
fo
sulaV
seulav
ssoL
seulav
seulavStochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
Figure7.UnderResNet18andCIFAR10,theperformanceofGAwiththedifferentnumberofsymmetricperturbationsforeachupdate.
Theleft-sidefigureshowsperformanceunderthevaryingnumberofsymmetricrandomperturbationsperupdatewherethenumberof
functionqueriesforeachsettingisthesame.Theright-sidefiguredemonstratesthatundervaryingtrainingsettings,theconvergenceof
GAwith10symmetricrandomperturbationsforgradientapproximationperupdate. Basically,wecanconcludethatonesymmetric
symmetricrandomperturbationperupdateconvergestosmallerlossvaluesunderthesamenumberoffunctionqueries.
2.4
GA
2.3 GA_2
2.3
GA_4
2.2 GA_10
2.2
2.1
2.1
2.0
2.0
1.9
GA_10
1.8 1.9 GA_10_LR_5e-5_eps_1e-3
GA_10_LR_2e-5_eps_2e-3
1.7 1.8 GA_10_LR_5e-5_eps_2e-3
GA_10_LR_1e-5_eps_1e-3
1.6 1.7 GA_10_LR_2e-5_eps_5e-4
GA_10_LR_1e-5_eps_5e-4
0 100 200 300 400 500 0 10 20 30 40 50
Number of epochs Number of epochs
(a) SameinitialLRandLRscheduler (b) VaryingαandρunderGA 10.
Figure8.PerformancecomparisonunderVGG11andCIFAR10.Theleft-sidefiguredemonstratesthedynamicsoftrainingloss;The
right-sidefiguredemonstratesthetrainingcostratioofreachingthesamespecificlossvalues.TheproposedmethodAS2Pconverges
fasterthanotherbaselinemethodsandnearlyrequires0.5×numberofqueriestoreachthesamespecificlossvalues. Notethatthe
hyper-parametersdirectlyfollowthesettingofReSNet18&CIFAR10inTable3.
2.4 A STS P2P GA 1.0 A STS P2P GA 1 11
2.2
0.8 0.8
2.0 0.688
0.6 0.6
1.8
0.462 0.478
1.6 0.4 0.4004.4 0.382
0.248
1.4
0.2 0.2
0.1102.13
1.2
0.0
0 100 200 300 400 500 1.557 1.320 1.234 1.203 1.198
Number of epochs Loss values
19
seulav
ssoL
seulav
ssoL
seulav
ssoL
oitar
tsoc
gniniarTStochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
Figure9.Performancecomparisonwithvariousbaselinesundercommondeepmodels&datasetswherethex-axisisthenumberof
functionqueries.ThisfigureisadoptedfromFigure1.
ResNet18&CIFAR10 ResNet50&CIFAR10 ResNet101&CIFAR100 ResNet152&CIFAR100
4.60 4.60
2.2 2.2
4.55 4.55
2.0 2.0 4.50 4.50
4.45
4.45
1.8 1.8 4.40
4.40
4.35
1.6 1.6 4.35 4.30
0 20000 40000 60000
Number of function query
AS2P AS2P_sign STP GA_sign GA
GA constant LR GA cosine decay LR AS2P cosine decay LR STP cosine decay LR GA constant LR GA cosine decay LR AS2P cosine decay LR STP cosine decay LR
SST2 Copa SQuAD SST2 Copa SQuAD
00 .. 56 50 22 .. 12 01 .. 90 00 .. 55 05 22 .. 12 01 .. 90
000000 ...... 233445 505050 111112 ...... 567890 00000 ..... 45678 000000 ...... 223344 050505 11112 ..... 67890 00000 ..... 45678
0.68 RTE 2.64 ReCoRD 2.1 DROP RTE 2.54 ReCoRD 2.3 DROP
00000 ..... 56666 80246 22222222 ........ 45555566 80246802 1111112 ....... 4567890 00000000 ........ 66666666 01234567 222222 ...... 444455 246802 111222 ...... 789012
1000 2000 3000 4000 1000 2000 3000 4000
number of function query number of function query
(b) Trainingloss (c) Evaluationloss
Figure10.Performancecomparisonwithfullfine-tuningOPT-13Bmodelwherethex-axisisthenumberoffunctionqueries.Thisfigure
isadoptedfromFigure2.
ResNet18&CIFAR10 ResNet101&CIFAR100
2.3
4.60
2.2
2.1 4.55
2.0
4.50
1.9
1.8 4.45
1.7
4.40
1.6
0 10 20 30 40 50
Number of epochs AS2P STP GA
Figure11.CorrespondingvalidationperformanceofFigure1(b)undersettingResNet18&CIFAR10andResNet101&CIFAR100.Using
oneseedonly.
20
seulav
ssol
seulav
ssol
seulav
ssoL
seulav
ssoL
seulav
ssol
seulav
ssolStochasticTwoPointsMethodforDeepModelZeroth-orderOptimization
Task Method Trainingcostratio
GA 1 0.80 0.60 0.40 0.20
ResNet18&CIFAR10 STP 1 1 0.80 0.43 0.17
AS2P 0.56 0.52 0.39 0.24 0.10
GA 1 0.80 0.60 0.40 0.20
ResNet50&CIFAR10 STP 0.98 0.87 0.54 0.14 0.05
AS2P 0.40 0.36 0.27 0.14 0.05
GA 1 0.80 0.60 0.40 0.20
ResNet101&CIFAR100 STP 0.93 0.85 0.61 0.37 0.14
AS2P 0.39 0.36 0.28 0.17 0.07
GA 1 0.80 0.60 0.40 0.20
ResNet152&CIFAR100 STP 0.63 0.59 0.45 0.25 0.10
AS2P 0.32 0.31 0.24 0.14 0.06
Table5.Trainingcostratioofreachingspecificlossvaluesunder
commondeepmodels&datasets.
Task Method Trainingcostratio
GAcosinedecayLR 1 0.75 0.50 0.25
GAconstantLR 0.47 0.50 0.38 0.23
SST-2
STPcosinedecayLR 0.38 0.38 0.30 0.19
AS2PcosinedecayLR 0.17 0.17 0.15 0.10
GAcosinedecayLR 1 0.75 0.50 0.25
GAconstantLR 0.35 0.35 0.30 0.12
RTE
STPcosinedecayLR 0.03 0.03 0.03 0.03
AS2PcosinedecayLR 0.03 0.03 0.03 0.03
GAcosinedecayLR 1 0.75 0.50 0.25
GAconstantLR 0.47 0.47 0.40 0.23
Copa
STPcosinedecayLR 0.26 0.26 0.23 0.11
AS2PcosinedecayLR 0.10 0.10 0.07 0.05
GAcosinedecayLR 1 0.75 0.50 0.25
GAconstantLR 0.42 0.42 0.42 0.23
ReCoRD
STPcosinedecayLR 0.23 0.23 0.19 0.07
AS2PcosinedecayLR 0.05 0.05 0.05 0.03
GAcosinedecayLR 1 0.75 0.50 0.25
GAconstantLR 0.65 0.65 0.53 0.30
SQuAD
STPcosinedecayLR 0.23 0.23 0.23 0.15
AS2PcosinedecayLR 0.15 0.15 0.12 0.07
GAcosinedecayLR 1 0.75 0.50 0.25
GAconstantLR 0.42 0.47 0.40 0.23
DROP
STPcosinedecayLR 0.34 0.34 0.30 0.19
AS2PcosinedecayLR 0.15 0.17 0.15 0.10
Table6.Trainingcostratioofreachingspecificlossvalueswhen
fullyfine-tuningOPT-13Bmodelundervarioustasks.
21