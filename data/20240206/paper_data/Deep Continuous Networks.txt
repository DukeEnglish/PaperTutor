Deep Continuous Networks
NergisTomen1 SilviaL.Pintea1 JanC.vanGemert1
Abstract calcircuits(Schrimpfetal.,2018),andemployCNNsas
modelsofbiologicalvision(Zhuangetal.,2020a). Specif-
CNNs and computational models of biological
ically,recentadvancesinCNNshaveenabledresearchers
visionsharesomefundamentalprinciples,which
to learn more accurate models of the response properties
openednewavenuesofresearch. However,fruit-
ofneuronsinthevisualcortex(Klindtetal.,2017;Cadena
ful cross-field research is hampered by conven-
etal.,2019;Eckeretal.,2019),aswellastotestdecadesold
tionalCNNarchitecturesbeingbasedonspatially
hypothesesfromneuroscienceinthedomainofcomputer
and depthwise discrete representations, which
vision(Lindseyetal.,2019). Hence,linksbetweenCNNs
cannot accommodate certain aspects of biolog-
andbiologicalmodelsfromneuroscienceisfruitfulforboth
icalcomplexitysuchascontinuouslyvaryingre-
researchfields.
ceptivefieldsizesanddynamicsofneuronalre-
sponses. Hereweproposedeepcontinuousnet- Contrarytomanybiologicalmodels,feed-forwardCNNs
works(DCNs),whichcombinespatiallycontinu- typically use spatio-temporally discrete representations:
ousfilters,withthecontinuousdepthframework CNNs employ spatially discretized, pixel-based kernels,
of neural ODEs. This allows us to learn the andinputisprocessedthroughadepthwise-discretepipeline
spatial support of the filters during training, as made up of successive convolutional layers. To clarify,
well as model the continuous evolution of fea- withinourframeworkweconsiderCNNdepthtobeanal-
ture maps, linking DCNs closely to biological ogous to time, similar to input-processing time-course in
models. We show that DCNs are versatile and biologicalmodels. UnlikeCNNs,large-scale,neuroscien-
highlyapplicabletostandardimageclassification tificneuralnetworkmodelsofthevisualsystemoftenadopt
andreconstructionproblems,wheretheyimprove continuous, closed-form expressions to describe spatio-
parameteranddataefficiency,andallowformeta- temporalreceptivefields,aswellastheinteractionstrength
parametrization. Weillustratethebiologicalplau- betweenpopulationsofneurons(Dayan&Abbott,2001).
sibilityofthescaledistributionslearnedbyDCNs Amongothers,suchdescriptionsservetolimitthescopeand
andexploretheirperformanceinaneuroscientifi- parameterspaceofamodel,byutilizingpriorinformation
callyinspiredpatterncompletiontask.Finally,we regardingreceptivefieldshapes(Jones&Palmer,1987)and
investigateanefficientimplementationofDCNs principlesofperceptualgrouping(Li,1998). Inaddition,
bychanginginputcontrast. the choice of continuous—and often analytic—functions
helpretainsomeanalyticaltractabilityincomplexmodels
involvingalargenumberofcoupledpopulations. Ourap-
proachdrawsinspirationfromsuchcomputationalmodels
1.Introduction
toproposecontinuousCNNs.
Computational neuroscience and computer vision have a
InthisworkweaimforabiologicallymoreplausibleCNN
longandmutuallybeneficialhistoryofcross-pollinationof
model: We bring together (a) spatially continuous recep-
ideas(Cox&Dean,2014;Sejnowski,2020). Thecurrent
tivefields,whereboththeshapeandthescaleofthefilters
state-of-the-artincomputervisionreliesheavilyonconvolu-
aretrainableinthecontinuousdomain,and(b)depthwise
tionalneuralnetworks(CNNs),fromwhichmultipleanalo-
continuousrepresentationscapableofmodelingthecontinu-
giescanbedrawntobiologicalcircuits(Kietzmannetal.,
ousevolutionofneuronalresponsesinfeed-forwardCNNs.
2018).Thus,basedonrecentdevelopmentsindeeplearning,
Continuous receptive fields provide a link between mod-
therehasbeenagrowingtrendtorelateCNNstobiologi-
ernCNNsandlarge-scalerate-basedmodelsofthevisual
1Computer Vision Lab, Delft University of Technology, system(Ernstetal.,2001). Inaddition,recentinfluential
Delft, Netherlands. Correspondence to: Nergis Tomen workindeeplearninghasintroducedneuralordinarydif-
<n.tomen@tudelft.nl>. ferential equations (ODEs) (Lu et al., 2018; Chen et al.,
2018;Ruthotto&Haber,2019)whichproposeacontinuous
Proceedings of the 38th International Conference on Machine
depth(ortime)interpretationofCNNs,whilehavingspa-
Learning,PMLR139,2021.Copyright2021bytheauthor(s).
4202
beF
2
]VC.sc[
1v75510.2042:viXraDeepContinuousNetworks
tiallydiscretefilters. Suchcontinuousdepthmodelsboth CNNarchitectureswiththespatio-temporallycontinuous
offerend-to-endtrainingcapabilitieswithbackpropagation modelsofbiologicalvision. Ourapproachmakesitpossi-
whicharehighlyapplicabletocomputervisionproblems bletooptimizethespatialextent(kernelsize)ofthefilters
(e.g. bywayofadoptingResNetblocks(Heetal.,2016)), duringtraining,aswellasexplicitlymodelthedynamicsof
as well as help bridge the gap to computational biology theneuronalresponsestoinputimages.
wherenetworksareoftenmodelledasdynamicalsystems
Structuredreceptivefields.Classicalreceptivefields(RFs)
whichevolveaccordingtodifferentialequations. Building
of cortical neurons display complex response properties
on this, we introduce deep continuous networks (DCNs),
withawidearrayofselectivitystructuresalreadyatearly
which are spatially and depthwise continuous in that the
visual areas (Van denBerghet al., 2010). Such response
neuronshavespatiallywell-definedreceptivefieldsbased
propertiesmayalsovarygreatlybasedonmultiplefactors.
on scale-spaces and Gaussian derivatives (Florack et al.,
For example the RF size (spatial extent) is known to de-
1996)andtheiractivationsevolveaccordingtoequationsof
pend on eccentricity (Harvey & Dumoulin, 2011), visual
motioncomprisingconvolutionallayers. Thus,wecombine
area(Smithetal.,2001)andevencorticallayer(Baueretal.,
depthwiseandspatialcontinuitybyemployingneuralODEs
1999). Similarly,studieshaveshownthatspatialfrequency
inanetworkwhichlearnslinearweightsforasetofanalytic
selectivityandreceptivefieldsizemayco-varywithinput
basisfunctions(asopposedtopixel-basedweights),which
contrast(Sceniaketal.,2002).
canalsointuitivelybeparametrizedasafunctionofnetwork
depth(ortime). Basedontheseobservations,weaimtobuildamodelwhich
can accommodate the biological realism better than con-
Our main contributions are: (i) We provide a theoretical
ventionalCNNs,byexplicitlymodellingtheRFsizeasa
formulationofdeepnetworkswithspatiallyanddepthwise
trainableparameter. Tothatend,weadoptaGaussianscale-
continuousrepresentations,buildingonGaussianbasisfunc-
space representation for the convolutional filters, which
tionsandneuralODEs;(ii)Wedemonstratetheapplicability
wecallstructuredreceptivefields(SRFs)(Jacobsenetal.,
of DCN models, namely, that they exhibit a reduction in
2016). Previously, Gaussian scale-spaces have been pro-
parameters, improve data efficiency and can be used to
posed as a plausible model of biological receptive fields
parametrizeconvolutionalfiltersasafunctionofdepthina
and feature extraction in low-level vision (Florack et al.,
straightforwardfashion,whileachievingperformancecom-
1992;Lindeberg,1993;Lindeberg&Florack,1994). Here,
parablewithorbetterthanResNetandODE-Netbaselines;
weareinspiredbycomputationalmodelswhichinvestigate
(iii)WeshowthatfilterscaleslearnedbyDCNsareconsis-
the origin of response properties in the visual system, by
tentwithbiologicalobservationsandweproposethatthe
employingRFsandrecurrentinteractionfunctionswhich
combination of our design choices for spatial and depth-
scaleasadifferenceofGaussians(Somersetal.,1995;Ernst
wisecontinuitymaybehelpfulinstudyingtheemergence
etal.,2001).
ofbiologicalreceptivefieldpropertiesaswellashigh-level
phenomenasuchaspatterncompletion;(iv)Weexplore,for Neural ODEs. Studies have shown that both the con-
thefirsttime,contrastsensitivityofneuralODEsandsug- trast(Albrechtetal.,2002)andspatialfrequency(Frazor
gestthatthecontinuousrepresentationslearnedbyDCNs etal.,2004)responsefunctionsofcorticalneuronsdisplay
maybeleveragedforcomputationalsavings. characteristictemporalprofiles. However,temporaldynam-
ics are not incorporated into standard feed-forward CNN
WebelieveDCNscanbringtogethertwocommunitiesas
models. Inaddition,ithasbeensuggestedthatlateralinter-
theybothprovideatestbedforhypothesesandpredictions
actionsplayanimportantroleinthegenerationofcomplex
pertainingtobiologicalsystems,andpushtheboundaries
andselectiveneuronalresponses(Angelucci&Bressloff,
ofbiologicallyinspiredcomputervision.
2006). Suchactivitydynamicsareoftencomputationally
modeled using recurrently coupled neuronal populations
2.DeepContinuousNetworks whoseactivationsevolveaccordingtocoupleddifferential
equations(Ben-Yishaietal.,1995;Ernstetal.,2001).
2.1.Neuroscientificmotivation
Todescribethecontinuousevolutionoffeaturemapsconsis-
Thereislittledoubtthatmoderndeeplearningframeworks
tentwithbiologicalmodels,weadopttheframeworkofneu-
willbeconducivetoeffectiveandinsightfulcollaborations
ralODEs(Chenetal.,2018). NeuralODEinterpretationof
betweenneuroscienceandmachinelearning(Richardsetal.,
ResNetmodelspresentsanopportunitytoexplicitlymodel
2019). Inparticularinvisionresearch,CNNsarebecoming
thedynamicsoffeatureextractorsinfeed-forwardCNNs.
increasinglypopularformodellingearlyvisualareas(Batty
Undercertainassumptions,neuralODEscanbeinterpreted
etal.,2017;Eckeretal.,2019;Lindseyetal.,2019). Here
asbiologicallyplausiblerecurrentinteractions(Liao&Pog-
we propose the DCN model which can facilitate such in-
gio,2016;Rousseauetal.,2019),wherethedepthdimen-
vestigationsbylinkingtheend-to-endtrainablebutdiscrete
sionrepresentstime. UnlikeneuralODEswithpixel-basedDeepContinuousNetworks
F
α
F
α
Figure1.SRFfiltersbasedonN-jetfilterapproximation.ConvolutionalfiltersaredefinedastheweightedsumofGaussianderivativebasis
functionsuptoorder2,withcorrespondingscalesσ
1
=2.28(left)andσ
2
=0.90(right).OurDCNmodelslearnboththecoefficientsα,
andthescaleσend-to-endduringtraining.
convolutionalfilters, DCNswithstructuredfilters(SRFs) oftheGaussian,whichalsoindirectlydeterminesthespa-
alsoprovideanintuitivewaytoparametrizetheevolution tialfrequencyresponseoftheSRF(Fig.1). Notethatthe
ofthekernelsasafunctionofdepth. GaussianSRFformulationallowsforlearningfilterswith
differentaspectratios,however,inthisworkweonlycon-
DeepContinuousNetworks. DCNspresentedherecom-
siderisotropicbasisfunctionswithσ =σ =σ .
binestructuredreceptivefieldswithneuralODEs. Weview x y
spatio-temporallycontinuousrepresentationsinend-to-end TheN-jetformulationofanSRFfilterF(x,y)isgivenby:
trainablenetworksasalinkbetweenmodernCNNarchitec-
turesandcomputationalmodelsofbiologicalvision. Specif- l+k N
≤
ically, we are inspired by large-scale models of popula- F α(x,y;σ) = α l,k Gl,k(x,y; σ)
tion activity. In contrast, networks modelling biological 0 ≤Xl,0 ≤k
phenomenaatsmallerspatio-temporalscalesmayrequire
discretedescriptionsofbiologicalneurons,suchasspatially-
l+k ≤N ∂l+k
= α G(x,y; σ), (1)
discretephotoreceptorsortemporally-discretespikingdy- l,k ∂xl∂yk
0 l,0 k
namics. However,continuousrate-basedpopulationmodels ≤X≤
provide reasonably good explanations of phenomena ob-
served at thenetwork or systemslevel (Ben-Yishaiet al., whereGl,k(x,y; σ)arethepartialderivativesoftheGaus-
1995;Dayan&Abbott,2001),whichwebelievealignwell sianG(x,y;σ)withrespecttoxandy,N isthedegreeof
withCNNstrainedonhigh-levelcomputervisiontasks. theTaylorpolynomialwhichdeterminesthebasisorder,and
αencodestheexpansioncoefficients.
Takentogether, DCNsprovideafullytrainableanalogto
biologicalmodelswithcontinuousreceptivefieldsandcon- N-jetSRFshavefavourablepropertiesoverpixel-basedfil-
tinuouslyevolvingstatevariables,whilepreservingthemod- ters. SRF filters are steerable by the coefficients α and
ularityofthevisualhierarchybystackingspatio-temporally thebasisfunctionsarespatiallyseparable. Likewise,due
continuousblocksinafeed-forwardstream(Fig.2). totheirspatiallycontinuousdescription,thefilterscanbe
triviallyscaled,orrotated,withoutinterpolation. Inaddi-
tion, SRFs can provide parameter efficiency when filters
2.2.Structuredreceptivefields
are constructed using a small number of basis functions.
WeusethemultiscalelocalN-jetformulation(Floracketal., Inthisworkweoptforbasisorder2(basisfunctionupto
1996) to define the filters in convolutional layers. Struc- thesecondorderderivative),whichyieldsrelativelysmooth
turedreceptivefields(SRFs)basedontheGaussianN-jet filters. However,thegeneralizedSRFframeworkallowsfor
basisfunctionsarehighlyapplicabletoCNNs,astheyrep- learningmoreirregularRFshapesbyincreasingthenumber
resent a Taylor expansion of the input image or feature ofbasisfunctions.
mapsinalocalneighbourhoodinspaceandscale,andcan
Fig.1showstheN-jetapproximationoftwofiltersindiffer-
beusedtoapproximatepixel-basedfilters(AppendixA.1).
entscalesσ andσ . Wenotethatboththecoefficientsα
This means that each filter F(x,y;σ) in the network is a 1 2
andthescaleσarelearnablefilterparameters.Insteadoffix-
weightedsumofN basisfunctions,whicharepartialderiva-
ingthescaleσaprioriandoptimizingforαasinJacobsen
tives of the isotropic two-dimensional Gaussian function
G(x,y;σ)= 2π1 σ2e−(x 22 σ+ 2y2) . The scale, or the spatial ex- e tht ea sl e.( p2 a0 ra1 m6) eta en rd siS no ts hn eo nv eik twe ot ra kl. o( p2 t0 im20 iz) a, tw ioe n,in thte ug sra let ae rb no inth
g
tent,ofthefilterisexplicitlymodelledintheσ parameter
notonlytheshapebutalsothespatialsupportofthefilters.DeepContinuousNetworks
2.3.NeuralODEs Input Image (32x32x3)
We model the continuous evolution of feature maps as a kxk conv, 32
functionofdepthtwithinan‘ODEblock’. Formally, an ODE Block 3, 128
ODE Block 1, 32 ODEblockcontainsastackofM convolutionallayers,each
Upscaling block 1, 64
with its own convolutional filters wm with m = 1...M, Downsampling block 1, 64
followedbynormalizationG norm()andnon-linearactiva- ODE Block 4, 64
· ODE Block 2, 64
tionCELU()functions. FollowingthenotationsofChen
· Upscaling block 2, 32
etal.(2018)andRuthotto&Haber(2019),wedefinethe Downsampling block 2, 128
equationsofmotionforthefeaturestatesh Rnas: ODE Block 5, 32
∈ ODE Block 3, 128
1x1 conv, 3
dh(t)
=f(h(t),t,wm,dm) (2) Global Average Pooling
dt
fc, 10
=G norm[K2(w2)g(K1(w1)g(h)+d1t)+d2t]
Figure2.DCNmodelarchitecturewithCIFAR-10inputimages.
where g(x) = CELU(G norm(x)), the linear opera- Convolutionalkernelsizekislearnedduringtraining.Theequa-
tors Km() Rn ×n denote the convolution operators tionsofmotion(Eq.2)aresolvedwithinODEblocks.
· ∈
parametrized by wm. The filters wm(θ), dm(θ) are func-
tionsoflearnableparametersθ. InconventionalCNNswm
aretypically3 3kernelswherethelearnableparameters conditionsh(0). Fortherestofthispaper,weusetheinter-
×
correspondtopixelweights. IntheDCNmodelwedefine pretationthatthenumberoffunctionevaluationsperformed
thefilterswm usingtheGaussianSRF,thus,thelearnable bythenumericalODEsolverisanalogoustonetworkdepth.
parametersarebasiscoefficientsαandscaleσ,andtheker- Inthissense,continuous‘depth’or‘time’referstothecon-
nel size is not fixed but scales with σ and is learned (see tinuous variable t within the ODE blocks, while the full
Section2.4). architectureisstillmodularandcomposedofmultipleODE
blocks. Itisalsoimportanttonotethatwhenwetalkabout
TheCNNconvolutionoperator,Km(wm)with2inputand
thespatio-temporaldynamicsofDCNs,werefertothetem-
2outputchannelscanbewrittenas
poralevolutionofthefeaturemapsintheODEblocksand
K1,1(w1,1) K1,2(w1,2) nottoinputdynamics,asinavideo. WhileDCNsarepri-
Km(wm)= Km 2,1(wm 2,1) Km 2,2(wm 2,2) (3) marilyfeed-forwardnetworks,theODEdefinitionmakesit
(cid:18) m m m m (cid:19) possibleforDCNstomodeltime-varyingneuronalactiva-
withwji theconvolutionalkernelsforinputchanneliand tionsviathe‘continuousdepth’,eveninresponsetostatic
m
inputimages(seeSection4formoredetailedcomparisons
outputchannelj ofthem-thconvolution. Thetime-offset
withrecurrentneuralnetworks).
termsdmtinEq.2makestheODEanexplicitfunctionof
t,whichseparatestheODEblockimplementationfroma
simpleconvolutionalblockwithweightsharingoverdepth. 2.4.DeepContinuousNetworkswithSRFsandODEs
In accordance with conventional ResNet blocks, we pick We formulate deep continuous networks (DCNs) by em-
M =2. BasedontheimplementationbyChenetal.(2018), ployinglearnable,continuousSRFfilterdescriptionstode-
G isdefinedasgroupnormalization(Wu&He,2018). finetheweightswintheevolutionofaneuralODE.This
norm
ForgeneralizedcompatibilitywithODEsandtheadjoint meansthatforDCNs,eachwj mi inEq.2isadiscretization
method,wechooseanon-linearactivationfunctionwitha ofthecontinuousSRFfilterF αj mi(x,y;σ m)giveninEq.1,
theoreticallyuniqueandboundedadjoint,namelycontinu- sampled in [ 2σ ,2σ ]. αji and σ are trainable filter
− m m m m
ouslydifferentiableexponentiallinearunits,orCELU(Bar- parameters,whereσji = σ issharedbetweenthefilters
m m
ron, 2017). Similarly, we keep the linear dependence of inaconvolutionallayermunlessstatedotherwise. Allour
theequationsofmotiononcontinuousnetworkdeptht. Fi- codeisavailableat2.
nally,weadapttheGPUimplementationofODEsolvers1to
Networkarchitectureandtraining. WeconstructDCNs
solvetheequationsofmotionforapredefinedtimeinterval
bystackingODEblocksseparatedbydownsamplingblocks
t [0,T]usingtheadaptivestepsizeDOPRImethod.
∈ (Fig.2). Eachdownsamplingblockisasequenceofnormal-
Timevs. depthIntheneuralODEdefinition(Chenetal., ization,nonlinearactivationandstridedconvolution.Weuse
2018), the discrete depth of feed-forward networks such aconvolutionallayerforincreasingthechanneldimension-
as ResNets is reimagined as a continuous dimension de- alityattheinputlevelandemployglobalaveragepooling
notedbytimet, wheretheinputimagedefinestheinitial and a fully connected layer at the output level. We train
1https://github.com/rtqichen/torchdiffeq/ 2https://github.com/ntomen/Deep-Continuous-Networks
)noitacifissalC(
redocnE
)noitcurtsnoceR(
redoceD
...DeepContinuousNetworks
Meta-parametrization. DCNsenableustoparametrizethe
Table1.CIFAR-10 validation accuracies of DCN models, aver-
trainablefilterparametersα andσ asafunctionofdepth
agedover3runs, comparedtobaselinemodels. ODE-Netand
t. This both enables the kernels to vary smoothly over
ResNet-blocksbaselinesareasintroducedin*Chenetal.(2018).
depth,andletsusdefinetemporaldynamicsfortheneuronal
DCNsperformonparwithspatiallyand/ortemporallydiscrete
baselines,despitehavingalowernumberoftrainableparameters. responses in our network. We test the viability of such
modelsbyintroducingDCNvariantswhereσand/orαare
Model Continuity Accuracy Para- definedusinglinearorquadraticfunctionsoftandlearnable
Spatial Temporal (%) meters parametersa,b,c,a s,b s,a αandb α(Table4).
ODE-Net* x ✓ 89.6 0.3 560K
ResNet-blocks* x x 89.0± 0.2 555K 3.ExperimentalAnalysis
±
ResNet-SRF-blocks ✓ x 88.3 0.03 426K
±
ResNet-SRF-full ✓ x 89.3 0.4 323K 3.1.Parameterreductionanddataefficiency
±
DCN-ODE ✓ ✓ 89.5 0.2 429K Similar to biological models, where analytical receptive
±
D DC CN N- σfu jl il ✓
✓
✓
✓
8 89 9. .2 7±0 0. .3
3
3 42 76 2K
K
fieldslimitthescopeofthemodelusingpriorinformation,
± wefindthatDCNsaremoreparameterefficientcomparedto
baselinenetworks. EvaluatedonCIFAR-10,DCNsperform
onparwithbaselines,despiteusingSRFsofasmallbasis
ournetworksusingcross-entropylossandtheCIFAR-10 order2,whichmeanseachfiltershapeisdefinedbyonly6
dataset(Krizhevsky,2009). (SeeAppendicesA.2-A.3for freeparametersasopposedto9fortheconventional3 3
furtherdetailsregardingtrainingparameters.) kernels(Table1). Inaddition,wefindthatparameterred× uc-
tionviatheuseofSRFswithasmallbasisorderalsoleads
As a baseline without spatial continuity, we compare
todataefficiency. WhentrainedonasubsetofCIFAR-10
DCNperformancetotheODE-NetintroducedinChenetal.
images(small-dataregime),DCNsoutperformthediscrete
(2018),wheretheconvolutionswithintheODEblocksare
baselinenetworks(Table2). Wealsofindaccuracyincrease
performedusingdiscrete,pixel-basedkernels,with3 3
× over the small-data performance reported by Arora et al.
parameters. As a baseline without (depthwise) temporal
(2020)fortheconvolutionalneuraltangentkernel(CNTK)
continuity,wedefinethe‘ResNet-blocks’modelwherethe
model(Table2).
ODEblocksarereplacedbygeneric,discreteResNetblocks,
comprisingtwoconvolutionallayersandaskipconnection, Moreover,wetrainencoder-decodernetworkstoreconstruct
with comparable number of parameters to the ODE-Net. CIFAR-10 images using mean squared error (MSE) loss.
This is also a baseline model used in Chen et al. (2018). WefindthattheDCNmodelsoutperformdiscretebaseline
IntheResNet-SRF-blocksmodel,weprovidethediscrete- models on the validation set (Table 3), despite having a
depthandcontinuous-spacebaselinebyreplacingthe3 3 lowernumberofparametersasbefore. Additionaldetails
×
filterdefinitionofResNet-blockswithSRFdefinitions. andexampleimagesareshowninAppendixA.4.
WetesttwoversionsofDCNsandResNet-SRF-blocksto We find that meta-parametrized DCN variants match the
quantify the viability of SRF filters outside of the ODE classificationperformanceofbaselinesandmayoutperform
blocks. InDCN-ODEandResNet-SRF-blocksweusethe DCNswithstaticweights(Table5). Thisisaninteresting
SRFfiltersonlywithintheODE(ResNet)blocks,andfor findingaswetestonlyafewmodels,withlittlehyperparam-
theremaininglayersweusediscretekernelswiththesame eteroptimization,indicatingthatDCNscanpotentiallybe
hyperparameters as the baselines. In the second version, usedtoparametrizethedependenceofconvolutionalkernel
DCN-full(ResNet-SRF-full),weusespatiallycontinuous weightsonnetworkdepth,forfurtherparameterreduction.
kernelseverywhere,includingthedownsamplinglayers.
AsanadditionaldemonstrationoftheversatilityofDCNs, 3.2.Linkwithbiologicalmodels
weconductanimagereconstructionexperimentonCIFAR-
Scalefitting. AsanadvantageoverconventionalCNNs,it
10. Weusethefeaturemapsgeneratedbyencodernetworks
ispossibletodirectlyinvestigatetheoptimalreceptivefield
(outputofODEBlock3inFig.2),asinputtoadecodernet-
(RF) size in each DCN block after training, since DCNs
work. Thedecodernetworksarecomposedof2DCN-ODE,
fit the kernel scale σ explicitly. We observe an upward
ODE-NetorResNetblocks,separatedbybilinearupscaling
trendintheSRFscaleσwiththedepthoftheconvolutional
layersand1 1convolutionstoreducedimensionality.
× layerwithinthenetwork(Fig.3a). WhiletheRFsizegrows
Finally,weinvestigatethecasewherewedropscalesharing withdepthalsoinconventionalCNNs, ittypicallygrows
withinalayer,andoptimizethescaleparameterσji inde- inapredeterminedmanner: foracascadeofconvolutional
m
pendently for each input channel i and output channel j, layerstheRFsizeisalinearfunctionofdepthgivenconstant
whichwecallDCNσji. kernelsize.Thus,thereceptivefieldsizeateveryCNNlayerDeepContinuousNetworks
Table2.ValidationaccuraciesfortheDCN-ODEmodelandbaselinestrainedonasubsetofCIFAR-10(small-dataregime).Firsttwo
rowsshowsmall-databaselineaccuraciestakenfrom†Aroraetal.(2020). ResNet-blocksandODE-Netmodelsareimplementedby
us,asinTable1.TheDCNmodeloutperformsspatiallyand/ortemporallydiscretebaselinesformediumtrainingsetsizeasparameter
efficiencyleadstodataefficiency.Allresultsareaveragedover3runs.
#imagesperclass
Model
2 4 8 16 32 52 64 103 128 512 1024
ResNet34† 17.5 2.5 19.5 1.4 23.3 1.6 28.3 1.4 33.2 1.2 – 41.7 1.1 – 49.1 1.3 – –
± ± ± ± ± ± ±
CNTK† 18.8 2.1 21.3 1.9 25.5 1.9 30.5 1.2 36.6 0.9 – 42.6 0.7 – 48.9 0.7 – –
± ± ± ± ± ± ±
ResNet-blocks 16.7 0.8 19.6 1.0 22.0 1.3 28.1 1.7 35.4 0.9 39.8 0.6 41.6 1.5 49.0 0.2 50.9 0.6 70.4 1.2 76.8 0.7
± ± ± ± ± ± ± ± ± ± ±
ODE-Net 16.8 2.8 20.5 0.8 23.1 2.5 29.8 0.8 36.4 1.0 41.7 1.2 42.3 0.2 48.6 0.5 50.7 0.7 71.7 1.5 77.4 0.5
± ± ± ± ± ± ± ± ± ± ±
DCN-ODE 16.4 1.6 19.8 0.7 26.5 0.9 31.2 0.6 37.7 0.6 44.5 0.8 48.0 1.3 54.2 0.8 58.2 0.7 75.5 0.8 79.7 0.3
± ± ± ± ± ± ± ± ± ± ±
Table3.DCNsachievelowerMSElossinthereconstructiontask Table4.Meta-parametrizationoffilterparametersσ andαasa
thandiscretebaselinesontheCIFAR-10validationset,despite functionofdepthtindifferentDCNvariants.
usingasmallernumberofparameters. SeealsoAppendixA.4
forreconstructedimageexamples.Allresultsareaveragedover3 Model Parametrization
runs.
DCNσ(t) σ =2at+b
DCNσ(t2) σ =2at2+bt+c
Reconstruction
Model
Loss(%)
DCNσ(t),α(t) σ =2ast+bs,α=a αt+b
α
ResNet-blocks 21.0 0.4
±
ODE-Net 20.2 1.3
± Table5.CIFAR-10validationaccuraciesaveragedover3runsfor
DCN-ODE 17.1 0.3
± DCNmodelswithmeta-parametrization.
isfixeddependingonthearchitectureandhyperparameters. Model Accuracy(%)
ThisisalimitationofCNNswhichthevisualsystemdoes DCNσ(t) 89.97 0.30
notnecessarilyhave. DCNs,ontheotherhand,canlearn DCNσ(t2) 89.93± 0.28
RF sizes which grow non-linearly as a function of depth, DCNσ(t)andα(t) 89.88± 0.25
whichseemstobeinlinewiththebehaviourindownstream ±
visualareas(Smithetal.,2001).
Inaddition,weplotthedistributionoflearnedσjiindiffer- thisresemblance,weexplorewhetherDCNsmaydisplay
entODEblocksofthemodelDCNσji(Fig.3b). Notethat similaremergentproperties. Specifically,wehypothesize
the scale parameter σ controls the bandwidth of the SRF thatDCNscanperformwellinthecaseoflocallymissing
filtersandisthusrelatedtotheirspatialfrequencyresponse. information in images, through pattern completion at the
Wefindthattheσjidistributionsaftertrainingareapprox- featuremaplevel.
imatelylog-normalanddisplayapositiveskew, whichis
We test this hypothesis on the DCN models trained on
consistentwiththescaleandspatialfrequencytuningdis- CIFAR-10 classification by masking n n pixels of the
tributions in the primate visual system (Yu et al., 2010). ×
validationimagesattesttime. Themaskshavezeropixel
Webelievetheseresultsarepromisingforbridgingthegap
values,andareplacedatthecenteroftheimage. Wefind
betweendeeplearningandtraditionalmodelsofbiological
thatwhenconfrontedwithasmallpatchofmissinginforma-
systems.
tionattesttime,DCNscangeneratefeaturemapssimilarto
Pattern completion. Established models from computa- thoseobtainedfromintactimages. Specifically,weobserve
tional neuroscience, with continuous temporal dynamics thatthetotaldifference:
and well-defined recurrent interaction structures, such as
1
the Ermentrout-Cowan model (Bressloff et al., 2001), or D(t)= him(t) himmasked(t) (4)
A | − |
neuralfieldmodels(Amari,1977),displayinterestinghigh-
X
level phenomena such as spontaneous pattern formation between the feature maps generated by an intact image
andtravellingwaves(Coombes,2005). Suchmodelsem- him(t) and a masked image himmasked(t), normalized by
ploylocal,distance-dependentinteractions,similartothe theamplitudeoftheintactimageA,isreducedwithinan
SRF-basedODEblocksintheDCNformulation. Basedon ODEblock(Fig. 4). IntermsoftheoverallclassificationDeepContinuousNetworks
performancewithimagesmaskedattesttime,wefindthat Lindeberg,2013). WeusetheN-jetbasis,whichenablesa
DCNs are marginally more robust against zero-masking spatiallycontinuousrepresentation,withalearnablescale
thanbaselines(Fig.3c). parameterσ,toapproximateconvolutionalfilters.
Similar to the N-jet basis, a set of oriented multi-scale
3.3.Contrastrobustnessandcomputationalefficiency
wavelets, called a steerable pyramid, is proposed by Si-
The selectivity of neuronal responses is invariant to con- moncellietal.(1992)andcomplexwaveletshavebeenused
trastinmammalianvision(Sclar&Freeman,1982;Skottun byMallat(2012)andBruna&Mallat(2013)aspartofscat-
etal.,1987). However,weobservethatDCNandODE-Net teringtransforms. CNNfiltersbasedonlinearcombinations
modelsaresensitivetochangesininputcontrast. Thisis ofGaborwaveletsareadoptedbyLuanetal.(2018),while
notunexpectedsinceODEblockscomputethesolutionto Worralletal.(2017)proposecircularharmonics,asspatially
theinitialvalueproblemposedbytheequationsofmotion continuousfilterrepresentations.
and the input h(0). To quantify this sensitivity we vary
Similartoourapproach,Shelhameretal.(2019)combine
the contrast c of the input images at test time, where for
free-form filters with Gaussian kernels, thus learning the
eachimageH intheCIFAR-10validationsetwedefinethe
filterresolution. Likewise,Xiongetal.(2020)learnfilter
networkinputasH =cH. Whennaivelychangingthein-
sizesusingGaussiankernelsoptimizedusingvariationalin-
putcontrastcthisway,wefindthatthevalidationaccuracy
ference. Finally,Loog&Lauze(2017)integratecontinuous
decaysrapidlyforbbothmodels(solidlinesinFig.5,top).
scale-selectivitythrougharegularizationhyper-parameter.
Empirically,wenoticethat,withtheappropriatechoiceof Here,weusetheN-jetframeworkbasedonGaussianderiva-
normalization functions, the input contrast c has a direct tivesasinJacobsenetal.(2016)andPinteaetal.(2021),
effectonthetimescalesofthesolutionh(t). Thismeans howeverourmainmotivationisretainingcompatibilitywith
thatunderdifferentcontrastvaluesc,thefeaturemaptrajec- biologicalmodels. Also,unlikeJacobsenetal.(2016)we
torieswithinanODEblockmayconvergefaster,andamore learnthescaleparameterσduringtraining.
efficientDCNimplementationmightbepossible. Basedon
Continuous depth representations in deep networks.
thisobservation,weheuristicallytestwhetherscalingthe
AlongwithworkbyLuetal.(2018)andRuthotto&Haber
integrationtimeintervalT (usedduringtraining)ofODE
(2019),networkscontinuousinthedepth(ortime)dimen-
block1bytheinputcontrastattesttime,asT = cT,can
sion have been proposed by Chen et al. (2018) under the
improvecontrastrobustnessattesttime. Wefindthatwith
nameneuralordinarydifferentialequations(ODEs). They
thescaledintegrationinterval,DCNvalidationbaccuracyis
proposeODE-NetsbasedontheResNetformulation(He
relatively robust against changes in contrast c, compared
etal.,2016)forclassificationtasks,whichweuseasabase-
tonaivebaselinesandODE-Net,untilc << 1whentime
line. Inthisworkwefocusmainlyonimageclassification,
scalesbecometoofastandtheODEsolverbecomesunsta-
however,thereisextensiveongoingworkongenerativemod-
bleforallmodels(dashedlinesinFig.5,top).
elsandnormalizingflowsusingtheneuralODEcontinuous
Interestingly,weobserveareductioninthenumberoffunc- depthinterpretation(Salmanetal.,2018;Grathwohletal.,
tionevaluations(NFEs)inODEblock1forc<1(Fig.5, 2019). WenotethatDCNscanbereadilyincorporatedinto
middle). Furthermore,weshowthataslongastheerrortol- continuousflowmodels,aswellasotherspatio-temporally
eranceoftheODEblocksarenotdecreased,thiseffectcan continuousCNNinterpretationsbasedonpartialdifferential
beexploitedbyscalingtheinputfeaturemapsofallODE equations(Ruthotto&Haber,2019).
blocksbycforsignificantcomputationalsavings. Wefind
Even though the adjoint method described in Chen et al.
thatdecreasingcleadstoconsiderableefficiencyimprove-
(2018)offersconsiderablecomputationalsavings,especially
ments, where total NFEs can be reduced from 102 to 60,
intermsofmemory,recentworkhasimproveduponitboth
(forc = 1and0.06),withlessthan0.5%lossinaccuracy
in terms of stability, computational efficiency and perfor-
(Fig.5,bottom).
mance (Dupont et al., 2019; Finlay et al., 2020; Zhuang
etal.,2020b). Likewise,thecontrastrobustformulationof
4.RelatedWork DCNs, aswellasthesynergybetweenthe (1)memory
O
complexity of the adjoint method and spatially separable
OurproposedDCNnetworksextendpriorworkoncontinu-
SRFfilters(theimplementationsofwhichmayotherwise
ousfiltersandcontinuousdepthneuralODEs.
inflatethememorycost)providepotentialcomputational
Spatially continuous filter representations. Structured benefitsoverconventionalCNNswherethenumberoffunc-
filtershavebeentraditionallyusedincomputervisionfor tionevaluationsisfixed.
extracting image structure at multiple scales. N-jet filter
Otherstudieshavesuggestedthat,similartoourDCNvari-
basisisfirstintroducedbyFloracketal.(1996)basedonpre-
antswherethefilterdefinitionsareindependentofdepth,
viousworkonGaussianscale-spaces(Floracketal.,1992;DeepContinuousNetworks
90 DCN-ODE 3500 block 1: conv1
1.6 DCN-full block 1: conv2 ResNet-SRF-blocks 3000 block 2: conv1
1.4 ResNet-SRF-full 2500 block 2: conv2 80 block 3: conv1
1.2 2000 block 3: conv2
1.0 1500 70 DCN
0.8 1000 ResNet-blocks
ODE-Net
500
0.6
block
1: 0 c. o4 n blv o1
ck
1: con blv o2
ck
2: con blv o1
ck
2: con blv o2
ck
3: con blv o1
ck
3: conv2
0
0 Learn2 ed scale σ4
baseline
m ask
4x4
m ask
6x6
m ask
8x m8
a s k
10x m1
as0
k
12x12
(a) (b) (c)
Figure3. (a)Learnedσvaluesincreasewithdepthwithinthenetwork.(b)σjidistributionswithintheODEblocksdisplayapositive
skewinlinewithbiologicalobservations.(c)CIFAR-10validationaccuraciesonthepatterncompletiontaskwithincreasingmasksize.
neural ODEs based on ResNet architectures with weight recurrentconvolutionalnetworks(RCNNs)havebeenpro-
sharingcanbeinterpretedasrecurrentneuralnetworks(Kim posed(Liang&Hu,2015;Spoereretal.,2017;Hu&Miha-
etal.,2016;Rousseauetal.,2019),whichbridgesthegap las,2018),whichcanemulatebiologicallateralconnectivity
betweendeeplearning,dynamicalsystemsandtheprimate structuresandextra-classicalreceptivefieldeffects. Similar
visualcortex(Liao&Poggio,2016;Massarolietal.,2020). toourworkwheredepthwise-continuitymimicsrecurrent
Similartotheseworks,weillustratetheparallelsbetween networks,ithasbeenshownthataddingrecurrentlayersto
neural ODEsand the dynamicalsystems approach ofthe convolutionaldeepnetworkscanfacilitatepatterncomple-
computationalmodelsofbiologicalcircuits. Asanovelcon- tioninamannerconsistentwithpsychophysicalandelectro-
tribution,weextendneuralODEstoDCNs,wherenotonly physiologicalexperiments(Tangetal.,2018). Furthermore,
thedepthofthenetworkiscontinuousbutalsotheshape ourDCNmodelshavethepotentialtocompressthedepth
andspatialresolutionofthefiltersareend-to-endtrainable. ofthenetwork,byreplacingmultiplesequentiallayerswith
meta-parametrized ODE blocks, which are analogous to
CNNsandRNNsasmodelsofbiologicalnetworks. There
recurrentnetworkswithcontinuouslyevolvingfilterparam-
isextensivepriorworkonCNNsandrecurrentneuralnet-
eters.Inasimilarlineofwork,ithaspreviouslybeenshown
works(RNNs)formodelingbiologicalcomputation. The
thatshallownetworkswithrecurrentlyconnectedlayerscan
visual cortex is highly recurrent (Dayan & Abbott, 2001;
achievehighobjectrecognitionperformancewhileretaining
Liao&Poggio,2016)whichisthoughttoberesponsible
brain-likerepresentations,andspecificallyreproducingthe
for complex neuronal dynamics (Ben-Yishai et al., 1995;
populationdynamicsinareaITofthevisualsystemmuch
Angelucci&Bressloff,2006). Accordingly,computational
morecloselythanfeed-forwarddeepCNNs(Karetal.,2019;
modelswithlateralconnections(Sompolinskyetal.,1988;
Kubiliusetal.,2019).
Ernstetal.,2001)andmorerecentlyRNNs(Laje&Buono-
mano,2013;Manteetal.,2013;Mastrogiuseppe&Ostojic, In contrast to standard RNNs, our model is based on the
2018)havebeenextensivelyusedasmodelsofbiological ResNetinspiredmodelofneuralODEs,andinitscurrent
neural computation. For example the first-order reduced form (Eq. 2), does not accept time-variant input. In that
andcontrollederror(FORCE)algorithm,havebeenusedto sense,thespatio-temporaldynamicsofDCNsrefertothe
reproducethedynamicsofdifferentbiologicalcircuits(Sus- dynamicsofthefeaturemaps,orneuronalresponses,and
sillo&Abbott,2009;Laje&Buonomano,2013;Carnevale nottheinput. Nevertheless,thisgivesDCNstheabilityto
et al., 2015; Rajan et al., 2016; Enel et al., 2016). Simi- modeltime-varyingresponses,eventostaticinputimages.
larly, optimization via gradient-based algorithms such as
Inaddition,DCNswithweightsharingcanbethoughtofas
theHessian-freemethod(HF)orstochasticgradient-descent
recurrentnetworks(Rousseauetal.,2019)andcanbeeas-
(SGD)havebeenadoptedtoreplicateexperimentalobser-
ilymodifiedtoprocesstime-variantinput(suchasvideos).
vations(Manteetal.,2013;Baraketal.,2013;Songetal.,
However,inthispaperweconsiderDCNmodelsasanex-
2016). Ithasalsobeensuggestedtousespikingrecurrent
tensionofconventionalfeed-forwardCNNs,withextended
networks (Kim & Chow, 2018; Kim et al., 2019) and in-
temporaldynamicsandcontinuousspatialrepresentations,
corporatesynapticdynamics(Baetal.,2016;Miconietal.,
whichareapplicabletofeed-forwardmodelsofthevisual
2018)forimprovedphysiologicalrealism.
systemsimilartoworksbySchrimpfetal.(2018);Lindsey
Bringing together the power of CNNs and neuroscience, etal.(2019);Eckeretal.(2019);Zhuangetal.(2020a).
σ elacs
denraeL
sretlif
fo
#
]%[
ycaruccA
noitadilaVDeepContinuousNetworks
input image masked image
80
60
ODE-Net (robust)
40 DCN (robust)
DCN:3 (robust)
ODE-Net (naive)
him(0) him_masked(0) D(t) 20 DCN (naive)
DCN:3 (naive)
0.6
ODE block 1
0.4 50
0 0.2
him(T) him_masked(T) D(t)
All ODE blocks 300
1.0 200
100
0.5 0.001 0.01 0.1 1
Contrast Level c
0 2
integration time t Figure5.OntheCIFAR-10validationset,DCNsaremorerobust
thanbaselineODE-Netstochangesininputcontrastcattesttime
Figure4.PatterncompletionintheDCNfeaturemapsduringclas- (top).Interestingly,thenumberoffunctionevaluations(NFEs)in
sificationofmaskedimages. Featuremapsinasinglechannel thefirstODEblock(middle)orthewholeDCNnetwork(bottom)
ofODEblock1areshownforanexampleimage. Wefindthat canbereducedconsiderablybymodulatingc.
thedifferenceD(t)betweenthefeaturemapshim(t)ofanintact
imageandhimmasked(t)ofamaskedimageisreducedast T.
WealsoshowthemeanD(t)for1000validationimages(bo→ ttom ingfiltersizesatlargelearningrates. Especiallyformeta-
right),wheretheshadedareaisthestandarddeviationoverdif- parametrization,itwouldbeadvisabletocliptheintegration
ferentimages. Examplefeaturemapsfrombaselinemodelsare timeandfilterparameterswithinareasonablerange.
providedinAppendixA.5.
Nevertheless,webelievethereareexcitingfutureresearch
opportunitiesinvolvingDCNs. NeuralODEformulations
5.Discussion
provideaninterestingopportunityforestablishingatheo-
We introduce DCNs, CNN models which learn spatio- reticalunderstandingofdeepnetworksbasedondynamical
temporallycontinuousrepresentations,consistentwithbi- systems.Theinterplayofinputcontrastandintegrationtime
ologicalmodels. WeshowthatDCNscanmatchbaseline isonesuchobservationwhichrequiresfurtherinvestigation.
performanceinanimageclassificationtaskandoutperform Similarly,ourchoiceoffiltersbasedonwell-behavedGaus-
baselinesinthesmall-dataregimeandinareconstruction sianderivativesallowforfurtheranalyticalstudies,unlike
task,whileusingasmallernumberofparameters. Similarly, conventionalCNNs.
weproposedifferentmethodsofmeta-parametrizationof
Similarly,DCNsofferinterestingpossibilitiesforbiological
theconvolutionalfilterasafunctionofdepth,whichmay
modelling. TheinbuiltsmoothevolutionoffiltersinDCNs
not only be applicable to network compression, but also
canbeused,forexample,toincorporateresponsedynamics
formodellingthetemporalprofilesofbiologicalresponses.
suchassynapticdepressionorshort-termpotentiation(Ba
Asafurtherlinkwithbiologicalmodels,wehavedemon-
etal.,2016;Miconietal.,2018). Likewise,theequationsof
stratedthatthelearnedfilterscaledistributionsinDCNsare
motioncanbemodifiedtoreflectaxonaldelaysorgenerate
compatiblewithexperimentalobservations. Thismakesthe
oscillations. Takentogether,webelievebyofferingalink
DCNmodelsviableforfutureneuroscientificinvestigations
betweendynamicalsystems,biologicalmodelsandCNNs,
regardingtheemergenceofRFsizes. Inaddition,wehave
DCNsdisplayaninterestingpotentialtobringtogetherideas
presented the capability of DCNs to reduce errors in fea-
frombothfields.
turemapscausedbymasking. Finally,wehaveempirically
shownaninterestinginterplaybetweentheinputcontrastto
Acknowledgements
ODEblocksandthetimescalesofthesolutions,whichcan
becapitalizedonforcomputationalsavings. Authors thank the reviewers for insightful comments, and
Prof.Dr.MarcoLoogforfruitfuldiscussions. Thispublication
However, one of the biggest limitations of DCN models
ispartoftheproject“Pixel-freedeeplearning”(TOPgrantwith
is that they may become unstable during training. Com- projectnumber612.001.805)whichisfinancedbytheDutchRe-
biningneuralODEswithscalefittingmayleadtoexplod- searchCouncil(NWO).
NCD
EFN
latoT
]%[
ycaruccA
noitadilaV
EFNDeepContinuousNetworks
References Cox, D. D. and Dean, T. Neural networks and neuroscience-
inspiredcomputervision. CurrentBiology,24(18):R921–R929,
Albrecht,D.G.,Geisler,W.S.,Frazor,R.A.,andCrane,A.M.
2014.
Visualcortexneuronsofmonkeysandcats:temporaldynamics
ofthecontrastresponsefunction. JournalofNeurophysiology,
Dayan, P.andAbbott, L.F. Theoreticalneuroscience: compu-
88(2):888–913,2002.
tationalandmathematicalmodelingofneuralsystems. Mas-
sachusettsInstituteofTechnologyPress,1stedition,dec2001.
Amari,S. Dynamicsofpatternformationinlateral-inhibitiontype
ISBN0262041995.
neuralfields. BiologicalCybernetics,27(2):77–87,1977.
Angelucci,A.andBressloff,P.C. Contributionoffeedforward, Dupont,E.,Doucet,A.,andTeh,Y.W. AugmentedneuralODEs.
lateralandfeedbackconnectionstotheclassicalreceptivefield InNeurIPS,pp.3140–3150,2019.
centerandextra-classicalreceptivefieldsurroundofprimatev1
Ecker,A.S.,Sinz,F.H.,Froudarakis,E.,Fahey,P.G.,Cadena,
neurons. ProgressinBrainResearch,154:93–120,2006.
S.A.,Walker,E.Y.,Cobos,E.,Reimer,J.,Tolias,A.S.,and
Arora,S.,Du,S.S.,Li,Z.,Salakhutdinov,R.,Wang,R.,andYu,D. Bethge,M.Arotation-equivariantconvolutionalneuralnetwork
Harnessingthepowerofinfinitelywidedeepnetsonsmall-data modelofprimaryvisualcortex. InInternationalConferenceon
tasks.InInternationalConferenceonLearningRepresentations LearningRepresentations(ICLR),2019.
(ICLR),2020.
Enel,P.,Procyk,E.,Quilodran,R.,andDominey,P.F. Reservoir
Ba,J.,Hinton,G.E.,Mnih,V.,Leibo,J.Z.,andIonescu,C. Using computingpropertiesofneuraldynamicsinprefrontalcortex.
fastweightstoattendtotherecentpast. InAdvancesinNeural PLoSComputationalBiology,12(6):e1004967,2016.
Information Processing Systems (NeurIPS), pp. 4331–4339,
2016. Ernst,U.,Pawelzik,K.,Sahar-Pikielny,C.,andTsodyks,M. In-
tracorticaloriginofvisualmaps. NatureNeuroscience,4(4):
Barak,O.,Sussillo,D.,Romo,R.,Tsodyks,M.,andAbbott,L. 431–436,2001.
Fromfixedpointstochaos:threemodelsofdelayeddiscrimina-
tion. ProgressinNeurobiology,103:214–222,2013. Finlay, C., Jacobsen, J.-H., Nurbekyan, L., and Oberman,
A. M. How to train your neural ODE. arXiv preprint
Barron,J.T. Continuouslydifferentiableexponentiallinearunits.
arXiv:2002.02798,2020.
arXivpreprintarXiv:1704.07483,2017.
Florack,L.,Romeny,B.T.H.,Viergever,M.,andKoenderink,J.
Batty,E.,Merel,J.,Brackbill,N.,Heitman,A.,Sher,A.,Litke,A.,
Thegaussianscale-spaceparadigmandthemultiscalelocaljet.
Chichilnisky,E.,andPaninski,L. Multilayerrecurrentnetwork
IJCV,18(1):61–75,1996.
modelsofprimateretinalganglioncellresponses. ICLR,2017.
Bauer,U.,Scholz,M.,Levitt,J.B.,Obermayer,K.,andLund,J.S. Florack,L.M.,terHaarRomeny,B.M.,Koenderink,J.J.,and
Amodelforthedepth-dependenceofreceptivefieldsizeand Viergever,M.A. Scaleandthedifferentialstructureofimages.
contrastsensitivityofcellsinlayer4cofmacaquestriatecortex. ImageandVisionComputing,10(6):376–388,1992.
Visionresearch,39(3):613–629,1999.
Frazor,R.A.,Albrecht,D.G.,Geisler,W.S.,andCrane,A.M.
Ben-Yishai,R.,Bar-Or,R.L.,andSompolinsky,H. Theoryof Visualcortexneuronsofmonkeysandcats:temporaldynamics
orientationtuninginvisualcortex. ProceedingsoftheNational ofthespatialfrequencyresponsefunction. JournalofNeuro-
AcademyofSciences,92(9):3844–3848,1995. physiology,91(6):2607–2627,2004.
Bressloff, P. C., Cowan, J. D., Golubitsky, M., Thomas, P. J., Grathwohl,W.,Chen,R.T.Q.,Bettencourt,J.,Sutskever,I.,and
andWiener,M.C. Geometricvisualhallucinations,euclidean Duvenaud, D. Ffjord: Free-form continuous dynamics for
symmetryandthefunctionalarchitectureofstriatecortex.Philo- scalablereversiblegenerativemodels. ICLR,2019.
sophicalTransactionsoftheRoyalSocietyofLondon.SeriesB:
BiologicalSciences,356(1407):299–330,2001. Harvey, B. M. and Dumoulin, S. O. The relationship between
corticalmagnificationfactorandpopulationreceptivefieldsize
Bruna,J.andMallat,S. Invariantscatteringconvolutionnetworks.
in human visual cortex: constancies in cortical architecture.
TPAMI,35(8):1872–1886,2013.
JournalofNeuroscience,31(38):13604–13612,2011.
Cadena,S.A.,Denfield,G.H.,Walker,E.Y.,Gatys,L.A.,Tolias,
He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearningfor
A.S.,Bethge,M.,andEcker,A.S. Deepconvolutionalmodels
imagerecognition. InCVPR,pp.770–778,2016.
improvepredictionsofmacaqueV1responsestonaturalimages.
PLoSComputationalBiology,15(4):e1006897,2019.
Hu,B.andMihalas,S. Convolutionalneuralnetworkswithextra-
classicalreceptivefields. CoRR,abs/1810.11594,2018. URL
Carnevale,F.,deLafuente,V.,Romo,R.,Barak,O.,andParga,
http://arxiv.org/abs/1810.11594.
N. Dynamiccontrolofresponsecriterioninpremotorcortex
duringperceptualdetectionundertemporaluncertainty.Neuron,
Jacobsen,J.-H.,vanGemert,J.,Lou,Z.,andSmeulders,A.W.
86(4):1067–1077,2015.
StructuredreceptivefieldsinCNNs. InCVPR,pp.2610–2619,
Chen,R.T.Q.,Rubanova,Y.,Bettencourt,J.,andDuvenaud,D. 2016.
Neuralordinarydifferentialequations. NeurIPS,2018.
Jones,J.P.andPalmer,L.A.Anevaluationofthetwo-dimensional
Coombes,S. Waves,bumps,andpatternsinneuralfieldtheories. gaborfiltermodelofsimplereceptivefieldsincatstriatecortex.
BiologicalCybernetics,93(2):91–108,2005. JournalofNeurophysiology,58(6):1233–1258,1987.DeepContinuousNetworks
Kar,K.,Kubilius,J.,Schmidt,K.,Issa,E.B.,andDiCarlo,J.J.Ev- Lindsey,J.,Ocko,S.A.,Ganguli,S.,andDeny,S. Aunifiedthe-
idencethatrecurrentcircuitsarecriticaltotheventralstream’s oryofearlyvisualrepresentationsfromretinatocortexthrough
executionofcoreobjectrecognitionbehavior. NatureNeuro- anatomicallyconstraineddeepCNNs. InInternationalConfer-
science,22(6):974–983,2019. enceonLearningRepresentations(ICLR),2019.
Kietzmann,T.C.,McClure,P.,andKriegeskorte,N. Deepneural Loog,M.andLauze,F. Supervisedscale-regularizedlinearconvo-
networksincomputationalneuroscience. BioRxiv,pp.133504, lutionaryfilters. InBMVC,2017.
2018.
Lu, Y., Zhong, A., Li, Q., and Dong, B. Beyond finite layer
Kim, C. M. and Chow, C. C. Learning recurrent dynamics in neuralnetworks: Bridgingdeeparchitecturesandnumerical
spikingnetworks. Elife,7:e37124,2018. differentialequations. InInternationalConferenceonMachine
Learning(ICML),pp.3276–3285.PMLR,2018.
Kim,J.,KwonLee,J.,andMuLee,K. Deeply-recursiveconvolu-
tionalnetworkforimagesuper-resolution. InIEEEConference Luan,S.,Chen,C.,Zhang,B.,Han,J.,andLiu,J. GaborConvolu-
onComputerVisionandPatternRecognition(CVPR),pp.1637– tionalNetworks. IEEETransactionsonImageProcessing,27
1645,2016. (9):4357–4366,2018.
Kim,R.,Li,Y.,andSejnowski,T.J. Simpleframeworkforcon- Mallat,S. Groupinvariantscattering. CommunicationsonPure
structing functional spiking recurrent neural networks. Pro- andAppliedMathematics,65(10):1331–1398,2012.
ceedingsoftheNationalAcademyofSciences,116(45):22811–
Mante, V., Sussillo, D., Shenoy, K. V., and Newsome, W. T.
22820,2019.
Context-dependentcomputationbyrecurrentdynamicsinpre-
frontalcortex. nature,503(7474):78–84,2013.
Klindt,D.,Ecker,A.S.,Euler,T.,andBethge,M. Neuralsys-
temidentificationforlargepopulationsseparating“what”and
Massaroli,S.,Poli,M.,Park,J.,Yamashita,A.,andAsama,H.
“where”.InAdvancesinNeuralInformationProcessingSystems
Dissecting neural ODEs. arXiv preprint arXiv:2002.08071,
(NeurIPS),pp.3506–3516,2017.
2020.
Krizhevsky, A. Learning multiple layers of features from tiny
Mastrogiuseppe,F.andOstojic,S. Linkingconnectivity,dynam-
images. Technicalreport,UniversityofToronto,Departmentof
ics,andcomputationsinlow-rankrecurrentneuralnetworks.
ComputerScience,042009.
Neuron,99(3):609–623,2018.
Kubilius,J.,Schrimpf,M.,Hong,H.,Majaj,N.J.,Rajalingham,
Miconi,T.,Stanley,K.O.,andClune,J. Differentiableplastic-
R.,Issa,E.B.,Kar,K.,Bashivan,P.,Prescott-Roy,J.,Schmidt,
ity:trainingplasticneuralnetworkswithbackpropagation. In
K.,Nayebi,A.,Bear,D.,Yamins,D.L.,andDiCarlo,J.J.Brain-
Proceedingsofthe35thInternationalConferenceonMachine
likeobjectrecognitionwithhigh-performingshallowrecurrent
Learning(ICML),volume80,pp.3556–3565,2018.
ANNs. InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),pp.12785–12796,2019.
Pintea,S.L.,Tomen,N.,Goes,S.F.,Loog,M.,andvanGemert,
J.C. Resolutionlearningindeepconvolutionalnetworksusing
Laje, R.andBuonomano, D.V. Robusttimingandmotorpat-
scale-spacetheory. arXivpreprintarXiv:2106.03412,2021.
terns by taming chaos in recurrent neural networks. Nature
Neuroscience,16(7):925–933,2013.
Rajan, K., Harvey, C.D., andTank, D.W. Recurrentnetwork
modelsofsequencegenerationandmemory. Neuron,90(1):
Li,Z. Aneuralmodelofcontourintegrationintheprimaryvisual 128–142,2016.
cortex. NeuralComputation,10(4):903–940,1998.
Richards,B.A.,Lillicrap,T.P.,Beaudoin,P.,Bengio,Y.,Bogacz,
Liang,M.andHu,X. Recurrentconvolutionalneuralnetworkfor R.,Christensen,A.,Clopath,C.,Costa,R.P.,deBerker,A.,
objectrecognition. InIEEEConferenceonComputerVision Ganguli,S.,Gillon,C.J.,Hafner,D.,Kepecs,A.,Kriegeskorte,
andPatternRecognition(CVPR),pp.3367–3375,2015. doi: N.,Latham,P.,Lindsay,G.W.,Miller,K.D.,Naud,R.,Pack,
10.1109/CVPR.2015.7298958. URLhttps://doi.org/ C. C., Poirazi, P., Roelfsema, P., Sacramento, J., Saxe, A.,
10.1109/CVPR.2015.7298958. Scellier,B.,Schapiro,A.C.,Senn,W.,Wayne,G.,Yamins,D.,
Zenke,F.,Zylberberg,J.,Therien,D.,andKording,K.P.Adeep
Liao,Q.andPoggio,T. Bridgingthegapsbetweenresiduallearn- learningframeworkforneuroscience. NatureNeuroscience,22
ing,recurrentneuralnetworksandvisualcortex. arXivpreprint (11):1761–1770,2019.
arXiv:1604.03640,2016.
Rousseau,F.,Drumetz,L.,andFablet,R. Residualnetworksas
Lindeberg,T. Discretederivativeapproximationswithscale-space flowsofdiffeomorphisms. JournalofMathematicalImaging
properties:Abasisforlow-levelfeatureextraction. Journalof andVision,pp.1–11,2019.
MathematicalImagingandVision,3(4):349–376,1993.
Ruthotto,L.andHaber,E. Deepneuralnetworksmotivatedby
Lindeberg,T. Scale-spacetheoryincomputervision,volume256. partialdifferentialequations. JournalofMathematicalImaging
SpringerScience&BusinessMedia,2013. andVision,pp.1–13,2019.
Lindeberg,T.andFlorack,L. Fovealscale-spaceandthelinear Salman, H., Yadollahpour, P., Fletcher, T., andBatmanghelich,
increaseofreceptivefieldsizeasafunctionofeccentricity.KTH K. Deep diffeomorphic normalizing flows. arXiv preprint
RoyalInstituteofTechnology,1994. arXiv:1810.03256,2018.DeepContinuousNetworks
Sceniak,M.P.,Hawken,M.J.,andShapley,R.Contrast-dependent Van den Bergh, G., Zhang, B., Arckens, L., and Chino, Y. M.
changes in spatial frequency tuning of macaque v1 neurons: Receptive-fieldpropertiesofv1andv2neuronsinmiceand
effectsofachangingreceptivefieldsize. JournalofNeurophys- macaquemonkeys. JournalofComparativeNeurology,518
iology,88(3):1363–1373,2002. (11):2051–2070,2010.
Schrimpf,M.,Kubilius,J.,Hong,H.,Majaj,N.J.,Rajalingham, Worrall, D. E., Garbin, S. J., Turmukhambetov, D., and Bros-
R.,Issa,E.B.,Kar,K.,Bashivan,P.,Prescott-Roy,J.,Geiger,F., tow,G.J. Harmonicnetworks: Deeptranslationandrotation
Schmidt,K.,Yamins,D.L.K.,andDiCarlo,J.J. Brain-score: equivariance. InCVPR,July2017.
Whichartificialneuralnetworkforobjectrecognitionismost
Wu, Y.andHe, K. Groupnormalization. InECCV,pp.3–19,
brain-like? bioRxivpreprint,2018.
2018.
Sclar,G.andFreeman,R.Orientationselectivityinthecat’sstriate
Xiong,Z.,Yuan,Y.,Guo,N.,andWang,Q. Variationalcontext-
cortexisinvariantwithstimuluscontrast. ExperimentalBrain
deformableconvnetsforindoorsceneparsing. InCVPR,pp.
Research,46(3):457–461,1982.
3992–4002,2020.
Sejnowski,T.J. Theunreasonableeffectivenessofdeeplearning Yu,H.-H.,Verma,R.,Yang,Y.,Tibballs,H.A.,Lui,L.L.,Reser,
inartificialintelligence. ProceedingsoftheNationalAcademy D.H.,andRosa,M.G. Spatialandtemporalfrequencytun-
ofSciences,2020. inginstriatecortex:functionaluniformityandspecializations
related to receptive field eccentricity. European Journal of
Shelhamer,E.,Wang,D.,andDarrell,T.Blurringthelinebetween Neuroscience,31(6):1043–1062,2010.
structureandlearningtooptimizeandadaptreceptivefields.
arXivpreprintarXiv:1904.11487,2019. Zhuang,C.,Yan,S.,Nayebi,A.,Schrimpf,M.,Frank,M.,DiCarlo,
J.,andYamins,D. Unsupervisedneuralnetworkmodelsofthe
Simoncelli, E.P., Freeman, W.T., Adelson, E.H., andHeeger, ventralvisualstream. bioRxiv,2020a.
D.J. Shiftablemultiscaletransforms. IEEEtransactionson
InformationTheory,38(2):587–607,1992. Zhuang,J.,Dvornek,N.,Li,X.,Tatikonda,S.,Papademetris,X.,
andDuncan,J.Adaptivecheckpointadjointmethodforgradient
Skottun,B.C.,Bradley,A.,Sclar,G.,Ohzawa,I.,andFreeman, estimationinneuralODE. arXivpreprintarXiv:2006.02493,
R.D. Theeffectsofcontrastonvisualorientationandspatial 2020b.
frequency discrimination: a comparison of single cells and
behavior. JournalofNeurophysiology,57(3):773–786,1987.
Smith, A.T., Singh, K.D., Williams, A., andGreenlee, M.W.
Estimatingreceptivefieldsizefromfmridatainhumanstriate
andextrastriatevisualcortex. CerebralCortex,11(12):1182–
1190,2001.
Somers,D.C.,Nelson,S.B.,andSur,M. Anemergentmodelof
orientationselectivityincatvisualcorticalsimplecells.Journal
ofNeuroscience,15(8):5448–5465,1995.
Sompolinsky, H., Crisanti, A., and Sommers, H.-J. Chaos in
randomneuralnetworks. PhysicalReviewLetters,61(3):259,
1988.
Song,H.F.,Yang,G.R.,andWang,X.-J. Trainingexcitatory-
inhibitoryrecurrentneuralnetworksforcognitivetasks:asim-
pleandflexibleframework. PLoSComputationalBiology,12
(2):e1004792,2016.
Sosnovik,I.,Szmaja,M.,andSmeulders,A. Scale-equivariant
steerablenetworks. ICLR,2020.
Spoerer,C.J.,McClure,P.,andKriegeskorte,N. Recurrentcon-
volutionalneuralnetworks:abettermodelofbiologicalobject
recognition. Frontiersinpsychology,8:1551,2017.
Sussillo, D.andAbbott, L.F. Generatingcoherentpatternsof
activityfromchaoticneuralnetworks. Neuron,63(4):544–557,
2009.
Tang, H., Schrimpf, M., Lotter, W., Moerman, C., Paredes, A.,
Caro,J.O.,Hardesty,W.,Cox,D.,andKreiman,G. Recurrent
computationsforvisualpatterncompletion. Proceedingsofthe
NationalAcademyofSciences,115(35):8835–8840,2018.Deep Continuous Networks - Supplementary Material
ICML 2021 Submission
Paper ID: 3234
A Appendix
A.1 Gaussian multiscale local N-jet
Based on the Schwartz theory of smooth test functions, the Gaussian scale-space paradigm states
that the derivatives L (⃗x;σ) of a function L (⃗x) with respect to the spatial variables x , with
i1...in 0 i
i=1...d, and at scale σ is given by the convolution
L (⃗x;σ)=L ∂ G(⃗x;σ) (A.1) i1...in 0
∗
i1...in
where ∂ is the n-th order partial derivative and G(⃗x;σ) is the normalized, isotropic Gaussian
i1...in
kernel with standard deviation σ =σ and mean µ =0 (?). Note that L (⃗x) does not need to be a
i i 0
smooth function, and therefore the Gaussian scale-space paradigm can be applied to obtain local
image derivatives in different scales, where ⃗x denote the spatial coordinates and σ can be interpreted
as the coordinate in the scale dimension.
We build upon this definition of local image derivatives to build our structured receptive fields
(SRFs), similar to ?. The main idea we leverage is that using a Taylor approximation, one can
decompose an input image into a superimposition of its local derivatives. This decomposition can
then be performed by local convolution kernels in CNNs, where the relative weight of each derivative
order can be learned during training. In order to show this, we observe that the N-th order Taylor
expansion of an image L:R2 R around a point (a,b) is given by
→
N N −l (x a)l(y b)k ∂l+k
L(x,y)= − − L(a,b) (A.2)
l!k! ∂xl∂yk
l=0 k=0
XX
where the partial derivatives of L with respect to x and y can be interpreted as L (x,y;σ ) and
11...1n 0
L (x,y;σ ) from equation A.1 at some original scale σ . This means that, via the linearity of
21...2n 0 0
convolution, and under the assumption that L(x,y;σ) does not diverge, it is equivalent to either use
the N-th order derivatives of the input image, or use the N-th order derivatives of the Gaussian
function G(⃗x;σ), to perform the image decomposition at scale σ. The local N-jets can thus be seen
to be parametrized by the coefficients in the expansion given in equation A.2. By definition, we
consider the Taylor expansion coefficients to be covariant derivatives of the image L(x,y;σ), which
are independent of the local coordinate system, to reach the filter approximations given in equation 1
in the paper, where the coefficients take the form α .
l,k
In short, the multiscale local N-jet provides a natural decomposition of an image in a local
neighborhoodinthespatialandscaledimensions. UnderconvolutionwithG(⃗x;σ),thisdecomposition
providesaframeworkfordefiningconvolutionalfiltersinaCNNusingN-thorderTaylorpolynomials.
The SRF filters we use are based on this N-jet definition and allow us to learn the scale, spatial
frequencyandorientationofthefiltersduringtraining,whicharefundamentalpropertiesofbiological
receptive fields (??).
In addition, the Gaussian scale-space formulation of SRFs (?) lead to theoretically interesting
properties which strengthen the motivation for our choice of filters based on the Gaussian N-jet. For
1
4202
beF
2
]VC.sc[
1v75510.2042:viXraexample, the semi-group property of Gaussian scale-spaces indicates for a Gaussian derivative kernel
Gl,k(x,y;t) parametrized by the variance t=σ2
Gl,k(x,y;t+t)=Gl,k(x,y;t) G(x,y;t) (A.3)
′ ′
∗
where the superscripts (l,k) denote the partial derivatives with respect to x and y. This means that
a translation in scale dimension can be simply achieved through convolution with a (0-th order)
Gaussian kernel G. For DCNs, this means that we have a solid understanding of the scale of the
feature maps (in the sense of the Gaussian scale-space) at every layer m in the network, as we
know the value of σ after training. In the absence of SRFs with an explicit scale parameter, this
m
information is lost.
Similarly, SRF filters based on Gaussian derivatives are steerable by the coefficients a . For
l,k
example, a second order filter with orientation θ can be described as a sum of basis functions
G2,0 =a G2,0+a G1,1+a G0,2 (A.4)
θ 2,0 1,1 0,2
=cos2(θ)G2,0 2cos(θ)sin(θ)G1,1+sin2(θ)G0,2.
−
Finally, the set of Gaussian derivative basis functions Gl,k(x,y;σ) are spatially separable
Gl,k(x,y;σ)=Gl(x;σ)Gk(y;σ) (A.5)
which is useful for computational efficiency in numerical applications.
A.2 Training procedure
The basic architecture of all our models is given in figure 2 of the paper. Unless otherwise stated, in
allmodelsweusegroupnormalization(?) with32groupsineverylayerasthenormalizationfunction.
As the nonlinear activation, we use continuously differentiable exponential linear units (?), or CELU
for DCN models. Based on the original ODE-Net implementation (?), for ODE-Net models and
ResNet-block models, we use rectified linear units (ReLU). Likewise, when defining the integration
time interval T, we stick to the original implementation, with T =1 for ODE-Net models, whereas
weuseT =2forDCNmodels. Otherwise, allthehyperparametersarekeptconstantbetweenmodels.
For a brief overview of hyperparameter optimization, see appendix A.3.
All models are trained for 100 epochs on the standard CIFAR-10 training set (?) using cross-
entropy loss. As data augmentation, we use random translations up to 4 pixels in each spatial
dimension and random horizontal flips. Optimization is performed using SGD with a mini-batch
size of 128, initial learning rate 10 1, momentum 0.9, and a learning rate decay by a factor of 0.1 at
−
epochs 40 and 70. For continuous time models based on neural ODEs, we use the adjoint method for
backpropagating the losses and ODE solvers with error tolerance set to 10 3.
−
Forconvolutionallayerswithpixel-basedk kfilters,theweightsareinitializedusingthestandard
×
Kaiming uniform initialization (?). For layers using SRF filters, the initial α values are randomly
sampled from a normal distribution with mean 0 and standard deviation 0.1, and initial σ values are
sampled from a normal distribution with mean 0 and standard deviation 2/3.
FortherestrictedCIFAR-10experiments(small-dataregime),wepickthetotalnumberoftraining
images to be a multiple of our mini-batch size of 128, or otherwise have a minimal number of samples
in the final batch (which is dropped in each epoch). For comparisons with the baseline results
from ?, we used exactly the same number of training images as in Table 2 of ?. In order to confirm
convergence, for the training set sizes [520,1030,5120], we increased the number of training epochs
by a factor of [10,5,2] respectively.
2For meta-parametrized models the initial values for the learnable parameters follow normal
distributions (µ,σ ): a (0,2/3), b (0,0.1), for the DCN σ(t) model; a (0,2/3),
b (0,2/3N ), c N (0,0∼ .1)N for the DC∼ N N σ(t2) model; and a (0,2/3), b ∼ N (0,0.1),
s s
∼ N ∼ N ∼ N ∼ N
a (0,0.1) and b (0,0.05) for the DCN σ(t), α(t) model.
α α
∼N ∼N
For all our models using SRF filters, except for the DCN σji model, we use scale sharing within a
convolutionallayersuchthatσji =σ forallconvolutionallayersm,withinputchanneliandoutput
m m
channel j. This makes the GPU implementation trivial, as all filters within a layer are sampled
in the interval [ 2σ ,2σ ], and hence the convolutional kernel sizes within a layer are uniform.
m m
−
However, for the DCN σji model, a GPU implementation would be highly inefficient if we truncated
the kernels at a fixed factor of σji, independently for each input and output channel i,j. Therefore
for the DCN σji model we fix the kernel size at 7 7, but we still learn the shape and the scale
×
(bandwidth) of the filters during training.
We use the Dormand–Prince (DOPRI) method (?) as the numerical ODE solver. The DOPRI
method is an explicit, adaptive solver of the Runge-Kutta family, which uses 6 function evaluations
to compute fourth- and fifth-order accurate solutions to ODEs, along with an error estimate. The
size of the adaptive steps taken by the solver can be regulated by specifying an error tolerance on
this error estimate.
As is the case with most modern ODE solvers, the GPU implementation of the DOPRI solver
that we use (?) considers the input arguments for the integration time interval t [0,T] (or t [0,2]
∈ ∈
in the case of all DCN models) as soft bounds. This means that the DOPRI algorithm may explore
time points outside of this interval, based on its internal error estimation, and may then employ
interpolation to return solutions within the specified bounds. In the meta-parametrized models,
where for example σ is an explicit function of t, this may lead to very large or very small kernel sizes,
ordinarily unexpected within the integration time interval. In order to avoid numerical instability
and memory issues in the meta-parametrized models, we scale down and clip the integration time t
when passing it into the parameter definitions as σ(τt ) and α(τt ). We clip the t values in the
clip clip
interval [ 0.5,2.5] and use τ =0.5.
−
A.3 Hyperparameter tuning
As mentioned in appendix A.2, we share all the design choices and hyperparameters between all
DCN and baseline ODE-Net models, except for the nonlinear activation function and integration
time interval T.
Table A.1: CIFAR-10 validation accuracy (aver-
agedover3runs)inthecontrolexperimentstesting
theeffectofDCNmodeldesignchoicesonthebase-
line ODE-Net.
Model Accuracy (%)
DCN-ODE 89.46 0.16
±
ODE-Net (baseline) 89.60 0.28
±
ODE-Net with T =2 89.50 0.07
±
ODE-Net with CELU 89.33 0.16
±
ODE-Net with CELU and T =2 89.25 0.30
±
This difference in design choices arises since for the ODE-Net baseline we stay faithful to the
3original ODE-Net implementation, where ReLU is the nonlinear activation function and T = 1,
whereasforDCNmodelsweuseCELUastheactivationfunction, duetoitsgeneralizedcompatibility
with the adjoint method, and T =2. In order to verify that our design choices do not provide an
unfair advantage over the ODE-Net baseline, we run some control experiments, where we vary the
activation function and T in the ODE-Net baseline.
WefindthatthechangeofactivationfunctionsorintegrationintervalT donotprovideasignificant
increase to the CIFAR-10 classification performance in the ODE-Net baseline (Table A.1).
A.4 CIFAR-10 image reconstruction
For the reconstruction task, we use an encoder with 3 DCN-ODE blocks and a decoder with 3
DCN-ODE blocks (as shown in figure 2 of the paper). For the baseline networks we replace the 3
DCN-ODE blocks with ODE-Net or ResNet-blocks. The encoder is pre-trained on the CIFAR-10
classification task. We use the feature maps at the end of ODE Block 3 as the input to the decoder
network. The decoder DCN network is made up of 2 ODE blocks separated by bilinear upscaling,
1 1 convolutions to reduce the number of channels, normalization, non-linear activation and a final
×
1 1 convolution to generate the output in RGB space.
×
We implement reconstruction as a regression task and use the mean squred error (MSE) as the
loss function. Otherwise, the training parameters are the same as the classification experiments: We
use the SGD optimizer with a mini-batch size of 128, learning rate 10 1 and momentum 0.9, together
−
with the adjoint method and an error tolerance of 10 3.
−
Some example image reconstructions (randomly selected) from the CIFAR-10 validation set by
the DCN and baseline networks are shown in figure A.1.
A.5 Pattern completion in feature maps
In figure A.2, we show the feature map evolution within the first ODE block (or ResNet block) of
different models with and without masking of some example input images. Size of the mask depicted
here is 6 6 pixels and the example images were chosen so as to have the mask located close to the
×
middleoftheobject. Wepickedsomechannelswithvisiblemask-relatedartifactsintheinputfeature
maps to the first ODE (ResNet) block. Since there is no feature map trajectory in the ResNet-blocks
model, but only one input and one output feature map, the difference between the feature maps of
the intact and masked image is given as a scatter plot of two data points connected by a red line.
Figure A.3 depicts the average difference of intact and masked feature maps D(t) averaged over
1000 images and the standard deviation for the DCN and baseline networks.
4Input ResNet-
Image DCN-ODE ODE-Net Blocks
Figure A.1: Example CIFAR-10 validation images and their reconstruction by the DCN-ODE model
as compared to baseline models.
5input image him(0) him(T) input image him(0) him(T)
D(t) D(t)
0.6
1.0
masked image him_masked(0) him_masked(T) 0.4 masked image him_masked(0) him_masked(T)
0.5
0.2
0 2 0 2
integration time t integration time t
input image him(0) him(T) input image him(0) him(T)
D(t) D(t)
0.6
masked image him_masked(0) him_masked(T) 0.4 masked image him_masked(0) him_masked(T) 1.0
0.5
0.2
0 1 0 1
input image him(0) him(T) input image him(0) him(T)
D(t) D(t)
0.6
masked image him_masked(0) him_masked(T) 0.4 masked image him_masked(0) him_masked(T) 1.0
0.5
0.2
0 1 0 1
Figure A.2: Feature map evolution within the first ODE block (or ResNet block) of different models.
DCN ODE-Net ResNet-Blocks
1.0 1.0 1.0
0.5 0.5 0.5
0.0 0.0 0.0
0 2 0 1 0 1
integration time t integration time t integration time t
Figure A.3: Evolution of the mean difference D(t) between feature maps of an intact input image
and a masked input image, averaged over 1000 images in the CIFAR-10 validation set. The shaded
areas (or in the case of ResNet, the errorbars) show the standard deviation.
6
NCD
teN-EDO
skcolb-teNseR
)t(D )t(D
NCD
teN-EDO
skcolb-teNseR
)t(D