IEEETRANSACTIONS,VOL.XX,NO.XX,XX2023 1
STAA-Net: A Sparse and Transferable Adversarial
Attack for Speech Emotion Recognition
Yi Chang, Zhao Ren∗, Member, IEEE, Zixing Zhang, Senior Member, IEEE, Xin Jing, Kun Qian∗, Senior
Member, IEEE, Xi Shao, Member, IEEE, Bin Hu, Fellow, IEEE, Tanja Schultz, Fellow, IEEE,
Bjo¨rn W. Schuller, Fellow, IEEE
✦
Abstract—Speechcontainsrichinformationontheemotionsofhumans, 1 INTRODUCTION
andSpeechEmotionRecognition(SER)hasbeenanimportanttopicin
theareaofhuman-computerinteraction.TherobustnessofSERmodels S PEECHEmotionRecognition(SER)hasbeenwidelyapplied
is crucial, particularly in privacy-sensitive and reliability-demanding in many areas in human-computer interaction [1], such
domainslikeprivatehealthcare.Recently,thevulnerabilityofdeepneural as diagnosis of depression and bipolar disorder [2], computer
networks in the audio domain to adversarial attacks has become a games[3],andintelligentcallcentres[4].Recently,duetotherapid
popularareaofresearch.However,priorworksonadversarialattacksin
developmentofefficientcomputingresourcesandadvanceddeep
theaudiodomainprimarilyrelyoniterativegradient-basedtechniques,
learningmethods,DeepNeuralNetworks(DNNs)haveachieved
whicharetime-consumingandpronetooverfittingthespecificthreat
good performance in SER [5]–[12]. From the perspective of the
model.Furthermore,theexplorationofsparseperturbations,whichhave
thepotentialforbetterstealthiness,remainslimitedintheaudiodomain. inputtypes,thereareprimarilytwocategoriesofDNNsforSER.
To address these challenges, we propose a generator-based attack One category utilises spectrum-based features [5], [6] while the
methodtogeneratesparseandtransferableadversarialexamplesto other processes audio recordings in an end-to-end manner [7],
deceiveSERmodelsinanend-to-endandefficientmanner.Weevaluate [12]. More recently, foundation models (e.g., wav2vec 2.0 [13],
ourmethodontwowidely-usedSERdatasets,DatabaseofElicitedMood or HuBERT [14]) pre-trained on large relative datasets (e.g.,
inSpeech(DEMoS)andInteractiveEmotionaldyadicMOtionCAPture Librispeech [15]) have been successfully fine-tuned on smaller
(IEMOCAP),anddemonstrateitsabilitytogeneratesuccessfulsparse
datasetforthespecifictaskathand[10],[11].
adversarialexamplesinanefficientmanner.Moreover,ourgenerated
Even though adversarial attacks were first demonstrated and
adversarial examples exhibit model-agnostic transferability, enabling
effectiveadversarialattacksonadvancedvictimmodels. have been extensively studied in the image domain [16]–[19],
recentresearchhasfoundthatanadversaryalsoposessignificant
securityandprivacythreatstotheaudiodomain,suchasautomatic
IndexTerms—Speechemotionrecognition,adversarialattacks,sparsity,
speechrecognition[20]–[23],speakerrecognition[24]–[27],and
transferability,efficiency,end-to-end
classificationofacousticscenesandevents[28].Moreover,adver-
sarialattacksonSERcanalsocausesevereissues.Forinstance,
ThispaperwasfundedpartiallybytheDeutscheForschungsgemeinschaft(DFG, these attacks can be used to spread toxic or hateful speech on
GermanResearchFoundation)undertheproject“MyVoice:MyoelectricVocal
socialmediaplatformsoronlinegamingplatforms,compromising
InteractionandCommunicationEngine”(460884988),theChinaScholarship
Council(CSC),Grant#202006290013,theNationalKeyR&DProgramof publicsafety.Additionally,maliciousactorscanmanipulatespeech
China(No.2023YFC2506804),theMinistryofScienceandTechnologyofthe contenttodeceiveautomaticspeechrecognitionsystemsandfurther
People’sRepublicofChinawiththeSTI2030-MajorProjects2021ZD0201900, manipulate SER models, potentially leading to the propagation
theNationalNaturalScienceFoundationofChina(No.62272044),theTeli
of harmful content [29]. Furthermore, in the context of mental
YoungFellowProgramfromtheBeijingInstituteofTechnology,China,the
NationalKeyResearchandDevelopmentProject(No.2020AAA0106200),and illnesspre-screening,targetedattacksonspeechdatacouldresult
theNationalNatureScienceFoundationofChinaunderGrants(No.61936005, inincorrectdiagnosesandinappropriatetreatmentforpatients[2].
No.62001038).
Giventheseconcerns,enhancingtherobustnessofSERmodelshas
Y. Chang and B.W. Schuller are with the GLAM – the Group on
emergedasacrucialresearcharea.Atpresent,however,thereare
Language, Audio, & Music, Imperial College London, United Kingdom
(y.chang20@imperial.ac.uk,schuller@ieee.org). relativelyfewstudies[30],[31]onadversarialattacksforSER.
Z.RenandT.SchultzarewiththeUniversityofBremen,Germany(zren@uni-
Previous adversarial attack techniques in the audio domain
bremen.de,tanja.schultz@uni-bremen.de).
have limitations in terms of their practicality and transferability.
Z.ZhangiswiththeCollegeofComputerScienceandElectronicEngineering,
HunanUniversity,China(zixingzhang@hnu.edu.com). White-boxapproaches(e.g., [20],[23],[25],[32])areoftenless
K.QianandB.HuarewiththeSchoolofMedicalTechnology,BeijingInstitute realistic in the real-world since the adversary has access to all
ofTechnology,China(qian@bit.edu.cn,bh@bit.edu.cn).
informationoftheattackedmodels;iterativegradient-basedattacks
X. Shao is with the College of Telecommunications and Information En-
gineering, Nanjing University of Posts and Telecommunications, China, often suffer from low transferability (e.g., [33], [34]), which
(shaoxi@njupt.edu.cn). can be attributed to the optimisation of perturbations using the
X. Jing and B.W. Schuller are also with the Chair of Embedded Intelli- gradientinformationofthevictimmodelwithrespecttoaspecific
gence for Health Care and Wellbeing, University of Augsburg, Germany
input.Additionally,most existingattacksonaudiotasksimpose
(xin.jing@informatik.uni-augsburg.de).
Correspondingauthors:ZhaoRen,KunQian constraints on the l 2 [20], [26] or l ∞ [22]–[25], [27] norms of
4202
beF
2
]DS.sc[
1v72210.2042:viXraIEEETRANSACTIONS,VOL.XX,NO.XX,XX2023 2
theadversarialperturbations,asthel normisatypicalNP-hard speech representations, along with a Transformer module that
0
problem[18],[35].However,sparseperturbationshavethepotential captures global contextual dependencies. Wav2vec 2.0 adopts a
toenhancethestealthinessofaudioattacksandprovideinsightsinto self-supervisedlearningapproach,wherethemodelistrainedon
therobustnessofDNNsintheaudiodomain,makingl -constrained a massive amount of speech data with a contrastive objective to
0
attackersworthexploring.Nonetheless,directlyadaptingexisting learn discriminative representations. A Wav2vec 2.0 model [13]
l -constrainedattackersfromtheimagedomaintoaudiofacestwo hasbeenwidelyadoptedinSERresearch[9],[11],[12],[40],[41]
0
challenges.Firstly,thepotentialhighdimensionalityof1-Dtime- becauseofitsremarkablecapabilityinextractingrepresentations.
sequentialaudiosignalscanhindertheefficiencyoftheattacker. TheHidden-UnitBERT(HuBERT)[14]appliesanarchitecture
Secondly,thesequentialinformationinherentinaudiomaynotbe similar to the one of wav2vec 2.0. HuBERT also applies self-
effectivelyextractedbysuchattackers. supervisedlearningbutwithadditionalauxiliarytasks(e.g.,frame-
Toaddresstheaforementionedchallenges,weproposeSTAA- wisefeaturespredictions),whichpromotesthemodel’sabilityto
Net, a generator-based adversarial attack method for end-to-end learncombinedacousticandlanguagefeaturesovertherawspeech
speechSER.OurapproachleveragesanadjustedWave-U-Net-like data.Moraisetal.[42]fine-tunedHuBERTasupstreammodelto
generatortogeneratesparseaudio adversarialperturbationsina providegeneratedutteranceembeddingsforemotionclassification.
singleforwardpass,enablingefficientandtransferableattacks.We WavLM [43] builds upon the success of self-supervised pre-
validate the effectiveness of STAA-Net through experiments on traininginspeechprocessingandaimstotacklefull-stackspeech
two widely-used emotional speech datasets. To the best of our processingtasksbyleveraginglarge-scaleunlabelleddata.Tobetter
knowledge, this study is the first to explore sparse adversarial capture the sequence information in the audio, WavLM further
attacks in the audio domain. Moreover, this work alleviates the extends the HuBERT approach by employing a gated relative
scarcityofadversarialattackstudiesinSER. position bias in the Transformer structure and augmenting the
Therestofthepaperisorganisedasfollows.Section2provides training data with an utterance mixing strategy. Feng et al. [44]
anoverviewofrelatedstudiesonSERandadversarialattacksin appliedtheWavLMforembeddingsextractionandalsoexplored
the audio domain. Section 3 depicts the detailed methodology. itstrustworthiness.
Section 4 presents the datasets applied and experimental setups.
Section 5 describes the results and provides an analysis of the
2.2 AdversarialAttacksintheAudioDomain
findings.Finally,Section6concludesthepaper.
Adversarial attacks in the audio domain can be categorised into
2 RELATED WORK two main types: iterative gradient-based attacks and generator-
based attacks. Iterative gradient-based attacks typically operate
2.1 End-to-endSER
in a white-box setting, utilising gradient information from the
Even though many SER systems with conventional machine victim model to iteratively find minimal perturbations that can
learning techniques utilise hand-crafted acoustic features (e.g., deceive the model. Carlini et al. [20] used a white-box iterative
Mel-frequencycepstralcoefficients(MFCC)features)asinput,the optimisation-based attack to turn any audio waveform into any
selectionofthesefeaturescanintroducebiasandtakeextratime. targettranscriptions.Neekharaetal.[21]discoveredaudio-agnostic
In recent years, there has been a growing interest in DNNs that universal quasi-imperceptible adversarial perturbation through
directlyprocessrawaudiosignals,bypassingtheneedformanual iteratively optimising the normalised Levenshtein distance for
feature extraction and potentially offering more comprehensive automatic speech recognition systems. Kim et al. [23] found
representationsforSERtasks. thetransferabilityofadversarialexamplesisrelatedtothenoise
Train-from-scratchModels.Tzirakisetal.[7](denotedasEmo18) sensitivityandproposedanoiseinjectedattackmethodtogenerate
proposedaconvolutionrecurrentneuralnetworkstructureinitially transferableadversarialexamplesbyiterativelyinjectingadditive
for continuous emotion recognition (e.g., arousal and valence). noiseduringthegradientascentprocess.Zhangetal.[24]employed
Emo18iscomposedof3convolutionallayersforfeatureextraction aProjectedGradientDescent(PGD)attackwiththemomentum
fromtherawaudiosignaland2-layerLongShort-TermMemory methodtogeneratetext-independentadversarialperturbationsfor
(LSTM) module for contextual dependencies. Zhao et al. [5] speakerverificationsystems(SVS).Chenetal.[26]attackedSVS
(denoted as Zhao19) proposed a similar network for discrete with a Fast Gradient Sign Method (FGSM) attack, PGD attack,
emotionrecognition,anditwascomposedof4convolutionallayers Carlini-Wagner(CW)attack,andFAKEBOB[25]attacktoaddress
and2stackedLSTMlayers.Emo18andZhao19arealsoutilised theoptimisationproblem.
and compared in [36] for the audio modality. Zhang et al. [37] Recently,generativemodelssuchasGenerativeAdversarialNet-
employedanattentionmechanismandamulti-tasklearningstrategy works(GAN)[45]andautoencoders[46]haveshownpromisein
to enhance the robustness of audio representations for emotion generatingadversarialperturbations.Comparedtoiterativegradient-
recognition. Sun et al. [38] utilised a gender information block based attackers, generator-based methods focus on learning the
besidestheresidualCNNblocktoimprovetherecognitionaccuracy. distributions of the training data, resulting in more transferable
Tzirakisetal.[39]fusedthehigh-levelsemanticinformationfrom perturbations.Xieetal.[28]proposedatargetattackapproachon
Word2Vec and Speech2Vec models and low-level paralinguistic variousaudiotasksbyconcatenatingthetargetclassembedding
featuresextractedbyCNNblocksforbetterperformance. featuremapwiththeintermediatefeaturemapofthegenerator.
Foundation Models.Bypre-trainingonalargeamountofdata,
foundationmodelslearnrobustrepresentationsthatcaptureboth
3 METHODS
acousticandlinguisticpropertiesofspeech,enablingittotransfer
knowledgeeffectivelytovariousspeechprocessingtasks. In this section, we first formulate the research problem in Sec-
Wav2vec 2.0 comprises a Convolutional Neural Network tion3.1,andthenintroducetheproposedapproachinSection3.2.
(CNN) module that serves as the feature encoder for latent ThedescriptionofthelossfunctionsisfinallygiveninSection3.3.IEEETRANSACTIONS,VOL.XX,NO.XX,XX2023 3
Up-sampling
Block
1
Clip
Perturbation
Magnitudes
... ...
Binary
Quantisation Perturbation
Original Down-sampling Positions
Audio Block
Fig.2.Wave-U-Netarchitecture.→describesdown-andup-sampling;
(cid:57)(cid:57)(cid:75)meansconcatenation.
Up-sampling
Block
2
3.2.1 OverallArchitecture
TheproposedframeworkisdepictedinFig.1.Sinceourframework
operates on an end-to-end basis, the original raw audio is fed
into the generator to produce the perturbation magnitudes and
positions.Theperturbationmagnitudesaimtolimitthevaluesof
theperturbationsateachframe,whiletheperturbationpositionsare
Adversarial Threat SER Emotion targetedtobesparse.Next,thesparseperturbationsarecalculated
Audio Model Class by multiplying the perturbation magnitudes with the positions.
Afterwards, the sparse perturbations are added to the original
Fig.1.OverallarchitectureoftheproposedSTAA-Net. audio samples to create adversarial audio samples. The training
of the generator aims to fool the local threat SER models and
thegeneratedadversarialaudiosamplescanalsobetransferredto
3.1 ProblemFormulation
attackotherunseentargetedSERmodels.
Wedenoteanoriginalaudioasx,itsgroundtruthemotionalclass
asy,addedadversarialperturbationasδ,thevictimSERmodelas 3.2.2 Wave-U-Net
f,andthecorrespondingadversarialexamplex
adv
= x+δ.In
U-Net [48] was originally developed for accurate and efficient
theun-targetedattackscenarioappliedinthiswork,weaimto: biomedicalimagesegmentation.Itutilisesasymmetricencoder-
decoder structure with skip connections. WaveNet [49], on the
other hand, is a deep generative model specifically designed for
minimise∥δ∥ ,
0 speechsynthesis.Itleveragestherepeatedapplicationofdilated
subjecttoargmaxf (x adv)̸=argmaxf (x)
convolutions,wherethedilationfactorsincreasesexponentially,to
and ∥δ∥ <ϵ, modelthelong-termdependenciesinaudiosignals.However,its
∞
highmemoryconsumption,attributedtothehighsamplingrateof
where ϵ is a pre-defined hyper-parameter that promotes the audio,haslimiteditsapplicationinreal-timescenarios.
imperceptibility of the added adversarial perturbation. However, Wave-U-Net [50] extends the U-Net architecture to tackle
directlysolvingtheaboveproblemisNP-hard.Inspiredby[35], audiosourceseparationtasks.ItcombinesthebenefitsofU-Net’s
[47]intheimagedomain,wefactorisethesparseperturbationδ to encoder-decoderstructureandWaveNet’sabilitytocapturelong-
element-wiseproductoftwovectorsasfollows: termdependencies.AsdepictedinFigure2,inthedown-sampling
stage,Wave-U-Netreducesthetemporalresolutionbydiscarding
δ =v⊗m, (1)
featuresforeveryothertimestep;intheup-samplingstage,linear
wherev∈RN denotestheperturbationmagnitudes,m∈{0,1}N interpolation is employed to up-sample the feature maps while
representstheperturbationlocations,N describesthenumberof alsoconcatenatinghigher-levelfeatureswithlocalfeatures.This
time frames of an audio waveform, and ⊗ denotes the element- approachenablesWave-U-Nettocapturelonger-termdependencies
wiseproduct.Inthetrainingprocedure,l regulisationisapplied inaudiosignals.
1
onmtopromptasparseperturbation.Theabovetwovectorsare As described in Section 1, many previous studies on audio
optimisedseparately:onemodulegeneratesvandtheothermodule adversarial attacks have predominantly relied on the gradient
producesm.Becausemisabinaryvector,wherethevalueonthe information of parameters of f with respect to x, leading to
timeframeiisperturbedifm =1andunperturbedifm =0,it limited cross-model transferability. To address these challenges,
i i
cannotbedirectlyoptimisedwithgradientback-propagation.To weproposetheadoptionofWave-U-Net[50]asthegeneratorin
addressthisissue,a0-1randomquantisationoperationisapplied. this work with one down-sampling block and two up-sampling
blocks:UB isforperturbationmagnitudesandUB controlsthe
1 2
perturbationlocations.
3.2 ProposedApproach
To introduce the proposed approach, the overall architecture is 3.2.3 PerturbationMagnitudesandPositions
firstgiveninSection3.2.1,followedbythedescriptionofthetwo Togenerateimperceptibleandsparseadversarialaudiosamples,we
parts:i)Wave-U-Netandii)PerturbationMagnitudesandPositions. dividethefinalperturbationintomagnitudesandpositions.Onthe
Finally,wedescribethetrainingprocedureofthegenerator. onehand,themagnitudescontroltheperturbationvaluestoensureIEEETRANSACTIONS,VOL.XX,NO.XX,XX2023 4
theyareextremelysmall,therebymakingthemnon-distinguishable whereϕisaconfidencepre-definedhyper-parametertocontrolthe
fromrealdata.Ontheotherhand,thepositionsfurthermitigatethe attackstrength.
negativeimpactonaudioqualityfromperturbationsbyreducing MagnitudeLoss.Followingtheapproachofpreviousworks[28],
thenumberofframesaffected. [53],weincorporateanl normregularisationtermtoensurethat
2
PerturbationMagnitudes.AfterUB ,aclipoperationisapplied thegeneratedadversarialperturbationremainsimperceptiblewhile
1
to bound the perturbation value into {−ϵ,+ϵ}, where ϵ is a allowingcontroloverthestrengthoftheattack.Thisisachieved
pre-defined hyper-parameter to constrain the l ∞ norm. The clip byaddingthel 2 normontheclippedperturbation,asshowninthe
operationthenleadstotheperturbationmagnitudes. followingequation:
L =∥v∥ . (4)
PerturbationPositions.Togeneratesparseperturbations,weuse mag 2
quantisation to convert the output of UB into binary represen-
2 Sparsity Loss. Directly constraining the l norm on the added
0
tationsastheperturbationpositions.Specifically,ifdenotingthe
perturbationδ isNP-hardasdiscussedbeforehand.Byfactorising
outputofUB as∂,inordertotransferitintoadiscretevector
2 theδ perturbationmagnitudevandperturbationlocationsm,we
m∈{0,1}N ,wepass∂ intoabinaryquantisationas:
cancontrolthesparsityofδ withthel normofm,sinceitonly
1
(cid:26) 0, ∂ ≥γ contains values 0 and 1, where 1 means the value is perturbed.
q(∂ i)= 1, ∂i <γ, (2) Therefore,wehavethesparsitylossasfollows:
i
L =∥m∥ . (5)
whereγ isahyper-parameter.Thebinaryquantisationdescribed spa 1
aboveperformswellduringinference,butitcanencounterissues Inthisway,thesparsityofthefinalperturbationdependsonhow
with gradient vanishing during training [51]. To address this, thegeneratorconverges.
a randomisation approach is introduced to ensure the gradual
Quantisation Loss. As explained in Section 3.2.1, during the
convergence of ∂ towards either 1 or 0. Specifically, a random
i trainingphase,thedecisiontoperformbinaryquantisationisdeter-
numbertisgeneratedfromauniformdistributionontheinterval
minedbyarandomnumbersampledfromauniformdistribution.
[0,1). If t ≥ 0.5, the binary quantisation operation is applied;
Incontrast,duringinference,binaryquantisationisalwaysapplied.
otherwise,thebinaryquantisationisnotperformed.
Consequently,therecanbeaperformancedisparitybetweenthe
generatorduringtrainingandinference.Tomitigatethisgap,we
3.2.4 TrainingoftheGenerator
introducethequantisationloss,whichisdefinedasfollows:
Thetrainingprocedureinvolvesfeedingtheadversarialexample
L =∥∂−m∥ . (6)
x intothethreatSERmodelf foremotionclassification.During qua 2
adv
thisprocess,thegenerator’sparametersareoptimisedtoidentify
OverallLoss.Theoveralllossiscalculatedastheweightedsum
andmanipulatesignificantpatternswithinthetrainingdata.These
oftheaforementionedlosses,expressedbytheequation:
patternspossessalevelofgeneralisability,whichpotentiallypro-
motesthemodel-independenceofthecraftedadversarialexamples. L=L adv+λ m·L mag+λ s·L spa+λ q·L qua, (7)
By leveraging these generalised patterns, the crafted adversarial
whereλ ,λ ,andλ representtheweightsassignedtothemag-
audiox caneffectivelydeceiveandattackothermodels. m s q
adv nitudeloss,sparsityloss,andquantisationloss,respectively.These
Wecarefullychooseoneoftheend-to-endtrain-from-scratch
weightsallowforfine-tuningtheinfluenceofeachcomponentin
models(i.e.,Emo18)andoneofthepre-trainedfoundationmodels
theoveralllossfunction.Byoptimisingtheoverallloss,thelocal
(i.e.,wav2vec2.0)aslocalthreatmodeltosupervisethegenerator
threatmodelguidesthetrainingofthegeneratortogeneratesparse,
training. Notably, if the generated sparse adversarial examples,
imperceptibleperturbationsthatachieveahighattacksuccessrate
undertheguidanceofrelativelysimplerthreatmodel,canstillfool
(ASR)(definitioncanbefoundinSection4.3).
moreadvancedvictimmodelssuccessfullyinanefficientmanner,it
wouldsignifythestrongcapabilityoftheadversary.Meanwhile,it
providesinsightsintothevulnerabilitiesandrobustnessofadvanced 4 EXPERIMENTAL IMPLEMENTATIONS
modelswhenfacingadversarialattacks,showcasingtheimportance 4.1 Datasets
ofdevelopingdefensemechanismsagainstsuchattacks.
TABLE1
EmotiondistributionoftheIEMOCAPdataset.
3.3 LossFunctionsDesign
Thetrainingofthegeneratorisguidedbyacombinationoflosses. # Anger Happiness Neutral Sadness (cid:80)
Specifically,anadversariallossisusedtosuccessfullyattackthe Train 536 1,047 1,130 636 3,349
targetedSERmodel;amagnitudelossaimstomaketheadversarial Val 327 303 258 143 1,031
Test 240 286 320 305 1,151
perturbations imperceptible; a sparsity loss encourages sparse (cid:80) 1,103 1,636 1,708 1,084 5,531
perturbations; and a quantisation loss bridges the performance
gapbetweentrainingandinference.
DEMoS:TheDEMoSdataset[54]usedinthisstudycomprises
AdversarialLoss.WeemploytheCarliniandWagner(C&W)[52] approximately7.7hoursofItalianemotionalspeechrecordings.It
loss as the primary objective function for crafting adversarial involvesatotalof68speakers,including23femalesand45males.
audioexamples.TheC&Wlossaimstoincreasetheprobabilityof Withoutconsideringthe332neutralspeechsamplesasprevious
misclassificationasfollows: works [9], [30], we employ the 9,365 speech samples (average
(cid:26)(cid:18) (cid:19) (cid:27) duration:2.86seconds±standarddeviation:1.26seconds),which
L adv =max f y(x adv)−max {f i(x adv)} ,−ϕ , (3) are categorised into seven classes: anger, disgust, fear, guilt,
i̸=yIEEETRANSACTIONS,VOL.XX,NO.XX,XX2023 5
happiness, sadness, and surprise. We split the dataset into 40% theoptimalpixelvalueandlocationtoleadtothemis-classfication
training, 40% validation, 30% testing in a speaker-independent oftheattackedmodel.Theoptimisationalgorithmsappliedthemost
manner. The detailed emotion distribution can be found in our areevolutionarystrategies.Theone-pixelattackalsoshowssome
previouswork[9]. transferabilityin[58].Inthiswork,weadjusttheone-pixelattack
IEMOCAP:TheInteractiveEmotionaldyadicMOtionCAP- methodtothe1-Ddimensionofaudiosignalsandaconstraintof
ture (IEMOCAP) database [55] consists of approximately 12 theperturbationmagnitudeϵisalsoappliedforimperceptibilityof
hours of English audio-visual recordings. Five pairs of actors perturbation.
(afemaleandamale)participatefiverecordingsessions(i.e.,1–5)
respectivelybyeitherimprovisingaffectivescenariosorperforming 4.3 ExperimentalSettings
theatrical scripts. The recorded dialogues are further manually
AudioPre-processing.Allaudiorecordingsaredown-sampledto
segmentedintoutterances,whicharecategorisedbyatleastthree
16kHz for faster processing. For the DEMoS dataset, the audio
annotators into different emotional states, i.e., anger, disgust,
lengths are unified to the maximum duration of 6.0 seconds by
excited state, fear, frustration, happiness, neutral state, sadness,
repeatingshortersamplestomatchthedesiredlength.Asforthe
andsurprise.
IEMOCAP dataset, sincethere is a huge differencebetween the
SimilartopriorworksonIEMOCAP[6],[56],[57],onlyfour
maximum duration (34.1 seconds) and the duration at the 90-th
classesareincludedinthisworktomitigatetheclassimbalance,
percentile (8.7 seconds), all audio durations are unified to 8.7
includinganger,happiness,neutralstate,sadness.Moreover,to
secondsbyremovingextrasignalsandrepeatingshortersamples
better compare with prior works [12], [56], [57], we merge the
classexcitedstateintohappiness.Asaresult,intotal5,531audio asnecessary.
samplesareappliedinthiswork(averageduration:4.55seconds ModelsPreparation.Inthiswork,weexperimentwithfourwidely
±standarddeviation:3.23seconds).Themajorityofpriorworks appliedend-to-endmodelswithrawaudiowavesasinput:Emo18,
donotsetavalidationdatasetexplicitlyandperform5-foldcross- Zhao19, wav2vec 2.0, and WavLM. The batch size is 8 and the
validation[12],[56],[57].Inordertomaintainconsistencywithour modeldevelopmentissupervisedbycross-entropyloss.TheEmo18
experimentsonDEMoSandconsideringthattheappliedadversarial andZhao19modelsaretrainedfromscratch.Thetrainingprocess
attack baselines are time-consuming, we randomly select three isoptimisedwiththeAdamoptimiserwithaninitiallearningrate
sessions (Session 1, 2, and 5) as the training set, one session of 1e−3 and stopped after 30 epochs. The wav2vec 2.0 and
(Session4)asthevalidationset,andtheremainingsession(Session WavLM models used in this study have been pre-trained on the
3)asthetestset.TheemotiondistributionisdescribedinTable1. 960-hourLibrispeechcorpus[15].Forthefine-tuningofSER,the
featureencoderisfrozen,andtheclassificationheadconsistingof
twolinearlayersisaddedtomakepredictionsbasedonthelearnt
4.2 AppliedAdversarialAttackBaselines
representations. The fine-tuning procedure employs the Adam
Many prior works focus on adversarial attacks with l 2 or l ∞ optimiser with an initial learning rate of 3e−5, and is stopped
constrains,perturbingallvaluesonthetime-frameaxis,whereas after20epochs.
sparseadversarialattackswithl constraintargetonlyafewvalues
0 ImplementationsofAppliedAdversarialAttackBaselines.The
tofoolthevictimmodel.
PGD, SparseFool, and one-pixel attack12 methods are adjusted
ProjectedGradientDecent(PGD).ProjectedGradientDecentis based on their pytorch implementations. For PGD, the number
proposedin[17]andititerativelyappliessmallperturbationstothe ofperturbedlocationsonthetimeaxisoftheaudiowaveformis
inputdatabasedonthegradientofthelossfunctionwithrespect settoachievecomparablesparsitywiththeproposedSTAA-Net.
totheinput. The one-pixel attack method specifically sets this number as 1,
whileSparseFooldynamicallydeterminesthenumberofperturbed
xi+1 =clip(xi+αsign(∇ L(θ,xi,y)). (8)
xi locations.Tomaintainconsistency,amaximumof20iterationsteps
Ineachiteration,themagnitudeoftheperturbationiscontrolledby issetforallthreemethods.Additionally,tomaintainstealthiness
astepsizeαandaftereachiteration,thereisaprojectionoperation as in STAA-Net, the perturbation bound ϵ is set to 0.05 for the
clip(·)tomakesurethegeneratedcurrentadversarialexamplesare DEMoSand0.01forIEMOCAPdatasets.
withinapre-definedrange(i.e.,ϵ-ball). GeneratorTraining.Accordingtotheirmodelcomplexity(num-
Usually,PGDisconsideredasanl 2-constrainedattack.How- ber of parameters), we divide the above four models into two
ever, in this work, to generate sparse adversarial perturbations, sub-groups:Emo18(1.30M)andZhao19(1.01M),Wav2vec2.0
weintroducethesparsityconstraintdirectly.Specifically,ineach (90.37M)andWavLM(90.38M).Inourstudy,wecarefullyselect
iteration,weonlyperturbapre-definednumberofpositionsalong Emo18andWav2vec2.0asthelocalthreatmodelstoguidethe
thetimeframesoftheaudio,whilekeepingtheremainingpositions trainingofourgenerator.Thesemodelsprovideasolidfoundation
unchanged. forsupervisingthegenerationofadversarialexamples.Additionally,
SparseFool.SparseFoolproposedin[18]isageometryinspired we utilise Zhao19 and WavLM as the victim models to better
sparseattackmethodonanimage.Byestimatingtheimpactofeach evaluatetheeffectivenessofthetransferredadversarialexamples.
pixelonthemodel’sdecisionboundarywithalinearapproximation, Theweightsassignedtothemagnitudelossλ m,sparsityloss
onlythepixelswiththehighestimpactareselectedforperturbation.
λ s,andquantisationlossλ
q
aresetas1e−3,1e−4,and1e−4,
Theselectedpixelsaremodifiedunderasparsityconstraint(i.e., respectively. As for the binary quantisation, the γ is set as 0.5.
ϵ)ineachiteration. Whentrainingthegeneratorusingend-to-endtrain-from-scratch
modelsontheIEMOCAPdataset,weobservethattheperformance
One-PixelAttack.Onepixelattackwasproposedin[19]withthe
target to attack models through perturbing just one pixel of the 1.https://github.com/Harry24k/adversarial-attacks-pytorch
image.Itsoptimisation-basedmethodhastheobjectiveoffinding 2.https://github.com/DebangLi/one-pixel-attack-pytorchIEEETRANSACTIONS,VOL.XX,NO.XX,XX2023 6
ofEmo18isnotasgoodasofthepre-trainedmodels.Consequently, TABLE3
wedecidetolowertheweightsforλ ,λ andλ byafactorof SERmodels’performances(Accuracy/UAR[%])onIEMOCAP.
m s q
0.1.Fortheclipoperation,wesettheboundϵas0.05forDEMoS
and0.01forIEMOCAP.Thisoperationensuresthattheattackon Models Validation Test
the IEMOCAP dataset is relatively less potent compared to the Liuetal. [10] -/- 64.8/-
Pepinoetal. [40] -/- -/67.2
attackontheDEMoSdataset.
Chenetal. [12] -/- -/74.3
Thebatchsizeissetto8andweutilisethe‘Adam’optimiser
Santosoetal. [62] -/- -/75.9
withaninitiallearningrateof1e−4.Thelearningrateisdecayed Emo18 54.03/52.81 51.26/52.40
by a factor of 0.5 every 5 epochs to facilitate convergence. The Zhao19 53.93/54.17 52.48/52.74
generatortrainingprocessisstoppedafter20epochs. Wav2vec2.0 67.12/68.07 66.46/66.74
WavLM 68.28/67.08 67.07/66.90
Evaluations Metrics. (1) Unweighted Average Recall (UAR) is
utilised as the standard evaluation metric to mitigate the class
imbalanceissue[59],apartfromaccuracy(i.e.,weightedaverage
alsotransferredtoattackothermodelsinablack-boxmanner.It
recall). (2) Attack Success Rate (ASR) calculates the ratio of the
needstobementionedthatSNRvalueandsparsityareaveraged
numberofadversarialexamplescausingamiss-classificationtothe
basedonthesuccessfuladversarialexamplesforafairevaluation.
numberoftotaladversarialones,describingthefoolingpowerof
Therefore, we can see that adversarial examples generated by
attackers. (3) Signal-to-Noise Ratio (SNR) expressed in decibels
the one-pixel attack with Emo18 do not have SNR and sparsity
(dB) measures the relative noise level of perturbation δ to the
i values since the one-pixel attack achieves 0.00% attack success
originalaudiox :SNR(x ,δ )=20∗log max(xi).Thelarger
i i i 10max(δi) rate(ASR).RegardingSNR,alladversarialexampleshavevalues
theSNR,themoreimperceptibletheaddedperturbation.According
around17dB,whichcanberegardedasimperceptibleaccording
to[60],anSNR(dB)valuecloseto20orlargercanberegardedas
to[60].However,theone-pixelattackwithwav2vec2.0asthreat
humanimperceptible.
modelachieveshigherSNR(i.e.,around37dB),whichmightbe
causedbytherandomnessofjustonevalueonthetimeframeaxis.
5 RESULTS AND ANALYSIS In terms of speed, the proposed generator-based method
demonstratesrapidgenerationofadversarialexamples(i.e.,0.01
5.1 SERModelsResults
seconds), which is more than 100 times faster than the PGD
TheEmo18andZhao19modelsweretrainedfromscratch,while
attacker when the threat model is Emo18 and about 73 times
the Wav2vec 2.0 and WavLM models were fine-tuned on the
faster on the validation dataset and 57 times faster on the test
DEMoSdataset.Theperformanceofthesemodelsissummarised
datasetwhenthethreatmodeliswav2vec2.0.Notably,whenthe
inTable2.IncomparisontopreviousworksonDEMoS,ourapplied
attackerisSparseFoolandone-pixel,thespeedisquiteslow,from
modelsdemonstratecomparableorevensuperiorperformances.
8.33secondsto418.36secondstogenerateonesingleadversarial
examples.OurproposedSTAA-Netisquiteefficientinthisregard.
TABLE2 This is essential because in practical attack scenarios, attackers
SERmodels’performances(Accuracy/UAR[%])onDEMoS.
wouldprefertoquicklygeneratetheadversarialperturbationusing
mobile devices and inject it into the victim’s ongoing speech
Models Validation Test
since attackers usually do not have the opportunities to record
Changetal. [9] 74.9/70.0 85.9/78.9
andmodifythewholespeechinrealtime.Thisrequiresahighly
Renetal. [61] -/87.5 -/86.7
Renetal. [8] -/91.8 -/91.4 efficientmethodwithlowcomputationalcomplexitytocraftrobust
Emo18 78.18/77.39 79.31/78.80 adversarialperturbationswithinalimitedtimebudget.
Zhao19 76.40/75.71 72.39/72.10
Sparsityandattackperformancessometimesaretrade-offs,but
Wav2vec2.0 92.99/92.82 91.01/91.01
theproposedSTAA-Netcanachieveabalancebetweenthem.In
WavLM 92.48/92.30 92.26/92.41
Table4,whenboththethreatmodelandvictimmodelareEmo18,
wecanseethatvalASR82.71%andtestASR78.50%bySTAA-
FortheIEMOCAPdataset,weassesstheperformanceofour
NetisconsiderablybetterthanthoseofPGD,SparseFool,andone-
applied models in comparison to the state-of-the-art (SOTA) 4-
pixelwithsparserpurturbations(i.e.,valsparsity12.50%andtest
classemotionrecognitionmethods,asshowninTable3.Itisworth
sparsity6.25%).Bylimitingthenumberofperturbationlocations
noting that the majority of previous works [11] on IEMOCAP
to be one, the one-pixel attacker fails to attack any of the audio
employed a dataset split where one session was used as the test
samples in DEMoS. When attacking other models with Emo18
dataset and the remaining four sessions were used for training.
as the threat model, STAA-Net achieves better transferability.
Somestudies(e.g., [44],[62])alsoutilised5-foldcross-validation.
Specifically, the validation and test ASR on Zhao19, wav2vec
Whileourfine-tunedresultsarecomparabletopreviousworks,it
2.0, and WavLM are the best, compared with other attackers
isimportanttoconsiderthatdifferencesinperformancemayarise
respectively. Notably, even though the threat model is relatively
due to variations in the dataset split. It is important to highlight
simple(i.e.,Emo18),whenusinggeneratedadversarialexamples
thatthisworkprimarilyfocusesonthemodels’performanceinthe
toattackmoreadvancedmodels(i.e.,wav2vec2.0andWavLM),
adversarialattacksettings,ratherthansolelyonIEMOCAP.
the ASR drops but is still quite high. This further proves the
effectivenessoftheproposedSTAA-Net.Whenthethreatmodel
5.2 AttackResults
iswav2vec2.0,ifthevictimmodelisalsowav2vec2.0,wecan
AsshowninTable4,withEmo18andwav2vec2.0asthethreat findthatASRsachievedbySTAA-Net(i.e.,valASR80.02%and
modeltotrainthegenerator,theproducedadversarialexamplesare testASR76.09%)arelowerthantheonesbyPGD(i.e.,valASR
usedtotestonEmo18andwav2vec2.0aswhite-boxattackand 98.46%andtestASR99.05%).However,thetransferabilityoftheIEEETRANSACTIONS,VOL.XX,NO.XX,XX2023 7
TABLE4
AttackperformancescomparisonontheDEMoSdataset.Therates[%]inthecolumnsofEmo18,Zhao19,Wav2vec2.0,WavLMareAttackSuccess
Rate(ASR).
SNR(dB) Sparsity(%) Speed(s) Emo18(%) Zhao19(%) Wav2vec2.0(%) WavLM(%)
ThreatModel Attacker Val Test Val Test Val Test Val Test Val Test Val Test Val Test
PGD 16.87 17.38 14.58 9.94 1.10 1.16 77.85 67.78 67.09 52.65 15.50 27.66 15.87 12.60
SparseFool 16.64 17.48 95.43 98.00 18.90 8.33 12.87 13.38 9.67 12.65 11.00 12.56 8.49 9.46
Emo18 One-Pixel – – – – 29.88 24.88 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.04
STAA-Net 16.86 17.52 12.50 6.25 0.01 0.01 82.71 78.50 84.95 53.63 33.76 41.29 54.31 28.39
PGD 16.87 17.53 22.29 23.13 0.73 0.57 79.93 74.19 81.11 65.12 98.46 99.05 48.17 33.55
SparseFool 16.69 17.31 99.61 99.73 90.64 79.87 53.40 39.31 53.88 43.23 61.65 48.47 51.01 37.03
Wav2vec2.0 One-Pixel 36.88 37.54 1(p) 1(p) 82.93 71.40 0.00 0.00 0.00 0.00 0.06 0.13 0.00 0.04
STAA-Net 16.87 17.21 11.88 12.93 0.01 0.01 82.84 78.07 84.44 82.41 80.02 76.09 64.67 52.09
5.3 AblationStudy
Original Audio Original Audio
Wefurtherassessthecontributionsofthemaincomponentswithin
our proposed framework. To this end, we selected Emo18 and
wav2vec2.0asthelocalthreatmodels,andtheDEMoSdatasetas
ourexperimentaldataset.TheresultsareindicatedinTable6.The
generationofadversarialexamplesmaintainsaconsistentspeed,
andthereisonlylittleobservedalterationintheSNR.
Effects of Factorisation. In order to assess the effectiveness of
Time (s) Time (s)
thefactorisationapproach,weremovetheup-samplingblockUB
2
Adversarial Audio Adversarial Audio
inFigure1fromourarchitecture,whichcontrolstheperturbation
locations, and solely use the output of the other up-sampling
blockUB togeneratethefinalperturbationv.Specifically,we
1
retained the magnitude loss L , but calculated the sparsity mag
loss as L = ∥v∥ , while the quantisation loss was omitted.
spa 1
Theresults,presentedinTable6,demonstratethatthegenerated
adversarialexamplesachieve100%sparsity.Thisoutcomesuggests
thatdirectlyapplyingl regularisationalonedoesnotyieldsparse
1
Time (s) Time (s)
solutionsinourstudy.Furthermore,thisablationstudyemphasises
(a) Waveform (b) Spectrogram thesignificanceofUB ineffectivelycontrollingthelocationsof
2
theperturbationswithinthegeneratedadversarialexamples.
Fig. 3. Comparison of the waveforms and log Mel spectrograms of
EffectsofMagnitudeLoss.Thepurposeofusingmagnitudeloss
one original audio sample from the DEMoS dataset and its STAA-
Net generated adversarial example. The file name of the sample is istwo-fold:oneistocontroltheattackstrengthandthesecondis
‘NP f 43 pau05b.wav’,spokenbyafemalewiththeemotionclass‘fear’. toalleviateextremelylargeperturbations.Withoutthemagnitude
loss,thegeneratorisnotstableandcanproduceperturbationwith
allvaluesas0withEmo18aslocalthreatmodelonthetestdataset
ofDEMoSasshowninTable6.Furthermore,itleadstopoorASR
STAA-Net is better and the sparsity of the perturbation is quite performances.
lower(i.e.,validationsparsity11.88%vs 22.29%andtestsparsity EffectsofSparsityLoss.Withoutthesparsityloss,thegenerated
12.93% vs 23.13%). Interestingly, when the threat model (i.e., adversarial perturbation is fully dense (i.e., sparsity = 100%).
wav2vec2.0)isrelativelymoreadvancedthanthevictimmodel, Specifically,whenthethreatmodelisEmo18andtheadversarial
theASRincreasesalittlebit.Thismightbecausedbythestrong examplesaretransferredtoattackwav2vec2.0andWavLM,the
capabilityofwav2vec2.0forlatentrepresentationsextraction. testASRdropsconsiderably.
EffectsofQuantisationLoss.Withoutthequantisationloss,we
TofurthervalidatetheeffectivenessofSTAA-Net,weextend
canseethatthesparsityincreases,especiallywhenthethreatmodel
ourexperimentsonthesecondemotiondatasetIEMOCAPandthe
iswav2vec2.0.
correspondingresultscanbefoundinTable 5.Intermsofspeed,
theproposedSTAA-Netisthefastestwhengeneratingadversarial
examples. For SNR, the STAA-Net generates imperceptible ad- 5.4 Discussion
versarialexamples,especiallywhenthethreatmodeliswav2vec TheproposedSTAA-Net,whileofferingadvantagesingenerating
2.0.Asforthesparsityoftheadversarialexamples,theSTAA-Net- sparseandtransferableperturbationsefficiently,hasseverallimita-
generatedadversarialexamplesarethemostsparse,whereasthe tionsthatshouldbetakenintoaccount.Firstly,theselectionand
SparseFoolgeneratesquitedenseperturbations.Whentheattackis fine-tuningofweightsfordifferentlosses,suchasλ ,λ ,andλ ,
m s q
white-boxandthethreatmodelisEmo18,eventhoughthePGD requirecarefulconsideration.Findingtheoptimalbalancebetween
attackerachievesbettertestASR(i.e.,80.28%),theperturbation these weights is crucial for achieving desired results. Secondly,
is denser and transferability is worse. When the threat model is althoughmanypriorworks[21],[24],[26]onadversarialattacksin
wav2vec2.0,wecanseethatadversarialexamplesgeneratedby theaudiodomainprimarilyuseSNRtoevaluatetheimperceptibility
STAA-Nethavethebesttransferabilityandsparsityofperturbations. of the adversarial perturbation, there may be scenarios where
Sparsefoolalsofailstogeneratesparseperturbations. stealthiness is more critical than ASR. In such cases, additional
edutilpmA
edutilpmA
leM
goL
leM
goLIEEETRANSACTIONS,VOL.XX,NO.XX,XX2023 8
TABLE5
AttackperformancescomparisonontheIEMOCAPdataset.Therates[%]inthecolumnsofEmo18,Zhao19,Wav2vec2.0,WavLMareAttack
SuccessRate(ASR).
SNR(dB) Sparsity(%) Speed(s) Emo18(%) Zhao19(%) Wav2vec2.0(%) WavLM(%)
ThreatModel Attacker Val Test Val Test Val Test Val Test Val Test Val Test Val Test
PGD 23.83 17.04 35.96 29.19 2.20 2.26 47.14 80.28 44.42 61.25 37.25 41.09 41.64 40.40
SparseFool 15.69 10.83 99.96 97.29 162.58 418.36 9.70 25.02 9.70 19.64 3.01 10.95 5.53 15.38
Emo18 One-Pixel – – – – 40.50 69.54 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
STAA-Net 22.99 15.99 24.34 24.97 0.01 0.01 58.68 71.16 53.15 79.24 29.97 46.39 36.76 51.17
PGD 26.34 19.85 8.27 15.89 0.78 0.74 17.75 54.39 19.50 47.52 95.64 91.83 23.86 35.71
SparseFool 27.51 19.87 98.41 96.32 247.38 171.09 26.77 26.59 24.64 23.46 36.47 30.67 36.86 24.41
Wav2vec2.0 One-Pixel 40.83 – 1(p) – 61.64 56.00 0.00 0.00 0.00 0.00 0.19 0.00 0.19 0.00
STAA-Net 24.74 19.75 3.12 4.61 0.01 0.01 33.95 61.86 27.64 54.56 53.35 65.33 42.58 45.61
TABLE6
AblationstudyofSTAA-NetontheDEMoSdataset.Therates[%]inthecolumnsofEmo18,Zhao19,Wav2vec2.0,WavLMareAttackSuccessRate
(ASR).
SNR(dB) Sparsity(%) Speed(s) Emo18(%) Zhao19(%) Wav2vec2.0(%) WavLM(%)
ThreatModel Attacker Val Test Val Test Val Test Val Test Val Test Val Test Val Test
w/ofactorisation 16.92 17.60 100.00 100.00 0.01 0.01 82.99 82.41 80.33 83.83 11.70 53.29 17.35 25.16
w/omagnitudeloss 16.86 -inf 12.50 0.00 0.01 0.01 83.26 12.86 86.34 12.77 28.50 2.37 52.01 1.59
Emo18 w/osparsityloss 16.96 17.61 100.00 100.00 0.01 0.01 82.90 78.97 80.02 84.04 35.27 15.27 45.81 7.83
w/oquantisationloss 16.87 17.51 12.48 6.25 0.01 0.01 83.14 78.67 85.31 64.34 29.13 41.55 51.65 28.47
STAA-Net 16.86 17.52 12.50 6.25 0.01 0.01 82.71 78.50 84.95 53.63 33.76 41.29 54.31 28.39
w/ofactorisation 16.88 17.43 100.00 100.00 0.01 0.01 82.77 78.32 84.38 81.03 88.03 83.91 83.05 60.86
w/omagnitudeloss 16.87 17.20 22.10 18.75 0.01 0.01 82.74 78.58 85.10 74.97 83.20 72.30 70.32 52.77
Wav2vec2.0 w/osparsityloss 16.91 17.40 100.00 100.00 0.01 0.01 83.35 79.91 84.47 80.00 86.82 81.72 86.37 72.90
w/oquantisationloss 16.87 17.19 20.40 20.31 0.01 0.01 83.02 78.19 85.62 79.91 81.57 73.81 66.70 54.02
STAA-Net 16.87 17.21 11.88 12.93 0.01 0.01 82.84 78.07 84.44 82.41 80.02 76.09 64.67 52.09
evaluationmetrics,suchashumanevaluation,canprovideamore weightdeterminationfordifferentlosscomponentsisaworthwhile
comprehensive assessment. Lastly, the availability and quantity avenueforfutureinvestigation.
of training data for the generator can impact its performance.
Insufficientorimbalanceddatamaylimitthegenerator’sabilityto REFERENCES
learneffectivelyandgeneratehigh-qualityadversarialexamples.
[1] M. B. Akc¸ay and K. Og˘uz, “Speech emotion recognition: Emotional
models,databases,features,preprocessingmethods,supportingmodalities,
andclassifiers,”SpeechCommunication,vol.116,pp.56–76,2020.
6 CONCLUSION [2] S. Latif, J. Qadir, A. Qayyum, M. Usama, and S. Younis, “Speech
technology for healthcare: Opportunities, challenges, and state of the
art,” IEEE Reviews in Biomedical Engineering, vol. 14, pp. 342–356,
In conclusion, the field of speech emotion recognition (SER)
2021.
lackssufficientresearchonadversarialattacks,withmostexisting [3] M.Song,Z.Yang,A.Baird,E.Parada-Cabaleiro,Z.Zhang,Z.Zhao,
attacks in the audio domain primarily focusing on l or l andB.Schuller,“Audiovisualanalysisforrecognisingfrustrationduring
2 ∞
norm constraints. To address this gap, we proposed STAA-Net, Game-Play:Introducingthemultimodalgamefrustrationdatabase,”in
Proc.ACII,Cambridge,UnitedKingdom,2019,pp.517–523.
a generator-based attacker that efficiently generates transferable
[4] M.Płaza,R.Kazała,Z.Koruba,M.Kozłowski,M.Lucin´ska,K.Sitek,and
and sparse adversarial perturbations in an end-to-end manner. J.Spyrka,“Emotionrecognitionmethodforcall/contactcentresystems,”
We trained the generator using Emo18 and WavLM as threat AppliedSciences,vol.12,no.21,pp.1–24,2022.
[5] J.Zhao,X.Mao,andL.Chen,“Speechemotionrecognitionusingdeep
models and produced adversarial examples in a single forward
1D & 2D CNN LSTM networks,” Biomedical Signal Processing and
pass.Thegeneratedadversarialexampleswerethenusedtoattack Control,vol.47,pp.312–323,2019.
the considered Zhao19 and wav2vec 2.0 models. Experimental [6] H.Meng,T.Yan,F.Yuan,andH.Wei,“Speechemotionrecognitionfrom
resultsontheDEMoSandIEMOCAPdatasetsdemonstratedthe 3D log-Mel spectrograms with deep learning network,” IEEE Access,
vol.7,pp.125868–125881,2019.
effectivenessofSTAA-Netinachievingabalancebetweensparsity,
[7] P.Tzirakis,J.Zhang,andB.W.Schuller,“End-to-endspeechemotion
speed,imperceptibility,transferability,andattacksuccessrate. recognitionusingdeepneuralnetworks,”inProc.ICASSP,Seoul,Korea,
In terms of future research directions, there are several di- 2018,pp.5089–5093.
[8] Z.Ren,T.T.Nguyen,Y.Chang,andB.W.Schuller,“Fastyeteffective
rections that can be explored. Firstly, while this work primarily
speech emotion recognition with self-distillation,” in Proc. ICASSP,
focusedonnon-targetedadversarialattacks,itwouldbevaluable Rhodes,Greece,2023,pp.1–5.
to investigate targeted attacks and evaluate the efficiency of the [9] Y.Chang,Z.Ren,T.T.Nguyen,K.Qian,andB.W.Schuller,“Knowledge
generatorinproducingtransferableandsparseaudioadversarial transferforon-devicespeechemotionrecognitionwithneuralstructured
learning,”inProc.ICASSP,Rhodesisland,Greece,2023,pp.1–5.
examples that are specifically designed to deceive a particular
[10] Z.Liu,X.Kang,andF.Ren,“Dual-TBNet:Improvingtherobustness
victim model or class. Secondly, there is room for exploring of speech features via dual-transformer-bilstm for speech emotion
the applicability and performance in other audio tasks, such as recognition,”IEEE/ACMTransactionsonAudio,Speech,andLanguage
Processing,vol.31,pp.2193–2203,2023.
automatic speech recognition. Investigating the effectiveness of
[11] J.Wagner,A.Triantafyllopoulos,H.Wierstorf,M.Schmitt,F.Burkhardt,
the method across different audio domains would help assess F.Eyben,andB.W.Schuller,“Dawnofthetransformererainspeech
its versatility and generalisation capabilities. Thirdly, exploring emotionrecognition:Closingthevalencegap,”IEEETransactionson
potentialdefensemechanismsagainstsuchadversarialattacksisan PatternAnalysisandMachineIntelligence,pp.1–13,2023.
[12] L.-W.ChenandA.Rudnicky,“Exploringwav2vec2.0finetuningfor
interestingareatoinvestigate,whichcancontributetotheoverall
improvedspeechemotionrecognition,”inProc.ICASSP,Rhodesisland,
robustness of SER models. Lastly, the exploration of automatic Greece,2023,pp.1–5.IEEETRANSACTIONS,VOL.XX,NO.XX,XX2023 9
[13] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A [40] L.Pepino,P.Riera,andL.Ferrer,“EmotionRecognitionfromSpeech
frameworkforself-supervisedlearningofspeechrepresentations,”inProc. Usingwav2vec2.0Embeddings,”inProc.Interspeech,Brno,Czechia,
NeurIPS,Vancouver,Canada,2020,pp.1–12. 2021,pp.3400–3404.
[14] W.-N.Hsu,B.Bolte,Y.-H.H.Tsai,K.Lakhotia,R.Salakhutdinov,and [41] X.Cai,J.Yuan,R.Zheng,L.Huang,andK.Church,“Speechemotion
A.Mohamed,“HuBERT:Self-supervisedspeechrepresentationlearning recognitionwithmulti-tasklearning,”inProc.Interspeech,Brno,Czechia,
bymaskedpredictionofhiddenunits,”IEEE/ACMTransactionsonAudio, 2021,pp.4508–4512.
Speech,andLanguageProcessing,vol.29,pp.3451–3460,2021. [42] E. Morais et al., “Speech emotion recognition using self-supervised
[15] V.Panayotov,G.Chen,D.Povey,andS.Khudanpur,“Librispeech:An features,”inProc.ICASSP,Singapore,2022,pp.6922–6926.
ASR corpus based on public domain audio books,” in Proc. ICASSP, [43] S.Chen,C.Wang,Z.Chenetal.,“WavLM:Large-scaleself-supervised
Brisbane,Australia,2015,pp.5206–5210. pre-trainingforfullstackspeechprocessing,”IEEEJournalofSelected
[16] I.J.Goodfellow,J.Shlens,andC.Szegedy,“Explainingandharnessing TopicsinSignalProcessing,vol.16,no.6,pp.1505–1518,2022.
adversarialexamples,”inProc.ICLR,SanDiego,CA,2015,11pages. [44] T.Feng,R.Hebbar,andS.Narayanan,“TrustSER:Onthetrustworthi-
[17] A.Madry,A.Makelov,L.Schmidt,D.Tsipras,andA.Vladu,“Towards nessoffine-tuningpre-trainedspeechembeddingsforspeechemotion
deep learning models resistant to adversarial attacks,” in Proc. ICLR, recognition,”arXivpreprintarXiv:2305.11229,pp.1–6,2023.
Vancouver,Canada,2018. [45] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
[18] A.Modas,S.-M.Moosavi-Dezfooli,andP.Frossard,“SparseFool:Afew S.Ozair,A.Courville,andY.Bengio,“Generativeadversarialnetworks,”
pixelsmakeabigdifference,”inProc.CVPR,LongBeach,CA,2019,pp. CommunicationsoftheACM,vol.63,no.11,pp.139–144,2020.
9089–9096. [46] P.Vincent,H.Larochelle,Y.Bengio,andP.-A.Manzagol,“Extractingand
[19] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling composingrobustfeatureswithdenoisingautoencoders,”inProc.ICML,
deepneuralnetworks,”IEEETransactionsonEvolutionaryComputation, Helsinki,Finland,2008,pp.1096–1103.
vol.23,no.5,pp.828–841,2019. [47] Y. Fan, B. Wu, T. Li, Y. Zhang, M. Li, Z. Li, and Y. Yang, “Sparse
[20] N.CarliniandD.Wagner,“Audioadversarialexamples:Targetedattacks adversarialattackviaperturbationfactorization,”inProc.ECCV,Virtual
onspeech-to-text,”inProc.SPW,SanFrancisco,CA,2018,pp.1–7. Event,2020,pp.35–50.
[21] P. Neekhara, S. Hussain, P. Pandey, S. Dubnov, J. J. McAuley, and [48] A. Jansson, E. Humphrey, N. Montecchio, R. Bittner, A. Kumar, and
F.Koushanfar,“Universaladversarialperturbationsforspeechrecognition T. Weyde, “Singing voice separation with deep U-Net convolutional
systems,”inProc.Interspeech,Graz,Austria,2019,pp.481–485. networks,”inProc.ISMIR,Suzhou,China,2017,pp.745–751.
[22] H. Guo, Y. Wang, N. Ivanov, L. Xiao, and Q. Yan, “SPECPATCH: [49] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,
Human-in-the-loopadversarialaudiospectrogrampatchattackonspeech A.Graves,N.Kalchbrenner,A.Senior,andK.Kavukcuoglu,“WaveNet:
recognition,”inProc.CCS,LosAngeles,CA,2022,pp.1353–1366. Agenerativemodelforrawaudio,”inProc.ISCAWorkshoponSSW9,
Sunnyvale,CA,2016,p.125.
[23] H.Kim,J.Park,andJ.Lee,“Generatingtransferableadversarialexamples
[50] S.D.DanielStoller,SebastianEwert,“Wave-U-Net:Amulti-scaleneural
forspeechclassification,”PatternRecognition,vol.137,p.109286,2023.
networkforend-to-endaudiosourceseparation,”inProc.ISMIR,Paris,
[24] W.Zhang,S.Zhao,L.Liu,J.Li,X.Cheng,T.F.Zheng,andX.Hu,
France,2018,pp.334–340.
“Attackonpracticalspeakerverificationsystemusinguniversaladversarial
[51] K.GuptaandT.Ajanthan,“Improvedgradient-basedadversarialattacks
perturbations,”inProc.ICASSP,Toronto,Canada,2021,pp.2575–2579.
forquantizednetworks,”Proc.AAAI,vol.36,no.6,pp.6810–6818,2022.
[25] G.Chen,S.Chenb,L.Fan,X.Du,Z.Zhao,F.Song,andY.Liu,“Whois
[52] N.CarliniandD.Wagner,“Towardsevaluatingtherobustnessofneural
realBob?Adversarialattacksonspeakerrecognitionsystems,”inProc.
networks,”inProc.IEEESymposiumonSP,SanJose,CA,2017,pp.
2021IEEESP,SanFrancisco,CA,2021,pp.694–711.
39–57.
[26] G.Chen,Z.Zhao,F.Song,S.Chen,L.Fan,andY.Liu,“AS2T:Arbitrary
[53] Z. Li, Y. Wu, J. Liu, Y. Chen, and B. Yuan, “AdvPulse: Universal,
source-to-targetadversarialattackonspeakerrecognitionsystems,”IEEE
synchronization-free,andtargetedaudioadversarialattacksviasubsecond
TransactionsonDependableandSecureComputing,pp.1–17,2022.
perturbations,”inProc.CCS,VirtualEvent,2020,p.1121–1134.
[27] M.Chen,L.Lu,Z.Ba,andK.Ren,“PhoneyTalker:Anout-of-the-box
[54] E. Parada-Cabaleiro, G. Costantini, A. Batliner, M. Schmitt, and
toolkitforadversarialexampleattackonspeakerrecognition,”inProc.
B.Schuller,“DEMoS:Anitalianemotionalspeechcorpus,”Language
INFOCOM,Orlando,FL,2022,pp.1419–1428.
ResourcesandEvaluation,vol.54,pp.341–383,2019.
[28] Y.Xie,Z.Li,C.Shi,J.Liu,Y.Chen,andB.Yuan,“Enablingfastand
[55] C.Busso,M.Bulut,C.-C.Lee,A.Kazemzadeh,E.Mower,S.Kim,J.N.
universalaudioadversarialattackusinggenerativemodel,”inProc.AAAI,
Chang,S.Lee,andS.S.Narayanan,“IEMOCAP:Interactiveemotional
VirtualEvent,2021,pp.14129–14137.
dyadicmotioncapturedatabase,”Languageresourcesandevaluation,
[29] R.Oak,“Poster:Adversarialexamplesforhatespeechclassifiers,”inProc.
vol.42,no.4,pp.335–359,2008.
CCS,London,UnitedKingdom,2019,p.2621–2623.
[56] Y.Li,P.Bell,andC.Lai,“FusingASRoutputsinjointtrainingforspeech
[30] Z.Ren,J.Han,N.Cummins,andB.Schuller,“Enhancingtransferability
emotionrecognition,”inProc.ICASSP,Singapore,2022,pp.7362–7366.
ofblack-boxadversarialattacksvialifelonglearningforspeechemotion
[57] C.Fu,C.Liu,C.Ishi,andH.Ishiguro,“Anadversarialtrainingbased
recognitionmodels,”inProc.Interspeech,Shanghai,China,2020,pp.
speechemotionclassifierwithisolatedgaussianregularization,”IEEE
496–500.
TransactionsonAffectiveComputing,pp.1–14,2022.
[31] Y.Chang,S.Laridi,Z.Ren,G.Palmer,B.W.Schuller,andM.Fisichella,
[58] S. Marrone and C. Sansone, “On the transferability of adversarial
“Robustfederatedlearningagainstadversarialattacksforspeechemotion
perturbation attacks against fingerprint based authentication systems,”
recognition,”pp.1–11,2022.
PatternRecognitionLetters,vol.152,pp.253–259,2021.
[32] Y. Qin, N. Carlini, G. Cottrell, I. Goodfellow, and C. Raffel, “Imper- [59] B. Schuller and A. Batliner, Computational Paralinguistics: Emotion,
ceptible,robust,andtargetedadversarialexamplesforautomaticspeech Affect and Personality in Speech and Language Processing. Wiley
recognition,”inProc.ICML,LongBeach,CA,2019,pp.5231–5240.
Publishing,2013,344pages.
[33] L.Zhang,Y.Meng,J.Yu,C.Xiang,B.Falk,andH.Zhu,“Voiceprint [60] T.Du,S.Ji,J.Li,Q.Gu,T.Wang,andR.Beyah,“SirenAttack:Generating
mimicryattacktowardsspeakerverificationsysteminsmarthome,”in adversarialaudioforend-to-endacousticsystems,”inProc.ASIACCS,
Proc.INFOCOM,Toronto,Canada,2020,pp.377–386. Taipei,Taiwan,2020,p.357–369.
[34] A.Jati,C.-C.Hsu,M.Pal,R.Peri,W.AbdAlmageed,andS.Narayanan, [61] Z.Ren,A.Baird,J.Han,Z.Zhang,andB.Schuller,“Generatingand
“Adversarialattackanddefensestrategiesfordeepspeakerrecognition protecting against adversarial attacks for deep speech-based emotion
systems,”ComputerSpeech&Language,vol.68,p.101199,2021. recognitionmodels,”inProc.ICASSP,Barcelona,Spain,2020,pp.7184–
[35] Z.He,W.Wang,J.Dong,andT.Tan,“Transferablesparseadversarial 7188.
attack,”inProc.CVPR,NewOrleans,LA,2022,pp.14963–14972. [62] L.Pepino,P.Riera,andL.Ferrer,“Speechemotionrecognitionbased
[36] P.Tzirakis,J.Chen,S.Zafeiriou,andB.Schuller,“End-to-endmultimodal onattentionweightcorrectionusingword-levelconfidencemeasure,”in
affect recognition in real-world environments,” Information Fusion, Proc.Interspeech,Brno,Czechia,2021,pp.1947–1951.
vol.68,pp.46–53,2021.
[37] Z. Zhang, B. Wu, and B. Schuller, “Attention-augmented end-to-end
multi-tasklearningforemotionpredictionfromspeech,”inProc.ICASSP,
Brighton,UnitedKingdom,2019,pp.6705–6709.
[38] T.-W.Sun,“End-to-endspeechemotionrecognitionwithgenderinforma-
tion,”IEEEAccess,vol.8,pp.152423–152438,2020.
[39] P.Tzirakis,A.Nguyen,S.Zafeiriou,andB.W.Schuller,“Speechemotion
recognition using semantic information,” in Proc. ICASSP, Toronto,
Canada,2021,pp.6279–6283.