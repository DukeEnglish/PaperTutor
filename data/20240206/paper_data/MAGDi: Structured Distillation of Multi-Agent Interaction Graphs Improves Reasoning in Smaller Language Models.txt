MAGDI: Structured Distillation of Multi-Agent Interaction Graphs
Improves Reasoning in Smaller Language Models
JustinChih-YaoChen*1 SwarnadeepSaha*1 EliasStengel-Eskin1 MohitBansal1
Abstract
A football team
played 22 games.
Multi-agentinteractionsbetweenLargeLanguage They won 8 more
than they lost.
Model(LLM)agentshaveshownmajorimprove- How many did
mentsondiversereasoningtasks. However,these they win?
(Gold Answer: 15)
involve long generations from multiple models Multi-Round Multi-Agent
Teacher Discussion Interaction Graph (MAG)
across several rounds, making them expensive.
Moreover, these multi-agent approaches fail to
Let L be the number MAGDI … L+(L+8)=22.
provide a final, single model for efficient infer-
of losses… so the Hence, they won L+8
ence. Toaddressthis,weintroduce MAGDI,a team won 14 games. = 7+8 = 15 games.
newmethodforstructureddistillationoftherea- Base Model + 10.71% MAGDI-Augmented Model
soninginteractionsbetweenmultipleLLMsinto
Figure1.Overviewofourdistillationmethod.Givenareasoning
smallerLMs. MAGDIteachessmallermodelsby
problem,multipleteacher-LLMsengageinamulti-rounddiscus-
representing multi-agent interactions as graphs,
sion,leadingtothegenerationofamulti-agentinteractiongraph
augmenting a base student model with a graph (MAG).Thenourstructureddistillationmethod,MAGDIdistills
encoder,anddistillingknowledgeusingthreeob- reasoningknowledgefromthesegraphsintoabasestudentmodel.
jective functions: next-token prediction, a con-
trastivelossbetweencorrectandincorrectreason-
ing,andagraph-basedobjectivetomodelthein-
discussing them with others. Similarly, we can improve
teractionstructure. Experimentsonsevenwidely-
LargeLanguageModels(LLMs)–whichoftenexhibitim-
used commonsense and math reasoning bench-
pressivemulti-stepreasoningcapabilities(Weietal.,2022;
marks show that MAGDI improves the reason-
Kojimaetal.,2022)–byallowingmultipleLLMinstances
ingcapabilitiesofsmallermodels,outperforming
to interact in a discussion (Du et al., 2023; Chen et al.,
severalmethodsthatdistillfromasingleteacher
2023b; Wu et al., 2023). These interactive frameworks
andmultipleteachers. Moreover, MAGDI also
enableeachagenttoiterativelyrefineitsreasoningbyob-
demonstratesanorderofmagnitudehighereffi-
taining feedback from others, thereby leading to a better
ciency over its teachers. We conduct extensive
consensusattheendofmultipleinteractionrounds.
analysestoshowthatMAGDI(1)enhancesthe
generalizabilitytoout-of-domaintasks,(2)scales Discussionframeworksaretypicallybuiltontopofpropri-
positivelywiththesizeandstrengthofthebase etary models, e.g., GPT-4, Bard, Claude, etc., which can
student model, and (3) obtains larger improve- actasgeneralconversationalagents,handlelongcontexts,
ments(via ourmulti-teacher training)whenap- and follow instructions (Bubeck et al., 2023). However,
plyingself-consistency–aninferencetechnique thesemodelsarealsocomputationallyandmonetarilyex-
thatreliesonmodeldiversity.1 pensive,especiallywhenusedinmulti-roundinteractions,
whichrequirenumerouslong-tokenlengthinferencecalls
totheunderlyingLLMs.2 Moreover,theseframeworksdo
not result in a final, joint model that can then be directly
1.Introduction used for inference and instead require invoking all inter-
acting LLMs at test time. To reduce this cost and train a
Debateanddialoguearenaturalwaystoimprovereasoning:
small, affordable yet capable model, we tackle the prob-
weformourbestideasnotinisolation,butbyrefiningand
2Forexample,onesuchmulti-LLMinteractionmethod,ReC-
*Equalcontribution 1UNCChapelHill. Correspondenceto: oncile(Chenetal.,2023b)usesaround1900tokenspersampleon
JustinChih-YaoChen<cyaochen@cs.unc.edu>.
amathreasoningtask,withotherdiscussion-basedmethods(e.g.,
1Code/data:https://github.com/dinobby/MAGDi. Duetal.(2023);Wuetal.(2023))usingevenlargertokenbudgets.
1
4202
beF
2
]LC.sc[
1v02610.2042:viXraMADGI:StructuredDistillationofMulti-AgentInteractionGraphs
Multi-Agent M M I un ultlt tie -i-AAr ga gec ennt ti t o I Inn nt teG errar aca ctp tiiooh nn ( G GM rraA appG hh ) ((MMAAGG)) LLL eee vvev ele l 11l : :1 PP: oo P ssiio ttiis vvei et Pi Pv rre ee- -iP innr ttee err- i NNn oot dde eer ss Nodes LLeevveell 22:: AAlL lll e PPov osse iil tt ii2 vvee: A NNooll dd P eessositive Nodes
A football team ApA lf afooyooetbtdbaa ll2l lt 2teea agmma p mplaleayysee.d d T 22h22 e ggyaa mwmeoessn. . T T8hh eemyy woworoenn 8t8h mamonor rete h tthehayan n l tothhseetyy. lolosstt..
How many d MMid uH uH lot ltohw tiw i-e -Am Aymg a gwaen enyniy nn t d t?d i di I d In( n Gttth theeo eeryl ryad a wwc ciAntin ti?n io? o s (n(G nwG Go Goeldlrrd ra : a A Ap1 pnn5 hs hs)w w((Me MerrA: A: 1G15 G5))) ) LLeevveR eR loR lo 1u 1uon :n : u d Pd Pn o0 o0d ss ii0 ttiivvee PPrree--iinntteerr NNooddeess R LR Lo eo euu vvnn eedd ll 2200 :: AAR llll o PPu oon ssid itti iv0 vee NNooddeess
Round 0 RARAo o fufounonododt t0b b0aalll l ttee Ha Ha om om wRw p op ml mal ua ay any ne ne dyd yd 0d 2 d2 i2 di2 d g tg thaR ha eRm eom yo yue ue wns wnsd. id . ni T n0 T ?0 ?h h e (e (Gy Gy o oww ldlo do n An A n8 n8 s s m wm wo eo er rr re :e : 1 R1t 5th 5h o)a )a unn n tt dhRhRe eo0oyuyu nlnlodods s 0t0t.. RRR RRR oooR ooo uuu uuuo nnn nnnu ddd ddd n 021 021d 1 RR RRR Roo ooo ouu uuu unn nnn ndd ddd d 21 021 0 Round 1
L SLL 1e W o=+ 5t W ' 7 t+ 8s h g,= +L ed aW L L y=e m+ = =n w e82 2o 1 o s2 2t 5 .ne L S…L S L …L SL 1Le W oLL L S 1e W =o L 1e LW + o= 5tL W 1 =e + W o 5t +W '
5
=t 7 t W+ + 8' s 57 tt + W ' h 7 t8t g+s RR s8I ,hs' = g 7 t+ h+L , g8h = s e d ,+L h= ag o+L Wed , a= oo a e Ld W+L aL y WL =e e mde L aLy=e W mL+ y =e = mL uu g+ L =yn== +e m w t==yn
e
+ =wn 82= nn 2 weo h82 =n 1r e2 82 wo 2 o s1o e282 12eo s2 dd2 tow 5 o s2 12 .et 25 noe s .t2 5 .2 ne t e n 5e . ne 21o
a
wn
n
si1 t wh5
e
1…g… r……… 5 …a
i
. st RRR Imt RR sIt .h RRR Ih t RR sh .I ooo a ooo 1ah e ooo a oe e uuu goo ae uu ge y uuu g 5ty nnn uu gy rnns ht y r nnn r e ddd nn .w her dd.w e e ddd we edd w ee 210 o e 21o e 210 o a 21o wn awn wnnwn ni 1 s i t1 i t1 si twh1 5th5 hw…5h 5 e 1 g 1 …e g 1 g… r1 5g 5a r 5a a5 i .ca m. sRR i.m . m.. s ..o m I.. .s 1 oo e . e 1e
d
no 5e uuss 5s. .fs
i
. nn. . si. t r dd ah m ge 21 r…t…a … e… h ……… n … e… … e cc RR R cs RR c RR R o RR o I ws o oo ows oo o Ias oo o nos oo d n uu uo nuu o dn uu uo in fe uu nn nf i tnn fi t nn n sfi it nn ri dd dt s hr shr i ddt rah dd d mhrm dd a h mwe m g 20 1 e i 21 e g 20 1 1e s 21 r te at a r et 4a htha e1 hn rn h en en e e es5s e
…
s 1 s w w w aw a w.w a5a ne ine ne tine . sr ts hr sr s hwr w wi i wis 1si s e1es e 4e 1 r1 4r1 r1 r 5 5 1 …5 15 1…1.. 5.5…. 55 .… .. .. .
.
RR ah s oo ge o uun r…… … . …… e… nn .….c t… .. h dd .
e
R RRe RR h R a RRh h RR ash eso doo e soo ge o s oo 21e ooo goe w u o uuno uu n u uu rn uu n n a r nn e
a
c nn tn c nnt eo c nn tc d t h ddh ndd e ed he nddhe dd ee n e e d 0 e 21s e 21 sd0 w 21w 21 w w w a
w
a1 aaoao ao n4o n nnennn n en sn s ss s sr w rwg 1 ww1 w1 w1 44 i4 ea e4 =eese e rr gm rrg rg rg 1 i a 1i a =ia sisa =se sm 4m 5m m 11 1s 11 1.e .e 4e4e 54. 4 5ss .s.s .... .. .. LL LL L ee e RRe RRRRR RRRRRRR vv e vv ooooo RRoo oooooR eoo e ee uv uuuuuu uuuuuu ou ollo nl nnnl ne nn nn nnn nn 3 uu3 dd3 ddu 3 ddd ddddddd l :: n n :: n 10 2 10 2 11 02 3 10 AA21 dA dAd : ll ll ll 10ll2 A NN NN( l oob oo l dd dd N) ee ee ss o ss d (( (( PP PP e oo oo s ss ss ++ ++ ( NN NPN ee ee o gg gg s )) )) +Neg) R RR RR LLR RRR RR RL LRR o oo oo ee oo oo o ooe eoo u uu uu uu uu u uvv uuvu v n nn nn nnn nn nneenn e e dd dd dddd dd dddd l ll l 4 44 214 10 0212 0 11 021 : :: : A AA A RRLl lR l ll ll( l ooeo N NN Nc uuu vo oo o) nnn ed dd d ddd e l ee e s ss 4 s 102 :+ ++ + AE EE Ed dd l dlgg gg Nee eess ssodes + Edges
…… s soo t thhee a annsswweerr i sis 1 155.. …… II ddisisaaggrreeee wwitithh 1144 …… ...... aaggrreeeedd aannsswweerr == 1155.. RRoR ouuo nnu ddn 22d 2 RRoouunndd 22 Round 2
(a) (d) (e)
Figure2.Left(a):IllustrationofaMulti-AgentInteractionGraph(MAG)constructedwithGPT4,Bard,andClaude2collaboratively
solvingamathreasoningproblemoverthreediscussionrounds.Right(b-e):Thefourlevelsthatcharacterizeourstructureddistillation
method(MAGDI);eachlevelprogressivelydistillsknowledgefromthehighlightedcomponentsofaMAG.
lemofteachingreasoningtosmallerlanguagemodelsvia thatenablesthisrefinementofmodelreasoning(edgesin
structureddistillationoftheinteractionsbetweenmultiple Fig.2(a)). Wecaptureallthisknowledgeviathefollowing
strongerteachermodels. Specifically,wedevelopastruc- fourlevelsofMAGcomponents,whicharethenusedinour
tureddistillationmethod,Multi-AgentInteractionGraphs distillationmethod, MAGDI (§3.4),andfurthertestedas
Distillation, or MAGDI, that enables a student model to partofourexperiments(§5.1).
learnfrommulti-teacherinteractions,withthegoalofde-
Level1: Learningfrommultipleteachers. Thestudent
velopingaperformantandefficientstandalonealternative
learnsfromthecorrectreasoningofmultipleteachers,rather
toexpensivemulti-agentsetups. Onsevenpopularbench-
thanone(correctpre-interactionnodesinaMAG,Fig.2(b)).
marksinbothcommonsenseandmathreasoning,wefind
increasingimprovementsoverdistillationbaselinesaswe Level2: Learningfromteacherinteractions. Thestudent
incorporatemorelevelsofteacherinteractionsandstructure. learns from both pre- and post-interaction data between
multipleteachers(allcorrectnodesinaMAG,Fig.2(c)).
Multi-agent,multi-roundinteractionsarecharacterizedby
Level3: Learningfromnegativereasoning. Thestudent
theirparticipatingagents,thenumberofinteractionrounds,
additionally distills from negative or incorrect reasoning
and an interaction function defining what information an
fromtheteachermodels(allnodesinaMAG,Fig.2(d)).
agent has access to while generating its responses. This
functiongivesrisetoastructurebetweenagentsandrounds. Level4:Learningfromstructure.Thestudentlearnsfrom
Tolearnfromthisstructure,werepresentitinMulti-Agent theoutputandgraph-structureofteacherLLMinteractions
InteractionGraphs(MAG),agraph-basedencodingofmulti- (allnodesandedgesinaMAG,Fig.2(e)).
agentinteractions(§3.1). SeeFig.2(a)foranexample. Con-
Notethateachlevelbuildsonthepriorlevels,motivating
cretely,aMAGisadirectedacyclicgraph(DAG)wherein
ourmainresearchquestion:
eachnoderepresentsanagent’sgeneration(inthiscase,the
Chain-of-Thoughtreasoning(Weietal.,2022)foragiven Research Question: How can we effectively distill from
problem) in a discussion round, annotated with a binary diverseteacherinteractionsintoasmaller,efficientstudent
labelindicatingwhethertheansweriscorrect. Theedges modelacrossincreasinglevelsofinteractionstructure,also
denotethediscussion’sstructure,indicatingwhichprevious demonstratingscalabilityandgeneralizability?
turnsagentsarerespondingto. MAGsareanintuitiveand
TheselevelsalsoshapeMAGDI,ourstructureddistillation
generalizablewayofrepresentingthelevelsofmanymulti-
method. MAGDI enables a student model to learn from
agentinteractionswithvaryingconversationpatterns(Wu
ourgraph-structuredinteractiondata(MAGs),withthegoal
etal.,2023),andwillallowustodistillthisinformationinto
ofdevelopingaperformantandefficientstandalonealter-
a student model for performing zero-shot inference from
nativetoexpensivemulti-agentsetups. Wefirstconstruct
justthequestion(i.e.,MAGsarenotrequiredattest-time).
atrainingdatasetofMAGsfromahigh-performingmulti-
Given a reasoning problem, MAGs capture rich knowl- agentdiscussionframework(Chenetal.,2023b),featuring
edge of (1) diverse pre- and post-interaction correct rea- discussionsbetweenthreeAPI-basedLLMs: GPT-4,Bard,
soningchainsgeneratedbydifferentLLMs(greennodesin andClaude2(§3.2). Wethendevelopstudentmodelsaug-
Fig.2(a)),(2)diverseandchallengingincorrectreasoning mentedwithaGraphNeuralNetwork(GNN)forlearning
chains generated by different LLMs that are refined over structure-aware representations of positive (correct) and
interaction rounds (red nodes in Fig. 2(a)), and (3) an it- negative (incorrect) reasoning chains and fine-tune them
erative and structured (graph-based) interaction process onMAGdata. MAGDI’sthreefine-tuningobjectivesare
2MADGI:StructuredDistillationofMulti-AgentInteractionGraphs
alignedtothefourlevels: (1)next-tokenprediction(Levels 2017; Yang et al., 2020). Following recent work, we fo-
1-2),(2)acontrastivelossbetweencorrectandincorrectrea- cus on distillation from samples from a model distribu-
soning(Level3),and(3)agraph-basednodeclassification tion, or symbolic distillation (West et al., 2022), a form
loss(Level4). Theseobjectivescaptureallusefulsignals ofdistillationespeciallycommononLLMs,e.g. ininstruc-
in MAGs (i.e., teachers’ correct and incorrect reasoning tiontuning(Wangetal.,2023;Taorietal.,2023;Chiang
aswellastheirunderlyingconversationstructure). Attest etal.,2023),whereinstruction-question-answertriplesare
time,thedistilledmodelperformszero-shotinferencegiven sourced from a teacher model. In reasoning with LLMs
justthequestionandthebasemodel(withouttheGNN). specifically,recentworkhasdistilledreasoningknowledge
from a single larger teacher model to a smaller student
WeevaluateMAGDI’seffectivenessonsevenwidely-used
model(Magisteretal.,2023;Shridharetal.,2023;Fuetal.,
commonsense (StrategyQA, CommonsenseQA, ARC-c,
2023;Hoetal.,2023;Sahaetal.,2023;Mukherjeeetal.,
BoolQ) and math (GSM8K, MATH, SVAMP) reasoning
2023;Mitraetal.,2023;Lietal.,2023;Dengetal.,2023;
benchmarks, consistently establishing the following find-
Liuetal.,2023)usingChain-of-Thoughtprompting(CoT;
ingsacrossdatasetsanddomains:
Weietal.,2022)andalso,acombinationofmultipleprompt-
• Multi-teacher distillation improves student perfor- ing techniques (Chenglin et al., 2023; Mukherjee et al.,
mance. When compared directly to distilling from a 2023;Mitraetal.,2023). Pastworkhasalsodistilledmodu-
singleteacher,distillingfrommultipleteachersimproves lartrajectoriesfromaGPT-4teacherforsolvinginteractive
performance(Level1). tasks(Chenetal.,2023a;Yinetal.,2023).Overall,different
• Thevalueofteacherinteractions: Distillingfromthe fromthesesingle-teachersettings,wedelveintotherealm
post-interactionoutputsofteachersfurtherimprovesstu- ofknowledgedistillationfrommultipleteachers. Goingone
dents(Level2). stepfurther,welearnfromtheinteractionsbetweenteach-
• Negativereasoninghelps.Addingacontrastiveobjective ers, bringing in fresh challenges on flexibly representing
to learn from incorrect reasoning provides a valuable andmodelingtheseinteractions.
signaltothestudentmodel(Level3).
Graph-basedInteractions. Dialogues,debates,andmulti-
• Distillingfromstructuremaximizesaccuracy. When
partyconversations(Kirchhoff&Ostendorf,2003;Leifeld,
MAGDIdistillsfromthefirst3levelsandthestructureof
2018;Weietal.,2023)havealong, richhistoryofbeing
aMAG,thestudentachievesthehighestaccuracy,e.g.,up
modeledasgraphsfordifferentdownstreamtaskssuchas
to10%absoluteimprovementoverazero-shotbaseline
emotionandsentimentidentification(Ghosaletal.,2019;
andupto4%overthebestsingle-teacherbaseline.
Shenetal.,2021),dialogueactrecognition(Qinetal.,2021),
• MAGDI balances performance with efficiency.
dialoguesummarization(Chen&Yang,2021),andmachine
MAGDI-distilled models reduce the number of tokens
reading(Ouyangetal.,2021). Ourmotivationinthiswork
predictedattesttimebyupto9xwhileoutperformingall
differsintwomajorrespects. Firstly,wefocusonmodel-
single-teacherdistillationbaselines.
modelinteractionsratherthanthehuman-humanorhuman-
Buildingontheseresults,wefurtheranalyzeMAGDIalong modelinteractionsfrompastwork. Secondly,ourobjective
thefollowingaxes: istoenhancereasoningcapabilitiesinsmallerstudentmod-
elsthroughstructureddistillation,asopposedtoutilizing
• Generalizability. MAGDI can be used to produce a
graphmodelingfordownstreamgraphtasks.
unifiedjointmulti-tasklearningmodelthatperformswell
onmultipledomainsatonceandalsogeneralizeswellto
held-outdatasetsnotseenduringtraining. 3.Method
• Scalability. MAGDIscalespositivelywiththesizeand
In Sec. 3.1, we provide a general, formal description of
strengthofthebasestudentmodel.
MAG,ourgraph-basedrepresentationofmulti-agentinter-
• Diversity. Theoutputdiversityresultingfromourmulti-
actions. Sec.3.2thendescribestheconstructionofMAGs
teachertrainingimprovesself-consistency(Wangetal.,
for several tasks that will serve as training data for distil-
2023), an inference-time ensemble method relying on
lation. InSec.3.3, weanalyzethistrainingdatainterms
diversemodelanswers.
ofitsstructuralproperties. Lastly,inSec.3.4,wedescribe
MAGDI, our structured distillation method for learning
2.RelatedWork
fromMAGs.
Knowledge Distillation. Knowledge distillation has
3.1.Multi-AgentInteractionGraph(MAG)
proven effective in transferring knowledge from a larger
teacher model to a more compact student model (Hinton Definition. Consideracollaborativemulti-agentsetting,
etal.,2015;Buciluaˇ etal.,2006;Chenetal.,2020)includ- whereA={A }n agentsareinteractingwitheachother
i i=1
ing distillation from multiple teacher models (You et al., forrroundstosolveatask. Amulti-agentinteractiongraph
3MADGI:StructuredDistillationofMulti-AgentInteractionGraphs
Training Data (MAG) Construction Learning from Positive Reasoning Learning from
Interaction
A football team
played 22 games.
They won 8 more
than they lost. How
many did they win? Learning from Negative Reasoning
(Gold Answer: 15)
Multi-Round Multi-Agent
Train Question Teacher Discussion Interaction Graph (MAG)
MAGDI Components:
Base Model + GCN
MAGDI Objectives for Distillation
/
Mistral Llama2
MAGDI Augmented
BaseStudent Model GCN for MAG Structure Modeling Student Model
Figure3. TrainingDataConstruction: Givenareasoningproblem,multipleteachersgothroughamulti-rounddiscussionprocess,
generatingmulti-agentinteractiongraphs(MAGs). MAGDI:Ourstructureddistillationmethodaugmentsabasestudentmodelwitha
GraphNeuralNetwork(specifically,aGCN)tolearnstructure-awarerepresentationsofreasoningchains.Theresultantmodelisthen
fine-tunedwithacombinationofthreeobjectivesinvolvingpositivechains,negativechains,andtheunderlyinginteractions.
(MAG)isastructuredencodingoftheinteractionsbetween additionallyannotateeachnodev ∈V withabinarylabel
these A agents over r rounds. Formally, a MAG is a Di- y ∈{0,1},indicatingthecorrectnessoftheanswer. These
v
rectedAcyclicGraphG =(V,E),whereV isasetofnodes aremarkedwithgreenandredcirclesinFig.2. Werefer
and E is a set of directed edges. A node v ∈ V repre- tothesetwosetsofnodesasV+andV−respectivelysuch
i,j
sents the output of an agent A ∈ A in interaction round that V = V+(cid:83) V−. Following the interaction pattern of
i
j ∈ [0,r].3 For example, see Fig. 2(a) where each node RECONCILEthatconditionseachsubsequentroundofin-
represents an agent’s Chain-of-Thought reasoning to the teractiononallagentoutputsfromthepreviousround,we
question. Edgesdenoteconditionaldependenciesbetween defineedgesfromallsourcenodesv toalltargetnodes
i,j
theagents’interactionsandencodehowanagent’soutput v ,i.e.,(v ,v )∈E ∀i,k ∈[1,n],∀j ∈[0,r].
k,j+1 i,j k,j+1
isrefinedoverinteractionrounds. Specifically,wedefinea
directededgebetweentwonodesifthetargetnode’sgener- 3.2.TrainingData(MAG)Construction
ationisconditionedonthesourcenode’sgeneration.
WiththespecificsofaMAGdefined,wenowwanttocon-
Example of a MAG. MAGs can be generally defined structthesegraphsforagiventask. Thesewouldthenserve
forarbitraryagentsandinteractionpatterns. Wefocuson asdatafortrainingdistilledmodels.Givenareasoningprob-
reasoning problems as a general class of domains where lem(e.g.,amathwordproblem),wefollowRECONCILE
interactionhaspositiveimpacts,definingMAGsforLLM anduseGPT-4,Bard,andClaude2asthethreeinteracting
agents on commonsense and math reasoning tasks. Past LLMagentsforamaximumofthreerounds(seeChenetal.
workshavedefinedseveralsuchinteractionframeworks(Du (2023b)forfurtherdetailsoftheframework). Thediscus-
et al., 2023; Liang et al., 2023; Chen et al., 2023b) for sioncontinuesuntilaconsensusisreached,i.e.,allagents
which MAGs can be defined. Of these, we choose REC- agreeonthesameanswer. Thismeansthatwhenthereisno
ONCILE(Chenetal.,2023b)asourinteractionframework interaction(i.e.,allagents’initialanswersarethesame),a
becauseof(1)itsperformance: itobtainsthehighestperfor- MAGwillhave3disconnectednodes,oneforeachagent
manceacrossmultiplebenchmarks,and(2)itsflexibility: it (andnoedges). Whenthereisasingleroundofinteraction,
allowseachagenttoconverseinnaturallanguagefollowing itwillhave6nodesand9edges,andsoon. Insummary,for
thegenericChain-of-Thoughtreasoningparadigmandthus, agiventask,ourtrainingdatawillconsistofgraphsthatcan
canbereadilyappliedtoanydownstreamtaskwhereCoT begroupedintofourstructuraltypes,basedonthenumber
isapplicable. MAGsmakeminimalassumptionsaboutthe ofinteractionrounds. Wewillrefertothesegraphtypesas
contentsofnodesandhence,canbesimilarlydefinedfor G ,G ,G ,andG (withthesubscriptdenotingthenumber
0 1 2 3
agentsconversingusingotherpromptingtechniques(Chen ofrounds). Fig.2(a)isanexampleofaG graph.
2
etal.,2022). Fig.2(a)showsanexampleofaRECONCILE-
Benchmarks. Followingtheaboveframework,wecon-
basedMAGforamathproblem. SincethenodesinaMAG
structMAGsfor5 widely-used benchmarksoncommon-
representmodelresponses(eithercorrectorincorrect),we
sense and math reasoning: (1) StrategyQA (Geva et al.,
3Round0referstoanagent’spre-interactionoutput. 2021),(2)CommonsenseQA(Talmoretal.,2019;Aggar-
4MADGI:StructuredDistillationofMulti-AgentInteractionGraphs
wal et al., 2021), (3) AI2 Reasoning Challenge (Clark tillingstudentLMs.
et al., 2018), (4) GSM8K (Cobbe et al., 2021) and (5)
Objective1: LearningfromPositiveReasoning. Learn-
MATH(Hendrycksetal.,2021). Wealsoexperimentwith2
ingfromateacherLLM’scorrectreasoninghasbeenshown
OODdatasets: BoolQ(Clarketal.,2019)andSVAMP(Pa-
toimprovesmallermodels(Magisteretal.,2023;Lietal.,
teletal.,2021)forwhichwedonotconstructMAGsand
2023). Hence, MAGDIfirstfine-tunesthestudentmodel
areexclusivelyusedtotestthetransferofourmodel.
onallcorrectreasoningchainsx+ ∈V+ usingastandard
next-tokenpredictionobjective(seeFig.3‘Learningfrom
3.3.StatisticsofTrainingData(MAGs)
PositiveReasoning’). Thelossisdefinedasfollows.
We construct 1000 training MAGs for each in-domain
|x+|
benchmark. Wecategorizethedataalongthreedimensions: (cid:88) (cid:88)
L+ =− logp (x+|x+ ,q) (1)
rounds,agents,andgraphstructures. SeeAppendixTable8 θ i <i
forstatisticsalongeachdimensionandforeachbenchmark. x+∈V+i=1
Objective 2: Learning from Negative Reasoning.
Round. Recall that a MAG consists of nodes belonging
TeacherLLMsalsomakemistakes,particularlywhensolv-
todifferentinteractionroundsi∈[0,3]. Alldatasetshave
ing morechallenging problems. However, instead ofdis-
morenodesinlowerrounds,indicatingthatconsensusbe-
tweenagentsistypicallyachievedintheseearlierrounds.
cardingthese,MAGDItreatsthemaschallengingnegatives
(generated by a strong teacher) that a student model can
Thenumberofnodesinthelaterroundsisalsoanindicator
learn from by contrasting with positive chains. Given a
of the difficulty of the benchmark. For example, MATH
positivereasoningchainx+ ∈ V+ andanegativereason-
hasthemostnumberofround-3nodes,suggestingthateven
ing chain x− ∈ V−, we first extract representations of
aftertworoundsofdiscussion,thestrongteacherLLMsdo
thesechainsbyperformingaweightedaveragepoolover
notconvergeonasingleanswer.
the final layer’s hidden representations of the constituent
Agent. We can also group the MAG nodes based tokens. We represent these as h ∈ Rd and h ∈ Rd
x+ x−
on the agent generating the response at that node respectivelywheredistheembeddingdimension. Usinga
(GPT4/Claude2/Bard). Thenumberofnodesforeachagent projectionmatrixandatanhactivation,wefurtherproject
isthesamebecauseallagentsengageinallrounds. these embeddings to two scalar scores s ∈ [−1,1] and
x+
GraphStructure. Lastly,wealsoshowthebreak-downof
s
x−
∈ [−1,1]. MAGDI thenoptimizesforthefollowing
margin-basedobjective(Cortes&Vapnik,1995)forpairs
different MAG structures. Like nodes, G graphs are the
0 ofpositiveandnegativechains{x+,x−}∈V+×V− ina
most represented in our datasets while G graphs are the
3
MAG(seeFig.3‘LearningfromNegativeReasoning’).
least,andallgraphstructuresadduptoto1Kdatapoints
per task. We show examples of MAGs in Appendix D. (cid:88) (cid:88)
L− = max(0,ρ−s +s ) (2)
Using these MAGs as supervision, we train task-specific x+ x−
andmulti-taskdistilledmodels,asdiscussedbelow. x+∈V+x−∈V−
whereρ∈[−1,1]isthemargin(setto1inourexperiments).
3.4.MAGDI: StructuredDistillationfromMAGs
Objective3: LearningfromInteraction. Beyonddis-
Wenowdiscussourproposedstructureddistillationmethod, tillationfromthecorrectandincorrectnodes, MAGDI is
MAGDI,thatdistillsreasoningcapabilitiesintoasmaller alsointendedtodistillfromtheentireconversationalstruc-
studentmodelviamulti-teacherinteractiongraphs(MAGs). turepresentintheteachers’discussion. Thiswouldallow
thestudentmodeltosummarizethediscussionprocessand
MAGDI Overview. Broadly, MAGDI performs struc-
acquireknowledgeofhowateacherrefinesitsreasoning
tured distillation by augmenting a student model (e.g.,
chain in each discussion round. Hence, while the previ-
Mistral-7B-Instruct or LLaMA-2-7B-Chat) with a light-
ous two objectives assumed nodes to be a disconnected
weight Graph Neural Network (GNN) that is responsible
set, MAGDI removes this assumption by also modeling
formodelingthe‘structure’instructureddistillation(see
theedges. MAGDIachievesthisbyaugmentingthebase
Fig. 3 ‘MAGDI Components’). This augmented student
studentmodelwithaGraphConvolutionNetwork(GCN)
modelisthenfine-tunedwithacombinationofthreeobjec-
module(Kipf&Welling,2017). ThegoaloftheGCNis
tivesthatdistillknowledgefromthepositivenodes,negative
tolearnimproved,‘structure-aware’representationsofrea-
nodes,andtheedges(i.e.,interactions)inaMAG(seeFig.3
soningchains(nodes)suchthatthestudentmodellearnsto
‘MAGDIObjectivesforDistillaton’). Wedenotethebase
discriminatebetweencorrectandincorrectnodesinaMAG,
modelasp (·)andtheinputreasoningproblemasq. For
θ eventuallyleadingtobettergenerationofreasoningchains.
brevity, we denote the generation at any MAG node as a
variable-lengthsequenceoftokens: x = {x 1,x 2,...,x |x|}. Givenanypositiveornegativenodex ∈ V,MAGDIgen-
Below,wedescribethethreeobjectivesforstructurallydis- eratesanoderepresentationh ∈RdfromthebaseLM.It
x
5MADGI:StructuredDistillationofMulti-AgentInteractionGraphs
thenlearnsstructure-awarerepresentationsofthesenodes Baselines. Wegroupallmethodsintothreecategoriesof
withatwo-layerGCNusingthefollowingequation: ‘Distillation Source Type’, described as follows. (1) No
Teacher. Thelowerboundofourdistilledmodelsisthe
h(l+1) =σ(D−1Mh(l)W(l)) zero-shotbasemodel(e.g.,Mistral-7B-Instruct). (2)Single-
x x
Teacher. Ournextsetofbaselinesonlyusestrainingdata
where M ∈ |V|×|V| is the adjacency matrix with self- fromasingleteacher(outofthethreeagentsusedtocon-
connections, D is the diagonal degree matrix of M, σ is structMAGs). Thisfollowsmultiplepriorworks(Lietal.,
theReLUactivationfunction,h(l)andh(l+1)aretheinput 2023;Magisteretal.,2023;Fuetal.,2023;Hoetal.,2023)
x x
and updated node representations of the l-th layer. We thatfine-tunestudentmodelsonCoTreasoningusingthe
set h(0) = h and W(l) as the weight matrix of the l-th next-tokenpredictionobjective(Equation1). Inparticular,
x x
layer. Aftertwolayersofmessagepassing,weobtainthe wefine-tunethreestudentmodelswithoneofGPT-4,Bard,
final node representation h(L), which is now conditioned orClaude2astheteacherusingonlythepositivesamples
x
o lin net ah re lg ayra ep rh pas rtr au mc etu tere ri. zeM dA byG WDIt ∈he Rn dp ×ro Cje (c wts heh rx( eL C) w isit th ha
e
f Sr io nm gleth -Te er ae csp he ec rt div ise tit le lea dch mer om deo lsde al s. SW ITe .w Ti hll er nef Se Ir Tt -o Gt Phe Ts 4e
,
c forexample,willrefertoadistilledmodeltrainedwithonly
number of node labels) and applies the softmax function
GPT4astheteacher. (3)Multi-Teacher. Multi-teacher
toderivetheprobabilitydistributionoverthenodelabels
modelscorrespondtodistilledmodelstrainedwithvariants
yˆ =softmax(h(L)W ). Finally,weusecross-entropyloss
x x c ofourproposedMAGDImethod.Hence,thesealsodirectly
forthe(correct/incorrect)nodeclassificationobjectiveover
demonstratetheutilityofthelevelsinFig.2.
allnodesinaMAG(seeFig.3‘LearningfromInteraction’),
C • MAGDI-R0(Level1). MAGDI-R0referstoamodel
L =−(cid:88)(cid:88) y(i)log(yˆ(i)) (3) thatisfine-tunedonlyontheRound-0(pre-interaction)
I x x
x∈Vi=1 correctreasoningofmultipleteachers(i.e.,GPT-4,Bard,
and Claude2) using only the L+ objective defined in
wherey isaone-hotencodingoftheground-truthlabelof
x Eqn.1,i.e.,Level1ofMAGDI.
anodex.
• MAGDI-CN (Level 2). Next, MAGDI-CN is a dis-
FinalObjective. Ourfinalloss,L MAG,asdefinedbelow, tilledmodelthatistrainedonallcorrect(pre-andpost-
isaweightedcombinationofthethreelosses.
interaction) reasoning from all teachers with again the
sameL+loss,correspondingtoLevel2ofMAGDI.
L =αL++βL−+γL (4)
MAG I
• MAGDI-AN(Level3). Goingonestepfurther,MAGDI-
withα,β,γ ∈[0,1]beingtherespectiveweights. ANisadistilledmodelthatistrainedonallnodes(correct
and incorrect) from all teachers. Hence, this is trained
MAGDIInference. TheGCNmoduleisonlyuseddur- with both L+ and L− objectives in Equations 1 and 2.
ingthedistillationprocess. Hence,attesttime,thestudent NotethatallthreemodelsforLevels1-3areunstructured
modelperformszero-shotinferencegivenjustthequestion multi-teacherdistilledmodelsthatviewMAGsasasetof
andusesonlythesamenumberofparametersandarchitec- correctandincorrectnodes.
tureasthebasestudentmodel.
• MAGDI (Level 4). This is our final full method that
distillsknowledgefromallnodesandedgesofaMAG
4.ExperimentalSetup usingallthreeobjectivesasdescribedinEquation4.
Implementation Details. We test our structured distilla-
tionmethod,MAGDI,withthreeinstruction-tunedstudent
modelsthatareofdifferentscalesandbelongtodifferent
5.ResultsandAnalysis
model families: Mistral-7B-Instruct, LLaMA-2-7B-Chat,
andLLaMA-2-13B-Chat. Wetrainbothtask-specificdis-
5.1.MainResults
tilledmodels(i.e.,trainedandtestedonasingledownstream
task)andalsoonejointmulti-taskdistilledmodel(trained Our primary results demonstrate the effectiveness of
on all in-domain tasks together and then tested on each MAGDI acrossfivereasoningbenchmarksoverdifferent
in-domainandout-of-domaintask). Themulti-taskmodel single-teacher and multi-teacher distillation setups. For
representsourunifiedmodelforOODtasks. Forconcise- themainresults,weuseMistral-7B-Instructasthestudent
ness,wereferMAGDItotheresultanttask-specificmodels modelandtraintask-specificdistilledmodels(seeSec.5.2
andMAGDI-MTwillrefertotheresultantmulti-taskmodel. forexperimentswithmulti-taskmodelswithlargerstudents
SeeAppendixAforotherimplementationdetails. belongingtodifferentmodelfamilies). Wereportaccuracy
6MADGI:StructuredDistillationofMulti-AgentInteractionGraphs
Table1.Comparisonofstructureddistillation(MAGDI)withnoteacher,single-teacher,andmulti-teacherdistillationbaselines.Firstly,
MAGDIoutperformsallbaselinesacrossallfivereasoningbenchmarks. Onaverage,MAGDIoutperformsthestrongestSIT-GPT4
baselineby4.61%andthenoteacherbaselineby10.71%.Secondly,knowledgedistillationfromeachcomponentofMAGimprovesthe
studentmodel,asdemonstratedbyaconsistentincreaseinperformancefromLevel1toLevel4.
Datasets
DistillationType DistillationData DistilledModel AverageAcc
StrategyQA CSQA ARC-c GSM8K MATH
NoTeacher(Jiangetal.,2023) - Mistral-7B-Instruct 61.57 57.89 60.32 44.05 7.02 46.17
Single-Teacher(Lietal.,2023; Claude2 SIT-Claude2 64.39 64.18 68.24 45.34 7.24 49.89
Magisteretal.,2023;Fuetal., Bard SIT-Bard 68.56 65.06 66.87 45.61 7.06 50.63
2023;Hoetal.,2023) GPT-4 SIT-GPT4 69.96 66.87 68.91 47.38 8.24 52.27
Round-0Nodes MAGDI-R0[Level1] 71.18 67.36 72.06 48.52 9.72 53.77[+1.50%]
Multi-Teacher CorrectNodes MAGDI-CN[Level2] 71.62 69.31 72.34 50.11 10.66 54.81[+2.54%]
(Ours) AllNodes MAGDI-AN[Level3] 72.10 70.65 71.92 50.69 11.98 55.47[+3.20%]
MAG MAGDI[Level4] 74.24 72.56 72.61 52.27 12.76 56.88[+4.61%]
Table2.Comparisonofthetokencountsgeneratedby RECON- 5e-3
CILE(amulti-agentinteractionframework)andMAGDI.
RECONCILE MAGDI Reduction 3e-3
ReConcile SiT-GPT4
StrategyQA 924.5 107.5 8.6x
MAGDi Mistral-7b
CSQA 936.9 104.2 9.0x 1e-3
ARC-c 448.3 86.4 5.2x 45 50 55 60 65 70 75
GSM8K 642.3 141.6 4.5x Accuracy
MATH 1900.1 645.0 2.9x
Figure4.Trade-offbetweenperformanceandefficiency.MAGDI
Average 970.4 216.9 4.5x exceedstheParetofrontierofpriorwork,surpassingsingle-teacher
modelsinperformanceandsurpassingRECONCILEinefficiency,
definedas1/avg(tokens).
foreachtask. BasedonTable1results,wesummarizeour
mainconclusionsbelow,addressingourresearchquestion
of3.20%(seeMAGDI-ANrowversusSIT-GPT4row).
posedinSection1: Howcanwedistilldiverseteacherinter-
actionsintoasmallerandmoreefficientstudent,utilizing Level4: Structureddistillationfrominteractionsoutper-
theincreasinglevelsofinteractionstructureinaMAG? formsallmulti-teacherbaselines. Ourfinalstructured
distillationmethod,MAGDI,thatdistillsknowledgefrom
Level1: Distillationfrommultipleteachersoutperforms
boththeoutputs(nodes)andstructure(edges)ofmulti-agent
distillationfromthesinglestrongestteacher. Knowledge
interactions, obtains the largest improvement and outper-
distillation from the correct reasoning chains of multiple
formsallsingle-andmulti-teachermodels.Whenappliedto
teachersi.e.,GPT-4,Bard,andClaude2,outperformsdis-
Mistral-7B-Instruct,MAGDIsurpassesamodelthatlearns
tillationfromthesinglestrongestteacherbyanaverageof
only from GPT4 by an average of 4.61%. Out of all the
1.50%(seeMAGDI-R0rowversusSIT-GPT4row). Dif-
levels,distillationfrominteractionsbringsthemostimprove-
ferentteachersbringdiversityintheirreasoning,leadingto
ments(bothindividuallyandacrossallfivebenchmarks),
improvedreasoningcapabilitiesofthestudentmodel.
specificallydemonstratingtheutilityofmodelingthecon-
Level 2: Distillation from pre- and post-interaction versationstructureofmulti-agentdiscussions. Overall,com-
reasoningoutperformsonlypre-interactionreasoning. paredtothebasemodel, MAGDI improvesthestudent’s
Trainingastudentmodelonallcorrectreasoningchainsaf- reasoning capabilities by a significant 10.71% (46.17 →
termultipleteachermodelshaveinteractedfurtherimproves 56.88)whilemaintainingsimilarinference-timeefficiency.
thestudent’sreasoning. Fine-tuningonpost-interactionrea-
MAGDI improves inference efficiency compared to
soning chains effectively increases the training data and
RECONCILE. ComparedtoRECONCILE,MAGDIalso
its usefulness is validated through an overall 2.54% im-
drasticallyimprovesinferenceefficiency. Becausethesizes
provement,comparedtothebestsingle-teachermodel(see
of gated models like those used in RECONCILE are not
MAGDI-CNrowversusSIT-GPT4row).
known,wemeasureefficiencyvianumberofoutputtokens
Level3: Negativereasoningchainsfurtherimprovedis- generated.4 As shown in Table 2, MAGDI achieves up
tillation. A student model that additionally learns by
4Thismetricisextremelystrictandunder-estimatesoureffi-
contrastingbetweenpositiveandnegativechainsimproves
ciencygains, aseachofthegatedLLMsusedby RECONCILE
overallpreviouslevelsandthesingle-teacherbaselines.Our exceeds 7B parameters, and each example for RECONCILE in-
MAGDI-ANmodelobtainsanoverallaverageimprovement volvesmultipleinferencecalls.
7
ycneiciffEMADGI:StructuredDistillationofMulti-AgentInteractionGraphs
Table3.Out-of-domain comparison between Single-Teacher
MAGDi No Teacher
Multi-Task(SIT-GPT4-MT)andMAGDIMulti-Task(MAGDI-
MT) models. MAGDI-MT performs up to 7% better than the 50
single-teacherbaselineevenonOODdatasets(57.52vs.64.30).
40
BoolQ SVAMP
SiT-GPT4-MT 60.70 57.52 30
MAGDi-MT 63.98 64.30 LLaMA2-7B LLaMA2-13B Mistral-7B
Base Student Model
toa9xreductionintokencount. MAGDIobtainsthisef-
Figure5.ScalingresultsofMAGDIwithdifferentbasestudent
ficiency by always answering questions in one inference
models. As the average (zero-shot) performance of the base
call;thisdiffersfromRECONCILEwhichhastogothrough modelimproves(Mistral-7B>LLaMA-2-13B>LLaMA-2-7B),
uptotwelve(expensive)LLMinferencecalls(involving3 MAGDIshowsacorrespondingincrease.
agents for the initial round and another 3 rounds). More-
over,Figure4showsthetrade-offbetweentokenefficiency Table4.Self-consistencywithMAGDIonGSM8Kachievesthe
andperformance(averagedacrossdatasets). While REC- largest gain (up to 15%) compared to the same with the base
ONCILE(astheupper-bound)hasthebestperformance,it studentmodelandthesingle-teacherdistilledmodel.
alsoproducesthemosttokens(cf. Table2);thezero-shot
Mistral-7B SIT-GPT4 MAGDI
andsingle-teachermodelsaremoreefficientthanRECON-
w/oSC 44.05 47.38 52.27
CILEbutsufferintermsofperformance. Ontheotherhand,
w/SC 48.44[+4.39%] 58.62[+11.24%] 67.42[+15.15%]
MAGDI achieves a better balance of efficiency and per-
formance than these baselines, with more efficiency than allfivebenchmarks(andseeTable5forindividualresults).
RECONCILEandhigherperformancethanthezero-shotand Theorderingofthethreemodelsisbasedontheirzero-shot
priorsingle-teacherdistillationmethods. performance. Weobservethat MAGDI bringsconsistent
improvementsoverallbasemodels,maintainingproportion-
5.2.Analysis: Generalizability,Scalability,Diversity ategains. Overall,thescalingtrendof MAGDI suggests
thatstructureddistillationshouldcontinuetoimproveeven
MAGDIcanbeusedtotrainonejointmulti-taskmodel.
stronger students. See Appendix B for other analyses of
In our main experiments (Sec. 5.1), we fine-tuned task-
MAGDIe.g.,theeffectoftheamountoftrainingdataand
specific models with structured distillation. While these
differentgraphstructures.
multi-teachertask-specificmodelsshowclearbenefitsover
single-teacher models, we would ideally like one joint MAGDIboostsself-consistency. Wehypothesizethata
modelthatcantacklealltasks. Therefore,wetrainajoint studentmodelthatlearnsfrommultipleteacherswillexhibit
multi-taskmodel(MAGDI-MT)bycombiningtrainingdata betterdiversityinitsgenerations. Totestthis,wecombine
fromallfivebenchmarksandevaluatingittogetheronall MAGDI withanensemblemethodlikeSelf-Consistency
benchmarks. MAGDI-MT obtains an average accuracy (SC) (Wang et al., 2023). SC computes a majority vote
thatiswithin1%oftask-specificmodels(56.89%versus over model answers and has proven effective for reason-
55.12%),showingitsapplicabilityfortrainingajointmodel ing tasks; SC’s improvements are predicated on answer
(refertoAppendixTable9forthefulltable). diversity (since a majority vote over the versions of the
sameanswerwouldnotyieldanyimprovements). Weshow
MAGDI generalizes to OOD tasks. We also evaluate
thatself-consistencywithourmulti-teacherdistilledmodel
MAGDI-MTontwoout-of-domainbenchmarks(BoolQfor
outperforms the same with (1) the base model and more
commonsensereasoningandSVAMPformath)thatwere
importantly,(2)asingle-teacherdistilledmodel. Asshown
notincludedinmulti-tasktraining. AsshowninTable3,
inTable4,onGSM8K,SCbetween10responsesimproves
MAGDI outperformssingle-teacherdistillationbyto3%
accuracyby15%whenappliedtoa MAGDI-trainedstu-
onBoolQand7%onSVAMP.Thebroaderimplicationof
dent,comparedtoonly4%improvementforthebasemodel
thisisthatthesingletakeawayMAGDI-MTmodelmain-
(Mistral-7B-Instruct)and11%forthesingle-teachermodel
tains good performance on OOD tasks and continues to
(SIT-GPT4). Broadly,thissuggeststhatinference-timeal-
outperformsingle-teacherbaselinesonnewdatasets.
gorithms relying on a model’s inherent diversity can be
MAGDIscalespositivelywithbetterbasemodels. We boostedfrommulti-teacherdistillation.
nowstudythescalingpropertiesofstructureddistillationby
varyingthebasestudentmodel. Inparticular,wetrainthree 6.Conclusion
structurallydistilledmodelswithMAGDI,usingLLaMA-2-
7B-Chat,LLaMA-2-13B-Chat,andMistral-7B-Instructas Wehavetackledtheproblemofstructureddistillationfrom
thebasemodels. InFig.5,weplottheaverageaccuracyof multi-agentinteractionsasawaytoequipmuchsmallerand
8
ycaruccA
egarevAMADGI:StructuredDistillationofMulti-AgentInteractionGraphs
moreefficientlanguagemodelswithimprovedreasoningca- ligence: Early experiments with gpt-4. arXiv preprint
pabilities. Toachievethisgoal,weproposedagraph-based arXiv:2303.12712, 2023. URL https://arxiv.
representation of these interactions, generated graphs for org/abs/2303.12712.
training,anddevelopedastructureddistillationmethodfor
Buciluaˇ, C., Caruana, R., and Niculescu-Mizil, A.
learningfromtheseinteractiongraphs. Ourresultsshowthe
Model compression. In Proceedings of the 12th
effectiveness,generalizability,andscalabilityofstructured
ACM SIGKDD international conference on Knowl-
distillationacrossmultiplereasoningbenchmarks.
edge discovery and data mining, pp. 535–541,
2006. URLhttps://dl.acm.org/doi/abs/10.
Acknowledgements
1145/1150402.1150464.
WethankPeterHaseandArchikiPrasadforusefulfeedback Chen, B., Shu, C., Shareghi, E., Collier, N., Narasimhan,
andsuggestionsregardingexperiments. Thisworkwassup- K., and Yao, S. Fireact: Toward language agent fine-
portedbyNSF-CAREERAward1846185,NSF-AIEngage tuning. arXivpreprintarXiv:2310.05915,2023a. URL
InstituteDRL-2112635,DARPAMCSGrantN66001-19- https://arxiv.org/abs/2310.05915.
2-4031,AccelerateFoundationModelsResearchprogram,
and a Google PhD Fellowship. The views contained in Chen,J.andYang,D. Structure-awareabstractiveconversa-
thisarticlearethoseoftheauthorsandnotofthefunding tionsummarizationviadiscourseandactiongraphs. In
agency. Proceedingsofthe2021ConferenceoftheNorthAmer-
icanChapteroftheAssociationforComputationalLin-
guistics: HumanLanguageTechnologies,pp.1380–1391,
BroaderImpacts
2021. URLhttps://aclanthology.org/2021.
ThecomputingresourcesusedbyLLMsincurasubstantial
naacl-main.109/.
carbonfootprint(Strubelletal.,2019),andrunningthem
Chen, J. C.-Y., Saha, S., and Bansal, M. Reconcile:
ininteractivemulti-agentsettingslikeRECONCILEfurther
Round-tableconferenceimprovesreasoningviaconsen-
increasestheseenvironmentalcosts. MAGDIdistillsfrom
susamongdiversellms.arXivpreprintarXiv:2309.13007,
severalLLMsintoasinglemoreefficientandsmallerLM,
2023b. URL https://arxiv.org/abs/2309.
thusreducingtheenvironmentalcostofrunningLLMswhile
13007.
stillmaintainingstrongperformance.
Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and
TheLLMsMAGDIdistillsfrom–andthestudentmodelsit
Hinton, G. E. Big self-supervised models are strong
distillsinto–canreflectstereotypes,biases,andothernega-
semi-supervisedlearners. Advancesinneuralinforma-
tivetraitspresentintheirpre-trainingdata(Weidingeretal.,
tionprocessingsystems, 33:22243–22255, 2020. URL
2021), which we do not have control over. Our distilled
https://arxiv.org/abs/2006.10029.
LLMshavethesamecapacityforundesirablegenerationas
theirteachermodelsandtheirrespectivezero-shotvariants; Chen, W., Ma, X., Wang, X., and Cohen, W. W. Pro-
assuch,themodelsresultingfromMAGDIdistillationhave gramofthoughtsprompting: Disentanglingcomputation
thesamepotentialformisuseasanyLLMormethoddistill- from reasoning for numerical reasoning tasks. arXiv
ingfromLLMs. Morestudiesareneededtoevaluateand preprint arXiv:2211.12588, 2022. URL https://
mitigatesuchbiasesinLLMs. arxiv.org/abs/2211.12588.
Chenglin,L.,Qianglong,C.,Caiyu,W.,andYin,Z. Mixed
References
distillationhelpssmallerlanguagemodelbetterreasoning.
Aggarwal, S., Mandowara, D., Agrawal, V., Khandelwal,
arXivpreprintarXiv:2312.10730,2023. URLhttps:
D.,Singla,P.,andGarg,D. ExplanationsforCommon-
//arxiv.org/abs/2312.10730.
senseQA:NewDatasetandModels. InProceedingsof
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
the59thAnnualMeetingoftheAssociationforComputa-
H.,Zheng,L.,Zhuang,S.,Zhuang,Y.,Gonzalez,J.E.,
tionalLinguisticsandthe11thInternationalJointConfer-
Stoica, I., and Xing, E. P. Vicuna: An open-source
enceonNaturalLanguageProcessing(Volume1: Long
chatbot impressing gpt-4 with 90%* chatgpt quality,
Papers),pp.3050–3065.AssociationforComputational
March 2023. URL https://lmsys.org/blog/
Linguistics, 2021. URL https://aclanthology.
2023-03-30-vicuna/.
org/2021.acl-long.238/.
Clark,C.,Lee,K.,Chang,M.-W.,Kwiatkowski,T.,Collins,
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., M.,andToutanova,K. BoolQ:Exploringthesurprising
Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., difficultyofnaturalyes/noquestions. InBurstein,J.,Do-
Lundberg, S., et al. Sparks of artificial general intel- ran,C.,andSolorio,T.(eds.),Proceedingsofthe2019
9MADGI:StructuredDistillationofMulti-AgentInteractionGraphs
ConferenceoftheNorthAmericanChapteroftheAssoci- in Natural Language Processing and the 9th Interna-
ationforComputationalLinguistics: HumanLanguage tionalJointConferenceonNaturalLanguageProcessing
Technologies, Volume 1 (Long and Short Papers), pp. (EMNLP-IJCNLP),pp.154–164,2019.
2924–2936,Minneapolis,Minnesota,June2019.Associ-
Hendrycks,D.,Burns,C.,Kadavath,S.,Arora,A.,Basart,
ationforComputationalLinguistics. doi: 10.18653/v1/
N19-1300. URL https://aclanthology.org/ S., Tang, E., Song, D., and Steinhardt, J. Measuring
N19-1300. mathematical problem solving with the math dataset.
NeurIPS, 2021. URL https://arxiv.org/abs/
Clark,P.,Cowhey,I.,Etzioni,O.,Khot,T.,Sabharwal,A., 2103.03874.
Schoenick, C., andTafjord, O. Thinkyouhavesolved
Hinton, G., Vinyals, O., and Dean, J. Distilling
questionanswering? tryarc,theai2reasoningchallenge.
arXivpreprintarXiv:1803.05457,2018. URLhttps: the knowledge in a neural network. arXiv preprint
//arxiv.org/abs/1803.05457. arXiv:1503.02531, 2015. URL https://arxiv.
org/abs/1503.02531.
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun,
H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Ho,N.,Schmid,L.,andYun,S.-Y. Largelanguagemod-
Nakano, R., Hesse, C., andSchulman, J. Trainingver- els are reasoning teachers. In Proceedings of the 61st
ifiers to solve math word problems. arXiv preprint Annual Meeting of the Association for Computational
arXiv:2110.14168, 2021. URL https://arxiv. Linguistics(Volume1: LongPapers),pp.14852–14882,
org/abs/2110.14168. Toronto, Canada, July 2023. Association for Compu-
tational Linguistics. doi: 10.18653/v1/2023.acl-long.
Cortes, C.andVapnik, V. Support-vectornetworks. Ma- 830. URL https://aclanthology.org/2023.
chinelearning,20:273–297,1995. acl-long.830.
Deng,Y.,Prasad,K.,Fernandez,R.,Smolensky,P.,Chaud- Hu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,
hary, V., and Shieber, S. Implicit chain of thought S.,Wang,L.,andChen,W. LoRA:Low-rankadaptation
reasoning via knowledge distillation. arXiv preprint oflargelanguagemodels. InInternationalConference
arXiv:2311.01460, 2023. URL https://arxiv. on Learning Representations, 2022. URL https://
org/abs/2311.01460. openreview.net/forum?id=nZeVKeeFYf9.
Du, Y., Li, S., Torralba, A., Tenenbaum, J. B., and Mor- Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
datch, I. Improving factuality and reasoning in lan- Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel,
guage models through multiagent debate, 2023. URL G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv
https://arxiv.org/abs/2305.14325. preprint arXiv:2310.06825, 2023. URL https://
arxiv.org/abs/2310.06825.
Fu,Y.,Peng,H.,Ou,L.,Sabharwal,A.,andKhot,T. Spe-
cializing smaller language models towards multi-step Kipf, T. N. and Welling, M. Semi-supervised classi-
reasoning. In Krause, A., Brunskill, E., Cho, K., En- fication with graph convolutional networks. In In-
gelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro- ternational Conference on Learning Representations,
ceedings of the 40th International Conference on Ma- 2017. URLhttps://openreview.net/forum?
chineLearning,volume202ofProceedingsofMachine id=SJU4ayYgl.
LearningResearch,pp.10421–10430.PMLR,23–29Jul
2023.URLhttps://proceedings.mlr.press/ Kirchhoff,K.andOstendorf,M. Directionsformulti-party
v202/fu23d.html. human-computerinteractionresearch. InProceedingsof
theHLT-NAACL2003WorkshoponResearchDirections
Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., in Dialogue Processing, pp. 7–9, 2003. URL https:
and Berant, J. Did aristotle use a laptop? a ques- //aclanthology.org/W03-0703/.
tionansweringbenchmarkwithimplicitreasoningstrate-
gies. Transactions of the Association for Computa- Kojima,T.,Gu,S.S.,Reid,M.,Matsuo,Y.,andIwasawa,
tionalLinguistics,9:346–361,2021. doi: 10.1162/tacl Y. Largelanguagemodelsarezero-shotreasoners. Ad-
a 00370. URL https://aclanthology.org/ vances in neural information processing systems, 35:
2021.tacl-1.21. 22199–22213, 2022. URL https://arxiv.org/
abs/2205.11916.
Ghosal,D.,Majumder,N.,Poria,S.,Chhaya,N.,andGel-
bukh, A. Dialoguegcn: A graph convolutional neural Leifeld,P. Discoursenetworkanalysis.policydebatesas
networkforemotionrecognitioninconversation. InPro- dynamic networks,[w:] jn victor, ah montgomery, m.
ceedingsofthe2019ConferenceonEmpiricalMethods lubell(red.),2018.
10MADGI:StructuredDistillationofMulti-AgentInteractionGraphs
Li, L. H., Hessel, J., Yu, Y., Ren, X., Chang, K.-W., and Language Technologies, pp. 2080–2094, Online, June
Choi,Y. Symbolicchain-of-thoughtdistillation: Small 2021. Association for Computational Linguistics. doi:
models can also “think” step-by-step. In Rogers, A., 10.18653/v1/2021.naacl-main.168. URL https://
Boyd-Graber,J.,andOkazaki,N.(eds.),Proceedingsof aclanthology.org/2021.naacl-main.168.
the61stAnnualMeetingoftheAssociationforCompu-
Qin, L., Li, Z., Che, W., Ni, M., and Liu, T. Co-gat:
tationalLinguistics(Volume1: LongPapers),pp.2665–
A co-interactive graph attention network for joint di-
2679,Toronto,Canada,July2023.AssociationforCom-
alog act recognition and sentiment classification. In
putationalLinguistics. doi: 10.18653/v1/2023.acl-long.
Proceedings of the AAAI conference on artificial in-
150. URL https://aclanthology.org/2023.
telligence, volume 35, pp. 13709–13717, 2021. URL
acl-long.150.
https://arxiv.org/abs/2012.13260.
Liang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang,
Saha, S., Hase, P., and Bansal, M. Can language mod-
R.,Yang,Y.,Tu,Z.,andShi,S. Encouragingdivergent
elsteachweakeragents? teacherexplanationsimprove
thinkinginlargelanguagemodelsthroughmulti-agent
students via personalization. In NeurIPS, 2023. URL
debate. arXiv preprint arXiv:2305.19118, 2023. URL
https://arxiv.org/abs/2305.19118.
https://arxiv.org/abs/2306.09299.
Liu,B.,Bubeck,S.,Eldan,R.,Kulkarni,J.,Li,Y.,Nguyen, Shen,W.,Wu,S.,Yang,Y.,andQuan,X. Directedacyclic
A., Ward, R., and Zhang, Y. Tinygsm: achieving graphnetworkforconversationalemotionrecognition. In
>80% on gsm8k with small language models. arXiv Proceedingsofthe59thAnnualMeetingoftheAssocia-
preprint arXiv:2312.09241, 2023. URL https:// tionforComputationalLinguisticsandthe11thInterna-
arxiv.org/abs/2312.09241. tionalJointConferenceonNaturalLanguageProcessing
(Volume1: LongPapers), pp.1551–1560, 2021. URL
Magister, L. C., Mallinson, J., Adamek, J., Malmi, E., https://arxiv.org/abs/2105.12907.
and Severyn, A. Teaching small language models to
reason. In Proceedings of the 61st Annual Meeting Shridhar,K.,Stolfo,A.,andSachan,M.Distillingreasoning
of the Association for Computational Linguistics (Vol- capabilitiesintosmallerlanguagemodels. InFindings
ume2: ShortPapers),pp.1773–1781,Toronto,Canada, oftheAssociationforComputationalLinguistics: ACL
July 2023. Association for Computational Linguistics. 2023, pp. 7059–7073, 2023. URL https://arxiv.
doi: 10.18653/v1/2023.acl-short.151. URL https: org/abs/2212.00193.
//aclanthology.org/2023.acl-short.151.
Strubell, E., Ganesh, A., and McCallum, A. Energy and
Mitra,A.,DelCorro,L.,Mahajan,S.,Codas,A.,Simoes,C., policy considerations for deep learning in nlp. In Pro-
Agarwal,S.,Chen,X.,Razdaibiedina,A.,Jones,E.,Ag- ceedingsofthe57thAnnualMeetingoftheAssociation
garwal,K.,etal.Orca2:Teachingsmalllanguagemodels forComputationalLinguistics,pp.3645–3650,2019.
howtoreason. arXivpreprintarXiv:2311.11045,2023.
Talmor, A., Herzig, J., Lourie, N., and Berant, J. Com-
URLhttps://arxiv.org/abs/2311.11045.
monsenseQA:Aquestionansweringchallengetargeting
Mukherjee,S.,Mitra,A.,Jawahar,G.,Agarwal,S.,Palangi, commonsenseknowledge. InBurstein,J.,Doran,C.,and
H.,andAwadallah,A. Orca: Progressivelearningfrom Solorio,T.(eds.),Proceedingsofthe2019Conferenceof
complex explanation traces of gpt-4. arXiv preprint theNorthAmericanChapteroftheAssociationforCom-
arXiv:2306.02707, 2023. URL https://arxiv. putationalLinguistics: HumanLanguageTechnologies,
org/abs/2306.02707. Volume1(LongandShortPapers),pp.4149–4158,Min-
neapolis,Minnesota,June2019.AssociationforCompu-
Ouyang, S., Zhang, Z., and Zhao, H. Dialogue graph
tationalLinguistics. doi: 10.18653/v1/N19-1421. URL
modelingforconversationalmachinereading. InFind- https://aclanthology.org/N19-1421.
ings of the Association for Computational Linguistics:
ACL-IJCNLP2021,pp.3158–3169,2021. URLhttps: Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,
//arxiv.org/abs/2012.14827. Guestrin,C.,Liang,P.,andHashimoto,T.B. Stanford
alpaca: Aninstruction-followingllamamodel,2023.
Patel, A., Bhattamishra, S., and Goyal, N. Are NLP
models really able to solve simple math word prob- Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,
lems? In Toutanova, K., Rumshisky, A., Zettlemoyer, Khashabi, D., and Hajishirzi, H. Self-instruct: Align-
L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, inglanguagemodelswithself-generatedinstructions. In
R.,Chakraborty,T.,andZhou,Y.(eds.),Proceedingsof Rogers,A.,Boyd-Graber,J.,andOkazaki,N.(eds.),Pro-
the2021ConferenceoftheNorthAmericanChapterof ceedingsofthe61stAnnualMeetingoftheAssociation
theAssociationforComputationalLinguistics: Human forComputationalLinguistics(Volume1: LongPapers),
11MADGI:StructuredDistillationofMulti-AgentInteractionGraphs
pp.13484–13508,Toronto,Canada,July2023.Associ- ACM SIGKDD International Conference on Knowl-
ationforComputationalLinguistics. doi: 10.18653/v1/ edge Discovery and Data Mining, pp. 1285–1294,
2023.acl-long.754. URLhttps://aclanthology. 2017. URLhttps://dl.acm.org/doi/pdf/10.
org/2023.acl-long.754. 1145/3097983.3098135.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,
Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought Appendix
prompting elicits reasoning in large language models.
AdvancesinNeuralInformationProcessingSystems,35: StrategyQA GSM8K
24824–24837, 2022. URL https://arxiv.org/
abs/2201.11903. 70 50
Wei, J., Shuster, K., Szlam, A., Weston, J., Urbanek, 60 45
J., and Komeili, M. Multi-party chat: Conversational
agentsingroupsettingswithhumansandmodels. arXiv 50 40
preprint arXiv:2304.13835, 2023. URL https://
arxiv.org/abs/2304.13835. 40 35
100 500 700 9001000 100 500 700 9001000
Sample Size Sample Size
Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato,
J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B., Figure6.Results with scaling training data on StrategyQA and
Loading [a11y]/accessibility-menu.js
Kasirzadeh, A., et al. Ethical and social risks of harm GSM8K.Withstructureddistillation,studentaccuracyincreases
fromlanguagemodels. arXivpreprintarXiv:2112.04359, withanincreaseintrainingdata.
2021.
West,P.,Bhagavatula,C.,Hessel,J.,Hwang,J.,Jiang,L., A.ImplementationDetailsof MAGDI
LeBras,R.,Lu,X.,Welleck,S.,andChoi,Y. Symbolic
LearningfromNegativeReasoning. Weprovideaddi-
knowledgedistillation: fromgenerallanguagemodelsto
commonsensemodels. InCarpuat,M.,deMarneffe,M.-
tionaldetailsaboutMAGDI’scontrastiveobjectiveinEqua-
tion 2. In practice, since a MAG can have many nodes,
C.,andMezaRuiz,I.V.(eds.),Proceedingsofthe2022
computingthecontrastivelossforeverypossiblepositive
ConferenceoftheNorthAmericanChapteroftheAssoci-
andnegativepaircanbeprohibitivelyexpensive. Insuch
ationforComputationalLinguistics: HumanLanguage
cases,werandomlysamplenodesfromtheminoritygroup
Technologies,pp.4602–4625,Seattle,UnitedStates,July
to pair up with the nodes from the majority group. For
2022. Association for Computational Linguistics. doi:
instance, ifthereare5positiveand3negativenodesina
10.18653/v1/2022.naacl-main.341. URL https://
MAG,thenitnaturallyformsthreepairsandfortheremain-
aclanthology.org/2022.naacl-main.341.
ing2positives,werandomlysample2negativestopairwith
Wu,Q.,Bansal,G.,Zhang,J.,Wu,Y.,Zhang,S.,Zhu,E.,Li, them.
B.,Jiang,L.,Zhang,X.,andWang,C.Autogen:Enabling
next-gen llm applications via multi-agent conversation ModelTraining. MAGDI-distilledmodelsarefine-tuned
framework.arXivpreprintarXiv:2308.08155,2023.URL using Low-Rank Adaptation (LoRA) for efficiency (Hu
https://arxiv.org/abs/2308.08155. et al., 2022). We set the rank to 16 and alpha to 32. We
fine-tunethestudentmodelfor10epochsusingalearning
Yang,Z.,Shou,L.,Gong,M.,Lin,W.,andJiang,D. Model
rateof5e-6andabatchsizeof16. Forthehyperparameters
compressionwithtwo-stagemulti-teacherknowledgedis-
α, β and γ, please refer to our code implementation for
tillation for web question answering system. In Pro-
detailedsettingsineachdataset. Allofourexperimentsare
ceedings of the 13th International Conference on Web
runonfourRTXA6000with48Gmemoryeach.
Search and Data Mining, pp. 690–698, 2020. URL
https://arxiv.org/abs/1910.08381.
B.FurtherAnalysesof MAGDI
Yin,D.,Brahman,F.,Ravichander,A.,Chandu,K.,Chang,
K.-W.,Choi,Y.,andLin,B.Y. Lumos: Learningagents
MAGDIscalespositivelywithbetterbasemodels. In
withunifieddata,modulardesign,andopen-sourcellms.
Table 5, we show MAGDI’s scaling trends on Strate-
gyQA,CommonsenseQA,GSM8KandMATH.Herewe
arXivpreprintarXiv:2311.05657,2023. URLhttps:
//arxiv.org/abs/2311.05657.
applyMAGDItothreebasemodels: Mistral-7B-Instruct,
LLaMA-2-7B-Chat,andLLaMA-2-13B-Chat. Acrossall
You, S., Xu, C., Xu, C., and Tao, D. Learning from thesebenchmarks,MAGDIdemonstratesconsistentgains
multipleteachernetworks. InProceedingsofthe23rd ontopofallbasemodels.
12
ycaruccA
55.46
63.36
68.76
72.12 74.24
39.2
46.85
48.68
51.62 52.27MADGI:StructuredDistillationofMulti-AgentInteractionGraphs
Table5.Results of MAGDI with different base student mod- Table7.Removingdenserinteractiongraphs(e.g.,G 3orG 2)from
els (LLaMA-2-7B-Chat, LLaMA-2-13B-Chat, and Mistral-7B- CSQAtrainingcorpusleadstoasignificantdropinstudentaccu-
Instruct).First,acrossallbenchmarksandforallthreebasemod- racy,highlightingtheimportanceoflearningfromsuchgraphs.
els,MAGDIoutperformsnoteacherbaselinesbyalargemargin.
Second, the effect of structured distillation correlates with the TrainingMAGs Accuracy
performanceofthebasemodel(Mistral-7B>LLaMA-2-13B> All 72.56
LLaMA-2-7B),highlightingthescalingpropertiesofMAGDI. w/oG 70.65
3
w/oG &G 69.65
2 3
Base Distilled StrategyQA CSQA GSM8K MATH
NoTeacher 51.53 46.81 18.60 2.50
LLaMA-2-7B
MAGDi 66.81 66.73 28.36 5.76
C.BenchmarkLicenses
NoTeacher 58.52 51.73 31.77 3.90
LLaMA-2-13B
MAGDi 69.00 67.05 40.56 6.46
ThelicensesofthesevendatasetsweusedareinTable6.
NoTeacher 61.57 57.89 44.05 7.02
Mistral-7B
MAGDi 74.24 72.56 52.27 12.76
D.QualitativeExamplesofMAG
Table6. Datasetlicenses
SeeFig.7andFig.8fortwoexamplesofMAGs.
Dataset License
StrategyQA MITLicense(License)
CommonsenseQA MITLicense(License)
ARC-c CCBY-SA4.0(License)
GSM8K MITLicense(License)
MATH MITLicense(License)
BooQ CCBY-SA3.0(License)
SVAMP MITLicense(License)
MAGDI scales positively with the amount of training
data. Next,weanalyzethescalingpropertiesofMAGDI
byvaryingtheamountoftrainingdata. Wetraindistilled
modelswithMAGDI(usingMistral-7B-Instructasthebase
model)byvaryingtheamountoftrainingdatafrom100to
1000samples. Figure6showsthatwithmoredata,MAGDI
exhibitsbetterperformance–e.g.,onStrategyQA,training
on1KMAGsimprovesreasoningperformanceby10.88%
compared to training on 500 samples. This suggests that
MAGDImaybringadditionalimprovementswithalarger
trainingcorpus.
Dense interaction graphs distill significant knowledge
to students. Recall that the MAGs in our corpus have
distinctstructures,withG beingthedensestgraph(having
3
themostnodesandedges). Weshowthatremovingthese
G graphs from our training corpus leads to a significant
3
dropinstudentaccuracy. AsshowninTable7,structured
distillationonCSQAwithouttheG graphscausesstudent
3
performancetodropby2%andadditionallyremovingthe
G graphs causes a further drop of 1%. Our result thus
2
highlightstheimportanceofdistillationfromdensergraphs
(eveniftheyaresparselyrepresentedinthecorpus).
13MADGI:StructuredDistillationofMulti-AgentInteractionGraphs
Table8.Trainingandteststatisticsforfivebenchmarks. Ourtrainingcorpus(i.e.,MAGs)iscategorizedalongthreedimensions: (1)
Round:numberofnodesbelongingtoeachinteractionround,(2)Agent:numberofnodesbelongingtoeachagent(GPT4/Bard/Claude2),
and(3)Graph:numberofgraphswithaspecificgraphstructure.
Train
Task Test
Round(0/1/2/3) Agent(Each) Graph(G /G /G /G /All)
0 1 2 3
StrategyQA 3K/843/438/102 1.4K 719/135/112/34/1K 229
CSQA 3K/2.1K/942/627 2.2K 306/380/105/209/1K 2.2K
ARC-c 3K/792/288/153 1.4K 736/168/45/51/1K 1.1K
GSM8K 3K/1.3K/588/354 1.7K 557/247/78/118/1K 1.3K
MATH 3K/2.3K/1.5K/1.2K 2.7K 215/269/89/427/1K 5K
Table9.Comparisonbetweentask-specificMAGDImodelsandthejointmulti-taskMAGDImodel(MAGDI-MT).Onaverage,MAGDI-
MT’saccuracyiswithin1%oftask-specificmodels.
DistilledModel StrategyQA CSQA ARC-c GSM8K MATH AverageAccuracy
MAGDI-MT 70.74 71.73 72.87 49.51 10.76 55.12
MAGDI 74.24 72.56 72.61 52.27 12.76 56.89
Multi-Agent Interaction Graph (MAG) for ARC-Challenge
The majority of the fresh water on Earth is frozen in glaciers and ice caps. If the climate changed around the world, causing glaciers and ice caps to melt, which situation would most
likely occur? (A) Land would become more fertile. (B) Air temperatures would decrease. (C) Ocean water would become saltier. (D) Land masses would become smaller. (Gold Answer: D)
Round 0 Round 0 Round 0
If glaciers and ice caps melted, a large amount of fresh
water would be released into the oceans and other
If the glaciers and ice caps were to melt due to a change bodies of water. Land would not necessarily become The majority of the fresh water on Earth is frozen in
in the global climate, the excess water would ultimately more fertile, since the release of glacial water could cause glaciers and ice caps. If the climate changed around the
flow into the oceans, causing them to rise. As the water flooding and soil erosion in some areas. Air temperatures world, causing glaciers and ice caps to melt, the sea level
level increases, it would engulf lower-lying land masses, would likely increase due to the loss of reflective ice would rise. This would cause land masses to become
making them shrink in size or disappear entirely, causing surfaces and changes in ocean currents. Land masses smaller, as they would be covered by more water.
land masses to appear smaller. So the answer is D. would not change in size significantly. Therefore, option So the answer is D.
C, ocean water would become less salty, is the most likely
outcome. So the answer is C.
Round 1 Round 1 Round 1
v i mnI of c s elt u e oh c ltm ba rle ii r n m e e leg e gc c al vo o ta gt ,ef m le c e lal awi , se ec s cc sa hr it a fs eta h sle u o r aa n e sr s o ln g tii a wdn yd ne d ,g e o , ti n dd c ui h s ot ie le o ta d tw i m o c n o mo ca ndc e aup o e o uhls l rd a a f ese a n n ef .nr rr s de Seco t.s he osu mTu h e n h sl ta t hd m w oi ssi e n cs aat w e eh t ala l es ae o e nn r n u n r s w . t wi l e wn od So a ec c ar trl rrl ee th ed t e ina e sha rm ad es Do tr e oe it cc .o o l i eot b n C aa a d et n s chu r it si o fee s t re m o ot mo i en cI al p aga u btarg hl sg r ea ii tr e snc cie a i o ge we v l m r o s osw s uel eu ui a bat lm fh dn m ll o d eet ce oh v ai o r dece usfe l ei s sa of de rcg nte a o Se l bs aop on h yr nfs it s td ts lw hm e o h t a em. w eh e t T -a ael a rt lh it nsyre s i si sis sd w n ne we gwo gsdl ee u ou tl rsc a ol ue edt in se l as dt d f hd s Do l o rl m ( .ea ic sw na on l ai k dms si Cnw is nta t e oe o i t s sser . if t i zlc h O nc o eh eh co vo a o a d eoi sn rc ric r ng e t ee t hge i ca mD e t, n a )y. a e .s n I ,,f d pI af ce a n i ot n lodh s uu p oe lni dl c e ic d ne ll a ei a c m c t rn aea e dda p alt o sse tc oe w a tc o u c- th l hs hya m i e anin n e ng rg g Sil g ste b o, ekcd it o sl to h l a hia io fe nr s e o n st s ota asu e ol c nn oa r ea sd m fr al w e e d n t esv ah o ue rcse l , r ull i a g sw dw r r e r Dsio eo s s r n .p i u nl ad tll a n sd d, c d ac ar i n nia m fs lg du e oa s . om mg i Tn dei ahg l i.l n ri i is Iog g nt nwl e,wa s aoc lo inoi u fe u edflr ld .d s
Figure7. AnexampleofaMAGfromtheARC-cdataset.
14MADGI:StructuredDistillationofMulti-AgentInteractionGraphs
Multi-Agent Interaction Graph (MAG) for GSM8K
The profit from a business transaction is shared among 2 business partners, Mike and Johnson in the ratio 2:5 respectively.
If Johnson got $2500, how much will Mike have after spending some of his share on a shirt that costs $200?(Gold Answer: 800)
Round 0 Round 0 Round 0
MIf M M iJ h kto ih ei ikk sh e e e isn s ''r hs ss a l a o est s ri fhn h eo ta a ,r wor e ri eet inc t he m ii ass i v $e $ sre e 1a h5d n p 0i0 r s 0r t0$ e , 0*e 2 s a 2a5 e -n c0 n = $dh0 t 2 e $ t p 0hw d 1 0a a h 0r b t =ti 0 c y c0i h s $o2 . 8s$c Bp t 0o 2 ua w 0r 5 tr r 0 ae t aMs s0s f tp / i i $ en k5o 2 re tn = 0 bh sd 0 ue p$s . y e5 g Tt in0 no i hv t0 g e e5 . s r tn o eS hp m fi r ea n oa r ec rt st eie s ho o , i o , f r tf . MT HJoh Mi is ke h ei n r kr a sa e sto pt 'i si ono e r n =g a =b tt o e $ i $$ott 2 12w $ 5 = 20e 2 0 20e 5 $0 9 n 0 5 o / 0 0 ln M e5. 0 f a S t0i =k o s ae x h f $' t tis (5h er2 ts 0e r/ . h (0 sS2ta p0o o+r . ete 5a MM n )la d) i ipn k=k ind r ee o g$ 'J wsf 5i o ot is 0 lh nh= l0 n a h t0Js hr a eo o x ev n h e = 2 s' n s h/ $Ts 7 i1s o o r h t4=n t .a a 2' s $r l 9 e p 1s -h 4i r s o $a 2 f2 2r 9ie t: 0.5 x 0/. sT ph ege $ ne 2 dtp s 5 ir no 02 gf 0 i p t , $ a ti 2s r ht 0 es s 0h na a on Mr nde i d k aJ e oi sn hh g int o rh ts t $,e o 2 6M nr / 5a 7 ig 7kt i e * .eo 1 t $ 4ws2 2 .: i5 l5 5 l . 0p h T 0a a h r v=ti ess . $ $m I 8f 8 5e J 57a o 7n .h .1 1s n 4 4t s .h o -Aa n $ft t 2gM e 0o ri 0tk e =
Round 1 Round 1 Round 1
T $ bh g 52 ee 0e:5 0t 2p , s . * r r 2 S 5o e i 0f s a ni p 0t n c e i ed =s c w Mts J $ih o iv 1 lia le h k 0r hel ne y 0 a s.d g 0 o vT . e en b h At e si $gs f t 1t 2ew em 0 t rpe s 0e Me a 05a rn n . t i - k ss OM $e, t 2 n hi h sk 0e i pa se 0 t ep s a nf h =aon dar d r t $r s e e 8 oJ $ v 0fo o 2 e 0th f 0hr n y t l0e eh s 7 fe fo p to .n p r p ro a ri afn or i t t sfs t i heh t, iqe M w rtu ,or i aka hulet s eli d o T 2 Mph : r5 e ie r= ko. c e p eT f( i5 r i th svo )/ pi .7 esf eMi ) dt m n * i i$ t s ke x 2 e $as . , ' 2h n 5M s 0a s 0 o ir 00kie rf e i, $d og t ' sh 1n s iob ne , 4s a e a (ht 2 5 lt so aw 9/ s ht r 7 h e a e i -r) a e l t $ = * r .pn e 2S x r ( 0M o 2 w o= 0f / ai i t7 k t =h$ s e )i e2 s (* $2a , x a5 1x /n , m70. , d 2t )0W h o 2*J ,e u 9 eo x $n n h 5k = tn J n , M0os $oo 0h 5w ik0n n , e0 s J =i 0on o h 0 n $h at 'h 1n s s ( e ,s t l4s o eo hr 2 ftna ta a 9 t r l ii . e so T r $h eb 2e su 5p p s 0eir 0cno t ef *i Mi vs t 2es if k lr p /y eo .a 7 m I wr ft = in Ja l le o $ b hr h 1s au n 0, vs s 0Mi eon 0 ne i $.k s 1Ae gs 0 f oat t 0tr en 0 a r$d n -2 s s J p $5a o e 20c h n 00t ni d 0,o s itn o n =h n gei s $ n i $ 8s n 2h M 0 t 0a 0h ir 0k ee le e od r ' fsa n ta .t sim ao h a s o 2 hrn e: i5g r ti ,s2
Round 2 Round 2 Round 2
The profit is shared among Mike and Johnson in a ratio of
2:5. This means that for every 2 parts that Mike receives, The profit from a business transaction is shared among 2 Based on the information provided, I agree with the
Johnson receives 5 parts of the profits. Therefore, each business partners, Mike and Johnson in the ratio 2:5 solutions stating the answer is $800. The reasoning stating
part of the profits is $2500 divided by 5, which gives $500. respectively. If Johnson got $2500, then Mike's share is the answer is $1,229 appears to incorrectly calculate the
Since Mike is to receive 2 parts, he initially receives 2*$500, $2500 * 2 / 7 = $1000. After spending some of his share on total profit amount rather than using the per share amount
which gives $1000. If he spends $200 on a shirt, then he a shirt that costs $200, Mike will have 1000 - 200 = 800 left. implied by Johnson's $2,500.
will have $1000 - $200 left, which gives $800.
Figure8. AnexampleofaMAGfromtheGSM8Kdataset.
15