Boximator: Generating Rich and Controllable Motions for Video Synthesis
JiaweiWang∗ YuchenZhang∗ JiaxinZou
YanZeng GuoqiangWei LipingYuan HangLi
ByteDanceResearch
∗ Equal Contribution
{wangjiawei.424, zhangyuchen.zyc, zoujiaxin.zjx,
zengyan.yanne, weiguoqiang.9, yuanliping.0o0 lihang.lh}@bytedance.com
https://boximator.github.io
Abstract ofconstraints: hardbox,whichpreciselydelineatesanob-
ject’s bounding box, and soft box, defining a broader re-
Generating rich and controllable motion is a pivotal gionwithinwhichtheobjectmustreside. Thesoftboxcan
challengeinvideosynthesis. WeproposeBoximator,anew be as tight as the object’s exact bounding box, or as loose
approachforfine-grainedmotioncontrol. Boximatorintro- astheframeboundary. Wecontrolmultipleobjectsacross
duces two constraint types: hard box and soft box. Users frames by associating unique object IDs with these boxes.
select objects in the conditional frame using hard boxes Ourproposedmethod,namedBoximator(combining“box”
and then use either type of boxes to roughly or rigorously and“animator”),offersseveralbenefits:
define the object’s position, shape, or motion path in fu- 1. Boximator serves as a flexible motion control tool. It
ture frames. Boximator functions as a plug-in for existing managesthemotionofbothforegroundandbackground
video diffusion models. Its training process preserves the objects, as well as modifies the pose of larger objects
base model’s knowledge by freezing the original weights (e.g.,human)byadjustingsmallercomponents. Referto
and training only the control module. To address train- Figure1forillustrations.
ing challenges, we introduce a novel self-tracking tech-
2. In scenarios where generation is conditioned on an im-
niquethatgreatlysimplifiesthelearningofbox-objectcor-
age,asseeninimage-to-videoandmanystate-of-the-art
relations. Empirically, Boximatorachievesstate-of-the-art
text-to-videomethods[9,41],userscaneasilyselectob-
videoquality(FVD)scores,improvingontwobasemodels,
jectsbydrawinghardboxesaroundthem. Thisvisually-
and further enhanced after incorporating box constraints.
groundedapproachismorestraightforwardcomparedto
Its robust motion controllability is validated by drastic in-
the language-grounded controls [15, 21], which require
creasesintheboundingboxalignmentmetric.Humaneval-
verbaldescriptionsforallobjects.
uation also shows that users favor Boximator generation
resultsoverthebasemodel. 3. For frames lacking user-defined boxes, Boximator al-
lows approximate motion path control via algorithm-
generated soft boxes. These soft boxes can be con-
1.Introduction structedbasedonapairofuser-specifiedboxes,orbased
on a hard box combined with a user-specified motion
Video synthesis has recently experienced remarkable ad- path. SeeFigure1(c)-(e)forexamplesofuser-specified
vancements [8, 9, 13, 14, 16, 26]. These models typi- motionpaths.
cally utilize either a text prompt or a key frame to gener- Boximatorfunctionsasaplug-inforexistingvideodiffu-
atevideos. Recentresearchfocusesonenhancingthecon- sionmodels. Weencodeeveryboxconstraintbyfourcoor-
trollabilitybyincorporatingframe-levelconstraints,suchas dinates,anobjectID,andahard/softflag. Duringtraining,
sketches,depthmaps[11,34],humanposes[7,32,38],tra- wefreezethebasemodel’stextencoderandU-Net,feeding
jectories[35,40],andconditionalimages[4,23,41]. theboxencodingthroughanewtypeofself-attentionlayer.
In this work, we introduce a novel approach utilizing This design is inspired by GLIGEN [17], where bounding
box-shaped constraints as a universal mechanism for fine- boxesarecombinedwithobjectdescriptiontextstoachieve
grained motion control. Our method introduces two types region control for image synthesis. However, Boximator
4202
beF
2
]VC.sc[
1v66510.2042:viXraFigure1. MotioncontrolwithBoximator: (a)usehardboxestocontroltheendingshapeandpositionofajumpingcat;(b)forcecamera
rotatingtotheleftbypushingbedandwindowtotheright;(c)controlhowapersonraisesacupofcoffee;(d)controlthemotionpathof
adogandaball;(e)Usemotionpathandhardboxestocontrolthetrajectoryandproximityoftwoballoons. Inallfigures,dottedboxes
representfirstframeconstraints;solid-lineboxesrepresentlastframeconstraints;Arrowedlinesrepresentmotionpaths. Examplevideos
areinitiallygeneratedas256x256x16,thenenhancedto768x768x16viaapretrainedsuper-resolutionmodel.
aims to control object motions without relying on textual boxesarenolongervisuallypresent,theirinternalrepresen-
grounding,thusrequiresthelearningofbox-objectcorrela- tationpersists,enablingthemodeltocontinuealigningwith
tionpurelyfromvisualinputs. Boximatorconstraints.
Empirically,wefindthatitishardforthemodeltolearn We developed an automatic data annotation pipeline to
this visual correlation through standard optimization. To generate 1.1M highly dynamic video clips with 2.2M an-
mitigatethischallenge,weintroduceannoveltrainingtech- notatedobjectsfromtheWebVid-10Mdataset[1]. Weuti-
niquetermedself-tracking. Thistechniquetrainsthemodel lizedthisdatasettotrainourBoximatormodelontwobase
to generate colored bounding boxes as a part of the video. models: the PixelDance model [41] and the open sourced
Itsimplifiesthechallengeintotwoeasiertasks: (1)produc- ModelScopemodel[31]. Extensiveexperimentsshowthat
ingaboundingboxforeachobjectwiththerightcolor,and Boximatorretainstheoriginalvideoqualityofthesemodels
(2) aligning these boxes with the Boximator constraints in while offering robust motion control in diverse real-world
every frame. We observe that video diffusion models can scenarios. OntheMSR-VTTdataset, Boximatorimproves
quickly master these tasks. After that, we train the model upon the base models in FVD score. With box constraints
tostopgeneratingvisibleboundingboxes. Althoughthese added, video quality significantly improved further (Pixel-Dance: 237 → 174, ModelScope: 239 → 216), and the Frame-levelVisualTokens
object detector’s average precision (AP) score, measuring
box-objectalignment, sawaremarkableincrease(1.9-3.7x SpatialCross-Attention CaptionTokens
higher on MSR-VTT and 4.4-8.9x higher on ActivityNet),
highlighting effective motion control. User study also fa-
vored our model’s video quality and motion control over
BoximatorSelf-Attention BoxCoordinates
the base model by large margins (+18% for video quality,
+74% for motion control). Furthermore, ablation studies MLP ObjectID
confirmthenecessityofintroducingsoftboxesandtraining
withself-trackingforachievingtheseresults. SpatialSelf-Attention Hard/SoftFlag
VisualToken
Frame-levelVisualTokens
2.RelatedWork ControlToken(perobject)
Figure 2. Overview of the control module: adding a new self-
Videodiffusionmodelsarenaturalextensionsofimagedif-
attentionlayertoeveryspatialattentionblock,betweenthespatial
fusion models. They extend the U-Net architecture from
self-attentionandthespatialcrossattention. Duringtraining, all
imagemodelsbyaddingtemporallayers[13,26]. Awidely
theoriginalmodelparametersarefrozen.
adopted method for improving computational efficiency is
todenoieinthelatentspace[12,43]. Text-to-video(T2V) 3.Background: VideoDiffusionModel
diffusionmodelsareoftenthefoundationforvariousforms
of conditional generation [2, 8, 31]. Recent advancements Boximator is built on top of video diffusion models [14]
suggest a two-step approach to T2V: initially creating an using the 3D U-Net architecture [24]. These models itera-
image based on text, followed by producing a video that tively predict the noise vector in noisy video inputs, grad-
considers both the text and the pre-generated image. This ually transforming pure Gaussian noise into high-quality
approachallowsthevideomodeltoconcentrateondynamic videoframes. TheU-Net,denotedbyϵ θ,processesanoisy
aspects by using a static image as a reference, leading to inputz (eitherinpixelspaceorlatentspace), alongwitha
improved video quality [9, 33, 41]. The reference image timestamptandvariousconditionsc,andpredictsthenoise
providesanaturalgroundingsourceformotioncontrol. in z. Optimization is achieved through a noise prediction
There is a surge in research focused on enhancing loss:
the controllability of T2V and I2V models. VideoCom- L θ =E z0,c,ϵ,t[∥ϵ−ϵ θ(z t,t,c)∥2 2],
poser[34]enablesconditionssuchassketches,depthmaps,
wherez representsthegroundtruthvideo,ϵisaGaussian
0
and motion vectors. In producing dance videos, hu-
noisevector,andz isanoisilytransformedversionofz :
t 0
man poses extracted from reference videos are commonly
√ √
used [7, 32, 38]. For more precise motion control, users z = α¯ z + 1−α¯ ϵ.
t t 0 t
can plot object or camera trajectories [35, 40]. However,
these methods did not provide a precise way to define ob- Here,α¯ denotesapredefinedconstantsequence.
t
jects, making it challenging to select and control a larger The 3D U-Net is structured with alternating convolu-
orcompositeobjectfromimage. Moreover,trajectorydoes tion blocks and attention blocks. Each block comprises
notcapturetheobject’sshapeandsize,crucialfordepicting twocomponents: aspatialcomponent,handlingindividual
poseorproximitychangeslikearmspreadingorapproach- video frames as separate images, and a temporal compo-
ingmovements. nent,enablinginformationexchangeacrossframes. Within
There are two concurrent research studying the use of every attention block, the spatial component typically in-
bounding boxes for motion control, but it should be noted cludes a self-attention layer, followed by a cross-attention
that their work differs from ours in key aspects. Trail- layer, the latter used for conditioning the generation on a
Blazer[21]isatraining-freemethodthatleveragesattention textprompt.
map edits to direct the model in generating a specific ob-
jectwithinadesignatedarea. Theobjectmustbedescribed 4.Boximator: Box-guidedMotionControl
in the text prompt. FACTOR [15] modified a transformer-
4.1.ModelArchitecture
basedgenerationmodel,Phenaki[30],byaddingaboxcon-
trolmodule. LikeTrailBlazer,FACTORrequiresatextde- Our objective is to endow existing video diffusion mod-
scriptionforeachbox,thusdoesnotsupportvisualground- elswithmotioncontrolcapabilities. Giventhatfoundation
ing. Neither of the above methods supports soft box con- modelsarepre-trainedonextensivecollectionsofweb-scale
straints,nordotheystudytheassociatedchallengesintrain- images and videos, it’s crucial to preserve their acquired
ing. knowledge. To achieve this, we freeze the original modelparametersandsolelyfocusontrainingthenewlyincorpo-
ratedmotioncontrolmodule.
The architecture of our model is illustrated in Figure 2.
In every spatial attention block of video diffusion mod-
els, there are two stacked attention layers: a spatial self-
attentionlayerandaspatialcross-attentionlayer. Weaug-
mentthisstackbyaddinganewself-attentionlayer.Specif-
ically, if v denotes the visual tokens of a frame, and h
text
andh representtheembeddingsofthetextpromptand
box
the box constraints, respectively, then the modified spatial Figure3. Trainingdata: allboundingboxesareprojectedtothe
croppedregion(whitedashedbox).
attentionblockisdescribedasfollows:
v =v+SelfAttn(v)
v =v+TS(SelfAttn([v,h ]))
box
v =v+CrossAttn(v,h )
text
where TS(·) is a token selection operation that exclusively
considersvisualtokens. Theboxembeddingsh isase-
box
quenceofcontroltokens. Eachtokenrepresentsaboxand
isdefinedby: Figure4.Self-tracking:trainthemodeltotrackeveryconstrained
object. Thisfigureshows3frameswheretheblackhorseandthe
t =MLP(Fourier([b ,b ,b ])).
b loc id flag yellowboxsurroundingitaregeneratedtogether.
Here, b isa4-dimensionalvectorencapsulatingthetop-
loc ”young man” or ”white shirt,” served as object prompts.
leftandbottom-rightcoordinatesofthebox,normalizedto
We then feed these prompts to a pre-trained grounding
the range [0,1]. The object ID, used to link boxes across
model [20] and object tracker [5] to generate bounding
frames, is represented by b , which in our experiments is
id boxes and populate them across all frames of the video.
expressed in RGB space. Each object thus corresponds to
This approach successfully yielded bounding boxes for a
aunique“color”foritsboxes,makingb a3-dimensional
id totalof2.4Mobjects.
vectornormalizedto[0,1]. Theb isabooleanindicator:
flag
During training, we take a random crop of the video,
it is set to 1 for hard boxes and 0 otherwise. These three
conforming to the specified target aspect ratio, and subse-
inputsareconcatenatedandprocessedviaaFourierembed-
quently project all bounding boxes onto this cropped re-
ding[22]followedbyamulti-layerperceptron(MLP).Note
gion (Figure 3). If a bounding box entirely fall outside
h contains a fixed number of control tokens (indicated
box
thecroppedarea,thenweprojectitaslinesegmentsalong
byN).WhentheactualnumberofboxesissmallerthanN,
the border of the crop. This allows users to control object
weusealearnablet topadtheemptyslots.
null
movementsbothintoandoutoftheframebydrawingline
4.2.DataPipeline segmentsontheframe’sborder(SeeFigure6(d)foranex-
ample).
Intheabsenceofapubliclyavailablevideodatasetwithob-
ject tracking annotations, we curated our training set from 4.3.Self-Tracking
the WebVid-10M dataset [1]. Through empirical analysis,
wefindthatavastmajorityofWebVidvideosdonotexhibit
substantial object or camera movements. Consequently, A significant challenge in video motion control lies in
samplingfromthiscollectionwouldbeinefficientfortrain- associating box coordinates with objects and maintaining
ingourmotioncontrolmodule.Toaddressthisissue,wecu- temporal consistency across frames, namely making sure
rated a more dynamic subset from WebVid. This involved that the same group of boxes always control the same ob-
evaluating every clip in the dataset, comparing their start ject. Inpractice,thisprovestobechallenging,asdiffusion
and end frames, and retaining only those clips where the modelsoftenstruggletoeffectivelylinkdiscretecontrolsig-
twoframesaresufficientlydifferent. Thisfiltrationyielded nals, like coordinates and IDs, with visual elements. This
atotalof1.1Mvideoclips. difficulty is exacerbated when the video contains multiple
For every clip in our refined dataset, we took the first overlapping boxes. As Section 5.4 shows, with traditional
frame to generate an image description using a visual lan- lossoptimization,themodelfailedtoaligntomostboxcon-
guage model [19]. Then we extract noun chunks from straintsafter110Kstepsoftraining.
thesedescriptions. Thesechunks,encompassingtermslike We propose self-tracking as a simple technique to miti-gatethischallenge. Wetrainourmodeltogeneratecolored
boundingboxesforeachconstrainedobjectineveryframe,
withcolorsspecifiedintheobject’scontroltoken(Figure4).
In other words, we train the model to perform generation
and object tracking at the same time. This approach sim-
plifies the problem into two easier tasks: (1) generating a
bounding box for each object with the right color and (2)
aligning these boxes with the Boximator constraints in ev-
eryframe. Previousresearchinimagesynthesis[25]shows
thatdiffusionmodelscangenerateboundingboxes.Wefur-
ther discover that diffusion models can maintain temporal
consistency, ensuring that boxes of the same color consis- Figure5. Softboxesininference. Weinterpolatesoftboxesand
tently track the same object across frames. With this ca- relaxthembasedonapairofuser-specifiedboxes(upperrow),or
pability, task (2) becomes straightforward. For hard box auser-specifiedboxcombinedwithamotionpath(lowerrow).
constraints, themodelonlyneedstoputboxesatthespec-
ifiedcoordinates,whileforsoftboxconstraints,itneedsto polated boxes by expanding the box regions (as described
putthemwithinaspecifiedregion. Intuitively,self-tracked in Section 4.4) and marking them as “soft box”. This ap-
boundingboxesactasanintermediaryrepresentation. The proachensuresthattheobjectroughlyfollowstheintended
model follows Boximator constraints to guide the genera- trajectory, while simultaneously offering the model suffi-
tion of these boxes, which in turn guide the generation of cient flexibility to introduce variations. In cases where a
objects. userdrawsahardboxinaframeanddefinesamotionpath
Upon completing the self-tracking training phase, we forit,welettheboxtoslidealongthepathtoconstructin-
proceed to further train the model using the same dataset, terpolatedboxesforeachsubsequentframe,thenrelaxthem
but excluding bounding boxes from the target frames. Re- to form soft box constraints. Figure 5 presents a visual il-
markably, themodelquicklylearntoceasegeneratingvis- lustrationfortheconstructionofsoftboxesinbothcases.
ibleboundingboxes, butitsboxalignmentabilitypersists.
5.Experiments
Thisindicatesthattheself-trackingphaseassiststhemodel
todevelopanappropriateinternalrepresentation.
5.1.ExperimentSettings
4.4.Multi-StageTrainingProcedure Base models We train Boximator on two base models:
PixelDance [41] and ModelScope [31]. Our experiments
We employ a multi-stage training procedure. Initially, in
use text prompts, box constraints, and optionally the first
Stage1,themodelistrainedusingalltheprovidedground
video frame as input conditions. PixelDance can directly
truth bounding boxes as hard box constraints. Since hard
use the first frame as an input condition. ModelScope
boxcontrolsareeasiertolearnthanthesoftones,thisstage
doesn’t support direct image input, but we can still condi-
servesasapreliminaryphase,establishingthemodel’sini-
tionitonanimagebyreplacingthefirstframeofthenoisy
tialunderstandingofcoordinatesandIDs. Subsequently,in
latents z with the ground-truth frame’s latents at each de-
Stage 2, we substitute 80% of these hard boxes with soft
noising step. In both cases, we freeze the original model
boxes. Thesoftboxesaregeneratedbyrandomlyandinde-
weightsandonlytrainthecontrolmodule. SeeAppendixA
pendently expanding the hard ones in four directions: left,
formoretrainingandinferencedetails.
right, up, and down. The expansion margin for each di-
rection is determined by a Beta(1,8) distribution, so that
theaverageexpansionis1/9oftheframe’swidthorheight, Datasets We test our models using the MSR-VTT [37],
whilethemaximumexpansioncanextenduptotheframe’s ActivityNet [3] and UCF-1011 [28] datasets. MSR-VTT
boundary. Both Stage 1 and Stage 2 use the self-tracking test set consists of 2,990 samples with 20 prompts per ex-
technique outlined in Section 4.3. Finally, in Stage 3, we ample. Fortextconstraint,following[15,41],werandomly
continuetheStage2trainingbutwithoutself-tracking. select one prompt per sample to generate one video. For
box constraint, MSR-VTT does not include bounding box
4.5.Inference annotations,soweautomaticallycreatereference(ground-
truth)boundingboxes. First,weidentifynounchunksfrom
During the inference stage, only a select few frames (such
thetextprompt. Then,weuseGroundingDINO[20]toget
asthefirstandlast)containuser-definedboxes. Toachieve
boundingboxesonthefirstframeandDEVA[5]toextend
robustcontrol,weinsertsoftboxestotheotherframes.This
theseboxestosubsequentframes.
isdonebyfirstapplyinglinearinterpolationofuser-defined
boxes to those empty frames, and then relaxing the inter- 1TheUCF-101datasetdetailsandresultsarediscussedinAppendixBConsidering that the automatic annotations on MSR- A similar observation was reported in the FACTOR pa-
VTTmaybenoisy,toincreasethecredibilityofourresults, per[15].
wemanuallyannotatedaportionoftheActivityNetvalida-
tionset. Specifically,wechose796videoclipsthatinclude Motion Control Precision Table 1 also presents the re-
noticeable object motion. The bounding boxes in the first sultsformotioncontrolprecision.Ineverycase,addingbox
frame have already been annotated by the ActivityNet En- constraints(Box)significantlyimprovestheaveragepreci-
titiesdataset[44],andwemanuallyextendedthebounding sion (AP) scores. This indicates that the model effectively
boxannotationstoall16frames. understandsandappliestheboxconstraints. TheFACTOR
paper[15]reportedthemAPscoreonMSR-VTTtoo. Al-
though our results aren’t directly comparable to theirs due
Evaluation metrics We measure video quality using
to differences in object annotations, we’ve included their
Fre´chetVideoDistance(FVD)[29]andmeasuretextalign-
number(markedwith∗)inTable1forreference.
ment using CLIP similarity score (CLIPSIM) [36]. We
Table 2 presents the results on ActivityNet. We inten-
compute the FVD metrics using the randomly selected 16
tionallychosetestvideosfromActivityNetthatfeaturesig-
frames of each ground truth video with an FPS of 4. For
nificantobjectmovements. Asaresult,thedisparityinAP
evaluating motion control, we use the average precision
scoresbeforeandafteraddingboxconstraintsiswidercom-
(AP) metric. We generate videos with ground-truth boxes
pared to MSR-VTT. The mAP scores with box constraints
on the first and last frames as constraints. After creating a
are4.4-8.9xhigherthanthatwithoutboxonActivityNet,in
video, we detect bounding boxes with the aforementioned
contrastto1.9-3.7xhigheronMSR-VTT.
DINO+DEVAdetectionsystem. Ifanobjectisconsistently
It’s important to note that the AP scores in our experi-
trackedacrossallframes,wecompareitsdetectedbounding
ments are not equal to success rate in motion control. To
boxwiththegroundtruthboxesonthefirst/lastframe. AP
calculate AP, we compare the reference object boxes with
iscalculatedfollowingtheMSCOCOprotocol[18]. When
thosegeneratedbythevideosynthesismodelanddetected
thefirstframeisagivencondition,weonlycompareboxes
by the DINO+DEVA system. This detector isn’t flawless;
on the last frame. We also report mean average precision
itmightmissobjects,detectirrelevantones,orfailtotrack
(mAP), calculated as the average AP over 10 Intersection
anobjectconsistentlyacrossallframes. Thesepotentialer-
overUnion(IoU)thresholds,from0.5to0.95.
rorsindetectioncanimpactthefinalAPscore. Therefore,
it’smoreinsightfultofocusonthedifferenceinAPscores
5.2.QuantitativeEvaluation
betweenmethods,ratherthantheabsolutevalues.
VideoQuality Table1comparesourmodelswithrecent
5.3.HumanEvaluation
video synthesis models on the MSR-VTT dataset. In text-
to-video synthesis, our Boximator model outperforms the We conducted a user preference study with four human
basemodels,achievingcompetitiveFVDscoresof237and raters on 100 samples. In each session, they were shown
239 with PixelDance and ModelScope, respectively. This two videos in a random order: one generated by the base
improvement, despite using frozen base model weights, is model(PixelDance),whichusesatextpromptandthefirst
probably due to the control module’s training on motion frame as input, and the other by the Boximator model,
data,enhancingdynamicscenehandling. which additionally uses box constraints. The raters were
The results in Table 1 indicates that the FVD score im- asked to evaluate their preference based on video quality
proveswhenextraconditionsareaddedtotheinput.Specif- andmotioncontrol. Detailedcriteriaforevaluationarepre-
ically, the introduction of box constraints (Box) enhances sentedinAppendixC.AsindicatedinTable3,theBoxima-
video quality (PixelDance: 237 → 174; ModelScope: 239 tormodelwaspreferredbyasignificantmargin. Itexcelled
→ 216). We hypothesize this improvement is due to box in motion controls in 76% of the cases, outperformed by
constraintsprovidingamorerealisticlayoutforvideogen- the base model in only 2.2% of the cases. The Boxima-
eration. However,whenthegenerationisbasedonthefirst tormodel’svideoqualitywasalsofavored(+18.4%),likely
frame(F0),theimpactofboxconstraintsonFVDisreduced due to the dynamic and vivid content resulting from box
(PixelDance: 113→102;ModelScope: 142→132). This constraints. SeeAppendixCforsomesamplevideos.
mightbebecausethelayoutisalreadysetbyF0.
5.4.AblationStudy
OurmodelsachieveCLIPSIMscoresthatareonparwith
state-of-the-artsystems. WenoticedaslightdropinCLIP- Wecarryoutablationstudiestounderstandtheeffectofour
SIMscoreswhenadditionalconditions(likeF0orBox)are designchoices. Initially,weexcludeself-trackingfromour
introduced. This occurs because the base model is opti- training process. This means we train the model to pre-
mized for aligning video with the text alone, whereas our dicttheoriginalvideowithoutanyvisibleboundingboxes.
modelhandlesmultipletypesofalignmentatthesametime. We observe that omitting self-tracking greatly challengesModels ExtraInput FVD(↓) CLIPSIM(↑) mAP/AP /AP (↑)
50 75
MagicVideo[43] - 1290 - -
LVDM[12] - 742 0.2381 -
ModelScope[31] - 550 0.2930 -
Show-1[42] - 538 0.3072 -
PixelDance[41] - 381 0.3125 -
Phenaki[30] - 384 0.2870 -
FACTOR-traj[15] Box 317 0.2787 0.290∗/-/-
- 237 0.3039 0.094/0.193/0.076
Box 174 0.2947 0.349/0.479/0.359
PixelDance+Boximator
F0 113 0.2890 0.194/0.330/0.177
F0+Box 102 0.2874 0.365/0.521/0.384
- 239 0.3013 0.096/0.195/0.084
Box 216 0.2948 0.312/0.470/0.309
ModelScope+Boximator
F0 142 0.2865 0.141/0.260/0.126
F0+Box 132 0.2852 0.300/0.456/0.299
Table 1. Zero-shot results on MSR-VTT. F0 means given the first frame as condition. Box means box constraints. The results show
thatBoximatorretainsorimprovesthevideoquality(FVD)ofthebasemodels. Inallcases,addingboxconstraints(Box)significantly
improvestheaverageprecision(AP)scoreofboundingboxalignment.
BaseModels ExtraInput mAP/AP 50/AP 75(↑) Methods mAP(Box) mAP(F0+Box)
- 0.050/0.103/0.041 MSR-VTT
Box 0.445/0.638/0.459 PixelDance+Boximator 0.349 0.365
PixelDance
F0 0.079/0.165/0.072 w/oself-tracking 0.118 0.187
F0+Box 0.394/0.607/0.409 w/osoftboxes 0.235 0.274
w/ofreezingweights 0.354 0.343
- 0.054/0.118/0.040
Box 0.361/0.563/0.372 ActivityNet
ModelScope
F0 0.069/0.128/0.068 PixelDance+Boximator 0.445 0.394
F0+Box 0.304/0.522/0.291 w/oself-tracking 0.083 0.085
w/osoftboxes 0.248 0.220
w/ofreezingweights 0.404 0.331
Table2.BoxalignmentresultsonActivityNet.Inallcases,adding
boxconstraintssignificantlyimprovestheAPscore.
Table 4. Ablation study: removing self-tracking and soft boxes
bothresultinsignificantdropintheboxalignmentmetric. Train-
Criteria Boximatorwins Draw Basemodelwins
ingallmodelweightsdoesn’tgiveextrabenefits.
VideoQuality 35.2% 48.0% 16.8%
MotionControl 76.0% 21.8% 2.2%
movement directions. Without this guide, the model tends
Table3.Humanside-by-sideblindcomparisonon100samples.
tomakemoremistakes.
Finally, we examine the impact of freezing the base
model weights. For comparison, we trained a new model
themodel’sabilitytoassociatecontroltokenswiththecor-
in which all parameters of the U-Net were optimized. We
respondingobjects. AsshowninTable4, theaveragepre-
find that the new model generates videos of roughly the
cision(AP)underboxconstraintsfallsdrastically,reaching
same quality, resulting in similar FVD scores as the stan-
a level that is only slightly better than the AP without box
dard model in Table 1. When it comes to motion control
constraints.
precision,asshowninTable4,thisnewmodelscoredsim-
Next, we examine the role of using soft boxes during
ilarlyasthedefaultoneonMSR-VTT,andloweronActiv-
inference. Accordingtothestandardinferencemethodde-
ityNet. Insummary,ourresultssuggestthatit’snotneces-
scribedinSection4.5,weinsertrelaxedsoftboxesinframes
sarytotrainalltheU-Netparameters.
2-15, where the user does not specify any box constraints.
Table4indicatesthatremovingtheserelaxedsoftboxes(by
5.5.CaseStudy
replacing their control tokens with t ) leads to a signif-
null
icant decrease in average precision scores. We hypothe- Inthissection,wehighlightthemodel’scapabilityofhan-
size that the inserted soft boxes act as a rough guide for dling complex scenarios. Figure 6(a) demonstrates a gen-Figure6.Casestudy:(a)Generationandmotioncontrolbasedonfourboxes;(b)Amotionthataffectssignificantportionoftheframe;(c)
Boxdefinedonacombinationofobjects(e.g.,“amanonahorse”);(d)Addingnewobjectstothescene.
eration task based on four boxes. Boximator successfully segmentlinealongtheleftborder. Themodelsuccessfully
populates each box with the target object (a pig) as spec- directs a man entering from the left edge, stopping at the
ified in the text prompt. This contrasts with the second center.
row,wherethemodelwithoutboxconstraintonlyproduces
two pigs. Indeed, previous research has found that text-
6.Conclusion
conditioned diffusion models struggle with precise object
countcontrolwithoutboxconstraints[39].
We proposed Boximator, a genral approach to controlling
Figure 6(b) illustrates a dynamic scene where a baby is object motion in video synthesis. Boximator utilizes two
moved across the entire frame. The box has guided the typesofboxestoallowuserstoselectarbitraryobjectsand
model to generate the motion, which appeared to be chal- define their motions without entering extra text. It can be
lenging to generate without box constraints (see the next built on any video diffusion model without modifying the
row). Figure 6(c) highlights the generalizability of box- original model weights, thus its performance can improve
based visual grounding. Here, a user wants to control an with evolving base models. Additionally, we proposed a
object combination: a man on a horse. The model inter- self-tracking method that significantly simplifies the train-
prets this constraint, moving the composite object towards ing for the control module. We believe that our design
the frame’s left edge. Finally, Figure 6(d) showcases the choices and training techniques can be adapted to enable
model’s capability to introduce a new object into a scene. other forms of control, such as conditioning with human
The user indicates the entry point of a man by drawing a posesandkeypoints.EthicalandSocialRisks [8] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon,
AndrewTao,BryanCatanzaro,DavidJacobs,Jia-Bin
Video generation technologies, especially advanced
Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve
video diffusion models, carry potential ethical and so-
yourowncorrelation:Anoisepriorforvideodiffusion
cial risks. These include the creation of deepfakes,
models. In Proceedings of the IEEE/CVF Interna-
which can lead to misinformation and privacy viola-
tionalConferenceonComputerVision,pages22930–
tions; biases in AI-generated content, potentially leading
22941,2023. 1,3,12
to unfair or discriminatory outcomes; and impacts on
[9] Rohit Girdhar, Mannat Singh, Andrew Brown,
intellectual property and creative industries, possibly
Quentin Duval, Samaneh Azadi, Sai Saketh Ramb-
undermining the value of human creativity. It’s cru-
hatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan
cial for developers and users of these technologies to
Misra. Emu video: Factorizing text-to-video gener-
be aware of these risks and ensure their responsible use.
ation by explicit image conditioning. arXiv preprint
arXiv:2311.10709,2023. 1,3
[10] Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu,
References
Xing Zhang, Zuxuan Wu, Songcen Xu, Wei Zhang,
[1] Max Bain, Arsha Nagrani, Gu¨l Varol, and Andrew Yu-GangJiang,andHangXu. Reuseanddiffuse: It-
Zisserman. Frozen in time: A joint video and image erative denoising for text-to-video generation. arXiv
encoderforend-to-endretrieval.InProceedingsofthe preprintarXiv:2309.03549,2023. 12
IEEE/CVFInternationalConferenceonComputerVi- [11] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh
sion,pages1728–1738,2021. 2,4 Agrawala, Dahua Lin, and Bo Dai. Sparsectrl:
[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Addingsparsecontrolstotext-to-videodiffusionmod-
DanielMendelevitch,MaciejKilian,DominikLorenz, els. arXivpreprintarXiv:2311.16933,2023. 1
YamLevi, ZionEnglish, VikramVoleti, AdamLetts, [12] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan,
et al. Stable video diffusion: Scaling latent video and Qifeng Chen. Latent video diffusion models for
diffusion models to large datasets. arXiv preprint high-fidelity video generation with arbitrary lengths.
arXiv:2311.15127,2023. 3,12 arXivpreprintarXiv:2211.13221,2022. 3,7,12
[3] Fabian Caba Heilbron, Victor Escorcia, Bernard [13] Jonathan Ho, William Chan, Chitwan Saharia, Jay
Ghanem, and Juan Carlos Niebles. Activitynet: A Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P
large-scale video benchmark for human activity un- Kingma, Ben Poole, Mohammad Norouzi, David J
derstanding. InProceedingsoftheieeeconferenceon Fleet, et al. Imagen video: High definition video
computer vision and pattern recognition, pages 961– generation with diffusion models. arXiv preprint
970,2015. 5 arXiv:2210.02303,2022. 1,3
[4] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, [14] Jonathan Ho, Tim Salimans, Alexey Gritsenko,
Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, William Chan, Mohammad Norouzi, and David J
Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short- Fleet. Videodiffusionmodels. InAdvancesinNeural
to-long video diffusion model for generative transi- Information Processing Systems, pages 8633–8646.
tionandprediction. arXivpreprintarXiv:2310.20700, CurranAssociates,Inc.,2022. 1,3
2023. 1 [15] Hsin-Ping Huang, Yu-Chuan Su, Deqing Sun, Lu
[5] HoKeiCheng,SeoungWugOh,BrianPrice,Alexan- Jiang, Xuhui Jia, Yukun Zhu, and Ming-Hsuan
derSchwing,andJoon-YoungLee. Trackinganything Yang. Fine-grained controllable video generation
withdecoupledvideosegmentation.InProceedingsof via object appearance and context. arXiv preprint
theIEEE/CVFInternationalConferenceonComputer arXiv:2312.02919,2023. 1,3,5,6,7
Vision,pages1316–1326,2023. 4,5 [16] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose´ Lezama,
[6] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Jonathan Huang, Rachel Hornung, Hartwig Adam,
and Tat-Seng Chua. Empowering dynamics-aware Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al.
text-to-video diffusion with large language models. Videopoet: A large language model for zero-shot
arXivpreprintarXiv:2308.13812,2023. 12 video generation. arXiv preprint arXiv:2312.14125,
[7] MengyangFeng,JinlinLiu,KaiYu,YuanYao,Zheng 2023. 1
Hui,XiefanGuo,XianhuiLin,HaolanXue,ChenShi, [17] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou
XiaowenLi,etal. Dreamoving: Ahumanvideogen- Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and
eration framework based on diffusion models. arXiv Yong Jae Lee. Gligen: Open-set grounded text-to-
e-prints,pagesarXiv–2312,2023. 1,3 image generation. In Proceedings of the IEEE/CVFConferenceonComputerVisionandPatternRecogni- classes from videos in the wild. arXiv preprint
tion,pages22511–22521,2023. 1 arXiv:1212.0402,2012. 5
[18] Tsung-YiLin,MichaelMaire,SergeBelongie,James [29] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol
Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, Kurach, Raphael Marinier, Marcin Michalski, and
and C Lawrence Zitnick. Microsoft coco: Com- Sylvain Gelly. Towards accurate generative models
mon objects in context. In Computer Vision–ECCV ofvideo: Anewmetric&challenges. arXivpreprint
2014: 13th European Conference, Zurich, Switzer- arXiv:1812.01717,2018. 6
land, September 6-12, 2014, Proceedings, Part V 13,
[30] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan
pages740–755.Springer,2014. 6
Kindermans, Hernan Moraldo, Han Zhang, Moham-
[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and madTaghiSaffar,SantiagoCastro,JuliusKunze,and
Yong Jae Lee. Visual instruction tuning. arXiv Dumitru Erhan. Phenaki: Variable length video gen-
preprintarXiv:2304.08485,2023. 4 eration from open domain textual description. arXiv
[20] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, preprintarXiv:2210.02399,2022. 3,7
Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,
[31] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya
Hang Su, Jun Zhu, et al. Grounding dino: Marrying
Zhang, Xiang Wang, and Shiwei Zhang. Mod-
dino with grounded pre-training for open-set object
elscopetext-to-videotechnicalreport. arXivpreprint
detection. arXivpreprintarXiv:2303.05499,2023. 4,
arXiv:2308.06571,2023. 2,3,5,7,12
5
[32] Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin,
[21] Wan-Duo Kurt Ma, J. P. Lewis, and W. Bastiaan
ZhengyuanYang,HanwangZhang,ZichengLiu,and
Kleijn. Trailblazer: Trajectory control for diffusion-
Lijuan Wang. Disco: Disentangled control for re-
basedvideogeneration. 2023. 1,3
ferring human dance generation in real world. arXiv
[22] BenMildenhall,PratulPSrinivasan,MatthewTancik,
preprintarXiv:2307.00040,2023. 1,3
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.
[33] WeiminWang,JiaweiLiu,ZhijieLin,JiangqiaoYan,
Nerf:Representingscenesasneuralradiancefieldsfor
Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu,
view synthesis. Communications of the ACM, 65(1):
Jun Hao Liew, Hanshu Yan, et al. Magicvideo-v2:
99–106,2021. 4,12
Multi-stage high-aesthetic video generation. arXiv
[23] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang
preprintarXiv:2401.04468,2024. 3
Wang, Yujie Wei, Yingya Zhang, Changxin Gao,
[34] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou
and Nong Sang. Hierarchical spatio-temporal de-
Chen, JiuniuWang, YingyaZhang, YujunShen, Deli
coupling for text-to-video generation. arXiv preprint
Zhao, and Jingren Zhou. Videocomposer: Compo-
arXiv:2312.04483,2023. 1
sitional video synthesis with motion controllability.
[24] OlafRonneberger,PhilippFischer,andThomasBrox.
arXivpreprintarXiv:2306.02018,2023. 1,3
U-net: Convolutional networks for biomedical im-
age segmentation. In Medical Image Computing and [35] ZhouxiaWang,ZiyangYuan,XintaoWang,Tianshui
Computer-AssistedIntervention–MICCAI2015: 18th Chen, MenghanXia, PingLuo, andYingShan. Mo-
InternationalConference,Munich,Germany,October tionctrl: A unified and flexible motion controller for
5-9, 2015, Proceedings, Part III 18, pages 234–241. video generation. arXiv preprint arXiv:2312.03641,
2023. 1,3
Springer,2015. 3
[25] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval [36] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li,
Kirstain,AmitZohar,OronAshual,DeviParikh,and Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan.
Yaniv Taigman. Emu edit: Precise image editing Godiva: Generatingopen-domainvideosfromnatural
via recognition and generation tasks. arXiv preprint descriptions. arXivpreprintarXiv:2104.14806,2021.
arXiv:2311.10089,2023. 5 6
[26] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, [37] JunXu,TaoMei,TingYao,andYongRui. Msr-vtt:A
Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, largevideodescriptiondatasetforbridgingvideoand
OronAshual,OranGafni,etal. Make-a-video: Text- language. In Proceedings of the IEEE conference on
to-video generation without text-video data. arXiv computervisionandpatternrecognition,pages5288–
preprintarXiv:2209.14792,2022. 1,3,12 5296,2016. 5
[27] Jiaming Song, Chenlin Meng, and Stefano Ermon. [38] ZhongcongXu,JianfengZhang,JunHaoLiew,Han-
Denoising diffusion implicit models. arXiv preprint shuYan,Jia-WeiLiu,ChenxuZhang,JiashiFeng,and
arXiv:2010.02502,2020. 12 Mike Zheng Shou. Magicanimate: Temporally con-
[28] KhurramSoomro,AmirRoshanZamir,andMubarak sistenthumanimageanimationusingdiffusionmodel.
Shah. Ucf101: A dataset of 101 human actions arXivpreprintarXiv:2311.16498,2023. 1,3[39] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Lin-
jie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng
Liu, Ce Liu, Michael Zeng, et al. Reco: Region-
controlled text-to-image generation. In Proceedings
oftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages14246–14255,2023. 8
[40] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi,
Houqiang Li, Gong Ming, and Nan Duan. Drag-
nuwa: Fine-grainedcontrolinvideogenerationbyin-
tegrating text, image, and trajectory. arXiv preprint
arXiv:2308.08089,2023. 1,3
[41] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou,
Yang Wei, Yuchen Zhang, and Hang Li. Make pix-
els dance: High-dynamic video generation. arXiv
preprintarXiv:2311.10982,2023. 1,2,3,5,7,12
[42] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,
Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and
MikeZhengShou. Show-1: Marryingpixelandlatent
diffusion models for text-to-video generation. arXiv
preprintarXiv:2309.15818,2023. 7
[43] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei
Lv,YizheZhu,andJiashiFeng.Magicvideo:Efficient
video generation with latent diffusion models. arXiv
preprintarXiv:2211.11018,2022. 3,7,12
[44] LuoweiZhou,YannisKalantidis,XinleiChen,JasonJ
Corso, and Marcus Rohrbach. Grounded video de-
scription. InCVPR,2019. 6A.MoreImplementationDetails
ControlModule WefollowNeRF[22]touseFourierembeddingstoencodeboxcoordinates,objectIDandthehard/soft
flag. Wemakesurethatallinputdimensionsarescaledbetween0and1. Foranygiveninputxwithinthisrange,itsFourier
embeddingisdefinedas:
Fourier(x)=[cos(x·1000/8),...,cos(x·1007/8),sin(x·1000/8),...,sin(x·1007/8)].
We combine these Fourier embeddings of each input to form the overall embedding, which has a dimension of 128. As
mentionedinSection4.1,theseembeddingsarethenprocessedthroughamulti-layerperceptron(MLP).ThisMLPhasthree
hiddenlayers,eachwithadimensionof512.Finally,theoutputcontroltokenisadjustedtomatchthedimensionofthevisual
token,whichis1024.
Training&InferenceDetails Ourmodelstrainon16-framesequenceswitharesolutionof256x256pixels,runningat4
framespersecond. WelimitthemaximumnumberofobjectstoN =8. ThetrainingusestheAdamoptimizer,withabatch
sizeof128across16NVIDIATeslaA100GPUs. AsoutlinedinSection4.4,trainingoccursinthreestages: 50kiterations
forstage1, 50kiterationsforstage2, and10kiterationsforstage3. Weuse2×10−4 learningrateforthefirststage, and
3×10−5 for later stages. All stages use linear learning rate scheduler with 7,500 warm-up steps. Since box conditioning
isoptional,weuse25%ofourtrainingdatafromvideoswithoutanyboxannotation. Sincefirstframeconditioningisalso
optional,welethalfofthetrainingsamplesincludethevideo’sfirstframeasacondition.
Forallexperiments,weusetheDDIMinferencealgorithm[27]with50inferencesteps.Toenableclassifier-freeguidance,
weconstructnegativeconditionsbysubstitutingeverycontroltokenwitht . Wesettheclassifier-freeguidancescaletobe
null
9.
B.ResultsonUCF-101
Models ExtraInput FVD(↓) mAP/AP /AP (↑)
50 75
MagicVideo[43] - 699 -
LVDM[12] - 641 -
ModelScope[31] - 410 -
Make-A-Video[26] - 367 -
VidRD[10] - 363 -
PYOCO[8] - 355 -
Dysen-VDM[6] - 325 -
PixelDance[41] - 242 -
StableVideoDiffusion[2] - 242 -
- 270 0.060/0.127/0.044
Box 263 0.228/0.354/0.229
PixelDance+Boximator
F0 132 0.171/0.272/0.163
F0+Box 142 0.284/0.419/0.279
- 310 0.063/0.131/0.047
Box 311 0.192/0.308/0.184
ModelScope+Boximator
F0 196 0.132/0.223/0.119
F0+Box 194 0.212/0.343/0.205
Table5.Zero-shotresultsonUCF-101.
We follow the experiment settings of PixelDance [41] to evaluate on UCF-101. Specifically, we sampled 2,048 videos
fromtheUCF-101testset,generatingdescriptivetextpromptsforeachofthem,andthengenerated10,24016-framevideos.
We compute the FVD real features from the original 2,048 videos by sampling 16 frames from each video. Reference
boundingboxeswereautomaticallyannotatedusingthesamemethodasforMSR-VTT.Givengeneratedvideos,weemployed
DINO+DEVA for bounding box detection and computed average precision (AP) scores. It’s noteworthy that UCF-101’spromptsaremoredetailedthanthoseforMSR-VTTandActivityNet. Sincetheautomaticannotationusesthetextpromptto
extractobjectnames,thelongerpromptsleadtomore,albeitnoisier,boxespervideo.
TableBpresentsourUCF-101results, showingtrendsconsistentwithMSR-VTT.TheBoximatormodelroughlymain-
tained or improves the FVD scores compared to the base model. While using the first frame (F0) as a condition notably
boostedFVDscores,boxconstraintshadminimalimpacttoFVD,likelyduetothenoisiernatureofUCF-101’sboxes.
Inallscenarios,usingboxconstraintssignificantlyincreasedAPscores,echoingresultsfromMSR-VTTandActivityNet.
However,theabsoluteAPvaluesonUCF-101werelowerthanontheotherdatasets,probablyduetothelowerqualityofbox
annotations.
C.HumanEvaluationDetails
Weselected100high-qualityvideosfeaturingprominentcameraorobjectmovementsfromWebVid(excludedfromtraining
set)andmanuallyannotatedtheirboundingboxes. ThenwegeneratenewvideosusingboththestandardPixelDancemodel
and PixelDance+Boximator, with the video caption and the first frame taken as inputs. The Boximator model additionally
usedboundingboxesfromthefirstandlastframes. Fourhumanratersassessedtheregeneratedvideos,markedas“Video1”
and“Video2,”presentedinarandomizedordertoobscurethegeneratingmodel. Ratersevaluatedthevideosforqualityand
motioncontrol,choosingbetween“Video1isbetter,”“Video2isbetter,”or“nopreference.”
VideoQuality Ratersevaluatedeachvideoforvisualdistortions,blurs,orotherqualitydefects,andfortemporalinconsis-
tencies,suchasinconsistentobjectappearancesacrossframes. Incaseswherebothvideoswerefreefromtheseissues,raters
favoredthevideowithrichercontent. Forinstance,whencomparingtwovideoswhereoneexhibitsinterestingmotionand
theotherremainsmostlystationary,ratersareexpectedtofavorthemoredynamicone.
MotionControl Theevaluationfocusedonwhethereachvideosatisfiedmotionconstraintssetbytheboundingboxesin
the initial and final frames. Preference was given to the video meeting these constraints. If both or neither video met the
constraints,ratersareexpectedtoselect“nopreference.”
SomesamplevideosandtheirevaluationsresultsaredisplayedinFigures7to9.Figure 7. Sample videos from human evaluation (Part 1). Each group displays two rows: the first generated by the Boximator model
andthesecondbythebasemodel. VoteresultsaredenotedasX/Y/Z,indicatingraters’preferences: XforBoximatormodel,Yforno
preference,andZforbasemodel.Figure8.Samplevideosfromhumanevaluation(Part2).Figure9.Samplevideosfromhumanevaluation(Part3).