Natural Counterfactuals With Necessary Backtracking
Guang-YuanHao*12 JijiZhang*2 BiweiHuang3 HaoWang4 KunZhang51
Abstract
Evidence
Counterfactualreasoningispivotalinhumancog- 𝐵𝐵𝐵𝐵𝐵𝐵 𝑇𝑇𝑇𝑇𝑇𝑇 𝐽𝐽𝐽𝐽𝐽𝐽𝐽𝐽𝐽𝐽
Sudden Braking Falling Down Injury
nitionandespeciallyimportantforprovidingex-
Non-backtracking
planations and making decisions. While Judea
𝐵𝐵𝐵𝐵𝐵𝐵 𝑇𝑇𝑇𝑇𝑇𝑇 𝐽𝐽𝐽𝐽𝐽𝐽𝐽𝐽𝐽𝐽
Pearl’s influential approach is theoretically ele- Sudden Braking Standing Still Safety
gant,itsgenerationofacounterfactualscenario
Ours
oftenrequiresinterventionsthataretoodetached
𝐵𝐵𝐵𝐵𝐵𝐵 𝑇𝑇𝑇𝑇𝑇𝑇 𝐽𝐽𝐽𝐽𝐽𝐽𝐽𝐽𝐽𝐽
fromtherealscenariostobefeasible. Inresponse, Slowing Down Standing Still Safety
weproposeaframeworkofnaturalcounterfactu- Figure1.MotivationalExample:TreatingBus,Tom,andJerryas
alsandamethodforgeneratingcounterfactuals Variables.Downwardarrowsindicatetheirvalues,whileupward
thatarenaturalwithrespecttotheactualworld’s arrowsrepresentinterventions.
datadistribution. Ourmethodologyrefinescoun-
terfactualreasoning,allowingchangesincausally
precedingvariablestominimizedeviationsfrom leaving its causally upstream features untouched. Such
realisticscenarios. Togeneratenaturalcounter- non-backtrackingcounterfactualreasoning(i.e.,reasoning
factuals,weintroduceaninnovativeoptimization abouttheconsequencesofanchangewithouttracingback
frameworkthatpermitsbutcontrolstheextentof tochangesincausallypreceedingvariables)canyieldvalu-
backtrackingwitha“naturalness”criterion. Em- ableinsightsintotheconsequencesofhypotheticalactions.
piricalexperimentsindicatetheeffectivenessof Consider a scenario: a sudden brake of a high-speed bus
ourmethod. causedTomtofallandinjureJerry,asillustratedinFig.1.
Non-backtracking counterfactual reasoning would tell us
thatifTomhadstoodstill(despitethesuddenbraking),then
1.Introduction Jerrywouldnothavebeeninjured. Pearl’sapproachsup-
pliesaprincipledmachinerytoreasonaboutconditionalsof
Counterfactualreasoning,whichaimstoanswerwhatafea- thissort,whichareusuallyusefulforexplanation,planning,
tureoftheworldwouldhavebeenifsomeotherfeatureshad andresponsibilityallocation.
beendifferent,isoftenusedinhumancognition,toperform
However,suchsurgicalinterventionsmaynotbefeasiblein
self-reflection,provideexplanations,andinformdecisions.
practiceandmaynotprovidesignificantassistanceinself-
ForAIsystemstomirrorsuchhuman-likedecision-making
reflection. Inthepreviousexample,preventingTom’sfall
processes, incorporating counterfactual reasoning is cru-
inasuddenbrakingscenariorequiresdefyingmechanisms
cial. Judea Pearl’s structural approach to counterfactual
thataredifficultorphysicallyimpossibletodisrupt,suchas
modelingandreasoningstandsasacornerstoneinmachine
thelawofinertia. Thissupposedhardinterventionmaythus
learning (Pearl, 2009). Within this framework, counter-
betoofar-fetchedtoberelevantforpracticalpurposes. For
factualsareconceptualizedasbeinggeneratedbysurgical
example,fromalegalperspective,Tom’sfallcausingJerry’s
interventionsonthefeaturestobechangedthatseverthe
injurycouldbegivena“necessitydefense,”acknowledging
originalcausallinksresponsibleforthosefeatures,while
thatthesuddenbrakinglefthimwithnoalternatives(Conde,
*Equal contribution 1Mohamed bin Zayed University 1981). Hence,forthepurposeofallocatingresponsibility,
of Artificial Intelligence 2The Chinese University of Hong reasoningaboutthecounterfactualsituationofTomstanding
Kong 3University of California San Diego 4Rutgers Uni-
stilldespitethesuddenbrakingisarguablyirrelevantoreven
versity 5Carnegie Mellon University. Correspondence
misleading.
to: Guang-Yuan Hao <guangyuanhao@outlook.com>,
Jiji Zhang <jijizhang@cuhk.edu.hk>, Biwei Huang
Ourpaperaddressescounterfactualreasoningwithafocus
<bih007@ucsd.edu>, Hao Wang <hw488@cs.rutgers.edu>,
onoutcomesthatareconstructive,whichareintendedtoen-
KunZhang<jkunz1@cmu.edu>.
hancepracticalsituationsandofferactionableinsights. To
achievethis,weensurethateverychangeinasystemisnat-
1
4202
beF
2
]IA.sc[
1v70610.2042:viXraNaturalCounterfactualsWithNecessaryBacktracking
ural. Thus,afterthesemodifications,thenewcounterfactual 2.RelatedWork
datapointwillbenaturalwithrespecttodatadistributionin
Non-backtrackingCounterfactualGeneration. Aswill
theactualworld. Accordingly,weintroducethenotionof
becomeclear,ourtheoryispresentedintheformofcoun-
“naturalcounterfactuals”toaddressthelimitationsofnon-
terfactual sampling or generation. (Ribeiro et al., 2023;
backtrackingcounterfactualsdiscussedabove. Forexample,
Kocaoglu et al.; Dash et al., 2022; Sanchez & Tsaftaris)
as depicted in Fig. 1, rather than the infeasible scenario
usethedeepgenerativemodelstolearnanSCMfromdata
whereTomdoesnotfallatasuddenbusstop,amorerealis-
givenacausalgraph;theseworksstrictlyfollowPearl’sthe-
ticinterventionwouldinvolvethebusslowingdownearlier,
oryofnon-backtrackingcounterfactuals. Ourcasestudies
achievedthroughbacktracking(Lewis,1979)thatkeepsthe
willexaminesomeofthesemodelsanddemonstratetheir
scenariowithinreal-worldbounds. Furthermore,onemust
difficultiesindealingwithinterventionsthatareunrealistic
ensurethatthecounterfactualdatapointremainsascloseas
relativetotrainingdata.
possibletotheoriginaldatapoint;otherwise,unnecessary
interventions may affect too many variables. To address BacktrackingCounterfactuals. Backtrackingincounter-
thisconcern,weformulateaminimalchangeprinciplethat factualreasoninghasdrawnplentyofattentioninphiloso-
guidesusinperformingonlythenecessarybacktracking. phy(Hiddleston,2005),psychology(Dehghanietal.,2012),
andcognitivescience(Gerstenbergetal.,2013). Hiddleston
Moreover,fromamachinelearningperspective,whenhard
(2005) proposes a theory that is in spirit similar to ours,
interventions lead to unrealistic scenarios relative to the
in which backtracking is allowed but limited by some re-
training data, predicting counterfactual outcomes in such
quirementofmatchingasmuchcausalupstreamaspossible.
scenarioscanbehighlyuncertainandinaccurate(Hassan-
Gerstenbergetal.(2013)showsthatpeopleusebothback-
pour&Greiner,2019). Thisissuebecomesparticularlypro-
trackingandnon-backtrackingcounterfactualsinpractice
nouncedwhennon-parametricmodelsareemployed,asthey
andtendtousebacktrackingcounterfactualswhenexplic-
oftenstruggletogeneralizetounseen,out-of-distribution
itlyrequiredtoexplaincausesforthesupposedchangein
data(Scho¨lkopfetal.,2021). Theriskofrelyingonsuch
acounterfactual. vonKu¨gelgenetal.(2022)isamostre-
counterfactualsisthussubstantial,especiallyinhigh-stake
centpaperexplicitlyonbacktrackingcounterfactuals. The
applications like healthcare and autonomous driving. To
maindifferencesbetweenthatworkandoursarethatvon
tacklethis,wedevelopanapproachthatamountstoutilizing
Ku¨gelgen et al. (2022) requires backtracking all the way
onlyfeasibleinterventionsthatkeepdatawithinitsoriginal
backtoexogenousnoisesandmeasuresclosenessonnoise
distribution,allowingbacktrackingwhenneeded. Thisstrat-
terms,whicharelessdesirablethanlimitingbacktrackingto
egy effectively reduces the risk of inaccurate predictions
whatwecall“necessarybacktracking”andmeasuringclose-
andensuresmorereliableresults.
nessdirectlyonendogenous,observablevariables,because
Consequently, our approach aims to achieve the goal of changestotheunobservedornoisetermsarebydefinition
ensuringthatcounterfactualscenariosremainsufficiently outsideofourcontrolandnotactionable. Moreover,their
realisticwithrespecttotheactualworld’sdatadistribution backtracking counterfactuals sometimes allow gratuitous
bypermittingminimalyetnecessarybacktracking. Itisde- changes,asweexplaininSec.GoftheAppendix.
signedontwomajorconsiderations: First,weneedcriteria
CounterfactualExplanations. Aprominentapproachin
todeterminethefeasibilityofinterventions,ensuringthey
explainableAIiscounterfactualexplanation(Wachteretal.,
arerealisticwithrespecttoactual-worlddatadistribution.
2018;Dhurandharetal.,2018;Mothilaletal.,2020;Baro-
Second,weappealtobacktrackingwhenandonlywhenit
casetal.,2020;Pawlowskietal.,2020a;Vermaetal.,2020;
isnecessarytoavoidinfeasibleinterventions,andneedto
Schutetal.,2021),onwhichourworkislikelytohaveinter-
developafeasibleoptimizationframeworktorealizethis
estingbearings. Mostworksonthistopicdefinesomesense
strategy. Thekeycontributionsofthispaperinclude:
of minimal changes of an input sample with a predicted
• Developingamoreflexibleandrealisticnotionofnat- classcsuchthataddingtheminimalchangesintotheinput
uralcounterfactuals,addressingthelimitationsofnon- would make it be classified into another (more desirable)
backtrackingreasoningwhilekeepingitsmeritsasfar class. Althoughthispaperdoesnotdiscusscounterfactual
aspossible. explanations,ourframeworkmaywellbeusedtodefinea
• Introducing an innovative and feasible optimization novelnotionofcounterfactualexplanationbyrequiringthe
frameworktogeneratenaturalcounterfactuals. counterfactualinstancestobe“natural”inoursense.
• Detailing a machine learning approach to produce
3.NotationsandBackground
counterfactualswithinthisframework,withempirical
resultsfromsimulatedandrealdatashowcasingthesu-
Inthissection,webeginbyoutliningvariousbasicconcepts
periorityofourmethodcomparedtonon-backtracking
in causal inference, followed by an introduction to non-
counterfactuals.
backtrackingcounterfactuals.
2NaturalCounterfactualsWithNecessaryBacktracking
Structural Causal Models. We use a structural causal causally preceding variables, depending on the “natural-
model (SCM)to represent thedata generating processof ness”ofinterventions.Todifferentiatefromtheintervention
acausalsystem. ASCMisamathematicalstructurecon- do(A=a∗)innon-backtrackingcounterfactuals,weintro-
sistingofatripletM:=<U,V,F>,withanexogenous duceaspecificoperatorchange(·)andchange(A = a∗)
(noise) variable set U = {U ,...,U }, an endogenous signifiesadesiredmodificationinA,whichmayresultfrom
1 N
(observed)variableV={V ,...,V },andafunctionset aninterventiononAoraninterventiononsomecausalan-
1 N
f ={f ,...,f }(Pearl,2009). Eachfunction,f ∈f,spec- cestorsofA.
1 N i
ifieshowanendogenousvariableV isdeterminedbyits
i Weintroducetwonewconceptstodeterminewheretocarry
parentsPA ⊆V:
i outfeasibleinterventionsonC(eitherAitselforthecausal
V i :=f i(PA i,U i), i=1,...,N (1) earliervariablesofA)torealizechange(A = a∗)innat-
LocalMechanisms. Alocalmechanismistheconditional ural counterfactuals: a naturalness constraint and a nec-
distributionofanendogenousvariablebasedonitsparent essarybacktrackingprinciple.1 First,weproposeusinga
variables,representedasp(V i|PA i)fori=1,...,N. This naturalnesscriteriontoassessthefeasibilityofaninterven-
definitioninherentlycapturesthepropertiesofnoisevari- tion. Weassumethatactual-worlddatacontainallrelevant
ables;givenafixedvalueforPA i,noiseU ientirelydictates causalmechanismsforagivencausalgraph;thus,out-of-
theprobabilityofV i(Pearl,2009). Hence,throughoutthis distributiondataarephysicallyinfeasible. Thismeanswe
paper,theterm“localmechanisms”willencompassboth limit changed values resulting from interventions to stay
theconditionaldistributionofanendogenousvariablegiven withintheobserveddatadistribution,ensuringthefeasibil-
itsparentsp(V i|PA i)andthedistributionofthenoisevari- ityofinterventionsonCandthenaturalnessofresulting
ablep(U i). counterfactuals.
Intervention. GivenanSCM,aninterventiononanendoge- Furthermore,weconsidertheproximitybetweentheactual
nousvariableAisrepresentedbyreplacingitsfunctionwith value and its counterfactual value, leading to a principle
aconstantfunctionA=a∗,wherea∗isthetargetvalueof
knownas“necessarybacktracking.” Thisprincipleaimsto
A, andleavingfunctionsandlocalmechanismsforother minimizechangesinthecounterfactualworldandbacktrack
variablesintact(Pearl,2009). aslittleaspossible,keepingCascloselyalignedwithA
as feasible. In summary, the naturalness constraint and
Non-Backtracking Counterfactuals. A counterfactual
necessary backbreaking both contribute to determining a
ponderswhatwouldhappeninascenariothatdiffersfrom
feasibleintervention.
theactual oneina certainway. Followinga standardno-
tation,termswitha∗superscriptrefertoacounterfactual
world. Forexample,u∗denotesU ’svalueinacounterfac- 4.1.FeasibleInterventionOptimization
i i
tualworld. LetA,B,andEbeendogenousvariables. Here
To address the challenge of determining a feasible inter-
isageneralcounterfactualquestion: givenevidenceE=e,
vention,weproposetreatingitasanoptimizationproblem.
whatwouldthevalueofBhavebeenifAhadbeena∗?The
Thisentailsoptimizingadefineddistancemetricbetween
Pearlian,non-backtrackingreadingofthisquestiontakesthe
the counterfactual outcome produced by the intervention
counterfactualsuppositionofA=a∗toberealizedbyan
andthecorrespondingreal-worlddatapoint,followingthe
interventiononA(Pearl,2009). Giventhisunderstanding,
principleofnecessarybacktracking. Thisoptimizationpro-
counterfactual inference includes three steps. (1) Abduc-
cedurealsoenforcesanaturalnessconstraintoncounterfac-
tion:Thenoisedistributionisupdatedbasedontheevidence
tuals. WedefinethisoptimizationframeworkasFeasible
E=e. (2)Action: Thecausalmodelismodified,inwhich
InterventionOptimization(FIO)asfollows:
Aisfixedtoa∗whilekeepingothercomponentsthesame
asbefore. (3)Prediction: Thecounterfactualoutcomeof minimize D(an(A),an(A)∗)
Bisinferredusingtheupdatednoisedistributionandthe an(A)∗
modifiedmodel. s.t. A=a∗, (2)
g (an(A)∗)>ϵ.
n
4.AFrameworkforNaturalCounterfactuals
where an(A) and an(A)∗ represent the actual value and
Do(·) and Change(·) Operators. In considering a counterfactualvalueofA’sancestorsAN(A)respectively,
counterfactual scenario in which A’s value is a∗ instead whereA ∈ AN(A). D(·)isaspecificdistancemetricto
ofa,thenon-backtrackingmodealwaysappealstoadirect
interventiononA, i.e., do(A = a∗)inPearl’sinfluential
1Givenafeasibleintervention,ournaturalcounterfactualinfer-
encealsofollowsthreesteps: abduction,action,andprediction,
notation. However, in our framework, the counterfactual
alignedwiththeprocedureofnon-backtrackingcounterfactuals
suppositionisnotnecessarilyrealizedbyadirectinterven- asmentionedinSec.3.Moredetailsfortheprocedureofnatural
tiononA,andmayberealizedbyinterventionsonsome counterfactualsareinSec.E.
3NaturalCounterfactualsWithNecessaryBacktracking
assessthedistancebetweenactualworldandcounterfactual v∗,withinthecounterfactualdatapointan(A)∗inthissec-
j
world. g (·) measures the naturalness of counterfactual tion,followedbyanexaminationoftheoverallnaturalness
n
valueofA’sancestorsandϵisasmallconstant. Wewill ofan(A)∗inthenextsection.
discussD(·)andg (·)later.
n We introduce “local ϵ-natural generation,” where a value
OptimizationScope. Allpossiblebacktrackingvariables satisfies this criterion if it is a natural outcome of its lo-
are contained within A’s ancestor set AN(A). Conse- cal mechanism. We focus on a specific value V =
j
quently,ouroptimizationfocusremainsconfinedtothese v∗, alongside its parent value PA = pa∗, noise value
j j j
nodes. ThroughFIO,weobtaintheoptimalvaluean(A)∗. U = u∗, and the corresponding local mechanism, ex-
j j
Thisidentifiesthechangedvariables,C,whichwetarget pressed by p(V |PA = pa∗) or p(U ). The cumula-
j j j j
forintervention,enablingthedefinitionofthefeasiblein- tive distribution function (CDF) for noise variable U at
j
tervention do(C = c∗), with c∗ as the post-optimization U = u∗ is F(u∗) = (cid:82)u∗ j p(U )dU , and for the con-
j j j −∞ j j
counterfactualvaluesofC. ditional distribution p(V |PA = pa∗) at V = v∗ is
j j j j j
NaturalnessConstraint.Inadditiontoensuringthechange F(V |pa∗)=(cid:82)v j∗ p(V =v∗|pa∗)dV .
j j −∞ j j j j
A = a∗, the counterfactual value an(A)∗ must satisfy a
Weproposethefollowingpotentialcriteriabasedonentropy-
naturalnesscriteriontoensurethefeasibilityofinterventions.
normalizeddensity,CDFofexogenousvariables,andCDF
Specifically,itsnaturalnessshouldexceedthethresholdϵto
ofconditionaldistributions. Theentropy-normalizednatu-
remainwithinthedatadistribution. Adetaileddiscussion
ralnessmeasureevaluatesthenaturalnessofv∗inrelation
ofpotentialnaturalnessconstraintsispresentedinSec.4.2. j
toitslocalmechanismp(V |PA =pa∗). TheCDF-based
j j j
DistanceMeasureD(·). D(·)considerstheconceptofnec- measures,namelythelattertwocriteria,considerdatapoints
essarybacktrackingandservesasametricforquantifying inthetailstobelessnatural. Eachofthesecriteriahasits
thedistancebetweenthecounterfactualandactualworlds. ownintuitiveappeal, andtheirrelativemeritswillbedis-
Foradetailedexploration,pleaserefertoSec.4.3. cussedsubsequently.Weformallyestablishthethreecriteria
asfollows:
4.2.NaturalnessConstraints
(1) Entropy-NormalizedMeasure:
Toensureanintervention’sfeasibility,westipulatethatthe
resultingcounterfactualvaluean(A)∗shouldfallwithinthe
p(v j∗|pa∗ j)eH(Vj|pa∗ j) > ϵ, where H(V j|pa∗ j) =
E[−logp(V |pa∗)];
datadistribution,thusbeingnaturalrelativetothisdistribu- j j
tion. Weintroduceanaturalnesscriterion,whichconfines (2) ExogenousCDFMeasure:min(F(u∗),1−F(u∗))>
j j
thefeasiblesupportforan(A)∗inEqn.2oftheFIOframe- ϵ,i.e.,ϵ<F(u∗)<1−ϵ;
j
work.
(3) Conditional CDF Measure: min(F(v∗|pa∗),1 −
Intuitively, the more frequently a value occurs, the more F(v∗|pa∗))>ϵ,i.e.,ϵ<F(v∗|pa∗)<1j −ϵ.j
“natural”itisconsidered. Therefore,weassessthisnatural- j j j j
nessbyexaminingthedistributioncharacteristics,suchas
wherethefunctionmin(·)returnstheminimumvaluebe-
density,ofeachvariable’svalueV =v∗withinAN(A).
j j tweentwogivenvalues.
Thisassessmentisrelativetothevariable’slocalmechanism
p(V |pa∗),whereV ∈AN(A)andpa∗denotestheparent Choice (1): Entropy-Normalized Measure. Specifi-
j j j j
valueofV j. Specifically,ourapproachdetermineswhether cally,Choice(1),p(v j∗|pa∗ j)eH(Vj|pa∗ j),canberewrittenas
the density of v j∗ surpasses a pre-established threshold ϵ, elog(p(v j∗|pa∗ j))+E[−logp(Vj|pa∗ j)], where −log(p(v j∗|pa∗ j))
utilizedasastandardforappraisingnaturalness.2 canbeseenasthemeasureofsurpriseofv∗givenpa∗and
j j
E[−logp(V |pa∗)] can be considered as the expectation
j j
4.2.1.LOCALNATURALNESSCRITERIA ofsurpriseofthelocalmechanismp(V |pa∗)(Ash,2012).
j j
Hence, the measure is the relative naturalness (i.e., nega-
Indeed,itmightbedifficulttofindauniversaldefinitionfor
tive surprise) of V . Implementing this measure may be
naturalnesscriteria,andthus,variouscriteriafornaturalness j
straightforwardwhenemployingaparametricSCMwhere
canbeexplored. Thisallowsforamoretailoredapproach
theconditionaldistributioncanbeexplicitlyrepresented.
toassessingnaturalnessindifferentcontextsorapplications.
Westartbyassessingthenaturalnessofonevariable’svalue, Choice(2): ExogenousCDFMeasure. Ifusingaparamet-
ricSCM,wemightdirectlymeasuredifferencesonexoge-
2Theconceptof“naturalness”canhavevariousinterpretations.
nousvariables. However,inanon-parametricSCM,exoge-
Inourcontext,itisdefinedbythedatadistributioninStructural
nousvariablesarenotidentifiable,anddifferentnoisevari-
CausalModels(SCMs). Wedonotclaimthatthisistheonlyor
uniquelybestinterpretation,butthatitisausefulonetostudyand ablesmayhavedifferentdistributions. Hence,wechooseto
deploy. usetheCDFofexogenousvariablestoalignthenaturalness
4NaturalCounterfactualsWithNecessaryBacktracking
ofdifferentdistributions,basedonacommonassumptionto minimizingchangesinobservablevariablestominimizeper-
achievenon-parametricSCMsinthemachinelearningsys- ceptibledifferencespost-intervention. Thesecondfocuses
tem. Undertheassumption,thesupportofthelocalmech- onreducingalterationsinlocalmechanisms, recognizing
anism p(V |PA = pa∗) does not contain disjoint sets, themastheinherentcostofintervention. Duetospacelim-
j j j
andanonlinear,non-parametricmodelV =f (pa∗,U ) itations, we will primarily elaborate on minimal changes
j j j j
isimplemented,wheref ismonotonicallyincreasingwith inobservablevariables. Amorethoroughinvestigationof
respecttoU . NoiseU usuallyassumedtobeastandard changesinlocalmechanismscanbefoundinSec.H.
j j
Gaussian(Luetal.,2020). Inthisway,counterfactualsare
Minimal Change in Observable Variables. The differ-
identifiable,discussedinSec.4.4. Datapointsfromthetails
encesinperceptionarecloselytiedtothealterationsinthe
ofastandardGaussiancanbethoughtofaslessimprobable
valuesofobservable(endogenous)variablesresultingfrom
events. Hence, V = v∗ satisfies local ϵ-natural genera-
j j theintervention. Intuitively,thetotaldistanceistheaggre-
tionwhenitsexogenousCDFF(u∗)fallswithintherange
j gateofdistancesacrossvariousvariablesintheancestorset
(ϵ,1−ϵ). In practice, for a single variable, U is a one-
j AN(A). Therefore,weutilizetheL1normtomeasurethis
dimensionalvariable,anditiseasiertoenforcethemeasure
distance, astheL1 normofferstheadvantageofadditive
thanChoice(1),whichinvolvesconditionaldistributions.
distancesacrossvariables. Wedefinethisdistanceasthe
Choice (3): Conditional CDF Measure. The measure perceptiondistance:
treats a particular value in the tails of local mechanism
p(V j|pa∗ j) as unnatural. Hence, V j = v j∗ meets local ϵ- D(an(A),an(A)∗)=∥an(A)−an(A)∗∥ 1 (3)
naturalgenerationwhenF(V =v∗|pa∗)fallswithinthe
j j j where an(A) and an(A)∗ represent the actual value and
range(ϵ,1−ϵ)insteadoftails. Thismeasurecanbeusedin
counterfactualvalueofA’sancestorsAN(A)respectively,
parametricmodelswheretheconditionaldistributioncanbe
whereA ∈ AN(A). Becauseendogenousvariablesmay
explicitlyrepresented. Notice,itcanbeeasilyusedinnon-
varyinscale,weutilizethestandarddeviationofvariables
parametricmodelswhenthosemodelssatisfytheassump-
tonormalizeeachendogenousvariablebeforecomputing
tionmentionedinChoice(2),andthemeasureisequivalent
thedistance. Thisnormalizationensuresaconsistentand
toChoice(2),sincetheCDFF(X|pa∗)hasaone-to-one
j fairevaluationofchangesacrossallvariables,irrespective
mappingwiththeCDFF(U ),i.e.,F(v∗|pa∗) = F(u∗),
j j j j oftheirindividualscales.
whenv∗ =f(pa∗,u∗).
j j j
Implicitly,ourdistancemetricfavorschangesinvariables
4.2.2.ϵ-NATURALGENERATION that are as proximal as possible to the target variable A,
since altering a more preceding variable typically results
Basedonthedefinitionoflocalϵ-naturalgeneration,wecan
inchangestomoredownstreamvariables. Whenthevalue
defineϵ-naturalgenerationtojudgewhetherthecounterfac-
an(A)∗,resultingfromahardinterventiononA,meetsthe
tualvaluean(A)∗isnatural.
ϵ-naturalgenerationcriterion,itsuggeststhatourdistance
Definition1(ϵ-NaturalGeneration). GivenaSCMcon-
metricD(an(A),an(A)∗)becomesminimal,i.e.,|a−a∗|.
taining a set A. A set AN(A) contains all ancestors of This effectively eliminates the need for backtracking in
A and A itself. Data point AN(A) = an(A)∗ satis- suchcases. However,Ifahardinterventiondoesnotmeet
fies ϵ-natural generation, if and only if, for any variable theϵ-naturalgenerationcriterion,itbecomesnecessaryto
V ∈ AN(A),V = v∗ satisfieslocalϵ-naturalgenera- backtrack.
j j j
tion,wherev∗isthevalueofV andϵisasmallconstant.
j j
4.4.IdentifiablityofNaturalCounterfactuals
wherethedefinitionoflocalϵ-naturalgenerationcouldbe
In practice, we often do not know the form of structural
oneoftheoptionaldefinitionsin4.2.1andalargervalue
equation models and noise variables are not identifiable
of ϵ implies a higher standard for the naturalness of the
from the observed variables. Hence, we assume causal
generateddatapointan(A)∗.Tomakeinterventionfeasible,
structural models satisfy the conditions of the following
werequirean(A)∗tomeetϵ-naturalgeneration.
theoremderivedfromTheorem1ofLuetal.(2020)with
proof in Sec. B, under which natural counterfactuals are
4.3.DistanceMeasureforNecessaryBacktracking
identifiablewithunknownstructuralequationmodels.
ThenaturalnessconstraintsoutlinedinEqn.2werecompre- Theorem4.1(IdentifiableCounterfactuals). SupposeV
i
hensivelyaddressedpreviously. Wenowfocusondefining satisfies the following structural causal model: V :=
i
thenecessarybacktrackingdistanceinEqn.2oftheFIO f (PA ,U ) for any V ∈ V, where U ⊥ PA and as-
i i i i i i
framework to minimize the change in the counterfactual sumeunknownf issmoothandstrictlymonotonicw.r.t. U
i i
datapoint. Ouranalysisintroducestwodistinctdefinitions forfixedvaluesofPA . IfwehaveevidenceE = e,with
i
ofdistanceinthecounterfactualcontext.Thefirstprioritizes aninterventiondo(C=c∗),thecounterfactualdistribution
5NaturalCounterfactualsWithNecessaryBacktracking
ofBisidentifiable;thatis,p(B|do(C = c∗),E = e)is sure of distance between two distinct worlds, while the
identifiable.3 secondtermenforcestheconstraintofϵ-naturalgeneration.
Here, the constant hyperparameter w serves to penalize
ϵ
Following Feasible Intervention Optimization, we obtain noisevaluessituatedinthetailsofnoisedistributions.
the feasible intervention do(C = c∗). This theorem en-
surestheidentifiabilityofournaturalcounterfactualsgiven 6.CaseStudies
do(C = c∗),underthestatedassumptions. Notethatthe
Inthissection,weapplyourmethodandevaluateitseffec-
assumptionsdonotrequiremoreinformationonthestruc-
tiveness through empirical experiments on four synthetic
turalfunctionsoronthenoisedistributions. Hence,even
datasetsandtworeal-worlddatasets: MorphoMNISTand
without full information about the SCM, we can still use
3DIdentBOX.
conditionaldistributionstoinfernaturalcounterfactuals,as
wellasnon-backtrackingcounterfactuals,iftheassumptions Weproposeusingthedifferencebetweengeneratedandac-
oftheabovetheoremhold. tualoutcomesasameasureofperformance. Weexpectour
natural counterfactuals to significantly reduce error com-
5.AMethodforGeneratingNatural paredtonon-backtrackingcounterfactuals. Thisadvantage
Counterfactuals canbeattributedtotheeffectivenessofourmethodinper-
formingnecessarybacktrackingthatidentifiesfeasiblein-
Inthissection,weprovideapracticalmethodforintegrating terventions,keepingcounterfactualvalueswithinthedata
naturalcounterfactualsintoamachinelearningsystem. In distribution,particularlywhendirectinterventionsarenot
our scenario, we start with real-world data and a causal feasible. Ontheotherhand,non-backtrackingcounterfac-
graph, lacking the SCM. We employ a non-parametric tuals,relyingsolelyondirectinterventions,oftenproduce
modeltomodeltheconditionaldistributionsofendogenous out-of-distributionvalues,posingchallengesformachine
variablesandconsiderthemodelasanon-parametricSCM, learningmodelgeneralization.
makingtheassumptionthateachnoisevariableadherestoa
6.1.SimulationExperiments
standardGaussiandistribution. ThislearnedSCMservesas
thebasisforgeneratingnaturalcounterfactuals.
We start with four simulation datasets, which we use de-
Toimplementaparticularmethod,weplugperceptiondis- signedSCMstogenerate. PleaserefertotheAppendixfor
tance (Eqn. 3) and naturalness constraint (Choice (3) in moredetailsaboutthesedatasets. LetfirstdevolveintoToy
Sec.4.2.1)intoEqn.2oftheFIOframework. Belowisthe 1, which contains three variables (n 1,n 2,n 3). n 1 is the
equationofoptimization: confounderofn 2andn 3,andn 1andn 2causesn 3.
ExperimentalSettings. Assumedataandacausalgraph
minimize∥an(A)−an(A)∗∥
an(A)∗ 1 are known, but not the ground-truth SCMs. We employ
s.t. A=a∗, (4) normalizingflowstocapturethecausalmechanismsofvari-
ables(n ,n ,n ).GiventhelearnedSCMsandadatapoint
ϵ<F(Vj =v j∗|pa∗ j)<1−ϵ,∀Vj ∈AN(A). fromthe1 tes2 tset3
asevidence,wesetn orn astargetvari-
1 2
Noticethattheequationcouldhavenosolutions,i.e.,there able A and randomly sample values from test dataset as
isnofeasibleintervention,whenthetwoconstraintsimpos- counterfactualvaluesofthetargetvariablen 1 orn 2. For
sibly hold at the same time. In practice, the Lagrangian ournaturalcounterfactuals,weuseEqn.5todeterminefea-
method(Boyd&Vandenberghe,2004)isusedtooptimize sibleinterventions,withϵ = 10−4 andw ϵ = 104. Innon-
ourobjectivelossasbelow: backtrackingcounterfactuals,n 1orn 2isdirectlyintervened
on. WereporttheMeanAbsoluteError(MAE)betweenour
L=(cid:13) (cid:13)an(A)−an(A)∗(cid:13)
(cid:13) + learnedcounterfactualoutcomesandground-truthoutcomes
1
wϵ(cid:88) [max(ϵ−F(Vj =v j∗|pa∗ j),0)+max(ϵ+F(Vj =v j∗|pa∗ j)−1,0)] onn 2or/andn 3withmultiplerandomseeds. Noticethere
j maybenofeasibleinterventionsforsomechanges,aswe
s.t. A=a∗ haveclaimedinSec.5,andthusweonlyreportoutcomes
(5) withfeasibleinterventions,whichiswithinthescopeofour
wherethefunctionmax(·)returnsthemaximumvaluebe- naturalcounterfactuals.
tweentwogivenvalues. Thefirsttermrepresentsthemea-
Visualization of Counterfactuals on a Single Sam-
3Iftheassumptiondoesnothold,identifiabilitymayfail.For ple. We assess the counterfactual outcomes for a sam-
example,forY = XU 1+U 2 whereY andX areendogenous ple (n 1,n 2,n 3) = (−0.59,0.71,−0.37), aiming for
variablesandU andU arenoise, thecounterfactualoutcome
1 2 change(n = 0.19). In Fig. 2 (a), we depict the origi-
isnotidentifiable.Similarly,ourapproachcanbeappliedtodis- 2
naldatapoint(yellow),thenon-backtrackingcounterfactual
cretevariablessatisfyingtheassumptionofcounterfactualstability
developedby(Oberst&Sontag,2019)intheory. (purple),andthenaturalcounterfactual(green)for(n 1,n 2).
6NaturalCounterfactualsWithNecessaryBacktracking
Table1.MAEResultsonToy1−4.Forsimplicity,weusedooperatorinthetabletosaveroom,andwhennaturalcounterfactualsare
referredto,domeanschange.OurapproachexhibitsanobviousMAEreductionwhenappliedton asexpected.
2
Dataset Toy1 Toy2 Toy3 Toy4
doorchange do(n ) do(n ) do(n ) do(n ) do(n ) do(n ) do(n ) do(n )
1 2 1 1 2 3 1 2
Outcome n n n n n n n n n n n n n
2 3 3 2 2 3 4 3 4 4 2 3 3
Nonbacktracking 0.477 0.382 0.297 0.315 0.488 0.472 0.436 0.488 0.230 0.179 0.166 0.446 0.429
Ours 0.434 0.354 0.114 0.303 0.443 0.451 0.423 0.127 0.136 0.137 0.158 0.443 0.327
Table2. AblationStudyonϵ
4 E N Ov o ui n rd sbe an cc ke tracking 11 .. 05 Model ϵ CFs tdo(t)
i
tdo(i)
i
2
0.5
0 0.0 - NB 0.336 4.532 0.283 6.556 n3=2.31 n3=0.03
0.5 10−4 0.314 4.506 0.171 4.424 42 11 .. 50 y N O= o un rx sbacktracking V-SCM 10−3 Ours 0.298 4.486 0.161 4.121
4 3 2 1 n10 1 2 3 4 1.5 1.0 x0.:5 n3's Gr0o.0undtrut0h.5 1.0 1.5 10−2 0.139 4.367 0.145 3.959
(a) Outcomeerroronasin- (b) Groudtruth-PredictionScatter
- NB 0.280 2.562 0.202 3.345
glesample Plot
10−4 0.260 2.495 0.105 2.211
Figure2.TheVisualizationResultsonToy1(Viewtheenlarged H-SCM
10−3 Ours 0.245 2.442 0.096 2.091
figureinFig.7intheAppendix).
10−2 0.093 2.338 0.083 2.063
Theground-truthsupportforthesevariablesisshownasa
Furthermore,ourmethodexcelsevenwheninterveningin
bluescatterplot.
thecaseofn ,arootcause,byexcludingpointsthatdonot
1
(1) Feasible Intervention VS Hard Intervention. Non- meettheϵ-naturalgenerationcriteria,furtherdemonstrating
backtrackingcounterfactualsapplyahardinterventionon itseffectiveness.
n (do(n = 0.19)),shiftingtheevidence(yellow)tothe
2 2 AdditionalCausalGraphStructures. Ourmethodalso
post-interventionpoint(purple),whichliesoutsidethesup-
shows superior performance on three other simulated
portof(n ,n ). Thisimpliesthatdirectinterventionscan
1 2 datasetswithvariedcausalgraphstructures(Toy2toToy4),
resultinunnaturalvalues. Conversely,ournaturalcounter-
asdemonstratedinTable1.
factual(green)remainswithinthesupportof(n ,n )due
1 2
tonecessarybacktrackingandfeasibleinterventiononn .
2
6.2.MorphoMNIST
(2)OutcomeError. Wecalculatetheabsoluteerrorbetween
n ’smodelpredictionandground-truthvalueusingeither
3 𝑡
thegreenorpurplepointasinputforthemodelp(n |n ,n ).
3 1 2
Theerrorforthegreenpointissignificantlylowerat0.03,
comparedto2.31forthepurplepoint. Thislowererrorwith
𝑖 𝑥
thegreenpointisbecauseitstayswithinthedatadistribu-
(a) Causal (b) Samples
tionafterafeasibleintervention,allowingforbettermodel
Graph
generalizationthantheout-of-distributionpurplepoint.
Figure3. CausalGraphandsamplesofMorpho-MNIST.
CounterfactualsonWholeTestSet. InFig.2(b),weillus-
MorphoMNIST involves three variables (t,i,x). As de-
tratethesuperiorperformanceofourcounterfactualmethod
pictedinFig.3(a), t(digitstrokethickness)causesboth
on the test set, notably outperforming non-backtracking
i(strokeintensity)andx(images),withibeingthedirect
counterfactuals. This is evident as many outcomes from
causeofx.
non-backtracking counterfactuals for n significantly di-
3
vergefromthey = xline, showingamismatchbetween Inourexperiments,mirroringthoseinSection6.1,wein-
predictedandground-truthvalues. Incontrast,ourmethod’s corporatetwokeychanges. First,weutilizetwoadvanced
outcomes largely align with this line, barring few excep- deep learning models, V-SCM (Pawlowski et al., 2020b)
tionspossiblyduetolearnedmodel’simperfections. This andH-SCM(Ribeiroetal.,2023),fordoingcounterfactu-
alignmentisattributedtoourmethod’sconsistentandfea- als. Second,duetotheabsenceofground-truthSCMfor
sible interventions, enhancing prediction accuracy, while assessingoutcomeerror,weadoptthecounterfactualef-
non-backtracking counterfactuals often lead to infeasible fectivenessmetricfrom(Ribeiroetal.,2023),asdeveloped
results. Table1supportsthesefindings,demonstratingthat in(Monteiroetal.,2023). Thisinvolvestrainingapredic-
ourapproachexhibitsaMAEreductionof61.6%whenap- tor on the dataset to estimate parent values (tˆ,ˆi) from a
pliedton ,comparedwiththenon-backtrackingmethod. counterfactualimagexgeneratedbylearnedmodelp(x|t,i)
2
7
2n
noitciderP
s'3n
:yNaturalCounterfactualsWithNecessaryBacktracking
6.3.3DIdentBOX
𝑏 Inthisstudy,weemploytwopracticalpublicdatasetsfrom
𝑣 𝛽 3DIdentBOX(Bizeuletal.,2023),namelyWeak-3DIdent
ℎ 𝛼
andStrong-3DIdent. Bothdatasetssharethesamecausal
𝑥
𝑑 𝛾 graph, asdepictedinFig.4(a), whichincludesanimage
(a) CausalGraph (b) Weak Causal (c) StrongCausal variablexanditssevenparentvariables, withthreepairs
Relationship Relationship of parent variables: (h,d), (v,β), and (α,γ), where one
Figure4.Causalgraphof3DIdentandthecausalrelationshipsof is the direct cause of the other in each pair. The primary
variables(d,h)inWeak-3DIdentandStrong-3DIdentrespectively.
distinctionbetweenWeak-3DIdentandStrong-3DIdentlies
in the strength of the causal relationships between each
d=0.26; h=0.24; d=0.07; h=0.06; d=0.62; h=0.40; d=0.02; h=0.04; d=0.40; h=0.14;
v=0.23; =1.02; v=0.09; =0.33; v=0.08; =0.96; v=0.58; =1.12; v=0.70; =1.46;
=1.29; =1.35 =1.51; =1.80 =0.00; =1.69 =1.50; =1.15 =1.58; =0.13 variable pair, with Weak-3DIdent exhibiting weaker con-
nections(Fig. 4(b)) comparedtoStrong-3DIdent (Fig.4
(c)). OurapproachmirrorstheMorphoMNISTexperiments,
usingH-SCMasthelearnedSCMwithϵ=10−3.
InfluenceofCausalEffectStrength. AsTable3reveals,
ourmethodoutperformsnon-backtrackingonbothdatasets,
with a notably larger margin in Strong-3DIdent. This in-
creasedsuperiorityisduetoahigherincidenceofinfeasi-
(a) ResultsofNon-backtrackingCounterfactuals blehardinterventionsinnon-backtrackingcounterfactuals
withintheStrong-3DIdentdataset.
d=0.01; h=0.04; d=0.11; h=0.04; d=0.01; h=0.02; d=0.01; h=0.04; d=0.04; h=0.01;
v=0.09; =0.08; v=0.06; =0.07; v=0.02; =0.01; v=0.02; =0.20; v=0.04; =0.12;
=0.41; =0.91 =0.09; =0.29 =0.04; =0.30 =0.18; =0.02 =0.12; =0.05 VisualizationonStrong-3DIdent. Fig.5displayscounter-
factuals,withthetextabovetheevidenceimages(firstrow)
indicatingerrorsforthecounterfactualimages(secondrow).
InFig.5(a),itisevidentthatsomeimages(second,thirdand
fifthimagesinparticular),generatedbynon-backtracking
counterfactualsarelessrecognizableandhavelargererrors.
Conversely,ourcounterfactualimagesexhibitbettervisual
clarityandmoredistinctshapes,asournaturalcounterfac-
(b) ResultsofNaturalCounterfactuals tualsconsistentlyensurefeasibleinterventions,resultingin
morenatural-lookingimages.
Figure5. VisualizationResultsonStong-3DIdent(Viewtheen-
largedfigureinFig.10intheAppendix). SeeAppendixforMoreDetails. Forin-depthinformation
withtheinput(t,i),andthencomputingtheabsoluteerror ondatasets,experimentalresults’standarddeviation,model
|t−tˆ|or|i−ˆi|betweentheinput(t,i)andtheirpredicted training settings, Feasible Intervention Optimization, dif-
counterparts(tˆ,ˆi)asinferredfromcounterfactualimages. ferences between our natural counterfactuals and related
works,andmechanismdistancefornaturalcounterfactuals,
Ablation Study on Naturalness Threshold ϵ. Table 2 refertotheAppendix.
demonstrates that our error decreases with increasing ϵ,
regardless of whether V-SCM or H-SCM is used. This 7.Conclusion
trend suggests that a larger ϵ sets a stricter standard for
Toaddresstheimpracticalityofhardinterventions,wepro-
naturalnessincounterfactuals,enhancingthefeasibilityof
pose “natural counterfactuals,” a new counterfactual in-
interventionsandconsequentlyloweringpredictionerrors.
ferenceapproachmoreapplicabletoreal-worldscenarios.
Thisimprovementislikelybecausedeep-learningmodels
This method integrates a naturalness constraint and nec-
aremoreadeptatgeneralizingtohigh-frequencydata(En-
essary backtracking, forming an optimization framework
gstrometal.,2019).
thatidentifiesfeasibleinterventions, leadingtooutcomes
Table3.ResultsonWeak-3DIdentandStrong-3DIdent(abbrevi- betteralignedwithdatadistribution. Wedemonstratethe
atedas“Weak”“Strong”forsimplicity).Forclarity,weuse”Non” effectivenessofourapproachindeeplearningcasestudies.
todenoteNonbacktracking. Future research will further investigate the use of natural
counterfactuals in vital areas like healthcare, economics,
Dataset - d h v γ α β b
andlaw.
Non 0.025 0.019 0.035 0.364 0.27 0.077 0.0042
Weak
Ours 0.024 0.018 0.034 0.349 0.221 0.036 0.0041
Non 0.100 0.083 0.075 0.387 0.495 0.338 0.0048
Stong
Ours 0.058 0.047 0.050 0.298 0.316 0.139 0.0047
8NaturalCounterfactualsWithNecessaryBacktracking
References Loshchilov,I.andHutter,F. Decoupledweightdecayreg-
ularization. In International Conference on Learning
Ash,R.B. Informationtheory. CourierCorporation,2012.
Representations,2018.
Barocas, S., Selbst, A. D., and Raghavan, M. The hid-
Lu, C., Huang, B., Wang, K., Herna´ndez-Lobato, J. M.,
denassumptionsbehindcounterfactualexplanationsand
principalreasons. InFAT,2020. Zhang,K.,andScho¨lkopf,B. Sample-efficientreinforce-
ment learning via counterfactual-based data augmenta-
Bizeul, A., Daunhawer, I., Palumbo, E., Scho¨lkopf, B., tion. arXivpreprintarXiv:2012.09092,2020.
Marx, A., and Vogt, J. E. 3didentbox: A toolbox for
identifiabilitybenchmarking. 2023. Maaløe,L.,Fraccaro,M.,Lie´vin,V.,andWinther,O. Biva:
Averydeephierarchyoflatentvariablesforgenerative
Boyd, S. P. and Vandenberghe, L. Convex optimization.
modeling. Advances in neural information processing
Cambridgeuniversitypress,2004.
systems,32,2019.
Conde,M.R. Necessitydefined: Anewroleinthecriminal
Monteiro, M., Ribeiro, F. D. S., Pawlowski, N., Castro,
defensesystem. UCLAL.Rev.,29:409,1981.
D. C., and Glocker, B. Measuring axiomatic sound-
Dash,S.,Balasubramanian,V.N.,andSharma,A. Evalu- ness of counterfactual image models. arXiv preprint
atingandmitigatingbiasinimageclassifiers: Acausal arXiv:2303.01274,2023.
perspectiveusingcounterfactuals. InProceedingsofthe
IEEE/CVFWinterConferenceonApplicationsofCom- Mothilal,R.K.,Sharma,A.,andTan,C. Explainingma-
puterVision,pp.915–924,2022. chinelearningclassifiersthroughdiversecounterfactual
explanations. InFAccT,2020.
Dehghani,M.,Iliev,R.,andKaufmann,S. Causalexplana-
tionandfactmutabilityincounterfactualreasoning.Mind Oberst,M.andSontag,D. Counterfactualoff-policyeval-
&Language,27(1):55–85,2012. uation with gumbel-max structural causal models. In
InternationalConferenceonMachineLearning,pp.4881–
Dhurandhar,A.,Chen,P.-Y.,Luss,R.,Tu,C.-C.,Ting,P.,
4890.PMLR,2019.
Shanmugam,K.,andDas,P. Explanationsbasedonthe
missing: Towardscontrastiveexplanationswithpertinent Pawlowski,N.,CoelhodeCastro,D.,andGlocker,B. Deep
negatives. InNeurIPS,2018. structuralcausalmodelsfortractablecounterfactualin-
ference. InNeurIPS,2020a.
Engstrom, L., Ilyas, A., Salman, H., Santurkar, S.,
and Tsipras, D. Robustness (python library),
Pawlowski,N.,CoelhodeCastro,D.,andGlocker,B. Deep
2019. URL https://github.com/MadryLab/
structuralcausalmodelsfortractablecounterfactualin-
robustness. License: MIT.
ference. Advances in Neural Information Processing
Gerstenberg,T.,Bechlivanidis,C.,andLagnado,D.A.Back Systems,33:857–869,2020b.
ontrack: Backtrackingincounterfactualreasoning. In
Pearl,J. Causality. Cambridgeuniversitypress,2009.
ProceedingsoftheAnnualMeetingoftheCognitiveSci-
enceSociety,volume35,2013.
Ribeiro, F. D. S., Xia, T., Monteiro, M., Pawlowski, N.,
Hassanpour,N.andGreiner,R.Learningdisentangledrepre- andGlocker,B. Highfidelityimagecounterfactualswith
sentationsforcounterfactualregression. InInternational probabilisticcausalmodels. 2023.
ConferenceonLearningRepresentations,2019.
Sanchez,P.andTsaftaris,S.A. Diffusioncausalmodelsfor
Hiddleston,E. Acausaltheoryofcounterfactuals. Nouˆs,39 counterfactualestimation. InFirstConferenceonCausal
(4):632–657,2005. LearningandReasoning.
Kingma, D. and Welling, M. Auto-encoding variational
Scho¨lkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalch-
bayes. InICLR,2014.
brenner, N., Goyal, A., and Bengio, Y. Toward causal
Kocaoglu,M.,Snyder,C.,Dimakis,A.G.,andVishwanath, representationlearning. ProceedingsoftheIEEE,109(5):
S.Causalgan:Learningcausalimplicitgenerativemodels 612–634,2021.
withadversarialtraining. InInternationalConferenceon
Schut,L.,Key,O.,McGrath,R.,Costabello,L.,Sacaleanu,
LearningRepresentations.
B.,Corcoran,M.,andGal,Y. Generatinginterpretable
Lewis, D. Counterfactual dependence and time’s arrow. counterfactualexplanationsbyimplicitminimisationof
Nouˆs,pp.455–476,1979. epistemicandaleatoricuncertainties. InAISTATS,2021.
9NaturalCounterfactualsWithNecessaryBacktracking
Verma,S.,Dickerson,J.P.,andHines,K.Counterfactualex-
planationsformachinelearning:Areview.arXivpreprint,
arXiv:2010.10596,2020.
vonKu¨gelgen,J.,Mohamed,A.,andBeckers,S. Backtrack-
ing counterfactuals. arXiv preprint arXiv:2211.00472,
2022.
Wachter,S.,Mittelstadt,B.,andRussell,C. Counterfactual
explanationswithoutopeningtheblackbox: Automated
decisions and the GDPR. Harvard Journal of Law &
Technology,2018.
10NaturalCounterfactualsWithNecessaryBacktracking
A.DatasetsandMoreExperimentalResults
Inthissection,wefirstprovidedetaileddatasetssettingsandadditionalexperimentalresults. Subsequently,wepresentthe
standarddeviationofallexperimentaloutcomesinSec.A.
A.1.ToyDatasets
Wedesignfoursimulationdatasets,Toy1-4,andusethedesignedSCMstogenerate10,000datapointsasatrainingdataset
andanother10,000datapointsasatestsetforeachdataset. Fig.6showscausalgraphsofToy1-4andscatterplotmatrices
oftestdatasetsineachdataset. Theground-truthSCMsofeachdatasetarelistedbelow.
Toy1.
n =u , u ∼N(0,1),
1 1 1
1
n =−n + u , u ∼N(0,1),
2 1 3 2 2
n =sin[0.25π(0.5n +n )]+0.2u , u ∼N(0,1),
3 2 1 3 3
wheretherearethreeendogenousvariables(n ,n ,n )andthreenoisevariables(u ,u ,u ). n istheconfounderofn
1 2 3 1 2 3 1 2
andn andn andn causesn .
3 1 2 3
Toy2.
n =u , u ∼N(0,1),
1 1 1
n =sin[0.2π(n +2.5)]+0.2u , u ∼N(0,1),
2 2 2 2
wheretherearethreeendogenousvariables(n ,n )andthreenoisevariables(u ,u ). n causesn .
1 2 1 2 1 2
Toy3.
n =u , u ∼N(0,1),
1 1 1
1
n =−n + u , u ∼N(0,1),
2 1 3 2 2
n =sin[0.1π(n +2.0)]+0.2u , u ∼N(0,1),
3 2 3 3
n =sin[0.25π(n −n +2.0)]+0.2u , u ∼N(0,1),
4 3 1 4 4
wheretherearethreeendogenousvariables(n ,n ,n ,n )andthreenoisevariables(u ,u ,u ,u ). n istheconfounder
1 2 3 4 1 2 3 4 1
ofn andn . (n ,n ,n )isachain,i.e.,n causesn ,followedbyn .
1 4 2 3 4 2 3 4
Toy4.
n =u , u ∼N(0,1),
1 1 1
1
n =−n + u , u ∼N(0,1),
2 1 3 2 2
n =sin[0.3π(n +2.0)]+0.2u , u ∼N(0,1),
3 2 3 3
wheretherearethreeendogenousvariables(n ,n ,n )andthreenoisevariables(u ,u ,u ). (n ,n ,n )isachain,i.e.,
1 2 3 1 2 3 1 2 3
n causesn ,followedbyn .
1 2 3
Table4.MAEResultsonToy1−4.Forsimplicity,weusedooperatorinthetabletosaveroom,andwhennaturalcounterfactualsare
referredto,domeanschange.
Dataset Toy1 Toy2 Toy3 Toy4
doorchange do(n ) do(n ) do(n ) do(n ) do(n ) do(n ) do(n ) do(n )
1 2 1 1 2 3 1 2
Outcome n n n n n n n n n n n n n
2 3 3 2 2 3 4 3 4 4 2 3 3
Nonbacktracking 0.477 0.382 0.297 0.315 0.488 0.472 0.436 0.488 0.230 0.179 0.166 0.446 0.429
Ours 0.434 0.354 0.114 0.303 0.443 0.451 0.423 0.127 0.136 0.137 0.158 0.443 0.327
11NaturalCounterfactualsWithNecessaryBacktracking
𝑛
1
𝑛 𝑛
2 3
(a) Toy1 (b) Toy1
𝑛 𝑛
1 2
(c) Toy2 (d) Toy2
𝑛
1
𝑛 𝑛 𝑛
2 3 4
(e) Toy3 (f) Toy3
𝑛 𝑛 𝑛
1 2 3
(g) Toy4 (h) Toy4
Figure6.CausalgraphsandScatterPlotMatricesofToy1-4.Figure(a)(c)(e)and(g)showcausalgraphsofToy1-4respectively.Figure
(b)(d)(f)and(h)indicatescatterplotmatricesofvariablesinToy1-4respectively.
12NaturalCounterfactualsWithNecessaryBacktracking
Inthemainpaper,wehaveexplainexperimentsonToy1indetails. AsshowninTable4,ourperformanceonToy2-4shows
bigmargincomparedwithnon-backtrackingcounterfactualssincenaturalcounterfactualsconsistentlymakeinterventions
feasible,whilepartofhardinterventionsmaynotfeasibleinnon-backtrackingcounterfactuals.
VisualizationResultsonToy1. InFig.7,largercounterfactualimagesaredisplayed,whichareidenticaltothoseshownin
Fig.2,withtheonlydifferencebeingtheirsize.
Evidence
4 Nonbacktracking 1.5
Ours
1.0
2
0.5
0 0.0
n 3=2.31 n 3=0.03
0.5
2 1.0 y=x
Nonbacktracking
1.5 Ours
4
4 3 2 1 0 1 2 3 4 1.5 1.0 0.5 0.0 0.5 1.0 1.5
n
1
x: n3's Groundtruth
(a) Outcomeerroronasinglesample (b) Groudtruth-PredictionScatterPlot
Figure7. TheVisualizationResultsonToy1.
A.2.MorphoMNIST
𝑡
𝑖 𝑥
(a) CausalGraph (b) Samples (c) ScatterPlotMatrix
Figure8. CausalGraphandsamplesofMorpho-MNIST.
The MorphoMNIST comes from (Pawlowski et al., 2020b), where there are 60000 images as training set and 10,000
imagesastestdataset. Fig.8(a)showsthecausalgraphforgeneratingMorpho-MNIST;specifically,strokethicknesst
causesthebrightnessintensityi,andboththicknesstandintensityicausethedigitx. Fig.8(b)showsomesamplesfrom
Morpho-MNIST.Theground-truthSCMstogeneratetheMorpho-MNISTareasfollows:
t=0.5+u , u ∼Γ(10,5),
t t
i=191·σ(0.5·u +2·t−5)+64, u ∼N(0,1),
i i
x=SetIntensity(SetThickness(u ;t);i), u ∼MNIST,
x x
whereu ,u ,andu arenoisevariables,andσisthesigmoidfunction. SetThickness(·;t)andSetIntensity(·;i)arethe
t i x
operationstosetanMNISTdigitu ’sthicknessandintensitytoiandtrespectively,andxisthegeneratedimage.
x
QuantitativeResultsofchange(i)ordo(i). WeuseV-SCMtodocounterfactualtaskofchange(i)(whereϵ=10−3)or
do(i)withmultiplerandomseedsontestset. InTable5,thefirstcolumnshowstheMAEof(t,i),indicatingourresults
outperformthatofnon-backtracking,sinceourapproachconsistentlydeterminefeasibleinterventions.
13
n
2
noitciderP
s'3n
:yNaturalCounterfactualsWithNecessaryBacktracking
Table5. MorphoMNISTresultsofchange(i)ordo(i)usingV-SCM
IntersectionbetweenOursandNB (NCO=1,NB=1) (NCO=1,NB=0) (NCO=0,NB=1) (NCO=0,NB=0)
NumberofIntersection 5865 3159 0 975
t’sMAE 0.283 0.159 0.460 0.000 0.450
Nonbacktracking
i’sMAE 6.56 3.97 8.95 0.000 14.3
t’sMAE 0.164 0.160 0.171 0.000 0.466
Ours
i’sMAE 4.18 4.01 4.49 0.000 14.1
The Effectiveness of Feasible Intervention Optimization (FIO). Next, we focus on the rest four-column results. In
bothtypesofcounterfactuals,weusethesamevalueiindo(i)andchange(i). Hence,afterinference,weknowwhich
image satisfies ϵ-natural generation in the two types of counterfactuals. In ”NC=1” of the table, NC indicates the set
of counterfactuals after feasible intervention optimization. Notice that NC set does not mean the results of natural
counterfactuals, since some results do still not satisfy ϵ-natural generation after FIO. “NC=1” mean the set containing
data points satisfying ϵ-natural generation and “NC=0” contains data not satisfying ϵ-natural generation after feasible
interventionoptimization. Similarly,“NB=1”meansthesetcontainingdatapointssatisfyingnaturalnesscriteria. (NC=1,
NB=1)presentstheintersectionof“NC=1”and“NB=1”. Similarlogicisadoptedtotheotherthreecombinations. The
numberofcounterfactualdatapointsare10,000intwotypesofcounterfactuals.
In (NC=1, NB=1) containing 5865 data points, our performance is similar to the non-backtracking, showing feasible
interventionoptimizationtendstobacktrackaslessaspossiblewhenhardinterventionshavesatisfiedϵ-naturalgeneration.
In(NC=1,NB=0),thereare3159datapoints,whichare“unnatural”pointsinnon-backtrackingcounterfactuals. After
naturalcounterfactualoptimation,thishugeamountofdatapointsbecomes“natural”. Here,ourapproachsignificantly
reduces errors, achieving a 62.8% reduction in thickness t and 49.8% in intensity i, the most substantial improvement
amongthefoursetsinTable5. Thenumberofpointsin(NC=0,NB=1)iszero,showingthestabilityofouralgorithmsince
ourFIOframeworkwillchangethehard,feasibleinterventionintounfeasibleintervention. Twotypesofcounterfactuals
performsimilarlyintheset(NC=0,NB=0),alsoshowingthestabilityofourapproach.
VisualizationofCounterfactualImages. Fig.9showscounterfactualimages(secondrow),basedontheevidenceimages
(first row), with intended changes on i. The third row illustrates the differences between evidence and counterfactual
images. Focusing on the first counterfactual image from non-backtracking and natural counterfactuals respectively, in
non-backtracking, despite do(i) where thickness value 4.2 should remain unchanged, the counterfactual image shows
reducedthickness,consistentwiththemeasuredcounterfactualthicknessof2.6. Incontrast,naturalcounterfactualsyield
an estimated counterfactual thickness (t∗ in MS) closely matching original counterfactual thickness (t∗ in CF), due to
backtrackingforafeasibleinterventionontheearliercausalvariablet,therebymaintaining(t,i)withinthedatadistribution.
Observingotherimagesalsoshowslargererrorsinnon-backtrackingcounterfactualimages.
A.3.3DIdentBOX
Table6.Detailsofvariablesin3DIndentBOX.Objectreferstoteapotineachimage.Thesupportofeachvariableis[−1,1].Thereal
visualrangearelistedinthecolumnVisualRange.
InformationBlock Variables Support Description VisualRange
x [-1,1] Objectx-coordinate -
Position y [-1,1] Objecty-coordinate -
z [-1,1] Objectz-coordinate -
γ [-1,1] Spotlightrotationangle [0◦,360◦]
Rotation α [-1,1] Objectα-rotationangle [0◦,360◦]
β [-1,1] Objectβ-rotationangle [0◦,360◦]
Hue b [-1,1] BackgroundHSVcolor [0◦,360◦]
The3DIdentBOXdatasets,firstintroducedinBizeuletal.(2023),comewithofficialcodeforgeneratingcustomizedversions
ofthesedatasets. TheyconsistofimagescreatedwithBlender,eachdepictingateapotwithsevenattributes,suchasposition,
rotation,andhue,determinedbysevenground-truthvariables.
14NaturalCounterfactualsWithNecessaryBacktracking
F:t=4.7;i=252 F:t=1.2;i=87 F:t=1.1;i=74 F:t=4.8;i=252 F:t=1.1;i=70 F:t=1.2;i=85 F:t=5.6;i=254 F:t=5.4;i=254
CF:t*=4.7;i*=82 CF:t*=1.2;i*=224 CF:t*=1.1;i*=179 CF:t*=4.8;i*=110 CF:t*=1.1;i*=194 CF:t*=1.2;i*=215 CF:t*=5.6;i*=97 CF:t*=5.4;i*=97
MS:t*=2.6;i*=96 MS:t*=2.3;i*=185 MS:t*=1.9;i*=130 MS:t*=2.9;i*=100 MS:t*=2.1;i*=153 MS:t*=2.0;i*=161 MS:t*=3.1;i*=106 MS:t*=2.6;i*=108
t=2.1; i=14 t=1.1; i=39 t=0.7; i=49 t=2.0; i=10 t=1.0; i=41 t=0.8; i=54 t=2.4; i=9 t=2.8; i=11
(a) ResultsofNon-backtrackingCounterfactuals
F:t=4.7;i=252 F:t=1.2;i=87 F:t=1.1;i=74 F:t=4.8;i=252 F:t=1.1;i=70 F:t=1.2;i=85 F:t=5.6;i=254 F:t=5.4;i=254
CF:t*=1.4;i*=82 CF:t*=3.1;i*=224 CF:t*=2.7;i*=179 CF:t*=2.3;i*=110 CF:t*=3.1;i*=194 CF:t*=3.0;i*=215 CF:t*=1.7;i*=97 CF:t*=1.6;i*=97
MS:t*=1.8;i*=76 MS:t*=3.2;i*=218 MS:t*=2.7;i*=174 MS:t*=2.4;i*=106 MS:t*=3.1;i*=192 MS:t*=2.9;i*=209 MS:t*=2.0;i*=89 MS:t*=2.1;i*=97
t=0.4; i=6 t=0.1; i=6 t=0.0; i=5 t=0.2; i=4 t=0.0; i=2 t=0.1; i=6 t=0.3; i=8 t=0.5; i=0
(b) ResultsofNaturalCounterfactuals
Figure9. VisualizationResultsonMorphoMNIST:“F”standsforfactualvalues,“CF”forcounterfactualvalues,and“MS”forestimated
counterfactualvaluesof(t,i). (∆t,∆i)representstheabsoluteerrorsbetweencounterfactualandestimatedcounterfactualvaluesof
(t,i).
Inourexperimentwiththe3DIdentBOX,whichcomprisessixdatasets,wefocusonthepositions-rotations-huedataset.
Weexpandthisintotwodatasets,Weak-3DIdentandStrong-3DIdent. Eachdatasetincludessevenvariables,besidesthe
imagevariablex,withspecificsoutlinedinTable6. Everyimagefeaturesateapot,withvariablescategorizedintothree
groups: positions(x,y,z),rotations(γ,α,β),andhueb,representingseventeapotattributes,asdepictedin11(a). Fig.11
(b)illustratesthatbothdatasetssharethesamecausalgraph. Thedistributionsofseveralparentvariablesofimagexinthese
datasetsaredetailedinTable7.
VisualizationonStrong-3DIdent. InFig.10, largercounterfactualimagesaredisplayed, whichareidenticaltothose
showninFig.5,withtheonlydifferencebeingtheirsize.
A.4.StandardDeviationofExperimentalResults
Thissectionpresentsthestandarddeviationofallexperimentalresults,demonstratingthatthestandarddeviationforour
naturalcounterfactualsisgenerallylower. Thisindicatestheincreasedreliabilityofourapproach,achievedbynecessary
backtrackingtoensurecounterfactualsremainwithindatadistributions.
15NaturalCounterfactualsWithNecessaryBacktracking
Table7.DistributionsinWeak-3DIdentandStrong-3DIdent.N (y,1)referstoanormaldistributiontruncatedtotheinterval[−1,1]
wt
andN (y,1)meansanormaldistributiontruncatedtotheinterval[min(1,y+0.2),max(−1,y−0.2)],whereminandmaxindicate
st
operations that select smaller and bigger values respectively. N (α,1) and N (α,1) are identical to N (y,1) and N (y,1)
wt st wt st
respectively.U referstouniformdistribution.
Variables Weak-3DIdentDistribution Strong-3DIdentDistribution
c=(x,y,z) c∼(N (y,1),U(−1,1),U(−1,1)) c∼(N (y,1),U(−1,1),U(−1,1))
wt st
s=(γ,α,β) s∼(N (α,1),U(−1,1),N t(z,1)) s∼(N (α,1),U(−1,1),N (z,1))
wt w st st
b b∼U(−1,1) b∼U(−1,1)
Table8. StandardDeviationofResultsonToy1−4.
Dataset Toy1 Toy2 Toy3 Toy4
doorchange do(n ) do(n ) do(n ) do(n ) do(n ) do(n ) do(n ) do(n )
1 2 1 1 2 3 1 2
Outcome n n n n n n n n n n n n n
2 3 3 2 2 3 4 3 4 4 2 3 3
Nonbacktracking0.001840.006280.004320.001640.004480.006860.00495 0.0112 0.005560.001420.0005140.006230.00238
Ours 0.004090.006840.002950.001910.001160.004610.002010.005040.005310.001550.0002350.005180.00143
Table9. StandardDeviationofResultsonMorphoMNIST
IntersectionbetweenOursandNB (NCO=1,NB=1) (NCO=1,NB=0) (NCO=0,NB=1) (NCO=0,NB=0)
NumberofIntersection 39.84 81.43 0.00 54.14
t’sMAE 0.00322 0.00172 0.00670 0.000 0.0178
Nonbacktracking
i’sMAE 0.0496 0.0596 0.0508 0.000 0.110
t’sMAE 0.00137 0.00222 0.00157 0.000 0.0149
Ours
i’sMAE 0.0359 0.0551 0.0157 0.000 0.0853
Table10. StandardDeviationofAblationStudyonϵ
do(t) do(i)
Model ϵ CFs
t i t i
- NB 0.000512 0.0172 0.00322 0.0496
10−4 0.00159 0.0210 0.00183 0.0561
V-SCM
10−3 Ours 0.00124 0.0217 0.00137 0.0359
10−2 0.000954 0.0382 0.000868 0.0556
- NB 0.000915 0.0229 0.000832 0.0245
10−4 0.000920 0.0178 0.000922 0.0138
H-SCM
10−3 Ours 0.000611 0.0206 0.000289 0.0264
10−2 0.000787 0.0244 0.000431 0.0258
Table11. StandardDeviationofResultsonWeak-3DIdentandStong-3DIdent
Dataset Counterfactuals d h v γ α β b
Nonbacktracking 3.68e-05 0.000133 0.000226 0.00422 0.00310 0.00357 1.29e-05
Weak-3DIdent
Ours 4.27e-05 7.22e-05 0.000249 0.00558 0.00278 0.00136 3.33e-05
Nonbacktracking 0.00233 0.000864 0.00127 0.00933 0.00307 0.00452 1.49e-05
Stong-3DIdent
Ours 0.00166 0.000774 0.000229 0.00908 0.00955 0.00816 2.97e-05
B.ProofforTheorem4.1
Theorem1fromLuetal.(2020)detailsidentifiablecounterfactualsunderacertaincondition:
TheoremB.1(IdentifiableCounterfactuals). SupposeV satisfiesthefollowingstructuralcausalmodel:
i
V :=f (PA ,U )
i i i i
16NaturalCounterfactualsWithNecessaryBacktracking
d=0.26; h=0.24; d=0.07; h=0.06; d=0.62; h=0.40; d=0.02; h=0.04; d=0.40; h=0.14;
v=0.23; =1.02; v=0.09; =0.33; v=0.08; =0.96; v=0.58; =1.12; v=0.70; =1.46;
=1.29; =1.35 =1.51; =1.80 =0.00; =1.69 =1.50; =1.15 =1.58; =0.13
(a) ResultsofNon-backtrackingCounterfactuals
d=0.01; h=0.04; d=0.11; h=0.04; d=0.01; h=0.02; d=0.01; h=0.04; d=0.04; h=0.01;
v=0.09; =0.08; v=0.06; =0.07; v=0.02; =0.01; v=0.02; =0.20; v=0.04; =0.12;
=0.41; =0.91 =0.09; =0.29 =0.04; =0.30 =0.18; =0.02 =0.12; =0.05
(b) ResultsofNaturalCounterfactuals
Figure10. VisualizationResultsonStong-3DIdent.
whereU ⊥PA andassumeunknownf issmoothandstrictlymonotonicw.r.t. U forfixedvaluesofPA ,Ifwehave
i i i i i
observedV =v andPA =pa ,withaninterventiondo(PA =pa∗),thecounterfactualoutcomeisidentifiable:
i i i i i i
V |do(PA =pa∗),V =v ,PA =pa (6)
i i i i i i i
WeusethistheoremtosupportourproofforTheorem4.1,whichextendsTheoremB.1toabroaderrangeofidentifiable
counterfactuals.
Proof. Weinitiallyassumethepresenceofcompleteevidence,meaningE=V. InlinewithTheoremB.1from(Luetal.,
2020),counterfactualoutcomeofB ∈BisidentifiablewhengiventhecounterfactualactionPA =pa∗ ,alongside
k Bk Bk
theobservablevaluesB =b andPA =pa ,whereB isanyvariableinBandPA istheparentsetofB .
k k Bk Bk k Bk k
With this full evidence assumption, pa∗ is conclusively determined by the complete evidence and the intervention
do(C=c∗),whereaspa andB
=bBk
areascertainedfromthefullevidence. Thisensuresthecounterfactualoutcome
Bk k k
ofB ,asinglevalue,isidentifiable. Extendingthislogic,thecounterfactualoutcomeofBasawhole,alsoasinglevalue,
k
isidentifiable. Consequently,p(B|do(C=c∗),E=e)isidentifiablewithasingularvalueasitssupport.
Next, we consider the case of partial evidence, where E ⊂ V. Given that p(B|do(C = c∗),E = e) = (cid:82) p(B|V =
v,do(C = c∗))p(V = v|E = e)dV and both terms p(B|V = v,do(C = c∗)) (treat V = v as full evidence) and
p(V=v|E=e)areidentifiable,itfollowsthatp(B|do(C=c∗),E=e)isalsoidentifiable.
17NaturalCounterfactualsWithNecessaryBacktracking
𝑏
d=0.85;h=0.61; d= 0.01;h=0.19;
v= 0.19; =0.60; v=0.86; = 0.57;
=0.38; = 0.61; = 0.79; =0.46; 𝑣 𝛽
b=-0.76 b=-0.29
ℎ 𝛼
𝑥
𝑑 𝛾
d=0.27;h=0(.7a2);Sampdle=s0.74;h=0.25; (b) CausalGraph
v= 0.41; =0.68; v=0.20; = 0.38;
=0.92; = 0.15; = 0.83; =0.50;
b=-0.18 b=0.58
(c) Weak-3DIdent (d) Strong-3DIdent
Figure11. Samples,CausalGraph,ScatterPlotMatricesofWeak-3DIdentandStrong-3DIdent.
C.ModelTraining
Ourstudyfocusesoncounterfactualinferenceandwedirectlyusetwostate-of-the-artdeep-learningSCMmodelstolearn
SCMsamongvariablesusingadataset,i.e.,D-SCM(Pawlowskietal.,2020b)andH-SCM(Ribeiroetal.,2023).Specifically,
weusecodeof(Ribeiroetal.,2023)containingtheimplementationofD-SCMandH-SCM.TakeMorpho-MNISTasan
example,inbothtwomodels,normalizingflowsarefirstlytrainedtolearncausalmechanismsforallvariablesexceptimage
x,i.e.,(t,i),andaconditionalVAEisusedtomodelimagexgivenitsparents(t,i). ForD-SCM,theconditionalVAE
usesnormalVAEframework,whileH-SCMuseshierarchicalVAEstructure(Maaløeetal.,2019)tobettercapturethe
distributionofimages.
ToyExperiments. Inthecaseoffourtoyexperiments,weexclusivelyemployednormalizingflowsduetothefactthatall
variablesareone-dimensional. Ourtrainingregimenfortheflow-basedmodelspanned2000epochs,utilizingabatchsize
of100inconjunctionwiththeAdamWoptimizer(Loshchilov&Hutter,2018). Weinitializedthelearningrateto1×10−3,
setβ to0.9,β to0.9.
1 2
MorphoMNIST.Wefirsttrainnormalizedflowstolearncausalmechanismsofthicknessandintensity(t,i). Otherhyper-
parametersaresimilartothoseoftoyexperiments. Then,wetraintwoVAE-basedmodels(D-SCMandH-SCM)tolearnx
given(t,i)respectively. Thearchitecturesofthetwomodelsareidenticalto(Ribeiroetal.,2023). D-SCMandH-SCM
underwenttrainingfor160epochs. Weemployedabatchsizeof32andutilizedtheAdamWoptimizer. Theinitiallearning
ratewassetto1e−3andunderwentalinearwarmupconsistingof100steps. Wesetβ to0.9,β to0.9,andapplieda
1 2
weightdecayof0.01. Furthermore,weimplementedgradientclippingatathresholdof350andintroducedagradientupdate
skippingmechanism,withathresholdsetat500basedontheL2norm. Duringthetesting,i.e.,counterfactualinference,we
testperformanceonbothmodelsrespectively,withthenormalizedflows.
3DIdentBOX.SimilartoexperimentsonMorphoMNIST,wefirsttrainnormalizedflows. ComparedwithD-SCM,H-SCM
ismorepowerfultomodelcomplexdatalike3DIdentBOX,ofwhichthesizeoftheimageis64×64×4. Then,wetrain
H-SCMtocapturethedistributionofxgivenitsparentsfor500epochswithabatchsizeof32. Thehyper-parametersare
thesameasexperimentsonMorphoMNIST.
18NaturalCounterfactualsWithNecessaryBacktracking
D.FeasibleInterventionOptimization
OurlossfunctionforFeasibleInterventionOptimizationisdefinedas:
(cid:88)
L=∥an(A)−an(A)∗∥ +w [max(ϵ−F(V =v∗|pa∗),0)+max(ϵ+F(V =v∗|pa∗)−1,0)]
1 ϵ j j j j j j
j (7)
s.t. A=a∗
BasedonTheorem4.1,Choice(3)ofnaturalnessconstraintsisequivalenttoChoice(2),asF(V =v∗|pa∗)=F(U =u∗)
j j j j j
withv∗ =f (pa∗,u∗). Hence,thelosscanberewrittenas:
j j j j
(cid:88)
L=∥an(A)−an(A)∗∥ +w [max(ϵ−F(u∗),0)+max(ϵ+F(u∗)−1,0)]
1 ϵ j j
j (8)
s.t. u∗ =f−1(a∗,pa∗ )
A A A
Notethatu isnotexplicitlyoptimizedasitsvalue,pa ,isdeterminedbyothernoisevaluesandtheconstanta∗through
A A
thereversiblefunctionf−1. Consequently,u isindirectlydeterminedbyothernoisevaluesanda∗. Theloss’sparameter
A A
isthusu∗ ,whichfullydeterminesthevaluean(A)∗usingthelearnedSCM,asexplainedinSec.C.
AN
Inallexperiments,weoptimizedu∗ usingtheAdamWoptimizeratalearningrateof1e−3for50,000steps. This
AN
approach’seffectivenessisvalidatedbytheMorphoMNISTexperiments.
E.StandardProcedureofNaturalCounterfactuals
In this section, we outline our natural counterfactuals’ standard procedure, assuming full evidence. Through Feasible
InterventionOptimization,weobtaintheoptimalvaluean(A)∗.Thisidentifiesthechangedvariables,C,whichwetargetfor
intervention,enablingthedefinitionofthefeasibleinterventiondo(C=c∗),withc∗asthepost-optimizationcounterfactual
valuesofC. Then,wecandonaturalcounterfactualsgiventhefeasibleinterventiondo(C=c∗). Accordingly,weprovide
athree-stepstandardprocedureforourgeneralnaturalcounterfactuals:
TheoremE.1(NaturalCounterfactuals). Withthemodel<M,p(U)>andϵ,theprobabilityp(B |e)ofacounterfactual
A
condition”GiventheevidenceE=e,ifA’svalueischangedintoa∗duetothefeasibleinterventionsoncausallyearlier
variables,thenB,”canbepredictedbythefollowingthreesteps.
(1) Abduction: Givenevidencee,obtainupdateddistributionp(U|e).
(2) Action: Determinefeasibleinterventiondo(C=c∗)byFeasibleInterventionOptimizationtoachievechange(A=
a∗)andmodifythemodelintoM bycuttingoffthelinkbetweenCandtheirparents.
C
(2) Prediction:ComputetheprobabilityofB,thecounterfactualconsequence,usingtheupdatedmodel<M ,p(U|e)>.
C
F.DifferencesbetweenNaturalCounterfactualsandNon-BacktrackingCounterfactuals(Pearl,
2009)orPrior-BasedBacktrackingCounterfactuals(vonKu¨gelgenetal.,2022)
F.1.DifferencesbetweenNon-backtrackingCounterfactualsandOurs
Non-backtrackingcounterfactualsonlydoadirectinterventionontargetvariableA,whileournaturalcounterfactualsdo
backtrackingwhenthedirectinterventionisinfeasible. NoticethatwhenthedirectinterventiononAisalreadyfeasible,our
procedureofnaturalcounterfactualswillbeautomaticallydistilledtothenon-backtrackingcounterfactuals. Inthissense,
non-backtrackingcounterfactualreasoningisourspecialcase.
F.2.DifferencesbetweenPrior-BasedBacktrackingCounterfactualsandOurs
(1)InterventionApproachandResultingChanges:
Prior-basedBacktrackingCounterfactuals: Thesecounterfactualsdirectlyinterveneonnoise/exogenousvariables,which
canleadtounnecessarychangesinthecounterfactualworld. Consequently,thesimilaritybetweentheactualdatapointand
19NaturalCounterfactualsWithNecessaryBacktracking
itscounterfactualcounterparttendstobelower. Inshort,prior-basedbacktrackingcounterfactualsmayintroducechanges
thatarenotneeded.
Natural Counterfactuals: In contrast, our natural counterfactuals only engage in necessary backtracking when direct
interventionisinfeasible. Thisapproachaimstoensurethatthecounterfactualworldresultsfromminimalalterations,
maintainingahigherdegreeoffidelitytotheactualworld.
(2)CounterfactualWorlds:
Prior-basedBacktrackingCounterfactuals: Thisapproachassignsvaryingweightstothenumerouspotentialcounterfactual
worldscapableofeffectingthedesiredchange. Theweightassignedtoeachworldisdirectlyproportionaltoitssimilarityto
theactualworld. itisworthnotingthatamongthisarrayofcounterfactualworlds,somemayexhibitminimalresemblance
totheactualworld,evenwhenequippedwithcompleteevidence,includingthevaluesofallendogenousvariables. This
divergencearisesbecausebysamplingfromtheposteriordistributionofexogenousvariables,evenhighlydissimilarworlds
maystillbedrawn.
NaturalCounterfactuals: Incontrast,ournaturalcounterfactualsprioritizetheconstructionofcounterfactualworldsthat
closelyemulatethecharacteristicsoftheactualworldthroughanoptimizationprocess. Asaresult,inmostinstances,one
actualworldcorrespondstoasinglecounterfactualworldwhenemployingnaturalcounterfactualswithfullevidence.
(3)ImplementationPracticality:
Prior-basedBacktrackingCounterfactuals: Thepracticalimplementationofprior-basedbacktrackingcounterfactualscanbe
adauntingchallenge. Todate,wehavebeenpreventedfromconductingacomparativeexperimentwiththisapproachdueto
uncertaintyaboutitsfeasibilityinpracticalapplications. Amongothertasks,thecomputationoftheposteriordistribution
ofexogenousvariablescanbeacomputationallyintensiveendeavor. Furthermore,itisworthnotingthatthepaper(von
Ku¨gelgenetal.,2022)providesonlyrudimentaryexampleswithoutpresentingacomprehensivealgorithmoraccompanying
experimentalresults.
NaturalCounterfactuals: Instarkcontrast,ournaturalcounterfactualshavebeenmeticulouslydesignedwithpracticality
at the forefront. We have developed a user-friendly algorithm that can be applied in real-world scenarios. Rigorous
experimentation,involvingfoursimulationdatasetsandtwopublicdatasets,hasconfirmedtheefficacyandreliabilityofour
approach. Thisextensivevalidationunderscorestheaccessibilityandutilityofouralgorithmfortacklingspecificproblems,
makingitavaluabletoolforpracticalapplications.
G.ObservationsaboutthePrior-BasedBacktrackingCounterfactuals(vonKu¨gelgenetal.,2022)
G.1.PossibilityofGratuitousChanges
A theory of backtracking counterfactuals was recently proposed by (von Ku¨gelgen et al., 2022), which utilizes a prior
distributionp(U,U∗)toestablishaconnectionbetweentheactualmodelandthecounterfactualmodel. Thisapproach
allowsforthegenerationofcounterfactualresultsunderanyconditionbyconsideringpathsthatbacktracktoexogenous
noisesandmeasuringclosenessintermsofnoiseterms. Asaresult,foranygivenvaluesofE = eandA∗ = a∗,itis
possibletofindasampledvalue(U=u,U∗ =u∗)fromp(U,U∗)suchthatE =eandA∗ =a∗,asdescribed
M(u) M∗(u∗)
in (von Ku¨gelgen et al., 2022). This holds true even in cases where V\E = ∅ and V∗ \A∗ = ∅, implying that any
combinationofendogenousvaluesE = eandA∗ = a∗ canco-occurintheactualworldandthecounterfactualworld,
respectively. Inessence,therealwaysexistsapath(v →− u →− u∗ →− v∗)thatconnectsV = vandV∗ = v∗ througha
value(U=u,U∗ =u∗),wherevandv∗representanyvaluessampledfromp (V)andp (V∗),respectively.
M M∗
However,thankstothisfeature,thisunderstandingofcounterfactualsmayallowforwhatappearstobegratuitouschanges
in realizing a counterfactual supposition. This occurs when there exists a value assignment U∗ = u∗ that satisfies
E∗ =eandA∗ =a∗inthesameworld. Insuchacase,intuitivelyweoughttoexpectthatE∗ =eshouldbe
M∗(u∗) M∗(u∗)
maintainedinthecounterfactualworld(asinthefactualone). However,thereisingeneralapositiveprobabilityforE∗ ̸=e.
Thisisduetotheexistenceofatleastone“path”fromE=etoanyvaluev∗sampledfromp (V∗|A∗ =a∗)bymeans
M∗
ofatleastonevalue(U=u,U∗ =u∗),allowingE∗totakeanyvalueinthesupportofp (E∗|A∗ =a∗).
M∗
In the case where A∗ = ∅, an interesting observation is that E∗ can take any value within the support of p (E∗).
M∗
Furthermore,whenexaminingtheupdatedexogenousdistribution,wefindthatinPearl’snon-backtrackingframework,itis
givenbyp (U∗|E∗ =e). However,in(vonKu¨gelgenetal.,2022)’sbacktrackingframework,theupdatedexogenous
M∗
20NaturalCounterfactualsWithNecessaryBacktracking
distributionbecomesp (U∗|E = e) = (cid:82) p(U∗|U)p (U|E = e)d(U) ̸= p (U∗|E∗ = e),sinceusingu∗ sampled
B M M∗
fromp(U∗|U=u)(whereuisanyvalueofU)canresultinanyvalueofallendogenousvariablesV∗. Therefore,(von
Ku¨gelgenetal.,2022)’sbacktrackingcounterfactualdoesnotreducetoPearl’scounterfactualevenwhenA∗ =∅.
G.2.IssueswiththeDistanceMeasure
In Equation 3.16 of (von Ku¨gelgen et al., 2022), Mahalanobis distance is used for real-valued U ∈ Rm, defined as
d(u∗,u)= 1(u∗−u)TΣ−1(u∗−u). However,itshouldbenotedthattheexogenousvariablesarenotidentifiable. There
2
areseveralissueswithusingtheMahalanobisdistanceinthiscontext.
Firstly,selectingdifferentexogenousdistributionswouldresultindifferentdistances. Thislackofidentifiabilitymakesthe
distancemeasuresensitivetothechoiceofexogenousdistributions.
Secondly,differentnoisevariablesmayhavedifferentscales. ByusingtheMahalanobisdistance,thevariableswithlarger
scaleswoulddominatethedistributionchanges,whichmaynotaccuratelyreflectthechangesineachvariablefairly.
Thirdly,eveniftheMahalanobisdistanced(u∗,u)isverycloseto0,itdoesnotguaranteethatthevaluesoftheendogenous
variablesaresimilar. ThismeansthattheMahalanobisdistancealonemaynotcapturethesimilarityordissimilarityofthe
endogenousvariablesadequately.
H.AnotherTypeofMinimalChange: MinimalChangeinLocalCausalMechanisms.
Changesinlocalmechanismsarethepricewepaytodointerventions,sinceinterventionsarefromoutsidethemodeland
sometimesareimposedonamodelbyus. Hence,weconsiderminimalchangeinlocalcausalmechanismsinA’sancestor
setAN(A). WithL1norm,thetotaldistanceofmechanismsinAN(A),calledmechanismdistance,isdefinedas:
(cid:88)
D(u ,u )= w ||F(u )−F(u∗)||
an(A) an(A)∗ j j j 1 (9)
j
whereu isthevalueofA’sexogenousancestorsetU whenAN(A)=an(A)intheactualworld. u is
an(A) AN(A) an(A)∗
thevalueofU whenAN(A)=an(A)∗inthecounterfactualworld. D(u ,u )representsthedistance
AN(A) an(A) an(A)∗
between actual world and counterfactual world. w represents a fixed weight, and F(·) is the Cumulative Distribution
j
Function(CDF).WeemploytheCDFofnoisevariablestonormalizedistancesacrossvariousnoisedistributions,ensuring
thesedistancesfallwithintherangeof[0,1],asnoisedistributionsarenotidentifiable.
WeightsintheDistance. Thenoisevariablesareindependentofeachotherandthus,unlikeinperceptiondistance,the
changeofcausalearliernoisevariableswillnotleadthechangeofcausallaternoisenodes. Therefore,iftheweighton
differenceofeachnoisevariableisthesame,thedistancewillnotpreferlesschangeonvariablesclosertoA. Toachieve
backtrackingaslessaspossible,wesetaweightw foreachnodeU ,definedasthenumberofendogenousdecedentsof
j j
V denotedasND(V ). Generallyspeaking,forallvariablescausallyearlierthanA,onewayistousethenumberof
j j
variablesinfluencedbyparticularinterventionasthemeasureofthechangescausedbytheintervention. Hence,thenumber
ofvariablesinfluencedbyavariable’sinterventioncanbetreatedasthecoefficientofdistance. Forexample,inacausal
graphwherewhereBcausesAandCistheconfounderofAandB. Ifchange(A=a∗),u ’sandu ’sweightis1and
A B
2respectively. Inthisway,variables(e.g.,u )withbiggerinfluenceonothervariablespossessbiggerweightsandthustend
B
tochangeless,reflectingnecessarybacktracking.
H.1.ConcretizationofNaturalCounterfactuals: AnExampleMethodology
AMethodBasedonMechanismDistance. WepluginmechanismdistanceEqn.9intoFIOframeworkEqn.2. Belowis
theequationofoptimization:
(cid:88)
min w ||F(u )−F(u∗)||
j j j 1
uan(A)∗
j
(10)
s.t. a∗ =f (pa∗ ,u∗ )
A A A
s.t. ϵ<F(u∗)<1−ϵ,∀u∗ ∈u
j j an(A)∗
Wherethefirstconstraintistoachievechange(A=a∗),thesecondconstraintrequirecounterfactualdatapointtosatisfy
ϵ-natural generation given the optional naturalness criteria (3) in Sec. 4.2, and the optimization parameter is the value
21NaturalCounterfactualsWithNecessaryBacktracking
u ofnoisevariablesetU givenAN(A) = an(A)∗. Forsimplicity,weuseAassubscriptasindicatorof
an(A)∗ AN(A)
terms related to A, instead of number subscript. In practice, the Lagrangian method is used to optimize our objective
function. Thelossisasbelow:
(cid:88)
L= w ||F(u )−F(u∗)||
j j j 1
j
(cid:88)
+w max((ϵ−F(u∗),0)+max(ϵ+F(u∗)−1,0)) (11)
ϵ j j
j
s.t. u∗ =f−1(a∗,pa∗ )
A A A
Inthe nextsection, weuseEqn. 11forfeasibleintervention optimizationacross multiplemachinelearningcase
studies,showingthatmechanismdistanceisaseffectiveasperceptiondistance,asdiscussedinthemainpaper.
H.2.CaseStudy
H.2.1.MORPHOMNIST
Table12. MorphoMNISTresultsofchange(i)ordo(i)usingV-SCM
IntersectionbetweenOursandNB (NCO=1,NB=1) (NCO=1,NB=0) (NCO=0,NB=1) (NCO=0,NB=0)
NumberofIntersection 5841 3064 0 1094
t’sMAE 0.286 0.159 0.462 0.000 0.471
Nonbacktracking
i’sMAE 6.62 4.00 8.88 0.000 14.2
t’sMAE 0.175 0.159 0.206 0.000 0.471
Ours
i’sMAE 4.41 4.00 5.19 0.000 14.2
Inthissection,westudytwotypesofcounterfactualsonthedatasetcalledMorphoMNIST,whichcontainsthreevariables
(t,i,x). FromthecausalgraphshowninFig.12(a),t(thethicknessofdigitstroke)isthecauseofbothi(intensityofdigit
stroke)andx(images)andiisthedirectcauseofx. Fig.12(b)showsasamplefromthedataset. Thedatasetcontains
60000imagesasthetrainingsetand10000asthetestset.
WefollowtheexperimentalsettingsofsimulationexperimentsinSec.6.1,exceptfortwodifferences. Oneisthatweuse
twostate-of-the-artdeeplearningmodels,namelyV-SCM(Pawlowskietal.,2020b)andH-SCM(Ribeiroetal.,2023),as
thebackbonestolearncounterfactuals. Theyusenormalizingflowstolearncausalrelationshipsamongx’sparentnodes,
e.g.,(t,i)inMorphoMNIST.Further,tolearnp(x|t,i),noticethatV-SCMusesVAE(Kingma&Welling,2014)andHVAE
(Maaløeetal.,2019). Anotherdifferenceisthat,insteadofestimatingtheoutcomewithMAE,wefollowthesamemetric
calledcounterfactualeffectivenessinRibeiroetal.(2023)developedbyMonteiroetal.(2023),First,trainedonthedataset,
parentpredictorsgivenavalueofxcanpredictparentvalues,i.e.,(t,i)’s,andthenmeasuretheabsoluteerrorbetween
parentvaluesafterhardinterventionorfeasibleinterventionandtheirpredictedvalues,whichismeasuredonimagethe
LearnedSCMgeneratesgiventheinputof(t,i).
Table13. AblationStudyonϵ
do(t) do(i)
Model ϵ CFs
t i t i
- NB 0.336 4.51 0.286 6.62
10−4 0.314 4.48 0.197 4.90
V-SCM
10−3 Ours 0.297 4.47 0.175 4.41
10−2 0.139 4.35 0.151 3.95
- NB 0.280 2.54 0.202 3.31
10−4 0.260 2.49 0.117 2.23
H-SCM
10−3 Ours 0.245 2.44 0.103 2.03
10−2 0.0939 2.34 0.0863 1.87
QuantitativeResultsofchange(i)ordo(i). WeuseV-SCMtodocounterfactualtaskofchange(i)(whereϵ=10−3)or
do(i)withmultiplerandomseedsontestset. InTable12,thefirstcolumnshowstheMAEof(t,i),indicatingourresults
22NaturalCounterfactualsWithNecessaryBacktracking
𝑡
𝑖 𝑥
(a) CausalGraph (b) Samples
Figure12. CausalGraphandsamplesofMorpho-MNIST.
Table14. ResultsonWeak-3DIdentandStong-3DIdent
Dataset Counterfactuals d h v γ α β b
Nonbacktracking 0.0252 0.0191 0.0346 0.364 0.266 0.0805 0.00417
Weak-3DIdent
Ours 0.0241 0.0182 0.0339 0.348 0.224 0.0371 0.00416
Nonbacktracking 0.104 0.0840 0.0770 0.385 0.495 0.338 0.00476
Stong-3DIdent
Ours 0.0633 0.0512 0.0518 0.326 0.348 0.151 0.00464
outperformthatofnon-backtracking. Next,wefocusontherestfour-columnresults. Inbothtypesofcounterfactuals,we
usethesamevalueiindo(i)andchange(i). Hence,afterinference,weknowwhichimagesatisfyingϵ-naturalgeneration
inthetwotypesofcounterfactuals. In”NC=1”ofthetable,NCindicatesthesetofcounterfactualsafterfeasibleintervention
optimization. NoticethatNCsetdoesnotmeantheresultsofnaturalcounterfactuals,sincesomeresultsdostillnotsatisfy
ϵ-naturalgenerationafterfeasibleinterventionoptimization. “NC=1”meanthesetcontainingdatapointssatisfyingϵ-natural
generationand“NC=0”containsdatanotsatisfyingϵ-naturalgenerationafterfeasibleinterventionoptimization. Similarly,
“NB=1”meansthesetcontainingdatapointssatisfyingnaturalnesscriteria. (NC=1,NB=1)presentstheintersectionof
“NC=1”and“NB=1”. Similarlogicisadoptedtotheotherthreecombinations. Thenumberofcounterfactualdatapointsare
10000intwotypesofcounterfactuals.
In (NC=1, NB=1) containing 5841 data points, our performance is similar to the non-backtracking, showing feasible
interventionoptimizationtendstobacktrackaslessaspossiblewhenhardinterventionshavesatisfiedϵ-naturalgeneration.
In(NC=1,NB=0),thereare3064datapoints,whichare“unnatural”pointsinnon-backtrackingcounterfactuals. After
naturalcounterfactualoptimation,thishugeamountofdatapointsbecome“natural”. Inthisset,ourapproachcontributesto
themaximalimprovementcomparedtotheotherthreesetsinTable12,improving55.4%and41.6%onthicknesstand
intensityi. Thenumberofpointsin(NC=0,NB=1)iszero,showingthestabilityofouralgorithmsinceourapproachwill
notmovethehard,feasibleinterventionintoanunfeasibleintervention. Twotypesofcounterfactualsperformsimilarlyin
theset(NC=0,NB=0),alsoshowingthestabilityofourapproach.
AblationStudyonNaturalnessThresholdϵ. Weusetwomodels,V-SCMandH-SCM,todocounterfactualswithdifferent
valuesofϵ. AsshowninTable13,ourerrorisreducedastheϵincreasesusingthesameinferencemodel,sincethehigherϵ
willselectmorefeasibleinterventions.
H.2.2.3DIDENTBOX
Inthistask,weutilizepracticalpublicdatasetscalled3DIdentBOX,whichencompassmultipledatasets(Bizeuletal.,2023).
Specifically,weemployWeak-3DIdentandStrong-3DIdent,bothofwhichsharethesamecausalgraphdepictedinFig.13
(a),consistingofanimagevariabledenotedasxandsevenparentvariables. Theseparentvariables,denotedas(d,h,v),
controlthedepth,horizonposition,andverticalpositionoftheteapotinimagexrespectively. Additionally,thevariables
(γ,α,β)governthreetypesofanglesassociatedwiththeteapotwithinimages,whilevariablebrepresentsthebackground
coloroftheimage. AsillustratedinFig.13(a),causalrelationshipsexistamongthreepairsofparentvariables,i.e.,(h,d),
(v,β)and(α,γ). ItisimportanttonoteadistinctionbetweenWeak-3DIdentandStrong-3DIdent. InWeak-3DIdent,there
existsaweakcausalrelationshipbetweenthevariablesofeachpair,asshowninFig.13(b),whereasinStrong-3DIdent,the
causalrelationshipisstronger,asdepictedinFig.13(c).
WefollowthesameexperimentalsetupasintheMophoMNISTexperiments. Usinganepsilonvalueofϵ = 10−3 we
employtheH-SCMastheinferencemodel. Weconductinterventionsorchangesonthevariables(d,β,γ)andtheresults
arepresentedinTable14. Inbothdatasets,ourapproachoutperformsthenon-backtrackingmethod,withStrong-3DIdent
23NaturalCounterfactualsWithNecessaryBacktracking
exhibitingamoresignificantmarginoverthenon-backtrackingmethod. Thisisbecausethenon-backtrackingmethod
encountersmoreunfeasibleinterventionswhenperforminghardinterventionsusingStrong-3DIdent.
Additionally,weperformvisualizationsonStrong-3DIdent. InFig.14,wedisplaycounterfactualoutcomesin(a)and(b),
wherethetextaboveeachimageinthefirstrow(evidence)indicatestheerrorinthecorrespondingcounterfactualoutcome
showninthesecondrow. InFig.14(a),wepresentcounterfactualimagesthatdonotmeettheϵ-naturalgenerationcriteria
inthenon-backtrackingapproach. Incontrast,Fig.14(b)showcasesourresults,whicharenotablymorevisuallyeffective.
Thisdemonstratesthatoursolutioncanalleviatethechallengesposedbyhardinterventionsinthenon-backtrackingmethod.
𝑏
𝑣 𝛽
ℎ 𝛼
𝑥
𝑑 𝛾
(a) CausalGraph (b) WeakCausalRelationship (c) StrongCausalRelationship
Figure13.Causalgraphof3DIdentandthecausalrelationshipsofvariables(d,h)inWeak-3DIdentandStrong-3DIdentrespectively.
24NaturalCounterfactualsWithNecessaryBacktracking
d=0.29; h=0.14; d=0.33; h=0.12; d=0.12; h=0.07; d=0.26; h=0.01; d=0.23; h=0.02;
v=0.09; =1.04; v=0.62; =1.51; v=0.26; =1.32; v=0.34; =1.51; v=0.29; =1.45;
=1.43; =1.54 =1.50; =0.04 =1.60; =0.64 =1.61; =0.36 =1.73; =0.47
(a) ResultsofNon-backtrackingCounterfactuals
d=0.05; h=0.02; d=0.00; h=0.08; d=0.05; h=0.02; d=0.01; h=0.01; d=0.01; h=0.05;
v=0.03; =0.46; v=0.07; =0.04; v=0.06; =0.01; v=0.08; =0.13; v=0.04; =0.14;
=0.21; =0.02 =0.13; =0.01 =0.03; =0.00 =0.00; =0.01 =0.01; =0.01
(b) ResultsofNaturalCounterfactuals
Figure14. VisualizationResultsonStong-3DIdent.
25