Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown
Hyperparameters Of Any Type
JuliuszZiomek1 MasakiAdachi1 MichaelA.Osborne1
Abstract 2023),logiccircuitdesign(Grosnitetal.,2022),andtuning
hyperparametersofmachinelearningalgorithms(Cowen-
BayesianoptimisationrequiresfittingaGaussian
Riversetal.,2022). BOalgorithmsconstructasurrogate
processmodel,whichinturnrequiresspecifying
model of the unknown black-box function based on the
hyperparameters—mostofthetheoreticallitera-
observed values for the points queried so far. The model
tureassumesthosehyperparametersareknown.
used in most cases is the Gaussian Process (GP), which
Thecommonlyusedmaximumlikelihoodestima-
requiresaspecificationofaprioranditshyperparameters.
torforhyperparametersoftheGaussianprocessis
consistentonlyifthedatafillsthespaceuniformly, Thesehyperparameterscould,forexample,belengthscales
whichdoesnothavetobethecaseinBayesian θ, which scale the input before applying the kernel, i.e.
optimisation. Since no guarantees exist regard- k (x,x′) = k(x,x′). However, hyperparameters can
θ θ θ
ingthecorrectnessofhyperparameterestimation, comeinmanyshapesandformsbeyondjustlengthscales.
and those hyperparameters can significantly af- Inhigh-dimensionalBO,itiscommontouseadditiveker-
fecttheGaussianprocessfit,theoreticalanalysis neldecompositions(Kandasamyetal.,2015;Rollandetal.,
ofBayesianoptimisationwithunknownhyperpa- 2018) of the form k(x,x′) = (cid:80) k (x ,x′ ), where
c∈g c [c] [c]
rametersisverychallenging.Previouslyproposed x selectsonlythedimensionsinsetcandg isacollec-
[c]
algorithmswiththeno-regretpropertywereonly tion of sets. This collection of sets essentially becomes
abletohandlethespecialcaseofunknownlength- a hyperparameter of the kernel. When dealing with peri-
scales,reproducingkernelHilbertspacenormand odicfunctions, theperiodofthefunctionbecomesanew
appliedonlytothefrequentistcase. Weproposea parameter that needs to be estimated. Finally, the prior
novelalgorithm,HE-GP-UCB,whichisthefirst meanfunctioncanalsoincludeunknownhyperparameters,
algorithmenjoyingtheno-regretpropertyinthe forexample,wemighthavetwocandidatemeanfunctions
case of unknown hyperparameters of arbitrary µA(x)andµB(x)giventousbydomainexperts,andusea
0 0
form,andwhichsupportsbothBayesianandfre- priormeanofµ (x) = ZµA(x)+(1−Z)µB(x),where
0 0 0
quentistsettings. Ourproofideaisnovelandcan Z ∈{0,1}istheunknownhyperparameter.
easilybeextendedtoothervariantsofBayesian
WhenanalysingtheperformanceofaBOalgorithm,weare
optimisation. Weshowthisbyextendingoural-
usuallyinterestedinprovingaboundonitsregret,whichis
gorithmtotheadversariallyrobustoptimisation
thesumofdifferencesbetweentheoptimalfunctionvalue
settingunderunknownhyperparameters. Finally,
and the one at the queried point. To show our algorithm
weempiricallyevaluateouralgorithmonasetof
provablyconverges,wewanttoprovethatitsregretbound
toyproblemsandshowthatitcanoutperformthe
issublinearinT,alsoreferredtoasno-regretproperty. GP-
maximumlikelihoodestimator.
UCBalgorithmhasbeenproven(Srinivasetal.,2010)to
enjoyano-regretguarantee,butonlyifthekernelandits
1.Introduction hyperparametervaluesareknown.
Inpractice,theparametersofthekernelfunctionareselected
Bayesian Optimisation (BO) (Garnett, 2023) emerged as
bythemaximumlikelihoodestimation(MLE).Statistical
asuccessfulparadigmforsolvingblack-boxoptimisation
literature(Bachoc,2013)providesasymptoticconsistency
problemsinasample-efficientmanner.Itfoundapplications
guaranteesforMLEofkernelhyperparameters,butonlyin
in numerous fields such as drug discovery (Khan et al.,
thecase,wherethepointsatwhichweknowthefunction
1Machine Learning Research Group, University of Oxford, valuesfillthespaceuniformly. ThisdoesnotholdforBO,
OX26ED,UnitedKingdom.Correspondenceto:JuliuszZiomek whichselectsthepointsinanon-uniformmanner. Assuch,
<firstname(dot)lastname[at]univ(dot)ac{dot}uk>. studyingthebehaviourofBOwithunknownkernelhyper-
parametersischallengingandnoguaranteesexistregarding
1
4202
beF
2
]GL.sc[
1v23610.2042:viXraNo-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
thecorrectnessofhyperparameterestimation. Theno-regret adversariallyrobustoptimisation.
proofofGP-UCB(Srinivasetal.,2010)requiresthatthe
Contributions
GPmodelprovidesuswithvalidconfidenceboundsforthe
functionvalues. Asdifferenthyperparametervaluesmay
• WeproposetheHE-GP-UCB,thefirstno-regretalgo-
producedifferentconfidenceboundsandweareunableto
rithmwhich:
reliablyrecoverthetruehyperparametervalue,derivinga
no-regretguaranteeinsuchasettingisverychallenging. – canhandlearbitraryhyperparametersofanyform
The only two previous works (Wang & de Freitas, 2014; – worksalsointheBayesiansetting
Berkenkamp et al., 2019) that proposed no-regret BO al- – employsdata-drivenhyperparameterupdates.
gorithmswithunknownhyperparametersbothconsidered
• Ourproofideaisnovelandeasilyextendabletoother
the special case of unknown lengthscales (the latter also
variants of BO. We show this by developing HE-
consideredaspecialcaseofunknownreproducingkernel
StableOpt,whichisaversionofHE-GP-UCBcapable
Hilbertspace(RKHS)norm)andstudiedonlythefrequen-
ofperformingadversariallyrobustoptimisationunder
tist case that is when the black-box function is a mem-
unknownhyperparameters.
ber of a RKHS of some kernel k(·,·), i.e. f ∈ H(k). In
theiralgorithms,thelengthscalehyperparameterisprogres- • Weempiricallycomparetheperformanceofouralgo-
sively shrunk. This shrinking is done regardless of the rithmtootheralgorithmsonasetoftoyproblemsand
observed data. The algorithms enjoy the no-regret prop- showthatitcanoutperformMLE.
erty, as for two lengthscales θ ≤ θ , in the case that
1 2
f ∈H(k ) =⇒ f ∈H(k ),whereH(k )istheRKHS
θ2 θ1 θ RelatedworkonBayesianOptimisationAsmentioned
definedbykernelk withlengthscaleθ. Asaresult,ifthe
θ before,theseminalno-regretproofsofGP-UCB(Srinivas
lengthscaleissmallenough,thetruefunctionhastoliein
etal.,2010)andIGP-UCB(Chowdhury&Gopalan,2017)
theRKHSoftheusedkernelandtheconfidenceboundsof
assumed the true hyperparameter value is known. Bull
theresultingmodelsarevalid. Inthecaseofarbitraryhyper-
(2011) derived a bound on simple regret of BO with un-
parameters,suchastrategywillnotwork,asitispossible
knownlengthscalesandoutputscalesonnoiselessfunctions,
thatRKHSoftwohyperparametervaluesdonotencapsu-
butwithoutcumulativeregretbound. Hoffmanetal.(2011)
late one another. This means that there exists a function
proposedanadversarialbandit-stylealgorithmGP-HEDGE
such that f ∈ H(k ) but f ∈/ H(k ) and vice versa for
u v forproblems,wheretheparametersoftheacquisitionfunc-
twohyperparametervaluesuandv. Moreover,thisstrategy
tion are unknown. Although they provide a cumulative
will also not work in the Bayesian case that is when the
regretbound,theyalsoadmitthatitisnotsublinear. Inthe
black-box function is a sample from a GP prior. This is
caseofunknownlengthscales,theaforementionedworks
because if the black-box function is a sample from prior
of Wang & de Freitas (2014); Berkenkamp et al. (2019)
with lengthscale θ , i.e. f ∼ GP , then that exact prior
2 θ2 proposedalgorithmswithsublinearregret. However,asex-
needstobeusedtoproduceavalidposterior. Itcannotbe
plainedbefore,thesealgorithmscannotbeextendedtodeal
replacedbyGP forsomeshorterlengthscaleθ <θ .
θ1 1 2 withunknownhyperparametersofarbitraryformanddonot
Inthiswork,weproposethefirstBOalgorithmwhichen- workintheBayesiancase. Morerecently,Haetal.(2023)
joystheno-regretpropertyinbothBayesianandfrequentist proposedtheUHE-BOalgorithmforunknownhyperparam-
settingswithunknownhyperparametersofarbitraryform, eterBOandprovidedsimpleregretguarantees,butwithout
aslongasthereisafinitesetofcandidatehyperparameter guaranteesoncumulativeregret.AsUHE-BOreliesonsam-
values.Ouralgorithmeffectivelyeliminateshyperparameter plingrandompoints,itisunlikelyforittohavetheno-regret
valuesthatarehighlyimplausiblegiventhedataobserved property. Hvarfner et al. (2023) proposed ScoreBO algo-
sofar,makingitalsothefirstno-regretalgorithmwithdata- rithm,whichcombinesBOwithanactivelearningstrategy
driven hyperparameter updates. When selecting the next tolearnthehyperparametervalues. However,theyprovide
pointtoquery,ouralgorithmisoptimisticwithrespectto notheoreticalanalysisoftheproposedalgorithm.
thehyperparametervaluesthathavenotyetbeeneliminated.
RelatedworkonMasterAlgorithmsinBanditsWithin
Ourprooftechniqueisnovelandreliesontheideathatif
themulti-armedbanditscommunity,itiscommontostudya
allhyperparametervaluesproducemodelsexplainingthe
problem,inwhichwearesupposedtodesignamasteralgo-
observationswellenoughthenitdoesnotmatterwhichis
rithmtocoordinateanumberofbasealgorithms(Agarwal
used. On the other hand, if this is not the case then we
etal.,2017;Abbasi-Yadkorietal.,2020;Pacchianoetal.,
caneasilyeliminatewrongmodels. Importantly,ourproof
2020;Moradiparietal.,2022).Anaturalquestionthatarises
techniqueisveryuniversalandwebelieveitcanbeeasily
iswhetherwecouldextendthosealgorithmstoBOsettings
extendedtodifferentvariantsofstandardBO.Weshowone
andrunsuchamasteralgorithmwithonebasealgorithm
concrete example, by extending our method to deal with
for each possible value of the hyperparameter. However,
2No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
asexplainedinAppendixA,suchconstructedalgorithms someR-subgaussiandistribution.
either do not achieve no-regret property or apply only to
In both cases, without the loss of generality, we assume
very special cases. On the other hand, the algorithm we
that ku(·,·) ≤ 1 for all u ∈ U. As the regret depends
proposelaterachievesno-regretpropertyunderthegeneral
on the kernel ku∗(·,·), typically in BO literature, the re-
caseofunknownhyperparametersofanyform.
gret bound is expressed in terms of maximum informa-
tion gain (MIG) (Srinivas et al., 2010) of kernel ku∗, de-
2.ProblemStatement finedasγu∗ :=max 1log|I+R−2Ku∗|,where
T A⊆X:|A|=T 2 A
Ku∗ = [ku∗(x,x′)] . Fortunately, γu∗ issublinear
Weconsiderastandardproblemsettingwherewewishto A x,x′∈A T
maximiseanunknown,black-boxfunctionf :X →Rover inT forcommonlyusedkernels.
somecompactsetX ⊂Rd,i.e. max x∈X f(x). Weassume Gaussian Process Model In BO, at each timestep t, we
weareallowedtoqueryasinglepointx tateachtimestept contructasurrogatemodeloftheblack-boxfunction,based
andobtainitsfunctionvaluecorruptedbysomei.i.d. noise onthequeriessofar. LetD ={(x ,y )}t−1bethecol-
t−1 i i i=1
ϵ t,i.e. y t =f(x t)+ϵ t. Theexactdistributionofthatnoise lectionofallqueriedpointsandtheircorrespondingnoisy
willdependonwhetherweareinBayesianorfrequentistset- values up to timestep t−1. If we use a GP model with
tings,asdefinedlater. Aswewanttomaximisethefunction, akernelku(x,x′),whereuisthekernelhyperparameter,
theinstantaneousregretisdefinedasr t =f(x∗)−f(x t), thengiventhedatasofarD t−1,themodelreturnsusthefol-
wherex∗ =argmax x∈X f(x). Similarly,wedefinecumu- lowingmeanµu t−1(x)andvariance(σ tu −1)2(x)functions:
lative regret as R =
(cid:80)T
r . We also assume there is
some”true”hyperT paramett e= r1 vat lueu∗ ∈U associatedwith µu t−1(x)=k tu −1(x)T(Ku t−1+R2I)−1y t−1
thefunctionf, formallydefinedlater. Typicallyinlitera-
(σu )2(x)=k(x,x)u−ku (x)T(Ku +R2I)−1ku (x),
ture,itwasassumed(Srinivasetal.,2010;Chowdhury& t−1 t−1 t−1 t−1
Gopalan,2017)thatthetruehyperparametervalueisknown, where y ∈ Rt−1 with elements (y) = y , ku(x) ∈
t−1 i i
but in this work, we lift this assumption. We assume we Rt−1 with elements ku(x) = ku(x,x ) and similarily
i i
haveaccesstoU and|U|<∞,butwedonotknowwhich Ku ∈Rt−1×t−1 withentriesKu =ku(x ,x ). While
t−1 i,j i j
elementofU isthetruehyperparametervalue. usingmeanandvariancefunctionsprovidedbytheGP,we
get the guarantee on predictive performance in Bayesian
Whydoweneedtoassume|U|<∞? Considerthecase
andfrequentistsettings,asstatedinthenextSection.
when |U| = ∞. There is always an infinite number of
functions going through any finite number of points. At
each timestep, we could thus have an infinite number of 3.Preliminaries
hyperparameter values producing models explaining our
Wenowrecallwell-knownresultsfromtheliterature. Those
observationsequallywell,butdifferingsignificantlywhen
resultswouldbelaterneededtoprovetheno-regretproperty
itcomestounexploredregions. Withoutanystrongassump-
ofourproposedalgorithm.
tionsaboutthesmoothnessofthemappingfromhyperpa-
rametervaluestomeanandvariancefunctionsofthefitted Theorem3.1. Leteitherofthefollowingtwobethecase:
GP,obtainingtheno-regretpropertywouldbeimpossible.
Noticethattherearesomesettings,wherethehyperparame- • Assume the Bayesian setting and let
terisnaturallydiscrete,forexampleintheaforementioned f ∼ GP(cid:0) µu∗(·),ku∗(·,·)(cid:1) , and set βu∗ =
0 t
high-dimensionalBOwhenwewanttochooseoneofthe (cid:114) (cid:16) (cid:17)
2log |X|π2t2
possibleadditivedecompositionsofthekernelfunction. We 6δA
nowdistinguishbetweentwoproblemsettings.
• Assume the frequentist setting and let f ∈
H(cid:0) ku∗(cid:1)
,
B bla ay ce ks -i ba on
x
S fue ntt ci tn iog nI fn wB aa sye ss ai man pls ee dtti fn rog m, w ae Ga Pss pu rm ioe r,o iu .er
.
su (cid:113)ch that ∥f∥ ku∗ ≤ B and set β tu∗ = B +
f ∼ GP(µu 0∗(·),ku∗(·,·))forsomemeanfunctionµu 0∗(·) R 2(γ tu −∗ 1+1+ln(1/δ A))
andkernelfunctionku∗(·)parametrisedbysomeu∗ ∈ U,
0
which we call true hyperparameter value. We require Then,withprobabilityatleast1−δ ,forallx ∈ X and
A
sup µu (x) < ∞ for each u ∈ U. We further as- t=1,...,T:
x∈X t−1
sumethedistributionofnoiseisϵ ∼N(0,R2).
t (cid:12) (cid:12)
(cid:12)f(x)−µu∗ (x)(cid:12)≤βu∗ σu∗
(x).
FrequentistSettingInfrequentistsetting,weassumethat (cid:12) t−1 (cid:12) t t−1
thetrueblack-boxfunctionf ∈H(ku∗),whereH(ku∗)is
theRKHSofkernelku∗(·,·)forsomeu∗ ∈ U,whichwe Proof. Bayesian case follows from Lemma 5.1 of Srini-
calltruehyperparametervalue. Weassumethenoiseϵ has vas et al. (2010) and frequentist case from Theorem 2 of
t
Chowdhury&Gopalan(2017).
3No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
Table1.ComparisonofdifferentBOalgorithmsandtheirabilitiestodealwithunknownhyperparameters. ThenotationO˜(·)hides
logarithmic factors for brevity. If logarithmic factors were not hidden, the frequentist bound of IGP-UCB would be better than
that of GP-UCB. A-GP-UCB is the method of Berkenkamp et al. (2019) and BOHO is the method of Wang & de Freitas (2014).
(cid:110) (cid:111)
AB =max g−1(maxθ0),b−1(B ) ,whereθ andB areupperandlowerboundsonlengthscaleandRKHSnormrespectivelyandθ
θ i θ B0 0 0
andBarethetruevaluesofthosequantities;g(·)andb(·)aresomefunctionsthatincreasewithoutbound.θ istheshortestlengthscale
T
consideredbyA-GP-UCBbytimeT andθ issomeknownlower-boundonlengthscalehyperparameter.Wedefinebyγˆ =max γu
L T u∈U T
andE =min{d,log|X|}.LSstandsforlengthscale.
Algorithm Unknown frequentist Bayesian Hypers.
hypers. R bound R bound updates
T T
√ √ √
GP-UCB ✗ O˜( T(B γ +γ )) O˜( Tγ E) ✗
√ √ T T T
IGP-UCB ✗ O˜( T(B γ +γ )) ✗ ✗
T T
√ (cid:113)
BOHO LS O˜( T(B γθL +γθL)) ✗ pre-defined
T T
(cid:18) √ (cid:113) (cid:19)
A-GP-UCB LS/RKHSnorm O˜ ABB+ T(B γθT +γθT) ✗ pre-defined
θ T T
(cid:16) √ √ (cid:17) √
HE-GP-UCB(ours) Any O˜ |U|B+ T(B γˆ +γˆ ) O˜(|U|B+ Tγˆ E) data-driven
T T T
Notethattheconfidenceparameterβ inTheorem3.1,de-
t if is observed:
pendson|X|intheBayesiansettingandonlymakessense
GP1 GP1 remains
if|X|<∞. Ifthisisnotthecase,weneedtoadoptastan-
GP2 is eliminated
dardassumption(Srinivasetal.,2010),statedinAppendix
B.Underthatassumption,wecanderivehigh-probability
if is observed:
confidenceintervalsintheBayesiancaseevenif|X|=∞.
observed GP1 remains
FollowingSrinivasetal.(2010),purelyforthesakeofanal-
GP2 GP2 remains
ysis, letusdefineadiscretizationD ofsize(τ )d sothat
t t
forallx ∈ D ,∥x−[x] ∥ ≤ r ,where[x] denotesthe
t t 1 dτt t query point
closestpointinD tox. WenowgetthefollowingTheorem.
t
Theorem 3.2. Let Assumption B.1 hold, set βu∗ =
t Figure1.Intuitionbehindthehyperparameterelimination.Solid
(cid:115)
(cid:16) (cid:17) (cid:18) (cid:114) (cid:16) (cid:17)(cid:19) linesrepresentthemeanfunctionsoftwomodels-GP1andGP2,
2log π2t2 +2dlog dt2br log 2da andsetthe
3δA δA producedbyfittingtheobservedpointswithdifferenthyperparam-
(cid:114) etervalues,andshadedregionsareconfidenceintervals.Thepoint
(cid:16) (cid:17)
sizeofdiscretisationtoτ =dt2br log 2da . Thenwith markedbythestarliesintheconfidenceintervalsofbothmodels,
t δA
whereasthepointmarkedbythediamondliesintheoneofGP1.
probabilityatleast1−δ wehaveforallx∈X:
A
1
|f(x)−µu∗
([x]
)|≤βu∗ σu∗
([x] )+
t−1 t t t−1 t t2 boththefunctionvalueandthemodel. Inotherwords,for
eachpoint,weusethebestpossibleupperconfidencebound
Andforanyx deterministicconditionedon(y ,x )t−1:
t i i i=1 amongtheonessuggestedbymodelsdefinedbyhyperpa-
|f(x )−µu∗ (x )|≤βu∗ σu∗ (x ). rametervaluesinU t−1. Wedenotetheselectedpointbyx t
t t−1 t t t−1 t andthehyperparametervalueunderwhichitachievedthe
highestUCBvalueisdenotedbyu . ThesetSu storesall
Proof. Lemma5.5and5.7ofSrinivasetal.(2010). t t
iterationsiuptothecurrentone t, duringwhichu = u,
i
foreachu∈U (line8). Afterobservingthefunctionvalue,
4.GP-UCBwithHyperparameterElimination wecalculatetheerrorη betweentheobservedvalueand
t
themeanatpointx ,accordingtothemodeldefinedbyu
WenowstateourproposedalgorithminAlgorithm1. Algo- t t
(line11). Then,eachmodelforwhichthesumoferrorsis
rithm1solvesbothBayesianandfrequentistsettingswith
toobigisremovedfromthesetofcandidates(lines12-14)
unknownhyperparameters,makingitthefirstalgorithmto
andwecontinuewiththeloop.
achievesublinearregretundersuchconditions,tothebest
ofourknowledge. Whenselectingthenextpointtoquery TheintuitionbehindthismechanismisdepictedinFigure
(line7),ouralgorithmisdoublyoptimistic,withrespectto 1. LetusassumewehavetwomodelsGP1andGP2,rep-
4
{No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
resented by red and blue solid lines, together with their Theorem 4.2 (Bayesian setting). Let f ∼
correspondingconfidenceintervalsindashedlines. After GP(µu∗(·),ku∗(·,·)), ϵ ∼ N(0,R2) for each t
0 t
wequeryanewpoint,itcaneitherliewithintheconfidence and u∗ ∈ U and denote γ = max γu. Set
T u∈U T
intervalsofbothmodels(likethestar)oronlyonemodel ξ =2R2log|U|π2t2 and:
t 3δ
(like the diamond). If we were to obtain a point like the
diamond,weclearlyseethattheredmodel’spredictionwas (cid:113)
(A) if|X|<∞,setβu = 2log|X|π2t2
veryinaccurateandweleantowardsrejectingthehyperpa- t 3δ
rameter value that produced it. On the other hand, if we
(B) if|X|=∞,letAssumptionB.1holdandset
observeapointlikethestarwhenqueryingthefunctionthen
itdoesnotmatter,whichmodelisthe”true”one,assuchan (cid:118)
observationisprobableunderbothmodelsandtheerrorsfor (cid:117) (cid:117) (cid:18) 2π2t2(cid:19) (cid:32) (cid:115) (cid:18) 4da(cid:19)(cid:33)
βu =(cid:116)2log +4dlog dtbr log
eitherofthemodelswouldbesmall. Thankstothismecha- t 3δ δ
nism,wecanconductBO,whilstsimultaneouslyestimating
the hyperparameter values. Such a strategy achieves the Then,Algorithm1withprobabilityatleast1−δachieves
no-regretpropertyinbothBayesianandfrequentistsettings. thecumulativeregretofatmost:
Wefirststatethefrequentistresult. (cid:16) (cid:112) (cid:112) (cid:17)
R ≤O |U|B+ |U|ξ T +β |U|γ T
T T T T
Algorithm1HyperparameterEliminationGP-UCB
(HE-GP-UCB) forsomeconstantB > 0andaslim T→0 R TT → 0, Algo-
rithm1enjoystheno-regretproperty.
Require: asetofpossiblehyperparametervaluesU;confi-
denceparameters{ξ }T and{{βu}T }
t t=1 t t=1 u∈U
1: SetD 0 =∅,U 0 =U,Su =∅foreachu∈U 5.Proofsketchoftheregretbound
2: fort=1,...,T do
Weprovideasketchoftheprooftechniqueinthissection.
3: foru∈U t−1do
4: FitaGP(µu t−1(x),σ tu −1(x))toD t−1 For the full proof, see Appendix C and D for frequentist
5: SetUCBu (x)=µu (x)+βuσu (x) and Bayesian versions, respectively. We first look at the
t−1 t−1 t t−1
frequentistcaseandthencommentonthedifferenceswith
6: endfor
7: Findx t,u t = argmax UCBu t−1(x) theBayesiancase. WefirstintroducethefollowingLemma
x,u∈X×Ut−1 fortheconcentrationofnoise,provedinAppendixE.
8: S tut =S tu −t 1∪{t}andS tu =S tu −1foru∈U;u̸=u t Lemma5.1. Foreachu∈U andt=1,...,T wehave:
9: Querytheblack-boxy t =f(x t)
 (cid:12) (cid:12) 
1 1 10 1 2: :
:
iS S fe e (cid:12)
(cid:12)
(cid:12)t t (cid:80)D η tt i∈== SuyD tt ηt −− i(cid:12)
(cid:12)
(cid:12)1 µ >∪ u t−t( 1 (cid:112)x (xt ξ, t ty ) |t S)
tut|+(cid:80) i∈Sut β iutσ iut(x i)
P  t=1∀ ,...,Tu∈∀ U(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)i(cid:88)
∈S
tuϵ i(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)≤(cid:112) ξ t|S tu|≥1−δ B,
t t
then
whereξ
=2R2log|U|π2t2
.
13: U t =U t−1\{u t} t 6δB
14: endif ObservethatbyunionboundTheorem3.1andLemma5.1
15: endfor hold together with probability at least 1−δ −δ . We
A B
chooseδ =δ = δ.Weprovetheregretbound,assuming
Theorem 4.1 (Frequentist setting). Let f ∈ H(cid:0) ku∗(cid:1) , A B 2
theprobabilisticstatementsinTheorem4.1andLemma5.1
s.t. ∥f∥ ku∗ ≤ B, u∗ ∈ U and denote γ T = hold,andassuchtheresultingboundholdswithprobability
max γu and β = max βu. Set βu = B +
u∈U T T u∈U T t atleast1−δ.
R(cid:112) 2(γu +1+ln(2/δ))andξ =2R2log|U|πt2 . Then,
t−1 t 3δ Preservationofu∗Wefirstshowthatifthestatementsfrom
Algorithm 1 with probability at least 1−δ achieves the
Theorem3.1andLemma5.1hold,thenthetruehyperpa-
cumulativeregretofatmost:
rametervalueu∗isneverrejected. Indeed,whenline12is
(cid:16) (cid:112) (cid:112) (cid:17) executedatiterationt:
R ≤O |U|B+ |U|ξ T +β |U|γ T
T T T T
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
andaslim T→0 R TT →0,Algorithm1enjoystheno-regret (cid:12) (cid:12)
(cid:12)
(cid:88) η t(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12)
(cid:12)
(cid:88) (cid:0) y t−f(x t)+f(x t)−µ∗ t−1(x t)(cid:1)(cid:12) (cid:12)
(cid:12)
property. (cid:12)i∈Su∗ (cid:12) (cid:12)i∈Su∗ (cid:12)
t t
(cid:12) (cid:12)
WenowturntotheBayesiansetting. Wenotethattheexact (cid:12) (cid:12)
bounddependsonwhetherX isafinitesetornot, butin ≤(cid:12) (cid:12)
(cid:12)
(cid:88) ϵ t(cid:12) (cid:12) (cid:12)+ (cid:88) (cid:12) (cid:12)f(x t)−µ∗ t−1(x t)(cid:12) (cid:12)
eithercase,theno-regretpropertyholds. (cid:12)i∈Su∗ (cid:12) i∈Su∗
t t
5No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
WecombinethisfacttogetherwithLemma5.1toarriveat:
≤(cid:113) ξ t|S tu∗|+ (cid:88) β iu∗ σ iu(x i),
R
≤2|U|B+2(cid:88)(cid:113)
ξ
|Su|+2(cid:88)
β+σ+ (x ).
i∈Su∗ T T T t t−1 t
t
u∈U t∈/C
wherethelastinequalityisduetoTheorem3.1andLemma
5.1. As such, the condition of the if statement always Dealingwith|S Tu|Oneoftheissueswiththeboundabove
evaluatestofalseandu∗ ∈ U
t
foranyt ≥ 0. Next,we isthatitdependsonthesizeofS Tu foreachu∈U,which
showhowthisfactallowsustoboundinstantaneousregret. isarandomvariable. WenowpresentaLemma,provedin
AppendixF,whichboundsthesecondtermintheexpression
BoundonsimpleregretIfu∗ ∈U t−1wetriviallyhave: aboveintheworstcase,removingthedependenceon|Su|.
T
f(x∗)≤µu∗ (x∗)+βu∗ σu∗ (x∗) Lemma5.2. Wehavethat:
t−1 t t−1
(cid:88)(cid:113) (cid:112)
≤ maxmaxµu t−1(x t)+β tuσ tu −1(x t), (1) |S Tu|≤ |U|T.
xt∈Xu∈Ut u∈U
wherefirstinequalityisduetoTheorem3.1. Forsimplicity
wewillwriteµ+ = µut , σ+ = σut andβ+ = βut. ExpressingboundintermsofMIGThefinalstepofthe
t−1 t−1 t−1 t−1 t t
Havingestablishedanupperboundonf(x∗),itremainsto proofistoexpressthethirdtermofthecumulativeregret
findalowerboundonf(x ). Weshowthefollowing: boundintermsofmaximuminformationgain. Wedothis
t
inthefollowingLemma,provedinAppendixG.
f(x t)≥µ+ t−1(x t)−η t+ϵ t. (2) Lemma5.3. ThereexistsaconstantC >0,suchthat:
(cid:88) (cid:112)
Theobservationaboveisakeycomponentofourproof,as β+σ+ (x )≤β CT|U|γ ,
t t−1 t T T
itrelatesthelowerboundonthefunctiontotheprediction t∈/C
errorofη ofthemodeldefinedbyhyperparametervalueu .
t t whereβ =max βu andγ =max γu.
CombiningInequalities1and2yieldsthefollowingbound T u∈U T T u∈U T
oninstantaneousregret:
ApplyingLemma5.2and5.3topreviouslydevelopedcu-
mulativeregretbound,weget:
r ≤β+σ+ (x )−η +ϵ .
t t t−1 t t t
(cid:112) (cid:112)
R ≤2|U|B+2 |U|ξ T +2β CT|U|γ ,
T T T T
Bound on cumulative regret With our bound on instan-
whichfinishesthefrequentistcasebound.
taneous regret, we now proceed to bound the cumulative
regret. LetusdefineCtobethesetof”critical”iterationsas Differences in Bayesian Case The first difference in
below: Bayesian case is that, since the black-box function f is
a random variable, there is no deterministic bound on its
(cid:40) (cid:12) (cid:12) (cid:41)
C = t≥1:(cid:12) (cid:12)
(cid:12)
(cid:88) η i(cid:12) (cid:12) (cid:12)>(cid:112) ξ t|Sut|+ (cid:88) β iutσ iut(x i) m foa llx oi wm iu nm gLv ea mlu me. a,A prs ovs eu dch in, Aw pe pr ee ns do il xve Ht .his issue by the
(cid:12) (cid:12)
i∈Sut i∈Sut
Lemma5.4. Forf ∼GP(µ (·),k(·,·))forsomemeanand
0
Noticethatforeacht∈C,wediscardonehyperparameter kernelfunctionsµ (·)andk(·,·)withproperties|µ (·)|<
0 0
valueandassuch|C| ≤ |U|. Onthoseiterations,wemay ∞andk(·,·)≤1,thereexistsaconstantB >0,suchthat:
sufferanarbitrarilyhighregret. Inthefrequentistsetting,
wehaver ≤2B forallt∈C. UsingEquation5,wethus sup|f(x)|≤B
t
seethatthecumulativeregretisboundedas: x∈X
aslongas|X|<∞andtheeventofTheorem3.1holdsor
(cid:88) (cid:88)
R T ≤ r t+ r t |X|=∞andeventofTheorem3.2holds.
t∈C t∈/C
If|X|<∞,byunionboundTheorem3.1andLemma5.1
(cid:88) (cid:88) (cid:88)
≤2|U|B+ β t+σ t+ −1(x t)+ (ϵ t−η t). hold together with probability at least 1−δ A −δ B and
t∈/C u∈Ut∈S Tu\C we choose δ A = δ B = 2δ. Now we have that for t ∈ C,
the instantaneous regret has bound r ≤ 2B and all the
t
Whenitcomestothetermsη ,observethatift∈/ C theif remainingstepsarethesameasinthefrequentistcase. If
t
statementinline12evaluatestofalse. Foreachu ∈ U |X|=∞,theninsteadofTheorem3.1,werelyonTheorem
wethushave: 3.2andtherestoftheproofworksinthesameway,except
forthefactthattheinstantaneousregrethasanadditional
(cid:88) (cid:88) −η ≤ (cid:88)(cid:113) ξ |Su|+(cid:88) (cid:88) βuσu (x ). 1 term. However,as(cid:80)T 1 ≤ π,itjustaddsaconstant
t T T t t−1 t t2 t=1 t2 6
thatdoesnotaffectthescalingofthefinalbound.
u∈Ut∈Su\C u∈U u∈Ut∈Su\C
T T
6No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
6.DiscussionandExtensions ϵ-regretbecomes:
TheregretofHE-GP-UCBscaleswithmax γu,i.e. the rϵ =max min f(x+δ)− min f(x +δ),
u∈U T t t
worstpossibleγu amongallpossiblehyperparameterval- x∈X δ∈∆ϵ(x) δ∈∆ϵ(xt)
T
uesu ∈ U. Bothofthepreviousworks, whichproposed where ∆ (x) = {x − x : x ∈ Dandd(x,x ) ≤ ϵ}.
ϵ 0 0 0
no-regretalgorithmsforunknownhyperparameterBOex- WeshowthattheStableOptalgorithmofBogunovicetal.
hibitthesamekindofscaling. ThealgorithmofWang& (2018)canbecombinedwithhyperparameterelimination,
de Freitas (2014), scales with γθL — the MIG of kernel producingHE-StableOpt,whichwepresentinAlgorithm
T
withshortestpossiblelengthscaleθ L (thushighestMIG), 2inAppendixI.StandardStableOptalgorithmselectsx t
whereasA-GP-UCBofBerkenkampetal.(2019)scalesas bymaximisingmin UCB (x +δ )andselectsδ
γ TθT,wherethelengthscaleθ tisprogressivelyshrunkwith
byminimisingLCBδ t(∈ x∆ tϵ( +x)
δ t).
Ht E-St tablet Optremainsopt
-
eachiteration(andconsequentlyMIGincreased). timistic with respect to unknown hyperparameters when
selectingx andpessimisticwhenselectingδ . SinceUCB
Inthefrequentistsetting, ifitisthecasethatthereexists t t
mightachieveitsmaximumforadifferenthyperparameter
onehyperparametervalueuˆ∈U,suchthatthecorrespond-
valuethantheoneforwhichLCBachievesitsminimum,
ingRKHSH encapsulatesRKHSassociatedwithevery
kuˆ
HE-StableOpthasnowtwocriteriaforrejectinghyperpa-
otherhyperparametervalue,thatis∀ H ⊆H ,then
u∈U ku kuˆ
rametervalues. Forsuchaconstructedalgorithm,wecan
runningGP-UCBorIGP-UCBwithhyperparametervalue
derivethefollowingϵ-regretbound,proveninAppendixI.
uˆwouldachieveno-regretpropertyandtheboundwould
alsoscalewithmax u∈Uγ Tu. Thusourboundismostuseful Theorem6.1. Assumetheconditionsofandsetβ tuandξ t
intheBayesiansetting,wherenopriorcanencapsulatean- asin,Theorem4.1(frequentist)orasinTheorem4.2caseB
otherprior,inthesamewayasRKHScanencapsulateother (continuousBayesian). Then,Algorithm2withprobability
RKHS.Indeed,ifafunctionwassampledfromapriorwith atleast1−δachievesthecumulativeϵ-regretofatmost:
hyperparametervalueu,thenthealgorithmhastousethis (cid:16) (cid:112) (cid:112) (cid:17)
Rϵ ≤O |U|B+ |U|ξ T +β |U|γ T
exactpriortoproduceavalidposterior. T T T T
O isu imrb po ou sn sid bli es ts otil fil nu dse afu hl ypin ert ph ae rf ar meq eu tee rn vti as lt us ee ut ˆti pn rg os d, uw ch ine gre ai nt and as lim
T→0
R TTϵ → 0, Algorithm 2 enjoys no-regret
propertywithrespecttotheϵ-regret.
RKHSencapsulatingtheRKHSofeveryparameteru∈U.
Forexample,whenusingaperiodickernelwithanunknown Trivially,asbestregretisupperboundedbyaverageregret,
periodP,onecaneasilyseethatfortwocandidateperiod wegetthatthebestregretachievedbytimeT decayswith
hyperparametersP 1 andP 2,therearefunctionswithape- timeatleastasfastasO(√1 ),whichisthesamerateasfor
T
riodofP 1,butwithoutaperiodofP 2andvice-versa,aslong theoriginalStableOptalgorithm.
astheperiodsarenotintegermultiplesofoneanother. An-
othersituation,whereourboundisuseful,iswhenuˆexists HyperparametereliminationvsMLEAsexplainedinthe
butdoesnotbelongtoU andγuˆ islargerthanmax γu. introduction,typeIIMLEinBOisnotguaranteedtorecover
T u∈U T
the true hyperparameter values. However, it is a popular
Anexampleofthiscaseiswhenweuseanadditivekernel,
practicalstrategyoffittingtheunknownhyperparameters
withonetermforeachinteractionbetweentwodimensions.
andassuchitisinterestingtocompareitwiththetheoreti-
AsprovenbyZiomek&Bou-Ammar(2023)inProposition
4.5,ford > 2includingapossiblychangingsubsetofall callysoundstrategyofHE-GP-UCB.Themaindifference
isthatinHE-GP-UCB,welookatthepredictiveerrorofthe
pairwise interactions has a smaller information gain than
model, asopposedtoitslikelihood, andwhenmeasuring
includingallpairwiseinteractionsatonce.
theerrorforthet-thdatapoint,weonlyconditiontheGPon
ExtendingHE-GP-UCBHE-GP-UCBisthealgorithmob- D ,thatisallobservationsuntilt. Inthissense,HE-GP-
t−1
tainedwhencombiningGP-UCBwiththehyperparameter UCBasksretrospectivelyhowgoodthemodelsproduced
eliminationstrategy. Withinthissubsection,weshowhow byagivenhyperparametervalueateachtimesteptare. We
hyperparametereliminationcanbecombinedwiththeSta- nowempiricallycomparethepracticalstrategyofMLEwith
bleOptalgorithm,designedforadversariallyrobustoptimi- thetheoreticallysoundstrategyofHE-GP-UCBandshow
sationwithminorchangesinthealgorithmandtheproofof thereareproblemswherethelatteroutperformstheformer.
regretbound. WebelieveonecansimilarlyextendHE-GP-
UCBtomanymorevariantsofBO.Inthesettinginitially
7.Experiments
proposedbyBogunovicetal.(2018),insteadofsimplymax-
imisingthefunction,wewishtofindapointwhichisstable Wenowempiricallyevaluateourproposedalgorithm. We
undersomeϵ-smallperturbation. Formally,thedefinitionof conduct the evaluation on multiple toy problems, where
eachtimewearegivensomenumberofcandidatehyper-
parametervalues. Thefirstbaselinewecompareagainstis
7No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
frequentist setting Bayesian setting
(a) lengthscale (b) periodic kernel (c) decomposition (d) GP mean
0.15 2 2
0.6
0.10
0.4 1
0.05
0.2 1
0
80
4
10
40
40
2
20
5
0 0
0 25 50 0 25 50 0 25 50 0 50 100
iteration iteration iteration iteration
MLE ExpectedUCB A-GP-UCB HE-GP-UCB (Ours)
Figure2.Empiricalevaluationoftheproposedalgorithmandselectedbaselines.Solidlinesrepresentmeanvaluesandshadedareasare
standarderrors.Theresultsareover50seedsfora),b)andd),and100seedsforc).
theMLE,whichsimplyselectsthehyperparametervalue offormk(x,x′)=(cid:80) k (x ,x′ ),wherex selects
c∈g c [c] [c] [c]
thatproducesamodelwiththehighestmarginallikelihood, onlythedimensionsinsetcandg istheunknownhyper-
i.e. u =argmax p(D|u)andthenoptimisestheUCB parameter. Weusedifferentdecompositionsascandidate
t u∈U
functionUCBut(x)producedbythatmodel. Anotherbase- hyperparametervaluesdetailedinAppendixJ.3. HE-GP-
t
lineisExpectedUCB,whichmaximisestheexpectedUCB UCBwasonaveragequickerinrealisingthatthefirsttwo
functionE [UCBu(x)],whereP(u)= 1 .Onthe dimensions areinseparable andthus itperforms betterin
u∼P(u|D) t |U|
experimentswithunknownlengthscales,wealsocompare thebeginningbeforeothermethodscancatchuptoit.
withA-GP-UCBmethodofBerkenkampetal.(2019). We
UnknownGPmean(d)Finally,wealsorunanexperiment
showcumulativeregretandsimple(best)regretplotsinFig-
in the Bayesian setting. Here the black-box function is a
ure2. SeeAppendixJformoredetailsoneachexperiment.
sample from a GP prior with an RBF kernel and a mean
Unknownlengthscale(a)Inthefirstexperiment,weuse functionthatiszeroeverywhere,exceptforcertainregions
thefunctionproposedbyBerkenkampetal.(2019)(intheir forming”hills”(seeFigure6foragraphicaldepiction).One
Figure2).Thisfunctionisverysmoothinmostofitsdomain ofthosehillsishigherthantheothersandthenumberof
butbecomeslesssmootharoundtheglobaloptimum. The thathighesthillistheunknownhyperparameter. Wecansee
unknownhyperparameterinthiscaseisthelengthscalewith thatExpectedUCBperformsmuchbetterthanMLEdueto
thecandidatesbeing[0.3,0.4,0.5,0.7,1.0]. WhileMLE increasedexplorationcausedbynotcommittingtoanyone
tends to opt for the long lengthscale due to smoothness, specific model. However, HE-GP-UCB still outperforms
oursselectsshortones. BothHE-GP-UCBandA-GP-UCB ExpectedUCBasitexploresthefunctionenoughbybeing
performverysimilarlyandoutperformotherbaselines. doublyoptimisticandthenrejectstheincorrectmodels.
Unknownperiod(b)Weusea1-dimensionalperiodicfunc-
8.Conclusions
tionconsistingoffourdistinctintervals,whicharerepeated
two times withinthe domain, see Figure 4, resulting ina
Withinthispaper,wemanagedtoproposethefirstno-regret
functionwithaperiodof1. Eachoftheintervals,however,
algorithmcapableofconductingBOwithunknownhyper-
containsasinewavewithamuchshorterperiodandassuch,
parametersofarbitraryform. Weprovidedaregretboundin
theMLEoftenfavouredanincorrectperiodvalue. Itwas
bothfrequentistandBayesiansettingsandwealsoshowed
lesslikelyforHE-GP-UCBtobemisledtowardsthewrong
howouralgorithmcanbeeasilyextendedtohandleadver-
periodvalueandassuchitachievedlowerregret.
sariallyrobustBO.Weempiricallyevaluatedouralgorithm
Unknown decomposition (c) We use the 3-dimensional andshowedthattherearecases, whereitcanoutperform
function proposed by Ziomek & Bou-Ammar (2023) (in thetypicallyusedMLE.
theirAppendixC).Thisfunctionlocallyappearstobefully
Onelimitationofourworkisthatitassumesthatthesetof
additive,buttheglobaloptimumliesintheregionwherethe
candidatehyperparametervaluesisfinite. Asexplainedbe-
firsttwodimensionsarenotseparable. Weusethekernel
fore,provingconvergencewithaninfinitenumberofcandi-
8
terger
elpmis
evitalumuc
tergerNo-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
datesisimpossiblewithoutstrongerassumptions. However, Gardner,J.,Pleiss,G.,Weinberger,K.Q.,Bindel,D.,and
itmightbepossibletodeviseamorepracticalversionofour Wilson,A.G. GPyTorch: Blackboxmatrix-matrixGaus-
algorithm,whichwouldautomaticallygenerateanumberof sian process inference with GPU acceleration. In Ad-
discretecandidates. Itcouldthushandlethecaseinwhich vances in Neural Information Processing Systems, pp.
there are infinitely many possible hyperparameter values. 7576–7586,2018.
Thisconstitutesapromisingdirectionoffuturework.
Garnett,R. Bayesianoptimization. CambridgeUniversity
Press,2023.
References
Grosnit,A.,Malherbe,C.,Tutunov,R.,Wan,X.,Wang,J.,
Abbasi-Yadkori,Y.,Pacchiano,A.,andPhan,M.Regretbal-
and Ammar, H. B. BOiLS: Bayesian optimisation for
ancingforbanditandRLmodelselection. arXivpreprint
logicsynthesis. In2022Design,Automation&Testin
arXiv:2006.05491,2020.
EuropeConference&Exhibition(DATE),pp.1193–1196.
IEEE,2022.
Agarwal,A.,Luo,H.,Neyshabur,B.,andSchapire,R.E.
Corrallingabandofbanditalgorithms. InConferenceon
Ha,H.,Nguyen,V.,Zhang,H.,andHengel,A.v.d. Prov-
LearningTheory,pp.12–38.PMLR,2017.
ablyefficientBayesianoptimizationwithunbiasedGaus-
sianprocesshyperparameterestimation. arXivpreprint
Bachoc,F. Crossvalidationandmaximumlikelihoodesti-
arXiv:2306.06844,2023.
mationsofhyper-parametersofGaussianprocesseswith
modelmisspecification. ComputationalStatistics&Data Hoffman,M.,Brochu,E.,anddeFreitas,N. Portfolioallo-
Analysis,66:55–69,2013. cationforBayesianoptimization. InProceedingsofthe
Twenty-SeventhConferenceonUncertaintyinArtificial
Balandat, M., Karrer, B., Jiang, D., Daulton, S., Letham,
Intelligence,pp.327–336,2011.
B., Wilson, A. G., and Bakshy, E. BoTorch: a frame-
work for efficient Monte-Carlo Bayesian optimization. Hong, K., Li, Y., and Tewari, A. An optimization-based
Advancesinneuralinformationprocessingsystems,33: algorithmfornon-stationarykernelbanditswithoutprior
21524–21538,2020. knowledge. In International Conference on Artificial
IntelligenceandStatistics,pp.3048–3085.PMLR,2023.
Berkenkamp,F.,Schoellig,A.P.,andKrause,A. No-regret
Bayesianoptimizationwithunknownhyperparameters. Hvarfner,C.,Hellsten,E.O.,Hutter,F.,andNardi,L. Self-
TheJournalofMachineLearningResearch,20(1):1868– correctingBayesianoptimizationthroughBayesianactive
1891,2019. learning. InThirty-seventhConferenceonNeuralInfor-
mationProcessingSystems,2023.
Bogunovic,I.,Scarlett,J.,Jegelka,S.,andCevher,V. Ad-
Kandasamy, K., Schneider, J., and Po´czos, B. High di-
versariallyrobustoptimizationwithGaussianprocesses.
mensionalBayesianoptimisationandbanditsviaadditive
Advancesinneuralinformationprocessingsystems,31,
models. InInternationalconferenceonmachinelearning,
2018.
pp.295–304.PMLR,2015.
Bull,A.D. Convergenceratesofefficientglobaloptimiza-
Khan, A., Cowen-Rivers, A. I., Grosnit, A., Robert,
tionalgorithms. JournalofMachineLearningResearch,
P. A., Greiff, V., Smorodina, E., Rawat, P., Akbar, R.,
12(10),2011.
Dreczkowski,K.,Tutunov,R.,etal. Towardreal-world
automatedantibodydesignwithcombinatorialBayesian
Chowdhury, S. R. and Gopalan, A. On kernelized multi-
optimization. CellReportsMethods,3(1),2023.
armedbandits. InInternationalConferenceonMachine
Learning,pp.844–853.PMLR,2017.
Lattimore,T.andSzepesva´ri,C. Banditalgorithms. Cam-
bridgeUniversityPress,2020.
Cowen-Rivers,A.I.,Lyu,W.,Tutunov,R.,Wang,Z.,Gros-
nit,A.,Griffiths,R.R.,Maraval,A.M.,Jianye,H.,Wang, Moradipari, A., Turan, B., Abbasi-Yadkori, Y., Alizadeh,
J.,Peters,J.,etal. HEBO:Pushingthelimitsofsample- M.,andGhavamzadeh,M. Featureandparameterselec-
efficienthyper-parameteroptimisation. JournalofArtifi- tion in stochastic linear bandits. In International Con-
cialIntelligenceResearch,74:1269–1349,2022. ferenceonMachineLearning,pp.15927–15958.PMLR,
2022.
Foster, D. and Rakhlin, A. Beyond UCB: Optimal and
efficientcontextualbanditswithregressionoracles. In Pacchiano,A.,Dann,C.,Gentile,C.,andBartlett,P. Regret
InternationalConferenceonMachineLearning,pp.3199– boundbalancingandeliminationformodelselectionin
3210.PMLR,2020. banditsandRL. arXivpreprintarXiv:2012.13045,2020.
9No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury,
J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N.,
and Antiga, L. PyTorch: An imperative style, high-
performancedeeplearninglibrary. Advancesinneural
informationprocessingsystems,32,2019.
Rolland,P.,Scarlett,J.,Bogunovic,I.,andCevher,V. High-
dimensionalBayesianoptimizationviaadditivemodels
withoverlappinggroups. InInternationalconferenceon
artificialintelligenceandstatistics,pp.298–307.PMLR,
2018.
Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M.
Gaussianprocessoptimizationinthebanditsetting: No
regretandexperimentaldesign. InInternationalConfer-
enceonInternationalConferenceonMachineLearning,
pp.1015–1022,2010.
Wang, Z. and de Freitas, N. Theoretical analysis of
BayesianoptimisationwithunknownGaussianprocess
hyper-parameters. arXivpreprintarXiv:1406.7758,2014.
Ziomek,J.K.andBou-Ammar,H. Arerandomdecomposi-
tionsallweneedinhighdimensionalBayesianoptimisa-
tion? InInternationalConferenceonMachineLearning,
pp.43347–43368.PMLR,2023.
10No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
A.DiscussiononMasterAlgorithms
Within this section we discuss issues that make prevent us from simply applying Master algorithms from the bandit
communitytoourproblemsetting. WededicateeachofthesubsectionsbelowtoaparticularMasteralgorithm.
A.1.CORRAL
ThepopularCORRALalgorithm(Agarwaletal.,2017)isamasteralgorithmdesignedforanadversarialsetting. However,
theproofofitsregretboundreliesonboundingthedifferencebetweentheregretoftheactuallyselectedalgorithmandthe
oneofthewell-specifiedalgorithm. ApplyingCORRALtoBOwillthusproducealinearboundsimilartothatofHoffman
etal.(2011), aseventhoughthisdifferencecanbesublinear, theregretofthewell-specifiedalgorithmdependsonthe
varianceatthepointitissuggestingandifthepointactuallychosenbythemasteralgorithmisnotthatonesuggestedbythe
well-specifiedalgorithm,thisvariancemightnotdecreaseandassuchthealgorithmwillnothavetheno-regretproperty.
A.2.RegretBalancing
Regretbalancing(Abbasi-Yadkorietal.,2020;Pacchianoetal.,2020)isanotherpopularstrategyfordesigningmaster
algorithms. However, it only works if all base algorithms have regret bounds growing equally fast with a number of
iterationsT,whichmightnotbethecaseinourproblemsetting. Forexample,thiswillnotbethecasewhenselectingthe
decompositionstheinformationgainandthusregretgrowsdifferentlydependingonthedecompositionasshownbyRolland
etal.(2018).
A.3.FeatureSelectioninLinearbandits
Moradipari et al. (2022) proposed FS-SCB algorithm, which solves the linear bandits problem with unknown features.
Theirregretbounddependsonthenumberofdimensionsofthefeaturevectorandassuchcannotbedirectlyextendedto
thekernelisedcase,wherefeaturevectorsarepossiblyinfinitelydimensional. TheiralgorithmisamodificationofSCB
algorithmofFoster&Rakhlin(2020),whichinthekernelisedsettingachievesthesameregretrateO(T3/4)regardless
ofthekernelfunction,whichisworsethanthekernel-dependentregretrateofGP-UCB.Assuchevenifitispossibleto
kerneliseFS-SCB,itwouldmostlikelyachievethesamekernel-independentscaling,whichwouldbesuboptimalcompared
toexistingkernel-dependnentboundsinBOliterature.
B.AssumptionincontinuousBayesiancase
AssumptionB.1. LetX ⊂ [0,r]d beacompactandconvexset,wherer > 0. Assumethatthekernelku∗ satisfiesthe
followingconditiononthederivativesofasamplepathf. Thereexistconstantsa,b>0suchthat,
(cid:18) (cid:12)
(cid:12) ∂f
(cid:12)
(cid:12)
(cid:19) (cid:18) L2(cid:19)
P sup(cid:12) (cid:12)>L ≤aexp − ,
(cid:12)∂x (cid:12) b
x∈X j
forj ∈[d],where[d]={1,...,d}.
C.ProofofTheorem4.1
Theorem 4.1 (Frequentist setting). Let f ∈ H(cid:0) ku∗(cid:1) , s.t. ∥f∥ ku∗ ≤ B, u∗ ∈ U and denote γ T = max u∈Uγ Tu and
β =max βu. Setβu =B+R(cid:112) 2(γu +1+ln(2/δ))andξ =2R2log|U|πt2 . Then,Algorithm1withprobability
T u∈U T t t−1 t 3δ
atleast1−δachievesthecumulativeregretofatmost:
(cid:16) (cid:112) (cid:112) (cid:17)
R ≤O |U|B+ |U|ξ T +β |U|γ T
T T T T
andaslim T→0 R TT →0,Algorithm1enjoystheno-regretproperty.
Proof. ObservethatbyunionboundTheorem3.1andLemma5.1holdtogetherwithprobabilityatleast1−δ −δ . We
A B
chooseδ =δ = δ. Wewillprovetheregretbound,assumingtheprobabilisticstatementsinTheorem3.1andLemma
A B 2
5.1hold,andassuchtheresultingboundholdswithprobabilityatleast1−δ.
11No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
Preservationofu∗Wefirstlookatthesumoferrortermsforthetruehyperparametervalue,whenline10isexecutedat
someiterationt:
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:88) η t(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12)
(cid:12)
(cid:88) (cid:0) y t−f(x t)+f(x t)−µ∗ t−1(x t)(cid:1)(cid:12) (cid:12)
(cid:12)
(cid:12)i∈Su∗ (cid:12) (cid:12)i∈Su∗ (cid:12)
t t
(cid:12) (cid:12)
(cid:12) (cid:12)
≤(cid:12) (cid:12)
(cid:12)
(cid:88) ϵ t(cid:12) (cid:12) (cid:12)+ (cid:88) (cid:12) (cid:12)f(x t)−µ∗ t−1(x t)(cid:12) (cid:12)
(cid:12)i∈Su∗ (cid:12) i∈Su∗
t t
≤(cid:113) ξ |Su∗|+ (cid:88) βu∗ σu(x ),
t t i i i
i∈Su∗
t
wherethelastinequalityisduetoTheorem3.1andLemma5.1. Assuchtheconditionoftheifstatementalwaysevaluates
tofalseandu∗ ∈U foranyt≥0.
t
BoundonsimpleregretIfu∗ ∈U ,wetriviallyhave:
t−1
f(x∗)≤µu∗ (x∗)+βu∗ σu∗ (x∗)
t−1 t t−1
≤ maxmaxµu (x )+βu∗ σu (x ). (3)
t−1 t t t−1 t
xt∈Xu∈Ut
Forsimplicitywewillwriteµ+ =µut andσ+ =σut . Letusnowdefinetheerrorofthelowerboundattimetand
t−1 t−1 t−1 t−1
pointx asfollows:
t
e (x )=f(x )−(cid:0) µ+ (x )−β+σ+ (x )(cid:1) (4)
t t t t−1 t t t−1 t
=y −ϵ −(cid:0) µ+ (x )−β+σ+ (x )(cid:1)
t t t−1 t t t−1 t
=η −ϵ +β+σ+ (x )
t t t t−1 t
Letuslookattheinstantaneousregret. CombiningInequality3andEquation4,weget:
r =f(x∗)−f(x )
t t
≤µ+ (x )+β+σ+ (x )
t−1 t t t−1 t
−µ+ (x )+β+σ+ (x )−e (x )
t−1 t t t−1 t t t
≤2β+σ+ (x )−e (x )=−η +ϵ +β+σ+ (x ). (5)
t t−1 t t t t t t t−1 t
BoundoncumulativeregretLetusdefineC tobethesetof”critical”iterationsasbelow:
(cid:40) (cid:12) (cid:12) (cid:41)
C = t≥1:(cid:12) (cid:12)
(cid:12)
(cid:88) η i(cid:12) (cid:12) (cid:12)>(cid:112) ξ t|Sut|+ (cid:88) β iutσ iut(x i)
(cid:12) (cid:12)
i∈Sut i∈Sut
Noticethatforeacht∈C,wewilldiscardonehyperparametervalueandassuch|C|≤|U|. Onthoseiterations,wecan
possiblysufferashighofregretaspossible,thatisfort∈C wehaver t ≤2|f(x)| ∞ ≤2|f(x)| ku∗ ≤2B. UsingEquation
5,wethusseethatthecumulativeregretwillbeboundedasfollows:
(cid:88) (cid:88)
R ≤ r + r
T t t
t∈C t∈/C
(cid:88) (cid:88) (cid:88)
≤2|U|B− η + ϵ + β+σ+ (x )
t t t t−1 t
t∈/C t∈/C t∈/C
(cid:88) (cid:88) (cid:88)
=2|U|B+ β+σ+ (x )+ (ϵ −η ) (6)
t t−1 t t t
t∈/C u∈Ut∈Su\C
T
12No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
Whenitcomestothetermsη ,observethatift∈/ C theifstatementinline10neverevaluatedtotrue. Foreachu∈U we
t
thushave:
(cid:88) (cid:88) (cid:88)(cid:113) (cid:88) (cid:88)
−η ≤ ξ |Su|+ βuσu(x ). (7)
t T T t t i
u∈Ut∈Su\C u∈U u∈Ut∈Su\C
T T
DuetoLemma5.1,wehave:
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
ϵ = ϵ + ϵ
t t t
u∈Ut∈Su\C u∈U;u̸=u∗t∈Su\C t∈Su∗
T T T
(cid:113) (cid:88) (cid:113) (cid:88)(cid:113)
≤ ξ |Su∗|+ ξ |Su \C|≤ ξ |Su|. (8)
T T T T T T
u∈U;u̸=u∗ u∈U
CombiningInequality7withInequality8andpluggingintoInequality6:
(cid:88)(cid:113) (cid:88)
R ≤2|U|B+2 ξ |Su|+2 β+σ+ (x ). (9)
T T T t t−1 t
u∈U t∈/C
Dealingwith|Su|ApplyingLemma5.2toInequality9weget:
T
(cid:112) (cid:88)
R ≤2|U|B+2 |U|ξ T +2 β+σ+ (x ). (10)
T T t t−1 t
t∈/C
ExpressingboundintermsofMIGApplyingLemma5.3toEquation10,weget:
(cid:112) (cid:112)
R ≤2|U|B+2 |U|ξ T +2β CT|U|γ
T T T T
D.ProofofTheorem4.2
Theorem 4.2 (Bayesian setting). Let f ∼ GP(µu∗(·),ku∗(·,·)), ϵ ∼ N(0,R2) for each t and u∗ ∈ U and denote
0 t
γ =max γu. Setξ =2R2log|U|π2t2 and:
T u∈U T t 3δ
(cid:113)
(A) if|X|<∞,setβu = 2log|X|π2t2
t 3δ
(B) if|X|=∞,letAssumptionB.1holdandset
(cid:118)
(cid:117)
(cid:117)
(cid:18) 2π2t2(cid:19) (cid:32) (cid:115) (cid:18) 4da(cid:19)(cid:33)
βu =(cid:116)2log +4dlog dtbr log
t 3δ δ
Then,Algorithm1withprobabilityatleast1−δachievesthecumulativeregretofatmost:
(cid:16) (cid:112) (cid:112) (cid:17)
R ≤O |U|B+ |U|ξ T +β |U|γ T
T T T T
forsomeconstantB >0andaslim T→0 R TT →0,Algorithm1enjoystheno-regretproperty.
Proof. Weconsidertwocases.
If|X|<∞
By union bound Theorem 3.1, Lemma 5.1 hold together with probability at least 1−δ −δ and we choose δ =
A B A
δ = δ = δ. Sinceconfidenceintervalsarevalidandnoiseisboundedthenu∗ ∈ U foranyt ≥ 0andweobtainthe
B C 2 t
13No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
instantaneousregretboundr ≤ϵ −η +β+σ+ (x )bythesameargumentasinfrequentistcase. Nowwehavethatfor
t t t t t−1 t
t∈C,theinstantregrethasboundr ≤2B(duetoLemma5.4). Thusthecumulativeregretboundin9becomes:
t
(cid:88)(cid:113) (cid:88)
R ≤2|U|B+2 ξ |Su|+2 β σ+ (x ).
T T T t t−1 t
u∈U t∈/C
Alltheremainingstepsarethesameasinthefrequentistcase.
If|X|=∞
ByunionboundTheorem3.2andLemma5.1holdtogetherwithprobabilityatleast1−δ −δ andwechooseδ =δ = δ.
A B A B 2
Sinceconfidenceintervalsarevalidfortheselectedpoints{x }T andnoiseisboundedthenu∗ ∈U foranyt≥0bythe
t t=1 t
sameargumentasinthefrequentistcase. However,theinstantaneousregretboundwillchangeaswerelyonTheorem3.2
insteadof3.1. Asu∗ ∈U ,wehave:
t−1
1
f(x∗)≤µu∗ ([x∗] )+βu∗ σu∗ ([x∗] )+
t−1 t t t−1 t t2
1
≤ maxmaxµu (x )+βuσu (x )+ .
xt∈Xu∈Ut t−1 t t t−1 t t2
Assuch,theinstantaneousregretboundwillber ≤ϵ −η +β+σ+ (x )+ 1. Therestofthestepsfollowsthefrequentist
t t t t t−1 t t2
case,withtheexceptionofboundingtheregretoniterationst∈C,whereweobtainthesameboundasinthecase|X|<∞.
Thisyieldsthefinalregretboundof:
T
(cid:112) (cid:112) (cid:88) 1 (cid:112) (cid:112) π
R ≤2|U|B+2 |U|ξ T +2β CT|U|γ + ≤2|U|B+2 |U|ξ T +2β CT|U|γ +
T T T T t2 T T T 6
t=1
E.ProofofLemma5.1
Lemma5.1. Foreachu∈U andt=1,...,T wehave:
 (cid:12) (cid:12) 
(cid:12) (cid:12)
(cid:12)(cid:88) (cid:12) (cid:112)
P  t=1∀ ,...,Tu∈∀ U(cid:12)
(cid:12)
ϵ i(cid:12) (cid:12)≤ ξ t|S tu|≥1−δ B,
(cid:12)i∈Su (cid:12)
t
whereξ
=2R2log|U|π2t2
.
t 6δB
Proof. Letusconsidersomefixedu∈U. Letusdefineanewsetofi.i.d. randomvariablesv fori=1,...,T,whereeach
i
ofthemfollowsthesameR-subgaussiandistributionasthenoise. Sincev isR-subgaussian,forany0<n≤T wehave:
i
(cid:32)(cid:12) (cid:12)(cid:88)n (cid:12) (cid:12) √ (cid:33) (cid:18) −An(cid:19) (cid:18) −A(cid:19)
P (cid:12) v (cid:12)≥ An ≤2exp =2exp ,
(cid:12) i(cid:12) 2R2n 2R2
(cid:12) (cid:12)
i=1
whichisawell-knownresultintheliterature(e.g. seeCorollary5.5inLattimore&Szepesva´ri(2020)). Letussample
eachv fori=1,...,T beforetheexperimentstartsandletthembeahiddenstateoftheenvironment. Wheneverforthe
i
currentiterationt,wehavethatthesetSu isexpanded,wesetthenoiseofthatiterationasϵ =v ,wherei=|Su|. Such
t−1 t i t
aconstructednoisemusthavethesamedistributionastheoriginalnoise. Foreachi=1,...,T,letusnowdefinetheevent
E as:
n (cid:12) (cid:12)
(cid:12)(cid:88)n (cid:12) (cid:112)
E : (cid:12) v (cid:12)> ξ n.
n (cid:12) i(cid:12) n
(cid:12) (cid:12)
i=1
Wenowhave:
 (cid:12) (cid:12)
(cid:12)(cid:88)
(cid:12) (cid:12)
(cid:12) (cid:112)
  | (cid:91)S Tu|  (cid:32) (cid:91)T (cid:33)
P ∃ t=1,...,T (cid:12)
(cid:12)
ϵ n(cid:12) (cid:12)> ξ n|S tu|=P  E n≤P E n
(cid:12)n∈Su (cid:12) n=1 n=1
t
14No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
T T (cid:18) (cid:19) T
≤ (cid:88) P (E )≤2(cid:88) exp −ξ t =(cid:88) 6δ B = δ B ,
n 2R2 |U|π2t2 |U|
n=1 t=1 t=1
wherethefirstinequalityisduetothefactthat(cid:83)|S Tu| E ⊆(cid:83)T E as|Su|≤T. Wefinishtheproofbytakingaunion
n=1 n n=1 n T
boundoverallu∈U.
F.ProofofLemma5.2
Lemma5.2. Wehavethat:
(cid:88)(cid:113) (cid:112)
|Su|≤ |U|T.
T
u∈U
Proof. Weareinterestedinsolvingtheoptimisationproblem:
(cid:88)√
max x (11)
u
x∈R|U|
u∈U
(cid:88)
s.t. x =T ∀ 0≤x ≤|U|
u u∈U u
u∈U
Wewillprovethesolutionisx = T foreachu ∈ U andthusoptimalvalueis(cid:112) |U|T. Thisprovesthestatementby
u |U|
identifying x = |Su|. Note that|Su| canonly take discretevaluesfrom {1,...,|U|}. Weconsider relaxationof this
u T T
problemandassuchtheoptimumvaluewefindmustbeatleastaslargeasfortheoriginalproblem. Wewillproveby
inductionthatthesolutiontoProblem11isx = T oreachu∈U. Considerthecasewhen|U|=2. Letv ∈U,thenthe
u |U|
functionwewanttooptimiseis:
√ (cid:112)
f(x )= x + T −x ,
v v v
andbytakingitsderivativeandsettingittozerowegetx∗ = T. Assumingthatfor|U|=nthestatementholds,wewill
v 2
proveitmustalsoholdfor|U|=n+1. Inthelattercasethefunctionwewanttooptimiseis:
(cid:115)
f(x )=√ x + (cid:88) √ x =√ x + (cid:88) T −x v,
v v u v |U|−1
u∈U;u̸=v u∈U;u̸=v
√
(cid:80)
wherethesecondequalityistrueasiftheinductionhypothesisholdswemusthavethatthesolutionto x
u∈U;u̸=v u
subjecttoconstraint(cid:80) u∈U;u̸=v = T −x v isx∗ u = |UT \− {xx vv }| = |T U− |−xv 1 forallu ∈ U;u ̸= v. Again,takingderivativeof
f(x )andsettingittozerogivesx∗ = T andthusx∗ = T− |UT | = T forallremainingu∈U;u̸=v.
v v |U| u |U|−1 |U|
G.ProofofLemma5.3
Lemma5.3. ThereexistsaconstantC >0,suchthat:
(cid:88) (cid:112)
β+σ+ (x )≤β CT|U|γ ,
t t−1 t T T
t∈/C
whereβ =max βu andγ =max γu.
T u∈U T T u∈U T
Proof. Firstly,observethatduetoCauchy-Schwarzwehave:
(cid:115) (cid:118)
(cid:88) β t+σ t+ −1(x t)≤β T (T −|C|)(cid:88) (σ t+ −1)2(x t)≤β T(cid:117) (cid:117) (cid:116)T (cid:88) (cid:88) (σ tu −1)2(x t), (12)
t∈/C t∈/C u∈U t∈/C
ut=u
whereβ =max βu. Wewillneedtointroducesomenewnotation. ForsomesetofinputsA⊂X,letusdefine:
T u∈U T
(σu)2(x)=k(x,x)u−ku(x)T(Ku +σ2I)−1ku(x),
A A A A
15No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
where Ku∗ = [ku∗(x,x′)] and ku∗ = [ku∗(x,x′)] Notice that under such notation σu = σu , where
A x,x′∈A A x′∈A t−1 Xt−1
X ={x }t−1. LetusdefineAu ={x :1≤i≤t−1,u =u}tobethesetofpointsqueriedwhenu =u. Forany
t−1 i i=1 t−1 i t t
twosetsS,S′suchthatS′ ⊆S wehaveσu (x)≤σu(x)forallx∈X. AsAu ⊆X ,wehave:
S′ S t−1 t
(σ+ )2(x )=(σu )2(x )≤(σu )2(x )≤Clog(1+R−2(σu (x))2),
t−1 t Xt t Au
t−1
t Au
t−1
forsomeC (dependingonR),wherethelastinequalityistrueduetothesamereasoningasintheproofofLemma5.4in
(Srinivasetal.,2010). ApplyingthisfacttoInequality12gives:
(cid:118) (cid:118)
(cid:117) (cid:88) (cid:88) (cid:117) (cid:88) (cid:88)
β T(cid:117) (cid:116)T (σ tu −1)2(x t)≤β T(cid:117) (cid:116)CT log(1+R−2(σ Au u t−1(x))2)
u∈U t∈/C u∈U t∈/C
ut=u ut=u
(cid:115)
(cid:88) (cid:112)
=β CT γu ≤β CT|U|γ
T |Au | T T
t−1
u∈U
wherethepenultimateequalityfollowsfromLemma5.3from(Srinivasetal.,2010).
H.ProofofLemma5.4
Lemma5.4. Forf ∼GP(µ (·),k(·,·))forsomemeanandkernelfunctionsµ (·)andk(·,·)withproperties|µ (·)|<∞
0 0 0
andk(·,·)≤1,thereexistsaconstantB >0,suchthat:
sup|f(x)|≤B
x∈X
aslongas|X|<∞andtheeventofTheorem3.1holdsor|X|=∞andeventofTheorem3.2holds.
Proof. Wewillconsidertwocases,dependingoncardinalityof|X|.
If|X|<∞
NotethatifeventofTheorem3.1holds,thenatt=1,wehavethatforallx∈X:
|f(x)−µ (x)|≤β σ (x)≤β k(x,x)≤β
1 1 0 1 1
Assuch:
max|f(x)|≤β +max|µ (x)|=B,
1 0
x∈X x∈X
whichcompletestheproof.
If|X|=∞
NotethatifeventofTheorem3.2holds,thenatt=1,wehavethatforallx∈X:
|f(x)−µ ([x)] |≤β σ ([x] )+1≤β k([x] ,[x] )+1≤β +1
1 1 1 0 1 1 1 1 1
Assuch:
sup|f(x)|≤β + max|µ (x)|+1≤β + sup|µ (x)|+1=B,
1 0 1 0
x∈X x∈Dt x∈X
whichcompletestheproof.
16No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
Algorithm2StableOptwithHyperparameterElimination
(HE-StableOpt)
Require: a set of possible hyperparameters values U; confidence parameters {ξ }T and {{βu}T } ; stability ϵ,
t t=1 t t=1 u∈U
distancefunctiond(·,·)
1: SetD 0 =∅,U 0 =U,Su =∅,Sˆu =∅foreachu∈U
2: fort=1,...,T do
3: foru∈U t−1do
4: FitaGP(µu t−1(x),σ tu −1(x))toD t−1
5: SetUCBu (x)=µu (x)+βuσu (x)
t−1 t−1 t t−1
6: SetLCBu (x)=µu (x)−βuσu (x)
t−1 t−1 t t−1
7: endfor
8: Findx t,u t = argmax min δ∈∆ϵ(x)UCBu t−1(x+δ)
x,u∈X×Ut−1
9: Findδ t,v t = argmin LCBu t−1(x t+δ)
δ,u∈∆ϵ(xt)×Ut−1
10: S tut =S tu −t 1∪{t}andS tu =S tu −1foru∈U;u̸=u t
11: Sˆ tvt =Sˆ tv −t 1∪{t}andSˆ tu =Sˆ tu −1foru∈U;u̸=v t
12: Querytheblack-boxy t =f(x t+δ)
13: SetD t =D t−1∪(x t+δ t,y t)
14: Setη t =y t−µu t−t 1(x t+δ t)
15: Setηˆ t =y t−µv t−t 1(x t+δ t)
16: if(cid:12) (cid:12) (cid:12)(cid:80) i∈Sut η i(cid:12) (cid:12) (cid:12)>(cid:112) ξ t|S tut|+(cid:80) i∈Sut β iutσ iut(x i+δ i) then
t t
17: U t =U t−1\{u t}
18: endif
(cid:12) (cid:12) (cid:113)
19: if(cid:12) (cid:12)(cid:80) i∈Sˆvt ηˆ i(cid:12) (cid:12)> ξ t|Sˆ tvt|+(cid:80) i∈Sˆvt β ivtσ ivt(x i+δ i) then
t t
20: U t =U t−1\{v t}
21: endif
22: endfor
17No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
I.AdversariallyRobustOptimisation
Theorem6.1. Assumetheconditionsofandsetβu andξ asin,Theorem4.1(frequentist)orasinTheorem4.2caseB
t t
(continuousBayesian). Then,Algorithm2withprobabilityatleast1−δachievesthecumulativeϵ-regretofatmost:
(cid:16) (cid:112) (cid:112) (cid:17)
Rϵ ≤O |U|B+ |U|ξ T +β |U|γ T
T T T T
andaslim
T→0
R TTϵ →0,Algorithm2enjoysno-regretpropertywithrespecttotheϵ-regret.
Proof. WestartwiththefrequentistcaseandthencommentonthedifferenceinBayesiancase. Observethatbyunion
boundTheorem3.1andLemma5.1holdtogetherwithprobabilityatleast1−δ −δ . Wechooseδ =δ = δ. Wewill
A B A B 2
provetheregretbound,assumingtheprobabilisticstatementsinTheorem3.1andLemma5.1hold,andassuchtheresulting
boundholdswithprobabilityatleast1−δ.
ByexactlythesameargumentasintheproofofTheorem4.1,wemusthavethatu∗ ∈U forall0≤t≤T,withtheonly
t
differencethatx isreplacedwithx +δ . Letusnowlookattheinstantaneousregret:
t t t
r =max min f(x+δ)− min f(x +δ)
t t
x∈Dδ∈∆ϵ(x) δ∈∆ϵ(xt)
≤max min
UCBu∗
(x+δ)− min
LCBu∗
(x +δ)
t t t
x∈Dδ∈∆ϵ(x) δ∈∆ϵ(xt)
≤UCBut(x +δ )−LCBvt(x +δ )
t t t t t t
=µut (x +δ )−y +βutσut (x +δ )−(cid:0) y −µvt (x +δ )−βvtσvt (x +δ )(cid:1)
t−1 t t t t t−1 t t t t−1 t t t t−1 t t
=η +βutσut (x +δ )+ηˆ +βvtσvt (x +δ ),
t t t−1 t t t t t−1 t t
wherethefirstinequalityisduetoTheorem3.1andsecondduetodefinitionsofx ,δ ,u andv . Similarlyasintheproof
t t t t
ofTheorem4.1,wedefinethesetofcriticaliterationsas:
 (cid:12) (cid:12) (cid:12) (cid:12) 
C = 0≤t≤T :(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)i∈(cid:88)
S
tutη i(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)>(cid:112) ξ t|S tut|+ i∈(cid:88)
S
tutβ iutσ iut(x i+δ i)OR (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)i∈(cid:88)
Sˆ
tvtη i(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)>(cid:113) ξ t|Sˆ tvt|+ i∈(cid:88)
Sˆ
tvtβ ivtσ ivt(x i+δ i) .
NoticethatwewillrejectonehyperparametervalueforeachiterationinC andassuch|C| < |U|. Wenowlookatthe
cumulativeregret:
T
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
r ≤ r + r ≤2|U|B+ η + β σut (x +δ )+ ηˆ + β σvt (x +δ ).
t t t t t t−1 t t t t t−1 t t
t=1 t∈C t̸∈C t̸∈C t̸∈C t̸∈C t̸∈C
ObservethatduetodefinitionofC,wehave:
(cid:88) (cid:88) (cid:88) (cid:88)(cid:113) (cid:88) (cid:88)
ηˆ = −η ≤ ξ |Su|+ βuσu(x +δ )
t t T T t t i i
t̸∈C u∈Ut∈Su\C u∈U u∈Ut∈Su\C
T T
(cid:113)
(cid:88) ηˆ = (cid:88) (cid:88) −ηˆ ≤ (cid:88) ξ |Sˆu|+ (cid:88) (cid:88) βuσu(x +δ ).
t t T T t t i i
t̸∈C u∈Ut∈Sˆu\C u∈U u∈Ut∈Sˆu\C
T T
Continuingwiththecumulativeregretbound,weget:
(cid:88)T
r ≤2|U|B+
(cid:88)(cid:113)
ξ
|Su|+2(cid:88)
βutσut (x +δ )+
(cid:88)(cid:113)
ξ
|Sˆu|+2(cid:88)
βvtσvt (x +δ ).
t T T t t−1 t t T T t t−1 t t
t=1 u∈U t̸∈C u∈U t̸∈C
ApplyingLemma5.2and5.3totheboundaboveweget:
T
(cid:88) (cid:16) (cid:112) (cid:112) (cid:17)
R = r ≤O |U|B+ |U|ξ T +β T|U|γ ,
T t T T T
t=1
whichfinishestheproofinthefrequentistcase. IntheBayesiancase,werelyonTheorem3.2insteadofTheorem3.1. The
restoftheprooffollowsinthesamemannerexceptthatnowwehaveanadditionalterm(cid:80) 1 ≤ π2 ,whichjustaddsa
t∈/C t2 6
constantwithoutaffectingtheregretbound.
18No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
J.ExperimentalDetails
We have implemented the code based on PyTorch (Paszke et al., 2019), GPyTorch (Gardner et al., 2018),
BoTorch (Balandat et al., 2020). To compute the maximum information gain, we used the implementa-
tion in the code https://github.com/kihyukh/aistats2023 (Hong et al., 2023). Our code is ready to review on
https://anonymous.4open.science/r/HE-GP-UCB-6868/. We have run our experiments on a MacBook Pro 2019, 2.4
GHz8-CoreIntelCorei9,64GB2667MHzDDR4. Forallexperiments,weselectedδ =0.1.
J.1.Unknownlengthscale
Figure3. ThesyntheticfunctionandGPsurrogatemodelwithusedforunknownlengthscaleproblemsetting.
Figure3showsthesyntheticfunctionusedfortheunknownlengthscaleproblem,firstlyintroducedinBerkenkampetal.
(2019). Weusedthefollowingequationtoproducethis:
f(x):=0.6x+0.8N(x;0.2,0.08) (13)
Thisfunctionisbroadlysmoothandhasagloballineartrend. IfthelimiteddatadoesnotcontainpointsfromtheGaussian
peak,asisthecaseinFigure3,MLEoptsforthesmoothinterporation,namelythelonglengthscale,andcannotcontain
thebumpwithintheconfidenceinterval. Thus,misspecifiedlonglengthscaleUCBleadstopersistentlyqueryingatthe
rightmostpoint,andwillneverfindthetrueglobalmaximumatthepeak. Ontheotherhand,shorterlengthscalecanhave
alargerconfidenceintervalthusitcanfindtheglobalmaximum. WeimplementedtheA-GP-UCB(Berkenkampetal.,
2019)usingtheirequation(18),andwesetg(t):=t0.9,followingtheirsection5. OriginalA-GP-UCBisforcontinuous
hyperparameters,soweusethenearestvaluesfromthecandidatelistinstead. Weusedthreeuniformi.i.d. samplesfromthe
domain[0,1]astheinitialsamples,thenweiteratedBOfor50iterations,andrepeatedtheBOexperiments50timeswith
differentrandomseeds.
J.2.Unknownperiod
Weusea1-dimensionalperiodicfunction,ofwhichthesignalissegmentedintofourdistinctintervalsandisrepeatedtwo
timeswithinthedomain,assuch:

sin(8π(x−1/8)), ifx<0.25
1.5sin(28/3π),
if0.25≤x<0.5
f(x)= . (14)
1 0. .2 8s si in n( (8 8π π( (x x− −1 5/1/ 7)2 )8 ,)), i if f0 5. /5 7≤ ≤x x<5/7
Theabovefunctionisboundedfor[0,1],andwerepeatthesamesignalto[1,2]tobeperiodical. Thewholedomainis[0,2].
WeshowtheplotofthefunctioninFigure4. Thisfunctionismisleading,asithasfourlocalperiodicsinewaves. Locally
19No-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
Figure4. Thesyntheticfunctionusedfortheunknownperiodproblemsetting.
fittingtheperiodhyperparametertoeachofsinewaveswillneverproducethetrueperiodof1.
WegeneratethetruehyperparameterbyfittingaGPusingmaximumlikelihoodwithalargenumberofuniformsamples
fromthewholedomain[0,2]. TheotherhyperparametercandidatesareproducedbyfittingaGPwithmaximumlikelihood
foreachofthelocalsinewaves. Weusedfiveuniformi.i.d. samplesfromthedomain[0,2]astheinitialsamples,thenwe
iterateBOfor50iterations,andrepeattheBOexperiments50timeswithdifferentrandomseeds.
J.3.Unknowndecomposition
Weusedthethree-dimensionalfunctioninitiallyproposedby(Ziomek&Bou-Ammar,2023),whichweshowinFigure5.
Althoughthefunctionisthree-dimensional,thelastdimensionisredundantandthusweonlyshowtheheatplotwithrespect
tothefirsttwodimensions. Weuseakernelofformk(x,x′)=(cid:80) k (x ,x′ ),withcandidatesforhyperparameterg
c∈g c [c] [c]
beingU ={((1),(2),(3)),((1,2),(3)),((1),(2,3)),((2),(1,3))}withthesecondhyperparameterbeingthetrueone. We
run100seedsforeachbaselineandeachseedconsistedof50iterations.
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
x1
Figure5.Heatplotofthefunctionusedforunknowndecompositionproblemsetting.Thefunctionusedisactuallythree-dimensional,but
asthelastdimensionx3isredundant,thefunctionvaluedoesnotchangeevenwhenx3changes.Thefunctionlocallyappearsasifit
couldbewrittenasanadditivedecomposition,however,thisisnotpossibleduetothecorrelatedmodeinthetoprightcorner.
20
2xNo-regretBayesianOptimisationWithUnknownHyperparametersOfAnyType
J.4.UnknownGPmean
WeusethemeanfunctionshowninFigure6,wherethereareten”hills”andone”hill”ishigherthanothers. Thenumber
ofthehighesthillistheunknownhyperparameter. ThecandidatesetisU = {1,2,3,4,5,6,7,8,9,10,−1}, where−1
correspondstononeofthehillsbeingthetallest. Foreachseed,wesampleanewblack-boxfunctionfromtheGPwiththe
aforementionedmeanfunctionandanRBFkernel. Werun50seedsforeachbaselineandeachseedfor100iterations. Each
baselinewasrunwiththesameseedsandassuch,withthesamesetof50random,black-boxfunctions.
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
0.0 0.2 0.4 0.6 0.8 1.0
x
Figure6.Plot of the mean function used for the unknown GP mean problem setting. Blue and orange curves show prior means
correspondingtothesecondandthirdhillbeingtallest.Thegreencurveshowsthecase,wherenoneofthehillsistallest.
21
)x(naem