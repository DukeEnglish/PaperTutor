Style Vectors for Steering Generative Large Language Models
KaiKonen SophieJentzsch DiaouléDiallo PeerSchütt
OliverBensch RoxanneElBaff DominikOpitz TobiasHecking
InstituteforSoftwareTechnology,GermanAerospaceCenter(DLR)
{first}.{last}@dlr.de
Abstract
Input Prompt Layer 0
...
"How is the weather?"
This research explores strategies for steering Layer 18
the output of large language models (LLMs) +
Style Vector for layer 18
towardsspecificstyles,suchassentiment,emo-
Layer 19
tion, orwritingstyle, byaddingstylevectors Style Vector for layer 19 +
totheactivationsofhiddenlayersduringtext
Layer 20
generation. We show that style vectors can Style Vector for layer 20
+
be simply computed from recorded layer ac-
Target Style: Positive
tivations for input texts in a specific style in Layer 21
...
contrast to more complex training-based ap-
Stylized Output
proaches. Through a series of experiments, "The weather is great!" Layer N-1
wedemonstratetheeffectivenessofactivation
engineering using such style vectors to influ- Figure1: TheLLMoutputissteeredbyaddingstyle
ence the style of generated text in a nuanced vectorstoselectedlayers(e.g.,layers18-20)duringa
andparameterisableway,distinguishingitfrom forward pass. For example, the answer of the LLM
prompt engineering. The presented research totheinputprompt“Howistheweather?” issteered
constitutesasignificantsteptowardsdevelop- towardsapositivestyle,withasampleanswerof“The
ingmoreadaptiveandeffectiveAI-empowered weatherisgreat!”,apositiveanswer.
interactivesystems.
1 Introduction
formationsearch(Alessioetal.,2023;Shahetal.,
2023), human-AI co-creation (Yuan et al., 2022;
Largelanguagemodels(LLMs)pre-trainedonvast
Chungetal.,2022),orcomplexgoal-orienteddia-
corporahavemarkedasignificantmilestoneinnat-
logues(Snelletal.,2022).
ural language processing, presenting remarkable
languageunderstandingandgenerationcapabilities. Inthesecomplexsettings, textgenerationona
ModelslikeGPT-2(Radfordetal.,2019)andmore lexical level alone is not sufficient for effective
recentvariantssuchasGPT-3(Brownetal.,2020) human-AIinteraction. Overandabovethat,acog-
andGPT-4(OpenAI,2023)havebecomeinfluen- nitive AI assistant should also be able to adapt
tialintransformingthelandscapeoftextgeneration. to the human user on an affective and emotional
LLMshavethepotentialtoencodeextensivepub- levelregardingengagement,regulation,decision-
licknowledgeandcanrespondtoawidearrayof making,anddiscovery(Zhaoetal.,2022). There
textpromptsinamannerthatoftencloselyresem- is evidence that LLMs perform well on affective
bleshumancommunication. OpenAI’sChatGPT, computingtasks,suchassentimentclassification
in particular, has garnered substantial attention, andpersonalityprediction,andcanhaveemotional
propelling discussions about generative AI from dialoguecapabilitiestosomeextent. However,the
the scientific community into the broader public resultingcapabilitiesdonotgofarbeyondsimpler
sphere(Brownetal.,2020;OpenAI,2023). Inthis specializedmodels,presumablyduetotheLLMs’
era of ever-advancing AI, it is becoming increas- generality (Zhao et al., 2023; Amin et al., 2023).
inglyapparentthatLLM-basedartificialassistants Thislimitationcallsformechanismstobettercon-
willplayaprominentroleinbothprofessionaland trolimplicitinformationandthestyleofanLLM’s
personalcontexts(Benderetal.,2021;Zhaoetal., output.
2023). Examples of these are conversational in- Prompt engineering has been a promising ap-
4202
beF
2
]LC.sc[
1v81610.2042:viXra
Pretrained
LLMproachinhuman-AIcollaborativetasks,improving cessitate training and fine-tuning whole models,
task efficiency and user collaboration (Wu et al., thesemethodsarenotapplicabletostate-of-the-art
2022). However,itisoftenhighlytask-specificand LLMs, given the immense parameter count and
entailsmanuallycraftingprompts. trainingcostsofLLMs(Huetal.,2021).
In this paper, we build upon and extend the
Anotherlineofresearchworthmentioningthat
worksofSubramanietal.(2022)andTurneretal.
aims to employ alternative approaches to the tra-
(2023),whichfocusonsteeringtheoutputofLLMs
ditional fine-tuning approach is the parameter-
by modifying their internal states. In a series of
efficienttransferlearningapproach(Houlsbyetal.,
experiments,usingdatasetsoftextsampleslabeled
2019)usingadaptermodules,whichseektomini-
withsentimentsandemotioncategories,weshow
mizetrainableparameters. Incontrast,inourwork,
that one can derive a vector representation of a
wefocusonadifferentefficiencyaspect,notonly
desired style class (e.g., positive sentiment) that,
on the minimal computational resources but also
whenaddedtotheactivationofcertainlayersofan
ontheminimaldataresourcesused.
LLM(inthisworkLLaMa(Touvronetal.,2023)),
A related but conceptually different approach
itsoutputshowscharacteristicsofthisstyleclass
toaffecttheoutputofLLMsistextstyletransfer
(see Fig. 1). Our experiments show that the ef-
(TST)(Jinetal.,2022;Reifetal.,2022). TSTaims
fect of the changed models is more salient when
to transfer the style of a given text into a desired,
promptedwithsubjectiveinput(e.g.,“Howdoyou
different style. In contrast, steering LLMs deals
define art?”) rather than with factual input that
withthetaskofgeneratingaresponseinadesired
allows little degrees of freedom (e.g., “What is
style. We refer to Jin et al. (2022) for a detailed
the world’s longest river?”). Our research aims
overviewofTST.
tobridgethegapbetweentheLLM’scapabilities
andthenuancedrequirementsofhuman-AIinter- Prompt engineering (Keskar et al., 2019; Rad-
actions,thusextendingthisnoveldimensiontothe ford et al., 2019; Shin et al., 2020; Brown et al.,
realmofcontrollingLLMoutputs. 2020;Lesteretal.,2021;LiandLiang,2021;Wei
An open-source implementation of the algo- etal.,2022;Wuetal.,2022)focusesoncontrolling
rithmsusedinthispaperisavailable1.
anddirectingtheoutputofalanguagemodelbyde-
signinginputpromptsorinstructions. Bytailoring
2 BackgroundandRelatedWork
thenaturallanguageprompts,themodel’soutput
canbesteeredtowardsproducingresponsesinthe
The introduction of transformer architectures in
desiredstyle.
neuralnetworks(Vaswanietal.,2017)hasledtoa
massiveleapinthedevelopmentofcontextualized Some recent approaches move in a new direc-
languagemodels,suchasGPT(Brownetal.,2020). tionbymodifyingthelayeractivationsofanLLM
Thesenovellargelanguagemodels(LLMs)capture during the forward pass (Subramani et al., 2022;
relationsinthenaturaldataandimplicitlyencode Turneretal.,2023;Hernandezetal.,2023). These
an unlimited number of more abstract concepts, approaches can be grouped under the term of ac-
such as sentiment or style. This quality has been tivationengineering. Subramanietal.(2022)pre-
exploitedinseveralrecentinvestigationsandcan sentedso-calledsteeringvectorsthat,whenadded
be both a risk (Wagner and Zarrieß, 2022) and a totheactivationsatcertainlayersofanLLM,steer
chance(Schramowskietal.,2022). themodeltogenerateadesiredtargetsentencex
Manyapproacheshavebeendevelopedwiththe from an empty input. The rationale behind this
aimofcontrollingoraffectingtheoutputofLLMs, is that the information needed to produce the tar-
also referred to as steering LLMs (Brown et al., getsentenceisalreadyencodedintheunderlying
2020;Zhangetal.,2022;Jinetal.,2022). neuralnetwork. Thus,theapproachworkswithout
Traditionally, methods for producing text in a re-trainingorfine-tuningthemodelitself.
specificstylefallunderthedomainofstylizedre-
Starting with an empty prompt, i.e., beginning
sponse generation (Sun et al., 2022; Yang et al., of sentence token <bos>, the vector z ∈ Rd
steer
2020;Gaoetal.,2019;Jinetal.,2020). Nonethe-
isaddedtotheactivationsofadefinedlayerofthe
less, as common approaches of this class ne-
model,wheredisthedimensionofthelayertogen-
eratethenextoftheT tokensofx. Theobjectiveis
1Find all resources at https://github.com/DLR-SC/
style-vectors-for-steering-llms tofindasteeringvectorˆz steer thatmaximizesthelogprobability: Activation- vs. Training-based
T Target Sentence empty <BOS> token
(cid:88)
ˆz = argmax log p(x |x ,z ) (1)
steer t <t steer LLM Trainable LLM
zsteer
t=1
Layer 0 Steering Layer 0
It was demonstrated on a subset of sentences of ... ...
Vector
theYelpSentimentdataset(Shenetal.,2017)that Layer i for Layer i, Layer i
steeringvectorscanbeusedforshiftingthestyleof iteration j +
... ...
asentencextowardsadedicatedtargetstyleusing
Loss j
thevectorarithmetic: Layer N-1 Layer N-1
-
ˆz = z +λz (2) Activation Vector
target source ∆
Target Sentence
z isthesteeringvectorthatproducessentence for Layer i j ≤ 400
source
x source. z ∆ = ¯z target −¯z source is the difference Figure2: Extractionofanactivationvector(left): The
betweentheaverageofallsteeringvectorslearned LLMs’valuesatlayeriforapromptinthetargetstyle
for sentences from the target and source domain. aresavedforlatercomputationofstylevectors. Trained
Thesteeringvectorˆz canthenbeusedtosteer steeringvectors(right): Thevaluesofthevectorsare
target
themodeltogenerateasentencex′ thatissimilar optimized over j = 400 epochs such that the model
producesaspecifiedsentenceinthetargetstylefroma
toxbutinthetargetstyle.
simplebeginningofasentence(BOS)token.
Moreover,layeractivationshavedemonstrated
utility in steering LLMs. Turner et al. (2023) ex-
emplify that steering vectors, derived from con- strengthofthestylevectoronthemodel’soutput
trastingactivationsforsemanticallyopposedinputs and,thus,allowsformorenuancedandcontrollable
like“love”and“hate”canguideLLMoutputsdur- modelsteeringcomparedtopromptengineering.
ing sentence completion. The difference in acti- Inthisstudy,wecomparetwomainapproaches
vations from such contrasting prompts at layer i tocalculatestylevectors,namelyTraining-based
canstraightforwardlybeaddedtoanotherinput’s StyleVectors(Sec.3.1)andActivation-basedStyle
activationstosteeroutputs. Vectors(Sec.3.2). Training-basedstylevectorsare
In this work, we add to this line of research found from the generative steering vectors (Sub-
a method that efficiently steers LLM outputs to- ramanietal.,2022). Incontrasttothisgenerative
wardsdesiredstyleswithnotablecontrolandtrans- approach,activation-basedstylevectorsarefound
parency. Incontrasttotheaforementionedsteering byaggregatinglayeractivationsforinputsentences
vectorandTSTtechniques,itrequiresnoadditional fromthetargetstyle(Turneretal.,2023). Theba-
optimization or prior knowledge about original sicassumptionbehindthisisthatLLMsinternally
styles. Unlike prompt engineering, our approach adapttothestyleoftheinputpromptwhenproduc-
offersquantifiableadjustmentsinstyle,providing ingoutput,andthus,stylevectorscanbederived
nuanceddifferencesinresponseswithoutrelying fromitshiddenstates. Thesetwomethodsarecon-
onvagueintensityindicators inprompts, such as trasted in Fig. 2 and introduced in more detail in
“extremelynegative”versus“negative.” thissection.
3 Methodology 3.1 Training-basedStyleVectors
WeaimtomodifytheLLMactivationsforaninput In the approach of Subramani et al. (2022) (see
xtogenerateanoutputthatissteeredtowardsaspe- Sec.2),anindividualsteeringvectorislearnedfor
cificstylecategorys ∈ S. AsshowninEq.3,this each target sentence. Thus, shifting the source
isachievedbyfindingstylevectorsv(i)
associated
style of an unsteered model output x towards a
s
tossuchthatwhenaddedtotheactivationsa(i)(x) modified output x′ (generated by steering vector
atlayeritheoutputbecomessteeredtowardss. ˆz x′) in the desired target style requires to com-
pute a steering vector z that leads the uncondi-
ˆa(i)(x) = a(i)(x)+λv(i) (3) x
s tionedmodeltoproducex(Eq.2). This,however,
Style categories can be, for example, positive leadstohighcomputationalcostsandisimpracti-
andnegativeforsentimentstylesordifferentemo- calforonlineadaptationofanLLMpromptedwith
tion classes such as joy and anger. The weight- arbitrary inputs. Furthermore, this vector arith-
ing parameter λ (Eq. 3) determines the influence meticonlyworksforstyleshiftswhenthesourcestyle is known. Many styles, such as emotions, Themean-aggregatedactivationsoflayeriforall
havemultiplecategories. Fornstyleclasses,one sentences from style category s ∈ S is denoted
wouldneedtobuildn×(n−1)contrastingvectors as ¯a(i) . Analogous to the procedure of Sec. 3.1,
s
¯z target −¯z source. Consequently, style-shifting is activation-basedstylevectorsforstylecategorys
limitedanddoesnotgeneralizetomorecomplex arecalculatedas:
styleconcepts.
v(i) =¯a(i)−¯a(i) (5)
Our adaptation In contrast to the approach of s s S\s
Subramani et al. (2022), we do not shift output
The advantage of this approach is that style vec-
stylesonsentencelevelfromsourcetotarget. In-
torsaresolelybasedonaggregatedactivationsof
stead, the steering vectors z learned to steer the
x chosenlayersthatarerecordedduringtheforward
modeltogenerateasamplexfromstylecategorys
passofasentenceofclasss,andnocostlytraining
(i)
aremean-aggregatedintoavector¯z s andallother ofsteeringvectorsisrequired.
steeringvectorsaremean-aggregatedintoavector
¯z(i) . Style vectors v(i) for different layers i can 4 Experiments
S\s s
thenbecalculatedasinEq.4.
We compare both introduced approaches, i.e.,
training-based style vectors (Sec. 3.1) and
v(i) =¯z(i)−¯z(i) (4)
s s S\s activation-based style vectors (Sec. 3.2) in terms
Using the average steering vector ¯z as an of how well they encode information about style
S\s
offsethastheadvantagethatnoknowledgeabout (Sec.4.3)andtheabilitytosteerthemodel’soutput
the source style is required to steer the produced (Sec.4.4).
outputtowardsatargetstyle.
4.1 DatasetsforStyleDefinitions
Thetrainingofanindividualsteeringvectorz
x
is presented in the right part of Fig. 2. The pro- Experiments are performed along different style
cess begins with the frozen model receiving an categories: sentiment,emotion,andwritingstyle
emptyinputtokenandasteeringvectorinitialized (modernvs. Shakespearean). Eachstylecategory
randomly to initiate sentence generation. The re- is defined through datasets with labeled samples.
sultingoutputisthenevaluatedagainstthetarget All datasets used contain English text only. For
sentencetocalculateacross-entropyloss,whichis thetraining-basedstylevectors,wefilteroutsam-
back-propagatedtolearnthesteeringvector. The plescontainingmorethan50charactersfromeach
training for an output x terminates when a steer- dataset to keep the time for computing steering
ing vector z that produces the target sentence x vectors feasible. For details, see Sec. 4.2. This
x
is found or after a maximum number of j = 400 limitation does not apply to the activation-based
epochs. WeusetheAdamoptimizer(Kingmaand stylevectors.
Ba,2014)withalearningrateof0.01. Forourexperiments,weusethefollowingpopu-
lardatasets:
3.2 Activation-basedStyleVectors
Yelp Review Dataset The dataset (Shen et al.,
Analternativetorelyingontrainedsteeringvectors
2017) contains unpaired data about restaurant re-
istoworksolelyinthespaceoflayeractivations
views on the Yelp platform labeled as positive or
whenthemodelispromptedwithsamplesfroma
negative. After dropping duplicates, the dataset
stylecategorysassuggestedbyTurneretal.(2023)
contains542ksamples.
(seeleft-handsideofFig.2). However,theeffect
ofthisapproachonthemodeloutputhasonlybeen GoEmotions Asamulti-classstyledataset,the
showntobeabletosteertheoutputofanLLMfor GoEmotionsdataset(Demszkyetal.,2020)com-
pairs of natural-language prompts by contrasting prises58k manuallycuratedusercommentsfrom
the activations of those (e.g., “love” and “hate”). theinternetplatformReddit2 labeledwith27emo-
Inthiswork,wetakeupthisideaandextenditto tional categories. We use 5k samples that can be
calculating general style vectors associated with unambiguouslymappedtotheestablishedsixbasic
stylecategoriesinsteadofsinglepairs. emotion categories (Ekman, 1992): sadness, joy,
fear,anger,surprise,anddisgust.
Our adaptation The vector of activations of
layer i of an LLM for input x is given as a(i)(x). 2Redditforum:https://www.reddit.com/Shakespeare TheShakespearedataset(Jhamtani these layers for the generation of prompts in the
etal.,2017)containspairedshorttextsamplesof nextsection.
Shakespeareantextsandtheirmoderntranslations.
4.4 EvaluationofGeneratedTexts
Weusethetrainingsetcontaining18,395sentences
foreachstyle: modernandShakespearean. As shown in Sec. 4.3, both trained steering and
activation vectors capture relevant style informa-
4.2 ExperimentalSetup
tion. However,thisdoesnotshowthatstylevectors
The aim is to investigate the ability to influence v(i) thatarecomputedfromthemcanbeusedto
s
thestyleofanLLMinasettingwhereananswer actuallysteerthestyleofthemodel’soutput. For
to a question or instruction prompt is expected. this reason, we assembled a list of 99 exemplary
Our experiments utilize the open-source Alpaca- promptsasinputfortheAlpaca-7Bmodel. Since
7B(Taorietal.,2023)ChatGPTalternative,which the style of an LLM’s output cannot be consid-
is based on Meta’s LLaMA-7B (Touvron et al., eredindependentlyofthetypeofinputprompt,we
2023)architecture. Choosingthismodelresulted createdtwodifferentsetsofprompts: Thefactual
in d = 4096-dimensional style vectors for each list comprises 50 prompts that ask about a hard
ofits33layers. WeusedasingleNVIDIAA100- fact with a clear, correct answer, such as ”Who
SXM4-80GBforourexperiments. painted the Mona Lisa?“. The subjective list in-
For the evaluation of the training-based style cludes 49 different prompts, allowing more indi-
vectors,weonlyincorporatesteeringvectorsthat vidual responses to express sentiments and emo-
reproduce the target sentence with loss < 5, as tions. Theyeitherinquireaboutapersonalopinion,
vectors with higher loss tend to yield grammati- e.g., ”What do German bread rolls taste like?“,
cally incorrect output sentences. This resulted in or general information and allow for a variety of
470 vectors per layer for the Yelp review dataset, responses, for instance, ”Describe a piece of art-
89forGoEmotions, and491fortheShakespeare work.“ Steering towards a sentiment or emotion
dataset. In a pre-study on a smaller subset of the categoryisexpectedtoaffecttheLLM’soutcome
data, we found that the steering vectors for the significantlymoreforsuchsubjectivepromptsthan
layers i ∈ {18,19,20} are most effective, which for factual prompts. The full list of prompts is
issupportedbythefindingsofourprobingstudy giveninSec.A.
(Sec.4.3). Weonlytrainsteeringvectorsforthese As described in Section 3, the parameter λ of
layers to keep the computational effort feasible. Eq.3influenceshowstronglythemodelissteered
Nevertheless,wehadtoruntheexperimentonthe towards the target style. We found that if this pa-
YelpandShakespearedatasetsfor150hourseach rameterischosentoolarge,themodelsometimes
andforGoEmotionsforaround100hours. Incom- produces nonsense texts, as shown in Ex. E2 in
parison,theextractionoftheactivationsonlytook Sec.4.4.2andinAppendixinSec.B.Thiseffect
atmost8hoursperdatasetandresultedinrecorded seems to be dependent on the input prompt and
activationvectorsforalldatasetsamples. styledomain.
4.3 ProbingStudy 4.4.1 Classification-basedEvaluation
Thereceiveroperatingcharacteristic(ROC)curves Weusestandardclassificationmodelstoevaluate
fortwoclasspredictions(positiveandnegativesen- thesteeredoutputoftrainingandactivation-based
timent)intheYelpreviewdatasetarepresentedin stylevectors. Thedashedlinesinallsteeringplots,
Fig. 3. It can be seen that, in general, activations e.g.,inFig.4andFig.5,indicatethemeanclassifi-
fromlayerthreeonwardsleadtoremarkablyhigh cationscoreachievedforapromptingbaseline. In
classificationaccuracy(AUC≥ 0.97,seeFig.3c) theseinstances,nosteeringvectorwasappliedto
andarealmostperfectforlayersi ∈ {18,19,20}. themodel. Instead,weappended“Writetheanswer
As expected, activations encode style more ex- ina[...] manner.” totheinputprompt,wherethe
plicitly than trained steering vectors, which still dotsarereplacedwiththerespectivetargetsteering
achieveconsiderableaccuracy. Theresultsaresim- style, e.g., positive, or angry. Thus, the model is
ilarfortheothertwodatasets,discussedinSec.C. informed in a neutral way to direct the output as
Wecan,therefore,determinethatthelayersi ∈ required.
{18,19,20} are candidates for effective steering, For the Yelp dataset-based style vectors, the
andweonlyusestylevectorsv(i) computedfrom positivity and negativity values of produced out-
s1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4 Layer 0 / AUC=0.60
Layer 1 / AUC=0.95
Layer 3 / AUC=0.98
0.2 0.2 0.2 Layer 5 / AUC=0.98
Layer 18 / AUC=0.91 Layer 18 / AUC=0.99 Layer 10 / AUC=0.98
Layer 19 / AUC=0.91 Layer 19 / AUC=0.99 Layer 20 / AUC=0.98
0.0 Layer 20 / AUC=0.90 0.0 Layer 20 / AUC=0.99 0.0 Layer 30 / AUC=0.97
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate False Positive Rate
(a)Trainedsteeringvectors (b)Correspondingactivationvectors (c)Activationvectorsof10ksentences
Figure3: ClassificationresultsontheYelpreviewdataset: Using(a)onlythe470trainedsteeringvectors,(b)the
correspondingactivationvectors, and(c)selectedlayersofactivationvectorsof10ksentences. Theactivation
vectorsshowsuperiorperformanceintheirabilitytopredictthesentimentofaninputsentence.
putswereinferredbytheVADERsentimentana- 4.4.2 SteeringOutputExamples
lyzer(HuttoandGilbert,2014)asastate-of-the-art Inthefollowing,selectedexamplesofmodelout-
model. Fig.4showstheaveragesentimentclassi- putsmodifiedbystylevectorsarepresentedtopro-
ficationscoresonthemodel’ssteeredoutputsfor vide an impression of the obtained results under
different values of λ and the 49 subjective input different settings. In each case, P is the prompt,
prompts. Itappearsthatsteeringintothepositive followedbydifferentmodeloutputs.
directionworksbetteringeneral,whilethesteering
E1-Oppositesentimentdirections. Examples
effectisstrongerforactivation-basedstylevectors.
of how the output changes when shifted in oppo-
As one could expect, for the 50 factual prompts,
sitedirectionsusingactivation-basedstylevectors
there are no notable differences since the factual
derivedfromtheYelpdatasetatλ = 1. Thetarget
answersaremostlyneutral. Thus, corresponding
styleispositiveforO1andnegativeforO2.
plotsareomitted. Thepromptbaseline,onaverage,
demonstrates only a minimal effect compared to
P: Shareapersonalanecdoteaboutavacation
themodel’sdefaultoutput.
youhad
O1: MyfavoritevacationwasatripItooktothe
beachwithmyfamily. Wehadagreattimeex-
In order to evaluate the ability of the style ploringthebeach,goingonwalks,andtasting
vectors derived from the GoEmotions dataset to allthedeliciousfood. Thebestpartwasthe
steer the model ’s output towards multiple emo- relaxingatmosphereandthegreatmemories
tional categories, a multi-class model based on wemade.
RoBERTa(Hartmann,2022)wasused. Fig.5de-
O2: IhadaterribleexperiencewithavacationI
pictstheshiftintheclassifiedemotionclassscore
had. Iwenttoabeachinadifferentcountry
whenthestylespaceisbuiltfromactivation-based
andIwasexpectingtohaveagreattime. In-
style vectors on the GoEmotions dataset. The
stead,Iwasmetwithahugeamountofracism.
notable increase in the respective emotion class
PeoplewereopenlysayingthatIwasa“strike-
scoreindicatesastrongmodelperformancewhen
o-bog”(atermthatisusedtorefertoablack
steeringtowardsaspecificemotion. Asexpected,
person) and that I was “not allowed” to be
stylingtheoutputonfactualpromptsshowsalmost
there. Iwasthenphysicallyattacked. [...]
no changes in the emotion class scores, similar
to the results for the Yelp style vectors described E2 - Too large λs. The parameter λ in Eq. 3
above. PromptbaselinescreatedwiththeGoEmo plays a central role for the outputs. Yet, there is
labels exhibit a substantially stronger impact on no general threshold for λ, but it depends on the
the generated outcomes compared to the prompt input prompt and the target style definition. Too
baselineinthecaseoftheYelpdataset. largeλvaluessometimesresultinnonsenseoutputs
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurT1.0 1.0 1.0
ppoossiittiivvee ((pprroommppttiinngg)) ppoossiittiivvee ((pprroommppttiinngg)) PPoossiittiivvee ((pprroommppttiinngg))
nneeggaattiivvee ((pprroommppttiinngg)) nneeggaattiivvee ((pprroommppttiinngg)) NNeeggaattiivvee ((pprroommppttiinngg))
0.8 ppoossiittiivvee 0.8 ppoossiittiivvee 0.8 PPoossiittiivvee nneeggaattiivvee nneeggaattiivvee NNeeggaattiivvee
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 1.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2
1.0 1.0 1.0
ppoossiittiivvee ((pprroommppttiinngg)) ppoossiittiivvee ((pprroommppttiinngg)) PPoossiittiivvee ((pprroommppttiinngg))
nneeggaattiivvee ((pprroommppttiinngg)) nneeggaattiivvee ((pprroommppttiinngg)) NNeeggaattiivvee ((pprroommppttiinngg))
0.8 ppoossiittiivvee 0.8 ppoossiittiivvee 0.8 PPoossiittiivvee nneeggaattiivvee nneeggaattiivvee NNeeggaattiivvee
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 1.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2
(a)Stylevectorsfromtrainedsteering (b)Stylevectorsfromthecorresponding (c)Stylevectorsfromallactivation
vectors activationvectors vectors
Figure4: SteeringoftheYelpReviewsamplestowardspositive(upperplots)andnegative(lowerplots)sentiment.
withrepetitions. Thisbehaviorcouldbeobserved P: How would you respond to a rude customer
especiallyfortraining-basedstylevectors. Rwas inacustomerservicerole?
theresultofsteeringtowardssadnesswithλ = 1.
J: I apologize for the inconvenience. I under-
P: Describeafictionalcharacter. standyourfrustrationandthankyouforbring-
ing this to my attention. I’d like to help you
R: Ms. Grey is an elderly woman with a sadly
resolvetheissueasquicklyaspossible. [...]
sadlysadlysadlysadly[...].
A: Iunderstandyouarefrustrated,butIcannot
E3-TargetBias. Inreality,datasetsdonotonly
allowyoutospeaktomeinthismanner. Iam
encodedesiredtargetstylesbutalsootherdomain-
heretohelpyou,butyoumusttreatmewith
specific characteristics. Style vectors from Yelp
respect. [...]
restaurant reviews often seem to carry a bias to-
wards food topics in addition to sentiment. R re-
E5-Steeringstylebeyondsentimentandemo-
sultsfromactivation-basedsteeringtowardsposi-
tions. Theseoutputswereshiftedwithactivation-
tivestylewithλ = 1.2.
based style vectors calculated upon the Shake-
P: Whatdidadayinatypicalfamilyintheyear spearedataset. WhileMwassteeredtowardsmod-
1980looklike? ern with λ = 0.8, S was steered towards Shake-
speareanwithλ = 1.6.
R: Adayinatypicalfamilyintheyear1980was Sisformulatedinamorefloweryandantiquated
filledwithgreatfood,funandmemorablemo- language. Presumably,themaximalλforshifting
ments. Themorningstartedwithadelicious towardsmodernissmallerbecausethisstyleisper
breakfastandgreatconversations. Thelunch semoresimilartotheLLM’sstyleand,therefore,
wasalwaysagreathome-mademealsandthe alsoliescloserinthelatentvectorspace.
dinnerwasalwaysagreatvarietyofflavours.
P: Howdoyoudefinehappiness?
E4 - Steering Emotion. Also, the underlying
emotionintheLLM’sresponsecansteer,asdemon- M: Happinessisastateofcontentment,joy,and
strated in this example. J and A were steered satisfaction in life. It is the feeling of being
activation-based towards joy and anger, respec- satisfiedwithwhoyouareandhavingasense
tively,withλ = 0.8. ofpurposeandfulfillmentinlife.
erocs
tnemitneS
erocs
tnemitneS
erocs
tnemitneS
erocs
tnemitneS
erocs
tnemitneS
erocs
tnemitneS1.0 1.0 1.0
aannggeerr ((pprroommppttiinngg)) ddiissgguusstt ((pprroommppttiinngg)) jjooyy ((pprroommppttiinngg))
ssaaddnneessss ssaaddnneessss ssaaddnneessss
jjooyy jjooyy jjooyy
0.8 ffeeaarr 0.8 ffeeaarr 0.8 ffeeaarr
aannggeerr aannggeerr aannggeerr
ssuurrpprriissee ssuurrpprriissee ssuurrpprriissee
ddiissgguusstt ddiissgguusstt ddiissgguusstt
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(a)Steeringtoanger, (b)Steeringtodisgust, (c)Steeringtojoy,
subjectiveprompts subjectiveprompts subjectiveprompts
1.0 1.0 1.0
ffeeaarr ((pprroommppttiinngg)) ssaaddnneessss ((pprroommppttiinngg)) ssuurrpprriissee ((pprroommppttiinngg))
ssaaddnneessss ssaaddnneessss ssaaddnneessss
jjooyy jjooyy jjooyy
0.8 ffeeaarr 0.8 ffeeaarr 0.8 ffeeaarr
aannggeerr aannggeerr aannggeerr
ssuurrpprriissee ssuurrpprriissee ssuurrpprriissee
ddiissgguusstt ddiissgguusstt ddiissgguusstt
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(d)Steeringtofear, (e)Steeringtosadness, (f)Steeringtosurprise,
subjectiveprompts subjectiveprompts subjectiveprompts
Figure5: Activation-basedstylevectors: EvaluationofgeneratedtextsforsubjectivepromptsusingGoEmotions’
stylevectors. Allactivationvectorswereused.
S: Happiness is a state of contentment and joy, inputstyleduringtheforwardpass. Steeringvec-
wherein the soul is freed from the bondage tor training is much more expensive than simply
ofsorrow,mischievousfancies,andunworthy recordingthehiddenlayeractivationduringasin-
thoughts,andwhereinthebodyisfreedfrom gleforwardpass. Therefore,theactivation-based
the bondage of pain, and wherein the soul stylevectorsarethepreferredapproachforsteer-
duly commends itself to the Lord, and is in ing style in largelanguage models, bothin terms
somemeasuremadepartakeroftheblessed- ofperformanceandresourceefficiency.
nesswhichispast,whichispresent,orwhich Wealsofoundthat,forfactualprompts,theout-
tocome. put can only marginally be influenced. It can be
consideredpositivethatonecannoteasilydissuade
5 DiscussionandConclusion the model from answering in a neutral tone to a
factual prompt while still being adaptable if the
This work investigated vector representations as- inputpermits,especiallyinconversationalsettings.
sociatedwithsentiments,emotioncategories,and Stylevectorsenableacontinuousandadjustable
general writing styles that can influence the out- modulationoftheoutputsoflargelanguagemod-
putstyleofLLMs. Inagenerativeapproach,style els. Unlikepromptengineering,whichoffersmore
vectorswerederivedfromsteeringvectorsfound step-wisecontroloverstyleintensities(like“Write
in a training procedure and steered the model to theanswerinapositiveway”versus“Writethean-
producesamplesinadesiredstylefromscratch. In swerinaverypositiveway”),stylevectorsprovide
contrast,activation-basedstylevectorsarederived smoothertransitions. Thisactivation-basedcontrol
fromtheactivationsofinputprompts,whichrelies isachievablebecausethevectorsinactivationen-
ontheassumptionthatLLMsinternallyadaptthe gineeringareconstructedfromknowndatasets. In
erocs
tnemitneS
erocs
tnemitneS
erocs
tnemitneS
erocs
tnemitneS
erocs
tnemitneS
erocs
tnemitneScontrast,traditionalpromptingmaytriggeractiva- arewidelyusedinpracticeandhaveshownstate-
tionsthatareunknownandinaccessibletotheuser, of-the-art results. However, they are not perfect,
limitingtheabilitytofine-tunetheoutput. Further- andthus,resultsonlyshowageneraltendency. In
more, activation-based steering has the potential thefuture,weplantoconductstudiesonindividual
togeneratenewstyles,expandingthepossibilities human perceptions of the text style produced by
beyondtheconstraintsofpre-trainingknowledge steeredLLMs.
inherentinpromptengineering. Whileprompten- The experiments have a strong focus on senti-
gineering relies on existing knowledge and often mentandemotionasstylecharacteristics. Results
involvesatrial-and-errorapproach,activationengi- ontheShakespearedatasetprovideevidencethat
neeringopensupnewavenuesforstylegeneration theoutputofLLMscanalsogenerallybesteered
andcustomization. Morecomplexstyles,suchas towardstoneandwritingstyle. This,however,has
multidimensionalcomposedstyles,presentunique tobeinvestigatedinmoredepthinthefuture, es-
challengeswhenapproachedthroughactivationen- peciallyconcerningtextsinlanguagesotherthan
gineering. However,theadvantagesitoffers,such English.
asenhancedcontrolovertheoutputandthecapac-
itytodevelopuniquestyles,significantlyoutweigh
EthicsStatement
theseinitialchallenges. Itisimportanttonotethat
thesemethodsarenotmutuallyexclusive;theycan
Ourmethodmaygeneratenegative,rude,andhate-
becombinedtoleverageeachapproach’sstrengths,
fulsentencesaboutaspecificpersonoracommer-
enhancingourmodel’soverallcapabilityandflexi-
cial site caused by the data distribution of Yelp
bility.
and GoEmotions datasets. Therefore, it could be
Tothebestofourknowledge,thisisoneofthe
usedwithmaliciousintentions,i.e.,bytargetedha-
first studies on steering language models beyond
rassmentorinflationofpositivereviews. Sinceour
GPT-2(inourcaseAlpaca-7B(Taorietal.,2023)).
workinvolvesapre-trainedgenerativeLLM,which
Results should, however, be transferable to any
was trained on text scraped from the web, it has
other type of LLM with direct access to hidden
acquiredsomebiasesthatwerepresentthere. Such
layeractivations. Howtodeterminetheexactinflu-
biasesmightbeextractedbycertainpromptsand
enceoftheweightingparameterλ(Eq.3)isstillan
could even be strengthened by our style steering.
openquestion. λallowsfornuancedstylesteering
Furthermore, it is important to note that steering
but,ifchosentoolarge,leadsthemodeltoproduce
thestyleofLLMsmaybearthepotentialtomimic
nonsensetexts. Moreover,thisseemstodependon
aspecificstyleofspeechfrompersonswhosestate-
thedomain(sentiment,emotion,writingstyle). We
mentswereusedtotrainthemodel. Therefore,the
leavethisforfutureresearch.
approachescouldbeabusedtocreaterealisticfake
statements.
Limitations
In the context of image generation, the idea
of shifting entities in the latent space during the
It was not feasible to derive trained steering vec-
generationprocesshasalreadybeenimplemented
tors for all considered samples since training in-
successfully (Brack et al., 2022) and can consid-
volves high computational costs and requires a
erably reduce harmful content in generated im-
maximal sample length of 50 characters. In con-
ages(Schramowskietal.,2023). Analogously,our
trast,activation-basedstylevectorscouldstraight-
approachcanalsobeusedtoreduceharmfulout-
forwardlybeobtainedforeverytextsamplewithout
put.
restrictions. Weconductedactivation-basedexper-
imentsonthecompletesamplesettoexplorethe
proposedapproachfully. However,toavoidapo- Acknowledgements
tentialbiastowardsactivation-basedstylevectors
andprovideafaircomparison,wealsoconducted Theauthorsgratefullyacknowledgethecomputa-
ourexperimentsonthesubsetofsamplesthatcould tionalanddataresourcesprovidedthroughthejoint
beconsideredforbothsettings. high-performance data analytics (HPDA) project
Weevaluatedtheabilitytoinfluencethestyleof “terrabyte”oftheGermanAerospaceCenter(DLR)
anLLM’soutputwithstylevectorsusingexisting andtheLeibnizSupercomputingCenter(LRZ).
sentimentandemotionclassifiers. BothclassifiersReferences Evan Hernandez, Belinda Z Li, and Jacob Andreas.
2023. Measuringandmanipulatingknowledgerep-
MarcoAlessio,GuglielmoFaggioli,andNicolaFerro. resentations in language models. arXiv preprint
2023. Decaf: a modular and extensible conversa- arXiv:2304.00740.
tionalsearchframework. InSIGIR’23: Proceedings
ofthe46thInternationalACMSIGIRConferenceon Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
ResearchandDevelopmentinInformationRetrieval Bruna Morrone, Quentin De Laroussilhe, Andrea
(Taipei,Taiwan).AssociationforComputingMachin- Gesmundo,MonaAttariyan,andSylvainGelly.2019.
ery,toappear. Parameter-efficienttransferlearningfornlp. InIn-
ternationalConferenceonMachineLearning,pages
MostafaM.Amin,ErikCambria,andBjörnW.Schuller. 2790–2799.PMLR.
2023. Willaffectivecomputingemergefromfoun-
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
dationmodelsandgeneralartificialintelligence? a
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
firstevaluationofchatgpt. IEEEIntelligentSystems,
and Weizhu Chen. 2021. Lora: Low-rank adap-
38(2):15–23.
tation of large language models. arXiv preprint
arXiv:2106.09685.
EmilyMBender, TimnitGebru, AngelinaMcMillan-
Major, and Shmargaret Shmitchell. 2021. On the
C.HuttoandEricGilbert.2014. Vader:Aparsimonious
dangersofstochasticparrots: Canlanguagemodels
rule-based model for sentiment analysis of social
betoobig? InProceedingsofthe2021ACMconfer-
mediatext. ProceedingsoftheInternationalAAAI
enceonfairness,accountability,andtransparency,
ConferenceonWebandSocialMedia,8(1):216–225.
pages610–623.
HarshJhamtani,VarunGangal,EduardHovy,andEric
ManuelBrack, PatrickSchramowski, FelixFriedrich, Nyberg. 2017. Shakespearizing modern language
Dominik Hintersdorf, and Kristian Kersting. 2022. usingcopy-enrichedsequencetosequencemodels.
Thestableartist: Steeringsemanticsindiffusionla- InProceedingsoftheWorkshoponStylisticVariation,
tentspace. arXivpreprintarXiv:2212.06013. pages10–19,Copenhagen,Denmark.Associationfor
ComputationalLinguistics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova,
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
and Rada Mihalcea. 2022. Deep learning for text
Neelakantan,PranavShyam,GirishSastry,Amanda
styletransfer: Asurvey. ComputationalLinguistics,
Askell,etal.2020. Languagemodelsarefew-shot
48(1):155–205.
learners. Advancesinneuralinformationprocessing
systems,33:1877–1901.
Di Jin, Zhijing Jin, Joey Tianyi Zhou, Lisa Orii, and
PeterSzolovits.2020. Hooksintheheadline: Learn-
John Joon Young Chung, Wooseok Kim, Kang Min
ingtogenerateheadlineswithcontrolledstyles. In
Yoo,HwaranLee,EytanAdar,andMinsukChang.
Proceedingsofthe58thAnnualMeetingoftheAsso-
2022. Talebrush:visualsketchingofstorygeneration
ciationforComputationalLinguistics,pages5082–
withpretrainedlanguagemodels. InCHIConference
5093.
onHumanFactorsinComputingSystemsExtended
Abstracts,pages1–4. NitishShirishKeskar,BryanMcCann,LavRVarshney,
CaimingXiong,andRichardSocher.2019. Ctrl: A
DorottyaDemszky,DanaMovshovitz-Attias,Jeongwoo conditionaltransformerlanguagemodelforcontrol-
Ko,AlanCowen,GauravNemade,andSujithRavi. lablegeneration. arXivpreprintarXiv:1909.05858.
2020. GoEmotions: ADatasetofFine-GrainedEmo-
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
tions. In58thAnnualMeetingoftheAssociationfor
methodforstochasticoptimization. arXivpreprint
ComputationalLinguistics(ACL).
arXiv:1412.6980.
PaulEkman.1992. Aretherebasicemotions? Psycho-
BrianLester,RamiAl-Rfou,andNoahConstant.2021.
logicalReview.
The power of scale for parameter-efficient prompt
tuning. InProceedingsofthe2021Conferenceon
XiangGao,YizheZhang,SungjinLee,MichelGalley,
EmpiricalMethodsinNaturalLanguageProcessing,
ChrisBrockett,JianfengGao,andBillDolan.2019.
pages3045–3059,OnlineandPuntaCana,Domini-
Structuringlatentspacesforstylizedresponsegen-
can Republic. Association for Computational Lin-
eration. InProceedingsofthe2019Conferenceon
guistics.
EmpiricalMethodsinNaturalLanguageProcessing
andthe9thInternationalJointConferenceonNatu- Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
ralLanguageProcessing(EMNLP-IJCNLP),pages Optimizing continuous prompts for generation. In
1814–1823,HongKong,China.AssociationforCom- Proceedingsofthe59thAnnualMeetingoftheAsso-
putationalLinguistics. ciationforComputationalLinguisticsandthe11th
InternationalJointConferenceonNaturalLanguage
JochenHartmann.2022. Emotionenglishdistilroberta- Processing (Volume 1: Long Papers), pages 4582–
base. https://huggingface.co/j-hartmann/ 4597, Online. Association for Computational Lin-
emotion-english-distilroberta-base/. guistics.OpenAI.2023. Gpt-4technicalreport. arXivpreprint AmericanChapteroftheAssociationforComputa-
arXiv:2303.08774. tionalLinguistics: HumanLanguageTechnologies,
pages3304–3318,Seattle,UnitedStates.Association
AlecRadford,JeffreyWu,RewonChild,DavidLuan, forComputationalLinguistics.
DarioAmodei,IlyaSutskever,etal.2019. Language
modelsareunsupervisedmultitasklearners. OpenAI Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
blog,1(8):9. Dubois,XuechenLi,CarlosGuestrin,PercyLiang,
andTatsunoriB.Hashimoto.2023. Stanfordalpaca:
EmilyReif,DaphneIppolito,AnnYuan,AndyCoenen, An instruction-following llama model. https://
ChrisCallison-Burch,andJasonWei.2022. Arecipe github.com/tatsu-lab/stanford_alpaca.
forarbitrarytextstyletransferwithlargelanguage
models. In Proceedings of the 60th Annual Meet- HugoTouvron,ThibautLavril,GautierIzacard,Xavier
ingoftheAssociationforComputationalLinguistics Martinet,Marie-AnneLachaux,TimothéeLacroix,
(Volume 2: Short Papers), pages 837–848, Dublin, Baptiste Rozière, Naman Goyal, Eric Hambro,
Ireland.AssociationforComputationalLinguistics. Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
PatrickSchramowski,ManuelBrack,BjörnDeiseroth,
arXiv:2302.13971.
and Kristian Kersting. 2023. Safe latent diffusion:
Mitigating inappropriate degeneration in diffusion
AlexTurner,LisaThiergart,DavidUdell,GavinLeech,
models. InProceedingsoftheIEEE/CVFConference
Ulisse Mini, and Monte MacDiarmid. 2023. Acti-
onComputerVisionandPatternRecognition,pages
vationaddition: Steeringlanguagemodelswithout
22522–22531.
optimization. arXivpreprintarXiv:2308.10248.
PatrickSchramowski,CigdemTuran,NicoAndersen,
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
ConstantinARothkopf,andKristianKersting.2022.
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Largepre-trainedlanguagemodelscontainhuman-
Kaiser,andIlliaPolosukhin.2017. Attentionisall
likebiasesofwhatisrightandwrongtodo. Nature
youneed. Advancesinneuralinformationprocessing
MachineIntelligence,4(3):258–268.
systems,30.
ChiragShah,RyenWhite,PaulThomas,BhaskarMitra,
JonasWagnerandSinaZarrieß.2022. Dogenderneu-
Shawon Sarkar, and Nicholas Belkin. 2023. Tak-
tralaffixesnaturallyreducegenderbiasinstaticword
ingsearchtotask. InProceedingsofthe2023Con-
embeddings? InProceedingsofthe18thConference
ferenceonHumanInformationInteractionandRe-
onNaturalLanguageProcessing(KONVENS2022),
trieval,pages1–13.
pages88–97.
TianxiaoShen,TaoLei,ReginaBarzilay,andTommi
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Jaakkola.2017. Styletransferfromnon-paralleltext
Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,
bycross-alignment. AdvancesinNeuralInformation
etal.2022. Chain-of-thoughtpromptingelicitsrea-
ProcessingSystems,30.
soninginlargelanguagemodels. AdvancesinNeural
TaylorShin,YasamanRazeghi,RobertL.LoganIV,Eric InformationProcessingSystems,35:24824–24837.
Wallace,andSameerSingh.2020. AutoPrompt:Elic-
Tongshuang Wu, Michael Terry, and Carrie Jun Cai.
itingKnowledgefromLanguageModelswithAuto-
2022. Ai chains: Transparent and controllable
maticallyGeneratedPrompts. InProceedingsofthe
human-ai interaction by chaining large language
2020ConferenceonEmpiricalMethodsinNatural
model prompts. In Proceedings of the 2022 CHI
LanguageProcessing(EMNLP),pages4222–4235,
Conference on Human Factors in Computing Sys-
Online.AssociationforComputationalLinguistics.
tems,pages1–22.
CharlieSnell,SherryYang,JustinFu,YiSu,andSergey
Levine.2022. Context-awarelanguagemodelingfor ZeYang,WeiWu,CanXu,XinnianLiang,JiaqiBai,
goal-orienteddialoguesystems. InFindingsofthe Liran Wang, Wei Wang, and Zhoujun Li. 2020.
AssociationforComputationalLinguistics: NAACL StyleDGPT:Stylizedresponsegenerationwithpre-
2022,pages2351–2366,Seattle,UnitedStates.Asso- trainedlanguagemodels. InFindingsoftheAssoci-
ciationforComputationalLinguistics. ationforComputationalLinguistics: EMNLP2020,
pages1548–1559,Online.AssociationforComputa-
Nishant Subramani, Nivedita Suresh, and Matthew E tionalLinguistics.
Peters.2022. Extractinglatentsteeringvectorsfrom
pretrainedlanguagemodels. InFindingsoftheAs- AnnYuan,AndyCoenen,EmilyReif,andDaphneIp-
sociationforComputationalLinguistics: ACL2022, polito. 2022. Wordcraft: story writing with large
pages566–581. languagemodels. In27thInternationalConference
onIntelligentUserInterfaces,pages841–852.
QingfengSun,CanXu,HuangHu,YujingWang,Jian
Miao,XiuboGeng,YiningChen,FeiXu,andDaxin HanqingZhang,HaolinSong,ShaoyuLi,MingZhou,
Jiang.2022. Stylizedknowledge-groundeddialogue and Dawei Song. 2022. A survey of controllable
generation via disentangled template rewriting. In textgenerationusingtransformer-basedpre-trained
Proceedings of the 2022 Conference of the North languagemodels. ACMComputingSurveys.GuoyingZhao,YanteLi,andQianruXu.2022. From
emotionaitocognitiveai. InternationalJournalof
NetworkDynamicsandIntelligence,pages65–72.
WeixiangZhao,YanyanZhao,XinLu,ShilongWang,
Yanpeng Tong, and Bing Qin. 2023. Is chat-
gptequippedwithemotionaldialoguecapabilities?
arXivpreprintarXiv:2304.09582.Appendix [F22]Whatistheformulaforcalculatingthevol-
umeofacylinder?
A EvaluationPrompts
[F23]WhatistheformulaforconvertingCelsius
In this investigation, we compared the system’s toFahrenheit?
performanceonfactualandsubjectiveonprompts.
[F24] What is the freezing point of water in
Comprehensivelistsofthesepromptsareprovided
Kelvin?
inSec.A.1andSec.A.2,respectively.
[F25]Whatisthelargestcountryintheworldby
A.1 FactualPrompts
landarea?
Therewere50factualpromptsusedinthisstudy,
[F26] What is the largest internal organ in the
whicharereferredtoasF01toF50:
humanbody?
[F01] How many bones are there in the human [F27]Whatisthelargestoceanintheworld?
body?
[F28] What is the largest organ in the human
[F02]Howmanychambersarethereinthehuman body?
heart?
[F29]Whatisthespeedoflightinavacuum?
[F03] How many elements are there in the peri-
[F30] What is the symbol for the chemical ele-
odictable?
mentiron?
[F04] How many planets are there in our solar
[F31]Whatisthetallestbuildingintheworld?
system?
[F32]Whatisthetallestmountainintheworld?
[F05] How many players arethere in abaseball
team? [F33]Whatistheworld’slongestriver?
[F06]Howmanyplayersarethereinavolleyball [F34]WhichcountryisfamousfortheTajMahal?
team?
[F35]WhichcountryisknownastheLandofthe
[F07] How many symphonies did Ludwig van RisingSun?
Beethovencompose?
[F36]Whichgasisknownaslaughinggas?
[F08]InwhichyeardidWorldWarIIend?
[F37]WhichgasmakesupthemajorityofEarth’s
[F09]InwhichyeardidtheBerlinWallfall? atmosphere?
[F10] In which year did the first moon landing [F38]Whodevelopedthetheoryofevolutionby
occur? naturalselection?
[F11]WhatistheboilingpointofwaterinFahren- [F39]Whodiscoveredpenicillin?
heit?
[F40]Whodiscoveredthetheoryofgeneralrela-
[F12]WhatisthecapitalcityofFrance? tivity?
[F13]Whatisthechemicalformulaformethane? [F41] Who is considered the father of modern
[F14]Whatisthechemicalformulafortablesalt? physics?
[F15]Whatisthechemicalformulaforwater? [F42] Who is credited with inventing the tele-
phone?
[F16]Whatisthechemicalsymbolforgold?
[F43]Whoistheauthoroftheplay“Romeoand
[F17]Whatisthechemicalsymbolforsodium?
Juliet”?
[F18] What is the deepest point in the Earth’s
[F44]WhoisthecurrentPresidentoftheUnited
oceans?
States?
[F19]Whatistheformulaforcalculatingdensity?
[F45]Whopainted“TheStarryNight”?
[F20]Whatistheformulaforcalculatingthearea
[F46]Whopaintedthe“LastSupper”?
ofacircle?
[F47]WhopaintedtheMonaLisa?
[F21]Whatistheformulaforcalculatingthearea
ofatriangle? [F48]Whowrotethenovel“PrideandPrejudice”?[F49]Whowrotethenovel“ToKillaMocking- [S24]Howdoyoustaymotivatedandfocusedon
bird”? long-termgoals?
[F50]Whowrotetheplay“Hamlet”? [S25]Howwouldyouhandleadisagreementwith
aclosefriend?
A.2 SubjectivePrompts
[S26]Howwouldyourespondtoarudecustomer
The 49 applied factual prompts are referred to as
inacustomerservicerole?
S01toS49:
[S27] If a roommate consistently borrows your
[S01]Announcetheweatherforecastfortheup- belongings without asking, how would you
comingweekend. handleit?
[S02] Ask your hairdresser for an appointment [S28]Orderavegandishfromthemenuofasteak
nextweektohaveyourhairdyed. house.
[S03]Commentonacriticalreviewofacustomer [S29]ReviewthepairofheadphonesthatIbought
ofyourbusiness. onlinelastweek.
[S04]Comparethecolorblueandgreen. [S30]Shareapersonalanecdoteaboutavacation
youhad.
[S05]Comparetheculturalvalueoftheatersand
cinemas. [S31]Shareapersonalexperienceofovercoming
afearorfacingachallenge.
[S06]Comparethequalitiesofcoffeeandtea.
[S32]Shareapersonalexperiencethatchallenged
[S07]Comparetherelaxationbasedonvacation
yourbeliefsandchangedyourperspective.
andcontinuoussport.
[S33] Share a quote or mantra that inspires you
[S08]Comparethetasteofastrawberrysmoothie
andexplainwhyitresonateswithyou.
tothatofavanillaone.
[S34]Tellmeajoke.
[S09]Composeafewlinesoflyricstalkingabout
society. [S35] Tell me about the concert in America last
year.
[S10]Describeafictionalcharacter.
[S36]Whatapproachwouldyoutakewhenfaced
[S11]Describeamealordishthatholdssentimen-
withachallengingworkcolleague?
talvaluetoyouandwhy.
[S37]Whatapproachesdoyoutaketofostercre-
[S12]Describeapersonwhohashadanimpact
ativityandinnovationinyourwork?
onyourlifeandwhy.
[S38]Whatarethestepstobakeaperfectpizza
[S13]Describeapieceofartwork.
fromscratch?
[S14]Describeanincidentthatcouldleadtoan
[S39] What did a day in a typical family in the
airplanecrashinmid-flight.
year1980looklike?
[S15]Discusstheimpactofsocialmediaoninter-
[S40]WhatdoGermanbreadrollstastelike?
personalrelationships.
[S41]Whatdoesafulfillingandmeaningfullife
[S16] How can I learn about Machine Learning
meantoyou?
mostefficiently?
[S42]Whatisyouropinionontheroleofgovern-
[S17]Howdocaterpillarsturnintobutterflies?
mentinsociety?
[S18] How do you approach decision-making
[S43]Whatisyourperspectiveonthesignificance
whenfacedwithmultipleoptions?
ofculturalheritage?
[S19]Howdoyoudefineart?
[S44]Whatisyourpreferredmethodoflearning
[S20]Howdoyoudefinehappiness?
newskillsoracquiringknowledge?
[S21]Howdoyoudefinesadness?
[S45]Whatisyourstanceontheroleofmachine
[S22]Howdoyoufeelaboutthedeathpenalty? learningineducation?
[S23] How do you prioritize your tasks and re- [S46]Whatstrategiesdoyoufindmosteffective
sponsibilitiesinyourdailylife? formanagingstress?[S47]Wouldyouliketoseeamovieaboutyour- Shakespeare Thecapabilitiesofthetrainedsteer-
self? ing vectors z(i) and activations a(i)(x) at layer i
x
toencodestyleintheShakespearedatasetarepre-
[S48] Write a recipe to make chocolate chip
sented in Fig. 6. In contrast to the Yelp review
muffins.
dataset,wewanttodifferentiatebetweenmodern
[S49]Writeanintroductionaboutyourselffora
andoriginalShakespeareanphrases. Thistaskdif-
CV.
fersfromtheothertwodatasetsinthatwedonot
changeemotionorsentimentbutawholewriting
Thefulllistofproducedanswerstotheprompts
style. The Shakespeare classifier on the trained
inSec.A.1andSec.A.2canbefoundat[redacted
steering vectors reaches a maximal AUC value
forpeer-review].
of 0.8, while their corresponding activation vec-
tors reach an AUC value of 0.96. Again, the lay-
B Effectoftheparameterλ
ers i ∈ {18,19,20} had high AUC values. This
The example below shows an example of the ef- supports our initial findings on the Yelp review
fect of the weighting parameter lambda for style dataset. As can be seen by comparing the AUC
vectors(Eq.3)onsteeringaresponsetowardneg- valuesfortheactivationvectorsfromShakespeare
ative sentiment. Larger values generally lead to (max. AUC=0.96/Fig.6c)withYelpinthesame
increasednegativity. However,ifthevaluesaretoo setting(max. AUC=0.99/Fig.6c), thestyledif-
large,themodel’soutputisnotadequateanymore ferencebetweenoriginalandmodernShakespeare
(oversteering). ishardertodistinguish, thanthesentimentinthe
Yelpreviews.
P: ReviewthepairofheadphonesthatIbought
onlinelastweek.
GoEmotions For this dataset, the ROC plots
need to be compared per layer because there are
λ = 0: I recently bought a pair of headphones
sixinsteadofnottwoclasses. Theresultsforlayer
onlineandIamverypleasedwiththem. The
19drawaslightlydifferentpicture(Fig.8)thanfor
sound quality is excellent and they are very
YelpandShakespeare. Probingtheactivationsof
comfortabletowear. Thedesignisalsostylish
all samples still results in the best micro-average
andtheadjustableheadbandallowsforacus-
AUC of 0.90. However, in the fair comparison
tomfit. Iwouldhighlyrecommendthesehead-
(activations for the 89 samples for which trained
phonestoanyonelookingforagoodpairof
steeringvectorsexist),theyhaveamicro-average
audioheadphones.
AUCof0.74,whilethecorrespondingtrainedvec-
λ = 0.6: I recently bought a pair of headphones torsreachanAUCof0.82. Nevertheless,thiscan
onlineandI’mverysatisfiedwiththem. The alsoresultfromthesmallnumberoftrainedsteer-
soundqualityisgreatandtheyareverycom- ing vectors found. The same result can be seen
fortable to wear. The design is also very forlayers18(Fig.7)and20(Fig.9). Weneedto
stylish and I love the adjustable fit. Highly investigatethisfindinginfuturestudiestoruleout
recommended! astatisticalanomalyasthecauseforthis. Still,the
layers i ∈ {18,19,20} have high micro-average
λ = 1.2: I love the pair of headphones you got! AUCvaluesofaround0.91forallactivationsand
Thesoundqualityisgreatandthecomfortable 0.81forthetrainedsteeringvectors.
design is perfect for all day listening. High
recommend!
Classifiertraining Duringourexperiments,we
triedtrainingtheregressionmodelinthreedifferent
λ = 1.9:<s> great great great great great great
settings: Predicting the class using only a single
...
layer, usingthreesubsequentlayers, andtraining
onalllayerstogether. Thedifferencebetweenthe
C Furtherresultsfromtheprobingstudy
resulting classifications is minimal, albeit perfor-
AnalogouslytotheanalysisoftheYelpdatasetin manceslightlyincreaseswhenusingmorelayers.
Sec.4.3,weperformedthesameexperimentwith Foreaseofpresentationandreadabilityoftheplots,
theShakespeareandtheGoEmotionsdatasets. wedecidedtoonlyincludesingle-layerclassifiers.1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4 Layer 0 / AUC=0.49
Layer 1 / AUC=0.89
Layer 3 / AUC=0.94
0.2 0.2 0.2 Layer 5 / AUC=0.95
Layer 18 / AUC=0.69 Layer 18 / AUC=0.96 Layer 10 / AUC=0.95
Layer 19 / AUC=0.74 Layer 19 / AUC=0.94 Layer 20 / AUC=0.95
0.0 Layer 20 / AUC=0.80 0.0 Layer 20 / AUC=0.95 0.0 Layer 30 / AUC=0.95
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate False Positive Rate
(a)Trainedsteeringvectors (b)Correspondingactivationvectors (c)Activationvectorsof17ksentences
Figure6: ComparisonbetweentheclassificationresultsontheShakespearedataset: Using(a)onlythetrained
steeringvectors,(b)thecorrespondingactivationvectors,and(c)activationvectorsof17ksentencesforselected
layers.
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 micro-average (AUC = 0.81) 0.4 micro-average (AUC = 0.71) 0.4 micro-average (AUC = 0.91)
sadness (AUC = 0.74) sadness (AUC = 0.68) sadness (AUC = 0.90)
joy (AUC = 0.78) joy (AUC = 0.70) joy (AUC = 0.96)
0.2 fear (AUC = 0.91) 0.2 fear (AUC = 0.45) 0.2 fear (AUC = 0.89)
anger (AUC = 0.74) anger (AUC = 0.69) anger (AUC = 0.90)
surprise (AUC = 0.88) surprise (AUC = 0.38) surprise (AUC = 0.92)
0.0 disgust (AUC = 0.51) 0.0 disgust (AUC = 0.71) 0.0 disgust (AUC = 0.83)
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate False Positive Rate
(a)Trainedsteeringvectors (b)Correspondingactivationvectors (c)Activationvectorsof2ksentences
Figure7: Classificationresultsofvectorsfromlayer18ontheGoEmotionsdataset: Using(a)onlythetrained
steeringvectors,(b)thecorrespondingactivationvectors,and(c)activationvectorsof2ksentences. Theactivation
vectorsonlyshowsuperiorperformanceifweincludemoresentencesthanwehavetrainedsteeringvectors.
D Furtherclassification-basedevaluation reason for this unstable behavior in GoEmotions
resultsforoutputsteering is probably the small number of trained steering
vectorsthatwerefound,whichwasespeciallylow
Thissectioncomparesthetraining-basedstylevec-
fortheclassesdisgustandsurprise.
torswiththeircorrespondingactivation-basedstyle
The steering is much more stable for the
vectors. Wedothistoensurefairnessinthecom-
activation-basedstylevectorsforfactualprompts
parisonsincethenumberofactivation-basedstyle
(Fig.11),whilethesubjectivearenotsteeredwell
vectorsissignificantlyhigherthanthenumberof
(Fig.13)prompts. Thegeneratedsentencesseem
training-based vectors. In the evaluation of the
tobebiasedtowardsjoy. Especially,disgustdoes
factual(Fig.10)andsubjective(Fig.12)prompts
notseemtobesteered. Theseresults,especiallyin
usingthetraining-basedstylevectorsontheGoE-
comparisontothesteeringwithallactivation-based
motionsdataset,wesawthatthesteeringseemsto
stylevectors(5),are,again,theresultofthesmall
workforallemotions,exceptdisgustandsurprise.
number of trained steering vectors, which limits
However,duringacloserexamination,itbecame
theamountofavailableactivation-basedstylevec-
evidentthatthemodel‘soutputwithλ ≥ 0.75did
tors. This,furthermore,highlightsthesuperiority
notrepresentpropersentencesanymoreandwere
oftheactivation-basedstylevectors,whichcanbe
mainlyrepetitionsofkeywordsrelatedtotheemo-
justextractedanddonotrequireacomputationally
tion,e.g.,“sadly”forsadness. FortheYelpdataset,
expensivelearningprocedure.
this happened as well, but only for higher λ. A
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurT1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 micro-average (AUC = 0.82) 0.4 micro-average (AUC = 0.74) 0.4 micro-average (AUC = 0.90)
sadness (AUC = 0.79) sadness (AUC = 0.71) sadness (AUC = 0.90)
joy (AUC = 0.75) joy (AUC = 0.77) joy (AUC = 0.96)
0.2 fear (AUC = 0.94) 0.2 fear (AUC = 0.73) 0.2 fear (AUC = 0.88)
anger (AUC = 0.75) anger (AUC = 0.68) anger (AUC = 0.91)
surprise (AUC = 0.91) surprise (AUC = 0.39) surprise (AUC = 0.92)
0.0 disgust (AUC = 0.55) 0.0 disgust (AUC = 0.62) 0.0 disgust (AUC = 0.83)
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate False Positive Rate
(a)Trainedsteeringvectors (b)Correspondingactivationvectors (c)Activationvectorsof2ksentences
Figure8: Classificationresultsofvectorsfromlayer19ontheGoEmotionsdataset: Using(a)onlythetrained
steeringvectors,(b)thecorrespondingactivationvectors,and(c)activationvectorsof2ksentences. Theactivation
vectorsonlyshowsuperiorperformanceifweincludemoresentencesthanwehavetrainedsteeringvectors.
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 micro-average (AUC = 0.81) 0.4 micro-average (AUC = 0.75) 0.4 micro-average (AUC = 0.91)
sadness (AUC = 0.80) sadness (AUC = 0.71) sadness (AUC = 0.91)
joy (AUC = 0.75) joy (AUC = 0.83) joy (AUC = 0.95)
0.2 fear (AUC = 0.94) 0.2 fear (AUC = 0.79) 0.2 fear (AUC = 0.88)
anger (AUC = 0.80) anger (AUC = 0.66) anger (AUC = 0.91)
surprise (AUC = 0.86) surprise (AUC = 0.39) surprise (AUC = 0.92)
0.0 disgust (AUC = 0.53) 0.0 disgust (AUC = 0.55) 0.0 disgust (AUC = 0.84)
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate False Positive Rate
(a)Trainedsteeringvectors (b)Correspondingactivationvectors (c)Activationvectorsof2ksentences
Figure9: Classificationresultsofvectorsfromlayer20ontheGoEmotionsdataset: Using(a)onlythetrained
steeringvectors,(b)thecorrespondingactivationvectors,and(c)activationvectorsof2ksentences. Theactivation
vectorsonlyshowsuperiorperformanceifweincludemoresentencesthanwehavetrainedsteeringvectors.
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurT1.0 1.0 1.0
sadness sadness sadness
joy joy joy
0.8 fear 0.8 fear 0.8 fear
anger anger anger
0.6 surprise 0.6 surprise 0.6 surprise
disgust disgust disgust
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(a)Steeringtoanger, (b)Steeringtodisgust, (c)Steeringtojoy,
factualprompts factualprompts factualprompts
1.0 1.0 1.0
sadness sadness
joy joy
0.8 fear 0.8 0.8 fear
anger anger
0.6 surprise 0.6 0.6 surprise
disgust disgust
sadness
0.4 0.4 joy 0.4
fear
anger
0.2 0.2 0.2
surprise
disgust
0.0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(d)Steeringtofear, (e)Steeringtosadness, (f)Steeringtosurprise,
factualprompts factualprompts factualprompts
Figure10: Training-basedstylevectors: EvaluationofgeneratedtextsforfactualpromptsusingGoEmotions’style
vectors.
erocs
ssalc
noitomE
erocs
ssalc
noitomE
erocs
ssalc
noitomE
erocs
ssalc
noitomE
erocs
ssalc
noitomE
erocs
ssalc
noitomE1.0 1.0 1.0
sadness sadness sadness
joy joy joy
0.8 fear 0.8 fear 0.8 fear
anger anger anger
0.6 surprise 0.6 surprise 0.6 surprise
disgust disgust disgust
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(a)Steeringtoanger, (b)Steeringtodisgust, (c)Steeringtojoy,
factualprompts factualprompts factualprompts
1.0 1.0 1.0
sadness sadness sadness
joy joy joy
0.8 fear 0.8 fear 0.8 fear
anger anger anger
0.6 surprise 0.6 surprise 0.6 surprise
disgust disgust disgust
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(d)Steeringtofear, (e)Steeringtosadness, (f)Steeringtosurprise,
factualprompts factualprompts subjectiveprompts
Figure11: Activation-basedstylevectors: EvaluationofgeneratedtextsforfactualpromptsusingGoEmotions’
stylevectors. Onlytheactivationvectorswereused,forwhichwehavetrainedsteeringvectors.
erocs
ssalc
noitomE
erocs
ssalc
noitomE
erocs
ssalc
noitomE
erocs
ssalc
noitomE
erocs
ssalc
noitomE
erocs
ssalc
noitomE1.0 1.0 1.0
sadness sadness sadness
joy joy joy
0.8 fear 0.8 fear 0.8 fear
anger anger anger
0.6 surprise 0.6 surprise 0.6 surprise
disgust disgust disgust
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(a)Steeringtoanger, (b)Steeringtodisgust, (c)Steeringtojoy,
subjectiveprompts subjectiveprompts subjectiveprompts
1.0 1.0 1.0
sadness sadness
joy joy
0.8 fear 0.8 0.8 fear
anger anger
0.6 surprise 0.6 0.6 surprise
disgust disgust
sadness
0.4 0.4 joy 0.4
fear
anger
0.2 0.2 0.2
surprise
disgust
0.0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(d)Steeringtofear, (e)Steeringtosadness, (f)Steeringtosurprise,
subjectiveprompts subjectiveprompts subjectiveprompts
Figure12: Training-basedstylevectors: EvaluationofgeneratedtextsforsubjectivepromptsusingGoEmotions’
stylevectors. Mostoutputsarenotpropersentences.
erocs
ssalc
noitomE
erocs
ssalc
noitomE
erocs
ssalc
noitomE
erocs
ssalc
noitomE
erocs
ssalc
noitomE
erocs
ssalc
noitomE1.0 1.0 1.0
aannggeerr ((pprroommppttiinngg)) ddiissgguusstt ((pprroommppttiinngg)) jjooyy ((pprroommppttiinngg))
ssaaddnneessss ssaaddnneessss ssaaddnneessss
jjooyy jjooyy jjooyy
0.8 ffeeaarr 0.8 ffeeaarr 0.8 ffeeaarr
aannggeerr aannggeerr aannggeerr
ssuurrpprriissee ssuurrpprriissee ssuurrpprriissee
ddiissgguusstt ddiissgguusstt ddiissgguusstt
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(a)Steeringtoanger, (b)Steeringtodisgust, (c)Steeringtojoy,
subjectiveprompts subjectiveprompts subjectiveprompts
1.0 1.0 1.0
ffeeaarr ((pprroommppttiinngg)) ssaaddnneessss ((pprroommppttiinngg)) ssuurrpprriissee ((pprroommppttiinngg))
ssaaddnneessss ssaaddnneessss ssaaddnneessss
jjooyy jjooyy jjooyy
0.8 ffeeaarr 0.8 ffeeaarr 0.8 ffeeaarr
aannggeerr aannggeerr aannggeerr
ssuurrpprriissee ssuurrpprriissee ssuurrpprriissee
ddiissgguusstt ddiissgguusstt ddiissgguusstt
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(d)Steeringtofear, (e)Steeringtosadness, (f)Steeringtosurprise,
subjectiveprompts subjectiveprompts subjectiveprompts
Figure13: Activation-basedstylevectors: EvaluationofgeneratedtextsforsubjectivepromptsusingGoEmotions’
stylevectors. Onlytheactivationvectorswereused,forwhichwehavetrainedsteeringvectors.
erocs
tnemitneS
erocs
tnemitneS
erocs
tnemitneS
erocs
tnemitneS
erocs
tnemitneS
erocs
tnemitneS