Immersive Video Compression using Implicit
Neural Representations
Ho Man Kwan1, Fan Zhang1, Andrew Gower2, and David Bull1
1Visual Information Laboratory, University of Bristol, Bristol, BS1 5DD, United Kingdom
1{hm.kwan, fan.zhang, dave.bull}@bristol.ac.uk
2Immersive Content & Comms Research, BT, UK
2andrew.p.gower@bt.com
Abstract—Recent work on implicit neural representations various solutions, which support current immersive video
(INRs) has evidenced their potential for efficiently representing formats including MultiView+Depth (MVD) and Point Cloud
and encoding conventional video content. In this paper we, for
(PP).Theformeriscommonlyusedinvideo-basedproduction
thefirsttime,extendtheirapplicationtoimmersive(multi-view)
workflows (e.g. 3D film) and the latter is mainly employed
videos, by proposing MV-HiNeRV, a new INR-based immersive
video codec. MV-HiNeRV is an enhanced version of a state-of- for 3D graphics-based production (e.g. 3D games). For both
the-artINR-basedvideocodec,HiNeRV,whichwasdevelopedfor MVD and PP, two standards have been recently developed
single-view video compression. We have modified the model to in the framework of MPEG-I, referred to as MIV (MPEG
learn a different group of feature grids for each view, and share
Immersive Video) [2] and V-PCC (Video-based Point Cloud
thelearntnetworkparametersamongallviews.Thisenablesthe
Compression) [3]. In this paper, we focus solely on video modeltoeffectivelyexploitthespatio-temporalandtheinter-view
redundancy that exists within multi-view videos. The proposed compression for MVD.
codec was used to compress multi-view texture and depth video In the MIV pipeline, the input comprises multiple source
sequences in the MPEG Immersive Video (MIV) Common Test viewsforbothtextureanddepthdata,togetherwithassociated
Conditions, and tested against the MIV Test model (TMIV) that
source camera parameters. Both texture and depth frames are
usestheVVenCvideocodec.Theresultsdemonstratethesuperior
first processed to identify basic and additional views. The
performanceofMV-HiNeRV,withsignificantcodinggains(upto
72.33%)overTMIV.TheimplementationofMV-HiNeRVwillbe redundancies in the additional views are then removed, while
published for further development and evaluation1. non-redundant information is combined with the basic video
IndexTerms—VideoCompression,Immersivevideo,Multiview and compressed using a standard video encoder (e.g., HEVC
video, Implicit neural representation, MV-HiNeRV
HM [4] or VVC VVenC [5]). It is noted that, while it is
convenient to employ a conventional video codec for texture
I. INTRODUCTION
and depth video compression, such codecs are not designed
As part of an extended video parameter space, including
to exploit the redundancy within multiple views (this relies
higher spatial resolution, greater dynamic range, higher frame
on a pre-processing tool used for extracting basic views and
rateandwidercolourgamut,newvideoformatshaveemerged
removing redundant information in additional views).
to enable more immersive viewing experience with three or
In the realm of learning-based video compression, deep
six degrees of freedom (3DoF or 6DoF). These underpin the
neuralnetworkshavebeendeployedtoenhancethetraditional
development of virtual reality (VR), augmented reality (AR)
coding pipeline [6–10] or build a fully learnable framework
ormixedreality(MR)systems[1].Rawimmersivevideodata
to achieve end-to-end optimisation [11–14]. More recently,
is typically generated based on computer-generated imagery
Implicit Neural Representations (INRs) [15–19] have shown
models or captured using multiple-camera systems and ded-
great potential to achieve comparable compression perfor-
icated geometry sensors. It is then converted into different
mancetobothstandardandotherlearning-basedvideocodecs,
data formats, such as point clouds, multi-view texture+depth
importantlyalsoachievingrelativelyfastdecodingspeeds.An
orequirectangularvideo,whicharecompressedusingdifferent
INRisaneuralnetworkthatisoptimisedtomapcoordinatesto
video/data codecs for transmission or storage. To present
pixelsduringvideoencoding,reconstructingtheentirevideoat
or display immersive video content, the compressed data
the decoder by performing inference. However, existing INR-
is decoded and synthesised/rendered to enable 3DoF/6DoF
based codecs were designed for encoding single-view videos;
viewing capabilities on VR, AR or MR devices.
the application to multi-view video coding has not previously
To standardise immersive video production and streaming,
been investigated.
the Moving Picture Experts Group (MPEG) has developed
To this end, we investigate the use of INRs for immersive
ThisworkwasfundedbyUKEPSRC(iCASEAwards),BTandtheUKRI video compression, and propose a new INR-based multi-
MyWorld Strength in Places Programme. High performance computational view video codec, MV-HiNeRV, in this paper. The proposed
facilities were provided by the Advanced Computing Research Centre, Uni-
approachisanextensionofastate-of-the-artINR-basedvideo
versityofBristol.
1https://github.com/hmkx/MV-HiNeRV codec, HiNeRV [18], which was developed for conventional
4202
beF
2
]VI.ssee[
1v69510.2042:viXravideo compression. Specifically, for multi-view video coding,
MV-HiNeRV is extended to learn a different set of feature
grids for each view, sharing the network parameters between
all views. This effectively exploits the spatio-temporal redun-
dancy among different views.
To evaluate its coding efficiency, MV-HiNeRV has been
testedonMIVCTCtestsequences[20]andcomparedagainst
(a)
the Test Model of MIV (TMIV) [21]. The results show its
significant performance improvement over the original TMIV,
with an average bit rate saving of 46.92%. To the best of
ourknowledge,MV-HiNeRVisthefirstINR-basedimmersive
video codec, and the results demonstrate the promise of using
INR-based models for immersive video compression.
II. METHOD
ImplicitNeuralRepresentation(INR)-basedvideocompres-
sion has been shown to outperform or compete with various
advanced standard codecs in terms of coding efficiency [15–
19, 22]. Due to its ability to exploit redundancies between
frames, this capability is likely to benefit the compression
of immersive videos which exhibit additional redundancies
throughmultipleviews.Basedonthisobservation,wepropose
an INR-based codec, MV-HiNeRV (a multi-view version of
(b)
HiNeRV [18]) for encoding multi-view videos with the aim
of achieving improved coding performance. Fig. 1: (a). HiNeRV. In HiNeRV, an input patch can be obtained by
interpolation from the feature grids, and the network layers output
A. MV-HiNeRV the corresponding video patch. (b). The proposed MV-HiNeRV. In
MV-HiNeRV, each view is represented by a dedicated set of feature
Existing multi-view video coding methods either compress
grids, and the network layers are shared across views. This allow
multiple views by leveraging inter-view redundancy [23, 24], efficientmultiviewvideocodingasitexploitingthespatial,temporal
or apply view pruning using a View Optimiser [2]. As and view redundancy simultaneously.
mentioned above, by utilising INR models, it is possible to
directly compress a large number of video views with a high
and 0 ≤ i < W. To compute the output of the k-th view,
coding efficiency. Moreover, using INR models for exploiting M
MV-HiNeRV first computes the input feature map, i.e., the
redundancy is potentially time efficient, since the encoding
encoding obtained from interpolation with the multi-temporal
process is the model training, which its length has a sub-
resolutiongrids[17]fromthek-thview,γ ;itthenapplies
linear relationship with the number of input frames or videos; k,base
a stem convolutional layer F , to obtain the first stage
thisdiffersfromconventionalandotherlearning-basedcoding stem
feature map:
methods.
In this work, to adapt HiNeRV for multiview video com- X =F (γ (i,j,t)). (1)
0 stem k,base
pression, we modified the original INR model by storing
Subsequently, N HiNeRV blocks are progressively processed
a separated set of feature grids for each view, where the
using the feature maps for both upscaling and transformation,
network parameters for the convolutional and fully connected
wherewedenotetheoutputofthen-thHiNeRVblockasX ,
n
layers are shared among views, in addition to the spatial and
0<n≤N:
temporal dimensions. This allows simultaneous exploitation
of redundancy in all three dimensions. Fig. 1a and 1b show X =F (U (X )+F (γ (i,j,t))),0<n≤N (2)
n n n n−1 enc k,n
the difference between the original HiNeRV and the proposed
It should be noted that the hierarchical encoding [18] in MV-
multiview variant.
HiNeRV is also view dependent, as the grids are compact
Following the approach in [18], assuming that we encode
andonlyneedasmallnumberofparameters.Thehierarchical
a multiview video with K views, we denote the k-th view as
encodingatthek-thviewofthen-thblockisdenotedasγ .
V , such that 0≤k <K. For simplicity, we assume that the k,n
k
InMV-HiNeRV,wesimplyfollowHiNeRV,whereweusethe
video size is the same for V ...V , i.e., T ×H ×W ×C,
0 K−1
ConvNeXt [25] block as the internal layer in the HiNeRV
where T, H, W and C are the temporal resolution, spatial
blocks.
resolution and the number of channels, respectively.
Finally, a linear head layer F is applied to transform
With a patch size M ×M, MV-HiNeRV represents each head
the feature maps X into the target output space:
video in multiple patches, where each patch is denoted by N
its patch index (i,j,t), where 0 ≤ t < T, 0 ≤ j < H Y =F (X ), (3)
M head NUnlike HiNeRV, MV-HiNeRV encodes both the texture and Previous work [19] employed a non-parametric model [28]
depthmapsatthesametime,henceweproduceafourchannel for estimating the distribution, introducing additional param-
(RGB and D) output with the linear head layer. eters that require training. This may increase complexity, so
During model training, we randomly sample video patches in this work we simply assume Gaussian distribution which is
fromallviewsandframes[18].Wealsoperformtrainingwith effective in practice.
overlapped patches in the feature space; this supports training
C. Model Training Pipeline
with patches while enhancing the encoding quality.
Regarding model training, previous work in [19] adopted a
B. Weight Quantisation and Entropy Regularisation two stage approach, in which two models are trained without
regularisation in Stage 1, while multiple models are trained in
Weight quantisation. Following [19], we applied the
Stage 2 with regularisation for different rate points, achieved
learned quantisation and entropy regularisation to reduce the
by adjusting the weight between the distortion and rate terms.
modelsize.Specifically,foraparametervectorθ ={θ }with
i
However, we found that the above approach will lead to sub-
a trainable quantisation step vector δ = {δ }, the quantised
i
parameter θˆ={θˆ} is computed as follows: optimal rate-distortion performance when the weight of the
i
ratetermisheavier,i.e.,forobtainingthemorecompactmod-
θˆ =δ×⌊θ i⌉. (4) els.Hence,inthispaper,wemodifiedthistwo-stageapproach
i δ [19], but using a different value for λ, and a different set of
i
the model hyper-parameters for each target rate point model,
We follow the model compression implementation in [26],
asinsomeotherINR-basedworks[15–18,22].InStage1,we
where the logarithm of δ is learned instead of δ. The actual
trainoneMV-HiNeRVmodelwithoutanyregularisationwhich
step sizes are also shared between a subset of the parameters
allows faster convergence. In Stage 2 we manually initialise
from the same layer, in order to reduce the overhead. For
the step sizes to a value such that the quantisation width to
instance, the same step size is employed for a row or a
7 at the beginning. Then, we train the model with entropy
column in the weight of the linear layer. Since quantisation
regularisation; where we found that using a smaller learning
is a non-differentiable operation, we use Quant-Noise [27] as
ratefornetworkparametersandalargeroneforthestepsizeis
in HiNeRV [18] in the training and employ the quantisation
actually more effective for preserving the model quality while
during evaluation.
reducing the size. We also apply a linear scheduling for the
Entropy regularisation. To train the model parameters
quantisationnoiserate[27].Inourapproach,only300epochs
and the quantisation step sizes at the same time, we follow
are used for Stage 1 training without regularisation, and 60
the common practice in lossy compression where entropy
epochs for Stage 2, while 1200 and 300 epochs are used in
regularisation is used to optimise both the rate and distortion
[19],respectively.MostofthetrainingconfigurationsforMV-
jointly, incorporating a Lagrangian multiplier, λ:
HiNeRV are the same as those in [18].
L=R+λD, (5)
III. EXPERIMENTS
Here, the rate term R is defined as the sum of the negative We employed six mandatory test sequences in the MIV
log likelihood of the quantised weights, i.e., the lower bound Common Test Conditions (CTC) [20] for testing. We com-
of the total code length, and the distortion term D can be pared MV-HiNeRV with the MPEG Test Model (TMIV) [21]
calculatedusingdifferentlossfunctionssuchastheMSEloss. using the main anchor configuration. TMIV employs VVenC
For a model with a quantised parameter vector θˆ={θˆ}, we [5]inRandomAccessmodetoencodebothtextureanddepth
i
compute the rate term by information. Following the CTC, we performed experiments
with the specified start frames and encoded 65 frames in
|θˆ|
R=(cid:88) −log p(θˆ) (6) each sequence. For MV-HiNeRV, we converted the original
2 i YUV 4:2:0 texture frames into the RGB colour space, then
i=0 concatenated them with the depth frames as the input.
In particular, we use a multivariate Gaussian [28, 29] for AfterdecodingusingMV-HiNeRV,weconverttheoutputof
estimating the weight distribution and thus the rate term. Un- MV-HiNeRVbacktotheoriginalYUV4:2:0format,andapply
like other end-to-end optimised learning-based video codecs the same view synthesizer in TMIV to generate both original
[11, 12], the distribution of the symbols, which is transmitted and new views for evaluation, as described in the MIV CTC
in INR-based compression, can be computed easily because [20]. We use both PSNR and IV-PSNR [31] to evaluate video
the symbols are simply the quantised network parameters. quality, and employ arithmetic coding for entropy coding to
Thus, we compute the Gaussian parameters directly in each obtaintheactualrate.Metricsarecomputedwithregardtothe
training step, and transmit them along with with the model best reference [20].
parameter bitstream. To estimate the rate of each symbol, For MV-HiNeRV training, we use MSE loss in the RGBD
we also use the continuous approximation of the quantised space as the distortion term D. It should be noted that the
variables in [30]. We only apply regularisation to the feature reconstruction quality is highly dependent on the accuracy of
grids and the weights of the linear and convolutional layers. thedepthmap.ComputingMSElossinthedepthchannelmayTABLE I: BD-Rate (%, measured in PSNR/IV-PSNR) results of
MV-HiNeRV on the MIV CTC test sequences (for both source and
synthesised frames). The anchor is TMIV.
Metric B02 D01 E01 J02 J04 W01 Overall
PSNR -17.60 -65.93 -36.59 -59.96 -80.03 -35.48 -49.27
IV-PSNR -38.11 -61.08 -6.28 -70.21 -72.33 -33.50 -46.92
(a)B02-p03 Forperceptual qualityassessment, weprovide visualexam-
ples of the synthesised views from MIV and MV-HiNeRV
(using the same viewer synthesiser) in Fig. 2. It can be
observed that, in general, the MV-HiNeRV produces high-
quality frames with rich spatial details. More importantly, the
outputs contain fewer visual artefacts as in TMIV, which may
be due to the MIV preprocessing. In these cases, the bit rates
for MV-HiNeRV examples are similar or even smaller than
that for MIV. The only issue we noticed is that MV-HiNeRV
(b)D01-v13 perform worse in cases where high depth map precision is
required, such as for the edges of the foreground objects in
sequence B02.
In terms of complexity, the encoding process with MV-
HiNeRV is relatively slow, similar to existing INR-based
methods [15–19, 22]. For example, encoding the sequence
W01, which comprises 21 full HD views, each consisting of
65 frames, can take up to 26 hours on a computer with a
V100GPU.WhiledecodingwithINRsistypicallyfast,inour
(c)J02-p02 case,theentiredecodingprocessislimitedbytheTMIVview
synthesizer.Forthesamesequence,decodingallsourceviews
withMV-HiNeRVcanbecompletedinlessthan2minutes,but
rendering a synthesised view with the TMIV view synthesizer
takes approximately 12 minutes.
IV. CONCLUSION
In this paper, we proposed an INR-based codec, MV-
(d)J04-v07 HiNeRV, for encoding multiview videos. MV-HiNeRV is an
extension of HiNeRV, which was developed for single-view
Fig. 2: Example of synthesised views (cropped) of the CTC test
sequence [20]. video compression. The proposed approach learns a different
set of feature grids for each view and shares the model
resultininsufficientprecisionduetothevaryingrangeofdepth parameters among all views, effectively exploiting the inter-
valuesacrossdifferentscenarios.Asaresult,wenormalisethe view redundancy. MV-HiNeRV has been evaluated against
depth map of each sequence to [0, 1]. Detailed network and the MPEG MIV Test Model, TMIV, and achieved significant
training configurations are provided in the code repository. performance improvement, with 46.92% average coding gain.
TheresultsdemonstratethesignificantpotentialofINR-based
videocodecsforthecompressionofimmersivevideoformats.
A. Quantitative and Qualitative Results
Future work should focus on better integration of viewer
TABLE I summarises compression results for MV-HiNeRV synthesiser with INR-based immersive video codecs.
compared to TMIV in terms of BD-rate values measured in
PSNR/IV-PSNR [31] (the average among both reconstructed REFERENCES
source and pose-trace views). MV-HiNeRV performs signifi- [1] D.BullandF.Zhang,Intelligentimageandvideocompression:
cantly better than TMIV on all six test sequences, achieving communicating pictures. Academic Press, 2021.
upto72.33%bitratesavings(measuredbyBjøntegaardDelta [2] J. M. Boyce, R. Dore´, A. Dziembowski, J. Fleureau, J. Jung,
B.Kroon,B.Salahieh,V.K.M.Vadakital,andL.Yu,“MPEG
Rate, BD-rate [32] using IV-PSNR). This performance is also
immersivevideocodingstandard,”Proc.IEEE,vol.109,no.9,
confirmed by Fig. 3, which plots the rate-PSNR curves for
pp. 1521–1536, 2021.
all sequences (including both the reconstructed views at the [3] D.Graziosi,O.Nakagami,S.Kuma,A.Zaghetto,T.Suzuki,and
source and the pose-trace viewports). A.Tabatabai,“Anoverviewofongoingpointcloudcompression[10] F.Zhang,M.Afonso,andD.R.Bull,“ViSTRA2:Videocoding
using spatial resolution and effective bit depth adaptation,”
Signal Processing: Image Communication, vol. 97, p. 116355,
2021.
[11] G. Lu, W. Ouyang, D. Xu, X. Zhang, C. Cai, and Z. Gao,
“DVC:AnEnd-To-EndDeepVideoCompressionFramework,”
in CVPR. Computer Vision Foundation / IEEE, 2019, pp.
11006–11015.
[12] J.Li,B.Li,andY.Lu,“DeepContextualVideoCompression,”
in NeurIPS, 2021, pp. 18114–18125.
[13] E. Agustsson, D. Minnen, N. Johnston, J. Balle´, S. J. Hwang,
and G. Toderici, “Scale-Space Flow for End-to-End Optimized
Video Compression,” in CVPR. Computer Vision Foundation
/ IEEE, 2020, pp. 8500–8509.
[14] J.Li,B.Li,andY.Lu,“NeuralVideoCompressionwithDiverse
Contexts,” in CVPR. IEEE, 2023, pp. 22616–22626.
[15] H.Chen,B.He,H.Wang,Y.Ren,S.Lim,andA.Shrivastava,
“NeRV:NeuralRepresentationsforVideos,”inNeurIPS,2021,
pp. 21557–21568.
[16] H. Chen, M. Gwilliam, S. Lim, and A. Shrivastava, “HNeRV:
AHybridNeuralRepresentationforVideos,”inCVPR. IEEE,
2023, pp. 10270–10279.
[17] J. C. Lee, D. Rho, J. H. Ko, and E. Park, “FFNeRV: Flow-
Guided Frame-Wise Neural Representations for Videos,” in
ACM Multimedia. ACM, 2023.
[18] H.M.Kwan,G.Gao,F.Zhang,A.Gower,andD.Bull,“HiN-
eRV: Video Compression with Hierarchical Encoding based
Neural Representation,” 2023.
[19] C. Gomes, R. Azevedo, and C. Schroers, “Video Compression
with Entropy-Constrained Neural Representations,” in CVPR.
IEEE, 2023, pp. 18497–18506.
[20] A.Dziembowski,B.Kroon,andJ.Jung,“Commontestcondi-
Fig.3:IV-PSNRresultsofTMIVandMV-HiNeRVontheMIVCTC tionsforMPEGimmersivevideo.” ISO/IECJTC1/SC29/WG
test sequences. 04, 2023.
[21] G.L.AdrianDziembowski,“Testmodel17forMPEGimmer-
sive video.” ISO/IEC JTC 1/SC 29/WG 4, 2023.
standardization activities: Video-based (v-pcc) and geometry- [22] B. He, X. Yang, H. Wang, Z. Wu, H. Chen, S. Huang, Y. Ren,
based(g-pcc),”APSIPATransactionsonSignalandInformation S.Lim,andA.Shrivastava,“Towardsscalableneuralrepresen-
Processing, vol. 9, p. e13, 2020. tation for diverse videos,” in CVPR. IEEE, 2023, pp. 6132–
[4] G. J. Sullivan, J. Ohm, W. Han, and T. Wiegand, “Overview 6142.
oftheHighEfficiencyVideoCoding(HEVC)Standard,”IEEE [23] A. Vetro, T. Wiegand, and G. J. Sullivan, “Overview of
Trans. Circuits Syst. Video Technol., vol. 22, no. 12, pp. the Stereo and Multiview Video Coding Extensions of the
1649–1668, 2012. [Online]. Available: https://doi.org/10.1109/ H.264/MPEG-4 AVC Standard,” Proc. IEEE, vol. 99, no. 4,
TCSVT.2012.2221191 pp. 626–642, 2011.
[5] A.Wieckowski,J.Brandenburg,T.Hinz,C.Bartnik,V.George, [24] G. Tech, Y. Chen, K. Mu¨ller, J. Ohm, A. Vetro, and Y. Wang,
G. Hege, C. Helmrich, A. Henkel, C. Lehmann, C. Stoffers, “Overview of the Multiview and 3D Extensions of High
I. Zupancic, B. Bross, and D. Marpe, “VVenC: An Open And Efficiency Video Coding,” IEEE Trans. Circuits Syst. Video
OptimizedVVCEncoderImplementation,”inProc.IEEEInter- Technol., vol. 26, no. 1, pp. 35–49, 2016.
nationalConferenceonMultimediaExpoWorkshops(ICMEW), [25] Z.Liu,H.Mao,C.Wu,C.Feichtenhofer,T.Darrell,andS.Xie,
pp. 1–2. “AConvNetforthe2020s,”inCVPR. IEEE,2022,pp.11966–
[6] R. Song, D. Liu, H. Li, and F. Wu, “Neural network- 11976.
based arithmetic coding of intra prediction modes in [26] J. Balle´, S. J. Hwang, and E. Agustsson, “TensorFlow
HEVC,” in 2017 IEEE Visual Communications and Image Compression: Learned data compression,” 2023. [Online].
Processing, VCIP 2017, St. Petersburg, FL, USA, December Available: http://github.com/tensorflow/compression
10-13, 2017. IEEE, 2017, pp. 1–4. [Online]. Available: [27] P.Stock,A.Fan,B.Graham,E.Grave,R.Gribonval,H.Je´gou,
https://doi.org/10.1109/VCIP.2017.8305104 and A. Joulin, “Training with Quantization Noise for Extreme
[7] M.Afonso,F.Zhang,andD.R.Bull,“Videocompressionbased Model Compression,” in ICLR. OpenReview.net, 2021.
onspatio-temporalresolutionadaptation,”IEEETransactionson [28] J. Balle´, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston,
Circuits and Systems for Video Technology, vol. 29, no. 1, pp. “Variational image compression with a scale hyperprior,” in
275–280, 2018. ICLR. OpenReview.net, 2018.
[8] F.Zhang,D.Ma,C.Feng,andD.R.Bull,“Videocompression [29] D.Minnen,J.Balle´,andG.Toderici,“Jointautoregressiveand
with CNN-based postprocessing,” IEEE MultiMedia, vol. 28, hierarchicalpriorsforlearnedimagecompression,”inNeurIPS,
no. 4, pp. 74–83, 2021. 2018, pp. 10794–10803.
[9] D. Ma, F. Zhang, and D. R. Bull, “MFRNet: a new cnn [30] J. Balle´, V. Laparra, and E. P. Simoncelli, “End-to-end Opti-
architecture for post-processing and in-loop filtering,” IEEE mized Image Compression,” in ICLR. OpenReview.net, 2017.
JournalofSelectedTopicsinSignalProcessing,vol.15,no.2, [31] A. Dziembowski, D. Mieloch, J. Stankowski, and A. Grzelka,
pp. 378–387, 2020. “IV-PSNR - the objective quality metric for immersive videoapplications,”IEEETrans.CircuitsSyst.VideoTechnol.,vol.32, between RD-curves,” in 13th VCEG Meeting, no. VCEG-
no. 11, pp. 7575–7591, 2022. M33,Austin, Texas, 2001, pp. USA: ITU–T.
[32] G. Bjøntegaard, “Calculation of average PSNR differences