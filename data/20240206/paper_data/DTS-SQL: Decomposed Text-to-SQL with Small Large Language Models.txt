DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models
MohammadrezaPourreza DavoodRafiei
UniversityofAlberta UniversityofAlberta
pourreza@ualberta.ca drafiei@uablerta.ca
Abstract Model EX EM
Fine-tuningmethods
Llama27B
Leadingmodelsforthetext-to-SQLtaskheav-
(Gaoetal.,2023) 66.7 63.9
ilyrelyonproprietaryLargeLanguageMod- Llama213B
els(LLMs),posingconcernsoverdataprivacy. (Gaoetal.,2023) 67.0 62.7
Closing the performance gap between small Promptingmethods
DAIL-SQL+GPT4
open-sourcemodelsandlargeproprietarymod-
(Gaoetal.,2023) 84.4 74.4
elsiscrucialtomitigatethisreliance. Tothis
DIN-SQL+GPT4
end,weintroduceanoveltwo-stagefine-tuning (PourrezaandRafiei,2023) 74.2 60.1
approach that decomposes the task into two
simplertasks. Throughcomprehensiveevalu- Table 1: Perfromance comparison of the prompting
ationontwolargecross-domaindatasetsand methods and fineutning method on Spider validation
twosmallLLMs,weshowthatthisapproach dataset
improvesexecutionaccuracyby3to7percent,
effectivelyaligningtheperformanceofopen-
addressthisdisparitybyintroducinganoveltwo-
sourcemodelswiththeirproprietarycounter-
parts. step decomposed fine-tuning method, employing
twosmallerLLMs,eachwithaparamatersizeof
1 Introduction 7 billion. This approach achieves a performance
comparabletomethodsthatareusingGPT-4with
Natural language interfaces for databases allow
few-shotlearningandwell-designedprompts.
userstoderiveinsightsfromstructureddatabases
We evaluate the performance of our proposed
using natural language instead of complex SQL
methodusingtwoText-to-SQLbenchmarks: Spi-
queries. Leadingopen-sourcemethods(Pourreza
der (Yuet al., 2018)and Spider-SYN (Ganet al.,
and Rafiei, 2023; Gao et al., 2023) for this task
2021)andtwo7BLLMs: DeepSeekDeepSeek-AI
heavilydependonproprietaryLargelanguagemod-
(2024) and Mistral Jiang et al. (2023). Our ap-
els(LLMs)likeGPT-4andGPT-3.5-turbo,which
proachdemonstratesaperformanceimprovement
havedemonstratedsuperiorperformanceinText-to-
of approximately 3 to 7 percent in execution ac-
SQLbenchamrks(Yuetal.,2018;Lietal.,2023c;
curacy compared to the conventional single-step
Ganetal.,2021). However,thisrelianceonlarge
fine-tuningmethodemployedinpreviousstudies
proprietary models has privacy and cost implica-
(Gao et al., 2023). This consistent performance
tions. Forinstance,manylargeenterprisescannot
gainacrossbothdatasetshighlightsthegeneraliz-
sharetheircustomerdatawiththemodel-providing
ability of our method. Moreover, our fine-tuning
companiesduetoprivacyconsiderations. Addition-
strategy,utilizinga7billionparameterLLM,sur-
ally,costisafactor,especiallyforsmallbusinesses,
passes all previous open-source methods on the
inadoptingthesemodels.
Spiderdevelopmentsetandachievescomparable
Recent attempts to utilize open-source LLMs
resultstothestate-of-the-artopen-sourcemethods
(Gao et al., 2023) and fine-tune them using
usingGPT-4(PourrezaandRafiei,2023;Gaoetal.,
question-SQLquerypairshavefallenshortofthe
2023)ontheSpidertestset. Wehaveprovidedall
zero-shot performance of GPT-3.5-turbo. Table
the necessary code to replicate the results, along
1 presents a performance comparison of the fine-
with the models’ predicted SQL queries, in our
tuned open-source LLMs on the Spider develop-
GitHubrepository1.
ment set, contrasting with methods that employ
GPT-4’spromptingtechniques. Thispaperaimsto 1https://anonymous.4open.science/r/
4202
beF
2
]LC.sc[
1v71110.2042:viXra2 Methodology givenquestionforgeneratingaccurateSQLqueries.
Therefore, a common approach in fine-tuning in-
A notable development in LLMs is their post-
volves including the schema of all tables within
pretrainingrefinement,whichenhancestheiralign-
the prompts together with the question and SQL
mentwithpreferredbehaviors,asdocumentedby
pairs. This method serves a dual purpose: teach-
Mishra et al. (2021); Victor et al. (2022); Thop-
ing the model to generate the correct SQL query
pilan et al. (2022). Common methods of align-
andtoidentifytherelevanttablesfromamongall
ment include Supervised Fine-Tuning (SFT) us-
theprovidedtables. Thisapproachoftrainingfor
inghumandemonstrations,asreportedbyOuyang
twoobjectivessimultaneouslycomplicatestheSQL
etal.(2022);Tunstalletal.(2023)andReinforce-
generationtaskforLLMs,particularlyforsmaller
mentLearningfromHumanFeedback(RLHF),as
models with only a few billion parameters. Each
detailed by Christiano et al. (2017); Ziegler et al.
task–generatingSQLqueriesandcorrectlylinking
(2019);Stiennonetal.(2020);Baietal.(2022).
totherelevantschema–demandsitsownreason-
Theabsenceofextensivedatasetscontainingei-
ing process. A significant proportion of errors in
therhumanorAIfeedback(Leeetal.,2023)hasled
large language models can be attributed to incor-
toapredominantfocusonsupervisedfine-tuningin
rect schema linking, highlighting this as a major
thetext-to-SQLfield. Thisapproachnecessitatesa
challengeinthefield(PourrezaandRafiei,2023;
collectionofspecificinstructionsorpromptsalong
Dongetal.,2023).
withtheircorrespondingoutputsorresponses. In
thefollowingsection,wewilldelveintotheestab- 2.2 DecomposedSupervisedFine-tuning
lishedmethodsofsupervisedfine-tuningforLLMs
Weproposeatwo-stagefine-tuningprocess,which
withintheText-to-SQLcontext. Subsequently,we
separatesschemalinkingandSQLgeneration,aim-
introduceournoveltwo-stepfine-tuningapproach,
ingtoenhancetheperformanceofNL-to-SQLsys-
designedtoenhancetheperformanceofmodelsin
tems.
theText-to-SQLdomain.
2.2.1 Schema-linkingFine-tuning
2.1 Supervisedfine-tuningforText-to-SQL
Schemalinkinginvolvesidentifyingthepertinent
In this section, we explore the supervised fine- columns and tables in a database in response to
tuningprocessforText-to-SQLtasks,aspracticed naturallanguagequeries. Ithasbeendemonstrated
intheopen-sourcecommunity(Gaoetal.,2023). to enhance cross-domain generalizability and fa-
Given a set of databases D i comprising pairs of cilitatethecreationofintricatequeries(Leietal.,
questionsq i andcorrespondingSQLqueriess i,the 2020). Inpriorstudies,schemalinkinghasprimar-
goalistofine-tunealargelanguagemodelM us- ily been accomplished through in-context learn-
ingasetoftrainingdataT = {(q i,s i,D i)},where ing methods or implicitly during the fine-tuning
q i and s i represent the natural language question processforSQLgeneration(PourrezaandRafiei,
anditsassociatedSQLqueryondatabaseD i. The 2023;Caoetal.,2021;Guoetal.,2019;Xuetal.,
objectiveofsupervisedfine-tuningistominimize 2021). Inthiswork,wetreatschemalinkingasa
theempiricallossdefinedas: distincttaskandexplicitlyfine-tuneLLMstoiden-
tify relevant tables and columns when presented
with a natural language query. Given the train-
|T|
1 (cid:88) ingdatasetT = {(q ,s ,D )},weextractallofthe
min L(M∗(σ (q ,D ,s )), (1) i i i
f i i i
σ,M∗ |T| columnsandtablesusedintheSQLqueriesandcre-
i=1
ateanewdatasetofT = {(q ,T ,C ,D )}where
i i i i
where L is the loss function used to measure T andC representlistsoftablesandcolumnsused
i i
thedifferencebetweentheSQLqueriesgenerated in the SQL query s . The primary objective dur-
i
bythemodelandtheactual,correct(groundtruth) ing supervised fine-tuning for schema linking is
queries. Thefunctionσ determinestheformatting to minimize the empirical loss, as defined by the
f
ofthequestion,thedatabaseschema,andtheSQL followingequation:
queries. A key challenge during inference is that
wedonotknowinadvanceamongallofthetables
|T|
inside the database which tables are relevant to a 1 (cid:88)
min L(M∗(σ (q ,T ,C ,D )), (2)
s i i i i
DTS-SQL-2A42 σ,M∗ |T|
i=1Here,Lrepresentsthelossrelatedtothemodel’s Model EX EM
DAIL-SQL+GPT-4
nexttokenprediction,comparingthepredictedcol-
(Gaoetal.,2023) 86.6 -
umnandtablenameswiththeactualgroundtruth DIN-SQL+GPT-4
(PourrezaandRafiei,2023) 85.3 60
names.
DTS-SQL+DeepSeek7B
Ours 84.4 73.7
2.2.2 SQLGenerationFine-tuning
C3+ChatGPT+Zero-Shot
After identifying the appropriate tables for SQL (Dongetal.,2023) 82.3 -
RESDSQL-3B+NatSQL
generation,thenextstepistoutilizeamodelthat
(Lietal.,2023a) 79.9 72
constructstheSQLquerybasedonthequestionand DIN-SQL+CodeX
the schema of the correct tables. Since we have (PourrezaandRafiei,2023) 78.2 57
DTS-SQL+Mistral
alreadyidentifiedthepotentiallycorrecttablesus-
Ours 77.1 69.3
ingtheschema-linkingmodule,thereisnoneedto Graphix-3B+PICARD
(Lietal.,2023b) - 74
includealltablesintheinputfortheSQLgenera-
tionmodel. Incontrasttopreviousapproachesfor
Table2: Thecomparisonofdifferentmethodsontest
fine-tuning LLMs, we extract the relevant tables setofSpider.
fromthetrainingdatasetT = {(q ,s ,D )}corre-
i i i
spondingtothegroundtruthSQLqueries. Wethen
FlashAttentiontechniquesasdetailedin(Daoetal.,
fine-tunetheLLMwhileminimizingthefollowing
2022;Dao,2023).
lossfunction:
3.3 Datasets
|T| Weconductedourevaluationusingcross-domain,
1 (cid:88)
min L(M∗(σ g(q i,T i,s i)), (3) challenging Text-to-SQL datasets. Spider, was
σ,M∗ |T|
i=1 introduced by Yu et al. (2018) and includes 200
database schemas. Of these, 160 schemas are al-
The loss function is same as the loss function
located for training and development, while the
definedinSection2.1. Thisdecompositionofthe
remaining 40 are set aside for testing purposes.
Text-to-SQL training process allows LLMs to be
Our second dataset was Spider-Syn (Gan et al.,
trained with a singular objective. By segregat-
2021),whichmodifiestheoriginalSpiderdataset
ingtheschema-linkingandSQLquerygeneration
byreplacingschema-relatedwordswithsynonyms
tasks, we improve the training process, enabling
and removing explicit mentions that link natural
morefocusedandeffectivefine-tuning.
languagequeries(NLQs)tothedatabaseschema.
3 Experiments
3.4 Metrics
3.1 Models In our evaluation of text-to-SQL models, we uti-
Ourmethodology’sperformancewasassessedus- lizedexactsetmatchaccuracyandexecutionaccu-
ing two recent LLMs from distinct architectures. racy. The former involves comparing the compo-
These models are Mistral 7B (Jiang et al., 2023) nentsofSQLqueries, suchasselect, where, hav-
and DeepSeek 7B (DeepSeek-AI, 2024). The ing, group by, and order by clauses, focusing on
DeepSeekmodel,sharingsimilararchitecturewith the matching of columns and predicates without
theLLamamodelfamily(Touvronetal.,2023),has consideringtheorder. Thelatterdeterminesequiva-
beenpretrainedonanextensivedatasetcomprising lencebetweenamodel-generatedqueryandarefer-
2 trillion tokens and supports a sequence length encequeryiftheyproduceidenticalresultsacross
of4096. Mistral7B,althoughnotspecificallypre- variousdatabaseinstances.
trainedforcodegeneration,surpassesmanycoun-
3.5 Results
terpartsinitsscalecategory(Jiangetal.,2023).
3.5.1 Spidertestset
3.2 Hyperparameters
As depicted in Table 2, our method employing
ThetwoLLMsweretrainedonNvidiaTeslaA100 DeepSeek 7B, when tested on the Spider test
GPUs,employingabatchsizesof64and32witha dataset, achieves results comparable to state-of-
learning rate of1*e-5 and 5*e-5 respectively. To the-artopen-sourcemethodsintermsofexecution
enhance the training efficiency, we incorporated accuracyandexactsetmatchaccuracy.Model Tuning EX EM Model EX EM
Mistral7B FTTuning 71.9 70.9 Instructiontuningmethods
Mistral7B DTS-SQL 78.6 73.3 DTS-SQL+Mistral7B
Mistral7B Upperbound 86.6 80.7 (our) 78.6 73.3
DeepSeek7B FTTuning 82.1 69.0 DTS-SQL+DeepSeek7B
DeepSeek7B DTS-SQL 85.5 79.1 (our) 85.5 79.1
DeepSeek7B Upperbound 90.3 84.2 Llama27B
(Gaoetal.,2023) 66.7 63.9
Table3: PerformanceoftheLLMswithdifferenttuning Llama213B
methodsonSpiderdevset.FTstandsforFulltablesfine- (Gaoetal.,2023) 67.0 62.7
Promptingmethods
tuning,Upperboundperformanceistheperformance
DIN-SQL+GPT4
whichwecanachievewithaperfectschemalinking.
(PourrezaandRafiei,2023) 74.2 60.1
DIN-SQL+CodeX
(PourrezaandRafiei,2023) 69.9 57.2
3.5.2 Spiderdevset DAIL-SQL+GPT4
(Gaoetal.,2023) 84.4 74.4
In Table 3, we showcase the results of our two-
C3+GPT-3.5
stagefine-tuningmethodonthedevsetofSpider. (Dongetal.,2023) 81.8 -
Theperformanceiscomparedagainsttwodistinct
Table4: PerformanceofdifferentmethodswithLLMs
scenarios: firstly, a one-stage scenario where the
onthedevsetofSpider.
modelisfine-tunedonalltableswithoutemploy-
ing our two-stage approach, and secondly, a per-
Model Dataset EX PR RE
fectschemalinkingscenariowhereweprovidethe DeepSeek Spider 93.1 98.4 97.7
groundtruthtablestoourfine-tunedSQLgenera- Mistral Spider 91.1 97.5 97.8
DeepSeek Spider-SYN 87.6 94.6 94.7
tors. Thislatterscenarioisdenotedasthe’Upper
Mistral Spider-SYN 85.3 91.2 90.5
Bound’inthetable. Ourtwo-stagemodel’sperfor-
manceismeasuredbyinitiallyusingourfine-tuned Table 5: Performance of the schema-linker model on
SpiderandSpider-SYNdevsets. PRstandsforPreci-
schemalinkermodeltoidentifypotentiallyrelevant
sion,REisrecall,andEXisexactsetmatchaccuracy.
tables, which are then provided as context to the
SQLgeneratormodel.
In Table 4, we offer a detailed comparison be-
4 Discussion
tween our method and various other baseline ap-
proaches. For the baselines, we selected diverse
While our two-step approach has achieved state-
methods from different families of approaches
of-the-art results on the development set of Spi-
that are using LLMs and are available as open
deranddemonstratedcomparableperformanceto
source. Ourtwo-stagedecomposedapproachwith
larger models like GPT-4 on the test set, there
DeepSeek7Battainedstate-of-the-artperformance
isstillsignificantroomforimprovement,particu-
ontheSpiderdevelopmentset,surpassingallprevi-
larlyfortheschema-linkingmodels. Currently,our
ousmethodsthatutilizedpromptingtechniquesand
schema-linkingmodelsachieveroughly90%exact
fine-tuning. Additionally, the results of our two-
setmatchaccuracy. However,asnotedinTable3,
stage method on Spider-SYN dataset is provided
thesubstantialgapbetweentheupperboundperfor-
intheappendixAsection.
manceoftheSQLgeneratorandthatofDTS-SQL
3.5.3 Schema-linkingPerformance callsforfurtherresearchintotheschema-linking. .
AsdiscussedinSection2,ourapproachemploys
twoLLMs: oneforschemalinkingandanotherfor 5 Conclusion
SQLquerygeneration. Theschema-linkingmodel
playsapivotalroleinourpipeline,asinaccuracies Before our research, small open-source models
in table detection could hinder the SQL genera- laggedbehindlargeproprietarymodelsinperfor-
tor’sabilitytoformulatethecorrectSQLqueries. mance on the text-to-SQL task. Our two-stage
Wefine-tunedtwomodels,basedontheDeepseek fine-tuningapproachbreaksdownthetaskintotwo
and Mistral models, for schema linking. Evalua- simplercomponents,enablingsmallopen-source
tionmetrics,includingexactsetmatch,precision, models to rival larger ones. Subsequent efforts
andrecall,wereusedtoassesstheirperformance. couldfocusonenhancingtheperformanceofthese
Detailed information about these models on two stagesandexploringimprovedmethodsfortrans-
distinctdatasetscanbefoundinTable5. ferringtheoutputofonestagetothenext.Limitations Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,
andChristopherRé.2022. FlashAttention: Fastand
This paper has placed its primary emphasis on memory-efficientexactattentionwithIO-awareness.
enhancing the performance of both small large InAdvancesinNeuralInformationProcessingSys-
language models for Text-to-SQL task. How- tems.
ever,thereremainsscopeforfurtherinvestigation
DeepSeek-AI. 2024. Deepseek llm: Scaling open-
andcomparisonofvarioustechniquesforschema- source language models with longtermism. arXiv
linking. Exploringapproacheslikeretrievalmeth- preprintarXiv:2401.02954.
ods or in-context learning when applied in con-
XuemeiDong, ChaoZhang, YuhangGe, YurenMao,
junctionwithlargermodelssuchasGPT-4forthe
YunjunGao,JinshuLin,DongfangLou,etal.2023.
schema-linkingtaskcouldyieldvaluableinsights C3: Zero-shot text-to-sql with chatgpt. arXiv
intoidentifyingthemosteffectivemethodologies preprintarXiv:2307.07306.
forschema-linking.
Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew
Purver, John R. Woodward, Jinxia Xie, and Peng-
EthicsStatement
sheng Huang. 2021. Towards robustness of text-
to-SQL models against synonym substitution. In
Inthispaper,weplaceastrongemphasisonthesig-
Proceedingsofthe59thAnnualMeetingoftheAsso-
nificanceofethicalconsiderationsineveryaspect
ciationforComputationalLinguisticsandthe11th
ofourresearch,fromitsinceptiontoitspresenta- InternationalJointConferenceonNaturalLanguage
tion. Wewholeheartedlycommittoadheringtothe Processing (Volume 1: Long Papers), pages 2505–
2515, Online. Association for Computational Lin-
ACLEthicsPolicyandupholdingethicalprinciples
guistics.
andguidelinesthroughoutourresearchjourney.
Wehavetakenproactivemeasurestominimize Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun,
anypotentialbiasesordiscriminatoryelementsin Yichen Qian, Bolin Ding, and Jingren Zhou. 2023.
Text-to-sql empowered by large language mod-
our research design, data selection, and interpre-
els: A benchmark evaluation. arXiv preprint
tation of results. Our dedication to transparency,
arXiv:2308.15363.
precision,andfairnessinreportingourfindingsis
unwavering,andwehavedulyacknowledgedand Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-
GuangLou,TingLiu,andDongmeiZhang.2019. To-
citedtheworkofotherstogivepropercredit.
wardscomplextext-to-sqlincross-domaindatabase
Byincorporatingthisethicsstatement,weaim
with intermediate representation. arXiv preprint
tounderscoreourunwaveringcommitmenttocon- arXiv:1905.08205.
ductingresearchwithintegrity,respectingethical
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
principles,andcontributingresponsiblytothead-
sch,ChrisBamford,DevendraSinghChaplot,Diego
vancementofknowledgeinourfield. delasCasas,FlorianBressand,GiannaLengyel,Guil-
laumeLample,LucileSaulnier,etal.2023. Mistral
Acknowledgements 7b. arXivpreprintarXiv:2310.06825.
References HarrisonLee,SamratPhatale,HassanMansoor,Kellie
Lu, Thomas Mesnard, Colton Bishop, Victor Car-
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
bune, and Abhinav Rastogi. 2023. Rlaif: Scaling
Askell, AnnaChen, NovaDasSarma, DawnDrain,
reinforcementlearningfromhumanfeedbackwithai
StanislavFort,DeepGanguli,TomHenighan,etal.
feedback. arXivpreprintarXiv:2309.00267.
2022. Trainingahelpfulandharmlessassistantwith
reinforcementlearningfromhumanfeedback. arXiv WenqiangLei,WeixinWang,ZhixinMa,TianGan,Wei
preprintarXiv:2204.05862. Lu, Min-Yen Kan, and Tat-Seng Chua. 2020. Re-
examiningtheroleofschemalinkingintext-to-sql.
Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao,
InProceedingsofthe2020ConferenceonEmpirical
Su Zhu, and Kai Yu. 2021. Lgesql: line graph en-
MethodsinNaturalLanguageProcessing(EMNLP),
hancedtext-to-sqlmodelwithmixedlocalandnon-
pages6943–6954.
localrelations. arXivpreprintarXiv:2106.01093.
PaulFChristiano,JanLeike,TomBrown,MiljanMar- HaoyangLi,JingZhang,CuipingLi,andHongChen.
tic, Shane Legg, and Dario Amodei. 2017. Deep 2023a. Resdsql: Decoupling schema linking and
reinforcementlearningfromhumanpreferences. Ad- skeleton parsing for text-to-sql. In Proceedings of
vancesinneuralinformationprocessingsystems,30. theAAAIConferenceonArtificialIntelligence,vol-
ume37,pages13067–13075.
Tri Dao. 2023. FlashAttention-2: Faster attention
withbetterparallelismandworkpartitioning. arXiv JinyangLi,BinyuanHui,ReynoldCheng,BowenQin,
preprintarXiv:2307.08691. ChenhaoMa,NanHuo,FeiHuang,WenyuDu,LuoSi,andYongbinLi.2023b. Graphix-t5: Mixingpre- Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
trainedtransformerswithgraph-awarelayersfortext- DongxuWang,ZifanLi,JamesMa,IreneLi,Qingn-
to-sqlparsing. arXivpreprintarXiv:2301.07507. ing Yao, Shanelle Roman, et al. 2018. Spider: A
large-scalehuman-labeleddatasetforcomplexand
Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi cross-domainsemanticparsingandtext-to-sqltask.
Yang,BowenLi,BailinWang,BowenQin,Rongyu arXivpreprintarXiv:1809.08887.
Cao, Ruiying Geng, et al. 2023c. Can llm already
serveasadatabaseinterface? abigbenchforlarge- DanielMZiegler,NisanStiennon,JeffreyWu,TomB
scaledatabasegroundedtext-to-sqls. arXivpreprint Brown, Alec Radford, Dario Amodei, Paul Chris-
arXiv:2305.03111. tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
guage models from human preferences. arXiv
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and preprintarXiv:1909.08593.
Hannaneh Hajishirzi. 2021. Cross-task generaliza-
tionvianaturallanguagecrowdsourcinginstructions.
arXivpreprintarXiv:2104.08773.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
CarrollWainwright,PamelaMishkin,ChongZhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
2022. Training languagemodelsto followinstruc-
tions with human feedback. Advances in Neural
InformationProcessingSystems,35:27730–27744.
Mohammadreza Pourreza and Davood Rafiei. 2023.
Din-sql: Decomposed in-context learning of
text-to-sql with self-correction. arXiv preprint
arXiv:2304.11015.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
DarioAmodei,andPaulFChristiano.2020. Learn-
ingtosummarizewithhumanfeedback. Advances
inNeuralInformationProcessingSystems,33:3008–
3021.
RomalThoppilan,DanielDeFreitas,JamieHall,Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
2022. Lamda: Languagemodelsfordialogapplica-
tions. arXivpreprintarXiv:2201.08239.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada,
Shengyi Huang, Leandro von Werra, Clémentine
Fourrier, Nathan Habib, et al. 2023. Zephyr: Di-
rect distillation of lm alignment. arXiv preprint
arXiv:2310.16944.
Sanh Victor, Webson Albert, Raffel Colin, Bach
Stephen,SutawikaLintang,AlyafeaiZaid,Chaffin
Antoine, Stiegler Arnaud, Raja Arun, Dey Manan,
etal.2022. Multitaskpromptedtrainingenableszero-
shottaskgeneralization. InInternationalConference
onLearningRepresentations.
KuanXu,YongboWang,YongliangWang,ZujieWen,
and Yang Dong. 2021. Sead: End-to-end text-to-
sqlgenerationwithschema-awaredenoising. arXiv
preprintarXiv:2105.07911.Model Tuning EX EM
Mistral7B FTTuning 67.0 63.9
Mistral7B DTS-SQL 71.1 64.6
Mistral7B Upperbound 81.9 74.5
DeepSeek7B FTTuning 70.4 56.6
DeepSeek7B DTS-SQL 76.2 68.9
DeepSeek7B Upperbound 85.5 78.1
Table6: PerformanceoftheLLMswithdifferenttuning
methods on Spider-SYN dev set. FT stands for Full
tablesfinetuning,Upperboundperformanceistheper-
formancewhichwecanachievewithaperfectschema
linking.
A Appendix Figure 2: The prompt used for Schema linking. The
databaseschemaiswhereweputthetablesrepresenta-
A.1 Spider-SYNdataset tions.
Toassesstheefficacyofourproposedmethod,we
evaluateditsperformanceonthedevelopmentset
ofSpider-SYN.AlthoughSpider-SYNpossessesa
distincttrainingset,weoptedtotestourfine-tuned
models directly on its development set, without
anyadditionaltuningontheSpider-SYNtraining
set. Thesameperformancegainisobservedonthis
dataset (see Table 6) even though the model was
notdirectlytrainedinthisdataset.
A.2 Prompt
Inconductingallourexperimentsonbothmodels,
weadheredtoastandardizedpromptformattoen-
sureconsistencyandfacilitatereliablecomparisons.
Thechosenpromptformatiswell-establishedasef-
fectiveintheText-to-SQLdomain,asdemonstrated
inpriorresearchbyGaoetal.(2023). Inthisfor-
mat, we provided information about the foreign
key constraints, primary keys, and column types.
Furthermore,toguidethemodelsinunderstanding
howdataisstoredwithinthedatabase,ourprompt
incorporatedthreesamplerows,showcasingdata
entries.
Thespecificpromptusedforourexperimentsis
asfollows:
Figure3:Asampletablerepresentation. Allofthetable
inadatabasearerepresentedasaboveandusedinthe
prompts.
Figure 1: The prompt used for SQL generation. The
databaseschemaiswhereweputthetablesrepresenta-
tions.