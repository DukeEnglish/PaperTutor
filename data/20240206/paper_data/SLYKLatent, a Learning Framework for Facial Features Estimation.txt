JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 1
SLYKLatent, a Learning Framework for efficient
Facial Features Estimation
Samuel Adebayo, Member, IEEE, Joost C. Dessing, and Sea´n McLoone Senior, IEEE,
Abstract—Inthisresearch,wepresentSLYKLatent,anovelap- Within the HRI field, the predominant approach has been
proach for enhancing gaze estimation by addressing appearance to concentrate on deciphering and encoding human verbal
instability challenges in datasets due to aleatoric uncertainties,
cues to determine an appropriate response from a robot.
covariant shifts, and test domain generalization. SLYKLatent
However, in this framework, non-verbal cues from humans
utilizes Self-Supervised Learning for initial training with facial
expression datasets, followed by refinement with a patch-based arefrequentlyneglectedandnotgivensufficientconsideration.
tri-branch network and an inverse explained variance weighted Understandably,sinceHRIisareplicationofhuman-to-human
training loss function. Our evaluation on benchmark datasets interaction, it is reasonable to want to emulate a large part
achieves an 8.7% improvement on Gaze360, rivals top MPI-
of the human communication process for a HRI framework
IFaceGazeresults,andleadsonasubsetofETH-XGazeby13%,
[6]. Human-to-human interactions rely not only on verbal
surpassingexistingmethodsbysignificantmargins.Additionally,
adaptability tests on RAF-DB and Affectnet show 86.4% and cues, but also on posture, gesture cues, and facial cues, all
60.9% accuracies, respectively. Ablation studies confirm the of which may carry information pertaining to the executed
effectiveness of SLYKLatent’s novel components. This approach task or action (i.e. intention) [6], [7]. Humans can achieve
has strong potential in human-robot interaction.
this by understanding and interpreting observable patterns in
Index Terms—Human-Machine Interaction, Facial feature the available information from the aforementioned cues. Here
extraction, Gaze estimation, Facial Expression Recognition, we will focus on a subset of these cues: facial cues.
Human-Robot Interaction, Intention inference, Computational
Recent developments in HRI systems proposed the fusion
Intelligence.
of two or more visual cues, such as hand, eye, and interacting
object, for inferring human intention [4], [54], [55] Other
I. INTRODUCTION methods also suggest facial cues as a way of encoding ex-
pressiveness [8]–[11]. While the combination of these visual
A. Background and Motivation
signals to classify intention is not within the scope of this
HUMAN-ROBOTinteraction(HRI)isaninterdisciplinary research, it is likely that the fusion of facial features such
field that focuses on the understanding, modelling, and as gazes and facial expressions with other obtainable visual
design of interactions between humans and robots. HRI plays cues would improve human intention inference in an HRI
a critical role in many areas such as education, healthcare, framework [6], [12].
manufacturing, and entertainment. Irrespective of the form of Facial cues can be considered perceptible physiological
interaction,thegoalofanyHRIframeworkistocreatenatural characteristicslocatedonthehumanfacethatcanbeanalysed
and effective interactions between humans and robots, where and converted into facial expressions, gestures, and gaze
the robot can understand and respond to the human’s needs, estimation, thereby facilitating additional forms of non-verbal
intentions, and goals. [1]–[3]. communication. The computation tasks involved in vision-
One crucial aspect of human-robot interaction (HRI) is based gaze and facial expression estimation exhibit areas of
the ability to infer human intention, that is the capability overlap,asbothentailtheexaminationoffacialimages,either
of a robot to predict and deduce a human’s future actions in their entirety or in part. Consequently, advancements in
before or during task execution [4], [52], [53]. This is a facial feature estimation have the potential to yield enhance-
vital component of HRI frameworks, as it enables the robot ments in gaze estimation capabilities.
to compute and anticipate human intentions at every stage Gaze estimation refers to the process of determining the
of interaction, leading to more efficient, safe, and effective directionofaperson’sgaze-itisadistinctaspectoffacialfea-
interactions [5]. Therefore, there is a need to identify efficient tureestimation,andhasreceivedconsiderableattentioninHRI
methods to capture, analyze, and process the observable cues research [13], [14], [30]–[32]. Gaze direction is considered a
required to infer human intention. Additionally, it requires crucial cue for human attention and intention, because of the
the development of advanced frameworks and techniques that tight link between attention, intention, and gaze control [15]–
can optimally represent human perspectives, activities, and [17].Inaddition,gazedirectionismoreuniversallyunderstood
intentions [5]. thanverbalcues,makingitapplicableacrossdifferentcultures
regardless of language. However, despite recent progress in
Code Repository will be at https://github.com/exponentialR/SLYKLatent. utilizing learning-based techniques for extracting facial fea-
The authors are with the Centre for Intelligent Autonomous Manufacturing tures, the practical implementation of gaze estimation often
Systems,Queen’sUniversityBelfast,NorthernIreland,UnitedKingdom.
faces limitations due to the model’s inability to accurately
E-mail:{sadebayo01,j.dessing,s.mcloone}@qub.ac.uk
******** infer useful facial feature points from visual appearance in
4202
beF
2
]VC.sc[
1v55510.2042:viXraJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 2
r ce ha al l- ll eif ne ges sce pn oa sr eio ds. byTh ais ppl eim ari at nat ci eon unp cri em rta ar inil ty iesari as ne ds f thro em neth ede eye tfeLhctaP
Downstream finetuning
f
t
a
o
dho
n
f
ie
rr
d
etd chvo
o
tea
im
tr ohi
f
naa
e
a
.ti
r
cin
o
e
Ofng
na
ase
c
nn
tti
d
hone err
s
sla
i
u
ol gi
t
b
tz
h
h
hsa
t
a
eeit
t
rn
qio
g
u
hcn
e
aa.
c
n
nnoA
t
dln
y
,p
id
np
di
fl
ite
omi
ua
o
mer pna
n
aasn
c
ic,
nc
e
tfe
a gttc
hu
ehi
en
na
ec
el
ee
rvp
s
ar
i
tt
o
ls
ia
is
mu
zi en
aa
a,t
tl
ty
ie
i
ooax
np nppe
pr
rr
e
o
eeta
s
ffa
esi rn
i
ra
go
ss
n
an
zct tso
oee,
egam
i tupnI
LOYm - B reyal kcenelttoB
reyalreyal
kcenelttoB
kcenelttoB
Gaze vector
t dh ae tam fro od mel’ ds ivc ea rp sa eci dty omto ainp se ,rf do ir sm tince tff fe rc ot mive tl hy eo tn rain ne iw ng, u dn as taee in
t
Self-supervised pretraining eye thgiRhctaP
was exposed to.
The task of gaze estimation in the field of human-robot Fig. 1. High-Level view of SLYKLatent. The framework consists of two
network modules; a self-supervised pretraining module and a patch module
interaction (HRI) is challenging due to various sources of
network.
uncertainty present in real-world scenarios. One of the major
sourcesofuncertaintyisheteroscedasticnoise,whichrefersto
variationsinthelevelofnoiseacrossdifferentsamples.Inthe besignificantlyimprovedifarichlatentrepresentationcanbe
context of gaze estimation, this type of noise can arise from drawnfromfacialimagesandindividual(leftorright)intrinsic
variations in lighting conditions, facial expressions, and other eye patch features. Hence, this study proposes a new learning
factorsthataffecttheappearanceofthefaceandgazedirection framework,SLYKLatent,forefficientfacialfeatureestimation,
[18], [19]. This leads to increased errors in the prediction of with a specific focus on gaze estimation in the context of
gaze direction. human-robot interaction. Our proposed framework addresses
Another significant challenge in gaze estimation is equiv- the challenges of heteroscedastic noise, domain adaptation,
ariance shift, which refers to variations in the position, ori- and equivariance shifts in facial feature estimation tasks. The
entation, and scale of the face in the image. These changes main contributions of this study are:
can cause the associated labels to vary, resulting in a domain • A framework that utilizes self-supervised learning incor-
adaptation problem when dealing with unseen data and un- porating a global branch and local branch to extract rich
accounted variations in the feature space. Most deep neural representation, and transfer feature learning to enhance
networks exhibit invariance but lack equivariant capabilities, generalizability, uncertainty robustness, noise tolerance,
indicating that they can correctly predict or estimate the and ability to account for unobserved facial features
gaze point regardless of the face’s position in the image. in facial feature estimation. To introduce appearance
However,theyaresensitivetochangesinfaceorientation[20]. equivariance, we incorporate transformations in the self-
Consequently, the learned visual representations have a lower supervised pretraining stage, which enables the model to
ratio of equivalence-to-shift unless the network architecture learnfromdiverseappearancesintheinputdata,reducing
inherently computes equivariance shifts [20]. its reliance on labelled data. The multi-head attention
Toaddressthesechallenges,arobustandefficientapproach layerallowsthemodeltoattendtomultiplefacialfeatures
to gaze estimation is needed to handle domain adaptation and simultaneously,whilethePatchModuleNetworks(PMN)
appearance uncertainties. In this paper, we propose the Self helpextractvaluablefeaturesfromtheeyesindependently
Learn Your Key Latent (SLYKLatent) features framework, a during downstream feature finetuning.
combination of self-supervised learning and transfer feature • We propose a novel modification to the Bootstrap Your
learning, that effectively extracts important features for gaze Own Latent (BYOL) self-supervised learning framework
estimation. By using a crowdsourced dataset that includes [17] that includes a new self-supervised loss function.
a diverse range of participants from different backgrounds, Our modification introduces a new loss function that
facial expressions, and non-facial images, our framework is incorporates the negative cosine similarity between the
able to adapt to different lighting conditions and appearance predictions and feature representations of two different
variations. Additionally, by introducing equivariance through augmented views of the same image. Additionally, we
the use of appearance transformations in self-supervised pre- introduceanovellossfunctionforgazeestimationduring
training, our framework extracts a rich latent representation downstream fine-tuning that addresses uncertainties in
of the face that is robust to different views and positions of the gaze prediction model by incorporating facial feature
the face. Our approach is evaluated on several datasets and is weights via explained variance of the gaze features.
shown to outperform existing methods while being robust to This approach results in improved accuracy and is a
uncertainties and image appearances [15]. unique contribution to the literature. Our modifications
represent a significant step forward in the development
ofeffectivedeeplearningmodelsforgazeestimationand
B. Purpose and Contribution
facialexpressionrecognition,demonstratingthepotential
Since a rich latent embedding can be extracted given a to improve the performance of deep learning models on
learning function (deep neural network) that solves a set of these tasks.
predefined equations (appearances) on a facial image via self- • A proof of concept of the effectiveness of the proposed
supervised pretraining [22], we hypothesize that the perfor- framework through the demonstration of state-of-the-art
mance of facial cue tasks (gaze estimation in particular) can performance on gaze estimation.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 3
• A comprehensive set of ablation studies that demonstrate discuss how these methods can be improved to better handle
the effectiveness of the components in the proposed the challenges of heteroscedastic uncertainty and equivariance
framework, as well as an evaluation of results using the shifts.
explained variance metric.
In summary, our proposed framework SLYKLatent offers B. Learning-based Appearance Gaze Estimation
a solution to the challenges of facial feature estimation by
Given an image, s from a sample space D a learning-
efficiently addressing the problem of achieving domain gen-
based appearance facial cues estimation approach seeks to
eralization and robustness to appearance uncertainties while
compute and extract useful deep features from image s
providingimprovedperformanceongazeestimationandfacial
to solve a regression or classification task. As with many
expression recognition tasks.
other vision-based tasks, the learning-based gaze estimation
approach often requires a large repertoire of facial image
II. BACKGROUND datasets.Basedonthetypeoflearningframeworkuseditcould
A. Gaze Estimation also require appropriate annotations. Different learning-based
methodshavebeenproposed,butthemostpopularframeworks
Recentadvancementsinthefieldoffacialfeatureestimation
have employed supervised learning. These methods often
havebeendrivenbytheavailabilityoflargedatasets,improve-
leverage convolutional neural networks (CNNs) to extract
ments in deep learning techniques, and a growing interest in
visual features from facial images and map them to gaze
utilizing facial cues for various applications [23], [24]. Facial
vectors. Common CNN architectures used in gaze estimation
feature estimation encompasses several tasks, including age
include DenseNet [66], ResNet [65], and VGGNet [67].
estimation, facial recognition, facial detection, and emotion
The first CNN-based gaze prediction network was intro-
estimation [25]. In this section, our primary focus is on
duced in [68]. They proposed the use of eye patches (feature
facial feature estimation within the scope of Human-Robot
space)andgazevectorannotation(targetlabel)tomapaligned
Interaction(HRI),withaspecificemphasisongazeestimation.
eye features to the corresponding gaze vector. An improved
Facial feature estimation is an important aspect of human-
version, GazeNet was later introduced [33]. This consisted
robot interaction, as it enables robots to ”understand” and re-
of a CNN-based architecture that aims to learn the low
spond to human needs, intentions, and goals. Gaze estimation
and high-level appearance and map learned features to gaze
isaspecifictaskwithinfacialfeatureestimationthathasbeen
vectorfeaturesfromafull-faceimageandcorrespondinghead
widely studied in the context of HRI [12]–[17]. Two distinct
pose annotation. A Spatial Weight CNN was introduced in
methods have been proposed for estimating gaze points from
[34] and aims to eliminate noise by allocating more weights
facial images: model-based and appearance-based methods
to important regions of a face image and low weights to
[26]–[28].
others that contribute to noise, which led to an improvement
Model-based methods for gaze estimation can be further
in the learned target gaze vector. Seeking to improve gaze
subdivided into 3D model methods and 2D methods. The 3D
estimation accuracy and enhance generalizability, Bayesian-
model approach regresses gaze points from pre-built person-
enabled CNNs were introduced for gaze estimation [35]. This
dependent 3D models of the eyes. However, these approaches
approach simply uses ensembles of point-estimate models,
are often limited by the need for calibration for each subject,
such that gaze regression can be carried out on more than a
aswellastheneedforaccurate3Dmodelsoftheeyes.Onthe
single set of parameters, hence, the framework benefits from
other hand, 2D model methods do not require person-specific
several features learned from each of the models. To encode
calibration, allowing gaze points to be directly regressed from
rich visual features extracted by CNNs and at the same time
features of 2D models of the eyes [26]–[28].
benefit from the temporal details in non-static image data, a
Appearance-based methods for gaze estimation can also be
combination of a CNN-based network architecture and other
subdividedintotwocategories:theconventionalapproachand
ArtificialNeuralNetworks(ANN)wasproposed.Examplesof
the learning-based approach. Conventional appearance-based
these methods include the use of Recurrent Neural Networks
approaches regress the human gaze from the raw facial image
(RNNs)andLongShort-termMemory(LSTM)networks[28],
and generally require algorithms for feature extraction and
[36], [37], [39]. A comprehensive survey of learning-based
a regression function that maps extracted features to gaze
appearance gaze estimation methods can be found in [29].
points. In contrast, the learning-based approach replaces both
the feature extraction and regression functions with a deep
C. Gaze Estimation using Facial Patches
neural network [26]–[28].
Inrecentyears,deeplearning-basedapproacheshaveshown The use of facial patches, particularly eye patches, has
promising results in gaze estimation, as they have proven been a prevalent approach in gaze estimation research, as
to be robust to variations in lighting, pose, expression, and the eyes contain rich information about the gaze direction
other factors that can affect the appearance of the face and of an individual [29], [40]. One popular approach for gaze
the gaze direction [26]–[29]. However, the performance of estimation using facial patches is to extract features from the
these methods is still limited by the need for large annotated eye patches, concatenate them, and then feed them into a
datasets, as well as the challenges of generalization to unseen learning function [14], [29].
data. In the next section, we will review the current state- Itracker [40] introduced the concept of fusing features of
of-the-art in learning-based appearance gaze estimation, and left,right,andfaceregionstoestimatecamera-to-eyedistance.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 4
In contrast, [50] used the same approach but to infer gaze face,makingitdifficulttoaccuratelyestimategazevectors.To
vectors from full-face images. Similarly, [42] trains ensemble addressthischallenge,recentworkhasfocusedondeveloping
networks of eye patches via the sums of supervised losses methods that are resistant to these uncertainties and variations
from the output of each eye network. It is also important in image appearances.
to note that while the use of facial patches can improve the Recently, a method called Swap Affine Transformations
performance of gaze estimation models, it is not a complete (SWAT)hasbeenproposedby[42].ItbuildsontheSwapping
solution on its own. Other factors such as model architecture, Assignments between multiple Views of the same image
featureextractiontechniques,anddatasetbiascanalsogreatly (SwAV) SSL framework [47] by introducing modifications
influence the overall performance of gaze estimation models. to the pretext task and augmentation techniques that support
Additionally, as gaze estimation is a challenging task due equivariance shifts in the input image. While this method has
to the high variability of head poses, lighting conditions, and been successful in mitigating the effects of unaccounted-for
facial expressions, it is important to consider the limitations equivariance properties in gaze estimation, our approach is
of the data and take into account factors such as appearance different in that we use a downstream finetuning technique
uncertainty. Various datasets have been used to train and thattakesintoaccountboththefeatureslearnedfromtheSSL
evaluategazeestimationmodelsusingfacialpatches.Themost pretraining process as well as features from the eye patches.
commonly used datasets are MPIIGaze [33], Gaze360 [39], Additionally, our SSL framework is based on a modified
EyeDiap,andETHX-gaze[53]allofwhichcontainimagesof version of BYOL [17], while SWAT is built on the SWAV
people looking in different directions with different lighting SSL framework. By combining the features learned from the
conditions, head poses, and facial expressions. SSL pretraining with eye patch features, our approach is able
to achieve better results than previous methods in terms of
gaze estimation accuracy.
D. Self-supervised Learning for Improving Gaze Estimation
Similar to our approach, the work by [15] proposed the
Despite the vast improvement in the performance of su- use of an eye patch module network to account for intrin-
pervised learning, it is usually limited by the requirement sic eye features in their Modulation Adaptive Eye Guiding
for large curated data and corresponding annotations. In the Network (MANet) framework. However, there are several key
context of face estimation, it is often expensive to set up, differences between our approach and MANet. Firstly, while
collect,clean,andanalyze.Ontheotherhand,SSLrequiresno they modified the BYOL framework at the representation
data annotation nor does it require the complicated setup of a level, our approach modifies the augmentation view, adds
laboratory environment [21], [22], [45]–[47]. Freely available an attention layer of 8 heads, and incorporates an added
unlabelled image-based data on the internet can be curated, eye patch network for downstream finetuning. Secondly, our
therefore eliminating the heavy lifting of data collection. approachachievedabetterresultwhencomparedwithMANet.
SSLhasemergedasaprominentalternativetoconventional Lastly,ourframeworkaimstoenrichandendowtheextracted
supervised learning in numerous vision tasks, encompassing facial representation with unaccounted appearance properties
gaze estimation [21], [22], [45], [46]. The primary benefit of the aligned face and features learned from the eye patches,
of SSL lies in its elimination of manual data annotation, whereas the main focus of MANet is on adapting eye patches
enhancingcost-effectivenessandefficiency.Furthermore,SSL for guiding gaze regression.
employs pretext tasks, which are auxiliary tasks designed Previous methods for gaze estimation have used supervised
to learn meaningful data representations without requiring or self-supervised approaches to fuse features from left, right,
explicitlabels[21],[22],[45],[46].Theserepresentationscan and face regions or to employ ensemble networks of eye
subsequently be fine-tuned for downstream applications like patches. However, these approaches can lead to sub-optimal
gaze estimation. performance in challenging conditions such as head pose
Recent work has explored the use of SSL for gaze esti- variations and lighting changes. To address these limitations,
mation, with a focus on learning latent representations of the we propose a novel approach that separately extracts features
full face or eye patches using methods such as Variational from the aligned full-face image via a self-supervised pre-
AutoEncoders (VAE) and Generative Adversarial Networks trainedmodelandaResNetencoder,aswellasfromfacialand
(GANs) [40]–[42]. These approaches have demonstrated the eyepatchesviaanensembleofbottlenecklayers.Bydoingso,
effectiveness of SSL in capturing universal features for gaze ourapproachcanbettercapturethedependenciesbetweeneye
redirection.However,thesemethodstypicallyfusethefeatures patches and full-face images and utilize an important visual
of the face and eyes, making it difficult to disentangle the representation of the full-face image for final gaze regression.
contributions of each. Our approach, SLYKLatent, aims to Furthermore, we employ self-supervised pretraining to reduce
overcomethislimitationbyseparatelyextractingfeaturesfrom reliance on labelled data and improve performance in the face
the full face and eye patches using self-supervised pretraining of appearance uncertainty.
and then fusing them during downstream finetuning. Other In the following sections, we describe our approach in
challenges in gaze estimation are the presence of various more detail, including the use of appearance transformations
sources of uncertainty and variations in image appearances. and multi-head attention mechanisms. We also present exper-
Theseinclude,butarenotlimitedto,lightingconditions,head imental results to demonstrate the effectiveness of our ap-
pose, facial expressions, and eye occlusion. These factors can proach, comparing it with existing methods on the MPIIGaze,
lead to large variations in the appearance of the eyes and Gaze360,andETHX-gazedatasets,whicharecommonlyusedJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 5
for evaluating gaze estimation models. are beneficial in mitigating appearance uncertainties. By ad-
dressing these challenges, the modified architecture is better
equipped to tackle the challenges of downstream finetuning,
III. THESLYKLATENTFRAMEWORK
leading to an overall improvement in performance.
A. mBYOL
2) ModificationoftheRepresentationlayer: Weintroduced
The SLYKLatent framework is designed to be able to anovel,local-globalarchitectureintothemBYOLframework.
tackle the challenges posed by uncertainties, invariance, and The modified model includes two separate local branches that
positional shifts that may be encountered in novel domains. independently process the left and right eye images of a face.
To do this, it makes use of a modified BYOL [17] (mBYOL) Each local branch consists of three Convolutional layers with
architecture for self-supervised feature extraction and super- 32, 64, and 128 filters, respectively, each followed by batch
vised transfer feature fine-tuning. The mBYOL architecture normalization and ReLU activation [61]. A global average
is a variant of the Bootstrap Your Own Latent (BYOL) pooling layer and a fully connected layer with 52 outputs are
architecture, a self-supervised learning framework comprising applied to the output of the last Convolutional layer. These
twoparallelbutasymmetricalneuralnetworksknownasonline local branches work in parallel to extract local features from
and target networks. each eye.
The global branch uses a configurable backbone Inception-
a𝑢𝑔𝑚𝑒𝑛𝑡𝑒𝑑v𝑖𝑒𝑤 r𝑒𝑝𝑟𝑒𝑠𝑒𝑛𝑡𝑎𝑡𝑖𝑜𝑛 p𝑟𝑜𝑗𝑒𝑐𝑡𝑖𝑜𝑛 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛 ResNet-V2 [63] model pre-trained on ImageNet. The global
branch processes the global face image and outputs a feature
𝑡 𝑣 𝑓𝜃 yθ 𝑔𝜃 𝒵𝜃 𝑞𝜃 𝑞𝜃(𝒵𝜃) 𝑜𝑛𝑙𝑖𝑛𝑒 vector.
To emphasize the importance of different regions in the
𝑥
mBYOL𝑙𝑜𝑠𝑠
global and local features, we also introduce multi-head at-
𝐼𝑛𝑝𝑢𝑡𝑖𝑚𝑎𝑔𝑒
𝑡′ 𝑣′ 𝑓𝜉 𝑦′𝜉 𝑔𝜉 𝓏′𝜉 𝑠𝑔 𝑠𝑔(𝓏 ′ 𝜉 ) 𝑡𝑎𝑟𝑔𝑒𝑡 tention mechanisms to each branch. An 8-head attention layer
is applied to the output of the global backbone and to each of
the local branches, allowing the model to selectively focus on
Fig. 2. The Modified Bootstrap Your Own Latent (mBYOL) Architecture.
the most relevant features in the input data.
The Architecture is made up of two parallel asymmetrical networks, the
target network τ′ and the online network τ. Each network is made up of Finally, the features from the local and global branches are
3 essential stages: augmented view v; representation stage y (this is where concatenated to form a combined representation. This repre-
imageembeddingsarecomputed),and;projectionlayerz.Inadditiontothis,
sentationthenpassesthroughtheprojectioninboththeonline
the online network includes a prediction stage. The mBYOL loss computes
theNegativeCosinesimilaritiesbetweentheonlineandtargetnetwork. and target networks. The online network further processes the
representationthroughapredictionhead,similartotheoriginal
To create the mBYOL framework, we made some changes BYOL framework [17]. This modified architecture, therefore,
to the BYOL architecture, in the augmentation view section, learns rich and diverse representations by capturing both the
the representation layer, and the loss computations. local eye-specific features and the global face features in a
self-supervisedmanner,whichleadstoimprovedperformance
Affine Flip Gauss noise Color Jitter Cutout in downstream tasks, as will be detailed next.
3) Architecture details of mBYOL: In the mBYOL ar-
chitecture, the encoder f is formed by a combination of
θ
global and local branches as detailed in Section III-A2. The
concatenated features from these branches are then passed
Original image through the projection head, a Multi-Layer Perceptron (MLP)
Grayscale Invert Guass blur Rotation Crop Transformed Images with dimensions (1536, 1024, 1024), using ReLU activation
[61]. This head refines the representation for the task of
Fig.3. TheAugmentationviewappliedonmBYOL. predicting the target view in the contrastive learning setup.
Online Network: The online network encompasses the
1) Augmentation View: We adjusted the augmented view complete architecture described above, including the global
by randomly applying a set of transformation sets in a spe- and local branches, multi-head attention, and the projection
cific order. Each transformation has a probability value p and prediction heads. Following the projection head, the
that determines if it will be included in the data samples prediction head, with dimensions (1024, 1024, 1024), further
for a given batch during training. The transformation sets processes the representation, shaping the learning objective.
includearangeoftechniquessuchasRandomHorizontalFlip, The online network’s weights are actively trained during the
Gaussian Blur, Random Affine, Random Rotation, Random learningprocess,anditprocessestwoaugmentedviewsv and
Crop, Center Crop, Random Grayscale, Color Jitter, Random v′, extracting representations as in (1):
Invert,GaussianNoise,andCutout.Theappliedmodifications
to the augmented view help to address the issues arising
y =(y ⊕y ⊕y ) (1)
θ global left right
from appearance uncertainties, equivariance, and positional
shifts.Forexample,theRandomAffineandRandomRotation forbothviews.Where⊕representstheconcatenationprocess.
transformations are effective in addressing positional shifts TargetNetwork:Thetargetnetworkmirrorstheonlinenet-
andequivariance,whileGaussianBlurandRandomGrayscale workbutexcludesthepredictionhead[17].ItalsoprocessesvJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 6
Self-Supervised Training (mBYOL)
Online Network
𝑙𝑜𝑐𝑎𝑙
𝑏𝑟𝑎𝑛𝑐ℎ
𝑤
𝐹𝑓
𝑒𝑖𝑉
𝑆𝑆𝐿𝐼𝑚𝑎𝑔𝑒𝐼𝑛𝑝𝑢𝑡
𝑑𝑒𝑡𝑛𝑒𝑚
𝑔𝑢𝐴
𝑔
𝑏𝑟𝑙𝑜𝑓 𝑎𝑏𝜃
𝑛𝑎 𝑐𝑙
ℎ
𝑓𝑓
𝑔𝜃
𝑝𝑟𝑜𝑗𝑒𝑐𝑡𝑖𝑜𝑛
𝑞𝜃
𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛
𝑞𝜃(𝑧𝜃)
noitcartxE
erutaeF
𝐹𝑓𝑏
𝐹𝑒𝑙
𝐹𝐹
]4201[
]652[
𝐹𝑇
𝑂𝑢𝑡𝑝𝑢𝑡𝐺𝑎𝑧𝑒
𝑑𝑒𝑡𝑛𝑒𝑚𝑤
𝑒𝑖𝑉
𝑓𝜃′ 𝑔 𝜃′ 𝑠𝑔(𝑧𝜉′)
𝐹𝑒𝑟
∑(4) 𝐹𝑒
[1280,1024, 256, 2]
𝑔𝑢𝐴 𝑓𝑓′
𝑝𝑟𝑜𝑗𝑒𝑐𝑡𝑖𝑜𝑛
InputImage
Downstream Finetuning (PMN)
Target Network
Concatenation Patch Module MLP mBYOLMLP GlobalbranchEncoder ∑ PoolingLayer Multi-headAttention Bottleneck layer mBYOLLocalbranch
Fig. 4. Schematic diagram of the proposed framework, SLYKLatent. SLYKLatent comprises mBYOL, a modification of BYOL framework [17]; and a
downstreamfinetunningwhichismadeupofaneye-patchmodulenetwork.Theresultantfeaturesareconcatenatedtoregressagainsttheground-truthgaze
vector.
and v′ creating projections for both views. The parameters of L compares the normalized version of the predictor
SSL
thetargetnetworkfollowtheparametersoftheonlinenetwork (onlinenetwork),andtheprojector(targetnetwork),calculated
with a delay, using an exponential moving average (EMA) aftereverytrainingstep.Afterthespecifiedtrainingepochsare
controlled by a momentum term [17], as detailed in (2) using completedtheencoderf issavedandalltheotherparameters
θ
a target decay rate τ ϵ [0,1]. are discarded. Further details on training and inference are
presented in Section IV-B.
ξ ←τξ+(1−τ)θ (2) This cohesive architecture, along with the collaboration
between the online and target networks, enables a robust self-
The target network’s role is to generate stable representations
supervised learning process. By aligning the features of vari-
that the online network’s predictions aim to match, contribut-
ous parts of the input image and transforming them through
ing to a more stable and effective learning process.
carefully designed network components, mBYOL provides a
The architecture also includes momentum updates for vari-
versatile and effective framework for learning rich represen-
ous components, including the backbone, local branches, and
tations from complex and high-dimensional data without the
multi-head attention layers, and utilizes the negative cosine
need for labeled supervision.
similarity loss function [17]. The regression loss L , de-
SSL
fined as:
B. Downstream Transfer Learning Fine-tuning
1(cid:16)
L = NS (p (v),z (v′)) Once the self-supervised pretraining process is complete,
SSL 4 cos θ ξ
theSLYKLatentframeworkprogressestothedownstreamfine-
+NS (p (v′),z (v))
cos θ ξ tuning stage. This stage diverges from traditional approaches
+NS cos(z θ(v),z ξ(v′)) that rely on linear evaluation and regression of image features
(cid:17)
+NS (z (v′),z (v)) (3) after self-supervised learning pretraining. Instead, the SLYK-
cos θ ξ
Latent framework introduces the concept of Patch Module
is minimized through stochastic optimization. In this expres- Networks (PMN) specifically designed for gaze estimation.
sion, p (v) and z (v) are the prediction and projection of ThePMNconsistsofbottleneckmodulesthatareinstrumental
θ θ
the online network for view v. z (v′) is the projection of in extracting relevant features from the face and eye patches,
ξ
the target network for view v′. NS represents the negative thereby facilitating the training of a precise supervised gaze
cos
cosine similarity loss function. This loss function aligns the estimation. During this stage, the mBYOL model is used
representations of the augmented views v and v′, encouraging as a backbone to extract features from the face and set to
the online network to predict the target network’s projections, evaluation mode with additional features learned from the
and vice versa, thereby enhancing the learning of meaningful PMN.Comprisingthreespecificbottleneckmodules—theface
representations. bottleneckB ,lefteyebottleneckB ,andrighteyebottleneck
f lJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 7
B —the PMN processes distinct facial regions for feature vectors are then combined to form a comprehensive face
r
extraction and transformation. feature vector F as follows:
F
The architecture of the bottleneck modules, as detailed in
Table I, consists of three convolutional layers each followed F =F ⊕F (5)
F f fb
by a batch normalization layer. The output of the final convo-
ThecomprehensivefacefeaturevectorF andthereduced-
lutional layer is passed through an adaptive average pooling F
dimensionality eye feature vector F are then concatenated to
layer, reducing the spatial dimensions while preserving the e
form the final feature vector F that is passed into the MLP:
channel information, and finally through a fully connected T
layer to produce a 512-dimensional feature vector. F =F ⊕F (6)
T F e
The MLP, denoted by MLP, varies based on the dataset
TABLEI
being used. For the MPIIFace dataset, the MLP is composed
BOTTLENECKMODULEARCHITECTURE
of three layers. The initial layer transforms the input feature
Layer Type OutputChannels KernelSize Padding
vector of size 1280 to a size of 1024, followed by batch
Conv1 Conv2d 32 3 1
BN1 BatchNorm2d 32 - - normalization. Subsequent layers further reduce the dimen-
Conv2 Conv2d 64 3 1 sionality to 256, and finally to 2, which represents the gaze
BN2 BatchNorm2d 64 - -
Conv3 Conv2d 128 3 1 vector. Dropout regularization with a rate determined by the
BN3 BatchNorm2d 128 - - dataset-specificconfigurationisappliedinthearchitecture.As
Pooling AdaptiveAvgPool2d 128 1x1 -
therangeofthegazevectorintheMPIIFacedatasetisbetween
FC Linear 512 - -
-1 and 1, the tanh activation function is utilized to ensure the
These bottleneck modules serve multiple purposes in the output values fall within this range:
SLYKLatent framework. They reduce the dimensionality of
the input features, making it easier to train the downstream MLP MPII :1280→1024→256→2 (7)
gaze estimation model, and provide a way to extract relevant
For other datasets, the MLP follows a similar architecture
features from the face and eyes. In addition to reducing
butwithoutthedropoutregularizationandthetanhactivation
dimensionality,thebottleneckmodulesintroducenon-linearity
function in the final layer:
into the network, allowing it to learn more complex features
specific to the task of gaze estimation. The face and eye MLP :1280→1024→256→2 (8)
MPII
bottleneck modules follow the same architecture and play a
The MLP predicts the gaze vector yˆ as:
crucial role in capturing the most important features of the
inputimage.Whilethefacebottleneckisfocusedoncapturing
yˆ=MLP(F ) (9)
global face features, the eye bottlenecks are tailored for T
extracting detailed eye-specific information. Together, these The output of the MLP is a 2-dimensional vector rep-
components enable the SLYKLatent framework to learn rich resenting the predicted pitch and yaw angles, that is, yˆ =
and robust representations that can be used to accurately (θ ,θ ). These angles are defined as follows:
pitch yaw
estimate gaze vectors in various scenarios. • θ pitch ranges from −90◦ to 90◦, with positive values
indicating an upward gaze direction.
C. SLYKLatent Downstream Gaze Estimation • θ yaw ranges from −180◦ to 180◦, with positive values
indicatingaclockwiserotationofthegazedirectionwhen
IntheSLYKLatentframework,downstreamgazeestimation
looking down from above.
is performed after feature extraction using the SSL encoder
andPMN.ThefeaturevectorsproducedfromtheSSLencoder These angles are subsequently transformed into a normal-
and PMN are then fed into a Multi-Layer Perceptron (MLP) ized 3D gaze direction vector. This conversion is a critical
that predicts the gaze vector. post-processing step in the gaze estimation pipeline, ensuring
The process begins with the generation of normalized land- that the predicted gaze direction can be represented in a
marks from the full-face image X, which is used to extract 3D space for accurate angular comparisons. The conversion
the eye patches, denoted as X and X for the left and process from angular outputs to 3D Cartesian coordinates is
el er
right eyes, respectively. These eye patches are input into the formalized as:
correspondingeyebottleneckmodulesofthePMN,producing
x=cos(θ )sin(θ )
feature vectors F and F . These feature vectors are then pitch yaw
el er
concatenated and an average pooling operation is performed, y =sin(θ pitch) (10)
resulting in a reduced-dimensionality eye feature vector F . z =cos(θ )cos(θ )
e pitch yaw
Mathematically, this can be represented as:
where x, y, and z represent the Cartesian components of the
resulting unit length vector v =(x,y,z).
F =Pool (F ⊕F ) (4)
e avg el er In this representation:
Simultaneously, the full-face image X is processed through • The x-coordinate corresponds to the lateral direction,
the SSL encoder F and the face bottleneck module to extract withpositivevaluesdenotingrightwardfromtheperspec-
θ
the facial feature vectors F and F , respectively. These tive of the participant.
f fbJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 8
• The y-coordinate corresponds to the vertical direction, IV. EXPERIMENTSANDMETHODOLOGY
with positive values denoting upward from the perspec- A. Datasets
tive of the participant.
For SSL pre-training, we used the AFFECTNet Dataset
• The z-coordinate corresponds to the frontal direction,
[52], and for downstream evaluation, we employed MPI-
with positive values denoting forward, away from the
IFaceGaze[34],Gaze360[39],andETHX-Gaze[53].Detailed
participant.
descriptions of these datasets, including their composition,
This normalized gaze direction is utilized in the computation
collection methodologies, and specific usage in our study, are
of the gaze angular error in the subsequent performance
provided in the Supplementary Material. We utilized these
evaluation.
datasetstoevaluateourapproachandcompareitsperformance
Duringtraining,thegazeestimationlossL iscomputed
SUP againstexistingstate-of-the-artmodelsinbothgazeestimation
toquantifythedifferencebetweenthepredictedgazevectorsyˆ
andfacialexpressionrecognitiontasks.Theoutcomesofthese
and the actual gaze vectors y. The loss function incorporates
assessments are presented and discussed in the subsequent
a weighting scheme based on the inverse of the explained
sections of this paper.
variance, ω. This strategy emphasizes the minimization of
larger errors, resulting in the model learning to accurately
B. Implementation details
predictgazevectorswhileconcurrentlypenalizinglargererrors
more heavily than smaller ones. Training details for SSL: For self-supervised pretraining,
The Mean Absolute Error (MAE) between the predicted the Stochastic Gradient Descent optimizer [62] was used
and actual gaze vectors for each sample in a batch of size n with a learning rate of 0.06 and a batch size of 112. The
is calculated as follows: number of epochs was set to 100. EMA was initially set to
0.996 and subsequently increased to 1 during training [17].
1 (cid:88)n For optimal efficiency and speed during training, the data
L= ||y −yˆ|| (11)
n i i loading was parallelized across 4 Nvidia A100 GPUs using a
i=1 distributed data-parallel technique. Training took a total of 80
where y and yˆ denote the actual and predicted gaze vectors hours. All model development, training and evaluation work
i i
for the ith sample, respectively. was undertaken using the Pytorch Lightning deep-learning
The Total Sum of Squares (SST), which measures the total framework [56].
variance in the gaze vectors, and the Sum of Squared Errors Training details for Gaze estimation:Forgazeestimation
(SSE), which measures the total deviation of the predicted downstream fine-tuning, the Adam optimizer [46] was used
gaze vectors from the actual gaze vectors, are calculated as for all datasets with different batch sizes and learning rates
follows: for each of the datasets. We used a learning rate of 0.0003
n
(cid:88) and a batch size of 16 for all gaze estimation datasets. As
SST= ||y −y¯||2 (12)
i with SSL training, a distributed data-parallel implementation
i=1
was adopted for data loading during the training phase. To
n adapt the model to the specific characteristics of each dataset,
(cid:88)
SSE= ||y i−yˆ i||2 (13) weemployeddataset-specificconfigurations.Foreverydataset,
i=1 a learning rate annealing strategy was adopted, where the
where y¯ denotes the mean of the actual gaze vectors. The learning rate was reduced upon observing plateaus in train-
explained variance V , which measures the proportion of ing/validation performance. This dynamic adjustment of the
EX
the gaze vector variance that is predictable from the predicted learningrateaidsinachievingbetterconvergenceandprevents
gaze vectors, is then calculated as: overshooting in the optimization landscape.
FortheGaze360dataset,themodelwastrainedforatotalof
SSE
V =1− (14) 51epochs.Theprimarymetricformonitoringthelearningrate
EX
SST
scheduler,inthiscase,wasthetrainingangularerror.Training
The weight ω for each sample, which corresponds to the took 4 hours on 4 x v100 Nvidia GPUs.
inverse of the explained variance, is computed as: In our experimentation with the MPIIFace dataset, we em-
ulated the original work’s methodology, implementing leave-
SSE
ω =1−V EX = (15) one-person-outcross-validation[34].Thisapproachrigorously
SST
assessed the model’s efficacy. The training spanned a maxi-
Finally, the gaze loss metric L , which quantifies the
SUP mum of 70 epochs for each participant. To avoid over-fitting
differencebetweenthepredictedandactualgazevectorsacross
early stopping was employed with a patience of 20 epochs
the entire batch, is calculated as the product of the MAE and
andaminimumdeltaof0.1fortheangularerror.Post-training,
the factor ω+1:
15distinctmodels,eachcorrespondingtoleft-outparticipants,
underwent testing. The resulting angular errors were averaged
L =L·(ω+1) (16)
SUP
to compute a mean angular error. Training took 17 hours on
By using this weighted loss function, the model prioritizes 4 x V100 Nvidia GPUs.
the minimization of larger gaze estimation errors, which For the ETH dataset, the validation angular error served as
contribute more significantly to the overall loss. the primary metric for the learning rate scheduler. CallbacksJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 9
were strategically placed to monitor model checkpoints based TABLEII
on this error,facilitating the saving of optimalmodel weights. COMPARISONOFTHEMEANGAZEANGULARERROR(Lang)
PERFORMANCEOFSLYKLATENTANDALTERNATIVEAPPROACHESINTHE
Additionally, an early stopping mechanism was integrated,
LITERATUREFORSELECTEDBENCHMARKDATASETS(MP:MPIIFACE,
endowed with a patience of 20 epochs and a minimum delta G360:GAZE360,ETH:ETH-XGAZE).THEBESTRESULTSARE
of 0.01. Training took 6 hours on 2 x A100 Nvidia GPUs. HIGHLIGHTEDINBLUEANDTHESECONDBESTRESULTSAREINBROWN.
In summary, our training methodology, underpinned by the
THESECONDCOLUMNISTHENUMBEROFMODELPARAMETERSIN
MILLIONS(M).
Adamoptimizer,dynamiclearningrateadjustments,andearly
stopping,ensuresrigorous,robust,andefficientmodeltraining Method Params InitArch. MP G360 G360 G360 ETH
(M) (±180◦) (±90◦) (±20◦)
tailored to each dataset’s nuances. GazeNet[33] - ResNet-50 - 12.8◦ - - -
EyeDiap[58] - ResNet-50 - 12.8◦ - - -
Full-Face[34] - AlexNet+SW 4.9◦ 15.0◦ - - -
V. RESULTS GD ail za ete 3d 6- 0N [e 3t 9[ ]60] 24-
.0
PD inil ba ate lld- LC SN TN
M
14 2. .8 1◦
◦
13-
.2◦
11-
.4◦
11-
.1◦
-
-
ETH-XGaze[53] 23.5 ResNet-50 4.8◦ - - - 4.5◦
A. Performance Evaluation MANet[15] 29.5 ResNet-18 4.3◦ 13.2◦ - - -
SWAT[44] - ResNet-50 5.0◦ 11.6◦ - - 4.4◦
In this section we present results comparing SLYKLatent L2CS-Net[59] 21.2 ResNet-50 3.92◦ - 10.41◦ 9.02◦ -
SLYKLatent 25.5 Inception-V1 3.96◦ 10.67◦ 10.31◦ 8.28◦ 3.89◦
with existing and current state-of-the-art methods in gaze
estimation for each of the gaze datasets presented in Section
IV-A.Wealsopresenttheresultsofablationstudiesonseveral
B. Ablation Studies
components of SLYKLatent to highlight the contribution of
eachcomponent.AsdetailedintheSectionIII-C,thepredicted In this section, we analyze the performance of SLYKLatent
2D angles from the MLP are converted to 3D unit vectors. through a series of ablation studies, where different compo-
Similarly, the ground truth data is also represented as 3D unit nents of the model are removed or altered to understand their
vectors. This ensures consistent and accurate representation impact on overall performance. Additionally, we compare the
for comparing the predicted and actual gaze directions. The performance of the SLYKLatent model with one that uses
traditional gaze angular error metric, L , is chosen as BYOL [17] as its backbone. Specifically, Table III presents
ang
the performance metric. This metric quantifies the angular results for the following ablated configurations:
discrepancy between the ground truth and the predicted gaze 1) without the PMN (w/o-PMN)
vectors in a 3D space. This is defined as 2) without SSL (w/o-SSL)
3) without inverse explained variance (w/o-inv-EV)
180
L ang = π cos−1(v·vˆ) (17) 4) without the mBYOL modification (w/o-mBYOL)
5) basic BYOL [17] without any enhancements
where v represents the ground truth 3D gaze direction, and vˆ
The performance metric used is the gaze angular error evalu-
represents the estimated direction.
1) Performance of SLYKLatent on Gaze Datasets: Table ated on the validation/test data for each dataset.
II provides an exhaustive evaluation of the performance of Impact of Patch Module Network: We evaluated the con-
SLYKLatent, our proposed framework, against existing state- tribution of the Patch Module Network (PMN) by analyzing
of-the-art methodologies for the MPIIFaceGaze, ETHX-Gaze, the performance of an SSL-only variant. This model variant
and Gaze360 datasets. Notably, SLYKLatent achieves supe- included the mBYOL backbone and inv-EV cost function but
rior performance to existing state-of-the-art models for the not the other enhancements of the PMN. The results indicate
Gaze360 dataset. It registers an angular error of 10.67◦ for thattheSSL-onlyversionperformsreasonablywellbutiscon-
the full Gaze360 dataset, 10.31◦ for data within ±90◦, and sistently surpassed by the complete SLYKLatent model. The
8.28◦ for data within ±20◦, surpassing previous benchmarks average impact on performance across the datasets is 7.7%.
[59][15]bybetween1%and8%fordatawithintheseangular ThisconfirmsthesignificanceofthePMNinconjunctionwith
ranges. For the MPIIFaceGaze dataset, SLYKLatent achieves mBYOL in providing a more balanced and effective solution
an angular error of 3.96◦, which is on a par with the best for gaze direction prediction.
performing method, L2CS-Net [59] (L = 3.92◦), and Impact of SSL pretraining: To assess the impact of the
ang
superiortoothercontemporaryself-supervisedgazeestimation SSL pre-trained model, we compared the results without SSL
methods such as MANet [15] (L = 4.30◦) and SWAT pretraining (w/o-SSL) with the full SLYKLatent architec-
ang
[44] (L = 5.0◦). L2CS-Net is a supervised approach that ture. We eliminated the self-supervision stage of SLYKLatent
ang
leverages a CNN architecture and combines loss functions for and replaced the mBYOL pre-trained model with the bare
both the regressionof the gaze vector and theclassification of backbone (InceptionResNet-V1) and retained the downstream
varying gaze angles. finetuning PMN. The resulting architecture was then trained
An objective comparison of our model’s performance with using the same experimental setup as used with SLYKLatent
the ETHX-Gaze dataset was not feasible as SLYKLatent (see Section IV-B). The results highlight the benefit of SSL
requires the presence of eye patches to perform inference - with the angular error increasing by 6.3% on average across
a requirement not shared by some of the alternative methods the datasets when it is omitted.
intheliterature.Consequently,wehadtoexcludeETHX-Gaze Impact of Inverse Explained Variance: To isolate the
sampleswheredetectionofeyepatcheswasnotpossible.This effect of inv-EV, we trained and tested a model without this
represented 65% of the dataset. For the remaining 35% of modification (w/o-inv-EV), keeping all other aspects and hy-
samples SLYKLatent achieves an angular error of 3.89◦. perparametersunchanged.ThisresultedinanaverageincreaseJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 10
in angular error of 7.9% over SLYKLatent, confirming the TABLEIV
positive effect inv-EV has on model performance. ANGULARERRORPERFORMANCEOFSLYKLATENT(SLYKL)ANDIT
Impact of mBYOL: To evaluate the impact of employing ABLATEDVARIANTSFORAPPEARANCEUNCERTAINTIES
our modified BYOL (mBYOL) backbone in the SLYKLa- Participants NoofImages w/o-PMN w/o-SSL w/o-inv-EV SLYKL
tent model, we contrasted its performance with a variant P03 98 3.46◦ 2.96◦ 3.03◦ 2.78◦
P11 1223 4.61◦ 4.26◦ 5.10◦ 3.91◦
that employs the standard BYOL backbone instead (denoted P13 686 4.53◦ 4.19◦ 7.53◦ 3.95◦
as w/o-mBYOL). Training was undertaken as described in Lang↑(%) - 19.0 7.2 43.4 -
Section IV-B. An examination of the results in Table III
reveals that w/o-mBYOL is outperformed by SLYKLatent on
The improvements achieved by SLYKLatent when com-
all three datasets, with its L on average 5.8% greater
ang pared with the ablated models is substantial for all models
than SLYKLatent. Notably, while inferior to SLYKLatent,
(19.0%, 7.2% and 43.4%), and considerably greater than the
w/o-mBYOLoutperformsthew/o-SSLvariant,indicatingthat
improvements observed for the full MPIIFace dataset pre-
the inclusion of BYOL assists the model in learning better
sentedinTableIII(14.1%,4.8%and6.8%).Mostnotably,for
representations. Overall, the results indicate that integrating
the selected high appearance uncertainty images the absence
mBYOL as the backbone in the SLYKLatent architecture
ofinv-EVresultsina43.4%increaseinL andtheabsence
ang
yields superior performance compared to the standard BYOL
of PMN results in an increase of 19.0%. This suggests that
backbone.
both components are critical to the robustness of SLYKLatent
Impact of all enhancements: To further demonstrate the
and its ability to mitigate challenges due to appearance un-
combinedbenefitofthedifferentcomponentsofSLYKLatent,
certainties such as low illumination. Similarly, the w/o-SSL
we trained an SSL backbone using only the BYOL frame-
configuration results in a 7.2% increase in L . Although
ang
work, that is, without the proposed mBYOL modification and
this is lower than the other enhancements, it shows that self-
without the PMN and inv-EV cost function enhancements.
supervisedlearningoffersacertaindegreeofresilienceagainst
This ‘Basic BYOL’ variant yielded an angular error of 4.56◦,
environmental variances.
13.56◦,and5.0◦ ontheMPIIFace,Gaze360,andETHX-Gaze
datasets, respectively. This corresponds to an average increase
in L of 23.0% compared to SLYKLatent. The scale of this
ang
increase underscores the accumulative benefit of the PMN,
inv-EV and mBYOL enhancements.
TABLEIII
ANGULARERROR(Lang)PERFORMANCEOFVARIOUSABLATED
SLYKLATENTMODELS:W/O-PMNDENOTESMODELSWITHOUTTHE
PATCHMODULENETWORK,W/O-SSLDENOTESMODELSTRAINED
WITHOUTSELF-SUPERVISEDLEARNING,W/O-INV-EVDENOTESTRAINING
WITHOUTTHEINVERSEEXPLAINEDVARIANCEMODIFICATION,
W/O-MBYOLDENOTESAIMPLEMENTEDUSINGTHEBYOLFRAMEWORK. Fig. 5. Visualization of ground truth and SLYKLatent predictions under
THEFINALCOLUMNREPORTSTHEAVERAGEPERCENTAGEINCREASEIN appearance uncertainties. The figure provides a side-by-side comparison
LangRELATIVETOSLYKLATENTOVERALLDATASETS. of gaze estimation results under various appearance uncertainties such as
low illumination and image blurriness. Red arrows are ground truth gaze
Method MP G360 G360 ETH Lang ↑ directions.BluearrowsareSLYKLatentpredictions.
(±180◦) (±90◦) (%)
w/o-PMN 4.52◦ 11.21◦ 10.82◦ 4.15◦ 7.7
w/o-SSL 4.15◦ 10.90◦ 10.54◦ 4.52◦ 6.3
w/o-inv-EV 4.23◦ 11.63◦ 11.30◦ 4.24◦ 8.6 C. Analysis of Equivariance Through Rotational Testing
w/o-mBYOL 4.22◦ 10.82◦ 10.91◦ 4.26◦ 5.8
Inthissection,weinvestigatetheequivariancepropertiesof
Basic BYOL 4.56◦ 13.56◦ 12.50◦ 5.00◦ 23.0
SLYKLatent,usingtheMPIIFacedatasetfortheanalysis.The
SLYKLatent 3.96◦ 10.67◦ 10.31◦ 3.89◦ -
images, patches, and gaze vectors for this dataset are rotated
1) TestingforAppearanceUncertainties: Therobustnessof to test for equivariance.
gazeestimationmodelsisparamount,particularlywhenfacing Gaze Rotation: The groundtruth gaze vectors ⃗y i were
real-world challenges characterized by appearance uncertain- rotated by an angle θ using the equation:
ties,including,butnotlimitedto,lowilluminationconditions.
⃗y =R(θ)⃗y , (18)
i(rotated) i
WeconductedarigorousexaminationoftheMPIIFacedataset
to pinpoint low-quality images indicative of such challenges. where R(θ) is the rotation matrix defined as:
Ourscanidentified98,686,and1223low-illuminationimages (cid:18) (cid:19)
cos(θ) −sin(θ)
within subsets p03, p11, and p13, respectively. We then R(θ)= . (19)
sin(θ) cos(θ)
subjectedablatedvariantsoftheSLYKLatentmodel(w/o-SSL,
w/o-PMN,andw/o-inv-EV)totestsontheseidentifiedimages Results and Observations: SLYKLatent and selected ab-
to evaluate the individual contribution of each component of lated variants were subjected to this rotational transformation
theSLYKLatentmodeltorobustnesstoappearanceuncertain- for rotation angles θ ∈ {5◦,10◦,15◦,20◦,25◦,30◦}. The
ties. The results of these tests are quantitatively summarized angular error results obtained are plotted in Fig. 6 as a
in Table IV and further illustrated in Figure 5. functionofθ.TheBasicBYOLmodelhasthehighestangularJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 11
identity and head pose, the results are still promising. The
Equivariance Analysis Through Rotational Testing IPD-FER model achieved an accuracy of 87.22% for RAF-
13 w Ba/o s- icm BB YY OO LL DB and 63.77% for AffectNet, benefiting from its targeted
w/o-SSL
12 SLYKLatent disentanglement approach. However, it is worth noting that
11 SLYKLatent demonstrates remarkable versatility, showing ro-
10 bustness not only in gaze estimation but also in facial expres-
sion recognition (FER), further showcasing its applicability to 9
a wide range of vision tasks.
8
7
VI. CONCLUSION
6
Inthispaper,wehavepresentedSLYKLatent,aninnovative
5
5 10 15 20 25 30 framework designed to tackle the intricate challenges inherent
Angles (degrees)
to gaze estimation, including appearance instability, covariate
Fig. 6. Equivariance performance of SLYKLatent and its ablated variants, shift, and the need for domain generalization. SLYKLatent
w/o-mBYOL,w/o-SSL,andBasicBY.OL,forrotationaltransformations integrates the resilience of Self-Supervised Learning (SSL)
with a specialized tri-branch, patch-centric network architec-
ture (PMN) and enhanced training via an inverse explained
errors followed by w/o-mBYOL, with errors in the range
variance (inv-EV) weighted loss function. Several ablation
10.8◦ − 13.0◦ and 8.8◦ − 11.2◦, respectively. In contrast,
studies presented provide compelling empirical insights into
SLYKLatent is the best performingmethod with angular error
the contribution of these different components to the overall
rates ranging from 5.2◦ to 9.3◦. While w/o-SSL performs
performance of SLYKLatent.
much better than the other ablated variants, it is consistently
Through extensive evaluation on diverse datasets (MPI-
outperformed by SLYKLatent by between 0.2◦ and 0.5◦. To
IFaceGaze, Gaze360, and ETHX-Gaze) and benchmarking
further elucidate SLYKLatent’s equivariance properties, it is
againstseveralcompetingmethodsintheliterature,wedemon-
worth noting that the difference in angular error from 0◦ (i.e.
strate that SLYKLatent achieves state-of-the-art performance
when not rotated) to 5◦ is 6.3◦ for BasicBYOL, 4.6◦ for w/0-
for gaze estimation. In addition, we show that the resulting
mBYOL, 1.4◦ for w/o-SSL, and 1.2◦ for SLYKLatent. These
model manifests a notable flexibility in addressing tasks be-
differences highlight the relative stability of SLYKLatent in
yond gaze estimation, such as facial expression recognition.
the face of rotational transformations.
Further tests on MPIIFace show that the PMN and inv-
Implications: The results make a compelling case for the
EV components of SLYKLatent contribute to its robust to
importance of our mBYOL modifications, which include a
rotationsalthoughacomprehensivebenchmarkforthisspecific
modified transformation and a specialized branch network in
capability is an avenue for future work.
theself-supervisedpretrainingstage.Thesubstantialimprove-
SLYKLatent does have some limitations. In particular, its
ment in error rates in the SLYKLatent model, as compared
effectiveness is contingent upon reliable eye patch detection,
to the Basic BYOL and without mBYOL configurations,
hence it cannot be employed in scenarios where such features
validates our architectural decisions. Specifically, it confirms
are not consistently present, as was the case for 65% of the
that our innovations contribute significantly to the model’s
ETHZ-Gaze dataset. Additionally, while the model demon-
resilienceagainstrotationaltransformations,therebyaffirming
strates promising versatility in tasks like facial expression
its robust equivariance capabilities.
recognition, it has yet to meet the gold standards established
by domain-specific state-of-the-art models.
D. Performance of SLYKLatent in Facial Expression Recog- In future work, we aim to broaden the scope of SLYKLa-
nition (FER): tent’sapplicabilitybyexploringotherfacialfeatureestimation
tasks and by examining its integrative potential in complex
Althoughthecoreobjectiveofourstudyisgazeestimation,
systems, such as human-robot interaction platforms. A fur-
we extend the scope to assess SLYKLatent’s adaptability in
ther potential refinement is the addition of Bayesian Neural
the domain of Facial Expression Recognition (FER). This
Networks to extend the model’s capabilities to further capture
evaluation is relevant as FER, like gaze estimation, relies
aleatoric uncertainties in the learned features. These enhance-
on the precise extraction of facial features. To this end, we
ments will contribute to a more nuanced understanding of the
repurpose our gaze estimation Multi-Layer Perceptron (MLP)
framework’s capabilities and limitations, thereby facilitating
to output class labels for facial expressions, specifically 7
its more effective deployment in real-world scenarios.
classesforRAF-DBand8forAffectNet.Bothdatasetsemploy
the weighted cross-entropy loss based on class weights.
On the RAF-DB and AffectNet benchmarks, SLYKLatent
ACKNOWLEDGMENT
yieldsaccuracyratesof86.4%and60.9%,respectively.While This research is funded by the Engineering and Physical
thesefiguresfallslightlyshortoftheperformanceofthestate- Sciences and Research Council (EPSRC), United Kingdom.
of-the-artmethodcitedin[64],whichemploysasophisticated We are grateful for the use of the computing resources from
IdentityandPoseDisentangledFacialExpressionRecognition the Northern Ireland High Performance Computing (NI-HPC)
(IPD-FER) model for disentangling facial expressions from service funded by EPSRC (EP/T022175).
)seerged(
rorrE
naeMJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 12
REFERENCES [24] R.Zhi,M.Liu,andD.Zhang,“AcomprehensivesurveyonAutomatic
Facial Action Unit Analysis,” The Visual Computer, vol. 36, no. 5, pp.
1067–1093,2019.
[1] K. Dautenhahn, ”Human-robot interaction: from AI to HCI and social
robotics,”AIMagazine,vol.27,no.4,pp.27–48,2006. [25] A.Gupta,K.Thakkar,V.Gandhi,andP.J.Narayanan,“Nose,eyesand
ears: Head pose estimation by locating facial keypoints,” ICASSP 2019
[2] M. A. Goodrich and A. C. Schultz, ”Human-robot interaction in search
- 2019 IEEE International Conference on Acoustics, Speech and Signal
andrescue,”JournalofFieldRobotics,vol.27,no.6,pp.443–475,2010.
Processing(ICASSP),2019.
[3] J.Fong,T.Williams,M.Neff,andR.N.Nagarajan,”Asurveyofsocially
[26] Y.Cheng,H.Wang,Y.Bao,andF.Lu,“Appearance-basedgazeestima-
interactiverobots,”RoboticsandAutonomousSystems,vol.42,no.3,pp.
tionwithDeepLearning:Areviewandbenchmark,”arXiv.org,26-Apr-
143–166,2003.
2021. [Online]. Available: https://arxiv.org/abs/2104.12668. [Accessed:
[4] S.Adebayo,S.McLoone,andJ.C.Dessing,“Hand-eye-objecttracking
16-June-2022].
forhumanintentioninference,”IFAC-PapersOnLine,vol.55,no.15,pp.
[27] X. Wang, J. Zhang, H. Zhang, S. Zhao and H. Liu, ”Vision-Based
174–179,2022.
Gaze Estimation: A Review,” in IEEE Transactions on Cognitive and
[5] Z.Li,Y.Mu,Z.Sun,S.Song,J.Su,andJ.Zhang,“Intentionunderstand-
Developmental Systems, vol. 14, no. 2, pp. 316-332, June 2022, doi:
inginhuman–robotinteractionbasedonvisual-NLPsemantics,”Frontiers
10.1109/TCDS.2021.3066465.
inNeurorobotics,vol.14,2021.
[28] N. Modi and J. Singh, “A review of various state of Art Eye
[6] S. P. Parikh, J. M. Esposito, and J. Searock, “The role of verbal and
Gaze Estimation Techniques,” SpringerLink, 01-Jan-1970. [Online].
nonverbal communication in a two-person, cooperative manipulation
Available: https://link.springer.com/chapter/10.1007/978-981-15-1275-9-
task,” Advances in Human-Computer Interaction, vol. 2014, pp. 1–10,
41.[Accessed:15-Jan-2023].
2014.
[29] P.Pathirana,S.Senarath,D.Meedeniya,andS.Jayarathna,“Eyegaze
[7] J.UrakamiandK.Seaborn,“Nonverbalcuesinhuman-robotinteraction:
estimation:Asurveyondeeplearning-basedapproaches,”ExpertSystems
A communication studies perspective,” ACM Transactions on Human-
withApplications,vol.199,p.116894,2022.
RobotInteraction,2022.
[8] M.BalconiandC.Lucchiari,“Encodingofemotionalfacialexpressions [30] D. Windridge, A. Shaukat and E. Hollnagel, ”Characterizing Driver
indirectandincidentaltasks:Twoevent-relatedpotentialstudies,”Aus- IntentionviaHierarchicalPerception–ActionModeling,”inIEEETrans-
tralianJournalofPsychology,vol.59,no.1,pp.13–23,2007. actionsonHuman-MachineSystems,vol.43,no.1,pp.17-31,Jan.2013,
doi:10.1109/TSMCA.2012.2216868.
[9] .L.SchmidtandJ.F.Cohn,“Humanfacialexpressionsasadaptations:
EvolutionaryquestionsinFacialExpressionResearch,”AmericanJournal [31] Blundell, J., Collins, C., Sears, R., Plioutsias, T., Huddlestone,
ofPhysicalAnthropology,vol.116,no.S33,pp.3–24,2001. J., Harris, D., ... Lamb, P. (2023). Multivariate analysis of Gaze
Behavior and task performance within Interface Design Evaluation.
[10] B. Farnsworth, N. Nguyen, K. Krosschell, P. Bu¨low, and M. Martin,
IEEE Transactions on Human-Machine Systems, 53(5), 875–884.
“Facialexpressionanalysis:TheCompletePocketGuide,”iMotions,17-
doi:10.1109/thms.2023.3305715
Nov-2022. [Online]. Available: https://imotions.com/blog/learning/best-
practice/facial-expression-analysis/.[Accessed:11-Nov-2022]. [32] A.Dini,C.Murko,S.Yahyanejad,U.Augsdo¨rfer,M.HofbaurandL.
Paletta, ”Measurement and prediction of situation awareness in human-
[11] Frith C. Role of facial expressions in social interactions. Philos
robot interaction based on a framework of probabilistic attention,”
Trans R Soc Lond B Biol Sci. 2009 Dec 12;364(1535):3453-8. doi:
2017 IEEE/RSJ International Conference on Intelligent Robots and
10.1098/rstb.2009.0142.PMID:19884140;PMCID:PMC2781887.
Systems (IROS), Vancouver, BC, Canada, 2017, pp. 4354-4361, doi:
[12] A.JaimesandN.Sebe,“MultimodalHuman–ComputerInteraction:A
10.1109/IROS.2017.8206301.
Survey,” Computer Vision and Image Understanding, vol. 108, no. 1-2,
pp.116–134,2007. [33] X.Zhang,Y.Sugano,M.Fritz,andA.Bulling,“MPIIGaze:Real-world
datasetand deepappearance-basedgaze estimation,” IEEETransactions
[13] M. Lombardi, E. Maiettini, D. De Tommaso, A. Wykowska, and L.
onPatternAnalysisandMachineIntelligence,vol.41,no.1,pp.162–175,
Natale,“Towardanattentiveroboticarchitecture:Learning-basedMutual
2019.
GazeEstimationinhuman–robotinteraction,”FrontiersinRoboticsand
AI,vol.9,2022. [34] X.Zhang,Y.Sugano,M.Fritz,andA.Bulling,“It’swrittenalloveryour
face:Full-faceappearance-basedgazeestimation,”2017IEEEConference
[14] Das, D., Rashed, Md. G., Kobayashi, Y., & amp; Kuno, Y. (2015).
on Computer Vision and Pattern Recognition Workshops (CVPRW),
Supporting human–robot interaction based on the level of visual focus
2017.
of attention. IEEE Transactions on Human-Machine Systems, 45(6),
664–675.doi:10.1109/thms.2015.2445856 [35] K. Wang, R. Zhao, H. Su, and Q. Ji, “Generalizing eye tracking with
[15] Y. Wu, G. Li, Z. Liu, M. Huang and Y. Wang, ”Gaze Estimation via Bayesianadversariallearning,”2019IEEE/CVFConferenceonComputer
Modulation-Based Adaptive Network With Auxiliary Self-Learning,” in VisionandPatternRecognition(CVPR),2019
IEEE Transactions on Circuits and Systems for Video Technology, vol. [36] X. Zhou, J. Lin, Z. Zhang, Z. Shao, S. Chen, and H. Liu, “Improved
32,no.8,pp.5510-5520,Aug.2022,doi:10.1109/TCSVT.2022.3152800. itracker combined with bidirectional long short-term memory for 3D
[16] H. Admoni and B. Scassellati, “Social Eye Gaze in human-robot gaze estimation using appearance cues,” Neurocomputing, vol. 390, pp.
interaction:AReview,”JournalofHuman-RobotInteraction,vol.6,no. 217–225,2020.
1,p.25,2017. [37] Palmero,C.,Selva,J.,Bagheri,M.A.andEscalera,S.,2018.Recurrent
[17] Benoˆıt Masse´. Gaze direction in the context of social human-robot cnn for 3d gaze estimation using appearance and shape cues. arXiv
interaction. Artificial Intelligence [cs.AI]. Universite´ Grenoble Alpes, preprintarXiv:1805.03064.
2018.English.〈NNT:2018GREAM055〉.〈tel-01936821v2〉 [38] K. Wang, H. Su, and Q. Ji, “Neuro-inspired eye tracking with Eye
[18] R. Shaw, C. H. Sudre, S. Ourselin, and M. J. Cardoso, “A MovementDynamics,”2019IEEE/CVFConferenceonComputerVision
heteroscedastic uncertainty model for decoupling sources of andPatternRecognition(CVPR),2019.
MRI Image Quality,” arXiv.org, 31-Jan-2020. [Online]. Available: [39] P. Kellnhofer, A. Recasens, S. Stent, W. Matusik, and A. Torralba,
https://arxiv.org/abs/2001.11927.[Accessed:22-Nov-2022]. “Gaze360: Physically unconstrained gaze estimation in the wild,” 2019
[19] A. Kendall and Y. Gal, “What uncertainties do we need in bayesian IEEE/CVFInternationalConferenceonComputerVision(ICCV),2019.
deep learning for computer vision?,” arXiv.org, 05-Oct-2017. [Online]. [40] K. Krafka, A. Khosla, P. Kellnhofer, H. Kannan, S. Bhandarkar, W.
Available:https://arxiv.org/abs/1703.04977.[Accessed:14-June-2022]. Matusik, and A. Torralba, “Eye tracking for everyone,” 2016 IEEE
[20] K. Lenc and A. Vedaldi, “Understanding image representations by ConferenceonComputerVisionandPatternRecognition(CVPR),2016.
measuring their equivariance and equivalence,” International Journal of [41] Y.Cheng,X.Zhang,F.LuandY.Sato,”Gazeestimationbyexploring
ComputerVision,vol.127,no.5,pp.456–476,2018. two-eyeasymmetry”,IEEETrans.ImageProcess.,vol.29,pp.5259-5272,
[21] R. Balestriero and Y. LeCun, “Contrastive and non-contrastive self- 2020.
supervised learning recover global and local spectral embedding meth- [42] T.Fischer,H.J.ChangandY.Demiris,”RT-GENE:Real-timeeyegaze
ods,”arXiv.org,10-Jun-2022.[Online].[Accessed:07-Nov-2022]. estimationinnaturalenvironments”,Proc.ECCV,pp.339-357,2018.
[22] S. Albelwi, “Survey on Self-Supervised Learning: Auxiliary Pretext [43] C. Palmero, J. Selva, M. A. Bagheri, M. B. Ca and S. Escalera,
Tasks and Contrastive Learning Methods in Imaging,” Entropy, vol. 24, ”Recurrent CNN for 3D gaze estimation using appearance and shape
no.4,p.551,Apr.2022,doi:10.3390/e24040551. cues”,Proc.BMVC,pp.1-13,2018.
[23] P.Punyani,R.Gupta,andA.Kumar,“NeuralNetworksforFacialAge [44] Farkhondeh, A. et al. (2022) Towards self-supervised gaze estimation,
estimation:Asurveyonrecentadvances,”ArtificialIntelligenceReview, arXiv.org. Available at: https://arxiv.org/abs/2203.10974 (Accessed: Oc-
vol.53,no.5,pp.3299–3347,2019. tober11,2022).JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST**** 13
[45] Ohri,K.andKumar,M.(2021)“Reviewonself-supervisedimagerecog- Computing, vol. 13, no. 4, pp. 1868-1878, 1 Oct.-Dec. 2022, doi:
nitionusingDeepNeuralNetworks,”Knowledge-BasedSystems,224,p. 10.1109/TAFFC.2022.3197761.
107090.Availableat:https://doi.org/10.1016/j.knosys.2021.107090. [65] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
[46] Deldari,S.etal.(2022)Beyondjustvision:Areviewonself-supervised recognition,” 2016 IEEE Conference on Computer Vision and Pattern
representation learning on multimodal and Temporal Data, arXiv.org. Recognition(CVPR),2016.
Available at: https://arxiv.org/abs/2206.02353 (Accessed: December 16, [66] G.Huang,Z.Liu,L.VanDerMaatenandK.Q.Weinberger,”Densely
2023). Connected Convolutional Networks,” 2017 IEEE Conference on Com-
[47] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and puterVisionandPatternRecognition(CVPR),2017,pp.2261-2269,doi:
A. Joulin, “Unsupervised learning of visual features by contrast- 10.1109/CVPR.2017.243.
ing cluster assignments,” arXiv.org, 08-Jan-2021. [Online]. Available: [67] S. Liu and W. Deng, ”Very deep convolutional neural network based
https://arxiv.org/abs/2006.09882.[Accessed:17-Nov-2022]. image classification using small training sample size,” 2015 3rd IAPR
[48] N.Dubey,S.Ghosh,andA.Dhall,“Unsupervisedlearningofeyegaze Asian Conference on Pattern Recognition (ACPR), 2015, pp. 730-734,
representation from the web,” 2019 International Joint Conference on doi:10.1109/ACPR.2015.7486599.
NeuralNetworks(IJCNN),2019. [68] X.Zhang,Y.Sugano,M.Fritz,andA.Bulling,“Appearance-basedgaze
[49] Z.Wu,S.Rajendran,T.VanAs,V.Badrinarayanan,andA.Rabinovich, estimationinthewild,”inTheIEEEConferenceonComputerVisionand
“EyeNet: A multi-task deep network for off-axis eye gaze estimation,” PatternRecognition(CVPR),June2015.
2019IEEE/CVFInternationalConferenceonComputerVisionWorkshop
(ICCVW),2019.
[50] S.JyotiandA.Dhall,“Automaticeyegazeestimationusinggeometric
&amp;texture-basednetworks,”201824thInternationalConferenceon
PatternRecognition(ICPR),2018. Samuel Adebayo earned a First Class BSc in
[51] S.Li,W.Deng,andJ.P.Du,“Reliablecrowdsourcinganddeeplocality- Electronic and Computer Engineering from Lagos
preserving learning for expression recognition in the wild,” 2017 IEEE State University, Lagos Nigeria (2016-2020) and is
ConferenceonComputerVisionandPatternRecognition(CVPR),2017. currently pursuing his PhD in Machine Learning
[52] A.Mollahosseini,B.Hasani,andM.H.Mahoor,“AffectNet:Adatabase at Queen’s University Belfast, UK (2020-2024).
forfacialexpression,Valence,andarousalcomputinginthewild,”IEEE Samuel’sresearchisdeeplyrootedincomputational
TransactionsonAffectiveComputing,vol.10,no.1,pp.18–31,2019. intelligence, with a primary focus on the intri-
[53] X.Zhang,S.Park,T.Beeler,D.Bradley,S.Tang,andO.Hilliges,“ETH- catedynamicsofhumanhand-eyecoordinationand
XGaze:Alargescaledatasetforgazeestimationunderextremeheadpose its implications for human-robot interactions. This
andgazevariation,”ComputerVision–ECCV2020,pp.365–381,2020. aligns with his broader interest in advancing the
[54] L.SidenmarkandA.Lundstro¨m,“Gazebehaviouroninteractedobjects fields of Computer Vision, Deep Learning, Natural
during hand interaction in virtual reality for eye tracking calibration,” Language Processing, and Bayesian Machine Learning. Beyond academics,
Proceedingsofthe11thACMSymposiumonEyeTrackingResearch& Samuel transitioned from his role as the branch chair of the IEEE Student
amp;Applications,2019. Branch at Lagos State University to his current leadership position as the
[55] D.Trombetta,G.S.Rotithor,I.Salehi,andA.P.Dani,“Humanintention StudentBranchChairatQueen’sUniversityBelfast.
estimationusingfusionofpupilandhandmotion,”IFAC-PapersOnLine,
vol.53,no.2,pp.9535–9540,2020.
[56] “Pytorch-Lightning,” PyPI. [Online]. Available:
https://pypi.org/project/pytorch-lightning/.[Accessed:24-Nov-2022].
[57] dlib. (n.d.). ”dlib: A toolkit for making real world machine learning Joost C. Dessing receivedMSc.AndPhDdegrees
and data analysis applications in C++.” [Python library]. Available: inHumanMovementSciencesfromVrijeUniversity
http://dlib.net/python/index.html Amsterdam, Amsterdam, The Netherlands, in 2001
[58] K.A.FunesMora,F.Monay,andJ.-M.Odobez,“Eyediap,”Proceedings and 2005, respectively. He is currently a Lecturer
of the Symposium on Eye Tracking Research and Applications, 2014. at the School of Psychology of Queen’s University
doi:10.1145/2578153.2578190 Belfast, where he co-directs the Science in Motion
[59] A. A. Abdelrahman, T. Hempel, A. Khalifa, and A. lab. His research interests are fundamental and ap-
Al-Hamadi, “L2CS-net: Fine-grained gaze estimation in pliedinstancesofeye-handcoordination,withapar-
unconstrained environments,” NASA/ADS. [Online]. Available: ticularfocusonsportsandmanufacturingscenarios.
https://ui.adsabs.harvard.edu/abs/2022arXiv220303339A/abstract. He studies these using 3D motion tracking, virtual
[Accessed:10-Oct-2022]. reality,andneurophysiologicaltools.
[60] Z.ChenandB.E.Shi,“Appearance-basedgazeestimationusingdilated-
convolutions,” Computer Vision – ACCV 2018, pp. 309–324, 2019.
doi:10.1007/978-3-030-20876-9 20
[61] V. Nair and G. E. Hinton, “Rectified linear units improve
restricted boltzmann machines: Proceedings of the 27th Sea´nMcLoone(S′94–M′88–SM′02)receivedan
International Conference on International Conference on Machine M.E.degreeinElectricalandElectronicEngineering
Learning,” Guide Proceedings, 01-Jun-2010. [Online]. Available: and a PhD in Control Engineering from Queen’s
https://dl.acm.org/doi/10.5555/3104322.3104425. [Accessed: 24-Nov- UniversityBelfast,Belfast,U.K.in1992and1996,
2022]. respectively. He is currently a Professor and Di-
[62] L. Bottou, “Large-scale machine learning with stochastic gradi- rector of the Energy Power and Intelligent Control
ent descent,” Proceedings of COMPSTAT’2010, pp. 177–186, 2010. ResearchCentreatQueen’sUniversityBelfast.His
doi:10.1007/978-3-7908-2604-3 16 researchinterestsareinAppliedComputationalIn-
[63] Szegedy,C.,Ioffe,S.,Vanhoucke,V.,&Alemi,A.A.(2017).Inception- telligence and Machine Learning with a particular
v4,Inception-ResNetandtheimpactofresidualconnectionsonlearning. focus on data based modelling and analysis of
In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. dynamical systems, with applications in advanced
31,No.1). manufacturing informatics, energy and sustainability, connected health and
[64] J. Jiang and W. Deng, ”Disentangling Identity and Pose for Fa- assisted living technologies. Prof. McLoone is a Chartered Engineer and
cial Expression Recognition,” in IEEE Transactions on Affective Fellow of the Institution of Engineering and Technology. He is a Past
ChairmanoftheUKandRepublicofIreland(UKRI)SectionoftheIEEE.