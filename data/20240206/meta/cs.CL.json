[
    {
        "title": "Position Paper: Generalized grammar rules and structure-based generalization beyond classical equivariance for lexical tasks and transduction",
        "authors": "Mircea PetracheShubhendu Trivedi",
        "links": "http://arxiv.org/abs/2402.01629v1",
        "entry_id": "http://arxiv.org/abs/2402.01629v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01629v1",
        "summary": "Compositional generalization is one of the main properties which\ndifferentiates lexical learning in humans from state-of-art neural networks. We\npropose a general framework for building models that can generalize\ncompositionally using the concept of Generalized Grammar Rules (GGRs), a class\nof symmetry-based compositional constraints for transduction tasks, which we\nview as a transduction analogue of equivariance constraints in physics-inspired\ntasks. Besides formalizing generalized notions of symmetry for language\ntransduction, our framework is general enough to contain many existing works as\nspecial cases. We present ideas on how GGRs might be implemented, and in the\nprocess draw connections to reinforcement learning and other areas of research.",
        "updated": "2024-02-02 18:44:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01629v1"
    },
    {
        "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "authors": "Jian XieKai ZhangJiangjie ChenTinghui ZhuRenze LouYuandong TianYanghua XiaoYu Su",
        "links": "http://arxiv.org/abs/2402.01622v2",
        "entry_id": "http://arxiv.org/abs/2402.01622v2",
        "pdf_url": "http://arxiv.org/pdf/2402.01622v2",
        "summary": "Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.",
        "updated": "2024-02-05 06:48:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01622v2"
    },
    {
        "title": "MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models",
        "authors": "Justin Chih-Yao ChenSwarnadeep SahaElias Stengel-EskinMohit Bansal",
        "links": "http://arxiv.org/abs/2402.01620v1",
        "entry_id": "http://arxiv.org/abs/2402.01620v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01620v1",
        "summary": "Multi-agent interactions between Large Language Model (LLM) agents have shown\nmajor improvements on diverse reasoning tasks. However, these involve long\ngenerations from multiple models across several rounds, making them expensive.\nMoreover, these multi-agent approaches fail to provide a final, single model\nfor efficient inference. To address this, we introduce MAGDi, a new method for\nstructured distillation of the reasoning interactions between multiple LLMs\ninto smaller LMs. MAGDi teaches smaller models by representing multi-agent\ninteractions as graphs, augmenting a base student model with a graph encoder,\nand distilling knowledge using three objective functions: next-token\nprediction, a contrastive loss between correct and incorrect reasoning, and a\ngraph-based objective to model the interaction structure. Experiments on seven\nwidely-used commonsense and math reasoning benchmarks show that MAGDi improves\nthe reasoning capabilities of smaller models, outperforming several methods\nthat distill from a single teacher and multiple teachers. Moreover, MAGDi also\ndemonstrates an order of magnitude higher efficiency over its teachers. We\nconduct extensive analyses to show that MAGDi (1) enhances the generalizability\nto out-of-domain tasks, (2) scales positively with the size and strength of the\nbase student model, and (3) obtains larger improvements (via our multi-teacher\ntraining) when applying self-consistency - an inference technique that relies\non model diversity.",
        "updated": "2024-02-02 18:35:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01620v1"
    },
    {
        "title": "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases",
        "authors": "Jiajie ZhangShulin CaoLinmei HuLing FengLei HouJuanzi Li",
        "links": "http://arxiv.org/abs/2402.01619v1",
        "entry_id": "http://arxiv.org/abs/2402.01619v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01619v1",
        "summary": "Program induction (PI) has become a promising paradigm for using knowledge\nbases (KBs) to help large language models (LLMs) answer complex\nknowledge-intensive questions. Nonetheless, PI typically relies on a large\nnumber of parallel question-program pairs to make the LLM aware of the schema\nof the given KB, and is thus challenging for many low-resourced KBs that lack\nannotated data. To this end, we propose KB-Plugin, a plug-and-play framework\nthat enables LLMs to induce programs over any low-resourced KB. Firstly,\nKB-Plugin adopts self-supervised learning to encode the detailed schema\ninformation of a given KB into a pluggable module, namely schema plugin.\nSecondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB\nto train another pluggable module, namely PI plugin, which can help the LLM\nextract question-relevant schema information from the schema plugin of any KB\nand utilize this information to induce programs over this KB. Experiments on\nfive heterogeneous KBQA datasets show that KB-Plugin achieves better or\ncomparable performance with 25$\\times$ smaller backbone LLM compared to SoTA PI\nmethods for low-resourced KBs, and even approaches the performance of\nsupervised methods. Our code and data are available at\nhttps://github.com/THU-KEG/KB-Plugin.",
        "updated": "2024-02-02 18:32:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01619v1"
    },
    {
        "title": "Style Vectors for Steering Generative Large Language Model",
        "authors": "Kai KonenSophie JentzschDiaoulé DialloPeer SchüttOliver BenschRoxanne El BaffDominik OpitzTobias Hecking",
        "links": "http://arxiv.org/abs/2402.01618v1",
        "entry_id": "http://arxiv.org/abs/2402.01618v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01618v1",
        "summary": "This research explores strategies for steering the output of large language\nmodels (LLMs) towards specific styles, such as sentiment, emotion, or writing\nstyle, by adding style vectors to the activations of hidden layers during text\ngeneration. We show that style vectors can be simply computed from recorded\nlayer activations for input texts in a specific style in contrast to more\ncomplex training-based approaches. Through a series of experiments, we\ndemonstrate the effectiveness of activation engineering using such style\nvectors to influence the style of generated text in a nuanced and\nparameterisable way, distinguishing it from prompt engineering. The presented\nresearch constitutes a significant step towards developing more adaptive and\neffective AI-empowered interactive systems.",
        "updated": "2024-02-02 18:31:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01618v1"
    }
]