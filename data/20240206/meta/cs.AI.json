[
    {
        "title": "L2G2G: a Scalable Local-to-Global Network Embedding with Graph Autoencoders",
        "authors": "Ruikang OuyangAndrew ElliottStratis LimniosMihai CucuringuGesine Reinert",
        "links": "http://arxiv.org/abs/2402.01614v1",
        "entry_id": "http://arxiv.org/abs/2402.01614v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01614v1",
        "summary": "For analysing real-world networks, graph representation learning is a popular\ntool. These methods, such as a graph autoencoder (GAE), typically rely on\nlow-dimensional representations, also called embeddings, which are obtained\nthrough minimising a loss function; these embeddings are used with a decoder\nfor downstream tasks such as node classification and edge prediction. While\nGAEs tend to be fairly accurate, they suffer from scalability issues. For\nimproved speed, a Local2Global approach, which combines graph patch embeddings\nbased on eigenvector synchronisation, was shown to be fast and achieve good\naccuracy. Here we propose L2G2G, a Local2Global method which improves GAE\naccuracy without sacrificing scalability. This improvement is achieved by\ndynamically synchronising the latent node representations, while training the\nGAEs. It also benefits from the decoder computing an only local patch loss.\nHence, aligning the local embeddings in each epoch utilises more information\nfrom the graph than a single post-training alignment does, while maintaining\nscalability. We illustrate on synthetic benchmarks, as well as real-world\nexamples, that L2G2G achieves higher accuracy than the standard Local2Global\napproach and scales efficiently on the larger data sets. We find that for large\nand dense networks, it even outperforms the slow, but assumed more accurate,\nGAEs.",
        "updated": "2024-02-02 18:24:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01614v1"
    },
    {
        "title": "Nomic Embed: Training a Reproducible Long Context Text Embedder",
        "authors": "Zach NussbaumJohn X. MorrisBrandon DuderstadtAndriy Mulyar",
        "links": "http://arxiv.org/abs/2402.01613v1",
        "entry_id": "http://arxiv.org/abs/2402.01613v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01613v1",
        "summary": "This technical report describes the training of nomic-embed-text-v1, the\nfirst fully reproducible, open-source, open-weights, open-data, 8192 context\nlength English text embedding model that outperforms both OpenAI Ada-002 and\nOpenAI text-embedding-3-small on short and long-context tasks. We release the\ntraining code and model weights under an Apache 2 license. In contrast with\nother open-source models, we release a training data loader with 235 million\ncurated text pairs that allows for the full replication of nomic-embed-text-v1.\nYou can find code and data to replicate the model at\nhttps://github.com/nomic-ai/contrastors",
        "updated": "2024-02-02 18:23:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01613v1"
    },
    {
        "title": "Natural Counterfactuals With Necessary Backtracking",
        "authors": "Guang-Yuan HaoJiji ZhangBiwei HuangHao WangKun Zhang",
        "links": "http://arxiv.org/abs/2402.01607v1",
        "entry_id": "http://arxiv.org/abs/2402.01607v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01607v1",
        "summary": "Counterfactual reasoning is pivotal in human cognition and especially\nimportant for providing explanations and making decisions. While Judea Pearl's\ninfluential approach is theoretically elegant, its generation of a\ncounterfactual scenario often requires interventions that are too detached from\nthe real scenarios to be feasible. In response, we propose a framework of\nnatural counterfactuals and a method for generating counterfactuals that are\nnatural with respect to the actual world's data distribution. Our methodology\nrefines counterfactual reasoning, allowing changes in causally preceding\nvariables to minimize deviations from realistic scenarios. To generate natural\ncounterfactuals, we introduce an innovative optimization framework that permits\nbut controls the extent of backtracking with a naturalness criterion. Empirical\nexperiments indicate the effectiveness of our method.",
        "updated": "2024-02-02 18:11:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01607v1"
    },
    {
        "title": "Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning",
        "authors": "Debarun BhattacharjyaJunkyu LeeDon Joven AgravanteBalaji GanesanRadu Marinescu",
        "links": "http://arxiv.org/abs/2402.01602v1",
        "entry_id": "http://arxiv.org/abs/2402.01602v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01602v1",
        "summary": "Foundation models (FMs) such as large language models have revolutionized the\nfield of AI by showing remarkable performance in various tasks. However, they\nexhibit numerous limitations that prevent their broader adoption in many\nreal-world systems, which often require a higher bar for trustworthiness and\nusability. Since FMs are trained using loss functions aimed at reconstructing\nthe training corpus in a self-supervised manner, there is no guarantee that the\nmodel's output aligns with users' preferences for a specific task at hand. In\nthis survey paper, we propose a conceptual framework that encapsulates\ndifferent modes by which agents could interact with FMs and guide them suitably\nfor a set of tasks, particularly through knowledge augmentation and reasoning.\nOur framework elucidates agent role categories such as updating the underlying\nFM, assisting with prompting the FM, and evaluating the FM output. We also\ncategorize several state-of-the-art approaches into agent interaction\nprotocols, highlighting the nature and extent of involvement of the various\nagent roles. The proposed framework provides guidance for future directions to\nfurther realize the power of FMs in practical AI systems.",
        "updated": "2024-02-02 18:00:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01602v1"
    },
    {
        "title": "BAT: Learning to Reason about Spatial Sounds with Large Language Models",
        "authors": "Zhisheng ZhengPuyuan PengZiyang MaXie ChenEunsol ChoiDavid Harwath",
        "links": "http://arxiv.org/abs/2402.01591v1",
        "entry_id": "http://arxiv.org/abs/2402.01591v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01591v1",
        "summary": "Spatial sound reasoning is a fundamental human skill, enabling us to navigate\nand interpret our surroundings based on sound. In this paper we present BAT,\nwhich combines the spatial sound perception ability of a binaural acoustic\nscene analysis model with the natural language reasoning capabilities of a\nlarge language model (LLM) to replicate this innate ability. To address the\nlack of existing datasets of in-the-wild spatial sounds, we synthesized a\nbinaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed\nSpatialSoundQA, a spatial sound-based question-answering dataset, offering a\nrange of QA tasks that train BAT in various aspects of spatial sound perception\nand reasoning. The acoustic front end encoder of BAT is a novel spatial audio\nencoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by\nitself achieves strong performance across sound event detection, spatial\nlocalization, and distance estimation. By integrating Spatial-AST with LLaMA-2\n7B model, BAT transcends standard Sound Event Localization and Detection (SELD)\ntasks, enabling the model to reason about the relationships between the sounds\nin its environment. Our experiments demonstrate BAT's superior performance on\nboth spatial sound perception and reasoning, showcasing the immense potential\nof LLMs in navigating and interpreting complex spatial audio environments.",
        "updated": "2024-02-02 17:34:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01591v1"
    }
]