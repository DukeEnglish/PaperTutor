[
    {
        "title": "kNN Algorithm for Conditional Mean and Variance Estimation with Automated Uncertainty Quantification and Variable Selection",
        "authors": "Marcos MatabuenaJuan C. VidalOscar Hernan Madrid PadillaJukka-Pekka Onnela",
        "links": "http://arxiv.org/abs/2402.01635v1",
        "entry_id": "http://arxiv.org/abs/2402.01635v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01635v1",
        "summary": "In this paper, we introduce a kNN-based regression method that synergizes the\nscalability and adaptability of traditional non-parametric kNN models with a\nnovel variable selection technique. This method focuses on accurately\nestimating the conditional mean and variance of random response variables,\nthereby effectively characterizing conditional distributions across diverse\nscenarios.Our approach incorporates a robust uncertainty quantification\nmechanism, leveraging our prior estimation work on conditional mean and\nvariance. The employment of kNN ensures scalable computational efficiency in\npredicting intervals and statistical accuracy in line with optimal\nnon-parametric rates. Additionally, we introduce a new kNN semi-parametric\nalgorithm for estimating ROC curves, accounting for covariates. For selecting\nthe smoothing parameter k, we propose an algorithm with theoretical\nguarantees.Incorporation of variable selection enhances the performance of the\nmethod significantly over conventional kNN techniques in various modeling\ntasks. We validate the approach through simulations in low, moderate, and\nhigh-dimensional covariate spaces. The algorithm's effectiveness is\nparticularly notable in biomedical applications as demonstrated in two case\nstudies. Concluding with a theoretical analysis, we highlight the consistency\nand convergence rate of our method over traditional kNN models, particularly\nwhen the underlying regression model takes values in a low-dimensional space.",
        "updated": "2024-02-02 18:54:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01635v1"
    },
    {
        "title": "Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type",
        "authors": "Juliusz ZiomekMasaki AdachiMichael A. Osborne",
        "links": "http://arxiv.org/abs/2402.01632v1",
        "entry_id": "http://arxiv.org/abs/2402.01632v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01632v1",
        "summary": "Bayesian optimisation requires fitting a Gaussian process model, which in\nturn requires specifying hyperparameters - most of the theoretical literature\nassumes those hyperparameters are known. The commonly used maximum likelihood\nestimator for hyperparameters of the Gaussian process is consistent only if the\ndata fills the space uniformly, which does not have to be the case in Bayesian\noptimisation. Since no guarantees exist regarding the correctness of\nhyperparameter estimation, and those hyperparameters can significantly affect\nthe Gaussian process fit, theoretical analysis of Bayesian optimisation with\nunknown hyperparameters is very challenging. Previously proposed algorithms\nwith the no-regret property were only able to handle the special case of\nunknown lengthscales, reproducing kernel Hilbert space norm and applied only to\nthe frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the\nfirst algorithm enjoying the no-regret property in the case of unknown\nhyperparameters of arbitrary form, and which supports both Bayesian and\nfrequentist settings. Our proof idea is novel and can easily be extended to\nother variants of Bayesian optimisation. We show this by extending our\nalgorithm to the adversarially robust optimisation setting under unknown\nhyperparameters. Finally, we empirically evaluate our algorithm on a set of toy\nproblems and show that it can outperform the maximum likelihood estimator.",
        "updated": "2024-02-02 18:52:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01632v1"
    },
    {
        "title": "Position Paper: Generalized grammar rules and structure-based generalization beyond classical equivariance for lexical tasks and transduction",
        "authors": "Mircea PetracheShubhendu Trivedi",
        "links": "http://arxiv.org/abs/2402.01629v1",
        "entry_id": "http://arxiv.org/abs/2402.01629v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01629v1",
        "summary": "Compositional generalization is one of the main properties which\ndifferentiates lexical learning in humans from state-of-art neural networks. We\npropose a general framework for building models that can generalize\ncompositionally using the concept of Generalized Grammar Rules (GGRs), a class\nof symmetry-based compositional constraints for transduction tasks, which we\nview as a transduction analogue of equivariance constraints in physics-inspired\ntasks. Besides formalizing generalized notions of symmetry for language\ntransduction, our framework is general enough to contain many existing works as\nspecial cases. We present ideas on how GGRs might be implemented, and in the\nprocess draw connections to reinforcement learning and other areas of research.",
        "updated": "2024-02-02 18:44:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01629v1"
    },
    {
        "title": "Stochastic Two Points Method for Deep Model Zeroth-order Optimization",
        "authors": "Yijiang PangJiayu Zhou",
        "links": "http://arxiv.org/abs/2402.01621v1",
        "entry_id": "http://arxiv.org/abs/2402.01621v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01621v1",
        "summary": "Large foundation models, such as large language models, have performed\nexceptionally well in various application scenarios. Building or fully\nfine-tuning such large models is usually prohibitive due to either hardware\nbudget or lack of access to backpropagation. The zeroth-order methods offer a\npromising direction for tackling this challenge, where only forward passes are\nneeded to update the model. This paper introduces an efficient Stochastic\nTwo-Point (S2P) approach within the gradient-free regime. We present the\ntheoretical convergence properties of S2P under the general and relaxed\nsmoothness assumptions. The theoretical properties also shed light on a faster\nand more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new\nconvergence properties that better represent the dynamics of deep models in\ntraining. Our comprehensive empirical results show that AS2P is highly\neffective in optimizing objectives for large deep models, including language\nmodels, and outperforms standard methods across various model types and scales,\nwith 2 $\\times$ speed-up in training over most conducted tasks.",
        "updated": "2024-02-02 18:39:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01621v1"
    },
    {
        "title": "A GP-based Robust Motion Planning Framework for Agile Autonomous Robot Navigation and Recovery in Unknown Environments",
        "authors": "Nicholas MohammadJacob HigginsNicola Bezzo",
        "links": "http://arxiv.org/abs/2402.01617v1",
        "entry_id": "http://arxiv.org/abs/2402.01617v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01617v1",
        "summary": "For autonomous mobile robots, uncertainties in the environment and system\nmodel can lead to failure in the motion planning pipeline, resulting in\npotential collisions. In order to achieve a high level of robust autonomy,\nthese robots should be able to proactively predict and recover from such\nfailures. To this end, we propose a Gaussian Process (GP) based model for\nproactively detecting the risk of future motion planning failure. When this\nrisk exceeds a certain threshold, a recovery behavior is triggered that\nleverages the same GP model to find a safe state from which the robot may\ncontinue towards the goal. The proposed approach is trained in simulation only\nand can generalize to real world environments on different robotic platforms.\nSimulations and physical experiments demonstrate that our framework is capable\nof both predicting planner failures and recovering the robot to states where\nplanner success is likely, all while producing agile motion.",
        "updated": "2024-02-02 18:27:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01617v1"
    }
]