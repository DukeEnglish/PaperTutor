[
    {
        "title": "Immersive Video Compression using Implicit Neural Representations",
        "authors": "Ho Man KwanFan ZhangAndrew GowerDavid Bull",
        "links": "http://arxiv.org/abs/2402.01596v1",
        "entry_id": "http://arxiv.org/abs/2402.01596v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01596v1",
        "summary": "Recent work on implicit neural representations (INRs) has evidenced their\npotential for efficiently representing and encoding conventional video content.\nIn this paper we, for the first time, extend their application to immersive\n(multi-view) videos, by proposing MV-HiNeRV, a new INR-based immersive video\ncodec. MV-HiNeRV is an enhanced version of a state-of-the-art INR-based video\ncodec, HiNeRV, which was developed for single-view video compression. We have\nmodified the model to learn a different group of feature grids for each view,\nand share the learnt network parameters among all views. This enables the model\nto effectively exploit the spatio-temporal and the inter-view redundancy that\nexists within multi-view videos. The proposed codec was used to compress\nmulti-view texture and depth video sequences in the MPEG Immersive Video (MIV)\nCommon Test Conditions, and tested against the MIV Test model (TMIV) that uses\nthe VVenC video codec. The results demonstrate the superior performance of\nMV-HiNeRV, with significant coding gains (up to 72.33%) over TMIV. The\nimplementation of MV-HiNeRV will be published for further development and\nevaluation.",
        "updated": "2024-02-02 17:49:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01596v1"
    },
    {
        "title": "NeuroCine: Decoding Vivid Video Sequences from Human Brain Activties",
        "authors": "Jingyuan SunMingxiao LiZijiao ChenMarie-Francine Moens",
        "links": "http://arxiv.org/abs/2402.01590v1",
        "entry_id": "http://arxiv.org/abs/2402.01590v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01590v1",
        "summary": "In the pursuit to understand the intricacies of human brain's visual\nprocessing, reconstructing dynamic visual experiences from brain activities\nemerges as a challenging yet fascinating endeavor. While recent advancements\nhave achieved success in reconstructing static images from non-invasive brain\nrecordings, the domain of translating continuous brain activities into video\nformat remains underexplored. In this work, we introduce NeuroCine, a novel\ndual-phase framework to targeting the inherent challenges of decoding fMRI\ndata, such as noises, spatial redundancy and temporal lags. This framework\nproposes spatial masking and temporal interpolation-based augmentation for\ncontrastive learning fMRI representations and a diffusion model enhanced by\ndependent prior noise for video generation. Tested on a publicly available fMRI\ndataset, our method shows promising results, outperforming the previous\nstate-of-the-art models by a notable margin of ${20.97\\%}$, ${31.00\\%}$ and\n${12.30\\%}$ respectively on decoding the brain activities of three subjects in\nthe fMRI dataset, as measured by SSIM. Additionally, our attention analysis\nsuggests that the model aligns with existing brain structures and functions,\nindicating its biological plausibility and interpretability.",
        "updated": "2024-02-02 17:34:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01590v1"
    },
    {
        "title": "Boximator: Generating Rich and Controllable Motions for Video Synthesis",
        "authors": "Jiawei WangYuchen ZhangJiaxin ZouYan ZengGuoqiang WeiLiping YuanHang Li",
        "links": "http://arxiv.org/abs/2402.01566v1",
        "entry_id": "http://arxiv.org/abs/2402.01566v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01566v1",
        "summary": "Generating rich and controllable motion is a pivotal challenge in video\nsynthesis. We propose Boximator, a new approach for fine-grained motion\ncontrol. Boximator introduces two constraint types: hard box and soft box.\nUsers select objects in the conditional frame using hard boxes and then use\neither type of boxes to roughly or rigorously define the object's position,\nshape, or motion path in future frames. Boximator functions as a plug-in for\nexisting video diffusion models. Its training process preserves the base\nmodel's knowledge by freezing the original weights and training only the\ncontrol module. To address training challenges, we introduce a novel\nself-tracking technique that greatly simplifies the learning of box-object\ncorrelations. Empirically, Boximator achieves state-of-the-art video quality\n(FVD) scores, improving on two base models, and further enhanced after\nincorporating box constraints. Its robust motion controllability is validated\nby drastic increases in the bounding box alignment metric. Human evaluation\nalso shows that users favor Boximator generation results over the base model.",
        "updated": "2024-02-02 16:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01566v1"
    },
    {
        "title": "Deep Continuous Networks",
        "authors": "Nergis TomenSilvia L. PinteaJan C. van Gemert",
        "links": "http://arxiv.org/abs/2402.01557v1",
        "entry_id": "http://arxiv.org/abs/2402.01557v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01557v1",
        "summary": "CNNs and computational models of biological vision share some fundamental\nprinciples, which opened new avenues of research. However, fruitful cross-field\nresearch is hampered by conventional CNN architectures being based on spatially\nand depthwise discrete representations, which cannot accommodate certain\naspects of biological complexity such as continuously varying receptive field\nsizes and dynamics of neuronal responses. Here we propose deep continuous\nnetworks (DCNs), which combine spatially continuous filters, with the\ncontinuous depth framework of neural ODEs. This allows us to learn the spatial\nsupport of the filters during training, as well as model the continuous\nevolution of feature maps, linking DCNs closely to biological models. We show\nthat DCNs are versatile and highly applicable to standard image classification\nand reconstruction problems, where they improve parameter and data efficiency,\nand allow for meta-parametrization. We illustrate the biological plausibility\nof the scale distributions learned by DCNs and explore their performance in a\nneuroscientifically inspired pattern completion task. Finally, we investigate\nan efficient implementation of DCNs by changing input contrast.",
        "updated": "2024-02-02 16:50:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01557v1"
    },
    {
        "title": "SLYKLatent, a Learning Framework for Facial Features Estimation",
        "authors": "Samuel AdebayoJoost C. DessingSeán McLoone",
        "links": "http://arxiv.org/abs/2402.01555v1",
        "entry_id": "http://arxiv.org/abs/2402.01555v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01555v1",
        "summary": "In this research, we present SLYKLatent, a novel approach for enhancing gaze\nestimation by addressing appearance instability challenges in datasets due to\naleatoric uncertainties, covariant shifts, and test domain generalization.\nSLYKLatent utilizes Self-Supervised Learning for initial training with facial\nexpression datasets, followed by refinement with a patch-based tri-branch\nnetwork and an inverse explained variance-weighted training loss function. Our\nevaluation on benchmark datasets achieves an 8.7% improvement on Gaze360,\nrivals top MPIIFaceGaze results, and leads on a subset of ETH-XGaze by 13%,\nsurpassing existing methods by significant margins. Adaptability tests on\nRAF-DB and Affectnet show 86.4% and 60.9% accuracies, respectively. Ablation\nstudies confirm the effectiveness of SLYKLatent's novel components. This\napproach has strong potential in human-robot interaction.",
        "updated": "2024-02-02 16:47:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01555v1"
    }
]