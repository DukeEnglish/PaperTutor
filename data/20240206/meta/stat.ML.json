[
    {
        "title": "kNN Algorithm for Conditional Mean and Variance Estimation with Automated Uncertainty Quantification and Variable Selection",
        "authors": "Marcos MatabuenaJuan C. VidalOscar Hernan Madrid PadillaJukka-Pekka Onnela",
        "links": "http://arxiv.org/abs/2402.01635v1",
        "entry_id": "http://arxiv.org/abs/2402.01635v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01635v1",
        "summary": "In this paper, we introduce a kNN-based regression method that synergizes the\nscalability and adaptability of traditional non-parametric kNN models with a\nnovel variable selection technique. This method focuses on accurately\nestimating the conditional mean and variance of random response variables,\nthereby effectively characterizing conditional distributions across diverse\nscenarios.Our approach incorporates a robust uncertainty quantification\nmechanism, leveraging our prior estimation work on conditional mean and\nvariance. The employment of kNN ensures scalable computational efficiency in\npredicting intervals and statistical accuracy in line with optimal\nnon-parametric rates. Additionally, we introduce a new kNN semi-parametric\nalgorithm for estimating ROC curves, accounting for covariates. For selecting\nthe smoothing parameter k, we propose an algorithm with theoretical\nguarantees.Incorporation of variable selection enhances the performance of the\nmethod significantly over conventional kNN techniques in various modeling\ntasks. We validate the approach through simulations in low, moderate, and\nhigh-dimensional covariate spaces. The algorithm's effectiveness is\nparticularly notable in biomedical applications as demonstrated in two case\nstudies. Concluding with a theoretical analysis, we highlight the consistency\nand convergence rate of our method over traditional kNN models, particularly\nwhen the underlying regression model takes values in a low-dimensional space.",
        "updated": "2024-02-02 18:54:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01635v1"
    },
    {
        "title": "Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type",
        "authors": "Juliusz ZiomekMasaki AdachiMichael A. Osborne",
        "links": "http://arxiv.org/abs/2402.01632v1",
        "entry_id": "http://arxiv.org/abs/2402.01632v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01632v1",
        "summary": "Bayesian optimisation requires fitting a Gaussian process model, which in\nturn requires specifying hyperparameters - most of the theoretical literature\nassumes those hyperparameters are known. The commonly used maximum likelihood\nestimator for hyperparameters of the Gaussian process is consistent only if the\ndata fills the space uniformly, which does not have to be the case in Bayesian\noptimisation. Since no guarantees exist regarding the correctness of\nhyperparameter estimation, and those hyperparameters can significantly affect\nthe Gaussian process fit, theoretical analysis of Bayesian optimisation with\nunknown hyperparameters is very challenging. Previously proposed algorithms\nwith the no-regret property were only able to handle the special case of\nunknown lengthscales, reproducing kernel Hilbert space norm and applied only to\nthe frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the\nfirst algorithm enjoying the no-regret property in the case of unknown\nhyperparameters of arbitrary form, and which supports both Bayesian and\nfrequentist settings. Our proof idea is novel and can easily be extended to\nother variants of Bayesian optimisation. We show this by extending our\nalgorithm to the adversarially robust optimisation setting under unknown\nhyperparameters. Finally, we empirically evaluate our algorithm on a set of toy\nproblems and show that it can outperform the maximum likelihood estimator.",
        "updated": "2024-02-02 18:52:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01632v1"
    },
    {
        "title": "Position Paper: Generalized grammar rules and structure-based generalization beyond classical equivariance for lexical tasks and transduction",
        "authors": "Mircea PetracheShubhendu Trivedi",
        "links": "http://arxiv.org/abs/2402.01629v1",
        "entry_id": "http://arxiv.org/abs/2402.01629v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01629v1",
        "summary": "Compositional generalization is one of the main properties which\ndifferentiates lexical learning in humans from state-of-art neural networks. We\npropose a general framework for building models that can generalize\ncompositionally using the concept of Generalized Grammar Rules (GGRs), a class\nof symmetry-based compositional constraints for transduction tasks, which we\nview as a transduction analogue of equivariance constraints in physics-inspired\ntasks. Besides formalizing generalized notions of symmetry for language\ntransduction, our framework is general enough to contain many existing works as\nspecial cases. We present ideas on how GGRs might be implemented, and in the\nprocess draw connections to reinforcement learning and other areas of research.",
        "updated": "2024-02-02 18:44:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01629v1"
    },
    {
        "title": "L2G2G: a Scalable Local-to-Global Network Embedding with Graph Autoencoders",
        "authors": "Ruikang OuyangAndrew ElliottStratis LimniosMihai CucuringuGesine Reinert",
        "links": "http://arxiv.org/abs/2402.01614v1",
        "entry_id": "http://arxiv.org/abs/2402.01614v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01614v1",
        "summary": "For analysing real-world networks, graph representation learning is a popular\ntool. These methods, such as a graph autoencoder (GAE), typically rely on\nlow-dimensional representations, also called embeddings, which are obtained\nthrough minimising a loss function; these embeddings are used with a decoder\nfor downstream tasks such as node classification and edge prediction. While\nGAEs tend to be fairly accurate, they suffer from scalability issues. For\nimproved speed, a Local2Global approach, which combines graph patch embeddings\nbased on eigenvector synchronisation, was shown to be fast and achieve good\naccuracy. Here we propose L2G2G, a Local2Global method which improves GAE\naccuracy without sacrificing scalability. This improvement is achieved by\ndynamically synchronising the latent node representations, while training the\nGAEs. It also benefits from the decoder computing an only local patch loss.\nHence, aligning the local embeddings in each epoch utilises more information\nfrom the graph than a single post-training alignment does, while maintaining\nscalability. We illustrate on synthetic benchmarks, as well as real-world\nexamples, that L2G2G achieves higher accuracy than the standard Local2Global\napproach and scales efficiently on the larger data sets. We find that for large\nand dense networks, it even outperforms the slow, but assumed more accurate,\nGAEs.",
        "updated": "2024-02-02 18:24:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01614v1"
    },
    {
        "title": "Hyperparameter tuning via trajectory predictions: Stochastic prox-linear methods in matrix sensing",
        "authors": "Mengqi LouKabir Aladin VerchandAshwin Pananjady",
        "links": "http://arxiv.org/abs/2402.01599v1",
        "entry_id": "http://arxiv.org/abs/2402.01599v1",
        "pdf_url": "http://arxiv.org/pdf/2402.01599v1",
        "summary": "Motivated by the desire to understand stochastic algorithms for nonconvex\noptimization that are robust to their hyperparameter choices, we analyze a\nmini-batched prox-linear iterative algorithm for the problem of recovering an\nunknown rank-1 matrix from rank-1 Gaussian measurements corrupted by noise. We\nderive a deterministic recursion that predicts the error of this method and\nshow, using a non-asymptotic framework, that this prediction is accurate for\nany batch-size and a large range of step-sizes. In particular, our analysis\nreveals that this method, though stochastic, converges linearly from a local\ninitialization with a fixed step-size to a statistical error floor. Our\nanalysis also exposes how the batch-size, step-size, and noise level affect the\n(linear) convergence rate and the eventual statistical estimation error, and we\ndemonstrate how to use our deterministic predictions to perform hyperparameter\ntuning (e.g. step-size and batch-size selection) without ever running the\nmethod. On a technical level, our analysis is enabled in part by showing that\nthe fluctuations of the empirical iterates around our deterministic predictions\nscale with the error of the previous iterate.",
        "updated": "2024-02-02 17:55:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.01599v1"
    }
]