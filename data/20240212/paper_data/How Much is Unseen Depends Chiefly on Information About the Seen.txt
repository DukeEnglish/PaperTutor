How Much is Unseen Depends Chiefly on Information About the Seen
SeongminLee1 MarcelBo¬®hme1
Abstract 1.1.Background
Itmightseemcounter-intuitiveatfirst: Wefind Consideramultinomialdistributionp=‚ü®p 1,¬∑¬∑¬∑p S‚ü©overa
that,inexpectation,theproportionofdatapoints supportsetX wheresupportsizeS =|X|andprobability
inanunknownpopulation‚Äîthatbelongtoclasses values are unknown. Let Xn = ‚ü®X 1,¬∑¬∑¬∑X n‚ü© be a set of
thatdonotappearinthetrainingdata‚Äîisalmost independent and identically distributed random variables
entirelydeterminedbythenumberf ofclasses representingthesequenceofelementsobservedinnsam-
k
thatdoappearinthetrainingdatathesamenum- plesfromp. LetN xbethenumberoftimeselementx‚ààX
ber of times. While in theory we show that the isobservedinthesampleXn. Fork :0‚â§k ‚â§n,letŒ¶ k be
differenceoftheinducedestimatordecaysexpo-
thenumberofelementsappearingexactlyktimesinXn.
nentially in the size of the sample, in practice
n
(cid:88) (cid:88)
the high variance prevents us from using it di- N = 1(X =x) and Œ¶ = 1(N =k)
x i k x
rectly for an estimator of the sample coverage.
i=1 x‚ààX
However,ourprecisecharacterizationofthede-
Letf (n)betheexpectedvalueofŒ¶ (Good,1953),i.e.,
pendency between f ‚Äôs induces a large search k k
k
spaceofdifferentrepresentationsoftheexpected (cid:18) n(cid:19)
(cid:88)
value,whichcanbedeterministicallyinstantiated f k(n)=
k
pk x(1‚àíp x)n‚àík =E[Œ¶ k]
asestimators. Hence,weturntooptimizationand x‚ààX
developageneticalgorithmthat,givenonlythe
Estimatingthemultinomial.Suppose,wewanttoestimate
sample,searchesforanestimatorwithminimal
p. WecannotexpectallelementstoexistinXn. Whilethe
mean-squarederror(MSE).Inourexperiments,
empirical estimator pÀÜEmp = N /n is generally unbiased,
our genetic algorithm discovers estimators that x x
haveasubstantiallysmallerMSEthanthestate-of- pÀÜE xmp distributes the entire probability mass only over the
the-artGood-Turingestimator.Thisholdsforover observedelements.Thisleavesa‚Äúmissingprobabilitymass‚Äù
96%ofrunswhenthereareatleastasmanysam- overtheunobservedelements. Inparticular,pÀÜE xmpgiventhat
plesasclasses. Ourestimators‚ÄôMSEisroughly N x >0overestimatesp x,i.e.,forobservedelements
80%oftheGood-Turingestimator‚Äôs. (cid:20) (cid:12) (cid:21)
E N nx (cid:12) (cid:12) (cid:12) N x >0 = 1‚àí(1p ‚àíx p )n.
x
1.Introduction Wenoticethatthebiasincreasesasp x decreases. Biasis
maximizedfortherarestobservedelement.
Suppose, wearedrawingballswithreplacementfroman
Missingmass,realizability,andnaturalestimation. Good
urnwithcoloredballs. Whatistheproportionofballsin
andTuring(GT)(Good,1953)discoveredthattheexpected
that urn that have a color not observed in the sample; or
value of the probability M = (cid:80) p 1(N = k) that
equivalently,whatistheprobabilitythatthenextballhas k x‚ààX x x
the(n+1)-thobservationX isanelementthathasbeen
apreviouslyunobservedcolor? Whatisthedistributionof n+1
observedexactlyktimesinXn(incl. k =0)isafunction
rarelyobservedcolorsinthaturn?Thesequestionsrepresent
oftheexpectednumberofcolorsf (n+1)thatwillbe
afundamentalprobleminmachinelearning: Howcanwe k+1
observedk+1timesinanenlargedsampleXn‚à™X .
extrapolatefrompropertiesofthetrainingdatatoproperties n+1
oftheunseen,underlyingdistributionofthedata? k+1
E[M ]= f (n+1). (1)
k n+1 k+1
1Max-PlanckInstituteforSecurityandPrivacy(MPI-SP),Ger-
WealsocallM astotalprobabilitymassovertheelements
many.Correspondenceto:SeongminLee<seongmin.lee@mpi- k
thathavebeenobservedexactlyktimes. Sinceoursample
sp.org>,MarcelBo¬®hme<marcel.boehme@mpi-sp.org>.
Xnisonlyofsizen,GTsuggestedtoestimatef (n+1)
k+1
usingŒ¶ . Concretely,MÀÜG = k+1Œ¶ .
k+1 k n k+1
1
4202
beF
8
]GL.sc[
1v53850.2042:viXraHowMuchisUnseenDependsChieflyonInformationAbouttheSeen
Fork =0,M givesthe‚Äúmissing‚Äù(probability)massover CompetitivenessofGT.UsingthePoissonapproximation,
0
theelementsnotinthesample. Ingeneticsandbiostatistics, OrlitskyandSuresh(Orlitsky&Suresh,2015)showedthat
thecomplement1‚àíM measuressamplecoverage,i.e.,the natural estimators from GT, i.e., pÀÜG = MÀÜG /Œ¶ , per-
0 x Nx Nx
proportionofindividualsinthepopulationbelongingtoa formsclosetothebestnaturalestimator. Regret,themetric
speciesnotobservedinthesample(Chao&Jost,2012). In ofthecompetitivenessofanestimatoragainstthebestnat-
thecontextofsupervisedmachinelearning,assumingthe uralestimator,ismeasuredasKLdivergencebetweenthe
trainingdataisarandomsample,thesamplecoverageof estimate pÀÜand the actual distribution p, D (pÀÜ||p). Orl-
KL
the training data gives the proportion of all data (seen or itskyandSureshalsoshowedthatfindingthebestnatural
unseen)withlabelsnotobservedinthetrainingdata. estimator for p is same as finding the best estimator for
M ={M }n .
UsinganestimateMÀÜ ofM ,weestimatetheprobability k k=0
k k
p of an element x ‚àà X that appears k times as MÀÜ /Œ¶ . Poissonapproximation. ThePoissonapproximationwith
x k k
The estimation of p in the presence of unseen elements parameterŒª =p nhasoftenbeenusedtotackleamajor
x x
xÃ∏‚ààXnisafundamentalprobleminmachinelearning(Or- challenge in the formal analysis of the missing mass and
litskyetal.,2003;Orlitsky&Suresh,2015;Painsky,2022; naturalestimators(Orlitsky&Suresh,2015;Orlitskyetal.,
Acharya et al., 2013; Hao & Li, 2020). For instance, in 2016;Acharyaetal.,2013;Efron&Thisted,1976;Valiant
naturallanguageprocessingtheestimationoftheprobabil- &Valiant,2016;Good,1953;Good&Toulmin,1956;Hao
ityofagivensequenceofwordsoccurringinasentenceis &Li,2020). Thechallengeisthedependenciesbetweenfre-
themainchallengeoflanguagemodels,particularlyinthe quenciesN fordifferentelementsx‚ààX. InthisPoisson
x
presenceofsequencesthatappearinthetrainingdatararely Productmodel, acontinuous-timesamplingschemewith
ornotatall. Differentsmoothingandbackofftechniques S = |X| independent Poisson distributions is considered
havebeendevelopedtotacklethisdatasparsitychallenge. wherethefrequencyN ofanelementxisrepresentedas
x
aPoissonrandomvariablewithmeanp n. Inotherwords,
Anaturalestimator ofp assignsthesameprobabilityto x
x inthePoissonapproximation,thefrequenciesN aremod-
allelementsxappearingthesamenumberoftimesinthe x
elledasindependentrandomvariables. Consequently,GT
sample Xn (Orlitsky & Suresh, 2015). For k > 0, pÀÜ =
x estimatorisanunbiasedestimatorforthePoissonProduct
M /Œ¶ givesthehypothetical1bestnaturalestimatorofp
k k x model(Orlitskyetal.,2016),yetitisbiasedinthemulti-
foreveryelementxthathasbeenobservedktimes.
nomialdistribution(Juang&Lo,1994). Hence,wetackle
BiasofGood-Turing(GT).Intermsofbias,JuanandLo thedependenciesbetweenfrequenciesanalytically,without
(Juang&Lo,1994)observethattheGTestimatorMÀÜG = approximationviathePoissonProductmodel.
k
k+1Œ¶ isanunbiasedestimateofM (Xn‚àí1),i.e.,where
n k+1 k
then-thsamplewasdeletedfromXnandfind: 1.2.ContributionofthePaper
(cid:12) (cid:12) (cid:12)E(cid:104) MÀÜ kG‚àíM k(cid:105)(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12)E(cid:2) M k(Xn‚àí1)‚àíM k(Xn)(cid:3)(cid:12) (cid:12) I dn ist th rii bs up tia op ner e, sw time ar te ii on nfo wrc ite ht ahe prf eo cu isn ed ca hti ao rn as cto ef rim zau til oti nno om fti ha el
k+2 (cid:18) 1(cid:19) thedependenciesbetweenN = (cid:80)n 1(X = x)across
‚â§ =O . x i=1 i
n+1 n differentx‚ààX (ratherthanassumingindependenceandus-
ingthePoissonapproximation). Thetheoreticalanalysisis
ConvergenceofGT.McAllesterandSchapire(McAllester basedontheexpectedvalueofthefrequencyoffrequencies
& Schapire, 2000) analyzed the convergence. With high E[Œ¶ k]=f k(n)betweendifferentkandn,whichis
probability, f (n) f (n+1) f (n+1)
k = k ‚àí k+1 . (2)
|MÀÜ kG‚àíM k(n)| (cid:0)n k(cid:1) (cid:0)n+ k1(cid:1) (cid:0)n k++ 11(cid:1)
Ô£± ‚àö
O(1/ n) fork =0(missingmass) Exploringthisnewtheoreticaltool,webringtwocontribu-
Ô£¥Ô£≤
‚àö
= O(log(n)/ n) forsmallk(rareelements) tionstotheestimationofthetotalprobabilitymassM k for
Œ¥
Ô£¥Ô£≥O(k/‚àö
n) forlargek(abundantelements).
anyk :0‚â§k ‚â§n. Firstly,weshowexactlytowhatextent
E[M ]canbeestimatedfromthesampleXnandhowmuch
k
ThisresultwasimprovedbyDrukhandMansour(Drukh remainstobeestimatedfromtheunderlyingdistributionp
&Mansour,2004)andmorerecentlybyPainsky(Painsky, andthenumberofelements|X|. Specifically,weshowthe
2022)whoshowedthatGTestimatorconvergesatarateof following.
‚àö
O(1/ n)forallkbasedonworst-casemeansquarederror Theorem1.1.
analysis. (cid:32) (cid:33)(cid:34)n‚àík (cid:44)(cid:32) (cid:33)(cid:35)
E[M ]= n (cid:88) (‚àí1)i‚àí1f (n) n +R
1Thebestnaturalestimatorisalsocalledoracle-aidedestimator k k k+i k+i n,k
i=1
foritsknowledgeaboutp (Orlitsky&Suresh,2015)butcannot
actuallybeusedforestimax tion. whereR =(cid:0)n(cid:1) (‚àí1)n‚àíkf (n+1)istheremainder.
n,k k n+1
2HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
ThisdecompositionshowsthattheGTestimatoristhefirst g k(n)‚àíg k+1(n+1)=g k(n+1)
term of E[M ] using the plug-in estimator Œ¶ for f (n). 0 -
k 1 1
Hence, it gives the exact bias of the GT estimator in the 1 -
g k(n)
multinomialsetting(whichwouldincorrectlybeidentified 2 -
asunbiasedusingthePoissonapproximation).
3 - g k(n+1) g k+1(n+1)
4 -
Our second contribution is the development of a class of 5 -
naturalestimators. Westartbydefininganearlyunbiased n6 -
estimator MÀÜ kB = (cid:0)n k(cid:1)(cid:104) (cid:80)n i=‚àí 1k(‚àí1)i‚àí1Œ¶ k+i(cid:46)(cid:0) kn +i(cid:1)(cid:105) that 7
8
-
-
g 1(n+1)=ùîº[M 0(n)]
usestheplug-inestimatorŒ¶ forf (n)inTheorem1.1and 9 -
l l
drops R . While the bias of MÀÜB decays exponentially, 10 -
thevarian n, ck eofMÀÜ kB istoohightobk epractical. 111
2
-
- ‚ù∂
‚ù∏ g k+1(n+1)=ùîº [M nk C(n k)
]
Weobservethatwecancasttheestimationoftheexpected 13 - ‚ù∑
k
totalmassasanoptimizationproblem. FromTheorem1.1 0 1 2 3 4 5 6 7 8 9 10 11 12 13
and Eqn. 2, we can see that there are many representa- Figure1. Lowertrianglematrixofg (n).
k
tionsofE[M ],allofwhichsuggestdifferentestimatorsfor
k
E[M k]. Weintroduceadeterministicmethodtoconstructa &Li,2020). Inthefollowing,wetacklethischallengeby
uniqueestimatorfromarepresentation,andshowhowtoes- formalizingthesedependenciesbetweenfrequencies. Thus,
timatethemeansquarederror(MSE)forsuchanestimator. weestablishalinkbetweentheexpectedvaluesofthecor-
Equippedwithalargesearchspaceofrepresentationsanda respondingtotalprobabilitymasses.
fitnessfunctiontoestimatetheMSEofacandidateestima-
tor,wedevelopageneticalgorithmthatfindsanestimator 2.1.DependencyAmongFrequencies
withaminimalMSE(andatmostNterms).
Recallthattheexpectedvaluef (n)ofthenumberofel-
k
Wecomparetheperformanceoftheminimal-biasestimator ements Œ¶ that appear exactly k times in the sample Xn
MÀÜ kB and the minimal-MSE estimators discovered by our isdefinedk asf k(n) = (cid:80)
x‚ààX
(cid:0)n k(cid:1) pk x(1‚àíp x)n‚àík. Forcon-
geneticalgorithtothethatofthewidelyusedGTestimator
vencience,letg (n)=f
(n)/(cid:0)n(cid:1)
. Wenoticethefollowing
onavarietyofmultinomialdistributionsusedforevaluation k k k
relationshipamongkandn:
inpreviouswork. Ourresultsshowthat1)theminimal-bias
estimatorhasasubstantiallysmallerbiasthantheGTesti- S
(cid:88)
matorbythousandsoforderofmagnitude,2)Ourgenetic g k(n+1)= pk x(1‚àíp x)n‚àík¬∑(1‚àíp x)
algorithmcanproduceestimatorswithMSEsmallerthan x=1
theGTestimatorover96%ofthetimewhenthereareat =g k(n)‚àíg k+1(n+1) (3)
leastasmanysamplesasclasses;theirMSEisroughly80% n‚àík
(cid:88)
oftheGTestimator. Wealsopublishalldataandscriptsto = (‚àí1)ig (n)+(‚àí1)n‚àík+1g (n+1)
k+i n+1
reproduceourresults. i=0
We can now write the expected value E[M ] of the total
2.DependenciesBetweenFrequenciesN k
x
probability mass in terms of the frequencies with which
Firstly,weproposeanew,distribution-free2 methodology differentelementsx‚ààX havebeenobservedinthesample
forreasoningaboutpropertiesofestimatorsofthemissing Xnofsizenasfollows
andtotalprobabilitymassesformultinomialdistributions. (cid:18) (cid:19)
(cid:88) n
ThemainchallengeforthestatisticalanalysisofM
k
has E[M k]=
k
pk x+1(1‚àíp x)n‚àík
been reasoning in the presence of dependencies between x‚ààX
frequenciesN fordifferentelementsx‚ààX. Asdiscussed (cid:18) n(cid:19)
x
= g (n+1) (4)
in Section 1.1, a Poisson approximation with parameter k k+1
Œª x = p xnisoftenusedtorenderthesefrequenciesasin- (cid:18) (cid:19)(cid:34)n‚àík (cid:35)
dependent(Orlitsky&Suresh,2015;Orlitskyetal.,2016; = n (cid:88) (‚àí1)i‚àí1g (n) +R (5)
Acharya et al., 2013; Efron & Thisted, 1976; Valiant & k k+i n,k
i=1
Valiant, 2016; Good, 1953; Good & Toulmin, 1956; Hao
whereR =(cid:0)n(cid:1) (‚àí1)n‚àíkf (n+1)isaremainderterm.
2Adistribution-freeanalysisisfreeofassumptionsaboutthe Thisdemn, ok nstrak tesTheoremn+ 11
.1.
shapeoftheprobabilitydistributiongeneratingthesample.Inthis
case,wemakenoassumptionsaboutparametersporn. Figure1illustratestherelationshipbetweentheexpected
frequencyoffrequenciesf (n)=g
(n)/(cid:0)n(cid:1)
,thefrequency
k k k
3
- - - - - - - - - - - - - -HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
k,andthesamplesizen. They-andx-axisrepresentsthe 3.1.EstimatorwithExponentiallyDecayingBias
sample size n and the frequency k, respectively. As per
Let
Eqn.(3),forevery2x2lowertrianglematrix,thevalueof
(th ge (l now ))e mr il nef ut sc te hl el v(g
ak
lu( en o+ ft1 h) e) li os wv ea rlu rie gho tf cth ele lu (gpper (l nef +tc 1e )l )l
.
MÀÜ kB
=‚àí(cid:18) n k(cid:19)n (cid:88)‚àík (‚àí (cid:0)1) niŒ¶
(cid:1)k+i
k k+1 i=1 k+i
Wecanusethisvisualizationtoquicklyseehowtorewrite Bias. Forsomeconstantk :0‚â§k ‚â§nandsomeconstant
g (n) as an alternating sum of values of the cells in the c>1,thebiasofMÀÜB isintheorderofO(nkc‚àín),i.e.,
k k
upper row, starting from the cell in the same column to
therightmostcell,andadding/subtractingthevalueofthe E(cid:104) MÀÜB ‚àíM (cid:105) =‚àíR =(‚àí1)n‚àík‚àí1(cid:18) n(cid:19) (cid:88) pn+1
k k n,k k x
rightmost cell in the current row. For instance, the value
x‚ààX
g in0( th1 e3) fii gn ut rh ee cb anot bto em eqr ue ad llc yel cl a( lr co uw latn ed= as13 th, eco vl au rm ion usk li= ne0 ar) (cid:12) (cid:12) (cid:12)E(cid:104) MÀÜ kB ‚àíM k(cid:105)(cid:12) (cid:12) (cid:12)‚â§(cid:18) n k(cid:19) (cid:88) c‚àí xn ‚â§nk (cid:88) c‚àí xn
combinationsofitssurroundingcells: (1)withg (12)and x‚ààX x‚ààX
0
g (13)(bluecolored),(2)withg (11),g (11),¬∑¬∑¬∑,g (13)
1 0 1 4 wherec >1forallx‚ààX areconstants.
x
(purple colored), or (3) with g (11), g (11), ¬∑¬∑¬∑, g (13)
0 1 8
(greencolored). Variance. ThevarianceofMÀÜB isgivenbythevariances
k
andcovariancesofthefrequenciesŒ¶ fori=1,¬∑¬∑¬∑ ,n‚àí
k+i
Missing Mass. The missing probability mass M 0 gives k. Underthecertainconditions,thevarianceofMÀÜB also
the proportion of all possible observations for which the k
decaysexponentiallyinn.
elementsx‚ààX havenotbeenobservedinXn. ByEqn.(4)
Theorem 3.1. The variance of MÀÜB decreases exponen-
and(5),theexpectedvalueofM 0is k
tiallywithnifp <0.5or (1‚àípmax)(1‚àípmin) <1,where
E[M 0]=g 1(n+1) p
max
=max x‚ààXma px xandp
min
=minpm x‚ààax
X
p x.
(cid:34) n (cid:35)
(cid:88)
= (‚àí1)k‚àí1g (n) +(‚àí1)nf (n+1). TheproofispostponedtoAppendixB.
k n+1
k=1 ComparisontoGood-Turing(GT).ThebiasofMÀÜB not
k
ThevaluesinthesecondcolumnofFigure1(k = 1)rep- onlydecaysexponentiallyinnbutisalsosmallerthanthat
resentstheexpectedvaluesofmissingmass;E[M ]being ofGTestimatorMÀÜG byanexponentialfactor. Forasim-
0 k
the cumulative sum of (‚àí1)k‚àí1g k(n) is intuitively clear plervariantofGTestimator,MÀÜ kG‚Ä≤ = nk ‚àí+1 kŒ¶ k+1(suggested
fromthefigure(theredcellintherown = 7). Itishere in(McAllester&Schapire,2000)),whichcorrespondsto
where we observe that E[M 0] = g 1(n+1) is almost en- thefirsttermintheexpectedtotalprobabilitymassE[M k]
tirelydeterminedbytheg ‚àó(n),theexpectedfrequenciesof inEqn.(5),weshowthatitsbiasislargerbyanexponential
frequencies in the sample Xn, and not by the number of factorthantheabsolutebiasofMÀÜB.Toseethis,weprovide
k
elements|X|ortheirunderlyingdistributionp. Infact,the boundsontheindividualsumsandthenonthebiasratio:
influenceofpintheremaindertermdecaysexponentially,
i.e., f n+1(n+1) = (cid:80)
x‚ààX
pn x+1 ‚â§ (cid:80)
x‚ààX
(cid:0) e1‚àípx(cid:1)‚àín‚àí1 Bias
G‚Ä≤
=E(cid:104) MÀÜ kG‚Ä≤ ‚àíM k(cid:105) ‚â•(cid:18) n k(cid:19) pk m+ in2(1‚àíp min)n‚àík‚àí1
whichisdominatedbythediscoveryprobabilityofthemost
(6)
abundantelementmax(p).
(cid:12) (cid:104) (cid:105)(cid:12) (cid:18) n(cid:19)
TotalMass. Similarly,theexpectedvalueofthetotalprob- |Bias B|=(cid:12) (cid:12)E MÀÜ kB ‚àíM k (cid:12) (cid:12)‚â§ k Spn m+ ax1 (7)
abilitymassE[M ](theredcellintherown=10),which
k
isequalto(cid:0)n k(cid:1)
g k+1(n+1),isalmostentirelydeterminedby whereS =|X|,suchthat
theexpectedfrequenciesofthesampleXnwithremainder
R n,k =(cid:0)n k(cid:1)(cid:80) x‚ààX pn x+1. (cid:12) (cid:12) (cid:12)Bias G‚Ä≤(cid:12) (cid:12) (cid:12)‚â• pk m+ in2 (cid:18) 1‚àíp min(cid:19)n‚àík‚àí1 .
(cid:12)Bias
B
(cid:12) Spk m+ ax2 p
max
3.ALargeClassofEstimators
Noticethat(1‚àíp )/p >1foralldistributionsover
min max
FromtherepresentationofE[M ]intermsoffrequenciesin X,exceptwhereS =2andp={0.5,0.5}. Thesamecan
k
Eqn.(4)andtherelationshipacrossfrequenciesinEqn.(3), beshownfortheoriginalGTestimatorMÀÜG = k+1Œ¶
k n k+1
wecanseethatthereisalargenumberofrepresentationsof forasufficientlylargesamplesize(seeAppendixA).
theexpectedtotalprobabilitymassE[M ]. Eachrepresen-
k Example(Missingmassfortheuniform).Suppose,weseek
tationmightsuggestdifferentestimators.
toestimatethemissingmassfromasequenceofelements
WestartbydefiningtheminimalbiasestimatorMÀÜB from Xn observedinnsamplesfromtheuniformdistribution;
k
therepresentationinEqn.(5)andexploreitsproperties. p = 1/S for all x ‚àà X. MÀÜG overestimates M on the
x 0 0
4HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
averageby(S‚àí1)n‚àí1/SnwhileournewestimatorMÀÜBhas Algorithm1GeneticAlgorithm
0
abiasof(‚àí1)n/Sn. Hence, fortheuniformdistribution, Input: Targetfrequencyk,SampleXn
our estimator exhibits a bias that is lower by a factor of Input: IterationlimitG,mutantsizem
1/(S‚àí1)n‚àí1. 1: PopulationP
0
={r 0}
2: Fitnessfbest =f =fitness(r )
WhilethebiasofourestimatorMÀÜ kB islowerthanthatof 3: LimitG
L
=G 0 0
MÀÜ 0G byanexponentialfactor,thevarianceishigher. The 4: forgfrom1toG Ldo
varianceofMÀÜBdependsonthevariancesofandcovariances 5: P =selectTopM(P g‚àí1,m)
k 6: P‚Ä≤ =lapply(P,mutate)
betweenŒ¶ ,Œ¶ ,...,Œ¶ ,i.e.,
k+1 k+2 n 7: P =P‚Ä≤‚à™{r }‚à™selectTopM(P ,3)
g 0 g‚àí1
8: f =min(lapply(P ,fitness))
n‚àík g g
Var(cid:16) MÀÜB(cid:17) = (cid:88) c2Var(Œ¶ ) 9: if(g=G L)‚àß((f g =f 0)‚à®(fbest >0.95¬∑f g))then
k i k+i 10: G =G +G
L L
i=1 (8) 11: fbest =f
(cid:88) g
+ (‚àí1)i+jc c Cov(Œ¶ ,Œ¶ ), 12: endif
i j k+i k+j
13: endfor
iÃ∏=j 14: EstimatorMÀÜEvo =instantiate(selectTopM(P ,1))
k GL
where c
i
= (cid:0)n k(cid:1)(cid:14)(cid:0) kn +i(cid:1) . In contrast, the variance of MÀÜ kG Output: Minimal-MSEEstimatorMÀÜ kEvo
depends only on the variance of Œ¶ . In the empirical
k+1
study,weinvestigatethedifferenceofthetwoestimatorsin
termsofthebiasandthevariance. accordingly. Forinstance,applyingEqn.(11)withŒ¥ =0.5
andEqn.(13)tor ,weobtainthefollowingrepresentation
0
3.2.EstimationwithMinimalMSEasSearchProblem r 1ofE[M k]:
T thh ae tr ce aa nre bm ea cn oy nr se trp ur ces tee dnt bat yio rn es cuof rsE iv[ eM lyk r] e= wr(cid:0) itn k i(cid:1) ng gk+ te1 r( mn s+ a1 c)
-
Ô£± Ô£¥Ô£¥Ô£¥Ô£≤ Ô£± Ô£¥Ô£¥Ô£¥Ô£≤(cid:0) (cid:0)n nk(cid:1) (cid:1)(cid:14) (cid:14)2
2
f fo or ri i= =k k+ +1 1a an nd dj
j
= =n n+1Ô£º Ô£¥Ô£¥Ô£¥Ô£Ω
(c co fr .d Ein qg n.to (3th &ed 4e ))p .e Tnd he en rc ey pra em seo nn tg atf ir oe nqu ue sen dci te os cw oe nsid tre un ct tifi oe ud
r
r 1 = Ô£¥Ô£¥Ô£¥Ô£≥Œ± i,j = Ô£¥Ô£¥Ô£¥Ô£≥0‚àík(cid:0)n k(cid:1)(cid:14) 2 f oo thr ei r= wisk e.+2andj =n+1 Ô£¥Ô£¥Ô£¥Ô£æ
minimal-biasestimatorMÀÜB wasoneofthem. However,we
k
noticethatthevarianceofMÀÜ kB istoohightobepractical. Estimatorinstantiation. Toconstructauniqueestimator
Tofindarepresentationfromwhichanestimatorwithamin- MÀÜr ofM fromarepresentationrofE[M ],wepropose
k k k
imalmeansquarederror(MSE)canbederived,wecastthe a deterministic method. But first, we define our random
efficientestimationofM k asanoptimizationproblem. To variablesonsubsamplesofXn. Foranym‚â§n,letN x(m)
efficientlynavigatethelargesearchspaceofrepresentations bethenumberoftimeselementx ‚àà X isobservedinthe
ofE[M k],wedevelopageneticalgorithm. subsampleXm =‚ü®X 1,¬∑¬∑¬∑X m‚ü©ofXn. LetŒ¶ k(m)bethe
Search space. Let E[M ] be represented by a suitable
numberofelementsappearingexactlyktimesinXm,i.e.,
k
choiceofcoefficientsŒ± suchthat
i,j m
(cid:88)
n+1n+1 N (m)= 1(X =x) NotethatN =N (n).
(cid:88)(cid:88) x i x x
E[M ]= Œ± g (j). (9)
k i,j i i=1
i=1 j=i Œ¶ (m)= (cid:88) 1(N (m)=k) NotethatŒ¶ =Œ¶ (n).
k x k k
OnerepresentationofE[M ]=(cid:0)n(cid:1) g (n+1)is x‚ààX
k k k+1
r
0
=(cid:40) Œ±
i,j
=(cid:40)(cid:0) 0n k(cid:1) f oo thr ei r= wisk e.+1andj =n+1(cid:41) (10) Hence,givenarepresentationr,wecanconstructMÀÜ kr as
M cou nsta trt uio ctn a. G nei wven rea pn rey sr ee np tare tis oe nnt ra ‚Ä≤ti oo fn Er [o MfE ],[M
s.tk
.], Ew qne .c (a 9n
)
MÀÜ kr
=Ô£Æ Ô£∞(cid:88)n (cid:88)n Œ±
(cid:0)ji, (cid:1)jŒ¶
i(j)Ô£π Ô£ª+(cid:34) (cid:88)n Œ±
(cid:0)ni, +n+ 1(cid:1)1Œ¶
i(cid:35)
k
i=1 j=i i i=1 i
holdsbyrecursivelyconsideringthefollowingidentities:
Œ±
i,j
¬∑g i(j)=Œ±
i,j
¬∑((1‚àíŒ¥)g i(j)+Œ¥g i(j)) (11) NoticethatŒ¶
i(j)(cid:14)(cid:0)j i(cid:1)
isjusttheplug-inestimatorforg i(j).
=Œ± i,j ¬∑(g i(j+1)+g i+1(j+1)) (12) Fitnessfunction. Todefinethequantitytooptimize,any
=Œ± ¬∑(g (j‚àí1)‚àíg (j)) (13) meta-heuristicsearchrequiresafitnessfunction. Ourfitness
i,j i i+1
functiontakesacandidaterepresentationrandreturnsan
=Œ± ¬∑(g (j‚àí1)+g (j)) (14)
i,j i‚àí1 i‚àí1 estimate of the MSE of the corresponding estimator MÀÜr.
k
foranychoiceofŒ¥ :0‚â§Œ¥ ‚â§1. Importantly,afterapplying We decompose the MSE as the sum of its variance and
these identities, we must work out the new coefficients squaredbias. Forconvenience,letg (n)=0.
n+1
5HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
4.Experiment
MSE(MÀÜ
kr)=(cid:34)n (cid:88)+1
Œ± i,n+1[g i(n+1)‚àíg
i(n)](cid:35)2
(15)
W oue rmde is ni ig mn ae l-x bp iae sri em ste in mts att oo re MÀÜva Blu aa nte dt (h iie )p oe fr of uo rrm tha en mce in( ii m) ao lf
-
k
i=1 MSEestimatorMÀÜEvo thatisdiscoveredbyourgenetical-
n n (cid:32) (cid:33)2 k
+(cid:88)(cid:88) Œ± i,j Var(Œ¶ (j)) gorithmagainsttheperformanceofthewidely-usedGood-
(cid:0)j(cid:1) i TuringestimatorMÀÜG(Good,1953).
i=1 j=i i k
n n n n
+(cid:88)(cid:88)(cid:88) (cid:88) Œ± i,j Œ± l,mCov(Œ¶ (j),Œ¶(m)) Distibutions.Weusethesamesixmultinomialdistributions
(cid:0)j(cid:1) (cid:0)m(cid:1) i l
that are used in previous evaluations (Orlitsky & Suresh,
i=1 j=i l=1 m=l i l
lÃ∏=i mÃ∏=j 2015; Orlitsky et al., 2016; Hao & Li, 2020): a uniform
WeexpandonthecomputationoftheMSEinAppendixC.
distribution (uniform), a half-and-half distribution where
Sincetheunderlyingdistribution{p } isunknown,we halfoftheelementshavethreetimesoftheprobabilityofthe
x x‚ààX
canonlyestimatetheMSE.Foranyelementxthathasbeen otherhalf(half&half),twoZipfdistributionswithparameters
observed exactly k > 0 time in the sample Xn, we use s = 1 and s = 0.5 (zipf-1, zipf-0.5, respectively), and
pÀÜ =MÀÜG/Œ¶ asnaturalestimatorofp ,whereMÀÜGisthe distributionsgeneratedbyDirichlet-1priorandDirichlet-
x k k x k
GTestimator. Tohandleunobservedelements(k =0),we 0.5prior(diri-1,diri-0.5,respectively).
firstestimatethenumberofunseenelementsE[Œ¶ ]=f (n)
0 0 OpenScienceandReplication. Forscrutinyandreplica-
usingChao‚ÄôsnonparamtericspeciesrichnessestimatorfÀÜ =
0 bility,wepublishallourevaluationscriptsat:
n‚àí n1 2Œ¶ Œ¶2 1
2
(Chao,1984),andthenestimatetheprobabilityof https://anonymous.4open.science/r/Better-Turing-157F.
each such unseen element as pÀÜ = MÀÜG/fÀÜ, where MÀÜG
y 0 0 0
istheGTestimator. Finally,weplugtheseestimatesinto 4.1.EvaluatingourMinimal-BiasEstimator
Eqn.(15)toestimatetheMSE.Itisinterestingtonotethat ‚Ä¢ RQ1. HowdoesourestimatorforthemissingmassMÀÜB
itispreciselytheGTestimatorwhoseMSEourapproachis comparetotheGood-TuringestimatorMÀÜG interms0 of
supposedtoimproveupon. 0
biasasafunctionofsamplesizen?
Geneticalgorithm.Withtherequiredconceptsinplace,we ‚Ä¢ RQ2. How does our estimator for the total mass MÀÜB
k
arereadytointroduceourgeneticalgorithm(GA)(Mitchell, comparetotheGood-TuringestimatorMÀÜG intermsof
1998). Algorithm1sketchesthegeneralprocedure. Givena k
biasasafunctionoffrequencyk?
targetfrequencyk(incl.k =0),thesampleXn,aniteration
‚Ä¢ RQ3. Howdotheestimatorscompareintermsofvari-
limitG,andthenumbermofcandidaterepresentationsto
anceandmean-squarederror?
be mutated in every iteration, the algorithm produces an
estimatorMÀÜ kEvowithminimalMSE. We focus specifically on the bias of MÀÜ kB, i.e., the aver-
Starting from the initial representation r (Eqn. (10); agedifferencebetweentheestimateandtheexpectedvalue
0 E[M ]. Weexpectthatthebiasofthemissingmassesti-
Line1),ourGAiterativelyimprovesapopulationofcandi- k
daterepresentationsP g,calledindividuals. Foreverygen-
mateMÀÜ 0B asafunctionofnacrossdifferentdistributions
erationg(Line4),ourGAselectsthemfittestindividuals providesempiricalinsightforourclaimthathowmuchis
fromthepreviousgenerationP (Line5),mutatesthem unseenchieflydependsoninformationabouttheseen.
g‚àí1
(Line6),andcreatesthecurrentgenerationP gbyaddingthe RQ.1. Figure 2a illustrates how fast our estimator MÀÜB
initialrepresentationr andtheTop-3individualsfromthe k
0 andthebaselineestimatorMÀÜG(GT)approachtheexpected
previousgeneration(Line7). TheinitialandpreviousTop-3 k
missingmassE[M ]asafunctionofsamplesizen. Asit
individuals are added to mitigate the risk of convergence 0
mightdifficultforthereadertodiscerndifferencesacross
toalocaloptimum. Tomutatearepresentationr,ourGA
distributionsforthebaselineestimator,werefertoFigure2b,
(i)choosesarandomtermr,(ii)appliesEqn.(11)whereŒ¥is
wherewezoomintoarelevantregion.
chosenuniformlyatrandom,(iii)appliesarandomidentity
fromEqn.(12‚Äì14),and(iv)adjuststhecoefficientsforthe The magnitude of our estimator‚Äôs bias is significantly
resultingrepresentationr‚Ä≤accordingly. Theiterationlimit smaller than the magnitude of GT‚Äôs bias for all distribu-
G Lisincreasedifthecurrentindividualsdonotimproveon tions(bythousandsofordersofmagnitude).3 Figure2aalso
theinitialindividualr 0 orsubstantiallyimproveonthose nicelyillustratestheexponentialdecayofourestimatorin
discoveredrecently(Line9‚Äì12). terms of n and how our estimator is less biased than GT
byanexponentialfactor. InFigure2b,wecanobservethat
Distribution-free. Whileourapproachitselfisdistribution-
GT‚Äôsbiasalsodecaysexponentially,althoughnotnearlyat
free,theoutputisdistribution-specific,i.e.,thediscovered
therateofourestimator.
estimator has a minimal MSE on the specific, unknown
distribution. Weexplorethispropertyinourexperiments. 3Recallthattheplotshowsthelogarithmoftheabsolutebias.
6HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
0e+00 ‚àí3e+00 0e+00 n MÀÜ 0 Bias Var MSE
‚àí4e+00 GT 3.6973e-003 2.3372e-03 2.3508e-03 ‚àí1e+04 ‚àí2e+03 100 ours 1.0000e-200 2.3515e-03 2.3515e-03
‚àí5e+00
GT 6.6369e-005 1.1430e-05 1.1434e-05
‚àí2e+04 ‚àí6e+00 ‚àí4e+03 500 ours <1.00e-200 1.1445e-05 1.1445e-05
‚àí7e+00 GT 4.3607e-007 4.3439e-08 4.3439e-08
‚àí3e+04 ‚àí6e+03 1000
2500 5000 7500 10000 2500 5000 7500 10000 500 1000 1500 2000 ours <1.00e-200 4.3441e-08 4.3441e-08
n n k
(a)M0(S=1000). (b)M0(S=1000,cropped). (c)Mk(S=1000,n=2000). (d)M0(uniform,S=100).
Figure2.Resultsforourminimal-biasestimatorMÀÜB.Forourandbaselineestimators,weshowthelogarithmoftheabsolutebias(a&b)
k
asafunctionofnfork=0and(c)asafunctionofkforn=2000.Wealsoshow(d)thebias,variance,andMSEofourandbaseline
estimatorforthreevaluesofn.MoreplotscanbefoundinAppendixE.
Table1.TheMSEofthebestevolvedestimatorMEvoandGTestimatorMÀÜGforthemissingmassM ,thesuccessrateAÀÜ ,andtheratio
0 0 0 12
(Ratio,MSE(MÀÜEvo)/MSE(MG))forthreesamplesizesnandsixdistributionswithsupportsizeS =200.
0 0
n=S/2 n=S n=2S
Dist.
MSE(MÀÜ 0G) MSE(M 0Evo) AÀÜ 12 Ratio MSE(MÀÜ 0G) MSE(M 0Evo) AÀÜ 12 Ratio MSE(MÀÜ 0G) MSE(M 0Evo) AÀÜ 12 Ratio
uniform 3.32e-03 2.04e-03 0.95 61% 1.17e-03 8.90e-04 0.99 76% 2.01e-04 1.70e-04 0.93 84%
half&half 3.33e-03 1.97e-03 0.96 59% 1.09e-03 8.58e-04 0.99 78% 2.11e-04 1.72e-04 1.00 81%
zipf-1 2.32e-03 2.41e-03 0.74 103% 8.16e-04 7.24e-04 0.88 88% 2.39e-04 2.11e-04 0.96 88%
zipf-0.5 3.23e-03 2.29e-03 0.89 71% 1.09e-03 8.52e-04 0.97 78% 2.30e-04 1.93e-04 1.00 83%
diri-1 2.99e-03 2.36e-03 0.85 78% 8.88e-04 6.65e-04 1.00 74% 1.96e-04 1.65e-04 0.96 84%
diri-0.5 2.55e-03 1.81e-03 0.94 71% 6.88e-04 4.86e-04 0.98 70% 1.61e-04 1.31e-04 0.93 81%
Avg. 0.88 74% 0.96 77% 0.96 84%
Intermsofdistributions,acloserlookattheperformance matelythesameasthatofGT.ThereasonisthattheMSEis
differencesconfirmsoursuspicionthatthebiasofoures- dominatedbythevariance. Wemakethesameobservation
timator is strongly influenced by the probability p of forallotherdistributions(seeAppendixE).TheMSEsof
max
themostabundantelementwhilethebiasofGTisstrongly bothestimatorsarecomparable.
influencedbytheprobabilityp oftherarestelement. In
min
fact,byEqn.(7)theabsolutebiasofourestimatorismin- 4.2.EvaluatingourEstimatorDiscoveryAlgorithm
imized when p is minimized. By Eqn. (6), GT‚Äôs bias
max
‚Ä¢ RQ1 (Effectiveness). How does our estimator for the
isminimizedifp ismaximized. Sincebothistruefor
min missingmassMÀÜEvocomparetotheGood-Turingestima-
theuniform,bothestimatorsexhibitthelowestbiasforthe 0
torMÀÜGintermsofMSE?
uniformacrossallsixdistributions. GTperformssimilaron 0
alldistributionsapartfromtheuniform(wherebiasseems ‚Ä¢ RQ2(Efficiency). Howlongdoesittakeforourgenetic
minimal)andzipf-1(wherebiasismaximized). Forouresti- algorithmtogenerateanestimatorMEvogivenasample?
k
mator,ifwerankedthedistributionsbyvaluesofp with
max ‚Ä¢ RQ3 (Distribution-awareness). How well does an esti-
thesmallestvaluefirst‚ü®uniform,half&half,zipf-0.5,zipf-1‚ü©,4
mator discovered from a sample from one distribution
we would arrive at the same ordering in terms of perfor-
performonanotherdistributionintermsofMSE?
manceofourestimatorasshowninFigure2a.
Tohandletherandomnessinourevaluation,werepeateach
RQ2. Figure2cillustratesforbothestimatorsofthetotal
experiment100times: 10runsoftheGAwith10different
massM howthebiasbehavesaskvariesbetween0and
k samplesXn.5 Moredetailsaboutourexperimentalsetup
n = 2000 when S = 1000. The trend is clear; the bias
canbefoundinAppendixD.
ofourestimatorisstrictlysmallerthanthebiasofGTfor
all k and all the distributions. The difference is the most RQ.1(Effectiveness). Table1showsaverageMSEofthe
significantforrareelements(smallk)andgetssmalleras estimatorMEvodiscoveredbyourgeneticalgorithmandthat
0
kincreases. Thebiasofourestimatorismaximizedwhen oftheGTestimatorMÀÜG forthemissingmassM across
0 0
k =1000=0.5n,thebiasforGTwhenk =0. threesamplesizes. WemeasureeffectsizeusingVargha-
DelaneyAÀÜ (Vargha&Delaney,2000)(successrate),i.e.,
RQ3. Table2dshowsvarianceandMSEofbothestimators 12
the probability that the MSE of the estimator discovered
forthemissingmassM
0
fortheuniformandthreevalues
byourgeneticalgorithmhasasmallerMSEthantheGT
ofn. Aswecansee,theMSEofourestimatorisapproxi-
5Fordiri-1,diri-0.5,eachofthetensamplesXn issampled
4diri-1 and diri-0.5 are not considered in the order because
from10distributionssampledfromtheDirichletpriorwiththe
multipledistributionsaresampledfromtheDirichletprior.
sameparameterŒ±=0.5,1,respectively.
7
|saib|
01gol
|saib|
01gol
|saib|
01golHowMuchisUnseenDependsChieflyonInformationAbouttheSeen
1e 4 (target). Applyinganestimatefromthezipf-1onthezipf-1
7
Evolved from givestheoptimalMSE(right-mostredbox). However,ap-
6 uniform plyinganestimatorfromthezipf-1ontheuniform(leftred
5 half&half box)yieldsahugeincreaseinvariance. Intermsofeffect
zipf-0.5 size,wemeasureaVarghaDelaneyAÀÜ >0.84betweenthe
4 zipf-1 12
‚Äúhome‚Äùand‚Äúaway‚Äùestimator. Whileeachoftheuniform
3 andhalf&halfalsoshowsthatthehomeestimatorperforms
2 bestonthehomedistribution(AÀÜ 12 =0.68(medium),0.58
(small),respectively),thedifferencebetweentheestimators
1
fromuniform,half&half,andzipf-0.5islesssignificant. Per-
0 hapsunsurprisingly, anestimatorperformsoptimalwhen
uniform half&half zipf-0.5 zipf-1
Target distribution thesourceofthesamplesissimilartothetargetdistribution.
Summary. Tosummarize,ourGAiseffectiveinfinding
Figure3.The MSE of an estimator discovered using a sample
theestimatorwiththeminimalMSEforthemissingmass
fromonedistribution(individualboxes)appliedtoanothertarget
distribution(clustersofboxes). M 0 with the smaller MSE than GT estimator MÀÜ 0G for all
distributionsandsamplesizes. Theeffectissubstantialand
estimator(largerisbetter). Moreover,wemeasuretheMSE significantandtheaveragedecreaseoftheMSEisroughly
of our estimator as a proprtion of the MSE of GT, called onefifthagainstGTestimatorMÀÜG.
0
ratio(smallerisbetter). ResultsforothersupportsizesS
canbefoundinAppendixE.
5.Discussion
Overall,theestimatordiscoveredbyourGAperformssig-
BeyondtheGeneralEstimator. Inthisstudy,wepropose
nificantlybetterthanGTestimatorintermsofMSE(avg.
AÀÜ > 0.9; ratio < 85%). The performance difference a meta-level estimation methodology that can be applied
12
to a set of samples from a specific unknown distribution.
increaseswithsamplesizen. Whenthesamplesizeistwice
Theconventionalapproachistodevelopanestimator for
thesupportsize(n = 2S),in96%ofrunsourdiscovered
an arbitrary distribution. Yet, each distributions has its
estimator performs better. The average MSE of our esti-
owncharacteristics,and,becauseofthat,themannerofthe
mator is somewhere between 70% and 88% of the MSE
(frequenciesof)frequenciesoftheclassesinthesamplecan
ofGT.ThehighsuccessrateandthelowratiooftheMSE
bedifferfrom,forexample,theuniformdistributiontothe
showsthattheGAiseffectiveinfindingtheestimatorwith
Zipfdistribution. Incontrasttotheconventionalapproach,
theminimalMSEforthemissingmassM . AWilcoxon
0
weproposeadistribution-freemethodologytodiscoverthe
signed-ranktestshowsthatallperformancedifferencesare
statisticallysignificantatŒ±<10‚àí9.
adistribution-specificestimatorwithlowMSE(givenonly
thesample). Notethat,whileweusethegeneticalgorithm
Intermsofdistributions,theperformanceofourestimator todiscovertheestimator,anyoptimizationmethodcanbe
issimilaracrossalldistributions,showingthegeneralityof usedtodiscovertheestimator,forinstance,aconstrained
ouralgorithm. Theonlyexceptionisthezipf-1,wherethe optimizationsolver.
successrateislowerthanforotherdistributionforn=S/2
ExtrapolatingtheFutureSampling. Estimatingthenum-
andS,andtheaverageratiois103%(yet,themedianratio
ber of unseen species is a well-known problem in many
is85%)forn = S/2. Thepotentialreasonforthisisdue
scientificfields,suchasecology,linguistics,andmachine
totheoverfittingtotheapproximateddistributionpÀÜ . Since
x
learning. Givennsamples,theexpectednumberofhitherto
the zipf-1 is the most skewed distribution, there are more
unseenspeciesthatwouldbeuncoveredifttimesmoresam-
elementsunseeninthesamplethaninotherdistributions,
plesweretakenisE[U(t)] = f (n)‚àíf (n+nt). Good
whichmakestheapproximateddistributionpÀÜ lessaccurate. 0 0
x
&Toulmin(1956)proposedaseminalestimatorusingthe
Yet,theperformancealreadyimprovesandbecomesimilar
frequenciesoffrequenciesŒ¶ ,similartotheGood-Turing
tootherdistributionsforn=S andn=2S. k
estimator. Untilrecently,varioussubsequentstudieshave
RQ.2(Efficiency). Thetimeittakestodiscovertheestima- beenconductedtoimprovetheestimator(Efron&Thisted,
tor is reasonable. To compute an estimator fo Table 1, it 1976;Orlitskyetal.,2016;Hao&Li,2020),whilemostof
takesaboutseven(7)minutesonaverageandfive(5)min- themstillreliesonthePoissonapproximationtodesignthe
utesonmedian. Theaveragetimeforeachiterationis1.25s estimator. Webelievethatouranalysiscanbeextendedto
(median: 0.92). theGood-Toulminestimatorseekingmoreaccurateestima-
torsforU(t).
RQ.3(Distribution-awareness). Figure3showstheperfor-
manceofanestimatordiscoveredfromasamplefromone
distribution(source)whenappliedtoanotherdistribution
8
ESMHowMuchisUnseenDependsChieflyonInformationAbouttheSeen
References Mitchell,M. Anintroductiontogeneticalgorithms. MIT
press,1998.
Acharya,J.,Jafarpour,A.,Orlitsky,A.,andSuresh,A.T.
Optimalprobabilityestimationwithapplicationstopre- Orlitsky, A. and Suresh, A. T. Competitive distribution
diction and classification. In Shalev-Shwartz, S. and estimation: Why is good-turing good. In Cortes, C.,
Steinwart,I.(eds.),Proceedingsofthe26thAnnualCon- Lawrence, N., Lee, D., Sugiyama, M., and Garnett,
ferenceonLearningTheory,volume30ofProceedings R. (eds.), Advances in Neural Information Pro-
ofMachineLearningResearch,pp.764‚Äì796,Princeton, cessing Systems, volume 28. Curran Associates,
NJ,USA,12‚Äì14Jun2013.PMLR. Inc., 2015. URL https://proceedings.
neurips.cc/paper/2015/file/
Chao,A.Nonparametricestimationofthenumberofclasses
d759175de8ea5b1d9a2660e45554894f-Paper.
inapopulation. ScandinavianJournalofstatistics,pp.
pdf.
265‚Äì270,1984.
Orlitsky,A.,Santhanam,N.P.,andZhang,J. Alwaysgood
Chao,A.andJost,L.Coverage-basedrarefactionandextrap-
turing: Asymptotically optimal probability estimation.
olation: standardizing samples by completeness rather
Science,302(5644):427‚Äì431,2003. doi: 10.1126/science.
thansize. Ecology,9312:2533‚Äì47,2012.
1088284.
Drukh,E.andMansour,Y. Concentrationboundsforuni- Orlitsky, A., Suresh, A. T., and Wu, Y. Opti-
gramslanguagemodel. InShawe-Taylor,J.andSinger, mal prediction of the number of unseen species.
Y.(eds.),LearningTheory,pp.170‚Äì185,Berlin,Heidel- Proceedings of the National Academy of Sciences,
berg,2004.SpringerBerlinHeidelberg. ISBN978-3-540- 113(47):13283‚Äì13288, 2016. doi: 10.1073/pnas.
27819-1. 1607774113. URLhttps://www.pnas.org/doi/
abs/10.1073/pnas.1607774113.
Efron, B. and Thisted, R. Estimating the number of un-
sen species: How many words did shakespeare know? Painsky,A.Convergenceguaranteesforthegood-turingesti-
Biometrika,63(3):435‚Äì447,1976. ISSN00063444. URL mator.JournalofMachineLearningResearch,23(279):1‚Äì
http://www.jstor.org/stable/2335721. 37,2022. URLhttp://jmlr.org/papers/v23/
21-1528.html.
Good,I.J. Thepopulationfrequenciesofspeciesandthe
estimationofpopulationparameters.Biometrika,40(3-4): Valiant,G.andValiant,P. Instanceoptimallearningofdis-
237‚Äì264,1953. cretedistributions. InProceedingsoftheForty-Eighth
AnnualACMSymposiumonTheoryofComputing,STOC
Good, I. J. and Toulmin, G. H. The number of new
‚Äô16,pp.142‚Äì155,NewYork,NY,USA,2016.Associa-
species,andtheincreaseinpopulationcoverage,when
tionforComputingMachinery. ISBN9781450341325.
asampleisincreased. Biometrika,43(1/2):45‚Äì63,1956. doi: 10.1145/2897518.2897641. URLhttps://doi.
ISSN 00063444. URL http://www.jstor.org/ org/10.1145/2897518.2897641.
stable/2333577.
Vargha,A.andDelaney,H.D. Acritiqueandimprovement
Hao, Y. and Li, P. Optimal prediction of the number of the cl common language effect size statistics of mc-
of unseen species with multiplicity. In Larochelle, grawandwong. JournalofEducationalandBehavioral
H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, Statistics,25(2):101‚Äì132,2000.
H. (eds.), Advances in Neural Information Processing
Systems, volume 33, pp. 8553‚Äì8564. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.
neurips.cc/paper/2020/file/
618790ae971abb5610b16c826fb72d01-Paper.
pdf.
Juang, B.-H. and Lo, S. On the bias of the turing-good
estimateofprobabilities. IEEETransactionsonsignal
processing,2(42):496‚Äì498,1994.
McAllester,D.andSchapire,R.E. Ontheconvergencerate
ofgood-Turingestimators. InProceedingsofthe13th
AnnualConferenceonComputationalLearningTheory,
pp.1‚Äì6.MorganKaufmann,SanFrancisco,2000.
9HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
A.ComparingtheBiasoftheEstimators
InSection3.1,wehaveshownthatthebiasofasimplervariantofGT,MÀÜG‚Ä≤ = k+1Œ¶ ,islargerbyanexponentialfactor
k n‚àík k+1
thantheabsolutebiasofourminimalbiasestimatorMÀÜB. Inthissection,weshowthatthebiasoftheoriginalGTestimator
k
MÀÜG = k+1Œ¶ isalsolargerbyanexponentialfactorthantheabsolutebiasofMÀÜB forasufficientlylargersamplesize.
k n k+1 k
Recallthat
Bias
=E(cid:104)
MÀÜG‚àíM
(cid:105)
=
k+1
f
(n)‚àí(cid:18) n(cid:19)
g
(n+1)=(cid:88)(cid:18) n(cid:19)
pk+2(1‚àíp )n‚àík‚àí1, (16)
G‚Ä≤ k k n‚àík k+1 k k+1 k x x
x
and
(cid:104) (cid:105) (cid:18) n(cid:19) k(k+1)
Bias =E MÀÜG‚àíM = g (n+1)‚àí f (n) (17)
G k k k k+2 n(n‚àík) k+1
(cid:18) (cid:19) (cid:18) (cid:19)
n n‚àí1
= g (n+1)‚àí g (n) (18)
k k+2 k‚àí1 k+1
(cid:18)(cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
(cid:88) n 1 n‚àí1
= pk+2(1‚àíp )n‚àík‚àí1 ‚àí ¬∑ (19)
x x k p k‚àí1
x
x
(cid:18) (cid:19) (cid:18) (cid:19)
(cid:88) n k
= pk+2(1‚àíp )n‚àík‚àí1 1‚àí (20)
k x x n¬∑p
x
x
(cid:18) (cid:19)
k
‚â• 1‚àí Bias , (21)
n¬∑p
G‚Ä≤
max
where1‚àí k >0whennissufficientlylarge. Aboveinequalityleadstothefollowing:
n¬∑pmin
Bias (cid:18) k (cid:19) Bias Spk+2 (cid:18) 1‚àíp (cid:19)‚àín+k+1
G ‚â• 1‚àí , while B ‚â§ max min , (22)
Bias
G‚Ä≤
n¬∑p
max
Bias
G‚Ä≤
pk m+ in2 p
max
whichprovesourclaim.
B.BoundingtheVarianceandtheMSEofMÀÜB
k
TheMSEofanestimatoreÀÜforanestimandeisdefinedasMSE(eÀÜ) = E[(eÀÜ‚àíe)2] = Var(eÀÜ)+Bias2(eÀÜ). Aswehave
shownthebiasofMÀÜB inSection3.1,theremainingparttocomputetheMSEofMÀÜB istocomputeitsvariance.
k k
Thevarianceofthelinearcombinationofrandomvariablesisgivenby
(cid:32) (cid:33)
(cid:88) (cid:88) (cid:88)
Var c X = c2Var(X )+ c c Cov(X ,X ). (23)
i i i i i j i j
i i iÃ∏=j
Therefore,thevarianceandthecovarianceofŒ¶ (n)sarethemissingpiecestocomputethevarianceofMÀÜB.
i k
TheoremB.1. Giventhemultinomialdistributionp=(p ,...,p )withsupportsizeS,thevarianceofŒ¶ =Œ¶ (n)from
1 S i i
nsamplesXnisgivenby
(cid:40) f (n)‚àíf (n)2+(cid:80) n! pipi(1‚àíp ‚àíp )n‚àí2i if2i‚â§n,
Var(Œ¶ (n))= i i xÃ∏=y i!2(n‚àí2i)! x y x y (24)
i f (n)‚àíf (n)2 otherwise.
i i
10HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
Proof.
Var(Œ¶ )=E(cid:2) Œ¶2(cid:3) ‚àíE[Œ¶ ]2 (25)
i i i
Ô£Æ(cid:32) (cid:33)2Ô£π
E(cid:2) Œ¶2 i(cid:3) =E Ô£∞ (cid:88) 1(N x =i) Ô£ª (26)
x
Ô£Æ Ô£π
(cid:88) (cid:88)
=E Ô£∞ 1(N x =i)+ 1(N x =i‚àßN y =i)Ô£ª (27)
x xÃ∏=y
(cid:40) f (n)+(cid:80) n! pipi(1‚àíp ‚àíp )n‚àí2i if2i‚â§n,
= i xÃ∏=y i!2(n‚àí2i)! x y x y (28)
f (n) otherwise.
i
(cid:40) f (n)+(cid:80) n! pipi(1‚àíp ‚àíp )n‚àí2i‚àíf (n)2 if2i‚â§n,
‚à¥Var(Œ¶ )= i xÃ∏=y i!2(n‚àí2i)! x y x y i (29)
i f (n)‚àíf (n)2 otherwise.
i i
NowwecomputetheupperboundofthevarianceofMÀÜB.
k
LemmaB.2.
(cid:40)
‚â§Sf (n)‚àíf (n)2 if2i‚â§n.
Var(Œ¶ ) i i (30)
i =f (n)‚àíf (n)2 otherwise.
i i
Proof. FromTheoremB.1,
Ô£Æ Ô£π
E(cid:2) Œ¶2 i(cid:3) =f i(n)+E Ô£∞(cid:88) 1(N
x
=i‚àßN
y
=i)Ô£ª (31)
xÃ∏=y
(cid:34) (cid:35)
(cid:88)
‚â§f (n)+(S‚àí1)E 1(N =i) (32)
i x
x
=Sf (n) (if2i‚â§n). (33)
i
(34)
Thelemmadirectlyfollowsfromtheaboveinequality.
LemmaB.3.
g (n)‚â§S¬∑Œ≤‚àínoi ,
i min max
whereS =|X|,p =max p ,Œ≤ = 1 ,ando = pmax .
max x‚ààX x min 1‚àípmin max 1‚àípmax
Proof. 1 and x areincreasingfunctionsforx‚àà(0,1). Therefore,
1‚àíx 1‚àíx
g (n)= (cid:88) pi(1‚àíp )n‚àíi =
(cid:88)(cid:18)
1
(cid:19)‚àín(cid:18)
p x
(cid:19)i
‚â§|X|¬∑Œ≤‚àínoi .
i x x 1‚àíp 1‚àíp min max
x x
x‚ààX x‚ààX
TheoremB.4. ThevarianceoftheestimatorMÀÜB isboundedasfollows:
k
Var(MÀÜB)‚â§c ¬∑n2k+1¬∑c‚àín,
k 1 2
wherec =S¬∑(cid:0)e(cid:1)2k ,c =min(cid:16) 1 , 1‚àípmax (cid:17) . Inotherwords,Var(MÀÜB)=O(n2k+1¬∑Œ≤‚àín ¬∑max(1,on )).
1 k 2 1‚àípmin pmax(1‚àípmin) k min max
11HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
Proof. FromLemmaB.2inthesupplementary,wehaveVar(Œ¶ )‚â§Sf (n)‚àíf (n)2. Thus,
i i i
(cid:32) (cid:33)
Œ¶ Sf (n)‚àíf (n)2 g (n) S2¬∑Œ≤‚àínok+i
Var k+i ‚â§ k+i k+i ‚â§S¬∑ k+i ‚â§ min max (byLemmaB.3)
(cid:0) n (cid:1) (cid:0) n (cid:1)2 (cid:0) n (cid:1) (cid:0) n (cid:1)
k+i k+i k+i k+1
(cid:16) (cid:17)k
(cid:18) n(cid:19)2 Var(cid:32)
Œ¶
k+i(cid:33)
‚â§SŒ≤‚àín
(cid:0)n k(cid:1)2
ok+i ‚â§SŒ≤‚àín
e2 kn 22
ok+i ‚â§SŒ≤‚àín
(cid:18) e2n(k+i)(cid:19)k
ok+i
k (cid:0) n (cid:1) min(cid:0) n (cid:1) max min (cid:16) (cid:17)k max min k2 max
k+i k+1 n
k+i
(cid:18) e2n2(cid:19)k (cid:16)en(cid:17)2k
‚â§SŒ≤‚àín O =SŒ≤‚àín O , whereO =max(ok ,on ),
min k2 M min k M M max max
Var(MÀÜB)=(cid:18) n(cid:19)2 Var(cid:32)n (cid:88)‚àík
(‚àí1)i‚àí1Œ¶
k+1(cid:33)
k k (cid:0) n (cid:1)
i=1 k+1
(cid:18) n(cid:19)2 (cid:32)
Œ¶
(cid:33)
‚â§(n‚àík) Var k+1 (35)
k (cid:0) n (cid:1)
k+1
(cid:16)en(cid:17)2k
‚â§S(n‚àík) ¬∑Œ≤‚àínO =O(n2k+1)¬∑Œ≤‚àínO ,
k min M min M
where(35)followsfromCauchy-Schwarzinequality(Var((cid:80)M
X ) ‚â§ M
¬∑(cid:80)M
Var(X )). Theprooffollowsfrom
j=1 i j=1 i
dividing the variance of the estimator into two cases: o < 1 and o > 1: If o < 1, O = ok , and
max max max M max
Var(MÀÜB)=O(n2k+1Œ≤‚àín). Ifo >1,O =on ,and,Var(MÀÜB)=O(n2k+1Œ≤‚àínon ).
k min max M max k min max
Therefore,thevarianceexponentiallydecreaseswithnifp <0.5or 1‚àípmax <1.
max pmax(1‚àípmin)
CorollaryB.5. Thereexistsaconstantc>1suchthat
MSE(MÀÜB)‚â§O(n2k+1c‚àín).
k
Proof. FromEqu.(7)inthemanuscript,thebias|E(MÀÜB)‚àíM |‚â§S¬∑p ¬∑nk¬∑pn . Theprooffollowsfromthefact
k k max max
thatMSE=Var+Bias2andtheboundofthevarianceandthebias.
C.ComputingtheVarianceandtheMSEoftheEvolvedEstimators
SameasMÀÜB,theevolvedestimatorsfromthegeneticalgorithmarealsolinearcombinationsofŒ¶ (n)s(whilevarying
k k
bothk andnunlikeMÀÜB). GiventheevolvedestimatorMÀÜEvo = (cid:80) c Œ¶ (n ),theexpectedvalueofMÀÜEvo isgivenby
k k i i ki i k
substitutingŒ¶ (n)withf (n):
k k
E(MÀÜEvo)=(cid:88)
c f (n ). (36)
k i ki i
i
Giventhemultinomialdistributionp,thecovariancebetweenŒ¶ (n)andŒ¶ (n‚Ä≤),whichisneededtocomputethevariance
k k‚Ä≤
ofMÀÜEvoasEqu.(23),canbecomputedasfollows:
k
TheoremC.1. Giventhemultinomialdistributionp = (p 1,...,p S)withsupportsizeS,letXntotal bethesetofn
total
samplesfromp. LetXn andXn‚Ä≤ bethefirstnandn‚Ä≤ samplesfromXntotal,respectively;WLOG,weassume1‚â§n‚Ä≤ ‚â§
n ‚â§ n . Then,thecovarianceofŒ¶ (n) = Œ¶ (Xn)andŒ¶ (n‚Ä≤) = Œ¶ (Xn‚Ä≤)(1 ‚â§ k ‚â§ n,1 ‚â§ k‚Ä≤ ‚â§ n‚Ä≤)isgivenby
total k k k‚Ä≤ k‚Ä≤
following:
Cov(Œ¶ (n),Œ¶ (n‚Ä≤))=E[Œ¶ (n)¬∑Œ¶ (n‚Ä≤)]‚àíf (n)¬∑f (n‚Ä≤) (37)
k k‚Ä≤ k k‚Ä≤ k k‚Ä≤
(cid:34)(cid:32) (cid:33) (cid:32) (cid:33)(cid:35)
(cid:88) (cid:88)
=E 1(N =k) ¬∑ 1(N‚Ä≤ =k‚Ä≤) ‚àíf (n)¬∑f (n‚Ä≤) (38)
x x‚Ä≤ k k‚Ä≤
x x‚Ä≤
(cid:88)(cid:88)
= E[1(N =k‚àßN‚Ä≤ =k‚Ä≤)]‚àíf (n)¬∑f (n‚Ä≤), (39)
x x‚Ä≤ k k‚Ä≤
x x‚Ä≤
12HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
where N‚Ä≤ is the number of occurrences of x‚Ä≤ in Xn‚Ä≤. Depending on the values of n,n‚Ä≤,k,k‚Ä≤,x, and x‚Ä≤, the
x‚Ä≤
E[1(N =k‚àßN‚Ä≤ =k‚Ä≤)]canbecomputedasfollows:
x x‚Ä≤
‚àÄn,n‚Ä≤s.t. ‚àÄx,x‚Ä≤s.t. ‚àÄk,k‚Ä≤s.t. E[1(N x=k‚àßN‚Ä≤
x‚Ä≤
=k‚Ä≤)]
k=k‚Ä≤ (cid:0)n(cid:1) pk(1‚àíp )n‚àík
x=x‚Ä≤ k x x
n=n‚Ä≤ kÃ∏=k‚Ä≤ 0(infeasible)
xÃ∏=x‚Ä≤
k+k‚Ä≤‚â§n k!k‚Ä≤!(nn ‚àí! k‚àík‚Ä≤)!pk xpk x‚Ä≤ ‚Ä≤(1‚àíp x‚àíp x‚Ä≤)n‚àík‚àík‚Ä≤
k+k‚Ä≤>n 0(infeasible)
k‚Ä≤‚â§k (cid:0)n‚Ä≤(cid:1) pk‚Ä≤(1‚àíp )n‚Ä≤‚àík‚Ä≤¬∑(cid:0)n‚àín‚Ä≤(cid:1) pk‚àík‚Ä≤(1‚àíp )(n‚àín‚Ä≤)‚àí(k‚àík‚Ä≤)
x=x‚Ä≤ k‚Ä≤ x x k‚àík‚Ä≤ x x
nÃ∏=n‚Ä≤ k‚Ä≤>k 0(infeasible)
xÃ∏=x‚Ä≤
k+k‚Ä≤‚â§n (cid:80)m i=in m( ak x,n (0‚àí ,kk ‚àí‚Ä≤) (n‚àín‚Ä≤))k‚Ä≤!i!(nn ‚Ä≤‚àí‚Ä≤! k‚Ä≤‚àíi)!(k‚àíi)!((( nn ‚àí‚àí nn ‚Ä≤‚Ä≤ )) ‚àí! (k‚àíi))!pk x‚Ä≤ ‚Ä≤pk x‚Ä≤ ‚Ä≤(1‚àíp x‚àíp x‚Ä≤)n‚Ä≤‚àík‚Ä≤‚àíipk x(1‚àíp x)(n‚àín‚Ä≤)‚àí(k‚àíi)
k+k‚Ä≤>n 0(infeasible)
Proof. TheproofisstraightforwardfromthedefinitionofN andN‚Ä≤ .
x x‚Ä≤
GiventheexpectedvalueandthevarianceofMÀÜEvo,theMSEofMÀÜEvonaturallyfollows.
k k
D.DetailsoftheHyperparametersoftheEvolutionaryAlgorithm
ForevaluatingAlgorithm1,weusethefollowinghyperparameters:
‚Ä¢ SameastheOrlitsky‚Äôsstudy(Orlitsky&Suresh,2015),whichassesstheperformanceoftheGood-Turingestimator,
weusethehybridestimatorpÀÜoftheempiricalestimateandtheGood-Turingestimatetoapproximatetheunderlying
distribution{p } forestimatingtheMSEoftheevolvedestimator. ThehybridestimatorpÀÜisdefinedasfollows: If
x x‚ààX
N =k,
x
(cid:40)
c¬∑ k ifk <Œ¶ ,
pÀÜ = N k+1
x
c¬∑
MÀÜ kG
otherwise,
Œ¶k
(cid:80)
wherecisanormalizationconstantsuchthat pÀÜ =1.
x‚ààX x
‚Ä¢ The number of generations G = 100. To avoid the algorithm from converging to a local minimum, we limit the
maximumnumberofgenerationstobe2000.
‚Ä¢ Themutantsizem=40.
‚Ä¢ Whenselectingtheindividualsforthemutation,weusetournamentselectionwithtournamentsizet = 3,i.e.,we
randomlychoosethreeindividualswithreplacementandselectthebestone,andrepeatthisprocessmtimes.
‚Ä¢ Whenchoosingthetopthreeindividualswhenconstructingthenextgeneration,weuseelitistselection,i.e.,choosing
thetopthreeindividualswiththesmallestfitnessvalues.
‚Ä¢ Toavoidtheestimatorfrombeingtoocomplex,welimitthemaximumnumberoftermsintheestimatortobe20.
TheactualscriptimplementingAlgorithm1canbefoundatthepublicallyavailablerepository
https://anonymous.4open.science/r/Better-Turing-157F.
13HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
E.AdditionalExperimentalResults
Table2.TheMSEoftheGood-TuringestimatorMÀÜG,minimalbiasestimatorMÀÜB,andthebestevolvedestimatorMÀÜEvo andforthe
0 0 0
missingmassM ,thesuccessrateAÀÜ oftheevolvedestimator(X )againsttheGood-Turingestimator(X ),andtheratio(Ratio,
0 12 2 1
MSE(MEvo)/MSE(MÀÜG))fortwosupportsizesS =100and200,threesamplesizesnandsixdistributions.
0 0
S n/S Distribution MSE(MÀÜB) MSE(MÀÜG) MSE(MÀÜevo) AÀÜ Ratio
0 0 0 12
uniform 6.834e-03 6.681e-03 6.267e-03 62% 93%
half&half 6.821e-03 6.694e-03 4.489e-03 85% 67%
zipf-0.5 6.676e-03 6.565e-03 3.311e-03 94% 50%
0.5
zipf-1 4.995e-03 4.943e-03 3.065e-03 98% 62%
diri-1 6.166e-03 6.086e-03 3.202e-03 96% 52%
diri-0.5 5.223e-03 5.167e-03 2.708e-03 100% 52%
uniform 2.365e-03 2.351e-03 1.905e-03 88% 81%
half&half 2.200e-03 2.190e-03 1.439e-03 97% 65%
100 zipf-0.5 2.207e-03 2.194e-03 1.982e-03 75% 90%
1.0
zipf-1 1.713e-03 1.704e-03 1.705e-03 75% 100%
diri-1 1.787e-03 1.778e-03 1.066e-03 100% 60%
diri-0.5 1.388e-03 1.381e-03 8.747e-04 97% 63%
uniform 4.047e-04 4.028e-04 3.428e-04 89% 85%
half&half 4.237e-04 4.221e-04 3.035e-04 97% 71%
zipf-0.5 4.580e-04 4.561e-04 3.321e-04 95% 72%
2.0
zipf-1 4.826e-04 4.810e-04 3.633e-04 98% 75%
diri-1 3.946e-04 3.932e-04 2.473e-04 100% 62%
diri-0.5 3.276e-04 3.264e-04 2.587e-04 87% 79%
uniform 3.361e-03 3.323e-03 2.044e-03 95% 61%
half&half 3.357e-03 3.326e-03 1.968e-03 96% 59%
zipf-0.5 3.254e-03 3.227e-03 2.293e-03 89% 71%
0.5
zipf-1 2.335e-03 2.324e-03 2.410e-03 74% 103%
diri-1 3.011e-03 2.992e-03 2.359e-03 85% 78%
diri-0.5 2.563e-03 2.550e-03 1.813e-03 94% 71%
uniform 1.172e-03 1.169e-03 8.900e-04 99% 76%
half&half 1.092e-03 1.090e-03 8.584e-04 99% 78%
200 zipf-0.5 1.091e-03 1.088e-03 8.525e-04 97% 78%
1.0
zipf-1 8.185e-04 8.165e-04 7.244e-04 88% 88%
diri-1 8.898e-04 8.876e-04 6.652e-04 100% 74%
diri-0.5 6.900e-04 6.882e-04 4.861e-04 98% 70%
uniform 2.017e-04 2.012e-04 1.702e-04 93% 84%
half&half 2.113e-04 2.109e-04 1.716e-04 100% 81%
zipf-0.5 2.307e-04 2.302e-04 1.929e-04 100% 83%
2.0
zipf-1 2.390e-04 2.387e-04 2.109e-04 96% 88%
diri-1 1.961e-04 1.958e-04 1.648e-04 96% 84%
diri-0.5 1.609e-04 1.607e-04 1.315e-04 93% 81%
14