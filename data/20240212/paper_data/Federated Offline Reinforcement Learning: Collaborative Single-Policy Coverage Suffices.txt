Federated Offline Reinforcement Learning:
Collaborative Single-Policy Coverage Suffices
Jiin Woo∗ Laixi Shi† Gauri Joshi∗ Yuejie Chi∗
CMU Caltech CMU CMU
February 2024
Abstract
Offline reinforcement learning (RL), which seeks to learn an optimal policy using offline data, has
garnered significant interest due to its potential in critical applications where online data collection
is infeasible or expensive. This work explores the benefit of federated learning for offline RL, aiming
at collaboratively leveraging offline datasets at multiple agents. Focusing on finite-horizon episodic
tabular Markov decision processes (MDPs), we design FedLCB-Q, a variant of the popular model-free
Q-learning algorithm tailored for federated offline RL. FedLCB-Q updates local Q-functions at agents
with novel learning rate schedules and aggregates them at a central server using importance averaging
and a carefully designed pessimistic penalty term. Our sample complexity analysis reveals that, with
appropriately chosen parameters and synchronization schedules, FedLCB-Q achieves linear speedup in
terms of the number of agents without requiring high-quality datasets at individual agents, as long as
thelocaldatasetscollectivelycoverthestate-actionspacevisitedbytheoptimalpolicy,highlightingthe
powerofcollaborationinthefederatedsetting. Infact,thesamplecomplexityalmostmatchesthatofthe
single-agentcounterpart,asifallthedataarestoredatacentrallocation,uptopolynomialfactorsofthe
horizonlength. Furthermore,FedLCB-Qiscommunication-efficient,wherethenumberofcommunication
rounds is only linear with respect to the horizon length up to logarithmic factors.
Keywords: offline RL, federated RL, Q-learning, the principle of pessimism, sample complexity, linear
speedup, collaborative coverage
Contents
1 Introduction 2
1.1 Our contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Background and problem formulation 5
2.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Problem formulation: federated offline RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3 Proposed algorithm and theoretical guarantees 7
3.1 Algorithm description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3.2 Choices of key parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.3 Theoretical guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4 Analysis 13
4.1 Basic facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.2 Proof of Theorem 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
∗DepartmentofElectricalandComputerEngineering,CarnegieMellonUniversity,Pittsburgh,PA15213,USA.
†DepartmentofComputingMathematicalSciences,CaliforniaInstituteofTechnology,CA91125,USA.
1
4202
beF
8
]GL.sc[
1v67850.2042:viXra5 Discussions 19
A Technical lemmas 23
B Proofs for main results 25
B.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B.2 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
B.3 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
B.4 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
B.5 Proof of Lemma 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
B.6 Proof of Corollary 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
1 Introduction
Offline RL (Levine et al., 2020), also known as batch RL, addresses the challenge of learning a near-optimal
policy using offline datasets collected a priori, without further interactions with an environment. Fueled
by the cost-effectiveness of utilizing pre-collected datasets compared to real-time explorations, offline RL
has received increasing attention. However, the performance of offline RL crucially depends on the quality
of offline datasets due to the lack of additional interactions with the environment, where the quality is
determined by how thoroughly the state-action space is explored during data collection.
Encouragingly,recentresearch(Lietal.,2022;Rashidinejadetal.,2021;Shietal.,2022;Xieetal.,2021b)
indicates that being more conservative on unseen state-action pairs, known as the principle of pessimism,
enables learning of a near-optimal policy even with partial coverage of the state-action space, as long as the
distribution of datasets encompasses the trajectory of the optimal policy. However, acquiring high-quality
datasets that have good coverage of the optimal policy poses challenges because it requires the state-action
visitation distribution induced by a behavior policy employed for data collection to be very close to the
optimal policy. Alternatively, multiple datasets can be merged into one dataset to supplement insufficient
coverage of one other, but this may be impractical when offline datasets are scattered and cannot be easily
shared due to privacy and communication constraints.
Federated offline RL. Driven by the need to harvest multiple datasets to address insufficient coverage,
there is a growing interest in implementing offline RL in a federated manner without the need to share
datasets (Khodadadian et al., 2022; Woo et al., 2023; Zhou et al., 2023). For model-based RL, a study
has proposed a federated variant of pessimistic value iteration (Zhou et al., 2023), which requires sharing
of model estimates. On the other hand, for model-free RL, while Woo et al. (2023) introduced a federated
Q-learning algorithm that achieves linear speedup with collaborative coverage of agents, due to the absence
of pessimism, it still carries the risk of overestimation on state-action pairs that are insufficiently covered by
theagents. Indeed,itremainsunknownwhethertheprincipleofpessimismcanbeimplementedinfederated
offline RL to eliminate the risk of overestimation, while fully utilizing the collaborative coverage provided
by agents, and without sharing datasets or model estimates.
Our goal in this paper is to develop a federated variant of Q-learning (Watkins and Dayan, 1992) for
offline RL, which allows agents to learn a near-optimal Q-function with improved sample efficiency and
relaxed coverage assumption. In the single-agent case, pessimism is implemented by penalizing the value
estimates by subtracting a penalty term measuring the uncertainty of the estimates (Shi et al., 2022; Yan
et al., 2023). However, federated settings are communication-constrained, implying that agents only have a
limited chance of synchronization and they perform multiple local updates without knowing other agents’
training progress. Allowing multiple local updates leads to higher uncertainty of local Q-estimates beyond
the control of the pessimism penalty, potentially impacting both sample complexity and communication
efficiency. Thisunderscoresthetechnicalchallengeofincorporatingpessimismwhilemanaginglocalupdates
and raises the question:
How to judiciously incorporate the principle of pessimism in federated RL without hurting its sample and
communication efficiency?
21.1 Our contribution
This work presents a federated Q-learning algorithm with pessimism for offline RL, which achieves lin-
ear speedup and low communication cost, while requiring only collaborative coverage of the optimal policy.
Formally, we consider episodic finite-horizon tabular Markov decision processes (MDPs) with S states, A
actions, and horizon length H. A total number of M agents, each with K trajectories (collected using its
localbehaviorpolicy),collaborateinafederatedsettingwiththehelpofacentralservertolearntheoptimal
policy. Our main contributions are summarized as below; see also Table 1 for a detailed comparison.
• Federated Q-learning for offline RL. We propose a federated offline Q-learning algorithm named
FedLCB-Q, which involves iterative local updates at agents and global aggregation at a central server
with scheduled synchronizations. We introduce essential components that implement pessimism com-
pensating for the uncertainty in both local and global Q-function updates. First, to address the
uncertainty arising from independent local updates, we employ learning rate rescaling at local agents
and importance averaging at server aggregation. The former restricts the drifts of local Q-estimates
by rapidly decreasing the learning rates during local updates, and the latter reduces uncertainty of
the aggregated Q-estimates by assigning smaller weights to rarely updated local values. Additionally,
for every global aggregation, a global penalty calculated based on aggregated visitation counts is sub-
tracted from the aggregated global Q-estimate. These design choices play a crucial role in achieving
both sample and communication efficiency while preventing the overestimation of the Q-function.
• Linear speedup with collaborative single-policy coverage. Our analysis of sample complexity
of FedLCB-Q (see Theorem 1) demonstrates that FedLCB-Q finds an ε-optimal policy, as long as the
total number of samples per agent T =KH exceeds
(cid:32) (cid:33)
H7SC⋆
avg
O(cid:101) ,
Mε2
where C⋆ denotes the average single-policy concentrability coefficient of all agents (see (9) for the
avg
formal definition). This shows linear speedup in terms of number agents M, which is achieved with a
significantlyweakerdatarequirementatindividualagentsthanpriorart. Intruth,eachagentaffordsto
have a non-expert dataset collected by a sub-optimal behavior policy, as long as all agents collectively
cover the state-action pairs visited by the optimal policy, even they don’t cover the entire state-action
space as in Woo et al. (2023). The bound nearly matches the sample complexity obtained for a single-
agent pessimistic Q-learning algorithm (Shi et al., 2022) with a similar Hoeffding-style penalty, up to
a factor of H, as if all the datasets are processed at a central location.
• Low communication cost. Under appropriate choices of synchronization schedules, FedLCB-Q re-
quires approximately O(cid:101)(H) rounds of synchronizations to achieve the targeted accuracy (see Corol-
lary 1), which is almost independent with the size of the state-action space and the number of agents.
The analysis suggests that frequent synchronizations are not necessary, outperforming prior art (Woo
et al., 2023).
1.2 Related work
Offline RL. Offline RL addresses the problem of learning improved policies from a logged static dataset.
ThemainchallengeofofflineRLishowtoreliablyestimatethevaluesofunseenorrarelyvisitedstate-action
pairs. To tackle this challenge, most offline RL algorithms prevent agents from taking uncertain actions by
regularizingthepolicytobeclosetothebehaviorpolicy(FujimotoandGu,2021;Fujimotoetal.,2019;Siegel
et al., 2020) or penalizing value estimates on out-of-distribution state-action pairs (Kostrikov et al., 2022;
Kumar et al., 2020; Liu et al., 2020; Wu et al., 2019), which is also known as the principle of pessimism.
Recently, the pessimistic approach has been developed and theoretically studied for various RL settings,
such as model-based approaches (Jin et al., 2021; Kidambi et al., 2020; Kim and Oh, 2023; Li et al., 2022;
Rashidinejad et al., 2021; Xie et al., 2021b; Yin and Wang, 2021; Yu et al., 2020), policy-based approaches
(Xieetal.,2021a;Zanetteetal.,2021),andmodel-freeapproaches(Shietal.,2022;Ueharaetal.,2023;Yan
3number of sample communication
type reference coverage
agents complexity rounds
VI-LCB (Xie et al., 2021b) 1 single H6SC⋆ -
ε2
model-based
PEVI-Adv (Xie et al., 2021b) 1 single H4SC⋆ -
ε2
VI-LCB (Li et al., 2022) 1 single H4SC⋆ -
ε2
LCB-Q (Shi et al., 2022) 1 single H6SC⋆ -
ε2
LCB-Q-Adv (Shi et al., 2022) 1 single H4SC⋆ -
model-free ε2
FedAsynQ (Woo et al., 2023) M collaborative H6 HM
Mdavgε2 davg
FedLCB-Q (Theorem 1) M collaborative
H7SC a⋆
vg H
Mε2
Table 1: Comparison of sample complexity upper bounds of model-based and model-free algorithms for
offlineRLtolearnanε-optimalpolicyinfinite-horizonnon-stationaryMDPs,wherelogarithmicfactorsand
burn-in costs are hidden. Here, S is the size of state space, A is the size of action space, H is the horizon
length, M is the number of agents, C⋆ and C⋆ denote the single-policy concentrability and the average
avg
single-policy concentrability, respectively (cf. (7) and (8)), and d is the minimum entry of the average
avg
stationary state-action occupancy distribution of all agents. We follow standard conversion to translate the
best sample complexity in Woo et al. (2023) to the finite-horizon setting for comparison.
et al., 2023). Most of these works have focused on the single-agent case and suggested that the state-action
visitation distribution induced by the behavior policy should cover that of the optimal policy (Rashidinejad
et al., 2021; Shi et al., 2022; Yan et al., 2023), and the distribution mismatch among the two visitation
distributions governs the hardness of offline RL (Li et al., 2022). Another interesting work (Shi et al., 2023)
consideredofflineRLfrommultipleperturbeddatasources,requiringacentralizedsettinginwhichanagent
has full access to all the datasets.
Federated RL. TherehasbeenanincreasinginterestinfederatedanddistributedRL,drivenbytheneed
toaddressmorerealisticconstraints, includingprivacy, communicationefficiency, anddataheterogeneity, as
well as training speedup. Recent works have investigated federated RL from various perspectives, such as
robustness to adversarial attacks (Fan et al., 2021; Wu et al., 2021), environment or task heterogeneity (Jin
et al., 2022; Wang et al., 2023; Yang et al., 2023; Zhou et al., 2023), as well as sample and communication
complexitiesunderasynschronoussampling(Khodadadianetal.,2022;Wooetal.,2023)andonlinesampling
(Zhangetal.,2024;Zhengetal.,2023). Formodel-basedRL,Zhouetal.(2023)studiedapessimisticvariant
ofvalueiterationwithmulti-taskofflinedatasetsunderthefederatedsettingandshowedtheimprovedsample
efficiency by sharing representations of common task structures. However, for model-free RL, although Woo
et al. (2023) provided a federated Q-learning algorithm that achieves linear speedup in terms of the number
of agents with relaxed coverage assumption for individual agents, it still requires agents to cover the entire
state-action space uniformly due to the lack of pessimism.
Q-learning. Characterizing the finite-sample complexity of single-agent Q-learning has been examined
extensively under various data collection and function approximation schemes, including but not limited
the synchronous setting (Beck and Srikant, 2012; Even-Dar and Mansour, 2003; Li et al., 2024; Wainwright,
2019), the asynchronous and offline setting (Li et al., 2024, 2021; Qu and Wierman, 2020; Shi et al., 2022;
Yan et al., 2023), the online setting (Bai et al., 2019; Jin et al., 2018; Wang et al., 2019), under function
approximation (Chen et al., 2019; Fan et al., 2020; Xu and Gu, 2020), to mention just a few.
4Notation. Inthispaper,weuse∆( )torefertotheprobabilitysimplexoveraset ,and[K]:= 1, ,K
S S { ··· }
for any positive integer K >0. In addition, f()=O(cid:101)(g()) or f ≲g (resp. f()=Ω(cid:101)(g()) or f ≳g) indicates
that f() is order-wise not larger than (resp.·not small·er than) g() up to s·ome loga·rithmic factors. The
notation· f g signifies that both f ≲g and f ≳g simultaneously h·old.
≍
2 Background and problem formulation
2.1 Background
Basics of episodic finite-horizon MDPs. Consider an episodic finite-horizon MDP represented by
=(cid:0) , ,H, P H , r H (cid:1) ,
M S A { h }h=1 { h }h=1
where isthestatespaceofsizeS, istheactionspaceofsizeA,H isthehorizonlength,P : ∆( )
h
and r S : [0,1] denote theAprobability transition kernel and the reward function atSth×eA h-→th timSe
h
step (1 S h×A H→ ), respectively.
A po≤licy≤is denoted by π = π H , where π : ∆( ) specifies the probability distribution over the
action space at time step h in{stah t}eh= s1 . With sligh htSab→use oAf notation, we use π (s) to denote the selected
h
action when the policy π is deterministic. For h=1,...,H, the value function Vπ(s) of policy π is defined
h h
as the expected cumulative rewards starting from state s at step h by following π, i.e.,
(cid:34) (cid:35)
Vπ(s):=E
(cid:88)H
r (cid:0) s ,a
(cid:1)(cid:12)
(cid:12)s =s , (1)
h t t t (cid:12) h
t=h
where the expectation is taken over the randomness of the trajectory s ,a ,r H induced by the policy π
as well as the MDP transitions according to a π ( s ) and s { Pt ( t s ,t a}t )= .h Similarly, the Q-function
t t t t+1 t t t
Qπ(s,a) of a policy π at step h in state-action p∼air (s·| ,a) is defined∼as ·|
h
(cid:34) (cid:35)
(cid:88)H (cid:12)
Qπ(s,a):=r (s,a)+E r (s ,a )(cid:12)s =s,a =a , (2)
h h t t t (cid:12) h h
t=h+1
where the expectation is again over the randomness induced by π and the MDP transitions.
Itiswell-known(Puterman,2014)thatonecanalwaysfindadeterministicoptimalpolicyπ⋆ = π⋆ H ,
which maximizes the value function (resp. the Q-function) simultaneously over all states (resp.
sta{te-h a}ch t= io1
n
pairs) among all policies. The resulting optimal value function V⋆ = V⋆ H and optimal Q-functions
Q⋆ = Q⋆ H are denoted respectively by { h}h=1
{ h}h=1
V⋆(s):=Vπ⋆ (s)=maxVπ(s), Q⋆(s,a):=Qπ⋆ (s,a)=maxQπ(s,a)
h h h h h h
π π
for any (s,a,h) [H]. Given an initial state distribution ρ ∆( ), the expected value of a given
policy π and tha∈t oSf×thAe o×ptimal policy π⋆ at the initial step are defin∈ed reSspectively by
Vπ(ρ):=E (cid:2) Vπ(s )(cid:3) and V⋆(ρ):=E (cid:2) V⋆(s )(cid:3) . (3)
1 s1∼ρ 1 1 1 s1∼ρ 1 1
Bellman equations. Of crucial importance are the Bellman equations that connect the value functions
across different time steps (Bertsekas, 2017). For any policy π, it follows that
Qπ h(s,a)=r h(s,a)+E s′∼Ph,s,a(cid:2) V hπ +1(s′)(cid:3) (4)
for all (s,a,h) [H], where Vπ (s) = 0 for any s . Moreover, Bellman’s optimality equation
says that ∈ S ×A× H+1 ∈ S
Q⋆ h(s,a)=r h(s,a)+E s′∼Ph,s,a(cid:2) V h⋆ +1(s′)(cid:3) (5)
for all (s,a,h) [H], and the optimal policy satisfies π⋆(s)=argmax Q⋆(s,a).
∈S×A× h a∈A h
52.2 Problem formulation: federated offline RL
In offline RL, one has access to a offline dataset containing episodes collected by following some behavior
policy. Here, we formulate a federated version of the offline RL problem with M agents, where each agent
has access to a local offline dataset. For 1 m M, the offline dataset m at agent m is composed of K
episodes,1 each generated independently ac≤cordin≤g to a behavior policy µmD = µm H , resulting in
{ h}h=1
m
:=(cid:110)(cid:0)
sm , am , rm , ...,sm , am , rm
(cid:1)(cid:111)K
,
D k,1 k,1 k,1 k,H k,H k,H k=1
where the initial state sm ρ is drawn from some initial state distribution ρ ∆( ), sm am , rm are the
state, action and rewardk, a1 t∼step h in the k-th episode, am µm( sm ) and∈ rm =S r (k s, mh k a, mh ).k,h
k,h ∼ h ·| k,h k,h h k,h k,h
Goal. The goal of federated offline RL is to learn an ε-optimal policy π = π H satisfying
(cid:98) {(cid:98)h }h=1
V⋆(ρ) Vπ(cid:98)(ρ) ε
1 − 1 ≤
using the history dataset = (cid:8) m(cid:9) without sharing the local offline datasets, with the help of a
D D 1≤m≤M
parameter server. Furthermore, it is greatly desirable to achieve as high accuracy as possible, in a memory-
and communication-efficient manner.
Metric. Obviously, the success of offline RL highly relies on the quality of the history dataset. In order to
define the metric, let us first introduce the occupancy distributions dπ(s) and dπ(s,a) induced by policy π
h h
at step h, given by
dπ(s):=P(s =s s ρ,π), dπ(s,a):=P(s =s s ρ,π)π (a s). (6)
h h | 1 ∼ h h | 1 ∼ h |
Recent works (Rashidinejad et al., 2021; Shi et al., 2022; Xie et al., 2021b) have advocated the notion of
single-policy concentrability, which measures the mismatch between the occupancy distributions induced by
the optimal policy π⋆ and the behavior policy µ, with the benefit that this assumes away the need for the
offline dataset to cover the entire state-action space, which is often impractical. Li et al. (2022) offered a
more refined notion called single-policy clipped concentrability, defined as follows.
Definition 1 (single-policy clipped concentrability). The single-policy clipped concentrability coefficient
C⋆ [1/S, ) of a behavior policy µ is defined to be the smallest quantity that satisfies
∈ ∞
min
dπ⋆
(s,a), 1/S
max { h } C⋆, (7)
(h,s,a)∈[H]×S×A dµ h(s,a) ≤
where we adopt the convention 0/0=0.
Thesingle-policyclippedconcentrabilitycoefficientC⋆ < isfinitewheneverthebehaviorpolicycovers
thestate-actionpairsvisitedbytheoptimalpolicy, rathertha∞nhavingtocovertheentirestate-actionspace.
Recall that since π⋆ is deterministic, dπ⋆ (s,a) = dπ⋆ (s)I(a = π⋆(s)), that is, dπ⋆ (s,a) is non-zero only for
h h h h
the optimal action a = π⋆(s). Compared with the unclipped counterpart introduced in Rashidinejad et al.
h
(2021), the clipping of the occupancy distribution dπ⋆ (s,a) by the threshold 1/S ensures that C⋆ will not
h
be excessively large when dπ⋆ (s) is highly concentrated in a small number of states in state space.
h
In the federated setting, we further introduce a tailored notion that highlights the potential benefit of
collaborative learning in the presence of multiple agents. For ease of notation, denote
dm(s)=dµm (s) and dm(s,a)=dµm (s,a)
h h h h
as the occupancy distributions induced by the behavior policy µm at agent m. Based on these, we define
the average occupancy distributions as
M M
1 (cid:88) 1 (cid:88)
davg(s)= dm(s) and davg(s,a)= dm(s,a). (8)
h M h h M h
m=1 m=1
1Forsimplicity,weassumealltheagentshavethesamenumberofepisodes. Itisstraightforwardtogeneralizetothescenario
whenthelocalofflinedatasetshavedifferentsizes.
6Algorithm 1: Federated pessimistic Q-learning (FedLCB-Q)
1: Parameters: horizon length H, number of agents M, total number of episodes per agent K,
(cid:16) (cid:17)
synchronization schedule (K), target error δ (0,1), ζ =log SAK2MH , c >0.
T ∈ 1 δ B
2: Initialization: set Qm 0,h(s,a)=0, V 0m ,h(s)=0, N 0m ,h(s,a)=0, nm 0,h(s,a)=0, N 0,h(s,a)=0,
n (s,a)=0 for all (m,s,a,h) [M] [H +1].
0,h
∈ ×S×A×
for k =1, ,K do
···
/* Update the local Q-estimate and visitation counts at each agent */
1 (Qm ,nm ) = Local-Q-learning();
k,h k,h
2 if k (K) then
∈T
/* Agent-to-server communication */
3
Agents communicate Qm and nm to the server;
k,h k,h
/* Global pessimistic averaging in a server */
4 (Q k,h,V k,h,π k,h) = Global-pessimistic-averaging();
/* Server-to-agent communication */
5 Server communication Q k,h, V k,h and N k,h to agents;
/* Synchronize local Q-estimates */
6 for (m,s,a,h) [M] [H] do
7 Qm
k,h(s,a)=∈
Q
k,h(× s,aS )×, VA
km
,h×
(s)=V k,h(s)
return: Q(cid:98) = Q
K,h h∈[H]
and π
(cid:98)
= π
K,h
h∈[H].
{ } { }
Definition 2 (average single-policy clipped concentrability). The average single-policy concentrability coef-
ficient C⋆ [1/S, ) of multiple behavior policies µm is defined to be the smallest quantity that
avg ∈ ∞ { }m∈[M]
satisfies
min
dπ⋆
(s,a),1/S
max { h } C⋆ , (9)
(h,s,a)∈[H]×S×A da hvg(s,a) ≤ avg
where we adopt the convention 0/0=0.
An important implication of the above definition is that, as long as the agents collaboratively cover the
state-actionpairsvisitedbytheoptimalpolicy,theaveragesingle-policyconcentrabilitycoefficientC⋆ <
is finite. Therefore, this is much weaker than the coverage requirement in the single-agent case. avg ∞
3 Proposed algorithm and theoretical guarantees
In this section, we first introduce the proposed model-free federated offline RL algorithm called FedLCB-Q,
followed by its theoretical performance guarantees.
3.1 Algorithm description
WeintroduceafederatedvariantofQ-learningalgorithmforofflineRL,calledFedLCB-Q,thatlearnsanear-
optimal Q-function without overestimation on unseen components of the state-action space. The complete
descriptionofFedLCB-QisprovidedinAlgorithm1,withitsagent-endandserver-endsubroutinesdescribed
in Algorithm 2 and Algorithm 3 respectively. On a high level, FedLCB-Q performs local Q-function updates
at all the agents using its own local offline dataset, and occasionally, globally aggregates the local estimates
in a pessimistic fashion at a central server. To facilitate flexible communication patterns, we follow a
synchronizationschedule (K),whichcontainstheindicesofepisodeswherecommunicationoccursbetween
the agents and the server.T
To begin, FedLCB-Q initializes the local estimate (Qm and Vm) at each agent m [M] and the global
estimates (Q and V ) at the server as follows: 0,h 0,h ∈
0,h 0,h
Qm (s,a)=0, Vm(s,a)=0, for all (s,a,h) [H +1], (10a)
0,h 0,h ∈S×A×
7k<latexit sha1_base64="dzjC6zTBey7maEvFYK8Vsu5D1L8=">AAAB+3icbVDLSsNAFL2pr1pfsS7dBItQNyURUZdFN4KbCn1BE8pkOmmHTiZhZiKWkF9x40IRt/6IO//GSZuFth4YOJxzL/fM8WNGpbLtb6O0tr6xuVXeruzs7u0fmIfVrowSgUkHRywSfR9JwignHUUVI/1YEBT6jPT86W3u9x6JkDTibTWLiReiMacBxUhpaWhWpy7lbojUBCOWtrP6/dnQrNkNew5rlTgFqUGB1tD8ckcRTkLCFWZIyoFjx8pLkVAUM5JV3ESSGOEpGpOBphyFRHrpPHtmnWplZAWR0I8ra67+3khRKOUs9PVknlIue7n4nzdIVHDtpZTHiSIcLw4FCbNUZOVFWCMqCFZspgnCguqsFp4ggbDSdVV0Cc7yl1dJ97zhXDach4ta86aoowzHcAJ1cOAKmnAHLegAhid4hld4MzLjxXg3PhajJaPYOYI/MD5/AJGDlCE=</latexit> (K) K<latexit sha1_base64="jKmO79SQ9SoqRzUlbAdsUD/6yLI=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KomIeix6Eby0YD+gDWWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDDPzgkRwbVz321lZXVvf2CxsFbd3dvf2SweHTR2nimGDxSJW7YBqFFxiw3AjsJ0opFEgsBWMbqd+6wmV5rF8MOME/YgOJA85o8ZK9fteqexW3BnIMvFyUoYctV7pq9uPWRqhNExQrTuemxg/o8pwJnBS7KYaE8pGdIAdSyWNUPvZ7NAJObVKn4SxsiUNmam/JzIaaT2OAtsZUTPUi95U/M/rpCa89jMuk9SgZPNFYSqIicn0a9LnCpkRY0soU9zeStiQKsqMzaZoQ/AWX14mzfOKd1nx6hfl6k0eRwGO4QTOwIMrqMId1KABDBCe4RXenEfnxXl3PuatK04+cwR/4Hz+AKP9jNU=</latexit>
Episodes 2T
1<latexit sha1_base64="SvhELfo3OXLhszEJ7nGzzUnRMWk=">AAAB8XicbVBNSwMxEJ2tX7V+VT16CRbBQym7Iuqx6EXwUsF+YLuUbDbbhmaTJckKZem/8OJBEa/+G2/+G9N2D9r6YODx3gwz84KEM21c99sprKyurW8UN0tb2zu7e+X9g5aWqSK0SSSXqhNgTTkTtGmY4bSTKIrjgNN2MLqZ+u0nqjST4sGME+rHeCBYxAg2Vnr0qj0SSqOrd/1yxa25M6Bl4uWkAjka/fJXL5QkjakwhGOtu56bGD/DyjDC6aTUSzVNMBnhAe1aKnBMtZ/NLp6gE6uEKJLKljBopv6eyHCs9TgObGeMzVAvelPxP6+bmujKz5hIUkMFmS+KUo6MRNP3UcgUJYaPLcFEMXsrIkOsMDE2pJINwVt8eZm0zmreRc27P6/Ur/M4inAEx3AKHlxCHW6hAU0gIOAZXuHN0c6L8+58zFsLTj5zCH/gfP4AjbiQMQ==</latexit> , ,K
···
Local updates Aggregation
Central server
… …
…
Agent 1
… …
Agent 𝑀 Q<latexit sha1_base64="FtHR7xtsWmxaZJC4riIRfghcI+o=">AAAB7HicbVBNS8NAEJ34WetX1aOXxSJ4KomIeix68SK0YNpCG8tmu2mXbjZhdyKU0t/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl3321lZXVvf2CxsFbd3dvf2SweHDZNkmnGfJTLRrZAaLoXiPgqUvJVqTuNQ8mY4vJ36zSeujUjUA45SHsS0r0QkGEUr+fWu93jfLZXdijsDWSZeTsqQo9YtfXV6CctirpBJakzbc1MMxlSjYJJPip3M8JSyIe3ztqWKxtwE49mxE3JqlR6JEm1LIZmpvyfGNDZmFIe2M6Y4MIveVPzPa2cYXQdjodIMuWLzRVEmCSZk+jnpCc0ZypEllGlhbyVsQDVlaPMp2hC8xZeXSeO84l1WvPpFuXqTx1GAYziBM/DgCqpwBzXwgYGAZ3iFN0c5L8678zFvXXHymSP4A+fzByFrjj4=</latexit> M Q<latexit sha1_base64="MgMvB3V9RNOTWl/hcjJ5TADu+R4=">AAAB7HicbVBNSwMxEJ3Ur1q/qh69BIvgqewWUY9FL16EFty20K4lm2bb0Gx2SbJCWfobvHhQxKs/yJv/xrTdg7Y+GHi8N8PMvCARXBvH+UaFtfWNza3idmlnd2//oHx41NJxqijzaCxi1QmIZoJL5hluBOskipEoEKwdjG9nfvuJKc1j+WAmCfMjMpQ85JQYK3nNfu3xvl+uOFVnDrxK3JxUIEejX/7qDWKaRkwaKojWXddJjJ8RZTgVbFrqpZolhI7JkHUtlSRi2s/mx07xmVUGOIyVLWnwXP09kZFI60kU2M6ImJFe9mbif143NeG1n3GZpIZJulgUpgKbGM8+xwOuGDViYgmhittbMR0RRaix+ZRsCO7yy6ukVau6l1W3eVGp3+RxFOEETuEcXLiCOtxBAzygwOEZXuENSfSC3tHHorWA8plj+AP0+QMi8Y4/</latexit> M Q<latexit sha1_base64="7GcedvK+pKEbFZR3nNqdtzjy/kc=">AAAB7HicbVBNS8NAEJ34WetX1aOXxSJ4KomIeix68SK0YNpCG8tmu2mXbjZhdyKU0t/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl3321lZXVvf2CxsFbd3dvf2SweHDZNkmnGfJTLRrZAaLoXiPgqUvJVqTuNQ8mY4vJ36zSeujUjUA45SHsS0r0QkGEUr+fXu8PG+Wyq7FXcGsky8nJQhR61b+ur0EpbFXCGT1Ji256YYjKlGwSSfFDuZ4SllQ9rnbUsVjbkJxrNjJ+TUKj0SJdqWQjJTf0+MaWzMKA5tZ0xxYBa9qfif184wug7GQqUZcsXmi6JMEkzI9HPSE5ozlCNLKNPC3krYgGrK0OZTtCF4iy8vk8Z5xbusePWLcvUmj6MAx3ACZ+DBFVThDmrgAwMBz/AKb45yXpx352PeuuLkM0fwB87nD3nHjng=</latexit> M Q<latexit sha1_base64="bxw3ux9PBeducStuvM1MaR7bVh8=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHstHM0nQj+hQ8pAzaqz00OiP++WKW3XnIKvEy0kFctT75a/eIGZphNIwQbXuem5i/Iwqw5nAaamXakwoG9Mhdi2VNELtZ/NTp+TMKgMSxsqWNGSu/p7IaKT1JApsZ0TNSC97M/E/r5ua8MbPuExSg5ItFoWpICYms7/JgCtkRkwsoUxxeythI6ooMzadkg3BW355lbQuqt5V1WtcVmq3eRxFOIFTOAcPrqEG91CHJjAYwjO8wpsjnBfn3flYtBacfOYY/sD5/AEqJo25</latexit>
1 2 k k
Figure 1: FedLCB-Q with M agents and a central server. Each agent m performs local updates on its
local Q-table Qm for each kth episode in a local history dataset m. When synchronization is scheduled at
k (K), the k agents send their local Q-tables to the server andDthe server aggregates the Q-tables into a
glo∈baTl Q-table and synchronizes local Q-tables.
Q (s,a)=0, V (s,a)=0, for all (s,a,h) [H +1]. (10b)
0,h 0,h
∈S×A×
Then, FedLCB-Q proceeds the following steps for each episode k [K].
∈
1. Local updates: Each agent m samples the kth trajectory (sm ,am ,rm ) H from its local offline
datasets m. For each step h [H], agent m updates its loc{alk Q,h -estk i, mh atk e,h Q} mh=1 as follows:
D ∈ k,h
(cid:40)
(1 ηm (s,a))Qm (s,a)+ηm (s,a)(rm +Vm (sm )) if (s,a)=(sm ,am )
Qm (s,a)= − k,h k−1,h k,h k,h k−1,h+1 k,h+1 k,h k,h ,
k,h Qm (s,a) otherwise
k−1,h
(11)
where ηm (s,a) is the learning rate, whose schedule will be specified later, and Vm (s) is set as
k,h k−1,h
Vm (s)=Vm (s)=V (s), for all (m,s,h,k) [M] [H +1] [K], (12)
k−1,h ι(k),h ι(k),h ∈ ×S× ×
where ι(k) denotes the most recent episode where aggregation occurs before the kth episode, i.e.,
ι(k):=max 1 k′ <k :k′ (K) .
k′ { ≤ ∈T }
2. Pessimistic aggregation: If synchronization is scheduled at episode k, i.e., k (K), each agent
sends its local Q-estimate to a central server for aggregation after finishing the ∈locTal update for the
kth episode. Then, the server updates the global Q-estimate Q by averaging the local Q-estimates
k,h
and subtracting a penalty as follows:
(cid:32) (cid:33)
M
(cid:88)
(s,a) : Q (s,a)= αm (s,a)Qm (s,a) B (s,a), (13)
∀ ∈S×A k,h k,h k,h − k,h
m=1
whereαm =[αm (s,a)] [0,1]SAisanentry-wiseweightmatrixassignedtoagentmforeach
h [H],k, ah nd Bk,h (s,a) i( ss,a a)∈ pS e× nA alt∈y term (to be specified later below) that introduces the pessimism
k,h
pr∈eventing the overestimation of unseen state-action pairs. Accordingly, the global value estimate is
updated as
(cid:26) (cid:27)
(s,a) : V (s)=max V (s), maxQ (s,a) . (14)
k,h ι(k),h k,h
∀ ∈S×A a∈A
8
…Algorithm 2: Local-Q-learning (agents)
1: for m=1, ,M do
Sample the· k·-·th trajectory (sm ,am ,rm ,sm ) H from m
{ k,h k,h k,h k,h+1 }h=1 D
for h=1, ,H do
···
for (s,a) do
Qm (s∈ ,aS )=×A Qm (s,a), Vm(s)=Vm (s)
k,h k−1,h k,h k−1,h
// Update the local counters and learning rates
1 nm k,h(sm k,h,am k,h)=nm k−1,h(sm k,h,am k,h)+1
2 η km ,h(sm k,h,am k,h)= Nι(k),h(sm k,h,am k,h)M +M(H (+ H1 +) 1)nm k,h(sm k,h,am k,h)
// Update local Q-estimates
(cid:0) (cid:1)
3 Qm k,h(sm k,h,am k,h)= 1 −η km ,h(sm k,h,am k,h) Qm k−1,h(sm k,h,am k,h)+η km ,h(s,a)(r km ,h+V km −1,h+1(sm k,h+1))
Algorithm 3: Global-pessimistic-averaging (server)
1: for (s,a,h) [H] do
∈S×A×
// Update the average counter
1 n k,h(s,a)=(cid:80)M m=1nm k,h(s,a), N k,h(s,a)=N ι(k),h(s,a)+n k,h(s,a)
// Compute global penalty and averaging weights
(cid:113)
2 B k,h(s,a)= Nk,( hH (s+ ,a1 )) +n Hk,h n( ks ,, ha ()
s,a)
NcB k,ζ h12 (H s,a4
)
if N k,h(s,a)>0, otherwise, B k,h(s,a)=0
3 for m=1 M do
4 αm (s,· a· )· = 1 Nι(k),h(s,a)+M(H+1)nm k,h(s,a) if nm (s,a)>0, otherwise, αm (s,a)= 1
k,h M Nk,h(s,a)+Hnk,h(s,a) k,h k,h M
// Update global Q-estimates
5 Q k,h(s,a)=(cid:80) (cid:8)M m=1α km ,h(s,a)Qm k,h(s,a) −B k, (cid:9)h(s,a)
6 V k,h(s)=max V ι(k),h(s),max a∈AQ k,h(s,a)
7 π k,h(s)=argmax a∈AQ k,h(s,a) if V k,h(s)=max a∈AQ k,h(s,a), otherwise, π k,h(s)=π ι(k),h(s)
wheretheoutermaximumensuresamonotonicupdate,asweexplainlaterintheanalysis. IfV (s)=
k,h
max Q (s,a),theglobalpolicyisupdatedasπ (s)=argmax Q (s,a),otherwiseπ (s)=
a∈A k,h k,h a∈A k,h k,h
π (s). Afteraggregation,theserversendstheglobalQ-functionandvalueestimatestoeveryagent,
ι(k),h
where
(k,m) (K) [M]: Qm =Q , Vm =V . (15)
∀ ∈T × k,h k,h k,h k,h
At the end of K episodes, FedLCB-Q outputs a global Q-estimate Q(cid:98)h(s,a)=Q K,h(s,a) for all (s,a,h)
[H] and a solution policy π (s) = π (s) for all (s,h) [H]. For simplicity, we assume tha∈t
(cid:98)h K,h
tSh×e aAgg×regation step always occurs after the last episode K, i.e.,∈ KS × (K).
∈T
3.2 Choices of key parameters
The success of FedLCB-Q relies on careful and judicious selections of key algorithmic parameters, in a data-
drivenmanner,whichwedetailbelow. Tobegin,letusintroducethefollowingusefulnotation,whichpertains
tothecountersforvisitsofagentsoneachstate-actionpair(s,a) . Forany(m,k,h) [M] [K] [H],
∈S×A ∈ × ×
• nm (s,a): the number of episodes in the interval (ι(k),k] during which agent m visits (s,a) at step h,
k,h
i.e., nm (s,a):= ι(k)<i k :(sm ,am )=(s,a) .
k,h |{ ≤ i,h i,h }|
• Nm (s,a): the number of episodes in the interval [1,k] during which agent m visits (s,a) at step h,
k,h
i.e., Nm (s,a):= 1 i k :(sm ,am )=(s,a) .
k,h |{ ≤ ≤ i,h i,h }|
• n (s,a): the total number of episodes in the interval (ι(k),k] during which all agents visit (s,a) at
k,h
step h, i.e., n (s,a):=(cid:80)M nm (s,a)= ι(k)<i k :(sm ,am )=(s,a) .
k,h m=1 k,h |{ ≤ i,h i,h }|
9• N (s,a): the total number of episodes in the interval [1,k] during which all agents visit (s,a) at step
k,h
h, i.e., N (s,a):=(cid:80)M Nm (s,a)= 1 i k :(sm ,am )=(s,a) .
k,h m=1 k,h |{ ≤ ≤ i,h i,h }|
Pessimism in the federated RL. In offline RL, pessimism is key to preventing the overestimation of Q-
function on unseen state-action space. For a single-agent case, the pessimism is implemented by subtracting
a penalty term computed based on the visiting counter of an agent for each state-action pair, which makes
the estimation highly dependent on the quality of agents’ datasets (Rashidinejad et al., 2021). For example,
when an agent has non-expert data collected using a highly sub-optimal behavior policy, it is inevitable
to subtract a large penalty for optimal actions that cannot be reached with the agent’s behavior policy,
and this leads to slow convergence or convergence to a sub-optimal policy close to the behavior policy. In
the federated setting, from the perspective of a server, as the aggregated information from multiple agents
increases confidence, it is natural to be less pessimistic compared to an individual agent. Based on this
intuition, given some prescribed probability δ (0,1), we suggest a global penalty computed with the
aggregated counters of agents at k (K): ∈
∈T
(cid:40) (cid:113)
(H+1)nk,h(s,a) cBζ 12H4 if N (s,a)>0
B k,h(s,a):= Nk,h(s,a)+Hnk,h(s,a) Nk,h(s,a) k,h , (16)
0 if N (s,a)=0
k,h
(cid:16) (cid:17)
where ζ = log SAMK2H and c is some positive constant. Here, the penalty for each state-action pair
1 δ B
decreasesaslongastheagentscollectivelyexplorethestate-actionpairenough. Thisrelaxesthedependency
on an individual agent and prevents the estimated policy from being restricted to a local behavior policy.
Local update uncertainty. To guarantee that the pessimism introduced by the global penalty is enough
to prevent overestimation on rarely seen state-action pairs, the penalty should dominate the uncertainty of
the Q-estimates. However, when agents independently update their own local Q-estimates without frequent
communication, the global penalty, which is subtracted only at the aggregation step, may fail to cover the
increasing uncertainty of the local Q-estimates during local updates. To handle this, we propose a choice
of key parameters (learning rates ηm and averaging weights αm ) that effectively controls the uncertainty
k,h k,h
arising from the local updates as follows.
• Importance averaging. In the federated setting, agents have offline datasets with heterogeneous
distributions induced by different behavior policies, leading to imbalanced uncertainty of local Q-
estimates. To minimize the uncertainty of the averaged estimate, we propose the following entrywise
weighting scheme for averaging:
(cid:40) 1 Nι(k),h(s,a)+(H+1)Mnm k,h(s,a) if n (s,a)>0
αm (s,a):= M Nk,h(s,a)+Hnk,h(s,a) k,h . (17)
k,h 1 if n (s,a)=0
M k,h
ByassigningsmallerweightstolessfrequentlyupdatedlocalQ-estimateswithsmallernm (s,a),which
k,h
has high uncertainty, the averaged Q-estimate can always maintain an uncertainty level low enough to
be dominated by the global penalty, regardless of the heterogeneity in local data distributions. The
idea aligns with the notion of importance averaging introduced by Woo et al. (2023), which favors
frequentlyupdatedlocalQ-values. Nevertheless,ourapproachdiffersinthat,unlikeWooetal.(2023),
where the assigned weights are determined solely based on local counters nm in a myopic manner,
k,h
our weights, factoring in the global counter N , limit bias towards specific agents as the training
ι(k),h
oflocalQ-estimatesstabilizes. Theweightingscheme, mindfuloftheentiretrainingprogress, prevents
some local values that have undergone intense updates recently from dominating the global learning
of the Q-function, preserving the information accumulated through old updates.
• Learning rates rescaling. Local updates without synchronization increase the deviation of local Q-
estimates, and this increases the variance of the global Q-estimate at aggregation. However, requiring
agents to communicate frequently may be too stringent for many applications in the federated setting.
10To address this issue, we propose a novel choice of learning rate that exhibits slower decay based on a
global counter N , and faster decay during local updates according to the local counter nm :
ι(k),h k,h
M(H +1)
ηm (s,a):= . (18)
k,h N (s,a)+M(H +1)nm (s,a)
ι(k),h k,h
Therescalingofthelearningrateiscrucialtoobtainlinearspeedupwithoutfrequentsynchronizations.
The gradual decay with a global counter allows more aggressive updates of the Q-estimates once
collective information from all agents is aggregated, which enables convergence speedup. On the other
hand,thefastdecreaseinlearningratesduringlocalupdatesensuresthatagentsadaptivelyslowdown
their drifts and maintain low variance of their local Q-estimates, without overly restricting the length
of local updates. We will further discuss how this effectively reduces the variance of local estimates in
Section 4.1.
Thecomputationoftheglobalpenalty(16)andimportanceaveraging(17)ataserverrequireslocalcounters
nm (s,a)fromeveryagent, anddeterminingthelearningrates(18)ateachagentrequiresaccesstorecently
k,h
aggregated global counters N (s,a). Therefore, for FedLCB-Q with the specified parameters choices,
ι(k),h
agents and a server additionally exchange the updated local and global counters at every aggregation step.
3.3 Theoretical guarantees
Given the parameters described above, we now give sample complexity guarantees on the performance of
the proposed FedLCB-Q algorithm.
Theorem 1. Consider δ (0,1) and let π be the solution policy of FedLCB-Q. If a synchronization schedule
(cid:98)
∈
(K) is independent of trajectories in datasets and satisfies
T D
(cid:114)
H2SC⋆ K τ 2
τ avg and u+1 1+ (19)
1
≤ M τ ≤ H
u
for any u 1, where τ is the number of episodes between the (u 1)-th and the u-th aggregations. Denoting
u
≥ −
the total number of samples per agent T =KH, the following holds:
(cid:115) 
H7SC⋆ ζ2 H4SC⋆ ζ
V⋆(ρ) Vπ(cid:98)(ρ) c avg 1 + avg 1  (20)
1 − 1 ≤ MT MT
(cid:16) (cid:17)
at least with probability 1 δ, where ζ =log SAMK2H and c>0 is some universal constant.
− 1 δ
Theorem1impliesthataslongastheinitialsynchronizationoccursearlyandthesynchronizationintervals
do not increase too rapidly (cf. (19)), FedLCB-Q is guaranteed to find an ε-optimal policy, i.e., V⋆(ρ)
Vπ(cid:98)(ρ) ε, for any target accuracy ε (0,H], if the total number of samples per agent T exceeds 1 −
1 ≤ ∈
(cid:32) (cid:33)
H7SC⋆
avg
O(cid:101) .
Mε2
A few implications are in order.
Linear speedup without expertdatasets. Thevaluefunctiongapshowslinearspeedupwithrespectto
the number of agents M, highlighting the benefit of collaboration. Notably, the guarantee holds even when
every agent has low-quality datasets collected by some sub-optimal behavior policy, as long as agents’ local
data distributions collectively cover the distribution of the optimal policy, where the average single-policy
concentrability C⋆ (cf. (9)) is finite. On the other end, when performing offline RL using a single agent, it
avg
requires that the behavior policy of the single agent individually cover the optimal policy, i.e., C⋆ < (cf.
(7)), which is much more stringent. Therefore, federated offline RL enables policy learning that oth∞erwise
will not be possible in the single-agent setting. Specializing to the case M = 1, our bound nearly matches
(cid:16) (cid:17)
the sample complexity bound O(cid:101) H6SC⋆ obtained for a single-agent pessimistic Q-learning algorithm with
ε2
a similar Hoeffding-style penalty (Shi et al., 2022), up to a factor of H.
11Periodic sync. (𝜏)
𝜏 𝜏 𝜏
Exponential sync. (𝛾)
𝐻 (1+𝛾)𝐻 1+𝛾 !𝐻
Local updates Aggregation
Figure 2: Illustration of the periodic synchronization with constant period τ and the exponential synchro-
nization with a rate γ.
Comparison with offline RL using shared datasets. Tobenchmarkthetightnessofourbound,letus
considertheminimaxlowerboundofthesamplecomplexityforsingle-agentofflineRL(Lietal.,2022), asif
we collect all the agents’ datasets at a central location. Note that the effective single-policy concentrability
coefficient (cf. (7)) for the combined datasets = M m becomes
Dall ∪m=1D
min dπ⋆ (s,a), 1/S min dπ⋆ (s,a), 1/S C⋆
max { h } = max { h } = avg , (21)
(h,s,a)∈[H]×S×A (cid:80)M m=1dm h(s,a) (h,s,a)∈[H]×S×A Mda hvg(s,a) M
leading to the minimax lower bound (Li et al., 2022)
(cid:32) (cid:33)
H4SC⋆
avg
Ω(cid:101) .
Mε2
Comparing with the sample complexity bound of FedLCB-Q, obtained as
O(cid:101)(cid:16) H7SC a⋆ vg(cid:17)
, this suggests that
Mε2
the performance of FedLCB-Q is near-optimal up to polynomial factors of H3 even when compared with the
single-agent counterpart assuming shared access to all agents’ datasets.
Communication efficiency. Theorem 1 suggests initiating the first synchronization early and avoiding
rapid increases in synchronization intervals (cf. (19)) to ensure fast convergence. This is attributed to large
deviations among agents in the early stages, arising due to coarse Q-estimates and large learning rates,
whichdiminishastrainingproceeds. Forcommunicationefficiency,itisessentialtodesignasynchronization
schedule that meets the constraints with the least number of synchronizations. We investigate the following
two specific synchronization schedules for FedLCB-Q:
(a) Periodic synchronization: For a fixed period τ 1, communication between agents and a server is
available for every τ episodes, i.e., τ =τ for all i ≥ 1, and we denote the synchronization schedule as
i
(K,τ). ≥
period
T
(b) Exponential synchronization: For a fixed ratio γ > 0, initializing τ = H, set τ = (1+γ)τ
1 i i−1
for each i 2. Under this scheduling, agents communicate frequently at initial iterat⌊ions, but the⌋
period betw≥een aggregation steps increases exponentially with the rate of (1+γ) and synchronization
occurs rarely as training proceeds enough. We denote the synchronization schedule as (K,γ).
exp
T
Now, we analyze the number of communication rounds required to achieve a target accuracy, for each
scheduling scheme.
12H3SC⋆
Corollary 1. For any given δ (0,1) and target error ε (0,min H, avg ], suppose the total number
∈ ∈ { M }
of samples per agent T =KH satisfies
H7SC⋆
avg
T ,
≍ Mε2
and FedLCB-Q performs under the periodic synchronization scheduling, i.e., (K) = (K,τ), with
period
(cid:113) T T
τ HSC a⋆ vgT , or the exponential synchronization scheduling, i.e., (K) = (K,γ), with γ = 2. Then,
≍ M T Texp H
each schedule requires the number of synchronizations at most
(cid:115)
MK
(Periodic) (K,τ) ≲ , (22a)
|Tperiod | H2SC⋆
avg
(Exponential) (K,γ) ≲H, (22b)
exp
|T |
respectively, and the solution policy π of FedLCB-Q is guaranteed to be an ε-optimal policy at least with
(cid:98)
probability 1 δ.
−
Corollary1impliesthatFedLCB-QrequiresonlyO(cid:101)(H)aggregationstoachievethetargetaccuracyunder
appropriatesynchronizationschedules, suchastheexponentialsynchronizationschedule. Notably, thenum-
ber of communication rounds is nearly independent of the size of the state-action space, the total number of
episodes, or the number of agents, and this outperforms prior art (Woo et al., 2023). Furthermore, analysis
suggests that exponential synchronization with a modest rate γ =2/H is a key to achieving such communi-
cationefficiency. Withourstrategicchoicesoflearningrates,localQ-estimatesstabilizeastrainingproceeds,
andthusagentscanperformmorelocalupdatesthanpreviousroundswithoutincreasinguncertaintybeyond
thecontroloftheglobalpessimismpenalty. Exponentialsynchronizationreducesthenumberofsynchroniza-
tions by capturing the additional room for local updates arising from the stabilization of Q-estimates. On
the other hand, periodic synchronization does not exploit this benefit, even if we set the period τ maximally
under (19) due to which it necessitates more communication rounds, which increase with K and M.
4 Analysis
In this section, we will outline useful properties of FedLCB-Q and the key steps of the proof of Theorem 1,
deferring the details, such as proofs of supporting lemmas, to Appendix A and B.
Throughout the paper, we adopt the following shorthand notation
P :=P ( s,a) [0,1]1×S, (23)
h,s,a h
·| ∈
which represents the transition probability vector given the current state-action pair (s,a) at step h. In
addition, define Pm 0,1 1×S as the empirical transition vector at step h of the k-th episode at agent m,
namely k,h ∈{ }
Pm (s)=I(s=sm ), for all s . (24)
k,h k,h+1 ∈S
These are the notations pertaining to the counters for visits of agents on each state-action pair (s,a)
. For any (m,k,h) [M] [K] [H], ∈
S×A ∈ × ×
• lm (s,a): a set of episodes in the interval (ι(k),k] during which agent m visits (s,a) at step h, i.e.,
k,h
lm (s,a):= ι(k)<i k :(sm ,am )=(s,a) .
k,h { ≤ i,h i,h }
• Lm (s,a): a set of episodes in the interval [1,k] during which agent m visits (s,a) at step h, i.e.
k,h
Lm (s,a):= 1 i k :(sm ,am )=(s,a) .
k,h { ≤ ≤ i,h i,h }
We also introduce the following notation related to the synchronization schedule (K). For any positive
integer k and u, T
• t : the index of episodes, after which the uth synchronization occurs.
u
• τ : the number of local updates (episodes) taken between the (u 1)th and the uth synchronizations.
u
−
• ι(k): the most recent episode where the aggregation occurs before the kth episode.
• ϕ(k): the minimum index of aggregation occurring after k-th episode.
134.1 Basic facts
Error recursion of Q-estimates. WebeginwiththefollowingkeyerrordecompositionoftheQ-estimate
at each synchronization, whose proof is provided in Appendix B.1.
Lemma 1 (Q-estimation error decomposition). Consider a Q-function Qπ = Qπ(s,a) and value
{ h }[H]×S×A
function Vπ = Vπ(s) induced by a policy π. Then, for any [H] and k (K), the error
{ h }[H]×S ×S ×A ∈ T
between Qπ and Q is decomposed as follows:
h k,h
Qπ(s,a) Q (s,a)=ω (s,a)(Qπ(s,a) Q (s,a))
h − k,h 0,k,h h − 0,h
(cid:124) (cid:123)(cid:122) (cid:125)
=:Dπ(s,a,k,h): initializationerror
1
M
(cid:88) (cid:88)
+ ωm (s,a)(P Pm)Vm
i,k,h h,s,a − i,h i−1,h+1
m=1i∈Lm (s,a)
k,h
(cid:124) (cid:123)(cid:122) (cid:125)
=:D2(s,a,k,h): transitionvariance
ϕ(k) ϕ(k)
(cid:88) (cid:89)
+ B tu,h(s,a) λ u′,h(s,a)
u=1 u′=u+1
(cid:124) (cid:123)(cid:122) (cid:125)
=:D3(s,a,k,h): globalpenalty
M
(cid:88) (cid:88)
+ ωm (s,a)P (Vπ Vm ), (25)
i,k,h h,s,a h+1− i−1,h+1
m=1i∈Lm (s,a)
k,h
(cid:124) (cid:123)(cid:122) (cid:125)
=:Dπ(s,a,k,h): recursion
4
where Lm (s,a) := 1 i k : (sm ,am ) = (s,a) and lm (s,a) := ι(k) < i k : (sm ,am ) = (s,a) .
k,h { ≤ ≤ i,h i,h } k,h { ≤ i,h i,h }
And, for simplicity, we use the shortened notations defined as
(cid:40)
1 if N (s,a)=0
λ (s,a)= k,h , v =ϕ(k), (26a)
v,h Nι(k),h(s,a) otherwise
Nk,h(s,a)+Hnk,h(s,a)
(cid:40)
1 if N (s,a)=0
ωm (s,a)= k,h , (26b)
0,k,h 0 otherwise
 
ϕ(k)−1
ωm (s,a)=
H +1

(cid:89) N tx,h(s,a)
, i Lm (s,a). (26c)
i,k,h N (s,a)+Hn (s,a) N (s,a)+Hn (s,a) ∈ k,h
k,h k,h
x=ϕ(i)
tx,h tx,h
Equally favoring episodes within the same local update round. According to the decomposition
(25)inLemma1,forany(s,a,h) [H],theQ-estimationerroratepisodek significantlydependson
theweightedsumoftransitiondiff∈ereSn×ceAfo×reachepisodewherethelocalupdateoccurs,namelyD (s,a,k,h).
2
Intuitively, the weight ωm (s,a) assigned to each episode i balances the accumulation of information from
i,k,h
old and new updates. Our choice of learning rates, which decreases fast during local updates, as illustrated
in Figure 3a, ensures that the weight ωm (s,a) within the same local update round is always equal for all
i,k,h
episodes and agents, as shown in (26c) and Figure 3b. The uniform weights allow the transition information
of each episode to be accumulated evenly, regardless of other transitions that occur in future episodes
or other agents’ episodes. This is essential to keep variance arising from local updates low, especially
when a synchronization interval is long. Assigning equal weight to every episode allows to fully utilize
transitions observed during local updates without forgetting old information, regardless of the length of the
synchronization interval.
Bounded visitation counters. Wenextintroducethefollowinglemmaregardingthevisitationcounters,
whose proof is provided in Appendix B.2.
141.0 0.10
η0 (s,a)(d0(s,a)=0.7) ω0 (s,a)(d0(s,a)=0.7)
i,h h i,60,h h
η1 (s,a)(d1(s,a)=0.3) ω1 (s,a)(d1(s,a)=0.3)
0.8 i,h h 0.08 i,60,h h
0.6 0.06
0.4 0.04
0.2 0.02
0.0 0.00
0 10 20 30 40 50 60 0 10 20 30 40 50 60
Episode(i) Episode(i)
(a) Rescaled learning rates (b) Episode weights
Figure3: Illustrationoftherescaledlearningrates(ηm(s,a))andtheepisodeweights(ωm (s,a))induced
i,h i,60,h
by the learning rates of two agents m = 0,1 for episodes 1 i 60, where H = 5, the occupancy
distribution of each agent on (s,a,h) [5] is d0(s,a)=0≤ .7 an≤d d1(s,a)=0.3, respectively, and the
synchronization schedule is (60)= ∈ 10S ,3× 0,A 60×. h h
T { }
Lemma 2 (Concentration bound on the visitation counters). Consider any δ (0,1) and some universal
∈
constant c >0, and let
1
(cid:18) (cid:19)
2 KH 4ζ
ζ :=log |S||A| and K (s,a,h):= 0 . (27)
0 δ 0 c Mdavg(s,a)
1 h
Then, for all (s,a,h) [H], the following holds
∈S×A×
1
when k K (s,a,h): kMdavg(s,a) N (s,a) 2kMdavg(s,a), (28a)
≥ 0 2 h ≤ k,h ≤ h
when k K (s,a,h): N (s,a) 8ζ /c (28b)
0 k,h 0 1
≤ ≤
with probability at least 1 δ.
−
Monotonic and pessimistic global value updates. Note that the global value estimate is always
monotonically non-decreasing, i.e., for k′,k (K) it holds
∈T
s : V k,h(s) V k′,h(s) when k′ k, (29)
∀ ∈S ≥ ≤
whichfollowsdirectlyfromtheupdaterule(14). Moreover,wehavethefollowingimportantlemmaregarding
the pessimistic property of the value estimate, whose proof is provided in Appendix B.3.
Lemma 3 (Pessimistic global value). Recall Q , V , and π in Algorithm 1. Let π = π .
k,h k,h k,h k k,h h∈[H]
{ }
Given any δ (0,1), for all (k,h) (K) [H], it holds with probability at least 1 δ that
∈ ∈T × −
(cid:115)
4c ζ2H4
(s,a) : D (s,a,k,h) D (s,a,k,h) B 1 , (30a)
2 3
∀ ∈S×A | |≤ ≤ max N (s,a),1
k,h
{ }
(s,a) : Q (s,a) Qπk(s,a) Q⋆(s,a), (30b)
∀ ∈S×A k,h ≤ h ≤ h
s : V (s) Vπk(s) V⋆(s). (30c)
∀ ∈S k,h ≤ h ≤ h
Inwords,Lemma3makesconcretetheroleofthepenaltytermindominatingthevariabilityofthevalue
estimates due to stochastic transitions, and ensures that the estimated value is a pessimistic estimate of the
true optimal value function.
15
))a,s(mη(setargninraeL
h,i
))a,s(
mω(sthgiewedosipE
h,06,i4.2 Proof of Theorem 1
Now we are ready to provide the proof of Theorem 1, which is divided into several key steps as follows.
Step 1: decomposition of the performance gap. The performance gap between the solution policy π
(cid:98)
of Algorithm 1 after K episodes and the optimal policy π⋆ can be bounded as follows:
V 1⋆(ρ) −V 1π(cid:98)(ρ)=E s1∼ρ[V 1⋆(s 1)] −E s1∼ρ[V 1πK(s 1)]
(i)
E [V⋆(s )] E [V (s )]
≤ s1∼ρ 1 1 − s1∼ρ K,1 1
ϕ(K)
(ii) 1 (cid:88) τ (cid:0)E [V⋆(s )] E [V (s )](cid:1)
≤ K v s1∼ρ 1 1 − s1∼ρ tv,1 1
v=1
ϕ(K)
= 1 (cid:88) τ (cid:88) dπ⋆ (s)(V⋆(s) V (s))
K v 1 1 − tv,1
(cid:124) (cid:123)(cid:122) (cid:125)
v=1 s∈S
=ρ(s)
ϕ(K)
1 max (cid:88) τ (cid:88) dπ⋆ (s)(V⋆(s) V (s)), (31)
≤ K h∈[H] v h h − tv,h
v=1 s∈S
where(i)followsfromLemma3, and(ii)followsfromthemonotonicitypropertyin(29)and(cid:80)ϕ(K)τ =K.
v=1 v
Since π⋆ = π⋆ is deterministic, for any k (K) and h [H], it follows that
{
h}h∈[H]
∈T ∈
(cid:88) dπ⋆ (s)(V⋆(s) V (s))=(cid:88) dπ⋆ (s,π⋆(s))(V⋆(s) V (s))
h h − k,h h h h − k,h
s∈S s∈S
(cid:88) dπ⋆ (s,π⋆(s))(cid:0) Q⋆(s,π⋆(s)) Q (s,π⋆(s))(cid:1) , (32)
≤ h h h h − k,h h
s∈S
where the inequality holds because Q (s,π⋆(s)) max Q (s,a) V (s) due to (14).
To continue, applying Lemma 1 byk,h settinh g π =≤ π⋆, tha e∈A Q-ek s, th imate e≤rrok r,h after k episodes is decomposed
as follows:
Q⋆(s,a) Q (s,a)=Dπ⋆ (s,a,k,h)+D (s,a,k,h)+D (s,a,k,h)+Dπ⋆ (s,a,k,h)
h − k,h 1 2 3 4
Dπ⋆ (s,a,k,h)+Dπ⋆ (s,a,k,h)+2D (s,a,k,h), (33)
≤ 1 4 3
where the second line follows from Lemma 3. Finally, inserting the decomposition (33) and (32) back into
(31), we control the performance gap with the following terms:
V⋆(ρ) Vπ(cid:98)(ρ)
1 − 1
ϕ(K)
1 max (cid:88) τ (cid:88) dπ⋆ (s)(cid:104) Dπ⋆ (s,π⋆(s),t ,h)+Dπ⋆ (s,π⋆(s),t ,h)+2D (s,π⋆(s),t ,h)(cid:105)
≤ K h∈[H] v h 1 h v 4 h v 3 h v
v=1 s∈S
1
=: max (D +D +2D ), (34)
1,h 4,h 3,h
K h∈[H]
for which we shall aim to bound each term individually, adopting the following short-hand notation:
ϕ(K)
D := (cid:88) τ (cid:88) dπ⋆ (s)Dπ⋆ (s,π⋆(s),t ,h) for i 1,4 ,
i,h v h i h v ∈{ }
v=1 s∈S
ϕ(K)
D := (cid:88) τ (cid:88) dπ⋆ (s)D (s,π⋆(s),t ,h). (35)
3,h v h 3 h v
v=1 s∈S
16Step 2: Bounding the decomposed terms. Here, we derive the bound of the decomposed terms
separately as follows under the event that (28) holds, which is denoted as and holds with probability at
0
least 1 δ. E
−
• Bounding D . Using the fact that 0 Q⋆(s,π⋆(s)) Q (s,π⋆(s)) H, which follows from
Lemma 3, it fo1 l, lh ows ≤ h h − 0,h h ≤
ϕ(K)
D = (cid:88) τ (cid:88) dπ⋆ (s,π⋆(s))ω (s,π⋆(s))(Q⋆(s,π⋆(s)) Q (s,π⋆(s)))
1,h v h h 0,tv,h h h h − 0,h h
v=1 s∈S
ϕ(K)
(cid:88) τ (cid:88) dπ⋆ (s,π⋆(s))ω (s,π⋆(s))H
≤ v h h 0,tv,h h
v=1 s∈S
ϕ(K)
=H(cid:88) dπ⋆ (s,π⋆(s)) (cid:88) τ I N (s,π⋆(s))=0 , (36)
h h v { tv,h h }
s∈S v=1
where the last line follows from (26b). To continue, note that
ϕ(K)
(cid:88) (cid:88)
τ I N (s,π⋆(s))=0 = τ I N (s,π⋆(s))=0
v { tv,h h } v { tv,h h }
v=1 v∈[ϕ(K)]:tv≤K0(s,π h⋆(s),h)
K (s,π⋆(s),h),
≤ 0 h
since under the event , N (s,π⋆(s))>0 when t >K (s,π⋆(s),h). Plugging the above inequality
and the definition of
KE0 (s,πtv ⋆, (h s),h)h
back to (36)
leav
ds
to0 h
0 h
D H(cid:88) dπ⋆ (s,π⋆(s))K (s,π⋆(s),h)
1,h ≤ h h 0 h
s∈S
=H(cid:88)min {dπ h⋆ (s,π h⋆(s)),1/S }(cid:18) 12ζ 0(cid:19) dπ h⋆ (s,π h⋆(s))
davg(s,π⋆(s)) M min dπ⋆(s,π⋆(s)),1/S
s∈S h h { h h }
HC⋆ S
≲ avg , (37)
M
where the last line follows from the definition of C⋆ and the fact that
avg
(cid:88) dπ h⋆ (s,π h⋆(s)) (cid:88)(cid:16) 1+dπ⋆ (s,π⋆(s))S(cid:17) =(cid:88)(cid:16) 1+dπ⋆ (s)S(cid:17)
=2S.
min dπ⋆(s,π⋆(s)),1/S ≤ h h h
s∈S { h h } s∈S s∈S
• Bounding D . The range of D (s,a,k,h) is bounded as shown in the following lemma, whose
3,h 3
proof is provided in Appendix B.4.
Lemma 4. For any (s,a,h) [H] and k (K), if N (s,a)=0, D (s,a,k,h)=0, and if,
k,h 3
∈S×A× ∈T
N (s,a)>0, the following holds:
k,h
(cid:34)(cid:115) (cid:115) (cid:35)
c ζ2H4 4c ζ2H4
D (s,a,k,h) B 1 , B 1 . (38)
3
∈ N (s,a) N (s,a)
k,h k,h
With the above lemma in hand, recalling (35) gives
ϕ(K)
D = (cid:88) τ (cid:88) dπ⋆ (s,π⋆(s))D (s,π⋆(s),t ,h)
3,h v h h 3 h v
v=1 s∈S
(cid:115)
ϕ(K)
(cid:88) dπ⋆ (s,π⋆(s)) (cid:88) τ 4c Bζ 12H4 . (39)
≤ h h v max N (s,π⋆(s)),1
s∈S v=1 { tv,h h }
17According to Lemma 2, N (s,a) 1t Mdavg(s,a) holds if t K (s,a,h) under the event .
Therefore, tv,h ≥ 2 v h v ≥ 0 E0
(cid:115) (cid:115)
ϕ(K)
(cid:88) H4 (cid:88) (cid:88) H4
τ τ H2+ τ
v v v
max N (s,a),1 ≤ max N (s,a),1
v=1 {
tv,h
} v:tv≤K0(s,a,h) v:tv>K0(s,a,h) {
tv,h
}
(cid:115)
(cid:88) H4
≲H2K (s,a,h)+ τ
0 v
max N (s,a),1
v:tv>K0(s,a,h) {
tv,h
}
(cid:115)
ϕ(K)
(cid:88) H4
≲H2K (s,a,h)+ τ . (40)
0 v Mt davg(s,a)
v=1 v h
Plugging the above inequality and the definitions of K (s,π⋆(s),h) (cf. (27)) and C⋆ to (39), we
0 h avg
obtain
(cid:115)
D ≲ H2 (cid:88) dπ h⋆ (s,π h⋆(s)) +ϕ (cid:88)(K) (cid:88) dπ⋆ (s,π⋆(s))τ H4
3,h M davg(s,π⋆(s)) h h v Mt davg(s,π⋆(s))
s∈S h h v=1 s∈S v h h
(cid:115) (cid:115)
≲ H2C a⋆ vg (cid:88) dπ h⋆ (s,π h⋆(s)) +ϕ (cid:88)(K) H4C a⋆ vgτ v2 (cid:88) (dπ h⋆(s,π h⋆(s)))2
M min dπ⋆(s,π⋆(s)),1/S Mt min dπ⋆(s,π⋆(s)),1/S
s∈S { h h } v=1 v s∈S { h h }
( ≲i) H2C a⋆ vgS +(cid:114) H4C a⋆ vgS ϕ (cid:88)(K)
√τ
(cid:114) τ
v
v
M M t
v
v=1
(cid:114)
(ii) H2C⋆ S H4SKC⋆
≲ avg + avg , (41)
M M
where (i) holds due to the Cauchy-Schwarz inequality and the fact that
(cid:88) dπ h⋆ (s,π h⋆(s)) (cid:88)(cid:16) 1+dπ⋆ (s,π⋆(s))S(cid:17) =(cid:88)(cid:16) 1+dπ⋆ (s)S(cid:17)
=2S,
min dπ⋆(s,π⋆(s)),1/S ≤ h h h
s∈S { h h } s∈S s∈S
and the last line (ii) follows from the Cauchy-Schwarz inequality and the fact that (cid:80)ϕ(K)τ =K and
v=1 v
(cid:80)ϕ(K) τv 1+logK, with the latter following from Lemma 6 (see Appendix A).
v=1 tv ≤
• Bounding D . In the following lemma, whose proof is provided in Appendix B.5, we extract the
4,h
recursive formulation of D as follows.
4,h
Lemma 5. Consider any δ (0,1). For any h [H], the following holds with probability at least 1 δ:
∈ ∈ −
ϕ(K) M
(cid:88) τ (cid:88) dπ⋆ (s,a) (cid:88) (cid:88) ωm (s,a)P (V⋆ V )
v h i,tv,h h,s,a h+1− ι(i),h+1
v=1 (s,a)∈S×A m=1i∈Lm (s,a)
tv,h
(cid:18) (cid:19)ϕ(K)
≲σ + 1+ 1 (cid:88) τ (cid:88) dπ⋆ (s)(V⋆ (s) V (s)), (42)
aux H u h+1 h+1 − tu−1,h+1
u=1 s∈S
(cid:113)
H2KSC⋆ H2SC⋆
where σ = avg + avg.
aux M M
Step 3: Recursion. Combining the bounds of the decomposed errors (cf. (37), (41), and (42)), for any
h [H], we obtain the following recursive relation:
∈
ϕ(K)
(cid:88) τ (cid:88) dπ⋆ (s)(V⋆(s) V (s))
v 1 h − tv,h
v=1 s∈S
18ϕ(K)
≲θ +(cid:16) 1+ 1 (cid:17) (cid:88) τ (cid:88) dπ⋆ (s)(cid:0) V⋆ (s) V (s)(cid:1)
K H u 1 h+1 − tu−1,h+1
u=1 s∈S
ϕ(K)−1
( ≲i) (θ +Hτ )+(cid:16) 1+ 1 (cid:17) (cid:88) τ (cid:88) dπ⋆ (s)(cid:0) V⋆ (s) V (s)(cid:1)
K 1 H u+1 1 h+1 − tu,h+1
u=1 s∈S
ϕ(K)−1
( ≲ii) (θ +Hτ )+(cid:16) 1+ 2 (cid:17)2 (cid:88) τ (cid:88) dπ⋆ (s)(cid:0) V⋆ (s) V (s)(cid:1) , (43)
K 1 H u 1 h+1 − tu,h+1
u=1 s∈S
where (i) holds because V⋆ (s) V (s) H and (ii) holds due to the condition τu+1 1+ 2 for all
1 u ϕ(K) and the fach t+ t1 hat V− ⋆ tu (, sh )+1 V ≤ (s) shown in Lemma 3, and we denoteτu ≤ H
≤ ≤ h+1 ≥ tu,h+1
(cid:114) (cid:114)
HC⋆ S H2C⋆ S H4SC⋆ k H2kSC⋆ H2SC⋆
θ := avg + avg + avg + avg + avg (44)
k
M M M M M
for any k [K]. Then, by invoking the recursion (H h+1) times, it follows that
∈ −
ϕ(K)
(cid:88) τ (cid:88) dπ⋆ (s)(V⋆(s) V (s))
v 1 h − tv,h
v=1 s∈S
ϕ(K)−2
≲(θ +Hτ )+(cid:16) 1+ 2 (cid:17)2 (θ +Hτ )+(cid:16) 1+ 2 (cid:17)4 (cid:88) τ (cid:88) dπ⋆ (s)(cid:0) V⋆ (s) V (s)(cid:1)
K 1 H tϕ(K)−1 1 H u 1 h+2 − tu,h+2
u=1 s∈S
(cid:16) 2 (cid:17)2 (cid:16) 2 (cid:17)2(H−h+1)
≲(θ +Hτ )+ 1+ (θ +Hτ )+ + 1+ (θ +Hτ )
K 1
H
tϕ(K)−1 1
··· H
tϕ(K)−H+h−1 1
≲Hθ +H2τ (45)
K 1
where the second line follows from the fact that V⋆ (s) V (s)=0 for any k [K], and the last line
H+1 − k,H+1 ∈
holds because θ θ for any k K and (1+ 2)2(H−h+1) (1+ 2)2H e4.
Finally, by pk lu≤ggiK ng the above≤bound into (3H 1), we obtain≤the boH und o≤f the performance gap as follows:
ϕ(K)
V⋆(ρ) Vπ(cid:98)(ρ) 1 max (cid:88) τ (cid:88) dπ⋆ (s)(V⋆(s) V (s))
1 − 1 ≤ K h∈[H] v h h − tv,h
v=1 s∈S
1
≲ (Hθ +H2τ )
K 1
K
(cid:114) (cid:114)
H3SC⋆ H6SC⋆ H2τ T=HK H7SC⋆ H4SC⋆
≲ avg + avg + 1 ≲ avg + avg , (46)
MK MK K MT MT
(cid:113)
where the last line holds if τ HSC a⋆ vgT, and this completes the proof.
1 ≤ M
5 Discussions
We investigated federated offline RL, which enables multiple agents with history datasets to collaboratively
learn an optimal policy, without sharing datasets. We proposed a federated offline Q-learning algorithm
called FedLCB-Q, which iteratively performs local updates with rescaled learning rates at agents, and global
aggregationwithweightedaveragingandglobalpenaltyataserver,whicheffectivelycontrolstheuncertainty
inbothlocalandglobalQ-estimates. OursamplecomplexityanalysisdemonstratesthatFedLCB-Qachieves
linearspeedupintermsofthenumberofagentsrequiringonlycollectivecoverageofagents’datasetsoverthe
distributionoftheoptimalpolicy,notrestrictedtothequalityofindividualdatasets. Furthermore,weshowed
that FedLCB-Q is communication-efficient, requiring only O(cid:101)(H) synchronizations under the exponential
synchronization scheduling. For future exploration, this work paves the way for many interesting directions,
some of which are outlined below.
19• Tightening H dependency. Although our sample complexity bound is nearly optimal with respect to
most salient problem parameters, such as state space size and single-policy concentrability coefficient,
it falls short of optimality in terms of horizon length compared to the minimax sample complexity
lower bound in the single-agent setting (Xie et al., 2021b). Closing this gap and improving sample
complexity with variance reduction techniques, as proposed by Shi et al. (2022), will be an interesting
avenue for future exploration.
• Beyond episodic tabular MDPs. Extending episodic tabular MDPs, it would be interesting to broaden
our analysis framework to encompass other RL settings, including, the infinite-horizon setting and the
integration of function approximation.
• Improving robustness. Our work focuses on a scenario in which agents collect datasets from a com-
mon MDP without any disturbances. Yet, in real-world scenarios, some agents may possess datasets
collected from perturbed MDPs. This introduces the need for additional considerations regarding ro-
bustness, as discussed in Shi et al. (2023). Therefore, enhancing our work to effectively handle the
variability or noisiness of MDPs would be a compelling avenue for improvement.
• Multi-task RL. In many applications with multiple clients, multi-task learning, where clients have
heterogeneous goals, holds significant interest due to diversity in clients. It will be of great interest to
extend our work to the multi-task RL setting (Jin et al., 2022; Yang et al., 2023; Zhou et al., 2023),
which enables agents to learn their own optimal policies for their personalized goals while benefiting
from collaboration by sharing common features of tasks.
Acknowledgement
ThisworkissupportedinpartbythegrantsNSFCCF-2007911,CCF-2106778,CNS-2148212,ONRN00014-
19-1-2404 to Y. Chi, and NSF-CCF 2007834, CCF-2045694, CNS-2112471, ONR N00014-23-1-2149 to G.
Joshi.
References
Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X. (2019). Provably efficient Q-learning with low switching cost.
In Advances in Neural Information Processing Systems, volume 32.
Beck,C.L.andSrikant,R.(2012). Errorboundsforconstantstep-sizeQ-learning. Systems&controlletters,
61(12):1203–1208.
Bertsekas, D. P. (2017). Dynamic programming and optimal control (4th edition). Athena Scientific.
Chen, Z., Zhang, S., Doan, T. T., Maguluri, S. T., and Clarke, J.-P. (2019). Performance of Q-learning with
linear function approximation: Stability and finite-time analysis. arXiv preprint arXiv:1905.11425.
Even-Dar,E.andMansour,Y.(2003). LearningratesforQ-learning. Journal of machine learning Research,
5(Dec):1–25.
Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2020). A theoretical analysis of deep Q-learning. In Learning for
Dynamics and Control, pages 486–489. PMLR.
Fan,X.,Ma,Y.,Dai,Z.,Jing,W.,Tan,C.,andLow,B.K.H.(2021). Fault-tolerantfederatedreinforcement
learning with theoretical guarantee. In Advances in Neural Information Processing Systems, volume 34,
pages 1007–1021.
Freedman, D. A. (1975). On tail probabilities for martingales. The Annals of Probability, 3(1):100–118.
Fujimoto, S. and Gu, S. S. (2021). A minimalist approach to offline reinforcement learning. Advances in
neural information processing systems, 34:20132–20145.
20Fujimoto, S., Meger, D., andPrecup, D.(2019). Off-policydeepreinforcementlearningwithoutexploration.
In International Conference on Machine Learning, pages 2052–2062. PMLR.
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is Q-learning provably efficient? In Advances
in Neural Information Processing Systems, pages 4863–4873.
Jin, H., Peng, Y., Yang, W., Wang, S., and Zhang, Z. (2022). Federated reinforcement learning with
environment heterogeneity. In International Conference on Artificial Intelligence and Statistics, pages
18–37.
Jin, Y., Yang, Z., and Wang, Z. (2021). Is pessimism provably efficient for offline RL? In International
Conference on Machine Learning, pages 5084–5096.
Khodadadian, S., Sharma, P., Joshi, G., and Maguluri, S. T. (2022). Federated reinforcement learning:
Linear speedup under Markovian sampling. In International Conference on Machine Learning, pages
10997–11057.
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. (2020). Morel: Model-based offline reinforce-
ment learning. arXiv preprint arXiv:2005.05951.
Kim,B.andOh,M.-H.(2023). Model-basedofflinereinforcementlearningwithcount-basedconservatism. In
International Conference on Machine Learning, volume202ofProceedings of Machine Learning Research,
pages 16728–16746. PMLR.
Kostrikov, I., Nair, A., and Levine, S. (2022). Offline reinforcement learning with implicit q-learning. In
International Conference on Learning Representations.
Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020). Conservative Q-learning for offline reinforcement
learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H., editors, Advances in
Neural Information Processing Systems, volume 33, pages 1179–1191. Curran Associates, Inc.
Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020). Offline reinforcement learning: Tutorial, review, and
perspectives on open problems. arXiv preprint arXiv:2005.01643.
Li, G., Cai, C., Chen, Y., Wei, Y., and Chi, Y. (2024). Is Q-learning minimax optimal? a tight sample
complexity analysis. Operations Research, 72(1):222–236.
Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2022). Settling the sample complexity of model-based offline
reinforcement learning. arXiv preprint arXiv:2204.05275.
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2021). Sample complexity of asynchronous Q-learning:
Sharper analysis and variance reduction. IEEE Transactions on Information Theory, 68(1):448–473.
Liu,Y.,Swaminathan,A.,Agarwal,A.,andBrunskill,E.(2020).Provablygoodbatchreinforcementlearning
without great exploration. In Advances in Neural Information Processing Systems, volume 34.
Mitzenmacher, M. and Upfal, E. (2005). Probability and computing. Cambridge University Press.
Puterman, M. L. (2014). Markov decision processes: discrete stochastic dynamic programming. John Wiley
& Sons.
Qu, G. and Wierman, A. (2020). Finite-time analysis of asynchronous stochastic approximation and Q-
learning. In Conference on Learning Theory, pages 3185–3205. PMLR.
Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021). Bridging offline reinforcement learning
and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems.
Shi, C., Xiong, W., Shen, C., and Yang, J. (2023). Provably efficient offline reinforcement learning with
perturbed data sources. In International Conference on Machine Learning, volume 202 of Proceedings of
Machine Learning Research, pages 31353–31388. PMLR.
21Shi, L., Li, G., Wei, Y., Chen, Y., and Chi, Y. (2022). Pessimistic Q-learning for offline reinforcement
learning: Towards optimal sample complexity. In International Conference on Machine Learning, volume
162, pages 19967–20025. PMLR.
Siegel, N., Springenberg, J. T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., Hafner, R.,
Heess, N., and Riedmiller, M. (2020). Keep doing what worked: Behavior modelling priors for offline
reinforcement learning. In International Conference on Learning Representations.
Uehara, M., Kallus, N., Lee, J. D., and Sun, W. (2023). Offline minimax soft-q-learning under realizability
and partial coverage. In Advances in Neural Information Processing Systems, volume 37.
Wainwright, M. J. (2019). Stochastic approximation with cone-contractive operators: Sharp ℓ -bounds for
∞
Q-learning. arXiv preprint arXiv:1905.06265.
Wang, H., Mitra, A., Hassani, H., Pappas, G. J., and Anderson, J. (2023). Federated temporal differ-
ence learning with linear function approximation under environmental heterogeneity. arXiv preprint
arXiv:2302.02212.
Wang, Y., Dong, K., Chen, X., and Wang, L. (2019). Q-learning with UCB exploration is sample efficient
for infinite-horizon MDP. In International Conference on Learning Representations.
Watkins, C. J. and Dayan, P. (1992). Q-learning. Machine learning, 8(3-4):279–292.
Woo,J.,Joshi,G.,andChi,Y.(2023). Theblessingofheterogeneityinfederatedq-learning: Linearspeedup
and beyond. In International Conference on Machine Learning, volume 202 of Proceedings of Machine
Learning Research, pages 37157–37216. PMLR.
Wu, Y., Tucker, G., and Nachum, O. (2019). Behavior regularized offline reinforcement learning. arXiv
preprint arXiv:1911.11361.
Wu, Z., Shen, H., Chen, T., and Ling, Q. (2021). Byzantine-resilient decentralized policy evaluation with
linear function approximation. IEEE Transactions on Signal Processing, 69:3839–3853.
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2021a). Bellman-consistent pessimism for
offline reinforcement learning. In Advances in Neural Information Processing Systems, volume 35.
Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. (2021b). Policy finetuning: Bridging sample-efficient
offline and online reinforcement learning. Advances in Neural Information Processing Systems, 35.
Xu, P. and Gu, Q. (2020). A finite-time analysis of Q-learning with neural network function approximation.
In International Conference on Machine Learning, pages 10555–10565. PMLR.
Yan, Y., Li, G., Chen, Y., and Fan, J. (2023). The efficacy of pessimism in asynchronous Q-learning. IEEE
Transactions on Information Theory, 69(11):7185–7219.
Yang, T., Cen, S., Wei, Y., Chen, Y., and Chi, Y. (2023). Federated natural policy gradient methods for
multi-task reinforcement learning. arXiv preprint arXiv:2311.00201.
Yin, M. and Wang, Y.-X. (2021). Towards instance-optimal offline reinforcement learning with pessimism.
In Advances in Neural Information Processing Systems, volume 35.
Yu,T.,Thomas,G.,Yu,L.,Ermon,S.,Zou,J.,Levine,S.,Finn,C.,andMa,T.(2020).MOPO:Model-based
offline policy optimization. arXiv preprint arXiv:2005.13239.
Zanette,A.,Wainwright,M.J.,andBrunskill,E.(2021). Provablebenefitsofactor-criticmethodsforoffline
reinforcement learning. In Advances in Neural Information Processing Systems, volume 35.
Zhang, C., Wang, H., Mitra, A., and Anderson, J. (2024). Finite-time analysis of on-policy heterogeneous
federated reinforcement learning. arXiv preprint arXiv:2401.15273.
22Zheng, Z., Gao, F., Xue, L., and Yang, J. (2023). Federated Q-learning: Linear regret speedup with low
communication cost. arXiv preprint arXiv:2312.15023.
Zhou, D., Zhang, Y., Sonabend-W,A., Wang, Z., Lu, J., andCai, T.(2023). Federatedofflinereinforcement
learning. arXiv preprint arXiv:2206.05581.
A Technical lemmas
Freedman’s inequality. We provide a user-friendly version of Freedman’s inequality (Freedman, 1975).
See Li et al. (2024, Theorem 6) for more details.
Theorem 2 (Li et al. (2024, Theorem 6)). Consider a filtration , and let E stand
0 1 2 k
for the expectation conditioned on . Suppose that Y
=(cid:80)n
X
F R⊂
,
F whe⊂ reF X⊂ · is··
a real-valued scalar
Fk n k=1 k ∈ { k }
sequence obeying
X R and E (cid:2) X (cid:3) =0 for all k 1
k k−1 k
| |≤ ≥
for some quantity R< . We also define
∞
n
W :=(cid:88) E (cid:2) X2(cid:3) .
n k−1 k
k=1
In addition, suppose that W σ2 holds deterministically for some given quantity σ2 < . Then for any
n
≤ ∞
positive integer m 1, with probability at least 1 δ one has
≥ −
(cid:114)
(cid:110) σ2(cid:111) 2m 4 2m
Y 8max W , log + Rlog . (47)
| n |≤ n 2m δ 3 δ
We next present a basic analytical result that is useful in the proof.
Lemma 6. Consider any sequence {x
z }z=1,···,Z
where x
z
≥
1 for all z and let X
z
=
(cid:80)z
z′=1x z′. Then, for
any Z 1, it follows that
≥
Z
(cid:88) x z
X(Z)= 1+logX .
Z
X ≤
z
z=1
Proof. For Z = 1, X(1) = x1 = 1. For Z > 1, suppose the claim holds for Z 1. Then, it holds for Z as
follows: x1 −
x X
Z Z−1
X(Z)=X(Z 1)+ 1+logX +1
Z−1
− X ≤ − X
Z Z
(cid:18) (cid:19)
X
1+logX log Z−1 =1+logX , (48)
Z−1 Z
≤ − X
Z
wherethefirstinequalityfollowsfromtheinductionhypothesisandx =X X , thesecondinequality
Z Z Z−1
follows from logy y 1 for any y >0. By induction, this completes the pro−of.
≤ −
Last but not least, we have the following useful properties regarding the parameters introduced in (26c).
Lemma 7. For any (s,a,h) [H], k′ k (K), where we denote u=ϕ(k), and i Lm (s,a).
∈S×A× ≤ ∈T ∈ k,h
Then, it follows that:
2H
ωm (s,a) , (49a)
i,k,h ≤ N (s,a)+Hn (s,a)
k,h k,h
M
(cid:88) (cid:88)
ωm (s,a) 1, (49b)
j,k,h ≤
m=1j∈Lm (s,a)
k,h
23M
(cid:88) (cid:88) ωm (s,a) (H +1)n k′,h , (49c)
j,k,h ≤ N +Hn
m=1j∈l km ′,h(s,a) k,h k,h
M
(cid:88) (cid:88) 2H
(ωm (s,a))2 , (49d)
i,k,h ≤ N (s,a)+Hn (s,a)
m=1j∈Lm (s,a) k,h k,h
k,h
∞ M (cid:18) (cid:19)
(cid:88) (cid:88) (cid:88) 1
n (s,a) ωm (s,a) n (s,a) 1+ . (49e)
tv,h i,tv,h ≤ k,h H
v≥u m=1i∈lm (s,a)
k,h
Proof. For notation simplicity, we will omit (s,a) for the following proofs. Moreover, u=ϕ(k) and t =k.
u
Proof of (49a). Recalling the definition of ωm in (26c) and using the fact that H 1,
i,k,h ≥
 
ϕ(k)−1
ωm = H +1  (cid:89) N tx,h  2H . (50)
i,k,h N +Hn N +Hn ≤ N +Hn
k,h k,h
x=ϕ(i)
tx,h tx,h k,h k,h
Proof of (49b). By rearranging the terms,
 
M ϕ(k) M ϕ(k)
(cid:88) (cid:88) ωm = (cid:88) (cid:88) (cid:88) H +1  (cid:89) N tx−1,h 
j,k,h N +Hn N +Hn
m=1j∈Lm (s,a) v=1m=1j∈lm tv,h tv,h x=v+1 tx,h tx,h
k,h tv,h
 
ϕ(k) ϕ(k)
=
(cid:88) (H +1)n tv,h

(cid:89) N tx−1,h

N +Hn N +Hn
v=1
tv,h tv,h
x=v+1
tx,h tx,h
 
ϕ(k)(cid:18) (cid:19) ϕ(k)
=
(cid:88)
1
N tv−1,h

(cid:89) N tx−1,h

− N +Hn N +Hn
v=1
tv,h tv,h
x=v+1
tx,h tx,h
 
ϕ(k) ϕ(k) ϕ(k)
=
(cid:88)

(cid:89) N tx−1,h (cid:89) N tx−1,h

N +Hn − N +Hn
v=1 x=v+1
tx,h tx,h
x=v
tx,h tx,h
ϕ(k)
=1 (cid:89) N tx−1,h 1. (51)
− N +Hn ≤
x=1
tx,h tx,h
Proof of (49c). Let v =ϕ(k′), i.e., k′ =t . Similarly to the proof of (49b), by arranging some terms, we
v
obtain the upper bound as follows:
 
M M ϕ(k)
(cid:88) (cid:88) ωm (s,a)= (cid:88) (cid:88) H +1  (cid:89) N tx−1,h 
j,k,h N +Hn N +Hn
m=1j∈l km ′,h(s,a) m=1j∈l tm v,h(s,a) tv,h tv,h x=v+1 tx,h tx,h
 
ϕ(k)
=
(H +1)n tv,h

(cid:89) N tx−1,h

N +Hn N +Hn
tv,h tv,h
x=v+1
tx,h tx,h
 
ϕ(k)−1
=
(H +1)n tv,h

(cid:89) N tx,h

N +Hn N +Hn
k,h k,h
x=v
tx,h tx,h
(H +1)n k′,h . (52)
≤ N +Hn
k,h k,h
24Proof of (49d). Using the bound in (49a) and (49b),
(cid:32) (cid:33)
M M
(cid:88) (cid:88) (cid:88) (cid:88) 2H
(ωm )2 = max ωm ωm max ωm . (53)
m=1j∈Lm j,k,h m∈[M],j∈Lm k,h j,k,h m=1j∈Lm j,k,h ≤m∈[M],j∈Lm k,h j,k,h ≤ N k,h+Hn k,h
k,h k,h
Proof of (49e). Recall that k =t . Then, reusing the intermediate result derived in (52),
u
∞ M ∞ (cid:32) v−1 (cid:33)
(cid:88) n (s,a) (cid:88) (cid:88) ωm (s,a)= (cid:88) n (H +1)n tu,h (cid:89) N tx,h
tv,h i,tv,h tv,h N +Hn N +Hn
v≥u m=1i∈l tm u,h(s,a) v≥u tv,h tv,h x=u (cid:124) tx,h (cid:123)(cid:122) tx,h (cid:125)
:=βx,h
∞ (cid:32) v−1 (cid:33)
=(H +1)n
(cid:88) n tv,h (cid:89)
β
tu,h
N +Hn
x,h
v≥u
tv,h tv,h
x=u
∞ (cid:32) v−1 (cid:33)
(cid:88) 1 (cid:89)
=(H +1)n (1 β ) β
tu,h
H −
v,h x,h
v≥u x=u
(cid:18) (cid:19)
1
n 1+ . (54)
k,h
≤ H
B Proofs for main results
B.1 Proof of Lemma 1
For any (h,s,a) [H] and k (K), according to the pessimistic aggregation update rule in (13),
the estimate erro∈r of Q×fuSn×ctAion at the∈ kT-th iteration can be written as follows:
(cid:32) (cid:33)
M
(cid:88)
Qπ(s,a) Q (s,a)=Qπ(s,a) αm (s,a)Qm (s,a) +B (s,a)
h − k,h h − k,h k,h k,h
m=1
M
= (cid:88) αm (s,a)(cid:0) Qπ(s,a) Qm (s,a)(cid:1) +B (s,a), (55)
k,h h − k,h k,h
m=1
where the last equality holds by the fact (cid:80)M αm (s,a)=1.
m=1 k,h
Then, invoking the local update rule in (11), for any i such that (sm ,am )=(s,a), the local Q-estimate
i,h i,h
error at each agent m can be written as follows:
Qπ(s,a) Qm (s,a)
h − i,h
=(1 ηm(s,a))(Qπ(s,a) Qm (s,a))+ηm(s,a)(Qπ(s,a) r (s,a) PmVm )
− i,h h − i−1,h i,h h − h − i,h i−1,h+1
=(1 ηm(s,a))(Qπ(s,a) Qm (s,a))+ηm(s,a)(r (s,a)+P Vπ r (s,a) PmVm )
− i,h h − i−1,h i,h h h,s,a h+1− h − i,h i−1,h+1
=(1 ηm(s,a))(Qπ(s,a) Qm (s,a))
− i,h h − i−1,h
+ηm(s,a)P (Vπ Vm )+ηm(s,a)(P Pm)Vm , (56)
i,h h,s,a h+1− i−1,h+1 i,h h,s,a − i,h i−1,h+1
where the second line follows from the Bellman’s equation. Then, by invoking the relation recursively, the
local Q-estimate error at each agent m obeys the following relation:
Qπ(s,a) Qm (s,a)= (cid:89) (1 ηm(s,a))(cid:0) Qπ(s,a) Q (s,a)(cid:1)
h − k,h − i,h h − ι(k),h
i∈lm (s,a)
k,h
(cid:88) (cid:89)
+ ηm(s,a) (1 ηm (s,a))P (Vπ Vm )
i,h − j,h h,s,a h+1− i−1,h+1
i∈lm (s,a) {j>i:j∈lm (s,a)}
k,h k,h
25(cid:88) (cid:89)
+ ηm(s,a) (1 ηm (s,a))(P Pm)Vm , (57)
i,h − j,h h,s,a − i,h i−1,h+1
i∈lm (s,a) {j>i:j∈lm (s,a)}
k,h k,h
where lm (s,a) denotes a set of episodes where agent m has visited (s,a) at step h within (ι(k),k].
k,h
By inserting (57) to (55) and letting v = ϕ(k), we obtain the following recursive relation for u-th local
updates:
Qπ(s,a) Q (s,a)
h − k,h
 
M
=(cid:88) α km ,h(s,a) (cid:89) (1 −η im ,h(s,a))(cid:0) Qπ h(s,a) −Q ι(k),h(s,a)(cid:1) +B k,h(s,a)
m=1 i∈lm (s,a)
k,h
(cid:124) (cid:123)(cid:122) (cid:125)
:=λv,h(s,a)
 
M
(cid:88) (cid:88) (cid:89)
+ α km ,h(s,a)η im ,h(s,a) (1 −η jm ,h(s,a))P h,s,a(V hπ +1−V im −1,h+1)
m=1i∈lm (s,a) {j>i:j∈lm (s,a)}
k,h k,h
 
M
(cid:88) (cid:88) (cid:89)
+ α km ,h(s,a)η im ,h(s,a) (1 −η jm ,h(s,a))(P
h,s,a
−P im ,h)V im
−1,h+1
m=1i∈lm (s,a) {j>i:j∈lm (s,a)}
k,h k,h
=λ (s,a)(cid:0) Qπ(s,a) Q (s,a)(cid:1) +B (s,a)
v,h h − ι(k),h k,h
M
(H +1) (cid:88) (cid:88)
+ P (Vπ Vm )
N (s,a)+Hn (s,a) h,s,a h+1− i−1,h+1
tv,h tv,h m=1i∈lm (s,a)
k,h
M
(H +1) (cid:88) (cid:88)
+ (P Pm)Vm . (58)
N (s,a)+Hn (s,a) h,s,a − i,h i−1,h+1
tv,h tv,h m=1i∈lm (s,a)
k,h
Here, the last line holds by invoking the definitions in (17) and (18) and observing with abuse of notation
(omit (s,a) when it is clear)
(cid:89)
αm (s,a)ηm(s,a) (1 ηm (s,a))
k,h i,h − j,h
{j>i:j∈lm (s,a)}
k,h
=
1 N ι(k),h+M(H +1)nm
k,h
M(H +1)
 nm
k,h
(cid:89)−nm
i,h(cid:16)N ι(i),h+M(H +1)(nm i,h+j
−1)(cid:17)

M N +Hn N +M(H +1)nm N +M(H +1)(nm +j)
k,h k,h ι(i),h i,h j=1 ι(i),h i,h
=
1 N ι(k),h+M(H +1)nm
k,h
M(H +1) N ι(i),h+M(H +1)nm
i,h
M N +Hn N +M(H +1)nm N +M(H +1)nm
k,h k,h ι(i),h i,h ι(i),h k,h
(H +1) (H +1)
= = (59)
N +Hn N +Hn
k,h k,h tv,h tv,h
where the last line holds since ι(i)=ι(k) for i lm (s,a) and k (K) leads to k =t =t .
Then,byinvokingtheaboverecursiverelati∈onk fo,h reachaggreg∈atTion,theQ-estimateeϕ r( rk o) raftv
erk episodes
is decomposed as follows:
Qπ(s,a) Q (s,a)
h − k,h
ϕ(k) ϕ(k) ϕ(k)
(cid:89) (cid:88) (cid:89)
= λ (s,a)(Qπ(s,a) Q (s,a))+ B (s,a) λ (s,a)
u,h h − 0,h tu,h x,h
u=1 u=1 x=u+1
(cid:124) (cid:123)(cid:122) (cid:125)
:=ω0,k,h(s,a)
 
ϕ(k) M ϕ(k)
(cid:88) (cid:88) (cid:88) H +1 (cid:89)
+  N +Hn λ x,h(s,a)(P h,s,a −P im ,h)V im −1,h+1
u=1m=1i∈lm (s,a) tu,h tu,h x=u+1
tu,h
(cid:124) (cid:123)(cid:122) (cid:125)
:=ωi,k,h(s,a)
26 
ϕ(k) M ϕ(k)
(cid:88) (cid:88) (cid:88) H +1 (cid:89)
+ 
N +Hn
λ x,h(s,a)P h,s,a(V hπ +1−V im −1,h+1)
u=1m=1i∈lm (s,a) tu,h tu,h x=u+1
tu,h
=ω (s,a)(Qπ(s,a) Q (s,a))
0,k,h h − 0,h
M
(cid:88) (cid:88)
+ ωm (s,a)(P Pm)Vm
i,k,h h,s,a − i,h i−1,h+1
m=1i∈Lm (s,a)
k,h
ϕ(k) ϕ(k)
(cid:88) (cid:89)
+ B (s,a) λ (s,a)
tu,h x,h
u=1 x=u+1
M
(cid:88) (cid:88)
+ ωm (s,a)P (Vπ Vm ). (60)
i,k,h h,s,a h+1− i−1,h+1
m=1i∈Lm (s,a)
k,h
Here, λ (s,a), ω (s,a), and ω (s,a) can be simply written as described in (26a), (26b), and (26c),
u,h 0,k,h i,k,h
respectively, which will be proved momentarily. For notational simplicity, we omit (s,a) in the derivations.
Proof of (26a). Consider k =t . First, consider a case that N =0. If n =0, λ =(cid:80)M αm =
v ι(k),h k,h v,h m=1 k,h
1. Otherwise, if n >0, where there exists at least one agent m [M] that visits the state-action at least
k,h
once until k-th episode, it follows that ∈
λ =
(cid:88)M 1 (H +1)Mnm
k,h
n (cid:89)m k,h(cid:18) M(H +1)(j −1)(cid:19)
v,h
M (H +1)n M(H +1)j
k,h
m=1 j=1
=
(cid:88)M nm
k,h +
(cid:88)M nm
k,h
n (cid:89)m k,h(cid:18) (H +1)(j −1)(cid:19)
=0. (61)
n n (H +1)j
m∈[M]:nm k,h=0(cid:124)(cid:123)k (cid:122),h (cid:125) m∈[M]:nm k,h>0 k,h j=1
=0 (cid:124) (cid:123)(cid:122) (cid:125)
=0
On the other hand, when N >0,
ι(k),h
λ =
(cid:88)M 1 N ι(k),h+M(H +1)nm
k,h
n (cid:89)m k,h(cid:18) N ι(k),h+M(H +1)(j −1)(cid:19)
v,h
M N +(H +1)n N +M(H +1)j
ι(k),h k,h ι(k),h
m=1 j=1
=
(cid:88)M 1 N ι(k),h+M(H +1)nm
k,h
N
ι(k),h =
N
ι(k),h . (62)
M N +(H +1)n N +M(H +1)nm N +Hn
m=1 ι(k),h k,h ι(k),h k,h k,h k,h
Proof of (26b). According to (26a), if N (s,a) = 0, then λ (s,a) = 1 for all 1 u ϕ(k). Thus,
k,h u,h
ω (s,a) = 1. Otherwise, let the epsiode when (s,a) is visited at step h by any of≤the≤agents for the
0,k,h
first time be j. Then, λ = 0 because N (s,a) = 0. Thus, if N (s,a) > 0, it always holds that
ϕ(j),h ι(j),h k,h
ω (s,a)=(cid:81)ϕ(k)λ (s,a)=0.
0,k,h u=1 u,h
Proof of (26c). For i such that ϕ(i)=u, by rearranging terms and applying (26a),
 
ϕ(k)
ωm = (H +1)  (cid:89) N tx−1,h 
i,k,h N +Hn N +Hn
tu,h tu,h
x=u+1
tx,h tx,h
 
ϕ(k)−1
= H +1  (cid:89) N tx,h . (63)
N +Hn N +Hn
k,h k,h
x=u
tx,h tx,h
27B.2 Proof of Lemma 2
Consideranygivenδ (0,1)and(k,s,a,h) [K] [H]. NotethatNm (s,a) Binomial(k,dm(s,a))
∈ ∈ ×S×A× k,h ∼ h
forallm [M]. ThenrecallthedefinitionofN (s,a)inSection3.2,wecanviewN (s,a)=(cid:80)M Nm (s,a)
∈ k,h k,h m=1 k,h
as a sum of kM independent Bernoulli variables with expectation ν :=E[N (s,a)]=kMdavg(s,a). There-
k,h h
fore, applying Chernoff bound (see Mitzenmacher and Upfal (2005, Theorem 4.4)) yields:
∀t ∈[0,1] : P((cid:12) (cid:12)N km ,h(s,a) −ν(cid:12) (cid:12) ≥νt) ≤exp(cid:0) −c 1νt2(cid:1) , (64a)
t 1 : P(Nm (s,a) ν tν) exp( c νt), (64b)
∀ ≥ k,h − ≥ ≤ − 1
for some universal constant c >0.
1
Armedwithabovefactsandnotations, nowwearereadytoprove(28). First, applying (64a)witht= 1,
2
we arrive at:
P(cid:16)(cid:12) (cid:12)Nm (s,a) ν(cid:12) (cid:12) ν(cid:17) exp(cid:16) c 1ν(cid:17) δ, (65)
k,h − ≥ 2 ≤ − 4 ≤
where the last line follows from the condition that ν =kMdavg(s,a) 4 log(1).
h ≥ c1 δ
To continue, when ν =kMdavg(s,a) 4 log(1/δ), applying (64b) with t= 4log(1/δ) 1 gives:
h ≤ c1 νc1 ≥
(cid:18) (cid:19)
4log(1/δ)
P Nm (s,a) ν exp( 4log(1/δ)) δ. (66)
k,h − ≥ c ≤ − ≤
1
Summing up (65) and (66) and taking the union bound over (k,s,a,h) [K] [H] complete
the proof by showing that: ∈ ×S ×A×
4
(cid:18) KH(cid:19) kMdavg
ν 3ν
when k log |S||A| : h = Nm (s,a) 2kMdavg,
≥ c Mdavg δ 2 2 ≤ k,h ≤ 2 ≤ h
1 h
(cid:18) (cid:19) (cid:18) (cid:19)
4 KH 8 KH
when k log |S||A| : Nm (s,a) log |S||A|
≤ c Mdavg δ k,h ≤ c δ
1 h 1
holds with probability at least 1 2δ.
−
B.3 Proof of Lemma 3
B.3.1 Proof of (30a)
Noticing that the (30a) involves two terms of interest, and we start with bounding D (s,a,k,h). For any
2
(s,a,h) [H] and any k (K), we can rewrite D (s,a,k,h) as
2
∈S×A× ∈T
k M
(cid:88) (cid:88)
D (s,a,k,h)= Xm (s,a), (67)
2 i,k,h
i=1m=1
where Xm (s,a) = ωm (s,a)(P Pm)Vm I (sm ,am ) = (s,a) . To continue, we first introduce
Lemma 8i, ,k w,h hose proofi,k is,h providedh, is n,a A−ppi e, nh dixi− B1, .h 3+ .31 .{ i,h i,h }
Lemma 8. For any (k,s,a,h) [H] and N [1,MK], let
∈S×A× ∈
X(cid:101) im ,k,h(s,a;N)=ω (cid:101)im ,k,h(s,a;N)(P
h,s,a
−P im ,h)V im −1,h+1I {(sm i,h,am i,h)=(s,a) }, (68)
where
 
ϕ(k)−1
ω (cid:101)im ,k,h(s,a;N):=
N
+HH n+1 (s,a) (cid:89)
N
(s,N at )x +,h( Hs, na) (s,a)I im ,h(s,a;N), (69)
k,h
x=ϕ(i)
tx,h tx,h
28and Im(s,a;N) := I (cid:80)M Nm′ (s,a)+(cid:80)m I (sm′ ,am′ ) = (s,a) N . Then, for any δ (0,1),
i,h { m′=1 i−1,h m′=1 { i,h i,h } ≤ } ∈
the following holds:
(cid:12) (cid:12)(cid:88)k (cid:88)M (cid:12) (cid:12) (cid:114) 81H4ζ2
(cid:12) X(cid:101)m (s,a;N)(cid:12) 1 (70)
(cid:12) (cid:12) i,k,h (cid:12) (cid:12)≤ N
i=1m=1
(cid:16) (cid:17)
at least with probability 1 δ, where we denote ζ =log
|S||A|MK2H
.
− 1 δ
Armed with the above lemma, for any (s,a,k,h) [K] [H] where k (K), the following
holds by setting N =N (s,a): ∈ S ×A× × ∈ T
k,h
(cid:12) (cid:12) (cid:115)
(cid:12)(cid:88)k (cid:88)M (cid:12) 81H4ζ2
when N k,h(s,a)>0: |D 2(s,a,k,h) |≤(cid:12)
(cid:12) (cid:12)
X(cid:101) im ,k,h(s,a;N k,h(s,a))(cid:12)
(cid:12) (cid:12)≤ N
k,h(s,1
a)
(71)
i=1m=1
withprobabilityatleast1 δ. AsitisobviousthatD (s,a,k,h)=0whenN (s,a)=0fromthedefinition
2 k,h
of D (s,a,k,h), we arrive−at
2
(cid:12) (cid:12) (cid:115)
(cid:12)(cid:88)k (cid:88)M (cid:12) 81H4ζ2
|D 2(s,a,k,h) |≤(cid:12)
(cid:12) (cid:12)
X(cid:101) im ,k,h(s,a;N k,h(s,a))(cid:12)
(cid:12) (cid:12)≤ N
k,h(s,1 a). (72)
i=1m=1
Finally, combining the results for D (s,a,k,h) (cf. (72)) and D (s,a,k,h) (cf. (38) in Lemma 4), we
2 3
conclude that for any (s,a,k,h) [K] [H] with k (K), it holds with probability at least 1 δ
that ∈S×A× × ∈T −
(cid:115) (cid:115)
81H4ζ2 c ζ2H4
D (s,a,k,h) 1 = B 1 D (s,a,k,h). (73)
2 3
| |≤ N (s,a) N (s,a) ≤
k,h k,h
B.3.2 Proof of (30b) and (30c)
For all (h,s,a,k) [H] (K), it is clear that Qπk(s,a) Q⋆(s,a) and Vπk(s) V⋆(s) by
definition. Hence, i∈t suffic×esSto×shAow×tThat h ≤ h h ≤ h
Q (s,a) Qπk(s,a) and V (s) Vπk(s)
k,h ≤ h k,h ≤ h
for all (h,s,a,k) [H] (K), which we will prove by an induction argument as below.
∈ ×S×A×T
• Base case. When h = H +1, for all (s,a,k) (K), the relation always holds since
Q (s,a) = 0 Qπk (s,a) and V (s) = 0∈ S V×πkA (× s)Taccording to the definition of Q
ank d,H V+1 , respe≤ctiveH ly+ .1 k,H+1 ≤ H+1 k,H+1
k,H+1
• Induction. When h [H], suppose the relation holds for h+1, i.e., Q (s,a) Qπk (s,a) and
V (s) Vπk (s) for∈all (s,a,k) (K). First, we will verifk y,h t+ h1 e Q-est≤imah t+ es1 at step h
ark e,h p+ e1 ssim≤istich .+ F1 or any (s,a,k) ∈ S ×A× (KT ), applying Lemma 1,
∈S×A×T
Qπk(s,a) Q (s,a)=Dπk(s,a,k,h)+D (s,a,k,h)+D (s,a,k,h)+Dπk(s,a,k,h). (74)
h − k,h 1 2 3 4
Wecontroltheabovefourtermsoneatatime. Here, Dπk(s,a,k,h) 0sinceQπk(s,a) Q (s,a)=
0. In addition, according to (30a), D (s,a,k,h) D (1 s,a,k,h). A≥nd it is cleah r that D≥ 0 0,h due to
2 3 4
| |≤ ≥
Vπk V V , (75)
h+1 ≥ k,h+1 ≥ ι(i),h+1
where the first inequality holds by the induction assumption, and the last inequality arises from the
monotonicity of the global value update in (14). Therefore, it is clear that for any (s,a,k)
(K), the Q-estimates at step h are pessimistic, i.e., ∈S×A×
T
Qπk(s,a) Q (s,a) 0. (76)
h − k,h ≥
29Next, to show that value estimates at step h are pessimistic, recalling the global update in (14),
Vπk(s) V (s)=Qπk(s,π (s)) max maxQ (s,a),V (s)
h − k,h h k,h − { a k,h ι(k),h }
=Qπk(s,π (s)) maxQ (s,a)
h k,h − a k0,h
=Qπk(s,π (s)) Q (s,π (s)) 0, (77)
h k0,h − k0,h k0,h ≥
where k denotes the most recent episode satisfying V (s) = max Q (s,a) and k k (K),
andthe0
lastinequalityholdsbecauseπ (s)=π
(s)k a,h ndQπk(s,a)a Qk0,h
(s,a)
0ca≥nb0
e∈simTilarly
verified using (74) and (75) for k . Nok w,h , we verk i0 f, yh that Qπk(h s,a) − Q k0 (, sh ,a) an≥d Vπk(s) V (s)
holds at step h for any (s,a,k)
0
(K), and this
dh
irectly
c≥ompk l, eh
tes the
induch
tion
a≥rgumk,h
ent.
∈S×A×T
B.3.3 Proof of Lemma 8
To begin with, for any time step h [H], we denote the expectation conditioned on the trajectories j i of
all agent as ∈ ≤
(i,m) [k] [M]: E []=E(cid:2) (cid:8) sm′ ,am′ ,Vm (cid:9) , (cid:8) sm′ ,am′(cid:9) (cid:3) . (78)
∀ ∈ × (i,m) · · | j,h j,h j,h+1 j<i,m′∈[M] i,h i,h m′≤m
Armed withthisnotation, fixing N, itis easilyverified thatE [X(cid:101)m(s,a;N)]=0since thenVm can
(i,m) i,k i−1,h+1
be regarded as fixed and (P Pm) is independent from ωm (s,a;N).
Consequently, we can
aph p, ls y,a F−reedi,h
man’s inequality (see
th(cid:101) ei, uk s,h
er-friendly version provided in Theorem 2)
and control the term of interest for any (s,a,k,h) [K] [H] and N [1,MK] as below:
∈S×A× × ∈
(cid:88)k (cid:88)M
X(cid:101) im
,k,h(s,a;N)( ≤i)(cid:112)
8B 1ζ 1+
4
3B 2ζ
1
( ≤ii)(cid:114) 32H N4ζ
1 +
3H N2ζ
1
≤(cid:114) 81H N4ζ 12
(79)
i=1m=1
at least with probability 1 δ. Here, (i) and (ii) arises from the following definition and facts about B and
1
B : −
2
(cid:88)k (cid:88)M (cid:20)(cid:16) (cid:17)2(cid:21) 4H4
B
1
:= E
(i,m)
X(cid:101) im ,k,h(s,a;N)
≤ N
, (80)
i=1m=1
(cid:12) (cid:12) 2H2
B
2
:= (i,m)m ∈[a kx ]×[M](cid:12) (cid:12)X(cid:101) im ,k,h(s,a;N)(cid:12)
(cid:12) ≤ N
(81)
where the proofs of (80) and (81) are provided as below, respectively.
Proof of (80). In view of that the events happen at any time step h are independent from the transitions
in later time steps including Pm, we have ωm (s,a;N) is independent from (P Pm)Vm , which
yields i,h (cid:101)i,k,h h,s,a − i,h i−1,h+1
k M k M
(cid:88) (cid:88) (cid:88) (cid:88)
E (i,m)[(X(cid:101) im ,k,h(s,a;N))2]= E (i,m)[(ω (cid:101)im ,k,h(s,a;N))2]Var Ph,s,a(V im −1,h+1)
i=1m=1 i=1m=1
k M
(cid:88) (cid:88)
H2 E [(ωm (s,a;N))2]
≤
(i,m) (cid:101)i,k,h
i=1m=1
(cid:18) 2H(cid:19)2 4H4
H2N = , (82)
≤ N N
where the penultimate inequality holds by the fact that ωm (s,a;N) 2H.
|(cid:101)i,k,h |≤ N
30Proof of (81). For any (i,m,h) [k] [M] [H] and fixed N [1,MK], it is observed that
∈ × × ∈
(cid:12) (cid:12) (cid:12)X(cid:101) im ,k,h(s,a;N)(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12)ω (cid:101)im ,k,h(s,a;N)(P h,s,a −P im ,h)V im −1,h+1I {(sm i,h,am i,h)=(s,a) }(cid:12) (cid:12)
2H2
ωm (s,a;N) P Pm Vm , (83)
≤|(cid:101)i,k,h
|·∥
h,s,a
−
i,h∥1
·∥
i−1,h+1∥∞
≤ N
where the last inequality follows from Vm H, P Pm 1, and ωm (s,a;N) 2H.
∥ i−1,h+1∥∞ ≤ ∥ h,s,a − i,h∥1 ≤ |(cid:101)i,k,h |≤ N
B.4 Proof of Lemma 4
With slight abuse of notation, we will omit (s,a) from some notation when it is clear from the context for
simplicity in this proof. Recall the definition of D (s,a,k,h) in (25) and the global penalty defined in (16).
3
When N (s,a) = 0, the global penalties are all 0, which yields D (s,a,k,h) = 0. Therefore, it suffices to
k,h 3
focus on the case when N (s,a)>0 and show that for c =81, c =4 and c =1,
k,h B u l
ϕ(k) ϕ(k) (cid:34)(cid:115) (cid:115) (cid:35)
D 3(s,a,k,h)=
(cid:88)
B tu,h(s,a)
(cid:89)
λ u′,h(s,a)
∈
Nc lc Bζ
(1
s2H ,a4
),
c Nuc Bζ (s12 ,H a)4
. (84)
u=1 u′=u+1 k,h k,h
Towards this, for any (s,a) , we consider a more general term as below: for any integer z 1,
∈S×A ≥
(cid:115)
(cid:88)z
B tu,h
(cid:89)z
λ u′,h
=(cid:88)z N(H + +1 H)n
ntu,h
c
B
Nζ 12H4 (cid:89)z
λ u′,h
u=1 u′=u+1 u=1 k,h tu,h tu,h u′=u+1
(cid:115)
(cid:113) (cid:88)z 1 (cid:89)z
= c Bζ 12H4
N
(1 −λ u,h) λ u′,h
u=1 tu,h u′=u+1
(cid:113)
= c ζ2H4Y(z) (85)
B 1
where the penultimate equality follows from
(H +1)n (s,a)
tu,h
=1 λ (s,a)
u,h
N +Hn (s,a) −
tu,h tu,h
for all (s,a) , and the last equality arises by defining
∈S×A
(cid:115)
z z
(cid:88) 1 (cid:89)
Y(z):= (1 λ u,h) λ u′,h. (86)
N −
u=1 tu,h u′=u+1
As a result, to show (84), it suffices to verify that
(cid:20)(cid:114) c (cid:114) c (cid:21)
Y(z) l , u , (87)
∈ N (s,a) N (s,a)
tz,h tz,h
which we proceed by an induction argument.
Proof of (87) by induction. To begin with, for the basic case z =1, it is easily verified that
(cid:40)(cid:113)
1 if n >0
Y(1)= Nt1,h t1,h , (88)
0 if n =0
t1,h
since when n >0 we have λ (s,a)=0, and otherwise λ (s,a)=1. Then suppose (87) holds for z 1,
namely,
t1,h 1,h 1,h
−
(cid:20)(cid:114) c (cid:114) c (cid:21)
Y(z 1) l , u , (89)
− ∈ N N
tz−1,h tz−1,h
31wehopetoshow (87)holdsforz. Towardsthis, wefirstshowtheupperboundin(87)holdsforz asfollows:
(cid:115)
1
Y(z)=Y(z 1)λ + (1 λ )
z,h z,h
− N −
tz,h
(cid:115)
(i)(cid:114) c N 1 (H +1)n
u tz−1,h
+
tz,h
≤ N N +Hn N N +Hn
tz−1,h tz,h tz,h tz,h tz,h tz,h
(cid:115) (cid:115)
(cid:114) c N 1 (H +1)n
u tz−1,h
+
tz,h
≤ N N +Hn N N +Hn
tz,h tz,h tz,h tz,h tz,h tz,h
(cid:114) c (cid:32)(cid:115) N (cid:114) 1 (H +1)n (cid:33)
=
u tz−1,h
+
tz,h
N N +Hn c N +Hn
tz,h tz,h tz,h u tz,h tz,h
(cid:114) c (cid:32)(cid:115) N (cid:114) 1 (cid:32) (cid:115) N (cid:33)(cid:32) (cid:115) N (cid:33)(cid:33)
=
u tz−1,h
+ 1
tz−1,h
1+
tz−1,h
N N +Hn c − N +Hn N +Hn
tz,h tz,h tz,h u tz,h tz,h tz,h tz,h
(cid:114) c
u , (90)
≤ N
tz,h
where(i)followsfromtheinductionassumptionand (H+1)ntz,h(s,a) =(1 λ (s,a))forall(s,a) ,
the penultimate equality holds by
Ntz,h+Hntz,h(s,a) − z,h ∈S×A
N N N +Hn (H +1)n
1 tz−1,h = tz,h − tz−1,h tz,h = tz,h ,
− N +Hn N +Hn N +Hn
tz,h tz,h tz,h tz,h tz,h tz,h
(cid:113) (cid:18) (cid:113) (cid:19)
and the last inequality arises from 1 1+ Ntz−1,h 1 as long as c 4.
cu Ntz,h+Hntz,h ≤ u ≥
Analogous to (90), the lower bound of Y(z) is derived as below:
(cid:115)
1
Y(z)=Y(z 1)λ + (1 λ )
z,h z,h
− N −
tz,h
(cid:115)
(cid:114) c N 1 (H +1)n
l tz−1,h
+
tz,h
≥ N N +Hn N N +Hn
tz−1,h tz,h tz,h tz,h tz,h tz,h
(cid:115)
(cid:114) c N 1 (H +1)n
l tz−1,h
+
tz,h
≥ N N +Hn N N +Hn
tz,h tz,h tz,h tz,h tz,h tz,h
(cid:114) c
l , (91)
≥ N
tz,h
where the first inequality follows from the induction assumption and (H+1)ntz,h(s,a) =(1 λ (s,a)) for
all (s,a) , and the last equality holds when 1 c . Finally,
byNt izn,h d+ uH ctn it ozn,h( as r,a g) uments−
,
(8z 7,h
) holds for
l
any z ϕ∈ (KS )×, aAnd this completes the proof. ≥
∈
B.5 Proof of Lemma 5
Recall the definition of D (see (35) and (25)), D can be rewritten as follows:
4,h 4,h
ϕ(K) M
D = (cid:88) τ (cid:88) dπ⋆ (s,a) (cid:88) (cid:88) ωm (s,a)P (V⋆ V )
4,h v h i,tv,h h,s,a h+1− ι(i),h+1
v=1 (s,a)∈S×A m=1i∈Lm (s,a)
tv,h
 
ϕ(K) v M
( =i) (cid:88) τ v (cid:88) dπ h⋆ (s,a)(cid:88) P h,s,a(V h⋆ +1−V tu−1,h+1) (cid:88)  (cid:88) ω im ,tv,h(s,a)
v=1 (s,a)∈S×A u=1 m=1 i∈lm (s,a)
tu,h
(cid:124) (cid:123)(cid:122) (cid:125)
=:ψu,v,h(s,a)
32ϕ(K) v
= (cid:88) (cid:88) (cid:88) dπ⋆ (s,a)(cid:88) P (V⋆ V )ψ (s,a), (92)
h h,s,a h+1− tu−1,h+1 u,v,h
(s,a)∈S×A v=1 tv−1<j≤tv u=1
where (i) holds by rewriting the sum as (cid:80) = (cid:80)v (cid:80) and the last equality holds by
i∈Lm (s,a) u=1 i∈lm (s,a)
the definition of τ . tv,h tu,h
v
To further control (92), we introduce the following lemma that bounds the expectation form (92) by an
empirical version; the proof is postponed to Appendix B.5.1.
Lemma 9. Consider any δ (0,1). For any h [H], the following holds:
∈ ∈
ϕ(K) v
(cid:88) (cid:88) (cid:88) dπ⋆ (s,a)(cid:88) P (V⋆ V )ψ (s,a)
h h,s,a h+1− tu−1,h+1 u,v,h
(s,a)∈S×A v=1 tv−1<j≤tv u=1
1 (cid:88)
ϕ (cid:88)(K) dπ⋆
(s,a)
(cid:88)v
≲ h n (s,a) P (V⋆ V )ψ (s,a)+σ (93)
M davg(s,a) tv,h h,s,a h+1− tu−1,h+1 u,v,h aux,1
(s,a)∈S×A v=1 h u=1
at least with probability 1 δ, where
−
(cid:114)
H2KSC⋆ H2SC⋆
σ ≲ avg + avg (94)
aux,1
M M
Then, applying concentration bounds, D is bounded as follows:
4,h
(i) 1 (cid:88) ϕ (cid:88)(K) (cid:88)v dπ⋆ (s,a)
D ≲ h n (s,a)P (V⋆ V )ψ (s,a)+σ
4,h M davg(s,a) tv,h h,s,a h+1− tu−1,h+1 u,v,h aux,1
(s,a)∈S×A v=1 u=1 h
1 (cid:88)
ϕ (cid:88)(K) dπ⋆
(s,a)
ϕ (cid:88)(K)
= h P (V⋆ V ) n (s,a)ψ (s,a)+σ
M davg(s,a) h,s,a h+1− tu−1,h+1 tv,h u,v,h aux,1
(s,a)∈S×A u=1 h v=u
(ii) 1 (cid:88) ϕ (cid:88)(K) dπ⋆ (s,a) (cid:16) 1 (cid:17)
h P (V⋆ V )n (s,a) 1+ +σ (95)
≤ M davg(s,a) h,s,a h+1− tu−1,h+1 tu,h H aux,1
(s,a)∈S×A u=1 h
where (i) follows from Lemma 9, and (ii) holds because
(cid:88)∞ (cid:88)M (cid:88) (cid:16) 1 (cid:17)
n (s,a) ωm (s,a) n (s,a) 1+ (96)
tv,h i,tv,h ≤ tu,h H
v≥u m=1i∈lm (s,a)
tu,h
according (49e) in Lemma 7.
To continue, we introduce the following lemma that transfers the distribution at time step h to the
distribution at time step h+1; the proof is provided in Appendix B.5.3.
Lemma 10. Consider any δ (0,1). For any h [H], the following holds:
∈ ∈
ϕ(K)
(cid:88) (cid:88) n tu,h(s,a) dπ⋆ (s,a)P (V⋆ V )
Mdavg(s,a) h h,s,a h+1− tu−1,h+1
u=1 (s,a)∈S×A h
ϕ(K)
≲ (cid:88) τ (cid:88) dπ⋆ (s)(V⋆ (s) V (s))+σ (97)
u h+1 h+1 − tu−1,h+1 aux,2
u=1 s∈S
at least with probability 1 δ, where
−
(cid:114)
H2KSC⋆ HSC⋆
avg avg
σ = + .
aux,2
M M
33Armed with the above lemma, rearranging the terms in (95) and applying Lemma 10,
ϕ(K)
D ≲(cid:16) 1+ 1 (cid:17) (cid:88) (cid:88) n tu,h(s,a) dπ⋆ (s,a)P (V⋆ V )+σ
4 H Mdavg(s,a) h h,s,a h+1− tu−1,h+1 aux,1
u=1 (s,a)∈S×A h
ϕ(K)
≲(cid:16) 1+ 1 (cid:17) (cid:88) τ (cid:88) dπ⋆ (s)(V⋆ (s) V (s))+σ +σ ,
H u h+1 h+1 − tu−1,h+1 aux,1 aux,2
(cid:124) (cid:123)(cid:122) (cid:125)
u=1 s∈S
=:σaux
and this completes the proof.
B.5.1 Proof of Lemma 9
Consider any given (s,a) and v [1,ϕ(K)]. Before proceeding, we introduce some notation and
auxiliary terms. Let ∈ S ×A ∈
v
(cid:88)
G (s,a):= P (V⋆ V )ψ (s,a). (98)
v,h h,s,a h+1− tu−1,h+1 u,v,h
u=1
Then, for any t <j t , we introduce the following auxiliary variables:
v−1 v
≤
Ym := (cid:88) (cid:0) davg(s,a) I (s,a)=(sm ,am ) (cid:1) dπ h⋆ (s,a) G (s,a) (99)
j,h h − { j,h j,h } davg(s,a) v,h
(s,a)∈S×A h
Y(cid:101) jm
,h
:= (cid:88) (cid:0) dm h(s,a) −I {(s,a)=(sm j,h,am j,h) }(cid:1) dd aπ h v⋆ g( (s s, ,a a) )G(cid:101)− v,j h,m(s,a), (100)
(s,a)∈S×A h
where we define
(cid:40)
G(cid:101)−j,m(s,a):=
ψ(cid:101) v− ,j v, ,m h(s,a)P h,s,a(V h⋆ +1−V tv−1,h+1)+(1 −ψ(cid:101) v− ,j v, ,m h(s,a))G v−1,h(s,a) if v >1
(101)
v,h P (V⋆ V ) if v =1
h,s,a h+1− 0,h+1
and
(H +1)(n (s,a) I (s,a)=(sm ,am ) )
ψ(cid:101)−j,m(s,a):= tv,h − { j,h j,h }
v,v,h N (s,a)+(H +1)(n (s,a) I (s,a)=(sm ,am ) )
tv−1,h tv,h − { j,h j,h }
(H +1)((cid:80) I (s,a)=(sm′ ,am′ ) )
= (m′,j′)∈[M]×(tv−1,tv]\{(j,m)} { j′,h j′,h } . (102)
N (s,a)+(H +1)((cid:80) I (s,a)=(sm′ ,am′ ) )
tv−1,h (m′,j′)∈[M]×(tv−1,tv]\{(j,m)} { j′,h j′,h }
WereplacedG v,h(s,a)withasurrogateG(cid:101) v− ,j h,m(s,a), wherethevisitsofagentmon(s,a)atthej-thepisode
aremaskedregardlessoftheactualvisitsofagentmon(s,a). Thesurrogateiscarefullydesignedtoremove
the dependency on the event I (s,a) = (sm ,am ) from G (s,a) while maintaining close distance to the
original value G (s,a). { j,h j,h } v,h
v,h
Beforecontinuing, weintroducesomeusefulpropertiesoftheabovedefinedauxiliarytermswhoseproofs
are provided in Appendix B.5.2: for any v [ϕ(K)],
∈
(cid:40)
ψ (s,a)P (V⋆ V )+(1 ψ (s,a))G (s,a) if v >1
G (s,a)= v,v,h h,s,a h+1− tv−1,h+1 − v,v,h v−1,h , (103a)
v,h P (V⋆ V ) if v =1
h,s,a h+1− 0,h+1
0 ≤G(cid:101)− v,j h,m(s,a), G v,h(s,a) ≤H, (103b)
(cid:26) 2H2 (cid:27)
|G(cid:101)− v,j h,m(s,a) −G v,h(s,a) |≤min H,
N (s,a)
. (103c)
tv,h
Now, we are ready to prove (93). Towards this, we first observe that moving the first term in the
right-hand side of (93) to the left-hand side, and multiplying by a factor of M, yields
 
(cid:88) ϕ (cid:88)(K) (cid:88)M (cid:88) dπ h⋆ (s,a)
−
dd aπ h v⋆ g( (s s, ,a a) )n tv,h(s,a)(cid:88)v P h,s,a(V h⋆ +1−V tu−1,h+1)ψ u,v,h(s,a)
(s,a)∈S×A v=1 m=1tv−1<j≤tv h u=1
34 
( =i) (cid:88) ϕ (cid:88)(K) (cid:88)M (cid:88) da hvg(s,a)
−
(cid:88)M nm tv,h(s,a) dd aπ h v⋆ g( (s s, ,a a) )G v,h(s,a)
(s,a)∈S×A v=1 m=1tv−1<j≤tv m=1 h
 
( =ii) (cid:88) (cid:88)M (cid:88)K da hvg(s,a) −(cid:88)K I {(s,a)=(sm j,h,am j,h) } dd aπ h v⋆ g( (s s, ,a a) )G v,h(s,a)
(s,a)∈S×Am=1 j=1 j=1 h
=(cid:88)K (cid:88)M (cid:88) (cid:0) davg(s,a) I (s,a)=(sm ,am ) (cid:1) dπ h⋆ (s,a) G (s,a)=(cid:88)K (cid:88)M Ym, (104)
h − { j,h j,h } davg(s,a) v,h j,h
j=1m=1(s,a)∈S×A h j=1m=1
where(i)holdsbypluggingin(98)andn (s,a)=(cid:80)M nm (s,a),(ii)followsfrom(cid:80)ϕ(K)(cid:80) 1=
tv,h m=1 tv,h v=1 tv−1<j≤tv
K and(cid:80)ϕ(K)nm (s,a)=(cid:80)K I (s,a)=(sm ,am ) ,andthelastequalityarisefromthedefinitionofYm
in (B.5.1)v .=1 tv,h j=1 { j,h j,h } j,h
Therefore, the above fact shows that to prove (93), it is suffices to show:
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)(cid:88)K (cid:88)M Y jm ,h(cid:12) (cid:12) (cid:12) (cid:12)≤(cid:12) (cid:12) (cid:12) (cid:12)(cid:88)K (cid:88)M Y(cid:101) jm ,h(cid:12) (cid:12) (cid:12) (cid:12)+(cid:12) (cid:12) (cid:12) (cid:12)(cid:88)K (cid:88)M (cid:16) Y jm ,h−Y(cid:101) jm ,h(cid:17)(cid:12) (cid:12) (cid:12) (cid:12)≲Mσ aux,1. (105)
(cid:12)j=1m=1 (cid:12) (cid:12)j=1m=1 (cid:12) (cid:12)j=1m=1 (cid:12)
We will control the two essential terms separately as below:
(cid:12) (cid:12)
• Controlling (cid:12)(cid:80)K (cid:80)M Y(cid:101)m(cid:12). Tobeginwith,weobservethattheapproximateG(cid:101)−j,m(s,a)(defined
(cid:12) j=1 m=1 j,h(cid:12) v,h
in (101)) is independent of agent m’s visits on (s,a) at the j-th episode since V , G (s,a)
tv−1,h+1 v−1,h
are independent of the j-th episode and ψ(cid:101)−j,m(s,a) is independent from agent m’s visits on (s,a) at
v,v,h
the j-th episode (see (102)). It follows that E j−1[Y(cid:101) jm ,h]=0, where we denote
(cid:104) (cid:105)
E []=E (sm′ ,am′ ),Vm′ .
j−1 · ·|{ i,h i,h i,h+1}i<j,m′∈[M]
Thus, applying the Freedman’s inequality for each h [H], we can show that the following holds:
∈
(cid:12) (cid:12)
(cid:12) K M (cid:12) (cid:114)
(cid:12)(cid:88) (cid:88) (cid:12) 2H 8 2H
(cid:12) Y(cid:101)m(cid:12) 8W log + Blog
(cid:12) j,h(cid:12)≤ δ 3 δ
(cid:12)j=1m=1 (cid:12)
(cid:113)
≲ H2MKSC⋆ +HSC⋆ (106)
avg avg
at least with probability 1 δ, where B and W is obtained as follows:
−
(cid:12) (cid:12)
(cid:12)Y(cid:101)m(cid:12) 2C⋆ (1+dπ⋆ (s,π⋆(s))S)maxG(cid:101)−j,m (s,π⋆(s)) 4SC⋆ H =:B (107)
(cid:12) j,h(cid:12) ≤ avg h s∈S ϕ(j),h ≤ avg
(cid:88)K (cid:88)M
E
j−1(cid:20)(cid:16)
Y(cid:101) jm
,h(cid:17)2(cid:21) ≤(cid:88)K (cid:88)M
E (sm j,h,am j,h)∼dm
h
 (cid:32) dd aπ
h
v⋆ g( (s sm
j
m,h, ,a am
j
m,h)
)G(cid:101)− ϕ(j j, )m ,h(sm j,h,am
j,h)(cid:33)2

j=1m=1 j=1m=1 h j,h j,h
(cid:88)K (cid:88)M (cid:88) (cid:18) dπ⋆ (s,π⋆(s)) (cid:19)2
≤
dm h(s,π⋆(s)) dah vg(s,π⋆(s))G(cid:101)− ϕ(j j, )m ,h(s,π⋆(s))
j=1m=1s∈S h
H2C⋆ (cid:88)K (cid:88) (cid:88)M dm(s,π⋆(s))dπ h⋆ (s,π⋆(s)) (1+dπ⋆ (s,π⋆(s))S)
≤ avg h davg(s,π⋆(s)) h
j=1s∈Sm=1 h
K
H2C⋆ (cid:88)(cid:88) Mdπ⋆ (s,π⋆(s))(1+dπ⋆ (s,π⋆(s))S)
≤ avg h h
j=1s∈S
2H2SC⋆ MK =:W (108)
≤ avg
usingthefactthat |G(cid:101)− ϕ(j j, )m ,h(sm j,h,am j,h) |≤H shownin(103b)and min{dd ππ h ⋆⋆ (( ss ,, ππ ⋆⋆ (( ss )) ))
,1/S}
≤1+dπ h⋆ (s,π⋆(s))S.
h
35• Bound on the approximation gap of Y(cid:101)m. The approximation gap of Y(cid:101)m is bounded as follows:
j,h j,h
(cid:12) (cid:12)
(cid:12) (cid:12)(cid:88)K (cid:88)M (cid:16) (cid:17)(cid:12) (cid:12)
(cid:12) Y(cid:101)m Ym (cid:12)
(cid:12) j,h− j,h (cid:12)
(cid:12)j=1m=1 (cid:12)
(cid:12) (cid:12)
=(cid:12) (cid:12) (cid:12) (cid:12)ϕ (cid:88)(K) (cid:88)M (cid:88) (cid:88) (cid:0) dm h(s,a) −I {(s,a)=(sm j,h,am j,h) }(cid:1) dd aπ h v⋆ g( (s s, ,a a) )(G(cid:101)− v,j h,m(s,a) −G v,h(s,a))(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)v=1 m=1tv−1<j≤tv(s,a)∈S×A h (cid:12)
( =i)ϕ (cid:88)(K) (cid:88)M (cid:88) (cid:88) I {(s,a)=(sm j,h,am j,h) }(1 −dm h(s,a)) dd aπ h v⋆ g( (s s, ,a a) )(cid:12) (cid:12) (cid:12)G(cid:101)− v,j h,m(s,a) −G v,h(s,a)(cid:12) (cid:12)
(cid:12)
v=1 m=1tv−1<j≤tv(s,a)∈S×A h
(ii)ϕ (cid:88)(K) (cid:88)M (cid:88) (cid:88) dπ⋆ (s,a) (cid:26) 2H2 (cid:27)
I (s,a)=(sm ,am ) h min ,H
≤ { j,h j,h }davg(s,a) N (s,a)
v=1 m=1tv−1<j≤tv(s,a)∈S×A h tv,h
(iii) (cid:88)ϕ (cid:88)(K) dπ⋆ (s,π⋆(s)) (cid:26) 2H2 (cid:27)
C⋆ n (s,π⋆(s)) h min ,H
≤ avg tv,h min dπ⋆(s,π⋆(s)),1/S N (s,π⋆(s))
s∈S v=1 { h } tv,h
(iv) 2H2C⋆ (cid:88) (1+dπ⋆ (s,π⋆(s))S)ϕ (cid:88)(K) min(cid:26) n tv,h(s,π⋆(s)) ,n (s,π⋆(s))(cid:27)
≤ avg h N (s,π⋆(s)) tv,h
s∈S v=1
tv,h
(v)
≲ C⋆ H2S (109)
avg
w ach ce or re di( ni) gh to old (1s 0b 3e ac )a ,u (s iie )ψ(cid:101) fov−
l,
lj
v
o, ,m
h
w( ss, fra o) m= (1ψ 0v− 3,j
v
c, ,m
)h
,( (s i, iia )) ni af t( us rm
j a,h
ll, yam
j h,h
o) ld̸=
s
a( cs c, oa r) da inn gd tG o(cid:101)−
v
t,j
h
h,m e( ds e, fia n) it= ionG
v o,
fh( Cs, ⋆a)
,
avg
(iv) holds because dπ h⋆ (s,π⋆(s)) 1+dπ⋆ (s,π⋆(s))S, and (v) holds because for any z [ϕ(K)],
min{dπ⋆(s,π⋆(s)),1/S} ≤ h ∈
h
(cid:88)z n tv,h(s,π⋆(s))
1+log(N (s,π⋆(s))), (110)
N (s,π⋆(s)) ≤ tz,h
v=1
tv,h
according to Lemma 6.
Now, combining the bounds obtained above (cf. (106) and (109)) into (105), we conclude that
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)(cid:88)K (cid:88)M Ym(cid:12) (cid:12) (cid:12)≲(cid:113) H2MKSC⋆ +H2SC⋆ =M(cid:32)(cid:114) H2KSC a⋆ vg + H2SC a⋆ vg(cid:33) (111)
(cid:12) j,h(cid:12) avg avg M M
(cid:12)j=1m=1 (cid:12)
which completes the proof.
B.5.2 Proof of (103)
Proof of (103a). We will proof (103a) by considering different cases separately. When v =1, we have
G (s,a)=P (V⋆ V )ψ (s,a)
v,h h,s,a h+1− tv−1,h+1 1,1,h
 
M
(cid:88) (cid:88)
=P h,s,a(V h⋆ +1−V 0,h+1)  ω im ,t1,h(s,a)=P h,s,a(V h⋆ +1−V 0,h+1) (112)
m=1 i∈lm (s,a)
t1,h
where the second equality follows from the definition of ψ (s,a) in (92), and the last equality holds since
u,v,h
M
(cid:88) (cid:88) ωm (s,a)= (H +1)n t1,h = (H +1)n t1,h =1.
i,t1,h N +Hn (H +1)n
m=1i∈lm (s,a) t1,h t1,h t1,h
t1,h
36When v >1, invoking the definition of ωm in (26c) yields that for any u<v,
i,tv,h
M
(cid:88) (cid:88)
ψ (s,a)= ωm (s,a)
u,v,h i,tv,h
m=1i∈lm (s,a)
tu,h
(cid:32) v−1 (cid:33)
=
(H +1)n tu,h (cid:89) N tx,h
N +Hn N +Hn
tv,h tv,h
x=u
tx,h tx,h
(cid:32) v−2 (cid:33)
=
(H +1)n tu,h (cid:89) N tx,h N tv−1,h
N +Hn N +Hn N +Hn
tv−1,h tv−1,h
x=u
tx,h tx,h tv,h tv,h
=ψ (s,a)(1 ψ (s,a)). (113)
u,v−1,h v,v,h
−
where the second equality holds by ϕ(i) = u for all i lm (s,a) and the fact (cid:80)M (cid:80) 1 =
∈ tu,h m=1 i∈l tm u,h(s,a)
n , and the last equality holds by 1 ψ (s,a) = 1 (H+1)ntv,h = Ntv−1,h+(H+1)ntv,h−(H+1)ntv,h =
tu,h − v,v,h − Ntv,h+Hntv,h Ntv,h+Hntv,h
Ntv−1,h .
Ntv, Ch+ onH sn et qvu,h
ently, inserting the above fact back into (98) complete the proof by showing that
v
(cid:88)
G (s,a)= P (V⋆ V )ψ (s,a)
v,h h,s,a h+1− tu−1,h+1 u,v,h
u=1
v−1
(cid:88)
=P (V⋆ V )ψ (s,a)+ P (V⋆ V )ψ (s,a)
h,s,a h+1− tv−1,h+1 v,v,h h,s,a h+1− tu−1,h+1 u,v,h
u=1
v−1
(cid:88)
=P (V⋆ V )ψ (s,a)+(1 ψ (s,a)) P (V⋆ V )ψ (s,a)
h,s,a h+1− tv−1,h+1 v,v,h − v,v,h h,s,a h+1− tu−1,h+1 u,v−1,h
u=1
=P (V⋆ V )ψ (s,a)+(1 ψ (s,a))G (s,a). (114)
h,s,a h+1− tv−1,h+1 v,v,h − v,v,h v−1,h
Proof of (103b). First, applying (30c) in Lemma 3 gives G (s,a) 0. Then we focus on deriving the
v,h
upper bound G (s,a). Towards this, we observe that ≥
v,h
v
(cid:88)
G (s,a)= P (V⋆ V )ψ (s,a)
v,h h,s,a h+1− tu−1,h+1 u,v,h
u=1
v
(cid:88)
P (V⋆ V ) ψ (s,a)
≤ h,s,a h+1− 0,h+1 u,v,h
u=1
v
(cid:88)
H ψ (s,a)
u,v,h
≤
u=1
 
v M
(cid:88) (cid:88) (cid:88)
=H  ωm (s,a) H, (115)
i,tv,h ≤
u=1m=1 i∈lm (s,a)
tu,h
where the first and second inequalities hold by the fact P (V⋆ V ) P (V⋆ V ) H
for any x [ϕ(K)] (see the monotonicity of the value estimh, as, ta es ih n+ (1 1−4) at nx, dh+ t1 he≤basih c,s b,a ounh d+1− V⋆ 0,h+1 ≤ H),
the last eq∈uality arises from the definition of ψ (s,a) in (92), and the last inequality foll∥owh s+ f1 r∥o∞ m≤(49b)
u,v,h
in Lemma 7.
Similarly, the same facts hold for G(cid:101)−j,m(s,a), which can be derived in the same manner. We omit it for
v,h
conciseness.
Proof of (103c). Consider v = ϕ(j). If v = 1, combing (103a) and (101) directly gives G(cid:101)−j,m(s,a) =
v,h
G (s,a). Then we turn to the case when v > 1 and bound the term of interest in two different cases,
v,h
respectively.
37• When (sm ,am )=(s,a). In this case, invoking the definition in (102) gives
j,h j,h ̸
(H +1)n (s,a)
ψ(cid:101)−j,m(s,a)= tv,h =ψ−j,m(s,a), (116)
v,v,h N (s,a)+(H +1)n (s,a) v,v,h
tv−1,h tv,h
which indicates (see the definition in (101))
G(cid:101)− v,j h,m(s,a)=G v,h(s,a) (117)
• When (sm ,am )=(s,a). In view of (103a) and (101), it holds that:
j,h j,h
|G(cid:101)− v,j h,m(s,a) −G v,h(s,a)
|
(cid:12) (cid:12)
=(cid:12) (cid:12)(ψ(cid:101) v− ,j v, ,m h(s,a) −ψ v,v,h(s,a))P h,s,a(V h⋆ +1−V tv−1,h+1)+(ψ v,v,h(s,a) −ψ(cid:101) v− ,j v, ,m h(s,a))G v−1,h(s,a)(cid:12)
(cid:12)
(cid:12) (cid:12)
=(cid:12) (cid:12)(ψ v,v,h(s,a) −ψ(cid:101) v− ,j v, ,m h(s,a))(G v−1,h(s,a) −P h,s,a(V h⋆ +1−V tv−1,h+1)(cid:12)
(cid:12)
≤(cid:12) (cid:12) (cid:12)ψ v,v,h(s,a) −ψ(cid:101) v− ,j v, ,m h(s,a)(cid:12) (cid:12) (cid:12)max(cid:8) G v−1,h(s,a), ∥P h,s,a ∥1(cid:13) (cid:13)V h⋆ +1−V tv−1,h+1(cid:13) (cid:13) ∞(cid:9)
(i) (cid:12) (cid:12)
≤
H(cid:12) (cid:12)ψ v,v,h(s,a) −ψ(cid:101) v− ,j v, ,m h(s,a)(cid:12)
(cid:12)
(ii) (cid:26) 2H2 (cid:27)
min H, , (118)
≤ N (s,a)
tv,h
where (i) holds by (103b), ∥P h,s,a ∥1 =1, and (cid:13) (cid:13)V h⋆ +1−V tv−1,h+1(cid:13) (cid:13) ∞ ≤H. Here, (ii) can be verified by
(iii)
0
≤
ψ v,v,h(s,a) −ψ(cid:101) v− ,j v, ,m h(s,a)
=
(H +1)n tv,h(s,a) (H +1)(n tv,h(s,a) −I {(s,a)=(sm j,h,am j,h) })
N (s,a)+(H +1)n (s,a) − N (s,a)+(H +1)(n (s,a) I (s,a)=(sm ,am ) )
tv−1,h tv,h tv−1,h tv,h − { j,h j,h }
(H +1)n (s,a) (H +1)(n (s,a) 1)
=
tv,h tv,h
−
N (s,a)+(H +1)n (s,a) − N (s,a)+(H +1)(n (s,a) 1)
tv−1,h tv,h tv−1,h tv,h
−
(H +1)
≤ N (s,a)+(H +1)n (s,a)
tv−1,h tv,h
(cid:26) (cid:27)
2H
min 1, . (119)
≤ N (s,a)
tv,h
where (iii) holds by the fact that x is monotonically increasing with x when a,x>0.
a+x
B.5.3 Proof of Lemma 10
For each j [K], let
∈
Zm := (cid:88) (cid:0)I (s,a)=(sm ,am ) dm(s,a)(cid:1) dπ h⋆ (s,a) P (V⋆ V ). (120)
j,h { j,h j,h }− h Mdavg(s,a) h,s,a h+1− tϕ(j)−1,h+1
(s,a)∈S×A h
(cid:12) (cid:12)
Then, to prove Lemma 10, it suffices to show (cid:12)(cid:80)K (cid:80)M Zm (cid:12)≲σ .
(cid:12) j=1 m=1 j,h(cid:12) aux,2
Since V is fully determined by the events before the j-th episode, E [Zm ] = 0, where we
tϕ(j)−1,h+1 j−1 j,h
denote
E []=E[ (sm′ ,am′ ), Vm′ ].
j−1 · ·|{ i,h i,h i,h+1}i<j,m′∈[M]
Thus, we can apply the Freedman’s inequality as follows:
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)(cid:88)K (cid:88)M Zm (cid:12) (cid:12) (cid:12) (cid:114) 8W log2H + 8 Blog2H ≲(cid:114) H2KSC a⋆ vg + HSC a⋆ vg (121)
(cid:12) j,h(cid:12)≤ δ 3 δ M M
(cid:12)j=1m=1 (cid:12)
38using the following properties:
(cid:32) (cid:33)
Zm 2C a⋆ vgH (cid:88) (1+dπ⋆ (s,π⋆(s))S) 4HSC a⋆ vg =:B (122)
| j,h|≤ M h ≤ M
s∈S
(cid:88)K (cid:88)M
E j−1[(Z jm ,h)2]
≤(cid:88)K (cid:88)M
E (sm j,h,am j,h)∼dm
h
 (cid:32) Md dπ
h
a⋆ v( gs (m
j s,h
m,a ,m
j a,h
m)
)P h,s,a(V h⋆ +1−V
tϕ(j)−1,h+1)(cid:33)2

j=1m=1 j=1m=1 h j,h j,h
(cid:88)K (cid:88)M (cid:88) (cid:18) dπ⋆ (s,π⋆(s)) (cid:19)2
H2 dm(s,π⋆(s)) h
≤ h Mdavg(s,π⋆(s))
j=1m=1s∈S h
H2C a⋆
vg
(cid:88)(cid:88)K (cid:18) dπ h⋆ (s,π⋆(s)) (cid:19)
(1+dπ⋆ (s,π⋆(s))S)
(cid:88)M
dm(s,π⋆(s))
≤ M Mdavg(s,π⋆(s)) h h
s∈Sj=1 h m=1
=
H2C a⋆
vg
(cid:88)(cid:88)K
dπ⋆ (s,π⋆(s))(1+dπ⋆ (s,π⋆(s))S)
M h h
s∈Sj=1
2H2KSC⋆
= avg =:W, (123)
M
which follows from that fact 0 ≤∥V h⋆ +1−V tϕ(j)−1,h+1 ∥∞ ≤H and min{dd ππ h ⋆⋆ (( ss ,, ππ ⋆⋆ (( ss )) )) ,1/S} ≤1+dπ h⋆ (s,π⋆(s))S.
h
B.6 Proof of Corollary 1
Note that if T
H7SC a⋆
vg, it always holds that
≍ Mε2
(cid:114)
HSC⋆ T
MT ≳H5SC⋆ and H avg , (124)
avg ≤ M
as long as ε H and ε
H3SC a⋆
vg. Now, we obtain the number of communication rounds of the specified
schedules, pe≤riodic and e≤xponeM ntial synchronization.
(cid:113)
Periodic synchronization. Consider τ HSC a⋆ vgT. Then, since MT ≳ HSC⋆ , the value gap is
bounded as ≍ M avg
(cid:114) (cid:114) (cid:114)
H4SC⋆ H7SC⋆ H3 HSC⋆ T H7SC⋆
V⋆(ρ) Vπ(cid:98)(ρ)≲ avg + avg + avg ≲ avg . (125)
1 − 1 MT MT T M MT
In this case, the number of synchronizations ϕ(K)= (K,τ) is
period
|T |
(cid:115) (cid:115)
(cid:108)K(cid:109) MK MT H2
ϕ(K)= ≲ .
τ H2SC⋆ ≍ H3SC⋆ ≍ ε
avg avg
(cid:113)
Exponential synchronization. Using the fact that MT ≳ HSC⋆ and τ = H HSC a⋆ vgT when
avg 1 ≤ M
ε
H3SC a⋆
vg, the value gap is bounded as
≤ M
(cid:114) (cid:114) (cid:114)
H4SC⋆ H7SC⋆ H3 HSC⋆ T H7SC⋆
V⋆(ρ) Vπ(cid:98)(ρ)≲ avg + avg + avg ≲ avg . (126)
1 − 1 MT MT T M MT
To continue, note that if γ = 2 and τ =H, for any u 1, τ is bounded as
H 1 ≥ u
(cid:16) 1 (cid:17)u−1 (cid:16) 2 (cid:17)u−1
1+ H τ 1+ H,
u
H ≤ ≤ H
39since
(cid:16) 1 (cid:17) (cid:16) 2 (cid:17) (cid:22)(cid:16) 2 (cid:17) (cid:23) (cid:16) 2 (cid:17)
1+ τ 1+ τ 1 τ = 1+ τ 1+ τ
i i i+1 i i
H ≤ H − ≤ H ≤ H
given the fact that τ H for any i 1. Then, considering the minimum number of synchronizations
i
ϕ(K)= (K,γ)
sati≥sfying ≥
exp
|T |
ϕ(K) ϕ(K)
(cid:88) (cid:88) (cid:16) 1 (cid:17)u−1 (cid:16)(cid:16) 1 (cid:17)ϕ(K) (cid:17)
τ H 1+ =H2 1+ 1 K,
u
≥ H H − ≥
u=1 u=1
we obtain
(cid:38) (cid:39)
log( K +1) (cid:16) K (cid:17)
ϕ(K)= H2 1+(1+H)log +1 ≲H (127)
log(1+ 1) ≤ H2
H
because x log(1+x) for any x> 1.
x+1 ≤ −
40