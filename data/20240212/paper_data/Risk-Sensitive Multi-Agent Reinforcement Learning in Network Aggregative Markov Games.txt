Risk-Sensitive Multi-Agent Reinforcement Learning in Network
Aggregative Markov Games
HafezGhaemi HamedKebriaei
UniversityofTehran,SchoolofECE UniversityofTehran,SchoolofECE
hafez.ghaemi@ut.ac.ir kebriaei@ut.ac.ir
AlirezaRamezaniMoghaddam MajidNiliAhamdabadi
UniversityofTehran,SchoolofECE UniversityofTehran,SchoolofECE
a.ramezany@ut.ac.ir mnili@ut.ac.ir
ABSTRACT years,especiallyinspecifictypesofMGs,suchaszero-sumMGs
Classicalmulti-agentreinforcementlearning(MARL)assumesrisk [2,36,41,47,63]andMarkovpotentialgames[14,16,22,27,29].
neutralityandcompleteobjectivityforagents.However,insettings However,therisk-neutralRLobjectiveoftenfallsshortwhenrepre-
whereagentsneedtoconsiderormodelhumaneconomicorsocial sentingagentswithdistinctsubjectivepreferences,suchasinternal
preferences,anotionofriskmustbeincorporatedintotheRLop- cognitivebiasesofthemselvesorofotheragents.Thus,toaddress
timizationproblem.ThiswillbeofgreaterimportanceinMARL thesepreferences,agentsintegrateariskmeasureintotheirRL
whereotherhumanornon-humanagentsareinvolved,possibly objective,usheringintotherealmofrisk-sensitivereinforcement
withtheirownrisk-sensitivepolicies.Inthiswork,weconsider learning(RSRL).Ingeneral,theliteratureonrisk-sensitiveMARL
risk-sensitiveandnon-cooperativeMARLwithcumulativeprospect ismoresparsecomparedtosingle-agentRSRL.Themajorityofthe
theory(CPT),anon-convexriskmeasureandageneralizationof worksthatconsiderrisk-sensitivemulti-agentMDPsareconcerned
coherentmeasuresofrisk.CPTiscapableofexplaininglossaver- notwithanRLsettingbuteitherwiththeoreticallyprovingthe
sioninhumansandtheirtendencytooverestimate/underestimate existenceofMarkovperfectNashequilibria,orfindingtheseequi-
small/largeprobabilities.Weproposeadistributedsampling-based libriausingiterativealgorithmsgivencompleteinformationofthe
actor-critic(AC)algorithmwithCPTriskfornetworkaggregative gameinacentralizedsettingforMDPswithspecificconstraints
Markovgames(NAMGs),whichwecallDistributedNestedCPT- [4,17,18,34,64,65].
AC.Underasetofassumptions,weprovetheconvergenceofthe RiskinRLcanbecategorizedintotwomaintypesbasedonthe
algorithmtoasubjectivenotionofMarkovperfectNashequilib- risk-sensitiveobjective,asdelineatedbyPrashanthandFu[37].
riuminNAMGs.Theexperimentalresultsshowthatsubjective Thefirstcategory,explicitrisks,involvesdirectlyincorporating
CPTpoliciesobtainedbyouralgorithmcanbedifferentfromthe theriskmeasureintotheobjectivefunction.Incontrast,implicit
risk-neutralones,andagentswithahigherlossaversionaremore risksareintegratedbyimposingaconstraintontheRLstochastic
inclinedtosociallyisolatethemselvesinanNAMG.1 optimizationproblem.Notably,inpractice,implicitrisk-sensitive
objectivesareoftentransformedintoexplicitobjectives.Thisis
achievedbyformulatingaLagrangianandcomputingitsgradient
KEYWORDS
toemployalgorithmsfoundedonpolicygradient(PG)methods[37].
Multi-AgentReinforcementLearning,Actor-Critic,Aggregative
WithinthespectrumofimplicitriskmeasuresinRLandMDPs,no-
Games,Risk-Sensitivity,CumulativeProspectTheory
tableexamplesincludevarianceasrisk([38,54,55]insingle-agent
ACMReferenceFormat: RSRL,and[43]inrisk-sensitiveMARL),andchanceconstraints
HafezGhaemi,HamedKebriaei,AlirezaRamezaniMoghaddam,andMajid ([10]insingle-agentRSRL).Ontheotherhand,explicitriskmea-
NiliAhamdabadi..Risk-SensitiveMulti-AgentReinforcementLearningin suresencompassentropicriskmeasurespredicatedonexponential
NetworkAggregativeMarkovGames.InProceedingsofPreprint(preprint).
return([6,15,30]insingle-agentRSRLand[32,51]inrisk-sensitive
ACM,NewYork,NY,USA,10pages.
MARL),coherentriskmeasures,andcumulativeprospecttheory
(CPT).
1 INTRODUCTION
Coherentriskmeasures[3,11],suchasthewell-knowncondi-
Markovgame(MG)isacommonframeworkforstudyingmulti- tionalvalueatrisk(CVaR),meansemi-deviation[48],andspectral
agentsystems(MAS),anditisthemaintheoreticalframeworkfor risk[1],arewidelyusedinthefieldsofeconomyandoperations
multi-agentreinforcementlearning(MARL)[26,49].Inclassical research.TheirapplicationhasalsobeenexploredwithinMDPs
MARL,eachagentisassumedtohavearisk-neutralobjective,i.e., asdynamicriskmeasures.Osogamietal.[33]showedthatrisk-
ittriestomaximizeanotionofexpectedreturnwithouttakinginto sensitiveMDPsgovernedbyMarkovcoherentriskmeasurescanbe
accountsubjectivepreferencesofitselforoftheotheragentsinthe classifiedunderthedomainofrobustMDPs.Subsequently,dynamic
MAS.Risk-neutralMARLinMGshasseengreatadvancesinrecent programmingmethodologieshavebeensuggestedforthistypeof
MDPs[7,45].Buildingontheseworks,PG-basedtechniquesand
1Codeavailableathttps://github.com/hafezgh/risk-sensitive-marl-namg
actor-critic(AC)algorithmshavealsobeendevelopedforRSRLwith
preprint,Risk-SensitiveMulti-AgentReinforcementLearninginNetworkAggregative coherentriskmeasures,asdetailedin[9,21,53,56]forsingle-agent
MarkovGames, RSRL,andin[31,42,67]forrisk-sensitiveMARL.
.
4202
beF
8
]GL.sc[
1v60950.2042:viXraCPTBackground. TheconceptofProspectTheory(PT)emerged thenestedstructure,whereintheCPToperatorisappliedtothe
asanalternativemodeltoexpectedutilitytheory,providingamore cumulativereturnaftereachstep(actiontaken)[23‚Äì25].Animpor-
accuratemodelofhumandecision-makingunderuncertainty[20]. tantadvantageofthisformulationisthatitensurestheexistenceof
ToenhancetheapplicabilityofPT,CumulativeProspectTheory aBellmanoptimalityequation.Recently,Tianetal.[58]extended
(CPT)wassubsequentlyintroduced[60].UnlikePT,CPTapplies thisnestedformulationtoamulti-agentsettingwithagentsthatare
weightingfunctionstocumulativeprobabilities,addressingthem characterizedbyboundedrationalityandoperatingunderquantal
separatelyforpositiveandnegativeoutcomes.Byintegratingthese level-ùëòstrategies[62].Restrictingtheirapproachtodeterministic
probabilityweightingfunctionsandanon-linearutilityfunction, policies,theyproposeacentralizedvalueiterationalgorithmto
CPTsuccessfullyillustratesvaryinghumanattitudestowardspo- determineoptimalrisk-sensitivepoliciesgivenacompletemodel
tentialgainsandlossesagainstasubjectivereferencepoint.Central oftheenvironmentandtherewardfunctions.
toCPTistheideathathumanstypicallyexhibitaversiontolosses, Inthesecondformulation,theCPToperatorisappliedsolely
i.e.,theygenerallytakemoreriskswhenfacingpotentialgains totheagent‚Äôsfinalcumulativereturnattheendofeveryepisode
andtakefewerriskswhenconfrontedwithpotentiallosses.Addi- [19,39].Contrarytothenestedformulation,thisformulationdoes
tionally,CPT‚Äôsframeworkelucidateshumaninclinationstoover- nothaveaBellmanequation.However,itcanbeapproachedfrom
estimatesmallprobabilitiesandunderestimatelargeonesduring astochasticoptimizationperspective,allowingpolicyoptimization
uncertaindecision-making.WhenweconsiderCPTinthecontext throughagradient-basedmethodakintoPGtechniques[19].This
ofeitherstaticordynamicMarkovriskmeasures,itmeetsonly PGmethodhasalsobeenimplementedbyconsideringneuralnet-
twoofthefourrequirementsthatdefineacoherentriskmeasure.A worksforpolicyapproximation[28].Itisimportanttoemphasize
riskmeasure,whenappliedtoarandomvariable(r.v.)representing thattheabsenceoftheBellmanequationinthiscontextnecessi-
potentialoutcomes,isdeemedcoherentifithasthefollowingfour tatespolicyoptimizationexclusivelythroughofflineMonteCarlo
characteristics:convexity,monotonicity,translationinvariance,and samplingconstrainedbyafinitetimehorizon.
positivehomogeneity[3].Amongthese,theCPTriskmeasureonly Todate,nocognitiveresearchhasbeenconductedtoascertain
possessesmonotonicityandpositivehomogeneity,andisneither whichofthetwoCPTRSRLformulationsbestrepresentsthedy-
translationinvariantnorconvex.Thenon-coherentnatureofCPT namicriskbehaviorexhibitedbyhumans,whetherinsingle-agent
makesitmorechallengingtoworkwithmathematically.CPTcan ormulti-agentenvironments.Nonetheless,thefollowingcanbe
beseenasageneralizationofcoherentriskmeasures,i.e.,byappro- saidaboutthetwoformulations:
priateselectionofCPTprobabilityweightingfunctions,onecan
‚Ä¢ ThenestedformulationbenefitsfromthepresenceofaBell-
derivevariouscoherentriskmeasureformulations[19,25].
manequation,enablingtheuseofonlineactor-criticalgo-
Contributions. Inthiswork,weconsiderrisk-sensitiveMARL rithmsandarecursivelydefinedvaluefunction.Thisadvan-
with CPT risk measure in network aggregative Markov games tageisabsentinthenon-nestedformulation,whereitisonly
(NAMGs),andproposeadistributedactor-criticalgorithmtofind plausibletousePGtechniquesusingofflineMonteCarlo
risk-sensitivepoliciesforeachagent.Wederiveapolicygradient sampling.
theoremforCPTMARLbasedonasubjectivesteady-statedistri- ‚Ä¢ Inbothformulations,duetothesubstitutionoftheexpecta-
butionoftheMDPfromeachagent‚Äôsprespective,andprovidea tionoperatorwiththenon-linearCPToperator,itispossible
sampling-basedapproachtoestimatethevaluefunctionswithas- fortheoptimalpolicytoexhibitnon-deterministiccharac-
ymptoticconsistency.SinceCPTisageneralizationofcoherent teristics[19,24]eveninsingle-agentRL.
riskmeasures,ourPGtheoremgeneralizesthepreviousPGworks ‚Ä¢ Thenon-nestedformulationalignswellwithfinite-horizon
forstaticanddynamiccoherentriskmeasures[9,53].Underaset episodictaskswheretheagentisrewardedattheendofeach
ofassumptions,weprovetheconvergenceofouralgorithmtoa episode.However,itsapplicabilityislimitedwhenconsider-
subjectiveandrisk-sensitivenotionofMarkovperfectNashequilib- inginfinite-horizontasks.Conversely,thenestedformula-
rium(MPNE)whichweshowisuniquegiventheaforementioned tionanditsBellmanequationaresuitablefortaskswhere
assumptions.Experimentally,wealsodemonstratethatahigher theagentisrewardedateverytimestep.
lossaversioncanmakeagentsmoreconservativeandincreasetheir ‚Ä¢ In scenarios without complete information of the model,
tendencyforsocialisolationinanNAMG. therewardfunction,orthepoliciesofotheragents,both
formulationsnecessitateastrategyforestimatingtheCPT
Remark1. (Application)Apotentialapplicationoftheproposed
valuegiventhatwehaveaccesstoasimulatoroftheMDPor
framework is calculating CPT risk-sensitive policies of human
alargeenoughexperiencedictionary(replaybuffer).Suchan
agentsinreal-worldsettings,suchasdrivingscenariosorfinan-
estimationtechniquetailoredforthenon-nestedformulation
cialmarkets,thatcanbemodeledbyNAMGs.Subsequently,these
hasbeenintroducedbyJieetal.[19].
policiescanservedualpurposes:guidingagentstowardsstrategies
optimizedfortheirindividualpreferencesorfacilitatingsocialor Remark2. (Motivation)Giventheaboveconsiderations,inrisk-
economicchangesintheenvironmenttosteeragentsinadirection sensitiveMARLwithCPT,inasettingwheretheagentsinteract
thatalignswithdesiredoutcomes. inanonlineinfinite-horizonMDPwithlimitedinformationabout
otheragents‚Äôpolicies,thenestedCPTformulationistheviable
2 RELATEDWORKS optiontoadopt.Duetothepossibilityofnon-deterministicopti-
InthecontextofMarkovriskmeasuresinMDPs,CPTisarticu- malpoliciesforeachagentinMARL,weoptforactor-criticstyle
latedthroughtwodistinctformulations.Thefirstformulationis algorithmsusingparameterizedpolicies.Furthermore,weconsiderNAMGsasourMARLframeworkduetothreereasons.First,be- persontopersonbasedontheirlevelofrisk-aversionandindividual
causetheyareinherentlysuitedtodistributedalgorithms.Second, characteristics.Theconventionalrepresentationsofweightingand
givenasetofassumptions,NAMG,anditsrisk-sensitiveversion utilityfunctionsgivenasetofsubjectiveparametersareplottedin
canbeshowntohaveauniqueMarkovperfectNashequilibrium Figures1and2.
whichouralgorithmconvergesto.Andthird,becauseNAMGsare
asuitableframeworktoshowthetangibleeffectoflossaversionin
human-likeagentsandontheirtendencyforsocialisolationand
conservatism.
3 PRELIMINARIES
3.1 CumulativeProspectTheory
Givenareal-valuedr.v.ùëã withdistributionP(ùëã),areferencepoint
ùë•0,twomonotonicallynon-decreasingweightingfunctions,ùúî+ :
[0,1] ‚Üí [0,1],ùúî‚àí : [0,1] ‚Üí [0,1],utilityfunctionsùë¢+ : R+ ‚Üí
R+,ùë¢‚àí : R‚àí ‚Üí R+,andgivenappropriateintegrabilityassump-
tions,wecandefinetheCPTvalueusingChoquetintegralsas
‚à´ ‚àû
CPT P[ùëã] := ùúî+(P(ùë¢+((ùëã ‚àíùë•0)+) >ùë•))ùëëùë•‚àí
0 (1) Figure1:ConventionalCPTweightingfunctions;ùúî+(ùëù) =
‚à´ ‚àû ùúî‚àí(P(ùë¢‚àí((ùëã ‚àíùë•0)‚àí) >ùë•))ùëëùë•., (ùëùùõæ+(1‚àíùëù ùëùùõæ
)ùõæ)(1/ùõæ)
andùúî‚àí(ùëù)= (ùëùùõø+(1‚àíùëù ùëùùõø
)ùõø)(1/ùõø)
withùõæ =ùõø =0.69.
0
wherewedenote(.)+ =ùëöùëéùë•(0,.)and(.)+ = ‚àíùëöùëñùëõ(0,.).Fora
discreter.v.,wecandefinetheCPTvaluesimilarlyas
ùëõ
CPT P[ùëã] :=‚àëÔ∏Å ùúô+(PùëÉ(ùëã =ùë• ùëñ))ùë¢+(ùë• ùëñ ‚àíùë•0 )
ùëñ=0
(2a)
‚àí1
‚àí‚àëÔ∏Å ùúô‚àí(P(ùëã =ùë• ùëñ))ùë¢‚àí(ùë•
ùëñ
‚àíùë•0 ),
‚àíùëö
ùëõ
ùúô+(P(ùëã =ùë• ùëñ))=ùúî+(cid:169) (cid:173)‚àëÔ∏Å P(ùëã =ùë• ùëó)(cid:170)
(cid:174)
ùëó=ùëñ
(cid:171) (cid:172) (2b)
ùëõ
‚àíùúî+(cid:169) (cid:173)‚àëÔ∏Å P(ùëã =ùë• ùëó))(cid:170) (cid:174),
ùëó=ùëñ+1
(cid:171) (cid:172)
ùëñ
ùúô‚àí(P(ùëã =ùë• ùëñ))=ùúî‚àí(cid:169)
(cid:173)
‚àëÔ∏Å P(ùëã =ùë• ùëó)(cid:170)
(cid:174) Figure2:ConventionalCPTutilityfunctions;Theplotshows
(cid:171)ùëó=‚àíùëö (cid:172) (2c) ùë¢+(ùë•) =ùë•ùõº forùë• ‚â• 0,and‚àíùë¢‚àí(ùë•) =‚àíùúÜ(‚àíùë•)ùõΩ)forùë• < 0,with
‚àíùúî‚àí(cid:169)
(cid:173)
‚àëÔ∏Åùëñ‚àí1
P(ùëã =ùë• ùëó)(cid:170) (cid:174),
ùõº =ùõΩ =0.65andùúÜ=2.6.
ùëó=‚àíùëö
(cid:171) (cid:172)
whereùë•0 servesasareferencepointthatseparatesgainsand
3.2 NetworkAggregativeMarkovGames
losses.Withoutlossofgenerality,weassumeùë•0=0throughoutthis
Throughoutthispaper,weassumethatagentsareinteractingin
paper.ConventionalrepresentationsofCPTweightingfunctions
includeùúî+(ùëù) = (ùëùùõæ+(1‚àíùëù ùëùùõæ
)ùõæ)(1/ùõæ)
andùúî‚àí(ùëù) = (ùëùùõø+(1‚àíùëù ùëùùõø
)ùõø)(1/ùõø)
a inn fie nr ig teo -d hi oc rn ize otw no cr rk itea rg ig or ne .g Aat niv Ne AM Mar Gko wv ig tham ùëÅew pli at yh ea rsdi is sc ao nun Mte Gd
[60], or ùúî+(ùëù) = exp(‚àí(‚àíùëôùëõùëù)ùõæ) and ùúî‚àí(ùëù) = exp(‚àí(‚àíùëôùëõùëù)ùõø) denotedbyùëÄ =(ùëÜ,ùëÅ,ùê¥,ùëÖ,ùëÉ,ùê∫,ùõæ,ùëù ùë†0),whereùëÜisthestatespace,
[40]. Note that by setting ùõø andùõæ equal to 1, the definition of ùê¥=ùê¥1√ó...√óùê¥
ùëÅ
isthejointactionspace;ùëÖ:ùëÜ√óùê¥√óùëÜ ‚ÜíRùëÅ isa
expectedutilityùê∏ P[ùë¢(ùëã)]isrecoveredwhichshowsthatCPTisa jointrewardfunctionboundedin[‚àíùëÖ ùëöùëéùë•,ùëÖ ùëöùëéùë•]whereùëÖ
ùëöùëéùë•
>0;
generalizationofexpectedutilitytheory.Furthermore,ùë¢+andùë¢‚àí ùëÉ(.|ùë†,ùëé) istheMDPtransitionprobabilitydistribution; G(N,E)
areusuallyconcavefunctions(‚àíùë¢‚àí isconvex)toreflectthehigher isagraphwithedgesetEonwhicheachagentinteractswithits
sensitivityofhumanstowardslossescomparedtogains[20].As neighbors;ùõæistheMDP‚Äôsdiscountfactor;andùëù istheinitialstate
ùë†0
aresult,theutilityfunctioncanhaveanalyticalrepresentations distribution.InNAMG,foreachagentùëõ,therewardfunctionis
ùë¢+(ùë•)=ùë•ùõº ifùë• ‚â•0,andùë¢‚àí(ùë•)=ùúÜ(‚àíùë•)ùõΩ ifùë• <0.Theparameters afunctionofitsownactionandanaggregativefunctionofother
ùõæ,ùõø,ùõº,ùõΩ,andùúÜaresubjectivemodelparametersthatcandifferfrom agents‚Äôactions,Therefore,byobservingtheactionsofneighboringagentsand
ùëÖùëñ (ùë†,ùëéùëñ,ùëé‚àíùëñ )=ùëÖùëñ (ùë†,ùëéùëñ,ùúéùëñ (ùëé‚àíùëñ )), (3) calculatingtheaggregativetermùúé‚àíùëñ,agentùëñcantreatP(ùúé‚àíùëñ|ùë†)as
wherewehave, aprobabilitydistributionsimilartothetransitionprobabilitiesfor
eachstate.
ùúéùëñ (ùëé‚àíùëñ )= ‚àëÔ∏Å ùúî ùëñùëóùëéùëó, (4)
ùëó‚ààN\ùëñ 4 DISTRIBUTEDNESTEDCPTPOLICY
whereùúî aretheedgeweightsofthecommunicationgraphG, GRADIENT
withùë§ denotingtheweightoftheedgefrom ùëó toùëñ.Therefore,
ùëñùëó Inthissection,wederiveagradientexpressionfortheMarkov
giventhegraph,andbyobservingitsneighbors‚Äôactions,agentùëñis
dynamicCPTriskmeasureinNAMGs,representedbythegradient
abletocalculateùúéùëñ(ùëé‚àíùëñ).Figure3showsaschematicofanNAMG.
oftheinitialstate‚ÄôsvaluefunctioninanergodicCPTrisk-sensitive
NAMG,‚àáùëâ ùúãùëñ ùúÉ(ùë†0).BeforepresentingthePGtheorem,westatethe
followingassumption,
Markov Environment Assumption1. Theweightfunctionsùë§¬±aredoubledifferentiable,
ùëé!" ùëé#" ùëé$" ùëé%" ùëé&" andthederivativesùë§ ¬±‚Ä≤ areLipschitzcontinuouswithcommoncon-
ùëÖ$"
ùúé$"
s nt oa tn edtùêø b. yF ùë¢u ‚Ä≤rt )h fe or rm ao llre a, gt eh ne tsu .tilityfunctionsùë¢¬±aredifferentiable(de-
ùëÖ!" P3 ùëÖ&" ¬±
ùúé!" P1 P5 ùúé&" Theaboveassumptionmayseemstrictatfirst.However,con-
ventionalformsoftheCPTutilityfunctions,specificallyùë¢+(ùë•)=
ùëÖ#" ùëÖ%" ùë•ùõº and‚àíùë¢‚àí(ùë•) = ‚àíùúÜ(‚àíùë•)ùõΩ,alongwiththeweightingfunctions
P2 P4 ùúî+(ùëù) =
ùëùùõæ
andùúî‚àí(ùëù) =
ùëùùõø
,depicted
ùúé#"
ùúé%"
inFigures(ùëù 1ùõæ a+ n(1 d‚àí 2ùëù ,) sùõæ a) t( i1 s/ùõæ fy) thisassumptio( nùëù .ùõø+(1‚àíùëù)ùõø)(1/ùõø)
Theorem1. (NestedCPTPolicyGradient)
Figure3:AnetworkaggregativeMarkovgame GivenAssumption1,thegradientoftheCPTreturnforagentùëñ,
Previously,invariousdomains,suchasresourceallocation[12],
ùëâ ùúãùëñ ùúÉ(ùë†0),withrespecttothepolicyparameterùúÉùëñ is
socialnetworks[66],electricalmicrogrids[57],andpowersystems
[ n1 a3 m], icei nth ete wr osi rn kg al ge- gs rt ea gt ae tn ive etw gao mrk esag hg ar ve eg ba eti ev ne sg tuam diees d( iN nA riG sks -) n, eo ur td ray l- ‚àáùëâ ùúãùëñ ùúÉ(ùë†0)‚àùE
ùúá ùëêùëñ ùëùùë°(ùë†)
(cid:20) ‚àëÔ∏Å ùúï(ùúãùëñ(ùëéùëñ|ùë†)P(ùúéùúïùúô
‚àíùëñ|ùë†)P(ùë†‚Ä≤|ùë†,ùëé))
ùëé,ùë†‚Ä≤ ùúÉ
setting.Furthermore,mostofthetheoreticalworksinthisdomain
(cid:21)
havefocusedonstudyingconvergencetotheMarkovperfectNash P(ùúé‚àíùëñ |ùë†)P(ùë†‚Ä≤|ùë†,ùëé)(‚àáùúã ùúÉùëñ(ùëéùëñ |ùë†))ùë¢(ùëÖùëñ (ùë†,ùëéùëñ,ùúé‚àíùëñ,ùë†‚Ä≤)+ùõæùëâ ùúãùëñ ùúÉ(ùë†‚Ä≤)) ,
orStackelbergequilibriuminsingle-stateordynamicNAGswith
(7)
quadraticcost/rewardfunctionsthatensuretheuniquenessofthe
equilibrium[8,35,46,50].Inthispaper,forthefirsttime,wecon- where,ùúôandùë¢representtheCPTcumulativeweightingandutility
siderrisk-sensitiveNAMGs. functionsoftheagentfrom(2)(superscriptùëñisdropped).Thedistri-
butionùúáùëñ isasubjectivesteady-stateprobabilitydistributionofthe
ùëêùëùùë°
3.3 CPTRisk-SensitiveMARLObjectivein
NAMGs
MDPinwhichùúá ùëêùëñ ùëùùë°(ùë†)=
(cid:205)
ùë†ùúÇ ‚ààùëÜùëêùëñ ùëù ùúÇùë° ùëêùëñ( ùëùùë† ùë°) (ùë†),whereùúÇùëñ CPT(ùë†)isasubjective
Usingthenestedformulation,theobjectiveoftherisk-sensitive measureoftimespentineachstateandcanbeobtainedbysolving
agentùëñinanNAMGmax ùêΩùúãùëñ,ùúã‚àíùëñ willbeequivalentto thefollowingsystemoflinearequations,
ùúãùëñ
m ùúãa ùëñxùëâ ùúãùëñ (ùë†0)=m ùúãa ùëñxCPT ùúã(ùë†0,.)√óP(.|ùë†0,ùëé0) (cid:2)ùëÖùëñ (ùë† ùúè,ùëé ùúè)+... ùúÇ ùëêùëñ ùëùùë°(ùë†)=ùëù0(ùë†)+‚àëÔ∏Å ùúÇ ùëêùëñ ùëùùë°(ùë†¬Ø)‚àëÔ∏Å ùúô(ùúã(ùëéùëñ |ùë†¬Ø)P(ùúé‚àíùëñ |ùë†¬Ø)P(ùë†|ùë†¬Ø,ùëé)) ùúïùëâùúïùë¢
ùëñ
(ùë†),
+ùõæùúèCPT
ùúã(ùë†ùúè,.)√óP(.|ùë†ùúè,ùëéùúè)
(cid:2)ùëÖùëñ (ùë† ùúè,ùëé ùúè)+...(cid:3)(cid:3) ùë†¬Ø‚ààùëÜ ùëé‚ààùê¥ ùúã (ùúÉ
8)
=m ùúãa ùëñxCPT ùúã(ùë†0,.)√óP(.|ùë†0,ùëé0) (cid:2)ùëÖùëñ (ùë†0,ùëé0)+ùõæùëâ ùúãùëñ (ùë†1)(cid:3), insw tah te er ùë†e ,ùëù a0 n( dùë†) ùë¢d ae nn dot ùúôes at rh ee thp ero ub ta ilb iti ylit cy umth ua lt at th ive eM wa er igk hov tinc gha fuin ncs tt ia or nts
s
(5)
ofagentùëñ from(2)(ùë¢¬± andùúô¬± arechosenaccordingtothesignof
tivew lyh .e Ure siùúã ng(ùë† ùë° th,. e) p= roùúã pùëñ e( rùë† tùë° i, e. s) o√ó fùúã N‚àí Aùëñ M(ùë† ùë° G,. s) ,aa nn dd cùëé oùë° n= sid(ùëé eùëñ ùë° r, inùëé gùë°‚àíùëñ P), (ùúére 0‚àís ùëñp |ùë†e 0c )- ùëÖùëñ(ùë†¬Ø,ùëé,ùë†)+ùõæùëâ ùúãùëñ ùúÉ(ùë†)).
astheprobabilitythatùúé 0‚àíùëñ occursatstateùë†0 foragentùëñ,wecan
Proof. Consideringagentùëñ,wedropthesubscriptùëñanddenote
rewritetheobjectiveas ùëÖùëñ(ùë† ùë°,ùëé ùë°,ùë† ùë°+1)asùëÖ
ùë°
andùëâ ùúãùëñ ùúÉ(ùë† ùë°)asùëâ ùúãùúÉ(ùë† ùë°).Furthermore,ùúã ùúÉ(ùë†,ùëé)
used below, whereùëé is equivalent to (ùëéùëñ,ùëé‚àíùëñ), is the more gen-
m ùúãa ùëñxùëâ ùúãùëñ(ùë†0)=m ùúãa ùëñxCPT ùúãùëñ(ùëéùëñ 0|ùë†0)√óP(ùúé 0‚àíùëñ|ùë†0)√óP(ùë†1|ùë†0,ùëé0) (cid:2)ùëÖùëñ(ùë†0,ùëé0)+ùõæùëâ ùúãùëñ(ùë†1)(cid:3). eralcaseofjointpoliciesinMarkovgames,whichencompasses
(6) ùúã ùúÉ(ùëéùëñ|ùë†)P(ùúé‚àíùëñ|ùë†)inanNAMG.ThegradientoftheCPTrisk-sensitivereturnconsideringitsrecursivedefinitioncanbewrittenas [52],Section9.2),ùúÇ(ùë†)canbecalculatedbysolvingthefollowing
‚àáùëâùúãùúÉ(ùë†0)= systemoflinearequations,
(cid:34) (cid:35)
‚àëÔ∏Å
‚àá
ùëé0,ùë†1
(cid:20)ùúô(ùúã(ùë†0,ùëé0)P(ùë†1|ùë†0,ùëé0))ùë¢(ùëÖ0+ùëâùúãùúÉ(ùë†1))
ùúÇ ùëêùëùùë°(ùë†)=ùëù0(ùë†)+
ùë†‚àëÔ∏Å
¬Ø‚ààùëÜùúÇ ùëêùëùùë°(ùë†¬Ø)
ùëé‚àëÔ∏Å
‚ààùê¥ùúô(ùúã(ùë†¬Ø,ùëé)P(ùë†|ùë†¬Ø,ùëé))
ùúïùëâùúïùë¢
ùúãùúÉ
(ùë†), (13)
‚àëÔ∏Å
= ‚àáùúô(ùúã(ùë†0,ùëé0)P(ùë†1|ùë†0,ùëé0))ùë¢(ùëÖ0+ùëâùúãùúÉ(ùë†1)) where ùëù0 is the probability distribution of the starting state.
ùëé0,ùë†1
Therefore,wecanwrite(9)as
(cid:21)
+ùúô(P(ùë†1|ùë†0,ùëé0))‚àáùë¢(ùëÖ0+ùëâùúãùúÉ(ùë†1))
‚àáùëâ ùúãùúÉ(ùë†0)=‚àëÔ∏Å ùúÇ ùëêùëùùë°(ùë†)‚àëÔ∏Å ‚àáùúô(ùúã(ùë†,ùëé)P(ùë†‚Ä≤|ùë†,ùëé))
‚àëÔ∏Å (cid:34) ùë† ùëé,ùë†‚Ä≤ (14)
=
ùëé0,ùë†1
‚àáùúô(ùúã(ùë†0,ùëé0)P(ùë†1|ùë†0,ùëé0))ùë¢(ùëÖ0+ùëâùúãùúÉ(ùë†1)) ùë¢(ùëÖ(ùë†,ùëé,ùë†‚Ä≤)+ùëâ ùúãùúÉ(ùë†‚Ä≤)).
+ùúô(ùúã(ùë†0,ùëé0)P(ùë†1|ùë†0,ùëé0)) ùúïùëâùúãùúï ùúÉùë¢
(ùë†1) (9)
AsùúÇ ùëêùëùùë°(ùë†)ispositiveforallùë†,wecandefineùúá ùëêùëùùë°(ùë†)=
(cid:205)
ùë†ùúÇ ‚ààùëÜùëêùëù ùúÇùë° ùëê( ùëùùë† ùë°)
(ùë†)
(cid:20) (cid:21)(cid:35) asthesubjectivelimiting(steady-state)distributionoftheCPTrisk-
‚àëÔ∏Å
‚àá ùúô(ùúã(ùë†1,ùëé1)P(ùë†2|ùë†1,ùëé1))ùë¢(ùëÖ1+ùëâùúãùúÉ(ùë†2)) sensitiveMDP,andtherefore,wehave
ùëé1,ùë†2
(cid:34)
= ùëé‚àëÔ∏Å
0,ùë†1
‚àáùúô(ùúã(ùë†0,ùëé0)P(ùë†1|ùë†0,ùëé ùúï0 ùë¢))ùë¢(ùëÖ0+ùëâùúãùúÉ(ùë†1)) ‚àáùëâ ùúãùúÉ(ùë†0)‚àùE ùúáùëêùëùùë°(ùë†) (cid:20) ‚àëÔ∏Å ùëé,ùë†‚Ä≤‚àáùúô(ùúã(ùë†,ùëé)P(ùë†‚Ä≤|ùë†,ùëé))
(15)
+ùúô(ùúã(ùë†0,ùëé0)P(ùë†1|ùë†0,ùëé0))
ùúïùëâùúãùúÉ(ùë†1) ùë¢(ùëÖ(ùë†,ùëé,ùë†‚Ä≤)+ùëâ
ùúãùúÉ(ùë†‚Ä≤))(cid:21)
.
(cid:20)
‚àëÔ∏Å
‚àáùúô(ùúã(ùë†1,ùëé1)P(ùë†2|ùë†1,ùëé1))ùë¢(ùëÖ1+ùëâùúãùúÉ(ùë†2))
Itisinterestingtocompare(15)withthesimilarexpressionin
ùëé1,ùë†2
(cid:20) (cid:21)
+ùúô(ùúã(ùë†1,ùëé1)P(ùë†2|ùë†1,ùëé1))‚àáùë¢(ùëÖ1+ùëâùúãùúÉ(ùë†2))(cid:21)(cid:35)
.
risk-neutralpolicygradienttheorem,E
ùúá(ùë†)
(cid:205) ùëé‚àáùúã(ùë†,ùëé)ùëÑ(ùë†,ùëé) .
Duetothenon-linearCPToperator(comparedtothelinearex-
Wedefine pectation operator), the policy is entangled with the transition
‚àëÔ∏Å ùúïùë¢ probabilitiesinsidethegradientofthecumulativeweightingfunc-
ùê∑ùëÉùëü(ùë†0‚Üíùë†1,ùëò=1,ùúã ùúÉ):=
ùëé0
ùúô(ùúã(ùë†0,ùëé0)P(ùë†1|ùë†0,ùëé0))
ùúïùëâùúãùúÉ(ùë†1)
(10)
tion,andtherefore,intherisk-sensitivecase,itisnotpossibleto
defineastand-aloneQ-functionasafunctionofstateandactionto
asthesubjective(distorted)visitationprobabilityofùë†1rightaf-
measurethequalityofanactioninagivenstate.AsnotedbyLin
terùë†0 followingpolicyùúã ùúÉ.Notethatsinceùë¢ isanon-decreasing
[23],thiscomplicationhastheconsequencethattheoptimalrisk-
functionwithpositivederivativeseverywhereandùúôisafunction
sensitivepolicyeveninthesingle-agentsettingcanbestochastic.
thatmaps[0,1]to[0,1],thistermisalwayspositive.Bydefining
Tofurtherexpandtheaboveexpression,wecanusethechainrule
ùê∑ùëÉùëü(ùë†0‚Üíùë†0,0,ùúã ùúÉ):=1,byrecursion,wecanwritethesubjective
ofcalculusandwrite,
probabilityofvisitingstateùë† ùëò+1afterùëò+1steps,startingfromùë†0
andfollowingpolicyùúã as
ùúÉ
(cid:20) ‚àëÔ∏Å ùúïùúô
ùê∑ùëÉùëü(ùë†0‚Üíùë† ùëò+1,ùëò+1,ùúã ùúÉ)=
‚àáùëâ ùúãùúÉ(ùë†0)‚àùE
ùúáùëêùëùùë°(ùë†)
ùëé,ùë†‚Ä≤
ùúï(ùúã ùúÉ(ùë†,ùëé)P(ùë†‚Ä≤|ùë†,ùëé))
(16)
‚àëÔ∏Å
ùë†ùëò
ùê∑ùëÉùëü(ùë†0‚Üíùë† ùëò,ùëò,ùúã ùúÉ)ùê∑ùëÉùëü(ùë† ùëò ‚Üíùë† ùëò+1,1,ùúã ùúÉ) (11)
P(ùë†‚Ä≤|ùë†,ùëé)(‚àáùúã ùúÉ(ùë†,ùëé))ùë¢(ùëÖ(ùë†,ùëé,ùë†‚Ä≤)+ùõæùëâ
ùúãùúÉ(ùë†‚Ä≤))(cid:21)
.
Therefore,afterrepeatedunrolling,wecanwrite(9)as
ThisisthegeneralcaseofPGinCPTrisk-sensitiveMARL.Given
‚àëÔ∏Å(cid:32)(cid:18) ‚àëÔ∏Å‚àû (cid:19) theaggregativetermùúé‚àíùëñ inNAMGs,wecanrewritethisequation
‚àáùëâ ùúãùúÉ(ùë†0)= ùê∑ùëÉùëü(ùë†0‚Üíùë†,ùëò,ùúã ùúÉ)
as(7).
ùë† ùëò=0
(12) ‚ñ°
(cid:33)
‚àëÔ∏Å ‚àáùúô(ùúã(ùë†,ùëé)P(ùë†‚Ä≤|ùë†,ùëé))ùë¢(ùëÖ(ùë†,ùëé,ùë†‚Ä≤)+ùëâ ùúãùúÉ(ùë†‚Ä≤)) .
Wenowprovideanalgorithmtoestimatetheabovegradientus-
ùëé,ùë†‚Ä≤
ingsamplesfromasimulatoroftheenvironmentoralargeenough
Similartoarisk-neutralMDP,givenùúã andthestatevaluefunc-
ùúÉ experiencedictionary.Thisapproximationschemewhichislater
tioncorresponding tothis policy,thefunctionùê∑ùëÉùëü isan inher-
usedtoalsoestimatethevaluefunctionisAlgorithm1isproposed
ent property of the CPT risk-sensitive MDP (this function can
byJieetal.[19]toestimatetheCPTvalueofanr.v.,ùëã,usingsam-
be compared with the function ùëÉùëü in the proof of risk-neutral
plesfromitsdistribution.ThefollowingAssumption(A2in[19])is
policygradienttheoremin[52],Section13.2).Therefore,welet
neededtoguaranteetheasymptoticconsistencyofthisestimation
ùúÇ ùëêùëùùë°(ùë†) := (cid:205) ùëò‚àû =0ùê∑ùëÉùëü(ùë†0 ‚Üí ùë†,ùëò,ùúã ùúÉ),whichcanbeconsidereda
algorithm.
subjective(perceived)measureoftimethattheCPTrisk-sensitive
agentspendsinstateùë†whenfollowingpolicyùúã ùúÉ andstartingfrom Assumption2. Theutilityfunctionsùë¢+andùë¢‚àíarecontinuousand
stateùë†0.Inasimilarfashionasinrisk-neutralergodicMDPs(see non-decreasingontheirsupportR+andR‚àí,respectively.Algorithm1CPTValueEstimation
1: R soe rq teu dir ine: asS ca em np dl ie ns gùëã or1 d,. e.. r, .ùëã ùëõ from the distribution of r.v. ùëã, ùëá ùëêùëùùë°ùëâ ùúãùúÉ(ùë†)=CPT
ùúãùúÉ(.|ùë†)√óP(.|ùë†,ùëé)
(cid:2)ùëÖ(ùë†,ùëé,ùë†‚Ä≤)+ùõæùëâ ùúãùúÉ(ùë†‚Ä≤)(cid:3) (17)
2: Let Thefollowingassumptionisneededtoensurethattheoperator
ùúåÀÜ ùëê+
ùëùùë°
:=‚àëÔ∏Å
ùëñ=ùëõ
1ùë¢+(ùëã
ùëñ)(cid:18) ùúî+(cid:18)ùëõ+ ùëõ1‚àíùëñ(cid:19) ‚àíùúî+(cid:18)ùëõ ùëõ‚àíùëñ(cid:19)(cid:19)
,
Ain s( s1 u7 m)i ps ta iosu np 3-n .o Tr hm ec uo tn ilt itr yac ft uio nn ct.
ionsùë¢+andùë¢‚àí areinvertible(de-
ùúåÀÜ ùëê‚àí ùëùùë° :=‚àëÔ∏Å ùëñ=ùëõ 1ùë¢‚àí(ùëã ùëñ)(cid:18) ùúî‚àí(cid:18) ùëõùëñ(cid:19) ‚àíùúî‚àí(cid:18)ùëñ‚àí ùëõ1(cid:19)(cid:19) . n w
‚à´
0o e ùõæt ùëêe hd ùúîavb +ey (ùë¢ Pùë¢ + (+‚àí ùëã(1 0a ) <n =d ùë•ùë¢ )ùë¢ )‚àí‚àí‚àí ùë¢1 ( +‚Ä≤0) ()a ùõæn ùëê=d ‚àí0d ùë•.i )Fff ùëëue ùë•rr te +hn ‚à´et 0ri ùõæa , ùëêb th ùúîle e ‚àír( ed (e e Pn x (o i ùëãst te sd >ùõΩb ùë•y ‚àà )ùë¢ )( ùë¢+‚Ä≤ 0
‚Ä≤
‚àí,a 1 (n ) ùë•d )sùë¢ ùëëu‚Ä≤ ‚àí ùë•ch) ‚â§, ta hn ùõΩad ùëêt
holdsforanyùëê >0andanynon-negativereal-valuedr.v.ùëã,whereùõæ
3: ReturnùúåÀÜ ùëêùëùùë° =ùúåÀÜ ùëê+ ùëùùë° ‚àíùúåÀÜ ùëê‚àí ùëùùë°. isthediscountfactoroftheMDP.
SimilartoAssumptions1and2,theaboveassumptioncanalso
beverifiedtoholdfortypicalanalyticalformsofùúî¬± andùë¢¬± in
The above assumption also holds for conventional forms of
Figures2and1asshownbyLinetal.[24].Underthisassumption,
weightingandutilityfunctionsinFigures1and2[19].GivenAs-
basedonTheorem6in[24],theùëáùê∑ operator(17)isasup-norm
sumptions1and2,Proposition4in[19]isverifiedandforagivenr.v.
contractiononaBanachspacedefinedovertheMDP‚Äôsstatespace
ùëã,Algorithm1isguaranteedtohaveasymptoticconsistency,i.e.,
thatincludesallpossiblestatevaluefunctionsùëâ .Therefore,for
ùëõit ,c ao pn pv re or ag ce hs esto inC fiP nT it[ yùëã .]asymptoticallyasthenumberofsamples, everyùëâ ùúãùúÉ,ùëâ¬Ø ùúãùúÉ ‚ààùêµ(ùëÜ),thereexistsùõº ‚àà (0,1)sucùúã hùúÉ that
Gradientestimation. Tohaveaestimateofthegradientin(7), ‚à•ùëá ùëêùëùùë°ùëâ ùúãùúÉ ‚àíùëá ùëêùëùùë°ùëâ¬Ø ùúãùúÉ‚à•‚àû ‚â§ùõº‚à•ùëâ ùúãùúÉ ‚àíùëâ¬Ø ùúãùúÉ‚à•‚àû (18)
weneedestimatesofCPTvaluescorrespondingto
ùúô(ùúã(ùëéùëñ|ùë†¬Ø)P(ùúé‚àíùëñ|ùë†¬Ø)P(ùë†|ùë†¬Ø,ùëé))and ùúï(ùúãùúÉ(ùëéùëñ|ùë†)P(ùúéùúïùúô
‚àíùëñ|ùë†¬Ø)P(ùë†‚Ä≤|ùë†,ùëé))
(which R coe nm tra ar ck tio3 n. o( fA tp hp eli ùëáca ùê∑bi oli pty ero af toli rn (e 7a )r hf au sn oct nio lyn ba ep ep nro vx aim lida at ti eo dn) foT rh ae
wedenotebyùúô‚Ä≤(cid:0)ùúã ùúÉ(ùëéùëñ|ùë†)P(ùúé‚àíùëñ|ùë†¬Ø)P(ùë†‚Ä≤|ùë†,ùëé)(cid:1)).NotethatAssump- tabular representation of the state-value function [24]. We also
tion1statesthatùúî‚Ä≤ areLipschitzandtherefore,theycanbeusedas assessedthepossibilityofapproximatingthestatevaluefunction
¬±
independentCPTweightingfunctionswithcorrespondingcumula- usinglinearfunctionsforscalinguptheproposedactor-criticalgo-
tiveweightingfunctionsùúô‚Ä≤.Giventhetransitionprobabilitiesand rithmtolargeorcontinuousstatespaces.Thetraditionalproofof
¬±
repeateddistributedsamplingofrewardsandtransitionsbyagents convergenceforaùëáùê∑criticwithlinearfunctionapproximationre-
fromtheenvironmentortheexperiencedictionaries,thetermin quiresthecontractionofthisoperatorwithrespecttotheùêø2norm
bracketscorrespondingtoeachstatecanbeestimatedusingAlgo- definedoverthesteady-statedistributionoftheMDP(Lemma4
rithm(1).Furthermore,usingthesesamplesandsolvingalinear inTsitsiklisandVanRoy[59]).However,viacounterexample,it
systemofequationsresultingfrom(13),thesubjectivesteady-state canbeseenthatthispropertydoesnotnecessarilyholdfortheùëáùê∑
distributionùúá ùëêùëùùë°(ùë†)canbefound.Wenotethatthisestimationalgo- operator(7).
rithmismodel-basedandrequirestransitionprobabilities,however,
itdoesnotassumeanyknowledgeoftherewardfunctionorthe Remark4. Thepreviousremarkandthefactthatwewererequired
tolimitourselvestotabularrepresentationsimpliesthatmathemat-
policiesofotheragents,andisthereforeprivacy-preserving.Having
icalpropertiesofCPT-sensitiveMDPsdonotallowthemtobelong
apolicygradienttheoremandacorrespondinggradientapprox-
tothefamilyofrobustMDPs[33]andenjoypropertiessuchas
imationscheme,wecannowdevelopourdistributedactor-critic
linearapproximationforthestatevaluefunctionandaconvenient
algorithm.
gradientestimationschemeaswithcoherentriskmeasures[53].
Itwouldbeinterestingtostudyandlookatthislimitationfrom
5 DISTRIBUTEDNESTEDCPTACTOR-CRITIC
acognitiveperspectiveandtoseewhetherdealingwithdynamic
Algorithm(2)laysoutthepseudocodeforDistributedNestedCPT
CPTrisk-sensitivecontinuouscontroliscognitivelycumbersome
Actor-Critic.Ascanbeseen,thecritic‚Äôsvaluefunctionisestimated
forhumansinbehavioralexperiments.
usingthesamplingstrategyinAlgorithm1,andweusethesam-
plesfromthesimulatorforbootstrapping(byaddingthemtothe
5.2 ConvergenceoftheActor
experiencedictionaryforlateruse).Althoughsamplingfromthe
simulatorforgradientandvaluefunctionapproximationcanbe
Fornotationalsimplicity,wedenoteùëâ ùúãùëñ ùúÉ(ùë†)byùëâ ùëñ(ùúÉ,ùë†).Weprove
theconvergenceofourACalgorithmtoasubjectiveMPNEofthe
computationallyintensive,itcanbecomelesssoaswebuildthe
gameifthefollowingassumptionsaresatisfied.
experiencedictionaryanddoawaywiththesimulator.Wenow
provetheasymptoticconvergenceoftheproposedalgorithm.
Assumption4. Foreachagentùëñ,ùëâ ùëñ(ùúÉ,ùë†)isconvexwithrespectto
ùúÉùëñ.Also,thegradientisuniformlybounded,i.e.,foreachagentùëñthere
5.1 ConvergenceoftheCritic existsùúâ suchthat,
ùëñ
Inordertocalculatethestatevaluefunctioncorrespondingtothe
currentpolicy,wedefinethefollowingùëáùê∑(0)CPToperator(note
sup(cid:13) (cid:13)‚àáùúÉùëñùëâ ùëñ(ùúÉ,ùë†)(cid:13) (cid:13)‚â§ùúâ ùëñ. (19)
ùë†‚ààS
thattheagent‚Äôssuperscriptùëõhasbeendropped),Algorithm2DistributedNestedCPTActor-Critic
1: Inputs: shared among agents: Initial state ùë†0, number of samples used for CPT estimation (ùëõ ùëöùëéùë•), learning rate sequences
( e{ mùõº pùëêùëü t, yùë°} eùë° x‚â• p0 e, r{ iùõº eùëé nùëê c,ùë° e} dùë° i‚â• c0 ti) o, na an rd yt ùê∏r ùë•an ùëùs ùê∑it ùëñi ùëêo ùë°n ùëõ.probabilities. Local variables for agentùëõ: Initialùëâ ùúãùëõ ùúÉ0, initial policy parameters (ùúÉ 0ùëõ), and an
2: Foreachagentùëõ,do:
3: Sampleactionùëéùëõ 0 frompolicyùúã ùúÉ 0ùëõ(.|ùë†0).
4: ùë° ‚Üê0.
5: Repeat
6: Sampleùëéùëõ ùë° fromùúã ùúÉùëõ(.|ùë† ùë°).
7: Executeùëéùëõ ùë° andobsùë° erveùëü ùë°ùëõ,ùë† ùë°+1,andùúé ùë°‚àíùëõ.
8: Push(ùëü ùë°,ùë† ùë°+1,ùúé ùë°‚àíùëõ)toùê∏ùë•ùëùùê∑ùëñùëêùë°ùëõ(ùë† ùë°,ùëéùëõ ùë°,ùúé ùë°‚àíùëõ).
9: Criticvalueestimation:
10: Createemptyarrayùëã ofsizeùëõ ùëöùëéùë•.
11: foreachùëñ =1,2,...,ùëõ ùëöùëéùë•,do
12: SampleùëéÀÜùëõ ùë° fromùúã ùúÉùëõ(.|ùë† ùë°)andconstructùúéÀÜ ùë°‚àíùëõbyobservingneighbors.
13: Sample(ùëüÀÜ ùë°ùëõ,ùë†ÀÜ ùë°+1)frùë° omùê∏ùë•ùëùùê∑ùëñùëêùë°(ùë† ùë°,ùëéÀÜùëõ ùë°,ùúéÀÜ ùë°‚àíùëõ)ifitislargeenough,andotherwisefromasimulatoroftheenvironment.
11 54 :: IL fe tt hùëã eùëñ s= amùëüÀÜ ùë°ùëõ pl+ ecùõæ aùëâ mùúãùëõ ùúÉ e( fùë†ÀÜ rùë° o+ m1). asimulator,push(ùëüÀÜ ùë°ùëõ,ùë†ÀÜ ùë°+1)toùê∏ùë•ùëùùê∑ùëñùëêùë°(ùë† ùë°,ùëéÀÜùëõ ùë°,ùúéÀÜ ùë°‚àíùëõ)forlateruse.
16: endfor
17: EstimateùëâÀÜ ùúãùëõ ùúÉùë°(ùë† ùë°)usingAlgorithm1.
18: Criticstep:
19: CalculatetheTD-error:
ùõø
ùë°
:=ùëâÀÜ ùúãùëõ ùúÉùë°(ùë† ùë°)‚àíùëâ ùúãùëõ ùúÉùë°(ùë† ùë°).
ùëâ ùúãùëõ ùúÉùë°(ùë† ùë°)‚Üêùëâ ùúãùëõ ùúÉùë°(ùë† ùë°)+ùõº ùëêùëü,ùë°ùõø ùë°.
20: Actorstep:
21: Compute‚àáùëâ ùúãùëõ ùúÉùë°(ùë†0)usingthegradientestimationschemedescribedinSection4.
ùúÉ ùë°ùëõ +1:=ùúÉ ùë°ùëõ +ùõº ùëéùëê,ùë°‚àáùëâ ùúãùëõ ùúÉùë°(ùë†0).
22: ùë° ‚Üêùë°+1.
23: Untilconvergence
Assumption5. Thepseudo-gradientmappingvaluefunction,‚àáùúÉùëâ(ùúÉ,ùë†)= UnderAssumptions4and5,basedonTheorem2inRosen[44],
col(cid:0)‚àáùúÉùëñùëâ1(ùúÉ,ùë†),...,‚àáùúÉùëÅùëâ ùëÅ(ùúÉ,ùë†)(cid:1),isstronglymonotonewithrespect thereexistsauniqueMPNEfortheNAMG.NotethatthisCPT-
toùúÉ,i.e.,foreveryùúÉ,ùúÉ‚Ä≤ ‚ààŒò,ùë† ‚ààS,thereexistsùúá >0suchthat sensitive(subjective)MPNEcanbedifferentfromtheMPNEofthe
gamewhentheagentsarerisk-neutral.Considertheparameter
(cid:0)‚àáùúÉùëâ(ùúÉ,ùë†)‚àí‚àáùúÉùëâ (cid:0)ùúÉ‚Ä≤,ùë†(cid:1)(cid:1)‚ä§(cid:0)ùúÉ‚àíùúÉ‚Ä≤(cid:1) ‚â•ùúá(cid:13) (cid:13)ùúÉ‚àíùúÉ‚Ä≤(cid:13) (cid:13)2 .
vectorùúÉ‚àóasthevectorthatconstructsthisuniqueNashpolicy,for
Furthermore,thismappingisLipschitzcontinuous,i.e., whichwehave‚àáùúÉùëñùêΩ ùëñ(ùúÉ‚àó) = 0forallùëñ.PerAssumption4andby
(cid:13) (cid:13)‚àáùúÉùëâ(ùúÉ,ùë†)‚àí‚àáùúÉùëâ (cid:0)ùúÉ‚Ä≤,ùë†(cid:1)(cid:13) (cid:13)‚â§ùêø ùëé(cid:13) (cid:13)ùúÉ‚àíùúÉ‚Ä≤(cid:13) (cid:13). definingŒîùúÉ ùëõùë° =ùúÉ ùëõùë° ‚àíùúÉ ùëõ‚àó,wehave
Theorem2. (Convergenceoftheactor)GivenAssumptions4and
5andacriticandanactorwithlearningstepssuchthat, (cid:13) (cid:13)ŒîùúÉ ùëõùë°+1(cid:13) (cid:13)2 =(cid:13) (cid:13)ŒîùúÉ ùëõùë° ‚àíùõº ùëéùëê,ùë°‚àáùúÉùëõùëâ ùëõ(cid:0)ùúÉùë°(cid:1)(cid:13) (cid:13)2
‚àëÔ∏Å ùë°‚àû =0ùõº ùëéùëê,ùë° =‚àû, ‚àëÔ∏Å ùë°‚àû =0ùõº ùëêùëü,ùë° =‚àû, ‚àëÔ∏Å ùë°‚àû =0ùõº ùëê2 ùëü,ùë° <‚àû, ‚àëÔ∏Å ùë°‚àû =0ùõº ùëé2 ùëê,ùë° <‚àû (2,
0)
ùë°l ‚Üíim ‚àûùõº ùõºùëé ùëêùëüùëê ,, ùë°ùë° =0, ‚àí= 2(cid:13) (cid:13) ùõºŒî ùëéùúÉ ùëêùëõùë° ,ùë°(cid:13) (cid:13) ‚àá2 ùúÉ+ ùëõ(cid:0) (cid:0)ùõº ùëâùëé ùëõùëê, (cid:0)ùë° ùúÉ(cid:1) ùë°2 (cid:1)(cid:13) (cid:13) (cid:1)‚àá ‚ä§ùúÉ Œîùëõ ùúÉùëâ ùëõùë°ùëõ(cid:0)ùúÉùë°(cid:1)(cid:13) (cid:13)2 (21)
algorithm(2)convergestotheuniquesubjectiveMarkovperfect ‚â§(cid:13) (cid:13)ŒîùúÉ ùëõùë°(cid:13) (cid:13)2 +(cid:0)ùõº ùëéùëê,ùë°(cid:1)2ùúâ ùëõ2 ‚àí2ùõº ùëéùëê,ùë°‚àáùúÉùëõùëâ ùëõ(cid:0)ùúÉùë°(cid:1)‚ä§ŒîùúÉ ùëõùë°.
NashequilibriumoftheNAMG,asymptotically.
Proof. We prove that the actor update will converge to the Bysummingtheaboveequationoverdifferentùëõ‚àà{1,...,ùëÅ}and
uniqueNashpolicyoftheMarkovgame,whichexistsifAssump- definingŒîùúÉùë° =col(cid:16) ŒîùúÉùë°...,ŒîùúÉùë° (cid:17) ,wehave
1 ùëÅ
tions4and5aresatisfied,startingfromanyinitialcondition.We
rewritetheactorupdateforagentùëõbelow,
ùúÉ ùë°ùëõ +1:=ùúÉ ùë°ùëõ +ùõº ùëéùëê,ùë°‚àáùúÉùëâùëõ (ùúÉ,ùë†0).
(cid:13) (cid:13)ŒîùúÉùë°+1(cid:13) (cid:13)2 ‚â§(cid:13) (cid:13)ŒîùúÉùë°(cid:13) (cid:13)2 + ùëõ‚àëÔ∏Å ‚ààNùúâ ùëõ2 (cid:0)ùõº ùëéùëê,ùë°(cid:1)2 ‚àí2ùõº ùëéùëê,ùë°‚àáùúÉùëâùë°‚ä§ŒîùúÉùë°, (22)where‚àáùúÉùëâùë° =col(cid:0)‚àáùúÉ1ùëâ1(cid:0)ùúÉùë°(cid:1),...,‚àáùúÉùëÅùëâ
ùëÅ
(cid:0)ùúÉùë°(cid:1)(cid:1).Wealsoknow
that‚àáùúÉùëâ‚àó =col(cid:0)‚àáùúÉ1ùëâ1(ùúÉ‚àó),...,‚àáùúÉùëÅùëâ ùëÅ (ùúÉ‚àó)(cid:1) =0.Therefore,ac- ùëÖ ùë†ùëíùëôùëì(ùë†,ùëéùëñ )‚àºùëÅùëúùëüùëöùëéùëô(0.5,0.1),‚àÄùëñ ‚àà1,...,ùëÅ. (27)
cordingtoassumption5,
Also, the reward coefficients ùëÖ ùëêùëñ ùëúùëö(ùë†) is randomly generated
‚àí‚àáùúÉùëâùë°‚ä§ŒîùúÉùë° =‚àí(cid:0)‚àáùúÉùêΩùë° ‚àí‚àáùúÉùëâ‚àó(cid:1)‚ä§ŒîùúÉùë° form
=‚àí(cid:0)‚àáùúÉùëâùë° (cid:0)ùúÉùë°(cid:1)‚àí‚àáùúÉùëâùë° (cid:0)ùúÉ‚àó(cid:1)(cid:1)‚ä§ŒîùúÉùë° (23)
ùëÖ ùëêùëñ ùëúùëö(ùë†)‚àº5¬∑ùë¢ùëõùëñùëìùëúùëüùëö[‚àí0.5,0.5]. (28)
‚â§‚àíùúá(cid:13) (cid:13)ŒîùúÉùë°(cid:13) (cid:13)2 .
Theabovesetupimpliesahighriskfortheagentifitdecidesto
Finally,withtelescopicsummation, takeanactiongreaterthanùëéùëñ = 0andbecomeinvolvedwithits
neighboringcommunity,e.g.,takeafinancialriskinaninteractive
‚àû
lim (cid:13) (cid:13)ŒîùúÉùë°(cid:13) (cid:13)2 ‚â§(cid:13) (cid:13)ŒîùúÉ0(cid:13) (cid:13)2 + ‚àëÔ∏Å ùúâ ùëõ2‚àëÔ∏Å(cid:0)ùõº ùëéùëê,ùë°(cid:1)2 market.Thus,itcanbesaidthateachagentinthisnon-cooperative
ùë°‚Üí‚àû environmentchoosesbyitsactionhowmuchriskitwantstotake
ùëõ‚ààN ùë°=0
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) andtowhatdegreeitwantstointeractwiththecommunity(other
(cid:124) (cid:123)(cid:122) (cid:125)
ùëá1
(24)
agentswhocould,forinstance,beeconomic,political,orsocial
‚àû competitors),andtiesitsreceivedrewardtotheiractions.Ifthe
‚àí‚àëÔ∏Å ùúáùõº ùëéùëê,ùë°(cid:13) (cid:13)ŒîùúÉùë°(cid:13) (cid:13)2 . agentchoosestheactionùëéùëñ = 0,itwillsettleforalow,butrisk-
ùë°= (cid:32)(cid:32)(cid:32)0 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) freeprofit.However,whenchoosinganotheraction,dependingon
(cid:124) (cid:123)(cid:122) (cid:125) theactionsoftheotheragents,itcanmakeasignificantprofitor
ùëá2
lossthatisalsoaffectedbystochasticityoftheenvironment.Our
Sinceùúá >0,ùëá2 ‚â•0.Therefore,asùëá1isbounded,ùëá2isbounded
a ass ùë°w ‚Üíell. ‚àûCo an ns de aq su aen rt el sy u, la ts ùúÉ(cid:205) coùë°‚àû = n0 vùõº erùëé gùëê, eùë° s= to‚àû ùúÉ, ‚àó.‚à•ŒîùúÉùë°‚à•2convergesto ‚ñ°0 o pb arje ac mti ev te eris ùúÜt io ns tt hu edy cut mhe ua lag te ivn ets p‚Äô er ris sk p- ea cv tie vr esi to hn eole rv ye uls tib lia ts ye fd uo nn ctit oh ne
(Figure2).Thehigherthevalueofthisparameter,themoreloss-
Giventheasymptoticproofofconvergencefortheactorandthe aversetheagentis.Weexpectthatinthedesignednon-cooperative
criticandconsideringtheconditionsofthelearningstepsequences environment,amoreloss-averseagentwillchoosetheactionùëé=0
in Theorem 2, we can apply Theorem 1.1 of Borkar [5], which withahigherprobabilityandhaveatendencytobecomesocially
shows the asymptotic convergence of the AC algorithm to the isolatedorconservative.Toevaluatethishypothesis,werunthe
uniqueMPNEoftheNAMG.NotethatifAssumptions4and5do proposedalgorithmforthisrisk-sensitiveNAMGandforfourdif-
nothold,giventheactor‚Äôsgradientexpression,wecanonlyensure ferentloss-aversionscenarios.Inthefirstscenario,allagentsare
theconvergenceoftheACalgorithmtoalocallyoptimalpolicy risk-neutral(correspondingtoavanillaACalgorithmwithlinear
parameterforeachagent. functionapproximation).Inthesecondscenario,allagentshave
thesameleveloflossaversion(ùúÜ=2.6).Inthethirdscenario,only
6 NUMERICALEXPERIMENT Agent1isrisk-sensitive(ùúÜ=2.6),andotheragentsarerisk-neutral.
ToexaminetheempiricalconvergenceofDistributedNestedCPT Finally,inthelastscenario,allagentsarerisk-sensitive,butAgent
AC,weconstructedarisk-sensitiveNAMGwithaninterpretable onehasahigherlossaversioncoefficient(ùúÜ = 3.2),whileothers
designformeasuringtheeffectoflossaversioninrisk-sensitive haveùúÜ=2.6.Figure4showstheconvergenceofthevaluefunctions
agentsontheconvergedpolicies.NotethatduetotheCPToperator, correspondingtooneofthestatesinthesecondscenario.Figure5
Assumptions4and5arehardtoverifyinanyexperimentalsetup showstheprobabilityofchoosingaction0(aquantitativeindicator
andwedidnotexpectthealgorithmtoconvergetothesubjective ofsocialconservatism)intheconvergedpoliciesofagentsineach
MPNE(whichmaynotbeuniqueiftheaforementionedassumptions scenario.Asobserved,thelevelofsocialisolationandtheprobabil-
arenotsatisfied).IntheconstructedNAMG,therearefouragents, ityofchoosingtheconservativeactionùëé=0isproportionaltothe
fivestates,andthreepossibleactions(N =4,S={0,1,2,3,4},A= risk-aversionleveloftheagentsinthecommunity.
{0,1,2}),andthecommunicationgraphisfully-connected.The
rewardfunctionisdefinedas 7 CONCLUSIONANDFUTUREWORKS
Inthiswork,weproposedadistributedrisk-sensitiveMARLalgo-
ùëÖùëñ (ùë†,ùëéùëñ,ùúéùëñ (ùëé‚àíùëñ ))=ùëÖ ùë†ùëñ ùëíùëôùëì(ùë†)+ùúéùëñ (ùëé‚àíùëñ )ùëÖ ùëêùëñ ùëúùëö(ùë†)ùëéùëñ, (25) rithminNAMGswiththeoreticalconvergenceguaranteesbasedon
cumulativeprospecttheory,acognitiveriskmeasurethatbroadens
wherethefirsttermistherewardsolelyaffectedbytheagent‚Äôs
thescopeofthetraditionallyadoptedcoherentriskmeasure.We
action,andthesecondtermistherewardthatisaffectedbythe
empiricallyshowedthepositivecorrelationbetweenlossaversion
actionsoftheneighboringùëêùëúùëöùëöùë¢ùëõùëñùë°ùë¶oftheagent.Theaggregative
andsocialisolationofagents.Weobservedthatscalingthepro-
termis
posedalgorithmtolargerenvironmentsandcontinuouscontrol
ùúéùëñ (ùëé‚àíùëñ )= ùëÅ1 ‚àí1( ‚àëÔ∏Å ùëé ùëó), (26) i es ven ro ,t ac po lm aup sa ibti lb el de iw rei ct th iot nhe oo fr fe ut ti uca relc wo on rv ke ir sg te hn ece apg pu rr oa pn rt ie ae tes. uH seow of-
ùëó‚ààN\ùëñ functionapproximationanddeepRLmethodstotacklethecurse
which indicates a communication graph with equal weights ofdimensionalityforlargestateandactionspacesinCPT-sensitive
amongtheneighbors.TherewardcoefficientùëÖùëñ (ùë†)foragentùëñis RL,andingeneralrisk-averseRL[61],inanempiricalframework,
ùë†ùëíùëôùëì
randomlygeneratedfrom albeitwithouttheoreticalconvergencegurantees.[10] YinlamChow,MohammadGhavamzadeh,LucasJanson,andMarcoPavone.2017.
Risk-constrainedreinforcementlearningwithpercentileriskcriteria.TheJournal
6 Agent #1 ofMachineLearningResearch18,1(2017),6070‚Äì6120.
Agent #2 [11] FreddyDelbaen.2002. Coherentriskmeasuresongeneralprobabilityspaces.
5 A Ag ge en nt t # #3 4 Advancesinfinanceandstochastics:essaysinhonourofDieterSondermann(2002),
1‚Äì37.
[12] ZhenhuaDeng.2019.Distributedalgorithmdesignforresourceallocationprob-
4 lemsofsecond-ordermultiagentsystemsoverweight-balanceddigraphs.IEEE
TransactionsonSystems,Man,andCybernetics:Systems51,6(2019),3512‚Äì3521.
3 [13] ZhenhuaDeng.2021. Distributedalgorithmdesignforaggregativegamesof
Euler‚ÄìLagrangesystemsanditsapplicationtosmartgrids.IEEETransactionson
Cybernetics52,8(2021),8315‚Äì8325.
2
[14] DongshengDing,Chen-YuWei,KaiqingZhang,andMihailoJovanovic.2022.
Independentpolicygradientforlarge-scalemarkovpotentialgames:Sharper
1 rates,functionapproximation,andgame-agnosticconvergence.InInternational
ConferenceonMachineLearning.PMLR,5166‚Äì5220.
0 1000 2000 3000 4000 5000
Timestep [15] YingjieFei,ZhuoranYang,YudongChen,andZhaoranWang.2021.Exponential
bellmanequationandimprovedregretboundsforrisk-sensitivereinforcement
learning.AdvancesinNeuralInformationProcessingSystems34(2021),20436‚Äì
Figure4:Smoothedmeanvaluefunctionofagivenstateover 20446.
[16] RoyFox,StephenMMcaleer,WillOverman,andIoannisPanageas.2022.Inde-
eightindependentrunsinDistributedNestedCPT-ACfor pendentnaturalpolicygradientalwaysconvergesinMarkovpotentialgames.In
scenario2(allagentsarerisk-sensitivewithùúÜ=2.6). InternationalConferenceonArtificialIntelligenceandStatistics.PMLR,4414‚Äì4425.
[17] MrinalKGhosh,SubrataGolui,ChandanPal,andSomnathPradhan.2022.
Nonzero-sumrisk-sensitivecontinuous-timestochasticgameswithergodiccosts.
AppliedMathematics&Optimization86,1(2022),6.
[18] MrinalKGhosh,SubrataGolui,ChandanPal,andSomnathPradhan.2023.
Discrete-timezero-sumgamesforMarkovchainswithrisk-sensitiveaverage
0.5 A Ag ge en nt t # #1 2 costcriterion.StochasticProcessesandtheirApplications158(2023),40‚Äì74.
Agent #3 [19] ChengJie,LAPrashanth,MichaelFu,SteveMarcus,andCsabaSzepesv√°ri.2018.
Agent #4 Stochasticoptimizationinacumulativeprospecttheoryframework.IEEETrans.
0.4 Automat.Control63,9(2018),2867‚Äì2882.
[20] DANIELKahnemanandAmosTversky.1979.Prospecttheory:Ananalysisof
0.3 decisionunderrisk.Econometrica47,2(1979),363‚Äì391.
[21] PrashanthLaandMohammadGhavamzadeh.2013.Actor-criticalgorithmsfor
risk-sensitiveMDPs.Advancesinneuralinformationprocessingsystems26(2013).
0.2 [22] StefanosLeonardos,WillOverman,IoannisPanageas,andGeorgiosPiliouras.
2021. Globalconvergenceofmulti-agentpolicygradientinmarkovpotential
0.1 games.arXivpreprintarXiv:2106.01969(2021).
[23] KunLin.2013.Stochasticsystemswithcumulativeprospecttheory.Ph.D.Disserta-
tion.UniversityofMaryland,CollegePark.
0.0 Scenario 1 Scenario 2 Scenario 3 Scenario 4 [24] KunLin,ChengJie,andStevenIMarcus.2018.Probabilisticallydistortedrisk-
sensitiveinfinite-horizondynamicprogramming.Automatica97(2018),1‚Äì6.
[25] KunLinandStevenIMarcus.2013.Dynamicprogrammingwithnon-convex
risk-sensitivemeasures.In2013AmericanControlConference.IEEE,6778‚Äì6783.
Figure5:Meanconvergedpoliciesovereightindependent [26] MichaelLLittman.1994.Markovgamesasaframeworkformulti-agentrein-
runs for different loss aversion scenarios. Scenario 1: all forcementlearning.InMachinelearningproceedings1994.Elsevier,157‚Äì163.
[27] ChinmayMaheshwari,ManxiWu,DruvPai,andShankarSastry.2022. Inde-
agents are risk-neutral, scenario 2: all agents are risk-
pendentanddecentralizedlearninginmarkovpotentialgames.arXivpreprint
sensitive(ùúÜ=2.6),scenario3:onlyAgent1isrisk-sensitive arXiv:2205.14590(2022).
(ùúÜ=2.6),scenario4:Agent1hasahigherlossaversioncoeffi- [28] JaredMarkowitz,MarieChau,andI-JengWang.2021.DeepCPT-RL:Imparting
Human-LikeRiskSensitivitytoArtificialAgents..InSafeAI@AAAI.
cient(ùúÜ=3.2)thanothers(ùúÜ=2.6).
[29] DavidHMguni,YutongWu,YaliDu,YaodongYang,ZiyiWang,MinneLi,Ying
Wen,JoelJennings,andJunWang.2021.Learninginnonzero-sumstochastic
gameswithpotentials.InInternationalConferenceonMachineLearning.PMLR,
7688‚Äì7699.
REFERENCES
[30] MehrdadMoharrami,YashaswiniMurthy,ArghyadipRoy,andRayadurgam
[1] CarloAcerbi.2002. Spectralmeasuresofrisk:Acoherentrepresentationof Srikant.2022.APolicyGradientAlgorithmfortheRisk-SensitiveExponential
subjectiveriskaversion.JournalofBanking&Finance26,7(2002),1505‚Äì1518. CostMDP.arXivpreprintarXiv:2202.04157(2022).
[2] AhmetAlacaoglu,LucaViano,NiaoHe,andVolkanCevher.2022. Anatural [31] MdShirajumMunir,SarderFakhrulAbedin,NguyenHTran,ZhuHan,Eui-Nam
actor-criticframeworkforzero-sumMarkovgames.InInternationalConference Huh,andChoongSeonHong.2021. Risk-awareenergyschedulingforedge
onMachineLearning.PMLR,307‚Äì366. computingwithmicrogrid:Amulti-agentdeepreinforcementlearningapproach.
[3] PhilippeArtzner,FreddyDelbaen,Jean-MarcEber,andDavidHeath.1999.Co- IEEETransactionsonNetworkandServiceManagement18,3(2021),3476‚Äì3497.
herentmeasuresofrisk.Mathematicalfinance9,3(1999),203‚Äì228. [32] ErfaunNooraniandJohnSBaras.2022.Risk-attitudes,Trust,andEmergence
[4] TamerBa≈üar.2021.Robustdesignsthroughrisksensitivity:Anoverview.Journal ofCoordinationinMulti-agentReinforcementLearningSystems:AStudyof
ofSystemsScienceandComplexity34(2021),1634‚Äì1665. IndependentRisk-sensitiveREINFORCE.In2022EuropeanControlConference
[5] VivekSBorkar.1997.Stochasticapproximationwithtwotimescales.Systems& (ECC).IEEE,2266‚Äì2271.
ControlLetters29,5(1997),291‚Äì294. [33] TakayukiOsogami.2012. Robustnessandrisk-sensitivityinMarkovdecision
[6] VivekSBorkar.2001.Asensitivityformulaforrisk-sensitivecostandtheactor‚Äì processes.Advancesinneuralinformationprocessingsystems25(2012).
criticalgorithm.Systems&ControlLetters44,5(2001),339‚Äì346. [34] ChandanPalandSomnathPradhan.2021. Zero-sumgamesforpurejump
[7] OzlemCavusandAndrzejRuszczynski.2014.Risk-aversecontrolofundiscounted processeswithrisk-sensitivediscountedcostcriteria.JournalofDynamicsand
transientMarkovmodels.SIAMJournalonControlandOptimization52,6(2014), Games9,1(2021),13‚Äì25.
3935‚Äì3966. [35] FrancescaParise,SergioGrammatico,BasilioGentile,andJohnLygeros.2020.
[8] CarloCenedese,GiuseppeBelgioioso,SergioGrammatico,andMingCao.2020. DistributedconvergencetoNashequilibriainnetworkandaverageaggregative
Time-varyingconstrainedproximaltypedynamicsinmulti-agentnetworkgames. games.Automatica117(2020),108959.
In2020EuropeanControlConference(ECC).IEEE,148‚Äì153. [36] JulienPerolat,BrunoScherrer,BilalPiot,andOlivierPietquin.2015.Approximate
[9] YinlamChowandMohammadGhavamzadeh.2014.AlgorithmsforCVaRopti- dynamicprogrammingfortwo-playerzero-sumMarkovgames.InInternational
mizationinMDPs.Advancesinneuralinformationprocessingsystems27(2014). ConferenceonMachineLearning.PMLR,1321‚Äì1329.
ytilibaborP
)0s(V[37] LAPrashanth,MichaelCFu,etal.2022.Risk-SensitiveReinforcementLearning [63] KaiqingZhang,ShamKakade,TamerBasar,andLinYang.2020.Model-based
viaPolicyGradientSearch.FoundationsandTrends¬ÆinMachineLearning15,5 multi-agentrlinzero-summarkovgameswithnear-optimalsamplecomplexity.
(2022),537‚Äì693. AdvancesinNeuralInformationProcessingSystems33(2020),1166‚Äì1178.
[38] LAPrashanthandMohammadGhavamzadeh.2016.Variance-constrainedactor- [64] KaiqingZhang,XiangyuanZhang,BinHu,andTamerBasar.2021.Derivative-free
criticalgorithmsfordiscountedandaveragerewardMDPs.MachineLearning policyoptimizationforlinearrisk-sensitiveandrobustcontroldesign:Implicit
105(2016),367‚Äì417. regularizationandsamplecomplexity.AdvancesinNeuralInformationProcessing
[39] LAPrashanth,ChengJie,MichaelFu,SteveMarcus,andCsabaSzepesv√°ri.2016. Systems34(2021),2949‚Äì2964.
Cumulativeprospecttheorymeetsreinforcementlearning:Predictionandcontrol. [65] HaiZhong,YutakaShimizu,andJianyuChen.2022.Chance-ConstrainedIterative
InInternationalConferenceonMachineLearning.PMLR,1406‚Äì1415. Linear-QuadraticStochasticGames.IEEERoboticsandAutomationLetters8,1
[40] DrazenPrelec.1998.Theprobabilityweightingfunction.Econometrica(1998), (2022),440‚Äì447.
497‚Äì527. [66] KunZhu,EkramHossain,andDusitNiyato.2013. Pricing,spectrumsharing,
[41] ShuangQiu,XiaohanWei,JiepingYe,ZhaoranWang,andZhuoranYang.2021. andserviceselectionintwo-tiersmallcellnetworks:Ahierarchicaldynamic
Provablyefficientfictitiousplaypolicyoptimizationforzero-sumMarkovgames gameapproach.IEEETransactionsonMobileComputing13,8(2013),1843‚Äì1856.
withstructuredtransitions.InInternationalConferenceonMachineLearning. [67] ZiqingZhu,KaWingChan,SiqiBu,BinZhou,andShiweiXia.2022. Nash
PMLR,8715‚Äì8725. EquilibriumEstimationandAnalysisinJointPeer-to-PeerElectricityandCarbon
[42] WeiQiu,XinrunWang,RunshengYu,RundongWang,XuHe,BoAn,Svetlana EmissionAuctionMarketWithMicrogridProsumers.IEEETransactionsonPower
Obraztsova,andZinoviRabinovich.2021.RMIX:Learningrisk-sensitivepolicies Systems(2022).
forcooperativereinforcementlearningagents.AdvancesinNeuralInformation
ProcessingSystems34(2021),23049‚Äì23062.
[43] DSaiKotiReddy,AmritaSaha,SrikanthGTamilselvam,PriyankaAgrawal,and
PankajDayama.2019.Riskaversereinforcementlearningformixedmulti-agent
environments.InProceedingsofthe18thinternationalconferenceonautonomous
agentsandmultiagentsystems.2171‚Äì2173.
[44] JBenRosen.1965.Existenceanduniquenessofequilibriumpointsforconcave
n-persongames.Econometrica:JournaloftheEconometricSociety(1965),520‚Äì534.
[45] AndrzejRuszczy≈Ñski.2010. Risk-aversedynamicprogrammingforMarkov
decisionprocesses.Mathematicalprogramming125(2010),235‚Äì261.
[46] MohsenSaffar,HamedKebriaei,andDusitNiyato.2017.Pricingandrateopti-
mizationofcloudradioaccessnetworkusingrobusthierarchicaldynamicgame.
IEEETransactionsonWirelessCommunications16,11(2017),7404‚Äì7418.
[47] MuhammedSayin,KaiqingZhang,DavidLeslie,TamerBasar,andAsuman
Ozdaglar.2021.DecentralizedQ-learninginzero-sumMarkovgames.Advances
inNeuralInformationProcessingSystems34(2021),18320‚Äì18334.
[48] AlexanderShapiro,DarinkaDentcheva,andAndrzejRuszczynski.2021.Lectures
onstochasticprogramming:modelingandtheory.SIAM.
[49] LloydSShapley.1953.Stochasticgames.Proceedingsofthenationalacademyof
sciences39,10(1953),1095‚Äì1100.
[50] MohammadShokriandHamedKebriaei.2020. Leader‚Äìfollowernetworkag-
gregativegamewithstochasticagents‚Äôcommunicationandactiveness. IEEE
Trans.Automat.Control65,12(2020),5496‚Äì5502.
[51] MehdiNaderiSoorki,WalidSaad,MehdiBennis,andChoongSeonHong.2021.
Ultra-reliableindoormillimeterwavecommunicationsusingmultipleartificial
intelligence-poweredintelligentsurfaces.IEEETransactionsonCommunications
69,11(2021),7444‚Äì7457.
[52] RichardSSuttonandAndrewGBarto.2018.Reinforcementlearning:Anintro-
duction.MITpress.
[53] AvivTamar,YinlamChow,MohammadGhavamzadeh,andShieMannor.2015.
Policygradientforcoherentriskmeasures. Advancesinneuralinformation
processingsystems28(2015).
[54] AvivTamar,DotanDiCastro,andShieMannor.2012. Policygradientswith
variancerelatedriskcriteria.InProceedingsofthetwenty-ninthinternational
conferenceonmachinelearning.387‚Äì396.
[55] AvivTamar,DotanDiCastro,andShieMannor.2013. Temporaldifference
methodsforthevarianceoftherewardtogo.InInternationalConferenceon
MachineLearning.PMLR,495‚Äì503.
[56] AvivTamar,ShieMannor,andHuanXu.2014.ScalinguprobustMDPsusing
functionapproximation.InInternationalconferenceonmachinelearning.PMLR,
181‚Äì189.
[57] ShaolinTan,YaonanWang,andAthanasiosVVasilakos.2021. Distributed
populationdynamicsforsearchinggeneralizednashequilibriaofpopulation
gameswithgraphicalstrategyinteractions.IEEETransactionsonSystems,Man,
andCybernetics:Systems52,5(2021),3263‚Äì3272.
[58] RanTian,LitingSun,andMasayoshiTomizuka.2021.Boundedrisk-sensitive
markovgames:Forwardpolicydesignandinverserewardlearningwithiterative
reasoningandcumulativeprospecttheory.InProceedingsoftheAAAIConference
onArtificialIntelligence,Vol.35.6011‚Äì6020.
[59] JohnTsitsiklisandBenjaminVanRoy.1997.Ananalysisoftemporal-difference
learningwithfunctionapproximation.IEEETrans.Automat.Control42,5(1997),
674‚Äì690.
[60] AmosTverskyandDanielKahneman.1992.Advancesinprospecttheory:Cu-
mulativerepresentationofuncertainty.JournalofRiskanduncertainty5(1992),
297‚Äì323.
[61] N√∫riaArmengolUrp√≠,SebastianCuri,andAndreasKrause.2021.Risk-averse
offlinereinforcementlearning.arXivpreprintarXiv:2102.05371(2021).
[62] JamesRWrightandKevinLeyton-Brown.2017. Predictinghumanbehavior
inunrepeated,simultaneous-movegames.GamesandEconomicBehavior106
(2017),16‚Äì37.