Classifying Nodes in Graphs without GNNs
DanielWinter NivCohen YedidHoshen
{daniel.winter, niv.cohen2, yedid.hoshen}@mail.huji.ac.il
SchoolofComputerScienceandEngineeringTheHebrewUniversityofJerusalem,Israel
Abstract Distillation methods, starting with the seminal work by
Zhangetal. (Zhangetal.,2021),proposedtoreplaceGNNs
Graphneuralnetworks(GNNs)arethedominant
at test time by simple node-level MLPs. They first train
paradigm for classifying nodes in a graph, but
aGNNonthetrainingdata,thenusethelabelspredicted
theyhaveseveralundesirableattributesstemming
bytheGNNasdistillationtargetsfortraininganode-level
from their message passing architecture. Re-
MLP.Attesttime,thesemethodsonlyusethenode-level
cently, distillation methods succeeded in elim-
MLP.Distillationmethodssometimesachieveevenbetter
inating the use of GNNs at test time but they
resultsthantheoriginalGNN.However,whilethislineof
still require them during training. We perform work removed the requirement for GNNs at test-time, it
a careful analysis of the role that GNNs play
stillrequirestrainingaGNNforproducingthedistillation
in distillation methods. This analysis leads us
targets.
toproposeafullyGNN-freeapproachfornode
classification,notrequiringthemattrainortest Here,weaimtotakethisresearcheffortonestepfurtherand
time. Ourmethodconsistsofthreekeycompo- create an entirely GNN-free node classification approach.
nents: smoothnessconstraints,pseudo-labelingit- We begin by investigating the reasons for the success of
erationsandneighborhood-labelhistograms. Our distillationmethods. Wesuggesttheabilitytoachievestate-
finalapproachcanmatchthestate-of-the-artac- theof-the-artperformancewithoutusingGNNsattest-time
curacyonstandardpopularbenchmarkssuchas hintsthattheiradvantageliesinGNNs’sampleefficiency
citationandco-purchasenetworks,withouttrain- ratherthanmessagepassing.
ingaGNN1.
OurfindingsmotivateustoproposeCoHOp,anewnode
classificationmethodconsistingofthefollowingkeycom-
ponents: (i) A loss encouraging smoothness between the
1.Introduction labelpredictionsofneighboringnodes(ii)Iterativepseudo-
labelling of the observed unlabelled nodes (iii) Label
Nodeclassificationtasksnaturallyoccurwhenwewishto
neighborhood-histogram for encoding local context. Co-
classify graph-structured data, such as paper citation net-
HOp does not require training or evaluating GNNs at all,
works (Namata et al., 2012; Sen et al., 2008) or product
and achieves competitive results with GNNs on popular
co-purchase networks (Shchur et al., 2018). Most state-
datasetssuchascitationandco-purchasenetworks.
of-the-art node classification methods use Graph Neural
Networks(GNN)(Hamiltonetal.,2017;Kipf&Welling, Ourkeycontributionsare:
2016;Velickovicetal.,2017). GNNsexploitthecontext
of the entire neighborhood of each node for determining
• Clarifyingtheroleofdistillationmethodsinnodeclas-
itsclass. Theirarchitectureusesmessagepassingtotrans-
sification.
ferinformationacrossmanynodes. GNNsconsiderlarge
neighborhoods, while this is very effective, their training
andinferencetimesareconsiderablyhigherthanmethods
• Introducingneighborhood-histogramfeaturestoincor-
consideringonlythenodefeatures.
poratelocalcontextinformation.
Therefore,severalresearcheffortsattemptedtofindsimpler
alternativestoGNNsthatdonotrequiremessagepassing.
• Achieving competitive results on popular node-
1The full source code of our method is available at classification datasets without training graph neural
https://github.com/dani3lwinter/CoHOp networks.
1
4202
beF
8
]GL.sc[
1v43950.2042:viXraClassifyingNodesinGraphswithoutGNNs
2.Relatedworks
90%
GraphNeuralNetworks(GNNs)haveemergedasapromi-
nenttoolinthedomainofgraphmachinelearning(Bruna
etal.,2013;Chenetal.,2020b;Defferrardetal.,2016;Li
80%
et al., 2019). These neural networks use aggregations of
featuresfromthelocalcontextofeachnodeatsuccessive
GNN
layers. Forexample,GraphConvolutionalNetworks(GCN)
Distillation
70%
(Kipf&Welling,2016)extendtraditionalconvolutionoper- NoDistillation
ationsfromtheEuclideandomaintooperationsongraphs.
GraphSAGE (Hamilton et al., 2017) uses arbitrary aggre- 0% 20% 40% 60% 80%
Portionofthesamplesinthetrainingset
gationfunctionwhilealsoconcatenatingthefeaturesprior
to the aggregation. GAT (Velickovic et al., 2017), GTN Figure1. Alinearmodelusingnode-onlyfeaturesachievescompa-
(Yun et al., 2019) and HAN (Wang et al., 2019b) meth- rableperformancetoafullGNNonthePubMeddataset.However,
odsgeneralizeattentionlayersandtransformerstographs. itsperformancedegradesquicklyasthetrainingsetsizedecreases.
ManyGNNsareformulatedintoaunifiedframeworkcalled
MessagePassingNeuralNetworks(Gilmeretal.,2017).
trainingsetinsemi-supervisedscenarios.
KnowledgeDistillationofGNNs. Addressingchallenges
Semi-SupervisedLearning. SSLisanapproachforlever-
relatedtomemoryconsumptionandlatency,severalmeth-
agingunlabeleddata,oftenusedinscenarioswherethesize
odshavebeenproposedtodistillknowledgefromalarge
ofthetrainingsetissmall. ApopularSSLmethod,termed
pre-trainedGNNteachermodeltoasmallerstudentmodel.
pseudo-labeling,usesthemodel’spredictionsaslabelsfor
The student model can be either a smaller GNN model
training (Lee et al., 2013; McLachlan, 1975; Rosenberg
(Guoetal.,2023;Lee&Song,2019;Tianetal.,2023;Yan
etal.,2005;Xieetal.,2020). AnotherprominentSSLap-
etal.,2020;Yangetal.,2020),orstructure-agnosticmodel.
Onesuchmethod,GLNN (Zhangetal.,2021),trainsMLP proachisconsistencyregularization(Bachmanetal.,2014;
Laine&Aila,2016;Sajjadietal.,2016),wherethemodelis
modeltopredictsoft-labelsobtainedfrompre-trainedGNN.
Anotherapproach,NOSMOG(Tianetal.,2022),usesthe enforcedtomaintainconsistentpredictionsthroughrandom
augmentation of its input. FixMatch (Sohn et al., 2020)
same underlying method with the addition of adversarial
featureaugmentationlossandSimilarityDistillationofhid- combinestheseideasinasimplemanner.
den features. NOSMOG also utilizes the graph structure
byconcatenatingpositionalfeaturesobtainedusingDeep- 3.Motivation
Walk(Perozzietal.,2014). WhileNOSMOGoffersbetter
Distillationmethodshaverecentlychallengedtheexisting
accuracyresultsthanstandardGLNN,itsuffersfromhigher
latency induced by positional feature computation. CPF paradigminnodeclassification. Thestandardpracticewith
GNNsistotrainthemodelonallthelabelednodesinthe
(Yang et al., 2021) also uses a non-GNN student model,
graph and use the same model for node classification at
althoughthestudentstillreliesoniterativelabelpropaga-
testtime. Distillationmethodsremovetheneedforusing
tionduringinferencetime, whichincreasestheinference
a GNN at test time, although they still require training a
runningtime.
GNN.TheyusetheGNNforpseudo-labelingunsupervised
NodeclassificationwithoutGNN.Varioustechniquesbe-
nodes. Subsequently, anMLPistrainedtopredictthela-
yondGraphNeuralNetworkshavebeendeveloped. Among belsofthesenodesbasedonitsnodefeaturesonly,without
themareGraph-MLP(Huetal.,2021)whichtrainsanMLP
consideringthefeaturesofitsneighbors. Remarkably,dis-
modelwithaneighborcontrastiveloss. Anothermethod, tillation methods are competitive with GNNs on popular
moresimilartoourapproach,CorrectandSmooth(C&S)
citationandco-purchasenetworkbenchmarks.Thisresultis
(Huangetal.,2020)alsoleveragesthecorrelationbetween confusing,asthegraphstructureappearsbeneficialduring
neighbors’labelstoenhancealinearorashallowMLPpre- training but not during test time. This begs the question:
dictor. However,itdeviatesfromourapproachbyrefining Whyaredistillationmethodssosuccessful?
predictionspost-trainingthroughlabelpropagation. More-
Toaddressthisquestion,weexaminewhetherthepowerof
over,theapplicabilityoftheC&Smethodtotheinductive
theGNNinthiscasecomesfromitsincreasedexpressive-
case(wherenewnodesareaddedtothegraphduringtest
nessattributedtomessagepassingorratherfromitsuseful
time)islimited,anditfocusesonthesupervised,ratherthan
inductivebias. Weplotthenodeclassificationaccuracyof
semi-supervisedsettings(seeSec. 4.3). Also,incontrastto
bothGNNsandnode-levelMLPsasafunctionofthetrain-
theheavyrelianceonlabelsofC&S,asignificantaspectof
ingsetsize(Fig. 1). Theobservedtrendindicatesthatwith
ourapproachaddressesthechallengesarisingfromasmall
anincreaseintrainingsize,theperformancegapbetween
2
ycaruccAClassifyingNodesinGraphswithoutGNNs
CoHOp
Iterative Training
(1) Preprocessing:
(2) Train with
Concatenating
Consistency Loss
Histograms
Train Set Predictions
Input graph, colors (3) Smoothed Pseudo-labels
represent node’s class
Figure2.OverviewofCoHOp.Ourmethodconsistsofthreeelements:(1)augmentingthenodefeaturesbyconcatenatingthemwiththe
histogramofnearbynodelabels(Sec.4.4).(2)trainingwithconsistencylossinadditiontothestandardcross-entropyclassificationloss
(Sec4.2).(3)iterativetrainingwithsmoothedpseudo-labels(Sec.4.3).
node-levelMLPsandGNNsdiminishes. Thissuggeststhat Table1. Notationsummary
MLPsoverfitduetosmalltrainingsizesonpopularnode Notation Explanation
classification datasets; while GNNs are implicitly better G Graph
regularized(i.e.,theyhaveausefulinductivebias). V Setofnodes
V Trainset,subsetofV
Next,weexaminethegapbetweenGNNsandMLPstrained train
V Validationset,subsetofV
on the distillation targets. Here, we observe that the gap val
A Adjacencymatrix
betweenthetwomodelsisnarrow,evenforsmalllabeled
d(v,u) Lengthoftheshortestpathbetweenuandv
trainingsets. Theseexperimentalfindingsleadustocon-
N (v) Setofimmediateneighborsofv
cludethat: thechallengeintheexamineddatasetliesnot
inincreasingmodelexpressivity,butratherindecreasing Nℓ(v) Setofnodesu∈V s.t. d(v,u)≤ℓ
modelsamplecomplexity. GNNsovercomethischallenge
X FeaturematrixinRn×d
x Thei’throwofX
throughausefulinductivebias,whiledistillationovercomes i
C Numberofclasses
itbyincreasingthesizeofthetrainingsetwithGNNpseudo-
y One-hotlabelofv in{0,1}C
labels. i i
Inlightofthisfinding,weask:aretherealternativemethods
forovercomingoverfittingbeyondtrainingafullGNN?The where C is the number of classes. We denote by d(u,v)
answertothisquestionisaffirmativeaswillbeshownin the length of the shortest path in G between the nodes u
Sec.4and5. andv. Furthermore,wedenotethesetofnodesthatcanbe
reachedfromvwithpathsofdistancenolongerthanℓby
Nℓ(v),i.e,Nℓ(v) = {u ∈ V|d(v,u) ≤ ℓ}. Weomitthe
4.CoHOp-ConsistencyandHistogram
superscriptforℓ=1,denotingN1(v)asN (v).
Optimization
Our goal is to predict the labels of all the nodes in
Preliminaries. WearegivenagraphG =(V,A)whereV
V/(V ∪V ). Followingcommonpractices,weonly
train val
isasetofnodes{v ,...,v }andAistheadjacencymatrix,
1 n useV foroptimizingthemodelweightsandV for
train val
i.e,
hyper-parameterselection. Notethatwedescribedthetrans-
(cid:40) ductivesettings,whereallthenodesofthetest-setareac-
1 v isdirectlyconnectedtov
A = i j cessibleduringtraining. Wedescribetheinductivesettings,
ij
0 else where some nodes of the test set are not present during
trainingandthecorrespondingexperimentsinSec. 5.2.1.
We ignore self-loops in the graph, hence A = 0 for all
ii
i∈{1,...,n}. Inaddition,wearegivennodefeaturematrix
4.1.PredictionNetwork
X ∈Rn×dwhereitsi’throwisthefeaturevectorofnodev
i
anddenotedbyx . WedefinethetrainsetV ⊂V and Ourmethodusesalinearmodelasabackboneonallthe
i train
thevalidationsetV ⊂V. Foreachv ∈V ∪V we datasets,exceptfortheOGBdatasets(ogbn-arxivandogbn-
val i train val
aregivenalabely ∈{0,1}C encodedasaone-hotvector, products)whereweusedtwo-layerMLPs. Wedenotethe
i
3ClassifyingNodesinGraphswithoutGNNs
predictorasΨ. Thepredictoristrainedusingthestandard Algorithm1Pseudo-codeofCoHOp
cross-entropyloss,betweenthepredictionofthemodeland Input: Graph G, Train-set V , Iterations T, Confi-
train
theprovidedgroundtruth(GT)labels. Formally,thelossis dencethresholdτ
givenby: ComputehistogramsaccordingtoEq. 7intoH
X ←Concatenate(X,H)
(cid:88)
L GT (Ψ)= CE(Ψ(x i),y i) (1) InitializeΨ,anMLPorlinearmodel.
(xi,y i)∈Vtrain V t1
rain
=V
train
fort=1toT do
4.2.ConsistencyLoss TrainΨonVt andG usingthelossfromEq. 3
train
Yˆ =Ψ(X)
WeshowedinSec.3thattrainingasimplenode-levelclassi-
Y∗ =λ·Yˆ +(1−λ)·AˆYˆ
fiertendstooverfitonstandardnode-classificationdatasets
Vt+1 ←V ∪{i|max (Y∗)>τ}
duetoverylimitedtrainingsetsizes. Toovercomethislim- train train j=1,...,C ij
endfor
itation,weproposetoincorporatepowerfulregularization
lossutilizinggraphpriors. Concretely,weincorporateaho-
mophilicprioronthenodepredictionsusingaconsistency
Thissetisusedfortrainingthepredictionnetworkinthe
loss. Inmanynodeclassificationtasks,suchaspredicting
followingiteration. Theground-truthtrainingnodesremain
attributes of academic papers in a citation network or at-
inthetrainingsetinalliterations,butthepseudo-labeled
tributesofproductsinaco-purchasenetwork,neighboring
high-confidence nodes are recomputed for each iteration.
nodesoftenhavethesamelabel. Consequently,homophilic
I.e. ifahigh-confidencenode becomesalow-confidence
priorsenforcelabelconsistencybetweenneighboringnodes.
nodeinthefollowingiteration,wewillexcludeitfromthe
In practice, our model does not output a single label but
trainingset,unlessitsground-truthlabelwasprovided. This
rather a probability distribution over the classes for each
adaptivemechanismallowsthemodeltorectifyerroneous
node. Thereforeinordertoenforceconsistencyweusea
earlydecisionsastrainingprogresses. Thetrainingsetthat
probabilitydiscrepancymeasurebetweenthepredictionsof
weuseiniterationI+1isgivenbythefollowingrule:
adjacentnodes. Specifically,wecomputetheaveragecross-
entropybetweenpredictedlabeldistributionforanodeand
eachofitsneighbors. Thislosstermtendstoproducecon-
VI+1 =V ∪{v | max (cid:0) ΨI(x )(cid:1) >τ} (4)
sistentpredictionsforadjacentnodes. train train i j=1,...,C i j
Weformulatetheconsistencylosstermas: WhereΨI isthemodelthatwastrainedonthetrainingset
  VI ,andτ istheconfidencethreshold.
train
(cid:88) 1 (cid:88)
L consist(Ψ)=  |N(v )| CE(Ψ(x i),Ψ(x j)) Pseudo-LabelSmoothing. Wefoundthatonmanypopular
i
vi∈V vj∈N(vi) datasets,smoothingthemodelpredictionsonthetargetnode
(2)
with the predictions on the neighboring nodes can result
Ourcompletelossfunctionis: in higher performance. Unfortunately, this improvement
comes at the cost of increased computational complexity
L(Ψ)=L (Ψ)+γ·L (Ψ) (3)
GT consist during inference, as it requires evaluating the model on
neighboring nodes. To address this challenge, we apply
Whereγ isahyper-parametercontrollingconsistencyregu-
predictionsmoothingonlytothepseudo-labelswithinthe
larizationstrength.
iterativetrainingalgorithm(butnotatinferencetime). We
found that with this strategy, our model achieves similar
4.3.Pseudo-LabellingIterations
finalaccuracytothatachievedthroughtest-timesmoothing,
Most standard node classification benchmarks are effec- withoutactuallysmoothingattesttime. Thisobservation
tivelysemi-supervised. I.e.,theirtrainingsetisverysmall suggeststhatthemodellearnstointegratethehomophilic
(sometimesassmallas0.3%ofthetotalnumberofnodes). priorintoitspredictions.
Tohavealargereffectivetrainingsetforourclassifier,we
Thesmoothingtechniqueappliedtopseudo-labelsinvolves
takethepredictedlabelsforsomeunlabellednodesandadd
generatingpredictionsYˆ ∈Rn×C forallnodesaftereach
themtothetrainingset.
trainingiteration. EachrowofYˆ representsthepredicted
Specifically,beforeeachiteration,weaddtothetrainingset distribution vector for a specific node. Subsequently, an
thenodesonwhichthemodelmadepredictionwithhigh adjustedpredictionY∗ iscomputedforeachnodebytak-
confidence along with the existing ground truth training ingaweightedaveragebetweenitsownpredictionandthe
nodes. Wetakethemodelpredictionasthelabelsofthese averagepredictionofitsneighbors. Theweightingfactor
non-ground-truthnodesandrefertothemaspseudo-labels. λ,determinedempiricallyusingthevalidationset,isintro-
4ClassifyingNodesinGraphswithoutGNNs
Table2.CoHOpachievesbetteraverageaccuracythangraph-distillationmethodsandapopularGNNmodel.Resultsshowaccuracyin
thetransductivesettings(higherisbetter).
Dataset SAGE GLNN NOSMOG CoHOp
Cora 80.52±1.77 80.54±1.35 83.04±1.26 82.92±1.15
Citeseer 70.33±1.97 71.77±2.01 73.78±1.54 75.64±1.68
Pubmed 75.39±2.09 75.42±2.31 77.34±2.36 77.22±2.49
A-computer 82.97±2.16 83.03±1.87 84.04±1.01 81.03±1.60
A-photo 90.90±0.84 92.11±1.08 93.36±0.69 93.06±1.56
Arxiv 70.92±0.17 72.15±0.27 71.65±0.29 71.35±0.25
Products 78.61±0.49 77.65±0.48 78.45±0.38 81.71±0.26
Mean 78.52 78.95 80.24 80.42
ducedinthesmoothingprocessthroughtheequation: Here,α∈[0,1]isahyper-parametercontrollingtherelative
importanceoffarawaynodes. Sincey isaone-hotvector
Y∗ =λ·Yˆ +(1−λ)·AˆYˆ (5) j
in {0,1}C, h′ represents a weighted sum of labels from
i
Here,Aˆdenotesthenormalizedadjacencymatrix,suchthat nodes within a local context of v i, with the size of the
thei-throwofAˆYˆ representstheaveragepredictionofthe contextdeterminedbyℓ.
neighborsofnodei. TheresultingY∗servesastherefined
Subsequently, to obtain a normalized histogram, h , we
i
predictionusedinthepseudo-labelingstrategydescribed divideh′ byitssum,thisdescriptorisconcatenatedtothe
above. i
originalinputvectorx .
i
4.4.HistogramsofNeighboringLabels h′
h = i (7)
i (cid:80)C h′
Theneighborhoodofanodemayallowustoinferfurther j=1 ij
information about that specific node label (which is not
asrelevanttofurthernodesfurtheraway). Localinforma- Therequirementofdeterminingthedistancebetweeneach
tionhasbeenincorporatedintopreviousmethodsinvarious nodeinthetrainingsetandallothernodesinthegraph,isa
ways. For example, NOSMOG (Tian et al., 2022) incor- taskwithacomputationalcomplexityofO(|V |·|E|),
train
poratesapositionalembedding,DeepWalk(Perozzietal., where|E|isthenumberofedgesinthegraph(assuming
2014),initsfeatures. Thisembeddingallowstheclassifier therearemoreedgesthannodes). Inthestandardsetting
tolearnaconnectionbetweenanodepositioninthegraph fornode-levelclassificationtasks,thesizeofV isoften
train
andthelabelsofthenodesaroundit. Here,weincorporate verysmall,socomputingthehistogramsisfeasible.
theinformationonthelabelsofneighborsdirectly.
Yet,forlargerdatasets,suchasogbn-products,thiscalcula-
We propose a method for using a descriptor of neighbor tionbecomescumbersome. Toaddressthis,weproposean
labelstoimproveourpredictor. Ourapproachdiffersfrom efficientapproximationforh′. Wecalculatehistogramfor
i
GNNs,asitonlyaggregatestheprovidedlabelsfromneigh- allnodesinthegraphjointlybyspreadingthelabelsusing
boringnodes. Ontheotherhand,GNNsusemessagepass- convolutionoperations. Specifically,thematrixH′whose
ing,whichrequirescomputinghiddenfeaturesfortheentire rowsrepresentun-normalizedhistogramsforeachnode,is
neighborhood. Ourmethodonlyrequiressimplecounting obtainedby:
o mf et nh te tn he ai ngh rub no nrh ino god al Gab Ne Nls ow vh ei rc th hi es ea nm tiru ec nh ew ige ha bk oe rr hr oe oq du .ire-
H′
=(cid:88)ℓ (cid:16) αAˆ(cid:17)k
Y˜ (8)
k=1
Specifically,foreachnodev,thedescriptorisaweighted
Wherethei’throwofY˜ isdefinedby:
histogramderivedfromthelabelsofallnodeswithapath
lengthtov notexceedingℓ. Theweightassignedtoeach (cid:40)
y v ∈V
nodeinthehistogramisdeterminedbythedistancebetween ˜y = i i train
therespectivenodeandv-thatis,theminimallengthofa i 0 v i ∈/ V train
pathbetweenthem.
WenormalizeH′inthesamemannerpresentedinEq. 7.
Thehistogramdescriptorh foranodev iscalculatedby
i i
firstcomputingh′ asfollows: Whenusingconvolutions,thiscomputationtakesarunning
i
timeofO(|E|). However, unlikethepreviousmethodof
(cid:88) (cid:16) (cid:17)
h′ = αd(vi,vj)·y (6) calculating histograms, the labels of some nodes in the
i j
trainingmightleakintothelabelhistogramfeature.Thiscan
vj∈Nℓ(vi)∩Vtrain
5ClassifyingNodesinGraphswithoutGNNs
affectthegeneralization,aswedonothavethisinformation thosepublishedin2018,andtestingiscarriedoutonpapers
attesttime. Weobservedthatusingsmallenoughvaluesfor publishedsince2019. Inthecaseofogbn-products,nodes
αeliminatedthegeneralizationgapduetothisissue. (representing products) are arranged based on their sales
ranking. Thetop8%ofproductsareassignedtothetraining
set, the subsequent top 2% to the validation set, and the
5.Experiments
remainingproductsconstitutethetestset. Thepartitioning
We empirically validate our approach on seven publicly schemeusedbytheOGBdatasetsisdesignedtoperforman
availablegraphbenchmarkdatasets,andcompareittostate- accuratesimulationofreal-lifescenarios.
of-the-artgraph-distillationbasedmethods. Wethenablate
Baselines. We compare our method to recent GNN
thedifferentcomponentsofourmethodsandevaluatethe
distillation-basedmethods: (1)GLNN-Agraphlearning
conditionsunderwhichtheyaresignificant.
based method which uses knowledge distillation from a
pre-trainedGNNteachermodeltoanMLPstudentmodel.
5.1.Datasets
The student is trained to predict the soft-labels obtained
Weevaluateaselectionofdatasetscommonlyusedinthe fromtheteacher. (2)NOSMOG-Inadditiontotrainingan
graphlearningcommunity. Wefollowpreviousworks(Tian MLPonsoft-labels,thismethodaddsanadversarialfeature
etal.,2022;Yangetal.,2021;Zhangetal.,2021)inonly augmentationloss,similaritydistillationofhiddenfeatures
consideringthelargestconnectedcomponentofeachgraph and fusing positional encoding features to the input. We
dataset,andregardtheedgesasundirected. Thestatisticsof further compare to the teacher used in the KD methods -
allthedatasetsarepresentedinApp. Tab.5. GraphSAGEwithGCNaggregationstrategy.
Cora, CiteSeer (Sen et al., 2008) and PubMed (Namata
5.2.Results
et al., 2012) are citation networks where each node rep-
resentsascientificpaper,edgessignifycitationsbetween Ourapproachyieldscompetitiveresultsdespitenotrequir-
papers,andlabelsdenotetheresearchfieldofeachpaper. In ingthetrainingofanygraphneuralnetworks. Asshown
CoraandCiteSeerthefeaturevectorofeachnodeisasparse in table 2, our method achieves better accuracy, on aver-
bag-of-wordsderivedfromthetextofthepaper. PubMed age,acrossallthesevendatasets,comparedtothebaselines.
isconstructedfrommedicalpublications,thenodefeatures TheperformanceofCoHOp oncitationnetworkdatasets
arerepresentedbyTF/IDF(Ramosetal.,2003)weighted isparticularlynoteworthy,whereitachievesmarkedlyim-
wordfrequency. Thelabelsindicatethetypeofdiabetesthe provedresultsonspecificdatasets. Thisobservationholds
publicationfocuseson. true for both standard transductive settings and the more
challenginginductivesettings,asdiscussedinSection5.2.1.
A-Computers and A-Photo (Shchur et al., 2018) are ex-
While our method achieved only slightly better accuracy
tractedfromtheAmazonco-purchasegraph(McAuleyetal.,
thanNOSMOG,itdoessowithouttraininganyGNN.This
2015). Thesedatasetsinvolvenodesrepresentingelectronic
result allows us to obtain interesting insights into the re-
goodssoldonAmazonwebstore. Edgesindicatewhether
quiredinformation;learningasimplemodelthatperforms
twoproductsarefrequentlyboughttogether. Thenodefea-
welloncomplexgraphstructureddata.
tures are product reviews encoded using a bag-of-words
representation. Thelabelsassignedtothenodescorrespond CoHOp achieves strong results on the larger datasets ob-
toproductcategories,withA-Computersencompassingcat- tained from the Open Graph Benchmark (OGB) without
egoriessuchasDesktops,Laptops,Monitors,andsoforth. iterativetraining. Thisisprimarilybecausethesedatasets
A-PhotoincludescategoriessuchasCameras,Lensesetc. includelargetrainingsplits,bothintermsoftheproportion
oftheentiregraphwhichcarrieslabelsattraintime,andin
ogbn-arxiv and ogbn-products are from the Open Graph
termsofabsolutenumberoflabelednodes. Consequently,
Benchmark(OGB)(Huetal.,2020)andarelargerdatasets.
inthesecases,wechosetoincludeonlytheconsistencyloss
TheformerconstitutesacitationnetworkofarXivpapers,
and label-histogram feature augmentations as part of our
whilethelatterisaco-purchasingnetwork.
method.
Datasetsplit. Wefollowtheprotocolusedinpreviousstud-
iesforpartitioningdatasetsintotraining,validation,andtest 5.2.1.INDUCTIVESETTING
sets. Inthetransductivesetting,Cora,CiteSeer,PubMed,
Intheinductivesettingwefurthersplittheunlabelledtest
A-ComputersandA-Photoarepartitionedbyrandomlysam-
set,denotedasU =V/(V ∪V ),intotwodisjointsets:
pling20instancesperclassfortraining, 30instancesper train val
(1)Unseentestnodes,asetofnodesexclusivelyavailable
classforvalidation,andtreatingtheremainingnodesasthe
during inference time and not in training time, denoted
testset.Fortheogbn-arxivdataset,thetrainingisconducted
by U . (2) Observed test nodes, a set of unlabeled
onpaperspublisheduntil2017,validationisperformedon unseen
nodeswithaccessiblefeaturesduringtraining,denotedby
6ClassifyingNodesinGraphswithoutGNNs
Table3. Ablationtable
Dataset CoHOp OnlyIterativeTraining OnlyHistograms OnlyConsistencyLoss Base
cora 82.92 78.39 78.86 73.59 65.26
citeseer 75.64 73.97 73.02 73.38 69.43
pubmed 77.22 71.98 75.77 68.86 68.8
a-computer 81.03 78.75 76.45 72.1 72.68
a-photo 93.06 89.19 86.59 86.43 83.11
Mean 81.97 78.5 78.1 74.9 71.86
U . Unliketheunseentestnodes,theobservedtestnodes Table4.Incorporatingthehomophilyprior,throughconsistency
seen
participate in the consistency loss and may have pseudo- lossandpseudo-labelssmoothing,improvesthemodel’saccuracy
labels. Intheinductivesetting,wetrainourmodelonthe by2.4%onaverage.
graphinducedbyallthenodesinthesetV ∪V ∪
train val
Dataset W.o. HomophilyPrior ∆
U .OnlythenodesofV areusedintheclassification
seen train
loss(Eq. 1). Attrainingtime,wediscardedgesthatconnect cora 79.64 -3.28
tonodesthatareinU . Thetestaccuracyiscomputed citeseer 73.97 -1.67
unseen
onthecombinationofthesetsU andU . pubmed 76.61 -0.61
seen unseen
a-computer 79.21 -1.82
Similarlytothetransductivecase, ourapproachprovides
a-photo 87.44 -5.62
betteraccuracyonaverageacrossallthedatasets,asshown
Arxiv∗ 71.35 0
inApp.Tab. 7.
Products∗ 78.07 -3.64
5.3.Ablationstudy Mean 78.04 -2.38
CoHOpintegratesdiversetechniquesaimedatenhancing
∗Sincewedonotuseiterativetraininginthesedatasets,accuracy
onlyreflectstheimpactoftheconsistencyloss.
asimplepredictorbyusingthegraphstructure. Thesein-
clude: iterative training to address the limitations of an
overlylimitedtrainingset,aconsistencylossforcontrolling AsdepictedinTable4, incorporatingthesetwoelements
thesmoothnessofthemodel’spredictions,andfeatureaug- inourmethodresultedinanaverageimprovementof2.4%
mentationwithlabelhistograms. Thissectionexaminesthe acrossallthedatasetsincludinginthisablationstudy. This
impactofthesetechniquesonouroverallapproach. is explained by the homophilic tendencies of the dataset
sources. Forinstance,incitationnetworks,itisreasonable
Westartbyexaminingourclassifierwithonlythestandard
toexpectthatpapersinthesamefieldmayciteeachother.
cross-entropyclassificationloss. WeseefromTab. 3that
Similarly, in co-purchasing networks, it is plausible that
theresultsofthisnaiveclassifierarefarfrombeingcom-
customerstendtobuyitemsfromthesamecategoryatthe
petitive. We show that each of the proposed components
sametime.
hasasignificantimpactontheperformance. Ascanbeseen
inApp.Tab. 6,pairsofthesecomponentsalreadyachieve Theogbn-arxivdatasetisanoutlierasitisnotpositively
strongresults. Yet,thebestperformanceisachievedwhen affectedbythehomophilyprior. Notably,thisdatasethas
usingourfullmethod. thelargestproportion(53.7%)oflabeledtrainingnodes. As
the number of labels is sufficient, the advantage of using
5.3.1.HOMOPHILYPRIOR additionalpriorssuchashomophilydecreasessignificantly.
Theuseofthehomophilyprior,whichpositsthatneighbor-
5.3.2.HISTOGRAMAPPROXIMATION
ingnodesexhibitpositivelycorrelatedlabels, isreflected
inourmethodintwoways: (1)Theinclusionofaconsis- As described in Sec. 4.4, we augment the input features
tencyloss,whichencouragesthemodeltomaintaincorrela- byconcatenatinghistogramsoflabelsderivedfromthelo-
tionamongneighboringnodes. (2)Smoothingthepseudo- cal context of each node. In large datasets, we speed the
labelsusedduringiterativetraining, furtherincentivizing methodupbyapproximatingthehistogramsusingconvolu-
the model to provide smoothed predictions across graph tions. Inthissectionweanalyzethetrade-offbetweenthe
edges. Inthissection,westudytheadvantagesassociated timesavedbytheapproximationanditspotentialimpacton
withincorporatingthishomophilyprior. accuracycomparedtoexactcomputation. Theresultsare
presentedinFig.3andFig.4. Theresultsshowthatwhile
7ClassifyingNodesinGraphswithoutGNNs
100 93 93.1 teredwhentrainingMLPsondatasetsthathaveveryfew
83 80.8 81 82.3 traininglabels. Interestingly,italsoplaysanimportantrole
80 75.673.6 77.2
71.1 ondatasetsthathadmanytraininglabelsbutcharacterized
byadistributionalmismatchbetweenlabeledtrainingnodes
60
andunlabeledtestnodes.
40
Augmentingnodefeatures. Inmanygraphdatasets, the
20 labelsofneighboringnodesarehighlyinformativefornode
classification. Inthecommonhomophiliccase,thecorre-
0
cora citeseer pubmed computer photo lation between the labels of neighboring nodes is highly
positive. Ourmethodbenefitsfromthispriorbyaugment-
Exact Approx.
ingnodefeatureswiththelabel-histogramsoftheneighbor-
Figure3. Themodel’saccuracyis1.66%higheronaveragewhen ingnodes. Somedistillationmethods,suchasNOSMOG,
usingtheexactformula(Eq.6)forhistogramcalculationcompared augmentnodefeaturesusingpositionalembeddings,partic-
toitsapproximation(Eq.8).
ularlyDeepWalk. However,ourlabel-histogramsaremuch
morecomputationallyefficientthanDeepWalkpositional
400 features.
300
7.Limitations
200
Heterophilicgraphs. Ourmethodisfocusedonutilizing
thehomophilicprior. Yet,somegraphdatasets(Platonov
100
etal.,2023)areheterophilic. Insuchdatasets,whilethela-
0 belofeachnodedoesnottendtobesimilartothelabelofits
2 4 6 8 10 neighbors,labelsofneighboringnodesmaystillcarryinfor-
mation. Somecomponentsofourmethodmaybesuitable
Exact 49 271.4 341 360.4 368.8
forheterophilicgraphs. E.g.,neighborhoodhistogramsmay
Approx. 1.2 1.8 2.6 3.2 4 carryinformationaboutthelabelsofthenode,evenwhen
neighborstendtocomefromdifferentclasses. Adaptingour
ContextSize(ℓ)
methodtoheterophilicgraphsisleftforfuturework.
Figure4.Runningtimesofthepre-processingprocedureofcalcu-
latingthehistogramsusingtheexactcalculation(Eq.6)andusing Dataset-specific variability. While our method outper-
itsapproximation(Eq.8)asfunctionasthecontextsize. formsNOSMOGonaverage,therearecaseswhereNOS-
MOG achieves higher accuracy. This variation suggests
theexacthistogramcalculationresultsinasuperioraccu- thatthedifferentapproachesmightbeinfluencedbydataset-
racy of 1.66% on average, approximating the histograms specificcharacteristics. Furtherinvestigationintospecific
significantlyreducescomputationtime. caseswhereNOSMOGoutperformsmayallowfuturere-
searchtodevelopmethodsthatenjoythebestofallworlds.
6.Discussion
8.Conclusion
Utilizationofunlabelleddata. Ouranalysisofdistillation
methodsrevealedthattheirmainsuccessfactorisefficiently WeintroducedCoHOp,afullyGNN-freemethoddesigned
usingtheunlabeleddata(seeSec. 3). However, todoso for node classification. We initially established that sim-
these methods needed to train GNNs and distill them on plenodeclassifiers,withoutdistillation,canmatchtheper-
theunlabelednodes. Incontrast, ourmethodwasableto formanceofGNNsordistillationmethodsonbenchmark
bypasstrainingaGNNentirelyusingtheproposediterative datasets,ifthenumberoflabelednodesissufficientlylarge.
trainingscheme. ThecorechallengewetackledwasadaptingGNN-lessMLP
methods to the label-poor setting popular in node classi-
Regularization. Ourmethodfurtherexploitstheunlabeled
fication benchmarks. Our proposed method consisted of
nodes by integrating a consistency loss that considers all
threecorecomponents: alabelconsistencyloss, anitera-
thenodesvisibleattrainingtime. Distillationmethodsen-
tive labeling scheme, and feature augmentation using la-
forcesmoothnessonlyindirectlythroughthedistillationof
belhistograms. Ourmethodwasabletosurpassormatch
GNNs,whichtendtoproducesimilarpredictionsforneigh-
nodeclassificationaccuracyonmultiplepopularbenchmark
boring nodes as they aggregate features across adjacent
datasets.
nodes(Chenetal.,2020a). Thisconsistencyregularization
provescrucialinmitigatingoverfittingchallengesencoun-
8
)%(ycaruccA
).cesilim(emiTgninnuRClassifyingNodesinGraphswithoutGNNs
9.Acknowledgment Huang, Q., He, H., Singh, A., Lim, S.-N., and Benson,
A. R. Combining label propagation and simple mod-
This research was partially supported by the Israeli data
elsout-performsgraphneuralnetworks. arXivpreprint
science scholarship for outstanding postdoctoral fellows
arXiv:2010.13993,2020.
(VATAT).
Kipf, T. N. and Welling, M. Semi-supervised classifica-
References tionwithgraphconvolutionalnetworks. arXivpreprint
arXiv:1609.02907,2016.
Bachman,P.,Alsharif,O.,andPrecup,D. Learningwith
pseudo-ensembles. Advancesinneuralinformationpro- Laine, S. and Aila, T. Temporal ensembling for semi-
cessingsystems,27,2014. supervisedlearning. arXivpreprintarXiv:1610.02242,
2016.
Bruna, J., Zaremba, W., Szlam, A., andLeCun, Y. Spec-
Lee, D.-H. et al. Pseudo-label: The simple and efficient
tralnetworksandlocallyconnectednetworksongraphs.
semi-supervised learning method for deep neural net-
arXivpreprintarXiv:1312.6203,2013.
works. In Workshop on challenges in representation
Chen, D., Lin, Y., Li, W., Li, P., Zhou, J., and Sun, X.
learning,ICML,volume3,pp.896.Atlanta,2013.
Measuringandrelievingtheover-smoothingproblemfor
Lee, S. and Song, B. C. Graph-based knowledge distil-
graphneuralnetworksfromthetopologicalview. InPro-
lationbymulti-headattentionnetwork. arXivpreprint
ceedingsoftheAAAIconferenceonartificialintelligence,
arXiv:1907.02226,2019.
volume34,pp.3438–3445,2020a.
Li,G.,Muller,M.,Thabet,A.,andGhanem,B. Deepgcns:
Chen,M.,Wei,Z.,Huang,Z.,Ding,B.,andLi,Y. Simple Can gcns go as deep as cnns? In Proceedings of the
anddeepgraphconvolutionalnetworks. InInternational IEEE/CVFinternationalconferenceoncomputervision,
conferenceonmachinelearning,pp.1725–1735.PMLR, pp.9267–9276,2019.
2020b.
McAuley,J.,Targett,C.,Shi,Q.,andVanDenHengel,A.
Defferrard, M., Bresson, X., and Vandergheynst, P. Con- Image-basedrecommendationsonstylesandsubstitutes.
volutionalneuralnetworksongraphswithfastlocalized In Proceedings of the 38th international ACM SIGIR
spectral filtering. Advances in neural information pro- conferenceonresearchanddevelopmentininformation
cessingsystems,29,2016. retrieval,pp.43–52,2015.
Gilmer,J.,Schoenholz,S.S.,Riley,P.F.,Vinyals,O.,and McLachlan,G.J.Iterativereclassificationprocedureforcon-
Dahl,G.E. Neuralmessagepassingforquantumchem- structinganasymptoticallyoptimalruleofallocationin
istry. InInternationalconferenceonmachinelearning, discriminantanalysis. JournaloftheAmericanStatistical
pp.1263–1272.PMLR,2017. Association,70(350):365–369,1975.
Namata,G.,London,B.,Getoor,L.,Huang,B.,andEdu,U.
Guo,Z.,Zhang,C.,Fan,Y.,Tian,Y.,Zhang,C.,andChawla,
Query-drivenactivesurveyingforcollectiveclassification.
N.V.Boostinggraphneuralnetworksviaadaptiveknowl-
In10thinternationalworkshoponminingandlearning
edgedistillation. InProceedingsoftheAAAIConference
withgraphs,volume8,pp. 1,2012.
on Artificial Intelligence, volume 37, pp. 7793–7801,
2023.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan,G.,Killeen,T.,Lin,Z.,Gimelshein,N.,Antiga,
Hamilton,W.,Ying,Z.,andLeskovec,J. Inductiverepre-
L.,etal. Pytorch: Animperativestyle,high-performance
sentationlearningonlargegraphs. Advancesinneural
deep learning library. Advances in neural information
informationprocessingsystems,30,2017.
processingsystems,32,2019.
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Catasta, M., andLeskovec, J. Opengraphbenchmark: Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Datasets for machine learning on graphs. Advances in Weiss,R.,Dubourg,V.,Vanderplas,J.,Passos,A.,Cour-
neuralinformationprocessingsystems,33:22118–22133, napeau,D.,Brucher,M.,Perrot,M.,andDuchesnay,E.
2020. Scikit-learn: Machine learning in Python. Journal of
MachineLearningResearch,12:2825–2830,2011.
Hu,Y.,You,H.,Wang,Z.,Wang,Z.,Zhou,E.,andGao,Y.
Graph-mlp: Nodeclassificationwithoutmessagepassing Perozzi,B.,Al-Rfou,R.,andSkiena,S. Deepwalk: Online
ingraph. arXivpreprintarXiv:2106.04051,2021. learningofsocialrepresentations. InProceedingsofthe
9ClassifyingNodesinGraphswithoutGNNs
20thACMSIGKDDinternationalconferenceonKnowl- Xie,Q.,Luong,M.-T.,Hovy,E.,andLe,Q.V. Self-training
edgediscoveryanddatamining,pp.701–710,2014. withnoisystudentimprovesimagenetclassification. In
ProceedingsoftheIEEE/CVFconferenceoncomputer
Platonov,O.,Kuznedelev,D.,Diskin,M.,Babenko,A.,and visionandpatternrecognition,pp.10687–10698,2020.
Prokhorenkova, L. A critical look at the evaluation of
gnnsunderheterophily: arewereallymakingprogress? Yan,B.,Wang,C.,Guo,G.,andLou,Y. Tinygnn: Learning
arXivpreprintarXiv:2302.11640,2023. efficient graph neural networks. In Proceedings of the
26thACMSIGKDDInternationalConferenceonKnowl-
Ramos,J.etal. Usingtf-idftodeterminewordrelevancein edgeDiscovery&DataMining,pp.1848–1856,2020.
documentqueries.InProceedingsofthefirstinstructional
Yang, C., Liu, J., and Shi, C. Extract the knowledge of
conferenceonmachinelearning,volume242,pp.29–48.
graph neural networks and go beyond it: An effective
Citeseer,2003.
knowledgedistillationframework. InProceedingsofthe
webconference2021,pp.1227–1237,2021.
Rosenberg, C., Hebert, M., and Schneiderman, H. Semi-
supervisedself-trainingofobjectdetectionmodels. 2005.
Yang,Y.,Qiu,J.,Song,M.,Tao,D.,andWang,X. Distill-
ing knowledge from graph convolutional networks. In
Sajjadi,M.,Javanmardi,M.,andTasdizen,T. Regulariza-
ProceedingsoftheIEEE/CVFConferenceonComputer
tion with stochastic transformations and perturbations
VisionandPatternRecognition,pp.7074–7083,2020.
fordeepsemi-supervisedlearning. Advancesinneural
informationprocessingsystems,29,2016. Yun,S.,Jeong,M.,Kim,R.,Kang,J.,andKim,H.J. Graph
transformernetworks. Advancesinneuralinformation
Sen,P.,Namata,G.,Bilgic,M.,Getoor,L.,Galligher,B.,
processingsystems,32,2019.
andEliassi-Rad,T. Collectiveclassificationinnetwork
data. AImagazine,29(3):93–93,2008. Zhang,S.,Liu,Y.,Sun,Y.,andShah,N. Graph-lessneural
networks: Teachingoldmlpsnewtricksviadistillation.
Shchur,O.,Mumme,M.,Bojchevski,A.,andGu¨nnemann, arXivpreprintarXiv:2110.08727,2021.
S. Pitfalls of graph neural network evaluation. arXiv
preprintarXiv:1811.05868,2018.
Sohn,K.,Berthelot,D.,Carlini,N.,Zhang,Z.,Zhang,H.,
Raffel, C.A., Cubuk, E.D., Kurakin, A., andLi, C.-L.
Fixmatch: Simplifying semi-supervised learning with
consistencyandconfidence. Advancesinneuralinforma-
tionprocessingsystems,33:596–608,2020.
Tian,Y.,Zhang,C.,Guo,Z.,Zhang,X.,andChawla,N.V.
Nosmog:Learningnoise-robustandstructure-awaremlps
ongraphs. arXivpreprintarXiv:2208.10010,2022.
Tian,Y.,Pei,S.,Zhang,X.,Zhang,C.,andChawla,N.V.
Knowledge distillation on graphs: A survey. arXiv
preprintarXiv:2302.00219,2023.
Velickovic,P.,Cucurull,G.,Casanova,A.,Romero,A.,Lio,
P.,Bengio,Y.,etal. Graphattentionnetworks. stat,1050
(20):10–48550,2017.
Wang, M., Zheng, D., Ye, Z., Gan, Q., Li, M., Song, X.,
Zhou, J., Ma, C., Yu, L., Gai, Y., et al. Deep graph
library: Agraph-centric,highly-performantpackagefor
graphneuralnetworks. arXivpreprintarXiv:1909.01315,
2019a.
Wang, X., Ji, H., Shi, C., Wang, B., Ye, Y., Cui, P., and
Yu,P.S. Heterogeneousgraphattentionnetwork. InThe
worldwidewebconference,pp.2022–2032,2019b.
10ClassifyingNodesinGraphswithoutGNNs
A.Appendix
A.1.Implementationdetails
• Weconductedeachexperimentusing10differentrandomseedsandreportedthemeanandstandarddeviationofthe
modelaccuracyonthetestset.
• Thebackboneweemployedconsistsofasinglelinearlayerforalldatasets,exceptforOGBdatasets,whereweutilized
atwo-layerMLPwithhiddendimensionsof512and1024forogbn-productsandogbn-arxiv,respectively.
• Acrossalldatasets,exceptforogbn-productsandogbn-arxiv,weemployed5iterations,asoutlinedinSection4.3.
Withineachiteration,themodelunderwenttrainingfor200epochs,andtheoptimalepochwasdeterminedbasedon
performanceonthevalidationset. Notably,fortheogbn-productsdataset,itwasobservedthatasingleepochand
iterationsufficed,owingtothedataset’ssubstantialsize.
• Theparameterdefiningthesizeofthelocalcontextutilizedforcomputingthehistogram,asexplainedinSection4.4,
wassetto10hops(i.e.,ℓ=10).
• WeusedEquation8forcalculationtheapproximationofthehistogramsinthelargerdatasetsfromOGB.Whileforall
theotherdatasetsweusedtheoriginalformula(Eq. 6).
• Theweightsofthemodelareinitializedonce,thenateachiterationwecontinuethetrainingfornumerousepochs.
PythonLibraries. WeuseDeepGraphLibrary(DGL)(Wangetal.,2019a)forstoringthegraphdatasetsandperforming
graphoperationsonthem. WealsousePyTorch(Paszkeetal.,2019)andscikit-learn(Pedregosaetal.,2011).
Table5. DatasetStatistics.
Dataset #Nodes #Edges #Features #Classes SplitStrategy SplitSizes(train/val/test)
Cora 2,485 5,069 1,433 7 Random 140/210/2,135
Citeseer 2,110 3,668 3,703 6 Random 120/180/1,810
Pubmed 19,717 44,324 500 3 Random 60 /90 /19,567
A-computer 13,381 245,778 767 10 Random 200/300/12,881
A-photo 7,487 119,043 745 8 Random 160/240/7,087
Arxiv 169,343 1,166,243 128 40 Public 53.7%/17.6%/28.7%
Products 2,449,029 61,859,140 100 47 Public 8%/1.6%/90.4%
Table6. Ablationtable
WithHistograms WithConsistency WithConsistency
Dataset CoHOp Base
andIterations andIterations andHistograms
cora 82.92 81.79 80.41 81.75 65.26
citeseer 75.64 74.95 74.85 74.19 69.43
pubmed 77.22 77.04 72.44 76.57 68.8
a-computer 81.03 80.57 79.87 75.38 72.68
a-photo 93.06 91.31 91.14 90.21 83.11
Mean 81.97 81.13 79.74 79.62 71.86
11ClassifyingNodesinGraphswithoutGNNs
Table7.Inductivesetting: Thetestsetisfurtherpartitionedinto80%testsetthatpresentduringtraining(seen)and20%unseentest
(unseen).Theformulausedforcomputingacompositemeasuredenotedasprodisexpressedasfollows:prod=0.8·seen+0.2·unseen.
Datasets Eval SAGE MLP GLNN NOSMOG CoHOp ∆ GLNN ∆ NOSMOG
prod 79.53 59.18 78.28 81.02 81.11 ↑2.83% ↑0.09%
Cora unseen 81.03±1.71 59.44±3.36 73.82±1.93 81.36±1.53 80.09±2.29 ↑6.27% ↓-1.27%
seen 79.16±1.60 59.12±1.49 79.39±1.64 80.93±1.65 81.37±1.74 ↑1.98% ↑0.44%
prod 68.06 58.49 69.27 70.60 72.94 ↑3.67% ↑2.34%
Citeseer unseen 69.14±2.99 59.31±4.56 69.25±2.25 70.30±2.30 71.77±3.37 ↑2.52% ↑1.47%
seen 67.79±2.80 58.29±1.94 69.28±3.12 70.67±2.25 73.23±3.13 ↑3.95% ↑2.56%
prod 74.77 68.39 74.71 75.82 74.51 ↓-0.2% ↓-1.31%
Pubmed unseen 75.07±2.89 68.28±3.25 74.3±2.61 75.87±3.32 74.84±3.39 ↑0.54% ↓-1.03%
seen 74.70±2.33 68.42±3.06 74.81±2.39 75.80±3.06 74.43±3.20 ↓-0.38% ↓-1.37%
prod 82.73 67.62 82.29 83.85 80.66 ↓-1.63% ↓-3.19%
A-computer unseen 82.83±1.51 67.69±2.20 80.92±1.36 84.36±1.57 80.19±2.65 ↓-0.73% ↓-4.17%
seen 82.70±1.34 67.60±2.23 82.63±1.4 83.72±1.44 80.78±2.29 ↓-1.85% ↓-2.94%
prod 90.45 77.29 92.38 92.47 92.11 ↓-0.27% ↓-0.36%
A-photo unseen 90.56±1.47 77.44±1.50 91.18±0.81 92.61±1.09 91.75±1.36 ↑0.57% ↓-0.86%
seen 90.42±0.68 77.25±1.90 92.68±0.56 92.44±0.51 92.20±1.35 ↓-0.48% ↓-0.24%
prod 70.69 55.35 65.09 70.90 71.32 ↑6.23% ↑0.42%
Arxiv unseen 70.69±0.58 55.29±0.63 60.48±0.46 70.09±0.55 71.42±0.34 ↑10.94% ↑1.33%
seen 70.69±0.39 55.36±0.34 71.46±0.33 71.10±0.34 71.29±0.22 ↓-0.17% ↑0.19%
prod 76.93 60.02 75.77 77.33 81.28 ↑5.51% ↑3.95%
Products unseen 77.23±0.24 60.02±0.09 75.16±0.34 77.02±0.19 81.69±0.19 ↑6.53% ↑4.67%
seen 76.86±0.27 60.02±0.11 75.92±0.61 77.41±0.21 81.18±0.18 ↑5.26% ↑3.77%
Mean prod 77.59 63.76 76.83 78.86 79.13 ↑2.31% ↑0.28%
12