Let Your Graph Do the Talking: Encoding Structured Data for LLMs
BryanPerozzi1 BahareFatemi1 DustinZelle1 AntonTsitsulin1
MehranKazemi1 RamiAl-Rfou2 JonathanHalcrow1
Abstract
Answer Answer
How can we best encode structured data into
sequential form for use in large language mod- LLM LLM
els (LLMs)? In this work, we introduce a
parameter-efficientmethodtoexplicitlyrepresent
structured data for LLMs. Our method, Graph-
Token, learns an encoding function to extend Text(G) GraphToken(G)
promptswithexplicitstructuredinformation. Un-
likeotherworkwhichfocusesonlimiteddomains
Is there a Is there a
(e.g.,knowledgegraphrepresentation),ourwork cycle in cycle in
this this
isthefirsteffortfocusedonthegeneralencoding graph? graph?
ofstructureddatatobeusedforvariousreasoning G Q G Q
tasks. We show that explicitly representing the
(a) (b)
graphstructureallowssignificantimprovements
to graph reasoning tasks. Specifically, we see
Figure1.Graph encoding options for a frozen LLM. a) Fixed
acrosstheboardimprovements-upto73%points
encoding,e.g.,(Fatemietal.,2024;Wangetal.,2023b;Stechly
- on node, edge and, graph-level tasks from the etal.,2023),b)ThisworkproposesusingGraphToken,alearned
GraphQAbenchmark. graphpromptfunctiontoexplicitlyencodegraphsinaparameter
efficientway.
1.Introduction
withnewandsupportinginformation,theyarecapableof
Therehasbeenanexplosionofrecentexcitementaround
adaptingtheirparametricbeliefstoeffectivelyincorporate
usingLLMs(Vaswanietal.,2017;Devlinetal.,2018;Rad-
newevidence.
ford et al., 2018; Raffel et al., 2020; Brown et al., 2020;
Touvronetal.,2023;Zhaoetal.,2023)torepresent, pro- AnautomaticwaytoenrichtheinputcontextofanLLM
cess,andanalyzetextualdata. Thesemodelstypicallytake withfactualandfreshinformationisthroughRetrievalAug-
sequentialtextastheirinputbutrecentworkhasextended mentedGeneration(RAG)(Khandelwaletal.,2019;Guu
inputstospatialandtemporalmodalities(e.g.,image(Chen etal.,2020). RAGworksbyaugmentingthepromptwith
etal.,2022)andvideo(Arnabetal.,2021)). additionalrelevant,factualandfreshinformation. Sources
forRAGmightincludewebsearchesorprivatedatabases.
Despite their success, current realizations of LLMs have
Oftenthisinformationisintheformofstructureddata–
noticeable problems – including a tendency to generate
datathathascomplexdependenciesbetweendifferent,dis-
outputswhichareuntrueorunsupportedbytheirprompt,
crete parts of the whole. For example, private relational
commonlyreferredtoashallucinations(Wangetal.,2023a).
databases,socialnetworks,ormoleculesallhaverelational
Anotherintimatelyrelatedissueistheproblemoffreshness,
informationbetweentheirdiscretedataitems.
where the knowledge required to answer a query exists
only after an LLM’s training date (Vu et al., 2023). One Structureddataisubiquitousintherealworld–itsurrounds
mitigation for these problems is through the enrichment ourdailylives–andunderstandinghowtorepresentthis
of the prompt with additional factual and fresh data. As dataoptimallyforitsinclusioninLLMsisacrucialresearch
Kadavathetal.(2022)showed, whenLLMsaresupplied question. The predominant mode of encoding structured
dataforLLMsistousevarioustypesofhand-crafted,text-
1GoogleResearch2WaymoResearch.
basedserialization(Guoetal.,2023;Wangetal.,2023b;
Correspondenceto:BryanPerozzi<bperozzi@acm.org>.
Stechly et al., 2023) (See Figure 1 (a)). This approach
can impose significant decoding complexity for the lan-
1
4202
beF
8
]GL.sc[
1v26850.2042:viXraLetYourGraphDotheTalking:EncodingStructuredDataforLLMs
guage model: from any text description, the model must previousones. Whileearliermodelsweremainlybasedon
firstcorrectlydecodeandunderstandthestructurebeforeit N-gram models (Jurafsky, 2021), newer models adopted
canutilizetheinformation. Recently,Fatemietal.(2024) neuralapproacheswiththeadventofdistributedwordrepre-
demonstrated that pure text representations of structured sentations(Bengioetal.,2000;Mikolovetal.,2013). The
dataareinsufficientforgraphreasoningwithLLMs. They increasedpowerofferedbytheneurallanguagemodelsand
showthatLLMsarenotabletoutilizestructureefficiently the increase in model and dataset sizes has led to a new
when posed with common reasoning tasks that are easily learningparadigmwherelargelanguagemodels(LLMs)are
answered by classical graph algorithms. This highlights pre-trainedinanunsupervisedwayonmassiveamountsof
the need to explore better and more expressive ways of textualdataandareusedasbase(foundation)models(De-
representingstructureddatatoanLLM. vlinetal.,2018;Radfordetal.,2019).Foreachdownstream
application,thebasemodelisfine-tunedonsmallamounts
In this paper, we propose GraphToken (Figure 1 (b)), a
oftask-specificdatatoadaptthemodeltothetask.
parameter-efficientmethodforrepresentingstructureddata
forLLMs. Pre-trainingLLMsontextcorporacloselyre-
latedtothedesiredreasoningtaskcanenhanceperformance,
but it can be computationally expensive, particularly for
largermodels. Additionally,fine-tuningrequiresdomain- Parameter-EfficientFine-Tuning: Withtherapidgrowth
specificdataandhumanexpertise,furtherincreasingcosts. inthenumberofparametersforthestate-of-the-artLLMs
Inspiredbyrecentadvancementsinparameter-efficientfine- (Achiametal.,2023;Teametal.,2023)fine-tuningforeach
tuning (Lester et al., 2021; Xu et al., 2023), our method, downstreamtaskhasbecomeprohibitivelyexpensiveinboth
GraphToken, learns an encoding function that generates time and resources. The goal of parameter-efficient fine-
fine-tunedsoft-tokenprompts. Thesoft-tokenpromptex- tuning(PEFT)(Xuetal.,2023)istoadaptmodelstonew
tendsatextualpromptwithexplicitGraphTokenencoded tasks by updating only a small number of (possibly new)
structural information, allowing us to train only a trivial parameters. ThereareafewdominantPEFTapproaches:
numberofGraphTokenparameterswhencomparedtothe
• Adapter-based approaches(Houlsbyetal.,2019;He
totalLLMparameterbudget.
etal.,2021)holdtheLLMparametersfrozenandadd
Ourworkisthefirsttodevelopparameter-efficientencoders newtrainableparameterstovariouspartsofthemodel,
specificallyforgeneralreasoningtasksonstructureddata. withthemaindifferentiatingfactorbetweendifferent
Wedemonstratethatexplicitlyrepresentingstructureleads approaches being where the adapter parameters are
tosignificantimprovementonthecomprehensiveGraphQA added.
benchmark(Fatemietal.,2024). • LoRAanditsvariants(Huetal.,2021;Edalatietal.,
2022; Valipour et al., 2022) similarly hold the LLM
OurContributions. Weproposethefollowinginnovations:
parametersfrozenandaddnewtrainableparameters,
• GraphToken,anovelparameter-efficientencoderfor however these trainable parameters are added to the
structureddatainclusioninLLMs. frozenLLMparameterssuchthatthefine-tunedLLM
• Extensive experiments on various graph reasoning isidenticalinarchitecturetotheinitialLLM,butwith
tasksshowingthatourmethodsignificantlyimproves onlythoseaddedparametersupdate.
LLMcapabilities. • Partial fine-tuning and partial masking approaches
• AnalysisdemonstratingthattheGraphTokenencoder (Zhaoetal.,2020;Zakenetal.,2021)onlyfine-tune
generalizestobothunseentasksandgraphs. or mask a subset of the LLM parameters – no new
parametersareintroduced.
• Finally, soft-prompt approaches (Li & Liang, 2021;
2.Background
Lesteretal.,2021)prependtokenswithlearnablepa-
WeintroducetherelatedworkinLLMs,promptingmethods, rameterstothebeginningoftheLLMinputortothe
GraphNeuralNetworks(GNNs),graphencoders,andgraph beginningofeveryLLMlayer–likeadapter-basedand
modelscombinedwithLLMs. LoRAapproaches,theyholdtheactualLLMparame-
tersfrozen.
2.1.LargeLanguageModels
Our work falls under the umbrella of soft-prompt ap-
proaches but can be extended to other PEFT approaches
Pre-Trained Large Language Models (LLMs): Lan-
aswell. MostrelevanttoourworkistheworkofLevine
guagemodels(Rosenfeld,2000;Zhaoetal.,2023)areprob-
etal.(2022),wheretheinputisfedintoaseparatetrainable
abilistic models that assign probabilities to sequences of
neuralnetworktoproducethesoft-prompt. Weextendthis
words by breaking the probability of a sequence into the
toencodingstructureddatainputviaaGNNtoproducethe
product of the probabilities of the next tokens given the
LLMsoft-prompt.
2LetYourGraphDotheTalking:EncodingStructuredDataforLLMs
2.2.GraphEncodingwithNeuralNetworks embeddingspaceas“graphtokens.”
The field of graph representation learning (Chami et al., Tomaintainthereasoningandlanguagecapabilitiesofthe
2022)seekswaystorepresentstructureddata(i.e.,discrete LLM,wefreezeitsparametersandteachthegraphencoder
andrelation)inacontinuousdomain–typicallyforusein toalignitsoutputrepresentationswiththeLLMembedding
adownstreammachinelearningtask. Whileseminalwork space: welearnonlythoseparametersofthegraphencoder
likeDeepWalk(Perozzietal.,2014)popularizedthenode during the training process. This reduces computational
embeddingproblem,laterworkutilizedGNNstogeneralize requirementssignificantly(graphencoderparameterscon-
andlearnrepresentationsoftheentiregraph(graphembed- stituteatrivialsumcomparedtotheLLM).Duringourtests,
dings). Many approaches learning graph representations theLLMispromptedwiththeoutputofthegraphencoder
(nodeorgraphembeddings)havefollowed(Tsitsulinetal., andataskaboutthegraph,forexample: ‘Doesthisgraph
2018;Xieetal.,2022). contain a cycle?’. As such, the quality of the results is
purelyafunctionofhowwellthegraphencoderrepresents
2.3.GraphsandLLMs theanswertothetaskandhowwelltheLLMinterpretsthat
output.
The confluence of graph representation learning and rea-
soningwithLLMsisarapidlygrowingfieldofresearch–
3.1.Architecture
likelanguage,structureddatasurroundsusbut,unlikeLLM
input, itisnotsequential. Someofthefirstgraphsinthe AnoverviewofthearchitectureisprovidedinFigure2. At
literatureareknowledgegraphsasin (Agarwaletal.,2020), a high level, our model only has two components. First,
wheretheretrievalcorpusofaretrievalLLMisaugmented the graph encoder takes a graph as input and outputs a
withtext-encodedknowledgegraphs.Yeetal.(2023)utilize fixednumberoftokenembeddings. Thesetokensarethen
instructionfine-tunedLLMsfornodeclassification. Simi- prependedtothesequenceofinitialtokenembeddingsin
larly,Chenetal.(2023b)leverageLLMstoenhancegraph thepromptforanLLM,whichisthendecodedtoproduce
learningmodelsbyincorporatingrichtextattributes. Wang ananswerastext.
etal.(2023b)showedthatlanguagemodelsdemonstratepre-
GraphEncoder. GNNmodelsrangefromsimpleaverag-
liminaryabilitiesforgraphreasoningtasks. Later,Fatemi
ingmethodstocomplexmodelswithmulti-headedattention.
etal.(2024)proposedGraphQA–acomprehensivebench-
Thusthereareawidevarietyofgraphrepresentationspossi-
marktosystematicallyevaluatemodelsforgraphreasoning
ble. Wesuspectthatsomeoftheserepresentationsaremore
tasks–findingthatgraphreasoningtasksarecurrentlydif-
suitedtobeconsumedbyanLLM.Therefore,weconducted
ficultandthatscalinglawsdonotseemtoapply. Finally,
a thorough study that includes several well-known graph
whilethereisagrowingbodyofworkinpre-training,fine-
encoderchoicesinSection4.2. Ourgraphencodertakesthe
tuning,andprompt-tuningwithGNNsbythemselves (Fang
relationalstructureofthegraphasinput,usingsomeformof
etal.,2023;Liuetal.,2023),theresearch,thoughconcep-
graphpositionalencodingasnodefeatures(eitherlearned,
tuallysimilar,differscruciallyfromourwork. GNN-based
Laplacian,oracombinationthereof)-seeSection4.2.2for
approacheslackthetextualunderstandingcapabilitiesthat
details.) Next,weapplyaGNNtoobtainarepresentation
arecentraltotheintegrationofLLMswithgraphlearning
ofthegraph,whichwereadoutusingoneofafewdifferent
andreasoning.
techniquestechniquesdependingonthetask.
• For graph-level tasks (e.g., cycle check) we do
3.GraphToken
globalpoolingforreadout,takingthemeanorsumof
WhenconsideringhowtopassstructureddatatoanLLM therepresentationsoverallofthenodes.
therearelargelytwofamiliesofoptions: (1)encodingitas • Fornode-leveltasks(e.g.,node degree)wesepa-
lexicaltokensforLLMembeddingasin(Fatemietal.,2024) ratelyoutputtherepresentationofeachnode. Thiscan
or (2) encoding it directly to a continuous representation beoptionallyconcatenatedwithagraph-levelpooling.
viaaneuralnetwork–skippinganyLLMtokenembedding. • For edge-level tasks (e.g., edge existence), we
Whilerepresentingagraphasasequenceoflexicaltokens useaglobalrepresentationorthetwonode-levelrepre-
has benefits in terms of interpretability, there is often no sentationsconcatenated.
clearchoiceinwhatordertosequentiallywritethestruc-
Wenotethattheexactoptionforreadoutused(e.g. mean
tureddata. Webelieveatextencodingofstructureddata
orsumpooling)isahyper-parameterchosenduringmodel
prohibitsrich,concise,andexpressiverepresentations. In-
selection. Whicheverthereadouttechnique,thisrepresenta-
stead,ourmethodeschewsrepresentingagraphintextin
tionisthenprojectedontothespaceoftokensusedbythe
favorofdirectlyproducing–usingaGNNasanencoder–
LLMwithafinaldenselayer.
thecontinuousrepresentationsfortheLLMinput. Werefer
tothesenewgraphencoderlearnedsoft-tokensintheLLM LLM.FortheexperimentsinthepaperweusePaLM2(Anil
3LetYourGraphDotheTalking:EncodingStructuredDataforLLMs
GraphToken Encoder
Graph
Graph Convolution Graph Convolution
…
Frozen LLM
Question Output
Is there a cycle in this graph? Is there a cycle in this graph ? Yes, there is a cycle in this graph.
Figure2.AvisualoverviewofthearchitectureofGraphToken.Theframeworktakesagraphandacorrespondingquestionasinput.The
graphencodertakesthegraphandgeneratesgraphtokens.Thequestionistokenizedandembeddedtoquestiontokens.AfrozenLLM
leveragesthegraphandquestiontokenstogenerateananswer.
etal.,2023),however,ourmethodgeneralizestonearlyany level,node-level,andedge-leveltasks.
LLMinusetoday. Forourpurposes,anylanguagemodel • R2: Theperformanceofdifferentgraphconvolution
whichcanacceptasequenceoftokenembeddingsandpro- architecturesvariesacrosstasks. Thishighlightsthe
ducetextisacceptable,solongasitispossibletocompute importanceofcarefullychoosingtherightarchitecture
agradientwithrespecttopartoftheinputsequence. forthespecificgraphreasoningproblemathand.
• R3: By intentionally breaking equivariance, we en-
3.2.Trainingprocedure hanceGraphToken’sgraphreasoningcapabilities.
Ourtrainingprocedureisverysimilartothatusedbysoft
promptingmethods(Lesteretal.,2021). Thetraininginput Datasets. Weconductourexperimentsonthegraphrea-
consistsoftriples(G,T,A)whereGisagraphstructure, soning tasks proposed in GraphQA (Fatemi et al., 2024).
T isastatementorquestiondescribingthetask(e.g.,‘Does This dataset presents multiple graph reasoning problems
thisgraphcontainacycle?’ forcycle check)andAis withdifferentdifficultylevels. Thesetaskscanbecatego-
thegroundtruthanswer(‘Yes,thereexistsacycleinthis rizedasfollows.
graph.’). • Graph-level. node count(countingthenumberof
Intheforwardpass,wecomputetheaugmentedqueryQ=
nodesinagraph),edge count(countingthenum-
E(G)||T(T), concatenating the GraphToken encoding of
berofedgesinagraph),cycle check(determining
thegraphE(G)withtheinitialembeddingofthetasktextual whether a graph contains a cycle), and triangle
representation,T(T). counting (counting the number of triangles in a
graph).
WetrainbyoptimizingthefinalLLMperplexity(totallog- • Node-level. node degree(calculatingthedegree
likelihood), L(A | Q), oftheexpectedanswerAwithre- ofagivennodeinagraph),andconnected nodes
spect to the augmented query, Q. We minimize this loss,
(findingallthenodesthatareconnectedtoagivennode
back-propagatingthegradientofLwithrespecttoE(G)to
inagraph),
the parameters of the GraphToken encoder – keeping all • Edge-level. reachability (finding if there is a
LLMparametersfrozen. WeusetheLionoptimizer(Chen pathfromonenodetoanother),edge existence
etal.,2023a)withalearningrateα=0.05.
(whether a given edge exists in a graph, and
shortest path(findingthelengthoftheshortest
4.Experiments pathfromonenodetoanother).
In this section, we summarize the key experiments con-
Setting. We implemented GraphToken in Tensor-
ductedwithGraphToken. Webeginbyhighlightingsome
flow (Abadi et al., 2015) using the TF-GNN library (Fer-
ofthemostexcitingresultsfromouranalysishere:
ludinetal.,2023). TheLLMusedinourexperimentsisthe
• R1: GraphTokendemonstratessuperiorperformance instruction-fine-tunedFlan(Chungetal.,2022)checkpoint
comparedtoestablishedbaselinesacrossacomprehen- ofPaLM2S(Aniletal.,2023). Experimentswerecarried
siverangeofgraphreasoningtasks,includinggraph- out on Google TPUv3 and TPUv5e (Jouppi et al., 2017).
4
gnidocnE
erutaeF
tuodaeR
pukool
gniddebmE +
gnidocne
lanoitisop
…
…
snekoThparG
snekoT
noitseuQLetYourGraphDotheTalking:EncodingStructuredDataforLLMs
Table1.ResultscomparingGraphTokenagainstpromptengineeringandsoftpromptingongraphreasoningtasksusingtheGraphQA
Test
benchmark(Fatemietal.,2024),bysimpleaccuracy.WeseethatGraphTokensubstantiallyimprovesLLMperformanceonallgraph,
node,andedge-leveltasks. Thefirstbestresultforeachtaskishighlightedinboldandthesecondbestresultishighlightedwithan
underline.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
ZERO-SHOT 0.217 0.124 0.760 0.015 0.140 0.147 0.849 0.445 0.115
ZERO-COT 0.146 0.094 0.323 0.127 0.104 0.088 0.735 0.335 0.336
FEW-SHOT 0.253 0.120 0.374 0.030 0.174 0.124 0.794 0.368 0.227
COT 0.276 0.128 0.580 0.081 0.292 0.131 0.452 0.428 0.386
COT-BAG 0.269 0.125 0.521 0.081 0.280 0.158 0.452 0.373 0.404
SOFT-PROMPT 0.056 0.018 0.832 0.162 0.098 0.068 0.838 0.544 0.462
GraphToken 0.996 0.426 0.956 0.348 0.962 0.264 0.932 0.738 0.638
Modelselectionwasperformedbyevaluatingperformance Results. The results of this experiment, summarized in
onGraphQA Table 1, demonstrate that GraphToken significantly out-
Train
performs existing methods on all graph, node, and edge-
4.1.Experiment1: GraphTokenPerformance leveltasks. WhileSOFT-PROMPTachievesthesecondbest
scoreonsometasks,thisismainlyduetoitsabilitytopre-
Inthisexperiment,werigorouslyevaluatetheperformance
dict majority labels. For example, 82% of the questions
ofGraphTokenagainstthefollowingcomprehensivesetof
in cycle check are about existent cycles. Similarly,
establishedbaselines:
54%ofthequestionsareaboutnon-existentedgesinedge
• ZERO-SHOT. In this approach, the model is given a existence.
taskdescriptionandimmediatelyaskedtoproducethe
desiredoutput. Noadditionalexamplesordemonstra- 4.2.Experiment2: EncoderDesign
tionsareprovided.
FromtheresultsinTable1,wecanseethatgraphencoders
• FEW-SHOT. This approach provides the model with
cansignificantlyimproveaLLM’scapabilityongraphrea-
a few examples of the task and their desired out-
soningtasks. Howeverthechoiceofgraphencodershasa
puts(Brownetal.,2020). Unliketraditionaltraining,
significanteffectontaskperformance. Herewestudyhow
these examples are included directly in the prompt,
differentarchitecturechoicesaffectthequalityofthegraph
allowingthemodeltolearnandadaptduringtheinfer-
representation for a language model’s use, including the
ence.
choicesofthegraphconvolution,thefeaturesavailableto
• COT.Chain-of-thought(CoT)prompting(Weietal.,
thenetwork,andthehyper-parameters.
2022)providesexampleseachshowingstep-by-step
reasoning, teaching the LLM to generate its own
4.2.1.CHOICE: GRAPHCONVOLUTION
thoughtprocessesfortacklingnewtasks.
• ZERO-COT.Zero-shotCoT(Kojimaetal.,2022)builds Thisexperimentinvestigatestheimpactofgraphconvolu-
uponChain-of-Thought(CoT)promptingbyeliminat- tionchoiceontheperformanceofGraphToken.Weevaluate
ingtheneedfortrainingexamples.TheLLMgenerates thefollowingdiversesetofencoders:
itsownstep-by-stepreasoningprocessusingasimple
• GraphConvolutionalNetwork(GCN):Oneofthe
triggerphraselike“Let’sthinkstepbystep”.
earliestGNNs,thismodeldoesmeanpoolingofneigh-
• COT-BAG. BAG prompting (Wang et al., 2023b) ex-
borfeatures,followedbyanon-lineartransformation.
tends COT to improve the performance of LLMs on
(Kipf&Welling,2017)
graph-related tasks by appending “Let’s construct a
• MessagePassingNeuralNetwork(MPNN):Agener-
graphwiththenodesandedgesfirst”totheprompt.
alizationoftheGCN,thismodelallowsformoreflexi-
• SOFT-PROMPT. Thisapproachusesthestandardsoft
bleaggregationofneighborfeatures,andhasadditional
prompt tuning of Lester et al. (2021). It optimizes
nonlinear feature transformations possible. (Gilmer
a global static prompt which is shared across prob-
etal.,2017)
lem instances to improve task performance. Unlike
• Graph Isomorphism Network (GIN): A GNN de-
our proposed method, it does not have access to the
signedspecificallytomaximizetheexpressivenessof
graphinformation,makingtheresultsofthisapproach
themodel,withrespecttoaclassicgraphisomorphim
equivalenttothatofamajorityclassifier.
test. (Xuetal.,2018)
• Multi-HeadAttention(GraphTransformer): This
GNNadaptstransformerstyleattention,allowingitto
5LetYourGraphDotheTalking:EncodingStructuredDataforLLMs
Table2.StudyofindividualgraphencoderperformanceonGraphQA tasks.Notethatthereis‘nofreelunch’here–nosingleencoder
Test
examineddominatesacrossalltasks.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 0.746 0.056 0.964 0.208 0.264 0.264 0.918 0.68 0.604
GIN 0.704 0.052 0.898 0.194 0.252 0.18 0.902 0.65 0.586
MPNN 0.792 0.368 0.956 0.348 0.962 0.25 0.934 0.648 0.638
HGT 0.252 0.084 0.934 0.234 0.266 0.184 0.944 0.718 0.6
MHA 0.912 0.264 0.962 0.266 0.552 0.244 0.932 0.738 0.608
NodeSet 0.996 0.080 0.948 0.198 0.19 0.118 0.942 0.596 0.568
EdgeSet 0.618 0.426 0.964 0.228 0.22 0.096 0.904 0.592 0.568
learndifferentwaysofpassingmessages(basedonthe question: doGNNsneedtobeequivariantinordertogener-
attentionmask). (Dwivedi&Bresson,2021) alize,especiallywithextremelypowerfuldecoders,suchas
• HeterogeneousGraphTransformer(HGT):Another LLMs?
adoptionoftransformerstyleattention(wenotethatit
Weinvestigatethisquestionbytestingthegraphreasoning
canbeappliedtonon-heterogeneousgraphsaswell).
capabilityofGraphTokenwiththreedistinctnodefeaturiza-
(Huetal.,2020)
tionsettings:
• Linear Aggregation In addition to the popular en-
codersfromtheliterature,wealsoevaluatedafamily • LPE:Laplacianpositionalencodingsusingthenormal-
ofmodelswhichuselinearaggregationoffeatures,as izedLaplacianmatrix,asin(Dwivedietal.,2023).
this has been shown to work surprisingly well on a • IDX:uniqueidentityencodingdesignedtobreakthe
numberoftasks(Bojchevskietal.,2020). GNNequivariance.
– NodeSet: Thismodelsimplypoolsallthenode • LPE+IDX:aconcatenationoftheabovetwostrategies.
featuresinthegraphtogether.
Setting. The experimental setup is similar to 4.2. Node
– EdgeSet: Thismodelsimplypoolsalltheedge
featuresofdimensionalityd=4areevaluatedforLPEand
featurestogether(edgefeaturesaredefinedasthe
IDXfeaturization. ModelsusingLPE+IDXcontainsnode
concatenationofitstwonodes’features).
featuresofsized=8.
Setting: The experimental setup is similar to the experi-
Result. TheoutcomeofthisexperimentareshowinFig-
mentinSection4.1. Again,GraphQA performancewas
Train ure3,whereweseethedifferenceofallmodelsfromthe
usedformodelselection,andwereportthecorresponding
SOFTPROMPTbaseline(Lesteretal.,2021)whenevaluted
model’sresultsonGraphQA .
Test onGraphQA . Thecoreresultisthatlearnedpositional
Test
Result: Table 2 shows the results for each model on embeddings(Fig. 3b)generallyoutperformLaplacianposi-
GraphQA . Ingeneral,weseethatthereisnoonemodel tionembeddings(Fig3a)formostencodersandmosttasks.
Test
that consistently dominates across graph encoding tasks. Theseresultsshowthatbreakingequivariancesurprisingly
Instead,weseethatdifferentgraphencoderarchitectures addsadditionalcapabilitiesforgraphreasoningwhenpow-
havestrengthsandweaknessesadvantages. erfuldecodersarepresent. Someadditionalobservations
include:
There is one notable pattern however, is that the simple
linearGNNmodelsperformquitestronglyattheirrespective • CountingTasks. Learnedfeaturesseemtoprovidees-
counting tasks (i.e. NodeSet does well at node count, sential lift for basic counting tasks (node count,
EdgeSet does well at edge count). However models edge count, and node degree) in many en-
withnon-lineareffectsarestillcapableonthesetasks(e.g., coders.
MHAdoeswellatnode count,andMPNNdoeswellon • Combination. Insomeveryinterestingcasesoftask
edge count). andencoder,thecombinationofbothtypesoffeatures
greatly improved model performance (Fig. 3c). For
4.2.2.CHOICE: GNNFEATURES example,GCNandNodeSetsignificantlyimprovedat
thenode counttask.
Recently, positional node encodings (Wang et al., 2022;
• Linearmodels. NodeSet(anencoderwhichdoesnot
Dwivedi et al., 2023; Lim et al., 2023) were proposed to
usethegraphedges)generallybenefitedfromspectral
enhancetheexpressivityofGNNs. Ontheotherhand,in
featuresastheyaddedpreviouslyunseeninformation
molecular modeling it has been shown recently that non-
aboutthegraphstructure.
equivariantencoderscanmatchorexceedqualityofequiv-
ariantones(Wangetal.,2023c). Thisraisesamoregeneral
6
raenil-noN
raeniLLetYourGraphDotheTalking:EncodingStructuredDataforLLMs
(a)SpectralFeatures(LPE) (b)LearnedFeatures(IDX) (c)LearnedandSpectralFeatures(LPE+IDX)
Figure3.Effectofvaryingnodefeaturesusedinthegraphencoder.ResultsshownareperformancedifferencefromtheSOFTPROMPT
baselineonGraphQA .Weseethatbreakingequivariancevialearnedfeatures(Fig.3b)generallyimprovethemodelperformance,but
Test
thecombinationoflearnedandspectralfeatures(Fig.3c)provesuniquelypowerfulforsomeencoders.
Table3. Totalnumberofparametersinthegraphencoder.
Body Head
GCN 17,152 1.1×107
GIN 17,152 1.1×107
MPNN 83,968 1.1×107
HGT 198,788 1.1×107
MHA 101,376 1.1×107
NodeSet 0 4.1×105
EdgeSet 0 7.4×105
Figure4.UMAP(McInnesetal.,2018)projectionofGraphToken
embeddingsproducedbytwodifferentencoders,coloredbythe
diameterofagraph.Weplotall8-nodegraphs.
4.2.3.PARAMETERUSAGEINGRAPHTOKEN
Setting:Weconsiderthegraphconvolutionevaluationfrom
eters(Touvronetal.,2023). Meanwhiletheclosedsource
Section4.2.1,usingLPEfeatureswithdimensionalityd=4.
PaLM1modelcontains540billionparameters(Chowdhery
Thegraphencodershavealatentspaceofsize128. Wethen
etal.,2022). Inlightofthis,wecanseethatGraphTokenis
projectthisintoapromptembeddingwithapproximately
highlyparameter-efficient,andsignificantlyimprovesthe
80,000parametersinGraphToken.
graphreasoningcapabilityofaLLMwhilebarelyadding
Results: Table3showsthenumberofparametersusedin anyparametersatall.
the graph encoder. Here ‘body’ refers to the number of
parameters in the graph encoder itself, and ‘Head’ refers 5.Discussion
totheparametersinthetransformationlayertothehigher-
dimensionalLLMtokenspace. SofarwehaveexaminedtheperformancebenefitsofGraph-
Token,andthedesignchoicesnecessarywhenbuildinga
Itsalsoinsightfultoconsiderthenumberofparametersused
graphencoder. Howeverseveralquestionsremain:(1)What
ineachofthemodels. Table3specifiestotalnumberofpa-
exactlyaretheencodersdoing,and(2)doesitgeneralize?
rametersusedbyeachGNNarchitecture. Wenotethatthis
Inthissectionweseektoprovidesomeinsight(ifnotan-
sizeisdominatedbythetotalnumberofparametersinthe
swers)tothesequestions,andlaythefoundationsforfuture
projectionlayertothetokenspace(roughly11million).Out
work.
ofallnon-lineararchitectures,attention-basedones(MHA
andHGT)addthemostencoder-basedparameters. Ingen-
5.1.GraphEncoderAnalysis
eral,thesizeofourgraphencodermodelsvariesfrom17kto
199kparameters. Thisissignificantlysmallerthantypical ThissectionstudiesthepropertieslearnedbyGraphToken’s
LLMs,whichcurrentlyoftencontaintensorhundredsofbil- graphencodersbydirectlyexaminingtherepresentations
lionsofparameters. Forexample,theopen-sourceLLama2 they produce. We study both the in-distribution and out-
languagemodelscalesfrom7billionto70billionparam- of-distributionpropertiesoftheseencoders. Weconsider
7LetYourGraphDotheTalking:EncodingStructuredDataforLLMs
Table4.Predictingbipartitenessusinggraphencoderstrainedfordifferenttasks,measuredonallgraphswith8nodes. Observethat
graphencoderstrainedoncycle checkandtriangle countinggeneralizewelltobipartitenessdetection.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 53.82 53.28 55.46 50.00 50.00 54.64 50.00 48.48 51.60
GIN 51.09 53.27 52.74 51.91 53.26 53.57 51.36 52.17 52.18
MPNN 68.01 71.34 56.82 76.82 60.13 60.95 61.77 62.58 54.37
HGT 50.00 54.35 68.53 95.03 50.27 59.81 68.85 74.58 50.00
MHA 50.27 66.39 87.00 72.14 58.74 66.38 51.63 54.12 64.45
NodeSet 56.55 57.38 56.30 55.74 56.29 56.28 55.73 57.93 56.56
EdgeSet 50.82 50.82 50.82 50.55 50.54 50.54 50.82 50.82 50.54
9 tasks in total: total number of edges; maximum node 5.2.FutureWork
degree;graphdiameter;numberoftriangles;averagelocal
Thisworkopensupanexcitingnewavenueofexploration
clusteringcoefficient;largestcorenumber;averageshortest
forreasoningwithstructureddataandLLMs. Somepoten-
pathlength;testingplanarity;testingbipartiteness.
tialavenuesthatweconsiderparticularlyexcitinginclude:
Onebenefitofstudyinggraphsisdataavailability:forsmall-
• This work just considers existing convolutions and
enoughgraphs,wecangenerateallpossiblegraphsexhaus-
measurestheireffectiveness. Anobviousandessential
tively using geng (McKay et al., 1981). The evaluation
next step is designing graph convolutions that best
goesasfollows: First,wetrainanencoderonataskfrom
supportLLMsinvariousgraphreasoningtasks.
GraphQA (e.g. cycle check). Then, to evaluate the
• Evaluatingtheusefulnessofthisapproachforfactual
cross-taskgeneralizabilityofthedifferentencoderswetrain
grounding. Can we improve the ability of an LLM
akNNclassifier(orregressor)withk =5ontherepresen-
to answer questions about the data using prompting
tationsof(i)anexhaustivesetofconnectedgraphswith8
overknowledgegraphs? CouldanLLManswernovel
nodes(calledgraph8cinBalcilaretal.(2021))and(ii)an
questions about a molecule given a GNN-produced
exhaustivesetoftreegraphswith15nodes. Wenotethatbe-
representationofit?
causewearegeneratingalargesetofgraphs(e.g. thereare
• GraphTokenimprovesperformancewithbrokenequiv-
11117graphsofsize8)andonlytrainedonGraphQA
Train ariance. Can this result inform other problems with
(1000instances),thevastmajorityofthegraphsweareus-
verystrongdecodermodels?
inghereareunseen. Asanillustration,aUMAP(McInnes
• This work examines how a GNN can be used to an
etal.,2018)visualizationoftheembeddingsforall8node
enhanceLLMs,butwhataboutthereverse? Canwe
graphsusingtwoGNNencodersispresentedinFigure4.
useanLLMtointerrogateaGNNtobetterexplainits
Results. Sincewepresentalotofexperimentsandit’shard resultsorprovidehigherqualityanswers?
tocoverthemall,wefocushereonthetaskofpredicting
whether a graph is bipartite and outsource the rest to the
6.Conclusions
Appendix. From the basic graph theory we know that, if
thereisatriangleoranoddcycleinagraph,itcannotbe Inthisworkwehavestudiedthestructureddataencoding
bipartite. Therefore, we expect triangle counting problemforLLMs. Ournovelmethod,GraphToken,learns
andcycle checktrainingobjectivestoperformwellon agraphembeddingfunctionthroughthegradientsprovided
thistask. InTable4wecanseethatthisispreciselywhat byafrozenLLM.GraphTokenisespeciallywellsuitedfor
happens,withattention-basedmethodstakingthelead. This projectingstructureddataintolatent‘promptspace’. Itisa
isaninterestingexampleofgeneralizationfromthegraph parameter-efficientmethodasitrequiresonlytrainingthe
encoderstoanewtask. graphencoderanddoesnotupdateLLMparameters. Our
extensiveexperimentalanalysisacross9graphreasoning
Overall, there is a significant performance gap between
tasksshowsthatGraphTokengreatlyimprovesgraphrea-
differentgraphencoders,MPNNandattention-basedones
soninginLLMs–weobserveuptoa73%improvementon
beinggenerallythebest.Weobservesignificantcorrelations
theGraphQAbenchmark.
in performance of in-distribution learning – for instance,
GraphTokentrainedonedge countperformsthebeston There is still much to do! We believe that our approach
edge countprediction. Whatisinterestingisthatnode isfundamentalforadaptingnewstructureddatasourcesto
countperformscomparablyhere.Thissuggeststhatgraph LLMs(whichareexpensiveandtimeconsumingtotrain),
encoderslearnsomeuniversalfeaturesthatareapplicable andpresentsaveryattractivewayofimprovingfundamental
tomanydifferentdownstreamtasks. problemsinLLMs,includinghallucinations,factuality,and
freshness.
8
raenil-noN
raeniLLetYourGraphDotheTalking:EncodingStructuredDataforLLMs
Acknowledgements Bojchevski,A.,Gasteiger,J.,Perozzi,B.,Kapoor,A.,Blais,
M.,Ro´zemberczki,B.,Lukasik,M.,andGu¨nnemann,S.
WethankOleksandrFerludin,JohannesGasteiger,Silvio
Scalinggraphneuralnetworkswithapproximatepager-
Lattanzi, Vahab Mirrokni and Jan Pfeifer for discussions
ank. InKDD,2020. Citedonpage6.
aboutthework.
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
References Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell,A.,etal. Languagemodelsarefew-shotlearners.
Abadi,M.,Agarwal,A.,Barham,P.,Brevdo,E.,Chen,Z.,
NeurIPS,2020. Citedonpages1and5.
Citro,C.,Corrado,G.S.,Davis,A.,Dean,J.,Devin,M.,
Ghemawat,S.,Goodfellow,I.,Harp,A.,Irving,G.,Isard, Chami,I.,Abu-El-Haija,S.,Perozzi,B.,Re,C.,andMur-
M.,Jia,Y.,Jozefowicz,R.,Kaiser,L.,Kudlur,M.,Lev- phy,K. Machinelearningongraphs: Amodelandcom-
enberg,J.,Mane´,D.,Monga,R.,Moore,S.,Murray,D., prehensivetaxonomy. JMLR,2022. Citedonpage3.
Olah,C.,Schuster,M.,Shlens,J.,Steiner,B.,Sutskever,
I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A.,
V.,Vie´gas,F.,Vinyals,O.,Warden,P.,Wattenberg,M., Padlewski, P., Salz, D., Goodman, S., Grycner, A.,
Wicke, M., Yu, Y., andZheng, X. TensorFlow: Large- Mustafa,B.,Beyer,L.,etal. PaLI:Ajointly-scaledmul-
scalemachinelearningonheterogeneoussystems,2015. tilinguallanguage-imagemodel. InICLR,2022. Cited
URLhttps://www.tensorflow.org/. Software onpage1.
availablefromtensorflow.org. Citedonpage4.
Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Liu,
Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I., Y., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., et al.
Aleman,F.L.,Almeida,D.,Altenschmidt,J.,Altman,S., Symbolic discovery of optimization algorithms. arXiv
Anadkat,S.,etal. Gpt-4technicalreport. arXivpreprint preprintarXiv:2302.06675,2023a. Citedonpage4.
arXiv:2303.08774,2023. Citedonpage2.
Chen,Z.,Mao,H.,Li,H.,Jin,W.,Wen,H.,Wei,X.,Wang,
Agarwal, O., Ge, H., Shakeri, S., and Al-Rfou, R. S.,Yin,D.,Fan,W.,Liu,H.,etal.Exploringthepotential
Knowledgegraphbasedsyntheticcorpusgenerationfor of large language models (llms) in learning on graphs.
knowledge-enhancedlanguagemodelpre-training. arXiv arXivpreprint2307.03393,2023b. Citedonpage3.
preprintarXiv:2010.12688,2020. Citedonpage3.
Chowdhery,A.,Narang,S.,Devlin,J.,Bosma,M.,Mishra,
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, G., Roberts, A., Barham, P., Chung, H. W., Sutton,
D.,Passos,A.,Shakeri,S.,Taropa,E.,Bailey,P.,Chen, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,
Z., et al. Palm 2 technical report. arXiv preprint S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
arXiv:2305.10403,2023. Citedonpages3and4. N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,
Pope,R.,Bradbury,J.,Austin,J.,Isard,M.,Gur-Ari,G.,
Arnab,A.,Dehghani,M.,Heigold,G.,Sun,C.,Lucˇic´,M., Yin,P.,Duke,T.,Levskaya,A.,Ghemawat,S.,Dev,S.,
and Schmid, C. Vivit: A video vision transformer. In Michalewski,H.,Garcia,X.,Misra,V.,Robinson,K.,Fe-
ICCV,2021. Citedonpage1. dus,L.,Zhou,D.,Ippolito,D.,Luan,D.,Lim,H.,Zoph,
B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,
Balcilar,M.,He´roux,P.,Gauzere,B.,Vasseur,P.,Adam,S., S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,
andHoneine,P. Breakingthelimitsofmessagepassing Lewkowycz,A.,Moreira,E.,Child,R.,Polozov,O.,Lee,
graphneuralnetworks. InICML,2021. Citedonpage8. K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,
Catasta,M.,Wei,J.,Meier-Hellstern,K.,Eck,D.,Dean,
Battaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez- J., Petrov, S., and Fiedel, N. Palm: Scaling language
Gonzalez,A.,Zambaldi,V.,Malinowski,M.,Tacchetti, modelingwithpathways,2022. Citedonpage7.
A.,Raposo,D.,Santoro,A.,Faulkner,R.,Gulcehre,C.,
Song,F.,Ballard,A.,Gilmer,J.,Dahl,G.,Vaswani,A., Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Allen, K., Nash, C., Langston, V., Dyer, C., Heess, N., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,
Wierstra, D., Kohli, P., Botvinick, M., Vinyals, O., Li, S.,etal. Scalinginstruction-finetunedlanguagemodels.
Y., and Pascanu, R. Relational inductive biases, deep arXivpreprintarXiv:2210.11416,2022. Citedonpage4.
learning,andgraphnetworks,2018. Citedonpage13.
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K. Bert:
Bengio,Y.,Ducharme,R.,andVincent,P. Aneuralprob- Pre-training of deep bidirectional transformers for lan-
abilistic language model. NIPS, 2000. Cited on page guageunderstanding. arXivpreprintarXiv:1810.04805,
2. 2018. Citedonpages1and2.
9LetYourGraphDotheTalking:EncodingStructuredDataforLLMs
Dwivedi,V.P.andBresson,X. Ageneralizationoftrans- Hu, Z., Dong, Y., Wang, K., and Sun, Y. Heterogeneous
former networks to graphs, 2021. Cited on pages 6 graphtransformer,2020. Citedonpage6.
and13.
Jouppi,N.P.,Young,C.,Patil,N.,Patterson,D.,Agrawal,
Dwivedi,V.P.,Joshi,C.K.,Luu,A.T.,Laurent,T.,Bengio, G.,Bajwa,R.,Bates,S.,Bhatia,S.,Boden,N.,Borchers,
Y.,andBresson,X.Benchmarkinggraphneuralnetworks. A.,Boyle,R.,Cantin,P.-l.,Chao,C.,Clark,C.,Coriell,J.,
JMLR,24(43):1–48,2023. Citedonpage6. Daley,M.,Dau,M.,Dean,J.,Gelb,B.,Ghaemmaghami,
T.V.,Gottipati,R.,Gulland,W.,Hagmann,R.,Ho,C.R.,
Edalati, A., Tahaei, M., Kobyzev, I., Nia, V. P., Clark,
Hogberg,D.,Hu,J.,Hundt,R.,Hurt,D.,Ibarz,J.,Jaffey,
J. J., and Rezagholizadeh, M. Krona: Parameter ef-
A.,Jaworski,A.,Kaplan,A.,Khaitan,H.,Killebrew,D.,
ficient tuning with kronecker adapter. arXiv preprint
Koch,A.,Kumar,N.,Lacy,S.,Laudon,J.,Law,J.,Le,
arXiv:2212.10650,2022. Citedonpage2.
D.,Leary,C.,Liu,Z.,Lucke,K.,Lundin,A.,MacKean,
G.,Maggiore,A.,Mahony,M.,Miller,K.,Nagarajan,R.,
Fang, T., Zhang, Y., Yang, Y., Wang, C., and Chen, L.
Narayanaswami, R., Ni, R., Nix, K., Norrie, T., Omer-
Universalprompttuningforgraphneuralnetworks,2023.
nick,M.,Penukonda,N.,Phelps,A.,Ross,J.,Ross,M.,
Citedonpage3.
Salek,A.,Samadiani,E.,Severn,C.,Sizikov,G.,Snel-
Fatemi,B.,Halcrow,J.,andPerozzi,B. Talklikeagraph: ham, M., Souter, J., Steinberg, D., Swing, A., Tan, M.,
Encoding graphs for large language models. In ICLR, Thorson,G.,Tian,B.,Toma,H.,Tuttle,E.,Vasudevan,
2024. Citedonpages1,2,3,4,and5. V., Walter, R., Wang, W., Wilcox, E., and Yoon, D. H.
In-datacenterperformanceanalysisofatensorprocessing
Ferludin,O.,Eigenwillig,A.,Blais,M.,Zelle,D.,Pfeifer, unit. SIGARCHComput.Archit.News,2017. Citedon
J.,Sanchez-Gonzalez,A.,Li,W.L.S.,Abu-El-Haija,S., page4.
Battaglia,P.,Bulut,N.,Halcrow,J.,deAlmeida,F.M.G.,
Gonnet,P.,Jiang,L.,Kothari,P.,Lattanzi,S.,Linhares, Jurafsky,Dan;Martin,J.H. N-gramlanguagemodels. In
A.,Mayer,B.,Mirrokni,V.,Palowitch,J.,Paradkar,M., SpeechandLanguageProcessing(3rded.).2021. Cited
She, J., Tsitsulin, A., Villela, K., Wang, L., Wong, D., onpage2.
and Perozzi, B. TF-GNN: Graph neural networks in
tensorflow,2023. Citedonpages4and13. Kadavath,S.,Conerly,T.,Askell,A.,Henighan,T.,Drain,
D.,Perez,E.,Schiefer,N.,Hatfield-Dodds,Z.,DasSarma,
Gilmer,J.,Schoenholz,S.S.,Riley,P.F.,Vinyals,O.,and N.,Tran-Johnson,E.,etal. Languagemodels(mostly)
Dahl,G.E. Neuralmessagepassingforquantumchem- knowwhattheyknow. arXivpreprintarXiv:2207.05221,
istry,2017. Citedonpage5. 2022. Citedonpage1.
Guo, J., Du, L., and Liu, H. Gpt4graph: Can large lan-
Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L.,
guagemodelsunderstandgraphstructureddata? anem-
and Lewis, M. Generalization through memorization:
pirical evaluation and benchmarking. arXiv preprint
Nearest neighbor language models. arXiv preprint
arXiv:2305.15066,2023. Citedonpage1.
arXiv:1911.00172,2019. Citedonpage1.
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.
Kipf,T.N.andWelling,M. Semi-supervisedclassification
Retrieval augmented language model pre-training. In
withgraphconvolutionalnetworks,2017. Citedonpage
ICML,2020. Citedonpage1.
5.
He,R.,Liu,L.,Ye,H.,Tan,Q.,Ding,B.,Cheng,L.,Low,
Kojima,T.,Gu,S.S.,Reid,M.,Matsuo,Y.,andIwasawa,Y.
J.-W.,Bing,L.,andSi,L. Ontheeffectivenessofadapter-
Largelanguagemodelsarezero-shotreasoners. NeurIPS,
basedtuningforpretrainedlanguagemodeladaptation.
35:22199–22213,2022. Citedonpage5.
arXivpreprintarXiv:2106.03164,2021. Citedonpage2.
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Lester, B., Al-Rfou, R., and Constant, N. The power of
DeLaroussilhe, Q., Gesmundo, A., Attariyan, M., and scaleforparameter-efficientprompttuning,2021. Cited
Gelly,S. Parameter-efficienttransferlearningfornlp. In onpages2,4,5,and6.
ICML,2019. Citedonpage2.
Levine, Y., Dalmedigos, I., Ram, O., Zeldes, Y., Jan-
Hu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang, nai, D., Muhlgay, D., Osin, Y., Lieber, O., Lenz, B.,
S.,Wang,L.,andChen,W. Lora:Low-rankadaptationof Shalev-Shwartz,S.,Shashua,A.,Leyton-Brown,K.,and
largelanguagemodels. arXivpreprintarXiv:2106.09685, Shoham, Y. Standing on the shoulders of giant frozen
2021. Citedonpage2. languagemodels,2022. Citedonpage2.
10LetYourGraphDotheTalking:EncodingStructuredDataforLLMs
Li,X.L.andLiang,P.Prefix-tuning:Optimizingcontinuous Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
promptsforgeneration.arXivpreprintarXiv:2101.00190, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
2021. Citedonpage2. Bhosale,S.,etal. Llama2: Openfoundationandfine-
tuned chat models. arXiv preprint arXiv:2307.09288,
Lim,D.,Robinson,J.,Zhao,L.,Smidt,T.,Sra,S.,Maron,
2023. Citedonpages1and7.
H.,andJegelka,S. Signandbasisinvariantnetworksfor
spectral graph representation learning. In ICLR, 2023. Tsitsulin, A., Mottin, D., Karras, P., Bronstein, A., and
Citedonpage6. Mu¨ller,E. Sgr: Self-supervisedspectralgraphrepresen-
tationlearning. arXivpreprintarXiv:1811.06237,2018.
Liu,Z.,Yu,X.,Fang,Y.,andZhang,X. Graphprompt: Uni-
Citedonpage3.
fyingpre-traininganddownstreamtasksforgraphneural
networks. InProceedingsoftheACMWebConference
Valipour,M.,Rezagholizadeh,M.,Kobyzev,I.,andGhodsi,
2023,2023. Citedonpage3.
A. Dylora: Parameter efficient tuning of pre-trained
modelsusingdynamicsearch-freelow-rankadaptation.
McInnes, L., Healy, J., and Melville, J. Umap: Uniform
arXivpreprintarXiv:2210.07558,2022. Citedonpage2.
manifold approximation and projection for dimension
reduction. arXivpreprintarXiv:1802.03426,2018. Cited
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
onpages7and8.
L.,Gomez,A.N.,Kaiser,Ł.,andPolosukhin,I.Attention
McKay, B. D. et al. Practical graph isomorphism. 1981. isallyouneed. NeurIPS,30,2017. Citedonpage1.
Citedonpage8.
Vu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei,
Mikolov,T.,Chen,K.,Corrado,G.,andDean,J. Efficient J.,Tar,C.,Sung,Y.-H.,Zhou,D.,Le,Q.,andLuong,T.
estimationofwordrepresentationsinvectorspace. arXiv Freshllms: Refreshinglargelanguagemodelswithsearch
preprintarXiv:1301.3781,2013. Citedonpage2. engineaugmentation,2023. Citedonpage1.
Perozzi,B.,Al-Rfou,R.,andSkiena,S. Deepwalk: online Wang, C., Liu, X., Yue, Y., Tang, X., Zhang, T., Jiayang,
learningofsocialrepresentations. InKDD,2014. Cited C.,Yao,Y.,Gao,W.,Hu,X.,Qi,Z.,Wang,Y.,Yang,L.,
onpage3. Wang,J.,Xie,X.,Zhang,Z.,andZhang,Y. Surveyon
factualityinlargelanguagemodels: Knowledge,retrieval
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.,
anddomain-specificity,2023a. Citedonpage1.
etal. Improvinglanguageunderstandingbygenerative
pre-training. 2018. Citedonpage1. Wang,H.,Yin,H.,Zhang,M.,andLi,P. Equivariantand
stablepositionalencodingformorepowerfulgraphneural
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
networks. InICLR,2022. Citedonpage6.
Sutskever,I.,etal. Languagemodelsareunsupervised
multitasklearners. OpenAIblog,1(8):9,2019. Citedon
Wang,H.,Feng,S.,He,T.,Tan,Z.,Han,X.,andTsvetkov,
page2.
Y. Canlanguagemodelssolvegraphproblemsinnatural
language? InNeurIPS,2023b. Citedonpages1,3,and5.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena,M.,Zhou,Y.,Li,W.,andLiu,P.J. Exploring
Wang, Y., Elhag, A. A., Jaitly, N., Susskind, J. M., and
thelimitsoftransferlearningwithaunifiedtext-to-text
Bautista,M.A. Generatingmolecularconformerfields.
transformer. TheJournalofMachineLearningResearch,
arXivpreprintarXiv:2311.17932,2023c. Citedonpage
21(1):5485–5551,2020. Citedonpage1.
6.
Rosenfeld,R. Twodecadesofstatisticallanguagemodeling:
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,
Wheredowegofromhere? ProceedingsoftheIEEE,88
Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought
(8):1270–1278,2000. Citedonpage2.
prompting elicits reasoning in large language models.
Stechly, K., Marquez, M., and Kambhampati, S. Gpt- NeurIPS,2022. Citedonpage5.
4 doesn’t know it’s wrong: An analysis of iterative
prompting for reasoning problems. arXiv preprint Xie, Y., Xu, Z., Zhang, J., Wang, Z., and Ji, S. Self-
arXiv:2310.12397,2023. Citedonpage1. supervisedlearningofgraphneuralnetworks: Aunified
review. IEEETPAMI,2022. Citedonpage3.
Team,G.,Anil,R.,Borgeaud,S.,Wu,Y.,Alayrac,J.-B.,Yu,
J.,Soricut,R.,Schalkwyk,J.,Dai,A.M.,Hauth,A.,etal. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How
Gemini: afamilyofhighlycapablemultimodalmodels. powerful are graph neural networks? arXiv preprint
arXivpreprintarXiv:2312.11805,2023. Citedonpage2. arXiv:1810.00826,2018. Citedonpage5.
11LetYourGraphDotheTalking:EncodingStructuredDataforLLMs
Xu, L., Xie, H., Qin, S.-Z. J., Tao, X., and Wang, F. L.
Parameter-efficient fine-tuning methods for pretrained
languagemodels:Acriticalreviewandassessment.arXiv
preprintarXiv:2312.12148,2023. Citedonpage2.
Ye, R., Zhang, C., Wang, R., Xu, S., and Zhang, Y.
Natural language is all a graph needs. arXiv preprint
arXiv:2308.07134,2023. Citedonpage3.
Zaken, E. B., Ravfogel, S., and Goldberg, Y. Bitfit:
Simple parameter-efficient fine-tuning for transformer-
based masked language-models. arXiv preprint
arXiv:2106.10199,2021. Citedonpage2.
Zhao,M.,Lin,T.,Mi,F.,Jaggi,M.,andSchu¨tze,H. Mask-
ingasanefficientalternativetofinetuningforpretrained
languagemodels.arXivpreprintarXiv:2004.12406,2020.
Citedonpage2.
Zhao,W.X.,Zhou,K.,Li,J.,Tang,T.,Wang,X.,Hou,Y.,
Min,Y.,Zhang,B.,Zhang,J.,Dong,Z.,etal.Asurveyof
largelanguagemodels. arXivpreprintarXiv:2303.18223,
2023. Citedonpages1and2.
12LetYourGraphDotheTalking:EncodingStructuredDataforLLMs
A.Appendix
A.1.GraphEncoders
Notation. Webrieflydescribethenotationwewilluse. ThegraphG=(V,E)containsthesetofV nodesandE edges.
Whilewewillonlydiscusssimplegraphs,everythingdiscussedcanbeextendedtoheterogeneousgraphsw.l.o.g.(Battaglia
etal.,2018;Ferludinetal.,2023).
UsingthenotationofFerludinetal.(2023),aGNNhastwoprimaryoperations. First,anextstatefunction(NEXTSTATE)
whichcomputesthehiddenstateh ofanode(oredge,m )giveninformationfromitsneighborsanditspreviousstate,
v (u,v)
andanaggregationfunction(EDGEPOOL)whichpoolsinformationforanode’simmediateneighborhoodintoafixedsize
representation. Moreformally,wecansaythatthenextstateofanodeis:
h(i+1) = NEXTSTATE(i+1)(h(i),m(i+1)).
v V v v
Thenthepooledmessagesm(i+1)aredefinedasfollows:
v
m(i+1) = NEXTSTATE(i+1)(h(i),h(i),m(i) ),
(u,v) E u v (u,v)
m(i+1) = EDGEPOOL(i+1)(h(i),{m(i+1) |u∈N(v)}).
v v (u,v)
DifferentrealizationsoftheNEXTSTATEandEDGEPOOLfunctionscanimplementawidevarietyofGNNoperations. This
canincludepowerfulmodelswhichuseTransformerstyleattentioninsteadoftheprovidedgraphedges(Dwivedi&Bresson,
2021).
The architecture of NodeSet and EdgeSet is shown in Figure 5. Other GNN models have graph convolutions before
node/edgestatesarereadout.
MLP MLP MLP
MLP MLP
MLP
MLP
pool pool pool
MLP
MLP
MLP
project project project
pool
… … …
Graph Tokens
project
(a)NodeSetarchitecture (b)EdgeSetarchitecture
…
Figure5.Figurativeillustrationsofset-basedGNNarchitecturesemployedinthepaper.Wepoolrepresentationsfromeithernodesor
edges,transformthemviaanGMrLaPphw iTthoksheanresdweights,pool,andprojecttotheGraphTokenspace.
A.2.Additionalexperiments
Wepresentadditionalresultsforgraphencoderanalysis. Tables5–15presentadditionalresultsonmoregraphproperties,as
wellasexperimentsontree-structuredgraphsofsize15. Ingeneral,completegraphpopulationsdemonstratesignificantly
betterperformancethantrees–wecanattributethattothefactthatGraphTokenwastrainedondiversesetsofdata,and
treesaresomewhatout-of-distribution. Nevertheless,forallconsideredcasesthebestoverallencodermodelachievedbetter
resultsthanna¨ıvesetencodings.
13
Graph Tokens Graph TokensLetYourGraphDotheTalking:EncodingStructuredDataforLLMs
Table5.AveragelocalclusteringcoefficientMSEmeasuredonallconnectedgraphswith8nodes.Wehighlightthebestperformanceper
trainingtaskincolumns.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 1.62 1.67 2.12 4.49 4.49 1.73 4.49 16.57 3.75
GIN 2.18 2.29 2.45 2.60 2.44 2.31 3.73 2.88 3.37
MPNN 1.03 0.95 1.38 0.81 1.50 1.34 1.68 1.87 1.47
HGT 2.63 2.25 2.08 1.23 2.49 2.17 1.90 1.62 2.52
MHA 2.69 1.01 1.23 0.96 1.56 1.25 2.08 1.59 1.29
NodeSet 2.59 2.56 2.59 2.59 2.58 2.60 2.58 2.58 2.56
EdgeSet 2.22 2.22 2.22 2.22 2.24 2.23 2.22 2.22 2.23
Table6.Degreeaccuracyonallconnectedgraphswith8nodes.Wehighlightthebestperformancepertrainingtaskincolumns.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 57.46 56.65 52.46 40.09 40.09 57.42 40.09 15.73 40.26
GIN 56.86 56.30 54.55 48.75 55.59 57.56 40.14 50.81 44.83
MPNN 69.45 69.60 67.19 71.84 64.56 67.62 61.37 58.66 63.18
HGT 55.20 55.70 56.54 60.17 56.62 57.65 58.02 59.06 55.46
MHA 54.86 64.33 62.86 65.63 61.67 63.22 56.98 61.60 63.97
NodeSet 54.66 54.91 54.98 55.06 54.78 54.64 54.50 54.94 54.72
EdgeSet 63.48 63.37 63.07 63.55 63.08 63.37 63.47 63.06 63.44
Table7.DiameterAccuracyonallconnectedgraphswith8nodes.Wehighlightthebestperformancepertrainingtaskincolumns.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 66.86 67.81 66.70 37.37 37.37 68.91 37.37 52.13 55.13
GIN 66.06 64.87 63.97 61.09 64.98 66.43 37.80 60.65 54.82
MPNN 76.92 76.86 73.63 78.33 74.78 77.18 74.42 69.56 76.23
HGT 63.97 65.24 66.88 70.45 65.30 68.45 69.64 68.97 66.04
MHA 63.76 74.17 76.00 74.03 73.50 74.71 68.45 69.32 72.95
NodeSet 67.28 67.24 67.01 66.97 66.81 67.19 67.09 66.87 66.79
EdgeSet 66.99 66.51 66.63 66.83 66.65 67.02 66.60 66.93 66.90
Table8.k-CoreAccuracyonallconnectedgraphswith8nodes.Wehighlightthebestperformancepertrainingtaskincolumns.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 69.49 69.15 66.61 58.33 58.33 69.16 58.33 25.18 61.55
GIN 68.03 65.98 64.85 62.67 66.74 67.84 58.84 63.34 59.08
MPNN 87.42 87.54 81.81 88.63 80.30 83.48 80.08 71.01 82.05
HGT 63.92 65.29 67.00 70.01 65.44 67.32 68.35 70.08 65.13
MHA 64.30 80.80 73.49 80.81 76.98 78.83 69.43 74.21 75.92
NodeSet 68.23 68.74 68.50 68.71 68.07 67.99 68.85 68.17 68.70
EdgeSet 66.30 65.78 65.58 66.15 65.76 65.91 65.94 65.77 65.71
Table9.#edgesAccuracyonallconnectedgraphswith8nodes.Wehighlightthebestperformancepertrainingtaskincolumns.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 38.91 39.19 35.94 11.60 11.60 40.24 11.60 2.19 14.58
GIN 38.13 37.33 36.57 31.66 37.74 38.34 11.88 31.45 25.92
MPNN 86.58 86.72 53.15 84.56 52.12 66.01 50.70 41.96 59.95
HGT 35.63 37.45 38.23 40.39 37.14 37.80 39.68 39.74 36.86
MHA 35.85 55.32 45.04 53.52 47.89 49.44 39.69 42.84 46.17
NodeSet 40.06 40.14 39.40 40.15 39.97 39.72 39.88 39.79 39.89
EdgeSet 37.93 38.11 38.05 37.92 38.05 37.67 37.64 37.82 37.91
14
raenil-noN
raeniL
raenil-noN
raeniL
raenil-noN
raeniL
raenil-noN
raeniL
raenil-noN
raeniLLetYourGraphDotheTalking:EncodingStructuredDataforLLMs
Table10.PlanarityAUConallconnectedgraphswith8nodes.Wehighlightthebestperformancepertrainingtaskincolumns.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 74.18 73.76 72.61 50.00 50.00 74.74 50.00 50.00 49.44
GIN 77.35 73.00 72.06 69.37 74.86 75.85 50.73 68.97 61.58
MPNN 86.14 86.52 84.16 86.64 83.74 85.17 84.32 77.84 85.55
HGT 69.24 71.41 71.02 74.07 71.47 72.20 72.20 73.59 71.55
MHA 69.96 80.87 78.35 80.46 81.53 81.21 74.98 78.29 80.58
NodeSet 78.41 78.76 78.86 78.82 78.18 78.54 78.72 78.76 78.78
EdgeSet 72.17 71.64 72.06 72.20 71.93 72.11 72.01 72.27 72.01
Table11.ShortestpathMSEonallconnectedgraphswith8nodes.Wehighlightthebestperformancepertrainingtaskincolumns.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 2.27 2.24 2.31 6.07 6.07 2.06 6.07 11.09 3.75
GIN 2.57 2.77 2.83 2.93 2.52 2.54 4.84 3.09 3.61
MPNN 0.29 0.29 0.76 0.31 0.71 0.49 0.75 1.58 0.51
HGT 3.03 2.64 2.27 1.60 2.60 2.14 1.80 1.95 2.81
MHA 3.04 0.71 0.95 0.78 1.01 0.74 1.74 1.55 1.05
NodeSet 2.35 2.35 2.35 2.36 2.36 2.35 2.34 2.36 2.34
EdgeSet 2.99 2.99 2.99 2.99 2.97 2.97 2.99 2.99 2.99
Table12.#oftrianglesMSEonallconnectedgraphswith8nodes.Wehighlightthebestperformancepertrainingtaskincolumns.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 132.94 129.03 164.53 316.07 316.07 127.17 316.07 690.03 293.53
GIN 152.13 168.35 182.95 201.64 169.71 156.16 251.23 200.45 251.65
MPNN 8.33 7.51 32.08 4.56 51.90 27.18 51.04 124.89 41.73
HGT 191.14 170.71 165.88 126.92 172.84 160.29 156.10 136.22 175.45
MHA 197.36 30.27 96.56 27.10 59.58 52.42 138.48 80.22 60.72
NodeSet 167.81 168.72 167.33 167.40 167.90 167.96 168.57 169.38 166.13
EdgeSet 181.44 181.21 181.18 181.32 180.86 179.44 181.08 181.68 181.40
Table13. DegreeAccuracyonalltreeswith15nodes.Wehighlightthebestperformancepertrainingtaskincolumns.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 53.57 55.15 55.24 25.91 25.91 54.86 25.91 11.08 36.51
GIN 60.35 58.79 56.36 55.11 59.88 68.04 42.01 66.72 55.25
MPNN 79.37 78.36 59.18 72.35 62.38 65.90 57.37 57.33 58.45
HGT 54.88 55.33 55.34 58.65 54.33 58.84 57.27 57.43 55.34
MHA 59.17 61.61 60.38 57.18 54.99 61.00 52.29 58.56 53.95
NodeSet 65.64 66.32 65.93 66.10 66.13 65.95 66.28 66.22 65.82
EdgeSet 69.59 69.87 69.44 69.40 69.86 69.56 69.32 69.55 69.66
Table14. DiameterAccuracyonalltreeswith15nodes.Wehighlightthebestperformancepertrainingtaskincolumns.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 50.77 50.36 49.54 25.97 25.97 50.01 25.97 6.77 26.64
GIN 58.29 54.44 52.24 49.41 51.47 59.62 24.11 58.77 46.27
MPNN 54.24 54.68 54.97 59.29 67.65 63.80 54.13 52.05 59.48
HGT 57.15 54.88 54.90 57.58 57.05 65.22 54.51 58.70 53.07
MHA 53.95 56.63 60.41 54.62 53.39 56.07 52.85 55.17 51.70
NodeSet 61.89 62.68 62.74 62.36 61.99 61.93 62.34 62.49 62.40
EdgeSet 56.57 56.19 56.27 56.83 56.25 56.53 56.31 56.72 56.84
15
raenil-noN
raeniL
raenil-noN
raeniL
raenil-noN
raeniL
raenil-noN
raeniL
raenil-noN
raeniLLetYourGraphDotheTalking:EncodingStructuredDataforLLMs
Table15. ShortestpathMSEonalltreeswith15nodes.Wehighlightthebestperformancepertrainingtaskincolumns.
GraphTasks NodeTasks EdgeTasks
Method Nodecount Edgecount Cyclecheck Trianglecounting Nodedegree Connectednodes Reachability Edgeexistence Shortestpath
GCN 12.95 12.31 12.62 26.17 26.17 12.22 26.17 49.78 21.71
GIN 9.57 10.69 11.32 11.88 11.03 8.37 19.35 9.76 14.39
MPNN 4.19 4.54 9.82 4.92 6.87 6.10 11.06 12.10 11.01
HGT 10.57 10.96 11.65 9.09 12.56 8.17 10.76 9.26 10.98
MHA 10.49 9.88 9.51 11.22 12.75 10.52 13.31 10.09 12.78
NodeSet 10.20 10.05 10.13 10.11 10.17 10.21 10.07 10.18 10.03
EdgeSet 9.92 9.87 9.92 9.93 9.88 9.88 10.01 9.91 9.87
16
raenil-noN
raeniL