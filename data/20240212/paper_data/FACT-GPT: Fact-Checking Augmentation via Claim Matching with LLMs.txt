FACT-GPT:Fact-CheckingAugmentationviaClaimMatchingwithLLMs
EUNCHEOLCHOI,EMILIOFERRARA,UniversityofSouthernCalifornia,LosAngeles,CA,90007,USA
Oursocietyisfacingrampantmisinformationharmingpublichealthandtrust.Toaddressthesocietalchallenge,weintroduce
FACT-GPT,asystemleveragingLargeLanguageModels(LLMs)toautomatetheclaimmatchingstageoffact-checking.FACT-GPT,
trainedonasyntheticdataset,identifiessocialmediacontentthatalignswith,contradicts,orisirrelevanttopreviouslydebunked
claims.OurevaluationshowsthatourspecializedLLMscanmatchtheaccuracyoflargermodelsinidentifyingrelatedclaims,closely
mirroringhumanjudgment.Thisresearchprovidesanautomatedsolutionforefficientclaimmatching,demonstratesthepotentialof
LLMsinsupportingfact-checkers,andoffersvaluableresourcesforfurtherresearchinthefield.
1 INTRODUCTION
Theurgentneedforextensivefact-checkinghasbeendrivenbytherapidproliferationofmisinformationondigital
platforms[24].Thefact-checkingprocess,thoughcomplexandlabor-intensiveencompassingseveralstagesfromclaim
identificationtodrawingfinalconclusions,[5,7]couldbemademoreefficientthroughAItools[1].Itis,however,
criticaltonotethatacompleteautomationcouldunderminejournalisticprinciplesandpractices[18],therebyindicating
thegoalliesinenhancing,notreplacing,humanexpertise[4].
Akeyelementinmonitoringthespreadoffalseclaimsacrossvariouscommunicationplatformsisclaimmatching,
wherenewinstancesofpreviouslyfact-checkedclaimsareidentified[21].Theimportanceofclaimmatchingstems
fromthetendencyoffalseclaimstobereusedandreiteratedindifferentformats[18].Effectiveclaimmatchingcan
expeditetheearlydetectionofmisinformation,contentmoderation,andautomateddebunking[8].
Thispaperexploresthepotentialutilizationoflargelanguagemodels(LLMs)tosupporttheclaimmatchingstagein
thefact-checkingprocedure.Ourstudyrevealsthatwhenfine-tunedappropriately,LLMscaneffectivelymatchclaims.
Ourframeworkcouldbenefitfact-checkersbyminimizingredundantverification,supportonlineplatformsincontent
moderation,andassistresearchersintheextensiveanalysisofmisinformationfromalargecorpus.
SYNTHETIC TRAINING DATA
CLAIMS DEBUNKED BY FACT-CHECKERS
Vaccinated People Emit Bluetooth Signals
GENERATED POSTS BY GPT
Crazy day, I'm fully vaccinated and now apparently I'm a walking
Bluetooth signal! Get connected, folks! #VaccineBluetooth
A SMALLER
FINE-TUNED GPT
Which of
the following ⇒
best describes
REAL-LIFE POSTS the relationship ⇒¬
omg my dad got vaccinated yesterday between
and I just connected him to bluetooth ⇏ ∧ ⇏¬
Fig.1. OverviewofFACT-GPT,ourframeworkaimedatassistingtheclaimmatchingstageofthefact-checkingprocess
2 RELATEDWORK
TheIntersectionofFact-checkersandAIFact-checkersareinstrumentalinthefightagainstmisinformation,asthey
havedevelopedreliablepracticesandprinciplesovertime[12].TheintegrationofAIintothefact-checkingprocess
1
4202
beF
8
]LC.sc[
1v40950.2042:viXraChoiandFerrara
shouldbeconductedwithgreatcare,withthegoalofenhancingefficiencywithoutunderminingestablishedprinciples
[18].AImodelsthatsupportratherthanreplacefact-checkersaremorelikelytobeembraced.Whilefact-checkers
haveshowninterestinAItoolsforidentifyingclaimsandassessingtheirvirality[1],theymaintainskepticismabout
AIentirelyreplacinghumanintervention,therebyhighlightingtheindispensableroleofhumanjudgment.
LLMsinAnnotationTasksLargeLanguageModels(LLMs)havegarneredsignificantinterestduetotheirpotential
toautomatediverseannotationtasks.DespiteplatformslikeAmazonMechanicalTurk(MTurk)enablingcrowd-sourced
annotation,creatingcomprehensivedatasetsforcomplextaskscontinuestobeexpensive.Giventheirflexiblenature,
LLMs’performanceinvariousannotationtasksisbeingscrutinized.ResearchhasevaluatedLLMsincontextssuchas
fact-checking[10],annotatingtweets[6],andbeyond.GeneratingsynthetictrainingdatatoenhanceLLMs’performance
inclassificationtaskshasalsobeenexplored[3].However,itiscrucialtoacknowledgeLLMs’inherentlimitations.Their
probabilisticnatureimpliesthattheiroutputscanvaryaccordingtopromptsandparameters[20].Whencompared
totask-specificmodels,ChatGPToftenunderperform[14,26],underliningtheneedformodelsthatarespecifically
designedandutilizedforcertaintasks.
3 PROPOSEDFRAMEWORK
3.1 TaskDescription
ToevaluatevariousLargeLanguageModels’(LLMs)performanceinclaimmatching,weemployatextualentailment
task.Textualentailmentinvolvescategorizingrelationshipsbetweenpairsofstatementsintothreeuniqueclasses:
Entailment,Neutral,andContradiction.ApairisclassifiedasEntailment iftheveracityofStatementAinherently
impliesthetruthofStatementB.ThepairislabeledasNeutralifthetruthfulnessofstatementAdoesn’taffirmor
denystatementB’struth.It’sidentifiedasContradictionifStatementA’struthinfersthatStatementBisfalse.Textual
entailmenttasksarecenteredaroundeverydayreasoningratherthanstrictlogic,hencehumanjudgmentandcommon
senseestablishthegroundtruth[17,19].Thiskindoftaskhaspreviouslyshowedeffectivenessindetectingrumors
[25].
Claimmatchingtaskscanbeconfiguredinvariousformsincludingbutnotlimitedtotextualentailment[16],ranking
[15,22],andbinarydetectiontasks[13].Definingclaimmatchingasa3-classentailmenttaskposesbothadvantages
andchallenges.Identifyingcontradictingpairsisimportantassuchrebuttalsplayacrucialroleinmitigatingthespread
ofmisinformation[8,23].However,it’schallengingduetothescarcityofcontradictionpairsinreal-worldinstances
[17].
3.2 Datasets
Inthisstudy,wefocusonmisinformationrelatingtopublichealth,specificallyCOVID-19relatedfalseclaimsthathave
beenfact-checked.1,225Falseclaimsdebunkedbyprofessionalfactcheckersin2020and2021wereobtainedfrom
GoogleFactCheckToolsandPolitiFact.
3.2.1 SyntheticTrainingDatasetsGeneration. WeutilizedLargeLanguageModels(LLMs)togeneratesynthetictraining
data,allowingforthecreationofabalanceddatasetspecificallydesignedforclaimmatchingtasks.Fine-tuninglanguage
modelsonsyntheticdatasetscanenhancetheiradaptabilitytospecifictasknuances,potentiallyleadingtobetter
classificationaccuracy.Inaddition,fine-tuningsmallermodelsreducesthecomputationalcostinvolvedinlarge-scale
operationswhilemakingiteasiertocustomizethesemodelsbasedonemergingnewclaims.
2FACT-GPT:Fact-CheckingAugmentationviaClaimMatchingwithLLMs
Table1. Descriptivestatisticsfortestdata.
Label Count Percentage
ENTAILMENT 647 52.8%
NEUTRAL 433 35.3%
CONTRADICTION 90 7.3%
(Two-wayties) 55 4.5%
TOTAL 1225 100%
Togeneratesynthetictrainingdata,weutilizedthreelanguagemodelsavailableviatheOpenAIAPIortheHuggingFace
InferenceAPI:GPT-4,GPT-3.5-Turbo,andLlama-2-70b-chat-hf.Usingacollectionofdebunkedclaimsasabasis,we
generatedtweetsthateithersupported,wereneutralto,orcontradictedtheseclaims.Togeneratevariedstylesinthe
outputsbythelanguagemodels,wesetthetemperatureparameterat1.Figure2providesanexampleofapromptused
fordatageneration.Atotalof3,675synthetictweetsweregeneratedfromeachmodel,ensuringanequaldistribution
acrossallthreecategories.
3.2.2 GroundTruthDataset. OurmethodforcreatingagroundtruthdatasetisillustratedinFigure3.Initially,we
pairedtweetsfromthepubliclyavailableCoronavirusTwitterDataset[2]withdebunkedfalseclaims,consideringboth
tokenandsemanticsimilarities.Thisprocessgeneratedauniquesetof1,225pairsconsistingoftweetsandclaims.
ExperiencedannotatorsonAmazonMechanicalTurkthenclassedeachofthesepairsintooneofthethreecategories.
Thefinalcategorizationwasbasedonwhichclassreceivedthemajorityofvotes,creatingafullyannotatedtestdataset,
asillustratedinTable1.
3.3 Experiments
3.3.1 Baselines. Weestablishedcomparisonbenchmarksbyassessingtheperformanceofseveralpre-trainedLarge
LanguageModels(LLMs),includingGPT-4,GPT-3.5-Turbo,Llama-2-13b,andLlama-2-7b,againsthumanannotations.
System GenerateTWEETsothatifTWEETistrue,thenCLAIMisalsotrue.Bebrief.Donotstartasentencewith’Just’.
Input VaccininatedpeopleemitBluetoothsignals.
Output Crazyday.I’mfullyvaccinatedandnowapparentlyI’mawalkingBluetoothsignal!Getconnected,folks!#VaccineBluetooth
Fig.2. Exampleofsynthetictweetgenerationprompts
TWEET BM25 TWEET S-BERT
(±14 days, (Rerank, TWEET
Top 1000) Top 1)
Human
Annotators
CLAIM CLAIM
Fig.3. Workflowoftestdataconstruction
3ChoiandFerrara
System WhichofthefollowingbestdescribestherelationshipbetweenTWEETandCLAIM?
YoumustchoosefromENTAILMENT,NEUTRAL,orCONTRADICTION.
IfTWEETistrue:
(ENTAILMENT)thenCLAIMisalsotrue.
(NEUTRAL)CLAIMcannotbesaidtobetrueorfalse.
(CONTRADICTION)thenCLAIMisfalse.
Input TWEET:omgmydadgotvaccinatedyesterdayandIjustconnectedhimtobluetooth
CLAIM:VaccininatedpeopleemitBluetoothsignals.
Output ENTAILMENT
Fig.4. Exampleofanentailmenttaskprompt
Weadjustedthetemperaturesettingto0(or0.01forLlamamodels)tomaketheannotationprocessasconsistentas
possible.WethenpresentedentailmenttaskpromptstoeachLLMandcollectedtheirresponses.
3.3.2 Fine-tuning. OurassessmentofFACT-GPT’seffectivenessinvolvedfine-tuningGPT-3.5-Turbo,Llama-2-13b,
andLlama-2-7bwiththesynthetictrainingdatasetoutlinedin3.2.1.Weallocated80%ofthedatafortrainingand
theremaining20%forvalidation.GPT-3.5-Turbounderwentfine-tuningusingOpenAI’sFine-tuningAPI.Meanwhile,
fortheLLaMamodels,weappliedLoRA(Low-RankAdaptation,[11])inLLaMa-Factory [9],whichisanefficient
tuningframeworkforLLMs.BERT-basemodelwasfine-tunedonGPT-4-generatedtrainsettoprovideanadditional
benchmark.Eachmodelwentthroughthreeepochs(fiveforBERT-base)offine-tuningonasingleA100GPU.
3.3.3 Results. TheoverallperformanceofFACT-GPTsaresummarizedinTable2.Notably,modelsfine-tunedon
syntheticdatasetsexhibitedsuperiorperformanceincomparisontothepre-trainedversions.Therewasaconsistent
patternintheperformanceamongthefine-tunedmodels,withallmodelsexhibitingimprovedoutcomeswhenfine-
tunedusingtrainingdatageneratedbyGPT-4asopposedtothosegeneratedbyGPT-3.5-TurboorLlama-2-70b.This
trendemphasizesthesignificanceofthequalityoftrainingdataindeterminingtheeffectivenessoftheresultingmodels.
Table3revealsthatourtop-performingmodelsaremoreadeptatclassifyingEntailmentandNeutrallabels,butface
challengeswithContradictionlabels.ThissuggeststhatourFACT-GPTsareproficientindeterminingtherelevanceor
irrelevanceofsocialmediapoststotheoriginaldebunkedclaims.However,giventhatrebuttalstofalseclaimsplaya
crucialroleinpreventingthespreadofmisinformation[8,23],futureworkshouldfocusonimprovingthedetectionof
contradictoryposts.
4 DISCUSSION
Thisstudyunderscoresthepotentialoflargelanguagemodels(LLMs)inaugmentingthefact-checkingprocess,particu-
larlyduringtheclaimmatchingphase.OurresearchdemonstratesthatLLMshavethecapacitytodiscernentailment
relationshipsbetweensocialmediapostsanddebunkedclaims.Importantly,ourstudyrevealsthatappropriately
fine-tuned,smallerLLMscanyieldaperformancecomparabletolargermodels,therebyofferingamoreaccessible
andcost-effectiveAIsolutionwithoutcompromisingquality.However,whileourmodelsexcelindetectingwhether
socialmediacontentisrelevanttoorirrelevantfromdebunkedclaims,theyshowstruggleswithcategorizingpoststhat
contradicttheseclaims.Thisisanareathatrequiresfurtherrefinement,giventheimportanceofrebuttalsincurbing
thespreadofmisinformation.
4FACT-GPT:Fact-CheckingAugmentationviaClaimMatchingwithLLMs
Table2. Overallperformanceofpre-trainedandfine-tunedmodels.
Model TrainSetFrom Precison Recall Accuracy
BERT-base GPT-4 .46 .46 .46
GPT-4 — .64 .70 .63
GPT-3.5-Turbo GPT-4 .64 .68 .73
GPT-3.5-Turbo .51 .59 .57
Llama-2-70b .57 .65 .60
— .56 .61 .58
Llama-2-13b GPT-4 .63 .69 .71
GPT-3.5-Turbo .52 .57 .60
Llama-2-70b .56 .64 .63
— .51 .47 .30
Llama-2-7b GPT-4 .64 .70 .73
GPT-3.5-Turbo .48 .52 .56
Llama-2-70b .60 .60 .68
— .42 .46 .40
Table3. Label-by-labelperformanceofpre-trainedandfine-tunedmodels.
Model TrainSetFrom 𝐹1𝐸𝑛𝑡 𝐹1𝑁𝑒𝑢 𝐹1𝐶𝑜𝑛
BERT-base GPT-4 .65 .30 .21
GPT-4 — .61 .63 .51
GPT-3.5-Turbo GPT-4 .83 .67 .44
GPT-3.5-Turbo .76 .35 .32
Llama-2-70b .73 .57 .34
— .58 .64 .39
Llama-2-13b GPT-4 .79 .69 .45
GPT-3.5-Turbo .73 .50 .34
Llama-2-70b .74 .57 .39
— .37 .36 .19
Llama-2-7b GPT-4 .79 .72 .46
GPT-3.5-Turbo .69 .41 .29
Llama-2-70b .74 .65 .40
— .63 .02 .26
Lookingforward,itiscrucialtoencourageongoingcollaborationsamongresearchers,developers,andfact-checkers
tofullyexploitAIbenefitswhilemitigatingitspotentialdrawbacks.Theimportanceofhumanexpertiseandsupervision
inthiscontextcannotbeoverstated.Completelyautomatingfact-checkingproceduresusingAIcarriescertainrisks
and limitations, such as the perpetuation of biases intrinsic to models and inherent inconsistencies due to their
probabilisticnature.However,withthoughtfulincorporation,technologiescouldsubstantiallyaugmentthecapabilities
offact-checkerstodetectanddebunkmisinformation.
Future studies should focus on discovering different methods for data synthesis and augmentation to further
optimizeFACT-GPT.Additionally,evaluatingthemodel’sperformanceacrossavarietyofreal-worlddatasetsiscrucial.
Explorationintotheintegrationofnaturallanguageexplanation(NLE)capabilitieswithinGPTmodelscanfurther
5ChoiandFerrara
enhancetransparency.ThisresearchaddssubstantivelytoagrowingbodyofworkexaminingtheuseofLLMsin
supportofhumanfact-checkers,offeringafoundationforcontinuedstudiesandtheresponsibleadvancementofAI
toolstoeffectivelycombatthespreadofmisinformationatalargerscale.
Acknoledgements.ThisworkwassupportedinpartbyDARPA(contractno.HR001121C0169).
REFERENCES
[1] P.Arnold.Thechallengesofonlinefactchecking,2020.URLhttps://fullfact.org/media/uploads/coof-2020.pdf.
[2] E.Chen,K.Lerman,E.Ferrara,etal.Trackingsocialmediadiscourseaboutthecovid-19pandemic:Developmentofapubliccoronavirustwitter
dataset.JMIRpublichealthandsurveillance,6(2):e19273,2020.
[3] H.Dai,Z.Liu,W.Liao,X.Huang,Y.Cao,Z.Wu,L.Zhao,S.Xu,W.Liu,N.Liu,S.Li,D.Zhu,H.Cai,L.Sun,Q.Li,D.Shen,T.Liu,andX.Li.Auggpt:
Leveragingchatgptfortextdataaugmentation,2023.
[4] S.Dégallier-Rochat,M.Kurpicz-Briki,N.Endrissat,andO.Yatsenko.Humanaugmentation,notreplacement:Aresearchagendaforaiandrobotics
intheindustry.FrontiersinRoboticsandAI,9,2022.ISSN2296-9144.doi:10.3389/frobt.2022.997386.URLhttps://www.frontiersin.org/articles/10.
3389/frobt.2022.997386.
[5] T.Elsayed,P.Nakov,A.Barrón-Cedeño,M.Hasanain,R.Suwaileh,G.DaSanMartino,andP.Atanasova. Checkthat!atclef2019:Automatic
identificationandverificationofclaims.InL.Azzopardi,B.Stein,N.Fuhr,P.Mayr,C.Hauff,andD.Hiemstra,editors,AdvancesinInformation
Retrieval,pages309–315,Cham,2019.SpringerInternationalPublishing.ISBN978-3-030-15719-7.
[6] F.Gilardi,M.Alizadeh,andM.Kubli.ChatGPToutperformscrowdworkersfortext-annotationtasks.ProceedingsoftheNationalAcademyof
Sciences,120(30),jul2023.doi:10.1073/pnas.2305016120.URLhttps://doi.org/10.1073%2Fpnas.2305016120.
[7] N.Hassan,G.Zhang,F.Arslan,J.Caraballo,D.Jimenez,S.Gawsane,S.Hasan,M.Joseph,A.Kulkarni,A.K.Nayak,V.Sable,C.Li,andM.Tremayne.
Claimbuster:Thefirst-everend-to-endfact-checkingsystem.Proc.VLDBEndow.,10(12):1945–1948,aug2017.ISSN2150-8097.doi:10.14778/3137765.
3137815.URLhttps://doi.org/10.14778/3137765.3137815.
[8] B.He,M.Ahamad,andS.Kumar.Reinforcementlearning-basedcounter-misinformationresponsegeneration:acasestudyofcovid-19vaccine
misinformation.InProceedingsoftheACMWebConference2023,pages2698–2709,2023.
[9] hiyouga.Llama-factory.https://github.com/hiyouga/LLaMA-Factory,2023.
[10] E.Hoes,S.Altay,andJ.Bermeo.Usingchatgpttofightmisinformation:Chatgptnails72%of12,000verifiedclaims.2023.
[11] E.J.Hu,Y.Shen,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang,andW.Chen.Lora:Low-rankadaptationoflargelanguagemodels.2021.
[12] IFCN.Codeofprinciples,2023.URLhttps://ifcncodeofprinciples.poynter.org/know-more/the-commitments-of-the-code-of-principles.
[13] Y.Jin,X.Wang,R.Yang,Y.Sun,W.Wang,H.Liao,andX.Xie.Towardsfine-grainedreasoningforfakenewsdetection.InProceedingsoftheAAAI
ConferenceonArtificialIntelligence,volume36,pages5746–5754,2022.
[14] J.Kocoń,I.Cichecki,O.Kaszyca,M.Kochanek,D.Szydło,J.Baran,J.Bielaniewicz,M.Gruza,A.Janz,K.Kanclerz,A.Kocoń,B.Koptyra,
W.Mieleszczenko-Kowszewicz,P.Miłkowski,M.Oleksy,M.Piasecki,ŁukaszRadliński,K.Wojtasik,S.Woźniak,andP.Kazienko. Chatgpt:
Jackofalltrades,masterofnone. InformationFusion,99:101861,2023. ISSN1566-2535. doi:https://doi.org/10.1016/j.inffus.2023.101861. URL
https://www.sciencedirect.com/science/article/pii/S156625352300177X.
[15] V.LaGatta,C.Wei,L.Luceri,F.Pierri,E.Ferrara,etal.Retrievingfalseclaimsontwitterduringtherussia-ukraineconflict.InWWW’23Companion:
CompanionProceedingsoftheACMWebConference2023,pages1317–1323,2023.
[16] J.Ma,W.Gao,S.Joty,andK.-F.Wong.Sentence-levelevidenceembeddingforclaimverificationwithhierarchicalattentionnetworks.Association
forComputationalLinguistics,2019.
[17] M.Marelli,S.Menini,M.Baroni,L.Bentivogli,R.Bernardi,andR.Zamparelli. Asickcurefortheevaluationofcompositionaldistributional
semanticmodels.InInternationalConferenceonLanguageResourcesandEvaluation,2014.URLhttps://api.semanticscholar.org/CorpusID:762228.
[18] P.Nakov,D.Corney,M.Hasanain,F.Alam,T.Elsayed,A.Barrón-Cedeño,P.Papotti,S.Shaar,andG.D.S.Martino.Automatedfact-checkingfor
assistinghumanfact-checkers,2021.
[19] S.PadóandI.Dagan.TextualEntailment.InTheOxfordHandbookofComputationalLinguistics.OxfordUniversityPress,062022.ISBN9780199573691.
doi:10.1093/oxfordhb/9780199573691.013.024.URLhttps://doi.org/10.1093/oxfordhb/9780199573691.013.024.
[20] M.V.Reiss.Testingthereliabilityofchatgptfortextannotationandclassification:Acautionaryremark,2023.
[21] S.Shaar,N.Babulkov,G.DaSanMartino,andP.Nakov.Thatisaknownlie:Detectingpreviouslyfact-checkedclaims.InProceedingsofthe58th
AnnualMeetingoftheAssociationforComputationalLinguistics,pages3607–3618,Online,July2020.AssociationforComputationalLinguistics.doi:
10.18653/v1/2020.acl-main.332.URLhttps://aclanthology.org/2020.acl-main.332.
[22] S.Shaar,N.Georgiev,F.Alam,G.DaSanMartino,A.Mohamed,andP.Nakov. Assistingthehumanfact-checkers:Detectingallpreviously
fact-checkedclaimsinadocument.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2022,pages2069–2080,2022.
[23] M.TambuscioandG.Ruffo.Fact-checkingstrategiestolimiturbanlegendsspreadinginasegregatedsociety.AppliedNetworkScience,4:1–19,2019.
[24] S.Vosoughi,D.Roy,andS.Aral.Thespreadoftrueandfalsenewsonline.science,359(6380):1146–1151,2018.
[25] A.Yavary,H.Sajedi,andM.S.Abadeh.Informationverificationimprovementbytextualentailmentmethods.SNAppliedSciences,1:1–6,2019.
[26] Y.Zhu,P.Zhang,E.-U.Haq,P.Hui,andG.Tyson.Canchatgptreproducehuman-generatedlabels?astudyofsocialcomputingtasks,2023.
6