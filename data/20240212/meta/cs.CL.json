[
    {
        "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models",
        "authors": "Peng GaoRenrui ZhangChris LiuLongtian QiuSiyuan HuangWeifeng LinShitian ZhaoShijie GengZiyi LinPeng JinKaipeng ZhangWenqi ShaoChao XuConghui HeJunjun HeHao ShaoPan LuHongsheng LiYu Qiao",
        "links": "http://arxiv.org/abs/2402.05935v1",
        "entry_id": "http://arxiv.org/abs/2402.05935v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05935v1",
        "summary": "We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory",
        "updated": "2024-02-08 18:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05935v1"
    },
    {
        "title": "Driving Everywhere with Large Language Model Policy Adaptation",
        "authors": "Boyi LiYue WangJiageng MaoBoris IvanovicSushant VeerKaren LeungMarco Pavone",
        "links": "http://arxiv.org/abs/2402.05932v1",
        "entry_id": "http://arxiv.org/abs/2402.05932v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05932v1",
        "summary": "Adapting driving behavior to new environments, customs, and laws is a\nlong-standing problem in autonomous driving, precluding the widespread\ndeployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a\nsimple yet powerful tool that enables human drivers and autonomous vehicles\nalike to drive everywhere by adapting their tasks and motion plans to traffic\nrules in new locations. LLaDA achieves this by leveraging the impressive\nzero-shot generalizability of large language models (LLMs) in interpreting the\ntraffic rules in the local driver handbook. Through an extensive user study, we\nshow that LLaDA's instructions are useful in disambiguating in-the-wild\nunexpected situations. We also demonstrate LLaDA's ability to adapt AV motion\nplanning policies in real-world datasets; LLaDA outperforms baseline planning\napproaches on all our metrics. Please check our website for more details:\nhttps://boyiliee.github.io/llada.",
        "updated": "2024-02-08 18:59:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05932v1"
    },
    {
        "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
        "authors": "Xing Han LùZdeněk KasnerSiva Reddy",
        "links": "http://arxiv.org/abs/2402.05930v1",
        "entry_id": "http://arxiv.org/abs/2402.05930v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05930v1",
        "summary": "We propose the problem of conversational web navigation, where a digital\nagent controls a web browser and follows user instructions to solve real-world\ntasks in a multi-turn dialogue fashion. To support this problem, we introduce\nWEBLINX - a large-scale benchmark of 100K interactions across 2300 expert\ndemonstrations of conversational web navigation. Our benchmark covers a broad\nrange of patterns on over 150 real-world websites and can be used to train and\nevaluate agents in diverse scenarios. Due to the magnitude of information\npresent, Large Language Models (LLMs) cannot process entire web pages in\nreal-time. To solve this bottleneck, we design a retrieval-inspired model that\nefficiently prunes HTML pages by ranking relevant elements. We use the selected\nelements, along with screenshots and action history, to assess a variety of\nmodels for their ability to replicate human behavior when navigating the web.\nOur experiments span from small text-only to proprietary multimodal LLMs. We\nfind that smaller finetuned decoders surpass the best zero-shot LLMs (including\nGPT-4V), but also larger finetuned multimodal models which were explicitly\npretrained on screenshots. However, all finetuned models struggle to generalize\nto unseen websites. Our findings highlight the need for large multimodal models\nthat can generalize to novel settings. Our code, data and models are available\nfor research: https://mcgill-nlp.github.io/weblinx",
        "updated": "2024-02-08 18:58:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05930v1"
    },
    {
        "title": "On the Convergence of Zeroth-Order Federated Tuning in Large Language Models",
        "authors": "Zhenqing LingDaoyuan ChenLiuyi YaoYaliang LiYing Shen",
        "links": "http://arxiv.org/abs/2402.05926v1",
        "entry_id": "http://arxiv.org/abs/2402.05926v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05926v1",
        "summary": "The confluence of Federated Learning (FL) and Large Language Models (LLMs) is\nushering in a new era in privacy-preserving natural language processing.\nHowever, the intensive memory requirements for fine-tuning LLMs pose\nsignificant challenges, especially when deploying on edge devices with limited\ncomputational resources. To circumvent this, we explore the novel integration\nof Memory-efficient Zeroth-Order Optimization within a federated setting, a\nsynergy we denote as FedMeZO. Our study is the first to examine the theoretical\nunderpinnings of FedMeZO in the context of LLMs, tackling key questions\nregarding the influence of large parameter spaces on optimization behavior, the\nestablishment of convergence properties, and the identification of critical\nparameters for convergence to inform personalized federated strategies. Our\nextensive empirical evidence supports the theory, showing that FedMeZO not only\nconverges faster than traditional first-order methods such as SGD but also\nsignificantly reduces GPU memory usage during training to levels comparable to\nthose during inference. Moreover, the proposed personalized FL strategy that is\nbuilt upon the theoretical insights to customize the client-wise learning rate\ncan effectively accelerate loss reduction. We hope our work can help to bridge\ntheoretical and practical aspects of federated fine-tuning for LLMs and\nfacilitate further development and research.",
        "updated": "2024-02-08 18:56:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05926v1"
    },
    {
        "title": "Efficient Stagewise Pretraining via Progressive Subnetworks",
        "authors": "Abhishek PanigrahiNikunj SaunshiKaifeng LyuSobhan MiryoosefiSashank ReddiSatyen KaleSanjiv Kumar",
        "links": "http://arxiv.org/abs/2402.05913v1",
        "entry_id": "http://arxiv.org/abs/2402.05913v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05913v1",
        "summary": "Recent developments in large language models have sparked interest in\nefficient pretraining methods. A recent effective paradigm is to perform\nstage-wise training, where the size of the model is gradually increased over\nthe course of training (e.g. gradual stacking (Reddi et al., 2023)). While the\nresource and wall-time savings are appealing, it has limitations, particularly\nthe inability to evaluate the full model during earlier stages, and degradation\nin model quality due to smaller model capacity in the initial stages. In this\nwork, we propose an alternative framework, progressive subnetwork training,\nthat maintains the full model throughout training, but only trains subnetworks\nwithin the model in each step. We focus on a simple instantiation of this\nframework, Random Path Training (RaPTr) that only trains a sub-path of layers\nin each step, progressively increasing the path lengths in stages. RaPTr\nachieves better pre-training loss for BERT and UL2 language models while\nrequiring 20-33% fewer FLOPs compared to standard training, and is competitive\nor better than other efficient training methods. Furthermore, RaPTr shows\nbetter downstream performance on UL2, improving QA tasks and SuperGLUE by 1-5%\ncompared to standard training and stacking. Finally, we provide a theoretical\nbasis for RaPTr to justify (a) the increasing complexity of subnetworks in\nstages, and (b) the stability in loss across stage transitions due to residual\nconnections and layer norm.",
        "updated": "2024-02-08 18:49:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05913v1"
    }
]