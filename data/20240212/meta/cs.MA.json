[
    {
        "title": "Consensus-driven Deviated Pursuit for Guaranteed Simultaneous Interception of Moving Targets",
        "authors": "Abhinav SinhaDwaipayan MukherjeeShashi Ranjan Kumar",
        "links": "http://arxiv.org/abs/2402.05918v1",
        "entry_id": "http://arxiv.org/abs/2402.05918v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05918v1",
        "summary": "This work proposes a cooperative strategy that employs deviated pursuit\nguidance to simultaneously intercept a moving (but not manoeuvring) target. As\nopposed to many existing cooperative guidance strategies which use estimates of\ntime-to-go, based on proportional-navigation guidance, the proposed strategy\nuses an exact expression for time-to-go to ensure simultaneous interception.\nThe guidance design considers nonlinear engagement kinematics, allowing the\nproposed strategy to remain effective over a large operating regime. Unlike\nexisting strategies on simultaneous interception that achieve interception at\nthe average value of their initial time-to-go estimates, this work provides\nflexibility in the choice of impact time. By judiciously choosing the edge\nweights of the communication network, a weighted consensus in time-to-go can be\nachieved. It has been shown that by allowing an edge weight to be negative,\nconsensus in time-to-go can even be achieved for an impact time that lies\noutside the convex hull of the set of initial time-to-go values of the\nindividual interceptors. The bounds on such negative weights have been analysed\nfor some special graphs, using Nyquist criterion. Simulations are provided to\nvindicate the efficacy of the proposed strategy.",
        "updated": "2024-02-08 18:52:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05918v1"
    },
    {
        "title": "Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games",
        "authors": "Hafez GhaemiHamed KebriaeiAlireza Ramezani MoghaddamMajid Nili Ahamdabadi",
        "links": "http://arxiv.org/abs/2402.05906v1",
        "entry_id": "http://arxiv.org/abs/2402.05906v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05906v1",
        "summary": "Classical multi-agent reinforcement learning (MARL) assumes risk neutrality\nand complete objectivity for agents. However, in settings where agents need to\nconsider or model human economic or social preferences, a notion of risk must\nbe incorporated into the RL optimization problem. This will be of greater\nimportance in MARL where other human or non-human agents are involved, possibly\nwith their own risk-sensitive policies. In this work, we consider\nrisk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT),\na non-convex risk measure and a generalization of coherent measures of risk.\nCPT is capable of explaining loss aversion in humans and their tendency to\noverestimate/underestimate small/large probabilities. We propose a distributed\nsampling-based actor-critic (AC) algorithm with CPT risk for network\naggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC.\nUnder a set of assumptions, we prove the convergence of the algorithm to a\nsubjective notion of Markov perfect Nash equilibrium in NAMGs. The experimental\nresults show that subjective CPT policies obtained by our algorithm can be\ndifferent from the risk-neutral ones, and agents with a higher loss aversion\nare more inclined to socially isolate themselves in an NAMG.",
        "updated": "2024-02-08 18:43:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05906v1"
    },
    {
        "title": "Cutsets and EF1 Fair Division of Graphs",
        "authors": "Jiehua ChenWilliam S. Zwicker",
        "links": "http://arxiv.org/abs/2402.05884v1",
        "entry_id": "http://arxiv.org/abs/2402.05884v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05884v1",
        "summary": "In fair division of a connected graph $G = (V, E)$, each of $n$ agents\nreceives a share of $G$'s vertex set $V$. These shares partition $V$, with each\nshare required to induce a connected subgraph. Agents use their own valuation\nfunctions to determine the non-negative numerical values of the shares, which\ndetermine whether the allocation is fair in some specified sense. We introduce\nforbidden substructures called graph cutsets, which block divisions that are\nfair in the EF1 (envy-free up to one item) sense by cutting the graph into \"too\nmany pieces\". Two parameters - gap and valence - determine blocked values of\n$n$. If $G$ guarantees connected EF1 allocations for $n$ agents with valuations\nthat are CA (common and additive), then $G$ contains no elementary cutset of\ngap $k \\ge 2$ and valence in the interval $\\[n - k + 1, n - 1\\]$. If $G$\nguarantees connected EF1 allocations for $n$ agents with valuations in the\nbroader CM (common and monotone) class, then $G$ contains no cutset of gap $k\n\\ge 2$ and valence in the interval $\\[n - k + 1, n - 1\\]$. These results rule\nout the existence of connected EF1 allocations in a variety of situations. For\nsome graphs $G$ we can, with help from some new positive results, pin down\n$G$'s spectrum - the list of exactly which values of $n$ do/do not guarantee\nconnected EF1 allocations. Examples suggest a conjectured common spectral\npattern for all graphs. Further, we show that it is NP-hard to determine\nwhether a graph admits a cutset. We also provide an example of a\n(non-traceable) graph on eight vertices that has no cutsets of gap $\\ge 2$ at\nall, yet fails to guarantee connected EF1 allocations for three agents with CA\npreferences.",
        "updated": "2024-02-08 18:18:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05884v1"
    },
    {
        "title": "Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices",
        "authors": "Jiin WooLaixi ShiGauri JoshiYuejie Chi",
        "links": "http://arxiv.org/abs/2402.05876v1",
        "entry_id": "http://arxiv.org/abs/2402.05876v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05876v1",
        "summary": "Offline reinforcement learning (RL), which seeks to learn an optimal policy\nusing offline data, has garnered significant interest due to its potential in\ncritical applications where online data collection is infeasible or expensive.\nThis work explores the benefit of federated learning for offline RL, aiming at\ncollaboratively leveraging offline datasets at multiple agents. Focusing on\nfinite-horizon episodic tabular Markov decision processes (MDPs), we design\nFedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for\nfederated offline RL. FedLCB-Q updates local Q-functions at agents with novel\nlearning rate schedules and aggregates them at a central server using\nimportance averaging and a carefully designed pessimistic penalty term. Our\nsample complexity analysis reveals that, with appropriately chosen parameters\nand synchronization schedules, FedLCB-Q achieves linear speedup in terms of the\nnumber of agents without requiring high-quality datasets at individual agents,\nas long as the local datasets collectively cover the state-action space visited\nby the optimal policy, highlighting the power of collaboration in the federated\nsetting. In fact, the sample complexity almost matches that of the single-agent\ncounterpart, as if all the data are stored at a central location, up to\npolynomial factors of the horizon length. Furthermore, FedLCB-Q is\ncommunication-efficient, where the number of communication rounds is only\nlinear with respect to the horizon length up to logarithmic factors.",
        "updated": "2024-02-08 18:09:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05876v1"
    },
    {
        "title": "Analysing the Sample Complexity of Opponent Shaping",
        "authors": "Kitty FungQizhen ZhangChris LuJia WanTimon WilliJakob Foerster",
        "links": "http://arxiv.org/abs/2402.05782v1",
        "entry_id": "http://arxiv.org/abs/2402.05782v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05782v1",
        "summary": "Learning in general-sum games often yields collectively sub-optimal results.\nAddressing this, opponent shaping (OS) methods actively guide the learning\nprocesses of other agents, empirically leading to improved individual and group\nperformances in many settings. Early OS methods use higher-order derivatives to\nshape the learning of co-players, making them unsuitable for shaping multiple\nlearning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses\nthese by reframing the OS problem as a meta-game. In contrast to early OS\nmethods, there is little theoretical understanding of the M-FOS framework.\nProviding theoretical guarantees for M-FOS is hard because A) there is little\nliterature on theoretical sample complexity bounds for meta-reinforcement\nlearning B) M-FOS operates in continuous state and action spaces, so\ntheoretical analysis is challenging. In this work, we present R-FOS, a tabular\nversion of M-FOS that is more suitable for theoretical analysis. R-FOS\ndiscretises the continuous meta-game MDP into a tabular MDP. Within this\ndiscretised MDP, we adapt the $R_{max}$ algorithm, most prominently used to\nderive PAC-bounds for MDPs, as the meta-learner in the R-FOS algorithm. We\nderive a sample complexity bound that is exponential in the cardinality of the\ninner state and action space and the number of agents. Our bound guarantees\nthat, with high probability, the final policy learned by an R-FOS agent is\nclose to the optimal policy, apart from a constant factor. Finally, we\ninvestigate how R-FOS's sample complexity scales in the size of state-action\nspace. Our theoretical results on scaling are supported empirically in the\nMatching Pennies environment.",
        "updated": "2024-02-08 16:17:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05782v1"
    }
]