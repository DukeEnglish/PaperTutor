A Decision-Language Model (DLM) for Dynamic Restless
Multi-Armed Bandit Tasks in Public Health
NikhilBehari∗1, EdwinZhang∗1, YunfanZhao1,
AparnaTaneja2, DheerajNagaraj2 and MilindTambe1,2
1HarvardUniversity 2Google
Abstract portionofMMsstemfrompreventablecauses[Organization
andothers,2015];assuch,programspromotingpreventative
Effortstoreducematernalmortalityrate,akeyUN
careawareness,forexamplethroughmobilehealthplatforms,
SustainableDevelopmenttarget(SDGTarget3.1),
haveshowntoconsiderablyimproveaccesstoessentialcare
relylargelyonpreventativecareprogramstospread information[Choudhuryetal.,2021].
criticalhealthinformationtohigh-riskpopulations.
However, twokeyproblemsremainforthesepreventative
Theseprogramsfacetwoimportantchallenges: ef-
careprograms,oftenoperatedbynon-profitssupportinglow-
ficientlyallocatinglimitedhealthresourcestolarge
resourcecommunities[HelpMum,2023;ARMMAN,2019].
beneficiary populations, and adapting to evolving
First,theseprogramstypicallyoperatewithinsufficientfinan-
policy priorities. While prior works in restless
cialandhumanresources,highlightingtheneedforeffective
multi-armedbandit(RMAB)demonstratedsuccess
resource allocation strategies to maximize health outcomes
in public health allocation tasks, they lack flexi-
[Kruk et al., 2018; Baltussen and Niessen, 2006]. Second,
bility to adapt to evolving policy priorities. Con-
these programs must frequently adapt to changing popula-
currently, Large Language Models (LLMs) have
tionneedsandinterventionpriorities[Organizationandoth-
emerged as adept, automated planners in various
ers,2015]. Adaptabilityisespeciallyimportantforprioritiz-
domains,includingroboticcontrolandnavigation.
ing care for known subpopulations with higher risk [Gra¨ler
In this paper, we propose DLM: a Decision Lan-
etal.,2022],andforadjustingstrategiesusingcommunityor
guageModelforRMABs. Toenabledynamicfine-
expert-driveninsights[Deardorffetal.,2018].
tuning of RMAB policies for challenging public
Restless multi-armed bandits (RMAB) have been shown
health settings using human-language commands,
to be successful in modelling resource allocation in public
we propose using LLMs as automated planners to
health[Nishtalaetal.,2023;Wangetal.,2023]. Intheclas-
(1)interprethumanpolicypreferenceprompts,(2)
sical RMAB formulation, a central planner chooses arms to
propose code reward functions for a multi-agent
allocateresourcesandobservesstate-dependentrewardfrom
RL environment for RMABs, and (3) iterate on
arms. Despite the strength of RMABs in planning resource
the generated reward using feedback from RMAB
allocation,asignificantchallengepersistsindevelopingmod-
simulations to effectively adapt policy outcomes.
elscapableofadaptingtochangingpolicyobjectives. Forex-
In collaboration with ARMMAN, an India-based
ample,publichealthexpertsmaywishtouseRMABmodels
public health organization promoting preventative
to direct resources towards specific underprivileged subpop-
careforpregnantmothers,weconductasimulation
ulations. However,existingworksinRMABsfocusonfixed
study,showingDLMcandynamicallyshapepolicy
policy objectives and require significant human tuning and
outcomes using only human language commands
re-trainingtomeetnewdesiredhealthoutcome.
asinput.
Recently,largelanguagemodels(LLMs)haveemergedas
powerful planners in a wide variety of tasks including navi-
1 Introduction gation [Shah et al., 2023], spatio-temporal reasoning [Vem-
pralaetal.,2023],andinteractivedecision-making[Lietal.,
Maternal and child health is a critical public health priority
2022]. RecentworkshavealsoshownthatLLMscanbeused
worldwide. Recent estimates indicate that the global ma-
directlyascodegeneratorstoproposerewardfunctionsforre-
ternal mortality ratio is more than double the UN Sustain-
inforcementlearning(RL)tocompletecomplexroboticma-
ableDevelopmentGoaltarget(SDGTarget3.1)ofbelow70
nipulation tasks [Ma et al., 2024]. However, the ability of
deathsper100klivebirths[UnitedNationsandDevelopment,
LLMsasplannersforresourceallocationinreal-worldpub-
2015]. Maternalmortality(MM)isofparticularconcernfor
lichealthsettingsremainsunstudied. Thisplanningabilityis
lowandmiddle-incomecountries,whichaccountfor99%of
especiallyusefulfornon-profitsthathaveevolvingpriorities
globalmaternaldeaths[Kurjaketal.,2023]. Further,alarge
butdonothavethecapacitytofrequentlyretrainmodels.
∗Equalcontribution. WeproposeaDecisionLanguageModelforRMABs,com-
4202
beF
22
]AM.sc[
1v70841.2042:viXrabining strengths of LLMs in semantic comprehension, code Husseinetal., [2017]reshaperewardthroughexpertbehav-
generation, and in-context learning, with the efficiency of ior observation, but they ignored that expert examples may
RMABmodelinginresourceallocationforpublichealth. To not be available in resource and data-limited public health
enable rapid policy adjustment using human language com- domains. Using LLMs as dense reward signals have been
mands,wepropose: 1)usingLLMstodisambiguatehuman- investigated [Kwon et al., 2022; Du et al., 2023]. Ma et
expressedpolicypreferences,2)usingLLMstodirectlypro- al. [2024] use LLMs to output reward function code from
pose reward functions as code to reach desired RMAB ob- language-specified goals. However, above works failed to
jectives, and 3) a mechanism for iteratively refining LLM- address multi-agent settings, where a central planner could
generated reward functions without any ground truth feed- prioritizecertaingroupsthroughrewarddesign.
back. Using simulation results of LLM-proposed reward
functions, we continually improve the alignment of reward 3 Background
functionstowardsspecifiedgoals.
We consider an RMAB problem with N arms, represent-
We partner with the India-based non-profit ARMMAN,
ing public health program beneficiaries. Each arm n ∈ [N]
a public health organization that spreads preventative care
followsaMarkovdecisionprocess(S ,A ,C ,T ,R ,β )
n n n n n n
awarenesstopregnantmothersthroughanautomatedphone-
withstatespaceS ,actionspaceA ,actioncostsC :A →
n n n n
based messaging system. To increase retention, ARMMAN R,andunknowntransitionprobabilitiesT :S ×A ×S →
n n n n
employshealthworkerstoprovideservicecallstobeneficia- [0,1]. WedefinerewardfunctionR :S →Randdiscount
n n
ries. Due to limited support staff, this assignment is similar
factorβ ∈ [0,1). WhenS ,A ,andC arethesameforall
n n n
to a limited resource allocation problem. Prior works have
armsn∈[N],asinthepublichealthsettingwefocuson,we
usedRMABstomodeltheuserretentionproblemintheAR-
omitthesubscriptn. Noteouralgorithmsalsoapplietomore
MMANsetting[Killianetal., 2023;Wangetal., 2023], but
generalsettings. Aplannerlearnsapolicyπthatmapsstates
theyfailtoaddressevolvingprioritiesofARMMAN. s ∈ RN toactionsinthesetA,giventheconstraintthatthe
Insummary,ourkeycontributionsareasfollows: sumcostofactionsdoesnotexceedbudgetB. Theobjective
• Tothebestofourknowledge,wearethefirsttopropose istolearnapolicythatmaximizestheBellmanequation:
usingLLMstoadapttochangingresourceallocationob- (cid:40) N (cid:41)
(cid:88)
jectivesinpublichealththroughrewarddesign. J(s)=max R(s )+βE[J(s′)|s,π] , (1)
n
π∈Π
• We introduce a reward proposal loop that enhances n=1
LLM-generated reward functions using feedback from N
(cid:88)
restless multi-armed bandit (RMAB) simulations, en- s.t. π(s n)∈A, ∀n and c π(sn) ≤B (2)
abling LLMs to iteratively refine reward design to n=1
achievespecific,human-specifiedpolicyoutcomes. forcostofactionc ∈ C. WetaketheLagrangianrelax-
π(sn)
ationtodecouplethevaluefunctionsacrossarms:
• To assess the feasibility of deploying such a system in
therealworld,weevaluateouralgorithms’performance
(cid:32) N (cid:33)
in a real-world task of resource allocation for mater- λB (cid:88)
J(s,λ⋆)=min + max{Q (s ,a ,λ)} ,
n rea wl aa rn dd -dc eh si il gd nc pa er re f, ora mnd and ce em
.
Oon us rtr aa lt ge orn ie tha mr h eu nm aba ln e- sle dv ye -l λ≥0 1−β n=1an∈A n n n
(3)
namically adjusting policies to match human-specified
outcomes,usingonlylanguagepromptsasinput. s.t.Q n(s n,a n,λ)=R(s n)−λc an
+βE[Q (s′ ,a ,λ)|π(λ)].
n n n
2 RelatedWork
forQ-functionQ,armnactiona ,transitionedstatefroms
n n
under action a as s′ , and π(λ) an optimal policy under λ.
RMABsTherestlessmulti-armedbanditproblemintroduced n n
by Whittle [1988], is classically solved through the Whit- [Shinn et al., 2024; Ma et al., 2024]. We build on deep RL
tle index policy [Weber and Weiss, 1990; Glazebrook et approachesforRMABs,whichwediscussinAppendixC.
al., 2006]. Subsequent works have generalized to multi-
4 Decision-LanguageModelforRMABs
actionRMABs[Glazebrooketal.,2011;Killianetal.,2022].
RMABsgainedprominenceinpublichealthdomainsandare Below,weprovideanoverviewoftheproposedDLMreward
deployedtodisseminatepreventativehealthcareinformation, generationtechnique.Anoverviewoftheproposedtechnique
monitorhealthprogramadherence,andmodeldiseasespread is provided in Fig. 1. We first describe the problem setting
[Tambe, 2022; Mate et al., 2022]. However, existing works of reward generation, and motivate the use of LLMs for re-
focusonfixedrewardfunctionsandfailtoconsiderthatpub- wardgenerationfromatheoreticalperspective. Wethendis-
lic health planners often have evolving priorities [Organiza- cuss the context is provided to the LLM for initial reward
tionandothers,2015]. candidategeneration. Then,wediscusstheindependentsim-
Reward Design: Designing reward functions that effec- ulationstagewhichidentifiesanoptimalpolicyπR∗ undera
tively condense long-term agent goals into immediate be- proposedrewardR,andthesubsequentpolicyoutcomecom-
havioral signals is a central problem in RL [Singh et al., parisonstagethatsimulatestherespectivepolicyrewardout-
2009]. Manualdesignsarepronetotaskmisspecificationand comes. To be clear, πR∗ references the optimal policy in-
overfitting [Booth et al., 2023]. Ho and Ermon [2016] and duced by a given reward function R. This does not refer toFigure1:OverviewoftheproposedDLMloop.WebeginbyprovidingthreekeycontextdescriptionstotheLLM.First,weprovidealanguage
command;weincludethespecificcommandsusedinAppendixTable1.Wealsoprovidearmfeaturecontext,whichcontainsalistofrelevant
per-armfeaturesavailabletouseintheproposedreward,aswellassyntaxcuesandtherelevantsubsetofthecodebasesothattheproposed
LLMrewardfunctionsmaybeuseddirectlyincode. Fromthiscontext,theLLMthenproposescandidaterewardfunctionswhichareused
totrainoptimalpoliciesunderproposedreward. Thesetrainedpoliciesarethenusedtogeneratepolicyoutcomecomparisons,whichshow
thedistributionofrewardsacrosskeyfeaturegroupsinsimulation. Finally,wequeryanLLMtoperformself-reflection[Shinnetal.,2024;
Maetal.,2024]bychoosingthebestcandidaterewardaligningwiththeoriginallanguagecommand;theselectedbestrewardsareusedas
contextforfuturerewardgenerationiterations.
an optimal reward R∗. Finally, we discuss the LLM reflec- 4.2 SampleInefficiencyofFitnessFunction
tion stage which considers the policy outcome comparison
andidentifiessuccessfulcandidatestoguidefuturein-context In order to evaluate each task description that we provide,
learningforLLMrewardgeneration. we design a ‘ground truth fitness function’. This is the re-
ward function (assigning a reward to each state and action)
4.1 ProblemSetting: RewardGenerationfor against which the learned policies are evaluated. However,
thisfitnessfunctionisunknowntotheLLM.The‘base’pol-
RMABs
icyinourcomparisonisobtainedbytraininganRLpolicyto
We first define the goal of language-conditioned reward de- maximizethediscountedrewardobtainedbythisgroundtruth
signintherestlessmulti-armedbanditsetting. Weconsidera fitness function and serves as a baseline to our experiments.
human-specifiedlanguagepolicygoalℓwithacorresponding We want to note that, despite being the ‘true reward func-
baserewardfunctionthatmapspoliciesπ ∈Πtoreal,scalar tion’, this might not be the best reward function to learn the
values: F : Π → R. Thisbaserewardfunctionestablishesa optimal policy in a learning theoretic sense. This is usually
scalar “ground truth” evaluation that corresponds directly to the case when rewards are sparse (i.e, hard to obtain) from
thehuman-specifiedcommand. Evaluationthroughthebase simple policies and the sparsity of the reward signal makes
reward is available through real or simulated queries of the learninghard. Thus,rewardshaping/rewardengineeringhas
proposedpolicy. Then,ourobjectiveistooptimizeR: beenextensivelystudiedintheliterature([Dannetal.,2023;
SuttonandBarto,2018;Ngetal.,1999;Singhetal.,2010]).
The idea here is to use domain knowledge and design a dif-
maxF(πR∗),where:
R∈R ferent, dense reward whose optimal policy is the same as
thatoftheoriginalreward. However, thelearningalgorithm
(cid:40) N (cid:41) deployed finds it much easier to learn the optimal policy
πR∗ =argmax (cid:88) R(s ,z)+βE[J(s′)|s,π] via the shaped reward. While the existing works (eg [Ng
n et al., 1999]) show that reward shaping methods can prov-
π∈Π
n=1 ably help learning in the single agent setting, their results
N
(cid:88) apply to the multi-agent problem of RMABs, where the La-
s.t. π(s )∈A, ∀n and c ≤B
n π(sn) grangianrelaxationdecouplesthevaluefunctionacrossarms
n=1 and allows us to focus on separate single-agent MDPs. In
Insummary,ourkeyobjectiveis,givenahuman-specified this work, we eschew the careful human design of the re-
language prompt ℓ, to generate the reward function R that ward function, and allow an LLM to assume this role. Our
maximizes base function F through optimal policy πR∗ for thesis is that the world knowledge the LLM obtains dur-
rewardR. WeproposeusingLLMstoautomaticallypropose ing its training, along with reflection, equips it with the do-
thisrewardfunctionR,andautomaticallyrefineproposedre- main knowledge essential to design efficient reward func-
wardsthroughaniterativeself-reflectionprocedure. tions. Automateddesignofsuchrewardfunctionshavebeenexplored in the literature: [Mataric, 1994; Sorg et al., 2010; Third, we provide context regarding the RMAB problem
Zheng et al., 2020]. Here, a meta-network is trained to formulationandcorrespondingcodeimplementation.Specif-
learn the best rewards along with the policy network which ically,weincludeinformationdescribingthestatevariableas
is trained to learn the optimal policy for a given reward. In aper-armbinaryvalue,aswellassyntaxforaccessingfeature
our work, reflection serves a similar purpose as training of information from the codebase. From this context, we eval-
the meta-network but with the number of steps being dra- uate the ability for the LLM to propose reward functions in
matically smaller and the output being in the form of code, theformofcode,aspreviouslyproposedby[Maetal.,2024;
makingitinterpretable. Lietal.,2023].
Algorithm1DLMAlgorithm 4.4 DRLSimulation
We evaluate each LLM-proposed reward function R by
1: Input: Taskℓ,featurez,andcodeζ context 1:K
training policy networks θi under each proposed reward R
2: Hyperparameters:LoopiterationsI,proposalbatchK, n i
for each arm in simulation ∀n ∈ [N] (shown Alg. 1, lines
sim. epochsn ,sim. stepsn ,num. armsN
e s 8:20).Weconsiderasimulationspacewhichdefinesthenum-
3: foriteration=1toI do
berofarmsN,eachwithfixed,buthidden,transitiondynam-
4: ##LLMcandidaterewardproposal
ics. Following the procedure defined in Alg. 1, for a given
5: R 1:K ∼LLM(ℓ,z,ζ) rewardR ,wefirstsampleanaction-chargeλ,whichisused
i
6: Init. reflectionstringY to decouple the learning of arm policies (Eq. 3, Alg. 1 line
7: ##Multi-agentsimulations 13). Then, we simulate n timesteps, sampling trajectories
s
8: forrewardi=1toK do foreacharmundertheaction-chargeλ(Alg. 1lines14:16).
9: Init. policyandcriticnets. θ n,ϕ n ∀n∈[N] These trajectories are used in a buffer D (Alg. 1 line 17) to
10: Init. λ-net. Λ,bufferD,statess=s 0,featuresz eventuallyupdatethepolicyandcriticnetworksforeacharm
11: ClearreflectionstringY =[] (θ ,ϕ ), as proposed in [Killian et al., 2022] (Alg. 1 lines
n n
12: forepoch=1ton do 19:20). We compute advantage estimates for the actor from
e
13: λ=Λ(s) estimatedQ-values.
14: fortimestept=1ton do
s
15: a n ∼θ n(s n,λ) ∀n∈[N] 4.5 ReflectionStage
16: s′,r=Simulate(s,a,R i,z)
We propose the reflection comparison stage to enable LLM
17: D ←D∪{(s,a,r,s′,λ)}
self-reflection and iteration on proposed rewards. At a high
18: s←s′
level,theoutcomecomparisonproducesastringshowing,un-
19: Updateeach(θ n,ϕ n)withPPOusingD
dereachproposedrewardR ,thefinalsimulateddistribution
20: UpdateΛwithD i
of reward across key feature groups. Distributions, for in-
21: ##Outcomecomparison stance,maybeshownoverage,gender,income,orotherpre-
22: UpdateY ←Y ∪OutAnalysis(θ n,ϕ n,λ,Z) defined feature characteristic groupings. This comparison is
23: ##Topcandidateselection usedinplaceofanumerical,scalarfeedbackfunction. Prior
worksassumeaccesstosomebasefitnessfunctionsforself-
24: R ←LLM(ℓ,Y,“choosebest...”)
DLM reflection (F as defined above) [Singh et al., 2009]. In the
25: Updatecodecontextζ ←ζ∪{R }
DLM
resource-constrainedpublichealthdomain,wedonotassume
26: Output: R
DLM we have access to this ground truth function and extend the
rewardinputfromascalartoadistribution.
Thus, rather than providing feedback through the base
4.3 ProvidedDLMContext
fitness function, we instead show the distribution of states
In the reward candidate generation phase, we prompt the achievedacrossvariousfeaturegroupsg ∈ G; forexample,
LLM with three key contextual components (shown Alg. 1, overageorincomebrackets.Intuitively,becausewefocuson
line5).First,weprovideahuman-writtenlanguagecommand resource allocation in the public health domain, we propose
thatelucidatesdesiredpolicyoutcome. Inpractice,thisisthe thatsimulatedstatedistributionsprovidesufficientsignalfor
onlyhumaninputrequiredfortheproposedDLMtechnique. LLM to analyze the efficacy of proposed rewards, and select
Second,weprovidearmfeaturecategoryinformation,de- topcandidateexamplesforfuturein-contextlearning.
noted z. In the public health setting, features may include Weprovideanoverviewofthepolicyoutcomecomparison
demographic information, socioeconomic data, or othe de- procedureinAlg. 2. Giventrainedpolicyandcriticnetworks
scriptorsrelevanttopolicyoutcomes. WeproposethatLLMs θ ,ϕ ,finallambdavalueλ,andfeaturematrixZ∈RN×m,
n n
mayreasonablyinterpretthecontextualrelevanceofrespec- we first simulate over n evaluation timesteps (Alg. 2 lines
s
tivefeaturesfromagivenlanguageprompt,andadeptlypro- 7:11).NotethatwestoreonlyaccumulatedagentstatesS;we
pose reward functions using critical feature information to use accumulated per-agent states to later reflect on achieved
converge efficiently on desired policy outcomes. That is, allocationoutcomes. Wethencomputethepercentageofre-
given a language prompt ℓ we consider that LLMs may be ward attributed to arms within each sub-group g ∈ G (Alg.
usedtoextractthedirectlyrelevantfeaturesz ⊆ zexplic- 2 lines 13:16). Finally, we report final reward distributions
dir
itlyreferencedinℓtodesignarewardfunctionR. inanoutputstring. WethenqueryanLLMtoselectthebestFigure2: Mainresults. Wereportinterquartilemean[Agarwaletal.,2021]andstandarderror. Taskcolorsdenotedifficulty. Greentasks
consider only one feature group (i.e. age, income, etc). Yellow tasks tasks consider more than one feature group, or single features not
explicitlydefined(e.g.MiddleIncome).Redtasksarehighlyambiguousandrequirecompletelyinferringthecorrectfeatures(e.g.Earlyand
LateWorkershasnodirectfeature,requiringinferencefrompreferredcalltimes).Ourmethodisabletomatchbaserewardperformanceina
completelyautomatedfashion.Weevaluateforatotal10seeds,eachseedevaluatingfor50separatetrialsand10stepspertrial,totaling500
samplesof10stepseachpertask.ForeachRMAB,wehaveatotalof48armswithabudgetof16.
candidate example given the original prompt ℓ and the out- erscollectedbyARMMANinJanuary2022toconstructthe
comedistributionstringY (Alg. 1lines22:24). Thisselected environment’stransitionprobabilities. Thisprogram,specif-
example is then added as context in the next iteration of re- ically focused on low-income communities [Killian et al.,
wardgeneration(Alg. 1line25). 2022], employs health workers to provide service calls to
This approach has two key strengths. First, we enable high-risk beneficiaries; however, the number of women en-
greatertransparencyofourapproach,byusingrewarddistri- rolled in the program far outnumbers the number of health
butionstoevaluatetheeffectivenessofapolicyunderaspec- workers. Assuch, the aimis toidentify beneficiaries tocall
ifiedrewardπR.Byprovidingonlysimulatedoutcomedistri- tomaximizeengagementwiththeprogram, allowingforthe
butionsfromsimulationtothereflectionLLM,weensurethat mostefficientallocationofavailableresources.
self-reflection focuses only on alignment between the stated
language goal and the resource allocation outcomes of our 5.1 Tasks,Baselines,andMetrics
publichealthsettings,withoutassuminganyaccesstoground We provide experimental results for 11 distinct resource al-
truthreward. Second,weallowforgreaterflexibility; byin- location prompts, shown in Appendix, Table 1. We sepa-
vestigatingonlytheoutcomedistributionsoftheproposedre- rate tasks into three broad difficulty levels: simple single-
wards, rather than original policies themselves, we enable a feature tasks, middle-complexity multiple-feature tasks, and
model-agnosticapproachtoself-iterationinrewarddesign. challenging, highly ambiguous tasks requiring complete in-
ferenceofrelevantfeatures(seeFigure2). Foreachprompt,
5 ExperimentalEvaluation
we compute normalized reward over 10 seeds; we describe
We evaluate DLM in a simulated setting representative of thenormalizationequationbelow. Toevaluateourproposed
real-world resource allocation for maternal healthcare. We Decision-LanguageModel(DLM),wecompareagainstsev-
collaboratewithARMMAN,anon-profitbasedinIndiathat eralbaselinerewardfunctions:
enrollspregnantwomenintoavoicemessagingserviceshar- Random: Thispolicysamplesarmsuniformlyatrandom.
ing preventative care information. We use real-world data Throughnormalization,wesetrandomperformancescoreto
from a service quality improvement study of 44,000 moth- be 0, as our floor. This would be equivalent to a maximumAlgorithm2OutcomeAnalysis andthegeneratedrewardfunction. Thedownstreampolicyis
trainedfor2epochstotaltopreventoverfitting. Wedoato-
1: aIn ctp iou nt: -chaT rr ga ein λe ,d fep ao tuli rc ey, mc ar ti rt ii xc Zne ∈ts. RNθ n ×, mϕ n∀n ∈ [N], talof2roundsofreflectionandgenerate2candidatereward
functionsperround. AsdetailedinFigure2,weevaluatefor
2: Hyperparameters: Sim. stepsn
s a total 10 seeds, each seed evaluating for 50 separate trials
3: ##Step1:Simulaten timestepsunderθ
s n and10stepspertrial,totaling5000totalstepspertask.
4: Init. evaluationtotalsS←[0]×N
5: Init. percentagedistributionP∈R|G| 5.3 DLMSuccessandFailureCases
6: Init. OutputStringasanemptystring
7: fortimestept=1ton do WepresenttheexperimentalevaluationofDLMinFigure2.
s
8: a n ∼θ n(s n,λ) ∀n∈[N] We find that the proposed DLM technique is a capable re-
9: s′,r=Simulate(s,a,R i,z) warddesigneracrossabroadrangeoftasksinpublichealth,
10: S←S+s demonstrating, to the best of our knowledge, first example
11: s←s′ ofLLMsascapablepolicydesignersforresourceallocation
tasksinpublichealth. Notethatweevaluateunderasample-
12: ##Step2:Computestateoutcomedistributions
limited regime; thus, results demonstrate the ability to con-
13: G←DefinefeaturegroupswithinZ
vergeunderlimitedsamplestoanoptimalpolicyforagiven
14: foreachgroupg ∈Gdo
15: P(g)←
(cid:80)
(cid:80)i∈g
SSi p thr eom rep st u. ltG si iv ne Fn igth ui rs es 2e .tting,wemakeseveralobservationson
16: AppendtoOutputString: “g: P(g)%ofreward;” Achievingbaserewardperformance: WefindDLMisable
17: returnOutputString toreachtheachievedbaserewardperformanceacrossabroad
range of tasks. In particular, we find that the reflection pro-
cessisabletoimprovetheperformanceoftheLLMin9outof
entropyreward,orsimplyaconstantnon-negativerewardfor 11tasks,nearlyreachingbaserewardinmanycircumstances.
anyaction. NoAction: Thispolicysamplesnoarmsateach We find, in many cases, that LLMs can indeed identify the
timestep. In most cases this will yield a negative normal- relevantfeaturestouseinproposedrewardfunctions. Anex-
ized score, although in certain sparse reward tasks this may amplecanbeseenforthetaskofbiasingtowardeithertailof
beclosetotherandompolicy. Default: Thispolicyistrained the age distribution (Age Distribution Tail Emphasis), from
withthesameunderlyingalgorithmasDLM,butthereward whichtheLLMisabletogeneratethebaserewardzeroshot:
functionissettoprioritizeallbeneficiariesequally;thus,for
state * (agent_feats.youngest_agegroup or
each arm, reward=1 for a positive state and reward=0 for a
agent_feats.oldest_agegroup)
negative state. Most prior work in RMABs assume this re-
ward.Base:Thisbaselinesetstherewardequaltotheground Benefits of reflection: We observe that the proposed DLM
truth fitness function that all other methods are evaluated reflectiontechnique,whichgroundsLLMrewardgeneration
against. Weusethisrewardastheconsistentstandardagainst by incorporating feedback from RMAB simulations, is able
whichwemeasuretheperformanceofourproposedmethod to consistently align policies closer to desired human pref-
and other baselines. To be clear, each method uses a differ- erences. We note, however, that the degree of improvement
ent reward at train time, but uses Base reward at evaluation fromreflectionmayvarybytasktype. Forinstance,incases
time. WenormalizewithBasesothatnormalizedrewardis1 with extremely simple prompts, for example single feature
foreachtask. DLM(NoReflection):Thisversionofourpro- grouppreferences,rewardreflectionmaynotsubstantiallyen-
posed method ablates the reflection and iterative refinement hance allocation outcomes over the zeroshot LLM-proposed
process.Itallowsustoassesstheimportanceofthereflection rewards(i.e. AgeDistributionTailEmphasisabove).
loop in improving the quality of the LLM-generated reward Additionally, in highly ambiguous cases, such as Techno-
functions. DLM(Reflection): This is the full version of our logically Challenged, we find that reflection may not have
proposedmethod.
sufficient prompt guidance to meaningfully iterate on pro-
Thus,wecomputemeannormalizedrewardastheaverage posed rewards. Further, for cases with extreme ambiguity,
rewardachievedforeachpolicy,over10trainingseeds,using
requiringhighdegreesofreasoning,wefoundthattheLLM
baserewardduringevaluationofeachrespectivepolicy. We responsewouldfrequentlystate: ”Therewasnotenoughin-
then normalize each reward R as R−Rrand . A normalized formationspecifiedtosolvethetask”; inthesecases, noze-
Rbase−Rrand
reward of 0 therefore implies a policy’s effectiveness equiv- roshotrewardisproposed,obviatingthebenefitsofreflection.
alent to random allocation, while a normalized reward of 1 One potential solution, we suggest, may be to add a human
implies reward at the level of base reward policy. We addi- intheproposalloop,tofurtherspecifypromptsandrefinethe
tionallyshowthe“noaction”policyillustratingrewardunder reward function in highly ambiguous cases. We leave these
azeroallocationstrategy. potentialstrategiesforaddressinghighlyambiguoustasksto
futurework.
5.2 TrainingDetails
Misguided LLM reward proposals: We note that in cer-
WeusetheGeminiProLLMmodel[Teametal.,2023]from taincases,proposedrewardsmayhaveextremelypoorperfor-
Google to generate reward functions, and train our down- mancepriortoreflection(i.e.taskOlderBias).Inthesecases,
stream policy with RL using PPO [Schulman et al., 2017] weobservethat,whileataskmaybesemanticallysimple,theLLM may attempt to use multiple features improperly. For
example,forOlderBias,theLLMproposed:
state * agent_feats.secondoldest_agegroup *
agent_feats.oldest_agegroup
Above, the LLM incorrectly uses features in the reward
function, as an arm cannot be in both the second-oldest and
the oldest age group simultaneously. Thus, in cases where
improperlydesignedrewardsareproposed,reflectionmaybe
helpful (see Appendix E). For more examples of generated
rewardfunctions,pleaseseeAppendixF.
Middle-complexity tasks: We observe that the reflection Figure 3: DLM reflection distribution and selection visualization.
procedure may be most consistently helpful for the “middle DLMusesoutcomedistributionsfromtrainedpolicysimulationsto
range” of tasks that involve a composition of feature groups refineproposedrewards. Above, weshowtheselectedbestcandi-
within the reasoning capabilities of the LLM. This may be daterewardfortwoprompts. DLMisabletoreasonablyinterpret
given prompts using distributions across feature groups; however,
particularly helpful in cases with complex goals that cannot
formoreambiguousprompts(“technologicallychallenged”),these-
easily be translated into updated policies by public health
lectioncriteriamaybecomemoreopaque,potentiallyrequiringad-
practitioners. Forexample,inARMMAN’sKilkariprogram
ditionalhumanfeedback.
(a national level program in India, or operational across 19
states in India) [Bashingwa et al., 2021], different regions
mayhaveslightlydifferentpolicyrequirements,whichastate from, sample efficiency becomes extremely important. In
program manager can achieve directly through language in- thiscase,becausetherewardfunctiongeneratedbytheLLM
terfacingwithDLM. gives an order of magnitude higher weighting to specific
arms,itseemslikelythattheunderlyingpolicygradientalgo-
5.4 DLMReflectionExample
rithm PPO gains a large amount of sample efficiency in our
Wevisualizefortwoprompts,twocandidaterewarddistribu- setting. Itisimportanttonotethattheimprovementthatthe
tionsandtheselectedbestindexinFigure3. Weshow4rele- LLM yields over the base reward will shrink as the number
vantfeaturecategoriesofage,income,education,andphone ofsamplestakenduringtraininggoestoinfinity,asbydefini-
ownersforbothprompts. WeobservethattheDLMselection tionthebaserewarddefinestheoptimalpolicy. However,the
isabletoreasonablyinterpretcommands,suchasprioritizing highlyshapedrewardgivenbytheLLMinthissettinghelps
older ages, and select the distribution that best achieves the PPOconvergefasterforthistask. Itisalsoimportanttonote
given prompt. However, we note that for more ambiguous that the base reward may not be the most sample efficient
criteria,suchas“technologicallychallenged”,DLMselection reward; other more complex rewards relying on correlated
may bemore opaque; while the basereward for thisprompt featuresorwithdifferentrewardscalingarelikelymoreeffi-
considersonlyincomeandphoneownership, additionalfea- cient. Thus, thisopensthepossibilityofachievingpotential
tures such as education level may also be considered for se- superhumanperformanceonourtaskinthefuture.
lection. Inthesecases,wehypothesizethatadditionalhuman
feedbackmayberequiredtobetterdisambiguatepromptsfor
6 Conclusion
improved reflection. We provide the full reward reflection
textforthebelowexamplesinAppendixE. We propose a Decision-Language Model for resource allo-
cation tasks in public health. The proposed model can take
5.5 ExceedingBaseinCriticalLowIncome
language prompts describing complex, potentially ambigu-
We find that empirically for the “Critical Low Income” task ouspolicyoutcomegoalsasinput,andgeneratedownstream
theLLMisactuallyabletoimproveoverthebasereward.We control policies specialized for such goals. The proposed
hypothesizeherethattheDLMmodelmayproposeareward LLM-based approach can accurately capture the referenced
thatisofmuchlargerscalingthantheoriginalreward. Fora promptfeaturesanddesigneffectiverewardfunctionsinthe
representativeexample,weshowasamplerewardgenerated formofcode. Critically,ouralgorithmsenableself-iteration
bytheLLMfortheCriticalLowIncomeprompt: on proposed reward functions in a fully automated fashion.
BygroundingLLMrewardproposalswithoutcomedistribu-
(100 * state * agent_feats.
tionsfromRMABsimulations,wedemonstratetheabilityfor
no_or_lowest_income) + (100 * state *
agent_feats.second_lowest_income) + state the proposed DLM model to achieve near-base performance
acrossavarietyofpolicypreferencetasks.
Incomparison,thisisthecorrespondingbasereward:
state + if_(state) * 10 * if_(agent_feats. EthicalStatement
no_or_lowest_income or agent_feats.
Consent is obtained from every beneficiary enrolling in the
second_lowest_income or agent_feats.
ARMMAN’smobilehealthprogram(seealsoAppendixA).
third_lowest_income),
Forfuturedeploymentoftheproposedalgorithm, onecould
As the results reported in Figure 2 are taken in the low takeextrastepstoavoidunsafepolicies,including:(1)exam-
finite-sample regime, where there is not much data to train ining per beneficiary policy outcomes and performing spot-checks on trajectories; (2) apply the proposed algorithm to [Dworketal.,2012] CynthiaDwork,MoritzHardt,Toniann
a subgroup of the population; (3) provide additional context Pitassi, Omer Reingold, and Richard Zemel. Fairness
toLLMafterobservingtheimpactondifferentfeatures. We throughawareness. InProceedingsofthe3rdInnovations
discussmeasurestoaddressfairnessinAppendixB. in Theoretical Computer Science Conference, ITCS ’12,
page 214–226, New York, NY, USA, 2012. Association
References forComputingMachinery.
[Glazebrooketal.,2006] KevinDGlazebrook,DiegoRuiz-
[Agarwaletal.,2021] Rishabh Agarwal, Max Schwarzer,
Hernandez, and Christopher Kirkbride. Some indexable
PabloSamuelCastro,AaronCCourville,andMarcBelle-
familiesofrestlessbanditproblems. AdvancesinApplied
mare. Deepreinforcementlearningattheedgeofthesta-
Probability,38(3):643–672,2006.
tisticalprecipice.Advancesinneuralinformationprocess-
ingsystems,34:29304–29320,2021. [Glazebrooketal.,2011] Kevin D Glazebrook, David J
Hodge, and Chris Kirkbride. General notions of index-
[ARMMAN,2019] ARMMAN. Assessing the impact of
ability for queueing control and asset management. The
mobile-basedinterventiononhealthliteracyamongpreg-
AnnalsofAppliedProbability,2011.
nantwomeninurbanindia,2019. Accessed: 2022-08-12.
[Gra¨leretal.,2022] LeonoorGra¨ler,LeonardaG.M.Brem-
[BaltussenandNiessen,2006] Rob Baltussen and Louis
mers, Pieter Bakx, Job van Exel, and Marianne E van
Niessen. Prioritysettingofhealthinterventions: theneed
Bochove. Informal care in times of a public health cri-
formulti-criteriadecisionanalysis. Costeffectivenessand
sis:Objectiveburden,subjectiveburdenandqualityoflife
resourceallocation,4(1):1–9,2006.
of caregivers in the netherlands during the covid-19 pan-
[Bashingwaetal.,2021] Jean Juste Harrisson Bashingwa, demic. Health&SocialCareintheCommunity,2022.
Diwakar Mohan, Sara Chamberlain, Salil Arora, Jai
[HelpMum,2023] HelpMum. Helpmum: Preventingmater-
Mendiratta,SaiRahul,VinodChauhan,KerryScott,Neha
nalmortalityinnigeria,2023. Accessed: 2023-12-30.
Shah,OsamaUmmer,etal. Assessingexposuretokilkari:
a big data analysis of a large maternal mobile messag- [HoandErmon,2016] Jonathan Ho and Stefano Ermon.
ingserviceacross13statesinindia. BMJGlobalHealth, Generative adversarial imitation learning. Advances in
6(Suppl5):e005213,2021. neuralinformationprocessingsystems,29,2016.
[Boothetal.,2023] Serena Booth, W Bradley Knox, Julie [Husseinetal.,2017] Ahmed Hussein, Eyad Elyan, Mo-
Shah, Scott Niekum, Peter Stone, and Alessandro Al- hamed Medhat Gaber, and Chrisina Jayne. Deep reward
lievi. The perils of trial-and-error reward design: misde- shapingfromdemonstrations. In2017InternationalJoint
signthroughoverfittingandinvalidtaskspecifications. In ConferenceonNeuralNetworks(IJCNN),pages510–517.
Proceedings of the AAAI Conference on Artificial Intelli- IEEE,2017.
gence,volume37,pages5920–5929,2023.
[Killianetal.,2022] Jackson A Killian, Lily Xu, Arpita
[Choudhuryetal.,2021] Avishek Choudhury, Onur Asan, Biswas, and Milind Tambe. Restless and uncertain: Ro-
and Murari M Choudhury. Mobile health technology to bust policies for restless bandits via deep multi-agent re-
improve maternal health awareness in tribal populations: inforcement learning. In Uncertainty in Artificial Intelli-
mobileformothers. JournaloftheAmericanMedicalIn- gence,pages990–1000.PMLR,2022.
formaticsAssociation,28(11):2467–2474,2021.
[Killianetal.,2023] JacksonAKillian,ArpitaBiswas,Lily
[Dannetal.,2023] Christoph Dann, Yishay Mansour, and Xu, Shresth Verma, Vineet Nair, Aparna Taneja, Aparna
MehryarMohri. Reinforcementlearningcanbemoreeffi- Hegde, Neha Madhiwalla, Paula Rodriguez Diaz, Sonja
cient with multiple rewards. In Andreas Krause, Emma Johnson-Yu, et al. Robust planning over restless groups:
Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan engagement interventions for a large-scale maternal tele-
Sabato,andJonathanScarlett,editors,Proceedingsofthe health program. In Proceedings of the AAAI Conference
40thInternationalConferenceonMachineLearning,vol- onArtificialIntelligence,volume37,2023.
ume 202 of Proceedings of Machine Learning Research,
[Kruketal.,2018] MargaretEKruk,AnnaDGage,Cather-
pages6948–6967.PMLR,23–29Jul2023.
ine Arsenault, Keely Jordan, Hannah H Leslie, Sanam
[Deardorffetal.,2018] Katrina Deardorff, Arianna Rubin Roder-DeWan, Olusoji Adeyi, Pierre Barker, Bernadette
Means, Kristjana Hro¨nn A´sbjo¨rnsdo´ttir, and Judd L. Daelmans,SvetlanaVDoubova,etal. High-qualityhealth
Walson. Strategies to improve treatment coverage in systems in the sustainable development goals era: time
community-based public health programs: A systematic for a revolution. The Lancet global health, 6(11):e1196–
review of the literature. PLoS Neglected Tropical Dis- e1252,2018.
eases,12,2018.
[Kurjaketal.,2023] Asim Kurjak, Milan Stanojevic´, and
[Duetal.,2023] Yuqing Du, Olivia Watkins, Zihan Wang, Joachim Dudenhausen. Why maternal mortality in the
Ce´dric Colas, Trevor Darrell, Pieter Abbeel, Abhishek worldremainstragedyinlow-incomecountriesandshame
Gupta, and Jacob Andreas. Guiding pretraining in re- forhigh-incomeones: willsustainabledevelopmentgoals
inforcement learning with large language models. arXiv (sdg) help? Journal of Perinatal Medicine, 51(2):170–
preprintarXiv:2302.06692,2023. 181,2023.[Kwonetal.,2022] Minae Kwon, Sang Michael Xie, Kale- [Shahetal.,2023] Dhruv Shah, Błaz˙ej Osin´ski, Sergey
sha Bullard, and Dorsa Sadigh. Reward design with lan- Levine,etal. Lm-nav: Roboticnavigationwithlargepre-
guagemodels. In TheEleventhInternational Conference trainedmodelsoflanguage,vision,andaction. InConfer-
onLearningRepresentations,2022. enceonRobotLearning,pages492–504.PMLR,2023.
[Lietal.,2022] ShuangLi,XavierPuig,ChrisPaxton,Yilun [Shinnetal.,2024] NoahShinn, FedericoCassano, Ashwin
Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Gopinath,KarthikNarasimhan,andShunyuYao. Reflex-
EkinAkyu¨rek,AnimaAnandkumar,etal. Pre-trainedlan- ion: Languageagentswithverbalreinforcementlearning.
guage models for interactive decision-making. Advances Advances in Neural Information Processing Systems, 36,
in Neural Information Processing Systems, 35:31199– 2024.
31212,2022.
[Singhetal.,2009] Satinder Singh, Richard L Lewis, and
[Lietal.,2023] Hao Li, Xue Yang, Zhaokai Wang, Xizhou AndrewGBarto. Wheredorewardscomefrom. InPro-
Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng ceedings of the annual conference of the cognitive sci-
Li, Lewei Lu, and Jifeng Dai. Auto mc-reward: Auto- ence society, pages 2601–2606. Cognitive Science Soci-
mateddenserewarddesignwithlargelanguagemodelsfor
ety,2009.
minecraft. arXivpreprintarXiv:2312.09238,2023.
[Singhetal.,2010] Satinder Singh, Richard L Lewis, An-
[Maetal.,2024] Yecheng Jason Ma, William Liang,
drew G Barto, and Jonathan Sorg. Intrinsically moti-
Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh
vated reinforcement learning: An evolutionary perspec-
Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandku-
tive. IEEE Transactions on Autonomous Mental Devel-
mar. Eureka: Human-levelrewarddesignviacodinglarge
opment,2(2):70–82,2010.
languagemodels.InTheTwelfthInternationalConference
onLearningRepresentations,2024. [Sorgetal.,2010] Jonathan Sorg, Richard L Lewis, and
SatinderSingh. Rewarddesignviaonlinegradientascent.
[Mataric,1994] MajaJMataric. Rewardfunctionsforaccel-
Advances in Neural Information Processing Systems, 23,
erated learning. In Machine learning proceedings 1994,
2010.
pages181–189.Elsevier,1994.
[Mateetal.,2022] Aditya Mate, Lovish Madaan, Aparna [SuttonandBarto,2018] Richard S Sutton and Andrew G
Barto. Reinforcement learning: An introduction. MIT
Taneja, Neha Madhiwalla, Shresth Verma, Gargi Singh,
press,2018.
AparnaHegde,PradeepVarakantham,andMilindTambe.
Fieldstudyindeployingrestlessmulti-armedbandits: As- [Tambe,2022] MilindTambe. Aiforsocialimpact: Results
sistingnon-profitsinimprovingmaternalandchildhealth. fromdeploymentsforpublichealthandconversation.Pro-
InProceedingsoftheAAAIConferenceonArtificialIntel-
ceedingsofthe28thACMSIGKDDConferenceonKnowl-
ligence,volume36,pages12017–12025,2022.
edgeDiscoveryandDataMining,2022.
[Nakhlehetal.,2021] Khaled Nakhleh, Santosh Ganji,
[Teametal.,2023] Gemini Team, Rohan Anil, Sebastian
Ping-ChunHsieh,IHou,SrinivasShakkottai,etal. Neur-
Borgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,
win: Neural whittle index network for restless bandits
Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja
via deep rl. Advances in Neural Information Processing
Hauth, et al. Gemini: a family of highly capable multi-
Systems,34:828–839,2021.
modalmodels. arXivpreprintarXiv:2312.11805,2023.
[Ngetal.,1999] Andrew Y Ng, Daishi Harada, and Stuart
[UnitedNationsandDevelopment,2015] Department
Russell. Policy invariance under reward transformations:
of Economic United Nations and Social Affairs Sustain-
Theory and application to reward shaping. In Icml, vol-
able Development. Transforming our world: the 2030
ume99,pages278–287.Citeseer,1999.
agendaforsustainabledevelopment,2015.
[Nishtalaetal.,2023] Siddharth Nishtala, Lovish Madaan,
[Vempralaetal.,2023] Sai Vemprala, Rogerio Bonatti,
Aditya Mate, Harshavardhan Kamarthi, Anirudh Grama,
ArthurBucker,andAshishKapoor. Chatgptforrobotics:
Divy Thakkar, Dhyanesh Narayanan, Suresh Chaudhary,
Design principles and model abilities. Microsoft Auton.
NehaMadhiwalla,RameshPadmanabhan,etal. Selective
Syst.Robot.Res,2:20,2023.
intervention planning using restless multi-armed bandits
toimprovematernalandchildhealthoutcomes. Proceed- [Wangetal.,2023] KaiWang,ShresthVerma,AdityaMate,
ingsofthe2023InternationalConferenceonAutonomous
Sanket Shah, Aparna Taneja, Neha Madhiwalla, Aparna
AgentsandMultiagentSystems,page1312–1320,2023.
Hegde, and Milind Tambe. Scalable decision-focused
[Organizationandothers,2015] World Health Organization learning in restless multi-armed bandits with application
etal. Strategiestowardsendingpreventablematernalmor- tomaternalandchildhealth. InProceedingsoftheAAAI
tality(epmm). WorldHealthOrganization,2015. Conference on Artificial Intelligence, volume 37, pages
12138–12146,2023.
[Schulmanetal.,2017] John Schulman, Filip Wolski, Pra-
fulla Dhariwal, Alec Radford, and Oleg Klimov. Prox- [WeberandWeiss,1990] Richard R Weber and Gideon
imal policy optimization algorithms. arXiv preprint Weiss. On an index policy for restless bandits. Journal
arXiv:1707.06347,2017. ofappliedprobability,27(3):637–648,1990.[Whittle,1988] PeterWhittle. Restlessbandits: Activityal-
locationinachangingworld. Journalofappliedprobabil-
ity,25(A):287–298,1988.
[Zhengetal.,2020] ZeyuZheng, JunhyukOh, MatteoHes-
sel, Zhongwen Xu, Manuel Kroiss, Hado Van Hasselt,
David Silver, and Satinder Singh. What can learned in-
trinsic rewards capture? In International Conference on
MachineLearning,pages11436–11446.PMLR,2020.Appendix
To learn an action charge λ, we follow an updating rule
[Killianetal.,2022]:
A Datausage,consent,andsecondary (cid:32) N H (cid:33)
B (cid:88)(cid:88)
analysis Λ ←Λ −α + Eβtc ,
t t−1 Λ 1−β n,t
The data is collected from a service quality improvement n=1t=0
study of 44K mothers conducted by ARMMAN in January wherec isthecostoftheactiontakenbytheoptimalpolicy
n,t
2022, and consent is obtained from every beneficiary en- onarmninroundt,andα isthestepsize.
Λ
rolling in the NGO’s mobile health program. The data col-
lected through the program is owned by the NGO and only
D ProvidedTasks
the NGO is allowed to share data. The data exchange and
usage were regulated by clearly defined exchange protocols We show provided tasks in Table 1. We note that we eval-
includinganonymization,read-accessonlytoresearchers,re- uate on a wide variety of tasks within the real of resource
stricted use of the data for research purposes only, and ap- allocation for maternal health. In particular, we test across
provalbyARMMAN’sethicsreviewcommittee. Noperson- featurecharacteristicssuchasage,income,spokenlanguages
allyidentifiableinformation(PII)isavailabletoauthors. We (animportantcharacteristicindicatingethnicityintheIndian
use anonymized call listenership logs to calculate empirical subcontienent), education, and other relevant features that
transitionprobabilitiesinexperiments. Ourexperimentfalls may be considered in real-world allocation tasks. We pro-
intosecondaryanalysisofthedatasharedbyARMMANand vide these tasks directly as context to the LLM, in addition
doesnotinvolvethedeploymentoftheproposedalgorithmor torelevantfeaturecharacteristicsandtheircorrespondingde-
anyotherbaselinestotheservicecallprogram. scriptions. Per seed, we fix the randomness in the sampled
setof48armsfromARMMAN’sdataset,thedeepRLneural
B AddressingFairness network initialization, batch sampling from the dataset, and
thesamplingofstatesforeacharmduringevaluation. How-
When LLM used is not well-aligned or calibrated to human
everpleasenotethatwecannotcontroltherandomnessofthe
values,theremaybeissuesoffairnessorbias. Althoughthis
GeminiLLMgenerationasweonlyhaveaccessthroughthe
workisasecondaryanalysisdoesnotinvolvedeploymentof
GoogleAPIinterface.
proposealgorithms,futureworksonfieldstudiesmayimpos-
ingfairnessconstraintsontherewardfunctiongenerationand
E SampleRewardReflectionOutputs
useautomaticdetectionofunfairness[Dworketal.,2012]to
incorporatefairness. Forfutureworksondeployment,toen- Here we provide the full output of two sample reward re-
suresafeandresponsibledeployment,itwouldbeinteresting flectionprocedures,inadditiontothecorrespondingselected
tostudyhowtodetectgivenpromptsthataredesignedsimi- LLMcandidatereward.
larlytoaleadingquestiontoinducehiddenbiases.
E.1 Task: OlderAgeBias
C DeepRLforRMABs
My goal was to create a Python reward
TheLagrangianrelaxationallowsustodisentanglethelearn- function for RL in resource allocation,
ingofarmpoliciesandanactionchargeλ. with the objective of: Focus on the most
Tolearnarmpolicies,itiscommontouseRLapproaches elderly by age distribution. I tried
such as tabular Q-learning, deep Q-learning, or policy gra- several reward functions for this task. Below
dientalgorithms[Nakhlehetal.,2021]. Inthispaper,weuse , I have the given reward function,
and the corresponding distribution of reward
PPO[Schulmanetal.,2017],apolicygradientalgorithmwith
achieved across 44 agent features.
an actor-critic structure. Specifically, the Q-function can be
A description of the features is as follows:
updatedasfollows:
Index Name DataType
Qt(s ,a )←(1−α )Qt−1(s ,a ) [sensitive features hidden]
n n n q n n n 7. Ages 10-20 - Binary
(cid:16) (cid:17)
+α R(s )−λc ,βmaxQ(s′ ,a) , 8. Ages 21-30 - Binary
q n an a n 9. Ages 31-40 - Binary
10. Ages 41-50 - Binary
whereα isthelearningrate.
q 11. Ages 51-60 - Binary
Foranactor-criticmethods,afterperforminganupdateof
12. Speaks Hindi - Binary
theQ-functioninthecritic,onemaycomputetheadvantage 13. Speaks Marathi - Binary
estimate: 14. Speaks Gujurati - Binary
A(s,a)=Q(s,a)−V (s). 15. Speaks Kannada - Binary
16. Education level 1/7 -- illiterate -
In practice, the values A(s,a),Q(s,a),V(s) will be evalu-
Binary
atedbasedonthecurrentpolicyπ. Intuitively,theadvantage
17. Education level 2/7 -- 1-5th Grade
estimate tells us how much better a is compared to the cur- Completed - Binary
rentpolicyπ.Theadvantageestimateisthenpluggedintothe 18. Education level 3/7 -- 6-9th Grade
policygradienttoupdatetheactor. Completed - BinaryTable1:Fullpromptlist
Label TaskCommand BaseFunction NoAction Base Human Default DLM (No DLM (Re-
Reflection) flection)
Afternoon Advantagethosewhoprefer state*.1 + if(state) * 10 -0.50±0.01 1.00±0.07 -0.21±0.06 0.88±0.05 0.44±0.07 0.82±0.06
Call+NGO beingcalledbetween * if(feature[29-1] and
Registered 12:30pm-3:30pmandare feature[33-1])
registeredatanNGO.
LowIncome Combinethedistributionsof state*.1 + if(state) * 10 * -0.47±0.00 1.00±0.02 0.54±0.03 0.42±0.15 0.56±0.02 0.96±0.02
+Younger ‘age’and‘education’toheavily if(feature[8-1]andfeature[17-
Bias giveprecedencetolowincome 1])
impoverishedyouth.
Marathi Fixateonthose state*.1 + if(state) * 10 -0.54±0.01 1.00±0.06 0.38±0.06 0.68±0.02 0.77±0.05 0.86±0.04
Speakers+ Marathi-speakerswith * if(feature[14-1] and
MiddleAged middle-agedfolks. (feature[10-1]orfeature[11-1]))
OlderBias Focusonthemostelderlyby state*.1 + if(state) * 10 * -0.70±0.05 1.00±0.18 0.48±0.16 0.78±0.01 -0.60±0.16 0.71±0.17
agedistribution. feature[12-1]
Age Focusonlyonboththeyoung state * if(feature[12-1] or -1.19±0.04 1.00±0.15 0.73±0.14 0.82±0.03 1.13±0.11 1.09±0.17
Distribution andelderly. feature[8-1])
TailEmphasis
Earlyand Givemoreattentionfor state*.1 + if(state) * 10 * -0.60±0.00 1.00±0.03 0.84±0.03 0.75±0.03 0.63±0.03 0.97±0.04
LateWorkers beneficiarieswholikelywork if(feature[27-1]orfeature[29-
earlyinthemorningandlateat 1])
night.
Techincally Infertechnicalchallengesin state*.1 + if(state) * 10 * -0.82±0.03 1.00±0.14 0.55±0.11 0.66±0.02 1.23±0.15 1.00±0.19
Challenged reachingthephonethatcould if((feature[25-1]orfeature[26-
indicate‘at-risk’beneficiaries 1])) + if(state) * 10 * 0.2 *
andgivepreference. if((feature[37-1]orfeature[38-
1]))
Middle Prefertheincomebracket state*.1 + if(state) * 10 * -0.65±0.00 1.00±0.03 0.50±0.03 0.95±0.02 0.37±0.02 1.01±0.03
Income boundsforthemiddle40%of if(feature[39-1]orfeature[40-
thepopulation. 1]orfeature[41-1])
Hindi Selectsolelythosewhospeak state*.1*if(feature[13-1]) -0.62±0.01 1.00±0.04 0.24±0.04 0.87±0.01 0.67±0.06 0.81±0.07
Speakers Hinditoanalyzehowthis
wouldaffectthecallsmadeto
ourbeneficiaries.
LowIncome Thelowestincomebracket state + if(state) * 10 * -0.47±0.00 1.00±0.04 0.11±0.05 0.72±0.03 0.74±0.04 0.96±0.04
distributiontailisvery feature[37-1]
important,focusheavilyonthe
top12.5%.
CriticalLow Weigththelowest state + if(state) * 10 * -0.66±0.00 1.00±0.03 0.33±0.04 0.71±0.03 0.32±0.02 1.46±0.03
Income incomebracketgroups,the if(feature[36-1]orfeature[37-
lowestearnersinthe 1]orfeature[38-1])
population.
19. Education level 4/7 -- 10th Grade Passed 41. Income bracket 6 (e.g., 25001-30000) -
- Binary Binary
20. Education level 5/7 -- 12th Grade Passed 42. Income bracket 7 (e.g., 30000-999999) -
- Binary Binary
21. Education level 6/7 -- Graduate - Binary
22. Education level 7/7 -- Post graduate -
Binary Below are the reward functions I used and
23. Phone owner 0 (e.g., woman) - Binary their corresponding reward
24. Phone owner 1 (e.g., husband) - Binary distributions:
25. Phone owner 2 (e.g., family) - Binary
26. To be called from 8:30am-10:30am - Index 0:
Binary Reward Function:
27. To be called from 10:30am-12:30pm - state*agent_feats[10]+state+(state*
Binary agent_feats[10]==0)*agent_feats[11]
28. To be called from 12:30pm-3:30pm - Reflection:
Binary
29. To be called from 3:30pm-5:30pm - Binary [sensitive features hidden]
30. To be called from 5:30pm-7:30pm - Binary
31. To be called from 7:30pm-9:30pm - Binary Category: Ages
32. NGO - Binary Ages 10-20: 1.30%
33. ARMMAN - Binary Ages 21-30: 13.46%
34. PHC - Binary Ages 31-40: 10.46%
35. Income bracket -1 (no income) - Binary Ages 41-50: 2.57%
36. Income bracket 1 (e.g., 0-5000) - Binary Ages 51-60: 72.23%
37. Income bracket 2 (e.g., 5001-10000) -
Binary Category: Income
38. Income bracket 3 (e.g., 10001-15000) - Income bracket -1 (no income): 0.00%
Binary Income bracket 1 (e.g., 0-5000): 7.38%
39. Income bracket 4 (e.g., 15001-20000) - Income bracket 2 (e.g., 5001-10000): 10.04%
Binary Income bracket 3 (e.g., 10001-15000): 79.31%
40. Income bracket 5 (e.g., 20001-25000) - Income bracket 4 (e.g., 15001-20000): 1.32%
Binary Income bracket 5 (e.g., 20001-25000): 1.29%Income bracket 6 (e.g., 25001-30000): 0.00% 3:30pm-5:30pm: 13.50%
Income bracket 7 (e.g., 30000-999999): 0.67% 5:30pm-7:30pm: 11.58%
7:30pm-9:30pm: 11.94%
Category: Calling Times
8:30am-10:30am: 79.52% Category: Education Levels
10:30am-12:30pm: 2.65% Illiterate: 10.12%
12:30pm-3:30pm: 7.85% 1-5th Grade Completed: 8.83%
3:30pm-5:30pm: 3.33% 6-9th Grade Completed: 26.09%
5:30pm-7:30pm: 2.68% 10th Grade Passed: 29.74%
7:30pm-9:30pm: 3.98% 12th Grade Passed: 11.35%
Graduate: 2.27%
Category: Education Levels Post graduate: 11.60%
Illiterate: 2.80%
1-5th Grade Completed: 2.56% Category: Languages Spoken
6-9th Grade Completed: 5.37% Speaks Hindi: 66.41%
10th Grade Passed: 9.15% Speaks Marathi: 33.59%
12th Grade Passed: 3.96% Speaks Gujurati: 0.00%
Graduate: 0.67% Speaks Kannada: 0.00%
Post graduate: 75.49%
Category: Phone Owners
Category: Languages Spoken Phone owner - Wom.: 95.01%
Speaks Hindi: 90.92% Phone owner - Hus.: 4.99%
Speaks Marathi: 9.08% Phone owner - Fam.: 0.00%
Speaks Gujurati: 0.00%
Speaks Kannada: 0.00% Category: Organizations
NGO: 71.38%
Category: Phone Owners ARMMAN: 28.62%
Phone owner - Wom.: 98.59% PHC: 0.00%’
Phone owner - Hus.: 1.41%
Phone owner - Fam.: 0.00% Based on the above reward distributions and
the given goal: Focus on the most
Category: Organizations elderly by age distribution., please identify
NGO: 90.70% the index of the most effective
ARMMAN: 9.30% reward function. Provide your answer EXACTLY
PHC: 0.00%’ IN the following format: ’The best
reward function is at index: [INDEX]’.
Index 1: The best reward function is at index: 0
Reward Function: state * agent_feats[10] *
agent_feats[11]
Reflection: E.2 Task: TechnologicallyChallenged
’
My goal was to create a Python reward
[some sensitive features hidden] function for RL in resource allocation,
with the objective of: Infer technical
Category: Ages challenges in reaching the phone that
Ages 10-20: 4.45% could indicate ’at-risk’ beneficiaries and
Ages 21-30: 49.12% give preference. I tried several
Ages 31-40: 37.26% reward functions for this task. Below, I have
Ages 41-50: 9.18% the given reward function, and the
Ages 51-60: 0.00% corresponding distribution of reward achieved
across 44 agent features. A
Category: Income description of the features is as follows:
Income bracket -1 (no income): 0.00% Index Name DataType
Income bracket 1 (e.g., 0-5000): 23.70% [sensitive features hidden]
Income bracket 2 (e.g., 5001-10000): 42.50% 7. Ages 10-20 - Binary
Income bracket 3 (e.g., 10001-15000): 24.82% 8. Ages 21-30 - Binary
Income bracket 4 (e.g., 15001-20000): 4.48% 9. Ages 31-40 - Binary
Income bracket 5 (e.g., 20001-25000): 4.50% 10. Ages 41-50 - Binary
Income bracket 6 (e.g., 25001-30000): 0.00% 11. Ages 51-60 - Binary
Income bracket 7 (e.g., 30000-999999): 0.00% 12. Speaks Hindi - Binary
13. Speaks Marathi - Binary
Category: Calling Times 14. Speaks Gujurati - Binary
8:30am-10:30am: 25.53% 15. Speaks Kannada - Binary
10:30am-12:30pm: 11.45% 16. Education level 1/7 -- illiterate -
12:30pm-3:30pm: 26.00% Binary17. Education level 2/7 -- 1-5th Grade Income bracket 2 (e.g., 5001-10000): 48.97%
Completed - Binary Income bracket 3 (e.g., 10001-15000): 47.48%
18. Education level 3/7 -- 6-9th Grade Income bracket 4 (e.g., 15001-20000): 1.17%
Completed - Binary Income bracket 5 (e.g., 20001-25000): 0.82%
19. Education level 4/7 -- 10th Grade Passed Income bracket 6 (e.g., 25001-30000): 0.00%
- Binary Income bracket 7 (e.g., 30000-999999): 0.00%
20. Education level 5/7 -- 12th Grade Passed
- Binary Category: Calling Times
21. Education level 6/7 -- Graduate - Binary 8:30am-10:30am: 2.75%
22. Education level 7/7 -- Post graduate - 10:30am-12:30pm: 2.44%
Binary 12:30pm-3:30pm: 45.75%
23. Phone owner 0 (e.g., woman) - Binary 3:30pm-5:30pm: 1.57%
24. Phone owner 1 (e.g., husband) - Binary 5:30pm-7:30pm: 45.09%
25. Phone owner 2 (e.g., family) - Binary 7:30pm-9:30pm: 2.39%
26. To be called from 8:30am-10:30am -
Binary Category: Education Levels
27. To be called from 10:30am-12:30pm - Illiterate: 0.00%
Binary 1-5th Grade Completed: 2.79%
28. To be called from 12:30pm-3:30pm - 6-9th Grade Completed: 48.13%
Binary 10th Grade Passed: 2.42%
29. To be called from 3:30pm-5:30pm - Binary 12th Grade Passed: 3.70%
30. To be called from 5:30pm-7:30pm - Binary Graduate: 0.40%
31. To be called from 7:30pm-9:30pm - Binary Post graduate: 42.56%
32. NGO - Binary
33. ARMMAN - Binary Category: Languages Spoken
34. PHC - Binary Speaks Hindi: 10.79%
35. Income bracket -1 (no income) - Binary Speaks Marathi: 89.21%
36. Income bracket 1 (e.g., 0-5000) - Binary Speaks Gujurati: 0.00%
37. Income bracket 2 (e.g., 5001-10000) - Speaks Kannada: 0.00%
Binary
38. Income bracket 3 (e.g., 10001-15000) - Category: Phone Owners
Binary Phone owner - Wom.: 16.37%
39. Income bracket 4 (e.g., 15001-20000) - Phone owner - Hus.: 83.63%
Binary Phone owner - Fam.: 0.00%
40. Income bracket 5 (e.g., 20001-25000) -
Binary Category: Organizations
41. Income bracket 6 (e.g., 25001-30000) - NGO: 95.53%
Binary ARMMAN: 4.47%
42. Income bracket 7 (e.g., 30000-999999) - PHC: 0.00%’
Binary
Index 1:
Reward Function: -3.*(agent_feats[30] or
Below are the reward functions I used and agent_feats[31] or agent_feats[32] or
their corresponding reward agent_feats[33] or agent_feats[34] or
distributions: agent_feats[35] or agent_feats[36] or
agent_feats[37] or agent_feats[38] or
Index 0: agent_feats[39]) + state*1.
Reward Function: 2*state*(agent_feats[0]<36) Reflection:
+ 3*state*(agent_feats[4]==1 or ’
agent_feats[4]==2) + 4*state*(agent_feats [sensitive features hidden]
[5]<2)
Reflection: Category: Ages
’ Ages 10-20: 0.91%
[sensitive features hidden] Ages 21-30: 50.16%
Ages 31-40: 48.70%
Category: Ages Ages 41-50: 0.23%
Ages 10-20: 1.97% Ages 51-60: 0.00%
Ages 21-30: 8.36%
Ages 31-40: 89.25% Category: Income
Ages 41-50: 0.42% Income bracket -1 (no income): 0.00%
Ages 51-60: 0.00% Income bracket 1 (e.g., 0-5000): 0.66%
Income bracket 2 (e.g., 5001-10000): 72.25%
Category: Income Income bracket 3 (e.g., 10001-15000): 25.95%
Income bracket -1 (no income): 0.00% Income bracket 4 (e.g., 15001-20000): 0.67%
Income bracket 1 (e.g., 0-5000): 1.56% Income bracket 5 (e.g., 20001-25000): 0.47%Income bracket 6 (e.g., 25001-30000): 0.00% 1: state * (agent_feats.age_51_to_60 or
Income bracket 7 (e.g., 30000-999999): 0.00% agent_feats.ages41_to_50)
2: state * agent_feats.age_51_to_60
Category: Calling Times 3: 2 * state * (agent_feats.age_51_to_60 or (
8:30am-10:30am: 1.10% agent_feats.ages41_to_50 and agent_feats.
10:30am-12:30pm: 1.38% to_be_called730pm_930pm))
12:30pm-3:30pm: 47.13% 4: state * agent_feats.ages41_to_50 or state
3:30pm-5:30pm: 0.89% * agent_feats.age_51_to_60
5:30pm-7:30pm: 24.65% 5: state * (agent_feats.age_51_to_60 or
7:30pm-9:30pm: 24.85% agent_feats.ages41_to_50) + 1
6: 2*state + 20 * (agent_feats.age_51_to_60
Category: Education Levels and state)
Illiterate: 0.00% 7: state * (agent_feats.ages41_to_50 or
1-5th Grade Completed: 1.36% agent_feats.age_51_to_60)
6-9th Grade Completed: 48.56% 8: state * (agent_feats.age_51_to_60 or (
10th Grade Passed: 1.14% agent_feats.ages41_to_50 and agent_feats.
12th Grade Passed: 25.36% age_51_to_60))
Graduate: 0.22% 9: state * (agent_feats.age_51_to_60 or
Post graduate: 23.37% agent_feats.ages41_to_50)
Category: Languages Spoken
F.2 Thelowestincome bracketdistributiontailis
Speaks Hindi: 27.75%
Speaks Marathi: 72.25% veryimportant,focusheavilyonthetop
Speaks Gujurati: 0.00% 12.5%.
Speaks Kannada: 0.00%
Ground truth base: lambda agent_feats, state:
Category: Phone Owners state + if_(state) * HEAVILY_MODIFIER *
Phone owner - Wom.: 8.81% agent_feats.second_least_income
Phone owner - Hus.: 69.26% 0: state * max(0, agent_feats.
Phone owner - Fam.: 21.93% highest_income_bracket - agent_feats.
no_or_least_income)
Category: Organizations 1: state * (agent_feats.
NGO: 97.52% third_highest_income_bracket or
ARMMAN: 2.48% agent_feats.second_highest_income_bracket
PHC: 0.00%’ or agent_feats.highest_income_bracket)
2: state * max(0, agent_feats.null - 0.875) *
(agent_feats.highest_income_bracket or
Based on the above reward distributions and agent_feats.second_highest_income_bracket
the given goal: Infer technical or agent_feats.
challenges in reaching the phone that could third_highest_income_bracket)
indicate ’at-risk’ beneficiaries and 3: state * ((agent_feats.second_least_income
give preference., please identify the index and not (agent_feats.no_or_least_income))
of the most effective reward or (agent_feats.
function. Provide your answer EXACTLY IN the third_lowest_income_bracket and not (
following format: ’The best reward agent_feats.second_least_income)) or (
function is at index: [INDEX]’. agent_feats.fourth_lowest_income_bracket
The best reward function is at index: 1 and not (agent_feats.
third_lowest_income_bracket)) or (
agent_feats.middle_income_bracket and not
F GeneratedRewardFunctions
(agent_feats.
HerewegiveallgeneratedrewardfunctionsseeninTable1 fourth_lowest_income_bracket)) or (
fromtheLLM.Wefirstgivethetaskdescription,whichisfed agent_feats.third_highest_income_bracket
totheLLM,thecorrespondinggroundtruthbasereward,and and not (agent_feats.
middle_income_bracket)) or (agent_feats.
thenthegeneratedLLMrewardsinorderofseed. Notethat
second_highest_income_bracket or
wehaveprocessedtheoriginalrawindicesintofeaturelabels
agent_feats.highest_income_bracket))
forinterpretability.
4: state * (agent_feats.
F.1 Focusonthemostelderlybyagedistribution. to_be_called730pm_930pm or agent_feats.
no_or_least_income) or (agent_feats.
Ground truth base: lambda agent_feats, state: second_least_income and agent_feats.
state*.1 + if_(state) * 10.0 * third_lowest_income_bracket and
agent_feats.age_51_to_60 agent_feats.fourth_lowest_income_bracket
0: state * (agent_feats.age_51_to_60 or ( and agent_feats.middle_income_bracket and
agent_feats.age_51_to_60 and agent_feats. agent_feats.third_highest_income_bracket
days_to_first_call >= 10)) and agent_feats.second_highest_income_bracket and 2: state * (agent_feats.ages10_to_20 or
agent_feats.highest_income_bracket) agent_feats.age_51_to_60)
5: (agent_feats.highest_income_bracket and 3: state * (agent_feats.ages10_to_20 or (
state * 8) - (agent_feats. agent_feats.ages41_to_50 and agent_feats.
no_or_least_income and state * 4) age_51_to_60))
6: state * (5*agent_feats. 4: state * (agent_feats.ages41_to_50 or
highest_income_bracket - (agent_feats. agent_feats.ages10_to_20)
no_or_least_income and not(agent_feats. 5: state * ((agent_feats.age_51_to_60 and not
second_least_income or agent_feats. agent_feats.ages10_to_20) or (
third_lowest_income_bracket or agent_feats.ages10_to_20 and not
agent_feats.fourth_lowest_income_bracket agent_feats.age_51_to_60))
or agent_feats.middle_income_bracket or 6: state * ((agent_feats.ages10_to_20 or
agent_feats.third_highest_income_bracket agent_feats.ages41_to_50) and not(
or agent_feats. agent_feats.ages_21_to_30 or agent_feats.
second_highest_income_bracket))) ages_31_to_40))
7: 100000 * state * (agent_feats. 7: state*(agent_feats.ages10_to_20 and
third_highest_income_bracket or agent_feats.age_51_to_60)
agent_feats.second_highest_income_bracket 8: state * ((agent_feats.ages10_to_20 and
or agent_feats.highest_income_bracket) agent_feats.ages41_to_50) or (agent_feats
8: state * 100 * agent_feats. .age_51_to_60 and agent_feats.
highest_income_bracket**2 ages41_to_50))
9: state * (agent_feats. 9: state * (agent_feats.ages10_to_20 or
second_highest_income_bracket and agent_feats.age_51_to_60)
agent_feats.highest_income_bracket)
F.5 Weigththelowestincome bracketgroups,the
lowestearnersinthepopulation.
F.3 SelectsolelythosewhospeakHinditoanalyze
howthiswouldaffectthecallsmadetoour Ground truth base: lambda agent_feats, state:
beneficiaries. state + if_(state) * 10.0 * if_(
agent_feats.least_income_bracket or
Ground truth base: lambda agent_feats, state: agent_feats.second_least_income or
state*.1 * if_(agent_feats.speaks_hindi) agent_feats.third_lowest_income_bracket)
0: state * agent_feats.speaks_hindi 0: state * (1 - agent_feats.
1: state * (agent_feats.speaks_hindi and not no_or_least_income - agent_feats.
(agent_feats.speaks_marathi or second_least_income)
agent_feats.speaks_gujurati or 1: state * (1-agent_feats.no_or_least_income)
agent_feats.speaks_kannada)) * (1-agent_feats.second_least_income) *
2: (2 * state * agent_feats.speaks_hindi) + (1-agent_feats.
(3 * state) third_lowest_income_bracket) * (1-
3: state * (agent_feats.speaks_hindi and (not agent_feats.fourth_lowest_income_bracket)
agent_feats.speaks_marathi and not * (1-agent_feats.middle_income_bracket)
agent_feats.speaks_gujurati and not * (1-agent_feats.
agent_feats.speaks_kannada)) third_highest_income_bracket) * (1-
4: state * agent_feats.speaks_hindi agent_feats.second_highest_income_bracket
5: state*(agent_feats.speaks_hindi) )
6: state * agent_feats.speaks_hindi 2: state * (not agent_feats.
7: 1.2 * state * agent_feats.speaks_hindi no_or_least_income or not agent_feats.
8: state * (agent_feats.speaks_hindi and ( second_least_income)
agent_feats.speaks_hindi or (not 3: state * (agent_feats.no_or_least_income or
agent_feats.speaks_hindi and not agent_feats.second_least_income or
agent_feats.speaks_marathi and not agent_feats.third_lowest_income_bracket
agent_feats.speaks_gujurati and not or agent_feats.
agent_feats.speaks_kannada))) fourth_lowest_income_bracket or
9: state * 2 * agent_feats.speaks_hindi agent_feats.middle_income_bracket or
agent_feats.third_highest_income_bracket
or agent_feats.
F.4 Focusonlyonboththeyoungandelderly. second_highest_income_bracket or
agent_feats.highest_income_bracket)
Ground truth base: lambda agent_feats, state: 4: 100 * state * agent_feats.
state * if_(agent_feats.ages_51_to_60 or no_or_least_income
agent_feats.ages_10_to_20) 5: 25 + state * agent_feats.
0: state * (agent_feats.ages10_to_20 or no_or_least_income + (50 * state *
agent_feats.age_51_to_60) agent_feats.second_least_income)
1: state * (agent_feats.ages10_to_20 and 6: state * (agent_feats.no_or_least_income or
agent_feats.age_51_to_60) agent_feats.second_least_income oragent_feats.third_lowest_income_bracket 5: state * (agent_feats.ages10_to_20 and (
or agent_feats. agent_feats.education_level_6 or
fourth_lowest_income_bracket or agent_feats.education_level_7) and
agent_feats.middle_income_bracket or agent_feats.no_or_least_income)
agent_feats.third_highest_income_bracket) 6: 100 * state * (agent_feats.ages10_to_20
7: (state * agent_feats.no_or_least_income) and (agent_feats.education_level_1 or
or (state * agent_feats. agent_feats.education_level_3 or
second_least_income) agent_feats.education_level5) and
8: state * (5 - (agent_feats. agent_feats.no_or_least_income)
no_or_least_income or agent_feats. 7: 2*state*(agent_feats.no_or_least_income
second_least_income)) and (agent_feats.ages41_to_50 or
9: max(1.0, state*(1.1 if agent_feats. agent_feats.age_51_to_60 or agent_feats.
no_or_least_income or agent_feats. speaks_hindi) and (agent_feats.
second_least_income else agent_feats. education_level_1 or agent_feats.
third_lowest_income_bracket or education_level_2 or agent_feats.
agent_feats.fourth_lowest_income_bracket education_level_3))
or agent_feats.middle_income_bracket or 8: state * (agent_feats.no_or_least_income
agent_feats.third_highest_income_bracket and (agent_feats.education_level_1 or
or agent_feats. agent_feats.education_level_2 or
second_highest_income_bracket or agent_feats.education_level_3))
agent_feats.highest_income_bracket)) 9: state * (agent_feats.no_or_least_income
and ((agent_feats.ages10_to_20 or
agent_feats.ages_21_to_30) and (
F.6 Combinethedistributionsof’age’and agent_feats.education_level_1 or
agent_feats.education_level_2 or
’education’toheavilygiveprecedencetolow
agent_feats.education_level_3)))
incomeimpoverishedyouth.
F.7 Advantagethosewhopreferbeingcalled
Ground truth base: lambda agent_feats, state: between12:30pm-3:30pmandareregistered
state*.1 + if_(state) * 10.0 * if_( atanNGO.
agent_feats.ages_10_to_20 and agent_feats
.education_level_1) Ground truth base: lambda agent_feats, state:
0: state * (agent_feats.no_or_least_income state*.1 + if_(state) * 10.0 * if_(
and ((agent_feats.ages10_to_20 or agent_feats.called1030am_to_1230pm and
agent_feats.ages_21_to_30) and ( agent_feats.ngo)
agent_feats.education_level_1 or 0: state * (agent_feats.ngo and agent_feats.
agent_feats.education_level_2))) called1230_to_330) * (agent_feats.
1: state * (agent_feats.no_or_least_income ages10_to_20 or agent_feats.ages_21_to_30
and agent_feats.education_level5 and or agent_feats.ages_31_to_40)
agent_feats.ages10_to_20) 1: state * agent_feats.ngo * agent_feats.
2: state * (agent_feats.ages41_to_50 and called1230_to_330 * agent_feats.
agent_feats.education_level_1 and age_51_to_60 * agent_feats.speaks_hindi
agent_feats.no_or_least_income and 2: state * (agent_feats.called1230_to_330 and
agent_feats.days_to_first_call > 15) + agent_feats.ngo)
state * (agent_feats.ages41_to_50 and 3: state * (agent_feats.called1230_to_330 + (
agent_feats.education_level_2 and agent_feats.ngo and agent_feats.
agent_feats.second_least_income and called1230_to_330))
agent_feats.days_to_first_call > 15) + 4:(state and (agent_feats.called1230_to_330
state * (agent_feats.ages41_to_50 and or agent_feats.ngo)) + (2*state*
agent_feats.education_level_3 and agent_feats.called1230_to_330*agent_feats
agent_feats.third_lowest_income_bracket .ngo)
and agent_feats.days_to_first_call > 15) 5:state * (agent_feats.called1230_to_330 and
3:(state and (agent_feats.no_or_least_income (agent_feats.ngo or not agent_feats.
or (agent_feats.education_level_6 and called1230_to_330))
agent_feats.ages10_to_20))) 6: state * (agent_feats.ngo and agent_feats.
4:state * ( (agent_feats.education_level_1 called1230_to_330) + state * (agent_feats
and agent_feats.no_or_least_income) or ( .ngo and agent_feats.ages_21_to_30 and
agent_feats.education_level_2 and not agent_feats.ages_31_to_40 and not
agent_feats.second_least_income) or ( agent_feats.ages41_to_50)
agent_feats.education_level_3 and 7: state * (agent_feats.ngo and agent_feats.
agent_feats.third_lowest_income_bracket) called1230_to_330 and (agent_feats.
or (agent_feats.education_level_4 and ages10_to_20 or agent_feats.ages_21_to_30
agent_feats.fourth_lowest_income_bracket) or agent_feats.ages_31_to_40 or
) agent_feats.ages41_to_50))8: state * (agent_feats.ngo and agent_feats. 7:state * agent_feats.
called1230_to_330 and ((agent_feats. fourth_lowest_income_bracket
to_be_called830am_to_1030am or 8: state * (agent_feats.second_least_income
agent_feats.to_be_called1030am_1230pm) or and agent_feats.
(not agent_feats. third_lowest_income_bracket and
to_be_called830am_to_1030am and not agent_feats.fourth_lowest_income_bracket
agent_feats.to_be_called1030am_1230pm))) and agent_feats.middle_income_bracket and
9:state * (agent_feats.ngo and agent_feats. not(agent_feats.no_or_least_income or
called1230_to_330 and agent_feats. agent_feats.third_highest_income_bracket
ages41_to_50 or (agent_feats.ngo and or agent_feats.
agent_feats.called1230_to_330)) second_highest_income_bracket or
agent_feats.highest_income_bracket))
F.8 Prefertheincome bracketboundsforthe 9: state * ((agent_feats.second_least_income
and agent_feats.
middle40%ofthepopulation.
third_lowest_income_bracket and
Ground truth base: lambda agent_feats, state: agent_feats.fourth_lowest_income_bracket
state*.1 + if_(state) * 10.0 * if_( and agent_feats.middle_income_bracket and
agent_feats.fourth_lowest_income_bracket not agent_feats.
or agent_feats.middle_income_bracket or third_highest_income_bracket) or (
agent_feats.third_highest_incom_bracket) agent_feats.second_least_income and
0:state * (agent_feats.second_least_income or agent_feats.third_lowest_income_bracket
agent_feats.third_lowest_income_bracket and agent_feats.
or agent_feats. fourth_lowest_income_bracket and
fourth_lowest_income_bracket or agent_feats.middle_income_bracket and
agent_feats.middle_income_bracket or agent_feats.third_highest_income_bracket)
agent_feats.third_highest_income_bracket) )
1: state * ((agent_feats.second_least_income
or agent_feats.
F.9 FixateonthoseMarathi-speakerswith
third_lowest_income_bracket or
agent_feats.fourth_lowest_income_bracket middle-agedfolks.
or agent_feats.middle_income_bracket) and
not (agent_feats.no_or_least_income or Ground truth base: lambda agent_feats, state:
agent_feats.third_highest_income_bracket state*.1 + if_(state) * 10.0 * if_(
or agent_feats. agent_feats.speaks_marathi and (
second_highest_income_bracket or agent_feats.ages31_to_40 or agent_feats.
agent_feats.highest_income_bracket)) ages41_to_50))
2: state * (agent_feats.second_least_income 0: state * (agent_feats.speaks_marathi and (
or agent_feats. agent_feats.ages_21_to_30 or agent_feats.
third_lowest_income_bracket or ages_31_to_40 or agent_feats.ages41_to_50
agent_feats.fourth_lowest_income_bracket ))
or agent_feats.middle_income_bracket) 1: state * (agent_feats.speaks_marathi and
3: state * ((agent_feats. agent_feats.ages_31_to_40 and agent_feats
fourth_lowest_income_bracket or .called530pm_to_730pm)
agent_feats.middle_income_bracket) and 2: state * (agent_feats.speaks_marathi and
not (agent_feats.second_least_income or agent_feats.ages_31_to_40)
agent_feats.third_lowest_income_bracket 3: 2 * state and (18 and agent_feats.
or agent_feats. speaks_marathi and agent_feats.
third_highest_income_bracket or ages_31_to_40 and agent_feats.
agent_feats.second_highest_income_bracket phone_owner_husband)
or agent_feats.highest_income_bracket)) 4: state * (agent_feats.speaks_marathi and
4: state * (agent_feats.second_least_income agent_feats.ages_31_to_40 and agent_feats
or agent_feats. .education_level_7)
third_lowest_income_bracket or 5: state * (agent_feats.speaks_marathi and
agent_feats.fourth_lowest_income_bracket agent_feats.ages_31_to_40)
or agent_feats.middle_income_bracket) 6: 2*state*(agent_feats.speaks_marathi and ((
5: state * (agent_feats. agent_feats.ages_31_to_40 or agent_feats.
third_lowest_income_bracket and ages41_to_50) or (agent_feats.
agent_feats.fourth_lowest_income_bracket ages_21_to_30 and agent_feats.
and agent_feats.middle_income_bracket) age_51_to_60)))
6: state * (agent_feats. 7: state * agent_feats.speaks_marathi *
third_lowest_income_bracket and agent_feats.ages41_to_50
agent_feats.fourth_lowest_income_bracket 8: 1 + 5*state + 40*(state and agent_feats.
and agent_feats.middle_income_bracket and speaks_marathi and agent_feats.
agent_feats.third_highest_income_bracket ages_31_to_40) + 10*agent_feats.
) ages_21_to_309: state * (agent_feats.speaks_marathi and ( 1: state * ((agent_feats.education_level_1 or
agent_feats.ages_31_to_40 or agent_feats. agent_feats.education_level_2) and
ages41_to_50)) agent_feats.highest_income_bracket)
2: state * (agent_feats.speaks_hindi and
agent_feats.highest_income_bracket and
F.10 Givemoreattentionforbeneficiarieswho
agent_feats.phone_owner_woman or
likelyworkearlyinthemorningandlateat agent_feats.speaks_gujurati and
night. agent_feats.to_be_called830am_to_1030am
and agent_feats.to_be_called730pm_930pm)
Ground truth base: lambda agent_feats, state: 3: state * ((not agent_feats.ages10_to_20)
state*.1+ if_(state) * 10.0 * if_( and (agent_feats.speaks_hindi or
agent_feats.to_be_called830am_to1030am or agent_feats.speaks_marathi))
agent_feats.called_1230pm_to_330pm) 4: state * ((agent_feats.
0: state * (agent_feats. enrollment_gestational_age < 37) and (
to_be_called830am_to_1030am or agent_feats.days_to_first_call > 15) or (
agent_feats.to_be_called730pm_930pm) agent_feats.to_be_called1030am_1230pm and
1: state * (agent_feats. agent_feats.speaks_marathi) or (
to_be_called830am_to_1030am or agent_feats.enrollment_delivery_status
agent_feats.to_be_called730pm_930pm) and not agent_feats.ages_21_to_30))
2: state * (agent_feats. 5: state + 5 * (agent_feats.ages10_to_20 or
to_be_called830am_to_1030am or agent_feats.education_level5 or
agent_feats.to_be_called730pm_930pm) agent_feats.to_be_called1030am_1230pm)
3: 10 * state * (agent_feats. 6: state * ((not agent_feats.
to_be_called830am_to_1030am or phone_owner_woman) or agent_feats.
agent_feats.to_be_called730pm_930pm) days_to_first_call > 7)
4: state * (agent_feats. 7: state * ((agent_feats.education_level_2 or
to_be_called830am_to_1030am and agent_feats.education_level_1) and (
agent_feats.to_be_called730pm_930pm) agent_feats.to_be_called1030am_1230pm or
5: state * (agent_feats. agent_feats.called1230_to_330))
to_be_called830am_to_1030am or 8: state * (1.2*agent_feats.age_51_to_60 + (
agent_feats.to_be_called730pm_930pm) agent_feats.live_births_count == 0) +
6: state * ((agent_feats.called1230_to_330 or agent_feats.ngo + agent_feats.armman)
agent_feats.to_be_called330pm_to_530pm) 9: state*(agent_feats.
and (agent_feats. enrollment_delivery_status and ((
to_be_called830am_to_1030am or agent_feats.ages10_to_20 or agent_feats.
agent_feats.to_be_called730pm_930pm)) ages_21_to_30 or agent_feats.
7: state * (((agent_feats. ages_31_to_40 or agent_feats.ages41_to_50
to_be_called830am_to_1030am and ) or (agent_feats.phone_owner_woman and
agent_feats.to_be_called730pm_930pm) or ( not agent_feats.
agent_feats.to_be_called1030am_1230pm and to_be_called830am_to_1030am and not
agent_feats.called1230_to_330)) or ( agent_feats.to_be_called1030am_1230pm)))
agent_feats.to_be_called330pm_to_530pm
and agent_feats.called530pm_to_730pm))
8: state * ((agent_feats.
to_be_called830am_to_1030am or
agent_feats.to_be_called730pm_930pm) or (
agent_feats.to_be_called1030am_1230pm or
agent_feats.called530pm_to_730pm) or (
agent_feats.called1230_to_330 or
agent_feats.to_be_called330pm_to_530pm))
9: state*(agent_feats.
to_be_called830am_to_1030am or
agent_feats.to_be_called730pm_930pm)
F.11 Infertechnicalchallengesinreachingthe
phonethatcouldindicate’at-risk’
beneficiariesandgivepreference.
Ground truth base: lambda agent_feats, state:
state*.1+ if_(state) * 10.0 * if_((
agent_feats.phone_owner_husband or
agent_feats.phone_owner_family))
0: state * (agent_feats.ages41_to_50 or
agent_feats.ngo)