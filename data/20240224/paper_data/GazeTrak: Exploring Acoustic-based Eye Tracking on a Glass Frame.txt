GazeTrak: Exploring Acoustic-based Eye Tracking on a
Glass Frame
Ke Li Ruidong Zhang Boao Chen
CornellUniversity CornellUniversity CornellUniversity
Ithaca,USA Ithaca,USA Ithaca,USA
kl975@cornell.edu rz379@cornell.edu bc526@cornell.edu
Siyuan Chen Sicheng Yin Saif Mahmud
CornellUniversity UniversityofEdinburgh CornellUniversity
Ithaca,USA Edinburgh,UnitedKingdom Ithaca,USA
sc2489@cornell.edu yinsicheng1999@outlook.com sm2446@cornell.edu
Qikang Liang François Guimbretière Cheng Zhang
CornellUniversity CornellUniversity CornellUniversity
Ithaca,USA Ithaca,USA Ithaca,USA
ql75@cornell.edu fvg3@cornell.edu chengzhang@cornell.edu
ABSTRACT CCSCONCEPTS
Inthispaper,wepresentGazeTrak,thefirstacoustic-based •Human-centeredcomputing→Ubiquitousandmo-
eyetrackingsystemonglasses.Oursystemonlyneedsone biledevices;•Hardware→Powerandenergy.
speakerandfourmicrophonesattachedtoeachsideofthe
glasses. These acoustic sensors capture the formations of KEYWORDS
theeyeballsandthesurroundingareasbyemittingencoded
EyeTracking,AcousticSensing,SmartGlasses,Low-power
inaudiblesoundtowardseyeballsandreceivingthereflected
signals.Thesereflectedsignalsarefurtherprocessedtocal-
ACMReferenceFormat:
culatetheechoprofiles,whicharefedtoacustomizeddeep KeLi,RuidongZhang,BoaoChen,SiyuanChen,SichengYin,Saif
learning pipeline to continuously infer the gaze position. Mahmud,QikangLiang,FrançoisGuimbretière,andChengZhang.
Inauserstudywith20participants,GazeTrakachievesan 2024.GazeTrak:ExploringAcoustic-basedEyeTrackingonaGlass
accuracyof3.6°withinthesameremountingsessionand4.9° Frame.InThe30thAnnualInternationalConferenceonMobileCom-
acrossdifferentsessionswitharefreshingrateof83.3Hzand putingandNetworking(ACMMobiCom’24),September30-October4,
apowersignatureof287.9mW.Furthermore,wereportthe 2024,WashingtonD.C.,DC,USA.ACM,NewYork,NY,USA,16pages.
https://doi.org/10.1145/3636534.3649376
performanceofourgazetrackingsystemfullyimplemented
onanMCUwithalow-powerCNNaccelerator(MAX78002).
Inthisconfiguration,thesystemrunsatupto83.3Hzand
1 INTRODUCTION
hasatotalpowersignatureof95.4mWwitha30HzFPS.
Currently,state-of-the-arteyetrackingtechnologiesutilize
camerastocapturegazepoints.However,cameras-basedeye
trackingsolutionsareknowntohavearelativelyhighpower
Permissiontomakedigitalorhardcopiesofallorpartofthisworkfor signature,whichmaynotworkwellforsmartglasseswitha
personalorclassroomuseisgrantedwithoutfeeprovidedthatcopies relativelysmallbatterycapacity.Forinstance,TobiiProGlass
arenotmadeordistributedforprofitorcommercialadvantageandthat
3 [2], which is considered as one of the best eye tracking
copiesbearthisnoticeandthefullcitationonthefirstpage.Copyrights
glasses,canonlylastfor1.75hourswithanextendedbattery
forcomponentsofthisworkownedbyothersthantheauthor(s)must
behonored.Abstractingwithcreditispermitted.Tocopyotherwise,or capacityof3400mAh.WhenusingthebatteryofaGoogle
republish,topostonserversortoredistributetolists,requirespriorspecific Glass(570mAh),thiseyetrackingsystemcanonlylast18
permissionand/orafee.Requestpermissionsfrompermissions@acm.org. minutes.Thelimitedtrackingtimehashindereditsabilityto
ACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA collectgazepointdataineverydaylife,whichcanbehighly
©2024Copyrightheldbytheowner/author(s).Publicationrightslicensed
informativeformanyapplications,suchas,monitoringusers’
toACM.
mental or physical health conditions [48, 58], gaze-based
ACMISBN979-8-4007-0489-5/24/09...$15.00
https://doi.org/10.1145/3636534.3649376 input,andattentionandinterestanalysis[15].
4202
beF
22
]CH.sc[
1v43641.2042:viXraACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
Figure1:EchoProfilesofDifferentMicrophoneswhenMovingGazetoDifferentRegionsofTheScreen.
Toovercomethischallenge,weintroduceGazeTrak,which anaveragetrackingaccuracyof4.9°forcross-sessionscenar-
exploresutilizingacousticsensing(knownforrelativelylow iosand3.6°forin-sessionscenarioswitharefreshingrateof
power, lightweight, and affordable) to continuously track 83.3Hz.Wemadeademovideo1todemonstratethetracking
gazepointsonaglassframe.Itssensingprincipleisbased performanceandreal-worldapplicationsofoursystem.
onthefactthateyeballsarenotperfectlysphericalandro- Althoughthecurrentaccuracyofoursystemwasworse
tatingthemwouldexposedifferentshapesandstretchthe thancommercialeyetrackerssuchasTobiiProGlasses3[2]
skinaroundthemwithuniqueformations.Thiscanprovide andPupilLabsGlasses[28],itisstillcomparabletosome
highlyvaluableinformationforinferringgazepoints.Gaze- webcam-basedeye-trackingsystems[21,53].Furthermore,
Trakusesonespeakerandfourmicrophonesoneachside duetothelow-powerfeatureofacousticsensors,GazeTrak,
oftheglassframe.Thespeakeremitsfrequency-modulated including the data collection system, has a relatively low
continuous-wave(FMCW)acousticsignalswiththefrequency powersignatureof287.9mW.Comparedtocamera-based
above18kHztowardstheeyeballs.Themicrophonescapture wearableeyetrackingsystems,ourproposedsystemreduces
thesignalsreflectedbytheeyeballsandtheirsurroundingar- thepowerconsumptionbyover95%.Ifusingabatterywith
eas,whichareusedtoprocessandcalculatetheechoprofiles. thecapacitysimilartoTobiiProGlasses3,oursystemcanex-
Theseechoprofilesarefedtoacustomizeddeeplearning tendtheusagetimefrom1.75hoursto38.5hours.Itcaneven
algorithmbasedonResNet-18topredictthegazepoint. last6.4hoursonthebatteryofnormalsmartglasses,suchas
Weconductedtworoundsofuserstudiestoevaluatethe GoogleGlass.Thepowersignatureofoursystemcanbefur-
performanceofGazeTrak.Duringthestudies,eachpartici- therimprovedusingarecentlyintroducedmicro-controller
pantwasaskedtolookatandfollowtheinstructionpoints withalow-powerCNNaccelerator(MAX78002).Hence,we
on the screen. In the first round of the study, 12 partici- implementedourgazetrackingpipelinefullyonMAX78002.
pantsevaluatedourfirsthardwareprototype(showninFig.3 Withtherefreshratesetas30Hz,thepowerconsumption
(f)),wherethemicrophonesandspeakersweregluedona ofthewholesystemincludingthedatapreprocessingand
glassframe.Theaveragecross-sessiontrackingaccuracywas modelinferenceismeasuredas95.4mW.
4.9°.Itconfirmedtheoptimalsettingsofthesensingsystem, Insummary,thecontributionsofourpaperareasfollows:
whichhelpedusdesignthefinalprototype.Thefinalproto-
• Wedesignedandimplementedthefirstacoustic-based
type(asshowninFig.3(g))featuresamorecompactform
continuouseyetrackingsystemonglasses.
factor,significantlylowersignalstrength,andimprovedenvi-
• Auserstudywith20participantsshowedanaverage
ronmentalsustainability.Toensureconsistentperformance
cross-sessionaccuracyof4.9°witharefreshingrate
betweenthetwoprototypes,weconductedasecondroundof
upto83.3Hzandapowersignatureof287.9mW.
studywith10participants,includingsomenewparticipants,
evaluatingthefinalprototype.Thefinalprototypeachieved
1https://youtu.be/XvNLNkfQY7QGazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA
• Theperformanceofthesystemremainedrobustunder Theaforementionedfrontalcamera-basedtechnologies
differentnoisyenvironmentsandwithdifferentstyles aremostlylocatedatfixedpositionsanddonotworkwell
ofglassframes. whileusersmovetoanotherpositionorarewalkingaround.
• Areal-timepipelinewasimplementedonMAX78002 In order to allow some mobility for users while they are
to make inferences on the board with a power con- usingtheeyetrackingtechnologies,manyresearchersin-
sumptionof95.4mWat30Hz. vestigatedutilizingthecamerasonmobiledevicestotrack
eyemovements,suchasmobilephones[25,30,67]ortablets
2 RELATEDWORK [7,25].However,theseeyetrackingtechnologiesbasedon
2.1 Webcam-basedEyeTrackingSystems mobiledevicesstillrequireuserstoholdthemobiledevicesin
frontoftheirfaceallthetimeandcannotprovidecompletely
Webcamshavebeenwidelyusedtoimplementeyetracking
hands-freeandmotion-freeexperiencesforusers.
technologiesbecauseofitsubiquityoncomputersandits
advantageoflow-cost.Researchershavedonemanyworkto
explorethepotentialofwebcamsforeyetracking[46,47,52, 2.3 WearableEyeTrackingTechnologies
62].Currently,thereareplentyofwebcam-basedeyetrack-
Toovercomethechallengesthatnon-wearableeyetracking
ingplatformsthatareavailableonline,suchasRealEye.io
technologiesfaceasdescribedinthelasttwosubsections,
[53],GazeRecorder[19],andWebGazer.js[21].
manywearableeyetrackingtechnologiesbasedoncameras
Thesewebcam-basedeyetrackingplatformsprovideaf-
[12,24,29,39,40,42,51,66],opticalsensors[8,34,35,56,57],
fordablesolutionsforeyetrackingwithacceptabletracking
acousticsensors[20],magneticsensors[61],Electrooculog-
performanceforeverydayusers.However,thepositionof
raphy(EOG)sensors[9,10],orinertialmeasurementunits
webcams are usually fixed and they have a relatively low
(IMU)[29]havebeendeployedondifferentkindsofwear-
resolution.Therefore,theirperformancecanbemoreeas-
ablesincludingglasses[8,12,20,35,39,40,51,56,57,66],
ilyimpactedbyfactorslikelightingconditions,occlusions,
goggles[9,10],hat[29],andhead-mounteddevices[24,34,
cameraorientations,etc.
42,61].Amongallthesewearableeyetrackingtechnologies,
camera-basedonesusuallyoutperformothersintermsof
2.2 OtherNon-wearableEyeTracking
trackingperformanceanddonotrequirelotsofcalibration
Technologies
datafromnewusers.Manywearableeyetrackersusingcam-
Inordertoprovidemoreaccurateandreliablesolutionsto eras,especiallyonglasses,havebecomecommercialandcan
eyetrackingandmakethemmoreapplicabletoassortedsce- beusedasareliablewaytotrackeyemovementscontinu-
narios,researchershaveputlotsofeffortsinimplementing ously,suchasTobiiProGlasses3[2],PupilLabs(Invisible,
other non-wearable eye tracking technologies other than Core,VR/ARadd-ons)[28],DikablisGlasses3[16],andSMI
webcam-based systems, most of which are based on cam- EyeTrackingGlasses[23].Withthesetechnologies,various
eraswithahigherresolutionthanwebcams.Frontalcamera- novel gaze-based applications have been enabled, includ-
basedeyetrackingtechnologiesbasedoncomputervision ingdetectionofeyecontacts[63],interactionwithdevices
techniques can take full advantage of the whole facial in- [26,38,45],monitoringmentalhealth[31,58],etc.
formationoftheuserforeyetracking,whichusuallyleads Despiteofthepromisingtrackingperformance,current
tohightrackingaccuracy.Differentkindsofcamerashave solutionstowearableeyetrackingsystemsstillhavesome
beenusedintrackingeyemovements,suchasRGBcameras limitations. First of all, many eye tracking systems above
[3], infrared (IR) cameras [44], and thermal cameras [59]. canonlyrecognizediscretegestures[9,10,56,57,66],limit-
Beyondusingjustonecamera,manytechnologiesadopted ingtheirperformanceinapplicationsthatneedcontinuous
multiplecamerasintheireyetrackingsystemsinorderto trackingoftheeyes.Camera-basedwearableeyetrackerscan
improvetheperformanceindifferentperspectivesincluding providehighaccuracyincontinuouseyetracking,butcam-
providinglargertrackingcoverage[4],allowingforusermo- erasareusuallypower-hungry,whichmakesthemrelatively
tion[22]andtrackingeyemovementsofmultipleusers[36]. impracticalwhiledeployedinwearablesthatneedtobeworn
Becauseofthereliabletrackingperformanceandreasonable ineverydaysettings.Toaddressthisissue,[39]and[40]pro-
calibrationtimeneeded,frontalcamera-basedeyetracking posedlow-powersolutionstotrackinggazepositionswith
technologieshavebeenwellcommercialized,amongwhich camerasonglasseswhilemaintainingpromisingaccuracies.
TobiiProFusion[1]isoneofthebestdesktopeyetrackers Despiteoftheimpressiveperformance,changinglighting
becauseitonlyrequiressecondsofcalibrationprocessfor conditionscanstillbeaproblemforcamera-basedsystems,
newusersandcanprovideatrackingaccuracyaslowas0.3° as[40]demonstratedthattheperformancebecameworse
inoptimalconditions.Asaresult,thisproducthasbeenused inanoutdoorsetting.Besides,commercialeyetrackersare
asreferenceinmanyresearchprojects. usuallyexpensiveanddonotprovideopen-sourcesoftwareACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
Table1:GazeTrakandOtherContinuousEyeTrackingTechniques.ThepowerofGazeTrak(Teensy4.1)doesnot
includedatapreprocessinganddeeplearninginference.Thereportedaccuracyistestedwithinthesamesession
withoutusersremountingthedevice.Bothweightandcostincludetherecordingunit.NS=NotSpecified.
Reference FormFactor Sensors Power Accuracy RefreshRate Weight Cost
Choetal.[12] Glasses Cameras >7W 0.79° NS NS NS
Ryanetal.[51] Glasses Cameras >1.6W 2° NS NS ~$700
iShadow[39] Glasses Cameras 0.07W 3° 30Hz NS NS
CIDER[40] Glasses Cameras 0.032W 0.6° 250Hz NS NS
PupilLabsGlasses[28] Glasses Cameras 8.6W 0.6° 30/60/120Hz 202.75g $2,849
TobiiProGlasses3[2] Glasses Cameras 10.7W 0.6° 50/100Hz 388.5g $16,055
SMIGlasses[23] Glasses Cameras NS 0.5° 60/120Hz NS $41,000
Lietal.[35] Glasses NIRLED&Photodiodes 395µW <2° 120Hz <25g NS
Lietal.[34] Head-mounted Photodiodes 791µW 6.3° 10Hz NS NS
GazeRecorder[19] Webcam Camera / 1.05° 30Hz / $500/month
WebGazer.js[21] Webcam Camera / 4.17° NS / Free
RealEye.io[53] Webcam Camera / ~5° 60Hz / $600/month
GazeTrak(Teensy4.1) Glasses AcousticSensors 0.288W 3.6° 83.3Hz 44.2g ~$75
GazeTrak(MAX78002) Glasses AcousticSensors 0.095W 4.2° 30Hz / /
forusers,preventingthemfrombeingeasilyaccessedand 3 PRINCIPLEANDALGORITHMS
adaptedbygeneralusers. Activeacousticsensingisbasedonaffordablesensors(speak-
Recently,Lietal.[35]proposedalow-costandbattery- ersandmicrophones),thesizesofwhicharerelativelysmall.
freesolutiontocontinuouseyetrackingusingnearinfrared Previousresearchworkhasprovedthatitisabletoprovide
emitters and receivers on glasses. It achieves competitive enoughinformationtotracksubtleskindeformationssuch
performancebuttheystatedinthepaperthatthissystem asfacialexpressions[18,33].Inthissection,wediscusshow
canbeimpactedbydirectsunlightandglassesmovement,i.e. thisapproachcanbeadaptedtoeyetracking.
theremountingoftheglasses.Besides,thisworktracksthe
positionandsizeofthepupilsowecannotdirectlycompare
3.1 FMCW-basedActiveAcousticSensing
ittooursystem.Afterconversion,itstrackingaccuracyof
Inordertocapturetheformationaroundeyeballs,weuse
gazepositionsissmallerthan2°inangularerror.Another
FMCW-basedacousticsensing,whichhasbeenwidelyproven
systemusingsimilartechnologyfromthesamegroup[34]
effectivetoestimatedistanceandmovementsfromcomplex
tracksgazepositionswithanaccuracyof6.3°,worsethan
environments[41,60].
thetrackingaccuracyofoursystemat4.9°.Golardetal.[20]
conductedamodelingandempiricalstudytoprovethatul-
3.1.1 EncodedFMCWSignals. WhilecustomizingtheFMCW
trasoundcanprovidealow-power,fastandlight-insensitive
signalsforoursystem,threemainfeaturesaretakenintoac-
alternativeforcamera-basedeyetrackingtechnologies.How-
count:1)Operatingfrequencyrange:Thedeviceisexpectedto
ever,itwasevaluatedonaphysical3Dmodelofahuman
bewornbyusersforalongperiodoftimeintheireveryday
eyeandusedtime-of-flightestimatedfromacousticsignals
lives.Asaresult,theFMCWsignalsneedtobetransmittedin
andnotclearhowitcanapplyonarealuser.
theinaudiblefrequencyrange.Besides,toensuretheencoded
Tothebestofourknowledge,GazeTrakisthefirstwear-
signalsareminimallyimpactedbythenoiseintheenviron-
able sensing technology based on active acoustic sensing
ment,theoperatingfrequencyrangewepickshouldalsobe
thatcantrackgazepointscontinuously.Wesummarizedand
uncommonindailysettings;2)Samplingrate:Toachievea
compared GazeTrak with some aforementioned wearable
reasonablespatialandtemporalresolutionoftrackingeye
andwebcam-basedeyetrackingtechniquesthatcancontin-
movements, the sampling rate of FMCW signals must be
uouslytrackgazepositionsinTab.1.Thesetechniquesare
highenough;3)Gain:Aspowersignatureincreaseswiththe
thosethataremostrelatedtooursystem.Pleasenotethat
signalgain,thesignalgainshouldbeproperlydetermined
commercialeyetrackingwearables[2,23,28]usuallyhave
tobalancesignalstrengthandpowerconsumption.
camera(s)recordingthevideooftheenvironmentaswellso
Considering all the factors above, we set the operating
wecanonlyroughlycomparethemtoourdeviceinterms
frequencyrangeoftheFMCWsignalsthatweemitinthe
ofpowerandweight.
GazeTraksystemabove18kHz,becausethisrangeisnear-
inaudibleanduncommoninthesoundsgeneratedbynormal
human activities. Because both eyes contain informationGazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA
Figure2:OverviewoftheGazeTrakSystem:UsetheSpeakerontheRightSide(18-21kHz)forIllustration.
while moving, we placed one speaker on each side of the 3.2 MachineLearningAlgorithms
glassframe.Wesetthespeakerontherightsidetooperateat
3.2.1 GroundTruthAcquisitionwithoutusingEyeTrackers.
18-21kHzwhiletheoneontheleftsideoperatesat21.5-24.5 Aprofessionaleye-tracker(e.g.,TobiiProFusion)canpro-
kHztomakesuretheydonotinterferewitheachother.To videhighlyaccurategroundtruth,butitisexpensive.Ifour
guaranteethatthesystemworksreliablyinthesefrequency systemneedsaprofessionaleye-trackertotrainthesystem,
ranges,wesettheADCsamplingrateas50kHzwiththe itwillmakeoureye-trackingsystemlessaccessible.
framelengthofFMCWsignalsas600samples.Thisgives Therefore,wedevelopedanewgroundtruthacquisition
thesystemarefreshrateofeyetrackingat83.3Hz(50000 andcalibrationsystemthatonlyneedsaprogramrunning
samples/s÷600samples).Webelievearefreshrateof83.3 onalaptop.Theprogramgeneratesinstructionpointson
Hzissufficienttoprovidecontinuousgazetrackingsince thescreenasthegroundtruth.Duringdatacollection,the
the frame rate of most videos are 30 Hz or 60 Hz. Lastly, usersonlyneedtolookatandfollowthemovementsofthe
thegainwasexperimentallyadjustedtomakesurethatthe instructionpoints.Thesegroundtruthdataalongwiththe
signaldoesnotsaturatethemicrophoneswhilethepower echoprofilesarefedintothemachinelearningmodelfor
consumptionisrelativelylow. training.Thismethodisgenerallyapplicableonanydevice
withascreen.Fordetailsabouthowtheinstructionpoints
aregenerated,pleaserefertoSec.5.Tobettercompareour
systemwithcommercialeyetrackers,wealsouseaTobii
3.1.2 AcousticPatternsforContinuousEyeTracking. After Pro Fusion (120 Hz) [1] to record the eye movements to
receivingthereflectedFMCWsignals,wefirstapplyaBut-
demonstratetheeffectivenessofourtrainingmethods.
terworthband-passfilterwithacut-offfrequencyrangeof
18-21kHzor21.5-24.5kHzonthesignaltoremovethesig- 3.2.2 Deep Learning Model. We developed a customized
nalsinthefrequencyrangethatwearenotinterestedin.It deep-learningpipelinetolearntheechoprofilescalculated
alsohelpsprotecttheprivacyofusersbecauseweremove on the received signals. Because in the echo profiles (See
theaudiblerangeofthesignals.Thenwefurtherprocessthe Fig.1),thetemporalinformationhasbeenconvertedtothe
filteredsignaltoobtainuniqueacousticpatterns.According spatialinformationonanimage,wedecidedtouseResNet-
topriorresearchwork[32,33,37,54,60,64,65],EchoProfile 18astheencoderofourdeeplearningmodelbecauseCNN
providesanaccuratedepictionofthestatusandmovements networksareknowntobegoodatextractingfeaturesfrom
ofthereflectingobjectsintheenvironment.Asaresult,in images.Thenafully-connectednetworkisusedasadecoder
thispaper,wealsouseechoprofilesastheacousticpatterns topredictgazepositionsbasedonthefeaturesextractedfrom
thatoursystemmonitors.AsshowninFig.2(a)-(c),echo theimages.
profile is obtained by continuously calculating the cross- Becauseofthelimiteddistancebetweenthesensorson
correlation between the received signals and transmitted theglassesandtheeyes,weareonlyinterestedinacertain
signals.Fig.1demonstratesthatdifferenteyefixationsand rangeoftheechoprofiles(Fig.2(c)).Asaresult,wecropthe
movements are correlated with different patterns in echo echoprofilesofeachchanneltogetthecenter70pixels(23.8
profiles.Basedontheseobservationsabove,webelievethat cm)vertically.Thenwerandomlycrop60pixels(20.4cm)
ourGazeTraksystemutilizingFMCW-basedactiveacoustic outofthese70pixelsfordataaugmentationpurposetomake
sensingisabletotrackeyemovementscontinuouslywith surethesystemwillnotbeseverelyimpactedbythevertical
highaccuracy. shiftingcausedbyremountingthedevice.TocontinuouslyACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
Figure 3: Hardware and Form Factor for GazeTrak: (a) Speaker board; (b) Microphone board (front view); (c)
Microphoneboard(backview);(d)CustomizedPCBboardfortheaudiochipNXPSGTL5000;(e)Teensy4.1;(f)
Glassesformfactorwithspeakersandmicrophonesattached(M1-8:microphones,S1-2:speakers);(g)Attachable
andmorecompactprototype;(h)MAX78002EvaluationKit.
trackthegazepositions,weapplyaslidingwindowof0.3 4 DESIGNANDIMPLEMENTATION
secondsontheechoprofiles.Asaresult,thedimensionof 4.1 HardwareDesign
theechoprofilethatweinputintothedeeplearningmodel
InordertoimplementtheFMCW-basedactiveacousticsens-
foronechannelis26(0.3s×50000Hz÷600samples+1)×
ing technique mentioned in the section above, we chose
60(pixels).Becauseweuse2speakersand8microphonesin
Teensy4.1[50]asthemicro-controllertoprovidereliable
oursystem,whichwillbeillustratedinSubsec.4.2,wecrop
FMCWsignalgenerationandreceiving.WedesignedaPCB
outthesamedimensionofechoprofilesforall2×8=16
boardtosupporttwoSGTL5000chipswhicharethesameas
channels,makingthedimensionoftheinputvectortothe
theoneontheTeensyaudioshield[49].Withthiscustomized
deeplearningmodelas26×60×16.
PCBboardpluggedontoTeensy,itcansupportasmanyas
Weusetheinstructionpointsasthelabels(seeSubsubsec.
8microphonesand2speakers.Wechosethespeakercalled
3.2.1)andthemeansquarederror(MSE)asthelossfunction.
OWR-05049T-38D[14]andtheMEMSmicrophonecalled
WechoseAdamoptimizerandsetthelearningrateas0.01.
ICS-43434[55]tosupportsignaltransmissionandreception.
Themodelistrainedfor30epochstogettheestimationof
WealsobuiltcustomizedPCBboardsforthespeakerandthe
thetwogazecoordinates(x,y).
microphonetomakethemassmallaspossible.Weusedthe
Inter-ICSound(I2S)busesontheTeensy4.1totransmitdata
3.2.3 EvaluationMetrics. Thepredictionofoursystemis
betweentheTeensy4.1andtheSGTL5000chips,speakers
thecoordinate(x,y)ofourestimatedgazepositiononthe
andmicrophones.ThecollecteddataisstoredintheSDcard
screeninpixels.ToevaluatetheaccuracyofGazeTrak,we
onTeensy4.1.Fig.3(a)-(e)showthesecomponents.
adoptedtheaccuracydefinedinCOGAINeyetrackeraccu-
racytermsanddefinitions[11].Theevaluationmetricwe
useinoursystemisthemeangazeangularerror(MGAE)
4.2 FormFactorDesign
betweenthecoordinateofourprediction(x,y)andthatof
thegroundtruth(x’,y’).TocalculateMGAEindegreesfrom Wedesignedthefirstformfactorusingacommodityglass
thecoordinates,wefirstneedtogettheangularerror𝜃 be- frame.Weglued1speakerand4microphonestoeachinner
tweenthepredictionandthegroundtruthofeachdatapoint. sideofapairoflight-weightglasses.Thespeakersandmi-
𝜃 canbecalculatedusingthelawofcosinesinatriangleas crophonesaresymmetricallyplacedontheglasses,asshown
follows: inFig.3(f).
Basedontheexperiencewelearnedduringtheiteration
process,therearethreekeyfactorswetookintoconsider-
𝑑2 +𝑑2 −𝑑2 ationwhiledesigningthefinalformfactorofGazeTrak:1)
𝑒𝑔 𝑒𝑝 𝑔𝑝
𝜃 =arccos( )×180÷𝜋 (1) Type of glass frame: We started designing the form factor
2×𝑑 𝑒𝑔×𝑑
𝑒𝑝 withalargeglassframebecausewebelieveithasmoreroom
forustoplacesensors.However,thelargertheglassframeis,
where𝑑 ,𝑑 and𝑑 arethedistancebetweenuser’seyes theeasieritwillbefortheframetotouchtheskin,blocking
𝑒𝑔 𝑒𝑝 𝑔𝑝
andgroundtruth,thedistancebetweenuser’seyesandpre- thesignaltransmissionandreception.Asaresult,wefinally
diction,andthedistancebetweengroundtruthandpredic- pickedarelativelysmallglassframewithanosepadthat
tionrespectively.MGAEisobtainedbyaveraging𝜃 overall can support the glass frame to a higher position. Besides,
thedatapointsinthetestingdataset. thelight-weightglassesminimizethepressureattachedonGazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA
theuser’snose,makingitmorecomfortabletowear;2)Sen- toreachthisgoal,wecarefullydesignedtheinstructionvideo
sor position: The speakers and microphones on two sides forparticipants’gazetofollow.Basically,onthewhitescreen,
are symmetric because we believe the movements of two therewouldbeonereddotmovingaroundandweaskedpar-
eyesareusuallysynchronized.Oneachside,weplacethe ticipantstostareatthepointandfollowitwiththeireyes.We
speakerontheframeoftheglassesnexttotheoutercanthi dividedthescreeninto100regions.Foreachdatapoint,the
becauseitiseasierforthespeakerstotouchtheskinifthey instructionpointappearedatarandompositionwithinone
areplacedabovethecheekbonesornexttotheeyebrows, randomregion.Theinstructionpointwouldmovequickly
consideringtheirheight.Themicrophonesarescatteredon tothatrandompositionandstaystaticatthatpositionfora
theframeasfarawayfromeachotheraspossibletocapture certainperiodoftimebecausewemainlywouldliketotest
moreinformationbyreceivingsignalstravellingindifferent howGazeTrakperformstotrackthefixationofparticipants.
paths.Thesensorsareattachedasfarawayfromthecenter Wesuccessfullyrecruited20participants(10femalesand
ofthelensesaspossibleinordertoavoidblockingtheview 10males,22yearsoldonaverage).Notethatsomepartic-
oftheuser;3)Stability:Wefoundthatthestabilityofthe ipants participated in the study for multiple times to test
deviceseverelyimpactstheperformanceofoursystemes- different settings. The study was conducted in an experi-
peciallywhenusersneedtoremountthedevicefrequently. mentroomonauniversitycampus.Duringthestudy,the
Theanti-slipperynosepadpreventstheglassesfromsliding participantssitonachairandputontheglassesformfactor
downtheuser’snose.Furthermore,weaddedtwoearloops with our GazeTrak system. For each participant, we pro-
attheendofthelegsoftheglassframe.Theygreatlyhelps duced12sessionsofinstructionpoints.Duringtheinterval
fixtheglassespositionfrombehindearsandimprovesthe between sessions, participants were instructed to remove
performanceofthesystem.Finally,wemadetheformfactor thedevice,placeitonthetable,andthenputitbackon.This
asshowninFig.3(f). stepwastakentodemonstratethatoursystemcontinuedto
functioncorrectlyevenafterthedevicewasremounted.In
4.3 FinalHardwarePrototype eachsession,theinstructionpointmovedtoallthe100pre-
definedregionsinarandomorder.Thedurationforwhich
Theprototypeaboveissuitableforinitialtestingandcompar-
theinstructionpointremainedateachpositionvariedfrom
isonofdifferentconfigurations.However,oncethedesignof
0.5to3.5seconds,withanaverageof2seconds.Asaresult,
theprototypeisfinalized,weaimtocreateamorecompact
theaveragelengthofeachinstructionsessionwas200sec-
andlessobtrusiveformfactorthatissuitableforeveryday
onds.Beforeeachsession,therewasa15-secondcalibration
use by users. To achieve this, we have designed two PCB
processwiththeinstructionpointmovingtothefourcorners
boards,eachcontainingonespeakerandfourmicrophones
ofthescreenandthecenterofthescreen.
onboard,whichcanbeattachedtoonesideoftheglasses.
Thefullstudytooknomorethan1.5hoursforeachpartic-
WehavealsodeployedtheTeensy4.1andthePCBboard
ipant,duringwhichwecollectedapproximately40minutes
with SGTL5000 chips directly onto one leg of the glasses.
of data (200 seconds × 12 sessions). Upon completing the
To connect the micro-controller and the customized PCB
studytasks,theparticipantwasaskedtocompleteaques-
boards,wehaveusedflexibleprintedcircuit(FPC)cables.
tionnairetocollecttheirdemographicinformationandtheir
Thesystemhasaninterfacethatallowsittobepoweredby
feedbackusingthissystem.
aLi-Pobattery.ThecompactprototypeisshowninFig.3(g),
andFig.1showsauserwearingtheprototype.Webelieve
6 EVALUATIONRESULTS
that this prototype can be easily adapted and attached to
differenttypesofglasses. Inthissection,wefirstevaluatedtheperformanceofGaze-
We have measured the weight of the prototype, and it Trakwiththeinitialprototype,comparingdifferentground
carriesatotalweightof44.2grams,includingtheglasses, truthacquisitionmethods,sensorconfigurationsandamounts
Teensy4.1,PCBboards,andtheLi-Pobattery.Comparedto oftrainingdata.Thenwetestedoursystemundernoisyenvi-
camera-basedeyetrackingglasses,ourGazeTrakdeviceis ronmentsandonglassesofvariousframestyles.Finally,we
muchlighter.Forexample,TobiiProGlasses3[2]weigh76.5 optimizedthesystemonthefinalprototypeandevaluatedit
gramsfortheglassesand312gramsfortherecordingunit. withanotherstudy,withpowerconsumptionmeasured.
Ourdevicehasasignificantadvantageovercamera-based
eyetrackingglassesintermsofweight. 6.1 User-dependentModel
WefirsttestedoursystemusingthefirstprototypeinFig.3(f)
5 USERSTUDYPROCEDURE
with12participants.Auser-dependentmodelwasapplied
Theobjectiveofouruserstudyistovalidatetheperformance to train a separate model for each participant. Among 12
ofGazeTrakoncontinuouslytrackinggazepoints.Inorder sessions we collected for each participant, we conductedACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
Table2:StudyResultsforDifferentMicConfigurations.
MicConfiguration M1+M5 M2+M6 M3+M7 M4+M8 Best4Mics(M2,4,6,8) Best6Mics(M1,2,4,5,6,8) All8Mics
MGAE 7.7° 7.2° 8.5° 6.9° 5.9° 5.5° 4.9°
a 6-fold cross validation to test the tracking performance thatTobiiProFusioncantrackthegazepointswithanav-
of our system by using 10 sessions (33.3 minutes) of data erageaccuracyof1.9°duringthecalibrationprocessforall
fortrainingand2sessions(6.7minutes)ofdatafortesting. participants.Weplottedthetrackingperformanceofboth
UsingtheevaluationmetricsdefinedinSubsubsec.3.2.3,we GazeTrakandTobiiforallparticipantsinFig.4.
calculatedthemeangazeangularerror(MGAE)indegree
forallparticipants,andtheresultweobtainedwas5.9°.To 6.2 ImpactofSensorConfigurations
furtherimprovetheperformance,weadoptedthe15-second
Inthissubsection,weevaluatedtheimpactofthenumber
calibrationdatabeforeeachsessiontofine-tunethemodel,
andplacementofmicrophonesontrackingperformanceto
which resulted in an improved performance of 4.9°. It is
determinetheoptimalsensorpositionforthebestresults.
worthmentioningthatasimilarcalibrationprocessisalso
Weassessedfourdifferentsettings:1)onemicrophoneon
requiredforcommodityeyetrackers(e.g.,TobiiPro).The
eachside(leftandright);2)twomicrophonesoneachside;
distancebetweentheparticipants’eyesandthescreencenter
3) three microphones on each side and 4) all four micro-
ismeasuredtobearound60cmsowehaveafieldofview
phonesoneachside.Inthefirstsetting,wecomparedthe
of60°(thelargestpossibleangularerror)inthisstudy.We
performanceusingdatafromfoursetsofmicrophoneset-
madeademovideoshowinghowourpredictionlookslike
tings(M1+M5,M2+M6,M3+M7,andM4+M8inFig.3(f)),
visuallywiththislevelofaccuracy.
whichispresentedinTab.2.Thefindingsdemonstratethat
theM4+M8pairofmicrophonesprovidesthebesttracking
performanceamongthefourpairstested.Weconducteda
one-wayrepeatedmeasuresANOVAtestontheresultsof
thefoursettingsandidentifiedastatisticallysignificantdif-
ference (𝐹(3,44) = 6.74,𝑝 = 0.001 < 0.05). These results
indicatethatmicrophoneplacementcanaffectgazetracking
performance,possiblyduetodifferencesinsignalreflection
beforearrivingatdifferentmicrophones.
We further conducted experiments to evaluate perfor-
manceusingdifferentcombinationsofmicrophonesunder
settings 2 and 3. The results showed that the best perfor-
mance was 5.9 degrees and 5.5 degrees, respectively. We
Figure4:MGAEDistributionacrossParticipants. alsoranaone-wayrepeatedmeasuresANOVAtestamong
theresultsofthesefoursettingsusingdatafrom12partici-
pants.Theresultsshowedastatisticallysignificantdifference
Next, we aimed to compare the impact on gaze track-
(𝐹(3,44) = 51.61,𝑝 = 0.00001 < 0.05).Thesefindingssug-
ingperformanceofusingdifferentgroundtruthacquisition
gestthatoursystemrequiresfourmicrophonesoneachside
methods:acommodityeyetracker(TobiiProFusion)versus
(eightmicrophonesintotal)toachievethebestperformance.
ourmethod(usinginstructionpointsonthescreen).Weused
theeyetrackingdatarecordedbyTobiiProFusionasthe
6.3 ImpactofBlinking
groundtruthtotrainthemodel,andtheMGAEafterfine-
tuningwas4.9°.Weconductedarepeatedmeasurest-test Blinkingcanintroducenoiseinourhighly-sensitiveacoustic
betweentheresultsusingTobiidataasthegroundtruthand sensingsystemasitcanleadtorelativelylargemovements
thoseusinginstructionpointsasthegroundtruthforall12 aroundtheeye.Weconductedanevaluationtodetermine
participants,anddidnotfindastatisticallysignificantdiffer- whetherblinkingaffectsthetrackingperformanceofoursys-
ence(𝑝 =0.92>0.05).Thissuggeststhatusinginstruction tem.Forthisevaluation,weselecteddatafromthreepartici-
points on a screen monitor as the ground truth can be as pantswiththebest,worst,andaveragetrackingperformance
effectiveasusingTobiidata. (P1, P2, P10). We removed the data where the participant
Apartfromthat,wealsorecordedtheeyetrackingaccu- blinks(about10%oftotaldata)basedonthegroundtruth
racyofTobiiProFusionitselfwhichwasreportedafterthe dataobtainedfromTobiiEyeTracker.Wethenusedthepro-
calibrationprocessoftheTobiiplatform.Theresultsshowed cessed data to retrain the user-dependent model for eachGazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA
participant.Ourresultsshowedthattheperformancedidnot 4)drivingnoise(65.6dB(A))recordedwhiledrivingavehi-
improveafterremovingtheblinkingdata.Onepossiblerea- cle.Afteroverlayingeachofthesefournoises,thetracking
sonforthisresultisthattheblinkingpatternsareconsistent performanceremainedunchangedforeveryparticipant.
andcanbelearnedbythemachinelearningmodel.There-
fore,ourfindingssuggestthatblinkingdoesnotsignificantly 6.5.2 Real-worldNoisyEnvironments. Inthesecondexperi-
ment,weinvitedeightparticipantsfromtheprevioususer
impacttheperformanceofoursystem.
studyandrecruitedtwonewparticipants(P13andP14)to
testourdeviceindifferentreal-worldnoisyenvironments.
6.4 User-adaptiveModel Sincethisstudyrequiredustomovetodifferentenviron-
To reduce the need of providing lots of training data for ments,thestudydesigndifferedslightlyfromtheprevious
a new user, we employed a three-step process to train a studydescribedinSec.5.
user-adaptivemodel.Firstly,wetrainedalargebasemodel Inthisstudy,weusedanAppleMacBookProwitha13.3-
usingdatafromallparticipantsexcepttheonebeingtested. inchdisplaytoplaytheinstructionvideos.Weusedthein-
Secondly,wefine-tunedthemodelusingthetrainingdata structionpointsasthegroundtruth.TheMacBookProwas
collectedfromthecurrentparticipant.Notably,theuseronly placedonamovabletable,andparticipantswereinstructed
needstoprovidetrainingdataonceduringtheinitialsystem tositinfrontofthetabletoconductthestudy.Additionally,
use.Finally,atthebeginningofeachsession,wefurtherfine- accordingtoSubsec.6.4,6sessionsoftrainingdataaresuffi-
tunedthemodelusingcalibrationdatacollectedfromthe cienttoprovideacceptabletrackingperformance.Therefore,
participantbeforetestingorusingthesystem.Todetermine foreachparticipant,wecollectedatotalof8sessionsofdata
theamountofdatarequiredtoachievecompetitivetracking inaquietexperimentroom,with6sessionsfortrainingand
performance,wereservedtwosessionsofdatafortestingand 2sessionsfortesting.Wethencollectedadditionaltesting
usedvaryingamountsoftrainingdatafromtheparticipant data under two different noisy environments. In the first
tofine-tunethelargemodel. environment,participantsusedoursystemwhileweplayed
Theresultsshowthatanewuseronlyneedstoprovide randommusicfor2sessions.Inthesecondenvironment,we
sixsessionsoftrainingdata(approximately20minutes)to collected2sessionsoftestingdataatacampuscafewhere
achievegoodperformance.Collectingmoredatadoesnot staffandpeopleweretalkingaroundduringbusinesshours.
necessarilyresultinbetterperformance.Additionally,with The noise levels under each environment were measured
onlytwoorthreesessionsofdata(approximately6minutes), using the CDC NIOSH app: 1) quiet room (33.8 dB(A)); 2)
thesystemcanachieveaperformanceof6.7°and6.1°,respec- playmusic(64.0dB(A));3)inthecafe(56.6dB(A)).Thisstudy
tively.Ifnouserdataiscollected,theperformanceis11.3°. designledtoatotalof12sessionsofdatacollectionforeach
Thisislikelybecausedifferentpeoplehaveuniquehead,face, participant,whichisthesameasthepreviousstudy.
andeyeshapes.Therefore,tofurtherreducetheamountof Wetrainedapersonalizedmodelforeachparticipantusing
trainingdatarequiredfromeachnewuser,wemayneedto 6sessionsofdatacollectedinthequietroom.Then,the2
collectasignificantlylargeramountoftrainingdatafroma testingsessionscollectedineachscenariowereusedtotest
morediversesetofparticipants. theperformance ofoursystem indifferent environments.
Theaveragegazetrackingperformanceofoursystemacross
10participantsremainedconsistentat3.8°and4.8°undertwo
6.5 ImpactofEnvironmentNoise
noisyenvironments,playingmusicandinthecafe,whilethe
Toensurethatouracousticsensingsystemisresistantto performanceinthequietroomwas4.6°.Overall,theaverage
differenttypesofenvironmentalnoise,weconductedtwo accuracyofgazetrackingdidnotchangesignificantlywith
experimentsasdescribedinthissubsection. thepresenceofnoiseintheenvironment.Weconducteda
one-wayrepeatedmeasuresANOVAtestamongtheresults
ofthesethreescenariosforall10participantsanddidnot
6.5.1 NoiseInjection. Inthefirstexperiment,werecorded
noisesindifferentenvironmentsusingthemicrophoneson findastatisticallysignificantdifference(𝐹(2,27) =2.46,𝑝 =
0.11 > 0.05). This again validates that our system is not
ourglassframe.Wethenoverlaidthenoiseontothedata
easilyaffectedbyenvironmentalnoise.
collectedintheuserstudytosimulatedifferentnoisyenvi-
ronments.Werecordedthenoiseinfourdifferentenviron-
6.6 ImpactofDifferentGlassFrames
mentsandmeasuredtheaveragenoiselevelsusingasound
levelmeterappcalledNIOSHprovidedbyCDC[17]:1)street Inouruserstudy,weonlytestedoursystemononeglass
noise(70.8dB(A)) recordedonthestreetnearacrossroad; frame(F1).However,webelieveourGazeTraksystemcanbe
2) music noise (64.5 dB(A)) recorded while playing music easilyappliedtoglasseswithdifferentframestyles.Inorder
onacomputer;3)cafenoise(54.5dB(A))recordedinacafe; tovalidatethisassumption,wedeployedoursystemontwoACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
otherpairsofglassesasshowninFig.5.Theoriginalglass 6.7 EvaluationontheFinalPrototype
frame in Fig. 5 (a) is frameless and relatively lightweight. Intheprevioususerstudies,weevaluatedGazeTrakwith
In this study, we applied our system on two new glasses variousconfigurationsunderdifferentscenarios,usingthe
with different styles, size and weight. The first new glass initialprototypethatwehaddeveloped.Theresultsofthese
frame (small glasses, F2) has a smaller size than F1 but a studieshelpedusconfirmtheprototypesettingsanddevelop
largerweightduetotheframearoundthelens(Fig.5(b)). anoptimizedsystemprototype,whichfeaturesamorecom-
Thesecondnewglassframe(largeglasses,F3)withaframe pactformfactorasshowninFig.3(g).Inthissubsection,
aroundlens(Fig.5(c))hasamuchlargersizeandweight ourobjectivewastoassesstheperformanceandpowercon-
comparedtoF1andF2.Toevaluateoursystemonthesenew sumptionofthisfinalprototype.
glassframes,threeparticipantsfromtheoriginalstudy(P1,
P5andP7)agreedtoparticipateinthisadditionalstudy.The 6.7.1 GazeTrackingAccuracy. Toevaluatethefinalproto-
type,werecruited10participants(fourofwhomparticipated
studysetupsandprocedureswereexactlythesameasthe
inthepreviousstudy).Thestudydesignwassimilartothe
previousstudydescribedinSec.5.
previousstudy,exceptthatweonlyusedinstructionpoints
asthegroundtruthacquisitionmethod.Eachparticipantcol-
lectedeightsessionsofdata(sixsessionsfortrainingandtwo
fortesting).Wereducedthesignalstrengthfromthespeaker
to20%oftheoriginalsetup,aswefoundthatevenwith2%of
theoriginalstrength,theperformancewassimilarinthepi-
lotstudy.Hence,thisfinalprototypehassignificantlylower
signalstrengthandimprovedenvironmentalsustainability.
Additionally,wesettheCPUspeedoftheTeensy4.1to150
(a)OriginalGlasses(F1) (b)SmallGlasses(F2) MHzinthisstudy(standardspeed:600MHz)tolowerpower
consumption.Withthissetting,thesystemexperienceda
datalossrateof0.002%,andtheperformanceofoursystem
wasnotaffectedbythisloss,asshowninTab.3.Apartfrom
thecross-sessionperformance,wealsoconductedatestof
thein-sessiontrackingaccuracyinwhichthetrainingdata
andtestingdataweresplitfromthesamesessionswithout
remountingthedevicetoshowtheoptimalperformanceof
(c)LargeGlasses(F3) oursystem.
Figure5:GazeTrakDeployedonGlasseswithVarious Table3:GazeTrackingPerformanceinMGAEwiththe
FrameStyles. FinalPrototype.
Settings P1 P2 P7 P10 P15 P16 P17 P18 P19 P20 Avg
Wecollected12sessionsofdataforeachparticipanttesting
Cross-session 4.4° 4.9° 5.9° 8.0° 3.6° 3.6° 3.9° 4.8° 4.7° 5.7° 4.9°
eachglassframe.SinceSubsec.6.4indicatesthat6sessionsof
In-session 3.9° 3.6° 4.9° 5.3° 1.8° 2.6° 2.6° 3.6° 3.1° 4.9° 3.6°
trainingdataissufficient,wedumpedthefirst4sessionsfor
eachglassframeandusedthelast8sessionstoruna4-fold AsshowninTab.3,themeangazeangularerror(MGAE)
crossvalidationinordertotestthetrackingperformanceof is4.9°forthecross-sessionevaluation,whichissimilarto
oursystemondifferentglasses.Inthiscase,wecanmake the previous study. When evaluating the performance of
sure that participants are familiar with the wearing of all GazeTrakwithinthesamesessions,theaccuracyimproves
theglassframesandeliminatetheimpactofsomerandom to3.6°.Wedidnotaddearloopstothisprototypebecausethe
factors.Theevaluationresultshowsthatthesmallglasses legsoftheglasseswerewiderthantheearloopswehad.For
(F2)yieldedasimilaraverageperformancetotheoriginal mostparticipants,theglassesfitwellontheirears,butone
glasses(F1)(bothat5.3°),whilethelargeglasses(F3)resulted participant(P10)reportedthattheglasseskeptslidingdown
inarelativelypooreraverageperformance(at6.1°),witha duringthestudy,whichmayhaveaffectedtheirperformance.
dropinperformanceof15%.Onepossiblereasonfortheper- Basedonthequestionnaires,noparticipantreportedbeing
formancedifferenceisthatthesensorsonthelargerglasses able to hear the signal emitted from our system. We also
weremuchclosertotheskin.Sometimes,thesensorsmay measuredthesignallevelfromoursystemusingtheNIOSH
directlytouchtheskin,whichcouldblockthetransmission app.Weplacedthephonerunningtheappcloseenoughto
andreceptionofsignals,asweexplainedinSubsec.4.2. thespeakersinoursystemandtheappgaveusanaverageGazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA
signallevelof43.1dB(A).Thisisbelowthemaximumallow- 7.1 MLModels
abledailynoiserecommendedbyCDC,whichis85dB(A) Toachievethisgoal,thedeeplearningmodelsweretrained
overeighthoursintheworkspace[43]. andsynthesizedinadvance,usingtheai8xlibraries[5].We
implementedtwomodelswithai8x,whichwereResNet-18
6.7.2 Power Consumption. We measured the power con- (usedinthepreviousstudy)andMobileNetforcomparison.
sumption of our system with a current ranger [27]. The
DuetothehardwarelimitofMAX78002,wemodifiedthe
averagecurrentflowingthroughthesystemwasmeasured
models to be compatible with the chip. Specifically, for a
as88.3mA@3.26V,whichgivesusapowerconsumption
Conv2dlayer,thekernelsizecouldonlybesetto1x1or3x3
of287.9mW.Thisvaluewastestedwithall8microphones
andthestrideisfixedto[1,1].Inaddition,someconvolu-
and 2 speakers working, and with the data being written
tionlayersofResNet-18weresubstitutedwithdepthwise
intotheSDcard.Oursystemcanlastupto38.5hourswitha
separable convolution layers to avoid exceeding the limit
batteryofsimilarcapacitytoTobiiProGlasses3(3400mAh),
of the number of parameters in the model. Furthermore,
whiletheworkingtimeofTobiiProGlasses3isonly1.75
wequantizedtheinputandtheweightsofthemodelswith
hours. If applied to non-eye-tracking glasses, like Google
ai8x,whichconvertedthemallinto8-bitdataformattosave
Glass,oursystemcanrunfor6.4hours.Itisworthnoting
memoryforstorageandincreasethespeedofinference.
thattheseestimatesdonotincludethepowerconsumption
ofdatapreprocessinganddeeplearninginferencerunning
onalocalserver.Wemeasuredthepowerconsumptionof
differentcomponentsinoursystem(Tab.4).Teensy4.1has 7.2 DataPreprocessing
ahighbasepowerconsumption,whilethesensors(speakers Before the deep learning model, we also need to apply a
andmicrophones)consumemuchlesspower. band-passfilteronthereceivedsignalsandperformcross-
correlationbetweenreceivedsignalsandtransmittedsignals
toobtainechoprofilesasdescribedinSubsubsec.3.1.2.Inthe
Table4:PowerConsumptionofDifferentComponents
implementationonMAX78002,weremovedtheband-pass
onTeensy4.1.Powerofdatapreprocessinganddeep
filtersinceallthecomputationsaredoneontheMCUandwe
learninginferenceisNOTincluded.
donothavetouseaband-passfiltertofilterouttheaudible
rangeofsignalstoprotectusers’privacy.Besides,thishelps
Total Speakers&Mics SDcardwriting Otheroperations
reducetheprocessingtime.
287.9mW 16.4mW 72.7mW 198.8mW Thenweexperimentedtwodifferentmethodstorealize
thecross-correlation:(1)bruteforcetocalculateechoprofiles
pointbypoint;(2)thedotproductfunctionintheCMSIS-DSP
library.Resultsofstandardtests[6]revealedthatittookthe
6.7.3 Usability. Aftertheuserstudy,wedistributedaques-
system178.3msand45.4mstocomputeoneechoframeand
tionnaire to every participant to ask for feedback on our
makeoneinferencewiththesetwomethodsutilizedrespec-
prototype.First,theparticipantsevaluatedtheoverallcom-
tively.Consideringthatoneframeofouraudiodatacomes
fortablenessandtheweightoftheprototypewitharating
from0to5.Acrossall10participants,theaveragescoresthey
every12msinoursystem(600samples÷50000samples/s),
thisprocessingtimeistoolongtokeepoursystemrunning
gavetothesetwoaspectsare4.5(std=0.7)and4.2(std=0.8),
in real-time with an FPS of 83.3 Hz. Finally, we explored
indicatingthatGazeTrakisoverallcomfortabletowearand
method(3)aConv2dlayer(kernelsize1x1)withtransmitted
easytouse.Furthermore,all10participantsanswered"No"
signalsastheuntrainedweightsandreceivedsignalsplaced
tothequestion"Canyouhearthesoundemittedfromour
alongthechannelaxisoftheinput.Thiscanincreasethe
system?",verifyingtheinaudibilityoftheacousticsignals
speedofechoprofilecalculationbecauseitusestheCNN
fromtheGazeTraksystem.
acceleratoronMAX78002.Wecompressedthesamplesused
forcross-correlationfrom600x600to34x34andthepixels
7 DATAPREPROCESSINGANDMODEL
ofinterestfrom60pixels(20.4cm)to30pixels(10.2cm)in
INFERENCEONMAX78002
thiscasetofurtherdecreasetheprocessingtime.
In the previous evaluation, we recorded audio data with WiththisConv2dlayeraddedontopofthedeeplearning
Teensy 4.1 first and run the signal processing and deep model,themodeldirectlytakestherawaudiodataasinputin
learningpipelineonalocalserveroffline.Toenablepredic- instanceswiththesizeof64(34+30samples)x26(frames)x
tionsofgazepositionsinreal-timeonanMCU,wedeployed 8(microphones).Thismethodallowsthesystemtomakeone
thewholepipelineonamicro-controllerwithanultra-low- inferencewithin10.3ms,whichisenoughforthereal-time
powerCNNaccelerator(MAX78002[13]). pipelinewithadouble-buffermethodapplied(DMAmovesACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
thecurrentframeinonebufferwhiletheCPUprocessesthe IfwecanuseMAX78002todirectlycontrolspeakersand
previousframeinanotherbuffer). microphonesinfuture,wewillbeabletooptimizethepower
efficiencyandkeeptheoverallpowerconsumptionofour
7.3 AccuracyandRefreshRate real-timesystemaround95.4mW,i.e.,79.0mW(MAX78002
Tovalidatethesemodificationsandcompression,weeval- withResNet-18runningat30HZ)+16.4mW(2speakersand
uatedthein-sessionperformanceofdifferentmodelswith 8microphones).Weadmitthatthisisjustanestimateofthe
differentsettingsusingdatacollectedwiththefinalproto- powerofthisreal-timesystemandthepowerconsumption
typeinSubsec.6.7andshowedtheresultsinTab.5. ofMAX78002mightincreaseifitdoesneedtocontrolthe
sensorsbutwedonotexpectittobeveryhighbecausethe
current power of MAX78002 already includes that of the
Table5:AverageIn-sessionPerformanceacross10Par-
CPUandtheCNNacceleratorrunningatfullspeed.
ticipantswithDifferentModelsandSettings.
Table6:PowerConsumptionofMAX78002withDif-
Models MLLibraries Compressed? Quantized? MGAE ferentModelsRunning.
pytorch ✗ ✗ 3.6°
ai8x ✗ ✗ 4.0°
ResNet-18 Models ResNet-18 MobileNet
ai8x ✓ ✗ 4.0°
ai8x ✓ ✓ 4.2° FPS(Hz) 83.3 30 83.3 30
ai8x ✓ ✗ 4.2°
MobileNet Power(mW) 96.9 79.0 86.0 75.7
ai8x ✓ ✓ 4.3°
Asshowninthetable,thesamemodeltrainedwithai8x 8 DISCUSSION
isslightlyworsethanthattrainedwithPyTorchgiventhe
8.1 EvaluatingSimplerRegressionModels
constraintsoftheconvolutionlayersdiscussedabove.Com-
pressingthesizeofinputdatadoesnotaffecttheaccuracy. We adopted two traditional regression models, which are
WhileMobileNetyieldscomparableaccuracytoResNet-18, linearregression(LR)andgradientboostedregressiontrees
bothmodelssufferaslightperformancedropafterquantiza- (GBRT),topredictgazepositionsusingthedatacollectedin
tionsincetheprecisionofdataisdecreased. Subsec.6.7andtheresultsshowedthattheaveragein-session
GiventhelimitationoftheI2SinterfacesonMAX78002, trackingaccuracyforthesetwomodelsacross10participants
totestoursysteminamorerealisticcondition,westilluse are11.6°and6.8°respectively.Comparedtotheresultsin
Teensy 4.1 to control the speakers and microphones and Tab.3,thetraditionalregressionmodelsoutputmuchworse
transfer the received audio data to MAX78002 via the se- accuraciesthanResNet-18(3.6°).Weconductedananalysis
rial port. This generates a steady stream of audio data to oftheimpurity-basedfeatureimportancewithGBRT,com-
MAX78002similartothestreamitwouldreceiveifitcould paringthefeaturesindifferentchannelsofmicrophonesin
accommodate8microphonesbyitself.Infuture,wewillex- Tab.7.Itturnsoutthatthechannelsreceivingsignalsfrom
ploreconnectingmicrophonesdirectlytoMAX78002using 18-21kHz(S1)aregenerallymoreimportantthanchannels
multi-channelaudioprotocolssuchasTime-divisionMulti- receivingsignalsfrom21.5-24.5kHz(S2).Furthermore,the
plexing(TDM).EvaluationresultsshowedthatforResNet- microphonesthatareclosertotheinnercornersoftheeyes
18andMobileNet,MAX78002spent124.1msand41.6ms (M1,M4,M5M8)aremoreimportantthanthosecloserto
respectivelyloadingtheweightsofthemodel.Thisisaone- thetailsoftheeyes(M2,M3,M6,M7).
time effort and can be done before running the real-time
Table7:FeatureImportanceAnalysisforDifferentMi-
pipeline.Thenittook12mstoloadoneinstanceandmake
crophonesusingGBRT(Scaledto0-100).
aninferencebasedonitforbothResNet-18andMobileNet,
givingarefreshrateof83.3Hz.
Microphones M1 M2 M3 M4 M5 M6 M7 M8
Importance(S1/S2) 100/27 57/23 33/12 96/29 66/81 42/17 28/15 85/79
7.4 PowerConsumption
WemeasuredthepowerconsumptionoftheMAX78002eval-
8.2 ImpactofReal-worldFactors
uationkitwhileitmadeinferences.Tab.6demonstratesthat
MAX78002consumes96.9mWand86.0mWrespectively 8.2.1 HeadMovements. Intheuserstudy,wedidnotuse
whenmakinginferenceswithResNet-18andMobileNetat achinresttofixtheparticipants’headsotheycouldturn
83.3Hz.Therefreshratecanbereducedto30Hztosave theirheadfreely.However,webelievethatathorougheval-
power,whichisenoughformanyapplications.Inthiscase, uationonhowheadmovementsaffectsystemperformance
thepowerbecomes79.0mWand75.7mWrespectively. ispreferredinfuturework.GazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA
8.2.2 Near-andFar-sighted. Intheuserstudy,wecollected forusers.However,Subsec.6.1showsthatthetrackingaccu-
participants’degreesofmyopiainthequestionnaires,which racywithoutfine-tuningisstillacceptable,at5.9°,compared
showednoconnectiontothegazetrackingperformance. totheaccuracyachievedwithfine-tuning(4.9°).
8.2.3 UserSpeaking. Oneresearcherevaluatedoursystem 8.4.3 Reducing Training Effort. Subsec. 6.4 suggests that
whenkeepingsilentandkeepingtalkingtohimself.Thegaze GazeTrakachievessatisfactoryperformanceonnewusers
trackingperformanceofthesilentsessionsandthetalking withapproximately20minutesoftrainingdatausingthe
sessionsisthesameat3.9°. user-adaptivemodel.Thistrainingeffortcanbefurtherre-
ducedbyconstructingalargerandmorediversedatasetfrom
muchmoreparticipantstotrainthebasedmodel.Moreover,
8.3 PotentialApplications
dataaugmentationmethods,suchasincludingsimulation
The goal of this paper is to demonstrate the feasibility of datatotrainthemodel,canbeexploredaswell.
our new acoustic-based gaze tracking system on glasses.
While our eye tracking accuracy of 4.9° is comparable to 8.4.4 TowardsaMoreIntegratedSystem. InSec.7,westill
somewebcam-basedmethods,itislowerthancommercial usedTeensy4.1tocontrolthespeakersandmicrophones,
eyetrackers(1.9°inourstudy).Therefore,oursystemmay andtransferaudiodatatotheMCUMAX78002.Infuture,
notbeimmediatelyapplicabletosomeapplicationsrequiring weplantofurthercustomizeourownPCBsforMAX78002
highlypreciseeyetracking.However,oursystemcanstillbe toallowittodirectlycontrolspeakersandmicrophones.We
usedinmanyapplications,suchasinteractionwithinterface believethatthepowerconsumptionofourreal-timesystem
elementslikebuttonsinAR,thatdonotrequireveryhigh canbefurtherreducedinthiscasebecauseTeensy4.1with
accuracyeyetrackers. ahighbasepowercanberemoved.Furthermore,wedonot
Oursystemcanalsobepotentiallyusedintrackingirreg- expectthatthepowerconsumptionofMAX78002willbe
ular eye movements, enabling healthcare applications for significantlyincreasedsincetheon-boardCPUandCNNac-
monitoringusers’healthconditionsineverydaylife.This celeratorofMAX78002werealreadyoperatingatmaximum
requiresmonitoringthegazemovementsthroughouttheday speedinourcurrentevaluation.Withasolidsystemimple-
foranalysisineverydaylife,insteadofjusttrackingtheirac- mentation,weplantocarryoutanextensiveevaluationof
curategazepositionsforafewhoursinacontrolledsettings. thismoreintegratedsysteminfutureworktovalidateour
The low-power and lightweight features of our GazeTrak speculation.
systemmakeitagoodcandidatesolutiontoenablingava-
rietyofapplicationsthatcamera-basedeyetrackerscannot 9 CONCLUSION
realize,bycontinuouslyunderstandingusergazemovements
Inthispaper,wepresentthefirstacoustic-basedeyetrack-
inthewildforextendedperiods.Furthermore,oursystem
ingglassescapableofcontinuousgazetracking.Thestudy
canalleviatetheprivacyconcernfromusersascomparedto
involving20participantsconfirmsthatoursystemcanac-
camera-basedmethods.
curatelytrackgazepointscontinuously,achievinganaccu-
racyof3.6°withinthesamesessionand4.9°acrossdifferent
8.4 LimitationandFutureWork sessions.Whencomparedtocommercialcamera-basedeye
trackingglassessuchasTobiiProGlasses3,oursystemre-
8.4.1 ImprovingthePerformance. Thereisroomforfurther
ducespowerconsumptionby95%.Areal-timepipelineis
improvements of the performance of our system. For in-
implementedonMAX78002tomakeinferenceswithapower
stance,wecanapplycalibrationprocessontheoutputofthe
signatureof95.4mWat30Hz.
systemtofurtherenhanceperformance.Weexperimented
with affine transformation and projective transformation
totransformtheoutputbuttheydidnotimmediatelyim- ACKNOWLEDGMENTS
provetheperformance.Accordingtotheanalysisoftheerror ThisworkissupportedbyNationalScienceFoundation(NSF)
distributionoftheeyetrackingresults,webelievethisisbe- underGrantNo.2239569,NSF’sInnovationCorps(I-Corps)
causetheerrordistributionisnotlinearinoursystemsowe underGrantNo.2346817,theIgniteProgramandtheAnn
needtoexploremorenon-lineartransformationmethodsto S.BowersCollegeofComputingandInformationScience
improvetheperformance. atCornellUniversity.TheauthorswouldliketothankProf.
SusanFussellandherlabforsharingtheaccesstothecom-
8.4.2 CalibrationProcessforFine-tuning. Oursystemcur- mercial eye tracker Tobii Pro Fusion with us. We also ap-
rentlyrequiresa15-secondcalibrationprocessbeforeeach preciatethehelpofaCornellstudent,VipinGunda,onthe
sessiontofine-tunethemodel,whichmaybeinconvenient developmentofthepipelineonMAX78002.ACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
REFERENCES
[17] CentersforDiseaseControlandPrevention(CDC).2023. NIOSH
[1] TobiiAB.2022. TobiiProFusion. RetrievedSept.13,2022from SoundLevelMeterApp. RetrievedMar13,2023fromhttps://www.
https://www.tobiipro.com/product-listing/fusion/ cdc.gov/niosh/topics/noise/app.html
[2] TobiiAB.2022. TobiiProGlasses3. RetrievedSept.13,2022from [18] YangGao,YinchengJin,SeokminChoi,JiyangLi,JunjiePan,LinShu,
https://www.tobiipro.com/product-listing/tobii-pro-glasses-3/ ChiZhou,andZhanpengJin.2022.SonicFace:TrackingFacialExpres-
[3] Artsiom Ablavatski, Andrey Vakunov, Ivan Grishchenko, Karthik sionsUsingaCommodityMicrophoneArray.Proc.ACMInteract.Mob.
Raveendran,andMatsveiZhdanovich.2020.Real-timePupilTracking WearableUbiquitousTechnol.5,4,Article156(dec2022),33pages.
fromMonocularVideoforDigitalPuppetry. CoRRabs/2006.11341 https://doi.org/10.1145/3494988
(2020).arXiv:2006.11341 https://arxiv.org/abs/2006.11341 [19] GazeRecorder.2022.GazeRecorderWebcamEyeTracking. Retrieved
[4] ChristerAhlstromandTaniaDukic.2010.ComparisonofEyeTrack- Sept.13,2022fromhttps://gazerecorder.com/webcam-eye-tracking-
ing Systems with One and Three Cameras. In Proceedings of the accuracy/
InternationalConferenceonMethodsandTechniquesinBehavioral [20] AndreGolardandSachinSTalathi.2021. UltrasoundforGazeEs-
Research(MB).Article3,4pages. https://doi.org/10.1145/1931344. timation—AModelingandEmpiricalStudy. Sensors21,13(2021),
1931347 4502.
[5] AnalogDevicesAI.2022.MaximIntegratedAI. RetrievedAug23,2023 [21] BrownHCIGroup.2021. WebGazer.js:DemocratizingWebcamEye
fromhttps://github.com/MaximIntegratedAI Tracking on the Browser. Retrieved Sept. 13, 2022 from https:
[6] Analog Devices AI. 2023. MAX7800x Power Monitor and //webgazer.cs.brown.edu/#publication
Energy Benchmarking Guide. Retrieved Aug 23, 2023 from [22] CraigHennesseyandJacobFiset.2012. Longrangeeyetracking:
https://github.com/MaximIntegratedAI/MaximAI_Documentation/ bringing eye tracking into the living room. In Proceedings of the
blob/master/Guides/MAX7800x%20Power%20Monitor%20and% SymposiumonEyeTrackingResearchandApplications.249–252.
20Energy%20Benchmarking%20Guide.md [23] iMotions.2022. SMIEyeTrackingGlasses. RetrievedSept.13,2022
[7] TanyaBafna,PerBækgaard,andJohnPaulinPaulinHansen.2021.Eye- fromhttps://imotions.com/hardware/smi-eye-tracking-glasses/
Tell:Tablet-basedCalibration-freeEye-typingusingSmooth-pursuit [24] MoritzKassner,WilliamPatera,andAndreasBulling.2014.Pupil:an
movements. In ACM Symposium on Eye Tracking Research and opensourceplatformforpervasiveeyetrackingandmobilegaze-
Applications.1–6. based interaction. In Proceedings of the 2014 ACM international
[8] FrankHBorsatoandCarlosHMorimoto.2016.Episcleralsurfacetrack- joint conference on pervasive and ubiquitous computing: Adjunct
ing:challengesandpossibilitiesforusingmicesensorsforwearable publication.1151–1160.
eyetracking.InProceedingsoftheNinthBiennialACMSymposium [25] KyleKrafka,AdityaKhosla,PetrKellnhofer,HariniKannan,Suchendra
onEyeTrackingResearch&Applications.39–46. Bhandarkar,WojciechMatusik,andAntonioTorralba.2016.EyeTrack-
[9] AndreasBulling,DanielRoggen,andGerhardTröster.2008. It’sin ingforEveryone.InProceedingsoftheIEEEConferenceonComputer
youreyes:Towardscontext-awarenessandmobileHCIusingwearable VisionandPatternRecognition(CVPR).
EOGgoggles.InProceedingsofthe10thinternationalconferenceon [26] MikkoKytö,BarrettEns,ThammathipPiumsomboon,GunA.Lee,and
Ubiquitouscomputing.84–93. MarkBillinghurst.2018. Pinpointing:PreciseHead-andEye-Based
[10] AndreasBulling,DanielRoggen,andGerhardTröster.2009.Wearable TargetSelectionforAugmentedReality.InProceedingsoftheCHI
EOGGoggles:Eye-BasedInteractioninEverydayEnvironments.In ConferenceonHumanFactorsinComputingSystems.1–14. https:
CHIExtendedAbstractsonHumanFactorsinComputingSystems. //doi.org/10.1145/3173574.3173655
3259–3264. https://doi.org/10.1145/1520340.1520468 [27] LowPowerLab.2018.IntroductiontoCurrentRanger. RetrievedMar
[11] Communication by Gaze Interaction Association. 2010. 13,2023fromhttps://lowpowerlab.com/guide/currentranger/
Woking copy of definitions and terminology for Eye Tracker [28] PupilLabs.2022.PupilInvisible. RetrievedSept.13,2022fromhttps:
accuracy and precision. Retrieved Sept. 13, 2022 from //pupil-labs.com/products/
http://old.cogain.org/forums/eye-tracker-accuracy-and-precision- [29] AntonioLanata,GaetanoValenza,AlbertoGreco,andEnzoPasquale
general-discussion/eye-tracker-accuracy-terms-and-definiti.html Scilingo.2015.Robustheadmountedwearableeyetrackingsystemfor
[12] ChulWooCho,JiWooLee,KwangYongShin,EuiChulLee,KangRy- dynamicalcalibration.JournalofEyeMovementResearch8,5(2015).
oungPark,HeekyungLee,andJihunCha.2012. GazeDetectionby [30] YaxiongLei.2021. EyeTrackingCalibrationonMobileDevices.In
WearableEye-TrackingandNIRLED-BasedHead-TrackingDevice ACMSymposiumonEyeTrackingResearchandApplications(ETRA
BasedonSVR.EtriJournal34,4(2012),542–552. Adjunct).Article4,4pages. https://doi.org/10.1145/3450341.3457989
[13] Analog Devices. 2022. MAX78002 Evaluation Kit. Retrieved [31] JueLi,HengLi,WaleedUmer,HongweiWang,XuejiaoXing,Shukai
Aug 23, 2023 from https://www.analog.com/media/en/technical- Zhao,andJunHou.2020.Identificationandclassificationofconstruc-
documentation/data-sheets/MAX78002EVKIT.pdf tionequipmentoperators’mentalfatigueusingwearableeye-tracking
[14] DigiKey. 2023. OWR-05049T-38D. Retrieved Mar 13, 2023 from technology.AutomationinConstruction109(2020),103000.
https://www.digikey.com/en/products/detail/ole-wolff-electronics- [32] KeLi,RuidongZhang,SiyuanChen,BoaoChen,MoseSakashita,
inc/OWR-05049T-38D/13683703 François Guimbretière, and Cheng Zhang. 2024. EyeEcho: Con-
[15] SiboDong,JustinGoldstein,andGraceHuiYang.2022.GazBy:Gaze- tinuous and Low-power Facial Expression Tracking on Glasses.
BasedBERTModeltoIncorporateHumanAttentioninNeuralInfor- arXiv:2402.12388[cs.HC]
mationRetrieval.182–192. https://doi.org/10.1145/3539813.3545129 [33] KeLi,RuidongZhang,BoLiang,FrançoisGuimbretière,andCheng
[16] Ergoneers. 2022. Dikablis Glasses 3. Re- Zhang.2022.EarIO:ALow-powerAcousticSensingEarableforCon-
trieved Sept. 13, 2022 from https://www.ergoneers. tinuouslyTrackingDetailedFacialMovements. Proceedingsofthe
com/en/mobile-eye-tracker-dikablis-glasses-3/?gclid= ACMonInteractive,Mobile,WearableandUbiquitousTechnologies
CjwKCAiA9tyQBhAIEiwA6tdCrFd7F7xBwNa4XVP09wRHlBATh_ 6,2(2022),1–24.
jafRuH5ErUVdhJt5WVLK2_FdVs7RoCpK4QAvD_BwE [34] TianxingLi,QiangLiu,andXiaZhou.2017.Ultra-LowPowerGaze
TrackingforVirtualReality.InProceedingsoftheACMConference
onEmbeddedNetworkSensorSystems(SenSys).Article25,14pages.GazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA
https://doi.org/10.1145/3131672.3131682 andforthediagnosisofdevelopmentaldisordersinyoungchildren.
[35] TianxingLiandXiaZhou.2018.Battery-FreeEyeTrackeronGlasses. InIEEEInternationalSymposiumonRobotandHumanInteractive
In Proceedings of the Annual International Conference on Mobile Communication (RO-MAN). 594–598. https://doi.org/10.1109/
ComputingandNetworking(MobiCom).67–82. https://doi.org/10. ROMAN.2007.4415154
1145/3241539.3241578 [49] PJRC.2023. AudioAdaptorBoardsforTeensy3.xandTeensy4.x.
[36] BhanukaMahanama.2022.Multi-UserEye-Tracking.InSymposium RetrievedMar13,2023fromhttps://www.pjrc.com/store/teensy3_
on Eye Tracking Research and Applications (ETRA). Article 36, audio.html
3pages. https://doi.org/10.1145/3517031.3532197 [50] PJRC.2023.Teensy4.1DevelopmentBoard. RetrievedMar13,2023
[37] SaifMahmud,KeLi,GuilinHu,HaoChen,RichardJin,RuidongZhang, fromhttps://www.pjrc.com/store/teensy41.html
FrançoisGuimbretière,andChengZhang.2023.PoseSonic:3DUpper [51] WayneJRyan,AndrewTDuchowski,andStanTBirchfield.2008.Lim-
BodyPoseEstimationThroughEgocentricAcousticSensingonSmart- bus/pupilswitchingforwearableeyetrackingundervariablelighting
glasses.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.7,3, conditions.InProceedingsofthe2008symposiumonEyetracking
Article111(sep2023),28pages. https://doi.org/10.1145/3610895 research&applications.61–64.
[38] MizukiMatsubara,JoachimFolz,TakumiToyama,MarcusLiwicki, [52] ShreshthSaxena,ElkeLange,andLaurenFink.2022. TowardsEffi-
Andreas Dengel, and Koichi Kise. 2015. Extraction of read text cientCalibrationforWebcamEye-TrackinginOnlineExperiments.
using a wearable eye tracker for automatic video annotation. In InSymposiumonEyeTrackingResearchandApplications(ETRA).
AdjunctProceedingsofthe2015ACMInternationalJointConference Article27,7pages. https://doi.org/10.1145/3517031.3529645
onPervasiveandUbiquitousComputingandProceedingsofthe2015 [53] RealEyesp.zo.o.2022. RealEyeWebcamEye-Tracking. Retrieved
ACMInternationalSymposiumonWearableComputers.849–854. Sept.13,2022fromhttps://www.realeye.io/
[39] Addison Mayberry, Pan Hu, Benjamin Marlin, Christopher Salt- [54] RujiaSun,XiaoheZhou,BenjaminSteeper,RuidongZhang,Sicheng
house, and Deepak Ganesan. 2014. IShadow: Design of a Wear- Yin,KeLi,ShengzhangWu,SamTilsen,FrancoisGuimbretiere,and
able, Real-Time Mobile Gaze Tracker. In Proceedings of the 12th ChengZhang.2023.EchoNose:SensingMouth,BreathingandTongue
Annual International Conference on Mobile Systems, Applications, GesturesinsideOralCavityusingaNon-contactNoseInterface.In
andServices(MobiSys’14).82–94. https://doi.org/10.1145/2594368. Proceedingsofthe2023ACMInternationalSymposiumonWearable
2594388 Computers(Cancun,QuintanaRoo,Mexico)(ISWC’23).Association
[40] AddisonMayberry,YaminTun,PanHu,DuncanSmith-Freedman, forComputingMachinery,NewYork,NY,USA,22–26. https://doi.
DeepakGanesan,BenjaminM.Marlin,andChristopherSalthouse. org/10.1145/3594738.3611358
2015. CIDER:EnablingRobustness-PowerTradeoffsonaCompu- [55] TDK.2023.ICS-43434. RetrievedMar13,2023fromhttps://invensense.
tational Eyeglass. In Proceedings of the 21st Annual International tdk.com/products/ics-43434/
Conference on Mobile Computing and Networking (MobiCom’15). [56] CihanTopal,AtakanDogan,andOmerNezihGerek.2008.Awearable
400–412. https://doi.org/10.1145/2789168.2790096 head-mountedsensor-basedapparatusforeyetrackingapplications.
[41] RajalakshmiNandakumar,ShyamnathGollakota,andNathanielWat- In2008IEEEConferenceonVirtualEnvironments,Human-Computer
son.2015. Contactlesssleepapneadetectiononsmartphones.In InterfacesandMeasurementSystems.IEEE,136–139.
Proceedingsofthe13thannualinternationalconferenceonmobile [57] CihanTopal,ÖmerNezihGerek,andAtakanDogˇan.2008. Ahead-
systems,applications,andservices.45–57. mounted sensor-based eye tracking device: eye touch system. In
[42] BasilioNoris,Jean-BaptisteKeller,andAudeBillard.2011.Awearable Proceedings of the 2008 symposium on Eye tracking research &
gazetrackingsystemforchildreninunconstrainedenvironments. applications.87–90.
ComputerVisionandImageUnderstanding115,4(2011),476–486. [58] MéLodieVidal,JaysonTurner,AndreasBulling,andHansGellersen.
[43] U.S.DepartmentofHealthandHumanServices.1998.Criteriafora 2012.WearableEyeTrackingforMentalHealthMonitoring.Computer
recommendedstandard:occupationalnoiseexposure.DHHS(NIOSH) Communications35,11(jun2012),1306–1311. https://doi.org/10.1016/
PublicationNo.98–126(1998). https://www.cdc.gov/niosh/docs/98- j.comcom.2011.11.002
126/ [59] QuanWang,LauraBoccanfuso,BeibinLi,AmyYeo-jinAhn,ClaireE.
[44] TakehikoOhno,NaokiMukawa,andShinjiroKawato.2003. Just Foster,MargaretP.Orr,BrianScassellati,andFrederickShic.2016.
blinkyoureyes:Ahead-freegazetrackingsystem.InCHI’03extended ThermographicEyeTracking.InProceedingsoftheBiennialACM
abstractsonHumanfactorsincomputingsystems.950–957. SymposiumonEyeTrackingResearch&Applications.307–310. https:
[45] LucasPaletta,HelmutNeuschmied,MichaelSchwarz,GeraldLodron, //doi.org/10.1145/2857491.2857543
MartinPszeida,StefanLadstätter,andPatrickLuley.2014. Smart- [60] TianbenWang,DaqingZhang,YuanqingZheng,TaoGu,Xingshe
phoneEyeTrackingToolbox:AccurateGazeRecoveryonMobileDis- Zhou,andBernadetteDorizzi.2018.C-FMCWbasedcontactlessres-
plays.InProceedingsoftheSymposiumonEyeTrackingResearch pirationdetectionusingacousticsignal.ProceedingsoftheACMon
andApplications(ETRA).367–68. https://doi.org/10.1145/2578153. Interactive,Mobile,WearableandUbiquitousTechnologies1,4(2018),
2628813 1–20.
[46] Alexandra Papoutsaki. 2015. Scalable Webcam Eye Tracking by [61] EricWhitmire,LauraTrutoiu,RobertCavin,DavidPerek,BrianScally,
LearningfromUserInteractions.InProceedingsoftheAnnualACM James Phillips, and Shwetak Patel. 2016. EyeContact: scleral coil
Conference Extended Abstracts on Human Factors in Computing eye tracking for virtual reality. In Proceedings of the 2016 ACM
Systems(CHIEA).219–222. https://doi.org/10.1145/2702613.2702627 InternationalSymposiumonWearableComputers.184–191.
[47] Alexandra Papoutsaki, James Laskey, and Jeff Huang. 2017. [62] KatarzynaWisiecka,KrzysztofKrejtz,IzabelaKrejtz,DamianSromek,
Searchgazer:Webcameyetrackingforremotestudiesofwebsearch.In AdamCellary,BeataLewandowska,andAndrewDuchowski.2022.
Proceedingsofthe2017conferenceonconferencehumaninformation ComparisonofWebcamandRemoteEyeTracking.InSymposiumon
interactionandretrieval.17–26. EyeTrackingResearchandApplications(ETRA).Article32,7pages.
[48] LorenzoPiccardi,BasilioNoris,OlivierBarbey,AudeBillard,Giusep- https://doi.org/10.1145/3517031.3529615
pinaSchiavone,FlavioKeller,andClaesvonHofsten.2007.WearCam: [63] ZhefanYe,YinLi,AlirezaFathi,YiHan,AgataRozga,GregoryD
A head mounted wireless camera for monitoring gaze attention Abowd,andJamesMRehg.2012.DetectingeyecontactusingwearableACMMobiCom’24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
eye-trackingglasses.InProceedingsofthe2012ACMconferenceon on Human Factors in Computing Systems (Hamburg, Germany)
ubiquitouscomputing.699–704. (CHI’23).AssociationforComputingMachinery,NewYork,NY,USA,
[64] RuidongZhang,HaoChen,DevanshAgarwal,RichardJin,KeLi, Article852,18pages. https://doi.org/10.1145/3544548.3580801
FrançoisGuimbretière,andChengZhang.2023. HPSpeech:Silent [66] Yanxia Zhang, Andreas Bulling, and Hans Gellersen. 2011. Dis-
SpeechInterfaceforCommodityHeadphones.InProceedingsofthe criminationofgazedirectionsusinglow-leveleyeimagefeatures.
2023ACMInternationalSymposiumonWearableComputers(Can- InProceedingsofthe1stinternationalworkshoponpervasiveeye
cun,QuintanaRoo,Mexico)(ISWC’23).AssociationforComputing tracking&mobileeye-basedinteraction.9–14.
Machinery, New York, NY, USA, 60–65. https://doi.org/10.1145/ [67] AnjieZhu,QianjingWei,YilinHu,ZhangweiZhang,andShiweiCheng.
3594738.3611365 2018. MobiET:ANewApproachtoEyeTrackingforMobileDe-
[65] Ruidong Zhang, Ke Li, Yihong Hao, Yufan Wang, Zhengnan Lai, vice.InProceedingsoftheACMInternationalJointConferenceand
FrançoisGuimbretière,andChengZhang.2023.EchoSpeech:Continu- International Symposium on Pervasive and Ubiquitous Computing
ousSilentSpeechRecognitiononMinimally-obtrusiveEyewearPow- andWearableComputers(UbiComp).862–869. https://doi.org/10.
eredbyAcousticSensing.InProceedingsofthe2023CHIConference 1145/3267305.3274174