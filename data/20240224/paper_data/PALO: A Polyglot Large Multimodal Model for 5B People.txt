PALO: A Polyglot Large Multimodal Model for 5B People
MuhammadMaaz1*,HanoonaRasheed1*,AbdelrahmanShaker1,SalmanKhan1,2
HishamCholakal1,RaoM.Anwer1,3,TimBaldwin1,4,MichaelFelsberg5,FahadS.Khan1,5
1MohamedbinZayedUniversityofAI,2AustralianNationalUniversity,3AaltoUniversity
4TheUniversityofMelbourne,5LinköpingUniversity
Abstract
InpursuitofmoreinclusiveVision-Language
Models(VLMs),thisstudyintroducesaLarge
MultilingualMultimodalModelcalledPALO.
PALO offers visual reasoning capabilities in
10 major languages, including English, Chi-
nese,Hindi,Spanish,French,Arabic,Bengali,
Russian, Urdu, and Japanese, that span a to-
talof∼5Bpeople(65%oftheworldpopula-
tion). Ourapproachinvolvesasemi-automated
translationapproachtoadaptthemultimodal
instruction dataset from English to the target
languagesusingafine-tunedLargeLanguage
Model,therebyensuringhighlinguisticfidelity
whileallowingscalabilityduetominimalman-
ual effort. The incorporation of diverse in-
struction sets helps us boost overall perfor- Figure1: PALOvs. English-VLMs. Theplotcompares
mance across multiple languages especially PALO with corresponding Vision-Language Models
thosethatareunderrepresentedlikeHindi,Ara- (VLMs)across10differentlanguages. Theselanguages
bic, Bengali, and Urdu. The resulting mod- includeEnglish,Chinese,Hindi,Spanish,French,Ara-
els are trained across three scales (1.7B, 7B bic,Bengali,Russian,Urdu,andJapanese,collectively
and 13B parameters) to show the generaliza- covering approximately 5B people and 65% of the
tionandscalabilitywhereweobservesubstan- global population. English-trained VLMs, such as
tial improvements compared to strong base- LLaVAandMobileVLM,exhibitpoorperformanceon
lines. We also propose the first multilingual low-resourcelanguagesincludingHindi,Arabic,Ben-
multimodalbenchmarkfortheforthcomingap- gali,andUrdu,duetotheunder-representationofthese
proachestoevaluatetheirvision-languagerea- languagesduringtheirtrainingphases. PALO,incon-
soning capabilities across languages. Code: trast, is a unified model that can hold conversations
https://github.com/mbzuai-oryx/PALO. simultaneouslyinallthetenlanguages,demonstrating
consistentperformanceacrosstheboard.
1 Introduction
leavingasignificantgapinmultimodalunderstand-
PropelledbyadvancementsingenerativeAI,Large
ing for non-English languages. As a result, the
Multimodal Models (LMMs) (Liu et al., 2023b;
existing LMMs generally overlook the linguistic
Zhu et al., 2023; Dai et al., 2023) have emerged
diversityoftheglobalpopulation,particularlylan-
as a pivotal advancement in the field, seamlessly
guages spoken by large groups, such as Chinese,
bridgingthegapbetweenvisionandlanguagetasks.
Hindi,Spanish,French,Arabic,Bengali,Russian,
While initial efforts such as LLaVA (Liu et al.,
Urdu,andJapanese,whichcollectivelyaccountfor
2023b) and miniGPT4 (Zhu et al., 2023) have
billions of native speakers. Our work addresses
demonstratedintriguingperformanceinsynthesiz-
this disparity by developing the first fully open-
ingeffectivetextualresponsesbasedonvisualin-
sourcemultilingualLMMcalled PALO,whichen-
puts,theyhavepredominantlyfocusedonEnglish,
compasses ten major languages covering 65% of
1Equallycontributingfirstauthors. theglobalpopulation,withaspecialfocusonlan-
4202
beF
22
]LC.sc[
1v81841.2042:viXraguagesunderrepresentedinthecurrentmultimodal and Urdu, without compromising its high-
models. performanceonhigh-resourcelanguagese.g.,
Thechallengeliesinthescarcityofhigh-quality English,Chinese,French,andSpanish.
multilingualmultimodaldatacomparedtoEnglish.
Addressing the challenge of limited high-quality
2 RelatedWorks
data, especially for under-represented languages
suchasHindi,Arabic,Bengali,andUrdu,ourap- The introduction of Large Language Models
proachinvolvescarefulanalysisandsubsequentre- (LLMs) has significantly advanced the field of
finementoftranslationsproducedbyastate-of-the- natural language processing. However, the de-
art Large Language Model (LLM) (Brown et al., velopment of multilingual LLMs has faced con-
2020)foreachtargetlanguage. Byidentifyingand siderablechallenges,primarilyduetotheskewed
correctingtranslationinaccuraciesthroughhuman distribution of language data (Costa-jussà et al.,
intervention,wegenerateahigh-qualitymultilin- 2022). EnglishandEuropeanlanguagesdominate
gual dataset. This curated dataset then serves as existingdatasets,leavingwidelyspokenlanguages
thefoundationforrefiningthetargetlanguagean- suchas MandarinChinese andHindiunderrepre-
notations, ensuring a more accurate and nuanced sented(Eberhardetal.,2015). Moreover,integrat-
representationofthetargetlanguageintraining. ing multiple languages into LLMs often leads to
Leveragingourhigh-qualitymultilingualvision- adeclineinEnglishlanguageperformance(Scao
language instruction dataset and the recent ad- et al., 2022), highlighting a major challenge in
vancesinlargemultimodalmodeling,wedevelop maintainingcross-lingualperformance.
PALO asaunified modelthatcansimultaneously Recenteffortshaveaimedtoaddressthesechal-
answer questions in ten different languages. Our lengesbydevelopingmultilingualLLMswithen-
training pipeline offers substantial gains in low- hancedcapabilities(Almazroueietal.,2023;Tou-
resourcelanguages(underrepresentedintheLLM vronetal.,2023;LeScaoetal.;Weietal.,2023).
trainingdatasets)whilemaintaining(orfurtherim- BLOOM (Le Scao et al.), trained on the ROOTS
proving)performanceonhigh-resourcelanguages. corpus (Laurençon et al., 2022) that comprises
Thecontributionsofthisworkareasfollows, sourcesin46languages,marksasubstantialstep
forwardinmakingLLMsaccessibleacrossawide
• WedevelopPALO: thefirstmultilingualLarge rangeoflanguages,includingthosewithfewerre-
MultimodalModel(LMM)coveringtenmajor sources. PaLM (Chowdhery et al., 2023) show-
languages,facilitatingvision-languagereason- cases the advantages of scaling, achieving im-
ingthroughagenericmodelcapableofgener- proved results in both monolingual and multilin-
atingresponsesinanyofthetenlanguages. gualtasksthroughsophisticatedtrainingtechniques
andanovelpathwaysarchitecture.
• We assemble an extensive multilingual (10
Advancements in Large Multimodal Models
languages)instruction-tuningdataset,through
(LMMs)haveevolvedfrombasicimage-levelin-
acriticalanalysisandsubsequentrefinement
teractions (Liu et al., 2023b; Chu et al., 2023) to
ofastate-of-the-artLargeLanguageModel’s
offeringflexibilitybyfocusingonregion-specific
target language translations. This dataset is
analysis(Rasheedetal.,2023)andspatio-temporal
pivotalinimprovingproficiencyinprocessing
conversations(Maazetal.,2023;Linetal.,2023),
and generating content that is linguistically
highlightingthesignificantprogressinthisdomain.
preciseacrossmultiplelanguages.
However,theexplorationofmultilingualcapabili-
• Weenhancethemultilingualperformanceof tieshasbeenlimited. Qwen(Baietal.,2023)and
state-of-the-artLMMs(Liuetal.,2023b;Chu mPLUG-Owl(Yeetal.,2023)extendLMMfunc-
et al., 2023) across three distinct scales i.e., tionalitiestoprocessvisualinputsinbothEnglish
1.7B,7B,and13Bparameterstodemonstrate andChinese,showcasingitsadaptabilityinprocess-
the scalability of our training pipeline. The ingbilingualvisualinformation. Ziya-Visual(Lu
resulting polyglot LMMs demonstrate per- et al., 2023) demonstrates the translation of En-
formance gains on diverse language tasks glishimage-textdatasetsintoChinese,employing
withsubstantialimprovementsinunderstand- in-contextlearningforinstruction-responsegenera-
ing and generating content for low-resource tion. However,theseLMMsremainlimitedtotwo
languages, e.g., Hindi, Arabic, Bengali, languages.Figure2: ArchitectureoverviewofPALO. (left)Themodelconsistsofavisionencoderthatencodestheimage,
followedbyaprojectorthatprojectsthevisionfeaturesintotheinputembeddingspaceofthelanguagemodel.
Theuser’stextqueryistokenized,andthetokensareconcatenatedwiththevisiontokensbeforebeinginputinto
thecausallanguagemodeltogeneratetheresponse. Forthe PALO 7Band13Bvariants, Vicunaisusedasthe
Large Language Model while MobileLLaMA (Chu et al., 2023) is used as the Small Language Model in our
MobilePALO-1.7Bvariant. CLIPViT-L/336pxisusedasthevisionencoderinallvariants. (right)Projectorsusedin
differentvariantsofPALOareshown. ForthePALO7Band13B,following(Liuetal.,2023b),weuseatwo-layer
MLPprojectorwithGELUactivation. ForourmobileversionofPALO(MobilePALO-1.7B),weuseaLightweight
DownsampleProjector(LDP)from(Chuetal.,2023). Itutilizesdepth-wiseseparableconvolutionstodownsample
theimagetokens,makingitfasterthanastandardMLPprojector.
WeintroducePALO,thefirstfullyopen-source ding space of the language model. Following
LMM,offeringvisualreasoningcapabilitiesacross LLaVA (Liu et al., 2023b), we use a two-layer
ten major languages, addressing the gap in mul- MLPwithGELUactivationastheprojectorforour
tilingual LMMs. In contrast to GPT-4 (Achiam 7/13Bmodels. However,alightweightdownsam-
et al., 2023) which is closed-source and only ac- pleprojector(LDP)(Chuetal.,2023)isusedfor
cessible via APIs, ours is the largest effort in the MobilePALO-1.7Bmodel. LDPutilizesdepth-wise
open-source domain to extend LMM capabilities separableconvolutionstodownsamplethevision
tomultiplelanguages. tokens, largely reducing the input tokens to the
languagemodelandhencesignificantlyreducing
3 PALO: APolyglotLMM thetrainingandinferencetime. Further,convolu-
tionsinLDPhavefewerparametersascompared
TowardsmoregloballyaccessibleVision-Language toMLP,makingourmobilemodelbothparameter
Models(VLMs),ourmodel PALO(PolyglotLarge and compute-efficient. The projector used in the
MultimodalModel)isdesignedtocomprehendand differentPALOversionsareshowninFigure2.
generatecontentintenmajorlanguages,servingan The projected vision tokens are then concate-
audiencethatspansnearlytwo-thirdsoftheglobal natedwiththetokenizedusertextqueryandpassed
population. The architecture of PALO is derived tothelanguagemodelforgeneratingtheresponse.
from LLaVA (Large Language and Vision Assis- As PALO trains on ten languages using an exten-
tant)(Liuetal.,2023b,a)forourlarger-scalemod- sivemulti-modalinstructiontuningdataset,thisnot
els(7/13B),andfromMobileVLMforourmobile- onlyenablesmoreeffectiveutilizationofthetok-
efficientmodel(1.7B),ensuringthatPALOremains enizer’scapacitybutalsoexpandsthesearchspace,
versatileacrossdifferentcomputationalsettings. providing a richer context and more challenging
Thearchitectureseamlesslyintegratesavision examples for training. the language model. This
encoder with a language model (see Figure 2). approachsignificantlyenhancestheabilityofthe
Givenaninputimageandusertextquery,themodel modeltounderstandandgenerateresponsesacross
generatesanaccuratenaturallanguageresponse. adiversesetoflanguages.
PALOusesCLIPViT-L/14(Radfordetal.,2021) WeuseVicuna(Zhengetal.,2023)asthelarge
as the vision encoder followed by a projector languagemodel(LLM)inour7/13Bmodelsand
to transform vision tokens to the input embed- MobileLLaMA(Chuetal.,2023)asthesmalllan-guagemodel(SLM)inMobilePALO-1.7Bmodel.
Vicunafine-tunesLLaMA-2onuser-sharedconver-
sationscollectedfromShareGPT,whileLLaMA-2
ispre-trainedon2Ttokenscollectedfromdifferent
publicsources(Touvronetal.,2023). Ontheother
hand,MobileLLaMAperformspretrainingon1.3T
tokensfromRedPajama-v1(Computer,2023)fol-
lowedbyfine-tuningonapubliclyavailableversion
ofShareGPTdata(Huggingface).
3.1 Dataset
Figure3: Qualitativeresultsshowingtheimpactof
The primary contribution of our work lies in the
fine-tuning. Comparative visualization of English to
meticulouspreparationofacomprehensivemulti-
Arabictranslations before and after fine-tuningthe
lingualvision-languageinstruction-tuningdataset.
LLM. The figure shows improvements in language-
We begin by selecting a state-of-the-art LMM
specificissuessuchasaccuratevocabularyusage,gen-
model (Liu et al., 2023b) for our focus. To tai- deragreement,andgrammaticalcorrectness,highlight-
lortheinstruction-tuningdatasetmoreeffectively ingtheenhancedperformanceofthefine-tunedmodel.
formultiplelanguagesinascalableway,welever-
ageanLLMmodel(Brownetal.,2020)todevelop speakers for each language provides detailed re-
a semi-automated translation pipeline. This ap- viewandcorrectionofasmallsubsetfrominitial
proachinvolvestranslatingtheEnglishdatasetinto translations, addressing language-specific issues,
thetargetlanguages,therebycreatingarobustmul- gender accuracy, and overall linguistic integrity.
tilingualdataset,whichsignificantlybroadensthe Automated scripts are tailored for each language
linguisticscopeandapplicabilityofthemodel. tocorrectcommonpunctuationmistakesandopti-
Translation Process and Challenges: A naive mizetheverificationprocess.
translationapproachfromEnglishtothetargetlan- Fine-tuningoftheLLM:Acknowledgingthelim-
guagesusinganLLMmodel(Brownetal.,2020) itations of the LLM for multilingual translations,
effectively conveys the basic meanings but intro- weleveragemanuallyverifiedandcorrectedtrans-
ducesseverallinguisticchallengesspecifictoeach lations(1Kconversationsperlanguage)asahigh-
language. Issues such as punctuation, grammati- qualitydatasetforfine-tuningtheLLM.Thisfine-
calnuances,translationconsistencies,andgender tuningisfocusednotonlyonimprovingtranslation
usageerrorsareobservedviaadirectLLM-based accuracybutalsoonaligningtheoutputswiththe
translation(referFigure.3). Thesechallengesvary specific attributes of each language, such as tone
greatly due to the linguistic diversity of the lan- and orthography. The enhanced and fine-tuned
guages involved, from the tonal complexities of LLM is then employed to translate the extensive
Chinese to the script variances in Hindi and the VLMinstructiontuningdataset(Liuetal.,2023b)
gender-specificintricaciesoflanguageslikeSpan- comprisingapproximately150Kinstructions(i.e.
ish,ArabicandRussian. Forinstance,inthecaseof LLaVA-Instruct-150K from (Liu et al., 2023b))
Arabic,commonpunctuationmistakesinvolvein- from English into the respective languages. We
correctspacingaroundcommasandperiods. Nun- use GPT3.5-Turbo as the translation model and
nation,vitalinArabicgrammar,issometimesomit- finetuneitusingOpenAIfinetuningplatform.
ted or wrongly applied. Additionally, certain En- ImpactoftheRefinedDataset: Thisprocessre-
glish words remain untranslated in the translated sultsinacomprehensiveandhigh-qualitymultilin-
text, and there are instances where verbs are in- gualdataset,crucialfortheeffectivefine-tuningof
correctly converted to nouns alongside incorrect PALO. The improved dataset not only addresses
genderalignmentintranslationsthatposesignifi- specificaspectsofeachlanguagebutalsomarkedly
cantconcerns,giventhegender-specificnatureof improves the ability of the model to process and
grammarinsometargetlanguages. generatecontextuallyrelevantandgrammatically
AddressingtheChallenges: Toimprovethequal- accuratecontentinallincludedlanguages. Forin-
ity of the translated dataset, we employ a combi- stance,Figure3highlightstwokeyimprovements
nationofautomatedandmanualverificationsteps. inEnglishtoArabictranslation,thefirstexample
Inthissemi-automatedpipeline,ateamofnative showsenhancedlexicalprecision,andthesecondModel Eng. Chinese French Spanish Russ. Japan. Arabic Hindi Bengali Urdu Avg.H Avg.L Avg.
LLaVA-7B 67.9 55.7 62.4 64.5 55.3 59.2 38.9 29.4 13.9 21.8 60.8 26.0 46.9
PALO-7B 64.2 55.7 58.3 61.0 57.4 57.5 57.8 57.6 51.7 55.3 59.0 55.6 57.7
-3.7 0.0 -4.1 -3.5 +2.1 -1.7 +18.9 +28.2 +37.8 +33.5 -1.8 +29.6 +10.8
LLaVA-13B 69.5 62.9 67.5 64.6 62.3 65.3 37.2 27.8 20.4 22.1 65.4 26.9 49.9
PALO-13B 65.5 62.1 66.4 65.9 62.4 60.6 56.9 66.8 53.5 59.6 63.8 59.2 61.9
-4.0 -0.8 -1.1 +1.3 +0.1 -4.7 +19.7 +39.0 +33.1 +37.5 -1.5 +32.3 +12.0
MobileVLM-1.7B 46.6 23.2 28.1 29.1 28.1 26.4 12.4 13.7 15.6 15.6 30.3 14.3 23.9
MobilePALO-1.7B 48.2 34.0 42.6 40.1 38.2 32.5 32.8 26.8 19.9 24.1 39.3 25.9 33.9
+1.6 +10.8 +14.5 +11.0 +10.1 +6.1 +20.4 +13.1 +4.3 +8.5 +9.0 +11.6 +10.0
Table1: StandardVLMsvsPALOonmulti-lingualmultimodalevaluation. Thetableshowsthecomparison
ofLLaVAandMobileVLMwithPALOontenlanguagesonthespeciallyadaptedmultilingualversionofLLaVA-
Bench(In-the-Wild). LLaVA7/13BandMobileVLM-1.7Barefine-tunedonLLaVA-Instruct-665K,andPALO
isfine-tunedonLLaVA-Instruct-665KplustheLLaVA-Instruct-150Ktranslatedinalltenlanguages. Allmodels
arepretrainedonCC-595K(Liuetal.,2023b)dataset. Avg.HandAvg.Lrepresenttheaverageoverhigh-resource
(English,Chinese,French,Spanish,RussianandJapanese)andlow-resource(Arabic,Hindi,BengaliandUrdu)
languagesrespectively. Avg. representstheaverageoverallthelanguages.
shows improved grammatical concordance. Inte- 40GBA-100GPUsfor1.7/7Bvariantsand80GB
gratingthisdatasetintotheLMM’strainingprocess A-100GPUsfor13Bvariants. Themodelisopti-
isthe key toexpandingits capabilitiesto include mizedusingAdamoptimizerandcosineLRsched-
bothEnglishandnineotherlanguageseffectively. ulerwith2e-5baselearningratefortheprojector
and 2e-4 for the language model. The finetuning
4 Experiments
takes around 12 hours for 1.7B, 42 hours for 7B
andalmost76hoursforthe13Bmodel.
4.1 ImplementationDetails
SimilartotheLLaVAandMobileVLMbaselines, 4.2 High-resourcevsLow-resourceLanguages
we pretrain our models on a subset of CC3M Our work trains and evaluates on ten languages
datasetcalledCC-595K(Liuetal.,2023b). During divided into two groups, high-resource and low-
pretraining, only the projector is learned and the resource languages. English, Chinese, French,
restofthemodelcomponentsarekeptfrozen. We Spanish,RussianandJapaneseareconsideredhigh-
trainthemodelfor1epochwithanoverallbatch resourcelanguagesasthelanguagemodeltraining
size of 256 with 32 batch size per GPU on eight datacontainsareasonablenumberofsamplesfrom
A-10040GBGPUs. Themodelisoptimizedusing theselanguages. Ontheotherhand,Arabic,Hindi,
Adam optimizer and cosine LR scheduler with a BengaliandUrduarecategorizedaslow-resource
learningrateof2e-3. Thepertainingtakesaround languagesastheyareunder-representedinthelan-
1.5 hours for 1.7B, 5 hours for 7B and almost 9 guagemodeltrainingdata.
hoursforthe13Bmodel. Forexample, LLaMA-2(Touvronetal.,2023)
We fine-tune our model on a diverse instruc- pretrainingdatacontainsalmost2trilliontokens,
tion dataset comprising conversations from ten out of which 89.7% are of English and almost
languages. Specifically, 665K instructions from 1.92% is for Chinese, French, Spanish, Russian,
LLaVA-Instruct-665K(Liuetal.,2023a)areused Japanese, and 21 more similar languages. While
forEnglish,andapproximately150Kconversations the representation of Arabic, Hindi, Bengali and
fromLLaVA-Instruct-150K(Liuetal.,2023b)for Urduisnegligible. Similarly,MobileLLaMA(Chu
Chinese,French,Spanish,Russian,Japanese,Ara- etal.,2023)pretrainsonRedPajama-v1(Computer,
bic, Hindi, Bengali and Urdu, summing up to al- 2023)datasetwhichconsistofalmost1.3trillion
most2.1Minstructionsintotal. Duringfine-tuning, tokens,predominantlyEnglishtokens.
only the vision encoder is kept froze and the rest
4.3 Results
of the model is trained. Projector is fully trained
while language model is LORA (Hu et al., 2022) InevaluatingthemultilingualcapabilitiesofVLMs,
fine-tuned with α = 128. We train the model for we conduct a comprehensive evaluation across
1epochwithanoverallbatchsizeof128with16 various languages, utilizing a high-quality eval-
batchsizeperGPUoneightA-100GPUs. Weuse uation set. This set is constructed by translat-Data English Chinese French Spanish Russian Japanese Arabic Hindi Bengali Urdu Avg.
665K-English 67.9 55.7 62.4 64.5 55.3 59.2 38.9 29.4 13.9 21.8 46.9
150K-Chinese 59.3 55.0 60.0 57.0 32.9 40.5 21.2 20.3 21.7 19.3 38.7
150K-French 51.0 41.0 57.8 54.4 35.4 54.6 17.6 23.2 13.1 16.7 36.5
150K-Spanish 61.1 52.2 54.8 61.6 50.1 51.7 27.8 24.4 15.4 18.5 41.8
150K-Russian 55.2 51.1 62.2 60.6 57.8 50.9 25.3 28.2 13.6 16.7 42.2
150K-Japanese 54.5 41.1 59.2 57.6 36.1 57.6 18.0 23.6 13.3 18.4 37.9
150K-Arabic 67.8 42.9 56.4 54.7 38.4 44.7 56.0 25.7 19.4 33.4 43.9
150K-Hindi 52.2 39.1 56.8 54.0 35.0 33.4 18.4 54.1 12.8 23.8 37.9
150K-Bengali 26.4 40.2 56.0 54.5 37.3 26.0 12.8 16.3 34.8 14.0 31.8
150K-Urdu 28.9 30.6 44.6 50.1 22.5 16.0 22.1 25.5 20.9 47.7 30.9
Combined 64.2 55.7 58.3 61.0 57.4 57.5 57.8 57.6 51.7 55.3 57.7
Table2:Ablationonmulti-lingualfine-tuningdataset. Thetableshowsaneffectofperformanceontenlanguages
whenusingfine-tuningdatafromdifferentlanguages. Modelswith7Bparametersareusedforthisablation.
ing the LLaVA-Bench (In-the-Wild) (Liu et al., EnglishandChinese. Thisperformancedifference
2023b) into all target languages using GPT-4- isattributedtothelanguagemodelpretrainingdata.
Turbo (Achiam et al., 2023), with particular at- LLaMA-2istrainedon2trilliontokenswithabet-
tention to preserving linguistic authenticity and terrepresentationofhigh-resourcelanguagescom-
mitigating common issues of automated transla- paredtoMobileLLaMA,whichispredominantly
tionsthroughcarefulhumancorrection. Thebench- trainedon1.3trillionEnglishtokens.
markcomprises24diverseandchallengingimages
4.4 Ablations
fromdifferentdomains,suchasindoorandoutdoor
scenes, memes, and artwork, each with detailed Table 2 shows an ablation where we trained our
descriptionsandasetof60questionsdesignedto 7B model on 150K translated instructions from
testtheunderstandingandgeneralizationabilities eachlanguageandevaluatedallmodelsacrossall
ofthemodel. languages. Theresultsshowthatthebaselineper-
The results in Table 1 show that PALO obtains formsbetterthanthelanguage-specificfine-tuned
robustperformanceinhigh-resourcelanguages,as modelsforhigh-resourcelanguages,includingChi-
shownbythe7/13Bmodelsscoringanaverageof nese, French, Spanish, and Japanese. This is be-
59.0and63.8respectivelyacrosstheselanguages. causetheselanguageshavelessmulti-modaldata
Thisdemonstratesthatourmultilingualextension comparedtothebaseline(i.e.,theEnglishmodel
hasbeeneffectivelyintegratedwithoutcompromis- is trained on 665K instructions, while language-
ingtheoriginalcapabilitiesofthemodel. Further, specificmodelsaretrainedon150Kinstructions),
themodelshowsgoodperformanceimprovements andduetothenoisysemi-automatictranslationpro-
inlow-resourcelanguages,withaveragescoresris- cess. Conversely,thelanguage-specificfine-tuned
ingfrom26.0and26.9to55.6and59.2points,for modelsperformbetterinthecaseofArabic,Hindi,
the7Band13Bmodels,respectively. Bengali, and Urdu, as these languages are under-
represented in the LLM pretraining data. Lastly,
Theoverallperformanceacrossalltenlanguages
combined training further improves performance
alsoimproves,withthe7Bmodelachievinganav-
onlow-resourcelanguages. Further,wefoundthat
eragescoreof57.65,andthe13Bmodelreaching
increasingthequantityoftranslatedmulti-modal
61.97. Thedatareflectsthatourapproachsuccess-
trainingdataenhancesperformance. Forinstance,
fully creates a more inclusive, diverse, and high-
translatinganadditional72Kinstructionsfromthe
performingVLM,capableofhandlingthecomplex
GQA dataset (Hudson and Manning, 2019) into
landscapeofgloballanguagesinvision-language
Bengaliandtrainingwithatotalof222Kinstruc-
tasks(seeFigures4and5forqualitativeresults).
tions improves Bengali results from 34.8 to 38.3.
Ourmobilemodeldemonstratesconsistentim-
Thisstudyislimitedto150Kinstructionsforeach
provements across both high-resource and low-
languageduetoresourceconstraints.
resourcelanguages,withanoverallaveragegainof
33.9pointscomparedtotheMobileVLMbaseline
5 Conclusion
of 23.9 points. Contrary to the trend observed in
the 7/13B model, our mobile version also shows WeintroducePALO,apolyglotLLMfor5Bpeople,
improvementsinhigh-resourcelanguagessuchas coveringalmosttwo-thirdsoftheworld’spopula-Figure4: QualitativeresultsdemonstratingthemultilingualcapabilitiesofPALO. Whenpresentedwithuser
queries, the model generates accurate textual responses related to the visual content and the relevant language.
The figure highlights its ability to bridge vision and language understanding across diverse languages. In this
illustration,weexploredialoguesintwohigh-resourcelanguages—SpanishandChinese—andtwolow-resource
languages—HindiandArabic. PALOaccuratelyinterpretstheunusualaspectsofanimagefeaturingtwoindividuals
in medieval attire within a contemporary supermarket setting. The model exhibits its creative imagination in
Chinese,proposingabackstorywherethesecharactersmightbeakingandqueenfromastorybook. InHindi,PALO
demonstratesscenario-buildingbydescribingapossiblesituationthatbroughtthemedievalcoupleintothecurrent
dayastimetravellers. Atthebottom,PALOdisplaysatouchofhumourinArabic,conjuringupaplayfuldialogue
thatakingmightsay, showcasingitssubtleunderstandingofcontextandculture-specifichumour. Thisimage
effectivelyvisualizestheadvancedabilitytoprocessandgeneratecontentinmultiplelanguages,reflectinghigh
linguisticprecisionandculturalintelligence.
tion. It takes image and user text query as input trainPALOacrossthreescales(1.7B,7B,and13B)
andeffectivelyconverseinbothhigh-resourcelan- to demonstrate its generalization and scalability
guages such as English, Chinese, French, Span- across ten languages. Our codes, models, and
ish, Russian and Japanese, and low-resource lan- datasetswillbepubliclyreleased.
guages such as Arabic, Hindi, Bengali and Urdu.
Totrainourmodelontenlanguages,wetranslate 6 Limitations
150Kinstructionsintoeachlanguageusingcustom-
Thesemi-automatedtranslationprocess,whileeffi-
tailoredLLMs. Tofine-tuneanLLMonalanguage-
cient,mightnotfullygraspthedeepcontextualand
translationtask,weuse1Khuman-annotatedcon-
cultural nuances inherent to each language. This
versations for each targeted language. Our final
could impact the capability of the model to com-
modelsimultaneouslyprovidescompetencyinten
prehend and generate content with the necessary
languages and provides an overall performance
cultural depth, accuracy and precision. Addition-
improvement on vision-language evaluation. We
ally,ourselectionoftenlanguages,thoughitspansFigure 5: Qualitative results demonstrating the visual reasoning of PALO and its adeptness in multiple
languages. PALOrespondsaccuratelytovisualcontentinacontextuallyappropriatemannerforeachlanguage. We
illustrateaconversationinthreehigh-resourcelanguages—French,RussianandJapaneseandonelow-resource
language—Urdu. IntheFrenchsegment,themodelshowspracticalreasoningbysuggestingarecipethatutilizes
theavailableingredientsinthefridge, connectingvisualperceptiontoculinarysuggestions. InRussian, PALO
identifiesitemsrichinVitaminCandintheUrduexample,themodelorganizesthefridgecontentsintofoodgroups,
demonstratingitsabilitytoclassifyitemsandapplynutritionalknowledge. Thiseffectivelyhighlightsitsability
toswitchbetweenlanguageswhilemaintainingthecontextoftheconversation,reflectingitscapacitytogenerate
relevantandculturallyawarecontentinbothhigh-resourceandlow-resourcelanguages.
two-thirdsoftheglobalpopulation,stillleavesout 7 PotentialRisks
a considerable number of the world’s languages,
Theuseofsemi-automatedtranslationscouldbring
indicatingroomforfurtherexpansiontoenhance
forward potential risks tied to biases inherent in
linguisticdiversityandinclusivitywithinVLMs.
LLMs, particularly for low-resource languages.Themodelmustaccountfornuancesinvisualdata, Barham,HyungWonChung,CharlesSutton,Sebas-
such as the interpretation of cultural symbols or tianGehrmann,etal.2023. Palm: Scalinglanguage
modelingwithpathways. JournalofMachineLearn-
gestures, to prevent any misrepresentations. The
ingResearch,24(240):1–113.
interpretations of the model, influenced by these
biases,couldleadtoinaccuraciesincontextsthat XiangxiangChu,LimengQiao,XinyangLin,Shuang
Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu
areculturallysensitive. Thereisaneedtoevaluate
Zhang, Bo Zhang, Xiaolin Wei, et al. 2023. Mo-
andadoptnecessarytrainingtomitigatesuchrisks.
bilevlm: Afast,reproducibleandstrongvisionlan-
guage assistant for mobile devices. arXiv preprint
8 UseofDataandAIAssistant arXiv:2312.16886.
WeuseLLaVA-Instruct(Liuetal.,2023b)dataset, TogetherComputer.2023. Redpajama: Anopensource
recipetoreproducellamatrainingdataset.
licensed under Creative Commons Attribution
(CCA) 4.0 International, available for use in re- MartaRCosta-jussà,JamesCross,OnurÇelebi,Maha
search. Further, the use of GPT models abides Elbayad,KennethHeafield,KevinHeffernan,Elahe
Kalbassi, JaniceLam, DanielLicht, JeanMaillard,
by(OpenAI).Respectingsourcelicenseinforma-
et al. 2022. No language left behind: Scaling
tion, we will release all datasets created in this
human-centeredmachinetranslation. arXivpreprint
workunderCCA4.0Internationallicense. arXiv:2207.04672.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony
9 HumanAnnotations
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
BoyangLi,PascaleFung,andStevenHoi.2023. In-
TheLLaVA-Bench(Liuetal.,2023b)evaluation
structblip: Towardsgeneral-purposevision-language
foreachlanguageisverifiedandcorrectedbyanno-
modelswithinstructiontuning. arXiv:2305.06500.
tatorsselectedtorepresentadiversemixofgenders
DavidMEberhard,GaryFrancisSimons,andCharlesD
anddemographics. Annotatorsareprovidedwith
Fenning.2015. Ethnologue: Languagesoftheworld.
theEnglishversionalongsidethetranslatedversion.
They are given specific instructions to neutralize Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
thetoneandbiasesduringthecorrectionprocess.
WeizhuChen.2022. LoRA:Low-rankadaptationof
largelanguagemodels. InInternationalConference
onLearningRepresentations.
References
Drew A Hudson and Christopher D Manning. 2019.
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama Gqa: Anewdatasetforreal-worldvisualreasoning
Ahmad, Ilge Akkaya, Florencia Leoni Aleman, andcompositionalquestionanswering. InProceed-
DiogoAlmeida,JankoAltenschmidt,SamAltman, ingsoftheIEEE/CVFconferenceoncomputervision
ShyamalAnadkat,etal.2023. Gpt-4technicalreport. andpatternrecognition,pages6700–6709.
arXivpreprintarXiv:2303.08774.
Huggingface. Huggingface dataset. https:
EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAl- //huggingface.co/datasets/Aeala/ShareGPT_
shamsi, Alessandro Cappelli, Ruxandra Cojocaru, Vicuna_unfiltered.
Mérouane Debbah, Étienne Goffinet, Daniel Hess-
Hugo Laurençon, Lucile Saulnier, Thomas Wang,
low, Julien Launay, Quentin Malartic, et al. 2023.
ChristopherAkiki,AlbertVillanovadelMoral,Teven
The falcon series of open language models. arXiv
LeScao, LeandroVonWerra, ChenghaoMou, Ed-
preprintarXiv:2311.16867.
uardoGonzálezPonferrada,HuuNguyen,etal.2022.
Thebigsciencerootscorpus: A1.6tbcompositemul-
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,
tilingual dataset. Advances in Neural Information
SinanTan, PengWang, JunyangLin, ChangZhou,
ProcessingSystems,35:31809–31826.
andJingrenZhou.2023. Qwen-vl: Afrontierlarge
vision-languagemodelwithversatileabilities. arXiv Teven Le Scao, Angela Fan, Christopher Akiki, El-
preprintarXiv:2308.12966. lie Pavlick, Suzana Ilic, Daniel Hesslow, Ro-
manCastagné,AlexandraSashaLuccioni,François
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Yvon, Matthias Gallé, et al. Bloom: A 176b-
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind parameteropen-accessmultilinguallanguagemodel.
Neelakantan,PranavShyam,GirishSastry,Amanda corr, abs/2211.05100, 2022. doi: 10.48550. arXiv
Askell,etal.2020. Languagemodelsarefew-shot preprintarXiv.2211.05100.
learners. Advancesinneuralinformationprocessing
systems,33:1877–1901. BinLin,BinZhu,YangYe,MunanNing,PengJin,and
LiYuan.2023. Video-llava: Learningunitedvisual
AakankshaChowdhery,SharanNarang,JacobDevlin, representationbyalignmentbeforeprojection. arXiv
MaartenBosma,GauravMishra,AdamRoberts,Paul preprintarXiv:2311.10122.Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae DeyaoZhu, JunChen, XiaoqianShen, XiangLi, and
Lee.2023a. Improvedbaselineswithvisualinstruc- MohamedElhoseiny.2023. Minigpt-4: Enhancing
tiontuning. arXiv:2310.03744. vision-languageunderstandingwithadvancedlarge
languagemodels. arXiv:2304.10592.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
Lee.2023b. Visualinstructiontuning. InNeurIPS.
Junyu Lu, Dixiang Zhang, Xiaojun Wu, Xinyu Gao,
Ruyi Gan, Jiaxing Zhang, Yan Song, and Pingjian
Zhang. 2023. Ziya-visual: Bilingual large vision-
language model via multi-task instruction tuning.
arXive-prints,pagesarXiv–2310.
Muhammad Maaz, Hanoona Rasheed, Salman Khan,
andFahadShahbazKhan.2023. Video-chatgpt: To-
wardsdetailedvideounderstandingvialargevision
andlanguagemodels. arXiv:2306.05424.
OpenAI. Openaitermsofuse. https://openai.com/
policies/terms-of-use.
AlecRadford,JongWookKim,ChrisHallacy,Aditya
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
etal.2021. Learningtransferablevisualmodelsfrom
naturallanguagesupervision.
HanoonaRasheed,MuhammadMaaz,SahalShaji,Ab-
delrahmanShaker,SalmanKhan,HishamCholakkal,
RaoM.Anwer,EricXing,Ming-HsuanYang,and
FahadS.Khan.2023. Glamm: Pixelgroundinglarge
multimodalmodel. ArXiv2311.03356.
Teven Le Scao, Thomas Wang, Daniel Hesslow, Lu-
cile Saulnier, Stas Bekman, M Saiful Bari, Stella
Biderman,HadyElsahar,NiklasMuennighoff,Jason
Phang, et al. 2022. What language model to train
ifyouhaveonemilliongpuhours? arXivpreprint
arXiv:2210.15424.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.
XiangpengWei,HaoranWei,HuanLin,TianhaoLi,Pei
Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei
Cao, Binbin Xie, et al. 2023. Polylm: An open
sourcepolyglotlargelanguagemodel. arXivpreprint
arXiv:2307.06018.
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,
Ming Yan, Yiyang Zhou, Junyang Wang, An-
wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.
mplug-owl: Modularization empowers large lan-
guage models with multimodality. arXiv preprint
arXiv:2304.14178.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
ZhuohanLi,DachengLi,Eric.PXing,HaoZhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judg-
ingllm-as-a-judgewithmt-benchandchatbotarena.
arXiv:2306.05685.