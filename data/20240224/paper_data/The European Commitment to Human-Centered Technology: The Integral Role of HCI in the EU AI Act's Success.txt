The European Commitment to Human-Centered
Technology: The Integral Role of HCI in the EU AI
Act’s Success
Andr´e Calero Valdez, Moreen Heine, Thomas Franke,
Nicole Jochems, Hans-Christian Jetter, Tim Schrills
University of Lu¨beck
Institute of Multimedia and Interactive Systems,
23562 Lu¨beck
Germany
Abstract
The evolution of AI is set to profoundly reshape the future. The European
Union, recognizing this impending prominence, has enacted the AI Act, regulating
market access for AI-based systems.
A salient feature of the Act is to guard democratic and humanistic values by
focusing regulation on transparency, explainability, and the human ability to un-
derstand and control AI systems. Hereby, the EU AI Act does not merely specify
technological requirements for AI systems. The EU issues a democratic call for
human-centered AI systems and, in turn, an interdisciplinary research agenda for
human-centered innovation in AI development.
Without robust methods to assess AI systems and their effect on individuals
and society, the EU AI Act may lead to repeating the mistakes of the General Data
Protection Regulation of the EU and to rushed, chaotic, ad-hoc, and ambiguous
implementation, causing more confusion than lending guidance.
Moreover, determinedresearchactivities inHuman-AIinteraction willbepivotal
for both regulatory compliance and theadvancement of AI in a manner that is both
ethical and effective. Such an approach will ensure that AI development aligns
with human values and needs, fostering a technology landscape that is innovative,
responsible, and an integral part of our society.
1 The increasing importance of AI
Intherapidlyevolvinglandscapeofartificialintelligence(AI),theEuropeanUnion’s
AI Act emerges as a pioneering legislative framework, aiming to safeguard human
values and ensure the safe utilization of AI technologies. This legislative initiative
categorizes AI systems into four distinct risk levels, with each category subject to
specific compliance criteria. However, the operationalization and implementation of
these criteria present significant challenges, underscoring the need for a meticulous
examination of the Act’s provisions and their practical implications.
1
4202
beF
22
]CH.sc[
1v82741.2042:viXra2 Calero Valdez et al.
The objective of the present paper is to critically analyze the EU’s AI Act, fo-
cusing on the ambiguities and challenges inherent in operationalizing and assessing
the compliance criteria. It delves into the intricacies of the certification processes,
shedding light on the uncertainties surrounding the authorities responsible for ver-
ification and the methodologies employed for assessment. The EU AI Act’s inter-
pretative challenges, particularly concerning provisions related to human oversight,
safety, and transparency, are scrutinized, highlighting the subjective natureof these
criteria and the potential inconsistencies in their application and enforcement.
Furthermore,thepresentpaperunderscoresthepivotalroleofHuman-Computer
Interaction(HCI)inthecontextoftheAIAct. Itpositsthattherequirementsforhu-
manoversight, safety, andtransparencyareintrinsically linkedtohumanperception
and interpretation of AI, thereby placing HCI at the forefront of AI development.
The paper argues for the integration of HCI principles in the development of AI
systems, emphasizing that fulfilling the Act’s criteria necessitates a comprehensive
understanding of human limitations and an adherence to fundamental HCI values.
In summary, our present paper provides a comprehensive examination of the
EU’s AI Act. First, we highlight the challenges in operationalizing its criteria.
Second,wediscusstheambiguitiesincertificationandverificationprocesses. Finally,
we describe why HCI plays a crucial role in shaping the future of AI development.
Through our analysis, we seek to contribute to the ongoing discourse on responsible
AI and emphasize the imperative of aligning AI technologies with human-centric
values and societal norms. The field of HCI must start to address these challenges
in the next five years to remain in the loop in decision-making about the use of AI.
2 The EU AI Act
The European regulation of Artificial Intelligence (AI) aims to strengthen research
and industry while simultaneously ensuring safety and fundamental rights by focus-
ing on excellence and trust. The legal text is currently still in the final stages of the
legislative process. However, the resulting activities in the standardization process
are not yet finalized. The formulation of the regulations is deliberately abstract.
This is intended to ensure flexibility, broad applicability, adaptability to changes,
and the avoidance of legal loopholes.
The AI act relies on risk-based regulations, which have to be specified through
codes of practice. These codes are developed in cooperation between industry,
academia, civil society, and the commission, supported by independent scientific
expert panels responsible for classifying and reviewing AI models and issuing risk
warnings. A new AI Office within the European Commission will ensure the de-
velopment of AI policy at the European level and monitor the execution of the
forthcoming AI Act. To promote innovation and test practical applications, AI
regulation also includes experimental clauses and the establishment of regulatory
sandboxes. These allow AI systems to be tested under real conditions in controlled
environments without neglecting regulatory standards.
2.1 Risk classes in the AI Act
The legislative framework classifies safety-critical areas into four distinct categories,
each with its own set of regulatory requirements.
The AI Act presents a structured framework for regulating AI applications by
categorizing them into different risk levels, namely 1) minimal, 2) high, 3) unaccept-
able, and 4) specific transparency risks. This categorization informs the regulatoryThe European Commitment to Human-Centered Technology 3
requirements necessary to achieve compliance.
AI applications classified under the unacceptable risk category encapsulate tech-
nologies perceived as antithetical to societal values and individual liberties. This
includes,butisnotlimitedto, systemslikesocialcreditscoring,emotion recognition
in workplaces and educational settings, AI designed to exploit vulnerabilities such
as age or disabilities, and technologies capable of behavioral manipulation or sub-
version of free will. Moreover, this category encompasses the untargeted collection
of facial imagery in public domains, public space facial recognition, biometric cat-
egorization mechanisms, selective predictive policing (with certain exclusions), and
the tightly constrained use of real-time biometric identification in law enforcement
contexts.
Conversely, the high-risk category is reserved for AI systems with a substan-
tial propensity to inflict harm on human safety, health, environmental integrity, or
property. This broad category encompasses a diverse array of applications ranging
from medical devices, vehicular technologies, human resource management systems,
educational tools, mechanisms influencing electoral behaviors, to the management
of critical infrastructures, and tools employed in law enforcement sectors.
Pertaining to high-risk AI, the regulations stipulate a series of stringent com-
pliance measures. These encompass conducting fundamental rights and conformity
assessments, mandatory registration in a designated public EU database, and the
institution of both risk and quality management systems. A paramount require-
ment for these AI systems is adherence to strict data governance protocols aimed
at bias mitigation and ensuring data representativeness. Transparency is a criti-
cal mandate, necessitating the provision of lucid documentation and instructional
materials. Furthermore, human oversight is a crucial requirement, mandating the
availability of auditable logs and, potentially, system explainability. These systems
are also obligated to meet elevated standards of accuracy, robustness, and cyberse-
curity, underscored by consistent testing and monitoring practices.
Systems that pose little or no risk to the rights or security of citizens fall into
the minimal risk category, for example, AI-enabled video games or spam filters.
2.1.1 General purpose AI
In an exceptional category, General Purpose AI, especially those employing founda-
tionmodels,isdistinguishedduetoitsinherentriskfactors,necessitatingaugmented
transparency and comprehensive disclosure protocols.
For foundation models, characterized by a significant investment in computa-
25
tional effort (exceeding 10 FLOPS), there are additional transparency require-
ments encompassing technical documentation, training data oversight, and pro-
tective measures for intellectual property rights. Foundation models that carry
systemic risks, exemplified by platforms such as ChatGPT, are subject to more
stringent measures, including model evaluations, supplementary risk assessments,
adversarial testing, and the establishment of incident reporting mechanisms. All
content and interactions generated by generative AI must be explicitly labeled and
be discernible to human users.
2.2 Enforcement and compliance
To ensure regulatory compliance, the AI Act imposes substantial penalties for
breaches. Entities may incur fines up to 7% of their global turnover or €35 mil-
lion for the deployment of AI in unacceptable applications and up to 3% of global4 Calero Valdez et al.
turnover or €15 million for other infractions, with tailored provisions for SMEs and
startups.
The enforcement framework is comprised of an EU AI office, an AI board, provi-
sionsforindividualcomplaints, andtheestablishmentofmarketsurveillanceauthor-
ities within member states. This comprehensive apparatus is designed to monitor
andregulateAIapplications acrosstheentirespectrumofassessedrisklevels, ensur-
ing aharmonized andeffective governance of AI technologies within thejurisdiction.
2.3 Transparency and human oversight in regulations
A particular focus of the AI Act is on transparency and human oversight. These
are important criteria as they tie the regulation to those most affected by any regu-
lation - humans. However, the terminology applied here is (partially) unexpectedly
deviation from current research in explainable ai (XAI). Terms such as explainabil-
ity, interpretability or understandability have been a key research interest in recent
years. However, they inadvertently tie the underlying quality (explainability) to
human (subjective) evaluation, as in “Does the application explain the decision to
me?” or“DoIunderstandthedecision?”. Transparency,ontheotherhand,isaterm
associated with physical properties of objects and alludes a more objective interpre-
tation of the desired quality of AI systems. This allows a technical interpretation
of transparency, as in, “Can we find/define a measurement tool that demonstrates
transparency?” Human oversight or controllability—as used in the regulation—are
muchclosertohumanevaluation andtieintoexistingwell-establishedresearchareas
in HCI and human factors research. As both these aspects are key aspects of the
regulation we must assume that the goals of the AI Act are intended to be not only
a mere technical interpretation of the aforementioned goal criteria. Nevertheless,
these still underdefinedconcepts beg the question, how can we attain these criteria?
3 There is no reliable AI regulation without a
sound theory of human-AI interaction
AmajorconcernwiththeAIActisthatmanytermsandconceptsarenotsufficiently
detailed, since sound theoretical models to describe human-AI interaction are still
in development. For example, the AI Act requires high-risk systems to provide
information about an AI system’s accuracy and limitations, but it does not specify
how detailed this information must be or what methods can be used. With fines
roughly twice as high as the GDPR, developers of AI systems are demanding less
ambiguity on their obligations to comply with the law.
But what is needed for the AI Act to be an impetus for innovation rather than
ambiguity? Following Lewin’s maxim that “there is nothing as practical as a good
theory”[23],adeeptheoretical understandingofhumaninteraction withAIsystems
is needed to realize the potential of regulation as a driver of innovation. Thus, to
support a successful environment for AI development, the HCI community must
incorporate existing theoretical concepts from psychological science, especially on
human action regulation and experience in technology-rich/artificial environments
(i.e., engineering psychology), into education of developers and users, research, and
product development.The European Commitment to Human-Centered Technology 5
3.1 Humans do more than just using AI systems
In its most recent version, the EU AI Act aims to “promote the uptake of human-
centric and trustworthy artificial intelligence” [16]. It also requires from high-risk
systemstoenablehumanoversight[16],bysupportingusersinunderstandingtheAI
system, staying aware of potential biases, and allowing them to correctly interpret
AIsystems’output. TheEUAIActthusalsocalls onusas thescientific community
(and general public) to view people not just as the executing instance who operates
a technical system, but as an independent, responsible entity with requirements to
responsibly handle the information processing tasks at hand.
First, when human users aim to interact with an AI system, they assess how the
system processes information, i.e., develop aninformation processingawareness [32].
Whilewedonotwanttodiscussthenotionof(situation)awarenessindetailhere,its
im important to highlight, that is a construct about a state of the human user [15].
In comparison to existing constructs in research on AI systems, for example, the
Explanation Satisfaction Scale (ESS, [19]), the concept of situation awareness ad-
dresses how AI interaction changes human perception and behavior. The ESS aims
to evaluate system properties, but is, in the end, focused on the system and not its
user. However, we postulate that users regulate their actions—for example, when
making a decision—based on the before-mentioned information processing aware-
ness. That is, the extent to which users perceive, understand, and predict an AI
system’s information processing [32] is the basis for their ability to oversee this
system.
Second, many AI systems are and probably also will be developed for tasks or
processes, that have been carried out by human operators (which is, after all, the
definition of automation [27]). On top of that, depending on the field they are
developed for, their integration into human tasks will potentially be limited [16].
A fruitful approach could therefore be to take human action control as a starting
point and to view the gradual integration of automated information processing as
an adaptation of this action control. This also allows us to address crucial scientific
and ethical questions: at what point is control over the action so far removed from
humansthattheycannolongerorshouldnolongerbeabletotake responsibilityfor
it? At what interaction points in a shared task can human oversight be established
by improving human awareness of information processing?
Third, in light of the AI Act, it is immensely important to note, that human
users of AI systems may approach these systems with biases and a limited ability
to cognitively process all presented information. That is, the perception of risk may
be different when users have a sufficient feeling of control compared to situations
wherethey did not feel in control [35]. In addition to this, huge amounts of informa-
tion may convince users to positively assess outcomes even without verifying their
accuracy [17]. When humans are modeled as rational, perfect users of AI systems,
such biases and ergonomic requirements will not be mitigated through AI systems,
but potentially enhanced—or in the worst case abused.
Allinall,theEUAIActisacalltofocusonadeepunderstandingofhumanusers
of AI - including potential human biases, emotional, motivational, and cognitive
states of human users, and, ultimately, the conditions under which human oversight
and responsibility are possible—or not.6 Calero Valdez et al.
3.2 AI systems constitute automated information pro-
cessing
With regard to the definition of AI systems in the AI Act as well as in the OECD
definition[42],animportantaspectofthesesystemsisthattheyprocessinformation
autonomously and their results can be decisive for human action regulation. This
characterizes AI systems as types of automation of information processing, that are
characterized by autonomy and connection to human actions. Hence, to discuss
AI systems’ effect on human users, they can be anchored in theoretical concepts of
automation.
Key considerations include the suitability of automation in various tasks and
how their design influences human activities. Sound models on how the ergonomic
design of AI systems differs from less automated systems are essential for designing
AI: Only by understanding users’ models, AI developers can support users’ ability
to operate a system successfully. Only sufficiently precise models of human reaction
to AI systems are the basis for developers to “trustworthy AI”, ensuring that users’
expectation ofsystemperformanceandtheirreality arenotopposed. Achievingthis
balance of detailed understanding and abstraction is critical to avoid over-reliance,
also discussed as complacency in the broader automation literature [41]. However,
the integration of AI systems into humaninformation processingcan fail when their
utilization is hampered by (unwarranted) mistrust [5]. We can assume that AI
systems are not only experienced differently depending on their design, but may
actively change how human users approach a task [40]. That is, the integration of
AIintohumanworkmodifieshowusersregulatetheiractionsandwhichinformation
they use to make decisions, requiring careful consideration of how humans control
and interact with automated systems.
The rich literature on signal detection and alarm systems, for example, can
provide insights for AI design, particularly in ensuring effective communication of
criticalinformationtoenhancesafetyandperformance[26]. ViewingAIthroughthe
lens of automation allows for the use of empirical research to develop technologies
that can augment human capabilities. On top of that, they allow predictions about
the AI systems’ influenceon human performance, accountability, and human ability
to exert control [31].
3.3 AI systems can have different levels of automation
Tocharacterizediversityinautomatedsystems,theconceptof“levelsofautomation”
has been well-established and extensively discussed in human factors and engineer-
ing psychology for decades. This perspective allows for the classification of different
levels of autonomy in systems, explicitly balancing the level of control retained by
human users against the autonomous actions of the system. For example, as one
of the most-cited approaches, Parasuraman et al., formulate ten different levels of
automation, ranging from manual operation to full autonomy. Their work provides
a nuanced approach to understanding the extent of function allocation and to some
degree to the structure of interaction between humans and automated systems [29].
Conceptualizing AI systems in “levels of automation” frameworks may enable
developers and regulators to dissect the complexity of AI systems, such as distin-
guishing between recommender systems and decision support systems, which may
have different levels of automation and consequently different implications for user
decision-making and information processing (see [39]). That is, levels of automa-
tion can be a framework that supports developers to understand how AI systems
can be improved. Contrary to a risk classification, levels of automation can beThe European Commitment to Human-Centered Technology 7
associated with empirically tested solutions instead of legally required documenta-
tion. It is important to note, that the level of automation should not be as high as
possible, but fit the context of an AI’s deployment and be designed to enable op-
timal joint human-AI performance. For example, Miller argues that recommender
systems (possibly representing higher levels of automation) may hamper the user’s
ability to make decisions in comparison to evaluative AI, which provides informa-
tion [24]. Understanding these distinctions is important to address accountability
and control within AI systems, aspects that are closely aligned with the objectives
of the EU AI Act to promote transparency, security, and users’ rights in the use of
AI technologies [16].
Hence, in research, education, and professional practice, characterizing the spe-
cificlevelofautomationofanAIsystembecomesafundamentalquestion. Questions
such as “what level of automation does my AI system possess?” and “what level
of control and autonomy is granted to the user?” are essential to ensuring that AI
systems are designed, implemented, and governed in an ethical, user-centered, and
compliant manner. Thus, it is a crucial task for HCI research to revisit research on
human-automation interaction and apply it to the design, study, and development
of AI systems. In this process, this will fill the concepts of the AI Act with life.
3.4 Research is only as reliable as its ability to develop
theory
In the face of the EU AI Act, HCI research not necessarily needs to create new
theories, tools, ormethods,aspreviousresearchonhumanreaction toautomation is
highlytransferable. Problemsidentifiedinautomationresearch, suchasopacity and
lack of transparency, illustrate how existing frameworks can guide the development
of more transparent and user-friendly AI. However, AI introduces new challenges
that require the application of past research for effective solutions. For example,
while the concept of Situation Awareness has been used in research on automated
systems, morespecialized concepts likeInformationProcessingAwareness canprove
useful to explicate and examine the challenges of AI systems [32].
All in all, the key challenge for HCI research was, is, and remains to develop
theories on howusersconstructmental models of AIsystems [21], andhow feedback
and learning influence such models. Research on AI has demonstrated, that users
might be convinced of a system without understanding it [8]. Existing research on
explainability pitfalls or even dark patterns in explainability [13] demonstrate: Sys-
tems containing more complex information processing can be a barrier for efficient
feedback and learning of users in comparison to existing, automated systems and
explanations could even be weaponized to convince users of an AI system’s accu-
racy. In addition, improving how diagnostic information is selected and presented
to users may be more complicated in data-driven models [2], demonstrated by a
rising number of causal research in AI systems [20].
In addition, refining collaborative actions between humans and AI based on
interdependence and shared experience is critical to advancing human-centered AI.
With a significantly increasing number of input parameters, interaction points, and
feedback channels (e.g., through anthropomorphic features), the influence user and
system exertover each other whilecooperatively engaginginataskincreases as well.
Theincreasingcouplingofhumansandsystemsduringtheprocessingofinformation
must be modeled more precisely in theory.8 Calero Valdez et al.
4 There is no trustworthy AI without HCI
The requirements for AI systems set out by the EU AI Act reflect an important per-
spective of the safety of AI systems: a human-centered view, that calls for empirical
research on humans’ reaction to AI systems. It is opposite to a purely technical
view, defining concepts such as the trustworthiness or robustness of AI systems
without integrating the human factor. The AI Act, striving to ensure the safety
of AI systems in terms of their controllability and transparency by governmental,
organizational, or natural individuals, raises questions about existing methods in
AI systems, e.g.: how do explanations [2] affect an individual’s ability to detect
errors? Which training can prevent complacency of novices or experts, and which
information do users need to provide oversight?
Already in 2020, the EU issued the European Committee for Standardization
(CEN, Comit´e Europ´een deNormalisation) to providestandards for trustworthy AI.
This standardization process is a crucial element of the EU’s approach on opera-
tionalizing the EU AI Act. The EU’s institutions took responsibility for defining
terms, that are still highly debated in HCI research: trustworthiness, controllability,
or human oversight are just a few examples. It is promising, for example, that the
detectability of a system’s state and a user’s influence on changing the state, are
central to defining what “controllability” of systems is (see for example [25]). How-
ever, it is the HCI community, that needs to provide tools, methods, and—most
important—replicable and theory-driven research to support the development of
human-centered AI.
Historically, HCI research has successfully mastered comparable challenges, e.g.,
when faced with disruptive technologies such as Personal Computers and the World
Wide Web. By understanding and formalizing formerly hard-to-grasp and fuzzy
concepts such as “user friendliness”,“usability”, or later “user experience”, HCI has
not only helped to establish industry standards for testing and improving complex
human-centered qualities of systems but also has arrived at a core insight about
their nature: Unlike the technological parameters of a system such as storage size,
response times, or energy consumption, human-centered qualities do not reside in
a system but emerge from how specified users are using a system for achieving
their goals in their specified context of use [9, 10]. Without understanding and
observing real users in real-world contexts, the design and the assessment of the
human-centered qualities of a system are bound to fail. In light of the coming AI
revolution, it is now even more important that HCI faces that challenge with the
same openness for—and insistence on—involving real-world users and considering
theirmanydifferentpracticesandcontextsofuseforachievingtrulyhuman-centered
technologies.
In the next five years, students, researchers, and practitioners of HCI need to
familiarize themselves with a new regulatory framework and contribute to its con-
cretization. Here, we identify three core challenges for HCI research in the coming
years: (1) developing metrics to evaluate human-AI interaction and its compliance
with EU legislation, thereby enabling a continuous improvement of the AI Act,
standards and regulatory monitoring; (2) conducting theory-driven research on in-
struction for users of AI systems and education fostering human abilities to control
AI appropriately; and (3), the (continuous) development of multimodal designs and
interaction patterns that enable developers to comply with the AI Act and support
ethical values of human-centered technology.The European Commitment to Human-Centered Technology 9
4.1 HCI must define and disseminate methods to eval-
uate human-centered AI
The scientific discourse around Human-Centered AI demonstrates how complex it
is to define and operationalize related concepts, such as trustworthiness or control
(see [36]). While EU legislation will not end the scientific discourse, it highlights
the importance of providing reliable theories and methods to designers, developers,
and deployers of AI systems. That is, two years after the AI Act comes into effect,
deployers ofAIsystems(includinggovernmental, educational, andindustrialactors)
need to beable to prove compliance. Thatis, a reliable and testable definition of AI
compliant with the AI Act needs to be established, and with it, adequate methods
to assess AI systems’ trustworthiness.
Previous, empirical research on trust has demonstrated that high levels of accu-
racy of AI systems alone are not sufficient to provide trustworthy AI systems [3].
In order to develop reliable, trustworthy AI systems, developers must examine how
users experience AI systems and how human-AI interaction may lead to compla-
cency or unwarranted caution [34]. Merely requesting explanations to be presented
by AI systems or explainable AI (XAI) will not suffice. Undesired effects of ex-
planations by XAI-system can happen, as has been demonstrated before [14, 32].
Unsurprisingly,thewayXAIcanimprovehumanusageofAIsystemsisanemerging
research topic [24], which is however still in its infancy. The effect of explanations
should therefore be well understood and—as stated in the AI Act—used in appro-
priate situations. Conversely, explanations should be applied with caution if they
are qualified to limit human control (e.g., through information overload [17]).
But what can HCI research do, given that requirements of trustworthy AI may
be highly dependent on task, context, or even individual user characteristics? In
the next years, the role of HCI will shift because core values of human-centered
design—namely controllability and human oversight—are central to the AI act.
Human-centered design has evolved from a requirement to ensure the safety of
automated systems to the requirement of European Legislation itself. Accordingly,
HCI must provide methods that are not only effective but reliable, transparent, and
able to serve in certification processes (see also [34]). In summary, HCI methods
will move beyond research and development as they will become integral tools of
EU legislation.
4.2 HCI must explore and quantify the impact of edu-
cation and instruction on users of AI
It can be assumed that, in addition to comprehensive documentation, the appropri-
ate presentation of information—in the form of a manual—will be key to meeting
the requirements of the AI Act. However, recent HCI research of AI systems, espe-
cially XAI systems, demonstrated a focus on visual or textual information, which
accompanies specific decisions (local methods, e.g., [30]), explains the model in
general (global methods, e.g., [38]), or aims to integrate both approaches (glocal
methods, e.g., [1]). Considering limited human capabilities to process information
and to understand algorithms, HCI research needs to examine how to train and
prepare users to be sufficiently qualified to exert control over AI systems and take
responsibility. That is, controllability is not only dependent on the transparency
and design of an AI system but also on an individual’s ability to utilize the given
information.
HCI research has two paths to support users’ ability to exert control over AI10 Calero Valdez et al.
systems and take appropriate responsibility for results: improving methods for edu-
cation andexamininghowtoinstructAIusers. Fortheimprovementofeducationin
AI systems, existing approaches like simulations [33] and scenario-based design [37]
must be tailored to challenges accompanying AI systems, e.g., high volumes of data
and continuous learning and adoption. Furthermore, learnings from the field of au-
tomation need to be adopted, e.g., how mental models of users can be improved to
avoid breakdowns[4]suchas thefirstfailureeffect[28]. ButhowcanHCIresearch—
and respectively EU authorities—define the requirements for users before they can
(legally) operate high-risk AI systems? As this question will shape the future of
work, HCI research needs to design valid educational approaches, addressing users
with diverse characteristics [18].
4.3 HCI must develop solutions for human-centered AI
Data-driven methods of Machine Learning present developers with more challenges
when it comes to transparency, traceability, and controllability (see [2]). Accord-
ingly, the HCI community has engaged in the development of techniques to provide
these features, even in systems utilizing deep learning or Large Language Models.
Forexample,thevisualizationofweightsofinputparametersviaShapleyValues[38]
can supportusers in identifying (undesired) biases in data-driven models. The visu-
alization of Shapley values, however, can be done in various ways (see [11]). Which
visualization is the correct one, given a human’s task to identify non-compliant
behavior? Which visualization may convince human users of a system’s fairness,
but does not support them in detecting unfair decisions? Adequate presentations
of machine learning models for a huge diversity of users will require creative and
human-centered design processes in the coming years.
Ontop of that, whilemany approaches to supporthuman-centered AI are visual
or textual, the HCI community needs to put its experience of interactions to work.
When aiming for high levels of user controllability, enriching AI interaction with
additional information is not enough. For example, the control of generative AI
systems can be highly customized through a variety of slider-based options [12],
but potentially overburdens users. Especially in the context of general purpose
models,designinghowuserscancontroltheirfunctionality, willbeacentralresearch
topic. The ergonomics of the systems will depend on how well the complexity of
the underlying models can be abstracted and users can be given control over them
in (sequential) interaction. Existing studies show that even small changes in the
sequence of interaction can lead to changes in human behavior [40].
5 There is no community without common
language and communication
Thelast large-scale regulation by the EU—the GDRP—caused tremendous changes
in worldwide software applications. While the intention of the GDPR was to pre-
serve privacy and allow control over data flows for end-users, most notably, the
introduction of cookie banners on websites was the most visible effect of the reg-
ulation. Additionally, this change has furthermore induced other unintended ill
consequences such as the use of dark design patterns in Cookie consent forms [22].
Privacy disclaimers must bethe most frequently presented and also most frequently
ignored legal speech in existence, causing an economic burden of 2.3 billion USD
annually [7].The European Commitment to Human-Centered Technology 11
While cookie banners and their atrocious usability were obviously not the in-
tended outcome of the GDPR, unintended consequences of large-scale regulations—
such as the AI Act—loom around the corner. Weaponized AI explanations, pro
forma certification, or shifting the burden of liability to end users, are just a few ex-
amples of what could go wrong and threaten the success of the AI Act. So, how can
we prevent a GDPR disaster 2.0? Who is capable of preventing similar unintended
malicious consequences and what tools will they need?
For the AI Act to become successful, the intended outcomes and the actual
outcomes must align. And as shown above, the intended outcome of the regulation
is to safeguard human values and ensure the safe utilization of AI. Measuring and
ascertaining these outcomes cannot be conducted by engineering sciences and HCI
1 2
alone. Other disciplines in SHAPE and STEM will provide insight into what to
safeguard and into how to safeguard these values. Thus, it is critical to include
other disciplines in this discourse and to provide a means of understanding what
this discourse is about. However, discourse about AI can only be non-superficial
and meaningful when the language used adequately represents the technological
intricacies of the topic. Thus, a transdisciplinary language of AI is needed.
From an interdisciplinary perspective, different fields utilize different paradigms,
methods, and theories with partial overlap. The fish-scale model of interdisciplinar-
ity by Campbell [6] posits that each discipline covers a certain aspect of our reality
and that by overlapping different disciplines a full pictureof reality may emerge. In
this metaphor, HCI is the scale that overlaps with both the scale of engineering and
the scale of social sciences and humanities. Thus, the responsibility of explanation
is on the field of HCI. We as a community must demonstrate that our insights are
able to translate between technological applications and value-oriented human in-
teraction. We must translate and connect the quantitative engineering approaches
to making AI transparent (e.g., Shapley values, LIME, or SHAP) with the qual-
itative meaning-seeking processes of the social sciences and humanities, that try
to understand what transparency is for a heterogeneous group of individuals and
communities. HCI is uniquely suited to be both a translator and an ambassador for
human-centeredAItohelpboththeengineeringsciencescreatemoretransparentAI,
and the humanities and social sciences to measure and reflect on the consequences
of wide-scale AI utilization. To fulfill this role, we must pick our obligations to
demonstrate that we will be able to do these roles justice. These obligations could
be concrete research goals aligned with demands from the AI Act. We must pick
the most pressing research questions, even if we will only know how to answer them
in five to ten years from now, we already do know that we should answer them
now. For example, should we as a field be able to demonstrate that the mandate
for human control—as issued by the AI Act—is fulfilled by a certain user interface
to AI-based systems? Otherwise, considerations of ethical and social implications
are doomed to remain dry runs in hypothetical scenarios.
6 Conclusion: Navigating the future of AI and
HCI within the EU AI Act framework
As we stand on the brink of a new era in Artificial Intelligence (AI) and Human-
Computer Interaction (HCI), it is imperative to recognize the intertwined future of
thesedisciplines underthelegislative umbrellaof theEUAIAct. Thepresentpaper
1
SHAPE - Social sciences, Humanities, and the Arts for People and the Economy
2
STEM - Science, Technology, Engineering, Mathematics12 Calero Valdez et al.
explored the multifaceted implications and preconditions of the EU AI Act, empha-
sizing the pivotal role of HCI in fostering systems that are not only technologically
advanced but also ethically aligned and human-centric.
The blooming landscape of AI, characterized by rapid advancements and inte-
gration into everyday life, demands a reevaluation of the role of the field of HCI.
That is, HCI must evolve from its traditional boundaries to address the complex-
ities of how AI systems are intertwined with humans, with technology becoming
increasingly autonomous inits information processingandaction regulation yetstill
intimately linked to human activities. Tackling this (co-)evolution of a new human-
technology relationship necessitates a collaborative, interdisciplinary approach that
merges technical advances on the frontier of XAI with insights from a deep under-
standing of the psychology of Human-AI interaction closely integrated with broad
perspective from social sciences, ethics, and law.
In light of the EU AI Act, this consequently underscores the necessity for HCI
to be at the forefront of designing AI systems that prioritize transparency, account-
ability, and inclusivity. That is, the Act’s regulatory framework provides a unique
opportunity for HCI researchers and practitioners to lead the development of stan-
dards and methodologies that ensure AI systems are understandable and beneficial
to all segments of society.
Thefuture challenges and opportunities for HCI within this framework are man-
ifold. They include crafting interfaces that enable meaningful human oversight,
developing evaluation methodologies that encompass ethical considerations, and en-
suring that AI systems enhance rather than diminish human capabilities. Further-
more,theroleofHCIineducationandpublicengagementiscriticalfordemystifying
AI technologies and fostering a society that is informed, prepared, and optimistic
about the AI-driven future.
We conclude with a call to action for the HCI community to proactively engage
with the challenges posed by the AI revolution. By embracing the principles of
human-centered design, interdisciplinary collaboration, and ethical responsibility,
HCI can lead the way in ensuring that AI technologies are developed and deployed
in a manner that truly benefits humanity. In doing so, HCI will not only respondto
the immediate demands of the EU AI Act but also shape the long-term trajectory
of AI and HCI for a future where technology serves to amplify human potential and
societal well-being.
References
[1] Achtibat, R., Dreyer, M., Eisenbraun, I., Bosse, S., Wiegand, T., Samek, W.,
and Lapuschkin, S. (2023). From attribution maps to human-understandable ex-
planations through concept relevance propagation. Nature Machine Intelligence,
5(9):1006–1019.
[2] Adadi, A. and Berrada, M. (2018). Peeking inside the black-box: a survey on
explainable artificial intelligence (xai). IEEE access, 6:52138–52160.
[3] Bansal, G., Nushi, B., Kamar, E., Lasecki, W. S., Weld, D. S., and Horvitz,
E. (2019). Beyond accuracy: The role of mental models in human-AI team
performance. In Proceedings of the AAAI conference on human computation and
crowdsourcing, volume 7, pages 2–11.
[4] Benner, D., Elshan, E., Scho¨bel, S., and Janson, A. (2021). What do you
mean? a review on recovery strategies to overcome conversational breakdownsThe European Commitment to Human-Centered Technology 13
of conversational agents. In International Conference on Information Systems
(ICIS), pages 1–17.
[5] Brauner, P., Philipsen, R., Calero Valdez, A., and Ziefle, M. (2019). What hap-
pens when decision support systems fail?—the importance of usability on perfor-
mance in erroneous systems. Behaviour & Information Technology, 38(12):1225–
1242.
[6] Campbell, D. T. (2017). Ethnocentrism of disciplines and the fish-scale model
of omniscience. In Interdisciplinary relationships in the social sciences, pages
328–348. Routledge.
[7] Castro, D.andMcQuinn,A.(2014). Theeconomiccosts oftheeuropeanunion’s
cookie notification policy. The Information Technology and Innovation Founda-
tion, pages 1–11.
[8] Chromik,M., Eiband,M., Buchner, F., Kru¨ger,A., and Butz, A.(2021). I think
I get your point, AI! the illusion of explanatory depth in explainable AI. In 26th
International Conference on Intelligent User Interfaces, pages 307–317.
[9] Cockton, G. (2004). Value-centred HCI. In Proceedings of the third Nordic
conference on Human-computer interaction, pages 149–160.
[10] Cockton, G. (2006). Designing worth is worth designing. In Proceedings of
the 4th Nordic conference on Human-computer interaction: changing roles, pages
165–174.
[11] Cooper,A.(2021). Explainingmachinelearningmodels: Anon-technical guide
to interpreting shap analyses.
[12] Dang, H., Mecke, L., and Buschek, D. (2022). Ganslider: How users control
generative models for images usingmultiple sliders with and without feedforward
information. In Proceedings of the 2022 CHI Conference on Human Factors in
Computing Systems, pages 1–15.
[13] Ehsan, U. and Riedl, M. O. (2021). Explainability pitfalls: Beyond dark pat-
terns in explainable ai. arXiv preprint arXiv:2109.12480.
[14] Eiband, M., Buschek, D., Kremer, A., and Hussmann, H. (2019). The impact
of placebic explanations on trust in intelligent systems. In Extended abstracts of
the 2019 CHI conference on human factors in computing systems, pages 1–6.
[15] Endsley, M. R. (1995). Toward a theory of situation awareness in dynamic
systems. Human factors, 37(1):32–64.
[16] EuropeanCommission(2024). ArtifcialIntelligence Act.REGULATIONofthe
Europeanparliamentandofthecouncillayingdownharmonisedrulesonartificial
intelligence(artificialintelligenceact)andamendingcertainunionlegislative acts,
eur-lex - 52021pc0206 - en - eur-lex.
[17] Ferguson, A. N., Franklin, M., and Lagnado, D. (2022). Explanations that
backfire: Explainable artificial intelligence can cause information overload. In
Proceedings of the Annual Meeting of the Cognitive Science Society, volume 44.14 Calero Valdez et al.
[18] Franke,T.,Attig, C.,andWessel, D.(2019). Apersonalresourcefortechnology
interaction: development and validation of the affinity for technology interaction
(ATI) scale. International Journal of Human–Computer Interaction, 35(6):456–
467.
[19] Hoffman, R. R., Mueller, S. T., Klein, G., and Litman, J. (2023). Measures for
explainable AI:Explanation goodness, user satisfaction, mental models, curiosity,
trust, and human-ai performance. Frontiers in Computer Science, 5:1096257.
[20] Holzinger, A., Malle, B., Saranti, A., and Pfeifer, B. (2021). Towards multi-
modal causability with graph neural networks enabling information fusion for
explainable AI. Information Fusion, 71:28–37.
[21] Johnson-Laird,P.N.(2010). Mental modelsandhumanreasoning. Proceedings
of the National Academy of Sciences, 107(43):18243–18250.
[22] Krisam,C., Dietmann, H., Volkamer, M., andKulyk,O.(2021). Dark patterns
in the wild: Review of cookie disclaimer designs on top 500 German websites. In
Proceedings of the 2021 European Symposium on Usable Security, pages 1–8.
[23] Lewin, K. (1943). Psychology and the process of group living. The Journal of
Social Psychology, 17(1):113–131.
[24] Miller, T. (2023). Explainable AI is dead, long live explainable AI! hypothesis-
driven decision support using evaluative ai. In Proceedings of the 2023 ACM
Conference on Fairness, Accountability, and Transparency, pages 333–342.
[25] Norman, D. A. (1986). Cognitive engineering. User centered system design,
31(61):2.
[26] Onnasch,L.,Ruff,S.,andManzey, D.(2014a). Operators’adaptationtoimper-
fect automation–impact of miss-prone alarm systems on attention allocation and
performance. International Journal of Human-Computer Studies, 72(10-11):772–
782.
[27] Onnasch, L., Wickens, C. D., Li, H., and Manzey, D. (2014b). Human per-
formance consequences of stages and levels of automation: An integrated meta-
analysis. Human factors, 56(3):476–488.
[28] Parasuraman, R., Molloy, R., and Singh, I. L. (1993). Performance conse-
quences of automation-induced ’complacency’. The International Journal of Avi-
ation Psychology, 3(1):1–23.
[29] Parasuraman, R., Sheridan, T. B., and Wickens, C. D. (2000). A model for
types and levels of human interaction with automation. IEEE Transactions on
systems, man, and cybernetics-Part A: Systems and Humans, 30(3):286–297.
[30] Ribeiro, M. T., Singh, S., and Guestrin, C. (2016). “Why should I trust you?”
Explaining the predictions of any classifier. In Proceedings of the 22nd ACM
SIGKDD international conference on knowledge discovery and data mining,pages
1135–1144.
[31] Ro¨ttger, S., Bali, K., and Manzey, D. (2009). Impact of automated decision
aidsonperformance,operatorbehaviourandworkloadinasimulatedsupervisory
control task. Ergonomics, 52(5):512–523.The European Commitment to Human-Centered Technology 15
[32] Schrills, T. and Franke, T. (2023). How do users experience traceability of AI
systems? examining subjective information processing awareness in automated
insulin delivery (AID) systems. ACM Transactions on Interactive Intelligent
Systems, 13(4):1–34.
[33] Schrills, T., Gruner, M., Peuscher, H., and Franke, T. (2023). Safe environ-
ments to understand medical AI-designing a diabetes simulation interface for
users of automated insulin delivery. In International Conference on Human-
Computer Interaction, pages 306–328. Springer.
[34] Shneiderman, B. (2020). Human-centered artificial intelligence: Reliable,
safe & trustworthy. International Journal of Human–Computer Interaction,
36(6):495–504.
[35] Sj¨oberg, L., Moen, B.-E., and Rundmo, T. (2004). Explaining risk perception.
Anevaluation of the psychometric paradigm inrisk perception research, 10(2):665–
612.
[36] Speith, T. (2022). A review of taxonomies of explainable artificial intelligence
(XAI) methods. In Proceedings of the 2022 ACM Conference on Fairness, Ac-
countability, and Transparency, pages 2239–2250.
[37] Sun, J., Liao, Q. V., Muller, M., Agarwal, M., Houde, S., Talamadupula, K.,
and Weisz, J. D. (2022). Investigating explainability of generative AI for code
through scenario-based design. In 27th International Conference on Intelligent
User Interfaces, pages 212–228.
[38] Sundararajan, M. and Najmi, A. (2020). The many Shapley values for model
explanation. In International conference on machine learning, pages 9269–9278.
PMLR.
[39] Tatasciore, M., Bowden, V. K., Visser, T. A., and Loft, S. (2022). Should we
just let the machines do it? The benefit and cost of action recommendation and
action implementation automation. Human Factors, 64(7):1121–1136.
[40] van Dongen, K. and van Maanen, P.-P. (2013). A framework for explaining
reliance on decision aids. International Journal of Human-Computer Studies,
71(4):410–424.
[41] Wickens, C. D., Clegg, B. A., Vieane, A. Z., and Sebok, A. L. (2015). Compla-
cency and automation bias in the use of imperfect automation. Human factors,
57(5):728–739.
[42] Yeung, K. (2020). Recommendation of the council on artificial intelligence
(OECD). Int. Leg. Mater., 59(1):27–34.