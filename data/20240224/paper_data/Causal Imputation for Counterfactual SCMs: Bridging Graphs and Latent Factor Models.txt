Causal Imputation for Counterfactual SCMs:
Bridging Graphs and Latent Factor Models
ÁlvaroRibot* ARIBOTBARRADO@G.HARVARD.EDU
SchoolofEngineeringandAppliedSciences,HarvardUniversity,andCFIS,UPC
ChandlerSquires CSQUIRES@MIT.EDU
LaboratoryforInformationandDecisionSystems,MIT,andBroadInstituteofMITandHarvard
CarolineUhler CUHLER@MIT.EDU
LaboratoryforInformationandDecisionSystems,MIT,andBroadInstituteofMITandHarvard
Abstract
Weconsiderthetaskofcausalimputation,whereweaimtopredicttheoutcomesofsomesetof
actionsacrossawiderangeofpossiblecontexts. Asarunningexample,weconsiderpredicting
howdifferentdrugsaffectcellsfromdifferentcelltypes. Westudytheindex-onlysetting,wherethe
actionsandcontextsarecategoricalvariableswithafinitenumberofpossiblevalues. Eveninthis
simplesetting,apracticalchallengearises,sinceoftenonlyasmallsubsetofpossibleaction-context
pairshavebeenstudied. Thus,modelsmustextrapolatetonovelaction-contextpairs,whichcanbe
framedasaformofmatrixcompletionwithrowsindexedbyactions,columnsindexedbycontexts,
and matrix entries corresponding to outcomes. We introduce a novel SCM-based model class,
wheretheoutcomeisexpressedasacounterfactual,actionsareexpressedasinterventionsonan
instrumentalvariable,andcontextsaredefinedbasedontheinitialstateofthesystem. Weshowthat,
underalinearityassumption,thissetupinducesalatentfactormodeloverthematrixofoutcomes,
withanadditionalfixedeffectterm. Toperformcausalpredictionbasedonthismodelclass,we
introduce simple extension to the Synthetic Interventions estimator (Agarwal et al., 2020). We
evaluateseveralmatrixcompletionapproachesonthePRISMdrugrepurposingdataset,showing
thatourmethodoutperformsallotherconsideredmatrixcompletionapproaches.
Keywords: Causalimputation,latentfactormodels,syntheticinterventions,matrixcompletion
1. Introduction
Acoregoalinscientificmodeling–oftenleftimplicit–istoconstructmodelsthataccuratelypredict
asystem’sbehavioracrossawiderangeofconditions. Moreprecisely,weconsiderthefollowing
generalcausalpredictionproblem:
WeknowsomecontextC,i.e.,somepartialinformationaboutthesystem’sstate.
WeareconsideringwhethertoperformanactionAthatwillaffectthesystem.
WewishtopredictsomeoutcomeY,i.e. somefeature(s)ofthesystemafterperformingA.
Ingeneral,causalpredictionmayinvolvehigh-dimensionalcontexts,actions,and/oroutcomes,
suchasimagesortext(Chalupkaetal.,2015;Castroetal.,2020;Federetal.,2022). Inthiswork,we
studycausalpredictionintheindex-onlysetting1,whereactionstakevaluesin[m] := {1,...,m}
*Á.Ribot’scontributionstothisworkweremadewhilevisitingMITIDSSandwhileaffiliatedwithCentredeFormació
InterdisciplinàriaSuperior(CFIS)-UniversitatPolitècnicadeCatalunya(UPC).
1.This setting is similar to the tabular setting in reinforcement learning; we choose to use the term index-only to
emphasizethelackofanyadditionalstructure.
© Á.Ribot,C.Squires&C.Uhler.
4202
beF
22
]LM.tats[
1v77741.2042:viXraRIBOTSQUIRESUHLER

 
Y 11 Y 12 ? ··· Y 1n  

 ? ? Y 23 ··· Y 2n   
  
Y = Y 31 ? Y 33 ··· ?  actionsi ∈ [m]
 
  . . . . . . . . . . . . . . .      

Y ? ? ··· ?  
m1
(cid:124) (cid:123)(cid:122) (cid:125)
contextsj ∈ [n]
Figure1: Thecausalmatrixcompletionproblem. EachrowofY correspondstoanaction,and
eachcolumncorrespondstoacontext. Y denotestheoutcomeafterperformingactioni
ij
incontextj. Werepresentmissingentrieswith"?".
and contexts take values in [n] := {1,...,n}, with the values i ∈ [m] and j ∈ [n] carrying no
semanticmeaning. Inparticular,theonlyinformationavailableaboutanaction/contextisanindex
which distinguishes it from other actions/contexts; there is no prior notion of similarity between
actions/contexts. Toemphasizeoursetting,wewillwriteI insteadofAandI insteadofC. By
A C
restrictingourfocustotheindex-onlysetting,weaimtomaximizetheclarityofthiswork,while
buildingasolidfoundationforfutureworks.
Asarunningexampleofcausalprediction,weconsidertheproblemofviabilitypredictionin
biology, an essential step in tasks such as drug repurposing (Wu et al., 2022). In this problem,
we are given a cell type j ∈ [n] (e.g., skin or lung) and a drug label i ∈ [m] (e.g. tyloxapol or
gepefrine),andweaimtopredictwhatproportionY ofcellswillsurviveifweadministerdrugitoa
largegroupoftype-j cells. Wenotethatexistingdatabases(Southanetal.,2013)offerextensive
additionalinformationaboutdrugs,whichcanbeusedwhenextendingbeyondtheindex-onlysetting
consideredhere. Thus,thepredictionaccuracyinoursettingshouldbeseenasalowerboundonthe
accuracythatcouldbeattainedbyleveragingthisadditionalinformation.
Togeneratecausalpredictions,weneedsomemodelspecifyinghowanactionI interactswith
A
thecontextI toproducetheoutcomeY. Instatisticsandmachinelearning,amodelisobtained
C
viadata-drivenapproaches, whichconsiderafixedmodelclassΘandusedatatoselectamodel
θ(cid:98)fromΘ(calledlearningormodelfitting). Inthiswork,weconsidersupervised learning,where
the available data consists of samples of (I ,I ,Y). For large m and n, the available data often
A C
containssamplesfromonlyasmallsubsetΩofallpossiblem·npossibleaction-contextpairs. For
example, given n ≈ 100 cell types and m ≈ 10,000 drugs, we have m·n ≈ 1 million, but our
datasetmightcoveronly5-10%ofthesepairs. Byarrangingtheavailabledataintoamatrixwith
rowsindexedbyactionsandcolumnsindexedbycontexts,weobtainapartially-observedmatrixY,
asinFig.1. Inthesesituations,causalpredictionrequiresextrapolatingfromΩtotheentirespace
[m]×[n],aproblemknownascausalimputation(Squiresetal.,2022).
Forextrapolationtobefeasible,themodelclassΘmustencodesomeinductivebiasesgoverning
the interplay between actions, contexts, and outcomes. The model class must also be reasonably
well-specified,i.e.,theremustexistsomeθ∗ ∈ Θwhichaccuratelydescribestherelationshipbetween
Y,I ,andI ,atleasttoagoodapproximation. GiventheimportanceofthemodelclassΘ,this
A C
2CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
I A = i u i Y ij v j I C = j
Figure2: Thelatentfactormodel(LFM)writtenasasimplestructuralcausalmodel(SCM).
LFMsassumethatY = ⟨u ,v ⟩+ε foru ,v ∈ Rr. Thisgenerativeprocesscanbe
ij i i ij i i
viewedasanSCMover3observed(shaded)and2latent(unshaded)variables. Here,I
A
andI representactionandcontextindices,respectively(notethattheyareindependent).
C
work aims to advance our understanding of the relationship between two model classes that are
commonly used for causal prediction. In particular, we study the class of latent factors models
(LFMs)andtheclassofstructuralcausalmodels(SCMs).
LFMs, also known as interactive fixed effects models, are widely used in econometrics and
recommendationsystems(Atheyetal.,2021;Korenetal.,2009). InanLFM,eachactioni ∈ [m]is
associatedwithanunknownvectoru ∈ Rr,andeachcontextj ∈ [n]isassociatedwithanunknown
i
vectorv ∈ Rr. LFMsassumeY = ⟨u ,v ⟩+ε forsomemean-zeroε ,andcanbeseenasa
j ij i j ij ij
simpleformofSCM,seeFig.2. Wecanalsoextendthesemodelstoincludetermsforafixedaction
effect and/or a fixed context effect by letting Y = ⟨v ,u ⟩+α +β +ε for some α ,β ∈ R.
ij i j i j ij i j
LFMsarerelatedtolow-rankfactorizations: lettingL ∈ Rm×n withL = E[Y ],thedefinitionof
ij ij
theLFM(withnofixedeffects)impliesthatrank(L) ≤ r.
LFMsareoftenassumedasamodelclasswithoutany“deeper”justification. Oneintuitiveway
to motivate LFMs is to show that they arise from other (potentially more transparent) modeling
assumptions,asinUdellandTownsend(2017). RelatingLFMstoothermodelclassesisimportantfor
severalreasons: suchconnectionsofferinsightsthatcouldlegitimizetrustinthemodel’sprediction,
andcanserveasastartingpointfromwhichtodevelopmoregeneralmodelclasses. Thus,aprimary
aim of this work is to offer a new justification for LFMs, starting from the assumption that the
system’sstateobeysastructuralcausalmodel(SCM),withY definedasacounterfactualoutcome
ij
under action i when the system is in context j. That is, in this paper, we work with quantities
thataredefinedintermsofacounterfactualdistribution,andwhichcannotbedefinedonlyusing
interventionaldistributions. Suchquantitiesarecommonlyseen,forexample,intheliteratureon
mediationanalysis(Malinskyetal.,2019).
Finally,notethatthereisasubstantialdifferencebetweentherolesthatcolumns(contexts)and
rows (actions) are playing. Inspired by this distinction, let us call a matrix completion approach
symmetricwhen,performedonY⊤,weobtainthesamepredictions. Wewanttouseanon-symmetric
approachthatiscompatiblewithourcausalviewpoint.
Organizationofthepaperandcontributions. AfteranoverviewofrelatedworkinSection2,we
reviewSCMsandformallydefineourmodelclassinSection3. Wespecializetolinearmodelsin
Section4,whereweestablishthefollowingresults:
• WegeneralizethemodelclassfromSquiresetal.(2022)toallowforthecontextI tobedefined
C
intermsofthesystem’sstate,whichwasnotpreviouslyallowed. InTheorem1,weshowthatour
newSCM-basedmodelclassimpliesanLFMwithfixedactioneffects.
• WeproposeaslightmodificationtotheSyntheticInterventionsestimatorofAgarwaletal.(2021)
thataccountsforfixedactioneffects.
3RIBOTSQUIRESUHLER
• Wegiveadditionalconditionsoncontextsandactions,whichimplyfurtherstructureonL;see
Prop.2 and Corollary3. We show how this additional structure can be leveraged to improve
estimationandhypothesistesting.
Finally,inSection5,wecomparetheperformanceofseveralcausalpredictionapproachesonthe
PRISMdrugrepurposingdataset,showingthatourmethodoutperformsalternativeapproaches.
2. RelatedWork
Since this paper bridges between latent factor models and structural causal models, the potential
scopeforrelatedworkisvast. Welimitourdiscussiontoworksthatconsidertheindex-onlysetting.
These works are complementary to recent works which give extrapolation guarantees for more
general/structuredactionspaces,suchasSaengkyongametal.(2023)andAgarwaletal.(2023). A
summaryofthealgorithmspresentedcanbefoundinTable1. Weprovidetheformulaeforthese
algorithmsinAppendixA.Finally,recallthatL ∈ Rm×n withL = E[Y ]andthatΩdenotesthe
ij ij
setofobservedaction-contextpairs.
LocalApproachestoMatrixCompletion. Wecallacausalpredictionapproachlocalifitpredicts
individualentriesL ,ratherthanpredictingtheentirematrixLinonego. Manylocalapproaches
ij
(cid:80)
predictL viasomeweightedaverage w Y ,withdifferentchoicesforw givingriseto
ij (k,ℓ)∈Ω kℓ kℓ kℓ
differentmethods. Forexample,inCollaborativeFiltering(CF)(Schaferetal.,2007;Lindenetal.,
2003)andnearest-neighborsapproaches(Dwivedietal.,2022),theweightw dependsonsome
kℓ
measureofdata-dependentsimilarity(e.g. cosinesimilarity)betweenactionsi,k andcontextsj,ℓ.
Very simple examples of local, weighting-based estimators include the mean-over-contexts
estimator (which has w ∝ 1 with the weights summing to one), and the similarly-defined
i′j′ i′=i
mean-over-actionsestimator. Theestimatorsareappropriateformodelswithonlyfixedactioneffects
and fixed context effects, respectively. To handle a two-way fixed effect model (one with both
fixedactionandcontexteffects),themean-over-actionsandmean-over-contextsestimatorscanbe
combinedintotheFixedEffects(FE)estimator. Moreprecisely,thisestimatorcorrespondstotaking
anaverageofthefixedactioneffectestimatorspresentedinSquiresetal.(2022).
Connections between causal prediction and matrix completion are explored in a number of
works. Inparticular,thewidely-usedSyntheticControlsmethodpredictsoutcomesfortreatedunits
underthecounterfactualsettingwheretheyreceivednotreatment(AbadieandGardeazabal,2003;
DoudchenkoandImbens,2016;BaiandNg,2019). Agarwaletal.(2020)generalizedthisideato
predictcounterfactualsundertreatmentwiththeSyntheticInterventions(SI)estimator,whichhas
alsobeenconnectedtocausalmatrix/tensorcompletion(Agarwaletal.,2021;Squiresetal.,2022).
GlobalApproachestoMatrixCompletion. Otherapproaches,suchasnuclearnormminimization
(NNM) (Candes and Plan, 2009), predict the entire matrix L in one go, often by casting matrix
completionasanoptimizationproblemanddevelopingfastoptimizers(Caietal.,2008;Candesand
Recht,2008;Mazumderetal.,2010). Infact,therearealsoglobalapproachesthattakeadvantage
oflatentfactormodels(Hastieetal.,2014;Jainetal.,2012). Thereisavastliteraturestudyingthe
optimalitypropertiesofNNManditsvariations(CandesandTao,2009;Rechtetal.,2010;Zhangand
Aeron,2015). However,theseresultsrelyontheassumptionofuniformlyatrandomobservations,
whichisusuallynotsatisfiedinrealdatasets,especiallyforbiologicalapplications(seeSection5).
Thus,wemightprefertouselocalapproacheswhenthemissingnesspatternisfarfromuniform.
4CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
Algorithm ModelClass Symmetric Dependenceon|Ω|
Mean-Over-Contexts FixedActionEffectModel ✗ Low
Mean-Over-Actions FixedContextEffectModel ✗ Low
FixedEffects(FE) Two-wayFixedEffectModel ✓ Low
CollaborativeFiltering(CF) MixtureModel ✗ Medium
SyntheticInterventions(SI) LatentFactorModel ✓* Medium
SI-mean-contexts LatentFactorModel ✗ Low
SI-FE LatentFactorModel ✓* Low
NuclearNormMin. (NNM) LowRank ✓ High
NNM-FE LowRank ✓ Low
Table1: Summary of the algorithms. Model Class indicates that the algorithm is consistent
whenthetruedata-generatingprocessbelongstothatclass,e.g.,SyntheticInterventionsis
consistentunderafactormodel. Dependenceon|Ω|(thenumberofobservedentries)isa
qualitativejudgementbasedontheperformanceresultsshowninSection5.
*SIissymmetricformatricesbutitisnon-symmetricforthird-ordertensors.
Among global approaches, the one most related to the present work is an extension of NNM
byAtheyetal.(2021). Thisapproachexcludesfixedeffectsfromregularization,hencewecallit
NNM-FE. Roughly speaking, if we let YˆFE denote the fixed effect estimator for Y, NNM-FE is
similartousingNNMonthematrixY−YˆFE.
InSection4,weusethesameideatoimprovetheSI
estimatorwhenthemodelclassincludesfixedeffects.
Causal Prediction in Biological Applications. We use the DepMap PRISM dataset (Corsello
etal.,2020),whichmeasurestheviabilityscoreofdrugandcelllinecombinations. Radhakrishnan
etal.(2022)usedadditionalinformationfromtheConnectivityMap(CMAP)dataset(Corselloetal.,
2020)topredicttheviabilityscoresfromDepMap,goingbeyondtheindex-onlysettingconsidered
here. In the index-only setting, Squires et al. (2022) used Synthetic Interventions for the CMAP
datasetwhile(Hodosetal.,2018)usedanearest-neighborapproach.
3. BackgroundandSetup
Webeginbyreviewingrelevantbackgroundonstructuralcausalmodels(Petersetal.,2017),and
thenusetheconceptstodefinethemodelclassthatweconsiderforcausalprediction.
3.1. Background
Definition1(StructuralCausalModel(SCM)) Astructuralcausalmodel(SCM)isdefinedbya
tupleS = (S,PE),whereS = (S ,...,S )isanindexedsetofq causalmechanisms
1 q
S : Z = f (pa(Z ),ε ), k = 1,...,q.
k k k k k
Here,pa(Z k) ⊆ {Z 1,...,Z q}\{Z k}arecalledtheparentsofZ k,andPE = Pε1,...,εq isthejoint
distributionoftheexogenousnoisevariablesε ,...,ε . ThecausalgraphG ofanSCMhasadirected
1 q
edgeZ → Z forallk andforallZ ∈ pa(Z ). WeassumethatG isacyclic,i.e.,itisaDAG.
l k l k
5RIBOTSQUIRESUHLER
Unlessotherwisenoted,weassumethatPE isaproductdistribution,i.e.,thatthenoisevariables
ε ,...,ε arejointlyindependent. AnSCMiscalledGaussianifε ∼ N(0,σ2)withσ > 0 ∀k.
1 q k k k
Itiscalledlinear ifallcausalmechanismsf arelinear. Inparticular,inalinearSCM,thecausal
k
mechanismsaredefinedbysomeparametersB ∈ Randtheycanbewrittenas
l,k
(cid:88)
Z = B Z +ε forallk = 1,...,q (1)
k l,k l k
Z∈pa(Z )
l k
Wenowdefineinterventionsandcounterfactuals,mostlyfollowingthenotationofPetersetal.
(2017). FixanSCMS = (S,PE)overnodesZ. A(soft)intervention2I isdefinedbyasetT(I) ⊆ Z
ofinterventiontargets,andanindexedset{fI} ofinterventionalcausalmechanisms,where
k k∈T(I)
f isgenerallyallowedtobeafunctionofpa(Z )andε . GivenaninterventionI,wedefinethe
k k k
interventionalSCM asS = (SI,PE),where(SI) = S ifk ∈ T(I)and(SI) = S otherwise.
I k k k k
On the other hand, letting C ⊆ Z and conditioning on C = c, we define the counterfactual
SCM as S = (S,PE|C=c). To combine counterfactuals and interventions (in that order), we
C=c
define S = (SI,PE|C=c). This SCM entails a new joint distribution over Z, which we
[C=c,I]
denote by P(Z | [C = c,I]), or in the special case of a do-intervention setting A ⊂ Z to a,
P(Z | [C = c,do(A = a)]).
3.2. AnSCM-basedModelClassforCausalPrediction
Wegivegeneralresultsforthesettingwhereweobserveavector-valuedoutcomeY ∈ Rp foreach
ij
action-contextpair(i,j). Thisgivesrisetoapartiallyobserved3-ordertensorY ∈ Rm×n×p where
therows(firstindex)correspondtoactionsandthecolumns(secondindex)correspondtocontexts.
LetΩdenotethesetofindices(i,j)forwhichwehaveobservedY ∈ Rp. Ourultimategoalisto
ij
imputethemissingentriesofE[Y]. Inthespecialcasep = 1,Y reducestoanm×nmatrix,and
tensorcompletionreducestomatrixcompletion.
We assume that there is an underlying SCM over some Z = (Z ,...,Z ) that defines our
1 q
system(eachpossiblecontext). LetI andI beindexvariablesthatdefinetheactionandcontext,
A C
respectively. WeaddthemtoourSCMasfollows. First,weobserveI beforeapplyinganyaction.
C
AsI definesacontext,itisdefinedasafunctionofZ,soitisdownstreamfromeverynode. Then,
C
conditioningonI = j correspondstoconditioningonsomeZ whereC ⊂ [q] := {1,...,q}.
C Cj j
On the other hand, the index I can be thought of as an instrumental variable (Newey and
A
Powell, 2003) or a regime indicator (Dawid, 2021), which encodes the (unknown) intervention
thateachactionapplies. Inparticular,usingactionicorrespondstosettingI = i,whichinduces
A
an intervention on some set of variables Z , A ⊂ [q]. Together, the intervention on I and
Ai i A
conditioning on I gives rise a new SCM, and defines a new set of variables Z(i) that represent
C
the counterfactual state of the system.3 Finally, we only observe a subset of Z(i), which we
denote with some indices X ⊂ [q], |X| = p, and the outcome Y is drawn from the distribution
ij
2.Theterminologyfordifferentclassesofinterventionsisfairlyinconsistent.Softinterventionshavemanyothernames,
e.g., parametric interventions (Eberhardt and Scheines, 2007) or mechanism shifts (Tian and Pearl, 2001). Our
definitionforsoftinterventionscontainsperfect(hard)interventionsanddo-interventionsasspecialcases,see(Squires
andUhler,2022)foradditionaltermsfortheseclasses.
3.ThisnotationisinspiredbySingleWorldInterventionGraphs(SWIGs)(RichardsonandRobins,2013).Indeed,we
wouldobtainaSWIGafterperformingdo-interventions. However,wealsoconsidersoftinterventions. Wecould
writeI =∅fordefiningthecontrolstate,i.e.noaction.InthatcaseZ(∅)=Z.
A
6CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
Pre-intervention Post-intervention
𝑍 𝜀 𝑍 (𝑖)
1 1 1
𝑍 𝜀 𝑍 (𝑖)
2 2 2
𝑍 𝜀 𝑍 (𝑖)
3 3 3
𝐼 =𝑗 𝐼 =𝑖
𝐶 𝐴
𝑍 𝜀 𝑍 (𝑖)
4 4 4
Context Action
⋮ ⋮ ⋮
𝑍 𝜀 𝑍 (𝑖)
𝑞 𝑞 𝑞
Figure3: DAGdefiningourmodelclass. Shadednodesareobserved,whileunshadednodesare
unobserved. DataisgeneratedbyconditioningonthecontextindexI = j (indicatedby
C
thedouble-edgeforthenodeI )followedbyinterveningtosettheactionindexI = i
C A
(indicated by the square node I ). The context may potentially depend on any subset
A
ofZandtheactionmaypotentiallyaffectanysubsetofZ. Theexogenousnoiseterms
ε ,ε ,...,ε are shared between the pre-interventional and post-interventional SCMs.
1 2 q
Here,theobservedoutcomeisY ∼ P(Z (i) | I = j)forX = {2,4}.
ij X C
P(Z (i) | I = j). This definition of our model class is summarized in Fig.3. Altogether, the
X C
expectedvalueofthe(i,j)-thentryofLis
E(Y ) = E(Z (i) | I = j) (2)
ij X C
Weclarifyoursetupwithasimpleexamplewhichusesonlydo-interventions.
Example1 LetZ → Z → Z followanSCMS = (S,PE). Assumethatcontextisdefinedsolely
1 2 3
basedonthevalueofthemarkergeneZ ,e.g.,I = 1ifandonlyifZ = c andI = 2ifandonly
3 C 3 1 C
if Z
3
= c 2. This gives us a new distribution on the noise, PE|Z3=cj for each possible c j. Assume
also that each action corresponds to a do-intervention on the gene Z , e.g. if I = 1, then we
1 A
haveperformedtheinterventiondo(Z = a ),ifI = 2,thenwehaveperformedtheintervention
1 1 A
do(Z
1
= a 2). SoforI
A
= iwehaveanewsetofstructuralequationsSdo(Z1=ai). Therefore,the
SCM after conditioning and intervening is S
[Z3=cj,do(Z1=ai)]
= (Sdo(Z1=ai)),PE|Z3=cj). Finally,
assumethatweuseanassaywhichmeasuresonlythegeneZ . Thenourmatrixsatisfies
2
E(Y ) = E(Z | [Z = c ,do(Z = a )])
ij 2 3 j 1 i
where we use the notation [Z = c ,do(Z = a )] to emphasize that the order of the terms is
3 j 1 i
important: the intervention is considered in the model obtained after conditioning. Using our
notation,showninEquation(2),wewouldwriteE(Y ) = E(Z (a ) | Z = c ).
ij 2 i 3 j
7RIBOTSQUIRESUHLER
3.3. SyntheticInterventions
TheSyntheticInterventions(SI)estimator(Agarwaletal.,2020)isalocalcausalpredictionapproach
forlatentfactormodels. TopredictY for(i,j) ̸∈ Ω,SIfollowsthefollowingprocedure: (1)take
ij
the set of columns C(i) for which we have observed row i, (2) take all rows for which we have
observedcolumnsC(i)∪{j},(3)fitalinearregressionontheavailabledatadefinedbythesesubsets,
and(4)usethelinearregressioncoefficienttopredictY asalinearcombinationof{Y } .
ij iℓ ℓ∈C(i)
Notethat,analogously,wecoulduseSIregressingalongcolumns. SeeAppendixAformoredetails.
OneofthemainadvantagesofusingSIisthat,incontrastofNNM,itdoesnotrequiresignificant
assumptionsonthemissingnessstructureofthedata. Instead,toprovetheoreticalguarantees,e.g.
finite-sampleconsistency(Agarwaletal.,2020),thedatashouldbegeneratedfromalatentfactor
model,andalsosatisfyalinearspaninclusionassumption,namelythatv ∈ span(v : k ∈ C(i)).
j k
Intuitively, if C(i) is sufficiently large in relation to the rank of our factor model, SI provides a
consistentestimator.
4. TheoreticalResults
Inthissection,weestablishexpressionsfortheexpectedvalueoftheentriesofourtensor/matrix
Y. Toconnecttheseexpressionstospecifictensorcompletionapproaches,wewillcallanestimator
consistent foranexpressionifitreturnsE[Y ]whengivenasinputtheexpectedvaluesforeach
ij
observed entry, i.e. the values E[Y ] for (u,v) ∈ Ω. This corresponds to taking a limit, when
uv
eachobservedentryistheaverageofK independentsamplesofY ∼ P ,andwelet
uv IC=v;do(IA=u)
K → ∞. Tosimplifynotation,foranyv ∈ Rd,wedefineaug(v) = (1,v ,...,v )⊤ ∈ Rd+1 asthe
1 d
vectorgivenbyprependinga1tov. TheproofsofthefollowingresultscanbefoundinAppendixB.
Assumption1 TheentriesofY comefromalinearSCMoverasetofvariablesZ = (Z ,...,Z )
1 q
asinEquation(1). Thatis,the(i,j)−thentrycorrespondstothecounterfactualwhenconditioning
onI = j andtheninterveningtosetI = i,asinEquation(2).
C A
Theorem1 UnderAssumption1,wehave
E(Y ) = E(Z (i) | I = j) = U v +U′w .
ij X C i i i j
for some U ,U′ ∈ R|X|×|Z|,v ∈ R|Z| depending on the action index i, and some w ∈ R|Z|
i i i j
dependingonthecontextindexj.
Proof[Sketch]ForalinearSCM,wehaveZ = (I −B)−1E withB ∈ Rq×q andE = (ε ,...,ε ).
1 q
ConditioningonI C=j givesaposteriorPE|IC=j overtheexogenousnoise. Interveninggivesanew
matrixB andnewexogenousnoiseE˜ ,whereA aretheinterventiontargetswhenI =j. Defining
i Ai i A
E′=E˜ ∪E ,thenZ(i)equals(I−B )−1E′indistribution. Thus,E(Z(i) | I = j) = U v +U′w
Ai Ai i C i i i j
forU = (I −B )−1 andU′ amaskedversionofU ,withv andw comingfromE[E′].
i i i i i j
Theorem1showsthatE[Y]followsafactormodelwithafixedactioneffect. Thefixedeffectcanbe
foldedintothefactormodelasfollows:
U v +U′w = (cid:0) U v U′ (cid:1) aug(w ) (3)
i i i j i i i j
(cid:124) (cid:123)(cid:122) (cid:125)
R|X|×(1+|Z|)
Thus,SyntheticInterventionswillbeconsistentforcompletingthetensor. However,weproposean
alternativetodealwiththefixedeffect.
8CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
4.1. Theoretically-motivatedextensionsofSyntheticInterventions
ThefactormodelinEquation(3)hasmorestructurethanageneralrank|Z|+1factormodel. We
haveafixedeffectalongrowsplusafactormodelofrank|Z|. Thebestapproximationofaone-way
fixedeffectisgivenbythemean-over-contextsestimator(seeAppendixC).Therefore,following
thesameideafromtheNNM-FEAtheyetal.(2021),weproposethefollowingmodificationtoSI:
(1)subtractthefixedeffect,i.e.,letD = Y−Yˆmean-over-contexts,(2)runSIonthismatrixandobtain
DˆSI,(3)returntheestimateYˆmean-over-contexts+DˆSI.
Intuitively,byremovingtheexactfixedeffect,
ourfactormodelwouldhaverank|Z|insteadof|Z|+1,soitwouldbeeasiertosatisfythelinear
spaninclusionassumption.
Tensorcase. Thenon-symmetryinthefactormodelfromEquation(3)isevenmorerelevantin
thetensorcase, i.e. |X| > 1(wehaveamatrixthatdependsoniandavectorthatdependsonj).
Therefore, it is more reasonable to regress along contexts than along actions. To show how SI is
non-symmetricfortensors,inAppendixD,weprovideadetailedexampleforatwo-nodecasewhere
weobservebothZ andZ . Followingoursetup,weprovidea4×4×2tensorthatcanonlybe
1 2
completedifweuseSIwithincolumns(contexts). Inparticular,forusingSIwithinrowswewould
needmoredatatosatisfythelinearspaninclusionassumption.
4.2. ExploitingAdditionalStructureintheModelClass
OnedrawbackaboutthefactormodelfromEquation(3)isthatithasrank|Z|+1,whichmightbe
considerablylarge. Wenowconsidertwoadditionalassumptionswhichimposeadditionalstructure
onhowI andI interactwiththeSCMoverZ. First,weconsidermodelswheretheallcontexts
A C
aredefinedbysomecommonsetofsystemvariables.
Assumption2 ThereissomeC ⊂ [q]suchthatC = C forallj ∈ [n].
j
Inoursetting,whereweobserveonlyaverycoarse-grainedcontextindex,suchascelltype,itis
reasonabletosupposethat(atleastapproximately)theindexcanbedeterminedfromonlyasmallset
ofcontextmarkersC. Forexample,theremayexistsomesmallsetofmarkergenesthatdefinecell
states. Assumption2isimportanttoconsider,sinceitcanhaveasubstantialeffectonidentifiability
andestimationbyreducingtherankofourfactormodelto|C|.
Proposition2 Let Assumptions 1 and 2 hold, and assume that the underlying SCM is Gaussian.
ThenthelatentfactormodelfromTheorem1is
U′w = U′Wc
i j i j
where W is a constant matrix and c is the value that context markers take to define context j.
j
Therefore,bydefiningU′′ = U′W ∈ R|X|×|C| weobtainalatentfactormodelofrank|C|.
i i
Assumption3 ThereissomeA ⊂ [q]suchthatA = Aforalli ∈ [m].
i
Assumption3indicatesthatallactionschangetheSCMinasimilarway. Althoughthismaynotbe
realisticinmostsettings,wemighthavethissetupforaparticularsub-matrixofourwholematrixY.
Forexample,Assumption3mayholdfordrugswhichusethesamemechanismofactions,orifthe
differentactionscorrespondtothesamedrugoverdifferentdosageconcentrations. Inthiscase,the
latentfactormodelreducestoatwo-wayfixedeffectsmodel.
9RIBOTSQUIRESUHLER
Corollary3 UnderAssumptions1and3wehave
E(Z (i) | I = j) = Uv +U′w .
X C i j
soFixedEffect(FE)isaconsistentestimatorforcompletingY.
Inourpreviousresults,wehaveconsideredonlythecaseofalinearSCM.Thiscanbeextendedin
thesimplifiedcasewhere(1)allinterventionsoccuronthesamesetofnodesand(2)allcontextsuse
thesamecontext-definingnodes. Whilelinearitymayappeartobeastrongassumption,itisclosely
relatedtothelow-rankassumption,whichisessentialinmatrixcompletion. InAppendixE,westudy
someexamplesofnon-linearSCMs. Inparticular,whenconsideringpolynomials,weobservehow
the rank of the LFM increases as the degree of the polynomials increases. These results provide
evidencethatsomeformoflinearityorlow-degreepolynomialassumptionmayberequiredtojustify
aconnectionbetweenfactormodelsandcausalmodels.
Finally, in real applications, we need to test the validity of our assumptions, i.e. we need to
testwhetherourtensorcomesfromacertaingraphicalmodel. Sofar,wehaveonlyexploitedthe
structure that the SCM induces on expectations of Y. However, we can also use the structure to
obtainfine-grainedimplicationsregardingthevarianceofourobservations. Thisstructureonthe
varianceishelpfulforinferenceandforhypothesistesting. WedemonstratethisideainAppendixF,
whereweproposeahypothesistestforthefixedeffectsmodelimpliedbyCorollary3. Inparticular,
we test Corollary3 implies homoscedastic errors within a row. Thus, we develop a hypothesis
test for homoscedasticity, and then a particular test for fixed effects which reduces to a Welch’s
test. Similarly,forthelatentfactormodel(LFM)casefromProp.2,wecantesthomoscedasticity
withincolumns,thenusethetestforLFMsproposedbyAgarwaletal.(2020). InAppendixF,we
demonstratetheperformanceofthesetestsonsyntheticdata.
5. Empiricalresults
WeworkwiththePRISMRepurposingdataset(Corselloetal.,2020). TheentriesofY measure
a viability score, which indicates how lethal a drug is for a given cell line (i.e., negative viability
meansthatalargeproportionofcellsdie). Sinceweconsidertheindex-onlysetting,weuseonlythe
observedentriesofthismatrixtomakeourpredictions. TorelatethePRISMdataintothemodel
classintroducedinSection4,wethinkoftheinitialcellstatesintermsofFig.4(left). Inparticular,
weassumethatthereissomelatentspace(definedbythecontextvariables)wherewecanperfectly
distinguishdifferentcelllines,e.g. theexpressionlevelsofsomemarkergenes.
Arranging drugs in rows and cell lines in columns, as in Fig.1 from Section1, we obtain a
4,686×568matrix. Tocheckthesymmetryofanalgorithm,wewantthesamenumberofrowsas
columns,sothatcompletingalongonedirection(e.g. columns)isnotbettersimplybecausethere
ismoredatatomakepredictions. Thus,weruntheexperimentsonasquaresubmatrixtoavoidof
thisbias. Forthesamereason,weusesymmetricmissingdatapatterns,asdescribedinSection5.1.
WecomparetheperformanceofthealgorithmsintermsoftheR2 score. Thebaselineusedinthe
denominatoroftheR2 istheaverageoverthemissingdata. Asanexample,forthematrixshownin
Fig.4(right)andamissingpatternfromFig.5,thebaselineMSEisapproximately0.87. Getting
highR2 isadifficulttaskbecausethematrixisrelativelynoisy(seeAppendixG.1).
10CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
Figure4: (Left)TranslatingthePRISMdatatoourmodelclass. LettingC = {k,l}indicatesthat
cellstateisdefinedsolelyintermsofZ andZ . (Right)Matrixofviabilityscores. Each
k l
entryrepresentstheviabilityscoreforadrug-celllinepair. Negativeviabilityindicates
celldeath. Viabilityscoresarenormalizedforvisualizationpurposes.
5.1. Aboutthemissingdatapattern
InthePRISMdataset,wehaveobservedallthematrix. Asaconsequence,wecandecidethemissing
datapatterntotesttheperformanceofthematrixcompletionapproaches. Butwhichmissingdata
patternismoreappropriateforbiologicaldata?
AswehaveseeninSection2,NuclearNormMinimizationalgorithmsarenear-optimalwhenwe
haveuniformlyatrandommissingentries. However,inbiologyapplicationsthisisusuallynotthe
case. Instead,itismorecommontohaveasmallportionofcelllinesthathasbeentestedagainst
manyofthedrugs,butforthevastmajorityofcelllineswehaveonlyrunafewexperiments. For
example,inAppendixG.2wecanseethemissingdatapatternfortheCMAPdataset.
Forthisreason,wetestthealgorithmsonmissingpatternsastheonesfromFig.5. Here,foreach
missingentry(i,j), thenumberofnon-missingentries(blacksquares)inthei-throwisequalto
thenumberofnon-missingentriesinthej-thcolumn. Weruntheexperimentsusingdifferentdata
patternstoseetheeffectofhavingless/moreavailabledata.
InAppendixG.3weconsideranalternativemissingdatapatternwherewedonothaveaconstant
numberofobservationsforeachmissingentry. Wecanobservesimilarresultsalsointhissetting.
Figure5: Missingdatapatternsusedinourex-
periments. Observed entries are de-
notedwithblack. Foreachmissingen-
trywehavethesamenumberofobser-
vationsalongrowsandcolumns.
11RIBOTSQUIRESUHLER
Figure6: VariantsofSyntheticInterventionshavethebestperformanceonmatrixcompletion
for the PRISM dataset. Using the missing data patterns are ones shown in Fig.5, we
testtheperformanceofeachalgorithmon20differentshufflesofrowsandcolumns. The
resultsareshownusingboxplots. Thenumbersatthetopdenotethemedian(redlines).
5.2. Performanceofmatrixcompletionalgorithms
Inthissectionweshowtheperformanceofthematrixcompletionalgorithmspresentedbefore. In
particular,weareinterestedinanalyzinghowthisperformancedependsontheamountofobserved
data. AnextensiveanalysisofthisdependencecanbeboundinAppendixG.4.
InFig.6weseethatmean-over-contextsisastrongbaselineandthatSyntheticInterventions(and
itsvariants)outperformalltheotherapproaches. NNMperformscompetitivelywhenweobservea
sufficientlylargeamountofentries(right-handsideplot)butitperformsquitepoorlyotherwise.
Theseresultsreinforceourtheoreticalfindings. InSection4weseehow,assuminganunderlying
linearSCM,thecounterfactualquantityofourinterestleadstoaparticularfactormodel,implying
consistencyoftheSyntheticInterventionsestimator. Forfurtheranalysis,inAppendixG.5wereport
resultsforothervariationsoftheseapproaches. InAppendixG.6werunsimilarexperimentsona
particularsubmatrix.
6. Discussion
Inthispaper,wegeneralizetheSCM-basedmodelclassintroducedbySquiresetal.(2022)toallow
for situations where the measured outcome is defined as a counterfactual. We showed that latent
factormodels(LFMs)naturallyariseoutofthismodelclass,thoughwithanadditionaltermforfixed
actioneffects. Thisdemonstratesthefundamentallydistinctivenatureofcausalmatrixcompletion,
where there is a difference between conditioning (completing within columns) and intervening
(completingwithinrows). Asapracticalconsequence,thisledustoproposeasimpleextensionof
12CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
theSyntheticInterventions(SI)estimatorwhichincludesafixedeffectterm. Thesemodel-inspired
causalmatrixcompletionapproachesworkconsiderablywell,especiallyinthelow-dataregime.
LimitationsandFutureWork. Inthispaper,welargelyfocusedonlinearmodels,andonlysought
todevelopconsistentestimatorsforcausalpredictions. Animportantnextstepistoanalyzethenoisy
case,focusingonstatisticalaspectssuchassamplecomplexityandefficientinference.
Akeylimitationofourworkisthatweonlyconsideredindex-onlycausalpredictionproblems.
In many applications, additional data is available for actions and/or contexts, e.g., the molecular
structure of a drug would be highly relevant to predicting its effect. We expect that the current
workwillbeausefulconceptualfoundationfordevelopingmodel-basedapproachestothesecausal
predictionproblems,especiallywhendevelopingfurtherconnectionsbetweenSCMsandlatentfactor
models. Itwouldbeespeciallyinterestingtoexploreconnectionswithrecentworksforintervention
extrapolation,suchasAgarwaletal.(2023)andSaengkyongametal.(2023).
Finally, while our work was mainly motivated by a biological problem, the causal prediction
problemisverygeneral,anditwouldbeinterestingtoapplyourframeworkinothersettings.
Acknowledgments
ThisworkwassupportedinpartbyfundingfromtheEricandWendySchmidtCenterattheBroad
InstituteofMITandHarvard.
Á.RibotacknowledgessupportbythemobilitygrantsprogramofCentredeFormacióInterdis-
ciplinàriaSuperior(CFIS)-UniversitatPolitècnicadeCatalunya(UPC),theMITSimonsMMLS
Foundationresearchgrant(6941629),andaMOBINT-MIFgrant.
C.SquiresandC.UhleracknowledgesupportbytheNSFTRIPODSprogram(DMS-2022448),
NCCIH/NIH(1DP2AT012345),ONR(N00014-22-1-2116),theUnitedStatesDepartmentofEnergy
(DOE),OfficeofAdvancedScientificComputingResearch(ASCR),viatheM2dtMMICCcenter
(DE-SC0023187),AstraZeneca,theEricandWendySchmidtCenterattheBroadInstitute,anda
SimonsInvestigatorAward.
References
Alberto Abadie and Javier Gardeazabal. The economic costs of conflict: A case study of the
basque country. American Economic Review, 93(1):113–132, March 2003. doi: 10.1257/
000282803321455188. URL https://www.aeaweb.org/articles?id=10.1257/
000282803321455188.
Abhineet Agarwal, Anish Agarwal, and Suhas Vijaykumar. Synthetic combinations: A causal
inferenceframeworkforcombinatorialinterventions,2023.
AnishAgarwal,DevavratShah,andDennisShen. Synthetica/btestingusingsyntheticinterventions,
2020. URLhttps://arxiv.org/abs/2006.07691.
AnishAgarwal,MuntherDahleh,DevavratShah,andDennisShen. Causalmatrixcompletion,2021.
Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar Khosravi.
Matrix completion methods for causal panel data models. Journal of the American Statistical
Association, 116(536):1716–1730, may 2021. doi: 10.1080/01621459.2021.1891924. URL
https://doi.org/10.1080%2F01621459.2021.1891924.
13RIBOTSQUIRESUHLER
JushanBaiandSerenaNg. Matrixcompletion,counterfactuals,andfactoranalysisofmissingdata,
2019. URLhttps://arxiv.org/abs/1910.06677.
Jian-FengCai,EmmanuelJ.Candes,andZuoweiShen. Asingularvaluethresholdingalgorithmfor
matrixcompletion,2008.
EmmanuelJ.CandesandYanivPlan. Matrixcompletionwithnoise,2009.
EmmanuelJ.CandesandBenjaminRecht. Exactmatrixcompletionviaconvexoptimization,2008.
URLhttps://arxiv.org/abs/0805.4471.
Emmanuel J. Candes and Terence Tao. The power of convex relaxation: Near-optimal matrix
completion,2009. URLhttps://arxiv.org/abs/0903.1476.
Daniel C Castro, Ian Walker, and Ben Glocker. Causality matters in medical imaging. Nature
Communications,11(1):3673,2020.
Krzysztof Chalupka, Pietro Perona, and Frederick Eberhardt. Visual causal feature learning. In
ProceedingsoftheThirty-FirstConferenceonUncertaintyinArtificialIntelligence,pages181–190,
2015.
StevenMCorsello,RohithTNagari,RyanDSpangler,JordanRossen,MustafaKocak,JordanG
Bryan, Ranad Humeidi, David Peck, Xiaoyun Wu, Andrew A Tang, et al. Discovering the
anticancerpotentialofnon-oncologydrugsbysystematicviabilityprofiling. Naturecancer,1(2):
235–248,2020.
PhilipDawid. Decision-theoreticfoundationsforstatisticalcausality. JournalofCausalInference,9
(1):39–77,2021.
NikolayDoudchenkoandGuidoWImbens. Balancing,regression,difference-in-differencesand
syntheticcontrolmethods: Asynthesis. Technicalreport,NationalBureauofEconomicResearch,
2016.
RaazDwivedi,KatherineTian,SabinaTomkins,PredragKlasnja,SusanMurphy,andDevavratShah.
Doublyrobustnearestneighborsinfactormodels,2022.
FrederickEberhardtandRichardScheines. Interventionsandcausalinference. Philosophyofscience,
74(5):981–995,2007.
CarlEckartandGaleYoung. Theapproximationofonematrixbyanotheroflowerrank. Psychome-
trika,1(3):211–218,September1936.
AmirFeder,KatherineAKeith,EmaadManzoor,ReidPryzant,DhanyaSridhar,ZachWood-Doughty,
JacobEisenstein,JustinGrimmer,RoiReichart,MargaretERoberts,etal. Causalinferencein
naturallanguageprocessing: Estimation,prediction,interpretationandbeyond. Transactionsof
theAssociationforComputationalLinguistics,10:1138–1158,2022.
TrevorHastie,RahulMazumder,JasonLee,andRezaZadeh. Matrixcompletionandlow-ranksvd
viafastalternatingleastsquares,2014.
14CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
RachelHodos,PingZhang,Hao-ChihLee,QiaonanDuan,ZichenWang,NeilRClark,AviMa’ayan,
FeiWang,BrianKidd,JianyingHu,etal. Cell-specificpredictionandapplicationofdrug-induced
geneexpressionprofiles. InPACIFICSYMPOSIUMONBIOCOMPUTING2018: Proceedingsof
thePacificSymposium,pages32–43.WorldScientific,2018.
PrateekJain,PraneethNetrapalli,andSujaySanghavi. Low-rankmatrixcompletionusingalternating
minimization,2012.
YehudaKoren,RobertBell,andChrisVolinsky. Matrixfactorizationtechniquesforrecommender
systems. Computer,42(8):30–37,2009. doi: 10.1109/MC.2009.263.
G. Linden, B. Smith, and J. York. Amazon.com recommendations: item-to-item collaborative
filtering. IEEEInternetComputing,7(1):76–80,2003. doi: 10.1109/MIC.2003.1167344.
DanielMalinsky,IlyaShpitser,andThomasRichardson.Apotentialoutcomescalculusforidentifying
conditionalpath-specificeffects,2019.
Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for
learninglargeincompletematrices. JournalofMachineLearningResearch,11(80):2287–2322,
2010. URLhttp://jmlr.org/papers/v11/mazumder10a.html.
WhitneyKNeweyandJamesLPowell. Instrumentalvariableestimationofnonparametricmodels.
Econometrica,71(5):1565–1578,2003.
JonasPeters,DominikJanzing,andBernhardSchölkopf. Elementsofcausalinference: foundations
andlearningalgorithms. TheMITPress,2017.
Adityanarayanan Radhakrishnan, Max Ruiz Luyten, Neha Prasad, and Caroline Uhler. Transfer
learningwithkernelmethods,2022.
BenjaminRecht,MaryamFazel,andPabloA.Parrilo. Guaranteedminimum-ranksolutionsoflinear
matrixequationsvianuclearnormminimization. SIAMReview,52(3):471–501,jan2010. doi:
10.1137/070697835. URLhttps://doi.org/10.1137%2F070697835.
ThomasSRichardsonandJamesMRobins. Singleworldinterventiongraphs(swigs): Aunification
ofthecounterfactualandgraphicalapproachestocausality. CenterfortheStatisticsandtheSocial
Sciences,UniversityofWashingtonSeries.WorkingPaper,128(30):2013,2013.
Sorawit Saengkyongam, Elan Rosenfeld, Pradeep Ravikumar, Niklas Pfister, and Jonas Peters.
Identifyingrepresentationsforinterventionextrapolation,2023.
JBenSchafer,DanFrankowski,JonHerlocker,andShiladSen. Collaborativefilteringrecommender
systems. Theadaptiveweb: methodsandstrategiesofwebpersonalization,pages291–324,2007.
Christopher Southan, Markus Sitzmann, and Sorel Muresan. Comparing the chemical structure
andproteincontentofchembl,drugbank,humanmetabolomedatabaseandthetherapeutictarget
database. Molecularinformatics,32:881–897,122013. doi: 10.1002/minf.201300103.
Chandler Squires and Caroline Uhler. Causal structure learning: A combinatorial perspective.
FoundationsofComputationalMathematics,pages1–35,2022.
15RIBOTSQUIRESUHLER
Chandler Squires, Dennis Shen, Anish Agarwal, Devavrat Shah, and Caroline Uhler. Causal
imputationviasyntheticinterventions. InConferenceonCausalLearningandReasoning,pages
688–711.PMLR,2022.
Jin Tian and Judea Pearl. Causal discovery from changes. In Proceedings of the Seventeenth
conferenceonUncertaintyinartificialintelligence,pages512–521,2001.
MadeleineUdellandAlexTownsend. Whyarebigdatamatricesapproximatelylowrank?,2017.
You Wu, Qiao Liu, Yue Qiu, and Lei Xie. Deep learning prediction of chemical-induced dose-
dependentandcontext-specificmultiplexphenotyperesponsesanditsapplicationtopersonalized
alzheimer’sdiseasedrugrepurposing. PLOSComputationalBiology,18(8):e1010367,2022.
ZeminZhangandShuchinAeron. Exacttensorcompletionusingt-svd,2015.
16CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
ContentsofAppendix
A Summaryofmatrixcompletionapproaches 18
B Proofs 19
C Bestapproachforestimatingaone-wayfixedeffect 21
D SIisnon-symmetricinthetensorcase 21
E Polynomialcase 23
E.1 Goingbeyondpolynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
F Hypothesistests 24
F.1 Homoscedasticitywithinrows/columns . . . . . . . . . . . . . . . . . . . . . . . 24
F.2 Testforfixedeffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
F.3 Testformean-over-contexts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.4 TestforSyntheticInterventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
G FurtheranalysisonPRISMRepurposingdataset 28
G.1 SVDanalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
G.2 MissingpatterninCMAPdataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
G.3 Analternativemissingdatapattern . . . . . . . . . . . . . . . . . . . . . . . . . . 30
G.4 Performanceofthealgorithmsdependingontheamountofdataobserved . . . . . 31
G.5 Performanceofallthematrixcompletionalgorithmsconsidered . . . . . . . . . . 33
G.6 Removalof"killerdrugs" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
17RIBOTSQUIRESUHLER
Supplementalmaterial
Allcodeforrunningtheexperimentspresentedinthispapercanbefoundat
https://github.com/alvaro-ribot/causal-matrix-completion-PRISM
AppendixA. Summaryofmatrixcompletionapproaches
InSection2,wepresentedthedifferentmatrixcompletionapproachesconsideredinthispaper. For
completeness,inthissection,weprovideformulaeforsuchapproaches.
WefollowthenotationusedinSquiresetal.(2022). LetY = (Y ) ∈ Rm×n beourmatrixof
ij
interest. Let Ω ⊂ [m]×[n] be the set of observed entries and M = [m]×[n]\Ω be the set of
missing entries. For a given row i, let C(i) = {j : (i,j) ∈ Ω} be the set of column indices j for
whichwehaveobservedY . Similarly,foracolumnj defineR(j) = {i : (i,j) ∈ Ω}. Thisnotation
ij
isextendedtosetsofrows(R)andsetsofcolumns(C): C(R) = ∩ C(i)andR(C) = ∩ R(j).
i∈R j∈C
Thefollowingexampleclarifiesournotation.
 
Y Y ? ? Y C(1) = {1,2,5}
11 12 15
 ? ? Y 23 Y 24 ?  =⇒ C({1,3}) = {1,5}
Y ? Y ? Y C(3) = {1,3,5}
31 33 35
• Mean-Over-Contexts: Yˆmean-over-contexts = 1 (cid:80) Y .
ij |C(i)| j′∈C(i) ij′
• Mean-Over-Actions: Yˆmean-over-actions = 1 (cid:80) Y .
ij |R(j)| i′∈R(j) i′j
• FixedEffects(FE):YˆFE = Yˆmean-over-contexts+Yˆmean-over-actions− 1 (cid:80) Y .
ij ij ij |R(j)||C(i)| i′∈R(j) i′j′
j′∈C(i)
• CollaborativeFiltering(CF):YˆCF-contexts = 1 (cid:80) cos-sim(j′,j)Y .
ij (cid:80) j′∈R(i)|cos-sim(j′,j)| j′∈R(i) ij′
• Synthetic interventions (SI): Let R = R(C(i)∪{j}), Y = [Y : i′ ∈ R ,j′ ∈
train train i′j′ train
C(i)],y = [Y : i′ ∈ R ],y = [Y : j′ ∈ C(i)]. Thensolvelinearregression:
output i′j train test ij′
YˆSI = y⊤ βˆ, whereβˆ= Y† y and†denotesthepseudoinverse.
ij test train output
• SI-FE:LetD = Y−YˆFE,thenYˆSI-FE = YˆFE+DˆSI.
ij ij ij
• NuclearNormMinimization(NNM):YˆNNM = argmin 1∥P (Y−L)∥2 +λ∥L∥ ,where
L 2 Ω F ∗
(P (M)) = M if(i,j) ∈ Ωand0otherwise.
Ω ij ij
• NNM-FE:YˆNNM-FE = Lˆ +Γˆ1⊤+1 ∆ˆ⊤,where1 = (1,...,1) ∈ Rn and
n m n
1
(Lˆ,Γˆ,∆ˆ) = argmin ∥P (Y−L−Γ1⊤−1 ∆⊤)∥2 +λ∥L∥ .
|Ω| Ω n m F ∗
L,Γ,∆
18CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
AppendixB. Proofs
Notation. Recallthat[q] := {1,...,q}. GivenatupleofindicesK = (k ,...,k )wherek ∈ [q]
1 p r
forallr = 1,...,p,wedefinethematrixI asfollows:
K
(cid:40)
1ifr = sandr ∈ K
I ∈ Rq×q s.t. (I ) =
K K rs
0otherwise
Furthermore,givenavectorZ ∈ Rq,wedefineZ ∈ R|K| suchthat(Z ) = Z . Finally,givena
K K l k
l
matrixM ∈ Rq×q, wedenoteby[M] ∈ R|K|×q thesub-matrixofM whoserowsindicesarein
K
K. Tosimplifynotation,foranyv ∈ Rd,wedefineaug(v) = (1,v ,...,v )⊤ ∈ Rd+1 asthevector
1 d
givenbyprependinga1tov.
Theorem1 UnderAssumption1,wehave
E(Y ) = E(Z (i) | I = j) = U v +U′w .
ij X C i i i j
for some U ,U′ ∈ R|X|×|Z|,v ∈ R|Z| depending on the action index i, and some w ∈ R|Z|
i i i j
dependingonthecontextindexj.
ProofLetZ = (Z ,...,Z )followalinearSEM,i.e.
1 q
Z = BZ+E
where B = (B ) ∈ Rq×q such that B = 0 whenever i ≤ j and E = (ε ,...,ε ). Let I be the
ij ij 1 q
identityq×q matrix. SinceB islowertriangular,(I −B)isinvertibleandwehavethat
Z = (I −B)−1E
Whenweconditiononthevalueofthecontextindex,i.e. onI = j,wegetanewdistributionfor
C
thenoise,PE|IC=j. Inparticular,
w := E(E | I = j) ∈ Rq
j C
isafunctionofj.
NowletA ⊆ [q]bethetargetsetofindicesfortheinterventionwithindexi. Foreachk ∈ A ,
i i
thek-throwofB ischanged(modifyingthedependencyfromitsparents). LetB betheweight
i
matrixaftertheintervention4.
Moreover,foreachk ∈ A ,wehaveanewnoisevariableε˜ ,independentofE. So,theexpected
i k
valueofthesenewvariablesdependontheinterventionindexibutnotonthecontextindexj. LetE˜
beaq-dimensionalrandomvectorsuchthatitsk-thentryisε˜ ifk ∈ A and0otherwise. Define
k i
v := E(E˜) ∈ Rq, andnotethatE(E˜) = E(E˜| I = j).
i C
Therefore,thevariablesZ(i)aftertheinterventionsatisfythefollowingSEM
(cid:16) (cid:17)
Z(i) = B Z(i)+E˜+I E =⇒ Z(i) = (I −B )−1 E˜+I E
i Ai i Ai
4.Intheparticularcaseofdo-interventions,wewouldremovethesedependencies,soalltheentriesinthek-throwof
B wouldbe0.
i
19RIBOTSQUIRESUHLER
whereA = [q]\A . Hence,
i i
E(Z(i) | I = j) = (I −B )−1E(E˜)+(I −B )−1I E(E | I = j)
C i i Ai C
RecallthatweareinterestedonlyinZ (thevariablesweobserve). Let
X
U = (cid:2) (I −B )−1(cid:3) ∈ R|X|×q U′ = U I ∈ R|X|×q
i i X i i Ai
Therefore,wehave
E(Z (i) | I = j) = U v +U′w
X C i i i j
Proposition2 Let Assumptions 1 and 2 hold, and assume that the underlying SCM is Gaussian.
ThenthethelatentfactormodelfromTheorem1is
U′w = U′Wc
i j i j
where W is a constant matrix and c is the value that context markers take to define context j.
j
Therefore,bydefiningU′′ = U′W ∈ R|X|×|C| weobtainalatentfactormodelofrank|C|.
i i
ProofWehaveE = (ε ,...,ε ) ∼ N(µ = 0,Σ = diag(σ2)),so
1 q ε ε i
Σ := Cov(Z) = (I −B)−1Σ (I −B⊤)−1
ε
LetZ bethecontextmarkersvariablesandC = [q]\C. UsingtheSchurComplementweobtain
C
E(Z | Z = c ) = Σ Σ−1c
C C j CC CC j
Of course, we also have E(Z | Z = c ) = c , so E(Z | Z = c ) = Mc for some matrix M.
C C j j C j j
OnewaytobemorespecificaboutthisM isthefollowing: LetP bethepermutationmatrixsuch
thatPZ = (Z ,Z )⊤. Thenwehave
C C
(cid:18)
Σ
Σ−1(cid:19) (cid:18)
Σ
Σ−1(cid:19)
E(PZ | Z = c ) = CC CC c =⇒ M = P−1 CC CC
C j I j I
|C| |C|
Hence,afterconditioning,theexpectedvalueofthenoiseis
E(E | Z = c ) = (I −B)E(Z | Z = c ) = (I −B)Mc
C j C j j
Therefore,wecandefine
(cid:18)
Σ
Σ−1(cid:19)
W = (I −B)M = (I −B)P−1 CC CC ∈ Rq×|C|
I
|C|
NotethatW dependsonthenodesweareconditioningon,butnotontheirvalues. Therestofthe
proofiscompletelyanalogoustothepreviousone.
Remark4 Althoughitiscommontoworkwithµ = 0,thereisnoneedtoassumethat. Ingeneral,
ε
againusingtheSchurcomplement,wewouldhave(letµ = µ )
ε
E(Z | Z = c ) = Σ Σ−1(c −µ )+µ = Σ Σ−1c −Σ Σ−1µ +µ
C C j CC CC j C C CC CC j CC CC C C
= (cid:0) µ −Σ Σ−1µ Σ Σ−1(cid:1) aug(c )
C CC CC C CC CC j
So,ifwedefinec′ = aug(c ) ∈ R1+|C| andM accordingly,alltheproofworksanalogously.
j j
20CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
AppendixC. Bestapproachforestimatingaone-wayfixedeffect
In Section4, we claimed that the best approximation of a one-way fixed effect is given by the
mean-over-contextsestimator. Indeed,considerthatfollowingproblem
min f(Γ) := ∥Y−Γ1⊤∥2 (4)
F
Γ∈Rm
whereY ∈ Rm×n,1 = (1,...,1) ∈ Rn,and∥·∥ denotestheFrobeniusnormofamatrix. Bythe
F
linearityoftracewehave
f(Γ) = tr(Y⊤Y)−2tr(1Γ⊤Y)+tr(1Γ⊤Γ1)
Usingthecyclicpropertyofthetraceandtakingthegradientoff weget
1
∇f(Γ) = −2Y1+2nΓ = 0 ⇐⇒ Γ = Y1
n
Thatis,Γ = 1 (cid:80) Y ,whichcorrespondstothemean-over-contextsestimator. Sincef isstrictly
i n j ij
convex,thisisthesolutiontoProblem4.
AppendixD. SIisnon-symmetricinthetensorcase
InSection4,wearguedthatSIisnon-symmetricinthetensorcase. Thefollowingexampleshowsthat
thisistrue. Inparticular,completingwithincolumnsrecoversthedesiredoutcomewhilecompleting
withinrowsdoesnot. Thekeyideaisthatwehavetoflattenthetensorinonedirectionoranother,
anddependingonthedirection,linearspaninclusiondoesordoesnothold. Considerthefollowing
3-ordertensor
   
Y Y Y Y Y Y Y Y 1 1 1 1 0 2 1 1
111 121 131 141 112 122 132 142
Y =   Y 211 Y 221 Y 231 Y 241 Y 212 Y 222 Y 232 Y 242   =   1 0 1 0 1 1 1 1  
 Y 311 Y 321 Y 331 Y 341 Y 312 Y 322 Y 332 Y 342   1 1 1 1 1 1 1 1 
Y Y Y Y Y Y Y Y 0 0 0 0 1 1 1 1
411 421 431 441 412 422 432 442
Forthe(i.j)-thentry,weusethenotationY = (Y ,Y ) ∈ R2. Supposewehaveobservedall
ij ij1 ij2
thematrixexceptforthe(4,4)-thentry. Byregressingalongrows,wearepredictingthe(flattened)
firstthreerowsofthe4thcolumn(1,0,1,1,1,1)fromthe(flattened)firstthreerowsofthe1st,2nd
and3rdcolumns(1,1,1,0,1,1),etc. Thus,weobtainthefollowingsystemofequations:
   
1 1 1 1
1 0 1 0
   
1 1 1 1
 β =  
0 2 1 1
   
1 1 1 1
1 1 1 1
Thislinearsystemisindependentandweobtainβˆ= (1,1,−1). Thus,ourpredictionforthe(4,4)-th
entryis
(cid:18) (cid:19) (cid:18) (cid:19)
0 0 0 0
Yˆ = βˆ= = Y
44 1 1 1 1 44
21RIBOTSQUIRESUHLER
Thus,regressingalongrowsworkssuccessfully. However,byregressingalongcolumnswewould
obtainthefollowing:
   
1 1 1 0
1 0 1 0
   
1 1 1 0
 α =  
0 1 1 1
   
2 1 1 1
1 1 1 1
which is an inconsistent system. The least-squares solution is αˆ = (0,0.6,0), so the prediction
wouldbe
(cid:18) (cid:19) (cid:18) (cid:19)
1 0 1 0
Yˆ = αˆ = ̸= Y
44 1 1 1 0.6 44
So this shows how SI works only in one direction. Moreover, this example can be related to our
causalsetup. LetZ → Z followtheSCM
1 2
(cid:40)
Z = ε
1 1
Z = Z +ε
2 1 2
where ε ,ε ∼ N(0,1) are independent noise variables. Suppose that we are observing both
1 2
variables. EachcolumnofY correspondstoconditioningonZ = (c ,c ). Inparticular,wehave
1 2
E[(ε ,ε ) | (Z ,Z ) = (c ,c )] = (c ,c − c ). The rows of Y correspond to an intervention
1 2 1 2 1 2 1 2 1
do(Z = a ,Z = a ). Thevaluesof(c ,c )and(a ,a )aredefinedinTable2,where"-"denotes
1 1 2 2 1 2 1 2
column(I ) c c row(I ) a a
C 1 2 A 1 2
j = 1 1 0 i = 1 1 -
j = 2 0 1 i = 2 - 1
j = 3 1 1 i = 3 1 1
j = 4 0 0 i = 4 0 1
Table2: Valuesusedfordefiningi-throwandj-thcolumnofY.
that we are not intervening on that variable. It is easy to check that E(Z(i) | I = j) = Y .
C ij
Therefore,ourmatrixcomesfromalinearcausalmodel. ToseewhySIonlyworkedononedirection,
recallthatEquation(3)tellsusthatwecanwrite
Y = P q
ij i j
forsomeP ∈ R2×3 andq = R3. FollowingtheProofofTheorem1,wecanconstructthesefactors:
i j
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
1 0 0 0 1 0 1 0 0 0 0 0
P = P = P = P =
1 1 0 1 2 1 0 0 3 1 0 0 4 1 0 0
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
q = 1 1 −1 q = 1 0 1 q = 1 1 0 q = 1 0 0
1 2 3 4
Notethatq ∈ span(q ,q ,q ). Infact,q = βˆ q +βˆ q +βˆ q ,sothelinearspaninclusion
4 1 2 3 4 1 1 2 2 3 3
issatisfiedalongcolumns. Nevertheless,P ∈/ span(P ,P ,P ),sothelinearspaninclusionisnot
4 1 2 3
satisfiedalongrows.
22CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
AppendixE. Polynomialcase
InSection4,wediscussedthepossibilityofextendingourresultstonon-linearSCMs. Here,weshow
anexamplewherethelatentvariablesfollowalinearSCMandthereisapolynomiallinkfunction
mapping the latent variables to the observed ones. For simiplicity in notation we use X = Z ,
X
A = Z ,andC = Z . NotethatAandC arethesameforallrowsandcolumns,respectively.
A C
Example2 ConsiderthefollowingSCM:

A = ε
 A

C = B A+ε
AC C

X = f(A,C)+ε , f ∈ R [Y ,Y ]
X d 1 2
Since there is linear relation between A and C, we can still use the Schur Complement. For a
gaussianvariableZ ∼ N(µ,σ2),wehaveE(Zk) = p (µ,σ)forsomepolynomialp ∈ R [T ,T ].
k k k 1 2
Therefore,E(X(a )|C = c )isapolynomialina andc . Inparticular,wecanwriteitas
i j i j
E(X(a )|C = c ) = q (c )+q (c )a +···+q (c )ad−1+q (c )ad
i j 0 j 1 j i d−1 j i d j i
 
q (c )
0 j
= (cid:0) 1 a i ··· ad
i(cid:1)

q 1(
.
.c j)
 
 . 
q (c )
d j
forsomepolynomialsq .5 Therefore,forapolynomialofdegreedwehaveafactormodelofdegree
k
d+1,sowecouldalsouseSyntheticInterventionsinthiscase.
IfwehadmultiplefixedactionnodesA ,...,A andapolynomialofdegreedfromthelatentnodes
1 r
to X, we could use the same idea and express E(X(a ) | C = c ) as a polynomial on a1,...,ar.
i j i i
The number of monomials in r variables of degree no greater than d is
(cid:0)r+d(cid:1)
, so that would be
d
the rank of our factor model. However, it is not clear how our decomposition would look like if
allowedinterveningondifferentnodes,becausetheq ’swoulddependontheseindices. Weleave
k
thisquestionopenasafuturedirection.
E.1. Goingbeyondpolynomials
Example3 ConsidernowthefollowingSCM:

A = ε
 A

C = A+ε
C

X = exp(AC)
Afterconditioningandintervening,wegetthat
(cid:18)
(cid:16) c(cid:17)
a2(cid:19)
X(a)|C = c ∼ Lognormal a a+ ,
2 2
5.Inthiscase,wecoulddefinedthispolynomialmoreexplicitlyasafunctionofE(εk |C =c ),butthisisnotnecessary
c j
forexpressingthefactormodel.
23RIBOTSQUIRESUHLER
Hence,
(cid:18)
(cid:16) c(cid:17)
a2(cid:19) (cid:18)
5
(cid:19)
(cid:16)ac(cid:17)
E(X(a)|C = c) = exp a a+ + = exp a2 exp
2 4 4 2
Butthisexpressioncannotbeexpressedasalow-rankfactormodel.
Example3suggeststhatitmightbedifficulttoobtainlow-rankfactormodelsforSCMsthatinvolve
functions"morecomplicated"thanpolynomials. Moreover,Example2showsthattherankofthe
factormodelincreasesasthedegreeofthepolynomialincreases.
Note that the expression obtained is the exponential of a polynomial in a,c. If we applied an
entry-wiselogarithmtoourmatrix,wecouldapplytheSIestimatorandthentaketheexponentialof
theoutcomeobserved. Withthisideainmind,wecouldthinkaboutdevelopingSItheoryforfeature
spaces. However,thisgoesbeyondthescopeofthisworkandweleaveitforfuturediscussion.
AppendixF. Hypothesistests
InSection4,wediscussedthenecessitytotestthevalidityofourassumptions. Here,weshowhow
ourcausalmodelingmayalsobeusefulforconstructinghypothesistests. Forsimplicity,assumethat
wehaveobservedn independentsamplesforeachobservedentry,i.e. foreach(i,j) ∈ Ω. Assume
s
σ2
that Y ∼ N(µ ,σ2) and let Y ∼ N(µ , ij) denote the average of these samples. We can
ij ij ij ij i,j ns
estimatethesamplevarianceasfollowsS2 = 1 (cid:80)ns (Y(k) −Y )2.
ij ns−1 k=1 ij ij
F.1. Homoscedasticitywithinrows/columns
Before discussing the different decompositions obtained in Section4, is it worth noting that the
SCMswestudiedgiverisetocertainstructureinthevarianceofourmatrix. Inparticular,Prop.2
leadstohomoscedasticitywithintheentriesofthesamerow(σ = σ ). Indeed,lookingatthe
i,j i,j′
SchurComplement,thevarianceofthenoiseafterconditioningonsomevariablesdependsonthe
variablesweconditionon,butnotontheactualvaluestheytake. Ontheotherhand,inCorollary3
wehavethesameSCMsaftertheinterventionsfordifferentrows,whichimplieshomoscedasticity
withincolumns(σ = σ ).
i,j i′,j
Thisleadtoaclassicaltestforcomparingthevariancesoftwoindependentsamples. Forexample,
fortestinghomoscedasticitywithincolumnj weconsider

σ2
 H
0
: σ2i,j = 1 ∀i ≥ 2,j
i−1,j
σ2
H : i,j ̸= 1, ∃i,j
 1 σ2
i−1,j
Sowecanusethefollowingestimator
S2
i,j
F =
S2
i−1,j
andwehaveF | H ∼ F . Atestforhomoscedasticitywithinrowiwouldbeanalogous.
0 ns−1,ns−1
24CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
F.2. Testforfixedeffects
SupposethatweareinthecaseofCorollary3. HowcanwetestifFEisappropriateforourmatrix?
For simplicity in notation, suppose that we have observed all the entries of Y ∈ Rm×n. For a
partiallyobservedentries,wewouldusethesetestsfortheobservedentries. Withthisnotation,all
thematriceswithatwo-wayfixedeffectaredefinedbythefollowingequations.
Y +Y −Y −Y = 0 ∀i ̸= i′,j ̸= j′ (5)
ij i′j′ i′j ij′
Many of this equations are linearly dependent. In fact, all of them can be expressed as a linear
combinationofequations
E : Y +Y −Y −Y = 0, i = 2,...,m,j = 2,...n
ij 11 ij i1 1j
Indeed, by computing E +E −E −E we get Equation (5). So we only need to check
ij i′j′ i′j ij′
(m − 1)(n − 1) equations instead of m(m − 1)n(n − 1). However, one drawback about using
Equation(F.2)forconstructinghypothesistestsisthattheyheavilyrelyonthe(1,1)-thentry. Sowe
canconsideramorerobustsetofequations:
E˜ : Y +Y −Y −Y = 0, i = 2,...,m,j = 2,...,n
ij i−1,j−1 i,j i,j−1 i−1,j
Letµ = E(Y ),thehypothesistestforFEisthefollowing.
i,j i,j
(cid:40)
H : µ +µ −µ −µ = 0, ∀i,j
0 i−1,j−1 i,j i,j−1 i−1,j
H : µ +µ −µ −µ ̸= 0, ∃i,j
1 i−1,j−1 i,j i,j−1 i−1,j
Therefore,wecanconsiderthefollowingstatistic:
Y +Y −Y −Y
Tˆ = i−1,i−1 i,j i,j−1 i−1,j
ij (cid:114)
(cid:16) (cid:17)
1 S2 +S2 +S2 +S2
ns i−1,j−1 i,j i,j−1 i−1,j
The key idea is that, in Corollary3, we have homoscedasticity within rows. This makes our test
similartoaWelch’stest,andwecanapproximatethedistributionofTˆ | H ast where
i,j 0 νˆ
(cid:16) (cid:17)2
(S2 +S2 )+(S2 +S2 )
i,j i−1,j i,j−1 i−1,j−1
νˆ = (n −1)
s (S2 +S2 )2+(S2 +S2 )2
i,j i−1,j i,j−1 i−1,j−1
Multiplecomparisonsproblem. WehaveN = (n−1)(m−1)differentnullhypothesis, cor-
responding toeach (i,j) pair fori,j ≥ 2. If wewant to achievea familywise error rate(FWER)
ofα,wecanrejectanullhypothesisifwegetap-valuelowerthan α (Bonferronicorrection),or
N
1−(1−α)N (Šidákcorrection).
Empirical Results. In Fig.7 we show the how this estimator is useful for testing a fixed-effect
behaviour. Inparticular,foreachplot,wesimulate200matricessatisfyingtheassumptionsfrom
Corollary3and200matricessatisfyingtheassumptionsfromProp.2. Morespecifically,weusethe
causaldagPythonlibrarytogenerateaDAGwithrandomweights(weusethedefaultparameters).
We fix the node we observe (Z ) and the context variables Z . For each simulated matrix, we
X C
25RIBOTSQUIRESUHLER
condition on a different set of values (z ), sampled from a uniform distribution over [0,10]. The
C
nodeswhereweinterveneonchangeforeachmatrix. Weperformado-interventiononeachnode
withprobability0.2andthevaluesweusedarealsosampledfromauniformdistributionover[0,10].
Forthematriceswherethefixedeffectbehaviorisnotsatisfied, wedonotinterveneonthesame
nodesforallthematrix,sowefollowtheprocedurementionedbeforeforeachrow.
We observe how, as the size of the matrix and the number of observed samples increase, the
statisticweproposecanfullydistinguishthetwotypesofmatrices.
Figure7: ROCcurves. mdenotesthesizeofthematrix,i.e. Y ∈ Rm×m. sdenotesthenumberof
samplesobservedpereachentry.
F.3. Testformean-over-contexts
Tobeprecise,beforetestingwhetherornotweourmatrixfollowsatwo-wayfixedeffectmodel,we
shouldtestwhetherithasonlyaone-wayfixedeffect. Thatis,testifwecanusemeanoveractions
orcontexts. Forexample,totestwhetherweonlyhaveafixedeffectalongrows,insteadofEquation
5wewouldhave
Y −Y = 0 ∀i,j ̸= j′
ij ij′
26CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
Andwecanrepeatallthereasoningfromtheprevioussectiontoobtainanappropriateestimator. The
caseofafixedeffectalongcolumnswouldbecompletelyanalogous.
F.4. TestforSyntheticInterventions
Agarwal et al. (2020) propose a hypothesis test for the SI estimator, so we could use it for our
particular case to check the linear span inclusion assumption. Moreover, as mentioned before,
assuming Gaussianity as in Prop.2 we can also test homoscedasticity within rows. Therefore,
constructingsuchtestsisaninterestingfuturedirection.
27RIBOTSQUIRESUHLER
AppendixG. FurtheranalysisonPRISMRepurposingdataset
G.1. SVDanalysis
InSection5,wearguedthatgettinghighR2 wasdifficultbecausethePRISMdataisrelativelynoisy.
HereweanalyzetheSingularValueDecomposition(SVD)ofourmatrixY shownFigure4. Itis
important to mention that here we are showing the SVD computed on all the matrix. That is, we
arealsousingtheentriesthatweresupposedtobemissingandthatwewanttoimputeasamatrix
completiontask. Therefore,theresultsshowninthissectionaremerelyananalysisofourdata,but
wearenotgoingtousethemfortheimputationtask.
In Figure 8, we show the Singular Values of Y. We can see that they are relatively diffuse,
suggesting that there is a considerably amount of noise in the data. For example, the explained
(cid:18) (cid:19)
(cid:80) σ2
variancebythetop10SVDvectors i≤10 i isjust78%.
(cid:80) σ2
i i
TheEckart-Youngtheorem(EckartandYoung,1936)statesthatthebestrankrapproximationto
amatrixisgivenbytruncatingitsSVDtothetoprsingularvalues. Wecanusethisresulttoestablish
upperboundsinperformancethatweshouldexpectfrommatrixcompletionapproaches. Forinstance,
mean-over-contextsproducesarank1approximationofourmatrix,sothisapproximationmustbe
worse(i.e. smallerR2)thantherank1truncatedSVD.
InFigure8,wealsoshowtheR2 valuesforthetruncatedSVDusingthemissingdatapattern
liketheonefromFigure5,with25%ofmissingentries. Itisimportanttoremarkthatinthiscase
wearecomputingtheSVDusingallthematrixY (includingmissingentries),sothisisnotavalid
completionapproach. Itshouldbeunderstoodastheoptimalperformancewecanexpect.
Generally, it is helpful to see how the first singular values pairs look like. Let S be rank 1
1
truncated SVD of Y. In Fig.9 we observe that S has almost constant rows, so it seems to be
1
capturingthefixedeffectinthatdirection. Thisindicatesthatthefixedeffectalongrowsismuch
moreimportantthanthefixedeffectalongcolumns. Therefore,wemayexpectthatthemean-over-
contexts is going to be a considerably good baseline. We also see how in Y −S we lose a big
1
partofthisfixedeffectalongrows,andourmatrixseemstohavelessvariability. Thissuggeststhat
subtractingthemean-over-contextsmayleadustobetterpredictions(seeSection4). Finally,SVD2
alsoseemstocapturesomeofthisfixedeffectbehavior,butinasmallerorderofmagnitude.
28CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
Figure8: DifficultyingettinghighR2 values. ThesingularvaluesofY presentarelativelyheavy
tail. Thismakesitdifficulttoobtaingoodmetricsinourpredictions. ThetruncatedSVD
of rank r is the best rank r approximation of a matrix. Therefore, the R2 obtained by
thetruncatedSVDmaybeconsideredasanupperboundforourcompletionalgorithms.
We should not expect to achieve an R2 of 0.90 as this would involve finding the best
approximationofrank80.
Figure9: One-wayfixedeffectbehavior. ThetruncatedSVDofrank1(SVD1=S )hasapproxi-
1
matelyconstantrows. Thissuggeststhatthereisafixedeffectforactionssomean-over-
contextsshouldbearelativelygoodestimator.
G.2. MissingpatterninCMAPdataset
AswementionedinSection5.1,inbiologyapplicationsweusuallydonothaveamissingatrandom
pattern. InFig.10weshowtheentriesobservedfortheCMAPdataset. Weusethisexampleasa
startingpointforthemissingdatapatternconsideredinSection5.1.
29RIBOTSQUIRESUHLER
Figure10: Missing pattern NOT at random. Availability matrix for CMAP dataset. A black
rectanglemeansthatwehaveobservedthatentry. Columnshavebeensortedfromleftto
rightaccordingtothenumberofavailableentries(foreachcolumn). Inasimilarway,
rowshavebeensortedfrombottomtotop. FigureobtainedfromSquiresetal.(2022).
G.3. Analternativemissingdatapattern
ThemissingdatapatternsfromSection5.1mayseemveryartificial,aswehaveaconstantnumberof
observedrowsandcolumnsforeachmissingentry. InFig.11weproposeanalternativepattern,still
followingthemotivationfromtheCMAPdataset(seeAppendixG.2),andweshowtheperformance
ofdifferentalgorithmsforthissetting. Theresultsarenotsignificantlydifferenttotheonesfrom
Fig.6.
Figure11: Non-squaremissingdatapattern. Here,wedonothaveaconstantvalueofrowsand
columnsforeachmissingentry.
30CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
G.4. Performanceofthealgorithmsdependingontheamountofdataobserved
InSection5.2,weshowedtheperformanceofdifferentmatrixcompletionapproachesforaspecific
numberofobservedentries. Theexperimentweshowhereconsistsonvaryingthenumberofrows
andcolumnsobservedpermissingentry. Thisparameterwillrangefrom5until275. Foreachcase,
thesquarecorrespondingtothemissingvalueshasconstantsize284×284. Thereasonforhaving
constant size square (same number of missing entries) is that the MSE is computed on the same
numberofsamples.
Wehaveusedthefollowingbootstraptechnique. Foreachcaseweshuffletherowsandcolumns
of the full matrix and take a sub-matrix of our desired size (e.g. 289×289). Then, we take the
corresponding missing data pattern and compute the R2 for each algorithm. We repeat the same
process for 20 different shuffles. See Fig.12. As a takeaway, it is worth noting that Synthetic
Interventionsbenefitsfromsubtractingthemean-over-contexts,especiallyinthelowdataregime.
Note that we can think about Fig.6 as taking slices from Fig.12. In particular, Fig.6 shows
thatCollaborativeFilteringapproachdoesnotimprovethebaselinealgorithmsandKernelLinear
RegressionisnotsignificantlybetterthanthestandardSyntheticInterventionsapproach. Forthat
reason,wedonoteincludetheminFig.12.
31RIBOTSQUIRESUHLER
(a) (b) (c)
(d) (e)
(f) (g)
(h)
Figure12: Dependence on number of observations. "obs" denotes the number of rows and
columnsobservedforeachmissingentry. Figuresfromtheright-handsidearefocused
onthelow-dataregime. WecanseesomespikesinthecurvescorrespondingtoNNM,
theseareproducedbyachangeintheregularizationparameter(e.g. from10−3 to10−4).
32CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
G.5. Performanceofallthematrixcompletionalgorithmsconsidered
InSection5.2,weshowedtheperformanceofsomematrixcompletionapproaches. Here,inFig.13,
weincludemoreapproachesandsomevariationsofthepreviousones.
Inparticular,weseethatthenearestneighborsapproachesarenotsignificantlybetterthanthe
baselines. Doubly Robust NN is the method proposed in Dwivedi et al. (2022). CF10 stands for
a 10-nearest neighbor Collaborative Filtering approach. Finally, it is worth noting that SI and its
variationsoutperformtheotherapproaches.
Figure13: Allthematrixcompletionapproachesconsidered. Hereweshowtheperformanceof
allthealgorithmsconsidered,usingthemissingpatternfromFigure5.
G.6. Removalof"killerdrugs"
Inthissection,werunthesameexperimentsoncertainportionsofthewholematrixY. Looking
backatouroriginalmatrixinFig.4,weobservethattherearesomeblue(negative)rowsthatseem
tobeconstantalongallthecolumns. Thiswouldmeanthatthisparticulardrugiskillingallthecell
lines.
Inlightofthiseffect,wecalladruga"killerdrug"ifitkillsmorethanacertainpercentageof
celllines. Itisnotclearwhichthresholdweshoulduse. InFig.17,weshowthenumberof"killer
drugs"dependingonthethresholdwedefine.
Forthefollowingresults,weusedathresholdof80%. TheresultingmatrixcanbeseeninFig.14.
Notethatwehaveremovedalmostallthebluelinesthatwementionedbefore. Nowthefixedeffect
alongrowsdoesnotseemthatclear. Inparticular,theMSEofthebaselinedecreasessignificantly
compared to the previous setup. This means that the task of completing the task is easier for the
baselinemodel. Inconsequence,gettinghighR2 valuesisgoingtobemuchharder.
33RIBOTSQUIRESUHLER
InFig.15werepeatthesameexperimentfromFig.8forthisnewdata. Asexpected,nowtheR2
valueweobtainaremuchlower. Finally,weshowtheperformanceofthedifferentmatrixcompletion
approachesonthismatrixinFig.16.
Figure14: Matrixobtainedwhenweremove"killerdrugs". Here,a"killerdrug"isadrugthat
killsmorethan80%ofcelllines.
Figure15: Getting high R2 values is even more difficult than before (compare it with Fig.8).
Afterremovingthe"killerdrugs",MSE(average)decreasesfrom0.87to0.28. Thismeans
thatthecompletiontaskiseasierforthisbaseline,soitismoredifficulttoobtainhigh
R2 values.
34CAUSALIMPUTATIONFORCOUNTERFACTUALSCMS
Figure16: Performanceafterremoving"killerdrugs". Thegapbetweenmean-over-actionsand
SIissimilarfromtheoneinFig.6,despitehavingmoreroomforimprovement.
Figure17: Definitionofa"killerdrug". Hereweshowthenumberofdrugsthatkillmorethan
threshold % of cell lines. We consider that a cell line is dead if the viability score is
negative. Inourexperiments,weusedathresholdof80%.
35