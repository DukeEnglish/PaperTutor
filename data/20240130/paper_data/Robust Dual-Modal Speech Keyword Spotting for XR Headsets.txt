Robust Dual-Modal Speech Keyword Spotting for XR Headsets
ZhuojiangCai ,YuhanMa ,andFengLu ,SeniorMember,IEEE
Hey
SV po ec ea cl h Cortana Volume up Open mail
Ultrasound
Echo
(a) Proposed system setup (b) Typical environment (c) Noisy environment (d) Silent environment
Fig.1:Ourdual-modalsystemsignificantlyextendstheavailablescenariosforspeechinteractionwithXRheadsets.(a)Weproposea
vocal-echoicdual-modalkeywordspottingsystem.(b)Intypicalenvironments,suchasworkingorentertainingindoors,dual-modal
systemperformsaccuratelyandcanblockoutthespeechofspeakersnearby.(c)Innoisyenvironments,suchasonthestreetorin
thesubway,ourdual-modalmethodoutperformstheirsingle-modalcounterparts.(d)Whenunableorunwillingtomakeavoice,for
example,whenothersareworkingorsleepingnearby,oursystemcontinuestofunctioneffectively.
Abstract—WhilespeechinteractionfindswidespreadutilitywithintheExtendedReality(XR)domain,conventionalvocalspeech
keywordspottingsystemscontinuetograpplewithformidablechallenges,includingsuboptimalperformanceinnoisyenvironments,
impracticalityinsituationsrequiringsilence,andsusceptibilitytoinadvertentactivationswhenothersspeaknearby.Thesechallenges,
however,canpotentiallybesurmountedthroughthecost-effectivefusionofvoiceandlipmovementinformation.Consequently,we
proposeanovelvocal-echoicdual-modalkeywordspottingsystemdesignedforXRheadsets.Wedevisetwodifferentmodalfusion
approchesandconductexperimentstotestthesystem’sperformanceacrossdiversescenarios.Theresultsshowthatourdual-modal
systemnotonlyconsistentlyoutperformsitssingle-modalcounterparts,demonstratinghigherprecisioninbothtypicalandnoisy
environments,butalsoexcelsinaccuratelyidentifyingsilentutterances. Furthermore,wehavesuccessfullyappliedthesystemin
real-timedemonstrations,achievingpromisingresults.Thecodeisavailableathttps://github.com/caizhuojiang/VE-KWS.
IndexTerms—Speechinteraction,extendedreality,keywordspotting,multimodalinteraction.
1 INTRODUCTION
Inrecentyears,ExtendedReality(XR)headsets,includingVirtualReal- Conventionalvocalkeywordspottingmethodsencountervarious
ity(VR),AugmentedReality(AR),andMixedReality(MR)headsets, challenging usage scenarios. Specifically, we have identified three
havegainedwidespreadattention. Thesedevices,servingasbridges common and demanding situations: 1) In noisy environments such
betweenthevirtualandrealworlds,havethepotentialtoprofoundly asbustlingstreets,crowdedmalls,ornoisypublictransport,keyword
reshapehowpeoplelive, work, andentertainthemselves. Withthe spottingaccuracysignificantlydegrades.2)Usersoftencannotvocalize
increasingpopularityofMRandARheadsetsliketheAppleVision whenothersareworkingorrestingnearby,orduetoprivacyconcerns
Pro and Microsoft HoloLens, XR headsets are finding utility in an andsocialawkwardness. Insuchcases,vocalkeywordspottingfails
ever-expandingarrayofscenarios. entirely.3)Vocalspeechkeywordspottingsystemsaresusceptibleto
Efforts to develop natural and effective user interaction methods interferenceandevenfalsetriggeringwhenothersarespeakingnearby.
with XR headsets have been a critical research focus. A growing Thesechallengeshighlightthelimitationsoftraditionalvocalkey-
numberofheadsets, includingtheAppleVisionPro, haveshowna word spotting approaches and emphasize the need for more robust
preferenceforintuitiveinteractionmethodssuchasspeech,gestures, andversatilesolutions. Addressingthesecondchallenge,significant
and gaze. These methods, in comparison to tools like controllers, progresshasbeenmadeinthefieldofsilentspeechinterfaces.Various
are more readily accepted and user-friendly. Among them, speech typesofdata,includingfacialimagery[11,52],ultrasonicimaging[22],
interactioncanbeusedformenuselection,locomotioncontrol,and EMG[17,25,48],motionsensing[37],strainsensing[24],andother
combinedwithgazeforobjectmovement,etc.However,theextensive sensing forms have been investigated for recognizing silent speech.
useofspeechinteractionhasbeenhinderedbylimitationsinspeech Furthermore, some research has leveraged the transmission and re-
recognitionmethods,includingAutomaticSpeechRecognition(ASR) ceptionofultrasonicwavestodetectmouthmovements[13,55,56],
andSpeechKeywordSpotting(KWS),whichlackrobustnessacross offeringcontactlessandcost-effectivesolutions.EchoSpeech[55],for
variousreal-worldenvironments. instance,employedFrequency-ModulatedContinuousWave(FMCW)
anddemonstratedimpressivesilentspeechspottingcapabilitiesusing
off-the-shelfspeakersandmicrophonesmountedonglassesframes.
• ZhuojiangCai,YuhanMa,andFengLuarewithStateKeyLaboratoryof
However, thisworkdidnotexploretheuseoflow-frequencyvocal
VirtualRealityTechnologyandSystems,SchoolofComputerScienceand
speechinformationfrommicrophoneaudio,whichcouldextendits
Engineering,BeihangUniversity,Beijing,China.E-mail:
applicationtoabroaderrangeofvocalspeechrecognitionscenarios,
{caizhuojiang|raphael.mayuhan|lufeng}@buaa.edu.cn.
therebyaddressingtheremainingtwochallenges.
• ZhuojiangCaiandYuhanMacontributeequallytothiswork.FengLuisthe
correspondingauthor. Recognizing that vocal speech and lip movement can contribute
Manuscriptreceivedxxxxx.201x;acceptedxxxxx.201x.DateofPublication from different angles to the comprehension of a speaker’s speech,
xxxxx.201x;dateofcurrentversionxxxxx.201x.Forinformationon anddrawinginspirationfromrelatedresearchinaudio-visualspeech
obtainingreprintsofthisarticle,pleasesende-mailto:reprints@ieee.org. recognition[9,34,42,57]thatsupportsthisidea,wepositthatvocal
DigitalObjectIdentifier:xx.xxxx/TVCG.201x.xxxxxxx speechinformationandmouthmovementinformationobtainedthrough
ultrasonicechocouldsimilarlyofferdiverseperspectivesonaspeaker’s
4202
naJ
62
]CH.sc[
1v87941.1042:viXra
Modality
Modality
Vocal
Echoic
Fusionspeech.Theintegrationofthesetwomodalitiesmayprovideawealthof basedonConvolutionalNeuralNetworks(CNNs)[6,19,41,44,53]has
priorknowledgeforspeechrecognition,potentiallyleadingtoimproved garneredsignificantattentionduetoitsstraightforwardarchitectureand
performance. easeoftuning[44].Notably,TangandLin[44]appliedtheconceptof
Therefore,thispaperintroducesanovelvocal-echoicdual-modal residuallearningtoKWS,creatinghigh-performancemodelswitha
keywordspottingmethodforXRheadsets.Bycombiningvocalspeech smallfootprint.Subsequently,TC-ResNet[6]replacedtheoriginalcon-
andultrasonicechofeatures,itachievesrobustkeywordspottingin volutionalmodulesinresidualblockswithtemporalconvolutions,and
variousscenarios.Wedesignedandimplementedthisdual-modalkey- DC-ResNet[53]introduceddepthwiseseparableconvolutions.These
wordspottingsystemonMicrosoftHoloLens2withcustomhardware. innovationsreducedtheparametercountandcomputationalcomplexity.
Wedevisedtwodifferentmodalfusionapproachesandverifiedtheir Kimetal.[19]introducedanovelnetworkarchitecturecalledBroadcast
effectiveness through experiments. Furthermore, we compared the ResidualLearning,whichconsistentlyoutperformspreviousmodels
dual-modalsystemwithsingle-modalsystemsinlow-noiseenviron- withanequivalentparametercount.However,despitedemonstrating
ments, diverse noisy environments, and scenarios with interference excellentperformanceonpublicdatasets,thesemethodsexperience
fromotherspeakers.Theresultsdemonstratethatourproposeddual- significantperformancedegradationinnoisyenvironments[7,30,35].
modalsystemconsistentlyoutperformsitssingle-modalcounterpartsin Thisissuewillbethoroughlyinvestigatedinthispaper.
themajorityofscenarios,withoutcompromisingsilentspeechspotting
performance. Finally, weappliedthesysteminpracticalexamples,
2.2 SilentSpeechInterfaceonWearables
achievingpromisingresults.Ourworksignificantlybroadensthescope
ofspeechkeywordspottingapplications. SilentSpeechInterface(SSI)canbeviewedasageneralizedformof
Insummary,ourcontributionscanbeoutlinedasfollows: speechrecognition.Itinvolvescapturingnon-vocalinformationusing
varioussensorstorecognizespeechutterancesfromusers.
• Weproposeavocal-echoicdual-modalspeechkeywordspotting
ContactingSSI.Inwearabledevices,SSIsimplementedthrough
(KWS)system,enablingrobustspeechrecognitioninabroader
diversesensortechnologieshavebeenwidelyresearched. Someap-
rangeofscenarios.
proachesplacemagnetometers[1,3,15]orcapacitivesensors[20,28]
• WeconductedanablationstudytodesignalighterCNNmodel insidethemouthtocapturemovementsofthemouthandtongue,re-
fortheechoicmodalKWS,reducingitsdemandforcomputing constructingspeechutterances.Othersemploysensorsattachedclosely
resourcesintheheadset. totheskin,recognizingspeechthroughformsofinformationlikeul-
trasoundimaging[22],Electromyography(EMG)[17,25,48],motion
• Weconductedexperimentstoassessthesystem’sperformance
sensors[37],orstresssensors[24]. However,thesemethodsrequire
invariousnoisyenvironments,situationswithnearbyspeakers,
skincontactorevenintrusionintothemouth,whichmaybeuncomfort-
andsilentscenarios,demonstratingthatourdual-modalsystem
ableforusers[55].
outperformstraditionalvocalsystemsinallofthesescenarios.
Contact-freeSSI.Contact-freeSSIsofferamorecomfortableuser
experience, leading to increased research interest. Some efforts at-
2 RELATEDWORK
tempted to install cameras on headphones [5] or neck-mounted de-
2.1 VocalSpeechKeywordSpotting vices[21,54];however,thesemethodsfacedchallengesrelatedtohigh
Keyword Spotting (KWS) is the task of detecting a predefined set powerconsumptionandprivacyrisks. Additionally,someworksuti-
ofkeywordsfromanaudiostream. ComparedtoAutomaticSpeech lizeactiveacousticmethods,employingmicrophonesandspeakerson
Recognition(ASR),KWScanbedeployedonedgedeviceswithlow smartphones[13],VRheadsets[56],andglass-frames[55]toachieve
computationalrequirementsandwithouttheneedforcloudconnectivity, low-power,high-performancesilentspeechkeywordspotting.Among
eliminatingprivacyconcerns.Therefore,itfindsextensiveapplications them,CELIP[56]requiresasetupdirectlyfacingtheuser’smouth,
in various domains such as user interface interactions, smart home includingapairofrelativelylargemicrophonesandspeakers,whereas
control,andcommandtriggers. EchoSpeech[55]integratescompactcomponentsunderthelowerframe
Typically,KWSreferstothedetectionofvocalspeech. However, ofglasses,ensuringalessobtrusivedesign.
inthispaper,wealsoemployasilentspeechkeywordspottingusing Althoughthesecontact-freemethodsachievesilentspeechrecogni-
echoicmodality.Todifferentiate,werefertotheconventionalKWSas tionwithlowpowerconsumptionandhighperformance,theirpotential
vocalspeechkeywordspottinginthiscontext. tointegratewithvocalkeywordspottingsystemsforenhancedperfor-
ASR-basedKWS.SomestudiesutilizeASRsystemstoconvert manceandbroaderapplicationscenariosremainsunexplored.
speechsignalsintotextandthenidentifykeywordsthroughtextmatch-
ingtechniques[12,31,40].Whilethisapproacheliminatestheneedfor 2.3 SpeechInteractioninXR
specializedtrainingofpredefinedkeywords,offeringgreaterflexibility,
itinheritsthedrawbacksofASR,includingcomputationaldemands SpeechinteractionallowsuserstocontroldevicesinXRthroughverbal
andprivacyconcerns. commands,eitherindependentlyorincollaborationwithotherinterac-
HMM-basedKWS.TheKWSsystembasedonHiddenMarkov tionmethodslikecontroller,gesture,andgaze.Thisapproachprovides
Models(HMM)wasproposedthreedecadesago[38,39,51]. Inthis amoreflexibleanduser-friendlywaytooperatedevicesinXR.
approach,speechsamplesofeachkeywordareusedtotrainthecor- Hand-freeInteraction.Theuseofspeechcommandsforhands-free
respondingHMMforthatkeyword,andnon-keyword(filler)speech interactionisacommonpracticeinXR[14,16,36].Forinstance,voice
segmentsareusedtotrainafillerHMM.Atruntime,theinputaudio commandsareutilizedtocontrollocomotioninVRwithouttheneed
streamismatchedagainsttheseHMMmodels.TheViterbialgorithm forhandgestures[16].Indentalimplantsurgerieswherehandsmight
iscommonlyemployedtofindthemostlikelystatesequence. Ifthe beoccupied,speechcommandscanreplacemenuclicksinAR[36].
matchingprobabilityexceedsapredefinedthreshold,thesystemiden- Studies suggest that voice-based interactions can be as efficient as
tifiestheinputascontainingthekeyword. Althoughthesesystems gesture-basedinteractions[26]andfreeuphandsforothertasks[32].
performwell,executingmultipleHMMmodelmatcheswiththeViterbi MultimodalInteraction. Otherstudieshaveexploredinteractive
algorithmstillrequiressignificantcomputationalpower[4,29]. methodscombiningspeechcommandswithgestureandgaze.Forex-
Deep KWS. In the past decade, the rapid advancement of deep ample,usinggesturestoselectatargetandthenexecutingoperations
learninghasledtoextensiveresearchandapplicationofdeepKWS throughspeechcommands[33]isconsideredmoreefficientandaccu-
approches[29].Theseadvancementshaveresultedinreducedcompu- ratethanrelyingsolelyongestures[26]. Anotherapproachinvolves
tationalcomplexityandimprovedperformanceofKWSsystems.Com- using gaze and speech to control the movement of objects [10,18].
monarchitectures,suchasfully-connectednetworks[4]andRecurrent Wangetal.[49]investigatedamultimodalinteractionmethodinAR
NeuralNetworks(RNNs)[23,43,58],havebeenstudiedandproven thatcombinesspeech,gazeandgestureanddemonstrateditssuperior
effectiveinKWSsystems.Inspiredbycomputervisionresearch,KWS efficiencycomparedtosingle-modalanddual-modalmethods.(3.2)
Speakers ESP32
Low-pass Vocal Modality (3.3)
Filter (3.5)
prediction
Microphones Preprocess CNN vectors
vocal audio
Fusion
Mounted Network
audio High-pass Echoic Modality (3.4)
XR Headset PC Filter
Preprocess CNN
echoic audio keyword
output
Fig.2: Overviewofvocal-echoicdual-modalKWSsystemforXRheadset. (left)Hardwarediagramofexperimentalequipment. Thespeakers
andmicrophonesaremountedontheXRheadset,connectedtoanESP32,whichsendstheaudiotoaPCoverthenetwork.ThePCisusedfor
algorithmimplementationandexperiments,anditsendsthedetectedkeywordsbacktotheapplicationontheheadset.(right)Algorithmflowchart.
TheaudioisseparatelyfilteredandinputintothevocalandechoicmodalKWSpipelines.Thepredictedvectorsobtainedfromthesepipelinesare
thenfedintothefusionmoduletogeneratethekeywordoutput.
3 DUAL-MODALKEYWORDSPOTTINGSYSTEM
3.1 SystemOverview ESP32 and
add-on board
Ourdual-modalkeywordspotting(KWS)systemrepresentsanelegant
enhancementofitscommonlyusedsingle-modalvocalcounterpart.It (b)
Microsoft
maintainsaudiostreamsasthesoleinputandpredictedkeywordsas HoloLens 2
theoutput.
Unliketraditionalsystemsthatdirectlyuseinputaudioforvocal
Microphones × 2
KWS, our system separates audio into vocal and echoic modalities
usingbandpassfilters. Theechoicaudiooriginatesfromultrasonic Speakers × 2
(a) (c)
wavesemittedbyspeakersandreflectedofftheuser’sskinnearthe
mouth.Thesetwosegmentsofaudioarethenseparatelyprocessedby
Fig.3:HardwareSetup.(a-b)Frontviewandbottomviewofourimple-
thevocalandechoicmodalKWSpipelinesbeforetheirpredictionsare
mentationwithHoloLensheadset.(c)ESP32Add-onboard.
fusedintoasingleoutput.
Thedifferenceinfrequencyrangesbetweenvocalandultrasonic
echoicaudiomakesthissystemfeasible.Researchhasshownthatthe
fundamentalfrequency(F0)ofvocalspeechtypicallyfallswithinthe notingthatthepositionsofthecomponentsintegratedintocommercial
rangeofapproximately100-240Hz. Evenconsideringtheharmonic XRheadsetsdonotalignoptimallywiththerequirementsofoursystem.
informationthatmaybepresentinvocalspeech, theupperlimitof Therefore, we employed a microcontroller board and a custom-
thefrequencybandsusedinconventionalspeechrecognitionMel-filter designedadd-onboardtoconnecttwopairsofsmalloff-the-shelfspeak-
banksisusuallyaround5000Hz.Incontrast,theultrasonicwavesused ersandmicrophones,whichweremountedonMicrosoftHoloLens2
inoursystemoperateatfrequenciesabove17kHz.Hence,bothmodali- toimplementthehardwareforthissystem,asshowninFig.3(a-b).
tiesofaudiocanbeconcurrentlycapturedusingthesamemicrophone, Thispositionalconfigurationhasdemonstratedstrongperformancein
providingrepresentationsoftwomodalitiesforthesamespeech. glasses-frames[55]forechoicmodalKWS.
BothmodalKWSpipelinesinthesystememploylightweightdeep Themicrophonesandspeakersarebothmountedontheloweredge
learning approaches. For vocal KWS, many methods have already ofthelensesofHoloLens2.Twospeakersarealongtheloweredgeof
achievedexcellentresultsintypicallow-noisescenarios. Inthispa- therightlens,whiletwomicrophonesarealongthatoftheleftlens.The
per,wedirectlyutilizeoneofthestate-of-the-artconvolutionalneural directionofthespeakersandmicrophoneholesisorienteddownward.
networks(CNNs)forvocalKWS,whichwillbediscussedinSec.3.3. Thedistancebetweenthespeakerandthemicrophoneclosertothenose
ForechoicKWS,EchoSpeech[55]usesaResNet18asthebackbone is7.2cm,whilethedistancebetweenthetwospeakersandbetweenthe
network.Wesignificantlyreducedthenetwork’sparameterswithnegli- twomicrophonesis1.8cm.Additionally,Theaverageverticaldistance
gibleperformancedegradationthroughanablationstudy,enablingitto fromthemicrophoneplanetothehorizontalmidlineofthemouthis
runwithlowerpowerconsumptionandlatency.Thiswillbediscussed 3.9cm,measuredacross15participantswearingthedevice.
inSec.3.4. Duringthekeywordspottingprocess,userutterancesarecaptured
ThepredictionsfromthetwoKWSpipelinesarefusedinthefinal bythemicrophones. Simultaneously, thespeakersemitcontinuous
stage of the system. We propose two fusion methods: reliability- ultrasonicwaves,withaportionbeingreflectedofftheuser’sfaceand
basedfusionandMLP-basedfusion. Bothhavebeenexperimentally mouth,subsequentlycapturedbythemicrophonesasechoes.Hence,
shown to achieve higher accuracy than single-modal results. This the signals received by the microphones can be separated into two
demonstratesthatourfusionmethodscancomprehensivelyconsider components:vocalmodalityandechoicmodality,enablingdual-modal
differentdimensionsofinformationfromthetwomodalitiesforthe keywordspottingandfusion.
samespeech,resultinginmoreaccuratekeywordspotting.Thesetwo Intheexperimentalsetup,anESP32developmentboardisemployed
fusionmethodswillbeintroducedinSec.3.5. tocontrolspeakerplayback,receivemicrophonesignals,andtransmit
themtoacomputerorHoloLensforsubsequentdetectionandfusion.
3.2 HardwareImplementation
Thedevelopmentboard,speakers,andmicrophonesareallcommon
Considering that the performance of the echoic modality is highly productspurchasedonline.ThedevelopmentboardusedistheESP32-
dependentonthepositionsofthespeakersandmicrophones,itisworth S3-DevKitC-1-N8R8, the speakers are OWR-05049T-38D, and the(a) (b) (c) microphone.Bycomputingthecross-correlationbetweentheechoes
Frequency -16 -16
andthetransmittedsignals,thecorrelationatdifferentsamplingoffsets
-12 -12
-8 -8 withinonechirpcanbeobtained.Thesampleshiftisproportionalto
-4 -4 thedistancefromthemicrophoneandspeakertothereflectingmedium.
0 0 Therefore,thecross-correlationreflectsthemagnitudeofreflectionat
4 4
differentdistances,allowingfortheresolutionof0.357cmwhenthe
8 8
12 12 samplingrateis48kHz.Thislevelofresolutionenablesthedetection
Time 16
0 0.5 1
16
0 0.5 1
ofpositionalchangesoftheskinneartheuser’smouthrelativetothe
Time (s) Time (s) XRheadsetduringspeech.
EchoProfile.Calculatingthecross-correlationforeachconsecutive
Fig.4:(a)Thefrequency-timediagramoftheFMCWsignalsinthetwo chirp of the received signal produces a correlation graph with time
frequencybands.(b-c)OriginalanddifferentialEchoProfile. onthehorizontalaxisandsampleshiftontheverticalaxis,referto
astheEchoProfile[27]. ThedifferentialEchoProfile, obtainedby
differencingtheEchoProfilealongthetimeaxis,revealsthetemporal
microphonesareICS-43434.Furthermore,anadd-onboardhasbeen movementcharacteristicsofthemouth(seeFig.4(b-c)).
designedtointerfacethemoduleswiththeESP32anddecodeaudio,as CNNModel. InechoicmodalKWS,thedifferentialEchoProfile
showninFig.3(c). serves as the input to the CNN network to predict keywords in the
audio.Previousstudy[55]utilizedResNet-18asthebackbonenetwork,
3.3 VocalModalKWS
achievinggoodperformance. However,ResNet-18hassignificantly
Keyword Spotting (KWS) in the vocal modality is a relatively ma- more parameters compared to the network used in vocal modality
ture technology, with a substantial amount of state-of-the-art work KWS,leadingtoadditionalcomputationaldemands. Therefore,we
currentlyutilizingdeeplearningapproaches[2,6,19,44,46]. These usedResNet-18asthebaselineandimprovedthearchitectureofthe
methodsinvolveconvertingaudiointoMel-frequencycepstralcoef- echoic modality network through ablation study. We achieved this
ficients(MFCCs)andfeedingtheMFCCsintoconvolutionalneural by reducing the network width and replacing regular convolutional
networks(CNNs)orotherneuralnetworkstoproducekeywordspotting moduleswithdepthwiseseparableconvolutions,resultinginmultiple
results,whichisthemethodologyoursystemadopts. modelswithdifferentparametercounts.Theablationstudyisdetailed
In our dual-modal KWS system, audio is first processed with a in Sec. 4.3. We selected ResNet-18-1/4-DS as the network for our
low-passfiltersettoa10kHzcutofffrequencytoisolatevocalaudio. echoicKWSpipelinebecauseitstrikesabalancebetweenperformance
Thisaudioisthensubjectedtoaseriesoftransformations,including andparametercount.
pre-emphasis,framing,windowing,fastFouriertransform(FFT),Mel-
frequencywarping,logarithmicscaling,anddiscretecosinetransform,
3.5 FusionStrategies
ultimately resulting in MFCCs features [8,29]. These features are
fedintoabroadcastresidualbasedCNNarchitecture[19]togenerate Thefusionstrategycomprisestwoobjectives:ononehand,fullylever-
predictionvectors. aging the distinct representations of user speech provided by both
Differingfromsingle-modalvocalKWSsystems,wheretheargmax modalitiestoenhancepredictionaccuracy;ontheotherhand,remaining
inthepredictionvectortypicallyservesasthesystemoutput,wecom- unaffectedbyunreliableinformationfromonemodalityandutilizing
binethisvectorwiththeonefromtheechoicmodalityKWSpipeline trustworthyinformationfromtheothermodality.
andinputbothvectorsintoafusionmoduletoobtainauniquepredic- Toachievethesegoals,weexploretwofusionmethods:reliability-
tionresult. basedfusionandMLP-basedfusion. Theformeremploysmanually
craftedfeatures,calculatingreliabilityindicatorsofpredictionvectors
3.4 EchoicModalKWS
frombothmodalities’pipelineoutputs.Theseindicatorsarethenadap-
TheechoicmodalKWSpipelineutilizesanactiveacousticapproach. tivelyusedtodeterminefusionstrategiesandperformfusionoperations.
SpeakersinstalledatthebottomoftheXRheadsetemitFrequency Thelatteremploysaneuralnetworkapproach,utilizingaMulti-Layer
ModulatedContinuousWaves(FMCW)intwofrequencybands. Si- Perceptronmodeltolearntherelationshipbetweenpredictionvectors
multaneously,microphones,alsopositionedatthebottom,receiveaudio frombothmodalitiesandthefusionresults.
signalsreflectedofftheskinnearthemouth(echoes).Bycomputing
thecross-correlationbetweentheechoesandthetransmittedsignalsin
3.5.1 Reliability-basedFusion
eachFMCWframe,featuresaboutmouthmovementscanbeobtained,
whichcanbelearnedbyaConvolutionalNeuralNetwork(CNN)to Fromthetwoobjectives,asweaimtopreventtheinfluenceofunreliable
achievekeywordspotting. information from individual modalities (for instance, the impact of
WhyFMCW?Differentactiveacousticmethods,includingDoppler vocalmodalitypredicting"silence"duringsilentspeechrecognition),
effect,CIR,andFMCW,havebeenemployedinpriorworksforsilent anaturalapproachistoassessthereliabilityofpredictionsfromeach
speechinterfaces. Allofthesemethodshavethepotentialtoreplace individualmodality.Therefore,wedevisedareliability-basedfusion
ourechoicmodalitypipeline,asourfusionapproachsolelytakesthe strategy,adaptivelyfusingmodalitiesbasedonreliabilityindicators.
keyword classification probability vectors as inputs. Among these Reliabilityindicator.WeutilizedareliabilityindexbasedonN-best
methods, a previous study demonstrated the advantages of FMCW log-likelihooddifferenceandN-bestlog-likelihooddispersion. This
incapturingfacialmovements[27]. Therefore,wehavechosenthis fusionmethod,asdemonstratedbyPotamianosetal.[34],hasshown
methodtoensureoptimalresultsafterfusion. effectivenessintherealmofaudio-videointegration.
Transmittedsignal.OurtransmittedsignalsarechirpsinFMCW,
Thefirstindicatorisusedtomeasuretheclassdiscriminationability
wherethefrequency f linearlyincreasesovertimetwithineachchirp.
ofeachmodality,whilethesecondindicatorsupplementstheadditional
Thiscanberepresentedas f(t)= f +(f −f )×t/T,where f and f
l h l l h N-bestclasslikelihoodratiosmissinginthefirstindicator.Theamalga-
representthelowerandupperboundsofthefrequency,andTrepresents
mationofthesetwoindicatorsallowsforamorerationalevaluationof
thechirpperiod,asillustratedinFig.4(a).Inoursystem,twomicro-
themodality’sreliability.Thedefinitionsofthetwoindicatorsareas
phonessimultaneouslyemitFMCWsignalsintwofrequencybands:
follows:
17-20kHzand20.5-23.5kHz,withaperiodof12ms.Thisfrequency
N-bestLog-LikelihoodDifference:
rangeisconsideredultrasonicandiscompatiblewiththesamplingrates
ofcommercialmicrophonesandspeakers,whichoperateat48kHz.
FMC Cr Woss m-c eo thr or del [a 4t 7io ]n isb emas pe ld oyeF dM toC pW ro. ceT sh se thc er eo cs hs- oc eo sr rr ee cla et ii vo en dbb yas te hd
e
Lm,t= N−1
1
n∑ =N 2logP P(cid:0) (oo mm ,, tt || cc mm ,, tt ,, n1 )(cid:1) (1)
)mc(
ecnatsiD
)mc(
ecnatsiDN-bestLog-LikelihoodDispersion: Inordertoobtainaparametervectorofashigh-qualityaspossible,
weusedtheLatinHypercubeSampling(LHS)initializationmethodto
Dm,t=
2 ∑N ∑N
log
P (cid:0)(om,t|cm,t,n)
(cid:1). (2)
m caa px ai bm ili iz tye .c To hv ee ir na ig tie ao lf soth lue tia ov na oil ba tb al ie nep dar wam ithet fe er ws ep ra ic tee rafo tir ob ne st ote nr ase smar ac lh
l
N(N−1) n=1n′=n+1 P om,t|c m,t,n′ datasetisusedtoreplacetheoptimalsolutionintheinitialpopulationto
copewiththehightimecostofthegeneticalgorithmanditsdependence
Here,P(om,t|cm,t,n)denotesthelikelihoodofobservingtheresult
ontheinitialpopulation.
om,t giventheclasscm,t,n. Intheseequations,mandt representthe
Inadditiontodeterminingtheaforementionedeightparameters,it
modalityandtimerespectively,andnindicatestheclassranking.
is imperative to ascertain the target values for parameter ai,t when
Fusionprocess.Duringthefusionprocess,reliabilityindicatorsare
bothmodalitiesyield“silence”and“unknown”resultsseparately.We
firstemployedtoseparatelydeterminewhetherthepredictionvectors
needtodeterminefourparameters,denotedasav,s,av,u,ae,s,andae,u,
fromthetwomodalitiesareutilized.Whenbothmodalitiesareutilized,
correspondingtothemodificationswheneachofthetwomodalities
thereliabilityindicatorsarethenusedtocalculatethefusionexponent.
provides “silence” and “unknown” outcomes.Once the above eight
Todeterminetheutilizationofamodality,athresholdmechanism
parameters are established, we can employ a grid search to swiftly
[t , t ] has been implemented. A modality is considered reli-
l,m,t d,m,t determinethevaluesofthesefourparameters.
ableonlyifitsreliabilityindicatorsmeetspecificthresholdcriteria.
Ifonemodalityisdeemedunreliable,werelysolelyonthemodality
3.5.2 MLP-basedFusion
consideredreliabletoensuretheaccuracyofthefusionresults.Thisap-
proachpreventsinterferencefromtheunreliablemodality.Theboolean TheMLP-basedfusionmethodabandonsmanuallycraftedfeaturesand
variableRm,t indicateswhetherthemodalitiesexceedtheirrespective employsastraightforwardMulti-LayerPerceptron(MLP)modelto
thresholds,determiningtheirreliabilityandwhethertheirdatawillbe generatefusionresults.Itconsistsofaninputlayer,ahiddenlayer,and
includedinthesubsequentfusionprocess. ThecalculationofRm,t is anoutputlayer,witheachlayerfullyconnectedtothenext.Theinput
definedas: vectorisformedbyconcatenatingtheoutputvectorsobtainedfromthe
vocalandechoicmodalpipelines.Thisapprochcanberepresentedas
follows:
Rm,t=(Lm,t>t l,m,t)∧(Dm,t>t d,m,t). (3)
Insituationswherebothmodalitiesareconsideredreliable,theinfor- P(o f,t|c)=MLP([P(ov,t|c),P(oe,t|c)]). (6)
mationfrombothmodalitiesshouldbefullyutilizedtoobtainamore
accurateresult.Inthiscase,wewillreapplythereliabilityindicatorsto Here, P(o f,t|c) represents the fusion result at time t given class
determinethefusionexponentλv,t: c, P(ov,t|c) and P(oe,t|c) represent the prediction vectors for vocal
andechoicmodalities,respectively.Thesquarebracketsindicatethe
1 concatenationofthetwooutputvectors.
λv,t=
1+exp(cid:0) −∑4 i=1wiai,td
i,t(cid:1). (4)
Weutilizedtheself-recordeddatasetintroducedinSec.4.1totrain
themodel. Duringtraining, thisdataset wasrigorouslypartitioned,
Here, dt =[Lv,t,Dv,t,Le,t,De,t] corresponds to the four reliability withastrictdemarcationbetweenthedatausedfortrainingandthe
indicatorsoftwomodalities. w i assignsweightstothereliabilityin- dataemployedinsubsequentexperiments.Multipledataaugmentation
dicators to ensure that the distinct reliability indicators of different techniques were employed during training. For each data instance,
modalitiesareappropriatelyalignedduringthefusionprocess. Itis fourpossibleprocessingmethodswereapplied:preservingclarity(no
importanttoemphasizethattheseweightsaresolelyrelatedtothevocal processing),introducingenvironmentalnoiseinterference,introducing
andechoicKWSmodelsinvolvedinthefusionandremainsunaffected vocalnoiseinterference,anddiscardingaportionofvocaldata. The
bychangesinthedata. environmentalnoiseusedinprocessingwasextractedfromtheDE-
Furthermore,ai,t signifiesthereliabilityadjustmentfortwoclasses: MAND[45]noisedataset,asdetailedinSec.4.4.Thisnoisedatawas
“silence” and “unknown”. Considering their auxiliary roles in the thenscaledtorandomsignal-to-noiseratiosbeforebeingcombined
classification,whenonemodalityprovides“silence”or“unknown”,it withthetrainingdata.Additionally,vocalnoisedatawassourcedfrom
isimperativetotakeintofullconsiderationtheinformationfromthe theGoogleSpeechCommands[50]dataset,asexplainedinSec.4.6.
othermodality,thusnecessitatingareductionintheirreliabilitywithin Thisvocalnoisedatawasaddedtothetrainingdataafterbeingmulti-
afiniterange. Differently,“silence”providedbythevocalmodality pliedbyafixedcoefficient.Amongthesefourprocessingmethods,only
shouldbeaccordedspecialattention. Typically,thereisvirtuallyno onewasrandomlyselected. Subsequently,thevectorwasmultiplied
possibilityofvocalizationwithoutanyfacialmusclemovement.The byarandomfactorrangingbetween0.95and1.05tosimulaterandom
valueofai,t willonlychangetoaspecificvaluewhenthereisatleast noiseinterference. Furthermore,thecross-entropylossfunctionand
onemodalityproviding“silence”or“unknown”;inallothercases,it theAdamoptimizerwereemployedduringthetrainingprocess.
remainsafour-dimensionalvectorconsistingofones. Inapplication,thesystemconcatenatesthepredictionvectorsfrom
Subsequently,weweightthedecisionsofthetwosingle-modalclas- boththevocalandechoicmodalities,andfeedtheresultingconcate-
sifiersandutilizetheirlog-likelihoodsforlinearcombination,thereby natedvectorintothetrainedmulti-layerperceptron.Subsequently,we
obtainingthefusionresults. Overall, ourfusionprocesscanbede- selecttheclasswiththehighestprobabilityfromtheresultingprediction
scribedby: vectorasthesystem’soutput.

P P( (o of f, ,t t| |c c) )= =P P( (o ov v, ,t t| |c c) )λ ,v,tP(oe,t|c)(1−λv,t), R Rv v, ,t t∧ ∧¬R Re e, ,t
t
(5)
44 .1EX DP aE taRIMENTS
P N( oo cf r,t e| dc i) b= leP re( so ue l,t t| ,c), ¬ ¬R Rv v, ,t t∧ ∧¬R Re e, ,t
t.
W olde )re wcr eu ai rt ie nd g1 o5 up ra er qti uci ip pa mn ets n( t2 af ne dm ra el ae da in nd g1 s3 pem ca ifile cs, kf er yo wm o2 r0 dsto to27 rey ce oa rr ds
audiodata.Eachparticipantread30repetitionsof10comandwords,
Determination of Parameters. This fusion strategy initially re- and5repetitionsofanother25auxiliarywordslabeledas“unknown”
quiresthedeterminationofeightparameters,fourforsettingthresholds tohelpdistinguishunrecognizedwords.Additionally,eachparticipant
andtheotherfourforcalculatingexponents.Thistaskemploysage- alsoworethedevicetorecorddatainwhichtheydidnotspeakand
neticalgorithmtomaximizetheobjectivefunctionthatreflectstheac- kepttheirmouthstill,labeledas“silence”.Thedatahasasamplingrate
curacyofthefusionresultsunderdifferentparameters.Theremarkable of48kHz,allowingforthepreservationofaudioinformationwitha
globalsearchcapabilityofthegeneticalgorithmenablesittoachieve maximumfrequencyof24kHz.Asaresult,therecordeddatacontains
satisfactoryoptimizationeffectsundercomplexobjectivefunctions. informationfrombothvocalandechoicmodalities.ThevocabularyusedintheexperimentsisderivedfromtheGoogle 4.3.2 ResultsandDiscussion
SpeechCommands[50]dataset, whichisthemostcommonlyused
TheresultsarepresentedinTab.1,illustratingeightdifferentmodel
open-sourcedatasetforvocalKWStask.Therefore,itwasnaturalfor
configurationsresultingfromcombinationsoffourdistinctnetwork
ustomigratethevocabularyfromthisdatasettoourdual-modalKWS
widthsandtheinclusionofdepth-wiseseparableconvolutions(DS).
experiments.Thisallowsourvocalmodalitymodeltobetrainedona
Thistableprovidesinsightsintotheaverageaccuracy,parametercount,
combinationofthisdataset,whichhasalargevolumeofdata,andour
andcomputationalcomplexityofthesemodels.
proprietarydataset,maximizingitsperformanceandrobustness.This
Inthemodelnamingconvention,thefractionfollowingResNet-18
approachensuresthatthecomparativeexperimentalresultsbetween
denotesthereductioninmodelwidthrelativetotheoriginalmodel,
single-modalanddual-modalperformancearereliable.
and the suffix DS indicates the utilization of depth-wise separable
convolutions.
4.2 EvaluationMetric Notably,weobservedthattransitioningfromResNet-18toResNet-
WordErrorRate(WER).TheWordErrorRate(WER)isametric 18-1/4-DSresultedinonlyamarginal0.91%increaseinaverageWER,
usedtoevaluatetheperformanceofspeechrecognitionsystemsornat- whilesubstantiallyreducingtheparametercountbyover100times.
urallanguageprocessingsystems.Itmeasuresthedifferencebetween Thishighlightsthepotentialforourmodelstobeefficientlydeployed
two texts, i.e., how many words in the predicted text are incorrect, onXRheadsets.
missing,orredundant.TheWERcanbecalculatedas:
Table1:Comparisonofdifferentmodels’performance,includingWord
S+D+I S+D+I ErrorRate,Parameters,andMultiply-AddOperations.
WER= = (7)
N S+D+C
Model WER Params MAdd
whereSisthenumberofsubstitutions,Disthenumberofdeletions,
Iisthenumberofinsertions,Cisthenumberofcorrectwords,andN ResNet-18 5.06%±0.650 11.22M 1.84G
ResNet-18-1/2 5.47%±0.466 2.820M 468.4M
isthetotalwordcountinthetext.
ResNet-18-1/4 5.81%±0.639 712.6K 121.11M
ResNet-18-1/8 7.69%±0.528 182.0K 32.28M
4.3 Experiment1: EchoicModel
ResNet-18-DS 5.33%±0.539 1.484M 264.1M
Inthisexperiment,weconductedanablationstudyontheechoicmodal- ResNet-18-1/2-DS 5.56%±0.714 394.0K 79.99M
itydeeplearningmodelusingourproprietarydataset. Theprimary ResNet-18-1/4-DS 5.97%±0.573 109.9K 24.74M
objectivewastoinvestigatethetrade-offbetweenmodelcomplexity ResNet-18-1/8-DS 8.00%±1.260 33.22K 8.93M
(parametercount)andaccuracy,withtheultimategoalofachievinga
morestreamlinedandlightweightmodel.
4.4 Experiment2: NoisyEnvironment
4.3.1 ExperimentSetup
Inthissection,weexaminedtheperformanceoftwofusionstrategies
WecommencedourexperimentswithResNet-18,amodelthatEchoS-
acrossdifferentenvironmentsandsignal-to-noiseratios(SNR).The
peechhaspreviouslyvalidatedforitsstrongperformanceinechoic
researchfindingsindicatethat,inallexperimentalconditions,theper-
modalKWS.
formanceoftheKeywordSpotting(KWS)systemusingVocal-Echoic
Our initial approach involved reducing the number of channels
dual-modalfusionsurpassesthatofitssingle-modalcounterparts.
withintheconvolutionallayers,referredtoaswidthofthenetwork.
ThisadjustmentwasmotivatedbythefactthatResNet-18wasorigi- 4.4.1 ExperimentSetup
nallydesignedfortaskssuchasimageclassificationandothercomputer
Inthisexperiment,wesimulateddiversescenariosbysuperimposing
visionapplications,wherevisualdatatypicallycarriesaricherinforma-
datawithnoiseofvaryingintensitiescorrespondingtospecificscenes.
tionloadcomparedtoaudiodata.Consequently,reducingthenetwork’s
Wecomparedtheaverageperformanceofdifferentmodalitieswithin
widthwasadeliberatestrategyaimedatdecreasingthemodel’spa-
thesescenariostoevaluatetheireffectiveness.
rametercountandmitigatingtheriskofoverfitting. Inourstudy,we
Thesignal-to-noiseratio(SNR)wasemployedtostraightforwardly
exploredwidthreductionincomparisontotheoriginalResNet-18,as
wellasversionswithwidthsreducedto1/2,1/4,and1/8oftheoriginal
measurethestrengthofnoise,definedastheratioofsignalpowerPsto
width. We assessed their respective performance within the echoic
noisepowerPn.Decibels(dB)wereutilizedastheunitofmeasurement
forSNR,asshowninthefollowingformula:
KWSpipeline.
Furthermore, we employed depthwise separable convolutions to
replaceconvolutionmodulesinthemodel.Depthwiseseparableconvo-
SNR(dB)=10log 10Ps/Pn (8)
lutionmodulesbreakdownstandardconvolutionsintodepthwisecon- Weinvestigatedtheperformanceofthefusionmodalityinenviron-
volutionandpointwiseconvolution,significantlyreducingthenumber mentswithSNRrangingfrom−10dBto10dB.Thenoisesignalswere
ofparametersandcomputationswhilemaintainingsimilarperformance. multipliedbycoefficients,whichweredeterminedbasedontheaver-
Wecomparedtheaccuracyandparametercountdifferenceswhenusing agepowerofthedataandthetargetSNR,andthenaddedtothedata
depthwiseseparableconvolutionmoduleswithdifferentwidths. tosimulatevariousintensitylevelsofnoisyenvironments. Thedata
Our experiments were conducted on approximately 1800 speech werethensubjectedtovocalandechoicmodalityprocessingsteps,as
samplesfromfiveparticipants,withadatasplitof80%fortrainingand previouslydescribed,togeneratepredictiveresults.Subsequently,the
20%fortesting.Randomseedswereusedtocontroldatasplittingand fusionmodalityproducedfusedresults.Wecomparedtheworderror
dataaugmentationparameters.Foreachmodelvariant,wesystemati- ratesoftheseresultstoassesstheperformanceofdifferentmodalities
callyadjustedtherandomseedsandconductedtenroundsoftraining inthesenoisyenvironments.
andtestingtoobtainareliableaverageaccuracy.Furthermore,weem-
ployedtorchstattoanalyzethemodel’sparametersandcomputational 4.4.2 ResultsandDiscussion
complexity. InFig.6,wepresenttheaverageresultsoftwosinglemodalities:vocal
Inthetrainingprocess,weutilizedtheSGDoptimizerwithalearning and echoic, and two dual-modal fusion strategies: reliability-based
ratefollowingwarm-upstrategy,startingfrom0andlinearlyincreasing fusion(RBfusion),andMLP-basedfusion(MLPfusion)acrossall
to0.1overthefirst50epochs. Subsequently,thetrainingcontinues scenarios. The standard deviations of these data are also depicted
for 1000 epochs with cosine learning rate decay. Additionally, we with shaded regions of the same color on the graph. Experimental
applieddataaugmentationincludingrandomnoise,randompadding, resultsshowthatbothMLPfusionandRBfusionoutperformthetwo
andoverlayingbackgroundnoisedata. individualmodalities.dual-modalKeywordSpotting(KWS)systemtoprovidedependable
resultsinenvironmentswhereusersilenceisnecessary.
4.5.1 ExperimentSetup
Inthisassessment,weemployeddatafromwhichvocalinformation
hadbeenremovedtoevaluatetheperformanceofthefusionmodality
inasilentenvironment.
Becausetheexperimentreliesonechoicinformation,wecontinued
to use the self-recorded dataset mentioned earlier. We applied the
previouslymentionedfilteringandseparationprocesstoalltestdata,
categorizingitintovocalandechoicsignals.Thevocalsegmentswere
Fig.5: FrequencydistributionofMeeting(left)andMetro(right). The substitutedwithcorrespondingportionsofrandomsilencesignals.In
frequencydistributionofnoisevariesacrossdifferentscenarios, with
thisscenario,vocalinformationisentirelyeliminated.
some noise having a significant presence in high frequencies, while
Followingthat,theechoicandvocalsignalsareseparatelyfedinto
anotherportionisprimarilyconcentratedinlowerfrequencies.
theircorrespondingmodalities,resultinginpredictions. Thefusion
modality will generate results based on these predictions and will
becomparedtoboththeechoicandvocalmodalitiestoevaluatethe
system’sperformanceinasilenceenvironment.
4.5.2 ResultsandDiscussion
Theexperimentalresults,showninFig.7(b),displaytheWordError
Rates (WER) for each modality in the form of bar charts. Various
colorsonthebarsrepresenttheproportionsofsubstitutions,deletions,
andinsertions. Asexpected, withtheexceptionofsegmentsinthe
testsetthatoriginallycontainedsilencesignals,allsignalshavebeen
transformedintodeletionsinthevocalmodality,effectivelyeliminating
thepossibilityofvocalsignalsprovidinginformation.Atthesametime,
theperformanceoftheechoicmodalityremainsunaffected,witherror
recognitionprimarilyconsistingofsubstitutionsanddeletions,which
alignswithourexpectations.
The results indicate that the Word Error Rate (WER) of the two
fusion strategies closely approximates that of the echoic modality,
whichissignificantlylowerthanthatofthevocalmodality.
In conclusion, the experimental results showcase that our fusion
Fig.6:ComparisonofaverageWordErrorRates(WER)betweensingle-
strategycanyieldaccurateresultsinanechoic-onlyenvironment,a
modalanddual-modalKWSsystemsinallnoisescenarios. Ourdual-
capability that cannot be achieved by the vocal-only single-modal
modalsystems(RBFusionandMLPFusion)achievelowerWERthan
approach.Thissignifiesthatourdevicecanoperatewithoutrequiring
single-modal systems (Echoic and Vocal) across all SNRs. At the
additional user intervention, allowing users the freedom to choose
strongestnoiselevel(SNR=-10.0),MLPfusionreducesWERby15.68%
and16.57%comparedtovocalandechoicsystems. whethertospeakorusesilentspeech,anddeliversreliableresults.
4.6 Experiment4: NearbySpeakerInterference
Inthefollowinganalysis,weconductedaninvestigationintotheper-
Toprovideamoredetailedassessmentofthesystem’sperformance
formance of fusion methods with vocal signals in the presence of
acrossdifferentmodalities,weillustratetheexperimentalresultsof
interferencefromothervocalsources.Researchfindingsindicatethat
RBfusioninsixdistinctscenariosinFig.7(a). RBfusionexhibits
thefusionstrategycaneffectivelyharnesstheinformationfromechoic
slightlyhigheraverageworderrorrates,makingitamorechallenging
signals. Ultimately,theaccuracyofthefusionstrategyiswithinan
testoffusionstrategyperformance.Theresultsforeachscenarioare
acceptablerange,slightlyhigherthantheperformancelevelofthepure
calculatedastheaverageofsub-scenarioexperimentaloutcomes. In
echoicmodalityandfarbelowthatofthevocalmodality.Thisdiscov-
thefirstthreescenarios,suchastheDomesticsettingwithpronounced
eryatteststhatoursystemcanexhibitnoteworthyperformanceevenin
high-frequencynoiseinterference,thevocalmodalityexhibitssuperior
thefaceofsubstantialinterferencefromvocalnoise,underscoringits
performanceovertheechoicmodality.Conversely,inthelatterthree
robustness.
scenarios,suchasthePublicsettingwherenoiseprimarilymanifests
as low-frequency disturbances, the echoic modality demonstrates a 4.6.1 ExperimentSetup
performance advantage over the vocal modality. Regardless of the
Duringthisexperiment,wesimulatedthepresenceofnearbyspeak-
individual modalities performance, RB fusion consistently exhibits
ers by superimposing the voices of other individuals onto the data,
lowerworderrorratesthaneitheroftheminanygivenenvironment.
enablinganexaminationofthesystem’srobustnessunderthesecondi-
Theexperimentshaveshownthatmultimodalfusionconsistently
tions. Onlythevocalcomponentwascontaminated;theechoiccom-
outperformsitssingle-modalcounterpartsinthemajorityofenviron-
ponentremainedunaffected. Weemployedthepreviouslydescribed
mentalconditions,confirmingitsremarkablerobustness.Itcombines
self-recordeddatasetforourexperimentation,withthevocaldataused
theadvantagesofbothmodalitiesandcanbeappliedinawiderrange
tointroduceinterferencesourcedfromtheGoogleSpeechCommands
ofenvironments.
v2dataset[50].Accountingfordifferencesinaveragepowerbetween
thetwodatasetsvocalcomponents,weappliedafixedscalingfactor
4.5 Experiment3: SilentSpeech
tothevocalsignalsemployedforsuperimpositiontomoreaccurately
In this portion of the experiment, we conducted a comparison be- simulatenearbyspeakers.
tweentwofusionstrategiesandtwosingle-modalapproachesinan Datafromtheself-recordeddataset,whensuperimposedwithother
echoic-onlystate. Researchfindingssuggestthatallfusionstrategy humanvocalsignals, willyieldresultsseparatelyforthesilentand
caneffectivelyutilizetheinformationprovidedbytheechoicmodality, vocalsinglemodalities.Predictionsfromthefusionmodality,basedon
resultinginasignificantlylowerworderrorratecomparedtousing theseoutcomes,willbecomparedtoassessthesystem’sperformance
thevocalmodalityalone.Thissupportsthecapacityofourdesigned inscenariosinvolvingnearbyspeakers.(a) Performance in Various Noisy Scenarios
(b) Performance in Silent Speech (c) Performance in the Presence of Nearby Speakers
Fig.7:Theperformanceofoursysteminthreechallengingscenarios,measuredbyWordErrorRate(WER).(a)Fusionconsistentlyachievesthe
lowestWER,outperformingsinglemodalitiesinallscenarios. (b)Insilentspeech,traditionalvocalKWSfailsentirely,whilefusionmatchesthe
performanceoftheechoicmodality,notablyexpandingthesystem’susagescenarios.(c)Inthepresenceofnearbyspeakers,thefusion’sWERis
significantlylowerthanthatofvocalmodalsystems,greatlyreducingfalsetriggersinspeech-interferenceenvironments.
4.6.2 ResultsandDiscussion various scenarios. Demonstration video can be viewed at https:
//youtu.be/fSQoEJ37uEw.
TheresultsofthisexperimentareillustratedinFig.7(c),whereerror
ratesforeachmodalityarepresentedintheformofbarcharts.Different
5 LIMITATIONSANDFUTUREWORK
colorsofbarsrepresenttheproportionsofsubstitution,deletion,andin-
Ourworkalsohassomelimitations,andwediscussthelimitationswe
sertionerrors.Inthiscontext,thevocalmodalitydisplayednearlyhalf
havedeterminedthroughadditionalstudiesinthissection.
oftheerrors,primarilycomposedofsubstitutionanddeletionerrors.In
PhysicalActivities. Walkingandheadshakingimpacttheperfor-
contrast,theechoicmodalitystillproducedrelativelyreliableresults,
manceofboththevocalandechoicmodalitypipelines,consequently
withacertainproportionofsubstitutionerrorsandveryfewdeletion
affectingtheoverallsystem.Toinvestigatetheinfluenceofthesefac-
errors. Fromtheresults,itisevidentthatthefusionmodalitysignif-
tors,weconductedaseriesofexperiments.Theresultsindicatethat:1)
icantlyalleviatedthehigherrorrateobservedinthevocalmodality,
Walkingandheadshakinghaveanobservablebutminorimpactonthe
includingbothinsertionandsubstitutionserrors.
vocalmodality.However,theysignificantlyimpacttheperformanceof
Thisexperimentdemonstratesthateveninsituationscharacterized
theechoicmodality,causinganincreaseofaround50%andaround33%
by highly conspicuous background human speech interference, the
inWER,respectively.2)Thefusionmethodoptimizedwithactivities
fusionstrategycaneffectivelypreventfalsetriggersandprovidereliable
datacanmitigatetheinterferencefromtheechoicmodality,resultingin
responses.Thesignificantreductioninsubstitutionerrorssuggeststhat
fusedWERsthatarestilllowerthantheindividualWERsofthevocal
users can have confidence in the recognition accuracy, even in the
modality. Nevertheless,physicalactivitiesstillweakenoursystem’s
presenceofhumanvoicenoise.Additionally,theneareliminationof
advantageoverconventionalvocalKWS.Furtherenhancementofthe
insertionerrorsaddressesconcernsrelatedtounintentionalactivations
echoicmodalitymethodremainsapotentialdirectionforimprovement.
innoisyorcrowdedsettings.Userscanconfidentlyutilizethisapproach
UltrasonicInterference.Anotherfactornotdirectlyaddressedin
inenvironmentssuchasdiscussions,classrooms,andsimilarsettings
thispaperistheinterferencefromultrasoundemittedbyotherdevicesof
withoutconcernfortheimpactofhigh-decibelhumanvoicesinthese
thesamekind.Theexperimentsindicatethat:1)Theinterferencefrom
scenarios.
astationaryultrasoundsourceisneglectableacrossmultipledirections
and distances. This may be because the differential processing in
4.7 Discussion
our echoic modality pipeline has a mitigating effect on ultrasound
Ourexperimentalresultsdemonstratethatourdual-modalapproachis interference. 2)Ontheotherhand,movingultrasoundsourcehasa
morerobustcomparedtobothvocalkeywordspottingmethodsand moresignificantimpact,withanaverageincreaseofapproximately7%
silentspeechmethods: inWERatadistanceof1m, andcloserultrasoundsourcescausing
ComparisonwithVocalKWSmethod.Ourdual-modalapproach greaterinterference.Thisisbecausemotiondiminishesthemitigating
demonstratessuperiorperformanceinvariousnoisyenvironmentscom- effectofthedifferentialprocessingoninterference.Furtherresearch
paredtoconventionalkeywordspottingmethods.Additionally,itcan intothisaspectisapotentialavenueforfuturework.
effectivelyfilteroutinterferencefromothernearbyspeakersandsup- Otherpotentialfutureworkincludesinvestigatingtheimpactonthe
portstheuseofsilentspeechwhenvocalizationisinconvenient. performanceanduserexperienceofintegratingadditionalmodalitiesin
Comparison with Silent Speech method. Our dual-modal ap- KWS,improvingtheperformanceoftheechoicmodalitymodelinthe
proachinheritsthecapabilityofpriorworksonsilentspeechinterfaces. system,andfurtherminimizingtheintroductionofadditionalhardware.
Moreover,inscenarioswheresilentspeechrecognitionperformance
experiencessignificantdegradation, suchasexcessivelynoisyenvi- 6 CONCLUSION
ronments,intensephysicalactivities,andultrasonicinterference,our In this study, we introduce a dual-modal keyword spotting (KWS)
system can seamlessly leverage vocal speech for recognition. This systemforXRheadsets,implementedontheMicrosoftHoloLens2
capabilitynotonlyaidsinavoidingpotentialsystemfailuresbutalso platform.Thekeyofoursystemliesinthefusionoffeaturesfromtwo
eliminatestheneedformanualmodeswitching. distinctmodalities: vocalspeechandmouthmovementinformation
As a supplement, we have developed a game to validate the ex- capturedthroughultrasonicechoes.Thisintegrationimpartssuperior
perimental results, showcasing the effectiveness of our system in noiserobustnessandadaptabilitytodiversescenarios. Specifically,ourapproachefficientlyutilizeshardware,requiringonlyoff-the-shelf ceedingsoftheACMonInteractive,Mobile,WearableandUbiquitous
speakers and microphones to obtain information from both modali- Technologies,4(3):1–27,Sept.2020.doi:10.1145/34118301,2
ties.Moreover,ourmethodiscomputationallylightweight,employing [14] A.Grinshpoon,S.Sadri,G.J.Loeb,C.Elvezio,andS.K.Feiner.Hands-
streamlinedmodelsandefficientfusionstrategies. Ourexperimental FreeInteractionforAugmentedRealityinVascularInterventions.In2018
resultsdemonstratetheexceptionalperformanceofthisdual-modalsys- IEEEConferenceonVirtualRealityand3DUserInterfaces(VR),pp.
temacrossvariouschallengingscenarios.Itoutperformssingle-modal 751–752.IEEE,Reutlingen,Mar.2018.doi:10.1109/VR.2018.8446259
systemsinnoisyenvironmentsandoffersadvantagesinsilentscenarios 2
andsituationswithnearbyspeechinterference,wheretraditionalvocal [15] R.Hofe,S.R.Ell,M.J.Fagan,J.M.Gilbert,P.D.Green,R.K.Moore,
andS.I.Rybchenko.Small-vocabularyspeechrecognitionusingasilent
KWS systems struggle. Overall, our proposed dual-modal method
speechinterfacebasedonmagneticsensing. SpeechCommunication,
enhancesthenoiserobustnessofKWSsystemsandnotablyexpands
55(1):22–32,Jan.2013.doi:10.1016/j.specom.2012.02.0012
theirapplicationscope.Thisadvancementempowersuserstoengage
[16] J.Hombeck,H.Voigt,T.Heggemann,R.R.Datta,andK.Lawonn.Tell
in speech interactions more frequently, providing not only superior
MeWhereToGo:Voice-ControlledHands-FreeLocomotionforVirtual
interactionexperiencesbutalsoenhancedflexibilityofchoice.
RealitySystems.In2023IEEEConferenceVirtualRealityand3DUser
Interfaces(VR),pp.123–134.IEEE,Shanghai,China,Mar.2023.doi:10.
1109/VR55154.2023.000282
REFERENCES
[17] A.Kapur,S.Kapur,andP.Maes. AlterEgo: APersonalizedWearable
[1] A.Bedri,H.Sahni,P.Thukral,T.Starner,D.Byrd,P.Presti,G.Reyes, SilentSpeechInterface.In23rdInternationalConferenceonIntelligent
M.Ghovanloo,andZ.Guo.TowardSilent-SpeechControlofConsumer UserInterfaces,IUI’18,pp.43–53.AssociationforComputingMachinery,
Wearables.Computer,48(10):54–62,Oct.2015.doi:10.1109/MC.2015. NewYork,NY,USA,Mar.2018.doi:10.1145/3172944.31729771,2
3102 [18] M.Kaur,M.Tremaine,N.Huang,J.Wilder,F.Flippo,andS.Mantravadi.
[2] A.Berg,M.O’Connor,andM.T.Cruz. KeywordTransformer:ASelf- Whereis“it”?EventSynchronizationinGaze-SpeechInputSystems.2
AttentionModelforKeywordSpotting. InInterspeech2021,pp.4249– [19] B.Kim,S.Chang,J.Lee,andD.Sung.BroadcastedResidualLearning
4253,Aug.2021.arXiv:2104.00769[cs,eess].doi:10.21437/Interspeech. forEfficientKeywordSpotting,July2023.arXiv:2106.04140[cs,eess].
2021-12864 2,4
[3] L. A. Cheah, J. M. Gilbert, J. A. Gonzalez, P. D. Green, S. R. Ell, [20] N.Kimura,T.Gemicioglu,J.Womack,R.Li,Y.Zhao,A.Bedri,A.Olwal,
R.K.Moore,andE.Holdsworth. AWearableSilentSpeechInterface J.Rekimoto,andT.Starner.Mobile,Hands-free,SilentSpeechTexting
basedonMagneticSensorswithMotion-ArtefactRemoval:.InProceed- UsingSilentSpeller.InExtendedAbstractsofthe2021CHIConferenceon
ingsofthe11thInternationalJointConferenceonBiomedicalEngineer- HumanFactorsinComputingSystems,pp.1–5.ACM,YokohamaJapan,
ingSystemsandTechnologies,pp.56–62.SCITEPRESS-Scienceand May2021.doi:10.1145/3411763.34515522
TechnologyPublications,Funchal,Madeira,Portugal,2018.doi:10.5220/ [21] N. Kimura, K. Hayashi, and J. Rekimoto. TieLent: A Casual Neck-
00065732005600622 MountedMouthCapturingDeviceforSilentSpeechInteraction.InPro-
[4] G.Chen,C.Parada,andG.Heigold. Small-footprintkeywordspotting ceedingsoftheInternationalConferenceonAdvancedVisualInterfaces,
usingdeepneuralnetworks.In2014IEEEInternationalConferenceon pp.1–8.ACM,SalernoItaly,Sept.2020.doi:10.1145/3399715.3399852
Acoustics,SpeechandSignalProcessing(ICASSP),pp.4087–4091.IEEE, 2
Florence,Italy,May2014.doi:10.1109/ICASSP.2014.68543702 [22] N. Kimura, M. Kono, and J. Rekimoto. SottoVoce: An Ultrasound
[5] T.Chen,B.Steeper,K.Alsheikh,S.Tao,F.Guimbretière,andC.Zhang.C- Imaging-BasedSilentSpeechInteractionUsingDeepNeuralNetworks.
Face:ContinuouslyReconstructingFacialExpressionsbyDeepLearning InProceedingsofthe2019CHIConferenceonHumanFactorsinCom-
ContoursoftheFacewithEar-mountedMiniatureCameras.InProceed- putingSystems,pp.1–11.ACM,GlasgowScotlandUk,May2019.doi:
ingsofthe33rdAnnualACMSymposiumonUserInterfaceSoftwareand 10.1145/3290605.33003761,2
Technology,pp.112–125.ACM,VirtualEventUSA,Oct.2020.doi:10. [23] R.Kumar,V.Yeruva,andS.Ganapathy.OnConvolutionalLSTMModel-
1145/3379337.34158792 ingforJointWake-WordDetectionandTextDependentSpeakerVerifi-
[6] S.Choi,S.Seo,B.Shin,H.Byun,M.Kersner,B.Kim,D.Kim,and cation.InInterspeech2018,pp.1121–1125.ISCA,Sept.2018.doi:10.
S.Ha.TemporalConvolutionforReal-timeKeywordSpottingonMobile 21437/Interspeech.2018-17592
Devices,Nov.2019.arXiv:1904.03814[cs,eess].2,4 [24] Y.Kunimi,M.Ogata,H.Hiraki,M.Itagaki,S.Kanazawa,andM.Mochi-
[7] C.Cioflan,L.Cavigelli,M.Rusci,M.DePrado,andL.Benini.Towards maru.E-MASK:AMask-ShapedInterfaceforSilentSpeechInteraction
On-deviceDomainAdaptationforNoise-RobustKeywordSpotting. In withFlexibleStrainSensors. InAugmentedHumans2022,pp.26–34.
2022IEEE4thInternationalConferenceonArtificialIntelligenceCircuits ACM,Kashiwa,ChibaJapan,Mar.2022.doi:10.1145/3519391.3519399
andSystems(AICAS),pp.82–85.IEEE,Incheon,Korea,Republicof,June 1,2
2022.doi:10.1109/AICAS54282.2022.98699902 [25] K.-S.Lee.EMG-BasedSpeechRecognitionUsingHiddenMarkovModels
[8] S.DavisandP.Mermelstein.Comparisonofparametricrepresentationsfor WithGlobalControlVariables.IEEETransactionsonBiomedicalEngi-
monosyllabicwordrecognitionincontinuouslyspokensentences.IEEE neering,55(3):930–940,Mar.2008.doi:10.1109/TBME.2008.9156581,
TransactionsonAcoustics,Speech,andSignalProcessing,28(4):357–366, 2
Aug.1980.doi:10.1109/TASSP.1980.11634204 [26] M.Lee,M.Billinghurst,W.Baek,R.Green,andW.Woo. Ausability
[9] R.Ding,C.Pang,andH.Liu. Audio-VisualKeywordSpottingBased studyofmultimodalinputinanaugmentedrealityenvironment.Virtual
onMultidimensionalConvolutionalNeuralNetwork.In201825thIEEE Reality,17(4):293–305,Nov.2013.doi:10.1007/s10055-013-0230-02
InternationalConferenceonImageProcessing(ICIP),pp.4138–4142. [27] K.Li,R.Zhang,B.Liang,F.Guimbretière,andC.Zhang. EarIO:A
IEEE,Athens,Oct.2018.doi:10.1109/ICIP.2018.84510961 Low-power Acoustic Sensing Earable for Continuously Tracking De-
[10] M.ElepfandtandM.Grund.Moveitthere,ornot?:thedesignofvoice tailedFacialMovements.ProceedingsoftheACMonInteractive,Mobile,
commandsforgazewithspeech.InProceedingsofthe4thWorkshopon WearableandUbiquitousTechnologies,6(2):1–24,July2022.doi: 10.
EyeGazeinIntelligentHumanMachineInteraction,pp.1–3.ACM,Santa 1145/35346214
MonicaCalifornia,Oct.2012.doi:10.1145/2401836.24018482 [28] R.Li,J.Wu,andT.Starner.TongueBoard:AnOralInterfaceforSubtle
[11] I.FungandB.Mak.End-To-EndLow-ResourceLip-ReadingwithMaxout Input.2019.2
CnnandLstm. In2018IEEEInternationalConferenceonAcoustics, [29] I.Lopez-Espejo,Z.-H.Tan,J.H.L.Hansen,andJ.Jensen.DeepSpoken
SpeechandSignalProcessing(ICASSP),pp.2511–2515.IEEE,Calgary, KeywordSpotting:AnOverview.IEEEAccess,10:4169–4199,2022.doi:
AB,Apr.2018.doi:10.1109/ICASSP.2018.84622801 10.1109/ACCESS.2021.31395082,4
[12] M.J.F.Gales,K.M.Knill,andA.Ragni.Low-ResourceSpeechRecog- [30] I.Lopez-Espejo,Z.-H.Tan,andJ.Jensen. ANovelLossFunctionand
nitionandKeyword-Spotting.InA.Karpov,R.Potapova,andI.Mporas, TrainingStrategyforNoise-RobustKeywordSpotting.IEEE/ACMTrans-
eds.,SpeechandComputer,vol.10458,pp.3–19.SpringerInternational actionsonAudio,Speech,andLanguageProcessing,29:2254–2266,2021.
Publishing,Cham,2017.SeriesTitle:LectureNotesinComputerScience. doi:10.1109/TASLP.2021.30925672
doi:10.1007/978-3-319-66429-3_12 [31] A.H.Michaely,X.Zhang,G.Simko,C.Parada,andP.Aleksic.Keyword
[13] Y. Gao, Y. Jin, J. Li, S. Choi, and Z. Jin. EchoWhisper: Exploring spottingforGoogleassistantusingcontextualspeechrecognition. In
anAcoustic-basedSilentSpeechInterfaceforSmartphoneUsers. Pro- 2017IEEEAutomaticSpeechRecognitionandUnderstandingWorkshop(ASRU),pp.272–278.IEEE,Okinawa,Dec.2017.doi:10.1109/ASRU. 3390/brainsci100704421,2
2017.82689462 [49] Z.Wang,H.Wang,H.Yu,andF.Lu. InteractionWithGaze,Gesture,
[32] P.Monteiro,G.Goncalves,H.Coelho,M.Melo,andM.Bessa. Hands- andSpeechinaFlexiblyConfigurableAugmentedRealitySystem.IEEE
freeinteractioninimmersivevirtualreality:Asystematicreview.IEEE TransactionsonHuman-MachineSystems,51(5):524–534,Oct.2021.doi:
TransactionsonVisualizationandComputerGraphics,27(5):2702–2713, 10.1109/THMS.2021.30979732
May2021.doi:10.1109/TVCG.2021.30676872 [50] P.Warden.SpeechCommands:ADatasetforLimited-VocabularySpeech
[33] T. Piumsomboon, D. Altimira, H. Kim, A. Clark, G. Lee, and Recognition,Apr.2018.arXiv:1804.03209[cs].5,6,7
M.Billinghurst.Grasp-Shellvsgesture-speech:Acomparisonofdirect [51] J.Wilpon,L.Miller,andP.Modi. Improvementsandapplicationsfor
andindirectnaturalinteractiontechniquesinaugmentedreality.In2014 key word recognition using hidden Markov modeling techniques. In
IEEEInternationalSymposiumonMixedandAugmentedReality(ISMAR), [Proceedings]ICASSP91:1991InternationalConferenceonAcoustics,
pp.73–82.IEEE,Munich,Germany,Sept.2014.doi:10.1109/ISMAR. Speech,andSignalProcessing,pp.309–312vol.1.IEEE,Toronto,Ont.,
2014.69484112 Canada,1991.doi:10.1109/ICASSP.1991.1503382
[34] G.Pomianos,C.Neti,G.Gravier,A.Garg,andA.Senior.Recentadvances [52] K.Xu,D.Li,N.Cassimatis,andX.Wang.LCANet:End-to-EndLipread-
intheautomaticrecognitionofaudiovisualspeech. Proceedingsofthe ingwithCascadedAttention-CTC. In201813thIEEEInternational
IEEE,91(9):1306–1326,Sept.2003.doi:10.1109/JPROC.2003.817150 ConferenceonAutomaticFace&GestureRecognition(FG2018),pp.
1,4 548–555.IEEE,Xi’an,May2018.doi:10.1109/FG.2018.000881
[35] R.Prabhavalkar,R.Alvarez,C.Parada,P.Nakkiran,andT.N.Sainath. [53] M.XuandX.-L.Zhang.DepthwiseSeparableConvolutionalResNetwith
Automaticgaincontrolandmulti-styletrainingforrobustsmall-footprint Squeeze-and-ExcitationBlocksforSmall-FootprintKeywordSpotting.
keywordspottingwithdeepneuralnetworks.In2015IEEEInternational InInterspeech2020,pp.2547–2551.ISCA,Oct.2020.doi: 10.21437/
ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp. Interspeech.2020-10452
4704–4708.IEEE,SouthBrisbane,Queensland,Australia,Apr.2015.doi: [54] R.Zhang,M.Chen,B.Steeper,Y.Li,Z.Yan,Y.Chen,S.Tao,T.Chen,
10.1109/ICASSP.2015.71788632 H.Lim,andC.Zhang. SpeeChin:ASmartNecklaceforSilentSpeech
[36] H.-R. Rantamaa, J. Kangas, M. Jordan, H. Mehtonen, J. Mäkelä, Recognition. Proceedings of the ACM on Interactive, Mobile, Wear-
K. Ronkainen, M. Turunen, O. Sundqvist, I. Syrjä, J. Järnstedt, and ableandUbiquitousTechnologies,5(4):1–23,Dec.2021.doi:10.1145/
R.Raisamo. Evaluationofvoicecommandsformodechangeinvirtual 34949872
realityimplantplanningprocedure. InternationalJournalofComputer [55] R.Zhang,K.Li,Y.Hao,Y.Wang,Z.Lai,F.Guimbretière,andC.Zhang.
AssistedRadiologyandSurgery,17(11):1981–1989,June2022.doi:10. EchoSpeech: Continuous Silent Speech Recognition on Minimally-
1007/s11548-022-02685-12 obtrusive Eyewear Powered by Acoustic Sensing. In Proceedings of
[37] J.RekimotoandY.Nishimura.Derma:SilentSpeechInteractionUsing the2023CHIConferenceonHumanFactorsinComputingSystems,pp.1–
TranscutaneousMotionSensing.InAugmentedHumansConference2021, 18.ACM,HamburgGermany,Apr.2023.doi:10.1145/3544548.3580801
pp.91–100.ACM,RovaniemiFinland,Feb.2021.doi:10.1145/3458709. 1,2,3,4
34589411,2 [56] Y.Zhang,Y.-C.Chen,H.Wang,andX.Jin.CELIP:Ultrasonic-basedLip
[38] J.Rohlicek,W.Russell,S.Roukos,andH.Gish. Continuoushidden ReadingwithChannelEstimationApproachforVirtualRealitySystems.
Markovmodelingforspeaker-independentwordspotting.InInternational InAdjunctProceedingsofthe2021ACMInternationalJointConference
ConferenceonAcoustics,Speech,andSignalProcessing,pp.627–630. onPervasiveandUbiquitousComputingandProceedingsofthe2021
IEEE,Glasgow,UK,1989.doi:10.1109/ICASSP.1989.2665052 ACMInternationalSymposiumonWearableComputers, pp.580–585.
[39] R.RoseandD.Paul.AhiddenMarkovmodelbasedkeywordrecognition ACM,VirtualUSA,Sept.2021.doi:10.1145/3460418.34801631,2
system. InInternationalConferenceonAcoustics,Speech,andSignal [57] P.Zhou,W.Yang,W.Chen,Y.Wang,andJ.Jia. ModalityAttention
Processing,pp.129–132.IEEE,Albuquerque,NM,USA,1990.doi:10. for End-to-end Audio-visual Speech Recognition. In ICASSP 2019 -
1109/ICASSP.1990.1155552 2019IEEEInternationalConferenceonAcoustics,SpeechandSignal
[40] A.Rosenberg,K.Audhkhasi,A.Sethy,B.Ramabhadran,andM.Picheny. Processing(ICASSP),pp.6565–6569.IEEE,Brighton,UnitedKingdom,
End-to-endspeechrecognitionandkeywordsearchonlow-resourcelan- May2019.doi:10.1109/ICASSP.2019.86837331
guages.In2017IEEEInternationalConferenceonAcoustics,Speechand [58] Y. Zhuang, X. Chang, Y. Qian, and K. Yu. Unrestricted Vocabulary
SignalProcessing(ICASSP),pp.5280–5284.IEEE,NewOrleans,LA, KeywordSpottingUsingLSTM-CTC.InInterspeech2016,pp.938–942.
Mar.2017.doi:10.1109/ICASSP.2017.79531642 ISCA,Sept.2016.doi:10.21437/Interspeech.2016-7532
[41] T.N.SainathandC.Parada. Convolutionalneuralnetworksforsmall-
footprintkeywordspotting.InInterspeech2015,pp.1478–1482.ISCA,
Sept.2015.doi:10.21437/Interspeech.2015-3522
[42] B.Shi,W.-N.Hsu,andA.Mohamed. RobustSelf-SupervisedAudio-
VisualSpeechRecognition,July2022.arXiv:2201.01763[cs,eess].1
[43] M.Sun,A.Raju,G.Tucker,S.Panchapagesan,G.Fu,A.Mandal,S.Mat-
soukas,N.Strom,andS.Vitaladevuni.Max-poolinglosstrainingoflong
short-termmemorynetworksforsmall-footprintkeywordspotting. In
2016IEEESpokenLanguageTechnologyWorkshop(SLT),pp.474–480.
IEEE,SanDiego,CA,Dec.2016.doi:10.1109/SLT.2016.78463062
[44] R.TangandJ.Lin.DeepResidualLearningforSmall-FootprintKeyword
Spotting,Sept.2018.arXiv:1710.10361[cs].2,4
[45] J.Thiemann,N.Ito,andE.Vincent. TheDiverseEnvironmentsMulti-
channel Acoustic Noise Database (DEMAND): A database of multi-
channelenvironmentalnoiserecordings.pp.035081–035081.Montreal,
Canada,2013.doi:10.1121/1.47995975
[46] R.VygonandN.Mikhaylovskiy.LearningEfficientRepresentationsfor
KeywordSpottingwithTripletLoss.InA.KarpovandR.Potapova,eds.,
SpeechandComputer,vol.12997,pp.773–785.SpringerInternational
Publishing,Cham,2021.SeriesTitle:LectureNotesinComputerScience.
doi:10.1007/978-3-030-87802-3_694
[47] T.Wang,D.Zhang,Y.Zheng,T.Gu,X.Zhou,andB.Dorizzi.C-FMCW
BasedContactlessRespirationDetectionUsingAcousticSignal. Pro-
ceedingsoftheACMonInteractive,Mobile,WearableandUbiquitous
Technologies,1(4):1–20,Jan.2018.doi:10.1145/31611884
[48] Y. Wang, M. Zhang, R. Wu, H. Gao, M. Yang, Z. Luo, and G. Li.
Silent Speech Decoding Using Spectrogram Features Based on Neu-
romuscularActivities. BrainSciences,10(7):442,July2020.doi: 10.