Discovering group dynamics in synchronous time series
via hierarchical recurrent switching-state models
MichaelWojnowicz1,2,PreetishRath1,EricMiller1,JeffreyMiller2,
CliffordHancock3,MeghanO’Donovan3,SethElkin-Frankston1,3,ThaddeusBrunye1,3,andMichaelC.Hughes1
1TuftsUniversity,2HarvardUniversity,3USArmyCCDCSoldierCenter
Abstract anessentialpropertyofsuchdatainmanyapplications: the
temporalbehaviorsoftheindividualentitiesarecoordinated
inasystematicbutfundamentallylatent(i.e.,unobserved)
Weseektomodelacollectionoftimeseriesaris-
manner.Forexample,considerthedynamicsofateamsport
ing from multiple entities interacting over the
likebasketball[TernerandFranks,2021]. Oneplayermight
sametimeperiod.Recentworkfocusedonmodel-
setascreentoallowateammatetodrivetothebasket. As
ingindividualtimeseriesisinadequateforourin-
anotherexample,considerasquadofsoldiersengagedina
tendedapplications,wherecollectivesystem-level
trainingexercise. Theymustworktogethertoprotecttheir
behaviorinfluencesthetrajectoriesofindividual
blindsidesfromattackwhileaccomplishingtheirmission.
entities. Toaddresssuchproblems,wepresenta
Inbothexamples,thedynamicsoftheindividualsarefar
newhierarchicalswitching-statemodelthatcan
fromindependent. Instead,theobservedtrajectoriesexhibit
betrainedinanunsupervisedfashiontosimulta-
“top-down”patternsofcoordinationlearnedfromextensive
neouslyexplainbothsystem-levelandindividual-
trainingtogetheraswellas“bottom-up”adaptationsofthe
leveldynamics. Weemployalatentsystem-level
individualsandthegrouptoevolvingsituationaldemands.
discrete state Markov chain that drives latent
Weseektobuildamodelthatcaninferhowgroupdynamics
entity-levelchainswhichinturngovernthedy-
evolveovertimegivenonlyentity-levelsensorymeasure-
namicsofeachobservedtimeseries. Feedback
ments,whiletakingintoaccounttop-downandbottom-up
from the observations to the chains at both the
influences. WereturntothesoldierslaterinSec.5.3and
entityandsystemlevelsimprovesflexibilityvia
tobasketballinSec.5.2. Similargroupdynamicsarisein
context-dependentstatetransitions. Ourhierar-
manyotherdomains,suchasthebehaviorofbusinessesin
chicalswitchingrecurrentdynamicalmodelscan
asharedeconomicsystem[vanDijketal.,2002]oranimals
belearnedviaclosed-formvariationalcoordinate
inasharedhabitat[Sunetal.,2021].
ascentupdatestoalllatentchainsthatscalelin-
earlyinthenumberofindividualtimeseries.This Whilemodelingindividualtimeserieshasseenmanyrecent
isasymptoticallynomorecostlythanfittingsepa- advances[Farnooshetal.,2021,Guetal.,2022,Linderman
ratemodelsforeachentity. Experimentsonsyn- et al., 2017], there remains a need for improved models
theticandrealdatasetsshowthatourmodelcan for coordinated collections of time series. Among works
producebetterforecastsoffutureentitybehavior that try to model collections of time series, the simplest
thanexistingmethods. Moreover,theavailability approachesrepurposemodelsforindividualtimeseries,ei-
oflatentstatechainsatboththeentityandsystem therfittingseparatemodelstoindividualentitiesorpooling
levelenablesinterpretationofgroupdynamics. allentitiestogetherasiidobservationsfromonecommon
model. Asastepbeyondthis,someeffortspursueperson-
alized models that allow custom parameters that govern
1 Introduction
eachentity’sdynamicswhilesharinginformationbetween
Weconsidertheproblemofjointlymodelingacollectionof entitiesviacommonpriorsontheseparameters[Alaaand
timeseries. Eachseriesinthecollectiondescribestheevolu- van der Schaar, 2019, Linderman et al., 2019, Severson
tionofoneentitywithinasharedenvironmentorsystemcon- etal.,2020],oftenusingmixedeffects[Altman,2007,Liu
tainingmultipleinteractingentitiesobservedoverthesame etal.,2011]. Butpersonalizedmodelsalloweachsequence
timeperiod. Ourworkismotivatedbytheneedtocapture tounfoldasynchronously,withoutinteractionovertime. In
contrast,ourgoalistodevelopmodelsspecificallycapable
ofcapturingthesynchronouslycoordinatedbehaviorofa
groupofinteractingentitiesinthesametimeperiod. Others
4202
naJ
62
]LM.tats[
1v37941.1042:viXraDiscoveringgroupdynamicsinsynchronoustimeseries
havepursuedthisgoalwithcomplexneuralarchitectures
Discretestatesforsystem s s s s
thatcanjointlymodel“multi-agent”trajectories[Alcornand 0 1 2 3
Nguyen,2021,Xuetal.,2022,Zhanetal.,2019]. Instead,
wefocusonparametricmethodsthatareeasiertointerpret
Discretestatesforentity z(j) z(j) z(j) z(j)
forstakeholdersandmorelikelytofitwellinapplications 0 1 2 3
withonlyafewminutesofavailabledata(suchasSec.5.3).
Onepotentialbarriertomodelingsynchronouscoordination Observations x(j) x(j) x(j) x(j)
0 1 2 3
acrossentitiesiscomputationalcomplexity. Forinstance,a
J
modelwithdiscretehiddenstateswhichallowsinteractions
amongentitieshasafactorialstructurewithinferencethat Figure1: Graphicalmodelrepresentationofourproposed
scalesexponentiallyinthenumberofentities(seeSec.3). hierarchicalswitchingrecurrentdynamicalmodel(HSRDM)
Inthispaperwepresentatractableframeworkformodeling for a system of J interacting entities. The colored edges
collectionsofsynchronoustimeseriesthatovercomesthis highlightthekeyinsightsbehindourflexibletransitionmod-
barrier. Allestimationcanbedonewithcostlinearinthe elsofsystem-levelhiddenstatessandentity-levelhidden
numberofentities,makingourmodel’sasymptoticruntime states z. Transitions to the next system state depend (via
complexitynomorecostlythanfittingseparatemodelsto bluearrows)onthecurrentsystemstateandrecurrentfeed-
eachentity. back from observations of all entities (up-diagonal blue).
Transitionstothenextentitystatedepend(viaredarrows)
Ourfirstkeymodelingcontributionisanexplicitrepre-
on the next system-level state (down), the current entity
sentationofthehierarchicalstructureofgroupdynamics,
state(horizontal),andrecurrentfeedbackfromentityobser-
usingswitching-statemodels[Rabiner,1989]asabuilding
vations(up-diagonalred).
block. AsshowninFig.1,ourmodelpositstwolevelsof
latentdiscretestatechains: asystem-levelchain(sharedby
fluence)andrecurrentfeedbackfrompreviousobservations
allentities)andanentity-levelchainuniquetoeachentity.
(“bottom-up”influence). Optionalexogenousfeatures(e.g.
Weassumethatthesystem-levelstateisthesolemediatorof
theballpositioninbasketball)canalsobeeasilyincorpo-
cross-entitycoordination;eachentity-levelchainiscondi-
rated. Wefurtherprovideavariationalinferencealgorithm
tionallyindependentofotherentitiesgiventhesystem-level
forsimultaneouslyestimatingmodelparametersandapprox-
state chain. Our model achieves “top down” patterns of
imateposteriorsoversystem-levelandentity-levelchains.
coordinationviathesystem-levelchain’sinfluenceoneach
Eachchain’sposteriormaintainsthemodel’stemporalde-
entity-levelchain’sstatetransitiondynamics. Inturn,our
pendency structure while remaining affordable to fit via
modelgeneratesobservedtimeseriesviaanemissionmodel
efficientdynamicprogrammingthatincorporatesrecurrent
conditionedontheentity-levelchain.
feedback. Weconductexperimentsonasyntheticdatasetas
Thesecondkeymodelingcontributionistoallowthetran- wellastworeal-worldtasks: modelingplayerpositionsin
sitionprobabilitiesamongstatesatalllevelstodependon
aprofessionalbasketballgameandmodelingsoldierhead-
feedbackfromobservationsattheprevioustimestep. Such ingsinasimulatedtrainingexercise. Comparedtorecent
feedbackprovidesanaturalmeansofmodelingtheinher- single-levelswitching-statebaselines,ourhierarchicalap-
ent context dependence of the state transitions, allowing proachcanproducebetterforecastsandrevealinterpretable
“bottomup”reactionstosituationaldemands. Inthebasket- groupdynamics,evenwhenbaselinesareallowedflexible
ballcontext,thisfeedbackcaptureshowabasketballplayer transitionsandemissionsvianeuralnetworks.
drivingtothebasketwillswitchtoanotherbehavioronce
theyreachtheirgoalandhowthisswitchmayinfluencethe 2 ModelFamily
trajectoriesoftheothersontheteam. Previously,Linder-
Here we present a family of hierarchical switching re-
manetal.[2017]incorporatedthisrecurrentfeedbackintoa
current dynamical models (HSRDMs) to describe a col-
modelwithaflatsingle-levelofswitchingstates. Weshow
lection of time series gathered from J entities that
how recurrent feedback can inform a two-level hierarchy
interact over a common time period (discretized into
ofsystem-levelandentity-levelstates,sothatentity-level
timesteps t ∈ {0,1,2,...,T}) and in a common envi-
observationscandrivesystem-leveltransitions.
ronment or system. For each entity, indexed by j ∈
Ouroverallcontributionisthusaproposedframework–hi- {1,...,J}, we observe a time series of feature vectors
erarchicalswitchingrecurrentdynamicalmodels–bywhich {x(j) ∈RD,t=0,1,2,...,T}.
t
our two key modeling ideas provide a natural solution to
Our HSRDM represents the j-th entity via two ran-
theproblemofunsupervisedmodelingoftimeseriesarising
dom variables: the observed features x(j) above and
fromagroupofinteractingentities. Unlikeothermodels, 0:T
ourframeworkallowseachentity’snext-stepdynamicstobe a hidden entity-level discrete state sequence zj =
0:T
drivenbybothasystem-leveldiscretestate(“top-down”in- {z(j) ∈{1,...,K },t=0,...,T}. We further assume
t j
a system-level latent time series of discrete statesWojnowicz,Rath,etal.
s ={s ∈{1,...,L},t=0,...,T}. Thecompletejoint Emission model. We generate the next observation for
0:T t
ofallrandomvariables,asdiagrammedinFigure1,factor- entityj viaastate-conditionedautoregression:
izesas
x(j) |x(j) ,z(j) ∼H where ζ =ζ(x(j) ,z(j)). (2.4)
T t t−1 t ζ t−1 t
p(x(1:J),z(1:J),s |θ)= p(s |θ) (cid:89) p(s |s ,x(1:J),θ)
0:T 0:T 0:T 0 t t−1 t−1 UserscanselectdistributionH tomatchthedomainofob-
ini(cid:124) tialsy(cid:123) s(cid:122) tems(cid:125) tatet=1(cid:124) systemsta(cid:123) te(cid:122)
transitions
(cid:125) servedfeaturesx(j):ourlaterexperimentsuseGaussiansfor
t
J (cid:20) T real-valuedvectorsandVon-Misesdistributionsforangles.
·(cid:89) p(z 0(j) |s 0,θ)(cid:89) p(z t(j) |z t( −j) 1,x( t−j) 1,s t,θ) Theparameterζ =ζ(x(j) ,z(j))dependsontheprevious
j=1 (cid:124) (cid:123)(cid:122) (cid:125) t=1(cid:124) (cid:123)(cid:122) (cid:125) t−1 t
initialentitystate entitystatetransitions observationx(j) andcurrententity-levelstatez(j).
t−1 t
T (cid:21)
·p(x(j) |z(j),θ)(cid:89) p(x(j) |x(j) ,z(j),θ) (2.1) Priors. TheAppendixdescribespriordistributionsp(θ)on
0 0 t t−1 t
(cid:124) (cid:123)(cid:122) (cid:125) t=1(cid:124) (cid:123)(cid:122) (cid:125) parametersassumedforthepurposeofregularization. We
initialobs. observationdynamics
use a “sticky" Dirichlet prior [Fox et al., 2011] to obtain
where θ represent all model parameters, and superscript smoothersegmentationsatthesystemlevel.
(1:J)denotestheunionoverallentities.
Specification. ToapplyHSRDMtoaconcreteproblem, a
The design principle of HSRDMs is to coordinate the usermustselectthenumberofsystemstatesLandentity
switching-state dynamics of multiple entities so they re- statesKaswellasfunctionalformsofg,f. Weassumethat
ceive top-down influence from system-level state as well gcanbeevaluatedinO(J).
asbottom-upinfluenceviarecurrentfeedbackfromentity
Special cases. If we remove the top-level system states
observations. Underthegenerativemodel,thenextentity-
s (or equivalently set L = 1), our HSRDM reduces to
level state depends on the interaction of three sources of 0:T
Linderman et al. [2017]’s recurrent autoregressive HMM
information: thenextstateofthesystem,thecurrentstate
(rAR-HMM).Ifweremovedrecurrentandcovariateterms
oftheentity,andthecurrententityobservation. Likewise,
fromthetransitionmodel,we’drecoveramulti-levelHMM.
thenextsystemstatedependsonthecurrentsystemstate
andobservationsfromallentities. 3 Inference
Transition models. To instantiate our two-level discrete Given observed time series x(1:J), we now explain how
statetransitiondistributions,weusecategoricalgeneralized 0:T
tosimultaneouslyestimateparametersθandinferapprox-
linearmodelstoincorporateeachsourceofinformationvia
imate posteriors over hidden states s for the system
additiveutilities 0:T
and z(1:J) for all J entities. Because all system-level
0:T
(cid:18)
and entity-level states are unobserved, the marginal like-
s |s ,x(1:J) ∼Cat-GLM (2.2)
t t−1 t−1 L lihoodp(x | θ)isanaturalobjectiveforparameteres-
0:T
(cid:19) timation. However,exactcomputationofthisquantity,by
Π(cid:101)Te st−1+Λg ψ(cid:0) x( t−1: 1J),υ t−1(cid:1) marginalizingoverallhiddenstates,isintractable. Given
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Lsystem-levelstatesandK entity-levelstates,computing
transitions recurrence
p(x | θ) naively via the sum rule requires a sum over
0:T
(LKJ)T values. While the forward algorithm [Rabiner,
(cid:18)
z(j) |z(j) ,x(j) ,s ∼Cat-GLM (2.3) 1989]resolvestheexponentialdependenceintime,theex-
t t−1 t−1 t K
ponential dependence in the number of entities persists:
(P(cid:101) j(st))Te
z(j)
+Ψ( jst)f ϕ(cid:0) x( t−j) 1,u( t−j) 1(cid:1)(cid:19) T onL HK S2 RJ Do Mp se .rat Tio hn iss ea xre por neq enu ti ir ae ld dt eo pd eo ndf eo nr cw ear md- ab ka ec sk iw na ferd
r-
(cid:124) (cid:123)(cid:122) t− (cid:125)1 (cid:124) recu(cid:123) rr(cid:122) ence (cid:125) enceprohibitivelycostlyeveninmoderatesettings;forin-
transitions
stance, when (T∗,J∗,L∗,K∗) = (100,10,2,4), a direct
Acrosslevels,commonsourcesofinformationdrivethese applicationoftheforwardalgorithmstillrequiresaround
utilities. First,thestate-to-statetransitiontermselectsan 220trillionoperations.
appropriatelogtransitionprobabilityvectorfrommatrices
Instead,wewillpursueastructuredapproximationqtothe
Π(cid:101),P(cid:101) viaaone-hotvectore
k
indicatingthepreviousstate
true (intractable) posterior over hidden states. Following
k. Second,recurrentfeedback governsthenextterm,via
featurizationfunctionsforthesystemg :RDJ →RR(cid:101) and previous work [Alameda-Pineda et al., 2021, Linderman
ψ
etal.,2017],wedefine
forentitiesf : RD → RD(cid:101) withparametersψ,ϕ(known
ϕ
orlearned)andweightsΛ,Ψ j. Ifoptionalexogenousco- q(s 0:T,z 0(1 :T:J))=q(s 0:T)q(z 0(1 :T:J)), (3.1)
variatesareavailableateitherthesystem-levelinυ or
t−1
entity-levelinu(j) ,theycanalsodrivethetransitionprob- intendingq(s ,z(1:J))≈p(s ,z(1:J) |x(1:J),θ).Each
t−1 0:T 0:T 0:T 0:T 0:T
abilities. Notethatinference(Sec.3)appliesnotmerelyto factorretainstemporaldependencystructure,avoidingthe
Eq.(2),buttoarbitraryinstantiations. problemsofcompletemean-fieldinference[Barberetal.,Discoveringgroupdynamicsinsynchronoustimeseries
2011]. Usingthisq,wecanformavariationallowerbound Mstepfortransition/emissionparameters. Updatesto
onthemarginalloglikelihood VLBO ≤logp(x(1:J)),de- someparameters,particularlyforemissionmodelparame-
0:T
finedas VLBO[θ,q]=E (cid:2) logp(x(1:J),z(1:J),s ,θ)(cid:3) + terswhenH hasexponentialfamilystructure(suchasthe
H(cid:2) q(z(1:J),s )(cid:3) . q 0:T 0:T 0:T Gaussian or Von-Mises AR likelihoods we use through-
0:T 0:T out experiments), can be done in closed-form. Other-
AsshownintheAppendix,computationofthisboundscales wise,ingeneral,weoptimizeθ bygradientascentonthe
asO(TJL2K2),cruciallylinearratherthanexponentialin VLBOobjective. Thishasthesamecostasthecomputation
thenumberofentitiesJ.Thisreducestheapproximatenum- ofthe VLBO,withruntimeO(TJL2K2).
berofoperationsrequiredforinferenceon(T∗,J∗,L∗,K∗)
Fulldetailsabouteachstep,aswellasrecommendationsfor
from220trillionto64thousand.
initialization,areintheAppendix. Wealsosharecode(built
Toestimateθandqgivendatax ,wepursuecoordinate upon JAX for automatic differentiation [Bradbury et al.,
0:T
ascentvariationalinference(CAVI;[Bleietal.,2017])on 2018]).
the VLBO(knownasvariationalexpectationmaximization
[Beal, 2003] when θ is approximated with a point mass). 4 RelatedWork
Given a suitable initialization, we alternate between the
updates: Belowwereviewseveralthreadsofthescientificliterature
inordertosituateourwork.
(cid:26) (cid:27)
q(z(1:J))∝exp E [logp(x(1:J),z(1:J),s |θ)] Continuous representations of individual sequences.
0:T q(s0:T) 0:T 0:T 0:T
Other efforts focus on latent continuous representations
(cid:26) (cid:27) ofindividualtimeseries. Thesecanproducecompetitive
q(s )∝exp E [logp(x(1:J),z(1:J),s |θ)]
0:T q(z(1:J)) 0:T 0:T 0:T predictions, but do not share our goal of providing a seg-
0:T
(cid:26) (cid:20) mentation at the system and entity level into distinct and
θ =argmax E (3.2) interpretable regimes. Probabilistic models with continu-
θ
q(z 0(1 :T:J))q(s0:T)
ouslatentstaterepresentationsareoftenbasedonclassic
(cid:21) (cid:27)
logp(x(1:J),z(1:J),s |θ) +logp(θ) lineardynamicalsystem(LDS)models[ShumwayandStof-
0:T 0:T 0:T
fer,1982]. DeepgenerativemodelsliketheDeepMarkov
Model[Krishnanetal.,2017]andDeepState[Rangapuram
whichgivethevariationalE-Zstep(VEZstep),variational etal.,2018]extendtheLDSapproachwithmoreflexible
E-S step (VES step), and M-step, respectively. All steps transitionsoremissionsvianeuralnetworks.
willimprovethe VLBO objectiveprovidedeachimproves
Discrete state representations of individual sequences.
itsownper-stepobjective.
Our focus is on discrete state representations which pro-
VESstepforsystem-levelstateposteriors. Wecanshow videinterpretablesegmentationsofavailabledata,alineof
theVESstepreducestoupdatingtheposteriorofoneHid- workthatstartedwithclassicapproachestoentity-level-only
denMarkovModelwithJ independentautoregressivecate- sequencemodelslikehiddenMarkovmodelsorswitching-
goricalemissions. Optimalvariationalparametersforthis statelineardynamicalsystems(SLDS)[Alameda-Pineda
posterior can be computed via a dynamic-programming etal.,2021,GhahramaniandHinton,2000]. Recentefforts
algorithm that extends classic forward-backward for an such as DSARF [Farnoosh et al., 2021], SNLDS [Dong
AR-HMM to handle recurrence. The runtime required is etal.,2020]andDS3M[XuandChen,2021]haveextended
O(cid:0) TJ(K2+KD+KL+KM)+TL2(cid:1)
. such base models to non-linear transitions and emissions
vianeuralnetworks. Alltheseeffortsstillrepresenteach
VEZstepforentity-levelstateposteriors. Wecanshow
timeseriesviaoneentity-leveldiscretestatesequence.
thattheVEZupdatecanreducetoaseparateHiddenMarkov
Model for each entity j with autoregressive categorical Discrete states via recurrence on continuous observa-
emissionswhichrecurrentlyfeedbackintothetransitions. tions. Lindermanetal.[2017]addanotionofrecurrence
Given a fixed system-level factor q(s 0:T), we can update toclassicSLDSmodels,increasingtheflexibilityineach
the state posterior for entity j independently of all other timestep’stransitiondistributionbyallowingdependence
entities. This means inference is linear in the number of onthepreviouscontinuousfeatures,notjusttheprevious
entities J, despite the fact that the HSRDM couples en- discretestates. Laterworkhasextendedrecurrenceideasin
tities via the system-level sequence. The linearity arises severaldirectionsthatimproveentity-levelsequencemodel-
even though our assumed mean-field variational family ing,suchasmulti-scaletransitiondependenciesviathetree-
of Eq. (3.1) did not make an outright assumption that structuredconstructionoftheTrSLDS[Nassaretal.,2019]
q(z(1:J)) = (cid:81)J q(z(j)). Optimal variational parame- orrecurrenttransitionmodelsthatcanexplicitlymodelstate
0:T j=1 0:T
tersforthisposteriorcanagainbecomputedbydynamic durations via RED-SDS [Ansari et al., 2021]. To model
programming. Theruntimerequiredtoupdateeachentity’s multiple recordings of worm neural activity, Linderman
factorisO(cid:0) T(cid:2) K2+KD2+KL+KM(cid:3)(cid:1)
. et al. [2019] pursue recurrent state space models that areWojnowicz,Rath,etal.
DSARFIndep. DSARFPool DSARFConcat
2 2 2
1 1 1
0 0 0
1 1 1
0 50 100 150 200 250 300 350 400 2 2 2
Time step
1 0 1 0.5 0.0 0.5 1.0 0.0 0.5
truth HSRDM rAR-HMMIndep. rAR-HMMPool rAR-HMMConcat
2 2 2 2 2
1 1 340 1 1 1
330
0 0 320 0 0 0
310
1 1 300 1 1 1
290
2 2 2 2
2 280
1 0 1 1 0 1 1 0 1 1 0 1 1 0 1
Figure2:ComparingmodelpredictionsofheldouttimesegmentofoneentityinFigureEighttask. Topleft:Datagenerating
process. Eachpanelgivesthe(x,y)coordinatesfromoneentity. Modelsweretrainedonalldatafromentities1-2andtimes
0-280forentity3,thenaskedtoforecasttimes281onwardforentity3. Bottomleft: OurHSRDM predictionsclosely
matchestruthinall3samples. Bottomright: BestsampleforrAR-HMMbaseline,undereachpossiblestrategy(Indep.,
Pool,andConcat.,definedinSec.5.1)foradaptinganentity-onlymodeltoourhierarchicalsetting. Topright: Bestof3
samplesfromDSARFbaselineundereachstrategy. SeeApp. forfurthervisuals.
describedashierarchicalbecausetheyencouragesimilarity lengthscalesofdependencywithinanindividualsequence.
between each worm entity’s custom dynamics model via WhileHHMMshavebeenappliedwidelytoapplications
commonparameterpriorsinhierarchicalBayesianfashion. liketextanalysis[Skounakisetal.,2003]orhumanbehav-
Theirmodelassumesonlyentity-leveldiscretestates. iorunderstanding[Nguyenetal.,2005],toourknowledge
HHMMshavenotbeenusedtocoordinatemultipleentities
Multi-level discrete representations. Stanculescu et al.
overlappingintime.
[2014]developedahierarchicalswitchinglineardynamical
system(HSLDS)formodelingthevitalsigntrajectoriesof Modelsofteamsinsportsanalytics. TernerandFranks
individualinfantsinanintensivecareunit. Therootlevel [2021] survey approaches to player-level and team-level
oftheirdirectedgraphicalmodelassumesadiscretestate modelsinbasketball. MillerandBornn[2017]applytopic
sequence(analogoustoours)indicatingwhetherdisease modelstotrackingdatatodiscoverhowlow-levelactions
was present or absent in the individual over time, while (e.g. run-to-basket)mightco-occuramongteammatesdur-
lowerleveldiscretestates(analogoustoourz)indicatethe ing the same play. Metulini et al. [2018] model the con-
occurrenceofspecific“factors”representingclinicalevents vex hull formed by the court positions of the 5-player
suchasbrachycardiaordesaturation. Whiletheirgraphical teamthroughoutapossessionviaonesystem-levelhidden
modellookssimilartooursinitsmulti-leveldiscretestruc- Markovmodel.Incontrast,ourworkprovidesacoordinated
ture,weemphasizethreekeydifferences. First,theymodel two-levelsegmentationrepresentingthesystemaswellas
individualtimeseriesnotmultipleinteractingentities. Sec- individuals.
ond,theyassumefully-superviseddatafortraining,where
Personalizedmodels. Severalswitchingstatemodelsas-
each timestep t is labeled with top-level and factor-level
sume each sequence in a collection have unique or per-
states. In contrast, our structured VI routines to estimate
sonalizedparameters,suchcustomtransitionprobabilities
parameters in the unsupervised setting are new. Finally,
oremissiondistributions[AlaaandvanderSchaar,2019,
theirHSLDSdoesnotincorporaterecurrentfeedbackfrom
Fox et al., 2014, Severson et al., 2020]. In this style of
continuousobservations.
work,entitytimeseriesmaybecollectedasynchronously,
Morebroadly,HierarchicalHiddenMarkovModels(HH- and entities are related by shared priors on their parame-
MMs) [Fine et al., 1998] and their extensions [Bui et al., ters. Incontrast,wefocusonentitiesthataresynchronous
2004,Helleretal.,2009]describeasingleentity’sobserved inthesameenvironment,andrelateentitiesdirectlyviaa
sequencewithmultiplelevelsofhiddenstates. Thechief system-leveldiscretechainthatmodifiesentity-levelstate
motivation of the HHMM is to model different temporal transitions.
spetsemiTDiscoveringgroupdynamicsinsynchronoustimeseries
Modelsofcoordinatedentities. Severalrecentmethodsdo structures.ForeachofrAR-HMMandDSARF,wetrythree
jointlymodelmultipleinteractingentitiesor“agents”,often differentstrategiesformodelingasystemofentities. First,
usingsophisticatedneuralarchitectures. Zhanetal.[2019] completeindependence(“Indep.”),whereaseparatemodel
develop a variational RNN where trajectories are coordi- isfittoeachentity. Next,completepooling(“Pool”),where
natedinshorttimeintervalsviaentity-specificlatentvari- one single model is fit on all data, treating each entity’s
ablescalled“macro-intents”. Yuanetal.[2021]developthe timeseriesasani.i.dobservation. Finally,concatentation
AgentFormer,atransformer-inspiredstochasticmulti-agent (“Concat.”),whichfitsonemodeltoonemultivariatetime
trajectory prediction model. Alcorn and Nguyen [2021] seriesofexpandeddimensionD′ = J ·D constructedby
developballer2vec++,atransformerspecificallydesigned stacking up all entity-specific features x(1),x(2),...x(J)
t t t
tocapturecorrelationsamongbasketballplayertrajectories. ateachtimet.
Xu et al. [2022] introduce GroupNet to capture pairwise
andgroup-wiseinteractions. Unliketheseapproaches,ours 5.1 FigureEight: Synthetictaskofcoordinated
buildsuponswitching-statemodelswithclosed-formpos- dynamicsovertime
teriorinference,producesdiscretesegmentations,andmay
ToillustratethepotentialofourHSRDMasamodelforco-
bemoresampleefficientforapplicationslikeSec.5.3with
ordinatedgroupdynamics,westudyasyntheticdatasetwe
onlyafewminutesofdata.
callFigureEight. Inthetruegenerativeprocess(detailedin
Modelsthatlearninteractiongraphs. Someworks[Kipf Appendix),eachentityswitchesbetweenclockwisemotion
etal.,2018,Sanchez-Gonzalezetal.,2020]pursuethegoal around a top loop and counter-clockwise motion around
oflearninganinteractiongraphgivenmanyentity-leveltime abottomloop,sotheobservedentity-level2Dspatialtra-
series, where nodes correspond to entities and edge exis- jectory over time approximates the shape of an “8”. The
tenceimpliesadirect,pairwiseinteractionbetweenentities. trajectories of the entities for each loop is governed by a
Forsomeapplications,discoveringpairwiseinteractionsis Gaussianvectorautoregressionprocess. Transitionbetween
aninterestinggoal. Inourchosenapplications(e.g. thesim- theseloopstatesdependonentity-levelrecurrentfeedback
ulatedbattleexercisesorbasketballplayermovements),we (switchesbetweenloopsareonlyprobableneartheorigin,
hypothesizethatthegraphwillalwaysbefully-connected. wheretheloopsintersect)andcruciallyonabinarysystem-
Moreover,interactiongraphapproachesburdensomelyre- levelstate(whichsetswhichloopisfavoredforallentities
quireruntimesthatarequadraticinthenumberofentities atthemoment). Thoughcoordinated,entitytrajectoriesare
J onourfully-connectedapplications. Incontrast,ourap- notperfectlysynchronized,varyingduetoindividualrota-
proachmodelsthesystem-levelgroupdynamicsexplicitly tionspeeds,initialpositions,andrandomeffects(visualsin
whilekeepingthecostofprocessingscalablylinearinJ. Appendix).
Givenadatasetofthreeentitiesobservedtogetherfor280
5 Experiments
timesteps,wepursuepartialforecasting(Sec.C.2)ofone
targetentity’sremainingtrajectoryfortimes281-350,given
We now compare our proposed model to several alterna-
fullyobservedtrajectoriesfromtheothertwoentities. The
tivesintermsofquantitativeperformance(viashort-term
truetargettrajectoryforthisheldoutwindowisillustrated
multi-step-aheadforecastingerror), aswellasqualitative
in Fig. 2: we see a smooth transition from the top loop
performanceatinferringusefuldiscreterepresentationsof
to the bottom loop. Each tested method is evaluated by
system-levelandentity-leveldynamics. Asarepresentative
howwellitsgeneratedsampletrajectoriesfortheheldout
ofsophisticatedneuralarchitecturesforcoordinatedentities,
periodadequatelymatchthetrueheldoutbehaviorfromthe
we selected AgentFormer [Yuan et al., 2021], due to its
generativeprocess.
availablecodethatsupportsmulti-stepforecasting. Unfortu-
nately,theballer2veccodebase[AlcornandNguyen,2021] WeapplyourHSRDMwithentity-levelrecurrencef settoa
onlysupportsone-step-aheadforecastingasofthiswriting. radialbasisfunction. Forsimplicity,wedonotusesystem-
levelrecurrenceg. Whilewedonotexpectrecurrenceto
We further selected two competitive baselines that only
improvetrainingfit,wedoexpectittoimproveforecasting,
capture entity-level dynamics (not system-level) but still
asitisnecessarytocaptureakeyaspectofthetrueprocess:
inferentity-leveldiscretesegmentations. First,wecompare
thatswitchesbetweenloopsareonlyprobableneartheori-
toarecurrentautoregressivehiddenMarkovmodel(rAR-
gin. Reproducibledetailsforourmethodandallalternative
HMM; [Linderman et al., 2017]). This is essentially an
methods(architectures,training,andhyperparameters)are
ablationofourmethodthatremovesourhierarchy. Second,
intheAppendix.
wecomparetothedeepswitchingautoregressivefactoriza-
tion model (DSARF; [Farnoosh et al., 2021]), which we Fig.2visualizesthebestsampleforecastof3foreachbase-
choseasarepresentativeofmodelsthatinferdiscretelatent line, as well as the worst forecast of 3 from our HSRDM.
segmentationswhilealsoreportingstate-of-the-artforecast- Whileourproposedmodelprovidesanaturalfit,thebase-
ingperformanceagainstrecentalternativesandusingdeep line models struggle to reproduce the coordinated group
neuralnetworkstoflexiblydefinetransitionandemission dynamics.Wojnowicz,Rath,etal.
L.James K.Love T.Mozgov J.Smith M.Williams
HSRDM
Nosystem
state(rARHMMs)
Norecurrent
feedback
Agent-
former
Figure 3: Sample forecasts of NBA player location trajectories. Shown in grey are true player trajectories from the
forecastingwindowofthetesteventonwhichourmodelhadmedianforecastingerror. Incolorarethreesampledforecasts
fromeachmodel;purple/red/greenhas1st/5th/10thbestforecastingerror(from20samples). Timerunsfromlighttodark.
5.2 Forecasting2Dpositiontrajectoriesofall10 Table1: ForecastingNBAplayertrajectories.
playersinprobasketballgames
Meanforecastingerror(infeet)
afterntraininggames
Wenextmodelthe5playersoftheNBA’sClevelandCava-
n=1 n=5 n=20
liers(CLE),togetherwiththeir5opponents,acrossmultiple HSRDM(ours) 16.3(–) 14.4(–) 14.4(–)
games in an open-access dataset [Linou, 2016] of player Nosystemstate(rARHMM) 15.9(0.2) 15.5(0.2) 15.6(0.2)
positionsovertimerecordedfromCLE’s2015-2016cham- Norecurrentfeedback 16.6(0.3) 16.0(0.2) 16.3(0.2)
pionshipseason. Wefocusedexclusivelyonthe29games Fixedvelocity 16.8(0.5) 16.8(0.4) 16.8(0.4)
Agentformer 33.5(0.4) 21.2(0.6) 25.8(0.3)
involvingoneofCLE’sfourmostcommonstartinglineups.
Werandomlyassignedthesegamestotraining(20games),
validation(4games),andtest(5games)sets. dependent rARHMMs [Linderman et al., 2017] for each
player;andanablationoftherecurrentfeedback. AsinYeh
Wespliteachgameintonon-overlappingbasketballevents,
etal.[2019],wealsotryacrudebutoftencompetitivefixed
typicallylasting20secondsto3minutes. Eventsconcate- velocitybaseline.TrainingHSRDMona2023Macbookwith
nateconsecutiveplays(e.g. shotblock→reboundoffense
AppleM2Prochiponn=1,5,20traininggamestook2,
→shotmade)fromtherawdatasetuntilthereisanabrupt
15,and45minutes,respectively. TrainingAgentFormeron
breakinplayermotionorasamplingintervallongerthan
anIntelXeonGold6226RCPUtook1.5,6,and13hours,
thenominalsamplingrate. Eacheventgivesan(x,y)court
respectively.
position for all 10 players, and is modeled as an i.i.d. se-
quence from our proposed HSRDM or competitor models. Toevaluatemethods,werandomlyselecta6secondfore-
We standardized the court so that CLE’s offense always castingwindowwithineachofthe75testsetevents. Pre-
facesthesamedirection(left),anddownsampledthedata cedingobservationsintheeventaretakenascontext,and
to5Hz. postcedingobservationsarediscarded. Wesample20fore-
castsfromeachmethod. Tab.1reportsthemeandistance
Oursystem-levelrecurrencegreportsallplayerlocations
infeetfromforecaststogroundtruth,withthemeantaken
x(1:J)tothesystem-leveltransitionfunction,allowingthe
overallevents,samples,players,timesteps,anddimensions.
t
probabilityoflatentgamestatestodependonplayerloca- We perform paired t-tests on the per-event differences in
tions. FollowingLindermanetal.[2017],ourentity-level meandistancesbetweenourmodelvscompetitors,using
recurrencefunctionf reportsanindividualplayer’slocation BenjaminiandHochberg[1995]’scorrectionformultiple
x(j) (andout-of-boundsindicators)tothatplayer’sentity- comparisonsoverpositivelycorrelatedtests. Thestandard
t
leveltransitionfunction,allowingeachplayer’sprobability errors for the mean differences are given in parentheses.
of remaining in autoregressive regimes to vary in likeli- Methods whose forecasting performance are not statisti-
hoodoverthecourt. Finally,ouremissionsdistributionisa callysignificantlydifferentfromHSRDMatthe.05levelare
Gaussianvectorautoregressionwithentity-state-dependent giveninbold. WefindthatHSRDMprovidesbetterforecasts
parameters(seeSec.F). thanAgentformerandfixedvelocity,andthatHSRDM’s
twokeymodelingcontributions(multi-levelrecurrentfeed-
WecomparetheforecastingperformanceoftheHSRDMto
backandsystem-levelswitches)improveforecasts.
multiplecompetitors: Agentformer[Yuanetal.,2021];
an ablation of the system-level switches, which gives in- Fig.3showssampledforecastsfromourmodelandbase-Discoveringgroupdynamicsinsynchronoustimeseries
Table2: StatisticsonNBAplayerforecasts.
90 (N) 0 0.30
1
135 45 2 0.25
%InBounds DirectionalVariation
3
HSRDM(ours) .915✗ .506✗ 4 0.20
5 Nosystemstate(rARHMM) .908✗ (.003) .631✗ (.010) 180 (W) 0 (E) 6 0.15
7
Norecurrentfeedback .814✗ (.007) .469✗ (.016) 0.10
0.05
225 315
lines. Weseethatsystem-levelswitcheshelptocoordinate 0.00
270 (S) 09:18:51 09:19:03 09:19:15 09:19:27 09:19:39 09:19:51
entities; players move in more coherent directions under
Squad state
HSRDM than without the top-level system-state. We also Green Yellow Red
see that multi-level recurrent feedback supports location- S 0.54 0.76 0.90
dependentstatetransitions;playersaremorelikelytomove
W 0.32 0.01 0.02
towardsfeasible(in-bounds)locationsunderHSRDMthan
withoutrecurrence. Theseobservationsarecorroboratedin N 0.14 0.01 0.09
Tab.2,whichsummarizestwostatisticscomputedoverthe
0.03 0.09 0.03
entiretestset: %InBounds,themeanpercentageofeach E 09:18:51 09:19:03 09:19:15 09:19:27 09:19:39 09:19:51
forecastthatisinbounds,andDirectionalVariation,which
Figure 4: Modeling the heading directions of a squad of
measuresincoherentmovementsbybasketballplayersvia
soldiersengagedinsimulatedbattle. Topleft: Headingdi-
thecircularvarianceacrossplayersofthemovementdirec-
rections(indegrees)ofasquadofsoldiersovertime. Each
tionbetweenthefirstandlasttimestepsintheforecasting
colorrepresentsadifferentsoldier. Timemovesfromcenter
window. WetesthypothesesasinTab.1. Methodssignif-
ofcircletoboundary. Topright: Inferredsquad-levelstates
icantlydifferentfromtheHSRDMbaselineatthe.01level
s (colors)superimposedoverblackcurverepresenting
aremarkedwitharedx. Wefindthatremovingrecurrence 0:T
thesquad’scumulativesecurityriskinthenorthdirection
significantlyreducesthemeanpercentageofforecaststhat
(elapsedtimesinceanysoldiercheckedtheirblindside)as
isinbounds,andremovingthesystem-levelswitchessignif-
a function of time. The learned red squad state seems to
icantlyincreasesthedirectionalvariationacrossplayers.
indicatehighsecurityrisk. Thesesquadstatescanmodu-
5.3 Maintainingvisualsecurityinasimulatedbattle latesoldier-levelheadingdynamics. Bottomleft: Inferred
entity-levelstatesz (colors)forSoldier6,superimposed
0:T
As a final demonstration, we investigate the ability of a
onobservedtimeseriesofheadingdirectionfromthatsol-
squad of active-duty soldiers in a NATO-affiliated army
dier’s helmet IMU. The light blue state’s autoregressive
to maintain visual security while engaged in a simulated
emissiondynamicsproducearapidturntothenorth. Twice
trainingexerciseinwhichenemyfirecomesfromthesouth.
this state persisted long enough for the soldier to reduce
Focusonthesouthcreatesapotentialblindsidetothenorth.
securityriskinthenorth(around19:03and19:20). Bottom
If this blindside is left unchecked for a sufficiently long
right: The learned probability that Soldier 6 turns to the
time,thenatleastonesoldiershouldbrieflyturntheirhead
northfromvarioussoldier-specificstates(z,rows)depends
toregainvisibilityandreducethesquad’svulnerabilityto
upon the squad-level states (s, columns). The soldier is
a blindside attack. The squad was instructed that visual
mostlikelytopersistinturningnorthwhenthesquadhasa
security was a key subtask among several overall goals.
securityvulnerability(sisred).
Strong performance at this subtask requires coordination
acrossallsoldiersinthesquad. useVonMisesautoregressionsastheemissionmodelH
ζ
forthek-thstateofthej-thsoldier:
Thedatasetconsistsofsoldier-specificunivariatetimeseries
ofheadingdirectionanglesx(j) recordedat130Hzfrom (cid:18) (cid:19)
t x(j) |x(j) ,{z(j)=k}∼VM µ (cid:0) x(j) (cid:1) , κ ,
helmetinertialmeasurementunits(IMU).Wedownsample t t−1 t j,k t−1 j,k
to6.5Hztoreduceautocorrelations. Weinvestigatea12 whereµ (x(j) )=α x(j) +δ (5.1)
minuterecordingofonesquadof8soldiers. Therawdata j,k t−1 j,k t−1 j,k
fromthefirstminuteofcontactisillustratedinFig.4. Due TheVonMisesdistribution[Banerjeeetal.,2005,Fisherand
toprivacyconcerns,dataisnotshareable. Thisstudywas Lee,1994],denotedVM(µ,κ),isadistributionoverangles
approvedbytheU.S.ArmyCombatCapabilitiesDevelop- ontheunitcircle,governedbymeanµandconcentration
ment Command Armaments Center Institutional Review κ>0. Here,α isanautoregressivecoefficient,δ isa
j,k j,k
BoardandtheArmyHumanResearchProtectionsOffice driftterm,andκ isaconcentrationforentityj instatek.
j,k
(ProtocolNumber: 18-003)
UsingtheinferencemethodfromSec.3,weobtainedthe
WefitourHSRDMtothisdata,capturingthegoalofvisual
resultsvisualizedinFig.4.Inspectionoftheinferredsystem-
securitybysettingthesystem-levelrecurrencefunctiongto
levelstates,entity-levelstates,andlearnedtransitionproba-
thenormalizedelapsedtimesinceanyoneoftheJ soldiers
lookedwithinthenorthquadrantofthecircle. Soldierhead- bilitiessuggeststhatthemodellearnsaspecialturnnorth
ingsmustremainontheunitcirclethroughouttime,sowe state(blue)forSoldier6thatisparticularlyprobablewhen
ksir
ytiruceS
)tnecer
tsom(
etats
reidloS
htroN
nruT
deR
rebmA
neerGWojnowicz,Rath,etal.
theentiresquadreachestheredstateofhighelapsedtime Alameda-Pineda,X.,Drouard,V.,andHoraud,R.P.(2021).
sinceanyblindsidecheck. Variationalinferenceandlearningofpiecewiselineardy-
namicalsystems. IEEETransactionsonNeuralNetworks
6 Conclusion
andLearningSystems,33(8):3753–3764.
We have introduced a family of models for capturing the Alcorn, M. A. and Nguyen, A. (2021). Baller2vec++: A
dynamicsofindividualentitiesevolvingincoordinatedfash- Look-AheadMulti-EntityTransformerForModelingCo-
ionwithinasharedenvironmentoverthesametimeperiod. ordinatedAgents.
Thesemodelsadmitefficientstructuredvariationalinference
Altman,R.M.(2007). Mixedhiddenmarkovmodels: an
in which coordinate ascent can alternate between E-step
extension of the hidden markov model to the longitu-
dynamicprogrammingroutinessimilartoclassicforward-
dinal data setting. Journal of the American Statistical
backwardrecursionstoinferhiddenstateposteriorsatboth
Association,102(477):201–210.
system-andentity-levelsandM-stepupdatestotransition
andemissionparametersthatalsouseclosed-formupdates Ansari,A.F.,Benidis,K.,Kurle,R.,Türkmen,A.C.,Soh,
whenpossible. Acrossseveraldatasets,we’veshownourap- H.,Smola,A.J.,Wang,Y.,andJanuschowski,T.(2021).
proachrepresentsanaturalwaytocapturesystem-to-entity DeepExplicitDurationSwitchingModelsforTimeSe-
andentity-to-systemcoordinationwhilekeepingcostslinear ries. InAdvancesinNeuralInformationProcessingSys-
inthenumberofentities. tems(NeurIPS).
Limitations. Several coordinate ascent steps in any per- Banerjee,A.,Dhillon,I.S.,Ghosh,J.,Sra,S.,andRidgeway,
entityrAR-HMMwithGaussianemissionsscalequadrati- G.(2005). Clusteringontheunithypersphereusingvon
callyinD,soscalingbeyondafewdozenfeaturespresents mises-fisherdistributions. JournalofMachineLearning
a challenge. Furthermore, the parametric forms of both Research,6(9).
transitions and emissions in our model allow tractability Barber,D.,Cemgil,A.T.,andChiappa,S.(2011). Bayesian
butclearlylimitexpressivitycomparedtorecentdeepprob- timeseriesmodels. CambridgeUniversityPress.
abilisticmodels[Krishnanetal.,2017]. Scalingtomany
Beal,M.J.(2003). Variationalalgorithmsforapproximate
moreentitieswouldrequireextensionsofourstructuredVI
Bayesian inference. University of London, University
to process minibatches of entities [Hoffman et al., 2013].
CollegeLondon(UnitedKingdom).
Scalingtomuchlongersequencesmightrequireprocessing
randomlysampledwindows[Fotietal.,2014]. Benjamini, Y. and Hochberg, Y. (1995). Controlling the
falsediscoveryrate: apracticalandpowerfulapproachto
Future directions. For some applied tasks, it may be
multipletesting. JournaloftheRoyalstatisticalsociety:
promisingtoextendourtwo-levelsystem-entityhierarchyto
seriesB(Methodological),57(1):289–300.
evenmorelevels(e.g. torepresentnestedstructuresofpla-
toons,squads,andindividualsoldiersallpursingthesame Blei,D.M.,Kucukelbir,A.,andMcAuliffe,J.D.(2017).
mission).Additionally,wecouldextendfromrARHMMsto Variationalinference: Areviewforstatisticians. Journal
switchinglineardynamicalsystemsbyaddinganadditional of the American statistical Association, 112(518):859–
latentcontinuousvariablesequencebetweendiscreteszand 877.
observationsxinthegraphicalmodel.
Bradbury,J.,Frostig,R.,Hawkins,P.,Johnson,M.J.,Leary,
C.,Maclaurin,D.,Necula,G.,Paszke,A.,VanderPlas,J.,
Acknowledgments
Wanderman-Milne,S.,andZhang,Q.(2018). JAX:Com-
ThisresearchwassponsoredbytheU.S.ArmyDEVCOM posabletransformationsofPython+NumPyprograms.
SoldierCenter,andwasaccomplishedunderCooperative
Bui, H. H., Phung, D. Q., and Venkatesh, S. (2004). Hi-
AgreementNumberW911QY-19-2-0003. Theviewsand
erarchical Hidden Markov Models with General State
conclusions contained in this document are those of the
Hierarchy. InProceedingsoftheNationalConferenceon
authors and should not be interpreted as representing the
ArtificialIntelligence(AAAI).
official policies, either expressed or implied, of the U.S.
ArmyDEVCOMSoldierCenter,ortheU.S.Government. Dong, Z., Seybold, B. A., Murphy, K. P., and Bui, H. H.
TheU.S.Governmentisauthorizedtoreproduceanddis- (2020). Collapsed amortized variational inference for
tributereprintsforGovernmentpurposesnotwithstanding switchingnonlineardynamicalsystems. InInternational
anycopyrightnotationhereon. ConferenceonMachineLearning.
ApprovedforPublicReleaseOPSEC#PR2023_28848 Farnoosh,A.,Azari,B.,andOstadabbas,S.(2021). Deep
SwitchingAuto-RegressiveFactorization:Applicationto
References TimeSeriesForecasting.InAAAIConferenceonArtificial
Intelligence.
Alaa, A. M. and van der Schaar, M. (2019). Attentive
state-spacemodelingofdiseaseprogression. Advances Felsen,P.,Lucey,P.,andGanguly,S.(2018). Wherewill
inneuralinformationprocessingsystems,32. theygo? predictingfine-grainedadversarialmulti-agentDiscoveringgroupdynamicsinsynchronoustimeseries
motion using conditional variational autoencoders. In Linderman, S., Nichols, A., Blei, D., Zimmer, M., and
Proceedings of the European conference on computer Paninski, L.(2019). Hierarchicalrecurrentstatespace
vision(ECCV),pages732–747. modelsrevealdiscreteandcontinuousdynamicsofneural
activityinC.elegans.
Fine,S.,Singer,Y.,andTishby,N.(1998). TheHierarchi-
calHiddenMarkovModel: AnalysisandApplications. Linou,K.(2016). NBAPlayerMovements.
MachineLearning,32.
Liu, D., Lu, T., Niu, X.-F., and Wu, H. (2011). Mixed-
Fisher, N. and Lee, A. (1994). Time series analysis of effects state-space models for analysis of longitudinal
circular data. Journal of the Royal Statistical Society: dynamicsystems. Biometrics,67(2):476–485.
SeriesB(Methodological),56(2):327–339.
Lucey,P.,Bialkowski,A.,Carr,P.,Morgan,S.,Matthews,
Foti, N., Xu, J., Laird, D., and Fox, E. (2014). Stochas- I.,andSheikh,Y.(2013). Representinganddiscovering
ticvariationalinferenceforhiddenMarkovmodels. In adversarial team behaviors using player roles. In Pro-
AdvancesinNeuralInformationProcessingSystems. ceedings of the IEEE Conference on Computer Vision
andPatternRecognition,pages2706–2713.
Fox,E.B.,Hughes,M.C.,Sudderth,E.B.,andJordan,M.I.
(2014).Jointmodelingofmultipletimeseriesviathebeta Metulini,R.,Manisera,M.,andZuccolotto,P.(2018). Mod-
processwithapplicationtomotioncapturesegmentation. ellingthedynamicpatternofsurfaceareainbasketball
AnnalsofAppliedStatistics,8(3):1281–1313. anditseffectsonteamperformance. JournalofQuantita-
tiveAnalysisinSports,14(3):117–130.
Fox,E.B.,Sudderth,E.B.,Jordan,M.I.,andWillsky,A.S.
(2011). AstickyHDP-HMMwithapplicationtospeaker Miller,A.C.andBornn,L.(2017). PossessionSketches:
diarization. Annals of Applied Statistics, 5(2A):1020– MappingNBAStrategies. InMITSloanSportsAnalytics
1056. Conference.
Ghahramani,Z.andHinton,G.E.(2000). Variationallearn- Nassar,J.,Linderman,S.W.,Bugallo,M.,andPark,I.M.
ingforswitchingstate-spacemodels. Neuralcomputa- (2019). Tree-StructuredRecurrentSwitchingLinearDy-
tion,12(4):831–864. namicalSystemsforMulti-ScaleModeling. InInterna-
tionalConferenceonLearningRepresentations.arXiv.
Gu,A.,Goel,K.,andRé,C.(2022). EfficientlyModeling
LongSequenceswithStructuredStateSpaces. InInterna- Nguyen,N.,Phung,D.,Venkatesh,S.,andBui,H.(2005).
tionalConferenceonLearningRepresentations(ICLR). Learninganddetectingactivitiesfrommovementtrajec-
arXiv. toriesusingthehierarchicalhiddenMarkovmodel. In
2005IEEEComputerSocietyConferenceonComputer
Hamilton, J. D. (1994). Time series analysis. Princeton
Vision and Pattern Recognition (CVPR’05), volume 2,
universitypress.
pages955–960vol.2.
Hamilton,J.D.(2010). Regimeswitchingmodels. Macroe-
Raabe,D.,Nabben,R.,andMemmert,D.(2023). Graph
conometricsandtimeseriesanalysis,pages202–209.
representationsfortheanalysisofmulti-agentspatiotem-
Hamilton, J. D. (2020). Time series analysis. Princeton poralsportsdata. AppliedIntelligence,53(4):3783–3803.
universitypress.
Rabiner,L.R.(1989).ATutorialonHiddenMarkovModels
Heller,K.A.,Teh,Y.W.,andGörür,D.(2009). Infinitehier- andSelectedApplicationsinSpeechRecognition. Proc.
archicalhiddenMarkovmodels. InArtificialIntelligence oftheIEEE,77(2):257–286.
andStatistics.
Rangapuram, S. S., Seeger, M., Gasthaus, J., Stella, L.,
Hoffman, M., Blei, D., Wang, C., and Paisley, J. (2013). Wang,Y.,andJanuschowski,T.(2018). DeepStateSpace
Stochastic Variational Inference. Journal of Machine Models for Time Series Forecasting. In Advances in
LearningResearch,14(1). NeuralInformationProcessingSystems(NeurIPS).
Kipf,T.,Fetaya,E.,Wang,K.-C.,Welling,M.,andZemel, Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R.,
R. (2018). Neural Relational Inference for Interacting Leskovec,J.,andBattaglia,P.(2020). LearningtoSimu-
Systems. InProceedingsofthe35thInternationalCon- lateComplexPhysicswithGraphNetworks. InProceed-
ferenceonMachineLearning,pages2688–2697.PMLR. ings of the 37th International Conference on Machine
Learning,pages8459–8468.PMLR.
Krishnan,R.G.,Shalit,U.,andSontag,D.(2017). Struc-
turedInferenceNetworksforNonlinearStateSpaceMod- Severson, K. A., Chahine, L. M., Smolensky, L., Ng, K.,
els. InAAAIConferenceonArtificialIntelligence.arXiv. Hu,J.,andGhosh,S.(2020). Personalizedinput-output
hiddenmarkovmodelsfordiseaseprogressionmodeling.
Linderman, S., Johnson, M., Miller, A., Adams, R., Blei,
InMachineLearningforHealthcareConference,pages
D., andPaninski, L.(2017). Bayesianlearningandin-
309–330.PMLR.
ferenceinrecurrentswitchinglineardynamicalsystems.
InArtificialIntelligenceandStatistics, pages914–922. Shumway,R.H.andStoffer,D.S.(1982). AnApproach
PMLR. toTimeSeriesSmoothingandForecastingUsingtheEmWojnowicz,Rath,etal.
Algorithm. JournalofTimeSeriesAnalysis, 3(4):253–
264.
Shumway,R.H.,Stoffer,D.S.,andStoffer,D.S.(2000).
Time series analysis and its applications, volume 3.
Springer.
Skounakis,M.,Craven,M.,andRay,S.(2003).Hierarchical
HiddenMarkovModelsforInformationExtraction. In
InternationalJointConferencesonArtificialIntelligence
(IJCAI).
Stanculescu,I.,Williams,C.K.I.,andFreer,Y.(2014). A
Hierarchical Switching Linear Dynamical System Ap-
plied to the Detection of Sepsis in Neonatal Condition
Monitoring. InUncertaintyinArtificialIntelligence.
Sun,J.J.,Karigo,T.,Chakraborty,D.,Mohanty,S.P.,Wild,
B.,Sun,Q.,Chen,C.,Anderson,D.J.,Perona,P.,Yue,
Y.,andKennedy,A.(2021). TheMulti-AgentBehavior
Dataset: MouseDyadicSocialInteractions.
Terner,Z.andFranks,A.(2021).ModelingPlayerandTeam
PerformanceinBasketball. AnnualReviewofStatistics
andItsApplication,8(1):1–23.
van Dijk, D., Teräsvirta, T., and Franses, P. H. (2002).
SmoothTransitionAutoregressiveModels—aSurveyof
RecentDevelopments. EconometricReviews,21(1):1–47.
Wojnowicz,M.,Buck,M.D.,andHughes,M.C.(2023).
Approximateinferencebybroadeningthesupportofthe
likelihood. InFifthSymposiumonAdvancesinApproxi-
mateBayesianInference.
Xu, C., Li, M., Ni, Z., Zhang, Y., and Chen, S. (2022).
GroupNet: Multiscale hypergraph neural networks for
trajectory prediction with relational reasoning. In Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition.
Xu,X.andChen,Y.(2021). DeepSwitchingStateSpace
Model(DS$3ˆ$M)forNonlinearTimeSeriesForecasting
withRegimeSwitching.
Yeh, R. A., Schwing, A. G., Huang, J., and Murphy, K.
(2019). Diversegenerationformulti-agentsportsgames.
In Proceedings of the IEEE/CVF Conference on Com-
puterVisionandPatternRecognition,pages4610–4619.
Yuan, Y., Weng, X., Ou, Y., and Kitani, K. M. (2021).
AgentFormer: Agent-Aware Transformers for Socio-
TemporalMulti-AgentForecasting. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,
pages9813–9823.
Zhan,E.,Zheng,S.,Yue,Y.,Sha,L.,andLucey,P.(2019).
GeneratingMulti-AgentTrajectoriesusingProgrammatic
WeakSupervision. InInternationalConferenceonLearn-
ingRepresentations.Discoveringgroupdynamicsinsynchronoustimeseries
Supplementary Appendix
ContentsofSupplementaryAppendix
A RecurrentAutoregressiveHMMs: ModelandInferenceDetails 13
A.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
A.2 StateEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.3 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B ProposedHSRDM:ModelandInferenceDetails 17
B.1 Priorsonmodelparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.2 Updatingtheposterioroversystem-levelstates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.3 Updatingtheposterioroverentity-levelstates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.4 Updatingtheparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.5 Variationallowerbound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.6 Smartinitialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.7 MultipleExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C Methodology: SupplementalInformation 22
C.1 Modelfit. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.2 Partialforecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
D FigureEightToyData: SupplementalInformation 23
D.1 Datageneratingprocess . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D.2 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.3 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.4 Results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
E VisualSecurityExperiment: SupplementalInformation 28
F BasketballExperiment: SupplementalInformation 28
F.1 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
F.2 Evaluationstrategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
F.3 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
F.4 Futuredirections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
G Code 33Wojnowicz,Rath,etal.
A RecurrentAutoregressiveHMMs: ModelandInferenceDetails
AsdetailedbelowinSec.A.1,arecurrentautoregressiveHiddenMarkovModel(rARHMM)generalizesastandardHidden
MarkovModelbyaddingautoregressiveandrecurrentedgestotheprobabilisticgraphicalmodel.AlthoughrARHMMmodels
havebeenpreviouslyproposedintheliterature[Lindermanetal.,2017],wedonotknowofanyexplicitproposition(or
justification)describinghowtoperformposteriorstateinferenceforthesemodels. Theliteratureprovidessuchaproposition
for(non-recurrent)autoregressiveHiddenMarkovModel(ARHMMs;e.g.,see[Hamilton,2020]),butnotforrARHMMs.
Hence,weprovidethemissingpropositionswithproofshere;seeProps.A.2.1andA.2.2. Webelievethattheseexplicit
propositionscanbeusefulwhencomposingrecurrenceintomorecomplicatedconstructions.Indeed,weusethemthroughout
thesupplementinordertoderiveinferenceforourHSRDMs;forexample,seetheVESstepinSec.B.2ortheVEZstep
in Sec. B.3. In fact, we also utilize the proofs of these propositions when describing how to perform inference with
HSRDMswhenthedatasetispartitionedintomultipleexamples;seeSec.B.7.
A.1 Model
Thecompletedatalikelihoodfora(K,m,n)-orderrecurrentAR-HMM(rARHMM)isgivenbyRadon-Nikody`mdensity
T
(cid:89)
p(x ,z |θ)=p(z |θ)p(x |z ,θ) p(z |z ,x ,θ)p(x |z ,x ,θ) (A.1)
1:T 1:T 1 1 1 t t−1 (t−m):(t−1) t t (t−n):(t−1)
(cid:124) (cid:123)(cid:122) (cid:125) t=2(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
initialization transitions emissions
wherex aretheobservations,z ∈{1,...,K}arethediscretelatentstates,andθaretheparameters.TherARHMMgen-
1:T 1:T
eralizesthestandardHMM[Rabiner,1989],whichcontainsneitherautoregressiveemissions(blue)norrecurrentfeedback
(red)fromemissionstostates. The(K,m,n)-orderrARHMMgivesa(K,n)-orderautoregressiveHMM(ARHMM)inthe
specialcasewhere
p(z
t
|z t−1,(cid:40)x (t(cid:40) −m(cid:40) ):(cid:40) (t−(cid:40) 1),θ)=p(z
t
|z t−1,θ) (A.2)
SeeFig.A.1foraprobabilisticgraphicalmodelrepresentationinthespecialcaseoffirst-orderrecurrence(m = 1)and
autoregression(n=1).
Discretestates z 0 z 1 z 2 z 3 z 0 z 1 z 2 z 3
Observations x 0 x 1 x 2 x 3 x 0 x 1 x 2 x 3
(a) Recurrent Autoregressive Hidden Markov Model (b)AutoregressiveHiddenMarkovModel(ARHMM).
(rARHMM).
FigureA.1: ProbabilisticgraphicalmodelrepresentationofaRecurrentAutoregressiveHMM(rARHMM),anditsspecial
case,anAutoregressiveHMM(ARHMM).Forsimplicitytheillustrationassumesfirst-orderautoregressionandrecurrence,
buthigher-orderdependenciescanalsobeaccomodated(seeEq.(A.1)andProps.A.2.1and A.2.2). Autoregressiveedges
areshowninblueandrecurrentedgesareshowninred.
RemarkA.1.1. (OngeneralizingaHMMwithautoregressiveemissionsandrecurrentstatetransitions.) Letushighlight
howarARHMMmodelgeneralizesaconventionalHMM:
• Recurrence: Alookbackwindowofnpreviousobservationsx caninfluencethetransitionsstructureforthe
t−m:t−1
currentstatez .
t
pa(z )={z }∪{x }
t t−1 (t−m):(t−1)
(cid:124) (cid:123)(cid:122) (cid:125)
ifrecurrent
• Autoregression: Alookbackwindowofmpreviousobservationsx caninfluencetheemissionsstructureforthe
t−n:t−1
currentobservationx .
t
pa(x )={z }∪{x }
t t (t−n):(t−1)
(cid:124) (cid:123)(cid:122) (cid:125)
ifautoregressive
Noteinparticularthateachnode(observationx orstatez )canhavemanyparentsamongpreviousobservationvariables
t t
x ,butonlyoneparentamongstatevariablesz (namely,theclosestintimefromthepresentorpast).1 Thisassumption
1:t 1:t
willbeimportantwhenderivingthesmootherinSec.A.2. △
1What if we wanted to relax the specification so that the emissions could depend on a finite number M of previous statesDiscoveringgroupdynamicsinsynchronoustimeseries
A.2 StateEstimation
HerewediscussstateestimationfortherARHMM.Webeginwithsomenotation.
NotationA.2.1. Givenasequenceofobservationsuptosometimet,wecandefinetheconditionalprobabilityofthestate
z atatargettimes∈{1,2,...T}viatheprobabilityvectorξ ∈∆ ⊂RK. Thek-thelementofthisvectorisgiven
s s|t K−1
byp (z =k |x ). Thatis,
θ s 1:t
(cid:20) (cid:21)T
ξ ≜p (z |x ) = p (z =1|x ), ..., p (z =K |x )
s|t θ s 1:t θ s 1:t θ s 1:t
Usingthisnotation,wecandefinethreecommoninferentialtasks:
1. Filtering. Inferthecurrentstategivenobservationsξ =p (z |x ).
t|t θ t 1:t
2. Smoothing. Inferapaststategivenobservationsξ =p (z |x ),wheres<t.
s|t θ s 1:t
3. Prediction. Predictafuturestategivenobservations,ξ =p (z |x ),whereu>t.
u|t θ u 1:t
△
NowwecangiveProps.A.2.1and A.2.2,whichparallelthepresentationoftheKalmanfilterandsmootherinthecontextof
statespacemodels[Hamilton,2020,Shumwayetal.,2000]. Inparticular,wewillpresenttheforwardalgorithmintermsof
ameasurementupdate(whichusestheobservationx totransformξ intoξ )andatimeupdate(whichtransforms
t t|t−1 t|t
ξ intoξ ,withoutrequiringanobservation). Thesepropositionsshowthatfilteringandsmoothingcanbedone
t|t t+1|t
usingthesamerecursionsasusedinaclassicalHMM[Hamilton,1994],exceptthatthevariableinterpretationsdifferfor
boththeemissionsstepandtransitionstep. Inthestatementsandproofsbelow,wecontinuetousethesamecolorschemeas
wasusedinEq.(A.1)andFig.A.1,wherebybluedesignatesautoregressiveedgesandreddesignatesrecurrenceedgesinthe
graphicalmodel. ThesecolorshighlightdifferencesfromclassicHMMs,whichlackbothtypesofedges.
Proposition A.2.1. (Filtering a Recurrent Autoregressive HMM.) Filtered probabilities ξ ≜ p (z | x ) for a
t|t θ t 1:t
RecurrentAutoregressiveHiddenMarkovModelcanbeobtainedbyrecursivelyupdatingsomeinitializationξ by
1|0
• Measurementupdate.
(ξ ⊙ϵ )
t|t−1 t
ξ =
t|t 1T(ξ ⊙ϵ )
t|t−1 t
• Timeupdate.
ξ =A ξ
t+1|t t t|t
wherehereϵ =(ϵ ,...ϵ )=(p (x |z =k,x ))K isthe(K×1)vectorwhosek-thelementistheemissions
t t1 tk θ t t 1:t−1 k=1
density,A representsthe(K×K)transitionmatrixwhose(k,k′)-thelementisp (z =k′ |z =k,x ),1represents
t θ t+1 t 1:t
a(K×1)vectorof1s,andthesymbol⊙denoteselement-by-elementmultiplication.
Proof.
• Measurementupdate.
ξ =p (z |x ) Notation
t|t θ t 1:t
∝p (z ,x |x ) Conditionaldensity
θ t t 1:t−1
=p (z |x ) ⊙ p (x |z ,x ) Chainrule
θ t 1:t−1 θ t t 1:t−1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
≜ξt|t−1 ≜ϵt
(ξ ⊙ ϵ )
=⇒ ξ = t|t−1 t Normalize
t|t 1T(ξ ⊙ ϵ )
t|t−1 t
p(x | z ,z ,...,z ,x ,ϕ)? This situation can be handled by simply redefining the states in terms of tuples z∗ =
t t t−1 t−M 1:t−1 t
(z ,z ,...z ), such that z∗ takes on KM possible values, one for each sequence in the look-back window [Hamilton, 2010,
t t−1 t−M t
pp.8].Wojnowicz,Rath,etal.
• Timeupdate.
ξ =p (z |x ) Def.
t+1|t θ t+1 1:t
K
(cid:88)
= p (z ,z =k|x ) LawofTotalProb.
θ t+1 t 1:t
k=1
K
(cid:88)
= p (z |z =k,x )p (z =k|x ) Chainrule
θ t+1 t 1:t θ t 1:t
k=1
K (cid:20) (cid:21)
(cid:88)
= A [ξ ] Notation
t t|t k
k=1 k,: (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) kthelementofξt|t
kthrowofAt
=A ξ Def.matrixmultiplication
t t|t
RemarkA.2.1. (InitializingthefilteringalgorithminProp.A.2.1.) InspiredbyHamilton[1994,pp.693],weprovidesome
suggestionsforinitializingthefilteringalgorithmofPropA.2.1. Inparticular,wecansetξ to
1|0
• Anyreasonableprobabilityvector,suchastheuniformdistributionK−11.
• Themaximumlikelihoodestimate.
• Thesteadystatetransitionprobabilities,iftheyexist.
△
PropositionA.2.2. (SmoothingaRecurrentAutoregressiveHiddenMarkovModel.) Smoothedprobabilitiesξ ≜
t|T
p (z |x )foraHiddenMarkovModelcanbeobtainedbytherecursion
θ t 1:T
(cid:26) (cid:20) (cid:21)(cid:27)
ξ =ξ ⊙ AT · ξ (÷)ξ
t|T t|t t t+1|T t+1|t
where the formula is initialized by ξ (obtained from the filtering algorithm of Prop. A.2.1) and is then iterated
T |T
backwards for t = T −1,T −2,... ,1, in a step analogous to the backward pass of the classic forward-backward
recursionsforplainHMMs[Rabiner,1989]. Here,A representsthe(K×K)transitionmatrixwhose(k,k′)-thelementis
t
p (z =k′ |z =k,x ),thesymbol⊙denoteselement-wisemultiplication,andthesymbol(÷)denoteselement-wise
θ t+1 t 1:t
division.
Proof. 2
Weproceedinsteps:
• Step1 Weshow p (z |z ,x )=p (z |z ,x ) . Thatis,thecurrentstatez dependsonfutureobserva-
θ t t+1 1:T θ t t+1 1:t t
tionsx onlythroughthenextstatez .
t+1:T t+1
– Step1a Weshow p (z |z ,x )=p (z |z ,x ) .
θ t t+1 1:t+1 θ t t+1 1:t
p (z |z ,x )=p (z |z ,x ,x ) splitofftermfromsequence
θ t t+1 1:t+1 θ t t+1 t+1 1:t
p (z ,x |z ,x )
= θ t t+1 t+1 1:t conditionaldensity
p (x |z ,x )
=
(cid:40)p
θ((cid:40)θ
x t(cid:40)
+t+ 1(cid:40)1
|
(cid:40)pz(cid:40)
θt
(,
(cid:40)t
(cid:40)
xz+
t
t1
(cid:40)+(cid:40)
+1
1(cid:40),(cid:40)1 |x: z(cid:40)t 1(cid:40) t:(cid:40)
t
+(cid:40)) 1p ,(
(cid:40)
xz
1t(cid:40) :(cid:40)
t| )z t+1,x 1:t)
chainrule
=p(z |z ,x ) FPOBN
t t+1 1:t
Inthelastline,thetwocanceledtermsareequalbyFPOBN(theFundamentalPropertyofBayesNetworks).3
2OurproofisinspiredbytheproofgivenbyHamilton[1994,pp.700-702]fortheARHMM(i.e,thespecialcaseofrARHMMinwhich
therearenorecurrentedges).
3TheFundamentalPropertyofBayesNetworksis:Anodeisindependentofitsnon-descendantsgivenitsparents.Inparticular,since
z isanon-descendentofx ,itisindependentofx givenitsparentsz andx .
t t+1 t+1 t+1 1:tDiscoveringgroupdynamicsinsynchronoustimeseries
– Step1b Weshow p (z |z ,x )=p (z |z ,x ) . Bythesameargumentasinstep1a(splitting
θ t t+1 1:t+2 θ t t+1 1:t+1
upthesequence,conditionaldensity,chainrule),butreplacing
x ←x , x ←x
1:t+1 1:t+2 t+1 t+2
thepropositionholdsif
p(x |z ,z ,x )=p(x |z ,x )
t+2 t t+1 1:t+1 t+2 t+1 1:t+1
thatisifwegetthesamecancelation. Andwesee
K
(cid:88)
p(x |z ,z ,x )= p(x ,z =k|z ,z ,x ) LTP
t+2 t t+1 1:t+1 t+2 t+2 t t+1 1:t+1
k=1
K
(cid:88)
= p(x |z =k,(cid:26)z ,z ,x )p(z =k|(cid:26)z ,z ,x ) chainrule,FPOBN
t+2 t+2 t t+1 1:t+1 t+2 t t+1 1:t+1
k=1
K
(cid:88)
= p(x ,z =k|z ,x ) undochainrule
t+2 t+2 t+1 1:t+1
k=1
=p(x |z ,x ) undoLTP
t+2 t+1 1:t+1
– Conclusion TheclaimfollowsfromSteps1aand1bbyaninductionargument.
p(z |x )
t 1:t
(cid:124) (cid:123)(cid:122) (cid:125)
• Step2. Weshowthat p(z ,z |x )=p(z |x ) p(z |z ,x ) filtered . Wehave
t t+1 1:T t+1 1:T t+1 t 1:t p(z |x )
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) t+1 1:t
smoothedpairwise smoothed transition (cid:124) (cid:123)(cid:122) (cid:125)
predicted
p(z ,z |x )=p(z |x )p(z |z ,x ) chainrule
t t+1 1:T t+1 1:T t t+1 1:T
=p(z |x )p(z |z ,x ) Step1
t+1 1:T t t+1 1:t
p(z |x )p(z |z ,x )
=p(z |x ) t 1:t t+1 t 1:t Bayesrule(on2ndterm)4
t+1 1:T p(z |x )
t+1 1:t
• Step3. Weprovetheproposition.
K
(cid:88)
p(z |x )= p(z ,z =k|x ) LawofTotalProb.
t 1:T t t+1 1:T
k=1
ξ t|T
=(cid:88)K (cid:20)
ξ
t+1|T(cid:21) (cid:20)
A
t(cid:21)
(cid:20)
ξ
t|t (cid:21) Step2,Notation
k=1 k :,zt+1=k ξ t+1|t
k
(cid:20) (cid:21)
ξ
K (cid:20) (cid:21) t+1|T
(cid:88)
=ξ t|t A t (cid:20) (cid:21)k Pulloutconstant
k=1 :,zt+1=k ξ t+1|t
k
(cid:26) (cid:20) (cid:21)(cid:27)
=ξ ⊙ AT · ξ (÷)ξ Def.matrixmultiplication
t|t t t+1|T t+1|t
RemarkA.2.2. AswesawinStep1,thederivationofthesmootherinProp.A.2.2reliesonthefactthatwhileeachnode
(observationorstate)canhavemanyobservationparents,itcanhaveonlyonestateparent(namely,theclosestintimefrom
thepresentorpast). △
4TojustifytheapplicationofBayesrule,imaginethatz playstheroleoftheparameterandz playstheroleoftheobserveddata.
t t+1
Thetermx isjustaconditioningsetthroughout.
1:tWojnowicz,Rath,etal.
RemarkA.2.3. Thefiltering(PropA.2.1)andsmoothing(PropA.2.2)formulaerevealthatstateestimationforrARHMMcan
behandledfor:
• anyorderofrecurrenceand/orautoregression5
• anyfunctionalformofemissionsandtransitions
Furthermore,althoughitwasnotexplicitlyrepresentedhere,thesameformulaeholdwhenthereare
• Modulationoftransitionsandemissionsbyexogenouscovariates.6
△
A.3 Entropy
Here,weprovidetheentropyofanrARHMMposterior. WewritethecompletedatalikelihoodofEq.(A.1)asp (x ,z ).
θ 1:T 1:T
Thentheposteriordistribution(ofthestatesz giventheobservationsx )is
1:T 1:T
p (x ,z )
p (z |x )= θ 1:T 1:T (A.3)
θ 1:T 1:T p (x )
θ 1:T
(cid:124) (cid:123)(cid:122) (cid:125)
≜Zθ
whereZ ≜ p (x )isthenormalizingconstant(asitisconstantinthestatesz ). Ourinterestisincomputingthe
θ θ 1:T 1:T
entropyoftheposterior,H[p (z |x )]. Wehave
θ 1:T 1:T
(cid:88)
H[p (z |x )]=− p (z =k |x ) logp (z =k |x )
θ 1:T 1:T θ 1:T 1:T 1:T θ 1:T 1:T 1:T def.entropy
k1:T∈{1,...,K}T
(cid:88)
=− p (z =k |x ) logp (z =k ,x )−logZ
θ 1:T 1:T 1:T θ 1:T 1:T 1:T θ byEq.(A.3)
k1:T∈{1,...,K}T
(cid:20) K
∗ (cid:88)
= − p (z =k |x ) logp (z =k)
θ 1 1:T θ 1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
k=1
posteriorinit modelinit
T K K
(cid:88)(cid:88) (cid:88)
− p (z =k′,z =k |x )log p (z =k′ |z =k, x )
θ t t−1 1:T θ t t−1 (t−m):(t−1)
t=2k=1k′=1(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
posteriorpairwisemarginals modeltransitions
T K (cid:21)
(cid:88)(cid:88)
− p (z =k |x )log p (x |z =k, x )
θ t 1:T θ t t (t−n):(t−1) byEq.(A.1)
t=1k=1(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
posteriorunarymarginals modelemissions
−log p (x )
θ 1:T defZθ.
(cid:124) (cid:123)(cid:122) (cid:125)
normalizingconstant
Sotheentropyisgivenbyacombinationofmodelfactorsandposteriorfactors(recoveredbysmoothing). Theformula
inthefirstlineoftheequationaboveisdifficulttocompute,becausesmoothingdoesnotdirectlyproduceaprobability
distributionoversequences-onlyoverunaryandpairwisemarginals. However,intheEqualitymarked(*),weutilizethe
graphicalstructureofthemodeltoobtainatractableexpression.
B ProposedHSRDM:ModelandInferenceDetails
WenowreviewmodelingandinferencedetailsforourproposedHSRDM,inthefollowingsections
• Sec.B.1coverstheprioronmodelparametersθ
5Infact,theproofrevealsthattheordercanincreasewithtimestept,openingthedoortoconstructionsinvolvingexponentialweighted
movingaverages.
6Asequenceofvectors{u }isconsideredtobeasequenceofexogenouscovariatesifeachu containsnoinformationaboutz that
t t t
isnotcontainedinx [Hamilton,1994,pp.692].
1:t−1Discoveringgroupdynamicsinsynchronoustimeseries
• Sec.B.2coverstheVESupdateforsystem-levelstateposteriors
• Sec.B.3coverstheVEZupdateforentity-levelstateposteriors
• Sec.B.4coverstheMstepfortransitionandemissionparameters
• ELBOcomputationandinitializationarecoveredinsubsequentsections
B.1 Priorsonmodelparameters
ThesymbolθdenotesallmodelparametersforourHSRDM.UsingthestructureofourmodelinEq.(2.1),wecanexpandθ
intoconstituentcomponents: θ =(θ ,θ ,θ ,θ ),whereθ aretheparametersthatgovernthesystem-leveldiscrete
ss es ee init ss
statetransitions,θ governtheentity-leveldiscretestatetransitions,θ governtheentity-levelemissions,andθ govern
es ee init
theinitialdistributionforstatesandregimes.
Wedefineaprioroverθwhosefactorizationstructurereflectsthisdecomposition:
p(θ)=p(θ ss)p(θ es)p(θ ee)p(θ init) (B.1)
AsweseeinSec.B.4,thischoiceofpriorsimplifiestheM-step.
Prioronsystem-levelstatetransitionparametersθ . Forthesystem-leveltransitionprobabilitymatrixΠ,aL×L
ss
matrixwhoseentriesareallnon-negativeandrowssumtoone,weassumeastickyDirichletprior[Foxetal.,2011]to
encourageself-transitionssothatintypicalsamples,onesystemstatewouldpersistforlongsegments. Concretely,foreach
rowweset
Π ,...Π ∼Dir(α,...α,α+κ,α,...α) (B.2)
j1 jL
whereallLentrieshaveasymmetricbasevalueα=1.0,andtheaddedvalueκthatimpactstheself-transitionentry(the
(j,j)-thentryofthematrix)issetto10.0. WethensetthelogtransitionprobabilityΠ(cid:101) totheelement-wiselogofΠ.
Prioronentity-levelstatetransitionparametersθ . Inourexperiments,weusedanon-informativeprior,p(θ )∝1.
es es
TheuseofastickyDirichletprior,aswasusedwiththesystem-leveltransitionparameters,couldbeexpectedtoproduce
smootherentity-levelstatesegmentations. Currently,theentity-levelsegmentationsarechoppierthanthoseatthesystem-
level(e.g.,comparethebottom-leftandtop-rightsubplotsofFig.4).
Prioronemissionsθ . Inourexperiments,weusedanon-informativeprior,p(θ )∝1.
ee ee
Prior on initial states and observations θ . For initial states at both system and entity level, we use a symmetric
init
Dirichletwithlargeconcentrationsothatallstateshavereasonableprobabilitya-priori. ThisavoidsthepathologyofML
estimationthatlocksintoonlyonestateasapossibleinitialstateearlyininferenceduetopoorinitialization.
B.2 Updatingtheposterioroversystem-levelstates
Inthissection,wediscusstheupdatetotheposterioroversystem-levelstates;thatis,thevariationalE-SstepofEq.(3.2).
Wefind
(cid:26) T J T (cid:27)
q(s )∝exp logπ (s )+(cid:88) logp(s |s ,x(1:J),θ)+(cid:88)(cid:88) E logp(z(j) |z(j) ,x(j) ,s ,θ)
0:T (cid:101) s 0 t t−1 t−1 q(z(1:J)) t t−1 t−1 t
(cid:124) (cid:123)(cid:122) (cid:125) 0:T
t=1 j=1t=1
initdist (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
transitions emissions
T J (cid:26) K (cid:27)
=π (s ) (cid:89) p(s |s ,x(1:J),θ) (cid:89) exp (cid:88) logπ (z(j) =k)q(z(j) =k)
s 0 t t−1 t−1 zj 0 0
(cid:124) (cid:123)(cid:122) (cid:125)
t=1 j=1 k=1
initstate (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
transitions initialemissions
J T (cid:26) K (cid:27)
(cid:89)(cid:89) exp (cid:88) logp(z(j) =k′ |z(j) =k,x(j) ,s ,θ)q(z(j) =k′,z(j) =k) (B.3)
t t−1 t−1 t t t−1
j=1t=1 k,k′=1
(cid:124) (cid:123)(cid:122) (cid:125)
remainingemissions
This can be considered as the posterior of an input-output Hidden Markov Model with J independent autoregressive
categoricalemissions. TheevaluationofthetransitionfunctionisO(cid:0) T(L2+LDJM )(cid:1) ,whereM isthedimensionofthe
s s
system-levelcovariates,andwherewehaveassumedthattheevaluationofthesystem-levelrecurrencefunctiong takes
DJM operations,asitwouldifg (cid:0) x(1:J),υ (cid:1) =(x(1) ,...,x(J),υ )T. Theevaluationoftheemissionsfunction
s ψ t−1 t−1 t−1 t−1 t−1Wojnowicz,Rath,etal.
isO(cid:0) TJL(K2+KDM )(cid:1) ,whereM isthedimensionoftheentity-levelcovariates,andwherewehaveassumedthatthe
e e
evaluationoftheentity-levelrecurrencefunctionf takesDM operations,asitwouldiff (cid:0) x(j) ,u(j) (cid:1) =(x(j) ,u(j) )T.
e ϕ t−1 t−1 t−1 t−1
Thus,byProps.A.2.1andA.2.2,filteringandsmoothingcanbecomputedwithO(cid:0) TJ(cid:2) L2+L(K2+DM +KDM )(cid:3)(cid:1)
s e
runtimecomplexity,undermildassumptionsontherecurrencefunctions. Asaresult,socanthecomputationoftheunary
andadjacentpairwisemarginalsnecessaryfortheVESandMsteps.
B.3 Updatingtheposterioroverentity-levelstates
Inthissection,wediscusstheupdatetotheposterioroverentity-levelstates;thatis,thevariationalE-ZstepofEq.(3.2).
Weobtain
J (cid:20) T (cid:26) L (cid:27)
q(z(1:J))∝(cid:89) π (z(j)) (cid:89) exp (cid:88) logp(z(j) |z(j) ,x(j) ,s =ℓ,θ)q(s =ℓ)
0:T (cid:101) zj 0 t t−1 t−1 t t
j=1 (cid:124) (cid:123)(cid:122) (cid:125) t=1 ℓ=1
initialstate(∈RK) (cid:124) (cid:123)(cid:122) (cid:125)
transitions(∈R(T(cid:101)−1)×K×K)
T (cid:21)
p(x(j) |z(j),θ) (cid:89) p(x(j) |x(j) ,z(j),θ) (B.4)
0 0 t t−1 t
(cid:124) (cid:123)(cid:122) (cid:125)
t=1
initialemission (∈RK) (cid:124) (cid:123)(cid:122) (cid:125)
remainingemissions(∈R(T(cid:101)−1)×K)
wherewehavedefinedT(cid:101)≜T +1todenotealltimestepsafteraccountingforthezero-indexing. Thisvariationalfactor
can be considered as posterior of J conditionally independent Hidden Markov Models with autoregressive categorical
emissions which recurrently feedback into the transitions. As per Sec. B.2, the transition function can be evaluated
withO(cid:0) TJL(K2+KDM )(cid:1) runtimecomplexity,whereM isthedimensionoftheentity-levelcovariates,andwhere
e e
we have assumed that the evaluation of the entity-level recurrence function f takes DM operations, as it would if
e
f (cid:0) x(j) ,u(j) (cid:1) =(x(j) ,u(j) )T. TheevaluationoftheemissionsfunctionisO(cid:0) TJKD2(cid:1) ,assumingthattheemissions
ϕ t−1 t−1 t−1 t−1
distributionhasadensitythatcanbeevaluatedwithO(cid:0) D2(cid:1)
operationsateachtimestep. Thus,byProps.A.2.1andA.2.2,
filteringandsmoothingcanbecomputedwithO(cid:0) TJ(cid:2) K2+KD2+KDM (cid:3)(cid:1) runtimecomplexity,undermildassumptions
e
ontheentity-levelrecurrencefunctionandtheemissionsdistribution. Asaresult,socanthecomputationoftheunaryand
adjacentpairwisemarginalsnecessaryfortheVEZandMsteps.
B.4 Updatingtheparameters
TheM-stepupdatesthetransitionparametersandemissionparametersθofourHSRDMgivenrecentestimatesofstate-level
posteriorq(s )andentity-levelposteriorsq(z(1:J)).
0:T 0:T
Thisupdaterequiressolvingthefollowingoptimizationproblem
θ=argmaxL(θ)
θ
(cid:20) (cid:21)
whereL(θ)≜E logp(x(1:J),s(1:J),z(1:J) |θ) +logp(θ) (B.5)
q(s(1:J))q(z(1:J)) 0:T 0:T 0:T
0:T 0:T (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) logprior
expectedlogcompletedatalikelihood
Based on the structure of the model in Eq. (2.1), we can decompose this into separate optimization problems over the
differentmodelpiecesθ =(θ ,θ ,θ ,θ )byassuminganappropriatelyfactorizedprior,aswasdoneinEq.(B.1).
ss es ee init
Tobeconcrete,foraHSRDMwithtransitionsgivenbyEq.(2)andGaussianvectorautoregressive(GaussianVAR)emissions
(cid:18) (cid:19)
x(j) |x(j) ,z(j) ∼N
A(zt(j))x(j) +b(zt(j)), Q(zt(j))
, (B.6)
t t−1 t j t−1 j j
asusedinSecs.5.1and5.2wehave
θ
ss
=(Λ,Π(cid:101)), θ
es
={Ψ j,P(cid:101)j}J j=1, θ
ee
={{A jk,b jk,Q jk}K k=1}J
j=1
Using this grouping of the parameters along with the complete data likelihood specification of Eq. (2.1) and the prior
assumptioninEq.(B.1),wecandecomposetheobjectiveas
J
(cid:88)
L(θ)=L (θ )+L (θ )+ L(j)(θ(j))++L(j)(θ(j))
init init ss ss es es ee ee
j=1Discoveringgroupdynamicsinsynchronoustimeseries
WecanthencompletetheoptimizationbyseparatelyperformingM-stepsforeachofthesubcomponentsofθ. Forexample,
tooptimizetheparametersgoverningtheentity-leveldiscretestatetransitionsθ(j) foreachentityj =1,...,J,weonly
es
needtooptimize
T (cid:20) (cid:21)
L(j)(θ(j))≜(cid:88) E logp(z(j) |z(j) ,x(j) ,s ,θ ) +logp(θ )
es es q(z 0(1 :T:J))q(s0:T) t t−1 t−1 t es
(cid:124) (cid:123)(cid:122)
es
(cid:125)
t=1
(cid:124) (cid:123)(cid:122) (cid:125) logprior
expectedlogentitydiscretestatetransitions
T K L
=(cid:88) (cid:88) (cid:88) logp(z(j) =k′ |z(j) =k,x(j) ,s =ℓ,θ )q(z(j) =k′,z(j) =k)q(s =ℓ)
t t−1 t−1 t es t t−1 t
t=1k,k′=1ℓ=1
+logp(θ es) (B.7)
Inparticular,wedonotrequirethevariationalposterioroverthefullentity-leveldiscretestatesequenceq(z(1:J)),butmerely
0:T
thepairwisemarginalsq(z(j) =k′,z(j) =k),obtainablefromtheVEZstepinEq.(3.2). Similarly,wedonotrequirethe
t t−1
variationalposterioroverthefullsystem-leveldiscretestatesequenceq(s ),butmerelytheunarymarginalsq(s =ℓ),
0:T t
obtainablefromtheVESstepinEq.(3.2).
Theothercomponentsofθareoptimizedsimilarly. Ingeneral,theoptimizationcanbeperformedbygradientdescent(e.g.
usingJAXforautomaticdifferentiation[Bradburyetal.,2018]),althoughitcanbeusefultobringinclosed-formsolutions
fortheMsubstepsincertainspecialcases. Forinstance,whenGaussianVARemissionsareusedasinEq.(B.6),theentity
emissionparametersθ ={{A ,b ,Q }K }J canbeestimatedwithclosed-formupdatesusingthesampleweights
ee jk jk jk k=1 j=1
q(z(j) =k)availablefromtheVEZ-step.
t
B.5 Variationallowerbound
Alowerboundonthemarginalloglikelihood VLBO ≤logp(x(1:J)),isgivenby
0:T
VLBO[θ,q]=E (cid:2) logp(x(1:J),z(1:J),s ,θ)(cid:3) +H(cid:2) q(z(1:J),s )(cid:3) (B.8)
q 0:T 0:T 0:T 0:T 0:T
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
energy entropy
TheenergytermE (cid:2) logp(x(1:J),z(1:J),s ,θ)(cid:3) isidenticaltotheobjectivefunctionfortheM-stepgiveninEq.(B.5).
q 0:T 0:T 0:T
BasedonthestructureofthemodelassumedinEq.(2.1),theenergytermdecomposesintoseparatepiecesforinitialization,
systemtransitions,entitytransitions,andemissions. Forexample,seeEq.(B.7)forthepiecerelevanttoentitytransitions.
NowweconsidercomputationoftheentropyH(cid:2) q(z(1:J),s )(cid:3) inEq.(B.8). Sincethevariationalfactorsq(s )and
0:T 0:T 0:T
{q(z(j))}J givenrespectivelybytheVESstepinSec.B.2andtheVEZstepinSec.B.3bothhavetheformofrARHMMs,
0:T j=1
wecancomputetheentropyH(cid:2) q(z(1:J),s )(cid:3) =(cid:80)J H(cid:2) q(z(j))(cid:3) +H(cid:2) q(s )(cid:3) viatheentropyforrARHMMsthatwas
0:T 0:T j=1 0:T 0:T
providedinSec.A.3.
B.6 Smartinitialization
Wecanconstructa“smart"(ordata-informed)initializationofaHSRDMviathefollowingtwo-stageprocedure:
1. WefitJ bottom-levelrARHMMs, oneforeachoftheJ entities. Inparticular, theemissionsforeachbottom-level
rARHMMaretheemissionsofthefullHSRDMgiveninEq.(2.4), andthetransitionsaretheentity-leveltransitions
giveninEq.(2.3).
2. Wefitonetop-levelARHMM.Here,theJ emissionsaretheentity-leveltransitionsgiveninEq.(2.3). Thetransitionsare
thesystem-leveltransitionsgiveninEq.(2.2). Theobservationsaretakentobethemost-likelyentity-levelstatesas
inferredbythebottom-levelrARHMMs.
Belowwegivedetailsontheseinitializations. Inparticular,boththebottom-levelandtop-levelmodelsthemselvesneed
initializations. Weusethetermpre-initializationtorefertotheinitializationsofthosemodels.
B.6.1 Initializationofbottom-levelrARHMMs
HerewefitJ bottom-levelrARHMMs,oneforeachoftheJ entities,independently. Inparticular,theemissionsforeach
bottom-level rARHMM are the emissions of the full HSRDM given in Eq. (2.4), and the transitions are the entity-level
transitionsgiveninEq.(2.3).Wojnowicz,Rath,etal.
Pre-initialization. TheJ bottom-levelrARHMMsthemselvesneedgood(data-informed)initializations. Asanexample,
wedescribethepre-initializationprocedureintheparticularcaseofGaussianVARemissions,asgiveninEq.(B.6). In
particular,wefocusonastrategyforpre-initializingtheseemissionparameters{A(k),b(k),Q(k)} ,sincethehigher-level
j j j j,k
parametersinthemodelcanbelearnedviathetwo-stageinitializationprocedure.
Inparticular,foreachj =1,...,J,
a) Weassigntheobservationsx(j) tooneofK statesbyapplyingtheK-meansalgorithmtoeithertheobservations
1:T
themselvesortotheirvelocities(discretederivatives)x(j) −x(j) ,dependinguponuserspecification. Weusethe
2:T 1:T−1
formerchoiceintheFigureEightdata,andthelatterchoiceforbasketballdata.
b) WetheninitializetheparametersbyrunningseparatevectorautoregressionswithineachoftheK clusters. Inparticular,
foreachstatek =1,...,K,
a) Wefindstate-specificobservationmatrixA(k)andbiasesb(k)byapplyinga(multi-outcome)linearregressionto
j j
predictx(j)fromthex(j) wheneverx(j)belongstothek-thcluster.
t t−1 t
b) Weestimatetheregime-specificcovariancematricesQ(k)fromtheresidualsoftheabovevectorautoregresssion.
j
Weinitializetheentity-leveltransitionparameters{Ψ j,P(cid:101)j}J
j=1
torepresentastickytransitionprobabilitymatrix. This
impliesthatweinitializeΨ =0forallj.
j
Expectation-Maximization. Afterpre-initialization,weestimatetheJ independentrARHMMsbyusingtheexpectation
maximizationalgorithm. Posteriorstateinference(i.e. theE-step)forthisprocedureisjustifiedinSec.A.Notethatthe
posteriorstateinferenceforthesebottom-levelrARHMMscanbeobtainedbyreusingtheVEZstepofEq.(B.4)bysetting
thenumberofsystemstatestoL=1.
B.6.2 Initializationoftop-levelARHMM
Herewefitatop-levelARHMM.Inparticular,theemissionsfortheARHMMaretheentity-leveltransitionsoftheHSRDMgiven
inEq.(2.3),andthetransitionsoftheARHMMarethesystem-leveltransitionsgiveninEq.(2.2). Wecanperformposterior
stateinferenceforthetop-levelARHMMbyreusingtheVESstepofEq.(B.3)withinputsbeingtheposteriorstatebeliefson
z(1:J)fromthebottom-levelrARHMMs.
0:T
B.7 MultipleExamples
Insomedatasets,wemayobservethesameJ entitiesoverseveraldistinctintervalsofsynchronousinteraction. Wecall
eachseparateintervalofcontiguousinteractionan“example”. Forexample,therawbasketballdatasetfromSec.5.2is
organizedasacollectionofseparateplays,whereeachplayisoneseparateexample. Betweentheendofoneplayandthe
beginningofthenext,theplayersmighthavechangedpositionsentirely,perhapsevenhavinggonetothelockerroomand
backforhalftime.
LetE bethenumberofexamples. Eachexample,indexedbye∈{1,2,...E},startsatsomereferencetimeτ andhas
e
T totaltimesteps,coveringthetimesequencet∈{τ ,τ +1,...,τ +T }. We’llmodeleachper-exampleobservation
e e e e e
sequencex(1:J) asaniidobservationfromourHSRDMmodel.
τe:τe+Te
Toefficientlyrepresentsuchdata,wecanstacktheobservedsequencesforeachexampleontopofoneanother. Thisyields
atotalobservationsequencex(1:J)thatcoversalltimestepsacrossallexamples,definingT =T +T +...+T . This
0:T 1 2 E
representationdoesn’twasteanystorageonunobservedintervalsbetweenexamples,easilyaccommodatesexamplesof
arbitrarilydifferentlengths,andintegrateswellwithmodernvectorizedarraylibrariesinourPythonimplementation. As
beforeinthesingleexamplecase,ourcomputationalrepresentationofx(1:J)isasa3-darraywithdimensionality(T,J,D).
0:T
Forproperlyhandlingthiscompactrepresentation,bookkeepingisneededtotrackwhereoneexamplesequenceendsand
anotherbegins. Wethustracktheendingindicesofeachexampleinthisstackedrepresentation: E ={t ,t ,...,t ,t },
0 1 E−1 E
where−1=t <t <t <...<t <t =T,andwheret =τ +T isthelastvalidtimestepobservedinthee-th
0 1 2 E−1 E e e e
examplefore=1,...,E.
By inspecting the inference updates gives above, including the filtering and smoothing updates for rARHMM (see
Props.A.2.1andA.2.2),wefindthatwecanhandlethissituationasfollows:Discoveringgroupdynamicsinsynchronoustimeseries
• E-steps(VEZorVES):Wheneverwegettoacross-exampleboundary,wereplacetheusualtransitionfunctionwithan
initialstatedistribution. Moreconcretely,thetransitionfunctionfortheVESstepinEq.(B.3)ismodifiedsothatany
timesteptthatrepresentsthestartofanewexamplesequence(thatis,satisfiest−1 ∈ E)isreplacedwithπ ,and
s
thetransitionfunctionfortheVEZstepinEq.(B.4)atsuchtimestepsisreplacedwithπ . Similarly,theemissions
zj
functionsatsuchtimestepsarereplacedwiththeinitialemissions. Thismaneuvercanbejustifiedbynotingthatfor
anytimesteptdesignatingtheonsetofanewexample,theinitialstatedistributionsplaytheroleofA andtheinitial
t
emissionsplaytheroleofϵ inProps.A.2.1andA.2.2.
t
• M-steps: Duetothemodelstructure,theobjectivefunctionLfortheM-stepcanbeexpressedasasumovertimestep-
specificquanities; forexample,seeEq.(B.7). Thus,inthecaseofmultipleexamples, wesimplyadjustthesetof
timestepsoverwhichwesumintheobjectivefunctionsrelativetoeachMsubstep. Weupdatetheentityemissions
parametersθ byalteringtheobjectivetosumovertimestepsthataren’tatthebeginningofanexample(sowesum
ee
overtimestepstwheret−1 ̸∈ E). Weupdatethesystemstateparametersθ andentitystateparametersθ by
ss es
alteringtheobjectivestosumonlyovertimestepsthathaven’tstraddledanexampletransitionboundary. Thatis,we
wanttoignoreanypairoftimesteps(t,t+1)wheret∈E,soweagainsumonlyovertimestepstwheret−1̸∈E.
Finally,weupdatetheinitializationparametersθ byalteringtheobjectivetosumoveralltimestepsthatareatthe
init
beginningofanexample.
C Methodology: SupplementalInformation
Herewedetailhowweassessmodelfit(Sec.C.1)andcomputeforecasts(Sec.C.2). Theprimarydifferencebetweenfitting
andforecastingisthatonlytheformerhasaccesstoobservationsfromevaluatedentitiesoveratimeintervalofinterest.
Hence,agoodfitismoreeasilyattained. Agoodforecastrequirespredictionsofthediscretelatentstatedynamicswithout
accesstofutureobservations,whereasfittingcanusethefutureobservationstoinferthediscretelatentstatedynamics.
However,modelfitisstillusefultoinvestigate;forinstance,itcanbeusefultodetermineifpiecewiselineardynamics
(includingthechoiceofK,thenumberofper-entitystates)provideagoodmodelforagivendataset.
C.1 Modelfit
Tocomputethefitofthemodelto{x(j),...x(j) },thej-thentity’sobservedtimeseriesoversomesliceofinteger-valued
t t+u
timepoints[t,...,t+u],weinitialize
µ(j) =x(j)
t−1 t−1
Andthenforwardsimulate. Inparticular,fortimeτ in[t,...,t+u],wedo
K
µ(j) ≜(cid:88) q(z(j) =k)µ(j) (C.1)
τ τ τ,k
k=1
where µ(j) is the conditional expectation of the emissions distribution from Eq. (2.4) with Radon-Nikody`m density
τ,k
p(x(j) |x(j) =µ(j) ,z(j)). Forexample,withGaussianvectorautoregressive(VAR)emissions,wehave
τ τ−1 τ−1 τ
µ(j) ≜A µ(j) +b
τ,k j,k τ−1,k j,k
Theresultingsequence{µ(j),...µ(j) }givesthevariationalposteriormeanforthej-thentity’sobservedtimeseriesover
t t+u
timepoints[t,...,t+u].
C.2 Partialforecasting
By partial forecasting, we mean predicting {x(j),...x(j) } , the observed time series from some to-be-forecasted
t t+u j∈J
entities(withindicesJ ⊂ [1,...,J])oversomeforecastinghorizonofinteger-valuedtimepoints[t,...,t+u], given
observations{x(j),...x(j) } fromthecontextualentitiesJc ≜{j ∈[1,...,J]:j ̸∈J}overthatsameforecasting
t t+u j∈Jc
horizon,aswellasobservationsfromallentitiesoverearliertimeslices{x(j),...x(j) } .
0 t−1 j∈[1,...,J]
Toinstantiatepartialforecasting,wemustfirstadjustinference,andthenperformaforwardsimulation.Wojnowicz,Rath,etal.
1. Inferenceadjustment. TheVEZstep(Sec.B.3)isadjustedsothatthevariationalfactorsontheentity-levelstates
over the forecasting horizon {q(z(j),...,z(j) )} are computed only for the contextual entities {j ∈Jc}. Like-
t t+u j
wise,theVESstep(Sec.B.2)isadjustedsothatthevariationalfactoronthesystem-levelstatesovertheforecast-
ing horizon q(s ,...,s ) is computed from the observations {x(j),...x(j) } and estimated entity-level states
t t+u t t+u j
{q(z(j),...,z(j) )} onlyfromthecontextualentities{j ∈Jc}.Asaresult,theM-steponthesystem-levelparameters
t t+u j
θ automaticallyexcludeinformationfromtheto-be-forecastedentitiesJ overtheforecastinghorizon[t,...,t+u].
ss
2. Forwardsimulation. UsingtheadjustedinferenceprocedurefromStep1,wecanusetheViterbialgorithm(orsome
otherprocedure)toobtainestimatedsystem-states{s ,...,s }thatdonotdependoninformationfromtheto-be-
(cid:98)t (cid:98)t+u
forecastedentitiesJ overtheforecastinghorizon[t,...,t+u]. Wethenmakeforecastsbyforwardsimulating. In
particular,fortimeτ in[t,...,t+u],wesample
z(j) ∼p(z(j) |z(j) ,x(j) ,s ,θ) (C.2)
t t t−1 t−1 (cid:98)t
x(j) ∼p(x(j) |x(j) ,z(j),θ) (C.3)
t t t−1 t
forallto-be-forecastedentitiesj ∈J.
NoteinparticularthatthedependenceofEq.(C.2)upons allowsourpredictionsaboutto-be-forecastedentities{j ∈J}
(cid:98)t
todependuponobservationsfromthecontextualentities{j ∈Jc}overtheforecastinghorizon.
D FigureEight ToyData: SupplementalInformation
D.1 Datageneratingprocess
ExampleD.1.1. (FigureEight.) Consideramodelwherewedirectlyobservecontinuousobservationsx(1:J),andwhere
0:T
each x(j) ∈ R2 lives in the plane (i.e. D = 2). We form “Figure Eights" by having the observed dynamics rotate
t
aroundan“uppercircle"C withunitradiusandcenterb(1) ≜(0,1)T anda“lowercircle"C withunitradiusandcenter
1 2
b(2) ≜(0,−1)T. Entitiestendtopersistentlyrotatearoundoneofthesecircles;however,whentheobservationapproaches
theintersectionofthetwocirclesC ∩C ={(0,0)},recurrentfeedbackcanshifttheentity’sdynamicsintoanewstate
1 2
(theothercircle). Theseshiftsoccuronlywhenthesystem-levelstatehaschanged;theseshiftsarenotpredictablefromthe
entity-leveltimeseriesalone. Inparticular,wehave
s |s =h(s ) (D.1a)
t t−1 t
(cid:124) (cid:123)(cid:122) (cid:125)
systemtransitions
(cid:18) (cid:19)
z t(j) |z t( −j) 1,x( t−j) 1,s
t
∼Cat-GLM
K
η t(j) =Ψ(st)f(x t( −j) 1)+P(cid:101)jTe
z(j)
(D.1b)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) t−1
entitytransitions recurrence (cid:124) (cid:123)(cid:122) (cid:125)
transitions
(cid:18) (cid:19)
x(j) |x(j) ,z(j) ∼N
A(zt(j))x(j) +b(zt(j)), Q(zt(j))
(D.1c)
t t−1 t j t−1 j j
(cid:124) (cid:123)(cid:122) (cid:125)
observationdynamics
Here,thenotationusedfollowsthatofEq.(2).Eachlineofthistruedata-generatingprocessisexplainedinthecorresponding
paragraphbelow.
System-levelstatetransitions. WetakethenumberofsystemstatestobeL=2. Wesetthesystemstatechain{s }T
t t=1
throughadeterministicprocesshwhichalternatesstatesevery100timesteps. Weemphasizethatinthetruedata-generating
process,thereisnorecurrentfeedbackfromobservationsxtosystemstatess.
Entity-level state transitions. We set entity-specific baseline transition preferences to be highly sticky, P =
j
(cid:20) (cid:21)
p (1−p)
, where p is close to 1.0 (concretely, p = .999). By design, these preferences can be overridden
(1−p) p
whenanentitytravelsneartheorigin. Wechoosetherecurrencetransformationf :RD →Rtobetheradialbasisfunction
f(x) = κexp(−||x||2 2),whichreturnsalargevaluewhentheobservationx(j) isclosetotheorigin. Similarly,weset
2σ2 t−1
theweightvectorfortheserecurrentfeaturestonudgeobservationsneartheorigintothesystem-preferredstate. Weset
Ψ(ℓ) ∈RK soentry(Ψ(ℓ)) =a iftheentity-levelstatekispreferredbythesystem-levelstateℓ,anda otherwise,
k high low
witha ≫a . Concretely,Weseta =2anda =−2.
high low high low
Emissions. Toconstructtheentity-levelemissiondistributionsforeachstate(indexedbyk),wechooseA(k) = A to
j j
bearotationmatrixwithangleθ = (−1)r 2π forallentity-levelstatesk,whereτ istheentity-specificperiodicityand
τj j
(
(Discoveringgroupdynamicsinsynchronoustimeseries
r ∈{0,1}determinestherotationdirection. WemayusearotationmatrixAtorotatetheobservationaroundacenterb,by
constructingdynamicsoftheformA(x−b)+b;therefore,toconstructcirclecentersthatarespecifictoentity-levelstates
usingEq.(D.1c),wesetb(k) =(I−A )b(k)forallentitiesj andallentity-levelstatesk. Weseteachoftheobservation
j j
noisecovariancematricesQ(k)tobediagonal,withdiagonalentriesequalto0.0001.
j
△
D.2 Dataset
We simulate data from the FigureEight model (Example D.1.1), where there are J = 3 entities, each with T = 400
observations,wheretheperiodicitiesforeachentityaregivenby(τ ,τ ,τ )=(5,20,40). Thegeneratedsequencesfrom
1 2 3
oneentityareshowninFig.D.6.
D.3 Methods
HSRDM. WefitourHSRDMwithtransitionsgiveninEq.(2)andGaussianvectorautoregressiveemissionsasinEq.(B.6).
WesetL=K =2. Wesettheentity-levelrecurrencef toaGaussianradialbasisfunctionandnosystem-levelrecurrence
g. WeperforminferenceasinSec.B.
rAR-HMM. A collection of J rARHMM models can be fit as a special case of a HSRDM model where the number of
systemstatesistakentobeL=1.
DSARF. WetraintheDeepSwitchingAutoregressiveFactorizationmodel[Farnooshetal.,2021]withseveraldifferent
parameters. Wetrainwithseveraldifferentchoicesoflags(l),spatialfactors(S),discretestates(K),andlearningrates,and
findl=[1,2,10,11],K =5,andS =5orS =10ispreferredforcapturingthedynamicsacrossallchannels. Wesetthe
samel,S andK forthecompleteindependence,completepoolingandthemulti-channelforecastingexperiments.
Additionally, for the forecasting experiments, we fit this model 4 separate times, using distinct random seeds for each
initialization,andchoosethemodelwiththelargestELBOaftertrainingfor500epochswithlearningratesof0.01and0.05.
Weperformlong-termpredictions,wherewedrawsamplesoftheheldoutobservationsfromentitythreefromthegenerative
modellearnedonthetrainingset. Todothis,wefirstfitthemodeltoall3entities(all400time-pointsforentity1andentity
2anduptotimepoint280forentity3). Time-points280to400inentity3arereplacedwithnan’sduringtraining,sinceour
goalistoforecastatthesetime-points.
Oncethemodelistrained,weusethelearnedparameters{θs,θw,θF,θz}todrawforecastsviaancestralsamplingfromthe
DSARFgenerativemodel:
s ∼p(s |s ,θs)
t t t−1
w ∼p(w |w ,s ,θw)
t t t−l t
z ∼p(z|θz)
f ∼p(F|z,θF)
1:K
Xforecast =[w ,w ,...,w ]T[f ,f ,...,f ]
280:400 280 281 400 1 2 K
D.4 Results
Asmentionedinthemainpaper,hereweshowall3sampleforecastsfromvariousmodels. Figs.D.3,D.4,andD.5showthe
sampleforecastsfromDSARF,rARHMM,andHSRDM,respectively. Thesampleforecastsarecomputedusingpartial
forecasting(Sec.C.2).
(
(
(
(Wojnowicz,Rath,etal.
DSARFIndep. DSARFPool DSARFConcat
2 2 2
1 1 1
0 0 0
1 1 1
2 2 2
1 0 1 0.5 0.0 0.5 1.0 0.0 0.5
2 2 2
1 1 1
0 0 0
1 1 1
2 2 2
1 0 1 0.5 0.0 0.5 0.0 0.5
2 2 2
1 1 1
0 0 0
1 1 1
2 2 2
1 0 1 0.5 0.0 0.5 1.0 0.0 0.5
FigureD.3: ModelpredictionsofheldouttimesegmentofoneentityinFigureEighttask. Shownareall3sampledpartial
forecastsfromtheDSARFbaselineundereachpossiblestrategy(Indep.,Pool,andConcat.,definedinSec.5.1). Each
columnrepresentsadifferentstrategy. Eachrowrepresentsadifferentmodelforecast.Discoveringgroupdynamicsinsynchronoustimeseries
rARHMMIndep. rARHMMPool rARHMMConcat
2 2 2
1 1 1
0 0 0
1 1 1
2 2 2
1 0 1 1 0 1 1 0 1
2 2 2
1 1 1
0 0 0
1 1 1
2 2 2
0 2 0 1 1 0 1
2 2 2
1 1 1
0 0 0
1 1 1
2 2 2
1 0 1 1 0 1 1 0 1
FigureD.4: ModelpredictionsofheldouttimesegmentofoneentityinFigureEighttask. Shownareall3sampledpartial
forecastsfromtherARHMMbaselineundereachpossiblestrategy(Indep.,Pool,andConcat.,definedinSec.5.1). Each
columnrepresentsadifferentstrategy. Eachrowrepresentsadifferentmodelforecast.
2 2 2
1 1 1
0 0 0
1 1 1
2 2 2
1 0 1 1 0 1 1 0 1
FigureD.5: ModelpredictionsofheldouttimesegmentofoneentityinFigureEighttask. Shownareall3sampledpartial
forecastsfromourHSRDM model. Eachcolumnrepresentsadifferentmodelforecast.
Fig.D.6showsthesystem-andentity-levelstatesegmentationslearnedbytheHSRDMmodelappliedtotheFigureEight
dataset.Wojnowicz,Rath,etal.
0 50 100 150 200 250 300 350
Time step
FigureD.6: System-andentity-levelstatesegmentationslearnedbytheHSRDMmodelappliedtotheFigureEightdataset.
Thetoprowgivesthesystem-levelsegmentations. Eachofthebottomthreerowsshowsthetimeseriesfromoneofthree
entities,withsuperimposedentity-levelstatesegmentations. Thetwodimensionsofthetimeseriesareplottedwithseparate
curves.
Althoughnotshownhere,wefindthatbaselinemodelscanfitthedatawell,despitehavingstruggledtoprovideaccurate
forecasts. Here,themodelfitiscomputedasthevariationalposteriormeantrajectory,asgiveninEq.(C.1). Itiseasierfor
modelstofitthedatathantoforecastthefuture. Agoodfitcanbeattainedsolongasthemodelcanlearnhowdramatic
shiftsintheobservationssuggestshiftsintheunderlyinglatentstates. Agoodforecastrequirespredictionsofthelatentstate
dynamicswithoutaccesstofutureobservations.
Fig.D.8showsthelearnedentity-leveltransitionprobabilitymatrices(tpms)asfunctionofthecontinuousobservationx(3)
t
andthesystem-levelstates . Here,wedefineanentitytobe“far"from(or“close"to)theoriginwhenx(j)hasaEuclidean
t t
distancetotheoriginthatisinthe95thpercentile(or5thpercentile,respectively),wherethepercentilesaretakenw.r.tthe
sequence{x(j)}T . Weseethatfarfromtheorigin,theHSRDMmodelassignsstickytpms,butclosetotheorigin,the
t t=1
entityisstronglypushedintoeitheranUPobservationoraDOWNobservation,accordingtothesystemstatuss ,whichin
t
thiscaseiscoordinatingan(eventual)synchronizationacrossentities. Incontrast,aflatSRDMcannotcoordinatetheentities
duetoitslackofgroup-levelswitches. Inthisway,theHSRDMforecaststhefuturebehavioroftheentityz(j),u > tby
u
integratingthreesourcesofinformation: (a)theentity’s(latentorphysical)locationx(j) ,(b)thepreviousbehaviorofthe
t−1
entityz(j) ,and(c)thestatusofthefullsystems .
t−1 tDiscoveringgroupdynamicsinsynchronoustimeseries
HSRDM SRDM
s = 1 s = 2
t t
1.00 0.00 1.00 0.00 0.99 0.01
Far from origin
0.00 1.00 0.00 1.00 0.01 0.99
1.00 0.00 0.00 1.00 0.98 0.02
Near origin
1.00 0.00 0.00 1.00 0.00 1.00
FigureD.8: Learnedtransitionprobabilitymatricesforentity3asafunctionofdistancetooriginandsystem-levelstatuss .
t
E VisualSecurityExperiment: SupplementalInformation
Forthevisualsecurityexperiment,basedonaquickexploratoryanalysis,wesetK =4andL=3. ForthestickyDirichlet
prioronsystem-leveltransitions,asgiveninEq.(B.2),wesetα=1.0andκ=50.0,sothatthepriorwouldputmostofits
probabilitymassonself-transitionprobabilitiesbetween.90and.99.
F BasketballExperiment: SupplementalInformation
F.1 Dataset
Rawdataset. WeobtainNBAbasketballplayerlocationdatafor636gameswithinthe2015-2016NBAseasonfroma
publiclyavailablerepo[Linou,2016]. Eachsampleprovidesthequarterofthegame,numberofsecondsleftinquarter,
timeonshotclock,(x,y,z)locationofball,andthe(x,y)locationsandIDsforthe10playersonthecourt. Thecourtis
representedastherectangle[0,96]×[0,50]inthespaceofsquaredfeet.
Selectionofgames. WefocusonmodelingthedynamicsingamesinvolvingtheClevelandCavaliers(CLE),the2015-2016
NBAchampions. Inparticular,outof40availablegamescontainingCLE,weinvestigatethe31gamescontainingoneofthe
fourmostcommonstartinglineups: 1. K. Irving - L. James - K. Love - J. Smith - T. Thompson;
2. K. Irving - L. James - K. Love - T. Mozgov - J. Smith; 3. L. James - K. Love - T.
Mozgov - J. Smith - M. Williams; 4. M. Dellavedova - L. James - K. Love - T. Mozgov
- J. Smith. Twogameshaddataerrors(lackoftrackingoreventdata),whichleftatotalofG=29gamesforanalysis.
Downsampling. Therawdataissampledat25Hz. FollowingAlcornandNguyen[2021],wedownsampleto5Hz.
From plays to examples. The raw basketball dataset is represented in terms of separate plays (e.g. shot block,
rebound offense, shot made). FollowingAlcornandNguyen[2021], wepreprocessthedatasetsothatthese
plays are non-overlapping in duration. We also remove plays that do not contain one of CLE’s four most common
startinglineups. Forthepurposeofunsupervisedtimeseriesmodeling, wethenconverttheplaysintocoarser-grained
observational units. Although plays are useful for the classification task pursued by Alcorn and Nguyen [2021], play
boundariesneedn’tcorrespondtoabrupttransitionsinplayerlocations. Forexample,theplayercoordinatesareessentially
continuousthroughoutshot block -> rebound offense -> shot madesequencementionedabove. Hence,
weconcatenateconsecutiveplaysfromtherawdatasetuntilthereisanabruptbreakinplayermotionand/orasampling
intervallongerthanthenominalsamplingrate. Theseobservationalunitsarecalledeventsinthemainbodyofthepaper
(Sec.5.2). Functionally,theseobservationalunitsserveasexamples(Sec.B.7). Thatis,whentrainingmodels,eachexample
istreatedasani.i.d.samplefromtheassumedmodel. FortheremainderoftheAppendix,werefertotheseobservational
unitsasexamples.Wojnowicz,Rath,etal.
Byconstruction,exampleshavealongertimescalethantheplaysintheoriginaldataset. Examplestypicallylastbetween20
secondsand3minutes. Forcomparison,arebound offenseplaytakesafractionofasecond.
Attheimplementationallevel,weinferanexampleboundarywheneveratleastoneconditionbelowismetinasequenceof
observations:
1. Thewallclockdifferencebetweentimestepsislargerthan1.2timesthenominalsamplingrate.
2. Theplayer’sstepsizeonthecourt(givenbythediscretederivativebetweentwotimesteps)isabnormallylargewith
respecttoeitherthecourt’slengthorwidth,whereabnormallylargeisdefinedashavinganabsolutez-scorelargerthan
4.0.
Courtrotation. Thelocationofateam’sownbasketchangesathalftime. Thiscanswitchcanalterthedynamicson
thecourt. Wewouldliketocontrolforthedirectionofmovementtowardstheoffensiveanddefensivebaskets,aswellas
forplayerhandedness. Tocontrolforthis,weassumethatthefocalteam(CLE)’sscoringbasketisalwaysontheleftside
ofthecourt. Whenitisnot,werotatethecourt180degreesaroundthecenterofthebasketballcourt. (Equivalently,we
negateboththexandycoordinateswithrespecttothecenterofthecourt.) Sincethebasketballcourthasawidthof94feet
andalengthof50feet,itscenterislocatedat(47,25)whenorientingthewidthhorizontally. Wepreferthisnormalization
strategytotherandomrotationsstrategyofAlcornandNguyen[2021],becausethenormalizationstrategyallowsustolearn
differentdynamicsforoffense(movementtotheleft)anddefense(movementtotheright).
Indexassignments. Eachsamplefromourdatasetgivesthecoordinatesonthecourtof10players. Herewedescribehow
wemaptheplayerstoentityindices. Recallthatweonlymodeltheplaysthatconsistofstartersfromafocalteam,CLE.We
assignindices0-4torepresentCLEstarters,andindices5-9torepresentopponents.
Index assignment for CLE is relatively straightforward. Although we model plays from the G games involving four
differentstartinglineups,wecanconsistentlyinterprettheindicesas0: Lebron James, 1: Kevin Love, 2:
J.R. Smith, 3: Starting Center, 4: Starting Guard. Dependingonthegame,thestartingcenter
waseitherT.MazgovorT.Thompson. Similarly,thestartingguardwaseitherK.Irving,M.Williams,orM.Dellavedova.
Indexassignmentfortheopponentsismoreinvolved. Theopponentteamscanvaryfromgametogame,andevenafixed
teamsubstitutesplayersthroughoutagame. Therearenumerousmechanismsforassigningindicesinthefaceofsuch
playersubstitutions[Raabeetal.,2023]. Althoughrole-basedrepresentationsarepopular(e.g. see[Felsenetal.,2018]or
[Zhanetal.,2019])becausetheycaptureinvariantslostwithinidentity-basedrepresentations[Luceyetal.,2013],weuseda
simpleheuristicwherebyweassignindices5-9basedonthetheplayer’stypicalpositions. Thetypicalpositionscanbe
scrapedfromWikipedia. Weletthemodeldiscoverdynamicallyshiftingrolesfortheplayersviaitshierarchicaldiscrete
staterepresentation.
One complication in assigning indices from these position labels is that the provided labels commonly blend together
multiplepositions(e.g. ‘Shootingguard/smallforward’or‘Center/powerforward’). Shouldthesecondplayerbelabeled
asacenteroraforward? Whatiftherearemultiplecenters? Howdowediscriminatebetweentwoforwards? Tosolvesuch
problems,weproceedasfollows,operatingonaplay-by-playbasis
1. Assignplayerstocoarsepositiongroups. Wefirstassignplayerstocoarsepositiongroups(forward,guard,center). We
assumethateachplayhas2forwards,1center,and2guards. Weuseindices5-6torepresenttheforwards,index7to
representthecenter,andindices8-9torepresenttheguards. Asnotedabove,agivenplayercanbemultiplyclassified
intoacoarsepositiongroup;however,areasonableassignmentforaplayercanbemadebyconsideringtheposition
labelsfortheotherplayerswhoareonthecourtatthesametime. Todothis,weformB,a5×3binarymatrixwhose
rowsareplayersontheteamandwhosecolumnsrepresentthecoarsepositiongroups. Anentryinthematrixisset
toTrueiftheplayerisclassifiedintothatpositiongroup. Westartwiththerarestpositiongroup(i.e. thecolumnin
Bwiththesmallestcolumnsum)andassignplayerstothatpositiongroup,startingwithplayerswhohavetheleast
classifications(i.e. theplayerswhoserowsinBhavethesmallestrowsum). Tiesarebrokenrandomly. Wecontinue
untilwehavesatisfiedthespecifiedassignments(2forwards,1center,and2guards). Ifitisnotpossibletomakesuch
coarseassignments,wediscardtheplayfromthedataset.
2. Orderplayerswithinthecoarsepositiongroups. Thissteponlyneedstobeperformedforforwardsandguards,since
thereisonly1orderingofthesinglecenter. Wedefineanarbitraryorderingofforwardpositionsby
FORWARD_POSITIONS_ORDERED = [Discoveringgroupdynamicsinsynchronoustimeseries
"Small forward / shooting guard",
"Small forward / point guard",
"Small forward",
"Small forward / power forward",
"Power forward / small forward",
"Power forward",
"Power forward / center",
"Center / power forward",
"Shooting guard / small forward",
]
andguardpositionsby
GUARD_POSITIONS_ORDERED = [
"Small forward / shooting guard",
"Shooting guard / small forward",
"Shooting guard",
"Shooting guard / point guard",
"Point guard / shooting guard",
"Point guard",
"Combo guard",
]
Foreachplayersassignedtoapositiongroupin{forward,guard},weordertheplayersintermsoftheirlocationof
theirpositionontheabovelists. Tiesarebrokenrandomly.
Normalization Toassistwithinitializationandlearningofparameters,wenormalizetheplayerlocationsonthecourt
fromtherectangle[0,96]×[0,50]inunitsoffeettotheunitsquare[0,1]×[0,1].
F.2 Evaluationstrategy
WedividetheG = 29totalgamesinto20gamestoformacandidatetrainingset,4gamestoformavalidationset(for
settinghyperparameters),and5gamestoformatestset. Ofthefirst20gameswithinourcandidatetrainingset,weconstruct
small(1game),medium(5games),andlarge(20games)trainingsets. Thesmall,medium,andlargetrainingsetscontained
20,215,and676examples,respectively.
Thetestsetcontained158examplesoverall. However,werequiredthateachexamplebeatleast10secondslong(i.e. 50
timesteps)tobeincludedintheevaluationrun. ThisexclusioncriterionleftE = 75examples. Foreachsuchexample,
weuniformlyselectatimepointT∗ ∈ [T ,T −T ]todemarcatewherethecontextwindowends.
min-context-length forecast-length
We set T = 4 seconds (i.e. 20 timesteps) and T = 6 seconds (i.e. 30 timesteps). The first
min-context-length forecast-length
[0,T∗] seconds are shown to the trained model as context, and forecasts are made within the forecasting window of
F :=[T∗+1,T∗+T ]seconds.
forecast-length
Forafixedexamplee,forecastingsamples,playerj,andforecastingmethodm,wesummarizetheerrorinaforecasted
trajectorybymeanforecastingerror(MFE)
(cid:118)
(cid:117) 1
1 (cid:88)(cid:117)(cid:88)
MFE m;e,s,j ≜ |F| (cid:116) (x (cid:98)e,t,j,d,m,s−x e,t,j,d)2 (F.1)
t∈F d=0
where x is the true observation on example e at time t for player j on court dimension d, and x is the
e,t,j,d (cid:98)e,t,j,d,m,s
forecastedobservationbyforecastingsamplesusingforecastingmethodm. SoMFE givestheaveragedistanceover
m;e,s,j
theforecastingwindowbetweentheforecastedtrajectoryandthetruetrajectory.
Toquantifytheperformanceofaforecastingmethods,wecandefineamodel’sexample-wisemeanforecastingerroras
S J
1 (cid:88)(cid:88)
MFE ≜ MFE (F.2)
m;e SJ m;e,s,j
s=1j=1
TakingthemeanofMFE anditsstandarderrorletsusquantifyamodel’stypicalsquaredforecastingerroronanexample,
m;eWojnowicz,Rath,etal.
aswellasouruncertainty,with
E
1 (cid:88)
MFE ≜ MFE (F.3)
m E m;e
e=1
(cid:113)
(cid:80)E (MFE −MFE )2
σ(MFE ) ≜ e=1 m;e m (F.4)
m E
AlthoughinSec.F.1,wedescribednormalizationofbasketballcoordinatestotheunitsquareforthepurposeofmodel
initialization and training, when evaluating models, we convert the forecasts and ground truth back to unnormalized
coordinates, so that MFE has units of feet. That is, we represent observations x and forecasts x on the
√ e,t,j,d (cid:98)e,t,j,d,m,s
basketballcourt(ofsize[0,94]×[0,50]feet). Thus MSE canbeinterpretedasamodel’stypicalamountoferrorinfeet
m
onthecourtatatypicaltimepointintheforecastingwindow(butofcourseforecastingerrortendstobelowerattimepoints
closertoT∗thanfartherfromT∗).
F.3 Models
F.3.1 OverviewofModels
WecomparetheperformanceofourHSRDMagainstanumberofbaselines,givingthemodelingstrategiesbelow. For
detailsoneachstrategy,seeSec.F.3.2.
1. HSRDM.Thisisourhierarchicalswitchingrecurrentdynamicalmodel,aspresentedinSec.2.
2. rARHMMs. Byablatingthetop-leveldiscrete"game"states(i.e.,thesystem-levelswitches)intheHSRDM,weobtain
10independentrARHMMs[Lindermanetal.,2017],oneforeachplayer.
3. HSDM.Byablatingthemulti-levelrecurrencefromtheHSRDM,weobtainahierarchicalswitchingdynamicalmodel.
4. AgentFormerisatransformer-inspiredstochasticmulti-agent(i.e. multi-entity)trajectorypredictionmodel.
5. Fixedvelocity. Herewecomputeeachplayer’svelocityfromthetwotimestepsimmediatelypriortotheforecasting
windowF,andtakethisvelocitytobeconstantthroughoutF. Thisisacommon,andoftensurprisinglycompetitive,
naivebaselineformulti-agentmodels;e.g. see[Yehetal.,2019].
F.3.2 ModelingDetails
HSRDM. Here we model J = 10 basketball player trajectories on the court with an HSRDM with Gaussian vector
autoregressiveemissions;thatis,weuse
(cid:18) (cid:19)
s
t
|s t−1,x( t−1: 1J) ∼Cat-GLM
L
Π(cid:101)Te
st−1
+ Λg ψ(cid:0) x t( −1: 1J),υ t−1(cid:1) (F.5)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
systemtransitions endogenoustransitionpreferences biasfromrecurrenceandcovariates
(cid:18) (cid:19)
z t(j) |z t( −j) 1,x( t−j) 1,s
t
∼Cat-GLM
K
(P(cid:101) j(st))Te
z(j)
+ Ψ( jst)f ϕ(cid:0) x( t−j) 1,u( t−j) 1(cid:1) (F.6)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) t− (cid:125)1 (cid:124) (cid:123)(cid:122) (cid:125)
entitytransitions endogenoustransitionpreferences biasfromrecurrenceandcovariates
(cid:18) (cid:19)
x(j) |x(j) ,z(j) ∼N A(z t(j)) x(j) +b(z t(j)) , Q(z t(j)) (F.7)
t t−1 t j t−1 j j
(cid:124) (cid:123)(cid:122) (cid:125)
observationdynamics
wherex(j) ∈(cid:0) [0,1]×[0,1](cid:1) givesplayerj’slocationonthenormalizedbasketballcourtattimestept.
t
Our system-level recurrence g (cid:0) x(1:J),υ (cid:1) = x(1:J) reports all player locations x(1:J) to the system-
ψ t−1 t−1 t−1 t
level transition function, allowing the probability of latent game states to depend on player loca-
tions. Inspired by Linderman et al. [2017], our entity-level recurrence function f (cid:0) x(j) ,u(j) (cid:1) =
ϕ t−1 t−1
(x(j) ,I[x(j) <0.0],I[x(j) >1.0],I[x(j) <0.0],I[x(j) >1.0])T, where x(j) is the d-th coordinate of
t−1 t−1,0 t−1,0 t−1,1 t−1,1 t,d
x(j) andI[·]istheindicatorfunction,reportsanindividualplayer’slocationx(j) (andout-of-boundsindicators)tothat
t t−1
player’sentity-leveltransitionfunction,allowingeachplayer’sprobabilityofremaininginautoregressiveregimestovaryin
likelihoodoverthecourt.Discoveringgroupdynamicsinsynchronoustimeseries
We set the number of system and entity states to be L = 5 and K = 10 based on informal experimentation with the
trainingset; weleaveformalsettingofthesevaluesbasedonthevalidationsettofuturework. ForthestickyDirichlet
prioronsystem-leveltransitions,asgiveninEq.(B.2),wesetα=1.0andκ=50.0sothatthepriorwouldputmostofits
probabilitymassonself-transitionprobabilitiesbetween.90and.99.
WeinitializethemodelusingthesmartinitializationstrategyofSec.B.6. Wepre-initializetheentityemissionsparameters
θ byapplyingthek-meansalgorithmtoeachplayer’sdiscretederivatives(solongasconsecutivetimestepsdonotspan
ee
anexampleboundary). Wepre-initializetheentitystateparametersθ
es
bysettingP(cid:101) tobethelogofastickysymmetric
transitionprobabilitymatrixwithaself-transitionprobabilityof0.90,andbydrawingtheentriesofΨi.i.dfromastandard
normal. We pre-initialize the system state parameters θ
ss
by setting Π(cid:101) to be the log of a sticky symmetric transition
probabilitymatrixwithaself-transitionprobabilityof0.95,andbydrawingtheentriesofΛi.i.dfromastandardnormal. We
pre-initializetheinitializationparametersθ bytakingtheinitialdistributiontobeuniformoversystemstates,uniform
init
overentitystatesforeachentity,andstandardnormaloverinitialobservationsforeachentityandeachentitystate. We
executethetwo-stageinitializationprocessvia5iterationsofexpectation-maximizationfortheJ bottom-halfrARHMMs,
followedby20iterationsforthetop-halfARHMM.
WerunourCAVIalgorithmfor2iterations,asinformalexperimentationwiththetrainingsetsuggestedthiswassufficient
forapproximateELBOstabilization.
rARHMMs. Byablatingthetop-leveldiscrete"game"states(i.e.,thesystem-levelswitches)intheHSRDM,weobtaininde-
pendentrARHMMs[Lindermanetal.,2017],oneforeachoftheJ =10players. Morespecifically,byremovingthesystem
transitionsinEq.(F.5)fromthemodel,theentitytransitionssimplifyasp(z(j) |z(j) ,x(j) ,s )=p(z(j) |z(j) ,x(j) ),be-
t t−1 t−1 t t t−1 t−1
causetheentitytransitionparameterssimplifyasP(cid:101) j(st) =P(cid:101)jandΨ j(st) =Ψ j.Asaresult,theJ bottom-levelrARHMMsare
decoupled. Implementationally, thisprocedureisequivalenttoanHSRDMwithL = 1systemstates. Initializationand
trainingisotherwiseperformedidenticallyaswithHSRDM.
HSDM By ablating the multi-level recurrence from the HSRDM, we obtain a hierarchical switching dynamical model
(HSDM).Thiscanbeaccomplishedbysettingg ≡ 0inEq.(F.5)andf ≡ 0inEq.(F.6). Initializationandtrainingis
ψ ϕ
otherwiseperformedidenticallyaswithHSRDM.
Agentformer. AgentFormer[Yuanetal.,2021]isamulti-agent(i.e. multi-entity)variantofatransformermodelwhose
forecastsdependuponbothtemporalandsocial(i.e. across-entity)relationships. Unlessotherwisenoted,wefollowYuan
etal.[2021]indeterminingthetraininghyperparameters. Inparticular,ourpredictionmodelconsistsof2stacksofidentical
layersfortheencoderanddecoderwithadropoutrateof0.1. Thedimensionsofkeys,queriesandtimestampsforthe
agentformeraresetto16,whilethehiddendimensionofthefeedforwardlayerissetto32. Thenumberofheadsforthe
multi-headagentawareattentionis8andallMLPsinthemodelhaveahiddendimensionof(512,256). Thelatentcode
dimensionoftheCVAEissetto32,andtheagentconnectivitythresholdissetto100. Becausethebasketballtraining
datasetshavemanymoreexamplesthanthepedestriantrajectorypredictionexperimentsinYuanetal.[2021](whichonly
have8examples),wetraintheagentformermodelandtheDLowtrajectorysamplerfor20epochseach(ratherthan100)to
keepthecomputationalloadmanageable. WethereforeapplytheAdamoptimizerwithlearningrateof10−3ratherthan
10−4 toaccommodatethereducednumberofepochs. Also,tomatchthespecificationsoftheevaluationstrategyfrom
Sec.F.2,wesetthenumberoffuturepredictionframesduringtrainingto30,andthenumberofdiversetrajectoriessampled
bythetrajectorysamplerto20. Weensureconvergencebytrackingthemean-squarederror.
F.4 Futuredirections
OpportunitiesaboundforimprovingtheabilityofHSRDMtomodelbasketballtrajectories. Hereweprovidesomeexamples:
1. Utilizecovariateswithintransitionfunctions. NotefromSec.F.3.2thatcovariateswerenotusedintherecurrence
functions. OtherinstantiationsofEq.(2)mightconsiderusingusefulcovariates,suchasthecoordinatesoftheball.
2. Makerecurrencefunctionsmoreinformative. Raabeetal.[2023]describeexpertfeaturesdeterminedbythesports
analyticscommunitytobeusefulforforecastingbasketballtrajectories. Thesefeaturescouldbeusefultoincorporate
withinthesystem-levelandentity-leveltransitionfunctions. Forinstance,Raabeetal.[2023,Table2,pp.3795]present
twogame-levelfeatures,teamballdistanceandinterteamdistance,whichcouldprovideusefulinformationtothe
x(1:J)-to-ssystem-levelrecurrencefunction. Similarremarkscouldbemadeaboututilizingplayer-levelfeaturesfrom
Raabeetal.[2023]toinformthex(j)-to-z(j)entity-levelrecurrencefunctions.Wojnowicz,Rath,etal.
3. Usehigher-orderrecurrencefunctions. OurcurrentinstantiationsofHSRDMusesfirst-orderrecurrencebyfeeding
backplayerlocationsintostatetransitionfunctions. Byextendingtosecond-orderrecurrence,wecouldfeedback
playerderivativestostatetransitionfunctions,whichmightprovidevaluableadditionalinformationaboutwherethe
playerswillgonext.
4. Recognizecourtboundariesmoredirectly. ThecurrentinstantiationofHSRDMrespectsenforcescourtboundaries
indirectly, via the linear entity-level recurrence function of [Linderman et al., 2017] and via biasing entity-level
transitionswithweightedout-of-boundsindicators. AsseeninSec.5.2,thisstrategydoesreducetheprobabilityof
wildlyout-of-boundsforecasts. However,thecourtboundariescouldperhapsbeenforcedmoredirectlybyforecasting
withGaussianvectorautoregressionsthataretruncated,eitherpost-hocaftertrainingWojnowiczetal.[2023]orduring
trainingitself. Notethecomplicationthatplayersdosometimesrunoutofbounds,althoughnotasoftenorasmuchas
happensduringsomeoftheforecastsmadebyanyofthemodelingstrategieswestudied.
FurtherimprovementstomodelingbasketballcouldperhapsbeobtainedbyexpandingtheHSRDMtoincludeanadditional
z(1:J)-to-srecurrence,whichmightbecalledatop-levelrecurrence. Suchrecurrencemightnaturallyacccommodatecertain
featuresfromRaabeetal.[2023]suchasteamstretchindexandteamseparateness.
G Code
Source code for running HSRDM and partially reproducing the experiments in this paper can be found at https://
github.com/mikewojnowicz/dynagroup.