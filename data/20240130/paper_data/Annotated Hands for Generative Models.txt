Annotated Hands for Generative Models
Yue Yang†, Atith N Gandhi†, Greg Turk
College of Computing, Georgia Institute of Technology, 225 North Avenue, Atlanta,
30332, GA, USA.
*Corresponding author(s). E-mail(s): turk@cc.gatech.edu;
Contributing authors: yygx@cs.unc.edu; agandhi98@gatech.edu;
†These authors contributed equally to this work.
Abstract
GenerativemodelssuchasGANsanddiffusionmodelshavedemonstratedimpressiveimagegeneration
capabilities. Despite these successes, these systems are surprisingly poor at creating images with
hands. We propose a novel training framework for generative models that substantially improves
the ability of such systems to create hand images. Our approach is to augment the training images
with three additional channels that provide annotations to hands in the image. These annotations
provide additional structure that coax the generative model to produce higher quality hand images.
We demonstrate this approach on two different generative models: a generative adversarial network
and a diffusion model. We demonstrate our method both on a new synthetic dataset of hand images
and also on real photographs that contain hands. We measure the improved quality of the generated
hands through higher confidence in finger joint identification using an off-the-shelf hand detector.
Keywords:generativemodels,handpose,diffusionmodel,generativeadversarialnetwork,imagequality
metrics.
1 Introduction
We speculate that there are several reasons
whygenerativemodelsarepooratcreatinghands.
Generative models have recently demonstrated
First,theremayberelativelyfewimagesofhands
impressive results, creating images of scenes that
inagiventrainingdataset,orthehandsmayoften
may contain a wide variety of realistic looking
takeuponlyasmallportionoftheimage.Second,
objects. Even the richness of human faces are
hands are high degree-of-freedom objects where
captured well by generative models. One notable
eachofthe15jointscanbeatmanynumberofdif-
failing of these models, however, is the genera-
ferent angles. For this reason, a generative model
tion of hands [5, 32, 35]. Generative models will
has to learn a high dimensional manifold of possi-
often create hands that have too many or too few
blehandshapes.Also,whereasmanyobjects(e.g.
fingers, left hands in a right hand context (and
faces, cars, houses) have a natural upright pose,
vice-versa),andfingersthatarebentatunnatural
hands are found in a wide variety of orientations.
angles. A web search for “AI art hands” leads to
Finally,thenon-thumbfingersofahandlookvery
numerousarticlesdiscussingtheproblemofdraw-
similar to each other. This similarity of appear-
inghandsusinggenerativemodels.Whyarehand
ance could cause issues in terms of the number of
imagessodifficultforgenerativemodelstocreate?
1
4202
naJ
62
]VC.sc[
1v57051.1042:viXrafingers that are generated and for distinguishing architectureenabledmorestructuredlatentspaces
right from left hands. to be trained [15, 16], which gave users more
Our approach to improving the hand gen- control over image generation.
eration capabilities of a generative model is to Variational autoencoders provided a new
provide additional information beyond the red, directionforgenerativemodels[17].Inparticular,
green and blue color channels of an image. We vector quantized variational autoencoders (VQ
create three additional channels that essentially VAE) proved to be a particularly powerful tool
helpto“teach”thegenerativemodelabouthands for image generation [4, 31]. Through contrastive
during training. These annotation channels indi- learning, the CLIP model learned a shared latent
cate that a hand has five fingers, that there are spaceforbothtextandimages[21].Bycombining
three separate segments on each finger, and that CLIPandaVQVAE,theoriginalDALL-Earchi-
a hand can be left or right. We train a generative tecture allowed a user to create images based on
model using six-channel images, which requires text prompts [23].
the model to match not only the rgb colors of Recently, diffusion models have proved to
the image, but also to match the three additional be highly effective at image generation [3, 11].
annotation channels.We have foundthat training Classifier-freeguidanceofdiffusionmodelsproved
a generative model using such augmented images to be an effective way of boosting the quality of
causes the generative model to produce higher images created by diffusion models [12]. DALL-E
quality hand images. 2 demonstrated the control of a diffusion model
Our training approach is model agnostic, and using text prompts using CLIP embeddings, and
can be used to train any image generation model. these results drew considerable attention to dif-
WedemonstrateourmethodwithStyleGAN2and fusion models [22]. DALL-E 3 improved the gen-
with a simple diffusion model. We show improved erated image quality by training with improved
handgenerationresultsinbothcases,asmeasured image captions [29]. Rombach et al. introduced a
by several hand quality metrics. latent space encoding of pixels that allows high
The key contributions in our work are as resolutionimagestobegeneratedbydiffusionata
follows: lower computational cost [24], and this approach
was used to train the popular Stable Diffusion
• Developed a method of creating annotations
model.
of hand images for use in training genera-
A variety of techniques have been introduced
tive architectures that produce improved hand
to control diffusion models. Textual inversion and
images.
Dreambootharetwoapproachesthatallowsadif-
• Created a synthetic hand image dataset that
fusion model to be tuned to generate images of
can be used for evaluating the effectiveness of
a particular object [6, 26]. Prompt-to-prompt is
hand synthesis.
a technique that leverages control of the cross-
• Developed measures for evaluating the quality
attentionlayerstoallowausertotuneanimageby
of hands in images.
performingeditstotheirtextprompt[9].Instruct
• Demonstrated our training approach for hands
Pix-2-Pix trains a diffusion model based on an
can be used with both generative adversarial
imageeditingsuite,andtheresultingmodelallows
networks and diffusion models.
text edits to images [2]. ControlNet fine-tunes a
diffusionmodeltoallowcontroloverimagegener-
2 Related Work
ationusingimagessuchasonethatcontainedges,
depth, or color-coded body parts [36].
While early autoencoder models were capable
of creating simple images, the rise of generative
3 Preliminaries
image models really took off with the invention of
generativeadversarialnetworks(GANs)byGood-
3.1 Generative Adversarial
fellow et al. [8]. Soon researchers were able to
Networks
create powerful GAN models with architectures
such as BigGAN [1] and Progressive GAN [14].
Generative Adversarial Networks (GANs), intro-
Further advances in the form of the StyleGAN
duced by Goodfellow et al., were a significant
2advanceingenerativemodels.GANsconsistoftwo denoiseanimagethatbeginsaspurenoiseand
neuralnetworks,agenerator(G)andadiscrimina- evolves until it converges to a real image.
tor (D), engaged in a minimax game. The genera-
Both the forward and reverse processes,
tor strives to create data that is indistinguishable
indexed by t, unfold over a finite number of time
fromrealsamples,whilethediscriminatoraimsto
steps, typically a large value such as T = 1000.
correctly classify whether a given sample is real
The process begins at t = 0 by sampling a real
or generated. This adversarial training process is
imagex fromthedatadistribution(e.g.,anIma-
driven by the following loss function: 0
geNet cat image). At each time step, the forward
processintroducesGaussiannoise,whichisadded
minmaxV(D,G)=E [logD(x)]+ to the image from the previous step. This gradual
G D
x∼pdata(x)
(1) noise addition, following a well-structured sched-
E [log(1−D(G(z)))] ule, eventually leads to convergence to what is
z∼pz(z)
known as an isotropic Gaussian distribution at
Here, p (x) represents the distribution of t=T.
data
real data, and p (z) is the distribution of noise During training, the neural network (typically
z
variables. GANs excel at generating high-quality, a U-Net) is trained to perform the reverse pro-
novel data and have applications across various cess, namely to take a partially noised image and
domains,fromimagesynthesistonaturallanguage takeasmallsteptowardsalessnoisyimage.After
processing. thenetworkistrained,imagegenerationiscarried
In this work, we are using StyleGAN2 [15] out by sampling from the Gaussian distribution,
which is an update over the first StyleGAN [16]. andthenperformingmany(small)stepsofdenois-
StyleGAN is a generative adversarial network ing until a fully denoised image has been created.
whosedesignisinspiredbystyletransfer[13].The Whileearlydiffusionmethodsrequiredmanysteps
majorcontributionofStyleGANpaperwastocre- tocreateanimagesample,thedenoisingdiffusion
ate a less entangled latent space. This along with implicit model (DDIM) can generate high qual-
other modifications in the generative adversarial ity images using a small number of steps [30]. By
networkresultedinthebetterqualityimagegener- conditioning the denoising process based on text,
ation.Furthermodificationsinthegeneratorwere a diffusion model can be created that generates
made in the StyleGAN2 to avoid generation of images based on a text prompt [22].
blob-like artifacts in the image, and improve the
overall quality. 4 Methods
3.2 Diffusion Models In this section, we provide a comprehensive intro-
duction to our method. First, we present the
Recently, diffusion models have garnered signif-
methodology for creating the additional annota-
icant attention due to their ability to generate
tion channels, which is the core of our approach
highqualityimagesbasedontextprompts.These
(Section4.1).Next,wedescribetheprocessofcre-
models start with pure noise and gradually gen-
ating annotation channels on two different types
eraterealisticimages[11].Thedenoisingdiffusion
of data: synthesized hands (Section 4.2) and real
model setup involves two essential processes:
hands (Section 4.3).
1. Forward Diffusion Process:Afixed(orpre-
defined) forward diffusion process, denoted as 4.1 Annotation Channels
q,isselected.ThisprocessintroducesGaussian
Generative models can typically learn and gener-
noisetoanimage,startingfromaninitialimage
ate high-quality images using the geometric and
from the training set.
color information available in images with the
2. Reverse Denoising Diffusion Process: A
traditional red, green, and blue (RGB) channels.
learned reverse denoising diffusion process,
However, generating high-quality hand images
denoted as p , is employed. In this process,
θ with correct finger count, natural finger posi-
a neural network is trained to progressively
tions, and proper finger length poses a unique
challenge due to the specific characteristics of
3Fig. 1: Hand Skeleton. Fig. 2: First Annotation Fig. 3: Second Annotation
Channel. Channel.
Fig. 4: (a) Left hand and its annotation channels. (b) Right hand and its annotation channels.
hand images discussed in Section 1. To overcome thatthered,greenandbluechannelsaredescribed
these challenges and to enable generative mod- by integers in the range of 0 to 255.
els to capture the intricate characteristics of hand In the first annotation channel, the grayscale
images, we provide additional information during pixel values from the thumb to the little finger
model training. Consequently, we introduce three take on the values [50, 150, 250, 100, 200], where
annotation channels to provide supplementary the segments creating the thumb have pixel val-
information beyond the RGB channels. ues 50 and the segments of the little finger have
Our annotation channel scheme is centered pixelvalues200.(Figure2).Thisdifferentialpixel
around producing colored hand skeletons. These value assignment for each finger aids the genera-
skeletons consist of 20 line segments that connect tive models in distinguishing between individual
the 21 keypoints of the hand (Figure 1), allow- fingers. Additionally, this annotation layer helps
ing generative models to more easily learn the the models learn when to halt the generation of
structure of a hand. By incorporating hand skele- fingers, preventing incorrect finger count in the
tons, we provide valuable geometric information generated images.
by assigning different colors to different skeleton In the second annotation channel, the
parts,whichimpartsadditionalusefulinformation grayscale pixel values from the hand base to the
tothemodel.Thisdesignensuresthattheannota- fingertips are [100, 200, 50, 250]; where the seg-
tion channels deliver essential information to the ments connecting to the hand base have pixel
generativemodelswithoutsignificantlymodifying value 100 and the fingertip segments have pixel
the learning process. values 250. (Figure 3). This pixel value assign-
In our annotation channel design, we employ mentservestoguidegenerativemodelsinlearning
three channels that share the same skeleton as a the gradual changes from the hand base to the
hand but that are indicated with different pixel fingertips.
colors. In our description below, we will assume The third annotation layer is dedicated to dis-
tinguishing between left and right hands. If the
4Fig. 5:Pipelineforgeneratingannotatedhands.Tocreatesynthetichandimages,wepasshandsizeand
pose information to the MANO hand model. We then render an image from the model and compose it
in front of a background. For real hands, we utilize the Onehand10k dataset and rely on Mediapipe to
generate annotations.
hand in the image is left, we assign all the joints the wide range of finger poses and hand textures,
100pixelvalue,whilefortherighthandweassign we take additional steps to enhance the variabil-
200 pixel value. This layer is crucial as left-right ity of our synthesized hand images. This includes
information plays a significant role in generating comprehensive rotation of the hand in all angles
accurate hand details. As can be seen in Figure 4, and the integration of diverse backgrounds [34]
thesameskeletonposecouldrepresenttheventral representing various scenarios like the kitchen,
side for the left hand but the dorsal side for the bedroom, bathroom, basement, and more.
right hand. Substantial differences exist between To generate annotation channels for synthe-
dorsal and ventral details, such as palmar flex- sizedhands,itisnecessarytospecify21handkey-
ion creases, nails, hand hair, and blood vessels. points along with their corresponding 20 lines to
By incorporating this annotation channel, gener- construct a complete hand skeleton. The MANO
ative models can more effectively categorize hand model provides precise 3-dimensional keypoints
details, avoiding the mixing of ventral and dorsal information,withwhichwecandeterminethecor-
hand features. responding positions of these keypoints within a
2-dimensional image. Once we have obtained the
4.2 Synthesized Hands 21 hand keypoints, we use the methods described
in Section 4.1 to draw skeletons by connecting
Prior to training on real hand photos, we initially
them.Thegenerationofthefirstandsecondanno-
assesstheeffectivenessofourproposedannotation
tation channels is straightforward. For the third
channels using synthetic data.
annotation channel, we can acquire left and right
As shown in the blue box of Figure 5, we
hand information during the synthesis of hands
leveragetheMANOmodel[25]togeneratesynthe-
using the MANO model. To ensure proper occlu-
sizedhandimagesinwhichtheshape,textureand
sion, we carefully manage the order in which
pose of the hand can be varied. The synthesized
the skeleton segments are drawn. We draw the
hand images are rendered with textures [20] that
farther skeleton segments first, followed by the
encompassvariationsinage,gender,andethnicity,
closer skeleton segments, ensuring that the closer
thereby ensuring the generation of realistic and
segments properly occlude the farther ones. For
visually appealing synthetic hands. In addition to
5identifying the z-dimension of the skeletal joint, To measure the quality of generated hand
themeanofthez-dimensionoftheconnectingkey- images, we design three new metrics as described
points are taken and then we draw the skeletal inSection5.2.Finally,weshowsynthesizedhands
joints in the order of farthest to nearest. results and comparative quantitative results in
We have created a synthetic hands dataset Section 5.3.
with 10,000 image samples using the aforemen- We evaluate the performance of the GAN
tioned method. This data was used to train our model on both real and synthetic hand images,
models on synthetic hand images. We plan to and for diffusion model we test our approach only
make this dataset publicaly available. on synthetic hand images. We evaluate the per-
formances of the model on 2000 images generated
4.3 Real Hands using seed 1-2000.
The large-scale generative models are typically
5.1 Design of GAN Models and
trained on real photographs from the internet.
Diffusion Models
These models are expected to generate high qual-
ityimagesofpeoplealongwithreallookinghands; To assess the effectiveness of our proposed
however, even the popular and commercial gen- method, we employ two types of generative mod-
erative models fail to generate consistently good els,namelyGANandtheDiffusionModel,known
looking hands. for their exceptional capabilities in image gener-
For training with real hand images we use the ation, to assess the effectiveness of our proposed
Onehand10k dataset consisting of 10,000 images methods.
[33]. The dataset consists of hand images in real ThearchitectureoftheGANmodelweemploy
life scenarios, with a wide range of variations in is derived from StyleGAN2 [16], recognized for
terms of rotation, gesture, poses, and representa- its impressive image generation performance. In
tion in terms of age, gender, and ethnicity. adapting the StyleGAN2 model to our methodol-
For generating annotations of the real hands ogy,wemadeakeymodificationbyexpandingthe
we use mediapipe, a powerful and popular library input channel from 3 to 6. This straightforward
forhandsidentification[19].Whenhandsarepro- architecturaladjustmentholdsbroadapplicability
cessedusingmediapipe,itidentifieslandmarksfor beyond StyleGAN2 and can be readily extended
each of the 21 keypoints of hands in 3D. Once we to other GAN models, thus underscoring the ver-
have the 3D positions of all the keypoints, we use satility of our approach in catering to various
the method described in Section 4.1 to draw the downstreammodels.Belowarethetrainingdetails
segments by joining the specific keypoints of the for StyleGAN2:
hands. Using the keypoint orderings obtained by The diffusion model architecture we employ is
mediapipe, the position of the joint segments can based on [18]. The embedded U-Net architecture
be identified and colored accordingly for generat- follows an encoder-decoder structure with a base
ing the first and the second annotation channels. dimension of 64 and multiplicative factors of (1,
For third channel, we get the handedness (left or 2, 4, 8).
right) information from mediapipe.
Insomeoftheimages,thehandswerepartially 5.2 Design of Hand Evaluation
present, and so the joint predicted by medi-
Metrics
apipe were outside the image grid. We discarded
those images for which all the hand joints were The Fr´echet Inception Distance (FID) [10] is a
not present inside the image. Our final training widely used method for evaluating the quality
dataset consisted of 9931 images. of images created by generative models. How-
ever, considering the unique features of hands,
5 Experiments and Results it becomes necessary for us to develop new
approachestailoredtoparticularlyassessthequal-
We evaluate the proposed method on both Gen- ity of generated hand images. In addition to
erative Adversarial Networks and the diffusion FID, we introduce several new metrics that are
model in Section 5.1. specifically devised for hands.
65.2.1 Mediapipe Confidence Dataset FID Mediapipe Above90% MeanJoints
Score↓ Confidence↑ Confidence↑ RatioDifferences↓
Synthetic
Mediapipe detects 21 keypoints of hands in an (3channels) 55.54 0.573 0.361 0.0476
image. It also assigns a hand detection confidence Synthetic 81.94 0.729 0.367 0.0178
(6channels)
score with each hand predicted in the image. If Real
164.75 0.4328 0.25 0.0367
(3channels)
the hands are not clear, blurry or there are no Real
133.63 0.693 0.443 0.0293
hands in the image, Mediapipe will still assign 21 (6channels)
keypoints but with less confidence. If the hands
Table 1: StyleGAN2 results trained over 3 and 6
are recognizable in the image, Mediapipe assigns
channels, both for synthetic (top) and real (bot-
ascorewithahigherconfidence.Thus,Mediapipe
tom) training images.
hand keypoints detection confidence is one of the
key metrics that we use to test the efficacy of our
approach.
5.2.2 Above 90% Confidence (cid:118)
(cid:117) 21
(cid:117)(cid:88)
Mean Joint Ratio Difference=(cid:116) (N [i]−N [i])2
In addition to hand detection confidence, we d g
assessed the percentage of generated hands i=1
deemed of high quality, using a 90% confidence
5.3 Results
threshold. The “Above 90% Confidence” metric
reports the percentage of generated hands with a 5.3.1 StyleGAN2
Mediapipehanddetectionconfidencegreaterthan
or equal to 90%. Table 1 lists StyleGAN2 results from training
on both 6-channel and 3-channel datasets, and
5.2.3 Mean Joint Ratio Difference assesses these results using the different evalua-
tionmetrics.TheMediapipeConfidence isnotably
Eventhoughexistinggenerativemodelscancreate higher for the 6-channel model in both real and
images of hands that are recognizable, the finger synthetic hands when compared to the 3-channel
segmentlengthsmaynotbeinaproperratiowith model. The introduction of annotations during
eachother.Forinstance,itiscommonforhumans training increases the percentage of images with
to have the middle finger longer than the other Mediapipe hand keypoint detection confidence,
fingers, and the pinky shorter than all other fin- achievingAbove 90% Confidence.TheMean Joint
gers. While there may be occasional exceptions, Ratio Difference metric measures the average
the lengths of the average human fingers gener- deviation of joint length ratios, calculated via
ally adhere to a specific ratio. To achieve lifelike Mediapipe keypoints, in generated hands from
hands, we assess our method through mean joint those in the dataset. Notably, the model trained
ratio difference. This involves obtaining the mean with6-channeldataproducesimageswithalower
joint length of generated hands, normalizing the (i.e.,better)Mean Joint Ratio Difference thanits
jointvector,andcalculatingthemeansquareddif- 3-channel counterpart, signifying a closer match
ference with the normalized mean joint length of in joint ratios to the ground truth dataset hands.
hands in the dataset. For the real hands dataset, the 6-channel
Let H i be the joint lengths for the i-th gen- model outperforms the 3-channel model in terms
erated hand, where H i = [h i,1,h i,2,...,h i,20] is a of the FID score. However, in the synthetic hands
list of 20 joints. dataset, the 3-channel model achieves a superior
Let H be the mean of hand joint lengths for FID score. The synthetic dataset includes hand
all generated hands, where H =[h 1,h 2,...,h 20]. images with backgrounds, and we suspect that
Let N g be the normalized mean joint length our 6-channel model prioritizes generating hands,
vector for the generated hands, where N g = as the additional channels exclusively focus on
(cid:104) (cid:105)
h1 , h2 ,..., h20 . hand features. The FID score, which measures
∥H∥ ∥H∥ ∥H∥
the distribution difference between generated and
Similarly, we calculate N be the normalized
d
dataset images, is influenced by the backgrounds
mean joint vector for the hands in the dataset.
generated by the model. Therefore, this dynamic
Then,
7Dataset FID Mediapipe Above90% MeanJoints No.ofChannels MediapipeConfidence
Score↓ Confidence↑ Confidence↑ RatioDifference↓
3channels 0.433
Synthetic
(3channels) 134.798 0.696 0.552 0.015 3channels+4thchannel 0.606
Synthetic 3channels+5thchannel 0.620
146.781 0.732 0.604 0.028
(6channels) 3channels+6thchannel 0.601
6channels 0.693
Table 2: Diffusion model results trained over 3
and 6 channels. Table 3: Average Mediapipe Accuracy in Style-
GAN2 was evaluated across various training con-
figurationsonrealhandsdataset:3channels,(3+
results in a lower (i.e., better) FID score for the
4th)channels,(3+5th)channels,(3+6th)chan-
3-channel model in the synthetic dataset, as it
nels, and all 6 channels.
captures both hand and background information.
To qualitatively compare the hands generated
by StyleGAN2 trained with 3-channel annotated hands are not distinctly recognizable, we suspect
data, please consult Figure 6 for synthetic hands that Mediapipe, uncertain about the presence of
and Figure 7 for real hands. hands, projects keypoints based on its under-
standing of the given image and knowledge of
5.3.2 Diffusion Model handstructures.Consequently,inscenarioswhere
generated hands lack clarity, Mediapipe tends to
Due to constraints on the size of the real hand
produce keypoints with joint segments featuring
dataset, coupled with high background variability
better ratios, albeit with lower confidence.
initsimagesandlimitedcomputingresources,the
Forseetheresultsofsynthetichandsgenerated
trained diffusion model produces poor results for
by the Diffusion model trained with 3 channels
both the 3 channels and the 6 channels datasets.
and annotated data, please refer to Figure 8.
Therefore,weareonlyreportingonsynthetichand
results, and exclude the real hand results.
5.3.3 Ablation Studies
Table 2 shows the results over various hand
evaluation metrics for the diffusion model trained We conducted ablation studies to explore how
on the synthetic hand dataset with 3 and 6 chan- measured accuracy changes with the individual
nels. The Mediapipe Confidence is higher for the use of different annotation channels in addition
model trained with 6 channels compared to 3 to the original image (RGB channel). Table 3
channels. Training with 6 channels also increases presents StyleGAN2 results when trained using
the percentage of images with Mediapipe hand real hand datasets with RGB plus one of the 4th,
keypoint detection confidence, achieving Above 5th, or 6th channels on real hands dataset. The
90% Confidence. findingsindicatethattheinclusionofanyannota-
The FID score is lower (i.e., better) for tion channel enhances Mediapipe Confidence. The
the model trained with 3 channels compared to accuracy over the 3-channel dataset is approxi-
the 6 channels trained model. As detailed in mately 0.4326, and with the addition of just one
Section 5.3.1, the synthetic hand dataset com- annotation channel, it improves to around 0.600.
prises hand images with backgrounds. The 6- Notably,trainingwithall6channelsyieldshigher
channel model prioritizes generating hands over accuracy than training with only 3 plus any one
backgroundimagesduetotheadditionalchannels additional channel, suggesting that utilizing all 3
dedicated to hand structure. This prioritization channelsresultsinamoresignificantperformance
contributes to the lower (i.e., better) FID score improvement.Additionally,the5thchannelproves
for the 3 channels model. to be the most effective, as it demonstrates a
The Mean Joint Ratio Difference is lower for moresubstantialincreaseinaccuracycomparedto
3 channels than for 6 channels, indicating that the other channels. For the 4th and 6th channels,
the diffusion model generates a higher proportion a similar increase in accuracy is observed when
of hands that are less recognizable compared to either of the channels is individually added along
StyleGAN2. Mediapipe, having been trained on a with the original image.
large set of hands, possesses prior knowledge of
hand joints and their ratios. In instances where
8Fig. 6:SyntheticHandimagesgeneratedbyStyleGAN2whentrainedover6channelsand3channels(1)
The images from the dataset + corresponding annotations, (2) The images generated after training the
StyleGAN2 model over synthetic hand images with background (3) The images and the corrseponding
annotations generated after training the StyleGAN2 model over 6 channel synthetic hand images with
background
Fig. 7: Real Hand images generated by StyleGAN2 when trained over 6 channels and 3 channels (1)
The images from the real hands dataset + corrseponding annotations (2) The images generated after
training the StyleGAN2 model over real hand images (3) The images and the corrseponding annotations
generated after training the StyleGAN2 model over 6 channeled real hand images
6 Discussion and Limitations
input during image generation. There are other
methods that require user intervention to create
Our approach to the problem of hand generation
images that contain improved hands. For exam-
is to alter the training of the generative model.
ple, some diffusion models have been trained to
Our method does not require any special user
9Fig. 8:Synthetichandimagesgeneratedbydiffusionmodelwhentrainedover6channelsand3channels.
(1) The images from the synthetic hands dataset + corrseponding annotations (2) The images generated
after training the Diffusion model over synthetic hand images with background (3) The images and the
corrseponding annotations generated after training the Diffusion model over 6 channeled synthetic hand
images with background
allow inpainting, where the user marks a region similartotheskeletonfeaturesfromtheOpenPose
of an image and the generative model then fills moduleinControlNet.Itisimportanttorecognize
in the marked regions in a manner that matches that we use such color-labeled skeleton features
its surroundings. If the model generates an image only during model training, whereas ControlNet
in which a hand looks unrealistic, the user can requires an OpenPose skeleton to be provided for
invoke inpainting to attempt to fix the hand. each image that is to be generated when this
This method not only requires user judgement, module is being used.
but sometimes several inpainting attempts are The biggest limitation to our work is the lack
required before a satisfactory result is generated. ofalargerdatabaseofimagesthatcontainhands.
Another approach to address the issue of poor The onehand10k database that we used is modest
hand generation for a diffusion model is to use in size compared to the large databases that are
ControlNet [36]. ControlNet requires a user to usedtotraindiffusionmodels.Forexample,Stable
provide a guiding image, and then uses features Diffusion1.5wastrainedonthe600millionimage
detected from such an image as part of the con- dataset called LAION Aesthetics 5+ [27, 28]. It
trolthatisusedtogenerateanothersimilarimage. would be ideal if we could use our method to
Several kinds of features can be used to guide train on such a large database of images that
image generation, including detected edges, skele- includeshandsinawidevarietyofcontexts.Some
ton segments or depth. To generate images with of these contexts might include hand and faces,
better hands, the user would need to provide an hands as part of whole body images of people,
image that has a hand in the same pose as they and hands interacting with a variety of objects.
desire in a generated image. While this method Unfortunately, gathering such a database would
can produce good resulting hands, this requires be an extensive undertaking. Creating the labels
the user to provide features from a control image for such a varied collection of images might also
for every new image that is to be generated. This requireautomatichandidentificationthatismore
requires considerably more intervention on the robust under these broader contexts.
user’s part than simply providing a text prompt. Another limitation of our work is that we
We note that our annotated channels look quite are unable to train large diffusion models due to
10lack of compute resources. While there are sev- [2] Tim Brooks, Aleksander Holynski, and
eral publicly available diffusion models that have AlexeiAEfros.2023. Instructpix2pix:Learn-
already been trained (e.g. Stable Diffusion), the ing to follow image editing instructions. In
more capable of these models required months of Proceedings of the IEEE/CVF Conference on
trainingonsupercomputerclusters.Ourownmod- Computer Vision and Pattern Recognition.
estacademiccomputeresourcesdonotallowusto 18392–18402.
trainadiffusionmodelatsuchscale.Asindicated
inTable5intheAppendix,thetrainingofthedif- [3] Prafulla Dhariwal and Alexander Nichol.
fusion model requires approximately 10 days (on 2021. Diffusion models beat gans on image
one Nvidia RTX 4090 Ti), even for a modest 5 synthesis. Advances in Neural Information
epochs. Processing Systems 34 (2021), 8780–8794.
Finally, the mediapipe library has its own
[4] Patrick Esser, Robin Rombach, and Bjorn
limitations. We are relying on mediapipe from
Ommer.2021. Tamingtransformersforhigh-
annotation generations in real hands and eval-
resolution image synthesis. In Proceedings
uations. Although, overall mediapipe does good
of the IEEE/CVF conference on computer
work in hand keypoint detections and handed-
vision and pattern recognition. 12873–12883.
ness (left or right) identification, still it is not
100%accurate.Ourevaluationmetricslike”Medi-
[5] BBCScienceFocus.2023.WhyAIGenerated
apipeConfidence”,”Above90%Confidence”,and
Hands are the Stuff of Nightmares. https://
”MeanJointRatioDifference”arebasedonmedi-
www.sciencefocus.com/future-technology/
apipe. Changing the underlying library for our
why-ai-generated-hands-are-the-stuff-of-nig
evaluation metrics may impact the outcomes.
htmares-explained-by-a-scientist. Accessed:
2023-11-18.
7 Conclusion
[6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or
We have demonstrated that by training a gen- Patashnik, Amit H Bermano, Gal Chechik,
erative model with additional annotated image and Daniel Cohen-Or. 2022. An image is
channels, we can improve the quality of hand worth one word: Personalizing text-to-image
image generation. Our method is not model spe- generation using textual inversion. arXiv
cific, and can be used to improve hand image preprint arXiv:2208.01618 (2022).
generation using any generative model paradigm
(VAEs, GANs, diffusion models, and so on). [7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi
Thereareseveralpossibledirectionsforfuture Mirza, Bing Xu, David Warde-Farley, Sherjil
work.First,wewouldliketocarryoutlargerscale Ozair, Aaron Courville, and Yoshua Bengio.
trainingusingmanymoreannotatedhandimages. 2014. Generative adversarial nets. Advances
We have not yet undertaken the task of collecting in neural information processing systems 27
a larger hand image dataset. Second, it would be (2014).
idealtotrainalargetext-to-imagediffusionmodel
using our method for annotating hands. Finally, [8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi
our general approach to annotation may be appli- Mirza, Bing Xu, David Warde-Farley, Sherjil
cabletoother3Dobjectsthathavehighdegreeof Ozair, Aaron Courville, and Yoshua Ben-
freedom configurations. gio. 2020. Generative adversarial networks.
Commun. ACM 63, 11 (2020), 139–144.
References
[9] Amir Hertz, Ron Mokady, Jay Tenenbaum,
Kfir Aberman, Yael Pritch, and Daniel
[1] Andrew Brock, Jeff Donahue, and Karen
Cohen-Or. 2022. Prompt-to-prompt image
Simonyan. 2018. Large scale GAN train-
editing with cross attention control. arXiv
ing for high fidelity natural image synthesis.
preprint arXiv:2208.01626 (2022).
arXiv preprint arXiv:1809.11096 (2018).
11[10] Martin Heusel, Hubert Ramsauer, Thomas MichaelHays,FanZhang,Chuo-LingChang,
Unterthiner, Bernhard Nessler, and Sepp Ming Guang Yong, Juhyun Lee, Wan-Teh
Hochreiter.2017.Ganstrainedbyatwotime- Chang, Wei Hua, Manfred Georg, and
scale update rule converge to a local nash Matthias Grundmann. 2019. MediaPipe: A
equilibrium. Advances in neural information FrameworkforBuildingPerceptionPipelines.
processing systems 30 (2017). arXiv:1906.08172 [cs.DC]
[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. [20] Neng Qian, Jiayi Wang, Franziska Mueller,
2020. Denoising diffusion probabilistic mod- Florian Bernard, Vladislav Golyanik, and
els. Advances in neural information process- Christian Theobalt. 2020. Html: A paramet-
ing systems 33 (2020), 6840–6851. ric hand texture model for 3d hand recon-
struction and personalization. In Computer
[12] Jonathan Ho and Tim Salimans. 2022. Vision–ECCV 2020: 16th European Confer-
Classifier-free diffusion guidance. arXiv ence, Glasgow, UK, August 23–28, 2020,
preprint arXiv:2207.12598 (2022). Proceedings, Part XI 16. Springer, 54–71.
[13] Xun Huang and Serge Belongie. 2017. Arbi- [21] Alec Radford, Jong Wook Kim, Chris Hal-
trarystyletransferinreal-timewithadaptive lacy, Aditya Ramesh, Gabriel Goh, Sand-
instance normalization. In Proceedings of the hini Agarwal, Girish Sastry, Amanda Askell,
IEEE international conference on computer Pamela Mishkin, Jack Clark, et al. 2021.
vision. 1501–1510. Learningtransferablevisualmodelsfromnat-
ural language supervision. In International
[14] Tero Karras, Timo Aila, Samuli Laine, and
conference on machine learning. PMLR,
Jaakko Lehtinen. 2017. Progressive growing
8748–8763.
of gans for improved quality, stability, and
variation. arXiv preprint arXiv:1710.10196 [22] Aditya Ramesh, Prafulla Dhariwal, Alex
(2017). Nichol, Casey Chu, and Mark Chen. 2022.
Hierarchical text-conditional image gener-
[15] Tero Karras, Samuli Laine, and Timo Aila.
ation with clip latents. arXiv preprint
2019.Astyle-basedgeneratorarchitecturefor
arXiv:2204.06125 1, 2 (2022), 3.
generative adversarial networks. In Proceed-
ings of the IEEE/CVF conference on com- [23] Aditya Ramesh, Mikhail Pavlov, Gabriel
puter vision and pattern recognition. 4401– Goh, Scott Gray, Chelsea Voss, Alec Rad-
4410. ford, Mark Chen, and Ilya Sutskever. 2021.
Zero-shot text-to-image generation. In Inter-
[16] Tero Karras, Samuli Laine, Miika Aittala,
national Conference on Machine Learning.
Janne Hellsten, Jaakko Lehtinen, and Timo
PMLR, 8821–8831.
Aila. 2020. Analyzing and improving the
image quality of stylegan. In Proceedings [24] Robin Rombach, Andreas Blattmann,
of the IEEE/CVF conference on computer Dominik Lorenz, Patrick Esser, and Bj¨orn
vision and pattern recognition. 8110–8119. Ommer. 2022. High-resolution image syn-
thesis with latent diffusion models. In
[17] Diederik P Kingma and Max Welling. 2013.
Proceedings of the IEEE/CVF conference
Auto-encoding variational bayes. arXiv
on computer vision and pattern recognition.
preprint arXiv:1312.6114 (2013).
10684–10695.
[18] Lucidrains.2023. DenoisingDiffusion Proba-
[25] Javier Romero, Dimitrios Tzionas, and
bilistic Models in PyTorch. https://github.c
Michael J Black. 2022. Embodied hands:
om/lucidrains/denoising-diffusion-pytorch.
Modeling and capturing hands and bodies
together. arXiv preprint arXiv:2201.02610
[19] Camillo Lugaresi, Jiuqiang Tang, Hadon
(2022).
Nash, Chris McClanahan, Esha Uboweja,
12[26] NatanielRuiz,YuanzhenLi,VarunJampani, rabbit-holes/the-uncanny-failures-of-ai-gen
Yael Pritch, Michael Rubinstein, and Kfir erated-hands. Accessed: 2023-11-18.
Aberman. 2023. Dreambooth: Fine tuning
text-to-image diffusion models for subject- [36] Lvmin Zhang and Maneesh Agrawala.
driven generation. In Proceedings of the 2023. Adding conditional control to text-
IEEE/CVF Conference on Computer Vision to-image diffusion models. arXiv preprint
and Pattern Recognition. 22500–22510. arXiv:2302.05543 (2023).
[27] RunwayML.2022. StableDiffusion1.5Model
Card. https://huggingface.co/runwayml/st
able-diffusion-v1-5. Accessed: 2023-11-18.
[28] Christoph Schuhmann. 2022. LAION Aes-
thetics. https://laion.ai/blog/laion-aesthetic
s/. Accessed: 2023-11-18.
[29] Zhan Shi, Xu Zhou, Xipeng Qiu, and Xiao-
dan Zhu. 2020. Improving image captioning
with better use of captions. arXiv preprint
arXiv:2006.11807 (2020).
[30] Jiaming Song, Chenlin Meng, and Stefano
Ermon. 2020. Denoising diffusion implicit
models. arXiv preprint arXiv:2010.02502
(2020).
[31] Aaron Van Den Oord, Oriol Vinyals, et al.
2017. Neuraldiscreterepresentationlearning.
Advances in neural information processing
systems 30 (2017).
[32] Vox.2023.WhyAIArtStruggleswithHands.
https://www.youtube.com/watch?v=24yjRb
Bah3w. Accessed: 2023-11-18.
[33] Yangang Wang, Cong Peng, and Yebin Liu.
2019.Mask-PoseCascadedCNNfor2DHand
Pose Estimation From Single Color Image.
IEEE Transactions on Circuits and Systems
for Video Technology 29, 11 (2019), 3258–
3268. https://doi.org/10.1109/TCSVT.2018
.2879980
[34] Jianxiong Xiao, James Hays, Krista A
Ehinger, Aude Oliva, and Antonio Torralba.
2010. Sun database: Large-scale scene recog-
nitionfromabbeytozoo.In2010 IEEE com-
puter society conference on computer vision
and pattern recognition. IEEE, 3485–3492.
[35] TheNewYorker.2023. TheUncannyFailure
of AI. https://www.newyorker.com/culture/
13A Training Details B More Results
The training hyper-parameters for StyleGAN Weincludemoreresultsonsubsequentpagesfrom
are included in Table 4. The training hyper- our annotation approach, both from StyleGAN2
parameters for diffusion model are included in and from a simple diffusion model.
Table 5.
B.1 StyleGAN2 - Synthetic Hands
Parameter Value
Refer-Figure9forimagesgeneratedusing3chan-
learningrate 1e-4
batchsize 64 nels dataset, and Figure 10 for images generated
imagesize 256×256 using the annotated dataset.
trainingsamplecount-RealHands 9,931
trainingclocktime ∼120hr
B.2 StyleGAN2 - Real Hands
epochs 2048
gammavalue 50
Refer - Figure 11 for images generated using
Table 4: Hyper-parameters for StyleGAN2
3 channels dataset, and Figure 12 for images
model.
generated using the annotated dataset.
B.3 Diffusion - Synthetic Hands
Refer - Figure 13 for images generated using
Parameter Value
3 channels dataset, and Figure 14 for images
learningrate 8e-5
batchsize 32 generated using the annotated dataset.
imagesize 128×128
trainingsamplecount 10,000
trainingclocktime ∼240hr
epochs ∼5
timestepsperimage 1000
totaltimesteps 1,500,000
Table 5: Hyper-parameters for
diffusion model.
14Fig. 9: Generated images from SyleGAN2 that was trained on synthetic hand data with 3 channels.
Seeds used 100 - 124
15Fig. 10: Generated images from StyleGAN2 model that was trained on synthetic hand data with 6
channels. Seeds used 100 - 124
16Fig. 11: Generated images from StyleGAN2 model that was trained on real hand data with 3 channels.
Seeds used 100 - 124
17Fig. 12: Generated images from SyleGAN2 that was trained on real hand data with 6 channels. Seeds
used 100 - 124
18Fig. 13: Generated images from a simple diffusion model that was trained on synthetic hand data with
3 channels. Seeds used 100 - 124
19Fig. 14: Generated images from a simple diffusion model that was trained on synthetic hand data with
6 channels. Seeds used 100 - 124
20