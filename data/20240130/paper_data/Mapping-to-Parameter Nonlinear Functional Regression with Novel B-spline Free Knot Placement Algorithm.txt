Mapping-to-Parameter Nonlinear Functional
Regression with Novel B-spline Free Knot Placement
Algorithm
Chengdong Shi, Ching-Hsun Tseng, Wei Zhao, Xiao-Jun Zeng‚Äª
Department of Computer Science, University of Manchester
‚Äªx.zeng@manchester.ac.uk
Abstract
We propose a novel approach to nonlinear functional regression, called the Mapping-to-
Parameter function model, which addresses complex and nonlinear functional regression
problems in parameter space by employing any supervised learning technique. Central to
this model is the mapping of function data from an infinite-dimensional function space to a
finite-dimensional parameter space. This is accomplished by concurrently approximating
multiple functions with a common set of B-spline basis functions by any chosen order, with
their knot distribution determined by the Iterative Local Placement Algorithm, a newly
proposed free knot placement algorithm. In contrast to the conventional equidistant knot
placement strategy that uniformly distributes knot locations based on a predefined number
of knots, our proposed algorithms determine knot location according to the local complexity
of the input or output functions. The performance of our knot placement algorithms is shown
to be robust in both single-function approximation and multiple-function approximation
contexts. Furthermore, the effectiveness and advantage of the proposed prediction model in
handling both function-on-scalar regression and function-on-function regression problems
are demonstrated through several real data applications, in comparison with four groups of
state-of-the-art methods.
Keyword: Functional Data analysis; Function-on-Function Regression; Functional Neural
Network; Supervised Learning1. Introduction
Advancements in scientific measurement systems have led to increased collection of data
that are observed repeatedly across a continuum, such as time or space, in various
applications areas, including health monitoring (Shang and Hyndman 2017), financial
markets (Das et al. 2019), and meteorology (Wang et. al 2020). Such data can be
characterized by one or more functions, known as functional data. The primary attributes
that distinguish other data categories are their intrinsically infinite-dimensional nature and
generation by smooth underlying processes. While their infinite dimensionality
configuration offers rich information for academic investigation, it renders standard data
analysis techniques inapplicable without modification. A common way to handle functional
data is to discretize the functions into vectors, and then process them through traditional
multivariate data analysis methods. However, this discretization strategy may result in a
high dimension of the input space as well as a failure in fully exploiting the inherent smooth
functional behavior underlying the data. Consequently, the field of Functional Data Analysis
(FDA) has garnered substantial research attention (e.g., Ramsay and Silverman 2005;
Ferraty and Vieu 2006; Wang et al. 2016). FDA stands out by treating each function as an
individual sample element and employing tailored techniques to analyze functional data for
specific tasks, including Functional Regression (Morris, 2015), Classification (Preda et.al
2007), or Clustering (Jacques and Preda 2014).
Functional Regression encompasses three categories: 1) Scalar-on-Function (SOF) with
function input and scalar output; 2) Function-on-Scalar (FOS) with scalar input and function
output; 3) Function-on-Function (FOF) with both function input and output. While linear
models for these regression problems have been extensively investigated, limited research
work has been dedicated to the nonlinear model, particularly concerning the FOF regression
problem.
Within the existing literature, the nonlinear FOF regression problem has been approached
through two main avenues. The first approach is a statistics-based methodology that follows
a two-step procedure. It begins by representing functional data through basis functionsexpansion and subsequently estimating the nonlinear relationship between functional
predictor and functional response using statistical methods. The second approach, known as
Functional Neuron Network (FNN), is a data-driven method that leverages the capabilities
of neural networks to capture and learn the complex nonlinear mapping between functional
inputs and outputs in an end-to-end manner.
In statistical methodologies, the initial step involves employing basis function expansion to
represent functional data with the coordinates of their projection on a well-chosen base while
preserving sufficient accuracy. Several families of basis functions have been investigated in
the FDA literature, including B-splines (Beyaztas and Shang 2022, Luo and Qi 2017),
Fourier (Thinda et a. 2023; Luo and Qi 2019), and Wavelet (Luo and Qi 2021). Functional
Principal Components (FPCs) serve as analogs to basis functions and are derived
empirically from the eigenfunctions of the covariance operator of functional data (Kowal
and Bourgeois, 2020; Yao and M√ºller 2005). Among the available options, B-spline basis
functions are often preferred due to their superior flexibility in capturing highly nonlinear
complex patterns. A B-spline has a piecewise polynomial structure where the polynomial
pieces are connected at knots. The accuracy of a B-spline approximation of a function is
significantly impacted by both the location and the number of knots. Despite their potential,
the application of B-spline in current FDA research remains oversimplified and less
effective, with most studies relying on a uniform knot distribution with a predetermined
knot number, suitable only for homogeneous curves (Jupp 1978).
Substantial efforts have been dedicated to the investigation of the optimal knot placement
in non-uniform spaces within the field of function approximation. Various approaches for
free knot placement have emerged, including iterative knot placement such as knot insertion
(Tjahjowidodo et al. 2017; Tjahjowidodo et al. 2015) and knot removal method (Kang et al.
2015; Conti et al. 2001), stochastic methods like Genetic Algorithms (Goldenthal and
Bercovier 2004) and Particle Swarm Optimization ( G√°lvez and Iglesias 2011), and heuristic
methods based on specific properties of the input dataset such as derivative (Yeh et al. 2020),
curvature (Liang et al. 2017) and arc-length (Michel and Zidna 2021). These existing free
knot placement techniques have consistently exhibited superior performance inapproximating a single function compared to the equidistant knot placement method.
However, their direct implementation in the FDA context is limited by the fact that the
primary concern of functional representation in the FDA lies in approximating a set of
functions rather than a single function.
The subsequent step after functional representation involves modeling the functional
relationship between functional predictors and responses. In the case of the standard
functional linear regression (FLR), a linear parametric form is assumed for this relationship:
ùëå(ùë†) = ùõΩ (ùë†)+‚à´ùëã(ùë°)ùõΩ(ùë†,ùë°)ùëëùë° +ùúÄ(ùë†) (1)
ùëú
where ùëã(ùë°) and ùëå(ùë†) denote the functional predictor and response, ùõΩ(ùë†,ùë°) represents the
bivariate coefficient function, and ùúÄ(ùë†)signifies the random error function (Luo and Qi 2017;
Luo and Qi 2021; Beyaztas and Shang 2022). To alleviate the linearity assumption in
functional modeling, Ferraty and Vieu (2006) investigated fully nonparametric function
models based on kernel smoothing techniques. However, selecting an appropriate metric for
the kernel in the function space remains challenging, as highlighted by Delaige and Hall
(2010), due to the inherent complexity of the function space itself. Alternatively, several
methods have introduced structural constraints to the model, allowing for a degree of
flexibility while restricting the learning to only certain types of nonlinear relations. For
example, the functional quadratic model, as studied by Sun and Wang (2020), assumes a
quadratic relationship between the functional predictor and response by incorporating a
quadratic term. Another well-known instance is the functional additive model (FAM),
explored by Maller, Wu, and Yao (2013), which assumes an additive effect of time. In
general, the current statistical approach in the FDA has been dominated by functional
modeling within infinite-dimensional spaces. This attribute poses challenges in developing
nonlinear functional regression models, given that the explicit expression for the nonlinear
functional mapping between predictor and response in infinite-dimensional spaces remains
elusive owing to the intricate nature of the functional spaces.
Unlike statistical approaches that manually identify nonlinear functional relationships
between functional predictor and response, the alternative approach, Functional NeuronNetwork (FNN), can automatically interrogate these nonlinear functional relationships
through neural networks based on the data itself. Pioneering works by Rossi and Conan-
Guez (2005) and Ross et al. (2005) first introduced the concept of FNN, with an extension
of the traditional neuron network architecture by transforming the first hidden layer into a
functional layer with functional neurons capable of handling functional inputs. The
exploration of FNN models has led to various modifications in the field. Thind et al. (2022)
proposed the FuncNN model, which goes beyond the traditional FNN architecture by
enabling the incorporation of both functional and scalar inputs. Furthermore, Yao, Mueller,
and Wang (2021) developed the AdaFNN model, which introduces a basis layer with each
neuron acting as a micro neural network, allowing for adaptive learning of optimal basis
functions specific to the functional data. While these advancements have primarily focused
on SOF regression, a limited number of studies have addressed the FOF regression problem
by transforming the output layer into a functional layer with functional neurons to deliver a
final functional estimation (Rao and Reimher 2023, and Hsieh et al. 2021). In contrast to
statistical methods consisting of two separate steps, the FNN operates in an end-to-end
fashion. In fact, however, the first hidden layer in FNNs serves the same purpose as the
function representation in statistical methods, with the number of neurons determining the
number of basis functions used to approximate functional input. Nevertheless, the FNN
model relies on a trial-and-error approach to determine the optimal number of neurons,
which could be time-consuming and lack a theoretical guide. In addition, the learning
process of FNNs proceeds still in the function space, where the nonlinear relationship is
learned through iterative adjustment of the functional weights. This leads to the fact that two
drawbacks commonly associated with neural networks, high computational cost and lack of
interpretability, can be further amplified in FNNs due to the complex nature of the function
space.
In this paper, we present a novel methodology called the Mapping-to-Parameter (M2P)
nonlinear FOF regression model. This model comprises two procedures: (1) representing
the input and output functions using B-spline basis functions expansion and mapping them
into finite-dimensional parameter space, and (2) learning the nonlinear functionalrelationship between these input and output functions in the parameter space using usual
supervised learning techniques. The significant advancement of our model is its capability
to overcome obstacles encountered by existing approaches in uncovering unknown
nonlinear functional relationships in complex function spaces, by shifting the learning
process into the parameter space. Furthermore, we introduce a pathway for approximating
a set of functions by determining a common set of B-spline basis functions capable of
representing all given functions with the required precision. For this purpose, we propose a
new free knot placement algorithm, Iterative Local Placement (ILP). The ILP algorithm
aims to identify the optimal common knot distribution that closely resembles the trajectory
of the given functions considering the maximum local complexity among all functions. This
ensures the approximation embodies the key characteristics of each function while
preserving the intended level of accuracy. The main contribution of this work includes:
(i) Functional Representation with the Novel Knot Placement Algorithm: The
proposed ILP algorithm delivers a theoretical guide for determining the optimal
common set of B-spline basis functions for input or output functions based on the
characteristics of the data itself. Functional representation based on the ILP
algorithm marks the pioneering integration of free knot placement techniques
within the scope of functional regression.
(ii) Uncovering Nonlinear Relationship in Parameter Space via M2P model: The
introduction of our novel M2P model facilitates the learning of the nonlinear
mapping between input and output functions within parameter space. This
framework is not restricted to neural network architecture alone but can also be
seamlessly integrated with any conventional supervised learning techniques. The
reduced dimensionality within the parameter space, as compared to the function
or observed discrete real-valued space, contributes to a substantial cutback in
computational expenses required.
(iii) Real-world Evaluation of Approximation Performance of the ILP algorithm:
An in-depth comparative study has been conducted to evaluate the approximation
performance of the proposed ILP algorithms against the equidistant knotplacement method under both single and multiple-function approximating
settings. This empirical analysis demonstrates the efficacy of our proposed
algorithms.
(iv) Validation and Benchmarking of Predictive Performance of the M2P Model:
The robustness and effectiveness of our M2P model are assessed through its
employment in handling SOF and FOF regression tasks, leveraging a variety of
real-world examples. By benchmarking against several widely used prediction
models, we provide empirical evidence affirming the superior performance of the
M2P model.
The organization of this paper is as follows: Section 2 outlines the two primary problems of
this study. Section 3 presents the proposed free knot placement algorithm, followed by a
description of the M2P function model in Section 4. Section 5 offers experimental results
for the proposed free knot algorithm in single and multiple-function approximation contents
and presents the prediction performance of the M2P model in both settings of Scalar Output
Prediction and Functional Output Prediction. The final section provides a summary and
proposes future research directions in the field.
2. Problem Formulation
In this section, we review the basics of B-spline theory, introduce the essential notations
used in this work, and subsequently formalize the two core problems addressed in this study.
Let ùëì ‚àà ùê∂([ùëá]) be a function defined over an interval ùëá = [ùëé,ùëè], and for integers ùëõ and ùëù
where ùëõ > ùëù ‚â• 0, consider a knot sequence ùùÉ:
ùëõ+ùëù+1
ùùÉ ‚âî {ùúâ } = {ùúâ ‚â§ ùúâ ‚â§ ‚ãØ ‚â§ ùúâ },ùëõ ‚àà ‚Ñï,ùëù ‚àà ‚Ñï (2)
ùëó 1 2 ùëõ+ùëù+1 0
ùëó=1
A degree ùëù B-spline approximation of the function ùëì can be written as:
ùëõ
ùëìÃÇ(ùë°) ‚âî ‚àë ùêµ (ùë°)ùõΩ , ùë° ‚àà [ùúâ ,ùúâ ] (3)
ùëû,ùëù,ùùÉ ùëû 1 ùëõ+ùëù+1
ùëû=1
where ùëìÃÇ(ùë°) is the fitted function at point ùë°, ùõΩ is the coefficient for the q-th basis function
ùëû
for ùëìÃÇ , and ùêµ is the qth B-spline basis functions of degree ùëù defined over the knot
ùëû,ùëõ,ùùÉsequence ùùÉ. It is assumed that the knot sequence ùùÉ is open with multiplicity ùëù+1 at the
interval [ùëé,ùëè], i.e.
ùëé ‚âî ùúâ = ‚ãØ = ùúâ < ùúâ ‚â§ ‚ãØ ‚â§ ùúâ < ùúâ = ‚ãØ = ùúâ =:ùëè (4)
1 ùëù+1 ùëù+2 ùëõ ùëõ+1 ùëõ+ùëù+1
The B-spline basis function ùêµ (ùë°) is recursively defined as:
ùëû,ùëù,ùùÉ
1, ùëñùëì ùë° ‚àà [ùúâ ,ùúâ )
ùêµ (ùë°) ‚âî { ùëû ùëû+1
ùëû,0,ùùÉ 0, ùëúùë°‚Ñéùëíùëüùë§ùë†ùëñùëí
(5)
ùë°‚àíùúâ ùúâ ‚àíùë°
ùëû ùëû+ùëù+1
ùêµ (ùë°) ‚à∂= ùêµ (ùë°)+ ùêµ (ùë°)
ùëû,ùëù,ùùÉ ùúâ ‚àíùúâ ùëû,ùëù‚àí1,ùùÉ ùúâ ‚àíùúâ ùëû+1,ùëù‚àí1,ùùÉ
ùëû+ùëù ùëû ùëû+ùëù+1 ùëû+1
Based on the recurrence relation (4), a B-spline exhibits the following key properties:
a. Local support. B-spline has local support over ùëù+1 knot spans, i.e.,
ùêµ (ùë°) = 0,ùë° ‚àâ [ùúâ ,ùúâ ) (6)
ùëû,ùëù,ùùÉ ùëû ùëû+ùëù+1
b. Nonnegativity. B-spline is nonnegative everywhere, and positive within its support.
c. Local partition of unity: According to the local Marsden identify (Lyche et.al 2018),
for ùëù+1 ‚â§ ùëö ‚â§ ùëõ,
ùëö
‚àë ùêµ (ùë°) = 1,ùë° ‚àà [ùúâ ,ùúâ ) (7)
ùëû,ùëù,ùùÉ ùëö ùëö+1
ùëû=ùëö‚àíùëù
Similar to classical function approximation theories, B-spline approximation primarily deals
with the single-function approximation problem. We now introduce a novel type of
approximation problem, named the Unified Multifunction Approximation:
Definition: Unified Multifunction Approximation
Given a set of ùëÅ functions ùêπ = {ùëì:ùëì ‚àà ùê∂(ùëá),ùëñ = 1,‚Ä¶,ùëÅ} the objective to identify a
ùëñ ùëñ
common set of ùëõ basis functions {ùúô ,ùúô ,‚Ä¶,ùúô } such that for each ùëì in ùêπ there exists a
1 2 ùëõ ùëñ
set of coefficients {ùõΩ ‚àà ‚Ñù}ùëõ for which the approximation:
ùëñ,ùëû ùëû=1
ùëõ
ùëìÃÇ(ùë°) = ‚àë ùúô (ùë°)ùõΩ (8)
ùëñ ùëû ùëñ,ùëû
ùëû=1
satisfies ‚ÄñùëìÃÇ(ùë°)‚àíùëì(ùë°)‚Äñ < ùúÄ for all ùë° ‚àà ùëá, and for all ùëñ = 1,‚Ä¶,ùëÅ where ùúÄ is some error
ùëñ ùëñtolerance and ‚Äñ‚àô‚Äñ represents a norm defined in a Hilbert space.
In our study, we consider N subjects, each with a continuous input function
ùëã (ùë°) ‚àà ùêø2(ùëá) over a compact interval T, and a corresponding continuous output function
ùëñ
ùëå(ùë†) ‚àà ùêø2(ùëÜ) over a compact interval S where ùêø2 denotes a Hilbert space. In practical
ùëñ
scenarios, the continuous processes of both ùëã(ùë°) and ùëå(ùë†) can be only measured discretely
at points {ùë° } for ùëó = 1‚Ä¶,ùêΩ and {ùë† } for ùëü = 1‚Ä¶,ùëÖ.
ùëó ùëü
Our first problem involves approximating each input function ùëã (ùë°) and output function
ùëñ
ùëå(ùë†) respectively using common sets of B-spline basis functions that satisfy a
ùëñ
predetermined error criterion. Determining separate common sets of B-spline basis
functions for both input and output functions ensures that the essential information from
each function of the corresponding variable is captured in the same coordinate system, thus
facilitating effective learning in subsequent regression models.
Once the knot sequence for the common set of B-spline basis functions is determined, the
coefficients corresponding to each function can be obtained by minimizing the least square
error between the approximated function and the true function, e.g., for the approximation
of ùëã (ùë°):
ùëñ
ùêΩ 2
ùëéùëüùëîùëöùëñùëõ‚àë (ùëãÃÇ (ùë° )‚àíùëã (ùë° )) (9)
ùëñ ùëó ùëñ ùëó
ùú∑ ùëó=1
where ùëã (ùë° ) is the actual observed value of ùëã (ùë°) at time point ùë°
ùëñ ùëó ùëñ ùëó
Hence, the first problem can be divided into two knot optimization challenges: to determine
the optimal common knot placement ùùÉ‚àó and ùùÉ‚àó for the input variable ùëã and the output
ùëã ùëå
variable ùëå , respectively, with the minimal number of knots such that both B-spline
approximation ùëãÃÇ and ùëåÃÇ satisfy the accuracy requirements
ùëñ ùëñ
‚ÄñùëãÃÇ (ùë°)‚àíùëã (ùë°)‚Äñ < ùúÄ(ùëã) , ‚ÄñùëåÃÇ(ùë†)‚àíùëå(ùë†)‚Äñ < ùúÄ(ùëå) (10)
ùëñ ùëñ ùëñ ùëñ
for all ùëñ and for all ùë° and ùë† within the domains of interest.Our second problem is to estimate the unknown nonlinear functional mapping ùëî ‚à∂ ùêø2(ùëá) ‚Üí
ùêø2(ùëÜ) between the input function ùëã (ùë°) and the output function ùëå(ùë†) through supervised
ùëñ ùëñ
learning methods. The optimal functional approximator ùëî is determined by minimizing the
mean squared error (MSE) loss function between the actual output ùëå(ùë†) and the predicted
ùëñ
output ùëåÃÉ(ùë†) across all subjects in an L2 sense:
ùëñ
1 ùëÅ
ùëéùëüùëîùëöùëñùëõ ‚àë ‚à´ (ùëå(ùë†)‚àíùëåÃÉ(ùë†))2ùëëùë† (11)
ùëÅ ùëñ ùëñ
ùíà‚ààùêø2 ùëñ=1 ùë†‚ààùëÜ
where ùëÅ is the total number of subjects and ùëåÃÉ(ùë†) = ùëî(ùëã (ùë°)) is the predicted output
ùëñ ùëñ
function.
3. Iterative Local Knot Placement for B-spline approximation
This section introduces a novel free knot placement algorithm called the Iterative Local
Placement (ILP) algorithm, which is used to determine a common knot distribution to form
a common set of B-spline basis functions which can approximate a set of functions with the
same desired level of approximation accuracy.
We first consider the problem of approximating a single function: let ùëì ‚àà ùê∂(ùëá) be a function
on an interval ùëá = [ùëé,ùëè], the goal is to find a set of degree ùëù B-spline basis function
ùëõ+ùëù+1
{ùêµ }ùëõ defined on a knot sequence ùùÉ = {ùúâ } ‚äÜ ùëá , such that the B-spline
ùëû,ùëù,ùùÉ ùëû=1 ùëó
ùëó=1
approximant ùí¨ùëì of ùëì, given by:
ùëõ
ùí¨ùëì(ùë°) ‚âî ‚àë ùêµ (ùë°)ùõΩ , ùë° ‚àà [ùúâ ,ùúâ ] (12)
ùëû,ùëù,ùùÉ ùëû 1 ùëõ+ùëù+1
ùëû=1
with suitable coefficients ùõΩ , satisfies
ùëû
|ùëì(ùë°)‚àíùí¨ùëì(ùë°)| < ùúÄ (13)
for all ùë° ‚àà ùëá and a given tolerance ùúÄ.
The local support property in (6) of B-splines implies that the value of ùí¨ùëì at a point ùë°
depends only on the values of ùëì in a local neighborhood of that point. Specifically, for ùëù+
1 ‚â§ ùëö ‚â§ ùëõ,ùëõ ùëö
‚àë ùêµ (ùë°)ùõΩ = ‚àë ùêµ (ùë°)ùõΩ , ùë° ‚àà [ùúâ ,ùúâ ] (14)
ùëû,ùëù,ùùÉ ùëû ùëû,ùëù,ùùÉ ùëû ùëö ùëö+1
ùëû=1 ùëû=ùëö‚àíùëù
This property motivates a localized approach to construct a B-spline approximation with the
desired accuracy by controlling the local approximation error over each knot span.
The Lagrange Error Theorem (Shadrin 1995) gives an upper bound for Tayler polynomial
approximation error. An extension of this theorem to B-spline spaces is provided in Theorem
32 of Lyche et.al (2018), providing a local error bound for the B-spline approximation over
a knot span. Based on these theorems, we hereby present a lemma on a local and a global
upper bound of B-spline approximation error in a ùêø norm.
‚àû
Lemma 1: Let ùëì ‚àà ùê∂ùëù+1(ùëá) be a function that is ùëù+1 times continuously differentiable on
a close interval T and let ùí¨ùëì be a degree ùëù B-spline approximation to ùëì defined over the
ùëõ+ùëù+1
knot sequence ùùÉ = {ùúâ } ‚äÜ ùëá. For any ùëö ‚àà [ùëù+1,ùëõ] the local error bound for ùí¨ùëì in
ùëó
ùëó=1
the ùêø -norm over the knot span [ùúâ ,ùúâ ] is given by
‚àû ùëö ùëö+1
ùëù+1
‚Ñé
‚Äñùëì‚àíùí¨ùëì‚Äñ = ùë†ùë¢ùëù |ùëì(ùë°)‚àíùí¨ùëì(ùë°)| ‚â§ ùëö ‚Äñùê∑ùëù+1ùëì‚Äñ (15)
ùêø‚àû([ùúâùëö,ùúâùëö+1])
ùëù!
ùêø‚àû([ùúâùëö,ùúâùëö+1])
ùë°‚àà[ùúâùëö,ùúâùëö+1]
where ‚Ñé = ùúâ ‚àíùúâ is the length of the m-th knot interval and ‚Äñùê∑ùëù+1ùëì‚Äñ ‚âî
ùëö ùëö+1 ùëö ùêø‚àû([ùúâùëö,ùúâùëö+1])
ùëöùëéùë• |ùëì(ùëù+1)(ùë°)| with ùëì(ùëù+1)(ùë°) denoting the (ùëù+1)-th derivative of ùëì evaluated at t.
ùúâùëö‚â§ùë°‚â§ùúâùëö+1
If there is a constant ùúÄ > 0 such that for all ùëö ‚àà [ùëù+1,ùëõ]:
ùëù+1
‚Ñé
ùëö ‚Äñùê∑ùëù+1ùëì‚Äñ < ùúÄ (16)
ùëù!
ùêø‚àû([ùúâùëö,ùúâùëö+1])
then the ùêø global error of the B-spline approximation ùí¨ùëì to ùëì is bounded by ùúÄ i.e.
‚àû
‚Äñùëì‚àíùí¨ùëì‚Äñ < ùúÄ (17)
ùêø‚àû(ùëá)
Proof of Lemma 1 is provided in Section 1.1 of the supplementary document.
We now extend the single function approximation problem to the unified multifunctions
approximation problem: given a set of ùëÅ functions ùêπ = {ùëì(ùë°):ùëì ‚àà ùê∂(ùëá),ùëñ = 1,‚Ä¶,ùëÅ} over
ùëñ ùëñthe same interval ùëá = [ùëé,ùëè], the goal is to find a common set of degree ùëù B-spline basis
ùëõ+ùëù+1
function {ùêµ‚àó }ùëõ defined on a knot sequence ùùÉ‚àó = {ùúâ‚àó} ‚äÜ ùëá , whose linear
ùëû,ùëù,ùùÉ ùëû=1 ùëó
ùëó=1
expansion approximant {ùí¨ùëì}ùëÅ for each respective function ùëì such that
ùëñ ùëñ
|ùëì(ùë°)‚àíùí¨ùëì(ùë°)| < ùúÄ (18)
ùëñ ùëñ
for all ùë° ‚àà ùëá and all ùëñ.
To solve this new function approximation problem, we adopt a strategy considering the ‚Äúworst-
case scenario‚Äù. Specifically, if a set of B-spline basis functions can accommodate the function
with the highest degree of complexity among all given functions, it can also handle each simpler
individual case within the same error bound. Here, the complexity of a function is quantified by
the magnitude of its (p + 1)-derivative, since the derivative signifies the rate of change of a
function. A higher derivative value at a specific point ùë° ‚àà ùëá corresponds to higher variability and
complexity of the function at that location. Consequently, we characterize the most complex
dynamics of a given set of functions by considering the maximum value of the ( p+ 1)-derivative
across all given functions at each point ùë° ‚àà ùëá. Under this setting, We apply Lemmas 1 to the
ùëó
case of the unified multifunction approximations problem, and we have the following
theorem:
Theorem 1:
Let {ùëì ‚àà ùê∂ùëõ+1(ùëá)}ùëÅ be a set of N functions defined over the same interval ùëá. And let
ùëñ ùëñ=1
{ùêµ‚àó }ùëõ be a set of degree ùëù B-spline basis functions defined upon a knot sequence ùùÉ‚àó =
ùëû,ùëù,ùùÉ ùëû=1
ùëõ+ùëù+1
{ùúâ‚àó} ‚äÜ ùëá. Supposed that for each ùë° ‚àà ùëá the (p+1)-th derivatives of the function ùëì are
ùëó ùëñ
ùëó=1
uniformly bounded, i.e. there exists a constant ùê∂ such that
ùë°
ùëöùëéùë•
|ùëì(ùëù+1)
(ùë°)| ‚â§ ùê∂ , ‚àÄùë° ‚àà ùëá (19)
ùëñ ùë°
ùëñ‚àà[1,ùëÅ]
If there exists a constant ùúÄ > 0 such that for all ùëö ‚àà [ùëù+1,ùëõ]
ùëöùëéùë• |ùê∂ |
ùë°
ùúâ ùëö‚àó ‚â§ùë°‚â§ùúâ ùëö‚àó +1 ‚Ñé‚àó ùëù+1 < ùúÄ (20)
ùëù! ùëöwhere ‚Ñé‚àó ùëù+1 = ùúâ‚àó ‚àíùúâ‚àó is the length of the m-th knot interval
ùëö ùëö+1 ùëö
then each function ùëì is approximated by the B-spline basis function set {ùêµ‚àó }ùëõ such that
ùëñ ùëû,ùëù,ùùÉ ùëû=1
the global approximation error in the ùêø norm for each i satisfies:
‚àû
‚Äñùëì ‚àíùí¨ùëì‚Äñ < ùúÄ (21)
ùëñ ùëñ ùêø‚àû(ùëá)
where ùí¨ùëì denotes the B-spline approximant to ùëì
ùëñ ùëñ
Proof of Theorem 1 is provided in Section 1.2 of the supplementary document.
Theorem 1 asserts that if one chooses a set of B-spline basis functions with an appropriate
knot sequence such that the local approximation errors for all ùëì on each span of knots are
ùëñ
bounded by Œµ in the ùêø norm, then it follows that the global ùêø errors for all these
‚àû ‚àû
approximations are also bounded by ùúÄ . Such insight serves as a guiding principle when
applying B-splines for unified multifunction approximation in practice. By trying to regulate the
local approximation error during the knot selection process, the theorem guarantees that the
desired global approximation error is achieved for all functions.
The computation of derivatives from discretely observed data of an unknown function
demands careful consideration due to the potential amplification of noise. Although this is
not our principal area of interest, several solutions have been put forth to mitigate this
challenge (Breugel, Kutz and Brunton, 2020). For our present objectives, we opt to employ
the Forward Finite Difference Approximation method. Specifically, the derivative of a
function ùëìat the point ùë° is computed as follows:
ùëì(ùë°+‚àÜùë°)‚àíùëì(ùë°)
ùëì‚Ä≤(ùë°) = lim (22)
‚àÜùë°‚Üí0 ‚àÜùë°
Higher-order derivatives can be computed analogously.
Based on the preceding theoretical analysis and groundwork, we now present the core
procedures of the ILP algorithm for unified multifunction approximations. This concise
representation is summarized in Algorithm 1.1. Calculate the (p+1)-th derivatives for all function ùëì(ùë°) at each ùë° in ùëá
ùëñ
2. Determine the maximum value of these (p+1)-th derivatives across all given functions
at each ùë°.
3. Begin with no knot in the knot sequence.
4. Find the longest knot span, from left to right, where the local approximation error
over such knot span meets the bound specified in Equation (20).
5. Position the knots at the boundaries of the identified subinterval.
6. Repeat the iterative process until the last subinterval satisfies the specific criterion.
Algorithm: The Iterative Local Placement Algorithm
Input: A set of N sequences of discrete observation ùëø ‚àà ‚ÑùùêΩ, corresponds to the
ùëñ
unknown functions ùëì at discrete timesteps ùë° where ùëé = ùë° < ùë° < ‚ãØ < ùë° = ùëè and ùëñ =
ùëñ ùëó 1 2 ùêΩ
1,‚Ä¶,ùëÅ, the degree of B-spline ùëù and error tolerance ùúÄ
Output: Interior knot sequence ùùÉ‚àó = {ùúâ‚àó,‚Ä¶}
1
Initialization of variables: assign 1 to variable ùëö, a to variable ùúâ‚àó
0
1 For each j=1 ‚Ä¶ J and for each i=1 ‚Ä¶ N compute ùëì(ùëù+1) (ùë° )
ùëñ ùëó
(ùëõ+1)
For each ùë° compute ùê∂ = ùëöùëéùë•|ùëì (ùë° )|
ùëó ùë° ùëó ùëñ‚àà[1,ùëÅ] ùëñ ùëó
2 While ùëö < ùêΩ do the following
3 for ùë° in [ùúâ‚àó ,ùëè]:
ùëû ùëö‚àí1
a.
Compute ùëÄ ‚Üê ùëöùëéùë• |ùê∂ |;
ùëöùëéùë• ùë°
ùúâ ùëö‚àó ‚àí1‚â§ùë° ùëó‚â§ùë°ùëû ùëó
b. set ùõø ‚Üê ùëÄùëöùëéùë•|ùë° ‚àíùúâ‚àó |ùëù+1
ùëû ùëö‚àí1
ùëù!
c. If ùõø > ùúÄ , then break.
end for
4 ùë° ‚Üê ùëò‚àó & m‚Üê ùëö+1
ùëû ùúâ
5 if ùë° = ùëè, then break.
ùëû
end while
return ùùÉ‚àó = {ùúâ‚àó,‚Ä¶}
1
Figure 1 illustrates an example where a real-world solar irradiance data sequence, collected
over a single day at minute intervals, is being approximated by a set of linear B-spline basis
functions, with the knot locations determined via the equidistant knot placement method
(top) and the ILP algorithm (bottom). The same number of knots is used in both cases. For
the equidistant knot strategy, underfitting is observed in areas of fluctuation since there arenot enough knots allocated in the area to capture the complexity, while too many knots are
located in the smooth region and cause overfitting. The ILP_pw algorithm addresses these
issues by increasing the density of knots in higher fluctuating areas, and conversely
decreasing knot density in smoother regions.
Figure 1. Demonstration of ILP and the equidistant knot placement algorithm. The
approximated curve is red. The knots used are indicated by the blue vertical dashed lines.
4. Mapping-to-Parameter Function-on-Function Regression Model
In this section, we introduce our proposed model framework for a nonlinear FOF regression
model, namely the Mapping-to-Parameter (M2P) functional regression model. The main
idea of the M2P functional model is to map functional data from an infinite-dimensional
function space into a finite-dimensional parameter space and learn the regression model in
the parameter space using any usual supervised learning techniques. This framework was
initially introduced in a previous work (Shi and Zeng, 2022) but was limited to the SOF
model. Figure 3 presents the overall architecture of the M2P model.
The main procedures of the M2P functional model are as follows:Figure 2. The schematic diagram of the proposed M2P model
i. Mapping from observed discrete real-valued space into function space (i.e.,
functional data representation)
In practice, for N pairs of functional data (ùëã (ùë°),ùëå(ùë†)) , their observations are usually
ùëñ ùëñ
ùëñ=1,‚Ä¶,ùëÅ
only available at discrete time points {ùë° }ùêΩ and {ùë† }ùëÖ , represented as (ùëø ‚àà ‚ÑùùêΩ,ùíÄ ‚àà
ùëó ùëó=1 ùëü ùëü=1 ùëñ ùëñ
‚ÑùùëÖ) . The initial procedure is to approximate each ùëø and ùíÄ into B-spline
ùëñ=1,‚Ä¶,ùëÅ ùëñ ùëñ
approximation, ùëãÃÇ (ùë°) and ùëåÃÇ(ùë°) in the form of Equation (3) based on the common knot
ùëñ ùëñ
distributions ùùÉ‚àó and ùùÉ‚àó for input and output, respectively. The ILP algorithm is employed to
ùëø ùíö
obtain the functional data representation with the optimal knot distribution.
ii. Mapping from functional space into parameter space
To address the infinite-dimensional challenge, the fitted functions ùëãÃÇ (ùë°) and ùëåÃÇ(ùë°) are
ùëñ ùëñ
represented in parameter space using the vector of their coefficients ùú∑(ùëã) ‚àà ‚Ñùùëõùë• and ùú∑(ùëå) ‚àà
ùëñ ùëñ‚Ñù ùëõùë¶, respectively. The dimension of these coefficients corresponds to the number of basis
functions used for the approximation, which is equal to the number of interior knots used in
addition to the order of the fitted B-spline. Hence, for any input function ùëã , the dimension
ùëñ
of the coefficient vectors is identical for all ùëñ, owing to the use of the same set of B-spline
basis functions for the approximation. This is also true for any output function ùëå . The
ùëñ
coefficient values are calculated by minimizing the L norm of the fitting error, as presented
2
in Equation (9).
iii. Learning the prediction model in the parameter space and mapping the result back
to the function space
A functional regression problem is subsequently reduced into a standard multiple
multivariate regression problem with an input dimension ùëõ and an output dimension ùëõ .
ùë• ùë¶
(ùëã) (ùëå)
The prediction model is trained based on (ùú∑ ,ùú∑ ) using any conventional
ùëñ ùëñ ùëñ=1,‚Ä¶,ùëÅ
supervisor learning algorithm. For the newly arrived data ùëø = {ùëã ,...,ùëã } it
ùëÅ+1 ùëÅ+1,1 ùëÅ+1,ùêΩ
needs to be represented to its functional form using the identical set of basis functions for
input functions with the same knot distribution ùùÉ‚àó , and then obtains its corresponding
ùëø
(ùëã)
coefficient into ùú∑ . This allows the data to be input into the trained model for the
ùëÅ+1
prediction of ùú∑(ùëå) . Finally, the targeted curve ùëåÃÇ (ùë°) can be determined in the form of
ùëÅ+1 ùëÅ+1
Equation (3) using the same set of basis functions for output functions with the same knot
distribution ùùÉ‚àó.
ùíö
5. Experiment Results:
This section contains two main subsections: the first subsection evaluates the approximation
performance for our proposed ILP algorithm in both contexts of single function
approximation and unified multifunction approximation, compared to the standard
equidistant knot placement strategy; the second subsection demonstrates the predictive
performance of our proposed M2P functional regression model compared with a wide rangeof state-of-the-art techniques in handling several different real-world nonlinear SOF and
FOF regression problems.
5.1. Function Approximating
In this subsection, we concentrate on a solar irradiance dataset for the year 2016, sourced
from the National Renewable Energy Laboratory Solar Radiation Database (NSRDB)
(Andreas and Stoffel, 1981). The dataset is selected for the true solar time span from 8:00
and 16:00 and is normalized by a linear scaling method to lie within the boundaries of
[‚àí1,1]. Our investigation includes two approximation tasks using this dataset. The first task,
single function approximation, seeks to estimate the solar irradiance curve for a particular
day from its one-minute discrete observations. The second task, the unified multifunction
approximation task, aims to identify a common set of B-spline basis functions that can
effectively approximate the solar irradiance curve for all 366 days in 2016.
For each task, we evaluate the performance of our proposed ILP algorithm by comparing it
to the equally spaced knot method. The approximation error between the true and
approximated function is quantified using both Maximum Absolute Error (MaxAE) and
Root Mean Squared Error (RMSE). The MaxAE computes the largest absolute discrepancy
between the true and approximated functions:
ùëÄùëéùë•ùê¥ùê∏ = max (|ùëì(ùë° )‚àíùëìÃÇ(ùë° )|) (23)
ùëó ùëó
ùëó=1,‚Ä¶,ùêΩ
The RMSE, in contrast, calculates the square root of the mean of the squared difference
between the true and approximated function values at all discrete time points across the
entire domain:
ùêΩ
1 2
ùëÖùëÄùëÜùê∏ = ‚àö ‚àë(ùëì(ùë° )‚àí ùëìÃÇ(ùë° )) (24)
ùêΩ ùëó ùëó
ùëó=1
In the above, ùëì(ùë° ) and ùëìÃÇ(ùë° ) represent the actual and predicted values at ùë° , |‚àô| denotes the
ùëó ùëó ùëó
absolute value and ùêΩ is the total number of observed function values within the domain.5.1.1. Single Function Approximation
We conduct the single function approximation task separately for one representative day
from each season of 2016, specifically January 1 (winter), March 20 (spring), June 21
(summer), and September 22 (fall). These representative days are selected using a standard
K-means algorithm with four clusters, and their respective variation patterns are
distinctively colored in Figure 4.
Figure 3. Daily Solar Pattern for four representative days in 2016
For a robust performance evaluation of our proposed ILP algorithm against the equally
spaced knot method, we employ two comparison metrics. First, we contrast the
approximation accuracies in terms of RMSE and MaxAE, while using an equal number of
knots. Secondly, we compare the number of knots necessary to attain the same level of error.
For the latter, we identify the optimal number of knots needed for each knot placement
method, maintaining an identical level of Generalized Cross-Validation (GCV) Error level,
a well-recognized criterion in function approximation that balances the trade-off between
goodness-of-fit and model complexity.
The GCV error commonly provides an unbiased estimate of the prediction error by
iteratively omitting one observation from the dataset, fitting the model to the remaining data,
and then computing the prediction error for the omitted observation. The GCV Error is given
by:ùêΩ
1 2
ùê∫ùê∂ùëâ = ‚àë(ùëì(ùë° )‚àí ùëìÃÇ (ùë° )) (25)
ùêΩ ùëó ‚àíùëó ùëó
ùëó=1
where ùëì(ùë° ) denote the actual function values at ùë° , and ùëìÃÇ indicates the fit using all training
ùëó ùëó ‚àíùëó
sampling except for the j-th pair (ùë° , ùëì).
ùëó ùëó
Often, the GCV expression is simplified based on the smoothing matrix ùë∫, represented as:
1 ùêΩ ùëì(ùë° )‚àí ùëìÃÇ(ùë° ) 2
ùëó ùëó
ùê∫ùê∂ùëâ = ‚àë[ ] (26)
ùêΩ 1‚àíùë∫
ùëñùëñ
ùëó=1
In B-spline modeling, the smoothing matrix ùëÜ ‚àà ‚Ñùùë±√óùë± operates as a projection mapping the
observed data to the space spanned by the B-spline basis functions ùùì. It is defined as ùë∫ =
ùùì(ùùìùëáùùì)‚àí1ùùìùëá with ùë∫ referring to the i-th diagonal element of matrix ùë∫. Here, ùùì ‚àà ‚Ñùùíé√óùë±
ùëñùëñ
signifies a matrix with each column corresponding to a B-spline basis function evaluated at
all observation points, with ùëö as the total number of basis functions for ùëìÃÇ(ùë°).
The selection of the order of the B-spline basis functions plays a pivotal role in B-spline
approximation, significantly influencing the accuracy of the resulting approximation. This
choice should be guided by the inherent complexity of the target function being
approximated. While higher-order B-splines can capture more intricate and wiggly patterns
in data, they risk amplifying oscillations and incurring overfitting, especially when
approximating smoother functions. Conversely, lower-order B-spline might inadequately
represent sophisticated functional features, potentially resulting in underfitting. Given the
specific complexity of the underlying curve for each representative day, we designate the
constant B-spline (Order 1) to the Spring curve, the linear B-spline (Order 2) to the Winter
curve, and the quadratic B-spline (Order 3) to the Summer and Fall curve.
Figure 4 synthesizes the experimental findings for each representative day, each including
four comparison criteria: (a) an evaluation of the knot counts required to meet the same
GCV error bound between our introduced ILP algorithms and the equidistant knot method,
(b) a comparison of the MaxAE error produced by both algorithms with the identical numberof knots, (c) a comparison of the RMSE error produced by these two algorithms using the
same number of knots, (d) the resultant fitted curve using the optimal knot distribution.
From these curve approximations, it is consistently observed that our proposed ILP
algorithm demands fewer knots to achieve the same GCV error bound as opposed to the
equidistant knot method. This superiority becomes particularly pronounced for lower GCV
error bound. To illustrate, in Plot (a) for Day_0621 (Summer Day), the minimal GCV error
achievable by the equal knot method is 1.92E-09, with 136 knots. This method exhibits
challenges in improving upon this accuracy threshold, with any additional knots potentially
deteriorating results, likely due to overfitting. In contrast, our ILP algorithm is not
constrained by this limitation, managing to achieve a GCV well below 1.92E-09 with fewer
knots. For instance, a GCV error of 1.32E-09 was obtained using only 85 knots by the ILP
algorithm. Furthermore, from each Plot (b) and (c), it is obvious that the ILP algorithm
consistently outperforms in terms of MaxAE and RMSE error with the same number of
knots used compared to the equidistant knot methods.
The search for an optimal knot distribution necessitates a careful balance between the model
fit and its inherent complexity. To illustrate, for Day_0101(Winter Day), the ILP algorithm
attains the minimal MaxAE error of 0.0015 with 83 knots and an RMSE error of 0.00013
using 105 knots. Interestingly, a considerable reduction to merely 27 knots only results in a
marginal rise in the MaxAE error to 0.0037 and the RMSE error to 0.00041. This
underscores the necessity to trade off approximation precision against the number of basis
functions employed. The optimal knot distributions for each representative day are
showcased in each Plot (d) of Figure 4. Specifically, Day_0101(Winter Day) employs 58
linear B-spline basis functions, Day_0320(Spring Day) uses 31 constant B-spline basis
functions, Day_0621(Summer Day) adopts 51 quadratic B-spline basis functions, and
Day_0922 (Fall Day) utilizes 74 quadratic B-spline basis functions.Figure 4. Methods comparison and fitted curve for Four Representative Days: Winter
0101, Spring 0320, Summer 0621, and Fall 0922, respectively.
5.1.2. Unified Multifunction Approximation
The superior performance of our proposed free knot placement algorithm has been
demonstrated in the case of single function approximation, compared to the equidistant knot
placement method. However, the task of the unified multifunction approximation introduces
greater complications. This is primarily because a common set of B-spline basis functions
is used to approximate a set of functions, each of which could potentially exhibit diverse
and randomly varying patterns. To assess the efficacy of our proposed method in addressing
the unified multifunction approximation problem against the equidistant knot method, we
calculate the mean value of MaxAE and RMSE errors across all 366 daily curve
approximations, denoted as Mean_MaxAE and Mean_RMSE. Both metrics can serve as the
indicators of the overall performance for all day-curve approximations throughout 2016
In our initial analysis, we compare the mean MaxAE and RMSE errors yielded by our
proposed ILP algorithm with those of the equidistant knot method when using a consistent
number of knots. The comparative results are illustrated in Figure 5. Specifically, Plots (b)
and (d) of Figure 5 provide a detailed assessment of mean approximation errors for varying
B-spline orders, ranging from Order 1 to Order 4. Our findings reveal that our algorithmsconsistently outperform the standard equal knot methods in overall approximation
performance in terms of both RMSE and MaxAE, regardless of the use of any order of B-
spline. An integrated visualization encompassing all B-spline orders is showcased in Plot (a)
and (c), with a notable superior performance of the order 3 and 4 B-spline.
Figure 5: Comparison of mean error using ILP algorithm and Equal knot algorithm for
Order 1-4 B-spline.
An alternative metric, the maximal error among all daily function approximations, serves to
illustrate the worst approximation scenario. Figure 6 provides a comparison of the
Max_RMSE achieved by ILP algorithms and the equal knot method using the same number
of knots. The observations from Plot (b) of Figure 6 align with those drawn from the mean
error analysis: our algorithms consistently exhibit a lower Max_RMSE in comparison to theconventional equal knot techniques given an identical number of knots for all orders.
Moreover, as depicted in Plot (a), B-spline basis functions of orders 2, 3, and 4 manifest
analogous performance in terms of Max_RMSE.
Figure 6: Comparison of Max error using ILP algorithm and Equal knot algorithm for
Order 1-4 B-spline.
Figure 7: Comparison of RMSE and MaxAE error of each daily curve approximation
using the proposed algorithm and Equal method.
Subsequently, the order 3 B-spline basis functions with 89 knots were chosen to approximate
all daily functions. As depicted in Figure 7, the ensuing RMSE and MaxAE errors for eachdaily function approximation based upon such optimal common knot distribution, are
markedly lower than their counterparts obtained using the equidistant knot method.
5.2. Functional Regression
In this subsection, we explore the practical application of the proposed M2P functional
model across two prediction paradigms: Scalar Output Prediction and Functional Output
prediction. Four different experiments were conducted for each paradigm using distinct real-
world datasets frequently employed in FDA research. The data for each experiment was
partitioned into 75% for training and 25% for testing. We offer a comprehensive comparison
between the proposed M2P functional model and three categories of prevailing techniques:
(i) Standard nonfunctional NN models, including the multilayer perceptron (MLP),
convolutional neuron network (CNN), long-short-term recurrent (LSTM), and the
bidirectional variant of LSTM (Bi-LSTM) (Graves and Schmidhuber 2005).
(ii) Statistical functional regression models, including the functional linear model
(FLR), functional principal component (FPC) regression and its variant with roughness
penalization (Yao and M√ºller 2005), functional partial least square (FPLS) regression
and its variant with roughness penalization (Preda, Saporta, and L√©v√©der 2007), and a
nonparametric functional model (Ferraty and Vieu 2003).
(iii) NN-based functional model, including the FuncNN model, AdaFNN model
mentioned in Section 1, and Functional Basis Neural Network (FBNN) (Rao and
Reimher 2023)
For comparison, the conventional MLP architecture was integrated into the M2P model
framework, and the tunning of its hyperparameters in the network mirrored the strategy
employed for other NN-related models. Except for the AdaFNN model, all functional
models necessitate an initial preprocessing to represent discrete measurements in functional
data form. An FPC basis function expansion was utilized in comparative functional models
for the functional data representation, requiring a priori determination of the optimal numberof basis functions. The list of hyperparameters and tuning settings for all methods can be
found in Section 3.1 of the supplemental materials. The final model configuration of the
neural network and the optimal knot distributions for the functional input or output in the
M2P model are provided in Section 3.2 in the supplementary materials. Noteworthy is that
some functional methods, such as AdaFNN, are designed only for univariate regression,
hence we adopted a single functional variable for consistency. However, the flexibility of
our methodology extends its applicability to multivariate contexts as well.
5.2.1. Scalar Output Prediction
The main objective of scalar output prediction is to predict the scalar output variable ùëå using
ùëñ
the input sequence (ùëã ,ùëã ,‚Ä¶,ùëã ). When using functional models, the input sequences
ùëñ,1 ùëñ,2 ùëñ,ùêΩ
were transformed in advance into a functional representation denoted as ùëã (ùë°). Alternatively,
ùëñ
with nonfunctional models, the sequences were combined into a vector input, denoted as
ùëø = [ùëã ,ùëã ,‚Ä¶,ùëã ] . The performance of these models was assessed via the mean
ùëñ ùëñ,1 ùëñ,2 ùëñ,ùêΩ
squared prediction error (MSPE) as the benchmark criterion:
1 ùëÅùë°ùëíùë†ùë°
2
ùëÄùëÜùëÉùê∏ = ‚àë (ùëåÃÇ ‚àíùëå) (27)
ùëÅ ùëñ ùëñ
ùë°ùëíùë†ùë° ùëñ=1
where ùëåÃÇ and ùëå represent the predicted and actual value of i-th observation of output
ùëñ ùëñ
variable ùëå, and ùëÅ indicates the count of subjects within the testing set.
ùë°ùëíùë†ùë°
Below is a brief description of the four target datasets and their corresponding experiments:
a) Tecator data (Thodbery 1996) contains near-infrared (NIR) spectra and chemical
composition measurements of 215 pork samples. The goal of this experiment was to
predict the percentage of the fat content of pork using the corresponding NIR spectra
measured at 100 wavelengths ranging from 850 to 1050 nm. Moreover, the percentage
of water content was used as a scalar predictor for additional information.
b) Gasoline data (Kalivas 1997) comprises NIR spectra and octane number of 60
gasoline samples. This experiment aimed to predict the octane number based on thecorresponding NIR spectra collected at 400 wavelengths between 900 to 1700 nm.
c) Bike Sharing data includes daily bike retail count and 24 hourly temperature values
for 102 Saturdays. The objective of the experiment was to predict the square root of the
daily bike retail count based on hourly temperatures for a specific day. For more details
of the data, see Fanaee-T and Gama 2014.
d) Diffusion Tensor Imaging (DTI) data consists of fractional anisotropy (FA)
collected at 93 locations along the corpus callous (cca) at 382 visits. The goal of the
experiment was to predict the Paced Auditory Serial Addition Test (pasat) score at some
visits based on the corresponding FA measurements. We removed 38 visits with missing
values. Further details of the data can be seen in Goldsmith et al. (2011).
Table 1. The MSPE errors of the proposed M2P model and all comparison models for four
scalar output prediction experiments
Tecator Gasoline Bike Sharing DTI
MLP 6.70 0.29 3.54 0.86
Non-
CNN 6.27 0.37 2.89 0.89
Functional
LSTM 4.39 0.82 2.52 0.99
Models
Bi-LSTM 4.50 0.89 2.74 1.00
FLR 7.20 1.28 3.37 0.85
FPC regression 6.50 0.78 3.23 0.87
FPC regression with 2nd Statistical
6.50 0.80 4.93 0.84
penalization
Functional
FPLS regression 7.38 0.91 3.37 0.95
Models
FPLS regression with 2nd
6.93 0.96 3.07 0.87
penalization
Nonparametric regression 6.87 0.88 4.95 1.04
FuncNN 3.12 0.09 3.45 0.77 NN-based
Functional
AdaFNN N/A 0.77 3.16 0.82
models
Proposed
M2P-MLP 2.80 0.04 1.71 0.72
methodTable 1 presents the final results for all prediction models. As reflected in Table 1, the
proposed M2P grounded in the standard MLP architecture outperformed all other models
across all four experiments. The NN-based functional model showcased superiority over the
statistical function model, possibly due to the power of the neuron network in managing
non-linear input-output relationships. Both nonfunctional and statistical functional models
demonstrated comparable efficacy throughout all scalar output prediction experiments. Note
that in the Tecator experiment, an ‚ÄúNA‚Äù result arose due to the limitation of AdaFNN to the
univariate scenarios.
5.2.2. Functional Output Prediction
The primary objective of functional output prediction is to predict the functional output
ùëåÃÇ(ùë†) based on a given input sequence (ùëã ,ùëã ,‚Ä¶,ùëã ) . Input preprocessing for both
ùëñ ùëñ,1 ùëñ,2 ùëñ,ùêΩ
functional and non-functional models follows the procedures detailed in the scalar output
experiments. Importantly, both SOF functional regression models and non-functional
models produce only scalar outputs. To enable a direct comparison across diverse model
outputs, the MSPE errors (in Equation 17) were calculated for both SOF functional
regression and non-functional models, whose scalar outputs are the mean values of the
output sequences. The performance of the FOF functional regression models was evaluated
through the Mean Square Prediction Error of the Mean (ùëÄùëÜùëÉùê∏ ), expressed as
ùëÄ
ùëÄùëÜùëÉùê∏ =
1 ‚àëùëÅùë°ùëíùë†ùë°
(ùëåÃÖÃÇ
‚àíùëåÃÖ)2
(28)
ùëÄ ùëÅ ùëñ ùëñ
ùë°ùëíùë†ùë° ùëñ=1
where ùëåÃÖÃÇ = 1 ‚àëùëÖ ùëåÃÇ(ùë† ) is the mean of the discretization for the i-th predicted function
ùëñ ùëü=1 ùëñ ùëñ,ùëü
ùëÖ
ùëåÃÇ(ùë†) at time steps {ùë† }ùëÖ , and ùëåÃÖ = 1 ‚àëùëÖ ùëå is the mean of the i-th actual output
ùëñ ùëñ,ùëü ùëü=1 ùëñ ùëü=1 ùëñ,ùëü
ùëÖ
sequence (ùëå ,ùëå ,‚Ä¶,ùëå ).
ùëñ,1 ùëñ,2 ùëñ,ùëÖ
Additionally, to comprehensively evaluate the predictive power of the proposed M2P model
in the functional output space, another error metric called the Mean Square Prediction Error
of the entire function (ùëÄùëÜùëÉùê∏ ) was employed. This metric quantifies the overall
ùêπ
discrepancy between the true function and the predicted function, considering theirdifferences at every point in the domain:
1 ùëÅùë°ùëíùë†ùë° ùëÖ
2
ùëÄùëÜùëÉùê∏ = ‚àë ‚àë (ùëåÃÇ ‚àíùëå ) (29)
ùêπ ùëÖ‚àóùëÅ ùëñ,ùë† ùëñ,ùë†
ùë°ùëíùë†ùë° ùëñ=1 ùë†=1
A concise overview of target datasets and their corresponding experiments is detailed below:
a) Gait data (Ramsay and Silverman 2005) contains hip and knee angles in degrees
through a 20-point movement cycle for 39 boys. In this case, we aim to predict a boy‚Äôs
a 20-point gait cycle for his knee angle based on the corresponding cycle for his hip
angle.
b) Daily data (Ramsay, Hooker and Graves 2009) comprises 365 daily temperature
and precipitation values for 35 Canadian cities. In this case, we aim to predict the annual
precipitation curve in a specific city based on its annual temperature curve.
c) Electricity data (UK Power Networks, 2015) includes the hourly electricity
consumption of 5015 individual households over two weeks. In this case, we aim to
predict a household‚Äôs weekly electricity consumption curve for the second week based
on their first week‚Äôs electricity curve. Each weekly curve consists of 336 data points.
d) Traffic data, collected from Caltrans performance measurement system (PeMS)
consists of 5-minute time resolution traffic occupancy rates on 727 non-holiday
workdays. In this case, we aim to predict the occupancy trajectories over T=[10:00,
24:00] using the corresponding trajectories within S=[0:00,10:00]. There are 120
observations in period ùëá and 168 measurements over ùëÜ.
Table 2 presents the ùëÄùëÜùëÉùê∏ values of the FOF functional models with the MSPE errors of
ùëÄ
the SOF functional and non-functional regression models across the four experiments. The
considered FOF models include the linear functional response model (FLR-FOF), the FBNN
model, and our proposed M2P model. All other functional models under examination belong
to the SOF category. Observations from Table 2 align with previous Scalar Output Prediction
experiments, underscoring that the M2P-MLP model outperforms other comparison modelsacross all four datasets. It is anticipated that the FOF models would yield superior outcomes
over their SOF counterparts, with models such as FLR-SOF and FLR-FOF as illustrative
cases. This is because averaging the output sequence in the SOF models is equivalent to
approximating its underlying output function using a consistent B-Spline without any
internal knot. Such an approximation inevitably results in substantial information loss and
consequently increases prediction errors. Lastly, Figure 8 provides a visual comparison
between the ùëÄùëÜùëÉùê∏ values of our proposed M2P model and the FBNN model across the
ùêπ
four experiments. The consistently lower the ùëÄùëÜùëÉùê∏ values achieved by the M2P model
ùêπ
across all experiments distinctly underline its superior predictive capability not only in
estimating the mean value of the function but also the entire function interval.
Table 2. The MSPE-M (or MSPE) errors of the proposed M2P model and all comparison
models for four functional output prediction experiments
Gait Daily Electricity Traffic
MLP 15.85 1.64 0.0034 3.69
CNN 12.68 3.26 0.0035 4.41 Non-
Functional
LSTM 9.92 1.73 0.0033 3.42 Models
Bi-LSTM 9.78 1.59 0.0031 3.21
FLR-SOF 10.43 2.21 0.02016 3.66
FLR- FOF 9.87 1.82 0.02020 3.65
FPC regression 10.33 1.08 0.0201 3.73
Statistical
FPC regression with 2nd
10.24 1.21 0.0203 4.78 Functional
penalization
Models
FPLS regression 9.78 1.56 0.0216 3.56
FPLS regression with 2nd
9.70 1.57 0.0212 3.63
penalization
Nonparametric regression 10.11 1.34 0.0271 4.35
NN-based
FuncNN 9.53 1.07 0.00311 2.90
Functional
AdaFNN 10.98 0.95 0.00302 2.43 models
FBNN (FOF) 7.64 1.06 0.00305 2.61
Proposed
M2P-MLP (FOF) 7.12 0.73 0.00276 2.22
methodFigure 8. A comparison between the ùëÄùëÜùëÉùê∏ values of the proposed M2P(F2F) model and
ùêπ
the FBNN model across the four experiments
6. Conclusions and Discussion
This paper introduces a novel approach to nonlinear functional regression by developing the
Mapping-to-Parameter (M2P) function model, complemented by the incorporation of free
knot B-spline approximation techniques. A new function approximation challenge termed
the unified multifunction approximation has been formally defined and tackled. This has
been accomplished through the introduction of a novel free knot algorithm devised for the
B-spline approximation of multiple functions. The newly proposed iterative local placement
approach has demonstrated superior performance over the traditional equidistant knot
placement strategy in both tasks of approximating a single daily solar irradiance curve and
approximating 366 daily curves.
The proposed M2P model offers a simpler, flexible, and effective alternative to both existing
statistical FDA regression models and prevailing neural network-based functional
regression models. It introduces a systematical approach to deciding the architecture and
configuration of the regression models to replace the trial-and-error approach in the
architecture and configuration determination in the NN functional regression model. This
obviates the necessity for searching nonlinear relationships in the function space and enablesthe use of a wide range of supervised learning techniques. In eight real-world experiments,
the M2P model consistently demonstrates superior predictive performance in comparison to
statistical FDA models, conventional deep learning models, and state-of-the-art FNN
models, addressing both scalar output and function output prediction problems.
Despite the promising results, it should be acknowledged that the proposed ILP algorithms
determine knot positions by considering the most complex local case among all functions,
which may lead to overfitting for some smoother functions. Future research should focus on
mitigating this limitation. This could be achieved by developing a local knot modification
strategy to eliminate unnecessary knots, or by integrating regularization techniques, such as
L1 or L2 penalties. Moreover, there is potential for investigating strategies to handle noisy
or sparse data.
7. Supplemental Materials
Section 1: The Proof for Lemma 1
Section 2: The Proof for Theorem 1
Section 3: Hyperparameter for All models and tuning setting
8. Acknowledgment
The authors report that there are no competing interests to declare. The DTI data were
collected at Johns Hopkins University and the Kennedy-Krieger Institute.
9. Reference
Das, S., Demirer, R., Gupta, R., & Mangisa, S. (2019). The effect of global crises on stock
market correlations: Evidence from scalar regressions via functional data analysis.
Structural Change and Economic Dynamics, 50, 132-147.
Shang, H. L., & Hyndman, R. J. (2017). Grouped functional time series forecasting: An
application to age-specific mortality rates. Journal of Computational and GraphicalStatistics, 26(2), 330-343.
Wang, Q., Wang, H., Gupta, C., Rao, A. R., & Khorasgani, H. (2020). A non-linear function-
on-function model for regression with time series data. 2020 IEEE International Conference
on Big Data (pp. 232-239). IEEE.
Wang, J. L., Chiou, J. M., & M√ºller, H. G. (2016). Functional data analysis. Annual Review
of Statistics and its application, 3, 257-295.
Ramsay, J., and Silverman, B. (2005),Functional Data Analysis, New York: Springer.
Ferraty, F., and Vieu, P. (2006),Nonparametric Functional Data Analysis: Theory and
Practice, New York: Springer-Verlag.
Jacques, J., & Preda, C. (2014). Model-based clustering for multivariate functional data.
Computational Statistics & Data Analysis, 71, 92-106.
Morris, J. S. (2015). Functional regression. Annual Review of Statistics and Its Application,
2, 321-359.
Preda, C., Saporta, G., & L√©v√©der, C. (2007). PLS classification of functional data.
Computational Statistics, 22(2), 223-235.
Beyaztas, U., & Shang, H. L. (2022). A comparison of parameter estimation in function-on-
function regression. Communications in Statistics-Simulation and Computation, 51(8),
4607-4637.
Luo, R., & Qi, X. (2019). Interaction model and model selection for function-on-function
regression. Journal of Computational and Graphical Statistics, 28(2), 309-322.
Thind, B., Multani, K., & Cao, J. (2023). Deep learning with functional inputs. Journal of
Computational and Graphical Statistics, 32(1), 171-180.Luo, R., & Qi, X. (2021). Functional regression for densely observed data with novel
regularization. Journal of Computational and Graphical Statistics, 30(1), 220-235.
Luo, R., & Qi, X. (2017). Function-on-function linear regression by signal compression.
Journal of the American Statistical Association, 112(518), 690-705.
Kowal, D. R., & Bourgeois, D. C. (2020). Bayesian function-on-scalars regression for high-
dimensional data. Journal of Computational and Graphical Statistics, 29(3), 629-638.
Yao, F., M√ºller, H. G., & Wang, J. L. (2005). Functional data analysis for sparse longitudinal
data. Journal of the American statistical association, 100(470), 577-590.
Jupp, D. L. (1978). Approximation to data by splines with free knots. SIAM Journal on
Numerical Analysis, 15(2), 328-343.
Tjahjowidodo, T., Dung, V. T., & Han, M. L. (2015). A fast non-uniform knots placement
method for B-spline fitting. In 2015 IEEE international conference on advanced intelligent
mechatronics (AIM) (pp. 1490-1495). IEEE.
Dung, V. T., & Tjahjowidodo, T. (2017). A direct method to solve optimal knots of B-spline
curves: An application for non-uniform B-spline curves fitting. PloS one, 12(3).
Kang, H., Chen, F., Li, Y., Deng, J., & Yang, Z. (2015). Knot calculation for spline fitting
via sparse optimization. Computer-Aided Design, 58, 179-188.
Conti, C., Morandi, R., Rabut, C., & Sestini, A. (2001). Cubic spline data reduction choosing
the knots from a third derivative criterion. Numerical Algorithms, 28(1-4), 45-61.
Goldenthal, R., & Bercovier, M. (2004). Spline curve approximation and design by optimal
control over the knots (pp. 53-64). Springer Vienna.
G√°lvez, A., & Iglesias, A. (2011). Efficient particle swarm optimization approach for data
fitting with free knot B-splines. Computer-Aided Design, 43(12), 1683-1692.Yeh, R., Nashed, Y. S., Peterka, T., & Tricoche, X. (2020). Fast automatic knot placement
method for accurate B-spline curve fitting. Computer-Aided Design, 128, 102905.
Michel, D., & Zidna, A. (2021). A new deterministic heuristic knots placement for B-Spline
approximation. Mathematics and Computers in Simulation, 186, 91-102.
Liang, F., Zhao, J., Ji, S., Fan, C., & Zhang, B. (2017). A novel knot selection method for
the error-bounded B-spline curve fitting of sampling points in the measuring process.
Measurement Science and Technology, 28(6), 065015.
Delaigle, A., & Hall, P. (2010). Defining probability density for a distribution of random
functions. The Annals of Statistics, 38(2), 1171-1193.
Sun, Y., & Wang, Q. (2020). Function-on-function quadratic regression models.
Computational Statistics & Data Analysis, 142, 106814.
M√ºller, H. G., Wu, Y., & Yao, F. (2013). Continuously additive models for nonlinear
functional regression. Biometrika, 100(3), 607-622.
Rossi, F., & Conan-Guez, B. (2005). Functional multi-layer perceptron: a non-linear tool for
functional data analysis. Neural networks, 18(1), 45-60.
Rossi, F., Conan-Guez, B., & Fleuret, F. (2002). Functional data analysis with multilayer
perceptions. In Proceedings of the 2002 International Joint Conference on Neural Networks.
IJCNN'02, 3, 2843-2848. IEEE.
Rao, A. R., & Reimherr, M. (2023). Nonlinear Functional Modeling Using Neural Networks.
Journal of Computational and Graphical Statistics, 1-10.
Hsieh, T. Y., Sun, Y., Wang, S., & Honavar, V. (2021). Functional autoencoders for
functional data representation learning. In Proceedings of the 2021 SIAM International
Conference on Data Mining (SDM) (pp. 666-674). SIAMYao, J., Mueller, J., & Wang, J. L. (2021). Deep learning for functional data analysis with
adaptive basis layers. In 2021 International Conference on Machine Learning (pp. 11898-
11908). PMLR.
Kunoth, A., Lyche, T., Sangalli, G., Serra-Capizzano, S., Lyche, T., Manni, C., & Speleers,
H. (2018). Foundations of spline theory: B-splines, Spline Approximation, and Hierarchical
refinement. In Splines and PDEs: From Approximation Theory to Numerical Linear Algebra
(pp. 1-76). Springer.
Shadrin, A. (1995). Error bounds for Lagrange interpolation. Journal of Approximation
Theory, 80(1), 25-49.
Van Breugel, F., Kutz, J. N., & Brunton, B. W. (2020). Numerical differentiation of noisy
data: A unifying multi-objective optimization framework. IEEE Access, 8, 196865-196877.
Shi, C., & Zeng, X. J. (2022). Machine Learning-Based Approach to Nonlinear Functional
Data Analysis for Photovoltaic Power Forecasting. In 2022 International Joint Conference
on Neural Networks (IJCNN) (pp. 01-08). IEEE.
Graves, A., & Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional
LSTM and other neural network architectures. Neural networks, 18(5-6), 602-610.
Thodberg, H. H. (1996). A review of Bayesian neural networks with an application to near
infrared spectroscopy. IEEE transactions on Neural Networks, 7(1), 56-72.
Kalivas, J. H. (1997). Two Data Sets of Near-Infrared Spectra, Chemometricsand Intelligent
Laboratory Systems, 37, 255‚Äì259.
Fanaee-T, H., & Gama, J. (2014). Event labeling combining ensemble detectors and
background knowledge. Progress in Artificial Intelligence, 2, 113-127.
Goldsmith, J., Feder, J., Crainiceanu, C.M., Caffo, B., and Reich, D. (2011), ‚ÄúPenalizedFunctional Regression,‚Äù Journal of Computational and Graphical Statistics, 20, 830‚Äì851.
Ramsay J, Hooker G, Graves S. Introduction to functional data analysis. In: Functional data
analysis with R and MATLAB. Springer; 2009.
Andreas, A.; Stoffel, T.; (1981). NREL Solar Radiation Research Laboratory (SRRL):
Baseline Measurement System (BMS); Golden, Colorado (Data); NREL Report No. DA-
5500-56488. http://dx.doi.org/10.5439/1052221
Thodberg, H. H. (2015), ‚ÄúTecator Meat Sample Dataset,‚Äù Retrieved from
http://lib.stat.cmu.edu/datasets/tecatorSupplemental Document
:
1. Proof of Lemma 1:
Let ùëù+1 ‚â§ ùëö ‚â§ ùëõ, for a function ùëì ‚àà ùëäùëô+1(ùêΩ ) with 1 ‚â§ ùëû ‚â§ ‚àû,0 ‚â§ ùëô ‚â§ ùëù,
ùëû ùëö
ùêΩ ‚âî [ùúâ ,ùúâ ]‚à©[ùëé,ùëè],
ùëö ùëö‚àíùëù‚àíùëâùêø ùëö+ùëù+1+ùëâùëà
where integers ùëâ ,ùëâ ‚â• ‚àíùëù,
ùêø ùëà
According to the theorem 32 of Lyche et.al (2018), we have
(2ùëù+ùëâ +ùëâ +1)ùëô+1
‚Äñùëì‚àíùí¨ùëì‚Äñ ‚â§ ùêø ùëà (1+ùê∂)‚Ñéùëô+1‚Äñùê∑ùëô+1ùëì‚Äñ ,
ùêøùëû([ùúâùëö,ùúâùëö+1]) ùëô! ùëö,ùúâ ùêøùëû(ùêΩùëö)
where ùí¨ùëì is a B-spline approximation to f in an ùêø norm, and ‚Ñéùëô+1 is the largest
ùëû ùëö,ùúâ
length of a knot interval in ùêΩ , and ùê∂ is some constant. However, we do not know the
ùëö
knot sequence in advance in our problem. Then, we choose ùëâ = ùëâ = ‚àíùëù and for
ùêø ùëà
ùêΩ = [ùúâ ,ùúâ ], the local error bound in the ùêø norm is given as the Equation (15)
ùëö ùëö ùëö+1 ‚àû
in the paper:
ùëù+1
‚Ñé
‚Äñùëì ‚àíùí¨ùëì‚Äñ ‚â§ ùëö ‚Äñùê∑ùëù+1ùëì‚Äñ ,
ùêø‚àû([ùúâùëö,ùúâùëö+1])
ùëõ!
ùêø‚àû([ùúâùëö,ùúâùëö+1])
Consider the entire interval [ùëé,ùëè] as the union of these subintervals,
ùëõ
[ùëé,ùëè] = ‚ãÉ ùêΩ ,
ùëö
ùëö=ùëù+1
the global approximation error is the maximum of the local error over all subintervals,
‚Äñùëì‚àíùí¨ùëì‚Äñ = ùëöùëéùë• ‚Äñùëì‚àíùí¨ùëì‚Äñ .
ùêø‚àû([ùëé,ùëè])
ùëö‚àà[ùëù+1,ùëõ]
ùêø‚àû(ùêΩùëö)
If there is a constant ùúÄ > 0 such that each local error is bounded by ùúÄ,
‚Äñùëì‚àíùí¨ùëì‚Äñ < ùúÄ, ‚àÄ ùëö ‚àà [ùëù+1,ùëõ].
ùêø‚àû(ùêΩùëö)
Then the global approximation error in the max norm is also bounded by ùúÄ,
‚Äñùëì ‚àíùí¨ùëì‚Äñ < ùúÄ.
ùêø‚àû([ùëé,ùëè])
‚àé2. The Proof of Theorem 1:
From Lemma 1 we know that the ùêø gobal error of a degree p B-spline approximation
‚àû
ùí¨ùëì to a function ùëì ‚àà ùê∂ùëõ+1(ùëá) is bounded by ùúÄ if only if the condition below is
ùëñ ùëñ
satisfied:
ùëù+1
‚Ñé
ùëö (ùëù+1)
ùëöùëéùë• |ùëì (ùë°)| < ùúÄ,‚àÄùëö.
ùëù! ùúâùëö‚â§ùë°‚â§ùúâùëö+1 ùëñ
By (19), the maximum value of the (p+1)-th derivatives of all functions at each ùë° ‚àà ùëá
is given as
(ùëù+1)
ùëöùëéùë• |ùëì (ùë°)| ‚â§ ùê∂ .
ùëñ ùë°
ùëñ‚àà[1,ùëÅ]
Then, we have
ùëù+1 ùëù+1
‚Ñé ‚Ñé
ùëö
ùëöùëéùë•
|ùëì(ùëù+1)
(ùë°)| ‚â§ ùúÄ
ùëö
ùëöùëéùë• |ùê∂ |,‚àÄùëö.
ùëù! ùúâùëö‚â§ùë°‚â§ùúâùëö+1 ùëñ ùëù! ùúâùëö‚â§ùë°‚â§ùúâùëö+1 ùë°
With the stated assumptions for all ùëö ‚àà [ùëù+1,ùëõ], we have
ùëù+1
‚Ñé
ùëö
ùëöùëéùë• |ùê∂ | < ùúÄ,
ùëù! ùúâùëö‚â§ùë°‚â§ùúâùëö+1 ùë°
the global bound for all function by ùúÄ in the max norm follows immediately.
‚àé
3. Hyperparameter for all models and tunning setting
The common hyperparameter in the network includes the Number of Layers, Number
of Neurons per Layer, Epochs, validation split, learning rate, the activation function per
layer, batch size, and early stopping patience. There are some other hyperparameters
for specific NN-related models. For CNN, we focused on CNN 1 with a kernel size of
two and filters of 32. For AdaFNN, each neuron of the basis layer is a micro neural
network, thus the number of layers and the number of neurons in the micro neural
network are the additional hyperparameters for AdaFNN. In addition, two levels of
regularization are also included in the AdaFNN model and the orthogonal regularization
penalty and the L1 regularization penalty need also to be tuned. For functional models,the choice of basis function and the number of basis functions for functional data are
typical hyperparameters. For FuncNN, another hyperparameters are the choice of
functional weight basis function and Functional weight basis expansion size. For our
proposed M2P model, the choice of the free knot algorithm and the error threshold are
two hyperparameters in the functional approximation process, and the common
hyperparameter in the network is applied in the prediction process.
The tunning approach is to take a list of possible values for each parameter and run a
5-fold cross-validation. We consider the number of layers between 2 to 5, the number
of Neurons in [16,512], the Epochs in [100, 500], the validation split rate in [0.01, 0.2],
the learning rate in the range [0.001, 0.1], the choice activation function among {‚Äúrelu‚Äù,
‚Äútanh‚Äù,‚Äù linear‚Äù}, batch size in [16, 256] and early stopping patience in [10,20]. For
AdaFNN, the orthogonal regularization penalty is among {0, 0.5, 1}, whereas the L1
penalty is among {0Ôºå1Ôºå2}. For the functional model, the choice of basis function is
the FPC basis function, and the number of basis functions is determined in advance, as
shown in Table A.1
Table A. The choice of basis function and the number of FPC basis functions for the
functional comparison model
Dataset Functional Input Functional Output
Tecator 29
Gasoline 65
Bike Sharing 31
DTI 27
Gait 13 11
Daily 65 20
Electricity 65 65
Traffic 21 31For the M2P model, the final optimal knot distribution and the maximum derivative
curve is shown in Figure below and final model configuration for each dataset are listed
on the bottom of the figure.
Figure A. The optimal knot distribution (order 2 B-spline with 12 knots) and the
maximum derivative curve for Tecator Data.
Table B. The final model configuration for Tecator Data
Activation
No. Neuron Learn Validation Batch Early
function Epoch
/layer rate split size stopping
/layer
Relu, Relu,
512,256,128,1 0.005 300 0.2 32 10
Relu, LinearFigure B. The optimal knot distribution (order 3 B-spline with 18 knots) and the
maximum derivative curve for Gasoline Data.
Table C. The final model configuration for Gasoline Data
Activation
No. Neuron Learn Validation Batch Early
function Epoch
/layer rate split size stopping
/layer
Relu, Relu,
64,64,64,1 0.002 500 0.2 32 20
Relu, LinearFigure C. The optimal knot distribution (order 2 B-spline with 8 knots) and the
maximum derivative curve for Bike Data.
Table D. The final model configuration for Bike Data
Activation
No. Neuron Learn Validation Batch Early
function Epoch
/layer rate split size stopping
/layer
sigmoid,
128,32,32 ,1 sigmoid, 0.002 500 0.15 32 15
Relu, LinearFigure D. The optimal knot distribution (order 1 B-spline with 22 knots) and the
maximum derivative curve for DTI Data.
Table E. The final model configuration for DTI Data
Activation
Learn Validation Batch Early
No. Neuron /layer function Epoch
rate split size stopping
/layer
256,256,256,256,256,1 Relu, Relu, 0.001 500 0.2 32 20
Relu, Relu,
Relu ,
LinearFigure E. The optimal knot distribution and the maximum derivative curve for Gait
Data. (1). Knee: order 4 B-spline with 6 knots (2). Hip: order 3 B-spline with 7 knots
determined by ILP_pw algorithm.
(1) Ôºà2Ôºâ
Table F. The final model configuration for Gait Data
Activation
Learn Validation Batch Early
No. Neuron /layer function Epoch
rate split size stopping
/layer
32,32,32,32,32,1 sigmoid, 0. 1 300 0.2 32 10
sigmoid ,
Relu, Relu,
Relu ,
LinearFigure F. The optimal knot distribution and the maximum derivative curve for Daily
Data. (1). Daily Prec: order 2 B-spline with 60 knots (2). Daily Temp: order 1 B-spline
with 33 knots
(1) Ôºà2Ôºâ
Table G. The final model configuration for Daily Data
Activation
Learn Validation Batch Early
No. Neuron /layer function Epoch
rate split size stopping
/layer
256,128,128 Tanh, tanh, 0. 001 300 0.2 32 20
tanh,
LinearFigure G. The optimal knot distribution and the maximum derivative curve for
Electricity Data. (1). Week 1: order 3 B-spline with 25 knots (2). Week2: order 2 B-
spline with 22 knots
(1) Ôºà2Ôºâ
Table H. The final model configuration for Electricity Data
Activation Learn Validation Batch Early
No. Neuron /layer Epoch
function /layer rate split size stopping
32,32,32,32,32 Relu, Relu, Relu, 0. 001 500 0.2 32 20
Relu, Relu, LinearFigure H. The optimal knot distribution and the maximum derivative curve for Traffic
Data. (1). X: order 2 B-spline with 44 knots; (2). Y: order 2 B-spline with 82 knots
(1) Ôºà2Ôºâ
Table I . The final model configuration for Traffic Data
Activation Learn Validation Batch Early
No. Neuron /layer Epoch
function /layer rate split size stopping
16,16,61 relu, relu, relu, 0. 001 500 0.2 32 20