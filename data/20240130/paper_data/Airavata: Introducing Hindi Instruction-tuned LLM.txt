TechnicalReport
AIRAVATA: INTRODUCING HINDI INSTRUCTION-TUNED LLM
JayGala1 ThanmayJayakumar1 JaavidAktarHusain1,3 AswanthKumar4
MohammedSafiUrRahmanKhan1 DipteshKanojia5 RatishPuduppully6
MiteshM.Khapra1,2 RajDabre7 RudraMurthy8 AnoopKunchukuttan1,2,9
1NilekaniCentreatAI4Bharat 2IITMadras 3IIITD&MKancheepuram
4Flipkart 5UniversityofSurrey 6A*STAR 7NICT 8IBMResearch 9Microsoft
https://ai4bharat.github.io/airavata
Figure1: ImageCourtesy: DALL-E3(Betkeretal.,2023).
1 INTRODUCTION
ThelastyearhaswitnessedtremendousinterestandactivityintheworldofLargeLanguageModels
(LLMs). LLMsholdthepotentialtounlockexcitingapplicationsinartificialintelligencethanksto
their ability to comprehend complex natural language instructions and excel in a broad spectrum
of tasks involving language, knowledge, reasoning, and creative generation. To foster research,
innovation,andwidespreadadoption,anopenecosystemisessential. Wehaveobservedsignificant
advancementsinthisareawiththelaunchofmodelslikeLlama2(Touvronetal.,2023)andMistral
(Jiangetal.,2023),aswellastheirinstruction-tunedvariantssuchasLlama2Chat(Touvronetal.,
2023),Mistral-Instruct(Jiangetal.,2023),andZephyr(Tunstalletal.,2023),amongothers. Major
progress has also been made in developing datasets for pre-training (e.g., RedPajama (Computer,
2023)), instruction tuning (e.g., Alpaca (Taori et al., 2023), UltraChat (Ding et al., 2023), Dolly
(Conoveretal.,2023),OpenAssistant(Köpfetal.,2023),LMSYS-Chat(Zhengetal.,2023a)),and
evaluation benchmarks (e.g., AlpacaEval (Dubois et al., 2023), MT-Bench (Zheng et al., 2023b)).
However,mostoftheseadvancementshavebeenpredominantlycenteredontheEnglishlanguage.
There is limited support for Indian languages, which can be attributed to the incidental inclusion
of some Indian language data that slipped through the data filters during the pre-training of these
language models. However, the representation of data, the efficacy of tokenizers, and task perfor-
mance for Indian languages are considerably behind that of English. The performance in Indian
languages,evenonclosed-sourcemodelssuchasChatGPT(OpenAI,2022),GPT-4(Achiametal.,
1
4202
naJ
62
]LC.sc[
1v60051.1042:viXraTechnicalReport
2023),andothers,isinferiorcomparedtoEnglish(Ahujaetal.,2023). Therefore,thereisanurgent
need to develop a similar ecosystem of tools, models, and datasets for Indian languages to foster
researchandinnovation. Inpursuitofthisobjective,wehaverecentlycollaboratedwithSarvamAI
to launch OpenHathi (SarvamAI, 2023), an open-source foundational model for Hindi, developed
byextendingLlama2(Touvronetal.,2023).
Today, we announce the next step – an initial release of “Airavata”, an instruction-tuned model
for Hindi built by finetuning OpenHathi (SarvamAI, 2023) with diverse, instruction-tuning Hindi
datasetstomakeitbettersuitedforassistivetasks.
Alongwiththemodel, wealsosharetheinstructiontuningdatasets1 toenablefurtherresearchfor
IndicLLMs. We rely on human-curated, license-friendly instruction-tuned datasets to build “Aira-
vata”. WedonotusedatageneratedfromproprietarymodelslikeGPT-4(Achiametal.,2023),etc.
Wethinkthisisamoresustainablewayofbuildinginstruction-tunedmodelsatscaleformostIndic
languages,whererelyingondistilleddatafromcommercialmodelswouldincreasecostsandrestrict
theirfreeusageindownstreamapplicationsduetolicensingrestrictions.
We also compile a collection of evaluation benchmarks2 along with an evaluation framework to
compare various LLMs for their abilities on diverse tasks when instructed in Hindi. Using this
benchmarkandhumanjudgments,wecomparedifferentLLMstoquantifythecurrentstateoftheir
Hindi capabilities. We conduct a detailed analysis of Airavata’s performance on various Natural
Language Understanding (NLU) and Natural Language Generation (NLG) tasks and find that the
instructionfine-tuninghelpsalignthemodeltovariousNLUtasks. Thereissignificantpotentialfor
improvementinNLGtasks, whichrequirethecreationof larger, more diverseinstructiondatasets
as well as innovations in aligning English model representations to Hindi representations to drive
bettercross-lingualtransfer.
2 INSTRUCTION TUNING DATASET CREATION
High-quality instruction tuning datasets are important for the good performance of LLMs. How-
ever, therearefewdiversedatasetsforHindi. Following(Weietal.,2023), werelyontranslating
high-quality English-supervised instruction-tuning datasets into Hindi. We use IndicTrans2 (Gala
etal.,2023),thestate-of-the-artopen-sourceMTmodelforIndianlanguages,fortranslation. Some
previousworks(Lietal.,2023;Weietal.,2023)haveusedChatGPT(OpenAI,2022)totranslatein-
structionsand/orgenerateresponsesintoHinditobetterusecontextduringtranslation(IndicTrans2
and most MT models are sentence-level). However, this is not cost-effective, and the translation
qualityofChatGPT(OpenAI,2022)islowerthanIndicTrans2(Galaetal.,2023),anditsgeneration
qualityinHindimightnotbeuptothemark(Ahujaetal.,2023). Afutureavenueofworkwouldbe
improvingtranslationqualitywhendocumentcontextisavailable.
We sample examples from different datasets listed in Table 1 to ensure balanced representations
across all the tasks while fitting into our instruction tuning budget. We translate the instructions,
input,andoutputsintoHindi. Thisresultsinatotalof404kexamplesspanningEnglishandHindi
language. ThetranslatedHindiexampleswerefilteredtoretainhigh-qualityexamples. Specifically,
examples were retained only when the chrF++ score (Popovic´, 2017) between the back-translated
example and the corresponding English example was 50 or above. The final dataset used for in-
structiontuningcontains385kexamples. Table1showsthedetailsofthefinaltrainingdataset. The
datasetcanbefoundontheHuggingFaceHub.3
WealsocreatetwonativeHindiInstructiondatasets:
• wikiHow: wikiHow4 isanonlinewiki-styleplatformthatservesasavaluableresourceforadi-
versearrayofhow-toarticlesspanningnumeroustopics. Thearticlesontheplatformarehuman-
moderated, ensuring a high-quality standard. The questions users pose in these articles closely
alignwithpotentialusecasesforthismodel, makingitarichresourcefortrainingmodels. Ad-
1https://huggingface.co/datasets/ai4bharat/indic-instruct-data-v0.1
2https://huggingface.co/collections/ai4bharat/airavata-evaluation-suite
-65b13b7b68165de71ba0b333
3https://huggingface.co/datasets/ai4bharat/indic-instruct-data-v0.1
4https://www.wikihow.com/Main-Page
2TechnicalReport
Unfiltered Filtered
Dataset
English Hindi English Hindi
FLAN-v2(Longpreetal.,2023) 67,463 67,463 67,463 65,228
Anthropic-HHH(Baietal.,2022) 5,000 5,000 5,000 4,911
Dolly(Conoveretal.,2023) 15,011 15,011 15,011 14,880
OpenAssistant(Köpfetal.,2023) 19,945 20,128 19,945 16384
LymSys-Chat(Zhengetal.,2023a) 50,000 50,000 50,000 37,422
WikiHow 20,400 6,055 20,400 6,055
Anudesh 5,234 7,577 5,234 7,577
Total 183,053 171,234 183,053 152,457
Table1: InstructionFine-tuningTrainingDatasetDetails
ditionally, this might also help induce reasoning capabilities and generate logical step-by-step
responses. Wecuratearound20kand6karticlesinEnglishandHindi, respectively, resultingin
atotalofaround27karticles. Wecurrentlyformulatethedataasacompletiontaskgiveneither
questionorquestionalongwithafewinitialsteps. ThedatasetisreleasedundertheCC-0license.
• Anudesh: Anudesh is a crowd-sourced collection of prompts accompanied by responses gener-
ated from the Llama-2 70B model. Participants are provided with clear guidelines detailing the
nature of the interaction required, including the specific language to be employed. These lan-
guagesencompassarangethatincludesIndiclanguages,English,transliteratedIndic,aswellasa
blendofIndicandEnglishinacode-mixedformat. Contributorscrafttheirpromptsinadherence
tothesedirectivesandthespecifiedlanguagecriteria.Subsequently,thesepromptsarethenpaired
withthecorrespondingtranslatedoutputsfromtheLlama270Bmodel(Touvronetal.,2023).The
datasetisreleasedunderCC-BY-4.0license. Moredetailsabouttheinteractionswillbereleased
soon.
Weprovideabriefdescriptionofalltheexistinginstructiondatasetsusedinadditiontotheabove
(andcorrespondinglicenses)below:
• FLAN-v2(Longpreetal.,2023): AcollectionofNLPtasksthatcombinesanumberofexisting
NLP datasets with various data augmentations, introduced by Chung et al. (2022). We sample
around 67K examples for our training mixture. The dataset is released under the Apache-2.0
license.
• Anthropic-HHH(Baietal.,2022): Acollectionofhuman-collectedpreferencedataforaligning
themodels tobehelpful andharmless. We sample5Kconversations fromthe“chosen" column
forourtrainingmixture. ThedatasetisreleasedundertheMITlicense.
• Dolly (Conover et al., 2023): A corpus of more than 15K records generated by thousands of
DatabricksemployeestoenableLLMstoexhibitthemagicalinteractivityofChatGPT.Thedataset
isreleasedundertheCC-BY-SA-3.0license.
• OpenAssistant(Köpfetal.,2023):Acollectionofhuman-generated,human-annotatedassistant-
styleconversationcorpusconsistingof38Kmessages,resultinginover3Kconversationtreesand
around20Kconversations. ThedatasetisreleasedundertheApache-2.0.
• LymSys-Chat (Zheng et al., 2023a): A collection of 1M real-world conversations spanning 25
SOTA LLMs similar to OpenAssistant (Köpf et al., 2023). We sample 50K conversations for
ourtrainingmixture. ThedatasetisreleasedundertheLMSYS-Chat-1MDatasetLicenseAgree-
ment.5
• NMT (Gala et al., 2023): A multi-domain human-annotated dataset containing 50K bitext
English-Hindi translation pairs from BPCC-Human (Gala et al., 2023) to enable better cross-
lingualtransfer. ThedatasetisreleasedundertheCC-BY-4.0license.
5https://huggingface.co/datasets/lmsys/lmsys-chat-1m#lmsys-chat-1m-dat
aset-license-agreement
3TechnicalReport
Hyper-Parameter Value
LoRARank 16
LoRAalpha 32
LoRADropout 0.05
LoRATargetModules q_proj,v_proj,k_proj,gate_proj
up_proj,down_proj
Epochs 4
Learningrate 5e-4
BatchSize 128
FloatingPointPrecision bfloat16
Table2: HyperparametersforFine-tuning
3 SUPERVISED FINE-TUNING
We fine-tune the OpenHathi model using the above-compiled datasets. We perform parameter-
efficientfine-tuningwithLoRA(Huetal.,2022).ThehyperparametersusedarelistedintheTable2.
During fine-tuning, the loss was computed only for the output tokens. We used the OpenInstruct
framework6forfine-tuningandcustomizingitforourrequirements(ourcustomversionisavailable
as IndicInstruct7). One fine-tuning example corresponds to one example in the dataset. However,
thisissuboptimalsincemanytokensarewastedaspaddingtokens. Weplantooptimizethisprocess
bypackingmultipledatasetexamplesintoasinglefine-tuningexample(Iyeretal.,2022;Krelletal.,
2023).
3.1 MODELSELECTION
Wefine-tunetheOpenHathimodelfor4epochsandsavethemodelaftereachepoch. Weevaluate
thecheckpointofeachepochonthedevset(IndicSentiment,IndicCOPA,IndicXNLI,IndicQA,and
Flores devtest)and compare the averageperformance. Weobserve that thecheckpoint of epoch 3
performs well on NLU tasks, while the checkpoint of epoch 4 performs well on NLG tasks. We
perform checkpoint averaging, where we interpolate the weights of the above two checkpoints to
obtainamodelthatperformswellacrossbothNLUandNLGtasks. Wefoundthebestinterpolation
factortobearound0.6.
interpolatedweights=0.6×checkpoint +(1−0.6)×checkpoint
3 4
3.2 FULLVS. LORAFINETUNING
Therearetwoprevalentmethodsforinstructionfine-tuningforLLMs: Fullfine-tuningandLoRA.
Fullfine-tuning(FFT)involvesupdatingallthemodelparameters,whereasLoRA(Huetal.,2022)
fine-tuninginvolvesintroducingasmallsetofadditionalparametersandonlyupdatingit. Weper-
formanablationstudytoexamineboththefine-tuningstrategiestotraintwomodels,oneemploying
fullfine-tuningandtheotherbyLoRAfine-tuning. Forthecontextofthisablationstudy,wecon-
sidertheFLANv2(Longpreetal.,2023)subsetfromourinstructiondatasetspanningEnglishand
Hindi. Forourevaluation,weusedasubsetofNLUtasksinHindi,alongwithBoolQ(Clarketal.,
2019)andMMLU(Hendrycksetal.,2021)tasksinEnglish,asdevelopmentsetstodecidebetween
full fine-tuning and LoRA fine-tuning (Figure 2). We observed that FFT models outperform the
OpenHathibasemodel(SarvamAI,2023)inIndicCopa(Doddapanenietal.,2022)andIndicXPara-
phrase (Doddapaneni et al., 2022) tasks. However, the FFT model performed poorly on English
taskscomparedtothebasemodelaswellastheLoRAfine-tunedmodel. LoRAfine-tunedmodel
demonstrated improvements or similar performance as the OpenHathi base model on both Hindi
NLUandEnglishtasks. Asaresult,wedecidedtoproceedwithLoRAfine-tuningfortrainingour
finalmodel. WereportresultsontheLoRAfine-tunedfinalmodelinthesubsequentsection.
6https://github.com/allenai/open-instruct
7https://github.com/AI4Bharat/IndicInstruct
4TechnicalReport
Figure 2: Ablation experiment to understand the performance gaps between Full fine-tuning and
LoRAfine-tuningacrossamixofEnglishandHindiNLUtasks.
4 EVALUATION ON NLP BENCHMARKS
WeevaluateourmodelonthestandardNLUandNLGbenchmarks,whichincludenativeHinditest
sets from IndicXTREME (Doddapaneni et al., 2022) and Indic NLG Suite (Kumar et al., 2022).
Further, to test the knowledge and reasoning capabilities of the model in Hindi, we translate the
English benchmarks such as MMLU (Hendrycks et al., 2021), Hellaswag (Zellers et al., 2019),
ARC(Clarketal.,2018),Winogrande(Sakaguchietal.,2020)andBoolQ(Clarketal.,2019)and
usethesetoevaluateourfinalmodel. TheIndicTrans2(Galaetal.,2023)modelwasemployedfor
translatingthebenchmarks.Althoughthisisnottheperfectevaluationsetuphowever,duetolimited
resources, we rely on this method as a proxy indicator for assessing trends in the performance of
themodelintermsofreasoningcapabilitiesinHindi. Animportantfuturedirectionwouldinvolve
creatingequivalentbenchmarksinthenativelanguageinsteadofsolelyrelyingonthismethod.
4.1 RESULTS
Tables3to6showsthecomparisonofAiravatawiththebasemodel(OpenHathi)andthetranslate-
test baseline using the Llama 2 7B Chat model. In the translate-test approach, the Hindi input is
translated into English using the IndicTrans2 model and is used as a prompt to the Llama 2 7B
Chat model. We observe that Airavata significantly outperforms the OpenHathi model on most
tasks, demonstrating that fine-tuning on the IndicInstruct dataset helps align the base model to a
varietyoftasks. Moreover,theperformanceofthetranslate-testapproachwiththeLlama27BChat
model has a lot of variance, while Airavata consistently performs well across tasks. OpenHathi
and Airavata exhibit comparable performance in the translation task. OpenHathi benefits from its
extensivetrainingonparallelcorpora,resultinginahighlyproficientbasemodelforthetranslation
task. We observe mixed results on generation tasks, indicating the scope for further improvement
ofAirvata,especiallyinopen-endedtextgenerationcapabilities. Table4showstheperformanceof
AiravataandothermodelsonEnglishtestsetsandtherespectivemachine-translatedHinditestset.
We observe a 5-15 point performance gap between the English baseline and the respective Hindi
baselineacrossvarioustasksforbothOpenHathiandAiravatamodels. Thisindicateslimitedcross-
lingualtransferofEnglishknowledgetoHindi,highlightingtheneedforfutureresearchtoimprove
cross-lingualalignmenttomaximizeknowledgetransferbetweenEnglishandHindi.
5TechnicalReport
0-Shot 5-Shot
Llama27BChat Llama27BChat
OpenHathi Airavata OpenHathi Airavata
(translate-test) (translate-test)
IndicSentiment 72.89 97.85 95.81 96.59 98.43 97.01
IndicCopa 68.69 76.53 63.75 42.77 78.34 72.97
IndicXNLI 16.67 23.67 73.26 42.25 47.96 74.70
IndicXParaphrase 71.72 09.54 76.53 66.67 48.56 69.87
Table3: F1scoresonIndicNLUandCommonsenseReasoningtasks
0-Shot 5-Shot
Variant
OpenHathi Airavata OpenHathi Airavata
English 36.16 41.39 40.12 43.28
MMLU
Hindi(Translated) 32.27 34.96 35.13 36.00
English 52.63 73.00 64.46 62.02
BoolQ
Hindi(Translated) 58.56 64.50 65.69 51.47
English 57.28 70.50 62.12 71.04
ARCEasy
Hindi(Translated) 44.28 54.00 49.87 54.84
English 39.85 45.90 46.25 48.29
ArcChallenge
Hindi(Translated) 32.68 35.92 36.60 36.26
English 31.48 34.37 32.45 36.83
HellaSwag
Hindi(Translated) 25.59 25.37 24.85 24.84
Winogrande English 49.17 49.72 - -
Table4: AccuracyonEnglishNLUandCommonsenseReasoningtasksanditstranslatedvariants
0-Shot 5-shot
Metric
OpenHathi Airavata OpenHathi Airavata
chrF++ 55.41 54.82 54.98 54.24
Flores
BLEURT 0.7103 0.6970 0.7115 0.7084
chrF++ 54.23 53.78 54.53 52.81
IN22-Gen
BLEURT 0.7156 0.7012 0.7181 0.7037
Table5: chrF++andBLEURTscoresonEnglish-Hinditranslationtask
1-shot
Metric
Llama27BChat
OpenHathi Airavata
(translate-test)
IndicQA(NoContext) F1 17 4.58 21.01
IndicQA(WithContext) F1 20.69 19.59 37.69
RougeL 11.26 23.06 12.32
IndicHeadline
BLEURT 0.4682 0.4590 0.3793
RougeL 20.45 41.01 10.66
IndicWikiBio
BLEURT 0.5185 0.6415 0.4279
Table6: F1,RougeLandBLEURTscoresonIndicNLGtasks
6TechnicalReport
5 HUMAN EVALUATION
WeassessAiravatausingauthenticpromptsprovidedbyrealusers,evaluatingitsperformanceacross
fivedistinctabilitiesoutlinedinTable7.
AbilityName Ability
Long Abilitytogeneratelong-formtextlikewritingessays,speeches,reports,etc.
Fact-Ops Abilitytogivefactualopinionsandexplanationslikeseekingrecommendations,
seekingadvice,opinions,explanations,etc.
Content Abilitytomakecontentaccessiblelikesummarizations,layman
explanations,etc
Lang-Creativity Abilitytobecreativeinlanguagelikefindinganagrams,rhymingwords,
vocabularyenhancement,etc
Culture AbilitytoanswerquestionsrelatedtoIndianCulture.
Table7: Descriptionofabilitiestotestthroughhumanevaluation
Wedefineasetofintentsanddomainsoffocus,whicharethenprovidedtousers,alongwithclear
instructionsonthetypeofpromptstheyshouldconstruct. Moredetailsaboutthebenchmarkwillbe
releasedsoon.
AlongwithAiravata,wealsoevaluateChatGPT(OpenAI,2022),GPT-4(Achiametal.,2023)and
BactrianX-llama-7B (Li et al., 2023) models for the same abilities. BactrianX-llama-7B is an in-
structedfine-tunedmodelforHindidirectlyfine-tunedwiththeLlamabasemodelonamultilingual
instruction dataset. This multilingual instruction dataset consisted of machine-translated instruc-
tionsfromAlpacaTaorietal.(2023)andDollyConoveretal.(2023)datasets,followedbyresponse
generation from ChatGPT (OpenAI, 2022). Annotators were presented with a prompt and a ran-
domlyselectedresponsefromanyofthemodelsandwereinstructedtoprovideratingsbasedonthe
metricsoutlinedinTable8.
Inadditiontotheabovemetrics, wealsoaskthehumanevaluatorstogiveafinalscorebetween1
and5,reflectingtheiroverallsatisfactionwiththeresponse.
Wesampleasetof50prompts8coveringvariousintentsanddomains(moredetailsaboutthebench-
markwillbemadeavailablesoon)andgeneratetheresponsesfromallthreemodels. Theprompt-
response pairs were shuffled randomly and anonymized, ensuring no indication of the generating
model,beforebeingpresentedtotheannotatorsforevaluation. Annotatorswereinstructedtoassess
responsesstrictlyadheringtotheaforementionedrubrics. WereportthevariousresultsinFigures3
to5.
Our observations suggest that while Airavata still trails significantly behind GPT-4 in terms of its
ability to follow instructions and the quality of its content, it performs relatively better when gen-
erating natural-sounding Hindi content compared to both GPT-4 and ChatGPT. Notably, Airavata
outperformstheBactrianX-llama-7Bmodelbyasignificantmargin. Thisdifferenceinperformance
8https://huggingface.co/datasets/ai4bharat/human-eval
Metric Details Range
IFA:InstructionFol- Thisassessesthemodel’sabilitytoaccuratelyandeffectivelyfol- 0-2
lowingAbility lowtheinstructionsprovidedintheprompt
CNS: Closeness to This assesses how naturally and fluently the model’s responses 0-2
NativeSpeaker alignwiththewayanativeHindispeakerwouldexpressthesame
ideas.
CQ: Content Qual- Thisevaluatestheresponseintermsofitsfactualaccuracy,logical 0-2
ity flowofideas,andoverallinformationalrelevance.
Table8: RubricsforHumanEvaluation
7TechnicalReport
Figure 3: Average satisfaction scores for various models based on a Likert scale between 1 and 5
reportedbyHumanannotators.
Figure 4: Human evaluation scores for assessing the instruction following and content generation
abilitiesofthemodelsbasedontherubricsdescribedinTable8.
can be attributed to a lack of vocabulary expansion in BactrainX-llama-7B to accommodate addi-
tionalHinditokensanditslackofcontinualpre-traininginHindi.Furthermore,BactrainX-llama-7B
maybetrainedonalower-qualitydatasetforinstructiontuningthatwascompletelygeneratedusing
ChatGPT. The OpenHathi model and Airavata, however, have addressed these shortcomings and
8TechnicalReport
Figure5: Fine-grainedhumanevaluationofcontentgenerationabilitiesofthemodelsdescribedin
Table7.
showsubstantialimprovementsovertheBactrainX-llama-7Bmodel. Wewillexploretheseperfor-
mancetrendsacrossvariouscapabilitiesinmoredetailbelow.
The findings indicate that amongst all abilities, Airavata particularly excels in providing factual
opinions and explanations, as demonstrated by the earlier examples. However, the model strug-
gleswithtasksdemandingcreativelanguageusage,likelybecauseourSFTdatalackscomponents
emphasizingcreativity. WhencomparingGPT-4andChatGPT(GPT-3.5)performance,GPT-4gen-
erallysurpassesitscounterpartinknowledge-intensiveorcreativity-demandingtasks. Nevertheless,
it’s noteworthy that ChatGPT either outperforms or competes well in tasks emphasizing language
generation,suchaslong-formcontentcreation,offeringfactualopinions,andensuringcontentac-
cessibility.
We acknowledge that our current human evaluation is not robust and thorough due to the limited
numberofpromptsinoursetandeachpromptandresponsepairbeingevaluatedbyonlyoneanno-
tator. However,ourevaluationstilloffersvaluablepreliminaryinsightsthatwillinformourefforts
toimprovethemodel. Expandingtheinstructiondatasettoincludecontentcoveringadiverserange
of abilities can help improve the model’s capabilities. At the same time, it is important to ac-
knowledge that a significant portion of knowledge stems from English, which possesses the most
extensiveknowledgerepository. Therefore,achievingbettercross-lingualalignmentbetweenHindi
andEnglishrepresentationsisessentialforaccuratelyansweringfactualquestionsandminimizing
erroneousresponses.
6 RESOURCES
Youcanfindallthedetailsabouttheprojectinthissection. Wereleasethefollowingresourcesto
facilitatefurtherresearchininstruction-tuningforIndianlanguageLLMs.
• AiravataGithubRepository(forfine-tuningandevaluation)
• AiravataSFTDatasetCollection: Huggingface
• AiravataEvaluationDatasetCollection: Huggingface
9TechnicalReport
7 SUMMARY AND FUTURE OUTLOOK
WereleaseAiravata, anopen-source, instruction-tunedHindimodeldemonstratingpromisingper-
formanceacrossvarioustaskscomparedtoexistingopen-sourcemodels. Weprovidebothdatasets
andmodelstofacilitatefurtherresearchinimprovingHindiLLMs. Thismarksaninitialstrideto-
wards developing high-quality, open-source LLMs for Indian languages that require multi-faceted
efforts ranging from the creation of extensive pre-training datasets, diverse instruction tuning
datasets,andhigh-qualityfoundationalmodels.
8 LIMITATIONS
Airavata, like other LLMs, faces common challenges such as potential hallucinations, resulting in
thegenerationoffabricatedinformation,andmaystrugglewithaccuracyincomplexorspecialized
topics. There is also a risk of generating objectionable or biased content. Its understanding of
cultural nuances and effectiveness in mixed-language contexts may be constrained. Furthermore,
themodel’sperformanceiscloselytiedtothequalityandscopeofitstrainingdata,influencingits
efficacyandreliability. Itiscrucialtonotethatthismodelisdesignedforresearchpurposesandis
notrecommendedforanyproductionuse-cases.
10TechnicalReport
REFERENCES
JoshAchiam,StevenAdler,SandhiniAgarwal,etal. Gpt-4technicalreport. 2023. URLhttps:
//api.semanticscholar.org/CorpusID:257532815.
KabirAhuja,HarshitaDiddee,RishavHada,MillicentOchieng,KrithikaRamesh,PrachiJain,Ak-
shayNambi,TanujaGanu,SameerSegal,MohamedAhmed,KalikaBali,andSunayanaSitaram.
MEGA:MultilingualevaluationofgenerativeAI. InHoudaBouamor,JuanPino,andKalikaBali
(eds.),Proceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcess-
ing,pp.4232–4267,Singapore,December2023.AssociationforComputationalLinguistics. doi:
10.18653/v1/2023.emnlp-main.258. URLhttps://aclanthology.org/2023.emnl
p-main.258.
Yuntao Bai, Andy Jones, Kamal Ndousse, et al. Training a helpful and harmless assistant with
reinforcementlearningfromhumanfeedback. arXivpreprintarXiv: 2204.05862,2022.
JamesBetker,GabrielGoh,LiJing,TimBrooks,JianfengWang,LinjieLi,LongOuyang,Juntang
Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao,
and Aditya Ramesh. Improving image generation with better captions, October 2023. URL
https://cdn.openai.com/papers/dall-e-3.pdf.
HyungWonChung,LeHou,ShayneLongpre,etal. Scalinginstruction-finetunedlanguagemodels.
arXivpreprintarXiv: 2210.11416,2022.
ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,andKristina
Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill
Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Lan-
guageTechnologies,Volume1(LongandShortPapers),pp.2924–2936,Minneapolis,Minnesota,
June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL
https://aclanthology.org/N19-1300.
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,and
OyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchallenge.
arXivpreprintarXiv: 1803.05457,2018.
TogetherComputer. Redpajama: anopendatasetfortraininglargelanguagemodels,October2023.
URLhttps://github.com/togethercomputer/RedPajama-Data.
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick
Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world’s first truly open
instruction-tunedllm, 2023. URLhttps://www.databricks.com/blog/2023/04/
12/dolly-first-open-commercially-viable-instruction-tuned-llm.
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong
Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional
conversations. arXivpreprintarXiv:2305.14233,2023.
SumanthDoddapaneni,RahulAralikatte,GowthamRamesh,ShreyanshGoyal,MiteshM.Khapra,
Anoop Kunchukuttan, and Pratyush Kumar. Towards leaving no indic language behind: Build-
ing monolingual corpora, benchmark and models for indic languages. Annual Meeting of the
AssociationforComputationalLinguistics,2022. doi: 10.18653/v1/2023.acl-long.693.
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for
methodsthatlearnfromhumanfeedback,2023.
JayGala,PranjalAChitale,AKRaghavan,VarunGumma,SumanthDoddapaneni,AswanthKumar
M,JankiAtulNawale,AnupamaSujatha,RatishPuduppully,VivekRaghavan,PratyushKumar,
MiteshMKhapra,RajDabre,andAnoopKunchukuttan. Indictrans2: Towardshigh-qualityand
accessible machine translation models for all 22 scheduled indian languages. Transactions on
MachineLearningResearch,2023. ISSN2835-8856. URLhttps://openreview.net/f
orum?id=vfT4YuzAYA.
11TechnicalReport
DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,andJacob
Steinhardt. Measuring massive multitask language understanding. In International Conference
onLearningRepresentations,2021. URLhttps://openreview.net/forum?id=d7KB
jmI3GmQ.
EdwardJHu,yelongshen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
andWeizhuChen. LoRA:Low-rankadaptationoflargelanguagemodels. InInternationalCon-
ferenceonLearningRepresentations,2022. URLhttps://openreview.net/forum?i
d=nZeVKeeFYf9.
SrinivasanIyer,XiVictoriaLin,RamakanthPasunuru,TodorMihaylov,DanielSimig,PingYu,Kurt
Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O’Horo, Gabriel Pereyra,
Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. Opt-
iml: Scalinglanguagemodelinstructionmetalearningthroughthelensofgeneralization. arXiv
preprintarXiv: 2212.12017,2022.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, et al. Mistral 7b. arXiv preprint arXiv:
2310.06825,2023.
Mario Michael Krell, Matej Kosec, Sergio P. Perez, and Andrew William Fitzgibbon. Efficient
sequencepackingwithoutcross-contamination: Acceleratinglargelanguagemodelswithoutim-
pacting performance, 2023. URL https://openreview.net/forum?id=ZAzSf9pz
Cm.
Aman Kumar, Himani Shrotriya, Prachi Sahu, Amogh Mishra, Raj Dabre, Ratish Puduppully,
Anoop Kunchukuttan, Mitesh M. Khapra, and Pratyush Kumar. IndicNLG benchmark: Multi-
lingualdatasetsfordiverseNLGtasksinIndiclanguages. InYoavGoldberg,ZornitsaKozareva,
and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing, pp. 5363–5394, Abu Dhabi, United Arab Emirates, December 2022. As-
sociation for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.360. URL
https://aclanthology.org/2022.emnlp-main.360.
AndreasKöpf,YannicKilcher,DimitrivonRütte,SotirisAnagnostidis,Zhi-RuiTam,KeithStevens,
AbdullahBarhoum,NguyenMinhDuc,OliverStanley,RichárdNagyfi,ShahulES,SameerSuri,
David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and
AlexanderMattick.Openassistantconversations-democratizinglargelanguagemodelalignment.
arXivpreprintarXiv: 2304.07327,2023.
Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-x: Multi-
lingualreplicableinstruction-followingmodelswithlow-rankadaptation. arXivpreprintarXiv:
2305.15011,2023.
S.Longpre,LeHou,TuVu,AlbertWebson,HyungWonChung,YiTay,DennyZhou,QuocV.Le,
BarretZoph,JasonWei,andAdamRoberts. Theflancollection: Designingdataandmethodsfor
effectiveinstructiontuning. InternationalConferenceonMachineLearning,2023. doi: 10.485
50/arXiv.2301.13688.
OpenAI. Gpt3.5,November2022. URLhttps://platform.openai.com/docs/model
s/gpt-3-5.
MajaPopovic´. chrF++: wordshelpingcharactern-grams. InProceedingsoftheSecondConference
onMachineTranslation,pp.612–618,Copenhagen,Denmark,September2017.Associationfor
ComputationalLinguistics. doi: 10.18653/v1/W17-4770. URLhttps://aclanthology.o
rg/W17-4770.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An ad-
versarialwinogradschemachallengeatscale. InTheThirty-FourthAAAIConferenceonArtifi-
cialIntelligence,AAAI2020,TheThirty-SecondInnovativeApplicationsofArtificialIntelligence
Conference,IAAI2020,TheTenthAAAISymposiumonEducationalAdvancesinArtificialIntelli-
gence,EAAI2020,NewYork,NY,USA,February7-12,2020,pp.8732–8740.AAAIPress,2020.
doi: 10.1609/AAAI.V34I05.6399. URLhttps://doi.org/10.1609/aaai.v34i05.
6399.
12TechnicalReport
SarvamAI. Openhathiseries: Anapproachtobuildbilingualllmsfrugally,December2023. URL
https://www.sarvam.ai/blog/announcing-openhathi-series.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.
https://github.com/tatsu-lab/stanford_alpaca,2023.
Hugo Touvron, Louis Martin, Kevin Stone, et al. Llama 2: Open foundation and fine-tuned chat
models. arXivpreprintarXiv: 2307.09288,2023.
LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,KashifRasul,YounesBelkada,
ShengyiHuang,LeandrovonWerra,ClémentineFourrier,NathanHabib,NathanSarrazin,Omar
Sanseviero,AlexanderM.Rush,andThomasWolf. Zephyr: Directdistillationoflmalignment.
arXivpreprintarXiv: 2310.16944,2023.
XiangpengWei,HaoranWei,HuanLin,TianhaoLi,PeiZhang,XingzhangRen,MeiLi,YuWan,
Zhiwei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li, Binyuan Hui, Bowen Yu, Dayiheng Liu,
BaosongYang,FeiHuang,andJunXie. Polylm: Anopensourcepolyglotlargelanguagemodel.
arXivpreprintarXiv: 2307.06018,2023.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a
machinereallyfinishyoursentence? InAnnaKorhonen,DavidTraum,andLluísMàrquez(eds.),
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.
4791–4800,Florence,Italy,jul2019.AssociationforComputationalLinguistics. doi: 10.18653
/v1/P19-1472. URLhttps://aclanthology.org/P19-1472.
LianminZheng,Wei-LinChiang,YingSheng,TianleLi,SiyuanZhuang,ZhanghaoWu,Yonghao
Zhuang, Zhuohan Li, Zi Lin, Eric. P Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang.
Lmsys-chat-1m: Alarge-scalereal-worldllmconversationdataset,2023a.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
ZiLin,ZhuohanLi,DachengLi,EricP.Xing,HaoZhang,JosephE.Gonzalez,andIonStoica.
Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv: 2306.05685,
2023b.
13TechnicalReport
A EXAMPLES
In this section, we show example generations from Airavata. Note that the model’s outputs in
examples #A3 and #A6 did not contain newlines between bullet points. They have been added
hereforreadability.
A.1 EXAMPLESWHERETHEAIRAVATAMODELGENERATESGOODOUTPUT
Airavataexcelsinspecificcontentgenerationtasks,suchassolicitingmaildrafts(#A1)andaddress-
ing general how-to inquiries (#A5, #A6). Furthermore, it demonstrates proficiency in adhering to
instructionsrelatedtooutputstructure(#A3and#A4).
Example #A1
Prompt
(cid:611)जसग्राहककोअपनेऑडर्रमेंदेरीकाअनुभवहुआहो, उसके (cid:609)लएएकमाफ(cid:655)ईमेलड्राफ्टकरेंऔरआ(cid:559)स्तकरें(cid:607)क
समस्याकासमाधानहोगयाहै।
Model Response
(cid:607)प्रय[ग्राहककानाम],।
मुझे आशा है (cid:607)क यह ईमेल आपको अ(cid:501)(cid:653) तरह से (cid:608)मल जाएगा। मैं आपको यह सू(cid:609)चत करने के (cid:609)लए (cid:609)लख रहा हूं (cid:607)क
हमारे(cid:607)पछलेऑडर्रमेंदेरीके कारण,[ऑडर्रसंख्या]के (cid:609)लएआपक(cid:655)(cid:607)डलीवरीमेंथोड़ीदेरीहुईहै।मैंसमझताहूं(cid:607)कयह
असु(cid:607)वधाजनकहोसकताहैऔरमैंमाफ(cid:655)मांगताहूं।।
मैंयहसु(cid:607)न(cid:610)(cid:558)तकरनेके (cid:609)लएहरसंभवप्रयासकररहाहूं(cid:607)कइसमुद्देकोजल्दसेजल्दहल(cid:607)कयाजाएऔरभ(cid:607)वष्यमेंऐसी
(cid:607)कसीभीघटनासेबचनेके (cid:609)लएहमजोभीकदमउठासकतेहैं,वहउठारहेहैं।।
इसबीच,मैंआपकोसू(cid:609)चतकरनाचाहताहूं(cid:607)क[ऑडर्रसंख्या]के (cid:609)लएआपक(cid:655)(cid:607)डलीवरीअब[(cid:607)त(cid:609)थ]परउपल(cid:548)है।कृपया
मुझेबताएं(cid:607)कक्याआपके पासकोईप्र(cid:561)या(cid:621)च(cid:652)ताएँहैं।।
आपके धैयर्औरसमझके (cid:609)लएधन्यवाद।मैंभ(cid:607)वष्यमेंआपके साथकामकरनेके (cid:609)लएतत्परहूं।।ईमानदारीसे,[आपका
नाम]
Example #A2 1
Prompt
भारतमेंबड़ोंके पैरछूनाऔरउनसेबहसनकरनानै(cid:607)तकआचरणमानाजाताथा।इसके पीछेक्यातकर् हैमुझेसमझाओ
Model Response
"हम्म,यहएक(cid:606)दलच(cid:570)सवालहै!भारतमेंबड़ोंकोसम्मा(cid:607)नत(cid:557)(cid:604)क्तमानाजाताहैऔरआमतौरपरबुजुग(cid:463)क(cid:655)ओररुख
(cid:607)कयाजाताहैजबकोईक(cid:606)ठन(cid:607)नणर्यलेनाहोताहैयाजानकारीलेनीहोतीहै।ऐसाइस(cid:609)लएहैक्यों(cid:607)कबड़ोंकाज्ञानऔर
अनुभवपी(cid:606)ढ़योंसेपा(cid:605)रत(cid:607)कएगएहैंऔरइन्हेंबहुमूल्यसंप(cid:610)(cid:521)मानाजाताहै।इस(cid:609)लए,बड़ोंके साथबातचीतकरतेसमय
सम्मानपूवर्क(cid:557)वहारकरनाऔरउनके पैरोंकोछूनेजैसे(cid:607)वनम्रइशारोंकाउपयोगकरनामहत्वपूणर्मानाजाताहै।।
यहयादरखनाभीमहत्वपूणर्है(cid:607)कबड़ोंकोअक्सरलोगोंके (cid:609)लएमागर्दशर्कऔरसलाहकारके रूपमेंदेखाजाताहै,और
उन्हेंआहतकरनेयाअपमा(cid:607)नतकरनेके बजायउनक(cid:655)सलाहलेनेक(cid:655)को(cid:609)शशकरनीचा(cid:607)हए।य(cid:606)द(cid:607)कसीबुजुगर्नेआपको
कुछअनु(cid:609)चतकहायाउसके (cid:557)वहारमेंकोईगलतीक(cid:655),तोउ(cid:609)चतप्र(cid:607)त(cid:607)क्रयादेनाऔर(cid:604)(cid:568)(cid:607)तकोठ(cid:654)कसेसंभालनामहत्वपूणर्
है।"
1
14TechnicalReport
Example #A3
Prompt
मैंअपनेसमयप्रबंधनकौशलकोकैसेसुधारसकताहूँ?मुझेपांच(cid:619)ब(cid:652)दुबताए।ं
Model Response
1.अपनेल(cid:219)योंको(cid:609)लखें।
2.अपनेल(cid:219)योंकोछोटे,प्रबंधनीयकाय(cid:463)में(cid:607)वभा(cid:611)जतकरें।
3.एकअनुसूचीबनाएँऔरउसपर(cid:606)टके रहें।
4.अपनेकाय(cid:463)कोप्राथ(cid:608)मकतादें।
5.अपनेकाय(cid:463)कोपूराकरनेके (cid:609)लएखुदकोपुरस्कृतकरें।
1
Example #A4
Prompt
मैंअपनेसमयप्रबंधनकौशलकोकैसेसुधारसकताहूँ?मुझेपांच(cid:619)ब(cid:652)दुबताएंऔरउनकावणर्नकरें।
Model Response
1. अपनेकाय(cid:463)कोप्राथ(cid:608)मकतादेंःउनकाय(cid:463)कोप्राथ(cid:608)मकतादें(cid:611)जन्हेंपूराकरनेक(cid:655)सबसेअ(cid:608)धकआवश्यकताहैऔरउन्हें
पहलेपूराकरें।
2. एकअनुसूचीबनाएःँ एकअनुसूचीबनाएँऔरउसपर(cid:606)टके रहें।यहआपकोअपनेसमयकाबेहतरप्रबंधनकरनेऔर
अपनेल(cid:219)योंकोप्राप्तकरनेमेंमददकरेगा।
3.ध्यानभटकानेसेबचेंःअपनेफोनकोबंदकरदेंऔरअपनेकामपरध्यानकें(cid:606)द्रतकरें।
4.ब्रेकलेंः(cid:607)नय(cid:608)मतब्रेकलेनेसेआपको(cid:605)रचाजर्करनेऔरउत्पादकबनेरहनेमेंमदद(cid:608)मलेगी।
5. अपनेआपकोपुरस्कृतकरेंःअपनेआपकोपुरस्कृतकरनेसेआपकोप्रे(cid:605)रतरहनेऔरअपनेल(cid:219)योंकोप्राप्तकरनेके
(cid:609)लएप्रे(cid:605)रतकरनेमेंमदद(cid:608)मलेगी।
1
15TechnicalReport
Example #A5
Prompt
मैनेबहोतजगहनौकरीके (cid:609)लएएप्लाई(cid:607)कयापरकहीसे(cid:605)रप्लाईनहीआया(cid:611)जस्सेमुझे(cid:607)डप्रेशनहोरहाहेमुझेइसकाकोई
उपायदो?
Model Response
यहसुनकरखेदहै(cid:607)कआपकोअवसादकासामनाकरनापड़रहाहै।यहयादरखनामहत्वपूणर्है(cid:607)कअकेलेपन,(cid:621)च(cid:652)ताऔर
(cid:607)नराशाजैसीभावनाओंकोदूरकरनेमेंमददकरनेके कईतरीके हैं।यहाँकुछसुझाव(cid:606)दएगएहैंजोसहायकलगसकतेहैंः
1. (cid:607)कसी(cid:609)च(cid:607)कत्सकयासलाहकारसेबातकरें-(cid:607)कसीमान(cid:609)सकस्वा(cid:569)यपेशेवरसेबातकरनाआपके अवसादकोप्रबं(cid:608)धत
करनेऔरइसेकमकरनेक(cid:655)(cid:606)दशामेंपहलाकदमहोसकताहै।वेआपक(cid:655)भावनाओंकोसमझनेऔरउनपरकाबूपानेके
(cid:609)लएएकयोजनाबनानेमेंआपक(cid:655)मददकरसकतेहैं।
2. (cid:607)नय(cid:608)मत(cid:557)ायामःशारी(cid:605)रकग(cid:607)त(cid:607)व(cid:608)धएडं ो(cid:631)फ(cid:652)नछोड़तीहै,जोप्राकृ(cid:607)तकमनोदशाबढ़ानेवालेहोतेहैं।यहांतक(cid:607)कछोटे
पैमानेके (cid:557)ायामजैसेचलना,तैरनायाकुछहल्का(cid:624)ख(cid:652)चावभीअवसादग्रस्तताक(cid:655)भावनाकोकमकरनेमेंमददकरसकता
है।
3. प्रकृ(cid:607)तमेंसमय(cid:607)बताएःँ प्रकृ(cid:607)तमेंसमय(cid:607)बतानेसेतनावकोकमकरनेऔरकल्याणक(cid:655)भावनाकोबढ़ावादेनेमेंमदद
(cid:608)मलसकतीहै।आपटहलनेजासकतेहैं,पैदलयात्राकरसकतेहैंयाबागवानीकाकामकरके प्रकृ(cid:607)तमेंअपनीरु(cid:609)च(cid:607)वक(cid:609)सत
करसकतेहैं।
4.(cid:607)प्रयजनोंके साथजुड़ेंःदोस्तों,प(cid:605)रवारयासहक(cid:632)म(cid:652)योंके साथजुड़ेंजोसमथर्नऔरप्रोत्साहनप्रदानकरतेहैं।सामा(cid:611)जक
बातचीतकोप्रोत्सा(cid:607)हतकरनेसेअलगावऔरअकेलेपनक(cid:655)भावनाओंकोकमकरनेमेंमदद(cid:608)मलसकतीहै।
5. रचनात्मकग(cid:607)त(cid:607)व(cid:608)धयोंमेंसंलग्नरहेंःकला,संगीतयालेखनजैसीरचनात्मकग(cid:607)त(cid:607)व(cid:608)धयोंमेंशा(cid:608)मलहोनेसेमनको
(cid:607)वच(cid:609)लतकरनेऔरखुशीक(cid:655)भावनालानेमेंमदद(cid:608)मलसकतीहै।
6. माइंडफुलनेसतकनीकोंकोआजमाएःँ ध्यान,गहरीसांसलेनेके अभ्यासयायोगस(cid:607)हत(cid:607)व(cid:610)भ(cid:545)माइंडफुलनेसतकनीकें
आरामकरनेऔरवतर्मानक्षणपरध्यानकें(cid:606)द्रतकरनेमेंमददकरसकतीहैं।
7. आत्म-देखभालकाअभ्यासकरेंःपयार्प्तनींदलेना,संतु(cid:609)लतआहारखानाऔरस्व(cid:568)जीवनशैलीबनाएरखनेजैसी
स्व-देखभालप्रथाओंमेंशा(cid:608)मलहोनाआवश्यकहै।इसमेंस्नानकरना,(cid:607)कताबपढ़नायाट(cid:653)वीदेखतेहुएनाश्ताकरनाजैसी
चीजेंशा(cid:608)मलहोसकतीहैं।
8. सहायतासमूहोंमेंभागलेंः(cid:568)ानीयसहायतासमूहयासामुदा(cid:608)यकसंगठनखोजनेकाप्रयासकरेंजोआपके समान
चुनौ(cid:607)तयोंसेजूझरहेहों।दूसरोंसेजुड़कर,आपअपनेअवसादकोकमकरनेमेंमददकरनेके तरीकोंक(cid:655)पहचानकरसकते
हैं।
9. पेशेवरमददलेंःय(cid:606)दआपके अवसादकाप्रबंधनकरनाबहुतक(cid:606)ठनलगताहै,तो(cid:609)च(cid:607)कत्सायापरामशर्प्राप्तकरनेपर
(cid:607)वचारकरें।एकप्र(cid:609)श(cid:610)क्षतमान(cid:609)सकस्वा(cid:569)यपेशेवरआपकोमुकाबलातंत्रसीखनेमेंमददकरसकताहैऔरआपकोअ(cid:608)धक
सकारात्मकमान(cid:609)सकता(cid:607)वक(cid:609)सतकरनेमेंमददकरसकताहै।यादरखें,अवसादकोदूरकरनाआसाननहींहै,ले(cid:607)कनसही
समथर्नऔररणनी(cid:607)तयोंके साथ,आपप्रग(cid:607)तकरसकतेहैंऔरबेहतरमहसूसकरसकतेहैं।
1
16TechnicalReport
Example #A6
Prompt
मैंउ(cid:521)रभारतीयहूंऔरमूल(cid:619)ह(cid:652)द(cid:653)भाषीहूं।हालहीमें,मुझेअपनीनौकरीके (cid:609)लएबैंगलोर(cid:609)शफ्टहोनापड़ा।मुझेयहांके
मूल(cid:607)नवा(cid:609)सयोंसेबातचीतकरनेमेंथोड़ीपरेशानीहोतीहैक्यों(cid:607)कवेक(cid:545)ड़बोलतेहैंऔर(cid:619)ह(cid:652)द(cid:653)नहींजानतेहैं।मैंक(cid:545)ड़
सीखनाचाहूंगीता(cid:607)कमुझेइससमस्याकासामनानकरनापड़े।मेरामागर्दशर्नकरें(cid:607)कमैंखुदकोक(cid:545)ड़भाषामेंकैसे
(cid:609)श(cid:610)क्षतकरसकताहूं
Model Response
आपके प्र(cid:561)के (cid:609)लएधन्यवाद!बेंगलुरुजानेपरआपको(cid:568)ानीयलोगोंक(cid:655)भाषासीखनेमेंकुछचुनौ(cid:607)तयोंकासामनाकरना
पड़सकताहै,ले(cid:607)कन(cid:621)च(cid:652)तामतकरो-मददके (cid:609)लएयहाँएकचरण-दर-चरणमागर्द(cid:633)श(cid:652)काद(cid:653)गईहैः
क(cid:545)ड़सीखनेके (cid:609)लएयेकदमउठाएगएहैंः
कनार्टकसरकारद्वाराप्रदान(cid:607)कएजारहेमुफ्तक(cid:545)ड़पाठ्यक्रम
2. क(cid:545)ड़सीखनेवालेमंचजैसे(cid:607)क"क(cid:545)ड़सीखें"या"अंगड़ी"जोक(cid:545)ड़पाठोंऔरअभ्यासकाय(cid:463)तकपहुंचप्रदानकरते
हैं।
3. क(cid:545)ड़वणर्माला,उ(cid:500)ारणऔरशब्दावलीसीखनाशुरूकरनेके (cid:609)लएक(cid:545)ड़के बु(cid:607)नयाद(cid:653)शब्दोंऔरवाक्यांशोंकोयाद
रखनेकाप्रयासकरें।आपअपनेदमपरऑनलाइनसंसाधनोंकाउपयोगकरके याअन्यक(cid:545)ड़बोलनेवालोंके साथअभ्यास
करके भीशुरुआतकरसकतेहैं।
4.क(cid:545)ड़(cid:557)ाकरणऔरसंरचनाओंके बारेमेंजाननेके (cid:609)लएक(cid:545)ड़(cid:607)कताबेंयाप(cid:607)त्रकाएंपढ़नाशुरूकरें।आपयूट्यूबचैनलों
परक(cid:545)ड़सबकयावातार्लापसत्रोंकोदेखनेका(cid:607)वकल्पभीचुनसकतेहैं(cid:611)जनमेंक(cid:545)ड़भाषाशा(cid:608)मलहोसकतीहै।
5. क(cid:545)ड़गीतों,(cid:607)फल्मोंऔरट(cid:653)वीशोकाआनंदलें(cid:611)जन्हेंआपसमझसकतेहैं।यहआपकोनएशब्दोंऔरअ(cid:610)भ(cid:557)(cid:604)क्तयों
कोसुननेऔरउनका(cid:607)व(cid:560)ेषणकरनेमेंमददकरेगा।
6.दै(cid:607)नकग(cid:607)त(cid:607)व(cid:608)धयोंऔरबातचीतमेंक(cid:545)ड़काउपयोगकरनेकाअभ्यासकरें।उदाहरणके (cid:609)लए,य(cid:606)दआपके पासकॉफ(cid:655)
शॉपजानाहै,तोपहलेवहाँजातेसमयक(cid:545)ड़मेंकुछछोटेवाक्यांशयाअ(cid:610)भवादनआजमाए।ँ
7. क(cid:545)ड़में(cid:607)कसीदोस्तयाप(cid:605)रवारके सदस्यसेबातकरें।उन्हेंभाषा(cid:609)सखानेऔरउनसेसवालपूछनेसेआपनईचीजें
सीखनेऔरसुधारकरनेके अवसरप्राप्तकरेंगे।
8. अन्यक(cid:545)ड़भाषकोंके साथजुड़ें(cid:611)जनकाआपअनुसरणकरसकतेहैंयाउनक(cid:655)सामग्रीदेखसकतेहैं।चाहेवहकोई
फेसबुकसमूहहोयाएकभाषा(cid:607)व(cid:607)नमयकायर्क्रम,दूसरोंके संपकर् मेंरहनाआपक(cid:655)प्रग(cid:607)तमेंसहायकहोगा।यादरखें,एक
अ(cid:501)(cid:653)नींवके साथ,आप(cid:607)न(cid:610)(cid:558)तरूपसेजल्दहीक(cid:545)ड़धाराप्रवाहहोनेमेंसक्षमहोंगे।गुडलक!
1
17TechnicalReport
A.2 EXAMPLESWHEREAIRAVATAOUTPUTHASERRORS
Themodelispronetohallucinationsandcanpresentdistortedfactsasagainsttheplotofthenovel
in#B1ormayfailtofollowtheinstructionsprovided(#B2).
Example #B1
Prompt
धरमवीरभारतीजीकाप्र(cid:609)सद्धउपन्यास"गुनाहोंकादेवता"के बारेमेंबताओऔरकहनीके मुख्य(cid:607)करदारसुधाऔरचंदर
के बारेमेंसं(cid:610)क्षप्तवणर्नकरें?
Model Response
गुनाहोंकादेवताएकलोक(cid:607)प्रयभारतीयक्ला(cid:609)सकहैजो18वींशताब्द(cid:653)के अंतमें(cid:609)लखागयाथा।यहकहानीसुधाऔरचंदर
नामकदोप्रे(cid:608)मयोंक(cid:655)कहानीबतातीहै,(cid:611)जन्हेंउनके प(cid:605)रवारोंनेअलगकर(cid:606)दयाथा।सुधाअमीरप(cid:605)रवारसेआतीहैऔरउसे
एकअ(cid:501)ेप(cid:607)तकोखोजनेके (cid:609)लएकईदावेदारोंपर(cid:607)वचारकरनापड़ताहै।चंदरएकगरीब(cid:607)कसानप(cid:605)रवारसेआताहैऔर
वहअपनी(cid:609)शक्षाऔरभ(cid:607)वष्यमेंअपनेसपनोंकोपूराकरनेके (cid:609)लएकड़ीमेहनतकरताहै।सुधाअंततःचंदरकोचुनतीहै
औरदोनोंशाद(cid:653)करलेतेहैं।हालाँ(cid:607)क,उनक(cid:655)खुशीअल्पका(cid:609)लकहोतीहैक्यों(cid:607)कसुधाकाअ(cid:610)भशापउसके पूवर्जोंद्वाराडाला
जाताहै(cid:607)कवहकभीभीखुशनहींहोगी।सुधाऔरचंदरतबतकप्यारकरतेरहेजबतक(cid:607)कसुधामरजातीऔरचंदर(cid:607)फर
अपनाशेषजीवनअकेलेरहनेके (cid:609)लएछोड़देता।उपन्यासकरुणाऔरप्रेमके (cid:607)वषयोंक(cid:655)पड़तालकरताहै,औरअक्सर
इसेरोमां(cid:606)टकसा(cid:607)हत्यक(cid:655)उत्कृ(cid:562)कृ(cid:607)तमानाजाताहै।
Example #B2 1
Prompt
मैंमधुबनीकलाकारीके बारेमेंज़्यादाजानकारीके (cid:609)लएउत्सुकहूँ|कृप्याइसकलाकारीके बारेमेंमुझे१००शब्दोंमें
जानकारीदें|
Model Response
मुब(cid:609)लकलाएकप्रकारक(cid:655)अमूतर्कलाहैजोमानवशरीरऔरअंगोंकाप्र(cid:607)त(cid:607)न(cid:608)धत्वकरतीहै।यहएकबहुमुखीकलारूपहै
(cid:611)जसेपारंप(cid:605)रकऔरसमकालीनदोनोंतरीकोंसे(cid:607)कयाजासकताहै,औरइसकाउपयोगअक्सर(cid:609)चत्रकला,मू(cid:631)त(cid:652)कलाऔर
प्रदशर्नजैसे(cid:607)व(cid:610)भ(cid:545)माध्यमोंमें(cid:607)कयाजाताहै।
1
18