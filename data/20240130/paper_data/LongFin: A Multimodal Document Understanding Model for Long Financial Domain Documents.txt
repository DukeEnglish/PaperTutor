LongFin: A Multimodal Document Understanding Model for Long Financial
Domain Documents
AhmedMasry,AmirHajian
ArteriaAI
ahmed.masry@arteria.ai,amir.hajian@arteria.ai
Abstract
DocumentAIisagrowingresearchfieldthatfocusesonthe
comprehension and extraction of information from scanned
anddigitaldocumentstomakeeverydaybusinessoperations
more efficient. Numerous downstream tasks and datasets
havebeenintroducedtofacilitatethetrainingofAImodels
capable of parsing and extracting information from various
documenttypessuchasreceiptsandscannedforms.Despite
these advancements, both existing datasets and models fail
toaddresscriticalchallengesthatariseinindustrialcontexts.
Existing datasets primarily comprise short documents con-
sistingofasinglepage,whileexistingmodelsareconstrained
byalimitedmaximumlength,oftensetat512tokens.Con-
sequently,thepracticalapplicationofthesemethodsinfinan-
cial services, where documents can span multiple pages, is
severely impeded. To overcome these challenges, we intro-
duceLongFin,amultimodaldocumentAImodelcapableof
encodingupto4Ktokens.WealsoproposetheLongForms
dataset, a comprehensive financial dataset that encapsulates Figure1:Firstpagefroma4-pageexamplefinancialforminthe
severalindustrialchallengesinfinancialdocuments.Through LongFormsdataset.Theinformationinthesedocumentsisspread
anextensiveevaluation,wedemonstratetheeffectivenessof overamixoftablesandtextspanningmultiplepageswhichmakes
the LongFin model on the LongForms dataset, surpassing itchallengingforshort-contextmodels.
theperformanceofexistingpublicmodelswhilemaintaining
comparableresultsonexistingsingle-pagebenchmarks.
Karatzas, and Jawahar 2021), and key information extrac-
tion (Huang et al. 2019). Nonetheless, a significant limita-
Introduction tion shared by these datasets is that they mostly consist of
single-pagedocumentswithalimitedamountofcontent.As
There has been a noticeable industrial interest surrounding
a consequence, these datasets fail to capture various chal-
the automation of data extraction from various documents,
lengesinherentinparsinglengthydocumentsspanningmul-
including receipts, reports, and forms to minimize manual
tiple pages, which are commonly encountered in the finan-
effortsandenableseamlessdownstreamanalysisoftheex-
cial industry. Financial reports and documents can become
tracteddata(Zhangetal.2020;Xuetal.2020).However,the
exceedingly lengthy, necessitating a comprehensive under-
process of parsing documents poses several challenges, in-
standingoftheentirecontexttoeffectivelyanalyzeandex-
cludingobscureinformationwithinscanneddocumentsthat
tractpertinentinformation.
may result in Optical Character Recognition (OCR) errors,
The limitations inherent in existing datasets have a di-
complexlayouts(suchastables),andintricatecontentstruc-
rect impact on the capabilities of the proposed models. In
tures.
the literature, two primary lines of work have emerged: (i)
To investigate and address these challenges, several
OCR-dependent architectures (Wang, Jin, and Ding 2022;
datasets have been made available. These datasets encom-
Xu et al. 2020, 2021; Huang et al. 2022; Tang et al. 2023)
passawiderangeoftasks,suchasclassification(Pramanik,
(ii) OCR-free models (Kim et al. 2022; Lee et al. 2023).
Mujumdar, and Patel 2022), semantic entity recognition
OCR-dependentmodelstypicallyemploytransformer-based
(Park et al. 2019; Guillaume Jaume 2019), relation extrac-
text encoders and incorporate spatial information by lever-
tion(GuillaumeJaume2019),questionanswering(Mathew,
agingthewords’coordinatesinthedocumentsasadditional
Copyright©2024,AssociationfortheAdvancementofArtificial embeddings. One notable exception is UDOP (Tang et al.
Intelligence(www.aaai.org).Allrightsreserved. 2023) which consists of an encoder-decoder architecture.
4202
naJ
62
]LC.sc[
1v05051.1042:viXraConversely,OCR-freemodelstypicallyemployavisionen- DocumentAIModels
codertoprocessthescanneddocumentimageandatextde- Numerous document understanding models have been de-
coder to generate the desired information. Nevertheless, a veloped to tackle the challenges posed by the aforemen-
common limitation shared by most of these models is their tioned benchmark datasets. These models can be broadly
designandpretrainingtohandleamaximumof512tokens categorized into two main groups: OCR-free and OCR-
orprocessasingleinputimage. dependentmodels.OCR-freemodels,exemplifiedbyDonut
Inthiswork,weintroducetwomaincontributions.Firstly, (Kimetal.2022)andPix2Struct(Leeetal.2023),typically
we present the LongForms dataset, a comprehensive finan- employ vision transformer-based encoders to process input
cialdatasetprimarilycomprising140longformswherethe imagesandtextdecoderstohandleoutputgeneration.These
task is formulated as named entity recognition. Due to pri- modelsareoftenpretrainedonOCR-relatedtasks,enabling
vacyconcernsandproprietarylimitations,wewereunableto themtocomprehendthetextembeddedwithinscanneddoc-
utilizeourinternalresourcestoconstructthisdataset.Con- umentseffectively.Ontheotherhand,OCR-dependentmod-
sequently, we obtained financial statements from the SEC els,includingLayoutLM(Xuetal.2020),LayoutLMv2(Xu
website1, aligning our tasks to encompass the significant etal.2021),LayoutLMv3(Huangetal.2022),LiLT(Wang,
challengesencounteredinthefinancialdocumentswhichre- Jin, and Ding 2022), DocFormer (Appalaraju et al. 2021)
quire a deep understanding of lengthy contexts. Secondly, and UDOP (Tang et al. 2023), rely on external OCR tools
we propose LongFin, a multimodal document understand- toinitiallyextractunderlyingtextfromscanneddocuments.
ing model capable of processing up to 4K tokens. Our ap- Toincorporatelayoutinformation,thesemodelsutilizespe-
proach builds upon LiLT (Wang, Jin, and Ding 2022), one cializedpositionalembeddings,encodingthecoordinatesof
of the state-of-the-art multimodal document understanding eachwordinthedocument.Additionally,somemodels,such
models.Additionally,weincorporatetechniquesthateffec- asLayoutLMv2,LayoutLMv3,DocFormer,andUDOP,em-
tively extend the capabilities of text-only models, such as ploy visual embeddings created by splitting the image into
RoBERTa (Liu et al. 2019), to handle longer sequences, as patches. These visual embeddings, along with the text and
demonstrated by Longformer (Beltagy, Peters, and Cohan layout embeddings, are fed into the models. While Lay-
2020).Byleveragingthesetechniques,ourproposedmodel outLM,LayoutLMv2,LayoutLMv3,DocFormer,andLiLT
exhibitsenhancedperformanceinprocessinglengthyfinan- adopt an encoder-only architecture, UDOP is based on the
cialforms.Theefficacyofourapproachisextensivelyeval- T5 model (Raffel et al. 2020), which follows an encoder-
uated, showcasing its effectiveness and paving the way for decoder architecture. Despite the impressive achievements
numerouscommercialapplicationsinthisdomain. of these models, they share a common limitation: they are
typicallydesignedtoprocessasinglepageoramaximumof
RelatedWork
512 tokens, thereby restricting their applicability to multi-
DocumentDatasets pagedocuments.(Phametal.2022)proposedamultimodal
documentunderstandingmodelthatcanprocessupto4096
Several recently released datasets in the field of document
tokens, however their code is not publicly available and
understanding have contributed significantly to advancing
their model performance deteriorates on the short-context
research in this area. The RVL-CDIP dataset (Pramanik,
datasets such as FUNSD (Guillaume Jaume 2019). In con-
Mujumdar,andPatel2022)introducedaclassificationtask,
trast, our proposed model, LongFin, works efficiently on
encompassing 400K scanned documents categorized into
bothshortandlongcontexts(toup4096tokens),makingit
16 classes, such as forms and emails. Another notable
particularlywell-suitedforavarietyofreal-worldindustrial
dataset, DocVQA (Mathew, Karatzas, and Jawahar 2021),
applications.
focuses on document question answering and comprises
50K question-answer pairs aligned with 12K scanned im-
LongFormsDataset
ages.Inaddition,theCORDdataset(Parketal.2019)con-
sistsof11Kscannedreceipts,challengingmodelstoextract Duetoprivacyconstraints,weareunabletoutilizeinternal
54differentdataelements(e.g.,phonenumbersandprices). documentsfordatasetconstruction.Instead,weturntopub-
Furthermore, the FUNSD dataset (Guillaume Jaume 2019) liclyavailablefinancialreportsandtailorourdataset,Long-
was proposed, featuring 200 scanned forms. This dataset Forms,toemulatethechallengesencounteredinourpropri-
primarily revolves around two key tasks: semantic entity etary datasets. This approach ensures the task’s alignment
recognition(e.g.,header,question,answer)andrelationex- withreal-worldfinancialcontextswithoutviolatingprivacy.
traction(question-answerpairs).FUNSDisparticularlyrel-
DatasetCollection&Preparation
evant to our dataset, LongForms, as it also mainly consist
of forms. However, FUNSD and all the above-mentioned ToconstructLongForms,weleveragetheEDGARdatabase
datasets mainly focus on short contexts, as they typically 2,acomprehensiverepositoryoffinancialfilingsandreports
consist of single-page documents. In contrast, our Long- submittedbyUScompanies.Thesefilingsarebasedondif-
Forms dataset primarily consists of multi-page documents, ferentfinancialformtypes(e.g.,10-K,10-Q)whichvaryin
presentinguniquechallengesthatdemandacomprehensive structure and content. Our dataset primarily centers around
understanding of lengthy contexts which is common in the theSECForm10-Q,whichprovidesadetailedquarterlyre-
financialindustry. port on a company’s finances. This specific form is chosen
1https://www.sec.gov/edgar/ 2https://www.sec.gov/edgar/Dataset longcontextswithinboththetextandlayoutencoders.One
#Forms #Pages #Words #Entities
Split keyadvantageofLongFinisitsabilitytoscalelinearlywith
Train 105 514 125094 843 theinputsequencelength,incontrasttothequadraticscal-
Test 35 171 43364 285 ing(O(n2))observedintheoriginaltransformers’(Vaswani
Overall 140 685 168458 1128
et al. 2017) attention mechanism. This linear scaling, in-
Table1:LongFormsdatasetstatistics. spiredbytheLongformermodel(Beltagy,Peters,andCohan
2020),allowsLongFintoefficientlyhandlelongcontextsup
due to its similarity in both structure and content to to the to4Ktokens.
documentswefrequentlyencounterinthefinancialservices TextEncoder ForthetextencoderinLongFin,weadopt
industry. the Longformer (Beltagy, Peters, and Cohan 2020) model,
We download 140 10-Q forms that were published be- which has been pretrained to handle long textual contexts
tween 2018 and 2023. This deliberate decision to keep the ofupto4096tokens.AsdepictedinFigure2a,theinputto
datasetrelativelysmallisintendedtomirrorthelimiteddata the text encoder consists of two types of embeddings: text
challenges commonly encountered in real-world scenarios, embeddings (E ) and absolute position embeddings (E ).
T P
particularly in the finance domain, where strict data con- These embeddings are added together to produce the final
fidentiality prevents access to large-scale datasets. Conse- embeddings (E ). Subsequently, a layer normalization
final
quently,itiscommonpracticetoconstructsmallerdatasets (Ba, Kiros, and Hinton 2016) operation is applied, and the
thatmimictheproprietarydatasets(Madletal.2023).Fur- resultingoutputisfedintotheencoder.
thermore, our dataset size aligns with recently published The attention mechanism in LongFin incorporates two
datasets, such as the FUNSD dataset (Guillaume Jaume types of attention: local attention and global attention. The
2019) which primarily consists of single-page forms. In- local attention employs a sliding window approach, where
spiredbytheFUNSDdataset,weperformarandomsplitof eachtokenattendstothe512localtokenssurroundingit.On
theLongFormsdatasetanddividethedatasetinto105train- the other hand, the global attention involves a set of global
ingdocuments,whichaccountfor75%ofthetotaldataset, tokens,selectedatintervalsof100.Whileotherapproaches
and35testingdocuments,representingtheremaining25%. (Beltagy, Peters, and Cohan 2020; Pham et al. 2022) may
employ different methods for selecting global tokens, such
DatasetDescription&Setup asrandomselectionortask-specificstrategies,welimitour
Our dataset, LongForms, is formulated as a Named Entity experimentation to interval-based selection for simplicity
Recognition (NER) task. The dataset consists of N exam- and due to limited computational resources. Each token in
ples, denoted as D = {d ,w ,b ,n }N , where d repre- the input sequence attends to these global tokens, in addi-
i i i i i=1 i
sents a PDF document, w represents the list of words, b tion to its local context as shown in Figure 2b. This com-
i i
represents the list of bounding boxes, and n represents a binationoflocalandglobalattentionmechanismsenhances
i
listofentitiespresentinthedocument.Toobtainthewords themodel’sabilitytocapturebothlocalcontextandbroader
(w ) and their bounding boxes (b ), each PDF document is globaldependencieswithinthelonginputsequences.
i i
processedusingthepdftotext3tool.Moreover,wedefinesix
Layout Encoder For the layout encoder in LongFin, we
entity types: (i) Total Assets, (ii) Cash at the beginning of
adoptthelayoutencoderutilizedintheLiLTmodel(Wang,
theperiod(BeginningCash),(iii)Cashattheendofthepe-
Jin, and Ding 2022). Similar to the text encoder, the input
riod (End Cash), (iv) Cash provided by financial activities
forthelayoutencodercomprisestwotypesofembeddings:
(FinancialCash),(v)Netchangeincash(ChangeinCash),
absolutepositionembeddingsandlayoutembeddings.Each
and(vi)QuarterKeys.AsshowninTable1,ourLongForms
word in the input document is associated with a bounding
datasetcontains140formsthatconsistof685pages,168458
box that defines its location within the document layout.
words,and1128entitiesintotal.Themodelsaretrainedto
This bounding box is represented by four numbers: x , y ,
0 0
predictn givenbothw andb .
i i i x ,andy ,whichcorrespondtothecoordinatesofthetop-
1 1
leftandbottom-rightpointsoftheboundingbox.Tonormal-
LongFinModel
ize these coordinates within the range [0,1000], we use the
Architecture page’sheightandwidth.
To generate the layout embedding for each word, each
Figure 2 illustrates the overall architecture of our proposed
coordinate in the normalized bounding box is used to ob-
model,LongFin,whichbuildsuponrecentlypublishedmod-
tain an embedding vector. The different coordinates’ em-
els:LiLT(Wang,Jin,andDing2022)andLongformer(Belt-
bedding vectors are then concatenated and projected using
agy, Peters, and Cohan 2020). Similar to LiLT (Wang, Jin,
alinearlayer.Theresultinglayoutembeddingsareaddedto
and Ding 2022), LongFin comprises three primary com-
theabsolutepositionembeddingstoobtainthefinalembed-
ponents: a text encoder, a layout encoder, and the BiACM
dings. These final embeddings are then fed into the layout
(bidirectional attention complementation mechanism) layer
encoder.Similartothetextencoder,wealsoemploythelo-
(Wang, Jin, and Ding 2022). However, LongFin introduces
cal&globalattentionmechanismsinthelayoutencoderto
additionalmechanisms,namelyslidingwindowlocalatten-
processlongsequences.
tionandinterval-basedglobalattention,toeffectivelyhandle
BiACM Tofacilitatecommunicationbetweenthetexten-
3https://pypi.org/project/pdftotext/ coderandlayoutencoder,weincorporatetheBiACMlayer(a)LongFin (b)Local+GlobalAtention
Figure2:(a)ThearchitectureoftheLongFinmodel.Itmainlyconsistsoftwoencoders:textencoderandlayoutencoderwhichareconnected
through the BiACM layer. (b) A visualization of the employed local (sliding window) and global attention mechanisms to process long
sequences.
from the LiLT model (Wang, Jin, and Ding 2022). As de-
pictedinFigure2a,theBiACMlayeraddsthescoresresult-
ing from the multiplication of keys and queries from both
encoders.InLiLT,adetachoperationisappliedtothescores
generatedbythetextencoderbeforepassingthemtothelay-
out encoder. This detachment prevents the layout encoder
frombackpropagatingintothetextencoderduringpretrain-
ing, promoting better generalization when fine-tuning the
modelwithdifferentlanguagetextencoders.However,since
our focus is primarily on the English language for our ap-
plications, we have chosen to remove the detach operation
toexpeditepretraining,givenourlimitedcomputationalre-
sources.
Pretraining
To pretrain LongFin, we utilize the IIT-CDIP (Lewis et al.
2006) dataset which contains 11M scanned images that
make up 6M documents. We obtain the OCR annotations Figure 3: LongFinpretraininglosscurve.Thelossstartsat2.84
(words and their bounding boxes) from OCR-IDL (Biten andoscillatedbetween1.97and1.94nearconvergence.
et al. 2022) which used the AWS Textract API4. We ini-
tialize our text encoder from Longformer (Beltagy, Peters, andCohan2020),wepretrainthemodelfor65Kstepswith
andCohan2020)andourlayoutencoderfromLiLT(Wang, a learning rate of 3e-5 and batch size of 12 on one A100
Jin, and Ding 2022) layout encoder. Since the LiLT layout GPU.Wesetthewarmupstepsto500andusetheAdaFactor
encoder was pretrained on inputs with a maximum length optimizer(ShazeerandStern2018).Also,weutilizegradi-
of512tokens,wecopyLiLT’spretrainedpositionalembed- entcheckpointing(Chenetal.2016)toenableusingalarge
dingseighttimestoinitializeourlayoutencoderpositional batchsize.ThepretraininglosscurveisshowninFigure3
embeddings,whichconsistof4096embeddingvectors.This
enablesthelayoutencodertohandlelongersequenceswhile Experiments&Evaluation
leveraging the pretrained positional information from the
Tasks&Datasets
LiLTmodel.
For the pretraining of LongFin, we employ the Masked ToassessthegeneralizabilityofLongFinonbothshortand
Visual-LanguageModelingtask(Devlinetal.2019;Wang, long contexts, we evaluate LongFin on two existing short
Jin, and Ding 2022). In this task, 15% of the tokens in the (single-page) datasets: FUNSD (Guillaume Jaume 2019)
input to the text encoder are masked. In 80% of the cases, and CORD (Park et al. 2019) to show the generalizability
we replace the masked tokens with the [MASK] token. In ofourmodelonshortcontextsaswellasournewlycreated
10% of the cases, we replace the masked tokens with ran- LongFormsdataset.
dom tokens. In the remaining 10%, we keep the original • FUNSD: This dataset comprises 200 scanned forms and
tokenunchanged.InspiredbyLongformer(Beltagy,Peters, requiresmodelstoextractfourmainentities:headers,ques-
tions,answers,andotherrelevantinformation.Additionally,
4https://aws.amazon.com/textract/ it involves linking questions with their corresponding an-Model Modalities FUNSD CORD Model Modalities Precision Recall F1
(F1) (F1) ShortContextModels
ShortContextModels(512tokens) BERTBASE T 35.82 30.18 32.76
BERTBASE T 60.26 89.68 LiLTBASE T+L 35.55 43.24 39.02
RoBERTaBASE T 66.48 93.54
LongContextModels
LayoutLM T+L 79.27 94.72
BASE Longformer T 49.50 45.20 47.25
LayoutLMv2 T+L+V 82.76 94.95 BASE
BASE LongFin (ours) T+L 47.67 56.16 51.57
LayoutLMv3 T+L+V 90.29 96.56 BASE
BASE
DocFormerBASE T+L+V 83.34 96.33 Table3:ResultsofthedifferentmodelsonLongForms.Weeval-
LiLTBASE T+L 88.41 96.07
uateusingtheentity-levelF1score.
LongContextModels(4096tokens)
L (Po hn ag mfo erm tae lr .B 2A 02S 2E ) TT +L 7 71 7. .4 1 90 –. 641 Model Be Cgi an sn hing E Cnd asin hg Fin Ca an sc hial Ch Can ag shein Q Ku ea yrt ser AT so st ea tl s
LongFin BASE(ours) T+L 87.03 94.81 LiLTBASE 27.39 35.89 7.40 30.55 64.93 47.05
Table2:AccuracyofthedifferentmodelsonFUNSDandCORD
LongFinBASE(ours) 47.61 56.16 15.15 45.94 75.17 45.71
Table4:AblationsresultsofLiLTandLongFinontheLongForms
datasets. The second column shows the modalities used by each
datasetbyentity.
modelwhereTreferstoText,LreferstoLayout,andVrefersto
Vision.
very short textual content (less than 512 tokens). Notably,
LongFinalsoachievescomparableperformancetotheshort-
swers,therebyencompassingnamedentityrecognitionand
context models on these datasets. This comparison high-
relationextractiontasks.Wemainlyfocusonthenameden-
lights the superior generalization ability of our model,
tityrecognitiontaskandusetheentity-levelF1scoreasour
LongFin,whichperformswellonbothshortandlongcon-
evaluationmetric.
texts. In contrast, the performance of (Pham et al. 2022)
•CORD:Withover11,000receipts,thisdatasetfocuseson
modeldeterioratesonshort-contextdocuments.
extracting54differentdataelements(e.g.,phonenumbers)
from receipts. The task can be formulated as named entity
LongFormsDataset
recognition or token classification. For evaluation, we use
theentity-levelF1score. As presented in Table 3, the performance results on our
LongForms dataset highlight the advantage of our model,
Baselines
LongFin,comparedtotheshort-contextmodels.Thisobser-
To demonstrate the effectiveness of LongFin on our Long- vation emphasizes the significance of long-context under-
Formsdataset,wecompareitagainstasetofpubliclyavail- standing when working with financial documents. There is
able text and text+layout baselines that are capable of han- alsoanoticeabledifferenceinperformancebetweenthetext
dlingbothshortandlonginputsequences.Forthetextbase- models (BERT (Devlin et al. 2019) and Longformer (Belt-
lines,weselectthefollowingmodels:(i)BERT(Devlinetal. agy,Peters,andCohan2020))andtext+layoutmodels(LiLT
2019) which is a widely used text-based model known for (Wang, Jin, and Ding 2022) and LongFin). This is mainly
its strong performance on short context tasks (512 tokens), because the documents in LongForms contain diverse lay-
(ii)Longformer(Beltagy,Peters,andCohan2020)whichis outsthatmightbechallengingfortext-onlymodels.
specificallydesignedtohandletextlongtexts(upto4096to- To provide a deeper analysis of the results on the Long-
kens).Forthetext+layoutbaseline,weutilizeLiLT(Wang, Forms dataset, we conduct ablations and report metrics
Jin, and Ding 2022), which is one of the state-of-the-art by entity for both LiLT (Wang, Jin, and Ding 2022) and
modelsfordocumentunderstanding5.Fortheshortcontext LongFin,asshowninTable4.Wenoticethatthegapinper-
models,wesplittheLongFormsdocumentsintochunksthat formanceismoresignificantintheentitiesthataretypically
canfitwithin512tokens.Table5showsthehyperparameters foundinlongtablessuchasBeginningCash,EndingCash,
of the different models when finetuning on the LongForms FinancialCash,andChangeinCash.Toillustratethechal-
dataset.Italsopresentsthehyperparametersweusedwhen lenges posed by long tables, we present an examples from
finetuningLongFinontheprevioussingle-pagedatasets.All ourtestsetinFigure4.Intheexample,thetableheaderindi-
thefinetuningexperimentswereperformedononeA100and cates”NineMonths,”implyingthatthetableincludesinfor-
oneT4GPUs. mationforanine-monthperiodthatshouldnotbeextracted
as we are only interested in the financial information per
Results quarter ”Three Months”. Due to the large number of rows
andcontentinthetable,theshort-contextmodelsmaynotbe
Previous(Single-Page)Datasets
abletoincludeallthetableinformationinasingleforward
As shown in Table 2, LongFin outperforms other long- passof512tokens.Consequently,whenthelongdocuments
context models such as Longformer (Beltagy, Peters, and are split into chunks, such tables might be divided as well,
Cohan2020)and(Phametal.2022)onthepreviousdatasets leadingtotheshort-contextmodelslosingimportantcontext
that mainly consist of single-page documents. The perfor- whenmakingpredictions.
mance disparity is particularly pronounced on the FUNSD
dataset(GuillaumeJaume2019),wherealldocumentshave Limitations
5LayoutLMv3 (Huang et al. 2022) is another state-of-the-art Despite the effectiveness of our model, LongFin, on both
document understanding model, but its usage is limited to non- short and long context document understanding datasets, it
commercialapplications has a few limitations. First, LongFin was trained and eval-
6Thecodeof(Phametal.2022)isnotpubliclyavailable. uated on the English language only. In future, we plan toFigure 4: Page6fromanexampledocumentfromtheLongFormstestset.Sincetheoriginaldocumenthas6pageswhichcannotfitin
asingleforwardpassof512tokens,thedocumentissplitintoseveralchunks,leadingtoalossofimportantcontent.Forexample,inthis
tablefromthesixthpage,thecontextfromthetopiscrucialtodecidewhethertopickthenetchangeincashentityornot,sinceweareonly
interestedtoextractquarterinformation”Threemonths”periodsonly.
expand it to support multiple languages. Second, although Experiment Steps LearningRate BatchSize
LongFin maximum input length (4096 tokens) can accom- FinetuningontheLongFormsdataset
modatethemulti-pagedocumentsintheLongFormsdataset BERT(Devlinetal.2019) 10000 2e-5 4
as well as most our proprietary datasets, it might not ac- LiLT(Wang,Jin,andDing2022) 8000 2e-5 4
Longformer(Beltagy,Peters,andCohan2020) 6000 2e-5 4
commodatecertainfinancialdocumentsthatcontaintensof LongFin(Ours) 6000 2e-5 4
pages.Toovercomethislimitation,wemayconsiderfurther FinetuningLongFinonthepreviousdatasets
expandingthepositionalembeddingstoaccomodate16Kto- LongFinFUNSD 6000 2e-5 4
kenssimilartotheLEDmodel(Beltagy,Peters,andCohan LongFinCORD 6000 5e-5 8
2020)orexploreutlizingamodelarchitecturethatusesrel-
Table5:Trainingdetailsforfinetuningthedifferentmodels
ative position embeddings (Shaw, Uszkoreit, and Vaswani
ontheLongFormsdataset.Thelowersectionalsoshowsthe
2018)suchasT5(Raffeletal.2020)insteadoftheabsolute
hyperparametersusedinfinetuningLongFinontheprevious
position embeddings. Third, due to limited computational
single-pagedatasets.
resources,wehavenotexploredmanydifferenthyperparam-
eterssetup.Hence,theremightberoomforimprovementin
ourmodelperformance.Finally,whileourLongFormsshed
uments, while achieving comparable results on previous
thelightonlongcontextunderstandingchallengeswhichare
datasets consisting of single-page documents. Moving for-
frequent in the financial industry, it is still limited in size.
ward,ourplanistodeployLongFinaftertrainingitonour
We encourage the research community to explore this un-
proprietarydatasetsinthefinancedomain.Furthermore,we
derchartedareaofresearchsinceithasvariouscommercial
areworkingonextendingLongFintosupportdifferentlan-
applicationsinmanyindustriessuchasfinanceandlegal.
guages.
Conclusion
EthicalStatement
We introduce LongFin, a multimodal document AI model
designed to handle lengthy documents. Additionally, we All the documents used in our LongForms dataset is col-
presenttheLongFormsdataset,whichaimstoreplicatereal- lected from the EDGAR database which grants the right
world challenges in understanding long contexts, specif- to use and distribute their data without permissions 7. The
ically in the financial industry. Through our evaluation, datasetannotationprocesswereaccomplishedbydataanno-
we demonstrate the superior performance of LongFin on
the LongForms dataset, which comprises multi-page doc- 7https://www.sec.gov/privacy#disseminationtatorswhoarefairlycompensated.Weprovidethehyperpa- Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
rametersandexperimentalsetupsofourexperimentstoen- Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V.
surethereproducibilityofourwork.Moreover,themodels, 2019. RoBERTa:ARobustlyOptimizedBERTPretraining
LiLT(Wang,Jin,andDing2022)andLongformer(Beltagy, Approach. arXiv:1907.11692.
Peters, and Cohan 2020), on which our LongFin model is
Madl, T.; Xu, W.; Choudhury, O.; and Howard, M. 2023.
built are published under permissive licenses 89 that allow
Approximate, Adapt, Anonymize (3A): a Framework for
commercialuse.
Privacy Preserving Training Data Release for Machine
Learning. arXiv:2307.01875.
References
Mathew, M.; Karatzas, D.; and Jawahar, C. V. 2021.
Appalaraju, S.; Jasani, B.; Kota, B. U.; Xie, Y.; and Man- DocVQA: A Dataset for VQA on Document Images.
matha, R. 2021. DocFormer: End-to-End Transformer for arXiv:2007.00398.
DocumentUnderstanding. arXiv:2106.11539.
Park, S.; Shin, S.; Lee, B.; Lee, J.; Surh, J.; Seo, M.; and
Ba,J.L.;Kiros,J.R.;andHinton,G.E.2016. LayerNor- Lee, H. 2019. CORD: A Consolidated Receipt Dataset for
malization. arXiv:1607.06450. Post-OCRParsing.
Beltagy,I.;Peters,M.E.;andCohan,A.2020. Longformer: Pham,H.;Wang,G.;Lu,Y.;Floreˆncio,D.A.F.;andZhang,
TheLong-DocumentTransformer. arXiv:2004.05150. C. 2022. Understanding Long Documents with Different
Biten,A.F.;Tito,R.;Gomez,L.;Valveny,E.;andKaratzas, Position-AwareAttentions. ArXiv,abs/2208.08201.
D. 2022. Ocr-idl: Ocr annotations for industry document Pramanik, S.; Mujumdar, S.; and Patel, H. 2022. To-
librarydataset. arXivpreprintarXiv:2202.12985. wards a Multi-modal, Multi-task Learning based Pre-
Chen,T.;Xu,B.;Zhang,C.;andGuestrin,C.2016.Training trainingFrameworkforDocumentRepresentationLearning.
DeepNetswithSublinearMemoryCost.arXiv:1604.06174. arXiv:2009.14457.
Devlin,J.;Chang,M.-W.;Lee,K.;andToutanova,K.2019. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
BERT:Pre-trainingofDeepBidirectionalTransformersfor Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Explor-
LanguageUnderstanding. arXiv:1810.04805. ingtheLimitsofTransferLearningwithaUnifiedText-to-
Text Transformer. Journal of Machine Learning Research,
Guillaume Jaume, J.-P. T., Hazim Kemal Ekenel. 2019.
21(140):1–67.
FUNSD: A Dataset for Form Understanding in Noisy
ScannedDocuments. InAcceptedtoICDAR-OST. Shaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-
Attention with Relative Position Representations. In Pro-
Huang, Y.; Lv, T.; Cui, L.; Lu, Y.; and Wei, F. 2022. Lay-
ceedings of the 2018 Conference of the North American
outLMv3:Pre-TrainingforDocumentAIwithUnifiedText
Chapter of the Association for Computational Linguistics:
andImageMasking. InProceedingsofthe30thACMInter-
Human Language Technologies, Volume 2 (Short Papers),
national Conference on Multimedia, MM ’22, 4083–4091.
464–468.NewOrleans,Louisiana:AssociationforCompu-
New York, NY, USA: Association for Computing Machin-
tationalLinguistics.
ery. ISBN9781450392037.
Shazeer,N.;andStern,M.2018.Adafactor:AdaptiveLearn-
Huang, Z.; Chen, K.; He, J.; Bai, X.; Karatzas, D.; Lu,
ingRateswithSublinearMemoryCost. arXiv:1804.04235.
S.; and Jawahar, C. V. 2019. ICDAR2019 Competition on
ScannedReceiptOCRandInformationExtraction. In2019 Tang, Z.; Yang, Z.; Wang, G.; Fang, Y.; Liu, Y.; Zhu, C.;
InternationalConferenceonDocumentAnalysisandRecog- Zeng, M.; Zhang, C.; and Bansal, M. 2023. Unifying Vi-
nition(ICDAR).IEEE. sion,Text,andLayoutforUniversalDocumentProcessing.
arXiv:2212.02623.
Kim, G.; Hong, T.; Yim, M.; Nam, J.; Park, J.; Yim, J.;
Hwang,W.;Yun,S.;Han,D.;andPark,S.2022. OCR-free Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
DocumentUnderstandingTransformer. arXiv:2111.15664. L.;Gomez,A.N.;Kaiser,L.;andPolosukhin,I.2017. At-
tentionIsAllYouNeed. arXiv:1706.03762.
Lee,K.;Joshi,M.;Turc,I.;Hu,H.;Liu,F.;Eisenschlos,J.;
Khandelwal, U.; Shaw, P.; Chang, M.-W.; and Toutanova, Wang, J.; Jin, L.; and Ding, K. 2022. LiLT: A Simple
K. 2023. Pix2Struct: Screenshot Parsing as Pretraining for yet Effective Language-Independent Layout Transformer
VisualLanguageUnderstanding. arXiv:2210.03347. for Structured Document Understanding. In Proceedings
of the 60th Annual Meeting of the Association for Compu-
Lewis,D.;Agam,G.;Argamon,S.;Frieder,O.;Grossman,
tational Linguistics (Volume 1: Long Papers), 7747–7757.
D.;andHeard,J.2006. BuildingaTestCollectionforCom-
Dublin,Ireland:AssociationforComputationalLinguistics.
plex Document Information Processing. In Proceedings of
the 29th Annual International ACM SIGIR Conference on Xu, Y.; Li, M.; Cui, L.; Huang, S.; Wei, F.; and Zhou, M.
ResearchandDevelopmentinInformationRetrieval,SIGIR 2020. LayoutLM: Pre-Training of Text and Layout for
’06, 665–666. New York, NY, USA: Association for Com- Document Image Understanding. In Proceedings of the
putingMachinery. ISBN1595933697. 26th ACM SIGKDD International Conference on Knowl-
edgeDiscovery&DataMining,KDD’20,1192–1200.New
8https://github.com/allenai/longformer York, NY, USA: Association for Computing Machinery.
9https://github.com/jpWang/LiLT ISBN9781450379984.Xu, Y.; Xu, Y.; Lv, T.; Cui, L.; Wei, F.; Wang, G.; Lu, Y.;
Florencio,D.;Zhang,C.;Che,W.;Zhang,M.;andZhou,L.
2021. LayoutLMv2:Multi-modalPre-trainingforVisually-
rich Document Understanding. In Proceedings of the 59th
Annual Meeting of the Association for Computational Lin-
guisticsandthe11thInternationalJointConferenceonNat-
uralLanguageProcessing(Volume1:LongPapers),2579–
2591.Online:AssociationforComputationalLinguistics.
Zhang,R.;Yang,W.;Lin,L.;Tu,Z.;Xie,Y.;Fu,Z.;Xie,Y.;
Tan, L.; Xiong, K.; and Lin, J. 2020. Rapid Adaptation of
BERTforInformationExtractiononDomain-SpecificBusi-
nessDocuments. arXiv:2002.01861.