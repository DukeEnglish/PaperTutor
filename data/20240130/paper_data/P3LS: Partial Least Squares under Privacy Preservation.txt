Graphical Abstract
P3LS:PartialLeastSquaresunderPrivacyPreservation
NguyenDuyDu,RaminNikzad-Langerodi
4202
naJ
62
]LM.tats[
1v48841.1042:viXraHighlights
P3LS:PartialLeastSquaresunderPrivacyPreservation
NguyenDuyDu,RaminNikzad-Langerodi
• Highlight 1: We developed an approach that
allows multiple companies along a value chain
to collaboratively construct and exploit a cross-
organizational process model while protecting
theirdataprivacyandsecurity.
• Highlight2: Weproposedanincentivemechanism
thatquantifiesparticipants’contributions,thuscre-
ating a basis for a profit-sharing scheme and mo-
tivating the companies to join the federation and
contributehigh-qualitydata.
• Highlight 3: We experimented on synthetic
datasets to evaluate the effectiveness of the pro-
posed method. The results show that our method
outperformedthelocalmodelandyieldedthesame
performanceastheconventionalapproachofcen-
tralizingdata.P3LS: Partial Least Squares under Privacy Preservation
NguyenDuyDua,RaminNikzad-Langerodia,∗
aSoftwareCompetenceCenterHagenberg,Austria
Abstract
Modern manufacturing value chains require intelligent orchestration of processes across company borders in
order to maximize profits while fostering social and environmental sustainability. However, the implementation
of integrated, systems-level approaches for data-informed decision-making along value chains is currently ham-
pered by privacy concerns associated with cross-organizational data exchange and integration. We here propose
Privacy-PreservingPartialLeastSquares(P3LS)regression,anovelfederatedlearningtechniquethatenablescross-
organizationaldataintegrationandprocessmodelingwithprivacyguarantees. P3LSinvolvesasingularvaluedecom-
position(SVD)basedPLSalgorithmandemploysremovable,randommasksgeneratedbyatrustedauthorityinorder
toprotecttheprivacyofthedatacontributedbyeachdataholder. WedemonstratethecapabilityofP3LStovertically
integrateprocessdataalongahypotheticalvaluechainconsistingofthreepartiesandtoimprovethepredictionper-
formanceonseveralprocess-relatedkeyperformanceindicators. Furthermore,weshowthenumericalequivalenceof
P3LSandPLSmodelcomponentsonsimulateddataandprovideathoroughprivacyanalysisoftheformer.Moreover,
weproposeamechanismfordeterminingtherelevanceofthecontributeddatatotheproblembeingaddressed,thus
creatingabasisforquantifyingthecontributionofparticipants.
Keywords: Cross-organizationalProcessModeling,PartialLeastSquaresRegression,PrivacyPreservingMachine
Learning,VerticalFederatedLearning
1. Introduction also forms the basis of various (non-) corporate digi-
tal passport initiatives, e.g., the European Blockchain
Value chains involving sequential manufacturing Partnership(EBP).WiththeupcomingEUBatteryreg-
stagesoperatedbydifferentcompanieshavebecomein- ulation that calls for a mandatory digital battery pass-
creasingly prevalent, e.g., in the automotive, semicon- port by 2026, the pace for implementing digital pass-
ductor, pharmaceutical, and waste management indus- ports is also picking up from the regulatory side [3].
try[1].Thetransitionawayfrom(petroleum-based)vir- Although digital passports have been promoted as the
gin-andtowardsincreasinguseofrecycled-feedstock key enabler of the circular economy and successfully
furtherincreasesthemutualinteractionanddependence implemented to realize circular business models, they
of the involved processes. Thus, achieving consistent come with several limitations [4]. First and foremost,
product characteristics requires closer collaboration of digitalpassports,whetherDLT-basedandequippedwith
thestakeholdersalongvaluechainstobeabletodynam- encryption technology to protect sensitive data or not,
icallyadaptthecorrespondingprocessestoincreasingly alwayscontainonlythosepiecesofinformationthatare
variableinputstreams. a priori deemed relevant for process and value chain
Distributed ledger technology (DLT) has become optimization. However,itisnotalwaysclearwhichma-
the technology of choice for trustworthy, cross- terial/process/product specifications have an impact on
organizational information exchange and has been downstream processes, and thus, it can be challenging
adoptedinseveralindustriessuchasmanufacturing,au- to determine which information should be included in
tomotive, real estate, energy, or healthcare [2]. DLT digitalpassportstoensuretheirmaximumeffectiveness
insupportingacirculareconomyandsustainabledevel-
opmentgoals[4].
∗Correspondingauthor
With the ever-increasing complexity of industrial
Emailaddress:ramin.nikzad-langerodi@scch.at(Ramin
Nikzad-Langerodi) processes and amounts of data produced along value
PreprintsubmittedtoJournalofProcessControl January29,2024chains, AI and big data analytics have become obvi- between process variables and some specific response
ous tools to help reveal such interactions across com- variables (e.g., product quality, KPIs, etc.) is not of
pany borders. Joint analysis of (process) data can pro- primary interest. Furthermore, previous methodology
vide stakeholders with a more comprehensive view of lack mechanisms to incentivize data federation, which
the entire value chain, including the relationships be- ispivotaltofosteringcollaborationamongstakeholders
tween different process stages and the factors that im- alongthevaluechain. Inparticular,participantsshould
pactthesustainabilityandcircularityofthesystemasa be rewarded based on their contribution to the overall
whole. However, despite the overwhelming consensus outcomes (e.g., cost savings) resulting from data fed-
that cross-organizational information exchange is im- eration. Accessing contribution based merely on data
perative for increasing efficiency, resilience as well as quantityiscertainlynotenough,asonepartymaycon-
socialandenvironmentalsustainabilityoftheprocesses tributealargevolumeofdatathatdoesn’thelpmuchin
involvedalongvaluechains,privacyconcernsrelatedto solvingtheproblem. Recentstudieshaveproposedthe
datasharingstillprevailamongtherelevantstakehold- useoftheShapleyvalueinevaluatingdatacontribution
ers. [9][10]. However,sincetheShapleyvaluehasanexpo-
Federated learning (FL) is a relatively new machine nentialtimecomplexity,thisapproachposessignificant
learning paradigm, first proposed by researchers at inefficiency[11].
Googlein2016,thataddressesprivacyandsecuritycon- To overcome the crucial shortcomings of MPCA-
cerns associated with centralized data storage and pro- basedMPSC,wehereproposePrivacy-preservingPar-
cessing. FLallowsmultiplepartiestojointlytraindata- tialLeastSquares(P3LS).P3LSisafederatedvariantof
drivenmodelswhilepreservingdataprivacy–eitherby PartialLeastSquares(PLS),apopularsupervisedtech-
entirelyavoidingdataexchangeorbymeansofdataen- nique for process modeling [12][13]. P3LS leverages
cryption[5]. VerticalFederatedLearning(VFL)refers Federated Singular Vector Decomposition (FedSVD)
to scenarios where the private datasets share the same [14]toenabledifferentcompaniestojointlytrainaPLS
sample space but differ in the feature space. Cross- modelwhileprotectingprivatedatasets. Tocreateaba-
organizational processes naturally fit this scenario be- sisforincentivemechanisms,wealsoproposeamethod
cause even though local datasets can be mapped using forsecurelyquantifyingthecontributionofparticipants
the product or batch IDs, the local features are often basedonthevarianceexplainedbythecontributeddata.
distinct because each company usually operates a dif- Inthenextsection, wefirstcoverthebackgroundof
ferenttypeofsub-process. Althoughsomeresearchef- the proposed method, including the concepts of PLS,
forts have been devoted to adopting VFL in the field FL, and FedSVD. In Section 3, the methodology and
of process modeling, limited progress has been made. implementation of P3LS are described in detail. We
In [6] [7], the authors have proposed federated vari- thenprovideasecurityanalysisinSection4anddiscuss
antsofPrincipalComponentAnalysis(PCA),whichis potential real-world applications of P3LS in manufac-
awidelyusedmethodforfaultdetectionanddiagnosis. turing in Section 5. After that, in Section 6, we pro-
Nevertheless, in these studies, the application of PCA videanempiricalevaluationofP3LSonsimulateddata
to process monitoring was not investigated. In [8], Du when its performance is compared with local and cen-
etal. recentlyproposedamultiblockprincipalcompo- tralized PLS models. Finally, conclusions drawn from
nentanalysis(MPCA)basedfederatedmultivariatesta- the results and potential topics for future research and
tistical process control (FedMSPC) approach to jointly developmentarepresentedinSection7.
model a semiconductor fabrication process over multi-
plestagesthatareoperatedbydifferentcompanies. Not
2. Background
only was the joint model more efficient (compared to
themodeltrainedonthedownstreamprocessonly)for 2.1. Preliminaries
predictingprocessfaults,butitalsorevealedimportant Weuseboldfacedlower-caseletters,e.g.,x,todenote
interactions between the two process stages that led to vectorsandboldfacedupper-caseletters,e.g., X,tode-
faulty batches in the downstream process. This study note matrices. The subscript is associated with indices
isamongthefirsttodemonstratetheprospectsof(ver- of different parties, e.g., the matrix X denotes the ith
i
tical) federated data analysis and process modeling for party’sdata.
collaborative risk identification, decision-making, and
valuechainoptimization. 2.2. PartialLeastSquares
However,asanunsupervisedtechnique,FedMSPC’s Supposetheprocessdataofmsamples(e.g.,batches)
applicationislimitedtoscenarioswheretherelationship and n variables is denoted as X ∈ Rm×n, and the cor-
2responding response variables (e.g., critical quality at- whereprivatedatasetssharethesamesamplespacebut
tributesofthefinalproduct)isstoredinY ∈ Rm×l. The differ in the feature space. The difference between the
generalunderlyingmodelofPLSappliedonXandYis: two scenarios is demonstrated in Figure 1. As noted
by[23][11][24],mostofthecurrentresearchinthefield
X =TP⊤+Θ ofprivacy-preservingmachinelearningconcentrateson
Y =UQ⊤+Φ, thehorizontalschema,whiletheverticalsettinghasre-
ceivedinadequatedevotion.
where
• T ∈ Rm×k and U ∈ Rm×k are, respectively, the X
scoresandtheYscores.
• kisthenumberoflatentvariables(LVs),oftende-
terminedbymeansofcross-validation[15].
• P ∈ Rn×k and Q ∈ Rl×k are, respectively, X load-
ingsandYloadings.
• Θ∈Rm×nandΦ∈Rm×laretheresidualsmatrices.
Figure1:FederatedLearningcategorization.
The decompositions of X and Y are made in a way
that maximizes the covariance between T and U. Dif-
2.4. FederatedSingularVectorDecomposition
ferent implementations of PLS exist, among which the
mostpopularareNIPALS[16],SIMPLS[17],andSin- FedSVD,proposedin[14],isalosslessapproachfor
gular Vector Decomposition (SVD) [18][19]. In this securely conducting SVD on distributed data without
study, we propose a privacy-preserving variant of PLS datacentralization.
thatleveragesafederatedversionofSVD.Adescription Suppose H ∈ Rm×m andG ∈ Rm×m aretworandom,
ofSVD-basedPLSisprovidedinAlgorithm1. orthogonalmatrices. Foranymatrix X ∈Rm×n,itholds
Afterthemodelistrained,itcanbeusedtocalculate
thatX′ = H⊤XGhasthesamesingularvaluesasX,and
scoresandpredicttheoutputforunseendataX : theirsingularvectorscanbetransformedintoeachother
new
byalineartransformation,i.e.,
T = X R
new new
w= Hw′
Yˆ = X B (1)
new new v=Gv′,
The PLS algorithms commonly require centralized
with w and v, and w′ and v′ denoting the first left and
data, which poses challenges in cross-organizational
rightsingularvectorsofXandX′,respectively.
processes where each company possesses data only
In order to operate, FedSVD requires a trusted au-
fromitsownproductionline. Privacyandsecuritycon-
thority (TA) to handle the key generation (i.e., matri-
cerns further restrict the sharing of data among com-
cesHandG)andacomputationserviceprovider(CSP)
panies. While some variations of PLS have aimed to
toperformdataaggregationandmatrixdecomposition.
handlemulti-sourcedata[20][21],theystillassumefull
Thedetaileddescriptionsofthemethodcanbefoundin
dataaccessibility,failingtoaddressthecriticalissueof
[14]. Briefly,FedSVDhasfourmainsteps:
dataprivacy.
• Step1: TheTAgeneratesrandomorthogonalma-
2.3. FederatedLearning
tricesandcreatessharedandprivatekeys.
FL is a collaborative learning paradigm that allows
• Step2:Eachdataholderdownloadsitsproprietary
multipleclientstojointlytrainaglobalmachinelearn-
keyandmaskstheprivatedata.
ingmodelwithoutrevealingprivatedata[22]. Depend-
ingonhowdataisdistributedamongtheclients,FLcan • Step3:TheCSPaggregatesthemaskeddatausing
begenerallycategorizedintotwosettings,namelyHori-
asecureaggregationprotocol,performsSVD,and
zontalFederatedLearning(HFL)andVerticalFederated
obtainsmaskedsingularvectors.
Learning(VFL).HFLreferstoscenarioswhereprivate
datasets share the same feature space but differ in the • Step4: ThedataholderscollaboratewiththeCSP
sample space. Conversely, VFL consists of situations torecovertherealresultsusingitsgivenkeys.
3Algorithm1:PLSTraining
Input: X,Y
Output: T,Q,W, P,B,R
1 FunctionPLS.train(X,Y,k):
2 Initializetwomatrices: E:= X,andF:=Y
3 for j=1→kdo
4 Calculatethecross-productmatrixS: S= E⊤F.
5 PerformSVDonSandextractthefirstleftandrightsingularvectors,wandv.
6 Calculatescores t andu:
t = Ew, u= Fv
7 Obtainloadingsbyregressingagainstthesamevector t:
E⊤t F⊤t
p= , q=
t⊤t t⊤t
8 DeflatethedatamatricesEandF:
E= E−tp⊤, F= F−tq⊤
9 Savew, t, p,andq.
10 end
11 Usevectorsw, t, pandqascolumnstoformmatricesW,T, PandQ,respectively.
12
Computetherotationmatrix(denotedasR)andregressioncoefficients(denotedasB):
R=W(P⊤W)−1, B= RQ⊤
13 End
Theapproachwasprovedtobeconfidentialunderthe (batches)andn isthenumberofprivatefeatures. And
i
semi-honestsettingwheretheTAisfullytrustedwhile let Y ∈ Rm×l represent the labels of the m samples.
the CSP and data holders are semi-honest [14]. More Withoutlossofgenerality,weassumethatYisprivately
specifically, it assumes that the CSP and data hold- ownedbythegthdataholder.
ers will strictly follow the pre-specified protocol, even
though they may try to deduce the private knowledge Basedontheircontribution,wecategorizedatahold-
of others. In addition, The security protocol also re- ersintotwotypes: FeatureContributor(FC)andLabel
quirestheabsenceofcollusionamongtheparticipants. Contributor(LC).Adataholderthatcontributesonlya
Collusion denotes any secret or illicit agreement be- feature block (i.e., X i) is designated as a Feature Con-
tween entities aiming to compromise the system’s se- tributor. On the other hand, a data holder that con-
curity. Forinstance,collusionmightoccuriftheTAor tributes to the target block (i.e., Y) is referred to as a
a data holder shares encryption keys with the CSP, al- LabelContributor. Afeaturecontributormayalsoplay
lowingthedecryptionofmaskeddataormaskedmodel theroleofalabelcontributor. Whiletherecanbemulti-
components,therebyunderminingsecurity. plelabelcontributors,thepresentworkfocusesonsce-
narioswherethereisonlyonesingleY blockfromone
label contributor. An extension to handle multi-label-
3. Privacy-PreservingPartialLeastSquares
contributor applications will be investigated in future
work.
3.1. Participantsandroles
The hypothetical scenario involves g data holders, Besides data holders, P3LS also requires the pres-
each holding a partition of features of the same sam- enceofatrustedauthorityandacentralserviceprovider.
ples. Let X
i
∈ Rm×ni denotes the features owned by Their functions within the system closely mirror the
the ith data holder, where m is the number of samples rolestheyplayinFedSVD.
43.2. Overallworkflow • Model decryption: The data holders collaborate
withtheTAandtheCSPtorecovertherealmodel
Suppose the data holders in the data federation aim componentsusingbothsharedandprivatekeys.
at training a PLS model on the concatenated matrix
X = [X ,X ,...,X ]andthematrixY,where X ∈ Rm×n The pseudo-code of P3LS is shown in Algorithm
1 2 g
and n = (cid:80)g n. Assume that the matrices have been 2. The following subsections will elaborate on the
i=1 i
standardized to have zero means and unit variance. In specificsofeachstep.
this case, the P3LS decomposition results are the fol-
lowing: 3.3. Securedataaggregation
The TA generates two random orthogonal matrices
A ∈ Rm×m, H ∈ Rn×n, G ∈ Rl×l using the block-based
[X ,...,X ]=T[P⊤,...,P⊤]+[Θ ,...,Θ ] (2)
1 g 1 g 1 g efficientmaskgenerationmethod[14],andthenitsplits
Y =UQ⊤+Φ (3) H⊤into[H⊤,...,H⊤]whereH⊤ ∈Rn×ni.
1 g i
FC-i downloads A and H from the TA, masks their
i
a
sA hac rc eo drd ri en sg ul ly t, aF mC o- ni gge at ls
l
pX ai rt= iciT paP ni⊤ ts,+ aΘ ndi, Pw ⊤he ∈re RT ni×is
k
p thr eiv Cat Se PX .- Ab tlo tc hk es su ac mh eth tia mt eX ,i′ th= eA LX CiH doi, wa nn ld oas den sd Gs X fri′ omto
i
andΘ i aretheprivateresults. Meanwhile,sincetheLC theTAandmaskstheY-blockthroughY′ = AYG,and
isthesolecontributoroftheY-block,itmustbetheonly sendsY′totheCSP.
party who has access to U, Q, and Φ. Additionally, The CSP aggregates X′ which is the sum of all X′.
i
in order to make predictions for new samples, all FCs Accordingtotheruleofmatrixblockmultiplication,we
mustalsoknowtheirlocalcoefficients(i.e., B i,seeAl- have:
gorithm1). Table1summarizesthesharedandprivate
components of the P3LS model. It is strictly required
(cid:88)g (cid:88)g
X′ = X′ = AXH = A[X ,...,X ][H⊤,...,H⊤]⊤
that FC-i’s contributed data and private model compo- i i i 1 g 1 g
i=1 i=1
nents cannot be leaked to any other parties during the
= AXH
computation.
Afterthisstep,eventhoughtheCSPreceives X′ and
Table1:SharedandprivatecomponentsoftheP3LSmodel. Y′, it has been proved that the CSP cannot recover the
Shared originaldata[14].
Role Privatecomponents
components
X i(Localfeatures), 3.4. Sequentialmatrixdecomposition
P (LocalX-loadings),
FC-i i Duringthisstep,thelatentvariablesaresequentially
T(X-scores) B (Localcoefficients),
i estimated. Inwhatfollows,weoutlinethemethodology
Θ (LocalX-residuals)
i involvedinthisprocessanddefinetherelationsbetween
U(Y-scores),
theoutcomesofP3LSandPLS.Asdescribedin2,inthe
Q(Y-loadings),
LC firstiteration,wehave
Y(Targetblock),
Φ(Y-residuals)
E′ := X′ = AEH
F′ :=Y′ = AFG
Overall,P3LShasthefollowingthreesteps:
withthecross-productmatrix
• Securedataaggregation: TheTAgeneratescom-
monandprivatekeys,whicharerandomorthogo- S′ = E′⊤F′ = H⊤E⊤A⊤AFG.
nalmatrices. Eachdataholderdownloadsitspro-
prietary key, masks their local data using the key, Since Aisanorthogonalmatrix, A⊤A= I,therefore
andthenforwardstheencrypteddatatotheCSP.
S′ = H⊤E⊤FG= H⊤SG.
• Sequential matrix decomposition: The CSP ag-
gregates the encrypted data, performs the stan- Note that both H⊤ and G are orthogonal matrices.
dard SVD-based PLS algorithm, and obtains all Thus, according to Equation 1, the results of perform-
encryptedmodelcomponents. ingSVDonSandS′areconvertible.
5Algorithm2:P3LSTraining
Input: X =[X ,...,X ],Y
1 g
Output: T,Q,W =[W ,...,W ], P=[P ,...,P ],B=[B ,...,B ]
1 g 1 g 1 g
1 FunctionP3LS.train([X 1,...,X g],Y,k):
2 TAdo:
3 Generateorthogonalmatrices A∈Rm×m,H∈Rn×n,G∈Rl×l.
4
SplitH⊤into[H 1⊤,...,H g⊤]whereH i⊤ ∈Rn×ni.
5 end
6 FeatureContributordo:
7
fori=1→g,FeatureContributorido
8 Download A,H i⊤fromTAandcomputeX i′ = AX iH i
9
SendX′totheCSP
i
10 end
11 end
12 LabelContributordo:
13 DownloadGfromtheTAandcomputesY′ = AYG
14
SendY′totheCSP
15 end
16 CSPdo:
17
ReceiveX′andY′,thenaggregateX′:
i
(cid:88)g
X′ = X′
i
i=1
18
AssignX′andY′toE′andF′respectively: E′ := X′,andF′ :=Y′
19 for j=1→kdo
20
Calculatethecross-productmatrixS′: S′ = E′⊤F′.
21
PerformSVDonS′andextractthefirstleftandrightsingularvectors,w′andv′.
22 Calculatescores t′andu′:
t′ = E′w′, u′ = F′v′
23 Obtainloadingsbyregressingagainstthesamevector t′:
E′⊤t′ F′⊤t′
p′ = , q′ =
t′⊤t′ t′⊤t′
24
DeflatethedatamatricesE′andF′:
E′ = E′−t′p′⊤, F′ = F′−t′q′⊤
25
Savew′, t′, p′,andq′.
26 end
27
Usevectorsw′, t′, p′andq′ascolumnstoformmatricesW′,T′, P′andQ′,respectively.
28
Computethemaskedrotationmatrixandregressioncoefficients:
R′ =W′(P′⊤W′)−1
B′ = R′Q′⊤
29 end
30 FCsandtheLCcollaboratewiththeCSPtorecoverrealmodelcomponentsusingAlgorithm3.
31 End
6Next,wecalculateX-scoresandY-scores. Since H⊤ 3.5. Decryption
andGareorthogonalmatrices,HH⊤ = IandGG⊤ = I. A challenge that P3LS faces is how to remove the
maskssecurelyfromtheencryptedresultscomputedby
t′ = E′w′ = AEHH⊤w= AEw= At
theCSP.MotivatedbyFedSVD,weproposeAlgorithm
u′ = F′v′ = AFGG⊤v= AFv= Au 3torecovertherealresultsofP3LS.
SinceallFCshavethekeys A,theycandownloadT′
TheX-scores t′arenormalised: and get the real T using Equation 4. Similarly, the LC
t′ At At cangetQ′ fromtheCSPandrecoverQusingEquation
t′ = √ = √ = √ . 7. Ontheotherhand, sinceFC-ionlypossessesapor-
t′⊤t′ t⊤A⊤At t⊤t
tionofH,itcanrecoverW, P,andB through:
i i i
Next, X- and Y-loadings are obtained by regressing
W = HW′ (10)
E′andF′againstthesamevector t′: i i
P = H P′ (11)
i i
p′ = E′⊤t′ = H⊤E⊤A⊤At = H⊤E⊤t = H⊤p B = HB′G⊤ (12)
i i
q′ = F′⊤t′ =G⊤F⊤A⊤At = H⊤F⊤t =G⊤q
However, during the computation, we want to guar-
Finally,E′andF′aredeflated: antee the confidentiality of H i, G, W′, P′, Q′, and B′,
i.e., theFCsshouldnotdirectlygetW′, P′, Q′, and B′
E′
i+1
= E′
i
−t′p′⊤ = AE iH− Atp⊤H= A(E i−tp⊤)H andtheCSPshouldnotbeabletolearnH iandG. Ifthe
= AE i+1H CSPknew H i andG,theycouldeasilyestimateW i, P i,
Q,andB.
F′ i+1 = F′ i −t′q′⊤ = AF iG− Atq⊤G= A(F i−tp⊤)G Weproi posetofirstmask H i ∈ Rni×n usingalocally-
= AF i+1G generatedrandommatrixC
i
∈Rni×ni accordingtoEqua-
tion 13. Next, FC-i sends [H i]Ci (i.e., the masked
The estimation of the next latent variables then can matrix) to the CSP, which will subsequently compute
start from the SVD of the cross-product matrix S′ i+1 = [W i]Ci according to Equation 14 and send [W i]Ci back
E′ i+⊤ 1F′ i+1 = H⊤S i+1G. Aftereveryiteration,thevectors to FC-i. Then FC-i can remove the random mask ac-
w′, t′, p′ and q′ aresavedascolumnstoformmatrices cordingtoEquation15andgetthefinalresult(i.e.,W).
i
W′,T′, P′andQ′,respectively.
TheX-rotationmatrixR′isdefinedas: [H]C =CH (13)
i i i
R′ =W′(P′⊤W′)−1
[W i]C =[H i]CiW′(=C iW i) (14)
= H⊤W(P⊤HH⊤W)−1
W
i
=C i−1[W i]Ci (15)
= H⊤W(P⊤W)−1 Thissameapproachcanbeappliedtorecover P i.
= H⊤R It is more complicated when it comes to recovering
B since B is protected by two keys, one of which is
i i
Meanwhile,thecoefficientmatrixB′isdefinedas: known only to the LC (.i.e, G). Our idea is to replace
GwithacommonkeyknowntoallFCs,thenperforma
B′ = R′Q′⊤ = H⊤RQ⊤G= H⊤BG similarprocedureasrecoveringW i. First,theTAgener-
ates a random matrix N ∈ Rn×n. Next, the LC down-
In summary, the relationships between the PLS and loads N and masks G⊤ through [G⊤]N = G⊤N, and
P3LSmodelcomponentsare: thensends[G⊤]N totheCSP.Then,theCSPcalculates
[B i]Ci bytheoperation[B i]Ci = [H i]CiB′[Q⊤]N. Math-
ematically, [H i]CiB′[Q⊤]N = C iH iB′G⊤N = C iB iN.
T = A⊤T′ (4) After that, the CSP sends [B i]Ci to FC-i. Finally, FC-
W = HW′ (5) i downloads N from the TA and recovers B i through
P= HP′ (6)
B
i
=C i−1[B i]CiN−1.
Q=GQ′ (7) 3.6. Incentivemechamisms
R= HR′ (8) Incentive mechanisms are pivotal in real-world ap-
B= HB′G⊤ (9) plications,topromoteparticipantengagement,enhance
7Algorithm3:P3LSRecover
1 FunctionP3LS.recover():
2 TAdo:
3 GeneratearandommatrixN ∈Rl×l
4 end
5 FeatureContributordo:
6
fori=1→g,FC-ido
7 GeneratearandommatrixC i ∈Rni×ni
8 MaskH ithrough: [H i]Ci =C iH i
9 Send[H i]Ci totheCSP
10 end
11 end
12 LabelContributordo:
13 DownloadNfromtheTA
14
MaskG⊤through: [G⊤]N =G⊤N
15 Send[G⊤]N totheCSP
16 end
17 CSPwaittoreceivedatathendo:
18 if Receive[H i]C then
19 Computethefollowing:
[W i]Ci =[H i]CiW′, [P i]Ci =[H i]CiP′, [B i]Ci =[H i]CiB′[Q⊤]N
20 Send[W i]Ci,[P i]Ci,and[B i]Ci toFC-i
21 end
22
SendQ′onlytotheLC
23 end
24 LabelContributordo:
25 RecoverQby: Q=GQ′
26 end
27 FeatureContributordo:
28
fori=1→g,FC-ido
29 DownloadNfromtheTA
30 Get[W i]C,[P i]C,and[B i]C fromtheCSP
31 RecoverW i, P i,andB ithrough:
W
i
=C i−1[W i]Ci, P
i
=C i−1[P i]Ci, B
i
=C i−1[B i]CiN−1
32 end
33 end
34 End
collaboration, and ensure the truthful sharing of data. theamountofvariancepresentinthedatablock X that
i
Intuitively, data providers with high contributions de- can be described by the latent variables T within the
serve a better payoff. Considering only the volume of P3LS model. This calculation is derived from the X-
contributeddataiscertainlynotsufficientbecausedata loadings matrix P and can be expressed through the
i
quality is equally essential as data quantity in building followingequation:
robustandeffectivemodels.
SS(P) SS(P)
Theexplainedvariance,representedasR2 i,quantifies R2 Xi = SS(Xi ) = (m−1i )n (16)
8Similarly,theexplainedvarianceinY canbequanti- Algorithm4:VarianceexplainedinYbyX
i
fiedbythefollowingequation:
1 FunctionP3LS.recover():
SS(Q) SS(Q) 2 TAdo:
R2 Y = SS(Y) = (m−1)l (17) 3 Generatetwoorthogonalmatrices
M ∈Rm×mandN ∈Rl×l
where SS(·) denotes the sum of squares of the term 4 end
inparentheses[15]. Sincemandlareknowntoalldata 5 FeatureContributordo:
holders,theaboveequationscanbecalculatedlocally.
6
fori=1→g,FC-ido
Toevaluatetherelevanceofdata,itisintuitivetoex- 7 Download MandNfromtheTA
aminehowwelleachdatablock(i.e., X i)isabletopre- 8 CalculateYˆ ithrough: Yˆ i = X iB i
d seic st st ph re edta icrg tie vt eb plo oc wk er(i i.e n. v, oY lv) e. sT dh ee tes rt man ind ia nr gd hw oa wy t mo ua cs h- 9 MaskYˆ ithrough: Yˆ i′ = MYˆ iN
10
SendYˆ′totheCSP
varianceinY canbeaccountedforby X. Algorithm4 i
outlinesthestepstocomputethismetric.i 11 end
Mathematically, R2 can be computed through the 12 end
followingequation: XiY 13 LabelContributordo:
14 Download MandNfromtheTA
SS(Y−Yˆ ) SS(Y−XB) 15 MaskYthrough: Y′ = MYN
R2
XiY
=1−
SS(Y)
i =1−
ml
i i (18)
16
SendY′totheCSP
17 end
TokeepYˆ i andY privatetoFC-iandtheLC,respec- 18 CSPwaittoreceivedatathendo:
tively,weproposeaprocedurethatinvolvestheTAand 19 Computetheresidualmatrix:
the CSP. First, the TA generates two orthogonal matri- E′ =Y′−Yˆ′
i i
ces M ∈ Rm×m and N ∈ Rl×l. Next, all FCs download 20 PerformSVDonE′⊤E′andextractall
i i
M and N from TA and use them to mask their local eigenvalues(λ)
i
predictionsYˆ i viaYˆ i′ = MYˆ iN andsendYˆ i′ totheCSP. 21 CalculateSS(E′ i)by: SS(E′ i)=(cid:80) λ i
Meanwhile, the LC mask Y via Y′ = MYN and sends 22 SendSS(E′)toFC-i
i
Y′totheCSP.Then,theCSPcalculatestheresidualma- 23 end
trixE′ i =Y′−Yˆ i′whichisequivalentof M(Y−Yˆ i)Nor 24 FeatureContributordo:
ME iN. After that, the CSP performs SVD on E′ i⊤E′
i 25
fori=1→g,FC-ido
(= N⊤E⊤ i E iN). Since N⊤ and N are two random or- 26 DownloadSS(E′ i)fromtheCSP
thogonalmatrices, ithasbeenprovedthattheSVDre- 27 CalculateR2 by:
s uu el sts [1o 4f ].N T⊤ hE u⊤ i s,E thiN eCan Sd PE ca⊤ i nE ci as lch ua lr ae teth te hesa sm ume e oi fg ee in gv ea nl -- R2 XiY
=1−X SiY
S(E′ i)/ml
28 end
valuesofE⊤E,whichisequivalenttothetraceofE⊤E
i i i i 29 end
andSS(E). Finally,theCSPforwardsSS(E)toFC-i,
whoprocei edstocalculateR2 usingEquationi 18. 30 End
XiY
4. Securityanalysis • Thedataholdersonlyseetheirprivatedataandthe
relevantcomponentsofthemodelthatcorrespond
AsP3LSincorporatesFedSVDatitscore,itinherits
tothedatathattheycontribute.
thesecurityassurancesprovidedbyFedSVD’sproofof
security[14],whichincludesthefollowing:
Despite their potential usefulness, specific model
• The TA only knows the keys and never sees the componentscouldbewithheldfromthedataholdersto
real/encrypteddata. ensure the preservation of security and confidentiality
withintheframework.
• TheCSPsolelyreceivesencrypteddataandlearns Thedecisionnottodisclosethelocalrotationmatrix
encrypted model components within the frame- R toFC-iisintentional. BywithholdingR,itprevents
i i
work. Importantly, it has been established that it FC-ifromaccessingboth B and R simultaneously, as
i i
is not feasible to directly deduce or infer the raw knowledgeofbothenablesestimationofQ⊤throughthe
datafromtheencrypteddatawithinthiscontext. formulaQ⊤ = (R⊤R)−1R⊤B. AsQcontainstheload-
i i i i
9ingsoftheresponsevariables,itiscrucialforthisinfor- or a costly parameter — by leveraging a blend of pro-
mationtobeknownexclusivelybytheLC.Whilehiding cess data and software-driven algorithms. Once vali-
R,weallowFC-itolearn B duetospecificfunctional dated, a soft sensor can significantly minimize process
i i
purposeswithintheframework: costs by enabling swift feedback control over the esti-
matedproperty. Thiscontrolmechanismaidsinreduc-
• Joint estimation of target variables: FC-i can still ingtheproductionofoff-specificationproducts. More-
jointlyestimatethetargetvariables. over,anadditionalbenefitofsoftsensorsistheircapac-
itytocurtailtheneedforextensivelaboratorysampling,
• Insights into cross-organizational process interac-
thus reducing manpower costs. Soft sensors leverag-
tions: Accessto B allowsFC-itolearnhowtheir
i ing latent variables predominantly rely on PLS mod-
process variables interact with the product qual-
els. Algorithm 5 outlines the steps involved in gener-
ityofthedownstreamcompany. Suchinformation
ating predictions for unseen data. However, unlike the
mightbeusefulwhentheytrytooptimizetheirpro-
global score matrix T, the prediction of the response
cessesandwanttotakethedownstreamcompany’s
variablesisconstrainedtotheLabelContributors(LC)
KPIsintoaccount.
exclusively.
Improved process understanding: The regression
In addition, we strategically decide to hide the local coefficients allow all data holders to grasp the correla-
scores T from data holders to prevent potential infer-
i tionoftheirprocessvariableswithsomeparticularKPIs
enceofthelocalrotationmatrix(R).
i of the final product. This acquired knowledge proves
immensely valuable, particularly during process opti-
T = XR → R =(X⊤X)−1X⊤T
i i i i i i i i mization,asitprovidesinsightsintohowtheirvariables
influence the desired outcomes. Moreover, this valu-
Eveniftheleftinverseisn’tunique,itdoesn’tneces-
ableunderstandingisonlyaccessibletotheparticipants
sarily imply that data holders are unable to find a suit-
throughtheirinvolvementinthedatafederation.
ablesolution.Ashighlightedearlier,theprimaryreason
forrestrictingaccessto R fromFCsisduetotheirpo-
i
tentialabilitytoestimateQ,whichencompassescrucial
6. Experimentalevaluation
informationrelatedtothetargetvariables.
ToassesstheefficacyofP3LS,anexperimentutiliz-
ing synthetic datasets has been designed. The primary
5. ApplicationsofP3LS
objectiveofthisexperimentistoaddressthefollowing
questions:
AsavariantofPLS,P3LSnaturallylendsitselftoap-
plications where PLS is commonly employed (e.g., in • Q1: What is the performance gap between P3LS
MSPC). One notable advantage of utilizing P3LS lies andacentralizedPLSmodelthatlacksprivacypro-
in its ability to account for variations from preceding tection?
stages. Consequently,ithasthepotentialtoenhancethe
qualitycontrolperformance[25]. Someoftheapplica- • Q2: Doesintroducingmoredatafromotherpartic-
tionsthatweconsiderrelevantforP3LSinclude: ipantsimprovethemodelperformance?
Process monitoring: PLS is widely acknowledged
In essence, Q1 delves into the inherent trade-offs
as one of the most popular methods in process mon-
associated with safeguarding privacy within a verti-
itoring [26]. P3LS can be directly employed for this
cal federated setting, while Q2 examines the potential
purpose. Algorithm5outlinesthestepsinvolvedines-
enhancements resulting from collaborative training in-
timating the global X-scores matrix (T). By utilizing
Talongwiththeresidualmatrix(Θ),itbecomesfeasi- volvingdiversedatasources.Wecontinuebydescribing
i
the datasets and then the experiment settings. Finally,
bletocomputecommonlyusedmonitoringmetricssuch
the experiment results will be employed to address the
asHotelling’sT2andQ-statistics,andthecontributions
previouslyposedquestions.
fromoriginalvariables. Theseindicesserveasvaluable
tools in process monitoring and fault detection and di-
6.1. Datasets
agnosis[12].
Softsensors: Thepurposeofaninferentialsensoris Due to the absence of realistic datasets in VFL as
todeduceapropertythatischallengingorexpensiveto indicated by [27], we implemented a multistage pro-
measure directly — usually a laboratory measurement cesssimulatoroutlinedin[28]togeneratefivesynthetic
10Algorithm5:P3LSInference
Input: X =[X 1,...,X g]∈Rm×n,whereX
i
∈Rm×ni
Output: T,Yˆ
1 FunctionP3LS.predict([X 1,...,X g]):
2 TAdo:
3 Generatearandommatrix M ∈Rm×m
4 end
5 FeatureContributordo:
6
fori=1→g,FC-ido
7 Download MfromtheTA
8 Makelocalpredictions: Yˆ i = X iB i
9 Masklocalpredictions: Yˆ i′ = MYˆ i
10 Encryptlocaldatausing MandH i: X i′ = MX iH i
11
SendYˆ′andX′totheCSP
i i
12 end
13 end
14 CSPdo:
15 AggregateX′: X′ =(cid:80)g i=1X i′ (= MXH)
16
AggregateYˆ′ =(cid:80)g i=1Yˆ i′ (= MYˆ)
17 Calculatethemaskedglobalscores: T′ = X′R′ (= MXHH⊤R= MXR= MT)
18 end
19 FeatureContributordo:
20
fori=1→g,FC-ido
21
DownloadT′fromtheCSP
22 Recovertherealglobalscores: T = M−1T′
23 end
24 end
25 LabelContributordo:
26
DownloadYˆ′fromtheCSP
27 Recovertherealpredictions: Yˆ = M−1Yˆ′
28 end
29 End
datasetsofvaryingsizes. Forcomprehensivedetailsre- provided in Figure 2. Furthermore, Table 2 presents a
garding the simulator itself and the specific configura- summary detailing the dataset owned by each individ-
tionsadoptedforgeneratingthesesyntheticdatasets,an ualcompany.
extensiveexplanationisavailableinAppendix A.
Eachofthegenerateddatasetssimulatesathree-stage
process. Inthecontextofaverticalfederatedscenario,
theassumptionmadeisthattherearethreedistinctman-
ufacturing companies, with each company controlling
andpossessingdatafromaspecificstageoftheprocess.
Althoughqualitycharacteristicsarerecordedacrossall Figure2:Thestructureofthesimulateddatasets.
stages,theprimaryobjectiveofthedatafederationisto
constructapredictivemodelsolelyfortheoutputqual-
6.2. Experimentsettings
ity of the final stage. The process variables specific to
company i’s are denoted as X, while the quality vari- All runs were conducted on a workstation using an
i
ables of the final product are represented as Y. An il- 11thGenIntel(R)Core(TM)i7-1185G7withfourcores
lustration of the structure of the simulated dataset is at3.00GHz,32.0GBRAM,andPyCharm2022.1.
11nentsofCenPLSandP3LSmodelsusingthemean
Table2:Processvariablesandcompanyassignment.
squareddistance.
Dataset X-blocks Y-block
Comp1: X 1 ∈R1000×10 • Prediction performance: We measured the R2
#1 Comp2: X 2 ∈R1000×20 of P3LS, CenPLS, and LocalPLS when applying
Comp3: X 3 ∈R1000×20 Comp3: thesemodelstothetestset.
#2 C Co om mp p1 2: : X X1 ∈ ∈R R1 10 00 00 0× ×2 40 0 Y ∈R1000×7 • Computation time: We measured the run time of
2
Comp3: X ∈R1000×40 allthreemodelswhenmakinginferences.
3
Comp1: X ∈R1000×50
1 SinceP3LSinvolvesusingrandommatricestogaina
#3 Comp2: X ∈R1000×100
2 comprehensive evaluation, we repeated the experiment
Comp3: X ∈R1000×100
3 100timesforeachdataset.
Comp1: X ∈R1000×100
1
#4 Comp2: X ∈R1000×200
2 6.3. Results
Comp3: X ∈R1000×200
3
Comp1: X ∈R1000×200 ThesimilaritybetweenP3LSandCenPLSisempiri-
1
#5 Comp2: X ∈R1000×400 cally confirmed when comparing corresponding model
2
Comp3: X ∈R1000×400 components. Figure 3 displays the mean squared dis-
3 tance between different components of the two models
across all datasets. It illustrates that the difference be-
Each private dataset was randomly partitioned into tween the two models is negligible. Consequently, as
threeseparatesubsets: trainingset(60%),validationset shown in Figure 4, the difference in their R2 scores is
(20%), and test set (20%). The training set was dedi- insignificant.
catedtoconstructingthemodels.Thevalidationsetwas It can also be seen in Figure 4 that for the same
utilizedforoptimizingthenumberoflatentvariablesin
dataset,eventhoughtheresultsofdifferentrunsarenot
theconstructedmodels. Inparticular,theoptimalnum- the same, the variations are ignorable. It is important
ber is the one that returns the highest accuracy on the to note that P3LS is theoretically lossless. However,
validation set. The test set was designated to estimate minor deviations in the experiment might occur due to
themodel’sperformanceonunseendata. floating-pointnumberrepresentationincomputers.
For each of the generated datasets, we constructed Figure 5 compares the overall R2 of LocalPLS and
three distinct models, each reflecting a scenario com- P3LS when evaluating these models on the test set.
monly observed in cross-organizational manufacturing Across all datasets, it shows that P3LS models outper-
processes: form the local PLS models. It can be explained by the
fact that the underlying process models involve vari-
• CenPLS: A PLS model trained by the last com-
ables from all stages. Since LocalPLS models only
pany using the centralized dataset. It simulates a
learn from the last company’s data, they miss crucial
scenariowhereasinglecompanyhasaccesstoand
information needed to make good predictions. How-
utilizesallavailabledataalongthevaluechain.
ever, in practice, there might be cases where the tar-
• LocalPLS: A PLS model trained exclusively on get variables are totally independent of preceding pro-
theprivatedatasetofthelastcompany(i.e., Xtrain ductionsteps,ortheinformationprovidedbyotherdata
3
andYtrain). providersisnotrelevant. Insuchcases,theadvantages
ofusingP3LSmightbelesssignificant.
• P3LS:AP3LSmodeltrainedonallavailabledata Figure6illustratesthecomputationtimeofthethree
along the value chain without direct data sharing models. As expected, compared to CenPLS or Lo-
betweencompanies. calPLS,ittookP3LSmoretimetoproduceinferences.
This results from processing more data and the addi-
A detailed description of the data used for develop-
tionalcomplexityintroducedbythedataencryptionand
ing and testing each model is described in Table 3. To
decryption steps. It should be highlighted that the ex-
answerQ1andQ2,weconsideredthefollowingfactors:
periment was performed in simulation mode, and the
• Losslessness: Eventhoughithasbeenprovedthat communicationtimewasnotconsidered.Therefore,the
the outcomes of CenPLS and P3LS are equiva- resultsmightdifferwhendeployinginrealsystemswith
lent,wewantedtoempiricallycomparethecompo- moreadvancedhardware.
12Table3:Trainingset,validationset,andtestsetforeachmodel.
Model TrainingData ValidationData TestData
CenPLS Xtrain,Ytrain Xval,Yval Xtest,Ytest
LocalPLS Xtrain,Ytrain Xval,Yval Xtest,Ytest
3 3 3
P3LS Xtrain,Xtrain,Xtrain,Ytrain Xval,Xval,Xval,Yval Xtest,Xtest,Xtest,Ytest
1 2 3 1 2 3 1 2 3
Figure3:ComparemodelcomponentsofCenPLSandP3LS.
Figure4:DifferenceintestR2betweenCenPLSandP3LS. Figure5:ComparisonofperformanceonthetestsetofCenPLS,Lo-
calPLS,andP3LS.
7. Discussionandconclusions
the horizontal setting. In the horizontal setting, each
This paper proposes P3LS, which allows multiple
dataholderactsasbothafeaturecontributorandalabel
companies to collaboratively build and exploit an inte-
contributorastheyownbothprocessvariables(i.e., X)
i
grated cross-organizational process model without re-
andresponsevariables(i.e.,Y). Underthissetting,the
i
vealing private datasets. The empirical experiment on
sharedmodelcomponentsaretheweights,loadings,and
simulated data shows that the proposed approach can coefficients. AllstepsinAlgorithm2arekeptthesame,
yieldamodelwithcomparableperformancetothecon-
exceptthefollowing:
ventional approach of centralizing data while protect-
ing the data privacy of the participants. Besides, we • Step2-4: Insteadofsplitting H, theTAsplits A⊤
alsoproposedanapproachforquantifyingthecontribu- into[A⊤,...,A⊤]where A⊤ ∈Rmi×n.
tionofparticipants. Thisapproachcreatesabasisfora 1 g i
profit-sharing scheme and, at the same time, motivates • Step6-15: FCidownloads A⊤andHfromtheTA
i
companies to join the federation and contribute high- andcomputesX′ = AXH,Y′ = AYG.
i i i i i i
qualitydata.
Eventhoughthepaperfocusesontheverticalsetting, • Step 17: In addition to aggregating X′, the CSP
the proposed method can also be extended to apply to needstoaggregatesY′ =(cid:80)g Y′.
i=1 i
13are crucial for practical application in real production
scenarios,suchasinfrastructurerequirementsandcom-
putational time, will also go through a comprehensive
assessment.
Acknowledgment
The research reported in this paper has been partly
fundedbytheFederalMinistryforClimateAction,En-
vironment, Energy, Mobility, Innovation and Technol-
ogy (BMK), the Federal Ministry for Digital and Eco-
nomicAffairs(BMDW),andtheStateofUpperAustria
Figure6:Computationtimeintheinferencephase. intheframeofSCCH,acenterintheCOMET-Compe-
tenceCentersforExcellentTechnologiesProgramman-
agedbytheAustrianResearchPromotionAgencyFFG
Attheendofthetrainingphase,alldataholderscan
andtheFFGprojectcircPlast-mr(GrantNo. 889843).
download the encrypted loadings, encrypted weights,
andencryptedcoefficientsfromtheCSP.Then,theycan
Appendix A. Multistageprocesssimulator
usethegivenkeys(i.e., A,HandG)torecoverthereal
i
modelcomponents.Afterthat,theycanusethesemodel
We implemented the multistage process simulator
componentstoestimatescoresandpredicttheresponse
proposed in [25] to generate synthetic data. Figure
variablesofunseendata.
A.7illustratesatypicaldiagramofamultistagesystem.
We will also investigate the scenarios where there At stage-i, the process variables and the measured re-
are multiple label contributors. In particular, multiple sponses are denoted as X and Y, respectively. Mean-
i i
companiesalongthevaluechaincancontributetheirre- while,V representsthemeasurementnoisesassociated
i
sponsevariableblocks(e.g.,Y i). Ultimately,companies with responses. To introduce the interaction between
canlearntheinteractionbetweentheirprocessvariables stages, some responses of upstream stages, denoted as
and all other companies’ KPIs, not just the last com- Y´ , will be used as inputs of stage-i’s process model.
i−1
pany.Thisextensionwillrequireachangeinkeygener- In this simulator, all stages’ process models are set to
ationandencrypteddataaggregationandwillbeinthe quadraticfunctionsthatcanbeexpressedas:
scopeofourfutureworks.
Another extension of P3LS that will be investigated Y = AU +BU +V (A.1)
i i i.lin i i.qd i
isMultiwayP3LS,whichcanhandlebatchprocessdata
whereU andU arematricescontainingthelin-
where X is in a 3D format with the additional dimen- i.lin i.qd
i eartermsandthequadratictermsofthemodel,respec-
sionoftime. Inanofflinemode, batchdatacanbeun-
tively. Meanwhile, A and B represent the linear and
foldedintoa2Dformatusingthebatch-wiseunfolding i i
quadraticmodelcoefficients,respectively.Theelements
[13],andP3LScanbenaturallyapplied. However,itis
of A and B aredrawnrandomlyfromaspecificdistri-
more challenging to estimate the response variables in i i
bution. While A is a dense matrix, B is intentionally
onlinemode. Itisbecausethe X matrixisnotcom- i i
new made sparse to imitate actual physical process models,
plete until the end of the batch operation. Suppose it
oftencontainingonlyasparsesubsetofquadraticterms.
takesK timeunitstocompleteabatch. Attimeinterval
k,thematrixX hasonlyitsfirstkrowscomplete,and
new
it is missing all the future observations (K − k rows).
A solution to this problem is to leverage the ability of
PLStohandlemissingdata. MultiwayPLSdoesthisby
projecting the already known observations up to time
interval k into the reduced space defined by theW and
Pmatrices[13]. Asimilarapproachwillbeadoptedto
enableP3LStooperateinonlinemode.
FigureA.7:Multistageprocesssimulatordiagram.
Furthermore,inourfutureresearchanddevelopment,
weaimtovalidatetheeffectivenessofP3LSinreal-life Followingaretheconfigurationsusedforgenerating
datasets, which tend to be more intricate. Factors that Dataset-1:
14• Stage 1: X 1 ∈ R1000×10, Y 1 ∈ R1000×5, A 1 ∼ [9] G.Wang,C.X.Dang,Z.Zhou,Measurecontributionofpartici-
U(−1, 2), B ∼ U(−0.01, 0.02). A has sparsity pantsinfederatedlearning,in:2019IEEEinternationalconfer-
1 1
enceonbigdata(BigData),IEEE,2019,pp.2597–2604.
probabilityof0.15.
[10] T.Wang,J.Rausch,C.Zhang,R.Jia,D.Song,Aprincipledap-
proachtodatavaluationforfederatedlearning,FederatedLearn-
• Stage 2: X ∈ R1000×20, Y ∈ R1000×6, Y´ ∈
2 2 1 ing:PrivacyandIncentive(2020)153–167.
R1000×3, A 2 ∼ U(−3, 3), B 2 ∼ U(−0.03, 0.03). [11] L.Yang,D.Chai,J.Zhang,Y.Jin,L.Wang,H.Liu,H.Tian,
A hassparsityprobabilityof0.2. Q.Xu,K.Chen,Asurveyonverticalfederatedlearning:From
2
alayeredperspective,arXivpreprintarXiv:2304.01829(2023).
• Stage 3: X ∈ R1000×20, Y ∈ R1000×7, Y´ ∈ [12] J.F.MacGregor,T.Kourti,Statisticalprocesscontrolofmul-
3 3 2 tivariate processes, Control engineering practice 3 (3) (1995)
R1000×3, A 3 ∼ U(−3, 2), B 3 ∼ U(−0.03, 0.02). 403–414.
A hassparsityprobabilityof0.25. [13] P.Nomikos,J.F.MacGregor,Multivariatespcchartsformoni-
3
toringbatchprocesses,Technometrics37(1)(1995)41–59.
For all stages, B i has a sparsity probability of [14] D.Chai,L.Wang,J.Zhang,L.Yang,S.Cai,K.Chen,Q.Yang,
0.999. ThenoisematrixV ∼ N(0, 0.001). Tore- Practicallosslessfederatedsingularvectordecompositionover
i
producethecorrelationsbetweenprocessvariables billion-scaledata,in: Proceedingsofthe28thACMSIGKDD
ConferenceonKnowledgeDiscoveryandDataMining,2022,
oftenobservedinpractice,wemade X alow-rank
i pp.46–55.
matrixwithbell-shapedsingularvalueswhereap- [15] H. Abdi, Partial least square regression (pls regression), En-
proximately four singular vectors are required to cyclopedia for research methods for the social sciences 6 (4)
explain90%ofthedata. (2003)792–795.
[16] P. Geladi, B. R. Kowalski, Partial least-squares regression: a
tutorial,Analyticachimicaacta185(1986)1–17.
We used the same configurations to generate other
[17] S. De Jong, Simpls: an alternative approach to partial least
datasetsexceptthenumberofprocessvariables. Inpar- squares regression, Chemometrics and intelligent laboratory
ticular, forDataset-2,-3, -4, and-5, thenumberofpro- systems18(3)(1993)251–263.
[18] R.Wehrens,B.-H.Mevik,Theplspackage: principalcompo-
cessvariablesismultipliedby2,5,10,and100respec-
nentandpartialleastsquaresregressioninr(2007).
tively.
[19] K.-A.LeˆCao,D.Rossouw,C.Robert-Granie´,P.Besse,Asparse
plsforvariableselectionwhenintegratingomicsdata,Statistical
applicationsingeneticsandmolecularbiology7(1)(2008).
References [20] J. F. MacGregor, C. Jaeckle, C. Kiparissides, M. Koutoudi,
Processmonitoringanddiagnosisbymultiblockplsmethods,
AIChEJournal40(5)(1994)826–838.
[1] M. Colledani, T. Tolio, A. Fischer, B. Iung, G. Lanza,
[21] L.Wangen,B.Kowalski,Amultiblockpartialleastsquaresal-
R.Schmitt,J.Va´ncza,Designandmanagementofmanufactur-
gorithmforinvestigatingcomplexchemicalsystems,Journalof
ingsystemsforproductionquality,CirpAnnals63(2)(2014)
chemometrics3(1)(1989)3–20.
773–796.
[22] B. McMahan, E. Moore, D. Ramage, S. Hampson, B. A.
[2] J.Lohmer,E.RibeirodaSilva,R.Lasch,Blockchaintechnology
y Arcas, Communication-efficient learning of deep networks
inoperations&supplychainmanagement: acontentanalysis,
fromdecentralizeddata,in:Artificialintelligenceandstatistics,
Sustainability14(10)(2022)6192.
PMLR,2017,pp.1273–1282.
[3] Proposalforaregulationoftheeuropeanparliamentandofthe
[23] Q.Li,Z.Wen,Z.Wu,S.Hu,N.Wang,Y.Li,X.Liu,B.He,A
councilconcerningbatteriesandwastebatteries, repealingdi-
rective2006/66/ecandamendingregulation(eu)no2019/1020, surveyonfederatedlearningsystems: Vision,hypeandreality
fordataprivacyandprotection,IEEETransactionsonKnowl-
accessedonDecember11,2023.
edgeandDataEngineering(2021).
URL https://eur-lex.europa.eu/legal-content/EN/
[24] A. Khan, M. t. Thij, A. Wilbik, Vertical federated learning:
TXT/?uri=CELEX:52020PC0798
Astructuredliteraturereview,arXivpreprintarXiv:2212.00622
[4] J. Walden, A. Steinbrecher, M. Marinkovic, Digital product
(2022).
passportsasenablerofthecirculareconomy,ChemieIngenieur
[25] J.Shi,S.Zhou,Qualitycontrolandimprovementformultistage
Technik93(11)(2021)1717–1727.
[5] D.NguyenDuy,M.Affenzeller,R.Nikzad-Langerodi,Towards systems:Asurvey,IIEtransactions41(9)(2009)744–753.
[26] Z.Ge,Z.Song,Multivariatestatisticalprocesscontrol:Process
verticalprivacy-preservingsymbolicregressionviasecuremul-
monitoringmethodsandapplications,SpringerScience&Busi-
tiparty computation, in: Proceedings of the Companion Con-
nessMedia,2012.
ference on Genetic and Evolutionary Computation, 2023, pp.
[27] Y.Liu,Y.Kang,T.Zou,Y.Pu,Y.He,X.Ye,Y.Ouyang,Y.-
2420–2428.
Q.Zhang,Q.Yang,Verticalfederatedlearning,arXivpreprint
[6] A.Hartebrodt,R.Nasirigerdeh,D.B.Blumenthal,R.Ro¨ttger,
arXiv:2211.12814(2022).
Federatedprincipalcomponentanalysisforgenome-wideasso-
[28] J.Jin,J.Shi,Statespacemodelingofsheetmetalassemblyfor
ciationstudies,in:2021IEEEInternationalConferenceonData
dimensionalcontrol(1999).
Mining(ICDM),IEEE,2021,pp.1090–1095.
[7] A.Grammenos,R.MendozaSmith,J.Crowcroft,C.Mascolo,
Federatedprincipalcomponentanalysis,AdvancesinNeuralIn-
formationProcessingSystems33(2020)6453–6464.
[8] D. N. Duy, D. Gabauer, R. Nikzad-Langerodi, Towards fed-
eratedmultivariatestatisticalprocesscontrol(fedmspc),arXiv
preprintarXiv:2211.01645(2022).
15