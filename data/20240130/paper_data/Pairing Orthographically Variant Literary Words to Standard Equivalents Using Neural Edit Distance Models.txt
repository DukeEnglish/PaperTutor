Pairing Orthographically Variant Literary Words to Standard Equivalents
Using Neural Edit Distance Models
CraigMessner TomLippincott
CenterforDigitalHumanities CenterforDigitalHumanities
JohnsHopkinsUniversity JohnsHopkinsUniversity
cmessne4@jhu.edu tom.lippincott@jhu.edu
Abstract modelcomparestwostringsandoutputsaproba-
bility that they are a match. These strings can be
Wepresentanovelcorpusconsistingofortho-
consideredamatchiftheirpairprobabilityexceeds
graphically variant words found in works of
acertainthreshold. Furthermore,theseprobabili-
19thcenturyU.S.literatureannotatedwiththeir
tiescanalsobeusedtorankasetofpossiblestring
corresponding"standard"wordpair. Wetraina
pairs. We obtain these probabilities using a neu-
setofneuraleditdistancemodelstopairthese
variantswiththeirstandardforms,andcompare ral edit distance model architecture, an approach
theperformanceofthesemodelstotheperfor- thathasproveneffectiveatpairingcognatewords.
manceofasetofneuraleditdistancemodels Specifically we train neural edit distances mod-
trainedonacorpusoforthographicerrorsmade elsonacorpusofnonliteraryorthographicvariants
byL2Englishlearners. Finally,weanalyzethe
producedbyL2Englishlearnersandanovelcorpus
relative performance of these models in the
ofliteraryvariantsinordertoofferthefollowing
lightofdifferentnegativetrainingsamplegen-
contributions:
erationstrategies,andofferconcludingremarks
ontheuniquechallengeliteraryorthographic
• Theaforementionednovelcorpusofliterary
variationposestostringpairingmethodologies.
orthographicvariantsthatcansupporttraining
1 Introduction andevaluationofstring-pairingmodels
Using orthographic information to pair similar • Analysis of this corpus and how its specific
stringsfromalistofvariantshasanumberofuses, set of challenges differ from datasets of or-
fromspellingcorrectiontocognatedetection. Be- thographicvariantsthatarenotderivedfrom
yondcharacterlevelsimilarity,whatitmeanstobe literarysources
a "good" neighbor to a given source string might
• Evaluationoftheimpactofnegativeexample
entailphoneticsimilarity(inthecaseofmanymis-
generation strategies on model performance
spellings), some sort of cognitive proximity (in
acrossdifferentdomains
certain "point" misspellings, say "k" for "c") or
it may reflect a shared linguistic history (in the
• Initial steps towards a general system able
caseofcognates). Usingorthographicinformation
topairliteraryorthographicvariantstotheir
to achieve a string-pair ranking that incorporates
standardforms
these axes of meaning requires that the orthogra-
phyofastringalsocapturesthisothermeaningful 2 Background
dimension,toagreaterorlesserdegree. Weintro-
2.1 LiteraryOrthographicVariation
ducethedomainof"literaryorthographicvariants,"
a set of orthographic modifications motivated by Whileorthographicvariationisoftenframedbyde-
literary aesthetic concerns instead of purely lin- viancefromanacceptedstandard,ithasalsobeen
guistic or cognitive principle, and posit that the usedinaliterarycontextasavehicleofmeaning.
orthographicresultsoftheseuniquemotivationsne- Thistechniqueisnotablyprevalentintheliterature
cessitatesre-evaluatingmodelingapproachesthat of the 19th century United States, where it often
haveprovensuccessfulinmorelinguisticallymoti- servedtoidentifyaparticularcharacterasbelong-
vateddomains(suchastheaforementionedcognate ingtoacertainrace,class,genderorregion(Ives,
detectionandspellingcorrectiontasks). Weevalu- 1971)(Jones,1999). BuoyedbyEnglishorthogra-
atethisclaimusingstringpairing,ataskwherea phy’shighlyredundantnature(Shannon,1951)the
4202
naJ
62
]LC.sc[
1v86051.1042:viXrapresenceoftopic-specificsurroundingcontext,and Wesampledsentenceswithpossibleorthovariant
thedesiretohavethevariationitselfbemeaningful tokensrandomly,andAuthor1providedstandard
(perhapsbyusingaparticularsystemoforthogra- tokenannotationsforthetokensdeemedactually
phythatsignifiesacertainsubjectposition)literary variant. Thefinalcorpusconsistsof3058variant
orthographic variants are typically more extreme tokenspairedwiththeirstandardvariantsandtheir
andmoreobscurelymotivatedthanthoseproduced sentence-levelcontext.
as the result of misspellings or other similar pro-
3.2 FCECorpus
cesses.
The Cambridge Learner First Certificate in En-
2.2 StringPairingUsingEditDistance
glish (FCE) corpus is comprised of short narra-
Methods
tives produced by English as a second language
Editdistancemeasures,mostcommonlytheLev- (ESL)learners(Yannakoudakisetal.,2011). The
enshteinDistance(Levenshteinetal.,1966)have corpusincludeshandtaggedcorrectionsforava-
been used to rank variant-standard token pairs. riety of observed linguistic errors. We subsetted
Morerecently,statisticaleditdistance(Ristadand the corpus to only include errors with a possible
Yianilos,1998)andneuraleditdistance(Libovický orthographiccomponent,indicatedbythe"S"class
andFraser,2022)haveallowededitdistancetobe oferrorcodes(Nicholls,2003). Thisresultedina
learnedempiricallyfromdata. Whilestatisticaledit subsetof4757samples.
distancelearnsasingledistributionofeditopera-
tionsoverpairedstrings,neuraleditdistanceusesa 3.3 EmpiricalCharacterizationofCorpora
differentiableversionoftheexpectationmaximiza-
tion(EM)algorithmasalossfunctionforaneural Corpus 1LD% 2LD% 3LD% 4+LD%
model. This allows neural edit distance to learn FCE 74.1 20.9 3.2 1.8
editoperationprobabilitiesfromcontextualembed- Gutenberg 43.8 28.9 17.2 10.1
dings. (LibovickýandFraser,2022)trainaneural
edit distance string pairing model that employs Table1: Levenshteindistancesofstandardandnonstan-
RNNlearnedembeddingsandrandomlygenerated dardtokensintaggedsamples,expressedaspercentage.
negative samples in order to achieve state of the
artperformanceonacognatedetectiontask(Rama Consistentwithourhypothesisaboutthediffer-
etal.,2018). ences between literary and nonliterary orthovari-
ants, Table 1 demonstrates that the nonstandard
3 MethodsandMaterials tokensfoundinGBtendtobemoredistantfrom
their"standard"pairings. Thisempiricallydemon-
3.1 ProjectGutenbergCorpus1
stratesatleastoneaxisofdifferencebetweenthe
We first use the Project Gutenberg (GB) catalog GB corpus and corpora commonly used to eval-
file2 to subset the full collection to English texts uate approaches to string pairing, alignment and
producedbyauthorslivinginthe19th century. We ranking.
thenlimitthissettothoseworksidentifiedaspart
oftheLibraryofCongress"PS"(AmericanLitera- 3.4 Experiment1: NeuralEditDistance
ture)classificationgroup. Wetokenizeeachofthis StringPairingforCandidateFiltering3
subsetoftextsandsplitintosentencesbeforeauto-
We train a neural edit distance model on a string
maticallyidentifyingpossibleorthovarianttokens
pairing task and empirically derive a probabil-
usingavarietyofcriteria,including:
ity threshold in order to separate likely vari-
ant/standard token pairs from unlikely pairs. We
• Presenceofnumericcharacters
generate negative samples by pairing variant ob-
• Presenceofcapitalization servedtokenswithtokensdrawnfromBrownusing
thefollowingmethods:
• Presence of candidate token in WordNet
(Miller,1995)ortheBrownCorpus(Francis 1. Random: n randomly selected known false
andKucera,1964) tokenssourcedfromBrown
1Thefullcorpusisavailablehere 3Codeforthefollowingexperimentsisavailableathttps:
2availablehere //github.com/comp-int-hum/edit-distance-mlF-FCE F-GB MRR-FCE MRR-GB
Model Count
LD 10 0.81 0.69 0.40 0.29
20 0.79 0.66 0.64 0.34
30 0.76 0.60 0.67 0.41
50 0.72 0.56 0.63 0.44
mixed 10 0.84 0.72 0.59 0.52
20 0.81 0.67 0.65 0.57
30 0.79 0.68 0.68 0.56
50 0.77 0.62 0.67 0.62
random 10 0.97 0.93 0.61 0.47
20 0.97 0.93 0.65 0.53
30 0.96 0.90 0.69 0.52
50 0.94 0.87 0.70 0.50
Table2:ThebluehighlightedcellisthebestperformingmodeltrainedontheFCEcorpus,redisthebestperforming
trainedontheGBcorpus. Fscoresindicateeachmodel’sabilitytodistinguishtrueandfalsestringpairs,MRR
scoresindicatestheabilityofeachmodeltorankasetofstandard-variantpairsgeneratedusingBrown
2. LD:nlowestLDfromsourcevariantknown random, mixed). As it might be expected, mod-
falsetokens elsgiventhemoredifficulttask,inwholeorpart,
ofdistinguishinglowLDvariantsperformworse.
3. Mixed: n/2 Random process tokens and n/2
However,theinclusionofthesedifficultpairsseem
LDprocesstokens
to benefit GB’s performance when it comes to
overallpairranking. TheseMRRscores(columns
Weperformthisprocedurefornof10,20,30and
MRR-FCEandMRR-GB)areproducedusingonly
50. Wesplitthedataintotest,trainandvalidation
Brownandastabletestsetofknownpositivepairs
sets,eachcontaininga(necessarilyunique)admix-
in each given corpus, and thus form the basis of
ture of known positive and known negative gen-
ourcomparison.
eratedpairs. Followingthemethodof(Libovický
andFraser,2022)wegenerateamatchprobability
thresholdforeachmodelduringtrainingbyadjust-
ing it to maximize evaluation F1 score, and then
evaluatedeachmodel’sabilitytodistinguishtrue
andfalsetokenpairingsalsousingF1score.
3.5 Experiment2: NeuralEditDistance
StringPairingforPairPrediction
Leavingasidenegativegeneratepairs,wepaireach
knowntruesourcetokeninagiventestsetwithall
ofthetokensfoundinBrown. Wethenemploythe
models trained in Experiment 1 to rank the prob-
abilityofeachsource-Brownpairingbeingatrue
Figure1: MRRbyNGeneratedNegatives
pair. We evaluate the accuracy of these rankings
usingmeanreciprocalrank(MRR).
Figure1showsthatforallnofnegativesamples,
themodelstrainedonFCEperformbestwhenpro-
4 ResultsandAnalysis
vided negative samples generated by the random
ResultsoftheexperimentscanbefoundinTable process. ThemodelsemployingcloseLDnegatives
2. TheF-scoreofeachexperimentnecessarilyde- performeduniformlytheworst. Thisissomewhat
pendsontheuniquesetofnegativesgeneratedby theoppositeofourexpectedresult. Thenegative
a given count (10, 20 etc.) and technique (LD, signal of incorrect close LD examples would onitsfaceseemparticularlyusefulgiventhenearLD setoftransformationsresidentintheFCEcorpus
natureofFCE’sspellingandusageerrors, asdis- asawhole.
tinguishingbetweenthecandidatesinthenearLD In contrast, the GB corpus contains samples
neighborhoodofavarianttokenbecomesimpera- drawn from multiple authors who each employ
tive. their own looser set of orthographic constraints.
These authors do not attempt in good faith to ad-
On the other hand, models trained on GB per-
heretoaparticularstandardorthography. Rather,
formuniformlythebestwhenprovidednegatives
theyuseorthographyasanexpressivetool,andmay
generatedbythemixedstrategy,acombinationof
notrelyasheavilyonfurtherorthographicprinci-
random and close LD pairs. This implies that in
ples. Consequently,thepositiveexamplesmaylose
the case of GB, but not FCE, the two sources of
somesignificantamountofexplanatoryvalue. Ex-
negativeexamplesprovideorthogonalinformation
amplesofthiseffectdrawnfromthecorpuscanbe
thatareeachoftheirownparticularuseduringthe
foundinTable3.
trainingprocess.
Each variant is a phonetic or pseudo-phonetic
Thespecificcharacterofthegeneratednegative
renderingofagivenwordinaformofparticularly
examplesmayexplainthisperformancedisparity.
motivated variant English orthography, yet each
Figure 2 shows the average LD from the target
setofcharacter-levelsubstitutionsvariestoalarge
varianttokensforeachtheofnegativegeneration
degree.4 Indeed, even though all of these forms
strategies. Random generation, the best perform-
are relatively low LD from their standard token,
ingstrategyoverFCE,producesnegativesamples
thesetoftransformationprinciplesencodedinone
onaverageabout8LDfromthetargetvariantto-
teaches us relatively little about the set found in
ken,nomatterthenumbergenerated. Logically,the
anyoftheothers. Thiscouldexplainthethemixed
mixedstrategy,whichperformedbestoverGB,pro-
strategy’ssuperiorperformanceonGB,astheposi-
ducedasetofsampleswithaverageanLDfalling
tiveexamplesunder-determinethespaceoflikely
between the uniformly high LD of the randomly
transformations among low LD candidates, leav-
generatedsamplesandthelowLDofthesamples
ingthegeneratedlowLDnegativeexamplesmore
generatedbytheLDprocess,which,forGBrange
roomtoprovideusefulinformation.
fromjustbelowtojustover3LDonaverage.
Inshort,FCEtrainedmodelsbenefitmostfrom
uniformlyhighLDnegativeexamples,whileGB
trained models benefit most from a mixture of
high and low LD negative examples. This may
speak to the distinct nature of the positive exam-
ples found in these corpora. The FCE corpus is
comprised of samples produced by multiple au-
thors. However,therangeofpossibleorthovariant
formstheyemployislimitedbytheirsharedintent
to adhere to a standard form of English orthogra-
phyasbestastheycan. Thisoverridingprinciple
could lead FCE’s variant forms to conform more
closelytoacentralizedsetofpossibleedits,typi- Figure2: AverageLDofGeneratedNegatives
fiedbycommoncharactersubstitutionsorphonetic
misspellings – it would be understandable for a 5 FutureWork
writer making a good faith attempt at producing
Thecomplexitiesinherentinliteraryvariantorthog-
standardEnglishorthographytoreplacea"c"with
raphyoffersmanyaxesonwhichtocontinuethese
a"k",butnever,say,anelisionapostrophe("’"). If
studies. Furtherexperimentscouldbeperformed
thisisthecase,muchoftheinformationthemodel
to validate the hypothesis concerning the mixed
wouldneedtodistinguishbetweenlowLDBrown
strategy’ssuccessontheGBcorpus. Thiscouldbe
candidatetokensisalreadycontainedinthefairly
uniform set of possibilities demonstrated by the 4Itshouldbenotedandacknowledgedthatmanyofthese
examplesareduetotheproliferationofexplicitlyracistdepic-
positive examples – the types of transformations
tionsofAfrican-Americansandotherminoritygroupsinthis
embodiedbytheseexamplescloselyresemblesthe periodofliteratureStandard Variants
afraid afear’d avraid ’feerd ’fraid ’afeared ofraid
children childens child’n chillunses chilther
master mars’ mars’r ’marse mauster
convenient convanient conwenient conuenenient
office awffice oflfis ohfice
calculate calkylate calkelate ca’culate
Table3: SamplesofpairedstandardandvariantwordformsfoundinGB
accomplishedanumberofways,includingtraining References
an additional set of models on a dataset of ortho-
Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
variantsgeneratedbyothermeans(forexamplethe
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Zéroecharacterleveladversarialbenchmark(Eger Schwenk, and Yoshua Bengio. 2014. Learning
andBenz,2020))andevaluatingtheperformance phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
ofthenegativegenerationstrategiesinthecontext
arXiv:1406.1078.
ofthatdataset’sownorthographicprecepts.
Additionally, future work could leverage the Steffen Eger and Yannik Benz. 2020. From hero to
sentence-levelcontextualinformationincludedin z\’eroe: A benchmark of low-level adversarial at-
tacks. arXivpreprintarXiv:2010.05648.
the GB corpus to aid in string pair ranking. This
could be an especially fruitful solution given the
W Nelson Francis and Henry Kucera. 1964. A stan-
multiply-systematicnatureofliteraryorthographic dardcorpusofpresent-dayeditedamericanenglish,
variation,aslocalinformationaboutthesemantics for use with digital computers. Brown University,
Providence,2.
of the source variant and the nature of the ortho-
graphic choices made in nearby neighbor tokens
Sumner Ives. 1971. A theory of literary dialect. A
couldaidinadjudicatingbetweensource-candidate variouslanguage:PerspectivesonAmericandialects,
pairsgrantedrelativelysimilarprobabilitiesbythe pages145–177.
neuraleditdistancemodel.
GavinJones.1999. Strangetalk: Thepoliticsofdialect
literatureinGildedAgeAmerica. UnivofCalifornia
Press.
VladimirILevenshteinetal.1966. Binarycodescapa-
bleofcorrectingdeletions,insertions,andreversals.
InSovietphysicsdoklady,volume10,pages707–710.
SovietUnion.
JindˇrichLibovickýandAlexanderFraser.2022. Neural
stringeditdistance. InProceedingsoftheSixthWork-
shoponStructuredPredictionforNLP,pages52–66,
Dublin,Ireland.AssociationforComputationalLin-
guistics.
GeorgeAMiller.1995. Wordnet: alexicaldatabasefor
english. CommunicationsoftheACM,38(11):39–41.
DianeNicholls.2003. Thecambridgelearnercorpus:
Errorcodingandanalysisforlexicographyandelt.
InProceedingsoftheCorpusLinguistics2003con-
ference,volume16,pages572–581.CambridgeUni-
versityPressCambridge.
TarakaRama,Johann-MattisList,JohannesWahle,and
Gerhard Jäger. 2018. Are automatic methods for
cognatedetectiongoodenoughforphylogeneticre-
constructioninhistoricallinguistics? InProceedings
ofthe2018ConferenceoftheNorthAmericanChap-
teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume2(ShortPa-
pers),pages393–400,NewOrleans,Louisiana.As-
sociationforComputationalLinguistics.
EricSvenRistadandPeterNYianilos.1998. Learning
string-editdistance. IEEETransactionsonPattern
AnalysisandMachineIntelligence,20(5):522–532.
Claude E Shannon. 1951. Prediction and entropy
of printed english. Bell system technical journal,
30(1):50–64.
HelenYannakoudakis,TedBriscoe,andBenMedlock.
2011. Anewdatasetandmethodforautomatically
grading ESOL texts. In Proceedings of the 49th
AnnualMeetingoftheAssociationforComputational
Linguistics: HumanLanguageTechnologies,pages
180–189, Portland, Oregon, USA. Association for
ComputationalLinguistics.
A Hyper-parametersandmodeldetails
The hyperparameters we employ closely follow
thosefoundin(LibovickýandFraser,2022). The
RNN embedding model employs gated recurrent
units (GRU) (Cho et al., 2014). The model was
trainedusingthreeequallyweightedlossfunctions:
EM,binarycrossentropy,andnon-matchingnega-
tivelog-likelihood.
Name Value
Embeddingmodel RNN
Embeddingsize 256
Hiddenlayers 2
Batchsize 512
Validationfrequency 50
Patience 10