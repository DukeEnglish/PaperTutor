PROXYQA: An Alternative Framework for Evaluating Long-Form
Text Generation with Large Language Models
HaochenTan♣†,ZhijiangGuo♠†,ZhanShi♢,LuXu♠,ZhiliLiu♠,♡,
XiaoguangLi♠,YashengWang♠,LifengShang♠,QunLiu♠,LinqiSong♣*
♣CityUniversityofHongKong♠HuaweiNoah’sArkLab
♢HuaweiHisilicon♡HongKongUniversityofScienceandTechnology
{haochetan2-c, linqi.song}@my.cityu.edu.hk
{guozhijiang, shizhan11, xu.lu2, liu.zhili}@huawei.com
{lixiaoguang11, wangyasheng, Shang.Lifeng, qun.liu}@huawei.com
Abstract
Traditional Evaluation
Crowdsource Eval
Large LanguageModels (LLMs)haveexhib-
Gold Reference
itedremarkablesuccessinlong-formcontext
comprehension tasks. However, their capac-
ity to generate long contents, such as reports
and articles, remains insufficiently explored.
Currentbenchmarksdonotadequatelyassess
Automatic Metrics
LLMs’abilitytoproduceinformativeandcom-
prehensive content, necessitating a more rig-
Meta Question: Please
Test LLM
orous evaluation approach. In this study, we introduce some of the highest-
grossing media franchises in
introduce PROXYQA, a framework for eval-
detail.
uatinglong-formtextgeneration,comprising
in-depthhuman-curatedmeta-questionsspan- Read
Proxy Question #n:
ningvariousdomains. Eachmeta-questioncon-
True or False: Star Wars has been
tainscorrespondingproxy-questionswithanno- expanded intovarious Read
filmsandother media,
tatedanswers. LLMsarepromptedtogenerate
includingtelevision series,video
extensive content in response to these meta- games,andcomic books.
questions. Utilizinganevaluatorandincorpo- Calculate Answer
Gold: True True
rating generated content as background con- Accuracy
Evaluator
text, PROXYQA evaluatesthequalityofgen-
ProxyQA
eratedcontentbasedontheevaluator’sperfor-
manceinansweringtheproxy-questions. We Figure 1: Prior efforts assess generated content by
matchingitwithreferencesthroughhumanevaluation
examinemultipleLLMs,emphasizingPROX-
YQA’s demanding nature as a high-quality
or automated metrics. PROXYQA evaluates texts by
checking if they contain sufficient information to an-
assessment tool. Human evaluation demon-
stratesthatevaluatingthroughproxy-questions swerasetofproxyquestions.
isahighlyself-consistentandhuman-criteria-
long-context LLMs evolve, several benchmarks
correlated validation method. The dataset
have emerged to evaluate their ability to handle
and leaderboard will be available at https:
extensivecontexts(Shahametal.,2023;Baietal.,
//github.com/Namco0816/ProxyQA.
2023;Anetal.,2023;Zhangetal.,2023). However,
1 Introduction
theseassessmentsprimarilyfocusonLLMs’com-
prehensionoflengthypassages,usingautomated
RecentLargeLanguageModels(LLMs)havemade
metricstomeasureperformance. Thisleavesasig-
significantadvancements(Brownetal.,2020;Tou-
nificant gap in understanding LLMs’ proficiency
vronetal.,2023a,b;OpenAI,2022a,2023b). GPU
ingeneratinglong-formtexts,anessentialaspect
technologyinnovationsandmemory-efficientatten-
thatrequiresfurtherinvestigation.
tionmechanisms(Daoetal.,2022;Dao,2023)have
furtherenabledLLMstomodelcontextsequences OneprimaryroadblocktounderstandingLLMs’
spanningtensofthousandsoftokens(Anthropic, capability to generate long-form texts is the lack
2023;OpenAI,2023c),pavingthewayforsophis- of competent evaluation methods. Current meth-
ticatedapplicationssuchasanalyzingcomplexsci- ods, often involving a combination of automated
entificessaysandgeneratingdetailedreports. As metricsandcrowdsourcedannotations,leavemuch
tobedesired(Xuetal.,2023). Forinstance,auto-
* Correspondingauthor.
†Theseauthorscontributedequally matedmetricsthatuseword-levelstring(Lin,2004)
4202
naJ
62
]LC.sc[
1v24051.1042:viXraor meaning representation matching (Yuan et al., Taorietal.,2023;ChiangandLee,2023;OpenAI,
2021)relyongoldreferences,whichunfortunately 2022a, 2023b), enhanced by strategies such as it-
do not exist for many generation tasks, such as erativereasoning(Yaoetal.,2023a)andretrieval
reportsoressaywriting. Furthermore,theseauto- augmentation(Bing,2023;Gemini,2023). Addi-
matedmetricsareinadequateforreliablyassessing tionally,asystematichumanevaluationiscarried
long-formcontentduetotheconsiderableandun- out and demonstrates that PROXYQA provides a
structuredspaceofpotentialoutputs(Celikyilmaz highly self-consistent scope of evaluation, which
etal.,2020;Krishnaetal.,2021). Humanevalua- surpasses the inter-human agreement rates but is
tion has its own set of issues too. Crowdsourced highly correlated with the majority preference of
workersmaylackthenecessaryexpertiseforevalu- humans,withaself-agreementrateover88%and
atingknowledge-richcontent,anddomainexperts’ 63%agreementrateagainsthuman.
subjectivepreferencescouldresultininconsistent
evaluations(Xuetal.,2023). Whilerecentstudies 2 RelatedWork
exploredusingLLMsforevaluation(Chiangand
2.1 Long-FormTextGeneration
Lee,2023;Liuetal.,2023),LLMshavebeenfound
to lack the most current information required for Significant strides have been made in long-form
precise verification. Moreover, their assessments text generation, particularly in story genera-
havebeenobservedtobeinconsistent(Shenetal., tion(Fanetal.,2019b;Xuetal.,2020),paragraph
2023). There is a clear need for more robust and completion (Kang and Hovy, 2020), long-term
preciseevaluationmethods. conversation (Xu et al., 2022) and article gener-
To bridge this gap, we propose PROXYQA, ation (Hua and Wang, 2020; Hu et al., 2022). A
which contains human-curated meta-questions closelyrelatedfieldislong-formquestionanswer-
spanning assorted fields, from computer science ing(Fanetal.,2019a;Dasigietal.,2021;Stelmakh
tohistory, eachnecessitatingrelevantdomainex- etal.,2022;Leeetal.,2023),whichinvolvesgen-
pertiseandpotentiallythemostup-to-dateknowl- eratingdetailedresponsestocomplexinformation-
edge. Eachmeta-questioniscoupledwithaseries seeking questions. ELI5 (Fan et al., 2019a) was
ofproxy-questionsandanswers,aimingtocapture apioneerdatasetforgeneratingexplanatorypara-
theessentialpointsofthebroadersubjectthemeta- graphsinresponsetoopen-endedquestions,utiliz-
questionpertainsto. Previousevaluationmethods inganswersfromReddit. QASPER(Dasigietal.,
gradethequalityofgeneratedtextbycomparingit 2021) and QASA (Lee et al., 2023) extend gen-
toareference. AsdepictedinFigure1,PROXYQA eral factoid questions to the domain of scientific
adoptsanalternativeapproachthatrequiresaneval- literature. Evaluating answers on these datasets
uator, which can be either human or machine, to relies on comparing the generated texts with the
answer proxy-questions based on the long-form provided single reference. However, open-ended
textproducedbyLLMs. IftheLLM’soutputissuf- questionscanbeansweredinmyriaddifferentways.
ficientlydetailedandaccurate,itshouldequipthe ASQA(Stelmakhetal.,2022)alsointroducesaset
evaluator with enough information to thoroughly ofdisambiguatedquestionsfromAmbigQA(Min
answerallassociatedproxy-questions. et al., 2023) for evaluating ambiguous questions.
PROXYQA offersseveralbenefits. Iteliminates Theyassumethatlong-formanswerstoambiguous
the need for direct comparison against a single questions should resolve ambiguity. In contrast,
goldreferencebyemployingproxyquestions,en- ProxyQA aims to gauge the informativeness and
abling a more accessible and subjective evalua- comprehensivenessoflong-formanswers,without
tion. Usingthisapproachallowsevaluatorswith- beingconfinedsolelytoambiguousquestions.
out specific domain knowledge to assess content.
2.2 TextGenerationEvaluation
Additionally, unlike previous datasets compiled
fromonlinesourcesthatriskembeddinganswers Automated metrics such as surface form match-
used in training LLMs (Nguyen et al., 2016; Fan ing(Lin,2004;BanerjeeandLavie,2005)andse-
etal.,2019a),potentiallyleadingtodatacontamina- mantic representation comparison (Zhang et al.,
tion(Sainzetal.,2023),alltheproxy-questionsand 2020;Yuanetal.,2021),facechallengeswithlong-
answersarehiddentothepublic,therebyprevent- form content due to their inability to handle the
ing data leakage. We apply PROXYQA to exten- diversityandcomplexityofpotentialoutputs(Ce-
sivelytestdifferentLLMs(Touvronetal.,2023a,b; likyilmazetal.,2020;Krishnaetal.,2021). TheyGame What exactly is a First-person shooter (FPS) Music 5% Others I In nt trr ood du uc ce e t th he e h too ptt 3e s mt och stil i p p rie cp ep de pr a, ih no tw in git s i as n b dr e thd e? ir artists.4%
game, could you generate a detailed report
Who are the best-selling
about it? 7% music artists of all time? Film History
AI A En cd on ow mh ica st are their genres? 4% W t Inh teh e i r rc e nh c a c o to ir odu n n i ant lr ti Fe h es e a h tB uo e rl sd et I a mn nt adr jo o 2d rn u id nc ve In e t d nh u te is o tc nro i sau .ln Rtr ei ve os lt uh ta iot n in s v ao nlv de ld is i tn t h1 est
… introduce these normalization W evh ea nt t sim (lip ka ec tt h d eo O m lye mga p is cp so or rt s F Oi slm ca rc sa t ae sg oo fr y 2 0a 2t 2t ?he C tho em sp ca ar lee , t dh ue r atw tioo n w , o wr eld a pw oa nr ,s a i nn d t e inrm vos l vo ef d
World Cup) have on host cities countries.
algorithm in detail. economically? 5% 10%
Technology Sports
… analyze and explain these
positional encoding algorithms. … introduce the H100 GPU and Is it possible for human to finish the
compare it with TPU-V5e in marathon within 2 hours and 1 minute?
detail. Who is holding the record?
… explain why is it significant in
Introduce European Championship and
NLP? …compare the VR headset from World Cup in detail.
some of the biggest tech
companies. What is the Grand Slam? please tell the
33% 15% details about it. 17%
Figure2: Meta-questionsinPROXYQAcovervariousdomains,suchasAIresearch,historicaleventinvestigations,
sportsandentertainmentanalysis,andmore.
oftendonotalignwithhumanjudgment(Xuetal., 3.1 Meta-Question
2023). AttemptstouseLLMsforevaluation(Chi-
Experiencedresearchersconductedtheannotation
angandLee,2023;Liuetal.,2023)arehindered
ofthemeta-questions. Meta-questionscanbeclas-
bytheirlimitedaccesstocurrentinformationand
sified into two levels of difficulty: easy and hard.
inconsistency in performance (Shen et al., 2023).
Easyquestionscanbesufficientlyansweredusing
Evaluators also have difficulties, particularly if
onlyinformationfromWikipedia,whilehardques-
they lack expertise, which can impair their judg-
tionsdemandtheintegrationofWikipediacontent
mentonkeydimensionslikeinformativenessand
withinsightsderivedfromawiderrangeofopen-
factuality (Gillick and Liu, 2010; Iskender et al.,
domainknowledgesourcestoformulateacompre-
2020). Strategies to enhance human evaluations
hensiveresponse. Generally,mostLLMscaneffec-
includeA/Btesting,asseenwithHURDLES(Kr-
tivelyaddresseasyquestionsgiventheirextensive
ishna et al., 2021) and WebGPT (Nakano et al.,
training on Wikipedia corpora. Conversely, hard
2021),withthelatterdemonstratingthatproviding
questionsposeachallengetothemodels’capacity
evidencehelpsannotatorsmakemoreinformedde-
to acquire information beyond the scope of com-
cisions. ThelistofproxyquestionsinPROXYQA
monlyusedpre-trainingdata,necessitatingaccess
can be viewed as evidence to assist the evaluator
to specialized private corpora, web searches, or
inmakingdecisions. Whilesomeresearchfocused
documentretrievalforanadequateresponse. Irre-
oncoherence(Goyaletal.,2022;Jiangetal.,2022;
spectiveofthelevelofdifficulty,allmeta-questions
Dengetal.,2022)andfactuality(GoyalandDur-
withinPROXYQArequireLLMstogeneratethor-
rett,2020;Labanetal.,2022;Minetal.,2023)in
oughanddetailedreportsorarticles.
relatedtaskslikesummarization,ourworkempha-
DomainSelectionAnnotatorswereinstructedto
sizesinformativenessandcoverage.
initiate meta-questions in areas with which they
were most familiar or had a keen interest. The
3 DataCollectionandAnnotation
predominant meta-questions pertained to preva-
We collected 100 human-curated meta-questions lentsubjectssuchasAIresearch,sports,andgam-
requiringdetailedandinformativeresponsesacross ing. Nonetheless,thedatasetalsocomprisesmeta-
various domains, such as computer science re- questionsassociatedwithtopicslikeinfrastructure
search, historical event investigations, sports and andagriculture,whichservetochallengelanguage
entertainment analysis, and more. The topic dis- modelswithlesspopularfactsandtopics(Mallen
tribution is provided in Figure 2. To ensure the et al., 2023). Figure 2 illustrates the distribution
highqualityofourquestions,expertsarerequired ofquestionsacrossthesediversedomains. Forin-
to formulate in-depth and high-quality questions stance, a pertinent question within the Computer
adhering to strict criteria: 1) answerable through Sciencedomaincouldbe: “Couldyouelaborateon
thorough research, 2) aligned with real-life sce- thedevelopmentofModelParallelismandPipeline
narios,3)devoidofoffensiveorethicalconcerns, Parallelism,detailingkeymilestonesandcontribu-
4) without an absolute or unique answer, and 5) tions?"Thismeta-questionalignswithinterestsin
addressableinlong-formreportsorarticles. parallel computing techniques. In contrast, ques-tionsofthesort“DidAristotleusealaptop?” from platforms,thereforepreservingtheintegrityofour
StrategyQA(Gevaetal.,2021)areomitteddueto dataset. As the foundation of the evaluation in
theirlackofoccurrenceinrealisticsettings. PROXYQA, proxy-questions are curated through
a multi-round annotation process, where experts
3.2 Proxy-Question
iterativelyexchangethemeta-questionswhilesup-
Toapproximatethesubjectiveassessmentwithan plementing and verifying proxy-questions anno-
objectivemetric,weutilizegeneratedtextsascon- tated by the others. Each meta-question is thus
textual background. An evaluator leverages the repeatedlygiventodifferentexpertstolabeldiffer-
contextualbackgroundtoansweraseriesofproxy- entproxy-questions,untilaconsensusisreached
questions, aiming to capture the critical contents that all experts agree that the points covered by
of the boarder subject the meta-question pertains theproxy-questionsaresufficientlycomprehensive.
to. Ifthecontextualbackgroundissufficientlyde- Suchanalternatelabelingprocessensuresamulti-
tailed and accurate, it should equip the evaluator perspective rubric. After the iterative annotation,
withenoughinformationtothoroughlyanswerall wemeasuretheinter-annotatoragreementofPROX-
associatedproxy-questions. Bydoingso,theeval- YQA.Werandomlyextractedasubsetof50proxy-
uation of the generated reports is proxied to the questionsandtaskedtheannotatorstore-annotate
accuracyofourproposedproxy-questions. Experts them. Following the Kazemi et al. (2021), the
aretaskedwithidentifyingthepivotalcontentthat Randolph’sfree-marginalmulti-raterκ(Randolph,
asatisfactoryanswertoameta-questionmustcon- 2010), an alternative to Fleiss’ κ are measured.
tain. Then they craft a series of proxy-questions PROXYQA achievesκ = 0.936thanksforthein-
thatprobetheseidentifiedkeypoints. Forinstance, troductionoftheiterativeannotationprocess.
regarding the example of model parallelism and
pipeline parallelism mentioned in Section 3.1, a 4 Experiments
thoroughanswerwouldincorporatein-depthinfor-
4.1 BaselineModels
mation about Gpipe (Huang et al., 2019), Mega-
tron(Shoeybietal.,2019),andotherpertinentsub- Allthebaselinesarepromptedwith“Writeawell-
jects. Therefore,annotatorsshoulddevelopproxy- structuredandextensivereporttoanswertheques-
questionsthatspecificallyfocusonGpipe,Mega- tion: [METAQUESTION]”.Wethenemploythe
tronandotherrelatedtopics. generatedresultsasthecontextualbackgroundand
Difficulty Level We present straightforward and force the GPT-4 and GPT-4-turbo to answer the
concise proxy-questions, deliberately avoiding proxy-questionsaccordingly. Multiplecompetitive
multi-hop and complex reasoning queries. Each baselinesaretestedunder PROXYQA.
annotatedresponseisprovidedinabooleanformat, BaseLLaMAisasetofopen-sourcedLLMspre-
ensuring that evaluators can effortlessly answer trained on diverse sources spanning multiple do-
these proxy-questions, given a sufficiently high- mains (Touvron et al., 2023a,b). The pretraining
qualitygeneratedcontext. Conversely,ifanevalu- corpora include the June 2022 Wikipedia dumps,
ator struggles to address these elementary proxy- which should enable the LLaMA family to ef-
questions based on the provided context, it indi- fectively address most ‘easy’ meta-questions in
catesasignificantdeficiencyinthegeneratedcon- PROXYQA.OurexperimentevaluatesLLaMA-7B,
text,withcrucialinformationbeingabsent. LLaMA2-7B,andLLaMA2-13B.
Instruction-Finetuned LLaMA includes Vi-
3.3 QualityAssurance
cuna (Chiang et al., 2023), Alpaca (Taori et al.,
The meta-question and its corresponding proxy- 2023), LLaMA2-Chat (Touvron et al., 2023b).
questionswereannotatedandquality-checkediter- Vicuna is a chat assistant trained by fine-tuning
atively. Duringtheannotation,weexcludedthree LLaMAonaround70kuser-sharedconversations
meta-questionsthatwereoffensive,politicallysen- collectedfromShareGPT(Eccleston,2022). Sim-
sitive, ethically concerning, or not safe for work ilarly, Alpaca is trained with 52k self-instructed
(NSFW). 8 meta-questions and their correspond- demonstrations adapted text-davinci-003 (Ope-
ingsimilarqueriesarereplacedastheyhavebeen nAI, 2022b). As an extension of base LLaMA2,
previouslypostedonrelevantRedditorQuorafo- LLaMA2-Chat is optimized specifically for dia-
rums. This ensures that LLMs cannot generate logueusageofover1millioninstructions.
answers by directly copying content from these OpenAI APIs includes GPT-3.5-turbo(OpenAI,GPT-4 GPT-4-Turbo Ensemble
Easy Hard avg. Easy Hard avg. Easy Hard avg.
BaseLLaMA
LLaMA-7B 5.25 0.68 3.05 5.89 1.23 3.64 5.57 0.96 3.34
LLaMA2-7B 4.74 0.55 2.72 5.38 0.55 3.04 5.06 0.55 2.88
LLaMA2-13B 6.15 1.65 3.97 6.91 1.37 4.24 6.53 1.51 4.11
Instruction-FinetunedLLaMA
Alpaca-7B 12.42 5.62 9.14 14.60 9.33 12.05 13.51 7.48 10.60
Vicuna-13B 19.85 17.15 18.54 22.66 21.26 21.99 21.25 19.20 20.26
LLaMA2-7B-Chat 21.25 16.74 19.07 20.23 18.11 19.20 20.74 17.42 19.14
LLaMA2-13B-Chat 21.13 16.87 19.07 22.02 17.42 19.80 21.57 17.15 19.44
GPTAPIs
GPT-3.5-Turbo 25.61 21.40 23.57 26.12 22.36 24.30 25.87 22.97 23.94
GPT-4 30.35 23.05 26.82 30.35 23.55 27.55 30.35 23.80 27.19
GPT-4-Turbo 35.21 31.69 33.50 34.83 33.88 34.37 35.02 32.78 33.94
Web-AugmentedLLMs
ReAct(GPT-4) 20.74 13.72 17.35 20.49 13.17 16.95 20.61 13.44 17.15
ReAct(GPT-4-Turbo) 23.94 18.11 21.13 23.56 18.79 21.26 23.75 18.45 21.19
Bard(GeminiPro) 26.63 22.22 24.50 25.48 25.51 25.50 25.06 23.87 25.00
NewBing(CreativeMode) 39.56 37.72 38.67 40.33 39.78 40.06 39.95 38.75 39.37
Table1: PerformanceofvariousLLMsonPROXYQA,accuracyontheeasyandhardsplitsarereported.
2023a),GPT-4andGPT-4-turbo(OpenAI,2023b). answers to the proxy-questions strictly from the
The default decoding configuration is utilized to informationpresentedwithinthecontextualback-
generateresponses,whilethemaximumdecoding ground. Theaccuracyoftheevaluators’responses
lengthissetasitsmaximumlimitation. BothGPT- totheseproxy-questionsismethodicallyquantified,
3.5-turbo and GPT-4-turbo are of version 1106, serving as an indicator of the overall quality of
while GPT-4 corresponds to GPT-4-0613. The thegeneratedreports. Moreover,weensemblethe
training data for GPT-4-turbo is up-to-date as of evaluation results from GPT-4 and GPT-4-Turbo
April2023,whiletheremainingmodelsaretrained to reinforce the reliability and robustness of the
withdatauptoSeptember2021. assessment. GPT-4 series is required to extract
Web-Augmented LLMs utilize external search thesalientcontentandanswertheproxy-questions
APIsareevaluated. Specifically,GPT-4andGPT-4- accuratelyevenwhentheessentialcontentisscat-
turboareintegratedwiththeGoogleSearchAPIun- teredinanoisygeneratedreport. Eachevaluator’s
dertheconfigurationofReAct(Yaoetal.,2023b). accuracyontheproxy-questionsiscalculatedand
These models are tasked with processing meta- ensembledtorepresentthevalidationresults.
questions, reasoning through search traces, and
extractingrelevantcontentfromsearchresultsfrom 4.3 MainResults
theinternetacrossmultipleturns. Theimplementa-
Table 1 presents the comparative performance of
tionisadoptedfromLangChain(Chase,2022). In
different large language models (LLMs) on the
additiontoReAct,theperformanceofNewBing
PROXYQA benchmark. The base versions of
(creativemode)(Bing,2023)andBardwithGemini
the LLaMAs series demonstrate a limited capac-
Pro(Gemini,2023)arealsoassessed.
ity to produce comprehensive and accurate long-
formcontent. However,notableenhancementsare
4.2 Evaluators
achievedthroughinstruction-basedsupervisedfine-
Byincorporatingthegeneratedreportsascontex- tuning(SFT).Interestingly,theVicuna-13Bmodel
tual background, GPT-4 and its variant, GPT-4- transcendsmostSFT-augmentedLLaMAvariants
turbo, are utilized to answer the proxy-questions in average accuracy and total points, evidencing
basedonthecontext. Theobjectiveassessmentof itssuperiorcapabilityindeliveringbroadandde-
thesereportsissubsequentlyanchoredtothepre- pendablecontent. Incontrast,whenevaluatingthe
cisionoftheGPT-basedevaluator. Theevaluator generationofdetailedreportsandarticles,theopen-
acquiresinformativeknowledgeonlyifthegener- sourceLLMslagbehindtheirGPT-familycounter-
atedcontextualbackgroundiscomprehensiveand parts. Infact,eventheGPT-3.5-turbooutperforms
extensive. We prompt the evaluator to formulate theentiresuiteofopen-sourceLLMsbyamargin,andGPT-4-turbomaintainsasubstantiallead. This Bard GPT-4-Turbo ReAct
underscorestheconsiderablegapthatopen-source Llama2-Chat-13B New Bing Vicuna-13B
LLMs must bridge to match the performance of technology
theirproprietarycounterparts. history
Unexpectedly, equipping GPT-4 and GPT-4- sports
turbo with ReAct results in a marked decrease
in performance. We hypothesize that this down- game
turn arises because ReAct repurposes the LLMs
CS
into a role more akin to a planner and executor, 0 0.1 0.2 0.3 0.4 0.5
constrainingtheircapacityascomprehensivepara-
music
metric knowledge bases. Meanwhile, Bard has
made a notable entry, surpassing GPT-3.5-turbo
others
significantly. ThispositionsBardasaformidable
film
competitorinthegenerationofcomprehensiveand
economics
extensivecontent. Unsurprisingly,empoweredby Figure3: PerformanceofLLMsondifferentdomains.
theBingsearchengineandGPT-4,theNewBing
outperformsallthebaselinesbyasignificantmar- ginessignificantlyenhancesthecapabilitiesofweb-
gin. Thisshowsthatareasonableweb-augmented augmentedlanguagemodelswithinthesenichedo-
solution can greatly improve the performance of mains. Thisempiricalobservationalignswiththe
LLMsandhelpgeneratemoreinformativecontent. insightspresentedbyMallenetal.,evidencingthat
Difficulty Level: Meta-questions in PROXYQA theparametricLLMsexhibitsuperiorperformance
are stratified by difficulty, basing the classifica- overthenon-parametricexternalknowledgebase
tion on whether they can be well-solved solely inwell-troddendomains,yetunderperformwhen
with information from Wikipedia. Table 1 illus- confrontedwithmoreunpopulartopics.
tratesapronounceddeclineinperformanceamong Robustness and Knowledge Coverage: PROX-
mostlargelanguagemodels(LLMs)onthemore YQA iscuratedthroughamulti-roundannotation
challenging subset of questions. Notably, even process, where experts iteratively exchange the
thepowerfulGPT-4exhibitsamarkeddecreasein meta-questionswhilesupplementingandverifying
efficacy(6.55 ↓). In contrast, the web-enhanced proxy-questions. Each round encapsulates a cu-
Bard and New Bing perform more robustly than ratedcollectionofproxy-questionsthathavebeen
otherLLMs,exhibitingacomparativelyminorloss identifiedascriticalbytheconsensusofexpertsin-
in performance. We attribute this to the fact that volved. Totesttherobustnessandknowledgecover-
meta-questionsnotcoveredbyWikipediaarerela- ageoftheLLMs,wetestLLMsonsubsetsfromthe
tivelyunpopularandlesslikelytobewell-learned firstannotationround-LLMsonlyneedtosatisfy
in most pre-training and SFT stages, leading to thecriteriaofoneexpertforagivenmeta-question.
challengesforLLMsinencodingsuchspecialized We then compare this performance with the re-
knowledge. Conversely, the web-augmented lan- sults obtained from the full, expert-consolidated
guage model compensates for this limitation by set,therebymeasuringthemodel’sknowledgecov-
utilizingsearchenginestoaccessawiderspectrum erage capabilities. Figure 4 presents a compari-
of online resources, thereby enriching its under- sonoftheLLMs’performanceonboththeexpert-
standingandimprovingitsresponseaccuracy. consolidatedandsingle-expert-focusedsubsets. As
Domain: Figure3illustratesthecomparativeper- the iterative verification and supplementation of
formancetrajectoriesofdiverselanguagemodels proxy-questions proceed, the performance of all
when tasked with meta-questions spanning mul- modelsdecreases,suggestingthatLLMsareunable
tiple domains. The data reveals a notable uni- tocatertothepreferencesofeveryindividual. Re-
formity in trends across the more established do- markably,NewBingsurpassesGPT-4,whichdoes
mains. However, it is within the less frequented notpossessaccesstoanexternalknowledgebase.
domains—categorizedcollectivelyas‘other’—that Moreover, the performance degradation of New
we observe a marked deterioration in model effi- Bingonthecompletesetislesspronouncedthan
cacy, even the strongest baseline GPT-4-turbo is that of GPT-4 and most other LLMs. This obser-
inadequate to generate well-structured and infor- vationfurtherreinforcestheeffectivenessofweb-
mative reports. In contrast, including search en- augmented LLMs in the context of open-domainvalidate the effectiveness of our proposed eval-
Expert Consolidated
Single Expert
uation approach, we compare our evaluation re-
0.4
sults with those of human evaluation. We en-
gagefivewell-educatedannotators,whohavenot
0.3 participated in the annotation of the meta and
proxy-questionsof PROXYQA,toscoreandrank
randomly sampled reports generated by different
0.2
LLMs. Wesampled10meta-questionsfromPROX-
YQA and employed four LLMs to generate com-
0.1 prehensivereports. Foreachmeta-question,human
evaluatorsareinstructedtoassignscoresranging
from 0 to 5 for the corresponding generated re-
0.0 llama-7b llama2-13b llama2-13b-chat vicuna-13b (GR Pe TA -c 4t -turbo) bard gpt-4 gpt-4-turbo new bing p tho ert ss e. ttA ins gsa ip no Min Tt -o Bf ec no cm hp (Zar hi es no gn, ew tae l.a ,l 2s 0o 2f 3o )l ,lo ww
e
Models
Figure 4: Performance difference on the experts- utilizebothGPT-Seperate,whichevaluatesasingle
consolidatedandsingle-expert-focusedset. reportatatime,andGPT-Batch,whichevaluates
Avg.Len. Acc. andcomparesmultiplereportssimultaneously,to
LLaMA2-13B 1906.87 4.11 scoreandrankeachreport. Giventhatfivehuman
LLaMA2-13B-Chat 869.42 19.44
evaluatorsareinvolvedinthecomparison,ween-
Vicuna-13B 727.84 20.26
sure fairness and robustness by requiring GPT-S,
GPT-3.5-turbo 823.32 23.94
GPT-B,andourproposed PROXYQA toevaluate
GPT-4 744.0 27.19
eachreportfivetimes. Wethencalculatethewin
GPT-4-turbo 1029.47 33.94
ratebasedonthepairwisecomparison.
ReAct(GPT-4-turbo) 355.80 21.19
As shown in Figure 5, majority of the evalu-
Bard(GeminiPro) 922.83 25.00
ation results highly correlated with the human’s
NewBing 1167.65 39.37
choices. Specifically, evaluators generally recog-
Table2: Averagewordcountofthegeneratedreports.
nizethequalityofthereportsgeneratedbyGPT-4-
turboandNewBing,i.e.,theirwinratesaremuch
knowledge-intensivelongcontentsgeneration.
betterthanthoseofVicunaandLlama2-Chat. Itis
Generation Length: The average generation
worthnoting,however,thatbothGPT-SandGPT-B
lengths are presented in Table 2. It is essential
exhibitoverconfidenceinthequalityofthereports
to emphasize that the degree of informativeness
generatedbyGPT-4-turbocomparedtoNewBing.
andcomprehensivenessisnotdirectlyproportional
Incontrast,bothhumanevaluatorsandProxyQA
to the length of the generated content. Specifi-
exhibit a preference for New Bing over GPT-4-
cally,LLaMA2-13Bgenerateslengthycontent,yet
turbo. This outcome not only attests to the effec-
itexhibitsthelowestqualityingeneratingcontex-
tivenessofProxyQAbutalsodemonstratesthecor-
tualbackgroundonPROXYQA.Incontrast,GPT-
relationbetweenProxyQAandhumanevaluation.
4-turboproducesconcisecontentwhileconveying
extensiveandcomprehensiveinformation. More-
5.2 AgreementEvaluation
over,whenGPT-4isincorporatedwiththesearch
To thoroughly examine the self-consistency of
engine,theNewBingCreativeModeyieldshighly
informativeandin-depthcontent,significantlysur-
PROXYQA,andthecorrelationagainsthumaneval-
uation criteria, we investigate the agreement rate
passing all other baseline models with an accept-
ableincreaseingenerationlength. Thesefindings
of human assessment and PROXYQA. Two cat-
egories of agreement rates are explored. To ef-
suggestthatimprovingthereadabilityandinforma-
fectively gauge the self-consistency level of our
tivenessofgeneratedcontentwithinlimitedtoken
budgetsremainsanareaforsystematicexploration.
proposedPROXYQA,weassesstheself-agreement
rate,whichcalculatestheinter-evaluatoragreement
5 Analysis rate. Furthermore, we establish that the evalua-
tionmethodproposedinthe PROXYQAexhibitsa
5.1 WinRate
strongcorrelationwithhumanjudgmentbydeter-
Weexaminethepairwisewinrateamongvarious miningtheagreementratebetweenhumanevalua-
LLMs on meta-questions within PROXYQA. To tions,referredtoasthehumanagreementrate. We
.ccA
elbmesnESelf G2G G2M M2G
GPT-4-turbo LLama2-Chat New Bing GPT-4-turbo LLama2-Chat New Bing
GPT-S 48.65 45.49 47.62 30.00
92.67% 76.11% 96.67% 90.00% 69.84% 78.89%
Vicuna-13B GPT-B 51.17 43.86 36.66 36.66
7.33% 23.89% 3.33% 10.00% 30.16% 21.11%
ProxyQA 88.00 66.00 63.33 66.19
23.81% 10.00% 60.56% 20.00% Human 52.19 - - -
New Bing
86.19% 90.00% 39.44% 80.00%
Table3: Agreementbetweeneachevaluationmethod
85.72% 84.77% andhumanevaluation. Self-agreementisalsoreported.
LLama2-Chat
24.28% 15.23%
Human Evaluation GPT-S Evaluation
overallagreementbetweenanevaluationmethod
GPT-4-turbo LLama2-Chat New Bing GPT-4-turbo LLama2-Chat New Bing and the human. Let V = {e ,...,e },V =
e 1 n h
100% 86.79% 96.67% 100% 90.00% 100%
{h ,...,h }representthesetofthevotingresults
Vicuna-13B 1 n
0.00% 13.21% 3.33% 0.00% 10.00% 0.00%
ofanevaluationmethodandhuman,respectively.
64.35% 6.67% 26.00% 3.33% ThemajorityvoteofasetisrepresentedasM(·).
New Bing
35.65% 93.33% 84.00% 96.67% Thentheagreementiscalculatedas:
100% 88.00%
LLama2-Chat 0% GPT-B Evaluation 12.00% ProxyQA Evaluation R ≜ 1 (cid:88)n 1{h = M(V )} (2)
M2G nC i e
2
Figure5: Comparisonofwinrateofvariousevaluation i=1
methods. GPT-evaluatorsarehighlyoverconfidentin 1 (cid:88)n
theresultsproducedbyGPT-4-turbo,whilePROXYQA R G2M ≜ nC 1{e i = M(V h)} (3)
2
significantlycorrelatedwithhumanpreference. i=1
n n
1 (cid:88)(cid:88)
employGPT-SeperateandGPT-Batch,asdiscussed R ≜ 1{e = h } (4)
G2G n2 i j
insection5.1,forcomparisonpurposes. i=1 j=1
Setup: Followingthesettingsinsection5.1,20re-
portsgeneratedbyGPT-4-turboandNewBingare
Table3illustratesthatPROXYQAoffersahighly
self-consistentmetricforevaluatinglongcontent
evaluated and compared by experts that have not
participated in the annotation of PROXYQA. We
generation. Furthermore, PROXYQA exhibits a
strongcorrelationwiththemajorityopinionofhu-
analyzetheagreementbetweendifferentevaluation
man evaluators, thereby emphasizing its efficacy
methodsandhumanevaluations.
invalidatinglong-formcontent. Notably,thesub-
Self-agreement: Givenapairofmeta-questionand
jective preferences of human experts vary, as the
itscorrespondinggeneratedreport,eachevaluation
agreementratereachesonly52.19%,leadingtorel-
methodisrequiredtoscoreandvotethepreferred
ativelyinconsistentevaluations. Thiscouldalsobe
reportsn times. Let V = {v ,...,v }be theset
1 n
of voting results, then the self-agreement rate is
observedinGPT-SandGPT-B.Incontrast,PROX-
calculatedas:
YQA achievesan88.00%agreementrate,indicat-
ingthatitsobjectiveevaluationrubricrendersita
1 (cid:88)n (cid:88)n highlyself-consistentperformanceindicator. More-
R ≜ 1{v = v } (1)
self i j over,whenevaluatingtheconsensusbetweendif-
n
i=1j=i+1 ferent evaluation methods and the majority opin-
Where1{·}denotetheindicatorfunction. The ionofhumanexperts,bothGPT-SandGPT-Bare
self-agreement rate, denoted as R , quantifies completelycontrarytohumanpreference. GPT-as-
self
theself-consistencyofanevaluationmethod. judgesisoverconfidentinthereportsgeneratedby
Human-agreement: The calculation is divided GPT-4-turboandNewBing,whichbecomeseven
intoMajority-to-Group(M2G),Group-to-Majority moreextremewhenevaluatingwithGPT-B,which
(G2M)andGroup-to-Group(G2G).WhereR comparesthereportsinthesamebatch. However,
M2G
quantifiestheproportionofthemajorityvoteofa thecriteriaof PROXYQA significantlyalignwith
specificevaluationmethodthatisinconcordance the majority opinion of human experts, surpass-
with the overall votes of human evaluation. Con- ing GPT-as-judges in all human-agreement rates
versely,R calculatestheproportionofoverall with a substantial margin and attaining a 66.19%
G2M
votesofanevaluationmethodthatconcurwiththe M2G-agreement rate. These findings provide ro-
majorityvoteofhumans,indicatinghowwellthe bust evidence that the proposed PROXYQA can
evaluation criteria are aligned with the majority effectively and reliably assess the capabilities of
opinionofhumans. R providesaviewofthe LLMsingeneratinglong-formcontent.
G2G6 Conclusion AnnualConferenceonNeuralInformationProcess-
ing Systems 2020, NeurIPS 2020, December 6-12,
Inconclusion,weintroduce PROXYQA,aframe- 2020,virtual.
workdesignedtoevaluateLLMs’abilitytogener-
Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.
atelong-formtext. Unliketraditionalmethodsthat
2020. Evaluation of text generation: A survey.
relyonadirectcomparisonwithareferencetext,
CoRR,abs/2006.14799.
PROXYQA includes human-curated proxy ques-
tionsthatfocusongeneratedcontent’sinformative- HarrisonChase.2022. Langchain.
ness. Byrequiringanevaluatortousetheinforma-
ShouyuanChen,ShermanWong,LiangjianChen,and
tionprovidedintheLLM-generatedtexttoanswer
YuandongTian.2023. Extendingcontextwindowof
proxy-questions,theframeworkassessesthetext’s large language models via positional interpolation.
depthandaccuracywithoutneedingafixedrefer- CoRR,abs/2306.15595.
enceorexpertknowledge. Bymitigatingconcerns
DavidCheng-HanChiangandHung-yiLee.2023. Can
overdatacontaminationandensuringtherelevance
large language models be an alternative to human
andfreshnessofevaluationcontent,PROXYQAnot evaluations? In Proceedings of the 61st Annual
onlyenhancesourunderstandingofLLMsbutalso Meeting of the Association for Computational Lin-
guistics(Volume1:LongPapers),ACL2023,Toronto,
drives innovation towards developing long-form
Canada,July9-14,2023,pages15607–15631.Asso-
generationmethodswithLLMs.
ciationforComputationalLinguistics.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
References ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephE.Gonzalez,Ion
Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Stoica, and Eric P. Xing. 2023. Vicuna: An open-
JunZhang,LingpengKong,andXipengQiu.2023. sourcechatbotimpressinggpt-4with90%*chatgpt
L-eval: Institutingstandardizedevaluationforlong quality.
contextlanguagemodels. CoRR,abs/2307.11088.
TriDao.2023. Flashattention-2: Fasterattentionwith
Anthropic.2023. Claude2.1with200KContextWin- better parallelism and work partitioning. CoRR,
dow. abs/2307.08691.
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,
JiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao andChristopherRé.2022. Flashattention: Fastand
Liu,AohanZeng,LeiHou,YuxiaoDong,JieTang, memory-efficientexactattentionwithio-awareness.
andJuanziLi.2023. Longbench: Abilingual,mul- InNeurIPS.
titask benchmark for long context understanding.
CoRR,abs/2308.14508. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,
NoahA.Smith,andMattGardner.2021. Adataset
SatanjeevBanerjeeandAlonLavie.2005. METEOR: of information-seeking questions and answers an-
anautomaticmetricforMTevaluationwithimproved chored in research papers. In Proceedings of the
correlationwithhumanjudgments. InProceedings 2021ConferenceoftheNorthAmericanChapterof
oftheWorkshoponIntrinsicandExtrinsicEvalua- theAssociationforComputationalLinguistics: Hu-
tionMeasuresforMachineTranslationand/orSum- manLanguageTechnologies,NAACL-HLT2021,On-
marization@ACL2005,AnnArbor,Michigan,USA, line,June6-11,2021,pages4599–4610.Association
June29,2005,pages65–72.AssociationforCompu- forComputationalLinguistics.
tationalLinguistics.
YuntianDeng,VolodymyrKuleshov,andAlexanderM.
Bing.2023. Ai-poweredbingwithchatgpt’sgpt-4. Rush.2022. Modelcriticismforlong-formtextgen-
eration. InProceedingsofthe2022Conferenceon
TomB.Brown,BenjaminMann,NickRyder,Melanie EmpiricalMethodsinNaturalLanguageProcessing,
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind EMNLP2022,AbuDhabi,UnitedArabEmirates,De-
Neelakantan,PranavShyam,GirishSastry,Amanda cember7-11,2022,pages11887–11912.Association
Askell, Sandhini Agarwal, Ariel Herbert-Voss, forComputationalLinguistics.
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, ZicanDong,TianyiTang,JunyiLi,WayneXinZhao,
ClemensWinter,ChristopherHesse,MarkChen,Eric and Ji-Rong Wen. 2023. BAMBOO: A compre-
Sigler,MateuszLitwin,ScottGray,BenjaminChess, hensive benchmark for evaluating long text mod-
Jack Clark, Christopher Berner, Sam McCandlish, eling capacities of large language models. CoRR,
Alec Radford, Ilya Sutskever, and Dario Amodei. abs/2309.13345.
2020. Languagemodelsarefew-shotlearners. InAd-
vancesinNeuralInformationProcessingSystems33: DomEccleston.2022. Sharegpt.AngelaFan,YacineJernite,EthanPerez,DavidGrang- JiquanNgiam,QuocV.Le,YonghuiWu,andZhifeng
ier,JasonWeston,andMichaelAuli.2019a. ELI5: Chen.2019. GPipe: EfficientTrainingofGiantNeu-
long form question answering. In Proceedings of ralNetworksUsingPipelineParallelism.CurranAs-
the57thConferenceoftheAssociationforCompu- sociatesInc.,RedHook,NY,USA.
tationalLinguistics,ACL2019,Florence,Italy,July
28-August2,2019,Volume1: LongPapers,pages Neslihan Iskender, Tim Polzehl, and Sebastian Mller.
3558–3567.AssociationforComputationalLinguis- 2020. Bestpracticesforcrowd-basedevaluationof
tics. Germansummarization: Comparingcrowd, expert
andautomaticevaluation. InProceedingsoftheFirst
AngelaFan,MikeLewis,andYannN.Dauphin.2019b.
WorkshoponEvaluationandComparisonofNLPSys-
Strategies for structuring story generation. In Pro-
tems,pages164–175,Online.AssociationforCom-
ceedingsofthe57thConferenceoftheAssociation
putationalLinguistics.
forComputationalLinguistics,ACL2019,Florence,
Italy,July28-August2,2019,Volume1: LongPa-
Yuchen Jiang, Tianyu Liu, Shuming Ma, Dongdong
pers, pages 2650–2660. Association for Computa-
Zhang,JianYang,HaoyangHuang,RicoSennrich,
tionalLinguistics.
RyanCotterell,MrinmayaSachan,andMingZhou.
Gemini. 2023. Gemini: A family of highly capable 2022. BlonDe: Anautomaticevaluationmetricfor
multimodalmodels. document-levelmachinetranslation. InProceedings
ofthe2022ConferenceoftheNorthAmericanChap-
MorGeva,DanielKhashabi,EladSegal,TusharKhot, teroftheAssociationforComputationalLinguistics:
DanRoth,andJonathanBerant.2021. Didaristotle HumanLanguageTechnologies,pages1550–1565,
usealaptop? aquestionansweringbenchmarkwith Seattle,UnitedStates.AssociationforComputational
implicit reasoning strategies. Transactions of the Linguistics.
Association for Computational Linguistics, 9:346–
361. DongyeopKangandEduardH.Hovy.2020. Planahead:
Self-supervisedtextplanningforparagraphcomple-
DanGillickandYangLiu.2010. Non-expertevaluation
tiontask. InProceedingsofthe2020Conferenceon
ofsummarizationsystemsisrisky. InProceedingsof
EmpiricalMethodsinNaturalLanguageProcessing,
theNAACLHLT2010WorkshoponCreatingSpeech
EMNLP2020,Online,November16-20,2020,pages
andLanguageDatawithAmazon’sMechanicalTurk,
6533–6543.AssociationforComputationalLinguis-
pages148–151,LosAngeles.AssociationforCom-
tics.
putationalLinguistics.
AshkanKazemi,KiranGarimella,DevinGaffney,and
TanyaGoyalandGregDurrett.2020. Evaluatingfactu-
ScottHale.2021. ClaimmatchingbeyondEnglish
alityingenerationwithdependency-levelentailment.
toscaleglobalfact-checking. InProceedingsofthe
InFindingsoftheAssociationforComputationalLin-
59thAnnualMeetingoftheAssociationforCompu-
guistics: EMNLP 2020, pages 3592–3603, Online.
tationalLinguisticsandthe11thInternationalJoint
AssociationforComputationalLinguistics.
Conference on Natural Language Processing (Vol-
TanyaGoyal, JunyiJessyLi, andGregDurrett.2022. ume1: LongPapers),pages4504–4517,Online.As-
SNaC:Coherenceerrordetectionfornarrativesum- sociationforComputationalLinguistics.
marization. InProceedingsofthe2022Conference
onEmpiricalMethodsinNaturalLanguageProcess- Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.
ing,pages444–463,AbuDhabi,UnitedArabEmi- Hurdlestoprogressinlong-formquestionanswering.
rates.AssociationforComputationalLinguistics. InProceedingsofthe2021ConferenceoftheNorth
AmericanChapteroftheAssociationforComputa-
Zhe Hu, Hou Pong Chan, Jiachen Liu, Xinyan Xiao, tionalLinguistics: HumanLanguageTechnologies,
HuaWu,andLifuHuang.2022. PLANET:dynamic pages4940–4957,Online.AssociationforComputa-
contentplanninginautoregressivetransformersfor tionalLinguistics.
long-form text generation. In Proceedings of the
60thAnnualMeetingoftheAssociationforCompu-
PhilippeLaban,TobiasSchnabel,PaulN.Bennett,and
tationalLinguistics(Volume1: LongPapers),ACL
MartiA.Hearst.2022. SummaC:Re-visitingNLI-
2022,Dublin,Ireland,May22-27,2022,pages2288–
basedmodelsforinconsistencydetectioninsumma-
2305.AssociationforComputationalLinguistics.
rization. TransactionsoftheAssociationforCompu-
tationalLinguistics,10:163–177.
XinyuHuaandLuWang.2020. PAIR:planningand
iterative refinement in pre-trained transformers for
longtextgeneration. InProceedingsofthe2020Con- Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol
ferenceonEmpiricalMethodsinNaturalLanguage Hwang,JaehyeonKim,Hong-InLee,andMoontae
Processing,EMNLP2020,Online,November16-20, Lee. 2023. QASA: advanced question answering
2020,pages781–793.AssociationforComputational on scientific articles. In International Conference
Linguistics. onMachineLearning,ICML2023,23-29July2023,
Honolulu,Hawaii,USA,volume202ofProceedings
YanpingHuang,YoulongCheng,AnkurBapna,Orhan ofMachineLearningResearch,pages19036–19052.
Firat,MiaXuChen,DehaoChen,HyoukJoongLee, PMLR.Chin-Yew Lin. 2004. ROUGE: A package for auto- OpenAI.2023c. GPT-4Turbowith128KContext.
maticevaluationofsummaries. InTextSummariza-
tionBranchesOut,pages74–81,Barcelona,Spain. OfirPress,NoahA.Smith,andMikeLewis.2022. Train
AssociationforComputationalLinguistics. short,testlong: Attentionwithlinearbiasesenables
inputlengthextrapolation. InTheTenthInternational
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, ConferenceonLearningRepresentations,ICLR2022,
Ruochen Xu, and Chenguang Zhu. 2023. G-eval: VirtualEvent,April25-29,2022.OpenReview.net.
NLGevaluationusinggpt-4withbetterhumanalign-
ment. In Proceedings of the 2023 Conference on Alec Radford, Jeff Wu, Rewon Child, David Luan,
Empirical Methods in Natural Language Process- DarioAmodei,andIlyaSutskever.2019. Language
ing,EMNLP2023,Singapore,December6-10,2023, modelsareunsupervisedmultitasklearners.
pages 2511–2522. Association for Computational
JustusRandolph.2010. Free-marginalmultiraterkappa
Linguistics.
(multirater kfree): An alternative to fleiss fixed-
marginalmultiraterkappa. volume4.
AlexMallen,AkariAsai,VictorZhong,RajarshiDas,
Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
NirRatner,YoavLevine,YonatanBelinkov,OriRam,
When not to trust language models: Investigating
InbalMagar,OmriAbend,EhudD.Karpas,Amnon
effectivenessofparametricandnon-parametricmem-
Shashua, KevinLeyton-Brown, andYoavShoham.
ories. InProceedingsofthe61stAnnualMeetingof
2022. Parallelcontextwindowsforlargelanguage
theAssociationforComputationalLinguistics(Vol-
models. In Annual Meeting of the Association for
ume 1: Long Papers), pages 9802–9822, Toronto,
ComputationalLinguistics.
Canada.AssociationforComputationalLinguistics.
OscarSainz,JonAnderCampos,IkerGarcía-Ferrero,
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Julen Etxaniz, Oier Lopez de Lacalle, and Eneko
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Agirre. 2023. NLP evaluation in trouble: On the
Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
needtomeasureLLMdatacontaminationforeach
Factscore: Fine-grainedatomicevaluationoffactual
benchmark. InFindingsoftheAssociationforCom-
precisioninlongformtextgeneration. InProceed-
putationalLinguistics:EMNLP2023,Singapore,De-
ingsofthe2023ConferenceonEmpiricalMethods
cember6-10,2023,pages10776–10787.Association
inNaturalLanguageProcessing,EMNLP2023,Sin-
forComputationalLinguistics.
gapore,December6-10,2023,pages12076–12100.
AssociationforComputationalLinguistics. Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant,
and Omer Levy. 2023. Zeroscrolls: A zero-shot
AmirkeivanMohtashamiandMartinJaggi.2023. Land- benchmark for long text understanding. CoRR,
mark attention: Random-access infinite context abs/2305.14196.
lengthfortransformers. CoRR,abs/2305.16300.
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori
ReiichiroNakano,JacobHilton,S.ArunBalaji,JeffWu, Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,
Ouyang Long, Christina Kim, Christopher Hesse, MorGeva,JonathanBerant,andOmerLevy.2022.
ShantanuJain,VineetKosaraju,WilliamSaunders, SCROLLS:standardizedcomparisonoverlonglan-
Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen guagesequences. InProceedingsofthe2022Con-
Krueger,KevinButton,MatthewKnight,Benjamin ferenceonEmpiricalMethodsinNaturalLanguage
Chess,andJohnSchulman.2021. Webgpt: Browser- Processing,EMNLP2022,AbuDhabi,UnitedArab
assisted question-answering with human feedback. Emirates,December7-11,2022,pages12007–12021.
ArXiv,abs/2112.09332. AssociationforComputationalLinguistics.
TriNguyen,MirRosenberg,XiaSong,JianfengGao, ChenhuiShen,LiyingCheng,Xuan-PhiNguyen,Yang
Saurabh Tiwary, Rangan Majumder, and Li Deng. You,andLidongBing.2023. Largelanguagemodels
2016. MS MARCO: A human generated machine are not yet human-level evaluators for abstractive
reading comprehension dataset. In Proceedings of summarization. In Findings of the Association for
theWorkshoponCognitiveComputation: Integrat- ComputationalLinguistics:EMNLP2023,Singapore,
ingneuralandsymbolicapproaches2016co-located December6-10,2023,pages4215–4233.Association
with the 30th Annual Conference on Neural Infor- forComputationalLinguistics.
mationProcessingSystems(NIPS2016),Barcelona,
Spain, December 9, 2016, volume 1773 of CEUR Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
WorkshopProceedings.CEUR-WS.org. PatrickLeGresley, JaredCasper, andBryanCatan-
zaro. 2019. Megatron-lm: Training multi-billion
OpenAI.2022a. ChatGPT. parameterlanguagemodelsusingmodelparallelism.
CoRR,abs/1909.08053.
OpenAI.2022b. Text-davinci-003.
IvanStelmakh,YiLuan,BhuwanDhingra,andMing-
OpenAI.2023a. GPT-3.5Turbo. Wei Chang. 2022. ASQA: factoid questions meet
long-formanswers. InProceedingsofthe2022Con-
OpenAI. 2023b. GPT-4 Technical Report. CoRR, ferenceonEmpiricalMethodsinNaturalLanguage
abs/2303.08774. Processing,EMNLP2022,AbuDhabi,UnitedArabEmirates,December7-11,2022,pages8273–8288. PengXu,MostofaPatwary,MohammadShoeybi,Raul
AssociationforComputationalLinguistics. Puri,PascaleFung,AnimaAnandkumar,andBryan
Catanzaro. 2020. MEGATRON-CNTRL: control-
JianlinSu,YuLu,ShengfengPan,BoWen,andYunfeng lablestorygenerationwithexternalknowledgeusing
Liu. 2021. Roformer: Enhanced transformer with large-scalelanguagemodels. InProceedingsofthe
rotarypositionembedding. ArXiv,abs/2104.09864. 2020ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,EMNLP2020,Online,Novem-
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann ber16-20,2020,pages2831–2845.Associationfor
Dubois,XuechenLi,CarlosGuestrin,PercyLiang, ComputationalLinguistics.
andTatsunoriB.Hashimoto.2023. Stanfordalpaca:
An instruction-following llama model. https:// Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
github.com/tatsu-lab/stanford_alpaca. Shafran, Karthik R. Narasimhan, and Yuan Cao.
2023a. React: Synergizing reasoning and acting
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
inlanguagemodels. InTheEleventhInternational
Martinet,Marie-AnneLachaux,TimothéeLacroix,
ConferenceonLearningRepresentations,ICLR2023,
BaptisteRozière,NamanGoyal,EricHambro,Faisal
Kigali,Rwanda,May1-5,2023.OpenReview.net.
Azhar,AurélienRodriguez,ArmandJoulin,Edouard
Grave,andGuillaumeLample.2023a. Llama: Open Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
and efficient foundation language models. CoRR, Shafran, Karthik R Narasimhan, and Yuan Cao.
abs/2302.13971. 2023b. React: Synergizing reasoning and acting
inlanguagemodels. InTheEleventhInternational
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
ConferenceonLearningRepresentations.
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
WeizheYuan,GrahamNeubig,andPengfeiLiu.2021.
Bhosale,DanBikel,LukasBlecher,CristianCanton-
Bartscore: Evaluatinggeneratedtextastextgenera-
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu, tion. InAdvancesinNeuralInformationProcessing
JudeFernandes,JeremyFu,WenyinFu,BrianFuller, Systems34: AnnualConferenceonNeuralInforma-
CynthiaGao,VedanujGoswami,NamanGoyal,An- tion Processing Systems 2021, NeurIPS 2021, De-
thonyHartshorn,SagharHosseini,RuiHou,Hakan cember6-14,2021,virtual,pages27263–27277.
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
IsabelKloumann,ArtemKorenev,PunitSinghKoura, Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di- Weinberger,andYoavArtzi.2020. Bertscore: Evalu-
anaLiskovich,YinghaiLu,YuningMao,XavierMar- atingtextgenerationwithBERT. In8thInternational
tinet,TodorMihaylov,PushkarMishra,IgorMoly- ConferenceonLearningRepresentations,ICLR2020,
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- AddisAbaba,Ethiopia,April26-30,2020.OpenRe-
stein,RashiRungta,KalyanSaladi,AlanSchelten, view.net.
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- XinrongZhang,YingfaChen,ShengdingHu,QihaoWu,
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Junhao Chen, Zihang Xu, Zhenning Dai, Xu Han,
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan, ShuoWang,ZhiyuanLiu,andMaosongSun.2023.
Melanie Kambadur, Sharan Narang, Aurélien Ro- Infinitebench: 128klong-contextbenchmarkforlan-
driguez,RobertStojnic,SergeyEdunov,andThomas guagemodels.
Scialom. 2023b. Llama 2: Open foundation and
fine-tunedchatmodels. CoRR,abs/2307.09288. LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Szymon Tworkowski, Konrad Staniszewski, Mikolaj Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
Pacek,YuhuaiWu,HenrykMichalewski,andPiotr JosephE.Gonzalez,andIonStoica.2023. Judging
Milos.2023. Focusedtransformer: Contrastivetrain- LLM-as-a-judgewithMT-benchandchatbotarena.
ingforcontextscaling. CoRR,abs/2307.03170. InThirty-seventhConferenceonNeuralInformation
ProcessingSystemsDatasetsandBenchmarksTrack.
Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol
Choi.2023. Acriticalevaluationofevaluationsfor
A Appendix
long-formquestionanswering. InProceedingsofthe
61stAnnualMeetingoftheAssociationforCompu-
A.1 AdvancementsinLong-ContextLLMs
tationalLinguistics(Volume1: LongPapers),ACL
2023,Toronto,Canada,July9-14,2023,pages3225– As LLMs gain global traction, their ability to
3245.AssociationforComputationalLinguistics.
adeptlyprocessextensivesequences—suchaspro-
JingXu, ArthurSzlam, andJasonWeston.2022. Be- tractedconversationhistoriesorintricatescientific
yondgoldfishmemory: Long-termopen-domaincon- documents—becomesincreasinglysignificant. Re-
versation. InProceedingsofthe60thAnnualMeet- centyearshavewitnessedtheemergenceofLLMs
ingoftheAssociationforComputationalLinguistics
capable of managing long context windows, a
(Volume1: LongPapers),ACL2022,Dublin,Ireland,
feat made possible by the advent of more pow-
May22-27,2022,pages5180–5197.Associationfor
ComputationalLinguistics. erful GPUs with expanded memory capacities,alongwithinnovationsinmemory-efficientatten- ongoingevolutionofstrategiestoenhancetheca-
tion mechanisms (Dao et al., 2022; Dao, 2023). pabilitiesofLLMsinaccommodatinglongcontext
Historically, the context window size for models sequences,acriticalrequirementfortheireffective
like GPT-2 encompassed 1024 tokens (Radford deploymentincomplex,real-worldapplications.
etal.,2019),whichwasthenextendedto2048in
A.2 EvaluationforLong-ContextLLMs
GPT-3 (Brown et al., 2020). Modern iterations,
suchasGPT-4-turbo,boastanimpressive128Kto- Theadventoflong-contextLLMshasusheredinan
kencapacity(OpenAI,2023c),whileClaude2.1ex- erawhereevaluatingperformanceoverextensive
tendsthisevenfurtherto200Ktokens(Anthropic, text sequences is crucial. Benchmarks like Zero-
2023). Nevertheless, scaling the context window SCROLLS(Shahametal.,2023)haveemergedto
during the pretraining phase remains a daunting challenge these models’ understanding of expan-
taskasthecomputationaldemandssurgequadrat- sivetextsinazero-shotsetting. ZeroSCROLLSex-
ically with the length of the attention span, and tendsthefoundationlaidbytheSCROLLSbench-
a majority of texts within standard corpora, like mark(Shahametal.,2022)—originallydesigned
CommonCrawl,tendtobecomparativelybrief. tohandlelongertextsthroughfine-tuning—byin-
A novel approach gaining momentum among troducingfournewtasks: query-basedsummariza-
researchersistheaugmentationoftheLLMs’con- tion,multi-hopquestionanswering,sentimentag-
textwindowthroughtheprocessofcontinuedtrain- gregation, and ordering book chapter summaries.
ingorfine-tuning. Forinstance,Tworkowskietal. Itdistinguishesitselfbyfocusingonzero-shotper-
(2023)successfullyrefinedthe3Band7BOpenL- formance,usingsimplenaturallanguageprompts
LaMAcheckpoints,employingcontrastivetraining andeschewingtrainingdata,relyingonnon-public,
techniques to adeptly handle contexts stretching high-qualityreferences.
upto8Ktokens. Similarly,MohtashamiandJaggi Another contribution to this domain is Long-
(2023)achievedanexpansionofthecontextlength Bench (Bai et al., 2023), a suite of 21 datasets
from 4K to 32K for LLaMA 7B by incorporat- across 6 categories of tasks such as single- and
ing“landmarktokens”thateffectivelyencapsulate multi-document question answering, summariza-
blocksoftheexistingcontext. Thesetokensallow tion,few-shotlearning,specificsynthetictasks,and
for focused fine-tuning of attention mechanisms, codecompletion. WhatsetsLongBenchapartisits
whichinturnfacilitatestheselectionofpertinent uniform format for all datasets, promoting a uni-
contextualblocks. fiedandautomatedevaluationprocesswithmetrics
Furthermore, Chen et al. (2023) introduced a likeF1andROUGE.Bamboo(Dongetal.,2023)
method known as positional interpolation, to be alsoprovidesavaluableframeworkforanalyzing
usedwithLLMsthatincorporateRotaryPosition comprehension over lengthy texts, offering a se-
Embeddings(RoPE)astheirchoiceofpositional lectionof10datasetsfrom5diverseactivitiesthat
encoding(Suetal.,2021). Thistechniqueyielded rangefromquestionansweringtohallucinationde-
promisingoutcomeswhenappliedtoLLaMAmod- tection,textsorting,languagemodeling,andcode
elsrangingfrom7Bto65Binsize,requiringmini- completion. Bamboospecificallytacklespotential
malfine-tuningefforts—amere1000optimization data contamination by exclusively using sources
steps. A different paradigm, ALiBi (Press et al., releasednoearlierthan2023,maintainingtherele-
2022),circumventsthenecessityoffine-tuningal- vanceandcontemporaneityofitsmaterial.
together for expanding the context window. By L-Eval(Anetal.,2023)introducesabifurcated
eschewingpositionalembeddingsandinsteadap- approachtoevaluateLLMs,featuringbothclosed-
plyingalinearbiastotheattentionscores—which endedandopen-endedtasks. Closed-endedtasks
isproportionatetothedistancebetweentokens—it focus on the model’s reasoning and comprehen-
elegantlyadjuststohandlelongercontexts. sion skills in a protracted context. In contrast,
Lastly, the strategy proposed by Ratner et al. itsopen-endedtasksprovideavarietyofsumma-
(2022) partitions extensive contexts into several rizationchallengesthatrequiremodelstosynthe-
sub-windows,employingthesamepositionalem- size information from lengthier documents. In-
beddings across them. This innovative reuse of finiteBench(Zhangetal.,2023)istailoredtoassess
embeddings enables the models to cope with ex- LLMsthatprocess,understand,andinferinforma-
tendedcontextswithouttheneedforadditionalfine- tionfromcontextsthatspanover100,000tokens.
tuning. Thiscollectivebodyofworkrepresentsthe Itshouldbenotedthatthesedatasetsprioritizetheassessmentoflong-contextunderstanding,andas
aresult,asignificantportionoftokensareusedas
inputsfortheLLMsratherthanoutputs.
A.3 CaseStudyandErrorAnalysis
Table4-9showthecasestudyofthereportsgener-
atedbyLLaMA2-13B-Chat,NewBingandGPT-
4-turbo. AlthoughLLaMA2-13B-Chatgenerates
lengthycontents,verylittleimportantinformation
arecontained. ThereforeGPT-4evaluatorisunable
tofetchusefulinformationfromthegeneratedcon-
tents,resultinlowaccuracyontheproxy-questions.
Incontrast,bothNewBingandGPT-4-turbogen-
eratemoreconciseresultsconveyedwithessential
contents. Specifically,criticalhallucinationcanbe
observedintheoutputoftheLLaMA2-13B-Chat,
whilenoneofthecontentscanbeusedtoanswer
the proxy-questions. However, New Bing covers
asmuchpointsasitcan, providingacomprehen-
sive overview of the given topic, while results of
GPT-4-turboiswell-structuredandoffersdetailed
andhelpfulintroductiontosomeofthecriticalcon-
tents.Question: Data parallelism, model parallelism, and pipeline parallelism play a vital role in the
trainingoflarge-scalelanguagemodels. Whataretherepresentativeworksandframeworksamong
thesetechnologies? Pleaseintroducethesetechnologiesandframeworksindetail.
LLaMA2-13B-Chat-Page1
Introduction:
Thetrainingoflarge-scalelanguagemodelshasbecomeincreasinglypopularinrecentyearsdueto
theirwiderangeofapplicationsinnaturallanguageprocessing(NLP)tasks. However,trainingsuch
modelscanbecomputationallyexpensiveandtime-consuming,especiallywhendealingwithlarge
datasets. Toaddressthischallenge,researchershaveproposedvariousparallelizationtechniquesto
speedupthetrainingprocess. Inthisreport,wewillfocusonthreekeyparallelizationtechniques:
data parallelism, model parallelism, and pipeline parallelism. We will introduce representative
worksandframeworksthatutilizethesetechniquesanddiscusstheiradvantagesandlimitations.
DataParallelism:
Data parallelism is a technique that involves dividing the input data into smaller chunks and
processing them in parallel across multiple devices or machines. This technique is particularly
useful for training large-scale language models, as it allows for faster training times and more
efficientuseofcomputationalresources.
OneofthemostpopularframeworksfordataparallelismisTensorFlow,whichisanopen-source
machinelearninglibrarydevelopedbyGoogle. TensorFlowprovidesabuilt-indataparallelism
feature called TensorFlow Estimator, which allows users to parallelize their training processes
across multiple machines. Another popular framework for data parallelism is PyTorch, which
providesamodulecalledPyTorchDistributedthatallowsuserstoparallelizetheirtrainingprocesses
acrossmultipledevices.
RepresentativeWorks:
One of the most influential works in data parallelism for large-scale language models is the
paper"DistributedTrainingofDeepNeuralNetworks"byGoogleresearchers(2016). Thispaper
introducedtheconceptofusingdataparallelismtotraindeepneuralnetworksonlargedatasets,
anddemonstratedtheeffectivenessofthisapproachfortraininglarge-scalelanguagemodels.
Anotherinfluentialworkisthepaper"DeepLearningwithCUDA:GeneralPurposeTrainingof
Neural Networks on GPU Clusters" by NVIDIA researchers (2017). This paper introduced the
concept of using GPU clusters for training large-scale language models, and demonstrated the
effectivenessofthisapproachforimprovingtrainingtimesandreducingcomputationalcosts.
ModelParallelism:
Modelparallelismisatechniquethatinvolvesdividingthemodelintosmallerpartsandprocessing
them in parallel across multiple devices or machines. This technique is particularly useful for
traininglarge-scalelanguagemodels,asitallowsforfastertrainingtimesandmoreefficientuseof
computationalresources.
Oneofthemostpopularframeworksformodelparallelismistheopen-sourcemachinelearning
libraryTensorFlow,whichprovidesabuilt-infeaturecalledTensorFlowModelParallelism. This
featureallowsuserstoparallelizetheirtrainingprocessesacrossmultiplemachines,andhasbeen
usedinanumberofinfluentialworksfortraininglarge-scalelanguagemodels.
Another popular framework for model parallelism is the PyTorch framework, which provides a
modulecalledPyTorchModelParallelthatallowsuserstoparallelizetheirtrainingprocessesacross
multipledevices.
Table4: CasestudyofLLaMA2-13B-ChatLLaMA2-13B-Chat-Page2
RepresentativeWorks:
Oneofthemostinfluentialworksinmodelparallelismforlarge-scalelanguagemodelsisthepaper
"Model Parallelism for Training Large-Scale Neural Networks" by Google researchers (2017).
Thispaperintroducedtheconceptofusingmodelparallelismtotrainlarge-scalelanguagemodels,
and demonstrated the effectiveness of this approach for improving training times and reducing
computationalcosts.
PipelineParallelism:
Anotherinfluentialworkisthepaper"ScalingUpNeuralMachineTranslationwithModelParal-
lelism"byGoogleresearchers(2018). Thispaperintroducedtheconceptofusingmodelparallelism
totrainlarge-scaleneuralmachinetranslationmodels,anddemonstratedtheeffectivenessofthis
approachforimprovingtranslationqualityandreducingcomputationalcosts.
Pipelineparallelismisatechniquethatinvolvesdividingthetrainingprocessintosmallerstages
andprocessingtheminparallelacrossmultipledevicesormachines. Thistechniqueisparticularly
useful for training large-scale language models, as it allows for faster training times and more
efficientuseofcomputationalresources.
Oneofthemostpopularframeworksforpipelineparallelismistheopen-sourcemachinelearning
libraryTensorFlow,whichprovidesabuilt-infeaturecalledTensorFlowPipelineParallelism. This
featureallowsuserstoparallelizetheirtrainingprocessesacrossmultiplemachines,andhasbeen
usedinanumberofinfluentialworksfortraininglarge-scalelanguagemodels.
Another popular framework for pipeline parallelism is the PyTorch framework, which provides
amodulecalledPyTorchPipelinethatallowsuserstoparallelizetheirtrainingprocessesacross
multipledevices.
RepresentativeWorks:
One of the most influential works in pipeline parallelism for large-scale language models is the
paper "Pipeline Parallelism for Training Large-Scale Neural Networks" by Google researchers
(2018). Thispaperintroducedtheconceptofusingpipelineparallelismtotrainlarge-scalelanguage
models, and demonstrated the effectiveness of this approach for improving training times and
reducingcomputationalcosts.
Another influential work is the paper "Scaling Up Neural Machine Translation with Pipeline
Parallelism"byGoogleresearchers(2019). Thispaperintroducedtheconceptofusingpipelinepar-
allelismtotrainlarge-scaleneuralmachinetranslationmodels,anddemonstratedtheeffectiveness
ofthisapproachforimprovingtranslationqualityandreducingcomputationalcosts.
Conclusion:
Inconclusion,dataparallelism,modelparallelism,andpipelineparallelismarethreekeyparalleliza-
tiontechniquesthathaveplayedavitalroleinthetrainingoflarge-scalelanguagemodels. These
techniqueshavebeenusedinanumberofinfluentialworks,andhavedemonstratedtheeffectiveness
ofparallelizingthetrainingprocesstoimprovetrainingtimesandreducecomputationalcosts. As
thefieldofNLPcontinuestoevolve,itislikelythattheseparallelizationtechniqueswillcontinue
toplayanimportantroleinthetrainingoflarge-scalelanguagemodels.
Table5: CasestudyofLLaMA2-13B-ChatNewBing-Page1
Hello,thisisBing. Icanhelpyouwithyourquestion.
Dataparallelism,modelparallelism,andpipelineparallelismarethreetypesofparallelismmethods
thatcanbeusedtotrainlarge-scalelanguagemodelsefficientlyonmultipleGPUsorotherdevices.
Hereisabriefintroductionofthesetechnologiesandsomerepresentativeworksandframeworks
thatusethem:
• Dataparallelism(DP):Thisisthesimplestandmostcommonformofparallelism,where
thesamemodelisreplicatedonmultipledevices,andeachdeviceprocessesadifferentslice
of the input data. The gradients are then aggregated and synchronized across the devices
toupdatethemodelparameters. Thismethodiseasytoimplementandcanscalewellwith
thenumberofdevices,butithassomelimitations,suchasrequiringlargememorytostore
thefullmodelandcommunicationoverheadforgradientsynchronization. Someexamples
of frameworks that support data parallelism are PyTorch’s DistributedDataParallel and
TensorFlow’sMirroredStrategy.
• Modelparallelism(MP):Thisisamorecomplexformofparallelism,wherethemodelissplit
intosmallerpartsanddistributedacrossmultipledevices,suchthateachdeviceonlystoresand
computesasubsetofthemodelparameters. Thismethodcanovercomethememorylimitation
of data parallelism and enable training very large models that cannot fit on a single device.
However, it also introduces more challenges, such as how to partition the model, how to
minimizethecommunicationcost,andhowtobalancetheworkloadamongthedevices. There
aredifferenttypesofmodelparallelism,suchastensorparallelismandpipelineparallelism,
whichwillbeexplainedbelow. Someexamplesofframeworksthatsupportmodelparallelism
areNVIDIA’sMegatron-LMandMicrosoft’sDeepSpeed.
• Tensorparallelism(TP):Thisisatypeofmodelparallelismwhereeachtensor(suchasa
weightmatrixoranactivationvector)issplitintosmallerchunksalongoneormoredimensions,
and each chunk is assigned to a different device. For example, a weight matrix of size [m,
n] can be split into [m/k, n/k] chunks, where k is the number of devices, and each device
only stores and computes one chunk. This method can reduce the memory footprint and
communicationcostbyafactorofk,butitalsorequiresmodifyingthemodelarchitectureand
thecomputationlogictohandlethetensorsharding. Someexamplesofworksthatusetensor
parallelismaregShardandMesh-TensorFlow.
• Pipelineparallelism(PP):Thisisanothertypeofmodelparallelismwherethemodelissplit
intosmallerstages(suchaslayersorgroupsoflayers)anddistributedacrossmultipledevices,
suchthateachdeviceonlystoresandcomputesonestageofthemodel. Forexample,amodel
withLlayerscanbesplitintoSstages,whereeachstagecontainsL/Slayers,andeachdevice
only processes one stage. The input data is then fed into the first device, and the output of
eachdeviceispassedtothenextdeviceinapipelinedfashion. Thismethodcanalsoreduce
the memory footprint and communication cost by a factor of S, but it also requires careful
schedulingandsynchronizationofthepipelinestagestomaximizethethroughputandavoid
idleperiods. SomeexamplesofworksthatusepipelineparallelismarePipeDream,GPipe,
andPipeEdge.
Table6: CasestudyofNewBingNewBing-Page2
These parallelism methods can be combined in different ways to achieve higher efficiency and
scalability. Forexample,onecanusedataparallelismandtensorparallelismtogethertotrainalarge
model on a large number ofdevices, as done in Megatron-LM. Alternatively, one can use data
parallelismandpipelineparallelismtogethertotrainadeepmodelwithhighthroughput,asdonein
GPipe. Furthermore, one can use tensor parallelism and pipeline parallelism together to train a
verylargeanddeepmodelwithlowmemoryandcommunicationcost,asdoneinDeepSpeed.
Ihopethishelpsyouunderstandthebasicsofparallelismmethodsforlarge-scalelanguagemodel
training. If you want to learn more, you can check out the links I provided or search for more
resourcesonBing. Haveaniceday!
(1) ModelParallelism-HuggingFace. https://huggingface.co/docs/transformers/v4.15.0/parallelism.
(2) undefined. https://github.com/nvidia/megatron-lm.
(3) Efficient Large-Scale Language Model Training on GPU Clusters - arXiv.org.
https://arxiv.org/pdf/2104.04473v2.
(4) PipeEdge: Pipeline Parallelism for Large-Scale Model Inference on ....
https://ieeexplore.ieee.org/document/9996638.
(5) Efficient Large-Scale Language Model Training on GPU Clusters - arXiv.org.
https://arxiv.org/pdf/2104.04473v2.
(6) Optimizing Distributed Training on Frontier for Large Language Models.
https://arxiv.org/html/2312.12705v2.
(7) AbstractarXiv:2102.07988v2[cs.LG]28Sep2021. https://arxiv.org/pdf/2102.07988.pdf.
Table7: CasestudyofNewBingGPT-4-turbo-Page1
A.4 Introduction
Thetrainingoflarge-scalelanguagemodels,suchasGPT-3,BERT,andT5,requiressignificant
computationalresourcesduetothevastnumberofparametersandthesizeofthedatasetsinvolved.
To address this challenge, parallelism techniques have been developed to distribute the compu-
tational workload across multiple processors or machines. The three main types of parallelism
usedintraininglarge-scalelanguagemodelsaredataparallelism,modelparallelism,andpipeline
parallelism. Thisreportprovidesanin-depthlookatthesetechnologiesandtheirrepresentative
worksandframeworks.
A.5 DataParallelism
A.5.1 DefinitionandOverview
Dataparallelisminvolvessplittingthetrainingdataacrossmultipleprocessorsordevices,witheach
processorholdingacompletecopyofthemodel. Eachprocessorcomputesthegradientsbasedon
itssubsetofthedata,andthegradientsarethenaggregatedtoupdatethemodelparameters.
A.5.2 RepresentativeWorksandFrameworks
• Horovod: Developed by Uber, Horovod is a distributed training framework that uses data
parallelism. It is designed to work with TensorFlow, Keras, PyTorch, and Apache MXNet.
Horovod simplifies the process of scaling out deep learning models by providing efficient
gradientaggregation.
• PyTorchDistributedDataParallel(DDP):PyTorch’sDDPmoduleallowsforeasydatapar-
allelismimplementation. Itwrapsthemodelduringtrainingandmanagesgradientaveraging
andsynchronizationacrossmultipleGPUs.
• TensorFlow’stf.distribute.Strategy: TensorFlowoffersseveralstrategiesfordistributed
training,includingMirroredStrategyfordataparallelism. Itsynchronizestrainingacross
multipleGPUsbyreplicatingthemodeloneachdeviceandmanaginggradientupdates.
A.6 ModelParallelism
A.6.1 DefinitionandOverview
Modelparallelisminvolvessplittingthemodelitselfacrossmultipleprocessorsordevices. Each
processor is responsible for a portion of the model’s layers or parameters. This technique is
particularlyusefulformodelsthataretoolargetofitintothememoryofasingledevice.
A.6.2 RepresentativeWorksandFrameworks
• GPipe: Developed by Google, GPipe is a library for efficiently training large-scale neural
networksusingmodelparallelism. Itpartitionsthemodelintodifferentsegmentsandpipelines
theexecutionacrossmultipleaccelerators.
• Megatron-LM: Created by NVIDIA, Megatron-LM is a framework designed to train
transformer-basedmodelsusingmodelparallelism. Itsplitsthemodel’slayersacrossmultiple
GPUs,allowingforthetrainingofmodelswithbillionsofparameters.
• Mesh-TensorFlow: Thisisalanguagefordistributeddeeplearning,developedbyGoogle,
thatallowsforeasyimplementationofmodelparallelism. ItprovidesasimpleAPItodistribute
themodel’scomputationacrossameshofprocessors.
Table8: CasestudyofGPT-4-turboGPT-4-turbo-Page2
A.7 PipelineParallelism
A.7.1 DefinitionandOverview
Pipeline parallelism combines elements of both data and model parallelism. The model is split
acrossmultipleprocessors,andthetrainingdataisalsosplitintomicro-batches. Eachprocessor
worksonadifferentmicro-batchandpassesitsoutputtothenextprocessorinthepipeline.
A.7.2 RepresentativeWorksandFrameworks
• GPipe: While GPipe is primarily associated with model parallelism, it also incorporates
aspectsofpipelineparallelismbypipeliningtheexecutionofmicro-batchesacrossdifferent
modelpartitions.
• DeepSpeed: DevelopedbyMicrosoft,DeepSpeedisadeeplearningoptimizationlibrarythat
providesapipelineparallelismimplementationcalledZeRO-DP.Itisdesignedtooptimize
memoryusageandimprovescalabilitywhentraininglargemodels.
• FairScale: An extension of PyTorch, FairScale includes several features for distributed
training,includingpipelineparallelism. Itallowsfortheefficienttrainingoflargemodelsby
partitioningtheworkloadacrossmultipleGPUs.
A.8 Conclusion
Data parallelism, model parallelism, and pipeline parallelism are essential techniques for train-
inglarge-scalelanguagemodels. Eachtechniqueaddressesdifferentchallengesassociatedwith
the size of the model and the dataset. Frameworks like Horovod, PyTorch DDP, TensorFlow’s
tf.distribute.Strategy,GPipe,Megatron-LM,Mesh-TensorFlow,DeepSpeed,andFairScale
providethenecessarytoolsforimplementingtheseparallelismstrategieseffectively.
Asthedemandforlargerandmorecomplexmodelscontinuestogrow,theseparallelismtechniques
andframeworkswillplayacrucialroleinenablingthedevelopmentofnext-generationAIsystems.
Researchers and practitioners must carefully choose the appropriate parallelism strategy based
on the specific requirements of their models and the available computational resources. With
ongoing advancements in distributed computing and parallel processing, we can expect to see
furtherimprovementsintheefficiencyandscalabilityoflarge-scalelanguagemodeltraining.
Table9: CasestudyofGPT-4-turbo