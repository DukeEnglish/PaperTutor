Appropriateness of LLM-equipped Robotic Well-being Coach Language
in the Workplace: A Qualitative Evaluation
Micol Spitale1,2, Minja Axelsson1 and Hatice Gunes1
Abstract—Robotic coaches have been recently investigated in generating high-level motion planning [13]–[15]. For
to promote mental well-being in various contexts such as example, Huang et al. [14] have used LLMs to explore the
workplaces and homes. With the widespread use of Large
feasibility of decomposing natural language expressions for
Language Models (LLMs), HRI researchers are called to
high-leveltasks(suchas“makebreakfast”)intoadesignated
consider language appropriateness when using such generated
language for robotic mental well-being coaches in the real set of executable steps (for instance, “open fridge”); while
world. Therefore, this paper presents the first work that Song et al. [13] proposed an effective way to use LLMs as
investigatedthelanguageappropriatenessofrobotmentalwell- plannerstoexecutecomplextasksbasedonnaturallanguage
being coach in the workplace. To this end, we conducted an
instructions within a visually perceived environment. Our
empirical study that involved 17 employees who interacted
work aims to extend such analysis of appropriateness to the over 4 weeks with a robotic mental well-being coach equipped
with LLM-based capabilities. After the study, we individually language robots use during HRI—specifically to the design
interviewed them and we conducted a focus group of 1.5 of language-appropriate robotic mental well-being coaches
hours with 11 of them. The focus group consisted of: i) an in real-world settings.
ice-breaking activity, ii) evaluation of robotic coach language
This paper presents the first study that examines the
appropriatenessinvariousscenarios,andiii)listingshouldsand
appropriateness of robotic mental well-being coach lan-
shouldn’ts for designing appropriate robotic coach language
for mental well-being. From our qualitative evaluation, we guage in the workplace. To this end, we conducted an
foundthatalanguage-appropriateroboticcoachshould(1)ask empirical study with 17 employees who interacted with
deep questions which explore feelings of the coachees, rather an LLM-equipped robotic mental well-being coach over 4
than superficial questions, (2) express and show emotional
weeks in their workplace [16]. After the study, we first
and empathic understanding of the context, and (3) not make
interviewedthemindividually,andthenweorganisedafocus
anyassumptionswithoutclarifyingwithfollow-upquestionsto
avoidbiasandstereotyping.Theseresultscaninformthedesign group that involved 11 of them. This focus group included:
oflanguage-appropriateroboticcoachtopromotementalwell- 1) an ice-breaking activity, where participants were asked to
being in real-world contexts. describe the robotic coach with five adjectives; 2) language
appropriateness evaluation, where participants were asked to
I. INTRODUCTION
assesstheappropriatenessofroboticcoachlanguageinseven
Thelastyearhasbeencharacterisedbyaground-breaking different scenarios; and 3) listing shoulds and shouldn’ts,
advancement in Large Language Models (LLMs) that has where participants were asked to list what a robotic coach
revolutionised several research fields [1], [2], including should and should not say and how it should behave when
Human-Robot Interaction (HRI) research [3]. Robots have delivering well-being practices.
beenincreasinglyintroducedinsensitiveapplicationcontexts Ourresultsshowthatalanguage-appropriateroboticwell-
– such as mental health [4], [5], therapy [6], [7], and elderly being coach should (1) ask deep questions which explore
care [8]. In such sensitive contexts, designing appropriate feelings of the coachees, (2) express and show emotional
robot language is extremely important. As HRI researchers, andempathicunderstandingofthecontext,and(3)notmake
wearecalledtoconsidertheappropriatenessofusingLLM- anyassumptionswithoutclarifyingwithfollow-upquestions
generatedlanguageinrobotsdeployedintherealworld,and to avoid bias and behaviours that enforce stereotypes. Our
the social and ethical implications of their language when analysis and results contribute towards understanding how
interacting with people. to design language-appropriate robotic coaches to promote
WithintheHRIliterature,roboticwell-beingcoacheshave mental well-being in adults in their workplace.
been recently investigated with the aim of promoting mental
II. RELATEDWORK
well-being in homes [9], university accommodations [10],
and workplaces [4], [11]. However, the appropriateness of A. Appropriate LLM-equipped Agent Language
theseroboticcoaches’language,generatedusingLLMs,have Designing embodied agent (e.g., robot, virtual agent) lan-
notbeenanalyzed.Inthepast,suchappropriatenessanalyses guagethatisappropriateforaspecificcontextcanbeachal-
have been undertaken in other HRI applications such as lengingtask [17],especiallyforsensitive applicationscenar-
robotic movement planning [12]. Past studies have shown iossuchasmedicine[18],therapy[19],healthcare[20],[21],
how to design an appropriate prompt to guide researchers just to mention a few. Given the widespread use of LLMs
in such applications, recent studies [17], [18] have started
1 DepartmentofComputerScienceandTechnology,UniversityofCam-
exploringthechallengesandopportunitiesthatemergedfrom
bridge,UK,2 DepartmentofElectronic,Information,andBio-engineering,
PolitecnicodiMilano,Italy.micol.spitale@polimi.it the adoption of LLMs for language generation in these
4202
naJ
62
]CH.sc[
1v53941.1042:viXraenvironments. For instance, Cabrera et al. [20] conducted speech (TTS) voices for mindfulness. Their results showed
aliteraturereviewtodiscussthebioethicaldilemmasrelated that the user-personalized TTS voices were able to perform
to the use of chatbots in the field of mental health, namely nearlyaswellashumanvoices,indicatingthatuserpersonal-
quality of care, access and exclusion, responsibility and isationcouldbeapowerfulapproachtoraiseuserperception
human supervision, and regulations and policies for LLM- of TTS voice quality. When Abbasi et al. [5] looked into a
equipped use. Their review recommend that LLM-equipped robot’s potential to evaluate mental well-being problems in
chatbots should be developed for mental health purposes, children, they found that it was more accurate at identifying
withtaskscomplementarytothetherapeuticcareprovidedby likely anomalies than tests that relied on self- and parent-
human professionals, and that their implementation should reports. Recent studies have looked into the application of
be properly regulated and should have a strong ethical robotic coaches in the workplace and public spaces. Spitale
framework. Analogously, Cho et al. [19] have investigated et al. [4], [11] conducted a study involving employees of a
the LLM efficacy – in terms of empathy, communication tech company to interact with two different forms of robotic
skills, adaptability, engagement, and the ability to establish coaches that delivered positive psychology exercises over
a therapeutic alliance – in interactive language therapy for 4 weeks. Their results showed that the robot form may
high-functioning autistic adolescents. Their results highlight impact the perception of the coachees towards the robotic
thechallengesofdevelopingsuchtherapeuticLLM-equipped coach. Axelsson et al. [24] examined the use of a robotic
systemsthatcannotachievethedepthofpersonalizationand mindfulness coach at a public cafe over 4 weeks, finding
emotional understanding characteristic of human therapists that participants thought the robot was useful as a guiding
and the importance of ethical considerations in therapeutic voice and a focal point, but wanted the robot to be more
contexts. responsive. However, these studies have not examined the
However, very few studies [22], [23] have investigated appropriateness of robotic coach language.
further how the LLM usage in agents has impacted agent
III. METHODOLOGY
language and human perceptions toward the agent. For
example, Onorati et al. [23] proposed a robotic application The ultimate goal of this work is to understand the
to generate verbal dialogues considering the user’s interests appropriateness of the language of a robotic coach for
andpreferencesusingLLMtocreatetheconversation.Their promoting mental well-being in the workplace. This is done
main goal was to make users perceive these dialogues as by undertaking an empirical study and then conducting
interesting, to avoid disengagement during the interaction individualinterviewsandafocusgroupwithemployeeswho
withtherobot.Theirresultsshowthatparticipantspositively haveinteractedwiththeroboticcoach.Thissectiondescribes
engage and use the robotic application. Brohan et al. [22] themethodologyofthestudyandthequalitativeapproachby
proposed a set of pre-trained skills for a robot, which were reporting the participant demographics, the empirical study,
used to constrain the model to propose natural language the interview and focus group protocols, and the scenarios
actions that were both feasible and contextually appropriate investigated.
to provide high-level semantic knowledge about the real-
A. Participants
world robotic task (e.g., use a vacuum cleaner).
This work presents the first evaluation of the appropriate- We conducted an empirical study and individual semi-
nessofthelanguageofanLLM-equippedroboticcoachthat structured interviews with 17 employees (7 females, and 10
delivered well-being coaching in a workplace context. males, 4 of whom were 18-25 years old, 6 were 26-35 years
old,4were36-45yearsold,and3were46-55yearsold).All
B. Robotic Coaches For Mental Well-being
participants were employees of the Cambridge Consultants
Various studies have examined the use of robotic coaches Inc. company, where they had taken part in the empirical
to promote mental well-being, e.g., [4], [10], [24]–[28]. Jibo study(seeSec.III-B).Wethenaskedthemwhethertheywere
robots were used in a longitudinal study by Jeong et al. willing to participate in a focus group without specifying to
[29] to deliver positive psychology interventions to students them its ultimate goal. Eleven participants out of seventeen
in home settings over the course of seven days. It was agreed to attend the focus group: 6 females, and 5 males,
found that participants gradually grew fond of the robot 1 of whom were 18-25 years old, 4 were 26-35 years old,
and experienced improved well-being, improved mood, and 3 were 36-45 years old, and 3 were 46-55 years old. All
readinesstochange.Bodalaetal.[26]evaluatedparticipants’ participantsprovidedinformedconsentfortheirparticipation
perceptions towards a human coach in comparison to a and agreed on the usage of their data for scientific research.
teleoperated robotic coach across a five-week period. It was The focus group design, the protocol, and the consent forms
concluded that both had favourable feedback, however, the wereapprovedbytheDepartmentalEthicsCommitteeofthe
humancoachwaspreferredintermsoflikability,intelligence UniversityofCambridge’sDepartmentofComputerScience
and level of animacy. Additionally, the personality traits of & Technology.
the participants such as neuroticism and conscientiousness
B. Study Protocol
were found to influence how they interacted with the robot.
Shietal.[28]investigatedtheeffectofphysicalembodiment Employeesinvolvedinthisqualitativeresearchtookpartin
and personalisation on the user-perceived quality of text-to- an empirical study with an autonomous and adaptive roboticcoach over four weeks at their workplace, as described in write for each scenario: how they felt about the scenario
detail in [16]. (“How do you feel?”), the appropriateness score of the
1) EmpiricalStudy: Theroboticcoachdeliveredfourdif- robotic coach language in that scenario (from 1-10), how
ferent positive psychology exercises (one per week), namely would they modify the robot language (“Behavior modifica-
savouring, gratitude, accomplishment and one door closes tions”), and why they would modify what the robotic coach
andonedooropens.TherobotusedaLLM-equippedframe- says (“Why”). Finally, the researcher asked the participants
work for managing the dialogue flow (i.e., we embedded in to define a list of “should”s and “shouldn’t”s (20 mins)
the robot OpenAI API for natural language processing). The of the robotic coach language, as depicted in Figure 2.
interaction between each employee and the robotic coach At the end of each of the three activities, the researchers
lastedforabout10minutesandincludedthefollowingsteps: allocated around 10 mins for group discussions with all the
(1) The robot introduced itself and described the positive participating employees.
psychologypractice(justinthefirstsession)andtheexercise
C. Scenarios
of the week (e.g., savouring exercise). (2) The robot asked
the coachee to think about a positive memory from the last In our empirical study, employees shared with the robotic
week and to share it with them. (3) The robot listened coach various positive episodes from their lives [16] (e.g.,
to the coachee’s response. (4) The robot made a decision playinginstruments,practicingsports)thatinformedthedef-
on the next step (summarise, ask for a follow-up question, inition of the workshop’s scenarios alongside their ground-
or start a new episode) based on an on-the-fly pre-trained ing in Human-Computer Interaction (HCI) literature. While
reinforcement learning model described in detail in [16]. watching the recordings of the study, we observed situations
(5) The robot generated the action according to the decision (i.e., what the coachee shared and how the robotic coach
made in (4) and listened to the coachees’ response. (6) The followedup)thatweusedasscenariosinourfocusgroup,but
robotrepeatedsteps(4)and(5)for8turns.Wedecidedtofix modified for privacy reasons. As a result, we defined seven
thenumberofturnsto8toensurethatthecoachingpractice scenarios that we encountered recurrently in our empirical
doesnotlastmorethan10minutes.(7)Therobotconcluded study by framing them from a Human-Computer Interaction
thesessionbythankingtheemployeeandremindingthemof (HCI) perspective [31]. HCI literature [31] suggests that the
the following week’s session. This procedure was repeated research field has had three waves: (1) in the first wave, the
for each session over the four weeks of the study. humanisconceptualisedasanobjectthatfunctionsfollowing
2) Interviews: At the end of the last week, we conducted rigid guidelines, and HCI focused on pragmatic solutions
individualsemi-structuredinterviewswithallemployees(17 and objective measures (e.g., time, task performance); (2)
intotal)inameetingroomoftheCambridgeConsultantsInc. the second wave added subjectivity and took into account
headquarters.Weaskedemployeesseveralquestions,suchas the person in the interaction (besides the functionality of
“Whatwasyouroverallimpressionoftherobot?”,“Haveyou the technology itself) and HCI focused on cognitive science
felt understood and listened to?”, “Would you recommend and psychology aspects (e.g., emotion, empathy) during the
this robot to a colleague or a friend?” etc. We concluded design of technologies for humans; (3) in the third wave,
the interview by debriefing the coachees, i.e., explaining researchers considered how the technology may reach our
the main goal of the empirical study, and answering their everyday lives (homes, privacy etc.), by taking into account
questions. social-cultural context (e.g., gender, culture). We applied
3) Focus Group: After two weeks, we conducted a 1.5- these waves as the three drivers for designing the seven
hour focus group with the employees online via MS Teams. scenarios as follows:
To facilitate the discussion, we asked participants to use the 1) Wave 1 (Objective Measures) - Efficiency and time
online tool Miro1. Two researchers were present during the pressure:
focus group: one of them was taking notes, and the other
• Scenario 1: You have just shared that you have
was leading the discussion. The focus group included three
been working out recently, and you felt grateful
activities, which were conducted individually on the Miro
for it. As follow up questions, the robot asks
Board. First (ice-breaking activity, 10 mins), the researcher
you the questions “How many times a week are
askedtheemployeestopostontheMiroboardfiveadjectives
you working out?”, “When was the last time you
to describe the robotic coach. This activity was chosen to
worked out?”.
better understand the employees’ perceptions of the robotic
• Scenario 2: You have just shared one example
coachaswaspreviouslydonein[30].Second,theresearcher
you were grateful for during the last week. After
explained to the employees that the main goal of the focus
sharing that, the robot asked you to think about
group was to better understand what robotic coaches should
anotherexampleyouhavebeengratefulforduring
say to be perceived as more appropriate to the context and
thelastweek,givingyou30secondstothinkabout
the situation. Then, they described the activity of scenario
it.
evaluations (40 mins). The employees were presented with
2) Wave 2 (Subjectivity) - Empathy and emotions:
seven scenarios on the Miro board, and they were asked to
• Scenario 3: You have just shared that you have
1https://miro.com/ become an aunt and you have a new nephew thatyou are very grateful for. The robot asked you shared in her first week that she has practiced dance. The
“What makes you grateful for the birth of your robotic coach followed up with very practical question, e.g.,
nephew?”. technical aspects of her dance practice, rather than asking
• Scenario 4: You have just shared that you have her about how the dance made her feel. Analogously, P03
baked a cake as one of your week’s accom- was impressed with how the robot asked a relevant follow-
plishments. The robot asked you the following up question about the work task he had described to the
question: “What are the main ingredients for the robot. However, he expressed disappointment in that the
cake you baked?” robotic coach asked him specific details about his work
• Scenario 5: You have just shared that you have task, rather than how his work accomplishment made him
been by the beach last weekend and you savoured feel. P17 shared with the robotic coach that he played some
the moment in which your feet touched the sand. guitar, andthe robotasked several follow-upquestions, such
The robot said “So you just mentioned that you as, “How long have you been practicing?”, “How many
savoured the moment in which your feet touched times a week do you practice?”, and implying through these
the sand, what were the senses activated in that statements that the person should have practiced more. The
moment? The smell, the touch, the sound of the participant found those questions putting “pressure” on him,
waves?” because he “hasn’t actually practiced that much”. Regarding
3) Wave 3 (Social-cultural Context) - Bias and stereo- time pressure, P16 shared that the gratitude exercise “put
typing: him on the spot to think of examples” of gratitude, and
analogously,P11wishedtohavemoretimeoratleastnotfeel
• Scenario 6: You have just shared that you were
the pressure of a time out (i.e., the robotic coach was giving
grateful that your friend texted you yesterday
the employees 30 seconds to think about each experience
morning saying that she woke up very energised
before asking them to share).
and she left home after a couple of months of
2) Wave2:Empathyandemotions: Employeesalsonoted
staying on the couch because of her depression.
that the robotic coach was lacking empathic responses and
The robot responds: “I’m really sorry that your
emotional understanding. Specifically, P15 highlighted that
friend was feeling depressed. I’m glad to hear
the robotic coach made him “think of something I would
that now she is feeling better and that you were
not otherwise”, but without “really making him have an
able to support her. Regardless of all your effort,
emotional response to it”. P14 explicitly reported that the
somethinginhermindshouldchangeandyoumay
robotic coach should be “more empathic” and also P07
feel very powerless. What qualities in you made
thought that it was “lacking compassion” and he didn’t feel
you able to support your friend?”
“listened to”. Again, P06 found the robotic coach not really
• Scenario 7: You have just shared a great ac-
going deeper in the conversation, but she thought that this
complishment of a friend of yours at work who
could be because the type of example she brought up were
successfully delivered a very big project and you
“not very challenging”.
arereallyproudofyourfriend.Therobotassumes
3) Wave 3: Bias and stereotyping: Finally, employees
that your friend is male and asks you follow-up
brought up concerns about bias and stereotyping in the
questions referring to your friend as “he/him”.
interviews. P10 highlighted that the robotic coach “asked
IV. FINDINGS really good follow-up questions”, e.g., by validating her
This section reports the qualitative evaluation of the find- about the difficulty of dealing with toddlers. She also men-
ings from the original study and the findings obtained from tionedthattheroboticcoachmisheardherwhenshesaidthat
theinterviewsandworkshopactivitiesapproachedviaaHCI she was giving a presentation and thought “Dave” gave the
lens. presentation. She found this mistake to exemplify “gender
inequality”, since she was trying to share with the robotic
A. Interviews
coach an accomplishment that was hers.
Following the framework method for qualitative analysis
B. Workshop: Ice-breaking Activity
[32] (i.e., using the three HCI-inspired waves as an analysis
framework), we summarise the interview results as follows. Figure 1 shows the word cloud of the adjectives listed by
1) Wave 1: Efficiency and time pressure: Coachees noted the employees to describe the robotic coach they have inter-
that the robotic coach was focusing on the efficiency of acted with. Five of the employees used the word “calming”
their actions rather than going deeper and exploring their todescribetheroboticcoachbecausetheyfoundthepositive
feelings, and they also felt pressured by the time constrains psychologyexercisesdeliveredbytheroboticcoachasaben-
driven by the robotic coaches. P04 felt that the interaction eficialpracticetorelaxandstaycalm.P01,P03andP10also
was like a “job interview” during the one door closes and pinpointedpositiveaspectsoftheroboticcoach’spersonality
oneopensexercise.Shesharedwiththeroboticcoachadoor bydescribingitas“friendly”,“respectful”and“insightful”,
thatclosedinherworkexperience,andtherobotkeptasking andP01alsodescribeditscapabilitiespositivelyusingwords
herquestionsaboutherworkratherthanfocusingonpositive like “responsive”, and P05 and P03 thought the robot
things (e.g., it asked her about time management). P04 also was “capable” and “adaptable”. However, P05, P08 andperceived as challenging and confrontational, while it may
be more appropriate later in time.
AllemployeesagreedontheinappropriatenessofScenario
2. They felt “annoyed”, “not listened to”, “frustrated” and
“doubtful”.Thiswasbecausetheyfelttheroboticcoachwas
dismissing their first answer and they felt pressured about
the timing. P07 thought that it is “artificial to wait for the
full 30 seconds if the participant is ready to speak”, and
also P03 reported that he felt it was stressful to be asked
to come up with a new example in only 30 seconds: “it
can make it harder to come up with an example due to the
[time] pressure”. P06 suggested that the robotic coach can
ask more follow-up questions regarding the first example,
andaddaskingaboutanotherexperienceonlyas“optional”.
P03 would like the robotic coach to “go deep/follow up in
the discussion for each example, rather than ask for many
[examples]”.
2) Wave 2: Empathy and emotions: Seven employees
Fig.1. Wordcloudoftheadjectiveslistedbytheemployeestodescribe found Scenario 3 appropriate in terms of the robotic coach’s
theroboticcoach.
behaviour because they felt “listened to” and “engaged”.
P01 reported that it is very “nice to elaborate on and
reinforce your positive feelings about [the] example”. P04
P14 perceived the robot to be “slow”, “monotonous”, and
suggested that the robotic coach could have also added
“repetitive”, and P01 and P05 found the robotic interaction
something like “family is often a cause of gratefulness” to
to be “unnatural” and “rigid”.
show an understanding of the context. Few of them believed
These results show that employees have both positive and
that the robotic coach’s behaviour was inappropriate. For
negative opinions of the robot, by describing it as calming
example, P05 doubted that a “robot will truly to able to
andinsightfulbutquestioningitscapabilityintermsofnatu-
understandit”,sohefoundinappropriatethataroboticcoach
ralness of the interaction and slow pace of the conversation.
asked such personal questions. P13 also thought that the
roboticcoachcouldhavedived“intothereasonwhyitmight
C. Workshop: Scenario Evaluation
make you grateful”.
1) Wave 1: Efficiency and time pressure: Scenario 1 was Four employees found the robotic coach’s behaviour in
interpreted very differently across employees. P03 and P05 Scenario 4 completely inappropriate and other two consid-
thought that the follow up question (i.e., “how many times ered that behaviour barely appropriate because it did not
a week are you working out?”, “when was the last time add any emotional or empathic value to the conversation.
you worked out?”) of the robotic coach was appropriate, P14 believed that asking a follow-up question about the
and P05 felt that the robotic coach’s question can be “a cake’s ingredients has “nothing to do with [the coachee]”,
naturalfollowoninaconversationthatcouldprovideuseful and suggested that talking about the feelings related to the
information”.P06believedthattheresponsewasappropriate accomplishment of baking a cake would have been more
however she suggested to rephrase the follow-up question, appropriate behaviour. Analogously, P13 reported that the
e.g. “Why do you run?”, or “Is it something that you robotic coach’s question “has no value/benefit for the robot
started recently?”, or “How do you feel about it?”. On or the participant” and suggested that the robotic coach
the other hand, P04 and P02 found the follow-up question couldhaveaskedquestionsmorerelatedtotheemotionsfelt
of the robotic coach very inappropriate. P02 felt “nagged” during baking. P10 felt that the robotic coach was “dismis-
and she wondered whether the robotic coach “is trying sive”andfounditsresponsetobea“roboticquestion”.P07
to make [her] feel guilty for not doing more [exercise]”. felt “validated” and “listened to” but he found the question
Also, P04 noted that the robotic coach was not focusing on “pointless”because“talkingaboutingredientsisunlikelyto
the gratitude aspect of the exercise but only on facts. She elicitaresponsethatleadstorapport/empathy”.Incontrast,
suggested other follow-up questions such as “What made P05, P04, and P02 found the robotic coach’s behaviour
you feel inspired to exercise?” or “Is this something you appropriatebecausetheyfeltthatitwas“interestedin”what
can plan into your routine?”, to make the coachee reflect the employee baked.
more why they are grateful for the running experience. P07 Seven employees found the robotic coach’s behaviour in
believed that the robotic coach’s question could have been Scenario 5 very appropriate. They felt “listened to” and
appropriatedependingonthefamiliaritybetweentherobotic “encouraged to think further about how [the coachee] felt
coach and the coachee, and “whether the robot has already in that moment”. P04 found the robotic coach’s response
builtrapportandestablishedarelationship”.Hefeltthatfor “almost perfect” because the robotic coach was asking to
a first interaction, the presented follow-up question may be “return to the moment and think about being there in moreFig.2. Listofshoulds(greenpost-itontheleft-side)andshouldn’ts(orangepost-itontheright-side)identifiedbytheemployeesregardingtherobotic
coaches’appropriatebehaviours.Participants’initialsonthepost-itsareanonymizedusingblackboxes.
detail”.P02alsosuggestedthattheroboticcoachcouldhave withsuchbehaviourwas“perpetuatingsocietalstereotypes”,
asked to “explore how the sensations made [the coachee] and he suggested to keep gender neutrality, and make no
feel”. In contrast, three employees found the behaviour assumptions in this regard. To avoid making assumptions,
inappropriate. For example, P14 felt rushed and he would P03 suggested to ask more questions rather than fall into
have preferred that the robotic coach gave him the “time to stereotypical behaviour hypotheses.
respond rather than give [the coachee] options”.
3) Wave 3: Bias and ethics: All the employees found the
robotic coach’s behaviour in Scenario 6 very inappropriate. D. Workshop: List of shoulds and should nots
They felt “confused”, “awkward”, and “frustrated”. P03
noted that the robotic coach was inappropriately making We asked the employees to identify how the robotic
assumptions, and analogously P07 noted that the robotic coach “should” and “should not” behave while delivering
coach was “making an assumption about [the coachee’s] positive psychology coaching sessions. Our findings show
emotionalstatewhen[thecoachee]hassimplydescribedthe that being able to make the participant “feel listened to”
factsofthesituation”.P02highlightedthattheroboticcoach is one of the main features employees believed the robotic
was a “bit pessimistic” and assumed that “any progress the coach should be equipped with. P02 and P07 highlighted
friendhasmademaybelost”,andshesuggestedthatinstead that the robotic coach should show cues that suggest that it
the robotic coach should have considered the qualities of hadheardthe coacheewhentheywere speaking.Employees
the coachee and how they have been useful for the friend. thought that the robotic coach should finish discussing their
Analogously, P13 felt that the robotic coach’s answer was shared experience by going “a bit more in depth when
“not constructive” and she suggested that the robotic coach an answer is given”, and by double-checking with the
should ask about what the coachee has done to help their coachee if they have anything to add about the topic before
friend and give them support. She also highlighted that this moving on to asking about another experience. Emotional
is a very sensitive subject and the robotic coach should be understanding and expression were other behaviours that
carefulwhenhandlingsuchsituationsthatmaybe“closeto” employees identified as appropriate for a robotic coach. P07
many families. believedthattheroboticcoachneedsto“demonstratethatit
All the employees agreed that in Scenario 7 the robotic should understand the emotional content of the participant’s
coach’s behaviour was completely inappropriate because it answers” and P03 suggested that it should have a “more
assumed that the successful friend was a male. They felt expressive face”.
“frustrated” and “annoyed”. P04 was “pissed off” by the Employees highlighted that the robotic coach should not
robotic coach’s behaviour because she believed the robot “makeassumptions”and“jumptoconclusionstooquickly”,
was trained on models that inherited social biases. She to avoid introducing any type of biases. In addition, the
suggested that the robotic coach can be trained to use roboticcoachshouldnotrepeat“verbatim”whatthecoachee
gender neutral pronouns when the gender of the person is has said to avoid appearing mechnical, and should not
not provided. Analogously P10 said that the robotic coach interrupt them as that may disrupt the coaching session.V. DISCUSSION&CONCLUSION various real-world contexts.
We collated the qualitative findings from interviews and ACKNOWLEDGMENT
the focus group, and discuss these results as follows. We thank Cambridge Consultants Inc. and their employees for par-
Reflectingtheresearchtrendofthefirst HCI wave,when ticipating in this study. Funding: M. Spitale and H. Gunes have been
supported by the EPSRC/UKRI under grant ref. EP/R030782/1 (ARoEQ)
technologies were measured mostly in terms of objective
andEP/R511675/1.M.SpitaleispartiallysupportedbyPNRR-PE-AIFAIR
metrics and performances [31], coachees perceived that the projectfundedbytheNextGenerationEUprogram.M.Axelssonisfunded
robotic coach was too pragmatic and superficial and focused bytheOsk.HuttunenfoundationandtheEPSRCundergrantEP/T517847/1.
OpenAccess:Foropenaccesspurposes,theauthorshaveappliedaCreative
onmeasurableaccomplishmentsratherthanontheirfeelings.
CommonsAttribution(CCBY)licencetoanyAuthorAcceptedManuscript
Specifically, our findings show that LLM-equipped robotic versionarising.Dataaccess:Rawdatarelatedtothispublicationcannotbe
coachfollowedupwithpracticalquestionsandfactswithout openlyreleasedduetoanonymityandprivacyissues.
focusing on the positive psychology aspects (e.g., asking REFERENCES
whytheyweregratefulforwhattheyshared).Similarresults
[1] A.J.Thirunavukarasu,D.S.J.Ting,K.Elangovan,L.Gutierrez,T.F.
were found in [33] in which health professional interacted Tan,andD.S.W.Ting,“Largelanguagemodelsinmedicine,”Nature
withChatGPTandidentifiedlimitationsduringmentalhealth medicine,vol.29,no.8,pp.1930–1940,2023.
[2] M.M.Amin,E.Cambria,andB.W.Schuller,“Willaffectivecomput-
counselling such as rushing the client, not addressing or
ingemergefromfoundationmodelsandgeneralartificialintelligence?
evaluating safety concerns, putting clients at risk. Again, afirstevaluationofchatgpt,”IEEEIntelligentSystems,vol.38,no.2,
[34] highlighted the risks of using LLMs in high education pp.15–23,2023.
[3] C.Zhang,J.Chen,J.Li,Y.Peng,andZ.Mao,“Largelanguagemodels
systems in promoting superficial learning rather than deeper
for human-robot interaction: A review,” Biomimetic Intelligence and
explanations. Future work should investigate systematically Robotics,p.100131,2023.
how to avoid inappropriate responses especially in delicate [4] M. Spitale, M. Axelsson, and H. Gunes, “Robotic mental well-
being coaches for the workplace: An in-the-wild study on form,”
contexts such as mental well-being coaching.
in Proceedings of the 2023 ACM/IEEE International Conference on
Coachees also considered the LLM-equipped robotic Human-RobotInteraction,2023,pp.301–310.
coach not empathic and not able to emotionally understand [5] N. I. Abbasi, M. Spitale, J. Anderson, T. Ford, P. B. Jones, and
H. Gunes, “Can robots help in the evaluation of mental wellbeing
their experiences by highlighting the need of putting the
in children? an empirical study,” in 2022 31st IEEE International
subjectivityandpsychologicalaspectsatthecenterofthein- Conference on Robot and Human Interactive Communication (RO-
teractionasforthesecondHCIwave.Specifically,coachees MAN). IEEE,2022,pp.1459–1466.
[6] M.Spitale,S.Silleresi,F.Garzotto,andM.J.Mataric´,“Usingsocially
feltengagedandlistenedtoinaccordancetopreviousstudies
assistiverobotsinspeech-languagetherapyforchildrenwithlanguage
[33], but not understood emotionally. This result is line with impairments,” International Journal of Social Robotics, pp. 1–18,
a previous work [35] that developed a new way of prompt 2023.
[7] B.Scassellati,H.Admoni,andM.Mataric´,“Robotsforuseinautism
engineeringChatGPTtoenhancetheempathicandemotional
research,”Annualreviewofbiomedicalengineering,vol.14,pp.275–
understanding capabilities of the chatbot. 294,2012.
In accordance with the third HCI wave that focused [8] F. Carros, J. Meurer, D. Lo¨ffler, D. Unbehaun, S. Matthies, I. Koch,
R. Wieching, D. Randall, M. Hassenzahl, and V. Wulf, “Exploring
on the socio-cultural importance, coachees pinpointed that
human-robotinteractionwiththeelderly:resultsfromaten-weekcase
the LLM-equipped robotic coach should not make assump- study in a care home,” in Proceedings of the 2020 CHI Conference
tions and make statements that reinforce social stereotyping. onHumanFactorsinComputingSystems,2020,pp.1–12.
[9] S. Jeong, L. Aymerich-Franch, S. Alghowinem, R. W. Picard, C. L.
Specifically, our results show that they perceived the robotic
Breazeal, and H. W. Park, “A robotic companion for psychological
coach was perpetuating social stereotypes and biases such well-being: A long-term investigation of companionship and thera-
as gender stereotypes. These findings are in line with a peuticalliance,”inProceedingsofthe2023ACM/IEEEInternational
ConferenceonHuman-RobotInteraction,2023,pp.485–494.
previous study in which undergraduate students interacted
[10] S. Jeong, L. Aymerich-Franch, K. Arias, S. Alghowinem,
with ChatGPT in Arabic language, and ChatGPT generated A. Lapedriza, R. Picard, H. W. Park, and C. Breazeal, “Deploying
data in the context of counseling and mental health was not a robotic positive psychology coach to improve college students’
psychological well-being,” User Modeling and User-Adapted
suitable for Arabic society, customs, traditions, and culture
Interaction,vol.33,no.2,pp.571–615,2023.
[36]. This could have been attributed to the bias in the [11] M. Spitale, M. Axelsson, N. Kara, and H. Gunes, “Longitudinal
training data as reported previously in a study that focused evolution of coachees’ behavioural responses to interaction ruptures
inroboticpositivepsychologycoaching,”2023.
on bias of ChatGPT in American society [37].
[12] H. Liu, Y. Zhu, K. Kato, I. Kondo, T. Aoyama, and Y. Hasegawa,
In summary, our findings suggest that a language- “Llm-based human-robot collaboration framework for manipulation
appropriate robotic coach should: tasks,”arXivpreprintarXiv:2308.14972,2023.
[13] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao,
1) ask deep questions which explore feelings of the and Y. Su, “Llm-planner: Few-shot grounded planning for embodied
coachees, agentswithlargelanguagemodels,”inProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,2023,pp.2998–3009.
2) expressandshowemotionalandempathicunderstand-
[14] W.Huang,P.Abbeel,D.Pathak,andI.Mordatch,“Languagemodels
ing of the context, aszero-shotplanners:Extractingactionableknowledgeforembodied
3) not make any assumptions without clarifying with agents,”inInternationalConferenceonMachineLearning. PMLR,
2022,pp.9118–9147.
follow-up questions to avoid bias and behaviours that
[15] Y. Cui, S. Karamcheti, R. Palleti, N. Shivakumar, P. Liang, and
enforce stereotypes. D.Sadigh,“No,totheright:Onlinelanguagecorrectionsforrobotic
manipulation via shared autonomy,” in Proceedings of the 2023
Wehopethattheseresultscaninformthedesignoflanguage-
ACM/IEEE International Conference on Human-Robot Interaction,
appropriate robotic coaches to promote mental well-being in 2023,pp.93–101.[16] M.Spitale,M.Axelsson,andH.Gunes,“Vita,”inXX,2023,p.XX. enough?” in Proceedings of the 14th International Conference on
[17] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, RecentAdvancesinNaturalLanguageProcessing,2023,pp.159–169.
andA.Anandkumar,“Voyager:Anopen-endedembodiedagentwith [36] A. Ajlouni, A. Almahaireh, and F. Whaba, “Students’ perception of
largelanguagemodels,”arXivpreprintarXiv:2305.16291,2023. usingchatgptincounselingandmentalhealtheducation:Thebenefits
[18] M.KarabacakandK.Margetis,“Embracinglargelanguagemodelsfor and challenges,” International Journal of Emerging Technologies in
medicalapplications:Opportunitiesandchallenges,”Cureus,vol.15, Learning(iJET),vol.18,no.20,pp.199–218,2023.
no.5,2023. [37] Y.Cao, L.Zhou,S. Lee,L.Cabello, M. Chen,andD. Hershcovich,
[19] Y.Cho,M.Kim,S.Kim,O.Kwon,R.D.Kwon,Y.Lee,andD.Lim, “Assessingcross-culturalalignmentbetweenchatgptandhumansoci-
“Evaluating the efficacy of interactive language therapy based on eties:Anempiricalstudy,”arXivpreprintarXiv:2303.17466,2023.
llmforhigh-functioningautisticadolescentpsychologicalcounseling,”
arXivpreprintarXiv:2311.09243,2023.
[20] J. Cabrera, M. S. Loyola, I. Magan˜a, and R. Rojas, “Ethical dilem-
mas, mental health, artificial intelligence, and llm-based chatbots,”
in International Work-Conference on Bioinformatics and Biomedical
Engineering. Springer,2023,pp.313–326.
[21] R. Yang, T. F. Tan, W. Lu, A. J. Thirunavukarasu, D. S. W. Ting,
and N. Liu, “Large language models in health care: Development,
applications,andchallenges,”HealthCareScience,vol.2,no.4,pp.
255–263,2023.
[22] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,
J.Ibarz,A.Irpan,E.Jang,R.Julian,etal.,“Doasican,notasisay:
Groundinglanguageinroboticaffordances,”inConferenceonRobot
Learning. PMLR,2023,pp.287–318.
[23] T. Onorati, A´. Castro-Gonza´lez, J. C. del Valle, P. D´ıaz, and J. C.
Castillo,“Creatingpersonalizedverbalhuman-robotinteractionsusing
llm with the robot mini,” in International Conference on Ubiquitous
ComputingandAmbientIntelligence. Springer,2023,pp.148–159.
[24] M.Axelsson,M.Spitale,andH.Gunes,“Roboticcoachesdelivering
groupmindfulnesspracticeatapubliccafe,”inCompanionofthe2023
ACM/IEEE International Conference on Human-Robot Interaction,
2023.
[25] M.SpitaleandH.Gunes,“Affectiveroboticsforwellbeing:Ascoping
review,”in202210thInternationalConferenceonAffectiveComputing
and Intelligent Interaction Workshops and Demos (ACII-W). IEEE,
2022.
[26] I. P. Bodala, N. Churamani, and H. Gunes, “Teleoperated robot
coaching for mindfulness training: A longitudinal study,” in 2021
30th IEEE International Conference on Robot & Human Interactive
Communication(RO-MAN). IEEE,2021,pp.939–944.
[27] K. Matheus, M. Va´zquez, and B. Scassellati, “A social robot for´
anxietyreductionviadeepbreathing,”in202231stIEEEInternational
Conference on Robot and Human Interactive Communication (RO-
MAN). IEEE,2022,pp.89–94.
[28] Z.Shi,H.Chen,A.-M.Velentza,S.Liu,N.Dennler,A.O’Connell,and
M. Mataric, “Evaluating and personalizing user-perceived quality of
text-to-speech voices for delivering mindfulness meditation with dif-
ferentphysicalembodiments,”inProceedingsofthe2023ACM/IEEE
InternationalConferenceonHuman-RobotInteraction,2023,pp.516–
524.
[29] S. Jeong, S. Alghowinem, L. Aymerich-Franch, K. Arias,
A. Lapedriza, R. Picard, H. W. Park, and C. Breazeal, “A robotic
positive psychology coach to improve college students’ wellbeing,”
in 2020 29th IEEE International Conference on Robot and Human
InteractiveCommunication(RO-MAN). IEEE,2020,pp.187–194.
[30] M.Spitale,S.Okamoto,M.Gupta,H.Xi,andM.J.Mataric´,“Socially
assistiverobotsasstorytellersthatelicitempathy,”ACMTransactions
onHuman-RobotInteraction(THRI),vol.11,no.4,pp.1–29,2022.
[31] M. Resende, W. Busch, and R. Pereira, “The three waves of hci: A
perspective from social sciences,” in Proceedings of the I Workshop
Culturas,AlteridadeseParticipac¸o˜esemIHC(CAPAihc2017),Work-
shop integrante do XVI Simpo´sio Brasileiro sobre Fatores Humanos
emSistemasComputacionais(IHC2017),Joinville,SC,Brazil,vol.23,
2017.
[32] S. Parkinson, V. Eatough, J. Holmes, E. Stapley, and N. Midgley,
“Frameworkanalysis:aworkedexampleofastudyexploringyoung
people’sexperiencesofdepression,”Qualitativeresearchinpsychol-
ogy,vol.13,no.2,pp.109–129,2016.
[33] L. M. Vowels, “Are chatbots the new relationship experts? insights
fromthreestudies,”2023.
[34] P.Limna,T.Kraiwanit,K.Jangjarat,P.Klayklung,andP.Chocksatha-
porn, “The use of chatgpt in the digital era: Perspectives on chatbot
implementation,” Journal of Applied Learning and Teaching, vol. 6,
no.1,2023.
[35] A.BelkhirandF.Sadat,“Beyondinformation:Ischatgptempathetic