Fully Independent Communication in Multi-Agent Reinforcement
Learning
RafaelPina VarunaDeSilva
LoughboroughUniversityLondon LoughboroughUniversityLondon
London,UnitedKingdom London,UnitedKingdom
r.m.pina@lboro.ac.uk v.d.de-silva@lboro.ac.uk
CorentinArtaud XiaolanLiu
LoughboroughUniversityLondon LoughboroughUniversityLondon
London,UnitedKingdom London,UnitedKingdom
c.artaud2@lboro.ac.uk xiaolan.liu@lboro.ac.uk
ABSTRACT knowledgeofcertainoccurrencescanhelptopreventcatastrophic
Multi-AgentReinforcementLearning(MARL)comprisesabroad events.
areaofresearchwithinthefieldofmulti-agentsystems.Several InconventionalMARLapproaches,communicationcanoften
recentworkshavefocusedspecificallyonthestudyofcommunica- be applied by adding an additional network that learns how to
tionapproachesinMARL.Whilemultiplecommunicationmethods producemessagesthatfollowacertaincommunicationprotocol
havebeenproposed,thesemightstillbetoocomplexandnoteasily [7,20,40].Thisnetworkcanbeintegratedintothelearningprocess
transferabletomorepracticalcontexts.Oneofthereasonsforthat oftheagentsandimprovetheperformanceofthestandardMARL
isduetotheuseofthefamousparametersharingtrick.Inthis algorithm.Insimpleterms,eachagentproducesacertainmessage
paper,weinvestigatehowindependentlearnersinMARLthatdo thatrepresentsitsownknowledgeorexperienceatacertainmo-
notshareparameterscancommunicate.Wedemonstratethatthis ment,andthenthismessageisbroadcastedtotheotheragents.
settingmightincurintosomeproblems,towhichweproposea Afteritisdelivered,themessageisusedasanadditionalinputto
newlearningschemeasasolution.Ourresultsshowthat,despite thestandardnetworksoftheagents,meaningthatnowtheyhave
thechallenges,independentagentscanstilllearncommunication someknowledgeaboutsomethingelsethatwassentbytheothers,
strategiesfollowingourmethod.Additionally,weusethismethod althoughitisencoded.
toinvestigatehowcommunicationinMARLisaffectedbydifferent InmostMARLapproaches,itisadoptedaconfigurationwhere
networkcapacities,bothforsharingandnotsharingparameters. theparametersofthelearningnetworksareshared[10,16,27,32,
Weobservethatcommunicationmaynotalwaysbeneededand 37].Thissettingisoftenreferredtosimplyasparametersharing
thatthechosenagentnetworksizesneedtobeconsideredwhen and,asthenamesuggests,approachesthatadoptthisconfiguration
usedtogetherwithcommunicationinordertoachieveefficient useonlyasinglenetwork(ortwo,ifthereisamixernetworkor
learning. acommunicationnetwork,forinstance)thatissharedbyallthe
agentsoftheteam.However,whenwelookatpracticalapplications,
sharingparametersbecomesunrealistic[39].Withinthemultiple
KEYWORDS
proposedcommunicationmethods,whensharingparametersis
Multi-AgentReinforcementLearning,Communication,Indepen-
notfeasibleaquestionnaturallyarises:cancommunicationstillbe
dentLearning,DeepLearning
conductedsuccessfullywhentheagentsdonotshareparameters?
Inthispaper,weinvestigatecommunicationamongindependent
learnersinMARLwhodonotsharetheparametersoftheirnet-
1 INTRODUCTION
worksandconsideragentsthathavedistinctnetworksfortheir
CommunicationinMulti-AgentReinforcementLearning(MARL)
policyandforgeneratingcommunicationmessages.Wedemon-
hasbeenanimportanttopicofresearchinthebroadfieldofMARL
stratethatthiskindofcommunicationcanbechallengingtoachieve
[6,8,15,16,18].Usually,inthestandardapproaches,agentslearn
duetotheparametersofthenetworksnotbeingshared,butitstill
thetasksbyobservingcertainpartsoftheenvironmentaround
ispossiblethankstoanewlearningschemethatweproposeinthis
them,andthenmakeadecisionbasedonwhattheysee.However,
paper.Additionally,inthecourseoftheexperiments,wearguethat
iftheagentsarecarriersofcommunicationcapabilities,theycan
communicationinMARLmightnotalwaysbenecessaryand,ifit
useotherinformationbesidesonlytheirownobservationstomake
isusednaivelywithoutconsideringtheenvironment,itendsup
abetterdecision.Forinstance,theycanreceiveobservationsfrom
bringingoverheadtothelearningprocess,withoutanybenefits.
theotheragents,ortheiractions[6,18].
Tofurtheranalysetheeffectofcommunicationinthelearning
Fromtheperspectiveofpracticalapplications,communicationis
processofMARL,wealsoinvestigatewhethersimplyincreasing
alsoseenasafeasiblewayofimprovinglearningduetoprogresses
thecapacityofthenetworksoftheagentscancompensateforthe
indiversefields[3,9,22].Forexample,inscenariosofautonomous
absenceofacommunicationnetwork.Thisisbecause,byincreasing
vehiclesorfactorieswithmultipleagents,communicationcanbe
thecapacityoftheagentnetwork,theamountofinformationthat
keytolearnbetterresponsestoeventsthatsomeagentscansee
atacertainmoment,buttheotherscannot[23].Havingthisprior
4202
naJ
62
]GL.sc[
1v95051.1042:viXrathisnetworkcanrepresentalsoincreases,whichcouldaccommo-
datefortheabsenceofcommunication.Ontheotherhand,commu-
nicationenablestheflowofinformationacrossagents,which,at
firstglance,shouldalwaysbebeneficial,posinganotherinteresting
question.
Overall,inthispaper,weintendtostudythechallengesofcom-
municationinindependentMARLwhenparametersarenotshared,
whichisanunderstudiedsettinginMARLthatcanbringbene-
(a)ParameterSharing (b)NoParameterSharing
fitsforpracticalapplications[39].Inthissense,weproposeaway
ofsuccessfullycommunicatingundertheseconditions.Wealso
showthatcommunicationmightnotalwaysbeuseful,bringing Figure1:Simpleoverviewoftheconfigurationsofsharing
uselessoverhead,andshowhowitaffectslearningwhentheagent (left)andnotsharing(right)parameters.Intheformer,a
networkshavehigherorlowercapacities. jointactionisproducedbypoliciesthatarepartofthesame
network(sharedbyalltheagents),whileinthelattereach
2 BACKGROUND agenthasitsownseparatepolicy.
2.1 DecentralisedPartiallyObservableMarkov
DecisionProcesses(Dec-POMDPs) whereeachagentmaintainsallthesecomponentsonitsown.For
In this work we formalise the treated scenarios following brevity,inthispaperwerefertothismethodalsoasIndependent
Decentralised Partially Observable Markov Decision Processes Q-learning(IQL),andusedeeprecurrentQ-networks,asintroduced
(Dec-POMDPs) [25]. These can be represented by the tuple in[12],toaccommodateforpartialobservability.Overall,IQLis
𝐺=⟨𝑆,𝐴,𝑟,𝑂,𝑍,𝑃,𝑁,𝛾⟩. At each state 𝑠 ∈ 𝑆, each agent 𝑖 ∈ trainedtominimisetheloss
N≡{1,...,𝑁} chooses an action 𝑎 ∈ 𝐴, forming a joint action (cid:20) (cid:21)
≡= {𝑎1,...,𝑎 𝑁}thatisexecutedasawholeintheenvironment,
L(𝜃)=E
𝑏∼𝐵
(cid:0)𝑟+𝛾m 𝑎a ′x𝑄(𝜏′,𝑎′;𝜃−)−𝑄(𝜏,𝑎;𝜃)(cid:1)2 , (2)
resultinginatransitiontoanextstate𝑠′ accordingtoaproba-
forasample𝑏sampledfromareplaybufferofexperiences𝐵,and
bility 𝑃(𝑠′|𝑠,𝑎) : 𝑆 ×𝐴 ×𝑆 → [0,1]. This results in a reward
where𝜃 and𝜃− aretheparametersofthelearningnetworkanda
𝑟(𝑠,𝑎) : 𝑆 ×𝐴 → Rthatissharedbytheteam.Becauseofpar-
targetnetwork,respectively.
tial observability, at each state each agent receives only a local
observation𝑜 𝑖 ∈ 𝑂(𝑠,𝑖) : 𝑆 ×N → 𝑍,andeachagentholdsan
action-observationhistory𝜏 𝑖 ∈ T ≡ (𝑍 ×𝐴)∗.Inthecontextof 2.3 SharingParametersandNotSharing
communication,eachagentgeneratesacertainmessage𝑚 𝑖 ∈M Parameters
thatisthenbroadcastedtotheothersandwillalsoconditiontheir ParametersharingisapopularstrategyadoptedbymostMARL
policies.Thus,ifasetofincomingmessagestoagent𝑖excepttheir approachesthatallowsalltheagentsofateamtosharethesame
ownisrepresentedby𝑚 −𝑖,thenitspolicycanberepresentedas learningnetworks[10].Onekeyaspectofthisapproachistheuse
𝜋 𝑖(𝑎 𝑖|𝜏 𝑖,𝑚 −𝑖).Thejointpolicyaimstooptimiseajointaction-value ofanagentIDtogetherwitheachagent’sinputwhentheseare
function𝑄 𝜋(𝑠 𝑡,𝑎 𝑡)=E 𝜋[𝑅 𝑡|𝑠 𝑡,𝑎 𝑡],where𝑅 𝑡 =(cid:205) 𝑘∞ =0𝛾𝑘𝑟 𝑡+𝑘 isthe fedtothenetwork.Asstatedandsupportedbymultipleprevious
discountedreturnand𝛾 ∈ [0,1)isadiscountfactor. works,thistrickwillallowthesamenetworktotreattheagentsas
independentunits,despitetheinputsofallagentsbeinggivenat
2.2 IndependentDeepQ-Learning(IQL) thesametimeandtothesamenetwork.Accordingtotheliterature,
Asoneofthefirstproposedapproachesformulti-agentlearning,In- theultimatebenefitofthisstrategyismainlyseeninthetraining
dependentQ-learning(IQL)canbeseenasthemoststraightforward timethattheagentstaketoreachconvergenceinthetasksand
MARLmethod[34].Insimplerterms,IQLconsistsofgeneralising henceonthesampleefficiency[4,27].
theconceptsfromsingle-agentreinforcementlearningtomulti- Not using parameter sharing is empirically not beneficial in
agentsettings,i.e.,eachagentproducesanindividualQ-function simulatedenvironments.However,itallowsustohaveadifferent
thatisupdatedfollowingtheequation perspectiveonMARLandlookintothelearningagentsasfully
self-containedentities(likehumans),andthatlearnbythemselves.
(cid:20) (cid:21)
𝑄(𝑠,𝑎)=(1−𝛼)𝑄(𝑠,𝑎)+𝛼 𝑟+𝛾max𝑄(𝑠′,𝑎′) , (1) Importantly,whenweconsiderpracticalapplications,itisoften
𝑎′ unfeasibletokeepanetworkthatissharedbyalltheagents[39].
where𝛼 isalearningrate.FollowingtheintroductionofDeepQ- InFigure1,wecanseethekeydifferencesbetweenthetwode-
Networks(DQN)[24],intheworkof[33]theauthorsusetogether scribedconfigurations.Asitisillustrated,intheparametersharing
theadvancesfromdeepreinforcementlearningandindependentQ- setting,allthepoliciesarecontrolledbythesamenetworkwith
learningtoproposeanimprovedindependentQ-learningwherethe parameters𝜃,producingajointactionthatcontainsalltheactions
agentsarenowcontrolledbyindividualdeepQ-networksinstead oftheagents.Instead,withthenon-parametersharingconfigu-
offollowingonlythesimplertabularcase.Importantly,inDQN ration,eachagentiscontrolledbyanindependentnetworkwith
theauthorsintroduceanexperiencereplaybufferthatiskeptby parameters𝜃 thatproducestheactiononlyforthisparticularagent.
𝑖
thelearningagentandtheuseofatargetnetworkthatstabilises Afteralltheactionsarecomputed,theseareputtogetherintoa
learning.Logically,thiscanbereplicatedinthemulti-agentcase, jointactionthatisexecutedintheenvironment.implicationsofnotsharingparameters.Importantly,whilesharing
parameters can be easily done in simulated environments, it is
somethingverydifficulttoachieveinpracticalscenarios[39].Thus,
itisimportanttostudytheimplicationsofnotsharingparameters.
Inthispaper,weintendtoanalysehowcommunicationcanbe
integratedwithindependentlearnersthatdonotshareparameters.
Inotherwords,theseagentshavetheirownnetworksandmust
learnhowtogeneratemessageswithoutreceivingdirectfeedback
fromthepoliciesoftheotheragents.In[8],despitetheyhavenot
shownit,theauthorsdiscusshowRIALcouldbeextendedtothe
(a)CommunicationwithParameterShar- (b) CommunicationwithNoParameter
casewhereparametersarenotshared.Inthecaseof[8]itwouldbe
ing(PS) Sharing(NPS)
simplesincethemessagesaretreatedasiftheywereactionsthatare
sent,andthataregeneratedfromthesamepolicynetwork,leading
Figure2:Illustrationofthemaindifferencesintheprocess tolowerqualitymessages[1,13].Thus,therearenoproblemswhen
ofgeneratingandbroadcastingmessagesbetweensharing thelossesarepropagatedinthenetworkbecausetheestimated
andnotsharingparametersofthelearningnetworks.Inthe Q-values ensure that the links to the policy networks are kept.
firstcase,boththepoliciesandcommunicationnetworks However,trainingapolicyandlearningmessagesatthesametime
arecontrolledbythesameparameters𝜃 and𝜇,whileinthe
usingthesamenetworkmightrepresentadifficultlearningtrade-
secondcase,thesehavedistinctparameters𝜃 𝑖 and𝜇 𝑖. off[26].Iftheagentshaveinsteadapolicyandadistinctnetwork
specialisedforcommunicationtheycanlearnstrongerbehaviours,
buttheprocessbecomeschallenging.Wenowformallydescribethis
2.4 CommunicationinMARL processandhighlightthemainproblemsthatneedtobeaddressed.
Asithasbeendiscussedthroughoutthispaper,communication We consider agents that are controlled by a standard policy
inMARLconsistsontheabilityoftheagentstosharesomeof network and also have a distinct network whose purpose is to
theirexperienceoftheenvironmentwiththeteammates.Thiscan generatemessagesforcommunication.Letusfirstconsiderthecase
includeelementssuchastheobservations,theactions,andevena ofIQLwithparametersharingandcommunication.Forsimplicity
fingerprint.Thus,byusingthisadditionallayerofinformationin ofnotation,inthedemonstration,weusetheobservations𝑜 𝑖instead
thelearningprocess,theagentswilllearnaQ-functionthatcondi- ofthehistory𝜏 𝑖.Letalso 𝑓 𝑖 and𝑔 𝑖 denotetwocertainfunctions
tionsnotonlyontheaction-observationhistoryasintraditional suchthat𝑓 𝑖 →𝑄and𝑔 𝑖 →M,forasetofallQ-values𝑄andaset
approachesbutalsoinasetofincomingmessagesfromtheothers, ofallmessagesM.Wehavethat
𝑄 𝑖( F[ i𝜏 g𝑖 u,𝑚 re− 2𝑖] d, e𝑎 p𝑖 i) c.
tsanoverviewofthemessagegenerationprocess
{𝑄 𝑖} 𝑖𝑁 =1={𝑓 𝑖(𝑜 𝑖,𝑚 −𝑖,𝑎 𝑖;𝜃)} 𝑖𝑁 =1, (3)
inMARL.InFigure2(a),wecanseethat,whentheagentssharepa- where𝑚 −𝑖 correspondstothemessagesfromallagentsexcept𝑖,
rametersofthelearningnetworks,theprocessisrelativelystraight- thatisproducedbyaneuralnetworkdenotedbyafunction𝑔 𝑗 with
forward:acommunicationnetworkcontrolledbytheparameters𝜇 parameters𝜇
w tioil nl sg )e ,n ae nr dat te ht eh ne tm hee ss esa ag re es s( ei nn tth tois thca es oe ta hn eren ac go ed ni tn sg anof dt fh ee do tb os te hrv eia r- 𝑚
−𝑖
={𝑔 𝑗(𝑜 𝑗;𝜇)}𝑁
𝑗=1,𝑗≠𝑖
∧𝑚
𝑖
=𝑔 𝑖(𝑜 𝑖;𝜇). (4)
AsperEq.(2),wecandefinethelossfunctionforthelearning
policynetworktogetherwiththeobservations.Figure2(b)now
problemas
depictsthecommunicationprocessbutwhenparametersarenot
wsh oa rr ke sd. bL oo thgi fc oa rll ty h, ein pt oh li is cyca cs oe n, te ra oc lh lea dg be ynt 𝜃co an nt dro fl os rin cod mep men ud ne in cat tn ioe nt- L𝑖(𝜃,𝜇)=𝑟+𝛾𝑚𝑎𝑥
𝑎
𝑖′𝑄 𝑖(𝑜 𝑖′,𝑚′ −𝑖,𝑎 𝑖′;𝜃−)−𝑄 𝑖(𝑜 𝑖,𝑚 −𝑖,𝑎 𝑖;𝜃)
controlledby𝜇
𝑖.Inthiscase,eachagent𝑖
independentlygenerates
=𝑟+𝛾𝑚𝑎𝑥
𝑎
𝑖′𝑓 𝑖(𝑜 𝑖′,{𝑔 𝑗(𝑜′ 𝑗;𝜇−)} 𝑗𝑗 == 1𝑁 ,𝑗≠𝑖,𝑎 𝑖′;𝜃−)
a simm pes lesa ag te fit rh sa t,t ii ts bth ree an ks se tn ht eto linth ke inot th he ers c. oW mh puil te at th iois nam li gg rh at ps hee fom
r
−𝑓 𝑖(𝑜 𝑖,{𝑔 𝑗(𝑜 𝑗;𝜇)} 𝑗𝑗 == 1𝑁 ,𝑗≠𝑖,𝑎 𝑖;𝜃). (5)
thecommunicationnetworkswhenthelossesarebackpropagated,
Fromtheabove,wecanwriteL𝑖(𝜃,𝜇)≡L𝑖(𝑓 𝑖(·;𝜃,𝜇),𝑔 𝑖(·;𝜇)),and
thenwecanalsowritethebackpropagationrulesforthegradients
sincetheparametersofthenetworksarenotsharedandthefinal
as
Q-valuesthatresultfromthepoliciesareconditionedonlyonthe
incomingmessages.Wediscussthisphenomenonfurtherahead. ∇𝜃L𝑖 = 𝜕 𝜕L 𝑓𝑖 𝜕 𝜕𝑓 𝜃𝑖 + 𝜕 𝜕L 𝑔𝑖 𝜕 𝜕𝑔 𝜃𝑖 = 𝜕 𝜕L 𝑓𝑖 𝜕 𝜕𝑓 𝜃𝑖 , (6)
𝑖 𝑖 𝑖
3 METHODS ∇𝜇L𝑖 = 𝜕L𝑖 𝜕𝑓 𝑖 + 𝜕L𝑖 𝜕𝑔 𝑖 , (7)
𝜕𝑓 𝜕𝜇 𝜕𝑔 𝜕𝜇
𝑖 𝑖
3.1 CommunicationinMARLforFully andfromthis,itfollowsthattheparametersofthenetworksare
IndependentLearners updatedas
I cn omth mis us ne ic ct ai to in on,w we its hta fr ut llb yy info dr em pea nll dy ed ne ts ac gr eib ni tn sg inth Me Aim RLpl ti hca at ti do ons no otf 𝜃 =𝜃−𝛼∇𝜃L𝑖 =𝜃−𝛼𝜕 𝜕L
𝑓
𝑖𝑖 𝜕 𝜕𝑓 𝜃𝑖 , (8)
s Mh Aar Re Lpa ar pa pm roe ate cr hs e. sS ,h aa nr din tg hp ua sr ia tm iset oe fr ts enis ft oa rk ge on ttf eo nrg tora cn ote nd sii dn erm to hs et 𝜇=𝜇−𝛼∇𝜇L𝑖 =𝜇−𝛼(cid:18)𝜕 𝜕L 𝑓𝑖 𝜕 𝜕𝑓 𝜇𝑖 + 𝜕 𝜕L 𝑔𝑖 𝜕 𝜕𝑔 𝜇𝑖(cid:19) . (9)
𝑖 𝑖Figure 3: Illustration of how our proposed scheme for independent communication without parameter sharing
(NPS+IQL+COMM)workswhencomparedtosharingparameters(PS).Thefigureshowsthatagentsthatdonotsharepa-
rametersalsoneedtoreceivetheirownmessageasinputtokeepthelinktothecomputationalgraphoftheircommunication
networkduringbackpropagation.Ontheotherhand,whenparametersaresharedthistrickisnotneededsinceallofthemuse
thesamenetworkandtherearenogradientpropagationproblemsbylosingthelinkstothecommunicationnetworksinthe
computationgraph.
ThisisthestandardprocedureforIQLwithparametershar- order to update 𝜇
𝑖
the corresponding gradient rule in this case
ing.However,whenwedonotshareparametersofthenetworks, wouldhavetobe
the case can be very different. We consider now the setting of
f au nl dly wi in thde cp oe mn mde un nt icl ae ta ir on ne .r Is n, ti h.e i. s, cI oQ nL figw ui rt ah tin ono ,p tha era am gee nte tsr as rh ea fr uin llg
y ∇𝜇𝑖L𝑗 =
𝜕 𝜕L 𝑓𝑗 𝜕𝜕 𝜇𝑓 𝑗
+
𝜕𝜕 𝑔L𝑗 𝜕𝑔 𝜇−𝑗
, (13)
𝑗 𝑖 −𝑗 𝑖
self-containedanddonotshareanyparameters.However,weal-
lowthemtocommunicate.Inthiscase,ifwefollowanequivalent
communicationschemeasinthepreviouscase(i.e.,learningfrom whichisanabsurd,because 𝑗 doesnotshareparameterswith𝑖,
theincomingmessagesfromtheothers),wenowhavethat andthus 𝜇 𝑖 willneverbeupdated∀𝑖 ∈ {1,...,𝑁}.Thiscanbe
summarisedasthefollowingremark.
{𝑄 𝑖} 𝑖𝑁 =1={𝑓 𝑖(𝑜 𝑖,𝑚 −𝑖,𝑎 𝑖;𝜃 𝑖)} 𝑖𝑁 =1, (10)
where𝑚 correspondsonceagaintothemessagesfromallagents Remark1. Theparametersofacommunicationnetwork 𝜇 𝑖 of
−𝑖
except𝑖,thatareproducedbyaneuralnetworkdenotedbyafunc-
agent𝑖willneverbeupdatediffullyindependentlearnersthatdonot
tion𝑔 withparameters𝜇 sharetheparameters𝜃 𝑖 and𝜇 𝑖 learnonlyfromtheirobservationsand
𝑗 𝑗
incomingmessagesfromtheothers.
𝑚
−𝑖
={𝑔 𝑗(𝑜 𝑗;𝜇 𝑗)}𝑁
𝑗=1,𝑗≠𝑖
∧𝑚
𝑖
=𝑔 𝑖(𝑜 𝑖;𝜇 𝑖). (11)
As a solution to the problem stated in Remark 1 that occurs
SimilarlytoEq.(2),wecandefinethelossfunctionforthelearn-
inindependentcommunicationwithoutsharingparameterswhen
ingproblemas
updatingthecommunicationnetworks,weproposeinsteadthe
L(𝜃 𝑖,𝜇 −𝑖)=𝑟+𝛾𝑚𝑎𝑥 𝑎 𝑖′𝑄 𝑖(𝑜 𝑖′,𝑚′ −𝑖,𝑎 𝑖′;𝜃 𝑖−)−𝑄 𝑖(𝑜 𝑖,𝑚 −𝑖,𝑎 𝑖;𝜃 𝑖) followinglearningschemeforindependentcommunication:
−=𝑟
𝑓
𝑖+ (𝑜𝛾 𝑖,𝑚 {𝑔𝑎𝑥 𝑗(𝑎 𝑜𝑖′𝑓 𝑗𝑖 ;( 𝜇𝑜 𝑗𝑖′ ), }{
𝑗
𝑗𝑔
=
=𝑗 1𝑁( ,𝑜 𝑗≠′ 𝑗 𝑖; ,𝜇 𝑎− 𝑗 𝑖;) 𝜃} 𝑖𝑗 𝑗 )= = .1𝑁 ,𝑗≠𝑖,𝑎 𝑖′;𝜃 𝑖−)
(12)
{𝑄 𝑖} 𝑖𝑁 =1={𝑓 𝑖(𝑜 𝑖,𝑚 −𝑖,𝑚 𝑖,𝑎 𝑖;𝜃 𝑖)} 𝑖𝑁 =1, (14)
Becausenownetworksarenotshared,fromtheabovewecanwrite where𝑚 correspondsonceagaintothemessagesfromallagents
−𝑖
L𝑖(𝜃 𝑖,𝜇 −𝑖)≡L𝑖(𝑓 𝑖(·;𝜃 𝑖,𝜇 −𝑖),𝑔 −𝑖(·;𝜇 −𝑖)).Thus,hypothetically,in except𝑖,thatareproducedbyafunction𝑔
𝑗
withparameters𝜇 𝑗,inthesamewayasinEq.(11).L𝑖 cannowbewrittenas Algorithm1FullyIndependentCommunication
1: InitialiseemptyreplaybufferDandtheparametersofpolicy
L𝑖(𝜃 𝑖,𝜇 −𝑖,𝜇 𝑖)= andcommunicationnetworksforeachagent𝑖
=𝑟+𝛾𝑚𝑎𝑥 𝑎 𝑖′𝑄 𝑖(𝑜 𝑖′,𝑚′ −𝑖,𝑚 𝑖′,𝑎 𝑖′;𝜃 𝑖−)−𝑄 𝑖(𝑜 𝑖,𝑚 −𝑖,𝑚 𝑖,𝑎 𝑖;𝜃 𝑖) 2 3:
:
for wst hep il= ee0 pt io som da ex ii sm nu om tds ot nep es dd oo
=𝑟+𝛾𝑚𝑎𝑥 𝑎 𝑖′𝑓 𝑖(𝑜 𝑖′,{𝑔 𝑗(𝑜′ 𝑗;𝜇− 𝑗 )}𝑗 𝑗= =1𝑁 ,𝑗≠𝑖,𝑔 𝑖(𝑜 𝑖′;𝜇 𝑖−),𝑎 𝑖′;𝜃 𝑖−) 4: Executeaction𝑎=(𝑎1,...,𝑎 𝑁)andget𝑟 and𝑠′
−𝑓 𝑖(𝑜 𝑖,{𝑔 𝑗(𝑜 𝑗;𝜇 𝑗)}𝑗 𝑗= =1𝑁 ,𝑗≠𝑖,𝑔 𝑖(𝑜 𝑖;𝜇 𝑖),𝑎 𝑖;𝜃 𝑖), (15) 5 6:
:
endU wpd ha ilt eebufferDwith(𝑠,𝑎,𝑟,𝑠′)
7: ifDisnotemptythen
and now, we have that L𝑖(𝜃 𝑖,𝜇 −𝑖,𝜇 𝑖) ≡ 8: foreachepisodeein𝐵∼Ddo
L𝑖(𝑓 𝑖(·;𝜃 𝑖,𝜇 −𝑖,𝜇 𝑖),𝑔 −𝑖(·;𝜇 −𝑖),𝑔 𝑖(·;𝜇 𝑖)),andwecannowwritethe 9: foreachagentido ⊲Messagegeneration
rulesas 10: Send𝑚 𝑖 ←𝑐𝑜𝑚𝑚_𝑛𝑒𝑡𝑤𝑜𝑟𝑘(𝜏 𝑖)toothers≠𝑖
11: endfor
∇𝜃𝑖L𝑖 = 𝜕 𝜕L
𝑓
𝑖𝑖 𝜕𝜕 𝜃𝑓 𝑖
𝑖
+ 𝜕𝜕 𝑔L −𝑖
𝑖
𝜕 𝜕𝑔 𝜃− 𝑖𝑖 + 𝜕 𝜕L
𝑔
𝑖𝑖 𝜕𝜕𝑔 𝜃𝑖
𝑖
= 𝜕 𝜕L
𝑓
𝑖𝑖 𝜕𝜕 𝜃𝑓 𝑖 𝑖, (16) 1 12 3:
:
for 𝑚ea −c 𝑖h ←age inn ct oi md io ngmessagesfro⊲ mT or ta hin erin sg ≠s 𝑖tage
14: 𝑚𝑑 −𝑖 ←detachedcopyof𝑚 −𝑖
In∇ t𝜇 u𝑖 iL tiv𝑖 e= ly,𝜕 𝜕 tL h𝑓 𝑖 i𝑖 s𝜕 s𝜕 t𝜇𝑓 e𝑖 𝑖 p+ so𝜕𝜕 l𝑔 vL e−𝑖 s𝑖 t𝜕 h𝜕𝑔 e𝜇− 𝑖 p𝑖 ro+ bl𝜕 𝜕 eL 𝑔 m𝑖𝑖 s𝜕𝜕 ta𝑔 𝜇 t𝑖 𝑖 ed= in𝜕 𝜕L R𝑓 𝑖 e𝑖 m𝜕𝜕 a𝜇𝑓 𝑖 r𝑖 k+ 1.𝜕 𝜕 HL 𝑔 o𝑖𝑖 w𝜕𝜕 e𝑔 𝜇
(
v1𝑖 𝑖 e7.
r)
,
1 1 15 6 7: :
:
𝑄 C Δ𝑄a𝑖 l≡ 𝑖cu =𝑄 la 𝑟𝑖 t( e +[𝜏 t 𝛾a𝑖 𝑚, r𝑚 g 𝑎e𝑑 − 𝑥t 𝑎𝑖 s, 𝑖′𝑄𝑚 𝑄𝑖′𝑖 𝑖′] u −, s𝑎 i 𝑄𝑖 n) 𝑖gtargetnetworks
18: Updateall𝜃 𝑖 parameters
nowwhendoingthefinalrule,itisimpliedthat 19: Updatetargetnetworkseverytargetinterval
20: endfor
∇𝜇 −𝑖L𝑖 = 𝜕 𝜕L
𝑓
𝑖𝑖 𝜕𝜕 𝜇𝑓 −𝑖
𝑖
+ 𝜕𝜕 𝑔L −𝑖
𝑖
𝜕𝜕𝑔 𝜇− −𝑖
𝑖
+ 𝜕 𝜕L
𝑔
𝑖𝑖 𝜕𝜕 𝜇𝑔 −𝑖
𝑖
2 21 2:
:
ende in fdfor
=
𝜕L𝑖 𝜕𝑓 𝑖
+
𝜕L𝑖 𝜕𝑔 −𝑖
. (18)
23: endfor
𝜕𝑓 𝜕𝜇 𝜕𝑔 𝜕𝜇
𝑖 −𝑖 −𝑖 −𝑖
FromEq.(18),wenotetheexistenceofasecondproblem(thatis
independentofoursolutiontotheprobleminRemark1),since𝜇 3.2 Setting
−𝑖
wouldbeupdatedas Toconducttheexperimentsinthispaper,weconsiderasetofdiffer-
entalgorithmicconfigurationstoenabletheanalysisoftheeffects
𝜇 −𝑖 =𝜇 −𝑖 −𝛼(cid:18)𝜕 𝜕L
𝑓
𝑖𝑖 𝜕𝜕 𝜇𝑓 −𝑖
𝑖
+ 𝜕𝜕 𝑔L −𝑖
𝑖
𝜕𝜕𝑔 𝜇− −𝑖 𝑖(cid:19) , (19) o uf seco am lwm ayu sni inca dt eio pn enw deit nh ta Qn -d lew ari nth ino gut (Ip Qa Lra wm ite hte dr es eh par Qin -ng e. tW we oro kp st ,ato
s
describedinsection2.2,duetoitsknownsimplicity,allowingfora
for𝑁 times,causinglossesofgradientwhenpropagatingthrough
fairanalysisoftheimplicationsofdifferentconfigurationsinvolv-
thesamevaluesseveraltimes.Wecanwriteasecondimportant
ingcommunicationandvaryinglevelsofinformationexchange.
remark
Inaddition,itfacilitatestheanalysisoftheimpactofsharingand
Remark2. Iffullyindependentagentsthatdonotsharethepa- notsharingparameters.Notethatthescopeofthispaperisnotto
rameters𝜃 𝑖 and𝜇 𝑖 oftheirnetworkslearnfromthemessagesofthe proposecomplexcommunicationarchitectures,asinotherworks
suchas[7,16,20].Weconsiderthefollowingconfigurations:
others,thentheincomingmessageswillbeusedforbackpropagation
𝑁 times,causingproblemsinthecomputationalgraph. • PS+IQL:referstotheuseofIQLandtheagentssharethe
parametersofthesamenetwork(parametersharing).
ToovercometheproblemdescribedinRemark2,foreachagent
• NPS+IQL:referstotheuseofIQLandtheagentsdonot
𝑖,wedetach𝑚 fromthecomputationalgraph,ensuringthatall
−𝑖 sharetheparametersoftheirnetworks(noparametershar-
𝜃
𝑖
∧𝜇 𝑖,𝑖 ∈{1,...,𝑁}areupdatedexactlyonce,accordingto
ing).
𝜃
𝑖
=𝜃
𝑖
−𝛼𝜕 𝜕L 𝑓𝑖 𝜕𝜕 𝜃𝑓 𝑖 , asperEq.(16), (20) • P shS a+ rI iQ ngL+ paC rO amM eM te: rsre (f se ar ms eto asth Pe S+u Is Qe Lo )f ,bIQ utL nw owith wt eh ie nca lg ue dn et as
𝑖 𝑖
communicationmodulethatallowstheagentstobroadcast
messages;weprovidemoredetailsahead.
𝜇 𝑖 =𝜇 𝑖
−𝛼(cid:18)𝜕L𝑖 𝜕𝑓 𝑖
+
𝜕L𝑖 𝜕𝑔 𝑖(cid:19)
, asperEq.(17). (21) • NPS+IQL+COMM(thatcorrespondstotheproposedlearn-
𝜕𝑓 𝜕𝜇 𝜕𝑔 𝜕𝜇
𝑖 𝑖 𝑖 𝑖 ingschemein3.1):referstotheuseofIQLwiththeagents
Withthislearningscheme,whichcanbesummarizedbyEq.(14), notsharingparameters(sameasNPS+IQL),butnowthey
wesolvebothproblemsdescribedinRemarks1and2thatoccur useacommunicationmodule(sameasinPS+IQL+COMM)
infullyindependentlearningwithcommunicationandwithout thatallowstheagentstobroadcastmessages;notethathere,
parametersharing.Thisschemeallowsalltheparameterstobeup- sincewedonotconsiderparametersharing,thecommuni-
dated,enablinglearningwithcommunication.Intheresultssection cationshouldalsobeself-containedandeachagentshould
ahead,weshowthattheagentsthatfollowthisconfigurationare encode its own messages (with its own communication
stillabletolearncommunicationstrategies. network).Algorithm1describeshowthismethodworks.(a)PS+IQL (b)PS+IQL+COMM
(a)3s_vs_5z (b)PredatorPrey
Figure4:Environmentsusedintheexperiments.Ontheleft,
3s_vs_5z,ascenariofromtheSMACcollection[29],andon
therightaPredatorPreygame,where4predatorsmustcatch
twomovingprey[19].
Figure 3 shows the general architecture of the communica-
(c)NPS+IQL (d)NPS+IQL+COMM
tion approaches. The figure depicts directly the architecture of
NPS+IQL+COMM(asdescribedinsubsection3.1).Forbrevity,we
donotshowthearchitecturesoftheotherconfigurations,although Figure5:Winratesachievedbytheattemptedmethodsin
thesecanbededuceddirectlyfromthisfigure.Ifthecommunica- 3s_vs_5z.Thedashedline(optimum)representstheoptimal
tionmodulesareremoved,thenitrepresentsNPS+IQL,andthen valuethatcouldbeachievedbytheagents,i.e.,awinrateof
bothconfigurationscanbeextendedtothePScaseifweassume 1.Forcompleteness,weinclude,inthesupplementary,the
thenetworkstoberepresentationsofthesamenetwork.Nonethe- correspondingrewardsofthesewinrates.
less,inthesupplementary,westillincludethearchitecturewithout
communication.
whenapreyiscaught,andthereisasteppenaltyof−0.1×𝑁.
3.3 NetworkCapacity Mostimportantly,thereisateampunishmentfornon-cooperative
Inthispaper,tocarryoutadeeperanalysisofhowcommunication
behavioursof−0.75×𝑁 everytimeanagentattemptstocapture
apreyalone.ThispunishmentinPredatorPreytaskshasshown
affectslearning,wealsoinvestigatehowchangingthedimension
tobeimportantinpreviousworkstoevaluatetheimportanceof
numberofthehiddenlayersoftheagents’networksaffectstheir
communicationapproaches[2,16,20,40],sincethesepunishments
learningprocessinthepresenceofcommunication.Whenwein-
willmakethetaskconfusingfortheagents.
creasethesizeofthehiddenlayers,itmeansthatthepolicynetwork
Withtheresultsfromtheexperimentscarriedoutinthispaper,
canrepresentmoreinformationthanwhenthissizeisdecreased.
weintendtoanswerthefollowingquestions:
Thismeansthat,ifagentscanlearnusingnetworksthathavelower
capacity,theseagentscanstillperformunderlightweightnetworks. • (Q1)Howdoessharingparametersaffectlearningwhen
Importantly,thesenetworkscanstillextracttheneededinforma- comparedtonotsharingparameters?
tionfromtheinputstothenetworktolearnthetask,whilekeeping • (Q2)Canagentsstillcommunicatewhentheydonotshare
therepresentationsoftheinputsmuchsmaller.Thiscanbecrucial parameters?
whentherearecertaincomputationalconstraints.Atthesametime, • (Q3)Iscommunicationalwaysnecessary(sinceitisoften
itmeansthatlotsofcapacitymightbeuselessandthereiswasteof naivelyapplied,bringinguselesscomplexitytothelearning
informationifthesameinformationcouldberepresentedwitha networks)?
lowercapacity. • (Q4)Howiscommunicationaffectedbydifferentsizesof
networkcapacityforwhenwebothshareandnotshare
4 EXPERIMENTSANDRESULTS1 parameters?
Inthissection,wepresenttheresultsofthediscusseddifferentcon- Inthesupplementary,wedescribethehyperparametersusedin
figurations.Importantly,oneofthekeypointsoftheseexperiments theexperiments,alongsideotherimplementationdetails.
istoevaluatewhethertheproposedmethodinsection3.1enables
successfulcommunicationforindependentlearnerswhodonot 4.1 CommunicationwhenSharingorNot
shareparameters.Weevaluateourhypothesesintheenvironments SharingParameters
3s_vs_5zandPredatorPrey(Figures4(a)and4(b)).3s_vs_5zisone
Aswehavediscussedpreviouslyinthispaper,oneofthekeypoints
oftheenvironmentsoftheSMACcollection[29],where3stalker
hereistostudytheimplicationsofsharingandnotsharingparam-
units(meleeunits)mustdefeat5zealots(rangedunits).Intheused
eters.Fromthepresentedexperiments,weobservethatsharing
versionofPredatorPrey,4agentsmustcapture2randomlymoving
parameters naturally brings advantages to the agents’ learning
preyina7×7gridworld.Theteamrewardthatisreceivedis5×𝑁
process.Inlinewithotherworksthathavearguedthatsharing
1Codesusedcanbefoundathttps://github.com/rafaelmp2/marl-indep-comm parametersworksmostlyasawayofspeedinguplearningandcanbeproblematic.Figures5(a)and5(b)showtheperformances
ofPS+IQLandPS+IQL+COMMinthe3s_vs_5zenvironment,re-
spectively.Inthiscase,wecanseethatcommunicationdoesnot
seemtoplayanimportantroleinthetask.Inbothcases,theagents
achieveoptimalperformances,meaningthatcommunicationbe-
comesredundantandonlybringsoverheadtotheagents,sincean
additionalnetworkisbeingused(andsimplyincreasingthecapac-
ityoftheagentnetworkswouldbeenough).Whenparametersare
(a)PS+IQL (b)PS+IQL+COMM notshared,NPS+IQLandNPS+IQL+COMMinFigures5(c)and5(d),
weobserveasimilarscenario,wheretheeffectofcommunication
isalmostunnoticeable.Ontheotherhand,whenwelookatthe
secondtestedenvironmentPredatorPrey,wecanseethatcommu-
nicationhasaverystrongimpact(Figure6).Whencommunication
isnotused,theagentscannotsolvethistaskatall.However,when
theycommunicate,theycanbreakthebarrierofnegativereturns,
bothwhentheyshareanddonotshareparameters.Thus,inthis
case,communicationisnecessaryforlearning,asopposedtothe
(c)NPS+IQL (d)NPS+IQL+COMM previousenvironment.Theseresultsurgefortheneedtoanalyse
whencommunicationisneededornotbeforeapplyingitnaively
Figure6:Rewardsachievedbytheattemptedmethodsinthe
asitmightcauseawasteofcomputationalresources(Q3).
PredatorPreytask,withapunishmentfornon-cooperative
behavioursof-0.75. 4.2 TheImpactofDifferentNetworkCapacity
Inordertostudytheamountofnetworkcapacityneededforlearn-
ingandhowcommunicationhelpswiththisinformation,wehave
savingcomputationalresources,hereweenhanceinFigure5(a)
alsoexperimentedwithdifferentsizesforthehiddenlayersofthe
that,whentheagentsshareparameters,theysolve3s_vs_5zmuch
agentnetworkbutfixedthecommunicationnetworkhiddendi-
faster,despitenotsharingparameterswillalsosolvetheenviron-
mensionsto64.Asitwasexpected,whentheagentshaveahigher
ment,butwhiletakingmuchlonger.Whenwestudytheeffect
networkcapacity,theirperformanceisdrasticallyimproved.Onthe
ofcommunication,itispossibletoseethattheimpactofcommu-
otherhand,whenthenetworkcapacityisnotenough,itmighttake
nicationisalsomoreevidentwhenparametersareshared.This
themlongertolearnthetasks.Thismeansthatnetworkswitha
isduetothefactthatthefeedbackresultingfromthemessages
highernetworkcapacityhaveahighersampleefficiencyastheycan
producedandthepoliciesoftheagentsisbackpropagatedtothe
learnfasterwiththesameamountofsamples.Thus,wecanseethat,
samenetworks.Inthecaseofnotsharingparameters,theproblem
whilewithsize32theagentsstruggletolearnin3s_vs_5z,when
becomesmuchmorecomplexbecausethereisnolinkbetween
weincreaseto64and128theylearnmoreeasilyandmuchfaster.
networksofdifferentagents,andthusthecommunicationnetwork
Onceagain,thisverifiesbothforPS+IQLandNPS+IQL(Figures
doesnotreceivedirectfeedbackofthemessagesproducedbyitself.
5(a)and5(c)).
Instead,ithastounderstandtheimpactofthesemessagesbyunder-
Whenwelookatthecommunicationside(Figures5(b)and5(d)),
standinghowtheyareaffectingtheteamrewardglobally.However,
we observe once again that, in the case of 3s_vs_5z, following
despitethislimitationinherenttothefactthatparametersarenot
ourhypothesisthatcommunicationmaynotbealwaysnecessary,
shared,wecanstillseetheimprovementsofcommunicationin
addingcommunicationwillnotyieldanysignificantimprovement
fullyindependentlearnersinFigure5(d).Theseimprovementsof
oversimplychangingthesizesofthenetworkcapacity.However,
communicationforfullyindependentagentsareevenstrongerin
whenwelookatthePredatorPreycase(Figure6),wecanseethatthe
PredatorPrey(Figure6(d)),wheretheagentsmanagetoachieve
agentscanonlysolveitwithcommunicationand,whileincreasing
positiverewardsinthetask,asopposedtowhentheydonotcom-
thenetworkcapacitywithoutcommunicationdoesnothaveany
municate(Figure6(c)).Thisdemonstratesthatourframeworkfor
impact,increasingittogetherwithaddingcommunicationmakesa
communicationwhenparametersarenotsharedenableslearning
bigdifference.Herewecanseethatcommunicationalliedtothe
inthischallengingconfiguration.Thisisanimportantobservation
rightnetworksizewillleadtobetterperformance.Thus,byusing
whenweconsiderscenarioswhereparameterscannotbeshared
acommunicationnetwork,wecanspareresourcesregardingthe
(Q1,Q2).
standardagentnetworks.Insummary,notetheimportantremark
ItiscommontoseeintheliteratureoncommunicationinMARL
thatwhileincreasingthenetworkcapacitymightbeenoughfor
methodswherecommunicationisnaivelyappliedtoanarbitrary
somecases,whencommunicationisnecessarysimplyincreasing
numberofvariedenvironments.Inmostcases,themethodsusing
thenetworkcapacityisnotenough,andbothareneeded(Q4).
communicationendupperformingbetterinthetestedenviron-
ments,buttherearecaseswheretheperformancesachievedarevery
closetobenchmarkswithoutcommunicationbeingused.Insuch 5 RELATEDWORK
cases,oneoftenwonderswhethercommunicationisreallyneces- RecentworksinMARLhavedevelopedstrongmethodstotackle
sary,orifitisjustbringinguselessoverheadtothenetworks,which complexproblemsbasedonneuralnetworkarchitecturesthatcangivegoodvaluefunctionestimations.Tonameafew,QMIXpro- 6 CONCLUSION
posesawayofmixingindividuallyestimatedaction-valuefunctions CommunicationisstillanopenareaofresearchinMARL.While
intoaglobalfunction,fromwhichoptimaljointpoliciescanbe remarkable progress has been made in the field, there are still
extracted[27].VDN,orQTRAN[30,32]areothersuchmethods aspectsthathavenotbeeninvestigatedindetail.Inthiswork,we
thatlearnadifferenttypeofmixoftheseindividualfunctions.Ul- haveparticularlyshownthatthereareseveraladvantageswhen
timately,thegoalisalwaystolearnanoptimaljointaction-value agentsareallowedtoshareparametersofthelearningnetworks,
function.Thesemethodscomposeawayoflearningwithoutcom- assupportedbytheliterature.However,thisconfigurationmay
munication,i.e.,theagentsdonotdirectlybroadcastinformation notalwaysbefeasible(forinstance,inpracticalapplications),and
toeachother,althoughtheyhaveindirectaccesstothepoliciesof thusthereisaneedtodeepenthestudyoftheimplicationsofnot
theothersduetothemixingduringtraining. sharingparameters.Wehaveproposedawayofcommunicating
Whilethesemethodsarecommunication-free,latelymultiple inMARLforindependentlearnersthathavedistinctnetworksfor
otherworkshavetargetedcommunicationinMARLfromdiffer- policyandmessagegenerationanddonotshareparameters.The
entangles.In[8]theauthorsdemonstratehowacommunication resultsachievedshowthatagentscanstilllearncommunication
protocolcanbelearnedwhenthemessageisgeneratedbythepol- strategiesunderthissetting.
icyoftheagentsandtreatedasanaction.In[31]itisproposed Takingintoaccountthenetworkcapacitycanalsobeadeal
anothermethodofcommunicationthataggregatesthemessages breakerinlearning.Evenwhenconsideringcommunicationamong
ofalltheagentsintoacumulativemessagethatisthensent.In agents,differentnetworkcapacitiesmayleadtodifferentoutcomes.
[5]theauthorsproposeamethodthatminimisesinformationloss Infact,itisimportanttoevaluatewhethercommunicationorextra
ofmessageaggregation,butrequiresaccesstoglobalinformation network capacity in MARL is really needed before jumping to
during training. Other recent approaches have proposed strate- theseoptionsnaively,resultinginunnecessaryoverhead.Inthe
giesforcommunicatingbasedonspecificfactorssuchaswhom, future,weintendtoextendourfindingstootherdifferentscenarios
when,andwhattocommunicate[6,15].In[11],theauthorsusea anddigdeeperintothelearningprocessofcommunicationthat
centralagentthatcontrolsmessagesandlearnswhatneedstobe resultsfromtheproposedlearningschemeforfullyindependent
senttotheagents.Othertypesofcommunicationhavebeenused communication.
togetherwithmethodsthatdonotinitiallyusecommunication.
Forinstance,methodssuchas[20,40]proposecommunicationar- REFERENCES
chitecturesthatcanbeusedtoimprovetheperformanceofother
[1] SushrutBhalla,SriramG.Subramanian,andMarkCrowley.2019. Training
non-communicativeapproaches.Amongallthementionedmeth- CooperativeAgentsforMulti-AgentReinforcementLearning.InProceedingsof
ods,notethat,duringthecommunicationprocess,thereisalways the18thInternationalConferenceonAutonomousAgentsandMultiAgentSystems
(MontrealQC,Canada)(AAMAS’19).InternationalFoundationforAutonomous
somesortofinformationexchange(whensharingmessages),and AgentsandMultiagentSystems,Richland,SC,1826–1828.
thereisalwayssomesortoflossofinformation(duringencoding [2] WendelinBöhmer,VitalyKurin,andShimonWhiteson.2019.DeepCoordination
oraggregation).Thistypeofinformationconditioningisstudied Graphs.CoRRabs/1910.00091(2019).arXiv:1910.00091 http://arxiv.org/abs/1910.
00091
inworkssuchas[38],wheretheauthorsproposeamethodthat [3] Kwang-ChengChenandHsuan-ManHung.2019.WirelessRoboticCommunica-
canlearnmessagesthatfollowcertainbandwidthconstraints.In tionforCollaborativeMulti-AgentSystems.InICC2019-2019IEEEInternational
[17],theauthorsalsouseadrop-outstrategybasedontheaverage
ConferenceonCommunications(ICC).1–7. https://doi.org/10.1109/ICC.2019.
8761140
weightsofseveralmessagenetworksduringtraining. [4] FilipposChristianos,GeorgiosPapoudakis,ArrasyRahman,andStefanoV.Al-
Aspectssuchastheimplicationsofcommunicatingunderlimited brecht.2021.ScalingMulti-AgentReinforcementLearningwithSelectiveParam-
eterSharing. arXiv:2102.07475[cs.MA]
bandwidthchannels,orthemeasurementofthequalityofcommu- [5] TianshuChu,SandeepChinchali,andSachinKatti.2020.Multi-agentreinforce-
nicationhavebeenstudiedinworkssuchas[21,28,35,38].Instead, mentlearningfornetworkedsystemcontrol. arXivpreprintarXiv:2004.01339
inthisworkweintendtoanalysewhethercommunicationisreally (2020).
[6] AbhishekDas,ThéophileGervet,JoshuaRomoff,DhruvBatra,DeviParikh,
necessarywhentheagentnetworksalreadyhaveenoughcapacity. MichaelRabbat,andJoellePineau.2019.TarMAC:TargetedMulti-AgentCommu-
Inworkssuchas[14]itisshownhowincreasingthenetworkca- nication.InProceedingsofthe36thInternationalConferenceonMachineLearning,
Vol.97.1538–1546. arXiv:1810.11187.
pacitycanimprovelearning,buthowdoesitaffectcommunication
[7] ZiluoDing,WeixinHong,LiwenZhu,TiejunHuang,andZongqingLu.2022.
methods?Furthermore,notethatin[36]itisanalysedcommunica- SequentialCommunicationinMulti-AgentReinforcementLearning. https:
tionamongcriticsthatapproximateadvantageutilities.However, //openreview.net/forum?id=xzeGP-PtPMI
[8] JakobN.Foerster,YannisM.Assael,NandodeFreitas,andShimonWhiteson.
theauthorsdonotfocusondirectagent-to-agentcommunication. 2016.LearningtoCommunicatewithDeepMulti-AgentReinforcementLearning.
Importantly,wealsonotethat,whilein[8]theauthorshavebriefly InAdvancesinNeuralInformationProcessingSystems,Vol.29. arXiv:1605.06676.
mentionedcommunicationforindependentagents,itisstillnot [9] AndreiFurda,LaurentBouraoui,MichelParent,andLjuboVlacic.2010. Im-
provingSafetyforDriverlessCityVehicles:Real-TimeCommunicationand
clearhowcommunicationcanbeachievedinfullyindependent DecisionMaking.In2010IEEE71stVehicularTechnologyConference.1–5. https:
agentsthatdonotshareparametersincomplexscenarios.Sharing //doi.org/10.1109/VETECS.2010.5494179
[10] JayeshK.Gupta,MaximEgorov,andMykelKochenderfer.2017.Cooperative
parametersisoftentakenforgrantedinMARL,leaningresearch
Multi-agentControlUsingDeepReinforcementLearning.InAutonomousAgents
towardsoblivionregardingtheunderstandingoftheimplications andMultiagentSystems,GitaSukthankarandJuanA.Rodriguez-Aguilar(Eds.).
ofeithersharingornotsharingparameters,asdiscussedinworks SpringerInternationalPublishing,Cham,66–83.
[11] NikunjGupta,GSrinivasaraghavan,SwarupMohalik,NishantKumar,and
like[4]. MatthewETaylor.2023.Hammer:Multi-levelcoordinationofreinforcement
learningagentsvialearnedmessaging.NeuralComputingandApplications(2023),
1–16.
[12] MatthewHausknechtandPeterStone.2017. DeepRecurrentQ-Learningfor
PartiallyObservableMDPs. arXiv:1507.06527[cs.LG][13] OmarHouidi,SihemBakri,DjamalZeghlache,JulienLesca,PhamTranAnh [34] MingTan.1993.Multi-AgentReinforcementLearning:Independentvs.Coop-
Quang,JérémieLeguay,andPaoloMedagliani.2023.AMAC:Attention-based erativeAgents.InProceedingsoftheTenthInternationalConferenceonMachine
Multi-AgentCooperationforSmartLoadBalancing.InNOMS2023-2023IEEE/IFIP Learning.330–337.
NetworkOperationsandManagementSymposium.1–7. https://doi.org/10.1109/ [35] MycalTucker,RogerLevy,JulieAShah,andNogaZaslavsky.2022. Trading
NOMS56928.2023.10154214 offUtility,Informativeness,andComplexityinEmergentCommunication.In
[14] JianHu,SiyangJiang,SethAustinHarding,HaibinWu,andShihweiLiao. AdvancesinNeuralInformationProcessingSystems,S.Koyejo,S.Mohamed,
2023. RethinkingtheImplementationTricksandMonotonicityConstraintin A.Agarwal,D.Belgrave,K.Cho,andA.Oh(Eds.),Vol.35.CurranAssociates,
CooperativeMulti-AgentReinforcementLearning. arXiv:2102.03479[cs.LG] Inc.,22214–22228. https://proceedings.neurips.cc/paper_files/paper/2022/file/
[15] JiechuanJiangandZongqingLu.2018.LearningAttentionalCommunication 8bb5f66371c7e4cbf6c223162c62c0f4-Paper-Conference.pdf
forMulti-AgentCooperation.arXiv:1805.07733[cs](Nov.2018). http://arxiv.org/ [36] SimonVanneste,AstridVanneste,TomDeSchepper,SiegfriedMercelis,Peter
abs/1805.07733arXiv:1805.07733. Hellinckx,andKevinMets.2023.Distributedcriticsusingcounterfactualvalue
[16] DaewooKim,SangwooMoon,DavidHostallero,WanJuKang,TaeyoungLee, decompositioninmulti-agentreinforcementlearning.InAdaptiveandLearning
KyunghwanSon,andYungYi.2019.LearningtoScheduleCommunicationin AgentsWorkshop(ALA),collocatedwithAAMAS,29-30May,2023,London,UK.
Multi-agentReinforcementLearning. arXiv:1902.01554[cs.AI] 1–9.
[17] WoojunKim,MyungsikCho,andYoungchulSung.2019. Message-dropout: [37] JianhaoWang,ZhizhouRen,TerryLiu,YangYu,andChongjieZhang.2021.
Anefficienttrainingmethodformulti-agentdeepreinforcementlearning.In QPLEX:DuplexDuelingMulti-AgentQ-Learning. arXiv:2008.01062[cs.LG]
ProceedingsoftheAAAIconferenceonartificialintelligence,Vol.33.6079–6086. [38] RundongWang,XuHe,RunshengYu,WeiQiu,BoAn,andZinoviRabinovich.
[18] WoojunKim,JongeuiPark,andYoungchulSung.2021.CommunicationinMulti- 2019.LearningEfficientMulti-agentCommunication:AnInformationBottleneck
AgentReinforcementLearning:IntentionSharing.InInternationalConference Approach.CoRRabs/1911.06992(2019).arXiv:1911.06992 http://arxiv.org/abs/
onLearningRepresentations. https://openreview.net/forum?id=qpsl2dR9twy 1911.06992
[19] AnuragKoul.2019.ma-gym:Collectionofmulti-agentenvironmentsbasedon [39] AnnieWong,ThomasBäck,AnnaVKononova,andAskePlaat.2023.Deepmul-
OpenAIgym.https://github.com/koulanurag/ma-gym. tiagentreinforcementlearning:Challengesanddirections.ArtificialIntelligence
[20] ZeyangLiu,LipengWan,Xuesui,KewuSun,andXuguangLan.2021.Multi-Agent Review56,6(2023),5023–5056.
IntentionSharingviaLeader-FollowerForest.TechnicalReportarXiv:2112.01078. [40] Sai Qian Zhang, Qi Zhang, and Jieyu Lin. 2019. Efficient Communica-
arXiv. http://arxiv.org/abs/2112.01078arXiv:2112.01078[cs]type:article. tion in Multi-Agent Reinforcement Learning via Variance Based Control.
[21] RyanLowe,JakobFoerster,Y.-LanBoureau,JoellePineau,andYannDauphin. arXiv:1909.02682[cs.LG]
2019.OnthePitfallsofMeasuringEmergentCommunication.TechnicalReport
arXiv:1903.05168.arXiv. http://arxiv.org/abs/1903.05168arXiv:1903.05168[cs,
stat]type:article.
[22] HangyuMao,ZhiboGong,ZhengchaoZhang,ZhenXiao,andYanNi.2019.
LearningMulti-agentCommunicationunderLimited-bandwidthRestrictionfor
InternetPacketRouting. arXiv:1903.05561[cs.MA]
[23] FedericoMason,FedericoChiariotti,AndreaZanella,andPetarPopovski.2023.
Multi-AgentReinforcementLearningforPragmaticCommunicationandControl.
arXivpreprintarXiv:2302.14399(2023).
[24] VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiA.Rusu,JoelVeness,
MarcG.Bellemare,AlexGraves,MartinA.Riedmiller,AndreasKirkebyFidjeland,
GeorgOstrovski,StigPetersen,CharlieBeattie,AmirSadik,IoannisAntonoglou,
HelenKing,DharshanKumaran,DaanWierstra,ShaneLegg,andDemisHassabis.
2015.Human-levelcontrolthroughdeepreinforcementlearning.Nature518
(2015),529–533. https://api.semanticscholar.org/CorpusID:205242740
[25] FransA.OliehoekandChristopherAmato.2016. AConciseIntroductionto
DecentralizedPOMDPs.Springer.
[26] MarieOssenkopf,MackenzieJorgensen,andKurtGeihs.2019.HierarchicalMulti-
AgentDeepReinforcementLearningtoDevelopLong-TermCoordination.In
Proceedingsofthe34thACM/SIGAPPSymposiumonAppliedComputing(Limassol,
Cyprus)(SAC’19).AssociationforComputingMachinery,NewYork,NY,USA,
922–929. https://doi.org/10.1145/3297280.3297371
[27] TabishRashid,MikayelSamvelyan,ChristianSchroederdeWitt,GregoryFar-
quhar,JakobFoerster,andShimonWhiteson.2018. QMIX:MonotonicValue
FunctionFactorisationforDeepMulti-AgentReinforcementLearning.InProceed-
ingsofthe35thInternationalConferenceonMachineLearning,Vol.80.4295–4304.
arXiv:1803.11485.
[28] CinjonResnick,AbhinavGupta,JakobFoerster,AndrewM.Dai,andKyunghyun
Cho.2020.Capacity,Bandwidth,andCompositionalityinEmergentLanguage
Learning.arXiv:1910.11424[cs,stat](April2020). http://arxiv.org/abs/1910.11424
arXiv:1910.11424.
[29] MikayelSamvelyan,TabishRashid,ChristianSchröderdeWitt,GregoryFar-
quhar,NantasNardelli,TimG.J.Rudner,Chia-ManHung,PhilipH.S.Torr,
JakobN.Foerster,andShimonWhiteson.2019.TheStarCraftMulti-AgentChal-
lenge.CoRRabs/1902.04043(2019).arXiv:1902.04043 http://arxiv.org/abs/1902.
04043
[30] KyunghwanSon,DaewooKim,WanJuKang,DavidHostallero,andYungYi.
2019.QTRAN:LearningtoFactorizewithTransformationforCooperativeMulti-
AgentReinforcementLearning.CoRRabs/1905.05408(2019).arXiv:1905.05408
http://arxiv.org/abs/1905.05408
[31] SainbayarSukhbaatar,arthurszlam,andRobFergus.2016.LearningMultiagent
CommunicationwithBackpropagation.InProceedingsofthe30thInternational
ConferenceonNeuralInformationProcessingSystems,D.D.Lee,M.Sugiyama,
U.V.Luxburg,I.Guyon,andR.Garnett(Eds.).2252–2260.
[32] PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,Vini-
ciusZambaldi,MaxJaderberg,MarcLanctot,NicolasSonnerat,JoelZ.Leibo,Karl
Tuyls,andThoreGraepel.2017.Value-DecompositionNetworksForCooperative
Multi-AgentLearning. arXiv:1706.05296[cs.AI]
[33] ArdiTampuu,TambetMatiisen,DorianKodelja,IlyaKuzovkin,KristjanKorjus,
JuhanAru,JaanAru,andRaulVicente.2015. MultiagentCooperationand
CompetitionwithDeepReinforcementLearning. arXiv:1511.08779[cs.AI]A SMACREWARDPLOTS
Forcompletenesspurposes,inFigure7wealsoshowtheresultingrewardsfor3s_vs_5zthatcorrespondtothewinratesintheexperiments
sectionofthemainpaper.Wecanseethattheplotsoftherewardstellthesamestoryasthecorrespondingwinrates.However,insome
cases,therewardsforsmallernetworkcapacitiesareclosetotheotherswithhighercapacity,meaningthattheseagentswithlowercapacity
arelikelytobeclosetobetterperformances.Forinstance,inFigure7(a)wecanseethattheperformancewithsize32isgettingverycloseto
theonewith64.AlsoinFigure7(d),wecanseethat,attheendoftraining,theagentswithsize32areveryclosetotheoneswithsize64and
eventheoneswithsize128.Thissuggeststhatitislikelythattheywouldreachthesameoptimalperformanceiftheyweretrainedfor
longer.Thisleadstotheinsightthatagentswithlowercapacitywillrequiremoretrainingtime,buttheymightreachthesameoptimal
performanceiftrainedforlongenough.Theseobservationscannotbeeasilyseenintheplotsofthewinrates.
(a)PS+IQL (b)PS+IQL+COMM (c)NPS+IQL (d)NPS+IQL+COMM
Figure7:Rewardsachievedbytheattemptedmethodsin3s_vs_5z.Thedashedline(optimum)representsthemaximumvalue
achievedamongallthemethodsinthisscenario.Itisplottedsimplyforbettervisualisationpurposes.
B ARCHITECTUREOFINDEPENDENTLEARNINGWITHOUTCOMMUNICATION
Figure8:ArchitectureforNPS+IQLasdescribedinthemaintextofthispaper.WhencomparedtoNPS+IQL+COMM,nowthere
isnocommunicationnetworkandtheactionsoftheagentsaretakenbasedonlyontheobservations,andnotonthemessages
aswell,asshownforNPS+IQL+COMMinsection3.1inthemainpaper.
Forcomparisonwiththearchitectureinsection3.1ofthemainpaper,inFigure8weshowthearchitectureofindependentlearning
withoutparametersharingthatwasusedintheexperimentsinthispaper.Intuitively,independentlearningwithparametersharingcanbe
deducedfromthisarchitecture,ifwelookatthenetworksoftheagentsasiftheyrepresentthesamenetworkthatreceivesanadditional
agentIDasinput.
C HYPERPARAMETERSANDIMPLEMENTATIONDETAILS
Intheexperimentspresentedinthemainpaper,alltheresultsrepresenttheaverageof3independentruns.Inourconfigurationofdeep
independentQ-learning,alltheagentsarecontrolledbyrecurrentdeepneuralnetworkswithaGatedRecurrentUnit(GRU)cell.Thedefault
widthoftheGRUis64.However,asdescribedintheexperiments,weincreaseordecreasethisvaluetoconductouranalysis.Weuseboth
parametersharingandnoparametersharingintheexperiments.Whenparametersarenotshared,eachindividualagentiscontrolledbya
separatenetwork,liketheonedescribed.Intheexperimentswithcommunication,weuseacommunicationneuralnetworkbasedonlinear
transformationsthatencodestheobservationsoftheagents.Wefixthedimensionofthehiddenlayersto64.Importantly,whenweuseparametersharing,anagentIDisincludedintheinputsoftheagents.However,thisisnotusedanymorewhenparametersarenotshared,
sinceitbecomesredundant.
Theexploration-exploitationtradeoffoftheagentsismadeaccordingtotheepsilon-greedymethod,withaninitialepsilonof𝜖 =1that
annealsdowntotaminimumof0.05throughout50000trainingepisodes.WeusetheRMSpropoptimisertotrainallthenetworks,witha
learningrate𝛼 =5×10−4.Thediscountfactorusedis𝛾 =0.99,andthemaximumsizeofthereplaybufferis5000,fromwhichminibatches
of32episodesaresampled.Every200trainingepisodes,thetargetnetworksareupdated.