Learning Neural Radiance Fields of Forest Structure for
Scalable and Fine Monitoring
JuanCastorena1
LosAlamosNationalLaboratory,LosAlamos,NM,USA,48124
jcastorena@lanl.gov
Abstract. This work leverages neural radiance fields and remote sensing for
forestry applications. Here, we show neural radiance fields offer a wide range
ofpossibilitiestoimproveuponexistingremotesensingmethodsinforestmon-
itoring.Wepresentexperimentsthatdemonstratetheirpotentialto:(1)express
finefeaturesofforest3Dstructure,(2)fuseavailableremotesensingmodalities
and(3),improveupon3Dstructurederivedforestmetrics.Altogether,theseprop-
ertiesmakeneuralfieldsanattractivecomputationaltoolwithgreatpotentialto
furtheradvancethescalabilityandaccuracyofforestmonitoringprograms.
Keywords: Neural radiance fields · Remote Sensing · LiDAR · ALS · TLS ·
Photogrammetry·Forestry
1 Introduction
Withapproximatelyfourbillionhectarescoveringaround 31%oftheEarth’slandarea
[7], forests play a vital role in our ecosystem. The increasing demand for tools that
helpmaintainabalancedandhealthyforestecosystemischallengingduetothecom-
plex nature of various factors, including resilience against disease and fire, as well as
overallforesthealthandbiodiversity[25].Activeresearchfocusesonthedevelopment
of monitoring methods that synergistically collect comprehensive information about
forestecosystemsandutilizeittoanalyzeandgeneratepredictivemodelsofthecharac-
terizingfactors.Thesemethodsshouldideallybecapableofeffectivelyandefficiently
copewiththedynamicchangesovertimeandheterogeneity.Thegoalistoprovidethe
toolswithsuchpropertiesforimprovedplanning,management,analysis,andmoreef-
fectivedecision-makingprocesses[1].Traditionaltoolsforforestmonitoring,suchas
nationalforestinventory(NFI)plots,utilizespatialsamplingandestimationtechniques
to quantify forest cover, growing stock volume, biomass, carbon balance, and various
treemetrics(e.g.,diameteratbreastheight,crownwidth,height)[23].However,these
surveyingmethodsconsistofmanualfieldsampling,whichtendstointroducebiasand
poses challenges in terms of reproducibility. Moreover, this approach is economically
costlyandtime-consuming,especiallywhendealingwithlargespatialextents.
Recent advancements, driven by the integration of remote sensing, geographic in-
formationandmoderncomputationalmethods,havecontributedtothedevelopmentof
moreefficient,cost/timeeffective,andreproducibleecosystemcharacterizations.These
advancementshaveunveiledthepotentialofhighlyrefinedanddetailedmodelsof3D
4202
naJ
62
]VC.sc[
1v92051.1042:viXra2 J.Castorena
forest structure. Traditionally, the metrics collected through standard forest inventory
plot surveys have been utilized as critical inputs in applications in forest health [15],
wood harvesting [13], habitat monitoring [24], and fire modeling [16]. The efficacy
of these metrics relies in their ability to quantitatively represent the full forest’s 3D
structure including its vertical resolution: from the ground, sub-canopy to the canopy
structure. Among the most popular remote sensing techniques, airborne LiDAR scan-
ning (ALS) has gained widespread interest due to its ability to rapidly collect precise
3D structural information over large regional extents [6]. Airborne LiDAR, equipped
with accurate position sensors like RTK (Real-Time Kinematic), enables large-scale
mappingfromhighaltitudesatspatialresolutionsrangingfrom5-20pointspersquare
meter. It has proven effective in retrieving important factors in forest inventory plots
[11]. However, it faces challenges in dense areas where the tree canopy obstructs the
LiDARsignal,evenwithitsadvancedfull-waveform-basedtechnology.In-situterres-
trial laser scanning (TLS) on the other hand provides detailed vertical 3D resolution
from the ground, sub-canopy and canopy structure informing about individual trees,
shrubs, ground surface, and near-ground vegetation at even higher spatial resolutions
[10]. Recent work by [20] has demonstrated the efficiency and efficacy of ecosystem
monitoringusingsinglescanin-situTLS.Thetechnologicaladvancesofsuchmodels
includenewcapabilitiesforrapidlyextractinghighlydetailedquantifiablepredictions
of vegetation attributes and treatment effects in near surface, sub-canopy and canopy
composition. However, these models have only been deployed across spatial domains
of a few tens of meters in radius due to the existing inherited limitations of TLS spa-
tialcoverage[20].Ontheothersideofthespectrum,imagebasedphotogrammetryfor
3D structure extraction offers the potential of being both scalable and the most cost
efficient. Existing computational methods for the extraction of 3D structure in forest
ecosystems, however, have not been as efficient. Aerial photogrammetry methods re-
sultin3Dstructurethatcontainsverylimitedstructuralinformationalongthevertical
dimensionandhaveencounteredoutputspatialresolutionsthatcanbeatmostonlyon
parwiththosefromALS[25].
Ourcontributionseekstofusetheexperimentalfindingsacrossremotesensingdo-
mains in forestry; from broad-scale to in-situ sensing sources. The goal is the ability
to achieve the performance quality of in-situ sources (e.g., TLS) in the extraction of
3D forest structure at the scalability of broad sources (e.g., ALS, aerial-imagery). We
propose the use of neural radiance field (NERF) representations [17] which account
for the origin and direction of radiance to determine highly detailed 3D structure via
view-consistency.Weobservethatsuchrepresentationsenableboththefinedescription
of forest 3D structure and also the fusion of multi-view multi-modal sensing sources.
Demonstrated experiments on real multi-view RGB imagery, ALS and TLS validate
the fine resolution capabilities of such representations as applied to forests. In addi-
tion, the performance found in our experiments of 3D structure derived forest factor
metrics demonstrate the potential of neural fields to improve upon the existing forest
monitoring programs. To the best of our knowledge, the demonstrations conducted in
thisresearch,namely,theapplicationofneuralfieldsfor3Dsensinginforestry,isnovel
andhasnotbeenshownpreviously.Inthefollowing,Sec.2providesabriefoverviewof
neuralfields.Sec.3includesexperimentsillustratingthefeasibilityofneuralfieldstoLearningNeuralRadianceFieldsofForestStructureforScalableandFineMonitoring 3
representfine3DstructureofforestrywhileSection4demonstratestheeffectivenessof
fusingNERFwithLiDARdatabyenforcingLiDARpointcloudpriors.Finally,Section
5 presents results that show the efficacy of NERF extracted 3D structure for deriving
forestfactormetrics,whichareofprimesignificancetoforestmanagersformonitoring.
2 Background
2.1 NeuralRadianceFields
Theideaofneuralradiancefields(NERF)isbasedonclassicalraytracingofvolume
densities [12]. Under this framework, each pixel comprising an image is represented
by a ray of light casted onto the scene. The ray of light is described by r(t) = o+
td with origin o ∈ R3, unit ℓ -norm length direction d ∈ R3 (i.e., ∥d∥ = 1) and
2 2
independentvariablet∈Rrepresentingarelativedistance.Theparametersofeachray
canbecomputedthroughthecameraintrinsicmatrixKwithinverseK−1,the6Dpose
transformationmatrixT ofimagemasinEq.(1)
m→0
(cid:18) d′ (cid:19)
(o,d)= T(4) , with
m→0 ∥d′∥
ℓ2
 u′
d′ =T−1 K−1 v′ −T(4) (1)
m→0 m→0
1
whereu′,v′areverticalandhorizontalthepixellocationswithintheimageandthesub-
script(i)denotesthei-thcolumnofamatrix.Castingraysr∈Rintothescenefromall
pixelsacrossallmulti-viewimagesprovidesinformationofintersectingraysthatcanbe
exploitedtoinfer3Dscenestructure.Suchinformationconsistsonsamplingalongaray
atdistancesamples{t }M anddetermineateachsampleifthecolorc ∈ [0,..,255]3
i i=1 i
of the ray coincides with those from overlapping rays. If it does not coincide then it
islikelythatthemediumfoundatthatspecificdistancesampleistransparentwhereas
theoppositemeansanopaquemediumispresent.Withsuchinformation,compositing
colorcanbeexpressedasafunctionofrayrasinEq.(2)by:
 
  
Cˆ(r)=(cid:88)N 

i (cid:89)−1
exp(−σ jδ j)(1−exp(−σ iδ i))c
i


(2)
 (cid:124) (cid:123)(cid:122) (cid:125) 
i=1 j=1 opacity 
(cid:124) (cid:123)(cid:122) (cid:125)
transparencysofar
whereσ ∈Randδ =t −t arethevolumedensitiesanddifferentialtimestepsat
i i i+1 i
sampleindexedbyi,respectively.InEq.(2)thefirstterminthesummationrepresents
thetransparentsamplessofarwhilethesecondtermisanopaquemediumofcolorc
i
presentatsamplei.Reconstructingascenein3Dcanthenbeposedastheproblemof
findingthesamplelocationst whereeachrayintersectsanopaquemedium(i.e.,where
i
eachraystops)forallrayscastedintothescene.Thoseintersectionsarelikelytooc-
curatthesamplelocationswherethevolumedensitiesaremaximized;inotherwords,4 J.Castorena
wheret = argmax {σ}.Accumulating,allrayscastedintothesceneandestimating
i i
thelocationst ’swherevolumedensityismaximizedoverallrays,rendersthe3Dge-
i
ometry of the scene. The number of rays required per scene is an open question; the
interested reader can go to [3] where a similar problem but for LiDAR sensing deter-
minesthenumberofpulsesrequiredfor3Dreconstructiondependingonaquantifiable
measureofscenecomplexity.
TheprobleminEq.(2)issolvedbylearningthevolumedensitiesthatbestexplains
image pixel color in a 3D consistent way. Learning can be done through a multilayer
perceptron(MLP)byrewritingEq.(2)asinEq.(3)as:
N
Cˆ(r)=(cid:88)
w c (3)
i i
i=1
wheretheweightsw ∈ RN encodetransparencyoropacityoftheN samplesalonga
ray and c is its associated pixel color. Learning weights is performed in an unsuper-
i
visedfashionthroughtheoptimizationofalossfunctionusingatrainingsetofM pairs
ofmulti-view RGBimages andits corresponding6D poses {(y ,T )}M , respec-
m m m=1
tively. This loss function f : RL → R is the average ℓ -norm error between ground
2
truthcolorandestimationbycompositingdescribedasinEq.(4):
L (Θ)=
(cid:88)(cid:104)
∥C(r)−Cˆ(r,Θ)∥2
(cid:105)
(4)
C ℓ2
r∈R
Optimizationbyback-propagationyieldstheweightsthatgraduallyimprovesuponthe
estimation of the volumedensities. Other important parameters of NERF are distance
zˆ(r) which can be defined using the same weights from Eq.(2) but here expressed in
termsofdistanceas:
N N
(cid:88) (cid:88)
zˆ(r)= ω t , sˆ(r)2 = ω (t −zˆ(r))2 (5)
i i i i
i=1 i=1
andsˆ(r)definedasthestandarddeviationofdistance.Onekeyissueaffecting3Dre-
construction resolution is on the way samples {t }N for each ray r ∈ R are drawn.
i i=1
AsmallnumberofsamplesN resultsinlowresolutionanderroneousrayintersection
estimationswhilesamplingvastlyresultsinmuchhighercomputationalcomplexities.
To balance this trade-off, the work in [17] uses two networks one at low-resolution
tocoarselysamplethe3Dsceneandanotherfine-resolutiononeusedsubsequentlyto
morefinelysampleonlyatlocationslikelycontainingthescene.
3 Areneuralfieldscapableofextracting3Dstructureinforestry?
Thehighcapacityofdeeplearning(DL)modelstoexpressdatadistributionswithhigh
fidelityanddiversityoffersapromisingavenuetomodelheterogenous3Dforeststruc-
turesinfinedetail.ThespecificconfigurationoftheselectedDLmodelaimstoprovide
a representation that naturally allows the combination of data from multiple sensing
modalitiesandview-points.Neuralfields[17]undertheDLrubrichaveproventobea
highlyeffectivecomputationalapproachforaddressingsuchproblems.However,their
applicationhasbeenonlydemonstratedforindoorandurbanenvironments.LearningNeuralRadianceFieldsofForestStructureforScalableandFineMonitoring 5
3.1 TerrestrialImagery.
Expanding on the findings of neural fields in man-made environments, we conducted
additionalexperimentstodemonstrateitseffectivenessinrepresentingfine3Dstructure
details in forest ecosystems. Figs. 1 and 2 shows the extracted 3D structure of a Pon-
derosapinetreeinNewMexico,capturedusingstandard12-megapixelcameraphone
images collected along an elliptical trajectory around the tree. Fig. 1a shows a few of
theinputexampleterrestrialmulti-viewRGBimagescollected.Figs.1band1cpresents
theimagesnapshottrajectoryrepresentedasredrectangles,alongwithtwo3Dstructure
viewsderivedfrom atraditionalstructurefrommotion (SFM)method[22]appliedto
themulti-viewinputimages.Notethatthelevelofspatialvariabilitydetailprovidedby
thisSFMmethodissignificantlylowconsideringtheresolutionprovidedbythesetof
inputimages.
(a)TerrestrialRGBmulti-viewimageryofPonderosaPineTree.
(b)SFMreconstructionview-1 (c)SFMreconstructionview-2
Fig.1: Even though SFM reconstruction is capable of extracting the 3D structure of
tree, its recontruction suffers from sparsity. Such sparsity limits the spatial variability
ofstructurethatcanbecapturedthorughsuchmodels.
Can the representational power of modern AI models do better than classical 3D
structureextractionmethodsinForestry?Weextract3Dstructurebyneuralfieldsusing
thesameinputimagesandobtaintheresultshowninFig.2.Notethatmuchfinerspatial
variabilitydetailscanberesolvedacrossthe3Dstructureincludingtheground,trunk,
branches,leaves.EvenfinewoodydebrisasshowninFigs2c-.2dand,barkcanbere-6 J.Castorena
solvedasshowninFigs.2e-2fincontrasttotheresultoftraditionalSFMinFig.1.Note
thatevenpointscomingfromimagesdegradedbysun-glareasshowninFig.1alanded
inthetreewithinreasonabledistancesasshowninFig.2a,thisissignificantspecially
consideringtheseverityoftheglareeffectspresentinthe2DRGBimages.Ingeneral,
terrestrialmulti-viewimagerybasedNERFcanbeusedtoextractfine3Dspatialreso-
lutionalongtheverticaldimensionofatreestandwithalevelofdetailsimilartoTLS
andwiththeadditionaladvantageofprovidingcolorforevery3Dpointestimate.
(a)Sideviewsillustratinghigh3Dspatialdetailalongtheverticaltreestem
(b)Tree3Dstructureview-5 (c)Fine3Dresolutionofforestfloorstructure
(d)Forestfloor3Dstructure (e)Treetrunkview-1 (f)Treetrunkview-2
Fig.2: Neural field models are capable of extracting fine 3D structure from terrestrial
multi-viewimagesinforestry.Reconstructionsdemonstratetheirpotentialtorepresent
finescalevariabilityinheterogeneousforestecosystems.LearningNeuralRadianceFieldsofForestStructureforScalableandFineMonitoring 7
4 Neural Radiance Fields: A framework for remote sensing fusion
inforestry
Neuralfields,havealsodemonstratedtheirabilitytoproviderepresentationssuitablefor
combining datafrom multiple sensing modalities in as longas these areco-registered
oraligned.Theneuralfieldsframework,whichextracts3Dstructurefrommulti-view
images,enablesdirectfusionofinformationwith3Dpointcloudsourcesthroughpoint
cloud prior constraints [21]. Here, we consider the case of fusing multi-view images
from an RGB camera and point clouds from LiDAR. The difficulty in fusing camera
andLiDARinformationisthatcamerameasurescolorradiancewhileLiDARmeasures
distance[5].Fortunately,theframeworkofneuralradiancefieldscanbeusedtoextract
3DstructurefromimagesthusenablingdirectfusionofinformationfromLiDAR.This
canbedonethoughalearningfunctionthatextractsa3Dstructurepromotingconsis-
tency between the multi-view images as leveraged by standard NERF [17] subject to
LiDARpointcloudpriors[21]as:
L(Θ)= (cid:88)(cid:104) ∥C(r)−Cˆ(r,Θ)∥2 (cid:105) +λ(cid:88)(cid:2) ∥z(r)−zˆ(r,Θ)∥2 (cid:3) (6)
ℓ2 ℓ2
r∈R r∈R
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
LC(Θ) LD(Θ)
where the first term L (Θ) is the standard NERF learning function promoting a 3D
C
structure with consistency between image views while the second term L (Θ) en-
D
forces the LiDAR point cloud priors with zˆ(r,Θ) given as in Eq.(5). The benefit of
imposingpointcloudpriorsintoneuralfieldsistwo-fold:(1)itenablesexpressingrel-
ative distances obtained from standard 3D reconstruction of multi-view 2D images in
termsofrealmetrics(e.g.,meters),and(2)neuralfieldstendtofacechallengesinac-
curately estimating 3D structures at high distances (typically in the order of several
tensofmeters),wheretheLiDARpointcloudpriorscanserveasasupervisorysignal
toguideaccurateestimation,especiallyatgreaterdistances.Thiscanbebeneficial,as
distancesinaerialimageryaregenerallydistributedaroundlargedistances,whichmay
posechallengesfor3Dstructureextractionmethods.
4.1 Filinginthemissingbelow-canopystructureinALSdatawithTLS
In-situ terrestrial laser scanning (TLS) has been demonstrated as a powerful tool for
rapid assessment of forest structure in ecosystem monitoring and characterization. It
is capable of very fine resolution including the vertical direction: surface, sub-canopy
andcanopystructure.However,itsutilityandapplicationisrestrictedbylimitedspatial
coverage. Aerial laser scanning (ALS) on the other hand, has the ability to rapidly
surveybroadscaleareasatthelandscapelevel,butislimitedasitsparselysamplesthe
scene providing only coarse spatial variability details and it also cannot penetrate the
treecanopy.Fig.3ashowsapointcloudexamplecollectedusingafull-waveformALS
systemwhichcollects≈10pointspermetersquare.InFig.3anotethatthesub-canopy
structureisnotspatiallyresolved.Incontrast,TLSisfinelyresolvedbelowthecanopy
asobservedinFig.3b.8 J.Castorena
(a)ALSside-view (b)TLSside-view
Fig.3: Forest structure from TLS and ALS: ALS provides sparse spatial information
andisnotcapableofresolvingsub-canopydetail.TLSontheotherhand,providesfine
spatialvariabilityandresolutionalongfull3Dverticalstands.
Fortunately,thedrawbacksofTLSandALSscanscanberesolvedbyco-registration
whichtransformsthedatatoenabledirectfusion.Here,weusetheautomaticandtarget-
lessbasedapproachof[4].Thiswasdemonstratedtooutperformstandardmethods[2],
[19],[8]innaturalecosystemsandtoberobusttoresolutionscales,view-points,scan
area overlap, vegetation heterogeneity, topography and to ecosystem changes induced
bypre/postlow-intensityfireeffects.Itisalsofullyautomatic,capableofself-correcting
incasesofnoisyGPSmeasurementsanddoesnotrequireanymanuallyplacedtargets
[9]whileperformingatthesamelevelsofaccuracy.AllTLSscanswhereco-registered
into the coordinate system of ALS. Once scans have been co-registered they can be
projectedintoacommoncoordinatesystem.Illustrativeexampleresultsfortwoforest
plots where included in Fig.4 where the two sources: ALS and TLS have been color
coded differently, with the sparser point cloud being that of the ALS. Throughout all
casestheco-registrationproducedfinelyalignedpointclouds.Ingeneral,theerrorpro-
duced by this co-registration method is <6 cm for the translation and <0.1o for the
rotationparameters.ThetranslationerrorinmainlyduetotheresolutionofALSat10
points/metersquare.
4.2 AerialImagery
Experimentsperformedonbroaderforestareaswerealsoconducted.AerialRGBim-
agery was collected with a DJI Mavic2 Pro drone at 30Hz and a 3840 × 2160 pixel
resolution. Figs. 5a-5f show examples of multi-view aerial image inputs used by the
SFMandneuralfieldsmodels.Theforest3Dstructureresultingfromrunningconven-
tionalSFM[22]ontheseimagesisinFigs.5i-5killustratingdifferentperspectiveviews.
Again,thesequenceofrectanglesinredillustratethedroneflightpathandthesnapshot
image locations. Note that SFM was capable of resolving 3D structure for the entire
scene.LearningNeuralRadianceFieldsofForestStructureforScalableandFineMonitoring 9
(a)Co-registrationExample1 (b)Co-registrationExample2
Fig.4: TLS to ALS co-registration: Forest features are well aligned qualitatively be-
tweenbothALSandTLSsensing.
ApplyingNERFdirectlyintotheRGBimagerydataset,didnotresultincompara-
bleperformanceasinthecaseofthePonderosapinetreeshowninSection3.1.Without
pointcloudconstraints,the3DstructureextractedbytheneuralfieldsinFig.5hshows
the presence of artifacts at large distances. The main reason for these artifacts is that
NERFhaddifficultiesinrecovering3Dstructuresfromimageswithobjectsdistributed
atfardistances(e.g.,groundsurfaceinaerialscanning).ImposingLiDARpointcloud
priorswehypothesizecanhelptoalleviatethisissue.Here,wefollowthemethodology
of [21] and conduct experiments for fusing camera and LiDAR information through
the learning function in Eq.(6). The LiDAR point cloud uses both co-registered TLS
andALSdatawhichprovidesinformationtoconstrainbothdistancesinthemid-story
below the canopy and those between the ground surface and the tree canopy. The co-
registrationapproachusedtoalignALSandTLSpointcloudsistheonedescribedin
Section4.1.NotethatTLSinformationisnotavailablethroughouttheentiretestedfor-
estarea;rather,onlyoneTLSscanwascollected.Wefoundtheinformationprovided
by just one single scan was enough to constraint the relative distances in sub-canopy
areasthroughouttheentirescene.Imposingadditionalconstraintsthroughconsistency
withtheinputpointcloudshowninFig.5g,resultsintheextracted3Dstructureshown
inFigs.5l-5n.Inthiscase,thepointcloudpriorimposesconstraintsthatresolvetheas-
sociateddifficultiesatlargedistances.Notethatthisreconstructionissignificantlyless
sparserthanthoseshowninFigs.5i-5jobtainedfromconventionalSFM.NERF+LIDAR
resultsinimprovedresolutionwhichinturnenablesthedetectionofamuchfinerspa-
tialvariability,speciallyimportantforcurrentexistingdemandsinforestmonitoringat
broad scale. This illustrates the capacity of neural fields models not only to represent
highlydetailed3Dforeststructurefromaerialmulti-viewdatabutalsothepossibility
ofcombiningmulti-sourceremotelysenseddata(i.e.,imageryandLiDAR).10 J.Castorena
(a)Imageview-1 (b)Imageview-2
(c)Imageview-3 (d)Imageview-4
(e)Imageview-5 (f)Imageview-6 (g)PointCloud (h)NERFartifacts
(i)COLMAPview-1 (j)COLMAPview-2 (k)COLMAPview-3
(m)NERF+LIDARview-2
(l)NERF+LIDARview-1 (n)NERF+LIDARview-3
Fig.5: AI-based extraction of 3D structures from aerial multi-view 2D images + 3D
point cloud data inputs. Imposing point cloud priors into 3D structure extraction im-
provesdistanceambiguitiesinstructureandresolvesartifactissueslikelyatfarranges.LearningNeuralRadianceFieldsofForestStructureforScalableandFineMonitoring 11
5 Predictionofforestfactormetrics
Demonstration of the described capabilities of neural fields on forest monitoring pro-
gramsconsistshereinperformanceevaluationsof3Dforeststructurederivedmetrics.
Thesecanincludeforexamplenumberoftrees,speciescomposition,treeheight,diam-
eteratbreastheight(DBH),ageonagivengeo-referencedarea.However,sinceourfo-
cusistodemonstratetheusefulnessofneuralradiancefieldsforrepresenting3Dforest
structure,weonlyillustrateitspotentialinpredictionofthenumberoftreesandDBH
alonggeo-referencedareas.ThedatausedincludesoverlappingTLS+ALS+GPS+aerial
imagery multi-view multi-modal data collected over forest plot units. Each of these
plotsrepresentsalocationareaofavaryingsize:someofsize20x50mandothersat
15 m radius. The sites in which data was collected is in northern New Mexico, USA
(theNMdataset).Thevegetationheterogeneityandtopographyvariabilityoftheland-
scapeissignificantlydiverse.TheNMsitecontainshighelevationponderosapineand
mixed-coniferforest:whitefir,limberpine,aspen,DouglasfirandGambeloakandto-
pographyisathighelevationandofhigh-variation(between5,000-10,200ft).TheTLS
datawascollectedusingaLiDARsensormountedonastatictripodplacedatthecenter
ofeachplot.TheALSdatawascollectedbyaGalaxyT2000LiDARsensormounted
onafixed-wingaircraft.ThenumberofLiDARpointreturnspervolumedependonthe
sensorandscanningprotocolsettings(e.g.,TLSorALS,rangedistribution,numberof
scans) and these vary across plots depending on the heterogeneity of the site. Ground
truthnumberoftreesperplotwasobtainedthroughstandardforestplotfieldsurveying
techniques involving actual physical measurements of live/dead vegetation composi-
tion.Datafromatotalof250plotswherecollectedintheNMdataset.Ineveryforest
plot overlapping ALS, GPS, TLS and multi-view aerial imagery data was collected
alongwiththecorrespondingfieldmeasuredgroundtruth.Predictionofthenumberof
treesy perplotgivenpointcloudX,wasperformedfollowingtheapproachoftheGR-
1
Net [27][26]. Ingeneral, the methodologyconsists in computing2D boundingboxes
eachcorrespondingtoatreedetectionfromabirdseyeview(BEV).Arefinementseg-
mentationapproachthenfollowswhichprojectseach2Dboundingboxinto3Dspace.
Theresultingpointsinsideeach3Dboundingboxarethensegmentedbyfoliage,upper
stemandlowerstemandemptyspaceandthisinformationisusedtoimproveestimates
overthenumberoftrees.Thismethodologyisusedindependentlyonseveralcasesce-
narioscomparingtheperformanceofacombinationofremotesensingapproaches:(1)
neuralfields(NF)fromaerialRGBImages,(2)ALSasinFig.6b,(3)TLSasinFig.6a,
(4)ALS+TLS,(5)NF-RGBimages+ALS,(6)NF-RGBimages+TLS,(7)NF-RGB
Images+TLS+ALS.NotethattheTLS,ALSandTLS+ALSpredictionresultsdoes
notmakeanyuseofneuralfields.Rather,theirperformancewasincludedonlyforcom-
parisonpurposes.Table1summarizestherootmeansquarederror(RMSE)resultsfor
eachofthetestedcases.
TheresultsinTable1corroboratesomeofthetrade-offsbetweenthesensingmodal-
itiesandinadditionsomeoftheadvantagesgainedthroughtheuseofneuralfieldsin
forestry. First, the superiority of TLS over ALS data on the number of trees metric
is mainly due to the presence of information in sub-canopy which is characteristic of
in-situTLS.Thisinalignmentwithcurrentdemonstrationsintheliteraturewhichhave
motivated the widespread usage of in-situ TLS in forestry applications even though12 J.Castorena
(a)In-situplot-scaleTLShasdemonstratedtobeaneffectivetoolin
estimatingplot-levelvegetationcharacteristics
(b) Broad-landscape scale ALS derived prediction, does not have
verticaldimensionresolutionresultinginunderestimatepredictions
Fig.6:LiDARderivedvegetationattributeestimationforsingleTLSandALS.
Table1:RMSEPredictionperformanceofnumberoftreesperplotinNMdataset.
MethodNF-RGBALSTLSALS+TLSNF-RGB+ALSNF-RGB+TLSNF-RGB+ALS+TLS
RMSE 10.61 8.44 1.77 1.67 1.41 1.39 1.32
it is not as spatially scalable as ALS is [20]. We would have seen the opposite rela-
tionshipsbetweenTLSandALS,however,incaseswhentheplotsizeissignificantly
higher than the range of a single in-situ TLS scan. A problem which can be resolved
byaddingmultipleviewco-registeredTLSscansperplot.Thislimitationiscausedas
thesensorremainsstaticatcollectiontimewhichmakesitmoresusceptibletoocclu-
sions, specially in dense forest areas where trees can significantly reduce the view of
TLS at higher ranges. TLS+ALS overcomes, on the other hand, the limitations of the
individualLiDARplatformsbyfillinginthemissinginformationcharacteristicofeach
platform. Structure from neural fields using only multi-view RGB images performed
slightlyworstthanbothALSandTLS.Thismaybeduetothelimitednumberofmulti-
view images collected per plot, the performance for deriving structure from NERF or
tothejointperformanceofNERFinconjunctionwiththeGRNet.Fortunately,fusing
neural fields from multi-view imagery with LiDAR shows a significant improvement
overallfusedcases(i.e.,NF+ALS,NF+TLSandNF+ALS+TLS).WeseethatthepriorLearningNeuralRadianceFieldsofForestStructureforScalableandFineMonitoring 13
supervisory signal imposed by the LiDAR point cloud helps on guiding the resulting
3D structure from NERF to alleviate the artifacts arising at far distances when using
multi-viewimageryonly.Wewouldliketofinalizethisdiscussionbyhighlightingthe
performanceoftheNF-RGB+ALSmethodwhichismarginallysimilartothebestper-
formingmethod(i.e.,NF-RGB+ALS+TLS).ThebenefitofusingNF-RGB+ALSisthat
beingbothairbornemakesthedatacollectionofthesetwomodalitiestimeandcostef-
ficient,incontrast,toin-situremotesensingmethodssuchasTLS.Thishassignificant
implicationstowardsachievingbothscalableandhighlyperformingforestmonitoring
programs.Ingeneral,onehastoresorttoabalancebetweenscalabilityandperformance
performancedependingonneeds.Ourworkinstead,offersamethodwhichcanpoten-
tially achieve similar performance as in-situ methods with the benefits of scalability
overthelandscapescalesthroughcomputationalmethods.
Additionalexperimentswereconductedtoexploretheabilityofneuralfieldsfrom
terrestrialbasedmulti-viewimagerytoachieveaperformancenearthatofTLSinmet-
ricsthatdependonsub-canopyinformation.Inthiscase,weevaluatedperformanceon
theDBHmetricforatotalof200trees.GroundtruthDBHwasmanuallymeasuredin
the field for each tree’s stem diameter at a height of 1.3m. A total of 5 co-registered
TLS scans where used per tree, each collected from a different location and viewing
eachtreefromadifferentperspectivetoreducetheeffectsofocclusionandtoremove
the degrading effects of lower point LiDAR return densities at farther ranges. Multi-
viewTLSco-registrationwasobtainedusingthemethodof[4].Terrestrialmulti-view
RGB imagery data for NERF was collected around an oblique trajectory around each
treeasexemplifiedinFig.1with10−15snapshotimagespertree.Algorithmicperfor-
manceforestimatingDBHwascomparedagainstTLS,ALS,TLS+ALSandNF-RGB.
The estimation approach of [26] relying on stem geometric circular shape fitting at a
heightof1.3moverthegroundwasusedfollowingtheirimplementation.Performance
is measured as the average error as a percentage of the actual field measured DBH
groundtruth,followingtheworkof[26].ComparisonresultsarereportedinTable2.
Table2:ComparisonofsensingmodalitiesonaverageerrorDBHestimation.
Method NF-RGB ALS TLS ALS+TLS
Avg.error% 1.7% 32.7%1.3% 3.3%
In table 2 ALS performs the worst DBH estimation due to its inherited limited
sub-canopy resolution. Multi-view TLS on the other hand, performs the best at 1.3%
errorconsistentwithTLSsuperiorityfindingsin[26]formetricsrelyingonsub-canopy
information. However, our neural fields approach from terrestrial imagery performs
marginallyonparwithmulti-viewTLS,withtheadditionaladvantagethatRGBcamera
sensorsaresimplertoaccesscommerciallyandsignificantlycheaperthanLiDAR.
Intermsofcomputationalspecifications,neuralradiancefieldsweretrainedusing
a set of overlapping 10-50 multi-view images per scene. The fast implementation of
[18]wasusedwithtrainingontheterrestrialandaerialmulti-viewimagerytakingfrom14 J.Castorena
30-60secsper3Dstructureextraction(e.g.,perplotintheaerialimagerycase,pertree
intheterrestrialimagerycase).AddingtheLiDARconstraintswasdonefollowingthe
implementationfrom[21].Theneuralradiancearchitectureisamultilayerperceptron
(MLP) with two hidden layers and a ReLU layer per hidden layer and a linear output
layerasin[18].TrainingwasperformedusingtheADAMoptimizer[14]withparam-
etersβ =0.9,β =0.99,ϵ=10−15usingNVIDIATeslaV100.
1 2
Themainlimitationofneuralfieldsfromaerialmulti-viewimageryisthepresence
ofocclusionofsub-canopystructure,speciallyindenselyforestedareas.Inourcase,fu-
sionwithTLSdatacanresolvethisproblemasterrestrialdataprovideshighlydetailed
sub-canopyinformation.Additionally,whenTLSisunavailable,terrestrialimagerycan
beusedinstead.Our3Dstructureexperimentsfromterrestrialmulti-viewinformation
inSec.3.1andtheDBHestimationperformanceresultsdemonstratethathighlydetailed
structurealongtheentireverticalstanddirectioncanbeextractedbyneuralfieldswhen
imageinformationisavailable.Intheabsenceofmulti-viewimagedata,however,neu-
ralfieldsarenotcapableofgeneratingsyntheticinformationbehindoccludedareasand
performance on metrics affected by occlusion are expected to yield large errors. This
problem can be alleviated through multi-view images capturing the desired areas of
interestintheecosystem.
6 Conclusion
In this work, we proposed neural radiance fields as representations that can finely ex-
pressthe3Dstructureofforestsbothinthein-situandatthebroadlandscapescale.In
addition,thepropertiesofneuralradiancefields;inparticular,thefactthattheyaccount
for both the origin and direction of radiance to define 3D structure enables the fusion
of data coming from multiple locations and modalities; more specifically those from
multi-viewLiDAR’sandcameras.Finally,weevaluatedtheperformanceof3Dstruc-
turederivedmetricstypicallyusedinforestmonitoringprogramsanddemonstratedthe
potentialofneuralfieldstoimproveperformanceofscalablemethodsatnearthelevel
of in-situ methods. This not only represents a benefit on sampling time efficiency but
alsohaspowerfulimplicationsonreducingmonitoringcosts.
Acknowledgements
ResearchpresentedinthisarticlewassupportedbytheLaboratoryDirectedResearch
and Development program of Los Alamos National Laboratory under project number
GRR0CSRN.
References
1. Atchley,A.,Linn,Rodman,J.A.,Hoffman,C.,Hyman,J.D.,Pimont,F.,Sieg,C.,Middleton,
R.S.:Effectsoffuelspatialdistributiononwildlandfirebehaviour.InternationalJournalof
WildlandFire30(3),179–189(2021)LearningNeuralRadianceFieldsofForestStructureforScalableandFineMonitoring 15
2. Besl, P.J., McKay, N.D.: A method for registration of 3-d shapes. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence 14(2), 239–256 (Feb 1992).
https://doi.org/10.1109/34.121791
3. Castorena, J., Creusere, C.D., Voelz, D.: Modeling lidar scene sparsity using compressive
sensing. In: 2010 IEEE International Geoscience and Remote Sensing Symposium. pp.
2186–2189.IEEE(2010)
4. Castorena, J., Dickman, L.T., Killebrew, A.J., Gattiker, J.R., Linn, R., Loudermilk, E.L.:
Automatedstructural-levelalignmentofmulti-viewtlsandalspointcloudsinforestry(2023)
5. Castorena,J.,Puskorius,G.V.,Pandey,G.:Motionguidedlidar-cameraself-calibrationand
accelerated depth upsampling for autonomous vehicles. Journal of Intelligent & Robotic
Systems100(3),1129–1138(2020)
6. Dubayah, R.O., Drake, J.B.: Lidar remote sensing for forestry. Journal of forestry 98(6),
44–46(2000)
7. FAO,U.:Thestateoftheworld’sforests2020.In:Forests,biodiversityandpeople.p.214.
Rome,Italy(2020).https://doi.org/https://doi.org/10.4060/ca8642en
8. Gao,W.,Tedrake,R.:Filterreg:Robustandefficientprobabilisticpoint-setregistrationusing
gaussianfilterandtwistparameterization.In:ProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition.pp.11095–11104(2019)
9. Ge, X., Zhu, Q.: Target-based automated matching of multiple terrestrial laser scans for
complexforestscenes.ISPRSJournalofPhotogrammetryandRemoteSensing179,1–13
(2021)
10. Hilker,T.,vanLeeuwen,M.,Coops,N.C.,Wulder,M.A.,Newnham,G.J.,Jupp,D.L.,Cul-
venor,D.S.:Comparingcanopymetricsderivedfromterrestrialandairbornelaserscanning
inadouglas-firdominatedforeststand.Trees24(5),819–832(2010)
11. Hyyppa¨,J.,Yu,X.,Hyyppa¨,H.,Vastaranta,M.,Holopainen,M.,Kukko,A.,Kaartinen,H.,
Jaakkola,A.,Vaaja,M.,Koskinen,J.,etal.:Advancesinforestinventoryusingairbornelaser
scanning.Remotesensing4(5),1190–1207(2012)
12. Kajiya,J.T.,VonHerzen,B.P.:Raytracingvolumedensities.ACMSIGGRAPHcomputer
graphics18(3),165–174(1984)
13. Kankare,V.,Joensuu,M.,Vauhkonen,J.,Holopainen,M.,Tanhuanpa¨a¨,T.,Vastaranta,M.,
Hyyppa¨,J.,Hyyppa¨,H.,Alho,P.,Rikala,J.,etal.:Estimationofthetimberqualityofscots
pinewithterrestriallaserscanning.Forests5(8),1879–1895(2014)
14. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980(2014)
15. Lausch, A., Erasmi, S., King, D.J., Magdon, P., Heurich, M.: Understanding forest health
withremotesensing-partii—areviewofapproachesanddatamodels.RemoteSensing9(2),
129(2017)
16. Linn,R.,Reisner,J.,Colman,J.J.,Winterkamp,J.:Studyingwildfirebehaviorusingfiretec.
Internationaljournalofwildlandfire11(4),233–246.(2002)
17. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.:
Nerf: Representing scenes as neural radiance fields for view synthesis. arXiv preprint
arXiv:2003.08934(2020)
18. Mu¨ller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives with
a multiresolution hash encoding. ACM Trans. Graph. 41(4), 102:1–102:15 (Jul 2022).
https://doi.org/10.1145/3528223.3530127,https://doi.org/10.1145/3528223.3530127
19. Myronenko,A.,Song,X.:Pointsetregistration:Coherentpointdrift.IEEEtransactionson
patternanalysisandmachineintelligence32(12),2262–2275(2010)
20. Pokswinski,S.,Gallagher,M.R.,Skowronski,N.S.,Loudermilk,E.L.,Hawley,C.,Wallace,
D., Everland, A., Wallace, J., Hiers, J.K.: A simplified and affordable approach to forest
monitoringusingsingleterrestriallaserscansandtransectsampling.MethodsX8,101484
(2021)16 J.Castorena
21. Roessle,B.,Barron,J.T.,Mildenhall,B.,Srinivasan,P.P.,Niebner,M.:Densedepthpriors
forneuralradiancefieldsfromsparseinputviews.In:ProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition.pp.12892–12901(2022)
22. Schonberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Proceedings of the
IEEEconferenceoncomputervisionandpatternrecognition.pp.4104–4113(2016)
23. Tomppo,E.,Gschwantner,T.,Lawrence,M.,McRoberts,R.E.,Gabler,K.,Schadauer,K.,
Vidal,C.,Lanz,A.,Staahl,G.,Cienciala,E.:Nationalforestinventories.PathwaysforCom-
monReporting.EuropeanScienceFoundation1,541–553(2010)
24. Vierling,K.T.,Vierling,L.A.,Gould,W.A.,Martinuzzi,S.,Clawges,R.M.:Lidar:shedding
newlightonhabitatcharacterizationandmodeling.FrontiersinEcologyandtheEnviron-
ment6(2),90–98(2008)
25. White,J.C.,Coops,N.C.,Wulder,M.A.,Vastaranta,M.,Hilker,T.,Tompalski,P.:Remote
sensingtechnologiesforenhancingforestinventories:Areview.CanadianJournalofRemote
Sensing42(5),619–641(2016)
26. Windrim,L.,Bryson,M.:Detection,segmentation,andmodelfittingofindividualtreestems
fromairbornelaserscanningofforestsusingdeeplearning.RemoteSensing12(9), 1469
(2020)
27. Xie,H.,Yao,H.,Zhou,S.,Mao,J.,Zhang,S.,Sun,W.:Grnet:Griddingresidualnetworkfor
densepointcloudcompletion.In:ECCV(2020)