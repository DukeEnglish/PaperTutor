Astructuredregressionapproachforevaluatingmodelperformanceacross
intersectionalsubgroups
CHRISTINEHERLIHY∗,UniversityofMaryland,CollegePark,USA
KIMBERLYTRUONG∗,OregonStateUniversity,USA
ALEXANDRACHOULDECHOVA,MicrosoftResearch,USA
MIROSLAVDUDÍK,MicrosoftResearch,USA
DisaggregatedevaluationisacentraltaskinAIfairnessassessment,withthegoaltomeasureanAIsystem’sperformanceacrossdifferent
subgroupsdefinedbycombinationsofdemographicorothersensitiveattributes.Thestandardapproachistostratifytheevaluationdata
acrosssubgroupsandcomputeperformancemetricsseparatelyforeachgroup.However,evenformoderately-sizedevaluationdatasets,
samplesizesquicklygetsmallonceconsideringintersectionalsubgroups,whichgreatlylimitstheextenttowhichintersectional
groupsareconsideredinmanydisaggregatedevaluations.Inthiswork,weintroduceastructuredregressionapproachtodisaggregated
evaluationthatwedemonstratecanyieldreliablesystemperformanceestimatesevenforverysmallsubgroups.Wealsoprovide
correspondinginferencestrategiesforconstructingconfidenceintervalsandexplorehowgoodness-of-fittestingcanyieldinsight
intothestructureoffairness-relatedharmsexperiencedbyintersectionalgroups.Weevaluateourapproachontwopubliclyavailable
datasets,andseveralvariantsofsemi-syntheticdata.Theresultsshowthatourmethodisconsiderablymoreaccuratethanthestandard
approach,especiallyforsmallsubgroups,andgoodness-of-fittestinghelpsidentifythekeyfactorsthatdrivedifferencesinperformance.
1 INTRODUCTION
AcoretaskwhenassessingthefairnessofanAIsystemismeasuringitsperformanceacrossdifferentsubgroupsdefined
bycombinationsofdemographicorothersensitiveattributes.Manyofthebest-knownstudiesofalgorithmicbiasare
groundedinthistypeofanalysis.ThisincludestheBuolamwiniandGebru’sGenderShadesstudy[5],whichfound
thatcommercialgenderclassifiershavemuchhighererrorratesfordarker-skinnedwomenthanothergroups,and
theObermeyeretal.’sstudy[27]findingbiasincommercialalgorithmsusedtoguidehealth-caredecisions,aswell
asmanyothers[1,14,22,32].
Intheirworkformalizingthistypeofanalysis,Barocasetal.[3]introducethetermdisaggregatedevaluationto
refertothistask.Theauthorsdrawattentiontothemanydecisionsthatoftenimplicitlygointoshapinganygiven
disaggregatedevaluation:fromwhowillbeinvolved,towhatdatawillbeused,tothestatisticalapproachtaken,to
drawinginferencesfromthedata.Inourwork,wefocusonthequestionofstatisticalmethodologygivenanavailable
datasetandpre-determinedsubgroupsandperformancemetricsofinterest.Specifically,weintroduceamethodfor
estimatingperformanceacrosssubgroupsthatweshow(i)ismoreaccuratethanapproachestakeninstandardpractice;
and(ii)canprovidegreaterinsightintowhichfactorsdriveobservedvariationinperformance.
The“standardapproach”todisaggregatedevaluationproceedsbystratifyingtheevaluationdataacrosssubgroups
andthenconductinginference(i.e.,computingperformancemetrics,confidenceintervals,orotherstatistics)separately
foreachgroup.Theprimarychallengewhenapplyingthisapproacharesmallsamplesizes.Evenformoderately-sized
evaluationdatasets,samplesizesquicklygetsmallonceconsideringintersectionalsubgroups.Forinstance,inamedical
diabetesmellitusdatasetweuselaterinthepaper,wehavea5000-patientevaluationdataset,ofwhich2689patients
∗Bothauthorscontributedequallytothisresearch.
Authors’addresses:ChristineHerlihy,cherlihy@umd.edu,UniversityofMaryland,CollegePark,USA;KimberlyTruong,truonkim@oregonstate.edu,
OregonStateUniversity,USA;AlexandraChouldechova,alexandrac@microsoft.com,MicrosoftResearch,USA;MiroslavDudík,mdudik@microsoft.com,
MicrosoftResearch,USA.
4202
naJ
62
]GL.sc[
1v39841.1042:viXra2 ChristineHerlihy,KimberlyTruong,AlexandraChouldechova,andMiroslavDudík
arefemale,620arefemaleandoverage80,butonly6arefemale,overage80,andHispanic.Indeed,ofthe32distinct
gender-age-race/ethnicitysubgroupsthatcanbeformedinthedata,8(i.e.,25%)havefewerthan10observations,and
nearlyhalfhavefewerthan25observations.Inferencebasedonsofewobservationsisoftenuninformative,andmay
beunreliable.Inpractice,subgroupsthataretoosmalltendtobeeitherexcludedfromanalysisormergedwithother
smallbutpotentiallyheterogeneoussubgroupstoformhigher-level“catch-all”categories(e.g.,“other”).Thesepractices
greatlylimittheextenttowhichintersectionalgroupsareevenconsideredinmanydisaggregatedevaluations.As
aconsequence,standardassessmentsmayfailtosurfacefairness-relatedharmsthatcoulddisproportionatelyaffect
intersectionalsubgroups[7],whichinturnmeansthatstepstomitigatethoseharmswillnotbetaken.
In this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate
can yield reliable system performance estimates even for very small subgroups (e.g., for groups with fewer than
25observations).Wealsoprovidecorrespondinginferencestrategiesforconstructingconfidenceintervalsforthe
subgroup-levelperformanceestimates.Wethendemonstratehowgoodness-of-fittestingcanprovideinsightintothe
structureoffairness-relatedharmsexperiencedbyintersectionalgroupsandalsoidentifysituationswhereobserved
variationinperformanceisattributabletobenignfactors.Lastly,wepresentresultsontwopubliclyavailabledatasets,
andseveralvariantsofsemi-syntheticdata.Theresultsshowthatourmethodisconsiderablymoreaccuratethanthe
standardapproach,especiallyforsmallsubgroups.Theyfurthershowthatourmethodoutperformsmorestatistically
sophisticatedbaselines,includingthemodel-basedmetricsmethodintroducedbyMilleretal.[25],whilealsooffering
additionaladvantages.Weconcludebydiscussinglimitationsandfuturedirections.
2 BACKGROUNDANDRELATEDWORK
Intheirtaxonomyofsociotechnicalharmsofalgorithmicsystems,Shelbyetal.[29]identifyfivehigh-levelcategories
ofharm:representational,allocative,qualityofservice,interpersonal,andsocialsystem.Ourworkcontributestothe
broaderliteraturecharacterizingandassessingallocativeandquality-of-serviceharmsthatcanresultfromtheuseof
algorithmicsystems.Allocativeharms,firstdiscussedbyBarocasetal.[2],occurwhensystemsproduceaninequitable
distributionofinformation,opportunity,orresourcesacrossgroups.Asarunningexample,weconsiderahypothetical
settinginwhichamodeltrainedtopredict30-dayhospitalreadmissionisusedtoprioritizehigh-riskpatientsfor
moreintensivepost-dischargecare.Allocativeharmsmightoccurinthissettingifcertainsubgroupsofpatientsare
disproportionatelyunder-prioritizedformoreintensivecare(i.e.,havelowselectionrates)orareunder-selectedrelative
totheirobservedrateofreadmission(i.e.,havehighfalsenegativerates).
Quality-of-serviceharmsoccurwhenalgorithmicsystemsunderperformforcertainsociallysalientgroupsofusers[29,
37].Weexaminequality-of-serviceharmsacrossraceandgendergroupsinthecontextofcommercialautomatedspeech
recognition(ASR)systemsusingdatapreviouslyanalysedbyKoeneckeetal.[22].Specifically,weassesswhetherthere
issignificantvariationintheworderrorrate(WER)oftheASRsystemsacrossintersectionalraceandgendersubgroups.
Theterm“intersectionality”wasintroducedbyCrenshaw[7]todescribethedistinctpatternsofdiscrimination
anddisadvantageexperiencedbyBlackwomen,whichshearguedcannotbeunderstoodintermsofraceorgender
discrimination alone. In recent years, algorithmic fairness research has examined intersectional bias from many
perspectives.Thisincludesworkintroducingquantitativemetricsintendedtocapturenotionsofintersectionalfairness,
suchassubgroupfairness[21],differentialfairness[11,12],andmulti-calibration[15],alongwithlearningalgorithms
for estimating and achieving these criteria. Wang et al. [36] study “predictivity differences” across intersectional
subgroups,anddiscusslimitationsofexistingsummarystatistics(suchasthemaximumdisparityacrossallgroups)in
capturingmeaningfulnotionsofintersectionalharm.Ourworkdiffersfromthisliteraturebecausewearespecifically3
interestedinthetaskofdisaggregatedevaluation.Thisentailsestimatingandreportingsystemperformanceforeach
intersectionalsubgroup,ratherthancomputingaparticularfairnessmetricorlearningafairness-constrainedmodel.
Ourworkmostdirectlycontributestothegrowingliteratureintroducingmoresample-efficientmethodsforcon-
ductingdisaggregatedevaluations.Thisliteratureincludesmethodsthatleverageunlabelleddatainmodelevaluation
[6,19,20];methodsthatboundorapproximateperformanceforintersectionalsubgroupsusingmarginalstatistics[26];
andsyntheticdataaugmentationapproaches[34].Inworkmorecloselyrelatedtothespiritofourstructuredregression
approach,Piratlaetal.[28]introducetheAttributedAccuracyAssay(AAA)method,whichmodelstheaccuracyofa
modelasafunctionofsensitiveattributesandotherfeaturesviaaGaussianProcess(GP).WhilewedonotrelyonGPs,
wedoproceedsimilarlybymodelingtheaccuracy(orerror)ofagivenmodel.Whereaswearespecificallyconcerned
withfairnessanddisaggregatedevaluation,Piratlaetal.[28]aimtoproducean“accuracysurface”modelthatclients
canusetoestimatetheperformanceofanexistingmodelontheirdata.
ThemostcloselyrelatedworkinrecentliteratureisthatofMilleretal.[25],whointroduceaBayesianstructuredre-
gressionapproachthattheycallmodel-basedmetrics(MBM).TheirmethodappliestoAImodelsthatproduceascore(say
topredictariskofhospitalreadmission).Bymodelingthedistributionofscoresgivenselectfeaturesandtheobservedout-
come,theyareabletomakeinferenceonanyperformancemetricofinterest,buttheapproachisnotdirectlyapplicableto
theevaluationofmodelsthatdonotproduceclassificationscores(e.g.,MBMdoesnotdirectlyapplytotheevaluationof
WERinASRsystems).UnliketheMBMapproach,wemodelthetargetmetricdirectlyandfitseparatemodelsforeachper-
formancemetricofinterest.OurexperimentsshowthatourmethodyieldsmoreaccurateestimatesthanMBM(see§5.1).
Ourapproachisalsorelatedtotheclassicallineofresearchonnormalmeansestimation,originatingwiththe
James-Stein(JS)estimator[16,30].TheJSestimatorworksbyshrinkingstandardestimatestowardszero(orsome
otherconstant),whichleadstoasubstantialdecreaseinvariance,whileonlyamoderateincreaseinbias.Thisfavorable
bias–variancetrade-offinturnleadstoamoreaccurateestimator.TheempiricalBayes(EB)approach[8]alsoleadsto
aformofshrinkage,butitsmotivationisdifferent.ItpositsahierarchicalBayesianmodelandestimatesmetricvalues
byposteriormeans,whilefixingpriorhyperparameterstotheirpointestimates.Ourestimatorworksbyoptimizing
bias–variancetrade-offsimilartoJS,butitenjoysadditionaladvantagescomparedwithJSandEB:availabilityof
confidenceintervalproceduresandflexibilitytoincorporateinformationintheformofcovariates.Inourexperiments
weshowthatourapproachmatchesandsometimesoutperformsJSandEB(see§5.1).
Manyimportantchallengeslieoutsidethescopeofthispaper.Wefocusonimprovingaccuracyofdisaggregated
evaluation,especiallyonsmallgroups,assumingthatrelevantsensitiveattributesandperformancemetricshavebeen
determinedandasuitableevaluationdatasetcollected.However,manykeysociotechnicalchallengesariseduringthe
evaluationconceptionanddatasetconstructionphases[3,24].Forinstance,asBarocasetal.[3]discuss,thesensitive
attributesoftenincludesociallyconstructed—andpotentiallycontested—features(likeraceandgender),whichmakes
thetaskofmappingpeopletoattributesandcorrespondingsubgroupspotentiallyfraught,particularlywhenitinvolves
inferenceoruseofproxyvariables,orposesariskformembersofalready-marginalizedsubgroups.Anotherset
ofchallengesariseswhendecidingonaperformancemetric.Inmanyhigh-stakesapplications(likeeducationand
health-care),wearenotabletodirectlymeasurewhomightbenefit,soweneedtorelyonproxies.Thisstepiscritical,
sinceapoorchoiceofaproxymayfurtherexacerbateexistinginequities,asisthecase,forinstance,whenpredicting
riskofre-offensefromarrestrecords[10]orpredictinghealth-careneedsbasedonhealth-careexpenditures[27].4 ChristineHerlihy,KimberlyTruong,AlexandraChouldechova,andMiroslavDudík
3 PROBLEMSETTING
Wewouldliketoassessthefairness-relatedharmsofanAIsystembyevaluatingitsperformanceonintersectional
subgroupsofusersspecifiedby𝑘 ≥2sensitiveattributes(likeraceandgender),takingvaluesinfinitesetsA1,...,A𝑘.
Thesetofallpossible𝑘-tuplesofsensitive-attributevaluesisdenotedA=A1×···×A𝑘.
Weassumethatwehaveaccesstoanevaluationdataset𝑆,consistingofindividualsdescribedbytuplesoftheform
(𝑋,𝐴,𝑌,𝑌ˆ )sampledi.i.d.fromsomeunderlyingdistributionD,where𝑋 containsapplication-relevantinformation
abouttheindividual(e.g.,thehealthhistoryofapatient),𝐴∈Aisa𝑘-tupleofsensitiveattributes,𝑌 isanobserved
outcomevariable(e.g.,whetherthepatientwasreadmittedwithin30daysofdischarge),and𝑌ˆ isanoutputproduced
bytheAIsystem(e.g.,ascoreusedforprioritizingpatientsintopost-dischargecare).
Forany𝑘-tuple𝑎∈A,wewrite𝑎[1],...,𝑎[𝑘]todenoteitscomponents.IntheASRexamplebelow,weconsider
twosensitiveattributes,raceandgender,withdomainsA1={Black,white}andA2={male,female}.Inthatcase,for
example,if𝑎=(Black,female),then𝑎[1] =Blackand𝑎[2] =female.Whenpossible,weusemnemonicindicesforcom-
ponentsof𝑎andwrite𝑎[race]and𝑎[gender]tomean𝑎[1]and𝑎[2],andsimilarlyA raceandA gendertomeanA1andA2.
Foreach𝑎∈A,wedefineD𝑎tobethedistributionofindividualswith𝐴=𝑎,soD𝑎istheconditionaldistribution
D(𝑋,𝐴,𝑌,𝑌ˆ |𝐴=𝑎),representinganintersectionalgroup.LetΔdenotethesetofallprobabilitydistributionsover
tuples(𝑋,𝐴,𝑌,𝑌ˆ ),soD ∈ΔandalsoD𝑎 ∈Δforall𝑎∈A.Aperformancemetricisafunction𝑚:Δ→Rthatmapsa
probabilitydistributionovertuples(𝑋,𝐴,𝑌,𝑌ˆ )intoarealnumber.Forexample,iftheunderlyingAIsystemperforms
binaryclassification,so𝑌,𝑌ˆ ∈{0,1},wecouldmeasureitsperformanceusingaccuracy,defined,forany𝑝 ∈Δ,as
ACC(𝑝)=P 𝑝[𝑌 =𝑌ˆ ],
whereP 𝑝[·] istheprobabilityofaneventwithrespectto𝑝.Theoverallsystemperformanceisthenquantifiedby
ACC(D)andtheperformanceonthegroup𝑎∈AbyACC(D𝑎).
Givenaperformancemetric𝑚,thegoalofdisaggregatedevaluationistoestimatethevalues𝑚(D𝑎)forall𝑎∈A.
Wedenotethesevaluesas
𝜇
𝑎
=𝑚(D𝑎).
OuronlysourceofinformationaboutD istheevaluationdataset𝑆 ofsize𝑛 = |𝑆|,sampledi.i.d.fromD.The
standardapproachtodisaggregatedevaluationsplitsthedataset𝑆intogroups
𝑆 𝑎 =(cid:8) (𝑋,𝐴,𝑌,𝑌ˆ ) ∈𝑆 : 𝐴=𝑎(cid:9)
ofsize𝑛
𝑎
=|𝑆 𝑎|,andthenevaluates𝑚oneach𝑆 𝑎(or,moreprecisely,ontheprobabilitydistributionthatputsanequal
probabilitymassoneachdatapointin𝑆 𝑎).Wedenotetheresultingstandardestimatesas
𝑍
𝑎
=𝑚(𝑆 𝑎). (1)
Forexample,if𝑚isaccuracy,then
𝑍 𝑎 =ACC(𝑆 𝑎)= 1 ∑︁ 1{𝑌 =𝑌ˆ },
𝑛
𝑎
(𝑋,𝐴,𝑌,𝑌ˆ)∈𝑆𝑎
where1{·}isanindicatorequalto1ifitsargumentistrueand0ifitisfalse.
Wenextconnectthisabstractframeworktotwoconcretescenariosalreadymentionedin§2.
Example1(Diabetes). WeconsideranAIsystemthatrefershigh-riskpatientsintoapost-dischargecareprogram.
Wewishtoassesstheallocativeharmsofthissystem.Toexplorethisscenario,weuseapubliclyavailabledataset5
ofdiabetespatientsdevelopedbyStracketal.[31].Thedatasetcontainsinformationaboutpatienthospitalvisits,
includingwhethereachpatientwasreadmittedwithin30daysafterdischarge.Weusethereadmissionasaproxyfor
whetherthepatientshouldberecommendedforthecareprogram.
Eachdatapointcorrespondstoapatientadmission,where𝑋 describesthepatienthistoryandhospitaltests;𝐴
describesthepatient’srace,gender,and(binned)age;𝑌 ∈{0,1}indicateswhetherthepatientwasreadmittedwithin
30daysafterdischarge;and𝑌ˆ ∈ [0,1]isthescoreproducedbytheAIsystemthathasbeentrainedtopredict𝑌.We
assumethatthehospitalusesathreshold𝑟,andpatientswith𝑌ˆ ≥𝑟 areautomaticallyreferredintothecareprogram.
Onetypeofallocativeharmoccurswhenasubgroupofpatientsisdisproportionatelyunder-prioritized,i.e.,ifa
subgrouphasalowselectionrate,denotedas
SEL(D𝑎)=P D𝑎[𝑌ˆ ≥𝑟].
Wealsoconsiderasecondtypeofharm,whichoccurswhenasubgroupofpatientsexperiencesadisproportionately
largerateoffalsenegatives(i.e.,manyofthosepatientsthatshouldberecommendedarenot),measuredbythefalse
negativerate
FNR(D𝑎)=P D𝑎[𝑌ˆ <𝑟 |𝑌 =1].
Example2(ASR). Toassessquality-of-serviceharmsofanASRsystem,weuseadatasetfromKoeneckeetal.[22],
consistingofaudiosnippets(oflengthbetween5sand50s)spokenbyvariousspeakers.Inthedataset,𝑋 describes
propertiesofthesnippet(likedurationinseconds),𝐴hastwocomponentscorrespondingtothespeaker’sraceand
gender,𝑌 istheground-truthtranscriptionofthesnippet,and𝑌ˆ isthetranscriptionprovidedbytheAIsystem.
Thequality-of-serviceharmsoccurwhenthesystemunderperformsforasubgroupofusers.Theperformanceis
evaluatedbytheworderrorrate
WER(D𝑎)=E D𝑎[wer(𝑌ˆ,𝑌)],
wherewerisasnippet-levelworderrorratedefinedas
wer(𝑌ˆ,𝑌)= subst+del+ins ,
|𝑌|
wheresubst,del,andinsisthenumberofwordsubstitutions,deletions,andinsertionsin𝑌ˆ comparedwiththeground
truth𝑌,and|𝑌|isthenumberofwordsin𝑌.
Toquantifytheaccuracyofanestimator,likethestandardestimatorintroducedabove,weoftenusemeansquared
error (MSE).WewilluseamodifieddefinitionofMSEthataccountsforthefactthatestimateslike𝑍 𝑎 =𝑚(𝑆 𝑎)are
sometimesundefined,forinstance,whenthemetric𝑚isdefinedasaconditionalprobability,likeFNRinExample1,and
theset𝑆 𝑎hasnosamplesthatsatisfythecondition(e.g.,nosampleswith𝑌 =1incaseofFNR).Foranestimator𝜇ˆofa
quantity𝜇,letEdenotetheeventthat𝜇ˆisdefined.Thebias,variance,andmeansquarederror(MSE)of𝜇ˆaredefinedas
Bias(𝜇ˆ)=E[𝜇ˆ | E]−𝜇, Var(𝜇ˆ)=E(cid:2)(cid:0)𝜇ˆ−E[𝜇ˆ | E](cid:1)2(cid:12) (cid:12)E(cid:3), MSE(𝜇ˆ)=E(cid:2)(cid:0)𝜇ˆ−𝜇(cid:1)2(cid:12) (cid:12)E(cid:3), (2)
wheretheexpectationsarewithrespecttothedata-generatingprocessgivingrisetothedatasetusedtocalculate𝜇ˆ
(whichisitselfarandomvariable).Anestimatorwithbiasequaltozeroiscalledunbiased.
Meansquarederrordecomposesintobiasandvariancetermsas
MSE(𝜇ˆ)=
(cid:2)Bias(𝜇ˆ)(cid:3)2
+Var(𝜇ˆ), (3)
soforunbiasedestimators,meansquarederrorisequaltovariance.6 ChristineHerlihy,KimberlyTruong,AlexandraChouldechova,andMiroslavDudík
1.0 method
standard
0.5 MBM
structured
regression
0.0
truth
1.0
0.5
0.0
2 3 3 5 5 6 6 9 15 18 19 20 21 23 27 28 31 34 58 66 80 94153157202244330432482525946956 group size
groups
(race: AA=African American, Wh=white, H=Hispanic, oth=other;
age: 20-39, 40-59, 60-79, 80-99; gender: M=male, F=female)
Fig.1. Pointestimatesand95%confidenceintervalsofselectionrate(SEL)andfalsenegativerate(FNR)ondiabetesdata.Confidence
intervalsofthestandardestimatorarecalculatedusingpooledvariance(seeEq.7).
Throughoutthepaper,weassumethatthestandardestimates𝑍 areunbiased.Writingthisconditionintermsofthe
𝑎
metric𝑚,weassumethatforallD ∈Δandall𝑛 ≥1,theperformancemetric𝑚satisfies
E 𝑆∼D𝑛[𝑚(𝑆) |𝑚(𝑆)isdefined] =𝑚(D), (4)
which is true for all the metrics in this paper. Substituting D𝑎 for D and𝑛 𝑎 for𝑛 in Eq. (4) implies that E[𝑍 𝑎 |
𝑍 𝑎isdefined] = 𝜇 𝑎.Intherestofthepaperwedropconditioningontheeventslike“𝑍 𝑎 isdefined,”andjustwrite
E[𝑍 𝑎] =𝜇 𝑎forsimplicity.
Sincethestandardestimates𝑍 𝑎areunbiased,theirMSEisequaltotheirvariance,whichtypicallyscalesas𝑂(1/𝑛 𝑎).
Thus,standardestimatesareaccuratewhen𝑛 islarge,butlessaccuratewhen𝑛 issmall.Unfortunately,evenfor
𝑎 𝑎
moderatelysizedevaluationdatasets,thesizesofintersectionalgroupscanbequitesmall.InFigure1,weshowstandard
estimatesofSELandFNRondiabetesdata(alongsideestimatesproducedbymethodsintroducedlaterinthepaper).
Althoughtheevaluationdatasethas5000datapoints,theintersectionalgroupsareassmallassize2,andalmosthalfof
thegroupsareofsizelessthan25,leadingtosubstantialerrorsinthestandardestimates.
4 STRUCTUREDREGRESSIONAPPROACH
Wenextdevelopastructuredregression(SR)approach,whichseekstoovercomethemainshortcomingofthestandardes-
timator:itslargevarianceforsmallgroups.Ourapproachbuildsontwomainideas.First,weleverageinformationacross
alldatapoints,notjustdatapointsin𝑆 𝑎,toestimate𝜇 𝑎,bypoolingthedataacrossrelatedgroups,forexample,across
intersectionalgroupsthatagreeinoneoftheirattributes(likeage),andbyusingadditionalexplanatoryvariables(like𝑋).
Thisisaccomplishedbyfittingaregressionmodelfor𝜇 s,with𝑍 sviewedasobservations.Second,wemaketheregres-
𝑎 𝑎
sionmodelsufficientlyexpressive,sothatitcanexpressstandardestimates.Regularizationisusedtooptimizethebias–
variancetrade-offbetweenthehigh-variancestandardestimatorandahigh-bias(butlow-variance)constantestimator.
Tostart,sincethestandardestimatesareunbiased,thatis,E[𝑍 𝑎] =𝜇 𝑎,wecanwrite
𝑍
𝑎
=𝜇 𝑎+𝜀
𝑎
setamitse
LES
setamitse
RNF
M
93-02
hto
F
93-02
hto
M
99-08
H
M
99-08
hto
M
93-02
H
F
93-02
H
F
99-08
H
F
99-08
hto
F
95-04
hto
M
95-04
H
F
95-04
H
M
95-04
hto
M
08-06
H
F
08-06
H
F
08-06
hto
M
08-06
hto
M
99-08
AA
M
93-02
AA
F
93-02
AA
M
93-02
hW
F
99-08
AA
F
93-02
hW
M
95-04
AA
M
08-06
AA
F
95-04
AA
F
08-06
AA
M
99-08
hW
F
95-04
hW
M
95-04
hW
F
99-08
hW
F
08-06
hW
M
08-06
hW7
for all 𝑎 ∈ A, where 𝜀 𝑎’s are independent random variables with E[𝜀 𝑎] = 0. We denote the variance of 𝑍 𝑎 as
𝜎 𝑎2 =Var(𝑍 𝑎)=E[𝜀 𝑎2].Inordertoestimate𝜇 𝑎,weconsideralinearmodeloftheform
𝜇
𝑎
=𝜃0+𝜽 ·𝝓𝑎
forall𝑎∈A,where𝝓𝑎 ∈R𝑑 isthefeaturevectordescribingthegroup𝑎,and𝜃0 ∈R,𝜽 ∈R𝑑 aretheparametersofthe
linearmodel.Itremainstospecifyhowtodefine𝝓𝑎,howtofittheparameters𝜃0and𝜽,andhowtoestimate𝜎 𝑎.
Definingfeaturevectors𝝓𝑎 . Thecoordinatesof𝝓𝑎arereferredtoasfeaturesanddenotedas𝜙𝑎 𝑗 for𝑗fromsomesuitable
indexset.Weallowfeaturestobelinearlydependent.Weconsiderthefollowingtypesoffeatures:
(1) Sensitivefeatures.Thesearederiveddirectlyfrom𝑎.Wealwaysincludegroup-identityindicatorsforallthe
groups𝑎′ ∈ A,yieldingfeaturesoftheform𝜙𝑎 = 1{𝑎 =𝑎′}.Thisallowsthelinearmodeltoexpressany
𝑎′
combinationofvalues𝜇 .Additionally,inordertopoolinformationacrossrelatedgroups,wealsodefine
𝑎
indicatorsforindividualattributevalues,thatis,featuresoftheform𝜙 𝑖𝑎
,𝑣
=1{𝑎[𝑖] =𝑣}for𝑖 ∈{1,...,𝑘}and
𝑣 ∈ A𝑖.Inourdiabetesexample,therearethreesensitiveattributes:race,age,andgender,with|A race| =4,
|A |=4,and|A |=2,so|A|=4·4·2=32.Weuseatotalof42sensitivefeatures:32group-identity
age gender
indicators,4indicatorsofrace,4indicatorsofage,and2indicatorsofgender.Anexampleofagroup-identity
indicatoris𝜙𝑎 andanexampleofasensitive-attributeindicatoris𝜙𝑎 .
(Hispanic,80–99,female) race,Hispanic
(2) Explanatoryfeatures.Thesearederivedfrom𝑋,𝑌,andpossibly𝑌ˆ.Wefirstfeaturize𝑋 usingsomereal-valued
functions𝑓 𝑗(𝑋),𝑗 =1,...,ℓ,andthendefineexplanatoryfeatures𝜙𝑎
𝑗
=E 𝑋∼𝑆𝑎[𝑓 𝑗(𝑋)].Additionally,when𝑌 is
categorical,wedefinefeatures𝜙𝑎
𝑦
=P 𝑌∼𝑆𝑎[𝑌 =𝑦]measuringratesofdifferentoutcomesinthegroup𝑎.In
ourdiabetesexample,weuse7explanatoryfeatures:5arederivedfromindividual-levelfeatures𝑓 ,including,
𝑗
forexample,thenumberofinpatientdaysofagivenpatientintheprioryear;andthereare2features𝜙𝑎
𝑦
correspondingto2possiblevaluesof𝑌.
(3) Interactionterms.Finally,itisalsopossibletoconsidervariousinteractionterms,bothamongfeaturesofthe
sametype(suchasinteractionsbetweengender andageindicators),orofdifferenttypes(likeinteractions
betweentheoutcome𝑌 andage).
Fittingthelinearmodel. Wefit(𝜃0,𝜽)bylassoregression[33],minimizinganℓ1-penalizedsquareloss.Toimprovethe
statisticalefficiencyoftheestimator,lossforeachgroup𝑎isweightedinverselyproportionaltothevarianceof𝑍 .
𝑎
Intuitively,sinceourmodelcanexpresstrue𝜇 ,weexpectthesquarelossoneachgrouptobeontheorderofthe
𝑎
varianceof𝑍 ,soinverseweighting“equalizesthescale”oflossesacrossgroups.Thepenalizedlossisthen
𝑎
𝐿 𝜆(𝜃0,𝜽)= ∑︁ 𝜎1 2(cid:16) 𝜃0+𝜽 ·𝝓𝑎 −𝑍 𝑎(cid:17)2 +𝜆∥𝜽∥1, (5)
𝑎∈A 𝑎
where𝜆istheregularizationhyperparameter.Denotingtheminimizerof𝐿
𝜆
(foragiven𝜆)as(𝜃ˆ 0,𝜽ˆ),weobtainthe
estimates𝜇ˆ
𝑎
=𝜃ˆ 0+𝜽ˆ·𝝓𝑎.
Tuning𝜆allowsustonavigatethebias–variancetradeoff.When𝜆=0,thelossisminimizedby𝜇ˆ
𝑎
=𝑍 𝑎,whichcan
alwaysbeexpressedbysuitable𝜃ˆ 0and𝜽ˆ,becausesensitivefeaturesincludeindicatorsofallvalues𝑎∈A.As𝜆→∞,
theoptimizationreturns𝜽ˆ ≈0.Fixing𝜽ˆ =0andoptimizingonlyovertheintercepttermyieldstheconstantsolution
𝜇ˆ 𝑎 =𝜇ˆ 0, with 𝜇ˆ 0= (cid:205) (cid:205)𝑎 𝑎∈ ∈A A𝑍 1𝑎 // 𝜎𝜎 𝑎2𝑎2 ,8 ChristineHerlihy,KimberlyTruong,AlexandraChouldechova,andMiroslavDudík
across all groups across small groups across large groups
0.008 0.015 0.0015 bias2
0.006 variance
0.010 0.0010 MSE
0.004
0.002 0.005 0.0005
0.000 0.000 0.0000
102 101 100 101 102 101 100 101 102 101 100 101
regularization parameter regularization parameter regularization parameter
Fig.2. Bias–variancetrade-offofstructuredregressionestimatesofselectionrate(SEL)ondiabetesdata.Averagedacrossallgroups,small
groups(sizeatmost25),andlargegroups(sizeabove25),across100drawsofevaluationdataset.ThescaleoftheMSEisdifferent
fordifferentgroupsizes,buttheminimumMSEisattainedaroundthesamevalueof𝜆,thankstotheweightingofthetrainingloss.
correspondingtoaweightedaverageof𝑍 s.Thissolutionhasasmallvariance,butitmaysufferfromalargebias
𝑎
whenthetruevalues𝜇 arefarfromidentical.Bytuning𝜆,wethusmovefromthestandardestimatetotheconstant
𝑎
estimate,decreasingthevariancewhileincreasingthebias.Themeansquarederroristypicallyminimizedatsome
intermediatevalueof𝜆(seeFigure2).Wetune𝜆by10-foldcross-validation,wheretheindividualfoldsareobtainedby
stratifiedsamplingofthedataset𝑆withrespecttothesensitiveattributetuple𝐴.
Estimatingvariance𝜎 𝑎2 . Variances𝜎 𝑎2areneededtodetermineweightsinouroptimizationprocedure.Asimpleapproach
is to estimate𝜎2 separately on each dataset𝑆 by using standard variance estimators (when available), or, more
𝑎 𝑎
generically,bybootstrap.Unfortunately,forsmallsamplesizes,thesevarianceestimatesthemselvesmightbeinaccurate.
Toovercomethislimitation,wepositaparametricmodelforvariance,namely,𝜎 𝑎2 =𝜎2/𝑛 𝑎,forsomeparameter𝜎.
Toestimate𝜎,weproceedintwostages.Wefirstusebootstraponeachset𝑆 toobtaintheinitialestimateof𝜎2,which
𝑎 𝑎
wedenote(𝜎ˆ 𝑎boot)2.Thus,𝑛 𝑎(𝜎ˆ 𝑎boot)2istheinitialestimateof𝜎2.Weexpectthevarianceofthisestimatetobeonthe
order𝑂(1/𝑛 𝑎).Takingaweightedaverageacrossgroups,withweightinginverselyproportionalto(1/𝑛 𝑎),yieldsour
finalestimatorof𝜎2,whichtranslatesintoanestimatorof𝜎2:
𝑎
𝜎ˆ2 =
(cid:205) 𝑎∈A𝑛 (cid:205)𝑎·(cid:2)𝑛
𝑎
𝑛(𝜎ˆ 𝑎boot)2(cid:3)
and 𝜎ˆ 𝑎2 =𝜎ˆ2 /𝑛 𝑎forall𝑎∈A. (6)
𝑎∈A 𝑎
Werefertotheseasthepooledestimatesofvariance.Inourpreliminaryexperiments,theseperformedbetterthanthe
initialestimates(𝜎ˆ 𝑎boot)2,particularlyonsmalldatasets.
4.1 Confidenceintervals
Sofarwehavefocusedonobtainingpointestimates𝜇ˆ .However,inorderfortheseestimatestobeusefulinpractice,
𝑎
wealsoneedtoquantifyouruncertaintyabouttheirvalues.Wedosobyusingconfidenceintervals.Forunbiased
estimators,likethestandardestimator𝑍 ,confidenceintervalscanbederivedbyestimatingthevarianceandthen
𝑎
usingnormalapproximation,whichworksquitewellfor𝑍 withthepooledestimatesofvariance(seeAppendixA).
𝑎
However,variance-basedapproachdoesnotworkwithlassoestimates,becausetheyarebiased—infact,theyachieve
theirimprovedaccuracybybeingbiased—andsoasimpleapproachofusingvariance-basedconfidenceintervalsor
bootstrappercentilesyieldsconfidenceintervalsthataretoonarrow.Fortunately,thereisarichliteratureonlasso-based
confidenceintervals[18,35,38].Weusetheresidualbootstraplasso+partialridge(rBLPR)approachofLiuetal.[23].As
ESM
,ecnairav
,2saib9
thenamesuggests,itisbasedonatwo-stagelasso+partialridge(LPR)pointestimator,whichfirstrunslassoasafeature-
selectionmethod,andthenfitsaridgeregressionmodel,whichonlypenalizesthefeaturesthatwerenotselectedby
lasso.TherBLPRmethodcalculatesconfidenceintervalsfortheLPRestimatebyresidualbootstrap(see[23]fordetails).
4.2 Goodness-of-fittesting
Whenpresentingtheresultsofdisaggregatedevaluations,themostcommonapproachistodisplaypointestimatesand
(sometimes)confidenceintervalsforeverysubgroup,aswesee,forexample,inFigure1.Whilethistypeofaplotcan
behelpfulinidentifyinggroupsthatmayexperiencepoorperformanceorallocation,itdoesnotprovideanarrativefor
understandinghowtheseharmsaccrue.Goodness-of-fittestingcancomplementdisaggregatedevaluationsbyallowing
ustoanswerquestionssuchas:
(1) Dointersectionalgroupsexperienceadditive,sub-additive,orsuper-additivefairness-relatedharms?Forexample,
whenamodelisfoundtoperformpoorlyforBlackwomen,isthisexplainedbythemodelperformingpoorly
forBlackpeopleandwomen,orarethereadditionalsourcesoferrorspecifictotheintersectionalgroupof
Blackwomen?Ananswertothisquestioncan,forexample,informfuturecollectionoftrainingdata.
(2) Aretherebenignfactorsthatexplainasignificantamountoftheobservedperformancevariationacrossgroups?For
example,areobserveddifferencesintheperformanceofanASRsystemattributabletosystematicallyworse
audioqualityintherecordingsforspeakersfromcertaingroups?Presenceofsuchbenignfactorsdoesnotlessen
theharm,buttheknowledgeofthefactorsthatdriveperformancedifferencescanbeusedtodesignmitigations
(forexample,denoisingalgorithmstargetedatspecifictypesofsensorsornoisecharacteristics).
Thesetypesofquestionscanbeframedasgoodness-of-fittests.Weconsidergoodness-of-fitteststhatcomparetwo
linearmodels:𝑀0,withfewerfeatures,and𝑀1,withsomeadditionalfeatures.Suchatestaskswhethertheadditional
featuresincludedinmodel𝑀1 improvethegoodnessoffitcomparedwithmodel𝑀0,wherethegoodness-of-fitis
measuredusingthesquarelossasinEq.(5).Toanswerthefirstquestionabove,wecancompareamodel𝑀0,which
includesonlyindicatorsofraceandgender,withamodel𝑀1,whichalsoincludesinteractionterms.Toanswerthe
secondquestion,wecancompareamodel𝑀′,whichonlyincludesbenignfactors,withamodel𝑀′,whichadditionally
0 1
includesindicatorsofrace,gender,andage.
Whiletherearegoodness-of-fitteststhathavebeendesignedforlassoregression[17],inthispaper,weusestandard
𝐹-testsdesignedforunregularizedlinearregression.Incontrasttotheforegoingdiscussion,wedonotincludefeatures
correspondingtotheindicatorsof𝑎(becausethesewouldtriviallyyieldstandardestimateswithperfectgoodness-of-fit,
whichinthiscasecorrespondstooverfitting).
5 EXPERIMENTS
Inthissection,weevaluatetheaccuracyofpointestimatesandcalibrationofconfidenceintervalsproducedbyour
structuredregression(SR)approach.Wealsodemonstratehowgoodness-of-fittestscanbeusedtoprovideinsights
aboutwhatdrivesthevariationofperformanceacrossgroups.
Inourevaluation,wecompareSRwithseveralbaselines.First,thereisthestandardestimator𝑍
𝑎
=𝑚(𝑆 𝑎).We
constructconfidenceintervalsfor𝑍 𝑎usingnormalapproximationwithpooledvarianceestimates(𝜎ˆ 𝑎)2fromEq.(6).
Givenaconfidencelevel𝛾 (say95%),orasignificancelevel𝛼 =1−𝛾 (say5%),weusetheconfidenceinterval
[𝑍 𝑎+𝑞 𝛼/2𝜎ˆ 𝑎, 𝑍 𝑎+𝑞 1−𝛼/2𝜎ˆ 𝑎], (7)10 ChristineHerlihy,KimberlyTruong,AlexandraChouldechova,andMiroslavDudík
where𝑞 isthe𝑝-thquantileofthestandardnormaldistribution.
𝑝
Oursecondbaselineisthemodel-basedmetrics(MBM)approach[25].Asmentionedin§2,MBMisaBayesian
approachtostructuredregressionthatmodelsthescoresproducedbyanAIsystem(like𝑌ˆ inthediabetesexample).
However,itisnotdirectlyapplicabletoperformancemetricsthatarenotbasedonscores,sowedonotuseitinthe
ASRexperiments.SimilartoSR,MBMuseslinearmodeling,andsorequiresspecifyingfeaturesforeachdatapoint.It
comeswithaboostrappingprocedureforconstructingconfidenceintervals.
WealsocompareourpointestimateswiththeclassicalJames-Stein(JS)estimator[16,30].Theestimatorworksby
shrinkingstandardestimatestowardszero(orsomeotherconstant).WeuseavariantduetoBock[4],whichisadapted
tounequalvariances(inourcase,pooledestimates𝜎ˆ 𝑎2 =𝜎ˆ2/𝑛 𝑎),givingriseto
𝜇ˆ 𝑎js =𝜇ˆ 0+(cid:18) 1−
(cid:205)
𝑎′∈( A|A
𝑛
𝑎| ′− (𝑍3 𝑎) ′𝜎ˆ −2
𝜇ˆ
0)2(cid:19) +(𝑍 𝑎−𝜇ˆ 0),
where𝜇ˆ 0=((cid:205) 𝑎∈A𝑛 𝑎𝑍 𝑎)/𝑛isaweightedaverageof𝑍 𝑎’s.ComparedwithBock’soriginalestimator[4],weuse|A|
inthenumerator,asthishasbeenpreviouslyobservedtoleadtobetterperformance[9].Since𝜇ˆjsisnotanunbiased
estimator,constructionofconfidenceintervalspresentsachallengeandwearenotawareofanystandardprocedure.
Andfinally,wecompareourmethodwiththeempiricalBayes(EB)approach[8],whichpositsahierarchical
Bayesianmodel,andthenestimates𝜇 byposteriormeans,whilefixinghyperparameterstotheirpointestimates.In
𝑎
AppendixBwederivethefollowingvariant,whichweuseinourexperiments:
(cid:18) 𝜎ˆ2 (cid:19)
𝜇ˆ 𝑎eb =𝜇ˆ+ 1− 𝜏ˆ2+𝑎
𝜎ˆ 𝑎2
(𝑍 𝑎−𝜇ˆ),
where𝜎ˆ2isthepooledestimateofvariance,and𝜏ˆ2and𝜇ˆareobtainedby
𝑎
𝜏ˆ2 =
(cid:18)(cid:205) 𝑎∈A𝑛 𝑎(𝑍 𝑎−𝜇ˆ 0)2−(|A|−1)𝜎ˆ2(cid:19)
and 𝜇ˆ=
(cid:205) 𝑎∈A𝑍 𝑎/(𝜏ˆ2+𝜎ˆ 𝑎2)
.
𝑛−(cid:205) 𝑎𝑛 𝑎2/𝑛
+
(cid:205) 𝑎∈A1/(𝜏ˆ2+𝜎ˆ 𝑎2)
SimilartoJS,wearenotawareofanystandardprocedureforconstructionofconfidenceintervals.
5.1 Diabetesexperiments
Inourfirstsetofexperiments,weexplorethescenariofromExample1usingthedatasetdevelopedbyStracketal.[31],
andpreviouslyusedinanAIfairnesstutorial[13]andtoevaluatetheMBMapproach[25].Thedatasetcontainshospital
admissionrecordsfrom130hospitalsintheU.S.overaten-yearperiod(1998–2008)forpatientswhowereadmittedwith
adiabetesdiagnosisandwhosehospitalstaylastedonetofourteendays.Itisatabulardatasetwith47featuresdescribing
eachencounter,includingpatientdemographicsandclinicalinformation(seeStracketal.[31]formoredetails).
FollowingMilleretal.[25],wefilteroutrecordswithmissingdemographicinformationandthosewithagebelow20.
Wepreprocessclinicalfeaturesasin[13].ToemulateanAIsystemthatscorespatientsforapost-dischargecare
program,weuse25%ofthedatatotrainalogisticregressionmodeltopredictwhetherthepatientwillbereadmitted
intohospitalwithin30days.Theremaining75%ofthedata,consistingof73,988hospitaladmissionsacross55,157
individuals,isusedasthegroundtruthDinallofourevaluationexperiments.
Weconsiderthreesensitiveattributes,race,age,andgender,withA race={AfricanAmerican,Hispanic,white,other},
A = {20–39,40–59,60–79,80–99},andA = {male,female}.Hospitaladmissionsarerepresentedastuples
age gender
(𝑋,𝐴,𝑌,𝑌ˆ ),where𝑋 includestheclinicalfeatures,𝐴=(race,age,gender),𝑌 ∈{0,1}indicateswhetherthepatientwas11
AUC SEL FNR FPR ACC PPV
method
0.3
standard
MBM
0.2
JS
EB
0.1
structured
regression
0.0
allsmall large allsmall large allsmall large allsmall large allsmall large allsmall large
groups groups groups groups groups groups
Fig.3. Meanabsoluteerrorofestimatesof6metricsusing5methodsondiabetesdata.Averagedacrossallgroups,smallgroups(sizeat
most25),andlargegroups(sizeabove25),across20drawsofevaluationdataset.
readmittedwithin30daysofdischarge,and𝑌ˆ ∈ [0,1]isthereadmissionprobabilitypredictedbythelogisticregression
model.Fromthegroundtruthwethensampleanevaluationdataset𝑆ofsize5000bystratifiedsamplingaccordingto𝐴.
AsinExample1,weassumethatthehospitalusesathreshold𝑟,andpatientswith𝑌ˆ ≥𝑟 areautomaticallyreferred
intothecareprogram.Wesetthethreshold𝑟 sothatP D[𝑌ˆ ≥𝑟] =0.2,meaningthatonly20%ofpatientsarereferred,
andwrite𝜋(𝑌ˆ )=1{𝑌ˆ ≥𝑟}todenotethisdecisionrule.Weconsider6performancemetrics(includingthosealready
introducedearlier),definedforany𝑝 ∈Δas
SEL(𝑝)=P 𝑝(cid:2)𝜋(𝑌ˆ )=1(cid:3), ACC(𝑝)=P 𝑝(cid:2)𝜋(𝑌ˆ )=𝑌(cid:3),
FNR(𝑝)=P 𝑝(cid:2)𝜋(𝑌ˆ )=0(cid:12) (cid:12)𝑌 =1(cid:3), FPR(𝑝)=P 𝑝(cid:2)𝜋(𝑌ˆ )=1(cid:12) (cid:12)𝑌 =0(cid:3),
PPV(𝑝)=P 𝑝(cid:2)𝑌 =1(cid:12) (cid:12)𝜋(𝑌ˆ )=1(cid:3), AUC(𝑝)=P (𝑌,𝑌ˆ)∼𝑝,(𝑌′,𝑌ˆ′)∼𝑝(cid:2)𝑌ˆ <𝑌ˆ′ (cid:12) (cid:12)𝑌 =0,𝑌′ =1(cid:3).
Thefirstfivemetrics(selectionrate,accuracy,falsepositiverate,falsenegativerate,andpositivepredictivevalue)are
derivedfromtheconfusionmatrix.ThefinalmetricistheareaundertheROCcurve;(𝑌,𝑌ˆ )and(𝑌′,𝑌ˆ′)initsdefinition
aresampledindependentlyaccordingto𝑝.
InordertoapplySR,weneedtospecifyfeatures𝝓𝑎.Assensitivefeatures,weuseindicatorsofrace,age,gender,as
wellasindicatorsofthetriple(race,age,gender).Weuse7explanatoryfeatures:indicatorsfor2possiblevaluesof𝑌,
and5additionalclinicalfeaturesdescribingthenumberofinpatientvisits,outpatientvisits,andemergencyvisitsinthe
precedingyear,numberofdiagnosesatadmission,andwhetheranyofthediagnoseswascongestiveheartfailure.For
MBM,weusethesamesetoffeatures,butwithoutthetripleindicators.
InFigure1fromearlier,pointestimatesobtainedbySRappeartobeclosertothegroundtruththanthoseobtained
bythestandardmethodandMBM.ConfidenceintervalsconstructedbySRareofsimilarsizeasthestandardconfidence
intervals,andoccasionallysmaller.MBMappearstoproducesmallerconfidenceintervalsthanSR,buttheyseemto
misstheground-truthmetricvaluesmoreoften.Wenextevaluatetheseanecdotalobservationsmoresystematically.
InFigure3,weevaluatethequalityofpointestimatesusingmeanabsoluteerror(MAE),whichisthemeandeviation
ofthepointestimatefromthetruth,averagedacross20drawsofevaluationdataset,andoverallgroups,orseparately
overthegroupsofsizeatmost25(whichwecallsmall)andgroupsofsizegreaterthan25(whichwecalllarge).We
seethatJS,EBandSRyieldsubstantiallymoreaccuratepointestimatesthanthestandardmethodandMBM.The
improvementisparticularlydramaticforsmallgroups.JS,EBandSRexhibitsimilarperformance,butSRtendstowork
bestonsmallgroups,andEBismarginallybetterthanJSandSRonlargegroups(seeFigure7inAppendixCfora
comparisonlimitedtothesethreemethods).WeuseMAEinsteadofMSE,becauseMAEvaluesareeasiertointerpret,
butMSEresultsarequalitativelysimilar.
rorre
etulosba
naem12 ChristineHerlihy,KimberlyTruong,AlexandraChouldechova,andMiroslavDudík
AUC SEL FNR FPR ACC PPV
1.0
method
0.8 standard
MBM
0.6
structured
0.4 regression
perfect
50 8090 9 5 50 8090 9 5 50 8090 9 5 50 8090 9 5 50 8090 9 5 50 8090 9 5
1.0
0.8
0.6
50 8090 9 5 50 8090 9 5 50 8090 9 5 50 8090 9 5 50 8090 9 5 50 8090 9 5
confidence level
Fig.4. Coverageandmeanrelativewidthofconfidenceintervalsfor6metricsconstructedby3methodsondiabetesdata.Averagedacross
allgroupsandacross20drawsofevaluationdataset.Relativewidthiswithrespecttothewidthofthestandardconfidenceinterval.
Table1. Goodness-of-fittestsondiabetesdata.Fromlefttoright,weconsiderincreasinglymorecomplexmodelswithagrowingsetof
featuresandreportthe𝑝-valuesofthecorrespondinggoodness-of-fittests;𝑝-valuesbelow0.05areinbold.
Estimated Goodness-of-fittest𝑝-values Modelabbreviations:
metric (comparingamoreexpressivevsalessexpressivemodel) ∅=interceptonly
expl sens expl+sens expl+sens+𝑌·sens expl=explanatoryfeatures
sens=sensitivefeatures
vs∅ vs∅ vsexpl vsexpl+sens
·=interactions
AUC 0.281 0.438 0.726 0.543
SEL 0.000 0.000 0.000 0.842
FNR 0.093 0.473 0.735 0.565
FPR 0.001 0.000 0.000 0.431
ACC 0.000 0.000 0.005 0.182
PPV 0.316 0.493 0.470 0.874
InFigure4,weshiftattentiontoconfidenceintervals.Inthetopplots,weevaluatecoverage,thatis,howoftenthe
groundtruthliesintheconfidenceintervals(across20drawsofevaluationdatasetandacrossallgroups).Weshow
coverageasafunctionoftheconfidencelevel.WeseethatbothstandardmethodaswellasSRarewell-calibrated,with
theircoverageclosetotheconfidencelevel,whereasMBMisover-confident,withcoveragewellbelowtheconfidence
level.Inthebottomplots,weevaluatethemeanrelativewidthofconfidenceintervals,meaningthemeanoftheratio
betweenthewidthofaconfidenceintervalandthewidthofthestandardconfidenceinterval.WeseethatMBMhasthe
narrowestintervals,butthisisattheexpenseofcoverage.Ontheotherhand,SRisabletomaintainwell-calibrated
coveragewhilestilldecreasingtheconfidenceintervalsbyupto20%comparedwiththestandardmethod.
Finally,inTable1,wedemonstratetheuseofgoodness-of-fittests.Fromlefttoright,weconsiderincreasinglymore
complexmodelswithagrowingsetoffeatures.Beginningwithjusttheintercept,addingexplanatoryfeatures,then
sensitivefeatures(justtheindicatorsofrace,age,andgender,butnotoftheircombination),andeventuallyinteraction
termsbetweentheoutcome𝑌 andsensitivefeatures.Thereisnoevidencetogobeyondtheintercept-onlymodelwhen
estimatingAUC,FNR,PPV.Thisdoesnotnecessarilymeanthatthereisnodisparityinperformance,butwemightnot
haveenoughdatatotell.Indeed,forinstance,confidenceintervalsforFNRinFigure1arelargeforavastmajority
ofthegroups,sointhiscaseusingSRisnotsufficienttoreduceuncertainty,andadditionaldatacollectionmaybe
egarevoc
htdiw
evitaler
naem13
Table2. Goodnessoffittestsonsyntheticdata.Fromlefttoright,weconsiderincreasinglymorecomplexmodelswithagrowingsetof
featuresandreportthe𝑝-valuesofthecorrespondinggoodness-of-fittests;𝑝-valuesbelow0.05areinbold.
Data-generating Goodness-of-fittest𝑝-values
model (comparingamoreexpressivevsalessexpressivemodel)
expl age expl+age expl+rc expl+age+rc expl+age+rc expl+age+rc+age·rc
vs∅ vs∅ vsexpl vsexpl vsexpl+age vsexpl+rc vsexpl+age+rc
model age 0.025 0.000 0.000 0.487 0.153 0.000 0.576
model expl 0.000 0.000 0.323 0.366 0.608 0.551 0.000
model age+rc 0.013 0.661 0.142 0.000 0.000 0.000 0.089
model age·rc 0.003 0.000 0.000 0.040 0.015 0.000 0.002
Modelabbreviations:∅=interceptonly,expl=explanatoryfeatures,rc=race,·=interactions
required.Ontheotherhand,thetableshowsthatbothexplanatoryandsensitivefeatureshelpwithmodelingSEL,FPR,
andACC.Infact,sensitivefeaturesimprovethefitaftertheexplanatoryfeatureshavealreadybeenadded,meaning
thatdifferencesinperformanceacrossthegroupscannotbeexplainedbythe“benign”explanatoryfeaturesalone.
5.2 Experimentswithsyntheticdata
Wenextprovideafewmoreexamplesofgoodness-of-fitanalysisonsyntheticdata.Wecontinuetousethediabetes
datasetasdescribedintheprevioussection,butwithdifferentvalues𝑌ˆ.Weconsidertheperformancemetric𝑚(𝑝)=
E 𝑝[𝑌ˆ ] (thisisquitesimilartoselectionrateorworderrorrate)andgenerate𝑌ˆ insuchawaythatground-truth
metricvalues𝜇 haveaspecificstructure.Weconsider4differentground-truthstructures,titledmodel ,model ,
𝑎 age age+rc
model age·rc,andmodel expl,accordingtothevariablestheydependon,with“+”denotinganadditivedependenceand“·”
presenceofinteractions(seeAppendixDfordetails).
InTable2,movingfromlefttoright,wetestgoodness-of-fitofmoreandmorecomplexmodels.Inthefirstrow,
ground-truthdependsonlyonage,butthereisasignificantimprovementingoodness-of-fitfrom∅toexpl,becauseof
correlationbetweenageandexpl.Aftertheexplanatoryfeatureshavebeenincluded,agestillhelps(theimprovement
fromexpltoexpl+ageissignificant),sothevariationintheperformancemetriccannotbeexplainedbythe“benign
factors”alone.Ontheotherhand,ifthedataisdrawnfrommodel ,thereisnoevidencethatageorracehelpafter
expl
explanatoryfeatureshavebeenadded.
Inthelasttworows,wedemonstratehowgoodness-of-fittestshandledatafromadditivemodelsversusmodelswith
interactiveterms.Theformercorrespondtothesituationwhenharmsexperiencedbyintersectionalgroupscombine
additively,thelatterwhenthereisanadditionalintersectionaleffect.Fortheadditivegroundtruth(model ),tests
age+rc
suggestasequenceofvariableadditionsexpl+rc+age,butthenshownosupportforincludinginteractionterms.For
thedatafrommodel ,testscorrectlyprovidesupportforaninclusionofinteractions.
age·rc
5.3 ExperimentswithASRdata
Finally,weexplorethescenariofromExample2usingthedataprovidedbyKoeneckeetal.[22]asasupplementto
theirpaperfindingracialdisparitiesincommercialASRsystems.SimilartoKoeneckeetal.[22],weusethematched
dataset,whichcontains4282snippetsacross105distinctspeakers.(Matchingensuresthatthereisthesamenumberof
snippetsfromBlackandwhitespeakersandthatthemarginaldistributionsofvariousdescriptivestatisticsmatch.)14 ChristineHerlihy,KimberlyTruong,AlexandraChouldechova,andMiroslavDudík
GOOGLE IBM AMAZON MSFT APPLE
0.6 method
standard 0.4
structured
0.2 regression
0.0
90112409721169 90112409721169 90112409721169 90112409721169 90112409721169 group size
(#snippets)
0.6
0.4
0.2
0.0
29 44 25 17 29 44 25 17 29 44 25 17 29 44 25 17 29 44 25 17 group size
(#speakers)
groups
(race: Bl=Black, Wh=white; gender: M=male, F=female)
Fig.5. Pointestimatesand95%confidenceintervalsofworderrorratesoffivedifferentASRsystems.
Foreachaudiosnippet,weareprovidedwithvariousstatistics(likedurationandwordcount),ananonymizedspeaker
id,speakerdemographics,andworderrorrates(WERs)onthatsnippetbyfiveASRsystems,developedbyGoogle,IBM,
Amazon,Microsoft,andApple.Thisinformationisencodedasatuple(𝑋,𝐴,𝑊1,...,𝑊5),where𝑋 containstheidentity
ofthespeaker,thedurationofthesnippetinseconds,andwordcount,𝐴containstwosensitiveattributes,genderand
race,withA
gender
= {male,female}andA
race
= {Black,white},andfinally,insteadof𝑌 (humantranscription)and
𝑌ˆ 1,...,𝑌ˆ 5(transcriptionsbyfiveASRsystems),wedirectlyhavethecorrespondingworderrorrates𝑊
𝑖
=wer(𝑌ˆ 𝑖,𝑌).
Theperformancemetricforthesystem𝑖isthus𝑚(𝑝)=E 𝑝[𝑊 𝑖]forany𝑝 ∈Δ.
Althoughthereappearstobealargenumberofsamples(𝑛=4282),thereareonly105distinctspeakers.Weexpect
theretobeasubstantialamountofcorrelationbetweenWERsofthesameindividual,soananalysisthattreatstheWERs
asindependentislikelytooverstatethestatisticalsignificanceoffindings,andmayarriveatincorrectconclusions,inpar-
ticular,whensomespeakershavemanymoresnippetsthanothers.Inourexperiments,wethereforepresentresultsboth
fromasnippet-levelanalysisthattreatstheWERsacrossallsnippetsasindependent(asdonein[22]),andaspeaker-level
analysisthatfirstreducesthedatatospeaker-levelWERsbytakinganaverageofWERsacrossthespeaker’ssnippets.
WefirstcomparedisaggregatedevaluationresultsobtainedbySRversusthestandardmethod.ToapplySR,we
needtospecifyfeatures𝝓𝑎.Assensitivefeatures,weuseindicatorsofraceandgender,aswellasindicatorsofthepair
(race,gender).Weuseonlyoneexplanatoryfeature,equaltothelogdurationofthesnippet.
InFigure5,wereporttheresults.Atthesnippetlevel,bothmethodsgenerallyreplicatetheresultsofKoenecke
etal.[22]:BlackmalespeakershavethelargestWER,followedbyBlackfemalespeakers,whitemalespeakers,and
whitefemalespeakers.ThemaindifferenceisthatSRsystematicallyshrinkstheWERvaluesoftheextremegroups
(Blackmalespeakersandwhitefemalespeakers)towardsthemean.Resultsatthespeakerlevelhavesubstantially
largerconfidenceintervalsthanthesnippet-levelresults,reflectingsmallergroupsizes.Also,duetosmallergroup
sizes,theSRpointestimatesareshrunktowardsthemeanmoreaggressively.
Wealsocarryoutthegoodness-of-fitanalysisofstructureofintersectionalharms.Atthespeakerlevel,wefindthat
thevariationofperformanceofallsystemsiswell-explainedbytheadditivemodelexpl+race+gender(the𝑝-valuesof
etar
rorre
drow
etar
rorre
drow
)level
teppins(
)level
rekaeps(
M
lB
F
lB
M
hW
F
hW
M
lB
F
lB
M
hW
F
hW
M
lB
F
lB
M
hW
F
hW
M
lB
F
lB
M
hW
F
hW
M
lB
F
lB
M
hW
F
hW15
addingeachvariableinturnarebelow0.003),butnotbyamodelwithinteractions.Thisisincontrastwiththesnippet-
levelanalysis,whichsupportsthemodelwithinteractions(with𝑝-valuesbelow0.001).Weinterpretthisconservatively
andconcludethatthereisevidenceforanadditivestructureofintersectionalharms,butnotforaninteractionterm.
Thisdoesnotmeanthattherearenointeractioneffects,justthatwecannotconcludethatfromthedataathand.
6 CONCLUSION
Wehaveintroducedastructuredregressionapproachtodisaggregatedevaluationandcompareditsperformance
withavarietyofbaselines.Wehaveseenthatthestructuredregression(SR),James-Stein(JS)andempiricalBayes
(EB)estimatorsallsubstantiallyimproveaccuracyofpointestimatescomparedwiththestandardapproachanda
moresophisticatedMBMbaseline.SR,JSandEBaresimpletoimplement,andarealsocloseintermsofperformance,
sothechoiceamongthemshouldbedrivenbytheirusability.Here,SRhassomeadvantages.Itsabilitytoinclude
application-specificfeaturesmakesitmoreflexible,andithasawell-developedinferenceprocedureslikeconstruction
ofconfidenceintervalsandgoodness-of-fittests.ExaminingJSandEBmorecloselyfrominferenceperspectiveinthe
contextofdisaggregatedevaluationisapromisingdirectionforfutureresearchandanecessityfortheirpracticaluse.
NotethatwehaveevaluatedSRonlyintwodomains,soanyapplicationsindomainswithdifferentcharacteristics(like
thenumberandtypesofexplanatoryandsensitivefeatures,ordatasetsize)requireadditionalvalidation.
In§2,wementionedchallengesthatariseinconceptualizationstagesofdisaggregatedevaluation.Oncethedisaggre-
gatedresultsareproduced,acomplementarysetofchallengesarisesinhowtointerpretthem.Wehaveconspicuously
omittedanalysisofregressioncoefficients,becauseinourpreliminaryexperiments,wefoundthatlassocoefficients
exhibittoomuchvarianceforreliableinference.Instead,wesuggesttousegoodness-of-fittestsandwehavedemon-
stratedseveralwayshow.Weacknowledgethatwehavejusttakensomeinitialstepsinthisarea,andtherearemany
opportunitiestoapplymoresophisticatedstatisticaltechniques.Ourexplorationalsocompletelyleavesoutimportant
sociotechnicalquestionsabouthowtodrawactionableconclusions,andhowtobestcommunicatetheresultstorelevant
stake-holders,bothofwhicharekeyintranslatingfairnessassessmentsintoareductioninfairness-relatedharms.
ACKNOWLEDGMENTS
WewouldliketothankMishaKhodakformanyhelpfuldiscussionsandinparticularforpointingouttheconnection
betweendisaggregatedevaluationandtheJames-Steinestimator.
REFERENCES
[1] JuliaAngwin,JeffLarson,SuryaMattu,andLaurenKirchner.Machinebias:There’ssoftwareusedacrossthecountrytopredictfuturecriminals.
andit’sbiasedagainstblacks.2016.https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.
[2] SolonBarocas,KateCrawford,AaronShapiro,andHannaWallach.Theproblemwithbias:Allocativeversusrepresentationalharmsinmachine
learning.In9thAnnualconferenceofthespecialinterestgroupforcomputing,informationandsociety,2017.
[3] SolonBarocas,AnhongGuo,EceKamar,JacquelynKrones,MeredithRingelMorris,JenniferWortmanVaughan,DuncanWadsworth,andHanna
Wallach.DesigningDisaggregatedEvaluationsofAISystems:Choices,Considerations,andTradeoffs.InAIES2021.ACM,May2021.
[4] M.E.Bock.Minimaxestimatorsofthemeanofamultivariatenormaldistribution.TheAnnalsofStatistics,3(1),1975.
[5] JoyBuolamwiniandTimnitGebru.Gendershades:Intersectionalaccuracydisparitiesincommercialgenderclassification.InSorelleA.Friedler
andChristoWilson,editors,Proceedingsofthe1stConferenceonFairness,AccountabilityandTransparency,volume81ofProceedingsofMachine
LearningResearch,pages77–91.PMLR,23–24Feb2018.
[6] AlexandraChouldechova,SiqiDeng,YongxinWang,WeiXia,andPietroPerona.Unsupervisedandsemi-supervisedbiasbenchmarkinginface
recognition.InEuropeanConferenceonComputerVision,pages289–306.Springer,2022.
[7] KimberléCrenshaw.Demarginalizingtheintersectionofraceandsex:Ablackfeministcritiqueofantidiscriminationdoctrine,feministtheoryand
antiracistpolitics.volume1989,Article8.1989.16 ChristineHerlihy,KimberlyTruong,AlexandraChouldechova,andMiroslavDudík
[8] BradleyEfronandCarlMorris. Stein’sestimationruleanditscompetitors—anempiricalBayesapproach. JournaloftheAmericanStatistical
Association,68(341):117–130.
[9] SergeyFeldman,MayaGupta,andBelaFrigyik.Multi-taskaveraging.InAdvancesinNeuralInformationProcessingSystems,volume25,2012.
[10] RiccardoFogliato,AlexandraChouldechova,andMaxG’Sell.Fairnessevaluationinpresenceofbiasednoisylabels.InInternationalconferenceon
artificialintelligenceandstatistics,pages2325–2336.PMLR,2020.
[11] JamesRFoulds,RashidulIslam,KamrunNaherKeya,andShimeiPan. Bayesianmodelingofintersectionalfairness:Thevarianceofbias. In
Proceedingsofthe2020SIAMInternationalConferenceonDataMining,pages424–432.SIAM,2020.
[12] JamesRFoulds,RashidulIslam,KamrunNaherKeya,andShimeiPan.Anintersectionaldefinitionoffairness.In2020IEEE36thInternational
ConferenceonDataEngineering(ICDE),pages1918–1921.IEEE,2020.
[13] TriveniGandhi,ManojitNandi,MiroslavDudík,HannaWallach,MichaelMadaio,HildeWeerts,AdrinJalali,andLisaIbañez. FairnessinAI
Systems:FromSocialContexttoPracticeusingFairlearn.Tutorialpresentedatthe20thannualScientificComputingwithPythonConference
(Scipy2021),VirtualEvent,2021.URLhttps://github.com/fairlearn/talks/tree/main/2021_scipy_tutorial.
[14] CanerHazirbas,JoannaBitton,BrianDolhansky,JacquelinePan,AlbertGordo,andCristianCantonFerrer.TowardsmeasuringfairnessinAI:The
casualconversationsdataset.IEEETransactionsonBiometrics,Behavior,andIdentityScience,4(3):324–332,2022.doi:10.1109/TBIOM.2021.3132237.
[15] UrsulaHébert-Johnson,MichaelKim,OmerReingold,andGuyRothblum. Multicalibration:Calibrationforthe(computationally-identifiable)
masses.InInternationalConferenceonMachineLearning,pages1939–1948.PMLR,2018.
[16] W.JamesandC.Stein.Estimationwithquadraticloss.InProc.FourthBerkeleySymposiumonMathematicalStatisticsandProbability,pages361––379,
1961.
[17] JanaJanková,RajenDShah,PeterBühlmann,andRichardJSamworth.Goodness-of-fittestinginhighdimensionalgeneralizedlinearmodels.
JournaloftheRoyalStatisticalSocietySeriesB:StatisticalMethodology,82(3):773–795,2020.
[18] AdelJavanmardandAndreaMontanari.Confidenceintervalsandhypothesistestingforhigh-dimensionalregression.JournalofMachineLearning
Research,15(82):2869–2909,2014.
[19] DisiJi,RobertL.Logan,PadhraicSmyth,andMarkSteyvers.Activebayesianassessmentforblack-boxclassifiers,2020.URLhttps://arxiv.org/abs/
2002.06532.
[20] DisiJi,PadhraicSmyth,andMarkSteyvers.CanITrustMyFairnessMetric?AssessingFairnesswithUnlabeledDataandBayesianInference,2020.
URLhttps://arxiv.org/abs/2010.09851.
[21] MichaelKearns,SethNeel,AaronRoth,andZhiweiStevenWu.Preventingfairnessgerrymandering:Auditingandlearningforsubgroupfairness.
InInternationalconferenceonmachinelearning,pages2564–2572.PMLR,2018.
[22] AllisonKoenecke,AndrewNam,EmilyLake,JoeNudell,MinnieQuartey,ZionMengesha,ConnorToups,JohnR.Rickford,DanJurafsky,and
SharadGoel.Racialdisparitiesinautomatedspeechrecognition.ProceedingsoftheNationalAcademyofSciences,117(14):7684–7689,2020.doi:
10.1073/pnas.1915768117.URLhttps://www.pnas.org/doi/abs/10.1073/pnas.1915768117.
[23] HanzhongLiu,XinXu,andJingyiJessicaLi.StatisticaSinica,30(3):1333–1355,2020.
[24] MichaelMadaio,LisaEgede,HariharanSubramonyam,JenniferWortmanVaughan,andHannaWallach.Assessingthefairnessofaisystems:Ai
practitioners’processes,challenges,andneedsforsupport.Proc.ACMHum.-Comput.Interact.,6(CSCW1),apr2022.doi:10.1145/3512899.URL
https://doi.org/10.1145/3512899.
[25] AndrewC.Miller,LeonA.Gatys,JosephFutoma,andEmilyFox.Model-basedmetrics:Sample-efficientestimatesofpredictivemodelsubpopulation
performance.InProceedingsofthe6thMachineLearningforHealthcareConference,pages308–336.PMLR,2021.URLhttps://proceedings.mlr.press/
v149/miller21a.html.
[26] MathieuMolinaandPatrickLoiseau.Boundingandapproximatingintersectionalfairnessthroughmarginalfairness.AdvancesinNeuralInformation
ProcessingSystems,35:16796–16807,2022.
[27] ZiadObermeyer,BrianPowers,ChristineVogeli,andSendhilMullainathan.Dissectingracialbiasinanalgorithmusedtomanagethehealthof
populations.Science,366(6464):447–453,2019.doi:10.1126/science.aax2342.URLhttps://www.science.org/doi/abs/10.1126/science.aax2342.
[28] VihariPiratla,SoumenChakrabarti,andSunitaSarawagi.Activeassessmentofpredictionservicesasaccuracysurfaceoverattributecombinations.
CoRR,abs/2108.06514,2021.URLhttps://arxiv.org/abs/2108.06514.
[29] ReneeShelby,ShalalehRismani,KathrynHenne,AJungMoon,NegarRostamzadeh,PaulNicholas,N’MahYilla-Akbari,JessGallegos,Andrew
Smart,EmilioGarcia,etal. Sociotechnicalharmsofalgorithmicsystems:Scopingataxonomyforharmreduction. InProceedingsofthe2023
AAAI/ACMConferenceonAI,Ethics,andSociety,pages723–741,2023.
[30] C.Stein.Inadmissibilityoftheusualestimatorforthemeanofamultivariatedistribution.InProc.ThirdBerkeleySymposiumonMathematical
StatisticsandProbability,pages197–206,1956.
[31] BeataStrack,JonathanDeshazo,ChrisGennings,JuanLuisOlmoOrtiz,SebastianVentura,KrzysztofCios,andJohnClore. ImpactofHbA1c
MeasurementonHospitalReadmissionRates:Analysisof70,000ClinicalDatabasePatientRecords.BioMedresearchinternational,2014:781670,04
2014.doi:10.1155/2014/781670.
[32] LatanyaSweeney.Discriminationinonlineaddelivery:Googleads,blacknamesandwhitenames,racialdiscrimination,andclickadvertising.
Queue,11(3):10–29,mar2013.ISSN1542-7730.doi:10.1145/2460276.2460278.URLhttps://doi.org/10.1145/2460276.2460278.
[33] RobertTibshirani.Regressionshrinkageandselectionviathelasso.JournaloftheRoyalStatisticalSociety.SeriesB(Methodological),58(1):267–288,
1996.17
[34] BorisvanBreugel,NabeelSeedat,FergusImrie,andMihaelavanderSchaar.Canyourelyonyourmodelevaluation?improvingmodelevaluation
withsynthetictestdata.arXivpreprintarXiv:2310.16524,2023.
[35] SaravandeGeer,PeterBühlmann,Ya’acovRitov,andRubenDezeure.Onasymptoticallyoptimalconfidenceregionsandtestsforhigh-dimensional
models.TheAnnalsofStatistics,42(3):1166–1202,2014.
[36] AngelinaWang,VikramVRamaswamy,andOlgaRussakovsky.Towardsintersectionalityinmachinelearning:Includingmoreidentities,handling
underrepresentation,andperformingevaluation.InProceedingsofthe2022ACMConferenceonFairness,Accountability,andTransparency,pages
336–349,2022.
[37] HildeWeerts,MiroslavDudík,RichardEdgar,AdrinJalali,RomanLutz,andMichaelMadaio.Fairlearn:AssessingandimprovingfairnessofAI
systems.JournalofMachineLearningResearch,24(257):1–8,2023.URLhttp://jmlr.org/papers/v24/23-0389.html.
[38] Cun-HuiZhangandStephanieS.Zhang.Confidenceintervalsforlowdimensionalparametersinhighdimensionallinearmodels.Journalofthe
RoyalStatisticalSociety.SeriesB(StatisticalMethodology),76(1):217–242,2014.18 ChristineHerlihy,KimberlyTruong,AlexandraChouldechova,andMiroslavDudík
AUC SEL FNR FPR ACC PPV
1.0
method
0.8 pooled var.
separate var.
0.6 bootstrap
percentiles
0.4 perfect
50 8090 9 5 50 8090 9 5 50 8090 9 5 50 8090 9 5 50 8090 9 5 50 8090 9 5
confidence level
Fig.6. Comparisonofmethodsforconstructingconfidenceintervalsforthestandardestimator.Showingcoverageofconfidenceintervals
constructedforsixmetricsondiabetesdata,averagedoverallgroupsandover20drawsofevaluationdataset.Confidenceintervals
constructedfrompooledvarianceareclosetotheperfectline(correspondingtocoverageequaltoconfidencelevel).Confidence
intervalsderivedfromseparatelyestimatedvariancesundercovertruevalues.
A CONFIDENCEINTERVALSFORSTANDARDESTIMATES
Weconsiderthreemethodsforconstructingconfidenceintervalsforstandardestimators𝑍 atagivenconfidence
𝑎
level𝛾 (e.g.,95%),orequivalently,atasignificancelevel𝛼 =1−𝛾 (e.g.,5%).
Twoofthemethodsarebasedonnormalapproximationandtakeform
[𝑍 𝑎+𝑞 𝛼/2𝜎ˆ 𝑎, 𝑍 𝑎+𝑞 1−𝛼/2𝜎ˆ 𝑎],
where𝑞 isthe𝑝-thquantileofthestandardnormaldistributionand𝜎ˆ2isanestimateofvarianceof𝑍 .Weconsider
𝑝 𝑎 𝑎
eitherthepooledestimateofvariancederivedinEq.(6),ortheestimate(𝜎ˆ 𝑎boot)2obtainedbyboostrapon𝑆 𝑎.Thethird
methodusesbootstrappercentileson𝑆 .
𝑎
InFigure6,wecomparecoveragepropertiesoftheresultingconfidenceintervalsondiabetesdata.Confidence
intervalsconstructedfrompooledvarianceestimatesarewell-calibrated,withcoveragecloselymatchingtheirconfidence
level.Theothertwomethodssubstantiallyundercovertruevalues.
B DERIVATIONOFTHEEMPIRICALBAYESESTIMATOR
WepositthefollowinghierarchicalGaussianmodel:
𝜇 𝑎 ∼N(𝜇,𝜏2 ) forall𝑎∈A,
(8)
𝑍 𝑎 ∼N(𝜇 𝑎,𝜎 𝑎2 ) forall𝑎∈A,
where𝜎 isknownand𝜇and𝜏 areunknownhyperparameters.Weobservevalues𝑍 andneedtopredict𝜇 .
𝑎 𝑎 𝑎
Conditioningonthepriorandobservations,weobtaintheposteriordistribution
𝜇 𝑎 |𝜇,𝜏,𝑍 𝑎 ∼N (cid:16) 𝜇ˆ 𝑎eb,(𝜎ˆ 𝑎eb )2(cid:17)
wheretheposteriormeanandvarianceareequalto
𝜏2 𝜏2
𝜇ˆ 𝑎eb =𝜇+
𝜏2+𝜎 𝑎2
·(𝑍 𝑎−𝜇) and (𝜎ˆ 𝑎eb )2 =
𝜏2+𝜎 𝑎2
·𝜎 𝑎2. (9)
TheempiricalBayesapproachtakespointestimatesofthehyperparameters𝜇and𝜏,andplugsthemintoEq.(9).
Theresulting𝜇ˆebisusedasapointestimatefor𝜇 andtheresulting𝜎ˆebisusedtoconstructcredibleintervalsfor𝜇 .
𝑎 𝑎 𝑎 𝑎
egarevoc19
Weestimate𝜏2byanalyzingasuitablesumofsquares(similarlyasintheanalysisofvariance).Tostart,notethatif
wemarginalizeout𝜇 fromEq.(8),wefindthatthevalues𝑍 areconditionallyindependentgiven𝜇and𝜏,with
𝑎 𝑎
𝑍 𝑎 |𝜇,𝜏 ∼N(𝜇,𝜏2 +𝜎 𝑎2 ) forall𝑎∈A. (10)
Foreach𝑎,weconsiderthesquare(𝑍 𝑎−𝜇ˆ 0)2,where
∑︁
𝜇ˆ 0= 𝑤 𝑎𝑍 𝑎 with 𝑤 𝑎 =𝑛 𝑎/𝑛forall𝑎∈A.
𝑎∈A
Theexpectationof(𝑍 𝑎−𝜇ˆ 0)2thentakesthefollowingform:
(cid:34)(cid:32) (cid:33)2(cid:12) (cid:35)
(cid:104)(cid:16) (cid:17)2(cid:12) (cid:105) ∑︁ (cid:12)
E 𝑍 𝑎−𝜇ˆ 0 (cid:12) (cid:12)𝜇,𝜏 =E 𝑤 𝑎′𝑍 𝑎′ −(1−𝑤 𝑎)𝑍 𝑎 (cid:12) (cid:12)𝜇,𝜏
𝑎′≠𝑎 (cid:12)
(cid:34)(cid:32) (cid:33)2(cid:12) (cid:35)
∑︁ (cid:12)
=E 𝑤 𝑎′(𝑍 𝑎′ −𝜇)−(1−𝑤 𝑎)(𝑍 𝑎−𝜇) (cid:12) (cid:12)𝜇,𝜏
𝑎′≠𝑎 (cid:12)
= ∑︁ 𝑤 𝑎2 ′(𝜏2 +𝜎 𝑎2 ′)+(1−𝑤 𝑎)2 (𝜏2 +𝜎 𝑎2 ) (11)
𝑎′≠𝑎
= ∑︁ 𝑤 𝑎2 ′(𝜏2 +𝜎 𝑎2 ′)+(1−2𝑤 𝑎)(𝜏2 +𝜎 𝑎2 ), (12)
𝑎′∈A
whereEq.(11)followsbyEq.(10)andconditionalindependenceof𝑍 s.MultiplyingEq.(12)by𝑤 andsummingacross
𝑎 𝑎
all𝑎,weobtain
(cid:34) (cid:12) (cid:35)
E ∑︁ 𝑤 𝑎(cid:16) 𝑍 𝑎−𝜇ˆ 0(cid:17)2(cid:12) (cid:12) (cid:12)𝜇,𝜏 = ∑︁ 𝑤 𝑎2 ′(𝜏2 +𝜎 𝑎2 ′)+ ∑︁ 𝑤 𝑎(1−2𝑤 𝑎)(𝜏2 +𝜎 𝑎2 )
𝑎∈A (cid:12) 𝑎′∈A 𝑎∈A
= ∑︁ 𝑤 𝑎(1−𝑤 𝑎)(𝜏2 +𝜎 𝑎2 ).
𝑎∈A
Werearrangethefinalexpressiontoobtainanunbiasedestimateof𝜏2:
𝜏ˆ2 =
(cid:205) 𝑎∈A𝑤 𝑎(𝑍 𝑎−𝜇ˆ 0)2−(cid:205) 𝑎∈A𝑤 𝑎(1−𝑤 𝑎)𝜎 𝑎2
. (13)
1−(cid:205) 𝑎∈A𝑤 𝑎(1−𝑤 𝑎)
SinceE[𝑍
𝑎
|𝜇,𝜏] =𝜇andVar[𝑍
𝑎
|𝜇,𝜏] =𝜏2+𝜎 𝑎2,weestimate𝜇bytakingaweightedaverageof𝑍 𝑎s,withtheweights
proportionaltotheinverseofthevariance,butwith𝜏ˆ2pluggedinfor𝜏2:
𝜇ˆ=
(cid:205) 𝑎∈A𝑍 𝑎/(𝜏ˆ2+𝜎 𝑎2)
. (14)
(cid:205) 𝑎∈A1/(𝜏ˆ2+𝜎 𝑎2)
Thelastmissingpieceis𝜎2,forwhichweusethepooledestimatefromEq.(6).
𝑎
Combiningitalltogether,weusethepooledestimatesofvariance𝜎ˆ 𝑎2andtheweightedmean𝜇ˆ 0alongsideobservations
𝑍 𝑎tocalculate𝜏ˆ2usingEq.(13);thenwecalculate𝜇ˆusingEq.(14);andfinallywecalculate𝜇ˆ 𝑎eband(𝜎ˆ 𝑎eb)2usingEq.(9).
C ADDITIONALDIABETESEXPERIMENTS
InFigure7,weevaluatethequalityofthepointestimatesproducedbythethreebest-performingmethods.InFigure8,
wecomparetheperformanceofthestructuredregressionapproachwithanintercept-onlymodel.20 ChristineHerlihy,KimberlyTruong,AlexandraChouldechova,andMiroslavDudík
AUC SEL FNR FPR ACC PPV
0.15
method
JS
0.10
EB
structured
0.05 regression
0.00
allsmall large allsmall large allsmall large allsmall large allsmall large allsmall large
groups groups groups groups groups groups
Fig.7. Meanabsoluteerrorofestimatesof6metricsusing3best-performingmethodsondiabetesdata.Averagedacrossallgroups,small
groups(sizeatmost25),andlargegroups(sizeabove25),across20drawsofevaluationdataset.
AUC SEL FNR FPR ACC PPV
0.15 method
intercept
0.10 only
structured
regression
0.05
0.00
allsmall large allsmall large allsmall large allsmall large allsmall large allsmall large
groups groups groups groups groups groups
Fig.8. Comparisonofstructuredregressionwiththeintercept-onlymodel.Showingmeanabsoluteerror,averagedacrossallgroups,
smallgroups(sizeatmost25),andlargegroups(sizeabove25),across20drawsofevaluationdataset.
D SYNTHETICDATAGENERATION
Togeneratesyntheticdata,weusetheground-truthdiabetesdatasetasdescribedin§5.1,butwithdifferentvalues𝑌ˆ.
Specifically,weconsidertheperformancemetric𝑚(𝑝) =E 𝑝[𝑌ˆ ](thisisquitesimilartoselectionrateorworderror
rate)andgenerate𝑌ˆ insuchawaythatground-truthmetricvalues𝜇 haveaspecificstructure.Weconsider4different
𝑎
ground-truthstructures,whichwerefertoasmodel ,model ,model ,andmodel ,dependingwhich
age age+rc age·rc expl
variablestheydependonandhow,with“+”denotingadditivedependenceand“·”presenceofinteractions:
Modelname Ground-truthvalueof𝜇 Data-generatingprocess
𝑎
model age 𝜇 𝑎 =0.35−0.3·𝜙 a𝑎 ge,40–60 𝑌ˆ =Bernoulli(𝜇 𝐴)
model expl 𝜇 𝑎 =−0.93+0.16·E D𝑎[𝑋 number_diagnoses] 𝑌ˆ =N(𝜇 𝐴,0.1)
model age+rc 𝜇 𝑎 =0.65−0.15·𝜙 a𝑎 ge,40–60−0.45·𝜙 r𝑎 ace,white 𝑌ˆ =Bernoulli(𝜇 𝐴)
model age·rc 𝜇 𝑎 =0.32−0.27·𝜙 a𝑎 ge,40–60·𝜙 r𝑎 ace,white 𝑌ˆ =Bernoulli(𝜇 𝐴)
ModelcoefficientshavebeenchosentoensurethatinallcasesE [𝑌ˆ ] ≈0.27and(cid:0)Var [𝑌ˆ ](cid:1)1/2 ≈0.44.
D D
rorre
etulosba
naem
rorre
etulosba
naem