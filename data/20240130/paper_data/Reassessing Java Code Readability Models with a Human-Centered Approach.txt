Reassessing Java Code Readability Models with a
Human-Centered Approach
AgniaSergeyuk OlgaLvova SergeyTitov
agnia.sergeyuk@jetbrains.com olga.lvova@jetbrains.com sergey.titov@jetbrains.com
JetBrainsResearch JetBrains JetBrainsResearch
Belgrade,Serbia Yerevan,Armenia Paphos,Cyprus
AnastasiiaSerova FaridBagirov EvgeniiaKirillova
anastasiia.serova@jetbrains.com farid.bagirov@jetbrains.com evgeniia.kirillova@jetbrains.com
JetBrains JetBrainsResearch JetBrainsResearch
Paphos,Cyprus Paphos,Cyprus Munich,Germany
TimofeyBryksin
timofey.bryksin@jetbrains.com
JetBrainsResearch
Limassol,Cyprus
ABSTRACT 1 INTRODUCTION
ToensurethatLargeLanguageModels(LLMs)effectivelysupport Recently, the field of Large Language Models (LLMs) has been
userproductivity,theyneedtobeadjusted.ExistingCodeRead- rapidlydeveloping,enablingawiderangeofapplicationsinvari-
ability(CR)modelscanguidethisalignment.However,thereare ousdomains,includingsoftwaredevelopment(forreview,see[14]).
concernsabouttheirrelevanceinmodernsoftwareengineering Inprogramming,code-fluentLLMsareusedascodingassistants
sincetheyoftenmissthedevelopers’notionofreadabilityandrely —AIismainlyusedforcodegenerationandcompletion.Italso
onoutdatedcode.ThisresearchassessesexistingJavaCRmodels cansuggestcoderefactoring,commitmessages,codeexplanations,
forLLMadjustments,measuringthecorrelationbetweentheirand andhasmanyotherpracticalprogramming-relatedapplications,
developers’evaluationsofAI-generatedJavacode.UsingtheReper- e.g.,Copilot,1 Codeium,2 CodeWhisperer,3 etc..AsAI-supported
toryGridTechniquewith15developers,weidentified12keycode programmingtoolsgrowandevolve,thisreshapesthecultureand
aspectsinfluencingCRthatwereconsequentlyassessedby390pro- contextofsoftwaredevelopment[2].Thisgrowthalsoinfluences
grammerswhenlabeling120AI-generatedsnippets.Ourfindings thedesignofAI-basedtoolswithafocusonimprovingtheiraccu-
indicatethatwhenAIgeneratesconciseandexecutablecode,it’s racy,optimizingmemoryusage,andenhancingothercompetitive
oftenconsideredreadablebyCRmodelsanddevelopers.However, parameters(e.g.,,[10,11,25,34]).
alimitedcorrelationbetweentheseevaluationsunderscoresthe However,whilethesetechnicalaspectsareimportant,itisequally
importanceoffutureresearchonlearningobjectivesforadjusting crucialtoconsidertheirimpactontheend-users.Therefore,besides
LLMsandontheaspectsinfluencingCRevaluationsincludedin improvingtechnicalperformance,academicdiscourseshouldad-
predictivemodels. dressimplicationsforusersatisfactionandoveralluserexperience.
ResearchrevealedthatAIsupportsprogrammers’productivityin
KEYWORDS
someways,but,ontheotherhand,developersnowtendtospend
CodeReadability,CodeReadabilityModels,RepertoryGridTech- moretimereviewingcodethanwritingit[23].Sincethetimespent
nique,AI-GeneratedCode,Human-ComputerInteraction oncodecomprehensionisdirectlyrelatedtoCodeReadability—
themorereadablethecodeis,thelesstimethedeveloperneedsto
ACMReferenceFormat:
understandit—wecanoptimizetheprogrammers’workflowby
AgniaSergeyuk,OlgaLvova,SergeyTitov,AnastasiiaSerova,FaridBagirov,
providingsuggestionsfromanLLMthatalignwiththeirnotionof
EvgeniiaKirillova,andTimofeyBryksin.2023.ReassessingJavaCodeRead-
abilityModelswithaHuman-CenteredApproach.InProceedingsofThe32nd CodeReadability.Inacademicresearch,CodeReadabilityisusually
IEEE/ACMInternationalConferenceonProgramComprehension(ICPC2024). definedasasubjective,mostlyimplicithumanjudgmentofhow
ACM,NewYork,NY,USA,11pages.https://doi.org/XXXXXXX.XXXXXXX easythecodeistounderstand[3,26,29].
Currently,withinthefieldofartificialintelligenceandmachine
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
learning,researchersareworkingtowardsenhancingthesuitability
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation ofLLMsforspecificdomains[20],forinstance,naturallanguage
onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM processing[6]andmedicalapplications[15],ratherthanfocusing
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora onmeetinguserexpectations.LLMsuitabilityforspecificdomains
fee.Requestpermissionsfrompermissions@acm.org.
ICPC2024,April15–16,2024,Lisbon,Portugal
©2023AssociationforComputingMachinery. 1Copilothttps://github.com/features/copilot
ACMISBN978-1-4503-XXXX-X/18/06...$15.00 2Codeiumhttps://codeium.com/
https://doi.org/XXXXXXX.XXXXXXX 3CodeWhispererhttps://aws.amazon.com/codewhisperer/
4202
naJ
62
]ES.sc[
1v63941.1042:viXraICPC2024,April15–16,2024,Lisbon,Portugal Sergeyuketal.
isenhancedthroughfine-tuning,wherethemodel’sweightsand with varying experience levels. This discovery extends the net-
parametersaremodifiedusingtask-specificdata.However,this workoffactorsconsideredinCodeReadabilityresearch,offeringa
fine-tuningprocessalsooffersthepotentialforaligningLLMswith foundationforcreatingandmaintainingcodethatismoreeasily
humans.ApproacheslikeReinforcementLearningfromHuman understoodbyhumans,therebyenhancingdeveloperproductivity
Feedback[17],PreferenceElicitationandPreferenceLearning[1], andcollaboration.
In-ContextLearning[4]canbeharnessedtoadjustmodelsbased AdvancementsinReadabilityevaluation—ourfindingofthe
onhumanpreferences,resultinginimprovedusability,accessibility, lowlevelofalignmentbetweenexistingCodeReadabilitymodels
andusersatisfaction. andhumanjudgmentsandthelowagreementlevelbetweenhuman
SuchanalignmentintermsofCodeReadabilityrequiresapro- raterswithinthecontextofAI-generatedcodeaddsanoveldimen-
foundunderstandingofdevelopers’perceptionsofwhatconstitutes siontothecurrentunderstandingofCodeReadabilityassessments.
readablecode.Thisinvolveshavingasubstantiallabeleddatasetof Itunderscorestheneedforfurtherresearchonreadability-related
codethatcaptureshumanpreferences.Alternatively,tomitigate codeaspectsandthehuman-alignedCodeReadabilitymodels.
theresource-intensivenatureofdatalabeling,onecanemploya Futureresearchdirections—toencouragefurtherexploration
modeltrainedonasmallerdatasettopredictlabelsforunlabeled andinnovation,wehavemadeourstudy’scodeanddataavailable.4
data[30].Forthispurpose,existingpredictivemodelsofCodeRead-
2 BACKGROUND
ability[5,21,26,28]maypotentiallybeused,despitebeinginitially
createdasafoundationforstaticanalysistoolsaimedathelping 2.1 Code-fluentLLMsasCodingAssistants
developersmaketheircodemorereadable.
OurresearchaimstoassesswhethertheexistingCodeReadabil- The development of code-fluent LLMs as coding assistants has
itymodelsevaluatethereadabilityofAI-generatedcodesimilarlyto fundamentallytransformedthecodingexperience.Severalresearch
howdevelopersdo.Forthispurpose,we(a)createdanAI-generated studieswereconductedtoexaminehowhumansandAIinteractin-
datasetofcode,(b)evaluatedAI-generatedcodewithexistingCode depthandtounderstandhowLLMsinfluenceprogrammerbehavior
Readability models, (c) understood what aspects of code devel- duringcodingactivities[19,23,31].
opersrelyonwhenassessingCodeReadability,(d)conducteda Theresearchersfoundthatdevelopersspendapproximately50%
human evaluation of AI-generated code in terms of readability, ofcodingtimeininteractionwiththeLLM,with35%dedicated
(e)comparedtheassessmentsmadebyCodeReadabilitymodels todouble-checkingsuggestionssincethegeneratedcodeislim-
anddevelopers.Thismakesourworknotonlyinnovativebutalso itedinmeetingbothfunctionalandnon-functionalrequirements.
significantinthecontextofmodernsoftwareengineering. DevelopersalsostruggletocomprehendtheoutputsofLLM.
Overall,ourresearchrepresentsapioneeringeffortindirecting Thestudieshighlightedtheimportanceofaligningmodelswith
attentiontowardsCodeReadabilityfromadeveloper’sperspec- humanrequirements,reducingcoders’timeandmentaleffortto
tiveandassessingwhethertheexistingCodeReadabilitymodels comprehendAIcodingassistants’suggestions.Developersneedto
appropriatelyrepresenttheirstandpointwithinthecontextofAI- quicklycomprehendthecodeproposedbyanAIcodingassistant
generatedcode.UsingtheRepertoryGridtechnique,atoolfrom beforeintegratingitintoaprojectandimplementinganychanges.
cognitivepsychology,weidentified12dimensionsthatdevelopers Acriticalaspectofthisprocessiswhatiscommonlyreferredto
consideressentialforCodeReadability.Wethengeneratedadataset as Code Readability— the ease with which developers can read
ofreadability-labeledJavacodesnippets,onwhichwereevaluated and understand code. In this notion, Code Readability forms a
existingCodeReadabilitymodels. perceivedbarriertocomprehensionthatdevelopersmustovercome
OurresearchindicatesaweakcorrelationbetweencurrentCode toefficientlyworkwithcode[3,26,28].
Readabilitymodelsanddeveloperevaluations,pointingtoasignifi-
2.2 AlignmentofCode-fluentLLMs
cantgapinthesemodels’abilitytoreflectdevelopers’perspectives
onCodeReadability.Itunderscorestheneedfordevelopingmore Addressingdevelopers’requirementsregardingthereadabilityof
accurateCodeReadabilitymetricsandmodels. model-suggestedcodemayinvolvevariousfine-tuningmethods.
Thecontributionsofthisresearchcanbesummarizedasfollows: Specifically, in addition to the fine-tuning process itself, when
UnderstandingAIOutputReadability—toourbestknowl- thedevelopermodifiesthemodel’sweightsandparameters,con-
edge,wewerethefirsttoevaluatethereadabilityofcurrentAI- trastive[18]andreinforcement[17]learningarevaluabletoolsfor
generated code using established Code Readability models and thispurpose.Theimplementationofthesemethodsencompasses
humanratersandcomparetheseevaluations.Ourfindingsindicate thedefinitionofalearningobjective—informationaboutwhat
thatwhenthegeneratedsolutionismeaningful,concise,andad- outputis“desirable”andwhatisnot.
herestostyleguidelines,ittendstobeconsideredreadablebothby Creatinganewdatasetforadjustingmodelsrequiressubstantial
CodeReadabilitymodelsandhumanassessors.Thisunderscores humanandcomputationalresources.Itinvolvesthecollectionof
theeffectivenessofAIsystems,tosomeextent,inproducingcode vastamountsofdata,preliminarytesting,processing,andanaly-
thatiseasytoreadbyhumans,butalsoemphasizestheimportance sis,followedbymultiplerefinementiterationstoensureoptimal
ofongoingdevelopmentandrefinementofLLMstoimproveboth performance.Tominimizeannotationeffortsofdata-demanding
thereadabilityandfunctionalityofAI-generatedcode. machine-learningtasks,severalapproachesaresuggested.These
Human-centeredrepresentationofReadability—ourre-
searchcontributestothefieldbyidentifying12specificaspectsof 4Foraccesstothecodeanddatausedinourresearch,visitthelink:https://zenodo.
codethatinfluenceCodeReadability,asperceivedbydevelopers org/records/10550937.ReassessingCodeReadabilityModels ICPC2024,April15–16,2024,Lisbon,Portugal
includereusingpre-trainedmodelsonnewtasks(i.e.,transferlearn- Dorn’sapproachextendedbeyondsyntacticanalysistoinclude
ing[12]),usingbothlabeledandunlabeleddatafortraining(i.e., structuralpatterns,visualperception,alignment,andnaturallan-
semi-supervisedlearning[32]),usingamodelthatwastrainedon guageelements,transformedintonumericalvectors.Usinglogistic
asmallersubsetofdatatopredictthelabelsoftheunlabeleddata regression, Dorn identified seven key features that most signif-
(i.e.,activelearning[30]). icantlycorrelatedwithhumanreadabilityjudgments,achieving
OurresearchfocusedonexploringthepotentialofexistingCode aSpearmancorrelationof.72.Thismodelalsosurpassedthere-
Readabilitymodels[5,21,22,26,28,29]tobelearningobjectives trainedBuseandWeimermodelby5%inF-measure,underscoring
toguidetheprocessoffine-tuning.TheseCodeReadabilitymodels, thevalueofincorporatingabroaderrangeofcodecharacteristics
designedtorepresentandpredicthumanevaluations,canaidinthe intoreadabilityassessments.
fine-tuningprocessofLLMs,essentiallymodelingthereadability Scalabrino’sModel.Scalabrino,Linares-Vásquez,andOliveto
ofthetrainingdata. expandedonCodeReadabilityresearchbyproposingaComprehen-
ByexaminingwhetherexistingCodeReadabilitymodelsaccu- siveModelthatintegratessyntactic,visual,structural,andtextual
ratelyreflecthumanevaluations,wecanpotentiallyrepurposethem elementsofcode[28].Theycuratedadatasetfrom200methods
asbenchmarksfortheadaptationandtuningofLLMs,reducing acrossfourJavaprojects,ratedonreadabilityby9CSstudents.
thetime,effort,andresourcestypicallyrequiredforthistask. MergingtheirdatasetwiththosefromBuseandWeimerand
Dorn,theycomputedfeaturesfromallprecedingmodelsplusnew
2.3 ModelsofCodeReadability
textualfeatures.Usinglogisticregression,theycraftedabinary
Buse’sModel.BuseandWeimerpioneeredamethodtoquantify CodeReadabilityclassifierwith104features.Thismodelsurpassed
CodeReadabilityusingmachinelearning[3].Theycraftedabi- thepreviousones,showingatleasta6.2%increaseinaccuracy,un-
naryclassifiermodelbasedoncodefeaturestheyhypothesized derscoringthebenefitoftextualalongsidestructuralandsyntactic
wouldinfluenceCodeReadability.Thismodelwastrainedusing featuresinCodeReadabilityprediction.
readabilityassessmentsfrom120computersciencestudentson Mi’sModel.Mi,Hao,Ou,andMaintroducedadeep-learning-
100Javacodesnippets,ratedona1to5scale.Despitethesubjec- basedCodeReadabilitymodelthattranscendstheneedformanual
tivenatureofreadabilityandonlymoderateinter-rateragreement, featureengineeringbyleveragingvisual,semantic,andstructural
their model successfully predicted readability and aligned with coderepresentations[21].Theirapproachutilizedanimagerecog-
establishedsoftwarequalitymetrics. nitionmodelforvisualfeatures,wordembeddingsforsemantic
Themodelutilizedmetricssuchascodestructure,complexity, understanding, and a character matrix for structural attributes,
and documentation, which were processed into vectors for ma- culminatinginacompositeneuralnetwork.
chinelearning.Theclassifierdemonstratedan80%accuracyrate Thismodel,withathreefoldapproachtofeatureextraction,fed
andaPearsoncorrelationof.63withthemeanhumanreadability intoaneuralnetworkclassifier,outperformedtraditionalmachine
scores,suggestingitseffectivenessinCodeReadabilityprediction. learningmodelsonacombineddatasetfromBuseandWeimer,
Aprincipalcomponentsanalysisfurtherrevealedthatsixoutof Dorn,andScalabrino.Withanaverageaccuracyof85.3%,itmarked
the25featuresaccountedformostofthevarianceinreadability, animprovementrangingfrom3.5%to13.8%,showcasingthepo-
highlightingthepotentialforamorestreamlinedmodel. tentialofdeeplearninginautomatingCodeReadabilityevaluation.
Posnett’sModel.Posnett,Hindle,andDevanbuintroduceda
SimplerModelofCodeReadabilitybasedononlythreefeatures, 2.4 SuitabilityforFine-TuningofCode
whichsurpassedtheperformanceofBuseandWeimer’searlier
ReadabilityModels
model[26].LeveragingthesamedatasetasBuseandWeimer,they
applied a forward stepwise refinement for feature selection, in Amongthefivedescribedmodels,fourareacknowledgedasstate-of-
contrasttotheoriginalbackwardstepwiseapproach.Conversely, the-art:Posnett’s,Dorn’s,Scalabrino’s,andMi’s.Whilepioneering,
Posnett’steammanuallyincorporatedfeatures,assessingtheircon- themodelintroducedbyBuseandWeimerhassubsequentlybeen
sequentimpactonthemodel’squality.Theselectionoffeatures outperformedbytheadvancementsofitscounterparts.Theremain-
intheirmodelwasdrivenbytheauthors’intuitionandfamiliarity ingfourmodelsofCodeReadabilitydo,however,exhibitcertain
withHalstead’ssoftwaresciencemetrics. shortcomingsintheirsuitabilityforthealignmentofLLMs.
Theirfindingsindicatethatalesscomplexmodel,incorporating Tobeginwith,thesemodelsaremorecode-centricasopposed
Halsteadvolume,tokenentropy,andlinecount,canmoreeffectively tohuman-centric.Theyprimarilyrelyoneitherhand-craftedor
predictCodeReadability.However,thisstudydidnotovercomethe hiddenfeatures.Thiscanresultinmisinterpretationorneglectof
limitationsofBuseandWeimer’slackofoperationalizationofCode importantCodeReadabilityaspects,leadingtosub-optimaldeci-
Readability,asitreliedonthesamedatasetanddidnotassessthe sionsduringLLMs’adjustment.Thehand-craftedfeaturemethod,
codeevaluators’expertiseandunderstandingofCodeReadability. applied in Posnett’s, Dorn’s, and Scalabrino’s models, may not
Dorn’sModel.Concernsaboutthegeneralizabilityofprevious fullyencapsulateallthecrucialfactorsaffectingCodeReadability
Java-basedreadabilitymodelsledDorntodevelopaGeneralSoft- sincethefeaturesselectedforthesemodelsaresolelybasedon
wareReadabilityModel,whichbroadenedthescopetomultiple researchers’intuition.Incontrast,Mi’sdeeplearningmodeldoes
programminglanguages[5].Dorn’sdatasetencompassed360code notexplicatespecificfeaturesitusesfortheassessments.Therefore,
snippetsfromopen-sourceprojectsinJava,Python,andC++with itispossiblethatnoneoftheabovementionedmethodsadequately
CUDA,varyingfrom10to50lines.Over5,000participantsrated encapsulatesCodeReadabilityaspectsdevelopersperceiveasimpor-
thesnippets’readabilityonascalefrom1to5. tant.Theneglectofhuman-centricconsiderationsinthesemodelsICPC2024,April15–16,2024,Lisbon,Portugal Sergeyuketal.
Authors #ofJavaSnippets #ofAnnotators #ofFeatures Accuracy∗
BuseandWeimer[3] 100 120 25 0.80
Posnettetal.[26] TakenfromBuseandWeimer 3 0.83
Dorn[5] 120 2789 7 F1∼0.81
Scalabrinoetal.[28] 200 9 104 0.85
Mietal.[21] Takenfrompreviousworks Unknown 0.853
∗Thepercentageofcodesnippetscorrectlyclassifiedasreadable/unreadable.
Table1:SomecriticalcharacteristicsofSOTAModelsofCodeReadability.
mightleadtoamismatchbetweentheAI’sunderstandingofCode onevaluatingthereadabilityofthecurrentoutputsgeneratedby
Readabilityandthatofreal-worlddevelopers. LLMsandexamininghowwellthesehumanevaluationsalignwith
Secondly,thereseemstobealackofastandardizedoruniversally assessmentsmadebyexistingCodeReadabilitymodels.
acceptedunderstandingofCodeReadabilityamongparticipants.
3.1 DataCollection
TheratingsofCodeReadabilityinthedatasetsofallfourstudiesare
basedonsubjectiveassessmentsusingaLikertscale,andinmost WeselectedCodeGolfchallenges5togenerateAI-basedcodesnip-
cases,theseratingswereprovidedbycomputersciencestudents pets,aligningwiththebrevityrequiredbytheCodeReadability
whomaynotfullyrepresentthediversityofthedeveloperpopula- modelsunderstudy.Afterextractingall95tasks,wenotedthat
tion.Thisraisesconcernsaboutthegeneralizabilityoftheresults. specifictasksexistedintwoversions—longandshort.Inourre-
LLMsadjustedusingbiaseddatamayendupreinforcingthewrong search,weretainedthelongerversionssincesolvingthemrequired
patterns,renderingtheoutputslessreadablefordevelopers. amoresubstantialpieceofcode.Furthermore,weexcludedtasks
Therefore,whiletheaforementionedCodeReadabilitymodels thatrequiredUnicodecharacters,likechesssymbolsoremojis.This
canbepotentiallyusedforfine-tuningpurposes,theirlimitations selectionprocessresultedinafinalsetof64programmingproblems
mustbeaddressedforeffectiveandaccurateoptimizationofLLMs. thatwerewell-suitedforpromptingtheLLM.Consequently,we
Itisessentialtocheckifthesemodelsarerobustandcomprehensive employedChatGPT3.5Turbo(duetothetimingofthestudy)to
enoughandadequatelycapturingthediversereadabilitynotions generateJavalanguagesolutionsforthesetasks.
fromdifferentdevelopers’perspectives. Using ChatGPT 3.5 Turbo, we generated Java solutions with
threepromptvariations:Basic,Intermediate,andExpert,following
2.5 PresentResearch bestpracticeguidelinesforspecificityandclarity[33].Basic:to
writeastraightforwardJavaprogramforagiventaskwithin50
TocheckwhethertheevaluationsmadebyexistingJavaCodeRead-
lines,withoutexplanations;Intermediate:todevelopaJavaprogram
abilitymodelsarealignedwiththeevaluationsofCodeReadability
adheringtoformattingandbestpracticesinefficiency,readabil-
madebyJavadevelopersinthecontextofAI-generatedcode,the
ity,andmaintainability,limitedto50lines;Expert:tocreatean
presentstudyaimstoanswerfourresearchquestions:
RQ1.HowreadableisJavacodeproducedbyacode-fluentLLM, advancedJavaprogramfollowingbestprogrammingpracticesand
formattingconventions,cappedat50lines,usinganexamplefor
accordingtoexistingCodeReadabilitymodels’evaluations?
RQ2.WhataretheaspectsofAI-generatedcodethatinfluence guidance.
To ensure that the brevity of generated snippets did not im-
itsreadabilityaccordingtoJavadeveloperswithvaryinglevelsof
pacttheirmeaningfulness,twoauthorsofcurrentresearchlooked
experience?
RQ3.HowdoJavadeveloperswithvaryingexperiencelevels throughthem,validatingifthegenerationswereacceptableforour
research.Then,snippetsweretestedonexecutability.Executable
assessthereadabilityofthecodeproducedbyacode-fluentLLM?
RQ4.Isthereacorrelationbetweenhumans’andmodels’eval- snippetswerefurtherassessedforadherencetoalengthlimitof50
lines,asdefinedbytheexaminedCodeReadabilitymodels.
uationsofCodeReadabilityinAI-generatedJavacode?
Theoutcomeofthesetestswas120Javacodesnippetsofso-
Toanswerthesequestions,weconductedastudyconsistingof
lutionsfor49tasks—15taskswereexcludedfromtheoriginal
threemainparts:(a)creatinganAI-generateddatasetofcode,(b)
tasks’listbecausethemodelcouldnotproducemeaningfuland
conductingastructuredinterviewapplyingtheRepertoryGrid
executablesolutionswithin50linesofcode.Therefore,inthefinal
technique[8],and(c)alarge-scalelabelingsurvey. Thestudywas
setofsnippets,somesolvedthesametaskbutweregeneratedby
conductedinlinewithourcompany’sethicalstandards,adhering
promptsofdifferentlevels.
tothevaluesandguidelinesoutlinedintheICC/ESOMARInterna-
tionalCode[13].Thestudy’sschematicsoverviewcanbefoundin 3.2 DataAnalysis
theonlineappendix.
ToassessthereadabilityofAI-generatedJavacode,weutilized
3 AI-GENERATEDDATASETOFJAVACODE existing predictive models for Code Readability: Posnett’s [26],
Dorn’s[5],Scalabrino’s[28],andMi’s[21].Usingthesemodels,
ThecrucialaspectofourresearchisasetofAI-generatedcodesnip- welabeledeachsnippetbasedonthereadabilityscoreasassessed
petsneededfortheinterviewandlabelingsurvey.Wedeliberately byeachmodel.Wealsoassignedthesnippetalabeloftheoverall
createdcodesnippetsfromscratchratherthanemployingsegments readabilityaccordingtoallfourmodels—“Readable”(R)ifthree
fromexistingopen-sourceprojectssinceourprimaryobjectivewas
toenhancehuman-AIinteraction.Consequently,ourfocuscenters 5CodeGolfgamehttps://code.golf/ReassessingCodeReadabilityModels ICPC2024,April15–16,2024,Lisbon,Portugal
ormoremodelsdeemedthesnippetreadable;“Unreadable”(U)if therefore“priming”themonaunifiedimplicitunderstandingof
theoppositewastrue;and“NotDefined”(N)ifthesnippetwas CodeReadability.
consideredbothreadableandunreadablebytwomodelseach. ToinvestigatetheaspectsinfluencingCodeReadabilityfrom
Posnett’smodelwaspresentedintheformofaformula[26]that thedevelopers’perspective,weconductedone-hourlongonline
weappliedtocalculatetheprobabilityofsnippetsbeingreadable. interviews,employingtheRepertoryGridtechnique[16].Details
Dorn’smodelwassharedwithusbytheauthoralongwiththescript aboutthistechnique,includinghowitfacilitatedthecollectionof
usedforcalculatingthereadabilityofsnippets.Scalabrino’smodel importantCodeReadabilityaspectsandtheirqualitativeanalysis,
hasanopen-sourcescriptfordeterminingreadability,6whichwe areprovidedfurtherinthissection.
utilizedinouranalysis.Finally,Mi’smodelwasemployedfrom
theiropenGitHubrepository.7 4.1 Sample
Toassesstheimpactofthepromptingstrategyonsnippetread-
Weinterviewed15JavaDevelopersofdifferentexperiencelevels—
ability,weconductedpairwisecomparisonsbetweengroupsusing
Beginner,Intermediate,andAdvanced—withfiveparticipantsin
the𝜒2testwitheachreadabilitymodel.
eachgroup.Asgratitudeforparticipatingintheresearch,external
3.3 Findings developerswereoffereda100USDAmazoneGiftCardordeclined
anycompensation.
TheabovementionedanalysiswasaimedtoanswerRQ1.How
Theparticipantswerepickedfromthosedeveloperswhoprevi-
readableisJavacodeproducedbyacode-fluentLLM,accord-
ouslyparticipatedinourresearchandgavepermissionforfuture
ingtoexistingCodeReadabilitymodels’evaluations?
contact,aswellasinternalcolleagues.Theywereinvitedtopartici-
Inourresearch,whenAIgeneratesJavacodethatisbothexe- pateinascreeningsurveywheretheysharedtheyearstheyspent
cutableandconcise,wefoundthatitismostlyconsideredreadable inprofessionalprogramming,includinginternships.Participants
accordingtotheexistingCodeReadabilitymodels(seeTable2). werealsoaskedtoself-evaluatetheirproficiencyinJava.
Theresultsalsohighlightthesignificantvarianceacrossdifferent Basedontheirresponses,wecategorizedparticipantsintothree
CodeReadabilitymodels’classificationofthesameAI-generated groupsthateffectivelyrepresentoursampleforresearchpurposes
Javacodesnippets.Ofparticularnoteisthehighcontrastbetween (wetreatedoutliercasese.g.,“Advancedwithlessthan3yearsof
Posnett’smodel,whichfoundmostsnippetsreadable,andDorn’s, experience”asimpossibleandusedthemasabasisforexcluding
Scalabrino’s,andMi’smodels,whichfoundamoresignificantpro- participantsfromfurtheranalysis):
portionofsnippetsunreadable. Beginners:developerswithlessthanoneyearofcodingexperi-
Giventhissignificantvariabilityinevaluations,thequestionof encewhoconsideredthemselvesIntermediateandthosewithless
howcloselythesemodelsalignwithhumanassessmentsofCode thanfiveyearsofexperiencewhoratedthemselvesasBeginners.
Readabilitybecomesparticularlysalient. Intermediate:developerswithmorethanoneyearbutlessthan
WealsonotedinstanceswhereAI-generatedcodewasunexe- tenyearsofcodingexperiencewhoself-identifiedasIntermediate.
cutable,excessivelylengthy,andunreadable.Thishighlightsthat WealsoincludeddeveloperswhoconsideredthemselvesBeginners
whileimpressivestrideshavebeenmadeinthereadabilityofAI- withoverfiveyearsofexperienceandthosewhodefinedthemselves
generatedcode,thereisstillsignificantroomforimprovement. asAdvancedwithlessthanthreeyearsofexperience.
Additionally,theresultsofthe𝜒2testrevealedthattheprompts
Advanced:developerswithmorethanthreeyearsofprogram-
didnothaveastatisticallysignificanteffectonreadabilityforeither mingexperiencewhoviewedthemselvesasAdvancedandthose
group(p>.05).Giventhisfinding,aswellasthefactthatsolutions withovertenyearsofprofessionalexperiencewhoclassifiedthem-
frequentlyhadobservabledistinctionsbasedonprompttypes,we selvesasIntermediate.
opted to treat them as a unified set of snippets for subsequent
researchpurposes.Wejustifythisapproachbecauseatnopointin 4.2 RepertoryGridTechnique
ourresearchparticipantscouldseetheentiresetofsnippets.This
TheRepertoryGridtechnique[8]wasproposedbyGeorgeKelly
meanstheycouldnotencounterallpossiblesolutionsforasingle
in1955,anditcentersonhisPersonalConstructTheory[16].Ac-
task.If,byrandomassignment,suchasituationdidoccurduring
cordingtothistheory,peopleperceivetheworldthroughbipolar
thesurveystage,oursurveymetricswouldaccountforit,andit
cognitiveconstructslikeHappy/SadorGood/Bad.Theseconstructs
wouldnotaffectthevalidityofourresults.
playakeyroleinshapinganindividual’sunderstandingandinter-
4 INTERVIEW pretationoftheworldaroundthem,influencingtheirbehavior.In
essence,theRepertoryGridtechniqueisatoolemployedtoelicit
PreviousstudiesofCodeReadabilitymodelsreliedonthecreation
theseindividual’simplicitconstructsbyutilizingtheirlanguage
oftheirdatasetsutilizingsubjectivereadabilityratingswithouta
andcognitiveframeworks,ensuringthatresearchfindingsremain
consistentdefinition.Toaddressthislimitation,inourresearch,
groundedintheperson’sreality.
wesetouttoprovidetheproxyofstandardizedunderstandingof
Thetechniquebuildsoninterviewsforconstructs’elicitation
CodeReadabilitybyprovidingannotatorsreadability-relatedcode
andfurtherquestionnairestovalidatetheobtainedconstructs[7].
aspectstoevaluatepriortoCodeReadabilityinthelabelingprocess,
Thisprocedureinitiatesbydeterminingaspecificareaofinvesti-
gation.Inourcase,thisareaisCodeReadability.Itcontinuesby
6Scalabrinoetal.,Experimentalmaterial,rawdata,andreadabilitytool:https://dibt.
identifyingrelevantelementswithinthedefinedarea.Forourstudy,
unimol.it/report/readability
7Mietal.,CodeReadabilitymodel:https://github.com/swy0601/readability-Features theelementsareAI-generatedcodesnippets.ThenextsteprequiresICPC2024,April15–16,2024,Lisbon,Portugal Sergeyuketal.
Posnett’s Dorn’s Scalabrino’s Mi’s
#ofReadable 118(97.52%) 77(64.89%) 73(60.83%) 82(68.42%)
#ofUnreadable 2(2.48%) 43(35.11%) 47(39.17%) 38(31.58%)
Table2:ReadabilityofAI-GeneratedSnippetsAccordingtoExistingCodeReadabilityModels.
theindividualtocompareandcontrasttheseelementsintriadsto asynchronousdiscussionstosortandgroupconstructsintocoher-
extract constructs that, from their perspective, are essential for entclustersuntilaunanimousdecisionwasreached.
distinguishingdifferentelementsofadefinedarea—inourcase,
4.5 Findings
determiningcodesnippetsasreadableorunreadable.Forexample,
suchcodecharacteristicsas“Nesting”mightbeaCodeReadability TheRepertoryGridinterviewwasutilizedtoanswerRQ2.What
constructrepresentedbypoles“Codeisflatandlinear”and“Code aretheaspectsofAI-generatedcodethatinfluenceitsread-
isoverlynested”.Thischaracteristicmaybeelicitedwhenthree abilityaccordingtoJavadeveloperswithvaryinglevelsof
codesnippetswithvariouslevelsofnestingarecompared. experience?
Oncetheconstructsaregathered,theindividualsareaskedto Thistechniquefacilitatedtheidentificationofoverall123bipolar
rateeachelement(i.e.,codesnippet)againstallidentifiedconstructs. CodeReadabilityconstructs.Fordescriptivestatistics,seeTable3.
This is usually done using a Likert scale. In the context of our
research,thisphasewaspresentedasthelabelingprocessdescribed
#ofTripletsSeen #ofConstructsElicited
belowinSection5.
Group Mean SD Mean SD
Thecollectedratingsaresubsequentlysubjecttoananalysis
aimingtodelineatetheindividual’sperceptionofthearea.Inour
Beginner 4.2 0.8 7.8 1.3
research,thisunderstandingwasusedtoassessthedegreeofalign-
Intermediate 5.2 1.6 8.0 1.2
mentbetweenexistingCodeReadabilitymodelsanddevelopers.
Advanced 5.0 2.0 8.8 2.6
4.3 DataCollection
Table3:DescriptivestatisticsforelicitedCodeReadability
Weselectedrandomsnippetsfromtheinitialsetof120,described constructs.
intheSection3,andgroupedthemintotenuniquetriplets.The
Subsequently,thelistofconstructswassubjectedtoaconsol-
tripletscomprisedallpotentialcombinationsofthe“Readable”(R),
idationprocessledbythreeexpertswhoidentified15,11,and8
“Unreadable”(U),and“NotDefined”snippets—RRR,RRU,RRN,
clustersofconstructsassociatedwithCodeReadability,respectively.
RUU,RUN,RNN,UUU,UUN,UNN,andNNN.Thesetripletsserved
Thenextstepinvolvedasynchronousdiscussionamongtheex-
asillustrativematerialfortheRepertoryGridinterview,providing
perts,where26distinctclusters,someofwhichweresimilarin
diverseexamplesofCodeReadability.
meaningtoeachother,weremergedintoafinalsetof12unique,
Duringthesession,participantsfirstprovidedbackgroundinfor-
comprehensiveclusters(seeTable4).
mationabouttheireducation,languageskills,andprogramming
tools,establishingacontextfortheirinsights.Inthetrainingphase,
CodeAspect Overall Beg. Int. Adv.
theyevaluatedthreecodesnippetsforCodeReadability,selecting
anddiscussingthemostsimilarpairanddifferentiatingthethird. CodeStructure 30 12 6 12
This process revealed their Code Readability constructs. In the CodeStyle 15 4 7 4
mainphase,participantsengagedinmultipleroundsofevaluating Naming 14 5 3 6
tripletsofcodesnippets,withtheorderoftripletsrandomizedto SufficientContextualInfo 11 4 5 2
preventsequencebias.Duetotheone-hourlimit,mostparticipants FamiliarCodePatterns 9 2 3 4
reviewedfourtripletsbeforeendingwithadebriefingforquestions ReadingFlow 8 3 3 2
andadditionalcomments. InlineActions 8 3 2 3
UnderstandableGoal 7 2 3 2
4.4 DataAnalysis CodeLength 7 1 5 1
Nesting 6 1 1 4
Duringthedataanalysis,wecompiledacomprehensiveyetcon- VisualOrganization 4 1 1 2
ciselistofkeyaspectsinfluencingCodeReadabilityfromallthe MagicNumbersUsage 3 0 1 2
constructsobtainedduringtheinterviews.
Table4:Readability-RelatedCodeAspectsandtheNumber
TheinitialcollectionofbipolarconstructsrepresentingCode
ofTheirMentionsAmongParticipants.
Readabilityaspectswaspresentedindependentlytothreeexperts:
aJavaDeveloperwhohadnopriorknowledgeofthestudy,aData Later, we converted this set into a rating list by assigning a
Analystwhoisoneoftheauthorsbutwasn’tinvolvedintheinter- bipolardescriptionforeachcharacteristic.Thesedescriptionswere
views,andaCognitivePsychologistwhoisoneoftheauthorsand chosen in synchronous expert discussions as the most relevant
conductedseveralinterviewsduringthestudy.Theirtaskwastocat- andrepresentativepairofreadability-relatedcodeaspectsobtained
egorizetheseaspectsintologicalanddistinctgroupsandthenname fromtheparticipants.
thesegroupsofconstructs,drawingontheirspecializedknowledge Theconstructsdescribedabovecollectivelyhighlightthatfrom
andexpertise.Afterward,theexpertsengagedinsynchronousand thedevelopers’pointofviewCodeReadabilityrequiresabalanceReassessingCodeReadabilityModels ICPC2024,April15–16,2024,Lisbon,Portugal
betweenbeinginformativeandconcise,structuredyetsimple,and evaluatedCodeReadabilityofthesnippetusingthelistofbipolar
conformingtoaknownpatternwhilebeingvisuallyappealingand characteristicswithafive-pointscalemeasuringhowmuchthe
user-friendly.Forthedistributionofmentionsofspecificclusters codeleanstothereadableorunreadablepoleofcharacteristics.
intheinterview,refertoTable4. Furthermore,respondentsansweredasingle-choicequestionto
Weemployedthesecharacteristicsofcodetocreateleadingques- identifywhetherthecodewasreadable.
tionsinafurtherlabelingsurvey.Thisway,raterscanassessread-
abilityconsistentlyandshareacommonunderstandingofwhich
5.4 DataAnalysis
aspectsofcodetoconsiderwhenevaluatingCodeReadability.
ThenextstepininvestigatingthecorrelationbetweenhumanCode
5 LABELINGSURVEY
ReadabilityevaluationsandpredictionsgeneratedbyCodeRead-
Weinitiatedalabelingsurveytocraftacarefullylabeleddatasetof abilitymodelsinvolvedassigningreadabilitylabelstoeachcode
AI-generatedJavacodesnippets,reflectingtheirCodeReadability snippetbasedonhumanassessments.Forthat,theresultsfromthe
basedonhumanperceptionsoftheconcept.Thislabeleddataset surveywereemployed.
wasthenemployedtoevaluatetheexistingCodeReadabilitymodels WecalculatedKrippendorff’salphatoassesstheagreementof
aslearningobjectivesforLLMfine-tuning. readabilityevaluationsamonglabelerssincethismetriccanhandle
casesofvariousnumbersofraters[9].Afterthat,codesnippets
5.1 Sample thatreceivedareadableratingfrommorethan50%ofraterswere
markedasreadableandtheremainingasunreadable.Asaresult
Thesamplewasgatheredbysendingthesurveylinktothelistof
ofthisprocess,afifthreadabilitylabelwasaddedtoeachcode
thoseJavaprogrammerswhopreviouslyparticipatedinoursurveys
snippetinthedataset,andthefourlabelswereassignedbasedon
andresearchandgavepermissionforfuturecontact.Noneofthem
evaluationsbytheCodeReadabilitymodels.
hasparticipatedintheinterviewstageofthecurrentstudy.Asa
Followingthislabelingprocess,weassessedeachmodel’salign-
thankyou,participantsweregiventheopportunitytoenteradraw
mentwithhumanratings.ThiswasdoneusingaMatthewscorrela-
foroneoffive100USDAmazoneGiftCardsoranequivalent-value
tioncoefficientasanestablishedmetrictomeasurethecorrelation
companyproductpack.
betweenbinarydata.Bythis,weidentifiedthemodelthatshowed
Theonlinedatalabelingprocessengaged390Javaprogrammers
the most substantial level of correlation with human ratings of
ofdifferentexperiencelevels,eachcontributingatleastonesnippet
CodeReadability.
evaluation.Onaverage,participantscompletedsevenassessments.
AlloftheparticipantsareproficientintheJavaprogramming
language.Amongthem,78%knowmorethanthreeprogramming 5.5 Findings
languages.MostparticipantsareDevelopers,Programmers,and
WeconductedthelabelingsurveytoanswerRQs3and4about
SoftwareEngineers—362of390.Additionally,SoftwareArchitects
agreementinassessmentsbetweendevelopersandexistingCode
andDevOpsEngineersparticipated,aswellasprogrammerswith
Readabilitymodels.
othervariousjobroles.
RQ3.HowdoJavadeveloperswithvaryingexperience
Todeterminetheparticipants’proficiency,wegatheredinfor-
levelsassessthereadabilityofcodeproducedbyacode-fluent
mation about their years of programming experience and their
LLM?
self-assessedJavaexpertise.Thedistributionofprogrammingex-
TheagreementlevelonCodeReadabilityassessmentsbetween
perienceamongparticipantsisasfollows:27individualswithless
humanswasfoundtobelowingeneral(𝛼 =.14)aswellaswithin
than 1 year, 60 with 1–2 years, 55 with 3–4 years, 44 with 5–6
different levels of expertise —𝛼 = .04 for Beginners,𝛼 = .1 for
years,42with7–8years,20with9–10years,and142withmore
Intermediate,𝛼 =.16forAdvanced.Thereisapossibilitythatthe
than10years.36participantsself-assessedasBeginners,162as
difficultyofassigningthereadabilitylabeltothecode,reflectedin
Intermediate,and192asAdvanceddevelopers.
theseresults,isdictatedbythesubjectivityofCodeReadabilityand
5.2 Materials itsaspects.Wediscusstheseresultsinmoredetailfurtherinthe
paper(seeSection6).
Inthelabelingsurvey,weutilizedmaterialsgatheredduringthe Toassignaparticularreadabilitylabeltoeachcodesnippet,given
previousstagesoftheresearch—thesetofAI-generatedJavacode thelowlevelofagreementamonglabelers,welabeledsnippetsac-
snippetsandtheratinglistofCodeReadabilityaspects.Theprimary cordingtothemajorityofvotes.Iftherewerenodefinitivemajority
purposeofthislistinourresearchwastoprovideunifiedguidance ofvotes(e.g.,fivereadablelabelsandsix—unreadable),thesnip-
tothelabelersduringtheassessmentofCodeReadability,ensuring pets,whichwerefertoas“Ambiguous”snippets,wereexcluded
thatthelabelersevaluatedcodeconsistentlyandwithafocuson fromfurtheranalysissinceifhumanraterscouldnotagreeontheir
keyreadability-relatedaspects. readability,thereisnopointinmeasuringcomplianceofthemodels
withthisevaluation.Asaresultofthisprocess,wecategorized109
5.3 DataCollection
codesnippets—93as“Readable”and16as“Unreadable”,discarding
Onthegreetingpageofthesurvey,participantsgavetheirconsent 11codesnippetsfromfurtheranalysis.
andprofessionalbackgroundinformation.Afterthat,theywere InFigure1,meanscoresforCodeReadabilityaspectsareshown
presentedwitharandomlyassignedJavacodesnippetfromthe foreachsnippetgroup.Thex-axisrepresentsreadabilityaspects,
previouslygenerateddatasetofsnippets(seeSection3).Participants andthey-axisrepresentsscoresonafive-pointscalemeasuringICPC2024,April15–16,2024,Lisbon,Portugal Sergeyuketal.
∗Thischaracteristicis“Readable”whenNeutraland“Unreadable”attheextremes
Figure1:VisualisationofmeanscoreforeachCodeReadabilityaspect.
howmuchthecodeleanstothereadableorunreadablepoleof continuousdevelopmentandrefinementtoenhanceboththeread-
characteristics,asaskedinthesurvey. abilityandfunctionalityofAI-generatedcode.Furtherresearchinto
RQ4.Isthereacorrelationbetweenhumans’andmodels’ whydiscrepanciesinmodels’evaluationsoccur—whatdefinitions,
evaluationsofCodeReadabilityinAI-generatedJavacode? standards,oraspectsofCodeReadabilityareemployedineach
TheresultsindicatethatScalabrino’smodel[28]exhibitsamod- model—isneeded.
eratecorrelationwithhumanevaluations(MCC=0.325).Incontrast, Moreover,thesediscrepanciessuggestthatattemptingtoformu-
othershaveaweakercorrelation—MCC=0.033forMi’smodel, lateoneall-encompassingdefinitionforCodeReadabilitymaynot
MCC=-0.036forPosnett’s,andMCC=-0.038forDorn’s—which fullycapturethecomplexlandscapeoftherealmasdeterminedby
makesthemnotverysuitableforservingasthelearningobjective diversemodels.Thesefindingssupportedourresearchonhowwell
inthefine-tuningofcode-fluentLLMs,particularlyregardingthe thesemodelsalignwiththehumannotionofCodeReadability.
readabilityoftheiroutput.
WhileAIsystemscanproducehuman-likeandeasy-to-read
code,thereisstillroomforimprovement.Also,notablediffer-
6 DISCUSSION
encesinreadabilityassessmentsofAI-generatedcodeamong
Themaingoalofourresearchwastocheckwhetherevaluations variousCodeReadabilitymodelshighlighttheneedforinves-
of Code Readability made by existing models are aligned with tigatingtheextenttowhichthesemodelsrepresenthuman
theevaluationsofCodeReadabilitymadebyJavadevelopersby judgmentsandwhatfactorscontributetothem.
answeringthefollowingRQs.
RQ2.WhataretheaspectsofAI-generatedcodethatinflu-
RQ1.HowreadableisJavacodeproducedbyacode-fluent enceitsreadabilityaccordingtoJavadeveloperswithvarying
LLM,accordingtoexistingCodeReadabilitymodels’evalua- levelsofexperience? ToanswerthisRQ,weemployedtheReper-
tions? ThereadabilityofJavacodesnippetsproducedbyacode- toryGridtechniquetoextract12distinctcodeaspectsthatinfluence
fluentLLMwasfirstevaluatedbyeachoftheexistingCodeReadabil- CodeReadabilityperceptionsamongJavadeveloperswithvarying
itymodels,includingPosnett’s[26],Dorn’s[5],Scalabrino’s[28], expertiselevels.
andMi’s[21]. Weempiricallyconfirmedthat,fromadeveloper’sperspective,
Overall,wefoundthatAI-generatedJavacode,whenitisboth CodeReadabilitydependsonbalancingbrevityandunderstand-
executableandconcise,ismostlyreadableaccordingtotheexisting ing.Thecode’spurposeplaysacrucialrole,withreadablecode
CodeReadabilitymodels.However,theresultsyieldedsignificant clearlyexpressingitsgoal,whileunreadablecodemayobscureits
variabilityacrossevaluationsfromthesemodels.Forinstance,ac- intention.Readablecodefollowsalogicalstructure,withactions
cordingtoPosnett’smodel,only2.48%ofsnippetsinthedataset separatedintodistinctlinesandnamedconstantsforclarity.Onthe
wereunreadable.Contrarily,theassessmentfromDorn’smodel otherhand,unreadablecodemightbeexcessivelynested,contain
wasconsiderablystricter,identifying35.11%ofthesnippetsasun- multipleactionsinasingleline,anduseundefined“magicnum-
readable.Similarly,Scalabrino’smodelandMi’smodelfoundnearly bers”.Readablecodeoftenadherestostyleguidesandmaintainsa
39.17%and31.58%ofsnippetsunreadable,respectively. visuallybalanceddistributionofcolorblocks,potentiallyrelatedto
ThesefindingsunderscorenotonlytheeffectivenessofAIsys- syntaxhighlighting.Incontrast,unreadablecodemaysufferfrom
tems in producing human-like code but also the importance of poorformattinganddistractingcolorblockpatterns.ReassessingCodeReadabilityModels ICPC2024,April15–16,2024,Lisbon,Portugal
Thereadability-relatedcodeaspectsidentifiedinourstudyalign Theinherentsubjectivityinreadabilityassessmentsbydevel-
withpriorresearch. opers,evenwhenguidedbykeyreadability-relatedaspects,
Forinstance,Fakhouryandcolleaguesexaminedover500code highlightstheneedformoregeneralizable,operationalized,
readabilityenhancementcommitsandfoundthatiftheyexplicitly andvalidateddefinitionsofCodeReadability.Theaimoffuture
aimed at improving readability, they often involved changes in researchcouldbetoexploretherelationshipsbetweenCode
Complexity, Documentation, and Size metrics. Their study also Readabilitycharacteristicsobtainedfromdevelopersandtheir
notedthatthemostsubstantialreadabilityimprovementswereseen weightsinone’sdecisionwhetherthecodeisreadableornot.
inimportstatements,codeformattingandstyle,andthereduction
of“magicnumbers”. RQ4.Isthereacorrelationbetweenhumans’andmodels’
IntheirfMRIstudy,Peiteketal.[24]examined41complexity evaluationsofCodeReadabilityinAI-generatedJavacode?
metricsandtheirimpactonprogramcomprehension,findingthat Ourfindingsrevealvariationsinthecorrelationbetweenexisting
boththetextuallengthandvocabularysizeofcodeincreasecogni- CodeReadabilitymodelsandhumanevaluations.Notably,Scal-
tiveloadandworkingmemorydemandforprogrammers. abrino’smodel[28]exhibitsamoderatecorrelationwiththehuman
TherearealsosimilaritiesbetweenobtainedCodeReadability assessments.Othermodels,suchasPosnett’s[26],Dorn’s[5],and
characteristics in the current research and those from previous Mi’s[21]demonstratesubstantiallyweakercorrelations.
models.Ourresultsshowthatdevelopersperceivecodeasreadable ThelowMCCscoresstemfromthebalancednatureofthemetric.
basednotonlyonstructuralcharacteristicsofthecodeasassumed DespitePosnett’smodelclassifying97.52%(118/120)andhumans
byearliermodelsproposedbyBuseandWeimer[3],andPosnettet classifying85.32%(93/109)ofsnippetsasreadable,indicatingan
al.[26],ratheronthecombinationofstructuralcharacteristicswith expectedmoderatetohighcorrelation,theobservedcorrelation
visual,textual,andlinguisticfeaturesasproposedbylatermodels isweak(MCC=-0.036).Thisweaknessarisesbecausethemetric
[5,21,28].Forinstance,VisualOrganizationfromcurrentresearch considersbothtruepositivesandtruenegatives,andincasesof
canbeviewedassimilartoVisualfeaturesfromDorn’sandMi’s significantclassimbalance(Posnett’smodelclassifyingonlytwo
models.MagicNumbersUsageandNamingechoNaturalLanguage samplesasunreadable),eachmissinthesmallerclasscarriesmore
FeaturesfromDorn’smodelandIdentifierTermsinDictionary weight.AsPosnett’smodelandourparticipantsdidn’tmatchin
fromScalabrinoetal.’sresearch. anyunreadablecases,itresultedinnear-zerocorrelation.
Atthesametime,unlikeinpreviousstudies,wehavealsoidenti- Thesefindingsareconsistentwithpriorresearchontherelation-
fiedcharacteristicsofCodeReadabilitythatareconnectedwiththe shipbetweenhumanjudgmentsandmetricsofcodeunderstand-
meaningofthecodethatapersontriestoextracttocomprehend ability,whichincludesreadability.Scalabrinoetal.demonstrated
thecode—UnderstandableGoal,SufficientContextualInfoand thatmetricstypicallyusedforeffortestimationandassociatedwith
FamiliarCodePatterns. understandability,likecyclomaticcomplexity,oftenhavelittleto
Thesefindingsagainemphasizethatreadabilityisacomplex, nocorrelationwithactualunderstandability[27].
individualconcepttiedtohowdevelopersmentallyrepresentthe Thisdisparityincorrelationvalueshasimplicationsfortheutility
codeandmakesenseofit. oftheseCodeReadabilitymodelsaslearningobjectivesinthefine-
tuningprocessofcode-fluentLLMstoimprovethereadabilityof
theirgeneratedoutputs.Themoderatecorrelationobservedwith
ToolsandstrategiesusedtoimproveCodeReadabilityshould Scalabrino’smodelsuggeststhatitmighthavesomepotentialfor
focusonbalancingbrevitywithunderstanding,maintaining enhancingthereadabilityofcodegeneratedbyLLMs.Still,amore
logicalstructure,adheringtostyleguides,andprovidingclearly robustandprecisemodelforguidingtheadjustmentprocessisyetto
expressedcodepurposeswhileaccountingthatnooneuniversal bedevelopedandmightbebuiltutilizingtheinsightsfromcurrent
standardofCodeReadabilitymightexist. researchregardingcodeaspectscrucialforCodeReadability.
Thefairlylowcorrelationbetweenexistingmodelsandhuman
evaluationsofCodeReadabilityimpliesthatthesemodelscould
RQ3.HowdoJavadeveloperswithvaryingexperiencelevels
berefinedorcomplementedwithhuman-centeredaspectsto
assessthereadabilityofcodeproducedbyacode-fluentLLM?.
guidebettertheprocessofadjustingcode-fluentLLMsoutput
ToaddressthisRQ,weemployedthelistofreadability-relatedcode
aimedatenhancingthereadabilityofAI-generatedcode.
aspectscollectedduringtheRepertoryGridstageofthecurrent
research.Weusedthislistinalabelingsurveywhereparticipants
6.1 OverallDiscussion
evaluated the readability of 120 AI-generated Java methods de-
signedforsolvingCodeGolfgametasks. OurstudyexploredthepotentialuseofexistingCodeReadability
Thelabelingsurveyresultsshowthat,despitetheeffortstopro- modelsforadjustingLLMstoprogrammers’needs.Theaimwas
videguidanceduringCodeReadabilityassessmenttoensureconsis- toinvestigatewhetherthesemodelscouldsignificantlycorrelate
tentevaluationofsnippetsbasedonkeyreadability-relatedaspects, withhumanratings.Suchafindingcouldsaveresourcesinfuture
individualstendtoassessreadabilitysubjectively,andtheireval- researchsincehavingarobustmodelofhumanperspectivecould
uationsdonotalwaysconsistentlyalignwithoneanother.These eliminatetheneedforcreatingextensivedatasetsforfine-tuning.
findings are in line with prior Code Readability studies, where However,ourfindingsrevealedthatnoneoftheexistingCode
humanannotatorsexhibitedimperfectagreement,havingacorre- Readabilitymodelssignificantlycorrelatedwithhumanratings.
lationaround.5withthemeanreadabilityscore[3,5]. Additionally,weobservedthathumansdidnotconsistentlyagreeICPC2024,April15–16,2024,Lisbon,Portugal Sergeyuketal.
onCodeReadabilityevaluations,evenafterreceivingleadingas- unfamiliarandinaccessibletosomeandpotentiallyimpactingtheir
sessmentsofreadability-relatedcodeaspects.Thishighlightsthe CodeReadabilityassessments,ourrobustsamplesizeof390par-
challengeposedbythesubjectivityofreadability-relatedfactorsand ticipantshelpsmitigatethislimitationandweakenotherpossible
CodeReadabilityitselfwhenaligningLLMs.Ifreadabilityissuch minorinterveningfactors.
asubjectivemetric,itmaynecessitateanindividualizedmodel’s Werandomizedsnippetpresentationsinboththeinterviewand
adjustmentineachcaseorbreakingthismetricdowntoseveral surveyphasestocounterpotentialordereffects.
lesscomplexones. ExternalValidityreferstotheextenttowhichourfindings
Nonetheless,thereispotentialforcertainaspectsofcodethat canbegeneralizedtoothercontexts.Weacknowledgethatour
impactreadabilitytobeuniversallyapplicabletomostprogram- findingsfocusprimarilyonCodeReadabilityofAI-generatedJava
mersorspecificprogrammergroups.Thisunderscorestheneedfor codesnippetsperceivedbyJavaprogrammers.Hence,thisstudy
furtherresearchinthisarea.Futurestudiesshouldfocusonvali- doesnotaddresshowtheseresultsextendtoCodeReadabilityin
datingcodeaspectsrelatedtoCodeReadability,developingmodels otherprogramminglanguages.
that incorporate these elements, and creating a comprehensive, Moreover,weacknowledgethatCodeGolftasks(adeliberate
accuratelylabeleddatasetaccordingly.Withthesecomponentsin methodologicalchoiceforgeneratingAI-basedcodesnippetsaimed
place,achievingalignmentofLLMswiththehumannotionofCode atexaminingLLMcapabilitiesinacontrolledenvironment,focus-
Readabilitybecomespossible. ingontheclarityandconcisenessofcode)maynotencompass
thetypicalcomplexityfoundinlargercodebases.Extendingour
researchbyincludingadiverserangeofprogrammingtasksthat
7 THREATSTOVALIDITY
mirrorthereal-worldscenariosinwhichLLMsarecommonlyem-
ConstructValidityreferstotheaccuracywithwhichastudy’s ployed would provide a more comprehensive understanding of
measurementscaptureandrepresenttheconceptsunderresearch. LLMs’potentialineverydaycodingpractices.
Inourstudyitinvolvesdeterminingwhethertheconstructsiden-
tifiedduringinterviewsandemployedinthelabelingsurveyade-
quatelyrepresenthowdevelopersconceptualizeCodeReadability
8 CONCLUSION
andwhetherthemeasurementsofCodeReadabilitytrulyreflectit.
Weusedthewell-regardedRepertoryGridtechniquetoensurecon- Inthisresearch,ourprimaryobjectivewastoinvestigatewhether
structvalidityforoperationalizingCodeReadability.Recognizing existingCodeReadabilitymodelscouldfunctionasaproxyfor
potentialoversightsduetoalimitednumberofintervieweesand thehumanevaluationsofCodeReadability.Weaimedtoexplore
theirperspectivesandexpertviewpoints,weincludedindividuals thepossibilityofusingtheseCodeReadabilitymodelstoalignthe
withdiverseexperiencelevelstoenhanceouroperationalization. outputsofLLMswiththeexpectationsandneedsofprogrammers,
FurthervalidationofCodeReadabilityrelatedconstructswasbe- allinthepursuitofenhancingprogrammers’productivitywhen
yondourresearchscope,whichfocusedonestablishingaconsistent workingwithAIcodingassistants.
understandingofCodeReadabilityamongsurveyparticipants. Toachievethis,wecreatedasetof120AI-generatedJavacode
WeacknowledgethattheconceptofCodeReadability,when snippetsandtookahuman-centeredapproachtodefinethenotion
reducedtobinaryterms,overlooksthecomplexityandthesubjec- ofCodeReadabilityinthiscode,consideringthediverseperspec-
tivenatureinherentintheevaluationofcodequality.However,the tivesofdevelopers.UtilizingtheRepertoryGridtechnique,weiden-
purposeofthisresearchwastoexaminethecorrelationbetween tified12distinctcodeaspectsthataffectCodeReadability.These
theassessmentsofCodeReadabilitybydevelopersandtheevalua- aspectsgaveusaframeworkforunderstandingCodeReadability
tionsderivedfrommodelsthatutilizebinarylabels.Tomaintain inAI-generatedJavacode,whichwasinstrumentalincreatinga
methodologicalconsistencyandtofacilitateadirectcomparison, datasetforevaluatingexistingCodeReadabilitymodels.Togather
thebinarylabelingapproachwasadoptedforthesurvey.Future thisdataset,weconductedasurveyinvolving390Javaprogram-
researchmayconsideramoregranularapproachtobettercapture merswhoassessedAI-generatedJavacodesnippetsintermsofthe
themultifacetedaspectsofCodeReadability. abovementionedaspectsofCodeReadability.
WeaddressedthepotentialinaccuracyofCodeReadabilitymod- OurfindingsindicatethatwhenAIgeneratesexecutableand
elsbysourcingthemmeticulouslyfromoriginalresources,like concisecode,ittendstobereadable.Additionally,developers’Code
papers,communicationswithauthors,andcitedwebsiteswiththe Readabilityassessmentsarebasedontheirimplicitnotionsabout
models.Thisensuredthemodels’measurementsalignedwiththeir CodeReadabilityandthusinvolvesubjectivity,leadingtodiffering
authenticformulations,reinforcingthereliabilityofourfindings. opinionsamonghumanraters.Furthermore,weobservedthatonly
InternalValidityensuresthatastudyidentifiesoutcomeswith- Scalabrino’sCodeReadabilitymodelmoderatelycorrelatedwithhu-
outbeingaffectedbyexternalfactors.Intheinterviewphase,in- manreadabilityratings.Thissuggeststhattheexistingmodelsmay
terviewersreceivedpeer-reviewedtrainingonhowtoapplythe notbewell-suitedforguidingtheprocessofaligningAI-generated
RepertoryGridtechniqueconsistentlyandunbiasedly,ensuring codewithhumannotionsofCodeReadability.
theinternalvalidityoftheelicitedCodeReadabilityaspects. Further research aimed at enhancing programmer efficiency
Inthelabelingsurveyphase,wemaintainedinternalvalidity might,drawingonthedatafromthisstudy,focusondeveloping
throughaweb-basedplatform,presentingAI-generatedcodesnip- a Code Readability model or creating a comprehensive labeled
petstoallparticipantsinaunifiedwaywithunifiedinstructions. datasetforLLMadjustment,therebyimprovingthealignmentof
AlthoughthesesnippetsweredisplayedinaDarkcolorscheme, AI-generatedcodewithhumanreadabilitystandards.ReassessingCodeReadabilityModels ICPC2024,April15–16,2024,Lisbon,Portugal
REFERENCES
[23] HusseinMozannar,GaganBansal,AdamFourney,andEricHorvitz.2023.Reading
[1] AvrimBlum,JeffreyJackson,TuomasSandholm,andMartinZinkevich.2004. BetweentheLines:ModelingUserBehaviorandCostsinAI-AssistedProgram-
Preferenceelicitationandquerylearning.JournalofMachineLearningResearch ming. arXiv:2210.14306[cs.SE]
5,Jun(2004),649–667. https://doi.org/10.1007/978-3-540-45167-9_3 [24] NormanPeitek,SvenApel,ChrisParnin,AndréBrechmann,andJanetSiegmund.
[2] Damian Brady. 2023. How Generative AI is Changing the Way Developers 2021.ProgramComprehensionandCodeComplexityMetrics:AnfMRIStudy.
Work. https://github.blog/2023-04-14-how-generative-ai-is-changing-the-way- In2021IEEE/ACM43rdInternationalConferenceonSoftwareEngineering(ICSE).
developers-work/AccessedonSeptember1,2023. 524–536. https://doi.org/10.1109/ICSE43902.2021.00056
[3] RaymondP.L.BuseandWestleyR.Weimer.2008. AMetricforSoftware [25] TelmoPessoaPires,AntónioV.Lopes,YannickAssogba,andHendraSetiawan.
Readability.InProceedingsofthe2008InternationalSymposiumonSoftware 2023.OneWideFeedforwardisAllYouNeed. arXiv:2309.01826[cs.CL]
TestingandAnalysis.AssociationforComputingMachinery,121–130. https: [26] DarylPosnett,AbramHindle,andPremkumarDevanbu.2011.ASimplerModel
//doi.org/10.1145/1390630.1390647 ofSoftwareReadability.InProceedingsofthe8thWorkingConferenceonMining
[4] QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,Xu SoftwareRepositories.AssociationforComputingMachinery,73–82. https:
Sun,JingjingXu,LeiLi,andZhifangSui.2023.ASurveyonIn-contextLearning. //doi.org/10.1145/1985441.1985454
arXiv:2301.00234[cs.CL] [27] Simone Scalabrino, Gabriele Bavota, Christopher Vendome, Mario Linares-
[5] JonathanDorn.2012. Ageneralsoftwarereadabilitymodel. (2012). http: Vásquez,DenysPoshyvanyk,andRoccoOliveto.2017. Automaticallyassess-
//www.cs.virginia.edu/weimer/students/dorn-mcs-paper.pdf ingcodeunderstandability:Howfararewe?.In201732ndIEEE/ACMInterna-
[6] AlexandreDuval,ThomasLamson,GaëldeLéséleucdeKérouara,andMatthias tionalConferenceonAutomatedSoftwareEngineering(ASE).417–427. https:
Gallé.2021.BreakingWriter’sBlock:Low-costFine-tuningofNaturalLanguage //doi.org/10.1109/ASE.2017.8115654
GenerationModels.InProceedingsofthe16thConferenceoftheEuropeanChapter [28] SimoneScalabrino,MarioLinares-Vásquez,RoccoOliveto,andDenysPoshy-
oftheAssociationforComputationalLinguistics:SystemDemonstrations.Associa- vanyk.2018.Acomprehensivemodelforcodereadability.JournalofSoftware:
tionforComputationalLinguistics,278–287. https://doi.org/10.18653/v1/2021. EvolutionandProcess30,6(2018),e1958. https://doi.org/10.1002/smr.1958
eacl-demos.33 [29] Simone Scalabrino, Mario Linares-Vasquez, Denys Poshyvanyk, and Rocco
[7] HelenM.Edwards,SharonMcDonald,andS.MichelleYoung.2009.Therepertory Oliveto.2016.Improvingcodereadabilitymodelswithtextualfeatures.In2016
gridtechnique:Itsplaceinempiricalsoftwareengineeringresearch.Information IEEE24thInternationalConferenceonProgramComprehension(ICPC).IEEE,1–10.
andSoftwareTechnology51,4(2009),785–798. https://doi.org/10.1016/j.infsof. https://doi.org/10.1109/ICPC.2016.7503707
2008.08.008 [30] BurrSettles.2009.Activelearningliteraturesurvey.(2009). http://digital.library.
[8] GuillemFeixasandJoséManuelCornejo.2002.AManualfortheRepertoryGrid wisc.edu/1793/60660
UsingtheGRIDCORprogramme(Version4.0). https://www.ub.edu/terdep/pag/ [31] PriyanVaithilingam,TianyiZhang,andElenaL.Glassman.2022.Expectation
index.htmlAccessedonJuly17,2023. vs.Experience:EvaluatingtheUsabilityofCodeGenerationToolsPoweredby
[9] GuangchaoCharlesFeng.2015.Mistakesandhowtoavoidmistakesinusing LargeLanguageModels.InExtendedAbstractsofthe2022CHIConferenceon
intercoderreliabilityindices.Methodology:EuropeanJournalofResearchMethods HumanFactorsinComputingSystems.AssociationforComputingMachinery,
fortheBehavioralandSocialSciences11,1(2015),13–22. https://doi.org/10.1027/ 1–7. https://doi.org/10.1145/3491101.3519665
1614-2241/a000086 [32] JesperEVanEngelenandHolgerHHoos.2020.Asurveyonsemi-supervised
[10] JonasGeipingandTomGoldstein.2023.Cramming:TrainingaLanguageModel learning.Machinelearning109,2(2020),373–440. https://doi.org/10.1007/s10994-
onasingleGPUinoneday.InInternationalConferenceonMachineLearning. 019-05855-6
PMLR,11117–11143. https://proceedings.mlr.press/v202/geiping23a.html [33] PriyankaVergadiaandKaliahWilliams.2023. Tipstoenhanceyourprompt-
[11] SuriyaGunasekar,YiZhang,JyotiAneja,CaioCésarTeodoroMendes,AllieDel engineering abilities. https://cloud.google.com/blog/products/application-
Giorno,SivakanthGopi,MojanJavaheripi,PieroKauffmann,GustavodeRosa, development/five-best-practices-for-prompt-engineeringAccessedonSeptem-
OlliSaarikivi,AdilSalim,ShitalShah,HarkiratSinghBehl,XinWang,Sébastien ber1,2023.
Bubeck,RonenEldan,AdamTaumanKalai,YinTatLee,andYuanzhiLi.2023. [34] DaquanZhou,KaiWang,JianyangGu,XiangyuPeng,DongzeLian,YifanZhang,
TextbooksAreAllYouNeed. arXiv:2306.11644[cs.CL] YangYou,andJiashiFeng.2023.DatasetQuantization. arXiv:2308.10524[cs.CV]
[12] AsmaulHosna,EthelMerry,JigmeyGyalmo,ZulfikarAlom,ZeyarAung,and
MohammadAbdulAzim.2022.Transferlearning:afriendlyintroduction.Journal
ofBigData9,1(2022),102. https://doi.org/10.1186/s40537-022-00652-w
[13] ICC/ESOMAR.2022.ICC/ESOMARInternationalCodeonMarket,Opinionand
SocialResearchandDataAnalytics. https://esomar.org/uploads/attachments/
ckqtawvjq00uukdtrhst5sk9u-iccesomar-international-code-english.pdf Ac-
cessedon:January23,2024.
[14] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta
Raileanu,andRobertMcHardy.2023. ChallengesandApplicationsofLarge
LanguageModels. arXiv:2307.10169[cs.CL]
[15] MertKarabacakandKonstantinosMargetis.2023.EmbracingLargeLanguage
ModelsforMedicalApplications:OpportunitiesandChallenges.Cureus15,5
(2023). https://doi.org/10.7759/cureus.39305
[16] GeorgeKelly.2003.Thepsychologyofpersonalconstructs:Volumetwo:Clinical
diagnosisandpsychotherapy.Routledge.
[17] NathanLambert,LouisCastricato,LeandrovonWerra,andAlexHavrilla.2022.
IllustratingReinforcementLearningfromHumanFeedback(RLHF). Hugging
FaceBlog(2022). https://huggingface.co/blog/rlhf.
[18] PhucH.Le-Khac,GrahamHealy,andAlanF.Smeaton.2020.Contrastiverepre-
sentationlearning:Aframeworkandreview.IEEEAccess8(2020),193907–193934.
https://doi.org/10.1109/ACCESS.2020.3031549
[19] JennyT.Liang,ChenyangYang,andBradA.Myers.2023.Understandingthe
UsabilityofAIProgrammingAssistants. arXiv:2303.17125[cs.SE]
[20] ChenLing,XujiangZhao,JiayingLu,ChengyuanDeng,CanZheng,Junxiang
Wang,TanmoyChowdhury,YunLi,HejieCui,XuchaoZhang,TianjiaoZhao,
AmitPanalkar,WeiCheng,HaoyuWang,YanchiLiu,ZhengzhangChen,Haifeng
Chen,ChrisWhite,QuanquanGu,JianPei,andLiangZhao.2023.DomainSpecial-
izationastheKeytoMakeLargeLanguageModelsDisruptive:AComprehensive
Survey. arXiv:2305.18703[cs.CL]
[21] QingMi,YiqunHao,LiweiOu,andWeiMa.2022.TowardsUsingVisual,Semantic
andStructuralFeaturestoImproveCodeReadabilityClassification.Journalof
SystemsandSoftware193,C(2022). https://doi.org/10.1016/j.jss.2022.111454
[22] QingMi,JackyKeung,YanXiao,SolomonMensah,andYujinGao.2018. Im-
provingcodereadabilityclassificationusingconvolutionalneuralnetworks.
InformationandSoftwareTechnology104(2018),60–71. https://doi.org/10.1016/j.
infsof.2018.07.006