Unrecognizable Yet Identifiable: Image Distortion with
Preserved Embeddings
Dmytro Zakharova, Oleksandr Kuznetsovb,∗, Emanuele Frontonib
aDepartment of Applied Mathematics, V.N. Karazin Kharkiv National University, 4
Svobody Sq., Kharkiv, 61022, Ukraine
bDepartment of Political Sciences, Communication and International Relations,
University of Macerata, Via Crescimbeni, 30/32, Macerata, 62100, Italy
Abstract
In the realm of security applications, biometric authentication systems
play a crucial role, yet one often encounters challenges concerning privacy
and security while developing one. One of the most fundamental challenges
lies in avoiding storing biometrics directly in the storage but still achieving
decently high accuracy. Addressing this issue, we contribute to both artificial
intelligence and engineering fields. We introduce an innovative image distor-
tion technique that effectively renders facial images unrecognizable to the
eye while maintaining their identifiability by neural network models. From
the theoretical perspective, we explore how reliable state-of-the-art biomet-
rics recognition neural networks are by checking the maximal degree of image
distortion,whichleavesthepredictedidentityunchanged. Ontheotherhand,
applying this technique demonstrates a practical solution to the engineering
challenge of balancing security, precision, and performance in biometric au-
thentication systems. Through experimenting on the widely used datasets,
we assess the effectiveness of our method in preserving AI feature represen-
tation and distorting relative to conventional metrics. We also compare our
method with previously used approaches. We publically release the source
code for this study1.
∗Corresponding author
Email addresses: zamdmytro@gmail.com (Dmytro Zakharov),
kuznetsov@karazin.ua (Oleksandr Kuznetsov), emanuele.frontoni@unimc.it
(Emanuele Frontoni)
1https://github.com/ZamDimon/distortion-generator/tree/v1.0.0
Preprint submitted to Engineering Applications of AI January 29, 2024
4202
naJ
62
]VC.sc[
1v84051.1042:viXraTable 1: An example of using our proposed image distortion technique on
images from MNIST (Deng, 2012) and LFW (Huang et al., 2007) datasets.
While authentic and generated images significantly differ, the feature vectors
of both images in pairs are relatively close.
Real Generated Real Generated Real Generated
MNIST
LFW
Keywords:
Cancelable Biometrics, Deep Learning, Triplet Loss, Feature Extraction,
Convolutional Neural Networks, Information Security
1. Introduction
In the digital age, one cannot overstate the necessity for robust cyber-
security systems. With the rapid proliferation of digital identities and the
increasing reliance on virtual platforms for many activities, safeguarding per-
sonal and organizational data has become a crucial problem (Hamme et al.,
2022). This surge in digitalization has simultaneously amplified cybersecu-
rity vulnerabilities, making exploring innovative and effective security solu-
2tions essential. Biometric technologies, utilizing unique physical or behav-
ioral characteristics for identification and authentication, have emerged at
the forefront of this endeavor (Amin et al., 2014).
Current biometric systems, while revolutionary in many respects, have
their drawbacks. A predominant concern is the risk of irreversible compro-
mise; once a biometric trait is exposed or stolen, it is compromised forever,
unlike traditional passwords or tokens that can be easily changed (Galbally
et al., 2007). Furthermore, issues like data privacy, susceptibility to spoofing
attacks, and the challenge of maintaining high accuracy under varied condi-
tions underscore the limitations of existing biometric technologies. Integrat-
ing these systems into diverse platforms also presents challenges in terms of
scalability, interoperability, and user accessibility.
Against this backdrop, our research introduces a new concept in bio-
metric technologies to redefine the approach to biometric data security and
management. The core of our vision is the development of novel intellectual
biometric technologies that address the existing challenges and open new
horizons in cybersecurity. This approach pivots on leveraging deep learn-
ing and machine learning algorithms to enhance the security and efficacy of
biometric systems.
The primary objective of this research is to explore and validate the fea-
sibility of an image distortion technique while preserving the feature vectors.
Thissystemrepresentsaparadigmshiftinbiometricauthentication, focusing
on maintaining the original quality of biometric data while ensuring its secu-
rity and privacy. Our approach diverges from traditional methods by avoid-
ing the distortion of original biometric data, instead employing advanced AI
algorithms for data analysis and template generation.
Our methodology involves an analysis of biometric data integrity and the
application of state-of-the-art AI techniques. We utilize the idea of Triplet
Networks to develop a sophisticated metric for biometric data comparison,
ensuring the security of the data while maintaining its original character-
istics. The research encompasses a series of experiments using the MNIST
(Deng, 2012) and LFW (Huang et al., 2007) dataset to validate our system’s
effectiveness empirically.
This research contributes significantly to the field of biometric security.
By introducing a non-distortive approach to cancelable biometrics, we pro-
vide a solution that balances the need for safety with the imperative of pro-
tecting individual privacy. Our findings could influence future developments
in biometric authentication, paving the way for more secure and privacy-
3conscious systems.
The paper is structured to explore each aspect of the proposed system
systematically. Following this introduction, we delve into the theoretical
framework, experimental methodology, results analysis, and a comprehensive
discussion of our research’s implications and future directions.
2. State of the Art
Ourstudystandsoutintherealmofbiometricsecuritybyofferinganovel
AI-driven solution that not only ensures high levels of security and privacy
but also addresses operational and scalability challenges inherent in current
systems. This holistic approach positions our research as a significant con-
tribution to the field, filling gaps left by previous studies and extending the
applicability and efficiency of cancelable biometrics across various biometric
modalities.
Bansal and Garg (2022) introduced a cancelable biometric template pro-
tection scheme combining format-preserving encryption with Bloom filters.
This method enhances security while maintaining recognition performance.
However, it primarily focuses on the encryption aspect without addressing
the potential complexities involved in the operational deployment of such
systems across diverse platforms. Our research aims to simplify operational
challenges by utilizing AI algorithms, thereby filling this gap.
Helmy et al. (2022) proposed a novel hybrid encryption framework based
on Rubik’s cube technique for cancelable biometric systems. Their approach
showcases an innovative method to secure multi-biometric systems. How-
ever, the research is heavily centered on encryption techniques, potentially
overlooking the ease of integration and scalability. Our study contributes by
emphasizing a more integrated approach where security is achieved without
overcomplicating the system architecture.
Kauba et al. (2022) explored practical cancelable biometrics for finger
vein recognition, analyzing three different approaches and their impact on
recognition performance and security. While their work provides valuable
insights into finger vein biometrics, it does not extensively cover other bio-
metric modalities. Our research broadens this scope by proposing a versatile
AI-driven metric applicable across various biometric modalities, thereby en-
hancing the applicability of cancelable biometrics.
Nayar et al. (2021) focused on secure cancelable palm vein biometrics
using a graph-based approach. Their method provides a novel perspective
4on ensuring template security. However, it is specific to palm vein biometrics
and may not be directly applicable to other biometric types. Our approach
offersamoreuniversalsolution, applicableacrossdifferentbiometricsystems.
Yang et al. (2022b) developed a linear convolution-based cancelable fin-
gerprintauthenticationsystem, emphasizingonsafeguardingfingerprinttem-
plate data. Although their approach contributes significantly to fingerprint
authentication, it does not address the broader spectrum of biometric modal-
ities. Our research fills this gap by offering a comprehensive solution that is
adaptable to various biometric types.
In their study, Wang, Deng, and Hu Wang et al. (2017b) innovatively
applied a partial Hadamard transform to cancelable biometrics, enhancing
the security of binary biometric representations. Their method effectively
prevents the reconstruction of original data, addressing a significant security
concerninbinarybiometricsystems. Ourresearchextendstheirfoundational
work by integrating AI algorithms, broadening the scope to various biometric
modalities and focusing on preserving the original data’s integrity, thereby
filling a crucial gap in user-friendly and secure biometric authentication.
In their study, Yang et al. (2021) addressed the vulnerability of tradi-
tional random projection-based cancelable biometrics to attack via record
multiplicity (ARM). While their feature-adaptive random projection method
enhancessecurity,thefocusremainsnarrow,primarilyaddressingonespecific
type of attack. Our research contributes by introducing a holistic approach
that encapsulates broader security concerns while ensuring high recognition
accuracy.
Akdogan et al. (2018) proposed two novel biometrics-based secure key
agreement protocols, focusing on integrating cancelability in biometric data.
Their approach, particularly in the SKA-CB protocol, underscores the im-
portance of cancelable biometrics in enhancing security. However, their focus
onkeyagreementprotocolsleavesagapinbroaderapplicationcontexts, such
as operational flexibility and cross-platform adaptability. Our research ad-
dresses these broader aspects, offering a more versatile solution in various
biometric applications.
KaurandKhanna(2020)emphasizedtheprivacyandsecurityinnetwork/cloud-
basedremotebiometricauthentication,combiningcancelablepseudo-biometric
identities with secret sharing. While this approach addresses some key se-
curity concerns, it primarily focuses on remote authentication, leaving room
for improvement in terms of local system integration and broader biomet-
ric modalities. Our research fills these gaps by proposing a system that is
5equally effective in both local and remote contexts and across various bio-
metric types.
Kausar (2021) introduced an iris-based cancelable biometric cryptosys-
tem for securing healthcare data on smart cards, combining biometrics with
symmetric key cryptography. This approach offers an interesting perspec-
tive on biometric data security in healthcare. However, the focus on iris
biometrics and healthcare applications indicates a need for a more gener-
alized approach that can be applied across different sectors. Our research
responds to this need by offering a generalizable and adaptable solution in
Non-Distortive Cancelable Biometrics.
Lee et al. (2021) proposed a tokenless cancellable biometrics scheme for
multimodal biometric systems, focusing on biometric template protection
without relying on tokens. Their approach is innovative in enhancing secu-
rity while simplifying the authentication process. However, their research
does not address the operational complexities related to the integration of
such systems across various platforms. Our study aims to provide a compre-
hensive solution that simplifies integration and operational aspects in various
application scenarios.
Murakami et al. (2019) developed a cancelable biometric scheme for fast
and secure biometric identification, focusing on correlation-invariant random
filtering. While their approach is innovative in terms of security and compu-
tational efficiency, it primarily targets large-scale identification systems. Our
research complements this by offering a scalable and adaptable solution that
caters to both large-scale and individualized biometric authentication needs.
Yangetal.(2018)focusedonacancelablemulti-biometricsystemcombin-
ing fingerprint and finger-vein biometrics. Their approach to multi-biometric
systemsunderscorestheimportanceoffeature-levelfusionforenhancedrecog-
nition accuracy and security. However, their focus is somewhat narrow,
mainly on fingerprint and finger-vein biometrics. Our research expands on
this by developing a framework applicable to a wider range of biometric
modalities, enhancing versatility and applicability in diverse scenarios.
In summary, while these studies contribute significantly to the field of
cancelable biometrics, each focuses on specific aspects or applications. Our
research seeks to bridge these gaps by providing a comprehensive, scalable,
and adaptable solution that enhances security, operational efficiency, and
applicability across various biometric modalities and application scenarios.
63. Background
3.1. Image Distortion Techniques
Currently, as outlined in Zhang et al. (2016), image security research
resolves around the following primary topics:
1. Steganography: Hiding information inside the cover image that is
unrecognizable for a human eye (Subramanian et al., 2021; Yang et al.,
2022a). Currently, as seen in (Subramanian et al., 2021), many neural
network architectures are capable of resolving this problem.
2. Cancelable Biometrics: The uninvertible image conversion into the
unrecognizable representation that can be further compared with an-
other similarly converted image directly.
3. Image Encrypting: The process of converting an image to another
unrecognizable representation that is decodable to a trusted party. In
this case, two encoded images cannot be compared without decoding
them (Zhang et al., 2016; Bok-Min et al., 2021; Matoba et al., 2009).
Our research is best related to the second and third topics. Therefore,
we focus on a comparison of these two subjects. We discuss the advantages
of our approach by considering an example of building the most simplistic
authorization system, which all these methods are intended for in the first
place.
First, consider the flow of a user registration, depicted in Figure 1. Sup-
pose some user wants to use a facial recognition authorization feature on
account. In that case, the system must be provided with the biometrics
data, which is then processed by a generator function G. Cancelable biomet-
rics converts this image to another representation that will look almost the
same if the same person takes another photo. Image encrypting will convert
an image to a seemingly random format, but which can be decoded using G−1
which is known to the trusted party (in our particular case, for the system
itself). Either way, the processed data gets saved in the database, which we
call a “template” data T.
Now, considerthesettingwhereauserpassesanimageX, andthesystem
wants to verify that the user exists in the database. Specifically, we need an
algorithm to output 1 if X belongs to the same person as some template T
from the database and 0 to different people. Here comes the main difference
between the methods considered.
7Figure 1: Facial recognition feature registration flow.
Consider cases (a) and (b) in Figure 2, where we depict the login flows for
cancelable biometrics and image encoding-based approaches, respectively. In
both cases, we use a metric of image comparison, or in our particular case,
image distance d. As the simplest example, d might be an Euclidean or
Hamming metric or can encapsulate some complex mechanism such as an
image-matching technique. Now, we explain the flows for each of the cases:
• In cancelable biometrics approach we need to firstly generate an image
G(X) and compare it with T using distance d. If d(G(X),T) is small
enough, we consider ownership of both X and T to be the same.
• In image encoding approach, we retrieve the original image from the
templateG−1(T)andcompareittoX. Ifd(G−1(T),X)issmallenough,
we again consider the ownership of X and T to be the same.
Note that in both cases, we need to conduct two steps: first, either apply
G or G−1, and then compare two images using d. We propose a novel ap-
proach depicted as the case (c) in Figure 2. In our implementation, we can
directly compare X and T using the secret comparison metrics d∗ unknown
to the external user. This way, we can directly find d∗(X,T) without any
image transformations, which is costly for any system. Simultaneously, the
traditional metric d between X and T would remain very large, making it
impossible or very complicated to determine how close X and T are.
3.2. Triplet Loss Usage for training an Embedding Model
Denote by I a set of images. To further avoid confusion with terminology,
we define the term embedding model as the function F : I → Rm, which
maps an image to a low-dimensional representation in Rm, sometimes called
a feature vector.
The embedding model is an excellent tool for various problems, not only
in terms of computational efficiency but also since we can encapsulate core
8(a) Cancelable Biometrics (b) Image Encoding
(c) Our login user registration flow
Figure 2: Comparison of different login flows using biometrics data, where
G denotes the generation function, d denotes the traditional image distance
metrics, while d∗ – a secret one. In flow (a), we first generate a distorted
image and compare it with a template. In flow (b), we find the inverse of a
templateandcompareitwithaninput. Inourproposedflow(c), wecompare
template and image directly.
9patterns in data using only hundreds of numbers (instead of ten thousands of
them). For example, consider papers (Spruyt, 2018) and (Guo et al., 2022),
where embeddings store information about geographical position (for more
examples, see subsection 3.3).
Denote by θ parametrization of an embedding model F. Similarly to
F
FaceNet paper by Schroff et al. (2015), we limit the output to the unit hy-
persphere Sm−1 ≜ {x ∈ Rm : ∥x∥ = 1} with the embedding size of m.
2
This step is optional, though: in fact, any function F might be provided, not
limited to deep learning ones, as long as the gradient descent algorithm can
be applied.
Themainpurposeofourneuralnetworkistocreate“similar”embeddings
for images from the same class and “different” for ones from different classes.
We define the measure of “distinctiveness” as follows:
d (X,Y) ≜ ∥F(X)−F(Y)∥2. (1)
F 2
This way, if X and X belong to the same class, while Y to a different one,
1 2
d (X ,X ) must be much smaller than both d (X ,Y) and d (X ,Y).
F 1 2 F 1 F 2
However, the neural network must know how to learn to produce such
embeddings. For that reason, we consider the dataset T = {(A ,P ,N )}nT
i i i i=0
where A and P are images from the same class (called anchor and positive
i i
images, respectively) whereas N from a different one (called negative image).
i
The idea of triplet loss is to constraint an embedding of an anchor image
A to be closer to the corresponding embedding of P than to an image N by
a positive value µ (called margin). So ideally, for all triplets (A,P,N) ∈ T
we want:
d (A,P) < d (A,N)−µ (2)
F F
Fromtheprobabilisticperspective,supposethatwetakesamplesT = (A,P,N)
from a true distribution p . Our goal is to maximize the probability of the
data
aforementioned relationship:
maxP [d (A,P) < d (A,N)−µ] (3)
(A,P,N)∼p F F
F data
Practically, the following loss function is considered, which is called a triplet
loss function:
ℓ (T;F) ≜ ReLU(d (A,P)−d (A,N)+µ), (4)
triplet F F
and then E [ℓ (T;F)] is minimized w.r.t. F.
T∼p triplet
data
10Figure 3: Triplet Network architecture. We input three images (anchor,
positive, and negatives), then using embedding model F with shared param-
eters retrieve three feature vectors, and then concatenate them to get the
loss value.
3.3. Triplet Network
Triplet loss and triplet neural networks play a crucial role in many areas
of computer vision: for instance, they are used in face recognition (Schroff
et al., 2015; Wang et al., 2017a), person reidentification (Zhang et al., 2018),
object tracking (Dong and Shen, 2018), and even generative neural networks
(Cao et al., 2017).
To examine the structure of a triplet neural network, we refer to Figure 3.
Triplet Network uses three copies of an embedding model with shared
parameters (Hoffer and Ailon, 2015). Using the triplet loss defined in sub-
section 3.2, we calculate the loss and update the weights of an embedding
model. We can then safely retrieve and use the embedding model for our
purposes. Specifically, the most basic example algorithm is outlined in Al-
gorithm 1.
11Algorithm 1 The simplest training algorithm of embedding model using
triplet network architecture.
for each batch B = {(A ,P ,N )}nB ⊂ T do
i i i i=1
1. Find embeddings {(F(A ),F(P ),F(N ))}nB .
i i i i=1
2. Concatenate them: V ← {F(A )⊕F(P )⊕F(N )}nB .
i i i i=1
3. Find the batch loss L(θ ;B) ← 1 (cid:80) ℓ(v) where
F nB v∈V
ℓ(v) = ∥v −v ∥2 −∥v −v ∥2 +µ.
0:m m:2m 2 0:m 2m:3m 2
4. Update θ using the gradient descent. For example, in its simplest
F
form we use
θ⟨j+1⟩ ← θ⟨j⟩ −η∇ L(θ⟨j⟩;B).
F F θF F
end for
4. Methods
4.1. Overview
Distortion generator is a function G : I → I, which generates a distorted
image from a given one. This generator must meet the following two criteria:
1. Difference between images G(X) and X is as large as possible. We call
the metrics for such difference d : I ×I → R .
img ≥0
2. Difference between embeddings F ◦ G(X) and F(X) is as small as
possible. We call the metrics for this difference d : Rm×Rm → R .
emb ≥0
Suppose inputs are taken from the true distribution p . This way,
data
informally, we want to have:
maxE [d (G(X),X)] (5)
X∼p img
G data
while minE [d (F ◦G(X),F(X))] (6)
X∼p emb
G data
Notethatinthiscase, wecannotemploytheideaofatwo-playerminimax
game used in GAN (Goodfellow et al., 2014) directly since we cannot modify
the embedding neural network F, although this idea does seem attractive at
first glance.
However, if we wanted to train a pair (F,G) together, that could be
possible. That being said, that is a great topic for future research, but for
now we restrict ourselves F to be fixed.
124.2. Loss Function
To represent the optimization problem above, we define the following loss
function for a single image:
ℓ(X;G,F) ≜ (1−π )·ℓ (X;G)+π ·ℓ (X;G,F), (7)
emb img emb emb
where π ∈ [0,1] is a positive hyperparameter, regulating the importance
emb
of ℓ in contrast to ℓ .
emb img
We define the two loss components as follows:
ℓ (X;G) ≜ −d (G(X),X), (8)
img img
ℓ (X;G,F) ≜ ReLU(d (F ◦G(X),F(X))−α). (9)
emb emb
Notethatℓ isalwaysnegativesincewewanttomaximize thedifference
img
between images. Also, we decide to use ReLU(d (·)−α) for ℓ instead
emb emb
of d (·) since otherwise neural network might focus primarily on reducing
emb
the distance between embeddings. However, if we use the ReLU function,
we do not punish the neural network for an embedding difference unless it
exceeds α. In this sense, α also serves as a parameter that regulates how well
we want our generator to fit embeddings: the larger α is, the more distinct
images are according to metrics d , but less similar according to d (see
img emb
subsection 6.3).
Let us now choose the concrete expressions for distances. We use d from
F
Equation 1 for the embedding difference:
d (X;G,F) ≜ d (G(X),X) = ∥F ◦G(X)−F(X)∥2. (10)
emb F 2
Choosing d is trickier. In the following subsections, we discuss several
img
choices.
4.2.1. L1 Distance
One of the most widely used (Le and Samaras, 2019; Liu et al., 2021;
Isola et al., 2016) loss function for image generations tasks is the L distance
1
between the ground truth and generated images:
(cid:88)W (cid:88)H (cid:88)nC
∥X,Y∥ ≜ |X −Y | (11)
1 ijk ijk
i=1 j=1 k=1
13We will define the distance between images as the average distance, sim-
ilarly to MSE:
ˆ
∥X,X∥
d⟨L1⟩(Xˆ
,X) =
1
, (12)
img W ·H ·n
C
ˆ
where W ×H ×n is the image shape, and by X we denote the predicted
C
value. In contrast to L distance, which we define in the next subsection, L
2 1
encourages less blurring.
4.2.2. L2 Distance
L distance is also frequently used in image generations tasks (Vasluianu
2
et al., 2021; Gatys et al., 2016). For grayscale images, it is defined as ∥X −
Y∥ , where ∥·∥ denotes the Frobenius norm. For RGB images, similarly
F F
to Equation 11, we define the L distance as follows:
2
(cid:32) (cid:33)1/2
(cid:88)W (cid:88)H (cid:88)nC
∥X,Y∥ = (X −Y )2 (13)
2 ijk ijk
i=1 j=1 k=1
The distance between images is in turn defined as the MSE value:
∥Xˆ ,X∥2
d⟨L2⟩(Xˆ ,X) = 2 (14)
img W ·H ·n
C
4.2.3. DSSIM
However, there are multiple ways for a neural network to “cheat” in this
case. For instance, the neural network might invert background pixels or
reduce pixels’ intensities since that would not affect embeddings drastically,
which in turn will not increase d . For this reason, we decided to try using
emb
the more advanced method such as a SSIM(X,Y) (structural similarity index
measure) metrics as suggested by (Zhao et al., 2017). It is defined as:
(2µ µ +κ )(2σ +κ )
X Y 1 XY 2
SSIM(X,Y) = , (15)
(µ2 +µ2 +κ )(σ2 +σ2 +κ )
X Y 1 X Y 2
where µ ,µ are pixel sample means, σ2 ,σ2 are variances, σ =
X Y X Y XY
cov[X,Y] is a covariance, and κ ,κ are constants to stabilize the division.
1 2
14Thedistancemeasure,called“structuraldissimilarity”(DSSIM)2,inturn,
is defined as
ˆ
1−SSIM(X,X)
d⟨DSSIM⟩(Xˆ ,X) ≜ , (16)
img 2
with a range [0,1].
4.2.4. Sobel Distance
After experiments, we decided to employ another loss function, which,
combined with L loss, performed best on the LFW dataset. Suppose we get
1
an image I as an input. We use two kernels:
   
−1 0 1 1 2 1
K x = −2 0 2, K y =  0 0 0 . (17)
−1 0 1 −1 −2 −1
Then, usingthesetwokernels, wefindthemask(alloperationsareperformed
elementwise):
(cid:113)
S(I) = (K ∗I)2 +(K ∗I)2. (18)
x y
Essentially, S(I) gives a map of regions of I which contain edges. Finally,
we define the distance measure as follows:
(cid:13) (cid:13)
d⟨sobel⟩(Xˆ ,X) = (cid:13)S(X)⊙Xˆ ,S(X)⊙X(cid:13) . (19)
img (cid:13) (cid:13)
1
Thedifferencebetweenthislossandonespecifiedinsubsubsection4.2.1is
that we account for the loss only in those regions where there are edges since
using the pure L distance does not restrict the neural network from simply
1
changing the content inside the face without bothering about the shape.
4.2.5. Combined Distance
Combined loss is just a linear combination of several distances. The best
results were achieved by combining the L distance (see subsubsection 4.2.1)
1
and Sobel distance (see subsubsection 4.2.4):
d⟨comb⟩(Xˆ
,X) = ω
·d⟨L1⟩(Xˆ ,X)+(1−ω)d⟨sobel⟩(Xˆ
,X), (20)
img img img
where by regulating ω we can adjust the importance of d⟨L1⟩ relative to
img
d⟨sobel⟩. In our experiments, we use ω = 0.5.
img
2Note that rigorously speaking, this is not a distance function since triangle inequality
is not necessarily satisfied. However, this is not a problem for us if we use this expression
as a loss function.
15Figure 4: Trainer Network architecture.
4.3. Trainer Network Architecture
Whenwefinallydefinedthelossℓ(X;G,F),weneedtotrainourgenerator
to minimize this expected loss, that is:
Gˆ = argminE ℓ(X;G,F) (21)
X∼p
data
G
To achieve this, inspired by Zhmoginov and Sandler (2016), we create a
helper network, which we call a Trainer Network. Its architecture is depicted
in the Figure 4.
For training, we form the dataset in a form {(X ,(X ,F(X )))}N where
i i i i=1
the input is an image X and output is a pair of this same image together
i
with its embedding F(X ).
i
The trainer network takes an image X, generates an image G(X), and
then takes the embedding of this image F ◦ G(X). It then outputs both
values and applies the loss from Equation 7 (since the target value has the
same shape). Note that we freeze the embedding network F and make only
G’s weights trainable.
5. Implementation
In this section, we specify the architecture used for training our models
on MNIST and LFW datasets.
16Figure 5: Embedding model architecture. S9 denotes the layer with 10
neurons which then gets L normalized.
2
5.1. Embedding Model
For the LFW dataset, we use the pre-trained FaceNet architecture. We
decided to employ this architecture since it provides one of the best values
of accuracy in the face recognition task: namely, 98.87% for fixed center
cropping, and99.63%fortheextrafacealignment(seeoriginalpaper(Schroff
et al., 2015) for reference). Note that any other embedding neural network
might be used, such as VGGFace (Parkhi et al., 2015), for example.
For the MNIST dataset, we build our own embedding model. For that,
we use the architecture specified in Figure 5.
We use the LeakyReLU function defined as x (cid:55)→ max{αx,x} (for α < 1).
We choose α = 0.01. For the output layer, we do not use an activation
function; instead, we normalize the retrieved vector by using x (cid:55)→ √ x
∥x∥2+ϵ
2
for sufficiently small 0 < ϵ ≪ 1. As a weight initializer, we use the He ini-
tialization (Kumar, 2017), which initializes weights according to the normal
(cid:16) (cid:17)
distribution N 0, 1 where n is the number of nodes feeding into the
nL L
layer. We choose our embedding dimensionality to be m = 10. We then ap-
ply the training algorithm described in subsection 3.3 using margin µ = 0.2
and a learning rate of η = 5·10−5 using Adam optimizer (Kingma and Ba,
2014).
5.2. Generator Model
For the generator model, we decided to employ the U-Net architecture
(Ronneberger et al., 2015), and get the structure specified in Figure 6 for the
17Figure 6: Generator model architecture for the MNIST dataset based on
U-Net.
MNIST dataset (architecture for the LFW dataset is the same with the only
difference in shapes). Similarly to the embedding model from subsection 5.1,
we use He weights initialization, LeakyReLU activation for all convolutional
layers except for the last one, and the sigmoid function before the output
to map pixel values to the interval (0,1). We use batch size of 64 with a
learning rate η = 10−4. Other parameters depend on the dataset:
• For the MNIST dataset, we use a margin α = 0.3, π = 0.9, and L
emb 2
distance as the loss function (see subsubsection 4.2.2).
• For the LFW dataset, we use a margin α = 0.2,π = 0.1, and the
emb
combined distance (see subsubsection 4.2.5).
6. Results
In this section, we analyze the efficacy of the proposed approach after
training the neural networks.
6.1. Image Distance Comparison
Despitethenoticeablechangesbetweentheoriginalandgeneratedimages,
depicted in the Table 1, we still need to provide a quantitative representa-
tion of the difference. We will compare images in the following three setups:
18“real vs generated same class”, “real vs real different classes”, “generated vs
generated different classes”. We use the L distance d⟨L2⟩ defined in subsub-
2 img
section 4.2.2 as a difference metric. We get results specified in Table 2.
As can be seen, the distance between authentic and generated images
have significant values. Consider the MNIST dataset as an example: even
for pairs of digit 4 with the minimum value of 0.773 and especially for
digit 1 with a maximum distance of 0.886. That highlights that the neural
network produced drastically different images in terms of MSE. At the same
time, for the MNIST dataset, the mean-squared difference remained the
samefor“generatedvsgenerated”pairs, indicatingthatgenerationstillkeeps
digits close to each other. In turn, for the LFW dataset, the opposite holds:
distances between generated faces are significant.
6.2. Image Encodings Comparison
To give an intuitive representation of predictions, we apply the PCA
(Ma´ckiewicz and Ratajczak, 1993) and convert Rm vectors to vectors R3,
which is easy to illustrate on the 3D plot.
That being said, we firstly take a batch of images B := {X }nB , generate
i i=1
distorted images B = G(B), and then generate two sets of embeddings:
G
F(B) and F(B ). Finally, we apply the PCA to generate three-dimensional
G
representations of Rm embeddings. Results are depicted in the Figure 7. As
can be seen, embeddings of the same class almost do not change under the
generator transformation and remain close to each other.
6.3. Dependency on the Margin Parameter
We also tried different values of α to find the best fit. Results for different
values of α for the MNIST dataset are depicted in the Figure 8.
Asseen, forgreaterα’s, embeddingsaftergenerationbecomemoredistant
from the original ones, but the image distortion is much more considerable.
For instance, for α = 0.8, embeddings of digit 1 become entirely different from
the original ones, and thus this value should not be used for training. α = 0.4
shows a slight shift of embeddings location after generation, but they still
remain relatively close. In turn, α = 0 and α = 0.1 keep embeddings almost
unchanged. In our experiments, α = 0.2 gave us the best results, providing
a sufficient trade-off between embedding and image distances.
19Table 2: L distances between images of the same digit in three different
2
setups specified as columns. We mark in bold extreme values and highlight
in green the best result and in orange the worst in terms of Real-Gen
distances. As can be seen for both MNIST and LFW datasets, the difference
between real and generated images greatly exceeds “Real-Real” distances.
We use 20% images from both datasets: approximately 2.6k images for LFW
and 12k for MNIST.
Class Real-Gen Real-Real Gen-Gen
0 0.791 0.129 0.082
1 0.886 0.057 0.031
2 0.850 0.128 0.108
3 0.850 0.113 0.109
4 0.773 0.104 0.049
5 0.843 0.120 0.106
6 0.826 0.112 0.058
7 0.845 0.096 0.062
8 0.838 0.115 0.087
9 0.800 0.097 0.048
Class Real-Gen Real-Real Gen-Gen
George W. Bush 0.295 0.046 0.129
Colin Powell 0.267 0.042 0.132
Tony Blair 0.298 0.046 0.122
Donald Rumsfeld 0.278 0.045 0.120
Gerhard Schro¨der 0.293 0.043 0.107
Ariel Sharon 0.273 0.046 0.145
Hugo Chavez 0.263 0.041 0.126
Junichiro Koizumi 0.319 0.045 0.126
John Ashcroft 0.282 0.039 0.116
Jacques Chirac 0.297 0.051 0.106
20(a) MNIST dataset (b) LFW dataset
Figure 7: Embeddings of real and generated images after applying PCA for
3 batches of different classes. We used roughly 300 embeddings per class for
the MNIST dataset and roughly 30 per person for the LFW dataset.
Figure 8: PCA representation of embeddings, corresponding example of a
distorted digit of 1 and margin α.
216.4. Mock Recognition System
In this section, we verify that confusion matrices and ROC curves do not
differ significantly if we store distorted images instead of real ones.
For that, we conduct the following experiment: we place distorted im-
ages of three classes (for MNIST dataset, take 1,2,3 for concreteness) in an
improvised storage, being simply an in-memory hashmap in our case. Then,
we:
1. take 1000 non-distorted images belonging to these three classes and try
entering into the “system”;
2. take 1000 non-distorted images of any other three classes (except for
previously chosen triplet) and try logging in.
We expect the former to be successful login attempts while the latter to
be invalid authorizations. We then build confusion matrices by providing a
number of TPs (true positives), TNs (true negatives), FPs (false positives),
and FNs (false negatives). We then calculate the following metrics:
TP TP
Precision = , Recall = , (22)
TP+FP TP+FN
2×Precision×Recall
F = . (23)
1
Precision+Recall
We take 1000 different values for a threshold τ in range [0,4] and classify
images X,Y to be of the same class if d (X,Y) < τ and of different ones
F
otherwise. We then chose a threshold that provided us with the best F score
1
and built the corresponding confusion matrix. We get results depicted in the
Table 3 and ROC curves depicted in Figure 9.
Aswecansee, accuracymetricsdonotdiffersignificantlyundertheimage
distortion and therefore we have successfully achieved our goal. Moreover,
the distortion-generated technique even slightly outscored the non-distortive
approach.
6.5. Limitations
Certainly, during the training process, we encountered numerous issues
and obstacles, some of which are depicted in the Table 4 together with the
causes. Some of them include:
• Vanishing or exploding gradients: the generator model produces
the same blank image regardless of the input.
22Table 3: Confusion matrices and metric values for authentication system
with(a) and without(b) distorting original inputs.
MNIST Dataset
(a) Without distortion (b) With distortion
Prediction Prediction
Positive Negative Positive Negative
Positive 1174 26 Positive 1171 29
Negative 29 1171 Negative 32 1168
Precision 97.59% Precision 97.34% (↓ 0.25%)
Recall 97.83% Recall 97.58% (↓ 0.25%)
F score 97.71% F score 97.46% (↓ 0.25%)
1 1
LFW Dataset
Prediction Prediction
Positive Negative Positive Negative
Positive 1130 70 Positive 1142 58
Negative 80 1120 Negative 57 1143
Precision 93.34% Precision 95.25% (↑ 1.91%)
Recall 94.17% Recall 95.17% (↑ 1.00%)
F score 93.78% F score 95.21% (↑ 1.43%)
1 1
23
lautcA
lautcA
lautcA
lautcA(a) MNIST dataset (b) LFW dataset
Figure 9: ROC curve for a mock authentication system using (a) MNIST
and (b) LFW datasets. Red color represents the curve for a case where we
store distorted images in the storage while blue color corresponds to storing
real images.
• Highlighting the contours without concealing effect: the gener-
ator model “cheats” by not changing the contours but instead changing
the content inside them. This results in an image, from which it is easy
to recognize the face.
• Changing the color gamma: the neural network simply changes the
image’s gamma, which surely does not conceal the face.
7. Comparison to other research
In this section, we delve deeper into the comparative analysis of our Non-
Distortive Cancelable Biometrics system with existing notable works in the
field of biometric security. The focus is on understanding how our approach
aligns with or diverges from these established methods, particularly in terms
of performance metrics like the Equal Error Rate (EER).
In the comparative analysis presented in Table 5, we juxtapose the EER
of various biometric authentication systems, including our own, against a
backdrop of diverse datasets and biometric modalities. This table serves as
24Table 4: Three primary challenges when training the generator model: van-
ishing gradients, highlighting the contours, and changing the color gamma,
and corresponding examples with possible causes.
MNIST LFW
Problem Possible Cause
Real Generated Real Generated
1. Too large learning rate.
Vanishing
2. Too small π or
or exploding emb
too large α: ignoring
gradients
preserving embeddings.
1. Too large π :
emb
focusing too much
Highlighting the
on saving embeddings.
contours without
2. Too small α.
concealing effect
3. Typically happens
for SSIM loss.
1. Bad balance
between π ,
Changing the emb
learning rate, and α.
color gamma
2. Typically happens
for L or L loss.
1 2
25a crucial benchmark, allowing us to contextualize our Non-Distortive Cance-
lable Biometrics system within the broader landscape of biometric security
research.
Table 5: Comparative Analysis of Biometric Authentication Systems
Source Type of Images, Dataset EER, (%)
Yang et al. (2022b) Fingerprint, FVC2002 0.5 – 4.5
Yang et al. (2022b) Fingerprint, FVC2004 2.7 – 6.3
Yang et al. (2022b) Face, LFW 1.9
Yang et al. (2022b) Fingerprint, FVC2002 7.6 – 9.4
Yang et al. (2022b) Fingerprint, FVC2004 15.6
Kaur and Khanna (2020) Face, CASIA 2.2 – 9.3
Yang et al. (2021) Fingerprint, FVC2002 1.0 – 4.0
Yang et al. (2021) Fingerprint, FVC2004 11
Wang et al. (2017b) Fingerprint, FVC2002 1.0 – 5.2
Wang et al. (2017b) Fingerprint, FVC2004 13.3
Our Work Numbers, MNIST 2.5
Our Work Face, LFW 4.8
The work of Lee et al. (2021) in multimodal biometric systems stands
out for its impressive EER range, particularly in fingerprint recognition on
the FVC2002 and FVC2004 datasets, and facial recognition on the LFW
dataset. Their EERs, spanning from as low as 0.5% to 6.3%, underscore
the efficacy of leveraging multiple biometric modalities. This multimodal
approach, by integrating diverse biometric data, enhances the overall system
robustness, a feature that our system aims to emulate in a single-modality
context.
Yang et al. (2022b) present a higher EER for fingerprint recognition, par-
ticularly on the FVC2004 dataset, where the EER peaks at 15.6%. This
elevated rate could be indicative of the challenges inherent in the dataset
or perhaps limitations in the methodological approach they employed. In
contrast, our system, while not directly comparable due to different modali-
ties, shows a more favorable EER of 4.8% for facial recognition on the LFW
dataset, suggesting a more robust performance in handling biometric vari-
ability.
Kaur and Khanna (2020) explore facial biometrics using the CASIA
dataset, with their EER ranging from 2.2% to 9.3%. The broad range of
26their EER might reflect the varying complexities within the dataset and the
adaptability of their system to different facial features. Our system, while
tested on a different facial dataset (LFW), demonstrates a competitive edge
with a consistent EER, highlighting its potential for reliable performance
across diverse facial data.
The studies by Yang et al. (2021) and Wang et al. (2017b) focus on fin-
gerprint biometrics, with EERs that offer a balanced perspective on security
and usability. Yang et al. (2021) report EERs ranging from 1.0% to 4.0% for
FVC2002 and 11% for FVC2004, while Wang et al. (2017b) present EERs
from 1.0% to 5.2% for FVC2002 and 13.3% for FVC2004. These results,
though specific to fingerprint biometrics, provide valuable insights into the
efficacy of different biometric processing techniques, which are instrumental
in guiding our approach to facial biometric authentication.
Our work, with an EER of 2.5% on the MNIST dataset and 4.8% on
the LFW dataset, demonstrates a promising balance between security and
usability. The MNIST dataset, though less complex, serves as a foundational
testbed, validating the core principles of our approach. The LFW dataset,
more representative of real-world scenarios, further affirms the robustness
and applicability of our system in a practical context.
Insummary,ourcomparativeanalysisnotonlysituatesourNon-Distortive
Cancelable Biometrics system within the current state of biometric security
research but also highlights its potential as a competitive and innovative so-
lution. Bymaintainingtheintegrityoforiginalbiometricdataandleveraging
advanced AI algorithms, our system emerges as a promising candidate for
future biometric authentication applications, balancing the dual imperatives
of security and user convenience.
8. Conclusion
This paper has presented a novel approach to biometric security that
maintains the integrity of original biometric data while ensuring robust se-
curity and privacy. The experimental results, leveraging the MNIST and
LFW datasets and advanced deep learning algorithms, have demonstrated
the feasibility and effectiveness of this innovative system.
Key findings include:
• Feasibility of Non-Distortive Approach. The experiments have
successfully shown that it is possible to generate cancelable biometric
27templates that retain high similarity in AI metrics while appearing
significantly different in traditional metrics. This finding is crucial as it
validates the core premise of the Non-Distortive Cancelable Biometrics
system.
• AI-Driven Metric Similarity. AI algorithms, particularly convo-
lutional neural networks, have proven effective in maintaining metric
similarity between the original and transformed biometric data. This
aspect underscores the potential of AI in enhancing biometric security.
• SecurityandPrivacy. Thesystem’sabilitytogeneratenon-invertible
and non-replicable biometric templates addresses significant concerns
regarding data security and user privacy in traditional biometric sys-
tems.
• Operational Flexibility. The adaptability of the system to various
biometric modalities and its scalability across different platforms.
The promising results of this study pave the way for further research and
development in this field. Future work could focus on:
• Enhancing AI Algorithms. Continuous improvement of the AI al-
gorithms for more nuanced feature extraction and comparison.
• Expanding Biometric Modalities. Exploringtheapplicationofthis
system to other biometric data types such as voice recognition or gait
analysis.
• Real-World Implementation. Testing the system in real-world sce-
narios to assess its practicality and performance under varied condi-
tions.
In conclusion, the Non-Distortive Cancelable Biometrics system represents a
significant step forward in biometric security. Its ability to balance security,
privacy, and operational efficiency sets a new benchmark for future biomet-
ric systems. The insights from this research contribute substantially to the
ongoing discourse in biometric technology, offering a viable and innovative
solution to the challenges faced in this rapidly evolving field.
289. CRediT authorship contribution statement
DmytroZakharov: Methodology,Writing–originaldraft. OleksandrKuznetsov:
Conceptualization & Data curation, Writing – review & editing. Emanuele
Frontoni: Investigation & Supervision.
10. Declaration of competing interest
The authors declare that they have no known competing financial inter-
ests or personal relationships that could have appeared to influence the work
reported in this paper.
11. Data availability
Data will be made available on request.
12. Acknowledgement
• This project has received funding from the European Union’s Horizon
2020 research and innovation programme under the Marie Sk(cid:32)lodowska-
Curie grant agreement No. 101007820 - TRUST. This publication re-
flects only the author’s view and the REA is not responsible for any
use that may be made of the information it contains.
• This research was funded by the European Union – NextGenerationEU
under the Italian Ministry of University and Research (MIUR), Na-
tionalInnovationEcosystemgrantECS00000041-VITALITY-CUPD83C22000710005.
References
Akdogan, D., Karaoglan Altop, D., Eskandarian, L., Levi, A., 2018. Se-
cure key agreement protocols: Pure biometrics and cancelable biometrics.
Computer Networks 142, 33–48. doi:10.1016/j.comnet.2018.06.001.
Amin, R., Gaber, T., ElTaweel, G., Hassanien, A.E., 2014. Biometric and
traditional mobile authentication techniques: Overviews and open issues.
Bio-inspiring cyber security and cloud services: trends and innovations ,
423–446.
29Bansal, V., Garg, S., 2022. A cancelable biometric identification scheme
based on bloom filter and format-preserving encryption. Journal of King
Saud University - Computer and Information Sciences 34, 5810–5821.
doi:10.1016/j.jksuci.2022.01.014.
Bok-Min, G., Abanda, Y., Tiedeu, A., Kom, G., 2021. Image encryption
with fusion of two maps. Security and Communication Networks 2021,
6624890. URL: https://doi.org/10.1155/2021/6624890, doi:10.1155/
2021/6624890.
Cao, G., Yang, Y., Lei, J., Jin, C., Liu, Y., Song, M., 2017. Tripletgan:
Training generative model with triplet loss. CoRR abs/1711.05084. URL:
http://arxiv.org/abs/1711.05084, arXiv:1711.05084.
Deng, L., 2012. The mnist database of handwritten digit images for machine
learning research. IEEE Signal Processing Magazine 29, 141–142.
Dong, X., Shen, J., 2018. Triplet loss in siamese network for object tracking,
in: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (Eds.), Computer
Vision – ECCV 2018, Springer International Publishing, Cham. pp. 472–
488.
Galbally, J., Fierrez, J., Ortega-Garc´ıa, J., 2007. Vulnerabilities in biometric
systems: Attacks and recent advances in liveness detection. Database 1,
1–8.
Gatys, L.A., Ecker, A.S., Bethge, M., 2016. Image style transfer using con-
volutional neural networks, in: 2016 IEEE Conference on Computer Vi-
sionandPatternRecognition(CVPR),pp.2414–2423. doi:10.1109/CVPR.
2016.265.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
Ozair, S., Courville, A., Bengio, Y., 2014. Generative adversarial nets.
Advances in neural information processing systems 27.
Guo, D., Ge, S., Zhang, S., Gao, S., Tao, R., Wang, Y., 2022. Deepssn: A
deep convolutional neural network to assess spatial scene similarity. Trans-
actions in GIS 26. doi:10.1111/tgis.12915.
30Hamme, T.V., Garofalo, G., Joos, S., Preuveneers, D., Joosen, W., 2022.
Ai for biometric authentication systems, in: Security and Artificial Intel-
ligence: A Crossdisciplinary Approach. Springer, pp. 156–180.
Helmy, M., El-Shafai, W., El-Rabaie, E.S.M., El-Dokany, I.M., El-Samie,
F.E.A., 2022. A hybrid encryption framework based on rubik’s cube
for cancelable biometric cyber security applications. Optik 258, 168773.
doi:10.1016/j.ijleo.2022.168773.
Hoffer, E., Ailon, N., 2015. Deep metric learning using triplet network,
in: Similarity-Based Pattern Recognition: Third International Workshop,
SIMBAD 2015, Copenhagen, Denmark, October 12-14, 2015. Proceedings
3, Springer. pp. 84–92.
Huang, G.B., Ramesh, M., Berg, T., Learned-Miller, E., 2007. Labeled
Faces in the Wild: A Database for Studying Face Recognition in Un-
constrained Environments. Technical Report 07-49. University of Mas-
sachusetts, Amherst.
Isola, P., Zhu, J., Zhou, T., Efros, A.A., 2016. Image-to-image translation
withconditionaladversarialnetworks. CoRRabs/1611.07004. URL:http:
//arxiv.org/abs/1611.07004, arXiv:1611.07004.
Kauba, C., Piciucco, E., Maiorana, E., Gomez-Barrero, M., Prommegger,
B., Campisi, P., Uhl, A., 2022. Towards practical cancelable biometrics for
finger vein recognition. Information Sciences 585, 395–417. doi:10.1016/
j.ins.2021.11.018.
Kaur, H., Khanna, P., 2020. Privacy preserving remote multi-server biomet-
ric authentication using cancelable biometrics and secret sharing. Future
Generation Computer Systems 102, 30–41. doi:10.1016/j.future.2019.
07.023.
Kausar, F., 2021. Iris based cancelable biometric cryptosystem for secure
healthcare smart card. Egyptian Informatics Journal 22, 447–453. doi:10.
1016/j.eij.2021.01.004.
Kingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization.
arXiv preprint arXiv:1412.6980 .
31Kumar, S.K., 2017. On weight initialization in deep neural net-
works. CoRR abs/1704.08863. URL: http://arxiv.org/abs/1704.
08863, arXiv:1704.08863.
Le, H.M., Samaras, D., 2019. Shadow removal via shadow image decomposi-
tion. CoRR abs/1908.08628. URL: http://arxiv.org/abs/1908.08628,
arXiv:1908.08628.
Lee, M.J., Teoh, A.B.J., Uhl, A., Liang, S.N., Jin, Z., 2021. A tokenless can-
cellable scheme for multimodal biometric systems. Computers & Security
108, 102350. doi:10.1016/j.cose.2021.102350.
Liu, Z., Yin, H., Wu, X., Wu, Z., Mi, Y., Wang, S., 2021. From shadow
generation to shadow removal. CoRR abs/2103.12997. URL: https://
arxiv.org/abs/2103.12997, arXiv:2103.12997.
Ma´ckiewicz, A., Ratajczak, W., 1993. Principal components analy-
sis (pca). Computers & Geosciences 19, 303–342. URL: https:
//www.sciencedirect.com/science/article/pii/009830049390090R,
doi:https://doi.org/10.1016/0098-3004(93)90090-R.
Matoba, O., Nomura, T., Perez-Cabre, E., Millan, M.S., Javidi, B., 2009.
Optical techniques for information security. Proceedings of the IEEE 97,
1128–1148. doi:10.1109/JPROC.2009.2018367.
Murakami, T., Ohki, T., Kaga, Y., Fujio, M., Takahashi, K., 2019. Can-
celable indexing based on low-rank approximation of correlation-invariant
randomfilteringforfastandsecurebiometricidentification. PatternRecog-
nition Letters 126, 11–20. doi:10.1016/j.patrec.2018.04.005.
Nayar, G.R., Thomas, T., Emmanuel, S., 2021. Graph based secure cance-
lable palm vein biometrics. Journal of Information Security and Applica-
tions 62, 102991. doi:10.1016/j.jisa.2021.102991.
Parkhi, O., Vedaldi, A., Zisserman, A., 2015. Deep face recognition, in:
BMVC 2015-Proceedings of the British Machine Vision Conference 2015,
British Machine Vision Association.
Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks
for biomedical image segmentation. CoRR abs/1505.04597. URL: http:
//arxiv.org/abs/1505.04597, arXiv:1505.04597.
32Schroff, F., Kalenichenko, D., Philbin, J., 2015. Facenet: A unified embed-
ding for face recognition and clustering. CoRR abs/1503.03832. URL:
http://arxiv.org/abs/1503.03832, arXiv:1503.03832.
Spruyt, V., 2018. Loc2vec: Learning location embed-
dings with triplet-loss networks. Sentiance web article:
https://www.sentiance.com/2018/05/03/venue-mapping .
Subramanian, N., Elharrouss, O., Al-Maadeed, S., Bouridane, A., 2021. Im-
age steganography: A review of the recent advances. IEEE Access 9,
23409–23423. doi:10.1109/ACCESS.2021.3053998.
Vasluianu, F.A., Romero, A., Van Gool, L., Timofte, R., 2021. Shadow re-
moval with paired and unpaired learning, in: 2021 IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops (CVPRW), pp.
826–835. doi:10.1109/CVPRW53098.2021.00092.
Wang, F., Xiang, X., Cheng, J., Yuille, A.L., 2017a. Normface: L hy-
2
persphere embedding for face verification. CoRR abs/1704.06369. URL:
http://arxiv.org/abs/1704.06369, arXiv:1704.06369.
Wang, S., Deng, G., Hu, J., 2017b. A partial hadamard transform approach
to the design of cancelable fingerprint templates containing binary bio-
metric representations. Pattern Recognition 61, 447–458. doi:10.1016/j.
patcog.2016.08.017.
Yang, P., Zhang, M., Wu, R., Su, Y., Guo, K., 2022a. Hiding image within
image based on deep learning. Journal of Physics: Conference Series
2337, 012009. URL: https://dx.doi.org/10.1088/1742-6596/2337/1/
012009, doi:10.1088/1742-6596/2337/1/012009.
Yang, W., Wang, S., Hu, J., Zheng, G., Valli, C., 2018. A fingerprint and
finger-vein based cancelable multi-biometric system. Pattern Recognition
78, 242–251. doi:10.1016/j.patcog.2018.01.026.
Yang, W.,Wang, S.,Kang, J.J.,Johnstone,M.N., Bedari, A.,2022b. Alinear
convolution-based cancelable fingerprint biometric authentication system.
Computers & Security 114, 102583. doi:10.1016/j.cose.2021.102583.
33Yang, W., Wang, S., Shahzad, M., Zhou, W., 2021. A cancelable bio-
metric authentication system based on feature-adaptive random projec-
tion. Journal of Information Security and Applications 58, 102704.
doi:10.1016/j.jisa.2020.102704.
Zhang, S., Zhang, Q., Wei, X., Zhang, Y., Xia, Y., 2018. Person re-
identification with triplet focal loss. IEEE Access 6, 78092–78099. doi:10.
1109/ACCESS.2018.2884743.
Zhang, Y., Zhang, L.Y., Zhou, J., Liu, L., Chen, F., He, X., 2016. A review
of compressive sensing in information security field. IEEE Access 4, 2507–
2519. doi:10.1109/ACCESS.2016.2569421.
Zhao, H., Gallo, O., Frosio, I., Kautz, J., 2017. Loss functions for image
restoration with neural networks. IEEE Transactions on Computational
Imaging 3, 47–57. doi:10.1109/TCI.2016.2644865.
Zhmoginov, A., Sandler, M., 2016. Inverting face embeddings with convo-
lutional neural networks. ArXiv abs/1606.04189. URL: https://api.
semanticscholar.org/CorpusID:15785666.
34