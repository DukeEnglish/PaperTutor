[
    {
        "title": "Annotated Hands for Generative Models",
        "authors": "Yue YangAtith N GandhiGreg Turk",
        "links": "http://arxiv.org/abs/2401.15075v1",
        "entry_id": "http://arxiv.org/abs/2401.15075v1",
        "pdf_url": "http://arxiv.org/pdf/2401.15075v1",
        "summary": "Generative models such as GANs and diffusion models have demonstrated\nimpressive image generation capabilities. Despite these successes, these\nsystems are surprisingly poor at creating images with hands. We propose a novel\ntraining framework for generative models that substantially improves the\nability of such systems to create hand images. Our approach is to augment the\ntraining images with three additional channels that provide annotations to\nhands in the image. These annotations provide additional structure that coax\nthe generative model to produce higher quality hand images. We demonstrate this\napproach on two different generative models: a generative adversarial network\nand a diffusion model. We demonstrate our method both on a new synthetic\ndataset of hand images and also on real photographs that contain hands. We\nmeasure the improved quality of the generated hands through higher confidence\nin finger joint identification using an off-the-shelf hand detector.",
        "updated": "2024-01-26 18:57:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.15075v1"
    },
    {
        "title": "From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities",
        "authors": "Chaochao LuChen QianGuodong ZhengHongxing FanHongzhi GaoJie ZhangJing ShaoJingyi DengJinlan FuKexin HuangKunchang LiLijun LiLimin WangLu ShengMeiqi ChenMing ZhangQibing RenSirui ChenTao GuiWanli OuyangYali WangYan TengYaru WangYi WangYinan HeYingchun WangYixu WangYongting ZhangYu QiaoYujiong ShenYurong MouYuxi ChenZaibin ZhangZhelun ShiZhenfei YinZhipin Wang",
        "links": "http://arxiv.org/abs/2401.15071v2",
        "entry_id": "http://arxiv.org/abs/2401.15071v2",
        "pdf_url": "http://arxiv.org/pdf/2401.15071v2",
        "summary": "Multi-modal Large Language Models (MLLMs) have shown impressive abilities in\ngenerating reasonable responses with respect to multi-modal contents. However,\nthere is still a wide gap between the performance of recent MLLM-based\napplications and the expectation of the broad public, even though the most\npowerful OpenAI's GPT-4 and Google's Gemini have been deployed. This paper\nstrives to enhance understanding of the gap through the lens of a qualitative\nstudy on the generalizability, trustworthiness, and causal reasoning\ncapabilities of recent proprietary and open-source MLLMs across four\nmodalities: ie, text, code, image, and video, ultimately aiming to improve the\ntransparency of MLLMs. We believe these properties are several representative\nfactors that define the reliability of MLLMs, in supporting various downstream\napplications. To be specific, we evaluate the closed-source GPT-4 and Gemini\nand 6 open-source LLMs and MLLMs. Overall we evaluate 230 manually designed\ncases, where the qualitative results are then summarized into 12 scores (ie, 4\nmodalities times 3 properties). In total, we uncover 14 empirical findings that\nare useful to understand the capabilities and limitations of both proprietary\nand open-source MLLMs, towards more reliable downstream multi-modal\napplications.",
        "updated": "2024-01-29 15:18:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.15071v2"
    },
    {
        "title": "Deep learning-based approach for tomato classification in complex scenes",
        "authors": "Mikael A. MousseBethel C. A. R. K. AtohounCina Motamed",
        "links": "http://arxiv.org/abs/2401.15055v1",
        "entry_id": "http://arxiv.org/abs/2401.15055v1",
        "pdf_url": "http://arxiv.org/pdf/2401.15055v1",
        "summary": "Tracking ripening tomatoes is time consuming and labor intensive. Artificial\nintelligence technologies combined with those of computer vision can help users\noptimize the process of monitoring the ripening status of plants. To this end,\nwe have proposed a tomato ripening monitoring approach based on deep learning\nin complex scenes. The objective is to detect mature tomatoes and harvest them\nin a timely manner. The proposed approach is declined in two parts. Firstly,\nthe images of the scene are transmitted to the pre-processing layer. This\nprocess allows the detection of areas of interest (area of the image containing\ntomatoes). Then, these images are used as input to the maturity detection\nlayer. This layer, based on a deep neural network learning algorithm,\nclassifies the tomato thumbnails provided to it in one of the following five\ncategories: green, brittle, pink, pale red, mature red. The experiments are\nbased on images collected from the internet gathered through searches using\ntomato state across diverse languages including English, German, French, and\nSpanish. The experimental results of the maturity detection layer on a dataset\ncomposed of images of tomatoes taken under the extreme conditions, gave a good\nclassification rate.",
        "updated": "2024-01-26 18:33:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.15055v1"
    },
    {
        "title": "Unrecognizable Yet Identifiable: Image Distortion with Preserved Embeddings",
        "authors": "Dmytro ZakharovOleksandr KuznetsovEmanuele Frontoni",
        "links": "http://arxiv.org/abs/2401.15048v1",
        "entry_id": "http://arxiv.org/abs/2401.15048v1",
        "pdf_url": "http://arxiv.org/pdf/2401.15048v1",
        "summary": "In the realm of security applications, biometric authentication systems play\na crucial role, yet one often encounters challenges concerning privacy and\nsecurity while developing one. One of the most fundamental challenges lies in\navoiding storing biometrics directly in the storage but still achieving\ndecently high accuracy. Addressing this issue, we contribute to both artificial\nintelligence and engineering fields. We introduce an innovative image\ndistortion technique that effectively renders facial images unrecognizable to\nthe eye while maintaining their identifiability by neural network models. From\nthe theoretical perspective, we explore how reliable state-of-the-art\nbiometrics recognition neural networks are by checking the maximal degree of\nimage distortion, which leaves the predicted identity unchanged. On the other\nhand, applying this technique demonstrates a practical solution to the\nengineering challenge of balancing security, precision, and performance in\nbiometric authentication systems. Through experimenting on the widely used\ndatasets, we assess the effectiveness of our method in preserving AI feature\nrepresentation and distorting relative to conventional metrics. We also compare\nour method with previously used approaches.",
        "updated": "2024-01-26 18:20:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.15048v1"
    },
    {
        "title": "Learning Neural Radiance Fields of Forest Structure for Scalable and Fine Monitoring",
        "authors": "Juan Castorena",
        "links": "http://arxiv.org/abs/2401.15029v1",
        "entry_id": "http://arxiv.org/abs/2401.15029v1",
        "pdf_url": "http://arxiv.org/pdf/2401.15029v1",
        "summary": "This work leverages neural radiance fields and remote sensing for forestry\napplications. Here, we show neural radiance fields offer a wide range of\npossibilities to improve upon existing remote sensing methods in forest\nmonitoring. We present experiments that demonstrate their potential to: (1)\nexpress fine features of forest 3D structure, (2) fuse available remote sensing\nmodalities and (3), improve upon 3D structure derived forest metrics.\nAltogether, these properties make neural fields an attractive computational\ntool with great potential to further advance the scalability and accuracy of\nforest monitoring programs.",
        "updated": "2024-01-26 17:42:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.15029v1"
    }
]