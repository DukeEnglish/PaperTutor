[
    {
        "title": "Watermark-based Detection and Attribution of AI-Generated Content",
        "authors": "Zhengyuan JiangMoyang GuoYuepeng HuNeil Zhenqiang Gong",
        "links": "http://arxiv.org/abs/2404.04254v1",
        "entry_id": "http://arxiv.org/abs/2404.04254v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04254v1",
        "summary": "Several companies--such as Google, Microsoft, and OpenAI--have deployed\ntechniques to watermark AI-generated content to enable proactive detection.\nHowever, existing literature mainly focuses on user-agnostic detection.\nAttribution aims to further trace back the user of a generative-AI service who\ngenerated a given content detected as AI-generated. Despite its growing\nimportance, attribution is largely unexplored. In this work, we aim to bridge\nthis gap by providing the first systematic study on watermark-based, user-aware\ndetection and attribution of AI-generated content. Specifically, we\ntheoretically study the detection and attribution performance via rigorous\nprobabilistic analysis. Moreover, we develop an efficient algorithm to select\nwatermarks for the users to enhance attribution performance. Both our\ntheoretical and empirical results show that watermark-based detection and\nattribution inherit the accuracy and (non-)robustness properties of the\nwatermarking method.",
        "updated": "2024-04-05 17:58:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04254v1"
    },
    {
        "title": "Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution",
        "authors": "Tim SeydePeter WernerWilko SchwartingMarkus WulfmeierDaniela Rus",
        "links": "http://arxiv.org/abs/2404.04253v1",
        "entry_id": "http://arxiv.org/abs/2404.04253v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04253v1",
        "summary": "Recent reinforcement learning approaches have shown surprisingly strong\ncapabilities of bang-bang policies for solving continuous control benchmarks.\nThe underlying coarse action space discretizations often yield favourable\nexploration characteristics while final performance does not visibly suffer in\nthe absence of action penalization in line with optimal control theory. In\nrobotics applications, smooth control signals are commonly preferred to reduce\nsystem wear and energy efficiency, but action costs can be detrimental to\nexploration during early training. In this work, we aim to bridge this\nperformance gap by growing discrete action spaces from coarse to fine control\nresolution, taking advantage of recent results in decoupled Q-learning to scale\nour approach to high-dimensional action spaces up to dim(A) = 38. Our work\nindicates that an adaptive control resolution in combination with value\ndecomposition yields simple critic-only algorithms that yield surprisingly\nstrong performance on continuous control tasks.",
        "updated": "2024-04-05 17:58:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04253v1"
    },
    {
        "title": "Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)",
        "authors": "Michael SaxonFatima JaharaMahsa KhoshnoodiYujie LuAditya SharmaWilliam Yang Wang",
        "links": "http://arxiv.org/abs/2404.04251v1",
        "entry_id": "http://arxiv.org/abs/2404.04251v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04251v1",
        "summary": "With advances in the quality of text-to-image (T2I) models has come interest\nin benchmarking their prompt faithfulness-the semantic coherence of generated\nimages to the prompts they were conditioned on. A variety of T2I faithfulness\nmetrics have been proposed, leveraging advances in cross-modal embeddings and\nvision-language models (VLMs). However, these metrics are not rigorously\ncompared and benchmarked, instead presented against few weak baselines by\ncorrelation to human Likert scores over a set of easy-to-discriminate images.\n  We introduce T2IScoreScore (TS2), a curated set of semantic error graphs\ncontaining a prompt and a set increasingly erroneous images. These allow us to\nrigorously judge whether a given prompt faithfulness metric can correctly order\nimages with respect to their objective error count and significantly\ndiscriminate between different error nodes, using meta-metric scores derived\nfrom established statistical tests. Surprisingly, we find that the\nstate-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we\ntested fail to significantly outperform simple feature-based metrics like\nCLIPScore, particularly on a hard subset of naturally-occurring T2I model\nerrors. TS2 will enable the development of better T2I prompt faithfulness\nmetrics through more rigorous comparison of their conformity to expected\norderings and separations under objective criteria.",
        "updated": "2024-04-05 17:57:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04251v1"
    },
    {
        "title": "Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models",
        "authors": "Sangwon JangJaehyeong JoKimin LeeSung Ju Hwang",
        "links": "http://arxiv.org/abs/2404.04243v1",
        "entry_id": "http://arxiv.org/abs/2404.04243v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04243v1",
        "summary": "Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.",
        "updated": "2024-04-05 17:45:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04243v1"
    },
    {
        "title": "Physical Property Understanding from Language-Embedded Feature Fields",
        "authors": "Albert J. ZhaiYuan ShenEmily Y. ChenGloria X. WangXinlei WangSheng WangKaiyu GuanShenlong Wang",
        "links": "http://arxiv.org/abs/2404.04242v1",
        "entry_id": "http://arxiv.org/abs/2404.04242v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04242v1",
        "summary": "Can computers perceive the physical properties of objects solely through\nvision? Research in cognitive science and vision science has shown that humans\nexcel at identifying materials and estimating their physical properties based\npurely on visual appearance. In this paper, we present a novel approach for\ndense prediction of the physical properties of objects using a collection of\nimages. Inspired by how humans reason about physics through vision, we leverage\nlarge language models to propose candidate materials for each object. We then\nconstruct a language-embedded point cloud and estimate the physical properties\nof each 3D point using a zero-shot kernel regression approach. Our method is\naccurate, annotation-free, and applicable to any object in the open world.\nExperiments demonstrate the effectiveness of the proposed approach in various\nphysical property reasoning tasks, such as estimating the mass of common\nobjects, as well as other properties like friction and hardness.",
        "updated": "2024-04-05 17:45:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04242v1"
    }
]