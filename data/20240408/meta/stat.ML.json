[
    {
        "title": "Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation",
        "authors": "Mingyuan ZhouHuangjie ZhengZhendong WangMingzhang YinHai Huang",
        "links": "http://arxiv.org/abs/2404.04057v1",
        "entry_id": "http://arxiv.org/abs/2404.04057v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04057v1",
        "summary": "We introduce Score identity Distillation (SiD), an innovative data-free\nmethod that distills the generative capabilities of pretrained diffusion models\ninto a single-step generator. SiD not only facilitates an exponentially fast\nreduction in Fr\\'echet inception distance (FID) during distillation but also\napproaches or even exceeds the FID performance of the original teacher\ndiffusion models. By reformulating forward diffusion processes as semi-implicit\ndistributions, we leverage three score-related identities to create an\ninnovative loss mechanism. This mechanism achieves rapid FID reduction by\ntraining the generator using its own synthesized images, eliminating the need\nfor real data or reverse-diffusion-based generation, all accomplished within\nsignificantly shortened generation time. Upon evaluation across four benchmark\ndatasets, the SiD algorithm demonstrates high iteration efficiency during\ndistillation and surpasses competing distillation approaches, whether they are\none-step or few-step, data-free, or dependent on training data, in terms of\ngeneration quality. This achievement not only redefines the benchmarks for\nefficiency and effectiveness in diffusion distillation but also in the broader\nfield of diffusion-based generation. Our PyTorch implementation will be\npublicly accessible on GitHub.",
        "updated": "2024-04-05 12:30:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04057v1"
    },
    {
        "title": "Estimating mixed memberships in multi-layer networks",
        "authors": "Huan Qing",
        "links": "http://arxiv.org/abs/2404.03916v1",
        "entry_id": "http://arxiv.org/abs/2404.03916v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03916v1",
        "summary": "Community detection in multi-layer networks has emerged as a crucial area of\nmodern network analysis. However, conventional approaches often assume that\nnodes belong exclusively to a single community, which fails to capture the\ncomplex structure of real-world networks where nodes may belong to multiple\ncommunities simultaneously. To address this limitation, we propose novel\nspectral methods to estimate the common mixed memberships in the multi-layer\nmixed membership stochastic block model. The proposed methods leverage the\neigen-decomposition of three aggregate matrices: the sum of adjacency matrices,\nthe debiased sum of squared adjacency matrices, and the sum of squared\nadjacency matrices. We establish rigorous theoretical guarantees for the\nconsistency of our methods. Specifically, we derive per-node error rates under\nmild conditions on network sparsity, demonstrating their consistency as the\nnumber of nodes and/or layers increases under the multi-layer mixed membership\nstochastic block model. Our theoretical results reveal that the method\nleveraging the sum of adjacency matrices generally performs poorer than the\nother two methods for mixed membership estimation in multi-layer networks. We\nconduct extensive numerical experiments to empirically validate our theoretical\nfindings. For real-world multi-layer networks with unknown community\ninformation, we introduce two novel modularity metrics to quantify the quality\nof mixed membership community detection. Finally, we demonstrate the practical\napplications of our algorithms and modularity metrics by applying them to\nreal-world multi-layer networks, demonstrating their effectiveness in\nextracting meaningful community structures.",
        "updated": "2024-04-05 07:02:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03916v1"
    },
    {
        "title": "Nonparametric Modern Hopfield Models",
        "authors": "Jerry Yao-Chieh HuBo-Yu ChenDennis WuFeng RuanHan Liu",
        "links": "http://arxiv.org/abs/2404.03900v1",
        "entry_id": "http://arxiv.org/abs/2404.03900v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03900v1",
        "summary": "We present a nonparametric construction for deep learning compatible modern\nHopfield models and utilize this framework to debut an efficient variant. Our\nkey contribution stems from interpreting the memory storage and retrieval\nprocesses in modern Hopfield models as a nonparametric regression problem\nsubject to a set of query-memory pairs. Crucially, our framework not only\nrecovers the known results from the original dense modern Hopfield model but\nalso fills the void in the literature regarding efficient modern Hopfield\nmodels, by introducing \\textit{sparse-structured} modern Hopfield models with\nsub-quadratic complexity. We establish that this sparse model inherits the\nappealing theoretical properties of its dense analogue -- connection with\ntransformer attention, fixed point convergence and exponential memory capacity\n-- even without knowing details of the Hopfield energy function. Additionally,\nwe showcase the versatility of our framework by constructing a family of modern\nHopfield models as extensions, including linear, random masked, top-$K$ and\npositive random feature modern Hopfield models. Empirically, we validate the\nefficacy of our framework in both synthetic and realistic settings.",
        "updated": "2024-04-05 05:46:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03900v1"
    },
    {
        "title": "Wasserstein F-tests for Fréchet regression on Bures-Wasserstein manifolds",
        "authors": "Haoshu XuHongzhe Li",
        "links": "http://arxiv.org/abs/2404.03878v1",
        "entry_id": "http://arxiv.org/abs/2404.03878v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03878v1",
        "summary": "This paper considers the problem of regression analysis with random\ncovariance matrix as outcome and Euclidean covariates in the framework of\nFr\\'echet regression on the Bures-Wasserstein manifold. Such regression\nproblems have many applications in single cell genomics and neuroscience, where\nwe have covariance matrix measured over a large set of samples. Fr\\'echet\nregression on the Bures-Wasserstein manifold is formulated as estimating the\nconditional Fr\\'echet mean given covariates $x$. A non-asymptotic\n$\\sqrt{n}$-rate of convergence (up to $\\log n$ factors) is obtained for our\nestimator $\\hat{Q}_n(x)$ uniformly for $\\left\\|x\\right\\| \\lesssim \\sqrt{\\log\nn}$, which is crucial for deriving the asymptotic null distribution and power\nof our proposed statistical test for the null hypothesis of no association. In\naddition, a central limit theorem for the point estimate $\\hat{Q}_n(x)$ is\nobtained, giving insights to a test for covariate effects. The null\ndistribution of the test statistic is shown to converge to a weighted sum of\nindependent chi-squares, which implies that the proposed test has the desired\nsignificance level asymptotically. Also, the power performance of the test is\ndemonstrated against a sequence of contiguous alternatives. Simulation results\nshow the accuracy of the asymptotic distributions. The proposed methods are\napplied to a single cell gene expression data set that shows the change of gene\nco-expression network as people age.",
        "updated": "2024-04-05 04:01:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03878v1"
    },
    {
        "title": "Dimension-free Relaxation Times of Informed MCMC Samplers on Discrete Spaces",
        "authors": "Hyunwoong ChangQuan Zhou",
        "links": "http://arxiv.org/abs/2404.03867v1",
        "entry_id": "http://arxiv.org/abs/2404.03867v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03867v1",
        "summary": "Convergence analysis of Markov chain Monte Carlo methods in high-dimensional\nstatistical applications is increasingly recognized. In this paper, we develop\ngeneral mixing time bounds for Metropolis-Hastings algorithms on discrete\nspaces by building upon and refining some recent theoretical advancements in\nBayesian model selection problems. We establish sufficient conditions for a\nclass of informed Metropolis-Hastings algorithms to attain relaxation times\nthat are independent of the problem dimension. These conditions are grounded in\nhigh-dimensional statistical theory and allow for possibly multimodal posterior\ndistributions. We obtain our results through two independent techniques: the\nmulticommodity flow method and single-element drift condition analysis; we find\nthat the latter yields a tighter mixing time bound. Our results and proof\ntechniques are readily applicable to a broad spectrum of statistical problems\nwith discrete parameter spaces.",
        "updated": "2024-04-05 02:40:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03867v1"
    }
]