[
    {
        "title": "Watermark-based Detection and Attribution of AI-Generated Content",
        "authors": "Zhengyuan JiangMoyang GuoYuepeng HuNeil Zhenqiang Gong",
        "links": "http://arxiv.org/abs/2404.04254v1",
        "entry_id": "http://arxiv.org/abs/2404.04254v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04254v1",
        "summary": "Several companies--such as Google, Microsoft, and OpenAI--have deployed\ntechniques to watermark AI-generated content to enable proactive detection.\nHowever, existing literature mainly focuses on user-agnostic detection.\nAttribution aims to further trace back the user of a generative-AI service who\ngenerated a given content detected as AI-generated. Despite its growing\nimportance, attribution is largely unexplored. In this work, we aim to bridge\nthis gap by providing the first systematic study on watermark-based, user-aware\ndetection and attribution of AI-generated content. Specifically, we\ntheoretically study the detection and attribution performance via rigorous\nprobabilistic analysis. Moreover, we develop an efficient algorithm to select\nwatermarks for the users to enhance attribution performance. Both our\ntheoretical and empirical results show that watermark-based detection and\nattribution inherit the accuracy and (non-)robustness properties of the\nwatermarking method.",
        "updated": "2024-04-05 17:58:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04254v1"
    },
    {
        "title": "Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)",
        "authors": "Michael SaxonFatima JaharaMahsa KhoshnoodiYujie LuAditya SharmaWilliam Yang Wang",
        "links": "http://arxiv.org/abs/2404.04251v1",
        "entry_id": "http://arxiv.org/abs/2404.04251v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04251v1",
        "summary": "With advances in the quality of text-to-image (T2I) models has come interest\nin benchmarking their prompt faithfulness-the semantic coherence of generated\nimages to the prompts they were conditioned on. A variety of T2I faithfulness\nmetrics have been proposed, leveraging advances in cross-modal embeddings and\nvision-language models (VLMs). However, these metrics are not rigorously\ncompared and benchmarked, instead presented against few weak baselines by\ncorrelation to human Likert scores over a set of easy-to-discriminate images.\n  We introduce T2IScoreScore (TS2), a curated set of semantic error graphs\ncontaining a prompt and a set increasingly erroneous images. These allow us to\nrigorously judge whether a given prompt faithfulness metric can correctly order\nimages with respect to their objective error count and significantly\ndiscriminate between different error nodes, using meta-metric scores derived\nfrom established statistical tests. Surprisingly, we find that the\nstate-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we\ntested fail to significantly outperform simple feature-based metrics like\nCLIPScore, particularly on a hard subset of naturally-occurring T2I model\nerrors. TS2 will enable the development of better T2I prompt faithfulness\nmetrics through more rigorous comparison of their conformity to expected\norderings and separations under objective criteria.",
        "updated": "2024-04-05 17:57:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04251v1"
    },
    {
        "title": "Physical Property Understanding from Language-Embedded Feature Fields",
        "authors": "Albert J. ZhaiYuan ShenEmily Y. ChenGloria X. WangXinlei WangSheng WangKaiyu GuanShenlong Wang",
        "links": "http://arxiv.org/abs/2404.04242v1",
        "entry_id": "http://arxiv.org/abs/2404.04242v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04242v1",
        "summary": "Can computers perceive the physical properties of objects solely through\nvision? Research in cognitive science and vision science has shown that humans\nexcel at identifying materials and estimating their physical properties based\npurely on visual appearance. In this paper, we present a novel approach for\ndense prediction of the physical properties of objects using a collection of\nimages. Inspired by how humans reason about physics through vision, we leverage\nlarge language models to propose candidate materials for each object. We then\nconstruct a language-embedded point cloud and estimate the physical properties\nof each 3D point using a zero-shot kernel regression approach. Our method is\naccurate, annotation-free, and applicable to any object in the open world.\nExperiments demonstrate the effectiveness of the proposed approach in various\nphysical property reasoning tasks, such as estimating the mass of common\nobjects, as well as other properties like friction and hardness.",
        "updated": "2024-04-05 17:45:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04242v1"
    },
    {
        "title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
        "authors": "Harsh KohliHuan Sun",
        "links": "http://arxiv.org/abs/2404.04237v1",
        "entry_id": "http://arxiv.org/abs/2404.04237v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04237v1",
        "summary": "The rapid progress of large language models (LLMs) has seen them excel and\nfrequently surpass human performance on standard benchmarks. This has enabled\nmany downstream applications, such as LLM agents, to rely on their\nsophisticated reasoning to navigate complex task requirements. However, LLMs\nare known to unexpectedly falter in simple tasks and under seemingly\nstraightforward circumstances - underscoring the need for better and more\ndiverse evaluation setups to measure their true capabilities. To this end, we\nchoose to study compositional and conditional reasoning, two cornerstones of\nhuman cognition, and introduce GroundCocoa - a lexically diverse benchmark\nconnecting these reasoning skills to the real-world problem of flight booking.\nOur task involves aligning detailed user preferences with available flight\noptions presented in a multiple-choice format. Results indicate a significant\ndisparity in performance among current state-of-the-art LLMs with even the best\nperforming model, GPT-4 Turbo, not exceeding 67% accuracy despite advanced\nprompting techniques.",
        "updated": "2024-04-05 17:36:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04237v1"
    },
    {
        "title": "player2vec: A Language Modeling Approach to Understand Player Behavior in Games",
        "authors": "Tianze WangMaryam Honari-JahromiStyliani KatsarouOlga MikheevaTheodoros PanagiotakopoulosSahar AsadiOleg Smirnov",
        "links": "http://arxiv.org/abs/2404.04234v1",
        "entry_id": "http://arxiv.org/abs/2404.04234v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04234v1",
        "summary": "Methods for learning latent user representations from historical behavior\nlogs have gained traction for recommendation tasks in e-commerce, content\nstreaming, and other settings. However, this area still remains relatively\nunderexplored in video and mobile gaming contexts. In this work, we present a\nnovel method for overcoming this limitation by extending a long-range\nTransformer model from the natural language processing domain to player\nbehavior data. We discuss specifics of behavior tracking in games and propose\npreprocessing and tokenization approaches by viewing in-game events in an\nanalogous way to words in sentences, thus enabling learning player\nrepresentations in a self-supervised manner in the absence of ground-truth\nannotations. We experimentally demonstrate the efficacy of the proposed\napproach in fitting the distribution of behavior events by evaluating intrinsic\nlanguage modeling metrics. Furthermore, we qualitatively analyze the emerging\nstructure of the learned embedding space and show its value for generating\ninsights into behavior patterns to inform downstream applications.",
        "updated": "2024-04-05 17:29:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04234v1"
    }
]