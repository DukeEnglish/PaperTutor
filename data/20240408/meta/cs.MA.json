[
    {
        "title": "ROMA-iQSS: An Objective Alignment Approach via State-Based Value Learning and ROund-Robin Multi-Agent Scheduling",
        "authors": "Chi-Hui LinJoewie J. KohAlessandro RonconeLijun Chen",
        "links": "http://arxiv.org/abs/2404.03984v1",
        "entry_id": "http://arxiv.org/abs/2404.03984v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03984v1",
        "summary": "Effective multi-agent collaboration is imperative for solving complex,\ndistributed problems. In this context, two key challenges must be addressed:\nfirst, autonomously identifying optimal objectives for collective outcomes;\nsecond, aligning these objectives among agents. Traditional frameworks, often\nreliant on centralized learning, struggle with scalability and efficiency in\nlarge multi-agent systems. To overcome these issues, we introduce a\ndecentralized state-based value learning algorithm that enables agents to\nindependently discover optimal states. Furthermore, we introduce a novel\nmechanism for multi-agent interaction, wherein less proficient agents follow\nand adopt policies from more experienced ones, thereby indirectly guiding their\nlearning process. Our theoretical analysis shows that our approach leads\ndecentralized agents to an optimal collective policy. Empirical experiments\nfurther demonstrate that our method outperforms existing decentralized\nstate-based and action-based value learning strategies by effectively\nidentifying and aligning optimal objectives.",
        "updated": "2024-04-05 09:39:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03984v1"
    },
    {
        "title": "Understanding the Impact of Coalitions between EV Charging Stations",
        "authors": "Sukanya KudvaKshitij KulkarniChinmay MaheshwariAnil AswaniShankar Sastry",
        "links": "http://arxiv.org/abs/2404.03919v1",
        "entry_id": "http://arxiv.org/abs/2404.03919v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03919v1",
        "summary": "The rapid growth of electric vehicles (EVs) is driving the expansion of\ncharging infrastructure globally. This expansion, however, places significant\ncharging demand on the electricity grid, impacting grid operations and\nelectricity pricing. While coordination among all charging stations is\nbeneficial, it may not be always feasible. However, a subset of charging\nstations, which could be jointly operated by a company, could coordinate to\ndecide their charging profile. In this paper we investigate whether such\ncoalitions between charging stations is better than no coordination.\n  We model EV charging as a non-cooperative aggregative game, where each\nstation's cost is determined by both monetary payments tied to reactive\nelectricity prices on the grid and its sensitivity to deviations from a nominal\ncharging profile. We consider a solution concept that we call\n$\\mathcal{C}$-Nash equilibrium, which is tied to a coalition $\\mathcal{C}$ of\ncharging stations coordinating to reduce their cumulative costs. We provide\nsufficient conditions, in terms of the demand and sensitivity of charging\nstations, to determine when independent (uncoordinated) operation of charging\nstations could result in lower overall costs to charging stations, the\ncoalition, and charging stations outside the coalition. Somewhat counter to\nintuition, we demonstrate scenarios where allowing charging stations to operate\nindependently is better than coordinating as a coalition. Jointly, these\nresults provide operators of charging stations insights into how to coordinate\ntheir charging behavior, and open several research directions.",
        "updated": "2024-04-05 07:05:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03919v1"
    },
    {
        "title": "Holon: a cybernetic interface for bio-semiotics",
        "authors": "Jon McCormackElliott Wilson",
        "links": "http://arxiv.org/abs/2404.03894v1",
        "entry_id": "http://arxiv.org/abs/2404.03894v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03894v1",
        "summary": "This paper presents an interactive artwork, \"Holon\", a collection of 130\nautonomous, cybernetic organisms that listen and make sound in collaboration\nwith the natural environment. The work was developed for installation on water\nat a heritage-listed dock in Melbourne, Australia. Conceptual issues informing\nthe work are presented, along with a detailed technical overview of the\nimplementation. Individual holons are of three types, inspired by biological\nmodels of animal communication: composer/generators, collector/critics and\ndisruptors. Collectively, Holon integrates and occupies elements of the\nacoustic spectrum in collaboration with human and non-human agents.",
        "updated": "2024-04-05 05:03:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03894v1"
    },
    {
        "title": "Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable Collaboration",
        "authors": "Xudong GuoDaming ShiJunjie YuWenhui Fan",
        "links": "http://arxiv.org/abs/2404.03869v1",
        "entry_id": "http://arxiv.org/abs/2404.03869v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03869v1",
        "summary": "The rise of multi-agent systems, especially the success of multi-agent\nreinforcement learning (MARL), is reshaping our future across diverse domains\nlike autonomous vehicle networks. However, MARL still faces significant\nchallenges, particularly in achieving zero-shot scalability, which allows\ntrained MARL models to be directly applied to unseen tasks with varying numbers\nof agents. In addition, real-world multi-agent systems usually contain agents\nwith different functions and strategies, while the existing scalable MARL\nmethods only have limited heterogeneity. To address this, we propose a novel\nMARL framework named Scalable and Heterogeneous Proximal Policy Optimization\n(SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL\nnetworks. we first leverage a latent network to adaptively learn strategy\npatterns for each agent. Second, we introduce a heterogeneous layer for\ndecision-making, whose parameters are specifically generated by the learned\nlatent variables. Our approach is scalable as all the parameters are shared\nexcept for the heterogeneous layer, and gains both inter-individual and\ntemporal heterogeneity at the same time. We implement our approach based on the\nstate-of-the-art backbone PPO-based algorithm as SHPPO, while our approach is\nagnostic to the backbone and can be seamlessly plugged into any\nparameter-shared MARL method. SHPPO exhibits superior performance over the\nbaselines such as MAPPO and HAPPO in classic MARL environments like Starcraft\nMulti-Agent Challenge (SMAC) and Google Research Football (GRF), showcasing\nenhanced zero-shot scalability and offering insights into the learned latent\nrepresentation's impact on team performance by visualization.",
        "updated": "2024-04-05 03:02:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03869v1"
    },
    {
        "title": "Laser Learning Environment: A new environment for coordination-critical multi-agent tasks",
        "authors": "Yannick MolinghenRaphaël AvalosMark Van AchterAnn NowéTom Lenaerts",
        "links": "http://arxiv.org/abs/2404.03596v1",
        "entry_id": "http://arxiv.org/abs/2404.03596v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03596v1",
        "summary": "We introduce the Laser Learning Environment (LLE), a collaborative\nmulti-agent reinforcement learning environment in which coordination is\ncentral. In LLE, agents depend on each other to make progress\n(interdependence), must jointly take specific sequences of actions to succeed\n(perfect coordination), and accomplishing those joint actions does not yield\nany intermediate reward (zero-incentive dynamics). The challenge of such\nproblems lies in the difficulty of escaping state space bottlenecks caused by\ninterdependence steps since escaping those bottlenecks is not rewarded. We test\nmultiple state-of-the-art value-based MARL algorithms against LLE and show that\nthey consistently fail at the collaborative task because of their inability to\nescape state space bottlenecks, even though they successfully achieve perfect\ncoordination. We show that Q-learning extensions such as prioritized experience\nreplay and n-steps return hinder exploration in environments with\nzero-incentive dynamics, and find that intrinsic curiosity with random network\ndistillation is not sufficient to escape those bottlenecks. We demonstrate the\nneed for novel methods to solve this problem and the relevance of LLE as\ncooperative MARL benchmark.",
        "updated": "2024-04-04 17:05:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03596v1"
    }
]