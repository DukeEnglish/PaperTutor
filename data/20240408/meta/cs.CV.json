[
    {
        "title": "Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation",
        "authors": "Zifu WanYuhao WangSilong YongPingping ZhangSimon StepputtisKatia SycaraYaqi Xie",
        "links": "http://arxiv.org/abs/2404.04256v1",
        "entry_id": "http://arxiv.org/abs/2404.04256v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04256v1",
        "summary": "Multi-modal semantic segmentation significantly enhances AI agents'\nperception and scene understanding, especially under adverse conditions like\nlow-light or overexposed environments. Leveraging additional modalities\n(X-modality) like thermal and depth alongside traditional RGB provides\ncomplementary information, enabling more robust and reliable segmentation. In\nthis work, we introduce Sigma, a Siamese Mamba network for multi-modal semantic\nsegmentation, utilizing the Selective Structured State Space Model, Mamba.\nUnlike conventional methods that rely on CNNs, with their limited local\nreceptive fields, or Vision Transformers (ViTs), which offer global receptive\nfields at the cost of quadratic complexity, our model achieves global receptive\nfields coverage with linear complexity. By employing a Siamese encoder and\ninnovating a Mamba fusion mechanism, we effectively select essential\ninformation from different modalities. A decoder is then developed to enhance\nthe channel-wise modeling ability of the model. Our method, Sigma, is\nrigorously evaluated on both RGB-Thermal and RGB-Depth segmentation tasks,\ndemonstrating its superiority and marking the first successful application of\nState Space Models (SSMs) in multi-modal perception tasks. Code is available at\nhttps://github.com/zifuwan/Sigma.",
        "updated": "2024-04-05 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04256v1"
    },
    {
        "title": "Watermark-based Detection and Attribution of AI-Generated Content",
        "authors": "Zhengyuan JiangMoyang GuoYuepeng HuNeil Zhenqiang Gong",
        "links": "http://arxiv.org/abs/2404.04254v1",
        "entry_id": "http://arxiv.org/abs/2404.04254v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04254v1",
        "summary": "Several companies--such as Google, Microsoft, and OpenAI--have deployed\ntechniques to watermark AI-generated content to enable proactive detection.\nHowever, existing literature mainly focuses on user-agnostic detection.\nAttribution aims to further trace back the user of a generative-AI service who\ngenerated a given content detected as AI-generated. Despite its growing\nimportance, attribution is largely unexplored. In this work, we aim to bridge\nthis gap by providing the first systematic study on watermark-based, user-aware\ndetection and attribution of AI-generated content. Specifically, we\ntheoretically study the detection and attribution performance via rigorous\nprobabilistic analysis. Moreover, we develop an efficient algorithm to select\nwatermarks for the users to enhance attribution performance. Both our\ntheoretical and empirical results show that watermark-based detection and\nattribution inherit the accuracy and (non-)robustness properties of the\nwatermarking method.",
        "updated": "2024-04-05 17:58:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04254v1"
    },
    {
        "title": "Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)",
        "authors": "Michael SaxonFatima JaharaMahsa KhoshnoodiYujie LuAditya SharmaWilliam Yang Wang",
        "links": "http://arxiv.org/abs/2404.04251v1",
        "entry_id": "http://arxiv.org/abs/2404.04251v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04251v1",
        "summary": "With advances in the quality of text-to-image (T2I) models has come interest\nin benchmarking their prompt faithfulness-the semantic coherence of generated\nimages to the prompts they were conditioned on. A variety of T2I faithfulness\nmetrics have been proposed, leveraging advances in cross-modal embeddings and\nvision-language models (VLMs). However, these metrics are not rigorously\ncompared and benchmarked, instead presented against few weak baselines by\ncorrelation to human Likert scores over a set of easy-to-discriminate images.\n  We introduce T2IScoreScore (TS2), a curated set of semantic error graphs\ncontaining a prompt and a set increasingly erroneous images. These allow us to\nrigorously judge whether a given prompt faithfulness metric can correctly order\nimages with respect to their objective error count and significantly\ndiscriminate between different error nodes, using meta-metric scores derived\nfrom established statistical tests. Surprisingly, we find that the\nstate-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we\ntested fail to significantly outperform simple feature-based metrics like\nCLIPScore, particularly on a hard subset of naturally-occurring T2I model\nerrors. TS2 will enable the development of better T2I prompt faithfulness\nmetrics through more rigorous comparison of their conformity to expected\norderings and separations under objective criteria.",
        "updated": "2024-04-05 17:57:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04251v1"
    },
    {
        "title": "Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner Attacks, And The Role of Distillation as Defense Mechanism",
        "authors": "Trilokesh Ranjan SarkarNilanjan DasPralay Sankar MaitraBijoy SomeRitwik SahaOrijita AdhikaryBishal BoseJaydip Sen",
        "links": "http://arxiv.org/abs/2404.04245v1",
        "entry_id": "http://arxiv.org/abs/2404.04245v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04245v1",
        "summary": "This technical report delves into an in-depth exploration of adversarial\nattacks specifically targeted at Deep Neural Networks (DNNs) utilized for image\nclassification. The study also investigates defense mechanisms aimed at\nbolstering the robustness of machine learning models. The research focuses on\ncomprehending the ramifications of two prominent attack methodologies: the Fast\nGradient Sign Method (FGSM) and the Carlini-Wagner (CW) approach. These attacks\nare examined concerning three pre-trained image classifiers: Resnext50_32x4d,\nDenseNet-201, and VGG-19, utilizing the Tiny-ImageNet dataset. Furthermore, the\nstudy proposes the robustness of defensive distillation as a defense mechanism\nto counter FGSM and CW attacks. This defense mechanism is evaluated using the\nCIFAR-10 dataset, where CNN models, specifically resnet101 and Resnext50_32x4d,\nserve as the teacher and student models, respectively. The proposed defensive\ndistillation model exhibits effectiveness in thwarting attacks such as FGSM.\nHowever, it is noted to remain susceptible to more sophisticated techniques\nlike the CW attack. The document presents a meticulous validation of the\nproposed scheme. It provides detailed and comprehensive results, elucidating\nthe efficacy and limitations of the defense mechanisms employed. Through\nrigorous experimentation and analysis, the study offers insights into the\ndynamics of adversarial attacks on DNNs, as well as the effectiveness of\ndefensive strategies in mitigating their impact.",
        "updated": "2024-04-05 17:51:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04245v1"
    },
    {
        "title": "DiffOp-net: A Differential Operator-based Fully Convolutional Network for Unsupervised Deformable Image Registration",
        "authors": "Jiong Wu",
        "links": "http://arxiv.org/abs/2404.04244v1",
        "entry_id": "http://arxiv.org/abs/2404.04244v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04244v1",
        "summary": "Existing unsupervised deformable image registration methods usually rely on\nmetrics applied to the gradients of predicted displacement or velocity fields\nas a regularization term to ensure transformation smoothness, which potentially\nlimits registration accuracy. In this study, we propose a novel approach to\nenhance unsupervised deformable image registration by introducing a new\ndifferential operator into the registration framework. This operator, acting on\nthe velocity field and mapping it to a dual space, ensures the smoothness of\nthe velocity field during optimization, facilitating accurate deformable\nregistration. In addition, to tackle the challenge of capturing large\ndeformations inside image pairs, we introduce a Cross-Coordinate Attention\nmodule (CCA) and embed it into a proposed Fully Convolutional Networks\n(FCNs)-based multi-resolution registration architecture. Evaluation experiments\nare conducted on two magnetic resonance imaging (MRI) datasets. Compared to\nvarious state-of-the-art registration approaches, including a traditional\nalgorithm and three representative unsupervised learning-based methods, our\nmethod achieves superior accuracies, maintaining desirable diffeomorphic\nproperties, and exhibiting promising registration speed.",
        "updated": "2024-04-05 17:46:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04244v1"
    }
]