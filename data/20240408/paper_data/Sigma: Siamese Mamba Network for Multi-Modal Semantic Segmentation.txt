Sigma : Siamese Mamba Network for
Multi-Modal Semantic Segmentation
Zifu Wan1 Yuhao Wang2 Silong Yong1 Pingping Zhang2
Simon Stepputtis1 Katia Sycara1 Yaqi Xie1
1 Robotics Institute, Carnegie Mellon University, USA
2 School of Future Technology, Dalian University of Technology, China
{zifuw, silongy, sstepput, katia, yaqix}@andrew.cmu.edu,
yuhaowangdlut@gmail.com, zhpp@dlut.edu.cn
Abstract. Multi-modal semantic segmentation significantly enhances
AIagents’perceptionandsceneunderstanding,especiallyunderadverse
conditions like low-light or overexposed environments. Leveraging addi-
tional modalities (X-modality) like thermal and depth alongside tradi-
tional RGB provides complementary information, enabling more robust
and reliable segmentation. In this work, we introduce Sigma, a Siamese
Mamba network for multi-modal semantic segmentation, utilizing the
Selective Structured State Space Model, Mamba. Unlike conventional
methods that rely on CNNs, with their limited local receptive fields,
or Vision Transformers (ViTs), which offer global receptive fields at the
cost of quadratic complexity, our model achieves global receptive fields
coverage with linear complexity. By employing a Siamese encoder and
innovating a Mamba fusion mechanism, we effectively select essential
information from different modalities. A decoder is then developed to
enhance the channel-wise modeling ability of the model. Our method,
Sigma, is rigorously evaluated on both RGB-Thermal and RGB-Depth
segmentation tasks, demonstrating its superiority and marking the first
successfulapplicationofStateSpaceModels(SSMs)inmulti-modalper-
ceptiontasks.Codeisavailableathttps://github.com/zifuwan/Sigma.
Keywords: Multi-Modal Scene Understanding · Semantic Segmenta-
tion · State Space Model · Vision Mamba
1 Introduction
Semantic segmentation, aiming to assign a semantic label for each pixel within
an image, has been increasingly significant for AI agents to accurately perceive
their environment [4,20]. However, current vision models still struggle in chal-
lenging conditions like low light or obstructive elements such as sun glare and
fire [4,71,72]. With the goal of enhancing segmentation under such challenging
conditions,additionalmodalitieslikethermalanddeptharebeneficialtoenhance
vision system robustness. With the supplementary information, the robustness
4202
rpA
5
]VC.sc[
1v65240.4042:viXra2 Z. Wan et al.
Sigma-S Sigma-B
60 Sigma-T CMNeXt23
EAEFNet23
58
CACFNet23 GMNet21
56 FEANet21 50M ABMDRNet21
100M
c 54 150M FuseSeg20
200M RTFNet19
75 100 125 150 175 200 225 250
FLOPs/G
(a) Comparisonofdifferentfusionmethods (b) ComparisonofFLOPsandmodelsize
Fig.1: (a) Comparative analysis of complexity across different fusion methods uti-
lizing Transformer and Mamba: Mamba-based fusion approaches significantly reduce
complexity by an order of magnitude compared to their Transformer-based counter-
parts. (b) Computation and model size comparison of Sigma and other methods. The
size of each circle indicates the model size (parameters).
and capabilities of vision pipelines can be improved [30,37,73]. However, utiliz-
ing multiple modalities introduces additional challenges, namely the alignment
and fusion of the information provided through these additional channels [12].
Previousapproachesinmulti-modalsemanticsegmentationrelyonConvolu-
tionalNeuralNetworks(CNN)orVisionTransformers(ViT).WhileCNN-based
approaches [13,29] are known for their scalability and linear complexity, they
suffer from the small receptive field limited by the kernel size, leading to local
reductive bias. Besides, CNNs utilize a weight-sharing kernel across different
parts of the input, limiting its flexibility in adapting to unseen or low-quality
images.Incontrast,ViT-basedmethods[1,30,52]offerenhancedvisualmodeling
by leveraging global receptive fields and dynamic weights. However, their self-
attention mechanism leads to quadratic complexity in terms of input sizes [16],
raising efficiency concerns. Attempts to improve this efficiency by reducing the
dimensions or stride of the processing windows compromise the extent of the
receptive fields [66].
To address these limitations, Selective Structured State Space Models –
Mamba [16] – have gained more popularity due to their global receptive field
coverage and dynamic weights with linear complexity. Mamba has shown out-
standingeffectivenessintasksinvolvinglongsequencemodeling,notablyinnat-
ural language processing [16]. Furthermore, more studies have explored its po-
tential for vision-related applications, such as image classification [33], medical
imagesegmentation[38,41,56,60,65],and3Dscenecomprehension[28].Inspired
by these benefits, we introduce Sigma , a Siamese Mamba network for multi-
modal sensor fusion utilizing the recent advances of Mamba and apply it to the
challenging domain of semantic segmentation.
AsdepictedinFig.2,ourSigmaintegratesaSiameseencoder[63]forfeature
extraction,fusionmodulesforaggregatinginformationfromdifferentmodalities,
and a decoder that is adapted to both spatial and channel-specific information.
The encoder backbone utilizes cascaded Visual State Space (VSS) Blocks with
UoIm
eziS
ledoMSigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation 3
downsamplingtoextractmulti-scaleglobalinformationfromvariousmodalities.
Subsequently, the extracted features are directed to a fusion module at each
level,wheremulti-modalfeaturesinitiallyinteractthroughaCrossMambaBlock
(CroMB) toaugmentcross-modalinformation.Followingthis,theenhancedfea-
tures are processed by a Concat Mamba Block (ConMB), which employs an
attention mechanism to select pertinent information from each modality. Our
fusion mechanism leverages the linear scaling property of Mamba, significantly
reducing computational demand, as illustrated in Fig. 1a. Finally, the fused fea-
tures are sent to multi-level Channel-Aware Visual State Space (CVSS) Blocks
to effectively capture multi-scale long-range information.
We conduct comprehensive experiments on both RGB-Thermal [21,45] and
RGB-Depth datasets [46,48], demonstrating that Sigma outperforms state-of-
the-art models on both accuracy and efficiency, as depicted in Fig. 1b. Detailed
ablation studies further validate the contribution of each component within
Sigma to the overall model efficacy.
Our contributions can be summarized as follows:
– To our best knowledge, this marks the first successful application of State
Space Models, specifically Mamba, in multi-modal semantic segmentation.
– Weintroduceanattention-basedMambafusionmechanismalongsideachannel-
aware Mamba decoder, to efficiently extract information across different
modalities and integrate them seamlessly.
– ComprehensiveevaluationsinRGB-ThermalandRGB-Depthdomainsshow-
caseourmethod’ssuperioraccuracyandefficiency,settinganewbenchmark
for future investigations into Mamba’s potential in multi-modal learning.
2 Related Work
2.1 Multi-Modal Semantic Segmentation
Multi-modal semantic understanding typically incorporates an RGB modality
for widespread application, alongside other complementary modalities such as
thermal, depth, LiDAR, etc. [12,75]. These supplementary sensors offer crucial
information to the vision system across various scenarios. For instance, thermal
sensors detect infrared radiation, enabling the identification of targets in dark
andfoggyconditionsthroughtemperaturedifferences.Thiscapabilityisessential
forapplicationssuchassurveillance,wildfirerescueoperations,andwildlifemon-
itoring [14]. Meanwhile, depth sensors ascertain the distance between the sensor
and objects in the environment, furnishing a three-dimensional representation
of the scene. This technology is extensively utilized in autonomous vehicles for
obstacle detection and scene understanding [11]. To optimize the utilization of
these additional modalities, the development of effective feature extractors and
fusion mechanisms has been of paramount importance.
In RGB-Thermal semantic segmentation, early attempts usually design an
encoder-decoderarchitecturewithshortcutconnection[21,45,49,50],densecon-
nection [50,61,77], dilated convolutions [62,76], knowledge distillation [13], etc.4 Z. Wan et al.
To mitigate the lack of global context understanding ability of CNNs, many
methodsapplyattentionmechanisminthefeaturefusionstage[29,50,77].With
the growing popularity of Transformers, more methods have begun to lever-
age them for extracting long-range dependencies from RGB and thermal im-
ages. CMX [30] utilizes SegFormer [59] for feature extraction and introduces a
rectification module along with a cross-attention module for the fusion of fea-
tures. Based on CMX [30], CMNeXt [73] proposes a self-query hub to select
informativefeaturesfromvarioussupplementarymodalities.Morerecently,Seg-
MiF [32] employs a cascading structure coupled with a hierarchical interactive
attentionmechanism,ensuringtheprecisemappingofcrucialinformationacross
two modalities.
In the domain of RGB-Depth semantic segmentation, methodologies that
have proven effective in RGB-Thermal segmentation have also demonstrated
impressive performance, such as CMX [30], CMNeXt [73]. Meanwhile, recent
developments in self-supervised pre-training have paved the way for its explo-
rationinRGB-Depthperception.Forinstance,MultiMAE[1]employsaMasked
Autoencoder [23] approach with pseudo labeling, ingesting tokens from various
modalities and reconstructing the masked tokens. DFormer [51] integrates both
RGB and Depth modalities within the pre-training architecture to learn trans-
ferable multi-modal representations.
AlthoughtheaforementionedTransformer-basedmethodshaveshownpromis-
ing results in RGB-X semantic segmentation due to their global context mod-
eling capabilities, the quadratic scaling nature of the self-attention mechanism
in Transformers limits the length of input sequences. Consequently, most ap-
proaches have to consolidate multi-modal tokens (F , F ∈ RN×D) into a
RGB T
single token (F ∈ RN×D) before fusion (Fig. 1a), inherently leads to the
fuse
loss of valuable information due to compressing the total sequence length. In
contrast, our proposed Sigma processes concatenated sequences, preserving all
valuable information while requiring significantly less computation.
2.2 State Space Models
State Space Models (SSM) [17,47], inspired by linear time-invariant (LTI) sys-
tems,areconsideredasefficientsequence-to-sequencemodels.Recently,theStruc-
turedState-SpaceSequencemodel(S4)[17]hasemergedasapioneeringworkin
deep state-space modeling, particularly for capturing long-range dependencies.
Furthermore,withtheselectivemechanismintroducedintoS4,Mamba[16]sur-
passes Transformers and other advanced architectures. Due to the remarkable
performance of SSM, researchers have extended it to the field of computer vi-
sion. Models such as ViS4mer [25], S4ND [39], TranS4mer [26] and the Selective
S4 model [53] demonstrate the effective modeling of image sequences with S4.
Recently, Vision Mamba [80] integrates SSM with bidirectional scanning, mak-
ingeachpatchrelatedtoanother.Meanwhile,VMamba[33]extendsscanningin
fourdirectionstofullycaptureinterrelationsamongimagepatches.Besides,state
space models have been extended to medical image segmentation [38,41,56,60],
image restoration [19] and point cloud analysis [28], all showing competitiveSigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation 5
results with lower complexity. However, recent works directly employ SSM as
a plug-and-play module, without the in-depth design tailored to specific tasks.
Additionally, there is a lack of exploration of SSM in multi-modal tasks. Thus,
we propose an attention-based Mamba fusion mechanism and a channel-aware
Mambadecoder,designedtoeffectivelyaugmentessentialinformationfromvar-
ious modalities and seamlessly integrate them. By leveraging the specialized
design of SSM for multi-modal tasks, our approach attains enhanced accuracy
while maintaining low complexity.
3 Sigma: Siamese Mamba Network
Inthissection,wegiveadetailedillustrationofourproposedSiamese Mamba
Network(Sigma)formulti-modalsemanticsegmentation.Tostart,weprovide
basic information on State Space Models. Subsequently, we present an overview
ofourSigmaarchitecture,followedbyin-depthdiscussionsoftheencoder,fusion
module, and decoder.
3.1 Preliminaries
State Space Models. State Space Models (SSM) [17,18,47] represent a class
of sequence-to-sequence modeling systems characterized by constant dynamics
overtime,apropertyalsoknownaslineartime-invariant(LTI).Withlinearcom-
plexity, SSM can effectively capture the inherent dynamics of systems through
an implicit mapping to latent states, which can be defined as:
y(t)=Ch(t)+Dx(t),h˙(t)=Ah(t)+Bx(t). (1)
Here,x(t)∈R,h(t)∈RN,andy(t)∈Rdenotestheinput,hiddenstate,andthe
output, respectively. N is the state size, and h˙(t) refers to the time derivative of
h(t).Additionally,A∈RN×N,B ∈RN×1,C ∈R1×N,andD ∈Rarethesystem
matrices. To process discrete sequences like image and text, SSMs adopt zero-
order hold (ZOH) discretization [17] to map the input sequence {x ,x ,...,x }
1 2 K
to the output sequence {y ,y ,...,y }. Specifically, suppose ∆∈RD is the pre-
1 2 K
defined timescale parameter to map continuous parameters A, B into a discrete
space, the discretization process can be formulated as:
A=exp(∆A), B =(∆A)−1(exp(A)−I)·∆B, C =C, (2)
y =Ch +Dx ,h =Ah +Bx . (3)
k k k k k−1 k
Here,allthematriceskeepthesamedimensionastheoperationiterates.Notably,
D, serving as a residual connection, is often discarded in the equation:
y =Ch . (4)
k k
Besides, following Mamba [16], the matrix B can be approximated by the first-
order Taylor series:
B =(exp(A)−I)A−1B ≈(∆A)(∆A)−1∆B =∆B (5)6 Z. Wan et al.
Selective Scan Mechanism. While SSM is effective for modeling discrete
sequences, they encounter limitations due to their LTI property, which results
in invariant parameters regardless of differences in the input. To address this
limitation,theSelectiveStateSpaceModel(S6,a.k.a Mamba)[16]isintroduced,
makingStateSpaceModelstobeinput-dependent.InMamba,thematricesB ∈
RL×N, C ∈ RL×N, and ∆ ∈ RL×D are derived from the input data x ∈ RL×D,
enabling the model to be contextually aware of the input. With this selection
mechanism, Mamba is capable of effectively modeling the complex interactions
present in long sequences.
Fig.2: Overall architecture of the proposed Sigma.
3.2 Overall Architecture
AsillustratedinFig.2,ourproposedmethodcomprisesaSiamesefeatureextrac-
tor (Sec. 3.3), a feature fusion module (Sec. 3.4), and an upsampling decoder
(Sec. 3.5), forming an architecture entirely composed of State Space Models.
Duringtheencodingphase,fourVisualStateSpace(VSS)Blockswithdownsam-
pling operations are sequentially cascaded to extract multi-level image features.
The two encoder branches share weights to reduce computational complexity.
Subsequently, features from each level, derived from two distinct branches, are
processed through a fusion module. In the decoding phase, the fused features at
each level are further enhanced by a Channel-Aware Visual State Space (CVSS)
Block with an upsampling operation. Ultimately, the final feature is forwarded
to a classifier to generate the outcome.
3.3 Siamese Mamba Encoder
Givenapairofmulti-modalinputs,theX-modalityinputispre-processedtothe
samedimensionastheRGBimage,whichcanbedenotedasI ,I ∈RH×W×3,
RGB X
where H and W represent the height and width of the input modalities. The
encoder starts with a stem module similar to ViT [10] which partitions the in-
putintopatches,generatingfeaturemapsF i1 ∈RH 4×W 4 ×C1,wherei∈{RGB,X}
refers to the RGB or X modality. Then we apply a Visual State Space (VSS)Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation 7
Visual State Space Block (VSS) Channel VSS Block
(CVSS)
VSS
LN
Conv
Selective Scan 2D Module (SS2D)
AvgPool MaxPool
Linear Linear
1 1 2 3 4 1 Sigmoid
2 1 3 2 4 2
3 4 4 4 3 2 2 3 1 1 (cid:3047) (cid:3047)(cid:2879)(cid:2869) (cid:3047) 3 4
(cid:3047) (cid:3047) (cid:3047)
Fig.3: The left part of the figure shows the Visual State Space (VSS) Block used in
the Sigma encoder and its component, Selective Scan 2D (SS2D) module. The right
part indicates the Channel-Aware VSS (CVSS) Block used in the Sigma decoder.
Block to process the features. This operation keeps the feature dimension un-
changed. The features are continually processed by three sets of downsampling
and VSS blocks, yielding multi-scale deep features {F i2 ∈ RH 8×W 8 ×C2,F i3 ∈
R 1H 6×W 16×C3,F i4 ∈ R 3H 2×W 32×C4}. The details of VSS Block are introduced as fol-
lows.
VSSBlock. FollowingVMamba[33]andMambaIR[19],weimplementtheVSS
BlockwithSelectiveScan2D(SS2D)modules.AsshownintheleftpartofFig.3,
theinputfeatureisprocessedbyasequenceoflinearprojection(Linear),Depth-
wise Convolution (DWConv) as the original Mamba [16], and an SS2D module is
used to model long-range spatial information from the feature followed by a
residual connection.
SS2D Module. WithintheSS2Dmodule,theinputfeatureofshapeRH×W×C
is first flattened to four R(H×W)×C sequences from four directions (top-left to
bottom-right,bottom-righttotop-left,top-righttobottom-left,andbottom-left
to top-right) as proposed in [33]. Then four distinctive Selective Scan Mod-
ules [16] are used to extract multi-direction information, where each of them
capturesthelong-rangedependenciesofthesequencewiththeoperationinEq.3.
Lastly, the four sequences are reversed to the same direction and summed.
3.4 Fusion Module
The detailed architecture of the feature fusion module is illustrated in Fig. 4,
where the multi-modal features from the Siamese backbone are enhanced by a
Cross Mamba Block (CroMB) followed by a Concat Mamba Block (ConMB).
NL
raeniL
raeniL
vnoCWD D2SS
NL
raeniL
egreM8 Z. Wan et al.
Cross Selective Scan Module (Cross SS)
Linear Linear Linear Linear
𝐵(cid:2928)(cid:2917)(cid:2912),𝐶(cid:2928)(cid:2917)(cid:2912) ∆(cid:2928)(cid:2917)(cid:2912)
DWConv DWConv 𝐴̅ (cid:2928)(cid:2917)(cid:2912),𝐵(cid:3364) (cid:2928)(cid:2917)(cid:2912)=exp∆(cid:2928)(cid:2917)(cid:2912)𝐴(cid:2928)(cid:2917)(cid:2912) ,∆(cid:2928)(cid:2917)(cid:2912)𝐵(cid:2928)(cid:2917)(cid:2912)
Cross SS 𝑥(cid:2928)(cid:2917)(cid:2912) ℎ 𝑦(cid:2928)(cid:3047) (cid:2928)(cid:2917)
(cid:3047)
(cid:2917)(cid:2912) (cid:2912)= =𝐴 𝐶̅ (cid:2928) (cid:2934)(cid:2917) ℎ(cid:2912) (cid:2928)(cid:3047)ℎ (cid:2917)(cid:2928)(cid:3047) (cid:2912)(cid:2917)(cid:2879) (cid:2912) +(cid:2869)+ 𝐷(cid:2928)𝐵(cid:3364) (cid:2917)(cid:2928) (cid:2912)(cid:2917) 𝑥(cid:2912) (cid:2928)(cid:3047)𝑥 (cid:2917)(cid:2928) (cid:2912)(cid:3047) (cid:2917)(cid:2912) 𝑦(cid:2928)(cid:2917)(cid:2912)
LN LN
𝐴̅ (cid:2934),𝐵(cid:3364) (cid:2934)=exp∆(cid:2934)𝐴(cid:2934) ,∆(cid:2934)𝐵(cid:2934)
Linear Linear
𝑥(cid:2934) ℎ(cid:2934)(cid:3047)=𝐴̅ (cid:2934)ℎ(cid:2934)(cid:3047)(cid:2879)(cid:2869)+𝐵(cid:3364) (cid:2934)𝑥(cid:2934)(cid:3047) 𝑦(cid:2934)
𝑦(cid:2934)(cid:3047)=𝐶(cid:2928)(cid:2917)(cid:2912)ℎ(cid:2934)(cid:3047)+𝐷(cid:2934)𝑥(cid:2934)(cid:3047)
𝐵(cid:2934),𝐶(cid:2934) ∆(cid:2934)
Linear Linear
Linear Linear
ConcatSelective Scan Module (ConcatSS)
DWConv DWConv
ConcatSS Linear Linear
LN LN 𝐵,𝐶 ∆
c 𝐴̅,𝐵(cid:3364)=exp∆𝐴 ,∆𝐵
𝑥
ℎ(cid:3047)=𝐴̅ℎ(cid:3047)(cid:2879)(cid:2869)+𝐵(cid:3364)𝑥(cid:3047)
𝑦
Linear 𝑦(cid:3047)=𝐶ℎ(cid:3047)+𝐷𝑥(cid:3047)
Fig.4: TheleftpartofthefigureshowstheCrossMambaBlock(CroMB)andConcat
MambaBlock(ConMB).TherightpartshowsthecomponentsofCroMBandConMB
respectively,namelyCrossSelectiveScan(CrossSS)ModuleandConcatSelectiveScan
(Concat SS) Module.
Specifically, CroMB employs a cross-multiplication mechanism to enhance the
featureswithoneanother,andConMBappliestheSelectiveScanmechanismto
concatenated features to obtain the fusion result. Suppose the features from the
kth Siamese encoder block are represented as Fk ∈ RHk×Wk×Ck, then the
RGB/X
entire fusion process can be represented as:
Fˆk , Fˆk =CroMB(Fk , Fk), (6)
RGB X RGB X
Fk =ConMB(Fˆk , Fˆk). (7)
Fuse RGB X
Here,Fˆk ,Fˆk,and Fk remainstheoriginaldimensionasRHk×Wk×Ck.Details
RGB X Fuse
of CroMB and ConMB are shown as follows.
CroMB. As demonstrated in the upper portion of Fig. 4, CroMB accepts two
featuresasinputandgeneratestwooutputs,preservingtheoriginalshapeofthe
features. The two input features are first processed by linear layers and depth-
wise convolutions respectively, then sent to the Cross Selective Scan (Cross SS)
Module.AccordingtotheselectionmechanismofMambamentionedinSec.3.1,
the system matrices B, C and ∆ are generated by the input to enable the
context-aware ability of the model. Here, linear projection layers are utilized to
generate thematrices. According toEq. 4, matrix C is usedto decodethe infor-
mation from the hidden state h to obtain the output y . Inspired by the cross-
k k
attention mechanism [3], which is extensively applied in multi-modal tasks, we
Scale Scale
Cross
Mamba
Block
(CroMB)
ConcatMamba
Block
(ConMB)
nacS
esrevnI
egreM
esrevnISigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation 9
aim to facilitate information exchange among multiple Selective Scan Modules.
Toachievethis,weproposeusingtheC matrixgeneratedbythecomplementary
modality in the Selective Scan operation, which enables the SSM to reconstruct
output from the hidden state guided by another modality. In particular, the
process can be represented as:
A =exp(∆ A ), A =exp(∆ A ), (8)
rgb rgb rgb x x x
B =∆ B , B =∆ B , (9)
rgb rgb rgb x x x
ht =A ht−1+B xt , ht =A ht−1+B xt, (10)
rgb rgb rgb rgb rgb x x x x x
yt =C ht +D xt , yt =C ht +D xt, (11)
rgb x rgb rgb rgb x rgb x x x
y =[y1 ,y2 ,...,yl ], y =[y1,y2,...,yl]. (12)
rgb rgb rgb rgb x x x x
Here, xt represents the input at time step t, and y denotes the selective
rgb/x rgb/x
scan output. C and C are the cross-modal matrices used for recovering the
x rgb
outputs at each time step from the hidden states.
ConMB. In CroMB, the features from two modalities interact with each other
throughtheCrossSelectiveScanoperationandobtainthecross-modal-enhanced
features.Tofurtherobtainafusedfeaturecontainingvitalinformationfromboth
modalities,weproposeConMBtointegratetheoutputsfromCroMB.Duetothe
quadratic scaling property, previous Transformer-based methods often partition
the input into small patches [10], which impedes the capture of information
withineachtoken.Incontrast,leveragingthelong-sequencemodelingcapability
of Mamba, our ConMB directly processes the concatenated features as input,
thereby preserving as much information from both modalities as possible.
Specifically, the outputs Fˆk ,Fˆk ∈RHk×Wk×Ck from CroMB are first pro-
RGB X
cess by linear and convolution layers, then sent to the Concat Selective Scan
(Concat SS) Module. Within the Concat SS Module, the two features are first
flattenedtoR(Hk×Wk)×Ck andthenconcatenatedonthesequencelengthdimen-
sion.ThisprovidesasequenceSk ofshapeR(2×Hk×Wk)×Ck.Besides,tocom-
Concat
prehensivelycaptureinformationfromtwomodalities,weinverselyscanthecon-
catenatedsequenceSk togetanadditionalsequenceSk ∈R(2×Hk×Wk)×Ck.
Concat Inverse
Afterwards,eachsequenceisprocessedbya1DSelectiveScanModuleproposed
in[16]tocapturelong-rangedependenciesfromtwomodalities,obtainingSˆk
Concat
and Sˆk . Then the inversed sequence output is flipped back and added with
Inverse
the processed concatenated sequence. The summed sequence is separated to re-
cover two outputs. This process can be represented as:
F(cid:101)k =DWConv(Linear(Fˆk )),F(cid:101)k =DWConv(Linear(Fˆk)), (13)
RGB RGB X X
Sk =Concat(F(cid:101)k ,F(cid:101)k,dim=0), (14)
Concat RGB X
Sk =Inverse(Sk ), (15)
Inverse Concat
Sˆk ,Sˆk =SSM(Sk ),SSM(Sk ), (16)
Concat Inverse Concat Inverse
Fk ,Fk =Seperate(Sˆk +Inverse(Sˆk )). (17)
RGB X Concat Inverse10 Z. Wan et al.
k k
After obtaining the scanned features F ,F , they are multiplied with two
RGB X
scaling parameters derived from Fˆk ,Fˆk and concatenated on the channel di-
RGB X
mension,formingafeatureofshapeRHk×Wk×(2×Ck).Finally,alinearprojection
layer is used to reduce the feature shape to RHk×Wk×Ck.
3.5 Channel-Aware Mamba Decoder
State Space Models are adept at extracting global spatial context, yet they fall
short in learning inter-channel information. To mitigate this issue, we propose a
Channel-Aware Mamba decoder. As shown in the right portion of Fig. 3, CVSS
BlockfirstincludesaVSSBlockusedintheencoder.Afterextractingthespatial
long-range dependencies, a Channel-Attention operation consisting of Average
Pooling and Max Pooling is introduced. In this manner, we form a spatial-
channel-aware scheme that has been proven effective in [57].
4 Experiments
4.1 Experimental Settings
Datasets. To verify the effectiveness of Sigma, we conduct extensive experi-
mentson twopublicly availableRGB-Thermal(RGB-T)semanticsegmentation
datasets,namelyMFNet[21]andPST900[45].Besides,tobetterunderstandthe
generalization ability of Sigma to other multi-modal scene understanding tasks,
we conduct experiments on two RGB-Depth (RGB-D) datasets, including NYU
DepthV2[46]andSUNRGB-D[48].Thedetailsofthesedatasetsareasfollows.
– MFNet dataset contains 820 daytime and 749 nighttime RGB-T images
witharesolutionof640×480.Thedatasetincludeseightcommonclassesof
objects in driving scenarios. We follow the training/testing split of [30].
– PST900 dataset provides 597 and 288 calibrated RGB-T images with a
resolutionof1280×720fortrainingandvalidation.Thisdatasetiscollected
from DARPA Subterranean Challenge and annotated with four classes.
– NYU Depth V2 dataset contains 1449 RGB Depth images annotated
with 40 semantic classes with the shape of 640×480. We divide them into
795/654 for training/testing following previous works [15,24].
– SUN RGB-D dataset incorporates 10335 RGB-D images with 37 classes.
We follow the common setting [24] to split 5285/5050 for training/testing,
and reshape the images to 640×480.
Evaluation. Following previous works [24,73], we report the Intersection over
Union (mIoU) averaged across the semantic classes for evaluation.Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation 11
Table 1: Quantitative comparisons for semantic segmentation of RGB-T images on
MFNet [21] and PST900 [45] datasets. The best and second best performance in each
block is highlighted in bold and underline, respectively.
Method Backbone Params(M)FLOPs(G)Unlabeled Car Person Bike Curve CarStopGuardrail Cone Bump mIoU
MFNet17[21] – – – 96.9 65.9 58.9 42.9 29.9 9.9 0.0 25.2 27.7 39.7
RTFNet19[49] ResNet-152 245.7 185.2 98.5 87.4 70.3 62.7 45.3 29.8 0.0 29.1 55.7 53.2
PSTNet20[45] ResNet-18 105.8 123.4 97.0 76.8 52.6 55.3 29.6 25.1 15.1 39.4 45.0 48.4
FuseSeg20[50] DenseNet-161 141.5 193.4 97.6 87.9 71.7 64.6 44.8 22.7 6.4 46.9 47.9 54.5
U2Fusion20[61] VGG-16 – – 97.7 82.8 64.8 61.0 32.3 20.9 – 45.2 50.2 50.8
AFNet21[62] ResNet-50 – – 98.0 86.0 67.4 62.0 43.0 28.9 4.6 44.9 56.6 54.6
ABMDRNet21[74] ResNet-50 64.6 194.3 98.6 84.8 69.6 60.3 45.1 33.1 5.1 47.4 50.0 54.8
FEANet21[7] ResNet-152 337.1 255.2 98.3 87.8 71.1 61.1 46.5 22.1 6.6 55.3 48.9 55.3
GMNet21[77] ResNet-50 149.8 153.0 97.5 86.5 73.1 61.7 44.0 42.3 14.5 48.7 47.4 57.3
TarDAL22[31] – 297 – 97.6 80.7 67.1 60.1 34.9 10.5 – 38.7 45.5 48.6
CMX22[30] MiT-B4 139.9 134.3 98.3 90.1 75.2 64.5 50.2 35.3 8.5 54.2 60.6 59.7
EAEFNet23[29] ResNet-152 200.4 147.3 – 87.6 72.6 63.8 48.6 35.0 14.2 52.4 58.3 58.9
CACFNet23[76] ConvNeXt-B 198.6 101.4 – 89.2 69.5 63.3 46.6 32.4 7.9 54.9 58.3 57.8
PAIF23[35] – 260 – – 88.1 72.4 60.8 – – – 56.0 57.2 56.5
CENet23[13] ResNet-50 – – – 85.8 70.0 61.4 46.8 29.3 8.7 47.8 56.9 56.1
SegMiF23[32] MiT-B3 – – 98.1 87.8 71.4 63.2 47.5 31.1 – 48.9 50.3 56.1
CMNeXt23[73] MiT-B4 119.6 131.9 98.4 91.5 75.3 67.6 50.5 40.1 9.3 53.4 52.8 59.9
CAINet24[37] MobileNet-V2 12.16 123.62 – 88.5 66.3 68.7 55.4 31.5 9.0 48.9 60.7 58.6
Sigma(Ours) VMamba-T 48.3 89.5 98.4 90.8 75.2 66.6 48.2 38.0 8.7 55.9 60.4 60.2
Sigma(Ours) VMamba-S 69.8 138.9 98.5 91.5 75.8 67.8 49.6 41.8 9.6 54.8 60.4 61.1
Sigma(Ours) VMamba-B 121.4 240.7 98.5 91.1 75.2 68.0 50.8 43.0 9.7 57.6 57.9 61.3
(a) QuantitativecomparisononMFNetday-nightevaluationset[21](9classes)
Method Backbone BackgroundExtinguisher Backpack Hand-Drill Survivor mIoU
MFNet [21] – 98.6 60.4 64.3 41.1 20.7 57.0
17
RTFNet [49] ResNet-152 98.9 52.0 75.3 25.4 36.4 57.6
19
PSTNet [45] ResNet-18 98.9 70.1 69.2 53.6 50.0 68.4
20
ABMDRNet [74] ResNet-50 99.0 66.2 67.9 61.5 62.0 71.3
21
GMNet [77] ResNet-50 99.4 73.8 83.8 85.2 78.4 84.1
21
CCFFNet 22[58] ResNet-101 99.4 82.8 75.8 79.9 72.7 82.1
EGFNet 23[8] ResNet-101 99.6 80.0 90.6 76.1 80.9 85.4
CACFNet 22[76] ConvNeXt-B 99.6 82.1 89.5 80.9 80.8 86.6
CAINet [37] MobileNet-V2 99.5 80.3 88.0 77.2 78.7 84.7
24
Sigma(Ours) VMamba-T 99.6 81.9 89.8 88.7 82.7 88.6
Sigma(Ours) VMamba-S 99.6 79.4 88.7 90.2 81.2 87.8
(b) QuantitativecomparisononPST900evaluationset[45](5classes)
Training Settings. We follow [30] to use the AdamW optimizer [36] with an
initial learning rate 6e−5 and weight decay 0.01. The model is trained with
a batch size of 8 for 500 epochs. We utilize the ImageNet-1K [42] pre-trained
model provided by VMamba [33] for the Siamese image encoder, leading to
three different sizes of models (Sigma-Tiny, Sigma-Small, and Sigma-Base). For
the tiny and small model, we use four NVIDIA RTX 3090 GPUs for training.
For the base model, we use four NVIDIA RTX A6000 GPUs for training. All
the models are evaluated with a single NVIDIA RTX 3090 GPU. More details
about the experimental settings are described in the appendix.
4.2 Quantitative and Qualitative Results
RGB-T Semantic Segmentation. Tab. 1(a) shows the per-class semantic
segmentation results, alongside comparisons of model size and computational
complexity, on the MFNet dataset. It is observed that our tiny model surpasses
othercomparedmethodswithfewermodelparametersandFLOPs,andourbase
model achieves a 1.1% performance improvement compared to the tiny variant.
Besides,asshowninTab.1(b),ourmethodoutperformsothermethodsbymore12 Z. Wan et al.
Table 2: Comparison of RGB-D Semantic Segmentation on NYU Depth V2 [46] and
SUNRGB-D[48].†indicatestheparametersformulti-tasklearningreportedfrom[68].
NYUDepthV2 SUNRGB-D
Method Backbone Params(M)
InputSize mIoU InputSize mIoU
ACNet 19[24] ResNet-50 116.6 480×640 48.3 530×730 48.1
SA-Gate 20[6] ResNet-101 110.9 480×640 52.4 530×730 49.4
CEN 20[55] ResNet-101 118.2 480×640 51.7 530×730 50.2
CEN 20[55] ResNet-152 133.9 480×640 52.5 530×730 51.1
SGNet 21[5] ResNet-101 64.7 480×640 51.1 530×730 48.6
ShapeConv 21[2] ResNext-101 86.8 480×640 51.3 530×730 48.6
ESANet 21[44] ResNet-34 31.2 480×640 50.3 480×640 48.2
FRNet 22[79] ResNet-34 85.5 480×640 53.6 530×730 51.8
PGDENet 22[78] ResNet-34 100.7 480×640 53.7 530×730 51.0
EMSANet 22[43] ResNet-34 46.9 480×640 51.0 530×730 48.4
TokenFusion 22[54] MiT-B2 26.0 480×640 53.3 530×730 50.3
TokenFusion 22[54] MiT-B3 45.9 480×640 54.2 530×730 51.0
MultiMAE 22[1] ViT-B 95.2 640×640 56.0 640×640 51.1
Omnivore 22[15] Swin-B 95.7 480×640 54.0 – –
PDCNet 23[64] ResNet-101 – 480×480 53.5 480×480 49.6
InvPT 23[67] ViT-L 423† 480×640 53.6 – –
TaskPrompter 23[68]TaskPrompter-L 401† 480×640 55.3 – –
SMMCL 24[9] SegNeXt-B – 480×640 55.8 – –
CMNeXt 23[73] MiT-B4 119.6 480×640 56.9 530×730 51.9
CAINet 24[37] MobileNet-V2 12.2 480×640 52.6 – –
Sigma(Ours) VMamba-T 48.3 480×640 53.9 480×640 50.0
Sigma(Ours) VMamba-S 69.8 480×640 57.0 480×640 52.4
than2%onPST900[45]dataset,demonstratingthesuperiorityofourproposed
method.
The qualitative analysis illustrated in Fig. 5 reveals that our Sigma model
outperformsbaselinemodelsbygeneratingmoreprecisesegmentationsandaccu-
rateclassifications,notablyinidentifyingintricatefeaturesliketactilepavingand
bollards. The enhanced outcomes stem from Sigma’s ability to extract valuable
informationfrombothRGBandthermalmodalities.Specifically,RGBenhances
color distinction, whereas thermal imaging excels in texture differentiation. In-
tegrating these modalities results in improved segmentation accuracy.
RGB-DSemanticSegmentation. InTable2,wecompareSigmaagainstvari-
ousRGB-DmethodstovalidateSigma’sgeneralizationcapabilityacrossdifferent
multi-modal segmentation tasks. Remarkably, our Sigma-S model surpasses the
performance of CMNeXt [73] while employing only 69.8M parameters, which is
49.8MlessthanCMNeXt.Thisdemonstratesthesuperiorbalanceourproposed
method achieves between accuracy and efficiency.
Fig. 6 demonstrates Sigma’s capability to generate more coherent segmenta-
tion by utilizing depth information effectively. For instance, in the case of the
roundchairadjacenttothesofa,shadowscausebaselinemodelstofragmentthe
chairintomultiplesegments.Sigmasuccessfullyrecognizesitasasingularentity,
highlighting its superior proficiency in leveraging depth data for segmentation.
4.3 Ablation Studies
As detailed in Table 3, we carried out ablation studies with Sigma-Tiny on
the MFNet [21] dataset by omitting the components we introduced. ComparedSigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation 13
RGB Thermal RTFNet[49] FEANet[7] EAEFNet[29] CMX[30] Sigma(Ours) GroundTruth
Fig.5: Qualitative comparison on MFNet [21] dataset.
RGB HHA SA-Gate[6] MultiMAE[1] CMX[30] CMNeXt[73] Sigma(Ours) GroundTruth
Fig.6: Qualitative comparison on NYU Depth V2 [46] dataset. We use HHA images
for better visualization of depth modality.
to the complete Sigma model, eliminating the Cross Mamba Block (CroMB)
and Concat Mamba Block (ConMB) individually results in decreases of 0.6%
and 0.8% in performance, respectively. The removal of both blocks leads to a
performancedeclineof2.1%,highlightingtheeffectivenessofourproposedfusion
module. Additionally, we evaluate our proposed decoder against a simple Multi-
Layer Perceptron (MLP) decoder and a Swin-Transform decoder, discovering
that our decoder surpasses them by 1.1% and 0.9%, respectively. In our final
analysis, to assess the efficacy of Mamba compared to other Transformer-based
architectures, such as the Swin Transformer, we substitute our encoder with a
pre-trained Swin Transformer [34] Tiny backbone and apply Swin Transformer
blocks in the decoder. This outcome suggests that our design with SSM can be
more effective than a simple integration of Transformers.14 Z. Wan et al.
Table 3: AblationstudiesontheMFNet[21]dataset.WereportthemIoUmetricand
the relative decrease (∇) in blue. The Encoder column applies either VMamba Tiny
or Swin-Transformer Tiny for feature extraction, and the decoder column indicates
usingtheChannel-AwareMambaDecoder,MLPdecoder,orSwinTransformerdecoder.
When both CroMB and ConMB show marks, we use a simple feature summation
(cid:37)
operation to obtain the fused feature.
# Encoder CroMB ConMB Decoder mIoU(∇)
1 VMamba-T (cid:33) (cid:33) CMD 60.5(0.0)
2 VMamba-T (cid:37) (cid:33) CMD 59.9(-0.6)
3 VMamba-T (cid:33) (cid:37) CMD 59.7(-0.8)
4 VMamba-T (cid:37) (cid:37) CMD 58.4(-2.1)
5 VMamba-T (cid:33) (cid:33) MLP 59.4(-1.1)
6 VMamba-T (cid:37) (cid:37) MLP 57.5(-3.0)
7 VMamba-T (cid:33) (cid:33) Swin 59.6(-0.9)
8 Swin-T (cid:37) (cid:37) Swin 56.3(-4.2)
5 Conclusion
In this work, we propose Sigma, a novel Siamese Mamba Network for multi-
modal semantic segmentation, which explores the application of State Space
Models in multi-modal scene understanding for the first time. A Siamese back-
bone consisting of a 2D selective scan mechanism is first applied to extract ro-
bust global long-range dependencies with linear complexity. Then we introduce
a fusion module incorporating a cross-selective scan and a concat-selective scan
operation. Finally, we design a channel-aware Mamba decoder to extract essen-
tial information from the fused features for predictions. Extensive experiments
onRGB-ThermalandRGB-Depthsemanticsegmentationbenchmarksshowthe
superiority of Sigma in both accuracy and efficiency.
Limitations and Future Works. While Sigma has achieved outstanding re-
sults in various RGB-X semantic segmentation tasks, two limitations remain. 1)
Underutilization of Mamba for Longer Sequences: Mamba’scapabilitytohandle
extremelylongsequencesisasignificantadvantage,particularlybeneficialinfu-
siontasksinvolvingmorethantwomodalities.However,ourcurrentexploration
primarilyfocusesontheapplicationofMambafortwomodalities,potentiallynot
fullyleveragingitscapacityformodelinglongersequences.Futureworkwillaim
toinvestigateMamba’sperformanceondatasetswithagreatervarietyofmodal-
ities, such as the DELIVER benchmark [73]. This exploration is pivotal for ad-
vancingresearchonenablingautonomousagentstonavigateenvironmentsusing
multiple sensors, including RGB, depth, thermal, and LiDAR. 2) Memory Con-
sumptionintheMambaEncoder: TheMambaencoderscansimagefeaturesfrom
fourdirections,allowingeachpixeltoassimilateinformationfromitssurrounding
pixels.Thisapproach,however,quadruplesmemoryusage,posingachallengefor
deployment on lightweight edge devices. Future endeavors will seek to incorpo-
rate positional information through alternative methods, such as positional en-
coders,andemploya1DSSMtodiminishcomputationalandmemorydemands.Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation 15
Acknowledgement
ThisworkhasbeenfundedinpartbytheArmyResearchLaboratory(ARL)un-
der grant W911NF-23-2-0007 and W911NF-19-2-0146, and the Air Force Office
ofScientificResearch(AFOSR)undergrantsFA9550-18-1-0097andFA9550-18-
1-0251.
References
1. Bachmann,R.,Mizrahi,D.,Atanov,A.,Zamir,A.:Multimae:Multi-modalmulti-
taskmaskedautoencoders.In:EuropeanConferenceonComputerVision.pp.348–
367. Springer (2022) 2, 4, 12, 13, 20
2. Cao,J.,Leng,H.,Lischinski,D.,Cohen-Or,D.,Tu,C.,Li,Y.:Shapeconv:Shape-
aware convolutional layer for indoor rgb-d semantic segmentation. In: Proceed-
ingsoftheIEEE/CVFinternationalconferenceoncomputervision.pp.7088–7097
(2021) 12
3. Chen, C.F.R., Fan, Q., Panda, R.: Crossvit: Cross-attention multi-scale vision
transformer for image classification. In: Proceedings of the IEEE/CVF interna-
tional conference on computer vision. pp. 357–366 (2021) 8
4. Chen, L., Boardley, B., Hu, P., Wang, Y., Pu, Y., Jin, X., Yao, Y., Gong, R., Li,
B.,Huang,G.,etal.:2023low-powercomputervisionchallenge(lpcvc)summary.
arXiv preprint arXiv:2403.07153 (2024) 1
5. Chen,L.Z.,Lin,Z.,Wang,Z.,Yang,Y.L.,Cheng,M.M.:Spatialinformationguided
convolutionforreal-timergbdsemanticsegmentation.IEEETransactionsonImage
Processing 30, 2313–2324 (2021) 12
6. Chen, X., Lin, K.Y., Wang, J., Wu, W., Qian, C., Li, H., Zeng, G.: Bi-directional
cross-modalityfeaturepropagationwithseparation-and-aggregationgateforrgb-d
semantic segmentation. In: European Conference on Computer Vision. pp. 561–
577. Springer (2020) 12, 13, 20
7. Deng, F., Feng, H., Liang, M., Wang, H., Yang, Y., Gao, Y., Chen, J., Hu, J.,
Guo, X., Lam, T.L.: Feanet: Feature-enhanced attention network for rgb-thermal
real-timesemanticsegmentation.In:2021IEEE/RSJInternationalConferenceon
Intelligent Robots and Systems (IROS). pp. 4467–4473. IEEE (2021) 11, 13
8. Dong,S.,Zhou,W.,Xu,C.,Yan,W.:Egfnet:Edge-awareguidancefusionnetwork
forrgb–thermalurbansceneparsing.IEEETransactionsonIntelligentTransporta-
tion Systems (2023) 11
9. Dong, X., Yokoya, N.: Understanding dark scenes by contrasting multi-modal ob-
servations. In: Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision. pp. 840–850 (2024) 12
10. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is
worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 (2020) 6, 9
11. Fayyad,J.,Jaradat,M.A.,Gruyer,D.,Najjaran,H.:Deeplearningsensorfusionfor
autonomous vehicle perception and localization: A review. Sensors 20(15), 4220
(2020) 3
12. Feng, D., Haase-Schütz, C., Rosenbaum, L., Hertlein, H., Glaeser, C., Timm, F.,
Wiesbeck, W., Dietmayer, K.: Deep multi-modal object detection and semantic
segmentation for autonomous driving: Datasets, methods, and challenges. IEEE
Transactions on Intelligent Transportation Systems 22(3), 1341–1360 (2020) 2, 316 Z. Wan et al.
13. Feng,Z., Guo, Y.,Sun, Y.: Cekd:Cross-modaledge-privileged knowledge distilla-
tion for semantic scene understanding using only thermal images. IEEE Robotics
and Automation Letters 8(4), 2205–2212 (2023) 2, 3, 11
14. Gade, R., Moeslund, T.B.: Thermal cameras and applications: a survey. Machine
vision and applications 25, 245–262 (2014) 3
15. Girdhar,R.,Singh,M.,Ravi,N.,vanderMaaten,L.,Joulin,A.,Misra,I.:Omni-
vore:Asinglemodelformanyvisualmodalities.In:ProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.16102–16112(2022)
10, 12
16. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state
spaces. arXiv preprint arXiv:2312.00752 (2023) 2, 4, 5, 6, 7, 9
17. Gu,A.,Goel,K.,Ré,C.:Efficientlymodelinglongsequenceswithstructuredstate
spaces. arXiv preprint arXiv:2111.00396 (2021) 4, 5
18. Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., Ré, C.: Combining
recurrent,convolutional,andcontinuous-timemodelswithlinearstatespacelayers.
Advances in neural information processing systems 34, 572–585 (2021) 5
19. Guo,H.,Li,J.,Dai,T.,Ouyang,Z.,Ren,X.,Xia,S.T.:Mambair:Asimplebaseline
for image restoration with state-space model (2024) 4, 7
20. Guo,Y.,Liu,Y.,Georgiou,T.,Lew,M.S.:Areviewofsemanticsegmentationusing
deepneuralnetworks.Internationaljournalofmultimediainformationretrieval7,
87–93 (2018) 1
21. Ha,Q.,Watanabe,K.,Karasawa,T.,Ushiku,Y.,Harada,T.:Mfnet:Towardsreal-
timesemanticsegmentationforautonomousvehicleswithmulti-spectralscenes.pp.
5108–5115 (2017) 3, 10, 11, 12, 13, 14, 21, 22
22. Hazirbas,C.,Ma,L.,Domokos,C.,Cremers,D.:Fusenet:Incorporatingdepthinto
semantic segmentation via fusion-based cnn architecture. In: Asian conference on
computer vision. pp. 213–228. Springer (2016) 21
23. He,K.,Chen,X.,Xie,S.,Li,Y.,Dollár,P.,Girshick,R.:Maskedautoencodersare
scalablevisionlearners.In:ProceedingsoftheIEEE/CVFconferenceoncomputer
vision and pattern recognition. pp. 16000–16009 (2022) 4
24. Hu, X., Yang, K., Fei, L., Wang, K.: Acnet: Attention based network to exploit
complementary features for rgbd semantic segmentation. In: 2019 IEEE Interna-
tional Conference on Image Processing (ICIP). pp. 1440–1444. IEEE (2019) 10,
12
25. Islam, M.M., Bertasius, G.: Long movie clip classification with state-space video
models. In: ECCV. pp. 87–104. Springer (2022) 4
26. Islam,M.M.,Hasan,M.,Athrey,K.S.,Braskich,T.,Bertasius,G.:Efficientmovie
scenedetectionusingstate-spacetransformers.In:CVPR.pp.18749–18758(2023)
4
27. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR
(2015) 20
28. Liang, D., Zhou, X., Wang, X., Zhu, X., Xu, W., Zou, Z., Ye, X., Bai, X.: Point-
mamba: A simple state space model for point cloud analysis (2024) 2, 4
29. Liang, M., Hu, J., Bao, C., Feng, H., Deng, F., Lam, T.L.: Explicit attention-
enhancedfusionforrgb-thermalperceptiontasks.IEEERoboticsandAutomation
Letters (2023) 2, 4, 11, 13
30. Liu,H.,Zhang,J.,Yang,K.,Hu,X.,Stiefelhagen,R.:Cmx:Cross-modalfusionfor
rgb-x semantic segmentation with transformers. arXiv preprint arXiv:2203.04838
(2022) 2, 4, 10, 11, 13, 20, 21Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation 17
31. Liu, J., Fan, X., Huang, Z., Wu, G., Liu, R., Zhong, W., Luo, Z.: Target-aware
dual adversarial learning and a multi-scenario multi-modality benchmark to fuse
infrared and visible for object detection. In: Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition. pp. 5802–5811 (2022) 11
32. Liu, J., Liu, Z., Wu, G., Ma, L., Liu, R., Zhong, W., Luo, Z., Fan, X.: Multi-
interactive feature learning and a full-time multi-modality benchmark for image
fusion and segmentation. In: Proceedings of the IEEE/CVF international confer-
ence on computer vision. pp. 8115–8124 (2023) 4, 11
33. Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba:
Visual state space model. arXiv preprint arXiv:2401.10166 (2024) 2, 4, 7, 11, 20
34. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin
transformer:Hierarchicalvisiontransformerusingshiftedwindows.In:CVPR.pp.
10012–10022 (2021) 13
35. Liu,Z.,Liu,J.,Zhang,B.,Ma,L.,Fan,X.,Liu,R.:Paif:Perception-awareinfrared-
visible image fusion for attack-tolerant semantic segmentation. In: Proceedings of
the 31st ACM International Conference on Multimedia. pp. 3706–3714 (2023) 11
36. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017) 11
37. Lv, Y., Liu, Z., Li, G.: Context-aware interaction network for rgb-t semantic seg-
mentation. IEEE Transactions on Multimedia (2024) 2, 11, 12
38. Ma,J.,Li,F.,Wang,B.:U-mamba:Enhancinglong-rangedependencyforbiomed-
ical image segmentation. arXiv preprint arXiv:2401.04722 (2024) 2, 4
39. Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., Ré, C.:
S4nd: Modeling images and videos as multidimensional signals with state spaces.
Advances in neural information processing systems 35, 2846–2861 (2022) 4
40. Pohlen,T.,Hermans,A.,Mathias,M.,Leibe,B.:Full-resolutionresidualnetworks
for semantic segmentation in street scenes. In: CVPR (2017) 21
41. Ruan,J.,Xiang,S.:Vm-unet:Visionmambaunetformedicalimagesegmentation.
arXiv preprint arXiv:2402.02491 (2024) 2, 4
42. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-
nitionchallenge.Internationaljournalofcomputervision115,211–252(2015) 11,
20
43. Seichter, D., Fischedick, S.B., Köhler, M., Groß, H.M.: Efficient multi-task rgb-d
sceneanalysisforindoorenvironments.In:2022InternationalJointConferenceon
Neural Networks (IJCNN). pp. 1–10. IEEE (2022) 12
44. Seichter, D., Köhler, M., Lewandowski, B., Wengefeld, T., Gross, H.M.: Efficient
rgb-dsemanticsegmentationforindoorsceneanalysis.In:2021IEEEinternational
conferenceonroboticsandautomation(ICRA).pp.13525–13531.IEEE(2021) 12
45. Shivakumar, S.S., Rodrigues, N., Zhou, A., Miller, I.D., Kumar, V., Taylor, C.J.:
Pst900:Rgb-thermalcalibration,datasetandsegmentationnetwork.In:2020IEEE
internationalconferenceonroboticsandautomation(ICRA).pp.9441–9447.IEEE
(2020) 3, 10, 11, 12
46. Silberman,N.,Hoiem,D.,Kohli,P.,Fergus,R.:Indoorsegmentationandsupport
inference from rgbd images. In: Computer Vision–ECCV 2012: 12th European
ConferenceonComputerVision,Florence,Italy,October7-13,2012,Proceedings,
Part V 12. pp. 746–760. Springer (2012) 3, 10, 12, 13, 20
47. Smith, J.T., Warrington, A., Linderman, S.W.: Simplified state space layers for
sequence modeling. arXiv preprint arXiv:2208.04933 (2022) 4, 518 Z. Wan et al.
48. Song,S.,Lichtenberg,S.P.,Xiao,J.:Sunrgb-d:Argb-dsceneunderstandingbench-
marksuite.In:ProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition. pp. 567–576 (2015) 3, 10, 12, 20
49. Sun, Y., Zuo, W., Liu, M.: Rtfnet: Rgb-thermal fusion network for semantic seg-
mentation of urban scenes 4(3), 2576–2583 (2019) 3, 11, 13, 21
50. Sun, Y., Zuo, W., Yun, P., Wang, H., Liu, M.: Fuseseg: Semantic segmentation of
urban scenes based on rgb and thermal data fusion. IEEE Trans. on Automation
Science and Engineering (TASE) (2020) 3, 4, 11, 21
51. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the incep-
tion architecture for computer vision. In: CVPR. pp. 2818–2826 (2016) 4
52. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Ł., Polosukhin, I.: Attention is all you need. NeurIPS 30 (2017) 2
53. Wang, J., Zhu, W., Wang, P., Yu, X., Liu, L., Omar, M., Hamid, R.: Selective
structured state-spaces for long-form video understanding. In: CVPR. pp. 6387–
6397 (2023) 4
54. Wang, Y., Chen, X., Cao, L., Huang, W., Sun, F., Wang, Y.: Multimodal token
fusion for vision transformers. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 12186–12195 (2022) 12, 20
55. Wang, Y., Huang, W., Sun, F., Xu, T., Rong, Y., Huang, J.: Deep multimodal
fusion by channel exchanging. Advances in neural information processing systems
33, 4835–4845 (2020) 12
56. Wang, Z., Ma, C.: Weak-mamba-unet: Visual mamba makes cnn and vit work
better for scribble-based medical image segmentation (2024) 2, 4
57. Woo, S., Park, J., Lee, J.Y., Kweon, I.S.: Cbam: Convolutional block attention
module. In: Proceedings of the European conference on computer vision (ECCV).
pp. 3–19 (2018) 10
58. Wu,W.,Chu,T.,Liu,Q.:Complementarity-awarecross-modalfeaturefusionnet-
work for rgb-t semantic segmentation. Pattern Recognition 131, 108881 (2022)
11
59. Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: Segformer:
Simpleandefficientdesignforsemanticsegmentationwithtransformers.Advances
in Neural Information Processing Systems 34, 12077–12090 (2021) 4, 21
60. Xing,Z.,Ye,T.,Yang,Y.,Liu,G.,Zhu,L.:Segmamba:Long-rangesequentialmod-
elingmambafor3dmedicalimagesegmentation.arXivpreprintarXiv:2401.13560
(2024) 2, 4
61. Xu,H.,Ma,J.,Jiang,J.,Guo,X.,Ling,H.:U2fusion:Aunifiedunsupervisedimage
fusion network. IEEE Transactions on Pattern Analysis and Machine Intelligence
44(1), 502–518 (2020) 3, 11
62. Xu, J., Lu, K., Wang, H.: Attention fusion network for multi-spectral semantic
segmentation. Pattern Recognition Letters 146, 179–184 (2021) 3, 11
63. Yan, T., Wan, Z., Zhang, P., Cheng, G., Lu, H.: Transy-net: Learning fully trans-
formernetworksforchangedetectionofremotesensingimages.IEEETransactions
on Geoscience and Remote Sensing (2023) 2
64. Yang, J., Bai, L., Sun, Y., Tian, C., Mao, M., Wang, G.: Pixel difference convo-
lutional network for rgb-d semantic segmentation. IEEE Transactions on Circuits
and Systems for Video Technology pp. 1–1 (2023). https://doi.org/10.1109/
TCSVT.2023.3296162 12
65. Yang,Y.,Xing,Z.,Zhu,L.:Vivim:avideovisionmambaformedicalvideoobject
segmentation (2024) 2Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation 19
66. Yang,Y.Q.,Guo,Y.X.,Xiong,J.Y.,Liu,Y.,Pan,H.,Wang,P.S.,Tong,X.,Guo,
B.:Swin3d:Apretrainedtransformerbackbonefor3dindoorsceneunderstanding
(2023) 2
67. Ye, H., Xu, D.: Inverted pyramid multi-task transformer for dense scene under-
standing. In: ECCV (2022) 12
68. Ye, H., Xu, D.: Taskprompter: Spatial-channel multi-task prompting for dense
scene understanding. In: ICLR (2023) 12
69. Yu,C.,Wang,J.,Peng,C.,Gao,C.,Yu,G.,Sang,N.:Bisenet:Bilateralsegmenta-
tionnetworkforreal-timesemanticsegmentation.In:ProceedingsoftheEuropean
conference on computer vision (ECCV). pp. 325–341 (2018) 21
70. Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N.: Learning a discriminative
feature network for semantic segmentation. In: CVPR (2018) 21
71. Zhang, C., Stepputtis, S., Campbell, J., Sycara, K., Xie, Y.: Robust hierarchi-
cal scene graph generation. In: NeurIPS 2023 Workshop: New Frontiers in Graph
Learning (2023) 1
72. Zhang,C.,Stepputtis,S.,Campbell,J.,Sycara,K.,Xie,Y.:Hiker-sgg:Hierarchical
knowledgeenhancedrobustscenegraphgeneration.In:IEEE/CVFConferenceon
Computer Vision and Pattern Recognition (2024) 1
73. Zhang,J.,Liu,R.,Shi,H.,Yang,K.,Reiß,S.,Peng,K.,Fu,H.,Wang,K.,Stiefelha-
gen,R.:Deliveringarbitrary-modalsemanticsegmentation.In:Proceedingsofthe
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1136–
1147 (2023) 2, 4, 10, 11, 12, 13, 14, 20
74. Zhang,Q.,Zhao,S.,Luo,Y.,Zhang,D.,Huang,N.,Han,J.:Abmdrnet:Adaptive-
weighted bi-directional modality difference reduction network for rgb-t semantic
segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 2633–2642 (2021) 11
75. Zhang, Y., Sidibé, D., Morel, O., Mériaudeau, F.: Deep multimodal fusion for
semanticimagesegmentation:Asurvey.ImageandVisionComputing105,104042
(2021) 3
76. Zhou, W., Dong, S., Fang, M., Yu, L.: Cacfnet: Cross-modal attention cascaded
fusion network for rgb-t urban scene parsing. IEEE Transactions on Intelligent
Vehicles (2023) 3, 11
77. Zhou,W.,Liu,J.,Lei,J.,Yu,L.,Hwang,J.N.:Gmnet:graded-featuremultilabel-
learningnetworkforrgb-thermalurbanscenesemanticsegmentation.IEEETrans-
actions on Image Processing 30, 7790–7802 (2021) 3, 4, 11, 21
78. Zhou, W., Yang, E., Lei, J., Wan, J., Yu, L.: Pgdenet: Progressive guided fusion
anddepthenhancementnetworkforrgb-dindoorsceneparsing.IEEETransactions
on Multimedia (2022) 12
79. Zhou,W.,Yang,E.,Lei,J.,Yu,L.:Frnet:Featurereconstructionnetworkforrgb-d
indoorsceneparsing.IEEEJournalofSelectedTopicsinSignalProcessing 16(4),
677–687 (2022) 12
80. Zhu,L.,Liao,B.,Zhang,Q.,Wang,X.,Liu,W.,Wang,X.:Visionmamba:Efficient
visualrepresentationlearningwithbidirectionalstatespacemodel.arXivpreprint
arXiv:2401.09417 (2024) 420 Z. Wan et al.
Sigma: Siamese Mamba Network for Multi-Modal
Semantic Segmentation
Supplementary Material
A Experimental Details
During training, we perform data augmentation, including random flipping and
scalingwithrandomscales[0.5,1.75],toalldatasets.WeadoptVMamba[33]pre-
trainedonImageNet[42]asthebackbone,whichincludesthreeversions,namely
VMamba-Tiny, VMamba-Small, and VMamba-Base. The detailed settings of
the three models are listed in Tab. A1. We select AdamW optimizer [27] with
weight decay 0.01. The original learning rate is set to 6e−5 and we employ
a poly learning rate schedule with 10 warm-up epochs. We use cross-entropy
as the loss function. When reporting testing results on NYU Depth V2 [46]
and SUN RGB-D [48] datasets, we use multiple scales {0.75,1,1.25} according
to most previous RGB-Depth semantic segmentation methods [30,73]. We use
mean Intersection over Union (mIoU) averaged across semantic classes as the
evaluation metric to measure the segmentation performance. For each of the
datasets, more implementation details are described as follows.
MFNet dataset. The tiny and small backbones are trained on four 3090Ti
GPUsandthebasebackboneistrainedonfourA6000GPUs.Weusetheoriginal
image size of 640×480 for training and inference. The batch size is set to 8 for
training. A single 3090Ti GPU is used for inferencing all the models.
PST900 dataset. The tiny and small backbones are trained on two A6000
GPUs. We use the original image size of 1280×720 for training and inference.
Thebatchsizeissetto4fortraining.AsingleA6000GPUisusedforinferencing
all the models.
NYU Depth V2 dataset. Unlike other methods [6,30] to use HHA format of
depth images for training, we directly use raw depth images and we found no
apparent performance difference between the formats. We take the whole image
with the size 640×480 for training and inference. 4 3090Ti GPUs are used to
train the tiny and small backbones with batch size 8, and 4 A6000 GPUs are
used to train the base model.
SUN-RGBD dataset. Unlike previous methods which use larger resolution
input (730 × 530 [54,73] or 640 × 640 [1]), we adopt the input resolution of
640×480 and keep the same training settings as NYU Depth V2 dataset. We
also use raw depth images instead of HHA format for training.Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation 21
Table A1: Details about three versions of backbone.
VSSBlockNumber
Backbone EmbeddedDimension
Stage1 Stage2 Stage3 Stage4
VMamba-Tiny 2 2 9 2 96
VMamba-Small 2 2 27 2 96
VMamba-Base 2 2 27 2 128
B Daytime and Nighttime Performance
To explore the effectiveness of our method on daytime and nighttime RGB-T
images, we use the MFNet [21] dataset and follow CMX [30] to use 205 daytime
images and 188 nighttime images in the test set for evaluation. As shown in
Tab. B2, our method delivers better results on both daytime and nighttime
results, demonstrating the effectiveness of our proposed method.
Table B2: Performance comparison on daytime and nighttime MFNet [21] dataset.
Method Modal Daytime mIoU (%) Nighttime mIoU (%)
FRRN[40] RGB 40.0 37.3
DFN[70] RGB 38.0 42.3
BiSeNet[69] RGB 44.8 47.7
SegFormer-B2[59] RGB 48.6 49.2
SegFormer-B4[59] RGB 49.4 52.4
MFNet[21] RGB-T 36.1 36.8
FuseNet[22] RGB-T 41.0 43.9
RTFNet[49] RGB-T 45.8 54.8
FuseSeg[50] RGB-T 47.8 54.6
GMNet[77] RGB-T 49.0 57.7
CMX(MiT-B2)[30] RGB-T 51.3 57.8
CMX(MiT-B4)[30] RGB-T 52.5 59.4
Sigma(VMamba-T) RGB-T 54.1 59.0
Sigma(VMamba-S) RGB-T 55.0 60.0
Sigma(VMamba-B) RGB-T 54.1 60.922 Z. Wan et al.
C Ablation Studies
Apart from the ablation studies on the effect of each of our components, we
further conduct experiments on the detailed design of the State Space Models.
InTab.C3,wecomparetheeffectofthestatesizeinStateSpaceModelsandthe
numberofCVSSblocksinourMambadecoder.Fromthetable,wecanfindthat
setting the state size to 4 and the decoder layers to [4,4,4] leads to the optimal
result.
Table C3: Ablation studies of decoder layers and the space size of the state space
models on the MFNet [21] dataset.
# Encoder State Size Decoder Layers mIoU (∇)
1 VMamba-T 4 [4,4,4] 60.5 (0.0)
2 VMamba-T 4 [3,3,3] 60.2 (0.3)
3 VMamba-T 4 [2,2,2] 59.4 (1.1)
4 VMamba-T 8 [4,4,4] 60.3 (0.2)
5 VMamba-T 16 [4,4,4] 59.7 (0.8)
D Qualitative Analysis of Different Modalities
Fig.D1: Comparative analysis of semantic segmentation results: single-modal vs.
multi-modal approach.
InFigureD1,weexaminethecontributionsofbothRGBandThermalmodal-
ities to the final prediction. By leveraging information from both modalities,
our Sigma model achieves more comprehensive segmentation and more precise
boundary delineation, illustrating its effectiveness in extracting valuable infor-
mation from both modalities.Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation 23
E Complexity Comparison of CroMB and Self-Attention
4.0 Concat Mamba (ConM) FLOPs
3.5 Concat Self-Attention (ConSA) FLOPs
3.0
2.5
2.0
1.5
1.0
0.5
0.0
400 600 800 1000 1200 1400
Sequence Length
Fig.E2: Qualitative computation comparison of Concat Self-Attention (ConSA) and
our Concat Mamba (ConM) mechanism.
In Fig. E2, we illustrate the qualitative growth in FLOPs as the input se-
quence length increases. It is evident that our ConM mechanism has much less
computation consumption than constituting the State Space Model with Self-
Attention. This underscores the exceptional efficiency of our proposed ConM in
integrating multi-modal features.
Table E4: Quantitative comparison of computation complexity between Concat Self-
Attention (ConSA) and our proposed Concat Mamba (ConM) mechanism.
Feature Size FLOPs (G)
Stage
Height Weight Channel ConM ConSA
1 120 160 96 1.82 –
2 60 80 192 1.71 77.89
3 30 40 384 1.65 15.94
4 15 20 768 1.62 8.19
InTableE4,wecomparethefloating-pointoperationspersecond(FLOPS)of
ourproposedConMBandConcatMamba(ConSA),whichemploysself-attention
insteadofSSM.The“Stage” columnindicatesthefourencodingstages,withthe
input feature size for each fusion block also provided. The findings reveal that
ConMBmaintainslowFLOPsacrossallstages,whereastheFLOPsfortheself-
attention mechanism escalate significantly with increases in height and width.
)G(
sPOLF