Wasserstein F-tests for Fréchet regression on
Bures-Wasserstein manifolds
Haoshu Xu∗ Hongzhe Li†
April 8, 2024
Abstract
This paper considers the problem of regression analysis with random covariance matrix
as outcome and Euclidean covariates in the framework of Fréchet regression on the Bures-
Wasserstein manifold. Such regression problems have many applications in single cell genomics
and neuroscience, where we have covariance matrix measured over a large set of samples.
Fréchet regression on the Bures-Wasserstein manifold is formulated as estimating the condi-
√
tional Fréchet mean given covariates x. A non-asymptotic n-rate of convergence (up to logn
√
factors) is obtained for our estimator Q(cid:98)n(x) uniformly for ∥x∥ ≲ logn, which is crucial for
deriving the asymptotic null distribution and power of our proposed statistical test for the
null hypothesis of no association. In addition, a central limit theorem for the point estimate
Q(cid:98)n(x) is obtained, giving insights to a test for covariate effects. The null distribution of the
test statistic is shown to converge to a weighted sum of independent chi-squares, which implies
that the proposed test has the desired significance level asymptotically. Also, the power per-
formance of the test is demonstrated against a sequence of contiguous alternatives. Simulation
results show the accuracy of the asymptotic distributions. The proposed methods are applied
to a single cell gene expression data set that shows the change of gene co-expression network
as people age.
Keywords: Fréchet regression; Functional calculus; Hypothesis testing; Optimal transport;
Wasserstein distance.
∗Graduate Group in Applied Mathematics and Computational Science, University of Pennsylvania, Philadelphia,
PA 19104, USA; email: haoshuxu@sas.upenn.edu
†DepartmentofBiostatistics,EpidemiologyandInformatics,UniversityofPennsylvania,Philadelphia,PA19104,
USA; email: hongzhe@upenn.edu.
1
4202
rpA
5
]EM.tats[
1v87830.4042:viXraContents
1 Introduction 3
1.1 Main contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2 Related works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.3 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.4 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Priliminaries 6
3 Problem Formulation 7
3.1 Fréchet regression on Bures-Wasserstein manifold . . . . . . . . . . . . . . . . . 7
3.2 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4 Statistical Inference for Fréchet Regression on Bures-Wasserstein manifold 10
4.1 Estimation under the Fréchet regression model . . . . . . . . . . . . . . . . . . 10
4.2 Hypothesis testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
4.2.1 Test statistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.2.2 Theoretical properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
5 Algorithm and Numerical Experiments 15
5.1 Riemannian gradient descent algorithm . . . . . . . . . . . . . . . . . . . . . . . 16
5.2 Simulation setup and results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
5.3 Sensitivity of the results when covariance matrices are unknown . . . . . . . . . 18
6 Application to Single-cell Gene Co-expression Networks 22
7 Discussion 23
A Background on optimal transport and functional calculus 30
A.1 Geometry of optimal transport . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
A.2 Functional calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B Technical lemmas 31
B.1 Differentials of optimal transport maps . . . . . . . . . . . . . . . . . . . . . . . 31
B.2 Concentration inequalities and uniform convergence . . . . . . . . . . . . . . . . 36
B.2.1 Proof of Lemma 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
B.3 Properties of F and Q∗ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
C Proof of Theorem 6 51
C.1 Proof of Lemma 31 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
C.1.1 Proof of Claim 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
C.1.2 Proof of Claim 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
C.2 Proof of Lemma 33 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
C.3 Proof of Lemma 34 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
C.4 Proof of Lemma 35 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
D Proof of Theorem 7 75
E Proof of Corollary 9 77
2F Proof of Theorem 11 78
F.1 Proof of Lemma 37 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
F.2 Proof of Lemma 38 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
F.3 Proof of Claim 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
G Proof of Proposition 13 85
G.1 Proof of Lemma 39 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
H Proof of Theorem 14 87
H.1 Proof of Lemma 40 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
1 Introduction
Dataintheformofpositivedefinitematricesfrequentlyariseinmoderndataanalysisincluding
medical imaging (Dryden et al., 2009; Fillard et al., 2007), neuroscience (Friston, 2011), signal
processing (Arnaudon et al., 2013) and computer vision (Caseiro et al., 2012). For example,
large-scalesinglecellRNA-seqdataallowsustoestimatetheindividual-specificcovariancema-
trix, which can be interpreted as co-expression network among a set of genes. In neuroimaging
data, covariance matrices (or correlation matrices after standardization) of multiple brain re-
gions are used to summarize as functional connectivity matrices. For all these applications,
one key question is how to perform regression analysis where the covariance matrix is treated
as outcome together with a set of covariates.
Regression models for covariance matrix outcomes have been studied before. Chiu et al.
(1996) proposed to model the elements of the logarithm of the covariance matrix as a linear
function of the covariates, which requires a large number of parameters to be estimated. Hoff
and Niu (2012) introduced a regression model where the covariance matrix is a quadratic
function of the explanatory variables. Zou et al. (2017) linked the matrix outcome to a linear
combinationofsimilaritymatricesofcovariatesandstudiedtheasymptoticpropertiesofvarious
estimators under this model. Zhao et al. (2021) developed Covariate Assisted Principal (CAP)
regression model for several covariance matrix outcomes. This model aims to identify linear
projectionsofthecovariancematricesthatareassociatedwiththecovariates. However,allthese
methods impose certain structures to the covariance matrices or involve many parameters.
A fundamental aspect in the investigation of regression models for covariance matrices
involves the choice of metric. Various metrics on the space S++ of d × d positive definite
d
matrices have been studied before, including the trace metric (Lang, 1999), affine-invariant
metric (Moakher, 2005; Fletcher and Joshi, 2007) and log-Cholesky metric (Lin, 2019), among
which the Bures-Wasserstein metric W, originally introduced by Bures (1969) , is defined by
(cid:16) (cid:17)1/2
W2(Q,S) = trQ+trS −2tr Q1/2SQ1/2 (1)
for any pair of Q,S ∈ S++. This metric has been of interest in quantum information (Bures,
d
1969), referred to as the Bures distance, and coincides with the Wasserstein distance between
two centered Gaussians with corresponding covariance matrices. Wasserstein distance, as a
special case of the problem of optimal transport (OT) which lies at the intersection of opti-
mization, analysis and geometry, is a metric between probability distributions defined as the
minimal cost to transport mass from one distribution to another (Villani, 2003, 2009). It has
proven valuable for various tasks in statistics and machine learning (Abadie and Imbens, 2006;
Deb and Sen, 2023; Arjovsky et al., 2017; Redko et al., 2017; Hallin et al., 2021). In single-cell
genomics, most measurement technologies are destructive assays, such that the same cell can-
not be observed twice nor profiled over time. As a result, the measurement at each time point
3is mathematically modeled as a distribution, and optimal transport techniques are well-suited
forstudyingvariousassociateddynamics(Schiebingeretal.,2019;Bunneetal.,2022;Somnath
et al., 2023; Bunne et al., 2023a,b). Given its connection with the theory of optimal transport,
the Bures-Wasserstein metric is therefore a natural choice for studying the gene expression co-
variancematrices. Equippedwiththismetric, S++ turnsintoaRiemanninanmanifold(Bhatia
d
et al., 2019), known as the Bures-Wasserstein manifold.
In this paper, we focus on regression analysis between (S++,W)-valued responses and
d
Euclidean predictors. Formally, suppose independent and identically distributed (i.i.d.) pairs
ofpredictorandresponsevariables(X ,Q ),...,(X ,Q ) ∈ Rp×S++ aregiven. Thegoalisto
1 1 n n d
perform inference, particularly to test for the effects of covariate X on the response variable Q.
To this end, we assume a Fréchet regression model on the Bures-Wasserstein manifold, which
is originally proposed in Petersen and Müller (2019) and defines a global regression function
between response data in an arbitrary metric space and Euclidean predictors.
Previous research on the Fréchet regression model has focused on consistency in the asymp-
totic regime (Petersen and Müller, 2019; Chen and Müller, 2022), and inference is only consid-
ered in the special case of one-dimensional (1D) density curves for the response variable under
the Wasserstein metric (Petersen et al., 2021). It is important to note that for any pair of 1D
distributions µ,ν ∈ P(R) with distribution functions F ,F , the squared 2-Wasserstein dis-
µ ν
tance between them is equal to (cid:82)1(cid:12) (cid:12)F−1(t)−F−1(t)(cid:12) (cid:12)2 dt. Therefore, the 1D Wasserstein space
0 µ ν
is very special in the sense that it has zero sectional curvature (Ambrosio et al., 2005), and
can be embedded into a Hilbert space. Closed-form expressions can then be obtained for the
1D Fréchet inference in this case and various other problems are also well-understood owing to
this flat geometry (Panaretos and Zemel, 2016; Chen et al., 2023; Bigot et al., 2017). However,
most metric spaces of interest are nonlinear and have nonzero curvature, for example, S++
d
has non-positive curvature when equipped with the log-Cholesky metric (Lin, 2019) and the
Wasserstein space of distributions in d dimension is positively curved when d > 1 (Ambrosio
et al., 2005), which includes the Bures-Wasserstein manifold as a special case. As a result, the
existence of a Hilbert embedding in such curved metric spaces cannot be assumed, making the
derivation of distributional results for Fréchet regression in these spaces challenging, let alone
statistical inference with guarantees on both significance level and power.
In the present paper, we aim to address these challenges on the Bures-Wasserstein manifold
by proposing a valid test for effects of X on Q. A central limit theorem for point estimate
Q(cid:98)n(x) is also obtained.
1.1 Main contribution
We focus on Fréchet regression on Bures-Wasserstein manifold. Our main contribution is
√
threefold. First,anon-asymptotic n-rateofconvergence(uptolognfactors)oftheregression
√
estimateQ(cid:98)n(x)atcovariatexisestablisheduniformlyfor∥x∥ ≲ logn. Toourbestknowledge,
this is the first non-asymptotic uniform (over a possibly diverging region) convergence result
for Fréchet regression. In addition to the standard assumptions that X,Q have light tails, we
only assume well-separation and a local curvature lower bound. The assumptions are mild
enough, and are verified in a simple case. The results would be crucial later when deriving the
asymptotic null distribution and power of our proposed test of association between covariance
matrix and covariates.
Next, we derive a central limit theorem for the point estimate Q(cid:98)n(x) that results in a
pointwise confidence region. The covariance operator of the limiting Gaussian distribution is
shown to have contributions from two parts. One is the variability in Q and the other is the
imperfect information of X.
Then,wecarefullyconstructateststatisticthathasatractableasymptoticnulldistribution,
4which is equal to a weighted sum of χ2s. The weights are determined by covariance of the
p
tangent vector that can be viewed as a generalization of the classical noise variance. The
proposed test is also shown to be powerful against a sequence of contiguous alternatives. To
ourbestknowledge, thisisthefirsttestforFréchetregressiononaspacewithnonzerosectional
curvature.
We also validate our theoretical results by numerical simulations.
1.2 Related works
Statistical OT AsidefromadvancesincomputationalOT(Cuturi,2013;PeyréandCuturi,
2019; Altschuler et al., 2017), there is a surge of interest in the statistical aspects of OT where
stability of the estimated densities (Weed and Berthet, 2019), Wasserstein distances (Barrio
and Loubes, 2019; Mena and Niles-Weed, 2019; del Barrio et al., 2023; Altschuler et al., 2022),
transport maps (Hütter and Rigollet, 2021; Pooladian and Niles-Weed, 2022; Manole et al.,
2022; Gonzalez-Sanz et al., 2022; Pooladian et al., 2023; Manole et al., 2023) and Fréchet
mean (Agueh and Carlier, 2011; Kim and Pass, 2017; Le Gouic and Loubes, 2017; Le Gouic
et al., 2022; Altschuler et al., 2021), are investigated in the presence of sampling noise. For the
WassersteinFréchetmean, LeGouicetal.(2022)establishesparametricrateofconvergencefor
empirical Fréchet mean in the more general Alexandrov spaces that include the 2-Wasserstein
spaceasaspecialcasebyintroducingabi-extendibilityconditionthattranslatesintoregularity
conditions on the Kantorovich potentials. This condition is later relaxed by Chewi et al.
(2020) when establishing the linear rate of convergence for gradient descent algorithms over
the Wasserstein space. Moving one step further, Panaretos and Zemel (2016); Agueh and
Carlier (2017) establish certain types of central limit theorem for the empirical Fréchet mean
of 1D distributions. Later, a central limit theorem for the multivariate Gaussians is established
by exploiting the first order differentiability of optimal transport maps Kroshnin et al. (2021).
Fréchet mean The Fréchet mean is a natural extension of the notion of average on an
abstract metric space. For its properties in general curved metric spaces, see Ohta (2012);
Yokota (2016); Le Gouic et al. (2022) and references therein. The existence and uniqueness of
Fréchet mean in the case of Riemanninan manifolds and Wasserstein spaces are established in
(Agueh and Carlier, 2011; Kim and Pass, 2017; Le Gouic and Loubes, 2017). The asymptotic
properties of empirical Frćhet mean on a Riemanninan manifold are addressed in Bhattacharya
and Patrangenaru (2003, 2005); Le Gouic et al. (2022).
Fréchet regression The Fréchet regression model can be viewed as an extension of the
Fréchet mean by considering weighted average, and is first introduced in Petersen and Müller
(2019). Petersen et al. (2021) proposed an F-test in the special case of 1D density response.
The uniform convergence is not needed for inference there due to the availability of explicit
expressions and the Hilbert embedding, while in our case, it is essential to show that contribu-
tions of the remainder term from the Taylor expansion is indeed negligible. It is worth noting
that even though the uniform convergence of Fréchet regression is also considered in Petersen
and Müller (2019); Chen and Müller (2022), they are asymptotic and only uniform for x in a
fixed compact set while our result is non-asymptotic, and the uniformity is within a compact
set with growing diameter to accommodate the possible unboundedness of suppX.
Regression model on manifolds ItisalsoimportanttonotethattheFréchetregression
model is defined solely in terms of distance, making it applicable in any abstract metric spaces.
Meanwhile,thereisaseparatelineofresearchdedicatedtoregressiononmanifolds(Yuanetal.,
2012; Cornea et al., 2017; Lin et al., 2023; Chen et al., 2023). Such regression models builds
5upon the notion of tangent spaces in differential geometry. The tangent space, being a linear
space, then enables regression on manifolds to essentially reduce to classical linear regression
in tangent spaces.
1.3 Organization
Therestofthepaperisorganizedasfollows. First,weprovidenecessarybackgroundonoptimal
transportinSection2. Next,theFréchetregressionmodelisformulatedinSection3.1,followed
by the assumptions in Section 3.2. The main results are presented in Section 4 where we show
uniform convergence of our estimator in Section 4.1, propose our test and provide theoretical
guarantees in Section 4.2. Finally, a Riemannian gradient descent algorithm and numerical
simulations are presented in Section 5 to validate our theory. The proofs of our theorems,
technical lemmas are deferred to the Appendix. We conclude with a discussion in Section 7.
1.4 Notation
We denote by Z and R the set of integers and the set of non-negative real numbers. For
+
any a,b ∈ R, we write a ∨ b = max(a,b) and a ∧ b = min(a,b). For any x > 0, we write
log+(x) := log(x)∨1. For any integer K ≥ 1, [K] = {1,...,K}. Given z ∈ R for i ∈ [n],
i
the set {z ,...,z } is denoted by zn. The Euclidean norm on Rp is denoted ∥·∥. For any
1 n 1
x ∈ Rp and L > 0, let B (L) = B(x,L) = (cid:8) y ∈ Rd : ∥x−y∥ ≤ L(cid:9). We denote by S ,S+,S++
x d d d
the set of all d×d symmetric, positive semi-definite and positive definite matrices. For any
real a < b, we define S (a,b) := {A ∈ S : aI ⪯ A ⪯ bI }. The subscript d is omitted when
d d d d
it’s clear from context. Given any A ∈ S , denote the largest and smallest eigenvalue of A
d
by λ (A) and λ (A). The Frobenius norm and operator norm of a matrix A is denoted
max min
by ∥A∥ and ∥A∥ . Given any matrix A ∈ Rm,n, let vecA ∈ Rmn denote the vectorized A
F op
obtained by stacking columns of A. Given a random variable X and α > 0, the ψ -"norm" of
α
X, denoted by ∥X∥ , is defined in Appendix B.2. The support of a probability distribution
ψα
is denoted by supp(·).
Given normed spaces Y and Z, let L(Y;Z) denote the space of all bounded linear operator
from Y to Z. Given a function ϕ : Y → Z and integer k ≥ 0, the k-th differential dkϕ, its
operator norm (cid:13) (cid:13)dkϕ(cid:13)
(cid:13)
and symmetric norm (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)dkϕ(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
are defined in Section 2.
Finally, the quantities C and c will refer to constants whose value may change from line to
line. All constants throughout may depend on the dimension and additional problem param-
eters, whenever they are clear from context. Given sequences (a )∞ and (b )∞ , we write
n n=1 n n=1
a ≲ b if there exists C > 0 such that a ≤ Cb , and we also write a ≍ b if b ≲ a ≲ b .
n n n n n n n n n
2 Priliminaries
Weprovideaconciseoverviewoffundamentalconceptsinoptimaltransport, alongwithassoci-
ated differential properties, specifically focusing on the case of centered Gaussian distributions.
Given a Polish space (E,d), let P (E) denote the collection of all (Borel) probability mea-
2
suresµonE suchthatE d(X,y)2 < ∞forsomey ∈ E. Onecanshowthatthedefinitionof
X∼µ
P (E)isindependentofthechoiceofy. WespecializetothecasewhenE = Rd withEuclidean
2
distance. For any pair of measures µ,ν ∈ P (Rd), let Π(µ,ν) be the set of couplings of between
2
µ and ν, that is, the collection of probability measures π on Rd×Rd such that if (X,Y) ∼ π,
then X ∼ µ and Y ∼ ν. The 2-Wasserstein distance between µ and ν is defined as
(cid:104) (cid:105)
W2(µ,ν) := inf E ∥X −Y∥2 (2)
(X,Y)∼π
π∈Π(µ,ν)
6Here,withaslightabuseofnotation,weuseW todenoteboththeWassersteindistancebetween
twodistributionsandtheBures-WassersteindistancebetweentwoPSDmatrices. Thisnotation
is justified by the fact that both distances coincide and have a closed-form expression (1) when
we identify centered Gaussian distributions with their covariance matrices.
Let P (Rd) denote the subset of measures in P (Rd) that are absolutely continuous with
2,ac 2
respect to the Lebesgue measure. Given µ ,µ ∈ P (Rd), Brenier’s theorem guarantees the
0 1 2,ac
existence of a unique optimal coupling π⋆ ∈ Π(µ ,µ ) that achieves the minimum in (2) and
0 1
that it is induced by the optimal transport map Tµ1 : Rd → Rd in the sense that Tµ1(X) ∼ µ
µ0 µ0 1
wheneverX ∼ µ . Specifically,whenµ ,µ arecenteredGaussiandistributionswithcovariance
0 0 1
matrices Q,S, the optimal transport map is given by the linear map
(cid:16) (cid:17)−1/2 (cid:16) (cid:17)1/2
TS = S1/2 S1/2QS1/2 S1/2 = Q−1/2 Q1/2SQ1/2 Q−1/2 (3)
Q
For completeness, additional background on the geometry of optimal transport is provided in
Appendix A.1.
Kroshnin et al. (2021) showed that for any fixed S ∈ S++, TS is (Fréchet) differentiable
d Q
with respect to Q, with the differential at Q denoted by dTS. They also showed that for fixed
Q
S ∈ S++, the squared Wasserstein distance W2(·,S) : S++ → R is twice differentiable, with
d d
the corresponding 1st and 2nd differential dW2(Q,S),d2W2(Q,S) satisfying
dW2(Q,S)(X) = (cid:10) I −TS,X(cid:11)
Q (4)
d2W2(Q,S)(X,Y) = −(cid:10) X,dTS(Y)(cid:11)
Q
In this paper, higher order differentials dkTS are essential for the development of the theory.
Q
Hence additional background on functional calculus are provided in Appendix A.2 for self-
containedness.
3 Problem Formulation
Inthissection, weformulatetheFréchetregressionmodelinSection3.1andintroduceassump-
tions in Section 3.2.
3.1 Fréchet regression on Bures-Wasserstein manifold
Given a metric space (Y,d), let P be a probability distribution over Rp × Y that generates
random objects (X,Y). The main difficulty of formulating a regression model between Y and
X lies in the fact that the metric-space-valued response Y is not amenable to linear operations.
The Fréchet regression model (Petersen and Müller, 2019) tries to generalize classical linear
regressioninageneralmetricspace,buildinguponthenotionofFréchetmeanthatweintroduce
first.
The concept of Fréchet mean E Y generalizes the notion of average in a general metric
Fréchet
space (Y,d), and is defined as follows.
E Y := argminE d2(y,Y) (5)
Fréchet Y
y∈Y
The above definition is motivated by the fact that when Y is an Euclidean space, E Y
Fréchet
coincideswiththeclassicalnotionofexpectationEY. Forthisreason,wewilldropthesubscript
and denote EY for the Fréchet mean from now on.
7WiththenotionofFréchetmeaninplace,theFréchetregressionmodelproposedbyPetersen
and Müller (2019) is defined as follows.
E[Y|X = x] = argminE (cid:2) s(x,X)d2(y,Y)(cid:3) (6)
(X,Y)∼P
y∈Y
where
s(x,X) = 1+(x−µ)⊤Σ−1(X −µ), µ = E(X),Σ = Var(X) (7)
Here the conditional expectation E[Y|X = x] on the left-hand side of (6) is defined as a simple
extension of (5) by considering the conditional minimizer.
E[Y|X = x] := argminE (cid:2) d2(y,Y)|X = x(cid:3)
Y
y∈Y
Meanwhile, the objective function E(cid:2) s(x,X)d2(y,Y)(cid:3) on the right-hand side of (6) also gener-
alizes from the Fréchet mean (5) by considering expectation weighted by s(x,X), in the same
flavor as the kernel estimator from non-parametric statistics (Wasserman, 2006). The specific
choice of the weight s(x,X) in (7) is motivated by the fact that the minimizer would equal to
the desired conditional expectation of Y at x under the classical linear regression model. More
specifically, when Y = R and E[Y|X = x] = a⊤(x−µ)+b, one can check that
(cid:104) (cid:105)
a⊤(x−µ)+b = argminE s(x,X)(y−Y)2
Y
y∈R
For further details regarding existence and uniqueness of various concepts defined above, see
Petersen and Müller (2019) and references therein.
WhenspecicalizedtotheBures-Wassersteinmanifold(Y,d) = (S++,W),wedenoteQ∗(x) :=
d
E[Q|X = x] and our model assumes
Q∗(x) = argmin F(x,S), F(x,S) := E(cid:2) s(x,X)W2(S,Q)(cid:3) (8)
S∈P2(Rd)
FordetaileddiscussionsontheexistenceanduniquenessofthepopulationandempiricalFréchet
mean on the Bures-Wasserstein manifold, we refer readers to Agueh and Carlier (2011); Krosh-
nin et al. (2021) and Panaretos and Zemel (2020).
3.2 Assumptions
In order to get provable theoretical guarantees for estimation and hypothesis testing, we im-
pose the following model assumptions that we believe are theoretically minimal while also
maintaining generality.
We begin with conditions on the marginal distribution of covariate X and conditional
distribution of Q given X in Assumption 1 and 2.
Assumption 1. X is sub-Gaussian with ∥X∥ ≤ C and λ (Σ) ≥ c for some constants
ψ2 ψ2 min λ
C ,c > 0.
ψ2 λ
Assumption 2. Given X = x ∈ suppX, the eigenvalues of Q are bounded away from 0 and
infinity in the sense that
P(cid:0) Q ∈ S (cid:0) γ (∥x−µ∥)−1,γ (∥x−µ∥)(cid:1) |X = x(cid:1) = 1 (9)
d 1 1
where γ : R+ → R+ is defined by
1
γ (t) := c (t∨1)C1 (10)
1 1
for some constant c ≥ 1 and C ≥ 0.
1 1
8Assumption 2 implies upper and lower bounds on both the conditional expectation Q∗(x)
(see Lemma 28) and noise. An upper bound on the population covariance matrix is often
assumed within the literature of covariance matrix estimation (Cai et al., 2010). The lower
bound on λ (Q∗(x)) here bears resemblance to the uniform upper bound on the the condi-
min
tional densities in the Fréchet regression of 1D density response curves (Petersen et al., 2021,
Assumption T4) since density is inversely proportional to the standard deviation in a 1D
location-scale family. Assumptions on both upper and lower bounds are natural in the context
of optimal transport. Hütter and Rigollet (2021); Manole et al. (2022); Pooladian and Niles-
Weed (2022) assumed smoothness and strong convexity of the Brenier potential for optimal
transport map estimation which translates to upper and lower bounds on eigenvalues of the
covariance matrix when specialized to the Gaussian case. Altschuler et al. (2021) also assume
both upper and lower bounds on the eigenvalues to ensure a variance inequality proposed in
√
Chewi et al. (2020), which is crucial to prove the n-convergence of empirical barycenter as
well as the linear convergence of a gradient descent algorithm on Bures-Wasserstein manifold.
Remark 1. The bounds here depend on γ (∥x−µ∥) which diverges as x → ∞. This is
1
motivated by the fact that the conditional mean E(Y|X = x) diverges as x → ∞ in the classical
linear regression. More specifically, when µ = 0 and E[Y|X] = a⊤X +b, one can show that
|E[Y|X]| ≤ γ (X) with c = ∥a∥+|b| and C = 1. In our setting, we believe that a polynomial
1 1 1
growth rate γ 1(t) ≲ tC1, which is allowed by (10), is often satisfied in practical applications.
Remark 2. The bounded noise assumption can be relaxed by assuming Q has a light tail
conditional on Q∗(X). One possibility is to make the assumption that
(cid:26) λ (Q∗(x)) λ (Q) (cid:12) (cid:27)
P min ∨ max > t(cid:12)X = x ≲ exp(−ctα), ∀t > 0
λ (Q) λ (Q∗(x)) (cid:12)
min max
for some constant α > 0. Note that again λ (Q) is bounded from above and λ (Q) is
max min
bounded from below. The proof presented in our paper remains valid by incorporating additional
concentration arguments.
Next is the assumption of the Fréchet regression model.
Assumption 3. For any x ∈ suppX, Q∗(x) is the unique minimizer of F(x,·).
Then, itisnaturaltoimposemoreassumptionsontheminimizerQ∗(x)ofF(x;·). Assump-
tion 4 below is concerned with the global behavior of F(x,·) outside a local ball around Q∗(x)
while Assumption 5 focuses locally on the eigenvalue lower bound of the second differential of
F(x,·) at Q∗(x).
Assumption 4. There exist constants α ≥ 1 and c > 0 such that for any x ∈ suppX and
1 δ
any (δ,∆) that satisfies 0 ≤ δ ≤ c ≤ ∆, the following
δ
(cid:110) (cid:111) δα1
inf F(x,S)−F(x,Q∗(x)) : δ ≤ ∥S −Q∗(x)∥ ≤ ∆ ≥ (11)
op γ (∥x−µ∥,∆)
2
holds where γ : R+×R+ → R+ is defined by
2
γ (t ,t ) = c (t ∨1)C2(t ∨1)C2
2 1 2 2 1 2
for constants c ≥ 1,C ≥ 0.
2 2
Remark 3. Assumption 4 is motivated by the well-separated-maximizer assumption in M-
Estimation (Van Der Vaart and Wellner, 1996, Lemma 3.2.1), which is also assumed in (Pe-
tersen and Müller, 2019). In the special case when X and Q are independent, Lemma 29
demonstrates that Assumption 4 holds with c = 1/(2c ), α = 2, C = 1 and some constant
δ 1 1 2
c large enough. Here we allow for a polynomial dependence on ∥x−µ∥ in the definition of γ
2 3
so that (12) is still expected to hold in the general case when X and Q are not independent.
9Assumption 5. For any x ∈ suppX, the following lower bound
(cid:16) (cid:17) 1
λ −Es(x,X)dTQ ≥ (12)
min Q∗(x) γ (∥x−µ∥)
3
holds where γ : R+×R+ → R+ is defined by
3
γ (t) := c (t∨1)C3
3 3
for some constants c ≥ 1 and C ≥ 0.
3 3
Remark 4. We offer some explanations of the notation λ (−Es(x,X)dTQ ) here. By
min Q∗(x)
definition (see Appendix B.1), dTQ ∈ L(Rd×d,Rd×d) is a symmetric linear operator from
Q∗(x)
Rd×d to Rd×d. When identifying matrices in Rd×d with vectors in Rd2, dTQ can be view as
Q∗(x)
a symmetric d2×d2 matrix. Hence λ (−Es(x,X)dTQ ) can be understood as the smallest
min Q∗(x)
(cid:16) (cid:17)
eigenvalue of the d2 ×d2 symmetric matrices that corresponds to −E s(x,X)dTQ . Here
Q∗(x)
(cid:16) (cid:17)
E −s(x,X)dTQ is equal to the second differential of F(x,·) at Q∗(x); see Appendix A.2
Q∗(x)
for relevant concepts in functional calculus and Appendix B.1 for explicit expression for dTQ .
Q∗(x)
Remark 5. In the special case when X and Q are independent, which is a consequence of
Assumption 6 and the null hypothesis of no effect (18) below, one can show that (Lemma 29
in Appendix B.3) Assumption 5 holds for C = 0 and c large enough. Again, dependence on
3 3
∥x−µ∥ is allowed in order to account for the possible unboundedness of x when X and Q are
independent.
Finally, we assume conditional independence between X and Q given Q∗(X) for hypothesis
testing. However, this is not required by the uniform convergence (Theorem 6) and the central
limit theorem (Theorem 7).
Assumption 6. X and Q are independent conditional on Q∗(X).
4 Statistical Inference for Fréchet Regression on Bures-
Wasserstein manifold
With the assumptions in Section 3 in place, we turn the focus in this section to hypothesis
testing under the Fréchet regression model. To this end, we first show the uniform convergence
of the Fréchet regression estimator and prove a central limit theorem in Section 4.1. The
uniform convergence is not only of theoretical interest itself, but also crucial later for deriving
the asymptotic size and power of our proposed test. Next, we introduce in Section 4.2.1 the
test statistic. Then we study its the asymptotic null distribution and the asymptotic power of
the proposed test in Section 4.2.2.
4.1 Estimation under the Fréchet regression model
We consider the Fréchet regression estimator defined as follows.
n
Q(cid:98)n(x) := argminF n(x,S), F n(x,S) := 1 (cid:88)(cid:2) s n(x,X i)W2(S,Q i)(cid:3) (13)
n
S∈S d++ i=1
10Here s n(x,X i) = 1+(x−X)Σ(cid:98)−1(X i−X) with
n n
X =
n−1(cid:88)
X i, Σ(cid:98) =
n−1(cid:88)(cid:0)
X
i−X(cid:1)(cid:0)
X
i−X(cid:1)⊤
i=1 i−1
This estimator is also studied in Petersen and Müller (2019) where consistency in the general
metric space is investigated under extra assumptions on the covering number based on the
theory of M-estimation. In the present paper, we show a non-asymptotic parametric rate
of convergence (up to log(n) factors) can be achieved uniformly for ∥x∥ ≲ (cid:112) log(n) on the
Bures-Wasserstein manifold. The results are summarized as Theorem 6.
√
Theorem 6. Suppose Assumption 1-4 hold. Let L := C logn for some constant C > 0
n L L
large enough. Then with probability at least 1−O(n−100), the following inequalities hold
(cid:13) (cid:13) logC2(n)
x∈Bsu (µp ,Ln)(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13) F
≤ C
uni
√
n
(14)
(cid:13) (cid:13) logC2(n)
1≤su i≤p n(cid:13) (cid:13)Q(cid:98)n(X i)−Q∗(X i)(cid:13)
(cid:13) F
≤ C
uni
√
n
(15)
for some constant C independent of n.
uni
The above theorem, to our best knowledge, is the first non-asymptotic uniform (over a pos-
sibly diverging region) convergence result for Fréchet regression. In the earlier work including
Petersen and Müller (2019), Chen and Müller (2022), asymptotic uniform convergence results
have been established over a fixed compact set. However, these are not enough to derive the
asymptotic size and power of our test when X has unbounded support since we would require
(15)toholdwhilemax ∥X −µ∥diverges. Tothisend, weresorttonon-asymptoticbounds
i∈[n] i
and derive (15) from (14).
The proof of Theorem 6 is involved, and we give an outline here. First, we follow a similar
argument as in Agueh and Carlier (2011) to show that the largest eigenvalue of the estimates
Q(cid:98)n(x) are uniformly bounded from above. Then we demonstrate that Q(cid:98)n(x) converges uni-
formly in a slow rate by applying the chaining method. Here some special efforts are needed
duetotheHöldercontinuityofthesquareddistanceW2; seeLemma31. Finally, weenhanceto
auniformfastrateofconvergencebysolvingaquadraticinequalityintermsoftheconvergence
rate. A detailed proof of Theorem 6 is provided in Appendix C.
Theorem 6 implies the pointwise consistency of the Fréchet regression estimator (13). Mov-
ingonestepfurther, weestablishacentrallimittheoremthatiselusiveingeneralmetricspaces
with nonzero curvature (Petersen and Müller, 2019). In order to present the theorem, we pause
to introduce several notations. First, for any x ∈ Rp, define ⃗x := (cid:0) 1 x⊤(cid:1)⊤. For any random
vector X ∈ Rp with covariance matrix Σ, let Σ⃗ ∈ R(p+1)×(p+1) denote the covariance matrix
of X⃗. Next, let (E(−s(x,X)dTQ ))−1 denote the inverse of E(−s(x,X)dTQ ) which is a
Q∗(x) Q∗(x)
linear operator in L(S ;S ) (Appendix B.1). Last, given elements x ,x of a Hilbert space H,
d d 1 2
the tensor product operator x ⊗x : H → H is defined by (x ⊗x )y = ⟨x ,y⟩x for any
1 2 1 2 1 2
y ∈ H; see Hsing and Eubank (2015) for properties of the tensor product operator and its role
in the central limit theorem for random elements of a Hilbert space.
Withthesenotationsinplace, thecentrallimittheoremfortheFréchetregressionestimator
Q(cid:98)n(x) is stated as Theorem 7.
Theorem 7. Suppose Assumption 1-4 hold. Then for any fixed x ∈ suppX, the following
central limit theorem
√ (cid:104) (cid:105) (cid:16) (cid:16) (cid:17)(cid:17)−1
n Q(cid:98)n(x)−Q∗(x) →w E
(X,Q)
−s(x,X)dT QQ
∗(x)
Z x, (16)
11holds. Here Z ∼ N(0,Ξ ) is a Gaussian random element of Rd×d with covariance operator
x x
Ξ equal to EV ⊗V where
x x x
V = V +V
x x,1 x,2
(cid:16) (cid:17)
V = s(x,X) TQ −I
x,1 Q∗ d
(cid:16) (cid:17) (cid:16) (cid:17)
V = − ⃗x⊤Σ⃗−1(X⃗X⃗⊤−Σ⃗)⊗I · EX⃗ ⊗(TQ −I )
x,2 d Q∗ d
Remark 8. Since a linear transformation of a multivariate normal distribution is still nor-
√
mal, Theorem 7 implies that asymptotically the entries of n(Q(cid:98)n(x)−Q∗(x)) jointly follow a
multivariate normal distribution in Rd2 when vectorized. More specifically, we have
√ (cid:16) (cid:17)
n vecQ(cid:98)n(x)−vecQ∗(x) →w N (0,Ω x) (17)
Here Ω = H−1E(vecV )(vecV )⊤H−1 where H ∈ Rd2×d2 is the matrix representing the
x x x x x x
invertible linear operator E(−s(x,X)dTQ ); see Appendix B.1 for a closed form expression
Q∗(x)
of H .
x
Theorem 7 shows that the covariance operator Ξ has contribution both from V and V .
x x,1 x,2
If both the expectation µ and the covariance matrix Σ of the predictor X are known and we
get an estimate Q(cid:101)n(x) of Q∗(x) by directly optimizing F(x,S) from (8), then Q(cid:101)n(x) would
follow a central limit theorem with covariance operator exactly equal to EV ⊗V . When µ
1,x 1,x
and Σ are unknown and empirical estimate µ
(cid:98)
and Σ(cid:98) are plugged in as in (13), we would get
an extra contribution in Ξ from V . Theorem 7 is validated through numerical experiments
x x,2
in Section 5, and the proof is given in Appendix D. To our best knowledge, the expression for
Ξ cannot be further simplified in general. However, under the hypothesis that X and Q are
x
independent, which can be a consequence of the null hypothesis in Section 4.2 and Assumption
6, V vanishes and Ξ only has contribution from V . This is summarized in Corollary 9
x,2 x x,1
below; see Appendix E for the proof.
Corollary 9. Instate the assumptions in Theorem 7. If X and Q are independent, then
√ (cid:104) (cid:105) (cid:16) (cid:16) (cid:17)(cid:17)−1
n Q(cid:98)n(x)−Q∗(x) →w E
(X,Q)
−s(x,X)dT QQ
∗(x)
Z x′,
holds where Z′ ∼ N(0,Ξ′) and
x x
(cid:104) (cid:16) (cid:17) (cid:16) (cid:17) (cid:105)
Ξ′ = E s(x,X) TQ −I ⊗ TQ −I s(x,X)
x Q∗ d Q∗ d
4.2 Hypothesis testing
In this section, we consider testing the global null hypothesis of no effects under the Fréchet
regression model on the Bures-Wasserstein manifold as follows.
H : Q∗(x) ≡ Q∗ for some unknown Q∗ (18)
0
We first motivate and introduce the test statistic in Section 4.2.1. Then its asymptotic null
distribution is investigated in Section 4.2.2, followed by a study on the asymptotic size and
power of our proposed test.
124.2.1 Test statistic
Note that one crucial difference between the Fréchet regression model (8) and the classical
linear or generalized linear regression model is that the conditional expectation is defined by
anoptimizationproblemratherthandirectlythroughsomelinkfunction, andhencethereisno
parameter like the slope β in the linear model. As a result, we can only test the null hypothesis
bydirectlyaggregatecomparisonsbetweenestimatedpredictionsQ(cid:98)n(X i)andtheFréchetmean
Q∗.
Given the uniform consistency of the Fréchet regression estimator Q(cid:98)n(x) in Theorem 6, we
propose the following test statistic for testing (18).
n n
T(cid:98)n = (cid:88)(cid:13) (cid:13) (cid:13)H(cid:98) ·(cid:16) Q(cid:98)n(X i)−Q(cid:98)n(X)(cid:17)(cid:13) (cid:13) (cid:13)2
F
, where H(cid:98) = − n1 (cid:88) dT QQ (cid:98)ni
(X)
(19)
i=1 i=1
To develop some intuition about T(cid:98)n, note that the "difference" between Q(cid:98)n(X i) and Q∗
should be small under the null hypothesis (18). Since Q∗ is unknown, Q∗ is then estimated by
Q(cid:98)n(X), the Fréchet mean of Q 1,...,Q n. Therefore, a sensible test statistic would be of the
form
n
(cid:88) (cid:16) (cid:17)
T
f
= f Q(cid:98)n(X i),Q(cid:98)n(X)
i=1
wheref : Rd×d×Rd×d → Rissomefunctionthatmeasuresthe"difference"betweenQ(cid:98)n(X i)and
Q(cid:98)n(X), and is expected to satisfy f(·,·) ≥ 0, f(Q,Q) = 0 and f(Q,S) = f(S,Q). Assuming
tightness of the 2nd order Taylor approximation, the uniform consistency of Q(cid:98)n(x) (Theorem
6) then implies
n (cid:28) (cid:29)
(cid:88) 1 (cid:16) (cid:17)
T
f
≈ H
f
Q(cid:98)n(X i)−Q(cid:98)n(X) ,Q(cid:98)n(X i)−Q(cid:98)n(X) (20)
2
i=1
where H
f
is the Hessian of f at Q(cid:98)n(X). This justifies the form of (19).
Furthermore,thespecificationofH
f
shoulddependonthedetaileddistributionofQ(cid:98)n(X i)−
Q(cid:98)n(X) in a way that (20) enjoys some tangible asymptotic distribution. To get the asymptotic
distribution of Q(cid:98)n(X i)−Q(cid:98)n(X), we resort to the optimality condition (cid:80) js n(x,X j)(T QQ (cid:98)nj (x)−
I ) = 0. Under the null hypothesis (18), assuming tightness of the first order approximation of
d
the optimality conditions at X and X we have
i
0 ≈ (cid:88) s n(X i,X j)(T QQ ∗j −I d)+(cid:88) s n(X i,X j)dT QQ ∗j (cid:16) Q(cid:98)n(X i)−Q∗(cid:17)
j j (21)
0 ≈ (cid:88) s n(X,X j)(T QQ ∗j −I d)+(cid:88) s n(X,X j)dT QQ ∗j (cid:16) Q(cid:98)n(X)−Q∗(cid:17)
j j
Take the difference and rearrange, we arrive at
 
− n1 (cid:88) dT QQ ∗j ·√ n(Q(cid:98)n(X i)−Q∗) ≈ √1
n
(cid:88) (s n(X i,X j)−1)(T QQ ∗j −I d)
j j
(cid:124) (cid:123)(cid:122) (cid:125)
a1(Xi) (22)
+ √1
n
(cid:88) (s n(X i,X j)−1)dT QQ ∗j (cid:16) Q(cid:98)n(X i)−Q∗(cid:17)
j
(cid:124) (cid:123)(cid:122) (cid:125)
a2(Xi)
13One can then show a (X ) is negligible compared to a (X ), hence
2 i 1 i
 
− n1 (cid:88) dT QQ ∗j ·√ n(Q(cid:98)n(X i)−Q∗) ≈ a 1(X i). (23)
j
Specifically, the intuition behind (23) is as follows. The null hypothesis (18) and Assumption 6
imply X and Q are independent. As a result, √1
n
(cid:80) j(s(X i,X j)−1)dT QQ ∗j has zero expectation
and is of order O p(1) by the central limit theorem, which implies that √1
n
(cid:80) j(s n(X i,X j) −
1)dT QQ ∗j is also of order O p(1) by an approximation argument. Then the consistency of Q(cid:98)n(x)
implies that a (X ) is of order o (1), which is negligible compared to a (X ).
2 i p 1 i
Further calculation confirms a tractable asymptotic distribution for (cid:80) ∥a (X )∥2, which
1 i
suggests setting H = (−1 (cid:80) dTQj)⊗(−1 (cid:80) dTQj). Finally, since Q∗ is unknown, H is
f n j Q∗ n j Q∗ f
approximated by H(cid:98) ⊗H(cid:98), which gives exactly our test statistic T(cid:98)n in (19).
Remark 10. Petersen et al. (2021) proposed the following test statistic when responses are 1D
densities which has a simpler form compared to ours.
n
(cid:88) (cid:16) (cid:17)
T(cid:98)n,1D = W2 Q(cid:98)n(X i),Q(cid:98)n(X)
i=1
However, their results rely heavily upon the Hilbert embedding of the 1D Wasserstein space and
cannot be expected to generalize to higher dimensions while ours (19) works in any dimension
and is motivated by the Wald statistic for the generalized linear models. Moreover, one can
show that T(cid:98)n and T(cid:98)n,1D are equivalent in the special case of 1D Gaussian distributions. As our
test statistic (19) can again be viewed as a generalization of the numerator of the global F-test
in multiple linear regression, we refer to T(cid:98)n in (19) as the Wasserstein F -statistic following
Petersen et al. (2021).
4.2.2 Theoretical properties
We proceed to discuss the theoretical guarantees of our test statistic T(cid:98)n and the corresponding
test. To begin with, Theorem 11 gives the asymptotic null distribution of T(cid:98)n.
Theorem 11. Suppose Assumption 1-3 and 6 hold. Then under the null (18), the test statistic
T(cid:98)n satisfies
T(cid:98)n →w (cid:88) λ iw
i
(24)
i
where w are i.i.d. χ2 random variables and λ are the eigenvalues of E(TQ −I )⊗(TQ −I ).
i p i Q∗ d Q∗ d
Remark 12. The proof of Theorem 11 relies on the uniform consistency in Theorem 6 since T(cid:98)n
involves estimated predictions Q(cid:98)n(·) at random covariates {X i} i∈[n]. The uniform consistency
is not needed in Petersen et al. (2021) thanks to the fact that W (R) is essentially flat (has
2
zero sectional curvature) which leads to a closed-form expression for T(cid:98)n,1D. Meanwhile, the
Bures-Wasserstein manifold (S++,W) is positively curved when d > 1 (Ambrosio et al., 2005)
d
and no closed-form expression is available for T(cid:98)n, hence we resort to uniform consistency to
ensure the tightness of Taylor approximation. See Appendix F for the proof.
Theorem 11 asserts that T(cid:98)n converges weakly to a weighted sum of χ2 ps with weights de-
termined by the eigenvalues of the covariance operator E(TQ − I ) ⊗ (TQ − I ). To get a
Q∗ d Q∗ d
14corresponding test, note that the asymptotic null distribution in Theorem 11 depends on un-
known parameters, namely the eigenvalues λ , which must be approximated to formulate a
i
rejection region. A natural approach would be to estimate the eigenvalues λ(cid:98)i of the sample
average n1 (cid:80)n i=1T QQ (cid:98)ni
(X)
⊗T QQ (cid:98)ni
(X)
and let q
(cid:98)1−α
be the 1−α quantile of (cid:80) i=1λ(cid:98)iw i. Then we
define our test Φ for any α ∈ (0,1) by
α
(cid:16) (cid:17)
Φ
α
= I T(cid:98)n > q
(cid:98)1−α
(25)
Equipped with Theorem 11, Proposition 13 below demonstrates that Φ has asymptotic
α
size α under the null; see Appendix G for the proof.
Proposition 13. Suppose Assumption 1-3 and 6 hold. Then under the null (18),
(cid:16) (cid:17)
P T(cid:98)n > q
(cid:98)1−α
→ α
as n → ∞.
Finally, letusturntoananalysisofthepowerofthetestΦ underasequenceofcontiguous
α
alternatives. To this end, we denote by P the set of distributions of (X,Q) that satisfy
Assumption 1 - 6,
P := (cid:8)P ∈ P (cid:0)Rp×S+(cid:1) : P satisfies Assumption 1−6(cid:9)
2 d
For any P ∈ P, We measure the deviation of Q∗(x) from being a constant function of x by
Edist2(Q∗(X),Q∗) and choose dist to be either the Wasserstein distance or the one induced by
the Frobenius norm. Then let us define the corresponding alternatives under either distances
as follows.
(cid:110) (cid:111)
H 1,n : P ∈ P F(a n) := P (cid:101) ∈ P : E (X,Q)∼P(cid:101)∥Q∗(X)−Q∗∥2 F ≥ a2 n ,
(cid:110) (cid:111)
H(cid:101)1,n : P ∈ P W(a n) := P (cid:101) ∈ P : E (X,Q)∼P(cid:101)W2(Q∗(X),Q∗) ≥ a2
n
.
Theorem 14 shows that Φ
α
is powerful against both H
1,n
and H(cid:101)1,n whenever a
n
≳ n−(1/2−α2)
for some constant α > 0. The proof is given in Appendix H.
2
Theorem 14. Consider a sequence of alternative hypotheses H with a being a sequence
1,n n
such that a ≳ 1 for some some constant α > 0. Then the worst case power converges
n n1/2−α2 2
uniformly to 1, that is
(cid:16) (cid:17)
inf P T(cid:98)n > q
(cid:98)1−α
→ 1
P∈PF(an)
asn → ∞. ThesameresultalsoholdsforalternativehypothesesH(cid:101)1,n definedbytheWasserstein
distance, that is
(cid:16) (cid:17)
inf P T(cid:98)n > q
(cid:98)1−α
→ 1
P∈PW(an)
as n → ∞
5 Algorithm and Numerical Experiments
In this section, we propose a Riemannian gradient descent algorithm for optimizing (13) in
Section 5.1 and present a series of numerical experiments in Section 5.2 to validate our theoret-
ical results on the central limit theorem (Theorem 7), asymptotic null distribution (Theorem
11) and power (Theorem 14). In Section 5.3, we also run simulations under the setting where
Q is not observed but estimated from data and see how much it deviates from the perfect
observation setting.
155.1 Riemannian gradient descent algorithm
Motivated by the Bures-Wasserstein gradient descent algorithm (Chewi et al., 2020; Altschuler
et al., 2021) for the vanilla Bures-Wasserstein barycenter, we propose a gradient descent algo-
rithm to compute Q(cid:98)n(x) in (13), which is given as Algorithm 1.
Algorithm 1 GD for Fréchet regression
1: Input: predictors {X }n , responses {Q }n , predictor x, learning rate η, initializa-
i i=1 i i=1
tion S , maximum number of iterations T.
0
2: Initialize S ← S .
0
3: for t = 1,...,T do
4: Set
n
1 (cid:88)
G ← I +η · s (x,X )(TQi −I ) (26)
d n n i S d
i=1
5: Set
S ← GSG (27)
6: Output: S.
Algorithm 1 can be viewed as a Riemannian gradient descent algorithm (see Appendix A
and Panaretos and Zemel (2016); Chewi et al. (2020); Altschuler et al. (2021)). Intuitively,
−1 (cid:80)n s (x,X )(TQi −I ) is the derivative of the objective function F (x,S) in (13) in the
n i=1 n i S d n
tangent space (Ambrosio et al., 2005, Corollary 10.2.7) and (26) corresponds to one gradient
step in the tangent space with step size η. Then (27) is mapping the gradient step in the
tangent space back to S++ through the exponential map (Appendix A). In terms of the step
d
size η, it is justified in Panaretos and Zemel (2016) that we can simply take η = 1 in practice
for computing the Wasserstein barycenter. In all our numerical experiments displayed here,
taking η = 1 with initialization S = I again leads to fast convergence within a few steps.
0 d
Hence we set η = 1 and S = I throughout.
0 d
5.2 Simulation setup and results
Tovalidateourtheoryanddemonstratethepracticalapplicabilityofourinferentialprocedures,
weperformaseriesofnumericalexperiments. Letusbeginwitharunningexamplethatfollows
the Fréchet regression model.
Example 1. Let X ∼ Uniform[−1,1]. The response Q ∈ Rd×d is generated as Q = (2+δ ·
x)2V2. Here δ ∈ [−1,1] is parameter that measures how much the model deviates from the null
(18), V = U(I +M)2U⊤ where U follows the Haar measure over the orthogonal group O , M
d d
is diagonal with M ∼ Uniform[−0.5,0.5] and X,U,M are independent. Then one can check
ii
that (X,Q) satisfies the Fréchet regression model with Q∗(x) = (2+δ·x)2I .
d
With Example 1 in hand, we proceed to check the validity of the central limit theorem
for Q(cid:98)n(x) in Theorem 7. To this end, random predictor-response pairs (X,Q) are generated
according to Example 1 with d = 40 and δ = 1. For each trial, n = 200 samples of (X ,Q ) are
i i
generated as above, and we get the Fréchet regression estimate Q(cid:98)n(x) via Algorithm 1. Then
16we compute
√ (cid:104) (cid:105)
n Q(cid:98)n(x)−Q∗(x)
ij
Q(cid:101)ij(x) = √
v
ij
√
where v
ij
denotes the asymptotic variance of the (i,j)-entry of n(Q(cid:98)n(x)−Q∗(x)) as specified
in Theorem 7 and (17). Figure 1 displays the Q-Q (quantile-quantile) plots of Q(cid:101)ij(x) vs. the
standard normal distribution over 200 Monte Carlo trials at x = 0. As stated in Theorem 7,
Q(cid:101)ij(x) asymptotically follows a standard normal distribution, resulting in a linear fit with unit
slopeandzerointerceptintheQ-Qplot. ItisevidentfromFigure1thattheempiricalquantiles
of Q(cid:101)ij(0) matches the quantiles of N(0,1) reasonably well, thereby validating Theorem 7.
(1,1) (2,1)
1.0
3
3
2 0.8
2
1
0.6 1
0
0
0.4
1
1
0.2
2 2
0.03 3
0.03 2 1 0.2 0 1 02.4 3 3 0.62 1 0 0.8 1 2 31.0
Quantiles of the Standard normal distribution
Figure 1: Simulation results showing the Q-Q plots of Q(cid:101) (0) and Q(cid:101) (0) with parameters d =
11 21
40,δ = 1,n = 200.
Next, we turn to Theorem 11, namely the asymptotic null distribution of the test statistic
T(cid:98)n. To this end, we set d ∈ {10,30} and δ = 0 in Example 1. Figure 2a and 2b display the
Q-Q plot of T(cid:98)n vs. (cid:80) iλ iw i. The empirical quantiles of T(cid:98)n are generated from 200 Monte Carlo
trials with n = 200 in each trial. To approximate the population quantiles of (cid:80) λ w , we first
i i i
run a separate trial with N = 2000 samples of (X i,Q i) and compute the eigenvalues {λ(cid:98)j}
j∈[d2]
of
N
1 (cid:88)(cid:16)
TQi −I
(cid:17) ⊗(cid:16)
TQi −I
(cid:17)
d d
N Q(cid:98)N(X) Q(cid:98)N(X)
i=1
to approximate the population eigenvalues {λ }. Then we generate N = 2000 samples of
i
(cid:80) iλ(cid:98)iw
i
which we use the approximate the population quantiles of (cid:80) iλ iw i. Figure 2a and
2b show that the obtained Q-Q plot has a linear fit approximately equal to y = x, validating
Theorem 11.
Then, to validate the power of the proposed test in Theorem 14, we set d = 10 and vary
δ ∈ {0,0.02,0.04,0.06,0.08,0.10,0.12}inExample1. Whentheparameterδ = 0,thegenerated
model satisfies the null hypothesis Q∗(x) ≡ 4I . As δ increases, the generated model deviates
d
17
)0(Q
fo
seirtne
fo
selitnauq
laciripmE4.5
1.8
4.0
1.6
1.4 3.5
1.2
3.0
1.0
2.5
0.8
0.6 2.0
0.4
1.5
0.4 0.6 0.8 1.0 1.2 1.4 1.6 2.0 2.2 2.4 2.6 2.8 3.0 3.2
Empirical quantiles of the null distribution Empirical quantiles of the null distribution
(a) d = 10 (b) d = 30
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0.00 0.02 0.04 0.06 0.08 0.10 0.12
(c) d = 10
Figure 2: Simulation results. (a) and (b): Q-Q plots of the test statistic T(cid:98) against the asymptotic
n
null distribution with n = 200 for different dimension of d = 10 and d = 30; (c): power curve as a
function of the effect size δ.
more and more from being a constant function of x. Figure 2c displays the power of our test
over 200 Monte Carlo simulations. It is shown that the power quickly goes from 0.06 up to 1
as δ increases from 0 to 1, demonstrating proper size (Proposistion 13) and power (Theorem
14).
5.3 Sensitivity of the results when covariance matrices are un-
known
Finally, we investigate the numerical performance of our results under the setting where Q is
not directly observed but estimated from the data. Specifically, consider the case when we only
18
llun
eht
rednu
n
fo
selitnauq
laciripmE
rewoP
llun
eht
rednu
n
fo
selitnauq
laciripmEhave access to {(X ;Z ,Z ,...,Z )} with Z ,Z ,...,Z i. ∼i.d. N(0,Q ). Then a natural
i i1 i2 in i∈[n] i1 i2 in i
(cid:101) (cid:101)
plug-in approach for Fréchet estimation and testing is to estimate Q via the sample covariance
i
Q := n−1(cid:80)n (cid:101) Z Z⊤, and plug in Q for Q in downstream estimators. Specifically, define
i (cid:101) j=1 i1 i1 i i
the estimator Q(cid:98)n,n(x) and test statistic T(cid:98)n,n as follows.
(cid:101) (cid:101)
n
1 (cid:88)
Q(cid:98)n,n(x) = argmin s n(x,X i)W2(S,Q i)
(cid:101) n
S∈S d++ i=1
n n
T(cid:98)n,n
(cid:101)
= (cid:88) i=1(cid:13) (cid:13) (cid:13)H(cid:98)n
(cid:101)
·(cid:16) Q(cid:98)n,n (cid:101)(X i)−Q(cid:98)n,n (cid:101)(X)(cid:17)(cid:13) (cid:13) (cid:13)2
F
, where H(cid:98)n
(cid:101)
= − n1 (cid:88) i=1dT QQ (cid:98)ni
,n(cid:101)(X)
First, we consider the central limit theorem (Theorem 7) under the estimated covariance
matrices. To this end, let us define
√ (cid:104) (cid:105)
(cid:104) (cid:105)
n Q(cid:98)n,n (cid:101)(x)−Q∗(x)
ij
Q(cid:101)n,n(x) := √
(cid:101) i,j v ij
(cid:104) (cid:105)
and study how far Q(cid:101)n,n(x) deviates from the standard normal distribution for finite n (cid:101).
(cid:101) i,j
Under the same model for (X,Q) as Figure 1, the Q-Q plots of [Q(cid:101)n,n(x)]
ij
against the standard
(cid:101)
normal distribution with n ∈ {50,100,200,400,∞} is displayed in Figure 3. Here n = ∞
(cid:101) (cid:101)
denotes the full observation case when Q is known, and is represented as the blue points and
i
line as a reference, which is the same as in Figure 1. In general, the Q-Q plot for finite n
(cid:101)
approaches that of n = ∞ as n → ∞, and can still be fitted by a straight line with slope
(cid:101) (cid:101)
slightly greater than 1, indicating a central limit theorem for the entries with larger variance
for smaller n. Also interestingly, detailed behavior of the bias differs between diagonal and
(cid:101)
off-diagonal entries. For off-diagonal entries, Figure 3b suggests that [Q(cid:98)n,n(0)]
ij
is unbiased
(cid:101)
while for diagonal entries, Figure 3a shows that the Q-Q plots for finite n are negatively
(cid:101)
shifted compared to that of n
(cid:101)
= ∞ (blue), suggesting that [Q(cid:98)n,n(0)]
ii
is negatively biased. We
(cid:101)
only focus on the theoretical properties of the Fréchet regression estimator (13) under the full
observation setting in this paper and leave theoretical investigation of its refined behavior in
the setting of estimated covariance matrices for future work.
Next,letusmoveontotheasymptoticnulldistributionofT(cid:98)n,n (Theorem11)withestimated
(cid:101)
covariance matrices in Figure 4a. For d = 10, under the same model for (X,Q) as Figure 2a,
the Q-Q plots of T(cid:98)n,n against the asymptotic null distribution specified in Theorem 11 with
(cid:101)
n ∈ {50,100,200,400,∞} is shown. Interestingly, the Q-Q plot for finite n can again be fitted
(cid:101) (cid:101)
by a straight line across origin with slope greater than 1, suggesting that the asymptotic null
distribution is a scaled version of (24) in Theorem 11. This would imply inflated false positives
for finite n.
(cid:101)
Finally, Figure 4b shows the Q-Q plots for n ∈ {100,200,1000,∞} under the same model
(cid:101)
as Figure 2c. The behavior at δ = 0 exhibits inflated false positives for finite n, as suggested
(cid:101)
by Figure 4a.
19(1,1) (2,2) (3,3)
1.0 3
3 3
2
2 2
0.8
1 1 1
0 0 0
0.6
1 1 1
0.42 2 2 n=
3 3 3 n= 400
0.24 4 4 n= 200
n= 100
5 5 5 n= 50
0.06 6 6
0.03 2 1 0 0.21 2 3 3 0.24 1 0 1 0.26 3 3 2 10.8 0 1 2 31.0
Quantiles of the Standard normal distribution
(a) Diagonal entries: (1,1),(2,2),(3,3)
(2,1) (3,1) (3,2)
1.04
4 4
3 3 3
0.8
2 2
2
1
0.61 1
0
0 0 1
0.41
1
2 n=
n= 400
3 2 2 n= 200
0.2 4
n= 100
3 3 5 n= 50
0.04 4 6
0.03 2 1 0 0.21 2 3 3 0.24 1 0 1 0.26 3 3 2 10.8 0 1 2 31.0
Quantiles of the Standard normal distribution
(b) Off-diagonal entries: (2,1),(3,1),(3,2)
Figure 3: Simulation results showing the Q-Q plots of diagonal and off-diagonal entries of Q(cid:101)(0)
with parameters d = 10,n = 200 and varying n ∈ {50,100,200,400,∞}.
(cid:101)
20
)0(Q
fo
seirtne
fo
selitnauq
laciripmE
)0(Q
fo
seirtne
fo
selitnauq
laciripmE2.00 n= 1.0 n=
n=1000 n= 1000
n=200 n= 200
1.75 n=100 n= 100
0.8
1.50
0.6
1.25
1.00
0.4
0.75
0.2
0.50
0.4 0.6 0.8 1.0 1.2 1.4 1.6 0.00 0.02 0.04 0.06 0.08 0.10 0.12
Quantiles of the null distribution
(a) Null (b) Power
Figure 4: Simulation results. (a) Q-Q plot of the test statistic T(cid:98) under the null against the
n,n˜
asymptotic null distribution with d = 10, n = 200 for different n˜ ∈ {50,100,200,∞}. (b) Power
curve as a function of the effect size δ for n ∈ {100,200,1000,∞}
(cid:101)
21
llun
eht
rednu
n,n
fo
selitnauq
laciripmE
rewoP6 Application to Single-cell Gene Co-expression Net-
works
Aging is a complex process of accumulation of molecular, cellular, and organ damage, leading
tolossoffunctionandincreasedvulnerabilitytodiseaseanddeath. Nutrient-sensingpathways,
namely insulin/insulin-like growth factor signaling and target-of- rapamycin can substantially
increase healthy life span of laboratory model organisms (Davinelli et al., 2012; de Lucia et al.,
2020). These nutrient signaling pathways are conserved in various organisms. We are inter-
ested in understanding the co-expression structure of 61 genes in this KEGG nutrient-sensing
pathways based on the recently published population scale single cell RNA-seq data of hu-
man peripheral blood mononuclear cells (PBMCs) from blood samples of over 982 healthy
individuals with ages ranging from 20 to 90 (Yazar et al., 2022).
We focus our analysis on CD4+ naive and central memory T (CD4NC) cells, which is the
most common cell type observed in the data. Age-associated changes in CD4 T-cell function-
ality have been linked to chronic inflammation and decreased immunity (Elyahu et al., 2019).
There are a total of 51 genes that are expressed in this cell type. Even though the Fréchet
regression still makes sense when the covariance matrix is potentially degenerate (see the re-
marks after Example 1), our theory relies on the strict positive definiteness. Hence, we retain
only the genes that have nonzero variances at any age, resulting in a total of 37 genes, see
Figure 5 for a concise overview of these covariance matrices for individuals at different ages,
showing difference across different ages.
Ingeneticsresearch,suchcovariancematricesrepresentindividual-specificgeneco-expression
neworks. Weareinterestedintestingwhethersuchnetworksareassociatedwithagesbytesting
whether there is an age effect on the gene expression covariance matrices, i.e. H : Q∗(t) ≡ Q∗
0
for some Q∗. The test we propose has a p-value 0.00019, suggesting a strong age effect on the
gene expression covariance matrices.
2220 30 40
0 0 0
5 5 5
10 10 10
15 15 15
20 20 20
25 25 25
30 30 30
35 35 35
0 5 10 15 20 25 30 35 0 5 10 15 20 25 30 35 0 5 10 15 20 25 30 35
50 60 70
0 0 0
5 5 5
10 10 10
15 15 15
20 20 20
25 25 25
30 30 30
35 35 35
0 5 10 15 20 25 30 35 0 5 10 15 20 25 30 35 0 5 10 15 20 25 30 35
80 90
0 0
2.0
5 5
1.5
10 10
1.0
15 15
0.5
20 20
0.0
25 25
0.5
30 30
35 35 1.0
0 5 10 15 20 25 30 35 0 5 10 15 20 25 30 35
Figure 5: Heatmap of the gene expression covariance matrices with diagonal elements omitted for
individual at age 20,30,...,90, respectively.
7 Discussion
We have develop methods for statistical inference for the Fréchet regression on the Bures-
Wassersteinmanifold,wherecovariancematrixistreatedastheoutcome,includingtheuniform
rate of concergence of the conditional Fréchet mean and the asymptotic distribution. Based on
these reuslts, we have further developed statistical test for testing the association between co-
variate outcome and Eucledean covariates. These results are further verified using simulations.
We have demonstrated the methods by testing the association between gene co-expression and
age, indicating the change of co-expressions among a set of genes in nutrient sensing pathway.
23The proposed methods have other applications, including in neuroimaing data analysis, where
covariancematrices(orcorrelationmatricesafterstandardization)ofmultiplebrainregionsare
used to summarize as functional connectivity matrices. The proposed methods can be used to
identify the factors that are associated with such functional connectivity matrices.
In this paper, we assume that the outcome covariance matrices are observed and we only
focus on the theoretical properties of the Fréchet regression estimator (13) under the full obser-
vation setting. This is also the setting considered in Petersen and Müller (2019) and Petersen
et al. (2021). An important future work is to develop the corresponding theoretical results in
the setting when one has to estimate the covariance matrices from the data.
References
Abadie, A. and Imbens, G. W. (2006). Large Sample Properties of Matching Estimators for
Average Treatment Effects. Econometrica, 74(1):235–267.
Agueh, M. and Carlier, G. (2011). Barycenters in the Wasserstein Space. SIAM Journal on
Mathematical Analysis, 43(2):904–924.
Agueh, M. and Carlier, G. (2017). Vers un théorème de la limite centrale dans l’espace de
Wasserstein? Comptes Rendus. Mathématique, 355(7):812–818.
Altschuler, J., Chewi, S., Gerber, P. R., and Stromme, A. J. (2021). Averaging on the Bures-
Wasserstein manifold: dimension-free convergence of gradient descent. Advances in neural
information processing systems, 34:22132–22145.
Altschuler, J., Niles-Weed, J., and Rigollet, P. (2017). Near-linear time approximation algo-
rithms for optimal transport via Sinkhorn iteration. Advances in neural information pro-
cessing systems, 30.
Altschuler, J. M., Niles-Weed, J., and Stromme, A. J. (2022). Asymptotics for Semidiscrete
Entropic Optimal Transport. SIAM Journal on Mathematical Analysis, 54(2):1718–1741.
Ambrosio, L., Gigli, N., and Savaré, G. (2005). Gradient Flows In Metric Spaces and in the
Space of Probability Measures. Birkhäuser-Verlag.
Arjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein Generative Adversarial Net-
works. In Proceedings of the 34th International Conference on Machine Learning, pages
214–223. PMLR.
Arnaudon, M., Barbaresco, F., and Yang, L. (2013). Riemannian medians and means with
applicationstoradarsignalprocessing. IEEEJournalofSelectedTopicsinSignalProcessing,
7(4):595–604.
Barrio, E. d. and Loubes, J.-M. (2019). Central limit theorems for empirical transportation
cost in general dimension. The Annals of Probability, 47(2):926–951.
Bhatia, R., Jain, T., and Lim, Y. (2019). On the Bures–Wasserstein distance between positive
definite matrices. Expositiones Mathematicae, 37(2):165–191.
Bhattacharya, R. and Patrangenaru, V. (2003). Large sample theory of intrinsic and extrinsic
sample means on manifolds. The Annals of Statistics, 31(1):1–29.
24Bhattacharya, R. and Patrangenaru, V. (2005). Large sample theory of intrinsic and extrinsic
sample means on manifolds—II. The Annals of Statistics, 33(3):1225–1259.
Bigot, J., Gouet, R., Klein, T., and López, A. (2017). Geodesic PCA in the Wasserstein space
by convex PCA. Annales de l’Institut Henri Poincaré, Probabilités et Statistiques, 53(1):1 –
26.
Bunne, C., Hsieh, Y.-P., Cuturi, M., and Krause, A. (2023a). The Schrödinger Bridge between
Gaussian Measures has a Closed Form. In Proceedings of The 26th International Conference
on Artificial Intelligence and Statistics, pages 5802–5833. PMLR.
Bunne, C., Papaxanthos, L., Krause, A., and Cuturi, M. (2022). Proximal Optimal Transport
Modeling of Population Dynamics. In Proceedings of The 25th International Conference on
Artificial Intelligence and Statistics, pages 6511–6528. PMLR.
Bunne, C., Stark, S. G., Gut, G., del Castillo, J. S., Levesque, M., Lehmann, K.-V., Pelkmans,
L., Krause, A., and Rätsch, G. (2023b). Learning single-cell perturbation responses using
neural optimal transport. Nature Methods, 20(11):1759–1768.
Bures, D. (1969). An extension of Kakutani’s theorem on infinite product measures to the
tensorproductofsemifinitew*-algebras.TransactionsoftheAmericanMathematicalSociety,
135(0):199–212.
Cai, T. T., Zhang, C.-H., and Zhou, H. H. (2010). Optimal rates of convergence for covari-
ance matrix estimation. The Annals of Statistics, 38(4):2118–2144. Publisher: Institute of
Mathematical Statistics.
Carlsson, M. (2018). Perturbation theory for the matrix square root and matrix modulus.
arXiv:1810.01464.
Caseiro, R., Martins, P., Henriques, J.F., andBatista, J.(2012). AnonparametricRiemannian
framework on tensor field with application to foreground segmentation. Pattern Recognition,
45(11):3997–4017.
Chen, Y., Lin, Z., and Müller, H.-G. (2023). Wasserstein Regression. Journal of the American
Statistical Association, 118(542):869–882.
Chen, Y. and Müller, H.-G. (2022). Uniform convergence of local Fréchet regression with
applications to locating extrema and time warping for metric space valued trajectories. The
Annals of Statistics, 50(3):1573–1592.
Chewi, S., Maunu, T., Rigollet, P., and Stromme, A. J. (2020). Gradient descent algorithms
for Bures-Wasserstein barycenters. In Proceedings of Thirty Third Conference on Learning
Theory, pages 1276–1304. PMLR.
Chiu, T. Y. M., Leonard, T., and Tsui, K.-W. (1996). The Matrix-Logarithmic Covariance
Model. Journal of the American Statistical Association, 91(433):198–210.
Cornea, E., Zhu, H., Kim, P., and Ibrahim, J. G. (2017). Regression models on Riemannian
symmetric spaces. Journal of the Royal Statistical Society Series B: Statistical Methodology,
79(2):463–482.
Cuturi, M. (2013). Sinkhorn Distances: Lightspeed Computation of Optimal Transport. In
Burges, C. J., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K. Q., editors,
Advances in Neural Information Processing Systems, volume 26.
25Davinelli, S., Willcox, D., and Scapagnini, G. (2012). Extending healthy ageing: nutrient
sensitive pathway and centenarian population. Immunity & Ageing, 9:9.
de Lucia, C., Murphy, T., Steves, C., and et al. (2020). Lifestyle mediates the role of nutrient-
sensing pathways in cognitive aging: cellular and epidemiological evidence. Communications
Biology, 3:157.
Deb, N. and Sen, B. (2023). Multivariate Rank-Based Distribution-Free Nonparametric
Testing Using Measure Transportation. Journal of the American Statistical Association,
118(541):192–207.
del Barrio, E., Sanz, A. G., Loubes, J.-M., and Niles-Weed, J. (2023). An Improved Cen-
tral Limit Theorem and Fast Convergence Rates for Entropic Transportation Costs. SIAM
Journal on Mathematics of Data Science, 5(3):639–669.
Del Moral, P. and Niclas, A. (2018). A Taylor expansion of the square root matrix function.
Journal of Mathematical Analysis and Applications, 465(1):259–266.
Dryden, I. L., Koloydenko, A., and Zhou, D. (2009). Non-Euclidean statistics for covariance
matrices, with applications to diffusion tensor imaging. The Annals of Applied Statistics,
3(3):1102–1123.
Dudley, R. M. and Norvaiša, R. (2011). Concrete Functional Calculus. Springer, New York,
NY.
Elyahu, Y., Hekselman, I., Eizenberg-Magar, I., Berner, O., Strominger, I., Schiller, M., Mittal,
K., Nemirovsky, A., Eremenko, E., Vital, A., Simonovsky, E., Chalifa-Caspi, V., Friedman,
N., Yeger-Lotem, E., and Monsonego, A. (2019). Aging promotes reorganization of the CD4
T cell landscape toward extreme regulatory and effector phenotypes. Science Advances,
5(8):eaaw8330.
Fillard, P., Arsigny, V., Pennec, X., Hayashi, K. M., Thompson, P. M., and Ayache, N. (2007).
Measuring brain variability by extrapolating sparse tensor fields measured on sulcal lines.
Neuroimage, 34(2):639–650.
Fletcher, P. and Joshi, S. (2007). Riemannian geometry for the statistical analysis of diffusion
tensor data. Signal Processing.
Friston, K. J. (2011). Functional and effective connectivity: a review. Brain Connectivity,
1(1):13–36.
Gonzalez-Sanz, A., Loubes, J.-M., and Niles-Weed, J. (2022). Weak limits of entropy regular-
ized Optimal Transport; potentials, plans and divergences. arXiv:2207.07427.
Hallin, M., Mordant, G., and Segers, J. (2021). Multivariate goodness-of-fit tests based on
Wasserstein distance. Electronic Journal of Statistics, 15(1):1328–1371.
Hoff, P.D.andNiu, X.(2012). Acovarianceregressionmodel. Statistica Sinica, pages729–753.
Hsing, T. and Eubank, R. (2015). Theoretical foundations of functional data analysis, with an
introduction to linear operators, volume 997. John Wiley & Sons.
Hütter, J.-C. and Rigollet, P. (2021). Minimax estimation of smooth optimal transport maps.
The Annals of Statistics, 49(2):1166–1194.
26Jin, C., Netrapalli, P., Ge, R., Kakade, S. M., and Jordan, M. I. (2019). A Short Note on
Concentration Inequalities for Random Vectors with SubGaussian Norm. arXiv:1902.03736.
Kim,Y.-H.andPass,B.(2017). WassersteinbarycentersoverRiemannianmanifolds. Advances
in Mathematics, 307:640–683.
Kroshnin, A., Spokoiny, V., and Suvorikova, A. (2021). Statistical inference for Bures–
Wasserstein barycenters. The Annals of Applied Probability, 31(3):1264–1298.
Kuchibhotla, A. K. and Chakrabortty, A. (2022). Moving beyond sub-Gaussianity in high-
dimensional statistics: Applications in covariance estimation and linear regression. Informa-
tion and Inference: A Journal of the IMA, 11(4):1389–1456.
Lang, S. (1999). Fundamentals of Differential Geometry, volume 191 of Graduate Texts in
Mathematics. Springer, New York, NY.
Le Gouic, T. and Loubes, J.-M. (2017). Existence and consistency of Wasserstein barycenters.
Probability Theory and Related Fields, 168(3-4):901–917.
Le Gouic, T., Paris, Q., Rigollet, P., and Stromme, A. J. (2022). Fast convergence of empir-
ical barycenters in Alexandrov spaces and the Wasserstein space. Journal of the European
Mathematical Society, 25(6):2229–2250.
Lin, Z. (2019). Riemannian Geometry of Symmetric Positive Definite Matrices via Cholesky
Decomposition. SIAM Journal on Matrix Analysis and Applications, 40(4):1353–1370.
Lin, Z., Müller, H.-G., and Park, B. U. (2023). Additive models for symmetric positive-definite
matrices and Lie groups. Biometrika, 110(2):361–379.
Manole, T., Balakrishnan, S., Niles-Weed, J., and Wasserman, L. (2022). Plugin Estimation of
Smooth Optimal Transport Maps. arXiv:2107.12364.
Manole, T., Balakrishnan, S., Niles-Weed, J., and Wasserman, L. (2023). Central Limit Theo-
rems for Smooth Optimal Transport Maps. arXiv:2312.12407.
Mena, G. and Niles-Weed, J. (2019). Statistical bounds for entropic optimal transport: sample
complexityandthecentrallimittheorem.Advancesinneuralinformationprocessingsystems,
32.
Moakher, M. (2005). A Differential Geometric Approach to the Geometric Mean of Symmetric
Positive-Definite Matrices. SIAM Journal on Matrix Analysis and Applications, 26(3):735–
747.
Ohta, S.-I. (2012). Barycenters in Alexandrov spaces of curvature bounded below. Advances
in Geometry, 12(4):571–587.
Panaretos, V. M. and Zemel, Y. (2016). Amplitude and phase variation of point processes. The
Annals of Statistics, 44(2):771–812.
Panaretos, V. M. and Zemel, Y. (2020). An Invitation to Statistics in Wasserstein Space.
Springer Nature.
Petersen, A., Liu, X., and Divani, A. A. (2021). Wasserstein F-tests and confidence bands for
the Fréchet regression of density response curves. The Annals of Statistics, 49(1):590–611.
27Petersen, A. and Müller, H.-G. (2019). Fréchet regression for random objects with Euclidean
predictors. The Annals of Statistics, 47(2):691–719.
Peyré, G. and Cuturi, M. (2019). Computational Optimal Transport: With Applications to
Data Science. Foundations and Trends® in Machine Learning, 11(5-6):355–607.
Pooladian, A.-A., Divol, V., and Niles-Weed, J. (2023). Minimax estimation of discontinuous
optimal transport maps: The semi-discrete case. In International Conference on Machine
Learning, pages 28128–28150. PMLR.
Pooladian, A.-A. and Niles-Weed, J. (2022). Entropic estimation of optimal transport maps.
arXiv:2109.12004.
Redko, I., Habrard, A., and Sebban, M. (2017). Theoretical Analysis of Domain Adaptation
with Optimal Transport. In Ceci, M., Hollmén, J., Todorovski, L., Vens, C., and Džeroski,
S., editors, Machine Learning and Knowledge Discovery in Databases, pages 737–753.
Santambrogio, F. (2015). Optimal Transport for Applied Mathematicians: Calculus of Varia-
tions, PDEs, and Modeling, volume 87. Springer, NY.
Schiebinger, G., Shu, J., Tabaka, M., Cleary, B., Subramanian, V., Solomon, A., Gould, J.,
Liu, S., Lin, S., Berube, P., Lee, L., Chen, J., Brumbaugh, J., Rigollet, P., Hochedlinger, K.,
Jaenisch, R., Regev, A., andLander, E.S.(2019). Optimal-TransportAnalysisofSingle-Cell
GeneExpressionIdentifiesDevelopmentalTrajectoriesinReprogramming. Cell, 176(4):928–
943.
Somnath, V. R., Pariset, M., Hsieh, Y.-P., Martinez, M. R., Krause, A., and Bunne, C. (2023).
Aligned Diffusion Schrödinger Bridges. In Proceedings of the Thirty-Ninth Conference on
Uncertainty in Artificial Intelligence, volume 216, pages 1985–1995. PMLR.
Talagrand, M. (1989). Isoperimetry and Integrability of the Sum of Independent Banach-Space
Valued Random Variables. The Annals of Probability, 17(4):1546–1570.
Van Der Vaart, A. W. and Wellner, J. A. (1996). Weak Convergence and Empirical Processes.
Springer Series in Statistics. Springer, New York, NY.
Vershynin,R.(2018). High-Dimensional Probability: An Introduction with Applications in Data
Science. Cambridge University Press, Cambridge.
Villani, C. (2003). Topics in Optimal Transportation. American Mathematical Society.
Villani, C. (2009). Optimal Transport: Old and New, volume 338. Springer, Berlin, Heidelberg.
Wainwright, M. J. (2019). High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cam-
bridge University Press.
Wasserman, L. (2006). All of Nonparametric Statistics. Springer Texts in Statistics. Springer,
New York, NY.
Weed, J. and Berthet, Q. (2019). Estimation of smooth densities in Wasserstein distance. In
conference on Learning Theory, pages 3118–3119. PMLR.
Wihler,T.P.(2009). OntheHöldercontinuityofmatrixfunctionsfornormalmatrices. Journal
of inequalities in pure and applied mathematics, 10(10).
28Yazar, S., Alquicira-Hernandez, J., Wing, K., Senabouth, A., Gordon, M.G., Andersen, S., Lu,
Q., Rowson, A., Taylor, T. R. P., Clarke, L., Maccora, K., Chen, C., Cook, A. L., Ye, C. J.,
Fairfax, K. A., Hewitt, A. W., and Powell, J. E. (2022). Single-cell eqtl mapping identifies
cell type–specific genetic control of autoimmune disease. Science, 376(6589):eabf3041.
Yokota, T. (2016). Convex functions and barycenter on CAT (1)-spaces of small radii. Journal
of the Mathematical Society of Japan, 68(3):1297–1323.
Yuan, Y., Zhu, H., Lin, W., and Marron, J. S. (2012). Local polynomial regression for sym-
metricpositivedefinitematrices. Journal of the Royal Statistical Society Series B: Statistical
Methodology, 74(4):697–719.
Zhao, Y., Wang, B., Mostofsky, S. H., Caffo, B. S., and Luo, X. (2021). Covariate assisted
principal regression for covariance matrix outcomes. Biostatistics, 22(3):629–645.
Zou, T., Lan, W., Wang, H., and Tsai, C.-L. (2017). Covariance Regression Analysis. Journal
of the American Statistical Association, 112(517):266–281.
29A Background on optimal transport and functional
calculus
In this section, we collect relevant background about optimal transport and functional calculus
to make the paper more self-contained.
A.1 Geometry of optimal transport
We begin with the geometry of optimal transport, and then specialize the general concepts to
theBures–Wassersteinmanifold. Forintroductoryexpositionsofoptimaltransport, wereferto
Villani (2003); Santambrogio (2015); Panaretos and Zemel (2020). For a more comprehensive
treatment, we refer to Ambrosio et al. (2005); Villani (2009).
Given µ ,µ ∈ P (Rd), the constant-speed geodesic (µ ) connecting µ to µ is
0 1 2,ac t t∈[0,1] 0 1
characterized by
µ = (cid:2) id+t(cid:0) Tµ1 −id(cid:1)(cid:3) µ , t ∈ [0,1]
t µ0 # 0
Here, # denotes the pushforward operation defined by T µ(E) = µ(T−1(E)) for any Borel set
#
E ⊂ Rd. Then define the tangent vector of the geodesic (µ ) at t = 0 to be the mapping
t t∈[0,1]
Tµ1−id. The tangent space T P (Rd) to P (Rd) at µ is defined in Ambrosio et al., 2005,
µ0 µ0 2,ac 2,ac 0
Thm 8.5.1 as
T P (Rd) := (cid:8) λ(Tν −id) : λ > 0,ν ∈ P (Rd)(cid:9)L2(µ0)
µ0 2,ac µ0 2,ac
Here the overline denotes closure with respect to the L2(µ ) measure.
0
Given two covariance matrices Q,S ∈ S++, the constant-speed geodesic connecting the
d
corresponding centered Gaussians is given by
(cid:0) I +t(cid:0) TS −I (cid:1)(cid:1) Q(cid:0) I +t(cid:0) TS −I (cid:1)(cid:1) , t ∈ [0,1]
d Q d d Q d
The tangent space T S++ can be identified with the space S of symmetric d×d matrices.
Q d d
For any S(cid:101)∈ T QS d++, its norm in the tangent space is given by
(cid:13) (cid:13) (cid:68) (cid:69)1/2
(cid:13)S(cid:101)(cid:13) := S(cid:101),QS(cid:101)
(cid:13) (cid:13)
Q
A.2 Functional calculus
To consider higher order differentials of TS on the Bures-Wasserstein manifold, we give a brief
Q
review of some key concepts in functional calculus that are essential for the development, and
direct readers to Dudley and Norvaiša (2011) for further details.
Let Y and Z be normed spaces with the norm on each denoted by ∥·∥, and let U be an open
subset of Y. Let L(Y,Z) denote the space of all bounded linear operators from Y into Z. A
function ϕ : U → Z is called (Fréchet) differentiable at u ∈ U if there exists an L(u) ∈ L(Y,Z)
such that for each y ∈ Y with u+Y ∈ Y,
∥ϕ(u+y)−ϕ(u)−L(u)y∥
lim = 0
y→0 ∥y∥
The linear operator L(u) is unique, is called the derivative of ϕ at u, and is denoted by Dϕ(u).
If the function ϕ is differentiable at each u ∈ U, then we say that ϕ is differentiable on U.
To consider higher order derivatives of ϕ, let L1(Y,Z) := L(Y,Z) with the usual operator
norm, andletLk+1(Y,Z) := L(Y,Lk(Y,Z))withtheoperatornormrecursivelyfork = 1,2,....
For k ≥ 2, we say that ϕ is (Fréchet) differentiable of order k at u if ϕ has a (k−1)st derivative
30Dk−1ϕ(y) at each point y of some neighborhood of u, and the mapping Dk−1ϕ is differentiable
at u. Then Dkϕ(u), the kth derivative of ϕ at u, is defined as the derivative of Dk−1ϕ at u. If
ϕ is differentiable of order k at u for each u ∈ U then we say that ϕ is differentiable of order k
on U. A mapping ϕ is called a Ck function on U ⊂ Y if the derivatives of ϕ through order k
all exist on U and are continuous. A mapping ϕ is called a C∞ function on U ⊂ X if it is a
Ck function for each k.
It would be convenient to introduce the notion of multilinear mappings in order study the
properties of higher order derivatives Dkϕ. A function A : Yk → Z is called k-linear if for each
j ∈ [k], A(y ,...,y ) is linear in y for any fixed values of y , i ̸= j. The function A is called
1 k j i
bounded if
∥A∥ := sup{∥A(y ,...,y )∥ : ∥y ∥ ≤ 1,j ∈ [k]} < ∞ (28)
1 k j
Let M (Y,Z) be the set of all bounded k-linear maps from Yk to Z with norm define by (28).
k
The space Lk(Y,Z) can be identified with M (Y,Z) through the natural isomorphism
k
Φ : Lk(Y,Z) → M (Y,Z) defined by
(k) k
Φ (A)(y ,...,y ) := A(y )(y )···(y )
(k) 1 k 1 2 k
:= [···[A(y )](y )···](y )
1 2 k
Then the kth differential at u is defined by dkϕ(u) := Φ (Dkϕ(u)).
(k)
The kth diffential dkϕ(u) is symmetric in the sense that
dkϕ(u)(y ,...,y ) = dkϕ(u)(y ,...,y )
σ(1) σ(k) 1 k
for any permutation σ of [k]. For simplicity, we also denote dkϕ(u)(y,...,y) by dkϕ(u)·y⊗k.
LetM (Y,Z)denotethesubspaceofallsymmetricelementsofM (Y,Z). Thenasidefrom
k,s k
the operator norm inherited from M (Y,Z), one can define another norm |||·||| on M (Y,Z)
k k,s
by
|||P||| := sup{∥P(y,y,...,y)∥ : ∥y∥ ≤ 1}
Properties of the high order differentials dkTS are investigated in Appendix B.1.
Q
B Technical lemmas
In this section, we collect technical lemmas and relevant notations for the proof. First, proper-
ties of differentials of TS are collected in Appendix B.1. Then various concentration results are
Q
given in Appendix B.2. Finally, properties of F(·,·) and Q∗(·) are summarized in Appendix
B.3.
B.1 Differentials of optimal transport maps
Recall that the optimal transport map between two centered Gaussian distributions N(0,Q)
and N(0,S) has the closed-form expression TS = S1/2(S1/2QS1/2)−1/2S1/2. In all relevant
Q
analysis, we need to consider differentials of TS when viewed as a function of Q for fixed S,
Q
which we denote as dkTS for k ≥ 1. It is shown in Kroshnin et al. (2021) that dTS can be
Q Q
defined as follows. For any H ∈ S ,
d
dTS(H) := −S1/2U⊤Λ−1/2δΛ−1/2US1/2
Q
31where U⊤ΛU is an eigenvalue decomposition of S1/2QS1/2 with UU⊤ = U⊤U = I and δ =
(δ )d with
ij i,j=1

√
∆i√j
i,j ≤ rank(S)
δ
ij
= λi+ λj , ∆ = US1/2HS1/2U⊤
0 otherwise
To consider higher order differentials of TS, we start with the map Q (cid:55)→ Q1/2 in Lemma 15,
Q
next move on to Q (cid:55)→ Q−1/2 in Lemma 16 which then leads to TS in Lemma 17. Connections
Q
between TS and W2(Q,S) are also investigated in Lemma 17.
Q
Lemma 15. The square root functional ϕ : Q ∈ S++ → ϕ(Q) = Q1/2 ∈ S++ is Fréchet
d d
differentiable at any order on S++. Moreover, for any Q ∈ S++ and any n ≥ 0, we have the
d d
estimates
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)dn+1ϕ(Q)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) ≤ C d,nλ min(Q)−(n+1/2)
Here C = dn/2·n!(cid:0)2n(cid:1) ·2−(2n+1).
d,n n
Proof. See Del Moral and Niclas (2018) Theorem 1.1.
Lemma 16. The inverse square root functional φ : Q ∈ S++ (cid:55)→ φ(Q) = Q−1/2 ∈ S++ is
d d
Fréchet differentiable at any order on S++. Moreover, for any A ∈ S++,H ∈ Sd, the following
d d
holds.
dφ(Q)·H = −Q−1/2(dϕ(Q)·H)Q−1/2
d2φ(Q)·H⊗2 = −Q−1/2(cid:0) d2ϕ(Q)·H⊗2(cid:1) Q−1/2+2Q−1/2(dϕ(Q)·H)Q−1/2(dϕ(Q)·H)Q−1/2
d3φ(Q)·H⊗3 = −Q−1/2(cid:0) d3ϕ(Q)·H⊗3(cid:1) Q−1/2+3Q−1/2(cid:0) d2ϕ(Q)·H⊗2(cid:1) Q−1/2(dϕ(Q)·H)Q−1/2
+3Q−1/2(dϕ(Q)·H)Q−1/2(cid:0) d2ϕ(Q)·H⊗2(cid:1) Q−1/2
−6Q−1/2(dϕ(Q)·H)Q−1/2(dϕ(Q)·H)Q−1/2(dϕ(Q)·H)Q−1/2
with
|||dφ(Q)||| ≤ C λ (Q)−3/2
d,0 min
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d2φ(Q)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) ≤ (C d,1+2C d2 ,0)·(λ min(Q))−5/2
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d3φ(Q)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) ≤ (cid:0) C d,2+6C d,1C d,0+6C d3 ,0(cid:1) λ min(Q)−7/2
where C is defined in Lemma 15.
d,n
Proof. By Lemma 15 we have for infinitesimal H ∈ S that
d
1 1
(Q+H)1/2 = Q1/2+dϕ(Q)·H+ d2ϕ(Q)·H⊗2+ d3ϕ(Q)·H⊗3+···
2 6
(cid:124) (cid:123)(cid:122) (cid:125)
Z1 (cid:124) Z(cid:123)(cid:122)
2
(cid:125) (cid:124) Z(cid:123)(cid:122)
3
(cid:125)
Let Z = Z +Z +Z +Z , and we obtain for infinitesimal H ∈ S that
1 2 3 4 d
(Q+H)−1/2
(cid:16) (cid:17)−1
= Q1/4(I +Q−1/4ZQ−1/4)Q1/4
d
= Q−1/4(I +Q−1/4ZQ−1/4)−1Q−1/4
d
( =i) Q−1/4(cid:16) I −Q−1/4ZQ−1/4+Q−1/4ZQ−1/2ZQ−1/4−Q−1/4ZQ−1/2ZQ−1/2ZQ−1/4+···(cid:17) Q−1/4
d
32= Q−1/2−Q−1/2ZQ−1/2+Q−1/2ZQ−1/2ZQ−1/2−Q−1/2ZQ−1/2ZQ−1/2ZQ−1/2+···
( =ii) Q−1/2−Q−1/2Z Q−1/2−Q−1/2Z Q−1/2+Q−1/2Z Q−1/2Z Q−1/2
1 2 1 1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
1storderinH 2ndorderinH
−Q−1/2Z Q−1/2+Q−1/2Z Q−1/2Z Q−1/2+Q−1/2Z Q−1/2Z Q−1/2−Q−1/2Z Q−1/2Z Q−1/2Z Q−1/2
3 2 1 1 2 1 1 1
(cid:124) (cid:123)(cid:122) (cid:125)
3rdorderinH
+higher order terms in H
Here(i)followsfromthevonNeumannseriesexpansion,and(ii)isobtainedbyarrangingterms
according to their orders in H. Then (16) follows, and we have the following estimate
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)d2φ(Q)·H⊗2(cid:13) (cid:13) ≤ (cid:13)Q−1/2(cid:0) d2ϕ(Q)·H⊗2(cid:1) Q−1/2(cid:13) +2(cid:13)Q−1/2(dϕ(Q)·H)Q−1/2(dϕ(Q)·H)Q−1/2(cid:13)
F (cid:13) (cid:13) (cid:13) (cid:13)
F F
≤ λ min(Q)−1·(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d2ϕ(Q)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)·∥H∥2 F+2λ min(Q)−3/2|||dϕ(Q)|||2·∥H∥2
F
≤ (C +2C2 )λ (Q)−5/2·∥H∥2
d,1 d,0 min F
and similarly
(cid:13) (cid:13)d3φ(Q)·H⊗3(cid:13) (cid:13)
F
≤ λ min(Q)−1(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d3φ(Q)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)·∥H∥3 F+6λ min(Q)−3/2(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d2φ(Q)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)·|||dφ(Q)|||·∥H∥3
F
+6λ (Q)−2|||dφ(Q)|||3·∥H∥3
min F
≤ (cid:0) C +6C C +6C3 (cid:1) λ (Q)−7/2·∥H∥3
d,2 d,1 d,0 d,0 min F
Here the last inequality follows by applying Lemma 15. And we arrive at the desired result.
Lemma 17. The following properties hold for the 2-Wasserstein distance W(Q,S) and the
optimal transport map TS. For any Q,Q ,Q ∈ S++, S ∈ S+ and X,Y ∈ S ,
Q 1 2 d d d
1. W2(Q,S) is upper bounded by
W2(Q,S) ≤ 2d(λ (Q)+λ (S)) (29)
max max
2. W2(Q,S) is twice differentiable with
d W2(Q,S)(X) = (cid:10) I −TS,X(cid:11)
Q Q
d2W2(Q,S)(X,Y) = −(cid:10) X,dTS(Y)(cid:11)
Q Q
Moreover, the following quadratic approximation holds:
2 (cid:10) −dTS (Q −Q ),Q −Q (cid:11)
(cid:16)
1+λ1/2
(Q′)(cid:17)2 Q0 1 0 1 0
max
≤W2(Q ,S)−W2(Q ,S)+(cid:10) TS −I,Q −Q (cid:11) (30)
1 0 Q0 1 0
≤ 2 (cid:10) −dTS (Q −Q ),Q −Q (cid:11)
(cid:16)
1+λ1/2
(Q′)(cid:17)2 Q0 1 0 1 0
min
with Q′ := Q−1/2 Q Q−1/2.
0 1 0
3. dTS is self-adjoint, negative semi-definite and enjoys the following two-sided bound.
Q
λ
m1/ i2 n(cid:0) S1/2QS1/2(cid:1)
(cid:13) (cid:13)Q−1/2XQ−1/2(cid:13) (cid:13)2
≤
(cid:10) −dTS(X),X(cid:11)
≤
λ
m1/ a2 x(cid:0) S1/2QS1/2(cid:1)
(cid:13) (cid:13)Q−1/2XQ−1/2(cid:13) (cid:13)2
2 (cid:13) (cid:13) F Q 2 (cid:13) (cid:13) F
(31)
334. W2(Q ,Q ) can be upper and lower bounded by the Frobenius norm as follows
0 1
1 λ (Q )λ−2 (Q ) λ (Q )λ−2 (Q )
max 0 min 0 ·∥Q −Q ∥2 ≤ W2(Q ,Q ) ≤ max 0 min 0 ·∥Q −Q ∥2
21+λ−1 (Q )λ (Q ) 1 0 F 0 1 1+λ−1 (Q )λ (Q ) 1 0 F
min 0 max 1 max 0 min 1
(32)
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
5. (cid:12)(cid:12)(cid:12)dkTS(cid:12)(cid:12)(cid:12)can be upper bounded by
(cid:12)(cid:12)(cid:12) Q(cid:12)(cid:12)(cid:12)
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)dT
QS(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12) ≤
λ min( 2Q)−2 ·(cid:16)
λ
max(cid:16) S1/2QS1/2(cid:17)(cid:17)1/2
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d2T QS(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) ≤ λ max(S)3(C d,1+2C d2 ,0)·(cid:16) λ min(cid:16) S1/2QS1/2(cid:17)(cid:17)−5/2 (33)
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d3T QS(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) ≤ λ max(S)4(cid:0) C d,2+6C d,1C d,0+6C d3 ,0(cid:1)(cid:16) λ min(cid:16) S1/2QS1/2(cid:17)(cid:17)−7/2
Moreover, if S,Q ∈ S (M−1,M), then we have
d
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)dTS(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) ≤ 1 M3
Q 2
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d2T QS(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) ≤ (C d,1+2C d2 ,0)M8
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d3T QS(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) ≤ (cid:0) C d,2+6C d,1C d,0+6C d3 ,0(cid:1) M11
Proof.
Proof of (29): By the closed form expression for W2(Q,S), one has
(cid:104) (cid:105)
W2(Q,S) = tr Q+S −2(S1/2QS1/2)1/2
(cid:104) (cid:105)
≤ tr Q+S +2(S1/2QS1/2)1/2
(cid:16) (cid:17)
≤ d λ (Q)+λ (S)+2λ (Q)1/2λ (S)1/2
max max max max
≤ 2d(λ (Q)+λ (S))
max max
Proof of (30), (31): see Kroshnin et al. (2021, Lemma A.2, A.3, A.4, A.6).
Proof of (32): Set S = Q in (30), one can obtain
0
2 (cid:68) (cid:69)
−dTQ0(Q −Q ),Q −Q ≤ W2(Q ,Q )
(cid:16)
1+λ1/2
(Q′)(cid:17)2 Q0 1 0 1 0 1 0
max
2 (cid:68) (cid:69)
≤ −dTQ0(Q −Q ),Q −Q
(cid:16)
1+λ1/2
(Q′)(cid:17)2 Q0 1 0 1 0
min
(34)
where Q′ = Q−1/2 Q Q−1/2. Next, apply (31) to get that for any X ∈ S , one has
0 1 0 d
λ (Q ) (cid:68) (cid:69) λ (Q )
min 0 λ−2 (Q )∥X∥2 ≤ −dTQ0(X),X ≤ max 0 λ−2 (Q )∥X∥2 (35)
2 max 0 F Q0 2 min 0 F
Combine (34) and (35) to get that
λ (Q )λ−2 (Q )
W2(Q ,Q ) ≤ max 0 min 0 ·∥Q −Q ∥2
1 0 (cid:16) (cid:17)2 1 0 F
1+λ1/2 (Q′)
min
34λ (Q )λ−2 (Q )
≤ max 0 min 0 ·∥Q −Q ∥2
1+λ (Q′) 1 0 F
min
λ (Q )λ−2 (Q )
≤ max 0 min 0 ·∥Q −Q ∥2
1+λ−1 (Q )λ (Q ) 1 0 F
max 0 min 1
and similarly
λ (Q )λ−2 (Q )
W2(Q ,Q ) ≥ min 0 max 0 ·∥Q −Q ∥2
1 0 (cid:16) (cid:17)2 1 0 F
1+λ1/2 (Q′)
max
1λ (Q )λ−2 (Q )
≥ max 0 min 0 ·∥Q −Q ∥2
2 1+λ (Q′) 1 0 F
max
1 λ (Q )λ−2 (Q )
≥ max 0 min 0 ·∥Q −Q ∥2
21+λ−1 (Q )λ (Q ) 1 0 F
min 0 max 1
Proof of (33): First, note that results for dTS follows directly from (31).
Q
Next since TS = S1/2(S1/2QS1/2)−1/2S1/2, applying Lemma 16 gives that for H small
Q
enough, one has
(cid:104) (cid:105)
d2TS ·H⊗2 = S1/2 d2φ(S1/2QS1/2)·(S1/2HS1/2)⊗2 S1/2
Q
(cid:104) (cid:105)
d3TS ·H⊗3 = S1/2 d3φ(S1/2QS1/2)·(S1/2HS1/2)⊗3 S1/2
Q
which implies
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d2T QS(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) ≤ λ max(S)3·(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d2φ(S1/2QS1/2)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
(cid:16) (cid:17)−5/2
≤ λ (S)3(C +2C2 )· λ (S1/2QS1/2)
max d,1 d,0 min
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d3T QS(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) ≤ λ max(S)4(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d3φ(S1/2QS1/2)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
≤ λ (S)4(cid:0) C +6C C +6C3 (cid:1) λ (S1/2QS1/2)−7/2
max d,2 d,1 d,0 d,0 min
Lemma 18. Let Y and Z be normed spaces with the norm on each denoted by ∥·∥. For any
y ∈ Y, P ∈ M (Y,Z), k ≥ 2, let Py denote a (k−1)-linear function defined by
k,s
Py(y ,...,y ) = P(y,y ,...,y )
1 k−1 1 k−1
Then Py ∈ M (Y,Z) and
k−1,s
kk
|||Py||| ≤ ∥P∥∥y∥ ≤ |||P|||∥y∥ (36)
k!
Proof. Py ∈ M (Y,Z) can be proved by definition. To prove (36), note that for any
k−1,s
∥y∥ ≤ 1, one has
(cid:101)
∥Py(y,...,y)∥ ≤ ∥P∥·∥y∥·∥y∥k−1
(cid:101) (cid:101) (cid:101)
≤ ∥P∥·∥y∥
(i) kk
≤ |||P|||·∥y∥
k!
Here (i) follows from Dudley and Norvaiša (2011, Theorem 5.7).
35B.2 Concentration inequalities and uniform convergence
First, let us introduce some additional notation. Given a random variable X we denote ∥X∥
ψα
for α > 0 as follows.
∥X∥ := inf{η > 0 : Eψ (|X/η|) ≤ 1}, where ψ (x) := exp(xα)−1 for x ≥ 0 (37)
ψα α α
Note that by the definition and Markov’s inequality, if ∥X∥ < ∞, then
ψα
(cid:32) (cid:33)
tα
P(|X| ≥ t) ≤ 2exp − (38)
∥X∥α
ψα
For α ≥ 1, ∥·∥ is a norm (Vershynin, 2018), and for α < 1, it is equivalent to a norm
ψα
(Talagrand, 1989). For a random vector X ∈ Rp, denote
(cid:13) (cid:13)
∥X∥ := sup (cid:13)u⊤X(cid:13)
ψα u∈Rp,∥u∥=1(cid:13) (cid:13)
ψα
Lemma 19 (Kuchibhotla and Chakrabortty (2022), Theorem 3.1). If X ,...,X are indepen-
1 n
dent, mean zero random variables with ∥X ∥ ≤ K for all i ∈ [n] and some α > 0, then the
i ψα
following bounds hold true:
• α ∈ (0,1]:
(cid:32)
1
(cid:12) (cid:12)(cid:88)n (cid:12)
(cid:12)
(cid:114)
t
t1/α(cid:33)
P (cid:12) X (cid:12) > C K +C K ≤ 2e−t for all t ≥ 0
n (cid:12) i(cid:12) 1 n 2 n
(cid:12) (cid:12)
i=1
where C ,C > 0 are constants that depends only on α.
1 2
• α > 1
(cid:32) 1 (cid:12) (cid:12)(cid:88)n (cid:12) (cid:12) (cid:114) t (cid:18) t(cid:19)1/α(cid:33)
P (cid:12) X (cid:12) > C K +C K ≤ 2e−t for all t ≥ 0
n (cid:12) i(cid:12) 3 n 2 n
(cid:12) (cid:12)
i=1
where C ,C > 0 are constants that depends only on α.
1 2
Lemma20. LetX,QberandomelementsinRd,Rd×d. LetΦ bearandomsymmetricoperator
k
in L(cid:0) (Rd×d)×k;Rd×d(cid:1) for some k ∈ N . Then
+
1. ∥X∥ ≤ ∥∥X∥ ∥ ≲ ∥X∥ for any α > 0.
ψα 2 ψα ψα
2. For any U ∈ Rd×d with unit Frobenius norm
∥⟨U,Q⟩∥ ≤ ∥∥Q∥ ∥ ≲ sup ∥⟨U,Q⟩∥
ψ2 F ψ2 ψ2
U:∥U∥ =1
F
3. For any U ∈ Rd×d with unit Frobenius norm
(cid:13)(cid:13) (cid:13) (cid:13) (cid:13)(cid:13) (cid:13) (cid:13)
(cid:13)(cid:13)Φ ·U⊗k(cid:13) (cid:13) ≤ ∥|||Φ |||∥ ≲ sup (cid:13)(cid:13)Φ ·U⊗k(cid:13) (cid:13)
(cid:13)(cid:13) k (cid:13) F(cid:13)
ψ2
k ψ2
U:∥U∥
=1(cid:13)(cid:13) k (cid:13) F(cid:13)
ψ2
F
The constants behind ≲ only depend on dimension d, k and possible on α.
Proof. This can be shown using similar techniques as in Vershynin (2018, Lem 4.4.1, 4.4.5.)
and Lemma 1 in Jin et al. (2019).
36Next, we give some properties of the weights s(x,X) and s (x,X ) that will be crucial to
n i
prove uniform concentration as well as a central limit theorem for the estimate Q(cid:98)n(x). Recall
that by definition, s(x,X) = 1+(x−µ)⊤Σ−1(X −µ) and s n(x) = 1+(x−X)Σ(cid:98)−1(X i−X).
For any vector z ∈ Rp, denote ⃗z = (1,z⊤)⊤ and Σ⃗ = EX⃗X⃗⊤.
Lemma 21. Suppose X,X ,...,X i. ∼i.d. P ∈ P (Rp). Let Σ⃗(cid:98) = n−1(cid:80)n X⃗ X⃗⊤. Then for any
1 n 2 i=1 i i
x ∈ Rp,
s(x,X) =⃗x⊤Σ⃗−1X⃗ (39)
−1
s (x,X ) =⃗x⊤Σ⃗(cid:98) X⃗ (40)
n i
Proof. For (39), by definition one has
(cid:18) 1 µ⊤ (cid:19)
Σ⃗ =
µ Σ+µµ⊤
Computing the inverse of the block matrix Σ⃗ then gives
(cid:18) 1+µ⊤Σ−µ −µ⊤Σ−(cid:19)
Σ⃗−1 =
−Σ−µ Σ−
Finally we arrive at (39) by computing ⃗x⊤Λ−1X⃗. (40) follows from similar arguments.
Lemma 22. Suppose Assumption 1 holds. Set
W := W (x) = X⊤Σ−1(x−X)−µ⊤Σ−1(x−µ)
0n 0n
W
1n
:= W 1n(x) = Σ−1(x−µ)−Σ(cid:98)−1(x−X)
Then have
s (x,X )−s(x,X ) = W +W⊤X (41)
n i i 0n 1n i
Moreover, for any L ≳ 1,
√
L logn
sup |W (x)| ≤ C √
0n
n
x∈Bµ(L) (42)
logn
sup ∥W (x)∥ ≤ CL √
1n 2 n
x∈Bµ(L)
with probability at least 1−O(n−100) for some large constant C > 0 that depends on d.
Proof. (41) is shown in (A.2) Petersen and Müller (2019). We only prove (42) here.
• For W ,
0n
(cid:13) (cid:13) (cid:13) (cid:13)
|W | ≤ (cid:13)X⊤Σ−1(x−X)−µ⊤Σ−1(x−X)(cid:13)+(cid:13)µ⊤Σ−1(x−X)−µ⊤Σ−1(x−µ)(cid:13)
0n (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
= (cid:13)(X −µ)⊤Σ−1(x−X)(cid:13)+(cid:13)µ⊤Σ−1(µ−X)(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
≤ (cid:13)(X −µ)⊤Σ−1(x−µ)(cid:13)+(cid:13)(X −µ)⊤Σ−1(X −µ)(cid:13)+(cid:13)µ⊤Σ−1(µ−X)(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
≤ (cid:13)(X −µ)⊤Σ−1(cid:13)·∥x−µ∥+(cid:13)(X −µ)⊤Σ−1(X −µ)(cid:13)+(cid:13)µ⊤Σ−1(µ−X)(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
∆n,1 ∆n,2
37Here ∥∆ ∥ ,∥∆ ∥ ≲ n−1/2 and does not depend on x. Hence, one can obtain
n,1 ψ2 n,2 ψ2
√
L logn
sup |W (x)| ≲ √
0n
n
x∈Bµ(L)
with probability at least 1−O(cid:0) n−100(cid:1).
• For W , similarly
1n
(cid:13) (cid:13) (cid:13) (cid:13)
∥W 1n∥ ≤ (cid:13) (cid:13)Σ−1−Σ(cid:98)−1(cid:13) (cid:13)∥x−µ∥+(cid:13) (cid:13)Σ(cid:98)−1(µ−X)(cid:13)
(cid:13)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
∆n,3 ∆n,4
By Exercise 5.6.4 in Vershynin (2018) and a truncation argument, one can obtain
(cid:115) (cid:115)
(cid:13) (cid:13) log2n log2n log2n
(cid:13)Σ(cid:98) −Σ(cid:13) ≲ ∨ ≲
(cid:13) (cid:13) n n n
with probability at least 1−O(n−100) which implies for any L ≳ 1,
√
logn logn
sup ∥W (x)∥ ≲ L √ + √
1n
n n
x∈Bµ(L)
logn
≲ L √
n
with probability at least 1−O(n−100)
Lemma 23. Suppose (X ,Q ) ∈ Rp×Rm,i = 1,...,n are i.i.d with ∥X ∥ ≤ C . Let (V,∥·∥)
i i i ψ2 ψ2
be a normed vector space and ψ : Rm ×Θ → V be mappings parametrized by θ ∈ Θ such that
almost surely ∥ψ(Q,θ)∥ ≤ K uniformly for θ ∈ Θ. Then for any L ≳ 1, the following
n
(cid:13) (cid:13)
n n
(cid:13)1 (cid:88) 1 (cid:88) (cid:13) logn
sup sup(cid:13) s (x,X )ψ(Q ;θ)− s(x,X )ψ(Q )(cid:13) ≲ LK · √ (43)
(cid:13)n n i i n i i (cid:13) n n
x∈Bµ(L)θ∈Θ(cid:13)
i=1 i=1
(cid:13)
holds with probability at least 1−O(n−100).
Proof. By (41) in Lemma 22, one can obtain for any x and θ ∈ Θ,
(cid:13) (cid:13)
n n
(cid:13)1 (cid:88) 1 (cid:88) (cid:13)
(cid:13) s (x,X )ψ(Q ;θ)− s(x,X )ψ(Q ;θ)(cid:13)
(cid:13)n n i i n i i (cid:13)
(cid:13) (cid:13)
i=1 i=1
(cid:13) (cid:13)
n n
(cid:13) 1 (cid:88) 1 (cid:88) (cid:13)
= (cid:13)W ψ(Q ;θ)+ W⊤X ψ(Q ;θ)(cid:13)
(cid:13) 0n n i n 1n i i (cid:13)
(cid:13) (cid:13)
i=1 i=1
n n
1 (cid:88) 1 (cid:88)
≤ |W n|· ∥ψ(Q ;θ)∥+∥W ∥ ∥X ∥ ∥ψ(Q ;θ)∥
0 n i 1n 2 n i 2 i
i=1 i=1
n
1 (cid:88)
≤ |W |·K +∥W ∥ K · ∥X ∥
0n n 1n 2 n n i 2
i=1
38Take supremum over x,θ and combine with (42), one can obtain
(cid:13) (cid:13)
n n
(cid:13)1 (cid:88) 1 (cid:88) (cid:13)
sup sup(cid:13) s (x,X )ψ(Q ;θ)− s(x,X )ψ(Q ;θ)(cid:13)
(cid:13)n n i i n i i (cid:13)
x∈Bµ(L)θ∈Θ(cid:13)
i=1 i=1
(cid:13)
(cid:32) n (cid:33)
logn 1 (cid:88)
≲ √ LK 1+ ∥X ∥
n n n i 2
i=1
with probability at least 1−O(n−100). Concentration of n−1(cid:80)n ∥X ∥ then implies (43).
i=1 i 2
We will need uniform upper bounds for various quantities of the form
supf(Zn;θ)
1
θ∈Θ
where Zn denotes (Z ,...,Z ). To this end, we decompose the above quantity as follows.
1 1 n
supf(Zn;θ)−Esupf(Zn;θ)+Esupf(Zn;θ)
1 1 1
θ∈Θ θ∈Θ θ∈Θ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
perturbation expectation
The expectation term is bounded in Lemma 24 below with a chaining argument, while the
perturbation term is shown to concentrate in Lemma 26 by exploiting its bounded difference
property. The proof of Lemma 24 is deferred to Appendix B.2.1.
Lemma 24. Given a function class
F(Θ) = {f(·;θ) : Rp → R : θ ∈ Θ}
where f is continuous jointly in (z,θ) and (Θ,d) is a separable metric space with finite diameter
D := sup d(θ ,θ ) ≳ 1. SupposearandomvectorZ ∈ Rp satisfiesthefollowinginequality
θ1,θ2∈Θ 1 2
∥f(Z;θ )−f(Z;θ )∥ ≤ τ (d(θ ,θ )), ∀θ ,θ ∈ Θ (44)
1 2 ψ2 1 2 1 2
for some increasing function τ : R+ → R+ that satisfies τ(0) = 0,τ(+∞) = +∞. Then
• the following inequality holds
(cid:90) τ(D)
Esup|f(Z;θ)| ≤ C (cid:112) logN(τ−1(t);Θ)·dt+supE|f(Z;θ)| (45)
θ∈Θ 0 θ∈Θ
where C > 0 is a fixed absolute constant and N(ϵ;Θ) is the ϵ-covering number of Θ.
• Specifically, if τ has the form
K
τ(ϵ) = √ (ϵ∨ϵα0)
n
for some constant α > 0, then for any D ≳ 1, one has
0
(cid:90) τ(D)
(cid:112)
KD1∨α0(cid:113)
logN(τ−1(t);Θ)·dt ≲ √ log+D (46)
n
0
• Specifically, if Z ,...,Z ∈ Rp are i.i.d. and f : (Rp)k ×Θ → R is equal to
1 n
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
n n
(cid:12)(cid:12)(cid:12)1 (cid:88) 1 (cid:88) (cid:12)(cid:12)(cid:12)
f(Zn;θ) = (cid:12)(cid:12)(cid:12) ψ(Z ;θ)−E ψ(Z ;θ)(cid:12)(cid:12)(cid:12), Zn := (Z ,...,Z )
1 (cid:12)(cid:12)(cid:12)n i n i (cid:12)(cid:12)(cid:12) 1 1 n
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
i=1 i=1
39where ψ(z;θ) ∈ M (Rm;Rm) is a symmetric k-linear operator for any z ∈ Rp,θ ∈ Θ.
k,s
Then the following inequality
(cid:13) (cid:13) 1 (cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
(cid:13) (cid:13)f(Z 1n;θ)−f(Z 1n;θ(cid:101))(cid:13)
(cid:13) ψ2
≲ √
n
(cid:13) (cid:13)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)ψ(Z 1;θ)−ψ(Z 1;θ(cid:101))(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:13)
(cid:13) ψ2
(47)
holds.
Remark 25. (47) will be applied in three ways as follows.
• m = 1,k = 1: ψ(z;θ) ∈ R and |||ψ(z;θ)||| reduces to the absolute value of ψ(z;θ). Examples
include ψ(X,Q;x,S) = s(x,X)W2(Q,S) in Lemma 31
• m = d,k = 1: ψ(z;θ) ∈ L(Rm;Rm) can be viewed as a matrix. Moreover, |||ψ(z;θ)||| reduces
to the matrix operator norm. Examples include ψ(X,Q;x) = s(x,X)(TQ −I ) in Lemma
Q∗(x) d
33.
• m = d×d,k = 1: this is a special case of the previous one by identifying d×d matrices as
a vector in Rd2. Examples include ψ(X,Q;x) = −s(x,X)dTQ in Lemma 33.
Q∗(x)
• m = d×d,k = 2: Examples include ψ(X,Q;x,S) = s(x,X)d2TQ in Lemma 33.
S
Lemma 26. Let ψ : Rp ×Θ → Rq be a class of functions indexed by θ ∈ Θ. Suppose ψ is
uniformly bounded in the sense that there exists a finite constant K such that ∥ψ(x;θ)∥ ≤ K
2
for any x ∈ Rp and θ ∈ Θ. For any fixed y ∈ W, define f : (Rp)n×Θ → Rq as
0
(cid:13) (cid:13)
n
(cid:13)1 (cid:88) (cid:13)
f(xn) := sup(cid:13) ψ(x ;θ)−y (cid:13)
1 (cid:13)n i 0(cid:13)
θ∈Θ(cid:13) (cid:13)
i=1 2
Then f satisfies the bounded difference property with parameter 2K/n, i.e. for any xn,xn ∈
1 (cid:101)1
(Rp)n such that (cid:80)n I(x ̸= x ) ≤ 1,
i=1 i (cid:101)i
2K
∥f(xn)−f(xn)∥ ≤ , (48)
1 (cid:101)1 2 n
Moreover, suppose X ,...,X are i.i.d. random element in Rp, then
1 n
2K
∥f(Xn)−Ef(Xn)∥ ≲ √ (49)
1 1 ψ2 n
Proof. Without loss of generality, assume x ̸= x and x = x for j = 2,...,n. Then one has
1 (cid:101)1 j (cid:101)j
(cid:13) (cid:13) (cid:13) (cid:13)
n n
(cid:13)1 (cid:88) (cid:13) (cid:13)1 (cid:88) (cid:13)
(cid:13)
(cid:13)n
ψ(x i;θ)−y 0(cid:13)
(cid:13)
−sup(cid:13)
(cid:13)n
ψ(x (cid:101)i;θ(cid:101))−y 0(cid:13)
(cid:13)
(cid:13)
i=1
(cid:13)
2
θ(cid:101)∈Θ(cid:13)
i=1
(cid:13)
2
(cid:13) (cid:13) (cid:13) (cid:13)
n n
(cid:13)1 (cid:88) (cid:13) (cid:13)1 (cid:88) (cid:13)
≤(cid:13) ψ(x ;θ)−y (cid:13) −(cid:13) ψ(x ;θ)−y (cid:13)
(cid:13)n i 0(cid:13) (cid:13)n (cid:101)i 0(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
i=1 2 i=1 2
(cid:13) (cid:13)
n n
(cid:13)1 (cid:88) 1 (cid:88) (cid:13)
≤(cid:13) ψ(x ;θ)−y − ψ(x ;θ)+y (cid:13)
(cid:13)n i 0 n (cid:101)i 0(cid:13)
(cid:13) (cid:13)
i=1 i=1 2
(cid:13) (cid:13)
(cid:13)1 (cid:13)
=(cid:13) ψ(x 1;θ)−ψ(x (cid:101)1;θ)(cid:13)
(cid:13)n (cid:13)
2
2K
≤
n
40Taking supremum over θ then gives
2K
f(xn)−f(xn) ≤
1 (cid:101)1 n
The other direction can be obtained with the role xn and xn reversed. Therefore we get (48).
1 (cid:101)1
By Corollary 2.21 in Wainwright (2019), we get (49). The proof is then complete.
Finally,eventhoughTheorem6showsfastconvergenceQ(cid:98)n(x) → Q∗(x),oneneedsQ(cid:98)n(X) →
Q∗(µ) when considering power in Theorem 14, and this is stated in Lemma 27 below. A cru-
cial observation here is that Q(cid:98)n(X) and Q∗(µ) are the empirical and population Fréchet mean
respectively. The proof is built upon Le Gouic et al. (2022) and Altschuler et al. (2021).
Lemma 27. Assume Assumption 1 and 2 hold. Then there exists an event E(cid:101) with probability
P(E(cid:101)) ≥ 1−n−100 under which the following holds
√
(cid:16) (cid:17) logn
W2 Q(cid:98)n(X),Q∗(µ) ≲ √ (50)
n
As a result, under E(cid:101), one has
√
(cid:13) (cid:13) logn
(cid:13) (cid:13)Q(cid:98)n(X)−Q∗(µ)(cid:13)
(cid:13) F
≲ √
n
(51)
for n large enough.
Proof. Note that Q(cid:98)n(X) and Q∗(µ) are equal to the empirical and population barycenter
respectively. For simplicity, we write Q∗ for Q∗(µ).
Proof of (50): From (3.8)-(3.9) in Le Gouic et al. (2022) and results in Altschuler et al.
(2021), one can obtain the following
(cid:13) (cid:13)
n
W2(cid:16) Q(cid:98)n(X),Q∗(cid:17)
≤ C
b(cid:13)
(cid:13)
(cid:13)n1 (cid:88)(cid:16)
T QQ ∗i −I
d(cid:17)(cid:13)
(cid:13)
(cid:13)
(52)
(cid:13) (cid:13)
i=1 Q∗
where C > 0 is a constant independent of n and ∥A∥ := ⟨A,Q∗A⟩. Moreover, Lemma 28
b Q∗(µ)
implies that c−1I ⪯ Q∗ ⪯ c I ; see Assumption 2 for the definition of c . As a result, one can
1 d 1 d 1
obtain
(cid:13) (cid:13) (cid:13) (cid:13)
n n
(cid:13) (cid:13)1 (cid:88)(cid:16)
TQi −I
(cid:17)(cid:13)
(cid:13) ≤ λ
(Q∗)·(cid:13) (cid:13)1 (cid:88)(cid:16)
TQi −I
(cid:17)(cid:13)
(cid:13)
(cid:13)n Q∗ d (cid:13) max (cid:13)n Q∗ d (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
i=1 Q∗ i=1 F
(cid:13) (cid:13)
n
≤ c
(cid:13) (cid:13)1 (cid:88)(cid:16)
TQi −I
(cid:17)(cid:13)
(cid:13) (53)
1(cid:13)n Q∗ d (cid:13)
(cid:13) (cid:13)
i=1 F
(cid:16) (cid:17)
Note that by the optimality condition for barycenter, one has E TQ −I = 0. Then one can
Q∗ d
apply Lemma 20 to get that
(cid:13)(cid:13)
n
(cid:13) (cid:13) (cid:13)(cid:42)
n
(cid:43)(cid:13)
(cid:13) (cid:13)(cid:13) (cid:13)1 (cid:88)(cid:16)
TQi −I
(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13) ≲ sup
(cid:13)
(cid:13) U,
1 (cid:88)(cid:16)
TQi −I
(cid:17) (cid:13)
(cid:13)
(cid:13)(cid:13)n Q∗ d (cid:13) (cid:13) (cid:13) n Q∗ d (cid:13)
(cid:13)(cid:13) (cid:13) (cid:13) ∥U∥ ≤1(cid:13) (cid:13)
i=1 F ψ2 F i=1 ψ2
1 (cid:13)(cid:68) (cid:69)(cid:13)
≲ sup √ (cid:13) U,TQ −I (cid:13)
∥U∥ ≤1 n (cid:13) Q∗ d (cid:13) ψ2
F
411 (cid:13)(cid:13) (cid:13) (cid:13)
≤ √ (cid:13)(cid:13)TQ −I (cid:13) (cid:13) (54)
n (cid:13)(cid:13) Q∗ d(cid:13) F(cid:13) ψ2
Recall that TQ = S−1/2(cid:0) S1/2QS1/2(cid:1)1/2 S−1/2, then one has
S
(cid:13)(cid:13) (cid:13) (cid:13) (i) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)(cid:13) (cid:13)T QQ
∗
−I d(cid:13)
(cid:13)
F(cid:13)
(cid:13)
ψ2
≲ (cid:13) (cid:13) (cid:13)(cid:13) (cid:13)T QQ
∗
−I d(cid:13)
(cid:13)
op(cid:13) (cid:13)
(cid:13) ψ2
(cid:13) (cid:13)
≤ (cid:13)1+c3/2 λ (Q)1/2(cid:13)
(cid:13) 1 max (cid:13)
ψ2
(ii) (cid:13) (cid:13)
≤ (cid:13)1+c3/2 c1/2 ∥X −µ∥1/2(cid:13)
(cid:13) 1 1 (cid:13)
ψ2
(cid:13) (cid:13)
≤ (cid:13)1+c3/2 c1/2 (1∨∥X −µ∥)(cid:13)
(cid:13) 1 1 (cid:13)
ψ2
≤ (cid:13) (cid:13)1+c2(1+∥X −µ∥)(cid:13) (cid:13)
1 ψ2
≲ 1 (55)
Here (i) follows since dimension d is fixed and absorbed into the constant factor independent
of n, (ii) is a result of Assumption 2.
Finally, combining (52) (53) (54) and (55) gives
(cid:13) (cid:16) (cid:17)(cid:13) 1
(cid:13) (cid:13)W2 Q(cid:98)n(X),Q∗(µ) (cid:13)
(cid:13) ψ2
≲ √
n
(56)
which implies (50).
Proof of (51): Apply (32) in Lemma 17 to get that under the event E(cid:101),
(cid:16) (cid:17)
(cid:13) (cid:13) (cid:16) (cid:17)
1+λ− m1 in(Q∗)λ
max
Q(cid:98)n(X)
(cid:13) (cid:13)Q(cid:98)n(X)−Q∗(cid:13)
(cid:13) F
≤ W2 Q(cid:98)n(X),Q∗ ·
λ max(Q∗)·λ− m2 in(Q∗)
(cid:16) (cid:17) (cid:16) (cid:16) (cid:17)(cid:17) (57)
≲ W2 Q(cid:98)n(X),Q∗ · 1+λ
max
Q(cid:98)n(X)
√
logn (cid:16) (cid:16) (cid:17)(cid:17)
≤ C √ · 1+λ
max
Q(cid:98)n(X)
n
for constant C > 0 large enough. Note that this implies
(cid:16) (cid:17) (cid:13) (cid:13)
λ
max
Q(cid:98)n(X) ≤ λ max(Q∗)+(cid:13) (cid:13)Q(cid:98)n(X)−Q∗(cid:13)
(cid:13)
√ F
logn (cid:16) (cid:16) (cid:17)(cid:17)
≤ c 1+C √ · 1+λ
max
Q(cid:98)n(X)
n
(cid:16) (cid:17)
Solving for λ
max
Q(cid:98)n(X) gives
√
(cid:16) (cid:17) c 1+C √lo ngn
λ max Q(cid:98)n(X) ≤ 1−C√ √logn (58)
n
≲ 1 for n large enough
Finally, plugging (58) back into (57) gives (51).
42B.2.1 Proof of Lemma 24
Proof of (45): Sincef(z,θ)iscontinuousinθandΘisseparable,wehaveforanycountable,
dense subset U ⊂ Θ, the following equality
Esup|f(Z,θ)| = Esup|f(Z,θ)|
θ∈Θ S∈U
holds. By the monotone convergence theorem, it suffices to assume that U is finite and get an
upper bound that is independent of the cardinality of U.
For each k ∈ Z, let U ⊂ U be a minimal ϵ -covering set of U where ϵ is defined by
k k k
ϵ = τ−1(2−k) (59)
k
Let N(ϵ ,U) be the cardinality of the minimal ϵ -covering set U of U. Since U is a subset of
k k k
Θ, N(ϵ ,U) can be upper bounded by
k
logN(ϵ ;U) = log|U | ≤ logN(ϵ ;Θ)
k k k
Since U is finite, there is a largest η ∈ Z and a smallest integer H ∈ Z such that
U = {θ } for some θ ∈ U, U = U
η 0 0 H
For each k ∈ Z, define the mapping π : Θ → U via
k k
π k(θ) = argmind(θ(cid:101),θ)
θ(cid:101)∈U
k
so that π (S) is the best approximation of θ ∈ Θ from the set U .
k k
For any θ ∈ U, apply the triangle inequality to see that
(cid:12) (cid:12) (cid:12) (cid:12)
E(cid:12) (cid:12)maxf(Z,θ)(cid:12) (cid:12) ≤ E(cid:12) (cid:12)max(f(Z,θ)−f(Z,θ 0))(cid:12) (cid:12)+E|f(Z;θ 0)|
(cid:12)θ∈U (cid:12) (cid:12)θ∈U (cid:12)
≤ Emax|f(Z,θ)−f(Z,θ )|+E|f(Z;θ )|
0 0
θ∈U
(cid:12) (cid:12)
(cid:12) H (cid:12)
( =i) Emax(cid:12) (cid:12) (cid:88) (f(Z,π k(θ))−f(Z,π k−1(θ)))(cid:12) (cid:12)+E|f(Z;θ 0)|
θ∈U (cid:12) (cid:12)
(60)
(cid:12)k=η+1 (cid:12)
H
(cid:88)
≤ Emax |f(Z,π (θ))−f(Z,π (θ))|+E|f(Z;θ )|
k k−1 0
θ∈U
k=η+1
H
(cid:88)
≤ Emax|f(Z,π (θ))−f(Z,π (θ))|+E|f(Z;θ )|
k k−1 0
θ∈U
k=η+1
Here (i) is a consequence of decomposing f(Z,θ)−f(Z,θ ) as a telescoping sum
0
H
(cid:88)
f(Z,θ)−f(Z,θ ) = (f(Z,π (θ))−f(Z,π (θ)))
0 k k−1
k=η+1
For any fixed θ ∈ Θ, one has
∥f(·,π (θ))−f(·,π (θ))∥ ≤ ∥f(·,π (θ))−f(·,θ)∥ +∥f(·,θ)−f(·,π (θ))∥
k k−1 ψ2 k ψ2 k−1 ψ2
(I)
≤ τ(ϵ )+τ(ϵ )
k k−1
43≤ 2τ(ϵ )
k−1
(I =I) 2·2−(k−1)
Here (I) is a result of (44), and (II) follows from (59). Then, one can obtain
(i)
(cid:112)
Emax|f(Z,π (θ))−f(Z,π (θ))| ≲ |U |·|U |·2·2−(k−1)
θ∈U k k−1 k−1 k (61)
(cid:112)
≲ logN(ϵ ;Θ)·2−(k−1)
k
Here (i) follows from the properties of the maximum of finitely many sub-Gaussian random
variablesandthefactthatthemaximumistakenoveratmost|U |·|U | ≤ N(ϵ ;Θ)2 random
k−1 k k
variables. Therefore,
H H
(cid:88) (cid:88) (cid:112)
Emax|f(Z,π (θ))−f(Z,π (θ))| ≲ logN(ϵ ;Θ)·2−(k−1)
k k−1 k
θ∈U
k=η+1 k=η+1
H (cid:113)
(cid:88)
= logN(τ−1(2−k);Θ)·2−(k−1)
(62)
k=η+1
(cid:90) ∞
(cid:112)
≲ logN(τ−1(t);Θ)·dt
0
(cid:90) τ(D)
(I) (cid:112)
= logN(τ−1(t);Θ)·dt
0
Here (I) follows by noticing that logN(τ−1(t);Θ) = 0 for any t ≥ τ(D).
Combine (60) and (62) to see that
(cid:90) τ(D)
(cid:112)
Esup|f(Z;θ)| ≤ C logN(τ−1(t);Θ)·dt+supE|f(Z;θ)|
θ∈Θ 0 θ∈Θ
for some constant C > 0.
Proof of (46): Consider two cases α ∈ (0,1] and α > 1.
0 0
Case I α ∈ (0,1]: Note that since α ∈ (0,1], one has
0 0
√ √
(cid:18) nt(cid:19) (cid:18) nt(cid:19)1/α0
τ−1(t) = ∧
K K
√ √
 nt nt ≥ K
= (cid:16)K √ nt(cid:17)1/α0 √
nt ≤ K
(63)
K
44As a result, one has
(cid:90) τ(D)
(cid:112)
logN(τ−1(t);Θ)·dt
0
√ (cid:115)
(i) 1 (cid:90) nτ(D) (cid:18) (cid:18) t (cid:19) (cid:19)
= √ logN τ−1 √ ;Θ ·dt
n n
0
√
(ii) 1 (cid:90) K(cid:113) (cid:0) (cid:1) 1 (cid:90) nτ(D) (cid:112)
= √ logN (t/K)1/α0;Θ ·dt+ √ logN (t/K;Θ)·dt
n n (64)
0 K
(cid:115) √ (cid:115)
1 (cid:90) K (cid:18) D (cid:19) 1 (cid:90) nτ(D) (cid:18) D (cid:19)
≲ √ log+ ·dt+ √ log+ ·dt
n
0
(t/K)1/α0 n
K
(t/K)
(cid:115) √ (cid:115)
(i =ii) √K (cid:90) 1 log+(cid:18) D (cid:19) ·dt+√K (cid:90) nτ(D)/K log+(cid:18) D(cid:19)
·dt
n
0
t1/α0 n
1
t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
a1 a2
Here (i), (iii) follows from a change of variables, and (ii) follows from (63).
• a : one has
1
(cid:115)
(i) (cid:90) 1 (cid:18) 1 (cid:19) (cid:90) 1(cid:113)
a ≤ log+ dt+ log+Ddt
1
0
t1/α0
0 (65)
(ii) (cid:113)
≲ log+D
√ √ √
Here (i) follows from the inequality s+t ≤ s+ t for s,t ≥ 0, and (ii) follows from the
assumption that D ≳ 1.
• a : for D ≳ 1,
2 √
(i) (cid:90) nτ(D)/K(cid:113)
a ≤ log+Ddt
2
1
√
(ii) nτ(D) (cid:113)
≤ · log+D
K (66)
(cid:113)
= (D∨Dα0) log+D
(iii) (cid:113)
≲ D log+D
Here (i) a consequence of the fact that D/t ≤ D for t ≥ 1, (ii) results from substituting the
definition of τ(·) and (iii) follows from the assumption α ∈ (0,1].
0
Combine (64), (65) and (66) to see that
(cid:90) τ(D) (cid:112) K (cid:113)
logN(τ−1(t);Θ)·dt ≲ √ D log+D
n
0
which proves (46) for α ∈ (0,1].
0
Case II α > 1: for α > 1, one has
0 0
√ √
(cid:18) nt(cid:19) (cid:18) nt(cid:19)1/α0
τ−1(t) = ∧
K K
√ √ (67)
 nt nt ≤ K
K
= (cid:16)√ nt(cid:17)1/α0 √
nt > K
K
45As a result, one can obtain
(cid:90) τ(D)
(cid:112)
logN(τ−1(t);Θ)·dt
0
√ (cid:115)
(i) 1 (cid:90) nτ(D) (cid:18) (cid:18) t (cid:19) (cid:19)
= √ logN τ−1 √ ;Θ ·dt
n n
0
√
(ii) 1 (cid:90) K (cid:112) 1 (cid:90) nτ(D)(cid:113) (cid:0) (cid:1)
= √ logN (t/K;Θ)·dt+ √ logN (t/K)1/α0;Θ ·dt
n n (68)
0 K
(cid:115) √ (cid:115)
1 (cid:90) K (cid:18) D (cid:19) 1 (cid:90) nτ(D) (cid:18) D (cid:19)
≲ √ log+ ·dt+ √ log+ ·dt
n
0
t/K n
K
(t/K)1/α0
(cid:115) √ (cid:115)
(i =ii) √K (cid:90) 1 log+(cid:18) D(cid:19) ·dt+√K (cid:90) nτ(D)/K log+(cid:18) D (cid:19)
·dt
n
0
t n
1
t1/α0
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
a3 a4
Here (i), (iii) follows from a change of variables, and (ii) follows from (67).
• a : one has
3
(cid:115)
(i) (cid:90) 1 (cid:18) 1(cid:19) (cid:90) 1(cid:113)
a ≤ log+ dt+ log+Ddt
3
0 t 0 (69)
(ii) (cid:113)
≲ log+D
√ √ √
Here (i) follows from the inequality s+t ≤ s+ t for s,t ≥ 0, and (ii) follows from the
assumption that D ≳ 1.
• a : for D ≳ 1,
4 √
(i) (cid:90) nτ(D)/K(cid:113)
a ≤ log+Ddt
4
1
√
(ii) nτ(D) (cid:113)
≤ · log+D
K (70)
(cid:113)
= (D∨Dα0) log+D
(iii) (cid:113)
≲ Dα0 log+D
Here(i)aconsequenceofthefactthatD/(cid:0) t1/α0(cid:1) ≤ Dfort ≥ 1,(ii)resultsfromsubstituting
the definition of τ(·) and (iii) follows from the assumption α > 1.
0
Combine (68), (69) and (70) to see that
(cid:90) τ(D) (cid:112) K (cid:113)
logN(τ−1(t);Θ)·dt ≲ √ Dα0 log+D
n
0
which proves (46) for α > 1.
0
46Proof of (47): Let Ψ (θ) := 1 (cid:80)n ψ(Z ;θ), then one has
n n i=1 i
(cid:13) (cid:13)
(cid:13)f(Zn;θ)−f(Zn;θ(cid:101))(cid:13)
(cid:13) 1 1 (cid:13)
ψ2
(cid:13) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
= (cid:13) (cid:13)|||Ψ n(θ)−EΨ n(θ)|||−(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ n(θ(cid:101))−EΨ n(θ(cid:101))(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:13)
(cid:13)
ψ2
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
≤ (cid:13) (cid:13)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ n(θ)−EΨ n(θ)−Ψ n(θ(cid:101))+EΨ n(θ(cid:101))(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:13)
(cid:13)
ψ2
(i) (cid:13)(cid:68) (cid:104) (cid:105) (cid:104) (cid:105) (cid:69)(cid:13)
≲ sup (cid:13)
(cid:13)
u, Ψ(cid:101)n(θ)−EΨ(cid:101)n(θ) − Ψ(cid:101)n(θ(cid:101))+Ψ(cid:101)n(θ(cid:101)) ·v⊗k (cid:13)
(cid:13)
u,v∈Sm−1 ψ2
(ii) 1 (cid:13)(cid:68) (cid:104) (cid:105) (cid:104) (cid:105) (cid:69)(cid:13)
≲ √ sup (cid:13) u, ψ(Z;θ)−ψ(Z;θ(cid:101)) −E ψ(Z;θ)−ψ(Z;θ(cid:101)) ·v⊗k (cid:13)
n u,v∈Sm−1(cid:13) (cid:13) ψ2
(iii) 1 (cid:13)(cid:68) (cid:104) (cid:105) (cid:69)(cid:13)
≲ √ sup (cid:13) u, ψ(Z;θ)−ψ(Z;θ(cid:101)) ·v⊗k (cid:13)
n u,v∈Sm−1(cid:13) (cid:13) ψ2
1 (cid:13)(cid:13)(cid:104) (cid:105) (cid:13) (cid:13)
≤ √ sup (cid:13)(cid:13) ψ(Z;θ)−ψ(Z;θ(cid:101)) ·v⊗k(cid:13) (cid:13)
n v∈Sm−1(cid:13)(cid:13) (cid:13) F(cid:13) ψ2
(iv) 1 (cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:13)
≤ √ sup (cid:13)(cid:12)(cid:12)(cid:12)ψ(Z;θ)−ψ(Z;θ(cid:101))(cid:12)(cid:12)(cid:12)·∥v∥k(cid:13)
n v∈Sm−1(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) F(cid:13) ψ2
1 (cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
≤ √ (cid:13)(cid:12)(cid:12)(cid:12)ψ(Z;θ)−ψ(Z;θ(cid:101))(cid:12)(cid:12)(cid:12)(cid:13)
n (cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13) ψ2
Here (i) is a result of Lemma 20, (ii) arises due to independence, (iii) follows from Lemma 2.6.8
in Vershynin (2018), and (iv) is derived from the definition of |||·|||. The proof is then complete.
B.3 Properties of F and Q∗
This section collects properties of F(·,·) and Q∗(·) that are needed later in the proof.
Lemma 28. Assume Assumption 1 and 2 holds. Then the following inequalities
γ (∥x−µ∥)−1I ⪯Q∗(x) ⪯ γ (∥x−µ∥)I , ∀x ∈ suppX (71)
1 d 1 d
hold for constant C > 0 large enough.
Proof. TheoptimalityconditionfortheconditionalFréchetmeanQ∗(x) := argmin E(cid:2) W2(S,Q)(cid:3)
S
and Lemma 17 imply that for any x, one has
(cid:16) (cid:17)
E TQ | X = x = I
Q∗(x) d
Recalling TQ = S−1/2(S1/2QS1/2)1/2S−1/2, one has
S
(cid:20) (cid:21)
(cid:16) (cid:17)1/2
Q∗(x) = E Q∗(x)1/2QQ∗(x)1/2 | X = x
Denote λ∗ (x) = λ (Q∗(x)) and λ∗ (x) = λ (Q∗(x)). Then one can obtain
max max min min
(cid:18) (cid:20) (cid:21)(cid:19)
(cid:16) (cid:17)1/2
λ∗ (x) = λ E Q∗(x)1/2QQ∗(x)1/2 | X = x
max max
(cid:20) (cid:18) (cid:19) (cid:21)
(cid:16) (cid:17)1/2
≤ E λ Q∗(x)1/2QQ∗(x)1/2 | X = x
max
≤ γ (∥x−µ∥)1/2λ∗ (x)1/2
1 max
47Here the second line follows from the convexity of the largest eigenvalue λ (·) over PSD
max
matrices. The last inequality follows from Assumption 2. Therefore, one can readily obtain
λ∗ (x) ≤ γ (∥x−µ∥). The lower bound λ∗ (x) ≥ γ (∥x−µ∥)−1 can be derived similarly by
max 1 min 1
the concavity of λ (·).
min
Lemma 29. Assume Assumption 1 and 2 hold. If X and Q are independent, then the following
holds.
• Q∗(x) ≡ Q∗(µ)
• Assumption 5 holds for C = 0 and c large enough.
3 3
• Assumption 4 holds for c = 1/(2c ), α = 2, C = 1 and some constant c large enough.
δ 1 1 2 2
Proof. Independence between X and Q implies that
F(x,S) = Es(x,X)W2(S,Q)
= Es(x,X)·EW2(S,Q)
= EW2(S,Q)
The existence and uniqueness of the minimizer of F(x,·) follow from Panaretos and Zemel
(2020, Proposition 3.2.3, Proposition 3.2.7). Moreover, since EW2(S,Q) does not depend on x,
we have Q∗(x) ≡ Q∗(µ). We denote Q∗ := Q∗(µ) from now on. Note that Lemma 28 implies
that Q∗ ∈ S (c−1,c )
d 1 1
Verification of Assumption 5: Again by independence, one has
(cid:104) (cid:105) (cid:104) (cid:105)
E −s(x,X)dTQ = E −dTQ
Q∗ Q∗
Therefore, one can obtain
(cid:16) (cid:104) (cid:105)(cid:17) (cid:16) (cid:104) (cid:105)(cid:17)
λ E −s(x,X)dTQ = λ E −dTQ
min Q∗ min Q∗
(i) λ1/2 (Q1/2Q∗Q1/2)
≥ E min ·λ2 (Q∗−1)
2 min
≳ Eλ1/2 (Q)
min
(ii)
≳ E[γ (∥X −µ∥)]−1/2
1
1
≳ E
1∨∥X −µ∥C1/2
(iii)
≥ c
eig
for some constant c small enough. Here (i) follows form Lemma 17, (ii) is a result of As-
eig
sumption 2 and (iii) is due to Assumption 1. The proof is then complete.
48VerificationofAssumption4: Inordertoverify(11). LetusdenoteQ′ = Q∗−1/2SQ∗−1/2.
Then (30) and (31) in Lemma 17 imply the following lower bound.
F(x,S)−F(x,Q∗)
=E(cid:2) W2(S,Q)−W2(Q∗,Q)(cid:3)
(cid:68) (cid:69) 2 (cid:68) (cid:69)
≥E I −TQ ,S −Q∗ +E −dTQ (S −Q∗),S −Q∗
d Q∗ (cid:16) 1+λ1/2 (Q′(x))(cid:17)2 Q∗ (72)
max
(i) 2
λ1/2 (cid:0) Q1/2Q∗Q1/2(cid:1)
(cid:13) (cid:13)2
≥E · min ·(cid:13)Q∗−1/2(S −Q∗)Q∗−1/2(cid:13)
(cid:16) 1+λ1/2 (Q′(x))(cid:17)2 2 (cid:13) (cid:13) F
max
Here (i) follows from the fact that E(TQ −I ) = 0 which is a consequence of the optimality
Q∗ d
condition and independence.
If we set c = c−1/2, then one has
δ 1
(cid:110) (cid:111) (cid:110) (cid:111)
S ∈ S++ : δ ≤ ∥S −Q∗∥ ≤ ∆ ⊂ S ∈ S++ : ∥S∥ ≤ c +∆
d op d op 1
which combined with (72) implies for any ∆ ≥ c ,
δ
inf F(x,S)−F(x,Q∗)
δ≤∥S−Q∗∥ ≤∆
op
2
λ1/2 (cid:0) Q1/2Q∗Q1/2(cid:1)
(cid:13) (cid:13)2
≥ inf E · min ·(cid:13)Q∗−1/2(S −Q∗)Q∗−1/2(cid:13)
δ≤∥S−Q∗∥ op≤∆ (cid:16) 1+λ1 m/ a2 x(Q′)(cid:17)2 2 (cid:13) (cid:13) F
(i) 1
≳ inf ·∥S −Q∗∥2
δ≤∥S−Q∗∥ ≤∆
(cid:0) 1+∆1/2(cid:1)2 F
op
1
≳ inf ·∥S −Q∗∥2
δ≤∥S−Q∗∥ ≤∆
(cid:0) 1+∆1/2(cid:1)2 op
op
1
≥c δ2
sep
∆
wherec > 0isaconstantsmallenough. Here(i)followsfromtheinequalitiesthatλ (Q′) ≲
sep max
λ (S) ≲ ∆, λ (cid:0) Q1/2Q∗Q1/2(cid:1) ≳ λ (Q) and Eλ (Q)1/2 ≳ 1.
max min min min
Combining results above, Assumption 4 holds with with c = 1/(2c ), α = 2, C = 1 and
δ 1 1 2
c = 1/c . The proof is then complete.
2 sep
Recall the definition of γ (·) from Assumption 2 and the definition of α ,c ,C from As-
1 1 δ 2
sumption4. DenoteM := γ (L)foranyL > 0. Withthesenotationsinplace, wenowpresent
L 1
the continuity theorem for Q∗(·) as follows.
Lemma 30 (Hölder continuity of Q∗(·)). Suppose Assumption 1-5 hold. Then for any L ≥ e
that satisfies M > c , any x,x ∈ B (L),
L δ (cid:101) µ
(cid:26) (cid:27)
∥Q∗(x)−Q∗(x)∥ ≤ C max
(cid:16) LC2MC2+1(cid:17)1/α1
∥x−x∥1/α1,LC2MC2+1∥x−x∥ (73)
(cid:101) H L (cid:101) L (cid:101)
holds as long as constant C is large enough.
H
Proof. Note that M ≥ 1 by the properties of γ in Assumption 2. The proof is divided into
L 1
3 steps.
491. First, we should that F(x,S) is Lipschitz in x for any fixed S ∈ S (M−1,M).
d
2. Next, the Lipschitzness in step 1 and Assumption 4 imply the local Hölder continuity of
Q∗(x) with respect to x.
3. Then, an argument based on linear interpolation between x and x implies Lipschitz conti-
(cid:101)
nuity for x,x separated far apart.
(cid:101)
Combining the 3 step above finally gives (73).
To start with, note that Lemma 28 implies
Q∗(x) ∈ S (M−1,M ), for any x ∈ B(µ,L)
d L L
Recall that F(x,S) = E(cid:2) s(x,X)W2(S,Q)(cid:3).
1. F is Lipschitz in x: for any x,x ∈ B (L), one can obtain
(cid:101) µ
|F(x,S)−F(x (cid:101),S)| = (cid:12) (cid:12)E(x−x (cid:101))Σ−1(X −µ)W2(S,Q)(cid:12) (cid:12)
≲ ∥x−x (cid:101)∥·E(cid:13) (cid:13)(X −µ)W2(S,Q)(cid:13) (cid:13)
(i)
≲ ∥x−x∥·E∥X −µ∥(λ (S)+λ (Q))
(cid:101) max max
= ∥x−x∥·[E∥X −µ∥λ (S)+E∥X −µ∥λ (Q)]
(cid:101) max max
(ii)
≲ ∥x−x∥(λ (S)+1) (74)
(cid:101) max
Here (i) follows from (29) in Lemma 17, and (ii) follows from the concentration of X
(Assumption 1) and boundedness of Q given X (Assumption 2).
2. Local Hölder continuity: for any x,x ∈ B (L), one has
(cid:101) µ
0 ≤F(x,Q∗(x))−F(x,Q∗(x))
(cid:101)
=(F(x,Q∗(x))−F(x,Q∗(x)))+(F(x,Q∗(x))−F(x,Q∗(x)))+(F (x,Q∗(x))−F(x,Q∗(x)))
(cid:101) (cid:101) (cid:101) (cid:101) (cid:101) (cid:101) (cid:101)
(cid:124) (cid:123)(cid:122) (cid:125)
≤0
≤|F(x,Q∗(x))−F(x,Q∗(x))|+|F (x,Q∗(x))−F(x,Q∗(x))|
(cid:101) (cid:101) (cid:101) (cid:101)
(i)
≲∥x−x∥·(λ (Q∗(x))+λ (Q∗(x))+2)
(cid:101) max max (cid:101)
(ii)
≤CM ∥x−x∥
L (cid:101)
(75)
provided that constant C > 0 is large enough. Here (i) follows from (74) and (ii) follows since
Q∗(x),Q∗(x) ∈ S (M−1,M ) for x,x ∈ B (L) by Lemma 28.
(cid:101) d L L (cid:101) µ
Note also that for any x,x ∈ B (L), one has ∥Q∗(x)−Q∗(x)∥ ≤ M . Recall the definition
(cid:101) µ (cid:101) L
of c ,α ,γ (·,·) from Assumption 4. Then for any x,x ∈ B (L) that satisfy
δ 1 2 (cid:101) µ
cα1
∥x−x∥ ≤ δ =: t (76)
(cid:101) 0
CM γ (L,M )
L 2 L
one can obtain
∥Q∗(x)−Q∗(x)∥ ≤ (CM γ (L,M ))1/α1 ·∥x−x∥1/α1 (77)
(cid:101) L 2 L (cid:101)
To see this, note that first we have ∥Q∗(x)−Q∗(x)∥ < c since otherwise Assumption 4 then
(cid:101) δ
implies that
cα1 (i)
F(x,Q∗(x))−F(x,Q∗(x)) > δ ≥ CM ∥x−x∥
(cid:101) L (cid:101)
γ (L,M )
2 L
50Here (i) results from (76). This is a contradiction with (75). Next applying Assumption 4 with
δ = ∥Q∗(x)−Q∗(x)∥ and ∆ = M gives
(cid:101) L
∥Q∗(x)−Q∗(x)∥α1
F(x,Q∗(x))−F(x,Q∗(x)) ≥ (cid:101)
(cid:101)
γ (L,M )
2 L
which combined with (74) implies (77).
3. x,x far apart: for any x,x ∈ B (L) such that ∥x−x∥ > t , let K := ⌈∥x−x∥/t ⌉ and
(cid:101) (cid:101) µ (cid:101) 0 (cid:101) 0
(cid:18) (cid:19)
k k
x := 1− x+ x, k = 0,...,K
k (cid:101)
K K
Then ∥x −x ∥ ≤ t , and one can apply (77) to see that
k+1 k 0
K−1
(cid:88)
∥Q∗(x)−Q∗(x)∥ ≤ ∥Q∗(x )−Q∗(x )∥
(cid:101) k+1 k
k=0
K−1
(cid:88)
≤ (CM γ (L,M ))1/α1 ·∥x −x ∥1/α1
L 2 L k+1 k
k=0
K−1 (78)
≤ (cid:88) (CM γ (L,M ))1/α1 ·t1/α1
L 2 L 0
k=0
= Kc
δ
∥x−x∥
≤ 2 (cid:101) c
δ
t
0
≲ M γ (L,M )·∥x−x∥
L 2 L (cid:101)
Here the last line follows since c is a constant defined in Assumption 4.
δ
Finally, combine (77) and (78) to see that for any x,x ∈ B (L), one has
(cid:101) µ
(cid:110) (cid:111)
∥Q∗(x)−Q∗(x)∥ ≲ max (M γ (L,M ))1/α1∥x−x∥1/α1,M γ (L,M )∥x−x∥
(cid:101) L 2 L (cid:101) L 2 L (cid:101)
C Proof of Theorem 6
The proof is mainly divided into two steps:
• First, a uniform slow rate is shown by exploiting closed-form expressions for the optimal
transport map and Assumption 4.
√
• Next, building upon the slow rate, a uniform fast( n) rate is derived by further analyzing
the Taylor expansion of the optimality condition.
Slow rate: Define the event
E := (cid:8) ∥X −µ∥ ≤ L,Q ∈ S (M−1,M ),i ∈ [n](cid:9) ,where L := C (cid:112) logn, M := γ (L)
0 i i d L L L L 1
(79)
We have P(E ) ≥ 1−O(n−100) as long as constant C > 0 is large enough. To see this, note
0 L
that Assumption 1 guarantees that
P(∥X −µ∥ ≤ L,i ∈ [n]) ≥ 1−O(n−100).
i
51Assumption 2 then implies that Q ∈ S (cid:0) M−1,M (cid:1) whenever ∥X −µ∥ ≤ L.
d L L
With the above results in hand, the following lemma shows the uniform consistency of
Q(cid:98)n(x) with a potentially slow rate; see Appendix C.1 for the proof.
Lemma 31. Instate the notations and assumptions in Theorem 6 and (79). Then under event
E 0, estimate Q(cid:98)n(x) satisfies
(cid:16) (cid:17)
sup λ
max
Q(cid:98)n(x) ⪯ M(cid:102)L (80)
x∈Bµ(L)
where M(cid:102)L := C M(cid:102)M LL4 for some constant C
M(cid:102)
large enough.
Moreover, there exists an event E ⊂ E with probability P(E ) ≥ 1−O(n−100) such that
1 0 1
under E , one has
1
(cid:13) (cid:13) polylog(n)
x∈s Bu µp (L)(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13)
≤
n1/(2α1)
(81)
Remark 32. We remind readers that in Lemma 31, α ≥ 1 is defined in Assumption 4.
1 √
Therefore, Lemma 31 might only lead to a uniform convergence rate slower than n. For
example, it is shown in Lemma 29 that α = 2 when X and Q are independent, which results
by (81) in a uniform convergence rate of n1/4. Such a show rate is not enough to derive the
asymptotic null distribution of our test statistic in Theorem 11. Therefore, we apply a finer
analysis in the following part of proof to further boost the uniform convergence rate to n1/2.
Fast rate: Recall definitions of Q∗(x) and Q(cid:98)n(x) that
Q∗(x) = argminE(cid:2) s(x,X)W2(S,Q)(cid:3)
S∈S++
d
n
1 (cid:88)
Q(cid:98)n(x) = argmin s n(x,X i)W2(S,Q i)
n
S∈S d++ i=1
First, the differential properties of W2 (Lemma 17) imply the following optimality conditions
for Q∗(x) and Q(cid:98)n(x).
(cid:16) (cid:17)
Es(x,X) TQ −I = 0 (82)
Q∗(x) d
n
1 (cid:88) s (x,X )(cid:16) TQi −I (cid:17) = 0 (83)
n i d
n Q(cid:98)n(x)
i=1
Next, one can apply Lemma 17 again to get the 2nd order Taylor expansion of (83) around
Q∗(x) as follows.
n n
0 = n1 (cid:88) s n(x,X i)(cid:16) T QQ ∗i (x)−I d(cid:17) + n1 (cid:88) s n(x,X i)dT QQ ∗i (x)·(cid:16) Q(cid:98)n(x)−Q∗(x)(cid:17)
i=1 i=1
n
+ 1 (cid:88) s n(x,X i)d2TQi ·(cid:16) Q(cid:98)n(x)−Q∗(x)(cid:17)⊗2
2n Q(cid:101)n(x)
i=1
52where Q(cid:101)n(x) lies between Q∗(x) and Q(cid:98)n(x). Rearranging then gives
 −1
 n  n
Q(cid:98)n(x)−Q∗(x) =   − n1 (cid:88) s n(x,X i)dT QQ ∗i (x) 

· n1 (cid:88) s n(x,X i)(cid:16) T QQ ∗i (x)−I d(cid:17)
 i=1  i=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(84)
=:Φn(x) =:An(x)
(cid:32) n (cid:33)−1 n
+ − n1 (cid:88) s n(x,X i)dT QQ ∗i
(x)
· 1
2
· n1 (cid:88) s n(x,X i)d2T QQ (cid:101)ni (x)·(Q(cid:98)n(x)−Q∗(x))⊗2
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
=:Ψn(x;Q(cid:101)n(x))
Taking Frobenius norm on both sides of (84) gives the following quadratic inequality for
(cid:13) (cid:13)
(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13)
.
F
(cid:13) (cid:13)
(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13)
F
≤(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)−Φ n(x)−1(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)·∥A n(x)∥ F+ 1
2
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)−Φ n(x)−1(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)·(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)Ψ n(x,Q(cid:101)(x))(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)·(cid:13) (cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13) (cid:13) (cid:13)2
F
(85)
To get fast uniform rate of convergence, let us pause and define the population counterparts of
A ,Φ and Ψ as follows.
n n n
n
A(cid:101)n(x) := n1 (cid:88) s(x,X i)(cid:16) T QQ ∗i (x)−I d(cid:17)
i=1
n
Φ(cid:101)n(x) := n1 (cid:88) s(x,X i)dT QQ ∗i
(x)
(86)
i=1
n
Ψ(cid:101)n(x;S) := n1 (cid:88) s(x,X i)d2T SQi
i=1
With the above definition in place, we demonstrate below the uniform concentration of
A(cid:101)n,Φ(cid:101)n,Ψ(cid:101)n in Lemma 33 as well as their small deviation from A n,Φ n,Ψ
n
in Lemma 34. The
proofs are deferred to Appendix C.2 and C.3 respectively.
Note that by (79) and (81) in Lemma 31, the event E defined in Lemma 31 satisfies
1
E ⊂ E ∩(cid:8) Q∗(x) ∈ S (cid:0) M−1,M (cid:1) ,∀x ∈ B (L)(cid:9)
1 0 d L L µ
(cid:110) (cid:111)
∩ Q(cid:98)n(x) ∈ S d(cid:0) (2M L)−1,2M L(cid:1) ,∀x ∈ B µ(L)
for n large enough.
Lemma 33. Instate the notations and assumptions in Theorem 6 and Lemma 31. Then there
exists an event E ⊂ E with probability P(E ) ≥ 1−O(n−100) under which
2 1 2
(cid:13) (cid:13) polylog(n)
x∈s Bu µp (L)(cid:13) (cid:13)A(cid:101)n(x)(cid:13)
(cid:13) F
≤ √
n
(87)
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) polylog(n)
sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x)−EΦ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
≤ √
n
(88)
x∈Bµ(L)
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) polylog(n)
sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(x;S)−EΨ(cid:101)n(x;S)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
≤ √
n
(89)
x∈Bµ(L)
S∈Sd((2ML)−1,2ML)
53(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)EΨ(cid:101)n(x;S)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
≤ polylog(n) (90)
x∈Bµ(L)
S∈Sd((2ML)−1,2ML)
hold.
Lemma 34. Instate the notations and assumptions in Theorem 6 and Lemma 31. Then the
following inequalities
(cid:13) (cid:13) polylog(n)
x∈s Bu µp (L)(cid:13) (cid:13)A n(x)−A(cid:101)n(x)(cid:13)
(cid:13) F
≲ √
n
(91)
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) polylog(n)
sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ n(x)−Φ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
≲ √
n
(92)
x∈Bµ(L)
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) polylog(n)
sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ n(x;S)−Ψ(cid:101)n(x;S)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
≲ √
n
(93)
x∈Bµ(L)
S∈Sd((2ML)−1,2ML)
hold with probability at least 1−O(n−100).
Consequences of Lemma 33, 34 are gathered in the following lemma, whose proof is given
in Appendix C.4.
Lemma 35. Instate the notations and assumptions in Theorem 6 and Lemma 31. Then one
has with probability at least 1−O(n−100) that
polylog(n)
sup ∥A (x)∥ ≤ √ (94)
n F n
x∈Bµ(L)
sup |||Ψ (x,S)||| ≤ polylog(n) (95)
n
x∈Bµ(L)
S∈Sd((2ML)−1,2ML)
1
inf λ (−Φ (x)) ≥ (96)
min n
x∈B polylog(n)
µ(L)
Thereby the operator −Φ is invertible and
n
sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)−Φ−1(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) ≤ polylog(n) (97)
n
x∈Bµ(L)
With Lemma 35 in place, one can then derive from (85) that the following quadratic in-
equality holds with probability at least 1−O(n−100) uniformly for any x ∈ B (L).
µ
(cid:13) (cid:13) (cid:13) (cid:13)2
(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13)
≤ a 0+a 2(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13)
F F
Here a ,a > 0 are uniform for any x ∈ B (L) and satisfies
0 2 µ
polylog(n)
a = √
0
n
a = polylog(n)
2
Therefore, taking the supremum over x gives
(cid:13) (cid:13) (cid:13) (cid:13)2
sup (cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13)
≤ a 0+a
2
sup (cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13)
x∈Bµ(L) F x∈Bµ(L) F
54(cid:13) (cid:13)
Solving the above quadratic inequality for sup x∈Bµ(L)(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13)
F
then gives
√ √
(cid:13) (cid:13) (cid:20) 1− 1−4a a (cid:21) (cid:20) 1+ 1−4a a (cid:19)
x∈s Bu µp (L)(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13) F
∈ 0,
2a 2
0 2 ∪
2a 2
0 2 ,+∞
Note that the slow rate of convergence (Lemma 31) suggests that with probability at least
1−O(n−100), only the smaller branch should be retained. Therefore, one has
√
(cid:13) (cid:13) 1− 1−4a a
x∈s Bu µp (L)(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13) F
≤
2a 2
0 2
4a a
0 2
= √
2a (1+ 1−4a a )
2 0 2
polylog(n)
≤ √
n
with probability at least 1−O(n−100). The proof is then complete.
C.1 Proof of Lemma 31
(cid:16) (cid:17)
Proof of (80): Denote λ(cid:98)max(x) = λ
max
Q(cid:98)n(x) . By the optimality condition for Q(cid:98)n(x),
one has
n
I = 1 (cid:88) s (X )TQi
d n i
n Q(cid:98)n(x)
i=1
Recall that TQ =
Q(cid:98)n(x)−1/2(cid:16) Q(cid:98)n(x)1/2QQ(cid:98)n(x)1/2(cid:17)1/2
Q(cid:98)n(x)−1/2, multiplying both sides by
Q(cid:98)n(x)
Q(cid:98)n(x)1/2(·)Q(cid:98)n(x)1/2 then gives
n
1 (cid:88) (cid:16) (cid:17)1/2
Q(cid:98)n(x) = s n(X i) Q(cid:98)n(x)1/2Q iQ(cid:98)n(x)1/2
n
i=1
Taking the largest eigenvalue on both sides, one has under E that
0
n (cid:13) (cid:13)
λ(cid:98)max(x) ≤ 1 (cid:88) |s n(X i)|·(cid:13) (cid:13)(cid:16) Q(cid:98)n(x)1/2Q iQ(cid:98)n(x)1/2(cid:17)1/2(cid:13) (cid:13)
n (cid:13) (cid:13)
i=1 op
n
≤ n1 (cid:88) |s n(X i)|·M L1/2 λ(cid:98) m1/ a2 x(x)
i=1
which implies
(cid:32) n (cid:33)2
1 (cid:88)
λ(cid:98)max(x) ≤ M
L
|s n(X i)|
n
i=1
(cid:32) n (cid:33)2
1 (cid:88)(cid:12) (cid:12)
= M (cid:12)1+(x−µ)⊤Σ−1(X −µ)(cid:12)
L n (cid:12) i (cid:12)
i=1
(cid:32) n (cid:33)2
1 (cid:88)
≲ M 1+∥x−µ∥· ∥X −µ∥
L i
n
i=1
55Taking supremum over x over both sides then gives
(cid:32) n (cid:33)2
1 (cid:88)
sup λ(cid:98)max(x) ≲ M
L
1+L· ∥X i−µ∥
n
x∈Bµ(L)
i=1
≤ C M(cid:102)M LL4 =: M(cid:102)L
for some constant C > 0 large enough.
M(cid:102)
Proof of (81): For convenience, we recall the definition of F(x,S), F (x,S) and define
n
F(cid:101)n(x,S) here.
F(x,S) = Es(x,X)W2(S,Q)
n
1 (cid:88)
F (x,S) = s (x,X )W2(S,Q )
n n i i
n
i=1
n
1 (cid:88)
F(cid:101)n(x,S) := s(x,X i)W2(S,Q i)
n
i=1
where W2(S,Q) = tr(cid:2) Q+S −2(S1/2QS1/2)1/2(cid:3).
To prove the convergence rate of Q(cid:98)n (81), observe that for any δ n,ϵ
n
> 0, one has
(cid:40) (cid:41)
(cid:13) (cid:13)
sup (cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13)
≤ δ
n
x∈Bµ(L)
(cid:26) (cid:27)
(i)
⊃ inf F (x,S) < inf F (x,S),∀x ∈ B (L)
n n µ
S:∥S−Q∗(x)∥≤δn S:∥S−Q∗(x)∥≥δn
(cid:26) (cid:27)
(ii) (cid:110) (cid:111)
⊃ F n(x,Q∗(x)) < inf F n(x,S),∀x ∈ B µ(L) ∩ Q(cid:98)n(x) ⪯ M(cid:102)LI d,∀x ∈ B µ(L)
S:∥S−Q∗(x)∥≥δn
 
 
(iii)   (cid:110) (cid:111)
⊃ F n(x,Q∗(x)) < inf F n(x,S),∀x ∈ B µ(L) ∩ Q(cid:98)n(x) ⪯ M(cid:102)LI d,∀x ∈ B µ(L)


S:∥S−Q∗(x)∥≥δn 

S⪯M(cid:102)LId
   
   
   
⊃ sup |F (x,S)−F(x,S)| ≤ ϵ ∩ F(x,Q∗(x)) ≤ inf F(x,S)−3ϵ ,∀x ∈ B (L)
n n n µ
 x∈Bµ(L)     S:∥S−Q∗(x)∥≥δn  
S⪯M(cid:102)LId S⪯M(cid:102)LId
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
E1 E2
(cid:110) (cid:111)
∩ Q(cid:98)n(x) ⪯ M(cid:102)LI d,∀x ∈ B µ(L) (98)
(cid:124) (cid:123)(cid:122) (cid:125)
E3
Here (i) and (iii) are due to Assumption 4 that Q(cid:98)n(x) is the unique minimizer of F n(x,·), (ii)
follows by letting S = Q∗(x).
With (98) in place, it suffices to choose appropriate δ ,ϵ such that E ∩E ∩E occurs with
n n 1 2 3
high probability. To this end, we first find the uniform convergence rate ϵ so that P(E ) =
n 1
1−O(n−100). Next, δ is set based on ϵ and properties of F so that P(E ) = 1−O(n−100).
n n 2
For E , we already showed in (80) that P(E ) ≥ 1−O(n−100). Combining results on E ,E and
3 3 1 2
E , the proof is then finished.
3
Analysis of E 1: for simplicity, denote θ = (x,S) and Θ = B µ(L) × S d(M(cid:102) L−1,M(cid:102)L). Let
F(θ) denote F(x,S) and similarly for other functions of (x,S). By triangle inequality, one can
56obtain
(cid:12) (cid:12) (cid:12) (cid:12)
sup|F n(θ)−F(θ)| ≤ sup(cid:12) (cid:12)F n(θ)−F(cid:101)n(θ)(cid:12) (cid:12)+sup(cid:12) (cid:12)F(cid:101)n(θ)−F(θ)(cid:12)
(cid:12)
(99)
θ∈Θ θ∈Θ θ∈Θ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ζ1 ζ2
For any t ,t > 0, define C (t ,t ) := sup(cid:8) W2(Q,S) : Q ∈ S (t−1,t ),S ∈ S (t−1,t )(cid:9).
1 2 sup 1 2 d 1 1 d 2 2
By Lemma 17, one has
C (t ,t ) ≲ t +t (100)
sup 1 2 1 2
Here dimension d is viewed as a fixed constant and absorbed into ≲.
• ζ 1: undereventE
0
(definedin(79)),onehasC sup(M L,M(cid:102)L) ≲ M(cid:102)L sinceM
L
≲ M(cid:102)L. Lemma
23thenimpliesthatthereexistsaneventE ⊂ E withprobabilityP(E ) ≥ 1−O(n−100)
1,1 0 1,1
under which ζ can be bounded as follows.
1
logn
ζ
1
≲ L2M(cid:102)L √ (101)
n
• ζ
2
can be bounded by truncation and the uniform concentration in Lemma 24. Let L(cid:101) ≥ e
be a parameter to be specified later, and let M
L(cid:101)
:= γ 1(L(cid:101)). Then Assumption 2 implies that
a.s.
(cid:16) (cid:17)
X ∈ B µ(L(cid:101)) =⇒ Q ∈ S
d
M L(cid:101)−1,M
L(cid:101)
(102)
Let X(cid:101) be a truncated form of X defined by
X(cid:101) := X(cid:101)(L(cid:101)) = (X −µ)I(∥X −µ∥ ≤ L(cid:101))+µ (103)
(cid:13) (cid:13)
Let X(cid:101)> := X −X(cid:101). By definition, one has (cid:13) (cid:13)X(cid:101) −µ(cid:13)
(cid:13)
≤ L(cid:101) and
(cid:40)
0, ∥X −µ∥ ≤ L(cid:101)
X(cid:101)> = (104)
X −µ, ∥X −µ∥ > L(cid:101)
With these definitions in hand, one has
X −µ = (X(cid:101) −µ)+(X −X(cid:101))
= (X(cid:101) −µ)+X(cid:101)>
which leads to the following decomposition.
s(x,X)W2(S,Q)
(cid:16) (cid:17)
= 1+(x−µ)⊤Σ−1(X(cid:101) −µ) W2(S,Q)+(x−µ)⊤Σ−1X(cid:101)>W2(S,Q)
(cid:16) (cid:17) (cid:16) (cid:16) (cid:17)(cid:17)
= 1+(x−µ)⊤Σ−1(X(cid:101) −µ) W2(S,Q)1 Q ∈ S M−1,M
L(cid:101) L(cid:101)
(cid:16) (cid:17) (cid:16) (cid:16) (cid:17)(cid:17)
+ 1+(x−µ)⊤Σ−1(X(cid:101) −µ) W2(S,Q)1 Q ∈/ S M−1,M
L(cid:101) L(cid:101)
+(x−µ)⊤Σ−1X(cid:101)>W2(S,Q)
( =i) (cid:16) 1+(x−µ)⊤Σ−1(X(cid:101) −µ)(cid:17) W2(S,Q)1(cid:16) Q ∈ S(cid:16) M−1,M (cid:17)(cid:17)
L(cid:101) L(cid:101)
(cid:16) (cid:16) (cid:17)(cid:17)
+W2(S,Q)1 Q ∈/ S M L(cid:101)−1,M
L(cid:101)
+(x−µ)⊤Σ−1X(cid:101)>W2(S,Q)
57(cid:16) (cid:17)
Here (i) follows from (102). Indeed, if Q ∈/ S M−1,M , then as a result of (102), one has
L(cid:101) L(cid:101)
∥X −µ∥ > L(cid:101) which then implies X(cid:101) −µ = 0.
As a consequence, one can obtain then following decomposition of F(cid:101)n(θ)−F(θ).
F(cid:101)n(θ)−F(θ) = α 1(θ)+α 2(θ)+α 3(θ), where
n
1 (cid:88)(cid:16) (cid:17) (cid:16) (cid:16) (cid:17)(cid:17)
α 1(θ) :=
n
1+(x−µ)⊤Σ−1(X(cid:101)i−µ) W2(S,Q i)I Q
i
∈ S M L(cid:101)−1,M
L(cid:101)
i=1
(cid:16) (cid:17) (cid:16) (cid:16) (cid:17)(cid:17)
−E 1+(x−µ)⊤Σ−1(X(cid:101) −µ) W2(S,Q)I Q ∈ S M−1,M
L(cid:101) L(cid:101)
n
1 (cid:88)
α 2(θ) := (x−µ)⊤Σ−1X(cid:101)i,>W2(S,Q i)−E(x−µ)⊤Σ−1X(cid:101)>W2(S,Q)
n
i=1
n
1 (cid:88) (cid:16) (cid:16) (cid:17)(cid:17) (cid:16) (cid:16) (cid:17)(cid:17)
α (θ) := W2(S,Q )I Q ∈/ S M−1,M −EW2(S,Q)I Q ∈/ S M−1,M
3 n i i L(cid:101) L(cid:101) L(cid:101) L(cid:101)
i=1
Then triangle inequality gives
ζ ≤ sup|α (θ)|+sup|α (θ)|+sup|α (θ)| (105)
2 1 2 3
θ∈Θ θ∈Θ θ∈Θ
– α (θ),α (θ): sup|α (θ)|andsup|α (θ)|can be upper bounded as in Claim 1. The proof
2 3 2 3
is deferred to Appendix C.1.1.
√
Claim 1. By taking L(cid:101) = C logn for some absolute constant C > 0 large enough,
L(cid:101) L(cid:101)
the following inequalities
√
logn
sup|α 2(θ)| ≲ M(cid:102)LL √
n
θ∈Θ
√
logn
sup|α 3(θ)| ≲ M(cid:102)L √
n
θ∈Θ
hold with probability at least 1−O(cid:0) n−100(cid:1) for n large enough.
– α (θ): After truncation, α (θ) is uniformly bounded. Hence sup |α (θ)| can be
1 1 θ∈Θ 1
upper bounded by applying Lemma 24. However, the term g(Q;S) := (Q1/2SQ1/2)1/2
in W2(Q,S) is only Hölder continuous in S which only implies Hölder (rather than
Lipschitz) continuity of the corresponding sub-Gaussian norm. The non-asymptotic
uniform concentration theorems in Wainwright (2019); Vershynin (2018) do not directly
√
apply. It turns out that n uniform convergence rate can still be attained under Hölder
continuity. We state results in Claim 2 whose proof is deferred to Appendix C.1.2.
Claim 2. Instate the notations and assumptions in Lemma 31 and Claim 1.
√
logn
sup|α 1(θ)| ≤ √
n
M(cid:102) L2
θ∈Θ
with probability at least 1−O(cid:0) n−100(cid:1).
– combine Claim 1, Claim 2 and (105) to see that
√
logn
ζ
2
≲ √
n
M(cid:102) L2 (106)
with probability at least 1−O(cid:0) n−100(cid:1).
58• combining upper bounds on ζ ,ζ (101) (106) with (99), one has
1 2
P(E ) ≥ 1−O(cid:0) n−100(cid:1)
1
by taking
√
logn
ϵ
n
= C δM(cid:102) L2 · √
n
for some constant C > 0 large enough.
δ
Analysis of E 2: Note that ∥S −Q∗(x)∥ ≤ max{∥S∥,∥Q∗(x)∥} ≤ M(cid:102)L, hence Assumption
4 implies that
inf F(x,S)−F(x,Q∗(x)) ≥ inf F(x,S)−F(x,Q∗(x))
S:∥S−Q∗(x)∥≥δn S:δn≤∥S−Q∗(x)∥≤M(cid:102)L
S⪯M(cid:102)LId
Therefore, by setting δ
n
=
(cid:104)
3ϵ nγ
2(L,M(cid:102)L)(cid:105)1/α1,
we have P(E 2) = 1 for n large enough.
Analysis of E : by (80) which is proved in the first half of the current lemma, one has
3
E ⊃ E (E is defined in (79)). Therefore, one has
3 0 0
P(E ) ≥ 1−O(n−100)
3
Combining E ,E ,E : Finally, combining results on E ,E and E above, one can obtain
1 2 3 1 2 3
P
 sup
(cid:13)
(cid:13)
(cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13) (cid:13) ≤
C(cid:34)
M(cid:102)
L2√
log √nγ
n2(L,M(cid:102)L)(cid:35)1/α1
 ≥ 1−O(cid:0) n−100(cid:1) (107)
x∈Bµ(L)
as long as constant C is large enough. Note that by definition, L,M(cid:102)L,γ 2(L,M(cid:102)L) = polylog(n),
the proof is then complete.
59C.1.1 Proof of Claim 1
Proof for α (θ): a crude upper bounded suffices here. By triangle inequality, one has
2
sup|α (θ)|
2
θ∈Θ
(cid:12) (cid:12)
n
(cid:12) (cid:12) (cid:12)1 (cid:88) (cid:12)
≤sup(cid:12) (cid:12)E(x−µ)⊤Σ−1X(cid:101)>W2(S,Q)(cid:12) (cid:12)+sup(cid:12)
(cid:12)n
(x−µ)⊤Σ−1X(cid:101)i,>W2(S,Q i)(cid:12)
(cid:12)
θ∈Θ θ∈Θ(cid:12) (cid:12)
i=1
n
(cid:13) (cid:13) 1 (cid:88) (cid:13) (cid:13)
≲sup∥x−µ∥·E(cid:13) (cid:13)X(cid:101)>(cid:13) (cid:13)·W2(S,Q)+sup
n
∥x−µ∥·(cid:13) (cid:13)X(cid:101)i,>(cid:13) (cid:13)·W2(S,Q i)
θ∈Θ θ∈Θ
i=1
(i) (cid:34) (cid:13) (cid:13) (cid:16) (cid:17) 1 (cid:88)n (cid:13) (cid:13) (cid:16) (cid:17)(cid:35)
≲L E(cid:13) (cid:13)X(cid:101)>(cid:13) (cid:13)· M(cid:102)L+γ 1(∥X −µ∥) +
n
(cid:13) (cid:13)X(cid:101)i,>(cid:13) (cid:13)· M(cid:102)L+γ 1(∥X i−µ∥)
i=1
(cid:34) n (cid:35)
(cid:13) (cid:13) (cid:16) (cid:17) 1 (cid:88)(cid:13) (cid:13) (cid:16) (cid:17)
≲L E(cid:13) (cid:13)X(cid:101)>(cid:13) (cid:13)· M(cid:102)L+(∥X −µ∥∨1)C1 +
n
(cid:13) (cid:13)X(cid:101)i,>(cid:13) (cid:13)· M(cid:102)L+(∥X i−µ∥∨1)C1
i=1
(cid:34) n (cid:35)
(cid:13) (cid:13) (cid:16) (cid:17) 1 (cid:88)(cid:13) (cid:13) (cid:16) (cid:17)
≲L E(cid:13) (cid:13)X(cid:101)>(cid:13) (cid:13)· M(cid:102)L+∥X −µ∥C1 +
n
(cid:13) (cid:13)X(cid:101)i,>(cid:13) (cid:13)· M(cid:102)L+∥X i−µ∥C1
i=1
=L(cid:34) E(cid:13)
(cid:13)
(cid:13)X(cid:101)>(cid:13)
(cid:13)
(cid:13)·(cid:18) M(cid:102)L+(cid:13)
(cid:13)
(cid:13)X(cid:101)>(cid:13)
(cid:13)
(cid:13)C1(cid:19)
+
n1 (cid:88)n (cid:13)
(cid:13)
(cid:13)X(cid:101)i,>(cid:13)
(cid:13)
(cid:13)·(cid:18) M(cid:102)L+(cid:13)
(cid:13)
(cid:13)X(cid:101)i,>(cid:13)
(cid:13)
(cid:13)C1(cid:19)(cid:35)
i=1
   
 n   n 
=LM(cid:102)L  2E(cid:13) (cid:13) (cid:13)X(cid:101)>(cid:13) (cid:13) (cid:13)+ n1 (cid:88) ∥X i,>∥−E(cid:13) (cid:13) (cid:13)X(cid:101)>(cid:13) (cid:13) (cid:13)  +L  2E(cid:13) (cid:13) (cid:13)X(cid:101)>(cid:13) (cid:13) (cid:13)1+C1 + n1 (cid:88) ∥X i,>∥1+C1 −E(cid:13) (cid:13) (cid:13)X(cid:101)>(cid:13) (cid:13) (cid:13)1+C1 

 (cid:124) (cid:123)(cid:122) (cid:125) i=1   (cid:124) (cid:123)(cid:122) (cid:125) i=1 
α2,1 (cid:124) (cid:123)(cid:122) (cid:125) α2,3 (cid:124) (cid:123)(cid:122) (cid:125)
α2,2 α2,4
Here (i) follows from Lemma 17 and Assumption 2.
• α ,α : It suffices to bound E∥X ∥1+c for c ≥ 0 and let c = 0,C respectively. Recall
2,1 2,3 > 1
the properties of X(cid:101)> in (104) that
(cid:40)
0, ∥X −µ∥ ≤ L(cid:101)
X(cid:101)> =
X −µ, ∥X −µ∥ > L(cid:101)
Then for any x > 0, one has
(cid:110)(cid:13) (cid:13) (cid:111) (cid:110) (cid:111)
(cid:13) (cid:13)X(cid:101)>(cid:13)
(cid:13)
> x = ∥X −µ∥ > (L(cid:101)∨x) (108)
First, note that by Lemma 2.6.8 in Vershynin (2018), one has ∥X −µ∥ ≲ ∥X∥ ≤ C .
ψ2 ψ2 ψ2
Then apply (38) to see that there exists constant C > 0 only depending on C such that
ψ2
60for any constant c ≥ 0, the following inequalities holds.
(cid:13) (cid:13)1+c (cid:90) ∞ (cid:16)(cid:13) (cid:13) (cid:17)
E(cid:13) (cid:13)X(cid:101)>(cid:13)
(cid:13)
= (1+c) xcP (cid:13) (cid:13)X(cid:101)>(cid:13)
(cid:13)
> x dx
0
( =i) (1+c)(cid:90) L(cid:101) xcP(cid:16) ∥X −µ∥ > L(cid:101)(cid:17) dx+(1+c)(cid:90) ∞ xcP(∥X −µ∥ > x)dx
0 L(cid:101)
(cid:32) (cid:33)
L(cid:101)2 (cid:90) ∞ (cid:18) x2(cid:19)
≤ 2L(cid:101)1+cexp − +2(1+c) xcexp − dx
C C
L(cid:101)
(cid:32) (cid:33) (cid:32) (cid:33)
L(cid:101)2 (cid:90) ∞ (L(cid:101)+s)2
= 2L(cid:101)1+cexp − +2(1+c) (L(cid:101)+s)1+cexp − ds
C C
0
(cid:32) (cid:33) (cid:32) (cid:33)
(ii) L(cid:101)2 (cid:90) ∞(cid:16) (cid:17) L(cid:101)2+s2
≲ L(cid:101)1+cexp − + L(cid:101)1+c+s1+c exp − ds
C C
0
(cid:32) (cid:33) (cid:32) (cid:33)
L(cid:101)2 L(cid:101)2 (cid:90) ∞(cid:16) (cid:17) (cid:18) s2(cid:19)
= L(cid:101)1+cexp − +exp − L(cid:101)1+c+s1+c exp − ds
C C C
0
(cid:32) (cid:33)
L(cid:101)2 (cid:16) (cid:17)
≲ exp − L(cid:101)1+c+1
C
Here (i) follows from (108), (ii) is a result of the convexity of x1+c.
By letting c = 0,C , one has
1
(cid:32) (cid:33)
(cid:13) (cid:13) L(cid:101)2 (cid:16) (cid:17)
α
2,1
= E(cid:13) (cid:13)X(cid:101)1,>(cid:13)
(cid:13)
≲ exp −
C
L(cid:101)+1
(cid:32) (cid:33)
α
2,3
=
E(cid:13)
(cid:13)
(cid:13)X(cid:101)1,>(cid:13)
(cid:13)
(cid:13)1+C1
≲ exp
−L(cid:101) C2 (cid:16)
L(cid:101)1+C1
+1(cid:17)
Therefore, for any L(cid:101) ≳ 1, one has
(cid:32) (cid:33)
L(cid:101)2
α 2,1∨α
2,3
≲ exp − L(cid:101)1+C1
C
√
By taking L(cid:101) = C logn for C large enough, one can obtain
L(cid:101) L(cid:101)
1
α ∨α ≲ (109)
2,1 2,3 n100
for n large enough.
• α ,α : For the constant C ≥ 0, by definition of ∥·∥ in (37), one has
2,2 2,4 1 ψα
(cid:13) (cid:13)
(cid:13) (cid:13)(cid:13)
(cid:13)
(cid:13)X(cid:101)>(cid:13)
(cid:13)
(cid:13)1+C1(cid:13)
(cid:13) =
(cid:13)
(cid:13)
(cid:13)(cid:13)
(cid:13)
(cid:13)X(cid:101)>(cid:13)
(cid:13)
(cid:13)(cid:13)
(cid:13)
(cid:13)1+C1
(cid:13) (cid:13)
ψ 2(1+C1)−1
ψ2
≲ ∥∥X∥∥1+C1
ψ2
≲ 1
Then by letting c = 0,C , Lemma 19,implies that for n large enough,
1
√
logn
α ∨α ≲ √
2,2 2,4
n
with probability at least 1−O(cid:0) n−100(cid:1).
61• Combine the results above to see that
√
logn
sup|α 2(θ)| ≲ LM(cid:102)L √ (110)
n
θ∈Θ
with probability at least 1−O(cid:0) n−100(cid:1) for n large enough.
Proof for α (θ): Apply triangle inequality to see that
3
sup|α (θ)|
3
θ∈Θ
(cid:12) (cid:12)
n
(cid:12) (cid:16) (cid:16) (cid:17)(cid:17)(cid:12) (cid:12)1 (cid:88) (cid:16) (cid:16) (cid:17)(cid:17)(cid:12)
≤sup(cid:12)EW2(S,Q)1 Q ∈/ S M−1,M (cid:12)+sup(cid:12) W2(S,Q )1 Q ∈/ S M−1,M (cid:12)
θ∈Θ(cid:12) L(cid:101) L(cid:101) (cid:12) θ∈Θ(cid:12) (cid:12)n i i L(cid:101) L(cid:101) (cid:12)
(cid:12)
i=1
n
(cid:16) (cid:16) (cid:17)(cid:17) 1 (cid:88) (cid:16) (cid:16) (cid:17)(cid:17)
=supEW2(S,Q)1 Q ∈/ S M−1,M +sup W2(S,Q )1 Q ∈/ S M−1,M
θ∈Θ
L(cid:101) L(cid:101)
θ∈Θ
n i i L(cid:101) L(cid:101)
i=1
n
(cid:16) (cid:17) 1 (cid:88) (cid:16) (cid:17)
≤supEW2(S,Q)1 X ∈/ B µ(L(cid:101)) +sup W2(S,Q i)1 X
i
∈/ B µ(L(cid:101))
n
θ∈Θ θ∈Θ
i=1
(i) (cid:16) (cid:17) (cid:16) (cid:17) 1 (cid:88)n (cid:16) (cid:17) (cid:16) (cid:17)
≲E M(cid:102)L+∥X −µ∥C1 1 X ∈/ B µ(L(cid:101)) + M(cid:102)L+∥X i−µ∥C1 1 X
i
∈/ B µ(L(cid:101))
n
i=1
n
(cid:16) (cid:17) (cid:16) (cid:17) 1 (cid:88)(cid:16) (cid:17) (cid:16) (cid:17)
≤E M(cid:102)L+(∥X −µ∥+1)C1∨1 1 X ∈/ B µ(L(cid:101)) + M(cid:102)L+(∥X i−µ∥+1)C1∨1 1 X
i
∈/ B µ(L(cid:101))
n
i=1
(ii) (cid:16) (cid:17) (cid:16) (cid:17) 1 (cid:88)n (cid:16) (cid:17) (cid:16) (cid:17)
≲E M(cid:102)L+1+∥X −µ∥C1∨1 1 X ∈/ B µ(L(cid:101)) + M(cid:102)L+1+∥X i−µ∥C1∨1 1 X
i
∈/ B µ(L(cid:101))
n
i=1
n
(cid:16) (cid:17) (cid:16) (cid:17) 1 (cid:88)(cid:16) (cid:17) (cid:16) (cid:17)
≲E M(cid:102)L+∥X −µ∥C1∨1 1 X ∈/ B µ(L(cid:101)) + M(cid:102)L+∥X i−µ∥C1∨1 1 X
i
∈/ B µ(L(cid:101))
n
i=1
n
1 (cid:88) (cid:16) (cid:17)
=M(cid:102)LE1(X ∈/ B µ(L(cid:101)))+M(cid:102)L· 1 X
i
∈/ B µ(L(cid:101))
n
(cid:124) (cid:123)(cid:122) (cid:125)
i=1
=:α3,1
(cid:124) (cid:123)(cid:122) (cid:125)
=:α3,2
(cid:16) (cid:17)
+2E∥X −µ∥C1∨1I X ∈/ B µ(L(cid:101))
(cid:124) (cid:123)(cid:122) (cid:125)
=:α3,3
n
1 (cid:88) (cid:16) (cid:17) (cid:16) (cid:17)
+ ∥X i−µ∥C1∨1I X
i
∈/ B µ(L(cid:101)) −E∥X −µ∥C1∨1I X ∈/ B µ(L(cid:101))
n
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
=:α3,4
Here (i) follows from the fact that S ∈ S d(M(cid:102) L−1,M(cid:102)L), Q ∈ S d(γ 1(∥X −µ∥)−1,γ 1(∥X −µ∥))
by Assumption 2 and (100). (ii) arises due to √the convexity of x1∨C1. Note that X ∈/ B µ(L(cid:101)) if
and only if ∥X −µ∥ ≥ L(cid:101). By taking L(cid:101) = C logn for C large enough, the following holds.
L(cid:101) L(cid:101)
• α : By Lemma 20 and centering,
3,1
∥∥X −µ∥∥ ≲ ∥X −µ∥
ψ2 ψ2
≲ ∥X∥
ψ2
Then by the definition of ∥·∥ , one can obtain
ψα
(cid:32) (cid:33)
L(cid:101)2
α ≤ 2exp −
3,1
CC
ψ2
62Hence
1
α ≤ (111)
3,1 n101
for n large enough.
• α : by Chernoff’s inequality (Theorem 2.3.1, Vershynin (2018)), one has
3,2
(cid:18) (cid:19)
1
P α ≥ ≤ e−nα3,1 ·enα
3,2 3,1
n
≤ enα
3,1
As a result, (111) then implies that
1
α <
3,2
n
with probability at least 1−O(n−100) for n large enough.
• α : similar to α ,α , one has
3,3 2,1 2,3
1
α ≲
3,3 n100
• α : similar to α ,α , one has
3,4 2,2 2,4 √
logn
α ≲ √
3,4
n
with probability at least 1−O(n−100).
• Combine results above to see that
√
logn
sup|α 3(θ)| ≲ M(cid:102)L √ (112)
n
θ∈Θ
with probability at least 1−O(cid:0) n−100(cid:1) for n large enough.
Finally, combining (110) and (112) then proves Claim 1.
C.1.2 Proof of Claim 2
√
For convenience, recall that L,L(cid:101) ≍ logn,M
L
:= γ 1(L), M(cid:102)L ≍ M LL4 and M
L(cid:101)
:= γ 1(L(cid:101)).
Denote Z = (X,Q) and define X(cid:101) as in (103). Define
(cid:16) (cid:17) (cid:16) (cid:16) (cid:17)(cid:17)
f(Z;θ) := 1+(x−µ)⊤Σ−1(X(cid:101) −µ) W2(S,Q)1 Q ∈ S M−1,M
L(cid:101) L(cid:101)
Then one has α (θ) = n−1(cid:80)n f(Z ;θ)−Ef(Z;θ). Apply triangle inequality to see that
1 i=1 i
sup|α (θ)| = sup|α (θ)|−Esup|α (θ)|+Esup|α (θ)|
1 1 1 1
θ∈Θ θ∈Θ θ∈Θ θ∈Θ (113)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
b1 b2
Analysis of b : due to truncation, f is uniformly bounded. To see this, note that recall the
1
definition of Θ,
θ ∈ Θ ⇐⇒ x ∈ B µ(L),S ∈ S d(M(cid:102) L−1,M(cid:102)L)
63Then for any θ ∈ Θ, one has
(cid:12) (cid:12) (cid:16) (cid:16) (cid:17)(cid:17)
s θ∈u Θpsu Zp|f(Z;θ)| = s θ∈u Θpsu Zp(cid:12) (cid:12)(1+(x−µ)⊤Σ−1(X(cid:101) −µ))W2(Q,S)(cid:12) (cid:12)1 Q ∈ S
d
M L(cid:101)−1,M
L(cid:101)
≲ s θ∈u Θpsu Zp(cid:16) 1+∥x−µ∥·(cid:13) (cid:13) (cid:13)X(cid:101) −µ(cid:13) (cid:13) (cid:13)(cid:17) ·(cid:12) (cid:12)W2(Q,S)(cid:12) (cid:12)1(cid:16) Q ∈ S d(cid:16) M L(cid:101)−1,M L(cid:101)(cid:17)(cid:17)
(cid:16) (cid:17)
≲ 1+LL(cid:101) ·(M
L(cid:101)
+M(cid:102)L)
≲ LL(cid:101)M(cid:102)L
(114)
By Lemma 26, one has ∥b 1(x)∥
ψ2
≲ LL √(cid:101)M(cid:102) nL which implies
√
b ≲
LL(cid:101)M(cid:102)
√L
logn
(115)
1
n
with probability at least 1−O(n−100).
Analysis of b : to apply Lemma 24, we follow the steps below.
2
• first we define a metric d on Θ by
(cid:110) (cid:13) (cid:13) (cid:111)
d(θ,θ(cid:101)) = max ∥x−x (cid:101)∥ 2,(cid:13) (cid:13)S −S(cid:101)(cid:13)
(cid:13)
F
(cid:110) (cid:111)
foranyθ = (x,S)andS(cid:101)= (x (cid:101),S(cid:101)). SinceS d(M(cid:102) L−1,M(cid:102)L)isasubsetof S ∈ Rd×d : |S ij| ≤ M(cid:102)L
(by noticing that for any S ∈ S+ , one has 0 ≤ S
ii
≤ M(cid:102)L and |S ij| ≤ (cid:112) S iiS
jj
≤ M(cid:102)L), the
diameter D and metric entropyM(cid:102) oLf Θ can be upper bounded by
(cid:110) (cid:111) (i)
D ≤ max diam(B µ(L)),diam(S d(M(cid:102) L−1,M(cid:102)L)) ≲ L∨M(cid:102)L ≲ M(cid:102)L (116)
and
(cid:16) (cid:17)
logN(ϵ;Θ) ≤ log N(ϵ;B (L))·N(ϵ;S+ )
µ
M(cid:102)L
(cid:32) (cid:33)
≲ log+
L∨M(cid:102)L
ϵ
(cid:32) (cid:33)
( ≲ii)
log+
M(cid:102)L
(117)
ϵ
Here both (i) and (ii) arise due to M(cid:102)L ≍ M LL4.
• Next, let us consider the sub-Gaussian norm of α 1(θ)−α 1(θ(cid:101)) as (44) in Lemma 24. By (47)
in Lemma 24, one has
(cid:13) (cid:13) 1 (cid:13)(cid:12) (cid:12)(cid:13)
(cid:13) (cid:13)α 1(θ)−α 1(θ(cid:101))(cid:13)
(cid:13) ψ2
≲ √
n
(cid:13) (cid:13)(cid:12) (cid:12)f(Z;θ)−f(Z;θ(cid:101))(cid:12) (cid:12)(cid:13)
(cid:13) ψ2
1 (cid:13)(cid:12) (cid:12)(cid:13)
= √
n
(cid:13) (cid:13)(cid:12) (cid:12)f(Z;x,S)−f(Z;x (cid:101),S(cid:101))(cid:12) (cid:12)(cid:13)
(cid:13) ψ2
1 (cid:13)(cid:12) (cid:12)(cid:13) 1 (cid:13)(cid:12) (cid:12)(cid:13)
≤ √
n
(cid:13) (cid:13)(cid:12) (cid:12)f(Z;x,S)−f(Z;x,S(cid:101))(cid:12) (cid:12)(cid:13)
(cid:13)
ψ2+√
n
(cid:13) (cid:13)(cid:12) (cid:12)f(Z;x,S(cid:101))−f(Z;x (cid:101),S(cid:101))(cid:12) (cid:12)(cid:13)
(cid:13) ψ2
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
β1 β2
(118)
64– β : Let g(Q;S) := (Q1/2SQ1/2)1/2. Then for any Q ∈ S (M−1,M ),
1 d L(cid:101) L(cid:101)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)g(Q;S)−g(Q;S(cid:101))(cid:13) = (cid:13)(Q1/2SQ1/2)1/2−(Q1/2S(cid:101)Q1/2)1/2(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
F F
(cid:13) (cid:13)1/2
≲ (cid:13)Q1/2SQ1/2−Q1/2S(cid:101)Q1/2(cid:13)
(cid:13) (cid:13)
F
(cid:13) (cid:13)1/2
≤ (cid:13)S −S(cid:101)(cid:13) ∥Q∥1/2
(cid:13) (cid:13) op
F
(cid:13) (cid:13)1/2
≤ M1/2(cid:13)S −S(cid:101)(cid:13)
(cid:13) (cid:13)
L(cid:101) F
Here in the second inequality, we exploit the following the Hölder continuity of matrix
square root in Wihler (2009); Carlsson (2018).
(cid:13) (cid:13)
(cid:13)A1/2−B1/2(cid:13) ≤ C ∥A−B∥1/2
(cid:13) (cid:13) d F
F
where C
d
is a constant only depending on dimension d. Hence for any θ,θ(cid:101)∈ Θ,
(cid:13)(cid:12) (cid:12)(cid:13)
β
1
= (cid:13) (cid:13)(cid:12) (cid:12)f(X,Q;x,S)−f(X,Q;x,S(cid:101))(cid:12) (cid:12)(cid:13)
(cid:13)
ψ2
(cid:13)(cid:12) (cid:12)(cid:13)
≤ (cid:13)(cid:12)1+(x−µ)⊤Σ−1(X(cid:101) −M u)(cid:12)(cid:13)
(cid:13)(cid:12) L(cid:101) (cid:12)(cid:13)
ψ2
(cid:13)(cid:16) (cid:17) (cid:16) (cid:16) (cid:17)(cid:17)(cid:13)
·(cid:13)
(cid:13)
g(Q;S)−g(Q;S(cid:101)) ·1 Q ∈ S
d
M L(cid:101)−1,M
L(cid:101)
(cid:13)
(cid:13)
F
(cid:18) (cid:13)(cid:13) (cid:13) (cid:13) (cid:19) (cid:13) (cid:13)1/2
≲ 1+|x−µ|·(cid:13)(cid:13)X(cid:101) −µ(cid:13) (cid:13) ·M1/2(cid:13)S −S(cid:101)(cid:13)
(cid:13)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
2 ψ2 L(cid:101) F
(cid:13) (cid:13)1/2
≲ L·M1/2(cid:13)S −S(cid:101)(cid:13) (119)
(cid:13) (cid:13)
L(cid:101) F
– β 2: for any θ,θ(cid:101)∈ Θ, one has
(cid:13) (cid:16) (cid:16) (cid:17)(cid:17)(cid:13)
β
2
= (cid:13) (cid:13)(x−x (cid:101))⊤Σ−1(X(cid:101) −µ)W2(Q,S)1 Q ∈ S
d
M L(cid:101)−1,M
L(cid:101)
(cid:13)
(cid:13)
ψ2
(i) (cid:13) (cid:13) (cid:16) (cid:17)
≲ ∥x−x (cid:101)∥(cid:13) (cid:13)X(cid:101) −µ(cid:13)
(cid:13)
ψ2
· M
L(cid:101)
+M(cid:102)L
≲ M(cid:102)L∥x−x (cid:101)∥
2
(120)
Here (i) follows from (100).
– As a result of (119) and (120), one can obtain
(cid:13) (cid:13)
(cid:13) (cid:13)f(Z;θ)−f(Z;θ(cid:101))(cid:13)
(cid:13)
≤ β 1+β
2
ψ2
≲ LM1/2 d(θ,θ(cid:101))1/2+M(cid:102)Ld(θ,θ(cid:101))
L(cid:101)
(cid:16) (cid:17)
≲ LM(cid:102)L d(θ,θ(cid:101))1/2∨d(θ,θ(cid:101)) (121)
Combining (118) and (121) then gives
(cid:13)
(cid:13) (cid:13)α 1(θ)−α
1(θ(cid:101))(cid:13)
(cid:13)
(cid:13)
ψ2
≲
L √M(cid:102)
nL
(cid:16) d(θ,θ(cid:101))1/2∨d(θ,θ(cid:101))(cid:17)
Hence τ(ϵ) in Lemma 24 can be chosen as
K (cid:16) (cid:17)
τ(ϵ) = √ ϵ1/2∨ϵ , K = C fLM(cid:102)L (122)
n
65• by Lemma 24, one has
KD(cid:113)
b := Esup|α (Z;θ)| ≲ √ log+D+supE|α (Z;θ)|
2 1 1
n
θ∈Θ θ∈Θ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ζ1 ζ2
– ζ : by (46) in Lemma 24 and (116), one can obtain
1
LM(cid:102)2(cid:113)
ζ
1
≲ √ L log+M(cid:102)L (123)
n
– ζ : For any θ ∈ Θ, we have
2
∥f(Z;θ)∥ ≤ ∥|f(Z;θ)|∥
ψ2 ψ2
(cid:13)(cid:12) (cid:12)(cid:13)
≤ (cid:13)(cid:12)1+(x−µ)⊤Σ−1(X(cid:101) −µ)(cid:12)(cid:13)
(cid:13)(cid:12) (cid:12)(cid:13)
ψ2
(cid:104) (cid:16) (cid:16) (cid:17)(cid:17)(cid:105)
· sup W2(Q,S)1 Q ∈ S M−1,M
S∈S d(M(cid:102) L−1,M(cid:102)L)
d L(cid:101) L(cid:101)
(cid:18) (cid:13)(cid:13) (cid:13)(cid:13) (cid:19) (cid:16) (cid:17)
≲ 1+∥x−µ∥(cid:13) (cid:13)(cid:13) (cid:13)X(cid:101) −µ(cid:13) (cid:13)(cid:13)
(cid:13)
ψ2
· M
L(cid:101)
+M(cid:102)L
≲ LM(cid:102)L
√
Therefore for any θ ∈ Θ, one has ∥α 1(θ)∥
ψ2
≲ LM(cid:102)L/ n and
E|α (Z;θ)| ≲ ∥α (θ)∥ ≲
L √M(cid:102)L
1 1 ψ2 n
Take supremum over θ ∈ Θ to see that
ζ ≲
L √M(cid:102)L
(124)
2
n
– As a result of (123) and (124), one can obtain
b
2
≲
L √M(cid:102) L2(cid:113)
log+M(cid:102)L (125)
n
Finally, combine (113), (115) and (125) to see that
√
logn
sup|α 1(θ)| ≤ b 1+b
2
≲ √
n
LM(cid:102) L2 (126)
θ∈Θ
with probability at least 1−O(cid:0) n−100(cid:1) for n large enough. The proof is then complete.
C.2 Proof of Lemma 33
If X is unbounded, then one can apply the truncation as (103) and follow similar arguments as
in Lemma 31. Hence we can assume without loss of generality that ∥X −µ∥ ≤ L(cid:101) almost surely
√
with L(cid:101) = C logn for some constant C large enough. Moreover, note that all the constants
L(cid:101) L(cid:101) √
in Claim 1 are independent of L (recall that L := C logn is defined in (79)), hence one can
L
further assume without loss of generality that L(cid:101) = L by possibly enlarging either C
L
or C L(cid:101).
Then we have a.e.
∥X −µ∥ ≤ L
Q,Q∗(X) ∈ S (M−1,M ) (127)
d L L
1(E ) = 1
0
where we remind reader that E := (cid:8) ∥X −µ∥ ≤ L,Q ∈ S (M−1,M ),i ∈ [n](cid:9) is defined in
0 i i d L L
(79).
66Proof of (87): one can obtain the following decomposition.
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
sup (cid:13) (cid:13)A(cid:101)n(x)(cid:13)
(cid:13)
≤ sup (cid:13) (cid:13)A(cid:101)n(x)(cid:13)
(cid:13)
−E sup (cid:13) (cid:13)A(cid:101)n(x)(cid:13)
(cid:13)
+E sup (cid:13) (cid:13)A(cid:101)n(x)(cid:13)
(cid:13)
x∈Bµ(L) F x∈Bµ(L) F x∈Bµ(L) F x∈Bµ(L) F (128)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
a1 a2
(cid:16) (cid:17)
Define φ(Z;x) = s(x,X) TQ −I .
Q∗(x) d
Analysis of a : By (127), one has almost surely that for any x ∈ B (L),
1 µ
(cid:13) (cid:13)
∥φ(Z;x)∥ ≲ (1+∥x−µ∥·∥X −µ∥)· sup (cid:13)TQ−I (cid:13)
F
Q,Q(cid:101)∈S(M
L−1,ML)(cid:13) Q(cid:101) d(cid:13) F
(i)
≲ L2M2
L
Here (i) follows by noticing that
(cid:13) (cid:13) (cid:13) (cid:13)
sup (cid:13)TQ(cid:13) = sup (cid:13)Q(cid:101)−1/2(Q(cid:101)1/2QQ(cid:101)1/2)1/2Q(cid:101)−1/2(cid:13) ≤ M2
(cid:13) (cid:13) (cid:13) (cid:13) L
Q,Q(cid:101)∈S(M L−1,ML) Q(cid:101) op Q,Q(cid:101)∈S(M L−1,ML) op
Lemma 26 then implies that ∥a 1∥
ψ2
≲ L √2M nL2. Therefore, one can obtain
√
logn
a ≲ L2M2 √ (129)
1 L n
with probability at least 1−O(n−100).
Analysis of a : to apply Lemma 24, we follow the steps below.
2
(cid:13) (cid:13) (cid:13) (cid:13)
• First, we consider the sub-Gaussian norm of (cid:13) (cid:13)A(cid:101)n(x)(cid:13)
(cid:13)
−(cid:13) (cid:13)A(cid:101)n(x (cid:101))(cid:13)
(cid:13)
as (44) in Lemma 24.
F F
For any x,x ∈ B (L), one has
(cid:101) µ
(cid:13)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)(cid:13) (cid:13)A(cid:101)n(x)(cid:13)
(cid:13)
−(cid:13) (cid:13)A(cid:101)n(x (cid:101))(cid:13)
(cid:13)
(cid:13)
(cid:13)
≤ (cid:13) (cid:13)(cid:13) (cid:13)A(cid:101)n(x)−A(cid:101)n(x (cid:101))(cid:13)
(cid:13)
(cid:13)
(cid:13)
(130)
F F ψ2 F ψ2
Note that
n
A(cid:101)n(x)−A(cid:101)n(x (cid:101)) =
n1 (cid:88)(cid:104)
s(x,X
i)(cid:16)
T QQ ∗i (x)−I
d(cid:17)
−s(x (cid:101),X
i)(cid:16)
T QQ ∗i
(x
(cid:101))−I
d(cid:17)(cid:105)
i=1
Since EA(cid:101)n(x) = 0 by (82), one has
(cid:13)(cid:13) (cid:13) (cid:13) (i) 1 (cid:13)(cid:13) (cid:16) (cid:17) (cid:16) (cid:17)(cid:13) (cid:13)
(cid:13) (cid:13)(cid:13) (cid:13)A(cid:101)n(x)−A(cid:101)n(x (cid:101))(cid:13)
(cid:13)
F(cid:13)
(cid:13) ψ2
≲ √
n
(cid:13) (cid:13)(cid:13) (cid:13)s(x,X) T QQ ∗(x)−I
d
−s(x (cid:101),X) T QQ
∗(x
(cid:101))−I
d
(cid:13)
(cid:13)
F(cid:13)
(cid:13) ψ2
1 (cid:13)(cid:13) (cid:16) (cid:17) (cid:16) (cid:17)(cid:13) (cid:13)
≤ √ (cid:13)(cid:13)s(x,X) TQ −I −s(x,X) TQ −I (cid:13) (cid:13)
n (cid:13)(cid:13) Q∗(x) d Q∗(x (cid:101)) d (cid:13) F(cid:13) ψ2
(cid:124) (cid:123)(cid:122) (cid:125)
b1
1 (cid:13)(cid:13) (cid:16) (cid:17) (cid:16) (cid:17)(cid:13) (cid:13)
+ √ (cid:13)(cid:13)s(x,X) TQ −I −s(x,X) TQ −I (cid:13) (cid:13)
n (cid:13)(cid:13) Q∗(x (cid:101)) d (cid:101) Q∗(x (cid:101)) d (cid:13) F(cid:13) ψ2
(cid:124) (cid:123)(cid:122) (cid:125)
b2
(131)
Here (i) follows from (47) in Lemma 24.
67– b : For any x,x ∈ B (L), one has Q∗(x),Q∗(x) ∈ S (M−1,M ). Then one can obtain
1 (cid:101) µ (cid:101) d L L
(cid:13)(cid:13) (cid:16) (cid:17)(cid:13) (cid:13)
b = (cid:13)(cid:13)s(x,X) TQ −TQ (cid:13) (cid:13)
1 (cid:13)(cid:13) Q∗(x) Q∗(x) (cid:13) (cid:13)
(cid:101) F ψ2
(cid:13) (cid:13) (cid:13) (cid:13)
≲ (cid:13)|1+∥x−µ∥·∥X −µ∥|·(cid:13)TQ −TQ (cid:13) (cid:13)
(cid:13) (cid:13) Q∗(x) Q∗(x)(cid:13) (cid:13)
(cid:101) F ψ2
(cid:13) (cid:13)
≲ ∥|1+∥x−µ∥·∥X −µ∥|∥ · sup (cid:13)TQ −TQ (cid:13)
ψ2
Q∈S(M
L−1,ML)(cid:13) Q∗(x) Q∗(x (cid:101))(cid:13)
F
(cid:13) (cid:13)
≲ L· sup (cid:13)TQ −TQ (cid:13)
(cid:13) Q∗(x) Q∗(x)(cid:13)
Q∈S d(M L−1,ML) (cid:101) F
(cid:13) (cid:13)
( =i) L· sup (cid:13)dTQ ·(Q∗(x)−Q∗(x))(cid:13)
(cid:13) Q′ (cid:101) (cid:13)
Q∈S d(M L−1,ML) F
(ii) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
≤ L·∥Q∗(x)−Q∗(x)∥ · sup (cid:12)(cid:12)(cid:12)dTQ(cid:12)(cid:12)(cid:12)
(cid:101) F (cid:12)(cid:12)(cid:12) S (cid:12)(cid:12)(cid:12)
Q,S∈S(M L−1,ML)
(iii)
≲ LM3 ·∥Q∗(x)−Q∗(x)∥ (132)
L (cid:101) F
Here (i) is a consequence of the mean value theorem (Dudley and Norvaiša, 2011, The-
orem 5.3) for some Q′ that lies between Q∗(x) and Q∗(x), (ii) arises from Lemma 18,
(cid:101)
and (iii) follows from Lemma 17.
– b :
2
(cid:13) (cid:13)
b 2 ≤ (cid:13) (cid:13)(cid:12) (cid:12)(x−x (cid:101))Σ−1(X −µ)(cid:12) (cid:12)(cid:13) (cid:13) ψ2 Q,S∈Ss (Mup L−1,ML)(cid:13) (cid:13)T SQ−I d(cid:13) (cid:13)
F
(i)
≲ ∥x−x∥∥∥X −µ∥∥ M2
(cid:101) ψ2 L
(ii)
≲ M2∥x−x∥ (133)
L (cid:101)
almost surely. Here (i) arises due to Lemma 20, and (ii) follows from bounds on TQ in
S
Lemma 17.
Combining (130), (131), (132) and (133) gives that for any x,x ∈ B (L),
(cid:101) µ
(cid:13)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1
(cid:13) (cid:13)(cid:13) (cid:13)A(cid:101)n(x)(cid:13)
(cid:13)
F−(cid:13) (cid:13)A(cid:101)n(x (cid:101))(cid:13)
(cid:13)
F(cid:13)
(cid:13) ψ2
≲ √
n
·LM L3(∥x−x (cid:101)∥+∥Q∗(x)−Q∗(x (cid:101))∥ F)
(i) 1 (cid:16) (cid:16) (cid:17)(cid:17)
≲ √ ·LM3 ∥x−x∥+M γ (L,M ) ∥x−x∥ ∨∥x−x∥1/α1
n L (cid:101) L 2 L (cid:101) 2 (cid:101)
(ii) C (cid:16) (cid:17)
≤ √ ·L1+C2M4+C2 ∥x−x∥ ∨∥x−x∥1/α1
n L (cid:101) 2 (cid:101)
(134)
Here (i) follows from Lemma 30 and the fact that Mγ (L,M) ≥ 1.
2
• With the Hölder continuity (134) in place, we can apply Lemma 24 with τ(ϵ) chosen as
K (cid:16) (cid:17)
τ(ϵ) = √ ϵ1/α1 ∨ϵ , K = CL1+C2M4+C2
n L
which gives
a
2
≲
KL √1∨ nα− 11(cid:113)
log+L+ x∈s Bu µp
(L)E(cid:13)
(cid:13)
(cid:13)A(cid:101)n(x)(cid:13)
(cid:13)
(cid:13)
F
68( =i) K √ nL(cid:113) log+L+ x∈s Bu µp (L)E(cid:13) (cid:13) (cid:13)A(cid:101)n(x)(cid:13) (cid:13)
(cid:13)
F
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
ζ1
ζ2
Here (i) follows since α ≥ 1 as defined in Assumption 4.
1
– ζ : direct calculation gives that
1
L2+C2M4+C2(cid:113)
ζ ≲ √ L log+L (135)
1
n
– ζ : for any U ∈ Rd×d with unit Frobenius norm, one has
2
(cid:13)(cid:13) (cid:13) (cid:13) (i) (cid:13)(cid:68) (cid:69)(cid:13)
sup (cid:13) (cid:13)(cid:13) (cid:13)A(cid:101)n(x)(cid:13)
(cid:13)
(cid:13)
(cid:13)
≲ sup sup (cid:13)
(cid:13)
U,A(cid:101)n(x) (cid:13)
(cid:13)
x∈Bµ(L) F ψ2 x∈Bµ(L)∥U∥ F=1 ψ2
(ii) (cid:13) (cid:13)(cid:42) 1 (cid:88)n (cid:43)(cid:13) (cid:13)
≲ sup sup (cid:13) U, φ(Z ;x)−Eφ(Z;x) (cid:13)
(cid:13) n i (cid:13)
x∈Bµ(L)∥U∥ F=1(cid:13)
i=1
(cid:13)
ψ2
1
≲ sup sup √ ∥⟨U,φ(Z;x)⟩∥
n ψ2
x∈Bµ(L)∥U∥ F=1
1
≤ sup √ ∥∥φ(Z;x)∥ ∥
n F ψ2
x∈Bµ(L)
1 (cid:13) (cid:13)
≤ √ sup ∥|s(x,X)|∥ · sup (cid:13)TQ−I (cid:13)
n x∈Bµ(L) ψ2 Q,S∈S(M L−1,ML)(cid:13) S d(cid:13) F
1
≲ √ L·M2
n L
Here (i) results from Lemma 20, (ii) follows from independence. Therefore,
LM2
ζ ≲ √ L (136)
2
n
• As a result of (135) and (136), one can obtain
L2+C2M4+C2(cid:113)
a ≲ √ L log+L (137)
2
n
Finally, combining (128), (129) and (137) gives (87).
Proof of (88): one can obtain the following decomposition.
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x)−EΦ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
≤ sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x)−EΦ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)−E sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x)−EΦ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
x∈Bµ(L) x∈Bµ(L) x∈Bµ(L)
(cid:124) (cid:123)(cid:122) (cid:125)
a3
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
+E sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x)−EΦ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
x∈Bµ(L)
(cid:124) (cid:123)(cid:122) (cid:125)
a4
(138)
Let Z = (X,Q). Define ϕ(Z;x) = s(x,X)dTQ .
Q∗(x)
Analysis of a : By (127), one has almost surely that for any x ∈ B (L),
3 µ
|||ϕ(Z;x)||| ≲ L2· sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)dTS(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)
Q
Q,S∈S d(M L−1,ML)
≲ L2M3
L
69By Lemma 26, one then can obtain
L2M3
∥a ∥ ≲ √ L (139)
3 ψ2 n
Analysis of a : to apply Lemma 24, we follow the steps below.
4
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
• First, let us consider the sub-Gaussian norm of (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x)−EΦ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)−(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x (cid:101))−EΦ(cid:101)n(x (cid:101))(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12).
For any x,x ∈ B (L), by (47) in Lemma 24, one can obtain
(cid:101) µ
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13) 1
(cid:13) (cid:13)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x)−EΦ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)−(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x (cid:101))−EΦ(cid:101)n(x (cid:101))(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:13)
(cid:13) ψ2
≲ √
n
∥|||ϕ(Z;x)−ϕ(Z;x (cid:101))|||∥
ψ2
(140)
Moreover,
∥|||ϕ(Z;x)−ϕ(Z;x)|||∥
(cid:101) ψ2
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
= (cid:13)(cid:12)(cid:12)(cid:12)s(x,X)dTQ −s(x,X)dTQ (cid:12)(cid:12)(cid:12)(cid:13)
(cid:13)(cid:12)(cid:12)(cid:12) Q∗(x) (cid:101) Q∗(x)(cid:12)(cid:12)(cid:12)(cid:13)
(cid:101) ψ2
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13) (cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
≤ (cid:13)(cid:12)(cid:12)(cid:12)s(x,X)dTQ −s(x,X)dTQ (cid:12)(cid:12)(cid:12)(cid:13) +(cid:13)(cid:12)(cid:12)(cid:12)s(x,X)dTQ −s(x,X)dTQ (cid:12)(cid:12)(cid:12)(cid:13)
(cid:13)(cid:12)(cid:12)(cid:12) Q∗(x) Q∗(x)(cid:12)(cid:12)(cid:12)(cid:13) (cid:13)(cid:12)(cid:12)(cid:12) Q∗(x) (cid:101) Q∗(x)(cid:12)(cid:12)(cid:12)(cid:13)
(cid:101) ψ2 (cid:101) (cid:101) ψ2
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
b3 b4
(141)
– b : we have for any x,x ∈ B (L),
3 (cid:101) µ
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
b ( =i) (cid:13)(cid:12)(cid:12)(cid:12)s(x,X)d2TQ ·(Q∗(x)−Q∗(x))(cid:12)(cid:12)(cid:12)(cid:13)
3 (cid:13)(cid:12)(cid:12)(cid:12) Q′ (cid:101) (cid:12)(cid:12)(cid:12)(cid:13)
ψ2
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
≲ L∥X −µ∥ ·∥Q∗(x)−Q∗(x)∥ · sup (cid:12)(cid:12)(cid:12)d2TQ(cid:12)(cid:12)(cid:12)
ψ2 (cid:101) F (cid:12)(cid:12)(cid:12) S (cid:12)(cid:12)(cid:12)
Q,S∈S d(M L−1,ML)
(ii)
≲ L·poly(M )∥Q∗(x)−Q∗(x)∥ (142)
L (cid:101) F
Here (i) is a consequence of the mean value theorem (Dudley and Norvaiša, 2011, The-
orem 5.3) for some Q′ that lies between Q∗(x) and Q∗(x) and (ii) follows from Lemma
(cid:101)
17.
– b : for any x,x ∈ B (L),
4 (cid:101) µ
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
b ≤ ∥|s(x,X)−s(x,X)|∥ sup (cid:12)(cid:12)(cid:12)dTQ(cid:12)(cid:12)(cid:12)
4 (cid:101) ψ2 (cid:12)(cid:12)(cid:12) S (cid:12)(cid:12)(cid:12)
Q,S∈S(M L−1,ML)
≲ M3∥x−x∥ (143)
L (cid:101) 2
Combining (140), (141), (142) and (143), one can obtain that
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
(cid:13) (cid:13)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x)−EΦ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)−(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x (cid:101))−EΦ(cid:101)n(x (cid:101))(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:13)
(cid:13)
ψ2
1
≲ √ ·Lpoly(M )(∥x−x∥+∥Q∗(x)−Q∗(x)∥ )
n L (cid:101) (cid:101) F
1 (cid:16) (cid:17)
≲ √ ·poly(L,M ) ∥x−x∥∨∥x−x∥1/α1
L (cid:101) (cid:101)
n
• With the Hölder continuity above, we can apply Lemma 24 with τ(ϵ) chosen as
K (cid:16) (cid:17)
τ(ϵ) = √ ϵ1/α1 ∨ϵ , K = poly(L,M )
L
n
70which gives
KL(cid:113) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
a
4
≲ √
n
log+L+ sup E(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
x∈Bµ(L)
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
ζ3
ζ4
– ζ : direct calculation gives
3
poly(L,M )
ζ ≲ √ L (144)
3
n
– ζ : for any U,V ∈ Rd×d with unit Frobenius norm, one has
4
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13) (i) (cid:13)(cid:68) (cid:69)(cid:13)
sup (cid:13) (cid:13)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:13)
(cid:13)
≲ sup sup (cid:13)
(cid:13)
V,Φ(cid:101)n(x)·U (cid:13)
(cid:13)
x∈Bµ(L) ψ2 x∈Bµ(L)∥U∥ F=∥V∥ F=1 ψ2
(ii) 1
≲ sup sup √ ∥⟨V,[ϕ(Z;x)−Eϕ(Z;x)]·U⟩∥
n ψ2
x∈Bµ(L)∥U∥ F=∥V∥ F=1
1
≲ sup sup √ ∥⟨V,ϕ(Z;x)·U⟩∥
n ψ2
x∈Bµ(L)∥U∥ F=∥V∥ F=1
1
≤ sup √ ∥|||ϕ(Z;x)|||∥
n ψ2
x∈Bµ(L)
1 (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
≤ √ sup ∥|s(x,X)|∥ · sup (cid:12)(cid:12)(cid:12)dTQ(cid:12)(cid:12)(cid:12)
n ψ2 (cid:12)(cid:12)(cid:12) S (cid:12)(cid:12)(cid:12)
x∈Bµ(L) Q,S∈S(M L−1,ML)
(iii) 1
≲ √ LM3
n L
Here(i)resultsfromLemma20,(ii)followsfromindependenceand(iii)isduetoLemma
17. Hence
LM3
ζ ≲ √ L (145)
4
n
• As a result of (144) and (145), one can obtain
poly(L,M )
a ≲ √ L (146)
4
n
Finally, combining (138), (139) and (146) gives (88).
Proof of (89): Denote Θ = B (L)×S ((2M )−1,2M ) and θ = (x,S) ∈ Θ.
µ d L L
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
sup(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(θ)−EΨ(cid:101)n(θ)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
≤ sup(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(θ)−EΨ(cid:101)n(θ)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)−Esup(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(θ)−EΨ(cid:101)n(θ)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
θ∈Θ θ∈Θ θ∈Θ
(cid:124) (cid:123)(cid:122) (cid:125)
a5 (147)
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
+Esup(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(θ)−EΨ(cid:101)n(θ)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
θ∈Θ
(cid:124) (cid:123)(cid:122) (cid:125)
a6
Let Z = (X,Q). Define ψ(Z;θ) = s(x,X)d2TQ and ψ(Z;θ) = ψ(Z;θ)−Eψ(Z;θ)
S
Analysis of a : By (127), one has almost surely that
5
sup|||ψ(Z;θ)||| ≤ sup |s(x,X)|· sup
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)d2TS(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
Q
Z,θ x,X∈Bµ(L) Q,S∈S((2ML)−1,2ML)
≲ L2·poly(M )
L
71Lemma 26 then implies that , ∥a 5∥
ψ2
≲ L2·po √ly n(ML). Therefore, one can obtain
√
logn
a ≲ L2·poly(M ) √ (148)
5 L
n
with probability at least 1−O(n−100).
Analysis of a : to apply Lemma 24, we follow the steps below.
6
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
• First, let us consider the sub-Gaussian norm of (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(θ)−EΨ(cid:101)n(θ)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)−(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(θ(cid:101))−EΨ(cid:101)n(θ(cid:101))(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12).
For any θ,θ(cid:101)∈ Θ, one has
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13) (i) 1 (cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
(cid:13) (cid:13)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(θ)−EΨ(cid:101)n(θ)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)−(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(θ(cid:101))−EΨ(cid:101)n(θ(cid:101))(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:13)
(cid:13) ψ2
≲ √
n
(cid:13) (cid:13)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)ψ(Z;θ)−ψ(Z;θ(cid:101))(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:13)
(cid:13) ψ2
(149)
Here (i) follows from Lemma 24. Moreover, one has
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
(cid:13)(cid:12)(cid:12)(cid:12)ψ(Z;θ)−ψ(Z;θ(cid:101))(cid:12)(cid:12)(cid:12)(cid:13)
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
ψ2
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
= (cid:13)(cid:12)(cid:12)(cid:12)s(x,X)d2TQ−s(x,X)d2TQ(cid:12)(cid:12)(cid:12)(cid:13)
(cid:13)(cid:12)(cid:12)(cid:12) S (cid:101) S(cid:101)(cid:12)(cid:12)(cid:12)(cid:13) ψ2 (150)
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13) (cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
≤ (cid:13)(cid:12)(cid:12)(cid:12)s(x,X)d2TQ−s(x,X)d2TQ(cid:12)(cid:12)(cid:12)(cid:13) +(cid:13)(cid:12)(cid:12)(cid:12)s(x,X)d2TQ−s(x,X)d2TQ(cid:12)(cid:12)(cid:12)(cid:13)
(cid:13)(cid:12)(cid:12)(cid:12) S S(cid:101)(cid:12)(cid:12)(cid:12)(cid:13) ψ2 (cid:13)(cid:12)(cid:12)(cid:12) S(cid:101) (cid:101) S(cid:101)(cid:12)(cid:12)(cid:12)(cid:13) ψ2
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
b5 b6
– b 5: for any θ,θ(cid:101)∈ Θ, one can obtain
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13)
b
5
( =i) (cid:13) (cid:13)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)s(x,X)d3T QQ
′
·(S −S(cid:101))(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:13)
(cid:13)
ψ2
(cid:13) (cid:13) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
≤ ∥|s(x,X)|∥ ·(cid:13)S −S(cid:101)(cid:13) · sup (cid:12)(cid:12)(cid:12)d3TQ(cid:12)(cid:12)(cid:12)
ψ2 (cid:13) (cid:13)
F Q∈Sd(M L−1,ML)
(cid:12)(cid:12)(cid:12) S (cid:12)(cid:12)(cid:12)
S∈Sd((2ML)−1,2ML)
(ii) (cid:13) (cid:13)
≲ L·poly(M L)(cid:13) (cid:13)S −S(cid:101)(cid:13)
(cid:13)
(151)
F
Here (i) is a consequence of the mean value theorem (Dudley and Norvaiša, 2011, The-
orem 5.3) for some Q′ that lies between Q∗(x) and Q∗(x) and (ii) follows from Lemma
(cid:101)
17.
– b 6: for any θ,θ(cid:101)∈ Θ, one has
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
b ≤ ∥|s(x,X)−s(x,X)|∥ sup (cid:12)(cid:12)(cid:12)d2TQ(cid:12)(cid:12)(cid:12)
6 (cid:101) ψ2 (cid:12)(cid:12)(cid:12) S (cid:12)(cid:12)(cid:12)
Q∈Sd(M L−1,ML)
S∈Sd((2ML)−1,2ML)
≲ poly(M )∥x−x∥ (152)
L (cid:101)
Combining (140), (150), (151) and (152), one can obtain
(cid:13)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:13) C
(cid:13) (cid:13)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(θ)−EΨ(cid:101)n(θ)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)−(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(θ(cid:101))−EΨ(cid:101)n(θ(cid:101))(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:13)
(cid:13) ψ2
≲ √
n
··poly(L,M L)d(θ,θ(cid:101))
• With the Lipschitz continuity above, we can apply Lemma 24 with τ(ϵ) chosen as
K
τ(ϵ) = √ ϵ, K = C ·poly(L,M )
L
n
which gives
KL(cid:113) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
a
6
≲ √
n
log+L+supE(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(θ)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
θ∈Θ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ζ5 ζ6
72– ζ : direct calculation gives
5
poly(L,M )
ζ ≲ √ L (153)
5
n
– ζ : for any U,V ∈ Rd×d with unit Frobenius norm, one has
6
(cid:13)(cid:68) (cid:69)(cid:13) 1
(cid:13)
(cid:13)
V,Ψ(cid:101)n(θ)·U (cid:13)
(cid:13) ψ2
≲ √
n
∥⟨V,[ψ(Z;θ)−Eψ(Z;θ)]·U⟩∥
ψ2
1
≲ √ ∥⟨V,ψ(Z;θ)·U⟩∥
n ψ2
1
≤ √ ∥|||ψ(Z;θ)|||∥
n ψ2
1 (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
≤ √ ∥|s(x,X)|∥ · sup (cid:12)(cid:12)(cid:12)d2TQ(cid:12)(cid:12)(cid:12)
n ψ2 (cid:12)(cid:12)(cid:12) S (cid:12)(cid:12)(cid:12)
Q∈Sd(M L−1,ML)
S∈Sd((2ML)−1,2ML)
1
≲ √ L·poly(M )
L
n
which combined with Lemma 20 implies
Lpoly(M )
ζ ≲ √ L (154)
6
n
• As a result of (153) and (154), one can obtain
poly(L,M )
a ≲ √ L (155)
6
n
with probability at least 1−O(n−100).
Finally, combining (147), (148) and (155) gives (89).
Proof of (90): by definition, one has
(cid:104) (cid:105)
EΨ(cid:101)n(x;S) = E
(X,Q)
s(x,X)d2T SQ
Therefore, by the truncation assumption (127), one can obtain
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)EΨ(cid:101)n(x;S)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
x∈Bµ(L)
S∈Sd((2ML)−1,2ML)
 
(cid:34) (cid:35)
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
≤ sup E|s(x,X)| ·  sup (cid:12)(cid:12)(cid:12)d2TQ(cid:12)(cid:12)(cid:12) 
 (cid:12)(cid:12)(cid:12) S (cid:12)(cid:12)(cid:12) 
x∈Bµ(L) Q∈Sd(M L−1,ML)
S∈Sd((2ML)−1,2ML)
(i)
≲L·poly(M )
L
Here (i) follows from Lemma 17.
√
Finally, note that by definition, L ≍ logn and M = poly(L), one can obtain (90).
L
73C.3 Proof of Lemma 34
As argued at the beginning of Appendix C.2, we can assume without loss of generality that
∥X −µ∥ ≤ L almost surely with L defined in (79). As a result, we have almost surely that
∥X −µ∥ ≤ L
Q,Q∗(X) ∈ S (M−1,M ) (156)
d L L
1(E ) = 1
0
where E := (cid:8) ∥X −µ∥ ≤ L,Q ∈ S (M−1,M ),i ∈ [n](cid:9).
0 i i d L L
With boundedness condition (156) in place, Lemma 17 implies the following upper bounds
(cid:13) (cid:13)
sup (cid:13)TQ−I (cid:13) ≲ M
(cid:13) S d(cid:13) L
S∈S d(M L−1,ML) F
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
sup (cid:12)(cid:12)(cid:12)dTQ(cid:12)(cid:12)(cid:12) ≲ M3
(cid:12)(cid:12)(cid:12) S (cid:12)(cid:12)(cid:12) L
S∈S d(M L−1,ML)
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
sup (cid:12)(cid:12)(cid:12)d2TQ(cid:12)(cid:12)(cid:12) ≲ M8
(cid:12)(cid:12)(cid:12) S (cid:12)(cid:12)(cid:12) L
S∈S(M(cid:102) L−1,M(cid:102)L)
almost surely. Applying Lemma 23 then gives the desired results.
C.4 Proof of Lemma 35
Proof of (94): apply the triangle inequality to see that
(cid:13) (cid:13) (cid:13) (cid:13)
sup ∥A n(x)∥
F
≤ sup (cid:13) (cid:13)A n(x)−A(cid:101)n(x)(cid:13)
(cid:13)
+ sup (cid:13) (cid:13)A(cid:101)n(x)(cid:13)
(cid:13)
x∈Bµ(L) x∈Bµ(L) F x∈Bµ(L) F
(i) polylog(n)
≤ √
n
Here (i) follows from Lemma 33 and Lemma 34
Proof of (95): apply the triangle inequality to see that
sup |||Ψ (x,S)|||
n
x∈Bµ(L)
S∈Sd((2ML)−1,2ML)
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
≤ sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ n(x,S)−Ψ(cid:101)n(x,S)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)+ sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(x,S)−EΨ(cid:101)n(x,S)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
x∈Bµ(L) x∈Bµ(L)
S∈Sd((2ML)−1,2ML) S∈Sd((2ML)−1,2ML)
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
+ sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)EΨ(cid:101)n(x,S)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
x∈Bµ(L)
S∈Sd((2ML)−1,2ML)
(i) polylog(n) polylog(n)
≤ √ + √ +polylog(n)
n n
≤ polylog(n)
Here (i) follows from Lemma 33 and Lemma 34.
74Proof of (96), (97): By the remark after Assumption 5 and the eigenvalue stability in-
equality, one can obtain
inf λ (−Φ (x))
min n
x∈B
µ(L)
(cid:16) (cid:17) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
≥ inf λ
min
−EΦ(cid:101)n(x) − sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x)−EΦ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)− sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ(cid:101)n(x)−Φ n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
x∈B µ(L) x∈B µ(L) x∈B µ(L)
(i) 1 polylog(n) polylog(n)
≥ − √ − √
polylog(n) n n
1
≥
polylog(n)
(cid:16) (cid:17)
Here (i) follows by noticing that E(−Φ(cid:101)n(x)) = E −s(x,X)dT QQ
∗(x)
, Assumption 5 and apply-
ing Lemma 33, Lemma 34.
By the remark after Assumption 5 and (96), one then has
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)−Φ− n1(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) = λ max(cid:0) −Φ− n1(x)(cid:1) ≤
λ
(−1
Φ (x))
min n
Then (97) follows from (96).
D Proof of Theorem 7
Tosimplifynotation,wefixxandwriteQ∗ forQ∗(x),Q(cid:98)n forQ(cid:98)n(x)whenthereisnoambiguity.
Following the same argument as (84) in Appendix C, one can obtain
(cid:16) (cid:17)
E s(x,X) TQ −I = 0 (157)
(X,Q)∼P Q∗ d
 −1
√ n(cid:16) Q(cid:98)n(x)−Q∗(x)(cid:17) =    − n1 (cid:88)n s n(x,X i)dT QQ ∗i (x)  

· √1
n
(cid:88)n s n(x,X i)(cid:16) T QQ ∗i (x)−I d(cid:17)
 i=1  i=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:Φn(x) =:an(x)
(cid:32) n (cid:33)−1 √ n
+ − n1 (cid:88) s n(x,X i)dT QQ ∗i
(x)
· 2n · n1 (cid:88) s n(x,X i)d2T QQ (cid:101)ni (x)·(Q(cid:98)n(x)−Q∗(x))⊗2 (158)
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
=:bn(x)
Analysis of Φ (x): Lemma 33 and 34 together imply that for any fixed x,
n
(cid:104) (cid:105)
Φ (x) a →.s. E −s(x,X)dTQ
n Q∗(x)
Then by Assumption 5, one has
(cid:104) (cid:16) (cid:17)(cid:105)−1
[Φ (x)]−1 a →.s. E −s(x,X)dTQ (159)
n Q∗(x)
Analysis of b (x): Theorem 6, Lemma 33 and 34 together imply that for any fixed x, with
n
probability at least 1−O(n−100),
√ (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
n
|b n(x| ≤ 2n ·(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)n1 (cid:88) s n(x,X i)d2T QQ (cid:101)ni (x)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)·(cid:13) (cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13) (cid:13) (cid:13)2
F
i=1
75√
n polylog(n)
≤ ·polylog(n)·
2 n
polylog(n)
= √ (160)
n
−1
Analysis of a (x): by Lemma 21, one has s(x,X) = ⃗x⊤Σ⃗−1X⃗ and s (x,X) = ⃗x⊤Σ⃗(cid:98) X⃗.
n n
Hence one can obtain
n
a (x) = √1 (cid:88) ⃗x⊤Σ⃗(cid:98)−1 X⃗ (cid:16) TQi −I (cid:17)
n n i Q∗ d
i=1
=
(cid:18) ⃗x⊤Σ⃗(cid:98)−1
⊗I
(cid:19)
·
√1 (cid:88)n
X⃗
⊗(cid:16)
TQi −I
(cid:17)
d n i Q∗ d
i=1
n
= (cid:16) ⃗x⊤Σ⃗−1⊗I (cid:17) · √1 (cid:88) X⃗ ⊗(cid:16) TQi −I (cid:17)
d n i Q∗ d
i=1
(cid:124) (cid:123)(cid:122) (cid:125) (161)
=:an,1(x)
√ (cid:18) −1 (cid:19) (cid:16) (cid:17)
+ n ⃗x⊤(Σ⃗(cid:98) −Σ⃗−1)⊗I ·EX⃗ ⊗ TQ −I
d Q∗ d
(cid:124) (cid:123)(cid:122) (cid:125)
=:an,2(x)
+√ n(cid:18) ⃗x⊤(Σ⃗(cid:98)−1
−Σ⃗−1)⊗I
(cid:19) ·(cid:34) 1 (cid:88)n
X⃗
⊗(cid:16)
TQi −I
(cid:17)
−EX⃗
⊗(cid:16)
TQ −I
(cid:17)(cid:35)
d n i Q∗ d Q∗ d
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
=:an,3(x)
• a (x): by (157) and Lemma 21, one can obtain
n,1
Ea (x) = 0 (162)
n,1
(cid:16) (cid:17) (cid:16) (cid:17)
Toseethis, itsufficestoshowE ⃗x⊤Σ⃗−1⊗I · X⃗ ⊗(TQ −I ) = 0. Directcomputation
d Q∗ d
shows that the LHS of the equality is equal to
(cid:16) (cid:17)
E⃗x⊤Σ⃗−1X⃗ TQ −I = Es(x,X)(TQ −I ) = 0
Q∗ d Q∗ d
Here the equality follows from the optimality condition (157).
• a (x): using the formula A−1−B−1 = A−1(B−A)B−1, one can obtain
n,2
√ (cid:18) −1 −1 (cid:19) (cid:16) (cid:17)
a (x) = n ⃗x⊤Σ⃗(cid:98) (Σ⃗ −Σ⃗(cid:98) )Σ⃗−1⊗I ·EX⃗ ⊗ TQ −I
n,2 d Q∗ d
√ (cid:18) (cid:19) (cid:16) (cid:17)
= n ⃗x⊤Σ⃗−1(Σ⃗ −Σ⃗(cid:98))Σ⃗−1⊗I ·EX⃗ ⊗ TQ −I
d Q∗ d
√ (cid:18) −1 −1 (cid:19) (cid:16) (cid:17)
− n ⃗x⊤(Σ⃗−1−Σ⃗(cid:98) )(Σ⃗ −Σ⃗(cid:98) )Σ⃗−1⊗I ·EX⃗ ⊗ TQ −I
d Q∗ d
(cid:32) n (cid:33) (cid:18) (cid:19)
= −√1 (cid:88) ⃗x⊤Σ⃗−1(X⃗ X⃗⊤−Σ⃗)⊗I ·(cid:16) EX⃗ ⊗(TQ −I )(cid:17) +O √1 (163)
n i i d Q∗ d p n
i=1
Here the last line follows from the fact that Σ⃗(cid:98) −Σ⃗ = O (n−1/2) which is itself due to the
p
sub-Gaussianity of X.
76(cid:104) (cid:16) (cid:17) (cid:16) (cid:17)(cid:105)
• a (x): note that 1 (cid:80)n X⃗ ⊗ TQi −I −EX⃗ ⊗ TQ −I = O (n−1/2) again by
n,3 n i=1 i Q∗ d Q∗ d p
the sub-Gaussianity of X and Assumption 2, one can obtain
a (x) = O (n−1/2) (164)
n,3 p
Combining (161), (162), (163), (164) and the functional central limit theorem (Hsing and
Eubank, 2015, Theorem 7.7.6), one can obtain
n
a (x) =
√1 (cid:88)(cid:104)(cid:16)
⃗x⊤Σ⃗−1⊗I
(cid:17) ·(cid:16)
X⃗ ⊗(TQi −I
)(cid:17)
−
n n d i Q∗ d
i=1
(cid:16) (cid:17) (cid:16) (cid:17)(cid:105)
⃗x⊤Σ⃗−1(X⃗ X⃗⊤−Σ⃗)⊗I · EX⃗ ⊗(TQ −I ) +o (1)
i i d Q∗ d p
→w Z (165)
x
where Z ∼ N (0,Ξ ) is a Gaussian random matrix with mean 0 and covariance Ξ . Here
x x x
Ξ := EV ⊗V with
x x x
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
V = s(x,X) TQ −I − ⃗x⊤Σ⃗−1(X⃗X⃗⊤−Σ⃗)⊗I · EX⃗ ⊗(TQ −I )
x Q∗ d d Q∗ d
Finally, (158), (159), (160) and (165) together imply
√ (cid:16) (cid:17) (cid:16) (cid:17)−1
n Q(cid:98)n−Q∗ →w −Es(x,X)dT QQ
∗
·Z
x
which completes the proof.
E Proof of Corollary 9
The optimality condition for Q∗(x) implies that
(cid:16) (cid:17)
Es(x,X) TQ −I = 0
Q∗(x) d
Note that independence between X and Q implies that
(cid:16) (cid:17) (cid:16) (cid:17)
Es(x,X) TQ −I = Es(x,X)E TQ −I
Q∗(x) d Q∗(x) d
( =i) E(cid:16) TQ −I (cid:17)
Q∗(x) d
Here (i) follows from the fact that Es(x,X) = 1. Therefore, one has
(cid:16) (cid:17)
E TQ −I = 0
Q∗(x) d
By independence, the above equality then implies
EX⃗ ⊗(TQ −I ) = 0
Q∗ d
Hence one has V = 0, the proof is then complete.
x,2
77F Proof of Theorem 11
First, we demonstrate in Lemma 36 below uniform fast convergence under Assumption 1, 2, 5,
6 and the null hypothesis. In order to apply Theorem 6, it suffices to verify Assumption 5 and
Assumption 4. Note that Assumption 6 and the null implies the independence between X and
Q, then both Assumption 5 and 4 are consequences of Lemma 29.
Lemma 36. Instate the assumptions in Theorem 11. Then with probability at least 1 −
O(n−100), one has
(cid:13) (cid:13) polylog(n)
x∈Bsu µp (Ln)(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13) F
≤ √
n
(cid:13) (cid:13) polylog(n)
1≤su i≤p n(cid:13) (cid:13)Q(cid:98)n(X i)−Q∗(X i)(cid:13)
(cid:13) F
≤ √
n
With Lemma 36 in place, we consider the Taylor expansion of the optimality condition for
Q(cid:98)n(x). Recall that the optimality condition for Q(cid:98)n(x) gives
n
1 (cid:88) s (x,X )(cid:16) TQi −I (cid:17) = 0
n i d
n Q(cid:98)n(x)
i=1
Then one can apply Lemma 17 to get the following 2nd order Taylor expansion around Q∗(x).
n n
0 = n1 (cid:88) s n(x,X i)(cid:16) T QQ ∗i (x)−I d(cid:17) + n1 (cid:88) s n(x,X i)dT QQ ∗i (x)·(cid:16) Q(cid:98)n(x)−Q∗(x)(cid:17) +R 2(x) (166)
i=1 i=1
where R 2(x) is the 2nd order remainder term with some Q(cid:101)n(x) lying between Q∗(x) and Q(cid:98)n(x)
defined as follows.
n
R 2(x) := 1 (cid:88) s n(x,X i)d2TQi ·(cid:16) Q(cid:98)n(x)−Q∗(x)(cid:17)⊗2
2n Q(cid:101)n(x)
i=1
Under the null hypothesis (18), one has Q∗(x) ≡ Q∗ and (166) reduces to
n n
0 = n1 (cid:88) s n(x,X i)(cid:16) T QQ ∗i −I d(cid:17) + n1 (cid:88) s n(x,X i)dT QQ ∗i ·(cid:16) Q(cid:98)n(x)−Q∗(x)(cid:17) +R 2(x) (167)
i=1 i=1
Setting x = X in (167) then gives
n n
0 = n1 (cid:88)(cid:16) T QQ ∗i −I d(cid:17) + n1 (cid:88) dT QQ ∗i ·(cid:16) Q(cid:98)n(x)−Q∗(x)(cid:17) +R 2(x) (168)
i=1 i=1
Take difference between (167) and (168) and rearrange, one can obtain
(cid:32) n (cid:33)
− n1 (cid:88) dT QQ ∗i ·(Q(cid:98)n(x)−Q(cid:98)n(X))
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
=:τ(x)
n n
= n1 (cid:88) (s n(x,X i)−1)(T QQ ∗i −I d)+ n1 (cid:88) (s n(x,X i)−1)dT QQ ∗i ·(Q(cid:98)n(x)−Q(cid:98)n(X)) (169)
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:α0(x) =:α1(x)
+R (x)−R (X)
2 2
(cid:124) (cid:123)(cid:122) (cid:125)
=:α2(x)
78Before delving further into the proof, we pause to introduce more notations. Define
(cid:32) n (cid:33)
τ (cid:98)(x) = −1 (cid:88) dTQi ·(Q(cid:98)n(x)−Q(cid:98)n(X))
n Q(cid:98)n(X)
i=1
Then by definition, the test statistic T(cid:98)n = (cid:80)n i=1∥τ (cid:98)(X i)∥2 F, and we the following decomposition
of τ(x).
τ(x) = τ(x)+τ(x)−τ(x)
(cid:98) (cid:98)
(cid:32) n n (cid:33)
= τ(x)+ n1 (cid:88) dT QQ ∗i − n1 (cid:88) dT QQ (cid:98)ni
(X)
·(Q(cid:98)n(x)−Q(cid:98)n(X))
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
=:α3(x)
= α (x)+α (x)+α (x)+α (x)
0 1 2 3
(cid:124) (cid:123)(cid:122) (cid:125)
=:R(x)
We also define counterparts of α ,...,α and R by replacing s (·,·) with s(·,·) as follows.
0 2 2 n
n
α (x) := 1 (cid:88) (s(x,X )−1)(TQi −I )
(cid:101)0 n i Q∗ d
i=1
n
α (cid:101)1(x) := n1 (cid:88) (s(x,X i)−1)dT QQ ∗i ·(Q(cid:98)n(x)−Q(cid:98)n(X))
i=1
α (cid:101)2(x) := R(cid:101)2(x)−R(cid:101)2(X)
where
n
R(cid:101)2(x) = 1 (cid:88) s(x,X i)d2TQi ·(Q(cid:98)n(x)−Q∗)⊗2
2n Q(cid:101)n(x)
i=1
With the above notation in place, one can obtain
n n n
T(cid:98)n = (cid:88) ∥α 0(X k)∥2 F+2(cid:88) ⟨α 0(X k),R(X k)⟩+(cid:88)(cid:13) (cid:13)R(X k)2(cid:13) (cid:13)
F (170)
k=1 k=1 k=1
(cid:124) (cid:123)(cid:122) (cid:125)
Remn
The proof is then divided into three steps.
• First, we give upper bounds for α as well as α −α and their consequences.
(cid:101)i (cid:101)i i
• Next, we show that the remainder term Rem is negligible.
n
• Then, we demonstrate that (cid:80) ∥α (X )∥2 converges weakly to the desired asymptotic null
i 0 i F
distribution (24).
Analysis of α and α−α: We give uniform upper bounds for α (x)−α (x) for i = 0,1,2
(cid:101) (cid:101) (cid:101)i i
in Lemma 37 as uniform upper bounds for α (x) and α (x) in Lemma 38; see Appendix F.1,
(cid:101)i 3
F.2 for the proof.
Lemma 37. Instate the notations and assumptions in Theorem 11.
polylog(n)
sup ∥α (x)−α (x)∥ ≤ √ (171)
0 (cid:101)0 F n
x∈Bµ(L)
polylog(n)
sup ∥α (x)−α (x)∥ ≤ (172)
1 (cid:101)1 F n
x∈Bµ(L)
79polylog(n)
sup ∥α (x)−α (x)∥ ≤ (173)
2 (cid:101)2 F n3/2
x∈Bµ(L)
with probability at least 1−O(cid:0) n−99(cid:1).
Lemma 38. Instate the notations and assumptions in Theorem 11.
polylog(n)
sup ∥α (x)∥ ≤ √ (174)
(cid:101)0 F n
x∈Bµ(L)
polylog(n)
sup ∥α (x)∥ ≤ (175)
(cid:101)1 F n
x∈Bµ(L)
polylog(n)
sup ∥α (x)∥ ≤ (176)
(cid:101)2 F n
x∈Bµ(L)
polylog(n)
sup ∥α (x)∥ ≤ (177)
3 F n
x∈Bµ(L)
with probability at least 1−O(cid:0) n−99(cid:1).
With the above lemmas in place, one can readily obtain that with probability at least
1−O(n−99),
polylog(n)
sup∥α (X )∥ ≤ sup∥α (X )−α (X )∥ + sup∥α (X )∥ ≤ √ (178)
0 i F (cid:101)0 i 0 i F (cid:101)0 i F n
i∈[n] i∈[n] i∈[n]
polylog(n)
sup∥α (X )∥ ≤ sup∥α (X )−α (X )∥ + sup∥α (X )∥ ≤ (179)
1 i F (cid:101)1 i 1 i F (cid:101)1 i F n
i∈[n] i∈[n] i∈[n]
polylog(n)
sup∥α (X )∥ ≤ sup∥α (X )−α (X )∥ + sup∥α (X )∥ ≤ (180)
2 i F (cid:101)2 i 2 i F (cid:101)2 i F n
i∈[n] i∈[n] i∈[n]
which then implies
3
(cid:88) polylog(n)
sup∥R(X )∥ ≤ sup∥α (X )∥ ≤ (181)
i F k i F n
i∈[n] i∈[n]
k=1
NegligibilityofRem n: Weconsidertwoterms(cid:80)n k=1⟨α 0(X k),R(X k)⟩and(cid:80)n k=1(cid:13) (cid:13)R(X k)2(cid:13) (cid:13)
F
separately.
Analysis of (cid:80)n ⟨α (X ),R(X )⟩: By (178) and (181), one has with probability at least
k=1 0 k k
1−O(n−99),
n n
(cid:88) (cid:88)
⟨α (X ),R(X )⟩ ≤ ∥α (X )∥ ·∥R(X )∥
0 k k 0 k F k F
k=1 k=1
n
(cid:88) polylog(n)
≤
n3/2
k=1
polylog(n)
= √
n
Analysisof(cid:80)n k=1(cid:13) (cid:13)R(X k)2(cid:13) (cid:13) F: Similarly,by(181),onehaswithprobabilityatleast1−O(n−99),
n
(cid:88)(cid:13)
(cid:13)R(X
k)2(cid:13)
(cid:13)
F
≤
polyl nog(n)
k=1
Therefore, the above results imply that with probability at least 1−O(n−99),
polylog(n)
Rem ≤ √ (182)
n
n
80Analysis of α : To consider the main term (cid:80)n ∥α (X )∥2, one has
0 i=1 0 i F
n
(cid:88)
∥α (X )∥2
0 k F
k=1
n (cid:42) n n (cid:43)
= n1
2
(cid:88) (cid:88) (X
k
−X)⊤Σ(cid:98)−1(X i−X)(T QQ ∗i −I d),(cid:88) (X
k
−X)⊤Σ(cid:98)−1(X
j
−X)(T QQ ∗j −I d)
k=1 i=1 j=1
n n
= n1
2
(cid:88) (cid:88) (X i−X)⊤Σ(cid:98)−1(X
k
−X)(X
k
−X)⊤Σ(cid:98)−1(X
j
−X)(cid:68) T QQ ∗i −I d,T QQ ∗j −I d(cid:69)
i,j=1k=1
n
= n1 (cid:88) (X i−X)⊤Σ(cid:98)−1(X
j
−X)(cid:68) T QQ ∗i −I d,T QQ ∗j −I d(cid:69)
i,j=1
(cid:13)
n
(cid:13)2
=(cid:13) (cid:13) (cid:13)√1
n
(cid:88) Σ(cid:98)−1/2(X i−X)⊗(T QQ ∗i −I d)(cid:13) (cid:13)
(cid:13)
(cid:13) (cid:13)
i=1 F
Note that
n
√1
n
(cid:88) Σ(cid:98)−1/2(X i−X)⊗(T QQ ∗i −I d)
i=1
(cid:34) n (cid:35)
= (cid:104) (Σ(cid:98)−1/2Σ1/2)⊗I d(cid:105) · √1
n
(cid:88) Σ−1/2(X i−X)⊗(T QQ ∗i −I d)
i=1
n
= (1+o (1))√1 (cid:88) Σ−1/2(X −X)⊗(TQi −I ) (183)
p n i Q∗ d
i=1
Also, one can obtain
(cid:32) n (cid:33) (cid:32) n (cid:33)
√1 (cid:88) Σ−1/2(X −X)⊗(TQi −I ) − √1 (cid:88) Σ−1/2(X −µ)⊗(TQi −I )
n i Q∗ d n i Q∗ d
i=1 i=1
n
=√1 (cid:88) Σ−1/2(µ−X)⊗(TQi −I )
n Q∗ d (184)
i=1
(cid:20) (cid:21)
1 (cid:16) (cid:17)
=Σ−1/2(µ−X)⊗ √ TQi −I
n Q∗ d
=o (1)
p
Here the last line follows since X−µ = o p(1) and √1
n
(cid:80)n i=1(T QQ ∗i−I d) is asymptotically normal
with zero mean. The zero mean is justified in the following claim whose proof is deferred to
Appendix F.3.
Claim 3. Under the null hypothesis (18) and Assumption 4, 6, X and Q are independent and
one has
(cid:16) (cid:17)
E(X −µ)⊗ TQ −I = 0 (185)
Q∗ d
(cid:16) (cid:17)
E TQ −I = 0 (186)
Q∗ d
Claim 3 also implies that EΣ−1/2(X −µ)⊗(TQi −I ) = 0. Therefore, by the functional
Q∗ d
central limit theorem (Hsing and Eubank, 2015, Theorem 7.7.6), one can obtain
n
√1 (cid:88) Σ−1/2(X −µ)⊗(TQi −I ) →w N (cid:16) 0,I ⊗E(cid:104) (TQ −I )⊗(TQ −I )(cid:105)(cid:17) (187)
n i Q∗ d p Q∗ d Q∗ d
i=1
81Combining (183), (184) and (187), we arrive at
n
√1
n
(cid:88) Σ(cid:98)−1/2(X i−X)⊗(T QQ ∗i −I d) →w N (cid:16) 0,I p⊗E(cid:104) (T QQ
∗
−I d)⊗(T QQ
∗
−I d)(cid:105)(cid:17)
i=1
which then implies that
n
(cid:88) ∥α (X )∥2 →w (cid:88) λ w (188)
0 k i i
k=1 i
(cid:104) (cid:105)
wherew arei.i.d. χ2randomvariablesandλ aretheeigenvaluesofE (TQ −I )⊗(TQ −I ) .
i p i Q∗ d Q∗ d
Finally, taking (170) (182) and (188) collectively yields
w (cid:88)
T(cid:98)n → λ iw
i
i
F.1 Proof of Lemma 37
Apply Lemma 36 and triangle inequality to see that under the null (18), with probability at
least 1−O(n−100), one has
(cid:13) (cid:13) polylog(n)
x∈s Bu µp (L)(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(cid:13)
(cid:13) F
≤ √
n
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) polylog(n)
x∈s Bu µp (L)(cid:13) (cid:13)Q(cid:98)n(x)−Q(cid:98)n(X)(cid:13)
(cid:13) F
≤ x∈s Bu µp (L)(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(cid:13)
(cid:13)
F+ x∈s Bu µp (L)(cid:13) (cid:13)Q(cid:98)n(X)−Q∗(cid:13)
(cid:13) F
≤ √
n
(189)
Proof of (171): note that
n
α (x)−α (x) = 1 (cid:88) (s (x,X )−s(x,X ))(TQi −I )
0 (cid:101)0 n n i i Q∗ d
i=1
= A n(x)−A(cid:101)n(x)
Hence (171) follows from Lemma 34.
Proof of (172): one can obtain
(cid:13) (cid:13)
n
∥α 1(x)−α (cid:101)1(x)∥
F
= (cid:13) (cid:13) (cid:13)n1 (cid:88) (s n(x,X i)−s(x,X i))dT QQ ∗i ·(Q(cid:98)n(x)−Q(cid:98)n(X))(cid:13) (cid:13)
(cid:13)
(cid:13) (cid:13)
i=1 F
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
n
≤ (cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)n1 (cid:88) (s n(x,X i)−s(x,X i))dT QQ ∗i(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)·(cid:13) (cid:13) (cid:13)Q(cid:98)n(x)−Q(cid:98)n(X)(cid:13) (cid:13)
(cid:13)
F
i=1
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:13) (cid:13)
= (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Φ n(x)−Φ(cid:101)n(x)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)·(cid:13) (cid:13)Q(cid:98)n(x)−Q(cid:98)n(X)(cid:13)
(cid:13)
F
Hence (172) follows from Lemma 34 and (189).
82Proof of (173): one can obtain
(cid:13) (cid:13)
n
∥α 2(x)−α (cid:101)2(x)∥
F
= (cid:13) (cid:13)
(cid:13)
(cid:13)n1 (cid:88) (s n(x,X i)−s(x,X i))d2T QQ (cid:101)ni (x)·(Q(cid:98)n(x)−Q∗)⊗2(cid:13) (cid:13)
(cid:13)
(cid:13)
i=1 F
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
n
≤ (cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)n1 (cid:88) (s n(x,X i)−s(x,X i))d2T QQ (cid:101)ni (x)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)·(cid:13) (cid:13) (cid:13)Q(cid:98)n(x)−Q(cid:98)n(X)(cid:13) (cid:13) (cid:13)2
F
i=1
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:13) (cid:13)2
= (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ n(x,Q(cid:101)n(x))−Ψ(cid:101)n(x,Q(cid:101)n(x))(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)·(cid:13) (cid:13)Q(cid:98)n(x)−Q(cid:98)n(X)(cid:13)
(cid:13)
F
Hence (173) follows from Lemma 34 and (189).
F.2 Proof of Lemma 38
Apply Lemma 36 and triangle inequality to see that under the null (18), with probability at
least 1−O(n−100), one has
(cid:13) (cid:13) polylog(n)
x∈s Bu µp (L)(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(cid:13)
(cid:13) F
≤ √
n
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) polylog(n)
x∈s Bu µp (L)(cid:13) (cid:13)Q(cid:98)n(x)−Q(cid:98)n(X)(cid:13)
(cid:13) F
≤ x∈s Bu µp (L)(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(cid:13)
(cid:13)
F+ x∈s Bu µp (L)(cid:13) (cid:13)Q(cid:98)n(X)−Q∗(cid:13)
(cid:13) F
≤ √
n
(190)
Proof of (174): Recall the definition of A(cid:101)n(x) in (86), one can see that (174) follows
from (87) in Lemma 33 with a slight and straightforward modification to accommodate the
s(x,X)−1 term here. For brevity, we omit the proof.
Proof of (175):
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
n
∥α (cid:101)1∥
F
≤ (cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)n1 (cid:88) (s(x,X i)−1)dT QQ ∗i(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)·(cid:13) (cid:13) (cid:13)Q(cid:98)n(x)−Q(cid:98)n(X)(cid:13) (cid:13)
(cid:13)
F
(191)
i=1
Recall the definition of Φ(cid:101)n(x) in (86), with a slight modification of the proof of (88) in Lemma
33, one can obtain
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
n
sup
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)1 (cid:88)
(s(x,X
)−1)dTQi(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12) ≤
poly √log(n)
(192)
(cid:12)(cid:12)(cid:12)n i Q∗(cid:12)(cid:12)(cid:12) n
x∈Bµ(L)(cid:12)(cid:12)(cid:12)
i=1
(cid:12)(cid:12)(cid:12)
For brevity, the proof is omitted.
Combining (190), (191) and (192) gives (175).
Proof of (176): Apply triangle inequality to see that
(cid:13) (cid:13) (cid:13) (cid:13)
sup ∥α (cid:101)2(x)∥
F
≤ (cid:13) (cid:13)R(cid:101)2(X)(cid:13)
(cid:13)
+ sup (cid:13) (cid:13)R(cid:101)2(x)(cid:13)
(cid:13)
x∈Bµ(L) F x∈Bµ(L) F
(cid:13) (cid:13)
≤ 2 sup (cid:13) (cid:13)R(cid:101)2(x)(cid:13)
(cid:13)
x∈Bµ(L) F
83Moreover, one can obtain
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
n
x∈s Bu µp (L)(cid:13) (cid:13) (cid:13)R(cid:101)2(x)(cid:13) (cid:13)
(cid:13)
F
≲ x∈s Bu µp (L)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)n1 (cid:88) i=1s(x,X i)d2T QQ (cid:101)ni (x)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)·(cid:13) (cid:13) (cid:13)Q(cid:98)n(x)−Q∗(cid:13) (cid:13) (cid:13)2
F
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:13) (cid:13)2
≤ sup (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)Ψ(cid:101)n(x,S)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)· sup (cid:13) (cid:13)Q(cid:98)n(x)−Q∗(cid:13)
(cid:13)
x∈Bµ(L) x∈Bµ(L) F
S∈Sd((2ML)−1,2ML)
(i) polylog(n)
≤
n
Here (i) follows from (89), (90) in Lemma 33 as well as (190).
Proof of (177): one can obtain
(cid:13)(cid:32) n n (cid:33) (cid:13)
x∈s Bu µp (L)∥α 3(x)∥
F
= x∈s Bu µp (L)(cid:13) (cid:13)
(cid:13)
(cid:13)
n1 (cid:88) i=1dT QQ ∗i − n1 (cid:88) i=1dT QQ (cid:98)ni
(X)
·(Q(cid:98)n(x)−Q(cid:98)n(X))(cid:13) (cid:13)
(cid:13)
(cid:13)
F
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
n n
≤ (cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)n1 (cid:88) i=1dT QQ ∗i − n1 (cid:88) i=1dT QQ (cid:98)ni (X)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12)
(cid:12)
(cid:12)· x∈s Bu µp (L)(cid:13) (cid:13) (cid:13)Q(cid:98)n(x)−Q(cid:98)n(X)(cid:13) (cid:13)
(cid:13)
F
(193)
Moreover, one has
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
n n n
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)1 (cid:88) dTQi − 1 (cid:88) dTQi (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12) ( ≤i) 1 (cid:88)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)dTQi −dTQi (cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) (cid:12)n Q∗ n Q(cid:98)n(X)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)
n (cid:12)(cid:12)(cid:12) Q∗ Q(cid:98)n(X)(cid:12)(cid:12)(cid:12)
i=1 i=1 i=1
n
( ≤ii) n1 (cid:88)(cid:12)
(cid:12)
(cid:12)(cid:12)
(cid:12)
(cid:12)(cid:12)
(cid:12) (cid:12)d2T QQ ′i
·(Q(cid:98)n(X)−Q∗)(cid:12)
(cid:12)
(cid:12)(cid:12)
(cid:12)
(cid:12)(cid:12)
(cid:12)
(cid:12)
i,n
i=1
n
(i ≤ii) n1 (cid:88)(cid:12)
(cid:12)
(cid:12)(cid:12)
(cid:12)
(cid:12)(cid:12)
(cid:12) (cid:12)d2T QQ
′
ii
,n(cid:12)
(cid:12)
(cid:12)(cid:12)
(cid:12)
(cid:12)(cid:12)
(cid:12)
(cid:12)·(cid:13)
(cid:13)
(cid:13)Q(cid:98)n(X)−Q∗(cid:13)
(cid:13)
(cid:13)
F
(194)
i=1
Here (i) follows from triangle inequality, (ii) from the mean value theorem (Dudley and Nor-
vaiša, 2011, Theorem 5.3) for some Q′
i,n
that lies on the segment between Q∗ and Q(cid:98)n(X), and
(iii) arises from Lemma 18. Therefore, with a slight modification of the proof of (90) in Lemma
33, one can obtain with probability at least 1−O(n−100),
n
1 (cid:88)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12)d2TQi
(cid:12) (cid:12)(cid:12) (cid:12)(cid:12)
(cid:12) ≤ polylog(n) (195)
n (cid:12)(cid:12)(cid:12) Q′ (cid:12)(cid:12)(cid:12)
i,n
i=1
For brevity, the proof is omitted.
Finally, combining (193), (194) and (195) gives
(cid:34) n (cid:35) (cid:34) (cid:35)2
x∈s Bu µp (L)∥α 3(x)∥
F
≲ n1 (cid:88) i=1(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)d2T QQ
′
ii ,n(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12)
(cid:12)
· x∈s Bu µp (L)(cid:13) (cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13) (cid:13)
(cid:13)
F
polylog(n)
≤
n
which finishes the proof for (177).
F.3 Proof of Claim 3
The independence follows directly from Assumption 6 and the null hypothesis (18).
84Proof of (185): by independence, we have
(cid:16) (cid:17) (cid:104) (cid:16) (cid:17)(cid:105)
E(X −µ)⊗ TQ −I = [E(X −µ)]⊗ E TQ −I
Q∗ d Q∗ d
= 0
Proof of (186): The optimality condition of Q∗(x) gives that
(cid:16) (cid:17)
Es(x,X) TQ −I = 0
Q∗(x) d
By independence, one then has
E(cid:16) TQ −I (cid:17) ( =i) [Es(x,X)]·(cid:104) Es(x,X)(cid:16) TQ −I (cid:17)(cid:105)
Q∗(x) d Q∗(x) d
(cid:16) (cid:17)
= Es(x,X) TQ −I
Q∗(x) d
= 0
Here (i) follows from the fact that Es(x,X) ≡ 1.
G Proof of Proposition 13
Note that under the null (18), it holds that Q∗(X) = Q∗. Then Theorem 6 implies that with
probability at least 1−O(n−100),
(cid:13) (cid:13) polylog(n)
(cid:13) (cid:13)Q(cid:98)n(X)−Q∗(cid:13)
(cid:13) F
≤ √
n
Therefore, one has with probability at least 1−O(n−100),
Q(cid:98)n(X) ∈ S d((2c 1)−1,2c 1) (196)
for n large enough. Here we recall that c ≥ 1 is a constant defined in Assumption 2.
1
One can apply the triangle inequality to see that
(cid:13) (cid:13)
n
(cid:13) (cid:13)1 (cid:88)(cid:16)
TQi −I
(cid:17) ⊗(cid:16)
TQi −I
(cid:17) −E(cid:16)
TQ −I
(cid:17) ⊗(cid:16)
TQ −I
(cid:17)(cid:13)
(cid:13)
(cid:13) (cid:13)n Q(cid:98)n(X) d Q(cid:98)n(X) d Q∗ d Q∗ d (cid:13)
(cid:13)
i=1 F
(cid:13) (cid:13)
n
≤(cid:13) (cid:13)1 (cid:88)(cid:16)
TQi −I
(cid:17) ⊗(cid:16)
TQi −I
(cid:17) −E(cid:16)
TQ −I
(cid:17) ⊗(cid:16)
TQ −I
(cid:17)(cid:13)
(cid:13)
(cid:13) (cid:13)n Q(cid:98)n(X) d Q(cid:98)n(X) d Q(cid:98)n(X) d Q(cid:98)n(X) d (cid:13)
(cid:13)
i=1 F
(cid:13) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)(cid:13)
+(cid:13)E TQ −I ⊗ TQ −I −E TQ −I ⊗ TQ −I (cid:13)
(cid:13) Q(cid:98)n(X) d Q(cid:98)n(X) d Q∗ d Q∗ d (cid:13) F
(cid:13) (cid:13)
n
( ≤i)
sup
(cid:13) (cid:13)1 (cid:88)(cid:16)
TQi −I
(cid:17) ⊗(cid:16)
TQi −I
(cid:17) −E(cid:16)
TQ−I
(cid:17) ⊗(cid:16)
TQ−I
(cid:17)(cid:13)
(cid:13)
(cid:13)n S d S d S d S d (cid:13)
S∈S d((2c1)−1,2c1)(cid:13)
i=1
(cid:13)
F
(cid:124) (cid:123)(cid:122) (cid:125)
ζ1
(cid:13) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)(cid:13)
+ sup (cid:13)E TQ−I ⊗ TQ−I −E TQ −I ⊗ TQ −I (cid:13)
S:∥S−Q∗∥ ≤polylog(n)/√ n(cid:13) S d S d Q∗ d Q∗ d (cid:13) F
F
(cid:124) (cid:123)(cid:122) (cid:125)
ζ2
(197)
Here (i) follows from (196).
Upperboundsforζ ,ζ in(197)aresummarizedinthelemmabelowwhoseproofisdeferred
1 2
to Appendix G.1. Note that Lemma 39 do not assume the null hypothesis so that it can be
reused later for the proof of the power (Theorem 14).
85Lemma 39. Suppose Assumption 1-6 hold. Then with probability at least 1−O(n−100),
polylog(n)
ζ ≤ √ (198)
1
n
polylog(n)
ζ ≤ √ (199)
2
n
Lemma 39 combined with (197) then implies
(cid:13) (cid:13)
n
(cid:13) (cid:13)1 (cid:88)(cid:16)
TQi −I
(cid:17) ⊗(cid:16)
TQi −I
(cid:17) −E(cid:16)
TQ −I
(cid:17) ⊗(cid:16)
TQ −I
(cid:17)(cid:13)
(cid:13) ≤
poly √log(n)
(200)
(cid:13) (cid:13)n Q(cid:98)n(X) d Q(cid:98)n(X) d Q∗ d Q∗ d (cid:13)
(cid:13)
n
i=1 F
As a result, if (λ i)
i∈[d2]
are sorted in order, then one has λ(cid:98)i → λ
i
uniformly for i ∈ [d2] in
probability which further implies that
d2 d2
(cid:88) p (cid:88)
λ(cid:98)iw
i
→ λ iw
i
i=1 i=1
Then the continuous mapping theorem implies that q → q in probability. Then one can
(cid:98)1−α 1−α
obtain
(cid:16) (cid:17) (cid:16) (cid:17)
P T(cid:98)n > q
(cid:98)1−α
≤ P T(cid:98)n > q 1−α−ϵ +P(|q (cid:98)1−α−q 1−α| > ϵ)
Taking the limit as n → ∞, followed by letting ϵ → 0 to get that
(cid:16) (cid:17)
limsupP T(cid:98)n > q
(cid:98)1−α
≤ α (201)
n→∞
A similar lower bound shows
(cid:16) (cid:17)
liminfP T(cid:98)n > q
(cid:98)α
≥ α (202)
n→∞
Finally, combining (201) and (202) completes the proof.
G.1 Proof of Lemma 39
Proof of (198): the proof is similar to Lemma 33, and is hence omitted for brevity.
Proof of (199): First, recall differential properties (Lemma 17) that
(cid:13) (cid:13)
(cid:13)TQ(cid:13) ≤ λ (Q)·λ (Q)−1/2·λ (S)−1/2
(cid:13) S (cid:13) max min min
op (203)
(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) 1
(cid:12)(cid:12)(cid:12)dTQ(cid:12)(cid:12)(cid:12) ≤ λ (Q)1/2·λ (S)−2·λ (S)1/2
(cid:12)(cid:12)(cid:12) S (cid:12)(cid:12)(cid:12) 2 max min max
(cid:16) (cid:17) (cid:16) (cid:17)
Also, denote ϕ(Q,S) := TQ−I ⊗ TQ−I .
S d S d
With these results in place, one can obtain
(cid:13)(cid:16) (cid:17) (cid:16) (cid:17)(cid:13) (cid:13)(cid:16) (cid:17) (cid:16) (cid:17)(cid:13)
∥ϕ(Q,S)−ϕ(Q,Q∗)∥ ≤ (cid:13) TQ−I ⊗ TQ−TQ (cid:13) +(cid:13) TQ−TQ ⊗ TQ −I (cid:13)
F (cid:13) S d S Q∗ (cid:13) (cid:13) S Q∗ Q∗ d (cid:13)
F F
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
≤ (cid:13)TQ−I (cid:13) ·(cid:13)TQ−TQ (cid:13) +(cid:13)TQ−TQ (cid:13) ·(cid:13)TQ −I (cid:13)
(cid:13) S d(cid:13) (cid:13) S Q∗(cid:13) (cid:13) S Q∗(cid:13) (cid:13) Q∗ d(cid:13)
F F F F
(cid:13) (cid:13) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (cid:13) (cid:13) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
≤ (cid:13)TQ−I (cid:13) ·(cid:12)(cid:12)(cid:12)dTQ(cid:12)(cid:12)(cid:12)·∥S −Q∗∥ +(cid:13)TQ −I (cid:13) ·(cid:12)(cid:12)(cid:12)dTQ(cid:12)(cid:12)(cid:12)·∥S −Q∗∥
(cid:13) S d(cid:13) (cid:12)(cid:12)(cid:12) S′(cid:12)(cid:12)(cid:12) F (cid:13) Q∗ d(cid:13) (cid:12)(cid:12)(cid:12) S′(cid:12)(cid:12)(cid:12) F
F F
86where S′ lies between Q∗ and S. Note that Assumption 2 and the condition ∥S −Q∗∥ ≤
√ F
polylog(n)/ n implies that
S,Q∗ ∈ S ((2c )−1,2c )
d 1 1
Then for any S ∈ S ((2c )−1,2c ), one has
d 1 1
(cid:13) (cid:13) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12) (i) (cid:16) (cid:17)
E(cid:13)TQ−I (cid:13) ·(cid:12)(cid:12)(cid:12)dTQ(cid:12)(cid:12)(cid:12) ≲ E ∥X −µ∥3C1/2+1 ·∥X −µ∥C1/2
(cid:13) S d(cid:13) (cid:12)(cid:12)(cid:12) S′(cid:12)(cid:12)(cid:12)
F
(ii)
≲ 1
Here C is defined in Assumption 2, (i) follows from (203) and (ii) is a result of the sub-
1
Gaussianity of X. Similarly, one has
(cid:13) (cid:13) (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)
E(cid:13)TQ −I (cid:13) ·(cid:12)(cid:12)(cid:12)dTQ(cid:12)(cid:12)(cid:12) ≲ 1
(cid:13) Q∗ d(cid:13) (cid:12)(cid:12)(cid:12) S′(cid:12)(cid:12)(cid:12)
F
Combining results above, one can obtain
ζ ≤ sup E ∥ϕ(Q,S)−ϕ(Q,Q∗)∥
2 √ Q F
S:∥S−Q∗∥ ≤polylog(n)/ n
F
≲ sup 1·∥S −Q∗∥
√ F
S:∥S−Q∗∥ ≤polylog(n)/ n
F
polylog(n)
≤ √
n
The proof is then complete.
H Proof of Theorem 14
As argued at the beginning of Appendix C.2, one can assume without loss of generality that
√
∥X −µ∥ ≤ L almost surely with L = C logn for some constant C large enough as in (79).
L L
Recall the notation that M := γ (L).
L 1
Power under Frobenius norm: Firstwedemonstrateconcentrationofvariousquantities
of interest and derive their consequences in Lemma 40 below. The proof is in Appendix H.1.
Lemma 40. Instate the notations and assumptions in Theorem 14. Assume in addition that
∥X −µ∥ ≤ L almost surely. Then there exists an event E that satisfies P(E ) ≥ 1−O(n−100)
n n
for any P ∈ P, under which the following inequalities
∥X −µ∥ ≤ L, Q ∈ S (M−1,M ) for i ∈ [n] (204)
i i d L L
(cid:13) (cid:13) polylog(n)
x∈s Bu µp (L)(cid:13) (cid:13)Q(cid:98)n(x)−Q∗(x)(cid:13)
(cid:13) F
≤ √
n
(205)
(cid:18) (cid:19)
(cid:110) (cid:111) (cid:16) (cid:17)−1
Q(cid:98)n(x) : x ∈ B µ(L) ⊂ S
d
21/6M
L
,21/6M
L
(206)
(cid:88)n na2
∥Q∗(X )−Q∗(µ)∥2 ≥ n (207)
i F 2
i=1
(cid:13) (cid:13)2 polylog(n)
(cid:13) (cid:13)Q(cid:98)n(X)−Q∗(µ)(cid:13)
(cid:13) F
≤ √
n
(208)
(cid:12) (cid:12)
(cid:12) (cid:12)λ(cid:98)i(cid:12)
(cid:12)
≤ 2λ 1, i ∈ (cid:2) d2(cid:3) (209)
hold for n large enough.
87(cid:16) (cid:17)
Next, note that (31) in Lemma 17 implies that for any Q,S ∈ S (cid:0) 21/6M (cid:1)−1 ,21/6M
d L L
(here 21/6 is chosen only for technical computation), one has
√
1 (cid:16) (cid:17) (cid:16) (cid:17) 2
√ ≤ λ −dTQ ≤ λ −dTQ ≤ M3
2 2M3 min S max S 2 L
L
which then implies that under E , the following holds.
n
n
λ min(cid:16) H(cid:98)(cid:17) ≥ 1 (cid:88) λ min(cid:16) −dTQi (cid:17) ≥ √1
n Q(cid:98)n(X) 2 2M3
i=1 L
Therefore, under E , one has
n
n
1 (cid:88)(cid:13) (cid:13)2
T(cid:98)n ≥
8M6
(cid:13) (cid:13)Q(cid:98)n(X i)−Q(cid:98)n(X)(cid:13)
(cid:13)
F
(210)
L i=1
Then from the following decomposition
Q(cid:98)n(X i)−Q(cid:98)n(X) = Q∗(X i)−Q∗(µ)+Q(cid:98)n(X i)−Q∗(X i)+Q∗(µ)−Q(cid:98)n(X)
(cid:124) (cid:123)(cid:122) (cid:125)
∆i
one can obtain
n
(cid:88)(cid:13) (cid:13)2
(cid:13) (cid:13)Q(cid:98)n(X i)−Q(cid:98)n(X)(cid:13)
(cid:13)
F
i=1
n n n
( =i)(cid:88) ∥Q∗(X )−Q∗(µ)∥2 +2(cid:88) ⟨Q∗(X )−Q∗(µ),∆ ⟩+(cid:88) ∥∆ ∥2
i F i i i F
i=1 i=1 i=1
n (cid:32) n (cid:33)1/2 (cid:32) n (cid:33)1/2 n
(ii)(cid:88) (cid:88) (cid:88) (cid:88)
≥ ∥Q∗(X )−Q∗(µ)∥2 −2 ∥Q∗(X )−Q∗(µ)∥2 · ∥∆ ∥2 + ∥∆ ∥2
i F i F i F i F
i=1 i=1 i=1 i=1
 
≥(cid:32) (cid:88)n
∥Q∗(X i)−Q∗(µ)∥2
F(cid:33) ·1−2(cid:32)
(cid:80)n
∥(cid:80) Q∗n
i=
(X1∥ )∆ −i∥ Q2
F
∗(µ)∥2(cid:33)1/2

(211)
i=1 i=1 i F
Here(i)followsbydevelopingthesquare,(ii)isaconsequenceoftheCauchy-Schwarzinequality.
Lemma 40 implies that under E , one has
n
(cid:13) (cid:13) (cid:13) (cid:13)
∥∆ i∥
F
≤ (cid:13) (cid:13)Q(cid:98)n(X i)−Q∗(X i)(cid:13)
(cid:13)
+(cid:13) (cid:13)Q∗(µ)−Q(cid:98)n(X)(cid:13)
(cid:13)
F F
polylog(n)
≲ √ (212)
n
Therefore, (211) and (212) together imply the following inequality for n large enough.
(cid:88)n (cid:13) (cid:13)2 (I) na2 3
(cid:13) (cid:13)Q(cid:98)n(X i)−Q(cid:98)n(X)(cid:13)
(cid:13) F
≥ 2n ·
4
(213)
i=1
Here (I) arises due to Lemma 40, (212) as well as the fact that polylog(n) = o(cid:0) na2(cid:1).
n
Combining (210) and (213) then gives that under E , one has
n
3na2
T(cid:98)n ≥ 64Mn
6
L
88(cid:112)
≥ na2 (214)
n
for n large enough.
Denote q the 1 − α quantile of (cid:80)d2 2λ w , which is a fixed constant. Then as a
(cid:101)1−α i=1 1 i
consequence of Lemma 40, (cid:80)d i=2 1λ(cid:98)iw
i
is stochastically dominated by (cid:80)d i=2 12λ 1w
i
under E n,
which then implies that
q < q , under E
(cid:98)1−α (cid:101)1−α n
Therefore, for any P ∈ P, one can obtain
(cid:16) (cid:17) (cid:16)(cid:110) (cid:111) (cid:17)
P T(cid:98)n > q
(cid:98)1−α
≥ P T(cid:98)n > q
(cid:101)1−α
∩{q
(cid:101)1−α
> q (cid:98)1−α}
(cid:16)(cid:110) (cid:111) (cid:17)
≥ P T(cid:98)n > q
(cid:101)1−α
∩E
n
(i) (cid:16)(cid:110)(cid:112) (cid:111) (cid:17)
≥ P na2 > q ∩E
n (cid:101)1−α n
for n large enough. Here (i) follows from (214).
Finally, one can take n → ∞ to see that
(cid:16) (cid:17) (cid:16)(cid:110)(cid:112) (cid:111) (cid:17)
liminf inf P T(cid:98)n > q
(cid:98)1−α
≥ liminf inf P na2
n
> q
(cid:101)1−α
∩E
n
n→∞ P∈P n→∞ P∈P
≥ 1
which implies
(cid:16) (cid:17)
lim inf P T(cid:98)n > q
(cid:98)1−α
= 1
n→∞P∈P
The proof is then complete.
Power under Wasserstein distance: by Lemma 17 and the boundedness assumption,
one has
1
E∥Q∗(X)−Q∗(µ)∥2 ≥ EW2(Q∗(X),Q∗(µ))
F polylog(n)
Therefore, the 1st part of the proof (Frobenius norm) can be applied.
H.1 Proof of Lemma 40
(204)-(206) are shown in the proof of Theorem 6 and (208) is due to Lemma 27.
Proof of (207): With these in place, we have almost surely that
∥Q∗(X)−Q∗(µ)∥2 ≤ d∥Q∗(X)−Q∗(µ)∥2
F op
≤ dM2 =: M ≍ polylog(n) (215)
L F
Define random variable Y as follows.
i
Y := ∥Q∗(X )−Q∗(µ)∥2 −E∥Q∗(X)−Q∗(µ)∥2
i i F F
With (215) in place, one then has Y ∈ [−M ,M ], EY = 0 and
i F F i
VarY = EY2
i i
≤ E|Y |M
i F
89≤ 2E∥Q∗(X )−Q∗(µ)∥2 ·M
i F F
= 2a2 ·M
n F
To prove (207), it suffices to show that ,
(cid:12) (cid:12)
(cid:12)(cid:88)n (cid:12) na2
(cid:12) Y (cid:12) ≤ n with probability at least 1−O(n−100) (216)
(cid:12) i(cid:12) 2
(cid:12) (cid:12)
i
Tothisend, notethatbyBernstein’sinequality(Vershynin,2018, Theorem2.8.4), onehaswith
probability at least 1−O(n−100),
(cid:12) (cid:12)
n
(cid:12)(cid:88) (cid:12) (cid:112) (cid:112)
(cid:12) Y (cid:12) ≲ M logn+ nVarY · logn
(cid:12) i(cid:12) F i
(cid:12) (cid:12)
i
(cid:112) (cid:112)
≲ M logn+ na2M · logn
F n F
(i) (cid:16) (cid:112) (cid:17)
≤ polylog(n)· 1+ na2 (217)
n
Then (217) implies (216) for n large enough due to the assumption that na2 ≳ n2α2 for some
n
constant α > 0. The proof is then complete.
2
Proof of (209): Note that the proof of Lemma 39 does not assume the null hypothesis.
Therefore, with (208) in place, one can exactly follow (196)-(200) as in the proof of Lemma 39
to get
(cid:13) (cid:13)
n
(cid:13) (cid:13)1 (cid:88)(cid:16)
TQi −I
(cid:17) ⊗(cid:16)
TQi −I
(cid:17) −E(cid:16)
TQ −I
(cid:17) ⊗(cid:16)
TQ −I
(cid:17)(cid:13)
(cid:13) ≤
poly √log(n)
(cid:13) (cid:13)n Q(cid:98)n(X) d Q(cid:98)n(X) d Q∗ d Q∗ d (cid:13)
(cid:13)
n
i=1 F
which implies (209).
90