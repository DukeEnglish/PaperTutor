Physical Property Understanding from Language-Embedded Feature Fields
AlbertJ.Zhai YuanShen EmilyY.Chen GloriaX.Wang XinleiWang
ShengWang KaiyuGuan ShenlongWang
UniversityofIllinoisatUrbana-Champaign
Abstract Input RGB Materials Mass Density
Cancomputersperceivethephysicalpropertiesofobjects
solely through vision? Research in cognitive science and
visionsciencehasshownthathumansexcelatidentifying
materials and estimating their physical properties based
purely on visual appearance. In this paper, we present a
novelapproachfordensepredictionofthephysicalproper-
tiesofobjectsusingacollectionofimages. Inspiredbyhow
humansreasonaboutphysicsthroughvision,weleverage
largelanguagemodelstoproposecandidatematerialsfor Figure1.Estimatingphysicalpropertiesfromimages.Humans
eachobject. Wethenconstructalanguage-embeddedpoint can predict physical properties of objects by associating visual
appearanceswithgroundedknowledgeaboutmaterials.Wepropose
cloudandestimatethephysicalpropertiesofeach3Dpoint
toequipcomputerswiththiscapabilitybycombininglanguage-
usingazero-shotkernelregressionapproach. Ourmethod
embeddedfeaturefieldswithLLM-basedmaterialreasoning.
isaccurate,annotation-free,andapplicabletoanyobjectin
theopenworld. Experimentsdemonstratetheeffectiveness
robotics, agriculture, urban planning, graphics, and many
oftheproposedapproachinvariousphysicalpropertyrea-
otherdomains. Nevertheless,challengesremain. Oneisthe
soningtasks,suchasestimatingthemassofcommonobjects,
difficultyofacquiringlabeledground-truthdata. Forexam-
aswellasotherpropertieslikefrictionandhardness. Code
ple,considertheendeavorofmeasuringthemassofatree,
isavailableathttps://ajzhai.github.io/NeRF2Physics.
ormeasuringthethermalconductivitydenselythroughout
a coffee machine. Another is the highly uncertain nature
ofthepredictiontaskduetohavinglimitedobservations–
1.Introduction
thereissimplynowaytoknowwithcertaintywhatisinthe
Imaginethatyouareshoppinginahomeimprovementstore. interiorofanobjectwithoutadditionalinformation.
Eventhoughthereisahugevarietyoftoolsandfurniture This paper presents a training-free approach towards
itemsinthestore, youcanmostlikelymakeareasonable uncertainty-aware dense prediction of physical proper-
estimateofhowheavymostoftheobjectsareinaglance. ties from a collection of images. Our method, named
Nowimaginethatyouarehikinginaforestandtryingto NeRF2Physics,integratesobject-levelsemanticreasoning
cross a stream by stepping on some stones in the water. withpoint-levelappearancereasoning. First,weuseaneural
Simplybylookingatthestones, youareprobablyableto radiancefieldtoextractasetof3Dpointsontheobject’s
identifywhichstoneshaveenoughfrictiontowalkonand surfaceandfuse2Dvision-languagefeaturesintoeachpoint.
whichstoneswouldcauseyoutoslipandfallintothewater. Then,inspiredbyhowhumansreasonaboutphysicsthrough
Humansareremarkablyadeptatpredictingphysicalprop- vision, we draw upon the semantic knowledge contained
ertiesofobjectsbasedonvisualinformation. Researchin within large language models to obtain a set of candidate
cognitivescienceandhumanvisionhasshownthathumans materialsforeachobject. Finally,weestimatethephysical
make such predictions by associating visual appearances propertiesofeachpointusingazero-shotretrieval-basedap-
with materials that we have encountered before and have proachandpropagatetheestimatesacrosstheentireobject
rich,groundedknowledgeabout[9,10]. viaspatialinterpolation. Ourmethodisaccurate,annotation-
Itishighlydesirableforcomputerstobeequippedwith free,andapplicabletoanyobjectintheopenworld.
similar or even better capabilities of material perception. WeevaluateNeRF2Physicsonthetaskofmassestimation
Havingcomputationalmodelsforperceivingphysicsfrom usingtheABOdataset,aswellasourowndatasetofreal-
visual data is crucial for various applications, including worldobjectswithmanuallymeasuredfrictionandhardness
1
4202
rpA
5
]VC.sc[
1v24240.4042:viXraMulti-view Input Images 3D Reconstruction CLIP Feature Fusion
Physical Property Field
Zero-shot CLIP-based
Kernel Regression
Material Density (kg/m3)
“a gray chair” Aluminum 2700
Foam 60
Oak Wood 775
View Selection Captioning LLM Material Proposal
Figure2.OverviewofNeRF2Physics.Givenacollectionofposedimages,wefirsttrainaneuralradiancefieldtocapturethe3Dgeometry
ofthescene.Then,wefusevision-languagefeaturesintoapointcloudextractedfromthefield.Next,weuseacaptioningmodeltoprovide
atextdescriptionofthesceneandpromptanLLMtoproduceadictionaryofpossiblematerialsinthescene,alongwiththeirphysical
properties. Fromhere,physicalpropertiescanbeestimatedatanyquerypointusingzero-shotCLIP-basedkernelregressionwithinthe
dictionary.ThekernelregressionprocessisillustratedinmoredetailinFig.3.
values.Theresultsdemonstratethatourmethodoutperforms Languagegrounding. CLIPisalargepre-trainedvision-
otherzero-shotbaselinesandevensupervisedbaselinesfor language model that can efficiently learn visual concepts
massestimation. Visualizationsofthepredictedfieldsshow fromnaturallanguagesupervision[30]. Fortraining,CLIP
thatourapproachcanproducereasonablepredictionsofa jointlytrainsitsimageandtextencoderstopredictthecor-
variety of physical properties without supervision. In our rectpairingsofimageandtextexamplesinaself-supervised
ablationstudy,wecompareourmethodwithalternativeap- fashion. Duetoitssuccessincapturingdiversevisualcon-
proachesforlanguage-drivenphysicalpropertyestimation. cepts,CLIPhasbeenwidelyusedforzero-shotandfew-shot
tasks,spanningmanyapplicationsfromsceneunderstand-
ing[6,16,17,28,32]totexturegeneration[24]tolanguage-
2.RelatedWork grounded reasoning [12, 14, 19, 31, 40]. The models are
trainedviacontrastivelearningwithalargeamountofdata
Visualphysicsreasoning. Reasoningaboutphysicalprop- and are thus capable of associating meaningful language-
ertiesfromvisualdataisalongstandingproblem[2,11,38]. drivensemanticswithimagepatches. Ourworkleverages
Studies have shown that deep learning models can poten- CLIPembeddingsofsmallpatchestoprovideasolidfounda-
tiallycarrysimilarphysicalreasoningcapabilitiesashumans, tionforphysicalpropertyreasoninginazero-shotmanner.
includingestimationofobjectmass, friction, electriccon-
3.NeuralPhysicalPropertyFields
ductivity,andothermaterialproperties[37–39,44]. Most
ofthepriorworkfocusesondynamicreasoningofobject
3.1.Overview
propertiesbyeitherobservingtargetdynamics[21,38,44]
ordirectlyinteractingwiththetargetina3Dphysicalen- OurmethodtakesasinputacollectionofposedimagesIand
gine [29, 42]. A few studies have also tackled estimating producesaphysicalpropertyfieldρ(x)thatcanbequeried
materialpropertiesfromstaticimagesdirectly[3,4,34,35]. toobtainphysicalpropertyestimatesatanyoccupiedpoint
Althoughpromising,existingworkmostlytacklesspecific within the scene. In this work, we focus on single-object
types of material properties, e.g., mass or tenderness, by scenes,butourapproachcanbeextendedtomultiple-object
collectingcorrespondingtask-dependentdata. Incontrast, scenesaslongassegmentationmasksareavailable. Inspired
our method can generate diverse physical properties like byhowhumansperceiveandreasonaboutphysicalproper-
massdensity,friction,andhardnessinazero-shotmanner tiesoftheobjectstheyencounter, weproposetoleverage
fromasinglelanguage-embeddedfeaturefield. Ourwork language-visionembeddingsaswellaslargelanguagemod-
isgreatlyinspiredbypioneeringworkfromvisionandcog- els(LLMs)toachievethisgoal. Fig.2depictsanoverview
nitivescience. Studieshavefoundthathumansaregoodat ofourapproach. First,webuildalanguage-embeddedpoint
recognizingmanymaterialproperties,e.g.,thermalconduc- cloudfromwhichper-pointsemanticfeaturescanbequeried
tivityandhardness,fromonlyvisualinputs,evenwithonlya (Sec.3.2). WethenpromptanLLMtoproposeadictionary
briefdemonstration[9,10,33]. Ourworkseekstoempower M of candidate materials based on object semantics and
computationalmodelswithsuchperceptioncapabilitiesof apply zero-shot CLIP-based retrieval for reasoning about
recognizingadiverserangeofmaterialproperties. physicalpropertiesbasedonM(Sec.3.3). Finally,forphys-
2Language-embedded Point Cloud Material Dictionary Volumetric Integration
Material Density (kg/m3) Thickness (cm)
Aluminum 2700 0.2
Foam 60 3.0
Leather 900 0.35
….
Wood 750 2.0
Stainless Steel 7800 0.1 Mass: 11.5 kg
CLIP
Density: 845 kg/m3
Similarity
CLIP Features Weights Physical Property
Figure3.Overviewofzero-shotphysicalpropertyprediction.Topredictphysicalpropertyvaluesfromthelanguage-embeddedpoint
cloud,weextractCLIPfeaturesandperformkernelregressionusingthepredicteddictionaryofmaterialsandtheirproperties.Topredictthe
totalmassofanobject,wethenintegratethepredictedmassdensityacrosscuboidsonthesurfaceoftheobject. Thethicknessofeach
cuboidisestimatedinthesamewayastheotherphysicalproperties.
icalpropertiesthatrequirevolumetricintegration,suchas 3DfusionofCLIPfeaturesthatareconducivetoobject-level
mass,weproposeanintegrationmethodusingLLM-based segmentationduetotheuseofobject-levelregionproposals.
estimatesofsurfacethickness(Sec.3.4). However, these methods are not well-suited for discrimi-
natingbetweendifferentmaterialswithinthesameobject,
3.2.Language-embeddedpointcloud
whichisrequiredforourusecase.
Accurateestimationofphysicalproperties,suchasobject Inthiswork,weusesimpleaveragingtofuseCLIPem-
mass,requiresbothaccurategeometricandmaterialunder- beddingsofsmallpatchesintheinputimages,whichusually
standing of the scene. To capture the 3D geometry of a containenoughappearanceinformationtodiscriminatebe-
scene,wetrainaneuralradiancefield[25]andextractaset tweendifferentmaterials. Foreachsourcepoints∈S and
of 3D “source” points via depth rendering. We then fuse inputimageI∈I,wedeterminethepixelcoordinates(u,v)
per-patchCLIP[30]featuresintothesourcepointsusinga of the point projected using the camera parameters of the
simplevisibility-awareaveragingscheme. image. WethentestforocclusionusingtheNeRF-estimated
depth to determine if the point is visible in the image. If
it is visible, we extract a patch of size P ×P centered at
Neural radiance field. For each scene, we first train a
(u,v), and apply a CLIP image encoder to obtain a 512-
neuralradiancefield(NeRF)[25]onthegivenimagesand
dimensionalfeaturevectorforthatpatch. Ifsegmentation
cameraposes. WechoosetouseNeRFbecauseittendsto
masksareavailable,theycanbeappliedheretofocusonan
give higher-quality, more robust depth maps compared to
objectofinterest. Oncethisisdoneforalloftheinputim-
other3Dreconstructionmethods, especiallyforreflective
agesandsourcepoints,thepatchfeaturesareaverage-pooled
surfaces. WedirectlyusetheNerfactomethodfromNerfstu-
tocreateafusedfeaturevectorzforeachsourcepoints.
dio[36],whichcombinesseveralstate-of-the-arttechniques
forimprovingperformance. OncetheNeRFistrained,we
3.3.Physicalpropertyreasoning
randomlysampleatotalofN raysfromtheinputviewsand
estimatethedepthalongeachrayviamediandepthrender-
Ourlanguage-embeddedpointcloudcontainsrichsemantic
ing[36]. Finally,weconvertthedepthsinto3Dpointsand
featuresforeverysourcepointinthe3Dscene.Suchfeatures
performvoxeldown-samplingtoremoveredundantpoints.
areusuallytightlyrelatedtothephysicalpropertiesofthe
TheresultisapointcloudS ⊂R3,where|S|≤N. Wewill
object(s)inthescene(seeFig.4).Inthissection,wepropose
refertothesepoints,whichshouldcoverallofthevisible
atwo-stageapproachforestimatingphysicalpropertyvalues
surfacesinthescene,asthesourcepointsforthescene.
fromthesemanticfeaturesofthesourcepoints,whichcan
thenbepropagatedtoanypointinthecontinuousspaceby
3D language feature fusion. CLIP features have been spatialinterpolation. Inthefirststage, wepromptaVQA
showntoperformwellinseveralzero-shotimageclassifi- modeltoproposeadictionaryofcandidatematerialsalong
cationtasks[27,30],givingreasontobelievethattheycan withtheirphysicalpropertiesbasedontheinputimages. In
be successfully applied for material recognition. In order thesecondstage,weperformakernelregressionoverthe
toenable3DreasoningbasedonCLIP,onemustaggregate materialsinthedictionaryforeachsourcepointusingCLIP
CLIPfeatures withina3Drepresentation ofthescene. A similarityinazero-shotmanner. Formally,foragivenpoint
numberofworks[6,16,28,32]haveproposedmethodsfor s with semantic embedding z, we formulate its physical
3propertyas bepropagatedfromthesourcepointstoany3Dquerypoint
vianearest-neighborinterpolation:
ρ(s)=F(z,M)=F(z,G(I)), (1)
ρ(x)=ρ(argmin ||s−x||). (3)
s∈S
whereGisthemappingfromtheinputimagesItocandidate
3.4.Object-levelphysicalpropertyaggregation
materialsM,andF isthemappingfromsemanticfeatures
andcandidatematerialstoourdesiredphysicalproperty. Sofar,wehavediscusseddensepredictionofphysicalprop-
ertiesinaper-pointmanner. Inpractice, onemayalsobe
LLM-basedmaterialproposal. Thesetofdifferentmate- interestedinobject-levelphysicalproperties(e.g. mass)that
rialsthatexistintheworldisextremelylargeanddifficultto requireintegrationovervolumes. AlthoughNeRFgivesus
define. Furthermore,manymaterialslookidenticalandthus an estimate of the geometry of the visible surfaces in the
cannotbedistinguishedbylocalappearancealone. Despite scene,mostobjectscontainalargeamountofemptyspace
this,humansareabletoguessthematerialcompositionof intheirinterior,whichheavilyaffectsvolumetricintegration
objects through high-level reasoning about object seman- butcannotbecapturedbyNeRFduetoocclusion. Tocir-
ticsontopoflow-levelappearancecues. Inspiredbythis, cumventthis,wepromptthelargelanguagemodelagainto
ourmethodcallsuponLLMsforopen-vocabularysemantic estimatethethicknesst kofeachmaterialinM,andthenuse
reasoningaboutmaterialsandtheirphysicalproperties. theestimatedthicknesstodefineasetofcuboidssampled
Given a set of input images I, we first select a canon- ontheobject’ssurface. Similartothesourcepointsampling,
ical view I ∈ I. If segmentation masks are available, thecuboidlocationsaresampledbyvoxel-downsampling
0
wecalculatetheareaofthemaskineachframeandselect surfacepoints,andwedefinetheirsizetobed×d×τ,where
the frame at the 75th percentile (as a heuristic method to d is the voxel size and τ is the predicted thickness at that
obtain an informative view). If masks are not available, point. Formally,thepredictionmˆ fortheintegraloverthe
we select a view uniformly at random. We then use a cuboidsisgivenby
VQA model (BLIP-2 [20]) to produce a text description
of I 0. Finally, we pass this description to an LLM (GPT mˆ = (cid:88)
(cid:80)K
k=1exp(w k[x]/T)y kt kd2, (4)
3.5) and prompt it to return a dictionary of K candidate
(cid:80)K
exp(w [x]/T)
x∈V k=1 k
materials M = {(key ,y )}, where key is the mate-
k k k
rial name text and y is the value of the physical prop- whereV isthesetofvoxel-downsampledpoints. Sincethis
k
erty,e.g. {(Aluminum,2700kg/m3),(Oak Wood,650− volumeestimationcanbebiaseddependingonthegeometry
900kg/m3)}. Notethaty maybearangeofvaluesdueto oftheobject,weintroduceascalarmultiplicationfactorc
k
theinherentuncertaintyinthetask. determinedbyvalidation. Wealsoclampthetotalvolumeto
Although it is theoretically possible for a VQA model anupperboundestimatedbydepthcarvingtoavoidwildly
suchasBLIP-2[20],LLaVA[22],orGPT-4V[41]topro- inaccuratethicknesspredictions.
pose the materials directly from the image, we find that
4.Experiments
decomposingthetaskintotwopartsproducesmorereliable
resultsinourexperiments. Inthefuture, asVQAmodels
WeevaluateNeRF2Physicsacrosstwodatasets. First,we
becomemorepowerful,onemodelmaybesufficient.
compareNeRF2Physicswithexistingmethodsforper-object
massestimationonasetof500objectsfromtheAmazon
Zero-shotCLIP-basedkernelregression. Oncethema- BerkeleyObjects(ABO)dataset[7],whichwerefertoas
terialdictionaryMhasbeenobtained,weuseCLIPfeatures ABO-500. Toevaluatethedensepredictioncapabilitiesof
toperformmaterialretrievalforeachsourcepoint. Wepre- NeRF2Physics, we collect our own dataset of real-world
dictthephysicalpropertyvaluesofeachpointbytakingits objectswithper-pointfrictionandhardnessmeasurements.
fused CLIP features and conducting a CLIP-based kernel
4.1.ImplementationDetails
regressionusingthematerialdictionary. Formallyspeaking,
thephysicalpropertyfieldisequalto: OurNeRFdirectlyusestheNerfactomethodfromNerfstu-
dio[36]withdefaultsettingsexceptwiththenear-planefor
ρ(s)=F(z,M)=
(cid:80)K
k=1exp(w k[s]/T)y k, (2) sampling setto 0.4, the far-planeset to 6.0, and theback-
(cid:80)K exp(w [s]/T) groundcolorsettorandom. Forourowndataset,wesetthe
k=1 k
scenescaleto2.0. Thecameraposesarescaledperscene
wherew [s] = ψ (z,key )isthecosinesimilaritybe- tofitina±1box. Wetraineachscenefor20Kiterations,
k CLIP k
tweensemanticfeaturezandthelanguageCLIPfeatureof whichtakesaround8minutesonanNVIDIAA40GPU.
thematerialnamekey ,andT isatemperatureparameter For source point extraction, we sample N = 100000
k
chosenthroughvalidation. Thephysicalpropertyvaluescan rays,voxel-downsamplewithagridsizeof0.01(0.02for
4InputRGB CLIPFeatures MaterialSegmentation MassDensity(kg/m3)
Figure4. Examplevisualizations. WevisualizeinputimagesfromABO-500alongwithourmodel’sCLIPfeaturePCAcomponents,
zero-shotmaterialsegmentation,andpredictedmassdensity.Ourmodelmakesreasonablepredictionsofmaterialsacrossdifferentpartsof
objectsin3D,allowingforgroundedpredictionsofphysicalproperties.
5InputRGB ShoreHardness InputRGB FrictionCoefficient
Figure5.Examplepredictionsofdifferentphysicalproperties.Wevisualizepredictionsofhardnessandfrictiononobjectsfromour
owncollecteddataset.Forevaluationpurposes,ShoreAandShoreDhardnesswascombinedintothesamescale.Thefrictioncoefficient
representsthecoefficientofkineticfrictionagainstafabricsurface. Wequantitativelyevaluatethesepredictionsusingasetofsparse
per-pointmeasurements(seeSec.4.3).
ourowndataset),andremoveoutliers(seesupplementary Metrics. Wefollowpioneeringworkonvisualmassesti-
fordetails). Forlanguagefeaturefusion,weusetheOpen- mation[35]andreportthefollowingmetrics,wheremisthe
CLIP[15]ViTB-16modeltrainedonDataComp-1Band ground-truthmassandmˆ istheestimatedmass:
setthepatchsizetoP =56andtheocclusionthresholdto • Absolutedifferenceerror(ADE):|m−mˆ|,
0.01. Forcaptioning,weuseBLIP-2-Flan-T5-XL[20]. For • Absolutelogdifferenceerror(ALDE):|lnm−lnmˆ|,
massdensity(andthickness), weuseGPT-3.5Turbo[43] • Absolutepercentageerror(APE):|m−mˆ|,and
m
andsetK =5,T =0.1. Forfrictionandhardness,weuse • Minratioerror(MnRE):min(m,mˆ).
mˆ m
GPT-4[5]andsetK =3,T =0.01. Theexactpromptswe
We agree with the authors of [35] that MnRE is the pre-
usedcanbefoundinthesupplementary. Formassintegra-
ferredmetric,becauseitisnotbiasedtowardsmodelsthat
tion,wevoxel-downsamplewithagridsizeof0.005,carve
systematically over- or under-estimate and also does not
withagridsizeof0.002,andscalethefinalmassbyc=0.6.
over-emphasizeperformanceonheavierinstances.
Sinceourmodelusuallyreturnsarangeofvalues,wetake
thecenteroftherangeasthefinalprediction.
Baselines. WecompareNeRF2Physicswiththefollowing
4.2.MassEstimation
baselinesontheABO-500dataset:
Dataset. TheABOdataset[7]containsthousandsofprod- • Image2mass [35] uses a CNN to predict mass directly
uctssoldonAmazontogetherwithmulti-viewposedimages, fromasingleimageand3Dboundingboxdimensions.We
segmentationmasks,massmeasurements,andotherprod- evaluatetheofficialmodelpretrainedonAmazonproducts
uct metadata. In order to create a diverse evaluation set, anduseboundingboxesextractedfromoursourcepoints.
wecreatedastratifiedsampleof500objectsinwhicheach • 2DCNNtakesafrozenResNet50[13]pretrainedonIma-
“product_type”(e.g.“chair”,“lamp”)appearednomorethan geNet[8]andtrainsthreeaddtionallayerstopredictmass
10times. Eachobject/scenehas30viewsfacingtheobject onourdataset. WeapplyanegativeLogSigmoidlayerto
withcameracentersrandomlydistributedoverahemisphere ensurethatthepredictionsarepositive.
aroundtheobject. WecallthisdatasetABO-500andsplit • LLaVA[22]isalargevision-languagemodelthatisde-
thescenesrandomlyinto300train/100val/100test. signedtofollowarbitraryinstructionsgivenanimage. We
6Table1.MassestimationonABO-500testset(100objects).ADE Table2. AblationstudyformassestimationonABO-500valset
ismeasuredinkilograms.Bold:bestmodel. (100objects).Bold:bestmodel.
Method ADE(↓) ALDE(↓) APE(↓) MnRE(↑) Method ADE(↓) ALDE(↓) APE(↓) MnRE(↑)
Image2mass[35] 12.496 1.792 0.976 0.341 Nothickness 18.587 0.749 1.364 0.552
2DCNN 15.431 1.609 14.459 0.362 Retrieval(T →0) 12.266 0.780 0.801 0.536
LLaVA[22] 17.328 1.893 1.837 0.306 UniformCLIP 10.396 0.637 1.102 0.597
Ours 8.730 0.771 1.061 0.552 Ours 9.786 0.610 0.931 0.609
promptLLaVAtoestimatethemassoftheobjectinthe emptyspaceintheirinterior. Next,weexaminetheeffectof
image(seesupplementaryforexactprompt). 1 performingkernelregressioninsteadofjustretrievingthe
Forallofthebaselines,weprovidethesamecanonicalview mostlikelymaterialperpoint(effectivelysettingthetemper-
asinourmethod,inwhichthebackgroundissettowhite. atureT tozero). Here,theretrievalperformsworsebecause
thereisinherentuncertaintyinpredictingmaterialsbasedon
QualitativeResults. Weshowexamplevisualizationsof visualappearance. Lastly, weevaluatetheuseofasingle
ourlanguage-embeddedpointcloudandmaterialpredictions global CLIP embedding of the canonical view instead of
inFig.4. CLIPfeatureswereconvertedtoRGBvaluesac- fusedpatchembeddings,whichgivesauniformprediction
cordingtothetop3PCAcomponentsperobject. ThePCA across the whole object. We find that the performance is
visualization suggests that the CLIP features give enough onlyslightlyworse,suggestingthatthetotalmassformost
information to perform material segmentation. The mate- objectsisdominatedbyasinglematerial.
rial visualization shows that our method can propose rea-
4.3.FrictionandHardnessEstimation
sonable candidate materials and use the CLIP features to
identifytheprimarymaterialindifferentpartsofanobject, Dataset. The task of mass estimation does not directly
suchasmetalinthelegsofatableandwoodonthetable- evaluatetheabilityofourmodeltoperformdenseprediction
top. However,theboundariesofeachpartarenotlocalized ofdifferentphysicalpropertyvalueswithinthesameobject.
perfectly, and the model will often mix similar materials Tothebestofourknowledge,theredoesnotexistarealis-
together (e.g.“stainless steel” and “aluminum”). The last tic dataset with images and paired measurements suitable
column of visualizations show that sensible mass density forthispurpose. Thus,wecollectourowndatasetcontain-
estimatesfollowfromthematerialpredictions. ing15householdobjectsacross13sceneswithreal-world
multi-view images and paired measurements of per-point
QuantitativeResults. Wereporttest-setmassestimation kineticfrictioncoefficientandShorehardness. Theimages
metricsonABO-500inTab.1. Masspredictionsforallmod- and poses were captured using Polycam on an iPhone 13
elswereclippedtobebetween0.01and100kilograms. The Pro,withamedianof82perscene. Coefficientofkinetic
image2mass[35]pretrainedmodelperformspoorlysinceit frictionwascollectedon6surfacesusinganiOLabwithfab-
doesnotgeneralizewelltoobjectslargerthanthosefoundin ricpadsattached,averagingover10trials. Shorehardness
itstrainingdata. The2DCNNbaselinealsodidnotperform wascollectedat31pointsusingGainExpressA/Ddurome-
well–itfailedtolearnmeaningfulpatternsandtendedtopre- ters,averagingover3trialswithShoreDbeingusedwhen
dicttowardsthemeanofthedataset. TheLLaVA[22]model the Shore A reading was above 90. Each point’s location
usually gives answers that are not metrically precise (e.g. isannotatedaspixelcoordinatesinanimage. Grounding
1kgor10kg),despiteextensivepromptengineering. Our SAM[18,23]wasusedtoobtainobjectmasks.
zero-shotmethod’spredictionsoutperformthesebaselines NotethatShoreAandShoreDdurometersusedifferent
byalargemargininallmetricsexceptAPE.Wenotethat indentersandthusdonotmeasureexactlythesamephysical
APEisheavilybiasedtowardsmodelsthatunderestimate,as property[26]. However,forevaluationpurposes,wecom-
itisdominatedbyoverestimatesonsmallobjects. binethemeasurementsintoasinglescalefrom0-200,where
theShoreAmeasurementslieinthe0-100rangeandShore
Ablation Studies. We perform ablations on various as- Dmeasurementslieinthe100-200range.
pectsofourmethodinTab.2. First,weremovethethickness
estimationstepandintegrateoveroccupiedvoxelproduced
Metrics. We report the same metrics as before, along
by depth-based carving. This results in consistent overes-
with an additional metric of Pairwise Relationship Accu-
timation since it ignores the fact that many objects have
racy (PRA), defined as the classification accuracy of the
1WealsotriedtoapplyGPT-4Vforthistaskbuthaddifficultypreventing predictedrelationships(greaterthanorlessthan)between
itfromproducingcomplaintsaboutnothavingenoughinformation. everypairofpoints. Thismetricfocusesonrelativecompar-
7Table3.Estimationofper-pointShorehardnessonthereal-world Predicted Mass: 4.8 kg
in-housecollecteddataset(31points,11objects).Bold:bestmodel.
Method ADE(↓) ALDE(↓) APE(↓) MnRE(↑) PRA(↑)
GPT-4V 32.752 0.330 0.304 0.758 0.609 Predicted Mass: 10.4 kg
CLIP 32.857 0.294 0.266 0.774 0.647
Ours 34.295 0.315 0.276 0.765 0.710
Table4.Estimationofper-pointkineticfrictioncoefficientonthe
Predicted Mass: 40.9 kg
in-housecollecteddataset(6points,6objects).Bold:bestmodel.
Method ADE(↓) ALDE(↓) APE(↓) MnRE(↑) PRA(↑)
GPT-4V 0.209 0.430 0.549 0.692 0.467
CLIP 0.222 0.455 0.602 0.654 0.533 Figure6. Digitaltwinswithrealisticphysicalproperties. We
Ours 0.155 0.321 0.360 0.736 0.800 show that realistic physical interactions can be simulated using
mass-awaredigitaltwinscreatedbyNeRF2Physics.Ineachexam-
pletrajectoryvisualizationhere,theballhitstheobjectwiththe
isonsandisthusmorerobusttomeasurementnoise,whichis sameinitialmomentum,andfrictioniszero.
especiallysignificantforthehardnessmeasurementsdueto
localdeformationsintheobjectsurfacearoundeachpoint. distinguishingbetweendifferentsurfaceswithinthesame
scene. AlsonotethatGPT-4Vmustrunoneachindividual
querypoint,whichisextremelycomputationallyexpensive.
Baselines. Therearenoexistingmethodsforpredictions
Incontrast,oncethefeaturefieldandcandidatematerialsfor
ofarbitraryphysicalpropertiesfromimages,sowedesign
ourmodelhavebeenprepared,thousandsofpointscanbe
thefollowingbaselinesforcomparison:
queriedwithlittlecomputationalcost.
• GPT-4V [41] is a large vision-language model that can
acceptmasksinitsprompt. Foreachpoint, weprovide 4.4.Applications
GPT-4Vwiththeassociatedimageandamaskhighlight-
NeRF2Physicscanbeappliedtocreatephysicallyrealistic
ingitspixellocation,andaskittoestimatethephysical
digitaltwinsforimmersivecomputingandcontentcreation
propertyatthatpoint.
(Fig.6). Improvedphysicalpropertyunderstandingisalso
• CLIP refers to using a global CLIP embedding of the
crucial for advancing embodied AI and robot simulation.
canonical view instead of fused patch features in our
Anotherapplicationisestimatingcropbiomass[1],whichis
method. Thiswasalsoconsideredinourablationsabove.
importantforagriculturebutlabor-intensiveanddestructive
WeinstructeachLLMtochooseShoreA/Dhardnessbased
tomeasuremanually.
onwhichismoreappropriateforthematerialinquestion.
5.Conclusion
Qualitative Results. We show example predictions of
hardnessandfrictionfromourmodelinFig.5. Again,the WepresentedNeRF2Physics,anovelmethodfordensepre-
modelisabletodistinguishdifferentmaterialsandderive dictionofphysicalpropertiesfromacollectionofimages.
reasonablephysicalpropertyestimatesfromthem,evenfor Our method fuses vision-language embeddings into a 3D
unusual objects such as the ripped piece of cardboard. In pointcloudandleveragesLLMstoprovidematerialinfor-
addition,themodelisfairlyrobusttoerrorsinthegeometry mation,enablingzero-shotestimationofanyphysicalprop-
fromNeRF,thankstoourfeaturefusionstrategy. Theexam- ertyforanyobjectintheopenworld. Experimentalresults
plewiththebathmatdemonstratesthatourmethodcanbe demonstratethatourmethodoutperformsbaselinesones-
appliedwithorwithoutobjectsegmentationmasks. timationofmass,hardness,andfrictioncoefficientsacross
a variety of objects. In the future, our approach may be
QuantitativeResults. Wereportquantitativeevaluation improvedbyincorporatingpriorknowledgetoreasonabout
metrics for hardness prediction in Tab. 3 and for friction materialsininternalobjectpartsthatcannotbeseen.
predictioninTab.4. Forhardness,weobservethatallthree
models perform similarly across most of the metrics, but Acknowledgments. ThisprojectissupportedbytheIn-
oursachievesthehighestPRA,indicatingthatitlocalizes tel AI SRS gift, the IBM IIDAI Grant, the Insper-Illinois
differentmaterialsmorepreciselythantheothermodels. For InnovationGrant,theNCSAFacultyFellowship,theAgroe-
friction,wefindthatourmodeloutperformstheothersbya cosystemSustainabilityCenteratUIUC,andNSFAwards
widemargininallofthemetrics.GPT-4Vperformssimilarly #2331878,#2340254,and#2312102. Wegreatlyappreciate
withtheuniformCLIPmodel,suggestingthatithastrouble NCSAforprovidingcomputingresources.
8References ceptfusion:Open-setmultimodal3dmapping. arXivpreprint
arXiv:2302.07241,2023. 2,3
[1] Estimating cover crop biomass. https://www.nrcs.usda.
[17] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo
gov/sites/default/files/2022-09/EstBiomassCoverCrops_
Kanazawa,andMatthewTancik.LERF:Languageembedded
Sept2018.pdf. Accessed:2023-11-17. 8
radiancefields. InICCV,2023. 2
[2] EdwardHAdelson. Onseeingstuff:theperceptionofmateri-
[18] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
alsbyhumansandmachines. InHumanvisionandelectronic
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
imagingVI.SPIE,2001. 2
head,AlexanderCBerg,Wan-YenLo,etal. Segmentany-
[3] SeanBell,KavitaBala,andNoahSnavely. Intrinsicimages thing. arXivpreprintarXiv:2304.02643,2023. 7
inthewild. SIGGRAPH,2014. 2 [19] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
[4] SeanBell,PaulUpchurch,NoahSnavely,andKavitaBala. Koltun, and Rene Ranftl. Language-driven semantic seg-
Materialrecognitioninthewildwiththematerialsincontext mentation. InICLR,2022. 2
database. InCVPR,2015. 2 [20] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-
[5] SébastienBubeck,VarunChandrasekaran,RonenEldan,Jo- 2: Bootstrappinglanguage-imagepre-trainingwithfrozen
hannesGehrke,EricHorvitz,EceKamar,PeterLee,YinTat imageencodersandlargelanguagemodels. arXivpreprint
Lee,YuanzhiLi,ScottLundberg,etal. Sparksofartificial arXiv:2301.12597,2023. 4,6
generalintelligence: Earlyexperimentswithgpt-4. arXiv [21] XuanLi,Yi-LingQiao,PeterYichenChen,KrishnaMurthy
preprintarXiv:2303.12712,2023. 6 Jatavallabhula,MingLin,ChenfanfuJiang,andChuangGan.
[6] BoyuanChen,FeiXia,BrianIchter,KanishkaRao,Keerthana PAC-NeRF:Physicsaugmentedcontinuumneuralradiance
Gopalakrishnan,MichaelSRyoo,AustinStone,andDaniel fieldsforgeometry-agnosticsystemidentification. InICLR,
Kappler. Open-vocabularyqueryablescenerepresentations 2022. 2
forrealworldplanning. InICRA,2023. 2,3 [22] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
[7] Jasmine Collins, Shubham Goel, Kenan Deng, Achlesh- Visualinstructiontuning. InNeurIPS,2023. 4,6,7
war Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, [23] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
TomasFYagoVicente,ThomasDideriksen,HimanshuArora, Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun
MatthieuGuillaumin,andJitendraMalik. Abo:Datasetand Zhu,etal. Groundingdino: Marryingdinowithgrounded
benchmarksforreal-world3dobjectunderstanding.InCVPR, pre-training for open-set object detection. arXiv preprint
2022. 4,6 arXiv:2303.05499,2023. 7
[8] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLi [24] OscarMichel,RoiBar-On,RichardLiu,SagieBenaim,and
Fei-Fei. Imagenet:Alarge-scalehierarchicalimagedatabase. RanaHanocka. Text2mesh:Text-drivenneuralstylizationfor
InCVPR,2009. 6 meshes. InCVPR,2022. 2
[25] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
[9] RolandW.Fleming. Visualperceptionofmaterialsandtheir
JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf:
properties. VisionResearch,2014. 1,2
Representingscenesasneuralradiancefieldsforviewsynthe-
[10] RolandWFleming,ChristianeWiebel,andKarlGegenfurtner.
sis. InECCV,2020. 3
Perceptualqualitiesandmaterialclasses. Journalofvision,
[26] MagdyIMohamedandGamalAAggag. Uncertaintyevalua-
2013. 1,2
tionofshorehardnesstesters. Measurement,33(3):251–257,
[11] KaterinaFragkiadaki, PulkitAgrawal, SergeyLevine, and
2003. 7
JitendraMalik. Learningvisualpredictivemodelsofphysics
[27] Zachary Novack, Julian McAuley, Zachary Lipton, and
forplayingbilliards. ICLR,2016. 2
Saurabh Garg. Chils: Zero-shot image classification with
[12] SamirYitzhakGadre,MitchellWortsman,GabrielIlharco,
hierarchicallabelsets. InICML,2023. 3
LudwigSchmidt,andShuranSong. Cliponwheels: Zero-
[28] SongyouPeng,KyleGenova,ChiyuJiang,AndreaTagliasac-
shotobjectnavigationasobjectlocalizationandexploration.
chi, Marc Pollefeys, Thomas Funkhouser, et al. Open-
arXivpreprintarXiv:2203.10421,2022. 2
scene:3dsceneunderstandingwithopenvocabularies. arXiv
[13] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. preprintarXiv:2211.15654,2022. 2,3
Deepresiduallearningforimagerecognition.InCVPR,2016.
[29] LerrelPinto,DhirajGandhi,YuanfengHan,Yong-LaePark,
6
and Abhinav Gupta. The curious robot: Learning visual
[14] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, representationsviaphysicalinteractions. InECCV,pages
JoshuaBTenenbaum,andChuangGan. 3dconceptlearning 3–18.Springer,2016. 2
andreasoningfrommulti-viewimages. InCVPR,2023. 2 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[15] GabrielIlharco,MitchellWortsman,RossWightman,Cade Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Gordon,NicholasCarlini,RohanTaori,AchalDave,Vaishaal AmandaAskell,PamelaMishkin,JackClark,etal. Learning
Shankar,HongseokNamkoong,JohnMiller,HannanehHa- transferablevisualmodelsfromnaturallanguagesupervision.
jishirzi,AliFarhadi,andLudwigSchmidt. OpenCLIP,2021. InICML,2021. 2,3
https://github.com/mlfoundations/open_clip. 6 [31] AdamRashid,SatvikSharma,ChungMinKim,JustinKerr,
[16] KrishnaMurthyJatavallabhula,AlihuseinKuwajerwala,Qiao LawrenceYunliangChen,AngjooKanazawa,andKenGold-
Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, berg. Languageembeddedradiancefieldsforzero-shottask-
SoroushSaryazdi,NikhilKeetha,AyushTewari,etal. Con- orientedgrasping. InCoRL,2023. 2
9[32] NurMuhammadMahiShafiullah,ChrisPaxton,LerrelPinto,
SoumithChintala,andArthurSzlam. Clip-fields: Weakly
supervisedsemanticfieldsforroboticmemory.arXivpreprint
arXiv:2210.05663,2022. 2,3
[33] LavanyaSharan,RuthRosenholtz,andEdwardAdelson. Ma-
terialperception:Whatcanyouseeinabriefglance? Journal
ofVision,2009. 2
[34] LavanyaSharan,CeLiu,RuthRosenholtz,andEdwardH
Adelson. Recognizingmaterialsusingperceptuallyinspired
features. IJCV,2013. 2
[35] Trevor Standley, Ozan Sener, Dawn Chen, and Silvio
Savarese. image2mass: Estimating the mass of an object
fromitsimage. InCoRL,2017. 2,6,7
[36] MatthewTancik,EthanWeber,EvonneNg,RuilongLi,Brent
Yi, Terrance Wang, Alexander Kristoffersen, Jake Austin,
KamyarSalahi,AbhikAhuja,etal. Nerfstudio:Amodular
frameworkforneuralradiancefielddevelopment. InSIG-
GRAPH,2023. 3,4,11
[37] Manik Varma and Andrew Zisserman. A statistical ap-
proachtomaterialclassificationusingimagepatchexemplars.
TPAMI,2008. 2
[38] JiajunWu,IlkerYildirim,JosephJLim,BillFreeman,and
JoshTenenbaum. Galileo:Perceivingphysicalobjectprop-
ertiesbyintegratingaphysicsenginewithdeeplearning. In
NeurIPS,2015. 2
[39] JiajunWu,JosephJLim,HongyiZhang,JoshuaBTenen-
baum,andWilliamTFreeman. Physics101:Learningphysi-
calobjectpropertiesfromunlabeledvideos. InBMVC,2016.
2
[40] YuefanWu,ZeyuanChen,ShaoweiLiu,ZhongzhengRen,
andShenlongWang.Casa:Category-agnosticskeletalanimal
reconstruction. NeurIPS,2022. 2
[41] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,
Chung-ChingLin,ZichengLiu,andLijuanWang. Thedawn
oflmms:Preliminaryexplorationswithgpt-4v(ision). arXiv
preprintarXiv:2309.17421,9,2023. 4,8
[42] ShaoxiongYaoandKrisHauser. Estimatingtactilemodels
ofheterogeneousdeformableobjectsinrealtime. InICRA,
2023. 2
[43] Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao,
ShichunLiu,YuhanCui,ZeyangZhou,ChaoGong,Yang
Shen, etal. Acomprehensivecapabilityanalysisofgpt-3
andgpt-3.5seriesmodels. arXivpreprintarXiv:2303.10420,
2023. 6
[44] IlkerYildirim,JiajunWu,YilunDu,andJoshuaBTenenbaum.
Interpretingdynamicscenesbyaphysicsengineandbottom-
upvisualcues. 2
10Physical Property Understanding from Language-Embedded Feature Fields
Supplementary Material
Abstract
System:Youwillbeprovidedwithcaptionsthateachdescribe
animageofanobject.Thecaptionswillbedelimitedwithquotes
Inthefollowingsupplementarymaterial,weprovidead- ("). Basedonthecaption,giveme5materialsthattheobject
ditionalexperimentaldetails(Sec.A)andadditionalresults
mightbemadeof,alongwiththemassdensities(inkg/m3)of
eachofthosematerials.Youmayprovidearangeofvaluesfor
(Sec.B).Weinvitereaderstowatchthesupplementaryvideo
themassdensityinsteadofasinglevalue.Trytoconsiderallthe
(supp.mp4)forfurthervisualization. possiblepartsoftheobject.Donotincludecoatingslike"paint"
inyouranswer.
FormatRequirement:
A.AdditionalExperimentalDetails You must provide your answer as a list of 5 (material: mass
density)pairs,eachseparatedbyasemi-colon(;).Donotinclude
anyothertextinyouranswer,asitwillbeparsedbyacodescript
A.1.PointCloudExtractionDetails
later.Youranswermustlooklike:
(material 1: low-high kg/m3);(material 2: low-high
WeusethedefaultpointcloudexporterinNerfstudio[36], kg/m3);(material 3: low-high kg/m3);(material 4: low-high
which allows one to define a bounding box for the points kg/m3);(material5:low-highkg/m3)
to keep. For ABO-500, we use a bounding box of size
1×1×1 centered at (0,0,0). For our in-house dataset,
weuseaboundingboxofsize1.5×1.5×1.5centeredat Figure8.Promptusedforproposingmaterialsandprovidingtheir
(0,0,−0.75). Pointsarefilteredoutiftheaveragedistance massdensityvalues.
totheir20nearestneighborsisover10standarddeviations
awayfromthemean.
System:Youwillbeprovidedwithcaptionsthateachdescribe
animage.Thecaptionswillbedelimitedwithquotes(").Based
A.2.PromptingDetails onthecaption,giveme3materialsthatthesurfacesintheimage
mightbemadeof,alongwiththekineticfrictioncoefficientof
We provide the prompts used for captioning (Fig. 7) and eachmaterialwhenslidingagainstafabricsurface. Youmay
providearangeofvaluesforthefrictioncoefficientinsteadofa
materialproposalwithmassdensity(Fig.8),frictioncoef-
singlevalue.Trytoconsiderallthepossiblesurfaces.
ficient(Fig.9),andhardness(Fig.10)values. Theprompt
used for thickness estimation is provided in Fig. 11. We FormatRequirement:
also provide the prompts used for LLaVA mass estima- Youmustprovideyouranswerasalistof3(material: friction
coefficient)pairs,eachseparatedbyasemi-colon(;). Donot
tion (Fig. 12), GPT-4V friction estimation (Fig. 13), and
includeanyothertextinyouranswer,asitwillbeparsedbya
GPT-4Vhardnessestimation(Fig.14). codescriptlater.Youranswermustlooklike:
(material1: low-high);(material2: low-high);(material3: low-
high)
Trytoprovideasnarrowofarangeaspossibleforthefriction
Question:Giveadetaileddescriptionoftheobject.Answer:
coefficient.
Figure7.PromptusedforcaptioningwithBLIP-2.
Figure9.Promptusedforproposingmaterialsandprovidingtheir
frictioncoefficients.
A.3.CNNBaselineDetails
B.AdditionalResults
Our CNN baseline takes one RGB image and outputs the
B.1.Young’sModulusandThermalConductivity
predictedmass. WeuseanImageNet-pretrainedResNet50
backbonewiththelastclassificationlayerremoved. Aset Ourmethodcanbeusedtopredictphysicalpropertiesinan
of fully connected layers with ReLU activations follows open-vocabularymanner. Weshowthatitcanbeusedtopre-
the ResNet50. Within the fully connected layers, feature dictYoung’smodulusandthermalconductivityonobjects
dimensionsareasfollows: 2048,32,16,and1. Weattach fromABO-500(val)inFig.15. Forvisualizationpurposes,
a LogSigmoid layer to ensure the output is non-negative. weforcethecandidatematerialstobethesameasthosepro-
OurinputimageisnormalizedbasedonImageNetstatistics posedformassdensityestimation. Theexactpromptsused
beforefeedingintoourmodel. Wetrainfor20epochsusing fortheseresultscanbefoundinFig.16(Young’smodulus)
anAdamoptimizerwithalearningrateofα=0.001. andFig.17(thermalconductivity). WeuseGPT-3.5Turbo
11System:Youwillbeprovidedwithcaptionsthateachdescribe System:Youwillbeprovidedwithcaptionsthateachdescribe
animageofanobject.Thecaptionswillbedelimitedwithquotes animageofanobject,alongwithasetofpossiblematerialsused
("). Basedonthecaption,giveme3materialsthattheobject tomaketheobject.Foreachmaterial,estimatethethickness(in
might be made of, along with the hardness of each of those cm)ofthatmaterialintheobject. Youmayprovidearangeof
materials.ChoosewhethertouseShoreAhardnessorShoreD valuesforthethicknessinsteadofasinglevalue.
hardnessdependingonthematerial.Youmayprovidearangeof
valuesforhardnessinsteadofasinglevalue.Trytoconsiderall FormatRequirement:
thepossiblepartsoftheobject. Youmustprovideyouranswerasalistof5(material:thickness)
pairs,eachseparatedbyasemi-colon(;). Donotincludeany
FormatRequirement: othertextinyouranswer,asitwillbeparsedbyacodescript
Youmustprovideyouranswerasalistof3(material:hardness, later.Youranswermustlooklike:
ShoreA/D)tuples,eachseparatedbyasemi-colon(;). Donot (material1:low-highcm);(material2:low-highcm);(material3:
includeanyothertextinyouranswer,asitwillbeparsedbya low-highcm);(material4: low-highcm);(material5: low-high
codescriptlater.Youranswermustlooklike: cm)
(material1:low-high,<ShoreAorShoreD>);(material2:low-
high,<ShoreAorShoreD>);(material3:low-high,<ShoreAor User:Caption:"alampwithawhiteshade"Materials:"fabric,
ShoreD>) plastic,metal,ceramic,glass"
MakesuretouseShoreAorShoreDhardness,notMohshard- Assistant: (fabric: 0.1-0.2 cm);(plastic: 0.3-1.0 cm);(metal:
ness. 0.1-0.2cm);(ceramic:0.2-0.5cm);(glass:0.3-0.8cm)
User:Caption:"agreyottoman"Materials:"wood,fabric,foam,
metal,plastic"
Assistant: (wood: 2.0-4.0 cm);(fabric: 0.2-0.5 cm);(foam:
Figure10.Promptusedforproposingmaterialsandprovidingtheir
5.0-15.0cm);(metal:0.1-0.2cm);(plastic:0.5-1.0cm)
Shorehardnessvalues.
User: Caption: "a white frame" Materials: "plastic, wood,
aluminum,steel,glass"
Assistant:(plastic:0.1-0.3cm);(wood:1.0-1.5cm);(aluminum:
andakernelregressiontemperatureofT =0.01. 0.1-0.3cm);(steel:0.1-0.2cm);(glass:0.2-0.5cm)
User: Caption: "a metal rack with three shelves" Materials:
B.2.FailureCases "steel,aluminum,wood,plastic,iron"
Assistant: (steel: 0.1-0.2cm);(aluminum: 0.1-0.3cm);(wood:
WeshowexamplefailurecasesofNeRF2PhysicsinFig.18, 1.0-2.0cm);(plastic:0.5-1.0cm);(iron:0.5-1.0cm)
covering the two main failure modes. In the case of the
woodenbox,BLIP-2mistakestheobjectforabrick,causing
GPTtooutputerroneousmaterialssuchassandandconcrete.
Figure11. Promptusedforestimatingthethicknessofproposed
Withnocorrectmaterialsinthedictionary,theCLIP-based
materials. Sincethisisasomewhatconfusingtask, weprovide
regressionisunabletoproduceaccuratepredictions. Onedi-
afewin-contextexamplestohelptheLLMunderstandwhatwe
rectionforfutureworkcouldbetoimplementamorerobust meanbythickness.
viewselectionstrategytoavoidsuchrecognitionfailures.
Inthecaseoftheblackcart,thecaptionandmaterialsare
correct,buttheCLIP-basedregressionmistakesthebulkof System:Estimatethemassoftheobjectinkilograms.Provide
thecartassteelinsteadofplastic. Thisoccursbecausethe youranswerasonlyadecimalnumber.
localappearancesofblack-paintedsteelandblack-painted
plastic can look identical, and the patch-based CLIP fea-
Figure12.PromptusedforestimatingmasswithLLaVA.
turesdonotcontainenoughglobalinformationtoaccurately
distinguishbetweenthem.
System: You will be given an image, followed by a mask
specifying a point on the image. Estimate the coefficient of
kineticfrictionbetweenafabricsurfaceandthesurfaceatthat
pointintheimage.
FormatRequirement:
Youmustprovideeitherasinglenumberorarange(e.g. "0.6-
0.8")asyouranswer.Giveyourbestguess.Donotincludeany
othertextinyouranswer,asitwillbeparsedbyacodescript
later.
Figure13.PromptusedforestimatingfrictionwithGPT-4V.
12InputRGB MaterialSegmentation MassDensity(kg/m3) Young’sMod.(logGPa) ThermalCond.(logW/mK)
Figure15.Examplepredictionsofdifferentphysicalproperties.Wevisualizemorezero-shotpredictionsofmassdensityofobjectsfrom
ABO-500,alongwithpredictionsofYoung’smodulusfieldsandthermalconductivityfields.Ourmethodproducesaccuratepredictions
acrossawidevarietyofobjectsandmaterials.
InputRGB InputRGB
BLIP-2Caption MaterialSegmentation MassDensity(kg/m3)
(DisplayView) (CanonicalView)
a grey brick on a
white background
a black utility
cart with two
shelves
Figure18. Examplefailurecases. WevisualizethemainfailuremodesofNeRF2Physicsintheaboveexamples. Thefirstexample
demonstratesobjectrecognitionfailureatthecaptioningstage,andthesecondexampledemonstratesmaterialrecognitionfailureatthe
CLIP-basedretrievalstage.
13System: You will be given an image, followed by a mask
specifyingapointontheimage. Estimatethehardnessofthe
objectintheimageatthegivenpoint. Choosewhethertouse
ShoreAhardnessorShoreDhardnessdependingonthematerial.
FormatRequirement:
Youmustprovideapairofeitherasinglenumberorarange(e.g.
"0.6-0.8")andwhetheritisinShoreAorShoreDasyouranswer.
Giveyourbestestimate,andmakesuretouseShoreAorShore
Dhardness,notMohshardness.Donotincludeanyothertextin
youranswer.Youranswermustlooklike:
(number,<ShoreAorShoreD>)
Figure14.PromptusedforestimatinghardnesswithGPT-4V.
System:Youwillbeprovidedwithcaptionsthateachdescribe
animageofanobject,alongwithasetofpossiblematerialsused
to make the object. For each material, estimate the Young’s
modulus(inGPa)ofthatmaterialintheobject.Youmayprovide
arangeofvaluesfortheYoung’smodulusinsteadofasingle
value.
FormatRequirement:
Youmustprovideyouranswerasalistof5(material:Young’s
modulus)pairs,eachseparatedbyasemi-colon(;).Donotinclude
anyothertextinyouranswer,asitwillbeparsedbyacodescript
later.Youranswermustlooklike:
(material1:low-highGPa);(material2:low-highGPa);(material
3:low-highGPa);(material4:low-highGPa);(material5:low-
highGPa)
Figure16.PromptusedforestimatingYoung’smodulus.
System:Youwillbeprovidedwithcaptionsthateachdescribe
animageofanobject, alongwithasetofpossiblematerials
usedtomaketheobject.Foreachmaterial,estimatethethermal
conductivity(inW/mK)ofthatmaterialintheobject.Youmay
providearangeofvaluesforthethermalconductivityinsteadof
asinglevalue.
FormatRequirement:
Youmustprovideyouranswerasalistof5(material: thermal
conductivity)pairs,eachseparatedbyasemi-colon(;). Donot
includeanyothertextinyouranswer,asitwillbeparsedbya
codescriptlater.Youranswermustlooklike:
(material 1: low-high W/mK);(material 2: low-high
W/mK);(material 3: low-high W/mK);(material 4: low-high
W/mK);(material5:low-highW/mK)
Figure17.Promptusedforestimatingthermalconductivity.
14