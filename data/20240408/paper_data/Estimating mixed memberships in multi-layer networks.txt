Estimating mixed memberships in multi-layer networks
HuanQinga,
∗
aSchoolofEconomicsandFinance,LabofFinancialRiskIntelligentEarlyWarningandModernGovernance,ChongqingUniversityof
Technology,Chongqing,400054,China
Abstract
Communitydetectioninmulti-layernetworkshasemergedasacrucialareaofmodernnetworkanalysis. However,
conventionalapproachesoften assume that nodes belong exclusively to a single community, which fails to capture
thecomplexstructureofreal-worldnetworkswherenodesmaybelongtomultiplecommunitiessimultaneously. To
addressthislimitation,weproposenovelspectralmethodstoestimatethecommonmixedmembershipsinthemulti-
layermixedmembershipstochasticblockmodel. Theproposedmethodsleveragethe eigen-decompositionofthree
aggregatematrices: thesumofadjacencymatrices,thedebiasedsumofsquaredadjacencymatrices,andthesumof
squaredadjacencymatrices. Weestablishrigoroustheoreticalguaranteesfortheconsistencyofourmethods. Specif-
ically,we deriveper-nodeerrorratesundermildconditionsonnetworksparsity, demonstratingtheirconsistencyas
the numberof nodesand/orlayersincreases underthe multi-layer mixedmembershipstochastic block model. Our
theoreticalresultsrevealthatthemethodleveragingthesumofadjacencymatricesgenerallyperformspoorerthanthe
othertwomethodsformixedmembershipestimationinmulti-layernetworks.Weconductextensivenumericalexper-
imentstoempiricallyvalidateourtheoreticalfindings.Forreal-worldmulti-layernetworkswithunknowncommunity
information,weintroducetwonovelmodularitymetricstoquantifythequalityofmixedmembershipcommunityde-
tection.Finally,wedemonstratethepracticalapplicationsofouralgorithmsandmodularitymetricsbyapplyingthem
toreal-worldmulti-layernetworks,demonstratingtheireffectivenessinextractingmeaningfulcommunitystructures.
Keywords: Multi-layernetworks,communitydetection,spectralmethods,mixedmembership,modularity
1. Introduction
Multi-layernetworkshaveemergedasapowerfultoolfordescribingcomplexreal-worldsystems. Unlikesingle-
layer networks, these structures consist of multiple layers, where nodesrepresententities and edgesrepresenttheir
interactions(Muchaetal., 2010; Kivela¨etal., 2014; Boccalettietal., 2014). In this paper, we focuson multi-layer
networkswiththesame nodessetinalllayersandnodessolelyinteractingwithintheirrespectivelayers, excluding
anycross-layerconnections. Suchnetworksareubiquitous,spanningfromsocialnetworkstotransportationsystems
and biological networks. For instance, in social networks, individuals tend to form connections within different
platforms(e.g., Facebook, Twitter, WeChat, LinkedIn),while cross-platformconnectionsare typicallynot allowed.
Intransportationnetworks,layersmightrepresentdifferentmodesoftransportation(roads,railways, airways),with
nodes corresponding to locations and edges indicating the availability of a specific mode between two locations
(Boccalettietal., 2014). The absence of cross-layer connections indicates the inability to directly switch modes
withoutintermediatestops. Biologicalnetworksalsoexhibitrichmulti-layerstructures.Ingeneregulatorynetworks,
layerscould representdifferenttypesof gene interactions, with nodesrepresentinggenes and edgesdepicting their
specific interactions(Narayananetal., 2010; Bakkenetal., 2016; Zhang&Cao, 2017). The absence of cross-layer
connections underscores the specialized nature of interactions within each layer and their vital role in governing
cellularfunctions.
Correspondingauthor.
∗
Emailaddress:qinghuan@cqut.edu.cn &qinghuan@u.nus.edu&qinghuan07131995@163.com(HuanQing)
Preprintsubmittedto April8,2024
4202
rpA
5
]IS.sc[
1v61930.4042:viXraCommunity detection in multi-layer networks is a crucialanalytical tool, revealing latent structures within net-
works(Kim&Lee,2015;Huangetal.,2021).Acommunity(alsoknownasagroup,cluster,orblock)typicallycom-
prisesnodesmoredenselyinterconnectedthanthoseoutside(Newman,2003b;Newman&Girvan,2004;Fortunato,
2010; Fortunato&Hric, 2016; Javedetal., 2018). In social networks, individualsoften form distinct communities
based on interests, occupations, or locations. For instance, on Facebook, users might belong to hobby-basedcom-
munities like photography or hiking, identifiable by dense interaction patterns among members. In transportation
networks,communitiesemergeduetogeographicalproximityorfunctionalsimilarity.Forinstance,citiesmightform
communities linked by tightly integrated railways. In practical applications, a node can simultaneously belong to
multiple communities. For example, in social networks, an individualmay be a member of severaldifferentsocial
groups. Similarly,intransportationnetworks,a specificlocationcanfunctionasahubformultiplemodesoftrans-
portation,bridgingdiversecommunities. Likewise,inbiologicalnetworks,agenecanbelongtoseveraloverlapping
communities,participatinginawiderangeofprocesses.
In the last few years, community detection in multi-layer networks, where each node belongs exclusively to a
single community, has attracted considerableattention. For instance, severalstudies have been proposedunder the
multi-layerstochasticblockmodel(MLSBM),a modelthatassumesthenetworkforeachlayercan bemodeledby
the well-known stochastic block model (SBM) (Hollandetal., 1983). MLSBM also assumes that the community
informationiscommontoalllayers. UndertheframeworkwithinMLSBM,(Hanetal.,2015)studiedtheconsistent
communitydetectionformaximumlikelihoodestimateandaspectralmethoddesignedbyrunningtheK-meansalgo-
rithmonafeweigenvectorsofthesumofadjacencymatriceswhenonlythenumberoflayersgrows. (Paul&Chen,
2020) studied the consistency of co-regularized spectral clustering, orthogonallinked matrix factorization, and the
spectralmethodin(Hanetal.,2015)asboththenumberofnodesandthenumberoflayersgrowwithintheMLSBM
context.Moreover,(Leietal.,2020)establishedconsistencyresultsforaleastsquaresestimationofcommunitymem-
bershipswithintheMLSBMframework. Recently,(Lei&Lin,2023)studiedtheconsistencyofabias-adjusted(i.e.,
debiased)spectralclusteringmethodusinganoveldebiasedsumofsquaredadjacencymatricesunderMLSBM.Their
numerical experiments demonstrated that this approach significantly outperformsthe spectral method proposed by
(Hanetal., 2015). Also see (Pensky&Zhang, 2019; Jingetal., 2021; Chen&Mo, 2022; Suetal., 2023) for other
recentworksexploringvariantsofMLSBM.
However,the MLSBM modelhasonesignificantlimitation, beingtailored exclusivelyformulti-layernetworks
with non-overlappingcommunities. The mixed membershipstochastic block (MMSB) model(Airoldietal., 2008)
isapopularstatisticalmodelforcapturingmixedcommunitymembershipsinsingle-layernetworks. UnderMMSB,
somemethodshavebeendevelopedtoestimatemixedmembershipsforsingle-layernetworksgeneratedfromMMSB,
includingvariationalexpectation-maximization(Airoldietal.,2008;Gopalan&Blei,2013),nonnegativematrixfac-
torization (Psorakisetal., 2011; Wangetal., 2011), tensor decomposition (Anandkumaretal., 2014), and spectral
clustering (Maoetal., 2021; Qing&Wang, 2023; Jinetal., 2024). In this paper, we consider the problem of es-
timating nodes’ common community memberships in multi-layer networks generated from the multi-layer mixed
membership stochastic block (MLMMSB) model, a multi-layer version of MMSB. The main contributions of this
paperaresummarizedasfollows:
• Weintroducethreespectralmethodsforestimatingmixedmembershipsinmulti-layernetworksgeneratedfrom
MLMMSB.Thesemethodsemployvertex-huntingalgorithmsonafewselectedeigenvectorsofthreeaggregate
matrices: the sum of adjacency matrices, the debiased sum of squared adjacency matrices, and the sum of
squaredadjacencymatrices.
• We establish per-nodeerrorratesfor these threemethodsundermildconditionsonnetworksparsity, demon-
stratingtheirconsistentmixedmembershipestimationasthenumberofnodesand/orlayersincreaseswithinthe
MLMMSBframework. Ourtheoreticalanalysisrevealsthatthemethodutilizingthedebiasedsumofsquared
adjacencymatricesconsistentlyoutperformsthemethodusingthesumofsquaredadjacencymatricesinterms
oferrorrate. Additionally,bothmethodsgenerallyexhibitlowererrorratesthanthemethodbasedonthesum
of adjacency matrices. This underscoresthe advantageof debiased spectral clustering in mixed membership
communitydetectionformulti-layernetworks. Tothebestofourknowledge,thisisthefirstworktoestimate
mixedmembershipsusingtheabovethreeaggregatematricesandthefirstworktoestablishconsistencyresults
withintheMLMMSBframework.
2• To assess the quality of mixed membership community detection in multi-layer networks, we introduce two
novelmodularitymetrics:fuzzysummodularityandfuzzymeanmodularity.Thefirstisderivedfromcomput-
ingthefuzzymodularityofthesumofadjacencymatrices,whilethesecondisobtainedbyaveragingthefuzzy
modularityofadjacencymatricesacrossindividuallayers. Tothe bestofourknowledge,ourtwo modularity
metricsarethefirsttomeasurethequalityofmixedmembershipcommunitydetectioninmulti-layernetworks.
• Weconductextensivesimulationstovalidateourtheoreticalfindingsanddemonstratethepracticaleffectiveness
ofourmethodsandmetricsthroughreal-worldmulti-layernetworkapplications.
Theremainderofthispaperisstructuredasfollows:Section2introducesthemodel,followedbySection3outlin-
ingthemethodsdeveloped. Section4presentsthetheoreticalresults. Subsequently,Section5includesexperimental
results on computer-generated multi-layer networks, while Section 6 focuses on real-world multi-layer networks.
Lastly,Section7concludesthepaperandtechnicalproofsareprovidedinAppendix A.
Notation. We employthe notation[m] to denotethe set 1,2,...,m . Furthermore, I standsfor the m-by-m
m m
{ } ×
identitymatrix. Foravector x, x representsitsl norm. Whenconsideringamatrix M andanindexset sthatis
q q
k k
a subset of [m], M(s,:) refersto the sub-matrix of M comprisingthe rows indexedby s. Additionally, M denotes
′
thetransposeofM, M isitsFrobeniusnorm, M representsthemaximumabsoluterowsum, M signifies
F 2
k k k k∞ k k →∞
the maximum row-wise l norm, rank(M) gives its rank, and λ (M) stands for the k-th largest eigenvalue of M in
2 k
magnitude. ThenotationE[]isusedtodenoteexpectation,whileP()representsprobability. Finally,e isdefinedas
i
· ·
theindicatorvectorwitha1inthei-thpositionandzeroselsewhere.
2. Multi-layermixedmembershipstochasticblockmodel
Throughout,we consider undirected and unweightedmulti-layer networkswith n commonnodes and L layers.
Forthe l-th layer,let then n symmetricmatrix A beits adjacencymatrixsuch that A(i, j) = A(j,i) = 1 if there
l l l
×
is an edge connecting node i and node j and A(i, j) = A(j,i) = 0 otherwise, for all i [n], j [n],l [L], i.e.,
l l
∈ ∈ ∈
A = A 0,1 n n. Additionally,weallowforthepossibilityofself-edges(loops)inthispaper. Weassumethatthe
l ′l
∈ { }
×
multi-layernetworkconsistsofK commoncommunities
, ,..., . (1)
1 2 K
C C C
Throughout,weassumethatthenumberofcommunitiesK isknowninthispaper. Thechallengingtaskoftheo-
reticallyestimatingK inmulti-layernetworksexceedsthescopeofthispaper. LetΠ [0,1]n K bethemembership
×
∈
matrix such that Π(i,k) representsthe “weight” that node i belongsto the k-th community for i [n],k [K].
k
C ∈ ∈
SupposethatΠsatisfiesthefollowingcondition
rank(Π)= K,0 Π(i,k) 1, Π(i,:) =1fori [n],k [K]. (2)
1
≤ ≤ k k ∈ ∈
Wecallnodeia“pure”nodeifoneentryofΠ(i,:)is1whiletheother(K 1)elementsarezerosanda“mixed”
−
nodeotherwise.Assumethat
Thek thcommunity hasatleastonepurenodefork [K]. (3)
k
− C ∈
BasedonEquation(3),let betheindexsetofpurenodessuchthat = p ,p ,...,p with p 1,2,...,n
1 2 K k
I I { } ∈ { }
being an arbitrary pure node in the k-th community for k [K]. Similar to (Maoetal., 2021), without loss of
k
C ∈
generality,wereorderthenodessuchthatΠ( ,:)=I .
K K
SupposethatallthelayersshareacommI onmixed× membershipmatrixΠbutwithpossiblydifferentedgeproba-
bilities. Inparticular,we workwitha multi-layerversionofthepopularmixedmembershipstochasticblockmodel
(MMSB)(Airoldietal.,2008). Tobeprecise,wedefinethemulti-layerMMSBbelow:
Definition1. SupposethatEquations(1)-(3)aresatisfied,themulti-layermixedmembershipstochasticblockmodel
(MLMMSB)forgeneratingamulti-layernetworkwithadjacencymatrices A L isasfollows:
{ l }l=1
Ω :=ρΠBΠ A(i, j)=A(j,i) Bernoulli(Ω(i, j))fori [n], j [n],l [L], (4)
l l ′ l l l
∼ ∈ ∈ ∈
whereB = B [0,1]K K forl [L]andρ (0,1].
l ′l
∈
×
∈ ∈
3Since B can vary for different l, A generated by Equation (4) may have different expectation Ω E[A] for
l l l l
≡
l [L]. Notably,whenL = 1,theMLMMSBmodeldegeneratestothepopularMMSBmodel. Whenallnodesare
∈
pure,MLMMSBreducestotheMLSBMstudiedinpreviousworks(Hanetal.,2015;Paul&Chen,2020;Leietal.,
2020; Lei&Lin, 2023). Define = B ,B ,...,B . Equation(4) says that MLMMSB is parameterizedby Π,ρ,
1 2 L
and . For brevity, we denoteMB LMM{ SB defined by} Equation(4) as MLMMSB(Π,ρ, ). Since P(A(i, j) = 1) =
l
B B
Ω(i, j) = ρΠ(i,:)BΠ (j,:) ρ, weseethatdecreasingthevalueofρresultsinasparsermulti-layernetwork,i.e.,ρ
l l ′
≤
controlstheoverallsparsityofthemulti-layernetwork.Forthisreason,wecallρthesparsityparameterinthispaper.
Weallowthesparsityparameterρtogotozerobyeitherincreasingthenumberofnodesn,thenumberoflayersL,
orbothsimultaneously. Wewillstudytheimpactofρontheperformanceoftheproposedmethodsbyconsideringρ
inourtheoreticalanalysis. TheprimalgoalofcommunitydetectionwithintheMLMMSBframeworkistoaccurately
estimatethecommonmixedmembershipmatrixΠ fromtheobservedLadjacencymatrices A L . Thisestimation
{ l }l=1
taskiscrucialforunderstandingtheunderlyingcommunitystructureinmulti-layernetworks.
3. Spectralmethodsformixedmembershipestimation
In thissection, we proposethreespectralmethodsdesignedto estimate the mixedmembershipmatrix Π within
the MLMMSB modelfor multi-layer networks in which nodes may belong to multiple communitieswith different
weights. We recall that Ω is the expectation of the l-th observed adjacency matrix A for l [L] and Π contains
l l
∈
the common communitymembershipsfor all nodes. To provide intuitions of the designs of our methods, we con-
sider the oracle case where Ω L are directly observed. First, we define two distinct aggregate matrices formed
by Ω L : Ω Ω{ al n}l d=1 S˜ Ω2, where the former represents the sum of all expectation adja-
cen{ cyl m}l= a1 trices su ,m th≡
e
latl t∈e[ rL] isl
the
sumsu om f≡
all
sql ∈u[L a] redl
expectation adjacency matrices, and both matrices contain the
information about thP e mixed membershipsP of nodes in the multi-layer network since Ω = ρΠ( B)Π and
sum l [L] l ′
S˜ = ρ2Π( BΠΠB)Π. Sincerank(Π) = K,itiseasytoseethatrank(Ω ) = K ifrank( ∈B) = K and
sum l [L] l ′ l ′ sum l [L] l
rank(S˜ )= K∈ifrank( B2)= K. AssumingthatthenumberofcommunitiesKismuchsmallerP∈thanthenumber
ofnodesu sm n,wP eobservethl ∈a[ tL] Ω l andS˜ possesslow-dimensionalstructure. Thislow-rankpropP ertyiscrucialfor
sum sum
thedevelopmentofourP spectralmethods,asitallowsustoefficientlyextractmeaningfulinformationfromthehigh-
dimensionaldata.Buildingontheseinsights,wepresentthefollowinglemmathatcharacterizesthegeometriesofthe
twoaggregatematricesΩ andS˜ . Thislemmaformsthetheoreticalfoundationforourmethods,enablingusto
sum sum
developaccurateandcomputationallyefficientalgorithmsforestimatingthemixedmembershipmatrixΠwithinthe
MLMMSBmodel.
Lemma1. (IdealSimplexes)UnderthemodelMLMMSB(Π,ρ, ),dependingontheconditionsimposedontheset
B
B L ,wearriveatthefollowingconclusions:
{ l }l=1
1. When rank( B) = K: LetUΣU denotethe top K eigen-decompositionofΩ where the n K matrix
U satisfiesU
l U∈[L =] Il andthek-thd′
iagonalentryoftheK K
diagonalmatrixΣsum representsthe×
k-thlargest
′ K K
eigenvalue(P inmagnit× ude)ofΩ fork [K]. Then,wehave× U =ΠU( ,:).
sum
∈ I
2. Whenrank( B2)= K: LetVΛV representthetopK eigen-decompositionofS˜ wherethen K matrix
V fulfillsV
Vl =∈[L I] l andthek-thdia′
gonalentryoftheK K
diagonalmatrixΛiss tu hm ek-thlargest×
eigenvalue
′ K K
(inmagnituP de)ofS˜× fork [K]. Inthiscase,wehaveV× =ΠV( ,:).
sum
∈ I
Recall that Π satisfies Equations (2) and (3), by U = ΠU( ,:) according to Lemma 1, the rows of U forms a
K-simplexin RK, referredto as the IdealSimplex of U (IS foI r brevity). Notably, the K rows of U( ,:) serve as
U
I
its vertices. Given that Π satisfies Equations(2) and (3), and U = ΠU( ,:), it follows that U(i,:) = Π(i,:)U( ,:)
I I
fori [n]. ThisimpliesthatU(i,:)isaconvexlinearcombinationoftheK verticesofIS withweightsdetermined
U
∈
by Π(i,:). Consequently, a pure row of U (call U(i,:) pure row if node i is a pure node and mixed row otherwise)
lies on one of the K vertices of the simplex, while a mixed row occupies an interior position within the simplex.
Such simplex structure is also observed in mixed membership estimation for community detection in single-layer
networks(Maoetal.,2021;Qing&Wang,2023;Jinetal.,2024;Qing&Wang,2024)andintopicmatrixestimation
for topic modeling (Ke&Wang, 2024). Notably, the mixed membership matrix Π can be precisely recovered by
setting Π = UU 1( ,:) provided that the corner matrix U( ,:) is obtainable. Thanks to the simplex structure in
−
I I
U =ΠU( ,:),theverticesofthesimplexcanbefoundbyavertex-huntingtechniquesuchasthesuccessiveprojection
I
4(SP) algorithm (Arau´joetal., 2001; Gillis&Vavasis, 2013, 2015). Applying SP to all rows of U with K clusters
enablestheexactrecoveryoftheK K cornermatrixU( ,:). DetailsoftheSPalgorithmareprovidedinAlgorithm
× I
1(Gillis&Vavasis,2015). SPcanefficientlyfindtheKdistinctpurerowsofU. Similarly,V =ΠV( ,:)alsoexhibits
I
a simplexstructure, leadingto the recoveryof Π via Π = VV 1( ,:) by Lemma 1. Consequently, applyingthe SP
−
I
algorithmtoV withK clustersalsoyieldsanexactreconstructionofthemixedmembershipmatrixΠ.
Remark 1. Let ˜ represent another index set, consisting of arbitrary pure nodes p˜ ,p˜ ,...,p˜ from respective
1 2 K
communities fI ork [K]. AccordingtotheproofofLemma1,wehaveU = ΠU(˜,:). Furthermore,thisimplies
k
that U = ΠUC (˜,:) =∈ ΠU( ,:), which in turn signifies that U(˜,:) is equivalenttoI U( ,:). In essence, the corner
I I I I
matrixU( ,:)remainsunchangedregardlessofthespecificpurenodeschosentoformtheindexset. Ananalogous
I
argumentholdsforV( ,:).
I
Forreal-worldmulti-layernetworks,theLadjacencymatricesA ,A ,...,A areobservedinsteadoftheirexpec-
1 2 L
tations. For our first method, set A = A. We have E[A ] = Ω under MLMMSB. Subsequently, let
sum l [L] l sum sum
UˆΣˆUˆ bethetopK eigen-decompositionofA∈ whereUˆ isanorthogonalmatrixwithUˆ Uˆ = I andΣˆ isadiag-
′ sum ′ K K
P ×
onalmatrixwithitsk-thdiagonalelementrepresentingthek-thlargesteigenvalueofA inmagnitudefork [K].
sum
Giventhattheexpectationof A isΩ , itfollowsthatUˆ canbeinterpretedasaslightlyperturbedversion∈ ofU.
sum sum
Toproceed,we applythe SPalgorithmtoallrowsofUˆ with K clusters, resultinginanestimatedindexsetofpure
nodes,denotedas ˆ. WeinferthatUˆ(ˆ,:)shouldcloselyapproximatethecornermatrixU( ,:).Finally,weestimate
themixedmemberI shipmatrixΠbycoI mputingUˆUˆ 1(ˆ,:). Ourfirstalgorithm,called“SucI cessiveprojectiononthe
−
I
sumofadjacencymatrices”(SPSum,Algorithm1)summarisestheaboveanalysis. Itefficientlyestimatesthemixed
membershipmatrixΠbyexecutingtheSPalgorithmonthetopKeigenvectorsofA .
sum
Algorithm1SPSum
Require: AdjacencymatricesA ,A ,...,A ,andnumberofcommunitiesK.
1 2 L
Ensure: EstimatedmixedmembershipmatrixΠˆ.
1: ComputeA sum = l [L]A l.
2: GetUˆΣˆUˆ ′,thetopK∈ eigen-decompositionofA sum.
3: ApplytheSPalgoP rithmonallrowsofUˆ withK clusterstoobtaintheestimatedindexset ˆ.
4:
SetΠˆ =max(0,UˆUˆ −1(ˆ,:)). I
5: NormalizeeachrowofI Πˆ byitsl 1norm:Πˆ(i,:)= kΠˆΠˆ (( ii ,, :: ))
k1
fori ∈[n].
Inoursecondmethodologicalapproach,wedefinetheaggregatematrixS asthesumofthesquaredadjacency
sum
matrices,adjustedforbias,specificallyasS = (A2 D). Here,D representsadiagonalmatrix,withitsi-th
diagonalentrybeingthedegreeofnodeiinsu lm ayerl,l ∈i[ .L e]
.,
Dl − (i,i)l
=
Al
(i, j)fori [n]andl [L]. Thematrix
S wasfirstintroducedin(Lei&Lin,2023)
wiP thinthecl ontextofMj ∈[ Ln] SBl
M,
serving∈ asadebias∈
edestimate ofthe
sum
sumofsquaredadjacencymatrices.Thisdebiasingisnecessaryas P A2aloneprovidesabiasedapproximationof
l [L] l
Ω2. BysubtractingthediagonalmatrixD fromeachsquareda∈djacencymatrix,wecaneffectivelyremovethis
bial ∈ s[ ,L] renl deringS areliableestimatorofS˜ l asdemonstratedinP (Lei&Lin,2023).Subsequently,letVˆΛˆVˆ bethe
sum sum ′
P topK eigen-decompositionofS . GiventhatVˆ providesacloseapproximationofV,applyingtheSPalgorithmto
sum
allrowsofVˆ yieldsareliableestimateofthecornermatrixV( ,:).Insummary,oursecondspectralmethod,centered
I
onS ,isoutlinedinAlgorithm2. Werefertothismethodas“Successiveprojectiononthedebiasedsumofsquared
sum
adjacencymatrices”(SPDSoSforbrevity).
The third method utilizes the summation of squared adjacency matrices, expressed as A2, as a substitute
l [L] l
for S in Algorithm 2. Notably, this approach excludes the crucial bias-removal step inhe∈rent in S . We call
sum sum
P
this method “Successive projection on the sum of squared adjacency matrices”, abbreviated as SPSoS. In the sub-
sequentsection,wewilldemonstratethatSPDSoSconsistentlyoutperformsSPSoS,andbothmethodsaregenerally
theoreticallysuperiortoSPSum.
5Algorithm2SPDSoS
Require: AdjacencymatricesA ,A ,...,A ,andnumberofcommunitiesK.
1 2 L
Ensure: EstimatedmixedmembershipmatrixΠˆ.
21 :: GCo etm Vˆp Λu ˆt Ve
ˆ
′S ,s tu hm e= topKl ∈[ eL] i( gA e2 l n-− deD cl o) m.
positionofS sum.
3: AppletheSPalgoP rithmonallrowsofVˆ withK clusterstoobtaintheestimatedindexset ˆ.
4:
SetΠˆ =max(0,UˆUˆ −1(ˆ,:)). I
5: UpdateΠˆ byΠˆ(i,:)= I Πˆ(i,:) fori [n].
kΠˆ(i,:) k1 ∈
4. Mainresults
In thissection, we demonstratethe consistencyof ourmethodsbypresentingtheir theoreticalupperboundsfor
per-node error rates as the number of nodes n and/or the number of layers L increases within the context of the
MLMMSB model. Assumption 1 provides a prerequisite lower bound for the sparsity parameter ρ to ensure the
theoreticalvalidityofSPSum.
Assumption1. (sparsityrequirementforSPSum)ρnL τ2log(n+L),whereτ=max (A(i, j) Ω(i, j)).
≥ i ∈[n],j ∈[n] | l ∈[L] l − l |
L,
τIn haA ss as num upp pti eo rn b1 o, uth ne dc Lh .oi Hce owof evτ ep rl ,a ty os ca oc nr su ic di ea rl aro nle evin ende st pe ar rm sein ri sn cg enth ae ril oe ,v wel eof ins tp ra or ds uit cy er aeg ciP om ne s. taS ni tnc pe
ositl i∈v[L e]
vA
al
l( ui, ej β)
≤
P
thatisstrictlylessthanLandremainsfixedregardlessofthegrowthofnorL. Thisensuresthattheaggregatenumber
ofedgesconnectinganytwo nodesiand j acrossalllayersremainsbelowβ evenin thelimitasn or L approaches
infinity. Undertheseconditions,Assumption1isrevisedtoreflectamorestringentrequirement: ρ
β2log(n+L).
This
≥ nL
revisedassumptioncharacterizesthemostchallengingscenarioforcommunitydetection. Toestablishthetheoretical
boundsforSPSum,weneedthefollowingrequirementon .
B
Assumption2. (SPSum’srequirementon ) λ ( B) c Lforsomeconstantc >0.
B | K l ∈[L] l |≥ 1 1
L
mA ats rs iu cem sp ,t ai no dn i2
t
i is
s
rm eail sd ona as
b|
lλ
eK
t(
o
pl ∈r[ eL s]
uB ml)
e|
r te hp ar tesP λen (ts the s Bm )al il ses ot nsi tn hg eu ola rr dev ral ou fe Ore (Lsu ).lti Tn og sf ir mom plit fh ye os uu rm tm hea ot rio etn ico af
l
analysis,wealsointroducethefolP lowingconditio| n:K l ∈[L] l |
P
Condition1. K =O(1)andλ (ΠΠ)=O(n).
K ′ K
Condition1ismildasK =O(1)impliesthatthenumberofcommunitiesremainsconstant,whileλ (ΠΠ)=O(n)
K ′ K
ensuresthebalanceofcommunitysizes,wherethesizeofthek-thcommunity isdefinedas Π(i,k)fork [K].
OurmainresultforSPSumoffersanupperboundonitsper-nodeerrorrateintC erk msofρ,n,andLi ∈u[n n] dertheMLM∈
MSB
P
framework.
Theorem 1. (Per-node error rate of SPSum) Under MLMMSB(Π,ρ, ), when Assumption 1, Assumption 2, and
Condition1hold,letΠˆ beobtainedfromtheSPSumalgorithm,thereexB istsaK K permutationmatrix suchthat
× P
withprobabilityatleast1 o( 1 ),wehave
− n+L
log(n+L)
max e(Πˆ Π ) =O( ).
i ∈[n] k ′i − P k1 s ρnL
AccordingtoTheorem1,itbecomesevidentthatasn(and/orL)approachesinfinity,SPSum’serrorratediminishes
to zero, highlighting the advantage of employing multiple layers in community detection and SPSum’s consistent
communitydetection.Additionally,Theorem1saysthattoattainasufficientlylowerrorrateforSPSum,thesparsity
parameterρmustdecreaseatarateslowerthan log(n+L) (i.e.,ρmustbesignificantlygreaterthan log(n+L)).Thisfinding
nL nL
aligns with the sparsity requirementstated in Assumption 1 for SPSum. It is noteworthythat the theoreticalupper
boundforSPSum’sper-nodeerrorrate,givenasO( log(n+L))inTheorem1,isindependentofhowweselectthevalue
ρnL
ofτinAssumption1. Thisunderscoresthegeneralqityofourtheoreticalfindings.
6Theassumption3statedbelowestablishesthelowerboundofthesparsityparameterρthatensurestheconsistency
ofbothSPDSoSandSPSoS.
Assumption3. (sparsityrequirementforSPDSoSandSPSoS)ρ2n2L τ˜2log(n+L),whereτ˜ =max max
i [n] j [n]
(A(i,m)A(m, j) Ω(i,m)Ω(m, j)). ≥ ∈ ∈
| l ∈[L] m ∈[n] l l − l l |
P AnaPlogoustoAssumption1,whenconsideringanextremelysparsescenariowhereτ˜ doesnotexceedapositive
constant β˜, Assumption 3 dictates that ρ must satisfy ρ 1 β˜2log(n+L). This requirementaligns with the sparsity
≥ n L
requirementinTheorem1of(Lei&Lin,2023). ThesubsequeqntassumptionservesasimilarpurposetoAssumption
2.
Assumption4. (SPDSoS’sandSPSoS’srequirementon ) λ ( B2) c Lforsomeconstantc >0.
B | K l ∈[L] l |≥ 2 2
Theorems2and3arethemainresultsofSPDSoSandSPSoSP,respectively.
Theorem 2. (Per-node error rate of SPDSoS)Under MLMMSB(Π,ρ, ), when Assumption 3, Assumption 4, and
Condition1hold,letΠˆ beobtainedfrom theSPDSoSalgorithm,thereeB xistsa K K permutationalmatrix ˜ such
× P
thatwithprobabilityatleast1 o( 1 ),wehave
− n+L
log(n+L) 1
max e(Πˆ Π˜) =O( )+O( ).
i ∈[n] k ′i − P k1 s ρ2n2L n
Theorem3. (Per-nodeerrorrateofSPSoS)UnderthesameconditionsasinTheorem2andsupposeρnL log(n+L),
letΠˆ beobtainedfromtheSPSoSalgorithm,withprobabilityatleast1 o( 1 ),wehave ≥
− n+L
log(n+L) 1
max e(Πˆ Π˜) =O( )+O( ).
i ∈[n] k ′i − P k1 s ρ2n2L ρn
AccordingtoTheorems2and3, itisevidentthatbothSPDSoSandSPSoSenjoyconsistentmixedmembership
estimation. Thisconsistencyarises fromthe factthat their per-nodeerror ratestend towardszero as the numberof
nodesn(and/orthenumberoflayersL)approachesinfinity.Furthermore,similartoTheorem1,thetheoreticalbounds
outlinedinTheorems2and3areindependentoftheselectionofτ˜ inAssumption3.
BytheproofofTheorem3,weknowthatSPSoS’stheoreticalupperboundofper-nodeerrorrateisthesummation
of SPDSoS’s theoretical upper bound of per-node error rate and O( 1). Therefore, SPDSoS’s error rate is always
ρn
smallerthanthatofSPSoSandthisindicatesthebenefitofthebias-removalstepinS . Furthermore,tocompare
sum
thetheoreticalperformancesbetweenSPSumandSPSoS,weconsiderthefollowingtwosimplecases:
• WhenSPSoS’serrorrateisattheorder 1, ifSPSumsignificantlyoutperformsSPSoS, wehave log(n+L)
ρn ρnL ≪
1 L ρnlog(n+L). q
ρn ⇔ ≫
• WhenSPSoS’serrorrateisattheorder log(n+L),ifSPSumsignificantlyoutperformsSPSoS,wehave log(n+L)
ρ2n2L ρnL ≪
log(n+L) ρn 1 ρ 1. Since qlog(n+L) 1,wehaveρ log(n+L). Combineρ log(n+L) witq hρ 1,
ρ2n2L ⇔ ≪ ⇔ ≪ n ρnL ≪ ≫ nL ≫ nL ≪ n
wqehave log(n+L) 1 L log(n+Lq).
nL ≪ n ⇔ ≫
The precedingpointsindicate thatSPSum’ssuperiorperformanceoverSPSoS is limited to scenarioswhere the
number of layers L is exceptionally large (L ρnlog(n+ L) or L log(n+ L)). However, this requirement is
≫ ≫
unrealisticforthemajorityofreal-worldmulti-layernetworks. GiventhatSPDSoSconsistentlyoutperformsSPSoS,
itfollowsthatSPDSoSalso prevailsoverSPSum in mostscenarios. Additionally,evenin the rare scenarioswhere
SPSum significantly surpasses SPDSoS for a significantly large L, Theorem 2 assures that SPDSoS’s error rate is
negligible. Therefore,amongthesethreemethods,wearriveatthefollowingconclusions: (a)SPDSoSconsistently
outperformsSPSoS;(b)SPSum’ssignificantsuperiorityoverSPDSoSandSPSoSnecessitatesanunreasonablyhigh
numberoflayers,L,whichisimpracticalformostreal-worldmulti-layernetworks.Consequently,wecanconfidently
assertthatSPDSoSandSPSoSvirtuallyalwayssurpassSPSum.
70.4 1 0.4 1
0.35
0.8 0.8
0.3 0.3
0.2 S SP PS Du Sm
oS
00 .. 46 S S SP P PS D Su oSm SoS 0. 02 .5 2 S S SP P PS D Su oSm SoS 00 .. 46 S S SP P PS D Su oSm SoS
0.1 SPSoS 0.15
0.2 0.2
0.1
0 0 0.05 0
0 0.05 0.1 0.15 0.2 0 0.05 0.1 0.15 0.2 0 20 40 60 80 100 0 20 40 60 80 100
L L
(a)Experiment1:Hammingerror (b)Experiment1:Relativeerror (c)Experiment2:Hammingerror (d)Experiment2:Relativeerror
0.5 1 0.4 0.8
0.4 0.8 0.35
0.3 0.6 SPSum 0.3 S SP PS Du Sm oS 0.6 S SP PS Du Sm oS 0.25 SPSum 0.4 S SP PD SoS SoS
0.2 SPSoS 0.4 SPSoS 0.2 SPDSoS
SPSoS
0.1 0.2 0.15 0.2
0.1
0 0 500 1000 1500 2000 0 0 500 1000 1500 2000 0 50 100 150 200 0 0 50 100 150 200
n n n0 n0
(e)Experiment3:Hammingerror (f)Experiment3:Relativeerror (g)Experiment4:Hammingerror (h)Experiment4:Relativeerror
Figure1:Numericalresults.
5. Numericalresultsonsyntheticmulti-layernetworks
Inthissection,weevaluatetheperformanceofourproposedmethodsthroughtheutilizationofcomputer-generated
multi-layernetworks. Forthese simulatednetworks, we possess knowledgeof the ground-truthmixedmembership
matrix Π. To quantifythe performanceof each method, we employ two metrics: the Hamming error and the Rel-
ativeerror. The Hammingerroris definedas Hammingerror = min kΠˆ −Π nPk1, while the relativeerrorisgivenby
R tiae lla lt ai bv ee le pr ero rmr= utam tii on
nP s∈ .S
FkΠˆ ok− rΠΠ keP
F
ak cF h.H pae rr ae m,
S
etere rp sr ee ts tie nn gts inth oe us ret sio mf ual ll atK io- nbyP e- x∈ KS ampe pr lm esu ,t wat eio rn epm oa rt tr ti hc ees a, va ec rc ao gu en oti fn eg af co hr mpo et te rn ic-
foreachproposedapproachacross100independentrepetitions.
Forallsimulationsconductedbelow,wesetK =3andletn bethenumberofpurenodeswithineachcommunity.
0
For mixed nodes, say node i, we formulate its mixed membership vector Π(i,:) in the following manner. Initially,
we generate two randomvalues r(1) = rand(1) and r(2) = rand(1), where rand(1) representsa randomvalue drawn
i i
fromaUniformdistributionovertheinterval[0,1].Subsequently,wedefinethemixedmembershipvectorasΠ(i,:)=
(r i(1) ,r i(2) ,1 r i(1) r i(2) ).Regardingtheconnectivitymatrices,forthel-thmatrixB,weassignB(k,k˜)= B(k˜,k)=rand(1)
2 2 − 2 − 2 l l l
forall1 k k˜ K andl [L]. Finally,thenumberofnodesn,thesparsityparameterρ,thenumberoflayersL,
≤ ≤ ≤ ∈
andthenumberofpurenodesn withineachcommunityareindependentlysetforeachsimulation.
0
Experiment1: Changingρ. Fix(n,L,n )=(500,100,100)andletρrangein 0.02,0.04,...,0.2 .Themulti-layer
0
{ }
networkbecomesdenser as ρ increases. The results shown in panel(a) and panel(b) of Figure 1 demonstratethat
SPDSoS performsslightly better than SPSoS, and both methods significantly outperformSPSum. Meanwhile, the
error rates of SPDSoS and SPSoS decrease rapidly as the sparse parameter ρ increases while SPSum’s error rates
decreaseslowly.
Experiment 2: Changing L. Fix (n,ρ,n ) = (500,0.1,100) and let L range in 10,20,...,100 . More layers
0
{ }
are observed as L increases. The results are presented in panel (c) and panel (d) of Figure 1. It is evident that
SPDSoS and SPSoS exhibitcomparable performance, both significantly surpassing SPSum. Furthermore, as L in-
creases,SPDSoSandSPSoSdemonstrateimprovedperformance,whereasSPSum’sperformanceremainsrelatively
unchangedthroughoutthisexperiment.
Experiment 3: Changingn. Fix (L,ρ) = (40,0.1), let n range in 200,400,...,2000 , and let n = n for each
{ } 0 4
choiceofn. Panel(e)andpanel(f)ofFigure1displaytheresults,whichdemonstratethatSPDSoSandSPSoSenjoy
similarperformancesandthatbothmethodsperformbetterthanSPSum. Meanwhile,wealsoobservethattheerror
ratesofallmethodsdecreaseasnincreaseshere.
Experiment4:Changingn .Fix(n,L,ρ)=(600,50,0.1)andletn rangein 20,40,...,200 .Thenumberofpure
0 0
{ }
8
rorre
gnimmaH
rorre gnimmaH
rorre
evitaleR
rorre evitaleR
rorre
gnimmaH
rorre gnimmaH
rorre
evitaleR
rorre evitaleRnodesincreasesasn grows. TheresultsaredisplayedinthefinaltwopanelsofFigure1. Ourobservationsindicate
0
thatSPDSoSandSPSoSexhibitnotablysuperiorperformancecomparedtoSPSum. Furthermore,bothSPDSoSand
SPSoSexhibitimprovedperformanceinscenarioswithahighernumberofpurenodes,whereasSPSumdemonstrates
weakerperformanceinthisexperiment.
6. Realdataapplications
In this section, we demonstrate the application of our methods to multi-layer networks in the real world. The
application of a mixed membership estimation algorithm to such networks consistently yields an estimated mixed
membershipmatrix, denoted as Πˆ. Notably, Πˆ may vary dependingon the specific algorithm used. Consequently,
accuratelyassessingthequalityoftheestimatedmixedmembershipcommunitypartitionbecomesacrucialproblem.
Toaddressthischallenge,weintroducetwomodularitymetricsinthispaper,designedtoquantitativelyevaluatethe
qualityofmixedmembershipcommunitydetectioninreal-worldmulti-layernetworks.
Recall that A = A represents the summation of all adjacency matrices. This summation effectively
sum l [L] l
quantifies the weight betw∈een nodes. Given that nodes sharing similar community memberships tend to exhibit
strongerconnectionsthanP thosewithdifferentmemberships,A canbeinterpretedastheweightedadjacencymatrix
sum
ofanassortativenetwork(Newman,2002,2003a). Ourfuzzysummodularity,Q ,isdefinedasfollows:
fsum
1 d (i)d (j)
Q = A (i, j) sum sum Πˆ(i,:)Πˆ (j,:),
fsum m sum − m ′
sum i [n]j [n] sum !
X∈ X∈
where fsum stands for “fuzzy sum”, d (i) = A (i, j) for i [n], and m = d (i). Notably,
when L = 1, our fuzzy sum
modularits yum
Q
simj ∈p[n li] fies sum
to the
fuzzy∈
modularity
is nu tm roducedi ∈[ in n] Esu qm
uation (14) of
fsum
(Nepuszetal.,2008).
Furthermore,whenL=1anP dallnodesarepure,ourmodularitymetricP
reducestotheclassical
Newman-Girvanmodularity(Newman&Girvan,2004).
Oursecondmodularitymetricisthefuzzymeanmodularity,definedastheaverageoffuzzymodularitiesacross
alllayers. Here’showitisformulated:
1 1 D(i,i)D(j, j)
Q = A(i, j) l l Πˆ(i,:)Πˆ (j,:),
fmean L m l − m ′
l [L]i [n]j [n] l l !
X∈ X∈ X∈
wherefmeanstandsfor“fuzzymean”andm = D(i,i)representsthesumofdiagonalelementsinthedegree
l i [n] l
matrix D for layer l for l [L]. When all node∈s are pure, our fuzzy mean modularity Q simplifies to the
l fmean
∈ P
multi-normalized average modularity introduced in Equation (3.1) of Paul&Chen (2021). When there is only a
single layer (L = 1), Q reduces to the fuzzy modularity described in (Nepuszetal., 2008). Furthermore, if
fmean
bothconditionshold(L = 1andallnodesarepure), Q degeneratestotheclassicalNewman-Girvanmodularity
fmean
(Newman&Girvan,2004).
Analogous to the Newman-Girvan modularity (Newman&Girvan, 2004), the fuzzy modularity (Nepuszetal.,
2008),andthemulti-normalizedaveragemodularityPaul&Chen(2021),ahighervalueofthefuzzysummodularity
Q indicatesabettercommunitypartition. Consequently,weconsistentlyfavoralargervalueofQ . Similarly,
fsum fsum
alargerQ alsoindicatesabettercommunitypartition. Tothebestofourknowledge,ourfuzzysummodularity
fmean
Q andfuzzymeanmodularityQ arethefirstmetricsdesignedtoevaluatethequalityofmixedmembership
fsum fmean
communitydetectioninmulti-layernetworks.
For real-world multi-layer networks, where the number of communities K is usually unknown, we adopt the
strategy introduced in (Nepuszetal., 2008) to estimate K. Specifically, we determine K by selecting the one that
maximizes the fuzzy sum modularity Q (or the fuzzy mean modularity Q ). This strategy ensures that we
fsum fmean
obtainanoptimalcommunitypartitionbasedonthechosenmodularitymetric.
Inthispaper,weconsiderthefollowingreal-worldmulti-layernetworks,whichcanbeaccessedathttps://manliodedomenico.c
• LazegaLawFirm:Thisdataisamulti-layersocialnetworkwithn=71nodesandL=3layers(Lazega,2001;
Snijdersetal.,2006). Forthisdata,nodesrepresentcompanypartnersandlayersdenotedifferentcooperative
relationships(Co-work,Friendship,andAdvice)amongpartners.
9• C.Elegans: This data is from biology and it collects the connection of Caenorhabditis elegans (Chenetal.,
2006). It has n = 279 nodesand L = 3 layerswith nodesdenotingCaenorhabditiselegansand layers being
differentsynapticjunctions.
• CS-Aarhus: This data is a multi-layer social network with n = 61 nodes and L = 5 layers, where nodes
representemployeesof the Computer Science departmentat Aarhusand layers denotedifferentrelationships
(Facebook,Leisure,Work,Co-authorship,Lunch)(Magnanietal.,2013).
• FAO-trade: This data collects different types of trade relationships among countries from FAO (Food and
AgricultureOrganizationoftheUnitedNations)(DeDomenicoetal.,2015). Ithasn=214nodesandL=364
layers, where nodes represent countries, layers denote products, and edges denote trade relationships among
countries.
Theestimatednumberofcommunitiesandthecorrespondingmodularityofeachmethodfortherealdataanalyzed
in this paperare comprehensivelypresented in Table 1 and Table 2. After analyzingthe results in these tables, we
arriveatthefollowingconclusions:
• Q and Q consistentlydemonstratesimilar valuesfor each method, indicatinga high degreeof agree-
fsum fmean
ment. Specifically,SPSumachievesthe highestmodularityscoreusingbothmetricsacrossmultipledatasets,
includingLazegaLawFirm,CS-Aarhus,andFAO-trade. Similarly,forSPSoS,itsQ andQ scoresfor
fsum fmean
CS-Aarhusranksecondamongallmethods,withscoresthatarecloselyaligned. Thisconsistencyacrossmea-
suressuggeststhatbothQ andQ effectivelycapturesimilaraspectsofcommunitystructure,providing
fsum fmean
reliableandcomparableassessmentsofdifferentmethods.
• While SPSum may not perform as well in numerical studies, it nevertheless exhibits superior performance
in terms of modularity for real-world datasets compared to SPDSoS and SPSoS. The only exception is the
C.Elegansnetwork,whereSPSum’smodularityscoreisslightlylowerthanSPDSoS’s.
• TheCS-Aarhusnetworkexhibitsa moredistinctcommunitystructurecomparedtotheotherthreerealmulti-
layer networks, as evidencedby the highermodularityscoresobtainedby ourmethodsfor CS-Aarhus. Con-
versely,theFAO-tradenetworkpossessestheleastdiscerniblecommunitystructureamongallrealmulti-layer
networks,asthemodularityscoresachievedbyallproposedmethodsforFAO-tradearelowerthanthoseofthe
otherthreenetworks.
• TheresultspresentedinTable1andTable2indicatethattheoptimalnumberofcommunitiesforthefourreal-
worldmulti-layernetworks,namelyLazegaLawFirm,CS-Aarhus,CS-Aarhus,andFAO-trade,are3,2,5,and
2,respectively.ThisdeterminationisbasedonselectingthevalueofK thatyieldsthehighestmodularityscore
acrossallmethodsforeachdataset.
Table1:(EstimatedK,Qfsum)obtainedbytheproposedapproachesfortherealdatausedinthispaper. Theboldfacevaluesrepresentthehighest
Qfsumscoresamongthethreemethods.
Dataset SPSum SPDSoS SPSoS
LazegaLawFirm (3,0.2025) (3,0.1604) (3,0.1993)
C.Elegans (2,0.2778) (2,0.2808) (2,0.2509)
CS-Aarhus (5,0.3575) (4,0.3474) (4,0.3559)
FAO-trade (2,0.1508) (2,0.1329) (2,0.1412)
To simplify ouranalysis and considerthe fact thatSPSum generatesmodularityscoresthat are either higheror
comparable to those of SPDSoS and SPSoS for the four real-world multi-layer networks under consideration, we
focusourfurtheranalysisontheSPSummethod. LetΠˆ representtheestimatedmixedmembershipmatrixobtained
by SPSum for a given real-world multi-layer network. For a node i within the network, we define its estimated
home base community as the one that corresponds to the maximum value in the i-th row of Πˆ, denoted as k˜ =
argmax Πˆ(i,k). Furthermore,we categorizenodesbasedontheirmembershipdistribution: anodeisconsidered
k [K]
∈
10Table2:(EstimatedK,Qfmean)obtainedbytheproposedapproachesfortherealdatausedinthispaper.Theboldfacevaluesrepresentthehighest
Qfmeanscoresamongthethreemethods.
Dataset SPSum SPDSoS SPSoS
LazegaLawFirm (3,0.1990) (3,0.1572) (3,0.1961)
C.Elegans (2,0.2779) (2,0.2780) (2,0.2495)
CS-Aarhus (5,0.3681) (4,0.3404) (4,0.3529)
FAO-trade (2,0.1487) (2,0.1274) (2,0.1403)
highly mixed if max Πˆ(i,k) 0.6, highly pure if max Πˆ(i,k) 0.9, and neutral otherwise. We introduce
k [K] k [K]
two additional metric∈ s, ς an≤ d ς , which represent th∈ e proportio≥ ns of highly mixed and highly pure nodes,
mixed pure
respectively. Additionally,wedefinethebalancedparameterυastheratiooftheminimumtothemaximuml norm
1
o Tf abth lee 3co pl ru em sen ns tso tf heΠˆ v, ai l. ue e., sυ of= themm sai en xk
k
t∈ ∈h[ [K
K
r] ]k ekΠ Πˆ eˆ( (:
:
i, ,k nk)
)
dk k1
1
i. ceA soh bi tg ah ine er dv ba ylu ae po pf lyυ ini gnd Si Pc Sat ue ms a tom tho ere reb aa l-la wn oc re ld dmm uu ll tt ii -- ll aa yy ee rr nn ee tt ww oo rr kk s.
studiedinthispaper. BasedontheresultsinTable3,wedrawthefollowingconclusions:
• Acrossallnetworks,thenumberofhighlypurenodessignificantlyexceedsthenumberofhighlymixednodes,
indicating that most nodes are strongly associated with a single community, while only a few exhibit mixed
membership.
• IntheLazegaLawFirmnetwork,approximately10nodesarehighlymixed,35nodesarehighlypure,and26
are neutral. Meanwhile, this network exhibits the highest proportion of highly mixed nodes among the four
real-worldmulti-layernetworksanalyzed.
• TheC.Elegansnetworkhasthelowestproportionofhighlymixednodesandthehighestproportionofhighly
purenodes.Itsbalancedparameter,υ,is0.9512,thehighestamongallnetworks,indicatingthatthesizesofthe
twoestimatedcommunitiesinC.Elegansarenearlyidentical.
• TheCS-AarhusnetworkexhibitsindicesthatarecomparabletothoseoftheLazegaLawFirmnetwork.
• FAO-tradedisplaysaproportionofhighlymixed(andpure)nodessimilartoC.Elegans. However,itsbalanced
parameteristhelowestamongallnetworks,indicatingthatFAO-tradeexhibitsthemostunbalancedcommunity
structureamongthefourreal-worlddatasets.
Table3:ςmixed,ςpure,andυcomputedfromΠˆ,whereΠˆ isreturnedbyrunningtheSPSumalgorithmtorealmulti-layernetworksusedinthispaper.
Here,weusetheEstimatedKofSPSuminTable1foreachdata.
Dataset ςmixed ςpure υ
LazegaLawFirm 0.1408 0.4930 0.7276
C.Elegans 0.0609 0.6882 0.9512
CS-Aarhus 0.1311 0.4590 0.7006
FAO-trade 0.0701 0.6449 0.5480
Figure2presentsaternarydiagramthatvisualizestheestimatedcommunitymembershipmatrixΠˆ obtainedfrom
theSPSumalgorithmfortheLazegaLawFirmnetworkwhentherearethreecommunities.Inthisdiagram,weobserve
thatnodeiispositionedclosertooneofthetriangle’sverticescomparedtonode jifmax Πˆ(i,k)>max Πˆ(j,k)
k [3] k [3]
∈ ∈
fori [71], j [71]. Apurenodeislocatedatoneofthetriangle’svertices,andaneutralnodeisclosertoavertex
∈ ∈
thanahighlymixednode.Therefore,Figure2indicatesthepurityofeachnodefortheLazegaLawFirmnetwork.
Figures3,4,and5presentthecommunitiesestimatedbyourSPSummethodfortheLazegaLawFirmnetwork,
the C. Elegans network, and the CS-Aarhus network, respectively. In these figures, nodes sharing the same color
representnodesbelongingtothesamehomebasecommunity,whileblacksquaresdenotehighlymixednodes. From
thesethreefigures,wecanclearlyfindthecommunitiesdetectedbySPSumineachlayerforeachmulti-layernetwork.
11Figure2: Ternarydiagramofthe71 3estimatedmembershipmatrixΠˆ forLazegaLawFirm. Eachdotrepresentsacompanypartner,andits
×
locationwithinthetrianglecorrespondstoitsmembershipscores.
(a) (b) (c)
Figure3:Illustrationofthe3layersofLazegaLawFirm.Colorsindicatehomebasedcommunitiesandtheblacksquarerepresentshighlymixed
nodesdetectedbySPSumwith3communities.
(a) (b) (c)
Figure4: Illustrationofthe3layersofC.Elegans. Colorsindicatehomebasedcommunitiesandtheblacksquarerepresentshighlymixednodes
detectedbySPSumwith2communities.
12(a) (b) (c) (d) (e)
Figure5:Illustrationofthe5layersofCS-Aarhus. Colorsindicatehomebasedcommunitiesandtheblacksquarerepresentshighlymixednodes
detectedbySPSumwith5communities.
7. Conclusion
Thispaperconsiderstheproblemofestimatingcommunitymembershipsofnodesinmulti-layernetworksunder
themulti-layermixedmembershipstochasticblockmodel,amodelthatpermitsnodestobelongtomultiplecommu-
nities simultaneously. We have developedspectral methodsleveragingthe eigen-decompositionof some aggregate
matrices,andhaveprovidedtheoreticalguaranteesfortheirconsistency,demonstratingtheconvergenceofper-node
errorratesasthenumberofnodesand/orlayersincreasesunderMLMMSB.Tothebestofourknowledge,thisisthe
firstworktoestimatethemixedcommunitymembershipsformulti-layernetworksbyusingtheseaggregatematrices.
Ourtheoreticalanalysisrevealsthatthealgorithmdesignedbasedonthedebiasedsumofsquaredadjacencymatrices
alwaysoutperformsthealgorithmusingthesumofsquaredadjacencymatrices,whiletheygenerallyoutperformthe
methodusingthesumofadjacencymatricesinthetaskofestimatingmixedmembershipsformulti-layernetworks.
Sucharesultisnewformixedmembershipestimationinmulti-layernetworkstothebestofourknowledge.Extensive
simulatedstudiessupportourtheoreticalfindings,validatingtheefficiencyofourmethodusingthedebiasedsumof
adjacencymatrices.Additionally,theproposedfuzzymodularitymeasuresofferanovelperspectiveforevaluatingthe
qualityofmixedmembershipcommunitydetectioninmulti-layernetworks.
Forfutureresearch,first,developingmethodswiththeoreticalguaranteesforestimatingthenumberofcommuni-
tiesinMLMMSBremainsachallengingandmeaningfultask. Second,acceleratingourmethodsfordetectingmixed
membershipsinlarge-scalemulti-layernetworksiscrucialforpracticalapplications. Third,exploringmoreefficient
algorithms for estimating mixed memberships would further enrich our understanding of community structures in
multi-layernetworks. Finally,extendingourframeworktodirectedmulti-layernetworkswouldbroadenthescopeof
ourworkandenabletheanalysisofevenmorecomplexsystems.
CRediTauthorshipcontributionstatement
HuanQing: Conceptualization;Datacuration;Formalanalysis;Fundingacquisition;Methodology;Projectad-
ministration;Resources;Software;Validation;Visualization;Writing-originaldraft;Writing-review&editing.
Declarationofcompetinginterest
Theauthordeclaresnocompetinginterests.
Dataavailability
Dataandcodewillbemadeavailableonrequest.
Acknowledgements
H.Q.wassponsoredbyNaturalScienceFoundationofChongqing,China(Grant:CSTB2023NSCQ-LZX0048).
13Appendix A. Proofs
Appendix A.1. ProofofLemma1
Proof. SinceΩ =ρΠ( B)Π =UΣU andU U = I ,wehave
sum l [L] l ′ ′ ′ K K
∈ ×
ρΠ( B)ΠP =UΣU Π(ρ B)ΠU =UΣU U =UΣ U =Π(ρ B)ΠUΣ 1.
l ′ ′ l ′ ′ l ′ −
⇒ ⇒
l [L] l [L] l [L]
X∈ X∈ X∈
SinceΠ( ,:)= I ,wehaveU( ,:)=(Π(ρ B)ΠUΣ 1)( ,:)=Π( ,:)(ρ B)ΠUΣ 1 =(ρ B)ΠUΣ 1,
i.e.,U(
I ,:)=(ρK ×K
B)ΠUΣ
I
1.
Therefore,Ul ∈[ =L] Πl U(′ ,:)− holdI s.SimilaI rly,wehl a∈v[L e] Vl =Π′ V(−
,:).
l ∈[L] l ′ −
I l ∈[L] l ′ − P I P I P
Appendix A.2. PProofofTheorem1
Proof. Thefollowinglemmabounds A Ω .
sum sum
k − k∞
Lemma2. UnderMLMMSB(Π,ρ, ),whenAssumption1holds,withprobabilityatleast1 o( 1 ),wehave
B − n+L
A Ω =O( ρnLlog(n+L)).
sum sum
k − k∞
Proof. Recallthat A Ω =max A (i, jp) Ω (i, j) =max (A(i, j) Ω(i, j)),
nextwebounditbyk us su im ng− thesu Bm k e∞ rnsteinini ∈ e[ qn] ualij ∈ty[n] b| els ou wm
.
− sum | i ∈[n] j ∈[n]| l ∈[L] l − l |
P P P
Theorem4. (Theorem1.4ofTropp(2012))Let X beindependent,random,self-adjointmatriceswithdimensiond.
i
WhenE[X]=0andand X Ralmostsurely.{ Th} en,forallt 0,
i i
k k≤ ≥
t2/2
P( X t) d exp( − ),
k i k≥ ≤ · σ2+Rt/3
i
X
whereσ2 := E[X2] and denotesspectralnorm.
k i i k k·k
i
[L ne ],t wx eb he aa vnP ey
:
n ×1 vector. Set y
(ij)
=
l
∈[L](A l(i, j) −Ω l(i, j)) for i
∈
[n], j
∈
[n], and T
(i)
=
j
∈[n]y (ij)x(j) for
∈ P P
• E(y x(j))=0fori [n], j [n].
(ij)
∈ ∈
• y x(j) τmax x(j) =τ x fori [n], j [n].
(ij) j
| |≤ | | k k∞ ∈ ∈
• LetVar(X)denotethevarianceofanyrandomvariableX. CombineA 0,1 n n forl [L]withthefactthat
l ×
A(i,m)andA(m, j)areindependentwhen j,i,wehave ∈ { } ∈
l l
E[y2 x2(j)]= x2(j)E[y2 ]= x2(j)Var(y )= x2(j) Var(A(i, j) Ω(i, j))
(ij) (ij) (ij) l − l
j [n] j [n] j [n] j [n] l [L]
X∈ X∈ X∈ X∈ X∈
= x2(j) Var(A(i, j))= x2(j) Ω(i, j)(1 Ω(i, j)) x2(j) Ω(i, j)
l l l l
− ≤
j [n] l [L] j [n] l [L] j [n] l [L]
X∈ X∈ X∈ X∈ X∈ X∈
x2(j) ρ=ρL x 2
≤ k kF
j [n] l [L]
X∈ X∈
ByTheorem4,foranyt 0,wehave
≥
t2
P(T t) exp( − ).
| (i) |≥ ≤ ρL kx k2
F
+ τ kx 3k∞t
Sett= α+1+√(α+1)(α+19) ρL x 2log(n+L)foranyα 0,ifρL x 2 τ2 x 2 log(n+L),wehave
3 k kF ≥ k kF ≥ k k∞
q
1 1
P(T t) exp( (α+1)log(n+L) ) .
| (i) |≥ ≤ − ≤ (n+L)α+1
18 + 2√α+1 τ2 x2log(n+L)
(√α+1+√α+19)2 √α+1+√α+19 k ρk∞L x2
r k kF
14Setx 1,1 n 1,wehave:whenρnL τ2log(n+L),withprobabilityatleast1 o( 1 )foranyα 0,
∈{− } × ≥ − (n+L)α+1 ≥
α+1+ √(α+1)(α+19)
T ρnLlog(n+L).
(i)
≤ 3
p
Setα=1,whenρnL τ2log(n+L),withprobabilityatleast1 o( 1 ),wehave
≥ − n+L
A Ω =max T =O( ρnLlog(n+L)).
sum sum i [n] (i)
k − k∞ ∈
p
ByTheorem4.2of(Capeetal.,2019),when λ (Ω ) 4 A Ω ,thereisanorthogonalmatrix such
K sum sum sum
| |≥ k − k∞ O
that
A Ω U
kUˆ −U Ok2 →∞ ≤14k sum − λ K(su Ωm sk u∞ m)k k2 →∞.
| |
Since̟:= UˆUˆ UU 2 Uˆ U ,weget
′ ′ 2 2
k − k →∞ ≤ k − Ok →∞
A Ω U
̟ 28k sum − sum k∞k k2 →∞.
≤ λ (Ω )
K sum
| |
Since U =O( K)=O( 1)byLemma3.1of(Maoetal.,2021)andCondition1,weget
k k2 →∞ n n
q q
A Ω
̟=O(k sum − sum k∞).
λ (Ω ) √n
K sum
| |
For λ (Ω ),byCondition1andAssumption4,wehave
K sum
| |
n
λ (Ω ) = λ (ρΠ( B)Π) =ρλ (Π( B)Π) =ρλ (ΠΠ B) ρλ (ΠΠ)λ ( B) =O(ρ L)=O(ρnL),
| K sum | | K l ′ | | K l ′ | | K ′ l |≥ K ′ | K l | K
l [L] l [L] l [L] l [L]
X∈ X∈ X∈ X∈
whichgivesthat
A Ω
̟=O(k sum − sum k∞).
ρn1.5L
BytheproofofTheorem3.2of(Maoetal.,2021),thereisaK-by-Kpermutationmatrix suchthat,
P
n A Ω
max
i ∈[n]
ke ′i(Πˆ −Π P)
k1
=O(̟κ(Π ′Π) λ 1(Π ′Π))=O(̟ K)=O(̟√n)=O(k sum ρ− nLsum k∞).
r
p
By Lemma 2, this theorem holds. Finally, recall that when we use Theorem 4.2 of (Capeetal., 2019), we require
λ (Ω ) 4 A Ω . Since λ (Ω ) = O(ρnL),itiseasytoseethataslongasρnL A Ω ,
K sum sum sum K sum sum sum
t| hisrequir| e≥ menk thold− s. Thek c∞ ondition| ρnL | A Ω holdsnaturally becausewe need≫ thek row-− wise erk r∞ or
sum sum
boundO(kAsum ρ−nΩ Lsumk∞)tobemuchsmallertha≫ n1k . − k∞
Appendix A.3. AnalternativeproofofTheorem1
Here,weprovideanalternativeproofofTheorem1byusingTheorem4.2.ofChenetal.(2021).
Proof. Set H = Uˆ U and let H = U Σ V be its top K singular value decomposition. Define sgn(H ) as
Uˆ ′ Uˆ H Uˆ H Uˆ H′ Uˆ Uˆ
U V . UnderMLMMSB(Π,ρ, ),thefollowingresultsaretrue.
H Uˆ H′ Uˆ B
• E[ (A(i, j) Ω(i, j))]=0fori [n], j [n].
l ∈[L] l − l ∈ ∈
P
15• E[( (A(i, j) Ω(i, j)))2] = E[ (A(i, j) Ω(i, j))2 + (A (i, j) Ω (i, j))(A (i, j)
Ω
(i,l ∈j[ )L )]
]
=l −E[l
(A(i, j) Ω(i,
j)l ∈)[ 2L ]] =l − Ω(l
i, j)(1
Ω(l1 i, ,l j2 ),l )1∈[L],l2∈[L] Ωl1
(i,
j)− l1
ρ
=l2
ρL
fo−
r
i
l2P
[n], j
[n].l ∈[L] l − l P l ∈[L] l − Pl ≤ l ∈[L] l ≤ l ∈[L]
∈ ∈ P P P P
• A (i, j) Ω (i, j) = (A(i, j) Ω(i, j)) τfori [n], j [n].
| sum − sum | | l ∈[L] l − l |≤ ∈ ∈
• S µe =tµ O= (1)n k bU yKk2 2 C→∞ o. ndB iy tioL ne 1m .mP a3.1of(Maoetal.,2021),wehave Kλ1(1 Π ′Π) ≤ kU k2 2
→∞
≤ λK(Π1 ′Π),whichgivesthat
• Setc = τ . µ=O(1)givesc =O( τ2log(n)) O(1)byAssumption1.
b √ρLn/(µlog(n)) b ρnL ≤
q
TheaboveresultsensurethatconditionsinAssumption4.1. Chenetal.(2021)aresatisfied. Then,byTheorem4.2.
Chenetal.(2021),when λ (Ω ) ρnLlog(n),withprobabilityatleast1 O(1),wehave
| K sum |≫ − n5
p
κ(Ω )√ρLµK+ KρLlog(n)
Uˆsgn(H ) U =O( sum ).
k Uˆ − k2 →∞ |λ K(Ω sump)
|
Sinceµ=O(1)andK =O(1),wehave
κ(Ω )√ρL+ ρLlog(n)
Uˆsgn(H ) U =O( sum ).
k Uˆ − k2 →∞ |λ K(Ω sump)
|
By Assumption 2 and Condition 1, we have λ (Ω ) = O(ρnL) and λ (Ω ) = Ω = ρ Π( B)Π
ρ ΠΠ B
=O(ρnL)=O(ρnL),which| gK ivess tu hm at|
κ(Ω
)=O(1)| an1
d
sum | k sum k k l ∈[L] l ′ k ≤
k ′ kk l ∈[L] l k K sum P
P
(√ρL+ ρLlog(n) 1 log(n)
Uˆsgn(H ) U =O( )=O( ).
k Uˆ − k2 →∞ ρ pnL ns ρL
Since̟= UˆUˆ UU 2 U Uˆsgn(H ) ,wehave
k
′
−
′ k2
→∞ ≤ k −
Uˆ k2
→∞
1 log(n)
̟=O( ).
ns ρL
BytheproofofTheorem3.2in(Maoetal.,2021),thereisapermutationmatrix suchthat,
P
n log(n)
max e (Πˆ Π ) =O(̟κ(ΠΠ) λ (ΠΠ))=O(̟ )=O(̟√n)=O( ).
i ∈[n] k ′i − P k1 ′ 1 ′ K s ρnL
r
p
Recall that λ (Ω ) O(ρnL) and we need λ (Ω ) ρnLlog(n) hold. It is easy to see that as long as
K sum K sum
| | ≥ | | ≫
O(ρnL) ρnLlog(n) ρnL log(n), λ (Ω ) ρnLlog(n) always holds. The conditionρnL log(n)
≫ ⇔ ≫ | K sum | ≫ p ≫
holdsnaturapllysincewerequiretherow-wiseerrorboundOp( log(n))tobemuchsmallerthan1.
ρnL
q
Appendix A.4. ProofofTheorem2
Proof. SinceE[S ] , S˜ , i.e., E[S (i, j) S˜ (i, j)] , 0fori [n], j [n], wecannotuseTheorem4.2in
sum sum sum sum
(Chenetal.,2021)toobtaintherow-wiseeigens− paceerror VˆVˆ VV∈ fo∈ rS andS˜ . ToobtainSPDSoS’s
′ ′ 2 sum sum
k − k →∞
error rate, we use Theorem 4.2 in(Capeetal., 2019). By Bernstein inequality, we have the following lemma that
bounds S S˜ .
sum sum
k − k∞
Lemma3. UnderMLMMSB(Π,ρ, ),whenAssumption3holds,withprobabilityatleast1 o( 1 ),wehave
B − n+L
S S˜ =O( ρ2n2Llog(n+L))+O(ρ2nL).
sum sum
k − k∞
q
16Proof. SinceA 0,1 n n,wehaveA2(i, j)= A(i, j)fori [n], j [n],l [L],whichgivesthat
l ∈{ } × l l ∈ ∈ ∈
S S˜ =max S (i, j) S˜ (i, j) =max (A2 D Ω2)(i, j)
k sum − sum k∞ i ∈[n] | sum − sum | i ∈[n] | l − l − l |
j [n] j [n] l [L]
X∈ X∈ X∈
=max (A(i,m)A(m, j) Ω(i,m)Ω(m, j)) D(i, j)
i [n] l l l l l
∈ | − − |
j [n] l [n]m [n] l [L]
X∈ X∈ X∈ X∈
=max ( (A(i,m)A(m, j) Ω(i,m)Ω(m, j)) + (A2(i,m) Ω2(i,m)) D(i,i))
i ∈[n] | l l − l l | | l − l − l |
j,i,j [n] l [L]m [n] l [L]m [n] l [L]
X∈ X∈ X∈ X∈ X∈ X∈
=max ( (A(i,m)A(m, j) Ω(i,m)Ω(m, j)) + (A(i,m) Ω2(i,m)) D(i,i))
i ∈[n] | l l − l l | | l − l − l |
j,i,j [n] l [L]m [n] l [L]m [n] l [L]
X∈ X∈ X∈ X∈ X∈ X∈
=max ( (A(i,m)A(m, j) Ω(i,m)Ω(m, j)) + Ω2(i,m))
i ∈[n] | l l − l l | l
j,i,j [n] l [L]m [n] l [L]m [n]
X∈ X∈ X∈ X∈ X∈
max ( (A(i,m)A(m, j) Ω(i,m)Ω(m, j)) + ρ2
i [n] l l l l
≤ ∈ | − |
j,i,j [n] l [L]m [n] l [L]m [n]
X∈ X∈ X∈ X∈ X∈
=max ( (A(i,m)A(m, j) Ω(i,m)Ω(m, j)) +ρ2nL.
i [n] l l l l
∈ | − |
j,i,j [n] l [L]m [n]
X∈ X∈ X∈
Next,webound (A(i,m)A(m, j) Ω(i,m)Ω(m, j)) fori [n].Letx˜beany(n 1) 1vector.
Sety˜ = j,i,j ∈[n (] A| (i,l ∈m[L )] A(m m∈,[n j] ) l Ω(i,ml )Ω(m,− j))l fori l [n], j ,| i, j ∈ [n]andT˜ = − y˜× x˜(j) for
i
[n( ]i .j) Thefl o∈l[ lL o] P wim n∈g[n r] esul P ltsholP dl
:
− l l ∈ ∈ (i) j,i,j ∈[n] (ij)
∈ P P P
• E(y˜ x˜(j))=0sinceA(i,m)andA(m, j)areindependentwhen j,ifori [n], j [n].
(ij) l l
∈ ∈
• y˜ x˜(j) τ˜max x˜(j) =τ˜ x˜ fori [n], j [n].
(ij) j
| |≤ | | k k∞ ∈ ∈
• CombineA 0,1 n nforl [L]withthefactthatA(i,m)andA(m, j)areindependentwhen j,i,wehave
l × l l
∈{ } ∈
E[y˜2 x˜2(j)]= x˜2(j)E[y˜2 ]= x˜2(j)Var(y˜ )
(ij) (ij) (ij)
j,i,j [n] j,i,j [n] j,i,j [n]
X∈ X∈ X∈
= x˜2(j) Var(A(i,m)A(m, j) Ω(i,m)Ω(m, j))= x˜2(j) Var(A(i,m)A(m, j))
l l l l l l
−
j,i,j [n] l [L]m [n] j,i,j [n] l [L]m [n]
X∈ X∈ X∈ X∈ X∈ X∈
= x˜2(j) E[(A(i,m)A(m, j) Ω(i,m)Ω(m, j))2]
l l l l
−
j,i,j [n] l [L]m [n]
X∈ X∈ X∈
= x˜2(j) E[A2(i,m)A2(m, j)+Ω2(i,m)Ω2(m, j) 2A(i,m)A(m, j)Ω(i,m)Ω(m, j)]
l l l l − l l l l
j,i,j [n] l [L]m [n]
X∈ X∈ X∈
= x˜2(j) (E[A2(i,m)A2(m, j)] Ω2(i,m)Ω2(m, j))
l l − l l
j,i,j [n] l [L]m [n]
X∈ X∈ X∈
= x˜2(j) (E[A(i,m)]E[A(m, j)] Ω2(i,m)Ω2(m, j))
l l − l l
j,i,j [n] l [L]m [n]
X∈ X∈ X∈
= x˜2(j) Ω(i,m)Ω(m, j)(1 Ω(i,m)Ω(m, j))
l l l l
−
j,i,j [n] l [L]m [n]
X∈ X∈ X∈
x˜2(j) Ω(i,m)Ω(m, j) x˜2(j) ρ2 =ρ2nL x˜ 2.
≤ l l ≤ k kF
j,i,j [n] l [L]m [n] j,i,j [n] l [L]m [n]
X∈ X∈ X∈ X∈ X∈ X∈
ByTheorem4,foranyt˜ 0,wehave
≥
t˜2
P(T˜ t˜) exp( − ).
| (i) |≥ ≤ ρ2nL kx˜ k2
F
+ τ˜ kx˜ 3k∞t˜
17Sett˜= α+1+√(α+1)(α+19) ρ2nL x˜ 2log(n+L)foranyα 0. Ifρ2nL x˜ 2 τ˜2 x˜ 2log(n+L),wehave
3 k kF ≥ k kF ≥ k k∞
q 1 1
P(T˜ t˜) exp( (α+1)log(n+L) ) .
| (i) |≥ ≤ − ≤ (n+L)α+1
18 + 2√α+1 τ˜2 x˜2log(n+L)
(√α+1+√α+19)2 √α+1+√α+19 k ρk 2∞nL x˜ 2
r k kF
Recallthatx˜isany(n 1) 1vector,settingx˜ 1,1 (n 1) 1givesthefollowingresult:whenρ2n2L ρ2n(n 1)L
− ×
− × ∈{− } ≥ − ≥
τ˜2log(n+L),withprobabilityatleast1 o( 1 )foranyα 0,wehave
− (n+L)α+1 ≥
α+1+ √(α+1)(α+19)
T˜ ρ2n(n 1)Llog(n+L).
(i)
≤ 3 −
q
Setα=1,whenρ2n2L τ˜2log(n+L),withprobabilityatleast1 o( 1 ),wehave
≥ − n+L
max T˜ =O( ρ2n2Llog(n+L)).
i [n] (i)
∈
q
Hence,wehave
S S˜ =O( ρ2n2Llog(n+L))+O(ρ2nL).
sum sum
k − k∞
q
ByTheorem4.2of(Capeetal.,2019),if λ (S˜ ) 4 S S˜ ,thereisanorthogonalmatrix ˜ suchthat
K sum sum sum
| |≥ k − k∞ O
S S˜ V
kVˆ −V O˜ k2
→∞
≤14k sum −
λ
K(su S˜m sk um∞ )k k2 →∞.
| |
Since̟˜ := VˆVˆ VV 2 Vˆ V ˜ bybasicalgebra,wehave
′ ′ 2 2
k − k →∞ ≤ k − Ok →∞
S S˜ V
̟˜ 28k sum − sum k∞k k2 →∞.
≤ λ (S˜ )
K sum
| |
Since V =O( 1)byLemma3.1(Maoetal.,2021)andCondition1,wehave
k k2 →∞ n
q S S˜
̟˜ =O(k sum − sum k∞).
λ (S˜ ) √n
K sum
| |
For λ (S˜ ),byCondition1andAssumption4,wehave
K sum
| |
λ (S˜ ) = λ (( Ω2)2)= λ (( ρ2ΠBΠΠBΠ )2)=ρ2 λ2( ΠBΠΠBΠ )
| K sum | K l K l ′ l ′ K l ′ l ′
s l [L] s l [L] s l [L]
X∈ X∈ X∈
=ρ2 λ2(Π( BΠ ΠB)Π)=ρ2 λ2(ΠΠ( BΠΠB)) ρ2λ (ΠΠ) λ2( BΠΠB)
K l ′ l ′ K ′ l ′ l ≥ K ′ K l ′ l
s l [L] s l [L] s l [L]
X∈ X∈ X∈
=O(ρ2λ2(ΠΠ)λ ( B2))=O(ρ2n2L),
K ′ | K l |
l [L]
X∈
whichgivesthat
S S˜
̟˜ =O(k sum − sum k∞).
ρ2n2.5L
ProofofTheorem3.2in(Maoetal.,2021)givesthat,thereisaK K permutationmatrix ˜ suchthat,
× P
n S S˜
max
i ∈[n]
ke ′i(Πˆ −Π P˜)
k1
=O(̟˜κ(Π ′Π) λ 1(Π ′Π))=O(̟˜ K)=O(̟˜ √n)=O(k sum ρ2− n2Lsum k∞).
r
ByLemma3,thistheoremholds.Finally,sincep λ (S˜ ) =O(ρ2n2L),therequirement λ (S˜ ) 4 S S˜
K sum K sum sum sum
is satisfied as long as ρ2n2L S S˜ | which| holds naturally since we nee| d the row|≥ -wik se erro− r bouk n∞ d
sum sum
O(kSsum ρ2− nS˜ 2s Lumk∞)tobemuchsma≫ llerk than1− . k∞
18Appendix A.5. ProofofTheorem3
Proof. Lemma4bounds A2 Ω2 .
k l ∈[L] l − l ∈[L] lk∞
Lemma4. UnderMLMMPSB(Π,ρ, )P,ifρnL log(n+L),withprobabilityatleast1 o( 1 ),wehave
B ≥ − n+L
A2 Ω2 = S S˜ +O(ρnL).
k l − lk∞ k sum − sum k∞
l [L] l [L]
X∈ X∈
Proof. Since A2 Ω2 =S S˜ + D andeachD isadiagonalmatrixforl [L],wehave
l ∈[L] l − l ∈[L] l sum − sum l ∈[L] l l ∈
k
A2 lP
−
Ω2 lk∞P= kS sum −S˜ sum+ D l kP
∞
=max i ∈[n] |S sum(i, j) −S˜ sum(i, j)+ D l(i, j)
|
l [L] l [L] l [L] j [n] l [L]
X∈ X∈ X∈ X∈ X∈
max S (i, j) S˜ (i, j) +max D(i, j)
i [n] sum sum i [n] l
≤ ∈ | − | ∈ | |
j [n] j [n] l [L]
X∈ X∈ X∈
= S S˜ +max D(i, j)= S S˜ +max D(i,i).
sum sum i [n] l sum sum i [n] l
k − k∞ ∈ k − k∞ ∈
l [L]j [n] l [L]
X∈ X∈ X∈
SinceLemma3providesanupperboundof S S˜ ,weonlyneedtoboundmax D(i,i). Tobound
it,weletW = D(i,i)
Ωk (s iu ,m j)−
=
sum k∞
(A(i, j) Ω(i,
j))forii ∈[n [] n],l T∈[ hL] efol
llowingresults
hold:
(i) l ∈[L] l − l ∈[L] j ∈[n] l l ∈[L] j ∈[n] l − l ∈ P
P P P P P
• E[(A(i, j) Ω(i, j))]=0forl [L],i [n], j [n].
l l
− ∈ ∈ ∈
• A(i, j) Ω(i, j) 1forl [L],i [n], j [n].
l l
| − |≤ ∈ ∈ ∈
• SinceE[A(i, j)]=Ω(i, j)andVar(A(i, j))=Ω(i, j)(1 Ω(i, j))forBernoullidistribution,wehave
l l l l l
−
E[(A(i, j) Ω(i, j))2]= Var(A(i, j))= Ω(i, j)(1 Ω(i, j)) Ω(i, j) ρ=ρnL.
l l l l l l
− − ≤ ≤
l [L]j [n] l [L]j [n] l [L]j [n] l [L]j [n] l [L]j [n]
X∈ X∈ X∈ X∈ X∈ X∈ X∈ X∈ X∈ X∈
ByTheorem4,foranyt˜˜ 0,wehave
≥
t˜˜2
P(W t˜˜) exp( − ).
| (i) |≥ ≤ ρnL+ t˜˜
3
Sett˜˜= α+1+√(α+1)(α+19) ρnLlog(n+L)foranyα 0,ifρnL log(n+L),wehave
3 ≥ ≥
P(W t˜˜p ) exp( (α+1)log(n+L) 1 ) 1 .
| (i) |≥ ≤ − 18 + 2√α+1 log(n+L) ≤ (n+L)α+1
(√α+1+√α+19)2 √α+1+√α+19 ρnL
q
Hence,whenρnL log(n+L),withprobabilityatleast1 o( 1 ),wehave
≥ − (n+L)α+1
W = D(i,i) Ω(i, j) t˜˜.
(i) l l
| | | − |≤
l [L] l [L]j [n]
X∈ X∈ X∈
Letα=1,whenρnL log(n+L),withprobabilityatleast1 o( 1 ),wehave
≥ − n+L
2+2√10
max W ρnLlog(n+L).
i [n] (i)
∈ | |≤ 3
D(i,i) Ω(i, j) 2+2√10 ρnLlog(n+L)fp ori [n]gives D(i,i) Ω(i, j)+
| l ∈[L] l − l ∈[L] j ∈[n] l |≤ 3 ∈ l ∈[L] l ≤ l ∈[L] j ∈[n] l
2+2√10 ρnLlog(n+L) ρnL + 2+2√10 ρnLlog(n+L) = O(ρnL) since we require ρnL log(n + L) to hold.
P3 P P≤ 3 p P ≥P P
Therefore,wehavemax D(i,i)=O(ρnL)andthislemmaholds.
p i ∈[n] l ∈[L] l p
FollowasimilarproofasPTheorem2,fortheSPSoSmethod,wehave
A2 Ω2
max
i ∈[n]
ke ′i(Πˆ −Π P)
k1
=O(k l ∈[L] l ρ2− n2Ll ∈[L] lk∞).
P P
ByLemma4andthefactthatρ 1,thistheoremholds.
≤
19References
Airoldi, E.M.,Blei, D.M.,Fienberg, S.E.,&Xing,E.P.(2008). Mixedmembershipstochasticblockmodels. JournalofMachineLearning
Research,9,1981–2014.
Anandkumar,A.,Ge,R.,Hsu,D.,&Kakade,S.M.(2014). Atensorapproachtolearningmixedmembershipcommunitymodels. TheJournalof
MachineLearningResearch,15,2239–2312.
Arau´jo,M.C.U.,Saldanha,T.C.B.,Galvao,R.K.H.,Yoneyama,T.,Chame,H.C.,&Visani,V.(2001). Thesuccessiveprojectionsalgorithm
forvariableselectioninspectroscopicmulticomponentanalysis. Chemometricsandintelligentlaboratorysystems,57,65–73.
Bakken,T.E.,Miller,J.A.,Ding,S.-L.,Sunkin,S.M.,Smith,K.A.,Ng,L.,Szafer,A.,Dalley,R.A.,Royall,J.J.,Lemon,T.etal.(2016). A
comprehensivetranscriptionalmapofprimatebraindevelopment. Nature,535,367–375.
Boccaletti,S.,Bianconi,G.,Criado,R.,DelGenio,C.I.,Go´mez-Gardenes,J.,Romance,M.,Sendina-Nadal,I.,Wang,Z.,&Zanin,M.(2014).
Thestructureanddynamicsofmultilayernetworks. Physicsreports,544,1–122.
Cape, J., Tang, M., &Priebe, C.E.(2019). Thetwo-to-infinity normand singular subspace geometry with applications tohigh-dimensional
statistics. AnnalsofStatistics,47,2405–2439.
Chen,B.L.,Hall,D.H.,&Chklovskii,D.B.(2006). Wiringoptimizationcanrelateneuronalstructureandfunction. ProceedingsoftheNational
AcademyofSciences,103,4723–4728.
Chen,Y.,Chi,Y.,Fan,J.,&Ma,C.(2021). Spectralmethodsfordatascience: Astatisticalperspective. FoundationsandTrends®inMachine
Learning,14,566–806.
Chen,Y.,&Mo,D.(2022). Communitydetectionformultilayerweightednetworks. InformationSciences,595,119–141.
DeDomenico,M.,Nicosia,V.,Arenas,A.,&Latora,V.(2015). Structuralreducibilityofmultilayernetworks. Naturecommunications,6,6864.
Fortunato,S.(2010). Communitydetectioningraphs. Physicsreports,486,75–174.
Fortunato,S.,&Hric,D.(2016).Communitydetectioninnetworks:Auserguide. Physicsreports,659,1–44.
Gillis, N.,&Vavasis,S.A.(2013). Fastandrobustrecursivealgorithmsforseparable nonnegative matrixfactorization. IEEEtransactions on
patternanalysisandmachineintelligence,36,698–714.
Gillis,N.,&Vavasis,S.A.(2015).Semidefiniteprogrammingbasedpreconditioningformorerobustnear-separablenonnegativematrixfactoriza-
tion.SIAMJournalonOptimization,25,677–698.
Gopalan,P.,&Blei,D.(2013). Efficientdiscoveryofoverlappingcommunitiesinmassivenetworks. ProceedingsoftheNationalAcademyof
SciencesoftheUnitedStatesofAmerica,110,14534–14539.
Han,Q.,Xu,K.,&Airoldi,E.(2015). Consistentestimationofdynamicandmulti-layerblockmodels. InInternationalConferenceonMachine
Learning(pp.1511–1520). PMLR.
Holland,P.W.,Laskey,K.B.,&Leinhardt,S.(1983). Stochasticblockmodels:Firststeps. Socialnetworks,5,109–137.
Huang,X.,Chen,D.,Ren,T.,&Wang,D.(2021).Asurveyofcommunitydetectionmethodsinmultilayernetworks.DataMiningandKnowledge
Discovery,35,1–45.
Javed,M.A.,Younis,M.S.,Latif,S.,Qadir,J.,&Baig,A.(2018). Communitydetectioninnetworks: Amultidisciplinaryreview. Journalof
NetworkandComputerApplications,108,87–111.
Jin,J.,Ke,Z.T.,&Luo,S.(2024).Mixedmembershipestimationforsocialnetworks. JournalofEconometrics,239,105369.
Jing,B.-Y.,Li,T.,Lyu,Z.,&Xia,D.(2021). Communitydetectiononmixturemultilayernetworksviaregularizedtensordecomposition. The
AnnalsofStatistics,49,3181–3205.
Ke,Z.T.,&Wang,M.(2024).Usingsvdfortopicmodeling. JournaloftheAmericanStatisticalAssociation,119,434–449.
Kim,J.,&Lee,J.-G.(2015). Communitydetectioninmulti-layergraphs:Asurvey.ACMSIGMODRecord,44,37–48.
Kivela¨,M.,Arenas,A.,Barthelemy,M.,Gleeson,J.P.,Moreno,Y.,&Porter,M.A.(2014). Multilayernetworks. Journalofcomplexnetworks,2,
203–271.
Lazega, E.(2001). Thecollegial phenomenon: Thesocial mechanisms ofcooperation amongpeers inacorporate law partnership. Oxford
UniversityPress,USA.
Lei,J.,Chen,K.,&Lynch,B.(2020). Consistentcommunitydetectioninmulti-layernetworkdata. Biometrika,107,61–73.
Lei,J.,&Lin,K.Z.(2023).Bias-adjustedspectralclusteringinmulti-layerstochasticblockmodels.JournaloftheAmericanStatisticalAssociation,
118,2433–2445.
Magnani,M.,Micenkova,B.,&Rossi,L.(2013).Combinatorialanalysisofmultiplenetworks. arXivpreprintarXiv:1303.4986,.
Mao, X., Sarkar, P., &Chakrabarti, D. (2021). Estimating mixed memberships with sharp eigenvector deviations. Journal ofthe American
StatisticalAssociation,116,1928–1940.
Mucha,P.J.,Richardson,T.,Macon,K.,Porter,M.A.,&Onnela,J.-P.(2010). Communitystructureintime-dependent,multiscale,andmultiplex
networks. science,328,876–878.
Narayanan,M.,Vetta,A.,Schadt,E.E.,&Zhu,J.(2010). Simultaneousclusteringofmultiplegeneexpressionandphysicalinteractiondatasets.
PLoScomputationalbiology,6,e1000742.
Nepusz,T.,Petro´czi, A.,Ne´gyessy,L.,&Bazso´,F.(2008). Fuzzycommunitiesandtheconceptofbridgenessincomplexnetworks. Physical
ReviewE,77,016107.
Newman,M.E.(2002). Assortativemixinginnetworks. PhysicalReviewLetters,89,208701.
Newman,M.E.(2003a). Mixingpatternsinnetworks. PhysicalReviewE,67,026126.
Newman,M.E.(2003b). Thestructureandfunctionofcomplexnetworks. SIAMreview,45,167–256.
Newman,M.E.,&Girvan,M.(2004). Findingandevaluatingcommunitystructureinnetworks. PhysicalreviewE,69,026113.
Paul,S.,&Chen,Y.(2020).Spectralandmatrixfactorizationmethodsforconsistentcommunitydetectioninmulti-layernetworks. TheAnnalsof
Statistics,48,230–250.
Paul,S.,&Chen,Y.(2021). Nullmodelsandcommunitydetectioninmulti-layernetworks. SankhyaA,(pp.1–55).
Pensky,M.,&Zhang,T.(2019). Spectralclusteringinthedynamicstochasticblockmodel.ElectronicJournalofStatistics,13,678–709.
20Psorakis,I.,Roberts,S.,Ebden,M.,&Sheldon,B.(2011). Overlappingcommunitydetectionusingbayesiannon-negativematrixfactorization.
PhysicalReviewE,83,066114.
Qing,H.,&Wang,J.(2023).Regularizedspectralclusteringunderthemixedmembershipstochasticblockmodel.Neurocomputing,550,126490.
Qing,H.,&Wang,J.(2024).Bipartitemixedmembershipdistribution-freemodel.anovelmodelforcommunitydetectioninoverlappingbipartite
weightednetworks. ExpertSystemswithApplications,235,121088.
Snijders,T.A.,Pattison,P.E.,Robins,G.L.,&Handcock,M.S.(2006). Newspecificationsforexponentialrandomgraphmodels. Sociological
methodology,36,99–153.
Su,W.,Guo,X.,Chang,X.,&Yang,Y.(2023). Spectralco-clusteringinrank-deficientmulti-layerstochasticco-blockmodels. arXivpreprint
arXiv:2307.10572,.
Tropp,J.A.(2012). User-friendlytailboundsforsumsofrandommatrices. Foundationsofcomputationalmathematics,12,389–434.
Wang,F.,Li,T.,Wang,X.,Zhu,S.,&Ding,C.(2011).Communitydiscoveryusingnonnegativematrixfactorization.DataMiningandKnowledge
Discovery,22,493–521.
Zhang,J.,&Cao,J.(2017). Findingcommonmodulesinatime-varyingnetworkwithapplicationtothedrosophilamelanogastergeneregulation
network. JournaloftheAmericanStatisticalAssociation,112,994–1008.
21