ProceedingsofMachineLearningResearchvolvvv:1–15,2024 6thAnnualConferenceonLearningforDynamicsandControl
Growing Q-Networks:
Solving Continuous Control Tasks with Adaptive Control Resolution
TimSeyde TSEYDE@MIT.EDU
MITCSAIL
PeterWerner WERNERPE@MIT.EDU
MITCSAIL
WilkoSchwarting WILKO@ISEE.AI
ISEEAI
MarkusWulfmeier* MWULFMEIER@GOOGLE.COM
GoogleDeepMind
DanielaRus* RUS@CSAIL.MIT.EDU
MITCSAIL
Editors:A.Abate,K.Margellos,A.Papachristodoulou
Abstract
Recent reinforcement learning approaches have shown surprisingly strong capabilities of bang-
bang policies for solving continuous control benchmarks. The underlying coarse action space
discretizationsoftenyieldfavourableexplorationcharacteristicswhilefinalperformancedoesnot
visiblysufferintheabsenceofactionpenalizationinlinewithoptimalcontroltheory. Inrobotics
applications, smooth control signals are commonly preferred to reduce system wear and energy
efficiency, but action costs can be detrimental to exploration during early training. In this work,
weaimtobridgethisperformancegapbygrowingdiscreteactionspacesfromcoarsetofinecon-
trolresolution,takingadvantageofrecentresultsindecoupledQ-learningtoscaleourapproachto
high-dimensionalactionspacesuptodim(A) = 38. Ourworkindicatesthatanadaptivecontrol
resolutionincombinationwithvaluedecompositionyieldssimplecritic-onlyalgorithmsthatyield
surprisinglystrongperformanceoncontinuouscontroltasks.
Keywords: ContinuousControl;Q-learning;ValueDecomposition;Growingresolution
1. Introduction
Reinforcement learning for continuous control applications commonly leverages policies parame-
terized via continuous distributions. Recent works have shown surprisingly strong performance of
discrete policies both in the actor-critic and critic-only setting (Tang and Agrawal, 2020; Tavakoli
et al., 2021; Seyde et al., 2021). While discrete critic-only methods promise simpler controller
designs than their continuous actor-critic counterparts, applications such as robot control tend to
favor smooth control signals to maintain stability and prevent system wear (Hodel, 2018). It has
previouslybeennotedthatcoarseactiondiscretizationcanprovideexplorationbenefitsearlyduring
training(Czarneckietal.,2018;Farquharetal.,2020),whileconvergedpoliciesshouldincreasingly
prioritizecontrollersmoothness(Bohezetal.,2019).
Our work aims to bridge the gap between these two objectives while maintaining algorithm
simplicity. We introduce Growing Q-Networks (GQN), a simple discrete critic-only agent that
©2024T.Seyde,P.Werner,W.Schwarting,M.Wulfmeier*&D.Rus*.
4202
rpA
5
]GL.sc[
1v35240.4042:viXraSEYDEWERNERSCHWARTINGWULFMEIER*RUS*
combines the scalability benefits of fully decoupled Q-learning (Seyde et al., 2022b) with the ex-
ploration benefits of dynamic control resolution (Czarnecki et al., 2018; Farquhar et al., 2020).
Introducinganadaptiveactionmaskingmechanismintoavalue-decomposedQ-Network,theagent
canautonomouslydecidewhentoincreasecontrolresolution. Thisapproachenhanceslearningeffi-
ciencyandbalancestheexploration-exploitationtrade-offmoreeffectively,improvingconvergence
speedandsolutionsmoothness. Theprimarycontributionsofthispaperarethreefold:
• Framework for adaptive control resolution: we adaptively grow control resolution from
coarse to fine within decoupled Q-learning. This reconciles coarse exploration during early
trainingwithsmoothcontrolatconvergence,whileretainingscalabilityofdecoupledcontrol.
• Insightsintoscalabilityofdiscretizedcontrol: ourresearchprovidesvaluableinsightsinto
overcoming exploration challenges in soft-contrained continuous control settings via simple
discreteQ-learningmethods,studyingapplicabilityinchallengingcontrolscenarios.
• Comprehensive experimental validation: we validate the effectiveness of our GQN algo-
rithm on a diverse set of continuous control tasks, highlighting benefits of adaptive control
resolutionoverstaticDQNvariationsaswellasrecentcontinuousactor-criticmethods.
The remainder of the paper is organized as follows: Section 2 reviews related work, Section
3 introduces preliminaries, Section 4 details the proposed GQN methodology, Section 4 presents
experimentalresults,andSection5concludeswithadiscussiononfutureresearchdirections.
2. RelatedWorks
Inthefollowing,wediscussseveralkeyrelatedworksgroupedbytheirprimaryresearchthrust.
Discretized Control Learning continuous control tasks typically relies on policies with con-
tinuous support, primarily Gaussians with diagonal covariance matrices (Schulman et al., 2017;
Haarnojaetal.,2018;Abdolmalekietal.,2018a;Hafneretal.,2020;Wulfmeieretal.,2020). Recent
works have shown that competitive performance is often attainable via discrete policies (Tavakoli
et al., 2018; Neunert et al., 2020; Tang and Agrawal, 2020; Seyde et al., 2022a) with bang-bang
control at the extreme (Seyde et al., 2021). Bang-bang controllers have been extensively investi-
gated in optimal control research (Sonneborn and Van Vleck, 1964; Bellman et al., 1956; LaSalle,
1959; Maurer et al., 2005) as well as early works in reinforcement learning (Waltz and Fu, 1965;
Lambert and Levine, 1970; Anderson, 1988), while the extreme switching behavior was often ob-
served to naturally emerge even under continuous policy distributions (Huang et al., 2019; Novati
and Koumoutsakos, 2019; Thuruthel et al., 2019). The direct application of discrete action-space
algorithmsthenharborspotentialbenefitsforreducingmodelcomplexity(Metzetal.,2017;Sharma
et al., 2017; Tavakoli, 2021; Watkins and Dayan, 1992), although control resolution trade-offs and
scalabilitymayrequirecomputationaloverhead(VandeWieleetal.,2020).
Scalability ScalabilityofQ-learningbasedapproacheshasbeenstudiedextensivelyinthecontext
ofmitigatingcoordinationchallengesandsystemnon-stationarity(Tan,1993;ClausandBoutilier,
1998; Matignon et al., 2012; Lauer and Riedmiller, 2000; Matignon et al., 2007; Foerster et al.,
2017;Busoniuetal.,2006;Bo¨hmeretal.,2019). Exponentialcouplingcanbeavoidedbyinformation-
sharing(Schneideretal.,1999;RussellandZimdars,2003;Yangetal.,2018),compositionoflocal
utility functions (Sunehag et al., 2017; Rashid et al., 2018; Son et al., 2019; Wang et al., 2020; Su
2GROWINGQ-NETWORKS
etal.,2021;Pengetal.,2021),andconsideringdifferentlevelsofinteraction(Guestrinetal.,2002;
Kok and Vlassis, 2006). Centralization can further be facilitated via high degrees of parameter-
sharing (Gupta et al., 2017; Bo¨hmer et al., 2020; Christianos et al., 2021; Van Seijen et al., 2017;
ChuandYe,2017)). DecoupledcontrolviaQ-learningwasproposedforAtari(Sharmaetal.,2017)
andextendedtomixingacrosshigher-orderactionsubspaces(Tavakolietal.,2021),withdecoupled
bang-bangcontroldisplayingstrongperformanceoncontinuouscontroltasks(Seydeetal.,2022b).
While coarse discretization can be beneficial for exploration, particularly in the presence of action
penalties,theymayalsoreducesteady-stateperformance. Conversely,finediscretizationcanexac-
erbatecoordinationchallenges(Seydeetal.,2022b;IrelandandMontana,2024). Here,weconsider
adaptingthecontrolresolutionoverthecourseoftrainingtoachievethebestofbothworlds.
ExpandingActionSpaces Smithetal.(2023)presentsanadaptivepolicyregularizationapproach
thatintroducessoftconstraintsonfeasibleactionregions,growingcontinuousregionslinearlyover
the course of training with adjustments based on dynamics uncertainty. They focus on learning
quadrupedallocomotiononhardwareandexpandlocallyaroundjointanglesofastableinitialpose.
In discrete action spaces, one can instead leverage iterative resolution refinement. Czarnecki et al.
(2018) considers DeepMind Lab navigation tasks (Beattie et al., 2016) with a natively discrete
action space that avoids reasoning about system dynamics stability. Their policy-based method
formulates a mixture policy that is optimized under a distillation objective to facilitate knowledge
transfer,adjustingthemixingweightsviaPopulationBasedTraining(PBT)(Jaderbergetal.,2017).
Similarly,Synnaeveetal.(2019)considersmulti-agentcoordinationinStarCraftandadjustsspatial
command resolution via PBT. Farquhar et al. (2020) grow action resolution under a linear growth
schedule, while showing limited application to simple continuous control tasks, as they enumerate
the action space and do not consider decoupled optimization. Beyond control applications, Yang
etal.(2023)demonstrateadaptivemeshrefinementstrategiesthatreducetheerrorsinfiniteelement
simulations. Theirrefinementpolicyrecursivelyaddsfinerelements,expandingtheactionspace.
Constrained Optimization Reward-optimal bang-bang policies may not be desirable for real-
world applications as they can be less energy efficient and increase wear and tear on physical sys-
tems, e.g. Hodel (2018). In the past, this behavior was generally avoided by employing penalty
functionsassoftconstraintsatthecostofpotentiallyhinderingexplorationorenablingrewardhack-
ing Skalse et al. (2022). The rewards and costs are automatically re-balanced to combat this issue
inBohezetal.(2019). Similarly,undesirablebehaviorsareavoidedbyautomaticallybalancingsoft
chanceconstraintswiththeprimaryrewardsinRoyetal.(2021). Here,wedonotassumeaccessto
explicitpenaltytermsandefficientlylearncontrollersdirectlybasedonenvironmentreward.
3. Preliminaries
WeformulatethelearningcontrolproblemasaMarkovDecisionProcess(MDP)describedbythe
tuple{S,A,T,R,γ},whereS ⊂ RN andA ⊂ RM denotethestateandactionspace,respectively,
T : S ×A → S thetransitiondistribution,R : S ×A → Rtherewardfunction,andγ ∈ [0,1)the
discountfactor. Lets anda denotethestateandactionattimet,whereactionsaresampledfrom
t t
policy π(a |s ). We define the discounted infinite horizon return as G = (cid:80)∞ γτ−tR(s ,a ),
t t t τ=t τ τ
where s ∼ T(·|s ,a ) and a ∼ π(·|s ). Our objective is to learn the optimal policy that max-
t+1 t t t t
imizes the expected infinite horizon return E[G ] under unknown dynamics and reward mappings.
t
Conventional algorithms for continuous control settings leverage actor-critic designs with a con-
3SEYDEWERNERSCHWARTINGWULFMEIER*RUS*
tinuouspolicyπ (a |s )maximizingexpectedreturnsfromavalueestimatorQ (s ,a )orV (s ).
ϕ t t θ t t θ t
Recentstudieshaveshownstrongresultswithsimplermethodsemployingdiscretizedactors(Tang
and Agrawal, 2020; Seyde et al., 2021) or critic-only formulations (Tavakoli et al., 2018, 2021;
Seyde et al., 2022b). Here, we focus on the light-weight critic-only setting and increase control
resolutionoverthecourseoftrainingtobridgethegapbetweendiscreteandcontinuouscontrol.
3.1. DeepQ-Networks
We consider the general framework of Deep Q-Networks (DQN) (Mnih et al., 2013), where the
state action value function Q (s ,a ) is represented by a neural network with parameters θ. The
θ t t
parameters are updated to minimize the temporal-difference (TD) error, where we leverage sev-
eral performance enhancements based on the Rainbow agent (Hessel et al., 2018). These include
target networks to improve stability in combination with double Q-learning to mitigate overesti-
mation (Mnih et al., 2015; Van Hasselt et al., 2016), prioritized experience replay (PER) to focus
sampling on more informative transitions (Schaul et al., 2015), and multi-step returns to improve
stabilityofBellmanbackups(SuttonandBarto,2018). Theresultingobjectivefunctionisgivenby
B
(cid:88)
L(θ) = L (y −Q (s ,a )), (1)
δ t θ t t
b=1
where action evaluation employs the target y = (cid:80)n−1γjr(s ,a ) + γnQ (cid:0) s ,a∗ (cid:1) ,
t j=0 t+j t+j θ− t+n t+n
actionselectionusesa∗ = argmax Q (s ,a),L (·)istheHuberlossandthebatchsizeisB.
t+1 a θ t+1 δ
3.2. DecoupledQ-Networks
TraditionalDQN-basedagentsenumeratetheentireactionspaceanddothereforenotscalewellto
high dimensional control problems. Decoupled representations address scalability issues by treat-
ing subsets of action dimensions as separate agents and coordinating joint behavior in expecta-
tion (Sharma et al., 2017; Sunehag et al., 2017; Rashid et al., 2018; Tavakoli et al., 2021; Seyde
et al., 2022b). The Decoupled Q-Networks (DecQN) agent introduced in Seyde et al. (2022b) em-
ploys a full decomposition with the critic predicting univariate utilities for each action dimension
aj conditionedontheglobalstates. Thecorrespondingstate-actionvaluefunctionisrecoveredas
Q (s ,a ) =
(cid:88)M Qj θ(s t,aj t)
, (2)
θ t t
M
j=1
wheretheobjectiveisanalogoustoEq.1,enablingcentralizedtrainingwithdecentralizedexecution.
4. GrowingQ-Networks
Discretecontrolalgorithmshavedemonstratedcompetitiveperformanceoncontinuouscontrolbench-
marks(TangandAgrawal,2020;Tavakolietal.,2018;Seydeetal.,2021). Onepotentialbenefitof
these methods is the intrinsic coarse exploration that can accelerate the generation of informative
environment feedback. In robot control, we typically prefer smooth controllers at convergence to
limithardwarestress. Ourobjectiveistobridgethegapbetweencoarseexplorationcapabilitiesand
smooth control performance while retaining sample-efficient learning. We leverage insights from
4GROWINGQ-NETWORKS
Figure1: SchematicofaGQNagentwithdecoupled5-bindiscretizationand3-binactivesubspace.
The available actions are highlighted in green while the masked actions are depicted in
gray. Thepredictedstate-actionvaluesQ(s,a0,...,aM)arecomputedvialinearcompo-
sitionoftheunivariateutilitiesQ(s,aj)byselectingoneactionperdimension(red).
the growing action space literature (Czarnecki et al., 2018; Farquhar et al., 2020) and consider a
decoupled critic that increases its control resolution over the course of training. To this end, we
definethediscreteactionsub-spaceatiterationg asAg ⊂ AandmodifytheTDtargettoyield
y =
n (cid:88)−1
γjr(s ,a
)+γn(cid:88)M
max
Qj θ−(s t+n,aj t+n)
, (3)
t t+j t+j
aj ∈Ag M
j=0 j=1 t+1
where ϵ-greedy action sampling is analogously constrained to Ag. The network architecture ac-
commodates the full discretized action space from the start and constrains the active set via action
masking, enabling masked action combinations to still profit from information propagation in the
shared torso (Van Seijen et al., 2017). A schematic depiction of a decoupled agent with 5-bin dis-
cretization and active 3-bin subspace is provided in Figure 1. In order to deploy such an agent we
requireascheduleforwhentoexpandtheactiveactionspaceAg → Ag+1. Here,weconsidertwo
simplevariationstolimitengineeringeffort. First, weconsideralinearschedulethatdoublescon-
trolresolutionevery 1 oftotaltrainingepisodes,whereN indicatesthenumberofsubspacesAg.
N+1
Second,weformulateanadaptiveschedulebasedonanupperconfidenceboundinspiredthreshold
overthemovingaveragereturns
G = (cid:0) 1.00−0.05sgnµG (cid:1) µG +0.90σG , (4)
threshold,t MA,t−1 MA,t−1 MA,t−1
whereµ andσ arethemovingaveragemeanandstandarddeviationoftheevaluationreturns,
MA MA
respectively. Theobjectiveunderestimatesthemeanby5%andexpandstheactionspacewhenever
thecurrentmeanreturnfallsbelowthethresholdµG < G ,signifyingperformancestagna-
t threshold,t
tion. This parameterization can avoid pre-mature expansion when exploring under sparse rewards,
butalternativeformulationsarealsoapplicable. Aqualitativeexampleofourapproachisprovided
inFigure2,wherewevisualizethestate-actionvaluefunctionoverthecourseoftrainingonapen-
dulum swing-up task. We consider a GQN agent with discretization 2 → 9 (meaning {2,3,5,9})
andprovidelearnedvaluesforeachactionbinstartingatinitializationandaddingaroweverytime
the action space is grown (top to bottom). The active bins are framed in green, where we observe
accuraterepresentationofthestate-actionvaluefunctionforactivebins,whiletheinactivebinsstill
providestructuredoutputduetohighdegreeofweightsharingprovidedbyourarchitecture.
In the following section, we provide quantitative results on a range of challenging continuous
controltasks. Weusethesamesetofhyperparametersthroughoutallexperiments,unlessotherwise
indicated,followingthegeneralparameterizationofSeydeetal.(2022b)withasimplemulti-layer
5SEYDEWERNERSCHWARTINGWULFMEIER*RUS*
Figure2: State-actionvaluesforapendulumswing-uptaskoverthecourseoftraining(toptobot-
tom). Theactivebinsareoutlinedingreen. Thevaluepredictionstransitionfromrandom
atinitializationtostructureduponactivation. Inactivebinsprofitfromtheemergentstruc-
turewithinthesharednetworktorsotowarm-starttheiroptimization.
perceptron architecture and dimensionality [512,512]. We evaluate mean performance with stan-
darddeviationacross4seedsand10evaluationepisodeforeachtask.
5. Experiments
We evaluate our approach on a selection of tasks from the DeepMind Control Suite (Tunyasuvu-
nakooletal.,2020),MetaWorld(Yuetal.,2020),andMyoSuite(Vittorioetal.,2022). Theformer
two benchmarks generally do not consider action penalties and have previously been solved with
bang-bang control (Seyde et al., 2022b). We therefore focus on action-penalized task variations to
encouragesmoothcontrolandhighlightexplorationchallengesinthepresenceofpenaltyterms.
We first evaluate performance on tasks from the DeepMind Control Suite with action dimen-
sionalityuptodim(A) = 38. Weconsider2penaltyweightsc ∈ {0.1,0.5},suchthatrewardsare
a
computed as r = ro −c (cid:80)M aj2 from original reward ro. We consider GQN agents that grow
t t a j=1 t t
theiractionspacediscretizationfrom2to9binsineachactiondimension,whereweevaluateboth
thelinearandadaptivegrowingschedulesdiscussedinSection4. Wecompareperformanceagainst
thestate-of-the-artcontinuouscontrolD4PG(Barth-Maronetal.,2018)andDMPO(Abdolmaleki
et al., 2018b) agents, while providing two discrete control DecQN agents with stationary action
spacediscretizationof2or9forreference. TheresultsinFigures3and4indicatethestrongperfor-
6GROWINGQ-NETWORKS
Figure3: Performance on tasks from the DeepMind Control Suite with action penalty −0.1|a|2.
Our GQN agent grows its action space from a 2 bin to a 9 bin discretization, where the
linear and adaptive expansion schedules yield similar results. The GQN agent performs
competitivetothediscreteDecQNaswellasthecontinuousD4PGandDMPObaselines,
achievingnoticeableimprovementsontheHumanoidStandandWalktasks.
mance of GQN agents, with the adaptive schedule improving upon the linear schedule in terms of
convergencerateandvariance. Growingcontrolresolutionfurtherprovidesaclearadvantageover
the stationary DecQN agents both in terms of final performance (vs. DecQN 2) and exploration
abilities (vs. DecQN 9). These observations mirror findings by Czarnecki et al. (2018), where
coarse control resolution was beneficial for early exploration, a characteristic that is amplified by
thepresenceofactionpenalties. WefurtherobservethestrongperformanceofdiscreteGQNagents
comparedtothecontinuousD4PGandDMPOagents.
In order to provide additional quantitative motivation for the presence of action penalties, we
compare smoothness of the converged policies in Figure 5. We consider the adaptive GQN agent
withactionpenaltiesc ∈ {0.1,0.5}andthecontinuousD4PGagentwithactionpenaltyc = 0.5.
a a
The metrics we consider are original non-penalized task performance, R, incurred action penalty,
P,actionmagnitude,|a|,instantaneousactionchange,|∆a|,andtheFastFourierTransform(FFT)
based smoothness metric from Mysore et al. (2021), SM. All metrics are normalized by the corre-
spondingvalueachievedbytheunconstrainedGQNagentwithc = 0.0. Theresultsindicatethat
a
increasing the action penalty yields noticeably smoother control signals while only having minor
impact on the original task performance as measured by the unconstrained reward, R. We further
findthatsmoothnessofthediscreteGQNagentisatleastasgoodasforthecontinuousD4PGagent
onthetasksconsidered(notethatD4PGisunabletosolvetheHumanoidtaskvariations,R ≈ 0).
We next extend our study to velocity-level control tasks for the Sawyer robot in MetaWorld.
Whileacceleration-levelcontroloftenprovidessufficientfilteringtointeractfavourablywithhighly
discretizedbang-bangexploration,velocity-levelcontroltendstorequiremorefine-grainedinputs.
WethereforeinvestigatescalabilityofgrowingactionspaceswithindecoupledQ-learningrepresen-
tations. To this end, we consider GQN agents with 2 → 9 and 9 → 65 (meaning {9,17,33,65})
discretization as well as a stationary DecQN agent with 9 bins. The results in Figure 6 indicate
that initial bang-bang action selection is not well-suited for generating velocity-level actions, with
7SEYDEWERNERSCHWARTINGWULFMEIER*RUS*
Figure4: Performance on tasks from the DeepMind Control Suite with action penalty −0.5|a|2.
Our GQN agent grows its action space from a 2 bin to a 9 bin discretization, where we
observe benefits of the adaptive variant over the linear schedule. The GQN agent yields
performanceimprovementsoverthediscreteDecQNaswellasthecontinuousD4PGand
DMPObaselines,withparticularlystrongdeltasontheHumanoidandFingertasks.
Figure5: Comparison of control smoothness and reward performance, relative to GQN without
action penalties. Increasing the action penalty coefficient yields smoother control while
onlyminorimpactontheoriginaltaskperformanceasmeasuredbyunconstrainedreward
R. ThediscreteGQNfurtherimprovesuponthecontinuousD4PGagent.
theagentachievinggoodperformanceoncetransitioningtomorefine-graineddiscretization(GQN
2 → 9). Interestingly, considering a larger growing action space with GQN 9 → 65 can surpass
the performance of a stationary DecQN 9 agent, despite the non-stationary optimization objective
induced by the addition of finer action discretizations over the course of training. Performance of
GQN9 → 65isfurthermorecompetitivewiththecontinuousD4PGagentonaverage.
Lastly,westress-testourapproachbyconsideringaselectionoftasksfromtheMyoSuitebench-
mark. The tasks require control of biomechanical models that aim to be physiologically accurate
with dim(A) = 39 and up to dim(O) = 115, and should constrain applicability of simple de-
coupled Q-learning approaches such as GQN. Indeed, we find that the agent capacity becomes a
limiting factor yielding overestimation errors that are further exacerbated by the large magnitude
reward signals. We therefore extend the network capacity to [512,512] → [2048,2048] and lower
8GROWINGQ-NETWORKS
Figure6: PerformanceonmanipulationtasksfromMetaWorldwithactionpenalty−0.5|a|2. These
tasks require control at the velocity level and are therefore more challenging to solve
withextremelycoarsediscretization. WethereforeinvestigatethescalabilityofourGQN
agent and consider growing discretizations from 9 up to 65 bins. The resulting policy
achieves stable learning and performs competitively with the continuous D4PG baseline
whileimprovingonthestationary9binsDecQNagent.
Figure7: Performance for controlling biomechanical models from the MyoSuite as measured by
tasksuccessattermination. Thesecontinuouscontroltasksstresstestgrowingdecoupled
discrete action spaces, due to their dimensionality and inherent complexity. Increasing
the network capacity and adjusting the discount factor to mitigate overestimation, we
observestrongperformanceforgrowingactionspacesuptoadiscretizationof65bins.
the discount factor γ = 0.99 → 0.95 (alternatively, increasing multi-step returns 3 → 5 worked
similarly well). With these parameter adjustments, we observe good performance as measured by
task success at the final step of an episode. This further underlines the surprising effectiveness
that decoupled discrete control can yield in continuous control settings and the benefit of adaptive
controlresolutionchangeoverthecourseoftraining.
6. Conclusion
Inthiswork,weinvestigatetheapplicationofgrowingactionspaceswithinthecontextofdecoupled
Q-learning to efficiently solve continuous control tasks. Our Growing Q-Networks (GQN) agent
leveragesalinearvaluedecompositionalongactuatorstoretainscalabilityinhigh-dimensionalac-
tion spaces and adaptively increases control resolution over the course of training. This enables
coarse exploration early during training without reduced control smoothness and accuracy at con-
vergence. The resulting agent is robust and performs well even for very fine control resolutions
9SEYDEWERNERSCHWARTINGWULFMEIER*RUS*
despiteinherentnon-smoothnessintheoptimizationobjectivearisingatthetransitionbetweenres-
olution levels. While GQN as a critic-only method displays very strong performance compared
to recent continuous actor-critic methods on the tasks considered, we also investigate scenarios
that prove challenging for decoupled discrete controllers as exemplified by velocity-level control
ofsimulatedmanipulatorsorapplicationstocontrolofbiomechanicalmodels. Interestingavenues
forfutureworkincludeaddressingcoordinationchallengesinincreasinglyhigh-dimensionalaction
spacesandmitigatingoverestimationbias. Generally,GQNprovidesasimpleyetcapableagentthat
efficiently bridges the gap between between coarse exploration and solution smoothness through
adaptivecontrolresolutionrefinement.
Acknowledgments
TimSeyde,PeterWerner,WilkoSchwarting,andDanielaRusweresupportedinpartbytheOffice
of Naval Research (ONR) Grant N00014-18-1-2830, Qualcomm, and the United States Air Force
ResearchLaboratoryandtheDepartmentoftheAirForceArtificialIntelligenceAcceleratorunder
Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this
documentarethoseoftheauthorsandshouldnotbeinterpretedasrepresentingtheofficialpolicies,
eitherexpressedorimplied, oftheDepartmentoftheAirForceortheU.S.Government. TheU.S.
Government is authorized to reproduce and distribute reprints for Government purposes notwith-
standing any copyright notation herein. The authors further would like to acknowledge the MIT
SuperCloudandLincolnLaboratorySupercomputingCenterforprovidingHPCresources.
References
Abbas Abdolmaleki, Jost Tobias Springenberg, Jonas Degrave, Steven Bohez, Yuval Tassa, Dan
Belov,NicolasHeess,andMartinRiedmiller. Relativeentropyregularizedpolicyiteration. arXiv
preprintarXiv:1812.02256,2018a.
AbbasAbdolmaleki,JostTobiasSpringenberg,YuvalTassa,RemiMunos,NicolasHeess,andMar-
tin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920,
2018b.
CharlesW.Anderson. LearningtoControlanInvertedPendulumwithConnectionistNetworks. In
ProceedingsoftheAmericanControlConference(ACC),1988.
GabrielBarth-Maron,MatthewWHoffman,DavidBudden,WillDabney,DanHorgan,DhruvaTb,
Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic
policygradients. arXivpreprintarXiv:1804.08617,2018.
CharlesBeattie,JoelZLeibo,DenisTeplyashin,TomWard,MarcusWainwright,HeinrichKu¨ttler,
AndrewLefrancq,SimonGreen,V´ıctorValde´s,AmirSadik,etal. Deepmindlab. arXivpreprint
arXiv:1612.03801,2016.
R.Bellman,I.Glicksberg,andO.Gross.Onthe“bang-bang”controlproblem.QuarterlyofApplied
Mathematics,14(1),1956.
StevenBohez,AbbasAbdolmaleki,MichaelNeunert,JonasBuchli,NicolasHeess,andRaiaHad-
sell. Valueconstrainedmodel-freecontinuouscontrol. arXivpreprintarXiv:1902.04623,2019.
10GROWINGQ-NETWORKS
Wendelin Bo¨hmer, Tabish Rashid, and Shimon Whiteson. Exploration with unreliable intrinsic
rewardinmulti-agentreinforcementlearning. arXivpreprintarXiv:1906.02138,2019.
WendelinBo¨hmer,VitalyKurin,andShimonWhiteson. Deepcoordinationgraphs. InInternational
ConferenceonMachineLearning,pages980–991.PMLR,2020.
LucianBusoniu,BartDeSchutter,andRobertBabuska. Decentralizedreinforcementlearningcon-
trol of a robotic manipulator. In 2006 9th International Conference on Control, Automation,
RoboticsandVision,pages1–6.IEEE,2006.
FilipposChristianos,GeorgiosPapoudakis,MuhammadARahman,andStefanoVAlbrecht. Scal-
ing multi-agent reinforcement learning with selective parameter sharing. In International Con-
ferenceonMachineLearning,pages1989–1998.PMLR,2021.
XiangxiangChuandHangjunYe. Parametersharingdeepdeterministicpolicygradientforcooper-
ativemulti-agentreinforcementlearning. arXivpreprintarXiv:1710.00336,2017.
CarolineClausandCraigBoutilier. Thedynamicsofreinforcementlearningincooperativemultia-
gentsystems. AAAI/IAAI,1998:2,1998.
Wojciech Czarnecki, Siddhant Jayakumar, Max Jaderberg, Leonard Hasenclever, Yee Whye Teh,
NicolasHeess,SimonOsindero,andRazvanPascanu.Mix&matchagentcurriculaforreinforce-
ment learning. In International Conference on Machine Learning, pages 1087–1095. PMLR,
2018.
GregoryFarquhar, LauraGustafson, ZemingLin, ShimonWhiteson, NicolasUsunier, andGabriel
Synnaeve. Growing action spaces. In International Conference on Machine Learning, pages
3040–3051.PMLR,2020.
JakobFoerster,NantasNardelli,GregoryFarquhar,TriantafyllosAfouras,PhilipHSTorr,Pushmeet
Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement
learning. InInternationalconferenceonmachinelearning,pages1146–1155.PMLR,2017.
Carlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In
ICML,volume2,pages227–234.Citeseer,2002.
Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using
deepreinforcementlearning. InInternationalconferenceonautonomousagentsandmultiagent
systems,pages66–83.Springer,2017.
TuomasHaarnoja,AurickZhou,KristianHartikainen,GeorgeTucker,SehoonHa,JieTan,Vikash
Kumar,HenryZhu,AbhishekGupta,PieterAbbeel,etal. Softactor-criticalgorithmsandappli-
cations. arXivpreprintarXiv:1812.05905,2018.
Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with dis-
creteworldmodels. arXivpreprintarXiv:2010.02193,2020.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney,
DanHorgan,BilalPiot,MohammadAzar,andDavidSilver.Rainbow: Combiningimprovements
indeepreinforcementlearning.InThirty-secondAAAIconferenceonartificialintelligence,2018.
11SEYDEWERNERSCHWARTINGWULFMEIER*RUS*
BenjaminJ.Hodel. LearningtoOperateanExcavatorviaPolicyOptimization. ProcediaComputer
Science,140,2018.
SandyH.Huang,MartinaZambelli,JackieKay,MuriloF.Martins,YuvalTassa,PatrickM.Pilarski,
andRaiaHadsell. LearningGentleObjectManipulationwithCuriosity-DrivenDeepReinforce-
mentLearning. arXiv:1903.08542,2019.
David Ireland and Giovanni Montana. Revalued: Regularised ensemble value-decomposition for
factorisablemarkovdecisionprocesses. arXivpreprintarXiv:2401.08850,2024.
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali
Razavi,OriolVinyals,TimGreen,IainDunning,KarenSimonyan,etal. Populationbasedtrain-
ingofneuralnetworks. arXivpreprintarXiv:1711.09846,2017.
Jelle R Kok and Nikos Vlassis. Collaborative multiagent reinforcement learning by payoff propa-
gation. JournalofMachineLearningResearch,7:1789–1828,2006.
J. Lambert and M. Levine. A two-stage learning control system. Trans. on Automatic Control, 15
(3),1970.
J.P.LaSalle. TimeOptimalControlSystems. ProceedingsoftheNationalAcademyofSciences,45
(4),1959.
MartinLauerandMartinRiedmiller. Analgorithmfordistributedreinforcementlearningincoop-
erative multi-agent systems. In In Proceedings of the Seventeenth International Conference on
MachineLearning.Citeseer,2000.
Lae¨titia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Hysteretic q-learning: an al-
gorithm for decentralized reinforcement learning in cooperative multi-agent teams. In 2007
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems,pages64–69.IEEE,2007.
LaetitiaMatignon,GuillaumeJLaurent,andNadineLeFort-Piat.Independentreinforcementlearn-
ers in cooperative markov games: a survey regarding coordination problems. The Knowledge
EngineeringReview,27:1–31,2012.
H. Maurer, C. Bu¨skens, J.-H. R. Kim, and C. Y. Kaya. Optimization methods for the verification
ofsecondordersufficientconditionsforbang–bangcontrols. OptimalControlApplicationsand
Methods,26(3),2005.
Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete sequential prediction of
continuousactionsfordeeprl. arXivpreprintarXiv:1705.05035,2017.
VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AlexGraves,IoannisAntonoglou,DaanWier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602,2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare,AlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal.Human-level
controlthroughdeepreinforcementlearning. nature,518:529–533,2015.
12GROWINGQ-NETWORKS
Siddharth Mysore, Bassel Mabsout, Renato Mancuso, and Kate Saenko. Regularizing action poli-
ciesforsmoothcontrolwithreinforcementlearning. In2021IEEEInternationalConferenceon
RoboticsandAutomation(ICRA),pages1810–1816.IEEE,2021.
Michael Neunert, Abbas Abdolmaleki, Markus Wulfmeier, Thomas Lampe, Tobias Springen-
berg, Roland Hafner, Francesco Romano, Jonas Buchli, Nicolas Heess, and Martin Riedmiller.
Continuous-discrete reinforcement learning for hybrid control in robotics. In Conference on
RobotLearning,pages735–751.PMLR,2020.
GuidoNovatiandPetrosKoumoutsakos. RememberandForgetforExperienceReplay. InInterna-
tionalConferenceonMachineLearning(ICML),2019.
Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr,
WendelinBo¨hmer,andShimonWhiteson. Facmac: Factoredmulti-agentcentralisedpolicygra-
dients. AdvancesinNeuralInformationProcessingSystems,34,2021.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
ShimonWhiteson. Qmix: Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforce-
ment learning. In International Conference on Machine Learning, pages 4295–4304. PMLR,
2018.
JulienRoy,RogerGirgis,JoshuaRomoff,Pierre-LucBacon,andChristopherPal. Directbehavior
specificationviaconstrainedreinforcementlearning. arXivpreprintarXiv:2112.12228,2021.
Stuart J Russell and Andrew Zimdars. Q-decomposition for reinforcement learning agents. In
Proceedingsofthe20thInternationalConferenceonMachineLearning(ICML-03),pages656–
663,2003.
TomSchaul,JohnQuan,IoannisAntonoglou,andDavidSilver.Prioritizedexperiencereplay.arXiv
preprintarXiv:1511.05952,2015.
JeffGSchneider,Weng-KeenWong,AndrewWMoore,andMartinARiedmiller.Distributedvalue
functions. InICML,1999.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
TimSeyde,IgorGilitschenski,WilkoSchwarting,BartolomeoStellato,MartinRiedmiller,Markus
Wulfmeier,andDanielaRus. Isbang-bangcontrolallyouneed? solvingcontinuouscontrolwith
bernoullipolicies. AdvancesinNeuralInformationProcessingSystems,34,2021.
Tim Seyde, Wilko Schwarting, Igor Gilitschenski, Markus Wulfmeier, and Daniela Rus. Strength
throughdiversity: Robustbehaviorlearningviamixturepolicies. InConferenceonRobotLearn-
ing,pages1144–1155.PMLR,2022a.
Tim Seyde, Peter Werner, Wilko Schwarting, Igor Gilitschenski, Martin Riedmiller, Daniela Rus,
andMarkusWulfmeier. Solvingcontinuouscontrolviaq-learning. InTheEleventhInternational
ConferenceonLearningRepresentations,2022b.
13SEYDEWERNERSCHWARTINGWULFMEIER*RUS*
SahilSharma,AravindSuresh,RahulRamesh,andBalaramanRavindran. Learningtofactorpoli-
cies and action-value functions: Factored action space representations for deep reinforcement
learning. arXivpreprintarXiv:1705.07269,2017.
Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and charac-
terizing reward gaming. Advances in Neural Information Processing Systems, 35:9460–9471,
2022.
Laura Smith, Yunhao Cao, and Sergey Levine. Grow your limits: Continuous improvement with
real-worldrlforroboticlocomotion. arXivpreprintarXiv:2310.17634,2023.
KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYungYi.Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In Interna-
tionalConferenceonMachineLearning,pages5887–5896.PMLR,2019.
L. M. Sonneborn and F. S. Van Vleck. The Bang-Bang Principle for Linear Control Systems.
JournaloftheSocietyforIndustrialandAppliedMathematicsSeriesAControl,2(2),1964.
JianyuSu, StephenAdams, andPeterABeling. Value-decompositionmulti-agentactor-critics. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11352–11360,
2021.
PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,Max
Jaderberg,MarcLanctot,NicolasSonnerat,JoelZLeibo,KarlTuyls,etal. Value-decomposition
networksforcooperativemulti-agentlearning. arXivpreprintarXiv:1706.05296,2017.
RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. 2018.
Gabriel Synnaeve, Jonas Gehring, Zeming Lin, Daniel Haziza, Nicolas Usunier, Danielle Rother-
mel, Vegard Mella, Da Ju, Nicolas Carion, Laura Gustafson, et al. Growing up together: Struc-
turedexplorationforlargeactionspaces. 2019.
MingTan. Multi-agentreinforcementlearning: Independentvs.cooperativeagents. InProceedings
ofthetenthinternationalconferenceonmachinelearning,pages330–337,1993.
YunhaoTangandShipraAgrawal. Discretizingcontinuousactionspaceforon-policyoptimization.
InProceedingsoftheAAAIConferenceonArtificialIntelligence, volume34, pages5981–5988,
2020.
Arash Tavakoli. On structural and temporal credit assignment in reinforcement learning. PhD
thesis,ImperialCollegeLondon,2021.
Arash Tavakoli, Fabio Pardo, and Petar Kormushev. Action branching architectures for deep rein-
forcementlearning. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume32,
2018.
Arash Tavakoli, Mehdi Fatemi, and Petar Kormushev. Learning to represent action values as a
hypergraph on the action vertices. In International Conference on Learning Representations,
2021.
14GROWINGQ-NETWORKS
Thomas George Thuruthel, Egidio Falotico, Federico Renda, and Cecilia Laschi. Model-Based
ReinforcementLearningforClosed-LoopDynamicControlofSoftRoboticManipulators. IEEE
T-RO,35(1),2019.
Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom
Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm control: Software and tasks for
continuouscontrol. SoftwareImpacts,6:100022,2020.
Tom Van de Wiele, David Warde-Farley, Andriy Mnih, and Volodymyr Mnih. Q-learning in enor-
mousactionspacesviaamortizedapproximatemaximization. arXivpreprintarXiv:2001.08116,
2020.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. InProceedingsoftheAAAIconferenceonartificialintelligence,volume30,2016.
Harm Van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, and Jeffrey
Tsang. Hybrid reward architecture for reinforcement learning. Advances in Neural Information
ProcessingSystems,30,2017.
Caggiano Vittorio, Wang Huawei, Durandau Guillaume, Sartori Massimo, and Kumar Vikash.
Myosuite – a contact-rich simulation suite for musculoskeletal motor control. https://
github.com/myohub/myosuite, 2022. URL https://arxiv.org/abs/2205.
13600.
M.WaltzandK.Fu. Aheuristicapproachtoreinforcementlearningcontrolsystems. IEEETACON,
10(4),1965.
Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Off-policy
multi-agentdecomposedpolicygradients. InInternationalConferenceonLearningRepresenta-
tions,2020.
ChristopherJCHWatkinsandPeterDayan. Q-learning. Machinelearning,8:279–292,1992.
Markus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost Tobias Springenberg, Michael Ne-
unert,NoahSiegel,TimHertweck,ThomasLampe,NicolasHeess,andMartinRiedmiller. Com-
positional Transfer in Hierarchical Reinforcement Learning. In Robotics: Science and Systems
(RSS),2020.
Jiachen Yang, Tarik Dzanic, Brenden Petersen, Jun Kudo, Ketan Mittal, Vladimir Tomov, Jean-
Sylvain Camier, Tuo Zhao, Hongyuan Zha, Tzanio Kolev, et al. Reinforcement learning for
adaptive mesh refinement. In International Conference on Artificial Intelligence and Statistics,
pages5997–6014.PMLR,2023.
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-
agent reinforcement learning. In International Conference on Machine Learning, pages 5571–
5580.PMLR,2018.
TianheYu,DeirdreQuillen,ZhanpengHe,RyanJulian,KarolHausman,ChelseaFinn,andSergey
Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learn-
ing. InConferenceonRobotLearning,pages1094–1100.PMLR,2020.
15