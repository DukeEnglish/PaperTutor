Identity Decoupling for Multi-Subject
Personalization of Text-to-Image Models
SangwonJang∗,1, JaehyeongJo∗,1, KiminLee†,1, SungJuHwang†,1,2
∗Equalcontribution †Equaladvising
KAIST1, DeepAuto.ai2
{ sangwon.jang, harryjo97, kiminlee, sjhwang82 }@kaist.ac.kr
Figure1: Givenafewimagesofmultiplesubjects(redbox),MuDIcanpersonalizeatext-to-image
model(suchasSDXL[34])togenerateimagesofmultiplesubjectswithoutidentitymixing. We
remark that some reference images (like Cloud Man and Blue Alien) are created by Sora [30],
introducingnovelconceptsnotpreviouslyencounteredbySDXL.
Abstract
Text-to-imagediffusionmodelshaveshownremarkablesuccessingeneratinga
personalizedsubjectbasedonafewreferenceimages. However,currentmethods
strugglewithhandlingmultiplesubjectssimultaneously,oftenresultinginmixed
identities with combined attributes from different subjects. In this work, we
presentMuDI,anovelframeworkthatenablesmulti-subjectpersonalizationby
effectivelydecouplingidentitiesfrommultiplesubjects. Ourmainideaistoutilize
segmentedsubjectsgeneratedbytheSegmentAnythingModelforbothtraining
andinference,asaformofdataaugmentationfortrainingandinitializationfor
the generation process. Our experiments demonstrate that MuDI can produce
high-qualitypersonalizedimageswithoutidentitymixing,evenforhighlysimilar
subjectsasshowninFigure1. Inhumanevaluation,MuDIshowstwiceasmany
successesforpersonalizingmultiplesubjectswithoutidentitymixingoverexisting
baselines and is preferred over 70% compared to the strongest baseline. More
resultsareavailableathttps://mudi-t2i.github.io/
Preprint.
4202
rpA
5
]VC.sc[
1v34240.4042:viXraReferences DreamBooth Cut-Mix MuDI(Ours)
Figure2: Comparisonofmulti-subjectpersonalizationmethodsusingCorgiandChowChow
images(redboxes). DreamBooth[39]producesmixedidentitydogs,suchasaCorgiwithChow
Chowears1. Cut-Mix[15]oftengeneratesartifactslikeunnaturalverticallines. Incontrast,ours
successfullypersonalizeseachdog,avoidingidentitymixingandartifactsobservedinpriormethods.
1 Introduction
Text-to-imagediffusionmodels,trainedonlargedatasetsofimageandtextpairs,haveshowngreat
successingeneratinghigh-qualityimagesforgiventextprompts[36,40,37,5]. Buildingonthis
success,thereisagrowinginterestinpersonalizingthesetext-to-imagemodels. Specifically,givena
fewimagesofasingleuser-definedsubject,severalmethodshavebeendevelopedtoenablethese
modelstogenerateimagesofthesubjectinnovelcontexts[13,39,47,23]. Furthermore,theseper-
sonalizationmethodshavebeenexpandedtoincludethecustomizationofstyle[42],background[44],
and activity [19], offering even greater flexibility and creativity in image generation, style [42],
place[44],relation[20],andaction[19].
Despitesignificantprogressinpersonalizingtext-to-imagemodelsforsinglesubjects,currentmethods
oftenstruggletohandlemultiplesubjectssimultaneously[23,15]. Whilerenderingeachsubject
individually is successful, these methods suffer from identity mixing during the composition of
subjects. For instance, as shown in Figure 2, recent works, such as DreamBooth [39], generate
imagesofmixedidentitieswhenappliedtotwodogs. Theproblemofidentitymixingbecomesmore
pronouncedwithsemanticallysimilarsubjectsthatshareattributes,suchascolorsortextures,leading
togreaterconfusioninmaintainingdistinctidentities.
Toaddressidentitymixinginmulti-subjectpersonalization,Hanetal.[15]proposedtoutilizeCut-
Mix[52], anaugmentationtechniquethatpresentsthemodelswithcut-and-mixedimagesofthe
subjectsduringpersonalization. However,usingCut-Mix-likeimagesinevitablyoftenresultsinthe
generationofunnaturalimageswithstitchingartifacts,suchasverticallinesthatseparatethesubjects.
Moreover,Cut-mixremainsunsuccessfulindecouplingsimilarsubjectsasshowninFigure2. There
arealternativeapproaches[8,26,14]thatrelyonpre-definedspatiallayouts,e.g.,boundingboxes
orControlNet[53],tospatiallyseparatetheidentities. However,suchauxiliaryinputsareusually
createdmanuallyandfallshortoffacilitatingnaturalinteractionsbetweensubjectswithinthepreset
layoutconstraints.
Inthiswork,weproposeMuDI,amulti-subjectpersonalizationframeworkthateffectivelyaddresses
identitymixing,evenforhighlysimilarsubjects. Ourkeyideaistoleveragethesegmentedsubjects
obtained by the Segment Anything Model (SAM) [22], enabling the decoupling of the identities
amongdifferentsubjects. Specifically,weextractsegmentationmapsofuser-providedsubjectsusing
SAMandutilizethemforbothtrainingandinference. Fortraining,weintroduceadataaugmentation
method that randomly composes segmented subjects, which allows efficient personalization by
removingidentity-irrelevantinformation. Additionally,weutilizethesegmentedsubjectstoinitialize
thegenerationprocess. InsteadofstartingfromGaussiannoise,webeginwithamean-shiftedrandom
noisegeneratedfromsegmentedsubjects. Wefindthatthisprovidesahelpfulhintforthemodelto
separatetheidentitiesandfurtherreducessubjectmissingduringgeneration. Notably,ourapproach
significantly mitigates identity mixing as shown in Figure 2, without relying on preset auxiliary
layoutssuchasboundingboxesorsketches.
1CustomDiffusion[23]alsoresultsinidentitymixingandweprovidetheexamplesinFigure14.
2Weevaluatetheeffectivenessoftheproposedframeworkusinganewdatasetofsubjectsthatareprone
toidentitymixing,spanningdiversecategoriesfromanimalstoobjectsandscenes. Tofacilitatethe
evaluation,weintroduceanewmetricdesignedtoassessthefidelityofmultiplesubjectsintheimages
accountingforidentitymixing. Inourexperiments,MuDIsuccessfullypersonalizesthesubjects
withoutmixedidentities,significantlyoutperformingDreamBooth[39],Cut-Mix[15],andTextual
Inversion[13],onbothqualitativeandquantitativecomparisons. Humanevaluationresultsshow
thathumanraterspreferMuDIoverothermethodsinside-by-sidecomparisons. Furthermore,we
demonstratethatourmethodcancontroltherelativesizebetweensubjectsandeffectivelydecouple
theidentitiesfrommorethantwosimilarsubjects. Finally,weshowthatourapproachcanbeeasily
appliedtomodularcustomization[14,33]forseparatingidentitywithoutauxiliarylayouts.
2 RelatedWork
Text-to-ImagePersonalization Personalizedtext-to-imagediffusionmodelshaveshownimpres-
siveabilitiestorenderasingleuser-specificsubjectinnovelcontextsfromonlyafewimages. Two
representativeclassesofpersonalizationmethodshavebeenproposedbyGaletal.[13]andRuizetal.
[39]. TextualInversion[13]optimizesnewtextembeddingforrepresentingthespecifiedsubjects
andhasbeenimprovedbyP+[47]andNeTI[1]forlearningonextendedembeddingspaces. Onthe
otherhand,DreamBooth[39]fine-tunestheweightsofthepre-trainedmodeltobindnewconcepts
withuniqueidentifiersandhasbeenfurtherdevelopedthroughrecentworks[23,15,45]forefficiently
fine-tuningthemodels. However,existingmethodsfallshortofsynthesizingmultipleuser-defined
subjects together, suffering from identity mixing. Han et al. [15] introduce Cut-Mix to address
identity mixing by augmenting Cut-Mix-like images during training but fails to separate similar
subjectsandgeneratesstitchingartifacts. Inthiswork,wedevelopanovelframeworkthatallowsthe
personalizationofmultiplesubjectswithoutmixingevenforsubjectswithsimilarappearances.
Layout-GuidedMulti-SubjectComposition Alineofworksdealswithidentitymixingformulti-
subject composition by using pre-defined spatial layout guidance. Anydoor [8] places multiple
concepts in user-specified locations in the scene image by recomposing extracted features and
Cones2[26]composesthesubjectsusingthespatialguidanceprovidedbyuser-definedbounding
boxesandthecross-attentionmaps. Additionally,ControlNet[53]canbeappliedtomitigateidentity
mixingbycontrollingthegenerationviapresetspatialconditions,e.g.,keyposeorsketch. However,
these methods require auxiliary layouts for each subject, which must be manually prepared in
advancewhichposesasignificantlimitationtotheirpracticality. Moreover,thelayouthindersnatural
interactionbetweensubjectswithinthegeneratedscenes.Incontrast,ourapproachenablesgeneration
inaprompt-onlymannerthatisfreefromhumaninvolvementandfurtheryieldsnaturalinteractions.
ModularCustomization Recentworks[23,14,33]exploreadifferentscenarioforpersonalizing
multiple subjects, namely modular customization, where the subjects are independently learned
by models and users mix and match the subjects during inference to compose them. Custom
Diffusion[23]mergesindividuallyfine-tunedmodelsbysolvingconstrainedoptimizationandMix-
of-Show [14] introduces gradient fusion to merge single-concept LoRAs [18]. When handling
multiplesubjects,theseworksalsosufferfromidentitymixing,andtheyrelyonpresetspatiallayouts
suchasControlNet[53]andregionalprompts[14]toaddresstheproblem. Notably,ourmethodcan
beappliedtothisscenariotodecoupletheidentitiesofthesubjectswithoutusingauxiliarylayouts.
3 Preliminary
Text-to-Image Diffusion Models Diffusion models [17, 43] generate samples from noise by
learningtoreversetheperturbation,i.e.,denoise,whichcanbemodeledbyadiffusionprocess. Tobe
specific,ateachstepofthediffusion,themodelpredictstherandomnoiseϵ∼N(0,I)thathasbeen
usedtocorruptthesample. Text-to-imagediffusionmodels[40,37]incorporatetextconditionsfor
thegeneration. GiventhedatasetDconsistingoftheimage-textpairs(x,c),text-to-imagediffusion
modelsparameterizedbythenoisepredictionmodelϵ canbetrainedwiththefollowingobjective:
θ
L DM(θ;D)=E (x,c)∼D,ϵ∼N(0,I),t∼U(0,T)(cid:104)(cid:13) (cid:13)ϵ θ(x t;c,t)−ϵ(cid:13) (cid:13)2 2(cid:105) , (1)
where ϵ is the random noise, time t is sampled from the uniform distribution U(0,T), and x =
t
α x+σ ϵforthecoefficientsα andσ thatdeterminesthenoisescheduleofthediffusionprocess.
t t t t
3PersonalizingText-to-ImageModels Givenafewimagesofasinglespecificsubject, Dream-
Booth[39]fine-tunestheweightofthediffusionmodelwithauniqueidentifierforthesubject,i.e.,
"a[identifier][classnoun]". Themodelweightsareupdatedtolearnthesubjectwhilepreservingthe
visualprior,whichcanbeachievedbyminimizingtheobjective:
L (θ)=L (θ;D )+λL (θ;D ), (2)
DB DM ref DM prior
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
personalizationloss priorpreservationloss
whereL isthelossdefinedinEq.(1),D isthedatasetconsistingofreferenceimagesofthe
DM ref
subject,D isthedatasetconsistingofclass-specificpriorimages,andλisacoefficientforthe
prior
priorpreservationloss. Similartopersonalizingasinglesubject,existingworks[23,15]jointlytrain
formultiplesubjectsbycombiningtheimagesfromthesetofuser-specifiedsubjectstoconstruct
D andusingdifferentidentifiersforeachsubject.
ref
4 MuDI:Multi-subjectPersonalizationforDecoupledIdentities
Inthissection, wepresentMuDI:Multi-subjectpersonalizationforDecoupledIdentities, which
leveragessegmentedsubjectstoseparateidentities. InSection4.1,wedescribeourtrainingmethod,
whichaugmentstrainingdatathroughrandomcompositionsofsegmentedsubjects.Wealsointroduce
asimpleinferencemethodthatinitializesnoiseforsamplegenerationbasedonsubjectsegmentation
inSection4.2. Finally,weprovideapplicationsofourapproachtovariousscenariosinSection4.3.
4.1 Training
PersonalizationwithAugmentation Toaddressidentitymixinginmulti-subjectpersonalization,
weintroduceanewdataaugmentationmethodfortrainingthepre-trainedtext-to-imagemodelcalled
Seg-Mix. Thismethodaimstomitigateidentitymixingbyleveragingsegmentedsubjectsduring
personalizingtext-to-imagemodels. Byisolatingeachsubjectfromthebackground,Seg-Mixenables
themodeltolearntodistinguishbetweendifferentidentitieseffectively. WeintegrateSeg-Mixwith
DreamBooth[39],atechniquethatpersonalizestext-to-imagemodelsusinguniqueidentifiers(see
Eq.(1)fordetails).
ToimplementSeg-Mix,wepreprocessreferenceimagesbyautomaticallyextractingsegmentation
mapsofuser-providedsubjectsusingtheSegmentAnythingModel(SAM)[22]. Specifically,this
process begins with the extraction of subject bounding boxes using the OWLv2 [28], an object
detectionmodelwithanopenvocabulary. Subsequently,SAMsegmentsthesubjectsbasedonthese
boundingboxes,asillustratedinFigure3(a). Afterthepreprocessingstep,wecreateaugmented
imagesbyrandomlypositioningtheresizedsegmentedsubjects,asillustratedinFigure3(b). We
alsoprovideourdetailedimplementationinAlgorithm1. Theseaugmentedimagesarepairedwith
a simple prompt "A photo of [V ] and [V ], simple background.", which is designed to remove
1 2
identity-irrelevantinformationexplicitly. Wealsoapplythisaugmentationtothepriordatasetby
creatingimagesfromsegmentedpriorsubjects.Usingaugmenteddatasets,wefine-tunetext-to-image
modelsbasedontheDreamBoothobjectivefunctioninEq.(1).
OneofthekeyadvantagesofSeg-Mixisitsabilitytotrainmodelswithoutidentity-irrelevantartifacts,
duetotheremovalofbackgrounds. Thisprocessalsomitigatesunnaturalartifacts,suchasstitching
artifactsobservedinpreviousmethodslikeCut-Mix[15]. Moreover,byallowingsubjectstooverlap
duringSeg-Mix,wepreventattributesfromleakingtoneighboringidentitiesandenhanceinteraction
amongthesubjects,asdemonstratedinAppendixC.2.
DescriptiveClass Intuitively,fortwosimilarsubjectsinthesamecategory,separatingthemsolely
withtheuniqueidentifiers,e.g.,"[V ]dog"and"[V ]dog",isachallengingtaskandpronetoidentity
1 2
mixing. Insingle-subjectpersonalization,Chaeetal.[7]observedthataddingdetaileddescriptions
infrontoftheclassnounshelpsincapturingthevisualcharacteristicsofraresubjects. Inspiredby
thisobservation, weadoptspecificclassnouns(e.g., Weimaranerinsteadofdog)oradddetailed
descriptionsinfrontofgeneralclassnouns(e.g.,whiterobottoyinsteadoftoy). Insteadofmanually
selectingappropriateclasses,weleverageGPT4-v[31]toautomaticallyobtainthesespecificclass
nounsordescriptions.Weempiricallyvalidatethatthissimplemodificationimprovesthepreservation
ofthedetailsformultiplesubjectsleadingtothedecouplingoftheidentitiesofhighlysimilarsubjects.
4(a) Preprocessing (b) Training
“A photo of [V1] and [V2],
Refs Priors simple background.”
“monster toy”
“robot toy”
𝝐 𝓛
𝜽 𝑫𝑩
OWLv2 SAM
√ Decoupled Id
√ Controlled Scale
(c) Inference
0 in latent space
Enc ℰ(𝑥𝑖𝑛𝑖𝑡)
⊙ ∗𝛾 𝝐 𝜽
R𝑒𝑠𝑖𝑧𝑒
Latent Random
(𝑀𝑖𝑛𝑖𝑡)
Noise Mean-shifted “[V1] and [V2], √ Decoupled Id
Noise floating on pool.”
ꭙ Subject Missing
Initialization
Figure3: OverviewofMuDI.(a)WeautomaticallyobtainsegmentedsubjectsusingSAM[22]and
OWLv2[28]inthepreprocessingstage. (b)Weaugmentthetrainingdatabyrandomlypositioning
segmentedsubjectswithcontrollablescalestotrainthediffusionmodelϵ . Werefertothisdata
θ
augmentationmethodasSeg-Mix. (c)Weinitializethegenerationprocesswithmean-shiftednoise
createdfromsegmentedsubjects,whichprovidesasignalforidentityseparation.
4.2 Inference
Ithasbeenobservedthatinitialnoisesinsamplegenerationaffecttheoverallqualityofgenerated
images[27,41]. Wealsofindthatthequalityofgeneratedsamplesfrompersonalizedmodelswith
Seg-Mixvariesdependingontheinitialnoises. Motivatedbythisobservation,weproposeanovel
inference method to improve identity decoupling without additional training and computational
overhead.
AsillustratedinFigure3(c),wefirstcreateanimagex ofsegmentedsubjectsfollowingSeg-Mix
init
andextractitslatentembeddingfromVAEencoderE [21]. Wethenaddthislatentembeddingtoa
randomGaussiannoiseϵ,scaledbyacoefficientγ:
z =(E(x )⊙Resize(M ))∗γ+ϵ, (3)
T init init
whereResize(M )denotestheresizedversionofsegmentationmaskM . Thismean-shifted
init init
noisez encodescoarseinformationaboutthesubjectsandtheirlayout,servingasagoodstarting
T
pointinsamplegeneration. TheentireinferenceprocessissummarizedinAlgorithm2.
Weremarkthatourinitializationmethodalsoaddressestheissueofsubjectdominance[46],where
certainsubjectdominatesthegenerationwhileothersubjectsareignored. Byprovidinginformation
throughtheinitialcomposition,ourinferencemethodguidesthemodeltoconsiderallsubjects. We
validatethatourinferencemethodcaneffectivelyalleviatesubjectdominance,whichiscrucialwhen
renderingmanysubjectssimultaneously,inSection5.3.
4.3 OtherUseCases
RelativeSizeControl Theproposedframeworkoffersanintuitivewaytocontroltherelativesize
between the personalized subjects. By resizing the segmented subjects according to user intents
inSeg-Mix,wefindthatpersonalizedmodelsgeneratesubjectswiththedesiredrelativesizes(see
Section5.4forsupportingresults). Thisshowcasesanotherbenefitofourmethodunlikeprevious
methods[39,23,15],whichoftenresultininconsistentrelativesizesduetoalackofsizeinformation
duringfine-tuning.
ModularCustomization OurSeg-Mixisprimarilydesignedforjointtraining,requiringre-training
fornewcombinationsofmultiplesubjectswewanttorendersimultaneously. However,itcanalso
be applied to modular customization. That is, the models are independently fine-tuned for each
subjectandthenefficientlymerged,avoidingtheneedfortrainingfromthebeginning. Specifically,
whenwepossesssingle-conceptLoRAsthathavebeenindependentlyfine-tuned,wefirstgenerate
5imagesofeachsubjectusingtheircorrespondingsingle-conceptLoRA.Thesegeneratedimagesare
subsequentlyutilizedasreferenceimages. Wethenmergethesingle-subjectfine-tunedLoRAsusing
anexistingmethodsuchasgradientfusion[14],andapplySeg-Mixfine-tuningwiththesegenerated
single-subjectimagesfor200-300iterationsinthecaseoftwoconcepts. Thisprocesseffectively
reducesidentitymixingsimilartoSeg-Mix,whilealsoenablingthereuseofsingle-subjectLoRAs,
whichisthebenefitofmodularcustomization. WeillustrateourapproachinFigure25andprovide
examplesformodularcustomizationinSection5.4.
LLM-guidedInferenceInitialization Generatingcomplexinteractionsinvolvingrelationslike
"[V ]toysittingon[V ]can"canbechallengingforpersonalizedsubjects.However,ourinitialization
1 2
method,whichprovidesthelayoutforsubjects,cansignificantlyassistthisifitprovidesawell-aligned
layoutreflectingtheprompt,suchasplacingthetoyabovethecan.
InspiredbyChoetal.[9],weinitiallyutilizeLargeLanguageModels(LLMs)togeneratethelayouts,
whichincludeboundingboxesforeachsubjectalignedwiththegivenprompt. Thisgeneratedlayout
canbeutilizedinourinitializationtopositionsegmentedsubjects. SuchLLM-guidedinitialization
significantlyenhancestheabilitytorendercomplexinteractionsbetweensubjects. Examplesare
providedinFigure21.
5 Experiments
5.1 ExperimentalSetup
Dataset Weconstructanewdatasettoevaluatetheperformanceofidentitydecouplingformulti-
subjectpersonalizationmethods. Thisdatasetconsistsof8pairsofsimilarsubjectsthatareproneto
identitymixing. WecollectedimagesfromtheDreamBenchdataset[39]andtheCustomConcept101
dataset [23], consisting of diverse categories including animals, objects, and scenes. For each
pair of subjects, we generate 5 evaluation prompts using ChatGPT [29], which describe scenes
involvingthesubjectswithsimpleactionsandbackgrounds. Weprovidemoredetailsofthedataset
inAppendixA.2.
Implementation Details For all experiments, we use Stable Diffusion XL (SDXL) [34] as the
pre-trainedtext-to-imagediffusionmodelandemployaLoRA[18]witharankof32forU-Net[38]
module. Forallmethods,wepairthereferenceimageswithcomprehensivecaptionsobtainedthrough
GPT-4v[31]insteadofusingasimplepromptlike"Aphotoofa[V]". Thisapproacheffectively
mitigatesoverfittingtothebackgroundandshowsbettertextalignment. Weevaluate400generated
imagesforeachmethod,across8combinationswith5evaluationpromptsand10imagesoffixed
randomseeds. WeprovidemoredetailsinAppendixA.1.
Baselines Weevaluateourmethodagainstthreemulti-subjectpersonalizationmethods: Dream-
Booth [39], DreamBooth using Cut-Mix [15] augmentation, namely Cut-Mix, and Textual Inver-
sion [13]. Note that we exclude Custom Diffusion [23] from the baselines due to its low quality
whenitisappliedtoSDXL(seeAppendixB.1). ForbothCut-MixandSeg-Mix, weuseafixed
augmentationprobabilityof0.3,andwedonotuseUnmixregularization[15]asitdegradestheimage
qualityforSDXL(seeAppendixBfordetaileddiscussion). Wedescribefurtherimplementation
detailsinAppendixA.1.
HumanEvaluation Weconducthumanevaluationstoassessthequalityofthegeneratedimages
fromthebaselinesandourmethod. Wefirstaskhumanraterstoevaluatethemulti-subjectfidelity
viabinaryfeedback,i.e.,goodorbad. Moreover,weprovidereferenceimagesofeachsubjectalong
withtwoanonymizedimages,i.e.,onefromMuDIandtheotherfromCut-Mix,bothusingthesame
randomseed. Weaskhumanraterstoevaluatewhichimagetheypreferbasedonthreecriteria: (1)
similaritytothesubjectsinthereferenceimages,(2)alignmentwiththegiventext,and(3)image
fidelity. Ifbothimagesfailtodepictthesubjectsinthereferenceimages, ratersareinstructedto
select"cannotdetermine". FurtherdetailsonthehumanevaluationareprovidedinAppendixA.4.
EvaluationMetrics Weevaluatemulti-subjectpersonalizationmethodsontwokeyaspects: multi-
subjectfidelitythatmeasuresthepreservationofsubjectdetailsformultiplesubjectsandtextfidelity
thatmeasuresthealignmentofthegeneratedimagestothegiventextprompt.
6“…in a garden “…playing in the waves “…on top of the hill “…having a tea party “…,seabirds circling above “…resting on a bench
full of flowers.” at the beach.” with full moon.” with table full of desserts.” in a clear blue sky.” in a sunny park.”
Figure4: QualitativecomparisonofMuDI,Cut-Mix,DreamBooth,andTextualInversion(TI).Im-
agesinthesamecolumnaregeneratedwiththesamerandomseed. MoreexamplesinAppendixA.5.
Existingmetricsdesignedformeasuringsubjectfidelity,suchasCLIP-I[35]orDINOv2[32],might
notbesuitablebecausetheydonotaccountforidentitymixingamongmultiplesubjects. Toaddress
thislimitation,weintroduceanewmetric,calledDetect-and-Compare(D&C),forevaluatingmulti-
subjectfidelity. Theproposedmetricinvolves(1)detectingthesubjectsinthegeneratedimageand
(2)comparingthedetectedsubjectswiththereferencesubjects.
We illustrate an overview of D&C in Figure 10 and provide a more detailed explanation in Ap-
pendixA.3. Fortheexperiments,wereporttheresultsusingbothDreamSim[12](D&C-DS)and
DINOv2[32](D&C-DINO)formulti-subjectfidelity. Wealsofindthatournewmetrichasdemon-
stratedasignificantlybettercorrelationwithhumanevaluationcomparedtoexistingmethods,as
furtheranalyzedinAppendixA.3.
Fortextfidelity,wereporttheresultsofImageReward[51]andCLIPscore(CLIPs)[35],wherewe
considerthepermutationsoftheorderofthesubjectsintheprompt,forexample,"[V ]and[V ]ina
1 2
jungle"and"[V ]and[V ]inajungle,"andreporttheaveragetoavoidpositionbias.
2 1
5.2 MainResults
Qualitativecomparison AsshowninFigure4,ourapproachsuccessfullygeneratesthesubjects
avoidingidentitymixing,evenforsimilarsubjectssuchastwodogs(2ndcolumn). Onthecontrary,
DreamBoothresultsinmixedidentities,andCut-Mixalsofallsshortofdecouplingtheidentities
whileproducingstitchingartifacts. TextualInversionfailstopreservethesubjects’details.
Human Evaluation Figure 5(Left) shows the human evaluation results for which our MuDI
significantly outperforms the prior works on multi-subject fidelity, demonstrating twice as much
successforpreventingidentitymixingcomparedtoCut-Mix. Inparticular,thehumanpreference
ratingshowsanincomparablystrongpreferencefortheimagesgeneratedbyMuDIovertheimages
generatedbyCut-Mix,sinceMuDIiscapableofdecouplingtheidentitiesofhighlysimilarsubjects
withoutproducingstitchingartifacts.
7
sfeR
)sruO(IDuM
xiM-tuC
htooBmaerD
ITFigure5: (Left)Humanevaluationresultsonmulti-subjectfidelityandoverallpreference. (Right)
Quantitative results on multi-subject fidelity and text fidelity. † denotes the text fidelity score
consideringthepermutationofthesubjectsintheprompttoavoidpositionbias.
Multi-subject Fidelity Overall Preference
Multi-SubjectFidelity TextFidelity
Method D&C-DS↑ D&C-DINO↑ ImageReward†↑ CLIPs†↑
TI[13] 0.021 0.019 -0.149 0.227
DB[39] 0.147 0.105 0.579 0.255
Cut-Mix[15] 0.237 0.181 -0.287 0.225
MuDI(Ours) 0.434 0.293 0.770 0.263
Ours DB Cut-Mix TI Ours Cut-Mix Tie
w/o Seg-Mix w/o Initialization w/o Descriptive Class MuDI (Ours)
References
[V1]
“A photo of [V1] and [V2] playing together, floating on the pool.”
[V2]
“A photo of [V1] and [V2] jumping in a muddy puddle.”
Figure6: AblationStudiesonMuDI.WhileourmethodsuccessfullypersonalizesCorgiandChow
Chow,ablatingSeg-Mixresultsinmixedidentitydogs. Inferencewithoutourinitializationgenerates
imagesofthesubjectmissing. Trainingwithoutdescriptiveclassfailstocatchsubjectdetails.
QuantitativeResults WereportthequantitativeresultsintheTableofFigure5,whereourframe-
work achieves the highest multi-subject fidelity and text fidelity, and the performance gaps with
thepreviousapproachesaresignificantlylarge. Theseresultsalignwiththequalitativeresultsand
humanevaluation,whereMuDIisabletopreservethesubjects’detailswithoutidentitymixingwhile
previousmethodsfailtodoso. Thehighesttextfidelityindicatesthatourframeworkiscapableof
generatingthesubjectswithoutmixingwhileproperlyfollowingthegivenprompt.
5.3 AblationStudies
Necessity of Seg-Mix To validate that our Seg-Mix is cru- Multi-SubjectFidelity
cialfordecouplingthesubjects’identities,wecompareMuDI
Method D&C-DS↑ D&C-DINO↑
against its variant without it. As shown in Table 1, ablating
w/oSeg-Mix 0.236 0.133
Seg-Mixresultsinsignificantlylowmulti-subjectfidelity,im- w/oInitialization 0.291 0.195
pactingthemostamongotherablations. Qualitativeablation w/oDesc.Class 0.378 0.271
in Figure 6 demonstrates that the attributes of the Corgi and MuDI(Ours) 0.434 0.293
ChowChowarecompletelymixedwithoutourSeg-Mix. In
Table 1: Quantitative results on
particular,weshowinFigure18thatusingadditionallayout
ablationstudies.
guidance,e.g.,ControlNet[53],withoutSeg-Mixstillsuffers
fromidentitymixing.
ImportanceofOurInitialization WeshowinFigure6thatusinginferenceinitializationimproves
identityseparationandeffectivelyreducessubjectdominance.Table1validatesthatimagesgenerated
withoutitresultinlowersubjectfidelity. Weempiricallyfindthattheinitializationofthegeneration
processprovidessignificantbenefitsinthreescenarios: (1)personalizingunusualsubjectsthatpre-
8“… walking together in Times Square.” “… boxing in the octagon, match day.”
+ Seg-Mix
(a) Controlling Relative Size (b) Modular Customization with Seg-Mix
“… on a wooden desk with laptop.”
“… gaze at bright fireworks in darkness.”
+ IT + KL Reg.
(c) More than Two Subjects (d) Iterative Training for MuDI
Figure7: (a)ControllingrelativesizewithSeg-Mix. WevisualizesamplesgeneratedbyMuDI
using size-controlled Seg-Mix. (b) Modular customization. Applying Seg-Mix after merging
LoRAs significantly improves identity decoupling. (c) Personalizing more than two subjects.
MuDI can decouple more than two similar subjects. (d) Comparison of samples generated by
MuDIandMuDIwithiterativetrainingforthesamerandomseed. Iterativetrainingimproves
identitydecouplingandKLregularizationalleviatesover-saturation.
trainedmodelsstruggletogenerate(e.g.,thecloudmanofFigure1bottom-right),(2)personalizing
morethantwosubjects,and(3)usingcomplexprompts. WeprovideexamplesinFigure20.
DescriptiveClass Lastly,weshowinFigure6thatusingdescriptiveclassestorepresentthesubjects
improvesthepreservationofthesubjects’detail,andTable1furthershowsthatthismethodenhances
subjectfidelity. Despitethisimprovement,usingdescriptiveclassesonlymaysometimescausea
dominanceofthesubject,leadingtosomesubjectsbeingignored. Thischallengewaseffectively
addressedbyapplyingourinitialization,whichresultedinsignificantlyimprovedoutcomes.
5.4 Applications
ControllingRelativeSize Here,wedemonstratethattherelativesizebetweensubjectscanbe
controlledusingSeg-Mix. AsshowninFigure7(a),wecanpersonalizemodeltousuallygenerate
thedoglargerthanthetoyorviceversa,bysettingtherelativesizeforthedogandthetoyduring
Seg-Mix. The generated images show a consistent relative size without the need for additional
prompts,whichweprovidemoreexamplesinFigure24.
ModularCustomization WeshowthatourSeg-Mixcanbeappliedtomodularcustomizationfor
separatingtheidentities. WecompareMix-of-Show[14]anditadditionallyfine-tunedwithSeg-Mix
onlyforafewiterations. WhileMix-of-showwithoutusingspatiallayoutproducesmixedidentities
charactersoftheotterandthemonster(Figure7(b),left),ourfine-tunedmodelproducesahigh-quality
imageofclearlyseparatedsubjects(Figure7(b),right). Wefurtherobservesignificantbenefitsfrom
integratingKullback-Leibler(KL)divergenceasregularization[11]intoourfine-tuningobjectives,
specificallywithrespecttothemergedLoRA.Thisapproacheffectivelymitigatesover-saturation
andpreventsoverfittingonself-generatedimageswhichweuseinSeg-Mix.
More than Two Subjects We demonstrate in Figure 1 and Figure 7(c) that our method can
successfullypersonalizemorethantwosubjectsthatarehighlysimilar. Inparticular,wefindthat
duringSeg-Mix,itissufficienttoaugmentthedatasetwithimagesofrandomlycomposedpairsof
subjects,ratherthancomposingallthesubjectstogether(threeormore). Comparedtothecasewith
twosubjects,ahigheraugmentationprobabilityisrequiredtoseparatemorethantwosubjects. When
werendermorethanthreesubjectssimultaneously,weobservethatsomesubjectsarefrequently
ignoredevenusingMuDI.WeprovidemoreexamplesinFigure26.
95.5 IterativeTraining
Tofurtherimprovethequality,weinvestigateaniterativetraining(IT)method[42],whichfine-tunes
personalized models using high-quality samples obtained from the previous model. Specifically,
we first generate multi-subject images using a personalized model, and then select high-quality
imagesbasedonourD&Cmetric. Theseimagesaregeneratedusingvariouspromptscreatedby
ChatGPT [29]. Using selected images, we fine-tune models with KL regularization [11], which
mitigates the over-saturation issues. We provide more details in Appendix D.2. As shown in
Figure7(d),iterativetrainingfurtherimprovesthequalitybydecouplingtheidentitiesevenforsimilar
subjects. WeprovidemoreexamplesinFigure22.
6 ConclusionsandLimitations
Inthiswork,wepresentMuDI,anovelpersonalizingframeworkformultiplesubjectsthataddresses
identitymixing. WeleveragesegmentedsubjectsautomaticallyobtainedfromtheSegmentAnything
Modelforbothtrainingandinferencethroughdataaugmentationfortrainingpre-trainedmodels
andinitializingthegenerationprocess. Weexperimentallyvalidateourapproachonanewdataset
comprisingcombinationsofsubjectspronetoidentitymixing,forwhichourssuccessfullyprevents
mixing even for highly similar subjects. We hope that our work can serve as a starting point to
developpersonalizingmethodsformultipleconceptsinmorechallengingscenarios.
LimitationsandFutureWork Wefindthatdecouplingtheidentitiesofremarkablyalikesubjects
isstillchallengingevenforourmethod,forexample,twobrownteddybearsintheproposeddataset.
Wesuspectthisisbecausesuchsubjectsareverycloseintheimagelatentspacewhichisdifficult
to separate with the current models. Furthermore, although our framework effectively alleviates
identitymixingforseveralsubjects,wenoticethatsubjectdominancebecomesstronger,whichwe
provideexamplesinFigure27. Lastly,weobservethatourmethodfacesdifficultieswhenthegiven
promptiscomplex,whichcouldbeimprovedbyoptimizingtothespecificprompt[2]. Webelieve
oursupervisedfine-tuningframeworkmaypotentiallyaddresstheselimitationsandcanbefurther
extendedbyapplyingrecentRLHFapproaches[24,11,6,48,10].
7 Acknowledgements
WethankJuyongLee,andJaewooLeeforprovidingvaluablefeedback.
ThisworkwassupportedbyInstituteforInformation&communicationsTechnologyPromotion(IITP)
grantfundedbytheKoreagovernment(MSIP)(No.2019-0-00075ArtificialIntelligenceGraduate
SchoolProgram(KAIST)).
References
[1] YuvalAlaluf,EladRichardson,GalMetzer,andDanielCohen-Or. Aneuralspace-timerepre-
sentationfortext-to-imagepersonalization. AssociationforComputingMachineryTransactions
onGraphics,42(6):243:1–243:10,2023.
[2] MoabArar,AndreyVoynov,AmirHertz,OmriAvrahami,ShlomiFruchter,YaelPritch,Daniel
Cohen-Or,andArielShamir. Palp: Promptalignedpersonalizationoftext-to-imagemodels.
arXiv:2401.06105,2024.
[3] OmriAvrahami,KfirAberman,OhadFried,DanielCohen-Or,andDaniLischinski. Break-a-
scene: Extractingmultipleconceptsfromasingleimage. InSIGGRAPHAsia,2023.
[4] YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat,JiamingSong,KarstenKreis,Miika
Aittala,TimoAila,SamuliLaine,BryanCatanzaro,TeroKarras,andMing-YuLiu. ediff-i:
Text-to-imagediffusionmodelswithanensembleofexpertdenoisers. arXiv:2211.01324,2022.
[5] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang,
JuntangZhuang,JoyceLee,YufeiGuo,etal. Improvingimagegenerationwithbettercaptions.
ComputerScience.https://cdn.openai.com/papers/dall-e-3.pdf,2023.
10[6] KevinBlack,MichaelJanner,YilunDu,IlyaKostrikov,andSergeyLevine. Trainingdiffusion
modelswithreinforcementlearning. arXiv:2305.13301,2023.
[7] DaewonChae,NokyungPark,JinkyuKim,andKiminLee.Instructbooth:Instruction-following
personalizedtext-to-imagegeneration. arXiv:2312.03011,2023.
[8] XiChen,LianghuaHuang,YuLiu,YujunShen,DeliZhao,andHengshuangZhao. Anydoor:
Zero-shotobject-levelimagecustomization. arXiv:2307.09481,2023.
[9] JaeminCho,AbhayZala,andMohitBansal. Visualprogrammingforstep-by-steptext-to-image
generationandevaluation. InAdvancesinNeuralInformationProcessingSystems,2023.
[10] KevinClark, PaulVicol, KevinSwersky, andDavidJ.Fleet. Directlyfine-tuningdiffusion
modelsondifferentiablerewards. InInternationalConferenceonLearningRepresentations,
2024.
[11] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter
Abbeel,MohammadGhavamzadeh,KangwookLee,andKiminLee. Reinforcementlearning
forfine-tuningtext-to-imagediffusionmodels. InAdvancesinNeuralInformationProcessing
Systems,2023.
[12] StephanieFu,NetanelTamir,ShobhitaSundaram,LucyChai,RichardZhang,TaliDekel,and
PhillipIsola. Dreamsim: Learningnewdimensionsofhumanvisualsimilarityusingsynthetic
data. InAdvancesinNeuralInformationProcessingSystems,2023.
[13] RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHaimBermano,GalChechik,and
DanielCohen-Or. Animageisworthoneword: Personalizingtext-to-imagegenerationusing
textualinversion. InInternationalConferenceonLearningRepresentations,2023.
[14] YuchaoGu,XintaoWang,JayZhangjieWu,YujunShi,YunpengChen,ZihanFan,Wuyou
Xiao,RuiZhao,ShuningChang,WeijiaWu,YixiaoGe,YingShan,andMikeZhengShou.
Mix-of-show: Decentralizedlow-rankadaptationformulti-conceptcustomizationofdiffusion
models. InAdvancesinNeuralInformationProcessingSystems,2023.
[15] LigongHan,YinxiaoLi,HanZhang,PeymanMilanfar,DimitrisN.Metaxas,andFengYang.
Svdiff: Compactparameterspacefordiffusionfine-tuning. InInternationalConferenceon
ComputerVision,2023.
[16] AmirHertz,RonMokady,JayTenenbaum,KfirAberman,YaelPritch,andDanielCohen-Or.
Prompt-to-promptimageeditingwithcross-attentioncontrol. InInternationalConferenceon
LearningRepresentations,2023.
[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In
AdvancesinNeuralInformationProcessingSystems,2020.
[18] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In
InternationalConferenceonLearningRepresentations,2022.
[19] SitengHuang,BiaoGong,YutongFeng,XiChen,YuqianFu,YuLiu,andDonglinWang.Learn-
ingdisentangledidentifiersforaction-customizedtext-to-imagegeneration. arXiv:2311.15841,
2023.
[20] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C. K. Chan, and Ziwei Liu. Reversion:
Diffusion-basedrelationinversionfromimages. arXiv:2303.13495,2023.
[21] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International
ConferenceonLearningRepresentations,2014.
[22] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloéRolland,LauraGustafson,
Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross B.
Girshick. Segmentanything. InInternationalConferenceonComputerVision,2023.
11[23] NupurKumari, BingliangZhang, RichardZhang, EliShechtman, andJun-YanZhu. Multi-
concept customization of text-to-image diffusion. In Conference on Computer Vision and
PatternRecognition,2023.
[24] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter
Abbeel,MohammadGhavamzadeh,andShixiangShaneGu. Aligningtext-to-imagemodels
usinghumanfeedback. arXiv.2302.12192,2023.
[25] ZhihengLiu,RuiliFeng,KaiZhu,YifeiZhang,KechengZheng,YuLiu,DeliZhao,Jingren
Zhou,andYangCao. Cones: Conceptneuronsindiffusionmodelsforcustomizedgeneration.
InInternationalConferenceonMachineLearning,2023.
[26] ZhihengLiu,YifeiZhang, YujunShen,KechengZheng,KaiZhu,RuiliFeng,YuLiu,Deli
Zhao, JingrenZhou, andYangCao. Cones2: Customizableimagesynthesiswithmultiple
subjects. arXiv:2305.19327,2023.
[27] JiafengMao,XuetingWang,andKiyoharuAizawa. Guidedimagesynthesisviainitialimage
editingindiffusionmodel. InAssociationforComputingMachineryInternationalConference
onMultimedia,2023.
[28] MatthiasMinderer,AlexeyA.Gritsenko,andNeilHoulsby. Scalingopen-vocabularyobject
detection. InAdvancesinNeuralInformationProcessingSystems,2023.
[29] OpenAi. Chatgpt,2023. URLhttps://chat.openai.com/.
[30] OpenAi. Sora: Videogenerationmodelsasworldsimulators,2024. URLhttps://openai.
com/sora.
[31] OpenAi. Gpt-4v(ision)technicalworkandauthors,2024. URLhttps://openai.com/
contributions/gpt-4v.
[32] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,
PierreFernandez, DanielHaziza, FranciscoMassa, AlaaeldinEl-Nouby, MahmoudAssran,
NicolasBallas,WojciechGaluba,RussellHowes,Po-YaoHuang,Shang-WenLi,IshanMisra,
MichaelG.Rabbat,VasuSharma,GabrielSynnaeve,HuXu,HervéJégou,JulienMairal,Patrick
Labatut,ArmandJoulin,andPiotrBojanowski. Dinov2:Learningrobustvisualfeatureswithout
supervision. arXiv:2304.07193,2023.
[33] RyanPo, GuandaoYang, KfirAberman, andGordonWetzstein. Orthogonaladaptationfor
modularcustomizationofdiffusionmodels. arXiv:2312.02432,2023.
[34] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,JonasMüller,Joe
Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXiv:2307.01952,2023.
[35] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgar-
wal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlya
Sutskever. Learningtransferablevisualmodelsfromnaturallanguagesupervision. InInterna-
tionalConferenceonMachineLearning,2021.
[36] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen. Hierarchical
text-conditionalimagegenerationwithCLIPlatents. arXiv:2204.06125,2022.
[37] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InConferenceonComputerVisionand
PatternRecognition,2022.
[38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for
biomedicalimagesegmentation. InMedicalImageComputingandComputer-AssistedInter-
vention,2015.
[39] NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman.
Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In
ConferenceonComputerVisionandPatternRecognition,2023.
12[40] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyL.Denton,Seyed
Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
JonathanHo,DavidJ.Fleet,andMohammadNorouzi. Photorealistictext-to-imagediffusion
models with deep language understanding. In Advances in Neural Information Processing
Systems,2022.
[41] DvirSamuel,RamiBen-Ari,SimonRaviv,NirDarshan,andGalChechik. Generatingimages
ofrareconceptsusingpre-traineddiffusionmodels. InAssociationfortheAdvancementof
ArtificialIntelligence,2024.
[42] Kihyuk Sohn, Lu Jiang, Jarred Barber, Kimin Lee, Nataniel Ruiz, Dilip Krishnan, Huiwen
Chang,YuanzhenLi,IrfanEssa,MichaelRubinstein,YuanHao,GlennEntis,IrinaBlok,and
DanielCastroChin. Styledrop: Text-to-imagesynthesisofanystyle. InAdvancesinNeural
InformationProcessingSystems,2023.
[43] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon,
andBenPoole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. In
InternationalConferenceonLearningRepresentations,2021.
[44] Luming Tang, Nataniel Ruiz, Qinghao Chu, Yuanzhen Li, Aleksander Holynski, David E.
Jacobs,BharathHariharan,YaelPritch,NealWadhwa,KfirAberman,andMichaelRubinstein.
Realfill: Reference-drivengenerationforauthenticimagecompletion. arXiv:2309.16668,2023.
[45] YoadTewel, RinonGal, GalChechik, andYuvalAtzmon. Key-lockedrankoneeditingfor
text-to-imagepersonalization. InErikBrunvand,AllaSheffer,andMichaelWimmer,editors,
Association for Computing Machinery Special Interest Group on Computer Graphics and
InteractiveTechniques,2023.
[46] Hazarapet Tunanyan, Dejia Xu, Shant Navasardyan, Zhangyang Wang, and Humphrey Shi.
Multi-conceptt2i-zero:Tweakingonlythetextembeddingsandnothingelse.arXiv:2310.07419,
2023.
[47] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. P+: extended textual
conditioningintext-to-imagegeneration. arXiv:2303.09522,2023.
[48] BramWallace,MeihuaDang,RafaelRafailov,LinqiZhou,AaronLou,SenthilPurushwalkam,
StefanoErmon,CaimingXiong,ShafiqJoty,andNikhilNaik. Diffusionmodelalignmentusing
directpreferenceoptimization. arXiv:2311.12908,2023.
[49] YuxiangWei,YaboZhang,ZhilongJi,JinfengBai,LeiZhang,andWangmengZuo. ELITE:
encodingvisualconceptsintotextualembeddingsforcustomizedtext-to-imagegeneration. In
InternationalConferenceonComputerVision,2023.
[50] GuangxuanXiao,TianweiYin,WilliamTFreeman,FrédoDurand,andSongHan. Fastcom-
poser: Tuning-freemulti-subjectimagegenerationwithlocalizedattention. arXiv:2305.10431,
2023.
[51] JiazhengXu,XiaoLiu,YuchenWu,YuxuanTong,QinkaiLi,MingDing,JieTang,andYuxiao
Dong. Imagereward: Learningandevaluatinghumanpreferencesfortext-to-imagegeneration.
InAdvancesinNeuralInformationProcessingSystems,2023.
[52] SangdooYun,DongyoonHan,SanghyukChun,SeongJoonOh,YoungjoonYoo,andJunsuk
Choe. Cutmix: Regularizationstrategytotrainstrongclassifierswithlocalizablefeatures. In
InternationalConferenceonComputerVision,2019.
[53] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-image
diffusionmodels. InInternationalConferenceonComputerVision,2023.
13Appendix
Organization The Appendix is organized as follows: In Section A, we describe the details of
our framework and provide the details of the experiments. In Section B, we provide additional
experimental results. In Section C, we provide additional analysis on MuDI. Lastly, we provide
furtherexperimentsonapplicationsofMuDIinSectionD.
A ExperimentalDetails
A.1 ImplementationDetails
Prior-Preservation Dataset We construct the prior dataset D by generating from the pre-
ref
trained text-to-image models using a prompt "A photo of <class>, simple background, full body
shot.". We use a descriptive class used for training serving as the prior class. We automatically
createsegmentationmasksfortheimagesinthepriordatasetusingOWLv2[28]andSAM[22]as
illustratedinFigure3,similartosegmentingthereferenceimagesdescribedinSection4.1. Weselect
thepriorimagesthatcontainasinglesubjectthatresultsinasinglesegmentationmask.
Training Details In our experiment, we determine the training iterations of Seg-Mix on each
combinationinthedatasetbasedonthedifficultyofpersonalizingthesubjectsindividuallyusing
DreamBooth.Forexample,combinationsincludinghighlydetailedsubjects,suchas"can"inFigure4,
requirefrom1400to1600trainingiterations,whilecombinationscontainingsubjectseasytolearn,
e.g.,"dog,"requireabout1200iterations. Weuseafixedaugmentationprobabilityof0.3forboth
Cut-MixandourSeg-Mixwithasamenumberoftrainingiterationsforafaircomparison. Toprevent
subjectoverfitting,weuse1000trainingiterationsforDreamBooth.
MuDIImplementationDetails Algorithm1providesadetailedimplementationofourSeg-Mix.
Forrandomcomposition,werandomlyrescalethesegmentedsubjectsandrandomlychoosewhich
subjectispastedontheleft. Additionally,werandomlysetamarginfrombothends,withlarger
marginsallowforthepossibilityofsubjectsoverlapping. Wealsoprovidesadetailedimplementation
ofourinitializationinAlgorithm2.
Algorithm1MuDItraining(Seg-Mix)
# imgs[0], imgs[1]: reference images
# masks[0], masks[1]: masks
# max_m: max margin from both ends of the image (if large, allow overlap)
# scales: control the relative size or random, else random resizing
def create_seg_mix(imgs, masks, out_size=(1024,1024), max_m=1, scales=None):
# randomly(or relative) resize each image & mask pair
imgs, masks = resize(imgs, masks, out_size, scales)
out, out_mask = np.zeros((*out_size, 3)), np.zeros(out_size) # blank image, mask
if random.random() < 0.5: # random order
imgs, masks = imgs[::-1], masks[::-1]
# random margin from ends of the image
m = [random.randint(0, max_margin) _ for in range(2)]
out, out_mask = paste_left(out, out_mask, imgs[0] * masks[0], m[0])
out, out_mask = paste_right(out, out_mask, imgs[1] * masks[1], m[1])
return out, out_mask
def train_loss(seg_mix_prob=0.3, **kwargs):
img, mask, class, prompt = dataloader.next()
if random.random() < seg_mix_prob: # do augmentation
# sample another class for seg-mix
new_class = random.choice(class_list - class)
new_img, new_mask = sample_img_mask(new_class)
imgs, masks = [img, new_img], [mask, new_mask]
img, mask = create_seg_mix(imgs, masks, **kwargs)
# DreamBooth training (Eq. 1)
loss = LDM_loss(img, prompt)
return loss.mean()
14Algorithm2MuDIinference(Initialization)
# class_list: classes in prompt
# gamma: guidance strength of our initialization
# kwargs: same arguments from Algorithm 1
def latent_initialize(class_list, gamma=1.0, **kwargs):
# sample reference images
imgs, masks = zip(*[sample_img_mask(cls) for cls in class_list])
out, mask = create_seg_mix(imgs, masks, **kwargs)
# encode image to latent
out_latent = encoder(out)
# resize to latent size
out_mask = resize(out_mask)
# segmented latent
init_latent = out_mask * out_latent
init_latent = init_latent * gamma + torch.rand_like(init_latent)
return init_latent
def inference(prompt, class_list, gamma=1.0, **kwargs):
# execute computation only once
init_latent = latent_initialize(class_list, gamma, **kwargs)
# existing inference pipeline
img = inference_pipe(prompt, init_latent=init_latent, **kwargs)
return img
InferenceusingNegativePrompt WhileHanetal.[15]proposetousenegativepromptstoreduce
stitchingartifacts,weobservethatthisproducesover-saturatedsamplesasshowninthetoprowof
Figure8andresultsinlow-qualityimages. Therefore,weoptnottousethenegativepromptsinour
evaluation. Forourframework,weuseasimplenegativeprompt"sticker,collage."thatalleviates
sticker-likeartifacts,whichisduetotrainingwiththesegment-and-mixedimages.
ev
ita
g
eN
/w
ev
ita
g
eN
o
/w
Figure8: Cut-Mixwithandwithoutnegativeprompt. Forourexperiments,weobservethatusing
thenegativeprompt"Adogandadog"leadstoreducedartifactsbutresultsinover-saturationas
showninthefirstrow.
A.2 Dataset
We introduce a new dataset to facilitate evaluation for multi-subject personalization, comprising
8 combinations of similar subjects prone to identity mixing. We visualize the subjects and the
identity-mixedsamplesfromDreamBoothinFigure9. Wegenerate5evaluationpromptsforeach
combinationusingChatGPT[29]thatdescribesasceneofthesubjectswithasimpleaction.
15Monster toy & Can Cat & Dog Teddy bear & Robot toy
0.166 “… floating on the pool.” 0.329 “… floating on the pool.” 0.360 “… floating on the pool.”
Monster toy & Robot toy Dog & Action figure Castle & Lighthouse
0.284 “… floating on the pool.” 0.148 “… floating on the pool.” 0.309 “… some trees are in background.”
Teddy bear & Teddy bear Dog & Dog
0.532 “… floating on the pool.” 0.415 “… floating on the pool.”
Figure9:Dataset. Weintroduceanewdatasetcomprisingeightcombinationsofsimilarsubjects.For
eachcombination,wevisualizeoneimagepersubjectandthreeimagesgeneratedbyDreamBooth,
andthescorebelowthesubjectsdenotestheDreamSim[12]similarityscorebetween0and1wherea
largervalueindicateshighersimilarity. Thebottom-mosttwocombinations,namelytwoteddybears
andtwodogs,arethosewiththehighestsimilaritymakingthemthemostchallengingcombinations.
References Mixed Identities
[[0.72, 0.29], Metrics Spearman AUROC
“monster toy” [0.52, 0.40]]
“robot toy” = 0.43 - 0.12 DS[12] 0.179 0.606
= 0.31
DINOv2[32] 0.099 0.559
Reference D&C-DS 0.455 0.764
OWLv2 DreamSim Algorithm 3
Similarities D&C-DINO 0.414 0.741
[[0.72, 0.28], = 0.44 + 0.44
[0.27, 0.71]] = 0.88
Figure 10: (Left) Overview of Detect-and-Compare. We measure the multi-subject fidelity by
detectingthesubjectwithOWLv2[28]andcomputingthesimilarities. (Right)Correlationbetween
metricsandhumanevaluation. WereporttheSpearman’srankcorrelationcoefficientandAUROC.
Algorithm3Detect&Compare(D&C)
Input:SimilaritymeasuremodelD,Referencesubjects(X,Y),Boxes(B ,B )
1 2
Output:Linearscore(0∼2,butitlargelydependsonsimilaritybetweenreferences)
1: (sX,sY)=D(B ,X,Y) ▷Meansimilaritymatrix;sX meanshowsimilarB andXare
1 1 1 1 1
2: (sX,sY)=D(B ,X,Y)
2 2 2
3: conf =sX −sY ▷Categoricalconfidence;ifpositive,predictobjectinB asaX
1 1 1 1
4: conf =sX −sY
2 2 2
5: ifconf ∗conf >0then ▷Samecategory;IdentitymixingorSubjectdominance
1 2
6: returnabs(conf −conf )
1 2
7: else ▷Differentcategory
8: returnabs(conf )+abs(conf )
1 2
9: endif
16Multi-SubjectFidelity
Method D&C-DS D&C-DS(Max) D&C-DINO D&C-DINO(Max) HumanEval.
TextualInversion[13] 0.021 0.213 0.019 0.211 0.3%
DreamBooth[39] 0.147 0.696 0.105 0.627 9.5%
Cut-Mix[15] 0.237 0.800 0.181 0.703 35.7%
MuDI(Ours) 0.434 0.998 0.293 0.857 70.3%
Table2: Quantitativeresultsonmulti-subjectfidelitybasedonD&Cvariations. D&C-DS(Max)
and D&C-DINO (Max) denote variants of our D&C-DS and D&C-DINO, respectively that uses
maximumreferencesimilaritiesinsteadofthedifference.
A.3 Detect-and-Compare
AsbrieflyintroducedinSection5.1,ourDetect-and-Compare(D&C)metricemploysatwo-step
process to evaluate multi-subject fidelity. Specifically, we first utilize OWLv2 [28] to detect the
subjects in the generated image with text query given as the super categories of the subjects, for
example,"monstertoy"or"robottoy"inFigure10. Wethenmeasurethesimilaritiesbetweeneach
detectedsubjectandthereferenceimagesusingsingle-subjectfidelitymetricssuchasDreamSim[12]
orDINOv2[32]. Wecomputethesimilaritybetweenthedetectedsubjectandthereferencesubject,
namelyreferencesimilarity,bytakingtheaverageofthesimilaritiestoimagesofeachreference
subject. ExamplesfothesereferencesimilaritiescanbeobservedinthecenterofFigure10,meaning
thecomputedsimilaritiesforeachdetectedsubjectwithrespecttoeachreferencesubject.
However, a subject with mixed identities may yield high reference similarity for more than one
subject. Toaccountforidentitymixing,weusethedifferenceinthereferencesimilarities,wherea
largedifferenceinreferencesimilaritiesindicatesbothahighsimilaritytothereferencesubjectanda
lowidentitymixing. Fromthereferencesimilarities,weobtainthemeasureofmulti-subjectfidelity
usingAlgorithm3. Forinstance,theleftsampleinFigure10demonstratesthateachdetectedsubject,
whichisnotidentitymixed,showshighsimilaritytoonlyonereferencesubject,resultinginahigh
D&Cscore. However,inthemixedidentitiessampleofFigure10,bothdetectedsubjectsfallunder
thesamecategory,leadingtoalowerD&Cscore. Additionally,thesubjectintheblueboxlookslike
amixoftworeferencesubjects,resultinginadecreaseddifferenceinreferencesimilarities.
TovalidatethatourD&Ciscapableofmeasuringmulti-subjectfidelity,wecompareD&Cmetrics
againstprevioussingle-subjectfidelitymetricsextendedtomulti-subjectsettingswhichcompute
themeanofsimilaritiestoallthereferenceimages[25]. Weassessthecorrelationbetweenhuman
evaluationsonmulti-subjectfidelityandeachmetricthroughSpearman’srankcorrelationcoefficient
andtheAreaundertheReceiverOperatingCharacteristic(AUROC).Ourfindingsinthetableof
Figure 10 reveal that our D&C with DreamSim (D&C-DS) exhibits the highest correlation with
humanevaluation. WeadditionallyprovideaquantitativecomparisonofourD&Canditsvariants,
whichusesmaximumreferencesimilaritiesinsteadofthedifference,inTable2andaqualitative
comparisoninFigure11,demonstratingthatD&Ciscapableofmeasuringidentitymixingandis
consistentwiththehumanevaluation.Inparticular,thealignmentwiththehumanevaluationindicates
thatD&Cmetricscanbeusedasarewardfunctionforreinforcementlearningwithhumanfeedback
(RLHF),whichwedemonstrateinoursupervisedfine-tuningframeworkofSection5.5.
A.4 HumanEvaluationDetails
Ourhumanevaluationwasconductedintwomainaspects: (1)multi-subjectfidelityand(2)overall
preference. Formulti-subjectfidelity,arandomsubsetcontaininganequalnumberofinstancesfrom
eachmethodwascreatedandprovidedtohumanratersforbinaryfeedback. Foroverallpreference,
imagesfrombothCut-Mixandourswereprovidedinrandomorder,withallimagesgeneratedfrom
thesameseed. AscreenshotofourhumanevaluationisprovidedinFigure12.
A.5 AdditionalGeneratedExamples
Weprovideadditionalnon-curatedgeneratedexamplesinFigure13.
17(a) D&C-DS
(b) D&C-DS (Max)
Figure11: QualitativecomparisonofD&Canditsvariant. Wesort24imagesbasedonD&Cand
itsvariant,wheretheimagesarerandomlychosenfromthesetofgeneratedimagesbyourMuDI,
Theyellowboxesdenoteidentity-mixedsamplesfromhumanevaluation. WefindthatourD&C
morealignswithhumanevaluationcomparedtoitsvariantthatusesmaximumreferencesimilarity.
(a) Multi-Subject Fidelity (b) Overall Preference
Figure12:Exampleofourhumanevaluationon(a)multi-subjectfidelity2and(b)overallpreference3.
2Weprovidethelabelinginstructionforthemulti-subjectfidelityinlink.
3Weprovidethelabelinginstructionfortheoverallpreferenceinlink.
18Refs MuDI (Ours) DreamBooth Cut-Mix Textual Inversion
“… at the beach under a bright sun.”
“… playing in a garden full of flowers.”
“… sitting on a stone bench.”
“… having a tea party with table full of desserts in a colorful room.”
“… on a snowy path in a quiet forest.”
“…, people watching fireworks.”
“… in a jungle..”
“…walking in a bustling city street corner.”
Figure13:Qualitativecomparisonofimagesgeneratedbyours,Cut-Mix,DreamBooth,andTextual
Inversion. Wevisualizenon-curatedimagesgeneratedwiththesamerandomseed.
19sfe
R
h
to
o
B
m
a
er
D
n
o
isu
ffiD
m
o
tsu
C
sr)
u
O
ID(
u
M
Figure14:QualitativecomparisonofimagesgeneratedbyDreamBooth[39],CustomDiffusion[23],
andours. Bothmethodsresultinidentitymixing..
Multi-SubjectFidelity TextFidelity
Method D&C-DS↑ D&C-DINO↑ ImageReward†↑ CLIPs†↑
TextualInversion[13] 0.021 0.019 -0.149 0.227
DreamBooth[39] 0.147 0.105 0.579 0.255
CustomDiffusion[23] 0.105 0.078 0.144 0.243
Cut-Mix[15] 0.237 0.181 -0.287 0.225
MuDI(Ours) 0.434 0.293 0.770 0.263
Table3: Quantitativeresultsonmulti-subjectfidelityandtextfidelity. †denotesthetextfidelity
scoreconsideringthepermutationofthesubjectsintheprompttoavoidpositionbias.
B AdditionalExperiments
B.1 CustomDiffusion
Here, we provide the results of Custom Diffusion [23] using SDXL [34] as the pre-trained text-
to-image diffusion model, for which we fine-tune the weights of LoRA [18] instead of directly
fine-tuningthemodelweightsduetoGPUconstraints. Weevaluatetwodifferentmodels,onethat
usesahighrank(i.e.,rank128)andtheotherthatusesthesamerankasours(i.e.,rank32). Yetwe
donotobservesignificantdifferencesbetweenthem.
AsshowninFigure14,CustomDiffusiondemonstratesdegradationinthesubjectfidelitycompared
toDreamBooth[39]. ThequantitativeresultsinTable3similarlyshowthatCustomDiffusionresults
inlowermulti-subjectfidelityaswellaslowertextfidelitycomparedtoDreamBoothandMuDI.
Therefore,weexcludeCustomDiffusionfromourbaselineinthemainexperiments.
B.2 Cross-attentionMapsofSDXL
Cross-attentionmapshavebeenwidelyusedinpriorworksrelatedtoimageediting[16], layout-
guidedgeneration[26,14],andsingle-subjectpersonalization[49,3,50],duetotheircontrollability
ontherelationbetweenthespatiallayoutsandthewordsintheprompt[16].
20Generated Image Generated Image
“a furry bear
“A furry bear “A olis monster toy
watches a bird.” reading a book.”
Identifier token: “olis”
watches a bird.
(a) Average cross-attention maps (b) Cross-attention map per each block
Figure15: Visualizationofcross-attentionmapsinSDXL.(a)Averagecross-attentionmaps.
Thetokenforthebeardemonstratesahighvalueintheregioncorrespondingtothebird,andthe
tokenforthebirdtakesahighvalueinanirrelevantlocation(rightbottom). Notethatthisfigure
canbecomparedtoFigure4ofPrompt-to-Prompt[16]. (b)Cross-attentionmappereachblock.
Thecross-attentionmapsoftheidentifiertoken(e.g.,olis)donotshowconsistentresultswiththe
correspondingsubject(i.e.,monstertoyinthisexample). Wehighlightthemapswithblackrectangles
thathavelowvaluesforthesubjectcomparedtothesubject-irrelevantregions.
Refs
(a) DreamBooth (b) + Seg-Mix (c) MuDI (Ours)
Figure16: ExperimentwithStableDiffusionv1.5[37]asapre-trainedtext-to-imagemodel.
WhileDreamBooth[39]resultsinidentitymixing,ourSeg-Mixpreventsidentitymixingbutoften
ignoresasubject. LeveragingbothSeg-Mixandourinitializationcansuccessfullypersonalizeboth
subjectsdistinctlywithoutidentitymixingorsubjectdominance. Theimagesofthesamepositions
inthe3×3gridaregeneratedusingthesamerandomseed.
Whilethecross-attentionmapsworkedsuccessfullyonStableDiffusion(SD)[37],thearchitectural
design of Stable Diffusion XL (SDXL) [34], where an additional text condition is added to the
timeembedding[4], significantlyreducestheconsistencyofthecross-attentionmaps, whichwe
demonstrateinFigure15. Therefore,previousapproachesbasedonthecross-attentionmapsarenot
applicablewhenusingSDXLasapre-trainedtext-to-imagediffusionmodel.
B.3 UnmixRegularization
Hanetal.[15]proposesUnmixregularization,atechniquethatusesMSEonnon-corresponding
regionsofthecross-attentionmaps,forreducingstitchingartifacts. However,weobservethatUnmix
regularizationisnotapplicablewhenusingSDXLasapre-trainedtext-to-imagemodel. Webelieve
thisisbecauseofthearchitecturalcharacteristicsofSDXLasexplainedinSectionB.2.
C AnalysisonMuDI
C.1 ExperimentwithStableDiffusionv1.5
InFigure16,weprovideaqualitativecomparisonofthegeneratedimagesofDreamBooth,MuDI
withoutinitialization,andMuDIthatuseStableDiffusionv1.5[37]asthepre-trainedtext-to-image
diffusion model. Similar to the case when using SDXL as the pre-trained model, DreamBooth
21Initial Latent Initial Latent
Seg-Mix Seg-Mix
Ours Ours
w/o subject overlap w/o subject overlap
Figure17: AblationstudyonSeg-Mixwithoutsubjectoverlap. WhileSeg-Mixwithoutsubject
overlapresultsinidentitymixingandsubjectdominance,Seg-Mixallowingtheoverlapsuccessfully
generatesdistinctsubjectswithoutmixing.
Reference Images
ControlNet – Edge map (a) DreamBooth (b) Cut-Mix
Figure18: ControlNet[53]withexistingmethods[39,15]. Evenwithstrongspatialguidancesuch
ascannyedge,existingmethodssufferfromidentitymixing.
generates identity-mixed subjects. While Seg-Mix prevents identity mixing, it often results in
ignoringcertainsubjects. OurMuDIsuccessfullypersonalizesthesubjectswithoutidentitymixing
orsubjectdominance.
C.2 SubjectOverlapinSeg-Mix
DuringSeg-Mix,werandomlypositionthesegmentedsubjectswhichallowsthesubjectstooverlap,
incontrasttoCut-Mix[15]whichisrestrictedtocreatingimagesofnon-overlappedsubjects. Here,
we show that training with images of overlapped subjects provides a significant advantage for
generatinginteractionbetweenthesubjects,especiallywhenusedwithourinitialization. Asshown
inFigure17,Seg-Mixwithoutoverlappedsubjectsresultsinidentitymixingforneighboringsubjects
(e.g., monster toy in the can) and subject dominance (e.g., showing only Chow Chow), whereas
Seg-Mix that allows overlap successfully personalizes the subjects and their interaction without
identitymixing.
C.3 ComposingMultipleSubjectswithControlNet
Here,weshowthatControlNet[53]failstoaddressidentitymixingforgeneratingmultiplesubjects.
AsshowninFigure18,bothDreamBooth[39]andCut-Mix[15]resultsinidentity-mixedsubjects
evenwiththespatialguidanceofControlNet. Wefurtherobservethattheregionalprompting[14]
doesnotshowimprovementwhenusingSDXLasthepre-trainedmodelduetoouranalysisinSection
B.2.
22(a) Initial Latent (b) Results with Our Initialization
Figure19: Diversityofimagesgeneratedbyourinitialization. Ourinitializationdoesnotfixthe
postureordetailsofthegeneratedsubject. Latentandimageslocatedatthesamepositionineach
3×3gridarepaired,andalltheimagesaregeneratedfromthesamerandomseed. Thepromptused
forthegenerationis"... havingateapartywithtablefullofdessertsinacolorfulroom."
(a) Unusual Concepts (b) More than Two Subjects (c) Complex Prompts
Figure20: Importanceofourinitializationonthreescenarios: (a)unusualsubjectssuchascloud
man, (b) more than two subjects, and (c) complex prompts like "... as astronaut, floating on
themoon,crater,spaceshuttle...". Eachleftimagewasgeneratedwithoutourinitialization,while
therightimagewasgeneratedusingourinitialization. Bothimageswereproducedusingthesame
Seg-Mix-trainedmodelandthesamerandomseedforeachpair.
C.4 DiversityofImagesGeneratedbyOurInitialization
In Figure 19, we demonstrate that our initialization can generate subjects with diverse postures
anddetails,allowingforanaturalintegrationwiththepromptandscene,andevenenablingglobal
interactions. Thisfeaturesetsourmethodapartfromtraditionalinpaintingorediting-basedworks,
offeringauniqueapproachtospatialmodulation.
C.5 NecessityofOurInitializationforChallengingSubjects
AsdiscussedinSection4.2,ourinitializationapproachalsoaddressestheissueofsubjectdominance,
whichmeanswhenonlyonetypeofsubjectisgeneratedinmulti-subjectgeneration. OurSeg-Mix-
trainedmodelcangenerateindividualconceptsandoccasionally,whentwosubjectsaregenerated,
theiridentitiesarenotmixed. However,subjectdominanceposesasignificantchallenge,particularly
incombinationsincludingchallengingconcepts(suchasthecloudmanandalienexampleshown
inFigure1,scenariosinvolvingmorethantwosubjects,orthosethatincludecomplexprompts. In
theseinstances,multi-subjectgenerationoftenfails. Ourinitializationeffectivelyaddressesthisissue,
asdemonstratedinFigure20.
23User:
“A monster toy
sitting on drink can.”
LLM
Randomly
Bot: “Monster toy”:
Composed
(300, 700, 300, 700)
“Drink can”:
(300, 700, 700, 900)
Initialize Latent Initialize Latent
“A monster toy sitting on drink can.” “A monster toy sitting on drink can.”
(a) Our Initialization (Random) (b) LLM-guided Initialization
Figure21: ExamplesofLLM-guidedinitializationforinteractions. Latentandimageslocatedat
thesamepositionineachgridarepaired,andwereallgeneratedfromthesamerandomseed. (a)Our
Initialization(Random). Forpromptsthatincludecomplexinteractions,suchas"monstertoysitting
ondrinkcan,"Ourinitializationcansometimesproduceresultsthatinterferewiththeinteraction.
(b)LLM-guidedinitialization. LLMseasilygeneratethetext-alignedboundingboxeswithgiven
prompt. ByutilizingLLM-guidedlayout,itcanalsogeneratecomplexsituationslike"sittingon".
D Applications
D.1 LLM-GuidedInitializationforInteractions
Whileourinitializationiseffectivefordecouplingtheidentitiesbyprovidinghintsaboutthecoarse
layoutofthesubjectduringtheinferencestage,therandomlayoutsmightnotfittheinteractionsof
thesubjectsdescribedintheprompts. Forexample,ifsegmentedsubjectsarerandomlycomposed
farapartwithourinitialization,thismaynotbeappropriateforapromptthatdescribesthesubjects
tobeclosetoeachother. Thiscanbeaddressedbypositioningthesegmentedsubjectstofitthegiven
promptusingLLM[9]. AsshowninFigure21,ourinitializationsometimeshindertext-alignment,
especiallyinteraction(sitting),LLM-guidedlayoutcompositioncanresolveit.
D.2 IterativeTrainingwithSelf-Generation
Our Detect-and-Compare has successfully measured multi-subject fidelity in generated images,
showingresultscloselyalignedwiththosefrommulti-subjectfidelityhumanevaluation. Therefore,
weexperimentedwithleveragingitasarewardmodeltoenhancemulti-subjectperformancethrough
additional model training, achieving improved outcomes. We also provide simple overview in
Figure23. Forasimpledemonstrationofourpipeline,weapplyiterativetraining(IT)method[42]
ratherthanmorecomplexmethodslikeDDPO[6]orDPO[48].
Inourexperiment,weinitiatedwithmodelstrainedviaSeg-Mixwithaslightlylowernumberof
training iterations than usual to avoid overfitting. To create the training dataset, we used simple
sentencesgeneratedbyChatGPT[29],forexample,"[V1]dogand[V2]dogwatchingbirdsfrom
a window sill." We generated 200 images and selected the top 50 based on our D&C score for
iterative training, maintaining the training for approximately 1000 iterations with LDM loss. In
thisexperiment,settingtheKLregularizationweightaround1.0yieldedagoodtrade-offbetween
preventing over-saturation and multi-subject fidelity. Our iterative training pipeline successfully
generatemulti-subjectimagesevenwithhighlysimilarconceptsinFigure22.
D.3 OtherResults
In this section, additional experiments conducted with our applications are presented. Figure 24
showcasestheoutcomesofimplementingrelativesizecontrolusingSeg-Mix. Anoverviewofthe
modularcustomizationcapabilitiesprovidedbySeg-MixisdetailedinFigure25. Lastly,Figure26
offers a qualitative comparison of images that include three subjects, demonstrating the results
generatedbyvariousmethods.
24(a) Seg-Mix (base) (b) Seg-Mix + IT
(c) Seg-Mix + Initialization (MuDI) (d) MuDI + IT
Figure22: Qualitativecomparisonofouriterativetraining(IT)pipeline. Imageslocatedatthesame
positionineachgridarepaired,andwereallgeneratedfromthesamerandomseed. (a)Seg-Mix
training. Itstillsuffersfromidentitymixingandsubjectdominancewithoutourinitialization. (b)
Withiterativetraining. Withoursimpleiterativetraining,itcangeneratewellunmixedsamples
withoutourinitialization. (c)Withourinitializationmethod. OurMuDIframeworkeffectively
addressesidentitymixingandsubjectdominance,butsometimesfailswithverysimilarconcepts. (d)
Bothwithiterativetrainingandinitialization. Byutilizingbothmethods,wecanachievevery
goodresultsevenforverysimilarconcepts.
25GPT
Self-generated ℒ
𝐾𝐿
dataset 𝝐𝟎
Multi-subject 𝜽
prompts …
Our Initially
Clone
Initialization
𝝐𝟎 D&C 𝝐∗
𝜽 𝜽
Seg-Mix trained Well-generated ℒ 𝐷𝐵
model dataset
Figure23: OverviewofMuDIwithiterativetrainingpipeline. WecanfurtherimproveSeg-Mix
trainedmodelwithwell-generatedsamples.Wefirstgenerateproperpromptsformulti-subjectimages
usingLLMs,followedbythecreationofmultipleimageswithourinitialization. Subsequently,our
D&Cmetricisemployedtoautomaticallyselectwell-generatedsamplesforfurthertraining. Notably,
addingKLregularizationtothetraininglosswithrespecttoSeg-Mixtrainedmodelpreventsimage
qualitydegradation.
[1:1] [0.5:1]
Training Training
samples samples
(a) Size Controlled Seg-Mix (1:1) (b) Size Controlled Seg-Mix (0.5:1)
Figure24: ExamplesofrelativesizecontrolusingSeg-Mix. Tocontroltherelativesizebetween
thesubjects,wecanfixtherelativesizeofthesegmentedsubjectswhencreatingsegment-and-mixed
samplesforSeg-Mix. Whentherelativesizeofthemonstertoyissettobesmallerduringtraining
like(b),theimagesgeneratedduringSeg-Mixalsoshowthedesiredrelativesize.
w/o Seg-Mix
Hugging Face Self-generated Images Seg-Mix Samples
& CivitAI
Download
𝝐𝟏,𝟐
𝝐 𝜽𝟏 𝝐 𝜽𝟏,𝟐 ℒ 𝐾𝐿 “[V1] 𝜽 and [V2],
playing boxing.”
𝝐𝟏,𝟐 Initially with Seg-Mix
… 𝜽 Clone
𝝐∗
𝝐 𝜽𝟐 Gradient Fusion 𝝐∗ 𝜽
𝜽
“[V1] and [V2],
Single
ℒ playing boxing.”
LoRAs 𝐷𝐵
(a) Modular Customization (b) Seg-Mix with KL Reg. (c) Inference
Figure25: OverviewofModularCustomizationwithSeg-Mix. (a)Wefirstgeneratesingle-subject
images using the pre-trained LoRAs, and then merge the LoRAs using gradient fusion [14]. (b)
Weusetheself-generatedimagestocreateSeg-Mixsamplesforfine-tuning. Wefindthatadding
KLregularizationtothetrainingobjectiveeffectivelypreventsoverfittingandsaturation. (c)Our
Seg-Mixfine-tuningsignificantlyimprovesidentitydecouplingwithmodularcustomizationscenario.
26DreamBooth Cut-Mix MuDI (Ours)
References
“... playing in the field full of dandelions, rushing.”
“... playing in the lavender field, rushing.”
Figure26: Qualitativecomparisonofimagesincludingthreesubjectsgeneratedbyours,Dream-
Booth,andCut-Mix. DreamBoothsuffersfromsignificantidentitymixing,andCut-Mixsuffersfrom
issueswithsubjectdominance.
“[V1] riding on red bobsleigh,
[V2] riding on blue bobsleigh…”
(a) Remarkably alike subject (b) Complex prompts (b) More than three subjects
Figure27: Limitations. (a)Remarkablyalikesubject. Whenverycloseinthelatentspace,as
withateddybear,perfectidentitydecouplingisdifficulttoachieveevenwithMuDI.(b)Complex
prompts.Ifthepromptistoocomplex,maintainingidentitybecomesdifficult,orthereisasignificant
occurrenceofidentitydominance. (c)Morethanthreesubjects. Whengeneratingmorethanthree
concepts,thankstoMuDI,identitymixingisreduced,butthereisoftenduplicationofthesametype
ofsubject. Adjustingtheourinitializationweightαcansolvethis,butitmayresultinsaturation.
27