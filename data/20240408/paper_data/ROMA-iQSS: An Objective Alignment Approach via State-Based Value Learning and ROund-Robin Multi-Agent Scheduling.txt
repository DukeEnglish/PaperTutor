ROMA-iQSS: An Objective Alignment Approach via State-Based Value
Learning and ROund-Robin Multi-Agent Scheduling
Chi-Hui Lin, Joewie J. Koh, Alessandro Roncone, Lijun Chen
Abstract—Effective multi-agent collaboration is imperative
for solving complex, distributed problems. In this context,
two key challenges must be addressed: first, autonomously
identifying optimal objectives for collective outcomes; second,
aligningtheseobjectivesamongagents.Traditionalframeworks,
often reliant on centralized learning, struggle with scalability
and efficiency in large multi-agent systems. To overcome these
issues, we introduce a decentralized state-based value learning
algorithmthatenablesagentstoindependentlydiscoveroptimal
states.Furthermore,weintroduceanovelmechanismformulti-
agent interaction, wherein less proficient agents follow and
adopt policies from more experienced ones, thereby indirectly
guiding their learning process. Our theoretical analysis shows Fig. 1. Motivating example: five agents are tasked with a collaborative
that our approach leads decentralized agents to an optimal transport problem where a large object needs to be moved to designated
collective policy. Empirical experiments further demonstrate locations marked by flags. In the case of failed coordination, the left
that our method outperforms existing decentralized state- subgroup(twoagents),strugglestoidentifytheoptimalobjectives,whilethe
rightsubgroup(threeagents),identifiestheobjectivesbutfailstoaligntheir
based and action-based value learning strategies by effectively
efforts effectively, leading to an undesired final location for the object. In
identifying and aligning optimal objectives.
contrast,successfulcoordinationmanifestswhenallagentsnotonlyidentify
the optimal objectives but also achieve precise alignment in their efforts,
I. INTRODUCTION culminatingintheobjectreachingitsintendedlocation.
Reinforcement Learning (RL) enables autonomous agents
to make informed decisions by leveraging past experiences
methods, and iii) it is impractical to maintain continuous,
to anticipate future rewards in a variety of contexts. How-
high-bandwidth communication with a central controller in
ever, despite its notable successes in optimizing the be-
dynamicenvironments,suchasself-drivingvehicles.Collec-
havior of single- or few-agent systems, RL’s limitations
tively, these limitations inevitably limit the scalability and
(e.g.; sample inefficiency, high variance, out-of-distribution
robustness of centralized training paradigms.
behaviors)becomemoreevidentwhentransitioningtomulti-
Conversely, decentralized learning approaches offer a
agent settings. These challenges highlight the critical need
compelling alternative to centralized training, allowing each
for effective inter-agent coordination, a requirement that
agent to estimate expected returns autonomously based on
is particularly important for applications such as robotic
individual experiences. Crucially, these algorithms preclude
warehouse management [1], [2], urban traffic light systems
direct inter-agent communication and restrict access to the
[3], [4], and human-robot collaboration [5], [6].
policies of other agents [11], [12], [13], [14], [15]. As such,
For optimal coordination, agents face two overarching
decentralized learning paradigms, such as independent Q-
challenges. First, they must identify which objectives will
learning [16], are particularly well-suited for highly scalable
maximize collective utility. Second, they must achieve goal
scenarios. Nevertheless, the limited flow of information cre-
alignment with their teammates to avoid counterproductive
ates obstacles in agents’ understanding of their peers and
outcomes.Traditionally,theabovechallengeswereaddressed
their ability to interpret the environment effectively. This
by methods that employ a centralized learning approach.
limitation frequently causes a lack of alignment in agents’
Specifically, these methods aggregate experiences from all
objectives or the selection of suboptimal goals, ultimately
agents to perform joint strategy optimization, a technique
resulting in poor coordination.
that proved effective in optimizing group outcomes [7],
Human coordination can thrive even without verbal com-
[8], [9], [10]. However, these approaches face three critical
munication,assilentinteractionsoftensufficeformutualun-
challenges: i) they incur exponential computational costs
derstanding.Motivatedbythisconcept,wetacklethehurdles
as the number of agents increases, ii) they necessitate the
of limited communication within decentralized systems by
global sharing of individual agent policies, which might not
examiningtheirinteractionpatterns.Typically,agentsoperate
be feasible in practice and limits the applicability of such
concurrently, deriving lessons from these experiences—a
The authors are with the Department of Computer Science, methodwehavetermedSynchronousMultiAgentInteraction
University of Colorado, Boulder, Colorado, USA. Emails: (SMA). While this strategy promotes efficient exploration, it
{firstname.lastname}@colorado.edu
alsoresultsinacontinuouslychangingenvironment,compli-
This work was supported by Army Research Laboratory under grant
#W911NF-21-2-0126.AlessandroRonconeiswithLab0,Inc. cating the agents’ capacity to align their strategies. To coun-
4202
rpA
5
]AM.sc[
1v48930.4042:viXrateractthisissue,wepresentROMA,aROund-RobinMultiA- Ourmaingoalistodevelopalearningmethodtoenableeach
gentScheduling protocoldesignedtostreamlineinformation agent to independently learn its optimal policy, π , using its
k
acquisitionduringinteractionforgoalalignment.Withinthis own experiences, without the explicit knowledge of policies
framework, interaction is structured in rounds, with each shared by other agents. More specifically, the joint of all
round permitting only one agent to collect experience from individual optimal policies should be an optimal joint policy
the environment. Additionally, more knowledgeable agents π∗ thatmaximizesthecumulativediscountedsharedrewards
utilize their better policies to guide the agent under consid- rwithadiscountfactordenotedasγ.Theserewardsdepends
eration toward beneficial outcomes, while less experienced on the current state, s , and the next state, s , arising
(t) (t+1)
agentsareaffordedtheopportunityforexploratorybehaviors. from a deterministic transition function, T. This function
This synergistic approach allows agents to capitalize on the uses the current state and the action derived from the policy,
expertise of their more experienced peers, culminating in π(s ), to generate the next state.
(t)
aligned objectives.
(cid:88) (cid:88)
π∗ =argmaxE[ γtr(s ,s )],
Beyond aligning objectives, equipping agents with the (t) (t+1)
π∈Π
skills to pinpoint the best goals is also crucial. To overcome s(0)∈St≥0
the constraints of decentralized algorithms, we propose in- s =T(s ,π(s )),
(t+1) (t) (t)
dependentQSSlearning (iQSS), an innovative approach to Π=Π ×Π ×...×Π
1 2 |K|
decentralized learning based on state values. iQSS works
by assessing the value of the current global state based Additionally, agents collect their own experiences when
on potential future states, utilizing insights gained from interacting with other agents in the environment. During an
environmental interactions. As a result, agents can more agent’sinteraction,itreceivesmultiplepiecesofinformation,
accuratelyanticipatethereturnsofpossiblefuturescenarios, eachrepresentedbyatuple(s,a k,s′,r).Eachtuplecontains
guidingthemmoreefficientlytowardstheoptimalobjectives. the current environment state s, an agent’s own action a k, a
subsequent environment state s′, and a shared reward r.
In conclusion, this paper introduces a tightly integrated
framework that combines independent state-based value
B. Overview of Value Learning Methods
learning, iQSS, with a specialized multi-agent interaction
Centralized Q learning provides a method to generate
protocol,ROMA.Thesetwocomponentsarenotstand-alone
an optimal policy that maximizes a group’s profits, but it
solutions;rather,theyoperatesynergisticallytoenableagents
suffers from scalability. On the other hand, independent Q
topinpointoptimalstatesandsynchronizingtheirobjectives.
learning enables agents to learn independently. However, it
Theoretical analysis reveals iQSS helps agents converge
is challenging for it to learn an optimal policy due to its
on effective policies for optimal states, while ROMA co-
partial understanding of the environment.
ordinates their efforts to a common goal. Our empirical
1) Centralized Q Learning (cenQ): In the context of
studies,featuringmulti-stagecoordinationtasks,demonstrate
our coordination problem, the utilization of centralized Q-
ROMA-iQSS’s superiority over the current state-of-the-art,
learningnecessitatesthepresenceofacentralcontroller.The
I2Q [15], and traditional independent Q learning methods.
central controller accesses the experience of all agents and
Crucially, ROMA-iQSS stands out for its dual capability to
leverages it to learn the joint strategy for all agents.
identifyoptimalobjectivesandensuregoalalignmentamong
Specifically,cenQmaintainsavaluefunction,representing
agents.
anexpectedlong-termreturnassociatedwiththeenvironment
state and joint action, a, denoted as Q(s,a), where a =
II. PRELIMINARIES
(a ,a ,...,a )anda∈A,whereA=A ×A ×...×A .
1 2 |K| 1 2 |K|
A. Coordination Problem
This value function is learned through a temporal difference
We aim to solve the following collaborative prob- learningprocess,incorporatingαasthelearningrate,γasthe
lem, represented by a 7–tuple Markov Decision Process, discount factor, and s′ as the subsequent environment state,
{K,S,(A ) ,T,r,(π ) ,γ}: determined by the environment transition function T(s,a).
k k∈K k k∈K
• K={1,...,k} is the set of indexes of all agents. Q(s,a)←(1−α)Q(s,a)
• S is the environment state space. +α(r+γmaxQ(s′,a′)) (1)
• A k is the discrete action space for agent k, while A −k a′∈A
denotes the action space for all agents except agent k. Temporal difference learning in eq. (1) is well-known for
• s (t+1) = T(s (t),a (t)) is the environment transition yielding the optimal Q-value, Q∗(s,a), upon convergence
function.Toclarify,timestepindicesareindicatedwith [17].
parentheses, such as a for joint action at time t, and
(t) Q∗(s,a)=r(s,T(s,a))+γmaxQ∗(s′,a′)
agent-IDindicesareshownwithoutparentheses,likea
k a′∈A
for agent k’s action.
With the value function Q∗(s,a), The centralized con-
• r is the shared reward received by all agents k ∈K.
troller can further induce the optimal policy, π∗:
• π k,belongstoadeterministicpolicyclassΠ k,generates
actions a
k
∈A
k
for agent k under environment states. π∗(s)=argmaxQ∗(s,a)
• γ is a discount factor. a∈AHowever, even with the promise of optimal convergence, with a state s ∈ S, and a neighboring state s′ ∈ N(s),
thepreferenceforcenQdiminisheswhenappliedtoextensive where N(s) is a set of possible subsequent states, observed
multi-agent systems due to inherent scalability concerns. by agents during the interaction.
2) IndependentQLearning(indQ): IndependentQLearn-
N(s)={s′|P(s′|s)>0}
ing becomes a preferable option in multi-agent systems due
to its superior scalability. This permits an agent to estimate ={s′|1 s′=T(s,a) >0,∀a∈A}
itsvaluebasedonindividualexperiences.Intechnicalterms,
Throughthetemporaldifferencelearning,thecentralcon-
each agent, k ∈ K, maintains the value Q , representing
k troller learns the converged state-state value Q∗(s,s′) [18].
the expected return associated with its own action and
environment state, through temporal difference learning: Q(s,s′)←(1−α)Q(s,s)
Q (s,a )←(1−α)Q (s,a ) +α(r+γ max Q(s′,s′′))
k k k k
+α(r+γ max Q (s′,a′)) (2) s′′∈N(s′)
k k
a′ k∈Ak
Q∗(s,s′)=r(s,s′)+γ max Q∗(s′,s′′)
In eq. (2), the subsequent state s′ is determined by the s′′∈N(s′)
transition function, T(s,a). However, an independent agent
Edwards et al. show the equivalence between Q∗(s,a) and
k is limited to accessing the information of its own action
Q∗(s,s′),wherethesubsequentstates′ isdeterminedbythe
denoted as a , instead of the complete action set a. It can
k transition T(s,a) [18]:
only access its action information, a . Therefore, the esti-
k
mated value, Q k(s,a k), depends on its observed transition, Qss∗(s,a)=Q∗(s,T(s,a))
P(s′|s,a ), which is a distribution conditioned on its action
k
We can utilize the equivalence to let the central controller
and environment state.
induce its optimal state-action value Qss∗(s,a) and further
Q∗ k(s,a k)=E s′∼P(s′|s,ak)[r(s,s′)+γ am
′
k∈a Ax kQ∗ k(s′,a′ k)] induce the policy π s∗ s, which is guaranteed to be optimal.
Theobserved transition,P(s′|s,a ),in anenvironmentwith
πss∗(s)=argmaxQss∗(s,a)
k
a∈A
deterministictransition,T(s,a ,a )canberepresentedas:
k −k
III. METHODS
(cid:88)
P(s′|s,a )= P(s′|s,a ,a )P(a |s,a )
k k −k −k k A. Independent QSS Value Learning (iQSS)
a−k∈A−k
(cid:88) Although centralized QSS learning can learn the optimal
= 1 P(a |s,a )
s′=T(s,ak,a−k) −k k policy,similartoothercentralizedapproaches,itisalsonota
a−k∈A−k scalablechoice.Therefore,wedevelopanindependentstate-
based value learning method, independent QSS learning.
(cid:40)
1 = 1 ,if s′ =T(s,a k,a −k) While centralized QSS learning enables a central con-
s′=T(s,ak,a−k)
0 ,otherwise troller to learn the state-state value, independent QSS learn-
ing enables each agent to learn its own value. Each agent
Therefore, P(s′|s,a ) is dependent on the responses,
k learns the value through the temporal difference learning,
P(a |s,a ), from other agents. If they keep changing
−k k used by the centralized QSS controller. Due to the conver-
their response strategy, the individual observed transition
gence property [17], [18], each agent can ultimately learn
can be non-stationary, making the convergence on the value the unique optimal state-state value Q∗(s,s′). Nevertheless,
function, Q (s,a ), challenging.
k k sinceagentslearnthevalueindependently,werepresenttheir
Furthermore, if other agents maintain stable but subop-
values, respectively, with an index k.
timal strategies, the agent could learn a suboptimal value.
This, in turn, results in its derived policy, π∗, also being Q k(s,s)←(1−α)Q k(s,s)
k
suboptimal. +α(r+γ max Q (s′,s′′)). (3)
k
s′′∈N(s′)
π∗(s)=argmaxQ∗(s,a )
k
ak∈Ak
k k Q∗(s,s′)=Q∗ k(s,s′)
3) Centralized QSS Learning (cQSS): QSS learning dis- =r(s,s′)+γ max Q∗(s′,s′′) (4)
k
s′′∈N(s′)
tinguishes itself as a state-based value learning. Unlike
action-based methods, its return estimate is not directly Like cQSS inducing its action-based value, iQSS agents
dependent on the actions. Instead, it is determined by only can also utilize their states-based value to induce their
a state and a subsequent state. action-based value function. However, unlike the centralized
When designing a centralized QSS learning, like central- approach, an independent agent cannot access the informa-
ized Q learning, it also necessitates including a central con- tion of actions, made by others. Therefore, an iQSS agent
trollerthatcanaccesstheinformationofallagents.However, maintains a value, Qss∗(s,a ) that takes only its own action
k k
instead of estimating the state-action value, it approximates and the environment state. It induces its state-action value
the state-state value, Q(s,s′), an expected return associated Qss∗(s,a )bymaximizingthestate-statevalueoverasetof
k ksubsequent states N(s,a ), which is observed and recorded a foreacha ∈A andthenmaximizesitovera ∈A .It
k k k k k k
during its experience collection stage. considersalltheCartesianproduct’sjointactions,A ×A .
k −k
The lower-hand side of the fourth equivalence maximizes
Qssa∗(s,a )= max Q∗(s,s′)
k k k (5) over the Cartesian product of the action sets. Therefore, the
s′∈N(s,ak)
fourthequivalenceholds.Thefinalequivalenceisbecauseof
the equivalence A=A ×A .
N(s,a )={s′|P(s′|s,a )>0} (6) k −k
k k The inequality shows the action generated by the πss∗(s)
k
With the converged value, iQSS agents can then induce istheoptimalactionwhenthecurrentstateiss.Theproperty
their policies: holds for all states s∈S. Therefore, πss∗(s) is optimal.
k
Since Theorem 3.1 requires the assumption, we show it is
πss∗(s)=argmaxQssa∗(s,a )
k k k trueunderaconditionintheLemma3.2.Inaddition,welater
ak∈Ak
create multi-agent interaction schemes to let the condition
Although the individual policies π kss∗(s) were induced hold.
without considering the action information of other agents, Lemma 3.2: The set equivalence assumption, mentioned
we show that the joint policies still have the equivalence to intheTheorem3.1,holdstrueundertherelationassumption:
the optimal policy π∗(s) under the assumption in Theorem
∃a ∈A ,1 >0⇒P(s′|s,a )>0
3.1. We later show the assumption is true in Lemma 3.2. −k −k s′=T(s,ak,a−k) k
Theorem 3.1: πss∗ is optimal under the set equivalence Proof: We show the equivalence by showing they are
k
assumption: subsets of each other. For simplicity of clarification, we
represent two sets by X and Y. We first prove the set X
{s′| ∃a −k ∈A −k,1 s′=T(s,ak,a−k) >0} is a subset of the set Y.
={s′|P(s′|s,a )>0}
k X ={s′|P(s′|s,a )>0}
Proof: k
(cid:88)
={s′| P(s′|s,a ,a )P(a |s,a )>0}
Qssa∗(s,a ) k −k −k k
k k a−k∈A−k
by eq. (5) = max Q∗(s,s′) (cid:88)
k ={s′| 1 P(a |s,a )>0}
s′∈N(s,ak) s′=T(s,ak,a−k) −k k
by eq. (4) = max Q∗(s,s′) a−k∈A−k
s′∈N(s,ak) ⊆{s′| (cid:88) 1 >0}
by eq. (6) = max Q∗(s,s′) s′=T(s,ak,a−k)
s′∈{s′|P(s′|s,ak)>0} a−k∈A−k
= max Q∗(s,s′) ={s′| ∃a −k ∈A −k,1 s′=T(s,ak,a−k) >0}=Y
s′∈{s′| ∃a−k∈A−k,1 s′=T(s,ak,a−k)>0}
By the law of the total probability, we have the first
= max Q∗(s,T(s,a k,a −k)) equivalence. Then, since the transition, P(s′|s,a ,a ),
a−k∈A−k k −k
is deterministic in our environments and represented by
The first three equivalence come from the definitions. The
s′ = T(s′|s,a ,a ), we have the second equivalence. In
fourth is due to the assumption, mentioned in the Theorem k −k
addition, we have the subset relation between the third and
3.1.Forthefifthequivalence,theupper-handsidemaximizes
the fourth line because of the inequality for any s′ ∈S:
state-state value over all subsequent states, which any joint
(cid:88)
actions containing a could, given the current state s. The 1 P(a |s,a )≤
k s′=T(s,ak,a−k) −k k
lower-hand side basically does the same thing, but it brings a−k∈A−k
thetransitionfunctionT(s,a k,a −k)intothestate-statevalue (cid:88) 1
function. Therefore, instead of maximizing over states, it
s′=T(s,ak,a−k)
maximizes over all possible joint actions while fixing the
a−k∈A−k
action made by agent k to be a k. The inequality is true because 0 ≤ P(a −k|s,a k) ≤ 1.
Let a∗ =πss∗(s). Finally, the final equivalence comes from the relation:
k k
(cid:88)
Qs ksa∗(s,a∗ k) 1 s′=T(s,ak,a−k) >0⇔
= max Q∗(s,T(s,a∗,a )) a−k∈A−k
a−k∈A−k k −k ∃a ∈A ,1 >0
= max max Q∗(s,T(s,a ,a ))
−k −k s′=T(s,ak,a−k)
ak∈Aka−k∈A−k k −k It is true because 1 is non-negative.
= max Q∗(s,T(s,a ,a )) Moreover, we can prove the set Y is a subset of the set
k −k
(ak,a−k)∈Ak×A−k X by the assumption, mentioned in the theorem.
≥ Q∗(s,T(s,a)),∀a∈A
Y ={s′| ∃a ∈A ,1 >0}
−k −k s′=T(s,ak,a−k)
Thefirstequivalencecomesfromthefinalequivalenceofthe
⊆{s′|P(s′|s,a )>0}=X
process,wehaveshown.Thesecondequivalenceisfromthe k
πss∗(s) definition. For the third equivalence, the upper-hand Since they are subsets of each, we have the set equivalence
k
side first maximizes the value over joint actions containing in Lemma 3.2.Lemma 3.3: The relation assumption, mentioned in Algorithm 1 Sychrnounous Multiagent Interaction
Lemma 3.2, holds true if P(a −k|s,a k) is always positive. 1: for t=1,2,...,t max do
Proof: From the law of total probability, we can infer 2: All agents execute their ϵ-greedy policy.
the equalities: 3: Allagentsrecordtheinteractiontothereplaybuffers.
(cid:88) 4: end for
P(s′|s,a )= P(s′|s,a ,a )P(a |s,a )
k k −k −k k
a−k∈A−k
(cid:88)
= 1 P(a |s,a )
s′=T(s,ak,a−k) −k k
a−k∈A−k
Additionally, we know that 1 is always non-
s′=T(s,ak,a−k)
negativeandweassumethatP(a |s,a )isalwayspositive,
−k k
Therefore, the relation is true:
∃a ∈A ,1 >0⇒P(s′|s,a )>0
−k −k s′=T(s,ak,a−k) k
Fig.2. ProblemDefinition:Thisisa2-agentgamefeaturingfourpotential
destination states, each represented by two numbers denoting Agent X’s
andAgentY’spolicies.In this game, both agents can identify optimal
Based on the given Theorem and Lemmas, we deduce that
states,indicatedinredtext,buttheymaketheirdecisionsindependently
the strategy πss∗ is deemed optimal when the conditional without knowledge of each other’s choices.SMA-Scenario(Left): Both
k
probability P(a−k|s,a ) is consistently greater than zero. agents observe all potential states, including two optimal states. Conse-
k quently,theymightestablishdivergentobjectives,whichpreventsthemfrom
Thisassumptionessentiallymeansthatforanychosenaction
reachingeitherofthetwooptimalstates.ROMA-Scenario(Right): Agent
a by agent k, there is always a nonzero probability of Xobservesallpotentialstatesandselectsitspolicyaccordingly.AgentY,
k
encountering any other actions a−k from other agents. Put ontheotherhand,observesonlythestatesthatAgentX’sselectioncanlead
to.Consequently,AgentX’sselectioninfluencesAgentY’schoice,enabling
simply, regardless of the decision made by agent k, there’s
ittoalignwithAgentX’sobjectiveandultimatelyreachtheoptimalstate.
always a possibility to interact with the varied actions of
other agents. To meet this requirement, one can implement
an ϵ-greedy strategy with ϵ > 0, ensuring that agents 2) Round-Robin Multiagent Interaction (ROMA): When
occasionally choose actions at random, thereby maintaining all agents follow the SMA interaction process, each agent
a positive probability of diverse actions. can learn an individual optimal policy through independent
QSSlearning.However,theymightnotformanoptimaljoint
B. Multiagent Interaction Schemes
policy if there exists more than one optimal joint policy
Agentsrequireexperienceforlearning.Theycollectexpe-
in the environment. More specifically, if agents learn their
riencewheninteractingwithotheragents.Inanindependent
individual optimal policy aiming at reaching a divergent
learning setting, each agent collects its own experience
optimal joint policy, the joint of their policies usually ends
and does not share it with others. Moreover, independent
up being policies, that are not the optimal policy, expected
QSS learning agents require an assumption to ensure the
by them. Take a robot coordination task as an example; all
individual learned policy πss∗ to be optimal. Therefore, we
k robots acknowledge that the optimal solution is that they all
design our interaction schemes to ensure it.
turn right or left. Therefore, each of them will learn that the
1) SynchronousMultiagentInteraction(SMA): FromThe-
optimalindividualactioncouldbeeitherturningleftorright.
orem 3.1, Lemma 3.2 and 3.3, we know that πss∗ is optimal
k A potential joint of its optimal individual policies would be
if P(a |s,a ) is always positive. That means πss∗ is
−k k k that some turn left, and some turn right. That is apparently
optimal if the probability of all possible joint actions should
not the result generated by an optimal joint policy.
stay positive no matter what action a certain agent executes.
We design an interaction scheme, round-robin multiagent
Therefore, to ensure P(a |s,a ) > 0, we need to ensure
−k k interaction (ROMA), for agents to collect limited, but suf-
agents always take a random action with probability > 0
ficient information that facilitates them in aligning their
when interacting with others. More specifically, we can
objectives with others so as to cooperate in learning the
enableagentstoexecuteϵ-greedypolicy,whichtakesrandom
joint optimal. In ROMA, in each iteration, only one agent
action with probability ϵ > 0, and learned action with
can collect experience. We refer to that single agent as the
probability 1−ϵ:
collector. We let a certain agent, c, be the only collector for
(cid:40) continuous t iterations. After agent c’s collection period,
π (s) ,with probability 1−ϵ u
πϵ(s)= k
we let its next agent, which has an index (c+1) mod |K|,
k a random action ,with probability ϵ
betheonlycollectoratthenextcontinuoust iterations.We
u
In synchronous multiagent interaction (SMA), each agent enable agents to repeat the rotation process.
executesϵ-greedypolicywithϵ>0andcollectsexperiences During the iterations, where agent c is the only collector,
to its replay buffers. Then, from its collected experience, agents with an index, smaller than c are seniors, and agents
agent k ∈ K would observe P(a |s,a ) > 0. Therefore, with an index, greater than c, are juniors. Moreover, seniors
−k k
with Theorem 3.1, we know each agent k ∈K would learn can only execute their learned policy to interact with others.
an individual policy, πss∗, which is optimal. All other agents, including all juniors and the collector,
kexecute their ϵ-greedy policies, which allows them to take state s′ with a probability higher than δ when it takes action
randomactions.Consequently,thecollector,agentcobserves a at the state s. Therefore, it can consider the subsequent
k
experience, caused and limited by the joint of the learned state s′ as a possible neighboring state, s′ ∈ Nδ(s,a ,θ).
k
policies of seniors. In other words, the collector cannot Additionally, when δ is zero, an agent considers all possible
observethetransitionconditionedonthejointactions,which states it has observed from the collected data. It is fine if
its seniors do not take at all. other agents do not change their policy at all. However, if
others change their policies, an agent might consider states,
Algorithm 2 Round-Robin Multiagent Interaction which were possible but not going to be reached. Therefore,
1: for t=1,2,...,t max do each agent should set δ to be close to, but greater than zero.
2: c=⌈ t ⌉ mod |K| Asaresult,itwouldonlyconsiderstatesthatarestillpossible
tu
3: collector = agent c to reach with a probability higher than δ, greater than zero,
4: seniors Z c = agents with an index <c and thus, would not consider those that will not be reached.
5: juniors J c = agents with an index >c Nδ(s,a ,θ)≈{s′|P(s′|s,a ,θ)>δ}
6: ∀z ∈Z c execute their learned policies, ∀j ∈J c and k k
agent c execute their ϵ-greedy policies Each agent also needs to estimate the state-based value
7: collector records the interaction to replay buffer Q(s,s′) through the temporal difference learning using
8: end for eq. (3). When applying it, agents must maximize the state-
based value over all possible neighboring states of the state.
However, the space of neighboring states is usually large.
Theobservationforacollectorislimitedtothetransitions
Therefore, an efficient way to identify optimal neighboring
causedbythejointpolicyofitsseniors.Therefore,ifseniors
states is desirable.
have coordinated in learning a joint optimal, the limited
max Q (s,s′)
observationisstillsufficienttoenablethecollectortoobserve k
s′∈N(s)
the optimal transitions so as to learn an optimal policy. = max max Q k(s,s′) (7)
Moreover, the limited observation, caused by seniors, also ak∈Aks′∈N(s,ak)
facilitates the collector to align its objectives with seniors. = max Q (s′,zˆ )
Consequently,followingROMA,thecollectorcancoordinate ak∈Ak
k s,ak
with its sensors to learn a joint optimal policy even if We can rewrite the maximization using individual neigh-
multiple optimal joint policies exist. boring states N(s,a k) as the second line of the eq. (7),
As a result, ROMA enables agents to reach optimal which maximizes over an individual action space and also
coordination. We place the proof of the theorem in the the individual neighboring state. The maximization over in-
appendix, which can be found on ArXiv. dividualneighboringstatesN(s,a k)iscomputingexpensive.
Theorem 3.4: If all agents follow the ROMA interaction Therefore, we develop a method to compute the optimal
process to collect experiences, the joint of independent QSS individual neighboring state zˆ s,ak. Consequently, we can
learning agents’ individual converged strategies, πss∗, is compute the maximization by only maximizing over an
k
optimal. individual action space.
zˆ = argmax Q (s,s′)
IV. APRACTICALIQSSLEARNINGPROCESS s,ak k
s′∈Nk(s,ak,θ)
During the interaction, each agent collects its experience
The optimal individual neighboring state zˆ is the state,
into its replay buffer. From the replay buffer, each agent
s,ak
which provides the max state-based value among all in-
k ∈ K can observe multiple experienced tuples, each of
dividual neighboring states N(s,a ). We enable agents to
which is represented as (s,a ,s′,r) that are the subsequent k
k learn zˆ by an iterative update method. We first assign an
state s′ and the received reward r after taking the action s,ak
agentarandomstateaszˆ .However,ifP(zˆ |s,a ,θ)is
a at the state s. With the data, each agent can learn
s,ak s,ak k
k smallerthanδ,thatmeanszˆ doesnotbelongtoN(s,a ).
its policy by modeling the transition P(s′|s,a ) and the s,ak k
k Therefore, we must update it by a neighboring state s′,
neighboring states N(s,a k), identifying the optimal states, which belongs to N(s,a ), where P(sˆ′ |s,a ,θ) > δ.
optimizing state-based value network Q(s,s′), and finally k s,ak k
Additionally, if it belongs to N(s,a ), but offers a worse
inducingthe state-actionvaluefunction Qssa(s,a ),and the k
k performance than the other individual neighboring state.
policy πss(s).
Usingthebetterstate,wemustupdatezˆ .Thesetwobasic
Each agent k ∈ K first learns the observed transition,
s,ak
principles enable agents to improve their understanding of
P(s′|s,a ), by modeling it using a neural network, repre-
k zˆ and ultimately learn the optimal neighboring state:
sented by parameters θ and maximizing the likelihood of
s,ak

the observed dat Pa (p so ′i |n s,t.
a k)≈P(s′|s,a k,θ)
s′, if anP d( Pzˆ s (, sak ′|| ss ,, aa kk ,, θθ )) >≤ δδ
zˆ ← s′, if Q (s,zˆ )<Q (s,s′)
A nen igha bg oen rit ngca sn tateu ss .e Ifth the ee ls ikti em lia ht oe od dl Pik (e sli ′h |so ,o ad k,t θo
)
ii snf ge rr eat th ee
r
s,ak zˆ
,
ea lsn
edk P(s′|s s,a ,k
a
k,θ)>k
δ
thanδ,thatmeansthatanagentk ∈K observesasubsequent s,akWith the optimal neighboring state, we can develop an learning approaches. This results in six distinct evaluative
efficient temporal difference learning method to update the comparisons:ROMA-iQSS,SMA-iQSS,ROMA-I2Q,SMA-
state-based value Q (s,s) by replacing the maximization I2Q, and the independent Q learning techniques including
k
with eq. (7): ROMA-indQ and SMA-indQ. We carry out fifteen simula-
Q (s,s′)←(1−α)Q (s,s′)+ tions for each method across scenarios involving 3, 5, and 7
k k
(8) agentstoanalyzetheaveragegrouprewardandperformance
α(r+γ max Q (s′,zˆ ))
ak∈Ak
k s′,ak indicators.Furthermore,wechooseanϵ-greedypolicysetting
of ϵ = 0.8 to encourage comprehensive exploration. This
The temporal difference learning in eq. (8) enables each
highϵvalueiscrucialforsimulatingtheobjectivealignment
agent to update the value Qk(s,s′). In addition, during each
challenge, ensuring that the majority of agents experience
iteration, they can use the states-based value Q(s,s′) to
both optimal states. Additionally, adopting a higher ϵ value
update the state-action value Qssa(s,a ) and also further
k k helps in minimizing the likelihood of agents becoming en-
compute the policy πss(s):
k trappedinlocaloptima.Moreover,weshowmoreexperiment
Qs ksa(s,a k)← max Q k(s,s′) setup details in the appendix, which can be found on ArXiv.
s′∈N(s,ak)
The analysis reveals that ROMA significantly diminishes
πss(s)←argmaxQssa(s,a ) performance variance compared to SMA algorithms. Specif-
k k k ically, in scenarios with three and five agents, the perfor-
ak∈Ak
manceofSMA-iQSSdemonstratesconsiderablefluctuations,
As a result, given enough time each agent learns optimal
whereas ROMA-iQSS stabilizes quickly, achieving consis-
values and the optimal policy. If an overview of practical
tentandsuperiorperformanceafterafewiterations.Further-
iQSS learning is desired, we provide the pseudocode in the
more, both ROMA-indQ and ROMA-I2Q exhibit marginally
appendix, which can be found on ArXiv.
better and notably steadier performance than their SMA
V. NUMERICALEXPERIMENTS equivalents. This reduction in variance can be attributed to
We evaluate our algorithms via a coordination challenge themorestationaryenvironmentcreatedbytheROMAstruc-
that involves independent agents. Every agent begins with ture. In SMA settings, all agents are concurrently exploring
knowledge of various strategies but does not know which and making decisions to achieve their goals, leading to
strategy offers the most group benefit and what strategies challenges in objective alignment when goals differ, thereby
otheragentsareconsidering.Thegoalisforagentstotackle producinganon-stationaryenvironment.Conversely,ROMA
two challenges: to independently identify the best strategies facilitates a more stable environment by enabling agents to
and to align their efforts with other agents. naturally follow the lead of more experienced or ”senior”
The issue is divided into three phases, where agents must agents in decision-making processes, thereby streamlining
address two challenges in each to increase collective earn- objectivealignmentwithouttheneedforexplicitdiscussions.
ings.Notpinpointingthebeststrategiesleadstolowrewards, The plots demonstrate that iQSS, particularly ROMA-
ranging from 0 to 10. Conversely, agents targeting divergent iQSS, outperforms I2Q and independent Q learning in ef-
optimal states face hefty fines of −200. Achieving success ficiently finding optimal states. ROMA-iQSS consistently
hinges on all agents aligning their goals and collaborating achievessuperiorresults,indicatingitseffectivenessiniden-
towards common objectives, which can yield significant tifying the best state. Although SMA-iQSS does not always
gains of 100. deliver top average performance, it reaches significantly
Agents independently choose from three actions at each higher peaks in some iterations, showing it can find the
stage, forming a joint strategy. Notably, only two specific optimal state despite challenges in aligning objectives. In
jointstrategiesyieldsuccessfulcoordination,directingagents contrast, I2Q and independent Q learning typically do not
towardoneofthetwooptimalstates.Furthermore,successful attain the significant rewards, emphasizing the advantage
coordination hinges on all agents converging to pursue the of iQSS in state optimization. This advantage stems from
same optimal objective; otherwise, effective coordination is I2Q’sdifferingmethodofstateestimationandthelimitations
unlikely. of action-based techniques like independent Q learning in
We simulate coordination among three, five, and seven achieving similar outcomes.
agents. For three agents, the task is to choose the best state
out of 65 states. For five and seven agents, the challenge
VI. CONCLUSIONANDDISCUSSION
increases with 100 states. Our investigation compares our In this work, we present a novel decentralized frame-
novel state-based Q learning technique (iQSS) against the work decentralized framework aimed at overcoming the
establishedstate-basedQlearningmethod(I2Q)andthecon- challenge of forming an optimal collective strategy with-
ventionalaction-basedQlearning(independentQLearning). out direct communication. To address this challenge, we
iQSSandI2Qbothrelyonstatevaluesformakingdecisions develop two critical components. Firstly, we develop an
but differ in their learning approaches and strategies for independent states-based learning method, iQSS, designed
selecting the optimal state. to facilitate each agent to efficiently identify optimal states
We assess two interaction strategies: SMA (standard) and andindividualpolicies.Secondly,weintroduceaninteraction
ROMA (innovative), applying them across all examined scheme, ROMA, which empowers agents to collect limitedapproach holds significant potential to enhance human-robot
coordination.
REFERENCES
[1] G. Papoudakis, F. Christianos, L. Scha¨fer, and S. V.
Albrecht,“BenchmarkingMulti-AgentDeepReinforcementLearning
Algorithms in Cooperative Tasks,” in Proceedings of the Neural
Information Processing Systems Track on Datasets and Benchmarks
(NeurIPS),2021.[Online].Available:http://arxiv.org/abs/2006.07869
[2] J. J. Koh, G. Ding, C. Heckman, L. Chen, and A. Roncone, “Co-
operative Control of Mobile Robots with Stackelberg Learning,”
in Proceedings of the 2020 IEEE/RSJ International Conference on
IntelligentRobotsandSystems(IROS),2020,pp.7985–7992.
[3] X. Wang, L. Ke, Z. Qiao, and X. Chai, “Large-scale Traffic Signal
Control Using a Novel Multiagent Reinforcement Learning,” IEEE
TransactionsonCybernetics,vol.51,no.1,pp.174–187,2021.
[4] H. Wei, N. Xu, H. Zhang, G. Zheng, X. Zang, C. Chen, W. Zhang,
Y.Zhu,K.Xu,andZ.Li,“CoLight:LearningNetwork-LevelCooper-
ationforTrafficSignalControl,”inProceedingsofthe28thACMIn-
ternationalConferenceonInformationandKnowledgeManagement.
ACM,Nov2019.
[5] J.Brawer,D.Ghose,K.Candon,M.Qin,A.Roncone,M.Va´zquez,
and B. Scassellati, “Interactive Policy Shaping for Human-Robot
Collaboration with Transparent Matrix Overlays,” in Proceedings
of the 2023 ACM/IEEE International Conference on Human-Robot
Interaction,2023,pp.525–533.
[6] Y.-S.Tung,M.B.Luebbers,A.Roncone,andB.Hayes,“Workspace
Optimization Techniques to Improve Prediction of Human Motion
During Human-Robot Collaboration,” in In Proceedings of the 2024
ACM/IEEE International Conference on Human-Robot Interaction
(HRI’24), ACM/IEEE. New York, NY, USA: ACM, Mar 2024.
[Online].Available:https://doi.org/10.1145/3610977.3635003
[7] K.Son,D.Kim,W.J.Kang,D.E.Hostallero,andY.Yi,“QTRAN:
Learningtofactorizewithtransformationforcooperativemulti-agent
reinforcementlearning,”inInternationalconferenceonmachinelearn-
ing. PMLR,2019,pp.5887–5896.
[8] J. K. Gupta, M. Egorov, and M. Kochenderfer, “Cooperative Multi-
Agent Control using Deep Reinforcement Learning,” in Autonomous
AgentsandMultiagentSystems:AAMAS2017Workshops,BestPapers,
RevisedSelectedPapers16. Springer,2017,pp.66–83.
Fig. 3. Teams of 3, 5, and 7 agents navigate three-stage coordination,
[9] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch,
targetingtopoutcomesinanenvironmentwithmultipleoptimalstrategies.
“Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Envi-
ronments,” in Advances in Neural Information Processing Systems,
2017.
yet sufficient data. This data allows each agent to implicitly [10] S. Iqbal and F. Sha, “Actor-Attention-Critic for Multi-Agent Rein-
forcementLearning,”inProceedingsoftheInternationalConference
guide less experienced peers in their learning journey while
onMachineLearning,2019.
simultaneously benefiting from the guidance of more senior [11] M.Tan,“Multi-agentReinforcementLearning:Independentvs.coop-
agentsduringtheirownlearningprocess.Asaresult,ROMA erativeAgents,”inProceedingsofthetenthinternationalconference
onmachinelearning,1993,pp.330–337.
plays a pivotal role in aligning the objectives of all agents,
[12] C. S. de Witt, T. Gupta, D. Makoviichuk, V. Makoviychuk, P. H. S.
ultimately leading to successful coordination. Torr, M. Sun, and S. Whiteson, “Is Independent Learning All You
We substantiate our claims with rigorous proof, affirming NeedintheStarCraftMulti-AgentChallenge?”2020.
that iQSS is capable of learning individual optimal poli- [13] G. Palmer, K. Tuyls, D. Bloembergen, and R. Savani, “Lenient
Multi-Agent Deep Reinforcement Learning,” in Proceedings of the
cies, while also demonstrating that ROMA fosters objective 17thInternationalConferenceonAutonomousAgentsandMultiagent
alignment among agents. Furthermore, we provide empiri- Systems(AAMAS),2018.
cal evidence through multi-agent coordination simulations, [14] L. Matignon, G. J. Laurent, and N. Le Fort-Piat, “Hysteretic Q-
learning: An algorithm for Decentralized Reinforcement Learning
showcasing our method’s superiority over the current state-
in Cooperative Multi-Agent Teams,” in Proceedings of the 2007
of-the-art state-based value learning approach, I2Q, and the IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems,
classical action-based, independent Q learning method. 2007.
[15] J. Jiang and Z. Lu, “I2Q: A Fully Decentralized Q-Learning Algo-
For future work, we aim to extend our multi-agent coor-
rithm,” Thirty-Sixth Annual Conference on Neural Information Pro-
dination methods to real-world applications, such as coor- cessingSystems,2022.
dinating robots in warehouses or ensuring safe autonomous [16] M. Tan, “Multi-Agent Reinforcement Learning: Independent vs. Co-
operativeLearning,”ReadingsinAgents,pp.487–494,1997.
drivingforautomobiles.Additionally,weplantoincorporate
[17] C.J.WatkinsandP.Dayan,“Q-learning,”MachineLearning,vol.8,
human-like features into our agents. While human strategies pp.279–292,1992.
areofteninaccessibletorobots,ourframeworkequipsagents [18] A. Edwards, H. Sahni, R. Liu, J. Hung, A. Jain, R. Wang, A. Ecof-
fet, T. Miconi, C. Isbell, and J. Yosinski, “Estimating Q(s,s’) with
to coordinate effectively with others even without insights
DeepDeterministicDynamicsGradients,”inProceedingsofthe37th
intotheirstrategies.Consequently,weareoptimisticthatour InternationalConferenceonMachineLearning,2020,pp.2825–2835.APPENDIX agent 2 learns its optimal. Moreover, the joint of agent 1
THEPROOFOFTHEOREM3.4 and agent 2’s learned policies is also optimal.
In Theorem 3.4, we state: If all agents follow the ROMA Inductive Step: For some c ≥ 2, we assume that
interaction process to collect experiences, the joint of all the seniors of agent c have aligned their objectives and
agents’ individual converged strategies, πss∗, is optimal. coordinated in learning their joint optimal policies. Agent
k
Proof: For the readability of the proof, we suppose c then observes the states:
all agents follow single-round ROMA, where all agents (cid:91)
N(s,a )
start their rounds to collect experience only if their seniors c
complete their learning. We can set t = t to satisfy ac∈Ac
the assumption. Additionally, we also suu ppose|K th| at they stop = (cid:91) {s′|P(s,π <ss c∗(s),a c,a >c)>0,∀a >c ∈A >c}
learning after stop collecting experience. ac∈Ac
After proving the property is true under this assumption, ={s′|P(s,π <ss c∗(s),a c,a >c)>0,∀a >c ∈A >c)>0,
we will consider it in the general multi-round ROMA inter- ∀a ∈A ,∀a ∈A }
<c <c ≥c ≥c
action.
={s′|P(s,πss∗(s),a )>0,∀a ∈A }
We want to show that, for all d ∈ K, the joint policy of <c −c −c −c
the first d agents is optimal if all agents follow single-round From the assumption, during agent c’s collection turn, its
ROMA to collect experience for independent QSS learning. senior execute their joint optimal policies. In addition, all
We show it by induction. junior agents execute random actions with a probability
Base Case, d=1: Given enough time, agent 1 observes greater than 0. Therefore, the first equivalence holds. The
all possible states. We can show it by showing the set of second and the third equivalence comes from the rule of the
agent 1;s observed neighboring states including all possible union operation on sets.
neighboring states: Agent c observes only the states, its senior’s joint policy,
(cid:91) πss∗ could cause. However, since its seniors’ joint policy is
N(s,a ) <c
1
optimal,agentccanstillobservetheoptimalstatetransition.
a1∈A (cid:91)1 As a result, agent c learns its optimal through independent
= {s′|P(s,a ,a )>0,∀a ∈A }
1 −1 −1 −1 QSS learning. Moreover, the joint of agent c and its seniors’
a1∈A1 learned policies is also optimal.
={s′|P(s,a ,a )>0,∀a ∈A ,∀a ∈A }
1 −1 1 1 −1 −1 Results of the Induction: Our proof shows the statement
={s′|P(s,a)>0,∀a∈A} istrue.Itimpliesthatthejointpolicyofallagentsisoptimal
ifallagentsfollowsingle-roundROMAtocollectexperience
The first equivalence is because all agents take random
for independent QSS learning.
actions with a probability higher than 0 during agent 1’s
Discussion on the multi-round ROMA: We can adapt
collection turn. The second and the third equivalence comes
the proof from single-round ROMA to multi-round ROMA
from the rule of union operations on sets. Given enough
by replacing the lower bound, 0, of the individual transition
time, independent QSS learning enables agent 1 to learn
byδ,whichiscloseto,butgreaterthan0.Inotherwords,the
an individual optimal policy based on the observation of all
neighboring state observed by agent k is NR(s,a ) instead
possible neighboring states. k
of N(s,a ):
BaseCase,d=2: Agent2observestheneighboringstates: k
(cid:91) NR(s,a )={s′|P(s′|s,a )>δ}
N(s,a ) k k
2
a2∈A2 It is because, in multi-round ROMA, agents observe a
(cid:91)
= {s′|P(s,π <ss 2∗(s),a 2,a >2)>0,∀a >2 ∈A >2} transition, which is not stationary if its seniors change their
a2∈A2 policies during their learning. The real transition probability
={s′|P(s,πss∗(s),a ,a )>0,∀a ∈A )>0, of a certain subsequent state may be large at the start but
1 2 >2 >2 >2
∀a ∈A ,∀a ∈A } may become zero if senior agents change their policy to
2 2 >2 >2
favor other states. In that scenario, the observed transition
={s′|P(s,πss∗(s),a )>0,∀a ∈A }
1 −1 −1 −1 probability is non-stationary at the start but should decrease
The first equivalence is because all agents except agent graduallytoavalueclosetozeroafterseniorscompletetheir
1 take random actions with a probability higher than 0 learning.
and agent 1 takes actions, suggested by its optimal policy, To filter the choices, dropped by seniors, the collector
during agent 2’s collection turn. The second and the third should consider states with a transition probability higher
equivalence comes from the rule of union operations. than a certain small number, which is close to, but greater
Consequently, agent 2 observes only the states, which than zero. Consequently, the collector would only consider
agent 1’s converged policy πss∗ could cause. However, thestates,itssenior’scurrentpolicycouldleadto.Asaresult,
1
since its observation depends on agent 1’s converged policy, we can show that the joint policy of it and its seniors is
which is optimal, agent 2 can still observe the optimal state optimal following a proving process, similar to the proof in
transition. As a result, through independent QSS learning, single-round ROMA.APPENDIX Algorithm3ROMAwithEarlyStoppingandPre-Collection
EMPRICALEXPERIMENTSSETTINGONROMA: Mechanism
EARLY-STOPPINGANDPRE-COLLECTIONMECHANISMS 1: t∗ =t max/|K|
2: t u =t∗/n rounds
Our empirical studies reveal that two mechanisms, Early-
3: for t=1,2,...,t max do
Stopping and Pre-Collection, show enhanced learning sta- 4: c=⌈ t ⌉ mod |K|
tionarity when the ROMA interaction scheme is applied. tu
5: nc=(c+1) mod |K|
Early-Stopping: IntheEarly-Stoppingmechanism,more
6:
experienced agents conclude their learning process ahead 7: if t<c t∗ then
of their junior counterparts. Specifically,we divide the total
8: collector = agent c
iteration count into |K| partitions, with |K| representing the
9: next−collector = agent nc
number of agents. Each section consists of t∗ iterations. An
10: seniors Z c = agents with an index <c
agent indexed by c discontinues its learning after reaching
11: juniors J c = agents with an index >c
c t∗ iterations.
12: ∀z ∈ Z c execute their learned policies, ∀j ∈ J c
This mechanism enhances the learning environment’s sta- and agent c execute their ϵ-greedy policies.
bility for less experienced agents because it relies on a 13: collector and next−collector record the inter-
straightforward principle: once senior agents cease their
action to replay buffer.
learning, they solidify their strategies.
14: end if
Pre-Collection: In the traditional ROMA framework, 15: end for
agentscollectexperiencesexclusivelyduringtheirownturns,
leading to a scenario where each agent learns from a unique
set of experiences. Such a method can render their learning theaction-basedvalueQssa andtofurtherdeducethepolicy
k
less synergistic with their peers’. For example, if some πss(s).
k
agents mainly investigates the left side of the environment
while the rest concentrate on the right, both groups acquire
Algorithm4Agentk:iQSSwithOptimalStateIdentification
specific insights. However, this disparate learning fails to
contribute to formulating an optimal joint strategy for either
1: Initialize Q k, Qs ksa, π kss(s), θ k, and ω k.
environment section. 2: Initialize the experience buffer D k
To address these challenges, a simple adjustment can 3: for t=1,2,...,t max do
be made: allowing agents to collect experiences not just 4: (s,a k,s′,r)∼D k
on their own turns, but also during the turn immediately 5: θ k ←argmax θlogP(s′|s,a k,θ)
preceding theirs. This change means that the experiences of 6: z =zˆ s,ak
theirpeersalsogetincludedintheirexperiencereplaybuffer,
7: if P(z|s,a k,θ)≤δ or Q(s,z)≤Q(s,s′) then
encouraging agents to learn from the same experiences and
8: if P(s′|s,a k,θ)>δ then
align their learning with one another. Furthermore, imple- 9: zˆ s,ak ←s′
10: end if
mentation details on this are specified in Line 5, 9 and 13
11: end if
of the pseudocode in the Algorithm titled ROMA with Early
Stopping and Pre-Collection Mechanism. Q (s,s)←(1−α)Q (s,s)+
k k
α(r+γ max Q (s′,zˆ ))
APPENDIX ak∈Ak
k s,ak
ANOVERVIEWOFAPRACTICALIQSSLEARNING 12: Qs ksa(s,a k)←max s′∈N(s,ak)Q k(s,s′)
Each iQSS agent carries out Practical iQSS Learning 13: π kss(s)←argmax ak∈AkQs ksa(s,a k)
with its collected dataset D . Initially, the agent initializes 14: end for
k
its models, including value functions Q , Qssa, the policy
k k
πss(s), along with the parameters for the state-likelihood
k
model θ , and the parameters for the best-state estimator
k
ω .
k
During training iterations, agent k selects a batch of ex-
periences (s,a ,s′,r), where s represents the current states,
k
a the actions, s′ the subsequent states, and r the rewards.
k
Withthisdata,agentkupdatesθ usingmaximumlikelihood
k
estimation as indicated on Line 5. It also refines its best-
stateestimatorinlinewiththeupdatestrategiesdiscussedin
”A Practical iQSS Learning Process”, specifically concern-
ing zˆ . Moreover, agent k revises the state-based value
s,ak
functionQ throughtemporaldifferencelearning.Following
k
this, agent k uses the derived state-based value to enhance