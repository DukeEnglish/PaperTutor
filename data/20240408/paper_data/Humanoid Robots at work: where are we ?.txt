Humanoid Robots at work: where are we ?
Fabrice R. Noreils ∗
April 8, 2024
1 Introduction
In2021ElonMuskshowedoffhumanoidrobotprototypeatTeslaAI day. Musk saidrobotsmadebyTeslacouldone
day be worthmore than its cars,and that thousands of them wouldbe put to workin Tesla factories,where humans
build cars and batteries.
Without knowing it, Elon Musk launched a race in which many companies have already engaged, mainly on the
North American continent and in Asia. Investment funds are scrambling to invest in what they think is a new El
Dorado [36].
A humanoid robot is an extremely complex machine at the intersection of many disciplines such as mecha-
tronics, control algorithms for underactuated dynamic systems, actuators design, energy consumption optimization,
autonomous system in terms of perception localization planning and locomotion...
In addition, all algorithms must be carried out by on-board computers in a very constraint footprint.
Finally, the objective is to put these very complex, therefore fragile, machines in a demanding industrial environ-
ment to perform at least as well as a human worker within 2 or 3 years?
This is reminiscent of the enthusiasm and certainties of promoters of autonomous vehicles in the 2010s.
The question we can ask ourselves is the following: is this objective realistic or is it achievable not in 2 or 3
years but within 10 years or more? The aim of this document and its main contributions is to provide some hints by
covering the following topics:
1. Ananalysisof12companiesbasedoneightcriteriadescribedinsection3. Althoughthesecriteriaaresubjective,
theyhavethebenefitofbeingabletodistinguishcompaniesbasedontheirmaturityandapproachtothemarket;
2. These humanoids are very complex systems facing technological challenges which are discussed in section 4;
3. Operationandmaintenance are criticalactivities specially when humanoids are deployedat scale. This topic is
covered in section 5;
4. Pilots are the last step to test the feasibility of a new system before mass deployment. This is an important
step to test the maturity of a product and the strategy of the humanoid supplier to address a market. This
topic is addressed in section 6.
2 General Considerations
In this section, contextual considerations are provided.
∗TheauthorgothisPhDonroboticsandArtificialIntelligenceatLAASCNRS,ToulouseFrance. Hewasaresearcherontaskplanning
andExecutiononwheeledexploratoryrobotsandledR&DteamsonresourcesoptimizationforaLEOconstellationofsatellites,character
animationofsmallhumanoidrobots,autonomousnavigationofanindoorflyingdroneandtaskplanningandnavigationofafleetofAMRs
in logistics environment. He had operational roles such as the creation and management of a team of field engineers to deploy AMRs
and a team of operators to remotely supervise them. The author is now a technical leader for L3/L4 highway automated vehicles. The
opinionsexpressedinthispublicationarethoseoftheauthor. Theydonotpurporttoreflecttheopinionsorviewsofhiscurrent/former
employers.
1
4202
rpA
5
]YC.sc[
1v94240.4042:viXra2.1 Humanoid Robots because Shortage of labor, really?
Most of the companies working on commercial humanoids, sees warehouses as an obvious entry point because the
recurrent argument from the logistics industry is: shortage of labor.
Let us have a look at the HSE (Health and Safety Executive) report on Transportation and Storage statistics in
Great Britain, 2022 [21]:
• 49 000 workers suffering from work-related ill health: musculoskeletal disorders (36%), stress, depression or
anxiety (41%) and other illness (23%),
• 16 fatal injuries to workers – broadly similar to the five-year average of 14 fatalities per year – of which 34%
were due to being struck by a moving vehicle, 21% resulted from a fall from height, and 11% involved being
struck by a moving/falling object,
• 31 000 non-fatal injuries: slip, trip or fall (32%), lifting/carrying (23%) and being struck by a moving/falling
object (13%).
Two more data to complete the overallpicture:
• The median annual earnings in the United Kingdom was 34,963 British pounds per year in 2023.
• A recent survey found that the average turnover rate in the logistics industry is around 31%.
What do these numbers tell us? For a relatively low salary, workers are dealing with a harsh environment and
have a high chance of getting hurt over a period of time of only a few years, and this is why turnover is very high.
This forces companies to spend money on recruitment, training and medical aid.
Saying that there is a shortageoflabors is a fallacious argumentbecause there arepeople willing to workbut not
at any condition. Besides, the logistics industry can rely on immigration to hire new workers as well.
An obvious solution consists in increasing salaries and improving working conditions, but it will hurt margins.
This is probably why the logistics industry turns towards automation and robotics. Robotics is covering a wide area
of solutions from product storage,AMRs, manipulator arms and ... the newcomer which is humanoid robots.
Regarding humanoid robots, two other pieces of information deserve to be cited to complete the picture:
• The maximum recommended weight limit for human workers under the National Institute for Occupational
Safety and Health’s Lifting Equation is 51 lb (23 kg). UPS requires labeling and special procedures when
shipping packages over 70 lb (31.5 kg). Implementation of the Manual Handling Directive in the European
Union varies by member state, but several countries specify a 25 kg limit for men and 15 kg limit for women.
These weight limits provide a general guideline for the dynamic loads that most humanoid robots should be
expected to carry.
• As mentioned above, the median annual earnings in the United Kingdom for a worker in a warehouse was
34,963 British pounds per year in 2023 (around US$ 44k). It is not a surprised if many companies working on
humanoids are targeting a price below US$ 50k or even lower.
2.2 After AI, China is betting on humanoid robots
China created a list of key technologies in which they want to be world-leader. In 2017, China has outlined plans to
become a world-leader in artificial intelligence by 2030 [42]. On November 2nd, 2023, the Ministry of Industry and
InformationTechnology(MIIT),publishedanine-pageguidelineonitswebsite,sayingthatChinashouldrealizemass
production of humanoid robots by 2025 and humanoid robots should become an important new engine of economic
growth by 2027[43].
InAugust2023,theWorldRobotConference(WRC)heldinBeijingwastheChina’slargestrobotexhibition[45].
More than 160 companies were represented and more than 600 robots were exhibited to the public. The public was
astonished by the humanoid robots with, for some of them, a more and more realistic human appearance. Clearly, it
wasanopportunityfortheChinesegovernmenttoshowtheleveloftechnologyreachedbysomeofitslocalcompanies.
A strongsignal has been sent to the Chinese R&D robotics community: the centralgovernmentwill pour a lot of
money in humanoids. Therefore, it is likely we will witness a wave of humanoid supplier companies in the years to
come and a fierce competition will take place so that 2 or 3 majors emerge to address the marketworldwide and few
other dedicated to the domestic market. We will have to wait until 2025/2026to know the winners.
22.3 What kind of business model for humanoid robots?
These humanoid robots will be expensive, even though they will be mass product. The question then is how to sell
them? OneanswermaybetolookattheAutonomousMobileRobot(AMR)market. Indeed,there’slotofcompanies
that are selling mobile robots to these exact same markets like logistics which are targeted by humanoid supplier
companies. The most commonly business model chosen by customers is robots as a service or RaaS. This business
model did not exist before the AMR market took off.
Customers at least in the early stages of the humanoid market want to see how this works (see also section 6 for
more details) and this is why it is likely they will go for robots as a service model. Besides there is an eco-system
with third partyfinancing groupsalreadyset upwith the AMR marketonwhichthe emerginghumanoidmarketcan
rely on.
Worth to mention that the companies which are making humanoid robots need a lot of cash a different stage of
their growth. Humanoid robots are very expensive to build (hardware,structural materials like aluminum or plastic,
3D printing or machining tools, Software development tools... and many highskill professionals). Therefore these
companiesarewellfunded(fromUS$20to50Millions)todevelopaprototypereliableenoughtoinitiate andfinance
pilots. Successfull pilots will trigger production. It means that a company will raise another US$ 200-300Millions to
build a factory, hire staff and start the production but also develop its operationand support teams. Later on, more
financing will be necessary either to cover the losses or expand the market share.
3 Which Are The Competitors in this new race?
3.1 Introduction
1
In this section we will present the companies which aim to commercialize a humanoid robot in the years to come.
Eachcompanydevelopsitsownstrategyandapproachtothemarket. Thereforeeachcompanywillemphasizecertain
technologies than others. In order to provide a global picture, 8 criteria are proposed which are described below
as well as a radar chart for each of them. The description of each company according to these criteria is given in
Appendix A.
We are considering the following criteria:
• Robotics background: it characterizes the expertise and experience of the teams in charge of developments
(electro-mechanics, control, localization, path planning, perception and AI). We propose 0.2: the company is
young and does not communicate on the skills of its engineers, 0.5 the company hired engineers/researchers
recognizedbythe communityfortheircontributiontohumanoidrobotics1: the companyhasalongexperience
in the development of humanoid and/or quadruped robots;
• Teleoperation: teleoperation can play an important role to train a deep net how to grap objects when the
robot is equiped with hands or to takeover by an operator when the robot is stucked for example. However
teleoperation is not mandatory for a humanoid robot which will focus on a single task like moving totes for
instance. We propose 0: teleoperation is not mentioned because the company is relying on another method,
0.5: teleoperation as a tool for training and 1: multi-purpose teleoperation (training, teleoperated to achieve a
very complex task or takeover in case of failure);
• Modularity: for certain tasks in a warehouse which is built on a flat floor, legs it is not a necessary asset,
wheels are enough or even the customer is looking for a fixed robot capable of sorting objects on a conveyor.
This means more opportinities that the company to sell its humanoid robot in parts (upper body on wheels,
legsorafixedplatform). Wepropose0.5: fullhumanoidonlyand1: upperbodyonwheels,legsorfixplatform;
• Dexterous hands: these humanoid robotsfor the most partare intended to be generalpurpose robots, which
means that they can work in various environments, pick up or sort different type of objects. It is therefore
necessary to develop an agile hand with tactile sensors. We propose 0.2: gripper, 0.6: hand with less than 10
DoF and 1: hand with more than 10 DoF;
1ForthesakeofgeneralityIwillusethetermcompany(ies)whenIwillrefertooneormoreofthesehumanoidmanufacturer(s)inthe
remainingofthisarticle,unlessImentiononespecifically.
3• Task planning: it means the possibility for the robotto planby itself a sequence oftasks fromanorder input
(textualor vocal). Althoughthis is normallypartofAI,I amdeliberately separatingit becauseit is stilla very
active area of research and which will not be integrated into these robots in the short term. We propose 0:
activity not disclosedor not mentioned, 0.2: mentioned by the company but not demonstrated, 0.6: mentioned
and demonstrated in videos or papers and 1: included in the commercialized robot within the year to come;
• AI: it’s a very generic term and I use it here to differentiate different stages of AI integration. We propose
0.2: basic capabilities such as mapping, localization, perception for path planning, control, 0.4 plus objects
detectionandclassification,0.6plusEnd-to-endPerception-graspingand1: end-to-endfromlocomotion/control
- perception - grasping;
• Walking gait: it is obvious that a humanoid robot must walk. This is the field where one can see spectacular
results but there is a difference between a humanoid which is steadily walking at 2m/s and another one which
canwalkat3 or 4m/s onanuneventerrainwhile anoperatoris pushing it. We propose0: notseenany video,
0.6: video of a dynamic walking gait and 1: videos showing robustness with respect to kicks/pushes;
• Market/pilot: all these companies want to address one or more markets. What I note here is whether there
are pilots in progress, the company’s willingness to describe the pilots and also the willingness to open the
platform to labs or other companies to develop new applications. We proposed 0: no pilot disclosed, 0.2:open
platform available, 0.5: pilots but no feedback disclosed and 1: pilot and information disclosed.
These criteria are subjective, but they have the advantage of being able to distinguish companies based on their
maturity and approach to the market.
3.2 Panorama and analysis
Thepicture1highlightstheradarplotsofthedifferentcompaniesweconsidered. Thisanalysisisbasedonaphotoat
the timethisdocumentwaswritten. Itisverylikelythatwithinsixmonthsoroneyearnewcompanieswillappearor
thatthe companiescitedinthisdocumentwillhaveimprovedtheirofferintermsofhumanoids(software,mechanics,
etc.).
The competition is between North America (4 companies) and China (6 companies, the central government’s
announcement to finance this technology is certainly not unrelated). Europe is represented by a single company.
From these plots, we can outline different strategies:
• platform oriented We note that Fourier, Unitree and LimX have a fairly similar footprint, the robot is an
elaboratehumanoidplatformandresearchcentersareencouragedtodevelopapplications. AlthoughtheUnitree
humanoid is equipped with electrical motors, it has exceptional performance in terms of locomotion;
• Modularity, teleoperation, AI, hands or legsSantuary,Apptronik and1Xhavethe samestrategy,teleop-
eration plays an essential role for learning, the modularity of the commercial offer as legs are not a must have
asset- atleastina shorttermframe- anda significantinvestmentinAI algorithmsandtaskplanning (there is
nevertheless a notable technologicaldifference concerning the developmentof the hand which is a technological
marker for Santuary and under development for 1X which);
• Pilot ready Agility, Figure and Tesla have a very similar approach. They have strong teams, a humanoid
robotdeveloped enoughto start pilots. Note that the radargradationgoes from 0 to 1 for Agility while it goes
from 0 to 0.8 for Figure.ai and Tesla. Agility Robotics, very pragmatic in its approach, is very advanced in its
deployments in logistics - see Section 6;
• Very Promising and to followSomecompanieslikeAgibotareveryyoung(lessthanayear)butwithavery
elaborate long-term vision and a high-performance biped robot very similar to the one from Agility Robotics
(see Table 4). This is definitely a company to follow. The same goes for Kepler which showed his humanoid at
CES 2024.
I focus on companies that are developing a humanoid robot with the aim of commercializing it in the next 2
or 3 years.This is why I did not include Boston Dynamics’ Atlas, the world’s most advanced humanoid robot, at
least in terms of locomotion, because on one hand it is a very expensive piece of engineering and on the other hand,
4Figure 1: Radar plots of the eleven companies. R-B (Robotics Background),Teleop (Teleoperation), Mod (Modular-
ity), D-H (Dextereous Hands), T-P (Task Planning), AI (AI), W-G (Walking Gait), M/P (Market/Pilot)
it is an R&D platform use to explore new algorithms on dynamic whole-body control. However Boston Dynamics
may consider Atlas as a product in the future as it released a video showing its Atlas humanoid picking and placing
automotive struts. The significance of the demonstration is that Atlas performs all of the object recognition using
the robot’sonboardsensors. Atlas acquiresthe automotivestruts, usingits grippersfroma verticalstorageunit, and
places them horizontally onto a flow cart [41].
There are many very successful companies in Europe in the humanoid robotics field like Pal Robotics which
5develops humanoid robots intended for researchlaboratories such as TALOS or KANGAROO.
We must also mention Enchanted Tools with his Mirokai robot which consists in humanoid upper body on a
ballbot. I did notconsideritinthe tablebecause allthe other robotshavelegs. However,We willtalk aboutit again
in the section related to deployment because the company’s approachis very pragmatic.
4 Technical Challenges
Thissectionaimsatprovidingapanoramaoftheadvancesindifferentfieldsthatfosterthedevelopmentofhumanoid
robots.
4.1 Mechanics/actuators/control
For a long time humanoids were similar in terms of kinematics. The rotary joints are powered with servo-motors in
a serial configuration (Figure 2 (A) from [6]). The structure has been thoroughly investigated and a closed-form
solution for the inverse kinematics can be derived which is one of its main benefit. Unfortunately there are several
drawbacks as the rotary actuators are usually located directly at the joint, the ones closer to the origin of the chain
need to carry the ones lower in the chain. Position errors are summed across the joints which decreases accuracy. A
serial architecture also has higher inertias and therefore lower accelerations. It is worth noting that H1 from Unitree
is the only one humanoid which keep this serial configuration.
Theexperienceandthetheoryshowsthatalow-mass,low-inertialegwithhighbandwidthactuatorsallowformany
control simplifications. And this is what we observe on this new generationof humanoids. Indeed a solution consists
inattachingthe actuatorsoff-axis,closesttothe rootofthe link andusing alightweightcoupling ortransmissionlike
a belt -see Figure2 (B)in whichthe rotaryactuatorresponsibleofthe knee is closeto the hip inorderto reducethe
leg inertia [17]. As the tigh and the shank usually provide enough space for housing actuators and couplings, several
efficient solutions have been developed such as crank-lever mechanims and linear actuators see Figure 2 (C). Almost
all the new humanoids using two linear actuators for pitching and rolling the ankle around a Cardan joint.
Figure2: Fromaclassicalwellknowserialconfiguration(A)toLinearparallelmechanism(B)orcombinationofserial
and parallel mechanisms (C)
In terms of actuators, one can notice that all companies designed and developped their own electrical rotatary
and linear actuators. These Humanoid robots will interact with their environment and they need to absorb impacts
while walking, force-feedback control is mandatory.
The developmentofquadrupedrobotswas anopportunity to designa generationofactuatorsreferredto asquasi
direct drives [26] [27] which have been developed to generate sufficiently large torques, without sacrificing back-
drivabilityand/ormechanicalresilience. Thesedrivesareoftencomposedoftorquemotors(incontrasttopower/speed
6motors)andsingle-stageplanetarygearingtransmissionwhichislowerthan1:10,sothattheoutputtorqueisamplified
to some extent and intrinsic back-drivability is achieved, without compromising the system bandwidth. Moreover,
the low reduction of the gearing and the resulting high efficiency allows the motor torque, that can be realized by
the current measurement, to be directly a valid indication of the output torque. Larger version of these quasi direct
drive actuators are used on the humanoid robots.
The new humanoids are also relying heavily on linear actuators. I cannot tell whether they are mechanical linear
motor (designed around ball screw drives and backdrivability is disputable) or direct drive linear actuators. Direct
drive linear motor consists of two parts: the slider and the stator. The slider is made of magnet while the stator
contains the motor windings. It has been shown that this kind of motor is highly backdrivable and suitable for
humanoid robots [34].
Interms ofcontrol,if reinforcementlearningleadto amazingresults forquadrupeds,”newtonian”optimizationis
mainly appliedto humanoidrobots[57]. Moreoveroverthe years,opensourcecontroltoolboxeshavebeendeveloped
such as DRAKE [54], pinocchio [11] or IHMC [51] to name a few. These toolboxes synthesize the ”know-how” of a
decade of researchon dynamic walking. New researcherscan leverageon them to quickly prototype a new algorithm
and test it on a humanoid.
4.2 End-to-end grasping with dextereous hands
This the robotic manipulation golden age! Indeed the number of publications investigating AI approachesfor 6-DoF
grasping have grown significantly in the last few years and the achievements were almost not thinkable 5 years ago.
It was possible because we observed amazing progress in different connected fields as well. Let us have a look.
General purpose humanoid means that this robot will work and manipulate the same objects as human workers
do. Therefore a dextereous multi-fingered hand will be meaningfull. The design and the development of this kind
of hand is a challenge by itself: the mechanical part to ensure high dexterity, intrinsic and extrinsic sensors and
the grasping force control. However, with the rapid development of manufacturing and processing technology, new
materials and electronic, mechanical are emerging, which could provide new structures, actuators, and sensors for
multifingered hand design [32] [58].
Simulation softwares like deep net algorithms benefit from GPU advances. Game-engine renderers can model
cameras well enough not only to test a perception system in simulation, but even to train perception systems in
simulation and expect them to work in the real world! There is also an amazing improvements in the quality and
performanceofcontactsimulation. Makingrobustandperformantsimulationsofmulti-bodycontactinvolvesdealing
with complex geometry queries and stiff (measure-) differential equations. These simulation softwares allow for the
fast development of new algorithms and/or training new policies [53].
Once you have designed a hand, you are halfway there, then you need to know how to recognize the object you
want to grab and how to pick it up. This is a very hard problem, and again the recent advances in AI will give us
the tools to solve it. In perception to grasping arena, imitation learning is heavily used to train a neural net how to
graspagivenobjectaccordingtoitsshapeandsoftness. Differenttechniqueshavebeenexploredsuchasvisionbased
teleoperation [31] [19] or learning dexterity from internet videos [49] for instance. The benefit of these techniques is
theirlowcostbuttheylackofhapticandforces/torquesfeedback. Someteleoperationequipmentsareabletoprovide
these feedbacks but they may be expensive to acquire at large scales.
Another approach which seems very promising is Diffusion Policy [12] which enables easy and rapid behavior
teachingfromdemonstration. ToyotaResearchInstitute (TRI)leveragesondiffusion policyanddevelopeda pipeline
such that TRI’s robot behavior model learns from haptic demonstrations from a teacher, combined with a language
description ofthe goal. A proprietaryAI-based Diffusion Policyis used to learnthe demonstratedskill. This process
allows a new behavior to be deployed autonomously from dozens of demonstrations [52]. Diffusion model is also the
foundation of new research aiming at developing a policy composition framework domains (simulation, real robot of
video) and modalities (vision, depth images, tactile...) [55].
Researchers are also developing tools to simplify as much as possible the collection of data. For instance this
paper[13]introduceswhattheycallaUniversalManipulationInterface(UMI)–adatacollectionandpolicylearning
framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot policies. In-
the-wildbecauseitemployshand-heldgripperscoupledwithcarefulinterfacedesigntoenableportable,low-cost,and
information-rich data collection. It is quite a breakthrough in the senses that it eases significantly the training of a
robot policy for a given task.
Toconcludethisbriefoverview,sofarallthesehumanoidrobotsaremanipulatingsmallandlightobjectsandthus
7the lowerpartoftheir body is rigid. Howeverif they haveto manipulate bulky objects they willhaveto engagetheir
entire body like a human worker to avoid injuries and for the robot to avoid actuators to be destroyed too quickly.
Some researchersare staring to work on this field as well [48].
4.3 Spatial reasoning and tasks planning (for manipulation)
We can start by mentioning two videos that have been posted recently:
• AgilityRobotics,inavideo,demonstratedthatitispossibletogiveverbalorderstodigitwhichwasabletoplan
a sequence of tasks to execute the order by using a dedicated LLM. Although the environment is very simple,
a set of towers of different heights as well as three boxes, each one with a a different color and pictograms, and
the orders like ”move the red box one the lowest tower” for instance, digit was able to execute the order;
• Figure,whichpartneredwithOpenAI,releasedademovideowheretherobotcantalklikeahuman. Therobot
was able to identify things put in front of it, answer the queries, do the task asked of it (giving an apple to the
person), and explain how it did that at the same time it was doing something else.
These demos were possible with the recent advances in what we can call Robotic Transformers and different
approaches are explored by researchers.
One approach consists in combining a Large Language Model (LLM) and Vision Language Model (VLM). LLMs
pretrainedonbroadweb-scaledatasetshavedemonstratedreasoningskillsovertext,leveragingtheirnaturallanguage
cpabilities and common-sense reasoning for generating robot tasks. However LLMs can generate plans but they
cannot perceive the environment. At the same time, VLMs enable open-vocabulary visual recognition and are able
to make complex inferences about object-agent interactions in images. A VLM, given an image, is able to perform
spatial reasoning and object classification. Based on these information a LLM is able to plan a sequence of actions
corresponding to a specific query. Palm-E is an example of this approach [15]. Note that the sequence of actions is
then executed by low level controllers. For old researchers, it may recall them STRIPS [18] the planner created for
Shakey by the SRI.
The aforementioned approach does not have knowledge about the robot and controller capabilities. Some re-
searchers argue that grounding the language belief with visual and motor feedback will lead to better perfor-
mances.Therefore another approach is trying to map natural task description to robot actions. One example is
RT-2, referred to as Vision-Language-Action(VLA) model, which consists in co-training the model on internet-scale
data with images and actions related tasks. The position, orientation and elevation of the end effector of the robot
are tokenized like text [10]. Note that a VLA leverages instantiations of the previous LLM+VLM models plus fine
tuning these models with robot actions.
In the section 4.2 I mentioned the amazing results obtained with diffusion policies. Indeed these policies are
remarquablyrobusttoexternaldisturbancesandfasttolearn-like50trainingbyanexpertmaybeenough. Another
approach is looking for generalist robot policies (GRPs) with the belief that these generalist robot policies have the
potential to transform how robot learning research is done: in the same way that current models in NLP are almost
universally derived from pretrained large language models, future robot policies might be initialized from GRPs and
finetuned with modest amounts ofdata. This is the path followedby TRI [52] and anopen sourceinitiative referred
to as Octo [38]. It will then be coupled with a LLM to plan a sequence of tasks.
Research on using LLM to enable robots to plan a sequence of (very) simple tasks to execute verbal orders is
just beginning. this section emphasizes some promising approaches relatedto manipulation and for a quite complete
overview of robotics transformers, the reader may refer to [59].
However, there are different challenges that need to be addressed by these robotics tranformers to be widely
adopted as reliable tasks planner for manipulation and a general purpose tasks planner:
• Long-horizon, multi-stage task planning requires reasoning over extended periods, which is a challenge for
LLMs. Tomitigatethis issue,promptingschemesChain-of-Thoughtsdistilcomplexproblemsintointermediate
reasoning steps with the aid of exemplars, facilitating effective reasoning [56];
• Tackling large-scale problems often leads to issues like hallucination or failing to consider important details,
rendering their plans ineffective or error-ridden;
8• Workingwithmulti-steptasksinroboticsalsoinvolvesdealingwithuncertaintiesandchangesintheenvironment
whichmayleadatagiventimetothefailureofataskandtriggerareplanningprocess. Thisawellknowproblem
for those who address tasks planning in the pre-transformer era;
• The size of modern VLMs can reach tens or hundreds of billions of parameters and it is impossible to directly
run such models on embedded GPUs available on the current humanoid robots - and this statement will last
for years. The currentsolution in order to enable efficient real-time inference consists in deploying these VLMs
in a multi-TPU cloud service and querying this service over the network. Consequences will be addressed in
Section 5.
4.4 Safety
Safety is mentioned in this section because it is a challenge for humanoid robots which is not really addressed yet.
InorderforbothAGVsandAMRstonotharmpeopleordamagethesurroundinginfrastructuresafetystandards
havebeendefined. AGVsandAMRsareequippedwithsafetysensorsthatwillpreventthesedevicesfromcontacting
humans andobjects. The AGVSafety Standardfor the United States is referencedas ANSI/ITSDF B56.5-2019,and
for the European Union it is referenced as EU EN ISO 3691-4:2020
To summarize, the key risk mitigation measures necessary for an automatic guided vehicle are divided into two
principal categories:
• Active Risk mitigation measures;
• Passive Risk mitigation measures.
The key AGV active risk mitigation measures are:
• Safety laser scanner with collision avoidance system;
• Pressure-sensitive bumper guards;
• Safety PLC.
The key important passive risk mitigation measures are:
• Emergency stop mechanisms;
• Warning lights – flashing and rotating lights;
• Audible warnings/ klaxon alarm signals;
• Awareness and safety signage on AGVs.
It is too early to know if part or totality of these measures will be applied to humanoid robots. However,there is
a unique feature that humanoid robots have compared to AGV and AMR: they can fall.
As soon as humanoid robots will be able to interact with humans in the workplace, it will be mandatory for the
humanoids to fall safely. It means:
• Do no harm humans and damage the surrounding infrastructure;
• Do not damage the humanoid itself.
To quote Jerry Pratt [3]:
it’scriticaltofallsafely,tosurviveafall,andbeabletogetbackup. Peoplefall—notveryoften,butthey
do—andtheygetbackup. Andtherewillbe timesinalmostanyapplicationwherethe robotfallsforone
reason or another and we’re going to have to just accept that. I often tell people working on humanoids
to build in a fall behavior. If the robot’s not falling, make it fall! Because if you’re trying to make the
robot so that it can never fall, it’s just too hard of a problem, and it’s going to fall anyway,and then it’ll
be dangerous.
I think falling can be done safely. As long as computers are still in control of the hardware, you can do
very graceful, judo-style falls. You should be able to detect where people are if you are falling, and fall
9awayfromthem. So,Ithink wecanmaketheserobotsrelativelysafe. Thehardestpartoffalling,Ithink,
is protecting your hands so they don’t break as you’re falling. But it’s definitely not an insurmountable
problem.
Safety is not really addressed by humanoid suppliers yet but will be required if humanoids are allowed to work
with human workers.
5 Operation and Maintenance
Why talking about operation and maintenance? Humanoid robots are very complex, fragile machines, and there are
no statistics available on the breakdowns and their frequencies that may occur. Moreover as soon as they will be
deployed in warehouses, companies will have to roll out all the necessary workforce and infrastructure to guarantee
that the humanoids are fully operational and deliver the work in accordance with contracts signed with customers.
5.1 Field engineers
As soon as the company is selling and deploying humanoid robot on a customer site, operation and maintenance
workforces specially trained will be involved. If the company is going for a Robotics as a Service (RaaS) business
model and it scales their humanoid fleets, support staff will need to do the same. Each customer, no matter the
number ofrobots,willruninto errorswith their robotsthatwillrequirehumansupportfromthe RaaSprovider. Let
us name a few:
• Considering dextereous hand, if a hand is designed with cable-driven actuators, it is known that cable drives
typically are not as durable and can be more finicky to keep calibrated;
• Motors are also quite fragile, as overloading them or operating in sensitive environments (dirt or dust which is
common in logistics worplaces)can quickly lead to them being destroyed;
• Force/torque (F/T) sensors issues: in locomotion, the stability assessment is done using an estimation of the
Center of Pressure (CoP) obtained by F/T sensors. It means the sensors must be robust enough to not only
carry the weight of the robot but also endure ground impact forces during walking. The drawback of currently
available F/T sensors is that the measurements are done using transducers (e.g. strain gauges), with a direct
couplingoftensioninducedbyforcesgoingthroughthesensor. Itleadstolong-termdeformationswhichrequires
recalibration in order to prolong the usability of a sensor [17].
Indeed, most of the time a subscription-based model for robots means customers don’t own the robot and thus
all costs of the maintenance of the hardwarefalls to the RaaSprovider. Therefore the company will have to hire and
train these staff members that will grow as the company does.
5.2 Integration with existing system
Companies will have to customize their software or hardware to meet the specific requirements of the customer’s
system, such as a warehouse management system (WMS). This can involve significant engineering, integration and
testing costs. This topic is addressed in section .
5.3 Need for cloud services
As mentionedinSection4.3ifsomecompanieswanttodeploygeneralisthumanoidrobots,theywillhavetorelyona
roboticstransformer/VLM/VLA.Suchmodelcannotrunonembedded GPUsandmustbe deployedona multi-TPU
cloud service. It raises several issues:
• The need to develop a dedicated protocol to exchange data between the robot and the VLM;
• Quarantee at least a realtime compatible with the tasks assigned to the humanoid;
10• Due to the protection of data such as the General Data Protection Regulation (GDPR) in EU, it may be
impossible to transfer data gathered by humanoids back to a cloud center in USA. Companies may be forced
to deploy a data center in different regions of the globe which lead to additional costs that may impact their
revenues.
• Another burden not addressed in this article will be the need to set up secure communication [9].
5.4 Communication
Reliable communication channels will be required:
• By companies that will need a connection to a remote data center either to download new task models in the
humanoid or upload data to train a LLM;
• By companies that will need a internet connection to allow an operator to teleoperate humanoids;
• By companies to remotely monitor the status of the humanoids;
• Bycustomerswithaccesstoinformationabouttheirfleets’performanceandstatuswhichiscriticalforcustomers
to understand how their humanoid robots are functioning and to identify any issues;
• By customers with access to a customer care front office.
Providing customers with a seamless and convenient experience is essential to a successful deployment- however,
integrating various communication channels can be complicated and costly, requiring significant IT resources and
infrastructure at the cost of the companies. And this cost will increase as the fleet of robots (and customers) will
increase.
5.5 Supervision
Why remote supervision? Because humanoids are supposed to be autonomous 24/7 but they can get stuck for
different reasons. However, if the company promised a 24/7 service, and the robots are autonomous let us say 60%
of the working time, the company needs to set up a remote supervision – if agreed with the client - to comply with
contractual commitments.
If humanoids offer a high availability and are equipped with an auto-diagnostic solution, it can greatly mitigate
the need of an active remote supervision.
Hirestafftoremotelymonitorhumanoidsisrelatedtothedegreeofautonomy/robustnessofthedeployedsolution.
The less robust the humanoids are, the more staff you need.
This aspect has to be quantified at the early stage of the project because the burden of the cost induced by the
supervision has to be included in the associated financial offer.
Remote supervision involves:
• R&D team has to develop and maintain supervision tools or purchase one in last resort ;
• Dedicated Hardware (servers) and Ethernet connection – probably redundant setup will be required. Another
option is to go for a cloud solution if the bandwidth/response time requirements are compliant with the SLA
agreed by the client;
• Hire and train a team to supervise the different sites.
Remote supervision is an important indicator of the robustness/availability of a company solution :
• Frequency of opened tickets provide an indication about the maturity of an “on site solution”;
• Frequencyoftheinterventions,i.e. helparobotwhichisstuckinanundesirablesituation,provideanindication
about a use case flaw;
• If the team growssignificantly with the number ofsites, it means that there are (basic)issues which need to be
solved still.
116 Can we outlook the best practices to deploy humanoid robots on
industrial sites?
6.1 Pilot: milestones and challenges
The deployment of new robotic technology can be decomposed into several stages:
• There is a first step which consists of evaluating whether this technology is sufficiently mature to be used in an
industrial environment.
To do this, the customer in contact with the robot supplier identifies a very specific task. A simple task like
transporting an object from point A to point B or sorting objects for example. The humanoid is placed in a
secure perimeter to avoid accidents with customer staff. Another important benefit of a secure perimeter is to
ensure that the environment does not change.
This makes it possible to evaluate repeatability, the rate of failure to complete the task, humanoid failures,
the number of times the humanoid falls (which is a severe incident in terms of safety), the number of manual
or teleoperation handovers when the humanoid is stuck, for example. The goal is to measure the industrial
2
maturity or reliability of the humanoid. In this phase, there are often hardware and or software modifications
that are made to the humanoid to correct bugs and improve its robustness. Meantime, the company gather a
lot of information for evaluating maintenance costs.
The evaluation is also critical for both parties to evaluate what we often call ”hidden costs”:
– Time to modify or adapt customer infrastructure;
– Network dependence, integration with other physical and software customer systems;
– Time to configure and/or train the robot;
– Time to train employees to setup and use the humanoid;
– Eventually time spent creating and editing 3D maps for humanoid localization and navigation.
Finally measure the acceptance of the robot by the workforce is also a critical factor. If the humanoid is not
intuitive and user-friendly, they will be frustrated as they spend countless hours figuring it out or trying to get
help from the company.
This stage is an opportunity for the company to improve the humanoid setup, humanoid operation in its
environment and maintenance directives.
• If the evaluation is satisfactory, the customer identifies whether there is a gain or not and which one:
– Productivity gain: the humanoid costs less than an employee and it works faster and/or longer;
– Health and social gain: the humanoid carries out tiring or very repetitive and unrewarding tasks, which
results in fewer accidents, less sick leave or a reduction in turnover.
• If the customer considers that the gain is significant, then the deployment phase can begin. It is very likely
that the humanoids will operate in secure environments initially to preserve the safety of employees, the time
necessary to certify that the humanoid robots are in compliance with the safety standards of the industry in
question.
• The last step in deployment consists in taking the humanoid out of the secured perimeter and putting it to
work in the factory with other human employees. This step is very complex because the humanoid must be
able to plan a sequence of tasks, including navigation, while facing a dynamic and noisy environment, it must
pay attention to workplace signs, stay inside walkways, stay alert for vehicle traffic (forklifts, order pickers...)
and so on. On the other hand, the humanoid robots must be safe for surrounding workers and compliant with
safety standards which has not been defined yet - see section 4.4 for more details.I do not think that this step
is reachable in a near future.
2Reliabilityistheprobabilityofamachineoperatingwithoutfailure.
12Following these steps is very important. Many robotics startups fail because they deployed robots on many
customer sites without having completely tested their robots. Obviously, the robots did not work as expected,
environmentalconstraintswhichwerenotanticipated,faulty mechanics. They werethen necessaryto sendpersonnel
onsites,carryoutmechanicalrepairsand/orchangesofpartsontherobots. Ifthishappensrepeatedlyacrossmultiple
sites,thecostofmaintenancecanquicklybecomeuncontrollableandputthestartupinadangerousfinancialsituation
without counting on the degradation of the startup’s brand image.
6.2 Robot robustness and robot supplier pragmatism : two examples
When a company wants to address a market quickly, a strategy consists of adapting its product to the constraints
imposed by the environment and targeted use cases, which sometimes means that it is necessary to make choices
about the design of the robot (which can also be considered as sacrifices for researchers). For instance, if the robot
has to handle boxes of those shapes and sizes, are unique and known, rather than designing a universal hand that
can theoretically take any object, but with a non controllable probability of success, it is wiser to design a gripper
adapted to the use case that will work at least 95%of the time. This point leads us to the robot reliability. Indeed a
pilot will have to pass the different steps described above and reliability is a key asset for the company.
6.2.1 Agility Robotics
Let’s take the case of Agility Robotics which, in my opinion, has a good approach to the market.
In October 24th 2023, Agility Robotics announced that Amazon will begin testing Digit for use in their opera-
tions [47].
Digit will first be tested at Amazon’s robotics research and development facility just south of Seattle. Amazon’s
initial use for Digit is to recycle empty totes. It takes totes off a stationary storage rack and carries them to a
conveyor. From available images 3, we can make the following remarks:
• The humanoid operates in a secure perimeter closed to Amazon employees;
• One can notice that there are ”April tags” probably on one hand to localize the robots and on the other hand
to label the space (in front of shelves, different positions along the conveyor...);
• Humanoids operate in a space clear off obstacles.The environment is structured so that the robots will be able
to carry out the task without contingencies;
• Thetaskisprobablyeventbased,atoteispushedinaninclinedmobileshelve,asensormaydetectthepresence
ofthetotewhichtriggersadigitrobottomoveinfrontoftheshelve,graspthetoteandcarryittotheconveyor
belt.
In December, Agility Robotics announced a new pilot with GXO Logistics [14] at Spanx’s facility in Flowery
Branch, Ga. In the pilot, Digit is moving totes off of autonomous mobile robots (AMRs) and onto a conveyor. The
AMRs brings the packedtotes to a transfer station. Digit uses its perception system to detect the AMR has arrived.
Next, Digit picks up a tote off the top or bottom shelf, carriesit over,and places it onto the conveyor. In the future,
Agility said Digit will also communicate directly with the AMR fleet manager.
From available images 4, we can make the following remarks:
• The robots operate in a secure perimeter closed to Amazon employees;
• The task is slightly more complex than the Amazon pilot, in the sense that there is an AMR that carries totes
filled with products. Once the ARM reached it destination, a message is probably sent through the back office
to the Digit. In this case Digit will carry ”heavy” totes from the AMR to the conveyor. It will be interesting
for Agility Robotics to observe how the actuators of both arms and shoulders will behave over a long run as
well as the ability of the dynamic walking gait to compensate for the additional weight;
• One can notice that there are ”April tags” again so the same algorithms are probably reused;
• Robots operate in a space clear off obstacles.The environment is structured so that the robots will be able to
carry out the task without contingencies.
13Figure 3: Digit to Assist Amazon Employees with Repetitive Tasks - Credit: Amazon.
Figure 4: Digit is moving full totes from mobile robots to a conveyor. Credit: Agility Robotics.
The tasks are quite simple and rely on a set of robust and proven algorithms such as: digit walking gait has
been developed,improvedandtestedfora decade,perceptionmaybe achievedby adeepnettrainedtorecognizeand
localizetotes,the3DenvironmenthasprobablybeenscannedandcreatedbyaSLAMalgorithm,efficientlocalization
algorithms are available to localize the robot and may be re-initializedwith the apriltags, grasping with a dedicated
gripper has been intensively tested before going for the pilot.
6.2.2 Enchanted Tools
Enchanted Tools is French startup that’s targeting application spaces like hospitals, nursing homes, and medical
clinics with its Mirokai robot, a humanoid torso mounted on a single sphericalwheel. The cobot takes an interesting
approach with its animated face built around a compact rear projection system. It will interact directly with people
and can carry small loads, around 3kg, using a pair of arms.
EnchantedToolsoffersapragmaticsolutionforgraspingobjects. Therobotcanonlygrasphandleswithaspecific
shape. These handles, as can be seen in the figure 5, are attached to various objects such as trays or trolleys which
will be used by the robot in applications for which it was designed. The robot has been trained to recognize and
14grasp these handles only.
Enchanted tools has also developed the concept of ”rune”. It is a connected object which communicates via
Bluetooth with the robot. The rune can also be located by the robot and can be programmed to be labeled as an
object such as vase, the tray number xxx, a place like bedroom-xxx or a kitchen for instance.
Figure 5: To be written. Credit: Enchanted Tools.
These runes can be used in the following way:
• A rune is placed in a room with the label room-xxx and one can ask the robot to move a tray in room-xxx.
The robot locates the rune with the label room-xxx and plans a path to the rune’s position;
• A rune can be fixed on a handle with the label tray-yyy, and this handle is fixed on a tray. One can ask the
robot to pick up the tray-yyy aand bring it to room-yyy.
This robot is being tested in Parisian hospitals to help hospital staff to move stuff around. With this device, the
deployment of the robot is faster and above all more efficient.
6.3 Conclusion
Myclaimhereiswhenarobotsupplierwantstosellitsrobotandbringittoindustrialenvironmentandcomplainwith
productivity constraints,it must rely on mature technologies,it might not be state-of-the-art,but these technologies
have been thoroughly tested so that you can trust them. The robot supplier must show with the pilot(s) that its
robot is capable of achieving the task at least 95%of the working time and this is very hardto accomplishwith such
complex machinery. This is not a proof of concept in a laboratory for pursuing grants. This does not prevent the
company and its R&D team from developing new algorithms or exploring possible applications of generative AI for
instance and maintain a roadmap to incomporate them in the future.
7 Conclusion
In the space of a year, a large number of companies specializing in humanoid robots showed up on the interna-
tional scene. This is an unprecedented situation because these robots are complex, requiring expertise ranging from
mechanics to artificial intelligence, and the objective of these companies is to commercialize these robots by 2025!
We are witnessing strategic movements in the high-tech industry. For example, OpenAI is betting on humanoid
robots [35] as a source of profit in the medium term. OpenAI is partnering with Figure to equip Figure 01 with
high-levelvisualbasedsituationanalysiscapabilities,taskplanningandlanguageintelligenceandisalsoashareholder
in 1X a competitor of Figure. NVIDIA has recently announced ProjectGR00T (Generalist Robot 00 Technology), a
general-purposefoundation model for humanoidrobots, acts as the mind of robots, making them capable of learning
skills to solve a variety of tasks [5].
15In this article we studied 12 companies. We also reviewed the technical challenges posed by these machines, the
operationand supportaspects once they are deployedanddetailed currentpilots whichtell us about the maturity of
these humanoids and the companies’ strategy.
Therearecompanies thatwantto conquermarketshareatafastpace andthis is why their humanoidrobotsrely
on proven technologies, the reliability of which is currently tested with pilots.
OthercompanieschoosetorelyoninnovativetechnologiessuchasgenerativeAI,stillattheresearchstage. These
technologiestheoreticallymakeitpossibletoaddressmarketsotherthanlogisticssuchashealthcare,householdservice
and/or companionship, but they have not proven their reliability or robustness yet. In addition, these technologies
require complex infrastructures to operate. They will be more expensive to implement and maintain. It is a bet on
the future, risky by definition.
Ifwetaketheanalogywithself-drivingcars,manyofthesecompaniespromiseageneralpurposeshumanoidrobot
for 2025 or 2026 but it is very likely, as for autonomous cars, that the promises will not be kept and we will have to
wait 5 or 10 years to see this type of robot actually operational. However, it is likely that these robots will work in
secure perimeters for a few years, while regulation entities define security standards for humanoid robots.
The next few years will be really exciting to follow, there are already pilots going on and we are seeing some
humanoids at work in warehouses in secure perimeters. Some pilots will be successfull and trigger the ”mass”
production of these humanoids. While other pilots will highlight the current limitations of these prototypes, which
will initiate new researchto resolve them. It will lead to more agile, reliable and efficient humanoid robots in a near
future.
16Contents
1 Introduction 1
2 General Considerations 1
2.1 Humanoid Robots because Shortage of labor, really? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2.2 After AI, China is betting on humanoid robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2.3 What kind of business model for humanoid robots? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3 Which Are The Competitors in this new race? 3
3.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.2 Panorama and analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
4 Technical Challenges 6
4.1 Mechanics/actuators/control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4.2 End-to-end grasping with dextereous hands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.3 Spatial reasoning and tasks planning (for manipulation) . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.4 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
5 Operation and Maintenance 10
5.1 Field engineers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5.2 Integration with existing system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5.3 Need for cloud services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5.4 Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
5.5 Supervision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
6 Can we outlook the best practices to deploy humanoid robots on industrial sites? 12
6.1 Pilot: milestones and challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
6.2 Robot robustness and robot supplier pragmatism : two examples . . . . . . . . . . . . . . . . . . . . . 13
6.2.1 Agility Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
6.2.2 Enchanted Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
7 Conclusion 15
A Short introduction of the competitors 22
A.1 Appolo A1 from Apptronik . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A.2 Phoenix from Sanctuary AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.3 Figure 01 from Figure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.4 RAISE-A1 from Agibot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A.5 Optimus from Tesla . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
A.6 H1 from Unitree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
A.7 GR1 from Fourier Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
A.8 NEO from 1X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
A.9 Digit from Agility Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
A.10XP5 from xpeng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
A.11Kepler from Kepler ExplorationRobotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
A.12CL-1 from LimX Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
17References
[1] Evan Ackerman. Agility’s latest digit robot prepares for its first job. https://spectrum.ieee.org/agility-r
obotics-digit,2023.
[2] Evan Ackerman. Apptronik developing general-purpose humanoid robot: Apollo will be a practical bipedal
platform that can do useful tasks. https://spectrum.ieee.org/humanoid-robot-apptronik-apollo/,2023.
[3] Evan Ackerman. Figure unveils its humanoid robot prototype. https://spectrum.ieee.org/figure-humanoi
d-robot-2665982283,2023.
[4] Evan Ackerman. Sanctuary’s humanoid robot is for general-purpose autonomy. https://spectrum.ieee.org/
sanctuary-humanoid-robot,2023.
[5] Evan Ackerman. Nvidia announces gr00t, a foundation model for humanoids. https://spectrum.ieee.org/n
vidia-gr00t-ros,2024.
[6] MuhammadAhmad Ali, Hyungjun Park,andC.S. GeorgeLee. Closed-forminversekinematic jointsolutionfor
humanoid robots. 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 704–709,
2010.
[7] Loz Blain. Figure’s brett adcock on the practicalities of humanoid robot workers. https://newatlas.com/rob
otics/figure-brett-adcock-interview-practicalities-humanoid-robot-workers/,2023.
[8] Loz Blain. Gr-1 general-purpose humanoid robot will carry nearly its own weight. https://newatlas.com/rob
otics/fourier-gr1-humanoid-robot/,2023.
[9] Alessio Botta, Sayna Rotbei, Stefania Zinno, and Giorgio Ventre. Cyber security of robots: A comprehensive
survey. Intelligent Systems with Applications, 18:200237,2023.
[10] AnthonyBrohan,NoahBrown,JusticeCarbajal,YevgenChebotar,KrzysztofChoromanski,TianliDing,Danny
Driess, Chelsea Finn, Peter R. Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan,
Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan C.
Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Sergey Levine, Henryk Michalewski, Igor Mordatch,
Karl Pertsch, Kanishka Rao, Krista Reymann, Michael S. Ryoo, Grecia Salazar, Pannag R. Sanketi, Pierre
Sermanet,JaspiarSingh,AnikaSingh,RaduSoricut,HuongTran,VincentVanhoucke,QuanHoVuong,Ayzaan
Wahid, Stefan Welker, Paul Wohlhart, Ted Xiao, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-
action models transfer web knowledge to robotic control. ArXiv, abs/2307.15818,2023.
[11] Justin Carpentier, Guilhem Saurel, Gabriele Buondonno, Joseph Mirabel, Florent Lamiraux, Olivier Stasse,
and Nicolas Mansard. The pinocchio c++ library – a fast and flexible implementation of rigid body dynamics
algorithms and their analytical derivatives. In IEEE International Symposium on System Integrations (SII),
2019.
[12] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric A. Cousineau, Benjamin Burchfiel, and Shuran Song.
Diffusion policy: Visuomotor policy learning via action diffusion. ArXiv, abs/2303.04137,2023.
[13] ChengChi,ZhenjiaXu,ChuerPan,EricCousineau,BenjaminBurchfiel,SiyuanFeng,RussTedrake,andShuran
Song. Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots. In arXiv, 2024.
[14] Steve Crowe. GXO logistics putting digit humanoid to test. https://www.therobotreport.com/gxo-logisti
cs-putting-digit-humanoid-to-test/,2023.
[15] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel
Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor
Mordatch,andPeterR.Florence.Palm-e: Anembodiedmultimodallanguagemodel.InInternationalConference
on Machine Learning, 2023.
18[16] LimX Dynamics. LimX dynamics unveils dynamic testing of humanoid robot, achieving real-time perceptive
stair climbing and more. https://medium.com/@limxdynamics/limx-dynamics-unveils-dynamic-testing-
of-humanoid-robot-achieving-real-time-perceptive-stair-51d37b0cc6a5,2023.
[17] Grzegorz Ficht and Sven Behnke. Bipedal humanoid hardware design: a technology review. Current Robotics
Reports, 2:201 – 210, 2021.
[18] Richard Fikes and Nils J. Nilsson. Strips: A new approach to the application of theorem proving to problem
solving. Artif. Intell., 2:189–208,1971.
[19] Ankur Handa, Karl Van Wyk, Wei Yang, Jacky Liang, Yu-Wei Chao, Qian Wan, Stan Birchfield, Nathan D.
Ratliff, andDieter Fox. Dexpilot: Vision-basedteleoperationofdexterous robotic hand-armsystem. 2020 IEEE
International Conference on Robotics and Automation (ICRA), pages 9164–9170,2019.
[20] Brian Heater. Openai-backed 1x raises another $100m for the race to humanoid robots. https://techcrunch.
com/2024/01/11/openai-backed-1x-raises-another-100m-for-the-race-to-humanoid-robots/,2024.
[21] HSE. Transportationand storagestatistics in greatbritain, 2022. Technicalreport, HSE, 2022. https://www.h
se.gov.uk/statistics/industry/transportation.pdf.
[22] Krystal Hu. Bmw taps humanoid startup figure to take on tesla’s robot. https://www.reuters.com/business
/autos-transportation/bmw-taps-humanoid-startup-figure-take-teslas-robot-2024-01-18/,2024.
[23] Eric Jang. Data collection for embodied learning and smart behavior. https://www.1x.tech/discover/data-
collection-for-embodied-learning,2023.
[24] Eric Jang. All neural networks.all autonomous. all 1x speed. https://www.1x.tech/discover/all-neural-n
etworks-all-autonomous-all-1x-speed,2024.
[25] Steven Jens Jorgensen and Ravi Bhadeshiya. Effective virtual reality teleoperation of an upper-body humanoid
with modified task jacobiansandrelaxedbarrierfunctions for self-collisionavoidance. In Workshop on Horizons
of an Extended Robotics Reality, 2022.
[26] Simon Kalouche. Goat: A legged robot with 3d agility and virtual compliance. 2017 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pages 4110–4117,2017.
[27] Benjamin Katz, Jared Di Carlo, and Sangbae Kim. Mini cheetah: A platform for pushing the limits of dynamic
quadrupedcontrol. 2019 International Conference on Robotics and Automation (ICRA),pages6295–6301,2019.
[28] Kepler. Ces 2024 spotlight: Kepler’s humanoid robot launch gains international recognition. https://www.prn
ewswire.com/news-releases/ces-2024-spotlight-keplers-humanoid-robot-launch-gains-internation
al-recognition-302040175.html,2023.
[29] John Koetsier. Apptronik has a totally different approachto building humanoid robots. https://johnkoetsie
r.com/humanoid-robots/,2023.
[30] John Koetsier. Sanctuary ai humanoid generalpurpose robot: a deep dive with ceo geordie ross. https://john
koetsier.com/sanctuary-ai-humanoid-robot/,2023.
[31] Shuang Li, XiaojianMa, HongzhuoLiang, MichaelG¨orner,Philipp Ruppel, Bin Fang, Fuchun Sun, and Jianwei
Zhang. Vision-based teleoperation of shadow dexterous hand using end-to-end deep neural network. 2019
International Conference on Robotics and Automation (ICRA), pages 416–422,2018.
[32] Yinlin Li, Peng Wang, Rui Li, Mo Tao, Zhiyong Liu, and Hong Qiao. A survey of multifingered robotic ma-
nipulation: Biological results, structural evolvements, and learning methods. Frontiers in Neurorobotics, 16,
2022.
[33] Rita Liao. Why this autonomous vehicle veteran joined a legged robotics startup. https://johnkoetsier.com
/sanctuary-ai-humanoid-robot/https://techcrunch.com/2023/11/06/why-this-autonomous-vehicle-v
eteran-joined-a-legged-robotics-startup/,2023.
19[34] Philippe Lucidarme, Nicolas Delanoue, Franck Mercier, Yannick Aoustin, Christine Chevallereau, and Philippe
Wenger. Preliminary survey of backdrivable linear actuators for humanoid robots. In ROMANSY 22 – Robot
Design, Dynamics and Control, pages 304–313.Springer International Publishing, 2019.
[35] Jyoti Mann. Sam altman’s openai wants to get humanoid robots talking. here’s why. https://www.businessi
nsider.com/openai-bets-big-on-humanoid-robots-with-figure-ai-2024-2,2024.
[36] GillianTanMarkGurmanandBloomberg.Jeffbezosandnvidiajoinopenaiandmicrosoftinbackingahumanoid
robot unicorn valued at $2 billion, sources say. https://fortune.com/2024/02/23/jeff-bezos-nvidia-open
ai-microsoft-robot-unicorn-figureai-funding-round/,2024.
[37] OSU news. Cassie sets a guinness world record. https://agilityrobotics.com/news/2022/cassie-sets-a-
guinness-world-record,2023.
[38] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey
Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Dorsa Sadigh, Chelsea Finn, and Sergey
Levine. Octo: An open-source generalist robot policy. https://octo-models.github.io,2023.
[39] Mike Oitzman. Figure 01 humanoid takes first public steps. https://www.therobotreport.com/figure-01-h
umanoid-takes-first-public-steps/,2023.
[40] Mike Oitzman. Unitree teases new h1 humanoid robot. https://www.therobotreport.com/unitree-teases-
new-h1-humanoid-robot/,2023.
[41] Mike Oitzman. Watch boston dynamics’ atlas humanoid handle automotive struts. https://www.therobotrep
ort.com/boston-dynamics-atlas-humanoid-handle-automotive-struts/,2024.
[42] South China Morning Post. China plans to be a world leader in artificial intelligence by 2030. https://multi
media.scmp.com/news/china/article/2166148/china-2025-artificial-intelligence/index.html, 2018.
originaldocumentinchinesehttps://www.miit.gov.cn/jgsj/kjs/wjfb/art/2023/art_50316f76a9b1454b89
8c7bb2a5846b79.html.
[43] South China Morning Post. China says humanoid robots are new engine of growth, pushes for mass production
by 2025 and world leadership by 2027. https://www.scmp.com/news/china/politics/article/3240259/chi
na-says-humanoid-robots-are-new-engine-growth-pushes-mass-production-2025-and-world-leaders
hip, 2023.
[44] JerryE.Pratt. Exploiting inherent robustness and natural dynamics in the control of bipedal walking robots. Phd
thesis, Massachusetts Institute of Technology, Cambridge, MA, USA, 2000., 2000.
[45] PROROBOTS. Wrc2023-china’slargestrobotexhibition—robotsandtechnologiesattheexhibitioninchina.
https://www.youtube.com/watch?v=8cJV08MTwA0,2023.
[46] Alireza Ramezani, Jonathan W. Hurst, Kaveh Akbari Hamed, and Jessy W. Grizzle. Performance analysis and
feedback control of atrias, a three-dimensional bipedal robot. Journal of Dynamic Systems Measurement and
Control-transactions of The Asme, 136:021012,2014.
[47] Agility Robotics. Agility robotics broadens relationship with amazon. https://agilityrobotics.com/news/2
023/expanded-partnership-amazon,2023.
[48] Mingyo Seo, Steve Han, Kyutae Sim, Seung Hyeon Bang, Carlos Gonzalez, Luis Sentis, and Yuke Zhu. Deep
imitation learning for humanoid loco-manipulation through human teleoperation. 2023 IEEE-RAS 22nd Inter-
national Conference on Humanoid Robots (Humanoids), pages 1–8, 2023.
[49] Kenneth Shaw, Shikhar Bahl, and Deepak Pathak. Videodex: Learning dexterity from internet videos. In
Conference on Robot Learning, 2022.
[50] Jill Shen. Xpeng tech day 2023: first mpv, self-driving timeline, flying cars, and humanoid
robots. https://technode.com/2023/10/25/xpeng-tech-day-2023-first-mpv-self-driving-timeline-f
lying-cars-and-humanoid-robots/,2023.
20[51] IHMC Development Team. IHMC open robotics software. https://github.com/ihmcrobotics/ihmc-open-r
obotics-software,2016.
[52] TRI Team. Toyota research institute unveils breakthrough in teaching robots new behaviors. https://www.t
ri.global/news/toyota-research-institute-unveils-breakthrough-teaching-robots-new-behaviors,
2023.
[53] Russ Tedrake. Robotic Manipulation. 2023.
[54] Russ Tedrake and the Drake Development Team. Drake: Model-based design and verification for robotics. htt
ps://drake.mit.edu,2019.
[55] Lirui Wang, Jialiang Zhao, Yilun Du, Edward H. Adelson, and Russ Tedrake. Poco: Policy composition from
and for heterogeneous robot learning. ArXiv, abs/2402.02511,2024.
[56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903,2022.
[57] Patrick M. Wensing, Michael Posa, Yue Hu, Adrien Escande, Nicolas Mansard, and Andrea Del Prete.
Optimization-based control for dynamic legged robots. IEEE Transactions on Robotics, 40:43–63,2022.
[58] Ziwei Xia, Zhen Deng, Bin Fang, Yiyong Yang, and Fuchun Sun. A review on sensory perception for dexterous
robotic manipulation. International Journal of Advanced Robotic Systems, 19, 2022.
[59] FanlongZeng,WenshengGan,YonghengWang,NingLiu,andPhilipS.Yu. Largelanguagemodelsforrobotics:
A survey. ArXiv, abs/2311.07226,2023.
21A Short introduction of the competitors
A.1 Appolo A1 from Apptronik
Appolo A1 From Apptronik
Robotics background (1) : The founders of Apptronik were
heavily involved in making Valkyrie, NASA’s first-ever bipedal
robot that was initially conceived for the 2013 DARPA Robotics
Challenge, while they were still working in the lab at the Univer-
sity of Texas at Austin. Actually Apptronik was created in 2016
to commercialize the work out of NASA. Since then, the company
built 13 robots.
Teleoperation(1): ApptronickdevelopedaVR teleoperationap-
proachfortheirupper-bodyhumanoidAstra[25]. Apptronickshort
to medium term approach might be hybrid autonomy, with a hu-
man overseeingfirst a few and eventually a lot of Apollos with the
abilitytostepinandprovidedirectguidancethroughteleoperation
when necessary [2].
Modularity(1)modular: Apolloismodularatthetorso. Soifyou
want to put it on wheels, you can put the upper body on wheels.
It’s modular at the end effectors, it’s also modular at the head as
well in terms of putting different sensor payloads on it.
Dexterous hands (0.6) Apptronik released a video showing Appolo A1 manipulating a tote.
Task planning (0.2): The topic has been addressed [29] but I did not see how Apptronik is
tackling these problem.
AI (0.4): It is difficult to tell because Apptronik did not mention if grasping is achieved through
deep learning or not.
Walking gait (0.6) : Apptronik released a video from which we can bet that Appolo A1 walking
speed is around 2/3 m/s.
Market (0.5)The earlyversionofthe Apollo humanoidis aimedathandling tasksinthe logistics
and manufacturing industries (palletizing, moving totes, item picking - probably interacting with
AMRs). InatalkwithJohnKoetsier[29], JeffCardenas,the CEOofApptronik,mentionedpilots
and we can expect some PR in 2024.
• Height: 173cm • walking speed: 1 m/s.
• payload: 25kg • Degrees of Freedom: 41 (hand 6DoF)
• weight: 73kg • Perception: Depth cameras
• runtime: 4hours swappable battery • target price: US$ 50k
Table 1: Short introduction of Appolo A1 from Apptronik
22A.2 Phoenix from Sanctuary AI
Phoenix the general-purpose hmanoid robot from Sanctuary AI
Robotics background (0.5): Sanctuary AI has been founded in
2018 by Geordie Rose, Suzanne Gildert and Olivia Norton. All
of them having top level position in Kindred AI, a company spe-
cialized in AI-powered manipulator robots which has been sold to
Ocado in 2020. The founders are experienced in finance, manage-
ment and in reinforcement learning.
Teleoperation (1): Sanctuary AI considers teleoperation such as
directlypiloted(orteleoperated)ifthereisatasktherobotcannot
perform autonomously, the pilot can take control of the robot to
complete the work while simultaneously helping to further train
the robot(imitation learning). Actually high-qualityteleoperation
isusedtocollectlotofdataaboutcomplexorhighvaluetasks(see
Market bullet point below) that will be used to feed a cognitive
architecture [4].
Modularity (1): Phoenix right now is either static and tethered,
whichmeansitdoesnotmoveoronafourwheelbasewhenlimited
untethered movement is required. A bipedal robot is not their top priority.
Dexterous hand (1): ”Sanctuary AI is in some ways a hand delivery mechanism” to quote his
CEO [30]. As Phoenix is willing to design a humanoid robot that is truly general-purpose, a
dexterous human-likehands is mandatory. This is why SanctuaryAI is actively workingon visual
servoing, real-time simulation of the grasping process, and mapping between visual and haptic
data.
Task planning (0.2): what Sanctuary AI wants to achieve is ”you speak to the robot where you
issue it a command and the robot has to interpret what you mean. And then in the context in
which it is in the world, execute that command for you” [30].To do so Sanctuary AI claims that
the control system (called Carbon) “enables Phoenix to think and act to complete tasks like a
person”.
AI (0.6): From what I can read, I can assume that Sanctuary AI aims at developing End-to-end
Perception-graspingdeep network either though reinforcement learning or other methods.
walking gait (0): as mentioned above, a bipedal robot is not in their priority although there are
planning to work on it.
Market/Pilot (0.5): Thanks to the unique hand dexterity of Phoenix, Sanstuary AI is going to
focus more onhigher value tasks,the ones whichare very difficult or expensive to do. This is why
Phoenix is looking very carefully at fly in, fly out jobs. Phoenix has already signed pilots through
its partnership with Canadian Tire Corporation (CTC).
• Height: 170cm • walking speed: max 1.5 m/s
• payload: max 25kg • Degrees of Freedom for the hand 20 DoF
• weight 70kg • Perception: depth cameras
• runtime: 5hours • target price: Not disclosed
Table 2: Short introduction of Phoenix from Sanctuary AI
23A.3 Figure 01 from Figure
Figure 01 from Figure
Robotics background (0.5): The CTO of Figure is Jerry
Pratt [3], a former PhD student of Gill Pratt (the man behind the
DARPA Robotics Challenge) who did his thesis on Flamingo [44].
Jerry Pratt spent 20 years at the Florida Institute for Human and
Machine Cognition (IHMC), where he led the team that took sec-
ond place at the DARPA Robotics Challenge Finals. He used to
workwithDRC Atlas,NASA’s Valkyrie,andmostrecently Nadia.
Jerry Prattis an important asset to Figure because he knows very
well how to make humanoids walking. Not only he has spent his
entirecareerdevelopingalgorithms,actuatorsandhardwareforhu-
manoidsbutheisexperiencedandhegainedadeepunderstanding
of dynamic walking.
Teleoperation (0.5): Figure relies on different technologies to
train their humanoids. Tele-operating to the robot and instructing them on how to pick up a
bin, box, or an object on the table is one of them [7].
Modularity (0): Figure is planning to sell a full humanoid robot.
Dexterous hand (0.6): from this video one can see the dexterous capabilities of Figure with its
6 DoF hands [39].
Task planning (0.7):Figure is partnering with OpenAI to demonstrate in this video that the
Figure 01 is able to understand verbal order and plan tasks.
AI (0.6):Although the dynamic walking is probably achieved with ”newtonian” control and opti-
mizationalgorithms,Figure is relying on anAI team whichis running humanoidrobots with end-
to-end neural networks performing highly complicated and dexterous tasks (”end-to-end” means
fromperceptionto grasping). The big picture is to teachrobots how to do tasks,andas the robot
fleet grows so will the training sets. Tasks and/or skills learned by a group of robots in a factory
are transferred to the other member of the fleet.
Figure claimed in this video that it took 10 hours watching videos to train their humanoid to
prepare a Coffee. It looks like that the learning strategy followed by Figure is watching videos.
This is a well done marketing video. We have to take it for granted. At least what we observe is
how fluid are the movements of arms, hands and fingers.
walking gait(0.6):Figure.ai released a video from which we can bet that Figure walking speed is
around 2/3 m/s.
Market/Pilot (0.5): Figure [22] announced a commercial agreement with BMW to supply the
automakerwithhumanoidworkersfortheirmanufacturingplantinSouthCarolina. Thisisagood
news for the company and I am eager to see what kind of tasks will be assigned to Figure 01 and
how it will perform.
• Height: 168cm • walking speed: 1.2m/s
• payload: 20kg • Degrees of Freedom: 41 (hand 6 DoF)
• weight 60kg • Perception: Not disclosed
• runtime: 5hours • target price: Not disclosed
Table 3: Short introduction of Figure 01 from Figure AI
24A.4 RAISE-A1 from Agibot
RAISE-A1 general purpose robot from Agibot
Robotics background(0.5): Peng Zhihui the founder of Agibot
joined Huawei in November 2020 through the Chinese tech giant’s
so-called“Geniuses”recruitmentprogramasanAIalgorithmengi-
neer. In 2022 he left Huawei to create Agibot. In the 2023 Agibot
HumanoidRobotRAISE-A1LaunchEventPengpresentedaquite
detaileddescriptionofA1aswellashisvisionforthecomingyears.
Asyoucanseefromthe pictureA1isverysimilartoDigit. Agibot
did not promote A1 outside of China.
Teleoperation (0): there is no mention of teleoperation in the
Agibot presentation.
Modularity (1) : Agibot is proposing the upper body of ARSE-
A1 in a static and tethered version on wheels as well.
Dexterous hand (0.6) : the vide is showing RAISE-A1 manipul-
ing objects - although the movements are not that fluid.
Task planning (0.6): Agibot has designed an AI architecture
which relies heavily on Large Language Models (LLM). They are
working on a LLM named ”WorkGPT” a large scale data pre-
trained language and image model with strong capabilities in se-
mantic understanding, logical reasoning, image recognition and code generation. The objective is
anend-to-endnaturallanguageinterface to giveorderto the robotwhichis able to transforminto
a chain of tasks.
They divide the robot’s cognitive system into four components : ”cloud deployment hyperbrain”
(WorkGPT which centralizes all the knowledge gathered by the robots fleet and is able to slove
new problems) - brain (a light Visual Language Model or VML) - cerebellum (command-level or
motion control such as MPC WBC) and brainstem (servo-level).
AI(0.3): Asmentionedabove,RAISE-A1willbeequippedwithalightVMLsoIcansayatleast
that the robot will be able to recognize and classify objects. However I cannot tell whether there
is an end-to-end deep net from perception to grasping.
walking gait (1): Agibot in the 2023 Agibot Humanoid Robot RAISE-A1 Launch Event showed
the different stage of the design and test of the biped platform and how engineers challenged the
robustness of the walking gait.
Market/Pilot (0.2): The targeted market is industrial manufacturing. However Agibot aims at
developing an ecosystem for developers to foster new applications.
• Height: 175cm • walking speed: 2m/s
• payload: 20kg • Degrees of Freedom: 49 (Hand 12 DoF +
5 passive DoF)
• weight: 53kg • Perception: depth (RGBD) cameras
• runtime: Not disclosed • price: RMB 200K
Table 4: Short introduction of RAISE-A1 from Agibot
25A.5 Optimus from Tesla
Optimus gen2 the general purpose humanoid robot from Tesla
Robotics background (0.5): Tesla is known for its electric cars
and its FSD (Full Self Driving) also called autopilot application
whichallowsitscarstobepartiallyautonomous(L2+inthejargon
ofcarmanufacturers). Teslawasthefirstcompanywhichentersthe
race with a bipedal robot named Optimus which was unveiled at
Tesla’s2022AIDay. Oneyearlater,Optimusgen2wasintroduced
atTesla’s2023AIDay. Musksaidtherobotwillbeabletooperate
tools and do useful things like carrying and manipulating objects
infactoriesandothersettings. Withoutknowingit,Teslatriggered
a race in which a large number of companies were engulfed.
Teleoperation (0): Tesla used motion capture to demonstrate
key frames and then ran them through its optimization program
for grasping and balance.
Modularity (0): Optimus has been developed as a full humanoid
robot.
Dexterous hand (0.6): Tesla posted a promotional video of Op-
timus Gen2 showing its dexterity.
Task planning (0): This topic has not been mentioned at this stage.
AI (0.6) Tesla concentrated it effort on state estimation, control, localization and path planning.
However from video we can see that Optimus is equipped with deep net achieving object recog-
nition, classification and segmentation. Elon Musk posted a video on Twitter featuring Tesla’s
Optimus demonstrating its ability to fold laundry. However, Elon Musk added in a subsequent
tweet “Important note: Optimus cannot yet do this autonomously, but certainly will be able to
do this fully autonomously and in an arbitrary environment (won’t require a fixed table with box
that has only one shirt.
Walking gait robustness(0.6): The video whichshows Optimus Gen2 walking. Movementsare
fluid but Tesla never show how robust the walking gait is with respect to disturbances.
Market(s) (0.5): As Tesla is manufacturing cars, Optimus will be able to operate tools and do
useful things like carrying and manipulating objects in factories and other settings. Nevertheless
Tesla has a significant advantage over its competitors: it has factories and set up pilots to test
Optimus gen2 is not an issue.
• Height: 173cm • walking speed: max 5 m/s
• payload: 20kg • Degrees of Freedom: 50 (hand 11 DoF)
• weight: 63kg • Perception: cameras, ultrasonic sensors
(same suite as Tesla cars)
• Battery: 2.3kilowatt-hour,52-voltbattery
pack • target price: US$ 20K
Table 5: Short introduction of Optimus from Tesla
26A.6 H1 from Unitree
H1 the general purpose humanoid robot from Unitree
Robotics background (1): Unitree is a world leader in the de-
sign,developmentandmanufacturingofquadrupedrobots. Unitree
designs its own actuators, locomotion and perception algorithms.
They haveaccumulatedalot ofexperiencewhichallowthem to go
froma quadrupedrobotto a biped robot[40] withoutmucheffort.
Teleoperation (0): H1 is not teleoperated so far.
Modularity (0): H1 has been developed has a full humanoid
robot.
Dexterous hand (0): H1 is not equiped with hands so far.
Task planning (0): This is not a topic addressed by Unitree en-
gineers at this stage.
AI (0.2): Unitree engineers focused on perception and navigation
using data from a depth camera and 3D Lidar. However we can
probablybetthatthealgorithmsregardingobjectsrecognitionand
classification deveoped for the quadrupeds are ported on H1.
Walking gait robustness (1): The video which shows the H1
humanoid robot walking in an unstructured environment is im-
pressive. Despite the fact that engineers push it, the robot dynamically stabilizes and continues
walking. This is clearly the most robust robot on the list. H1 is equipped with very powerful
electricalactuatorsdeliveringa torque upto 360N.m atthe hip - with suchactuators,H1 may be
able to walk/run at a maximum speed of 5m/s.
Market(s) (0.2): H1 is considered as a research platform by Unitree, at least for now. Therefore
the market is research labs and/or companies which wants to develop applications. Like Agibot
and Fourier Intelligence they will rely on a network of labs and a startup network to develop
industrial applications.
• Height: 180cm • walking speed: max 3.4 up to 5 m/s
• payload: 20kg • Degrees of Freedom: 18 (no hand)
• Perception: depthcamera(D435i)and3D
• weight: 47kg
Lidar (LIVOX MID360)
• Battery: 850 Wh • target price: US$ 90K
Table 6: Short introduction of H1 from Unitree
27A.7 GR1 from Fourier Intelligence
GR-1 general purpose humanoid robot from Fourier Intelligence
Robotics background(1) Fourier Intelligence is developing ex-
oskeleton and rehabilitation robotics since 2015. its RehabHub
platform offers a series of integrated physical therapy devices for
treating variousissues,fromwriststrengthgamesto handandfin-
ger grip training, all the way up to lower-body exoskeletons for
training people to walk,sit, stand, balance andclimb stairs. Their
engineers studied the human walking gait and they are familiar
with actuators and control algorithms that are necessary to build
at least the lowerpart (from toes to the hip) of a humanoid robot.
Teleoperation(0): FourierIntelligenceisnotworkingonthepos-
sibility to teleoperate GR1.
Modularity (0) : GR1 is a full humanoid robot - there is no plan
to sell the upper body only.
Dexteroushand(0.5): FourierIntelligencepostedvideosinwhich
we can GR1 with hands (11 DoF) or a simple gripper.
Task planning (0): FourierIntelligencedidnotmentionthatitis
working on that subject.
AI (0.2): It looks like that Fourier Intelligence engineers focused
onthedynamicwalkinggaitanddevelopedverybasiccapabilitiesregardingnavigation,perception
andgrasping. ThestrategyofFourierIntelligenceconsistsinproducingmaybehundredGR-1and
ship them to R&D labs worldwide where local researchers will develop algorithms to make them
more capable [8].
Walking gait (1): Fourier Intelligence published videos showing an impressive robustness of the
robot with respect to engineers pushes and kicks. Notice that the electric motors in the hips, the
largest ones, will be capable of generating up to 300 Nm of torque.
Market(s) (0.2): Fourier said it expects to sell the GR-1 for applications such as research and
education,conciergeandguiding,entertainmentandexhibition,industrialproductionandlogistics,
healthcare and rehabilitation, safety inspection, household service, and companionship.
• Height: 165cm • walking speed: max 5 m/s
• payload: 20kg • Degrees of Freedom: 40 (hand 11 DoF)
• Perception: depth cameras in head and
• weight: 55kg
torso
• runtime: not disclosed • Target price: not disclosed
Table 7: Short introduction of GR-1 from Fourier Intelligence
28A.8 NEO from 1X
Neo from 1X
Robotics background (0.5) : the Norwegian firm closed a fund-
ing round of $23.5 million in April 2023 and OpenAI was the
round’s lead [20].Given the role OpenAI has played in generative
AI’s rise over the past year. That underlying technology will al-
most certainly play an outsized role in shaping robotics’ future.
OpenAI decided to invest in 1X and will probably work closely
with the company. On the other hand, 1X is actively developing
EVEwhichisanupperbody(torso,twoarmsandahead)mounted
on wheels.
Teleoperation (1) : 1X relies on a team of (tele)operators to
train ML models allowing EVE to exhibit specific behaviors [23].
Taking over the humanoid which is stuck, by a teleoperator is also
considered.
Modularity (1) :1X is proposing EVE which is an articulated
upper body on wheels. Although 1X is showing artistic pictures
of NEO, it is likely that There will be synergies between EVE and
NEO. The AI capabilities can be transfered from one android to
the other.
Dexterous hand (0.2): 1X advertizes a video in which EVE is packing objects in a box with a
gripper.
Task Planning (0.3): As 1X is backed up by OpenAI, the company is certainly working on that
hot topic. However no videos and/or PR have been released so far.
AI (0.6): The recent note published by 1X [24] shows EVE robots handling various tasks with
End-to-end Perception-graspingneural nets trained with algorithms based on diffusion models.
Walking gait robustness (0): 1X is selling EVE but there is no videos of NEO in action.
Market/Pilot (0): 1X is still working on its humanoid robot and thus it is too soon to mention
any pilot. However 1X is willing to address industrial tasks (logistics, manufacturing), support
individuals with mobility challenges or helping the robotics community to explore fields like psy-
chology and artificial intelligence.
• Height: 165cm • walking speed: 3 m/s
• payload: 20 kg • Degrees of freedom: Not disclosed
• weight: 30kg
• Perception: Not disclosed
• runtimebetween2and4hours(depending
on the payload) • Target price: Not disclosed
Table 8: Short introduction of NEO from 1X
29A.9 Digit from Agility Robotics
digit from Agility Robotics
Robotics background (1) : Digit is the result of a long term
research starting with ATRIAS [46] developed at the Dynamic
Roboticslab,OregonStateUniversity(OSU),directedbyJonathan
Husrt. ItwasfollowedbyCassie[37]whichwasdesignedbyAgility
Robotics,thespinoffofOSU,thoseChiefRobotOfficerisJonathan
Husrt.
Teleoperation(0): AgilityRoboticsdidnotmentionifitisusing
a teleoperation mode.
Modularity (0) : Digit is sold as a full biped.
Dexteroushand(0.2): Digit’shandsaredesignedtodoonething:
move totes (plastic bins that control the flow of goods in a ware-
house). They’re not especially humanlike, and do not look fancy,
but they’re exactly whatDigit needs to do the job that it needs to
do [1].
Task Planning (0.7): in a recent video, Agility Robotics demon-
strated that it is possible to give verbal orders to digit and digit
wasable to planasetoftasksto execute the orderby using aded-
icated LLM. Although the environment is very simple, a set of towers of different heights as well
asthreeboxes,eachonewitha adifferentcolorandpictograms,andthe orderslike”movethe red
box one the lowest tower” for instance, digit was able to execute the order. For old researchers,
it may recall them STRIPS [18] the planner created for Shakey by the SRI. Research on using
LLM to enable robots to plan a sequence of tasks to execute verbal orders is just beginning, this
demonstration showed us that it is possible.
AI (0.4) : Agility Robotics relies on AI for perception. Nevertheless, I cannot tell whether the
grasping algorithms relie on C++ or on deepnet.
Walking gait robustness (1) : Digit is the result of at least a decade of research on dynamic
walking gaits.
Market/Pilot (1) : Agility Robotics is targeting logistics based tasks. These tasks like moving
totes from point A to point B that should be automatized because companies are having a lot of
troublefindingpeopletodothem[1]. AgilityRoboticsisverypragmaticinitsapproachtomarket,
it is running two pilots and we will go into details in section 6.
• Height: 175cm • walking speed: 1.5m/s
• payload: 15kg • Degrees of freedom:22
• Perception: (RGBD)depthcamerasanda
• weight 65kg
3D Lidar
• runtime: not disclosed • Target price: Not disclosed
Table 9: Short introduction of Digit from Agility Robotics
30A.10 XP5 from xpeng
XP5 from Xpeng
Robotics background (0.2) Xpeng follows in Tesla’s footsteps.
Xpengwasestablishedin2014andspecializesinthedevelopmentof
electric vehicles. Like Tesla, he developed X-NGP advanceddriver
assistance system and like Tesla every year there is Xpeng Tech
Day during which its CEO He Xiaopeng makes announcements on
upcomingproducts.DuringtheXpengTechDay2023thehumanoid
robotic prototype, the PX5, made its first public appearance [50].
Teleoperation (0): Xpeng did not mention whether it uses tele-
operation or not.
Modularity (0): Xpeng is selling a walkinghumanoidrobotonly.
Dexteroushand(1)Thisvideodemonstratedthedexterousrobot
hand with 11 degrees of freedom which performs tasks like lifting
boxes, grasping pens, and pouring water into cups. The robotic
handhas11degreesoffreedom,withtwofingersweighingonly430
grams but capable of gripping up to 1 kilogram of load. The mechanical arm has 7 degrees of
freedom, a positioning accuracyof 0.05millimeters, a maximumloadof 3 kilograms,and weighs 5
kilograms.
Task planning (0): Xpeng did not mention whether they are investigating task planning or not.
AI (0.4):Xpeng Robotics relies on AI for perception. Nevertheless, I cannot tell whether the
grasping algorithms relie on C++ or on deepnet.
Walking gait robustness(1): This videoshowedthe PX5humanoidrobotwhichcanwithstand
impact and keep steady even when kicked. Through self-developed high-performance joints, the
robothasachievedhigh-stabilitywalkingcapabilitiesandcancompleteindoorandoutdoorwalking
and obstacle crossing for more than 2 hours.
Market/Pilot (0): According to He, Xpeng is aiming to introduce its PX5 robots to factories
and stores by next year’s Tech Day event, utilizing them for tasks such as factory patrolling and
in-store product sales.
• Height: 150cm • walking speed: 1-2m/s
• payload: unknown • Degrees of Freedom: 51 (Hand 11 DoF)
• weight: Not disclosed • Perception: Depth Cameras
• runtime: 2hours • Target Price: Not disclosed
Table 10: Short introduction of XP5 from Xpeng
31A.11 Kepler from Kepler Exploration Robotics
Kepler from Kepler ExplorationRobotics
Robotics background (0.2) Despite the fact that Kepler was
showcased at CES 2024 [28], there is few information avail-
able regarding the company itself. However, in this video from
TheAIGRID one can see Kepler walking and manipulating some
objects.
Teleoperation (0) Kepler Exploration Robotics did not mention
teleoperationasatooltoeithertrainthehumanoidorapossibility
to take over in case the humanoid is stucked.
Modularity (0) Kepler is selling as a full humanoid robot.
Dexterous hand (1) Kepler Exploration Robotics designed a
hand with 12 DoF. This video shows Kepler manipulating some
objects.
Task planning (0.2) Kepler Exploration Robotics has equipped
the humanoid robot with a cloud-based multimodal large model
while simultaneously deploying a smaller, industry-specific model
on the robot itself for quicker response times [28]. It means that
engineers are probably working on task planning but not demon-
strated it.
AI (0.4) Kepler Exploration Robotics is developing its own AI stack named NEBULA enabling
the robot to interact with the surrounding environment in real time. The humanoid is equipped
with a processor providing 100 TOPS of computing performance into the NEBULA system com-
posed of visual recognition, visual SLAM (Simultaneous Localization and Mapping), multimodal
interaction, and hand-eye coordination [28].
Walking gait robustness (0.6) This video showed Kepler and its balancing ability but the
company did not disclose videos demonstrating dynamic walking capabilities of Kepler.
Market/Pilot (0.2) According to company website,Warehousing, smart inspection, automated
product lines, outdoor tasks and high risk operations are the targeted markets. Different versions
K1,S1 and D1) of Kepler will be built to address these different markets. Kepler Exploration
Robotics offers an open development platform, inviting developers and integrators to create inno-
vative solutions that leverage the robot’s advanced capabilities.
• Height: 175cm • walking speed 1-2m/s
• payload: unknown • Degrees of Freedom: 40 (Hand 12 DoF)
• Perception: sensor set Infrared binocular
• weight 85kg
3D Camera
• runtime: unknown • Target Price: Around US$ 30K
Table 11: Short introduction of Kepler from Kepler Exploration Robotics
32A.12 CL-1 from LimX Dynamics
CL-1 Biped from LIMX Dynamics
Robotics background (0.5): LimX Dynamics has been founded
in 2022 and is based in Shenzhen. They already propose X1 a
quadruped robot and W1 a wheeled quadruped robot. CL-1 the
humanoid robot was showcased in December 2023 [16]. Like Uni-
tree-seeTable6-thedevelopmentofCL-1reliesonthealgorithms
and actuators developed for the quadurped robots. Founded by a
groupofroboticsscientists,the startuphasalreadyraised200mil-
lion yuan (US$27.5 million) in angel and pre-A financing. Along
withthehiringofZhangLi,TheformerWeRidechiefoperatingof-
ficer,thestartupisalsoappointingDr. JiaPan,atenuredassociate
professoratthe UniversityofHongKong,asits chiefscientist[33].
Teleoperation (0.5): according to this video, LIMX Dynamics
uses tleoperation to train their humanoid.
Modularity (0): CL-1 Biped is selling a walking humanoid robot
only.
Dexterous hand (0): CL-1 is not equipped with hands yet.
Taskplanning(0): LIMXDynamicsdidnotmentionwhethertheyareinvestigatingtaskplanning
or not.
AI (0.4):LIMX Dynamics relies on AI for perception and objects classification and the humanoid
is not equipped with hands so far.
Walking gait robustness (0.6): This video showedthe CL-1 humanoid robot is able to walk on
differenttypes ofsurfaceandevenclimbstairs. This video demonstratesthe expertise ofthe team
inits capability to developa veryrobustanddynamic gaitfor a biped, without upper body, using
reinforcement learning.
Market/Pilot(0): LimX Dynamics humanoidrobots will be progressivelydeployedin both B2B
andB2Capplications,focusingonhazardousscenarios,high-endservices,automobilemanufactur-
ing, and in-home services [16]. At this moment, the humanoid is still in development.
• Height: Not disclosed • Walking speed: 1-2m/s
• Payload: Not disclosed • Degrees of Freedom:18 (no hand)
• Weight: Not disclosed • Perception: Depth Cameras
• Runtime: Not disclosed • Target Price: Not disclosed
Table 12: Short introduction of CL-1 from LimX Dynamics
33