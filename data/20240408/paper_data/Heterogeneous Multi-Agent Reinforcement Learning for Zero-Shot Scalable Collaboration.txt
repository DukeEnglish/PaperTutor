HETEROGENEOUSMARLFORSCALABLECOLLABORATION 1
Heterogeneous Multi-Agent Reinforcement
Learning for Zero-Shot Scalable Collaboration
Xudong Guo, Daming Shi, Junjie Yu, and Wenhui Fan
Abstract—The rise of multi-agent systems, especially the suc-
cessofmulti-agentreinforcementlearning(MARL),isreshaping
our future across diverse domains like autonomous vehicle
networks. However, MARL still faces significant challenges,
particularly in achieving zero-shot scalability, which allows
trained MARL models to be directly applied to unseen tasks
with varying numbers of agents. In addition, real-world multi-
agent systems usually contain agents with different functions
and strategies, while the existing scalable MARL methods only
have limited heterogeneity. To address this, we propose a novel
MARLframeworknamedScalableandHeterogeneousProximal
Policy Optimization (SHPPO), integrating heterogeneity into
parameter-sharedPPO-basedMARLnetworks.wefirstleverage
a latent network to adaptively learn strategy patterns for each
agent. Second, we introduce a heterogeneous layer for decision-
making, whose parameters are specifically generated by the
learned latent variables. Our approach is scalable as all the
parameters are shared except for the heterogeneous layer, and
gains both inter-individual and temporal heterogeneity at the
same time. We implement our approach based on the state-of-
the-art backbone PPO-based algorithm as SHPPO, while our
Fig. 1: Illustration of scalability and heterogeneity in
approach is agnostic to the backbone and can be seamlessly SMAC. (a) The original task without heterogeneity, where all
plugged into any parameter-shared MARL method. SHPPO
exhibitssuperiorperformanceoverthebaselinessuchasMAPPO the agents have the same strategy. (b) The original task with
andHAPPOinclassicMARLenvironmentslikeStarcraftMulti- heterogeneity,wheretheagentsformtwogroupstoattractthe
Agent Challenge (SMAC) and Google Research Football (GRF), fire(blue)andattackinthedistance(red).(c)Theunseentask
showcasing enhanced zero-shot scalability and offering insights
with a new agent without heterogeneity, where the new agent
into the learned latent representation’s impact on team perfor-
has the same strategy. (d) The new task with heterogeneity,
mance by visualization.
where the new agent can adaptively choose the strategy.
IndexTerms—Multi-agentreinforcementlearning(MARL),ar-
tificial intelligence, multi-agent system, heterogeneity, scalability,
representation learning.
Specifically, in this paper, we focus on zero-shot scalability
and the generalizability of agents’ population sizes. This
I. INTRODUCTION involves directly applying trained MARL models to other
MULTI-AGENT systems are playing more and more unseen tasks with similar settings but varying numbers of
importantrolesinfuturehumansocietywiththedevel- agentswithouttheneedforfurthertraining,whichisnecessary
opmentofartificialintelligence[1]–[5].Multi-agentreinforce- in various real-world applications. For instance, traffic flows
ment learning (MARL) has seen widespread application in fluctuate throughout the day, requiring a traffic junction co-
cooperative multi-agent tasks across various domains like au- ordination algorithm capable of transferring to scenarios with
tonomousvehiclenetworks[6]–[8],guaranteeddisplayads[9], varying numbers of vehicles.
and video games [10], significantly enhancing collaboration Moreover, agents within a team may have different strate-
among agents. With the introduction of actor-critic [11] to giesorevendifferentfunctions;thus,learningandtransferring
MARL, recent advances [12], [13] in MARL algorithms ex- heterogeneous roles in a team stands out as an important
hibit great potential for addressing more complex cooperative problem of zero-shot scalable collaboration. First, lack of
tasks in dynamic environments. Despite its successes, tradi- heterogeneity hinders agents from effectively collaborating
tional MARL methods are confined to specific environments with diverse counterparts, constraining team performance. As
where models are trained. Lack of generalizability poses a shown in Fig. 1(a), when heterogeneity is not considered, the
significant barrier to real-world deployment. agents will have similar strategies and simply form a circle
to attack in the game screenshot, while there are different
Xudong Guo, Daming Shi, Junjie Yu, and Wenhui Fan are with kinds of units with different capacities in the team. But
the Department of Automation, Tsinghua University, Beijing 100084,
in Fig. 1(b), after the introduction of heterogeneity, some
China (e-mail: gxd20@mails.tsinghua.edu.cn; shidm18@tsinghua.org.cn;
yjj23@mails.tsinghua.edu.cn;fanwenhui@tsinghua.edu.cn) agents may attack in the front to attract the fire (blue circle)
4202
rpA
5
]GL.sc[
1v96830.4042:viXraHETEROGENEOUSMARLFORSCALABLECOLLABORATION 2
while others stay back to be covered. Moreover, in zero- ity improves the team’s performance.
shot scalable tasks, teamwork roles change with team sizes We list the main contributions as follows.
(refer to our experiments in Fig. 8). An adaptive policy is
1) New network design: we introduce actor-critic-like
needed to flexibly assign and adjust agents’ heterogeneous
latent net and inference net along with corresponding
roles for better zero-shot scalability. We can implement this
losses to learn latent representations of the agents’
policybasedonheterogeneoussettings(Fig.1(d))butmayfail
strategy patterns, facilitating the parameter generation
without heterogeneity (Fig. 1(c)).
of the heterogeneous layer.
The literature has attempted to address heterogeneity and
2) Novel MARL framework: the proposed approach can
scalability issues independently. Some works [14], [15] ex-
add both inter-individual and temporal heterogeneity to
plore giving each agent a distinct model to improve hetero-
any parameter-shared MARL architecture. The flexible
geneity, but they struggle to scale with the number of agents.
and adaptive heterogeneous strategies help the agents
Theysufferfromthedimensioncurse,andtherearenotraining
scale to unseen scenarios.
processesforthenewlyaddedagents.Whileotherworks[16],
3) Superior performance: our method SHPPO outper-
[17] try to build population-invariant MARL for scalability
forms the baselines, such as HAPPO and MAPPO, on
but ignore the heterogeneity. In addition, parameter sharing
theoriginalandzero-shotscalablecollaborationtasksof
is scalable and widely used as network parameters for all
SMAC and GRF.
agents are shared, bringing benefits of training efficiency and
superiorperformance[18]–[20],which,however,alsolimitthe
diversity and temporal flexibility of the policies. Motivated II. RELATEDWORKS
by these challenges, we aim to enhance zero-shot scalable
A. MARL for Collaboration
collaborationbyintegratingbothinter-individualandtemporal
heterogeneity. Multi-AgentReinforcementLearning(MARL)hasemerged
In this paper, we propose a novel MARL framework called as a powerful paradigm for training groups of AI agents to
Scalable and Heterogeneous Proximal Policy Optimization collaborate and solve complex tasks [6]–[10]. QMIX [24]
(SHPPO) to add a heterogeneous layer to a parameter-shared addresses the challenge of credit assignment by computing
versionoftheclassicPPO[21]undermulti-agentsettings(see jointQ-valuesfromindividualagentsasavalue-basedmethod,
Fig. 2). We introduce a latent network to adaptively learn while MADDPG [12] extends the policy gradient method
low-dimensionlatentvariablestorepresentthestrategypattern DDPG [25] to multi-agent settings using centralized critics
for each agent, according to its observations and trajectories. and decentralized actors. Different from the off-policy MAD-
And then, based on the latent variable, SHPPO generates the DPG, MAPPO [20], based on on-policy PPO [21], achieves
parametersoftheheterogeneouslayerintheactornetwork.We strong performance in various MARL benchmarks. Despite
alsointroduceacentralizedinferencenettoguidethelearning significant progress, challenges in MARL persist, including
of the latent network to form a symmetrical structure as the generalizability for unseen scenarios, sample efficiency, non-
actor-criticinPPO(seeFig.2(b)).Notethatourapproachcan stationarity,andagentcommunication[26]–[29].Inthispaper,
be applied to any parameter-shared MARL backbone, and we we mainly focus on zero-shot scalability as one of the gener-
take PPO as an example to evaluate the performance in this alizability issues.
paper.
In this way, though all the network parameters except
B. Scaling MARL
for the heterogeneous layers are shared for scalability, the
heterogeneous layer for every agent is specifically designed The first step to generalize MARL is to scale the learned
to enhance the diversity of the team. In addition to the inter- MARL models to different population sizes without fur-
individual heterogeneity, the agents are able to update the ther training, which is particularly challenging when each
strategy with the change of observations during the task, so agent’s policy is independently modeled [14], [30]. Some
the agents also gain temporal heterogeneity. When transferred workstacklethisbydesigningpopulation-invariant[31],input-
to a new scenario with new agents added or removed, the length-invariant[32],andpermutation-invariant[33]networks.
latent variable is accordingly updated with the learned labor Graph neural networks [34] and transformers [17], [35] are
division knowledge. Thus the following generation of new adopted to handle the varying populations. UPDeT [16] fur-
heterogeneous layerscan transfer the learnedstrategy patterns ther decouples the observation and action space, using an
while keeping heterogeneity. importanceweightdeterminedwiththeaidoftheself-attention
By conducting extensive experiments on two classic and mechanism. In addition, curriculum learning [31], multi-task
complex MARL environments Starcraft Multi-Agent Chal- transfer learning [36], and parameter sharing [19] are also
lenge(SMAC)[22]andGoogleResearchFootball(GRF)[23], applied to scale MARL. SePS [18] models all the policies
we demonstrate the superior performance of our method over as K parameter-shared networks, and learns a deterministic
several baselines such as MAPPO [20] and HAPPO [14]. functiontomaptheagenttooneofthepolicies,whichcannot
Thanks to the heterogeneity, our approach shows better zero- be adaptively updated during the task. In sum, however, these
shotscalabilitywhendirectlytransferredtoscenarioswithvar- methodsallignoretheheterogeneityorlimittheheterogeneity
ied agent populations. Furthermore, we illustrate the learned topredefinedandfixedK policieswhenscalingMARL,which
latentspacewiththetaskprogresstoanalyzehowheterogene- may lead to ineffective labor division of the agent teams.HETEROGENEOUSMARLFORSCALABLECOLLABORATION 3
C. Heterogeneous MARL to learn this aggregated policy. Nonetheless, scalability is
hampered by the curse of dimensionality, as the state-action
To implement heterogeneity without separately modeling
space grows exponentially with the number of agents.
each agent like HAPPO [14], HetGPPO [37] and CDS [15],
existing works have delved into this much to learn the role To address this scalability issue, the Centralized Training
assignment in MARL [38]–[41]. ROMA [42] tailors indi- with Decentralized Execution (CTDE) paradigm [12] com-
vidual policies based on roles and exclusively depends on bines the advantages of both DTDE and CTCE. CTDE allows
the present observation to formulate the role embedding, agents to utilize additional information during training, yet
which may not fully capture intricate agent behaviors. On duringexecution,eachagentactsindependently,anddecision-
the other hand, RODE [43] links each role with a specific making relies solely on its own policy.
subset of the complete action space to streamline learning When agents are unable to observe the concise state of
complexity. LDSA [44] introduces heterogeneity from the the environment, commonly referred to as partial observa-
subtaskperspective,butitishardtodeterminethetotalnumber tion (PO), a Partially Observable Markov Decision Process
of subtasks by prior knowledge. Despite these efforts, the (POMDP) extends the MDP model by incorporating observa-
existing methods encounter difficulties when scaled to new tions and their conditional probability of occurrence based on
scenarios with varied population sizes as the number of roles the state of the environment [48], [49]. Within the POMDP
ishardtopre-determinetofitnewscenarios,andtakingtheIDs and CTDE paradigm, the decentralized partially observable
of agents as inputs to learn the roles makes the integration of Markov decision processes (DEC-POMDP) [50] have become
anewagentunfeasible.Therefore,achievingabalancedtrade- awidelyadoptedmodelinMARL.ADEC-POMDPisdefined
off between scalability and heterogeneity remains an ongoing by < N,S,A,O,r,P,γ >. o = O(s;i) represents the
i
challenge in MARL. local observation for agent i at the global state s. Each
agent utilizes its policy π (a |o ), parameterized by θ , to
θi i i i
III. BACKGROUND produceanactiona
i
fromthelocalobservationo i.Andagents
A. DEC-POMDP jointly aim to optimize the discounted accumulated reward
J(θ)=E [(cid:80) γtr(s,a)],whereaisthejointactionattime
We begin by providing an overview of the background a,s t
step t.
related to Markov Decision Processes (MDP) in multi-agent
systems. A Markov game [45] is defined by a tuple <
N,S,A,P,r,γ >, where N = {1,...,n} represents the set
of agents, S denotes the finite state space, A =
(cid:81)n
A
i=1 i
is the product of finite action spaces for all agents, forming B. Policy Gradient (PG)
the joint action space. The transition probability function
P :S×A×S →[0,1]andtherewardfunctionr :S×A→R Policy Gradient (PG) reinforcement learning offers the dis-
describe the dynamics of the environment, while γ ∈[0,1) is tinctadvantageofexplicitlylearningapolicynetwork,setting
the discount factor. The agents engage with the environment it apart from value-based reinforcement learning methods. PG
according to the following protocol: at time step t ∈ N, the methods aim to optimize the policy parameter θ to maximize
agents find themselves in state s∈S; each agent i selects an the objective function J(θ)=E S[V πθ(s)].
action a i ∈A i, drawnfrom itspolicy π i(·|s); theseindividual However, choosing an appropriate learning rate in rein-
actions collectively form a joint action a=(a 1,...,a n)∈A, forcement learning proves challenging due to the variance of
sampled from the joint policy π(·|s) =
(cid:81)n
i=1π i(· i|s); the environments and tasks. Suboptimal learning rates may lead
agentsreceiveajointrewardr =r(s,a)∈Randtransitionto to either slow optimization or value collapse, where updated
a new state s′ with probability P(s′|s,a). The joint policy π, parameters rapidly traverse the current region in policy space.
transition probability function P, and initial state distribution
To address this challenge and ensure stable policy op-
ρ collectivelydeterminethemarginalstatedistributionattime
0 timization, Trust Region Policy Optimization (TRPO) [51]
t, denoted by ρπ.
t introduces a constraint on the parameter difference between
In the development of multi-agent reinforcement learning,
policy updates. This constraint restricts parameter changes
various paradigms have been introduced to adapt single-agent
to a small range, preventing value collapse and facilitating
training methodologies. Initially, the Decentralized Training
monotonic policy learning.
with Decentralized Execution (DTDE) [46] paradigm is em-
Denote the advantage function as A(s,a) = Q(s,a) −
ployed, where each agent autonomously makes decisions
R, with R = (cid:80) γtr(s,a). In the training episode
and learns independently, operating without access to the t
observations, actions, or policies of other agents. However,
k, the parameter update in TRPO follows θk+1 =
argmax L(θ ,θ) s.t.D¯ (θ||θ ) ≤ δ, where L(θ ,θ) =
this approach encounters the non-stationary problem from the θ k KL k k
E[ πθ(a|s) A (s,a)] approximates the original policy gradi-
perspective of every single agent when all agents individually πθk(a|s) πθk
update their policies. ent objective J(θ) within the constraint of KL divergence.
Toenhancethestabilityofthelearningprocess,theCentral- Building upon TRPO, Proximal Policy Optimization (PPO)
ized Training with Centralized Execution (CTCE) paradigm [21] offers a simplified version that maintains the learning
was introduced [47]. This method conceptualizes the joint stepconstraintwhilebeingmorecomputationallyefficientand
policyasacentralizedpolicyandutilizessingle-agentmethods easier to implement.HETEROGENEOUSMARLFORSCALABLECOLLABORATION 4
Fig. 2: Schematics of our approach SHPPO. (a) Architecture of LatentNet. (b) Framework of SHPPO. (c) Architecture of
ActorNet with the heterogeneous layer (HeteLayer). (d) Architecture of HeteLayer. The pink blocks denote the losses. The
yellow and green bars denote different HeteLayers.
In PPO, the objective function is given by: follows:
 
L(s,a,θ k,θ)=E(cid:20) min(cid:18) ππ θθ k( (a a| |s s) )A πθk(s,a),
(1)
L Actor(θ)=E minπ πθ θi im mi im m(( aa ii || ss )) Mi1:m(s,a),
(cid:18) π (a|s) (cid:19) (cid:19)(cid:21)  k   (2)
clip
π
θθ k(a|s),1−ϵ,1+ϵ A πθk(s,a) ,
clip
ππ
θ
ii mm
im
(( aa ii || ss ))
,1±ϵMi1:m(s,a),
θim
forcingtheratio πθ(a|s) tobewithintheinterval(1−ϵ,1+ϵ). k
Thisensuresthatπ tθ hk e(a n| es) wparameterθ remainsclosetotheold where i 1:m denotes the permutation of updated m agents, θ kim
is the network parameter of agent i in training episode k,
parameter θ . m
k and Mi1:m(s,a) = (cid:81)i1:m π θi i(ai|s) (V(s,ϕ)−R) represents
i=1 πi (ai|s)
the weighted advantage in
Hθ Aki
PPO.
C. PG in Multi-agent Both MAPPO and HAPPO calculate the critic loss as
follows:
Multi-agent PPO (MAPPO) extends PPO to the multi-
L
(ϕ)=E(cid:2) (V(s,ϕ)−R)2(cid:3)
. (3)
Critic
agent scenario [20], specifically considering DEC-POMDP.
MAPPO employs parameter sharing among homogeneous In our work, we adopt the parameter-shared HAPPO as the
agents,whereagentssharethesamenetworkstructureandpa- baselineandbackboneforconstructingourapproach,SHPPO.
rameters during both training and testing. MAPPO is a CTDE
framework, with each PPO agent maintaining a parameter- IV. SCALABLEANDHETEROGENEOUS
shared actor network π
θ
for policy learning and a critic MULTI-AGENTREINFORCEMENTLEARNING
network V(s,ϕ) for value learning, where θ and ϕ are the Existing heterogeneous MARL methods such as HAPPO
parametersofthepolicyandvaluenetworks,respectively.The model each agent as an independent and different network; it
value function requires the global state and is used during suffers from its inflexibility and huge parameter costs when
training to reduce variance. the scale of the multi-agent system increases. In this section,
MAPPO faces challenges when dealing with heteroge- weproposeanovelMARLframeworkSHPPOanditstraining
neous agents or tasks and lacks the monotonic improve- methodtotacklethetrade-offofscalabilityandheterogeneity.
ment guarantee present in trust region learning. To address We try to solve this problem in another direction different
theselimitations,[14]introducedHeterogeneous-AgentTRPO from HAPPO: we add adaptive heterogeneity to one single
(HATRPO) and Heterogeneous-Agent PPO (HAPPO) algo- parameter-shared model. Specifically, as in Fig. 2(b), we
rithms. These algorithms leverage the multi-agent advantage design the new framework as two parts, latent learning and
decomposition lemma and a sequential policy update scheme action learning. First, we implement a latent learning model
to ensure monotonic improvement. HATRPO/HAPPO extend (LatentNet) to adaptively generate a latent distribution D for
single-agentTRPO/PPOmethodstothecontextofmulti-agent each agent based on its observation and memory (in practice,
reinforcement learning with heterogeneous agents. we take the hidden state of the RNN in the actor network
Building on the multi-agent advantage decomposition (ActorNet) as the memory). The learned latent distribution
lemma, HAPPO calculates the actor loss for each agent as represents the strategy pattern of the specific agent, whichHETEROGENEOUSMARLFORSCALABLECOLLABORATION 5
is further sampled and applied to the computation of the get the action a . The ActorNet for different agents shares
n
heterogeneous layer parameters for this agent. Except for the parameters, except for the heterogeneous layer, whose
the heterogeneous layer, the other layers’ parameters in the parameters are generated with each agent’s l.
ActorNet are shared by all the agents. When the number of The heterogeneous layer computes hete_f as Equation 5.
n
agents varies, the LatentNet can always generate a unique
latent distribution for each agent so that the proposed method hete_f =w ∗o_f +b . (5)
n n n n
can handle the scalability and heterogeneity at the same time.
Here,theweightsw andthebiasb areuniquefordifferent
Our approach is agnostic to the implementation of the action n n
agents, so the decision-making policies are heterogeneous
learning part, and in practice, we adopt the shared version
for them. With a decoder implemented as an MLP and two
of HAPPO as the backbone for its monotonic improvement
different linear decoders to generate w and b based on
property. n n
l , respectively. Namely, the LatentNet learns to represent
n
the strategy patterns, and the heterogeneous layer turns the
A. Scalable Latent Learning latent variable into parameters of the ActorNet. Now, the
Todesignaheterogeneouslayerforeachagent,wefirstneed agents can have distinguished styles to take action, though
to represent the agent’s strategy pattern as a low-dimensional they share most of the parameters. Also, they can change the
latent variable l as shown in Fig. 2(a). Take agent i as an style adaptively as the task goes on or team size changes. The
example,wetakea3-layerMLPastheEncodertoconvertthe execution of one time step is shown in Algorithm 1.
observation o and the memory from the RNN ht−1 at time
i i Algorithm 1 Execution of one time step t.
step t to a latent distribution. Therefore, the Encoder outputs
the mean µ and the standard deviation σ of the multi- Input: observations o and hidden states from the last time
i i
dimensional Gaussian D . Like humans, the agents’ strategy
stepht−1foralltheagents,LatentNetandActorNetsharing
i
patterns may not be exactly the same under similar situations, parameters among the agents.
so the Encoder learns a distribution instead of a vector. The
final latent variable l is generated by sampling to keep the Distributed execution
i
randomness as Computelatentvariablel i byLatentNetwiththeinputs
l
i
∼N(µ i,σ i). (4) of o i,ht i−1 for each agent.
Compute the parameters w ,b for the HeteLayer in
i i
Inspired by the classic Actor-Critic architecture [11], an each agent’s ActorNet with the input l .
i
inference net (InferenceNet) is introduced as a critic to guide Generatetheactiona foreachagentbyActorNetwith
i
the learning of the LatentNet (see Fig. 2(b)). Though the o ,ht−1 as inputs.
i i
LatentNet generates the latent variable individually for each Store the hidden state ht for the next step.
i
agent, the InferenceNet centrally learns an overall value func-
tiontoevaluatethewholeteam.Specifically,theInferenceNet Interaction with the environment
takes all the µ and σ as the inputs, together with the joint Agents execute the the actions a in the environment
globalobservationo g,andthencomputesV I astheevaluation together.
for the LatentNet. The InferenceNet is trained by supervised The environment returns the reward r and the flag
learningtominimizethedifferencebetweenV I andthereturn whether this episode is finished.
from the environment R. In another way, it judges whether
the learned latent distributions are good enough to help with
the decision-making based on global observations. Thus, the
C. Loss Formulation
LatentNet’s goal is to maximize the V .
I
It is noteworthy that all the agents share the same param- A good latent representation should be identifiable, diverse,
eters. Thanks to the CTDE architecture, during the execution and helpful for decision-making. Therefore, we design three
and transfer, we only forward the LatentNet and ActorNet, distinct loss items to guide the learning of the LatentNet.
namely, the Agent part in gray color in Fig. 2(b). Though the For the first two demands, we apply unsupervised losses to
input dimensions of the InferenceNet and CriticNet vary with learnthelatentvariable.ThefirstitemisL e,themeanentropy
the population sizes, this will not influence the transfer to the of the D i, defined as follows:
new scenario to execute. In this way, our method is scalable 1 (cid:88)n
to fit scenarios with different agent numbers. L e(θ L)=
n
H(D i(o i,h it−1;θ L))
i=1
n
1 (cid:88)
B. Heterogeneous Layer Design = H(N(µ (o ,ht−1;θ ),σ (o ,ht−1;θ ))),
n i i i L i i i L
With the generated latent variable for each agent, we can i=1
(6)
furtherdesigntheheterogeneouslayerforeachagentdescribed
in Fig. 2(c, d). The heterogeneous layer is a linear layer in where n is the team size, H is the entropy function of
the ActorNet, which also has an MLP and an RNN ahead the distribution, θ is the parameters of the LatentNet. To
L
and an MLP behind. The ActorNet for agent n takes the minimizetheL (θ ),thelatentdistributionstendtobesharper
e L
observation o and the hidden states of the RNN ht−1 to and thus more identifiable.
n nHETEROGENEOUSMARLFORSCALABLECOLLABORATION 6
The second item is the distance among the agents’ latent D. Overall Training Procedure
variables L (θ ):
d L In one episode, the first phase is the interaction with
1 (cid:88)n (cid:88)n the environment to collect data. Every agent gains a latent
L d(θ L)=
n(n−1)
Norm(1− variable and then decides its action decision at each step (see
i=1 j̸=i Algorithm 1). Data such as observations, hidden states, latent
cos_sim(l (o ,ht−1;θ ),l (o ,ht−1;θ ))), (7) variables, actions, and rewards are saved in the buffer. After
i i i L j j j L
theagentssucceed,fail,orthesteplimitisreached,thesecond
wherecos_simisthecosinesimilaritybetweentwovectors.
phase is training (see Algorithm 2). The agents update their
Norm is the normalization of all the distances to ensure the
ActorNets one by one, following the procedure in HAPPO
distance distribution :
[14]. Of course, here all the agents share the same ActorNet.
x −min(x)
Norm(x )| = i . (8) Thus, the ActorNet will be updated for n times based on
i xi∈x max(x)−min(x)+10−12
differentagents’observations,actions,andrewards.Afterthat,
BymaximizingtheL d(θ L),thelatentvariableswillbediverse theCriticNet,theLatentNet,andtheInferenceNetupdatetheir
toencouragetheagentstohaveheterogeneousstyles.Notewe parameters.PleaserefertoAppendix BandCforthedetailed
implement the sampling function as rsample() in Pytorch to network and hyperparameter configurations.
getreparameterizedsamples,sothesamplingisdifferentiable.
The third item is the value V from the InferenceNet L : Algorithm 2 Overall Training Procedure.
I v
Input: batch size B, number of agents n, number of
L (θ )=V (o ,µ(o,ht−1;θ ),σ(o,ht−1;θ )), (9) episodes K, max steps per episode T.
v L I g L L
Initialize:parameter-sharednetworks:ActorNet{θ },Crit-
A
where µ and σ are the concatenation of each agent’s µ
i icNet {θ }, LatentNet {θ }, InferenceNet {θ }, replay
C L I
and σ . Note that when calculating L (θ ) to update the
i v L buffer B.
LatentNet, the parameters of the InferenceNet are fixed and
for k =0:K do
will not be updated. To maximize L , we can expect that the
v Collect trajectories and latent distributions.
learned latent variables will be improved to help the agents Push transitions {(ot,ht−1,lt,at,ot+1,rt)},0 ≤ t ≤
choose a better heterogeneous layer. i i i i i
T,i=1,...,n into B
Up to this point, we have the overall loss for the LatentNet
Sample a random minibatch of B transitions from B
L (θ ) to be minimized as follows,
L L Draw a random permutation of agents i .
1:n
L L(θ L)=λ eL e(θ L)−λ dL d(θ L)−λ vL v(θ L), (10) for agent i m =i 1,...,i n do
UpdateActorNetasEquation13witho ,ht−1,l
where λ e,λ d, and λ
v
are the weights of the three items of the
end for
im im im
final loss.
Update CriticNet as Equation 14
The InferenceNet is learned by an MSE loss between the
ComputeL (θ ),L (θ )byforwardingLatentNetwith
value V and environment return R: e L d L
I all the agents’ o and ht−1 as Equation 6 and Equation 7.
Compute L (θ ) by InferenceNet as Equation 9.
v L
L I(θ I)=MSE_Loss[V I(o g,µ(o,ht−1;θ I), Update LatentNet by Equation 10:
σ(o,ht−1;θ I)),R], (11) L L(θ L)=λ eL e(θ L)−λ dL d(θ L)−λ vL v(θ L)
Update InferenceNet by L (θ ) as Equation 11
I I
where the MSE loss is calculated as
end for
B
1 (cid:88)
MSE_Loss[X,Y]= (x −y )2, (12)
B k k
k=1
V. EXPERIMENTS
where B is the minibatch size, x and y are the samples
k k
A. Environments and Metrics
in the minibatch. The InferenceNet will be able to infer how
good the latent distributions are when L converges. Then, To demonstrate the effectiveness of our method, especially
I
The value V will be a good estimation of the return. when scaled to unseen scenarios, we adopt two classic and
I
Besides, the ActorNet and the CriticNet are updated fol- hard MARL environments - Starcraft Multi-Agent Challenge
lowingtheHAPPOlossEquation2andEquation3,including (SMAC) [22] and Google Research Football (GRF) [23], and
theparametersofthedecoder,w_decoder,andb_decoder.The further modify them to design scalability tasks.
lossoftheActorNetisaTemporalDifference(TD)loss,L : StarCraftIIisareal-timestrategygameservingasaclassic
TD
benchmark in the MARL community. In the Starcraft Multi-
L (θ )=L (s,a,θ )
TD A Actor A Agent Challenge (SMAC) tasks, a team of allied units aims
=L Actor(V C,R,a(o,ht−1,l;θ A)), (13) to defeat the adversarial team in various tasks.
Google Research Football (GRF) contains diverse subtasks
note that here l is taken as an input without gradients.
in a football game. The agents cooperate as part of a football
The loss for the critic is L :
C
teamtoscoreagoalagainsttheopposingteamunderdifferent
L (θ )=L (s,θ )
C C Critic C scenarios such as counterattack, shooting in the penalty area,
=L (V (o ,ht−1;θ ),R). (14) etc.
Critic C g c CHETEROGENEOUSMARLFORSCALABLECOLLABORATION 7
Fig. 3: Visualization of the tasks. (a) MMM2 in SMAC. (b) 8m_vs_9m in SMAC. (c) 3_vs_1_with_keeper in GRF. (d)
counterattack _easy in GRF.
TABLE I: Win/score rate and reward on SMAC and GRF.
Task Metric SHPPO HAPPO HAPPO (share)* MAPPO (share) HATRPO (share)
Win rate 71.2±6.5 76.3±5.1 31.2±20.4 62.7±10.1 6.8±5.9
MMM2
Reward 17.5±1.3 17.8±1.1 14.7±3.1 16.9±1.5 8.9±1.2
Win rate 85.5±5.8 81.0±10.5 70.5±9.3 65.2±12.8 78.0±8.8
8m_vs_9m
Reward 18.9±0.8 18.3±1.2 17.2±1.6 17.5±1.9 17.5±1.5
Score rate 94.2±2.1 90.8±2.4 91.3±3.7 78.7±11.5 41.3±20.8
3_vs_1_with_keeper
Reward 19.9±0.1 19.6±0.2 19.7±0.3 17.0±1.1 13.3±1.3
Score rate 91.2±3.0 92.0±4.1 86.4±5.8 81.8±7.3 61.4±22.5
counterattack_easy
Reward 18.9±0.2 19.0±0.3 18.5±0.4 17.3±0.5 16.5±1.3
*
Implemented based on HAPPO but every agent shares the same parameters.
In order to directly zero-shot transfer the learned models and for a fair comparison, we adopt its parameter-shared
to tasks with varied populations, we modify the original version as a baseline. MAPPO [20] is a classic homogeneous
environments to fix the number of allies and enemies that MARL method, and in this paper, we also use parameter-
oneagentcanobserve.Thisway,theobservationsandactions shared MAPPO.
lengthisthesamebeforeandafterthetransfer.Hence,wecan
concentrate on transferring strategies without the need to map
observations,whichisnottheprimaryfocusofthispaper.For B. Results on SMAC
example,inthetaskMMM2ofSAMC,thereare10controlled
We first evaluate SHPPO on the original tasks. The first
agents and 12 enemies, respectively. In the original tasks of
task is a heterogeneous task MMM2, where each side has
SMAC, the agent can observe all the enemies, resulting in
threekindsofdifferentunits-Marine,Marauder,andMedivac
easy and unscalable tasks. We modify the tasks to limit the
(Fig. 3(a)). They have different health points, hit points,
observation range to the closest 10 enemies and the closest 8
and even special skills. Therefore, heterogeneous strategies
alliessothatwecancreateaseriesofnewtaskswithdifferent
will improve the team’s performance by maximizing each
numbers of agents and enemies to test scalability. While in
agent’s advantages. In Fig. 4(a), HAPPO performs best for its
GRF, there are always 11 players in one team. We change
great heterogeneous representation ability. However, HAPPO
the number of players that the algorithm can control to test
models every agent individually and thus lacks scalability.
scalabilitybutkeeptheobservationsastheglobalobservations
In contrast, our approach, SHPPO, outperforms all the other
ofalltheplayersinthefield.Moredetailsofthemodifiedtasks
parameter-shared baselines without modeling each agent. The
can be found in Appendix Table IV. All the experiments in
training reward and win rate of SHPPO are very close to
our paper are conducted on the new scalable tasks.
those of HAPPO, showing that SHPPO has a similar hetero-
As for metrics, we adopt training rewards to evaluate the geneous representation ability to HAPPO (See Table I). The
training progress. We also test the performance on the test performance of the two heterogeneous methods, HAPPO and
environments to calculate the win rate for SMAC or score HATRPO, is the worst when parameters are shared.
rate for GRF. We also evaluate on a homogeneous task 8m_vs_9m
We compare the performance of our approach with four (Fig. 3(b)), where each side has the same kind of unit -
otherbaselines.AsSHPPOisbuiltbasedonparameter-shared Marine. Though the agents are all the same, they can have
HAPPO, we take both HAPPO and HAPPO (share) as base- different strategies and adaptively alter during the task. As
lines.HATRPO[14]isanotherheterogeneousMARLmethod, shown in Fig. 4(b) and Table I, SHPPO outperforms all theHETEROGENEOUSMARLFORSCALABLECOLLABORATION 8
Fig. 4: Performance on SMAC and GRF. We plot the win rate on SMAC and the score rate on GRF during training.
(a) MMM2 (SMAC). (b) 8m_vs_9m (SMAC). (c) 3_vs_1_with_keeper (GRF). (d) counterattack_easy (GRF). The confidence
interval is calculated on 5 seeds.
TABLE II: Scalability test on SMAC.
Task* SHPPO HAPPO** HAPPO (share) MAPPO (share) HATRPO (share)
MMM2 (721_831)*** 71.2±6.5 76.3±5.1 31.2±20.4 62.7±10.1 6.8±5.9
722_831 96.3±1.5 87.5±10.1 95.6±2.5 96.1±3.8 45.4±5.2
821_831 82.5±7.5 12.5±4.9 70.0±19.2 95.1±4.5 18.7±7.5
731_831 95.6±4.5 31.8±30.2 96.4±3.5 97.5±2.5 41.6±7.7
711_731 5.6±2.5 2.5±1.8 0.0±0.0 5.1±2.5 0.0±0.0
711_821 65.8±8.2 72.5±11.8 35.2±17.5 63.5±20.1 3.7±2.4
621_821 77.5±7.8 56.8±11.2 25.1±12.5 75.2±12.5 9.5±5.6
621_631 62.5±2.5 35.1±22.4 26.4±14.1 62.5±15.9 2.6±2.1
8m_vs_9m 85.5±5.8 81.0±10.5 70.5±9.3 65.2±12.8 78.0±8.8
6m_vs_7m 17.5±10.4 2.5±1.2 12.7±9.8 15.7±11.9 12.6±8.2
7m_vs_8m 42.5±10.7 2.5±2.5 40.5±10.2 37.1±23.5 37.5±12.1
10m_vs_11m 70.2±20.1 0.0±0.0 51.3±23.8 42.5±18.3 68.1±22.5
*
The first row is the original task, while the others are unseen tasks with varied numbers of agents, whose results are achieved
by zero-shot transfer.
**
When transferring HAPPO, We randomly select a model of the same type for the added agent, or removing extra models
when there are fewer units in the new task.
***
We use the format ABC_DEF to represent the team sizes of both sides. A, B, C are the number of Marines, Marauders,
Medivacsoftheallysidecontrolledbyouralgorithm.D,E,FarethenumberofMarines,Marauders,Medivacsoftheenemy
side. The original MMM2 can be represented as 721_831.
baselines including HAPPO. The training process of SHPPO D. Sacability Test
also converges faster than other baselines.
TheproposedmethodSHPPOnotonlyperformswellonthe
original tasks on SMAC and GRF, but also has extraordinary
zero-shot scalability. We construct a series of new tasks with
C. Results on GRF
differentnumbersofagentsandenemiestotestthescalability.
We test the performance on two tasks of GRF. In task In the original MMM2 task, the ally team has 7 Marines,
3_vs_1_with_keeper (Fig. 3(c)), our algorithm can control 2 Marauders, and 1 Medivac, while the enemy team has 8
three football players against one adversary player and the Marines, 3 Marauders, and 1 Medivac. This task is labeled as
keeper. SHPPO is the most effective method among all the 721_831.Toadjustthedifficultyofthenewtaskappropriately,
methods in Fig. 4(c) and Table I. we either add one ally unit or remove one ally unit and one
In another task counterattack_easy (Fig. 3(d)), four agents enemy unit together. This ensures that the task is neither too
need to cooperate together to do a counterattack. They need difficult nor too easy, avoiding situations where any method
topasstheballagainstoneadversaryplayerandthekeeperto would have a win rate of 0% or 100%.
score. In Fig. 4(d), HAPPO converges fastest and has the best When transferring the learned model from the original
performance. SHPPO reaches a similar performance, though task to the unseen tasks, SHPPO does not require additional
at a slow speed. Still, SHPPO outperforms all the parameter- training. While HAPPO cannot be directly transferred due
shared baselines (also see Table I). to each agent having a distinct model, we address this byHETEROGENEOUSMARLFORSCALABLECOLLABORATION 9
Fig. 5: Ablation studies. The experiments are conducted on MMM2 and 8m_vs_9m in SMAC. (a, b) Ablation studies of
LatentNet’s inputs. (c, d) Ablation studies of LatentNet’s losses. The confidence interval is calculated on 5 seeds.
TABLE III: Scalability test on GRF.
Task SHPPO HAPPO HAPPO (share) MAPPO (share) HATRPO (share)
3_vs_1_with_keeper 94.2±2.1 90.8±2.4 91.3±3.7 78.7±11.5 41.3±20.8
4_vs_1_with_keeper 90.1±9.5 82.3±10.7 47.5±20.6 61.2±15.3 37.8±21.5
2_vs_1_with_keeper 92.3±7.7 91.0±3.5 91.3±6.4 63.4±22.1 45.2±17.3
counterattack_easy*
91.2±3.0 92.0±4.1 86.4±5.8 81.8±7.3 61.4±22.5
(4_vs_1_counterattack_with_keeper)
5_vs_1_counterattack_with_keeper 87.8±3.7 89.2±4.9 86.5±3.4 77.1±5.7 68.2±12.3
3_vs_1_counterattack_with_keeper 27.4±4.1 1.0±1.0 24.7±5.2 15.1±6.5 23.4±5.7
*
The original task counterattack_keeper can be represented as 4_vs_1_counterattack_with_keeper. The algorithm can control 4 players to
counterattack against 1 adversary player and the adversary keeper.
randomly selecting a model of the same type for the added E. Ablation Studies
agent, or removing extra models when there are fewer units
We further conduct ablation studies to examine the effects
in the new task.
of different parts of latent learning.
From Table II, SHPPO performs the best on most of the In order to test the effectiveness of the latent variables, we
unseen zero-shot tasks. Even in cases where SHPPO is not first only mask the inputs of the LatentNet with zeros so that
optimal, it still performs comparably to the best. It’s worth the latent variables cannot be adaptively updated according
noting that although we attempt to transfer HAPPO, it only to the new observations. Moreover, in another experiment, we
outperformsSHPPOinonescenario(711_821),despitehaving replaceallthelatentvariableswithzerostototallyremovethe
a better win rate on the original task. latent learning. In Fig. 5(a, b), when the inputs of LatentNets
are zeros, the performance on two tasks of SMAC reduces
On another task 8m_vs_9m of SMAC, we also create new
significantly, especially in task 8m_vs_9m. When the latent
tasks with both fewer and more agents, such as 6m_vs_7m
variables are set as zeros, it is equal to the removal of the
and 10m_vs_11m. SHPPO is superior in all the unseen tasks.
latent learning part, and we can see even worse performance
Surprisingly, HAPPO almost cannot win after the transfer.
in such a case. Therefore, heterogeneous strategies and flex-
We suppose that though the agents can have distinct roles
ible adjustment of strategies are important for the agents to
in a homogeneous task, their strategies may not be extremely
cooperate.
differentastheybelongtothesameunittype.HAPPOpossibly
Next, we test the effectiveness of the losses of latent
overfitseveryagent’sheterogeneouspolicytotask8m_vs_9m,
learning. We first remove the guidance from the InferenceNet
which may not be suitable for the new tasks. Therefore, when
L (θ ) while keeping the unsupervised losses L (θ ) and
v L e L
we try to transfer HAPPO, the learned strategies do not work
L (θ ). Results in Fig. 5(c, d) show that without the Infer-
d L
on the unseen tasks.
enceNet, there will be a performance decline on both MMM2
Similarly, we test scalability by removing or adding one and 8m_vs_9m. It suggests that the InferenceNet is able
agentinthetwotasksofGRF.SHPPOmaintainsthebestper- to assess the value of latent variables to help with agents’
formanceaftertransferonthe3_vs_1_with_keeperseries.For decision-making.
the counterattack_easy series, despite HAPPO’s best perfor- Similarly, in Fig. 5(c, d), we then keep L (θ ) but remove
v L
manceontheoriginaltask,SHPPOachievesthesuperiorscore L (θ ) and L (θ ), respectively. The two experiments have
e L d L
rate on 3_vs_1_counterattack_with_keeper after the transfer. similarperformancereductionsontaskMMM2.Nevertheless,
And when adding a new agent, SHPPO also has comparable on the homogeneous task 8m_vs_9m, the experiment without
performance. the entropy loss L (θ ) has the lowest win rate. We believe
e LHETEROGENEOUSMARLFORSCALABLECOLLABORATION 10
Fig. 6: Latent variables in one episode in the original and unseen tasks with different population sizes. The latent
variables have three dimensions X, Y, Z and change dynamically with the steps. All the ally agents are illustrated and we
use colors to distinguish the unit type. The new agent is marked dots. (a) The original task MMM2. (b) The transferred task
722_831 with a new Medivac. (c) The transferred task 821_831 with a new Marine. (d) The transferred task 731_831 with a
new Marauder.
Fig. 7: Visualization of MMM2 and latent variables. The agents adaptively adopt different strategies during the task. The
latentvariablesclusteraccordingly.Red:staybacktobecovered.Green:attackinthedistance.Blue:attractthefirebymoving
to the front.
thisisbecause,thestrategiesforahomogeneoustaskmayhave different types of units. The two Marauders’ latent variables
some extent of diversity for labor division, but the strategies basically have the same trends, while those of the Marines
cannot be too random and different as the agents are of the are much more diverse. Most of the agents do not stick to
same unit type. While the entropy loss L (θ ) can make sure onlyonestrategyduringthetask.Instead,theyadaptivelyalter
e L
thatthelearnedlatentvariablesarerelativelydeterministicand the latent variables to change the strategy. With the learned
identifiable. heterogeneous strategies, we transfer the models to three
unseen tasks adding one agent of different types (Fig. 6(b, c,
d)).Wecanseethat,generally,theagents’latentvariablesstill
F. Strategy Analysis
follow similar heterogeneous patterns to those in the original
Finally, we visualize the learned latent variables and the task,withslightdifferencesinstrategyupdatesduringthetask.
progress of the task to analyze the heterogeneous strategies Thanks to the shared LatentNet, the newly added agent can
during the task. Take MMM2 as an example. In Fig. 6, we initially adopt one of the strategies from units of the same
plot how the latent variables update in one episode. In the type. However, over time, the new agents may also develop
originaltask(Fig.6(a)),thelatentvariablepatternsvaryamong novel strategies to enhance their performance in the new taskHETEROGENEOUSMARLFORSCALABLECOLLABORATION 11
Fig. 8: Visualization of the transferred task 821_831 and latent variables. The agents adaptively adjust strategies after
being transferred to an unseen task with one added Marine (Number 9). The latent variables cluster accordingly. Green: attack
in the distance. Blue: attract the fire by moving to the front.
by temporal heterogeneity (Fig. 6(b) after step 20, Fig. 6(c) Marine 3 changes its strategy to replace the dead Marauder,
after step 15). moving to the front (blue circle).
We also illustrate the screenshots with the corresponding We can see that the latent variables successfully learn the
latent variables before (Fig. 7) and after (Fig. 8) the transfer. heterogeneous strategy patterns in this task, which are located
In Fig. 7, at the early stage of the task (Step 5), the agents diversely in the latent space. These latent variables contribute
form two groups, the Marines move forward and attack in the to the adaptive policy updates during the task through the
front (green circle) to cover the Marauders and the Medivac HeteLayer in the ActorNet, so the agents can not only have
behind (red circle). At step 11, the two armies encounter and distinct strategies but also change their strategies as the task
thestrategiesalteraccordingly.TheMaraudersgetclosetothe progresses and the population size varies.
enemiestoattractthefirewhiletheMedivackeepscuringthem
(blue circle). Some Marines (Agent 2, 5, and 8) are hurt with
VI. CONCLUSION
low health points, thus they retreat to be covered (red circle), In this paper, we propose a novel MARL framework SH-
while the other Marines attack the enemies (green circle). In PPO to add both inter-individual and temporal heterogeneity
the final stage, some agents are dead and the enemies are to any parameter-shared actor-critic architecture so that the
almostdefeated.TheonlyMarauderkeepsattractingthefirein frameworkcanbescalableforunseenzero-shotnewtasks.We
the front. However, the Marines attack the enemies regardless introduce latent learning to adaptively update the parameters
of their health points to achieve final victory. of the heterogeneous layer and the agents’ strategies. Our
In Fig. 8, one new Marine is added while the enemies approach shows superior performance over baselines, espe-
are the same, making the new task easier. Thus, the agents cially for zero-shot scalable collaboration, on several classic
inheritthepreviousstrategiesandrolesintheteam,butfurther MARLtasks.Futureworkcouldimplementtheheterogeneous
adaptively align them with the easier scenario. At step 7, the layer with more complex network designs like transformers,
added Marine (Agent 9) joins its fellow Marines to attack or integrate population-invariant methods to further enhance
the enemies (green circle), while other Marines are moving scalability. One can even take advantage of the emerging
forward to attract and disperse the enemies’ fires, together largelanguagemodels(LLMs),convertingthelatentvariables
with the Medivac and Marauders (blue circle). Note that no to heterogeneous prompts for LLM agents to gain better
agents keep staying back to be covered as the red circle in scalability.
Fig. 7, because they are taking a more aggressive strategy in
easiertasks.Atstep16,alltheMarinesstopmovingandfocus
APPENDIXA
on attacks (green circle). The Marauders go on moving to the
ENVIRONMENTSETTINGS
frontwiththesupportoftheMedivac(bluecircle).Atthefinal We modify and keep the number of observations the same
stage(step33),Marauder1dies,andalmostatthesametime, in one task series, so that we can focus on the transferHETEROGENEOUSMARLFORSCALABLECOLLABORATION 12
of strategies without mapping the observations. The detailed [2] V. R. Lesser, “Reflections on the nature of multi-agent coordination
settings are shown in Table IV. anditsimplicationsforanagentarchitecture,”Autonomousagentsand
multi-agentsystems,vol.1,pp.89–111,1998.
[3] X.Guo,P.Chen,S.Liang,Z.Jiao,L.Li,J.Yan,Y.Huang,Y.Liu,and
TABLE IV: Number of observations in the modified tasks
W.Fan,“Pacar:Covid-19pandemiccontroldecisionmakingvialarge-
for scalability test. scaleagent-basedmodelinganddeepreinforcementlearning,”Medical
DecisionMaking,vol.42,no.8,pp.1064–1077,2022.
[4] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S.
Task #Obsofallies* #Obsofenemies
Bernstein,“Generativeagents:Interactivesimulacraofhumanbehavior,”
inProceedingsofthe36thAnnualACMSymposiumonUserInterface
MMMseries 9 10
SoftwareandTechnology,2023,pp.1–22.
8m_vs_9mseries 5 6
[5] H.Li,Y.Q.Chong,S.Stepputtis,J.Campbell,D.Hughes,M.Lewis,
andK.Sycara,“Theoryofmindformulti-agentcollaborationvialarge
3_vs_1_with_keeperseries 11 11 languagemodels,”arXivpreprintarXiv:2310.10701,2023.
counterattack_easyseries 11 11 [6] H.Sha,Y.Mu,Y.Jiang,L.Chen,C.Xu,P.Luo,S.E.Li,M.Tomizuka,
W.Zhan,andM.Ding,“LanguageMPC:Largelanguagemodelsasdeci-
*
Thenumberofobservationsoftheallyagentsincludestheagentitself. sionmakersforautonomousdriving,”arXivpreprintarXiv:2310.03026,
2023.
[7] H.Zhang,S.Feng,C.Liu,Y.Ding,Y.Zhu,Z.Zhou,W.Zhang,Y.Yu,
H. Jin, and Z. Li, “CityFlow: A multi-agent reinforcement learning
APPENDIXB environment for large scale city traffic scenario,” in The World Wide
NETWORKSETTINGS WebConference. ACM,2019,pp.3620–3624.
[8] J. Wang, T. Shi, Y. Wu, L. Miranda-Moreno, and L. Sun, “Multi-
We provide the network configurations as follows: agentgraphreinforcementlearningforconnectedautomateddriving,”in
Proceedingsofthe37thInternationalConferenceonMachineLearning
TABLE V: Network configurations for SHPPO. (ICML),2020,pp.1–6.
[9] L. Wang, L. Han, X. Chen, C. Li, J. Huang, W. Zhang, W. Zhang,
X. He, and D. Luo, “Hierarchical multiagent reinforcement learning
Module Implementation for allocating guaranteed display ads,” IEEE Transactions on Neural
MLPdim 64 NetworksandLearningSystems,pp.1–13,2021.
Encoder 3-layerMLP [10] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,
Decoder 3-layerMLP J.Chung,D.H.Choi,R.Powell,T.Ewalds,P.Georgiev,J.Oh,D.Hor-
InferenceNet 3-layerMLP gan,M.Kroiss,I.Danihelka,A.Huang,L.Sifre,T.Cai,J.P.Agapiou,
w_decoder linearlayer M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard,
b_decoder linearlayer D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang,
HeteLayer linearlayer T. Pfaff, Y. Wu, R. Ring, D. Yogatama, D. Wünsch, K. McKinney,
ActorNetRNNhiddendim 64 O.Smith,T.Schaul,T.Lillicrap,K.Kavukcuoglu,D.Hassabis,C.Apps,
CriticNetRNNhiddendim 64 and D. Silver, “Grandmaster level in StarCraft II using multi-agent
activation ReLU reinforcementlearning,”Nature,vol.575,no.7782,pp.350–354,2019.
optimizer Adam [11] V. Konda and J. Tsitsiklis, “Actor-critic algorithms,” in Advances in
latentvariabledim 3 NeuralInformationProcessingSystems,vol.12,1999.
[12] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mordatch,
“Multi-agent actor-critic for mixed cooperative-competitive environ-
ments,” Advances in neural information processing systems, vol. 30,
2017.
APPENDIXC [13] S.IqbalandF.Sha,“Actor-attention-criticformulti-agentreinforcement
PARAMETERSETTINGS learning,” in International conference on machine learning. PMLR,
2019,pp.2961–2970.
The hyperparameters used for training of SHPPO is pro- [14] J.G.Kuba,R.Chen,M.Wen,Y.Wen,F.Sun,J.Wang,andY.Yang,
vided in Table VI. “Trustregionpolicyoptimisationinmulti-agentreinforcementlearning,”
arXivpreprintarXiv:2109.11251,2021.
[15] C.Li,T.Wang,C.Wu,Q.Zhao,J.Yang,andC.Zhang,“Celebrating
TABLE VI: Hyperparameters settings for SHPPO.
diversity in shared multi-agent reinforcement learning,” Advances in
NeuralInformationProcessingSystems,vol.34,pp.3991–4002,2021.
Parameter Value [16] S.Hu,F.Zhu,X.Chang,andX.Liang,“Updet:Universalmulti-agent
minibatchnum 1
rlviapolicydecouplingwithtransformers,”inInternationalConference
ActorNetlearningrate 0.0005
onLearningRepresentations,2020.
CriticNetlearningrate 0.0005 [17] T.Zhou,F.Zhang,K.Shao,K.Li,W.Huang,J.Luo,W.Wang,Y.Yang,
LatentNetlearningrate 0.0005 H. Mao, B. Wang et al., “Cooperative multi-agent transfer learning
InferenceNetlearningrate 0.005
withlevel-adaptivecreditassignment,”arXivpreprintarXiv:2106.00517,
entropylossweightλe 0.001 2021.
distancelossweightλ d 0.01 [18] F. Christianos, G. Papoudakis, M. A. Rahman, and S. V. Albrecht,
inferencelossweightλv 0.1 “Scaling multi-agent reinforcement learning with selective parameter
discountfactorγ 0.95 sharing,” in International Conference on Machine Learning. PMLR,
evaluateinterval 25 2021,pp.1989–1998.
evaluatetimes 40 [19] J.K.Terry,N.Grammel,S.Son,B.Black,andA.Agrawal,“Revisiting
clip 0.2 parameter sharing in multi-agent deep reinforcement learning,” arXiv
GAElambda 0.95
preprintarXiv:2005.13625,2020.
maxstepsperepisode 160 [20] C.Yu,A.Velu,E.Vinitsky,J.Gao,Y.Wang,A.Bayen,andY.Wu,“The
surprising effectiveness of ppo in cooperative multi-agent games,” Ad-
vancesinNeuralInformationProcessingSystems,vol.35,pp.24611–
24624,2022.
[21] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov,“Prox-
REFERENCES imalpolicyoptimizationalgorithms,”arXivpreprintarXiv:1707.06347,
2017.
[1] R. Axelrod and W. D. Hamilton, “The evolution of cooperation,” [22] M. Samvelyan, T. Rashid, C. Schroeder de Witt, G. Farquhar,
Science,vol.211,no.4489,pp.1390–1396,1981. N. Nardelli, T. G. Rudner, C.-M. Hung, P. H. Torr, J. Foerster, andHETEROGENEOUSMARLFORSCALABLECOLLABORATION 13
S.Whiteson,“Thestarcraftmulti-agentchallenge,”inProceedingsofthe learning,”AdvancesinNeuralInformationProcessingSystems,vol.35,
18th International Conference on Autonomous Agents and MultiAgent pp.1698–1710,2022.
Systems,2019,pp.2186–2188. [45] M. L. Littman, “Markov games as a framework for multi-agent rein-
[23] K.Kurach,A.Raichuk,P.Stan´czyk,M.Zaja˛c,O.Bachem,L.Espeholt, forcementlearning,”inMachinelearningproceedings1994. Elsevier,
C. Riquelme, D. Vincent, M. Michalski, O. Bousquet et al., “Google 1994,pp.157–163.
research football: A novel reinforcement learning environment,” in [46] M. Tan, “Multi-agent reinforcement learning: Independent vs. cooper-
Proceedings of the AAAI conference on artificial intelligence, vol. 34, ative agents,” in Proceedings of the tenth international conference on
no.04,2020,pp.4501–4510. machinelearning,1993,pp.330–337.
[24] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, [47] G. Palmer, K. Tuyls, D. Bloembergen, and R. Savani, “Lenient multi-
and S. Whiteson, “Monotonic value function factorisation for deep agent deep reinforcement learning,” in Proceedings of the 17th Inter-
multi-agentreinforcementlearning,”TheJournalofMachineLearning national Conference on Autonomous Agents and MultiAgent Systems,
Research,vol.21,no.1,pp.7234–7284,2020. 2018,pp.443–451.
[25] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, [48] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and
D.Silver,andD.Wierstra,“Continuouscontrolwithdeepreinforcement actinginpartiallyobservablestochasticdomains,”Artificialintelligence,
learning,”arXivpreprintarXiv:1509.02971,2015. vol.101,no.1-2,pp.99–134,1998.
[26] A.OroojlooyandD.Hajinezhad,“Areviewofcooperativemulti-agent [49] M.A.WieringandM.VanOtterlo,“Reinforcementlearning,”Adapta-
deepreinforcementlearning,”AppliedIntelligence,vol.53,no.11,pp. tion,learning,andoptimization,vol.12,no.3,p.729,2012.
13677–13722,2023. [50] F.A.Oliehoek,C.Amatoetal.,Aconciseintroductiontodecentralized
[27] Y. Du, J. Z. Leibo, U. Islam, R. Willis, and P. Sunehag, “A review of POMDPs. Springer,2016,vol.1.
cooperationinmulti-agentlearning,”arXivpreprintarXiv:2312.05162, [51] J.Schulman,S.Levine,P.Moritz,M.I.Jordan,andP.Abbeel,“Trust
2023. regionpolicyoptimization,”ComputerScience,pp.1889–1897,2015.
[28] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement
learningformultiagentsystems:Areviewofchallenges,solutions,and
applications,”IEEEtransactionsoncybernetics,vol.50,no.9,pp.3826– APPENDIXD
3839,2020. BIOGRAPHYSECTION
[29] X. Guo, D. Shi, and W. Fan, “Scalable communication for multi-
agent reinforcement learning via transformer-based email mechanism,”
inProceedingsoftheThirty-SecondInternationalJointConferenceon
ArtificialIntelligence,IJCAI-23,2023,pp.126–134. Xudong Guo received the B.Eng. degree in au-
[30] Z.Ding,T.Huang,andZ.Lu,“Learningindividuallyinferredcommu- tomationfromTsinghuaUniversity,Beijing,China,
nication for multi-agent cooperation,” Advances in Neural Information in 2020. He is now pursuing the Ph.D. degree at
ProcessingSystems,vol.33,pp.22069–22079,2020. theDepartmentofAutomation,TsinghuaUniversity,
[31] Q. Long, Z. Zhou, A. Gupta, F. Fang, Y. Wu, and X. Wang, “Evo- Beijing,China.Hisresearchinterestsincludemulti-
lutionary population curriculum for scaling multi-agent reinforcement agentsystemsandreinforcementlearning.
learning,”arXivpreprintarXiv:2003.10423,2020.
[32] W. Wang, T. Yang, Y. Liu, J. Hao, X. Hao, Y. Hu, Y. Chen, C. Fan,
and Y. Gao, “From few to more: Large-scale dynamic multiagent cur-
riculumlearning,”inProceedingsoftheAAAIConferenceonArtificial
Intelligence,vol.34,no.05,2020,pp.7293–7300.
[33] X. Hao, H. Mao, W. Wang, Y. Yang, D. Li, Y. Zheng, Z. Wang,
and J. Hao, “Breaking the curse of dimensionality in multiagent
Daming Shi received the B.S. degree in automatic
state space: A unified agent permutation framework,” arXiv preprint
sciencefromBeihangUniversity,Beijing,China,in
arXiv:2203.05285,2022.
2018andthePh.D.degreeinautomaticsciencefrom
[34] A. Agarwal, S. Kumar, and K. Sycara, “Learning transferable cooper-
Tsinghua University, Beijing, China, in 2023. His
ativebehaviorinmulti-agentteams,”arXivpreprintarXiv:1906.01202,
researchinterestsincludereinforcementlearningand
2019.
multi-agentsystemsingamingandproduction.
[35] M. Wen, J. Kuba, R. Lin, W. Zhang, Y. Wen, J. Wang, and Y. Yang,
“Multi-agent reinforcement learning is a sequence modeling problem,”
Advances in Neural Information Processing Systems, vol. 35, pp.
16509–16521,2022.
[36] R. Qin, F. Chen, T. Wang, L. Yuan, X. Wu, Z. Zhang, C. Zhang, and
Y.Yu,“Multi-agentpolicytransferviataskrelationshipmodeling,”arXiv
preprintarXiv:2203.04482,2022.
[37] M. Bettini, A. Shankar, and A. Prorok, “Heterogeneous multi-robot Junjie Yu received the B.S. degree in automatic
reinforcementlearning,”arXivpreprintarXiv:2301.07137,2023. science from Beihang University, Beijing, China,
[38] D.Nguyen,P.Nguyen,S.Venkatesh,andT.Tran,“Learningtotransfer in 2023. He is now pursuing the M.S. degree at
role assignment across team sizes,” arXiv preprint arXiv:2204.12937, theDepartmentofAutomation,TsinghuaUniversity,
2022. Beijing,China.Hiscurrentresearchinterestsinclude
[39] S.Hu,C.Xie,X.Liang,andX.Chang,“Policydiagnosisviameasuring multi-agentsystemsandreinforcementlearning.
rolediversityincooperativemulti-agentrl,”inInternationalConference
onMachineLearning. PMLR,2022,pp.9041–9071.
[40] D.Wang,F.Zhong,M.Wen,M.Li,Y.Peng,T.Li,andY.Yang,“Romat:
Role-based multi-agent transformer for generalizable heterogeneous
cooperation,”NeuralNetworks,p.106129,2024.
[41] Z. Hu, Z. Zhang, H. Li, C. Chen, H. Ding, and Z. Wang, “Attention-
guided contrastive role representations for multi-agent reinforcement
learning,” in The Twelfth International Conference on Learning Rep- Wenhui Fan received the Ph.D. degree in me-
resentations,2024. chanical engineering from Zhejiang University,
[42] T. Wang, H. Dong, V. Lesser, and C. Zhang, “Roma: Multi-agent Hangzhou,China,in1998.Heobtainedthepostdoc-
reinforcementlearningwithemergentroles,”inInternationalConference toral certificate from Tsinghua University, Beijing,
onMachineLearning. PMLR,2020,pp.9876–9886. China, in 2000. He is a vice president of China
[43] T.Wang,T.Gupta,B.Peng,A.Mahajan,S.Whiteson,andC.Zhang, Simulation Federation. He is currently a professor
“Rode:learningrolestodecomposemulti-agenttasks,”inProceedings at Tsinghua University, Beijing, China. His current
of the International Conference on Learning Representations. Open- researchinterestsincludemulti-agentmodelingand
Review,2021. simulation, large scale agent modeling and simula-
[44] M.Yang,J.Zhao,X.Hu,W.Zhou,J.Zhu,andH.Li,“Ldsa:Learning tion,andmulti-agentreinforcementlearning.
dynamic subtask assignment in cooperative multi-agent reinforcement