Who Evaluates the Evaluations? Objectively Scoring
Text-to-Image Prompt Coherence Metrics with
(TS2)
MichaelSaxon FatimaJahara MahsaKhoshnoodi
YujieLu AdityaSharma WilliamYangWang
UniversityofCalifornia,SantaBarbara FatimaAl-FihriPredoctoralFellowship
Equalcontribution Contact: saxon@ucsb.edu
T2IScoreScore.github.io
Dataset Meta-metrics
Node pair-wise
Prompt: A boy in a green shirt Imagewise scores separation scores
poses with some fruit T2I Faithfulness Metrics (black-box) Separation: K-S Statistic Node pair Sep.
0-0.jpg Semantic Error Graph 0-1.jpg ImgNodeErr #Score 1 K-S = 0.5 0, 1a 1.0
0-0 0 0 0.89 .5
0 err Shirt not TIFA SC cL oI rP e SV coIE re 0-1 0 0 0.75 .25 0, 1b 0.5
No fruit green 1-0 1a 1 0.56 0 1a, 2a ...
1-0.jpg 1-2.jpg 0 .25 .5 1
1-1 1b 1 0.59
Node seq Ord.
1 err 1 err 1-2 1b 1 0.79 Ordering: Spearman Corr.
2-0.jpgSh gi rr et en not No fruit No boy 2-1.jpg CLIPScore 2-0 2a 2 0.23 1 0, 1a, 2a 0.84
.5 0, 1b, 2a 0.57
2-1 2b 2 0.41
2 err 2 err Metric Wrapper .25 0, 1b, 2b 0.68
2-2 2c 2 0.63
0
ρs = 0.68
SEG walk-wise
0 1 2
ordering scores
Figure1: OverviewofT2IScoreScore. T2Ievaluationmetricsarescoredbasedontheirability
tocorrectlyorganizeimagesinasemanticerrorgraph(SEG)relativetotheirgeneratingprompt,
checkingordering(Spearman’sρ)andseparationofnodes(Kolmogorov–Smirnovstatistic).
Abstract
Withadvancesinthequalityoftext-to-image(T2I)modelshascomeinterestin
benchmarking their prompt faithfulness—the semantic coherence of generated
images to the prompts they were conditioned on. A variety of T2I faithfulness
metrics have been proposed, leveraging advances in cross-modal embeddings
andvision-languagemodels(VLMs). However,thesemetricsarenotrigorously
compared and benchmarked, instead presented against few weak baselines by
correlationtohumanLikertscoresoverasetofeasy-to-discriminateimages.
We introduce T2IScoreScore (TS2), a curated set of semantic error graphs
containingapromptandasetincreasinglyerroneousimages. Theseallowusto
rigorouslyjudgewhetheragivenpromptfaithfulnessmetriccancorrectlyorder
imageswithrespecttotheirobjectiveerrorcountandsignificantlydiscriminate
betweendifferenterrornodes,usingmeta-metricscoresderivedfromestablished
statisticaltests. Surprisingly,wefindthatthestate-of-the-artVLM-basedmetrics
(e.g.,TIFA,DSG,LLMScore,VIEScore)wetestedfailtosignificantlyoutperform
simple feature-based metrics like CLIPScore, particularly on a hard subset of
naturally-occurringT2Imodelerrors.TS2willenablethedevelopmentofbetterT2I
promptfaithfulnessmetricsthroughmorerigorouscomparisonoftheirconformity
toexpectedorderingsandseparationsunderobjectivecriteria.
Preprint.Underreview.
4202
rpA
5
]VC.sc[
1v15240.4042:viXra1 Introduction
Text-to-image (T2I) models are improving at a breakneck pace in terms of quality, fidelity, and
coherence of generated images to their conditioning prompts [39–42]. Despite this, persistent
challengesinachievingimage-promptfaithfulness[34,6]remain—particularlyinfreelyavailable
modelsthatdon’tsitbehindproprietaryAPIs. Indeed,manytechniquestoimproveT2Imodelshave
been proposed of late, aiming to reduce hallucination [51, 9], duplication [25], and composition
errors[9,29],andmissingobjects[55,10]. However,thereisnoconsensusonhowtobestcompare
thesemanymodelsandmethods,soitishardtoobjectivelytrackT2Iprogress[43,21].
Avarietyofautomatedimage-promptcoherencemetricshavebeenproposed,whichratethefaithful-
nessofgeneratedimages,definedasthedegreetowhichatheysatisfytheimplicitrequirementsset
forthinthegeneratingprompt[11,13,30,20]. Theseproposedmetricsvaryconsiderablyindesign;
asratinghowwellanimagematchestoitspromptisanontrivialmultimodalchallenge[21,8,22].
Thisvarietyitselfpresentsameta-evaluationproblem:thereisnoconsensusonhowthesefaithfulness
metricsoughttobecompared,andconsequentlyeachnewmetricisvalidatedonitsownad-hoctest
setagainstpriorbaselines. Typicallytheseself-evaluationsconsistofasetofprompt-imagepairs
withaccompanyinghumanannotations(usuallysimpleLikertscores[8,13]),andmetricsarejudged
ontheircorrelationtothesehumanjudgements[22].
Suchself-evaluationisnotideal;authorsmayunwittinglytiltthescalesbyusingevaluationexamples
whichcatertotheparticularstrengthsoftheirproposedmethod,andvarianceofmetricperformance
between different evaluation sets (containing different images and prompt semantics [45, 31]) is
high[58]. Additionally,relyingoncorrelationtohumanjudgementsofsmallsetsofimagesacross
different prompts is highly subjective [15, 32] and prone to including judgements of quality and
stylethatareorthogonaltopromptcoherence. Asacommunityweneedaconsistentandobjective
evaluationforT2Iprompt-coherencemetricstoenabletrustworthycomparison.
TothisendweproposeT2IScoreScore(TS2),anobjectivebenchmarkforbenchmarks,toresolve
theseissues. Whileitcontainsasimilarnumberofimagestopreviouslyproposedcoherencemetric
evaluationsets,itcontainsfewerprompts. Thishighimage-to-promptratioallowsustoorganizethe
imagesalongsemanticerrorgraphs(Figure1),whereeachedgecorrespondstoanerroroftheimage
topresentsomeattribute,element,orrelationdescribedintheprompt. Thesesemanticerrorgraphs
permitobjectivescoringofametricthroughtwoquestions:
1. Canametriccorrectlyorderincreasinglywrongimagesagainsttheirgeneratingpromptas
errorcountsgoup? (Ord.)
2. Canametricreliablyseparatesetsofimagesbasedonasingleprompt-relevantsemantic
difference? (Sep.)
We adapt Spearman’s correlation ρ [47] and the two-sample Kolmogorov–Smirnov statistic [19]
respectively to answer these questions. We score a broad set of T2I faithfulness metrics along
all semantic error graphs (SEGs) in TS2. We find some surprising results: despite their inferior
performanceincorrelatingtohumanpreferences[13,6,20,30],simplefeature-spacemethodslike
CLIPScore[11]areactuallyquiteperformantintermsofobjectivecorrectness,comparableoreven
superiortothemorecomplicatedvision-languagemodel(VLM)-basedones(section6).
In light of these surprising results, we hope to see the T2I evaluation community adopt TS2 to
rigorously guide the further advancement of T2I prompt coherence metrics that perserve the in-
terpretability and subjective human correlation benefits of more rigorous methods, but also can
significantlyoutperformsimplebaselinesonourobjectivetest. Tosummarize,inthiswork:
• WeformalizethetaskofobjectivelyassessingT2Ipromptcoherencemetricsbytheirability
tocorrectlyorderandseparateimagepopulationswithinsemanticerrorgraphs(SEGs).
• WepresentT2IScoreScore(TS2),ourevaluationforthistask:abenchmarkdatasetof165
objectiveSEGsand2,840images,andmeta-metricsfororderingandseparationinSEGs.
• We evaluate a broad and representative set of T2I faithfulness benchmarks using TS2,
demonstratehowitidentifiesnovelfailurecases.
• MotivatehowfutureT2IfaithfulnessmetricscanbeimprovedbytreatingTS2performance
asanorthogonalconsiderationtohumanpreferencecorrelation.
2Num NumTotal Img.perPrompt MaxTree MaxTree Ad-hoc Objective
Dataset Prompts Images Min:Avg:Max Depth Width
ImagenHubT2I[21] 197 – – – – ✓ –
TIFAv1.0[13] 4k – – – – ✓ –
DrawBench[42] 200 – – – – – –
DSG-1k[6] 1k – – – – ✓ –
T2I-CompBench[14] 6k – – – – – –
ABC-6K[9] 6.4k – – – – – –
Flickr8k[12] 40k 8k 1:1:1 1 5 – –
Flickr30k[35] 155k 31k 1:1:1 1 5 – –
MSCOCOCaptions[4] 1.5m 330k 1:1:1 1 5 – –
SeeTRUE[52] 31k 31k 1:1:1 1 1 ✓ –
Pick-a-Pic[18] 35k 1M 2:2:21 1 2 ✓ –
T2IScoreScore 165 2.8k 4:17:76 5 5 – ✓
Table1:ComparisonofT2Imetricevaluationbenchmarks. Thefirstgroupofbenchmarksareprompt-
only2, while the second group provides images. Ad-hoc benchmarks were introduced alongside
aspecificmetric, objectivebenchmarksassessmetricsbasedonexplicitlyenumeratederrors(eg.
missingobjects)ratherthanrelativelyarbitraryhumanpreferenceratings.
2 RelatedWork
Advancementsintext-to-imageevaluationmetricsandbenchmarksnecessitateevaluationforthese
metricstounderstandtheircapabilitescomprehensively. Wediscussexamplesofbothhere.
Text-to-ImageMetricsandBenchmarks SeveralbenchmarksforT2Ievaluationonlycontain
prompts.TheImagenHubdataset[21]isabenchmarkforevaluatingtext-to-image(T2I)modelsusing
prompts,specificallydesignedfortasksliketext-guidedimagegeneration. DrawBench[42]provides
adetailedevaluationofT2Imodelsbyincorporatingtextpromptstoprobesemanticattributeslike
compositionality,cardinality,andspatialrelations. TIFAv1.0[13]selectstextinputsfromtheCOCO
validationset,DrawBench,PartiPrompt[54],andPaintSkill[5],whileT2I-ComBench[14]covers
compositionality,attributebinding,objectrelationships,andcomplexcompositions. SomeotherT2I
evaluationbenchmarksderivedfromcaptioningdatasets—likeFlickr8k,Flickr30k,andMSCOCO—
focusoncomparingmetricscorrelationtocaptionsgeneratedfromnaturalimages[12,35,4]. The
ABC-6KandCC500benchmarks[9]evaluateattributebindingfortext-to-imagemodels,butthey
onlyfocusoncolorattributes. Pick-a-Pic[18]isdesignedforgatheringhumanpreferencesfrom
generated images, and can be used to compare metrics. It contains over 500k examples, each
consistingofaprompt,twogeneratedimages,andalabelindicatingthepreferredimage,orifthere
isatiewhereneitherimageissignificantlyfavored. TIFAandDSG[6]aretwoprominentmetric
proposalpapersthatusead-hocevals. TIFAusesacollectionof160newannotationson800images
generatedfrom160promptsintheTIFAv1.0dataset. DSGcollectsper-item(text-imagepair)for
TIFA160prompts[13]andper-questionhumanjudgmentsforthefullDSG-1kprompts. Moredetails
onspecificmetricsareinsection5. Otherevaluationdimensionssuchasmultilinguality[44,46]and
stereotypebias[3]havealsobeenexplored.
EvaluationforText-to-ImageEvaluationMetrics SeeTRUE[52]is—toourknowledge—the
onlyotherexistingbenchmarkformeta-evaluatingT2Imetrics,thoughtheyonlyevaluatetheTIFA
metric on visual entailment. They focus on accuracy in detecting binary misalignments in test
imagesratherthanonranking. Whilethisisalsoobjectivethisevaluationdoesnotcapturethescore
ranking itself, only visual-text entailment. Their set also involves a mix of machine and human-
producedaugmentations,includinggenerativealternativeerroneouspromptsforimages(compared
to our approach of generating images that are erroneous relative to prompt). As a consequence,
thereexistcorrespondingsetsofmultiplepromptsforsomeimagesandmultipleimagesforsome
prompts,butmetricsarenotassessedspecificallyontheirabilitytodifferentiatebetweentheserelated
examplesinSeeTRUE.Tothebestofourknowledge,ourT2IScoreScoreisthefirstmeta-metric
tocomprehensivelyevaluateprompt-imagecoherencemetricsforT2I.
1Althoughthisbenchmarkhasmultiplepairsofimagesperprompt,pairisseparatelyannotatedwithone
preferenceimage.Withnowaytocomparecompareacrosssets,theeffectivenumberofimagesperpromptis2.
2ThoughthesebenchmarksareprimarilyintendedfordirectlyevaluatingT2Imodelsalongsidetheirproposed
proposedmetric,theyareusuallyalsousedformetricevaluationbyhavinghumanannotatorsprovidefaithfulness
Likertscoresforeachprompt/imagepairseparatelyoversomemodelsgeneratingimagesforeachprompt.
3Distribution of image sources Distribution of prompt source Distribution of error type
Missing objects
SD 2.0
41.3%
46.0%
SD 2.1 Manual Comp.
18.0% COCO 47.0% 41.0% 9.0%
7.7%
2.0% Verb
14.0% 5.0%
15.0% SD 1.5 12.0% 41.9%
SDXL Natural
DALL-E2 PartiPrompt Wrong attribute
(a)ImageSource (b)PromptSource (c)Errornode(SEGedge)type
Figure2: OverviewofthedistributionofsampletypesinTS2: (a)Thesourceexampleimagescame
from;5%ofimagesinthebenchmarkarerealphotographsfromastockimagerepository,whilethe
remainderweregeneratedbyStableDiffusion(SD)orDALL-Evariants. (b)Sourceoftheeliciting
prompt;eitherexistingresourcesorus(Manual). (c)DistributionoferrortypesedgesinallSEGs.
3 TheT2IScoreScoreDataset
ThegoalsofTS2aretodeterminewhetheratext-to-imagemodelfaithfulnessscoringmetriccan
correctlyorderasetofimagesofincreasingincorrectnesswithrespecttoaprompt,andwhetherit
canseparatethemwithin. Weorganizethisevaluationbasedonsemanticerrorgraphs(SEGs),which
containonepromptand4-76imagesannotatedwitherrornode. WeproducetheSEGsusingthree
differentprocedures,documentedinsubsection3.2.
3.1 DatasetStructure
TS2contains165SEGs. EachSEGisbasedonasingleprompt. Node“0”oneachSEGcontainsat
leastoneimagethathasbeenassessedbyhumanannotatorstocontainnoerrorsofverbalinformation
(eg,entityintheimageisn’tperformingthedescribedaction),compositionality(eg,objectdescribed
as“ontop”butisbeneathobject),missingobjects,orincorrectobjectattributes.
Eachgraphthencontainsasetoferrornodesformingadirectedacyclicgraph. Eachedgerepresents
anerrorofoneoftheaforementionedtypes. Eachnodeislabeledwiththenumberofedgesalongits
shortestpathbacktonode0,representingitserrorcount(Figure1,Figure4). Eachnodecontainsat
leastoneimagewhichiserroneousaccordingtothedescribederrors.
Theimagesaresourcedfromavarietyoftext-to-imagegenerationmethods,includingDALL-E2[40]
StableDiffusion1.5[41],2.0,2.1,andSDXL[36],andfromnaturalrepositories. Theoriginating
prompts are sourced from MS-COCO [26], PartiPrompt [54], and manually written by us. The
proportionofimagesources,promptsources,anderrortypesaredocumentedinFigure2.
3.2 DatasetCollectionProcedure
Figure3depictsthethreeproceduresbywhichweproduceandpopulateSEGswithimages,synthetic
imagesfromasyntheticgraph(Synth),graphfromnaturalimages(Nat),andgraphfromrealerrors
(Real). Theyaredifferentiatedintermsofpromptsourcing,imagesourcing,andtheorderinwhich
thosetwostepsareintegratedwiththegraph.
Synth. Thesyntheticimagesareproduced“graphfirst.” Fromaninitialpromptsourcedfromone
oftheaforementionedpromptsources,weconstructanerrorgraphbyfirstlistingallentitiesand
properties prescribed in the prompt, and then ablatively writing prompts where the elements are
sequentiallyremoved.
Forexample,intheleftpanelofFigure3,theinitialprompt“aChristmastreewithlightsandateddy
bear”isconvertedsubtractivelydowntotheprompt,“aChristmastreewithlights,”“aChristmas
tree,”etc. Eachofthesepromptsdescribestheerrornodeswheretheirrespectivelyremovedobjects
aremissing. Fromthepromptsrepresentingeachnodewegenerateasetofimagesusingoneofour
T2Imodels,andverifythateachimageiscorrectforitserrornode.
4Synthetic Error, Image Synth. Error, Natural Image Real Error, Synth. Image
(Synth) (Nat) (Real)
Image collection
Starting prompt: A Christmas tree Free Online Stock Starting prompt: A gray elephant
with lights and a teddy bear and a pink flamingo
Image Repositories
Error Graph Design (by error accumulation) Image Synthesis Attempts (T2I model)
No lights
No lights or
No errors
bear
No bear Human curation and sorting relation graph
Graph-informed prompt edits Brown Orange donut, Orange Errors human-counted
donut, happy unhappy Donut, happy
...with lights ...with lights ...with lights EC: 0 EC: 1 EC: 1
and bear and bear and bear Both happy Both orange
Error graph from error taxonomy
"Prompt" chosen to structure relations
Image Synthesis (Stable diffusion, etc)
into an error graph, error counts assigned. Flamingo
No errors
gray
Prompt: A happy woman No errors
eating an orange donut. No
EC: 0 elephant
EC: 0 EC: 1 EC: 2 Unhappy B dr oo nw un t Final Error Counts, graph structure
Final Error Counts human-verified. EC: 1 EC: 1 human-verified.
Figure3: Thethreemethodsusedtocollectimagestopopulatesemanticerrorgraphswithrespectto
theinputprompts. Synth. (imagesgeneratedfrommultiplepromptswrittentopopulateaSEG),Nat.
(naturalimagespopulateaSEG),andReal(realerrorsfromimagegenerationattemptsfromone
promptpopulateaSEG).
Nat. The natural error trees exclusively contain real images sourced from the free stock image
repositoryPexels3. Forthispopulation,wefollowan“image,graph,prompt”order. Multipleimages
withthesamemodels,props,andscenes,withdifferentarrangementsfromphotoshootsaresourced.
Fromthesesets,wecuratearelationgraphdescribingthewayinwhichtheimagesdifferintermsof
objects,actions,attributes,andcomposition. Wethenselectaheadnodebasedoncentralityinthis
relationgraph. Finally,apromptiswrittentodescribethisheadnodewitherrorcount0. Anexample
isinthecenterpanelofFigure3.
Weproducedasetofnaturalimageerrorgraphstoassesswhetherdistributionaldifferencesbetween
syntheticimagesandrealimagesmightleadtomeasurableimpactsonperformanceforthefaithfulness
metricsthattypicallyusebasemodelspretrainedexclusivelyonnaturalimages[38].
Real. Therealerror,syntheticimageexamplesarefollowinga“prompt,image,graph”order. From
astartingprompt,eitherwrittenbyusorsourcedfromtheaforementionedpromptsources,weuse
oneoftheT2Imodelstogenerateasetofimages,exclusivelyusingthisstartingpromptwithout
modifications.
Whenweidentifyapromptthatcausessomemodeltoproducemanyimagescontainingerrors,we
firstcounttheamountoferrorsineachgeneratedimage,anddrawtheerrorgraphbetweenthemto
producethefinalSEG.ThisprocedureisdocumentedintherightpanelofFigure3.
4 T2IScoreScoreMeta-Metrics
Agoodpromptcoherencemetricshouldcorrectlyreproducetherankorderofthesemanticdistance
byscoringmoredistantimagesaslesscoherentthannearerones(ordering)andshouldseparatesets
ofimagesatdifferentnodes(separation). Wecheckbothofthesepropertiesbycomparingscores
assignedbyametrictoeachsemanticerrorgraph(SEG)
ForeachSEGS ∈ SEGS weintroducemeasuresofametricm: rank m(S)andsep m(S). Both
ofthesemeasuresarederivedfromwell-establishedstatistics,appliedovereveryvalidwalkW of
adjacentnodesN throughgraphS. TheaverageoftheserankandsepoverallS representthefinal
TS2scoresforametric. Foreaseofnotation,werefertoeachSEGasasetofvaliddescendingwalks
3Availableathttps://pexels.com
5CLIPScorenode-wise CDF
1
(a)
0.8
0.5
0.6
1
Node 0 0 1b 1b 2b 2b 0.4
CLIPScore↑ 0.87 0.7 0.7 0.69 0.33 0.27 0.2
TIFA-LLaVA↑ 1 1 0.71 0.71 0.57 0.57 0
0.4 0.6 0.8 10 0.5
LLMScore↓ 0 1 6 1 14 5
TIFA-LLaVAnode-wise CDF
1
(b) 0.8
0.6 1 1
0.4
Node 0 1a 1a 1a 1a 1a
0.2
CLIPScore↑ 0.65 0.54 0.5 0.49 0.57 0.52
0
TIFA-LLaVA↑ 0.4 0.4 0.2 0.4 0.4 0.4 0.5 0.75 1 0.3 0.5 0.7
LLMScore↓ 6 4 4 4 4 5 Node 0 Node 1a Node 2a Node 1a
Figure4: Examplesofscoresassignedbythreemetricstoexamplesfromaneasy(a)andhard(b)
semanticerrorgraph(left). Computationoftheseparationscoresep (S)fortwometricsisdepicted
m
attheright. Colorcodingofeachcellcorrespondstothemetric’sscorefortheimagebeingbetter
(blue)orworse(red);morecorrelatedmeasures(presentingahigherrankorderscorerank (S))
m
willshowthesameprogressionfromredtoblue(a),whileharder-to-rankexampleswillnot(b).
W ∈Sovernodesofincreasingerrorcount(eg,(0,1a,2a),(0,1a,2b),etc),whereeachwalkdefined
asthein-ordersetofall(image,prompt,num. error)triples(I,P,N)∈W,eventhougheachSEG
containsonlyoneprompt,andnodesmaycontainmultipleimages.
4.1 Rankingcorrectnessassessment(Ordering)
WeuseSpearman’srankcorrelationcoefficientr [47]betweenimage-levelrankandmetric-assigned
s
scoreovereverydescendingwalkontheerroraccumulationtreetoassesshowwelleachmetric
correctlyorderseachimageset. ItisdefinedasthePearson’scorrelationoftherankordersR(X),
R(Y)forsetsX,Y:
r s(X,Y)= cov σ(R(X σ),R(Y)) ; R(X)=(cid:8) (cid:88) 1(x
i
<x)(cid:12) (cid:12)x
i
∈X(cid:9) (1)
R(X) R(Y)
xi∈X
Thus,inourcasetheSEG-levelrankorderscorerank (S)isdefinedas:
m
1 (cid:88)
rank (S)= r ({m(I,P)|(I,P,N)∈W},{N|(I,P,N)∈W}) (2)
m |S| s
W∈S
OnelimitationforusingSpearman’srforcharacterizingsetsofscoresisthatitisundefinedifone
setR(U)exclusivelycontainsidenticalelements,asσ =0. Fortractabilityinthesescenarios
R(U)
wedefiner (·,U) := 0,whichisareasonabledefinitiontoadopt,asifametricassignsidentical
s
scorestoallexamplesacrossdifferenterrorlevels,itpresentsnodiscerniblerelationshipbetween
errorseverityandscoreforthatimageset.
4.2 Assessingseparationoferrorpopulations(Separation)
We assess the two-sample Kolmogorov–Smirnov statistic [19] pairwise between the populations
ofmetricm’sscoresassignedtoeachsamplebetweentwoerrornodesn andn aspopulations.
i j
The Kolmogorov–Smirnov statistic is a non-parametric measure of the separation between two
distributions[37,2],definedasthemaximumverticaldifferencebetweentheirempiricalcumulative
distributionfunctionsD (X,Y):
KS
D (X,Y)= sup |F (x)−F (x)| (3)
KS X Y
x∈Rm
6WhereF (x)isproportionofsamplesinpopulationX forwhichthemetricvaluem(i)≤x,(see
X
Figure4foravisualdepiction). WecomputeD foreverypairofadjacenterrornodesineachtree
KS
walkW4,andreporttheaverageoveralloftheseastheSEGseparationscoresep (S):
m
1 (cid:88)
sep (S)= D ({m(P,I)|(P,I)∈N },{m(P,I)|(P,I)∈N }) (4)
m |S| KS i i+1
Ni∈W
5 Experiments
Using T2IScoreScore we evaluated three kinds of T2I evaluation metrics: embedding-based
(comparingembeddingswithinoracrossmodalities),caption-based(comparingcaptionsextracted
fromthegeneratedimagestotheoriginalprompt)question-based(usingVQAorVLMstoanswer
generatedquestionsfromtheprompt). Withintheseclassesofmetrics,weevaluatedusingmultiple
differentmultimodalLMsasbackbonesforquestionansweringorcaptioning.
Foreachmetric,wescoreeveryimageineverySEGagainstitsrespectiveprompt. Wereportthe
resultsofour(section4)OrderingandSeparationmetricsacrossallSEGs,aswellasforourproposed
subsetsofSEGs.
5.1 Embedding-correlationMetrics
Thefirstclassoftext-to-imagescoringtechniquesweconsidereddirectlycompareencodingsofthe
promptandimageinasharedembeddingspace,usingeg. cosinesimilarity.
CLIPScore [11]isapopularapproachforcomparingthefaithfulnessofacaptiontoanimage
(analogoustothetaskofcomparingasyntheticimage’sfaithfulnesstoitsprompt)andhasconse-
quentlybecomepopularforevaluatingtext-to-imagemodels. CLIPScoreiscomputedasthecosine
similarityoftheL -normalizedCLIP-assessed[38]imageandtextembeddingsforCLIPmodelM,
2
forimageiandpromptp:
clip-s(p,i)=cos(||M I(i)|| 2,||M T(p)|| 2) (5)
ALIGNScore (nottobeconfusedwiththetext-onlyAlignScore[56])isavariantembedding-based
similarityscorewhereweusetheALIGN[16]dualtext-imageembeddingmodelfromKakaoBrain
toembedthepromptandimage. ItiscomputedequivalentlytoEquation5,usingalign-basefor
modelM.
5.2 QG/AMetrics
Question Generation & Answering Metrics (QG/A) first use an LLM M to produce a set of
QG
requirementquestion/answerpairs(q,a) ∈ Qfromthepromptp,andthenuseavision-language
modelM tocheckifeachrequirementhasbeensatisfied,reportingtherequirementsatisfaction
VL
rateastheimage’sfaithfulnessscorerelativetotheprompt. Alternaterealizationsofthistechnique
varyprimarilyinhowquestionsaregeneratedandhowtheyrelatetoeachother.
TIFA [13]promptsanLLM(eg,GPT-3)togenerateasetofmultiplechoiceandyes-noquestions
andtheirexpectedanswersrelativetotheprompt. ThenavisionlanguagemodelM produces
VL
“free-form”answerstoeachquestion,whichareconvertedintomultiplechoiceanswersa′usingan
SBERTmodel. TheTIFAscoreforagivenimageisthentherateofcorrectanswers,
1 (cid:88)
tifa-s(p,i)=
|Q|
1(M B(M VL(i,q))=a) (6)
(q,a)∈Q
4WereformulateW toreturnnodesN ∈W containingpairsofprompt,image(P,I)∈W tomakethis
equationeasiertoread.
7DSG (Davidsonianscenegraph)[6]sharesQAstructureofTIFA,butgeneratesasetofrequirement
questionswhicharenon-overlapping,haveexclusivelyyes/noanswers,andsitonadirectedacyclic
graphsuchthataquestionisonlysatisfiedifitandallitsparentquestionsareansweredyes.
5.3 Caption-comparisonMetrics
Thefinalclassofmetricsweconsideruseatwo-tieredapproach,firstgeneratingcaptionscofthe
generated images i using some captioning model M , and then comparing the captions to the
C
originalpromptpusingsomeLMM .
L
LLMScore [30]capturesthefine-grainedsimilaritybetweentheimageandtextwithrationalesby
leveragingthevisualdetailsunderstandingcapabilityfromvisionexpertsandthereasoningcapability
ofLLMs. Thevisualinformationisparsedinhierarchicalscenedescriptionswithglobalandlocal
captions. Thenthetext-onlyLLMswillcomparethemulti-granularityvisualdescriptionswiththe
inputtextprompttogiveascoreaccordingtotheevaluationguidelineprompt.
VIEScore [20]ratesaspectsofsemanticconsistency(SC)andperceptualquality(PQ)ultimately
providingaratingscoreonascaleof0and10. Weuse0-shotLLaVA-1.5asthebackboneMLLMto
evaluatehowsuccessfullytheimagefollowsthetext-to-imageprompt.
5.4 EvaluatingmultiplebackendVLMs
AllQG/AandCaption-comparisonmetricsrelyontheuseofagenerativevision-languagemodel
(VLM)eitherforperformingquestionanswering(M insubsection5.2)orcaptioning(M in
VL C
subsection5.3). ThankstothesimpledecomposableframeworkoftheQG/Amethodologies,we
wereabletoefficientlytesttheperformanceofbothTIFAandDSGusingfivedifferentVLMsas
visualquestion-answeringbackendsM . Wetested:
VL
mPLUG isaclassofvision-languagemodelsthatuseskipconnectionsbetweenvisualencoder
embeddinglayersbetweencross-modalattentionblocksinthetransformerstack[23]. Weusethe
mPLUG-OWL[53]7bcheckpointwhichusesLLaMA7b[49]asthepretrainedtextencoder.
LLaVA isafine-tuneofVicuna[57](decoder-onlytransformermodel)thatusesalearnedMLP
“vision-language connector” layer to map a single input image’s CLIP encodings into a shared
embeddingspace[28,27]. WeuseLLaVA1.513b. BecauseLLaVAwasinstructionfine-tunedfor
chatapplications,weexperimentwithavariantsystempromptthatrequestsconciseanswersfrom
thesystem. WemarkthisalternateoptionLLaVa1.5(alt)inplotsandfigures.
BLIP isajointly-trainedself-attentionViTtrainedwithcrossattentiontomultipletransformer
encoder and decoder pipelines with different tasks [24]. We use the BLIP encoder/causal LM
decodercombinationasatransformerencoder-decodermodeltoproduceVQAanswersfromthe
blip-vqa-basecheckpoint.
InstructBLIP extendsBLIPbyincludinganinstructionfine-tuned“Q-Former”thatselectssalient
instruction-relatedvisualfeaturesfromafrozenViTforinputtoafrozenLLMthatanswersthequery
conditionedontheselectedfeatures[7]. Weuseinstructblip-flan-t5-xl.
Fuyu isadecoder-onlyVLMthatsplitsaninputimageintoasequenceofpatchesthatareseparately
projecteddirectlyintothetransformerembeddingspace,whichjointlylearnsViTandLMbehaviors
[1]. WeuseFuyu-8b.
6 Results
Table 2 shows the results for the Spearman Correlation Ordering feature rank and the K-S
m
Separationfeaturesep foreachmetricweassessed,onaverageforallSEGs(Avg),andthethree
m
SEGsubsetsSynth,Nat,andReal.
8CLIPScore
ALIGNScore
mPLUG
LLaVA1.5
LLaVA1.5(alt)
InstructBLIP
BLIP1
Fuyu
mPLUG
LLaVA1.5
LLaVA1.5(alt)
InstructBLIP
BLIP1
Fuyu
LLMScoreEC LLMScoreOver
VIEScore
Emb-based TIFA DSG Caption-based
Avg 71.4 73.9 71.0 74.5 74.4 76.5 73.8 38.7 70.4 76.2 75.0 79.0 76.6 29.5 48.8 57.7 37.8
Synth 75.0 77.6 72.6 79.2 79.2 80.2 78.8 44.5 74.6 80.1 81.6 85.1 81.6 35.4 50.2 61.6 42.5
Nat 58.0 70.2 66.9 62.8 64.0 65.1 62.2 23.5 65.3 65.9 68.8 70.7 71.6 20.5 36.2 44.4 22.4
Real 69.3 62.6 68.2 66.7 64.5 71.6 64.0 29.7 58.4 70.0 54.2 62.0 61.2 14.2 54.4 54.1 33.2
Avg 90.7 92.8 80.6 84.3 81.9 85.0 81.8 67.2 78.4 83.1 80.3 84.2 80.8 63.6 73.6 73.5 51.8
Synth 90.5 94.1 80.6 87.5 85.2 86.7 84.1 67.3 80.9 85.7 83.9 87.8 84.9 65.8 71.1 72.8 53.7
Nat 91.5 92.6 84.2 83.4 75.6 82.8 77.9 75.7 71.2 80.9 76.7 81.8 73.3 68.6 80.5 76.7 44.5
Real 90.3 87.9 77.4 72.7 74.4 80.5 76.4 59.3 75.1 74.5 69.4 71.9 70.8 50.3 77.3 73.6 50.7
Table2: AverageSpearmanCorrelationrank (Ord.) andKolmogorov–Smirnovseparationscore
m
sep (Sep.) foreachmodel(reportedas%forreadability). Bestbold,within2%ofbestitalic,top
m
threecoloredbymetrictype(emb-based,TIFA,DSG).
Prompt: A yellow school bus and a red stop sign and
SEG 143 a blue car.
Node 0
Q1: Is there a yellow school bus?
Yellow school VQA: Yes there is a yellow school bus in
b Bu lus e ✅ car ✅ t Qh 2e : i Im s a thg ee r. e ( C ao rr ere dc st, t os ph so iu gl nd ? be yes) Cor Sre cc ot r eDSG DSG LLaVA CLIPScore
Red stop sign ✅ VQA: No (Incorrect, should be yes) 1 (3/3) 0.33 0.67
Q3: Is there a blue car?
Target Score = 1 VQA: No (Incorrect, should be yes)
Node 1a
Q1: Is there a yellow school bus?
Yellow school VQA: Yes there is a yellow school bus in
bus ❌ the image. (Incorrect, should be no) Correct DSG DSG LLaVA CLIPScore
Blue car ✅ Q2: Is there a red stop sign? Score
Red stop sign ✅ VQA: No (Incorrect, should be yes) 0.67 (2/3) 0.33 0.56
Q3: Is there a blue car?
Target Score = 0.67 VQA: No (Incorrect, should be yes)
Figure5: Exampleoftwoimagesonnodes0and1fromahardSEGthatarecorrectlyseparated(and
ranked)byCLIPScorebutarenotseparatedbyDSG-LLaVA.
Asweexpected,wefoundthattheSynthsetconsistingofhand-designed(andprobablymoreobvious)
errorswastheeasiestsubsetforallmetricstocorrectlyorder. Theaveragerank scoreforSynth
m
acrossallmetricswas70%,forNat55%andforReal56%. However,differentsubsetswerehardfor
differentclassesofmetrics. FortheEmbedding-correlationandCaption-comparisonmetrics,Nat
wasthehardersubsetthanReal. FortheQG/Ametrics,Realwasharder.
As the embedding-based metricsare the original prompt faithfulness techniques proposed, TIFA
[13], DSG [6], LLMScore [30], and VIEScore [20] all compare themselves against CLIPScore
[11]intheirintroductoryworks. Tooursurprise,despitethecommandingleadthesemetricshold
overLLMScoreontheirownad-hocevaluations,thesimpler(andlesscomputationallyexpensive)
embedding-correlationmetricsarecompetitive(oroutperform)theseVLM-basedmetrics(TIFA,
DSG,caption-based),particularlyonthehardestsubsets.
Intermsofseparatingthenodewisepopulationsofimagescores(Sep.),theembedding-basedmetrics
arestrictlybetterthantheVLM-basedones.
6.1 Discussion
OnedisadvantageofusingSpearman’sristhatit‘expects’tiestobethesameinbothdistributions.For
example,ifasetofimageshaserrorcount(0,1,1,2),theordering(1,0.5,0.5,0)willhaveaperfect
r = 1, while the ordering (1,0.51,0.49,0) will be penalized, despite it also presenting a correct
9
.drO
.peSAverage Correlation of Score Real Error Correlation Scores
70 70
ALIGNScore 10052 55 45 32 24 48 45 51 65 33 44 21 28 48 56 32 ALIGNScore 10057 66 61 43 18 48 27 38 77 3 22 8 9 63 85 45
DSG-BLIP1 5210074 27 38 9 57 52 66 57 40 42 13 24 53 41 24 DSG-BLIP1 5710072 11 45-1169 59 85 77 29 54 -4 -1277 71 20
TIFA-BLIP1 55 7410035 35 27 56 54 58 57 40 53 27 35 57 43 32 60 TIFA-BLIP1 66 7210026 44 34 71 69 64 83 25 69 14 -0 72 72 5 60
CLIPScore 45 27 3510026 26 35 35 18 31 14 29 12 28 23 29 35 CLIPScore 61 11 26100 7 32 23 2 -1329-14 -1 -1 -1 9 35 35
DSG-Fuyu 32 38 35 2610031 31 27 34 35 24 31 10 20 40 30 22 DSG-Fuyu 43 45 44 7 10018 28 39 45 59 13 46 -8 1 47 39 10
50 50
TIFA-Fuyu 24 9 27 26 3110011 4 8 15 -1 22 3 10 16 14 19 TIFA-Fuyu 18-1134 32 18100 7 23 -9 25 -4 36-18 3 15 -1 -13
DSG-InstructBLIP 48 57 56 35 31 1110070 59 52 49 47 21 29 48 47 13 DSG-InstructBLIP 48 69 71 23 28 7 10067 66 65 30 62 25 -7 60 64 -6
TIFA-InstructBLIP 45 52 54 35 27 4 7010054 59 56 60 19 39 46 48 14 40 TIFA-InstructBLIP 27 59 69 2 39 23 6710073 68 45 82 4 13 48 42-20 40
DSG-LLaVA (alt) 51 66 58 18 34 8 59 5410066 65 61 18 25 62 46 28 DSG-LLaVA (alt) 38 85 64-1345 -9 66 7310071 64 65 16 9 78 56 10
TIFA-LLaVA (alt) 65 57 57 31 35 15 52 59 6610054 73 9 18 48 67 33 30 TIFA-LLaVA (alt) 77 77 83 29 59 25 65 68 7110021 64 -1 -9 84 81 23 30
DSG-LLaVA 33 40 40 14 24 -1 49 56 65 5410062 16 26 50 40 30 DSG-LLaVA 3 29 25-1413 -4 30 45 64 2110043 48 47 40 9 12
TIFA-LLaVA 44 42 53 29 31 22 47 60 61 73 6210014 22 41 44 36 TIFA-LLaVA 22 54 69 -1 46 36 62 82 65 64 43100 1 3 55 37-10
LLMScore EC 21 13 27 12 10 3 21 19 18 9 16 1410067 20 12 19 20 LLMScore EC 8 -4 14 -1 -8 -1825 4 16 -1 48 1 10057 13 22 16 20
LLMScore Over 28 24 35 28 20 10 29 39 25 18 26 22 6710022 21 19 LLMScore Over 9 -12 -0 -1 1 3 -7 13 9 -9 47 3 57100 3 -5 29
DSG-MPlug 48 53 57 23 40 16 48 46 62 48 50 41 20 2210052 26 10 DSG-MPlug 63 77 72 9 47 15 60 48 78 84 40 55 13 3 10072 26 10
TIFA-MPlug 56 41 43 29 30 14 47 48 46 67 40 44 12 21 5210021 TIFA-MPlug 85 71 72 35 39 -1 64 42 56 81 9 37 22 -5 7210031
VIEScore 32 24 32 35 22 19 13 14 28 33 30 36 19 19 26 21100 VIEScore 45 20 5 35 10-13 -6 -2010 23 12-1016 29 26 31100
0 0
(a)AllSEGs (b)RealSEGsonly
Figure6: CorrelationbetweentheSpearmancorrelationscoreforeachprompttreeforeachmetric,
foralltrees(a)andfortherealerrorsubset(b).
ordering.ThismeansthatourOrderingscoresep systematicallypunishestheembedding-based
m
metricsrelativetotheVLM-basedones, astheembedding-correlationmetricsCLIPScoreand
ALIGNScorecantakecontinuousvalues,whieTIFA,DSG,LLMScore,andVIEScorehaveadiscrete
range. Inlightofthisdisparitythestrongembeddingmetricperformanceisstriking.
AnotherinterestingweaknessoftheQG/AmetricsisthatmanyunluckysituationswheretheVLM
backendpresentsamixoftrueandfalsepositivesthatcauseincorrectrankingsorpoorseparation
(DSGfailstoordersampleswhileCLIPScoresucceedsinFigure5)tooccur. However,theseVLM
failures cases are interpretable and can be targeted; T2IScoreScore will hopefully drive future
workinmakingLMsmorerobusttothesesortsoferrorsforVQAtomitigatethisissue. Inaddition
totheseinterpretabilityadvantages,themoresophisticatedVLM-basedmetricsstilldopresentbetter
subjectivehumanpreferencecorrelationthanCLIPScore[13,6,30,20]. Byfocusingexclusivelyon
objectivesimilar-imageorderingandseparation,TS2iseffectivelyorthogonaltothesepreference
evals.
GiventhedocumentedbiasesLLMshaveindirectlyoutputtingnumbers[48,33],itisn’tasurprise
thatthetechniquewhichdirectlypromptsVLMstooutputanumericalpreferencevalue(VIEScore)
isatpresenttheleastrobust.
In general it seems that the most successful methods that leverage VLMs (TIFA and DSG) still
ultimatelyproducescoresusingadeterministicalgorithm. TheyuseVLMsinaperceptualmannerto
separatelycheckeachrequirement,butthefinalscoreistheaccuracyestimatefromeachseparate
VQAquestion. ThiscomportswiththetheoriesofLLMfunctionthattreatitasa“system1”[50];
effectivelyTIFAandDSGareexamplesofVLM-moduloframeworksoutperformingpureLLMson
thetaskofpromptcoherencescoring[17].
Ultimately,allimagecoherenceevaluationmetricsstandtoimprovefromfurtheradvancesingeneral
VLMquality. Duetoratelimitsatthetimeofsubmission,wecouldn’tfeasiblyassessthemetrics
usingGPT4Vasabackbone. AsaconsiderablystrongermodelthanLLaVA,mPLUG,etc,itislikely
thattheperformanceoftheVLM-basedmetricswillbeboostedwhenGPT4Visused.
ArethesameSEGshardforthesamemodels?Figure6andFigure7presentcorrelationplotsbetween
SEG-wiserank andsep scoresrespectivelybetweeneachpairofmetrics. Forbothweshow(a)
m m
thecorrelationsoverallSEGs,and(b)thecorrelationsbetweenonlySEGsintheRealsubset. These
plotsshowthatbroadly,similarmethodshavesimilar“blindspot”SEGs,whiledifferentonescan
varywildlyintermsofwhichexamplestheysucceedandfailatorderingandseparating. Notethatall
TIFAorDSGQG/Ametricshaveappreciablecorrelationtoeachother,providedtheyuseastrong
enoughVLM.ThemetricsemployingweakVLMssuchasFuyudonotperformwell. Similarly,the
10
erocSNGILA 1PILB-GSD 1PILB-AFIT erocSPILC uyuF-GSD uyuF-AFIT PILBtcurtsnI-GSD PILBtcurtsnI-AFIT )tla(
AVaLL-GSD
)tla(
AVaLL-AFIT
AVaLL-GSD AVaLL-AFIT CE
erocSMLL
revO
erocSMLL
gulPM-GSD gulPM-AFIT erocSEIV erocSNGILA 1PILB-GSD 1PILB-AFIT erocSPILC uyuF-GSD uyuF-AFIT PILBtcurtsnI-GSD PILBtcurtsnI-AFIT )tla(
AVaLL-GSD
)tla(
AVaLL-AFIT
AVaLL-GSD AVaLL-AFIT CE
erocSMLL
revO
erocSMLL
gulPM-GSD gulPM-AFIT erocSEIVAverage Correlation of Score Real Error Correlation Scores
70 70
ALIGNScore 10040 49 59 23 38 47 50 45 49 30 47 36 38 31 41 36 ALIGNScore 100 9 43 41-1049 17 46 27 40 -7 30 61 37 14 59 62
DSG-BLIP1 4010063 29 28 17 54 45 59 42 41 29 20 29 50 37 24 DSG-BLIP1 9 10030-23 3 -1455 1 72 35 35-1022-2072 40 1
TIFA-BLIP1 49 6310045 18 22 58 61 51 44 35 53 24 44 45 47 31 60 TIFA-BLIP1 43 3010014-2528 64 62 17 53 16 48 47 35 33 64 9 60
CLIPScore 59 29 4510025 28 35 49 24 26 30 32 30 50 35 38 27 CLIPScore 41-231410023 32-2017-30-12-3320 25 46 -8 8 9
DSG-Fuyu 23 28 18 2510026 14 6 16 15 20 11 1 12 11 26 19 DSG-Fuyu -10 3 -2523100-4 -43-4913-41 -1 -30 -2 6 12-15 -6
50 50
TIFA-Fuyu 38 17 22 28 2610020 25 16 32 17 24 25 26 11 41 13 TIFA-Fuyu 49-1428 32 -4100-5 38 -6 28-1410 64 15 3 51 13
DSG-InstructBLIP 47 54 58 35 14 2010061 50 50 58 50 14 25 40 48 19 DSG-InstructBLIP 17 55 64-20-43 -510062 48 54 40 41 34 5 50 51 6
TIFA-InstructBLIP 50 45 61 49 6 25 6110038 45 46 53 32 39 41 51 28 40 TIFA-InstructBLIP 46 1 62 17-4938 62100 4 55 -6 66 46 21 6 56 28 40
DSG-LLaVA (alt) 45 59 51 24 16 16 50 3810053 53 45 26 27 57 49 26 DSG-LLaVA (alt) 27 72 17-3013 -6 48 4 10027 60 1 40-1664 43 33
TIFA-LLaVA (alt) 49 42 44 26 15 32 50 45 5310041 57 28 30 37 56 27 30 TIFA-LLaVA (alt) 40 35 53-12-4128 54 55 2710028 37 53 12 49 70 9 30
DSG-LLaVA 30 41 35 30 20 17 58 46 53 4110050 12 28 52 51 13 DSG-LLaVA -7 35 16-33 -1 -1440 -6 60 2810016 30 16 57 22-21
TIFA-LLaVA 47 29 53 32 11 24 50 53 45 57 5010012 31 35 47 23 TIFA-LLaVA 30-1048 20-3010 41 66 1 37 1610018 28 8 49 11
LLMScore EC 36 20 24 30 1 25 14 32 26 28 12 1210052 23 24 17 20 LLMScore EC 61 22 47 25 -2 64 34 46 40 53 30 1810038 52 73 32 20
LLMScore Over 38 29 44 50 12 26 25 39 27 30 28 31 5210036 36 20 LLMScore Over 37-2035 46 6 15 5 21-1612 16 28 3810030 11 1
DSG-MPlug 31 50 45 35 11 11 40 41 57 37 52 35 23 3610057 21 10 DSG-MPlug 14 72 33 -8 12 3 50 6 64 49 57 8 52 3010051-13 10
TIFA-MPlug 41 37 47 38 26 41 48 51 49 56 51 47 24 36 5710031 TIFA-MPlug 59 40 64 8 -1551 51 56 43 70 22 49 73 11 5110024
VIEScore 36 24 31 27 19 13 19 28 26 27 13 23 17 20 21 31100 VIEScore 62 1 9 9 -6 13 6 28 33 9 -2111 32 1 -1324100
0 0
(a)AllSEGs (b)RealSEGsonly
Figure7: CorrelationbetweentheKolmogorov-SmirnovstatisticSep. correlationscoreforeach
prompttreeforeachmetric,foralltrees(a)andfortherealerrorsubset(b).
twoLLMScoremetricsarehighlycorrelatedtoeachother;thepureVLMnumericalratingmethods
arenotproducingrandomnoise. ThesecorrelationsarestrongerinthefullsetofSEGs(including
naturalimagesandtheeasy,pre-designedSynthSEGs)thantheyareinthehardestRealsetofSEGs.
Althoughthescoresofmanymodelsonourmetricarehigh,thismeta-evaluationisfarfrom“solved.”
Inprincipleitshouldbepossibletogetmuchcloserto100onaverageforbothmeta-metricsthanwe
find. WeviewuseofTS2asanecessarysecondaryevaluationforanynewproposedT2Ifaithfulness
metric; ifithashighcorrelationtosubjectivehumanjudgementsbutdoesnotperformwellhere,
skepticismmightbewarranted.
7 Conclusion
Inconclusion,ourinvestigationintotheeffectivenessofvarioustext-to-image(T2I)promptfaith-
fulnessmetricsthroughtheintroductionofT2IScoreScore,acomprehensivedatasetofsemantic
errorgraphs,revealscriticalinsightsintothecurrentstateofevaluatingT2Imodelperformance. The
relativelysmallnumberofpromptsvsimages(andconsequenthighimage-promptratio)uniquely
enablesourmeta-metricstoobjectivelyanalyzeprompt-imagecoherencemetrics,byassessingtheir
abilitytoseparateandordersemanticallydifferentsubpopulationsrelativetoaspecificprompt.
Despite the sophistication of vision-language models (VLMs) and other advanced metrics, our
findings indicate that simpler feature-based metrics like CLIPScore still do perform comparably
onthisobjectiveevaluation,especiallywhendealingwithcomplex,naturally-occurringT2Imodel
errors. Thisunderscoresthenecessityforamorenuancedapproachtobenchmarkinganddeveloping
metricscapableofcapturingthesubtlesemanticnuancesbetweenpromptsandgeneratedimages.
TheestablishmentofT2IScoreScoreasabenchmarkingtoolisasignificantstepforward,offering
astructuredwaytorigorouslytestandimproveT2Ipromptfaithfulnessmetrics,ensuringtheycan
more accurately reflect the semantic coherence between prompts and generated images, thereby
facilitatingthedevelopmentofmorereliableandeffectiveT2Imodels.
Acknowledgements
WethanktheorganizersandsponsorsoftheFatimaAl-FihriPredoctoralFellowshipprogramfor
computeresources. ThisworkwassupportedinpartbytheNationalScienceFoundationGraduate
ResearchFellowshipunderGrantNo. 1650114,andCAREERAwardunderGrantNo. 2048122.
11
erocSNGILA 1PILB-GSD 1PILB-AFIT erocSPILC uyuF-GSD uyuF-AFIT PILBtcurtsnI-GSD PILBtcurtsnI-AFIT )tla(
AVaLL-GSD
)tla(
AVaLL-AFIT
AVaLL-GSD AVaLL-AFIT CE
erocSMLL
revO
erocSMLL
gulPM-GSD gulPM-AFIT erocSEIV erocSNGILA 1PILB-GSD 1PILB-AFIT erocSPILC uyuF-GSD uyuF-AFIT PILBtcurtsnI-GSD PILBtcurtsnI-AFIT )tla(
AVaLL-GSD
)tla(
AVaLL-AFIT
AVaLL-GSD AVaLL-AFIT CE
erocSMLL
revO
erocSMLL
gulPM-GSD gulPM-AFIT erocSEIVImpactStatement
Thisresearchsignificantlycontributestosteeringtext-to-image(T2I)generationtechnologyinthe
right direction by addressing the critical need for precise evaluation of the semantic coherence
betweenpromptsandthegeneratedimages. TheintroductionofT2IScoreScore,withitssemantic
errorgraphs,offersanewbenchmarkingtoolthatnotonlyteststhefidelityofT2Ipromptfaithfulness
metrics but also sheds light on the comparative effectiveness of simpler versus more complex
evaluationmodels. Thisnuancedunderstandinghelpsinrefiningtheevaluationmetrics,whichis
essentialforguidingthedevelopmentofT2Imodelstowardsgeneratingimagesthatareaccurately
alignedtotheirprompts.
ContributionStatement
MScheckedasubsetoftheSEGs,designedthebenchmarkandmeta-metrics,implementedtheSEG
treeiterationprocessandevaluationcodefortheSpearmanOrderingandK–SSeparationscores,
collatedtheQG/AanswersintoID-levelscores,andassessedthefinalscores.
MKproducedtheNatSEGsandproduced,annotated,andcheckedasubsetoftheRealSEGs. MK
generatedtheTIFAandDSGquestionsforallprompts,implementedandevaluatedCLIPScoreand
ALIGNScore,collectedanswersfortheQG/AmetricsfromBLIP,InstructBLIP,andrefactoredcode.
FJproducedandannotatedtheSynthSEGsandproduced,annotated,andcheckedasubsetofthe
RealSEGs. FJcollectedanswersforFuyufortheQG/Ametricsandcleanedandorganizedthefinal
datasetrelease.
YLevaluatedLLMScorefortheexamplesandconceivedofmeasuringfaithfulnesserrorsinT2I
faithfulnessmetrics. AScollectedanswersfortheQG/AmetricsfromLLaVAandVIEScore.
References
[1] R.Bavishi,E.Elsen,C.Hawthorne,M.Nye,A.Odena,A.Somani,andS.Tas¸ırlar. Introducing
ourmultimodalmodels(fuyu-8b),2023. URLhttps://www.adept.ai/blog/fuyu-8b. 8
[2] V.W. Berger and Y. Zhou. Kolmogorov–smirnov test: Overview. Wiley statsref: Statistics
referenceonline,2014. 6
[3] F. Bianchi, P. Kalluri, E. Durmus, F. Ladhak, M. Cheng, D. Nozza, T. Hashimoto, D. Ju-
rafsky, J. Zou, and A. Caliskan. Easily Accessible Text-to-Image Generation Amplifies
Demographic Stereotypes at Large Scale. In Proceedings of the 2023 ACM Conference
on Fairness, Accountability, and Transparency, FAccT ’23, pages 1493–1504. Association
for Computing Machinery. ISBN 9798400701924. doi: 10.1145/3593013.3594095. URL
https://dl.acm.org/doi/10.1145/3593013.3594095. 3
[4] X.Chen,H.Fang,T.-Y.Lin,R.Vedantam,S.Gupta,P.Dollar,andC.L.Zitnick. Microsoft
cococaptions: Datacollectionandevaluationserver,2015. 3
[5] J.Cho,A.Zala,andM.Bansal. Dall-eval: Probingthereasoningskillsandsocialbiasesof
text-to-imagegenerationmodels,2023. 3
[6] J. Cho, Y. Hu, R. Garg, P. Anderson, R. Krishna, J. Baldridge, M. Bansal, J. Pont-Tuset,
andS.Wang. Davidsonianscenegraph: Improvingreliabilityinfine-grainedevaluationfor
text-to-imagegeneration,2024. 2,3,8,9,10,17
[7] W.Dai,J.Li,D.Li,A.M.H.Tiong,J.Zhao,W.Wang,B.Li,P.Fung,andS.Hoi. Instructblip:
Towardsgeneral-purposevision-languagemodelswithinstructiontuning,2023. 8
[8] E.L.Denton,S.Chintala,R.Fergus,etal. Deepgenerativeimagemodelsusingalaplacian
pyramidofadversarialnetworks. Advancesinneuralinformationprocessingsystems,28,2015.
2
[9] W.Feng,X.He,T.-J.Fu,V.Jampani,A.Akula,P.Narayana,S.Basu,X.E.Wang,andW.Y.
Wang. Training-freestructureddiffusionguidanceforcompositionaltext-to-imagesynthesis.
arXivpreprintarXiv:2212.05032,2022. 2,3
12[10] W. Feng, W. Zhu, T.-j. Fu, V. Jampani, A. Akula, X. He, S. Basu, X. E. Wang, and W. Y.
Wang. Layoutgpt: Compositionalvisualplanningandgenerationwithlargelanguagemodels.
AdvancesinNeuralInformationProcessingSystems,36,2024. 2
[11] J. Hessel, A. Holtzman, M. Forbes, R. Le Bras, and Y. Choi. Clipscore: A reference-free
evaluationmetricforimagecaptioning. InProceedingsofthe2021ConferenceonEmpirical
MethodsinNaturalLanguageProcessing,pages7514–7528,2021. 2,7,9
[12] M. Hodosh, P. Young, and J. Hockenmaier. Framing image description as a ranking task:
Data, models and evaluation metrics. In M. Wooldridge and Q. Yang, editors, IJCAI 2015
- Proceedings of the 24th International Joint Conference on Artificial Intelligence, IJCAI
InternationalJointConferenceonArtificialIntelligence,pages4188–4192.InternationalJoint
ConferencesonArtificialIntelligence,2015. 24thInternationalJointConferenceonArtificial
Intelligence,IJCAI2015;Conferencedate: 25-07-2015Through31-07-2015. 3
[13] Y.Hu,B.Liu,J.Kasai,Y.Wang,M.Ostendorf,R.Krishna,andN.A.Smith. TIFA:Accurate
and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering. URL
http://arxiv.org/abs/2303.11897. 2,3,7,9,10,17
[14] K.Huang,K.Sun,E.Xie,Z.Li,andX.Liu. T2i-compbench: Acomprehensivebenchmarkfor
open-worldcompositionaltext-to-imagegeneration,2023. 3
[15] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional
adversarialnetworks. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages1125–1134,2017. 2
[16] C.Jia,Y.Yang,Y.Xia,Y.-T.Chen,Z.Parekh,H.Pham,Q.Le,Y.-H.Sung,Z.Li,andT.Duerig.
Scalingupvisualandvision-languagerepresentationlearningwithnoisytextsupervision. In
Internationalconferenceonmachinelearning,pages4904–4916.PMLR,2021. 7
[17] S.Kambhampati,K.Valmeekam,L.Guan,K.Stechly,M.Verma,S.Bhambri,L.Saldyt,and
A.Murthy. Llmscan’tplan,butcanhelpplanninginllm-moduloframeworks. arXivpreprint
arXiv:2402.01817,2024. 10
[18] Y.Kirstain,A.Polyak,U.Singer,S.Matiana,J.Penna,andO.Levy.Pick-a-pic:Anopendataset
ofuserpreferencesfortext-to-imagegeneration. AdvancesinNeuralInformationProcessing
Systems,36,2024. 3
[19] A.N.Kolmogorov. Sulladeterminazioneempiricadiunaleggedidistribuzione. GiornDell’inst
ItalDegliAtt,4:89–91,1933. 2,6
[20] M.Ku, D.Jiang, C.Wei, X.Yue, andW.Chen. Viescore: Towardsexplainablemetricsfor
conditionalimagesynthesisevaluation. arXivpreprintarXiv:2312.14867,2023. 2,8,9,10
[21] M.Ku,T.Li,K.Zhang,Y.Lu,X.Fu,W.Zhuang,andW.Chen. Imagenhub: Standardizingthe
evaluationofconditionalimagegenerationmodels,2023. 2,3
[22] K.Lee,H.Liu,M.Ryu,O.Watkins,Y.Du,C.Boutilier,P.Abbeel,M.Ghavamzadeh,andS.S.
Gu. Aligningtext-to-imagemodelsusinghumanfeedback. arXivpreprintarXiv:2302.12192,
2023. 2
[23] C.Li,H.Xu,J.Tian,W.Wang,M.Yan,B.Bi,J.Ye,H.Chen,G.Xu,Z.Cao,etal. mplug:
Effectiveandefficientvision-languagelearningbycross-modalskip-connections.arXivpreprint
arXiv:2205.12005,2022. 8
[24] J.Li,D.Li,C.Xiong,andS.Hoi. Blip: Bootstrappinglanguage-imagepre-trainingforunified
vision-languageunderstandingandgeneration. InICML,2022. 8
[25] L. Lian, B. Li, A. Yala, and T. Darrell. Llm-grounded diffusion: Enhancing prompt un-
derstanding of text-to-image diffusion models with large language models. arXiv preprint
arXiv:2305.13655,2023. 2
13[26] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,P.Dollár,andC.L.Zitnick.
Microsoftcoco: Commonobjectsincontext. InComputerVision–ECCV2014: 13thEuropean
Conference,Zurich,Switzerland,September6-12,2014,Proceedings,PartV13,pages740–755.
Springer,2014. 4
[27] H.Liu,C.Li,Y.Li,andY.J.Lee. Improvedbaselineswithvisualinstructiontuning,2023. 8
[28] H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning,2023. 8
[29] N.Liu,S.Li,Y.Du,A.Torralba,andJ.B.Tenenbaum. Compositionalvisualgenerationwith
composablediffusionmodels. InEuropeanConferenceonComputerVision,pages423–439.
Springer,2022. 2
[30] Y.Lu,X.Yang,X.Li,X.E.Wang,andW.Y.Wang. Llmscore: Unveilingthepoweroflarge
languagemodelsintext-to-imagesynthesisevaluation. arXivpreprintarXiv:2305.11116,2023.
2,8,9,10
[31] J.P.McKenna,S.Choudhary,M.Saxon,G.P.Strimel,andA.Mouchtaris.Semanticcomplexity
inend-to-endspokenlanguageunderstanding. arXivpreprintarXiv:2008.02858,2020. 2
[32] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon. Sdedit: Guided image
synthesisandeditingwithstochasticdifferentialequations. arXivpreprintarXiv:2108.01073,
2021. 2
[33] D. Petrak, N. S. Moosavi, and I. Gurevych. Improving the numerical reasoning skills of
pretrainedlanguagemodels. arXivpreprintarXiv:2205.06733,2022. 10
[34] V.Petsiuk,A.E.Siemenn,S.Surbehera,Z.Chin,K.Tyser,G.Hunter,A.Raghavan,Y.Hicke,
B.A.Plummer,O.Kerret,etal. Humanevaluationoftext-to-imagemodelsonamulti-task
benchmark. arXivpreprintarXiv:2211.12112,2022. 2
[35] B.A.Plummer,L.Wang,C.M.Cervantes,J.C.Caicedo,J.Hockenmaier,andS.Lazebnik.
Flickr30kentities: Collectingregion-to-phrasecorrespondencesforricherimage-to-sentence
models,2016. 3
[36] D.Podell,Z.English,K.Lacey,A.Blattmann,T.Dockhorn,J.Müller,J.Penna,andR.Rombach.
Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimagesynthesis,2023. 4
[37] J.W.Pratt,J.D.Gibbons,J.W.Pratt,andJ.D.Gibbons. Kolmogorov-smirnovtwo-sample
tests. Conceptsofnonparametrictheory,pages318–344,1981. 6
[38] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clark,etal.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,2021. 5,7
[39] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever.
Zero-shottext-to-imagegeneration. InInternationalConferenceonMachineLearning,pages
8821–8831.PMLR,2021. 2
[40] A.Ramesh,P.Dhariwal,A.Nichol,C.Chu,andM.Chen. Hierarchicaltext-conditionalimage
generationwithcliplatents,2022. 4
[41] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer.High-resolutionimagesynthesis
withlatentdiffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR),pages10684–10695,June2022. 4
[42] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon-
tijoLopes,B.KaragolAyan,T.Salimans,etal. Photorealistictext-to-imagediffusionmodels
withdeeplanguageunderstanding. AdvancesinNeuralInformationProcessingSystems,35:
36479–36494,2022. 2,3
[43] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved
techniquesfortraininggans. Advancesinneuralinformationprocessingsystems,29,2016. 2
14[44] M. Saxon and W. Y. Wang. Multilingual conceptual coverage in text-to-image models. In
A.Rogers,J.Boyd-Graber,andN.Okazaki,editors,Proceedingsofthe61stAnnualMeeting
oftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages4831–4848,
Toronto,Canada,July2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.
acl-long.266. URLhttps://aclanthology.org/2023.acl-long.266. 3
[45] M.Saxon,X.Wang,W.Xu,andW.Y.Wang. Peco: Examiningsinglesentencelabelleakagein
naturallanguageinferencedatasetsthroughprogressiveevaluationofclusteroutliers. InPro-
ceedingsofthe17thConferenceoftheEuropeanChapteroftheAssociationforComputational
Linguistics,pages3053–3066,2023. 2
[46] M.Saxon,Y.Luo,S.Levy,C.Baral,Y.Yang,andW.Y.Wang. Lostintranslation? translation
errors and challenges for fair assessment of text-to-image models on multilingual concepts.
arXivpreprintarXiv:2403.11092,2024. 3
[47] C.Spearman. Theproofandmeasurementofassociationbetweentwothings. TheAmerican
Journal of Psychology, 15(1):72–101, 1904. ISSN 00029556. URL http://www.jstor.
org/stable/1412159. 2,6
[48] A.Thawani,J.Pujara,andF.Ilievski.Numeracyenhancestheliteracyoflanguagemodels.InM.-
F.Moens,X.Huang,L.Specia,andS.W.-t.Yih,editors,Proceedingsofthe2021Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,pages6960–6967,OnlineandPuntaCana,
DominicanRepublic,Nov.2021.AssociationforComputationalLinguistics. doi: 10.18653/v1/
2021.emnlp-main.557. URLhttps://aclanthology.org/2021.emnlp-main.557. 10
[49] H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozière,N.Goyal,
E.Hambro,F.Azhar,A.Rodriguez,A.Joulin,E.Grave,andG.Lample. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023. 8
[50] K.Valmeekam,A.Olmo,S.Sreedharan,andS.Kambhampati. Largelanguagemodelsstill
can’t plan (a benchmark for llms on planning and reasoning about change). arXiv preprint
arXiv:2206.10498,2022. 10
[51] S. Wu, H. Fei, H. Zhang, and T.-S. Chua. Imagine that! abstract-to-intricate text-to-image
synthesiswithscenegraphhallucinationdiffusion. AdvancesinNeuralInformationProcessing
Systems,36,2024. 2
[52] M.Yarom,Y.Bitton,S.Changpinyo,R.Aharoni,J.Herzig,O.Lang,E.Ofek,andI.Szpektor.
Whatyouseeiswhatyouread? improvingtext-imagealignmentevaluation,2023. 3
[53] Q.Ye,H.Xu,G.Xu,J.Ye,M.Yan,Y.Zhou,J.Wang,A.Hu,P.Shi,Y.Shi,C.Jiang,C.Li,
Y.Xu,H.Chen,J.Tian,Q.Qi,J.Zhang,andF.Huang. mplug-owl: Modularizationempowers
largelanguagemodelswithmultimodality,2023. 8
[54] J.Yu,Y.Xu,J.Y.Koh,T.Luong,G.Baid,Z.Wang,V.Vasudevan,A.Ku,Y.Yang,B.K.Ayan,
etal. Scalingautoregressivemodelsforcontent-richtext-to-imagegeneration. arXivpreprint
arXiv:2206.10789,2(3):5,2022. 3,4
[55] J.Zakraoui,M.Saleh,S.Al-Maadeed,andJ.M.Jaam. Improvingtext-to-imagegeneration
withobjectlayoutguidance. MultimediaToolsandApplications,80(18):27423–27443,2021. 2
[56] Y.Zha,Y.Yang,R.Li,andZ.Hu. Alignscore: Evaluatingfactualconsistencywithaunified
alignmentfunction. arXivpreprintarXiv:2305.16739,2023. 7
[57] L.Zheng,W.-L.Chiang,Y.Sheng,S.Zhuang,Z.Wu,Y.Zhuang,Z.Lin,Z.Li,D.Li,E.P.Xing,
H.Zhang,J.E.Gonzalez,andI.Stoica. Judgingllm-as-a-judgewithmt-benchandchatbot
arena,2023. 8
[58] W.Zhu,X.E.Wang,P.Narayana,K.Sone,S.Basu,andW.Y.Wang. Towardsunderstanding
samplevarianceinvisuallygroundedlanguagegeneration: Evaluationsandobservations. arXiv
preprintarXiv:2010.03644,2020. 2
15A SupplementaryDetails
Formoreinformationonthestructureofthesemanticerrorgraphs(SEGs),weprovideexamples
here. SEG85isoneexamplewithamoreinterestingtopologythantheexampleinFigure1. Figure8
hasastructureincludingtwo-erroredges,single-parentnodes,single-childnodes,multi-parentnodes,
andmulti-childnodesinthesamegraph,correspondingtoprompt“guywithumbrellahatsittingata
tablewithanotherpersonwithahatunderaredumbrella.”
Figure8: ExampleofaSEG(85)withamorecomplexstructure. Somenodeshavemultiplechild
nodes,andsomeedgescorrespondtomorethanoneerror(darkred).
Figure9exemplifieswhywechoosetoonlyscorerankorderalongwalksofthegraph,ratherthan
betweenallpairsofnodes. Apriorithere’snoreasonthebeach-lessimagesshouldbeworsethanthe
umbrellas,yetmetricsconsistentlyratethebeacherrormoresevere.
Node 0 Node 1a Node 1b
Avg Scores: Avg Scores:
Avg Scores:
CLIPScore: .82 CLIPScore: .69
TIFA-L: .78 TIFA-L: .44 CLIPScore: .37
TIFA-mP: . 85 1 TIFA-mP: .78 TIFA-L: .15
TIFA-mP: .24
DSG-L: .78 DSG-L: .22
DSG-MP: 1.0 DSG-mP: .89 DSG-L: .20
LLM-EC: 0.33 0 2 LLM-EC: 1.6 0 DSG-mP: .30 0
LLM-EC: 5.2
No umbrellas
3 4 5 1 2 1 2
No beach
Figure9: ExamplesfromSEG71(Thebeachiscrowdedwithredandwhiteumbrellas). Eventhough
both nodes 1a and 1b have the same error count (1) they systematically differ across all metrics:
allmetricspunishtheimageswheretheumbrellasarejustinwater(nobeach,1b)morethanthey
penalizeanemptybeachwithnoumbrellas(1a).
16alt)
P
alt)
P
( I ( I
5 5 L 5 5 L
mPLUG
LLaVA1. LLaVA1. InstructB
BLIP1
Fuyu
mPLUG
LLaVA1. LLaVA1. InstructB
BLIP1
Fuyu
DSGw/TIFAaccumulation DSGw/DSGaccumulation
Avg 70.4 76.2 75 79.0 76.6 29.5 68.8 80.0 75.6 80.2 76.9 35.8
Synth 74.6 80.1 81.6 85.1 81.6 35.4 73.5 83.8 82.1 86.1 81.7 45.5
Nat 65.3 65.9 68.8 70.7 71.6 20.5 61.9 74.9 68.9 70.2 71 21.5
Real 58.4 70.0 54.2 62 61.2 14.2 56.4 69.6 55.9 65.8 62.8 10
Avg 78.4 83.1 80.3 84.2 80.8 63.6 75.5 82.5 80.5 84.3 80.6 66
Synth 80.9 85.7 83.9 87.8 84.9 65.8 77.1 85.5 83.8 88.8 84.1 68.7
Nat 71.2 80.9 76.7 81.8 73.3 68.6 70.6 75.1 77.2 81.5 75.1 71
Real 75.1 74.5 69.4 71.9 70.8 50.3 73.1 76.8 70.6 68.9 71.4 50.8
Table3: ComparinghowusingDSGvsTIFA-styleaccumulationforscoringeachimagebyDSG
questionsimpactsperformancealongbothourmetrics. Therighthalfofthistableisidenticaltothe
DSGsectioninTable2,andbold,italic,andhighlightingfollowsthesamerules,exceptcellsinthe
TIFAhalfaremarkedasiftheywerereplacingtherighthalfcellsintheDSGsectioninTable2.
B SupplementaryResults
B.1 ComparingDSGquestionevaluationusingDSGandTIFAscoreaccumulationmethods
ThefirstfewstepsofTIFA[13]andDavidsonianSceneGraph(DSG)[6]scoringmethodsarenearly
identical: anLLMgeneratesasetofrequirementsasquestions,andaVQAsystemanswersthem.
However,thetwomethodsdifferchieflyinhowtheanswersarecombinedintoasingleimage-level
score. TIFAsimplyscoresimagesbythecorrectanswerrate,whileDSGusesthegraphstructureof
therequirementstobuildinsomerobustness: ifanupstreamrequirementisnotmet(e.g.,isthere
aboy? : no),thendownstreamrequirementsareallalsoassessedasnotbeingmet,regardlessof
answer. Intheexampleprovided,ifthequestion“istheboy’sshirtgreen?” wereansweredyes,the
DSGaccumulationtechniquewouldstillscorethisrequirementasbeingnotmet,duetotheupstream
requirement,whiletheTIFAaccumulationmethodwouldscoreitasbeingmet.
As a supplementary experiment, we compare how accumulating the DSG questions using the
DSGtechniquecomparestoaccumulatingthemwiththeTIFAtechniqueinTable3. Interstingly,
the impact of this change differs between strong and weak VLMs, between ordering and and
separation scores, and between the easier and harder subsets. For example, switching from the
DSGtoTIFAstyleacculumationconsistentlyimprovesorderingperformanceformPLUG,whileit
worsensperformanceforLLaVA,InstructBLIP,andBLIP1. ForFuyu,theweakestmodel,DSGstyle
accumulationsignificantlyimprovesperformanceoverTIFA.ThisstrengthenstheclaimfromCho
etal.[6]thatusingthescenegraphtocheckrequirementsaddsrobustness;itmakesalotofsense
thatthisrobustnessbenefitsthelowest-performingVQAsystemsthemost.
Forseparationscores,TIFAaccumulationimprovesperformanceofmoremodels. Inparticular,TIFA
accumulationpushesInstructBLIPintothetop3forseparationontheSynthsubset,whilenoDSG
metricusingDSGaccumulationbreaksintothetop3(redhighlightedcell).
B.2 ModelwiseSpearmanOrderingScoreHistograms
HereweprovidefullhistogramsforourSpearmanOrderingandKolmogorov–SmirnovSeparation
scores,acrosseverySEG,forallmetricsweassessed.
CLIPScore ALIGNScore
100 100
80 80
60 60
40 40
20 20
0 0
1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0
17
.drO
.peSTIFA-mPLUG TIFA-LLaVA
100 100
80 80
60 60
40 40
20 20
0 0
0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
TIFA-instructBLIP TIFA-BLIP1
100 100
80 80
60 60
40 40
20 20
0 0
0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0
TIFA-Fuyu DSG-mPLUG
100 100
80 80
60 60
40 40
20 20
0 0
1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0
DSG-LLaVA DSG-LLaVA (alt)
100 100
80 80
60 60
40 40
20 20
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
DSG-instructBLIP DSG-BLIP1
100 100
80 80
60 60
40 40
20 20
0 0
0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
DSG-Fuyu LLMScore EC
100 100
80 80
60 60
40 40
20 20
0 0
0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 0.50 0.25 0.00 0.25 0.50 0.75 1.00
LLMScore Over VIEScore
100 100
80 80
60 60
40 40
20 20
0 0
0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8
B.3 ModelwiseK–SSeparationScoreHistograms
CLIPScore ALIGNScore
100 100
80 80
60 60
40 40
20 20
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
TIFA-mPLUG TIFA-LLaVA
100 100
80 80
60 60
40 40
20 20
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
18TIFA-instructBLIP TIFA-BLIP1
100 100
80 80
60 60
40 40
20 20
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
TIFA-Fuyu DSG-mPLUG
100 100
80 80
60 60
40 40
20 20
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
DSG-LLaVA DSG-LLaVA (alt)
100 100
80 80
60 60
40 40
20 20
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
DSG-instructBLIP DSG-BLIP1
100 100
80 80
60 60
40 40
20 20
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
DSG-Fuyu LLMScore EC
100 100
80 80
60 60
40 40
20 20
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
LLMScore Over VIEScore
100 100
80 80
60 60
40 40
20 20
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
19HereweprovidelineplotsforasetofmetricsandSEGs. Notethatfornormalized_rank,higher
isworse(moreerrors). High-correlationisassessedwhenthemetriclines(higherbetter)godownas
themetriclinesgoup.
Scores for ID = 5
1.0 llava_tifa
llava_dsg
0.8 clipscore_norm
normalized_rank
0.6
0.4
0.2
0.0
0 5 10 15 20 25 30
Images ID (0 to 31)
Scores for ID = 53
1.0 llava_tifa
llava_dsg
0.8 mplug_tifa
mplug_dsg
0.6 normalized_rank
0.4
0.2
0.0
0 5 10 15 20 25 30 35
Images ID (0 to 37)
Scores for ID = 80
1.0 llava-alt_tifa
llava-alt_dsg
0.8 fuyu_tifa
fuyu_dsg
0.6 instructblip_tifa
instructblip_dsg
0.4 clipscore_norm
normalized_rank
0.2
0.0
0 5 10
Images ID (0 to 10)
Scores for ID = 106
1.0 llava-alt_tifa
llava-alt_dsg
0.8 fuyu_tifa
fuyu_dsg
0.6 instructblip_tifa
instructblip_dsg
0.4 clipscore_norm
normalized_rank
0.2
0.0
0 5 10
Images ID (0 to 10)
Scores for ID = 148
1.0 fuyu_tifa
fuyu_dsg
0.8 instructblip_tifa
instructblip_dsg
0.6 clipscore_norm
normalized_rank
0.4
0.2
0.0
0 5 10 15 20 25 30 35 40 45 50
Images ID (0 to 50)
20
erocS
erocS
erocS
erocS
erocSC SupplementaryAnalysis
Figure10andFigure11showscatterplotsfortheOrdering(Spearman)andSeparation(KSstatistic)
scoresforeverySEGbetweenthemosthighly-correlated(a,b)andlow-correlation(c,d)pairsof
metricsunderevaluation,respectively.Figure12andFigure13showallthemetric-metriccorrelations
forOrderingandSeparation,respectively.
BothofthesesetsoffiguresconfirmthatsimilarunderlyingVLMsby-and-large“think”similarlyin
termsofscoringmodels,evenoverdifferentsetsofquestions(TIFAandDSG).Thissuggeststhat
developmentofoverallbetterVLMswillgeneralizetomanydifferenttypesofVLMevaluations.
1.00 1.00
0.75 0.75
0.50
0.50
0.25
0.25
0.00
0.00
0.25
0.25
0.50
0.50 Synthetic Error, Synthetic Image (Synth) 0.75 Synthetic Error, Synthetic Image (Synth)
Synthetic Error, Natural Image (Nat) Synthetic Error, Natural Image (Nat)
0.75 Natural Error, Synthetic Image (Real) 1.00 Natural Error, Synthetic Image (Real)
0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
TIFA-BLIP1 TIFA-LLaVA (alt)
(a)Top-1 (b)Top-2
1.00
1.0
0.75
0.8
0.50
0.6
0.25 0.4
0.00
0.2
0.0 0.25
0.2 0.50
Synthetic Error, Synthetic Image (Synth) Synthetic Error, Synthetic Image (Synth)
0.4 Synthetic Error, Natural Image (Nat) 0.75 Synthetic Error, Natural Image (Nat)
Natural Error, Synthetic Image (Real) Natural Error, Synthetic Image (Real)
0.6 1.00
1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
BLIPScore CLIPScore
(c)Bottom-1 (d)Bottom-2
Figure10: Scatterplotscomparingthetwomostcorrelatedmetrics(a,b)bySpearmancorrelation
OrderingscoreacrosstheSynth,Nat,andRealpopulations,andthetwoleast-correlated(c,d). Note
thatthetwohighest-correlatedmetricsarebothQG/AmetricsusingthesameunderlyingVLM(DSG
andTIFAusingBLIP1,(a);TIFAusingLLaVAwithtwodifferentsystemprompts,(b)).
21
1PILB-GSD
erocSNGILA
AVaLL-AFIT
erocSPILB1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
Synthetic Error, Synthetic Image (Synth) Synthetic Error, Synthetic Image (Synth)
Synthetic Error, Natural Image (Nat) Synthetic Error, Natural Image (Nat)
0.0 Natural Error, Synthetic Image (Real) 0.0 Natural Error, Synthetic Image (Real)
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
DSG-BLIP1 TIFA-instructBLIP
(a)Top-1 (b)Top-2
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
Synthetic Error, Synthetic Image (Synth) Synthetic Error, Synthetic Image (Synth)
Synthetic Error, Natural Image (Nat) Synthetic Error, Natural Image (Nat)
0.0 Natural Error, Synthetic Image (Real) 0.0 Natural Error, Synthetic Image (Real)
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
BLIPScore DSG-Fuyu
(c)Bottom-1 (d)Bottom-2
Figure11: Scatterplotscomparingthetwomostcorrelatedmetrics(a,b)byKolmogorov–Smirnov
SeparationscoreacrosstheSynth,Nat,andRealpopulations,andthetwoleast-correlated(c,d). Note
thatthetwohighest-correlatedmetricsarebothQG/Ametricsusingthesameorrelatedunderlying
VLMs(DSGandTIFAusingBLIP1,(a);TIFAusingBLIP1andInstructBLIP,(b)).
22
1PILB-AFIT
AVaLL-AFIT
1PILB-AFIT
CE
erocSMLLAverage Correlation of Score Synthetic Error Correlation Scores
70 70
ALIGNScore 10052 55 45 32 24 48 45 51 65 33 44 21 28 48 56 32 ALIGNScore 10045 47 52 30 30 62 61 50 59 50 53 23 25 37 54 34
DSG-BLIP1 5210074 27 38 9 57 52 66 57 40 42 13 24 53 41 24 DSG-BLIP1 4510076 29 34 13 52 49 50 46 41 37 23 40 43 33 22
TIFA-BLIP1 55 7410035 35 27 56 54 58 57 40 53 27 35 57 43 32 60 TIFA-BLIP1 47 7610030 32 17 53 51 55 48 46 48 36 45 57 42 35 60
CLIPScore 45 27 3510026 26 35 35 18 31 14 29 12 28 23 29 35 CLIPScore 52 29 3010027 21 38 50 33 52 38 45 12 32 34 56 32
DSG-Fuyu 32 38 35 2610031 31 27 34 35 24 31 10 20 40 30 22 DSG-Fuyu 30 34 32 2710034 27 25 29 30 19 27 7 17 30 37 27
50 50
TIFA-Fuyu 24 9 27 26 3110011 4 8 15 -1 22 3 10 16 14 19 TIFA-Fuyu 30 13 17 21 3410023 6 18 8 8 15 12 17 13 23 19
DSG-InstructBLIP 48 57 56 35 31 1110070 59 52 49 47 21 29 48 47 13 DSG-InstructBLIP 62 52 53 38 27 2310058 54 58 60 52 11 23 41 48 29
TIFA-InstructBLIP 45 52 54 35 27 4 7010054 59 56 60 19 39 46 48 14 40 TIFA-InstructBLIP 61 49 51 50 25 6 5810048 66 64 68 16 35 48 50 35 40
DSG-LLaVA (alt) 51 66 58 18 34 8 59 5410066 65 61 18 25 62 46 28 DSG-LLaVA (alt) 50 50 55 33 29 18 54 4810053 64 56 12 24 52 48 40
TIFA-LLaVA (alt) 65 57 57 31 35 15 52 59 6610054 73 9 18 48 67 33 30 TIFA-LLaVA (alt) 59 46 48 52 30 8 58 66 5310070 85 2 20 30 64 43 30
DSG-LLaVA 33 40 40 14 24 -1 49 56 65 5410062 16 26 50 40 30 DSG-LLaVA 50 41 46 38 19 8 60 64 64 7010075 1 15 55 53 41
TIFA-LLaVA 44 42 53 29 31 22 47 60 61 73 6210014 22 41 44 36 TIFA-LLaVA 53 37 48 45 27 15 52 68 56 85 75100 9 25 36 59 45
LLMScore EC 21 13 27 12 10 3 21 19 18 9 16 1410067 20 12 19 20 LLMScore EC 23 23 36 12 7 12 11 16 12 2 1 9 10068 28 2 24 20
LLMScore Over 28 24 35 28 20 10 29 39 25 18 26 22 6710022 21 19 LLMScore Over 25 40 45 32 17 17 23 35 24 20 15 25 6810028 21 25
DSG-MPlug 48 53 57 23 40 16 48 46 62 48 50 41 20 2210052 26 10 DSG-MPlug 37 43 57 34 30 13 41 48 52 30 55 36 28 2810055 37 10
TIFA-MPlug 56 41 43 29 30 14 47 48 46 67 40 44 12 21 5210021 TIFA-MPlug 54 33 42 56 37 23 48 50 48 64 53 59 2 21 5510034
VIEScore 32 24 32 35 22 19 13 14 28 33 30 36 19 19 26 21100 VIEScore 34 22 35 32 27 19 29 35 40 43 41 45 24 25 37 34100
0 0
(a)AllSEGs (b)Synth
Natural Image Correlation Scores Real Error Correlation Scores
70 70
ALIGNScore 10049 47 33 8 7 17 31 59 56 17 45 36 48 38 33 11 ALIGNScore 10057 66 61 43 18 48 27 38 77 3 22 8 9 63 85 45
DSG-BLIP1 4910075 47 8 7 39 50 46 43 31 19 16 35 11 18 22 DSG-BLIP1 5710072 11 45-1169 59 85 77 29 54 -4 -1277 71 20
TIFA-BLIP1 47 7510042 10 29 35 37 46 34 22 31 18 38 24 16 33 60 TIFA-BLIP1 66 7210026 44 34 71 69 64 83 25 69 14 -0 72 72 5 60
CLIPScore 33 47 4210023 23 33 24 14 -0 -1715 13 29 10-1033 CLIPScore 61 11 26100 7 32 23 2 -1329-14 -1 -1 -1 9 35 35
DSG-Fuyu 8 8 10 2310018 13 5 6 6 19 3 33 28 40 1 -8 DSG-Fuyu 43 45 44 7 10018 28 39 45 59 13 46 -8 1 47 39 10
50 50
TIFA-Fuyu 7 7 29 23 18100-26-35-12 4 -41 6 -4 -13 9 0 34 TIFA-Fuyu 18-1134 32 18100 7 23 -9 25 -4 36-18 3 15 -1 -13
DSG-InstructBLIP 17 39 35 33 13-2610093 41 17 34 9 37 61 34 35-20 DSG-InstructBLIP 48 69 71 23 28 7 10067 66 65 30 62 25 -7 60 64 -6
TIFA-InstructBLIP 31 50 37 24 5 -359310041 27 43 10 33 62 31 46-31 40 TIFA-InstructBLIP 27 59 69 2 39 23 6710073 68 45 82 4 13 48 42-20 40
DSG-LLaVA (alt) 59 46 46 14 6 -1241 4110076 62 64 42 40 49 38 18 DSG-LLaVA (alt) 38 85 64-1345 -9 66 7310071 64 65 16 9 78 56 10
TIFA-LLaVA (alt) 56 43 34 -0 6 4 17 27 7610044 52 29 29 33 62 6 30 TIFA-LLaVA (alt) 77 77 83 29 59 25 65 68 7110021 64 -1 -9 84 81 23 30
DSG-LLaVA 17 31 22-1719-4134 43 62 4410041 22 22 33 35 2 DSG-LLaVA 3 29 25-1413 -4 30 45 64 2110043 48 47 40 9 12
TIFA-LLaVA 45 19 31 15 3 6 9 10 64 52 4110035 24 25 11 43 TIFA-LLaVA 22 54 69 -1 46 36 62 82 65 64 43100 1 3 55 37-10
LLMScore EC 36 16 18 13 33 -4 37 33 42 29 22 3510074 8 26 -2 20 LLMScore EC 8 -4 14 -1 -8 -1825 4 16 -1 48 1 10057 13 22 16 20
LLMScore Over 48 35 38 29 28-1361 62 40 29 22 24 7410022 41-14 LLMScore Over 9 -12 -0 -1 1 3 -7 13 9 -9 47 3 57100 3 -5 29
DSG-MPlug 38 11 24 10 40 9 34 31 49 33 33 25 8 2210025-17 10 DSG-MPlug 63 77 72 9 47 15 60 48 78 84 40 55 13 3 10072 26 10
TIFA-MPlug 33 18 16-10 1 0 35 46 38 62 35 11 26 41 25100-27 TIFA-MPlug 85 71 72 35 39 -1 64 42 56 81 9 37 22 -5 7210031
VIEScore 11 22 33 33 -8 34-20-3118 6 2 43 -2 -14-17-27100 VIEScore 45 20 5 35 10-13 -6 -2010 23 12-1016 29 26 31100
0 0
(c)Nat (d)RealSEGsonly
Figure12: CorrelationbetweentheSpearmancorrelationscoreforeachprompttreeforeachmetric,
forallSEGs(a),forthesyntheticerrorSEGs(b),forthenaturalimage/syntheticerrorSEGs(c)and
fortherealerrorsubset(d).
23
erocSNGILA
erocSNGILA
1PILB-GSD
1PILB-GSD
1PILB-AFIT
1PILB-AFIT
erocSPILC
erocSPILC
uyuF-GSD
uyuF-GSD
uyuF-AFIT
uyuF-AFIT
PILBtcurtsnI-GSD
PILBtcurtsnI-GSD
PILBtcurtsnI-AFIT
PILBtcurtsnI-AFIT
)tla(
AVaLL-GSD
)tla(
AVaLL-GSD
)tla(
AVaLL-AFIT
)tla(
AVaLL-AFIT
AVaLL-GSD
AVaLL-GSD
AVaLL-AFIT
AVaLL-AFIT
CE
erocSMLL
CE
erocSMLL
revO
erocSMLL
revO
erocSMLL
gulPM-GSD
gulPM-GSD
gulPM-AFIT
gulPM-AFIT
erocSEIV
erocSEIV
erocSNGILA
erocSNGILA
1PILB-GSD
1PILB-GSD
1PILB-AFIT
1PILB-AFIT
erocSPILC
erocSPILC
uyuF-GSD
uyuF-GSD
uyuF-AFIT
uyuF-AFIT
PILBtcurtsnI-GSD
PILBtcurtsnI-GSD
PILBtcurtsnI-AFIT
PILBtcurtsnI-AFIT
)tla(
AVaLL-GSD
)tla(
AVaLL-GSD
)tla(
AVaLL-AFIT
)tla(
AVaLL-AFIT
AVaLL-GSD
AVaLL-GSD
AVaLL-AFIT
AVaLL-AFIT
CE
erocSMLL
CE
erocSMLL
revO
erocSMLL
revO
erocSMLL
gulPM-GSD
gulPM-GSD
gulPM-AFIT
gulPM-AFIT
erocSEIV
erocSEIVAverage Correlation of Score Synthetic Error Correlation Scores
70 70
ALIGNScore 10040 49 59 23 38 47 50 45 49 30 47 36 38 31 41 36 ALIGNScore 10044 50 63 27 38 66 56 44 55 46 52 41 44 35 44 37
DSG-BLIP1 4010063 29 28 17 54 45 59 42 41 29 20 29 50 37 24 DSG-BLIP1 4410071 39 30 28 52 55 52 43 38 33 27 47 44 38 36
TIFA-BLIP1 49 6310045 18 22 58 61 51 44 35 53 24 44 45 47 31 60 TIFA-BLIP1 50 7110051 26 30 63 68 65 50 46 58 28 57 53 54 43 60
CLIPScore 59 29 4510025 28 35 49 24 26 30 32 30 50 35 38 27 CLIPScore 63 39 5110023 34 63 59 40 47 51 42 36 59 48 51 37
DSG-Fuyu 23 28 18 2510026 14 6 16 15 20 11 1 12 11 26 19 DSG-Fuyu 27 30 26 2310035 27 11 20 40 19 29 9 21 10 37 33
50 50
TIFA-Fuyu 38 17 22 28 2610020 25 16 32 17 24 25 26 11 41 13 TIFA-Fuyu 38 28 30 34 3510033 26 25 36 28 38 18 32 16 42 20
DSG-InstructBLIP 47 54 58 35 14 2010061 50 50 58 50 14 25 40 48 19 DSG-InstructBLIP 66 52 63 63 27 3310065 55 55 64 56 19 44 40 53 33
TIFA-InstructBLIP 50 45 61 49 6 25 6110038 45 46 53 32 39 41 51 28 40 TIFA-InstructBLIP 56 55 68 59 11 26 6510053 49 57 55 33 49 48 52 33 40
DSG-LLaVA (alt) 45 59 51 24 16 16 50 3810053 53 45 26 27 57 49 26 DSG-LLaVA (alt) 44 52 65 40 20 25 55 5310051 53 51 33 44 56 58 35
TIFA-LLaVA (alt) 49 42 44 26 15 32 50 45 5310041 57 28 30 37 56 27 30 TIFA-LLaVA (alt) 55 43 50 47 40 36 55 49 5110045 65 36 41 29 60 41 30
DSG-LLaVA 30 41 35 30 20 17 58 46 53 4110050 12 28 52 51 13 DSG-LLaVA 46 38 46 51 19 28 64 57 53 4510060 13 33 55 58 24
TIFA-LLaVA 47 29 53 32 11 24 50 53 45 57 5010012 31 35 47 23 TIFA-LLaVA 52 33 58 42 29 38 56 55 51 65 6010024 35 37 56 26
LLMScore EC 36 20 24 30 1 25 14 32 26 28 12 1210052 23 24 17 20 LLMScore EC 41 27 28 36 9 18 19 33 33 36 13 2410055 23 14 28 20
LLMScore Over 38 29 44 50 12 26 25 39 27 30 28 31 5210036 36 20 LLMScore Over 44 47 57 59 21 32 44 49 44 41 33 35 5510039 43 32
DSG-MPlug 31 50 45 35 11 11 40 41 57 37 52 35 23 3610057 21 10 DSG-MPlug 35 44 53 48 10 16 40 48 56 29 55 37 23 3910060 38 10
TIFA-MPlug 41 37 47 38 26 41 48 51 49 56 51 47 24 36 5710031 TIFA-MPlug 44 38 54 51 37 42 53 52 58 60 58 56 14 43 6010043
VIEScore 36 24 31 27 19 13 19 28 26 27 13 23 17 20 21 31100 VIEScore 37 36 43 37 33 20 33 33 35 41 24 26 28 32 38 43100
0 0
(a)AllSEGs (b)Synth
Natural Image Correlation Scores Real Error Correlation Scores
70 70
ALIGNScore 10041 45 68 17 4 -23 1 64 38-1634 -0 7 17 -8 -1 ALIGNScore 100 9 43 41-1049 17 46 27 40 -7 30 61 37 14 59 62
DSG-BLIP1 4110059 38 37 -6 42 36 56 37 42 35 23 3 54 36 -9 DSG-BLIP1 9 10030-23 3 -1455 1 72 35 35-1022-2072 40 1
TIFA-BLIP1 45 5910046 27-2926 13 34 14 7 32 2 -5 14 -3 3 60 TIFA-BLIP1 43 3010014-2528 64 62 17 53 16 48 47 35 33 64 9 60
CLIPScore 68 38 4610048 -8 -7 26 18 -8 8 7 4 -7 -7 -8 13 CLIPScore 41-231410023 32-2017-30-12-3320 25 46 -8 8 9
DSG-Fuyu 17 37 27 48100-1 16 42-22-1316-24-11-25 7 27 9 DSG-Fuyu -10 3 -2523100-4 -43-4913-41 -1 -30 -2 6 12-15 -6
50 50
TIFA-Fuyu 4 -6 -29 -8 -1100-20 -7 4 35 -5 -9 7 -7 -3 8 -5 TIFA-Fuyu 49-1428 32 -4100-5 38 -6 28-1410 64 15 3 51 13
DSG-InstructBLIP -2342 26 -7 16-2010042 12 18 47 30 9 -8 37 35-16 DSG-InstructBLIP 17 55 64-20-43 -510062 48 54 40 41 34 5 50 51 6
TIFA-InstructBLIP 1 36 13 26 42 -7 4210015 23 62 21 32 10 32 41 -4 40 TIFA-InstructBLIP 46 1 62 17-4938 62100 4 55 -6 66 46 21 6 56 28 40
DSG-LLaVA (alt) 64 56 34 18-22 4 12 1510075 29 65 15 33 57 30-13 DSG-LLaVA (alt) 27 72 17-3013 -6 48 4 10027 60 1 40-1664 43 33
TIFA-LLaVA (alt) 38 37 14 -8 -1335 18 23 7510032 46 12 28 51 46 5 30 TIFA-LLaVA (alt) 40 35 53-12-4128 54 55 2710028 37 53 12 49 70 9 30
DSG-LLaVA -1642 7 8 16 -5 47 62 29 3210046 14 35 42 71 16 DSG-LLaVA -7 35 16-33 -1 -1440 -6 60 2810016 30 16 57 22-21
TIFA-LLaVA 34 35 32 7 -24 -9 30 21 65 46 46100-7 35 40 27 15 TIFA-LLaVA 30-1048 20-3010 41 66 1 37 1610018 28 8 49 11
LLMScore EC -0 23 2 4 -11 7 9 32 15 12 14 -710053 16 7 -22 20 LLMScore EC 61 22 47 25 -2 64 34 46 40 53 30 1810038 52 73 32 20
LLMScore Over 7 3 -5 -7 -25 -7 -8 10 33 28 35 35 5310039 32 -2 LLMScore Over 37-2035 46 6 15 5 21-1612 16 28 3810030 11 1
DSG-MPlug 17 54 14 -7 7 -3 37 32 57 51 42 40 16 3910057-23 10 DSG-MPlug 14 72 33 -8 12 3 50 6 64 49 57 8 52 3010051-13 10
TIFA-MPlug -8 36 -3 -8 27 8 35 41 30 46 71 27 7 32 57100-1 TIFA-MPlug 59 40 64 8 -1551 51 56 43 70 22 49 73 11 5110024
VIEScore -1 -9 3 13 9 -5 -16 -4 -13 5 16 15-22 -2 -23 -1100 VIEScore 62 1 9 9 -6 13 6 28 33 9 -2111 32 1 -1324100
0 0
(c)Nat (d)RealSEGsonly
Figure13: CorrelationbetweentheK–SSeparationscoreforeachprompttreeforeachmetric,forall
SEGs(a),forthesyntheticerrorSEGs(b),forthenaturalimage/syntheticerrorSEGs(c)andforthe
realerrorsubset(d).
24
erocSNGILA
erocSNGILA
1PILB-GSD
1PILB-GSD
1PILB-AFIT
1PILB-AFIT
erocSPILC
erocSPILC
uyuF-GSD
uyuF-GSD
uyuF-AFIT
uyuF-AFIT
PILBtcurtsnI-GSD
PILBtcurtsnI-GSD
PILBtcurtsnI-AFIT
PILBtcurtsnI-AFIT
)tla(
AVaLL-GSD
)tla(
AVaLL-GSD
)tla(
AVaLL-AFIT
)tla(
AVaLL-AFIT
AVaLL-GSD
AVaLL-GSD
AVaLL-AFIT
AVaLL-AFIT
CE
erocSMLL
CE
erocSMLL
revO
erocSMLL
revO
erocSMLL
gulPM-GSD
gulPM-GSD
gulPM-AFIT
gulPM-AFIT
erocSEIV
erocSEIV
erocSNGILA
erocSNGILA
1PILB-GSD
1PILB-GSD
1PILB-AFIT
1PILB-AFIT
erocSPILC
erocSPILC
uyuF-GSD
uyuF-GSD
uyuF-AFIT
uyuF-AFIT
PILBtcurtsnI-GSD
PILBtcurtsnI-GSD
PILBtcurtsnI-AFIT
PILBtcurtsnI-AFIT
)tla(
AVaLL-GSD
)tla(
AVaLL-GSD
)tla(
AVaLL-AFIT
)tla(
AVaLL-AFIT
AVaLL-GSD
AVaLL-GSD
AVaLL-AFIT
AVaLL-AFIT
CE
erocSMLL
CE
erocSMLL
revO
erocSMLL
revO
erocSMLL
gulPM-GSD
gulPM-GSD
gulPM-AFIT
gulPM-AFIT
erocSEIV
erocSEIV