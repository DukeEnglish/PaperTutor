LastUpdate: April8,2024
Nonparametric Modern Hopfield Models
JerryYao-ChiehHu†∗1,Bo-YuChen‡∗2,DennisWu†3,FengRuan§4,HanLiu†§5
† DepartmentofComputerScience,NorthwesternUniversity,Evanston,IL60208,USA
‡ DepartmentofPhysics,NationalTaiwanUniversity,Taipei10617,Taiwan
§ DepartmentofStatisticsandDataScience,NorthwesternUniversity,Evanston,IL60208,USA
We present a nonparametric construction for deep learning compatible modern Hopfield models
and utilize this framework to debut an efficient variant. Our key contribution stems from inter-
pretingthememorystorageandretrievalprocessesinmodernHopfieldmodelsasanonparametric
regressionproblemsubjecttoasetofquery-memorypairs. Crucially,ourframeworknotonlyre-
coverstheknownresultsfromtheoriginaldensemodernHopfieldmodelbutalsofillsthevoidin
theliteratureregardingefficientmodernHopfieldmodels,byintroducingsparse-structured mod-
ern Hopfield models with sub-quadratic complexity. We establish that this sparse model inherits
the appealing theoretical properties of its dense analogue — connection with transformer atten-
tion, fixed point convergence and exponential memory capacity — even without knowing details
of the Hopfield energy function. Additionally, we showcase the versatility of our framework by
constructingafamilyofmodernHopfieldmodelsasextensions,includinglinear,randommasked,
top-K andpositiverandomfeaturemodernHopfieldmodels. Empirically,wevalidatetheefficacy
ofourframeworkinbothsyntheticandrealisticsettings.
1jhu@u.northwestern.edu
2b12202023@ntu.edu.tw
3hibb@u.northwestern.edu
4fengruan@northwestern.edu
5hanliu@northwestern.edu
*Theseauthorscontributedequallytothiswork. CodeisavailableatGitHub.
4202
rpA
5
]LM.tats[
1v00930.4042:viXraContents
1 Introduction 2
2 Background: ModernHopfieldModels 5
2.1 Soft-MarginSupportVectorRegression . . . . . . . . . . . . . . . . . . . . . . 5
2.2 ModernHopfieldModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3 NonparametricModernHopfieldModels 8
3.1 RetrievalDynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.2 NonparametricDenseandSparse-StructuredModernHopfieldModels . . . . . . 12
4 TheoreticalAnalysisofSparse-StructuredModernHopfieldModels 16
4.1 MemoryRetrieval: ErrorBounds&Convergence . . . . . . . . . . . . . . . . . 16
4.2 MemoryCapacity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
5 ConclusionandDiscussion 20
A TableofNotations 24
B ProofsofMainText 25
B.1 Theorem3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B.2 Lemma3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B.3 Theorem3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.4 Theorem4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.5 Corollary4.1.1andCorollary4.1.2 . . . . . . . . . . . . . . . . . . . . . . . . . 31
B.6 Lemma4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
B.7 Theorem4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
B.8 Lemma4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
C NonparametricModernHopfieldFamily 36
C.1 LinearModernHopfieldModel . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
C.2 Multi-HeadModernHopfieldModels . . . . . . . . . . . . . . . . . . . . . . . 36
C.3 PRFs(PositiveRandomFeatures)KernelModernHopfieldModel . . . . . . . . 37
D NonparametricModernHopfieldLayersforDeepLearning 39
E ExperimentalStudies 40
E.1 MemoryRetrievalTask(Figure3) . . . . . . . . . . . . . . . . . . . . . . . . . 41
E.2 MultipleInstanceLearningonMNIST(Figure4&Figure5) . . . . . . . . . . . 43
E.3 MultipleInstanceLearningonRealWorldDatasets . . . . . . . . . . . . . . . . 46
E.4 TimeSeriesPrediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
E.5 ComputationalEfficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
11 Introduction
We tackle the challenges in computational efficiency of modern Hopfield models [Wu et al.,
2024b, Hu et al., 2023, Ramsauer et al., 2020] by presenting a nonparametric framework, and
then debuting the first (to our knowledge) efficient modern Hopfield model with sub-quadratic
complexity and appealing theoretical properties. Such a construction is of practical importance.
As in many Hopfield-centric methods [Hu et al., 2024a, Wu et al., 2024a, Xu et al., 2024, Wu
et al., 2024b, Schimunek et al., 2023, Fürst et al., 2022, Paischer et al., 2022, Seidl et al., 2022,
Widrich et al., 2020], modern Hopfield models (and their derived deep learning layers) serve as
powerfulalternativestotheattentionmechanismwithadditionalfunctionalities,butlackefficient
implementationforgiganticdeepmodels[Huetal.,2023,SectionC.2]. Thisissuebecomesmore
prominent in this era of Large Foundation Models [Bommasani et al., 2021]. Foundation models
huge attention-based models pretrained on massive datasets, and play a central role not only in
machine learning but also in a wide range of scientific domains, such as ChatGPT [Brown et al.,
2020, Floridi and Chiriatti, 2020] for natural language, BloombergGPT [Wu et al., 2023] for fi-
nance, DNABERT [Zhou et al., 2024a,b, Ji et al., 2021] for genomics, and many others. To push
toward Hopfield-based large foundation models, this work provides a timely efficient solution,
back-bonedbyasolidtheoreticalground.
Modern Hopfield models [Ramsauer et al., 2020], motivated by the dense associative memory
models[Demircigiletal.,2017,KrotovandHopfield,2016],are(auto-)associativememorymod-
els that (i) have exponential memory capacity, (ii) retrieve stored patterns based on input queries
with only one retrieval step, and (iii) are compatible with deep learning architectures. They
achieve (i) by adopting highly non-linear energy functions, (ii) by adopting a memory-retrieval
dynamics ensuring monotonic minimization of the energy function, and (iii) by the connection
between their memory retrieval dynamics and attention mechanism. Deepening (ii) and (iii), Hu
etal.[2023]andWuetal.[2024b]proposeatheoreticalframeworkforderivingmodernHopfield
models using various entropic regularizers. In addition, they introduce a sparse extension of the
original modern Hopfield model to handle its computational burden and vulnerability to noise.
As a result, their proposal not only connects to sparse attention mechanism [Correia et al., 2019,
Martins and Astudillo, 2016] but also offers both provably computational advantages and robust
empiricalperformance.
However, there are still some missing pieces toward a unified theoretical framework for modern
Hopfieldmodels:
• (P1) Lack of Efficiency. Computationally, while Hu et al. [2023] and Wu et al. [2024b]
indeed introduce sparsity into their model, this sparsity does not implies computational
efficiency. In fact, it only increases efficiency at the level of memory retrieval, (i.e. the
sparsity in [Hu et al., 2023, Wu et al., 2024b] only leads to faster memory retrieval but not
2necessarily shorter running time, as discussed in [Hu et al., 2023, Section C.2]). Namely,
the sparse modern Hopfield model still suffers by the O(n2) complexity (with the input
sequencelengthn),whichhampersitsscalability1.
• (P2) Lack of Rigorous Analysis on Sparsity. Theoretically, because Hu et al. [2023]
choose not to make strong assumptions (on the memory and query patterns) in order to
maintain their model’s generality, they only offer qualitative justifications [Hu et al., 2023,
Section 3]. They do not rigorously characterize how sparsity impacts different aspects of
the sparse model, e.g., the retrieval error, the well-separation condition, and the memory
capacity.
• (P3) Incomplete Connection between Attention and Hopfield Models. Methodologi-
cally, while numerous variants of the attention module exist [Choromanski et al., 2021,
Katharopoulos et al., 2020, Beltagy et al., 2020, Child et al., 2019], Hu et al. [2023] only
bridge a subset of them to modern Hopfield models. A natural question arises: How can
we integrate the advancements of state-of-the-art attention into modern Hopfield models?
As noted in [Hu et al., 2024b, 2023, Wu et al., 2024b], this question is far from trivial.
Naively substituting the softmax activation function with other alternatives does not neces-
sarilyyieldwell-definedHopfieldmodelsandmightsabotagetheirdesirablepropertiesand
functionalities.
To fill these gaps, this work presents a nonparametric framework for deep learning compatible
modern Hopfield models. To fill (P1), this framework allows us to not only recover the standard
dense modern Hopfield model [Ramsauer et al., 2020], but also introduce an efficient modern
Hopfieldmodel,termedsparse-structuredmodernHopfieldmodel(Theorem3.2). Tofill(P2),our
frameworkfacilitatesthederivationofaretrievalerrorboundofthesparsemodernHopfieldwith
explicit sparsity dependence (Theorem 4.1). This bound offers rigorous characterizations of the
sparsity-induced advantages of the sparse model compared with its dense counterpart, including
higher precision in memory retrieval (Corollary 4.1.1 and Corollary 4.1.2), enhanced robustness
to noise (Remark 4.2) and exponential-in-d capacity (Theorem 4.2 and Lemma 4.2, d refers to
pattern size). Interestingly, unlike existing Hopfield models [Hu et al., 2023, Wu et al., 2023,
Ramsaueretal.,2020]requiringanexplicitenergyfunctiontoguaranteethestabilityofthemodel,
we show that the sparse modern Hopfield model guarantees the fixed-point convergence even
withoutdetailsoftheHopfieldenergyfunction(Lemma4.1). Tofill(P3),beyondintroducingthe
sparsemodernHopfieldmodel,ourframeworksupportsafamilyofmodernHopfieldmodelsthat
connect with various attention variants. This complements the findings in [Hu et al., 2023, Wu
etal.,2024b],pushingustowardamoreunifiedunderstanding.
Contributions. Ourcontributionsareasfollows:
1SeeRemark3.7fortheconnectionbetweentimecomplexityofattentionandofmodernHopfieldmodels.
3• We propose a nonparametric framework for deep learning compatible modern Hopfield
models. Building upon this, we introduce the first efficient sparse modern Hopfield model
withsub-quadraticcomplexity.
• We provide rigorous characterizations of the sparsity-induced advantages of the proposed
efficientmodel: tighterretrievalerrorbound(Corollary4.1.1andCorollary4.1.2),stronger
noiserobustness(Remark4.2)andexponential-d-capacity(Theorem4.2andLemma4.2).
• Based on the proposed framework, we construct a family of modern Hopfield models con-
necting to many existing attention variants [Choromanski et al., 2021, Zaheer et al., 2020,
Beltagyetal.,2020,Katharopoulosetal.,2020],andverifytheirefficacythroughthorough
numericalexperimentsinbothsyntheticandrealisticsettings.
Related Works
Modern Hopfield Models for Deep Learning. The classical Hopfield models [Hopfield, 1984,
1982, Krotov and Hopfield, 2016] are canonical models of the human brain’s associative mem-
ory. Their primary function is the storage and retrieval of specific memory patterns. Recently,
a resurgence of interest in Hopfield models within the machine learning field is attributed to de-
velopments in understanding memory storage capacities [Krotov and Hopfield, 2016, Demircigil
et al., 2017, Wu et al., 2024a], innovative architecture [Hoover et al., 2023, Seidl et al., 2022,
Fürst et al., 2022, Ramsauer et al., 2020], and their biological plausibility [Kozachkov et al.,
2022, Krotov and Hopfield, 2021]. Notably, the modern Hopfield models [Wu et al., 2024b, Hu
et al., 2023, Ramsauer et al., 2020, Brandstetter, 2021], demonstrate not only a strong connec-
tion to the transformer attention mechanisms in deep learning, but also superior performance,
and a theoretically guaranteed exponential memory capacity. In this regard, seeing the modern
Hopfield models as an advanced extension of attention mechanisms opens up prospects for craft-
ing Hopfield-centric architectural designs. Therefore, their applicability spans diverse areas like
drugdiscovery[Schimuneketal.,2023],immunology[Widrichetal.,2020],tabularlearning[Xu
et al., 2024], time series forecasting [Wu et al., 2024b, Auer et al., 2024], reinforcement learning
[Paischer et al., 2022], and large foundation models [Hu et al., 2024a, Fürst et al., 2022]. This
workemphasizesrefiningthislineofresearchtowardsefficientmodels. Wepositthatthiseffortis
crucialinguidingfutureresearchtowardsHopfield-drivendesignparadigms,especiallyforlarger
models.
Sparse Modern Hopfield Model. [Ramsauer et al., 2020] establish a connection between Hop-
field models and the vanilla softmax attention. Motivated by this connection, [Hu et al., 2023,
Wu et al., 2024b] (and later [Martins et al., 2023]) propose a theoretical framework for modern
Hopfieldmodelsbasedontherelationshipbetweenentropicregularizersandfinite-domaindistri-
butions with varying support sets. Importantly, they not only show that [Ramsauer et al., 2020] is
4just special case within their framework but also propose a sparse extension with superior prop-
erties (e.g., robust representation learning, fast fixed-point convergence, and exponential memory
capacity)andconnectiontocertaintypesofsparseattention. However,thisisnotendofthestory.
As highlighted in [Hu et al., 2023, Section E], their framework only bridges a subset of exist-
ing attention variants (with dense quadratic attention score matrix) and hence is not complete.
This work fills this theoretical gap by providing a principle construction for the many modern
Hopfieldmodelswiththeoreticalguarantees. Moreover,ourframeworksupportsafamilyofmod-
ernHopfieldmodelsmirroringmanypopularstructuredefficientattentionmechanisms,including
Attention with Pre-defined Patterns (each sequence token attends to a predetermined subset of
tokens instead of the entire sequence, e.g, Big Bird [Zaheer et al., 2020], Longformer [Beltagy
et al., 2020], Blockwise [Qiu et al., 2019], Sparse [Child et al., 2019]), and Kernelized Attention
(e.g.,Performer[Choromanskietal.,2021],Linear[Clevertetal.,2015]andMulti-head[Vaswani
etal.,2017]).
Notations. Wedenotevectorsbylowercaseboldletters,andmatricesbyuppercaseboldletters.
We write ⟨a,b⟩ := aTb as the inner product for vectors a,b. Let a[i] denotes the i-th element
of vector a. The index set {1,··· ,I} is denoted by [I], where I ∈ N . The spectral norm
+
is denoted by ∥·∥, which is equivalent to the l -norm when applied to a vector. We denote the
2
memory patterns by ξ ∈ Rd and the query pattern by x ∈ Rd, and Ξ := [ξ ,··· ,ξ ] ∈ Rd×M as
1 M
shorthand for stored memory patterns {ξ } . Moreover, we set m := Max ∥ξ ∥ be the
µ µ∈[M] µ∈[M] µ
largestnormofmemorypatterns.
Organization. Section 2 reviews modern Hopfield models. Section 3 presents a nonparametric
constructionformodernHopfieldmodels,anddebutthesparse-structured(efficient)modernHop-
fieldmodels. Section4providesthetheoreticalanalysisonthesparse-structuredmodernHopfield
models. Appendix C includes a family of modern Hopfield models as possible extensions. We
conductnumericalexperimentstosupportourframeworkinAppendixE.
2 Background: Modern Hopfield Models
Thissectionpresentstheideaswebuildon.
2.1 Soft-Margin Support Vector Regression
Soft-margin Support Vector Regression (SVR) [Awad et al., 2015, Jaggi, 2014] aims to fit the
best hyperplane (in the higher dimensional feature space) to the data, within a certain margin of
errortoleranceϵ′,whileallowingcontrollableflexibility(softened margin)fordatapointsthatare
outside this margin. Given a feature map Φ : Rd → RDΦ, a training dataset {(x ,y )}M , where
µ µ µ=1
5x ∈ Rd is a feature vector and y ∈ Rd is the target output, the objective of SVR is to find a
µ µ
functionf(x) = WΦ(x)+bthathasatmostϵ′-deviationfromtheactualtargetsy forallthedata
µ
points, with W := [w ,...,w ]T ∈ Rd×DΦ being the weight matrix, and b ∈ Rd the bias term.
1 d
Thesoft-marginSVRformulationintroducesslackvariablesη ,η ≥ 0tohandleconstraintsthat
µ (cid:101)µ
might otherwise be infeasible due to data noise. The soft-margin SVR with squared loss (ℓ -loss)
2
isformulatedas:
M
1 (cid:88)
min ∥W∥2 +C ⟨1,(η +η )⟩ (2.1)
µ (cid:101)µ
W,η,η 2
(cid:101) µ=1

 y −⟨W,Φ(x )⟩−b ≤ ϵ′1+η,
 µ µ

subjectto ⟨W,Φ(x )⟩+b−y ≤ ϵ′1+η,
µ µ (cid:101)


 η ,η ≥ 0, µ ∈ [M],
µ (cid:101)µ
where C > 0 is a penalty coefficient that balances the trade-off between the accuracy of f(x)
andtheextenttowhichdeviationsgreaterthanϵ′ arepermitted. As(2.1)isstronglyconvex,there
existsauniqueminimizer.
SolvingthisoptimizationprobleminvolvesconstructingaLagrangianwithdualvariablesforeach
constraint,
d M d
1 (cid:88) (cid:88)(cid:88)
L := ∥w i∥2 +C (λ µ[i]η µ[i]+λ(cid:101) µ[i]η (cid:101)µ[i])
2
i=1 µ=1 i=1
M d
(cid:88)(cid:88)
− α [i](ϵ′ +η [i]−y [i]+⟨w ,Φ(x )⟩+b[i])
µ µ µ i µ
µ=1 i=1
M d
(cid:88)(cid:88)
− α [i](ϵ′ +η [i]−⟨w ,Φ(x )⟩−b[i]+y [i]),
(cid:101)µ (cid:101)µ i µ µ
µ=1 i=1
where a[i] denotes the i-th element of a vector a, λ µ, λ(cid:101) µ, α
µ
and α
(cid:101)µ
are Lagrange multipliers;
andthenuseKarush-Kuhn-Tucker(KKT)conditionstofindtheoptimalsolution.
2.2 Modern Hopfield Models
Letx ∈ Rd betheinputquerypatternandΞ = [ξ ,··· ,ξ ] ∈ Rd×M theM memorypatterns.
1 M
Hopfield Models. The aim of Hopfield models [Hopfield, 1982, 1984] is to store these memory
patterns Ξ and retrieve a specific memory ξ when given a query x. They achieve these by
µ
embeddingthememoriesintheenergylandscapeE(x)ofaphysicalsystem,whereeachmemory
6ξ corresponds to a local minimum. When a query x is presented, the model initiates energy-
µ
minimizing retrieval dynamics T at the query, which then navigate the energy landscape to find
thenearestlocalminimum,effectivelyretrievingthememorymostsimilartothequery.
These models comprise two primary components: an energy function E(x) that encodes mem-
ories into its local minima, and a retrieval dynamics T (x) that fetches a memory by iteratively
minimizingE(x)startingwithaquery.
Constructing the energy function, E(x), is straightforward. As outlined in [Krotov and Hopfield,
2016],memoriesgetencodedintoE(x)usingtheoverlap-construction: E(x) = F(ΞTx),where
F : RM → Risasmoothfunction. Thisensuresthatthememories{ξ } sitatthestationary
µ µ∈[M]
points of E(x), i.e. ∇ F(ΞTx)| = 0 for all µ ∈ [M]. The choice of F results in different
x ξµ
Hopfield model types, as demonstrated in [Krotov and Hopfield, 2016, Demircigil et al., 2017,
Ramsauer et al., 2020, Krotov and Hopfield, 2021]. However, determining a suitable retrieval
dynamics, T , for a given energy E(x) is more challenging. For effective memory retrieval, T
must:
(T1) MonotonicallyreduceE(x)whenappliediteratively.
(T2) EnsureitsfixedpointscoincidewiththestationarypointsofE(x)forpreciseretrieval.
Modern Hopfield Models. Ramsauer et al. [2020] propose the modern Hopfield model with a
specific set of E and T satisfying above requirements, and integrate it into deep learning archi-
tectures via its strong connection with attention mechanism, offering enhanced performance, and
theoretically guaranteed exponential memory capacity. Specifically, they introduce the energy
function:
1
E(x) = −lse(β,ΞTx)+ ⟨x,x⟩, (2.2)
2
wheretheretrievaldynamicsisgivenby
xnew = T (x) = Ξ·Softmax(βΞTx). (2.3)
Dense
(cid:16) (cid:17)
The function lse(β,z) := log
(cid:80)M
exp{βz } /β is the log-sum-exponential for any given
µ=1 µ
vectorz ∈ RM andβ > 0. Theiranalysisrevealsthat:
(i) TheT dynamicsconvergewell(T2)andcanretrievepatternsaccuratelyinjustonestep
Dense
(T1).
(ii) ThemodernHopfieldmodelfrom(2.2)possessesanexponentialmemorycapacityinpattern
sized.
7(iii) Notably,theone-stepapproximationofT mirrorstheattentionmechanismintransform-
Dense
ers,leadingtoanoveldeeplearningarchitecturedesign: theHopfieldlayers.
Attention ↔ Modern Hopfield Model. To see above (iii), suppose that X and Ξ are embedded
from the raw query R and Y memory patterns, respectively, via XT = RW := Q, and ΞT =
Q
YW := K, with some projection matrices W and W . Then, taking the transpose of T in
K Q K
(2.3)andmultiplyingwithW suchthatV := KW ,weobtain
V V
(cid:0) (cid:1)
Z := QnewW = Softmax βQKT V. (2.4)
V
This enables modern Hopfield models to serve as alternatives to attention mechanism with extra
functionalities.
Given the equivalence (2.4), one might wonder if the quest for efficient modern Hopfield models
is equivalent to seeking efficient attention mechanisms [Tay et al., 2022], specifically in terms of
finding efficient implementations of the Softmax matrix computation. We contend that they are
not the same. To build a modern Hopfield model, we expect not only its retrieval dynamics to
connect to attention mechanism, but also it to serve as an associative memory model [Hu et al.,
2024a,Wuetal.,2024b,Huetal.,2023,Ramsaueretal.,2020]bydesign. Moreover,weobserve
that(T1)and(T2)areessentiallyaboutencodingmemoriesontothefixedpointsofT .
ThesemotivateustoviewtheconstructionofT asalearningproblem: weaimtolearnafunction
T satisfying (T2) from a dataset consisting of query-memory pairs. Thus, rather than using the
traditionalHopfieldmodel’slearningrule—wherethemodelmemorizesmemoriesbydefiningan
energy function, like the overlap-construction [Hu et al., 2023] — we interpret the memorization
process as learning a function that maps queries to memories. This new perspective allows us to
constructnovelmodernHopfieldmodelsthatareequivalenttovariousattentionvariants.
3 Nonparametric Modern Hopfield Models
Figure1: AHigh-levelVisualization
OverviewofOurFramework.
8• In Section 3.1, we formulate the memory storage and retrieval of modern Hopfield models
as a nonparametric regression problem. We first align the definition of T (the retrieval
dynamics(2.3))withanonparametricregressionproblemsubjecttoasetofquery-memory
pairs. Then, by solving for optimality, we derive a nonparametric formulation of T . We
provideahigh-levelvisualizationofourframeworkinFigure1.
• In Section 3.2, we showcase our framework with two special cases: the standard dense
modern Hopfield model [Ramsauer et al., 2020] (Lemma 3.1), and a new, efficient sparse-
structuredmodernHopfieldmodel(Theorem3.2).
3.1 Retrieval Dynamics
The retrieval dynamics (2.3) T (x) : Rd → Rd maps an input query x to T (x), with the aim of
Ξ Ξ
retrievingthememorypatternξ closesttox. Toformalizethisnotionofretrieval,weneedafew
µ
definitionsandnotation.
Definition 3.1 (Generalized Fixed Point [Sriperumbudur and Lanckriet, 2009]). We say a set
S ⊆ Rd ageneralizedfixedpoint withrespecttoT ifT (y) ∈ S foreveryy ∈ S.
Ξ Ξ
Remark3.1(FixedPoint). IncontrasttoDefinition3.1,afixedpointofT isapointy forwhich
Ξ
T (y) = y.
Ξ
Remark 3.2. A generalized fixed point S with respect to T is also an invariant set with respect
Ξ
toT .
Ξ
In particular, if the retrieval dynamics is initiated at x ∈ S where S is an invariant set, then
subsequentiteratessuchasT (x),T ◦T (x),...remainintheinvariantsetS.
Ξ Ξ Ξ
Nowweintroduceaneighborhood—S ,aballofradiusR —ateverymemorypatternξ :
µ µ
S = {ξ | ∥ξ −ξ ∥ ≤ R},
µ µ
where
1
R := min ∥ξ −ξ ∥.
µ ν
2 µ,ν∈[M];µ̸=ν
Bydefinition,neighborhoodsassociatedwithdistinctmemorypatternsdonotoverlap: S ∩S =
µ ν
∅ for µ ̸= ν. To measure the progress of the dynamics in retrieving the memory pattern, we
introducethenotionofmemorystorageandϵ-retrieval.
Definition3.2(Storageandϵ-Retrieval). Amemorypatternξ isstoredifS isageneralizedfixed
µ µ
pointofT . Amemorypatternξ getsϵ-retrieved byT withaninputqueryxif∥T (x)−ξ ∥ ≤
µ Ξ Ξ µ
ϵ.
9In below, when the context is clear, we suppress the notation dependence of T on the memory
Ξ
patternsΞforsimplicity.
Definition 3.2 states that for an input x around a stored memory pattern ξ, its corresponding
mapping output T (x) should be located in the same sphere S. This motivates us to view T as a
functionaimingtomapthequeryxontoitsnearestmemoryξwithinanerror-tolerancemarginR.
More precisely, in this work, we construct such a function satisfying Definition 3.2 as a learning
problem, using memory patterns as data. A natural choice for doing this function is through the
soft-marginSVR(seeSection2.1): itfitsthebesthyperplanetothedatapointswithinapredefined
error margin, aiming to minimize the error rate while ensuring the model remains insensitive to
errorswithinacertainthreshold.
We first define the regression model. Given a weight matrix W ∈ Rd×DΦ, and a feature map
Φ : Rd → RDΦ,denotef : Rd → Rd tobethemapping
W,Φ
f (x) = WΦ(x). (3.1)
W,Φ
Denote K(x ,x ) := ⟨Φ(x ),Φ(x )⟩. This is a positive semidefinite kernel, and there is a unique
1 2 1 2
RKHSH thatisassociatedwiththiskernelK [Wainwright,2019,Theorem12.11].
To cast T as a SVR problem using (3.1), we now specify the data points that f(x) should fit.
Since the goal of T is to retrieve the memory pattern most similar to given query x, we consider
thetrainingdatasetD = {(ξ +δξ ,ξ )} . Namely,theinputqueryx = ξ +δξ isthecon-
µ µ µ µ∈[M] µ µ
taminated target memory pattern with noise δξ , and the output y = ξ is target memory pattern.
µ µ
For convenience, we shorthand [ξ +δξ ,··· ,ξ +δξ ] = Ξ ∈ Rd×M as the contaminated
1 1 M M δ
memorypatterns.
Next, we frame the memorization in modern Hopfield models as fitting f to the dataset D, and
obtainthefollowingnonparametric(supportvector)regressionproblem.
Given a dataset D = {(ξ + δξ ,ξ )} , consider the support vector regression using the
µ µ µ µ∈[M]
featuremapΦ
M
1 (cid:88)
Min ∥W∥2 +C ⟨1,(η +η )⟩ (3.2)
µ (cid:101)µ
W,η,η 2
(cid:101) µ=1
10subjectto



−(ϵ′1+η (cid:101)µ) ≤ ξ
µ
−⟨W,Φ(ξ
µ
+δξ µ)⟩ ≤ ϵ′1+η
µ
 √
ϵ′1+η ≤ ϵ1/ d
µ


 η ≥ 0,η ≥ 0,∀µ ∈ [M],
µ (cid:101)µ
where ϵ′ > 0 is a component-wise error margin, C ≥ 0 is a penalty coefficient, and ϵ > 0 is
the memory retrieval error. We denote the unique (given the strong convexity of the optimization
problem (3.2)) minimizer as (W∗,η∗,η∗), and the solution to (3.2) as T (x). By solving the
Φ Φ (cid:101)Φ SVR
optimalityviatheLagrangianduality,weobtainthefollowing.
Theorem 3.1. Let α,α denote the Lagrangian multipliers of the dual problem of (3.2). Let
(cid:101)
W⋆ := (w⋆,...w⋆)T ∈ Rd×DΦ denotetheminimizerof(3.2). Then,
1 d
M
(cid:88)
w⋆ = (α [i]−α [i])Φ(ξ +δξ ), (3.3)
i µ (cid:101)µ µ µ
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
µ=1
∈R ∈RDΦ
wherea[i]denotesthei-thelementofavectora.
Proof. SeeAppendixB.1foradetailedproof.
ForanyfeaturizationmapΦ,Theorem3.1introducesamap
T := f .
SVR,Φ W∗,Φ
Φ
Byconstruction,foranyΦ,T obeystheϵ-retrievalproperty∥T (x)−ξ ∥ ≤ ϵ,foranyµ
SVR,Φ SVR,Φ µ
andx ∈ S . Hence,wearriveanonparametricframeworkforconstructingmanymodernHopfield
µ
models. Given an input query x, the i-th component of the retrieved pattern by applying T (x)
SVR
onceis
xnew[i] := T (x)[i] = ⟨w⋆,Φ(x)⟩. (3.4)
SVR i
Remark 3.3. Note that ϵ′ is the component-wise SVR error, not the ϵ in Hopfield retrieval error
definedinDefinition3.2.
Remark 3.4. Without any assumption on ϵ, T converges to generalized fixed points, in con-
SVR
trast to the fixed point convergence in [Hu et al., 2023, Ramsauer et al., 2020]. Thus, there is
no multiple update convergence for T without specifying Φ (and thereby proving the fixed
SVR
point convergence property.) We provide specific Φ with provably fixed point convergence in
Section3.2andRemark4.3.
11Remark 3.5. This regression problem is nonparametric. That is, it does not assume a specific
functional form for T and is flexible in the number of parameters, allowing the number of
SVR
supportvectorstoadjustbasedonthedata.
Intuitively, this optimization problem learns a T to replace T from the training dataset D =
SVR,Φ
{(ξ + δξ ,ξ )} . Thus, for any given query ξ + δξ , T retrieves a target memory
µ µ µ µ∈[M] µ µ SVR,Φ
pattern ξ with ϵ precision, for all µ ∈ [M]. Specifically, this ϵ precision comes from the upper
µ √
bound of the maximum component-wise error ϵ′ + η [i] (and ϵ′ + η [i]) ≤ ϵ/ d, defined in
µ (cid:101)µ
(3.2). This choice of SVR error margin mimics the ϵ-retrieval of modern Hopfield models via the
flexibilityofsoft-marginSVR.Asaresult,theobjectiveoftheSVRproblem(3.2)coincideswith
thememorizationandretrievalprocessesofmodernHopfieldmodels. WhileT retrievesmemory
patterns{ξ } basedonxwithanerrortoleranceϵ,theSVRproblem(3.2)
µ µ∈[M]
• (Memorization:) FitsafunctionT satisfyingDefinition3.2,which
SVR
• (Retrieval:) Maps queries onto memory patterns within a component-wise error-margin
√
ϵ/ d.
Importantly,Theorem3.1enablesustoderiveafamilyofnonparametricmodernHopfieldmodels
throughconstructingtheirretrievaldynamicswithvariouskernelfunctionsΦ(·),includingDense
[Ramsauer et al., 2020], Linear [Katharopoulos et al., 2020], Multi-Head [Vaswani et al., 2017],
Sparse-Structured [Zaheer et al., 2020, Beltagy et al., 2020, Child et al., 2019] and Generalized
Kernelizable or PRFs (Positive Random Features) [Choromanski et al., 2021] modern Hopfield
models.
In Appendix C, we present constructions of these modern Hopfield models as extensions of our
framework.
3.2 Nonparametric Dense and Sparse-Structured Modern Hopfield Models
In this section, we showcase the nonparametric framework Theorem 3.1 with two special cases.
First, we recover the standard dense modern Hopfield model [Ramsauer et al., 2020]. Then, we
introducetheefficientsparse-structuredmodernHopfieldmodelswithsub-quadraticcomplexity.
DenseModernHopfieldModel[Ramsaueretal.,2020].
Lemma 3.1 (Nonparametric Dense Modern Hopfield Model). Let Φ(·) =
12(ϕ(0),ϕ(1),...,ϕ(1),...,ϕ(n),...,ϕ(n),...)with,for1 ≤ D′ ≤ D ,
0 1 D1 1 Dn n
√ √
( βx )ℓ1···( βx )ℓ d
ϕ(n) := 1 d √ , (3.5)
D′ (cid:80)M
⟨Φ(ξ +δξ ),Φ(x)⟩· ℓ !···ℓ !
µ=1 µ µ 1 d
where ℓ +···+ℓ = n, and D :=
(cid:0)d+n−1(cid:1)
. By Theorem 3.1, fitting T on D following (3.2)
1 d n n SVR
gives
T (x) = ΞSoftmax(cid:0) βΞTx(cid:1) ∈ Rd, (3.6)
Dense δ
whereΞ := [ξ +δξ ,··· ,ξ +δξ ] ∈ Rd×M denotesthecontaminatedmemorypatterns.
δ 1 1 M M
ProofSketch. We first select Φ to be the Taylor expansion of the exp function via the homoge-
neous infinite polynomial kernel [Chen et al., 2005]. By solving the optimization problem (3.2),
wearrivearetrievaldynamicsresembling(2.3). SeeAppendixB.2foradetailedproof.
Remark 3.6 (Hetero- v.s. Auto-Associative Memory.). So far, we derive a nonparametric frame-
work for hetero-associative modern Hopfield models, differentiating x and y by incorporating
inherent noise δξ into D. If we eliminate noises {δξ } from the training memory patterns,
µ µ∈[M]
(3.6) reduces to that of the standard auto-associative dense modern Hopfield model, as shown in
(2.3).
With Remark 3.6, Lemma 3.1 facilitates the replication of known results from the standard dense
modern Hopfield model [Ramsauer et al., 2020]. The recovery of dense modern Hopfield model
providesasanitycheckforournonparametricframework.
Sparse-Structured Modern Hopfield Models. Next, we present a set of efficient modern Hop-
fieldmodelswithsparse-structuredpatternsviathefollowingmask.
Definition 3.3 (Sparse-Structured Mask). Let M := {M(1),...,M(k)} ⊆ {1,...,M} be the
reducedsupportsetforT ofsizek ≤ M. Then,forµ ∈ [M],theoptimizationproblemin(3.2)
SVR
reducesto
1 (cid:88)
Min ∥W∥2 +C ⟨1,(η +η )⟩ (3.7)
µ (cid:101)µ
W,η,η 2
(cid:101)
µ∈M
13subjectto



−(ϵ′1+η (cid:101)µ) ≤ ξ
µ
−⟨W,Φ(ξ
µ
+δξ µ)⟩ ≤ ϵ′1+η
µ
 √
ϵ′1+η ≤ ϵ1/ d
µ


 η ≥ 0,η ≥ 0,∀µ ∈ M.
µ (cid:101)µ
WithDefinition3.3,weobtainthefollowingsparse-structuredretrievaldynamics(andtherebyits
correspondingHopfieldmodel(s))byfittingT onD maskedbyM.
SVR
Theorem 3.2 (Sparse-Structured Modern Hopfield Models). Let Φ(·) =
(ϕ(0),ϕ(1),...,ϕ(1),...,ϕ(n),...,ϕ(n),...)with,for1 ≤ D′ ≤ D ,
0 1 D1 1 Dn n
√ √
( βx )ℓ1···( βx )ℓ d
ϕ(n) := 1 d √ , (3.8)
D′ (cid:80)M
⟨Φ(ξ +δξ ),Φ(x)⟩· ℓ !···ℓ !
µ=1 µ µ 1 d
where ℓ +···+ℓ = n, and D :=
(cid:0)d+n−1(cid:1)
. By Theorem 3.1, fitting T on D masked by M
1 d n n SVR
following(3.7)gives
T (x) = (cid:88) (cid:2) Softmax(cid:0) βΞTx(cid:1)(cid:3) ξ ∈ Rd, (3.9)
Sparse δ µ µ
µ∈M(cid:124) (cid:123)(cid:122) (cid:125)
∈R
whereΞ := [ξ +δξ ,··· ,ξ +δξ ] ∈ Rd×M denotesthecontaminatedmemorypatterns.
δ 1 1 M M
Proof. SeeAppendixB.3foradetailedproof.
We emphasize that (3.9) is in fact generic and is able to describe many sparse-structured modern
Hopfield models with various support set. Importantly, it allows us to construct efficient variants
withsub-quadraticcomplexity,andhencefillsthevoidintheliteratureregardingefficientmodern
Hopfieldmodels,asdiscussedin[Huetal.,2023].
We present three efficient variants based on (3.9) below. To analyze efficiency for long query
sequences2, we first generalize (3.9) from a single query x to a sequence of L query denoted by
X = [x ,...,x ]. LetthebinarymatrixI bethecorrespondingsparse-sturcturedmask.
1 L M
Random Masked Modern Hopfield Model with O(kL) Complexity. By setting M to ran-
domlymask(M −k)entries,weobtainanefficientmodernHopfieldmodelwithasub-quadratic
O(kL)complexity. ThismodelconnectstotherandomattentionofBigBird[Zaheeretal.,2020].
2Considering long query sequences is crucial, as they contribute to inefficiency (see [Hu et al., 2023, Sec-
tionC.2]).
14√
Efficient Modern Hopfield Model with O(L L) Complexity. By setting M for each query in
√
a way that I reproduces the sliding window pattern of window size L, we obtain an efficient
M √
modern Hopfield model with a sub-quadratic O(L L) complexity. This model connects to the
Longformerattention[Beltagyetal.,2020]bydesign.
Top-K ModernHopfieldModel. Letthesequence{p } betheinnerproductsofmemories
µ µ∈[M]
{ξ } andqueryx:
µ µ∈[M]
p = ⟨x,ξ ⟩ (3.10)
µ µ
and let p⋆ be the K-th largest element in {p } . Then we obtain a sparse-structured mask M
µ µ∈[M]
suchthat
(cid:40)
µ ∈ M, ifp ≥ p⋆
µ
(3.11)
µ ̸∈ M, ifp < p⋆.
µ
With(3.11),wearriveatop-K modernHopfieldmodelwithquadraticcomplexity,i.e.,inefficient.
Thismodelconnectstothetop-K attention[Guptaetal.,2021]bydesign.
Remark3.7(TimeComplexityofModernHopfieldModelsandAttentionMechanism). Thetime
complexityofmodernHopfieldmodelsandHopfieldlayersisgivenby:
• TimecomplexityofmodernHopfieldmodel: O(Md2)
• When used as cross-attention (Hopfield layer) with length-L (query) and length-M (mem-
ory)inputsequences: O(LMd2)
• Whenusedasself-attentionwithlength-Linputsequence(setM = L): O(n2d2)
Our efficient modern Hopfield models achieve high efficiency through two means: a sparse-
structured mask and various choices of the kernel Φ. The sparse-structured mask, with a support
set size of k ≤ M, reduces the complexity from O(Md2) to O(kd2). Additionally, different
choices of kernel, such as the linear kernel and positive random kernel discussed in Appendix C,
leadtoefficientimplementations.
Next, we provide analytic characterizations of how sparsity affects the sparse-structured models
definedin(3.9).
154 Theoretical Analysis of Sparse-Structured Modern Hopfield
Models
Inthissection,ourtheoreticalanalysisonsparsemodernHopfieldmodels3 consistsofthefollow-
ingtwoaspects:
1. Derive the sparsity-dependent retrieval error bound of sparse modern Hopfield model and
proveitsrapidconvergencepropertycomparedwithitsdensecounterpart.
2. Characterize the fundamental limit of memory capacity of the sparse-structured modern
Hopfieldmodels.
As a reminder, we adopt Definition 3.2 for memory storage and retrieval. Additionally, we recall
thefollowingdefinitionregardingtheseparationbetweenmemorypatterns.
Definition 4.1 (Separation of Patterns). The separation of a memory pattern ξ from all other
µ
memorypatternsΞisdefinedasitsminimalinnerproductdifferencetoanyotherpatterns: ∆ :=
µ
Min [⟨ξ ,ξ ⟩−⟨ξ ,ξ ⟩].
ν,ν̸=µ µ µ µ ν
4.1 Memory Retrieval: Error Bounds & Convergence
Memory Retrieval Error Bounds. To analyze the accuracy of memory retrieval, we derive the
upperboundonretrievalerrorofthesparse-structuredmodels.
Theorem 4.1 (Sparsity-Dependent Retrieval Error). Let T be the sparse-structured retrieval
Sparse
dynamics(3.9). Forqueryx ∈ S ,itholds
µ
∥T (x)−ξ ∥ ≤ (4.1)
Sparse µ
(cid:26) (cid:18) (cid:19)(cid:27)
m(M +k −2)exp −β ⟨ξ ,x⟩− Max ⟨ξ ,ξ ⟩ ,
µ µ ν
ν∈[M],ν̸=µ
for all µ ∈ M, where k := |M| ∈ [M] denotes the size of the support set M, and m =
Max ∥ξ ∥.
µ µ
Proof. SeeAppendixB.4foradetailedproof.
Interestingly, the retrieval error bound in Theorem 4.1 is sparsity-dependent, which is governed
bythesizeofthesupportsetM,i.e. sparsitydimensionk := |M|.
3Weuseplural“models"asMin(3.9)isagenericexpressionformanymodelswithdifferentsparsepatterns.
16Remark 4.1 (Comparing with the Sparse Modern Hopfield Model [Hu et al., 2023]). Compared
to the retrieval error bound in [Hu et al., 2023], which lacks explicit dependence on its input
(data)-dependent sparsity, the sparsity (size of M) here is pre-specified. When there are fewer
elements in the sparse-structured mask, i.e., when k is small, the retrieval error bound is tighter,
andviceversa.
Remark 4.2 (Noise Robustness). By Theorem 4.1, in cases involving contaminated query or
memory, i.e. x
(cid:101)
= x+δx (noise in query) or ξ(cid:101)= ξ +δξ (noise in memory), the impact of noise
on the sparse retrieval error (4.1) is less than that its impact on the dense counterpart due to the
smallercoefficient(M +k −2).
Corollary 4.1.1. Let T and T be the dense (3.9) and sparse-structured (3.9) retrieval
Dense Sparse
dynamics,respectively. Foranyquerypatternx ∈ S andµ ∈ M,itholds
µ
∥T (x)−ξ ∥ ≤ ∥T (x)−ξ ∥. (4.2)
Sparse µ Dense µ
Proof. SeeAppendixB.5foradetailedproof.
Computationally, Corollary 4.1.1 suggests that T necessitates fewer iterations to reach fixed
Sparse
points compared to T , given the same error tolerance level. In other words, T retrieves
Dense Sparse
storedmemorypatternsfasterthanT .
Dense
Remark 4.3 (Multiple-Update). Another important implication of Corollary 4.1.1 is that T
Sparse
exhibitssimilarmultiple-updatefunctionalitytoexistingmodels[Huetal.,2023,Wuetal.,2024b,
Ramsaueretal.,2020].
To bridge to deep learning methodologies, we show that T retrieves memory patterns with
Sparse
high accuracy after a single activation in the following corollary, akin to [Hu et al., 2023, Wu
etal.,2024b,Ramsaueretal.,2020].
Corollary 4.1.2 (One-Step Retrieval with High Accuracy). For any query x ∈ S and µ ∈ M,
µ
T retrievethememorypatternξ withretrievalerrorϵexponentiallysuppressedby∆ .
Sparse µ µ
Proof. SeeAppendixB.5foradetailedproof.
Corollary 4.1.2 indicates that, with sufficiently large ∆ , T retrieves memory patterns in a
µ Sparse
single iteration, allowing the integration of sparse-structured modern Hopfield models into deep
learningarchitecturessimilarlyto[Schimuneketal.,2023,Hooveretal.,2023,Seidletal.,2022,
Fürstetal.,2022,Paischeretal.,2022].
17Fixed Point Convergence. By design, the retrieval dynamics constructed via Lemma 3.1 satisfy
(T2). We now verify this adherence as a sanity check. Interestingly, while previous studies [Hu
et al., 2023, Wu et al., 2024b, Ramsauer et al., 2020] rely on the detailed energy functions to
showtheconvergencepropertiesofmodernHopfieldmodels,weprovethemforsparse-structured
modernHopfieldmodelsevenwithoutknowingE inthenextlemma.
Lemma 4.1 (Fixed Point Convergence). Let T be the sparse-structured retrieval dynamics
Sparse
(3.9). For all µ ∈ M, the query x ∈ S converges to a fixed point if it is iteratively applied by
µ
T .
Sparse
Proof. SeeAppendixB.6foradetailedproof.
Lemma4.1affirmsthatT in(3.9)satisfies(T2).
Sparse
4.2 Memory Capacity
Tocharacterizethefundamentallimitofmemorycapacity,weaskthefollowingtwoquestionsfor
sparse-structuredmodernHopfieldmodelsfollowing[Huetal.,2023]:
(A) What is the necessary condition for a pattern ξ being considered well-stored, and correctly
µ
retrieved?
(B) Whatistheexpectednumberofmemorypatternssuchthattheaboveconditionissatisfied?
Well-Separation Condition. To address (A), we identify the necessary condition for a pat-
tern being well-stored and retrieved by the sparse-structured modern Hopfield models: the well-
separationcondition.
Theorem 4.2 (Well-Separation Condition). Following Definition 3.2, for µ ∈ M, suppose every
(cid:8) (cid:9)
memory pattern {ξ } is enclosed by a sphere S := x | ∥x−ξ ∥ ≤ R , with finite radius
µ µ∈M µ µ
R := 1 Min ∥ξ −ξ ∥. Then,theretrieveddynamicsT mapsS toitselfif
2 µ,ν∈M;µ̸=ν µ ν Sparse µ
1. ThestartingpointxisinsideS : x ∈ S .
µ µ
2. Thewell-separationcondition:
(cid:18) (cid:19)
1 (M +k −2)m
∆ ≥ ln +2mR. (4.3)
µ
β R
Proof. SeeAppendixB.7foradetailedproof.
18Figure 2: Overview of Appendix E: Numerical Justifications for Theoretical Results.
(UpperLeft): MemoryCapacitymeasuredbysuccessfulretrievalratesfromhalf-maskedqueries
(Lemma 4.2). (Bottom Left): Memory Robustness measured by success retrieval rates from
noisy queries with different scales of Gaussian noise (Remark 4.2). For all Hopfield models, we
set β = .01/0.1 (for MNIST/CIFAR10) for better visualizations. A query pattern is considered
correctlyretrievedifitssum-of-squaresimilarityerrorisbelowasetthreshold. Forbothdatasets,
we set the error thresholds to be 20%. Plotted are the means and standard deviations of 10 runs.
WeseethatTop-K HopfieldshowssimilarexponentialcapacityasDense[Ramsaueretal.,2020]
and Sparse Hopfield [Hu et al., 2023]. Note that the Random Masked Hopfield models perform
poorly. This is because they violate the µ ∈ M assumption in Theorem 3.2, as the random mask
might inadvertently mask out the correct pattern in the memory set. (Upper Right): Training
loss and accuracy curve of different Hopfield models on MNIST multiple instance learning task
(Theorem4.1). (BottomRight): ValidationlossandaccuracycurveofdifferentHopfieldmodels
onMNISTmultipleinstancelearningtask(Theorem4.1). Wetrainallthemodelwith150epochs
with cosine annealing learning rate decay. Plotted are the means over 10 runs. We observe that
withSparseHopfieldhavingthehighestvalidationaccuracy,RandomfeatureHopfieldalsoshows
competitive performance with faster convergence speed. On the other hand, Top 20% Hopfield
also converges fast with almost no performance drop. More experimental details can be found in
AppendixE.
Intuitively,thewell-separationconditionestablishesathresholdthatensuresanypattern{ξ }
µ µ∈M
is distinguishable from all others, enabling patterns to be well-stored at a fixed point of T
Sparse
and retrieved with R precision by T . Notably, Theorem 4.2 reveals that the lower bound
Sparse
on ∆ diminishes as k decreases. Consequently, as M becomes sparser, satisfying the well-
µ
separation condition becomes easier, facilitating the storage of patterns and leading to a larger
memorycapacitylowerboundforsparse-structuredmodernHopfieldmodels.
Memory Capacity. To address (B), we derive the lower bound for the maximum number of
memorypatternsthatarewell-storedandretrievableaccordingtoTheorem4.2:
19Lemma 4.2 (Modified from [Hu et al., 2023]). Define the probability of storing and retrieving a
memorypatternas1−p. Memorycapacity,themaximumnumberofpatternsrandomlysampled
from a sphere with radius m that the sparse modern Hopfield models can store and retrieve, has
√
an lower bound: M Sparse ≥ pCd− 41 , where C is the solution for the identity C = b/ W0(exp{a+lnb})
√
with the principal branch of Lambert W function, a := (4/ d−1)(ln[m( p+k−1)/ R]+1) and b :=
4m2β/ 5(d−1).
Proof. SeeAppendixB.8foradetailedproof.
Remark 4.4. Theorem 4.2 gives a memory capacity exponential in the pattern size d (maximum
allowed value k). Since k ≤ M, the scaling behavior of sparse-structured modern Hopfield
models is similar to that of [Ramsauer et al., 2020, Hu et al., 2023]. This result mirrors findings
in[Wuetal.,2024b,Huetal.,2023,Ramsaueretal.,2020].
5 Conclusion and Discussion
We introduce a nonparametric framework for modern Hopfield models. We use two examples
to validate our framework: the original dense & the sparse-structured modern Hopfield mod-
els. With Lemma 3.1, we replicate the known results of the original modern Hopfield model
[Ramsauer et al., 2020]. With Theorem 3.2, we introduce the efficient sparse-structured Hopfield
models with robust theoretical properties: tighter retrieval error bound (Corollary 4.1.1 & Corol-
lary 4.1.2), stronger noise robustness (Remark 4.2) and exponential-in-d capacity (Theorem 4.2
&Lemma4.2).
ComparingwithExistingWorks. Ourframeworkcomplementsexistingworks[Huetal.,2023,
Wu et al., 2024b, Martins et al., 2023] by filling the efficiency gaps and connecting to various
attentions in the following. Notably, when the size of the support set k = M, the results of
Theorem 4.1, Theorem 4.2 and Lemma 4.2 reduce to those of the dense modern Hopfield model
in[Ramsaueretal.,2020].
Extensions. InAppendixC,wepresentafamilyofmodernHopfieldmodelsconnectingtomany
other existing attention mechanisms, including Linear [Katharopoulos et al., 2020], Multi-Head
[Vaswanietal.,2017],andGeneralizedKernelizableorPRFs(PositiveRandomFeatures)[Choro-
manskietal.,2021]modernHopfieldmodels.
Hopfield Layers and Numerical Experiments. In line with [Hu et al., 2023, Wu et al., 2024b,
Ramsauer et al., 2020], we introduce deep learning layers as competitive attention alternatives
with memory-enhanced functionalities, corresponding to our nonparametric modern Hopfield
20models (sparse-structured and above extensions) in Appendix D and verify them numerically in
AppendixE.
Accuracy-Efficiency Tradeoff. For learning tasks, we do not expect generally superior perfor-
mancefromefficientmodels. Ultimately,thereistheprovablyaccuracy-efficiencytradeoff[Keles
et al., 2023, Deng et al., 2023] based on complexity analysis of matrix multiplication (hence, this
result is transferable to modern Hopfield models [Hu et al., 2024b]). Therefore this work only
provides a theoretical framework supporting the derivation of efficient variants of modern Hop-
field model, with no strictly superior performance guarantee. However, we do observe that, in
manycases,linearandrandomfeaturesmodernHopfieldmodelsdeliveracceptableresults.
Limitations and Future Work. A notable limitation of this work is the absence of theoretical
analysisfortheextensionsdiscussedinAppendixC.Weleavethemforfutureworks.
21Boarder Impact
This is a theoretical work. We expect no negative social impacts. As discussed in introduction
and related works, this work aims to shed some light on the foundations of large Hopfield-based
foundationmodels.
Acknowledgments
JHwouldliketothankDinoFengandAndrewChenforenlighteningdiscussions,theRedMaple
Familyforsupport,andJiayiWangforfacilitatingexperimentaldeployments. Theauthorswould
alsoliketothanktheanonymousreviewersandprogramchairsfortheirconstructivecomments.
JH is partially supported by the Walter P. Murphy Fellowship. BY is supported by the National
Taiwan University Fu Bell Scholarship. HL is partially supported by NIH R01LM1372201, NSF
CAREER1841569,DOEDE-AC02-07CH11359,DOELAB20-2261andaNSFTRIPODS1740735.
This research was supported in part through the computational resources and staff contributions
provided for the Quest high performance computing facility at Northwestern University which is
jointly supported by the Office of the Provost, the Office for Research, and Northwestern Univer-
sity Information Technology. The content is solely the responsibility of the authors and does not
necessarilyrepresenttheofficialviewsofthefundingagencies.
22Appendix
A TableofNotations 24
B ProofsofMainText 25
B.1 Theorem3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B.2 Lemma3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B.3 Theorem3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.4 Theorem4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.5 Corollary4.1.1andCorollary4.1.2 . . . . . . . . . . . . . . . . . . . . . . . . . 31
B.6 Lemma4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
B.7 Theorem4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
B.8 Lemma4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
C NonparametricModernHopfieldFamily 36
C.1 LinearModernHopfieldModel . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
C.2 Multi-HeadModernHopfieldModels . . . . . . . . . . . . . . . . . . . . . . . 36
C.3 PRFs(PositiveRandomFeatures)KernelModernHopfieldModel . . . . . . . . 37
D NonparametricModernHopfieldLayersforDeepLearning 39
E ExperimentalStudies 40
E.1 MemoryRetrievalTask(Figure3) . . . . . . . . . . . . . . . . . . . . . . . . . 41
E.2 MultipleInstanceLearningonMNIST(Figure4&Figure5) . . . . . . . . . . . 43
E.3 MultipleInstanceLearningonRealWorldDatasets . . . . . . . . . . . . . . . . 46
E.4 TimeSeriesPrediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
E.5 ComputationalEfficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
23A Table of Notations
Table1: MathematicalNotationsandSymbols
Symbol Description
a[i] Thei-thcomponentofvectora
⟨a,b⟩ Innerproductforvectorsa,b∈Rd
[I] Indexset{1,···,I},whereI ∈N+
∥·∥ Spectralnorm,equivalenttothel -normwhenappliedtoavector
2
d Dimensionofpatterns
M Numberofstoredmemorypatterns
√
β Scalingfactoroftheenergyfunctioncontrollingthelearningdynamics.Wesetβ=1/ dinpractice
x State/configuration/querypatterninRd
x⋆ StationarypointsoftheHopfieldenergyfunction
ξ Memorypatterns(keys)inRd
δξ NoisesinmemorypatternsinRd
D Trainingdataset{(ξ +δξ ,ξ )}
µ µ µ µ∈[M]
Ξ ShorthandforM storedmemory(key)patterns{ξ } inRd×M
µ µ∈[M]
Ξ ShorthandforM contaminatedmemory(key)patterns{δξ } inRd×M
δ µ µ∈[M]
ΞTx M-dimensionaloverlapvector(⟨ξ ,x⟩,···,⟨ξ ,x⟩,···,⟨ξ ,x⟩)inRM
1 µ M
Φ(·) KernelizedfeaturemappingΦ(·):Rd→D
ϕ
ϕ ElementintheΦ(·)=(ϕ(0),ϕ(1),...,ϕ(1),...,ϕ(n),...,ϕ(n),...)
0 1 D1 1 Dn
D Dimensionofthekernelspace,i.e.,dimensionofoutputofΦ(·)
Φ
h(·) Normalizationmappingintheregressionmodeldefinedby(3.1)
W Weightedmatrixintheregressionmodeldefinedby(3.1)inRd×DΦ
w
i
i-throwoftheweightedmatrixWinRDΦ
K(·,·) KernelfunctiontakestheinnerproductformK(·,·)=⟨Φ(·),Φ(·)⟩inK:RDΦ×RDΦ →R
+
ϵ′ Component-wisetermerrormargininthesupportvectorregressionproblem
η,η Slackvariablesinthesupportvectorregression
(cid:101)
C Penalizedcoefficientofthesupportvectorregression
L Lagrangiancorrespondingto(3.2)
α,α (cid:101),λ,λ(cid:101) DualvariablesintheLagrangianL
M ReducedsupportsetforT M:={M(1),...,M(k)}⊆{1,...,M}
SVR
1 IndicatorfunctioncorrespondingtoM,where1 =1forµ∈Mand1 =0forµ̸∈M
M(µ) M(µ) M(µ)
k SizeofthesupportsetM,definedask:=|M|
m Largestnormofmemorypatterns,denotedasm:=Max ∥ξ ∥
µ∈[M] µ
R MinimalEuclideandistanceacrossallpossiblepairsofmemorypatterns,denotedasR:= 1Min ∥ξ −ξ ∥
2 µ,ν∈[M] µ ν
S Spherecenteredatmemorypatternξ withfiniteradiusR
µ µ
x⋆ FixedpointofT coveredbyS ,i.e.,x⋆ ∈S
µ µ µ µ
∆ Separationofamemorypatternξ fromallothermemorypatternsΞ,definedin(4.1)
µ µ
∆(cid:101)µ Separationofξ µatagivenxfromallmemorypatternsΞ,definedin(4.1)
24B Proofs of Main Text
B.1 Theorem 3.1
ProofofTheorem3.1. TheLagrangianofconvexoptimizationproblemdefinedin(3.2)is
d M d
1 (cid:88) (cid:88)(cid:88)
L := ∥w i∥2 +C (λ µ[i]η µ[i]+λ(cid:101) µ[i]η (cid:101)µ[i])
2
i=1 µ=1 i=1
M d
(cid:88)(cid:88)
− α [i](ϵ′ +η [i]−ξ [i]+⟨w ,Φ(ξ +δξ )⟩)
µ µ µ i µ µ
µ=1 i=1
M d
(cid:88)(cid:88)
− α [i](ϵ′ +η [i]−⟨w ,Φ(ξ +δξ )⟩+ξ [i]), (B.1)
(cid:101)µ (cid:101)µ i µ µ µ
µ=1 i=1
where λ µ[i], λ(cid:101) µ[i], α µ[i] and α (cid:101)µ[i] are Lagrange multipliers. Next, we solve stationary condi-
tion with respect to w ,η [i] and η [i] from above Lagrangian and derive corresponding optimal
i µ (cid:101)µ
solution. TheLagrangianin(B.1)admitsastationarysolution,whichisgivenby:




w
i
−(cid:80)M
µ=1(α µ[i]−α (cid:101)µ[i])Φ(ξ
µ
+δξ µ) = 0,
C −λ [i]−α [i] = 0, (B.2)
µ µ


 C −λ(cid:101) µ[i]−α (cid:101)µ[i] = 0.
Substitute(B.2)into(3.1)towrite
xnew[i] = T (x)[i] := ⟨w⋆,Φ(x)⟩, (B.3)
SVR i
withthelearnedweightmatrix
M
(cid:88)
w⋆ := (α [i]−α [i])Φ(ξ +δξ ) ∈ RDΦ. (B.4)
i µ (cid:101)µ µ µ
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
µ=1
∈R ∈RDΦ
Thecomplementaryslacknessconditionanddualfeasibilityof(B.1)aregivenby

 α [i](ϵ′ +η [i]−ξ [i]+⟨w ,Φ(ξ +δξ )⟩) = 0
 µ µ µ i µ µ

α [i](ϵ′ +η [i]−⟨w ,Φ(ξ +δξ )⟩+ξ [i]) = 0 (B.5)
(cid:101)µ (cid:101)µ i µ µ µ


 α µ[i],α (cid:101)µ[i],λ µ[i],λ(cid:101) µ[i] ≥ 0,
25forallµ ∈ [M]andi ∈ [d].
B.2 Lemma 3.1
Tosimplifyourproofs,wedefine
Φ(x)
Φ(x) := , (B.6)
h(x)
whereh(·) : Rd → Rissomenormalizationfunctionforlaterconvenience.
ToproveLemma3.1,weintroducethefollowingthreeauxiliarylemmas.
Lemma B.1. Let α [i] ≥ 0, α [i] ≥ 0 be a solution to (B.2) with KKT conditions (B.5). Then
µ (cid:101)µ
α [i]−α [i]hasthefollowingbounds
µ (cid:101)µ
−C ≤ α [i]−α [i] ≤ C,∀µ ∈ [M],i ∈ [d] (B.7)
µ (cid:101)µ
Proof. Weprovethislemmabycontradiction. Recallthatforeachfixedvaluesofµandi
α [i] ≥ 0,α [i] ≥ 0. (B.8)
µ (cid:101)µ
Firstly, we assume α [i],α [i] ∈ R (non-zero), for all µ ∈ [M] and i ∈ [d]. Recall complemen-
µ (cid:101)µ +
taryslacknessconditionsfrom(B.5)
(cid:40)
ϵ′ +η [i]−ξ [i]+⟨w ,Φ(ξ +δξ )⟩ = 0
µ µ i µ µ
(B.9)
ϵ′ +η [i]−⟨w ,Φ(ξ +δξ )⟩+ξ [i] = 0.
(cid:101)µ i µ µ µ
Combineabovetwoequationstowrite
η [i]+η [i] = −2ϵ′ ≤ 0. (B.10)
µ (cid:101)µ
Sincethecomponent-wiseerrorϵ′ ≥ 0,wehaveη [i]+η [i] ≤ 0. Thisconclusioncontradictsthe
µ (cid:101)µ
assumption of the non-negative condition on slack variables η [i],η [i] ≥ 0. Therefore, together
µ (cid:101)µ
with(B.2),atleastoneofα [i],α [i]mustbe0,forallµandalli. Subsequently,wehave
µ (cid:101)µ
0 ≤ α [i] ≤ C and 0 ≤ α [i] ≤ C, (B.11)
µ (cid:101)µ
26whichleadsto
−C ≤ α [i]−α [i] ≤ C. (B.12)
µ (cid:101)µ
LemmaB.2(MultinomialExpansion). Givenx,y ∈ Rd. Theidentity
(cid:32) (cid:33)(cid:32) (cid:33)
(xTy)n (cid:88) xℓ1···xℓ d yℓ1···yℓ d
= √1 d √1 d
n! ℓ !···ℓ ! ℓ !···ℓ !
1 d 1 d
ℓ1+···+ℓ d=n
holdsforalln ∈ N.
Proof.
(xTy)n 1
= (x y +···+x y )n
1 1 d d
n! n!
(cid:34) (cid:35)
d
1 n! (cid:89)
= (x y )n +···+ (x y )ℓi +···+(x y )n (cid:0)(cid:80)d ℓ =n(cid:1)
n! 1 1 ℓ !···ℓ ! i i d d i=1 i
1 d
i=1
d d
(cid:88) 1 (cid:89) (cid:89)
= (x )ℓi (y )ℓi
i i
ℓ !···ℓ !
1 d
ℓ1+···+ℓ d=n i=1 i=1
(cid:16) (cid:17)(cid:16) (cid:17)
xℓ1···xℓ
d
yℓ1···yℓ
d
(cid:88) 1 d 1 d
=
ℓ !···ℓ !
1 d
ℓ1+···+ℓ d=n
(cid:32) (cid:33)(cid:32) (cid:33)
(cid:88)
xℓ1···xℓ
d
yℓ1···yℓ
d
= √1 d √1 d .
ℓ !···ℓ ! ℓ !···ℓ !
1 d 1 d
ℓ1+···+ℓ d=n
LemmaB.3. LetK(·,·)bethehomogeneousinfinitepolynomialkernel[Chenetal.,2005]:
(cid:10) (cid:11)
(cid:88)∞ (xTy)n
K(x,y) = Φ(x),Φ(y) := , (B.13)
n!
n=0
where x,y ∈ Rd and Φ maps the feature vectors x and y into infinite dimensional space. Then,
Φ(·) = (ϕ(0),ϕ(1),...,ϕ(1),...,ϕ(n),...,ϕ(n),...)hasaclosedformsolution
0 1 D1 1 Dn
xℓ1···xℓ
d
ϕ(n) = √1 d , (B.14)
D′ ℓ !···ℓ !
1 d
27whereℓ +···+ℓ = n,1 ≤ D′ ≤ D andD :=
(cid:0)d+n−1(cid:1)
.
1 d n n n
Proof. ApplyingLemmaB.2onhomogeneousinfinitepolynomialkernel,wehave
(cid:10) (cid:11)
(cid:88)∞ (xTy)n
Φ(x),Φ(y) =
n!
n=0
∞ d d
(cid:88) (cid:88) 1 (cid:89) (cid:89)
= (x )ℓi (y )ℓi
i i
ℓ !···ℓ !
1 d
n=0ℓ1+···+ℓ d=n i=1 i=1
(cid:16) (cid:17)(cid:16) (cid:17)
∞
xℓ1···xℓ
d
yℓ1···yℓ
d
(cid:88) (cid:88) 1 d 1 d
=
ℓ !···ℓ !
1 d
n=0ℓ1+···+ℓ d=n
(cid:32) (cid:33)(cid:32) (cid:33)
(cid:88)∞
(cid:88)
xℓ1···xℓ
d
yℓ1···yℓ
d
= √1 d √1 d .
ℓ !···ℓ ! ℓ !···ℓ !
1 d 1 d
n=0ℓ1+···+ℓ d=n
From above, we observe that, for each fixed n, there are
(cid:0)d+n−1(cid:1)
terms in the summation. Conse-
n
quently,Φ(x)hasasolution
Φ(x) = (ϕ(0),ϕ(1),...,ϕ(1),...,ϕ(n),...,ϕ(n),...), (B.15)
0 1 D1 1 Dn
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(d+1−1)elements (d+n−1)elements
1 n
whereD =
(cid:0)d+n−1(cid:1)
and
n n
xℓ1···xℓ
d
ϕ(n) = √1 d (B.16)
D′ ℓ !···ℓ !
1 d
for1 ≤ D′ ≤ D andℓ +···+ℓ = n.
n 1 d
Proofof Lemma3.1. RecallthatthelearnedweightmatrixW iscomposedof
M
(cid:88) Φ(ξ +δξ )
w⋆ = (α [i]−α [i]) µ µ . (B.17)
i µ (cid:101)µ h(x)
µ=1
Substitutew⋆ into(3.1)towrite
(cid:32) (cid:88)M α µ[1]−α (cid:101)µ[1](cid:10) Φ(ξ
µ
+δξ µ),Φ(x)(cid:11) (cid:88)M α µ[d]−α (cid:101)µ[d](cid:10) Φ(ξ
µ
+δξ µ),Φ(x)(cid:11)(cid:33)
T (x) = ,··· , .
Dense
h(ξ +δξ ) h(x) h(ξ +δξ ) h(x)
µ µ µ µ
µ=1 µ=1
(B.18)
28(cid:16) (cid:17)
Let αµ[1]−α (cid:101)µ[1],..., αµ[d]−α (cid:101)µ[d] = ξ and h(x) := (cid:80)M (cid:10) Φ(ξ +δξ ),Φ(x)(cid:11) . Then T
h(ξµ+δξµ) h(ξµ+δξµ) µ µ=1 ν ν Dense
reducestofollowingformulation:
(cid:88)M (cid:10) Φ(ξ
µ
+δξ µ),Φ(x)(cid:11)
T (x) = ξ . (B.19)
Dense (cid:80)M (cid:10)
Φ(ξ +δξ
),Φ(x)(cid:11) µ
µ=1 ν=1 ν ν
FollowingLemmaB.3,herewedefinetheinnerproductofΦasakernelK : RD ϕ ×RD ϕ → R
+
(cid:10) (cid:11)
Φ(x),Φ(ξ +δξ ) := K(x,ξ +δξ ). (B.20)
µ µ µ µ
T isnowgivenby
Dense
M
(cid:88) K(x,ξ +δξ )
µ µ
T (x) = ξ . (B.21)
Dense (cid:80)M
K(x,ξ +δξ )
µ
µ=1 ν=1 ν ν
Observe that (2.3) T takes a Boltzmann form:
exp{·}/(cid:80)M
exp{·}. Recall Lemma B.3, we
Dense ν=1
take
√ √
( βx )ℓ1···( βx )ℓ d
ϕ(n) = √1 d , (B.22)
D′
ℓ !···ℓ !
1 d
withthekernel
√ √ √
(cid:88)∞ ((cid:10) βx, βξ
µ
+ βδξ µ(cid:11) )n
K(x,ξ +δξ ) = . (B.23)
µ µ
n!
n=0
Substitute(B.23)into(B.21)towrite
√ √ √
T (x) =
(cid:88)M (cid:80)∞ n=0(cid:0)(cid:10) β √x, β √ξ
µ
+ β √δξ µ(cid:11)(cid:1)n /n!
ξ . (B.24)
Dense (cid:80)M (cid:80)∞ (cid:0)(cid:10)
βx, βξ + βδξ
(cid:11)(cid:1)t
/t!
µ
µ=1 ν=1 t=0 ν ν
ByTaylor’stheorem,T takestheform
Dense
M
T (x) =
(cid:88) exp{β⟨x,ξ µ +δξ µ⟩}
ξ =
ΞSoftmax(cid:0) βΞTx(cid:1)
, (B.25)
Dense (cid:80)M
exp{β⟨x,ξ +δξ ⟩}
µ δ
µ=1 ν=1 ν ν
where Ξ = (ξ ,··· ,ξ ) ∈ Rd×M and Ξ = (ξ +δξ ,··· ,ξ +δξ ) ∈ Rd×M denote memo-
1 M δ 1 1 M M
riesandnoisesinmemories,respectively.
29B.3 Theorem 3.2
ProofofTheorem3.2. To take w⋆ for the sparse-structured model, the partial derivatives of L
withrespecttow ,η [i]andη [i]mustsatisfythestationaritycondition
i µ (cid:101)µ

(cid:80)



w
i
− µ∈M(α µ[i]−α (cid:101)µ[i])Φ(ξ
µ
+δξ µ) = 0,
C −λ [i]−α [i] = 0, (B.26)
µ µ


 C −λ(cid:101) µ[i]−α (cid:101)µ[i] = 0.
Then,wearrive
(cid:88) Φ(ξ +δξ )
w⋆ = (α [i]−α [i]) µ µ (B.27)
i µ (cid:101)µ h(x)
µ∈M
ByapproachsimilartoAppendixB.2,weobtaintheretrievaldynamicsforsparse-structuredmod-
ernHopfieldmodel:
(cid:88) (cid:2) (cid:0) (cid:1)(cid:3)
T (x) = Softmax βΞTx ξ . (B.28)
Sparse δ µ µ
µ∈M
B.4 Theorem 4.1
Proofof Theorem4.1. ToconnectT with∆ ,firstwederivetheboundon∥T (x)−ξ ∥
Sparse µ Sparse µ
via[Ramsaueretal.,2020]forµ ∈ M
(cid:13) (cid:13)
(cid:13) (cid:88) (cid:2) (cid:0) (cid:1)(cid:3) (cid:13)
∥T (x)−ξ ∥ ≤ (cid:13)ξ − Softmax βΞTx ξ (cid:13)
Sparse µ (cid:13) µ δ ν ν(cid:13)
(cid:13) (cid:13)
ν∈M
(cid:13) (cid:13)
(cid:13) (cid:0) (cid:1) (cid:88) (cid:0) (cid:1) (cid:13)
≤ (cid:13)(1−[Softmax βΞTx ] )ξ + [Softmax βΞTx ] ξ (cid:13) (B.29)
(cid:13) δ µ µ δ ν ν(cid:13)
(cid:13) (cid:13)
ν∈M,ν̸=M
ϵ (cid:88) ϵ
(cid:101) (cid:101)
≤ ϵ∥ξ ∥+ ∥ξ ∥≤ϵm+ (k −1)m (B.30)
(cid:101) µ ν (cid:101)
M −1 M −1
ν∈M,ν̸=µ
M +k −2
≤ m ϵ (B.31)
(cid:101)
M −1
(cid:26) (cid:18) (cid:19)(cid:27)
= m(M +k −2)exp −β ⟨x,ξ ⟩− Max⟨x,ξ ⟩ , (B.32)
µ ν
ν∈[M]
30(cid:8) (cid:0) (cid:1)(cid:9)
where k := |M|, m := Max ∥ξ ∥, ϵ := (M − 1)exp −β ⟨x,ξ ⟩−Max ⟨x,ξ ⟩ and
µ µ (cid:101) µ ν∈[M] ν
theinequality
(cid:26) (cid:18) (cid:19)(cid:27)
exp{β(⟨x,ξ ⟩−⟨x,ξ ⟩)}
(cid:2) Softmax(βΞTx)(cid:3)
=
ν µ
≤ exp −β ⟨x,ξ ⟩− Max⟨x,ξ ⟩ ,
ν 1+(cid:80) ν′̸=µexp{β(⟨x,ξ ν′⟩−⟨x,ξ µ⟩)} µ ν∈[M] ν
(B.33)
isusedin(B.32).
B.5 Corollary 4.1.1 and Corollary 4.1.2
Proofof Corollary4.1.1andCorollary4.1.2. Since the support set of dense modern Hopfield
modelisfull,i.e. k = M,(B.32)reducesto
(cid:26) (cid:18) (cid:19)(cid:27)
∥T (x)−ξ ∥ ≤ 2m(M −1)exp −β ⟨x,ξ ⟩− Max⟨x,ξ ⟩ . (B.34)
Dense µ µ ν
ν∈[M]
Comparing(B.32)with(B.34),weobtain
∥T (x)−ξ ∥ ≤ ∥T (x)−ξ ∥. (B.35)
Sparse µ Dense µ
From[Ramsaueretal.,2020,Theorem4],foranyqueryx,T approximatelyretrievesamem-
Dense
orypatternξ withretrievalerrorϵexponentiallysuppressedby∆ :
µ µ
(cid:8) (cid:0) (cid:2) (cid:13) (cid:13)(cid:3)(cid:1)(cid:9)
∥T (x)−ξ ∥ ≤ 2m(M −1)exp −β ∆ −2mMax ∥x−ξ ∥,(cid:13)x−x⋆ (cid:13) . (B.36)
µ µ µ µ
By(B.35),T alsoenjoysaboveretrievalerrorbound. Therefore,T (x)retrievesamemory
Sparse Sparse
patternξ withhighaccuracyafterasingleactivationwithasufficientlylarge∆ .
µ µ
B.6 Lemma 4.1
Proofof Lemma4.1. Recall[Huetal.,2023,Lemma2.2]thatforinitialqueryx ∈ S
0 µ
lim ∥x −ξ ∥ = 0, (B.37)
t µ
t→∞
where{x }∞ isasequencegeneratedbyT fromx ,i.e. T (x ) = x .
t t=0 Dense 0 Dense t t+1
Moreover,recallthatforanyquerypatternx ∈ S
µ
0 ≤ ∥T (x)−ξ ∥ ≤ ∥T (x)−ξ ∥. (B.38)
Sparse µ Dense µ
31Byapplyingsqueezetheoremon(B.38)and(B.37),wehave
lim ∥x −ξ ∥ = 0, (B.39)
(cid:101)t µ
t→∞
where{x }∞ isasequencegeneratedbyT ,i.e. T (x ) = x .
(cid:101)t t=0 Sparse Sparse (cid:101)t (cid:101)t+1
B.7 Theorem 4.2
Proofof 4.2. Following [Wu et al., 2024b, Hu et al., 2023], we define the separation of ξ at a
µ
givenxfromallmemorypatternsΞas
∆(cid:101) := Min [⟨x,ξ ⟩−⟨x,ξ ⟩]. (B.40)
µ µ ν
ν,ν̸=µ
Plugaboveinto(B.32),andget
(cid:110) (cid:111)
∥T (x)−ξ ∥ ≤ m(M +k −2)exp −β∆(cid:101) . (B.41)
Sparse µ µ
ByCauchy-Schwartzinequality,forallµ ∈ M,
|⟨ξ ,ξ ⟩−⟨x,ξ ⟩| ≤ ∥ξ −x∥·∥ξ ∥ ≤ ∥ξ −x∥m, (B.42)
µ µ µ µ µ µ
wewrite∆(cid:101) intermsof∆ :
µ µ
∆(cid:101) = ∆ −2∥ξ −x∥m = ∆ −2mR, (cid:0) Byx∈S (cid:1)
µ µ µ µ µ
whereR isradiusofthesphereS . SinceT isamappingT : S → S ,outputofthemappingT
µ µ µ
fallsinS withradiusR. Therefore,R islower-boundedby
µ
R ≥ (M +k −2)exp{−β(∆ −2mR)}m ≥ ∥T (x)−ξ ∥, (B.43)
µ µ
andthus
(cid:18) (cid:19)
1 (M +k −2)m
∆ ≥ ln +2mR. (B.44)
µ
β R
B.8 Lemma 4.2
32Webuiltourproofontopof[Huetal.,2023,Lemma2.1],whichconsists3steps:
• (Step 1.) We establish a more refined well-separation condition, ensuring that patterns
{ξ } arewell-storedinH andcanberetrievedbyT withanerrorϵatmostR.
µ µ∈[M]
• (Step 2.) This condition is then related to the cosine similarity of memory patterns,
from which we deduce an inequality governing the probability of successful pattern
storageandretrieval.
• (Step 3.) We pinpoint the conditions for exponential memory capacity and confirm
theirsatisfaction.
ProofofLemma4.2. Ourproofisbuiltontopof[Huetal.,2023,Corollary3.1.1]withadifferent
well-separationcondition.
Let∆ := Min ∆ andθ Herewedefine∆ andθ betheanglebetweentwopatterns
min µ∈[M] µ µν min µν
ξµ andξν.
Inorderforapatternξ tobewell-stored,byTheorem4.2,weneed
µ
(cid:18) (cid:19)
1 (M +k −2)m
∆ ≥ ln +2mR. (B.45)
min
β R
Ontheotherhand,weobserve
(cid:2) (cid:3)
∆ = Min m2(1−cos(θ )) = m2[1−cos(θ )], (B.46)
min µν min
1≤µ≤ν≤M
whereθ := Min θ ∈ [0,π]. Then,wehave
min 1≤µ≤ν≤M µν
(cid:18) (cid:19)
1 (M +k −2)m
m2[1−cos(θ )] ≥ ln +2mR. (B.47)
min
β R
As a result, the probability of successful storage and retrieval, i.e., the minimal separation ∆
min
thatsatisfiesTheorem4.2,isgivenby
(cid:18) (cid:18) (cid:19) (cid:19)
1 (M +k −2)m
P ∆ ≥ ln +2mR = 1−p. (B.48)
µ
β R
33Inserting(B.47)intoabove,weobtain
(cid:18) (cid:18) (cid:19) (cid:19)
1 (M +k −2)m
P m2[1−cos(θ )] ≥ ln +2mR = 1−p. (B.49)
min
β R
From[Olveretal.,2010,Equation(4.22.2)],for0 ≤ cos(θ ) ≤ 1,cos(θ )hasanupperbound
min min
θ2
cos(θ ) ≤ 1− min. (B.50)
min
5
Itholds
(cid:18) m2θ2 1 (cid:18) (M +k −2)m(cid:19) (cid:19)
P min ≥ ln +2mR = 1−p, (B.51)
5 β R
whichleadsto
(cid:32) √
2 (cid:20) (cid:18) (cid:19)
(cid:21)1(cid:33)
2 5Md−1 1 (M +k −2)m 2
P Md−1θ
min
≥ ln +2mR = 1−p. (B.52)
m β R
Forlaterconvenience,hereweintroduceanextraM2/d−1 onbothsides.
Let ω :=
2πd+1/2
be the area of a d-dimensional unit sphere manifold, with Γ(·) denoting the
d Γ(d+1)
2
gammafunction.
Following[Brauchartetal.,2018,Lemma3.5],wehave
(cid:32) √
2 (cid:20) (cid:18) (cid:19)
(cid:21)1(cid:33)
2 5Md−1 1 (M +k −2)m 2
P Md−1θ
min
≥ ln +2mR = 1−p
m β R
(cid:20) (cid:18) (cid:19) (cid:21)d−1
≥ 1− 1 γ d−15d− 21 M2m−(d−1) 1 ln (M +k −2)m +2mR 2 , (B.53)
2 β R
whereγ istheratiobetweenthesurfaceareasoftheunitspheresin(d−1)andddimensions:
d
(cid:0) (cid:1)
1ω 1 Γ d+1
γ := d−1 = √ 2 . (B.54)
d
d ω d π
Γ(cid:0) d(cid:1)
d
2
√
Recalld,M ∈ N ,p ∈ [0,1]. Hence,itholdsM = pCd−1 forsomerealvaluesC ∈ R.
+ 4
34Then,by(B.53),wehave
(cid:16)√ (cid:17)2
(cid:40)
1
(cid:16)√ pCd− 41 +k −1(cid:17) m 1(cid:41)d− 21
5d− 21 pCd− 41 m−(d−1) ln + −p ≤ 0, (B.55)
β R β
andthus
(cid:40)
1
(cid:16)√ pCd− 41 +k −1(cid:17) m 1(cid:41)d− 21
5d− 21 Cd− 21 m−(d−1) ln + ≤ 1. (B.56)
β R β
Further,werewrite(B.56)as
(cid:16)√ (cid:17) 
(cid:40) pCd−1 +k −1 m (cid:41)
5C 4
ln +1 −1 ≤ 0, (B.57)
m2β R
andidentify
(cid:40) √ (cid:41)
4 (cid:20) m( p+k −1)(cid:21) 4m2β
a := ln +1 , b := . (B.58)
d−1 R 5(d−1)
By[Huetal.,2023,Lemam3.1],C takestheform
b
C = , (B.59)
W (exp{a+lnb})
0
whereW (·)istheupperbranchoftheLambertW function. SincethedomainoftheLambertW
0
functionisx > (−1/ e,∞)andthefactexp{a+lnb} > 0,thesolutionfor(B.59)exists.
When the inequality (B.56) holds, the lower bound on the exponential storage capacity M can be
writtenas:
√
d−1
M ≥ pC 4 . (B.60)
In particular, the above lower bound takes a form similar to [Ramsauer et al., 2020, Theorem 3].
35C Nonparametric Modern Hopfield Family
Inthissection,wederiveafamilyofmodernHopfieldmodelsaspossibleextensionsbasedonthe
proposedframework(Theorem3.1). 4
C.1 Linear Modern Hopfield Model
PropositionC.1(LinearModernHopfieldModel). LetΦ(x) = (ϕ (x),...,ϕ (x))withthecom-
1 d
ponentϕ:
elu(x[i])+1
ϕ (x) := , ∀i ∈ [d], (C.1)
i (cid:80)M
⟨Φ(x),Φ(ξ +δξ )⟩
µ=1 µ µ
where elu(·) denotes the exponential linear unit activation function proposed by [Clevert et al.,
2015]. ByTheorem3.1,fittingT onD following(3.2)gives
SVR
(cid:80)M
⟨Φ(x),Φ(ξ +δξ )⟩ξ
µ=1 µ µ µ
T (x) = . (C.2)
Linear (cid:80)M
⟨Φ(x),Φ(ξ +δξ )⟩
ν=1 ν ν
BysettingthekernelmappingΦtolinearfeaturemap(C.1),weobtainalinearmodernHopfield
modelwithlinearcomplexityO(n). ComparedwithdensemodernHopfieldmodel,ourproposed
linear modern Hopfield model has time and memory complexity O(n) instead of O(n2) since we
only need to compute
(cid:80)M
Φ(ξ +δξ )ξ and
(cid:80)M
Φ(ξ +δξ ) once and reuse them for the
µ=1 µ µ µ µ=1 µ µ
computationofeveryquerypattern. Thismodelisbydesignconnectedtotherandomattentionof
linearattention[Katharopoulosetal.,2020].
C.2 Multi-Head Modern Hopfield Models
To derive the multi-head Hopfield model, we cast T as multiple SVR problems such that the
Multi
memorizationofmemorypatternsΞcorrespondstotrainingaregressionmodelT ondatasets
Multi
{Ξ } withnoises{Ξ}. TheseStrainingdatasetsaregivenas{(ξ1+δξ1,ξ1)} ,··· ,{(ξH+
s s∈[H] µ µ µ µ∈[M] µ
δξH,ξH)} . To handle multiple regression problems, we extend the regression model (3.1)
µ µ µ∈[M]
intothefollowing.
Definition C.1 (Multi-Head Regression Model). Given an input vector x ∈ Rd. The output
4Hu et al. [2024b] provide a theoretical characterization of these possible extensions from the perspective of
fine-grainedcomplexitytheory.
36y ∈ Rd oftheregressionmodelT isdefinedas:
(cid:98) multi
H
(cid:88)
y = T (x) := Ws (WsΦs(x)) ∈ Rd, (C.3)
(cid:98) Multi O
s=1
where Ws ∈ Rd×d, Ws = [ws,··· ,ws]T ∈ Rd×DΦ for all s ∈ [H], and Φs(x) =
O 1 d
(ϕs(x),··· ,ϕs (x)) : Rd → RDΦ denote aseries of output projection matrices, weighted matrix
1 DΦ
andkernelmapping,respectively.
Adopting this multi-head regression model, we introduce the following multi-head modern Hop-
fieldmodel.
Proposition C.2 (Multi-Head Modern Hopfield Models). Let Φ(·) =
(ϕ(0),ϕ(1),...,ϕ(1),...,ϕ(n),...,ϕ(n),...)with,for1 ≤ D′ ≤ D ,
0 1 D1 1 Dn n
√ √
( βx )ℓ1···( βx )ℓ d
ϕ(n) := 1 d √ , (C.4)
D′ (cid:80)M
⟨Φ(ξ +δξ ),Φ(x)⟩· ℓ !···ℓ !
µ=1 µ µ 1 d
where ℓ + ··· + ℓ = n, and D :=
(cid:0)d+n−1(cid:1)
. By Theorem 3.1, fitting T on H training data
1 d n n SVR
sets{(ξ1 +δξ1,ξ1)} ,··· ,{(ξH +δξH,ξH)} following(3.2)gives
µ µ µ µ∈[M] µ µ µ µ∈[M]
H
(cid:88) (cid:0) (cid:1)
T (x) = Ws Ξ Softmax(βΞTx) . (C.5)
Multi O s δ
s=1
Thismodelisbydesignconnectedtothestandardmulti-headattention.
C.3 PRFs (Positive Random Features) Kernel Modern Hopfield Model
PropositionC.3(PositiveRandomFeaturesModernHopfieldModel). LetΦ(·) = (ϕ ,...,ϕ )
1 DΦ
with
Ψ(x)
Φ(x) := √ (ψ (⟨p ,x⟩),...,ψ (⟨p ,x⟩),...,ψ (⟨p ,x⟩),...,ψ (⟨p ,x⟩)), (C.6)
1 1 1 m l 1 l m
D
Φ
where D = l · m, Ψ : Rd → R , ψ ,...,ψ are functions that map from R → R, and
Φ 1 m
p ,...,p i ∼id P are vectors from some distribution P ∈ ∆d (∆d := {p ∈ Rd | (cid:80)d p = 1} is
1 m + i=1 i
the(d−1)-dimensionalunitsimplex.). ByTheorem3.1,fittingT onD following(3.2)gives
SVR
M
(cid:88)
T (x) = E [D(cid:98)−1⟨Φ(x),Φ(ξ +δξ )⟩]ξ , (C.7)
PRF D µ µ µ
µ=1
37whereweadoptthenormalizationmapD(cid:98)−1 := ⟨ξ ,x⟩givenby[Choromanskietal.,2021].
1
Comparing with regular modern Hopfield model, PRF Hopfield model only has the linear space
andtimecomplexity,withoutanyadditionaltreatmentsuchasintroducingsparsityorlow-rankness.
Thesignificanceofthisrepresentationalcapabilityliesinitsabilitytofacilitateaprecisecompar-
ison between softmax and alternative kernels in the context of extensive tasks, surpassing the
capabilities of regular modern Hopfield models and enabling a comprehensive exploration of op-
timal kernels. This model is by design connected to the Performer-type attention [Choromanski
etal.,2021]. Inpractice,thedefaultoptionforP isstandardGaussian[Choromanskietal.,2021].
38D Nonparametric Modern Hopfield Layers for Deep Learning
BuildingonthelinkbetweenthenonparametricmodernHopfieldmodelsandtheattentionmech-
anisms,weintroducetheNonparametricHopfield(NPH)layersfordeeplearning.
Following [Hu et al., 2023, Ramsauer et al., 2020], we say X and Ξ are in the associative space
(embedded space), as they are embedded from the raw query R and Y memory patterns, respec-
tively, via XT = RW := Q, and ΞT = YW := K, with some W and W . Taking
Q K Q K
the transpose of T in (3.4) (with a given feature map Φ) and multiplying with W such that
V
V := KW ,wehave
V
(cid:0) (cid:1)
Z := QnewW = T βQKT V, (D.1)
V SVR
which leads to an attention mechanisms with various T as activation functions. Plugging back
SVR
therawpatternsRandY,wearrivetheNonparametricModernHopfield(NPH)layer(s),
(cid:0) (cid:1)
NPH(R,Y) = T βRW WTYT YW W , (D.2)
SVR Q K K V
which can be seamlessly integrated into deep learning architectures. Concretely, the NPH lay-
ers take matrices R, Y as inputs, with the weight matrices W , W , W . Depending on its
Q K V
configuration,itoffersseveralfunctionalities:
1. Memory Retrieval: In this learning-free setting, weight matrices W , W , and W are
K Q V
set as identity matrices. Here, R represents the query input, and Y denotes the stored
memorypatternsforretrieval.
2. NPH: This configuration takes R and Y as inputs. Intending to substitute the attention
mechanism, the weight matrices W , W , and W are rendered learnable. Furthermore,
K Q V
R, Y, and Y serve as the sources for query, key, and value respectively. Achieving a self-
attention-likemechanismrequiressettingRequaltoY.
3. NPHPooling: With inputs Q and Y, this layer uses Q as a static prototype pattern, while
Y contains patterns over which pooling is desired. Given that the query pattern is replaced
bythestaticprototypepatternQ,theonlylearnableweightmatricesareW andW .
K V
4. NPHLayer: TheNPHLayerlayertakesthequeryRasitssingleinput. Thelayerequipswith
learnable weight matrices W and W , which function as our stored patterns and their
K V
corresponding projections. This design ensures that our key and value are decoupled from
theinput. Inpractice,wesetW andY asidentitymatrices.
Q
39E Experimental Studies
Weverifythemethodproposedinthemaincontentwiththefollowingexperimentalsections.
1. MemoryRetrievalTask(Figure3).
2. MultipleInstanceLearningonMNIST(Figure5).
3. MultipleInstanceLearningonRealWorldDatasets.
4. TimeSeriesPrediction.
5. ComputationalEfficiency.
WeconsiderthefollowingvariationsofModernHopfieldModelsinthispaper:
• DenseModernHopfield[Ramsaueretal.,2020]
• SparseModernHopfield[Huetal.,2023]
• Sparse-StructuredModernHopfield:
– RandomMaskedModernHopfield
– WindowModernHopfield
– Top-KModernHopfield
• LinearModernHopfield
• RandomFeatureModernHopfield
40E.1 Memory Retrieval Task (Figure 3)
In the memory retrieval task, we examine two datasets: MNIST (sparse) and CIFAR10 (dense).
We employ the sum-of-squares distance between the retrieved image and the ground truth image
tomeasureretrievalerror. Thisexperimentencompassestwosettings:
1. Half-maskedimagerecovery,and
2. Noisyimagerecovery.
In the half-masked image recovery scenario, we obscure half of the pixels in the image. The
memory set size (M) is varied from 10 to 200, and we report the average retrieval error (sum-of-
squaredifference)over50runs. Inthenoisyimagerecoveryscenario,wefixthememorysetsize
at 100, and introduce varying scales of Gaussian noise to the image, with variance ranging from
0.1to1.4.
Implementation Details. The memory set itself is chosen randomly from the dataset in each
iteration. Weadheretotheimplementationoutlinedin[Huetal.,2023].
41Figure 3: Numerical Justifications for Theoretical Results: Memory Capacity and Noise
Robustness. (Upper): MemoryCapacitymeasuredbysuccessfulretrievalratesfromhalf-masked
queries (Lemma 4.2). (Bottom): Memory Robustness measured by success retrieval rates from
noisy queries with different scales of Gaussian noise (Remark 4.2). For all Hopfield models,
we set β = .01/0.1 (for MNIST/CIFAR10) for better visualizations. Plotted are the means and
standard deviations of 10 runs. We see that Top-K Hopfield shows similar exponential capacity
as Dense [Ramsauer et al., 2020] and Sparse Hopfield [Hu et al., 2023]. Note that the Random
Masked Hopfield models perform poorly, especially on MNIST. This is because they violate the
µ ∈ MassumptioninTheorem3.2,astherandommaskmightinadvertentlymaskoutthecorrect
patterninthememoryset.
42E.2 Multiple Instance Learning on MNIST (Figure 4 & Figure 5)
Quotedfrom[Huetal.,2023,Section4.2]:
Multiple Instance Learning (MIL)[Ilse et al., 2018, Carbonneau et al., 2018] is a variation
of supervised learning where the training set consists of labeled bags, each containing multi-
pleinstances. ThegoalofMIListopredictthebaglabelsbasedontheinstancestheycontain,
whichmakesitparticularlyusefulinscenarioswherelabelingindividualinstancesisdifficult
orimpractical,butbag-levellabelsareavailable. Examplesofsuchscenariosincludemedical
imaging (where a bag could be an image, instances could be patches of the image, and the
labelcouldindicatethepresenceorabsenceofdisease)anddocumentclassification(wherea
bagcouldbeadocument,instancescouldbethewordsorsentencesinthedocument,andthe
labelcouldindicatethetopicorsentimentofthedocument).
Figure 4: Comparison of MIL performance varying bag sizes. The y-axis represents the
accuracy on test set. We employ various variants of the HopfieldPooling layers to observe
to performance change with respect to different bag size. We see that for those generate actual
sparse matrices (Top-K, Random Masked and Sparse Hopfield), their performances are more ro-
bustagainstbagsizeincrease.
In this experiment, we designate one digit from MNIST as a negative signal, and the remaining
digits as positive signals. The objective is to predict whether a given bag of instances (digits)
containsthenegativesignal. Wevarythememorysetsize(M)from5to100andreportthemean
accuracy over 10 runs. We compare the performance of Dense Hopfield, Sparse Hopfield, Top-K
Hopfield (with 20%, 50%, and 80%), Random Feature Hopfield, Random Masked Hopfield and
Linear Hopfield. We omit Window Hopfield for reasons mentioned earlier. The result can be
foundinFigure4. Additionally,wealsoconductaconvergenceanalysisinFigure5withbagsize
=50. WeplotthelossandaccuracycurveonMNISTMILtrainingandtestset.
ImplementationDetails. WeemployanembeddinglayertoprojecttheflattenedMNISTimages
into the hidden space, followed by a layer of layer normalization. Subsequently, we utilize the
43Hopfield Pooling layer to pool over all the instances in the bag, followed by a second layer nor-
malizationlayer. Finally,afullyconnectedlayerisusedtoprojectthehiddenrepresentationofthe
bagintothelabelspace. AllmodelsaretrainedusingtheAdamWoptimizerfor150epochs,with
a cosine annealing learning rate decay applied to all models. Note that we exclude Window Hop-
field in this and the subsequent MIL experiment since Window Hopfield requires both the query
and memory pattern numbers to be large to perform the sliding window operation. However, in
our model structure, the number of query patterns in the pooling layer is set to 2. The details of
thehyperparameterscanbefoundinTable2.
Table2: HyperparameterusedintheMILMNISTexperiment.
parameter values
batchsize 256
learningrate 1e-3
embeddingdimension 256
numberofheads 4
headdimension 64
testsetsize 500
trainsetsize 2000
scaling 0.1
numofpattern 2
epochs 150
44Figure 5: Numerical Justifications for Theoretical Results: Convergence Analysis. (Upper):
TraininglossandaccuracycurveofdifferentHopfieldmodelsonMNISTmultipleinstancelearn-
ingtask(Theorem4.1). (Bottom): ValidationlossandaccuracycurveofdifferentHopfieldmod-
els on MNIST multiple instance learning task (Theorem 4.1). We train all the model with 150
epochs with cosine annealing learning rate decay. Plotted are the means over 10 runs. We ob-
serve that with Sparse Hopfield having the highest validation accuracy, Random feature Hopfield
also shows competitive performance with faster convergence speed. On the other hand, Top 20%
Hopfield also converges fast with almost no performance drop. More experimental details can be
foundinAppendixE.
45E.3 Multiple Instance Learning on Real World Datasets
For this experiment, we follow [Ramsauer et al., 2020, Hu et al., 2023] to conduct MIL experi-
mentsonrealworlddatasets. However,weemployasimplermodelstructureandasmallerhyper-
parameter search space, rendering our results incomparable. We utilize four datasets: Elephant,
Fox, and Tiger for image annotation [Ilse et al., 2018], and UCSB breast cancer classification
[Kandemir et al., 2014]. We compare Dense Hopfield, Sparse Hopfield, TopK Hopfield at 20%,
50%, and 80%, Random Feature Hopfield, and Linear Hopfield. Random Masked Hopfield is
excluded due to its non-deterministic inference, and Window Hopfield is omitted as previously
mentioned. TheresultsarepresentedinTable5.
Dataset Details. The experiment is conducted on four MIL datasets. Elephant, Fox, and Tiger
are designed for image annotation and consist of preprocessed and segmented colored images.
Eachimageischaracterizedbydescriptorsforcolor,texture,andshape. Thesedatasetseachcon-
tain 100 positive images featuring the specified animal and 100 negative images drawn from a
set of images depicting other animals. Additionally, we evaluate our model on the UCSB breast
cancer classification task. In the UCSB dataset, each instance comprises a patch of a histopatho-
logical image depicting either cancerous or normal tissue. The detailed statistics of the datasets
arereportedinTable3.
Table3: StatisticsofMILbenchmarkdatasets
Name Instances Features Bags +bags −bags
Elephant 1391 230 200 100 100
Fox 1302 230 200 100 100
Tiger 1220 230 200 100 100
UCSB 2002 708 58 26 32
ImplementationDetails. Wefollowtheexperimentalsettingin[Ramsaueretal.,2020]andem-
ploy stratified 10-fold cross-validation to evaluate the performance of each baseline Hopfield
model. In each fold, we utilize a stratified sampling process to partition the data into a train-
ing set and a validation set, with a split rate of 0.1. Hyperparameters are optimized via random
search by maximizing the ROC-AUC score on the validation set. All reported ROC-AUC scores
represent the average results over 5 runs with different random seeds. The random search space
is delineated in Table 4, with the number of trials set to 50 for each fold. The embedding layer,
a pre-HopfieldPooling linear network, has its layer width determined by the number of hidden
units. A dropout operation, also referred to as bag dropout, is applied post the embedding layer
and the Hopfield Pooling layer. Notably, to better showcase the performance of Top-k Hopfield,
dropout is not applied to the attention weight. All models are trained using the Adam optimizer
over 50 epochs. To mitigate overfitting, an early-stopping mechanism is employed, selecting the
bestcheckpointbasedonthevalidationset.
46Table 4: Hyperparameter random search space on the respective validation sets of the Elephant,
Fox,TigerandUCSBbreastcancerdatasets.
parameter values
batchsize {4,8,16}
learningrates {10−3,10−4,10−5}
weightdecay {0,10−3,10−4,}
layerwidth {128,256,512}
numberofheads {4,8}
scalingfactors {0.1,1}
dropout {0.0,0.30.5}
Table 5: Results for MIL benchmark datasets in terms of AUC score. The results suggest that
the proposed model achieves performance comparable to the existing Dense and Sparse Modern
Hopfield models [Hu et al., 2023, Ramsauer et al., 2020]. Note that, since our aim here is to con-
duct an atomic setting for fair comparison, we employ a simpler network structure (with smaller
hyperparameter search space) compared to the ones used in [Hu et al., 2023, Ramsauer et al.,
2020]. Consequently,ourresultsdonotalignwiththosein[Huetal.,2023]forDenseandSparse
ModernHopfieldModels.
Method Tiger Fox Elephant UCSB
DenseHopfield[Ramsaueretal.,2020] 0.813 0.563 0.877 0.524
SparseHopfield[Huetal.,2023] 0.830 0.573 0.893 0.585
Top-20%Hopfield 0.824 0.562 0.848 0.586
Top-50%Hopfield 0.812 0.566 0.852 0.572
Top-80%Hopfield 0.812 0.560 0.872 0.551
RandomFeatureHopfield 0.802 0.508 0.875 0.566
LinearHopfield 0.797 0.571 0.869 0.561
47E.4 Time Series Prediction
We further showcase the performance (in Table 6) and efficiency (in Figure 6) of the proposed
nonparametricmodernHopfieldmodelswithmultivariatetimeseriespredictiontasks.
Table 6: Time series prediction using different Hopfield layers (Appendix D) across five
datasets. We evaluate each dataset with different prediction horizons (showed in the second
column). We report the average Mean Square Error (MSE) and Mean Absolute Error (MAE)
metricsof5runs. RFdenotestheRandomFeatureHopfieldlayer. Onenotableobservationisthat
thenoiselevelofthedatasetsignificantlyinfluencestimeseriesprediction. Therefore,employing
Hopfield layers with strong noise-robustness offers performance improvements. Moreover, based
on our results, the proposed efficient Hopfield models not only offer significant computational
efficiency but also maintain comparable performance. Especially, the Random Feature Hopfield
and Linear Hopfield layers (models) not only match but even outperform Dense Hopfield model
inseveralsettings. Asasidenote,WindowHopfieldexhibitssignificantperformancedegradation
in most settings. This degradation arises because it solely focuses on local information. Being
the only Hopfield model that does not span the entire associative range (i.e., sequence length), it
overlooks a substantial portion of the autoregressive correlation present in time series data. We
alsorecordthetimeusedforoneepochonETTh1datasetwithdifferentpredictionhorizon(input
lengthaswell). ThedurationtimeperepochwasshowedinFigure6.
Models Dense Sparse Top20% Top50% Top80% Window RF Linear
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
96 0.137 0.307 0.144 0.314 0.148 0.319 0.153 0.321 0.147 0.318 1.043 0.881 0.147 0.312 0.149 0.320
192 0.153 0.326 0.152 0.325 0.146 0.318 0.161 0.333 0.150 0.320 1.003 0.870 0.158 0.332 0.141 0.313
336 0.148 0.319 0.146 0.319 0.156 0.327 0.122 0.286 0.160 0.333 0.889 0.767 0.151 0.322 0.138 0.307
720 0.169 0.331 0.148 0.314 0.184 0.345 0.161 0.327 0.123 0.287 0.756 0.761 0.141 0.271 0.171 0.333
96 0.148 0.301 0.147 0.301 0.144 0.311 0.151 0.310 0.142 0.31o 0.943 0.854 0.151 0.314 0.155 0.319
192 0.189 0.350 0.187 0.340 0.191 0.347 0.185 0.338 0.188 0.341 1.054 0.893 0.190 0.347 0.192 0.348
336 0.163 0.320 0.165 0.322 0.168 0.331 0.161 0.312 0.169 0.330 0.873 0.334 0.175 0.333 0.176 0.337
720 0.159 0.300 0.161 0.303 0.165 0.313 0.167 0.313 0.169 0.320 0.764 0.731 0.162 0.309 0.165 0.310
96 0.378 0.371 0.373 0.370 0.384 0.382 0.386 0.386 0.383 0.376 0.989 0.854 0.390 0.403 0.365 0.378
192 0.486 0.426 0.535 0.507 0.502 0.427 0.501 0.464 0.519 0.481 1.000 0.843 0.543 0.438 0.549 0.464
336 0.748 0.693 0.760 0.688 0.650 0.549 0.674 0.571 0.638 0.545 1.012 0.849 0.767 0.588 0.672 0.578
720 0.961 0.711 0.993 0.758 1.145 0.843 1.166 0.847 1.211 0.872 1.061 0.865 1.362 0.896 1.052 0.770
96 0.347 0.474 0.347 0.477 0.348 0.474 0.348 0.474 0.356 0.479 0.952 0.819 0.345 0.470 0.355 0.476
192 0.399 0.505 0.386 0.497 0.360 0.482 0.370 0.490 0.361 0.482 0.977 0.828 0.368 0.487 0.354 0.478
336 0.407 0.512 0.387 0.501 0.376 0.489 0.397 0.503 0.403 0.505 0.931 0.808 0.392 0.504 0.407 0.514
720 0.669 0.631 0.632 0.623 0.590 0.604 0.569 0.593 0.618 0.618 0.564 0.595 0.564 0.595 0.747 0.676
96 1.466 0.654 1.489 0.638 1.483 0.645 1.517 0.630 1.477 0.638 1.520 0.625 1.515 0.635 1.489 0.644
192 1.551 0.654 1.550 0.657 1.557 0.649 1.548 0.657 1.551 0.652 1.570 0.637 1.551 0.654 1.551 0.653
336 1.595 0.663 1.595 0.662 1.599 0.663 1.592 0.665 1.604 0.657 1.612 0.646 1.613 0.646 1.614 0.646
720 1.660 0.681 1.671 0.671 1.664 0.674 1.676 0.663 1.682 0.661 1.683 0.661 1.682 0.661 1.681 0.660
E.4.1 ImplementationDetails
For ease of comparison, we employ the simplest possible architecture: an embedding layer to
project each signal into a hidden space, followed by a single Hopfield layer. By doing so, we
treat every signal as a query pattern. Next, we employ a Hopfield Pooling layer to pool over all
48
1hTTE
1mTTE
LCE
HTW
cfifarTthe signals into a single hidden vector. Finally, we utilize a fully connected layer to generate the
prediction. Forallexperiments,wemaintainthesameinputandpredictionhorizonforsimplicity.
TheresultscanbefoundinTable6andFigure6.
Datasets. Weconducttheexperimentsonfourmultivariatetimeseriesreal-worlddatasets: ETTh1
(ElectricityTransformerTemperature-hourly),ETTm1(ElectricityTransformerTemperature-minutely),
WTH(Weather),ECL(ElectricityConsumingLoad),Traffic.
Setup. Foreachdataset,weusetheirunivariatesettingforourtimeseriespredictionexperiment.
We choose Dense, Sparse, Random Feature, Linear, TopK and Window Hopfield as baselines.
We select 4 different prediction horizons for demonstration, which are 96,196,336,720. We
report the average error of 5 runs, evaluated using Mean Square Error (MSE) and Mean Abso-
lute Error (MAE) metrics. For window Hopfield, we set the window size as 8,12,14,16, w.r.t.
96,196,336,720.
49Figure 6: The processing time comparison among different Hopfield models utilized in the time
series prediction task described in Table 6. We evaluate the efficiency of multivariate time se-
ries prediction on ETTh1 dataset. The findings are consistent with the efficiency discussion
in Section 3.2, where the Sparse/Dense/Top-K models (all with O(d2) complexity) necessitate
more time to complete an epoch. In conjunction with the results in Figure 5, it is evident that
the efficient modern Hopfield models (Window, Linear, Random Feature) not only converge in
fewer or comparable epochs but also require less time per epoch compared to the less efficient
(Sparse/Dense/Top-K)Hopfieldmodels.
50E.5 Computational Efficiency
HerewedemonstratethecomputationaloverheadfordifferentefficientmodernHopfieldvariants.
WefocusonthecomputationaltimedurationandFlops(thenumberofFloatingpointoperations).
Theresultsdemonstrate
• ForrandommaskedHopfield,thecomputationaltimescalesupwithrespecttoprobability.
• Random feature Hopfield, Linear Hopfield and Window Hopfield demonstrates fast com-
putational overhead in practice. In addition, these efficient Hopfield models also enjoy
significantlylowerfloatingpointoperationswithonlyamarginalsacrificeinperformance.
• Under PyTorch (version 1.11.0) framework, random masked Hopfield is not able to obtain
computationalefficiencyimprovementdespitefromitssparse-structurednature.
Figure 7: (LHS:) Comparison of duration (ms) per batch for different Hopfield Models. (RHS:)
The scaling behavior of Random Masked Hopfield with different masking ratios. The probability
denotes the ratio being masked out. We employ various variants of the Hopfield layers to
process a batch of tensors, with a batch size of 4 and a hidden dimension of 16. We vary the
inputmemorysize(inputlength). NotethatweseparatetheRandomMaskedHopfieldfromother
baselines since the sparse matrix operation in PyTorch, still in the beta stage, may not be as fully
optimizedasdensetensoroperations.
Implementation Details. In this section, we exclusively evaluate the computational efficiency
of different Hopfield models with respect to varying input lengths using the Hopfield layer. We
report the average duration time per batch, as shown in Figure 7, and the FLOPs concerning
different input lengths (memory sizes), as depicted in Figure 8. It’s notable that different code
implementation methods could potentially affect computational efficiency. We use a randomized
batchedtensorasinputx,wherex ∈ Rmemorysize×16,andthebatchsizeis45. ForRandomFeature
5approximately(4×4×16×memorysize)bytes
51Figure 8: (LHS:) The FLOPs comparison for Random Masked Hopfield with different proba-
bilities is depicted. The lines for Dense and Sparse Hopfield are overlapped, as are the lines for
Random Feature Hopfield and Linear Hopfield. (RHS:) The FLOPs comparison across different
Hopfield Models is shown. We employ the same settings as in the duration figure. Note that the
fvcore package may count sparse matrix operations as normal floating point operations, which is
whywemightnotseeadifference.
Hopfield and Linear Hopfield, we adhere to the Performer implementation6, while for Window
Hopfield, we follow the Longformer implementation7. For Random Masked Hopfield, we utilize
the torch.sparse.sampled_addmm8 feature, and for other baselines, we employ standard
PyTorch built-in functions for implementation. We report the average forward pass time over 10
runs,alongsidetheFLOPs,withbothmetricsevaluatedondifferentinputlengths. FLOPsarecal-
culatedusingthefvcorepackage9. NotethatmostpubliclyavailablepackagesforFLOPsprofiling
areeitherunderdevelopmentorinbeta,hencecalculationerrorsareanticipated. Additionally,the
torch.sparse package is also in beta, implying its performance may not be fully optimized,
especiallyregardingFLOPscalculationandoperationoverhead.
Discussion. Notethat,bynature,bothDenseandSparseHopfieldexhibitthesameFLOPs. More-
over,itisobservedthatRandomFeatureHopfieldandLinearHopfieldalsosharethesameFLOPs,
as the only distinction between them lies in the kernel function. Regarding Window Hopfield, its
FLOPs fall in between, demonstrating notable efficiency compared to both Dense and Sparse
Hopfield. In terms of duration time per batch, Sparse Hopfield appears slightly faster than its
dense counterpart, likely due to the additional zeros generated by sparsemax. Window Hopfield,
on the other hand, showcases a significant reduction in duration compared to Sparse Hopfield.
Lastly,itisnotedthattheprocessingtimeforbothRandomFeatureHopfieldandLinearHopfield
convergesasthememorysizeincreases.
6https://github.com/lucidrains/performer-pytorch
7https://github.com/allenai/longformer
8https://pytorch.org/docs/stable/generated/torch.sparse.sampled_addmm.html#torch.sparse.sampled_addmm
9https://github.com/facebookresearch/fvcore
52References
Andreas Auer, Martin Gauch, Daniel Klotz, and Sepp Hochreiter. Conformal prediction for time
series with modern hopfield networks. Advances in Neural Information Processing Systems,
36,2024. URLhttps://arxiv.org/abs/2303.12783.
Mariette Awad, Rahul Khanna, Mariette Awad, and Rahul Khanna. Support vector regres-
sion. Efficient learning machines: Theories, concepts, and applications for engineers
and system designers, pages 67–80, 2015. URL https://link.springer.com/chapter/10.1007/
978-1-4302-5990-9_4.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
arXivpreprintarXiv:2004.05150,2020. URLhttps://arxiv.org/abs/2004.05150.
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the op-
portunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. URL
https://arxiv.org/abs/2108.07258.
Johannes Brandstetter. Blog post: Hopfield networks is all you need, 2021. URL https://ml-jku.
github.io/hopfield-layers/. Accessed: April4,2023.
Johann S Brauchart, Alexander B Reznikov, Edward B Saff, Ian H Sloan, Yu Guang Wang, and
Robert S Womersley. Random point sets on the sphere-hole radii, covering, and separation.
ExperimentalMathematics,27(1):62–81,2018. URLhttps://arxiv.org/abs/1512.07470.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural information processing systems,
33:1877–1901, 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/hash/
1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_medium=email&utm_source=
transaction.
Marc-André Carbonneau, Veronika Cheplygina, Eric Granger, and Ghyslain Gagnon. Multiple
instance learning: A survey of problem characteristics and applications. Pattern Recognition,
77:329–353,2018.
Degang Chen, Qiang He, and Xizhao Wang. The infinite polynomial kernel for support vector
machine. In Advanced Data Mining and Applications: First International Conference, ADMA
2005, Wuhan, China, July 22-24, 2005. Proceedings 1, pages 267–275. Springer, 2005. URL
https://link.springer.com/chapter/10.1007/11527503_32.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with
sparse transformers. arXiv preprint arXiv:1904.10509, 2019. URL https://arxiv.org/abs/1904.
10509.
53Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea
Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser,
David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with
performers. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=Ua6zuk0WRH.
Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015. URL
https://arxiv.org/abs/1511.07289.
GonçaloMCorreia,VladNiculae,andAndréFTMartins. Adaptivelysparsetransformers. arXiv
preprintarXiv:1909.00015,2019. URLhttps://arxiv.org/abs/1909.00015.
Mete Demircigil, Judith Heusel, Matthias Löwe, Sven Upgang, and Franck Vermet. On a model
ofassociativememorywithhugestoragecapacity. JournalofStatisticalPhysics,168:288–299,
2017. URLhttps://link.springer.com/article/10.1007/s10955-017-1806-y.
Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the performance
edgeoverlinearattention. arXivpreprintarXiv:2310.11685,2023.
Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences.
Minds and Machines, 30:681–694, 2020. URL https://link.springer.com/article/10.1007/
s11023-020-09548-1.
AndreasFürst,ElisabethRumetshofer,JohannesLehner,VietTTran,FeiTang,HubertRamsauer,
David Kreil, Michael Kopp, Günter Klambauer, Angela Bitto, et al. Cloob: Modern hopfield
networks with infoloob outperform clip. Advances in neural information processing systems,
35:20450–20468,2022. URLhttps://arxiv.org/abs/2110.11316.
Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient
transformers via top-k attention. arXiv preprint arXiv:2106.06899, 2021. URL https://arxiv.
org/abs/2106.06899.
Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik Strobelt, Duen Horng
Chau, Mohammed J Zaki, and Dmitry Krotov. Energy transformer. arXiv preprint
arXiv:2302.07253,2023. URLhttps://arxiv.org/abs/2302.07253.
John J Hopfield. Neural networks and physical systems with emergent collective computational
abilities. Proceedings of the national academy of sciences, 79(8):2554–2558, 1982. URL
https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554.
John J Hopfield. Neurons with graded response have collective computational properties like
those of two-state neurons. Proceedings of the national academy of sciences, 81(10):3088–
3092,1984. URLhttps://www.pnas.org/doi/abs/10.1073/pnas.81.10.3088.
54Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han Liu. On
sparsemodernhopfieldmodel. InThirty-seventhConferenceonNeuralInformationProcessing
Systems,2023. URLhttps://arxiv.org/abs/2309.12673.
Jerry Yao-Chieh Hu, Pei-Hsuan Chang, Robin Luo, Hong-Yu Chen, Weijian Li, Wei-Po Wang,
andHanLiu. Outlier-efficienthopfieldlayersforlargetransformer-basedmodels. 2024a.
Jerry Yao-Chieh Hu, Thomas Lin, Zhao Song, and Han Liu. On computational limits of modern
hopfieldmodels: Afine-grainedcomplexityanalysis. arXivpreprintarXiv:2402.04520,2024b.
MaximilianIlse,JakubTomczak,andMaxWelling. Attention-baseddeepmultipleinstancelearn-
ing. In International conference on machine learning, pages 2127–2136. PMLR, 2018. URL
https://arxiv.org/abs/1802.04712.
Martin Jaggi. An equivalence between the lasso and support vector machines. Regularization,
optimization, kernels, and support vector machines, pages 1–26, 2014. URL https://arxiv.org/
abs/1303.1152.
Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. Dnabert: pre-trained bidirectional
encoderrepresentationsfromtransformersmodelfordna-languageingenome. Bioinformatics,
37(15):2112–2120, 2021. URL https://academic.oup.com/bioinformatics/article/37/15/2112/
6128680?login=false.
Melih Kandemir, Chong Zhang, and Fred A Hamprecht. Empowering multiple instance
histopathology cancer diagnosis by cell graphs. In Medical Image Computing and Computer-
Assisted Intervention–MICCAI 2014: 17th International Conference, Boston, MA, USA,
September 14-18, 2014, Proceedings, Part II 17, pages 228–235. Springer, 2014. URL
https://link.springer.com/chapter/10.1007/978-3-319-10470-6_29.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In International conference on
machine learning, pages 5156–5165. PMLR, 2020. URL https://proceedings.mlr.press/v119/
katharopoulos20a.html.
Feyza Duman Keles, Pruthuvi Mahesakya Wijewardena, and Chinmay Hegde. On the compu-
tational complexity of self-attention. In International Conference on Algorithmic Learning
Theory,pages597–619.PMLR,2023.
Leo Kozachkov, Ksenia V Kastanenka, and Dmitry Krotov. Building transformers from neurons
and astrocytes. bioRxiv, pages 2022–10, 2022. URL https://www.pnas.org/doi/abs/10.1073/
pnas.2219150120.
DmitryKrotovandJohnJHopfield. Denseassociativememoryforpatternrecognition. Advances
inneuralinformationprocessingsystems,29,2016. URLhttps://proceedings.neurips.cc/paper_
files/paper/2016/hash/eaae339c4d89fc102edd9dbdb6a28915-Abstract.html.
55Dmitry Krotov and John J. Hopfield. Large associative memory problem in neurobiology and
machinelearning. InInternationalConferenceonLearningRepresentations,2021. URLhttps:
//arxiv.org/abs/2008.06996.
Andre Martins and Ramon Astudillo. From softmax to sparsemax: A sparse model of attention
and multi-label classification. In International conference on machine learning, pages 1614–
1623.PMLR,2016. URLhttps://arxiv.org/abs/1602.02068.
Andre F. T. Martins, Vlad Niculae, and Daniel McNamee. Sparse modern hopfield networks.
Associative Memory & Hopfield Networks in 2023. NeurIPS 2023 workshop., 2023. URL
https://openreview.net/pdf?id=zwqlV7HoaT.
Frank WJ Olver, Daniel W Lozier, Ronald F Boisvert, and Charles W Clark. NIST handbook
of mathematical functions hardback and CD-ROM. Cambridge university press, 2010. URL
https://dlmf.nist.gov/.
FabianPaischer,ThomasAdler,VihangPatil,AngelaBitto-Nemling,MarkusHolzleitner,Sebas-
tian Lehner, Hamid Eghbal-Zadeh, and Sepp Hochreiter. History compression via language
models in reinforcement learning. In International Conference on Machine Learning, pages
17156–17185.PMLR,2022. URLhttps://proceedings.mlr.press/v162/paischer22a.html.
Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise
self-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019. URL
https://arxiv.org/abs/1911.02972.
Hubert Ramsauer, Bernhard Schafl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas
Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, et al. Hopfield
networks is all you need. arXiv preprint arXiv:2008.02217, 2020. URL https://arxiv.org/abs/
2008.02217.
Johannes Schimunek, Philipp Seidl, Lukas Friedrich, Daniel Kuhn, Friedrich Rippmann, Sepp
Hochreiter, and Günter Klambauer. Context-enriched molecule representations improve few-
shot drug discovery. In The Eleventh International Conference on Learning Representations,
2023. URLhttps://openreview.net/forum?id=XrMWUuEevr.
PhilippSeidl,PhilippRenz,NataliaDyubankova,PauloNeves,JonasVerhoeven,JorgKWegner,
MarwinSegler,SeppHochreiter,andGunterKlambauer. Improvingfew-andzero-shotreaction
templatepredictionusingmodernhopfieldnetworks.Journalofchemicalinformationandmod-
eling,62(9):2111–2120,2022. URLhttps://pubs.acs.org/doi/full/10.1021/acs.jcim.1c01065.
Bharath K Sriperumbudur and Gert RG Lanckriet. On the convergence of the concave-
convex procedure. In Advances in neural information processing systems, vol-
ume 9, pages 1759–1767, 2009. URL https://papers.nips.cc/paper_files/paper/2009/file/
8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf.
56Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.
ACMComputingSurveys,55(6):1–28,2022. URLhttps://dl.acm.org/doi/10.1145/3530811.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
tionprocessingsystems,30,2017. URLhttps://proceedings.neurips.cc/paper_files/paper/2017/
hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
MartinJWainwright. High-dimensionalstatistics: Anon-asymptoticviewpoint,volume48. Cam-
bridgeuniversitypress,2019.
Michael Widrich, Bernhard Schäfl, Milena Pavlovic´, Hubert Ramsauer, Lukas Gruber, Markus
Holzleitner, Johannes Brandstetter, Geir Kjetil Sandve, Victor Greiff, Sepp Hochreiter, et al.
Modernhopfieldnetworksandattentionforimmunerepertoireclassification. AdvancesinNeu-
ralInformationProcessingSystems,33:18832–18845,2020. URLhttps://proceedings.neurips.
cc/paper/2020/hash/da4902cb0bc38210839714ebdcf0efc3-Abstract.html.
Dennis Wu, Jerry Yao-Chieh Hu, Teng-Yun Hsiao, and Han Liu. Uniform memory retrieval with
largercapacityformodernhopfieldmodels. 2024a.
Dennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu. STanhop: Sparse tan-
demhopfieldmodelformemory-enhancedtimeseriesprediction. InTheTwelfthInternational
ConferenceonLearningRepresentations,2024b. URLhttps://arxiv.org/abs/2312.17346.
ShijieWu,Ozan Irsoy, StevenLu,VadimDabravolski,Mark Dredze,SebastianGehrmann,Prab-
hanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language
model for finance. arXiv preprint arXiv:2303.17564, 2023. URL https://arxiv.org/abs/2303.
17564.
ChenweiXu,Yu-ChaoHuang,JerryYao-ChiehHu,WeijianLi,AmmarGilani,Hsi-ShengGoan,
and Han Liu. Bishop: Bi-directional cellular learning for tabular data with generalized sparse
modernhopfieldmodel. 2024.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,
Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big
bird: Transformers for longer sequences. Advances in neural information process-
ing systems, 33:17283–17297, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
c8512d142a2d849725f31a9a7a361ab9-Abstract.html.
Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana V Davuluri, and Han Liu.
DNABERT-2: Efficient foundation model and benchmark for multi-species genomes. In
The Twelfth International Conference on Learning Representations, 2024a. URL https:
//openreview.net/forum?id=oMLQB4EZE1.
57Zhihan Zhou, Winmin Wu, Harrison Ho, Jiayi Wang, Lizhen Shi, Ramana V Davuluri, Zhong
Wang, and Han Liu. Dnabert-s: Learning species-aware dna embedding with genome founda-
tionmodels. arXivpreprintarXiv:2402.08777,2024b. URLhttps://arxiv.org/abs/2402.08777.
58