Dynamic Conditional Optimal Transport through
Simulation-Free Flows
Gavin Kerrigan gavin.k@uci.edu
Department of Computer Science
University of California, Irvine
Giosue Migliorini gmiglior@uci.edu
Department of Statistics
University of California, Irvine
Padhraic Smyth smyth@ics.uci.edu
Department of Computer Science
University of California, Irvine
Abstract
We study the geometry of conditional optimal transport (COT) and prove a dynamical
formulationwhichgeneralizestheBenamou-BrenierTheorem. Withthesetools,weproposea
simulation-free flow-based method for conditional generative modeling. Our method couples
an arbitrary source distribution to a specified target distribution through a triangular COT
plan. We build on the framework of flow matching to train a conditional generative model
by approximating the geodesic path of measures induced by this COT plan. Our theory
and methods are applicable in the infinite-dimensional setting, making them well suited for
inverse problems. Empirically, we demonstrate our proposed method on two image-to-image
translation tasks and an infinite-dimensional Bayesian inverse problem.
1 Introduction
Many fundamental tasks in machine learning and statistics may be posed as estimating or
sampling from a distribution, where one only has access to the distribution via a finite set of
observations. Such tasks range from generative modeling and density estimation to Bayesian
inference. A broad range of methods for sampling from an intractable distribution p may be
seen through the lens of measure transport. Intuitively, one fixes a reference distribution
q which is easy to sample from, and samples z ∼ q are transformed by a learned mapping
T such that T(z) is approximately distributed as p. The distribution obtained in this way
from q, denoted T q, is the pushforward of q along T.
#
Generative models (Ho et al., 2020; Song et al., 2020; Kingma and Welling, 2013;
Goodfellow et al., 2014; Papamakarios et al., 2021) typically find such a mapping T by
minimizing a divergence between the target and generated distribution. In the standard
maximum-likelihood based setting, the mapping T is learned by minimizing (an upper bound
on) the KL divergence between p and the pushforward T q. However, a natural alternative
#
approach, based on optimal transport (Villani et al., 2009; Santambrogio, 2015), is to search
for a map T which satisfies the constraint T q = p and has minimal cost (in some sense).
#
In many scenarios, we are interested in conditional sampling. That is, we have access
to some observation y ∈ Y, and from this information we would like to infer information
4202
rpA
5
]GL.sc[
1v04240.4042:viXraKerrigan, Migliorini, Smyth
regarding an unknown quantity u ∈ U. For instance, in Bayesian inference, y denotes the
data and u the parameters of a model. A joint distribution p(y,u) = p(y | u)p(u) is specified
via a likelihood p(y | u) and a prior p(u), and one would like to draw samples from the
posterior p(u | y). However, in applications such as Bayesian inverse problems (Dashti and
Stuart, 2013), one is interested in approximating p(u | y) for many different observations
y. Moreover, in applications such as simulation-based inference (Cranmer et al., 2020), the
likelihood function may be intractable while we are still able to sample from p(y,u). This
motivates an amortized and likelihood-free (Amos et al., 2023) approach, where a single
model may be trained and applied across different observations.
In the framework of measure transport, we first specify a source distribution q(u).
For any given, fixed y ∈ Y, we seek to find a mapping T(y), depending on y, such that
T(y) q(u) ≈ p(u | y). Finding such a mapping for each individual y via optimal transport,
#
though, is typically intractable, as we frequently do not have samples from the posterior
p(u | y). As such, we must leverage information from other observations. One way to achieve
this is through triangular mappings (Baptista et al., 2020), where a joint source distribution
q(y,u) is transformed by a mapping of the form T : (y,u) (cid:55)→ (T (y),T (y,u)). It can be
Y U
shown (see Section 3.2) that if T q = p, then T (y,−) almost surely couples q(u | y) and
# U
p(u | y). Thus, if we are able to find such a triangular T using samples from the joint
distribution p(y,u), then we may obtain conditional samplers.
In this work, we develop a dynamical framework for conditional optimal transport (COT).
In particular, we study absolutely curves of measures in a conditional Wasserstein space over
Y ×U. Ourmainresults(Theorem14andTheorem15)showthatsuchcurvesare, informally
stated, generated by the flow induced by a triangular vector field. Intuitively, a triangular
vector field on Y ×U has no velocity in the Y component and thus induces a flow for each
conditional measure. As a consequence, in Theorem 16 we obtain a conditional version of
the well-known Benamou-Brenier theorem (Benamou and Brenier, 2000). Importantly, our
framework is applicable in infinite-dimensional spaces, making it well suited for function
space inference problems.
We apply our results on dynamical COT to design conditional generative models, based
on the flow matching framework (Lipman et al., 2022; Albergo et al., 2023a) . In particular,
we generalize the methods of Pooladian et al. (2023a) and Tong et al. (2023), which utilize
optimal transport to pair source and target samples, to the conditional setting. Our method
works by solving a relaxed optimal transport problem (Hosseini et al., 2023) during training.
In this way, we amortize solutions to the dynamic COT problem across arbitrary y. Our
method may use arbitrary source distributions, and is applicable across a wide range of
conditional generative modeling tasks.
Specifically, the main contributions of our work are as follows.
1. In Section 4, we study the conditional Wasserstein space Pµ(Y ×U) equipped with
p
the conditional Wasserstein distance Wµ(Y ×U). We show that this space is a metric
p
space, and moreover, admits constant speed geodesics between any two points.
2. In Section 5, we characterize the absolutely continuous curves of measures in Pµ(Y ×U)
p
via the continuity equation and triangular vector fields. As a corollary, we obtain a
conditional version of the Benamou-Brenier theorem (Benamou and Brenier, 2000).
2Dynamic Conditional Optimal Transport
3. In Section 6 we propose a method for learning dynamic COT maps based on flow
matching. In Section 7, we demonstrate our method on two image-to-image translation
tasks, as well as an infinite-dimensional Bayesian inverse problem.
2 Related Work
Optimal Transport. Machine learning based approaches to unconditional optimal trans-
port plans typically focus on the static problem (Makkuva et al., 2020; Korotin et al., 2022;
Taghvaei and Jalali, 2019). Approaches to learning optimal transport maps in a dynamic
fashion often require restricted architectures (Bunne et al., 2022b) or use likelihood-based
approaches (Tong et al., 2020; Finlay et al., 2020; Onken et al., 2021; Huguet et al., 2022),
which require simulating from the model during training. While most work focuses on
the squared norm cost, the recent approaches of Pooladian et al. (2023b) and Neklyudov
et al. (2023) study dynamical optimal transport with costs specified by a Lagrangian. These
frameworks, however, typically have no clear interpretation as a static problem.
Conditionaloptimaltransport(COT),ontheotherhand, isrelativelyunexplored. Bunne
et al. (2022a) and Wang et al. (2023) learn supervised COT maps in a static setting via
convex networks (Makkuva et al., 2020). Hosseini et al. (2023) recently develop a framework
for conditional optimal transport that is applicable in infinite-dimensional spaces. This
builds on the work of Carlier et al. (2016) which applies COT for vector quantile regression.
A common approach to COT is to employ triangular plans (Baptista et al., 2020). Our work
develops a dynamic formulation of the static COT problem (Hosseini et al., 2023; Carlier
et al., 2016) using a notion of triangular vector fields. Additionally, Wang et al. (2023)
propose a likelihood-based approach using continuous normalizing flows, but do not develop
the theory of dynamical COT as we do here.
Simulation-Free Continuous Normalizing Flows. Flow matching (Lipman et al.,
2022) and the equivalent stochastic interpolant framework (Albergo and Vanden-Eijnden,
2022; Albergo et al., 2023a) are a class of methods for building continuous-time normalizing
flows in a simulation-free manner. These methods transform an arbitrary source distribution
into a target distribution via a specified path of measures, which is modeled through a
learned vector field. These methods have been extended to setting of Riemannian manifolds
(Chen and Lipman, 2023) and infinite-dimensional spaces (Kerrigan et al., 2023).
Notably, these works do not approximate an optimal transport between the source and
target measures and instead use an independent coupling, although stochastic interpolants
(Albergo and Vanden-Eijnden, 2022; Albergo et al., 2023b) may be rectified (Liu et al., 2022;
Lee et al., 2023) through an iterative optimization procedure. Importantly, the paths in flow
matching (Lipman et al., 2022) maybe taken to be conditionally optimal, but marginally
they are not. Pooladian et al. (2023a) and Tong et al. (2023) propose instead to couple the
source and target distributions via optimal transport.
While some works (Lipman et al., 2022; Davtyan et al., 2023; Gebhard et al., 2023) have
studied flow matching approaches for conditional distributions, these approaches simply
provide additional conditioning information to the model and do not study COT. Concurrent
work (Isobe et al., 2024) proposes extended flow matching, which learns a matrix field to
perform conditional generation. However, the results of Isobe et al. (2024) are limited to the
finite-dimensional setting and do not develop the theory of COT.
3Kerrigan, Migliorini, Smyth
Atthetimeofoursubmission,concurrentworkbyBarbonietal.(2024)andChemseddine
et al. (2024) appeared with similar results. Our results were developed independently from
these works.
3 Background and Notation
Let X,X′,X′′ represent arbitrary separable Hilbert spaces, equipped with the Borel σ-
algebra. We use P(X) to represent the space of Borel probability measures on X, and
P (X) ⊆ P(X) to represent the subspace of measures having finite pth moment. If η ∈ P(X)
p
is a probability measure on X and T : X → X′ is measurable, then the pushforward measure
T η(−) = η(T−1(−)) is a probability measure on X′. Maps of the form πX : X ×X′ → X
#
represent the canonical projection onto the respective space. We will also use maps such as
π1 : X×X′×X′′ → X and π1,2 : X×X′×X′′ → X×X′ to denote projection onto specific
coordinates. The set C (X) represents the space of continuous and bounded functions and
b
C∞(X) is the space of smooth and compactly supported functions.
c
In our specific case, we assume that we have two separable Hilbert spaces of interest.
The first, Y, is a space of observations, and the second, U, is a space of unknowns. These
spaces may be of infinite dimensions, but a special case of practical interest is when Y and U
are finite dimensional Euclidean spaces. We will consider the product space Y ×U, equipped
with the canonical inner product obtained via the sum of the inner products on Y and U,
under which the space Y ×U is also a separable Hilbert space.
Let η ∈ P(Y × U) be a joint measure. The measures πYη ∈ P(Y) and πUη ∈ P(U)
# #
obtained via projection are the marginals of η. We use ηy ∈ P(U) to represent the measure
obtained by conditioning η on the value y ∈ Y. By the disintegration theorem, such
conditional measures exist and are essentially unique, in the sense that there exists a Borel
set E ⊆ Y with πYη(E) = 0, and ηy are unique for y ∈/ E.
#
3.1 Unconditional Optimal Transport
We provide here a brief overview of optimal transport in the standard unconditional setting.
For more details, we refer to the standard references of Villani et al. (2009); Santambrogio
(2015); Ambrosio et al. (2005). Fix a cost function c : X ×X → R∪{+∞}. Suppose we
have two Borel measures η,ν ∈ P(X). The Monge Problem seeks to find a measurable
transport map T : X → X minimizing the expected cost of transport, i.e. corresponding to
the optimization problem
(cid:26)(cid:90) (cid:27)
inf c(x,T(x))dη(x) | T η = ν . (1)
#
T X
This optimization problem is challenging, though, as it involves a nonlinear constraint
and the set of feasible maps may be empty. In contrast, the Kantorovich problem is a
relaxation which seeks to find an optimal coupling γ ∈ Π(η,ν), i.e. a probability distribution
over X ×X with marginals η,ν, which solves
(cid:26)(cid:90) (cid:27)
inf c(x ,x )dγ(x ,x ) | γ ∈ Π(η,ν) . (2)
0 1 0 1
γ
X×X
4Dynamic Conditional Optimal Transport
Under fairly weak conditions (e.g., the cost is lower semicontinuous and bounded from
below (Ambrosio et al., 2013, Theorem 2.5)), minimizers to the Kantorovich problem are
guaranteed to exist. If the cost function is c(x ,x ) = |x −x |p for some 1 < p < ∞, under
0 1 0 1
sufficient regularity conditions on η a solution T⋆ to the Monge problem is guaranteed to
exist and, moreover, the coupling γ⋆ = (I,T⋆) η is optimal for the Kantorovich problem.
#
See Ambrosio et al. (2013, Chapter 2) and Ambrosio et al. (2005, Theorem 6.2.10).
Wasserstein Space. In the special case that c(x ,x ) = |x −x |p for 1 ≤ p < ∞, and
0 1 0 1
η,ν ∈ P (X), the Kantorovich problem admits a finite-cost solution. The cost of such an
p
optimal coupling is the p-Wasserstein distance
(cid:26)(cid:90) (cid:27)
Wp(η,ν) = min |x −x |pdγ(x ,x ) | γ ∈ Π(η,ν) (3)
p 0 1 0 1
γ
X×X
which, as the name suggests, is a metric on the space P (X) (Ambrosio et al., 2005,
p
Section 7.1) (Santambrogio, 2015, Section 5.1). The Wasserstein distance admits a dynamical
formulation via the Benamou-Brenier theorem (Benamou and Brenier, 2000). Namely, the
p-Wasserstein distance can be obtained by finding a time-dependent vector field transforming
η to ν across time t ∈ [0,1] with minimal energy:
(cid:26)(cid:90) 1(cid:90) (cid:27)
W (η,ν) = min |v (x)|pdγ (x)dt | γ = η,γ = ν,∂ γ +div(v γ ) = 0 . (4)
p t t 0 1 t t t t
(γt,vt) 0 X
Here, we constrain our minimization problem over the set of measures and vector
fields (γ ,v ) interpolating between η and ν, satisfying a continuity equation (see Section
t t
5.1). In Section 4, we study a generalization of the Wasserstein distances for conditional
optimal transport problems. In particular, Theorem 16 provides a generalization of the
Benamou-BreniertheoremtotheconditionalsettingwhichrecoversaconditionalWasserstein
distance.
3.2 Static Conditional Optimal Transport
In the unconditional setting, one seeks an optimal coupling between two given measures.
In contrast, in the conditional setting, we would like to find a family of couplings that can
transport a given source measure into any conditional measure of the target distribution. In
other words, given a target measure ν ∈ P(Y ×U) and some source measure η ∈ P(U), one
seeks a transport map T : Y ×U → U such that
T (y,−) η = νy ∀y ∈ Y. (5)
# #
If such a map were available, by drawing samples u ∼ η and transforming them, one
0
would obtain samples T(y,u) ∼ νy. Solving this transport problem for each fixed y is
expensive at best, or impossible when only has a single (or no) samples (y,u) ∼ ν for any
given y.
Thus, one must leverage information across different observations y. To that end, recent
work has focused on the notion of triangular mappings T : Y ×U → Y ×U (Hosseini et al.,
2023; Baptista et al., 2020) (Bogachev and Ruas, 2007, Section 10.10) of the form
T(y,u) = (T (y),T (T (y),u)) (6)
Y U Y
5Kerrigan, Migliorini, Smyth
for some T : Y → Y and T : Y ×U → U. Triangular mappings are of interest as they
Y U
allow us to obtain conditional couplings from joint couplings.
Proposition 1 (Theorem 2.4 (Baptista et al., 2020), Prop. 2.3 (Hosseini et al., 2023)).
Suppose η,ν ∈ P(Y × U) and T : Y × U → Y × U is triangular. If T η = ν, then
#
T U(T Y(y),−) #ηy = νTY(y) for π #Yη-almost every y.
Inmanyscenariosofpracticalinterest,thesourcemeasureηandthetargetmeasureν have
the same Y-marginals. We will henceforth make this assumption, and use µ = πYη = πYν
# #
to represent this marginal. In this case, we may take T to be the identity mapping, so that
Y
the conclusion of Proposition 1 simplifies to T (y,−) ηy = νy for µ-almost every y. We
U #
note that in situations where such an assumption does not hold, one may simply preprocess
the source measure η via an invertible mapping T satisfying [T ] [πYη] = πYν (Hosseini
Y Y # # #
et al., 2023, Prop 3.2).
Definition 2 (Conditional Wasserstein Space).
Suppose µ ∈ P(Y) is a given probability measure over Y and 1 ≤ p < ∞. The set
Pµ(Y ×U) = (cid:8) γ ∈ P (Y ×U) | πYγ = µ(cid:9) (7)
p #
is the space of joint measures on Y ×U having fixed Y-marginals µ. The set Pµ(Y ×U) :=
p
Pµ(Y × U) ∩ P (Y × U) is the conditional p-Wasserstein space, consisting of joint
p
measures with finite pth moments and Y-marginals µ.
Givenasourceandtargetmeasuresη,ν ∈ Pµ(Y ×U), theconditional Monge problem
seeks to find a triangular mapping solving
(cid:26)(cid:90) (cid:27)
inf c(y,u,T(y,u))dη(y,u) | T η = ν,T : (y,u) (cid:55)→ (y,T (y,u)) . (8)
# U
T Y×U
The conditional Monge problem also admits a relaxation under which one only considers
couplings whose Y-components are almost surely equal. To that end, we consider the subset
C ⊂ (Y ×U)2 whose Y components are identical, i.e.,
C := (cid:8) (y ,u ,y ,u ) ∈ (Y ×U)2 | y = y (cid:9) (9)
0 0 1 1 0 1
and we define the set of (Y)-restricted probability measures R ⊂
P(cid:0)
(Y
×U)2(cid:1)
Y
such that every γ ∈ R is concentrated on C. In other words, if γ ∈ R , then samples
Y Y
(y ,u ,y ,u ) ∼ γ have y = y almost surely. In addition, for any η,ν ∈ P(Y ×U), we
0 0 1 1 0 1
define the set of (Y)-restricted couplings Π (η,ν) to be the probability measures in R
Y Y
whose marginals are η and ν, i.e.
(cid:110) (cid:111)
Π (η,ν) = γ ∈ R | π1,2γ = η,π3,4γ = ν . (10)
Y Y # #
The conditional Kantorovich problem seeks a triangular coupling γ⋆ solving
(cid:40) (cid:41)
(cid:90)
inf c(y ,u ,y ,u )dγ(y ,u ,y ,u ) | γ ∈ Π (η,ν) . (11)
0 0 1 1 0 0 1 1 Y
γ (Y×U)2
6Dynamic Conditional Optimal Transport
Hosseini et al. (2023) prove the existence of minimizers to the conditional Kantorovich
and Monge problems under very general assumptions. Moreover, optimal couplings to the
conditional Kantorovich problem induce optimal couplings for µ-almost every conditional
measure. Assuming sufficient regularity assumptions on the conditional measures, unique
solutions to the conditional Monge problem exist.
Proposition 3 (Prop 3.3 (Hosseini et al., 2023)).
Fix η,ν ∈ Pµ(Y ×U). Suppose the cost function c is continuous, infc > −∞, and there exists
a finite cost coupling γ ∈ Π (η,ν). Then, the conditional Kantorovich problem admits a
Y
minimizer γ⋆. Moreover, γ⋆,y0(y 1,u 0,u 1) = γˆ⋆,y0(u 0,u 1)δ(y 1−y 0) where for µ-almost every
y the measure γ⋆,y is an optimal coupling for ηy,νy under the cost cy(u ,u ) = c(y,u ,y,u )
0 1 0 1
Proposition 4 (Prop 3.8 (Hosseini et al., 2023)).
Fix 1 < p < ∞ and η,ν ∈ Pµ(Y ×U). Suppose c(y ,u ,y ,u ) = |u −u |p. If ηy assign zero
p 0 0 1 1 0 1
measure to Gaussian null sets for µ-almost every y, then there is a unique solution T⋆ to
the conditional Monge problem, and γ⋆ = (I,T⋆) η is the unique solution to the conditional
#
Kantorovich problem. If νy also assign zero measure to Gaussian null sets for µ-almost
every y, then T⋆ is injective η-almost everywhere.
4 Conditional Wasserstein Space
We now introduce a metric on the space Pµ(Y ×U). Intuitively, the conditional Wasserstein
p
distance measures the usual Wasserstein distance between all of the conditional distributions
in expectation under the fixed Y-marginal µ.
Definition 5 (Conditional p-Wasserstein Distance).
Suppose η,ν ∈ Pµ(Y ×U) and 1 ≤ p < ∞. The function
p
(cid:18)(cid:90) (cid:19)1/p
Wµ(η,ν) = (cid:0)E (cid:2) Wp(ηy,νy)(cid:3)(cid:1)1/p = Wp(ηy,νy)dµ(y) (12)
p y∼µ p p
Y
is the conditional p-Wasserstein distance. Here, Wp : P (U)×P (U) → R is the
p p p
usual p-Wasserstein distance for measures on U.
By an application of Jensen’s inequality we observe that Wµ(η,ν) ≥ E [W (ηy,νy)].
p y∼µ p
For p > 1, this inequality is strict unless W (ηy,νy) is µ-almost surely constant. We
p
emphasize that Definition 5 is purely formal at this stage. We do not know if this integral is
well-defined, or if it actually defines a metric. Before proceeding, we first note that Wµ(η,ν)
p
may be viewed as the minimal value of the constrained Kantorovich problem in Equation
(11) when one takes the cost to be the metric on the space Y ×U. Although Hosseini et al.
(2023) studied the existence of minimizers to this problem, they did not study the metric
properties of the resulting space.
7Kerrigan, Migliorini, Smyth
Proposition 6 (Equivalent Formulation of the p-Wasserstein Distance).
Fix η,ν ∈ Pµ(Y ×U) and 1 ≤ p < ∞. Then, Wµ(η,ν) is well-defined, finite, and
p p
(cid:40) (cid:41)
(cid:90)
Wµ,p(η,ν) = min dp(y ,u ,y ,u )dγ | γ ∈ Π (η,ν) (13)
p 0 0 1 1 Y
γ (Y×U)2
where Wµ,p(η,ν) represents the p-th power of the conditional p-Wasserstein distance.
p
Proof. The cost function dp is clearly continuous and non-negative, and hence by Proposition
3 it suffices to exhibit a finite-cost coupling γ ∈ Π (η,ν) between η and ν. Indeed, take the
Y
conditionally independent coupling
γ(y ,u ,y ,u ) = η(u | y )ν(u | y )δ(y −y )µ(y ) (14)
0 0 1 1 0 1 1 1 1 0 1
which is clearly in Π (η,ν). We then have that
Y
(cid:90) (cid:90)
dp(y ,u ,y ,u )dγ(y ,u ,y ,u ) = ∥(y ,u )−(y ,u )∥p dγ(y ,u ,y ,u )
0 0 1 1 0 0 1 1 0 0 1 1 Y×U 0 0 1 1
(Y×U)2 (Y×U)2
(cid:90)
≤ 2p (cid:0) ∥(y ,u )∥p +∥(y ,u )∥p (cid:1) dγ(y ,u ,y ,u )
0 0 Y×U 1 1 Y×U 0 0 1 1
(Y×U)2
(cid:18)(cid:90) (cid:90) (cid:19)
= 2p ∥(y ,u )∥p dη(y ,u )+ ∥(y ,u )∥p dν(y ,u ) < +∞.
0 0 Y×U 0 0 1 1 Y×U 1 1
Y×U Y×U
Hence,Equation(13)admitsaminimizerγ⋆ ∈ Π (η,ν). ByProposition3,thisminimizer
Y
may be taken to have the form γ⋆ = γ⋆,y1(u 0,u 1)δ(y 1−y 0)µ(y 1) where γ⋆,y1(u 0,u 1) is µ(y 1)-
almost surely an optimal coupling between ηy1,νy1 for the cost |u 1−u 0|p. Thus,
(cid:90) (cid:90) (cid:90)
dpdγ⋆ = |u −u |pdγ⋆,y(u ,u )dµ(y) (15)
1 0 0 1
(Y×U)2 Y U2
(cid:90)
= Wp(ηy,νy)dµ(y) = Wp,µ(η,ν). (16)
p p
Y
Here, we emphasize that the µ-almost sure uniqueness of the disintegrations of η,ν along
Y result in a well-defined expression.
Moreover, if η ∈ Pµ(Y ×U) it follows that ηy ∈ P (U) for µ-a.e. y, because
p p
(cid:90) (cid:90) (cid:90) (cid:90)
|u|pdηy(u)dµ(y) ≤ |(y,u)|pdηy(u)dµ(y) (17)
Y U Y U
(cid:90)
= |(y,u)|pdη(y,u) < +∞. (18)
Y×U
Thus all considered p-Wasserstein distances on U are finite.
We now show that Wµ is a metric on Pµ(Y ×U).
p p
Proposition 7 (Wµ is a Metric).
p
For 1 ≤ p < ∞, the conditional p-Wasserstein distance Wµ is a metric on Pµ(Y ×U).
p p
8Dynamic Conditional Optimal Transport
Proof. Fix η,ν,ρ ∈ Pµ(Y ×U). Since W is a metric on P (U), we immediately obtain the
p p p
symmetry of Wµ. Moreover, we have that Wµ(η,ν) = 0 if and only if ηy = νy for µ-almost
p p
every y. Thus, if Wµ(η,ν) = 0 and E ⊆ Y ×U is Borel measurable,
p
(cid:90) (cid:90)
η(E) = ηy(Ey)dµ(y) = νy(Ey)dµ(y) = ν(E). (19)
Y Y
which shows that η = ν. Here, Ey = {u | (y,u) ∈ E} is the y-slice of E. Conversely, if
η = ν, then ηy = νy up to a µ-null set by the essential uniqueness of disintegrations. Thus,
Wµ(η,ν) = 0 if and only if η = ν.
p
By Minkowski’s inequality and the triangle inequality for W on P (U), we see
p p
Wµ(η,ν) ≤ (E [(W (ηy,ρy)+W (ρy,νy))p])1/p (20)
p y∼µ p p
≤ E [Wp(ηy,ρy)]1/p+E [Wp(ρy,νy)]1/p (21)
y∼µ p y∼µ p
= Wµ(η,ρ)+Wµ(ρ,ν). (22)
p p
4.1 Metric Properties of Conditional Wasserstein Distances
We now briefly explore some properties of the space Pµ(Y ×U) equipped with the conditional
p
WassersteindistanceWµ. Inparticular,weobservethatthetopologyonPµ(Y ×U)generated
p p
by Wµ is stronger than that generated by W . Note that we easily see Wµ(η,ν) ≥ W (η,ν)
p p p p
foreveryη,ν ∈ Pµ(Y ×U)bythecharacterizationinProposition6. Theconverseis, however,
p
false, and the metrics are inequivalent.
Proposition 8 (Wµ is stronger than W ).
p p
There does not in general exist a constant C > 0 such that Wµ(η,ν) ≤ CW (η,ν) for all
p p
η,ν ∈ Pµ(Y ×U).
p
Proof. We provide a counterexample. Fix any u ̸= 0 ∈ U and y ,y ∈ Y such that y ̸= y .
0 0 1 0 1
Define µ = 1 (δ +δ ). Set u = (k +1)u for k = 1,2,... and for each k, define two
2 y0 y1 k 0
measures on Y ×U by
1 1
η = (δ +δ ) ν = (δ +δ ). (23)
k 2 y0u0 y1u k k 2 y1u0 u ky0
It is clear that
Wµ,p(η ,ν ) = kp|u |p Wp(η ,ν ) = min{kp|u |p,|y −y |p}. (24)
p k k 0 p k k 0 1 0
Moreover, as k → ∞ we have Wµ(µ ,ν ) → ∞ but Wp(ν ,η ) remains bounded.
p k k p k k
In addition, we can show that the conditional Wasserstein distance dominates the
Wasserstein distance on the U marginals.
9Kerrigan, Migliorini, Smyth
Proposition 9 (Conditional Wasserstein Dominates U-Marginal Distance).
For 1 ≤ p < ∞ and η,ν ∈ Pµ(Y ×U), we have
p
W (cid:0) πUη,πUν(cid:1) ≤ Wµ(η,ν). (25)
p # # p
Proof. Let γ⋆(y 0,u 0,y 1,u 1) = γ⋆,y1(u 0,u 1)δ(y 1−y 0)µ(y 1) be an optimal γ⋆ ∈ Π Y(η,ν). We
claim that γ(u ,u ) := (cid:82) γ⋆,y(u ,u )dµ(y) couples πUη and πUν. Let π0 : (u ,u ) (cid:55)→ u
0 1 Y 0 1 # # 0 1 0
be the projection onto the first coordinate of U ×U. Observe that for µ-almost every y,
we have that γ⋆,y ∈ Π(ηy,νy) is optimal, and, in particular, π0γ⋆,y = ηy. Fix an arbitrary
#
φ ∈ C (U). We then have
b
(cid:90) (cid:90)
φ(u )dπ0γ(u ) = (φ◦π0)dγ(u ,u ) (26)
0 # 0 0 1
U U2
(cid:90) (cid:90) (cid:90) (cid:90)
= (φ◦π0)dγ⋆,y(u ,u )dµ(y) = φ(u )dπ0γ⋆,y(u )dµ(y) (27)
0 1 0 # 0
Y U2 Y U
(cid:90) (cid:90) (cid:90)
= φ(u )dηy(u )dµ(y)= φ(u )dη(u ,y) (28)
0 0 0 0
Y U Y×U
(cid:90) (cid:90)
= (φ◦πU)dη(u ,y) = φdπUη(u ). (29)
0 # 0
Y×U U
Thus πUγ = πUη. A similar argument shows that for the map π1 : (u ,u ) (cid:55)→ u we
# # 0 1 1
have π1γ = πUν, so that γ ∈ Π(πUη,πUν).
# # # #
Now, as γ⋆,y1(u 0,u 1) ∈ Π(ηy1,νy1) is µ-almost surely optimal in the usual Wasserstein
sense,
(cid:90) (cid:90)
Wp,µ(η,ν) = |u −u |pdγ⋆,y(u ,u )dµ(y) (30)
p 0 1 0 1
Y U2
(cid:90)
= |u −u |pdγ(u ,u ) (31)
0 1 0 1
U2
≥ Wp(πUη,πUν) (32)
p # #
since γ ∈ Π(πUη,πUν) is a coupling but potentially sub-optimal.
# #
Example: Gaussian Measures. We conclude with an example where the conditional
2-Wasserstein distance may be explicitly computed. In particular, this example shows that
the inequality in Proposition 9 may be strict. Suppose Y and U are Euclidean spaces (of
possibly different dimensions), and that η,ν ∈ Pµ(Y ×U) are Gaussians of the form
p
(cid:18)(cid:20) m(cid:21) (cid:20) Σ Ση (cid:21)(cid:19) (cid:18)(cid:20) m(cid:21) (cid:20) Σ Σν (cid:21)(cid:19)
η = N , 12 ν = N , 12 . (33)
mη Ση Ση mν Σν Σν
u 21 22 u 21 22
It follows that µ = πYη = πYν = N(m,Σ). Because we may obtain ηy,νy in closed
# #
form, we can compute Wµ(η,ν) in closed form. Let
p
Qη = Ση −Ση Σ−1Ση Qν = Σν −Σν Σ−1Σν R = (Ση −Σν )Σ−1 (34)
22 21 12 22 21 12 21 21
10Dynamic Conditional Optimal Transport
Then, we obtain by a brute-force calculation that
(cid:18) (cid:19)
(cid:16) (cid:17)1/2
Wµ,2(η,ν) = |mη −mν|2+Tr Qη +Qν −2 (Qη)1/2Qν(Qη)1/2 +RΣRT . (35)
2 u u
Notethatwhenη,ν haveuncorrelatedY,U components,wepreciselyrecoverW2(πUη,πUν)
2 # #
as one may expect. As a special case of interest, if Y = U = R and
(cid:18) (cid:20) (cid:21)(cid:19)
1 ρ
η = N(0,I) ν = N 0, |ρ| < 1 (36)
ρ 1
then we obtain as a special case of Equation (35) that Wµ,2(η,ν) = 2(1−(cid:112) 1−ρ2).
2
This is zero if and only if ρ = 0, i.e. η = ν. However, πUη = πUν = N(0,1).
# #
4.2 Conditional Wasserstein Space as a Geodesic Space
In this section, we show that Pµ(Y ×U) is a geodesic space. In fact, we show the stronger
p
statementthatthereexistsaconstant speed geodesicbetweenanytwomeasuresinPµ(Y ×U),
p
which is a generalization of known results (Santambrogio, 2015, Theorem 5.27). First, we
introduce some preliminary notions. A curve is a continuous function γ : I → Pµ(Y ×U)
• p
where I = (a,b) ⊆ R is any open interval of finite length. A curve is said to be absolutely
continuous if there exists m ∈ L1((a,b)) such that
(cid:90) t
Wµ(γ ,γ ) ≤ m(τ)dτ ∀a < s ≤ t < b. (37)
p s t
s
If (γ ) is an absolutely continuous curve, then its metric derivative
t
Wµ(γ ,γ )
|γ′|(t) = lim p s t (38)
s→t |s−t|
exists for almost every t ∈ (a,b), and, moreover, we almost surely have |γ′|(t) ≤ m(t)
pointwise for any m satisfying Equation (37) (Ambrosio et al., 2005, Theorem 1.1.2). A
curve (γ ) is called a constant speed geodesic if for all a < s ≤ t < b, we have
t
Wµ(γ ,γ ) = |t−s|Wµ(γ ,γ ). (39)
p s t p a b
It is straightforward to show that every constant speed geodesic is absolutely continuous.
Theorem 10 (Pµ(Y ×U) is a Geodesic Space).
p
Pµ(Y ×U) admits constant-speed geodesics. That is, for any η,ν ∈ Pµ(Y ×U), there exists
p p
a constant speed geodesic, contained within Pµ(Y ×U), between η and ν.
p
Proof. Write λ : (Y ×U)2 → Y ×U for the linear interpolant
t
λ (y ,u ,y ,u ) = (ty +(1−t)y ,tu +(1−t)u ) 0 ≤ t ≤ 1. (40)
t 0 0 1 1 0 1 0 1
Let γ⋆ ∈ Π (η,ν) be an optimal restricted coupling, and consider the path of measures
Y
in P (Y ×U) given by
p
γ = [λ ] γ⋆ 0 ≤ t ≤ 1. (41)
t t #
11Kerrigan, Migliorini, Smyth
Step one: We check that for each 0 ≤ t ≤ 1, we have γ ∈ Pµ(Y ×U). That is, we need
t p
to check that for all Borel A ⊆ Y, we have γ (A×U) = µ(A). Indeed, recall that restricted
t
measures are concentrated on the set C (see Equation (9)). Thus,
γ (A×U) =
γ⋆(cid:8) λ−1(A×U)(cid:9)
t t
= γ⋆{(y,u ,y,u ) | y ∈ A}
0 1
= π1γ⋆(A) = (π1◦π1,2) γ⋆(A)
# #
= π1η(A) = µ(A)
#
i.e. γ (A×Y) = µ(A) as claimed.
t
Step two: We show that Wµ(γ ,γ ) = |t − s|Wµ(η,ν). Set γs := (λ ,λ ) γ⋆ for
p t s p t t s #
0 ≤ s < t ≤ 1. We claim γs ∈ Π (γ ,γ ). Indeed, we have π1,2γs = γ because for all Borel
t Y t s # t t
A ⊆ Y ×U,
(λ ,λ ) γ⋆(A×Y ×U) = γ⋆(cid:0) λ−1(A)(cid:1) = (λ ) γ∗(A). (42)
t s # t t #
An analogous calculation shows that π3,4γs = γ , so that γs ∈ Π(γ ,γ ). We now check
# t s t t s
that γs ∈ R (Y ×U). Indeed, suppose E ⊆ Y ×U is a Borel set such that E ∩C = ∅.
t Y
In other words, for every (y ,u ,y ,u ) ∈ E we have y ̸= y . Set D := (λ ,λ )−1(E). We
0 0 1 1 0 1 t s
claim D∩C = ∅, so that
γs(E) = (λ ,λ ) γ⋆(E) = γ⋆((λ ,λ )−1(E)) (43)
t t s # t s
= γ⋆(D∩C) = 0. (44)
Indeed, if c = (y,u ,y,u ) ∈ C, then
0 1
(λ ,λ )(c) = (y,tu +(1−t)u ,y,su +(1−s)u ) ∈/ E (45)
t s 0 1 0 1
=⇒ c ∈/ (π ,π )−1(E). (46)
t s
Thus γs ∈ Π (η,ν) as claimed. Now, we have
t Y
(cid:90)
Wµ,p(γ ,γ ) ≤ dp(y ,u ,y ,u ) dλs(y ,u ,y ,u )
p t s 0 0 1 1 t 0 0 1 1
(Y×U)2
(cid:90)
= dp(λ (y ,u ,y ,u ),λ (y ,u ,y ,u )) dγ⋆(y ,u ,y ,u )
t 0 0 1 1 s 0 0 1 1 0 0 1 1
(Y×U)2
(cid:90)
= (cid:0) |(t−s)(y −y )|2+|(t−s)(u −u )|2(cid:1)p/2 dγ⋆(y ,u ,y ,u )
0 1 0 1 0 0 1 1
(Y×U)2
(cid:90)
= |t−s|p dp(y ,u ,y ,u )dγ⋆(y ,u ,y ,u )
0 0 1 1 0 0 1 1
(Y×U)2
= |t−s|pWµ,p(η,ν).
p
Conversely, an application of the previous inequality and the triangle inequality show
that for 0 ≤ s ≤ t ≤ 1,
Wµ(η,ν) ≤ Wµ(η,γ )+Wµ(γ ,γ )+Wµ(γ ,ν) (47)
p p s p s t p t
≤ sWµ(η,ν)+Wµ(γ ,γ )+(1−t)Wµ(η,ν). (48)
p p s t p
Rearrangingthepreviousinequalityimplies|t−s|Wµ(η,ν) ≤ Wµ(γ ,γ )foralls,t ∈ [0,1],
p p s t
and hence Wµ(γ ,γ ) = |t−s|pWµ(η,ν).
p t s p
12Dynamic Conditional Optimal Transport
When an optimal restricted coupling γ⋆ ∈ Π (η,ν) is induced by an injective triangular
Y
map T⋆, we may recover a constant speed geodesic in Pµ(Y ×U), generalizing the McCann
p
interpolant(McCann,1997)totheconditionalsetting. WerefertoProposition4forsufficient
conditions on η,ν under which such a T⋆ exists. Informally, samples from (y ,u ) ∼ η flow
0 0
in a straight path at a constant speed to their destination T⋆(y ,u ). We note that this
0 0
theorem, while primarily about geodesics, relies on notions discussed in Section 5. In fact,
this may be viewed as a special case of the results in Section 5, where we may explicitly
identify the vector field.
Theorem 11 (Conditional McCann Interpolants).
Fix η,ν ∈ Pµ(Y ×U). Suppose T⋆(y,u) = (y,T⋆(y,u)) is a block triangular map solving the
p U
conditional Monge problem (8). Define the maps T : Y ×U → Y ×U for 0 ≤ t ≤ 1 via
t
T = (1−t)I +tT⋆ (y,u) (cid:55)→ (y,(1−t)u+tT⋆(y,u)) (49)
t U
and define the curve of measures γ = [T ] η ∈ Pγ(Y ×U). Then,
t t # p
1. (γ ) is absolutely continuous and a constant speed geodesic between η,ν
t
2. The vector field v (T⋆(y,u)) = (0,T⋆(y,u)−u) generates the path γ , in the sense that
t t U t
(γ ,v ) solve the continuity equation (61).
t t
Proof. Consider the function w : Y ×U → U given by
t
w (y,u) = (0,T⋆(y,u)−u) = (0,w (y,u)) (50)
t U t,U
and note this is precisely w (y,u) = ∂ T⋆(y,u). Define the vector field
t t t
(cid:16) (cid:17) (cid:16) (cid:17)
v (y,u) = w ◦T⋆,−1 (y,u) = 0,(w ◦T⋆,−1)(y,u) . (51)
t t t t,U t,U
For any φ ∈ Cyl(Y ×U), we have
(cid:90) (cid:90)
d d
φ(y,u)dγ (y,u) = φ(y,u)d[T ] η(y,u) (52)
t t #
dt dt
Y×U Y×U
(cid:90)
d
= φ(y,T⋆ (y,u))dη(y,u) (53)
dt t,U
Y×U
(cid:90)
= ⟨∇φ(y,T⋆ (y,u),w (y,u))⟩dη(y,u) (54)
t,U t
Y×U
(cid:90)
= ⟨∇φ(y,u),v (y,u)⟩dγ (y,u) (55)
t t
Y×U
which shows that (γ ,v ) solve the continuity equation.
t t
13Kerrigan, Migliorini, Smyth
Now, note that for 0 ≤ a ≤ b ≤ 1, we have
(cid:90) b (cid:90) b(cid:18)(cid:90) (cid:12) (cid:12)p (cid:19)1/p
∥v ∥ dt = (cid:12)w ◦T⋆,−1(cid:12) (y,u)dγ (y,u) dt (56)
t Lp(γt,Y×U) (cid:12) t t (cid:12) t
a a Y×U
(cid:90) b(cid:18)(cid:90) (cid:19)1/p
= |w |p(y,u)dη(y,u) dt (57)
t
a Y×U
(cid:90) b(cid:18)(cid:90) (cid:19)1/p
= |u−T⋆(y,u)|p(y,u)dη(y,u) dt (58)
U
a Y×U
= (b−a)Wµ(η,ν). (59)
p
(cid:82)1
In particular, ∥v ∥ dt < ∞ and so by Theorem 15 (γ ) is absolutely continu-
0 t Lp(γt,Y×U) t
ous. A similar calculation shows that (b−a)Wµ(η,ν) = Wµ(γ ,γ ) = (cid:82)b |γ′(t)|, where the
p p b a a
last line follows from the absolute continuity of γ . Thus, ∥v ∥ = |γ′|(t) for almost
t t Lp(γt,Y×U)
every t ∈ [0,1] by Lebesgue differentiation.
5 Conditional Benamou-Brenier Theorem
In this section, we prove a characterization of all absolutely continuous curves in the space
Pµ(Y ×U). Here we assume 1 < p < ∞. First, we introduce the notion of a triangular
p
vector field on Y ×U, which informally has no velocity in the Y component. Triangular
vector fields serve as the dynamic analog of triangular maps (6).
Definition 12 (Triangular Vector Fields).
Let I ⊂ R be an open interval. A time-dependent Borel vector field v : I ×Y ×U → Y ×U
is said to be triangular if there exists a Borel vector field vU : I ×Y ×U → U such that
v (y,u) =
(cid:0) 0,vU(y,u)(cid:1)
. (60)
t t
In other words, v =
ι(cid:0) vU(cid:1)
where ι : U → Y ×U is the inclusion map.
t t
5.1 Continuity Equation
Let I ⊂ R be an open interval. The continuity equation
∂ γ +div(v γ ) = 0 on Y ×U ×I (61)
t t t t
describes the evolution of a measure γ which flows along a given vector field v . This
t t
equation must be understood distributionally, i.e. for every φ in an appropriate space of
test functions,
(cid:90) (cid:90)
(∂ φ(y,u,t)+⟨v (y,u),∇ φ(y,u,t)⟩) dγ (y,u)dt = 0. (62)
t t y,u t
I Y×U
We consider cylindrical test functions φ ∈ Cyl(Y ×U ×I), i.e. of the form φ(y,u,t) =
ψ(πd(y,u),t) where πd : Y × U → Rd maps (y,u) → (⟨(y,u),e ⟩,...,⟨(y,u),e ⟩) where
1 d
14Dynamic Conditional Optimal Transport
{e ,e ,...,e } is any orthonormal family in Y ×U. In the finite dimensional setting, one
1 2 d
may take φ ∈ C∞(Y ×U) to be smooth and compactly supported (Ambrosio et al., 2005,
c
Remark 8.1.1).
We now prove a lemma that will be critical to our later results. Informally, a solution to
the continuity equation with a triangular vector field will result in the conditional measures
almost surely satisfying the continuity equaiton as well.
Lemma 13 (Triangular Vector Fields Preserve Conditionals).
Suppose v (y,u) = (0,vU(y,u)) is triangular and that (γ ) ⊂ Pµ(Y ×U) is a path of measures
t t t p
such that (v ,γ ) satisfy the continuity equation (61) in the distributional sense. Then, it
t t
follows that for µ-almost every y ∈ Y, we have
∂ γy +∇·(vU(y,−)γy) = 0. (63)
t t t t
Proof. Fix any φ ∈ Cyl(U ×I). Suppose ψ ∈ Cyl(Y) is given, and note that ψ(y)φ(u,t) ∈
Cyl(Y ×U ×I). As (v ,γ ) solve the continuity equation, it follows from the triangular
t t
structure of v that upon testing against ψφ we have
t
(cid:90) (cid:90) (cid:90)
ψ(y) (cid:0) ∂ φ(u,t)+⟨vU(y,u),∇ φ(u,t)(cid:1) dγy(u)dµ(y)dt = 0. (64)
t t u t
I Y U
Because ψ(y) ∈ Cyl(Y), it is of the form ρ(π(y)) where π : Y → Rk for some k ≥ 1 and
ρ ∈ C∞(Rk). Taking ρ to be a sequence of smooth approximations to the indicator function
c
of an arbitrary rectangle E = E ×E ×···×E ⊆ Rk, we see
1 2 k
(cid:90) (cid:90) (cid:90)
(cid:0) ∂ φ(u,t)+⟨vU(y,u),∇ φ(u,t)(cid:1) dγy(u)dtdµ(y) = 0. (65)
t t u t
π−1(E) I U
As Y is separable, the Borel σ-algebra on Y is generated by the cylinder sets, i.e. those
which are precisely of the form π−1(E) for some finite-dimensional rectangle E. We have
thus shown that for an arbitrary Borel measurable set E ⊆ Y,
(cid:90) (cid:90) (cid:90)
(cid:0) ∂ φ(u,t)+⟨vU(y,u),∇ φ(u,t)(cid:1) dγy(u)dtdµ(y) = 0. (66)
t t u t
E I U
From this, it follows that
(cid:90) (cid:90)
(cid:0) ∂ φ(u,t)+⟨vU(y,u),∇ φ(u,t)(cid:1) dγy(u)dµ(y)dt = 0 µ-almost every y. (67)
t t u t
I U
We note that having v be triangular is sufficient, but certainly not necessary, for the
t
conditional continuity equation to almost surely hold. For instance, the vector field in Rd
that rotates N(0,I) about the origin preserves all conditional distributions.
15Kerrigan, Migliorini, Smyth
5.2 Absolutely Continuous Curves
We will show in Theorem 14 that every absolutely continuous curve (γ ) ⊆ Pµ(Y × U)
t p
is generated (in the sense of satisfying the continuity equation) by a triangular vector
field such that ∥v ∥ ≤ |γ′|(t) for almost every t ∈ I. This result is obtained as
t Lp(γt,Y×U)
a generalization of the technique in Ambrosio et al. (2005, Theorem 8.3.1), which is an
application of variational methods for PDEs.
Conversely, we show in Theorem 15 that if the pair (γ ,v ) solve the continuity equation
t t
and v is triangular, then the curve (γ ) is absolutely continuous and |γ′(t)| ≤ ∥v ∥ .
t t t Lp(γt,Y×U)
Themaintechniqueofthisresultistostudythecollectionofconditional continuityequations
(which is feasible by Lemma 13) and to apply the converse of Ambrosio et al. (2005,
Theorem 8.3.1). In this setting, the infinite-dimensional result is obtained via a finite-
dimensional approximation argument.
We define the map j : Lq(γ,Y ×U) → Lp(γ,Y ×U) for 1/p+1/q = 1 via
q
(cid:40)
|w|q−2w w ̸= 0
j (w) = (68)
q
0 w = 0
which is the Fr´echet differential of the convex functional 1 ∥w∥q . A straightforward
q Lq(γ,Y×U)
calculation shows that this map satisfies
(cid:90)
∥j (w)∥p = ∥w∥q = ⟨j (w),w⟩dγ(y,u). (69)
q Lp(γ,Y×U) Lq(γ,Y×U q
Y×U
Theorem 14 (AC Curves in Pµ(Y ×U)).
p
Let I ⊂ R be an open interval, and suppose γ : I → Pµ(Y ×U) is an absolutely continuous
t p
in the Wµ metric with |γ′|(t) ∈ L1(I). Then, there exists a Borel vector field v (x) such that
p t
1. v is triangular, i.e. v (y,u) = ι(vU(y,u)) for some Borel vU : Y ×U → U
t t t t
2. v ∈ Lp(γ ,Y ×U) for a.e.-t
t t
3. ∥v ∥ ≤ |γ′|(t) for a.e.-t
t Lp(γt,Y×U)
4. (v ,γ ) solve the continuity equation distributionally.
t t
Proof. Assume without loss of generality that |γ′|(t) ∈ L∞(I) and that I = (0,1) (Ambrosio
et al., 2005, Lemma 1.1.4, Lemma 8.1.3). Fix any φ ∈ Cyl(Y ×U). For s,t ∈ I there exists
an optimal triangular coupling γ ∈ Π (γ ,γ ). By H¨older’s inequality,
st Y s t
|γ (φ)−γ (φ)| ≤ Lip(φ)Wµ(γ ,γ ). (70)
t s p s t
It follows that t (cid:55)→ γ (φ) is absolutely continuous. We can introduce the upper semicon-
t
tinuous and bounded map
(cid:40)
|∇φ(y ,u )| (y ,u ) = (y ,u )
0 0 0 0 1 1
H(y ,u ,y ,u ) = . (71)
0 0 1 1 |φ(y0,u0)−φ(y1,u1)|
(y ,u ) ̸= (y ,u )
|(y0,u0)−(y1,u1)| 0 0 1 1
16Dynamic Conditional Optimal Transport
For |h| sufficiently small, choose any optimal coupling γ ∈ Π (γ ,γ ) and note
(s+h)h Y s+h s
that
(cid:90)
|γ (φ)−γ (φ)| 1
s+h s
≤ |(y ,u )−(y ,u )|H(y ,u ,y ,u )dγ (72)
|h| |h| 0 0 1 1 0 0 1 1 (s+h)s
(Y×U)2
(cid:32) (cid:33)1/q
Wµ(γ ,γ ) (cid:90)
≤ p s+h s Hq(y ,u ,y ,u )dγ . (73)
|h| 0 0 1 1 (s+h)h,s
(Y×U)2
If t is a point of metric differentiability for t (cid:55)→ γ , note that γ → (I,I) γ narrowly,
t (t+h)t # t
where I is the identity map on Y ×U. Moreover, since γ ∈ Pµ(Y ×U), it follows that on
t p
the diagonal we have that almost surely H(y ,u ,y ,u ) = ι(|∇ φ(y ,u )|. Thus,
0 0 0 1 u 0 0
|γ (φ)−γ (φ)|
(cid:18)(cid:90) (cid:19)1/q
limsup t+h t ≤ |γ′|(t) |H|q(y ,u ,y ,u )dγ (y ,u ) (74)
0 0 0 0 t 0 0
|h|
h→0 Y×U
= |γ′|(t)∥ι(∇ φ)∥ = |γ′|(t)∥∇ φ∥ . (75)
u Lq(γt,Y×U) u Lq(γt,U)
(cid:82)
Taking Q = Y ×U ×I and γ = γ dt, fix any φ ∈ Cyl(Q). We have that
t
(cid:90)
∂ φ(y,u,s)dγ(y,u,s)
s
Q
(cid:32) (cid:33) (76)
(cid:90) (cid:90) (cid:90)
1
= lim φ(y,u,s)dγ (y,u)− φ(y,u,s)dγ (y,u) ds.
s s+h
h↓0 I h Y×U (Y×U)
An application of Fatou’s Lemma, Equation (74), and H¨older’s inequality gives us
(cid:12)(cid:90) (cid:12) (cid:18)(cid:90) (cid:19)1/p(cid:18)(cid:90) (cid:19)1/q
(cid:12) (cid:12) ∂ sφ(y,u,s)dγ(y,u,s)(cid:12) (cid:12) ≤ |γ′|(s)ds |∇ uφ(y,u,s)|qdµ(y,u,s) (77)
(cid:12) (cid:12)
Q J Q
for any interval J ⊂ I with supp φ ⊂ J ×Y ×U.
Fix the subspace
V = {ι(∇ φ(y,u,s)) : φ ∈ Cyl(Q)} ⊆ Y ×U (78)
u
and denote by V its Lq(γ,Y ×U ×I) closure. Define the linear functional L : V → R via
(cid:90)
L(∇ φ) = − ∂ φ(y,u,s)dγ(y,u,s) (79)
u s
Q
and note that Equation (77) implies that L is a bounded linear functional on V. Thus
(by Hahn-Banach and the fact that V ⊆ V is dense) we may uniquely extend L to V. We
thus have a convex minimization problem
(cid:90)
1
min |w(y,u,s)|qdγ(y,u,s)−L(w) (80)
q
w∈V Q
which admits the unique solution w such that j (w)−L = 0. In particular, the estimate
q
(77) shows that the above functional is coercive and hence admits a minimizer which we
17Kerrigan, Migliorini, Smyth
may obtain via its differential as a consequence of convexity. Thus, we obtain a triangular
vector field v = j (w) such that for all φ ∈ Cyl(Q),
q
(cid:90) (cid:90)
⟨v,∇φ⟩ = ⟨v(y,u,s),∇φ(y,u,s)⟩dγ(y,u,s) = ⟨L,∇φ⟩ = − ∂ φ(y,u,s)dγ(y,u,s).
s
Q Q
(81)
This precisely shows that (v ,γ ) is a triangular distributional solution to the continuity
t t
equation.
Now, choose any interval J ⊂ I and choose a sequence ηk ∈ C∞(J), with 0 ≤ ηk ≤ 1
c
and η → 1 as k → ∞. Moreover choose a sequence (∇ φ ) ⊂ V converging to w = j (v)
k J u n p
in Lq(γ,Q). Our previous calculations give
(cid:90) (cid:90) (cid:90)
ηk(s)|v(y,u,s)|pdγ(y,u,s) = ηk(s)⟨v,w⟩dγ = lim ηk⟨v,∇ φ ⟩dγ (82)
u n
n→∞
Q Q Q
(cid:18)(cid:90) (cid:19)1/p(cid:18)(cid:90) (cid:19)1/p
= lim ⟨L,∇ (ηkφ )⟩ ≤ |γ′|p(s)ds |v|pdγ . (83)
u n
n→∞
J J×Y×U
Taking k → ∞ we see that
(cid:90) (cid:90) (cid:90)
|v (y,u)|pdγ (y,u)dt ≤ |γ′|p(s)ds (84)
t t
J Y×U J
and since J ⊂ I was arbitrary, we conclude
∥v ∥ ≤ |γ′|(t) a.e.-t. (85)
t Lp(γt,Y×U)
Theorem 15 (Continuous Curves Generated by triangular Vector Fields).
Suppose that γ : I → Pµ(Y ×U) is narrowly continuous and (v ) is a triangular vector
t p t
field such that (γ ,v ) solve the continuity equation with ∥v ∥ ∈ L1(I). Then,
t t t Lp(γt,Y×U)
γ : I → Pµ(Y ×U) is absolutely continuous in the Wµ metric and |γ′|(t) ≤ ∥v ∥
t p p t Lp(µ,Y×U)
for almost every t.
Proof. We first assume that U is finite dimensional. Our strategy is to check the hypotheses
necessary for Ambrosio et al. (2005, Theorem 8.3.1) to hold for µ-almost every y, followed
by an application of this theorem. By Lemma 13, for µ-almost every y we have that
(γy,vU(y,−)) solve the continuity equation distributionally on I ×U.
t t
By Jensen’s inequality (and the assumption p ≥ 1) we see
(cid:90) I∥v t∥
Lp(γt,Y×U)
dt = (cid:90) IE y∼µ(cid:104)(cid:13) (cid:13)v tU(y,−)(cid:13) (cid:13)p
Lp(γ
ty,U)(cid:105)1/p dt (86)
≥ (cid:90) E y∼µ(cid:104)(cid:13) (cid:13)v tU(y,−)(cid:13) (cid:13) Lp(γy,U)(cid:105) dt (87)
I t
(cid:20)(cid:90) (cid:21)
= E y∼µ (cid:13) (cid:13)v tU(y,−)(cid:13) (cid:13) Lp(γy,U) dt . (88)
I t
18Dynamic Conditional Optimal Transport
Since the first term is finite, it follows that
(cid:13) (cid:13)v tU(y,−)(cid:13) (cid:13) Lp(γy,U) ∈ L1(I) µ-almost every y. (89)
t
Now Ambrosio et al. (2005, Lemma 8.1.2) shows that for µ-almost every y we have that
(γy) admits a narrowly continuous representative (γ˜y) with γ˜y = γy for almost every t. It
t t t t
follows from Ambrosio et al. (2005, Theorem 8.3.1) that for any t ≤ t in I, we have
1 2
(cid:90) t2
Wp(γ˜y,γ˜y) ≤ (t −t )p−1 |vU(y,u)|pdγ˜y(u)dt (90)
p t1 t2 2 1 t t
t1
(cid:90) t2
= (t −t )p−1 |vU(y,u)|pdγy(u)dt (91)
2 1 t t
t1
where the second line follows as γ˜y = γy for almost every t.
t t
Let γ˜ = (cid:82) γ˜ydµ(y) be the measure obtained via marginalizing over the Y-variables.
t Y t
Taking an expectation over y ∼ µ, the previous inequality shows us that
Wµ,p(γ˜ ,γ˜ ) 1 (cid:90) t2
p t1 t2 ≤ ∥v ∥p dt. (92)
(t 2−t 1)p t 2−t
1 t1
t Lp(γt,Y×U)
Now, note that t is almost surely a Lebesgue point of the right-hand side and γ˜ = γ .
1 t1 t1
Taking t → t along a sequence where γ˜ = γ shows us that
2 1 t2 t2
|γ′|(t) ≤ ∥v ∥ (93)
t Lp(γt),Y×U
for almost every t ∈ I.
In the case that U is infinite dimensional, fix any y ∈ Y such that Lemma 13 holds
(which is of full measure) and fix a countable orthonormal basis (e ) for U. Set πd : U → Rd
k
to be the projection operator for this basis, i.e. u (cid:55)→ (⟨u,e ⟩,...,⟨u,e ⟩). We consider the
1 d
collection of finite dimensional conditional measures γd,y = πdγy. By the same argument
t # t
in Ambrosio et al. (2005, Theorem 8.3.1), there exists a vector field vd,y on Rd such that
t
(γd,y,vd,y) solve the continuity equation and
t t
(cid:13) (cid:13)
(cid:13) (cid:13)v td,y(cid:13)
(cid:13) Lp(γd,y,Rd)
≤ (cid:13) (cid:13)v tU(y,−)(cid:13) (cid:13)
Lp(γ
ty,U). (94)
t
It follows from the finite-dimensional case above that for almost every t ≤ t , we have
1 2
W pp(γ td 1,y,γ td 2,y) ≤ (t 2−t
1)p−1(cid:90) t1t2(cid:13)
(cid:13)v tU(y,−)(cid:13) (cid:13)p
Lp(γ ty,U)
dt. (95)
Let γˆy,d = (πd)⋆ γy,d where (πd)⋆ : Rd → U maps z (cid:55)→ (cid:80)d z e . As d → ∞ we
t # t k=1 k k
have γˆd,y → γy narrowly for all t ∈ I. Since (πd)⋆ is an isometry, Ambrosio et al. (2005,
t t
Lemma 7.1.4) shows that
W pp(γ ty 1,γ ty 2) ≤ li dm →i ∞nfW pp(γ td 1,y,γ td 2,y) ≤ (t 2−t
1)p−1(cid:90) t1t2(cid:13)
(cid:13)v tU(y,−)(cid:13) (cid:13)p
Lp(γ ty,U)
dt. (96)
19Kerrigan, Migliorini, Smyth
Now, integration with respect to dµ(y) yields
(cid:90) t2
Wp,µ(γ ,γ ) ≤ (t −t )p−1 ∥v ∥p dt. (97)
p t1 t2 2 1 t Lp(γt,Y×U)
t1
Taking t → t shows that for almost every t we have
2 1
|γ′|(t) ≤ ∥v ∥ . (98)
t Lp(γt,Y×U)
As a corollary of Theorem 14 and Theorem 15, we obtain a conditional version of the
Benamou-Breniertheorem(BenamouandBrenier,2000). WenotethattheproofofTheorem
16 largely follows the unconditional case (see e.g. Ambrosio et al. (2005, Chapter 8)), but
we include it for the sake of completeness.
Theorem 16 (Conditional Benamou-Brenier).
Suppose Y,U are separable Hilbert spaces, 1 < p < ∞, and µ ∈ P(Y). Then, for any
η,ν ∈ Pµ(Y ×U), we have
p
(cid:26)(cid:90) 1 (cid:27)
Wp,µ(η,ν) = min ∥v ∥p | (v ,γ ) solve (61), γ = η,γ = ν, and v is triangular .
p
(γt,vt) 0
t Lp(µt) t t 0 1 t
(99)
Proof. Write M for the infimum on the right-hand side.
(cid:82)1
First,supposethat(v ,µ )areadmissibleand ∥v ∥ < ∞. ItfollowsfromTheorem
t t 0 t Lp(µt)
15 that (γ ) is an absolutely continuous curve in Pµ(Y ×U) and ∥v ∥ ≥ |γ′|(t).
t p t Lp(µt,Y×U)
Thus,
(cid:18)(cid:90) 1 (cid:19)p (cid:90) 1
Wµ,p(η,ν) ≤ |γ′|(t)dt ≤ ∥v ∥p dt ≤ M. (100)
p t Lp(µt,Y×U)
0 0
Conversely, by Theorem 10 there exists a constant speed geodesic (γ ) ⊂ Pµ(Y ×U)
t p
connecting η and ν. Recall that constant speed geodesics are absolutely continuous. By
Theorem 14, there exists a Borel triangular vector field v such that (v ,γ ) solve the
t t t
continuity equation, and moreover ∥v ∥ ≤ |γ′|(t). In fact, because (v ,γ ) solve the
t Lp(µt,Y×U) t t
continuity equation, Theorem 15 yields that ∥v ∥ = |γ′|(t).
t Lp(µt,Y×U)
Since γ is a constant speed geodesic in Pµ(Y ×U), it follows that |µ′|(t) = Wµ(η,ν) for
t p p
almost every t ∈ (0,1). Hence,
(cid:90) 1 (cid:90) 1
Wµ,p(η,ν) = |γ′|(t)pdt = ∥v ∥p ≥ M. (101)
p t Lp(γt,Y×U)
0 0
Thus, Wµ,p(η,ν) = M as desired.
p
20Dynamic Conditional Optimal Transport
Figure 1: Results for the Darcy flow inverse problem. We compare the posterior mean and
variance of our method (COT-FM) to pCN (Cotter et al., 2013) and WaMGAN (Hosseini
et al., 2023).
6 COT Flow Matching
We have thus far seen that the COT problem (11) admits a dynamical formulation (99),
where one may take the underlying vector fields to be triangular. We use these results to
design a principled model for conditional generation based on flow matching (Lipman et al.,
2022; Albergo et al., 2023b). More specifically, recent work (Tong et al., 2023; Pooladian
et al., 2023a) has proposed to couple the source and target measures in flow matching via
optimal transport. We extend these techniques to the conditional setting.
In this section we make several simplifying assumptions. First, we use the squared-
distance cost |(y ,u )−(y ,u )|2 throughout (i.e. p = 2). Second, we assume that Y and
0 0 1 1
U are finite dimensional Euclidean spaces, but we emphasize that our techniques may be
applied to infinite dimensional spaces under additional regularity assumptions (Kerrigan
et al., 2023, 2022; Lim et al., 2023; Baldassari et al., 2024). Third, we assume that our
source and target measures admit densities with respect to the Lebesgue measure on Y ×U.
This is sufficient (Prop. 4) for the existence of a unique COT plan which is induced by a
triangular map. We will henceforth, in an abuse of notation, identify measures with their
densities.
Flow Matching. We assume that we have access to samples z = (y ,u ) ∼ η(y ,u ) ∈
0 0 0 0 0
Pµ(Y ×U) from a source measure, and samples z = (y ,u ) ∼ ν(y ,u ) ∈ Pµ(Y ×U) from
p 1 1 1 1 1 p
a target measure. Let z = (z ,z ) ∼ ρ(z ,z ) ∈ Π(η,ν) be any coupling of the source and
0 1 0 1
target measure. Following Tong et al. (2023), we specify a collection of measures and vector
fields via
γ (y,u | z) = N (y,u | tz +(1−t)z ,C) (102)
t 1 0
u (y,u | z) = z −z (103)
t 1 0
where C is any trace-class covariance operator (Da Prato and Zabczyk, 2014), e.g.
C = σ2I.
As is standard in flow matching (Lipman et al., 2022; Tong et al., 2023; Kerrigan et al.,
2023), we obtain from Equations (102) and (103) a marginal measure γ (y,u) and vector
t
field u (y,u) satisfying the continuity equation via
t
(cid:90)
γ (y,u) = γ (y,u | z)dρ(z) (104)
t t
(Y×U)2
21Kerrigan, Migliorini, Smyth
Figure 2: Further visualizations of our results on the Darcy flow inverse problem. The first
row shows the ground-truth posterior. Row two shows samples obtained from out COT-FM
method, and row three shows the same for WaMGAN (Hosseini et al., 2023). The prior and
conditioning information are shown in the final two rows.
(cid:90)
dγ (y,u | z)
t
u (y,u) = u (y,u | z) dρ(z). (105)
t t
dγ (y,u)
(Y×U)2 t
To model the intractable u (y,u), we minimize the loss1
t
(cid:13) (cid:13)2
L(θ) = E (cid:13)uθ(t,y,u)−u (y,u | z)(cid:13) (106)
t,ρ(z),γt(y,u|z)(cid:13) t (cid:13)
which has the same gradient (with respect to θ as the MSE loss to the true vector field
of interest u (y,u) (Tong et al., 2023, Theorem 3.2).
t
COT Flow Matching. In the preceding section, ρ(z) may be an arbitrary coupling
between η and ν. Motivated by Propositions 1 and 4, we will choose ρ to be a conditional
optimal coupling, so that (y ,u ,y ,u ) ∼ ρ has the form (y,u,y,T (y,u)). By Theorem 16
0 0 1 1 U
and Theorem 11, under such a ρ the vector fields in Equation (103) and Equation (105) will
be triangular. Thus, we parametrize our model uθ to also be triangular. If the covariance
operator C in Equation (102) is taken such that tr(C) → 0, then we recover the optimal path
of measures given by the conditional Benamou-Brenier Theorem via a pointwise application
of (Tong et al., 2023, Proposition 3.4).
1. This is often called the conditional flow matching loss (Tong et al., 2023), which is not to be confused
with the notion of conditioning that we focus on in this work.
22Dynamic Conditional Optimal Transport
Given a collection of samples {zi,zi}n drawn from η and ν, we approximate a condi-
0 1 i=1
tional optimal coupling ρ using standard numerical techniques (e.g., those available in the
POT package (Flamary et al., 2021)) with the cost function
c (y ,u ,y ,u ) = |y −y |2+ϵ|u −u |2 (107)
ϵ 0 0 1 1 1 0 1 0
for some 0 < ϵ ≪ 1. Intuitively, such a cost penalizes mass transfer along the Y
dimension, which is precisely the constraint sought in the COT problem (11). Hosseini et al.
(2023, Prop. 3.11) show that as ϵ ↓ 0, we recover the true optimal triangular map. When
our datasets are not too large, we precompute this COT coupling prior to training our flow
matching model. Otherwise, we compute this COT coupling for each minibatch drawn at
training time.
After training, we obtain a learned triangular vector field uθ(y,u). Given an arbitrary
t
fixedy ∈ Y,wemayapproximatelysamplefromthetargetν(u | y)bysamplingu ∼ η(u | y)
0 0
and numerically solving the corresponding differential equation. For some tasks, like domain
translation, note that we are given a fixed u of interest rather than sampling u from the
0 0
source. In these cases, we are effectively sampling from ν(u | y,u ).
0
Source Measure. Our framework is agnostic to the choice of source measure η, allowing
for great flexibility in the modeling process. The main requirement is that the Y-marginal
of η must match the Y-marginal of the target ν. In some scenarios, this is trivially satisfied.
For instance, if one is interested in using a source distribution which is simply random noise,
one may take η(y ,u ) = πYν(y )η (u ) to be the product of two independent distributions
0 0 # 0 U 0
where η (u) is arbitrary, e.g. Gaussian noise. Sampling from such an η can be achieved by
U
sampling noise from η (u) and an independent y from the target measure. We explore more
U
complex choices for η in Section 7.
7 Experiments
The following section is devoted to illustrate our methodology on a variety of different
problems related to conditional simulation. First, we explore a likelihood-free inference
problem in function space on a Darcy flow inverse problem. The dataset used in this case
mirrors those in Hosseini et al. (2023), as we aim to directly compare with their Conditional
Wasserstein Monge GAN (WaMGAN) method. In addition, we demonstrate our method
on two unpaired image-to-image translation tasks (Nazarpour and Chen, 2017; Choi et al.,
2020)
7.1 Bayesian Inverse Problems
In this experiment, we consider simulation-based posterior inference on the parameters
characterizing a stochastic forward model, whose likelihood evaluation is intractable.
Darcy Flow In this experiment the forward model corresponding to the 2D Darcy flow,
an ellipitic PDE on a smooth domain Ω ⊂ Rd characterized by a permeability field exp(u),
a pressure field ρ, and a source term f:
−divexp(u)∇ρ = f in Ω, ρ = 0 on ∂Ω. (108)
23Kerrigan, Migliorini, Smyth
For simplicity we denote the deterministic part of the forward model as F : U → Y, and
the stochasticity arises from only having access to noisy measurements y = F(u)+ϵ ∈ Y,
where ϵ ∼ N(0,σ2I). Although the general setting of the experiment corresponds to the one
in Hosseini et al. (2023), we consider the more general task of performing posterior inference
for u when both u and y are observed at arbitrary resolutions, effectively posing the problem
in function space. Hence, we consider Y ⊆ Y, U ⊆ U, where Y,U are infinite-dimensional
Hilbert spaces. We remark that, although the measurements Y are observed on a grid
their resolution does not need to be fixed, allowing for training on observations at different
resolutions. It follows that ϵ follows a Gaussian measure on Y with a white noise kernel.
Notice that although the conditional probability measure νu := πUν is Gaussian, evaluating
#
it requires simulating from Equation (108).
We assign to u a prior correspond-
1
ing to the Gaussian measure N(0,C ),
1
where C corresponds to a symmetric, non-
1
negative and trace-class covariance operator
(Da Prato and Zabczyk, 2014). In order
to obtain a dataset D = {y ,u }n of
1 1,i 1,i i=1
samples on the target domain, we simulate
y from Equation (108) parameterizing the
1
forward model with samples u ∼ N(0,C ).
1 1
We denote the target measure as ν. We con-
struct the source measure η as the product
measure N(0,C )×µ, where C is symmet-
0 0
ric, non-negative, trace-class and µ = πYη
#
corresponds to the marginal data distribu-
tion. The choice of picking C ̸= C , while
0 1
complicating the problem, reflects a scenario
where we want to perform inference with a
weaklyinformativepriorthatdoesnotreflect
the true data generating process. Notice
that sampling from a conditional coupling
Figure 3: Zero-shot super-resolution on the
γ ∈ Π (η,ν) would still recover samples
Y Darcy flow inverse problem. Our model (COT-
from the true posterior. Here, we assume
FM) is trained using a Fourier Neural Oper-
that C ,C are sufficiently regular such that
0 1 ator (Li et al., 2020), allowing us to generate
the path of measures in Equation (104) is
samples at higher resolutions than available
well-defined.
at training time. We compare to WaMGAN
Our goal is to perform posterior sam- (Hosseini et al., 2023) trained with the same
pling, that is sampling from the probabil- architechture.
ity measure ηy obtained by disintegration.
Function-space Markov chain Monte Carlo
approaches to solve this problem include the preconditioned Crank-Nicolson (pCN) (Cotter
etal.,2013)andgradient-basedmethodssuchastheinfinite-dimensionalMetropolis-adjusted
Langevin algorithm (∞−MALA) (Beskos et al., 2008) and infinite-dimensional Hamiltonian
Monte Carlo (∞−HMC) (Beskos et al., 2011). In order to make learning feasible in this
space we adapt the architecture of a Fourier Neural Operator (FNO) (Li et al., 2020)
24Dynamic Conditional Optimal Transport
(a) (b)
Figure 4: Samples from our model on a supervised image-to-image translation task, where
translation is performed from Chinese MNIST to MNIST (a) and from MNIST to Chinese
MNIST(b). Thisdemonstratesthatourmethodisabletousenon-trivialsourcedistributions
and is able to successfully draw samples from the corresponding conditional measure.
to accomodate for conditioning information observed at an arbitrary resolution. We do
so by introducing a projection layer mapping the conditioning information to match the
hidden channels of the input lifting block, and a pooling operation to project to the input
dimensions. The two are then concatenated and passed through an FNO block mapping
from 2hidden dim×input dim to hidden dim×input dim, before following the original
architecture.
The resulting amortized sampler, denoted for simplification by the mapping (y,u ) (cid:55)→
0
u = T˜ (y,u ), will parameterize an approximate posterior measure. Notice that, in contrast
1 U 0
to variational inference techniques, no distributional assumptions are made about the
approximate posterior. In turn, integrals are obtained numerically by Monte Carlo sampling
K−many samples from the prior, resulting in the approximation
(cid:90) K
νy(f) ≈ fdδ dN(0,C ) ≈ 1 (cid:88) f(T˜ (y ,u )), {u }K i. ∼i.d. N(0,C ).
T˜ U(y,u0) 0 K U k 0,k 0,k k=1 0
k=1
(109)
In Figure 1, we show an example of how our method compares to pCN and WaMGAN
on recovering the posterior mean and variance on a given example. For a fair comparison
and to ensure resolution-invariance, we used the same architecture for both neural models,
with the discriminator having an additional layer projecting to a scalar value. However,
the WaMGAN method used twice as many parameters as it requires two neural networks.
Each statistic is computed on 2×103 samples from each method. We showcase the ability
of our method to perform zero-shot super-resolution in Figure 3. A random selection of
samples from our method and from WaMGAN, as well as the prior sample and the different
25Kerrigan, Migliorini, Smyth
Figure 5: Uncurated sample from the test set of the Animal Faces HQ dataset (Choi et al.,
2020) from the dogs class, translated to cats. These results demonstrate our method on an
unsupervised image-to-image task, where we utilize a pre-trained content extractor.
conditions used for generation, are displayed in Figure 2. Qualitatively, our samples appear
to better match the true posterior compared to WaMGAN (Hosseini et al., 2023), while
being cheaper to sample than pCN (Cotter et al., 2013).
7.2 Unpaired Image-to-Image Translation
We now demonstrate our method on an image-to-image translation task. In particular, in
this section our source measures are distributions over images. The objective of domain
translationistoconvertinformationfromonedomaintoanotherbyadjustingdomain-specific
attributes, denoted style, while preserving the domain-invariant ones, denoted content (Isola
et al., 2017). Semantic attributes can be associated with human-assigned labels (supervised)
or inferred by a learning algorithm that learns domain-invariant representations of the data
(unsupervised). Pseudocode is available in Appendix A.
Historically, the problem of domain translation was tackled through methods that heavily
relied on parallel datasets—pairs of corresponding samples from the source and target
domains. However, obtaining such paired datasets is often impractical or impossible in many
real-world applications, prompting the shift towards methods that do not require paired
data (unpaired domain translation) (Pang et al., 2021). Domain translation is possible for a
learning algorithmif it satisfies cycle consistency, positing thattranslation of a sourcesample
u into a target sample uˆ and then back to the source domain should recover uˆ = u , and
s t s s
the same shall hold by starting from a target sample. Notice that this property is achieved
naturally by our model, and hence does not require additional terms in the loss as is the
case for GAN- or VAE-based methods (Zhu et al., 2017; Isola et al., 2017; Liu et al., 2017).
Here, the space U corresponds to the space of images. For the space Y, we explore two
choices. First, if supervised information (i.e., a class label) is available, we take Y to be
26Dynamic Conditional Optimal Transport
the space of labels. In this setting, if the class distribution in both the source and target
distribution is balanced, then the Y-marginals of the source and target measures match.
When such information is not available, we extract semantic information from our images
using a pre-trained encoder, mapping images to a latent space Y. In this case, we assume
the encoding distribution matches across the source and target distributions. Exploring
techniques for balancing this distribution is an interesting avenue for future work.
Supervised. We showcase in Figure 4 the ability of our method to address the task of
class-conditional image translation, using the MNIST and the Chinese MNIST (Nazarpour
and Chen, 2017) datasets. The second dataset was preprocessed in order to center the
images around their content, and both are reshaped to a resolution of 32×32. The size of
the training set was small enough for us to compute the empirical COT plan on the entire
dataset, and pairs were resampled at each epoch. We parameterize the neural network in
COT-FM with a UNet (Ronneberger et al., 2015), and we compute the Y−distance in the
COT plan by taking the L distance between one-hot encoded vectors for the labels. The
2
UNet has hidden dimensionality of 128, and we use three downsampling/upsampling blocks
with multipliers (1,2,2). The optimizer is Adam (Kingma and Ba, 2014) with a learning
rate of 10−4, at a batch size of 128 for 3×105 training steps.
Unsupervised. Samples are mapped into a shared representation space by a pre-trained
feature extractor F : Rd → [0,1]e, obtaining content variables y = F(u). These effectively
act as pseudo-labels when building the empirical COT matrix. In our example, we perform
translation from the dogs class to the cats class in the AnimalFacesHQ dataset (Choi
et al., 2020) and we use CLIP as our feature extractor (Radford et al., 2021) as found in
the official Hugging Face repository in its version clip-vit-large-patch14. The training
set is comprised of 5067 images of cats and 4680 images of dogs, enabling the computation
of the COT plan in advance. As with the previous experiment, samples are repaired by
sampling from the COT plan at each epoch. Images are observed at a native resolution of
512×512, but to ease the memory requirements we employ another pre-trained encoder
E : Rd → Rz for dimensionality reduction, effectively performing flow matching in latent
space (Dao et al., 2023). The procedure is detailed in Algorithm 1. We take the encoder E
to be the variational autoencoder (Kingma and Welling, 2013) from Stable Diffusion v2.1,
enabling training of the COT-FM network at a dimensionality of 4×64×64. We use the
Adam optimizer (Kingma and Ba, 2014) with a learning rate of 2×10−4, with a batch size
of 32 for 4×105 training steps. We show several uncurated conditional samples from our
model in Figure 5.
8 Conclusion
We analyze conditional optimal transport from a geometric and dynamical point of view.
Our analysis culminates in the characterization of absolutely continuous curves of measures
in a conditional Wasserstein space, resulting in a conditional analog of the Benamou-Brenier
Theorem. We use these result to build on the framework of triangular transport and flow
matchingtodevelopsimulation-freemethodsforconditionalgenerativemodels. Ourmethods
are applicable across a wide class of problems, and we demonstrate our methodology on an
infinite-dimensional Bayesian inverse problem as well as image translation tasks.
27Kerrigan, Migliorini, Smyth
Acknowledgments
This research was supported by the Hasso Plattner Institute (HPI) Research Center in
Machine Learning and Data Science at the University of California, Irvine, by the National
Science Foundation under award 1900644, and by the National Institutes of Health under
award R01-LM013344.
References
Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants.
arXiv preprint arXiv:2209.15571, 2022.
Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying
framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023a.
Michael S Albergo, Mark Goldstein, Nicholas M Boffi, Rajesh Ranganath, and Eric Vanden-Eijnden.
Stochastic interpolants with data-dependent couplings. arXiv preprint arXiv:2310.03725, 2023b.
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar´e. Gradient flows: in metric spaces and in the
space of probability measures. Springer Science & Business Media, 2005.
Luigi Ambrosio, Alberto Bressan, Dirk Helbing, Axel Klar, Enrique Zuazua, Luigi Ambrosio, and
Nicola Gigli. A user’s guide to optimal transport. Modelling and Optimisation of Flows on
Networks: Cetraro, Italy 2009, Editors: Benedetto Piccoli, Michel Rascle, pages 1–155, 2013.
Brandon Amos et al. Tutorial on amortized optimization. Foundations and Trends in Machine
Learning, 16(5):592–732, 2023.
LorenzoBaldassari,AliSiahkoohi,JosselinGarnier,KnutSolna,andMaartenVdeHoop. Conditional
score-based diffusion models for Bayesian inference in infinite dimensions. Advances in Neural
Information Processing Systems, 36, 2024.
Ricardo Baptista, Bamdad Hosseini, Nikola B Kovachki, and Youssef Marzouk. Conditional sam-
pling with monotone GANs: from generative models to likelihood-free inference. arXiv preprint
arXiv:2006.06755, 2020.
Rapha¨elBarboni,GabrielPeyr´e,andFranc¸ois-XavierVialard. Understandingthetrainingofinfinitely
deep and wide ResNets with conditional optimal transport. arXiv preprint arXiv:2403.12887,
2024.
Jean-David Benamou and Yann Brenier. A computational fluid mechanics solution to the Monge-
Kantorovich mass transfer problem. Numerische Mathematik, 84(3):375–393, 2000.
Alexandros Beskos, Gareth Roberts, Andrew Stuart, and Jochen Voss. MCMC methods for diffusion
bridges. Stochastics and Dynamics, 8(03):319–350, 2008.
Alexandros Beskos, Frank J Pinski, Jesu´s Marıa Sanz-Serna, and Andrew M Stuart. Hybrid Monte
Carlo on Hilbert spaces. Stochastic Processes and their Applications, 121(10):2201–2230, 2011.
VladimirIgorevichBogachevandMariaAparecidaSoaresRuas. Measure Theory,volume2. Springer,
2007.
Charlotte Bunne, Andreas Krause, and Marco Cuturi. Supervised training of conditional monge
maps. Advances in Neural Information Processing Systems, 35:6859–6872, 2022a.
28Dynamic Conditional Optimal Transport
Charlotte Bunne, Laetitia Papaxanthos, Andreas Krause, and Marco Cuturi. Proximal optimal
transport modeling of population dynamics. In International Conference on Artificial Intelligence
and Statistics, pages 6511–6528. PMLR, 2022b.
GuillaumeCarlier,VictorChernozhukov,andAlfredGalichon. Vectorquantileregression: Anoptimal
transport approach. The Annals of Statistics, 44(3):1165 – 1192, 2016. doi: 10.1214/15-AOS1401.
URL https://doi.org/10.1214/15-AOS1401.
Jannis Chemseddine, Paul Hagemann, Christian Wald, and Gabriele Steidl. Conditional Wasserstein
distances with applications in Bayesian OT flow matching. arXiv preprint arXiv:2403.18705, 2024.
RickyTQChenandYaronLipman. Riemannianflowmatchingongeneralgeometries. arXiv preprint
arXiv:2302.03660, 2023.
Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for
multiple domains. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 8188–8197, 2020.
S. L. Cotter, G. O. Roberts, A. M. Stuart, and D. White. MCMC methods for functions: Modifying
old algorithms to make them faster. Statistical Science, 28(3):424 – 446, 2013.
Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference.
Proceedings of the National Academy of Sciences, 117(48):30055–30062, 2020.
Giuseppe Da Prato and Jerzy Zabczyk. Stochastic Equations in Infinite Dimensions. Cambridge
University Press, 2014.
Quan Dao, Hao Phung, Binh Nguyen, and Anh Tran. Flow matching in latent space. arXiv preprint
arXiv:2307.08698, 2023.
MasoumehDashtiandAndrewMStuart. TheBayesianapproachtoinverseproblems. arXiv preprint
arXiv:1302.6989, 2013.
Aram Davtyan, Sepehr Sameni, and Paolo Favaro. Efficient video prediction via sparsely conditioned
flow matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pages 23263–23274, 2023.
Chris Finlay, J¨orn-Henrik Jacobsen, Levon Nurbekyan, and Adam Oberman. How to train your
neural ODE: the world of Jacobian and kinetic regularization. In International Conference on
Machine Learning, pages 3154–3164. PMLR, 2020.
R´emi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aur´elie Boisbunon, Stanislas
Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, L´eo Gautheron,
Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet,
Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and
Titouan Vayer. POT: Python optimal transport. Journal of Machine Learning Research, 22(78):
1–8, 2021. URL http://jmlr.org/papers/v22/20-451.html.
Timothy D Gebhard, Jonas Wildberger, Maximilian Dax, Daniel Angerhausen, Sascha P Quanz, and
Bernhard Scho¨lkopf. Inferring atmospheric properties of exoplanets with flow matching and neural
importance sampling. arXiv preprint arXiv:2312.08295, 2023.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural Information
Processing Systems, 27, 2014.
29Kerrigan, Migliorini, Smyth
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
Neural Information Processing Systems, 33:6840–6851, 2020.
Bamdad Hosseini, Alexander W Hsu, and Amirhossein Taghvaei. Conditional optimal transport on
function spaces. arXiv preprint arXiv:2311.05672, 2023.
GuillaumeHuguet,DanielSumnerMagruder,AlexanderTong,OluwadamilolaFasina,ManikKuchroo,
Guy Wolf, and Smita Krishnaswamy. Manifold interpolating optimal-transport flows for trajectory
inference. Advances in Neural Information Processing Systems, 35:29705–29718, 2022.
Noboru Isobe, Masanori Koyama, Kohei Hayashi, and Kenji Fukumizu. Extended flow match-
ing: a method of conditional generation with generalized continuity equation. arXiv preprint
arXiv:2402.18839, 2024.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2017.
Gavin Kerrigan, Justin Ley, and Padhraic Smyth. Diffusion generative models in infinite dimensions.
arXiv preprint arXiv:2212.00886, 2022.
Gavin Kerrigan, Giosue Migliorini, and Padhraic Smyth. Functional flow matching. arXiv preprint
arXiv:2305.17209, 2023.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Neural optimal transport. arXiv
preprint arXiv:2201.12220, 2022.
Sangyun Lee, Beomsu Kim, and Jong Chul Ye. Minimizing trajectory curvature of ode-based
generative models. In International Conference on Machine Learning, pages 18957–18973. PMLR,
2023.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew
Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential
equations. arXiv preprint arXiv:2010.08895, 2020.
JaeHyunLim, NikolaBKovachki, RicardoBaptista, ChristopherBeckham, KamyarAzizzadenesheli,
Jean Kossaifi, Vikram Voleti, Jiaming Song, Karsten Kreis, Jan Kautz, et al. Score-based diffusion
models in function space. arXiv preprint arXiv:2302.07400, 2023.
YaronLipman,RickyTQChen,HeliBen-Hamu,MaximilianNickel,andMatthewLe. Flowmatching
for generative modeling. In The Eleventh International Conference on Learning Representations,
2022.
Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks.
Advances in Neural Information Processing Systems, 30, 2017.
Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and
transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.
30Dynamic Conditional Optimal Transport
AshokMakkuva,AmirhosseinTaghvaei,SewoongOh,andJasonLee. Optimaltransportmappingvia
input convex neural networks. In International Conference on Machine Learning, pages 6672–6681.
PMLR, 2020.
Robert J McCann. A convexity principle for interacting gases. Advances in Mathematics, 128(1):
153–179, 1997.
K Nazarpour and M Chen. Handwritten Chinese Numbers. 1 2017. doi: 10.17634/137930-3. URL
https://data.ncl.ac.uk/articles/dataset/Handwritten_Chinese_Numbers/10280831.
Kirill Neklyudov, Rob Brekelmans, Alexander Tong, Lazar Atanackovic, Qiang Liu, and Alireza
Makhzani. A computational framework for solving Wasserstein Lagrangian flows. arXiv preprint
arXiv:2310.10649, 2023.
DerekOnken,SamyWuFung,XingjianLi,andLarsRuthotto. Ot-flow: Fastandaccuratecontinuous
normalizing flows via optimal transport. In Proceedings of the AAAI Conference on Artificial
Intelligence, pages 9223–9232, 2021.
Yingxue Pang, Jianxin Lin, Tao Qin, and Zhibo Chen. Image-to-image translation: Methods and
applications. IEEE Transactions on Multimedia, 24:3859–3881, 2021.
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Laksh-
minarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine
Learning Research, 22(57):1–64, 2021.
Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman,
and Ricky Chen. Multisample flow matching: Straightening flows with minibatch couplings. arXiv
preprint arXiv:2304.14772, 2023a.
Aram-Alexandre Pooladian, Carles Domingo-Enrich, Ricky TQ Chen, and Brandon Amos. Neural
optimal transport with lagrangian costs. In ICML Workshop on New Frontiers in Learning,
Control, and Dynamical Systems, 2023b.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International Conference on Machine Learning,
pages 8748–8763. PMLR, 2021.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In Medical image computing and computer-assisted intervention–MICCAI
2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III
18, pages 234–241. Springer, 2015.
Filippo Santambrogio. Optimal transport for applied mathematicians. Birka¨user, NY, 55(58-63):94,
2015.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint
arXiv:2011.13456, 2020.
Amirhossein Taghvaei and Amin Jalali. 2-Wasserstein approximation via restricted convex potentials
with application to improved training for GANs. arXiv preprint arXiv:1902.07197, 2019.
Alexander Tong, Jessie Huang, Guy Wolf, David Van Dijk, and Smita Krishnaswamy. Trajectorynet:
A dynamic optimal transport network for modeling cellular dynamics. In International Conference
on Machine Learning, pages 9526–9536. PMLR, 2020.
31Kerrigan, Migliorini, Smyth
Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian
Fatras, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models
with minibatch optimal transport. In ICML Workshop on New Frontiers in Learning, Control,
and Dynamical Systems, 2023.
C´edric Villani et al. Optimal Transport: Old and New, volume 338. Springer, 2009.
Zheyu Oliver Wang, Ricardo Baptista, Youssef Marzouk, Lars Ruthotto, and Deepanshu Verma.
EfficientneuralnetworkapproachesforconditionaloptimaltransportwithapplicationsinBayesian
inference. arXiv preprint arXiv:2310.16975, 2023.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 2223–2232, 2017.
32Dynamic Conditional Optimal Transport
Appendix A. Pseudocode
Algorithm 1: DCOT Flow for Unsupervised Image-to-Image Translation, Training
Step
1: Input: Data {u 0,i,u 1,i}B
i=1
∼ γ C⋆ OT, pretrained content extractor F, interpolant
network v , pretrained encoder E (optional).
θ
2: Initialize: L = 0
3: {y 0,i,y 1,i}B
i=1
← {F(u 0,i),F(u 1,i)}B
i=1
4: if E is given then
5: {u 0,i,u 1,i}B
i=1
← {E(u 0,i),E(u 1,i)}B
i=1
6: end if
7: for i = 1,...,B do
8: Sample t i ∼ U([0,1])
9: Interpolate: u t,i = (1−t i)u 0,i+t iu 1,i
10: Evaluate interpolant network:
vˆ0 = v (u ,t ,y ), vˆ1 = v (u ,t ,y )
ti θ t,i i 0,i ti θ t,i i 1,i
11: Compute interpolant loss:
L +=(cid:16)(cid:13) (cid:13)vˆ t0
i
−(u 1,i−u 0,i)(cid:13) (cid:13)2 2+(cid:13) (cid:13)vˆ t1
i
−(u 1,i−u 0,i)(cid:13) (cid:13)2 2(cid:17)
12: end for
13: θ new ← OptimizerStep(∇ θL)
33