PLAYER2VEC: A LANGUAGE MODELING APPROACH TO
UNDERSTAND PLAYER BEHAVIOR IN GAMES
APREPRINT
TianzeWang∗ MaryamHonari-Jahromi∗
KTHRoyalInstituteofTechnology King
Sweden Sweden
tianzew@kth.seu maryam.honari@king.com
StylianiKatsarou OlgaMikheeva
King KingandKTHRoyalInstituteofTechnology
Sweden Sweden
stella.katsarou@king.com olga.mikheeva@king.com
TheodorosPanagiotakopoulos
King
Sweden
theodoros.panagiotakopoulos@king.com
SaharAsadi† OlegSmirnov†
King King
Sweden Sweden
sahar.asadi@king.com oleg.smirnov@king.com
ABSTRACT
Methodsforlearninglatentuserrepresentationsfromhistoricalbehaviorlogshavegainedtraction
forrecommendationtasksine-commerce,contentstreaming,andothersettings. However,thisarea
stillremainsrelativelyunderexploredinvideoandmobilegamingcontexts.Inthiswork,wepresent
anovelmethodforovercomingthislimitationbyextendingalong-rangeTransformermodelfrom
the natural language processing domain to player behavior data. We discuss specifics of behav-
iortrackingingamesandproposepreprocessingandtokenizationapproachesbyviewingin-game
eventsinananalogouswaytowordsinsentences,thusenablinglearningplayerrepresentationsina
self-supervisedmannerintheabsenceofground-truthannotations. Weexperimentallydemonstrate
the efficacy of the proposed approach in fitting the distribution of behavior events by evaluating
intrinsiclanguagemodelingmetrics. Furthermore,wequalitativelyanalyzetheemergingstructure
ofthelearnedembeddingspaceandshowitsvalueforgeneratinginsightsintobehaviorpatternsto
informdownstreamapplications.
1 Introduction
Usermodelinganduserunderstandingareimportantingredientstodeliverhigh-qualitycustomerexperienceinmany
onlineplatforms,rangingfrommusicandvideostreamingtoretailande-commerce[35,10],tohealthinformatics[8].
Example downstream applications include understanding user journey and conversion funnel [2], delivering person-
alized content recommendations [29], classifying intents [16], and predicting lifetime value and churn [4, 1] among
∗Bothauthorscontributedequallytothisresearch.
†Jointseniorauthorship.
4202
rpA
5
]GL.sc[
1v43240.4042:viXraplayer2vec: ALanguageModelingApproachtoUnderstandPlayerBehaviorinGames APREPRINT
others. Traditionalapproachesthatrelyonexplicitannotationscollectedfromusersurveysareexpensivetoscaleto
modernplatformswithmillionsofusers. Asaresult,researchersandpractitionersinthisfieldoftenturntolearning
fromimplicitsignals[28,17],suchasbehaviortrackingdatathatcanbesourcedfromonlineinteractionlogsandis
availableinabundantamounts. Duetothelackofground-truthlabels,trainingisframedinaself-supervisedlearning
paradigm,whereamodelispretrainedonacarefullyconstructedpretextobjective. Inthisscenario,pseudolabelsare
derivedfromtheinherentstructureofdata,allowingthealgorithmtolearntheunderlyingdistribution,therebyserving
asadvantageousinitializationsforawiderangeofdownstreamtasks. Denoisingautoencoderisasuccessfulexample
of self-supervised objectives in the natural language processing (NLP) domain, where a model is trained to predict
randomlyomittedtokensfromasequence,whichserveasclassificationpseudolabels[14].
Despitebeinglargelysuccessfulinothersettings,large-scalemodelingofuserbehaviorremainsrelativelyuntapped
inthegamingdomain,wherepreviousstudiesweremainlyfocusedonexploratoryanalysis[7],psychology-informed
supervised learning [23, 34], as well as clustering and other unsupervised methods [6, 27]. In this work, we aim
to bridge this gap and investigate the applicability of self-supervised learning from tracking data with the purpose
of modeling player behavior in casual mobile games. Inspired by recent advances in the natural language domain,
weadoptLongformer[5], avariantofTransformer[31]architecturetailoredtolongcontextlengths, alongwiththe
maskedlanguagemodeling(MLM)objective[11]thathasprovensuccessfulinvarioussequencemodelingmethods.
Similar to conventional language modeling approaches, we exploit pretraining a model on a large unlabeled corpus
to learn latent representations that are applicable to downstream tasks. We hypothesize common patterns in how
individualplayersinteractwithgamecontentandmechanicscanaddanewdimensiontoplayerunderstanding.
Inthispaper,weproposeanovelapproachtounderstandingplayerbehaviorinthemobilegamingdomainbyextend-
ing long-range Transformer models from the NLP domain to create a context-rich representation of the player from
sessionizedrawplayerbehaviorlogsinaself-supervisedmanner. InSection3.1,wedescribetheplayerbehaviordata
collected from a large mobile gaming platform. We outline the challenges in modeling raw data and propose a pre-
processingpipelineinSection3.2. Then,weintroducethemodelarchitectureandtrainingprocedureinSection3.3.
QuantitativeandqualitativeexperimentalresultsarepresentedinSection4.Finally,weconcludeanddiscusspotential
directionsforfutureworkinSection6.
2 RelatedWork
Modelingsequentialinteractionsbetweenusersanditemswasexploredinpriorworksforrecommendationtasks.Ear-
lier, learningfromtemporaldataforcollaborativefilteringwasinvestigatedunderasimplifyingMarkovianassump-
tion[36],whichwaslaterextendedtoaMarkovdecisionprocess[26]. Predictingfuturebehavioraltrajectoriesusing
contextual and sequential information was addressed with an autoregressive Long Short-Term Memory model [33].
AnotherresearchintroducedacoupledRecurrentNeuralNetworkarchitecturetojointlymodeltheuser/iteminterac-
tionstrajectoryintheembeddingspace[13]. Furthermore,ithasbeenshownthatexplicitlymodelingdifferenttypes
of user behavior, such as repeated consumption and various actions, can improve the performance in downstream
metrics[3,24].
Leveraginglanguagemodelsforembeddingsequentialdatainrecommendationsettingswaspioneeredinthecontext
oflearningmusictrackrepresentations[18]withWord2Vecobjective[20]fromuser-generatedplaylists. Later, this
approachwasextendedtomodelingsequencesoflisteningsessionswithaRecurrentNeuralNetwork[9].
Morerecently, aself-attentionsequentialmodelwasintroduced[28], wheretheauthorsarguedthattheTransformer
frameworkfindstheoptimaltrade-offbetweensimplerMarkovchainmodels, whichtypicallyexcelinhigh-sparsity
settings, and neural network methods that can capture more complex scenarios but require dense training data. In
follow-up work, a multi-task customer model for personalization [17] outperformed the previous state-of-the-art
BERT4Recmodel[28]byleveraginganoveldataaugmentationandtask-awarereadoutmodule.
However, the applicationsof languagemodelsfor usermodeling remainunderstudied forgaming-specifictasks. To
thebestofourknowledge,weproposethefirstapproachforlearningrepresentationsofmobilegameplayersbasedon
pretrainingaTransfomerarchitectureinaself-supervisedmanner.
3 Methodology
Player behavior data, while being superficially similar to tracking data in other domains, possesses several notable
differencesthatmakeitchallengingtomodelwithconventionalapproaches. First,sincevideogamesaredesignedto
beadynamicandengagingenvironment, in-gameuserinteractionshappenatamuchhigherfrequencythaninWeb
2player2vec: ALanguageModelingApproachtoUnderstandPlayerBehaviorinGames APREPRINT
browsing and other applications. Thereafter, it leads to large amounts of potentially redundant behavior events that
requirecarefuldatapreprocessingandmodelinglong-rangedependenciesforcapturinginformativepatterns.
Second,playerbehaviordataoftencontainaspecifictypeofnoisethatisnotpresentand,hence,notwell-studiedin
otherscenarios. Specifically,afractionofeventsinagiventimewindowmayhavewrongorderingordonotcontain
anyorderinginformation. Thisproblemtypicallymanifestsduetoreal-worldengineeringandoperationsconstraints,
forexample,usersswitchingbetweenonlineandofflineplayingmodes.Thisnoisecandeterioratemodelperformance
duringtrainingandinference.
3.1 Dataset
Whenaplayerinteractswithamobilegameapplication,theirbehaviorgeneratesasequenceoftime-orderedevents,
which are recorded locally on a user’s device and sent to the central game server. Example events include starting
theapplication,startinganewgameround,purchasingin-gameitems,anddisplayingpop-upsandnotificationsfrom
the application side, among others. Supported behavior events are grouped in a vocabulary of 12 semantic classes,
whereeachclassisuniquelyidentifiedbyitsnameandhasanassociatedeventschemathatconsistsofacollectionof
mandatoryandoptionalcontinuousandcategoricalfeatures.
Session 1 Session K
E1 E2 E3 E4 E5 E6 E7 E8 E9 E10 E11 E12 E13 E14 E15 E16
...
EN1 E1 E2 E3 E4 E5
...
ENK
...
app start Event N1 : game end
game start { "player_id": 12345,
game end "server_time": 170506690,
"client_time": time(12:34),
interaction event "client_timezone": GMT+01:00,
impression event "level": 123,
"end_reason": "success",
other event "score": 12345
}
Figure1: Anillustrationofthecategorizationofeventsintosessions. Thefinaleventinsession1,markedasagame-
endevent,isexpandedtoshowdetailsaboutitsassociatedfieldsandvalues.
Playerbehavioreventsaregroupedintosessionsbasedonplayeractivity,asillustratedinFigure1. Aninstanceofa
game endeventisdepictedinthefigure,withproprietaryinformationremoved.Giventheproductdomainknowledge
and prioranalysis, weconsider a sessionas ended ifa playeris inactive for 15 minutes. The distribution of session
lengths in player data is shown in Figure 2a. Figure 2b depicts the distribution of the number of sessions of active
players over a period of 15 days. We observe that both session lengths and player activities approximately follow
geometricdistribution,whichisexpectedforthiskindofdata.
AsillustratedinFigure2c,sometypesofeventsaremuchmoreprevalentinthedata.Thisimbalanceinthedistribution
cancreateabiasproblemandskewthemodel’spredictionsunlessaccountedforinthepreprocessingstages. Notably,
although the data is sequential, the event structure is non-trivial to map into tokens, hence allowing many possible
designchoicesthatwediscussinthenextsection.
0.6 0.6
interactions
combined
0.5 0.5
impressions
combined
0.4 0.4
gamestart
0.3 0.3
gameend
0.2 0.2
other
0.1 0.1
appstart
0.0 0.0
0.0 0.1 0.2 0.3 0.4
Sessionlength(numberofevents) Numberofsessions Probability
(a) (b) (c)
Figure2: (a)Histogramofsessionlengthsinthedataset. (b)Distributionofplayeractivityovera15-dayperiod. (c)
Eventdistribution,whereeventsbelongingtosimilarsemanticclassesaregroupedtogether. Plotsin(a)and(b)show
datauptothe99thpercentile.
3
ytilibaborP ytilibaborPplayer2vec: ALanguageModelingApproachtoUnderstandPlayerBehaviorinGames APREPRINT
{"app start": {"player_id": 123, "server_time": 0001, ...}}, {"app start": {"player_id": 123, "server_time": 0001, ...}}, {"app start": {"player_id": 123, "client_time": 0001, ...}},
{"app log": {"player_id": 321, "server_time": 0011, ...}}, {"app log": None}, {"app log": None},
{"game start": {"player_id": 123, "server_time": 0011, ...}} {"game start": {"player_id": 123, "server_time": 0011, ...}} {"game start": {"player_id": 123, "game_type": "A", ...}}
Retain relevant events Drop uninformative fields Values type conversion
{"app start": {"player_id": 123, ...}},
"player_123" "app start" "field_1" "value_1" [SEP] "game "player_123": [{"app start": {"field_1": "value_1", ...}}, {"game start": {"player_id": 123,
start" "game_type" "type description" [SEP] {"game start": {"game_type": "type description", ...}} "game_type": "type description", ...}}
Join events Group by player and session, sort by
timestamp
Figure 3: Data preprocessing pipeline. Raw event logs are passed through filtering, type conversion, grouping, and
joiningstagestoproducetextualdata.
Inthiswork,weuseadatasetofplayerbehaviorsessionscollectedfromalargemobilegameproviderover15days
with10,000playersuniformlysampledfromouruserbase.Theresultingdatabaseconsistsof125,000sessions,where
67%isallocatedforthetrainingsplitandtherest33%forthevalidationsplit.
3.2 Preprocessing
PlayerbehavioreventscollectedfromgameapplicationsarecontainedinstructuredJSONlogs. Inordertoleverage
approachesfromnaturallanguagemodeling,wedesignapipelinetotransformraweventsintorichyetcompacttextual
sequencesthatamodelcanadequatelyconsume.
The preprocessing pipeline, as shown in Figure 3, initially eliminates superfluous events and event fields. We build
upon product knowledge to filter out data that is not informative for the player’s behavior, such as device-specific
logs, where many fields are mere artifacts of the application internals. At this stage, the number of event fields is
reduced on average by over 90%. Following, raw numerical values and identifiers are converted to text, assessing a
curateddictionarythatmapsatomicidentitiestodescriptiveterms. Forinstance,allidentifierscorrespondingtosocial
activitiesarecondensedto“social”token. Inthenextstage,eventsaregroupedbyusersandsessionsandorderedby
theirtimestampstopreservethenarrativeflowofinteractions. Finally,individualbehaviorsessionsareconcatenated
toformatextualrepresentationofagameplayer.
We observe a fraction of the events in the raw data do not contain correct chronological ordering. This issue arises
due to the logging platform limitations in situations when players are interacting with the application offline, which
prevents online communications with the game server. We theorize that this type of position noise can hinder the
performanceoflanguagemodel-basedmethods,whichareinductivelybiasedtowardlearningfromconsecutivetokens.
However, to enrich our analysis and maintain a realistic scenario that allows for ordering noise, we chose a dataset
thatunderwentminimalpreprocessing,retainingahigherproportionofmixedeventscomparedtoaversionreduced
tolessthan1%noisebymeansofadditionalpreprocessingsteps.
Wemakeuseofaword-leveltokenizerthatsplitsaspace-separatedinputstringintoindividualtokensandthenmaps
tokenstouniqueidentifiers.Word-leveltokenizationismotivatedbyarelativelysmallvocabularysizeofbehaviordata
(∼13,500tokens)comparedtothevocabulariesofnaturallanguages. However,theresultingtokenizedsequencesare
significantlylongerthanthosethataregenerallyusedinNLPtasks,suchassentimentanalysisandquestionanswering.
3.3 Modeling
Modeling long sequences poses a serious challenge to Transformer-based approaches due to self-attention opera-
tion, which scales quadratically with the input length in memory and computational complexity. As a result, se-
quenceslongerthan512tokensthatarecommonlyfoundinplayerbehaviordataremainoutofreachforconventional
BERT[11]architectureanditsderivatives. Theproblemisfurtherexacerbatedbymodelingdistantdependenciesin
extended playing history that involves concatenating multiple sessions. To address this limitation, we adopt Long-
former[5],amodelthatisspecificallydesignedforprocessinglongdocuments. Longformerreliesonacombination
ofdilatedslidingwindowattentiontocapturelocalcontextandglobalattentiononafewpre-selectedinputlocations.
Thisformulationscaleslinearlywiththeinputsize,enablingtheprocessingofsequencesupto4,096tokensinasingle
pass,whichissufficientinmostbehaviormodelingscenarios. Moreover,Longformer’ssparseattentionpatternexcels
insettingswhereaportionoftokensintheimmediatelocalcontextareexpectedtoberedundant,whichisthecasefor
high-frequencygamingbehaviordata.
4player2vec: ALanguageModelingApproachtoUnderstandPlayerBehaviorinGames APREPRINT
Wevalidatetheproposedapproachusingseveralmodelvariantswithdifferentcapacitiesusingthehyperparameters
listedinTable1. WeexperimentwiththebaselineLongformerconfiguration,denotedasplayer2vec-large,aswellas
twosmallermodelsizevariantswithreducednumbersofinternallayersandself-attentionheads.
Modelvariant Hiddenlayers #Heads Hiddendimensions Blocksize #Param(∼)
player2vec-small 2 2 128 1024 2M
player2vec-medium 6 6 384 2048 20M
player2vec-large 12 12 768 4096 121M
Table1: Modelhyperparameterspermodelsizevariant.
4 Experiments
To ensure a fair comparison in all experiments, we train each network from randomly initialized weights for 100
epochswithabatchsizeof4andgradientaccumulationover4steps,whichresultsinaneffectivebatchsizeof16(216
tokens). Wealsoapplyweightdecayof10−2 onallparametersforregularization. Themodelswereoptimizedwith
MLMobjectivebyAdamalgorithm[12]withafixedlearningrate2×10−5throughouttraining.WeuseHuggingFace
Transformers [32] library and PyTorch framework [22] for model implementation. All networks were trained with
thehalf-precision(FP16)formatonasingleNVIDIAA100GPUaccelerator,wherethelargestmodeltakes≈50hto
train.
4.1 Intrinsicperformance
First,weevaluatethegoodnessoffitoftheproposedapproachwithintrinsicMLMmetrics. Wereportcross-entropy
andmulti-classclassificationaccuracyofpredictingmaskedtokenscomputedonthevalidationsplitfortestedmodel
architecturesinTable2. WealsoreporttheperplexityscorefollowingtheexistingmethodologyforevaluatingMLM
pretrainingperformance[15]. Asexpected,weobservethatalargercapacitymodelisabletofitthebehaviorsessions
moreaccuratelywhilenotenteringtheoverfittingregime.
Modelvariant Blocksize Accuracy↑ Perplexity↓ Cross-entropy↓
player2vec-small 1024 0.698±0.067 3.272±0.714 1.161±0.222
player2vec-medium 2048 0.934±0.015 1.287±0.093 0.250±0.069
player2vec-large 4096 0.958±0.007 1.161±0.046 0.149±0.040
Table2: Maskedlanguagemodelingintrinsicmetricsmeanvaluesandstandarddeviationscomputedover5training
runs.
4.2 Clusteranalysis
Next,weperformqualitativeanalysisbyvisualizingat-SNE[30]plotofthelearnedlatentspacetoidentifyclusters
that are representative of player behavior. To this end, we extract embeddings of input token sequences from the
converged player2vec large variant that showed the strongest performance with respect to MLM accuracy. We use
4096by768-dimensionalrepresentationsproducedbythelastTransformerlayer,whichisfurtheraggregatedwithmax
poolingoperationoversequencelengthtocomputeanembeddingvectorforaplayerinputsequence. Weadditionally
projectobtainedsessionembeddingsontothefirst50principalcomponentswithlinearPCAmethodtosuppressnoise
and speed up computation. The obtained projections are then mapped to 2D space via t-SNE and clustered using
GaussianMixtureModel[25](8components). Theresultingt-SNEplotisdepictedinFigure4a.
Analyzing the average player behavior in the well-separated t-SNE clusters in Figure 4b, we rediscover known seg-
mentsintheplayers’populationfromtheproductpointofview:
1. competitivedevoted: ahighlyskilledplayerwhoplayslessfrequentlyonadailybasisbutforlongsessions,
occasionallypurchasesitems,andcollectsutilities.
2. casualdevoted–aresourcefulplayerwhoplayslongsessionsbutnotfrequently, engagesin-gamequests,
enjoyscollectingutilitiesandrewards,butprefersthefreegameplayexperience.
3. persistent devoted – a resourceful player who plays frequent and long sessions while enjoying the free
gameplayexperience.
5player2vec: ALanguageModelingApproachtoUnderstandPlayerBehaviorinGames APREPRINT
Cluster
1
2
3
4
5
6
7
8
(a) (b)
Figure 4: (a) t-SNE of latent embedding space obtained from pre-trained player2vec-large with subsequent GMM
clustering. (b)Histogramofthequantizedplayereventsinidentifiedclusters. Weexcludecluster8duetothesmall
clustersizeandnogameplay.
4. lean-in casual economy aware – a skilled player who plays less often but longer sessions, collects and
occasionallypurchasesitems.
5. lean-incasual–askilledplayerwhoplayslessoftenbuthaslongersessions.
6. persistentcasual–anotveryskillfulplayerwhoplaysshortandfrequentsessionswhilebeinglessengaged
insocialandeconomicaspectsofthegame.
7. persistentcollector–aplayerwithfrequentshortsessions,collectingutilitiestopassthelevels.
Weconcludethatthemaincomponentsofvariationintheembeddingspacemeaningfullycorrespondtothehigh-level
player behavior segmentation. This observation underscores the potential utility of the embeddings for downstream
tasks. Furthermore,thistypeofanalysiscannotcapturetheentireessenceofaplayerduetosomeattributesnotbeing
clearly presented by a set of features. We will extend the features to reflect features highlighted in player insights
studies. Regardless,thismethodofexaminingplayersegmentsoffersaninterestingperspectivetogamedesignersand
callsforadeeperunderstandingofusers’behaviorandmotivation.
5 EthicalConsiderations
Computational modeling of players in games has raised numerous concerns in research and industrial communi-
ties[19]. Inthiswork,weusenon-personallyidentifiabletrackingdatarecordedfromin-gameinteractionstocreate
vectorized representations of player behaviors. Our goal is to leverage such representations to support personalized
and enhanced player experiences. We identify potential ethical risks that can arise from i) bias in the input dataset,
e.g.,underrepresentinglessfrequentplayersdata,orii)misapplicationofwell-validatedmodelstoadifferentdatadis-
tribution,e.g.,usingamodeltrainedonexpertplayersfornewplayers,knownasTypeIIIerrors[19]. Totacklethose
challenges,weleveragetoolingfordatavalidationandautomatedmodelanalysis,whichareavailableinproduction-
ready machine learning frameworks [21]. Additionally, we capture underrepresented or misrepresented player be-
haviorbyemployingqualitativeevaluationmethodssuchasembeddingspacevisualization. Weaddressdistribution
shifts by periodically retraining the model with recent data, where the retraining cadence is determined empirically
based on performance and distribution drift. We envision applying model explainability and uncertainty estimation
methods on the downstream recommendation system to understand better the model’s robustness, biases, and other
ethicalconsiderations.
6 ConclusionandFutureWork
Thispaperintroducesanoveluserbehaviormodelingapproachinspiredbylanguagemodelingprinciples.Togenerate
playerembeddings,weutilizetrackingeventsthatformsessions,mirroringthewayhowwordtokensinNLPcompose
6player2vec: ALanguageModelingApproachtoUnderstandPlayerBehaviorinGames APREPRINT
sentences and paragraphs. We showcase a method for modeling player behavior data self-supervised by pretraining
aTransformer-basedarchitecturewithlongcontextlengthonadatasetoftrackingeventsinthegamingdomain. We
experimentallydemonstratedtheefficacyoftheproposedpipelinebyevaluatingintrinsicMLMmetrics.Moreover,we
qualitativelyanalyzedtheemergingstructureofthelearnedembeddingspaceextractedfromapre-trainedmodel. We
showedtheexistenceofsemanticstructures,suchasclustersofusersbasedontheirplaystylesandin-gamespending
behavior,thatfurthersupporttheviabilityoftheproposedmethodforplayerrepresentationlearning. Wediscovered
previouslyunknownusersubpopulationsthatserveasvaluableinsightsintodownstreamproductapplications.
Forthenextsteps,weplantoextendthetrainingproceduretosingle-andmultitaskfine-tuningwithlabeleddatasets
tobenchmarkthemodelperformanceagainstfully-supervisedbaselines. Furthermore,infuturework,wewillfocus
on quantifying and mitigating the effect of position-based noise on representation learning tasks that are commonly
presentinmobilegamesettings.
Acknowledgements
The authors would like to thank Gabriela Zarzar Gandler and Bjo¨rn Brinne, who helped with the conceptualization
of the project in the early phases, and Labinot Polisi, Martin Lundholm, and Dionysis Varelas, who assisted with
acquiringplayerbehaviordatainaliveproductionenvironment.
References
[1] AHN, J., HWANG, J., KIM, D., CHOI, H., AND KANG, S. A survey on churn analysis in various business
domains. IEEEAccess8(2020),220816–220839.
[2] ALY, M., HATCH, A., JOSIFOVSKI, V., AND NARAYANAN, V. K. Web-scaleusermodelingfortargeting. In
Proceedingsofthe21stinternationalconferenceonworldwideweb(2012),pp.3–12.
[3] ANDERSON, A., KUMAR, R., TOMKINS, A., AND VASSILVITSKII, S. Thedynamicsofrepeatconsumption.
InProceedingsofthe23rdinternationalconferenceonWorldwideweb(2014),pp.419–430.
[4] BAUER,J.,ANDJANNACH,D.Improvedcustomerlifetimevaluepredictionwithsequence-to-sequencelearning
andfeature-basedmodels. ACMTransactionsonKnowledgeDiscoveryfromData(TKDD)15,5(2021),1–37.
[5] BELTAGY, I., PETERS, M. E., AND COHAN, A. Longformer: Thelong-documenttransformer. arXivpreprint
arXiv:2004.05150(2020).
[6] DRACHEN, A., SIFA, R., BAUCKHAGE, C., AND THURAU, C. Guns, swords and data: Clustering of player
behavior in computer games in the wild. In 2012 IEEE conference on Computational Intelligence and Games
(CIG)(2012),IEEE,pp.163–170.
[7] DRACHEN,A.,THURAU,C.,TOGELIUS,J.,YANNAKAKIS,G.N.,ANDBAUCKHAGE,C. Gamedatamining.
Gameanalytics: Maximizingthevalueofplayerdata(2013),205–253.
[8] FEELY, C., CAULFIELD, B., LAWLOR, A., AND SMYTH, B. Modelling the training practices of recreational
marathonrunnerstomakepersonalisedtrainingrecommendations. InProceedingsofthe31stACMConference
onUserModeling,AdaptationandPersonalization(2023),pp.183–193.
[9] HANSEN, C., HANSEN, C., MAYSTRE, L., MEHROTRA, R., BROST, B., TOMASI, F., AND LALMAS, M.
Contextualandsequentialuserembeddingsforlarge-scalemusicrecommendation. InProceedingsofthe14th
ACMConferenceonRecommenderSystems(2020),pp.53–62.
[10] HARIRI, N., MOBASHER, B., AND BURKE, R. Context-aware music recommendation based on latenttopic
sequentialpatterns. InProceedingsofthesixthACMconferenceonRecommendersystems(2012),pp.131–138.
[11] KENTON, J. D. M.-W. C., AND TOUTANOVA, L. K. Bert: Pre-trainingofdeepbidirectionaltransformersfor
languageunderstanding. InProceedingsofnaacL-HLT (2019),vol.1,p.2.
[12] KINGMA,D. Adam: amethodforstochasticoptimization. InIntConfLearnRepresent(2014).
[13] KUMAR,S.,ZHANG,X.,ANDLESKOVEC,J. Predictingdynamicembeddingtrajectoryintemporalinteraction
networks. InProceedingsofthe25thACMSIGKDDinternationalconferenceonknowledgediscovery&data
mining(2019),pp.1269–1278.
[14] LEWIS, M., LIU, Y., GOYAL, N., GHAZVININEJAD, M., RAHMAN MOHAMED, A., LEVY, O., STOYANOV,
V.,ANDZETTLEMOYER,L.Bart:Denoisingsequence-to-sequencepre-trainingfornaturallanguagegeneration,
translation,andcomprehension. InAnnualMeetingoftheAssociationforComputationalLinguistics(2019).
7player2vec: ALanguageModelingApproachtoUnderstandPlayerBehaviorinGames APREPRINT
[15] LIU, Y., OTT, M., GOYAL, N., DU, J., JOSHI, M., CHEN, D., LEVY, O., LEWIS, M., ZETTLEMOYER, L.,
ANDSTOYANOV,V. Roberta: Arobustlyoptimizedbertpretrainingapproach.
[16] LU,S.,DOU,Z.,XIONG,C.,WANG,X.,ANDWEN,J.-R. Knowledgeenhancedpersonalizedsearch. InPro-
ceedingsofthe43rdInternationalACMSIGIRconferenceonresearchanddevelopmentininformationretrieval
(2020),pp.709–718.
[17] LUO, R., WANG, T., DENG, J., AND WAN, P. Mcm: Amulti-taskpre-trainedcustomermodelforpersonaliza-
tion. InProceedingsofthe17thACMConferenceonRecommenderSystems(2023),pp.637–639.
[18] MEHROTRA, R., MCINERNEY, J., BOUCHARD, H., LALMAS, M., AND DIAZ, F. Towardsafairmarketplace:
Counterfactualevaluationofthetrade-offbetweenrelevance,fairness&satisfactioninrecommendationsystems.
In Proceedings of the 27th acm international conference on information and knowledge management (2018),
pp.2243–2251.
[19] MIKKELSEN, B., HOLMGA˚RD, C., AND TOGELIUS, J. Ethicalconsiderationsforplayermodeling. InWork-
shopsattheThirty-FirstAAAIConferenceonArtificialIntelligence(2017).
[20] MIKOLOV,T.,CHEN,K.,CORRADO,G.,ANDDEAN,J. Efficientestimationofwordrepresentationsinvector
space. arXivpreprintarXiv:1301.3781(2013).
[21] MODI, A. N., KOO, C. Y., FOO, C. Y., MEWALD, C., BAYLOR, D. M., BRECK, E., CHENG, H.-T.,
WILKIEWICZ, J., KOC, L., LEW, L., ZINKEVICH, M. A., WICKE, M., ISPIR, M., POLYZOTIS, N., FIEDEL,
N., HAYKAL, S. E., WHANG, S., ROY, S., RAMESH, S., JAIN, V., ZHANG, X., AND HAQUE, Z. Tfx: A
tensorflow-basedproduction-scalemachinelearningplatform. InKDD2017(2017).
[22] PASZKE, A., GROSS, S., MASSA, F., LERER, A., BRADBURY, J., CHANAN, G., KILLEEN, T., LIN, Z.,
GIMELSHEIN, N., ANTIGA, L., ET AL. Pytorch: Animperativestyle,high-performancedeeplearninglibrary.
Advancesinneuralinformationprocessingsystems32(2019).
[23] PEDERSEN,C.,TOGELIUS,J.,ANDYANNAKAKIS,G.N. Modelingplayerexperienceinsupermariobros. In
2009IEEESymposiumonComputationalIntelligenceandGames(2009),IEEE,pp.132–139.
[24] REN, P., CHEN, Z., LI, J., REN, Z., MA, J., AND DE RIJKE, M. Repeatnet: A repeat aware neural recom-
mendation machine for session-based recommendation. In Proceedings of the AAAI Conference on Artificial
Intelligence(2019),vol.33,pp.4806–4813.
[25] REYNOLDS,D.A.,ETAL. Gaussianmixturemodels. Encyclopediaofbiometrics741,659-663(2009).
[26] SHANI, G., HECKERMAN, D., BRAFMAN, R. I., AND BOUTILIER, C. Anmdp-basedrecommendersystem.
JournalofMachineLearningResearch6,9(2005).
[27] SIQUEIRA, E. S., CASTANHO, C. D., RODRIGUES, G. N., AND JACOBI, R. P. A data analysis of player in
worldofwarcraftusinggamedatamining. In201716thBrazilianSymposiumonComputerGamesandDigital
Entertainment(SBGames)(2017),IEEE,pp.1–9.
[28] SUN,F.,LIU,J.,WU,J.,PEI,C.,LIN,X.,OU,W.,ANDJIANG,P.Bert4rec:Sequentialrecommendationwith
bidirectionalencoderrepresentationsfromtransformer.InProceedingsofthe28thACMinternationalconference
oninformationandknowledgemanagement(2019),pp.1441–1450.
[29] TIAN,L.,YANG,B.,YIN,X.,ANDSU,Y. Asurveyofpersonalizedrecommendationbasedonmachinelearn-
ingalgorithms. InProceedingsofthe20204thInternationalConferenceonElectronicInformationTechnology
andComputerEngineering(2020),pp.602–610.
[30] VAN DER MAATEN, L., AND HINTON, G. Visualizingdatausingt-sne. Journalofmachinelearningresearch
9,11(2008).
[31] VASWANI, A., SHAZEER, N., PARMAR, N., USZKOREIT, J., JONES, L., GOMEZ, A. N., KAISER, Ł., AND
POLOSUKHIN,I. Attentionisallyouneed. Advancesinneuralinformationprocessingsystems30(2017).
[32] WOLF,T., DEBUT, L.,SANH,V., CHAUMOND,J., DELANGUE,C., MOI,A., CISTAC,P., RAULT, T.,LOUF,
R., FUNTOWICZ, M., ET AL. Huggingface’stransformers: State-of-the-artnaturallanguageprocessing. arXiv
preprintarXiv:1910.03771(2019).
[33] WU, C.-Y., AHMED, A., BEUTEL, A., SMOLA, A. J., AND JING, H. Recurrentrecommendernetworks. In
ProceedingsofthetenthACMinternationalconferenceonwebsearchanddatamining(2017),pp.495–503.
[34] YANNAKAKIS, G. N., SPRONCK, P., LOIACONO, D., AND ANDRE´, E. Playermodeling. ArtificialandCom-
putationalIntelligenceinGames(2013),45.
[35] ZHOU, C., BAI, J., SONG, J., LIU, X., ZHAO, Z., CHEN, X., AND GAO, J. Atrank: An attention-based
user behavior modeling framework for recommendation. In Proceedings of the AAAI conference on artificial
intelligence(2018),vol.32.
8player2vec: ALanguageModelingApproachtoUnderstandPlayerBehaviorinGames APREPRINT
[36] ZIMDARS, A., CHICKERING, D. M., AND MEEK, C. Using temporal data for making recommendations. In
ProceedingsoftheSeventeenthconferenceonUncertaintyinartificialintelligence(2001),pp.580–588.
9