Demystifying Functional Random
Forests: Novel Explainability Tools for
Model Transparency in
High-Dimensional Spaces
Fabrizio Maturo1* and Annamaria Porreca2†
1*Department of Economics, Statistics and Business, Faculty of
Technological and Innovation Sciences, Piazza Mattei, 10, Rome, 66100,
Italy.
2Department of Economics, Statistics and Business, Faculty of
Economics and Law, Piazza Mattei, 10, Rome, 66100, Italy.
*Corresponding author(s). E-mail(s): fabrizio.maturo@unimercatorum.it;
Contributing authors: annamaria.porreca@unimercatorum.it;
†These authors contributed equally to this work.
Abstract
The advent of big data has raised significant challenges in analysing high-
dimensional datasets across various domains such as medicine, ecology, and
economics.FunctionalDataAnalysis(FDA)hasproventobearobustframework
for addressing these challenges, enabling the transformation of high-dimensional
data into functional forms that capture intricate temporal and spatial patterns.
However,despiteadvancementsinfunctionalclassificationmethodsandveryhigh
performance demonstrated by combining FDA and ensemble methods, a critical
gappersistsintheliteratureconcerningthetransparencyandinterpretabilityof
black-box models, e.g. Functional Random Forests (FRF). In response to this
need,thispaperintroducesanovelsuiteofexplainabilitytoolstoilluminatethe
innermechanismsofFRF.WeproposeusingFunctionalPartialDependencePlots
(FPDPs), Functional Principal Component (FPC) Probability Heatmaps, vari-
ous model-specific and model-agnostic FPCs’ importance metrics, and the FPC
Internal-External Importance and Explained Variance Bubble Plot. These tools
collectively enhance the transparency of FRF models by providing a detailed
analysis of how individual FPCs contribute to model predictions. By applying
1
4202
guA
22
]LM.tats[
1v88221.8042:viXrathesemethodstoanECGdataset,wedemonstratetheeffectivenessofthesetools
in revealing critical patterns and improving the explainability of FRF.
Keywords:FunctionalDataAnalysis,RandomForests,Explainability,
High-DimensionalData,FunctionalPrincipalComponents,ModelTransparency,
SupervisedClassification,FeatureImportance
1 Introduction
In the era of big data, analyzing high-dimensional datasets has become a critical
challengeacrossvariousfields,includingmedicine,ecology,andeconomics.Theexplo-
sion of data availability has provided outstanding opportunities for insight, but it has
also introduced significant analytical challenges, mainly when dealing with the curse
of dimensionality. This phenomenon, where the volume of data points grows expo-
nentially with the number of dimensions, complicates extracting meaningful patterns
and relationships using traditional statistical methods. Furthermore, when data are
observed over time or across spatial dimensions, these challenges intensify due to the
interconnected nature of the observations.
FunctionalDataAnalysis(FDA)hasemergedasapowerfulsolutiontothesechal-
lengesbytransforminghigh-dimensionaldataintofunctionalforms,whichcancapture
complex temporal and spatial patterns more coherently and interpretably [1, 2]. FDA
treats data as continuous curves rather than discrete points, enabling data analysis
that varies over a continuum [2, 3]. By focusing on the underlying functional struc-
ture, FDA reduces dimensionality while preserving essential information, making it a
robust tool for exploring complex datasets [4–6].
Overthepasttwodecades,FDAhasseenconsiderableadvancementsandhasbeen
applied successfully in a wide range of fields, particularly in biomedical applications.
For instance, FDA has been used to analyze electrocardiogram (ECG) data, where
the continuous monitoring of heart activity over time results in functional data well-
suited to FDA techniques (see e.g., [7]). In these applications, FDA helps not only
reduce dimensionality but also uncover critical patterns that are crucial for accurate
classification and diagnosis.
In the FDA literature, there are many supervised classification methods, mostly
obtained as extensions of classification methods to the case of high dimensionality.
In recent literature, also thanks to the success of machine learning methods in terms
of performance, a new line of research is emerging that combines FDA and ensemble
methods.Oneofthemostpromisingmethodsforthesupervisedclassificationoffunc-
tionaldataistheFunctionalRandomForest(FRF),whichintegratestheflexibilityof
random forests with the analytical strengths of FDA [7, 8]. FRFs have demonstrated
impressive performance in handling high-dimensional functional data for many appli-
cations. However, despite their effectiveness, FRF is a black-box model. Despite the
excellent performances in accuracy, sensitivity, specificity, and AUC, today, from a
statistical point of view, it leaves some doubts about the difficulties in explaining the
results. The need for transparency and interpretability in machine learning models
2is increasingly recognized, especially in critical fields such as healthcare and envi-
ronmental science, where decisions based on model predictions can have far-reaching
consequences. This is why medical doctors or entrepreneurs often prefer simpler but
more interpretable methods. For these reasons, the debate between black-box and
transparent-box models remains a central and dynamic topic in contemporary statis-
tical and computer science literature. In the era of big data, the quest for adequate
interpretability and explainability methods has become one of the most pressing and
significant challenges.
Focusing on the case of random forest for functional data, trained on FPCs (or
equivalently on the coefficients of a fixed bases expansion), the lack of transparency
makes it difficult to interpret how individual features contribute to the model’s pre-
dictions. In response to this need, this paper presents a novel set of explainability
tools designed to enhance the interpretability of FRF. These tools aim to bridge the
gap between model accuracy and understanding, providing users with the means to
demystify the decision-making process of FRFs.
The key contributions of this work include the development of Functional Partial
Dependence Plots (FPDPs), which illustrate the influence of individual FPCs on the
model’spredictions,andFPC Probability Heatmaps,whichvisualizestraightforwardly
how changes in FPC scores affect the predicted probabilities for different classes.
Besides, we present various model-specific and model-agnostic FPC importance met-
rics, including the FPC Internal-External Importance and Explained Variance Bubble
Plot, which provides a comprehensive view of the significance of each FPC from both
internal and external perspectives.
These methods are demonstrated through an application to an ECG dataset, a
classic example of functional data where the shape and variability of heart activity
curves are crucial for accurate diagnosis. By applying these explainability tools, we
show that it is possible to gain deep insights into how individual FPCs influence
the classification of ECG signals, thereby enhancing the understanding of the FRF
model. In summary, this paper makes a fascinating contribution to the FDA field by
addressing a critical gap in the explainability of FRF. The novel explainability tools
introducedhereofferapathwaytogreatertransparency,makingthesepowerfulmodels
more accessible and trustworthy for practitioners across various domains.
Theremainderofthispaperisstructuredasfollows.Section2outlinesthefounda-
tional concepts of FDA, Functional Classification Trees (FCTs), and FRF. Section 3
presentstheinnovativeexplainabilitytoolsdevelopedinthisstudy,suchasFPDPsand
the FPC Internal-External Importance and Explained Variance Bubble Plot, among
others.Section4appliesthesemethodstoanECGdataset,demonstratingtheirprac-
tical utility and explaining their functionality in detail. Finally, Section 5 concludes
with a discussion of the results and potential directions for future research.
2 Material and methods preliminaries
2.1 Functional Data Analysis (FDA)
Functional Data Analysis (FDA) focuses on treating data as functions, viewing each
function as a single entity. In practice, however, these functional data are often
3observed as discrete points, reducing the function z = f(x) to a series of discrete
observations,representedaspairs(x ,z )wherex ∈ℜandz arethefunctionvalues
j j j j
at the points x , for j =1,2,...,T [9].
j
InFDA,weconsiderafunctionalvariableX asarandomvariablethattakesvalues
inafunctionalspaceΞ,withafunctionaldatasetbeingasample{x ,...,x },denoted
1 N
as x (t),...,x (t), drawn from this functional variable X [1]. When considering a
1 N
Hilbert space with a metric d(·,·) associated with a norm, where d(x (t),x (t)) =
1 2
∥x (t)−x (t)∥ and the norm ∥·∥ is linked to an inner product ⟨·,·⟩, a specific case is
1 2
thespaceL ,whichconsistsofsquare-integrablefunctionsdefinedonτ.Inthisspace,
2
if x(t)∈L , a basis system ϕ (t) can be used to represent functions as a finite linear
2 j
combination of basis functions.
ThefirststepinFDAisconvertingobservedvaluesz ,z ,...,z intoafunctional
i1 i2 iT
form. The most common method for estimating the functional data is using a fixed-
basis approximation, where the function x (t) is approximated as:
i
S
(cid:88)
x (t)≈ c ϕ (t), (1)
i is s
s=1
where c is the coefficient in the linear combination of the basis functions ϕ (t) and
is s
subject i.
Alternatively, Functional Principal Components (FPCs) offer a data-driven
approach that reduces dimensionality while effectively preserving the essential infor-
mation content of the data. The functional data can then be approximated as:
K
(cid:88)
x (t)≈ ν ξ (t), (2)
i ik k
k=1
where ν are the FPC scores, and ξ (t) are the principal components. By truncating
ik k
this representation to the first p FPCs, the sample curves can be approximated, with
the explained variance given by
(cid:80)p
λ , where λ is the variance of the k-th FPC.
k=1 k k
FPCs decompose the functional data into a series of orthogonal components
that capture the maximum variance in the data. Mathematically, for a functional
observation x (t), the FPC decomposition can be also expressed as:
i
K
(cid:88)
x (t)=µ(t)+ ν ξ (t)+ϵ (t), (3)
i ik k i
k=1
where µ(t) is the mean function across all observations, ξ (t) is the k-th FPC, ν
k ik
are the scores associated with each principal component for the i-th observation, ϵ (t)
i
is the residual noise term. The enormous advantage of using this decomposition in
supervised classification lies in its ability to filter out noise, achieve dimensionality
reduction,andproduceorthogonalfeatures.Bydoingso,itaddressesmanychallenges
associated with multicollinearity in machine learning, leading to more robust and
interpretable models.
Proximity measures between functions are crucial in FDA, with the L -distance
2
being the most commonly used:
4(cid:26) 1 (cid:90) (cid:27)1/2
∥x 1(t)−x 2(t)∥ 2 = (cid:82) w(t)dt |x 1(t)−x 2(t)|2w(t)dt , (4)
τ τ
where w(t) is a positive weight function. Semi-metrics, such as the r-order derivative
semi-metric, can also be used to capture more detailed information, and the FPC-
based semi-metric is particularly useful for dimensionality reduction and interpreting
similarity between functional data [4, 6, 9, 10].
2.1.1 Functional Classification Trees (FCTs)
Infunctionalclassification,thegoalistopredicttheclassorlabelY ofanobservation
X byconstructingamappingf :Ξ−→{0,1,...,U},calledaclassifier.Thisclassifier
assigns a predicted label to x with an error probability P{f(X)̸=Y}.
The continuous domain T can represent different aspects, such as time or space.
While this study focuses on the time domain, the approach can be extended to
other domains. The response could be categorical or numerical, leading to classifica-
tion or regression problems. Here, we concentrate on scalar-on-function classification,
particularly focusing on the so-called Functional Random Forest (FRF) [7].
Functional Classification Trees (FCTs) form the foundation of the FRF method,
which is essentially an ensemble of weak FCTs. Therefore, to understand the FRF
method fully, it is crucial to first illustrate how FCTs operate within the context of
FDA. Classification Trees (CT) are a supervised learning method used to predict a
categorical response by learning decision rules from features [11–13]. They can be
extended to the context of FDA by using the coefficients of a basis representation as
features to train the functional classifier. This method is termed the Functional Clas-
sificationTreeclassifierandisfullyillustratedinMaturoandVerde[14],inparticular
regarding the meaning of the splitting rules in the functional context.
For a fixed basis system, such as that in Equation 1, the feature matrix C is
structured as follows:
 
c ... c
11 1S
C=

. .
.
... . .
.
 , (5)
c ... c
N1 NS
wherec representsthecoefficientofthei-thcurve(i=1,...,N)relativetothes-th
is
basis function ϕ (t).
s
Alternatively, for a data-driven basis system, as shown in Equation 2, the feature
matrix V is defined as:
 
ν ... ν
11 1K
V=

. .
.
... . .
.
 , (6)
ν ... ν
N1 NK
where ν is the score of the i-th curve (i = 1,...,N) corresponding to the k-th
ik
functional principal component ξ (k =1,...,K).
k
Inthisstudy,wefocusonFCTstrainedusingFPCs,whichinvolvesrecursivebinary
partitioning of the feature space into regions (terminal nodes or leaves) composed
5of sets of functions x (t) ∈ X. To construct FCTs, the algorithm optimizes a cost
i
criterion (e.g., Gini index or Shannon-Weiner entropy index) [13, 15] at each step to
achieve the best binary partition. The process begins with the complete set of FPCs’
scores from Equation 6 and continues until terminal nodes are reached. Initially, a
large FCT is generated, then pruned to balance complexity and accuracy.
As the FPCs’ scores are used as features to predict Y, interpreting FCTs dif-
fers from traditional CTs. The split values must be interpreted based on the domain
portions represented by specific FPCs ξ (t) and the score thresholds. The scores’
k
thresholdsdeterminewhethercurvesbelongtoonesubsetoranotherbasedonwhether
the score ν is above or below this threshold (further details in [7, 8]).
ik
2.1.2 Functional Random Forest (FRF)
Classical Random Forest (RF) [16] is a powerful machine learning algorithm, which is
an extension of the bagging technique applied to decision trees. In RF, at each split
during the tree-building process, a random subset of predictors is selected from the
entire set. This random selection helps to decorrelate the trees and reduce the overall
modelvariance,improvinguponstandardbaggingwherealltreescouldbedominated
by the same strong predictor. In FDA, a similar issue arises in bagging using FPCs
becauseeventhoughthefunctionaldatasetisbootstrapped,theresultingFCTsofthe
ensemblecanbehighlycorrelated.ThisisbecausethesameFPCsoftendominatethe
top splitting rules across multiple trees, leading to a lower reduction in variance.
FRF addresses this issue by further decorrelating the trees. Instead of using all
availableFPCsateachsplit,FRFrandomlyselectsasubsetofmFPCsascandidates
for splitting, from the total set of K FPCs. When m < K, the method is referred
to as FRF; when m = K, FRF reduces to bagging. By reducing the correlation
betweenFCTs,FRFachievesgreatervariancereductionandmorerobustpredictions.
√
In practice, a common heuristic is to set m ≈ K. This ensures that, on average, a
significant portion of FPCs are not considered at each split, further decorrelating the
trees and improving the reliability of the averaged predictions from the forest.
InFRF,thepredictionforanobservationx (t)ismadebyaggregatingthepredic-
i
tions from all the individual FCTs within the ensemble. The final decision is typically
made by majority voting, where the most frequent class label predicted by the FCTs
is selected as the final prediction. The decision rule for predicting the class label δ for
an observation x (t) can be expressed as:
i
(cid:110) (cid:111)
fˆ(m)(x (t))=mode fˆ(1)(x (t)),fˆ(2)(x (t)),...,fˆ(M)(x (t)) (7)
rf i i i i
where fˆ(m)(x (t)) is the predicted label by the FRF for the observation x (t),
rf i i
fˆ(j)(x (t)) is the predicted label by the j-th FCT in the ensemble, M is the total
i
number of trees in the ensemble, and the mode function selects the most frequently
predicted class among the predictions from all M trees.
Unlike individual FCTs, which might require pruning to prevent overfitting, FRF
benefit from the combined effects of bootstrap aggregation and feature selection at
each split, reducing the likelihood of overfitting even without pruning. Consequently,
6in FRF, FCTs are not pruned, in contrast to the approach used for individual FCTs
discussed earlier.
The key to the success of FRF lies in the diversity of the individual FCTs in
the ensemble. This diversity is achieved through double randomization of units and
features. By reducing the correlation between the FCTs, FRF ensures that each FCT
captures different aspects of the data, leading to a more robust and accurate final
model, improving its generalization ability.
3 Novel Explainability Tools for Functional Random
Forest
3.1 The Intuition Behind the Proposed Explainability Tools
The intuition behind explainability using Functional Partial Dependence Plots
(FPDPs)andFunctionalPrincipalComponentProbabilityHeatmap(FPCPH),inthe
FDAcontext,liesintheabilitytostudythebehaviourofreconstructedcurvesthrough
a single FPC, while keeping all other components fixed. This approach allows us to
observehowvariationsinthecoefficientofthelinearcombinationaffectthefunctional
data reconstruction.
In other words, we can analyze the behaviour of curves’ reconstruction when only
one component’s score is altered while the scores of the other components remain
constant. Similarly, another possibility is to study what happens to the curve when
it is reconstructed using only one FPC at a time, varying the score of that single
componentwhilesettingallotherscorestozero.Thisapproachallowsustoisolateand
understandthespecificcontributionofeachFPCtotheoverallshapeofthefunctional
data. By analyzing the curve in this manner, we can gain insights into how individual
FPCs influence the reconstructed data and how their variations impact the overall
functional form.
When reconstructing the functional data x (t) using only a single FPC, say the
i
k-th component, and ignoring all other components, the approximation xˆ (t;ν ) can
i ik
be expressed as:
xˆ (t;ν )=ν ξ (t), (8)
i ik ik k
where ν is the score associated with the k-th FPC, and ξ (t) is the k-th FPC. It
ik k
followsthatthereconstructionxˆ (t;ν )inEquation8considerssolelythecontribution
i ik
of the k-th FPC, without involving the other FPCs. This simplified representation is
valuable for understanding how the variation in a single score ν affects the shape of
ik
the reconstructed function xˆ (t). By varying ν while assuming the other scores are
i ik
zero (i.e., ν = 0 for all j ̸= k), we can observe the isolated effect of the k-th FPC
ij
ξ (t) on the reconstruction of the curve x (t).
k i
Mathematically, this means that the reconstruction of the functional data x (t) is
i
entirelydeterminedbythek-thprincipalcomponentξ (t)anditscorrespondingscore
k
ν . As ν varies, the shape of the function xˆ (t) will vary according to the k-th FPC
ik ik i
ξ (t).Thisconceptiscrucialwhenmovingontointerprettheexplainabilitymeasures
k
in the subsequent subsections, where the goal is to understand how individual FPCs
7contributetothevariabilityoftheresponsevariable.Theisolatedvariationofasingle
FPC provides a clear understanding of how each component can influence the model
output, thereby forming the basis for interpreting explainability metrics and tools.
To illustrate this concept, we use the ECG200 dataset [17], which we will also
employintheapplicationsection.WecomputeFPCsonthisdatasetandshowhowthe
FPCs derived from the ECG data contribute to the reconstruction of the signals and
how the variation in individual FPC scores affects the reconstructed curves. Figure 1
demonstrates this concept by displaying how, for example, the first four FPCs change
as their scores vary, thereby providing insight into the contribution of each FPC to
theoverallvariationinthedata.Thisvisualizationhelpsinunderstandingtheimpact
ofeachFPConthefunctionaldata,facilitatingaclearerinterpretationofthemodel’s
behavior in terms of the principal components.
Byisolatingtheeffectofonecomponentatatime,wecanobservehowituniquely
contributes to the overall shape and behavior of the functional data. This under-
standing is crucial because it provides a foundation for interpreting more complex
explainability measures. In the next section, these measures aim to go beyond the
mere reconstruction of data; they seek to explain the contribution of specific features
to the response variable in predictive models. Essentially, if we grasp how each FPC
score affects the data reconstruction, we can extend this insight to understand how
each feature influences the model’s output. This knowledge is not just theoretical
but practical, particularly in fields where model transparency and interpretability are
essential. For example, in predictive modeling, it’s important to identify which fea-
turesaredrivingpredictionsandtowhatextent.Theabilitytoisolateandunderstand
the impact of individual components or features enables us to build models that are
not only accurate but also interpretable and trustworthy.
Alternatively,tosimplifytheinterpretationofhowvariationsinasingleFPCscore
affect the reconstructed curve, we can use 2D plots that stratify the curves based on
rangesofFPCscores.Figure2showsanexampleofthisapproach,wherewevisualize
thefirst FPC.Thereconstructedcurves are visualisedto reflectthedifferentintervals
of the first FPC scores. The shaded regions highlight the range of variability within
specific score intervals, providing a visual stratification of the FPC’s influence on the
functional data. This approach allows for a clearer interpretation of how variations in
the FPC score can alter the reconstructed curve shapes across different regions of the
domain. The colored bands in the figure represent the potential range of curve values
corresponding to each score interval. By focusing on these shaded areas, it becomes
evident how the first FPC contributes to the overall shape of the functional data.
For instance, the red band, representing higher score intervals, shows a significant
deviation from the mean curve, indicating that higher scores have a stronger impact
on the curve’s amplitude and shape in the the central part of the domain, but not
only. Moreover, the black dashed line represents the mean curve, where the FPCS
scores are zero, serving as a reference point. The variation around this mean curve
across different score intervals reveals how specific functional features are emphasized
or diminished depending on the score, aiding in the interpretation of the FPC’s role
in the data.
8(a) First FPC variation. (b) Second FPC variation.
(c) Third FPC variation. (d) Fourth FPC variation.
Fig. 1: Variation of the first four Functional Principal Components (FPCs) with
changes in their scores.
3.2 Functional Partial Dependence Plots (FPDPs)
This research proposes the concept of Functional Partial Dependence Plots (FPDPs)
and thus extends the concept of Partial Dependence Plots (PDPs) [18] to functional
data. The aim is to visualise the marginal effect of a single FPC on the predicted
probability of a binary outcome. FPDPs illustrate how changes in a specific FPC
(whileholdingotherfeaturesconstant)influencetheprobabilityofthepredictedclass.
This allows us to assess which ranges of the FPCs’ score values increase or decrease
the probability of belonging to a specific class. These plots can help us understand
the functional model’s behavior and how individual FPCs contribute to the final
prediction.
Classical PDPs are used to visualize the marginal effect of a single feature on the
predicted outcome of a machine learning model, such as a random forest or gradient
boosting machine. The idea behind PDPs is to show how the model’s predictions
changeasthevalueofaparticularfeaturevaries,whileaveragingouttheeffectsofall
9Fig. 2: Variation of the reconstructed curves using the First Functional Principal
Components (FPC) based on a categorization of the ranges of its score.
other features. By fixing the feature of interest at different values and averaging the
predictionsacrossallpossiblevaluesoftheotherfeatures,PDPsrevealtherelationship
between that feature and the outcome, providing insight into the model’s behavior.
Consideramodelwherethepredictedoutcomeyˆ=f(x ,x ,...,x )dependsonaset
1 2 p
ofscalarfeaturesx ,x ,...,x .ThePDPforafeaturex iscalculatedbyaveragingthe
1 2 p j
model’spredictionsoverthejointdistributionofallotherfeaturesx .Mathematically,
\j
the partial dependence function for a feature x is defined as:
j
N
PDP(x )= 1 (cid:88) f(x ,x(i)) (9)
j N j \j
i=1
where N is the number of observations, and x(i) represents the values of all features
\j
except x for the i-th observation. This function is plotted against the values of x to
j j
visualize how changes in x affect the average predicted outcome PDP(x ).
j j
When extending the concept of Partial Dependence Plots (PDPs) to FDA, where
the coefficients of FPCs represent the features, the mathematical approach remains
largelysimilar,buttheinterpretationbecomesdistinctandmorenuanced.Theprocess
involvescalculatingandvisualizinghowthepredictedoutcomechangesaswevarythe
scoreν ofaspecificFPCwhileaveragingouttheeffectsoftheotherFPCcoefficients.
ik
Thisisdonebyselectingarangeofvaluesforν andfixingitatthesevaluesoneata
ik
time. For each fixed value, the model’s predictions are calculated by averaging across
10all other FPC coefficients. The results are then plotted, showing how the predicted
probability changes with the varying coefficient.
ThisextensionofFPDPstoFPCcoefficientsallowsustounderstandthemarginal
effect of individual components of the functional data on the model’s predictions. For
instance, if the FPDP for a particular FPC’s coefficient shows a steep increase, this
would indicate that as this coefficient increases, the FPC it represents contributes
significantlytoincreasingthepredictedprobabilityofacertainclass.Conversely,aflat
FPDPsuggeststhatthecoefficienthaslittletonoeffectontheprediction.Thismethod
is precious in FDA, where understanding the contribution of specific components to
the model’s output can be complex. The goal of FPDPs is to show how the model’s
predictionyˆchangesaswevaryaspecificFPCscoreν whileaveragingouttheeffects
ik
of all other FPC scores ν .
\k
The partial dependence function for the score ν is given by:
k
N
FPDP(ν )=
1 (cid:88) f(cid:16)
ν
,ν(i)(cid:17)
(10)
k N k \k
i=1
where ν(i) denotes all other FPCs’ scores for the i-th observation except ν , and the
\k k
function f(ν ,ν(i)) gives the predicted outcome based on the scores. The FPDP is
k \k
computed by varying the score ν over a range of values while holding all other scores
k
ν at their observed values. The plot of FPDP(ν ) versus ν reveals how changes in
\k k k
this particular FPC score influence the predicted outcome.
WhencreatingFPDPs,wehavetheoptiontouseeithertheprobabilityscaleorthe
logit scale. The probability scale is intuitive and directly interpretable, representing
the predicted probability of an event occurring. This makes it easy to understand
and communicate the results. On the other hand, the logit scale, which expresses
probabilities in terms of log-odds, can provide greater sensitivity to changes in the
middlerangeofprobabilities.Thisisparticularlyusefulwhenanalysingmodelswhere
thedistinctionbetweendifferentclassesissubtle.Thechoicebetweenthetwodepends
on whether clarity or sensitivity is more critical for the analysis.
ThisextensionofPDPstoFPCs’scoresallowsforinterpretingthecontributionof
specific scores to the model’s predictions. Nevertheless, it is crucial to reconstruct a
function across the range of score values using the corresponding FPC to go beyond
interpretingthescore’seffectandgainadeeperunderstandingwithinthetimedomain.
Thisapproachallowsustodiscernthespecificshapesandcharacteristicsofthecurves
that lead to the prediction effects suggested by the FPDP. For this reason, the inter-
pretation of the FPDP should always be accompanied by a graph of the function
reconstructedwiththatspecificFPC,asillustratedinFigure2.InSection4,thejoint
reading of these pictures will be described in detail.
3.3 Functional Principal Components Probability Heatmap
(FPCPH)
TheideabehindtheFunctionalPrincipalComponentsProbabilityHeatmap(FPCPH)
istounderstandinasinglegraphtheeffectoftheFPCs’scoresvaluesontheresponse
variable. The proposal is to generate a range of values for each FPC’s score while
11keeping the scores of other FPCs constant. Then, based on these scores, the FRF
classifier predicts the probability of belonging to a specific class. For each FPC ξ (t),
k
a sequence of scores ν is defined over a range, and the predicted probability of the
ik
observation being classified into a particular class (e.g., Class 1) is calculated using
the trained FRF.
Statistically, a FRF is trained using the scores ν from FPCs as input features.
ik
The model aims to predict the class label Y for each observation i, where Y ∈{0,1}
i i
in the case of binary classification. The classification model is built as follows:
Yˆ =f(ν ,ν ,...,ν ), (11)
i i1 i2 iK
wheref(·)isthefunctionlearnedbyFRF,mappingtheFPCs’scorestothepredicted
class label Yˆ.
i
To assess the impact of each FPC on the classification probabilities, we vary the
score ν of each FPC while holding the other scores constant (e.g., at their mean
ik
values).ForagivenFPCk,wegenerateasequenceofscoresν overaspecifiedrange
ik
[ν ,ν ]:
min max
ν(1),ν(2),...,ν(M), (12)
ik ik ik
whereM representsthenumberofscorevaluessampled.Foreachscoreν(m),theFRF
ik
is used to predict the probability of belonging to a specific class:
Pˆ(m) =Pr(Y =1|ν =ν(m),ν =ν¯ ,∀j ̸=k), (13)
i i ik ik ij ij
where ν¯ represents the mean score for FPC j.
ij
The predicted probabilities Pˆ(m) are then used to construct a heatmap, where the
i
x-axis represents different FPCs and the y-axis represents the range of scores for each
FPC. The intensity of the color in the heatmap indicates the predicted probability of
the class of interest:
Heatmap(k,m)=Pˆ(m), (14)
i
where k indexes the FPC and m indexes the score within the range for that FPC.
The results are then visualised as a heatmap (namely FPCPH), where the x-axis
represents different FPCs, the y-axis represents the range of scores for each FPC,
and the colour intensity indicates the predicted probability of class one membership.
This heatmap provides a transparent and interpretable visual representation of how
changes in each FPCs’ score affect the FRF predictions. By analysing the heatmap,
one can determine which aspects of the functional data, as captured by the FPCs,
are most critical for distinguishing between classes in the classification model. To
fully understand how score variations impact model predictions, especially in time-
dependent data, it is essential to reconstruct the function across the entire range of
score values for the relevant FPC. This allows us to identify the specific shapes and
patterns that drive the effects shown by the FPDP. Therefore, similarly to FPDPs,
theinterpretationoftheFPCPHshouldalwaysbepairedwithagraphofthefunction
reconstructed using that FPC, as shown in Figure 2.
123.4 Ranking and Assessing Functional Principal Components
Importance
In statistical modeling and machine learning, ranking variables based on their impor-
tance in predicting the outcome is crucial. This section describes different methods
for ranking FPCs’ importance, distinguishing between internal and external tools.
3.4.1 Functional Principal Components’ Internal Importance
Measures (Model-Specific Feature Importance)
Mean Decrease in Gini Impurity
The simplest classic measure of explainability to extend to FRF, as presented in
[7], is the Mean Decrease in Gini Impurity. In general, Random Forests can inher-
ently measure feature importance through metrics such as the Mean Decrease in Gini
Impurity because it is part of the process of creating the individual constituents, i.e
classification trees.
The Gini Index G(t) at a node o of a FCT can be calculated as:
C
(cid:88)
G(o)=1− p2 (15)
i
i=1
whereC isthenumberofclassesandp istheproportionofcurvesbelongingtoclassi
i
atnodeo.TheMeanDecreaseinGini(MDG)foraFPCξ iscomputedbyaveraging
k
the total decrease in Gini Index due to splits on ν across all the trees in the forest:
k
O
1 (cid:88) (cid:88)
MDG(ν )= ∆Go (16)
k T s
o=1s∈St
j
where O is the total number of FCTs in the forest, So is the set of all nodes in tree o
j
where feature ν is used for splitting, ∆Go is the decrease in Gini Index at node s in
k s
tree o due to the split on ν .
k
Permutation Importance
Permutation Importance assesses the importance of a FPC by measuring the
increaseinthemodel’spredictionerrorwhenthevaluesofthatvariablearerandomly
shuffled, thereby breaking the relationship between the variable and the outcome. For
a given FPC ξ , its importance is computed as:
k
PI(ν )=Error (ξ )−Error (17)
k perm k orig
whereError (ν )isthepredictionerroraftershufflingν ,andError istheorig-
perm k k orig
inal prediction error. A higher difference indicates greater importance, as the model’s
performance deteriorates more when an important FPC is perturbed.
133.4.2 Functional Principal Components’ External Importance
Measures and Tools (Model-Agnostic Feature Importance)
FPCs Scores Distribution Conditioned on the Response
The basic idea of an external measure is to understand how much a FPC helps
explainadifferencebetweenoutcomecategories.Model-agnosticimportancemeasures
are the typical explainability measure because they are external tools that try to help
us understand what a back-box model hides and do not depend on the model.
Hence, the primary objective is to identify which FPCs can contribute to the
predictive model by analysing their variability to the response. The simplest thing to
doasapreliminaryanalysisiscertainlytoexaminetheconditionaldistributionofFPC
scores based on the response variable. Let ν denote the score of the i-th observation
ik
for the k-th FPC. Consider the binary response variable Y with two classes Y = 0
and Y = 1. The conditional distribution of ν given Y = y can be explored using
ik
violinplots,whichallowustocomparethedistributionsofFPCscoresacrossthetwo
classes and combine the benefits of a boxplot and a density plot:
f(ν |Y =y), y ∈{0,1} (18)
ik
F statistic
ToquantitativelyassesstherelationshipbetweentheFPCS’scoresandthebinary
response variable, we can use an Analysis of Variance (ANOVA). ANOVA allows us
to test whether the mean FPC scores differ significantly between the two response
classes. A significant ANOVA result indicates that the FPC’s score is related to the
response variable and might be an influential feature in the predictive model.
Given the FPC scores ν and the binary response Y , the ANOVA model can be
ik i
extended to this context as follows:
ν =µ +α Y +ϵ (19)
ik k k i ik
where µ is the overall mean of the k-th FPC score, α represents the effect of
k k
the binary response variable on the k-th FPC score, and ϵ is the error term. The
ik
hypothesis tested in ANOVA is:
H : α =0 (no difference in FPC scores between classes)
0 k
H : α ̸=0 (significant difference in FPC scores between classes)
1 k
As in the classical case, for ANOVA to yield robust results, the fundamental
assumptions must be met: independence of observations, normality of residuals, and
homogeneityofvariancesacrossgroups.Iftheseassumptionsareviolated,thevalidity
of the results may be affected. A significant ANOVA result indicates that the FPC
is associated with the response variable, suggesting its potential importance in the
predictive model. Given an FPC coefficient ν associated with the k-th FPC, the
ik
ANOVA F-statistic is used to quantify the contribution of ν to the variability in the
ik
response variable Y. The F-statistic for ν is defined as:
ik
14MS
F = model,k (20)
k MS
error
where MS represents the mean square for the model associated with the k-th
model,k
FPC coefficient ν . It is calculated as:
ik
SS
MS = model,k
model,k df
model,k
where SS is the sum of squares due to the model, and df is the degrees
model,k model,k
of freedom for the model associated with ν .
ik
MS is the mean square of the residual error, calculated as:
error
SS
MS = error
error df
error
whereSS isthesumofsquaresoftheresiduals,anddf isthedegreesoffreedom
error error
associated with the error.
The F-statistic F tests the null hypothesis that the FPC’s score ν (response
k ik
variable)isnotaffectedbythegroupingvariableY (responsevariableinFRFbutnot
intheANOVAcontext).AlargerF valuesuggestsahigherimportanceofthatFPC.
k
η2 statistic
ToquantifytheeffectsizeofeachFPC,wecanusetheη2 statistic,whichmeasures
the proportion of the total variance in the response explained by the FPC coefficient
ν . The η2 for ν is defined as:
ik ik
SS
η2 = model,k (21)
k SS
total
whereSS isthesumofsquaresattributedtothemodelassociatedwiththek-th
model,k
FPC coefficient ν , reflecting the variance explained by this coefficient. SS is the
ik total
total sum of squares, which represents the total variance in the response variable Y.
Theη2 valuecanprovideameasureoftheimportanceofeachFPCcoefficientν ,
k ik
indicating how much of the total variance in Y is explained by ν . Higher η2 values
ik k
suggest that the corresponding FPC should have a greater impact on the response,
despite what happens in FRF.
WhiletheF-statisticisprimarilyusedforhypothesistesting,η2 offersadescriptive
measure of effect size, providing valuable insight into the magnitude of each FPC’s
contribution to the prediction.
3.5 Model-Specific VS Model-Agnostic Feature Importance:
the FPCs Internal-External Importance and Explained
Variance Bubble Plot
Understanding the importance of input features is critical for model explainability
in predictive modelling, particularly with complex models like FRF. Each method
15proposed in Section 3.4 offers a different perspective on FPCs importance. Internal
measures capture how much each FPC contributes to FRF’s predictive accuracy, but
they are model-dependent; internal measures reflect the importance of FPCs in the
context of how FRF interprets and utilises them. The MDG from FRF offers an
ensemble method’s view, emphasising how well each variable splits the data. Indeed,
FRF inherently provides MDG because it is connected to the impurity in each FCT
node and aggregates the impurity reduction across all trees in the FRF. Permutation
Importance, on the other hand, stresses how sensitive the model’s performance is to
eachFPC,deliveringadirectmeasureoftheFPC’scontributiontopredictiveaccuracy.
While model-specific metrics are helpful, they provide a view restricted to the
algorithm employed. To gain a more comprehensive understanding, it is beneficial to
compare these internal measures with external measures derived from simpler, inter-
pretable models, like ANOVA. ANOVA-based metrics can evaluate the importance
of FPCs via statistical information on the variance explained by each FPC. Thus
this approach is useful in understanding the effect size or the ratio of the variabil-
ity between groups to the variability within groups, producing metrics such as the
F-statistic and Eta Squared (η2). These metrics offer a model-agnostic perspective on
FPCs importance, reflecting the inherent predictive power of a FPC independently of
the complexities of the predictive model.
ByvisualisingtheimportanceofFPCsusinginternalandexternalmetrics,wecan
identify FPCs that are universally important across different modeling perspectives
and those whose importance is more model-specific. This dual approach helps in sev-
eral ways. Critical FCPs identification involves recognising FPCs that score highly on
internal and external metrics. These FPCs are likely essential to the predictive model
as they drive its performance and possess strong, independent predictive power, mak-
ingthemreliableoutcomeindicators.Model-specificbiasoccurswhenFPCsarecrucial
internally but not externally, indicating possible model-specific biases. The model’s
RFRmechanismmightoverestimatetheseFPCsdespitethefactthattheyarenotgen-
uinelypredictivefeatures;thelattercheckisvitalfordetectingoverfittingorspurious
correlations. Understanding external validity is highlighted when features are impor-
tant externally but not internally, suggesting that the model might be underutilising
important information. This discrepancy could signal areas for model improvement,
where adjustments to the model’s structure might be necessary to grasp the full
predictive potential of these features.
The presence of FPCs explaining much variability does not guarantee that they
are essential for supervised classification tasks. Indeed, the features explaining less
variability than the first FPCs are often decisive for achieving high performance
since, particularly the first FPCs, usually capture a variability common to all curves.
Thus, to integrate the information just described with the information on variability
explainedwithinthedecompositionintoFPCs,weproposethelastexplainabilitytool,
namelytheFPCsinternal-externalimportanceandexplainedvariancebubbleplot.This
instrument exploits quadrant analysis and allows us to categorise FPCs by plotting
importance metrics in a two-dimensional space, with external importance on one axis
andinternalimportanceontheother(clearly,wecanselectoneinternalandoneexter-
nalbasedonthepreviouslyproposedmetrics).Thisapproachenablestheclassification
16ofFPCsintoquadrants,providinganuancedinterpretationofeachFPC’sroleinboth
the model and the data. This approach underscores the importance of cross-verifying
the model-specific interpretations with more generalised, model-agnostic assessments
of feature importance. It helps ensure that the model is not just fitting the data but
also relying on genuinely predictive features, thus enhancing its explainability and
robustness.
4 Application
To illustrate the proposed instruments, we utilise the ECG200 dataset, initially pro-
posed by Olszewski at Carnegie Mellon University in 2001 as part of his work
titled “Generalized feature extraction for structural pattern recognition in time-
series data” [17]. The ECG200 dataset is a benchmark for testing new classifiers
and consists of time-series data, where each series represents the electrical activ-
ity recorded during a single heartbeat. The dataset is divided into two classes:
Normal Heartbeat (NH) and Myocardial Infarction (MI). It contains 100 sig-
nals in the training set and 100 signals in the test set, making it a compact
yet challenging dataset for classification tasks. The data is publicly accessible
at https://www.timeseriesclassification.com/description.php?Dataset=ECG200. The
primary objective is to predict whether a new patient, based on their observed ECG,
is healthy or suffering from a myocardial infarction.
This prediction task provides a practical example to demonstrate the effectiveness
of our proposed method in handling functional data. We stress that in this study we
are not interested in the performance of the model, which other studies have already
extensively verified and compared with other classifiers (see [7, 14]). In this context,
the aim is to show how to use the proposed explainability tools for demystifying the
FRF. For this reason, comparison with other functional classifiers, or evaluations of
performance measures are not the purpose of the paper. Similarly, the comparison
withotherexplainabilitytools,withintheFRFcontext,isnotpossiblebecausetodate
theonlyoneswhohavedealtwiththesubjectinliteraturehavelimitedthemselvesto
using the traditional feature importance measures, e.g. Gini or Shannon index-based
mean decrease.
Figure3presentstheoriginalsignalsfromboththetrainingandtestsets.Inthese
figures, the blue curves correspond to signals from healthy patients (NH), while the
orange curves represent those diagnosed with heart disease (MI). Figure 4 illustrates
the first fifteen FPCs. Each curve in the graph represents a different FPC, and the
associatedlegenddisplaysthepercentageoftotalvariabilityexplainedbyeachcompo-
nent. The first FPC explains 45.11% of the variability, while subsequent components
explainprogressivelysmallerportions,withthe15thcomponentaccountingfor0.19%.
Acommonquestionthatarisesis:whyshouldweconsideralloftheseFPCs?Inthe
context of supervised classification it is widely proven that even FPCs that account
for very small percentages of the total variability can be crucial. This is because
these minor FPCs may capture subtle but significant variations in the data that are
essential for distinguishing between different classes. Therefore, this analysis does not
focus solely on the first few FPCs that collectively explain 70-80% of the variability,
17whichisoftenthecaseintheclassicalunsuperviseddimensionalityreductioncontexts.
Hence, the analysis deliberately retains a large number of FPCs, focusing not only
on the major components but also on those with smaller contributions to the overall
variability. Indeed, previous studies (see e.g., [14]) have shown that often some FPCs
(particularly the first) may catch a variability common to all signals, often neglecting
sub-patterns, as evidenced also by Maturo and Verde [8].
Fig. 3:OriginalECGsignalsfromthetrainingandtestsets.Theleftpanelshowsthe
signals from the training set, and the right panel shows the signals from the test set.
Thebluecurvescorrespondtohealthypatients(NH),whilethegreencurvesrepresent
those diagnosed with heart disease (MI).
Figure 5 shows the first explainability tool of this research: the Functional Partial
Dependence Plots (FPDPs). This study extends the application of PDPs to the FDA
context, with a particular emphasis on the time domain, which is crucial in FDA. In
FDA, interpreting results within the time domain is essential, making our proposal
necessary for accurately capturing the dynamics of the decision-making process. In
the previous work on FRF and FCTs mentioned above, excellent performance was
achievedintermsofaccuracyandAUC,butitwasnotclearwhetherthetotaleffectof
FPCs(orb-splinecoefficients)eventuallybecamepositiveornegativeontheoutcome
variable, how in the time domain, and with what effect in terms of outcome.
FPDPs display the relationship between the predicted probability and the coeffi-
cientvalueofeachofthefirst15FPCs.Theplotsareorderedbytheamountofvariance
explained by each FPC, with the most significant FPCs displayed first. The FPDPs
show how varying the score of a single FPC while holding others constant affects the
model’s predicted probability. This allows us to visualise the importance and effect
of each FPC. The FPCs are ordered from the one that explains the most variance to
theonethatexplainstheleast.ThevaryingshapesofthePDPssuggestthatdifferent
18Fig. 4: Functional Principal Components Decomposition of the dataset. The plot
shows the first 15 Functional Principal Components (FPCs), with the associated
explained variance.
FPCscontributedifferentlytothemodel.SomeFPCsshowstrongnonlinearrelation-
shipswiththepredictedprobability,indicatingtheircriticalroleincapturingessential
patterns in the data. This analysis helps understand which FPCs are most influential
in the model and how they affect the predictions, providing valuable insights into the
model’s behavior and the underlying data structure.
In this analysis, we utilise the logit scale for the FPDPs to emphasise changes in
the predicted probabilities, mainly when these probabilities are near 0.5. The logit
scale transforms the probability range [0,1] into the log-odds range, making it more
sensitivetovariationsinpredictorvariables.Thissensitivityisbeneficialfordetecting
subtleeffectsthatmaybelessapparentontherawprobabilityscale.Byspreadingout
the middle range of probabilities, the logit scale allows us to gain deeper insights into
how specific FPCs influence the classification model’s predictions. Consequently, the
interpretationoftheFPDPsmustbemadewithreferencetopositivityandnegativity,
which in our application correspond respectively to a higher probability for the class
desease and the class healthy.
Figure 6 displays a pruned FCT that illustrates the decision-making process of a
single base classifier, ultimately leading to the classification of individuals as either
Healthy or Diseased. Each node in the tree represents a split based on the value of a
particularFPCs’score,andthebranchesindicatethedirectionofthesplit.Theleaves
oftheFCTshowthefinalclassification,withassociatedprobabilitiesandpercentages
19Fig. 5: Functional Partial Dependence Plots (FPDPs) for the First 15 Functional
Principal Components (FPCs). Each subplot shows the variation in predicted proba-
bility as the score of a single FPC is varied, while the other scores remain constant.
The plots are ordered by the amount of variance explained by each FPC.
20ofthetotaldatasetthatfallintoeachcategory.TheFCTclearlyvisualizeshowspecific
feature thresholds determine the classification outcomes. For instance, the root node
uses the second FPC to make the initial split, leading to different paths based on
whether its score is less than -0.27.
Although in the paper we focus on the FRF and therefore, the interpretation of
a single FCT loses meaning because we have an ensemble of FCTs all different and
therefore unable to interpret as we have for a single FCT, we show the single FCT for
aspecificreason.ItisexcitingtonotethatasingleFCT,trainedonthewholedataset
(not bootstrap) and all FPCs (not subjected to random extraction at each split),
revealssomesalientaspectsinaccordancewiththeFPDP.Thefirstsplitofthesingle
FCTtakesplaceonthesecondFPCforascorevalueequalto-0.27;thesamecircum-
stance is found in the FPDP, proving that the method works well because we note in
the FPDP of the second FPC that for values of the scores less or equal to -0.27 there
is a high probability of having heart problems. Similarly, as the FCT shows, the same
FPCinteractswithitselfinthesingletree;forvaluesgreaterthan-5.5,theimpacton
the outcome is reduced by returning to the negative zone (tendency towards healthy
prediction of the outcome). Thus, the FCT displayed in Figure 6 is presented as an
illustrativeexampleandtoshowthecorrespondenceofsomeintuitions.However,itis
essential to note that the model we use is FRF. If we rely solely on a FCT, the result-
ing model would be more easily interpretable, providing a clear and straightforward
pathfrominputfeaturestoclassificationoutcomes.However,asingleFCThassignif-
icant limitations, such as a higher risk of overfitting and reduced predictive accuracy
compared to ensemble methods like FRF. While it offers interpretability, it may fail
to capture the full complexity and interactions within the data, leading to less robust
and generalisable predictions due also to high variance.
Nonetheless,whiletheFPDPsoffervaluableinsightsintotheinfluenceofindividual
FPCs on the model’s predictions, it is crucial to recognise that there can be complex
interactions between FPCs that a simple comparison between a single tree and the
PDPs cannot fully explain. These interactions may involve multiple FPCs working
together in ways that are not immediately apparent from examining individual plots.
Thus, while the PDPs are highly useful for enhancing the explainability of the model,
they represent only a part of the broader picture. Above all, since we are in the FDA
domain, the reading of the time domain also assumes great importance, and thus the
study of FPDPs must be refined by functional reconstruction through the FPCs in
Figure 7.
InFigure7,thereconstructedcurvesarevisualisedtoreflectthedifferentintervals
of scores for each of the first fifteen FPCs. Each subplot represents the influence of a
single FPC (linked to its score) on the shape of the reconstructed curve. The shaded
regionshighlightthevariability(curves’range)withinspecificscores’intervals,provid-
ing a visual stratification of how each FPC affects the functional data reconstruction.
This approach allows for a more straightforward interpretation of the relationship
between FPC scores and the resulting curve shapes across different domain regions.
The coloured bands in each subplot represent the potential range of curve values cor-
responding to specific intervals of FPC scores indicated in the legends (the evaluation
thresholds are different between the various FPCs’ scores because different ranges
21Fig. 6: Pruned FCT illustrating the relationship between various features and the
predicted health outcome. Each node represents a decision point based on one of the
features,leadingtoaclassificationofeitherHealthyorDiseased.Thepercentagevalues
withineachnodeindicatetheproportionofsamplesclassifiedintothatcategory,while
the color intensity reflects the purity of the node’s classification.
characterise each, but each is divided into four windows). The more pronounced devi-
ations from the mean curve (indicated by the black dashed line) show the significant
impact specific score ranges can have on the curve’s amplitude and shape.
Focusing on these shaded areas reveals how each FPC contributes to the overall
structureofthefunctionaldata.Forexample,inmanysubplots,thebandrepresenting
higher score intervals (in absolute terms) often shows a significant deviation from the
mean curve. This indicates that higher scores generally strongly influence the curve’s
shape, emphasizing or diminishing specific functional features. Understanding these
variations is fundamental for interpreting FPDPs in the time domain and capturing
how and how much the scores of specific FPCs influence the predicted probabilities
of outcome classes in a supervised classification setting. However, only by closely
examining the individual charts in Figure 7 can one fully appreciate how the shape
of the reconstructed curve drive by different FPC score intervals—affects the model’s
predictions.
To better understand how to interpret the relationship between FPDPs and func-
tional data reconstructions, we focus closely on the first three FPCs as illustrated in
Figure8.Thiscomparisonservesasamagnifyinglens,helpingtoclarifythejointread-
ing of PDPs alongside the corresponding reconstructed functional curves. In the left
column (Figure 8a), the FPDPs show the relationship between each FPC’s coefficient
value and the predicted outcome probability. These plots help identify how different
rangesofFPCscoresinfluencethemodel’spredictions.ThecoloredareasinthesePDP
plots correspond to the score intervals used in the right column (Figure 8b), where
the reconstructed curves are plotted based on the shapes of FPCs (depending on t).
22Fig. 7: Variation of the Reconstructed Curve with the First Fifteen Functional Prin-
cipal Components (FPCs). Each subplot shows how variations in the score of a single
FPC influence the shape of the reconstructed curve.
23Hence, the right column (Figure 8b) visualises the effect of varying the FPC scores
within specific intervals on the shape of the reconstructed curves in the time domain.
TheareasintheseplotsareshadedaccordingtothesameintervalsshowninthePDPs,
providing a clear visual link between the FPC score ranges and their impact on both
therangeofthereconstructedcurveshapesandthemodelpredictions.Thiscombined
approach offers a more comprehensive understanding of how each FPC contributes to
the prediction model, particularly in understanding how changes in the FPC scores
translate into variations in the functional data, affecting the predicted probabilities.
Let’s focus on the third FPC as an example of interpreting the associated plots in
Figure 8. In the FPDP, we observe that within the first interval from the left, there
is a significant increase in the probability of a positive outcome, which, in the context
of ECGs, indicates a higher likelihood of a heart problems. The crucial question is:
WhatspecificshapemustanECGsignalhaveforthistooccur?Theanswerliesinthe
corresponding figure on the right, which shows how a generic curve is reconstructed
usingonlythethirdFPC.Thisfigurerevealsthattheprobabilityofapositiveoutcome
increases when the score of the third FPC is less than 2.3 (as indicated also by the
FPDP). When we reconstruct the curve using scores below 2.3, we observe a marked
increase in the ECG values in the early part of the domain. This suggests that when
the third FPC score is very low, the ECG curve exhibits a specific pattern correlating
withahigherriskofheartproblems(apeakinthefirstpartofthetimedomainanda
lower value in the second half). The same analytical approach and interpretation can
be extended to other FPCs and regions of the time domain. This method provides a
novelandinsightfulwaytounderstandtheinfluenceofindividualFPCsontheoverall
data structure, offering a more nuanced interpretation of the functional data.
Figure 9 demonstrates how FPCs’ scores influence the probability of being clas-
sified as Diseased. The horizontal axis represents the FPCs, while the vertical axis
correspondstothepossiblescoresofthesecomponents.Thecolorgradientfromgreen
to red indicates the probability of classification: green signifies a higher likelihood of
beingclassifiedasHealthy,whereasredindicatesahigherprobabilityofbeingclassified
as Diseased. This visual tool provides insight into which FPCs and their correspond-
ingscorerangeshavethemostsignificantimpactontheclassificationoutcome,aiding
in interpreting the FPC’s role in predicting health status.
While this heatmap provides an overview of the impact of each FPC on classifica-
tion outcomes, for a more nuanced understanding of how these components influence
theshapeofthefunctionaldataovertime,itisessentialtoanalyzetheplotsthatrecon-
struct the original curves using these FPCs. Such analysis, similar to the approach
used with FPDPs, allows us to link specific variations in the FPCs to changes in the
curve shapes, offering more profound insights into the underlying data patterns and
their relationship to the predicted health outcomes.
Figure 10 displays violin plots for the first 15 FPCs scores, divided by class into
Healthy and Diseased groups. The violin plots provide a comprehensive view of the
distribution of FPC scores within each class, with box plots included within the vio-
lins further to emphasise the median and interquartile range of the data. The titles
of each plot include the p-value from an ANOVA test, indicating the statistical sig-
nificance of the difference between the score distributions of the two classes. Notably,
24(a) Functional Partial Dependence Plots (b)VariationoftheReconstructedCurves
for the First 3 FPCs with the First 3 FPCs
Fig. 8: Comparison of FPDPs and FPC Variation for different scores’ intervals.
FPCswithverylowp-values,suchasFPC1,FPC2,andFPC3,andFPC4showasig-
nificantdifferenceinscoredistributionsbetweenthetwoclasses,suggestingthatthese
components may be critical in distinguishing between healthy and diseased subjects.
Conversely,FPCswithhigherp-valuesindicatelessevidenceofasignificantdifference
between the two groups. This plot could already indicate each FPC’s positive or neg-
ative impact on prediction. However, it is essential to note that this is not guaranteed
because the violin plot of the distribution of a single FPC cannot capture complex
interactionsamongmultipleFPCs.Nevertheless,alsothroughANOVA,itservesasan
initial visual and inferential indication of noteworthy differences between the groups.
Both internal and external importance measures can assess the role of each FPC
in the classification task. The results are visually summarized in Figure 11. The top
row of Figure 11 displays the internal importance measures derived from the FRF,
specificallytheGiniImportanceandPermutationImportance.Thesemeasuresreflect
the contribution of each FPC to the model’s predictive accuracy. Notably, FPC2,
FPC1,andFPC3standoutwiththehighestimportancevalues,suggestingthatthese
components capture the most significant patterns in the data relevant for predicting
the outcome. The Gini Importance and Permutation Importance together provide
insight into which FPCs are most influential within the model itself.
On the other hand, it is insightful to complement the internal model-based mea-
sures with external measures, which are often overlooked in the literature. External
25Fig. 9: Functional Principal Component Probability Heatmap showing the impact of
FPCcoefficientsonthepredictedprobability.Greenareasindicateahigherprobability
of being classified as Healthy, and red areas indicate a higher probability of being
classified as Diseased.
measures, such as Eta Squared and F-Statistics derived from ANOVA, offer an inde-
pendent evaluation of how much variance in the outcome can be attributed to each
FPC, separate from the model’s influence. By comparing these external metrics with
internal measures like Gini Importance and Permutation Importance, we can assess
whether the model’s internal rankings are robust or potentially biased or overly
dependent on the specific model architecture. This dual approach helps identify any
discrepancies, providing a more balanced and thorough understanding of the FPCs’
true importance in the data structure and their predictive power. Thus, the bot-
tom row of Figure 11 presents the external importance measures, which include the
Eta Squared and F-Statistic obtained from ANOVA tests. Once again, FPC2, FPC1,
and FPC3 emerge as the most essential components, underscoring their significant
explanatory power beyond the model’s internal mechanics. This dual analysis of FPC
importance allows us to understand not only which components are crucial for the
classification model but also how they relate to the underlying structure of the data.
By integrating internal and external measures, we gain a comprehensive view of the
functional principal components essential for distinguishing between classes, which is
particularly valuable in applications where model explainability is key. We emphasize
26Fig. 10: Violin plots of the first 15 FPCs scores by class. The p-values from ANOVA
testsareincludedintheplottitlestoindicatethestatisticalsignificanceofdifferences
between the two outcome classes.
27thatinthiscasethemostimportantfeaturesarethefirstthreebutveryoftenthisdoes
not happen, especially when the data has such a high variability in the time domain.
Finally, Figure 12 illustrates the importance of FPCs based on internal and exter-
nal importance measures, i.e. the FPCs Internal-External Importance and Explained
VarianceBubblePlot.Forthisplot,wechoseeta-squaredasamodel-agnosticmeasure
and the Gini index as a model-specific measure, with the explained variance repre-
sentedbythesizeofthebubbles.TheX-axisrepresentstheEtaSquaredvalue,which
reflects the external model importance derived from ANOVA. The Y-axis shows the
Gini index, a measure of internal model importance derived from the FRF. The size
of each bubble corresponds to the explained variance of each FPC, while the color
denotestheFPCidentity.Theplotisdividedintofourquadrantsbydashedlinesrep-
resentingthemedianinternalandexternalimportancevalues.Thetoprightquadrant
indicatesFPCsthatarehighlyimportantbothinternallyandexternally,makingthem
critical components. The top left quadrant shows FPCs that are mainly important
internally, while the bottom right quadrant identifies FPCs with external importance
but less internal significance. Finally, the bottom left quadrant represents FPCs that
are less important in both aspects.
Understanding the gaps between model-specific and model-agnostic FPCs impor-
tance is essential because a feature that appears highly important within the model
(highinternalimportance)mighthaveanegligibleeffectwhenevaluatedindependently
(low external importance). This could indicate that the model may overvalue the fea-
tureduetointeractionswithothervariablesoroverfitting.Thishappens,forexample,
with the FPC7, which shows a value above the median in terms of model-specific
importance, but the model-agnostic measures slightly underestimate this importance.
FPC2 stands out as a critical component with high importance both internally and
externally. It explains a significant amount of variance, suggesting that FPC2 is a
robust component, essential both within the model context and in broader analysis.
FPC1showshighinternalbutsignificantlylowerexternalimportancedespiteexplain-
ing much of the total variance. This might indicate that FPC1 is strongly influenced
by the specific model used and may not be as relevant in a more general context.
WhileFPC3islessimportantinternallythanFPC1,ithasdecentexternalimportance,
suggesting a balance between its contribution to the model and general relevance.
Althoughlesscritical,otherFPCs,likeFPC9,stillcontributetothemodel’spredictive
power in different ways.
5 Discussion and Conclusions
In the era of big data, investigating high-dimensional datasets has become a crucial
challenge across various domains, including medicine, ecology, and economics. The
exponential growth in data availability offers exceptional opportunities for gaining
insights but also raises significant analytical challenges, particularly when grappling
with the curse of dimensionality. As the number of dimensions increases, the volume
of data points grows exponentially, complicating extracting meaningful patterns and
relationships using traditional statistical approaches. FDA has emerged as a powerful
solutiontothesechallenges,transforminghigh-dimensionaldataintofunctionalforms
28Fig. 11: Comprehensive comparison of the importance measures of the first 15 Func-
tional Principal Components (FPCs). The importance measures are divided into
internal (model-specific) and external (model-agnostic).
29Fig. 12: FPC Internal-External Importance and Explained Variance Bubble Plot:
ScatterplotofFPCsbasedoninternalandexternalimportancemeasures.TheX-axis
shows the external importance (Eta Squared), and the Y-axis represents the internal
importance (Gini). The size of the bubbles corresponds to the explained variance of
each FPC.
capable of catching complex temporal and spatial patterns in a more coherent and
interpretable manner [1, 2]. This capability has made FDA a robust tool in various
applications, particularly in biomedical fields, where continuous monitoring data like
electrocardiogram (ECG) signals are prevalent [7].
One of the most promising methods for the supervised classification of functional
data is the FRF. This approach combines the flexibility of random forests with the
strengths of FDA, allowing for effective handling of high-dimensional functional data
[7, 14]. However, despite the demonstrated effectiveness of FRFs in predictive tasks,
30a significant drawback persists: the need for more transparency and explainability.
Like many machine learning models, FRFs often function as black boxes, making it
challenging to comprehend individual features contribution.
The implication of transparency in machine learning models is increasingly recog-
nized, especially in critical fields such as healthcare and environmental science, where
model predictions can have significant real-world consequences [7]. To address this
need, our paper introduces a novel set of explainability tools designed to enhance the
interpretability of FRF. These tools aim to bridge the gap between model accuracy
and understanding, providing users with the means to demystify the decision-making
process of FRFs.
The key contributions of this work include the development of Functional Par-
tial Dependence Plots (FPDPs), which illustrate the influence of individual FPCs on
the model’s predictions, and FPC Probability Heatmaps, which visualize how changes
in FPC scores affect the predicted probabilities for different classes. Additionally,
we introduce various FPC importance metrics, including the FPC Internal-External
Importance and Explained Variance Bubble Plot, which offers a comprehensive view
of the significance of each FPC from both internal and external perspectives.
These methods are demonstrated through an application to an ECG dataset, a
classic example of functional data where the shape and variability of heart activity
curves are crucial for accurate diagnosis. Applying these explainability tools reveals
deep insights into how individual FPCs influence the classification of ECG signals,
thereby enhancing the interpretability of the FRF model. To the best of our knowl-
edge,thisstudyisthefirsttospecificallyaddresstheissueofexplainabilitywithinthe
context of FRF. Prior works, including those by Maturo and Verde, have primarily
focused on the interpretability of individual Functional Classification Trees (FCTs),
without delving into the explainability of ensemble methods like FRFs [7? ]. Existing
literaturepredominantlydiscussestraditionalvariableimportancemeasures,butmore
isneededtoexplaintheintricaciesofFRFs.Asaresult,thereneedstobemorealterna-
tive methods in this domain, justifying the absence of references to other approaches.
Moreover, this paper intentionally avoids discussing accuracy or other performance
metrics,astheseaspectsarenotthefocusofthisstudy.Thepredictivepowerofensem-
ble methods for functional data has already been well-documented in the literature,
particularlyintheworksofMaturoandVerde[7?].Thus,thisstudydoesnotengage
in performance comparisons, as it lies beyond the scope of our objectives.
In conclusion, this paper significantly contributes to Functional Data Analysis by
addressing a critical gap in the explainability of Functional Random Forests. The
novel explainability tools introduced here provide a pathway to greater transparency,
makingthesepowerfulmodelsmoreaccessibleandtrustworthyforpractitionersacross
variousdomains.Futureresearchcouldexplorefurtherenhancementstothesetoolsor
their application to other complex datasets, thereby improving the transparency and
usability of machine learning models in high-dimensional functional data contexts.
31Declarations
Funding and/or Conflicts of interests/Competing interests
Alltheauthorsdeclarethattheydidnotreceivesupportfromanyorganisationforthe
submitted work. All authors certify that they have no affiliations with or involvement
inanyorganizationorentitywithanyfinancialornon-financialinterestinthesubject
matter or materials discussed in this manuscript.
Use of generative AI in scientific writing
While preparing this work, the authors used Grammarly AI to improve the English
language.Whileusingthistool,theauthorsreviewedandeditedthecontentasneeded
and took full responsibility for the publication’s content.
References
[1] Ferraty,F.,Vieu,P.:Curvesdiscrimination:anonparametricfunctionalapproach.
Computational Statistics & Data Analysis 44(1-2), 161–173 (2003) https://doi.
org/10.1016/s0167-9473(03)00032-x
[2] Ramsay, J., Dalzell, C.: Some tools for functional data analysis. Journal of the
Royal Statistical Society. Series B: Methodological 53(3) (1991)
[3] Shang, H.: A survey of functional principal component analysis. AStA
Advances in Statistical Analysis 98(2), 121–142 (2013) https://doi.org/10.1007/
s10182-013-0213-1
[4] Febrero-Bande, M., Fuente, M.O.: Statistical computing in functional data anal-
ysis: The R package fda.usc. Journal of Statistical Software 51, 1–28 (2012)
https://doi.org/10.18637/jss.v051.i04
[5] Cuevas, A.: A partial overview of the theory of statistics with functional data.
Journal of Statistical Planning and Inference (2014) https://doi.org/10.1016/j.
jspi.2013.04.002
[6] Ferraty, F., Vieu, P.: Nonparametric Functional Data Analysis. Springer, New
York (2006). https://doi.org/10.1007/0-387-36620-2
[7] Maturo, F., Verde, R.: Pooling random forest and functional data analysis for
biomedical signals supervised classification: theory and application to electrocar-
diogram data. Statistics in Medicine 41, 2247–2275 (2022) https://doi.org/10.
1002/sim.9353
[8] Maturo, F., Verde, R.: Combining unsupervised and supervised learn-
ing techniques for enhancing the performance of functional data classi-
fiers. Computational Statistics 39(1), 239–270 (2024) https://doi.org/10.1007/
s00180-022-01259-8
32[9] Ramsay, J., Silverman, B.: Functional Data Analysis, 2nd Edn. Springer, New
York (2005). https://doi.org/10.1007/b98888
[10] Cuevas, A.: A partial overview of the theory of statistics with functional data.
Journal of Statistical Planning and Inference 147, 1–23 (2014) https://doi.org/
10.1016/j.jspi.2013.04.002
[11] Hyafil, L., Rivest, R.L.: Constructing optimal binary decision trees is
NP-complete. Information Processing Letters (1976) https://doi.org/10.1016/
0020-0190(76)90095-8
[12] Quinlan,J.R.:InductionofDecisionTrees.MachineLearning(1986)https://doi.
org/10.1023/A:1022643204877
[13] Hastie, T., Tibshirani, R., Friedman, J.: The Elements of Statistical Learning -
DataMining,Inference,andPrediction.Springer,???(2009).https://doi.org/10.
1007/978-0-387-84858-7
[14] Maturo, F., Verde, R.: Supervised classification of curves via a combined use of
functionaldataanalysisandtree-basedmethods.ComputationalStatistics38(1),
419–459 (2023) https://doi.org/10.1007/s00180-022-01236-1
[15] Therneau, T., Atkinson, B.: Rpart: Recursive Partitioning and Regression Trees.
(2019).https://doi.org/https://CRAN.R-project.org/package=rpart.Rpackage
version 4.1-15
[16] Ho, T.K.: The random subspace method for constructing decision forests. IEEE
Transactions on Pattern Analysis and Machine Intelligence (1998) https://doi.
org/10.1109/34.709601
[17] Olszewski, R.: ECG200 Dataset. Carnegie Mellon University. Accessed: 2024-08-
22 (2001). https://www.timeseriesclassification.com/description.php?Dataset=
ECG200
[18] Friedman, J.H.: Greedy function approximation: A gradient boosting machine.
Annals of Statistics 29(5), 1189–1232 (2001) https://doi.org/10.1214/aos/
1013203451
33