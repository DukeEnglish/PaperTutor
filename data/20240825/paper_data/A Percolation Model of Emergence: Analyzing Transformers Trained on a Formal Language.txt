Preprint
A PERCOLATION MODEL OF EMERGENCE: ANALYZING
TRANSFORMERS TRAINED ON A FORMAL LANGUAGE
EkdeepSinghLubana∗1,5,KyogoKawaguchi∗2,3,4,RobertP.Dick5,HidenoriTanaka1,6
1CBS-NTTPrograminPhysicsofIntelligence,HarvardUniversity
2NonequilibriumPhysicsofLivingMatterRIKENHakubiResearchTeam,
RIKENCenterforBiosystemsDynamicsResearch
3RIKENClusterforPioneeringResearch
4InstituteforPhysicsofIntelligence,DepartmentofPhysics,TheUniversityofTokyo
5EECSDepartment,UniversityofMichigan,AnnArbor
6Physics&InformaticsLaboratories,NTTResearch,Inc.,Sunnyvale,CA
ABSTRACT
Increase in data, size, or compute can lead to sudden learning of specific capa-
bilitiesbyaneuralnetwork—aphenomenonoftencalled“emergence”. Beyond
scientificunderstanding,establishingthecausalfactorsunderlyingsuchemergent
capabilitiesiscrucialtoenableriskregulationframeworksforAI.Inthiswork,
weseekinspirationfromstudyofemergentpropertiesinotherfieldsandpropose
aphenomenologicaldefinitionfortheconceptinthecontextofneuralnetworks.
Ourdefinitionimplicatestheacquisitionofspecificstructuresunderlyingthedata-
generatingprocessasacauseofsuddenperformancegrowthforspecific,narrower
tasks. We empirically investigate this definition by proposing an experimental
systemgroundedinacontext-sensitiveformallanguageandfindthatTransform-
erstrainedtoperformtasksontopofstringsfromthislanguageindeedexhibit
emergentcapabilities. Specifically,weshowthatoncethelanguage’sunderlying
grammar and context-sensitivity inducing structures are learned by the model,
performanceonnarrowertaskssuddenlybeginstoimprove. Wethenanalogizeour
network’slearningdynamicswiththeprocessofpercolationonabipartitegraph,
establishing a formal phase transition model that predicts the shift in the point
ofemergenceobservedinexperimentwhenchangingthedatastructure. Overall,
ourexperimentalandtheoreticalframeworksyieldasteptowardsbetterdefining,
characterizing,andpredictingemergenceinneuralnetworks.
1 INTRODUCTION
Modernneuralnetworks,e.g.,largelanguagemodels(LLMs)(GeminiTeam,2023;OpenAI,2023;
Anthropic,2023;Touvronetal.,2023),exhibitabroadspectrumofcapabilities,allowingthemto
serveasthe“foundation”fordownstream,application-specificsystems(Bommasanietal.,2022;
Ahnetal.,2022;Driessetal.,2023;Schicketal.,2024). Asthesemodelsscale,eitherviaadditionof
moredata,parameters,orcompute,anintriguingbehaviorisattimesobserved: untilacertaincritical
scaleisreached,therearecapabilitiesthatthemodeldoesnotexhibit;however,beyondthispoint,
suchcapabilitiescansuddenly“emerge”(Weietal.,2022;Srivastavaetal.,2022;Brownetal.,2020;
Yuetal.,2022;Steinhardt,2023;Panetal.,2022;Raeetal.,2021;Aniletal.,2023;Kirschetal.,
2022;Heetal.,2024;Elhageetal.,2021). Morespecifically,theperformanceofthemodelonatask
orbenchmarkmeanttoevaluatesaidcapabilitiescanwitnesssubstantialgrowthinperformance,even
thoughtheoveralltraininglossundergoesminimal,ifany,improvements(Arora&Goyal,2023;
Duetal.,2024). Empiricalevidenceinfactsuggeststhat,attimes,severalcapabilitiescanemerge
simultaneously(Weietal.,2022;Wei,2022).
Beyonddevelopingabetterscientificunderstandingofneuralnetworks, understandingemergent
capabilitiesiscrucialtoenablerisk-centricregulationframeworksforAI,whichassumeasystem’s
∗Equalcontribution.Codeathttps://github.com/EkdeepSLubana/ConceptPercolation.
1
4202
guA
22
]GL.sc[
1v87521.8042:viXraPreprint
capabilitiescanbepreemptivelyconjectured(AI,2023;CounciloftheEuropeanUnion,2024;OSTP,
2023; Anwar et al., 2024; Kaminski, 2023; Ganguli et al., 2022). To this end, recent work has
made attempts at identifying factors that decide whether a capability will emerge. For example,
Okawaetal.(2023)andArora&Goyal(2023)implicatetheunderlyingcompositionalstructure
ofacapabilityasthecauseforitssuddenlearning. Hoffmannetal.(2023)arguecapabilitiesthat
involve interactions between specialized components within a model are likely to yield sudden
performanceimprovementsoncethecorrectinteractionmechanismislearned;e.g.,theinteraction
betweentheprevioustokenandcopyattentionheadstoenablein-contextlearning(Elhageetal.,
2021;Olssonetal.,2022;Reddy,2023). Meanwhile,Schaefferetal.(2023)argueemergentabilities
areanartifactofpoorlydefined,discontinuousevaluationmetrics,claimingthatmodelsundergo
continuous,persistentimprovementsduringtraining. Recentworkhashoweverdemonstratedthat
evencontinuousmetricscanwitnesssuddenimprovements,withsuchchangesco-occurringwith
themodel’slearningofanewcapability(Chenetal.,2024;Duetal.,2024;Cuietal.,2024). This
underminestheclaimthatemergentcapabilitiesaremerelyanartifactofevaluationprotocols.
Takentogether,theorthogonalexplanationsanddisparateresultsabovehaveresultedinemergence
becoming an unclear phenomenon in machine learning. At its core, however, we claim that the
concepthasneverbeendefinedinpriorwork. Thishasarguablyledtodistinctmechanismscausing
suddenchangesinmodelperformancetoallbelabeledas“emergence”. Whatisthephenomenology
thatthistermismeanttocaptureinthecontextofneuralnetworks? Isitmerelyasuddenincrease
inperformancewithscale,orbroaderthanthat? Givenareasonabledefinition,canweshow,even
ifinasimplifiedsystem,thatemergentcapabilitiesarecommonplaceandcanweusethesystem’s
simplicitytobetterunderstandwhatdrivestheirsuddenlearning?
Thiswork.Toaddressthequestionsabove,
Phase: Solid phase Liquid phase Gas phase
weproposeaphenomenologicaldefinition
foremergenceandtrytounderstandwhat (ρ,Ψ6)= (1,1) Ψ6=N1 b∑ i=6 1ei6θi (1,0) ρ=N V (0,0)
drives it in a toy task of learning formal
θi
languages(Chomsky,1956;Allen-Zhu&
Li, 2023b; Cagnetta & Wyart, 2024; Liu
etal.,2023a;Wenetal.,2023;Liuetal.,
Ψ6: Bond-orientation ρ: Density
2022a; Friedman et al., 2023; Jain et al.,
2023; Merrill et al., 2023). Specifically,
Temperature
wearguethreecharacteristicsshouldbeob-
served to claim a capability is emergent
Phase: Initialization Capability 1 learned Capability 2 learned
(see Def. 1): beyond (i) sudden perfor- (c 1,c 2)= (0,0) (1,0) (1,1)
manceimprovementforaspecifictask,we
claimemergenceismorelikelytorepresent
ameaningfulconceptif(ii)performanceon
severaltasksimprovessimultaneouslyand
c: Capability 1 c: Capability 2
1 2
(iii)thereareprecisestructuralchangesin
themodelatthepointofemergence. The Data, Model, Optimization Scale
intuition,borrowedfromthestudyofemer-
genceinotherfields(seeFig.1),isthatif
Figure 1: Emergence as phases of learning. Emer-
multipletaskswitnessimprovementinper-
gence is a well-characterized phenomenon in natural
formance,thereislikelysomesharedstruc-
sciences(Anderson,1972;Newmanetal.,2001;New-
ture to them and the model acquires this
man, 2003) and deeply entangled with the notion of
structureatthepointofemergence. Forex-
phase changes: when a change in some control vari-
ample,whenin-contextlearningemerges
able(e.g.,temperature)yieldssystematicchangesina
in LLMs, precise context-sensitive struc-
system’sunderlyingstructure(e.g.,hexagonalconfig-
tures emerge and interact with general-
urationsinacrystal),simultaneouslyaffectingseveral
purposecapabilities,leadingtoin-parallel
ofitspropertiesandnecessitatingstudyofeachphase
improvementinseveraldownstreamtasks’
independently. Weargueforasimilarcharacterization
performance(Wei,2022;Weietal.,2022;
ofemergenceinmachinelearning: identifyingsystem-
Lu et al., 2023; Guo et al., 2023; Hendel
atic changes in a model’s behavior that influence its
etal.,2023);thus,in-contextlearningcan
downstreamabilitiesandleadtosuddenperformance
be deemed an emergent capability under
improvements. Forexample,learningalanguage’ssyn-
the scope of our definition. In this sense,
taxwillaffectalldownstreamcapabilitieswherecoher-
understanding emergence can be formal-
ent,grammaticallycorrectgenerationsarenecessary.
izedasastudyofidentifyingstructuresthe
2Preprint
modelacquiresatthepointofsuddenlearningofasetof capabilities,andunderstandingwhythat
structureisrelevanttosaidcapabilities. Adoptingthisviewpoint,wemakethefollowingfindingsin
ourexperiments.
• FormalLanguagesasanExperimentalSystemforStudyingEmergence. Wedefineaproba-
bilisticcontext-sensitivegrammar(PCSG)withtypeconstraintsthatallowanentityorasubjectin
asentence(e.g.,man)tobeseeninthecontextofonlyapredefinedsetofproperties(e.g.,walk).
Wetrainmodelstoperformminimalisticreasoningtasksoversamplesofthislanguageandfind
theirdatascalingcurvessimultaneouslyshowsuddenlearningacrossseveralmetrics.
• Learningofgeneraldatastructuresunderliessimultaneousjumpsinspecificmetrics. We
find points of sudden change for metrics evaluating individual tasks correlate with the model
learningtworelevantstructuresthatunderliethelanguage: grammaticalrulesandtypeconstraints.
Despitethesimplicityofoursetup,weclaimlearningofsuchgeneralstructuresiswhatleadsto
suddengrowthintheperformanceofnarrowertaskswherethesestructuresareimportant.
• Apercolationmodelpredictsthescalingofwhencapabilitiesemerge. Weproposeaformal
modelgroundedinthetheoryofgraphpercolation(Cohenetal.,2002)thatcapturesourexperimen-
talobservations,andshowthatifwecandescribethestructurethemodelislearningatthepoint
ofemergence,apredictivetheoryforsuddenlearningcan(attimes)beconstructed—analogousto
theoriesofphasetransitionsinphysics;seeFig.1.
2 RELATED WORK
Explainingemergence.Focusingonthesuddenlearningcharacteristicofemergentcapabilities,afew
recentworkshavetriedtoexplainthefactorsdrivingthisphenomenon.Forexample,compositionality
hasbeenimplicatedforhavinga“multiplicative”effectonamodel’sperformance,wheretheargument
isthatamodelcannotperformwellonacompositionaltaskuntiltheabilitiesneededtoperform
individualtasksinvolvedinthatcompositionareacquired(Okawaetal.,2023;Arora&Goyal,2023;
Yu et al., 2023; Srivastava et al., 2022; Wei et al., 2022; Hoffmann et al., 2022; Gokhale, 2023);
whentheyareacquired,performancesuddenlygrows. Afewpapershavealsoshownthatlearningof
specificcapabilities(i.e.,onesnotcompositionalinnature)canbesudden(Chenetal.,2024;Nam
etal.,2024;Kirschetal.,2022;Heetal.,2024;Michaudetal.,2023). Incontrast,Schaefferetal.
(2023)argueemergentscalingcurvesareaconsequenceofpoorlydefined,discontinuousevaluation
metrics,andtheseeminglysuddenlearninggoesawayoncepartial,continuouscreditisgiventothe
model. Weemphasizethatifthestructureofataskisignored,itiscertainlyeasytodefinearbitrary
continuousmetricsforatask;however,suchmetricsareunlikelytohelpmeasureprogresstoward
learningatask. Forexample,considertheadditionoftwonumbers,say10and11,andthemetric
calledtokeneditdistance(Schaefferetal.,2023)thatassessestheaveragedistancebetweendigits
in the model’s output, denoted xy, from the ground truth, i.e., (|x−2|+|y−1|)/2. For both xy = 22
and xy = 11, this metric equals 1; however, clearly 22 is a better approximation for the ground
truth(21). Thus,onceweaccountforthestructureofthetask,i.e.,thefactthaterrorinthemost
significantdigitshouldbepenalizedmorethanerrorintheleastsignificantone,weseelimitations
intokeneditdistanceasametricforassessingamodel’sabilitytoaddnumbers. Weargueclaims
relatingemergencetosensitivityofmetricscanbeconfoundedbyuseofmetricsthatdonotrespect
thestructureofthetask.
Grokkingvs.Emergence. Wefocusontheeffectofdatascalingonamodel’scapabilities;often
called‘learningcurve’or‘datascaling’analysis(Viering&Loog,2022;Blumeretal.,1989;Bousquet
etal.,2021;Seungetal.,1992;Watkinetal.,1993;Amari,1993;Haussleretal.,1994). Onsurface,
thismightlooksimilartotheseeminglyrelatedphenomenonofgrokking(Poweretal.,2022;Liu
etal.,2023b;Žunkovicˇ &Ilievski,2022;Murtyetal.,2023;Baraketal.,2022;Edelmanetal.,2023;
Nandaetal.,2022),whereinamodel’sperformanceonataskrapidlyimproveslongafterithasfitthe
trainingdata. However,weemphasizethatwefocusonanonlinelearningsettinginourexperiments,
i.e.,agivensampleisunlikelytobeseenmultipletimesduringtraining. Emergenceisgenerally
studiedinsuchonlinelearningscenarios. Sincethereisnodistinctionbetweentrainversustestdata
insuchasetting,wearguemechanisticexplanationsofgrokkingidentifiedinpastwork(Nandaetal.,
2023;Liuetal.,2022b)areunlikelytohelpexplainourresultsofemergenceunderdatascaling.
3Preprint
3 A PHENOMENOLOGICAL DEFINITION OF EMERGENCE
Toanalyzeemergence,wefirstestablishwhatwemeanbythetermforthepurposeofthiswork.
Specifically,wedefineemergenceinaphenomenologicalmanner,i.e.,byassemblingthecharacteristic
propertiesassociatedwithscalingcurvesclaimedtodepictemergentlearning. Weemphasizeour
definition is merely a definition for emergence, and does not necessarily represent all possible
perspectives(Luccioni&Rogers,2023). Forexample,oftenmodelcapabilitiesthatarisedespiteany
explicitsupervisionarecalledemergentinself-supervisedlearning(Caron,2021;Caronetal.,2021;
Ziyinetal.,2022). Asourgoalistoanalyzetheeffectsofscaling,regardlessofsupervisionprotocol
used,wedonottrytocapturethisproperty.
Definition1. (Emergenceofacapability.) WesayacapabilityC isemergentwithscalingalonga
relevantaxis(e.g.,amountofdata,compute,parameters)if:
• P1: nonlinearimprovementoccursintheperformanceofataskwhereC isrequired;
• P2: multipletaskssimultaneouslyshownonlinearperformanceimprovement;and
• P3: themodelundergoesastructuralchangethatisinstrumentaltolearningthecapabilityC,
andnonlinearprogressinC’slearningdirectlycorrelateswiththelearningofsaidstructure.
Thedefinitionaboveassignsabroadermeaningtoemergencethanmeresuddenperformanceimprove-
mentonanarrowtask: itarguesthereshouldbeprecisestructuralchangesinthemodelthathave
downstreameffectsonseveralcapabilities,henceleadingtosuddenimprovementsinperformanceon
severaltasks. Notethatweintentionallyleavethenotionof‘structure’informalinthedefinition. The
salientpropertyofastructureisthatifamodellearnsit,downstreamtasksshouldbecomeeasierto
perform. Forexample,afine-grainednotionofastructurecanbeprevioustokenandcopyattention
headsthatleadtoin-contextlearning(Reddy,2023;Edelmanetal.,2024;Olssonetal.,2022);amore
coarse-grainedstructurecanincludethemodellearningthesyntacticalrulesofalanguagethathelpit
withgenerationofcoherentlanguageandhencewithanytaskwherecoherenceisimportant(Chen
etal.,2024). Inthissense,whatisemergentisastructure,andwhatisobservedisachangeinthe
model’scapabilities. Hypothesizingwhatthisstructureisbyidentifyingsharedcharacteristicsof
asetoftasksthatsimultaneouslyshowsuddenlearning, onecandevelopanevaluationmeantto
preciselygaugelearningofthecorrespondingstructureandhenceinferatwhatpointanindependent
trainingrunwillshowsuddenimprovements.
We note the intuition for Defn. 1 comes from prior work in the fields of complex systems and
physics(Anderson,1972;Newmanetal.,2001;Newman,2003),fromwherethetermhassought
itsinspirationinrecentmachinelearningliterature(Steinhardt,2023;Weietal.,2022). Therein,
emergencedescribesthescenariowhererapidchangesoccurinasystem’spropertiesassomecontrol
parameterisvaried. Arangewherethesystem’spropertieschangerelativelysmoothlyiscalleda
phase,andachangeofphasewithachangeinthecontrolvariableiscalledaphasetransition. A
crucialstepinstudyingemergenceinphysicsisidentifyinganorderparameter—ameasurethat
capturestheformationofsomespecificstructureinthesystemsuchthatthedevelopmentofthis
structureiswhataltersthesystem’spropertiesanddrivesaphasetransition. Forexample,inFig.1a,
asystemofparticlestransitionsthroughphases(solid,liquid,gas)asthetemperatureischanged;the
formationofacrystallinestructurewiththedecreaseintemperaturecanbeidentifiedbyanalyzing
thebond-orientationorderparameter,whiletheliquid-to-gastransitioncanbedescribedbyajumpin
particledensity. Wearguethatwemustsimilarlydefineorderparametersforstudyingemergencein
neuralnetworksaswell,i.e.,wemustdevelopevaluationmeasuresthatarefocusedtowardsdetecting
thelearningofspecific,narrowstructuresthataregenerallyofusetoseveraldownstreamcapabilities.
4 FORMAL LANGUAGES AS AN EXPERIMENTAL SYSTEM FOR EMERGENCE
Havingestablishedourperspectiveonemergence,wenowdefineatoyexperimentalsystemthat
allows us to precisely study the concept in a controlled setting. We note that our focus will be
on emergence under data scaling in an online learning scenario (i.e., a sample is unlikely to be
seenmultipletimes). Tothisend,wefollowrecentworkonunderstandinglanguagemodelingand
useformallanguagestodefineourexperimentalsetup(Allen-Zhu&Li,2023b;Jainetal.,2023;
Allen-Zhu&Li,2023a;Valvodaetal.,2022;Liuetal.,2023a;2022a). Asdiscussedindetailnext,
theformallanguageweuseinthisworkis(minimally)context-sensitive,withunderlyingsyntactical
4Preprint
(a) (b) (c)
Man tall S
S → NP VP
Woman walk
NP →p1 Adj N Lawyer eat NP VP
NP
→p2
N
NP
→p3
NP Conj. NP
Doctor jump VP NP
VP
→p 1′
Adv V
T Ca hb al ie
r
w vio no tad ge en Adj N Adj V Adj N
VP →p 2′ VP Conj. VP Sofa antique Tall man slowly walked tosmall library
VP →p 3′ VP NP Tall sofa slowly walked tosmall library
Kitchen foldable Incompatible type!
Figure2: Grammarandtypeconstraintstodefineourformallanguage. (a)WeuseaPCFG
to define our language’s grammar (shown rules are examples; see App. D.3 for precise details).
Thegrammar’sterminalsareparts-of-speechfromEnglishandyieldsymbolicsentencesthatcan
be populated by tokens from the language’s vocabulary. (b) Akin to natural language, wherein
propertiesofanentityconstrainsentencesseeninadatasetcorrespondingtothatentity,wedefine
constraints(calledtypeconstraints)onourlanguagethatrestrictwhichtokenscanbeseentogetherin
asentence. Theseconstraintsmapentitiestodescriptiveorrelativeproperties,hencerestrictingwhich
descriptiveadjectivesandverbsarevalidforanentity. (c)Onceasymbolicsentenceissampledfrom
thegrammar,wepopulateitwithtokensfromthelanguagewhilerespectingthetypeconstraints.
Trainingonstringfromthislanguageinfactshowsthatthemodeldeemssentencesthatdonotrespect
typeconstraintstobeextremelyunlikely(seeApp.D.2).
rules defined using a context-free grammar and context-sensitivity enabled through posthoc type
constraints. Thegrammarandtypeconstraintsserveastwostructuresthatunderlieourlanguage,
and,asweshowinSec.7.2,theirlearningbottleneckslearningofother,narrowercapabilities.
Definition2. (Grammar.) ConsiderasetofsymbolsT(alsocalledterminals). Asymbolicstringσ
isdesignedbychainingsymbolsfromt∈T;e.g.,σ =t t ...EOS,whereEOSisaspecialsymbol
1 2
thatmarkstheendofstringσ. ThesetT∗ denotesallpossiblestringsdefinedusingsymbolsfrom
T∪EOS. Meanwhile,asetofsymbolsNT(alsocallednon-terminals)defineproductionrulesQofthe
grammar,representedasA→αβ,whereA∈NT∪SiscalledtheLHSandα,β ∈T∪NTiscalled
theRHSoftherule,andSdenotesaspecial‘start’symbol. Aprobabilisticcontext-freegrammarGis
definedasarandomizedprocessthatfirstmapsStoasetofnon-terminalsaccordingtorulesQ,and
thenrepeatstheprocessovertheseintermediatenon-terminalsbyrandomlysamplingrulesforwhich
previouslysamplednon-terminalsaretheLHS.Theprocesscontinuesuntilonlyterminalsymbols
constitutetheRHS.Thisyieldsastringσ ∈Σ,whereΣ⊂T∗denotesthesetofallstringsthatcan
besampledfromG. Wesayarandomlysampledstringσ ∈T∗isgrammaticalifσ ∈Σ.
For an overview, see Fig. 2 (a) or App. A.1. The terminal symbols used in our work include
standard parts-of-speech from English, specifically: subjects, objects, verbs, adjectives, adverbs,
conjunctions,determiners,andprepositions. Thisimpliesmultipleshortphrasescanbecombined
togetherviaconjunctionsandverbs(e.g.,averbcanconnectasubjectphraseandanobjectphrase).
Weoftenusetheterm‘entities’tojointlyrefertosubjectsandobjects. Wealsodistinguishbetween
attributiveandpredicativeadjectives,usingthetermadjectivestosolelyrefertoadjectivesthatare
attributiveinnatureandtheterm‘descriptors’torefertoadjectivesthatarepredicative. Overall,
thegrammarGyieldssymbolicstringsthataresolelycomprisedofpartsofspeech. Forexample,
ourgrammarmightyieldastringlikeadjective subject adverb verb preposition
adjective object. Wenextmapthesesymbolicstringstoourlanguage.
LetV denotethevocabularyofourlanguageL. Eachtokenv ∈V hasasymbolicrolet:=role(v)
associated with it. The set of possible roles is denoted T, which is equal to the set of terminal
nodes of grammar G. Thus, one can define a context-free language by simply sampling a string
σ from G, and then replacing its terminals with tokens from the vocabulary that match the roles
specifiedbysymbolsinσ. Forexample,theexamplestringabovecanberesolvedasTall man
slowly walked to short building. However,naturallanguageisrichwithconstraints
definedbythephysicalpropertiesofanentity,whichtherebyrestrictwhichtokensareseeninthe
contextofwhichothertokens,henceyieldingcontext-sensitivity. Forexample,onedoesnotexpectto
seeasentenceTall telephone slowly walked to short building,sincetheentity
5Preprint
telephone is neither expected to be tall nor to walk. We develop an abstraction for such
constraintsbyrepresentingthemasabipartitegraph(seeFig.2(b,c)).
Definition 3. (Type Constraints Graph.) Let a property k be a binary variable; the set of all
propertiesisdenotedK. Propertiescanbeeitherdescriptive(usedtodefinedescriptors;e.g.,tall)
orrelative(usedtodefineverbs;e.g.,walk). AconceptclassCisdefinedviathesetK ⊂ Kthat
C
denoteswhichpropertiesarevalidforthatclass. WhenthepropertiesinK takevalues,wegetan
C
entityefromtheclass,denotedase ∈ C. Entitieshaveuniqueidentifiersassociatedwiththemto
helpdefinesubjectsandobjectsinasentence. ThesetofallpossibleentitiesisdenotedE. Thetype
constraintsgraphG := (E,K,I)isabipartitegraphoverentitiesandpropertiesinthelanguage
whoseedgesIdenotewhetheranentitye∈Epossessespropertyk∈K.
Asanexample,considertheclassofHumans,whichincludesentitiesconnectedtopropertieslike
tall,right-handed,etc.;anentityfromhumanswillbeassignedasubsetoftheseproperties.
Whensamplingsentencesfromourformallanguage,thetypeconstraintswillrestrictwhichtokens
can be seen together, i.e., which descriptors and verbs go with an entity, hence yielding context-
sensitivityandmakingLaprobabilisticcontext-sensitivelanguage. Giventworandomlysampled
entitiesfromthesameclass,theycanbeexpectedtoshareasubsetofvalues,givingasignaltothe
modeltrainedonLthattheseentitiesarerelated(i.e.,theybelongtothesameclass).
5 LEARNING TASKS AND EXPERIMENTAL SETUP
Having described our language L, now we
Task Input Output
briefly discuss our experimental setup (see Free: [null] “tall man slowly walked to small library.”
App.Cfordetails). WetrainaGPTarchitecture Unscramble: [library, walked, …, to, slowly] “tall man slowly walked to small library.”
model (Andrej Karpathy, 2023) with the stan- Conditional: [man, walked] “tall man slowly walked to small library.”
dard autoregressive language modeling objec-
Figure3: Taskdefinitions. Ourmodelistrained
tive. Dataissampled“online”,i.e.,wesample
andevaluatedonthreetypesoftasks. (i)Freegen-
afreshbatchofstringseveryiterationfromL.
eration:themodelgeneratessentenceswithcorrect
Unlessmentionedotherwise,Lisconstitutedof
grammar. (ii) Unscrambling: the model is pro-
|E| = 900entitiesand|K| = 18000properties,
videdwithasetofwordsandmustreorderthemto
equallyanddisjointlydistributedover|C|=10
formvalidsentences. (iii)Conditionalgeneration:
classes, and with edges connecting entities to
modelisgivenasetofentitiesorpropertiesand
p = 0.1 fraction valid properties of a class in
must generate valid sentences using them. Note
a uniformly random manner; results ablating
thatexamplesinthefigurearemerelyindicative.
thesesettingsareinApp.D.Beforebeingfed
SeeApp.A.1fordetails.
intothemodelfortrainingorevaluation,strings
sampledfromthelanguagearerestructuredintoaformatthatenablesthespecificationofparticular
tasks(seeFig.3). Specifically,wetrainthemodeltolearnthefollowingtaskswith80/10/10%splits.
• Freegeneration: Produceavalidstring,i.e.,onethatrespectsthegrammarandtypeconstraints.
• Unscrambling: AstringissampledfromLandrandomlypermuted;themodelisexpectedto
unscrambleit. ThistaskisknowntoshowsuddenlearninginLLMs(Weietal.,2022).
• ConditionalGeneration: Asetoftokenscorrespondingtoentitiesorpropertiesareshowntothe
model,whichisexpectedtogenerateastringcombiningthesetokensinavalidmanner.
EvaluationProtocols. Givenaninputx, whichmaycorrespondtoanyofthethreetasksabove,
denote the model output as f(x). Let δ(.) be an indicator variable that evaluates to 1 if its input
is true. We track several metrics throughout training, as described below. We often decompose
theseevaluationsaccordingtostringsoftwotypes: (i)descriptive,i.e.,thatdescribesthatanentity
possessesadescriptiveproperty, and(ii)relative, i.e., thatclaimsasubject, object, andverbcan
matcheachothertocreateavalidsentence. Unlessnotedotherwise,resultsareaveragedover3seeds.
• Grammaticality/TypeCheck. Usedforevaluatingfreegeneration. Grammaticalityinvolves
checkingwhethermodeloutputfollowstheunderlyinggrammarG,i.e.,f(x)∈Σ. Typechecks
involvefirstextractingsubjects,objects,andpropertiesfromthesentenceandthenevaluating
whetherthissetoftokensisallowedinthecontextofeachother. Wedecomposetypechecksas
descriptive(doentitiesanddescriptorsmatch),relative(dosubject,object,andverbmatch),and
all(productofallconstraints,includingadjectivesandadverbs).
6Preprint
• ExactMatch/PerTokenAccuracy. Usedforevaluatingunscrambling. Assumetheground-truth
unscrambledsentenceyhasltokens. Wecomparewhetherthemodeloutputexactlymatchesthe
ground-truth(cid:0) Πl i=1δ(y
i
=f(x) i)(cid:1) ortheper-tokenmatchratio(cid:16) 1/l(cid:80)l i=1δ(y
i
=f(x) i)(cid:17) .
• ConditionsSatisfied. Usedforevaluatingconditionalgeneration. Ifthemodelisexpectedto
produceasentencewithmconditioningtokens{v ,...,v },weanalyzehowmanyofthose
tokensarepresentinf(x),i.e.,weevaluate1/m(cid:80)m ic =1
1δ(v
cic ∈m
f(x)).
• AverageProbabilityofValidTokens. Usedforevaluatingdescriptivetypeconstraintsinfree
generation and unscrambling. Specifically, we sample a sentence from L that remarks on an
entity possessing a property (i.e., a descriptive sentence), and then evaluate the probability of
thepropertybeingthenexttokenwhenthissentenceisinputtedtothemodel. Forexample,let
x =The fire was large. WeevaluatePr(δ(f(x) =large)|x ),wherex denotes
1 −1 −1
thesentenceuptothelasttokenandf(x) denotesthefirsttokenpredictedbythemodel.
1
MiscellaneousEvaluations. Wealsoperformseveralotherevaluations(seeApp.D),e.g.,computing
log-likelihoodsofvalidversusadversariallyperturbedsentencesthatdonotfollowgrammarortype
constraintstocheckhowwellthemodelfollowsourlanguage;comparingthedistributionoflengths
andparsetreedepthformodel’sgenerationswiththelanguage’s;analyzinggrammaticalityandtype
checkaccuracyforunscramblingandconditionalgeneration;rankofpropertypredictions;andthe
evolutionofattentionmapsacrosstime.
6 RESULTS: EMERGENT CAPABILITIES IN FORMAL LANGUAGE LEARNING
Wenowevaluate(i)whetheroursetupdemonstratesemergence(seeDef.1),and(ii)whetherwecan
extractinsightsintothemechanismsofwhatleadstoemergence. Inthefollowing,weoftenusethe
terms“phase”and“phasechange”;seediscussionaroundDef.1forcontextontheseterms.
6.1 PHASESOFLANGUAGEANDCAPABILITIESACQUISITION
We plot the model’s performance as a function of training iterations. Since we are in an online
learning, constant learning rate setting, this analysis corresponds to studying the effects of data
scaling. ResultsarereportedinFig.4andshowtherearethreephasestothelearningdynamics.
Phase1: Grammaracquisition. Wefindthemodelfirstlearnstoproducegrammaticallycorrect
sentences,asmeasuredbythegrammaticalitymeasuredefinedinSec.5. Thisprocessisrelatively
rapid,asweseethemodelstartsgeneratinggrammaticallyaccuratesentencesinashortperiodof
approximately100iterations;attentionheadsalsorapidlyevolveandreflecttheparsestructureofa
sentence(seeApp.D.6)Inthisregime,however,thenarrowertasksofunscramblingandconditional
generationexhibitpoorperformance. However,preciselywhengrammaticalityimproves,wefind
thatper-tokenaccuracystartstoimprove. Thisindicatesthatthemodellearningabroadstructure
underlyingthedata(i.e.,grammar)hasanimpactonthelearningofothercapabilities.
Phase 2: Acquisition of relative type constraints. At around 1000 iterations, we find there is
a sudden increase in the model’s performance on relative types from essentially zero to perfect
accuracy; preciselyatthispoint,wefindthelossforalltasks,especiallyfreegeneration,showa
suddendrop. Interestingly,wefindthissuddenimprovementoccurspreciselyatthepointwherethe
modelreachesitsmaximumperformanceongrammaticalityforthefirsttime. Thatis,assoonasthe
firststructureunderlyingthedataisacquired,themodelrapidlylearnsthenextrelevantstructureof
relativetypeconstraints. Improvementoccursindescriptiveconstraintsaswell(andhencetheoverall
TypeCheckperformance),buthoversaroundslightlyaboverandomperformanceof0.1. With|C|
classes,ifamodelproducesgrammaticallycorrectsentences,itwillachievearandomperformance
of1/|C|ondescriptivetypechecks. Thishoweveralsoimpliesthatthemodelisprimarilyrelyingon
itssyntacticalknowledgeanddoesnotrespectdescriptivetypeconstraintsmuch.
Duringthisphase,weseethatshortlyafterthephasechange,thereisasuddenincreaseinperformance
forbothunscramblingandconditionalgeneration,acrossallmetrics. Thesetasks’lossesalsoshow
anotherlossdropoccursatthispoint;thoughthedropseemssmootherinthetotalloss,likelydue
to averaging effects (Michaud et al., 2023). As shown in Fig. 4e, we find that this performance
improvementisdrivenbysentencesthatrequireprimarilycorrectnessofgrammarandrelativetype
7Preprint
constraints,i.e.,knowledgeofwhichdescriptorsareassociatedwithanentityisnotnecessaryto
performwellonthesesentences. ThisalsoexplainsthelossdropseeninFig.4: oncegrammarand
relativetypeconstraintsarelearned,themodellearnstousethemtosolveinputsthatdonotrequire
knowledgeofdescriptiveproperties,leadingtoasuddenimprovementinbothlossandaccuracy.
Phase 3: Learning of descriptive type constraints. During Phase 2, we find that the model’s
performance on descriptive type checks witnesses minimal improvement. However, as training
proceeds,themodelentersathirdphaseatwhoseboundaryweseeasuddenchangeofslopefroma
saturationregiontoapproximatelylineargrowthintheperformanceofdescriptivetypecheckswith
log-amountofdata/iterations(i.e.,sublineargrowth). Withaslightdelay,weseeasimilareffectkicks
infortheunscramblingandconditiongenerationtasksaswell,whichstarttoshowapproximately
linearimprovementwithlog-amountofdata/iterations. Zoominginatthispoint(seeinsetplotsin
Fig.4),weseethereisinfactasmall,butneverthelessnoticeable,lossdropintheunscrambling
andconditiongenerationtasks. Weemphasizethatsincethemodelhasseenmerelyanorderof104
iterationsuptothispoint,ifweassumethemodelcanperfectlylearninonlyafewobservations
thatsomeentityandapropertycanbeseentogetherinasentence,thenourexperimentalsetting
canonaverageseeonlyupto15%performance(whichmatchestheobservedperformance)and
20%atbest(seeApp.C.0.1; theargumentisthatonlyasubsetofpairsisshownduringtraining,
restrictingmaximumperformance). However,asthemodelentersandprogressesthroughthethird
phase,itshowsamuchlargerrateofimprovementandreaches∼30–35%performance,indicating
itisgeneralizingbeyondthepairsofentitiesandobjectsithasseentogetherduringtraining. Ifthe
modelweresimplyrelyingonmemorizedknowledge,theperformancevaluesweobservedwould
(a) (b) (c)
(d)
(e)
Figure 4: Learning of structures drives emergent capabilities. For a detailed discussion, see
main text. (a) Grammaticality and Type Check evaluations as a function of iterations or data
(iterations×batch-size). Weseephasesinthelearningdynamicscorrespondingtoemergentacquisi-
tionofstructuresunderlyingourlanguage: grammar(black),relativetypeconstraints(pink),and
descriptivetypeconstraints(green),shadedgray,pink,andgreenrespectively. (b,c)Performance
onUnscramblingandConditionalGeneration. Afteraslightdelayfromphaseboundaries,wesee
suddenimprovementsintheperformanceofindividualtasks. (d)Learningcurves. Lossalsoshows
suddenchangesatphaseboundariescorrespondingtotheacquisitionofstructures. (e)Performance
ondescriptive/relativesentences. Decomposingbysentencetype,wefindasublineargrowthin
descriptivetypechecksdrivesperformanceboostondescriptivesentencesfortheunscramblingtask.
8Preprint
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Type Check (Desc.) Unscramble (Desc.) Type Check (Rel.) Unscramble (Rel.)
0.35 0.4 1.0
0.30 0.8 0.3
0.3
0.25 0.6
0.2 0.2
0.20 0.4
0.15 0.1 0.1
0.2
0.10
0.0 0.0 0.0
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure5: Effectofscalingnumberofdescriptiveproperties. Scalingthenumberofdescriptive
propertiesinourlanguage(seelegend),wefindrelativetypechecksandunscramblingperformance
forrelativesentencesareessentiallyunaffectedbythenumberofproperties. Meanwhile,bothtype
checkandunscramblingperformancefordescriptiveconstraintsandsentencesshowachangein
performanceanddelayintransitionpoints(mostprominentlyseenforthedelayintheemergence
of unscrambling ability). Despite these effects, we find the geometry of performance curves is
extremelyconsistent;fordescriptivetypechecks,thisgeometryalignsindicatesamemorizationto
generalizationpicture.
notbefeasible. Wethusclaimthatthemodelisimplicitlyinferring,basedonthestructureofthe
typeconstraintsgraph, whichpropertiesandentitiesconstituteavalidcontext. Thissuggestsa
memorizationeffectisatplayduringPhase2,andthephaseboundarycorrespondstoatransition
fromamemorizingtoageneralizingsolution.
Takeaways. Whiletheresultsaboveareforaspecificconfiguration,wefindtheclaimsconsistently
generalize to an extremely broad array of experimental settings (see App. D). Specifically, we
consistently see that the model first learns the two structures underlying our language (grammar
andtypeconstraints),andthenwitnessesperformanceimprovementsonnarrowertasksinthedata.
Drawingonthephysicsanalogybefore,weemphasizethatinthissensegrammaticalityandtype
constraintsserveasourorderparameters(hereorderisthegrammarandtypes),andaresufficientto
approximatelypredictwhensuddenperformanceimprovementswillbeseenforatask. Wealsonote
thatourresultsarenotsensitivetospecificchoiceofmetrics,asshownbylosscurvesandseveral
othermetricsdiscussedinApp.D.
6.2 EFFECTOFNUMBEROFDESCRIPTIVEPROPERTIESONLANGUAGEACQUISITION
Given the above picture of a model’s learning dynamics, we next ask how the phase boundaries
changewithanincreaseinthenumberofproperties|K|(seeFig.5). Weintentionallyscaleonly
the number of descriptive properties and hypothesize the learning of both grammar and relative
typeconstraintstonotbeaffectedbythischange. Werelegategrammarlearningtoappendix(see
App. D.2), which, as expected, is not affected by |K| since it is an entirely independent structure
fromtypeconstraints. However,weseeevenrelativetypeconstraints’learningisnotaffectedbythe
increaseindescriptiveproperties,indicatingthemodeldeemsthem(justifiably)tobeindependent
structureswhoselearningdoesnotinteractwitheachother. Focusingondescriptivetypeconstraints
then, we see performance curves for descriptive type checks and unscrambling performance on
descriptivesentencesareindeedaffected,achievinghighervaluesforfewerproperties(i.e.,theeasier
task).
Wefurthermaketwomoreinterestingobservations. (i)Thepointoftransitionfrommemorizationto
generalizationisdelayedasweincreasethenumberofproperties. Thisismostprominentlyseenin
thedelayinthetransitionpointwheretheabilitytounscrambledescriptivesentencesemerges. (ii)
Wefindthegeometryoftheseperformancecurvesareextremelysimilartothegeometryweobserved
forourbasesettingstudiedinSec.6.1,indicatingdespitetheincreaseindifficultyofthetask,the
samelearningdynamicsareatplay. Wedevotethenextsectiontoformulateahypothesisjustifying
theseobservations.
9Preprint
7 A PERCOLATION MODEL OF EMERGENCE
We next propose a framework for modeling the emergence of capabilities that require a model
to compose unseen entities and descriptive properties, e.g., learning descriptive type constraints,
which,beyondallowingamodeltoproduceaccuratefreegenerations,willaidwithnarrowertasks
likeconditionalgenerationandunscrambling. Wearguetherelevantstructuretoanalyzeforthis
purposeistheconceptclass: ifamodelunderstandswhatentitiesandpropertiesbelongtothesame
conceptclass,regardlessofwhethertheyhavebeenseentogetherinasentence,itwilldeemtheir
co-occurrencetobevalid. Wethusdevelopanabstractionforconceptclassesasbipartitegraphsand
casttheirlearningasaproblemofpercolationonabipartitegraph.
7.1 MATRIXREPRESENTATIONOFDATAANDLEARNINGCOMPOSITIONS
(a) (b)
Recall that a concept class is defined as
a set of entities that are expected to have
sharedproperties(seeDef.3). Thequestion
iswhetheruponsub-samplingpairsofenti-
tiesandpropertiesfromaconceptclass,can
themodellearnthat,infact,allpairsofen-
titiesandpropertiesarevalidandcompose
theconceptclass. Forinstance,inthecase
of a concept class such as human, the set Figure6: Castingtheabilitytocomposeunseencon-
cepts (e.g., entities and properties) as percolation
ofentitiescanincludehumanswithdiffer-
ent genders (e.g., man) as well as human- on a bipartite graph. (a) When only a fraction of
associatedentitiessuchasalawyer(see theconceptclassesareincludedinthedatasetandthe
edge density is low, nodes (e.g., entities and proper-
Fig. 2). The corresponding properties for
thehumanconceptclasswillbe,forexam- ties) form many disconnected clusters, indicated by
ple,walk,jump,tall. Aman,beinghuman, different colors (left). As more concept classes are
added(dashededges)tothebipartitegraph,thesmall
isexpectedtohavealltheseproperties,al-
clusters begin to merge (middle). With a sufficient
though strings specifying these properties
foralawyermayberareorevenabsent numberofedges,amacroscopicnumberofnodescan
beconnected,formingasinglecluster(right).Thissug-
in the training data. We are interested in
geststhatamodelcancompositionallygeneralizeto
thecasewherethedata,suchasstrings,in-
mostconceptclasseswithenoughedgesandinference
cludes examples of these pairs of entities
steps. (c)Ourformalismestablishesthistransitionas
andproperties. Wecanrepresentthisbya
asecond-orderphasetransition,wherethesizeofthe
matrixwhoserowsandcolumnsrepresent
largest cluster increases non-linearly as the fraction
theentitiesandtheproperties,andthema-
ofconnectednodepairsisscaled. Showncurvesare
trixvaluesindicatethequantityordensity
fromsimulationsonbipartitegraphswiththenumber
ofdataavailableforeachcomposition,such
asanentity-descriptorpairing. Wecallthis of nodes equal to the number of entities and proper-
tiesinourformallanguageexperiments(seeSec.5or
matrixtheconceptdensitymatrix.
App.C).
Definition 4. (Concept Density Matrix.)
Let D be an |E| × |K| matrix with real-valued entries between 0 and 1, inclusive. Each entry
D represents the density for the entity and property pair (e,k) (e.g., the amount of data that
ek
representsthespecificcomposition),wheree∈{1,...,|E|}andk ∈{1,...,|K|}aretheindicesofthe
entitiesandproperties,respectively.
Forexample,considerthecasewheretherearethreevaluesofentitiesandproperties(|E|=|K|=3),
withentities(rows)being{Man,Lawyer,Telephone},andproperties(columns)being{Walk,
Stoic,Ring}. ThecorrespondingDcanbe:
(cid:32)1 1 0(cid:33)
D = 1 0 0 . (1)
0 0 1
AcommoncompositionsuchasMan walkingwillleadtoavalueof1attheintersectionofMan
andWalk,i.e.,D =1,whereD denoteselementatrow-iandcolumn-j. Conversely,ahighly
00 ij
unlikelycompositionlikeLawyer ringingwillbeabsentinthedataset,andwillberepresented
byazeroattherespectivematrixposition,i.e.,D =0. WecanalsoassumeforexamplethatMan
12
ringingoraTelephone walkingarerare,whichyieldsD =D =0.
13 31
10Preprint
Wenextintroducetheconceptpropagationmatrixtomodeltheinferenceofnovelentity-feature
combinationsfromtheincompletedatarepresentedinD.
Definition5. ConceptPropagationMatrix. Ann-thorderconceptpropagationmatrix(n≥0)is
definedasT(n) =(DDT)nD =CnD,whereC :=DDT.
Theconceptpropagationmatrixcanbeintuitivelyunderstoodusingabipartitegraph,asshownin
Figure6a. Abipartitegraphinthiscaseisasub-graphofthetypeconstraintsgraph(seeDef.3and
Fig.2),whereonesetofnodesrepresentsentitieswhiletheotherrepresentsproperties,withedges
indicatingthepresenceofentity-featurepairingsinthetrainingdata. Thestrengthofconnectivityof
thegraphdirectlycorrespondstothevaluesintheconceptcompositionpropagationmatrix,T(n).
Specifically,iftwoconceptsareconnectedbyapathofminimallength2k+1(i.e.,theshortestpath
betweenthemalternatesbetweenthetwosetsk times),thecorrespondingentryinT(n) becomes
non-zeroonlyforn ≥ k. Thatis, thenumberofpropagationstepsnrequiredfortheobjectand
featurepairtobeassociatedisdeterminedbytheminimalnumberofhopsneededtoconnectthe
twonodesinthegraph. Conversely,iftwonodesbelongtodisconnectedregionsofthegraph,their
compositionremainsfundamentallyunlearnable,regardlessoftheorderofpropagation,indicating
thatcompositionisnotvalid. ThisisreflectedbythecorrespondingentryinT(n)remainingzerofor
alln. Inthebipartitegraph,thisamountstohavingtwodistinctclustersthatareconnectedwithin
themselvesbutnotacrosseachother. Forexample,inthecasewheretheconceptsrepresentedbythe
firstandthirdrowsbelongtodisconnectedregionsofthegraph,andconsequently,theircomposition
(e.g.,Lawyer ringing)cannotbeachievedevenafteraninfinitenumberofhopsbetweennodes.
Wecallsuchasituationlearningofaconceptclass: thesystemunderstandsthatManandLaywer
arebothhumans,whereasTelephoneisnot. Ourexperimentsintheformallanguagesetupshow
thatthemodeldeemssentencescomposingentitiesandpropertiesfromincorrectclassestobemuch
lesslikelythanthecorrectones(seeFig.D.3).
7.2 PERCOLATIONTRANSITIONONDESCRIPTIVECONSTRAINTS
Usingthebipartitegraphframework,thegeneralization,orthelearningoftheconceptclass,canbe
definedasthesituationwherealargeclusterofentity-propertyconnectedpairsarisesdespitethe
sparseconceptdensitymatrix. Acriticalaspecttoexamineistheproportionoftheinferencematrix
valueswhereT(∞)isnon-zero,outofthetotalpossiblepairs|E|×|K|. Thisparticularscenarioaligns
ek
withthebondpercolationproblemonabipartitegraph. Inbondpercolation, weinvestigatehow
thelargestconnectedcluster’ssizevarieswiththeprobabilitypofeachedge(bond)beingpresent.
Inatypicalsetting,thereexistsacriticalthresholdvalue,p=p ,calledthepercolationthreshold.
c
Belowthisthreshold(p<p ),thegraphtypicallyexhibitsadisconnectedphasecharacterizedbythe
c
absenceofextensivelyconnectedclusters,withmostnodeseitherisolatedorpartofsmallerclusters.
Abovethisthreshold(p>p ),thegraphtransitionstoaconnectedphase,significantlyincreasingthe
c
likelihoodofavastconnectedcomponentspanningalargeportionofthegraph. Thisshiftfroma
predominantlydisconnectedstatetoonewithamacroscopicclusterisadefiningcharacteristicofthe
percolationprocess,andthistransitionsharpensasthenumberofcomponentsinthesystemincreases.
SeeFig.6foraschematic.
Inasimplepercolationscenario,whereconnectingedgesareselectedrandomlyonthegraphwith
(cid:112)
probabilityp,thepercolationthresholdisobtainedasp ≃ 1/|E||K|forlarge|E|and|K|(Newman
c
(cid:112)
etal.,2001),whichmeansthatwhenaround |E||K|edgesareconnected(outofthetotal|E||K|)there
isaqualitativechangeinthegrowthoftheclustersize. Forp>p ,thenumberofnodesincludedin
c
theconnectedclusterwillbecomemacroscopic,meaningthattheprobabilitythatarandomlyselected
pairofanobjectandafeatureisconnectedwillbefinite. WepresentinAppendixBthederivation
ofthepercolationthresholdforbipartitegraphsthatareuncorrelated,andhowtheclustersize(i.e.,
numberofnodesinthelargestconnectedgraph)scalesas∼(p−p )β withβ =1forusualcases.
c
Wepositthatthepercolationthresholdcorrespondstothepointatwhichourmodelgeneralizesfrom
thesparselearningofpairstoacompleterepresentationoftheconceptclasses. Whenthenumber
ofedgessurpassesthethreshold,themodelcaninfernovelcompositions,evenforentity-feature
pairsthatwerenotexplicitlypresentinthetrainingdata. Themodelshouldalsostarttobeableto
discriminatebetweendistinctconceptclassesbeyondthisthreshold; inthecommunitydetection
problem,forexampleinthestochasticblockmodel(Abbe,2018;Decelleetal.,2011),thedetection
thresholdforthepartitionshavethesamescalingasp Florescu&Perkins(2016). Sinceincreasing
c
11Preprint
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Avg. Prob. (Freegen) × |K|1.5 Ty. Check (Desc.) × |K|0.5 Avg. Prob. (Unscmb) × |K|0.0Ex. Match (Unscmb) × |K|0.0
700 40 0.4
0.3
35
600 0.3
30 0.2
500 0.2
25
400 0.1
0.1
20
300
15 0.0 0.0
101 102 101 102 10 4 10 3 10 2 10 4 10 3 10 2
Iterations / |K|0.5 Iterations / |K|0.5 Iterations / |K|1.5 Iterations / |K|1.5
Figure7: Scalingofpointofemergencematcheswithourtheory. WereplottheresultfromFig.5
byrescalingthex-axiswithapowerofthenumberofdescriptiveproperties;SeeApp.E.1forfurther
discussionontheintuitionunderlyingthisvisualization. ForAverageprobabilityofgeneratinga
validdescriptivepropertygivensomeobjectincontextunderfreegenerationanddescriptivetype
checkaccuracy,weseea0.5exponentscalingyieldsacollapseofthetransitionpoint—thismatches
thetoymodelofpercolationpositedinSec.7.2. Wealsoseeaveryclearscalingofthepointwhere
theabilitytounscrambledescriptivesentencesstartstoemerge,butwithanexponentof1.5.
theiterationsthroughonlinelearningshouldamounttoincreasingp(i.e.,seeingmorecombinations
in data), the iteration point at which the transition occurs in the model performance should be
(cid:112)
proportionalto |E||K|.
Tocheckwhethersuchtheoreticallypredictedscalingcanbeseeninourexperimentswithformal
languagelearning,weplotagainthevariousperformancesofthemodelasafunctionofiterations
dividedbythesquarerootofthenumberofproperties,sincewekeepthenumberofobjectsconstant
intheseexperiments. ResultsareshowninFig.7. Wefindthatindeedthereisagrowthtrendinthe
descriptivetypecheckmetricandtheaverageprobabilityscoresoffreegenerationofdescriptive
(cid:112)
sentencesthatoccuratiterationsproportionalto |K|. Thisisincontrasttothefirstlargegrowth
thatisobservedintheseevaluations,whichseemstobeoccurringatiterationnumbersthatdonot
dependon|K|(seeFig.5andFig.52),likelybecausethisstepcorrespondstothepointwherethe
modellearnsaboutsyntax,whichrequiresaconstantamountofdatairrespectiveofthenumberof
properties(seeFig.17).
Wealsoanalyzedthescalingofthetransitionpointforthenarrowertaskofunscramblinguponchange
inthenumberofproperties. Here,wefoundadifferentscalingof|K|3/2. Wesofardonothavean
explanationforthisscaling,butweassumeitislikelyduetothelearningofothermechanisms,asit
seemstobesensitivetoothercharacteristicsofthelanguage;e.g.,whenweincreased|E|from900
to1800,theexponentshiftedfrom3/2tocloseto2(seeFig.44),butremained0.5fordescriptive
typechecks(seeFig.54). Giventhetaskinvolvesthecompositionofentitiesandpropertiestoo,we
expectatransitionandscalingeffecttooccurforunscramblingaswell;however,learningtheprecise
circuittoperformunscramblingoverthelearnedgrammarandtypeconstraintswillyieldadelay(as
wasseeninexperiments)thatlikelyhassomeinteractionwiththecomplexityofthelanguage. We
leaveexplainingthisscalingforfuturework.
8 CONCLUSION
Inthiswork,wetakeinspirationfromotherfields(e.g.,physicsandcomplexsystems)andpropose
aphenomenologicaldefinitionforemergenceofcapabilitiesinneuralnetworks. Specifically,the
definition argues that at the point of emergence, the model acquires broad structures which are
instrumentaltothelearningofspecific, narrowercapabilities; acquisitionofsuchstructuresthen
leadstosuddenperformanceimprovementonseveraltasks(oftenwithsomedelay). Whilerelatively
informal, this definition brings the notion of emergence in the context of neural networks closer
to its meaning in physics, wherein the acquisition of specific structures is known to drive phase
changesthatinvolvesuddenchangesinthesystem’sproperties. Characterizingthesephasechanges
12Preprint
requireshypothesizingwhatthestructureis,anddefiningan“orderparameter”thatcanhelpgauge
itschange. Drawingonthisdefinitionandperspective,wethenproposeanexperimentalsetupthat
involveslearningofaformallanguagewithtwopreciselydefinedstructures—grammarandtype
constraints(whatpropertiesarevalidinthecontextofwhatentities)—andasetofnarrowlydefined
tasks. Definingorderparametersforthesestructures(grammaticalityandtypechecks),wefindthere
indeedarephasesinthemodel’slearningdynamics,andthemodelsuddenlyacquirescapabilities
correspondingtothenarrowerscopetasks(unscramblingandconditionalgeneration)closetothese
phaseboundaries. Interestingly,thelearningcurvesshowaratherdistinctgeometrythatremains
consistentaswealterthenumberofpropertiesinourlanguage.
Toexplaintheseresults,weproposeamodelthatanalogizeslearningoftypeconstraintsintheformal
languagelearningtasktotheproblemofgraphpercolation. Drawingonthetheoryofpercolationon
bipartitegraphs,whichshowsphasetransitionsintheformationofconnectedcomponentsonagraph,
wearguethisproblemissimilartolearningofconceptsclassesortypeconstraintsinoursetting,and
henceshouldshowscalingofthepointofemergencewherethemodelstartstofollowdescriptive
(cid:112)
typeconstraintsthatisoftheorderof |E||K|. Ourresultsshowastrongqualitativematchwith
thishypothesis. Wealsofindanextremelycleanscalingforothertasks’transitionpoint,e.g.,for
unscrambling’sresultsondescriptivesentences;explainingtheseresultsisleftforfuturework.
Whileourgoalinthisworkwasprimarilydemonstrative,i.e.,todevelopabridgewithotherfields
studying emergence, we believe several exciting avenues now open up. For example, given that
thewholepointofthetheoryofphasetransitionsandemergenceisthatwecanpredictthepoint
oftransition,canwedrawonthisrichliteraturetoproposemodelsforexplainingandpredicting
emergentcapabilitiesinneuralnetworks? Canwegobeyondthetoytaskofformallanguagelearning
studiedinthisworkandanalyzeamorenaturalisticsetting,e.g.,canweidentifystructuresunderlying
emergent capabilities in open-source models, e.g., Pythia variants (Biderman et al., 2023), and
demonstratethatourproposedperspectiveenablespredictionofwhencapabilitiesemergeinLLMs?
Tobegin,wecanperhapsfocusoncapabilitiesthatrequiresimilarknowledgeacquisitionandits
compositionalgeneralizationonadownstreamtask.
ACKNOWLEDGMENTS
TheauthorsthankIntelligentSystemsgroupatHarvard,especiallyMayaOkawa,CoreFrancisco
Park, and Leni Shor for feedback on the project and contributions to predecessors of this work.
ESL thanks Gautam Reddy, Naomi Saphra, Pulkit Gopalani, Wei Hu, and Yonatav Belinkov for
fruitfulconversations.ESL’stimeatUniversityofMichiganwassupportedbyNSFunderawardCNS-
2211509andatHarvardbytheCBS-NTTPhysicsofIntelligenceprogram.KKacknowledgessupport
fromJSPSKAKENHIGrantNumbersJP19H05795,JP19H05275,JP21H01007,andJP23H00095.
13Preprint
REFERENCES
EmmanuelAbbe. Communitydetectionandstochasticblockmodels: recentdevelopments. Journal
ofMachineLearningResearch,18(177):1–86,2018.
MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes,ByronDavid,Chelsea
Finn,ChuyuanFu,KeerthanaGopalakrishnan,KarolHausman,AlexHerzog,DanielHo,Jasmine
Hsu,JulianIbarz,BrianIchter,AlexIrpan,EricJang,RosarioJaureguiRuano,KyleJeffrey,...,
andAndyZeng. DoAsICanandNotAsISay: GroundingLanguageinRoboticAffordances. In
arXivpreprintarXiv:2204.01691,2022.
NISTAI. ArtificialIntelligenceRiskManagementFramework(AIRMF1.0),2023.
ZeyuanAllen-ZhuandYuanzhiLi. PhysicsofLanguageModels: Part3.2,KnowledgeManipulation.
arXivpreprintarXiv:2309.14402,2023a.
ZeyuanAllen-ZhuandYuanzhiLi. PhysicsofLanguageModels: Part1,Context-FreeGrammar.
arXivpreprintarXiv:2305.13673,2023b.
Shun-IchiAmari. Auniversaltheoremonlearningcurves. Neuralnetworks,6(2):161–166,1993.
PhilipWAnderson. Moreisdifferent: Brokensymmetryandthenatureofthehierarchicalstructure
ofscience. Science,177(4047):393–396,1972.
AndrejKarpathy. nanoGPT,2023. https://github.com/karpathy/nanoGPT.
RohanAnil, AndrewM.Dai, OrhanFirat, MelvinJohnson, DmitryLepikhin, AlexandrePassos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,
LaurentElShafey,YanpingHuang,KathyMeier-Hellstern,GauravMishra,EricaMoreira,Mark
Omernick,KevinRobinson,...,andYonghuiWu. PaLM2TechnicalReport,2023.
Anthropic. Introducing Claude, 2023. https://www.anthropic.com/index/
introducing-claude.Accessedon: June21,2023.
Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase,
EkdeepSinghLubana,ErikJenner,StephenCasper,OliverSourbut,etal. Foundationalchallenges
in assuring alignment and safety of large language models. arXiv preprint arXiv:2404.09932,
2024.
SanjeevAroraandAnirudhGoyal. Atheoryforemergenceofcomplexskillsinlanguagemodels.
arXivpreprintarXiv:2307.15936,2023.
BoazBarak,BenjaminEdelman,SurbhiGoel,ShamKakade,EranMalach,andCyrilZhang. Hidden
progressindeeplearning: SGDlearnsparitiesnearthecomputationallimit. AdvancesinNeural
InformationProcessingSystems,35:21750–21764,2022.
StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleO’Brien,Eric
Hallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal.
Pythia: Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling. InInternational
ConferenceonMachineLearning,pp.2397–2430.Pmlr,2023.
StevenBird,EwanKlein,andEdwardLoper. NaturallanguageprocessingwithPython: analyzing
textwiththenaturallanguagetoolkit. "O’ReillyMedia,Inc.",2009.
AnselmBlumer,AndrzejEhrenfeucht,DavidHaussler,andManfredKWarmuth. Learnabilityand
theVapnik-Chervonenkisdimension. JournaloftheACM(JACM),36(4):929–965,1989.
RishiBommasani,DrewA.Hudson,EhsanAdeli,RussAltman,SimranArora,SydneyvonArx,
Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,
ShyamalBuch,DallasCard,RodrigoCastellon,NiladriChatterji,AnnieChen,KathleenCreel,
JaredQuincyDavis,DoraDemszky,...,andPercyLiang. OntheOpportunitiesandRisksofFoun-
dationModels,jul2022. URLhttp://arxiv.org/abs/2108.07258. arXiv:2108.07258
[cs].
14Preprint
OlivierBousquet,SteveHanneke,ShayMoran,RamonVanHandel,andAmirYehudayoff. Atheory
ofuniversallearning. InProceedingsofthe53rdAnnualACMSIGACTSymposiumonTheoryof
Computing,pp.532–541,2021.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
models are few-shot learners. Advances in neural information processing systems, 33:
1877–1901, 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
Francesco Cagnetta and Matthieu Wyart. Towards a theory of how the structure of language is
acquiredbydeepneuralnetworks. arXivpreprintarXiv:2406.00048,2024.
MathildeCaron. Self-supervisedlearningofdeepvisualrepresentations. PhDthesis, Université
GrenobleAlpes[2020-....],2021.
MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski,and
ArmandJoulin. Emergingpropertiesinself-supervisedvisiontransformers. InProceedingsofthe
IEEE/CVFinternationalconferenceoncomputervision,pp.9650–9660,2021.
Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, and Naomi Saphra.
SuddenDropsintheLoss: SyntaxAcquisition,PhaseTransitions,andSimplicityBiasinMLMs.
In The Twelfth International Conference on Learning Representations, 2024. URL https:
//openreview.net/forum?id=MO5PiKHELW.
NoamChomsky. Threemodelsforthedescriptionoflanguage. IRETransactionsoninformation
theory,2(3):113–124,1956.
ReuvenCohen,DanielBen-Avraham,andShlomoHavlin. Percolationcriticalexponentsinscale-free
networks. PhysicalReviewE,66(3):036113,2002.
MichaelCollins. Probabilisticcontext-freegrammars(pcfgs). LectureNotes,2013.
CounciloftheEuropeanUnion.ProposalforaRegulationoftheEuropeanParliamentandoftheCoun-
cilonArtificialIntelligence(ArtificialIntelligenceAct),2024. https://data.consilium.
europa.eu/doc/document/ST-5662-2024-INIT/en/pdf.
Hugo Cui, Freya Behrens, Florent Krzakala, and Lenka Zdeborová. A phase transition between
positional and semantic learning in a solvable model of dot-product attention. arXiv preprint
arXiv:2402.03902,2024.
AurelienDecelle,FlorentKrzakala,CristopherMoore,andLenkaZdeborová. Asymptoticanalysis
ofthestochasticblockmodelformodularnetworksanditsalgorithmicapplications. Physical
ReviewE,84(6):066106,2011.
DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,Ayzaan
Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal
languagemodel. arXivpreprintarXiv:2303.03378,2023.
Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent abilities of
languagemodelsfromthelossperspective. arXivpreprintarXiv:2403.15796,2024.
BenjaminLEdelman,SurbhiGoel,ShamKakade,EranMalach,andCyrilZhang. Paretofrontiersin
neuralfeaturelearning: Data,compute,width,andluck. arXivpreprintarXiv:2309.03800,2023.
Benjamin L Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The
EvolutionofStatisticalInductionHeads: In-ContextLearningMarkovChains. arXivpreprint
arXiv:2402.11004,2024.
NelsonElhage,NeelNanda,CatherineOlsson,TomHenighan,NicholasJoseph,BenMann,Amanda
Askell,YuntaoBai,AnnaChen,TomConerly,NovaDasSarma,DawnDrain,DeepGanguli,Zac
Hatfield-Dodds,DannyHernandez,AndyJones,JacksonKernion,LianeLovitt,KamalNdousse,
...,andChrisOlah. AMathematicalFrameworkforTransformerCircuits. TransformerCircuits
Thread,2021. https://transformer-circuits.pub/2021/framework/index.html.
15Preprint
Laura Florescu and Will Perkins. Spectral thresholds in the bipartite stochastic block model. In
ConferenceonLearningTheory,pp.943–959.PMLR,2016.
DanFriedman,AlexanderWettig,andDanqiChen. LearningTransformerPrograms. arXivpreprint
arXiv:2306.01128,2023.
Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom
Conerly,NovaDassarma,DawnDrain,NelsonElhage,etal. Predictabilityandsurpriseinlarge
generativemodels. In2022ACMConferenceonFairness,Accountability,andTransparency,pp.
1747–1764,2022.
Gemini Team. Gemini: a family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805,2023.
Shreyas Gokhale. The semantic landscape paradigm for neural networks. arXiv preprint
arXiv:2307.09550,2023.
TianyuGuo,WeiHu,SongMei,HuanWang,CaimingXiong,SilvioSavarese,andYuBai. How
DoTransformersLearnIn-ContextBeyondSimpleFunctions? ACaseStudyonLearningwith
Representations. arXivpreprintarXiv:2310.10616,2023.
DavidHaussler,HSebastianSeung,MichaelKearns,andNaftaliTishby. Rigorouslearningcurve
boundsfromstatisticalmechanics. InProceedingsoftheseventhannualconferenceonComputa-
tionallearningtheory,pp.76–87,1994.
Tianyu He, Darshil Doshi, Aritra Das, and Andrey Gromov. Learning to grok: Emergence
of in-context learning and skill composition in modular arithmetic tasks. arXiv preprint
arXiv:2406.02550,2024.
RoeeHendel,MorGeva,andAmirGloberson.In-contextlearningcreatestaskvectors.arXivpreprint
arXiv:2310.15916,2023.
DavidTHoffmann,SimonSchrodi,NadineBehrmann,VolkerFischer,andThomasBrox. Eureka-
momentsintransformers: Multi-steptasksrevealsoftmaxinducedoptimizationproblems. arXiv
preprintarXiv:2310.12956,2023.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Anempiricalanalysisofcompute-optimallargelanguagemodeltraining. AdvancesinNeural
InformationProcessingSystems,35:30016–30030,2022.
DieuwkeHupkes,VernaDankers,MathijsMul,andEliaBruni. Compositionalitydecomposed: How
doneuralnetworksgeneralise? JournalofArtificialIntelligenceResearch,67:757–795,2020.
SamyakJain,RobertKirk,EkdeepSinghLubana,RobertP.Dick,HidenoriTanaka,EdwardGrefen-
stette, Tim Rocktäschel, and David Scott Krueger. Mechanistically analyzing the effects of
fine-tuningonprocedurallydefinedtasks,2023.
MargotKaminski. RegulatingtheRisksofAI. BostonUniversityLawReview,103:1347,2023.
LouisKirsch,JamesHarrison,JaschaSohl-Dickstein,andLukeMetz. General-purposein-context
learningbymeta-learningtransformers. arXivpreprintarXiv:2212.04458,2022.
BingbinLiu,JordanTAsh,SurbhiGoel,AkshayKrishnamurthy,andCyrilZhang. Transformers
learnshortcutstoautomata. arXivpreprintarXiv:2210.10749,2022a.
Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Exposing
AttentionGlitcheswithFlip-FlopLanguageModeling. arXivpreprintarXiv:2306.00946,2023a.
Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric J. Michaud, Max Tegmark, and Mike Williams.
Towards Understanding Grokking: An Effective Theory of Representation Learning, October
2022b.
ZimingLiu,EricJ.Michaud,andMaxTegmark. Omnigrok: GrokkingBeyondAlgorithmicData,
March2023b.
16Preprint
ShengLu,IrinaBigoulaeva,RachneetSachdeva,HarishTayyarMadabushi,andIrynaGurevych.
Are Emergent Abilities in Large Language Models just In-Context Learning? arXiv preprint
arXiv:2309.01809,2023.
AlexandraSashaLuccioniandAnnaRogers. Mindyourlanguage(model): Fact-checkingllmsand
theirroleinnlpresearchandpractice. arXivpreprintarXiv:2308.07120,2023.
William Merrill, Nikolaos Tsilivis, and Aman Shukla. A Tale of Two Circuits: Grokking as
CompetitionofSparseandDenseSubnetworks. arXivpreprintarXiv:2303.11873,2023.
EricJMichaud,ZimingLiu,UzayGirit,andMaxTegmark. Thequantizationmodelofneuralscaling.
arXivpreprintarXiv:2303.13506,2023.
Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher D. Manning. Grokking of
HierarchicalStructureinVanillaTransformers,May2023.
YoonsooNam,NayaraFonseca,SeokHyeongLee,andArdLouis. Anexactlysolvablemodelfor
emergenceandscalinglaws. arXivpreprintarXiv:2404.17563,2024.
NeelNanda,LawrenceChan,TomLieberum,JessSmith,andJacobSteinhardt.Progressmeasuresfor
grokkingviamechanisticinterpretability. InTheEleventhInternationalConferenceonLearning
Representations,sep2022. URLhttps://openreview.net/forum?id=9XFSbDPmdW.
Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent Linear Representations in World
Models of Self-Supervised Sequence Models, sep 2023. URL http://arxiv.org/abs/
2309.00941. arXiv:2309.00941[cs].
M.E.J.Newman. TheStructureandFunctionofComplexNetworks. SIAMReview,45(2):167–256,
January2003. ISSN0036-1445,1095-7200.
MarkEJNewman,StevenHStrogatz,andDuncanJWatts. Randomgraphswitharbitrarydegree
distributionsandtheirapplications. PhysicalreviewE,64(2):026118,2001.
Maya Okawa, Ekdeep Singh Lubana, Robert P Dick, and Hidenori Tanaka. Compositional abil-
ities emerge multiplicatively: Exploring diffusion models on a synthetic task. arXiv preprint
arXiv:2310.09336,2023.
CatherineOlsson,NelsonElhage,NeelNanda,NicholasJoseph,NovaDasSarma,TomHenighan,
BenMann,AmandaAskell,YuntaoBai,AnnaChen,TomConerly,DawnDrain,DeepGanguli,
Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane
Lovitt,...,andChrisOlah. In-contextLearningandInductionHeads. TransformerCircuitsThread,
2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
OpenAI. Gpt-4, 2023. https://openai.com/research/gpt-4. Accessed on: June 21,
2023.
TheWhiteHouseOSTP. BlueprintforanAIBillofRights,2023.
AlexanderPan,KushBhatia,andJacobSteinhardt. Theeffectsofrewardmisspecification: Mapping
andmitigatingmisalignedmodels. arXivpreprintarXiv:2201.03544,2022.
AletheaPower,YuriBurda,HarriEdwards,IgorBabuschkin,andVedantMisra. Grokking: Gen-
eralizationbeyondoverfittingonsmallalgorithmicdatasets. arXivpreprintarXiv:2201.02177,
2022.
JackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods,analysis&insightsfromtraininggopher. arXivpreprintarXiv:2112.11446,2021.
Gautam Reddy. The mechanistic basis of data dependence and abrupt learning in an in-context
classificationtask. arXivpreprintarXiv:2312.03002,2023.
RylanSchaeffer,BrandoMiranda,andSanmiKoyejo. AreemergentabilitiesofLargeLanguage
Modelsamirage? arXivpreprintarXiv:2304.15004,2023.
17Preprint
TimoSchick,JaneDwivedi-Yu,RobertoDessì,RobertaRaileanu,MariaLomeli,EricHambro,Luke
Zettlemoyer,NicolaCancedda,andThomasScialom. Toolformer: Languagemodelscanteach
themselvestousetools. AdvancesinNeuralInformationProcessingSystems,36,2024.
HyunjuneSebastianSeung,HaimSompolinsky,andNaftaliTishby. Statisticalmechanicsoflearning
fromexamples. PhysicalreviewA,45(8):6056,1992.
MichaelSipser. Introductiontothetheoryofcomputation. ACMSigactNews,27(1):27–29,1996.
AarohiSrivastava,AbhinavRastogi,AbhishekRao,AbuAwalMdShoeb,AbubakarAbid,Adam
Fisch,AdamRBrown,AdamSantoro,AdityaGupta,AdriàGarriga-Alonso,etal. Beyondthe
imitationgame: Quantifyingandextrapolatingthecapabilitiesoflanguagemodels. arXivpreprint
arXiv:2206.04615,2022.
Jacob Steinhardt. Emergent Deception and Emergent Optimization, feb 2023. URL https:
//bounded-regret.ghost.io/emergent-deception-optimization/.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,LukasBlecher,Cris-
tianCantonFerrer, MoyaChen, GuillemCucurull, DavidEsiobu, JudeFernandes, JeremyFu,
WenyinFu,...,andThomasScialom. Llama2: OpenFoundationandFine-TunedChatModels,
2023.
JosefValvoda,NaomiSaphra,JonathanRawski,AdinaWilliams,andRyanCotterell. Benchmarking
compositionalitywithformallanguages. arXivpreprintarXiv:2208.08195,2022.
TomVieringandMarcoLoog. Theshapeoflearningcurves: areview. IEEETransactionsonPattern
AnalysisandMachineIntelligence,2022.
TimothyLHWatkin,AlbrechtRau,andMichaelBiehl. Thestatisticalmechanicsoflearningarule.
ReviewsofModernPhysics,65(2):499,1993.
JasonWei. 137emergentabilitiesoflargelanguagemodels,2022. https://www.jasonwei.
net/blog/emergence.Accessedon: October20,2023.
JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,
MaartenBosma,DennyZhou,DonaldMetzler,etal. Emergentabilitiesoflargelanguagemodels.
arXivpreprintarXiv:2206.07682,2022.
KaiyueWen,YuchenLi,BingbinLiu,andAndrejRisteski. (Un)interpretabilityofTransformers: a
casestudywithDyckgrammars. 2023.
DingliYu,SimranKaur,ArushiGupta,JonahBrown-Cohen,AnirudhGoyal,andSanjeevArora.
Skill-Mix: A flexible and expandable family of evaluations for AI models. arXiv preprint
arXiv:2310.17567,2023.
JiahuiYu,YuanzhongXu,JingYuKoh,ThangLuong,GunjanBaid,ZiruiWang,VijayVasudevan,
AlexanderKu,YinfeiYang,BurcuKaragolAyan,etal. Scalingautoregressivemodelsforcontent-
richtext-to-imagegeneration. arXivpreprintarXiv:2206.10789,2(3):5,2022.
Liu Ziyin, Ekdeep Singh Lubana, Masahito Ueda, and Hidenori Tanaka. What shapes the loss
landscapeofself-supervisedlearning? arXivpreprintarXiv:2210.00638,2022.
BojanŽunkovicˇ andEnejIlievski. Grokkingphasetransitionsinlearninglocalruleswithgradient
descent,October2022.
18Preprint
A DATA-GENERATING PROCESS: DEFINING OUR FORMAL LANGUAGE
Our data-generating process involves defining a formal language, sampling sentences from this
language,andthendefiningtaskstobeperformeduponthesesentences(specifically,freegeneration,
unscrambling,orconditionalgeneration). Inthissection,wediscusstheprecisedetailsofhowthe
languageisimplemented.
A.1 DEFININGAGRAMMARUSINGPCFGS
Todefineagrammarforourlanguage,weusetheframeworkofProbabilisticContext-FreeGrammars
(PCFGs). Tokeepthepaperself-contained,weprovideashortprimeronPCFGsbelowandthen
discussourpreciseversionofitindetail. ForamorethoroughdiscussiononPCFGs,wereferthe
readertooneoftheseveralwell-writtentutorials(Collins,2013)andbooks(Sipser,1996).
A.1.1 SHORTPRIMERONPCFGS
Broadly,aPCFGisdefinedviaa5-tupleG=(N,Σ,R,S,P),where:
• NTisafinitesetofnon-terminalsymbols.
• Tisafinitesetofterminalsymbols,disjointfromNT.
• R is a finite set of production rules, each of the form A → αβ, where A ∈ N and
α,β ∈(NT∪T).
• S ∈N isthestartsymbol.
(cid:80)
• P isafunctionP :R→[0,1],suchthatforeachA∈NT, P(A→αβ)=1.
α:A→α∈R
TogenerateasentencefromaPCFG,thefollowingprocessisused. Pseudocodeforthisgeneration
processisprovidedinAlgo.1.
1. StartwithastringconsistingofthestartsymbolS.
2. Whilethestringcontainsnon-terminalsymbols,randomlyselectanon-terminalAfromthe
string. ChooseaproductionruleA→αβ fromRaccordingtotheprobabilitydistribution
P(A→α).
3. Replacethechosennon-terminalAinthestringwithα,theright-handsideoftheproduction
rule.
4. Repeat the production rule selection and expansion steps until the string contains only
terminalsymbols(i.e.,nonon-terminalsremain).
5. Theresultingstring,consistingentirelyofterminalsymbols,isasentencesampledfromthe
grammar.
Algorithm 1: Pseudocode for generating a sentence from a PCFG: Process to sample a
sentencefromagivenPCFGG.
def generate_sentence(G):
# Initialize the string with the start symbol S
string = [S]
# While the string contains non-terminal symbols
while any(is_nonterminal(symbol) for symbol in string):
# Select a non-terminal A from the string
A = select_nonterminal(string)
# Choose a production rule A→α according to P
rule = sample_rule(A,G.P)
# Replace A in the string with α
string = apply_rule(string, A, rule)
# Return the generated sentence composed of terminal symbols
return string
19Preprint
A.1.2 INSTANTIATINGTHEGRAMMARUNDERLYINGOURLANGUAGE
Whilegenerallyonedirectlysamplessentencesfromagrammar,inthiswork,wedefineagrammar
that operates over symbols, i.e., whose terminals are variables that are not yet populated by any
specificvaluesfromthelanguage’svocabulary. Weemphasizethisisanunconventionalmannerfor
definingaPCFG,asonewouldgenerallyuseastandardvocabularyofthelanguagetodirectlydefine
terminalsymbols. However,toenforcetypeconstraints,wefindthisunconventionalformataidsin
makingtheimplementationeasier. Specifically,onecansimplysampleanentirelysymbolicsentence,
andthenenforcetypeconstraintsatthestepwhenthesesymbolshavetobepopulated.
Overall,ourgrammar,denotedG,isdefinedusingthefollowing.
• Terminalsymbols: T={Subj,Obj,Verb,Conj,lVerb,Desc,eAdj,dAdj,Adv,Prep}.
– Here, Subj is a symbol for a subject, Obj for an object, Verb for verbs, Conj for
conjunctions,lVerbforalinkingverb,Descfordescriptors,eAdjforadjectivesused
forentities,dAdjforadjectivesusedfordescriptors,Advforadverbs,andPrepfor
prepositions.
• Non-terminalsymbols: NT={S,sNP,sT,oNP,oT,VP,vT,descT}.
– Here,Sdenotesthestartsymbol,sNPcanbeinterpretedasanounphrasewithasubject
init,sTastheimmediateancestorofthesubjectsymbol,oNPasanounphrasewithan
objectinit,oTastheimmediateancestorbeforetheobjectsymbol,VPasaverbphrase,
vTastheimmediateancestoroftheverbsymbol,anddescTastheimmediateancestor
ofadescriptorsymbol.
• ProductionrulesR:
S→sNPVP [1.0]
sNP→sT [0.8] | sNPConjsNP [0.2]
VP→lVerbdescT [0.4] | VerbPrepoNP [0.4] | VPConjVP[0.2]
oNP→oT [0.7] | oTConjoNP [0.3]
sT→eAdjSubj [0.8] | Subj [0.2]
oT→eAdjObj [0.8] | Obj [0.2]
descT→dAdjDesc [0.8] | Desc [0.2]
Notethatsincenon-terminalscanappearonbothleftandrighthandsideofarule,thereisrecursion
possible in our grammar and hence sentences can get very long. We restrict sentence lengths to
75,yieldingalanguagewheresentencelengthsvaryfrom4–75tokens. Probabilityoverruleswas
partiallyadaptedfrompriorworkby(Hupkesetal.,2020).
Giventheabove,wecannowsamplesymbolicsentencessuchasSubjlVerbDesc.Wewillpopulate
thesesymbolswithtokensfromourvocabularyV. Asnotedabove,whileingeneralthispopulation
stepwouldbeperformedasthefinalstepofthegrammar,toenforcetypeconstraintsandenable
context-sensitivity,weseparateitfromthegrammar.
Implementation. Toimplementthegrammar,weusetheNLTKpackage(Birdetal.,2009),which
providesaneasyinterfacetodefinePCFGs. Moreover,thepackageprovidespre-implementedparsers
thathelpperformgrammaticalitychecks,i.e.,ifourmodelproducesasentencef(x)forsomeinput
x,wecansimplyusetheparsertocheckwhetherf(x)isgrammaticallycorrect.
A.2 TYPECONSTRAINTS
Asdescribedinthemainpaper,weinstantiateaminimalnotionofcontextsensitivitybyconstraining
whenanentityisseeninthecontextofapropertyorverb. Therearetwosubtlewaysinwhichsuch
constraintswillaffectthegeneratedsentences.
• Constraining properties. When a symbolic sentence with a descriptor is sampled, the
descriptorsymbolwillbepopulatedwithapropertythatisvalidfortherelevantentityin
thesentence.
20Preprint
• Constrainingsubjectsandobjects. Subjectsandobjectsbroadlydistinguishentities(or,
tobeprecise,nouns)inasentence. Forpropertiesthathelpdefineverbs(e.g.,Walk),we
instantiateanotionofdirectionalitythatdetermineswhethertheentitycantaketheaction
suggestedbytheverbcorrespondingtothepropertyorwhethertheactioncanbetakenupon
it. Accordingly,whenaverbisselected,onlyasubsetofsubjectsandobjectsthatcantake
andhavetheactionofverbbetakenuponthemareleftvalidtoformasentence.
Overall,then,saywehaveasymbolicsentence. Wepopulatethesymbolsinthesentenceasfollows.
• Checkifthereisaverbtopopulate. Ifso:
1. Randomlysampleaverbfromthevocabularyandfillitin.
2. Samplerequirednumberofentitiesthatcanoccurontherightsideoftheverb,i.e.,can
populateobjects
3. Samplerequirednumberofentitiesthatcanoccurontheleftsideoftheverb,i.e.,can
populatesubjects
• Checkiftherearedescriptortopopulate. Ifso:
1. Iftheentitiesarenotpopulatedyet,populatethem.
2. Usetheparsetreeforthesymbolicsentencetoidentifypropertyofwhichentityshould
populatethedescriptor.
3. Randomlysampleadescriptorfromthevalidpropertiesofsaidentity.
• Checkifthereareadjectivestopopulate. Ifso:
1. Identifywhethertheadjectivecorrespondstoanentityoraproperty
2. Populatetheadjectivewithavalidadjectivefromthegroupofadjectivesreservedfor
entitiesversusproperties
• Checkifthereareadverbs,linkverbs,prepositions,orconjunctionstopopulate. Ifso:
1. Sample an adverb, link verb, preposition, or conjunction from the vocabulary. We
intentionallydonotmakethesepartscontext-sensitive,sincetheremainingpartsare
sufficienttoinducecontext-sensitivityandenableourexperiments.
PseudocodedescribingtheprocessaboveisdetailedinAlgo2.
A.3 DEFININGTHEOVERALLCONTEXT-SENSITIVELANGUAGE
OurlanguageLisdefinedbyfirstinstantiatingtheunderlyinggrammarasdescribedinApp.A.1
andthenthetypeconstraintsinApp.A.2. Wenotethatsincethegrammarisarandomizedprocess
andtokenrolesarerandomlyfilledbyusingthetypeconstraintsgraph,theoddsofseeingthesame
samplemultipletimesareexceedinglylow. PrimaryhyperparametersfordefiningLincludenumber
ofentitiesandnumberofproperties,denoted|E|and|K|,respectively. Unlessmentionedexplicitly,
wefixthesehyperparametersto900and18000respectively. Inseveralexperimentswedovarythese
variablesthough. Thus,wealsonotethatweareslightlyabusingnotationshereandusingLtorefer
toasinglelanguage. Inactuality,however,whatwehaveisafamilyoflanguageswiththesame
grammar,butvaryingnumberofentitiesandproperties. Thevocabularyconsistsofentities(subjects
andobjects),descriptors,verbs,adjectives,adverbs,prepositions,andconjunctions. Alllanguages
weanalyzehavethesamenumberofverbs(=200),linkingverbs(=2),adjectives(=20),adverbs
(=20),prepositions(=3),andconjunctions(=2).
Wealsonotethatthetypeconstraintsgraphmerelydescribeswhichpropertiesarevalidforaclass.
Foraspecificentity,onlyafractionoftheseentitiesmightbevisibleduringtraining. Specifically,we
constrainthesamplingprocesssuchthatonly10%ofvalidpropertiesofaclassareactuallyassociated
with an entity. However, as training occurs, the model gets to see several entities in the context
ofseveralproperties. Eventhoughcertainpairswillneverbeseentogetherduetotherestriction
discussedabove,tworandomlysampledentitieswillstillhaveanon-zeroproportionofpropertiesin
whosecontexttheyhavebothbeenseen,hencegivingthemodelsomesignalthattheentitieshave
sharedcharacteristics(seeFig.8). Thisislikelywhatleadstothepercolation-likeprocessweobserve
inthemainpapertocomeintoplay,andhenceyieldsusa0.5powerlawscalingforthetransition
21Preprint
Algorithm2: Pseudocodeforpopulatingasymbolicsentence: Fillsinsymbolswhilerespect-
ingthetypeconstraints.
def populate_sentence():
# Check if there is a verb to populate
if has_verb():
# Sample a verb from the vocabulary and fill it in
verb = sample_verb(vocabulary)
populate_verb(verb)
# Sample entities for the right side (objects)
objects = sample_entities(right)
# Sample entities for the left side (subjects)
subjects = sample_entities(left)
# Check if there are descriptors to populate
if has_descriptors():
# Populate entities if not already populated
if not entities_populated():
populate_entities()
# Use parse tree to identify which entity’s property to populate
entity = get_entity_for_descriptor(parse_tree)
# Sample a descriptor from the valid properties
descriptor = sample_descriptor(valid_properties,entity)
populate_descriptor(descriptor)
# Check if there are adjectives to populate
if has_adjectives():
# Identify if the adjective corresponds to an entity or a
property
role = identify_role(token)
# Sample a valid adjective based on the role
adjective = sample_adjective(role)
populate_adjective(adjective)
# Check if there are adverbs, link verbs, prepositions, or
conjunctions to populate
if has_adverbs_etc():
# Sample an adverb, link verb, preposition, or conjunction from
the vocabulary
word = sample_word(vocabulary)
populate_word(word)
Iters = 1 Iters = 1000 Iters = 5000 Iters = 10000 Iters = 50000 Iters = 100000
Properties Properties Properties Properties Properties Properties
Figure8: Adjacencymatrixwithtime. Asthemodelundergoestraining,objectsareseeninthe
contextofmoreproperties. Recordingwhetherapairofobjectandpropertyhavebeenseentogether,
wecangettheadjacencymatrixforabipartitegraphcorrespondingtoentitiesandpropertiesthat
constitute the data-generating process, as shown in the plots. The red-dotted lines indicate class
boundaries. Thismatrixcanbedeemedastheadjacencymatrixrepresentationoftheempiricaltype
constraintsgraph,and,astraininggoeson,itwillgetclosertothegroundtruthgraph. Notethat
severalobjectsandpropertieswillneverbeseentogether,however,otherobjectsfromtheclassmay
bepairedwithaproperty,allowingthemodelasignaltoinferthattheentitieslikelybelongtothe
sameclass.
22
seititnEPreprint
Task: freegeneration
Input: null
Output: eAdj19subj5haspAdj7descriptor1496
Task: unscrambling
Input: haseAdj19descriptor1496pAdj7subj5
Output: eAdj19subj5haspAdj7descriptor1496
Task: conditionalgeneration
Input: subj5descriptor1496
Output: eAdj19subj5haspAdj7descriptor1496
Figure9: Exemplarswhereanentity’spropertiesaredescribed. Eachboxrepresentsadifferent
task;specifically,freegeneration,unscrambling,andconditionalgeneration.
Task: freegeneration
Input: null
Output: eAdj16subj102adv9verb64ineAdj1obj41
Task: unscrambling
Input: eAdj16subj102adv9verb64ineAdj1obj41
Output: eAdj16subj102adv9verb64ineAdj1obj41
Task: conditionalgeneration
Input: verb64obj41
Output: eAdj16subj102adv9verb64ineAdj1obj41
Figure10: Exemplarswhereasubjectandobjectareboundviaaverb. Eachboxrepresentsa
differenttask;specifically,freegeneration,unscrambling,andconditionalgeneration.
pointwheremodel’sperformanceongeneratingsentenceswithdescriptorsorperformingreasoning
tasksonsentenceswithdescriptorsstartstoimprove.
AfewexamplesentencesfromthelanguagearereportedinFigs9,10. Notethattherearealarge
numberofsymbolicsentencespossible;wemerelyreporttwoofthesetoprovideintuition. Wealso
reemphasizethatnaturalisticsentencesusedasexamplesinthemainpaperweretomerelyanalogize
the structures our language is trying to capture. It is not difficult to see that the sentences in the
examplesprovidedherehaveasimilarstructureandconstraintsasthosenaturalisticexamples.
23Preprint
B PERCOLATION THRESHOLD IN THE BIPARTITE GRAPH SETUP
Forgeneralbipartitegraphsthatareuncorrelated,meaningthattheyarecompletelydescribedby
thedegreedistributionsP (k)andP (k)fortheobjectsandfeatures,respectively,thepercolation
1 2
thresholdis
(cid:115)
⟨k⟩ ⟨k⟩
p = 1 2 . (2)
c ⟨k(k−1)⟩ ⟨k(k−1)⟩
1 2
Here, ⟨·⟩ denotes the expected value with respect to P (k), and we require |E|⟨k⟩ = |K|⟨k⟩
i i 1 2
for consistency. The case of randomly selecting connecting edges as demonstrated in the main
textwillcorrespondtostartingfromacompletebipartitegraph,inwhichcaseP (k) = δ and
1 k,|K|
(cid:112) (cid:112)
P (k)=δ ,leadingtop = 1/(|E|−1)(|K|−1)≃ 1/|E||K|.
2 k,|E| c
ToderiveEq.(2)weusethegeneratingfunctionasexplainedin(Newmanetal.,2001). Firstly,we
introducethegeneratingfunctionforthedegreedistributionoftwoconcepts,i=1,2:
∞
(cid:88)
G0(x) := P (k)xk (3)
i i
k=0
Thegeneratingfunctioncanbeusedtocalculatemomentsoftheprobabilitydistribution,suchasthe
meanandvariance,bytakingderivatives:
dG d0 i x(x)(cid:12) (cid:12) (cid:12) (cid:12) x=1 = k(cid:88)∞ =0kP i(k)xk−1(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
x=1
= k(cid:88)∞ =0kP i(k)=⟨k⟩ i (4)
d2G dx0 i 2(x)(cid:12) (cid:12) (cid:12) (cid:12) x=1 = k(cid:88)∞ =0k(k−1)P i(k)xk−2(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
x=1
= k(cid:88)∞ =0k(k−1)P i(k)=⟨k(k−1)⟩ i. (5)
Herewedenotedtheaverageoverthedegreedistributionofconceptias⟨·⟩ .
i
Anotherusefulpropertyofgeneratingfunctionsisthatthegeneratingfunctionofthesumofthe
degreescanbedescribedbythepowerofgeneratingfunctions. Forexample,thedistributionofthe
sumofdegreesfromtworandomlyselectednodesfromsetsiandj,denotedbyP˜ (k),willsatisfy
ij
∞
(cid:88) P˜ (k)xk =G0(x)G0(x). (6)
ij i j
k=0
Withthesepropertiesinmind,wefurtherintroducethegeneratingfunctionforthedistributionof
outgoingedgesfromanodethatwearriveatbyfollowingarandomlychosenedge:
(cid:80)∞ kP (k)xk−1 G0′(x)
G1(x) := k=0 i = i . (7)
i (cid:80)∞ kP (k) ⟨k⟩
k=0 i i
which can be obtained by noticing that the probability of the degree of a node arrived at from a
randomlychosenedgeisproportionaltokP (k). Thedecreasedpowerofxbyoneinthenumerator
i
istoexcludetheoriginallychosenedge.
Wefurtherintroducethegeneratingfunctionforthedistributionofthenumberofconceptsinithat
canbereachedfromanodeintheconceptj(̸= i)thatisconnectedtoarandomlychosenedgeas
G˜1(x),andthesamewhenrandomlychoosinganodeinconceptj,G˜0(x). Thesefunctionssatisfy
i i
G˜0(x) = G0(G1(x)) (8)
i i j
G˜1(x) = G1(G1(x)). (9)
i i j
Wealsointroducethegeneratingfunctionforthedistributionofthesizesofcomponentsinconcept
ithatarereachedbychoosinganedge,H1(x),andthesamewhenchoosinganodeinconcepti,
i
H0(x). Thesesatisfy
i
H0(x) = xG˜0(H1(x)) (10)
i i j
H1(x) = xG˜1(H1(x)). (11)
i i j
24Preprint
Here,thekeyassumptionisthatthereisnoclosedloopofedgesinthenetwork,whichholdsifthe
fractionofconnectionislowandthereisnocluster(i.e.,sub-criticalregime).
Theaverageclustersizeofconcepti,i.e.,thenumberofnodesinithatareconnectedwitheachother,
isthen⟨S ⟩=H0′(1),whichis
i i
G˜0′(1)
⟨S ⟩=1+ i , (12)
i 1−G˜1′(1)
i
using the derivatives of Eqs. (10),11). The percolation threshold is when the denominator in the
secondtermofEq.(12)becomeszero,so
G0′′(1)G0′′(1) ⟨k(k−1)⟩ ⟨k(k−1)⟩
G˜1′ (1)=G1′ (1)G1′ (1)= 1 2 = 1 2 =1 (13)
i i j ⟨k⟩ ⟨k⟩ ⟨k⟩ ⟨k⟩
1 2 1 2
Now,whentheconnectionofeachaprobabilityofconnectionpassociatedwitheachbondontopof
theoriginalgraph,thegeneratingfunctionofthedegreeswillbecome
∞ ∞ (cid:18) (cid:19)
(cid:88)(cid:88) n
G0(x;p) = P (n) pk(1−p)n−kxk (14)
i i k
k=0n=k
∞
(cid:88)
= P (n)(px+1−p)n =G0(1+(x−1)p). (15)
i i
n=0
Fromthefirstlinetothesecondline,weused(cid:80)∞ (cid:80)∞ =(cid:80)∞ (cid:80)n
. Wecanthenrewrite
k=0 n=k n=0 k=0
Eq.(13)as
⟨k(k−1)⟩p⟨k(k−1)⟩p ⟨k(k−1)⟩ ⟨k(k−1)⟩
1 2 =p2 1 2 =1, (16)
⟨k⟩p⟨k⟩p ⟨k⟩ ⟨k⟩
1 2 1 2
fromwhichweobtainEq.(2). Hereweused⟨k(k−1)⟩p =G0′′(1;p)=p2G0′′(1)=p2⟨k(k−1)⟩
i i i i
and⟨k⟩p =G0′(1;p)=pG0′(1)=p⟨k⟩ .
i i i i
B.1 EXPONENTINTHECLUSTERSIZE
Thecriticalexponentassociatedwiththenumberofnodesintheclusterforp>p ,S ∼(p−p )β,
c c
isdeterminedtobeβ =1whenthereisnospecificstructureinthegraph. Toseethis,letusconsider
thatu=H0(1)istheprobabilitythatanodeiniisincludedinafinitesizecluster(i.e.,notthelarge
i
connectedcluster). RecallthatH0(1)wasthegeneratingfunctionofthenumberofnodesinconcept
i
iincludedintheclusterinthesubcriticalregime(p<p );wearehereassumingthatthestatistics
c
willnotchangeeveninthesupercriticalregime(p > p )whenneglectingthelargecluster. Then,
c
fromEqs.(10,11),wehave
H0(1)=u = G˜0(H1(1))=G˜0(G˜1(u)) (17)
i i j i j
= G0(G1(G0(G1(u))))=:f(u) (18)
i j j i
whichisaself-consistentequation.
Bywritingu=1−ϵ,wehavef(1−ϵ,p)=1−ϵf′(1,p)+ϵ2f′′(1,p)/2...,wherethederivativeis
takenforu. Noticingthatf′(1,p )=1,weobtaintherelation
c
∂2 (cid:12) (cid:12) (cid:34) 1 ∂3 (cid:12) (cid:12) (cid:35)
ϵ = (p−p c) ∂u∂pf(u,p)(cid:12)
(cid:12) 2
∂u2∂pf(u,p)(cid:12)
(cid:12)
+o(p−p c)+o(ϵ)(19)
u=1,p=pc u=1,p=pc
∼ (p−p ), (20)
c
indicatingβ =1
Asaninterestinggeneralization,aclassicresult(Cohenetal.,2002)showsthatevenforthesituation
wherep >0,thepowerβcandeviatefromone.Thiscorrespondstowhenthedifferentialcoefficients
c
inEq.(19)diverge,correspondingtocaseswherethesecondorthirdmomentbeingill-defined. For
thecaseofP (k)∼k−γ with3<γ <4,wecanshowthatβ =1/(γ−3).
i
25Preprint
B.2 TRANSITIONBEHAVIORFORFINITEINFERENCESTEPS
Themappingoftheinferenceschemetothepercolationproblembecomespreciseonlyinthecontext
ofinfiniteinferencesteps. Forafinitenumberofsteps,denotedasn,thepertinentquestionisthe
numberofnodepairsacrossthesetsconnectedwithin2n+1edges. Usingtheaveragedegrees⟨k⟩
1
and⟨k⟩ respectively,anodeinthefirstsetcanreachapproximately⟨k⟩n+1⟨k⟩nnodesafter2n+1
2 1 2
steps. Hence,theapproximatefractionofconnectededgeswithin2n+1stepsis|E|⟨k⟩n+1⟨k⟩n.
1 2
26Preprint
C EXPERIMENTAL DETAILS
Modelarchitecture. Wetrainatwo-blockTransformerbasedonthenanoGPTarchitecture(Andrej
Karpathy, 2023) using the standard autoregressive language modeling objective, i.e., next token
prediction. Each block contains two attention heads, an MLP, GELU activation, and processes /
produces128dimensionrepresentations. Bothtokenandpositionembeddingsarelearnedduring
training.
Optimizationsetting. ModelsaretrainedusingtheAdamoptimizerwith10−3learningrate,batch-
size of 128, and 10−4 weight decay for 105 iterations (or until the run collapses due to cluster
challenges;e.g.,poweroutages). Gradientclippingatnormof1isapplied. Nolearningrateschedule
isused. Unlessstatedotherwise,resultsareaveragedoverthreeseeds.
Dataconfiguration. Sentencesaresampled“online”,i.e.,wesampleafreshbatchofdataevery
iteration by following the rules of the language. The language has M entities and N properties
uniformly distributed over C classes, with edges connecting properties sparsely and randomly
distributedovervalidpropertiesforagivenobject(specifically,only10%connectionsaremade).
WeslightlyabusenotationsbyusingLtorefertoourlanguage,sinceinactualitywehaveafamily
oflanguageswiththesamegrammar,butvaryingnumberofentitiesandproperties. Wenotethat
sincethegrammarisarandomizedprocessandtokenrolesarerandomlyfilledbyusingthetype
constraintsgraph,theoddsofseeingthesamesamplemultipletimesareexceedinglylow.
Tokenization. We use a one-hot, manually defined tokenization scheme wherein each token is
associatedwithauniquetokenID.
C.0.1 PERFORMANCEOFAMEMORIZINGSOLUTIONONDESCRIPTIVESENTENCES
Asthemodelundergoestraining,itsaccuracyatgettingdescriptiveconstraintsrightcan,atmax,
(cid:16) (cid:17)
bethefollowing: Acc = 0.1+f ∗max 1,0.25Bt|C| ,wheref isfractionofpairsfromthetype
|E||K|Rf
constraintsgraphthemodelcanseeduringtraining,0.25isapproximatelytheproportionofrandomly
sampledsentencesthataredescriptiveinnature,B isbatch-size,tisnumberofiterations,andR
isnumberofrepetitionsneededtointernalizethatanentityandpropertyconstituteavalidcontext.
Sinceweseethethirdphaseinaregimewheret∼104,assumingatleast4repetitionsarenecessary
forinternalizingapair,wehaveAcc∼0.15.
27Preprint
D FURTHER RESULTS: ROBUSTNESS ACROSS SETTINGS AND EVALUATIONS
Inthissection,wereportseveralmoremetricsrelevanttoassesshowwellthemodelhasinternalized
thelanguageandhowwellitisabletoperformtasksontopofstringsfromthelanguage. Wereport
resultsacrossseveralconfigurationsaswell. Rarely,butcertainlysometimes,runscrasheddueto
clusterissues. Theseconfigurationsarenotreported,orreporteduntilthepointofcrashifsufficient
timehadpassedintraining.
• Basesetting: Thisisthesettingusedthroughoutthepaper, i.e., with10classesand900
entities.
• Varyingnumberofproperties: rangingfrom14800—38800,inincrementsof1600.
• Differentclasssetting: wechangenumberofclassesto2andrepeatallevaluationsinthis
setting.
• Differententitiessetting: wechangenumberofentitiesto1800andrepeatallevaluationsin
thissetting.
Wespecificallyreportthefollowingresults.Bothinthemainpaperandintheresultsbelow,evaluation
metricsareaveragedover1000randomlysampledstrings.
• Loss/learningcurvesunderdifferentsettings: App.D.1.
• Grammaticalityandtypechecksunderdifferentsettings: App.D.2.
• Negativeloglikelihoodsofsentencesfromthelangaugeandtheirperturbedversions(e.g.,
wheretypeconstraintsarenotcorrect): App.D.3.
• How well does the model follow our language, where we analyze NLLs of sentences
generatedbythemodel,distributionoflength,andparsetreedepth: App.D.4.
• Furtherresultsonunscrambling: App.D.5.
• EvolutionofAttentionmaps: App.D.6.
• FurtherresultsonConditionalGeneration: App.D.7. Conditionalgenerationevaluations
turnouttobeextremelytime-expensive,withasingleruntakingapproximately4daysto
finishwhenconditionalgenerationisevaluated(comparedto12hourswithout). Thisis
likelyaresultofmodelgeneratingextremelylongsentencestocomposeconditioningtokens
thatcaninvolvemultiplesubjects,objects,andproperties. Wethusprimarilyfocusonfree
generationandunscramblingintheresultsreportedinthissection. Wedoprovideresults
forconditionalgenerationinonemoresettingwith27600propertiestodemonstratethatour
findingsfromthemainpaper(i.e.,inthe18000propertiessetting)generalize.
28Preprint
D.1 LEARNINGCURVES
Weplotlearningcurvesfordifferentsettingsinthissection. Resultsarereportedforvaryingnumber
ofproperties,averagedover3seeds,andresultsforthebasesettingusedinthemainpaperwhere
numberofpropertiesisfixedtobe18000. Forthelattersetting,weshowtheaveragerunalongside
individualruns.
D.1.1 BASESETTINGWITHVARYINGNUMBEROFPROPERTIES
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Total Free generation Unscrambling Conditional generation
6×101
3.8×101 8×101
3.6×101 7×101 5×101
5×101 3.4×101
6×101
3.2×101 4×101
5×101
4×101
3×101
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure11: Learningcurveswithvaryingnumberofproperties(basesetting). Asnumberof
propertiesarevaried,weseetheoveralllossissubstantiallymorecontinuous,butindividualtasks
canseesuddenlearning. Forexample,infreegeneration,weseeasuddenlossdrop. Thispointis
preciselywhenthemodellearnstoproducegrammaticallycorrectsentences. Veryslightlyafterthis
point,bothtasksofunscramblingandconditionalgenerationseeimprovementaswell. Thereisa
secondchangeofslopeforthesetasksbetween103to7×103iterations,dependingonthenumber
ofproperties. ThesepointsmatchmatchthemomentwherethemodelstartstoimproveinitsType
Checkperformance.
Total Free generation Unscrambling Conditional generation
6×101 seed 0 8×101
seed 1 3.6×101 5×101
seed 2 7×101
5×101 Average 3.4×101
6×101
3.2×101 4×101
4×101 3×101 5×101
2.8×101
4×101
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure12: Learningcurvesforsettingwith18000properties(basesetting). Wereportthisplotto
zoomintoaspecificconfiguration. Ascanbeseen,thereissomeminimalvarianceacrossruns,but
mostlypointsoftransitionareinasimilarrange.
29
ssoL
ssoLPreprint
D.1.2 CHANGINGTO1800OBJECTSANDVARYINGNUMBEROFPROPERTIES
We change the number of objects to 1800 (compared to base setting of 900) and report learning
curvesundervaryingnumberofproperties. SeeFigs.13,14.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Total Free generation Unscrambling Conditional generation
4×101
8×101
6×101
7×101 5×101
5×101
6×101
4×101
5×101
4×101
3×101
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure13: Learningcurveswithvaryingnumberofproperties(1800objects). Asnumberof
propertiesarevaried,weseetheoveralllossissubstantiallymorecontinuous,butindividualtasks
canseesuddenlearning. Forexample,infreegeneration,weseeasuddenlossdrop. Thispointis
preciselywhenthemodellearnstoproducegrammaticallycorrectsentences. Veryslightlyafterthis
point,bothtasksofunscramblingandconditionalgenerationseeimprovementaswell. Thereisa
secondchangeofslopeforthesetasksbetween103to7×103iterations,dependingonthenumber
ofproperties. ThesepointsmatchmatchthemomentwherethemodelstartstoimproveinitsType
Checkperformance.
Total Free generation Unscrambling Conditional generation
6×101
s se ee ed
d
0
1
3.8×101 8×101
seed 2 3.6×101 7×101 5×101
Average
5×101 3.4×101
6×101
3.2×101 4×101
5×101
4×101 3×101
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure14: Learningcurvesforsettingwith18000properties(1800objects). Wereportthisplot
tozoomintoaspecificconfiguration. Ascanbeseen,thereissomeminimalvarianceacrossruns,but
mostlypointsoftransitionareinasimilarrange.
30
ssoL
ssoLPreprint
D.1.3 CHANGINGTO2CLASSESANDVARYINGNUMBEROFPROPERTIES
Wechangethenumberofclassestodivideobjectsandpropertiesoverto2(comparedtobasesetting
of10)andreportlearningcurvesundervaryingnumberofproperties. SeeFigs.15,16.
14800 18000 21200 27600 30800 34000 37200 38800
16400 19600 24400 29200 32400 35600
Total Free generation Unscrambling Conditional generation
3.5×101 5×101
3.4×101 7×101
5×101 3.3×101
3.2×101
6×101
4×101
3.1×101
4×101 3×101 5×101
2.9×101
2.8×101
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure 15: Learning curves with varying number of properties (2 classes). As number of
propertiesarevaried,weseetheoveralllossissubstantiallymorecontinuous,butindividualtasks
canseesuddenlearning. Forexample,infreegeneration,weseeasuddenlossdrop. Thispointis
preciselywhenthemodellearnstoproducegrammaticallycorrectsentences. Veryslightlyafterthis
point,bothtasksofunscramblingandconditionalgenerationseeimprovementaswell. Thereisa
secondchangeofslopeforthesetasksbetween103to7×103iterations,dependingonthenumber
ofproperties. ThesepointsmatchmatchthemomentwherethemodelstartstoimproveinitsType
Checkperformance.
Total Free generation Unscrambling Conditional generation
s se ee ed
d
0
1
33 .. 34 ×× 11 00 11 7×101 4 4.7 .5 5× ×1 10 01
1
5×101 s Ae ve ed
ra
2
ge
3.2×101
6×101
4.25×101
3.1×101 4×101
3×101 3.75×101
4×101 2.9×101
5×101
3.5×101
2.8×101 3.25×101
4×101
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure16: Learningcurvesforsettingwith18000properties(2classes). Wereportthisplotto
zoomintoaspecificconfiguration. Ascanbeseen,thereissomeminimalvarianceacrossruns,but
mostlypointsoftransitionareinasimilarrange.
31
ssoL
ssoLPreprint
D.2 GRAMMATICALITYANDTYPECHECKS
As the model learns rules of our language, we can track how grammatical its sentences are and
whethertheysatisfytypeconstraints,asdoneinthemainpaper. Wereportsimilarresultsfordifferent
settingsinthissection. Specifically,wereportresultsforvaryingnumberofproperties,averaged
over3seeds,andforthebasesettingusedinthemainpaperwherenumberofclassesisfixedtobe
18000. Forthelattersetting,weshowtheaveragerunalongsideindividualruns.
Forgrammaticality,wemerelyusetheNLTKparsertocheckwhetherthegeneratedsentencesby
themodelunderfreegenerationaregrammaticallyvalid,i.e.,theyfollowtherulesofthegrammar.
For Type Checks, we extract the subjects, objects, and any properties in the sentence to checks
whethertheyarevalidunderthetypeconstraintsgraph(seeDef.3).
D.2.1 BASESETTINGWITHVARYINGNUMBEROFPROPERTIES
We report results under varying number of properties with the base experimental setting. See
Figs.17,18.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Grammaticality Type Check (Descriptive) Type Check (Relative) Type Check (All)
100 100
3×10 1
10 1
6×10 1 2×10 1
4×10 1 10 1
10 2
3×10 1 10 1
2×10 1
6×10 2 10 3
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure17: GrammaticalityandTypeCheckswithvaryingnumberofproperties(basesetting).
Asnumberofpropertiesarevaried,weseethegrammarislearnedaroundbroadlythesametime,
i.e.,grammarlearningisinvarianttonumberofproperties(asalsoseeninlearningcurves). Fortype
check,weseeassoonasgrammaticalityreachesitsmaximum,relativeconstraintsquicklyimprove,
leadingtoaboostinaccuracyofallconstraintsevaluation(rightmostpanel). Descriptiveconstraints
seeatransitionatthispointaswell,butthenafteraperiodofsaturation(akintoasaddlepoint),start
toimproveatanapproximatelylinearrate(onlog-logscale)untilsaturatingagain. Theseresults
matchthebasesettingshowninpaper.
Grammaticality Type Check (Descriptive) Type Check (Relative) Type Check (All)
100
seed 0
4×101 100
seed 1 3×101
6×101 seed 2
101
Average 2×101
4×101 101
3×101
102
101
2×101
6×102 102 103
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure18: GrammaticalityandTypeCheckswith18000properties(basesetting). Wereportthis
plottozoomintoaspecificconfiguration,inparticulartheoneusedinmainpaper. Ascanbeseen,
thereissomeminimalvarianceacrossruns,butmostlypointsoftransitionareinasimilarrange.
32Preprint
D.2.2 CHANGINGTO1800OBJECTSANDVARYINGNUMBEROFPROPERTIES
Wechangethenumberofobjectsto1800(comparedtobasesettingof900)andreportresultsunder
varyingnumberofproperties. SeeFigs.19,20.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Grammaticality Type Check (Descriptive) Type Check (Relative) Type Check (All)
3×10 1
100
10 1
6×10 1 2×10 1
4×10 1
10 1
3×10 1 10 1 10 2
2×10 1
6×10 2
10 3
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure19: GrammaticalityandTypeCheckswithvaryingnumberofproperties(1800objects).
Todemonstraterobustnessofresults,weincreasenumberofobjectsinthisexperiment. Weagain
findthatasnumberofpropertiesarevaried,thegrammarislearnedaroundbroadlythesametime,
i.e.,grammarlearningisinvarianttonumberofproperties(asalsoseeninlearningcurves). Fortype
check,weseeassoonasgrammaticalityreachesitsmaximum,relativeconstraintsquicklyimprove,
leadingtoaboostinaccuracyofallconstraintsevaluation(rightmostpanel). Descriptiveconstraints
seeatransitionatthispointaswell,butthenafteraperiodofsaturation(akintoasaddlepoint),start
toimproveatanapproximatelylinearrate(onlog-logscale)untilsaturatingagain. Theseresults
matchthebasesettingshowninpaper.
Grammaticality Type Check (Descriptive) Type Check (Relative) Type Check (All)
100 100
seed 0
seed 1 101
6×101 seed 2
Average 101
4×101 102
3×101
101
2×101 102 103
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure20: GrammaticalityandTypeCheckswith18000properties(1800objects). Wereport
thisplottozoomintoaspecificconfiguration. Ascanbeseen,thereissomeminimalvarianceacross
runs,butmostlypointsoftransitionareinasimilarrange.
33Preprint
D.2.3 CHANGINGTO2CLASSESANDVARYINGNUMBEROFPROPERTIES
Wechangethenumberofclassestodivideobjectsandpropertiesoverto2(comparedtobasesetting
of10)andreportresultsundervaryingnumberofproperties. SeeFigs.21,22.
14800 18000 21200 27600 30800 34000 37200 38800
16400 19600 24400 29200 32400 35600
Grammaticality Type Check (Descriptive) Type Check (Relative) Type Check (All)
100 100
6×10 1
6×10 1
4×10 1
6×10 1
5×10 1 3×10 1
4×10 1 6×10 1
2×10 1
3×10 1 4×10 1
2×10 1 4×10 1
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure21: GrammaticalityandTypeCheckswithvaryingnumberofproperties(2classes). To
demonstraterobustnessofresults,weincreasefewerclassesinthisexperiment. Weagainfindthatas
numberofpropertiesarevaried,thegrammarislearnedaroundbroadlythesametime,i.e.,grammar
learningisinvarianttonumberofproperties(asalsoseeninlearningcurves). Fortypecheck,we
seeassoonasgrammaticalityreachesitsmaximum,relativeconstraintsquicklyimprove,leading
toaboostinaccuracyofallconstraintsevaluation(rightmostpanel). Descriptiveconstraintsseea
transitionatthispointaswell,butthenafteraperiodofsaturation(akintoasaddlepoint),startto
improveatanapproximatelylinearrate(onlog-logscale)untilsaturatingagain. Theresultsareless
prominentforthissetting,butzoomingin(seefigurebelow)showstheclaimsdofollowsthebase
settingshowninpaper.
Grammaticality Type Check (Descriptive) Type Check (Relative) Type Check (All)
100
seed 0
7×101 100
6×101
seed 1
seed 2
6×101
6×101
Average
4×101
5×101 3×101
4×101 6×101
3×101 2×101
4×101
2×101
4×101
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure22: GrammaticalityandTypeCheckswith18000properties(2classes). Wereportthis
plottozoomintoaspecificconfiguration. Ascanbeseen,thereissomeminimalvarianceacross
runs,butmostlypointsoftransitionareinasimilarrange.
34Preprint
D.3 NEGATIVELOGLIKELIHOODOFSENTENCESFROMLANGUAGEANDTHEIRPERTURBED
VERSIONS
Inthissection,wereportnegativelog-likelihoods(NLL)assignedbythemodeltorandomlysampled
sentencesfromthelanguageduringthecourseoftraining. Tocheckhowwellthemodelislearning
thelanguage,andnotperhapsoverfittingtosomespecificsamples(wenotethisisunlikelytooccur
inonlinelearning,sothisevaluationisjustasanitycheckbutnotcrucial). Tothisend,weevaluate
themodelassignedNLLsforfollowingsettings.
• Seen. This is essentially the training distribution. We define valid sentences from the
language by using the part of the type constraints graph that has connections between
entitiesandproperties,andevaluatethemodelNLLs.
• Uniform. Sinceonlyafractionofvalidconnectionsareshowntothemodelduringtraining,
it is not necessary for it to generalize to other valid connections. To assess whether the
modelcanmakesuchinferences,inthisevaluation,weallowanyvalidconnectionbetween
entitiesandpropertiestobeuniformlysampled. Thisisalsotheprimaryevaluationsetting
formostexperimentsconductedinthiswork.
• Randomize values. Arguably, the model can overly generalize and even start deeming
sentencesthatdonotsatisfythetypeconstraintstobevalid. Toassessthis,inthisevaluation,
weensurethesentenceremainsgrammaticallycorrect,butintentionallyuseentitiesand
propertiesthatyieldasentencethatdoesnotfollowtypeconstraints.
• Randomizegrammar. Wesimplysampleasentenceandpermuteittobreakthegrammati-
calrules,while,technicallyspeaking,preservingtypeconstraintssincetokensseeninthe
sentenceareallowedtobeinthesamecontext.
Resultsarereportedforvaryingnumberofproperties,averagedover3seeds,andforthebasesetting
used in the main paper where number of classes is fixed to be 18000. For the latter setting, we
showtheaveragerunalongsideindividualruns. Broadly,ourresultsshowthefollowingprocessis
underwayasthemodelundergoestraining.
1. Firstthemodellearnsthegrammar. Atthispoint,Seen,Uniform,andRandomizeValuesall
seeimprovedNLL.Thisiswhatweexpect. Sinceagrammaticallycorrectsentencedoesnot
havetosatisfytypeconstraints,inthisfirstphase,themodelisboundtoshowimproved
NLLforsentencesthatrespectvs.donotrespecttypeconstraints.
2. Then,thereisasuddenimprovementinNLLforbothSeenandUniformevaluations. At
preciselythispoint,theRandomizevaluesevaluationhugelydegrades. Thisimpliesthat
oncethemodellearnstypeconstraints,itdoesnotdeemlikelysentencesthatdonotrespect
them.
3. Forthemostpart,themodelneverdeemsgrammaticallyincorrectsentencestobelikely.
However,thereisasudden,largedegradationinNLLsforgrammaticallyincorrectsentences
laterintraining.
35Preprint
D.3.1 BASESETTINGWITHVARYINGNUMBEROFPROPERTIES
We report results under varying number of properties with the base experimental setting. See
Figs.23,24.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Seen Uniform Randomize Values Randomize Grammar
4.8×101 4.8×101 6×101
4.6×101 4.6×101 4×102
4.4×101 4.4×101
3×102
4.2×101 4.2×101 2×102
5×101
4×101 4×101
3.8×101 3.8×101 102
3.6×101 3.6×101
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure23: NLLofsentenceswithdifferentperturbationsandvaryingnumberofproperties
(basesetting).SeetextinApp.D.3foradetaileddiscussion.Broadly,weseeNLLofSeenevaluation
isveryslightlybetterthanUniform.
Seen Uniform Randomize Values Randomize Grammar
4.6×101 seed 0 4.6×101 6×101
4.4×101 seed 1 4.4×101
seed 2
4.2×101 Average 4.2×101
4×101 4×101 5×101
102
3.8×101 3.8×101
3.6×101 3.6×101
3.4×101 3.4×101
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure24: NLLofsentenceswithdifferentperturbationsforsettingwith18000properties(base
setting). Wereportthisplottozoomintoaspecificconfiguration. Ascanbeseen,thereissome
minimalvarianceacrossruns,butmostlypointsoftransitionareinasimilarrange.
36
LLN
LLNPreprint
D.3.2 CHANGINGTO1800OBJECTSANDVARYINGNUMBEROFPROPERTIES
Wechangethenumberofobjectsto1800(comparedtobasesettingof900)andreportresultsunder
varyingnumberofproperties. SeeFigs.25,26.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Seen Uniform Randomize Values Randomize Grammar
5×101 5×101 6×101 4×102
3×102
2×102
5×101
4×101 4×101 102
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure25: NLLofsentenceswithdifferentperturbationsandvaryingnumberofproperties
(1800objects). SeetextinApp.D.3foradetaileddiscussion. Broadly,theseplotsshowresultswith
fewerclassesshowsimilarbehaviorasthebasesetting.
Seen Uniform Randomize Values Randomize Grammar
5×101
4.8×101 seed 0 6×101 3×102
seed 1
4.6×101 seed 2
4.4×101 Average 2×102
4.2×101
5×101
4×101 4×101
102
3.8×101
3.6×101
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure26:NLLofsentenceswithdifferentperturbationsforsettingwith18000properties(1800
objects). Wereportthisplottozoomintoaspecificconfiguration. Ascanbeseen,thereissome
minimalvarianceacrossruns,butmostlypointsoftransitionareinasimilarrange.
37
LLN
LLNPreprint
D.3.3 CHANGINGTO2CLASSESANDVARYINGNUMBEROFPROPERTIES
Wechangethenumberofclassestodivideobjectsandpropertiesoverto2(comparedtobasesetting
of10)andreportresultsundervaryingnumberofproperties. SeeFigs.27,28.
14800 18000 21200 27600 30800 34000 37200 38800
16400 19600 24400 29200 32400 35600
Seen Uniform Randomize Values Randomize Grammar
4.4×101 4.4×101
5×101
4×102
4.2×101 4.2×101 3×102
4×101 4×101 2×102
3.8×101 3.8×101
102
3.6×101 3.6×101
4×101
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure27: NLLofsentenceswithdifferentperturbationsandvaryingnumberofproperties(2
classes). SeetextinApp.D.3foradetaileddiscussion. Broadly,theseplotsshowresultswithfewer
classesshowsimilarbehaviorasthebasesetting.
Seen Uniform Randomize Values Randomize Grammar
4.2×101 seed 0 4.2×101 4×102
seed 1 5×101
4×101 seed 2 4×101 3×102
Average
2×102
3.8×101 3.8×101
3.6×101 3.6×101
102
3.4×101 3.4×101 4×101
6×101
103 104 105 103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations Iterations
Figure28: NLLofsentenceswithdifferentperturbationsforsettingwith18000properties(2
classes). Wereportthisplottozoomintoaspecificconfiguration. Ascanbeseen,thereissome
minimalvarianceacrossruns,butmostlypointsoftransitionareinasimilarrange.
38
LLN
LLNPreprint
D.4 HOWWELLDOESTHEMODELFOLLOWTHEGRAMMAR: DISTRIBUTIONOFNLLS,
DEPTHS,ANDLENGTHS
In this section, we analyze how well the model learns our language. Specifically, we let model
produceasentenceandthenusethedata-generatingprocess(PCFGandourtypeconstraintsgraph)
toanalyzethemax,min,andmeanvaluesofmetricslistedbelow. Notethatinthefollowing,we
restrict evaluations to grammatically valid sentences only, i.e., only model generations that are
grammaticallyvalidareusedforthisevaluation(elsetheNLLwillbeinfinity). Sincethemodel
producesgrammaticallysentences≈90–95%ofthetime(seeApp.D.2),thisconditioningleadsto
filteringofonlyaveryminimalnumberofgenerations.
• NLL. We analyze how likely the sentences generated by the model are under the data-
generatingprocess.
– Notethatifsentencesfromthegrammarwereuseditselfforthisevaluation,min,max,
andmeanvaluesoverabatchof1000sentencesturnouttobe1.0,7.63,and68.2for
thebasesetting;evaluationsofmodel’sgenerationsarewithintheserangesaswell.
• Parse Tree Depth. We use the NLTK parser to compute the parse tree underlying our
model’sgeneratedsentencesandthetree’sdepth.
– Note that if sentences from the grammar were used itself for this evaluation, min,
max,andmeanvaluesoverabatchof1000sentencesturnsouttobe2,4.78,and15;
evaluationsofmodel’sgenerationsarewithintheserangesaswell.
• Lengths. Wecomputethenumberoftokensinmodel’sgeneratedsentences.
– Note that if sentences from the grammar were used itself for this evaluation, min,
max,andmeanvaluesoverabatchof1000sentencesturnsouttobe4,10,and107;
evaluationsofmodel’sgenerationsarewithintheserangesaswell.
D.4.1 BASESETTINGWITHVARYINGNUMBEROFPROPERTIES
Wereportresultsundervaryingnumberofpropertieswiththebaseexperimentalsetting. SeeFig.29.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
NLL Parse Tree Depth Lengths
101
101
101
max
mean 100 100
min
103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations
Figure29: Grammarcharacteristicsofmodelgenerationasnumberofpropertiesarevaried
(basesetting). SeeApp.D.4foradetaileddiscussion. Broadly,modelgeneratedsentencesareina
similarrangeasourlanguage’s.
39Preprint
D.4.2 CHANGINGTO1800OBJECTSANDVARYINGNUMBEROFPROPERTIES
Wechangethenumberofobjectsto1800(comparedtobasesettingof900)andreportresultsunder
varyingnumberofproperties. SeeFig.30.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
NLL Parse Tree Depth Lengths
101
101
101
max
mean 100
100
min
103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations
Figure30: Grammarcharacteristicsofmodelgenerationasnumberofpropertiesarevaried
(1800objects). SeeApp.D.4foradetaileddiscussion. Broadly,modelgeneratedsentencesareina
similarrangeasourlanguage’s.
D.4.3 CHANGINGTO2CLASSESANDVARYINGNUMBEROFPROPERTIES
Wechangethenumberofclassestodivideobjectsandpropertiesoverto2(comparedtobasesetting
of10)andreportresultsundervaryingnumberofproperties. SeeFig.31.
14800 18000 21200 27600 30800 34000 37200 38800
16400 19600 24400 29200 32400 35600
NLL Parse Tree Depth Lengths
101
101
101
max
mean
100
min 100
103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations
Figure31: Grammarcharacteristicsofmodelgenerationasnumberofpropertiesarevaried(2
classes). SeeApp.D.4foradetaileddiscussion. Broadly,modelgeneratedsentencesareinasimilar
rangeasourlanguage’s.
40Preprint
D.5 MORERESULTSONUNSCRAMBLING
Inthissection,wereportseveralmoreresultsevaluatingourmodels’performanceonunscrambling
undervaryingnumberofpropertiesforthebasesetting,settingwithincreasednumberofobjects,and
withonly2classestodivideentitiesandpropertiesover. Wereportresultsforthefollowingmetrics.
• ExactMatch. Accuracyofthemodelforgettingeverytokenofthescrambledsentenceinto
itsrightpositionintheunscrambledversion.
• PerTokenAccuracy. ThismetriccanbethoughtofasasmootherversionofExactMatch,
i.e.,itprovidespartialcredittothemodelasitlearnstosolvethetask.
• Accuracy on Descriptive Sentences. In this evaluation, we compute the exact match
accuracyforsentencesthataredescriptiveinnature,i.e.,sentenceswhereinclaimsaremade
aboutanentitypossessingaproperty.
– Wenotethattheprecisewaythisevaluationisdoneisbyrestrictingthesentencelength
totheshortestsentences(≤6tokens). Thisrangeisprimarilyconstitutedofsentences
thataredescriptiveinnature(≈94%).
• AccuracyonRelativeSentences. Inthisevaluation,wecomputetheexactmatchaccuracy
forsentencesthatareprimarilyrelativeinnature,i.e.,sentenceswhereinclaimsaremade
aboutasubjectrelatingtoanobjectviaaverb.
– Wenotethattheprecisewaythisevaluationisdoneisbyrestrictingthesentencelength
totherangeof(7—9tokens). Thisrangeisprimarilyconstitutedofsentencesthatare
relativeinnature(≈85%).
• Grammaticality. Giventheoutputgeneratedbythemodelwhenitisfedinascrambled
input,weevaluatewhethertheoutputisgrammaticallycorrectornot.
• TypeCheck. Giventheoutputgeneratedbythemodelwhenitisfedinascrambledinput,
weevaluatewhethertheoutputfollowstypeconstraintsornot.Wemeasureaccuracyoverall
constraints,i.e.,wedonotdecomposethisevaluationoverdescriptive/relativeproperties.
41Preprint
D.5.1 BASESETTINGWITHVARYINGNUMBEROFPROPERTIES
We report results under varying number of properties with the base experimental setting. See
Figs.32,33.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Exact Match Per Token Accuracy Descriptive Sentences
0.25 0.7 0.40
0.6 0.35
0.20
0.5 0.30
0.15 0.25
0.4
0.20
0.10 0.3 0.15
0.2 0.10
0.05
0.05
0.1
0.00 0.00
103 104 105 103 104 105 103 104 105
Relative Sentences Grammaticality Type Check
0.35 0.9 0.7
0.30 0.8 0.6
0.25 0.7
0.5
0.6
0.20
0.5 0.4
0.15
0.4 0.3
0.10
0.3 0.2
0.05
0.2
0.00 0.1 0.1
103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations
Figure32: Resultsonunscramblingtaskasnumberofpropertiesarevaried(Basesetting). See
App.D.4foradetaileddiscussiononmetricsused. Broadly,weseetheresultspresentedinthemain
paperareconsistentacrossvaryingnumberofproperties: themodelfirstwitnessesimprovementin
exactmatchbecauserelativesentencesstarttoimprove,i.e.,thegrammarislearned,performance
thensaturates,andthenitfinallystartsimprovingagainoncedescriptivesentences’accuracystartsto
improve. Weemphasizemetricsassigningpartialcreditalsoshowsuddenchanges,i.e.,ourresults
arenotsensitivetometricsused.
Exact Match Per Token Accuracy Descriptive Sentences
0.30 seed 0 0.7 0.5
0.25 s se ee ed d 1 2 0.6 0.4
0.20 Average 0.5
0.3
0.15 0.4
0.3 0.2
0.10
0.05 0.2 0.1
0.1
0.00 0.0
103 104 105 103 104 105 103 104 105
Relative Sentences Grammaticality Type Check
0.8
0.40
0.35 0.7
0.8
0.30 0.6
0.25 0.6 0.5
0.20 0.4
0.15 0.4 0.3
0.10
0.2
0.05 0.2
0.1
0.00
103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations
Figure33: Resultsontheunscramblingtaskforsettingwith18000properties(basesetting). We
reportthisplottozoomintoaspecificconfiguration. Ascanbeseen,thereissomeminimalvariance
acrossruns,butmostlypointsoftransitionareinasimilarrange.
42Preprint
D.5.2 CHANGINGTO1800OBJECTSANDVARYINGNUMBEROFPROPERTIES
Wechangethenumberofobjectsto1800(comparedtobasesettingof900)andreportresultsunder
varyingnumberofproperties. SeeFigs.34,35.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Exact Match Per Token Accuracy Descriptive Sentences
0.4
0.20 0.6
0.5 0.3
0.15
0.4
0.10 0.3 0.2
0.05 0.2 0.1
0.1
0.00 0.0
103 104 105 103 104 105 103 104 105
Relative Sentences Grammaticality Type Check
0.35 0.9
0.7
0.30 0.8
0.25 0.7 0.6
0.20 0.6 0.5
0.15 0.5 0.4
0.4
0.10 0.3
0.3
0.05 0.2 0.2
0.00 0.1 0.1
103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations
Figure34: Resultsonunscramblingtaskasnumberofpropertiesarevaried(1800objects).
SeeApp.D.4foradetaileddiscussiononmetricsused. Broadly, weseetheresultspresentedin
themainpaperareconsistentacrossvaryingnumberofpropertiesandalargernumberofobjects
comparedtothebasesetting: themodelfirstwitnessesimprovementinexactmatchbecauserelative
sentencesstarttoimprove,i.e.,thegrammarislearned,performancethensaturates,andthenitfinally
startsimprovingagainoncedescriptivesentences’accuracystartstoimprove. Weemphasizemetrics
assigningpartialcreditalsoshowsuddenchanges,i.e.,ourresultsarenotsensitivetometricsused.
Exact Match Per Token Accuracy Descriptive Sentences
seed 0 0.7 0.5
0.25 s se ee ed d 1 2 0.6 0.4
0.20 Average 0.5
0.3
0.15 0.4
0.10 0.3 0.2
0.05 0.2 0.1
0.1
0.00 0.0
103 104 105 103 104 105 103 104 105
Relative Sentences Grammaticality Type Check
0.8
0.40
0.35 0.8 0.7
0.30 0.6
0.25 0.6 0.5
0.20 0.4
0.15 0.4
0.3
0.10
0.2
0.05 0.2
0.1
0.00
103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations
Figure35: Resultsontheunscramblingtaskforsettingwith18000properties(1800objects).
Wereportthisplottozoomintoaspecificconfiguration. Ascanbeseen, thereissomeminimal
varianceacrossruns,butmostlypointsoftransitionareinasimilarrange.
43Preprint
D.5.3 CHANGINGTO2CLASSESANDVARYINGNUMBEROFPROPERTIES
We report results under varying number of properties with the base experimental setting. See
Figs.36,37.
14800 18000 21200 27600 30800 34000 37200 38800
16400 19600 24400 29200 32400 35600
Exact Match Per Token Accuracy Descriptive Sentences
0.7 0.5
0.25
0.6
0.4
0.20
0.5
0.3
0.15 0.4
0.10 0.3 0.2
0.05 0.2 0.1
0.1
0.00 0.0
103 104 105 103 104 105 103 104 105
Relative Sentences Grammaticality Type Check
0.40 1.0 0.9
0.35 0.9
0.30 0.8 0.8
0.7
0.25
0.6 0.7
0.20
0.5
0.15 0.6
0.4
0.10
0.3
0.05 0.5
0.2
0.00
103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations
Figure36: Resultsonunscramblingtaskasnumberofpropertiesarevaried(Basesetting). See
App.D.4foradetaileddiscussiononmetricsused. Broadly,weseetheresultspresentedinthemain
paperareconsistentacrossvaryingnumberofpropertiesandfewerclassesthanthebasesetting: the
modelfirstwitnessesimprovementinexactmatchbecauserelativesentencesstarttoimprove,i.e.,
thegrammarislearned,performancethensaturates,andthenitfinallystartsimprovingagainonce
descriptivesentences’accuracystartstoimprove. Weemphasizemetricsassigningpartialcreditalso
showsuddenchanges,i.e.,ourresultsarenotsensitivetometricsused.
Exact Match Per Token Accuracy Descriptive Sentences
0.30 seed 0 0.7 0.5
0.25 s se ee ed d 1 2 0.6 0.4
0.20 Average 0.5
0.3
0.15 0.4
0.3 0.2
0.10
0.05 0.2 0.1
0.1
0.00 0.0
103 104 105 103 104 105 103 104 105
Relative Sentences Grammaticality Type Check
1.0 0.9
0.4
0.8 0.8
0.3
0.6 0.7
0.2
0.4 0.6
0.1
0.2 0.5
0.0
103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations
Figure37: Resultsontheunscramblingtaskforsettingwith18000properties(2classes). We
reportthisplottozoomintoaspecificconfiguration. Ascanbeseen,thereissomeminimalvariance
acrossruns,butmostlypointsoftransitionareinasimilarrange.
44Preprint
D.6 EVOLUTIONOFATTENTIONMAPS
Asthemodelacquiresthegrammar,weexpectittoattendtospecificpartsofthecontexttopredictthe
nexttoken. Giventhesimplicityofourmodel,wecanexpecttheattentionmapstobesemantically
meaningful, especially after the points of emergence. To assess this, every 1000 iterations of
training,werecordmodel’sattentionmapsinthebaseexperimentalsettingon100sentencesthatare
eitherdescriptiveorrelativeinnature. Specifically,wedefineasymbolicsentencethatdefinethe
grammaticalconfigurationofthesentence,andthensamplevaluesforindividualtokensaccordingto
theirrolesa100times. Theattentionmapsarethenrecordedandaveraged.
D.6.1 DESCRIPTIVESENTENCES
Duetospaceconstraints,weonlyreportattentionmapsatiterations[0,1000,10000,30000,50000].
Theprimarymotivationhereisthatarounditeration1000iswhenthemodelfirstseemstolearn
the grammar. Similarly, between 1000–10000, it starts to learn type constraints. Then, between
10000–50000,itstartstolearnaboutdescriptiveproperties.
(a)Initialization (b)1000iterations (c)5000iterations
(d)10000iterations (e)30000iterations (f)50000iterations
Figure38: Evolutionofattentionmapsfordescriptivesentences. Sincevisualizingtheevolution
acrosstimeisdifficult,wepickafewsalientpointsaccordingtoregionidentifiedasdifferentphase
accordingtolearningcurvesandvisualizethem. Wefindthatindeedthefirsttimethemodelsees
asparseattentionstructureisaround1000iterationsoftraining,i.e.,whenitbecomesaccurateat
producinggrammaticallycorrectsentences. From5–30000iterations,themodellearnstofocuson
subjectwhenproducingnexttoken,butthelinkverbanddescriptivepropertydonotpaysubstantial
attention to subject. However, there is a point after 30000 iterations post which the attention on
subjectsubstantiallyincreases;thisistherangewhereotherevaluationshowachangeinperformance
aswell.
45Preprint
D.6.2 RELATIVESENTENCES
Duetospaceconstraints,weonlyreportattentionmapsatiterations[0,1000,10000,30000,50000].
Theprimarymotivationhereisthatarounditeration1000iswhenthemodelfirstseemstolearnthe
grammar. Similarly,between1000–10000,itstartstolearnrelativetypeconstraints. Weexpectafter
thisrange,theattentionpatterntonotchangemuch—thisisindeedwhathappens. Weseethemodel
isbasicallyimprovingthesharpnessofitsattentionmap(paymoreattentiontotokensthatwere
alreadybeingattended).
(a)Initialization (b)1000iterations (c)5000iterations
(d)10000iterations (e)30000iterations (f)50000iterations
Figure39: Evolutionofattentionmapsforrelativesentences. Sincevisualizingtheevolution
acrosstimeisdifficult,wepickafewsalientpointsaccordingtoregionidentifiedasdifferentphase
accordingtolearningcurvesandvisualizethem. Wefindthatindeedthefirsttimethemodelsees
asparseattentionstructureisaround1000iterationsoftraining,i.e.,whenitbecomesaccurateat
producinggrammaticallycorrectsentences. Oncethemodellearnsrelativetypeconstraints(around
5000iterationsgenerally),weseetheattentionpatternsessentiallystabilizesanddoesnotchange
much; albeit, it does get sharper (more attention to the tokens that were already being attended).
Interestingly,weseeobjecttokenspayinglargeattentiontoverbs,i.e.,objectsarebeingselectedby
analyzingwhichverbscamebeforethem. Weseesomenon-trivialdependencebetweenverband
theadverbprecedingit,whichitselfsolelyattendstothesubjecttoken(i.e.,itcopiesthattoken’s
representation);thiscouldsufficetoensurethemodelgetsverbsrightforasubject.
46Preprint
D.7 ANOTHERSETOFRESULTSWITHCONDITIONALGENERATION
Asmentionedbefore,Conditionalgenerationevaluationsturnouttobeextremelytime-expensive,
withasingleruntakingapproximately4daystofinishwhenconditionalgenerationisevaluated
(comparedto12hourswithout). Thisislikelyaresultofmodelgeneratingextremelylongsentences
tocomposeconditioningtokensthatcaninvolvemultiplesubjects,objects,andproperties.
Whilewefocussolelyonfreegenerationandunscramblingintheresultsreportedinthesections
above,todemonstratethatourfindingsfromthemainpaper(i.e.,inthe18000propertiessetting)
generalizetoanothersetting,weprovideresultsforsimilartoFig.5inanothersettingwith27600
properties. ShowninFig.40,wecanseeourfindingsperfectlyalignwithresultsfromthemainpaper
andotherresultsshownintheappendix: themodelfirstlearnsthegrammar,thentypeconstraints,
and witnesses improvements on unscrambling and conditional generation tasks as these relevant
structuresareacquired.
(a) P1 P2 P3 (b) P1 P2 P3 (c) P1 P2 P3
Figure40: Demonstration(27600properties). Wereportthisplottozoomintoaspecificconfigura-
tion. Ascanbeseen,thereissomeminimalvarianceacrossruns,butmostlypointsoftransitionare
inasimilarrange.
47Preprint
E SCALING OF POINT OF EMERGENCE (AKA TRANSITION POINT)
In this section, we repeat experiments from the main paper for different settings to analyze how
thepointofemergence(interchangeablycalledtransitionpointorphasetransitionhere)scaleswith
increaseinnumberofpropertiesinthelanguage. Weagainreportresultsforbothunscramblingand
freegenerationtasksandalsoaddseveralmoremetricsnotreportedinthemainpaper.
Beforeproceedinghowever,wefurtherdiscussourevaluationprotocolwhereinwerescalex-axisby
somepowerofnumberofpropertiesinthelanguagebyconnectingitbacktothenotionofphase
transitionsandemergenceinphysics. Wealsoclarifywhywemightattimesneedtorescalethe
y-axis.
E.1 COLLAPSEDCURVESHELPDEMONSTRATESCALINGOFTHETRANSITIONPOINT
AssumeS denotesacontrolvariable(e.g.,edgedensityinourbipartitegraph). AssumechangeinS
inducesaphasetransitioninoursystem(e.g.,thebipartitegraph),asmeasuredbysuddenchangein
thevalueofsomeorderparameterM(e.g.,ratiooflargestclustersizetographsize). Further,say
thetransitionpointS dependsonsomeotherpropertyofthesystemν (e.g.,numberofnodesinthe
c
bipartitegraph)viaapowerlawrelationship. Thatis,wehave
S ∝να.
c
Accordingly,ifwetrackedMasS ischanged,wewouldfinditsvaluerapidlystartstochangeas
S/να → 1. Aswechangethevalueofν,assumethevalueofMatthepointoftransitionissome
constantvalue. Thus,ifweplotMasafunctionofS/να,wewouldfindtheresults“collapse”onto
eachotheratthepointoftransition. Thisistheintuitionbehindourexperimentsinthemainpaper:
asweexpectasquare-rootdependenceonthenumberofproperties,ifwedividethecontrolvariable
(cid:112)
(trainingiterations)by |K|,weshouldseethecurvescorrespondingtolanguageswithdifferent
number of properties collapse onto each other. This argument however assumes we are tracking
theperfectorderparameterwithrespecttowhichtheoryisdefined. Thisneednotbethecase,as
discussedinthenextsection.
E.2 WHATMETRICSMAKESENSE?
Notethatthetheoryofpercolationonabipartitegraphanditscorrespondingphasetransitionfocuses
ontheratioofsizeoflargestclusterinthegraphtotheoverallgraphsize. Thatis, thetheoryof
gaugingwhichnodesarememberofthelargestclusterandhowdothismetricincreasewithscaling
of edge density. By itself, however, standard metrics one would evaluate in tasks defined in this
work,e.g.,accuracy,neednotlinearlycorrelatewiththeclustersize. Thiscanaffectthecollapse
visualizationdiscussedinApp.E.1. Toelaboratefurther,webuildonthetoysetupfromabove.
Say, the order parameter M is difficult to experimentally gauge—this is in fact the case for our
work, where describing and evaluating a notion of membership within largest cluster is difficult.
Accordingly,wemustdefinealternativemetricsthatweexpecttocorrelatewithM. Denotethis
alternativeparameterasM′ andsayM′ :=M×νβ,i.e.,anotherdependenceonν getsinvolved
inourexperimentsaswegofromMtoM′. Accordingly,ifwetrackM′ astherescaledcontrol
variableS/να isvaried,wewillfindthatinsteadofcollapsingontoaconstantvalue,systemswith
differentvaluesofν haveadifferentvalueforM′. However,importantly,wewillseethatthevalue
ofM′atthispointitselffollowsapowerlawrelationshipM′ ∝νβ. Accordingly,ifwerescaledthe
y-axisbydividingitbyνβ,wewouldseethecurvescollapseontoeachotheragain;thatis,wewill
seethat
S M′
as →1, wehave →O(1).
να νβ
Thus,whenusingalternativemetricsthataremeanttocorrelatewiththegold-standardmetric(e.g.,
M′insteadofMinthediscussionabove),arescalingofthey-axisaccordingtosomepropertyof
thesystemmaybeneededtohelpinduceacollapseofdifferentexperimentalcurves. Asdiscussedin
thenextsection,thissubtletyturnsouttobeextremelycrucialforourwork.
48Preprint
E.3 EVALUATIONMETRICSFOREVALUATINGEMERGENCEINOURWORK
Wefindartifactsofthetoyproblemdiscussedinsectionsaboveinourexperiments. Specifically,
the theory of percolation on bipartite graph focuses on largest cluster size as an order parameter.
However,itcanbedifficulttodefineacheaplycalculablemetricthatcapturesanotionof‘largest
cluster’andevaluatesmembershipofpropertiesandentitiestotheclusterinthecontextofaneural
networkbeingtrainedonsomedatadistribution. Tocircumventthis,wedefineseveralalternative
metricsthatapproximatethenotionoflargestclustertoanextent,butarenotnecessarilyexpected
toshowperfectcollapseofexperimentalcurveswhenthex-axisisrescaledbysomepowerofthe
numberofproperties. However,ifamererescalingofthey-axisbyanindependentvariable(e.g.,the
controlinthisexperiment,i.e.,numberofproperties)inducesacollapseofexperimentalcurves,then
wecanbeconfidentthetransitionpointfollowsourexpectedscaling.
EvaluationMetrics. Havingdiscussedthesubtletiesabove,wenowdiscussthesetofevaluation
metricsusedinthispapertoevaluatehowthepointofemergence(i.e.,transitionpoint)scaleswith
increaseinnumberofproperties. Weanalyzethefollowingtwotasksinthissection: unscrambling
andfreegeneration. Weusesomemetricswhicharespecifictoagiventaskandanotherbatchthatis
commontoboth,asdiscussednext.
• Unscrambling.Followingmetricsarereportedsolelyforunscramblingandgaugemodel’saccu-
racyonthetask. Asthemodellearnswhichpropertiesbelongtowhichentities,wecanexpectit
toexploitthatknowledgetoreducethehypothesisspacefornext-tokenpredictionsandgetmore
accurateonunscrambling. Hence,weexpectaccuracytosuddenlystartincreasingoratleastfor
itsrateofincreasetochangeoncethemodelundergoesapercolationtransition.
– ExactMatch: Evaluatewhetherthemodel’sunscrambledsentenceperfectlymatchesthe
ground-truth.
– Per-TokenAccuracy: Evaluatehowmanyofthetokensfrommodel’sunscrambledsentence
matchtheground-truth.
– Descriptive Sentences Accuracy: Exact match accuracy for solely sentences that are
descriptiveinnature. Similartopriorexperiments,wesimplyfiltersentencesforlengthand
useoneswith≤ 6tokensforthisevaluation,since94%suchsentencesaredescriptivein
natureandthisallowsforeasierbatchingandfastevaluation.
• FreeGeneration.Followingmetricsarereportedsolelyforfreegeneration. Similartounscram-
bling,thesemetricsevaluateamodel’sperformanceonthetaskoffreegeneration,whereinthegoal
istoproduceasentencethatisgrammaticallyvalidandrespecttypeconstraints. Wespecifically
focusontypeconstraintsinthissection. Specifically,asthemodellearnswhichpropertiesbelong
towhichentities,wecanexpectTypeCheckcorrespondingtodescriptivesentences(seebelow)
willstarttoimprovesubstantially. Incontrast,forTypeChecksofrelativeproperties(i.e.,validity
ofverbs),wedonotexpecttoseeanyeffectofhowmanydescriptivepropertiesarethereinthe
language.
– TypeCheck(Descriptive): Atypecheckevaluation,asdiscussedinmainpaper,thatchecks
whetherdescriptivepropertiesassociatedwithanentitybythemodelareinfactvalid. We
expectthepercolationphasechangetoaffectthisevaluation,yieldingcloseto0.5scaling
withnumberofproperties.
– TypeCheck(Relative): Atypecheckevaluation,asdiscussedinmainpaper,thatchecks
whether relative properties associated with an entity by the model are in fact valid. We
expectthepercolationphasechangetonotaffectthisevaluation,sinceitreliessolelyonthe
grammar,andhencethereshouldbenocleareffectofscalingnumberofpropertiesonthis
metric.
– TypeCheck(All): Atypecheckevaluation,asdiscussedinmainpaper,thatcheckswhether
allpropertiesandcorrespondingentitiesinagivensentenceareallowedtobeseenineach
other’scontext.Weexpectthepercolationphasechangetoaffectthisevaluation,since,unless
themodelgetsdescriptiveconstraintsright,thismetricwillbezero. However,therewillbea
non-trivialproportionofsentencesthatdonothaveanydescriptortokenswithinthem;we
expectimprovementonthesesentencestoincreasetheoverallmetric,leadingtoasaturation
phaseuntilthepercolationtransitionkicksinandthemodelstartsinferringwhichproperties
areassociatedwithwhichentity.
49Preprint
• Commonmetrics.Followingmetricsarereportedforboththeunscramblingandfreegeneration
tasks. Thesemetricsassesswhetherthemodeldeemsagivenpropertyandanentitybelongto
eachother,regardlessofwhetherithasseenthemtogetheraspartofthesamecontext. Inthis
sense,thesemetricstestaminimalnotionofclustermembership,wheretheclusterisdefinedby
classesdividingthebipartitegraph.
– AverageProbabilityofValidTokens. Usedforevaluatingdescriptivetypeconstraintsin
freegenerationandunscrambling. Specifically,wesampleasentencefromLthatremarkson
anentitypossessingaproperty,andthenevaluateprobabilityofthispropertybeingthenext
tokenwhenthesentenceisinputtedtothemodel. Forexample,letx=The fire was
large,weevaluatePr(δ(f(x) ,large)|x ),wherex 1denotesthesentenceuptothe
1 −1 −
lasttokenandf(x) denotesthefirsttokenpredictedbythemodel. Theresultisaveraged
1
over1000sentences.
– NegativeLog-LikelihoodofValidSentences. Forfreegeneration,wesampleadescriptive
sentencefromthelanguageandevaluatehowlikelythemodeldeemsthissentence,reporting
it as negative log-likelihood (NLL). Similarly, for unscrambling, we sample a random
descriptivesentence,scrambleit,andthenevaluatehowlikelythemodeldeemstheground-
truthunscrambledversion.
– NormalizedRankofValidTokens. Thisevaluationissimilartotheaverageprobability
evaluationabove. However,wenowcomputetherankofrandomlysampleddescriptortoken
insteadoftheprobabilityassociatedbythemodeltothistoken. Ifthemodelknowswhich
properties go with an entity, the rank of tokens associated to said entity’s properties will
be low, indicating they are highly likely to be sampled. This metric scales as a function
ofvocabularysize;hence,wedivideitbythenumberofproperties|K|andcalledthatthe
normalizedrank.
– PercentTop-K.Similartotherankmetricabove,thismetricmerelycheckswhethertherank
islessthansomethreshold;ifso,itreturnsTrue,indicatingthemodelunderstandsthatthe
propertybeingevaluatedisvalidforthegivenentity. Wesetthethresholdtobeequalto
numberofpropertiesassociatedwithaclass,i.e.,|K|/C.
E.4 EXPERIMENTALSETTINGS
Weanalyzethefollowingsettings,sweepingthenumberofpropertiesintherange14800—38800,in
incrementsof1600.
• Basesetting: Thisisthesettingusedthroughoutthepaper, i.e., with10classesand900
entities.
• Differentclasssetting: wechangenumberofclassesto2andrepeatallevaluationsinthis
setting.
• Differententitiessetting: wechangenumberofentitiesto1800andrepeatallevaluationsin
thissetting.
50Preprint
E.5 SCALINGINTHEUNSCRAMBLINGTASK
E.5.1 BASESETTINGWITHVARYINGNUMBEROFPROPERTIES
Wereportresultsundervaryingnumberofpropertieswiththebaseexperimentalsettingfirst. See
Fig.41formetricsspecifictounscrambling,Fig.42forthecommonmetricsthatmorecloselycapture
anotionofclustermembership,andFig.43fordifferentx-axisrescalingsfortheaverageprobability
curvesthatdemonstratevalidityofclaimedscalingofthetransitionpoint.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Exact Match Per Token Acc. Descriptive Sentences Acc.
0.25 0.40
0.6 0.35
0.20
0.30
0.5
0.15 0.25
0.4 0.20
0.10 0.15
0.3 0.10
0.05
0.05
0.2
0.00 0.00
104 103 102 104 103 102 104 103 102
Iterations / |K|1.5 Iterations / |K|1.5 Iterations / |K|1.5
Figure41: Resultsonmetricsspecifictounscrambling(Basesetting). Weseeacollapseofall
metricsundera1.5scalingexponent.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Avg. Probability NLL × |K| 0.06 1eN 5orm. Rank × |K| 1 Percent Top-K (All) × |K|0.2
0.35 1.7 89 7
0.30
1.6 7
0.25 6
6
0.20 1.5
5
0.15 1.4 4 5
0.10 1.3 3
4
0.05 2
1.2
0.00 1
104 103 102 106 105 104 101 102 103 101 102 103
Iterations / |K|1.5 Iterations / |K|2.0 Iterations / |K|0.5 Iterations / |K|0.5
Figure42: Resultsonmetricsdesignedtoapproximateclustermembership(Basesetting). We
seeanapproximatecollapseofinflectionpointsinthenormalizedrankandpercenttop-Kmetrics
undera0.5scalingexponent. Averageprobabilityisexpectedtofollowaccuracycurves,yieldinga
1.5scalingexponent. Interestingly,NLLhasascalingexponentof2.0.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Avg. Probability Avg. Probability Avg. Probability Avg. Probability
0.35 0.35 0.35 0.35
0.30 0.30 0.30 0.30
0.25 0.25 0.25 0.25
0.20 0.20 0.20 0.20
0.15 0.15 0.15 0.15
0.10 0.10 0.10 0.10
0.05 0.05 0.05 0.05
0.00 0.00 0.00 0.00
101 102 103 101 100 104 103 102 106 105 104
Iterations / |K|0.5 Iterations / |K|1.0 Iterations / |K|1.5 Iterations / |K|2.0
Figure43: Averageprobabilitycurvesfordifferentx-axisrescalings(Basesetting). Anexponent
of1.5inducesthebestcollapseofexperimentalcurves. Insetplotszoominneartransitionpoint.
51Preprint
E.5.2 CHANGINGTO1800OBJECTSANDVARYINGNUMBEROFPROPERTIES
Wechangethenumberofobjectsto1800(comparedtobasesettingof900)andreportresultsunder
varying number of properties. See Fig. 44 for metrics specific to unscrambling, Fig. 45 for the
commonmetricsthatmorecloselycaptureanotionofclustermembership,andFig.46fordifferent
x-axisrescalingsfortheaverageprobabilitycurvesthatdemonstratevalidityofclaimedscalingof
thetransitionpoint.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Exact Match Per Token Acc. Descriptive Sentences Acc.
0.4
0.20 0.6
0.15 0.5 0.3
0.10 0.4 0.2
0.3
0.05 0.1
0.2
0.00 0.0
106 105 104 106 105 104 106 105 104
Iterations / |K|2.0 Iterations / |K|2.0 Iterations / |K|2.0
Figure44: Resultsonunscramblingtaskasnumberofpropertiesarevaried(1800objects). We
seeacollapseofallmetricsundera2.0scalingexponent,indicatinganeffectofnumberofentities
possiblyonthetransitionpoint. Arguably,thisisexpectedsinceunscramblingisaffectedbyboth
numberofentitiesandpropertiesinvolvedinthelanguage.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
0.40
Avg. Probability NLL × |K| 0.06 91eN 5orm. Rank × |K| 1 7.0Percent Top-K (All) × |K|0.2
0.35 1.8 8 6.5
0.30 1.7 7
6.0
0.25 1.6 6
0.20 1.5 5 5.5
0.15 1.4 4 5.0
0.10 3 4.5
1.3
0.05 2 4.0
0.00 1.2 1
106 105 104 106 105 104 101 102 103 101 102 103
Iterations / |K|2.0 Iterations / |K|2.0 Iterations / |K|0.5 Iterations / |K|0.5
Figure45: Resultsonunscramblingtaskasnumberofpropertiesarevaried(1800objects). We
seeanapproximatecollapseofinflectionpointsinthenormalizedrankandpercenttop-Kmetrics
witha0.5scalingexponent. Averageprobabilityisexpectedtofollowaccuracycurves,yieldinga2.0
scalingexponent. NLLagainshowsascalingexponentof2.0,similartobasesetting.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Avg. Probability Avg. Probability Avg. Probability Avg. Probability
0.40 0.40 0.40 0.40
0.35 0.35 0.35 0.35
0.30 0.30 0.30 0.30
0.25 0.25 0.25 0.25
0.20 0.20 0.20 0.20
0.15 0.15 0.15 0.15
0.10 0.10 0.10 0.10
0.05 0.05 0.05 0.05
0.00 0.00 0.00 0.00
101 102 103 101 100 104 103 102 106 105 104
Iterations / |K|0.5 Iterations / |K|1.0 Iterations / |K|1.5 Iterations / |K|2.0
Figure46: Averageprobabilitycurvesfordifferentx-axisrescalings(1800objects). Anexponent
between1.5–2.0inducesthebestcollapseofexperimentalcurves,similartobasesetting. Insetplots
zoominneartransitionpoint.
52Preprint
E.5.3 CHANGINGTO2CLASSESANDVARYINGNUMBEROFPROPERTIES
We change the number of classes to 2 (compared to base setting of 10) and report results under
varying number of properties. See Fig. 44 for metrics specific to unscrambling, Fig. 45 for the
commonmetricsthatmorecloselycaptureanotionofclustermembership,andFig.46fordifferent
x-axisrescalingsfortheaverageprobabilitycurvesthatdemonstratevalidityofclaimedscalingof
thetransitionpoint.
14800 18000 21200 27600 30800 34000 37200 38800
16400 19600 24400 29200 32400 35600
Exact Match Per Token Acc. Descriptive Sentences Acc.
0.7 0.5
0.25
0.6 0.4
0.20
0.5 0.3
0.15
0.10 0.4 0.2
0.05 0.3 0.1
0.00 0.2 0.0
104 103 102 104 103 102 104 103 102
Iterations / |K|1.5 Iterations / |K|1.5 Iterations / |K|1.5
Figure47: Resultsonunscramblingtaskasnumberofpropertiesarevaried(2classes). Wesee
acollapseofallmetricsundera1.5scalingexponent.
14800 18000 21200 27600 30800 34000 37200 38800
16400 19600 24400 29200 32400 35600
Avg. Probability NLL × |K| 0.06 1eN 5orm. Rank × |K| 1 Percent Top-K (All) × |K|0.2
1.6 6 7.5
0.4 7.0
5
1.5
0.3 6.5
4
1.4 6.0
0.2 3
1.3 5.5
0.1 1.2 2 5.0
1 4.5
0.0 1.1
104 103 102 106 105 104 101 102 103 101 102 103
Iterations / |K|1.5 Iterations / |K|2.0 Iterations / |K|0.5 Iterations / |K|0.5
Figure48: Resultsonunscramblingtaskasnumberofpropertiesarevaried(2classes). Wesee
anapproximatecollapseofinflectionpointsinthenormalizedrankandpercenttop-Kmetricswith
a0.5scalingexponent. Averageprobabilityisexpectedtofollowaccuracycurves,yieldinga1.5
scalingexponent. NLLagainshowsascalingexponentof2.0,similartoothersettings.
14800 18000 21200 27600 30800 34000 37200 38800
16400 19600 24400 29200 32400 35600
Avg. Probability Avg. Probability Avg. Probability Avg. Probability
0.4 0.4 0.4 0.4
0.3 0.3 0.3 0.3
0.2 0.2 0.2 0.2
0.1 0.1 0.1 0.1
0.0 0.0 0.0 0.0
101 102 103 101 100 104 103 102 106 105 104
Iterations / |K|0.5 Iterations / |K|1.0 Iterations / |K|1.5 Iterations / |K|2.0
Figure49: Averageprobabilitycurvesfordifferentx-axisrescalings(2classes). Anexponentof
1.5inducesaperfectcollapseofexperimentalcurves,similartobasesetting. Insetplotszoominnear
transitionpoint.
53Preprint
E.6 SCALINGINTHEFREEGENERATIONTASK
E.6.1 BASESETTINGWITHVARYINGNUMBEROFPROPERTIES
Wereportresultsundervaryingnumberofpropertieswiththebaseexperimentalsettingfirst. See
Fig.50formetricsspecifictofreegeneration, Fig.51forthecommonmetricsthatmoreclosely
captureanotionofclustermembership,Fig.52fordifferentx-axisrescalingsfortheaverageproba-
bilitycurvesthatdemonstratevalidityofclaimedscalingofthetransitionpointanditscorresponding
variantinFig.53wherethey-axisisnotrescaledtodemonstratethetransitionpointsalignbetter
withourclaimedscalingexponent.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Type Check (Desc.) × |K|0.5 Type Check (Rel.) × |K|0.0 Type Check (All) × |K|0.5
1.0 40
40
35
35 0.8 30
30 0.6 25
25 20
20 0.4 15
10
15 0.2
5
10
0.0 0
100 101 102 103 103 104 105 100 101 102 103
Iterations / |K|0.5 Iterations / |K|0.0 Iterations / |K|0.5
Figure 50: Results on metrics specific to unscrambling (Base setting). We see a collapse of
descriptiveandallconstraintsmetricsundera0.5scalingexponent;relativeconstraintsareclearly
invarianttonumberofproperties.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Avg. Probability × |K|1.5 NLL × |K|0.05 Norm. Rank × |K|0.5 Percent Top-K × |K|0.5
80
400 1.44
0.040 70
350 1.42
0.035 60
300 1.40
1.38 0.030 50
250
1.36 0.025 40
200
1.34 0.020 30
150
100 1.32 0.015 20
100 101 102 103 100 101 102 103 100 101 102 103 100 101 102 103
Iterations / |K|0.5 Iterations / |K|0.5 Iterations / |K|0.5 Iterations / |K|0.5
Figure 51: Results on metrics designed to approximate cluster membership (Base setting).
Weseeallmetricsshowanapproximatecollapseundera0.5scalingexponent. Collapseisinthe
inflectionpointsforthenormalizedrankandpercenttop-Kmetrics.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Avg. Probability × |K|1.5 Avg. Probability × |K|1.5 Avg. Probability × |K|1.5 Avg. Probability × |K|1.5
700 700 700 700
600 600 600 600
500 500 500 500
400 400 400 400
300 300 300 300
103 104 105 101 102 103 101 100 104 103 102
Iterations / |K|0.0 Iterations / |K|0.5 Iterations / |K|1.0 Iterations / |K|1.5
Figure52: Averageprobabilitycurvesfordifferentx-axisrescalings(Basesetting). Anexponent
between0.5–1.0canbeexpectedtoinducethebestcollapseforthemetricofaverageprobability;
resultsinFig.53showtheexponentiscloserto0.5.
54Preprint
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
1e 4Avg. Probability 1e 4Avg. Probability 1e 4Avg. Probability 1e 4Avg. Probability
2.25 2.25 2.25 2.25
2.00 2.00 2.00 2.00
1.75 1.75 1.75 1.75
1.50 1.50 1.50 1.50
1.25 1.25 1.25 1.25
1.00 1.00 1.00 1.00
0.75 0.75 0.75 0.75
0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25
103 104 105 101 102 103 101 100 104 103 102
Iterations / |K|0.0 Iterations / |K|0.5 Iterations / |K|1.0 Iterations / |K|1.5
Figure53: Averageprobabilitycurvesfordifferentx-axisrescalings(Basesetting). Anexponent
of0.5betteralignsthetransitionpoints.
E.6.2 CHANGINGTO1800OBJECTSANDVARYINGNUMBEROFPROPERTIES
Wechangethenumberofobjectsto1800(comparedtobasesettingof900)andreportresultsunder
varyingnumberofproperties. SeeFig.54formetricsspecifictofreegeneration, Fig.55forthe
common metrics that more closely capture a notion of cluster membership, Fig. 56 for different
x-axis rescalings for the average probability curves that demonstrate validity of claimed scaling
ofthetransitionpointanditscorrespondingvariantinFig.57wherethey-axisisnotrescaledto
demonstratethetransitionpointsalignbetterwithourclaimedscalingexponent.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Type Check (Desc.) × |K|0.5 Type Check (Rel.) × |K|0.0 Type Check (All) × |K|0.5
35
35 30
0.8
30 25
0.6
25 20
20 0.4 15
10
15
0.2
5
10
0.0 0
100 101 102 103 103 104 105 100 101 102 103
Iterations / |K|0.5 Iterations / |K|0.0 Iterations / |K|0.5
Figure 54: Results on metrics specific to unscrambling (1800 objects). We see a collapse of
descriptiveandallconstraintsmetricsundera0.5scalingexponent;relativeconstraintsareclearly
invarianttonumberofproperties.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Avg. Probability × |K|1.5 NLL × |K|0.05 Norm. Rank × |K|0.5 Percent Top-K × |K|0.5
0.045 80
400 1.48
0.040 70
350 1.46
300 1.44 0.035 60
250 1.42 0.030 50
40
1.40 0.025
200
1.38 0.020 30
150
100 1.36 0.015 20
100 101 102 103 100 101 102 103 100 101 102 103 100 101 102 103
Iterations / |K|0.5 Iterations / |K|0.5 Iterations / |K|0.5 Iterations / |K|0.5
Figure 55: Results on metrics designed to approximate cluster membership (1800 objects).
Weseeallmetricsshowanapproximatecollapseundera0.5scalingexponent. Collapseisinthe
inflectionpointsforthenormalizedrankandpercenttop-Kmetrics.
55Preprint
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
Avg. Probability × |K|1.5 Avg. Probability × |K|1.5 Avg. Probability × |K|1.5 Avg. Probability × |K|1.5
700 700 700 700
600 600 600 600
500 500 500 500
400 400 400 400
300 300 300 300
103 104 105 101 102 103 101 100 104 103 102
Iterations / |K|0.0 Iterations / |K|0.5 Iterations / |K|1.0 Iterations / |K|1.5
Figure56: Averageprobabilitycurvesfordifferentx-axisrescalings(1800objects). Anexponent
between0.5–1.0canbeexpectedtoinducethebestcollapseforthemetricofaverageprobability;
resultsinFig.57showtheexponentiscloserto0.5.
14800 18000 21200 24400 27600 30800 34000 37200
16400 19600 22800 26000 29200 32400 35600 38800
1e 4Avg. Probability 1e 4Avg. Probability 1e 4Avg. Probability 1e 4Avg. Probability
2.25 2.25 2.25 2.25
2.00 2.00 2.00 2.00
1.75 1.75 1.75 1.75
1.50 1.50 1.50 1.50
1.25 1.25 1.25 1.25
1.00 1.00 1.00 1.00
0.75 0.75 0.75 0.75
0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25
103 104 105 101 102 103 101 100 104 103 102
Iterations / |K|0.0 Iterations / |K|0.5 Iterations / |K|1.0 Iterations / |K|1.5
Figure57: Averageprobabilitycurvesfordifferentx-axisrescalings(1800objects). Anexponent
of0.5betteralignsthetransitionpoints.
E.6.3 CHANGINGTO1800OBJECTSANDVARYINGNUMBEROFPROPERTIES
We change the number of classes to 2 (compared to base setting of 10) and report results under
varyingnumberofproperties. SeeFig.58formetricsspecifictofreegeneration, Fig.59forthe
common metrics that more closely capture a notion of cluster membership, Fig. 60 for different
x-axis rescalings for the average probability curves that demonstrate validity of claimed scaling
ofthetransitionpointanditscorrespondingvariantinFig.61wherethey-axisisnotrescaledto
demonstratethetransitionpointsalignbetterwithourclaimedscalingexponent.
14800 18000 21200 27600 30800 34000 37200 38800
16400 19600 24400 29200 32400 35600
Type Check (Desc.) × |K|0.1 Type Check (Rel.) × |K|0.0 Type Check (All) × |K|0.1
1.0
1.6
1.6 0.9
1.4
0.8
1.2
1.4
0.7 1.0
1.2 0.6 0.8
0.6
0.5
1.0
0.4
0.4
100 101 102 103 103 104 105 100 101 102 103
Iterations / |K|0.5 Iterations / |K|0.0 Iterations / |K|0.5
Figure58: Resultsonmetricsspecifictounscrambling(2classes). Weseeacollapseofdescriptive
andallconstraintsmetricsundera0.5scalingexponent;relativeconstraintsareclearlyinvariantto
numberofproperties.
56Preprint
14800 18000 21200 27600 30800 34000 37200 38800
16400 19600 24400 29200 32400 35600
Avg. Probability × |K|1.1 NLL × |K|0.04 Norm. Rank × |K|0.2 Percent Top-K × |K|0.1
1.45 0.8 0.8
3.0 1.44 0.7
1.43 0.7
0.6
2.5 1.42 0.6
1.41 0.5
2.0 1.40 0.5 0.4
1.5 11 .. 33 89 0.4 0.3
0.2
1.37 0.3
101 100 101 102 103 100 101 102 103 101 100 101 102 103 101 100 101 102 103
Iterations / |K|0.5 Iterations / |K|0.5 Iterations / |K|0.5 Iterations / |K|0.5
Figure59: Resultsonmetricsdesignedtoapproximateclustermembership(2classes). Wesee
allmetricsshowanapproximatecollapseundera0.5scalingexponent. Collapseisintheinflection
pointsforthenormalizedrankandpercenttop-Kmetrics.
14800 18000 21200 27600 30800 34000 37200 38800
16400 19600 24400 29200 32400 35600
Avg. Probability × |K|1.1 Avg. Probability × |K|1.1 Avg. Probability × |K|1.1 Avg. Probability × |K|1.1
3.0 3.0 3.0 3.0
2.5 2.5 2.5 2.5
2.0 2.0 2.0 2.0
1.5 1.5 1.5 1.5
101 102 103 104 105 101 100 101 102 103 103 102 101 100 101 106 105 104 103 102
Iterations / |K|0.0 Iterations / |K|0.5 Iterations / |K|1.0 Iterations / |K|1.5
Figure60: Averageprobabilitycurvesfordifferentx-axisrescalings(2classes). Anexponent
between0.5–1.0canbeexpectedtoinducethebestcollapseforthemetricofaverageprobability;
resultsinFig.61showtheexponentiscloserto0.5.
14800 18000 21200 27600 30800 34000 37200 38800
16400 19600 24400 29200 32400 35600
1e 5Avg. Probability 1e 5Avg. Probability 1e 5Avg. Probability 1e 5Avg. Probability
8 8 8 8
7 7 7 7
6 6 6 6
5 5 5 5
4 4 4 4
3 3 3 3
2 2 2 2
1 1 1 1
101 102 103 104 105 101 100 101 102 103 103 102 101 100 101 106 105 104 103 102
Iterations / |K|0.0 Iterations / |K|0.5 Iterations / |K|1.0 Iterations / |K|1.5
Figure61: Averageprobabilitycurvesfordifferentx-axisrescalings(2classes). Anexponentof
0.5betteralignsthetransitionpoints.
57Preprint
Number of properties: 14800 Number of properties: 18000 Number of properties: 21200
0.4 D Fia tta 0.30 D Fia tta
0.20
D Fia tta
12843.47 0.25 18294.00 27843.71
0.3
0.20 0.15
0.2 0.15 0.10
0.10
0.1 0.05
0.05
0.0 0.00 0.00
103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations
(a)14800properties (b)18000properties (c)21200properties
Number of properties: 27600 Number of properties: 32400 Number of properties: 38800
0.14 D Fia tta 0.10 D Fia tta 0.06 D Fia tta
34300.40 43982.51 47085.87
0.12 0.05
0.08
0.10 0.04
0.08 0.06
0.03
0.06 0.04
0.02
0.04
0.02 0.02 0.01
0.00 0.00 0.00
103 104 105 103 104 105 103 104 105
Iterations Iterations Iterations
(d)27600properties (e)32400properties (f)38800properties
Figure62: BilinearSplineFits. Workingwiththehypothesisthatthemodel’saccuracyonunscram-
blingdescriptivesentencesundergoesfirstasaturationregimeatlowperformance,beforesuddenly
changingslopeandrapidlyimprovinginperformance,wefitbilinearsplinestomaximallyexplain
theseresults. Thebreakpointidentifiedusingthesefitsisusedtodefinethetransitionpointscaling
curveinFig.63. Onecaneasilyseeinthesecurvesthatthebreakpointismovingrightwardsasthe
numberofpropertiesareincreased.
F ALTERNATIVE ANALYSIS OF SCALING OF TRANSITION POINT
Amoreconventionalanalysisofhowthetransitionpoints
Transition Point Scaling (averaged across 3 runs)
scalesinvolvessimplyidentifyingthetransitionpointfor
Slope: 1.40
differentexperiments,collatingtheresults,andfittinga
curve to the identified transition points. We chose the 4×104
collapseofexperimentalcurvesprotocoloverthismethod-
ology since, except for unscrambling, defining an algo- 3×104
rithmic objective for curve fitting is difficult. However,
atleastforunscrambling,wecanfollowthemoreusual
pipelineandgetthecurvefitstoseeiftheyalignwithour 2×104
alternativeprotocolofcollapseofexperimentalcurves.
Setup. One can easily see that when the x-axis is log-
scaled,bothaverageprobabilityanddescriptivesentences
2×104 3×104 4×104
accuracy show a scaling curve wherein there is first a Number of properties
saturationatlowperformance,andthensudden(approx-
Figure 63: Transition point scaling.
imately) linear growth. Exploiting this pattern, we can
Working with the hypothesis that the
simplyfitabilinearsplinetominimizethemeansquare
model’s accuracy on unscrambling de-
errorfromthedataandusethebreakpointofthisspline
scriptivesentencesundergoesfirstasatu-
asanapproximationtothetransitionpoint.
rationregimeatlowperformance,before
Results. We find a power law with an exponent of 1.4 suddenlychangingslopeandrapidlyim-
explainsthedatafairlywell(seeFig.63). Thatis,thetran- provinginperformance,wefitbilinear
sitionpoint,accordingtothismethod,scalesasapower splines to maximally explain these re-
lawinnumberofpropertieswithanexponentof1.4. This sults. The breakpoint identified using
exponent is fairly close to the one identified using the thesefitsisusedtodefinethetransition
collapseprotocol,i.e.,1.5. pointscalingcurve.
For completeness, we also show a few of the Bilinear
splinefitsfordescriptivesentences’accuracyasafunctionofdatascalinginFig.62.
58
ycaruccA
ycaruccA
ycaruccA
ycaruccA
tniop
noitisnarT
ycaruccA
ycaruccA