Towards Evaluating and Building Versatile Large Language
Models for Medicine
Chaoyi Wu1,2,∗, Pengcheng Qiu1,2,∗, Jinxin Liu3, Hongfei Gu4, Na Li3,
Ya Zhang1,2, Yanfeng Wang1,2,† and Weidi Xie1,2,†
1Shanghai Jiao Tong University 2Shanghai AI Laboratory
3China Mobile Communications Group Co., Ltd.
4China Mobile Communications Group Shanghai Co., Ltd.
Inthisstudy,wepresentMedS-Bench,acomprehensivebenchmarkdesignedtoevaluatetheperformanceof
largelanguagemodels(LLMs)inclinicalcontexts. Unlikeexistingbenchmarksthatfocusonmultiple-choice
questionanswering,MedS-Benchspans11high-levelclinicaltasks,includingclinicalreportsummarization,
treatment recommendations, diagnosis, named entity recognition, and medical concept explanation, among
others. We evaluated six leading LLMs, e.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and
Claude-3.5 using few-shot prompting, and found that even the most sophisticated models struggle with
thesecomplextasks. Toaddresstheselimitations,wedevelopedMedS-Ins,alarge-scaleinstructiontuning
dataset for medicine. MedS-Ins comprises 58 medically oriented language corpora, totaling 13.5 million
samplesacross122tasks. Todemonstratethedataset’sutility,weconductedaproof-of-conceptexperiment
by performing instruction tuning on a lightweight, open-source medical language model. The resulting
model, MMedIns-Llama 3, significantly outperformed existing models across nearly all clinical tasks. To
promote further advancements in the application of LLMs to clinical challenges, we have made the MedS-
Ins dataset fully accessible and invite the research community to contribute to its expansion. Additionally,
wehavelaunchedadynamicleaderboardforMedS-Bench,whichweplantoregularlyupdatethetestset
to track progress and enhance the adaptation of general LLMs to the medical domain. Leaderboard:
https://henrychur.github.io/MedS-Bench/. Github: https://github.com/MAGIC-AI4Med/MedS-Ins.
1 Introduction
Large Language Models (LLMs) have recently achieved significant advancements across various natural
language processing tasks, demonstrating remarkable capabilities in language translation, text generation,
dialogue, and beyond. These developments have also extended into the medical domain, where LLMs have
achievedhighscoresonmultiple-choicequestion-answering(MCQA)benchmarksinhealthcare,andsuccessfully
passed the UMLS examination, as noted by Singhal et al. [37, 38]. Moreover, LLMs have shown expert-level
performance in clinical text summarization when appropriate prompting strategies are employed [42].
Alongside these advancements, however, there has been growing criticisms and concerns regarding the
application of LLMs in clinical settings, primarily due to their deficiencies in fundamental medical knowledge.
Forinstance,LLMshavedemonstratedpoorcomprehensionofICDcodes[39],producedinaccuratepredictions
related to clinical procedures [11], and misinterpreted Electronic Health Record (EHR) data [9]. We posit that
these polarized views on the efficacy of LLMs arise from the stringent standards required for AI deployment
in clinical environments. Current benchmarks, which largely focus on multiple-choice problems [48, 38, 5], fail
to adequately reflect the practical utility of LLMs in real-world clinical scenarios.
To address this gap, we introduce MedS-Bench (S for Super), a comprehensive benchmark that extends
beyond multiple-choice question answering (MCQA), to include 11 advanced clinical tasks, such as clinical
report summarization, treatment recommendations, diagnosis, and named entity recognition, among others.
This benchmark provides clinicians and researchers with a detailed understanding of where LLMs excel
and where they fall short in medical tasks. Specifically, we evaluate six mainstream models for medicine:
MEDITRON [5], Mistral [12], InternLM 2 [3], Llama 3 [41], GPT-4 [18] and Claude-3.5 [40]. Our findings
indicate that even the most advanced LLMs struggle with complex clinical tasks, even when utilizing few-shot
prompting, underscoring the gap between high performance on MCQA benchmarks and the actual demands
∗ Equal contributions.
† Corresponding author. Email addresses: {wtzxxxwcy02, weidi}@sjtu.edu.cn
4202
guA
22
]LC.sc[
1v74521.8042:viXraof clinical practice.
Toadvancethedevelopmentofopen-sourcemedicalLLMscapableoftacklingabroadspectrumofclinicaltasks,
we take inspiration from the idea of Super-NaturalInstructions [46], and construct the first, comprehensive
instruction tuning dataset for medicine, MedS-Ins. It aggregates 58 open-source biomedical natural
language processing datasets from five text sources, including exams, clinical texts, academic papers, medical
knowledge bases, and daily conversations, resulting in 13.5 million samples across 122 clinical tasks, each
accompanied with hand-written task definitions. We performed extensive instruction tuning on open-source
medical language models, and explored both zero-shot and few-shot prompting strategies. The outcome is a
new medical LLM – MMedIns-Llama 3, for the first time, showing the effectiveness of training on diverse
medical tasks through instruction tuning, enabling open-source medical LLMs to surpass leading closed-source
models, including GPT-4 and Claude-3.5, across a wide range of clinical tasks.
While our final model serves primarily as an academic proof of concept, we believe that MedS-Ins represents
aninitialsteptowardadvancingmedicalLLMsforreal-worldclinicalapplications, movingbeyondtheconfines
of online chat or multiple-choice question answering.
2 Results
In this section, we first introduce MedS-Bench, the benchmark employed in our study, designed to provide a
comprehensive evaluation across a range of tasks critical to clinical applications. We then present detailed
statisticsonourinstructiontuningdataset,MedS-Ins,whichwascarefullycuratedtocoverabroadspectrum
of medical language processing tasks. Finally, we provide an in-depth analysis of the evaluation results,
comparingtheperformanceofleadingmainstreammodelswithourownmodel, MMedIns-Llama 3, adapted
from an open-source language model and fine-tuned on comprehensive medical instructions.
To ensure clarity in our subsequent discussion and analysis, we define key terminologies used throughout this
study. For additional examples, please refer to Table A.3 in Supplementary.
• Text domains: Refers to the nature or type of the data, such as clinical texts, examination materials,
academic papers, and so forth.
• Data sources: Incontrasttothe“textdomains” whichdescribetheattributeofthedata, “datasources”
refer to the specific origins of the data, such as MIMIC-IV or PubMed papers. Different data sources
may belong to the same text domain.
• Task categories: These denote the broad types of language processing tasks, such as multiple-choice
question answering or named entity recognition, etc. Tasks within the same category share a common
objective.
• Tasks: Thesedenotethefundamentalunits(leafnodes)inourdatacollectionpipeline,includingspecific
tasks like outcome extraction, drug dose extraction, pathology summarization, etc. Each task may be
defined by unique combinations of data sources, task categories, or text domains.
2.1 The Description of MedS-Bench
ToevaluatethecapabilitiesofvariousLLMsinclinicalapplications,wedevelopedMedS-Bench,acomprehen-
sivemedicalbenchmarkthatextendsbeyondtraditionalmultiple-choicequestions. MedS-Benchencompasses
11 high-level clinical task categories, derived from 39 existing datasets, as illustrated in Figure 1.
Each dataset was reformatted into an instruction-prompted question-answering structure, complete with
hand-crafted task definitions (instructions), as shown in Figure 2a. The task categories we considered include:
Multi-choice Question Answering, Text Summarization, Information Extraction, Explanation, Rationale,
Named Entity Recognition, Diagnosis, Treatment Planning, Clinical Outcome Prediction, Text Classification,
Fact Verification, and Natural Language Inference. A more detailed description of each category is provided
in Supplementary Sec. A.2.
In addition to defining these task categories, we also provide detailed statistics on the number of tokens and
distinguishtherequiredcompetenciesforLLMstoaddresseachtask,aspresentedinTable9inSupplementary.
Following previous work [15], we manually classified the tasks into two categories based on the skills required:
(i) recalling facts from the model, and (ii) retrieving facts from the provided context. Broadly speaking,
|2Category Number PubMed QA Med QA Head QA
MedMC QA
Diagnosis 134529 Igaku QA
Clinical Outcome
127527
Prediction CT Findings Sum.
MedQA HeadQA
Text Sum. 41238
Info Ext. 16856
DO
MCQA 13701
MIMIC-IV
NER 5159
MCQA
Treatment MMedBench
Planning 3535 Text Sum. Explanation
RCT
Explanation 3584
HoC
NLI 1499
SEER
Fact Ver. 979 Hospitalization NLI
MIMIC4ED NER DDXPlus
Text Cls. 158
72h ED Revisit MedNLI Diagnosis Decision
Item Number
EBMS
Categories 11 Critical Triage Generative
PICO
Source
39
Dataset
Tasks 52
Samples 0.4M
Figure1|BenchmarkStatistics. Thehierarchicalringchartmeticulouslydisplaysthedatadistributionwithintheevaluation
benchmarks. Thefirsttiercategorizesthetypesoftasks,withthebenchmarksencompassing11primarytaskcategories. The
second tier outlines the datasets involved, including 39 datasets in total. The third tier details the specific tasks, with the
benchmarkscollectivelyaddressing52distincttasks. Overall,thisbenchmarkallowsforathoroughandcomprehensiveevaluation
ofmodelperformanceacrossmultipledimensions.
the former involves tasks that require to access knowledge encoded in the model’s weights from large-scale
pre-training, while the latter involves tasks that necessitate extracting information from the provided context,
such as in summarization or information extraction. As shown in Table 9 in Supplementary, eight of the
task categories require the model to recall knowledge from the model, while the remaining three require fact
retrieval from the given context.
2.2 The Description of MedS-Ins
In this section, we introduce our proposed instruction dataset, MedS-Ins, with the data collected from
5 distinct text sources and 19 task categories, 122 distinct clinical tasks. The statistics of MedS-Ins are
summarized in Figure 2.
Text Domains
Our proposed instruction tuning dataset is composed of samples drawn from five distinct sources: exams,
clinical texts, academic papers, medical knowledge bases, and daily conversations.
Exams: This category consists of data from medical examination questions across various countries. It
encompasses a broad spectrum of medical knowledge, ranging from fundamental medical facts to complex
clinical procedures. While the exam domain is a vital resource for understanding and assessing medical
|3a. Task Design b. Dataset Collection c. Data Processing
MedQA Definition: You need to
Summarize the MedQA SEER
summarize …
note's conclusion
in a few words MIMIC-IV HealthFact MIMIC-IV I bn rep au st: t T tisis ss uu ee …density: C - The
… Output: No evidence of
BC5Chem DDXPlus malignancy …
…
Text Summarization MedNLI 13.5M Samples
19 categories of clinical tasks defined 58 medically-oriented One unified instruction datase×t for various
by clinicians. language datasets. downstream tasks.
d. MedS-Ins Statistics
Clinical Outcome Prediction MIMIC4ED Benchmark
Clinical Text
Text Summarization MIMIC-IV
RCT
Natural Language Inference MeQSum
Wrong Candidate Generation PubMed
Exam
Text Classification MIMIC-CXR
Readibility
Sentence Composition Analysis MS2
Fact Verification MedNLI
Academical Papers LitCovid
PubMed MeSH
HoC
Question Answering MIMIC4 ICD-10
Radiopaedia Caption NER
MedlinePlus
HealthFact
Intent Identification MedQA
MedicationQA
MEDIQA-AnS
Explanation BioLORD
PubMedQA
Medical Knowledge Information Extraction
Translation W BIOiki AD So Qc
MASHQA
IgakuQA
Text Completion R Cu OM Ve IDd -B Qe Anch
MedQuAD
MedAlpaca
Text Retrieval
Word Relation Classification CMedQA2
Daily Conversation NamTr ee da t Em ne tin tyt P Rl ea cn on gin ng ition F E LIr B Me Mn Ac Sh uM me mdM arC isQ erA Corpus
HeadQA
MedMCQA
Diagnosis Catalonia Independence Corpus
ADE Corpus V2
MMedBench
ICD-10-CM
PICO
Dialogue
PMC-Patient
Text Summarization EM Ced Di Cca -Tl Q Muestion Pair Dataset
Information Extraction DO
Text Classification MIMIC-IV-Notes
Question Answering
Clinical Outcome Prediction SEER
E M Cx liea ndm icic aa l l T K en xo twledge E N F Nax a ap cm ttl ua e V rn ad ea l r t E Lii fn ao ic ntn ai gt ty ui o aR n ge ec o Ing fn ei rt eio nn ce A L N B C B BiN C C CC ln innE 5 4 2B ia - - GcM I C C e aD Mh hu l-i e es Ts m me rC ia ao s lsr ep .g u C os o vrpus
Academical Papers Treatment Planning S Bp ioe Cc ri ee as- ti8 v0 e0 V
Daily Conversation Sentence Composition Analysis
Dialogue
Intent Identification
Wrong Candidate Generation DDXPlus
Translation
Text Completion
Text Retrieval
Word Relation Classification ChatDoctor
Made at SankDeyMiAaTICg.conmosis LiveQA
Figure 2 | Overview of MedS-Ins. a The task collection pipeline. For each task, we add a task category along with a
hand-written definition to it, resulting in a total of 19 task categories. b We collect the existing 58 public datasets. c We
converttheformatsofdifferentdatasetsintooneunifiedmedicalinstructiondataset,MedS-Ins. dThefinaldatadistributionof
ourcollectedMedS-Ins. TheSankeydiagramshowshowthedifferenttextdomains(left),taskcategories(middle),anddata
sources(right)contributetothefinaldatasets. Ontheleftofthebottom,twopiechartsshowthedatadistributionsontext
domainsandtaskcategoriesrespectively.
|4education, it is important to note that the highly standardized nature of exams often results in over-simplified
cases compared to real-world clinical tasks. 7% of the tokens in our dataset are from the exams.
Clinical texts: Generated during routine clinical practice, these texts support diagnostic, treatment, and
preventive processes within hospitals and clinical centers. This category includes Electronic Health Records
(EHRs), radiology reports, lab results, follow-up instructions, and medication recommendations, among others.
These texts are indispensable for disease diagnosis and patient management, making accurate analysis and
understanding crucial for the effective clinical application of LLMs. 35% of the tokens in our dataset are from
clinical texts. Notably, the significant proportion of clinical texts ensures that the instruction tuning data
aligns closely with clinical demands.
Academic papers: This data is sourced from medical research papers, covering the latest findings and
advancements in the medical research field. Given their accessibility and structured organization, extracting
data from academic papers is relatively straightforward. These cases help models grasp cutting-edge medical
research information, guiding them to better understand contemporary developments in medicine. 13% of the
tokens in our dataset are from academic papers.
Medical knowledge bases: This domain comprises well-organized and comprehensive medical knowledge,
including medical encyclopedias, knowledge graphs, and glossaries of medical terms. Such data forms the
backbone of medical knowledge bases, supporting both medical education and the application of LLMs in
clinical practice. 43% of the tokens in our dataset are from medical knowledge.
Daily conversations: This source refers to the daily consultation generated between doctors and patients,
primarily sourced from online platforms and other interactive scenarios. These interactions reflect the real-life
interactions between medical professionals and patients, playing a critical role in understanding patients’
needs and enhancing the overall experience of medical service. 2% of the tokens in our dataset are from daily
conversations.
Task Categories
Beyond categorizing the text domains from which the original data is sourced, the samples in MedS-Ins
are further organized into distinct task categories. We have identified 19 task categories, each representing a
critical capability that we believe a medical LLM should possess. By constructing this instruction-tuning
dataset and fine-tuning models accordingly, we aim to equip the LLMs with the versatility needed to address
a broad spectrum of medical applications.
These 19 task categories include but are not limited to, the 11 categories in the MedS-Bench benchmark.
The additional categories encompass a range of linguistic and analytical tasks essential for comprehensive
medical language processing, including Intent Identification, Translation, Word Relation Classification, Text
Retrieval, Sentence Composition Analysis, Wrong Candidate Generation, Dialogue, and Text Completion and
the MCQA category is extended to general Question Answering, which also includes free-text answering
cases. The diversity of task categories—ranging from common question answering and dialogue to various
downstream clinical tasks—guarantees a comprehensive understanding of the potential medical applications.
A detailed description of each category is provided in Supplementary Sec. A.2.
2.3 Quantitative Results on Various Tasks
For each task type, we start by discussing the performance of various existing LLMs, followed by a comparison
with our final model, MMedIns-Llama 3. All results presented here were obtained using a 3-shot prompting
strategy (more details in Supplementary Sec. A.1), except for MCQA tasks, where we used a zero-shot
prompting setting to align with previous studies [5, 48, 32]. As our comparisons include proprietary models
like GPT-4 and Claude 3.5, which incur usage costs, we randomly sampled 50-100 test cases per benchmark to
managethecostconstraints. ThetaskdescriptionandspecificsamplingnumbersaredetailedinSupplementary
Sec. A.2. For simplicity, the percentage mark (%) is omitted in all the following tables and results analysis.
|5Multilingual Multiple-choice Question-answering
Here, we present evaluation results on the widely used multiple-choice Question-answering (MCQA) bench-
marks, as shown in Table 1. Some numbers are directly incorporated from our previous studies [32, 48].
On these multi-choice question answering datasets, existing proprietary LLMs have demonstrated very high
accuracies, for example, on MedQA, GPT-4 can achieve 85.8, which is almost comparable to human experts,
and Llama 3 can also pass the exam with 60.9 scores. Similarly, in languages other than English, LLMs also
demonstrate superior results in multiple-choice accuracy on MMedBenh [32]. The results indicate that as
multi-choice questions have been extensively considered in existing research, different LLMs may have been
specifically optimized for such tasks, resulting in high performance. It is therefore essential to build up a
more comprehensive benchmark, to further push the development of LLMs towards clinical applications.
Our proposed model, MMedIns-Llama 3, although not primarily trained on multi-choice questions, still
shows notable improvement, achieving an average accuracy of 63.9 across different benchmarks, significantly
surpassing GPT-3.5.
Table 1 | Resultsonmedicalmultiple-choicequestionanswering,asreportedwithAccuracyscore.
MMedBench
Method MedQA MedMCQA PubMedQA Avg.
ZH JA FR RU ES
Close-sourceModels
GPT-3.5 57.7 72.7 53.8 52.3 34.6 32.5 66.4 66.1 54.5
GPT-4 85.8 72.3 70.0 75.1 72.9 56.6 83.6 85.7 75.3
Open-sourceModels
MEDITRON 47.9 59.2 74.4 61.9 40.2 35.1 67.6 53.3 54.9
InternLM2 - - - 77.6 47.7 41.0 68.4 59.6 -
Mistral 50.8 48.2 75.4 71.1 44.7 48.7 74.2 63.9 49.1
Llama3 60.9 50.7 73.0 78.2 48.2 50.8 71.5 64.2 62.2
MMedIns-Llama3 63.6 57.1 78.2 78.6 54.3 46.0 72.3 61.2 63.9
Text Summarization
As shown by Table 2, the performance of text summarization is reported as ‘BLEU/ROUGE’ scores, on
multiple report types across various modalities, including X-ray, CT, MRI, ultrasound, and other medical
questions. Among the models, closed-source LLMs, such as GPT-4 and Claude-3.5, perform better than
all the open-source ones, achieving an average of 24.46/25.66 and 26.29/27.36, respectively. In open-source
models, Mistral 3 demonstrates the best results, with 24.48/24.90 in BLEU/ROUGE respectively and Llama
3 follows, with 22.20/23.08.
Our model (MMedIns-Llama 3), trained on the medical-specific instruction dataset (MedS-Ins), significantly
outperforms the others, including the top closed-source model, GPT-4 or Claude-3.5, achieving average scores
of 46.82/48.38.
Information Extraction
The performance of information extraction is summarized in Table 3. InternLM 2 shows exceptionally good
performance in this task with an average score of 81.58. For example, In the PICO tasks, InternLM 2 leads
in both Intervention and Outcome Extraction, with scores of 79.07 and 76.74, respectively. Closed-source
models such as GPT-4 and Claude-3.5 outperform all other open-source counterparts, with average scores of
77.49 and 78.86, respectively. Analysis of individual benchmark components reveals that most LLMs perform
better at extracting less medically complex information, such as basic patient details, compared to more
specialized medical data like outcomes and interventions. For instance, in extracting basic information from
PMC Patients, most LLMs score above 90, with Claude-3.5 achieving the highest score of 98.02. In contrast,
performance on Clinical Outcome Extraction tasks within PICO is relatively poor.
Our proposed model, MMedIns-Llama 3, demonstrates the best overall performance, achieving an average
|6Table 2 | Resultsontextsummarization,asreportedwithBLEUandROUGEscores,formattedas‘BLEU/ROUGE’.“Con.”
denotesconclusions.
MedQSum RCT-Text MIMIC-CXR MIMIC-IV
Method Avg.
MedQuestion StudyCon. X-ray Ultrasound CT MRI
Close-sourceModels
GPT-4 25.06/27.30 34.32/31.09 27.26/29.71 11.17/14.53 23.97/29.52 25.76/32.06 24.46/25.66
Claude-3.5 21.14/25.06 41.02/36.16 27.76/29.93 15.24/18.28 21.98/26.38 26.43/31.05 26.29/27.36
Open-sourceModels
MEDITRON 15.64/23.14 4.00/16.44 5.21/16.50 3.75/6.07 16.30/23.93 20.11/27.98 7.15/15.54
InternLM2 15.69/21.63 14.48/15.16 11.83/13.41 13.48/20.96 20.88/27.82 23.43/31.40 13.87/17.79
Mistral 23.49/26.03 27.24/26.13 22.09/24.71 25.09/22.72 27.60/30.77 29.87/31.81 24.48/24.90
Llama3 22.45/25.08 15.38/14.60 32.92/32.64 18.06/20.00 24.47/29.35 24.82/30.50 22.20/23.08
MMedIns-Llama3 54.16/56.95 57.82/55.60 54.91/57.64 20.40/23.32 42.18/46.46 40.53/43.38 46.82/48.38
score of 83.18, surpassing InternLM 2 by 1.6 points. Notably, in the PICO tasks, MMedIns-Llama 3 excels in
Participant Extraction, scoring 83.72, which exceeds the second-best model by 9.3 points.
Table 3 | Resultsoninformationextraction,asreportedwithAccuracyscore. “Ext.” denotesextractionand“Info.” denotes
information.
PICO ADE PMCpatient
Method Avg.
ParticipantExt. InterventionExt. OutcomeExt. DrugDoseExt. BasicInfo. Ext.
Close-sourceModels
GPT-4 72.73 72.09 67.44 79.17 96 77.49
Claude-3.5 67.44 74.42 67.44 86.96 98 78.86
Open-sourceModels
MEDITRON 62.79 53.49 60.47 73.91 68 63.73
InternLM2 74.42 79.07 76.74 95.65 82 81.58
Llama3 65.12 74.42 60.47 91.3 94 77.06
Mistral 62.79 62.79 58.14 91.3 90 73.01
MMedIns-Llama3 83.72 76.74 62.79 95.65 97 83.18
Concept Explanation
We conduct evaluations on medical concept explanation and reported the BLEU-1 and ROUGE-1 scores
across all relevant datasets and models.
In Table 4, we evaluate the model on medical concept explanation, GPT-4, Llama 3, and Mistral perform well
on this task, achieving an average of 19.18/21.33, 21.14/23.33, and 19.18/21.33 respectively. On the contrary,
the scores of Claude-3.5, InternLM 2, and MEDITRON are relatively lower, with averages of 14.78/19.29,
13.56/18.93,and11.21/22.66,respectively. WeconjecturethattherelativelypoorperformanceofMEDITRON
can be attributed to its training corpus, which is more focused on academic papers and guidelines, and thus
falls short in basic medical concept explanation.
Our final model, MMedIns-Llama 3, significantly outperforms the other ones across all concept explanation
tasks,particularlyinHealthFactExplanation(35.56/32.32)andBioLORDConceptExplanation(32.36/40.39),
and achieving the highest average scores of 33.89/37.68. Following MMedIns-Llama 3, Llama 3 and GPT-4
also showed strong performance, with Llama 3 scoring 21.14/23.33 and GPT-4 closely behind at 19.18/21.33.
Answer Explanations (Rationale)
In Table 5, we evaluate the complex rationale, i.e., explaining the answer, and comparing the reasoning
abilitiesofvariousmodelsusingtheMMedBench[32]datasetacrosssixlanguages. Amongthemodelstested,
the closed-source model Claude-3.5 exhibited the strongest performance, with average scores of 46.03/37.65,
demonstrating consistently high scores across all languages, particularly in French and Spanish. This superior
|7Table 4 | Resultsonmedicalconceptexplanation,asreportedwith‘BLEU/ROUGE’scores. “Exp.” denotesExplanation
Method HealthFactExp. DoEntityExp. BioLORDConceptExp. Avg.
Close-sourceModels
GPT-4 18.27/20.45 20.69/22.85 18.59/20.69 19.18/21.33
Claude-3.5 14.55/19.61 11.74/17.08 18.05/21.19 14.78/19.29
Open-sourceModels
MEDITRON 12.48/18.83 8.17/25.89 12.96/23.27 11.21/22.66
InternLM2 22.40/27.18 6.28/12.69 12.01/16.91 13.56/18.93
Llama3 20.47/21.55 20.94/24.13 22.01/24.33 21.14/23.33
Mistral 17.81/24.00 17.34/21.66 22.18/22.79 19.11/22.82
MMedIns-Llama3 35.56/32.32 33.74/40.34 32.36/40.39 33.89/37.68
performance may be attributed to the similarity of this task to chain-of-thought reasoning, a capability that
has been specifically enhanced in many general-purpose LLMs. Among open-source models, Mistral and
InternLM 2 showed comparable performance, with average scores of 37.61/31.55 and 30.03/26.44, respectively.
It is important to note that GPT-4 was excluded from this evaluation, because the rationale component of
the MMedBench dataset was primarily constructed using GPT-4 outputs, which could introduce bias and
bring unfair comparisons.
Consistent with our observations in concept explanation, our final model, MMedIns-Llama 3, demonstrated
the best overall performance with average BLEU-1/ROUGE-1 scores of 47.17/34.96 across all languages,
notably, achieving 53.31/36.83 in Japanese reasoning tasks, 48.54/38.22 in English, and 47.64/39.24 in French,
respectively. This superior performance is likely due to the fact that our base language model (MMed-Llama
3) was initially developed to be multilingual [32]. Consequently, even though our instruction tuning did not
explicitly target multilingual data, the final model outperforms others across multiple languages.
Table 5 | Resultsonrationale,asreportedwith‘BLEU/ROUGE’scores. Notehere,wedonotincludetheresultsforGPT-4
sincetheoriginalevaluationsetsweregeneratedwithit,thatmaybringunfaircomparisonbias.
MMedBench
Method Avg.
Chinese English French Japanese Russian Spanish
Close-sourceModels
Claude-3.5 45.77/35.79 46.37/39.57 49.28/41.04 46.65/40.56 38.49/28.50 49.64/40.45 46.03/37.65
Open-sourceModels
MEDITRON 20.18/20.76 39.50/30.88 35.01/29.36 21.41/26.46 26.56/20.99 37.49/30.17 30.03/26.44
InternLM2 36.66/30.52 44.62/39.36 36.50/34.21 29.10/34.77 27.97/23.24 41.97/36.13 36.14/33.04
Llama3 33.71/27.83 44.86/38.75 23.68/22.22 9.44/12.92 31.92/24.69 32.33/28.04 29.32/25.74
Mistral 34.98/28.34 47.29/37.06 38.48/35.04 29.08/27.50 32.10/24.94 43.74/36.39 37.61/31.55
MMedIns-Llama3 51.50/35.35 48.54/38.22 47.64/39.24 53.31/36.83 34.91/23.71 47.08/36.42 47.17/34.96
Named Entity Recognition (NER)
As shown in Table 6, among the six existing models, GPT-4 is the only one that consistently demonstrates
robust performance across Named Entity Recognition (NER) tasks, achieving an average F1-Score of 44.30. It
excelsparticularlyintheBC5ChemChemicalRecognitiontaskwithascoresof63.77. InternLM2alsodelivers
competitive results, with an average F1-Score of 40.81, showing strong performance in both the BC5Chem
and BC5Disease tasks. Llama 3 and Mistral, with average F1-Scores of 24.70 and 20.10, respectively, exhibit
moderate performance. MEDITRON, not optimized for NER tasks, shows limited effectiveness in this area.
Ourmodel,MMedIns-Llama 3,significantlyoutperformsallothermodels,achievingananaverageF1-Score
of 68.58. It excels in the BC4Chem and BC5Chem Chemical Recognition tasks, with F1 scores of 78.62 and
89.71, respectively. Furthermore, MMedIns-Llama 3 leads in the BC5Disease Disease Recognition task with
an F1-Score of 51.97 and in the Species800 Organism Recognition task with 54.02, demonstrating superior
capability in handling complex NER tasks across various biomedical domains.
|8Table 6 | ResultsonNERtasks,asreportedwithF1-Scorescores. ‘Rec.’ isshortfor‘recognition’.
BC4Chem BC5Chem BC5Disease Species800
Method Avg.
ChemicalRec. DiseaseRec. OrganismRec.
Close-sourceModels
GPT-4 47.41 63.77 35.77 30.26 44.30
Claude-3.5 30.77 48.61 27.14 9.88 29.10
Open-sourceModels
MEDITRON 0.83 6.53 2.45 0.80 2.65
InternLM2 32.79 50.39 42.02 38.03 40.81
Llama3 24.62 42.55 24.00 7.64 24.70
Mistral 21.48 41.48 13.25 4.21 20.10
MMMedIns-Llama3 78.62 89.71 51.97 54.02 68.58
Diagnosis, Treatment Planning, and Clinical Outcome Prediction
Weevaluatetheperformanceontasksinvolvingdiagnosis,treatmentplanning,andclinicaloutcomeprediction,
using the DDXPlus benchmark for Diagnosis, the SEER benchmark for Treatment Planning, and the
MIMIC4ED benchmark for Clinical Outcome Prediction. The results, presented in Table 7, are measured in
terms of accuracy. Here, the use of accuracy as a metric is appropriate in this generative prediction problem,
as each of these datasets simplifies the original problem into a closed-set choice. Specifically, DDXPlus utilizes
a predefined list of diseases, from which models must select one based on the provided patient context. In
SEER, treatment recommendations are categorized into eight high-level categories, while in MIMIC4ED, the
final clinical outcome decisions are binary, with options of either True or False.
Overall, the open-source LLMs underperform the closed-source counterparts in these tasks, and in some
instances, they fail to provide meaningful predictions. For example, Llama 3 struggles with predicting critical
triage and the 72-hour ED revisit binary indicator. In the SEER treatment planning task, InternLM 2 and
MEDITRON achieve moderate accuracy scores of 68 and 60, respectively. For the DDXPlus diagnosis task,
InternLM 2 and Llama 3 perform slightly better, with an accuracy of 32. However, closed-source models such
as GPT-4 and Claude-3.5 demonstrate significantly better performance. Claude-3.5, for instance, achieves a
90 accuracy score in treatment planning, while GPT-4 attains 52 in diagnosis, highlighting the considerable
gap between open-source and closed-source LLMs. Despite these results, the scores remain insufficient for
reliable clinical use.
In contrast, MMedIns-Llama 3 demonstrate superior accuracy in clinical decision support tasks, with a 98
accuracy score in treatment planning, 95 in diagnosis, and an average accuracy of 86.67 (mean on the scores
of Hospitalization, 72h ED Revisit, and Critical Triage) in clinical outcome prediction.
Table 7 | Resultsontreatmentplanning,diagnosisclinicaloutcomeprediction,andtextclassificationresults. Thefirst3
tasksarereportedwithAccuracyscore,andtextclassificationisreportedwithPrecision,Recall,andF1score.
MIMIC4ED HoCClassification
Method SEER DDXPlus
Hospitalization 72hEDRevisit CriticalTriage Precision Recall F1
Close-sourceModels
GPT-4 78 52 62 69 66 55.07 72.37 60.38
Claude-3.5 90 48 64 63 57 55.18 80.96 63.32
Open-sourceModels
MEDITRON 60 24 36 4 11 23.39 36.85 26.21
InternLM2 68 32 60 76 69 22.97 83.16 32.72
Llama3 52 32 51 4 3 26.6 69.59 36.18
Mistral 36 28 58 39 43 35.18 59.27 40.80
MMedIns-Llama3 98 95 69 97 94 91.29 85.57 87.37
|9Text Classification
In Table 7, we present the evaluation on the Hallmarks of Cancer (HoC) multi-label classification task, and
report macro-Precision, macro-Recall, and macro-F1 scores. For this task, all candidate labels are input into
the language model as a list, and the model is asked to select its preferred answers, allowing for multiple
selections. The metrics are then calculated based on these model selections.
GPT-4 and Claude-3.5 perform well on this task, with GPT-4 achieving a macro-F1 score of 60.38 and
Claude-3.5slightlybetterat63.32. Bothmodelsshowstrongrecallcapabilities, particularlyClaude-3.5, which
achieves a macro-Recall of 80.96, underscoring its proficiency in identifying relevant labels. Mistral shows
moderate performance with a macro-F1 score of 40.8, maintaining a balance between precision and recall. In
contrast, Llama 3 and InternLM 2 display lower overall performance, with macro-F1 scores of 36.18 and 32.72,
respectively. These models, particularly InternLM 2, demonstrate high recall but struggle with precision,
resulting in lower F1 scores. MEDITRON ranks lowest in this task, with a macro-F1 score of 26.21.
MMedIns-Llama 3clearlyoutperformsallothermodels,achievingthehighestscoresacrossallmetrics,with
a macro-Precision of 91.29, a macro-Recall of 85.57, and a macro-F1 score of 87.37. These results highlight
MMedIns-Llama 3’s superior ability to accurately classify and recall multiple labels, making it the most
effective model for this complex task.
Fact Verification
In Table 8, we evaluate the models on fact verification tasks. For PubMedQA Answer Verification and
HealthFact Verification, the LLMs are required to select a single answer from a list of provided candidates,
with accuracy serving as the evaluation metric. In contrast, for EBMS Justification Verification, where the
task involves generating free-form text, performance is assessed using BLEU and ROUGE scores.
InternLM2achievesthehighestaccuracyonPubMedQAAnswerVerificationandHealthFactVerification,with
scores of 98 and 92, respectively. In the EBMS benchmark, GPT-4 demonstrates the strongest performance,
with BLEU/ROUGE scores of 16.36/16.33. Claude-3.5 follows closely with scores of 14.22/15.82, though it
performs poorly on PubMedQA Answer Verification. Llama 3 achieves accuracy of 94 and 64 on PubMedQA
and HealthFact Verification, respectively, and a BLEU/ROUGE score of 12.96/14.37. Mistral, while achieving
a higher BLEU/ROUGE score of 15.19/15.56, has relatively lower accuracy of 54 and 68. MEDITRON ranks
lowest across all metrics, with accuracy of 26 and 40 and BLEU/ROUGE scores of 14.08/25.69.
MMedIns-Llama 3 continues surpassing existing models, achieving the highest accuracy score as InternLM
2, excelling in PubMedQA Answer Verification and HealthFact Verification while in EMBS, MMedIns-Llama
3 slightly falls behind the GPT-4 with 11.99/12.90 in BLEU and ROUGE, which we treat as future work for
further improvement.
Table8|ResultsonfactverificationandNLIresults,asreportedwithbothaccuracyandBLUE/ROUGEscores. ‘Ver.’ denotes
‘verification’.
PubMedQA PUBLICHEALTH EMBS MedNLItextualentailment
Method
AnswerVer. HealthFactVer. JustificationVer. DiscriminativeTask GenerativeTask
Close-sourceModels
GPT-4 52 76 16.36/16.33 82 27.09/23.71
Claude-3.5 4 64 14.22/15.82 86 17.73/16.70
Open-sourceModels
MEDITRON 26 40 10.86/13.73 66 3.92/14.27
InternLM2 98 92 8.54/15.10 84 25.42/24.91
Llama3 94 64 12.96/14.37 62 33.69/27.28
Mistral 54 68 15.19/15.56 72 20.81/19.03
MMedIns-Llama3 98 92 11.99/12.90 84 37.56/32.1
|10Natural Language Inference (NLI)
Table 8 shows the evaluation on medical Natural Language Inference (NLI) using the MedNLI textual
entailment dataset. The results are measured with accuracy for the discriminative tasks (selecting the right
answer from a list of candidates) and BLEU/ROUGE metrics for the generative tasks (generating free-form
text answers).
InternLM 2 achieves the highest scores among the open-source LLMs, scoring 84. For the closed-source LLMs,
GPT-4 and Claude-3.5 all show relatively high scores, with 82 and 86 accuracy scores respectively. In the
generative task, Llama 3 shows the highest consistency with the referenced ground truth with 33.69/27.28 for
BLEU and ROUGE. Mistral and Llama 3 exhibit mid-range performance, with Mistral achieving 72 accuracy
and BLEU/ROUGE scores of 20.81/19.03. GPT-4 follows it, resulting in 27.09/23.71 scores while Claude-3.5
is not ideal in the generative task.
MMedIns-Llama 3 achieves the highest accuracy in the discriminative task, scoring 84, while slightly
weaker than Claude-3.5. MMedIns-Llama 3 also excels in the generative task, with BLEU/ROUGE scores of
37.56/32.17, significantly outperforming other models in generating accurate and relevant entailments.
3 Discussion
Overall, this paper makes several key contributions:
A Comprehensive Evaluation Benchmark – MedS-Bench
The development of medical LLMs has largely relied on benchmarks focused on multiple-choice question
answering (MCQA). However, this narrow evaluation framework risks overlooking the broader capabilities
required for LLMs in various clinical scenarios. In this work, we introduce MedS-Bench, a comprehensive
benchmark designed to assess the performance of both closed-source and open-source LLMs across diverse
clinical tasks, including those that require fact recall from the model or reasoning from given context. Our
results reveal that while existing LLMs perform exceptionally well on MCQA benchmarks, they struggle to
align with the actual clinical practice, particularly in tasks such as treatment planning and explanation. This
finding underscores the need for further efforts to develop medical LLMs that are better suited to a wider
range of clinical and medical scenarios beyond MCQA.
A New Comprehensive Instruction Tuning Dataset – MedS-Ins
We have developed MedS-Ins, a novel medical instruction tuning dataset, by extensively sourcing data
from existing BioNLP datasets and converting these samples into a unified format, with semi-automated
prompting strategies. Previous efforts have focused primarily on constructing question-answer pairs from daily
conversations, exams, or academic papers, often neglecting the texts generated from real clinical practice.
In contrast, MedS-Ins integrates a broader range of medical text sources, encompassing five primary text
domains and 19 task categories, as illustrated in Figure 2d. This systematic analysis on data composition is
crucial for aligning LLMs with the diverse queries encountered in clinical practice.
A Strong Large Language Model for Medicine – MMedIns-Llama 3
On the model front, we demonstrate that by conducting instruction tuning on MedS-Ins, we can significantly
enhance the alignment of open-source medical LLMs with clinical demands. Our final model, MMedIns-
Llama 3, serves as a proof-of-concept, featuring a medium-scale architecture with 8 billion parameters, has
exhibited a deep understanding of various clinical tasks and adapts flexibly to multiple medical scenarios
through zero-shot or few-shot instruction prompts, without the need for further task-specific training. As
evidenced by the results, our model outperforms existing LLMs, including GPT-4, Claude-3.5, across a range
of medical benchmarks, covering different text sources.
Limitations. Here, we highlight the limitations of our paper and the potential improvements in future work.
First, MedS-Bench currently covers only 11 clinical tasks, which does not fully encompass the complexity of
all clinical scenarios. Additionally, while we evaluated six mainstream LLMs, some models remain absent from
|11our analysis. To address these limitations, we plan to release an open leaderboard for medical LLMs alongside
this paper. This initiative aims to encourage contributions from the community to continually expand and
refine comprehensive benchmarks for medical LLMs. Specifically, this will involve updating the test set to
better reflect real clinical demands and including a broader range of medical LLMs. By incorporating more
task categories from diverse text sources into the evaluation process, we hope to gain a deeper understanding
of the ongoing advancements in LLMs within the medical field.
Second, although MedS-Ins now encompasses the widest range of medical tasks available, it remains
incomplete, and certain practical medical scenarios may be missing. To address this, we have made all our
collecteddataandresourcesavailableasopen-sourceonGitHub. Weencouragecontributionsfromthebroader
AI4medicine community to help maintain and dynamically expand this instruction tuning dataset, similar
to efforts for Super-NaturalInstructions in the general domain[47]. Detailed guidelines are provided on our
GitHub page, and we will acknowledge every contributor involved in updating the dataset. The current
limited number of tasks may explain why we have not yet observed the models exhibiting emergent abilities
to generalize to unseen clinical tasks, a capability seen in LLMs trained on thousands of diverse tasks in the
general domain [46, 25].
Third, we plan to incorporate more languages into MedS-Bench and MedS-Ins to support the development
of more robust multilingual LLMs for medicine. For now, although we include some multilingual tasks in
both MedS-Bench and MedS-Ins, these resources are predominantly English-centered. Expanding to include a
broader range of languages would be a promising future direction, ensuring that the latest advancements in
healthcare AI can benefit a wider and more diverse range of regions equitably.
Finally, all our code, data, and evaluation pipelines are open-sourced. We hope this work would inspire the
medical LLM community to focus more on aligning these models with real-world clinical applications.
|124 Methods
4.1 Data Collection
In this section, we describe the procedure for constructing MedS-Ins, as shown in Figure 3a. In order to
organize the different tasks, we assign a domain tag and category tag to each task, the former denotes the
domain covered by the instructions, while the category tag denotes the applicable task. We start by filtering
the medical-related sentence in natural instruction datasets, followed by prompting specific BioNLP into
free-text response formats.
Filtering Natural Instructions
We start by filtering medical-related tasks from the 1,616 tasks collected in Super-NaturalInstructions [47].
As this work focuses more on different natural language processing tasks in general-purpose domains, the
granularity of classification is relatively coarse for the medical domain. We first extract all the instructions in
“Healthcare” and “Medicine” categories, subsequently, we manually added more detailed granularity to the
domain labels for them, while the task category remains unchanged.
In addition, we found that many of the organized instruction fine-tuning datasets in the generic domain also
cover some healthcare-related data, such as LIMA [52] and ShareGPT [23]. To filter out the medical part of
these data, we used InsTag [26] to classify the domain of each instruction at a coarse granularity. Specifically,
InsTag is an LLM, specialized for tagging different instruction samples. Given an instruction query, it will
analyze which domain and task it belongs to. Finally, by filtering instruction datasets in the general domain,
we collect 37 tasks, for a total of 75373 samples.
Prompting Existing BioNLP Datasets
In the literature, there exists many excellent datasets on text analysis in clinical scenarios. However, as most
datasets are collected for different purposes, like classification or text completion, they can not be directly
used for training large language models. Here, we convert these existing former medical NLP tasks into a
format that can be used for training generative models, naturally adding them into instruction tuning.
Specifically, we use MIMIC-IV-Note as an example, which provides high-quality structured reports with both
findings and impressions, they are used as a proxy task for text summarization, where impressions act as an
abstract summary of the findings. We first manually write prompts to define the task, for example, “Given
the detailed finding of Ultrasound imaging diagnostics, summarize the note’s conclusion in a few words.”.
Considering the diversity for instruction tuning, we ask 5 individuals to independently describe a certain
task with 3 different prompts. This results in 15 free-text prompts for each task, with similar semantic
meanings but as varied as possible in wording and format. Then, inspired by the Self-Instruct [45], we use
these manually written instructions as seed prompts and asked GPT-4 [18] to rewrite more task instructions
based on the following prompt:
Rewrite the following instruction definition directly. You can change the wording, but keep the meaning
the same. Output the rewritten definition directly without any additional information.
Finally, for each task, we will describe it with 7 key elements as shown at the bottom of Figure 3a, i.e.,
{“Categories”, “Domains” “Definitions”, “Input Language”, “Output Language”, “Instruction Language” and
“Instances”},where“Definition” consistsofthemanuallywrittenorGPT-4enhancedinstructiontodescribethe
tasks, “Input Language”, “Output Language”, and “Instruction Language” respectively describe the languages,
suchasEnglishorChinese,usedinthecorrespondingcomponentsofaspecificinstanceofthistask. “Categories”
and“Domains” describewhattextdomainsandcategoriesthetaskbelongsto. Finally, in“Instances”, different
training or evaluation instances with Input and Output contents are stored.
Through the above procedure, we prompt an extra 85 tasks into a unified free-form question-answering format,
combined with the filtered data, resulting in a totaling 13.5 million high-quality samples, covering 122 tasks,
termed as MedS-Ins (the detailed 122 task information can be found in Supplementary Sec. A.3), which has
shown to significantly improve the LLMs on clinical tasks.
|13a. b.
Medical Comprehensive
Natural Instructions Various BioNLPDatasets
Alignment Dataset
MedS-Ins
Human Prompting Seeds
Instruction Tuning
Medical Related Tagging
GPT Augmentation Zero-shot Few-shot
Ins: Assuming you are Ins:
MedS-Ins a doctor, I hope you Case1: …Case2: …
perform a diagnosis Please follow the
A comprehensive medical instruction tuning dataset containing 13.5Msamples, covering 5 task based on … given case …
domains, 19 categories, and 122tasks.
Task Category Distribution
A Task Structure 14 Medical LLM regA ru esto s- ive
Example 12 Loss
10
Categories: … 8
Domains: … 6
4 Evaluation
Definitions: … 2
Input Language: … 0
• Multiple Choice Question Answering;
Output Language: …
• Text Summarization;
Definition Language: …
Instances: … • Explanation;
• … (11Task Categories, 39Datasets)
Figure 3 | This pipeline of our method. aThedatacollectionpipeline. Wemainlycollectdatathroughfilteringnatural
instructionsandpromptingwell-organizedBioNLPdatasets. bThetrainingandevaluationpipelineforourmodelleveragingthe
collectedMedS-Ins. Weleveragetheinstructiontuningtrainingmethodtocombinedifferentdatasetsandevaluatethefinal
modelonmultiplebenchmarkscomprehensively.
4.2 Model Training
In this section, we detail the training procedure, as shown in Figure 3b. We take the same approach as our
previous work [48, 32], which have shown that further auto-regressive training on medical-elated corpus can
inject medical knowledge into the models, thus allowing them to perform better in different downstream tasks.
We start from a multilingual LLMs base model (MMed-Llama 3 [32]), and further train it with comprehensive
instructions from MedS-Ins.
Instruction Tuning
Given the base model, trained on a large-scale medical corpus with auto-regressive prediction, we further
fine-tuneittobetterfollowhumaninstructionsorprompts. Consideringaninputsequencewithaninstruction
I and a context C, and an output sequence O, the model is trained to maximize the probability:
|O|
(cid:89)
P(O|C,I)= P(o |o ,o ,...,o ,C,I;θ) (1)
t 1 2 t−1
t=1
Similarly, the loss function used in instruction tuning is cross-entropy loss and can be calculated as follows:
|O|
(cid:88)
Loss=− logP(o |o ,o ,...,o ,C,I;θ) (2)
t 1 2 t−1
t=1
Thekeyinsighthereistoconstructdiverseinstructions,thatenablesthemodeltorobustlyoutputthepreferred
answers. Here, we mainly consider two types of instructions, namely, zero-shot and few-shot prompting.
Zero-shot Prompting. Here, the I contains some semantic task descriptions as hints, and the model is
therefore asked to directly answer the questions based on its internal model knowledge. In our collected
MedS-Ins, the “Definition” contents for each task can be naturally used as the zero-shot instruction input.
Due to the coverage of a wide range of different medical task definitions, the model is expected to learn the
|14
000001
xsemantic understanding of various task descriptions. The input template is as follows:
{INSTRUCTION}
Input: {INPUT}
Few-shot Prompting. Here, the I contains the few-shot examples, that allow the model to learn the
input-output mapping on the fly. We simply obtain such instructions by randomly sampling other cases from
the training set of the same task, and organizing them using a straightforward template as follows::
Case1: Input: {CASE1_INPUT}, Output: {CASE1_OUTPUT}
...
CaseN: Input: {CASEN_INPUT}, Output: {CASEN_OUTPUT}
{INSTRUCTION}
Please learn from the few-shot cases to see what content you have to output.
Input: {INPUT}
Implementation Details
Weconductallourexperimentsusing PyTorchframeworkandTransformerspythonpackage. Specifically,we
setthemaximumlengthto2048, andpadthesequencetothelongestcasewithpaddingtokensinabatch. We
employ the Fully Sharded Data Parallel (FSDP) implemented with Transformers.trainer function to save
the memory cost per GPU. We also adopt BF16 as default training precision and gradient checkpointing [4]
techniques to optimize memory usage. We use a global batch size of 128 and a learning rate of 1e-5. We
choose the medical-knowledge-enhanced model MMed-Llama 3 in our previous work as the foundation model.
We further train the model by supervised fine-tuning on Meds-Ins for 5 Epoch with 32 Ascend910B for 383.5
hours.
4.3 Baselines
Here, we provide details for the baseline large language models (LLMs). Note that, we evaluate all models in
few-shotsettings,asweobservethatopen-sourcemodelsstruggletocompletezero-shotevaluation. Specifically,
three example cases are given to the model, the detailed prompting strategy can be found in Supplementary
Sec. A.1.
The first category includes the powerful closed-source LLMs, known for their robust performance in the
general domain. We evaluate these models on various medical-specific tasks:
• GPT-4[18],developedbyOpenAI,standsforoneofthemostsophisticatedLLMstodate. Itisrenowned
for its strong capabilities in language processing in general domains, including medical applications.
• Claude-3.5 [40], developed by Anthropic, is a frontier AI language model designed to be secure,
trustworthy, and reliable. It exhibits advanced reasoning capabilities that enable it to perform complex
cognitive tasks effectively. We adopt the Claude-3.5-Sonnet for comparison, which is claimed as the best
model among the Claude family.
The second category comprises the mainstream open-source LLMs:
• Llama 3 [41], developed by Meta AI, is one of the most notable open-source LLMs globally. As part
of the LLaMA series, it is designed for high performance in natural language processing tasks, with
enhancements over its predecessors in accuracy and contextual understanding. Since our trained model
MMedIns-Llama 3 is based on the 8B version, for a fair comparison, we also mainly consider the 8B
version for Llama 3.
• Mistral [12], developed by Mistral AI, is an innovative open-source LLM that claims superiority over
Llama 2 13B across all evaluated benchmarks. Similarly, for a fair comparison, we also consider its 7B
version.
|15• Internlm 2 [3], developed by Shanghai AI Laboratory, stands out as a leading open-source multilingual
LLM, showcasing exceptional performance, particularly in English and Chinese. We adopt the 7B
version of Internlm 2.
The third category of models we choose is the open-sourced medical LLMs, which have been further trained
on medical data.
• MEDITRON [5] is a large-scale medical LLM with 70B parameters, further pre-trained on Llama
2. It leverages 21.1M medical papers, guidelines for further pre-training, and supervised finetuning on
different MCQA datasets with context and chain-of-thought prompt styles.
4.4 Metrics
In this section, we delineate the metrics employed across various tasks and categories within our benchmark.
Accuracy: For tasks requiring the model to select a single correct answer from multiple choices, we employ
‘accuracy’ as a direct metric. This metric is applied to tasks MedQA, MedMCQA, and MMedBench in
Multilingual Multiple-choice Question-answering; participant, intervention, and outcome extraction in PICO,
drug dose extraction in ADE, and patient information extraction in PMC-patient for Information Extraction.
It is also used in SEER for Treatment Planning, DDXPlus for Diagnosis, MIMIC4ED for Clinical Outcome
Prediction, PubMedQA and PUBLICHEALTH Verification for Fact Verification, as well as MedNLI textual
entailment discriminative tasks for NLI.
Precision, Recall, F1 Score: For tasks where the model is required to select multiple correct answers,
we utilize Precision, Recall, and the F1 Score. These metrics are relevant forBC4Chem and BC5Chem for
chemical recognition, BC5Disease for disease recognition, Species800 for organism recognition inNamedEntity
Recognition (NER), and HoC in Classification.
BLEU, ROUGE: For tasks necessitating the generation of free-form text, which are inherently more
challenging to evaluate, we utilize BLEU and ROUGE metrics to assess the similarity between the generated
text and the ground truth. Specifically, we use BLEU-1 and ROUGE-1 by default if no other statements in
this paper. These tasks include MedQSum, RCT-Text, MIMIC-CXR, MIMIC-IV for Text Summarization;
EBMS for Fact Verification; PUBLICHEALTH Explanation, Do, BioLORD and MMedBench for Concept
Explanation / Rationale; along with generative tasks in textual entailment in MedNLI for NLI.
5 Conclusion
In this paper, we construct a new medical evaluation benchmark, MedS-Bench, which contains 11 clinical
tasks beyond multiple choice problems, and thus aims to provide a more comprehensive assessment for clinical
scenarios. We evaluate 6 mainstream LLMs, showing that existing models are fragile in complex real medical
usage. Moreover, to alleviate the gap, we systematically collect a wide range of existing biomedical NLP
datasets, across 5 text domains and with 122 tasks, resulting in MedS-Ins. We conduct further instruction
tuning on the multilingual base model from our previous work, resulting in a new medical LLM, namely
MMedIns-Llama 3. Our final model demonstrates superior performance across almost all tasks on the
benchmark. To further advocate the development of LLMs towards clinical tasks, we have fully opened our
instruction tuning set and encourage more researchers to participate in building up more comprehensive
medical instruction data together. Moreover, we have built up a leaderboard for MedS-Bench, that will
dynamically update and expand the test set to monitor progress, for better adapting general LLMs towards
the medical domain.
|16References
[1] Simon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan Högberg, Ulla Stenius, and Anna Korhonen. Au-
tomaticsemanticclassificationofscientificliteratureaccordingtothehallmarksofcancer. Bioinformatics,
32(3):432–440, 2016.
[2] PavelBlinov,ArinaReshetnikova,AleksandrNesterov,GalinaZubkova,andVladimirKokh. Rumedbench:
A russian medical language understanding benchmark, 2022.
[3] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi
Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024.
[4] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory
cost. ArXiv, abs/1604.06174, 2016.
[5] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco
Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. Meditron-70b:
Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079, 2023.
[6] Shizhe Diao, Rui Pan, Hanze Dong, Ka Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang.
Lmflow: An extensible toolkit for finetuning and inference of large foundation models. arXiv preprint
arXiv:2306.12420, 2023.
[7] Snigdha Dubey, Gaurav Tiwari, Sneha Singh, Saveli Goldberg, and Eugene Pinsky. Using machine
learning for healthcare treatment planning. Frontiers in Artificial Intelligence, 6:1124182, 2023.
[8] ArseneFansiTchango,RishabGoel,ZhiWen,JulienMartel,andJoumanaGhosn.Ddxplus: Anewdataset
for automatic medical diagnosis. Advances in neural information processing systems, 35:31306–31318,
2022.
[9] Scott L Fleming, Alejandro Lozano, William J Haberkorn, Jenelle A Jindal, Eduardo Reis, Rahul
Thapa, Louis Blankemeier, Julian Z Genkins, Ethan Steinberg, Ashwin Nayak, et al. Medalign: A
clinician-generated dataset for instruction following with electronic medical records. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 38, pages 22021–22030, 2024.
[10] Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-Apitius,
andLucaToldo. Developmentofabenchmarkcorpustosupporttheautomaticextractionofdrug-related
adverse effects from medical case reports. Journal of biomedical informatics, 45(5):885–892, 2012.
[11] PaulHager, FriederikeJungmann, RobbieHolland, KunalBhagat, IngaHubrecht, ManuelKnauer, Jakob
Vielhauer, Marcus Makowski, Rickmer Braren, Georgios Kaissis, et al. Evaluation and mitigation of the
limitations of large language models in clinical decision-making. Nature Medicine, pages 1–10, 2024.
[12] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.
arXiv preprint arXiv:2310.06825, 2023.
[13] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease
does this patient have? a large-scale open domain question answering dataset from medical exams.
Applied Sciences, 11(14):6421, 2021.
[14] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: A dataset for
biomedical research question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan,
editors,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the
9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567–2577,
Hong Kong, China, Nov. 2019. Association for Computational Linguistics.
[15] Tian Jin, Nolan Clement, Xin Dong, Vaishnavh Nagarajan, Michael Carbin, Jonathan Ragan-Kelley, and
Gintare Karolina Dziugaite. The cost of down-scaling language models: Fact recall deteriorates before
in-context learning. arXiv preprint arXiv:2310.04680, 2023.
[16] Alistair Johnson, Matt Lungren, Yifan Peng, Zhiyong Lu, Roger Mark, Seth Berkowitz, and Steven
Horng. Mimic-cxr-jpg-chest radiographs with structured labels. PhysioNet, 2019.
[17] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom J
Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. Mimic-iv, a freely accessible electronic health
record dataset. Scientific data, 10(1):1, 2023.
|17[18] Jungo Kasai, Yuhei Kasai, Keisuke Sakaguchi, Yutaro Yamada, and Dragomir Radev. Evaluating GPT-4
and ChatGPT on Japanese medical licensing examinations, 2023.
[19] NeemaKotonyaandFrancescaToni. Explainableautomatedfact-checkingforpublichealthclaims. arXiv
preprint arXiv:2010.09926, 2020.
[20] Yanis Labrak, Adrien Bazoge, Richard Dufour, Béatrice Daille, Pierre-Antoine Gourraud, Emmanuel
Morin, and Mickael Rouvier. FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset
for Medical domain. working paper or preprint, Oct. 2022.
[21] Jooyeon Lee, Huong Dang, Ozlem Uzuner, and Sam Henry. Mnlp at mediqa 2021: fine-tuning pegasus for
consumer health question summarization. In Proceedings of the 20th Workshop on Biomedical Language
Processing, pages 320–327, 2021.
[22] Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter
Davis, Carolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. Biocreative v cdr task corpus: a
resource for chemical disease relation extraction. Database, 2016, 2016.
[23] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to enhance inference
efficiency of large language models. arXiv preprint arXiv:2310.06201, 2023.
[24] Chin-Yew Lin and Eduard Hovy. Manual and automatic evaluation of summaries. In Proceedings of the
ACL-02 workshop on automatic summarization, pages 45–51, 2002.
[25] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le,
Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction
tuning. arXiv preprint arXiv:2301.13688, 2023.
[26] Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren
Zhou. # instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In
The Twelfth International Conference on Learning Representations, 2023.
[27] Diego Mollá and Maria Elena Santiago-Martinez. Development of a corpus for evidence based medicine
summarisation. In Proceedings of the Australasian Language Technology Association Workshop 2011,
pages 86–94. Australian Language Technology Association, 2011.
[28] Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang, Iain Marshall, Ani Nenkova, and Byron Wallace.
A corpus with multi-level annotations of patients, interventions and outcomes to support language
processing for medical literature. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the
56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
197–207, Melbourne, Australia, July 2018. Association for Computational Linguistics.
[29] Evangelos Pafilis, Sune P Frankild, Lucia Fanini, Sarah Faulwetter, Christina Pavloudi, Aikaterini
Vasileiadou, Christos Arvanitidis, and Lars Juhl Jensen. The species and organisms resources for fast
and accurate identification of taxonomic names in text. PloS one, 8(6):e65390, 2013.
[30] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale multi-
subject multi-choice dataset for medical domain question answering. In Gerardo Flores, George H Chen,
Tom Pollard, Joyce C Ho, and Tristan Naumann, editors, Proceedings of the Conference on Health,
Inference, and Learning, volume174ofProceedings of Machine Learning Research, pages248–260.PMLR,
07–08 Apr 2022.
[31] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu. Bleu: amethodforautomaticevaluation
of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational
Linguistics, pages 311–318, 2002.
[32] Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng
Wang, and Weidi Xie. Towards building multilingual language model for medicine. arXiv preprint
arXiv:2402.13963, 2024.
[33] François Remy, Kris Demuynck, and Thomas Demeester. Biolord: Learning ontological representations
fromdefinitions(forbiomedicalconceptsandtheirtextualdescriptions). arXiv preprint arXiv:2210.11892,
2022.
[34] Alexey Romanov and Chaitanya Shivade. Lessons from natural language inference in the clinical domain.
|18[35] Max E Savery, Willie J Rogers, Malvika Pillai, James G Mork, and Dina Demner-Fushman. Chemical
entity recognition for medline indexing. AMIA Summits on Translational Science Proceedings, 2020:561,
2020.
[36] Lynn M Schriml, Elvira Mitraka, James Munro, Becky Tauber, Mike Schor, Lance Nickle, Victor
Felix, Linda Jeng, Cynthia Bearer, Richard Lichenstein, et al. Human disease ontology 2018 update:
classification, content and workflow expansion. Nucleic acids research, 47(D1):D955–D962, 2019.
[37] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan
Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical
knowledge. Nature, 620(7972):172–180, 2023.
[38] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen
Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with
large language models. arXiv preprint arXiv:2305.09617, 2023.
[39] Ali Soroush, Benjamin S Glicksberg, Eyal Zimlichman, Yiftach Barash, Robert Freeman, Alexan-
der W Charney, Girish N Nadkarni, and Eyal Klang. Large language models are poor medical
coders—benchmarking of medical code querying. NEJM AI, 1(5):AIdbp2300040, 2024.
[40] Anthropic Team. Introducing the next generation of claude, 2024. Accessed on March 4, 2024.
[41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient
foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[42] Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali, Christian
Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes Reis, Anna Seehofnerová, et al. Adapted
large language models can outperform medical experts in clinical text summarization. Nature medicine,
30(4):1134–1142, 2024.
[43] David Vilares and Carlos Gómez-Rodríguez. HEAD-QA: A healthcare dataset for complex reasoning. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 960–966,
Florence, Italy, July 2019. Association for Computational Linguistics.
[44] Byron C. Wallace, Sayantani Saha, Frank Soboczenski, and Iain James Marshall. Generating (factual?)
narrative summaries of rcts: Experiments with neural multi-document summarization. AMIA Annual
Symposium, abs/2008.11293, 2020.
[45] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv
preprint arXiv:2212.10560, 2022.
[46] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, An-
jana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-
naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint
arXiv:2204.07705, 2022.
[47] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva
Naik,ArjunAshok,ArutSelvanDhanasekaran,AnjanaArunkumar,DavidStap,EshaanPathak,Giannis
Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi,
Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney,
Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur
Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-
NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Yoav Goldberg,
Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing,pages5085–5109,AbuDhabi,UnitedArabEmirates,Dec.2022.Association
for Computational Linguistics.
[48] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. Pmc-llama:
toward building open-source language models for medicine. Journal of the American Medical Informatics
Association, page ocae045, 2024.
[49] Feng Xie, Jun Zhou, Jin Wee Lee, Mingrui Tan, Siqi Li, Logasan S/O Rajnthern, Marcel Lucas Chee,
Bibhas Chakraborty, An-Kwok Ian Wong, Alon Dagan, et al. Benchmarking emergency department
|19prediction models with machine learning and public electronic health records. Scientific Data, 9(1):658,
2022.
[50] Yuhao Zhang, Daisy Yi Ding, Tianpei Qian, Christopher D Manning, and Curtis P Langlotz. Learning
to summarize radiology findings. arXiv preprint arXiv:1809.04698, 2018.
[51] Zhengyun Zhao, Qiao Jin, Fangyuan Chen, Tuorui Peng, and Sheng Yu. A large-scale dataset of patient
summaries for retrieval-based clinical decision support systems. Scientific Data, 10(1):909, 2023.
[52] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,
Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing
Systems, 36, 2024.
|20A Supplementary
A.1 Evaluation Settings
To assess the model performance on different tasks, there are mainly two prompting strategies, i.e., zero-shot
prompting and few-shot prompting. In our experience, we find that in a zero-shot setting, though the detailed
descriptions of different datasets are given, existing LLMs can hardly understand the strict format or clinical
requirements, even for the strongest close-source model GPT-4 and Claude-3.5. Thus, all evaluations are
based on a few-shot (3-shot) prompting strategy following the below template:
Case1: ...;
Case2: ...;
Case3: ...;
...
{Manually Written Definitions}
Please learn from the few-shot cases to see what content you have to output.
{Input Case}
A.2 Task Category Details
In this paper, we conclude 19 medical-related task categories in total. For evaluation, we adopt 11 categories,
covering 39 datasets, as shown by Supplementary Table 9. Notably, as the evaluation involves commercial
models, for example, GPT-4 and Claude 3.5, it is extremely costly to adopt the original large-scale test split.
Therefore, for some benchmarks (as will be detailed in the following), we randomly sampling a number of test
cases. For the MCQA tasks, we use all of them, as the close-source models have been evaluated by other
papers.
Inthefollowing, wewillintroduceeachtaskcategoryindetail, andfurtherlistouttheusedbenchmarknames,
if the category has been considered in the evaluation.
Medical Multi-choice Question Answering (MCQA). Medical Multiple-choice Question Answering is a
straightforward method for evaluating the performance of medical models. During evaluation, the model is
required to select the correct answers. This method has been widely adopted as an evaluation benchmark
to assess whether the models contain enough medical knowledge. Therefore, we also adopt the popular
multiple-choice question-answering benchmarks, covering both English and multilingual medical domains.
• MedQA [13] is a collection of medical multiple-choice questions, providing both 4-option and 5-option
versions. It covers three languages, i.e., English, Simplified Chinese, and Traditional Chinese. In this
task, to be aligned with existing works [48, 37], we adopt the 4-option English version with official
split. There are 1273 samples in the testset.
• PubMedQA [14] is an English question-answering medical dataset based on PubMed abstracts. The
task of PubMedQA is to answer research questions with yes/no/maybe, which can also be treated as a
close-domain multiple-choice question-answering problem. Officially, it is split into three subsets: 1K
manually labeled pairs (PQA-L), 61.2K unlabeled pairs (PQA-U), and 211.3K artificially generated
pairs(PQA-A).Followingexistingworks[6], weadoptPQA-Lasthetestingset. Thereare1000samples
in the testset.
• MedMCQA [30] is a large-scale English multiple-choice question-answering samples. MedMCQA has
more than 194k high-quality AIIMS & NEET PG entrance exam multiple-choice questions covering
2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and
high topical diversity. The official train split contains 182,822 questions, and the test split contains 4183
questions. Each question has 4 choices. We adopt the official test split to evaluate our model.
• MMedBench [32] is a multilingual medical multiple-choice question-answering datasets covering 6
main languages, i.e., English, Chinese, French, Japanese, Russian and Spanish. It collects multiple-
choice questions from the following datasets: MedQA [13], lgakuQA [18], FrenchMedMCQA [20],
RuMedDaNet [2],Head-QA [43]. We have to emphasize that its English evaluation part is the same
|21as MedQA, thus the reporting scores on it may be overlapped with MedQA for English. We followed
its official split to test our model. That is, 3426 samples for Chinese, 199 samples for Japanese, 622
samples for French, 256 samples for Russian, and 2742 samples for Spanish.
All the above benchmarks are based on multiple-choice questions. Therefore, the model ability can be easily
assessed via the default Accuracy score as previous work [32, 48, 6, 37, 38].
Text Summarization. Text summarization denote a generative task, where a model is tasked to extract
key information from extensive pieces of text. This task is particularly prevalent in the clinical field, for
example, quickly extracting essential information from lengthy examination reports or helping clinicians and
patients better understand complex medical reports. We evaluate the model’s performance using the following
benchmarks.
• MedQSum [21] is a large, freely available database of de-identified health-related data associated
with over fifty thousand patients. It contains notes, reports, and electronic health records (EHR) of the
patients, formatted with a fixed template. For instance, imaging diagnostic reports always start with
detailed findings and end with a conclusion. We reconstruct this dataset by using the detailed findings
as input to the model and expect the model to generate a summary, with the conclusion of the note
serving as the ground truth. During testing, we randomly sample 100 instances for evaluation.
• RCT-Text [44] is a dataset designed to summarize medical evidence from various clinical studies as
presented in literature reviews. For this task, we provide the LLM with the titles and abstracts of
studies, and task the model to output the primary conclusions of each study.
• MIMIC-CXR [16] is a comprehensive, publicly available dataset that includes chest radiographs,
each paired with structured labels derived from free-text radiology reports. The dataset encompasses
227,827 radiology reports, formatted with one template. For this task, we provide the LLM with
descriptionshighlightingkeyaspectsofthechestX-rayimages. Themodelisthenrequiredtosummarize
the pertinent findings from these descriptions. During testing, we randomly sample 100 instances for
evaluation.
• MIMIC-IV Report [17] is a sub-set of MIMIC-IV. Specifically, MIMIC-IV is a large deidentified
medical dataset of patients admitted to the emergency department or an intensive care unit at the
Beth Israel Deaconess Medical Center in Boston, MA. It contains data for more than 65K patients
admitted to an ICU and over 200K patients admitted to the emergency department. In this large-scale
medicalinformaticsdatabase,thereisaradiologynotesplitthatcontains2,321,355deidentifiedradiology
reports for 237,427 patients. For each radiology report, inspired by existing works [42, 50], we treat the
impression part as a general summarization of the findings. We randomly sampled 100 cases for each
covered modality and body region part, like Chest CT, Brain MRI, etc..
In all the above benchmarks, the model is required to generate free-form texts. We evaluate the quality of the
generated text by calculating the similarity between the generated text and the ground truth with BLEU [31]
and ROUGE [24].
Information Extraction. In contast to summarization that aims at concluding the main information
from given contexts, information extraction instead is expected to extract the detailed clinical or medical
information from long texts based on various user queries.
• ADE Corpus [10] provides information on the drug and its corresponding adequate dose within each
sentence. For the drug dose extraction task, we input into the model both the sentence and the drug
name mentioned therein. The model is then required to identify the dosage levels of the specified drug.
In this benchmark, we directly use the dataset prompted by Super-Instruction [46] with a few cases,
and divide it at a ratio of 9:1 for instruction tuning and evaluation. During testing, we evaluate using
the entire test set, which consists of 23 instances.
• PICO [28] consists of 5,000 abstracts from medical articles on the randomized controlled clinical trials,
annotated to identify text spans that describe the patient population enrolled, the interventions studied
and their comparisons, and the outcomes measured. We employ this dataset to develop three tasks. The
|22first task, Outcome Extraction, requires the model to identify phrases in a study report sentence that
provide information about the study’s outcomes. The second task, Intervention Extraction, involves
the model identifying phrases in a study report sentence that describe the study’s interventions. The
final task, Participant Extraction, demands the model to extract phrases from a study report sentence
that detail information about the study’s participants. In this benchmark, we directly use the dataset
promptedbySuper-Instruction[46]withafewcases, anddivideitataratioof9:1forinstructiontuning
and evaluation. During testing, we evaluate using the entire test set, which consists of 43 instances for
each task.
• PMC-patient basic information extraction [51] is a pioneering collection that includes 167,000
patientsummaries,extractedfromcasereportsinPubMedCentral(PMC).Thisdatasethasbeenfurther
enriched with annotations detailing basic patient information. For this specific task, we provide the
Large Language Model (LLM) with an extensive clinical text about the patient, and require the model
to accurately extract information regarding the patient’s gender and age. During test, we randomly
sample 100 instances for evaluation.
Explanation. Explanation considers the task of providing a detailed justification or description of a clinical
claim or concept. It is useful in clinical to help physicians or patients better understand professional medical
terminologies or claims from other specialists. Our benchmark includes the following datasets.
• PUBHEALTH Explanation [19] is a comprehensive dataset geared towards explainable automated
fact-checking of public health claims. The task requires models to furnish an explanation for a specified
claim using supporting material from the provided paragraph. During testing, we randomly sample 50
instances for evaluation.
• HumanDiseaseOntology(DO)[36]isadatabaseprovidingthebiomedicalcommunitywithconsistent,
reusable, and sustainable descriptions of human disease terms, phenotype characteristics, and related
medical vocabularies. In this task, we ask the model to explain the specified given medical professional
entity,andthedescriptionfromthedatabasefunctionsasthegroundtruth. Duringtesting,werandomly
sample 100 instances for evaluation.
• BioLORD Explanation [33] comprises pairs of biomedical concepts names and descriptions. In this
task, we challenge models to elaborate on concise concepts by generating long, detailed definitions.
During testing, we randomly sample 50 instances for evaluation.
• MMedBench-Rationale [32] is a multilingual medical multiple-choice question and answering bench-
mark along with rationale for each sample. In this task, we will give the LLMs a question and its
answer and require it to generate an explanation statement or step-by-step reasoning. During testing,
we randomly sample 50 instances for each language.
For these tasks, the models are required to generate free-form text. The quality of the generated text is
evaluated by comparing it to the ground truth using BLEU and ROUGE metrics.
Named Entity Recognition. Named Entity Recognition (NER) in the biomedical field is a specialized
task in natural language processing that identifies and classifies entities in biomedical texts, such as research
articles, clinical notes, and patient records. These entities include gene and protein names, diseases, chemicals,
drugs, anatomical structures, and other medical terms. This task can assist clinicians to structure clinical
free-form texts, thus enables more readable documents. Due to the required domain-specific knowledge, this
taskposessignificantchallenges,particularlyforlargelanguagemodels. Ourbenchmarkevaluatesperformance
across the following benchmarks.
• BC4CHEMD [35] is a dataset comprising 10,000 PubMed abstracts with a total of 84,355 chemical
entity mentions, manually annotated by expert chemistry literature curators. In this task, the LLM is to
input an abstract, and is required to recognize the name of the chemical. During testing, we randomly
sample 50 instances for evaluation.
• BC5CDR [22] is a widely-used resource in biomedical natural language processing, annotated for
chemical and disease entities and their relationships. This benchmark is split into 2 task according
|23to the category of the entity. In the first category, the LLM is required to recognize the name of the
chemical. In the other category, the LLM is required to write the name of the disease. During testing,
we randomly sample 50 instances for each task.
• Species800 [29] comprises 800 PubMed abstracts in which organism mentions were identified. In this
task, the LLM is input an abstract and is required to recognize the name of the organism. During
testing, we randomly sample 50 instances for evaluation.
Based on the entity numbers, we categorize these datasets into two cases, i.e., datasets containing exactly one
entity per sentence, and those that may contain arbitrary entities (multiple or none), challenging the model
to differentiate the text structure effectively. For the former cases, performance is assessed using accuracy
metrics, and for the latter ones, Recall, Precision, and mixed F1 scores are used.
Diagnosis. Diagnosis is an important and basic task for clinical practices. The context of patients will be
given in this task, with EHR or free-text format, and models are expected to predict the final diseases from a
suspicious disease list or directly give positive or negative judgment for a certain targeting disease proposed
by the user. The following benchmark is used for evaluation:
• DDXPlus [8] is a pioneering large-scale dataset tailored for Automatic Symptom Detection (ASD)
and Automatic Diagnosis (AD) systems in the medical domain, featuring synthesized patient data from
a proprietary medical knowledge base and commercial AD system. In this task, the model must make
diagnostic decisions based on dialogues, selecting from a provided list of potential diagnoses. During
testing, we randomly sample 50 instances for evaluation.
We report the accuracy scores to reflect the models’ diagnosis ability.
Treatment Planning. Similar to diagnosis, treatment planning is also a common clinical routine. The input
texts for this task are also the context of a patient, while the output of models is expected to be treatment
recommendations, like medication or therapy types. Treatment is a complex task in clinical, as the detailed
treatmentplancanbehighlycustomisedanddetailedinfree-formtexts,asanevaluationsub-task, wesimplify
this task into picking the best treatment decision from a given close-set. The following benchmarks are
considered:
• SEER [7] is a treatment planning dataset leveraging the Surveillance, Epidemiology, and End Re-
sults (SEER) custom breast cancer databases. It collects 32,922 patient records and for each record
with 19 attributes where the recommended treatment plans are noted, commonly in five types, i.e.,
Surgery Chemotherapy, Hormonal therapy, Biological therapy, and Radiation therapy. During testing,
we randomly sample 50 instances for evaluation.
As we have simplified this task into a close-set recommendation format, we can therefore assess the model’s
performance with accuracy scores.
Clinical Outcome Prediction. Beyonddiagnosisandtreatment,therearealsoothertaskstohelpclinicians
make clinical decisions, for example, disease risks and survival outcomes. This task judges whether the models
can provide assistance in accurate clinical decision-making. The following benchmark is considered in our
paper.
• MIMIC4ED Benchmark [49] provides a standardized benchmark derived from the Medical In-
formation Mart for Intensive Care IV-Emergency Department (MIMIC-IV-ED) database, including
data covering several key prediction tasks within the emergency department. We use this dataset for
the comparison of LLMs for predicting clinical outcomes in emergency medicine. According to the
differential of the categories of the clinical output, we split it into 3 tasks. We give the model an EHR
of a patient and require the model to predict whether the patient needs, or the patient may revisit
the emergency department in 72 hours, or the patient should be classified into a critical triage queue,
separately. During testing, we randomly sample 100 instances for each task.
Considering that, in this paper, the most clinical output is binary (yes or no), we can assess the final model
performance with commonly binary classification metrics, like accuracy, precision, recall, and F1 scores.
|24Text Classification. Text classification aims to classifying a given text into a certain category. This is useful
for clinical text collation and identifying public attitudes towards certain clinical processes. The following
datasets are used here.
• HoC [1] is a specialized dataset containing 1,852 PubMed publication abstracts, which are expertly
annotated according to a taxonomy. In this task, LLM is required for you to classify the hallmarks of
cancer according to the given biomedical publication abstracts. During testing, we randomly sample 50
instances for evaluation.
The above benchmark typically involve multi-class classification, thus, we use the accuracy as evaluation
metric.
Fact Verification. Fact verification involves detecting some basic knowledge errors in a given context. This
plays a role for large-scale verification on biomedical documents.
• PUBHEALTH Verification [19] is a dataset designed for explainable automated fact-checking of
public health claims. Beyond asking models to explain the fact details, it can also be modified as a fact
verication task, i.e., given a paragraph along with a related claim and models are required to determine
whether the claim contradicts the evidence provided in the paragraph. During testing, we randomly
sample 25 instances for evaluation.
• PubMedQA Verication [14] is a question-answering medical dataset based on PubMed abstracts.
Unlike direct use in a Question Answering benchmark, we provide the LLM with a passage accompanied
by a question and its answer. The Task for LLM is to verify if the answer answers the question. During
testing, we randomly sample 50 instances for evaluation.
• EBMSjustificationverification[27]Duringtesting,werandomlysample140instancesforevaluation.
Natural Language Inference (NLI). Natural Language Inference (NLI) is one of the critical tasks for
understanding natural language. The objective of NLI is to determine if a given hypothesis can be inferred
from a given premise. In medical this task is useful for making inferences form the textual medical history of
patients.
• MedNLI [34] is a nature language inference dataset for clinical domain, which is grounded in patient
medical histories and annotated by medical professionals. We have divided the dataset into two tasks:
‘Entailment (discriminative)’ and ‘Entailment (generative)’. In the discriminative task, the model is
initially presented with a formal clinical premise, such as condition descriptions or quantitative results.
It is then given a hypothesis statement and must determine whether the hypothesis can be logically
inferred from the clinical condition. In the generative task, the model is presented with a formal clinical
premise and is required to generate a coherent and contextually correct hypothesis statement based on
the given premise. During testing, we randomly sample 50 instances for each task.
Text Retrieval. Textretrieval referstoextractingthereferenced textfromalonggiventext, which issimilar
as introduced in Sec. 2.2. It can be used to match some similar cases from a large-scale case basis or related
medical knowledge from encyclopedias for a given textual query.
Intent Identification involves understanding and recognizing the intent behind user inputs. In clinical
scenarios, patients may propose various queries, and the basic step for analysis is to distinguish their general
intent like ‘treatment’ or ‘disease’. This can help the doctor understand the patient’s request quickly and
accurately, speeding up the process.
Word Relation Classification involves classifying whether two given medical words are linked with some
specific relation, like what drugs may interact with others or what symptoms may link with some diseases.
This task can inject the basic medical knowledge about professional terminologies into models.
Translation relates to translating text from one language to another. It can help bridge language barriers
between doctors and patients, as well as translate the latest medical research and literature into different
languages, facilitating the global sharing and dissemination of medical knowledge.
|25Dialogue involves constructing systems capable of engaging in natural conversations with humans. Clinically,
this can aid in the initial collection of patient histories and descriptions of symptoms, improving the efficiency
of consultations. It holds significant importance for achieving personalized and precise medical care.
Sentence Composition Analysis involves decomposing a medical sentence into a structural linking graph,
a task that can enhance LLMs’ understanding of complex medical sentences. By doing so, it improves the
model’s ability to comprehend and process medical-related sentences more effectively.
Wrong Candidate Generation involves generating false answers to user questions, which, while not useful
in real clinical contexts, serves as a fundamental ability for LLMs. By leveraging this capability, LLMs can
identify outputs that are completely unacceptable and can also automatically generate data for other task
categories, such as fact verification and multiple-choice question answering (MCQA).
Text Completion relates to automatically finishing some clinical texts. In healthcare scenarios, staff are
asked to fill a large number of records or reports in daily workflow. Leveraging LLMs to aquatically initial a
reference can improve the efficiency critically. This is a more general task category compared to others, we
mainly include the tasks falling out the above introduced categories into this type.
A.2.1 The Detailed Statistic of Our Benchmark
In Table 9, we further demonstrate the detailed token length of different task category in our combined
benchmark, along with its corresponding language model abilities.
A.3 Detail Tasks in MedS-Ins
InTable10,wefurtherlistoutthedetailedtasksincludedinMedS-Ins.
Table 10 | Thedetailed122tasksinourinstructiondataset,alongwiththecorrespondingsources,categories,anddomains.
Task Source Category Domain
participantextraction PICO InformationExtraction AcademicalPapers
interventionextraction PICO InformationExtraction AcademicalPapers
outcomeextraction PICO InformationExtraction AcademicalPapers
pubmedqaquestiongeneration PubMedQA TextCompletion AcademicalPapers
pubmedqaclassification PubMedQA FactVerification AcademicalPapers
pubmedqaquestiongeneration PubMedQA TextCompletion AcademicalPapers
pubmedqaclassification PubMedQA IntentIdentification AcademicalPapers
pubmedqaanswergeneration PubMedQA QuestionAnswering AcademicalPapers
healthfactclassification HealthFact FactVerification MedicalKnowledge
healthfactsentencegeneration HealthFact Sentence Composition MedicalKnowledge
Analysis
healthfactsentencegeneration HealthFact Explanation MedicalKnowledge
europaecdctmensvtranslation ECDC-TM Translation MedicalKnowledge
europaecdctmendetranslation ECDC-TM Translation MedicalKnowledge
europaecdctmfrentranslation ECDC-TM Translation MedicalKnowledge
headqaanswergeneration HeadQA QuestionAnswering Exam
headqalanguagetranslationentoes HeadQA Translation Exam
headqalanguagetranslationestoen HeadQA Translation Exam
headqaclassification HeadQA TextClassification Exam
drugextractionade ADECorpusV2 NamedEntityRecogni- MedicalKnowledge
tion
diseaseentityextractionncbidataset NCBIDiseaseCorpus NamedEntityRecogni- MedicalKnowledge
tion
diseaseentityextractionbc5cdrdataset BioCreativeV NamedEntityRecogni- MedicalKnowledge
tion
drugdoseextraction ADECorpusV2 InformationExtraction MedicalKnowledge
geneextractionbc2gmdataset BC2GM NamedEntityRecogni- MedicalKnowledge
tion
geneextractionlinnaeusdataset LinnaeusCorpus NamedEntityRecogni- MedicalKnowledge
tion
Continuedonnextpage
|26Task Source Category Domain
cellextractionanemdataset ANEM NamedEntityRecogni- MedicalKnowledge
tion
organism substance extraction anem ANEM NamedEntityRecogni- MedicalKnowledge
dataset tion
adversedrugeventclassification ADECorpusV2 TextClassification MedicalKnowledge
medicalquestionpairdatasettextclassifi- Medical Question Pair TextRetrieval DailyConversation
cation Dataset
datasetcardforcataloniaindependencecor- Catalonia Indepen- IntentIdentification MedicalKnowledge
pustextclassification denceCorpus
limamedicalfilteredanswergeneration LIMA QuestionAnswering DailyConversation
mednlitextualentailmentdiscrimitive MedNLI NaturalLanguageInfer- ClinicalText
ence
mednlitextualentailmentgenerative MedNLI NaturalLanguageInfer- ClinicalText
ence
mednliwrongtextualentailmentgenerative MedNLI WrongCandidateGen- ClinicalText
eration
mednlitextualentailmentclassification MedNLI NaturalLanguageInfer- ClinicalText
ence
dosynonymsgeneration DO TextCompletion MedicalKnowledge
doentityexplanation DO Explanation MedicalKnowledge
doentityrelationclassification DO WordRelationClassifi- MedicalKnowledge
cation
biolordtextmatching BioLORD TextRetrieval MedicalKnowledge
biolordsummarization BioLORD TextSummarization MedicalKnowledge
biolordexplanation BioLORD Explanation MedicalKnowledge
mmedbenchexplanationchinese MMedBench Explanation MedicalKnowledge
mmedbenchexplanationenglish MMedBench Explanation MedicalKnowledge
mmedbenchexplanationfrench MMedBench Explanation MedicalKnowledge
mmedbenchexplanationjapanese MMedBench Explanation MedicalKnowledge
mmedbenchexplanationrussian MMedBench Explanation MedicalKnowledge
mmedbenchexplanationspanish MMedBench Explanation MedicalKnowledge
medqaquestionansweringen MedQA QuestionAnswering MedicalKnowledge
medqaquestionansweringzh MedQA QuestionAnswering MedicalKnowledge
igakuqaquestionanswering IgakuQA QuestionAnswering MedicalKnowledge
frenchmedmcqaquestionanswering FrenchMedMCQA QuestionAnswering MedicalKnowledge
rumedbenchquestionanswering RuMedBench QuestionAnswering MedicalKnowledge
liveqamedicalfilteredconversation LiveQA Dialogue DailyConversation
medmcqamultiplechoiceexplanationqa MedMCQA Explanation Exam
medmcqamultiplechoiceqa MedMCQA QuestionAnswering Exam
medicationqaquestionanswering MedicationQA QuestionAnswering Exam
chatdoctorconversation ChatDoctor Dialogue DailyConversation
chatdoctorhealthcaremagicconversation ChatDoctor Dialogue DailyConversation
chatdoctoricliniqconversation ChatDoctor Dialogue DailyConversation
medalplacawikidoctextbookqa WikiDoc QuestionAnswering MedicalKnowledge
medalplacawikidocpatientinformationqa WikiDoc QuestionAnswering MedicalKnowledge
medalplacaflashcardqa MedAlpaca QuestionAnswering MedicalKnowledge
reportentityextraction Radiopaedia Caption Sentence Composition ClinicalText
NER Analysis
pmcpatientcasereporttitlegeneration PMC-Patient TextCompletion AcademicalPapers
pmcpatientcasereportbasicinformation PMC-Patient InformationExtraction AcademicalPapers
extraction
pmcpatientsimilarcaseretrieval PMC-Patient TextRetrieval AcademicalPapers
mimic iv note discharge instruction com- MIMIC-IV-Notes TextCompletion MedicalKnowledge
pletion
clinicaltrialsgovdatanamedentityrecog- Clinical-Trials.gov NamedEntityRecogni- Exam
nition tion
mimicultrasoundsummarization MIMIC-IV TextSummarization ClinicalText
mimicangiographysummarization MIMIC-IV TextSummarization ClinicalText
mimicmammogramsummarization MIMIC-IV TextSummarization ClinicalText
Continuedonnextpage
|27Task Source Category Domain
mimicpathologysummarization MIMIC-IV TextSummarization ClinicalText
mimicfluoroscopysummarization MIMIC-IV TextSummarization ClinicalText
mimicctchestsummarization MIMIC-IV TextSummarization ClinicalText
mimicctheadandnecksummarization MIMIC-IV TextSummarization ClinicalText
mimicctbrainsummarization MIMIC-IV TextSummarization ClinicalText
mimicctabdomensummarization MIMIC-IV TextSummarization ClinicalText
mimicctpelvissummarization MIMIC-IV TextSummarization ClinicalText
mimicctspinesummarization MIMIC-IV TextSummarization ClinicalText
mimicmrichestsummarization MIMIC-IV TextSummarization ClinicalText
mimicmriheadandnecksummarization MIMIC-IV TextSummarization ClinicalText
mimicmribrainsummarization MIMIC-IV TextSummarization ClinicalText
mimicmriabdomensummarization MIMIC-IV TextSummarization ClinicalText
mimicmripelvissummarization MIMIC-IV TextSummarization ClinicalText
mimicmrispinesummarization MIMIC-IV TextSummarization ClinicalText
mimicmodalityclassification MIMIC-IV TextClassification ClinicalText
mimicanatomyclassification MIMIC-IV TextClassification ClinicalText
medlineplusdrugqa MedlinePlus QuestionAnswering MedicalKnowledge
medlineplusencyclopediaqa MedlinePlus TextSummarization MedicalKnowledge
ebmsquestionanswering EBMSummariserCorpus QuestionAnswering MedicalKnowledge
ebmsanswervertification EBMSummariserCorpus QuestionAnswering MedicalKnowledge
mashqaquestionanswering MASHQA QuestionAnswering MedicalKnowledge
mediqaansquestionanswering MEDIQA-AnS QuestionAnswering MedicalKnowledge
medquadquestionanswering MedQuAD QuestionAnswering MedicalKnowledge
bioasqquestionanswering BIOASQ QuestionAnswering MedicalKnowledge
covidqaquestionanswering COVID-QA QuestionAnswering MedicalKnowledge
hoctextclassification HoC TextClassification MedicalKnowledge
pubmedmeshtextclassification PubMedMeSH TextClassification MedicalKnowledge
litcovidtextclassification LitCovid TextClassification MedicalKnowledge
ms2textsummurization MS2 TextSummarization MedicalKnowledge
rcttextsummurization RCT TextSummarization MedicalKnowledge
pubmedtextsummarization PubMed TextSummarization MedicalKnowledge
mimiccxrtextsummurization MIMIC-CXR TextSummarization ClinicalText
readibilitytextsummurization Readibility TextSummarization MedicalKnowledge
medqsumtextsummurization MeQSum TextSummarization MedicalKnowledge
icd10codeexplanation ICD-10-CM Explanation MedicalKnowledge
mimic4icd10textclissification MIMIC4ICD-10 TextClassification ClinicalText
mimic4edbenchmarkhospitalization MIMIC4ED Bench- Clinical Outcome Pre- ClinicalText
mark diction
mimic4ed72hedrevisitdisposition MIMIC4ED Bench- Clinical Outcome Pre- ClinicalText
mark diction
mimic4edbenchmarkcriticaltriage MIMIC4ED Bench- Clinical Outcome Pre- ClinicalText
mark diction
cmedqa2questionanswering CMedQA2 QuestionAnswering MedicalKnowledge
pubmedqatrainset PubMedQA QuestionAnswering AcademicalPapers
bc4chemnamedenetityrecognition BC4-Chem NamedEntityRecogni- MedicalKnowledge
tion
bc5chemnamedenetityrecognition BC5-Chem NamedEntityRecogni- MedicalKnowledge
tion
species800namedenetityrecognition Species-800 NamedEntityRecogni- MedicalKnowledge
tion
headqaquestionanswering HeadQA QuestionAnswering MedicalKnowledge
DDXPlustextclassificationtrain DDXPlus Diagnosis MedicalKnowledge
SEERtextclassificationtrain SEER TreatmentPlanning MedicalKnowledge
factvericationshortmedicaltesttrain MedlinePlus FactVerification MedicalKnowledge
fact verication short genetic conditions MedlinePlus FactVerification MedicalKnowledge
train
factvericationshortgenetrain MedlinePlus FactVerification MedicalKnowledge
factvericationshortencyclopediatrain MedlinePlus FactVerification MedicalKnowledge
factvericationshortdrugstrain MedlinePlus FactVerification MedicalKnowledge
|28|29
.skramhcnebdesufosliatedcitsitatsehT
|
9
elbaT
htgneLnekoTdegarevA
scirteM
ytilibAdessessA
skramhcneB
seirogetaCksaT
latoT
noitinfieD
tuptuO
tupnI
ycaruccA
081
12
51
441
gniniart-erPmorfstcafgnillaceR
,AQCMdeM
,AQdeMbuP
,AQdeM
gnirewsnAnoitseuQeciohc-itluM
hcneBdeMM
,UELB
923
64
05
342
gniniart-erPmorfstcafgnillaceR
-namuH
,noitanalpxE
HTLAEHBUP
elanoitaR/noitanalpxE
EUGOR
-xEDROLoiB,)OD(ygolotnOesaesiD
elanoitaR-hcneBdeMM,noitanalp
ycaruccA
146
253
9
082
gniniart-erPmorfstcafgnillaceR
sulPXDD
sisongaiD
ycaruccA
272
79
9
661
gniniart-erPmorfstcafgnillaceR
REES
gninnalPtnemtaerT
ycaruccA
385
43
1
845
gniniart-erPmorfstcafgnillaceR
kramhcneBDE4CIMIM
noitciderPemoctuOlacinilC
ycaruccA
064
721
01
423
gniniart-erPmorfstcafgnillaceR
CoH
noitacfiissalCtxeT
ycaruccA
983
74
72
513
gniniart-erPmorfstcafgnillaceR
-buP
,noitanalpxE
HTLAEHBUP
noitacfiireVtcaF
noitacireVAQdeM
ycaruccA
511
87
5
33
gniniart-erPmorfstcafgnillaceR
ILNdeM
ecnerefnIegaugnaLlarutaN
,UELB
074
02
97
273
stxetnoCmorfstcafgniveirteR
,muSQdeM
,noitazirammuS
deMbuP
noitazirammuStxeT
EUGOR
tropeRVI-CIMIM
,UELB
862
621
7
731
stxetnoCmorfstcafgniveirteR
OCIP,suproCEDA
noitcartxEnoitamrofnI
EUGOR ,ycaruccA
29
25
5
53
stxetnoCmorfstcafgniveirteR
,ABPLNJ
,suproC
esaesiD
IBCN
noitingoceRytitnEdemaN
,llaceR
,RDC5CB
,DMEHC4CB
,MG2CB
-icerP
,sueanniL
,torPmehC
,08seicepS
1F,nois
MEnA