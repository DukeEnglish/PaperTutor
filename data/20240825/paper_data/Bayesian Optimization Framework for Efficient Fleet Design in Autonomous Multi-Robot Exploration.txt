1
Bayesian Optimization Framework for Efficient
Fleet Design in Autonomous Multi-Robot
Exploration
David Molina Concha, Jiping Li, Haoran Yin, Kyeonghyeon Park, Hyun-Rok Lee, Taesik Lee, Dhruv Sirohi,
Chi-Guhn Lee
Abstract—This study addresses the challenge of fleet design the form of developing goal assignment mechanisms [2]–[4]
optimization in the context of heterogeneous multi-robot fleets, andreliablecommunicationmechanisms[5]–[7].Furthermore,
aiming to obtain feasible designs that balance performance and
the escalating complexity of missions has given rise to the
costs. In the domain of autonomous multi-robot exploration,
demand for larger fleet sizes and greater heterogeneity in
reinforcement learning agents play a central role, offering
adaptability to complex terrains and facilitating collaboration robot capabilities [8]. Despite these challenges, determining
among robots. However, modifying the fleet composition results the optimal composition of a heterogeneous fleet for efficient
in changes in the learned behavior, and training multi-robot exploration has seen partial efforts within the context of
systems using multi-agent reinforcement learning is expensive.
autonomous robots.
Therefore,anexhaustiveevaluationofeachpotentialfleetdesign
The fleet design problem involves a delicate balance be-
isinfeasible.Totacklethesehurdles,weintroduceBayesianOpti-
mizationforFleetDesign(BOFD),aframeworkleveragingmulti- tween performance and costs [9]. The selection of hardware
objective Bayesian Optimization to explore fleets on the Pareto components within a multi-robot fleet significantly impacts
frontofperformanceandcostwhileaccountingforuncertaintyin its overall performance. For example, a robot type with an
the design space. Moreover, we establish a sub-linear bound for
extended sensor should be able to explore better than another
cumulative regret, supporting BOFD’s robustness and efficacy.
robot type with a shorter sensor range. Opting for full-
Extensive benchmark experiments in synthetic and simulated
environmentsdemonstratethesuperiorityofourframeworkover featured robots might seem attractive as a means to maximize
state-of-the-art methods, achieving efficient fleet designs with fleet performance. However, it’s essential to acknowledge that
minimal fleet evaluations. such component choices also come with substantial budget
Index Terms—Design optimization, Multi-robot systems, Au- considerations. Take into account the impressive capabilities
tonomous agents, Deep reinforcement learning. of a full-featured ground robot like Boston Dynamics’ Spot,
which comes at a significant cost of US $74,000 [10]. In
contrast, a more budget-friendly, yet less feature-rich option
I. INTRODUCTION
like the Turtlebot3 Burger can be acquired for a substantially
THEdesignandoptimizationofheterogeneousmulti-robot
lower price, around US $600 [11]. The balance between cost
systems for various applications, such as environmental and performance is a pivotal factor in designing an efficient
monitoring, search and rescue operations, and industrial in- fleet composition to effectively achieve the exploration task.
spection, have been the focus of extensive research recently In the domain of multi-robot exploration, autonomous
due to the advantages over homogeneous fleets [1]. With the robots are frequently trained as reinforcement learning (RL)
increasingcomplexityoftasksandtheimperativeneedforeffi- agents. This choice is driven by several reasons. Firstly, RL
cientresourceallocation,researchershavehonedtheirfocuson empowers robots to adapt and learn from their interactions
enhancingcoordinationamongrobots.Thisenhancementtakes withtheenvironmentandthefeedbackfromtheiractions.This
adaptability allows them to navigate and explore unfamiliar
This work was supported in part by the International Doctoral Cluster
or complex terrains, resulting in heightened efficiency and
Program of the University of Toronto, #DSF20-01; and in part by the
NationalAgencyforResearchandDevelopmentofChile,programBecaChile effectivenessinexplorationtasks[12].Secondly,RLfacilitates
DoctoradoExtranjero#72200533. coordination and cooperation among multiple robots work-
DavidMolinaConchaandChi-GuhnLee(Correspondingauthor)arewith
ing as a team. By training robots as RL agents, they can
the Dynamic Optimization and Reinforcement Learning Laboratory of the
Department of Mechanical & Industrial Engineering, Jiping Li and Haoran master communication and collaboration, enabling them to
Yin are with the Department of Electrical & Computer Engineering, Dhruv share information and coordinate actions to achieve common
SirohiiswiththeDivisionofEngineeringScienceoftheFacultyofApplied
exploration goals [13]. This teamwork significantly enhances
Science and Engineering, University of Toronto, Toronto, ON M5S3G8,
Canada. (e-mail david.molina@mail.utoronto.ca; cglee@mie.utoronto.ca; the exploration capabilities of the robot fleet by harnessing
jiping.li@mail.utoronto.ca; haoran.yin@mail.utoronto.ca; collective knowledge and experiences.
dhruv.sirohi@mail.utoronto.ca).
The performance of a multi-agent system of RL agents
Kyeonghyeon Park and Taesik Lee are with the Complex System Design
LaboratoryoftheDepartmentofIndustrialandSystemsEngineering,Korea depends on their joint behavior, learned over multiple training
AdvancedInstituteofScienceandTechnology,Daejeon,34141,Korea.(e-mail episodes. Introducing, removing, or replacing agents in the
kyeonghyeon.park@kaist.ac.kr;taesik.lee@kaist.ac.kr).
system results in changes in the learned behavior, necessi-
Hyun-Rok Lee is with the Department of Industrial Engineering, Inha
University,Incheon,22212,Korea.(e-mailhyunrok.lee@inha.ac.kr). tating retraining of the entire fleet of agents. Multi-agent
4202
guA
12
]OR.sc[
1v15711.8042:viXra2
reinforcement learning (MARL) algorithms face significant In the domain of reward design, [16] introduces a bi-level
computational challenges due to the exponential increase in optimization for the reward design problem, optimizing a
state and action space relative to the number of agents [14]. continuous reward parameter at the upper level using single-
This scalability limitation makes it infeasible to explore the objective BO and solving the underlying Markov Game at the
behavior of all possible fleet combinations with multiple lower level using a MARL algorithm. [17] implemented the
heterogeneous agents. bi-level framework to design rewards for transportation firms.
Inthecontextofoptimizingexpensive-to-evaluatefunctions, While it is feasible to extend these frameworks to address the
Bayesian Optimization (BO) emerges as a potent tool for fleet design problem, single-objective Bayesian Optimization
addressing such problems [15]. BO leverages probabilistic is unable to effectively capture the trade-off between cost and
modelstoguidethesearchforoptimalsolutions,effectivelyre- performance.
ducingthenumberofexpensivefunctionevaluationsrequired. The fleet design problem has been modeled as a multi-
Single-objectiveBOhasrecentlyfoundapplicationinalgorith- objective optimization problem in [18], presenting a general
mic reward design within MARL [16], [17]. This approach is methodforintegratingmulti-objectivesystemdesignandfleet
particularly suitable for assessing promising reward parame- acquisitionplanning.Theirworkisfocusedontheinterdepen-
ters in continuous spaces with a limited evaluation budget. denceofbothproblemsviamixed-integerlinearprogramming,
However, extending these methods for fleet design requires without dealing with autonomous agents or expensive-to-
additional efforts for discrete search space and formulating evaluatefunctions.Foraddressingthefleetdesignproblemef-
the problem as single-objective might not capture the trade- ficiently,multi-objectiveBayesianoptimizationmethods,such
off between performance and cost. as USeMO [22], offer valuable approaches. USeMO employs
While multi-objective (MO) optimization [18] can handle Gaussian processes to model black-box objective functions,
thetrade-offbetweenacquisitioncostsandperformance,there constructing a cheap MO problem. It selects solutions from
exists no prior work addressing algorithmic approaches to the Pareto front that maximize the volume of uncertainty
solving fleet design in the context of autonomous robots. hyper-rectangles. While USeMO exhibits simplicity and sub-
To address this problem efficiently, we propose Bayesian linear regret properties, making it computationally efficient
OptimizationforFleetDesign(BOFD),abi-leveloptimization and competitive with state-of-the-art methods, it lacks con-
framework. The upper-level problem utilizes Multi-Objective sideration of inherent characteristics specific to fleet design
BayesianOptimization(MOBO)tooptimizefleetcomposition. optimization.Moreover,itseffectivenessinthecontextoffleet
In the lower-level problem, it evaluates new fleet designs by design remains untested.
trainingagentsusingMARLalgorithms.Ourcontributionsare:
• We introduce an efficient framework for addressing the III. PRELIMINARIES
fleet design problem with heterogeneous autonomous
A. Multi-robot exploration
robots, requiring only a few fleet evaluations.
• Weestablishasub-linearregretboundwithrespecttothe Multi-robot exploration involves deploying multiple robots
number of iterations, supporting the effectiveness of our to explore and map an environment efficiently. The primary
approach. objective in multi-robot exploration is to identify the optimal
• We showcase the performance of the BOFD framework path that minimizes either the total distance traveled or the
throughcomprehensiveevaluationsinsyntheticandsimu- time taken to explore the entire environment. In the context
lated environments. Our benchmark studies include com- of this optimization, the objective function of the exploration
parisons against state-of-the-art methods in both single problem aims to determine the path P∗ that minimizes the
and multi-objective Bayesian Optimization. overall distance traveled or time spent from the current loca-
tion of each robot to a specific destination point p. Assuming
a grid-like map, the optimization problem can be formulated
II. RELATEDWORK
as follows:
The impact of fleet size in exploration tasks is studied in
[19], showing that increasing fleet size does not always lead (cid:88)N (cid:88)Tmax
P∗ =argmin g(pi), (1)
to improvement in performance due to the efforts to avoid t
p∈P
collision with other robots while exploring the environment. i=1 t=1
Their work provided insights into the effects of modifying here, P denotes the set of all points on the grid map, T
max
the number of robots in the system without proposing an representsthetotaltimestepsofroboti,N isthetotalnumber
algorithmic solution for the fleet design problem. Similarly, ofrobotsinthefleet,pi isthedestinationpointchosenbyrobot
t
[20] highlights the importance of considering the trade-off i at time t, and g(p) is a function describing the distance or
between fleet size and efficiency in autonomous car systems time from robot i’s current location to the destination point p.
without delving into an algorithmic method for fleet design. Deploying multiple robots offers advantages such as con-
The work of [21] explores the trade-offs involved in the currency, reduced mission time, and the ability to perform
selection of different fleet sizes, contributing a mathematical tasks more efficiently than single robots [23]. However, it
analysis of the time gains versus costs. Their discussion is presents challenges related to coordination, merging infor-
in the context of homogeneous robots using graph search mation obtained by several robots, and dealing with limited
algorithms for exploration. communication[24].Toaddressthesechallenges,RLhasbeen3
applied to enable robots to learn and adapt their exploration C. Bayesian optimization
strategies.
Evaluating the performance of each fleet design is chal-
lenging due to the sheer computation required to obtain π
i
for all i ∈ N using MARL. BO is a suitable approach
B. Partially Observable Markov Games
for optimizing single-objective Markov Games’ design with a
limited number of evaluations [16], [17]. BO sets a Gaussian
The application of RL in multi-robot exploration is signifi-
process (GP) to model the system’s objective F(n) evaluated
cantasitallowsrobotstooptimizetheirexplorationstrategies
at some values of n as prior. The GP is characterized by the
based on the received rewards. This is particularly relevant
mean µ, which is often set to 0, and a kernel function κ that
in scenarios where robots need to coordinate in challenging
captures the flexibility and generalization ability of the GP.
terrainsandlarge-scalegeometries,asitenablesthemtoadapt
By using acquisition functions (AF) as the lower confidence
their behavior to maximize the exploration efficiency [25].
bound (LCB), we can select the next design n to be evaluated
Furthermore, the introduction of RL aligns with the need
by minimizing regret through the optimization process [15].
for robots to learn and adapt to the environment during the
LCB is calculated as follows:
exploration process.
(cid:113)
The multi-robot exploration problem is traditionally mod- LCB(n)=µ∗(n)− 2log(td/2+2π2/3δ)σ∗(n), (3)
eled as a Partially Observable Markov Game (POMG), which
isanextensionofaPartiallyObservableMarkovDecisionPro-
cess (POMDP) [26] to the multi-agent scenario. Specifically, where µ∗(n),σ∗(n) are the posterior mean and the standard
the POMG is defined by the tuple: deviation at n, d is the dimension of search of space n, t is
the iteration number in BO, and δ is a parameter.
<N,S,O,{σ i},A,{R i},T,γ >, (2) The extension of BO for multi-objective problems provides
powerful techniques to search for optimal trade-offs between
where N represents the set of agents with i ∈ N, S is the conflicting black-box objectives [29]. In MOBO, new points
state space shared by all agents, and O ≡O ×...×O and are sequentially added using an infill criterion that guides the
1 N
A ≡ A ×...×A are the observation and action spaces, search towards the Pareto front. The state-of-the-art approach
1 N
withO ⊆S foralli∈N.Moreover,{σ }and{R }represent USeMO[22],generatesaposteriordistributionbyfittingaGP
i i i
theobservationandrewardfunctionsforalli∈N,whereσ : in each of the objectives independently to build a cheap MO
i
S →O andR :S×A×S →R,respectively.Thestochastic problem using the AF, defined as:
i i
state transition model T is given by T :S×A×S →[0,1].
N∗ ←min (AF (n),...,AF (n)), (4)
Finally, γ represents the discount factor. n∈N F1 Fk
At each time-step t, agent i receives an observation o t,i = where N∗ is the Pareto set , n ∈ N is the decision variable,
σ (s )∈O whichcorrespondstoaportionoftheglobalstate
i t i AF (n) is the AF of the objective F , where j ∈ [1,...,k].
s ∈ S. Each agent employs a stochastic policy π to select
Fj j
t i This formulation allows the use of computationally efficient
an action a ∼ π (·|o ). The agents’ joint actions a =
t,i i t,i t solvers to obtain the Pareto front of solutions, as the GP-AF
(a ,...,a ) ∈ A and the current state s are used in the
t1 tN t is given to any point in the search space.
transition model to generate the next state s ∼T(·|s ,a ).
t+1 t t
The obtained reward r =R (·|s ,a ,s ) is then provided
t,i i t t t+1
IV. PROBLEMDEFINITION
to agent i.
Theprimaryobjectiveofeachagentistomaximizethesum In this work, we consider physical heterogeneous fleets
of discounted rewards V = (cid:80)T γkr over an episode [30],whereeachtypeofrobotpossessesadistinctivecombina-
i k=0 t+k,i
withafiniteorpotentiallyinfinitehorizonT.Eachagentseeks tionofhardwarecomponentsthatdiffersfromtheothertypes.
to optimize its value function V (o ) = E [V |o ], which We define the POMG to consider multiple types of agents as
i t,i πi i t,i
represents the expected return starting from observations o follows:
t,i
and following policy π i. • N isthesetoffleets,witheachfleetn∈N representedas
Recent work in RL for multi-robot exploration has focused anM-dimensionalvector.Here,M representsthenumber
on the use of centralized training for decentralized execution of available types of agents and each entry in the vector
(CTDE) [27]. This approach uses a centralized controller to is the number of agents of a specific type.
coordinate the actions of multiple robots and learn a global • S is the global state of the system, which collects the
policy. This centralized training allows for the exchange of individualobservationoftheagentsandtheenvironment.
information and coordination among the robots, leading to • o i considers the local information agent i has collected
improved exploration efficiency and performance [28]. How- giventhecharacteristicofitssensor.Robotscanexchange
ever,duringtheexecutionphase,nocentralizedinformationis local information if they are close to each other.
required, and each robot acts autonomously based on its own • A assumes robots can take actions to move in any of the
observations and learned policy. This decentralized execution fourcardinaldirectionsandalsodiagonallytoexplorethe
is desirable in many multi-robot systems as it reduces the environment.
reliance on communication and allows for robust and scalable • R agents receive a reward based on the area explored at
operation. every time step and the total distance moved as in [28].4
The reward function promotes exploration of the whole
environment while minimizing the traveled distance.
The fleet design problem can be modeled as a single-
objectiveproblemmin F(n)=P(n)+C(n),whereP(n)
n∈N
is the fleet performance and C(n) is the acquisition cost for
thefleetn.However,inthecontextofBayesianOptimization,
conflicting objectives are better captured by formulating the
problem as multi-objective [31]. Therefore, we model the
trade-off between performance and cost as a multi-objective
optimization problem, defined as min [P(n),C(n)].
n∈N
As mentioned in Section III-A, the performance of the
multi-robot system is often measured as the total time to
explore the environment. The system performance depends
on the joint policy of the robots in the fleet π. Obtaining
π is not trivial, as we have to solve the POMG using
MARL algorithms, which are computationally expensive in
large state and action spaces. As a result, in this work, we
assumethefleetperformanceP(n)asanexpensive-to-evaluate
function. For the cost function, we follow a linear acquisition
cost C(n) = An, where A is a M-dimensional column
vector containing the acquisition cost for all the M types Fig. 1. BOFD architecture using MO BO to obtain optimal fleet n at the
of exploration robots. This formulation means that the multi- upperlevelandthejointpolicyπissoughtforthegivennatthelowerlevel
multi-robotsystem.
objective problem is a hybrid with one black-box function,
P(n), and a known function C(n). We leverage the hybrid
objective to propose a tailor-made method for the fleet design
a candidate to evaluate its performance, for this purpose, we
problem,isolatingtheuncertaintyfromtheblack-boxfunction
select the solution that minimizes F˜(n). The selection rule
and building a cheap MO problem using the AF of P(n).
from the Pareto front prioritizes fleets with high potential
and high uncertainty. This strategy ultimately works towards
V. METHODOLOGY
minimizing uncertainty in the predicted MO function.
We introduce a novel framework, named Bayesian Opti-
Since we present the MO problem as a minimization task,
mization for Fleet Design (BOFD), tailored to address fleet
we propose LCB as the AF. LCB is a weighted difference of
design challenges through the utilization of Multi-Objective
theexpectedperformanceofthefleetcapturedbyµ∗(n)ofthe
BayesianOptimizationformulti-robotsystems.Inthissection,
GP, and of the uncertainty captured by the standard deviation
westartbygivinganoverviewofBOFD.Then,wedelveinto
of the GP σ∗(n). The balance between exploitation and ex-
the specifics of its main components. Afterward, we offer a
plorationisdynamicallyadjustedbyfollowingtheformulation
theoreticalanalysisofBOFD,focusingonitsasymptoticregret
providedinEquation3.Forthegeneticalgorithm,wepropose
bounds.
Non-dominatedSortingGeneticAlgorithmII(NSGA-II)[33],
a widely used multi-objective evolutionary algorithm that has
A. Bayesian optimization for fleet design been applied successfully in the context of MOBO [22].
The proposed architecture for BOFD, shown in Figure 1, NSGA-II employs a combination of genetic operators, such
addresses fleet design as a multi-objective problem at the as selection, crossover, and mutation, to evolve a population
upper level to find promising fleet composition n and then, of candidate solutions. The algorithm utilizes a fast non-
atthelowerlevel,weobtainthesystemperformanceP(n)by dominated sorting technique to rank individuals based on
computing agents’ joint policy π for the given fleet n. their dominance relationships, allowing for the identification
At the upper level, we set as posterior a GP over the of Pareto-optimal solutions in low computational time.
performanceofthefleetsrepresentingthekernelastheproduct Forsolvingthelowerlevel,weproposetheCTDEalgorithm
of M squared exponential (SE) kernels, which works well Multi-Agent Deep Deterministic Policy Gradient (MADDPG)
in discrete search with multiple variables [32]. We build a [34].InMADDPG,eachagenthasitsactor-networkthatmaps
cheap MO problem as in [22], using the AF score of the observations to actions and a centralized critic network that
performance and the known cost function. We leverage the estimates the value function based on the joint actions of
cheap MO information to generate a biased initial population all agents. This centralized critic network allows agents to
scoreforthegeneticalgorithm,employingthesingle-objective learn from the experiences of other agents, enabling them
formulation F˜(n) = AF(n)+C(n). This method of initial to better predict the actions that other robots will take [28].
population assessment promotes fleets with high potential and The actor-network is trained using the deterministic policy
high uncertainty, where the later is captured by the AF. Once gradient algorithm, which updates the network parameters to
the genetic algorithm obtains the Pareto front of the cheap maximizetheexpectedreturn.Thisapproachallowstheagents
MO problem, we have to select a solution from the front as tolearncomplexpoliciesthatcanadapttodifferentexploration5
scenarios. unknown function over the space N and F (n) a known cost
2
Algorithm 1 provides a functional algorithmic pseudo code function over the space N.
for BOFD. The integration of multi-objective Bayesian opti- Theorem 1. Let n∗ be a solution in the Pareto set N∗
mization and MADDPG enables the exploration of the design and n a solution in the Pareto set N of the cheap multi-
t t
space to identify promising fleet configurations, while the objective problem obtained at the tth iteration of BOFD.
MARL algorithm enables the robots to learn and refine their Let R(n∗) = ||R (n∗),R (n∗)||, where ||.|| is the norm
1 2
policies based on the feedback received from the environment of the vector, R (n∗) = (cid:80)Tmax(F (n ) − F (n∗)) and
1 t=1 1 t 1
andtheinteractionwithdifferentagents.Wehighlightthatthe R (n∗)=(cid:80)Tmax(F (n )−F (n∗)),thenthefollowingholds
2 t=1 2 t 2
selection of AF, genetic algorithm and MARL algorithm can with probability 1−γ:
be replaced by any other method without loss of generality.
(cid:113) (cid:112)
R(n∗)≤ kT β γi +maxAD T M, (6)
max Tmax Tmax
a
max
Algorithm 1 BOFD Framework
Inputs: Multi-objective function [P(n),C(n)], the number where k is a constant and γ Ti
max
is the maximum information
of total evaluations T max, and the initial number of evalu- gain about F 1 after T max iterations. The exploration factor
ations t , POMG <N,S,O,{σ },A,{R },T,γ >. is defined as β = 2log(Mπ2t2/6δ) where δ ∈ [0,1] is a
0 i i
Initialize a GP(0,(cid:81)M k (n,n′)) prior on P(n). confidence parameter. A is a M-dimensional vector A =
Set t←t 0. j=1 j [a 1,...,a M]T where each entry is the acquisition cost of each
Evaluate [P(n),C(n)] at t randomly chosen n to obtain a robot type, and D is the size of the decision space.
0
prior set {[P(n ),C(n )],··· ,[P(n ),C(n )]}. Proof. We aim to show a sub-linear regret for the multi-
1 1 t0 t0
while t≤T do objective problem by proving a sub-linear upper bound for
max
Compute a GP posterior probability distribution of P(n) regretineachobjectiveasin[22].Inthisproof,wefirstderive
conditioned on the prior set. the upper bound for R 1(n∗) and then R 2(n∗).
Obtain AF score LCB(n). Let F 1 be modeled using GP-LCB, then the cheap multi-
Compute the population score as F˜(n) = LCB(n) + objective optimization problem is defined as:
C(n).
min (LCB (n),F (n)), (7)
Calculate Pareto frontier N of the cheap MO problem n∈N t 2
t
Fˆ(n)=[LCB(n),C(n)]usingNSGA-IIinitializedwith
then, the lower confidence bound (LCB) acquisition function
the highest F˜(n) scores. for F at any iteration t can be defined as:
1
Obtain the next fleet to be evaluated by selecting:
Rn′ un=arg Mm Ain Dn∈ DN Pt GF˜(n) t.
o solve the POMG <
LCB t(n)=µ t−1(n)−β t1/2σ t−1(n). (8)
n′,S,O,{σ i},A,{R i},T,γ > and obtain the joint According to Lemma 5.1 of [15], the following inequality
policy π. holds with probability 1−γ:
Evaluate [P(n′),C(n′)] given π and append to the prior
set. |F 1(n)−µ t−1(n)|≤β t1/2σ t−1(n). (9)
t←t+1.
UndertheassumptionofoptimalityofN ,eitherthereexist
end while t
a n ∈N such that:
Return n that minimizes F(n). t t
LCB (n )≤LCB (n∗), (10)
t t t
or n∗ is in the optimal Pareto set N generated by multi-
B. Theoretical Analysis t
objective solver (i.e., n =n∗).
t
In this section, we discuss the theoretical properties of the Using lemma 1 of [22] for function F :
1
BOFDframework,specificallytheupperboundfortotalregret
andtheeffectofthebiasinitializationusingasingle-objective LCB t(n t)≤LCB t(n∗)≤F 1(n∗), (11)
score for the initial population.
then, according to Lemma 5.2 from [15]
We consider the multi-objective bound within the frame-
work of LCB as an AF, as presented in [22]. This approach R (n∗)=F (n )−F (n∗)≤F (n )−LCB (n ), (12)
1 1 t 1 1 t t t
is an extension of the cumulative regret measure initially
introduced for single-objective BO in [15]. Given the unique R (n∗)≤F (n )−µ (n)+β1/2σ (n), (13)
1 1 t t−1 t t−1
characteristics of the fleet design problem, our primary objec-
tive is to establish a sub-linear upper bound for cumulative R 1(n∗)≤2β t1/2σ t−1(n), (14)
regret. We generalize the multi-objective problem definition
considering Lemma 5.4 from [15], R (n∗) ≤
to: (cid:113) 1
aT β γi with probability 1−γ.
min n∈N(F 1(n),F 2(n)), (5) max Tmax Tmax
Now,weproceedtoderiveanupperboundforR (n∗).We
2
where n ∈ N is an M-dimensional vector containing the proceed to show that F is a Lipschitz continuous function in
2
number of agents of each type in the fleet. F (n) is an a bounded decision space. We aim to find an L such that for
16
any pair of vectors n 1 and n 2 in the d-dimensional space, the • Naive BO (NBO): Implements a rounding operation in
Lipschitz continuity condition holds: the recommended design to achieve a feasible discrete
fleet.
|F (n )−F (n )|≤L∗||n −n ||, (15)
2 1 2 2 1 2 • Discrete-BO (DBO) [38]: Optimizes the exploration fac-
By the definition of cost function, we have F (n)=A∗n. tor of the acquisition function (AF) and the length scale
2
Then, of a covariance function to prevent premature repetition
of designs.
|F 2(n 1)−F 2(n 2)|=|A∗n 1−A∗n 2|, (16) • Probabilistic Reparametrization (PR) [39]: Instead of
directly optimizing the AF over discrete designs, it op-
SinceAisaM-dimensionalpositiveconstant,theLipschitz
timizes the expectation of the AF over a probability
constant L depends on the components of A=[a ,...,a ]T
1 M distribution defined by continuous parameters.
and we can set the upper bound to the maximum of the
absolute values of the components of A: WhilebothPRandDBOhavedemonstratedsuperiorperfor-
mance compared to NBO in various problems, there is a lack
|A∗n −A∗n |=|A∗(n −n )|, (17)
1 2 1 2 ofbenchmarkstudiescomparingthem.Theimplementationof
PRisbasedontheofficialrepository1,andtheimplementation
|A∗(n −n )|≤maxA||n −n ||. (18)
1 2 a 1 2 of DBO is derived from a public repository 2. NBO and
TheLipschitzconstantsatisfiestheLipschitzcontinuitycon- USeMO are implemented based on the descriptions provided
dition for all pairs of vectors n and n in the d-dimensional in the papers by [17] and [22], respectively.
1 2
space. Following [35], we can express the upper bound for We consider two primary design features in the exploration
R (n∗) as follows: robots: sensor range and field of view. Robots with long
2
sensorrangescanperceivealargerportionoftheenvironment,
(cid:112)
R (n∗)≤AD T ∗M. (19)
2 max providing extensive information about distant objects and
obstacles. In contrast, robots with short sensor ranges have
Therefore, the upper bound for total regret presented after
limitedperception,affectingtheirefficiencyinexploringlarger
T iterations is:
max
environments. Similarly, the field of view is a crucial factor.
(cid:113) (cid:112)
R(n∗)≤ aT β γi +maxAD T M. (20) Robots with a wide field of view possess a comprehensive
max Tmax Tmax
a
max
awareness of the environment, detecting objects and obstacles
Theorem 1 states that as the number of iterations increases, fromvariousangles.Ontheotherhand,robotswithanarrower
the regret incurred converges to zero at a sub-linear rate. field of view may need to scan their surroundings more
This property is highly desirable in optimization problems frequently toachieve similarcoverage, impactingtheir overall
because it demonstrates that the approach becomes increas- efficiency.
ingly efficient and effective over time in terms of minimizing Weassumeeachrobotcanhaveeitheralongorshortsensor
cumulative regret [36]. In practical applications like fleet range and a wide or narrow field of view, resulting in four
design, where a large number of evaluations may be involved, distinct types of robots (M = 4). The acquisition cost is
sub-linear regret bounds are a key indicator of the algorithm’s assigned proportionally to the robot’s capabilities, with fully-
practical utility and scalability [37]. The sub-linear upper featured robots being the most expensive and low-featured
bound underscores the robustness and effectiveness of the robots the least expensive.
approach in real-world scenarios, making it a fundamental
To define the search space, we consider the possibility of
metricforassessingtheperformanceoftheBOFDframework.
acquiring up to three robots per type, assuming the absence
of robots in the environment is not a feasible solution. These
VI. EMPIRICALRESULTS assumptions result in a comprehensive search space compris-
In this section, we conduct benchmark studies to assess the ing |N| = 255 distinct fleet compositions. This search space
empirical performance of BOFD in synthetic and simulated complexity surpasses that of [17], where the search space
problems. In the synthetic problem, we have access to the for their single-objective reward parameter is limited to 100
ground truth fleet performance, which allows us to compare different values. This expanded search space adds an extra
the recommended fleet composition and the best fleet com- layerofcomplexitytothefleetdesignproblem,posingamore
position to obtain the regret of each benchmark method. In challenging optimization task.
the simulated problem, we aim to train and deploy each fleet Weconductexperimentsusingfivedistinctrandomseedsto
in different environments to assess their performance without evaluate the performance of BOFD across various scenarios,
knowing the optimal fleet. These experiments are particularly providing a comprehensive understanding of its capabilities.
relevantastheyaddressfundamentalcharacteristicsofrobotics For each seed, a unique set of five initial random fleet
environments and multi-robot exploration tasks. compositions is generated, as detailed in Table I. These initial
The benchmark study involves a variant of the BOFD random fleets serve as the prior information required by
framework utilizing USeMO [22] to address the fleet design BO algorithms. This information is used to fit a Gaussian
problem at the upper level. To extend the single-objective bi-
level optimization framework for reward design from [17] to 1https://github.com/facebookresearch/bo pr
discrete optimization, we propose three distinct approaches: 2https://github.com/huuphuc2609/DiscreteBO/tree/master7
TABLEI TABLEII
PRIORFLEETSPERRANDOMSEED.EACHENTRYINTHEARRAYSHOWS REGRETINEACHSETOFRANDOMPRIORS
THENUMBEROFAGENTSPERTYPE.
Tmax=10 P1 P2 P3 P4 P5 Avg.
Prior Fleet1 Fleet2 Fleet3 Fleet4 Fleet5 NBO 112.9 39.03 35.38 233.23 5.79 85.27
1 [2,0,0,3] [0,3,3,0] [1,1,1,2] [3,2,2,1] [3,1,3,1] DBO 39.03 39.03 254.43 39.03 49.01 84.11
2 [0,1,1,3] [2,2,2,1] [3,0,0,2] [1,3,3,0] [1,0,2,0] PR 205.17 0.00 124.99 35.38 137.49 100.61
3 [3,3,2,1] [1,0,0,3] [0,2,3,0] [2,1,1,2] [2,2,0,2] USeMO 5.79 0.00 112.90 293.65 0.00 82.47
4 [1,3,1,2] [2,0,2,1] [3,2,0,3] [0,1,3,0] [0,2,2,0] BOFD 0.00 0.00 0.00 0.00 0.00 0.00
5 [1,3,3,0] [3,1,0,3] [2,2,2,1] [0,0,1,2] [0,2,0,2] Tmax=15 P1 P2 P3 P4 P5 Avg.
NBO 39.03 39.03 35.38 112.90 5.79 46.43
DBO 39.03 39.03 205.17 39.03 49.01 74.25
PR 205.17 0.00 124.99 35.38 137.49 100.61
distribution,whichissubsequentlyupdatedaftereachiteration
USeMO 0.00 0.00 0.00 112.90 0.00 22.58
based on the recommended fleets. BOFD 0.00 0.00 0.00 0.00 0.00 0.00
In the subsequent sections, we provide detailed information Tmax=20 P1 P2 P3 P4 P5 Avg.
on the implementation and present the results of experiments NBO 39.03 39.03 35.38 112.90 5.79 46.43
DBO 39.03 39.03 5.79 39.03 35.38 31.65
conductedinthesyntheticproblemandthesimulatedenviron-
PR 35.38 0.00 124.99 35.39 137.49 66.65
ments. USeMO 0.00 0.00 0.00 0.00 0.00 0.00
BOFD 0.00 0.00 0.00 0.00 0.00 0.00
A. Synthetic Experiments
The fleet design problem involving autonomous robots is to enhance its performance, indicating that single-objective
characterized by an expensive-to-evaluate black-box function, methods encounter difficulties in balancing the trade-off be-
withtheprimarybottleneckbeingthetraininganddeployment tween performance and cost, resulting in the exploration of
of RL agents. In this set of experiments, we bypass the fleets with sub-optimal objective functions.
MARL algorithm by randomly assigning a performance value With a budget of T max = 20, USeMO manages to achieve
to each fleet, creating a known performance distribution. This zero regret across all priors. NBO fails to improve its regret
approach enables us to benchmark the empirical regret of in the additional iterations but still outperforms PR, which
eachmethod.Specifically,wecomparethebest-suggestedfleet exhibits the highest average regret (66.65) and the maximum
within a fixed budget of evaluations against the truly optimal regret in the third prior (124.99). DBO makes significant
fleet. progress, reducing its regret by more than half and providing
Regret is measured as the absolute difference between the lowest average regret among single-objective methods.
Overall, the results underscore the efficacy of multi-
the best fleet found in each method and the optimal fleet
under three different budgets: T = 10, T = 15, objective methods in swiftly reaching optimal fleet designs,
max max
and T = 20. Fleet performance is measured as the time withBOFDcapitalizingMOBObetterthanUSeMO,achieving
max
the optimal solution in five or fewer iterations. Surprisingly,
required to complete exploration, randomly sampled from a
uniform distribution P(n)∼U(100,750) for all n∈N. The NBO demonstrates competitive performance compared to the
specialized single-objective methods, outperforming PR in
acquisition cost for full-feature robots is 150, for low-feature
most scenarios and DBO when T = 15. The promising
robots is 50, for robots with a long sensor range and narrow max
performance of BOFD is further validated in a simulated
field of view is 100, and for robots with a short sensor range
and wide field of view is 75, i.e. A=[150,100,75,50]T. We environment in the subsequent section.
report in Table II the regret given by F(n) = P(n)+C(n)
B. Simulated Experiments
for each different random set of priors to allow comparison
between single-objective and multi-objective methods. In this section, we present benchmark studies conducted
We observe distinctive performance patterns among the in simulated environments, featuring four distinct maps. Each
methods with varying iteration budgets. BOFD stands out as mapisgeneratedusing2-dimensionallayoutsoffourdifferent
the sole method capable of achieving the optimal solution, sizes to introduce various levels of navigation complexity.
yielding zero regret across all priors. The earliest BOFD finds Subsequently, we construct a 3-dimensional representation
the optimal solution is at two iterations and the latest is as illustrated in Figure 2. Map #1 considers 120x120 grids
at five iterations. With a budget of 10 iterations, USeMO with an acquisition cost of A = [400,320,375,300]T.
1
demonstrates commendable performance by achieving zero Map #2 spans 125x125 grids with an acquisition cost
regret in two priors but faces challenges, notably in the fourth of A = [70,60,50,40]T. Map #3 covers an area of
2
prior,leadingtothehighestregretinthesyntheticexperiments. 150x150 grids, accompanied by an acquisition cost of A =
3
Amongsingle-objectivemethods,theiraverageperformanceis [150,120,140,95]T. Lastly, Map #4 extends over 175x175
comparable,withNBOandDBOmostlyinfluencedbyasingle grids with an acquisition cost of A = [285,255,270,240]T.
4
prior. Notably, PR achieves zero regret in one of the priors. The deliberate diversity incorporated into the design of these
Expanding the budget to 15 iterations significantly benefits environments leads to varying fleet performances and costs,
USeMO, allowing it to achieve zero regret in four priors and therebypresentinguniqueoptimizationchallengesforthefleet
substantially reducing regret in the fourth prior. In contrast, designer.
single-objective methods, particularly NBO and DBO, show As in the synthetic experiments, we aim to minimize total
improvementsinonlyalimitednumberofpriors.PRstruggles timetoexplorationwhilealsominimizingthefleetacquisition8
Fig. 2. Maps’ layout for multi-robot exploration. The first row has the 2-
dimensionalversionandthebottomrowhasthe3-dimensionalrepresentation.
TABLEIII
HYPERPARAMETERSFORMADDPG
Hyperparamter Value
Batchsize 60 Fig. 3. Sensor capabilities for each of the four types of robots in a blank
Discountfactor 0.95 environmentof175x175grids.
targetnetworkupdatefactor 0.001
CriticLearningRate 0.001
ActorLearningRate 0.0001
Timesteplimitperepisode 50
Episodes 5000
Fieldofview {5,60}
Sensorrange {10,30}
cost, however, in this experimental setup, the performance of
eachfleetistheresultofthejointpolicylearnedbytrainingthe
robots in the environments using MADDPG. The algorithmic
implementation is based on the public repository3 of the
work of [28]. The Critic architecture considers two layers of
Convolutional Neuronal Networks (CNN), a Long short-term
memory (LSTM) network, and a fully connected layer. The
input layer has |S|+|A| neurons and the output layer is 1-
dimensional. Each Actor follows a similar architecture with
the difference that the input layer has a number of neurons
equivalent to the observation space |σ | of each agent i to
i
allow the decentralized execution. The main hyperparameters
Fig. 4. Average objective value F(n) and standard deviation across five
for MADDPG are shown in the Table III. priorsfor(a)Map#1,(b)Map#2,(c)Map#3and(d)Map#4.
The combination of field of view values and sensor range
provides each type of robot with distinct exploration capa-
bilities, shown in Figure 3. Each fleet is trained to learn a F(n) across all prior sets in T max =20 iterations are shown
jointpolicyusingMADDPGinthe2-dimensionalmap.Then, in Figure 4.
the learned joint policy is deployed in the 3-dimensional en- In Map #1, BOFD demonstrates remarkable efficiency by
vironments implemented in a Gazebo multi-robot simulation, identifying promising fleet configurations within only two
version 11.14, and the robots’ models are built using ROS iterationsacrossallpriors,effectivelyminimizingthestandard
Noetic. We use TurtleBot 3 Burger as the base model for deviation resulting from diverse prior fleets within five itera-
the exploration robots and we modify the Light Detection tions.Regardlessofthepriorset,BOFDconsistentlyidentifies
and Ranging (LiDAR) system parameters for range and field the best solution n = [0,0,0,1], showcasing its robustness
of view to obtain M = 4 types of robots with different and effectiveness. USeMO initially exhibits high standard
capabilities. deviation, indicating a varied exploration across priors, yet it
The complexity of training multiple fleets makes it un- eventually converges to the best fleet in four out of five initial
feasible to have access to the true performance distribution prior sets, closely mirroring PR’s performance. NBO achieves
and compute the regret of each solution. Therefore we report convergence to the same fleet as BOFD in only two prior
the best performance obtained by each method measured as sets, but its tendency to converge early to sub-optimal designs
F(n) = P(n)+C(n) to allow comparison between single- impactsitsoverallperformance.Incontrast,DBOstrugglesto
objective and multi-objective methods. The comparison of the navigate the performance-cost trade-off effectively, failing to
average and the standard deviation of best objective value consistently improve fleet designs.
In Map #2, BOFD stands out as the sole method to attain
3https://github.com/hedingjie/DME-DRL fleet n = [3,1,0,2], delivering the best objective value ob-9
served across all methods and priors. While USeMO initially TABLEIV
discovers better fleets on average within the first seven iter- FLEETCOMPARISONINEACHMAP.
ations, BOFD surpasses it from the eighth iteration onwards.
Map Fleet n Time Cost F
Among single-objective methods, PR converges to solutions BOFDFleet [0,0,0,1] 740 300 1040
with similar objective values as NBO, whereas DBO faces 1 FastestFleet [2,3,3,0] 81 2885 2966
LargestFleet [3,3,3,3] 123 4185 4308
challenges in enhancing fleet designs after seven iterations.
BOFDFleet [3,1,0,2] 231 350 581
Overall,multi-objectivemethodsexhibitsuperiorperformance 2 FastestFleet [3,3,3,3] 129 660 789
compared to single-objective methods in this environment. LargestFleet [3,3,3,3] 129 660 789
BOFDFleet [2,0,1,0] 331 440 771
In Map #3, BOFD exhibits a proactive approach by identi-
3 FastestFleet [2,1,3,2] 165 1030 1195
fying promising fleet designs early in the search process and LargestFleet [3,3,3,3] 289 1515 1804
further improving them in subsequent iterations, ultimately BOFDFleet [0,1,0,0] 932 255 1187
4 FastestFleet [2,3,3,3] 225 2865 3090
securing the best design, n = [2,0,1,0], in four out of five
LargestFleet [3,3,3,3] 292 3150 3442
prior sets. However, in the prior 3, BOFD achieves a slightly
inferior fleet design. While NBO and PR manage to find the
best fleet in two out of the five prior sets, their inability to might lead to sub-optimal actions such as avoiding collisions.
replicate similar objective values across the remaining three BOFD trades 102 time units for savings of 310. For Map
sets affects their overall performance. Conversely, DBO and #3, BOFD trades 166 and 42 time units compared to the
USeMOfailtoreachthefleetofferingthebestobjectivevalue, fastest and largest fleet, respectively, resulting in savings of
resulting in worse average solutions compared to the other 590 and 1,075. Lastly, in Map #4, BOFD trades over 700
three methods. time units to save up to 2,895. The joint policies learned by
In Map #4, we observe a pattern similar to that of Map #1, each fleet in map #4 are shown in the Supplementary Video
where BOFD consistently identifies promising fleet configu- (link: https://youtu.be/sS4iAXMNSqk). Across all scenarios,
rations within only two iterations across all priors. Moreover, BOFD consistently provides fleet designs that offer greater
it requires only five iterations to minimize the standard devia- cost-efficiency for the exploration task at hand.
tion resulting from diverse prior fleets, consistently achieving
the best fleet n = [0,1,0,0]. The noteworthy performance VII. CONCLUSIONS
achieved in a few iterations holds particular significance in We present a pioneering approach for algorithmic fleet
Map #4, given the complexity of this environment. Notably, design with autonomous robots, using MOBO to tackle the
training the agents using MADDPG typically consumes an fleet design problem and a MARL algorithm to train the
average of six hours per fleet configuration, highlighting the robots and evaluate fleet performance. Our method BOFD
substantial time savings facilitated by the BOFD framework. capitalizesontheinherentstructureofthefleetdesignproblem
The results highlight the consistency of BOFD in navigat- by integrating the known cost function and the Acquisition
ing the exploration performance-cost trade-off, yielding fleet Function (AF) of performance to formulate a cheap MO
designs with minimal exploration times and costs. BOFD’s problem. By obtaining the Pareto front, BOFD effectively
performance aligns with the sub-linear regret outlined in navigates the trade-off between conflicting objectives.
Theorem1,showcasingimprovedfleetdesignsastheiteration The versatility of BOFD extends beyond exploration tasks,
budget increases. Moreover, the tailor-made components of making it applicable to various multi-robot problems across
BOFD, such as the bias score F˜, the customized kernel, and diverse domains where fleet design involves balancing com-
the isolation of the uncertainty from the black-box objective, peting objectives. A key area for future research lies in ad-
contribute to its superiority over other methods in addressing dressingtheprimarybottleneckoffleetoptimization—training
fleet design for autonomous robots. agents using MARL algorithms. Introducing transfer learning
We further analyze the trade-off between performance and mechanisms could offer a promising avenue to mitigate this
costineachmaptoevaluatewhethertheadditionalexploration bottleneck, enabling the reuse of learned knowledge across
time corresponds proportionally to a relevant reduction in different fleet designs and reducing the computational burden
acquisition costs of the recommended fleet. To accomplish of the optimization process.
this, we compare the fleet suggested by BOFD against the
largest fleet (highest acquisition cost) and the fastest fleet REFERENCES
(lowestexplorationtime),assumingequalimportanceforboth
[1] J. J. Rolda´n-Go´mez and A. Barrientos, “Special issue on multi-robot
metrics. The comparison for each map is presented in Table systems:Challenges,trends,andapplications,”AppliedSciences,vol.11,
IV. no.24,2021.
[2] P.Gautier,J.Laurent,andJ.-P.Diguet,“Deepq-learning-baseddynamic
In Map #1, BOFD trades 659 time units to achieve savings
management of a robotic cluster,” IEEE Transactions on Automation
of 2,585 compared to the fastest fleet. Similarly, it trades 617 ScienceandEngineering,vol.20,no.4,pp.2503–2515,2023.
time units to realize savings of 3,885, indicating a trade-off [3] X.Bai,A.Fielbaum,M.Kronmu¨ller,L.Knoedler,andJ.Alonso-Mora,
“Group-based distributed auction algorithms for multi-robot task as-
where performance is decreased by fourfold to reduce costs
signment,”IEEETransactionsonAutomationScienceandEngineering,
by 14 times. vol.20,no.2,pp.1292–1303,2023.
Moving to Map #2, we observe it is the only map where [4] S. Al-Hussaini, J. M. Gregory, and S. K. Gupta, “Generating task
reallocation suggestions to handle contingencies in human-supervised
the largest fleet is also the fastest fleet, which aligns with
multi-robot missions,” IEEE Transactions on Automation Science and
the analysis presented in [19] where increasing the fleet size Engineering,vol.21,no.1,pp.367–381,2024.10
[5] A. Mannucci, L. Pallottino, and F. Pecora, “Provably safe multi-robot of subterranean environments using legged and aerial robots,” 2022
coordination with unreliable communication,” IEEE Robotics and Au- InternationalConferenceonRoboticsandAutomation(ICRA),2022.
tomationLetters,vol.4,pp.3232–3239,2019. [26] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and
[6] K.Goel,W.Tabib,andN.Michael,“Rapidandhigh-fidelitysubsurface actinginpartiallyobservablestochasticdomains,”ArtificialIntelligence,
explorationwithmultipleaerialrobots,”inProceedingsofInternational vol.101,no.1,pp.99–134,1998.
SymposiumonExperimentalRobotics(ISER’20),2020,pp.436–448. [27] A. H. Tan, F. P. Bejarano, Y. Zhu, R. Ren, and G. Nejat, “Deep
[7] B.Bru¨ggemann,A.Tiderko,andM.Stilkerieg,“Adaptivesignalstrength reinforcement learning for decentralized multi-robot exploration with
predictionbasedonradiopropagationmodelsforimprovingmulti-robot macro actions,” IEEE Robotics and Automation Letters, vol. 8, no. 1,
navigationstrategies,”Proceedingsofthe2ndInternationalConference pp.272–279,2023.
onRoboticCommunicationandCoordination,2009. [28] D. He, D. Feng, H. Jia, and H. Liu, “Decentralized exploration of
[8] B. Gerkey and M. J. Mataric, “A formal framework for the study of a structured environment based on multi-agent deep reinforcement
taskallocationinmulti-robotsystems,”InternationalJournalofRobotics learning,”in2020IEEE26thInternationalConferenceonParalleland
Research,vol.23,no.9,pp.939–954,2004. DistributedSystems(ICPADS),2020,pp.172–179.
[9] X. Cai, B. Schlotfeldt, K. Khosoussi, N. Atanasov, G. J. Pappas, and [29] J.Qing,I.Couckuyt,andT.Dhaene,“Arobustmulti-objectivebayesian
J. P. How, “Non-monotone energy-aware information gathering for optimization framework considering input uncertainty,” Journal of
heterogeneousrobotteams,”in2021IEEEInternationalConferenceon GlobalOptimization,vol.86,no.3,pp.693–711,2023.
RoboticsandAutomation(ICRA),2021,pp.8859–8865. [30] M. Bettini, A. Shankar, and A. Prorok, “Heterogeneous multi-robot
[10] D.Industries,2023.[Online].Available:https://dabit.industries/products/ reinforcementlearning,”arXivpreprintarXiv:2301.07137,2023.
turtlebot-3-burger [31] L.Shu,P.Jiang,X.Shao,andY.Wang,“Anewmulti-objectivebayesian
[11] B. Dynamics, 2023. [Online]. Available: https://bostondynamics.com/ optimizationformulationwiththeacquisitionfunctionforconvergence
products/spot/ anddiversity,”JournalofMechanicalDesign,vol.142,2020.
[32] D.Duvenaud,“Automaticmodelconstructionwithgaussianprocesses,”
[12] I.D.Miller,A.Cohen,A.Kulkarni,J.Laney,C.J.Taylor,V.Kumar,
Ph.D.dissertation,UniversityofCambridge,2014.
F.Cladera,A.Cowley,S.S.Shivakumar,E.S.Lee,L.Jarin-Lipschitz,
[33] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A fast and elitist
A. Bhat, N. Rodrigues, and A. Zhou, “Mine tunnel exploration using
multiple quadrupedal robots,” IEEE Robotics and Automation Letters, multiobjective genetic algorithm: Nsga-ii,” IEEE Transactions on Evo-
lutionaryComputation,vol.6,no.2,pp.182–197,2002.
vol.5,pp.2840–2847,2020.
[34] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mordatch,
[13] M. S. d. Silva, L. Clark, V. Thangavelu, J. A. Edlund, K. Otsu,
G. J. Correa, V. S. Varadharajan, A`. Santamaria-Navarro, T. Touma, “Multi-agent actor-critic for mixed cooperative-competitive environ-
ments,” Advances in neural information processing systems, vol. 30,
A.Bouman,H.Melikyan,T.Pailevanian,S.Kim,A.Archanian,T.S.
2017.
Vaquero,G.Beltrame,N.Napp,G.Pessin,andA.Agha–mohammadi,
[35] S.Bubeck,“Convexoptimization:Algorithmsandcomplexity,”Found.
“Achord:communication-awaremulti-robotcoordinationwithintermit-
TrendsMach.Learn.,vol.8,no.3–4,p.231–357,2015.
tent connectivity,” IEEE Robotics and Automation Letters, vol. 7, pp.
[36] N. Srinivas, A. Krause, S. Kakade, and M. Seeger, “Information-
10184–10191,2022.
theoreticregretboundsforgaussianprocessoptimizationinthebandit
[14] G.Qu,Y.Lin,A.Wierman,andN.Li,“Scalablemulti-agentreinforce-
setting,”IEEETransactionsonInformationTheory,vol.58,pp.3250–
mentlearningfornetworkedsystemswithaveragereward,”Advancesin
3265,2012.
NeuralInformationProcessingSystems,vol.33,pp.2074–2086,2020.
[37] B.Shahriari,K.Swersky,Z.Wang,R.Adams,andN.Freitas,“Taking
[15] N. Srinivas, A. Krause, S. Kakade, and M. Seeger, “Gaussian process
thehumanoutoftheloop:areviewofbayesianoptimization,”Proceed-
optimizationinthebanditsetting:Noregretandexperimentaldesign,”
ingsoftheIEEE,vol.104,pp.148–175,2016.
in Proceedings of the 27th International Conference on International
[38] P.Luong,S.Gupta,D.Nguyen,S.Rana,andS.Venkatesh,“Bayesian
ConferenceonMachineLearning,2010,p.1015–1022.
optimizationwithdiscretevariables,”inAI2019:AdvancesinArtificial
[16] D. Mguni, J. Jennings, E. Sison, S. Valcarcel Macua, S. Ceppi, and
Intelligence: 32nd Australasian Joint Conference, Adelaide, SA, Aus-
E.MunozdeCote,“Coordinatingthecrowd:Inducingdesirableequilib-
tralia,December2–5,2019,Proceedings32,2019,pp.473–484.
riainnon-cooperativesystems,”inProceedingsofthe18thInternational
[39] S. Daulton, X. Wan, D. Eriksson, M. Balandat, M. A. Osborne, and
Conference on Autonomous Agents and MultiAgent Systems, 2019, pp.
E. Bakshy, “Bayesian optimization over discrete and mixed spaces
386–394.
via probabilistic reparameterization,” Advances in Neural Information
[17] Z.ShouandX.Di,“Rewarddesignfordriverrepositioningusingmulti-
ProcessingSystems,vol.35,pp.12760–12774,2022.
agentreinforcementlearning,”TransportationresearchpartC:emerging
technologies,vol.119,p.102738,2020.
[18] S.M.Henry,M.J.Hoffman,L.Waddell,andF.M.Muldoon,“Holistic
fleet optimization incorporating system design considerations,” Naval
ResearchLogistics(NRL),vol.70,pp.675–690,2023.
[19] Z.Yan,L.Fabresse,J.Laval,andN.Bouraqadi,“Teamsizeoptimization
formulti-robotexploration,”inSimulation,Modeling,andProgramming
for Autonomous Robots: 4th International Conference, SIMPAR 2014,
Bergamo,Italy,October20-23,2014.Proceedings4. Springer,2014,
pp.438–449.
[20] J.BarriosandJ.Godier,“Fleetsizingforflexiblecarsharingsystems,”
TransportationResearchRecordJournaloftheTransportationResearch
Board,vol.2416,pp.1–9,2014.
[21] F. Cabrera-Mora and J. Xiao, “Fleet size of multi-robot systems for
explorationofstructuredenvironments,”in2014IEEE/RSJInternational
ConferenceonIntelligentRobotsandSystems,2014,pp.370–375.
[22] S. Belakaria, A. Deshwal, N. K. Jayakodi, and J. R. Doppa,
“Uncertainty-awaresearchframeworkformulti-objectivebayesianopti-
mization,”ProceedingsoftheAAAIConferenceonArtificialIntelligence,
vol.34,no.06,pp.10044–10052,2020.
[23] S. S. K. Ahreum Lee, Wonse Jo and B.-C. Min, “Investigating the
effectofdeicticmovementsofamulti-robot,”InternationalJournalof
Human–ComputerInteraction,vol.37,no.3,pp.197–210,2021.
[24] Q. Bi, X. Zhang, J. Wen, Z. Pan, S. Zhang, R. Wang, and J. Yuan,
“Cure: A hierarchical framework for multi-robot autonomous explo-
ration inspired by centroids of unknown regions,” IEEE Transactions
onAutomationScienceandEngineering,pp.1–14,2023.
[25] M.Kulkarni,M.Dharmadhikari,M.Tranzatto,S.Zimmermann,V.Rei-
jgwart, P. D. Petris, H. Nguyen, N. Khedekar, C. Papachristos, L. Ott,
R.Siegwart,M.Hutter,andK.Alexis,“Autonomousteamedexploration