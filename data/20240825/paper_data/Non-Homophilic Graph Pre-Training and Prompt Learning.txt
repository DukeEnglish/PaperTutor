Non-Homophilic Graph Pre-Training and Prompt Learning
XingtongYu∗ JieZhang∗
SingaporeManagementUniversity NationalUniversityofSingapore
Singapore Singapore
starlien0905@gmail.com jiezhang_jz@u.nus.edu
YuanFang† RenheJiang†
SingaporeManagementUniversity TheUniversityofTokyo
Singapore Japan
yfang@smu.edu.sg jiangrh@csis.u-tokyo.ac.jp
Abstract
enterthecorrectconferencetitlefromyourrightsconfirmationemai(Confer-
Graphsareubiquitousformodelingcomplexrelationshipsbetween enceacronym’XX).ACM,NewYork,NY,USA,15pages.https://doi.org/
XXXXXXX.XXXXXXX
objectsacrossvariousfields.Graphneuralnetworks(GNNs)have
becomeamainstreamtechniqueforgraph-basedapplications,but 1 Introduction
theirperformanceheavilyreliesonabundantlabeleddata.Tore-
ducelabelingrequirement,pre-trainingandpromptlearninghas Graphdataarepervasiveinreal-worldapplications,suchascitation
becomeapopularalternative.However,mostexistingpromptmeth- networks[18,56],socialnetworks[16,68],andmoleculargraphs
odsdonotdifferentiatehomophilicandheterophiliccharacteristics [21,51].Traditionalmethodstypicallytraingraphneuralnetworks
ofreal-worldgraphs.Inparticular,manyreal-worldgraphsare (GNNs) [20, 47] or graph transformers [59, 66] in a supervised
non-homophilic, not strictly or uniformly homophilic with mix- manner.However,theyrequirere-trainingandsubstantiallabeled
inghomophilicandheterophilicpatterns,exhibitingvaryingnon- dataforeachspecifictask.
homophiliccharacteristicsacrossgraphsandnodes.Inthispaper, Tomitigatethelimitationsofsupervisedmethods,pre-training
weproposeProNoG,anovelpre-trainingandpromptlearning methods have gained significant traction [15, 36, 48, 60]. They
frameworkforsuchnon-homophilicgraphs.First,weanalyzeex- firstlearnuniversal,task-independentpropertiesfromunlabeled
istinggraphpre-trainingmethods,providingtheoreticalinsights graphs,andthenfine-tunethepre-trainedmodelstovariousdown-
intothechoiceofpre-trainingtasks.Second,recognizingthateach streamtasksusingtask-specificlabels[48,60].However,asignif-
nodeexhibitsuniquenon-homophiliccharacteristics,wepropose icantgapoccursbetweenthepre-trainingobjectivesanddown-
aconditionalnetworktocharacterizethenode-specificpatterns streamtasks,resultinginsuboptimalperformance[44,61].More-
indownstreamtasks.Finally,wethoroughlyevaluateandanalyze over,fine-tuninglargepre-trainedmodelsiscostlyandstillrequires
ProNoGthroughextensiveexperimentsontenpublicdatasets. sufficienttask-specificlabelstopreventoverfitting.Asanalter-
nativetofine-tuning,promptlearninghasemergedasapopular
CCSConcepts parameter-efficienttechniqueforadaptationtodownstreamtasks
[7,26,42,43,46,62].Theyfirstutilizeauniversaltemplatetounify
•Informationsystems→Webmining;Datamining;•Com-
pre-traininganddownstreamtasks.Then,alearnablepromptis
putingmethodologies→Learninglatentrepresentations.
employedtomodifytheinputfeaturesorhiddenembeddingsof
Keywords thepre-trainedmodeltoalignwiththedownstreamtaskwithout
updatingthepre-trainedweights.Sinceaprompthasfarfewer
Graphmining,non-homophilicgraph,promptlearning,pre-training, parametersthanthepre-trainedmodel,promptlearningcanbe
few-shotlearning. especiallyeffectiveinlow-resourcesettings[61].
ACMReferenceFormat: However, current graph “pre-train, prompt” approaches rely
XingtongYu∗,JieZhang∗,YuanFang†,andRenheJiang†.2018.Non-Homophilic on the homophily assumption or overlook the presence of het-
GraphPre-TrainingandPromptLearning.InProceedingsofMakesureto erophilicedges.Specifically,thehomophilyassumption[29,75]
statesthatneighboringnodesshouldsharethesamelabels,whereas
∗Co-firstauthors.WorkwasdonewhileattheUniversityofTokyo. heterophilyreferstotheoppositescenariowheretwoneighboring
†Correspondingauthors.
nodeshavedifferentlabels.Weobservethatreal-worldgraphsare
typicallynon-homophilic,meaningtheyareneitherstrictlyoruni-
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
formlyhomophilicandmixbothhomophilicandheterophilicpatterns
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation [54,55].Inthiswork,weinvestigatethepre-trainingandprompt
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe learningmethodologyfornon-homophilicgraphs.Wefirstrevisit
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
existing graph pre-training methods for such graphs, followed
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. byproposingaPromptlearningframeworkforNon-homophilic
Conferenceacronym’XX,June03–05,2018,Woodstock,NY Graphs(orProNoGinshort).Thesolutionisnon-trivial,asthe
©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
notionofhomophilyencompassestwokeyaspects,eachwithits
ACMISBN978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX ownuniquechallenge.
4202
guA
22
]GL.sc[
1v49521.8042:viXraConferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.,XingtongYu∗,JieZhang∗,YuanFang†,andRenheJiang†
performance.Forexample,astandardgraphpromptlearningap-
proach[26]generallyperformsworsewhenthehomophilyratios
1  0
ofnodesdecrease,asshowninFig.1(d),evenwithanon-homophily
3  2 pretexttask[60].Thoughsomerecentworks[5,46]haveproposed
node-specificprompts,theyarenotdesignedtoaccountforthe
3  2 variationinnodes’non-homophiliccharactersitics.Inspiredbycon-
ditional prompt learning [71], we propose generating a unique
Lab  e0 l Music Sports M  o1 vies promptfromeachnodewithaconditionalnetwork(condition-net)
Cora Citeseer PROTEINS Cornell Chameleon Wisconsin HomopC hl ia lcss
edge ratio
Gender Hobby Wto ec fiap rst tu cr ae pt th ue refi tn he e-g nr oa ni -n he od m,d oi ps hti in lic ct pc ah tta er ra nc ste or fis et ai cc hso nf odea ec bh yn ro ead de -.
(a) Varying non-homophilic patterns (b) Dependence of 2ho7moph4il7iy ratio
ingoutitsmulti-hopneighborhood.Then,conditionedonthese
across different graphs on the target label
non-homophilicpatterns,thecondition-netproducesaseriesof
prompts,oneforeachnodethatreflectsitsvaryingnon-homophilic
characteristics.Thesepromptscanadjustthenodeembeddingsto
betteralignthemwiththedownstreamtask.
Insummary,thecontributionsofthisworkarethreefold:(1)
Weobservevaryingdegreesofhomophilyacrossgraphs,which
motivatesustorevisitgraphpre-trainingtasks.Weprovidedtheo-
reticalinsightswhichguideustochoosenon-homophilytasksfor
graphpre-training.(2)Wefurtherobservethat,withinthesame
graph,differentnodeshavediversedistributionsofnon-homophilic
characteristics.Toadapttotheuniquenon-homophilicpatternsof
(c) Diverse non-homophilic patterns (d) Performance of standard graph
across nodes in the same graph prompt across nodes eachnode,weproposetheProNoGframeworkfornon-homophilic
Figure1:Non-homophiliccharacteristicsofgraphs. promptlearning,whichisequippedwithacondition-nettogener-
ateaseriesofpromptsconditionedoneachnode.Thenode-specific
promptsenablesfine-grained,node-wiseadaptationforthedown-
First,differentgraphsexhibitvaryingdegreesofnon-homophily.
streamtasks.(3)Weperformextensiveexperimentsontenbench-
AsshowninFig.1(a),theCoracitationnetworkthatisgenerallycon-
markdatasets,demonstratingthesuperiorperformanceofProNoG
sideredlargelyhomophilicwith81%homophilicedges1,whereas
comparedtoasuiteofstate-of-the-artmethods.
theWisconsinwebpagegraphlinksdifferentkindsofwebpages,
whichishighlyheterophilicwithonly21%homophilicedges.More-
over,thenon-homophiliccharacteristicsofagraphalsodependson 2 RelatedWork
thetargetlabel.Forexample,inadatingnetworkshowninFig.1(a),
takinggenderasthenodelabel,thegraphismoreheterophilic Graphrepresentationlearning.GNNs[11,20,47,57,63]are
with2/7homophilicedges.However,takinghobbiesasthenode mainstreamtechniqueforgraphrepresentationlearning.Theytyp-
label,thegraphbecomesmorehomophilicwith4/7homophilic icallyoperateonamessage-passingframework,wherenodesit-
edges.Hence,howdowepre-trainagraphmodelirrespectiveofthe eratively update their representations by aggregating messages
graph’shomophilycharacteristics? Inthiswork,weproposedefi- receivedfromtheirneighboringnodes.However,theeffectiveness
nitionsforhomophilytasksandhomophilysamples.Weshowthat ofGNNsheavilyreliesonabundanttask-specificlabeleddataand
pre-trainingwithnon-homophilysamplesincreasesthelossofany requiresre-trainingforvarioustasks.Inspiredbythesuccessof
homophilytask.Meanwhile,alesshomophilicgraphresultsina pre-trainingmethodsinthelanguage[4,6,9,40]andvision[1,67,
highernumberofnon-homophilysamples,subsequentlyincreas- 71,72]domains,pre-trainingmethods[14,15,19,27,36,45,48,60]
ingthepre-traininglossforhomophilytasks.Thismotivatesusto havebeenwidelyexploredforgraphs.Thesemethodsfirstpre-
moveawayfromhomophilytasksforgraphpre-training[26,42] trainagraphencoderbasedonself-supervisedtasks,thentransfer
andinsteadchooseanon-homophilytask[54,60]. priorknowledgetodownstreamtasks.However,alltheseGNNs
Second,differentnodeswithinthesamegrapharedistributed andpre-trainingmethodsarebasedonthehomophilicassumption,
differently in terms of their non-homophilic characteristics. As overlookingthatreal-worldgraphsaregenerallynon-homophilic.
shown in Fig. 1(c), on both Cora and Cornell, their nodes have Non-homophilicgraphlearning.ManyGNNs[2,28,29,35,73–
adiversehomophilyratios2.Hence,howdowecapturethefine-
75]havebeenproposed fornon-homophilic graphs,employing
grained,node-specificnon-homophiliccharacteristics? Duetothe
methodssuchascapturinghigh-frequencysignals[2],discover-
diversecharacteristicsacrossnodes,aone-size-fits-allsolutionfor
ingpotentialneighbors[17,33],andhigh-ordermessagepassing
allnodeswouldbeinadequate.However,existingapproachesgen-
[75].Moreover,recentworkshaveexploredpre-trainingonnon-
erallyapplyasingleprompttoallnodes[7,26,42,43],treatingall
homophilicgraphs[13,54,55]bycapturingneighborhoodinfor-
nodesuniformly.Thus,thesemethodsoverlookthefine-grained
mationtoconstructunsupervisedtasksforpre-trainingthegraph
node-wisenon-homophiliccharacteristics,leadingtosuboptimal
encoderandthentransferringpriornon-homophilicknowledge
1Definedasedgesconnectingtwonodesofthesamelabel;seeEq.(1)inSect.3. todownstreamtasksthroughfine-tuningwithtask-specificsuper-
2Definedasthefractionofanode’sneighborswiththesamelabel;seeEq.(2)inSect.3. vision.However,asignificantgapexistsbetweentheobjectivesNon-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronym’XX,June03–05,2018,Woodstock,NY
ofpre-trainingandfine-tuning[23,44,61].Whilepre-trainingfo- where𝜃𝑙 arethelearnableparametersinthe𝑙-thlayer,andAggr(·)
cusesonlearninginherentgraphattributeswithoutsupervision, istheaggregationfunction,whichcantakevariousforms[11,20,47,
fine-tuningadaptstheseinsightstodownstreamtasksbasedon 57,63].Inthefirstlayer,theinputnodeembeddingh0 𝑣 istypically
task-specificsupervision.Thisdiscrepancyhinderseffectiveknowl- initializedfromthenodefeaturevectorx𝑣.Thefullsetoflearnable
edgetransferandnegativelyimpactsdownstreamperformance. parametersisdenotedasΘ={𝜃1,𝜃2,...}.Forsimplicity,wedefine
Graphpromptlearning.Originallydevelopedforthelanguage theoutputnoderepresentationsofthefinallayerash𝑣,whichcan
domain,promptlearningeffectivelyunifiespre-traininganddown- thenbefedintothelossfunctionforaspecifictask.
streamobjectives[4,22,24].Recently,graphpromptlearninghas Problemstatement.Inthiswork,weaimtopre-trainagraphen-
emergedasapopularalternationtofine-tuningmethods[7,26, coderanddevelopapromptlearningframeworkfornon-homophilic
42,43,46,62,64].Thesemethodsfirstproposeaunifiedtemplate, graphs.Morespecifically,boththepre-trainingandpromptlearn-
thendesignpromptsspecificallytailoredtoeachdownstreamtask, ingarenotsensitivetothehomophiliccharacteristicsofthegraph
allowingthemtobetteralignwiththepre-trainedmodelwhile anditsnodes.
keepingthepre-trainedparametersfrozen.However,currentgraph Toevaluateournon-homophilicpre-trainingandpromptlearn-
promptlearningmethodstypicallyassumegraphsarehomophilic ing,wefocusontwocommontasksongraphdata:nodeclassi-
[43,61],neglectingthefactthatreal-worldgraphsaregenerallynon- fication and graph classification, in few-shot settings. For node
homophilic,exhibitingamixtureofhomophilicandheterophilic classificationwithinagraph𝐺 = (𝑉,𝐸),let𝑌 bethesetofnode
patterns.Furthermore,thesemethodsusuallyapplyasingleprompt classes.Eachnode𝑣 𝑖 ∈ 𝑉 hasaclasslabel𝑦 𝑖 ∈ 𝑌.Similarly,for
forallnodes,overlookingtheuniquecharacteristicsofeachnode’s graphclassificationacrossasetofgraphsG,letY bethesetof
non-homophilicpattern. possiblegraphlabels.Eachgraph𝐺 𝑖 ∈Ghasaclasslabel𝑌 𝑖 ∈Y.
Inthefew-shotsetting,thereareonly𝑘labeledsamplesperclass,
3 Preliminaries where𝑘isasmallnumber(e.g.,𝑘 ≤10).Thisscenarioisknownas
𝑘-shotclassification[26,62,65].Notethatthehomophilyratiois
Graph. A graph is defined as𝐺 = (𝑉,𝐸), where𝑉 represents
definedwithrespecttosomepredefinedsetoflabels,whichmay
thesetofnodesand𝐸representsthesetofedges.Thenodesare
alsoassociatedwithafeaturematrixX∈R|V|×𝑑 ,suchthatx𝑣 ∈ notberelatedtotheclasslabelsindownstreamtasks.
R𝑑 is a row of X representing the feature vector for node 𝑣 ∈ 4 RevisitingGraphPre-training
𝑉.Foracollectionofmultiplegraphs,weusethenotationG =
{𝐺 1,𝐺 2,...,𝐺 𝑁}. Inthissection,werevisitgraphpre-trainingtaskstocopewithnon-
homophilicgraphs.Wefirstproposethedefinitionofhomophily
Homophilyratio.Givenamappingbetweenthenodesofagraph
tasksandrevealitsconnectiontothetrainingloss.Thetheoretical
andapredefinedsetoflabels,let𝑦 𝑣denotethelabelmappedtonode
insightsfurtherguideusinchoosinggraphpre-trainingtasks.
𝑣.ThehomophilyratioH(𝐺)evaluatestherelationshipsbetween
thelabelsandthegraphstructure[29,75],measuringthefraction 4.1 TheoreticalInsights
ofhomophilicedgeswhosetwoendnodessharethesamelabel.
Wefocusoncontrastivegraphpre-trainingtasks.Givenamain-
Moreconcretely,
streamcontrastivetask[12,26,36,60,62,76],𝑇 = ({A𝑢 : 𝑢 ∈
H(𝐺)= |{(𝑢,𝑣) ∈𝐸:𝑦 𝑢 =𝑦 𝑣}| . (1) 𝑉},{B𝑢 : 𝑢 ∈ 𝑉}),itslossfunction L𝑇 canbegeneralizedtoa
|𝐸|
standardformbelow.
∑︁
Additionally,thehomophilyratiocanbedefinedforeachnode L𝑇 =− ln𝑃(𝑢,A𝑢,B𝑢), (4)
basedonitslocalstructure[30,55],measuringthefractionofa 𝑢∈𝑉
n cao nde b’ esn de ei fig nh eb dor as sthatshar |e {𝑢th ∈es Nam (𝑣e )l :a 𝑦b 𝑢el. =T 𝑦h 𝑣is }|node-specificratio 𝑃(𝑢,A𝑢,B𝑢) ≜
(cid:205)
𝑎∈A𝑢sim(cid:205) (h𝑎 𝑢∈ ,A h𝑢 𝑎)si +m (cid:205)(h 𝑏𝑢 ∈, Bh 𝑢𝑎) sim(h𝑢,h𝑏), (5)
H(𝑣)= , (2) wheresim(·,·)representsasimilarityfunctionsuchascosinesim-
|N(𝑣)|
ilarity[37]inourexperiment,A𝑢 isthesetofpositiveinstances
where|N(𝑣)|isthesetofneighboringnodesof𝑣.Notethatboth fornode𝑢,andB𝑢 isthesetofnegativeinstancesfor𝑢.Theopti-
H(𝐺)andH(𝑣)arein[0,1].Graphsornodeswithalargerpropor- mizationobjectiveoftask𝑇 inEq.(4)istomaximizethesimilarity
tionofhomophilicedgeshaveahigherhomophilyratio. between𝑢anditspositiveinstanceswhileminimizingthesimilarity
Graphencoder.Graphencoderslearnlatentrepresentationsof between𝑢anditsnegativeinstances.Basedonthisloss,wefurther
graphs,embeddingtheirnodesintosomefeaturespace.Awidely proposethedefinitionsofhomophilytasksandhomophilysamples.
usedfamilyofgraphencodersisGNNs,whichtypicallyutilizea
Definition1(HomophilyTask). Onagraph𝐺 =(𝑉,𝐸),apre-
message-passingmechanism[53,70].Specifically,eachnodeaggre-
trainingtask𝑇 =({A𝑢 :𝑢 ∈𝑉},{B𝑢 :𝑢 ∈𝑉})isahomophilytask
gatesmessagesfromitsneighborstogenerateitsownrepresenta-
ifandonlyif,∀𝑢 ∈𝑉,∀𝑎 ∈A𝑢,∀𝑏 ∈B𝑢,(𝑢,𝑎) ∈𝐸∧(𝑢,𝑏) ∉𝐸.A
tion.Bystackingmultiplelayers,GNNsenablesrecursivemessage
taskthatisnotahomophilytaskiscalledanon-homophilytask. □
passingthroughoutthegraph.Formally,theembeddingofanode
𝑣inthe𝑙-thGNNlayer,denotedash𝑙
𝑣,iscomputedasfollows. Inparticular,thewidelyusedlinkpredictiontask[26,32,34,62,
h𝑙 𝑣 =Aggr(h𝑙 𝑣−1,{h𝑢𝑙−1:𝑢 ∈N(𝑣)};𝜃𝑙 ), (3) 6 to4, 𝑢6 a5 n] dis Ba 𝑢h io sm ao sp uh bi sl ey tt oa fsk n, ow deh se nre otA li𝑢 nkis edas toub 𝑢s .etofnodeslinkedConferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.,XingtongYu∗,JieZhang∗,YuanFang†,andRenheJiang†
Definition 2 (Homophily Sample). On a graph𝐺 = (𝑉,𝐸), non-homophilymethods,sinceA𝑢 andB𝑢 intheirpre-training
consideratriplet (𝑢,𝑎,𝑏) where𝑢 ∈ 𝑉, (𝑢,𝑎) ∈ 𝐸 and (𝑢,𝑏) ∉ 𝐸. tasksarenotrelatedtotheconnectivitywith𝑢.Furtherdetails
Thetriplet (𝑢,𝑎,𝑏) isahomophilysampleifandonlysim(𝑢,𝑎) > ofthesemethodsareshowninAppendix.B.Inourexperiment,
sim(𝑢,𝑏),anditisanon-homophilysampleotherwise. □ weusethenon-homophilymethodGraphCLasthepretexttask
toobtainourmainresultsfornon-homophilygraphs,asitisa
Subsequently,wecanestablishthefollowingtheorems.
classicpre-trainingmethodwithcompetitiveperformance.Wealso
Theorem1. Forahomophilytask𝑇,addingahomophilysample experimentwithlinkprediction[26]andGraphACLforfurther
alwaysresultsinasmallerlossthanaddinganon-homophilysample. evaluation,asshowninTable5.
Proof. Considerahomophilysample(𝑢,𝑎,𝑏)forsome(𝑢,𝑎) ∈ 5 Non-homophilicPromptLearning
𝐸and(𝑢,𝑏)∉𝐸,aswellasanon-homophilysample(𝑢,𝑎′,𝑏′)for
some(𝑢,𝑎′) ∈𝐸and(𝑢,𝑏′) ∉𝐸.Lettheoveralllosswith(𝑢,𝑎,𝑏) Inthissection,weproposeProNoG,ourpromptlearningframe-
be𝐿 𝑇,andthatwith(𝑢,𝑎′,𝑏′)be𝐿 𝑇′.Since(𝑢,𝑎,𝑏)ishomophily, workfornon-homophilicgraphs.Wefirstintroducetheoverall
wehavesim(𝑢,𝑎) >sim(𝑢,𝑏),andthus𝑝(𝑢,𝑎,𝑏) >0.5.Moreover, framework,andthendevelopthepromptgenerationandtuning
since(𝑢,𝑎′,𝑏′)isnon-homophily,wehavesim(𝑢,𝑎′) ≤sim(𝑢,𝑏′), process.Finally,wepresenttheoverallalgorithmandanalyzeits
andthus𝑝(𝑢,𝑎′,𝑏′) ≤0.5.Hence,𝑝(𝑢,𝑎,𝑏) >𝑝(𝑢,𝑎′,𝑏′),implying complexity.
that𝐿
𝑇
<𝐿 𝑇′. □
5.1 Overallframework
Theorem2. Consideragraph𝐺 =(𝑉,𝐸)withalabelmapping WeillustratetheoverallframeworkofProNoGinFig.2.Itinvolves
function𝑉 →𝑌,andlet𝑦
𝑣
∈𝑌 denotethelabelmappedto𝑣 ∈𝑉.
twostages:(a)graphpre-trainingand(b)downstreamadaptation.
Supposethelabelmappingsatisfiesthat Ingraphpre-training,wepre-trainagraphencoderusinganon-
∀𝑢,𝑎,𝑏 ∈𝑉,𝑦 𝑢 =𝑦 𝑎∧𝑦 𝑢 ≠𝑦 𝑏 ⇒sim(𝑢,𝑎) >sim(𝑢,𝑏). homophilicpre-trainingtask,asshowninFig.2(a).Subsequently,
toadaptthepre-trainedmodeltodownstreamtasks,wepropose
LetE 𝑇 denotetheexpectednumberofhomophilysamplesforaho-
aconditionalnetwork(condition-net)thatgeneratesaseriesof
mophilytask𝑇 onthegraph𝐺.Then,E 𝑇 increasesmonotonicallyas
prompts,asdepictedinFig.2(b).Asaresult,eachnodeisequipped
thehomophilyratioH(𝐺)definedw.r.t.𝑌 increases.
with its own prompt, which can be used to modify its features
Proof. Forahomophilytask𝑇 =({A𝑢 :𝑢 ∈𝑉},{B𝑢 :𝑢 ∈𝑉}), toalignwiththedownstreamtask.Morespecifically,theprompt
atriplet(𝑢,𝑎,𝑏)forsome𝑢 ∈𝑉,𝑎∈A𝑢and𝑏 ∈B𝑢isahomophily generationisconditionedontheuniquepatternsofeachnode,in
samplewithaprobabilityof𝑃(𝑦 𝑢 =𝑦 𝑎)(1−𝑃(𝑦 𝑢 =𝑦 𝑏)),since𝑦 𝑢 = ordertoachievefine-grainedadaptationcateringtothediverse
𝑦 𝑎 ∧𝑦 𝑢 ≠𝑦 𝑏 impliessim(𝑢,𝑎) > sim(𝑢,𝑏).Hence,theexpected non-homophiliccharacteristicsofeachnode,asdetailedinFig.2(c).
numberofhomophilysamplesfor𝑇 is
5.2 PromptGenerationandTuning
∑︁
E 𝑇 = |A𝑢||B𝑢|𝑃(𝑦 𝑢 =𝑦 𝑎)(1−𝑃(𝑦 𝑢 =𝑦 𝑏)). (6)
𝑢∈𝑉
Promptgeneration.Innon-homophilicgraphs,differentnodes
Foraconstantnumberofnodeswithlabel𝑦 𝑢,asH(𝐺)increases, arecharacterizedbyuniquenon-homophilicpatterns.Specifically,
𝑃(𝑦 𝑢 =𝑦 𝑎)alwaysincreaseswhile𝑃(𝑦 𝑢 =𝑦 𝑏)alwaysdecreases, differentnodestypicallyhavediversehomophilyratiosH(𝑣),indi-
leadingtoalargerE 𝑇. □ catingdistincttopologicalstructureslinkingtotheirneighboring
node.Moreover,evennodeswithsimilarhomophilyratiosmay
In the next part, the theorems will guide us in choosing the havedifferentneighborhooddistributionsintermsofthevarying
appropriatepre-trainingtasksfornon-homophilicgraphs. homophilyratiosoftheneighboringnodes.Therefore,insteadof
learningasinglepromptforallnodesasinstandardgraphprompt
4.2 Non-homophilicGraphPre-training
learning[26,42,43,62],wedesignacondition-net[71]togenerate
Considerahomophilytask𝑇.AccordingtoTheorem2,fornon- aseriesofnon-homophilicpattern-conditionedprompts.Conse-
homophilicgraphswithlowerhomophilyratios,onaveragethere quently,eachnodeisequippedwithitsownuniqueprompt,aiming
arefewerhomophilysamplesandmorenon-homophilysamples toadapttoitsdistinctnon-homophiliccharacteristics.
for𝑇.Consequently,basedonTheorem1,addinganon-homophily First,thenon-homophilicpatternsofanodecanbecharacterized
samplealwaysresultsinalargerlossthanaddingahomophily byconsideringamulti-hopneighborhoodaroundthenode.Specifi-
sample. Therefore, for non-homophilic graphs, especially those cally,givenanode𝑣,wereadouttheir𝛿-hopego-network𝑆 𝑣,which
withlowhomophilyratio,non-homophilytasksareabetterchoice isaninducedsubgraphcontainingthenode𝑣andnodesreachable
comparedtohomophilytaskswhenoptimizingthetrainingloss. from𝑣 inatmost𝛿 steps.InspiredbyGGCN[58],thereadoutis
Werevisitmainstreamgraphpre-trainingmethodsandcatego- weightedbythesimilaritybetween𝑣andtheirneighbors,asshown
rizethemintotwocategories:homophilymethodsthatemploy inFig.2(c),obtainingarepresentationofthesubgraph𝑆 𝑣 givenby
homophilytasks,andnon-homophilymethodsthatdonot.Specifi- 1 ∑︁
cally,GMI[34],GraphPrompt[26],MultiGPrompt[65],HGPrompt
s𝑣 =
|𝑆 𝑣|
h𝑢 ·sim(h𝑢,h𝑣), (7)
[64]andGraphPrompt+[62]areallhomophilymethods,sincetheir
𝑢∈𝑆𝑣
pre-trainingtasksutilizesaformoflinkprediction,whereA𝑢 isa where|𝑆 𝑣|denotesthenumberofnodesin𝑆 𝑣.Inourexperiment,
setofnodeslinkedto𝑢,andB𝑢 isasetofnodesnotlinkedfrom weset𝛿 = 2tobalancebetweenefficiencyandcapturingmore
𝑢.Incontrast,DGI[48],GraphCL[60],andGraphACL[55]are uniquenon-homophilicpatternsintheneighborhoodof𝑣.Non-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Prompt generation
Non-homophily Tuned
pre-training loss No md ae t re im xb Readout eS mu bb g mr aa tp rh ix Frozen           (  ,      )                     ∙                (( ( ( (            ,, , , ,                )) ) ) )
Graph Pre-trained Downstream
Condition-net
encoder graph encoder


Condition-net


P mro am trip xt Prom ∙pting sub  g2  r- ah po hp  o  f v promp  t 
(a) Pre-training (b) Downstream adaptation with conditional prompting (c) Details of prompt generation
Figure2:OverallframeworkofProNoG.
Next,foreachdownstreamtask,ourgoalistoassignaunique 𝑌.Thedownstreamlossfunctionis
p pr ro om mp pt tv ve ec ct to or rt so wea oc uh ldn so id ge n. iH fio caw ne tv lyer i, nd cir re ec at sl ey tp ha era nm ue mte br eiz ri on fg lt eh ae rs ne
- L down(𝜙 𝑡)=− ∑︁ ln
exp(cid:16) 𝜏1si (cid:16)m(h˜ 𝑡,𝑥𝑖,h¯ 𝑡,𝑦𝑖)(cid:17)
(cid:17), (10)
ableparameters,whichmayoverfittothelightweightsupervision (𝑥𝑖,𝑦𝑖)∈D𝑡 (cid:205) 𝑐∈𝑌exp 𝜏1sim(h˜ 𝑡,𝑥𝑖,h¯ 𝑡,𝑐)
infew-shotsettings.Tocatertotheuniquenon-homophilicchar-
acteristicsofeachnodewithminimalparameters,weproposeto whereh˜ 𝑡,𝑥𝑖 denotestheoutputembeddingofnode𝑣/graph𝐺for
employacondition-net[71]togeneratenode-specificpromptvec- task𝑡.Specifically,fornodeclassificationh˜ 𝑡,𝑣 istheoutputembed-
tors.Specifically,conditionedonthesubgraphreadouts𝑣ofanode dinginEq.9;forgraphclassification,h˜ 𝑡,𝐺 =(cid:205) 𝑢∈𝑉 h˜ 𝑡,𝑢 involving
𝑣,thecondition-netgeneratesauniquepromptvectorfor𝑣w.r.t.a anadditionalgraphreadout.Theprototypeembeddingforclass𝑐,
task𝑡,denotedbyp𝑡,𝑣,asfollows. h¯ 𝑡,𝑐,istheaverageoftheoutputembeddingofallnodes/graphs
p𝑡,𝑣 =CondNet(s𝑣;𝜙 𝑡), (8)
belongingtoclass𝑐.
Duringprompttuning,weupdateonlythelightweightparame-
whereCondNetisthecondition-netparameterizedby𝜙 𝑡.Itoutputs tersofthecondition-net(𝜙 𝑡),whilefreezingthepre-trainedGNN
auniquepromptvectorp𝑡,𝑣,whichvariesbasedontheinput𝑠 𝑣 weights.Thus,ourprompttuningisparameter-efficientandamenable
thatcharacterizesthenon-homophilypatternsofnode𝑣.Notethat tofew-shotsettings,whereD𝑡 containsonlyasmallnumberof
thisisaformofhypernetworks[10],whichemploysasecondary
trainingexamplesfortask𝑡.
networktogeneratetheparametersforthemainnetworkcondi-
5.3 AlgorithmandComplexityAnalysis
tionedontheinputfeature.Inourcontext,thecondition-netis
thesecondarynetwork,generatingpromptparameterswithoutex- Algorithm.Wedetailthemainstepsforconditionalpromptgen-
pandingthenumberoflearnableparametersinthemainnetwork.
erationandtuninginAlgorithm1,AppendixA.
ThesecondarynetworkCondNetcanbeanylearnablefunction,
Complexityanalysis.Foradownstreamgraph𝐺,thecomputa-
suchasafully-connectedlayeroramulti-layerperceptron(MLP).
tionalprocessofProNoGinvolvestwomainparts:encodingnodes
WeemployanMLPwithacompactbottleneckarchitecture[52].
viaapre-trainedGNN,andconditionalpromptlearning.Thefirst
Subsequently,weperformfine-grained,node-wiseadaptationto
task𝑡.Concretely,thepromptp𝑡,𝑣 fornode𝑣isemployedtoadjust part’scomplexityisdeterminedbytheGNN’sarchitecture,akinto
𝑣’sfeaturesoritsembeddingsinthehiddenoroutputlayers[62].In othermethodsemployingapre-trainedGNN.InastandardGNN,
eachnodeaggregatesfeaturesfromupto𝑛 neighborsperlayer.
ourexperiments,wechooseasimpleyeteffectiveimplementation
Assumingtheaggregationinvolvesatmost𝐷neighbors,thecom-
thatmodifiesthenodes’outputembeddingsthroughanelement-
plexityofcalculatingnodeembeddingsover𝐿layersis𝑂(𝐷𝐿·|𝑉|),
wiseproduct,asfollows.
where|𝑉|denotesthenumberofnodes.Thesecondpart,condi-
h˜ 𝑡,𝑣 =p𝑡,𝑣 ⊙h𝑣, (9) tional prompt learning, has two stages: prompt generation and
prompttuning.Inthepromptgenerationstage,eachsubgraphem-
wherethepromptp𝑡,𝑣 isgeneratedwithanequaldimensionash𝑣. beddingisfedintothecondtion-net.Inourexperment,weusea2
Prompttuning.Inthiswork,wefocusontwocommontypes layerMLPascondition-net,resultinginacomplexityof𝑂(2·|𝑉|).
ofdownstreamtask:nodeclassificationandgraphclassification. Duringprompttuning,eachnodein𝐺 isadjustedusingaprompt
Theprompttuningprocessdoesnotdirectlyoptimizetheprompt vector,withacomplexityof𝑂(|𝑉|).Therefore,thetotalcomplexity
vectors;insteaditoptimizesthecondition-net,whichsubsequently forconditionalpromptlearningis𝑂(3·|𝑉|).
generatesthepromptvectors,foragivendownstreamtask. Inconclusion,theoverallcomplexityof ProNoGis𝑂((𝐷𝐿 +
Weutilizealossfunctionbasedonnode/graphsimilarityfol-
3)·|𝑉|).Thefirstpartdominatestheoverallcomplexity,as𝑂(𝑛𝐿·
lowingpreviouswork[26,62].Formally,foratask𝑡 withalabeled |𝑉|) ≫ 𝑂(3·|𝑉|),whereweset𝐿 = 2forexperimentsonlow-
trainingsetD𝑡 ={(𝑥 1,𝑦 1),(𝑥 2,𝑦 2),...},where𝑥 𝑖 canbeeithera homophilygraphs.Thus,theadditionalcomputationalcostintro-
nodeoragraph,and𝑦 𝑖 ∈𝑌 is𝑥 𝑖’sclasslabelfromasetofclasses ducedbytheconditionalprompttuningstepisminimal.
Readout MeanConferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.,XingtongYu∗,JieZhang∗,YuanFang†,andRenheJiang†
Table1:Summaryofdatasets.
[43],arebasedonthemeta-learningparadigm[8],whichrequires
anadditionalsetoflabeledbaseclassesinadditiontothefew-shot
Homophily Graph Avg. Avg. Node Node
Graphs classes.Consequently,thesemethodsarenotdirectlycomparable
ratio classes nodes edges features classes
toourframework.
Wisconsin 1 0.21 - 251 199 1,703 5 Parametersettings.Forallbaselines,weusetheoriginalauthors’
Squirrel 1 0.22 - 5,201 217,073 2,089 5
codeandfollowtheirrecommendedsettings,whilefurthertuning
Chameleon 1 0.23 - 2,277 36,101 2,325 5
theirhyperparameterstoensureoptimalperformance.Detailed
Cornell 1 0.30 - 183 295 1,703 5
PROTEINS 1,113 0.66 2 39.06 72.82 1 3 descriptionsoftheimplementationsandsettingsforboththebase-
ENZYMES 600 0.67 6 32.63 62.14 18 3 linesandourProNoGareprovidedinAppendixE.
Citeseer 1 0.74 - 3,327 4,732 3,703 6 Setup of downstream tasks. We conduct two types of down-
Cora 1 0.81 - 2,708 5,429 1,433 7
streamtask:nodeclassification,andgraphclassification.Thesetasks
BZR 405 - 2 35.75 38.36 3 - aresetupas𝑘-shotclassificationproblems,meaningthatforeach
COX2 467 - 2 41.22 43.45 3 -
class,𝑘instances(nodesorgraphs)arerandomlyselectedforsu-
HomophilyratioiscalculatedbyEq.1.NotethatBZRandCOX2donothaveany
pervision.Giventhatalllow-homophilydatasets,i.e.,Wisconsin,
nodelabel,andthusitisnotabletocalculatetheirhomophilyratios.
Squirrel,ChameleonandCornellonlycompriseasinglegraphand
cannotbedirectlyusedforgraphclassification.Thus,followingpre-
6 Experiments viousresearch[27,64],wegeneratemultiplegraphsbyconstruct-
ingego-networkscenteredonthelabelednodesineachdataset.
Inthissection,weconductexperimentstoevaluateProNoG,and
Wethenperformgraphclassificationontheseego-networks,each
analyzetheempiricalresults.
labeledaccordingtoitscentralnode.Fordatasetswithhighho-
mophilyratios,PROTEINS,ENZYMES,BZRandCOX2haveoriginal
6.1 ExperimentalSetup
graphlabels,sowedirectlyconductgraphclassificationonthese
Datasets. We conduct experiments on ten benchmark datasets. graphs.Sincethe𝑘-shottasksarebalancedclassificationproblems,
Wisconsin[33],Cornell[33],Chameleon[39],andSquirrel[39]are weuseaccuracytoevaluateperformance,inlinewithpriorstud-
allwebpagegraphs.Eachdatasetfeaturesasinglegraphwhere ies[25,26,49,62].Wepre-trainthegraphencoderonceforeach
nodescorrespondtowebpagesandedgesrepresenthyperlinks datasetandthenusethesamepre-trainedmodelforalldownstream
connectingthesepages.Cora[31]andCiteseer [41]arecitation
tasks.Wegenerate100𝑘-shottasksforbothnodeclassificationand
networks.Thesedatasetsconsistofasinglegrapheach,withnodes graphclassificationbyrepeatingthesamplingprocess100times.
signifyingscientificpapersandedgesindicatingcitationrelation- Eachtaskisexecutedwithfivedifferentrandomseeds,leadingtoa
ships.PROTEINS[3]consistsofaseriesofproteingraphs.Nodes totalof500resultspertasktype.Wereportthemeanandstandard
inthesegraphsdenotesecondarystructures,whileedgesdepict deviationofthese500outcomes.
neighboringrelationshipseitherwithintheaminoacidsequence
orinthree-dimensionalspace.ENZYMES[50],BZR[38],andCOX2 6.2 Few-shotPerformanceEvaluation
[38]arecollectionsofmoleculargraphs.Thesedatasetsdescribe
enzymestructuresfromtheBRENDAenzymedatabase,ligandsre- Wefirstevaluateone-shotclassificationtasks.Then,wevarythe
latedtobenzodiazepinereceptors,andcyclooxygenase-2inhibitors, numberofshotstoinvestigatetheirimpactonperformance.
respectively.WesummarizethesedatasetsinTable1,andpresent One-shotperformance.Wepresenttheresultsofone-shotnode
furtherdetailsinAppendixC. and graph classification tasks on non-homophilic graphs in Ta-
Baselines.WeevaluateProNoGagainstaseriesofstate-of-the-art bles2and3,respectively.Wemakethefollowingobservations:(1)
methods,categorizedintothreeprimarygroups: ProNoGsurpassesallbaselinemethodsacrossallsettings,outper-
(1)End-to-endgraphneuralnetworks:GCN[20],GAT[47],H2GCN formingthebestcompetitorbyupto21.49%onnodeclassification
[75],andFAGCN[2]aretrainedinasupervisedmannerdirectly and6.50%ongraphclassification.Theseresultsdemonstrateits
usingdownstreamlabels.Specifically,GCNandGATareoriginally effectivenessinlearningpriorknowledgefromnon-homophilic
designedforhomophilicgraphs,H2GCNforheterophilicgraphs, graphs and capturing nodes’ specific patterns. (2) Other graph
andFAGCNfornon-homophilicgraphs. promptlearningmethods,i.e.,GPPT,GraphPrompt,andGraph-
(2)Graphpre-trainingmodels:DGI[48],GraphCL[60],DSSL Prompt+,significantlylagbehindProNoG.Theirsuboptimalper-
[54],GraphACL[55]followthe“pre-train,fine-tune”paradigm. formancecanbeattributedtotheirinabilitytoaccountforavariety
(3)Graphpromptlearningmodels:GPPT[42],GraphPrompt[26], ofnode-specificpatterns.Theseresultsunderscoretheimportance
andGraphPrompt+[62]useself-supervisedpre-trainingtaskswith ofourconditionalpromptingincharacterizingnodeembeddings
asingletypeofpromptfordownstreamadaptation.NotethatGPPT tocapturenodes’specificpatterns.(3)GPPTisatbestcomparable
isspecificallydesignedfornodeclassificationandcannotbedirectly to,andoftenperformsworsethanotherbaselinesbecauseitisnot
usedforgraphclassification.Therefore,inourexperiments,weuse specificallydesignedforfew-shotlearning.
GPPTexclusivelyfornodeclassificationtasks. Few-shotperformance.ToassesstheperformanceofProNoG
WeprovidefurtherdetailsonthesebaselinesinAppendixD.It’s with different amounts of labeled data, we vary the number of
worthnotingthatsomegraphfew-shotlearningmethods,suchas shotsinthedownstreamtasksandpresenttheresultsinFig.3
Meta-GNN[69],AMM-GNN[49],RALE[25],VNT[46],andProG andAppendixF.NotethatgiventhelimitednumberofnodesinNon-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Table2:Accuracyevaluationonfew-shotnodeclassification.
Methods Wisconsin Squirrel Chameleon Cornell PROTEINS ENZYMES Citeseer Cora
GCN 21.39± 6.56 20.00± 0.29 25.11± 4.19 21.81± 4.71 43.32± 9.35 48.08± 4.71 31.27± 4.53 28.57± 5.07
GAT 28.01± 5.40 21.55± 2.30 24.82± 4.35 23.03±13.19 31.79±20.11 35.32±18.72 30.76± 5.40 28.40± 6.25
H2GCN 23.60± 4.64 21.90± 2.15 25.89± 4.96 32.77±14.88 29.60± 6.99 37.27± 8.73 26.98± 6.25 34.58± 9.43
FAGCN 35.03±17.92 20.91± 1.79 22.71± 3.74 28.67±17.64 32.63± 9.94 35.87±13.47 26.46± 6.34 28.28± 9.57
DGI 28.04± 6.47 20.00± 1.86 19.33± 4.57 32.54±15.66 45.22±11.09 48.05±14.83 45.00± 9.19 54.11± 9.60
GraphCL 29.85± 8.46 21.42± 2.22 27.16± 4.31 24.69±14.06 46.15±10.94 48.88±15.98 43.12± 9.61 51.96± 9.43
DSSL 28.46±10.31 20.94± 1.88 27.92± 3.93 20.36± 5.38 40.42±10.08 66.59±19.28 39.86± 8.60 40.79± 7.31
GraphACL 34.57±10.46 24.44± 3.94 26.72± 4.67 33.17±16.06 42.16±13.50 47.57±14.36 35.91± 7.87 46.65± 9.54
GPPT 27.39± 6.67 20.09± 0.91 24.53± 2.55 25.09± 2.92 35.15±11.40 35.37± 9.37 21.45± 3.45 15.37± 4.51
GraphPrompt 31.48± 5.18 21.22± 1.80 25.36± 3.99 31.00±13.88 47.22±11.05 53.54±15.46 45.34±10.53 54.25± 9.38
GraphPrompt+ 31.54± 4.54 21.24± 1.82 25.73± 4.50 31.65±14.48 46.08± 9.96 57.68±13.12 45.23±10.01 52.51± 9.73
ProNoG 44.72±11.93 24.59± 3.41 30.67± 3.73 37.90± 9.31 48.95±10.85 72.94±20.23 49.02±10.66 57.92±11.50
Resultsarereportedinpercent.Thebestmethodisboldedandtherunner-upisunderlined.
Table3:Accuracyevaluationonfew-shotgraphclassification.
Methods Wisconsin Squirrel Chameleon Cornell PROTEINS ENZYMES BZR COX2
GCN 21.39± 6.56 11.77± 3.10 17.21± 4.80 26.36± 4.35 51.66±10.87 19.30± 6.36 45.06±16.30 43.84±13.94
GAT 24.93± 7.59 20.70± 1.51 25.71± 3.32 22.66±12.46 51.33±11.02 20.24± 6.39 46.28±15.26 51.72±13.70
H2GCN 22.23± 6.38 20.69± 1.42 26.76± 3.98 23.11±11.78 53.81± 8.85 19.40± 5.57 50.28±12.13 53.70±11.73
FAGCN 23.81± 9.50 20.83± 1.43 25.93± 4.03 25.71±13.12 55.45±11.57 19.95± 5.94 50.93±12.41 50.22±11.50
DGI 29.77± 6.22 20.50± 1.52 24.29± 4.33 18.60±12.79 50.32±13.47 21.57± 5.37 49.97±12.63 54.84±14.76
GraphCL 27.93± 5.27 21.01± 1.86 26.45± 4.30 20.03±10.05 54.81±11.44 19.93± 5.65 50.50±18.62 47.64±22.42
DSSL 22.05± 3.90 20.74± 1.61 26.19± 3.72 18.38±10.63 52.73±10.98 23.14± 6.71 49.04± 8.75 54.23±14.17
GraphACL 22.98± 5.89 20.80± 1.28 26.28± 3.93 26.50±17.18 56.11±13.95 20.28± 5.60 49.24±17.87 49.59±23.93
GraphPrompt 28.34± 3.89 21.22± 1.80 26.51± 4.67 24.06±13.71 53.61± 8.90 21.85± 6.17 50.46±11.46 55.01±15.23
GraphPrompt+ 26.95± 7.42 20.80± 1.45 26.03± 4.17 25.31± 7.65 54.55±12.61 21.85± 5.15 53.26±14.99 54.73±14.58
ProNoG 31.54± 5.30 20.92± 1.37 28.50± 5.30 27.17± 9.58 56.11±10.19 22.55± 6.70 51.62±14.27 56.46±14.57
Table4:Ablationstudyontheeffectsofkeycomponents.
Nodeclassification Graphclassification
Methods
Wisconsin Squirrel Chameleon PROTEINS ENZYMES Citeseer Wisconsin Squirrel Chameleon PROTEINS ENZYMES COX2
NoPrompt 25.41±3.13 20.60±1.30 22.71±3.54 47.22±11.05 66.59±19.28 43.12±9.61 20.85±6.74 20.18±1.30 22.34±4.15 53.61±8.90 21.85±6.17 54.29±17.31
SinglePrompt 32.76±5.21 20.85±1.32 22.78±3.35 30.33±19.59 65.32±21.67 48.64±10.09 25.77±6.24 20.68±0.91 27.03±3.98 56.35 ±10.59 19.38±7.12 47.24±15.53
NodeCond 35.56±4.65 21.26±3.95 21.13±2.23 36.01±19.70 68.54±19.31 48.30±10.22 25.30±4.62 20.98±1.56 27.24±5.24 56.61±10.03 20.70±6.67 55.92±14.66
ProNoG\sim 30.65±4.05 20.05±0.59 20.96±4.21 33.73±17.82 36.02±20.64 18.74±2.66 22.05±5.86 19.93±0.42 20.20±1.11 52.30±10.94 16.70±1.28 50.05±17.67
ProNoG 44.72 ±11.93 24.59 ±3.41 30.67 ±3.73 48.95 ±10.85 72.94 ±20.23 49.02 ±10.66 31.54 ±5.30 20.92 ±1.37 28.50 ±5.30 56.11±10.19 22.55 ±6.70 56.46 ±14.57
6.3 AblationStudy
Wisconsin and Cornell, we only conduct tasks up to 3-shot. We
observethat:(1)ProNoGsignificantlyoutperformsallbaselines Tocomprehensivelyunderstandtheinfluenceofconditionalprompt-
inlow-shotscenarioswithverylimitedlabeleddata(e.g.,𝑘 ≤5),
inginProNoG,weperformanablationstudycomparingProNoG
showcasingtheeffectivenessofourapproachinthesesituations. withfourofitsvariants:NoPromptreplacesconditionalprompting
(2)Asthenumberofshotsincreases,allmethodsgenerallyshow withaclassifierfordownstreamadaptation.SinglePromptusesa
improvedperformanceasexpected.However,ProNoGremains singlepromptinsteadofconditionalpromptingtomodifyallnodes.
competitiveandoftensurpassestheothermethods,demonstrating NodeConddirectlyusestheoutputembeddingofthepre-trained
therobustnessof ProNoG. graphencoderasinputtothecondition-nettogeneratetheprompt
withoutreadingoutthesubgraphinEq.7.ProNoG\simreadoutConferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.,XingtongYu∗,JieZhang∗,YuanFang†,andRenheJiang†
Table5:Comparisonbetweenhomophilyandnon-homophilypre-training.
Nodeclassification Graphclassification
Pre-trainingtask Wisconsin Cornell PROTEINS ENZYMES Wisconsin Cornell PROTEINS ENZYMES
0.21 0.30 0.66 0.67 0.21 0.30 0.66 0.67
LinkPrediction[42] 23.01±11.40 26.27± 7.61 35.88± 5.41 36.74± 2.61 20.96± 4.21 25.38± 2.50 51.50± 6.02 17.47± 4.04
LinkPrediction[26] 28.93±11.74 16.29± 7.93 48.95±10.85 52.87±14.73 23.15± 5.67 22.05±13.80 55.83±10.87 22.23± 5.51
GraphACL[55] 33.91± 9.04 29.55±12.30 44.08±10.03 50.57±13.11 26.42± 7.25 26.15± 3.87 54.15±10.58 21.64± 5.88
GraphCL[60] 44.72±11.93 37.90± 9.31 48.28±11.09 51.46±13.93 31.54± 1.37 27.17± 5.30 53.91± 5.51 21.78±12.12
Wisconsin PROTEINS Wisconsin Cornell
Cora Citeseer
Squirrel ENZYMES
Figure4:Resultsondifferentnodepatterns.
linkpredictionusedinGraphPrompt[26],andnon-homophilytasks
GraphCL[60]andDSSL[54],respectively.Notethatlinkpredicition
inGPPT[42]isinagenerativeformat,thusfallingbeyondthe
scopeofhomophilytask,butit’salsoaffectedbynon-homophily
ingraphs.Wecomparethesepretexttasksandshowtheresultsin
Table5.Weobservethatforgraphswithalowhomophilyratio,the
Chamelon Cora non-homophilytasksignificantlyoutperformsthehomophilytasks.
Conversely,forgraphswithahighhomophilyratio,theresultsof
Figure3:Impactsofdifferentshotsonnodeclassification. thesetwomethodsaremixed,witheachhavingtheirownstrengths
andweaknesses.
6.5 AnalysisonVaryingNodePatterns
thesubgraphviamean-poolingwithoutsimilaritybetweencen-
tralnodesandtheirneighborsasinEq.7.AsshowninTable4, Toevaluatetheabilityof ProNoGincapturingnode-specificpat-
ProNoGconsistentlyoutperformsorisatleastcompetitivewith terns,wecalculatetheaccuracyondifferentnodegroupswithvary-
thesevariants.Thishighlightsthenecessityofreadoutsubgraphs inghomophilyratios,i.e.,[0.0,0.2),[0.2,0.4),[0.4,0.6),[0.6,0.8),
weightedbysimilaritytocapturenodes’non-homophilicpatterns, [0.8,1.0].WecompareProNoGwithseveralcompetitivebaselines
andtheadvantagesofusingconditionalpromptingtospecifically andpresenttheresultsinFig.4.WeobservethatProNoGcon-
characterizenodes. sistentlyoutperformsorisatleastcompetitivewithallbaselines
acrossallnodepatterns,regardlessoftheirhomophilyratio.These
6.4 AnalysisonPre-TrainingMethods
resultsfurtherdemonstratetheeffectivenessof ProNoGincap-
Tofurtherevaluatehomophilyandnon-homophilytasks,using turingnode-specificpatternsandhighlighttheadvantagesofour
ProNoGfordownstreamadaptation,weemployhomophilytasks proposedconditionalprompting.Non-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronym’XX,June03–05,2018,Woodstock,NY
7 Conclusions
Inthispaper,weexploredpre-trainingandpromptlearningon
non-homophilicgraphs.Theobjectivesaretwofold:learningcom-
prehensiveknowledgeirrespectiveofthevaryingnon-homophily
characteristicsofgraphs,andadaptingthenodeswithdiversedistri-
butionsofnon-homophilypatternstodownstreamapplicationsina
fine-grained,node-wisemanner.Wefirstrevisitgraphpre-training
onnon-homophilicgraphs,providingtheoreticalinsightsintothe
choiceofpre-trainingtasks.Then,fordownstreamadaptation,we
proposedcondition-nettogenerateaseriesofpromptsconditioned
onvariousnon-homophilicpatternsacrossnodes.Finally,wecon-
ductedextensiveexperimentsontenpublicdatasets,demonstrat-
ingthatProNoGsignificantlyoutperformsdiversestate-of-the-art
baselines.Conferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.,XingtongYu∗,JieZhang∗,YuanFang†,andRenheJiang†
References
[30] HaitaoMao,ZhikaiChen,WeiJin,HaoyuHan,YaoMa,TongZhao,NeilShah,and
[1] HangboBao,LiDong,SonghaoPiao,andFuruWei.2022. BEiT:BERTPre- JiliangTang.2023.Demystifyingstructuraldisparityingraphneuralnetworks:
TrainingofImageTransformers.InICLR. Canonesizefitall?.InNeurIPS.
[2] DeyuBo,XiaoWang,ChuanShi,andHuaweiShen.2021.Beyondlow-frequency [31] AndrewKachitesMcCallum,KamalNigam,JasonRennie,andKristieSeymore.
informationingraphconvolutionalnetworks.InAAAI.3950–3957. 2000.Automatingtheconstructionofinternetportalswithmachinelearning.
[3] KarstenMBorgwardt,ChengSoonOng,StefanSchönauer,SVNVishwanathan, InformationRetrieval(2000).
AlexJSmola,andHans-PeterKriegel.2005.Proteinfunctionpredictionviagraph [32] Trung-KienNguyenandYuanFang.2024.Diffusion-basedNegativeSampling
kernels.Bioinformatics21,suppl_1(2005),i47–i56. onGraphsforLinkPrediction.InWWW.948–958.
[4] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, [33] HongbinPei,BingzheWei,KevinChen-ChuanChang,YuLei,andBoYang.
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda 2020. Geom-gcn:Geometricgraphconvolutionalnetworks. arXivpreprint
Askell,etal.2020.Languagemodelsarefew-shotlearners.NeurIPS33(2020), arXiv:2002.05287(2020).
1877–1901. [34] ZhenPeng,WenbingHuang,MinnanLuo,QinghuaZheng,YuRong,Tingyang
[5] MouxiangChen,ZeminLiu,ChenghaoLiu,JundongLi,QihengMao,andJianling Xu,andJunzhouHuang.2020. Graphrepresentationlearningviagraphical
Sun.2023. Ultra-dp:Unifyinggraphpre-trainingwithmulti-taskgraphdual mutualinformationmaximization.InWWW.259–270.
prompt.arXivpreprintarXiv:2310.14845(2023). [35] OlegPlatonov,DenisKuznedelev,MichaelDiskin,ArtemBabenko,andLiudmila
[6] LiDong,NanYang,WenhuiWang,FuruWei,XiaodongLiu,YuWang,Jianfeng Prokhorenkova.2023.AcriticallookattheevaluationofGNNsunderheterophily:
Gao,MingZhou,andHsiao-WuenHon.2019. Unifiedlanguagemodelpre- Arewereallymakingprogress?.InICLR.
trainingfornaturallanguageunderstandingandgeneration.NeurIPS32(2019). [36] JiezhongQiu,QibinChen,YuxiaoDong,JingZhang,HongxiaYang,MingDing,
[7] TaoranFang,YunchaoZhang,YangYang,ChunpingWang,andLeiChen.2024. KuansanWang,andJieTang.2020.GCC:Graphcontrastivecodingforgraph
Universalprompttuningforgraphneuralnetworks.NeurIPS(2024). neuralnetworkpre-training.InSIGKDD.1150–1160.
[8] ChelseaFinn,PieterAbbeel,andSergeyLevine.2017. Model-agnosticmeta- [37] FaisalRahutomo,TeruakiKitasuka,MasayoshiAritsugi,etal.2012.Semantic
learningforfastadaptationofdeepnetworks.InICML.1126–1135. cosinesimilarity.InICAST.
[9] TianyuGao,AdamFisch,andDanqiChen.2021.MakingPre-trainedLanguage [38] RyanA.RossiandNesreenK.Ahmed.2015.TheNetworkDataRepositorywith
ModelsBetterFew-shotLearners.InACL.3816–3830. InteractiveGraphAnalyticsandVisualization.InAAAI.4292–4293.
[10] DavidHa,AndrewDai,andQuocVLe.2016.Hypernetworks.arXivpreprint [39] BenedekRozemberczki,CarlAllen,andRikSarkar.2021.Multi-scaleattributed
arXiv:1609.09106(2016). nodeembedding.JournalofComplexNetworks(2021),cnab014.
[11] WillHamilton,ZhitaoYing,andJureLeskovec.2017.Inductiverepresentation [40] TimoSchickandHinrichSchütze.2021.It’sNotJustSizeThatMatters:Small
learningonlargegraphs.NeurIPS(2017),1025–1035. LanguageModelsAreAlsoFew-ShotLearners.InNAACL.2339–2352.
[12] KavehHassaniandAmirHoseinKhasahmadi.2020. Contrastivemulti-view [41] PrithvirajSen,GalileoNamata,MustafaBilgic,LiseGetoor,BrianGalligher,and
representationlearningongraphs.InICML.4116–4126. TinaEliassi-Rad.2008.Collectiveclassificationinnetworkdata.AImagazine
[13] DongxiaoHe,JitaoZhao,RuiGuo,ZhiyongFeng,DiJin,YuxiaoHuang,Zhen (2008).
Wang,andWeixiongZhang.2023.Contrastivelearningmeetshomophily:two [42] MingchenSun,KaixiongZhou,XinHe,YingWang,andXinWang.2022.GPPT:
birdswithonestone.InInternationalConferenceonMachineLearning.12775– GraphPre-trainingandPromptTuningtoGeneralizeGraphNeuralNetworks.
12789. InSIGKDD.1717–1727.
[14] WeihuaHu,BowenLiu,JosephGomes,MarinkaZitnik,PercyLiang,VijayPande, [43] XiangguoSun,HongCheng,JiaLi,BoLiu,andJihongGuan.2023.AllinOne:
andJureLeskovec.2020.StrategiesforPre-trainingGraphNeuralNetworks.In Multi-TaskPromptingforGraphNeuralNetworks.InSIGKDD.
ICLR. [44] XiangguoSun,JiawenZhang,XixiWu,HongCheng,YunXiong,andJiaLi.2023.
[15] ZiniuHu,YuxiaoDong,KuansanWang,Kai-WeiChang,andYizhouSun.2020. Graphpromptlearning:Acomprehensivesurveyandbeyond. arXivpreprint
GPT-GNN:Generativepre-trainingofgraphneuralnetworks.InSIGKDD.1857– arXiv:2311.16534(2023).
1867. [45] ShiyinTan,DongyuanLi,RenheJiang,YingZhang,andManabuOkumura.2024.
[16] ShuoJi,XiaodongLu,MingzheLiu,LeileiSun,ChuanrenLiu,BowenDu,and Community-InvariantGraphContrastiveLearning.InICML.
HuiXiong.2023. Community-baseddynamicgraphlearningforpopularity [46] ZhenTan,RuochengGuo,KaizeDing,andHuanLiu.2023.VirtualNodeTuning
prediction.InSIGKDD.930–940. forFew-shotNodeClassification.arXivpreprintarXiv:2306.06063(2023).
[17] WeiJin,TylerDerr,YiqiWang,YaoMa,ZitaoLiu,andJiliangTang.2021.Node [47] PetarVeličković,GuillemCucurull,ArantxaCasanova,AdrianaRomero,Pietro
similaritypreservinggraphconvolutionalnetworks.InWSDM.148–156. Lio,andYoshuaBengio.2018.Graphattentionnetworks.InICLR.
[18] AnshulKanakia,ZhihongShen,DarrinEide,andKuansanWang.2019.Ascalable [48] PetarVelickovic,WilliamFedus,WilliamLHamilton,PietroLiò,YoshuaBengio,
hybridresearchpaperrecommendersystemformicrosoftacademic.InWWW. andRDevonHjelm.2019.DeepGraphInfomax.InICLR.
2893–2899. [49] NingWang,MinnanLuo,KaizeDing,LinglingZhang,JundongLi,andQinghua
[19] ThomasNKipfandMaxWelling.2016. Variationalgraphauto-encoders.In Zheng.2020.Graphfew-shotlearningwithattributematching.InCIKM.1545–
BayesianDeepLearningWorkshop. 1554.
[20] ThomasNKipfandMaxWelling.2017.Semi-supervisedclassificationwithgraph [50] SongWang,YushunDong,XiaoHuang,ChenChen,andJundongLi.2022.FAITH:
convolutionalnetworks.InICLR. Few-ShotGraphClassificationwithHierarchicalTaskGraphs.InIJCAI.
[21] NamkyeongLee,KanghoonYoon,GyoungSNa,SeinKim,andChanyoungPark. [51] XuWang,HuanZhao,Wei-weiTu,andQuanmingYao.2023. Automated3D
2023. Shift-robustmolecularrelationallearningwithcausalsubstructure.In pre-trainingformolecularpropertyprediction.InSIGKDD.2419–2430.
SIGKDD.1200–1212. [52] YuzhongWuandTanLee.2018. ReducingmodelcomplexityforDNNbased
[22] BrianLester,RamiAl-Rfou,andNoahConstant.2021.ThePowerofScalefor large-scaleaudioclassification.InICASSP.331–335.
Parameter-EfficientPromptTuning.InEMNLP.3045–3059. [53] ZonghanWu,ShiruiPan,FengwenChen,GuodongLong,ChengqiZhang,and
[23] PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,and SYuPhilip.2020.Acomprehensivesurveyongraphneuralnetworks.TNNLS
GrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyof 32,1(2020),4–24.
promptingmethodsinnaturallanguageprocessing. Comput.Surveys(2023), [54] TengXiao,ZhengyuChen,ZhimengGuo,ZeyangZhuang,andSuhangWang.
1–35. 2022.Decoupledself-supervisedlearningforgraphs.NeurIPS(2022),620–634.
[24] XiaoLiu,YananZheng,ZhengxiaoDu,MingDing,YujieQian,ZhilinYang,and [55] TengXiao,HuaishengZhu,ZhengyuChen,andSuhangWang.2023. Simple
JieTang.2021.GPTunderstands,too.arXivpreprintarXiv:2103.10385(2021). andasymmetricgraphcontrastivelearningwithoutaugmentations.Advancesin
[25] ZeminLiu,YuanFang,ChenghaoLiu,andStevenCHHoi.2021.Relativeand NeuralInformationProcessingSystems(2023).
absolutelocationembeddingforfew-shotnodeclassificationongraph.InAAAI. [56] ChenyanXiong,RussellPower,andJamieCallan.2017.Explicitsemanticranking
4267–4275. foracademicsearchviaknowledgegraphembedding.InWWW.1271–1279.
[26] ZeminLiu,XingtongYu,YuanFang,andXinmingZhang.2023.GraphPrompt: [57] KeyuluXu,WeihuaHu,JureLeskovec,andStefanieJegelka.2019.Howpowerful
Unifyingpre-traininganddownstreamtasksforgraphneuralnetworks.InWWW. aregraphneuralnetworks?.InICLR.
417–428. [58] YujunYan,MiladHashemi,KevinSwersky,YaoqingYang,andDanaiKoutra.
[27] YuanfuLu,XunqiangJiang,YuanFang,andChuanShi.2021.Learningtopre-train 2022. Twosidesofthesamecoin:Heterophilyandoversmoothingingraph
graphneuralnetworks.InAAAI.4276–4284. convolutionalneuralnetworks.InICDM.1287–1292.
[28] SitaoLuan,ChenqingHua,QinchengLu,JiaqiZhu,MingdeZhao,Shuyuan [59] ChengxuanYing,TianleCai,ShengjieLuo,ShuxinZheng,GuolinKe,DiHe,
Zhang,Xiao-WenChang,andDoinaPrecup.2022. Revisitingheterophilyfor YanmingShen,andTie-YanLiu.2021.Dotransformersreallyperformbadlyfor
graphneuralnetworks.Advancesinneuralinformationprocessingsystems(2022), graphrepresentation?.InNeurIPS.28877–28888.
1362–1375. [60] YuningYou,TianlongChen,YongduoSui,TingChen,ZhangyangWang,and
[29] YaoMa,XiaoruiLiu,NeilShah,andJiliangTang.2022.Ishomophilyanecessity YangShen.2020.Graphcontrastivelearningwithaugmentations.NeurIPS33
forgraphneuralnetworks?.InICLR. (2020),5812–5823.Non-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronym’XX,June03–05,2018,Woodstock,NY
[61] XingtongYu,YuanFang,ZeminLiu,YuxiaWu,ZhihaoWen,JianyuanBo,Xin-
mingZhang,andStevenCHHoi.2024. Few-ShotLearningonGraphs:from
Meta-learningtoPre-trainingandPrompting.arXivpreprintarXiv:2402.01440
(2024).
[62] XingtongYu,ZhenghaoLiu,YuanFang,ZeminLiu,SihongChen,andXinming
Zhang.2024.Generalizedgraphprompt:Towardaunificationofpre-training
anddownstreamtasksongraphs. IEEETransactionsonKnowledgeandData
Engineering(2024).
[63] XingtongYu,ZeminLiu,YuanFang,andXinmingZhang.2023. Learningto
countisomorphismswithgraphneuralnetworks.InAAAI.
[64] XingtongYu,ZeminLiu,YuanFang,andXinmingZhang.2024.HGPROMPT:
BridgingHomogeneousandHeterogeneousGraphsforFew-shotPromptLearn-
ing.InAAAI.
[65] XingtongYu,ChangZhou,YuanFang,andXinmingZhang.2024.MultiGPrompt
forMulti-TaskPre-TrainingandPromptingonGraphs.InWWW.
[66] SeongjunYun,MinbyulJeong,RaehyunKim,JaewooKang,andHyunwooJKim.
2019.Graphtransformernetworks.NeurIPS32(2019).
[67] YuhangZang,WeiLi,KaiyangZhou,ChenHuang,andChenChangeLoy.2022.
Unifiedvisionandlanguagepromptlearning.arXivpreprintarXiv:2210.07225
(2022).
[68] ShiqiZhang,YiqianHuang,JiachenSun,WenqingLin,XiaokuiXiao,andBo
Tang.2023.Capacityconstrainedinfluencemaximizationinsocialnetworks.In
SIGKDD.3376–3385.
[69] FanZhou,ChengtaiCao,KunpengZhang,GoceTrajcevski,TingZhong,andJi
Geng.2019.Meta-GNN:Onfew-shotnodeclassificationingraphmeta-learning.
InCIKM.2357–2360.
[70] JieZhou,GanquCui,ShengdingHu,ZhengyanZhang,ChengYang,ZhiyuanLiu,
LifengWang,ChangchengLi,andMaosongSun.2020.Graphneuralnetworks:
Areviewofmethodsandapplications.AIopen(2020),57–81.
[71] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiweiLiu.2022.Conditional
promptlearningforvision-languagemodels.InCVPR.16816–16825.
[72] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiweiLiu.2022.Learning
topromptforvision-languagemodels.IJCV(2022),2337–2348.
[73] JiongZhu,JunchenJin,DonaldLoveland,MichaelTSchaub,andDanaiKoutra.
2022.Howdoesheterophilyimpacttherobustnessofgraphneuralnetworks?
theoreticalconnectionsandpracticalimplications.InSIGKDD.2637–2647.
[74] JiongZhu,RyanARossi,AnupRao,TungMai,NedimLipka,NesreenKAhmed,
andDanaiKoutra.2021. Graphneuralnetworkswithheterophily.InAAAI.
11168–11176.
[75] JiongZhu,YujunYan,LingxiaoZhao,MarkHeimann,LemanAkoglu,andDanai
Koutra.2020.Beyondhomophilyingraphneuralnetworks:Currentlimitations
andeffectivedesigns.NeurIPS(2020),7793–7804.
[76] YanqiaoZhu,YichenXu,FengYu,QiangLiu,ShuWu,andLiangWang.2020.
Deepgraphcontrastiverepresentationlearning.arXivpreprintarXiv:2006.04131
(2020).Conferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.,XingtongYu∗,JieZhang∗,YuanFang†,andRenheJiang†
Appendices
sites.Inthisdataset,eachnoderepresentsasecondarystruc-
A Algorithm ture,andeachedgesignifiesaneighboringrelationshipei-
therwithintheaminoacidsequenceorinthree-dimensional
Wedetailthemainstepsforconditionalpromptgenerationandtun-
space.Nodesareclassifiedintothreecategories,whilethe
inginAlgorithm1.Inbrief,weiteratethrougheachdownstream
graphsthemselvesaredividedintotwoclasses.Theedge
task to learn the corresponding prompt vectors individually. In
homophilyratiois0.66.
lines3–5,wecomputetheembeddingforeachnodeusingthepre- • ENZYMES8[50]isacollectionof600enzymes,sourcedfrom
trainedgraphencoder,withthepre-trainedweightsΘ 0remaining
theBRENDAenzymedatabase.Theenzymesaredivided
fixedthroughouttheadaptationprocess.Inlines8–22,weoptimize
into6differentclasses,followingtheirtop-levelECenzyme
thecondition-net.Specifically,weperformtsimilarity-weighted
classification.Theedgehomophilyratiois0.67.
readout(lines9–11),generateprompts(lines12–13),modifynodes’ • Citeseer9[41]contains3,312scientificpapers,dividedintosix
embeddingsusingtheseprompts(lines12–15),andupdatetheem-
differentcategories.Thedatasetincludesacitationnetwork
beddingsfortheprototypicalnodes/graphsbasedonthefew-shot
with 4,732 edges. Each paper is represented by a binary
labeleddataprovidedinthetask(lines18–19).Notethatupdating
wordvector,indicatingthepresenceorabsenceofeachword
prototypicalnodes/graphsisnecessaryonlyforclassificationtasks.
fromadictionarycomprising3,703uniqueterms.Theedge
homophilyratiois0.74.
B HomophilyandNon-HomophilyMethods
• Cora10 [31] includes 2,708 scientific papers, divided into
WeprovidefurtherdetailsaboutthesetofpositivesamplesAand sevendistinctcategories.Thedatasetfeaturesacitationnet-
negativesamplesBofhomophilyandnon-homophilymethodsin workwith5,429edges.Eachpaperisrepresentedbyabinary
Table5. wordvector,indicatingwhethereachofthe1,433unique
wordsfromthedictionaryispresentorabsent.Theedge
C FurtherDescriptionsofDatasets
homophilyratiois0.81.
Weconductexperimentsontenbenchmarkdatasets. • BZR11[38]comprisesadatasetof405ligandslinkedtothe
• Wisconsin3[33]isanetworkof251nodes,whereeachnode benzodiazepinereceptor,representedasindividualgraph
structures. These ligands are divided into 2 distinct cate-
standsforawebpage,and199edgessignifythehyperlinks
gories.
connectingthesepages.Thefeaturesofthenodesarederived
• COX212[38]consistsofadatasetof467molecularstructures
fromabag-of-wordsrepresentationofthewebpages.These
representing cyclooxygenase-2 inhibitors. In this dataset,
pagesaremanuallyclassifiedintofivecategories:student,
eachnodecorrespondstoanatomandeachedgedenotesa
project,course,staff,andfaculty.Theedgehomophilyratio
chemicalbond—single,double,triple,oraromatic—between
is0.21.
• Cornell4 [33]isalsoawebpagenetwork.Itcomprises183 atoms.Themoleculesareclassifiedintotwocategories.
nodes,eachsymbolizingawebpage,and295edges,which
D FurtherDescriptionsofBaselines
represent the hyperlinks between these pages. The node
featuresareobtainedfromabag-of-wordsrepresentation Inthissection,wepresentmoredetailsforthebaselinesusedin
ofthewebpages.Thesepagesaremanuallysortedintofive ourexperiments.
categories:student,project,course,staff,andfaculty.The (1)End-to-endGraphNeuralNetworks
edgehomophilyratiois0.22. • GCN[20]:GCNutilizesamean-poolingstrategyforneigh-
• Chameleon5[39]isaWikipedianetwork,consistingof2,277 borhoodaggregationtointegrateinformationfromneigh-
Wikipediapages.Thepagesaredividedintofivecategories boringnodes.
accordingtotheiraveragemonthlytraffic.Thisdatasetcre- • GAT[47]:GATalsoleveragesneighborhoodaggregationfor
atesanetworkofpageswith36,101connections,andthe end-to-endnoderepresentationlearning,uniquelyassigns
nodefeaturesconsistofvariouskeynounsextractedfrom varyingattentionweightstodifferentneighbors,thereby
theWikipediapages.Theedgehomophilyratiois0.23. adjustingtheirimpactontheaggregationprocess.
• Squirrel6[39]comprises5,201Wikipediawebpagesofdis- • H2GCN[75]:H2GCNimprovesnodeclassificationbysepa-
cussingthedefinedtopics.Thedatasetisalsodividedinto ratingego-andneighbor-embeddings,usinghigher-order
fivecategoriesaccordingtotheiraveragemonthlytraffic. neighborhoods,andcombiningintermediaterepresentations.
Thisdatasetisapage-pagenetworkwith217,073edges,and Thesedesignshelpitperformwellonbothhomophilousand
thenodefeaturesarebasedonseveralinformativenounsin heterophilousgraphs.
theWikipediapages.Theedgehomophilyratiois0.30. • FAGCN[2]:FAGCNimprovesnoderepresentationbyadap-
• PROTEINS7[3]comprisesadatasetofproteingraphs,reflect- tivelycombininglow-andhigh-frequencysignalsusinga
ingvariouscharacteristicssuchasaminoacidsequences, self-gatingmechanism,makingiteffectivefordifferentnet-
conformations,structures,anduniquefeatureslikeactive worktypesandreducingover-smoothing.
3https://github.com/bingzhewei/geom-gcn/tree/master/new_data/wisconsin 8http://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip
4https://github.com/bingzhewei/geom-gcn/tree/master/new_data/cornell 9https://nrvis.com/download/data/labeled/citeseer.zip
5https://github.com/SitaoLuan/ACM-GNN/tree/main/new_data/chameleon 10https://relational.fit.cvut.cz/dataset/CORA
6https://github.com/SitaoLuan/ACM-GNN/tree/main/new_data/squirrel 11https://www.chrsmrrs.com/graphkerneldatasets/BZR.zip
7https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip 12https://www.chrsmrrs.com/graphkerneldatasets/COX2.zipNon-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Table6:Positiveandnegativesamplesforhomophilyandnon-homophilymethods.
Pre-trainingtask PositiveinstancesA𝑢 NegativeinstancesB𝑢 Homophilytask
Linkprediction[26,62,64] anodeconnectedtonode𝑢 nodesdisconnectedtonode𝑢 Yes
DGI[48] nodesingraph𝐺 nodesincorruptedgraph𝐺′ No
GraphCL[60] anaugmentedgraphfromgraph𝐺 augmentedgraphsfrom𝐺′≠𝐺 No
GraphACL[55] nodeswithsimilarego-subgraphtonode𝑢 nodeswithdissimilarego-subgraphtonode𝑢 No
Algorithm1ConditionalPromptLearningforProNoG nodebyenforcingidentityrepresentationsfromtwo-hop
Input: Pre-trainedgraphencoderwithparametersΘ 0, neighbors.
1: asetofdownstreamtasksT={𝑡 1,...,𝑡𝑛}. (3)GraphPromptModels
O 2u :tp fou rt: 𝑖←Opt 1im toiz 𝑛ed doparameters{𝜙𝑡 1,...,𝜙𝑡𝑛}of𝑛condition-nets
• GPPT[42]:GPPTutilizesaGNNmodelpre-trainedviaa
3: /*Encodinggraphsviapre-trainedgraphencoder*/ linkpredictiontaskwhichisastronghomophilymethod.
4: foreachgraph𝐺=(𝑉,𝐸,X)intask𝑡𝑖do The downstream prompt module is designed specifically
5: H←GraphEncoder(𝐺;Θ 0) fornodeclassification,aligningitwiththepre-traininglink
6: h𝑣 ←H[𝑣],where𝑣isanodein𝐺 predictiontask.
7: 𝜙𝑖 ←initialization • GraphPrompt[26]:GraphPromptemployssubgraphsim-
8: whilenotconvergeddo ilaritycalculationsasaunifiedtemplatetobridgethegap
9: foreachnode𝑣 ∈𝑉 intask𝑡𝑖do
betweenpre-traininganddownstreamtasks,includingnode
10: /*SubgraphsamplingandreadoutEq.(7)*/
andgraphclassification.Alearnablepromptisfine-tuned
11: Sample𝑣’s𝑘-hopsubgraph𝑆𝑣
12: s𝑣 ←Average({h𝑢·sim(h𝑢,h𝑣):𝑢 ∈𝑉(𝑆𝑣)}) duringdownstreamadaptationtoincorporatetask-specific
13: /*Generatepattern-basedpromptsbyEq.(8)*/ knowledge.
14: p𝑡𝑖,𝑣 ←CondNet(s𝑣;𝜙𝑡𝑖) • GraphPrompt+[62]:GraphPrompt+buildsonGraphPrompt
15: /*PromptmodificationbyEq.(9)*/ byintroducingaseriesofpromptvectorswithineachlayer
16: h˜ 𝑡𝑖,𝑣 ←p𝑡𝑖,𝑣⊙h𝑣 ofthepre-trainedgraphencoder.Thistechniqueutilizeshi-
17: h𝑡𝑖,𝐺 =Average(h˜ 𝑡𝑖,𝑣 :𝑣 ∈ V) erarchicalinformationfrommultiplelayers,beyondjustthe
18: /*Updateprototypicalsubgraphs*/ readoutlayer.
19: foreachclass𝑐intask𝑡𝑖do
20: h¯ 𝑡𝑖,𝑐 ←Average(h˜ 𝑡𝑖,𝑥:instance𝑥belongstoclass𝑐) E ImplementationDetailsofApproaches
21: /*Optimizingtheparametersincondition-net*/
22: CalculateL down(𝜙𝑖)byEq.(10) GeneralsettingsOptimizer.Forallexperiments,weusetheAdam
23: Update𝜙𝑖bybackpropagatingL down(𝜙𝑡𝑖) optimizer.
24: return{𝜙𝑡 1,...,𝜙𝑡𝑛}
Environment.Theenvironmentinwhichwerunexperimentsis:
– Linuxversion:5.15.0-78-generic
– Operatingsystem:Ubuntu18.04.5LTS
(2)GraphPre-trainingModels
– CPUinformation:Intel(R)Xeon(R)Platinum8352V
• DGI [47]: DGI operates as a self-supervised pre-training – GPUinformation:GeForceRTX4090(24GB)
methodologytailoredforhomogeneousgraphs.Itispredi-
Detailsofbaselines.Weusetheofficialcodeprovidedforallopen-
catedonthemaximizationofmutualinformation(MI),aim-
sourcebaselines.Eachmodelistunedaccordingtothesettings
ingtoenhancetheestimatedMIbetweenlocallyaugmented
recommendedintheirrespectivepublicationstoensureoptimal
instancesandtheirglobalcounterparts.
• GraphCL[60]:GraphCLleveragesavarietyofgraphaug- performance.Weuseearlystoppingstrategyfortrainingandset
patienceto50steps.Thenumberoftrainingepochsissetto2,000.
mentations for self-supervised learning, tapping into the
intrinsicstructuralpatternsofgraphs.Theoverarchinggoal • ForthebaselineGCN[20],weemploya3-layerarchitecture
istoamplifytheconcordancebetweendifferentaugmenta- onWisconsin,Squirrel,Chameleon,Cornelldatasetsand2-
tionsthroughoutgraphpre-training. layerarchitectureonCora,Citeseer,ENZYMES,PROTEINS,
• DSSL[54]:DSLLuseslatentvariablemodelingtodecouple COX2,BZRdatasets.Hiddendimensionsis256.
semanticsinneighborhoods,avoidingaugmentationsand • ForGAT[47],weemploya2-layerarchitectureandsetthe
optimizingwithvariationalinferencetocapturelocaland hiddendimensionto256.Additionally,weapply8attention
globalinformation,enhancingnoderepresentationlearning. headsinthefirstGATlayer.
• GraphACL[55]:GraphACLconsiderseachnodefromtwo • ForH2GCN[75],weemploya2-layerarchitectureandset
perspectives:identityrepresentationandcontextrepresenta- thehiddendimensionto256.
tion.Themodeltrainstheformerbypredictingthecontext • ForFAGCN[2],weemploya2-layerarchitecture.Thehyper-
representationofone-hopneighborsusinganasymmetric parametersettingis:eps=0.3,dropout=0.5,hidden=256.
predictor,andthenreconstructsthesamelatterofthecentral weusereluasactivationfunction.Conferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.,XingtongYu∗,JieZhang∗,YuanFang†,andRenheJiang†
Node classification Table7:Comparisonofthenumberoftunableparameters
duringthedownstreamadaptationphase.
Methods Wisconsin Chameleon Citeseer Cora
GCN 501,504 660,736 947,968 366,848
FAGCN 440,654 601,130 956,654 370,994
GraphCL 1,280 1,280 1,536 1,792
GraphACL 1,280 1,280 12,288 14,336
GraphPrompt 256 256 256 256
GraphPrompt+ 512 512 512 512
Cornell Citeseer ProNoG 1,024 1,024 1,024 1,024
Graph classification
keeptheotherhyper-parametersthesameasintheoriginal
demonstrationsintheirGithubrepository.
• ForGPPT[42],weutilizea2-layerGraphSAGEasitsbase
model,settingthehiddendimensionsto256.ForbaseGraph-
SAGE,wealsoemployameanaggregator.
• For GraphPrompt [26], we employ a 3-layer architecture
onWisconsin,Squirrel,Chameleon,Cornelldatasetsand2-
layerarchitectureonCora,Citeseer,ENZYMES,PROTEINS,
Wisconsin Cornell COX2,BZRdatasets.Hiddendimensionsaresetto256.
• ForGraphPrompt+[62],weemploya2-layerGCNonCora,
Citeseer,ENZYMES,PROTEINS,COX2,BZRdatasetsand
3-layerGCNontherestdatasets.Hiddendimensionsareset
to256.
DetailsofProNoG.ForourproposedProNoG,weutilizea2-layer
FAGCNarchitectureasbackboneforpre-trainingtaskwithgraph
contrastivemethodsforWisconsin,Squirrel,Chameleon,Cornell.
Especially,weimplementedge-droppingonsub-graphlevelfor
Wisconsin,Squirrel,Chameleon,Cornell.Hiddendimensionsare
BZR COX2 setto256.ForCora,Citeseer,BZR,COX2,weemploy1-layerGCN
asbasemodelforpre-trainingtask.Hiddendimensionsaresetto
256.ForPROTEINS,weemploy1-layerGCNonlinkpredictiontask
Figure5:Impactsofdifferentshotsonnodeandgraphclassi- forpre-training.Hiddendimensionsissetto64.ForENZYMES,
fication. weimplementDSSLforpretraining.Hiddendimensionsissetto
64.Allexperimentsareundertakenwiththeseedof39.Especially,
wefoundthatonChameleon,Squirrel,keepingtheoriginalnode
featuresasinputwithoutnormalizationperformsthebest,whilefor
• ForDGI[47],weutilizea1-layerGCNasthebasemodeland others,normalizationofnodefeaturesremainsroutine.Exceptfor
setthehiddendimensionsto256.Additionally,weemploy DSSL,weusecosine-similaritylossonnodelevelaslossfunction.
preluastheactivationfunction.
• ForGraphCL[60],a1-layerGCNisalsoemployedasitsbase F ImpactofShots
model,withthehiddendimensionssetto256.Specifically,
Wevarythenumberofshotsandconductnodeclassificationon
weselectedgedroppingastheaugmentations,withadefault
CornellandCiteseer,andgraphclassificationtasksonWisconsin,
augmentationratioof0.2.
Cornell,BZRandCOX2.TheresultsareillustratedinFig.5,and
• ForDSSL[54],thehiddendimensionsearchspaceisin{64,
weobservesamepatternsasshowninnodeclassificationtaskson
256,2048}.WereportthebestperformanceonPROTEINS
otherdatasets.
andENZYMESwithhiddensizeof64,CoraandCiteseer
with2048,andtherestdatasetswith256.Wekeeptheother
G Parametersefficiency
hyper-parametersthesameasintheoriginaldemonstrations
intheirGithubrepository. We evaluate the parameter efficiency of ProNoG compared to
• ForGraphACL[55],thehiddendimensionsearchspaceis other notable methods. Specifically, we evaluate the number of
in{64,256,1024,2048}.Wereportthebestperformanceon parameters that need to be updated or tuned during the down-
PROTEINS and ENZYMES with hidden size of 64 , Cora streamadaptationphase,andpresenttheresultsinTable7.For
andCiteseerwith2048,andtherestdatasetswith256.We GCNandFAGCN,sincethesemodelsaretrainedend-to-end,allNon-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Node classification Graph classification
Figure6:Sensitivitystudyof𝑚.
model weights must be updated, leading to the lowest parame-
terefficiency.Incontrast,forGraphCLandxxx,onlythedown-
streamclassifierisupdated,whilethepre-trainedmodelweights
remainunchanged,significantlyreducingthenumberofparam-
etersthatrequiretuning.Prompt-basedmethodsGraphPrompt,
GraphPrompt+,andProNoGarethemostparameter-efficient,as
promptsorcondition-netarelightweightandcontainfewerparam-
etersthantypicalclassifierslikefullyconnectedlayers.Notethat
thereportednumberofparametersforProNoGarebasedon𝑑 =2,
giventhatProNoGstillperformscompetitivelywithsuchhyperpa-
rametersetting.Althoughourconditionalpromptdesignrequires
toupdatemoreparametersthanGraphPromptandGraphPrompt+
duringdownstreamadaptation,theincreaseisminorcomparedto
updatingtheentireclassifierormodelweights,andthusdoesnot
poseamajorissue.
H HyperparameterAnalysis
Inourexperiment,weusea2-layerMLPwithabottleneckstruc-
tureasthecondition-net.Weevaluatetheimpactofthehidden
dimensionofthecondition-net𝑚 andreportthecorresponding
performanceinFig.6.Weobservethatforbothnodeandgraph
classification,as𝑚increasesfrom2,theperformancegenerallyfirst
decreasesbecausealarger𝑚introducesmorelearnableparameters,
whichmayleadtoworseperformanceinfew-shotsettings.How-
ever,afterreachingatrough,accuracystartstograduallyincrease
as𝑚growsfurther,sincehigherdimensionsincreasemodelcapac-
ity,untilreachingapeak.Thentheperformancefurtherdeclines
as𝑚 improves,givenmorelearnableparameters.Notethatthe
overallvariationinperformanceisgenerallysmall,andthepeakis
generallyat𝑚=2or𝑚=64.Inourexperiment,weset𝑚=64in
ourexperiments.