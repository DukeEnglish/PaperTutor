Factor Adjusted Spectral Clustering for Mixture Models
Shange Tang, Soham Jana and Jianqing Fan∗
Abstract
Thispaperstudiesafactormodeling-basedapproachforclusteringhigh-dimensional
data generated from a mixture of strongly correlated variables. Statistical modeling
with correlated structures pervades modern applications in economics, finance, ge-
nomics, wireless sensing, etc., with factor modeling being one of the popular tech-
niques for explaining the common dependence. Standard techniques for clustering
high-dimensional data, e.g., naive spectral clustering, often fail to yield insightful re-
sultsastheirperformancesheavilydependonthemixturecomponentshavingaweakly
correlatedstructure. Toaddresstheclusteringprobleminthepresenceofalatentfac-
tor model, we propose the Factor Adjusted Spectral Clustering (FASC) algorithm,
which uses an additional data denoising step via eliminating the factor component to
cope with the data dependency. We prove this method achieves an exponentially low
mislabelingrate,withrespecttothesignaltonoiseratiounderageneralsetofassump-
tions. Our assumption bridges many classical factor models in the literature, such as
the pervasive factor model, the weak factor model, and the sparse factor model. The
FASC algorithm is also computationally efficient, requiring only near-linear sample
complexity with respect to the data dimension. We also show the applicability of the
FASC algorithm with real data experiments and numerical studies, and establish that
FASC provides significant results in many cases where traditional spectral clustering
fails.
Keywords: Dependency modeling, dimensionality reduction, data denoising, mislabeling.
1 Introduction
1.1 Overview
Clustering is an important unsupervised problem in statistics and machine learning that
aims to partition data into two or more homogeneous groups. Applications exist in diverse
areas, ranging over biology (Herwig et al. 1999), finance (Cai et al. 2016), wireless sensing
(Mamalis et al. 2009), etc. In many modern applications, we encounter high-dimensional
data, which can significantly increase the computational complexity of standard clustering
algorithms such as K-means (Kumar et al. 2004). In addition, a stylized feature of high-
dimensional data is a common dependence among features that make clustering algorithms
statistically inefficient. These make it essential to produce clustering algorithms that in-
corporate a dimension reduction-based dependent-adjustment step before performing the
clustering step.
∗S.T. and J.F. are with the Department of Operations Research and Financial Engineering, Princeton
University, Princeton, NJ, email: shangetang@princeton.edu and jqfan@princeton.edu. S.J. is with the
DepartmentofACMS,UniversityofNotreDame,NotreDame,IN,email: sjana2@nd.edu.
1
4202
guA
22
]TS.htam[
1v46521.8042:viXraSpectral clustering (Von Luxburg 2007, Bach & Jordan 2003, Ng et al. 2001) has been
attheforefrontofsuchdimension-reduction-basedmethodsduetotheirefficiency. Spectral
methods attempt to utilize a low-dimensional data-driven projection strategy, which can
preservetheclustermembershipinformation. Forexample, L¨offleretal.(2021)attemptsto
utilize the properties of projecting the data along the subspace containing the span of the
actual cluster centers. The spectral clustering methods provably work well in the presence
of sub-Gaussian data (L¨offler et al. 2021, Abbe et al. 2022). However, the performance
guarantees of the vanilla spectral clustering techniques deteriorate significantly when the
underlyingdistributionshaveill-conditionedcovariancematrices(Davisetal.2021)orhighly
correlatedstructures(Lietal.2020). Thisisreasonableasthedirectionsofvariationsinthe
data set can be generated from both the directional components of the covariance matrix
and the subspace spanned by the cluster centroids, and the standard spectral methods are
incapable of differentiating them.
High-dimensional measurements are often highly related and admit a factor structure
that result in an ill-conditioned covariance matrix. Compared to numerous results for clus-
tering spherical data distributions (see Lu & Zhou (2016), Jana et al. (2024) and the refer-
ences therein), few papers in the literature address the challenges of clustering with depen-
dent and ill-conditioned covariance structures. In general, analyzing highly correlated data
is a stylized feature in many applications, and the dependency structure adversely affects
standard statistical tasks, such as model selection Fan, Ke & Wang (2020) and multiple
testing (Fan et al. 2019). Factor modeling (Bai & Ng 2002, Fan, Li, Zhang & Zou 2020)
is one of the widely used strategies for analyzing such data; important examples exist in
macroeconomics (Bai & Ng 2002), finance (Fama & French 2015), computational biology
(Fan et al. 2014), social science (Zhou et al. 2022), etc. It decomposes the measurements
intocommonpartsthatdependonlatentfactorsthroughaloadingmatrixandidiosyncratic
componentsthatareweaklycorrelated. Thenumberoflatentfactorsthatdrivedependence
among measurements is usually small.
This paper proposes the Factor Adjusted Spectral Clustering (FASC) algorithm that
guarantees remarkable data clustering performance. Our method first obtains dependence-
adjusted data before applying a spectral clustering method. This preliminary step ensures
that the transformed data have approximately isotropic covariance, enabling the spectral
steptoguaranteebetterperformance. Wewillformallyshowthatouralgorithmcanguaran-
teeasignificantlysmallclusteringerrorunderverygeneralconditionsonthedatadimensions
and the factor loading matrices. Notably, our assumptions allow covariances to be signifi-
cantly ill-conditioned, where the vanilla spectral method might fail. Simulation studies and
real data expositions lend further support to the claimed performance improvement.
To gain the insights of our method, let us consider the following toy example of the
Gaussian mixture model in d-dimensions:
1
P(y =1)=P(y =−1)= and x |y ∼N(y µ,Σ), µ∈Rd,Σ∈Rd×d, (1)
i i 2 i i i
withthegoaltorecovertheunobservedclasslabely ∈{±1}foreachobservationx . Given
i i
µandΣ,theoptimalclassifierissign(⟨x ,Σ−1µ⟩),wheresign(a)= a . Themisclassication
√ i |a|
rate is Φ(− SNR), where the signal-to-noise ratio is SNR = µTΣ−1µ. However, when the
variables are highly correlated, as the following numerical experiment exhibits, this optimal
2Figure 1: Comparison of mislabeling errors for FASC, vanilla spectral clustering, and the
optimal one for t=1,...,100.
mislabeling rate is not always achievable by spectral clustering. Consider model (1) with
n=200, d=100, µ=(10,0,··· ,0)T, Σ=tBBT+I ,t∈{0,1,...,100}
d
B ∈R100×3, B i ∼id N(0,1).
ij
For each given t, we implement the spectral clustering algorithm (Algorithm 1 (Abbe et al.
2022, Zhang & Zhou 2022)) on {x }n to detect the two clusters. We plot the mislabeling
i i=1 √
rate of spectral clustering and the optimal rate Φ(− SNR) as a function of t in Fig. 1. The
result reveals that the vanilla spectral clustering algorithm’s performance deteriorates as
the covariance matrix deviates from the isometric structure. An explanation is as follows.
It is known that the vanilla spectral clustering algorithm (Abbe et al. 2022, Zhang & Zhou
2022) can achieve a mislabe √ling rate of e−Ω(∥µ∥2 2/∥Σ∥2), which might be significantly smaller
than the optimal rate Φ(− SNR) ≈ e−Ω(SNR). When t increases, the SNR remains high,
but ∥Σ∥ can be quite large, resulting in a small ∥µ∥2/∥Σ∥ , which can explain the poor
2 2 2
mislabelingrateofthevanillaspectralclustering,asFig.1shows. Incomparison,theabove
figure shows that the mislabeling obtained by the FASC algorithm is close to the optimal
performance.
Our proposed method, FASC, has many advantages. Firstly, FASC applies to a wide
range of scenarios, including
• the classical factor model with pervasiveness (Fan et al. 2021), where the singular
values of the factor loading matrix scales as to the square root of data dimension,
• the weak factor model (Onatski 2012, Bryzgalova 2015, Lettau & Pelger 2020) where
the singular values of the factor loading matrix are of constant order,
3• the sparsity induced factor models (Uematsu & Yamagata 2022) where the singular
values of the factor loading matrix can scale at different rates.
Secondly, our clustering scheme works in high-dimensional setups with dimensions close
to the sample size (up to logarithmic factors) and is computationally efficient. We also
show that under a general data-generating model that encompasses all the above scenarios,
the mislabeling rates produced by FASC are similar to the optimal mislabeling rate for
clustering anisotopic Gaussian mixture models (Chen & Zhang 2021). Under the scenarios
we consider, where the covariance matrices are ill-conditioned, previous works that achieve
nice mislabeling guarantees are either computationally inefficient or require significantly
smallerdimensionalregimesthantheoneweconsider, e.g., samplecomplexityofn=Ω(cid:101)(d2)
or even higher (Davis et al. 2021, Brubaker & Vempala 2008, Ge et al. 2015, Bakshi et al.
2022). Thirdly, although a few previous works considered clustering with an underlying
factor structure (Longford & Muth´en 1992, Subedi et al. 2013, Viroli 2010), none of them
provide a theoretical guarantee for the proposed algorithms since the underlying factor
structuremakestheanalysisofclusteringmuchmorecomplicated. Inparticular,theideaof
factoradjustmentinFASCtoremovethefactorcomponentfromthedatahasbeenexplored
beforeinFan,Ke&Wang(2020)andFanetal.(2019)inthecontextofmodelselectionand
multipletesting. Comparedwiththeabove,theclusteringtaskissignificantlychallengingas
multipledata-generatingclustersmakeitdifficulttodifferentiatethedirectionalcomponents
in the data due to factors and the cluster locations. We provide guarantees for our method
FASC by carefully analyzing the relationship between the factors and the idiosyncratic
components while preserving the information on cluster memberships.
The rest of the paper is organized as follows. We describe the problem setup of the
mixturemodelwithfactorstructureinSection2. Section3introducestheFASCalgorithm.
The theoretical guarantees of the FASC algorithm is described in Section 4. We provide
numerical examples and real data-based studies in Section 5 and Section 6. The proof
sketchforourmaintheoreticalresultsisprovidedinSection7. Thedetailsoftheproofand
related technical results are included in Appendix A and Appendix B, respectively, in the
supplemental material.
1.2 Related works
The study of mixture models has an illustrious history. The problem of data classifica-
tion from a mixture model dates back to the seminal work Fisher (1936). Friedman (1989)
showed the necessity for the sample size n to exceed the dimension d to achieve the Bayes
optimal rate of misclassification. In scenarios where d surpasses n, Bickel & Levina (2004)
explainedthesuperiorityofthe“independence”ruleovertheFisherlineardiscriminantrule.
Additionally, Fan & Fan (2008) contributed to this body of knowledge by shedding light on
thechallengesposedbynoiseaccumulationinhigh-dimensionalfeaturespaceswhenemploy-
ing all features for classification. In response, they introduced FAIR, which selects a subset
of critical features for high-dimensional classification, yielding commendable performance.
WhenthedataisgeneratedfromaGaussianmixturemodel,anotherinterestinglineofwork
focuses on estimating the underlying mixture distribution, e.g., using EM algorithms (Wu
& Zhou 2021, Balakrishnan et al. 2017, Daskalakis et al. 2017, Kwon & Caramanis 2020),
the moment methods (Ma et al. 2023, Doss et al. 2023, Liu & Moitra 2021, 2022), etc.
Clustering problems, particularly mislabeling minimization, are relatively less explored
until recent developments. In the case of sub-Gaussian distributions, when the covariance
matrix is known to be nearly spherical, i.e., Σ = I , many methods have been proposed,
d
4including iterative methods(Lu & Zhou 2016, Jana et al.2023, 2024) and spectral methods
(Vempala & Wang 2004, Jin et al. 2017, Ndaoud 2018, Lo¨ffler et al. 2021). Of particular
relevance to our investigation, L¨offler et al. (2021) demonstrated that vanilla spectral clus-
teringachievestheoptimalmislabelingratewithalinearsamplecomplexityn=Ω(d)inthe
context of Gaussian mixture models with spherical covariance matrices. In particular, they
show that when the Gaussian components have centroids given by µ ,...,µ and each of
1 K
the error coordinates has a variance σ2, the optimal mislabeling rate is given by e−Ω(SNR)
with SNR =
mini̸=j∈[K]∥µi−µj∥2
2. In scenarios where the covariance matrix Σ is unknown,
σ2
Abbeetal.(2022),Zhang&Zhou(2022)showedspectralclusteringachievesthemislabeling
rate e−Ω(S), where S =
mini̸=j∈[K]∥µi−µj∥2
, with a (nearly) linear sample size n = Ω(cid:101)(d) for
∥Σ∥
sub-Gaussian mixture models. However, we will show that the FASC algorithm achieves a
much smaller mislabeling rate with an underlying factor structure.
The FASC algorithm we propose here draws inferences about the underlying factor
structure using the top eigenvectors of the sample covariance matrix and requires the exact
knowledgeofthenumberoffactors. Thisiscloselyrelatedtotheuseofprincipalcomponents
analysis (PCA) (J.H. & Watson 2002) in factor analysis, which has emerged as the most
prevalent technique in the literature. Multiple variants of PCA have also been introduced
forfactorestimation, includingprojectedPCA(Fanetal.2016), diversifiedprojection(Fan
& Liao 2019), and others. Extensive exploration of PCA’s asymptotic behavior under high-
dimensionalscenarioshasbeenconductedintheliterature,includingJohnstone&Lu(2009),
Fan et al. (2013), Shen et al. (2016), Fan & Wang (2015), and more. On the other hand,
a long line of research has been dedicated to the estimation of the number of factors, with
notablestudiesconductedbyBai&Ng(2002),Luoetal.(2009),Hallin&Liˇska(2007),Lam
&Yao(2012),andAhn&Horenstein(2013),Fanetal.(2022),amongothers. Inparticular,
Fan&Gu(2023)suggestedthatonecandrawsignificantconclusionsifthenumberoffactors
is known within a neighborhood of the true value; however, their strategy relies on using a
neuralnetworktoapproximatetheunderlyingstructureefficiently. Whethersuchastrategy
is implementable for clustering purposes is beyond the scope of the current work.
2 Problem setup
2.1 Notations
Let [K] denote the set of integers {1,...,K}. Let ∥ · ∥ be the ℓ norm of a vector or
2
the spectral norm of a matrix. We denote by ∥ · ∥ the Frobenius norm of a matrix.
F
For a matrix M, we denote by σ (M) and σ (M) the smallest singular value and
min max
the largest singular value of M, respectively. For a symmetric matrix A, we denote by
λ (A) and λ (A) the smallest eigenvalue and the largest eigenvalue of A, respectively.
min max
In addition, given any symmetric matrix A ∈ RK×K we will denote its eigenvalues as
λ (A) := λ (A) ≥ λ (A) ≥ ··· ≥ λ (A) = λ (A). For a random variable X ∈ R,
max 1 2 K min
we say X ∈ SG(σ2), where SG(σ2) is the class of (one-dimensional) sub-Gaussian random
variables with parameter σ2, if Eet(X−E[X]) ≤ e1 2σ2t2 for any t ∈ R. For a random vector
X ∈ Rd, we say X ∈ SG (σ2), where SG (σ2) denotes the class of d-dimensional sub-
d d
Gaussian random variables with parameter σ2, if vTX ∈ SG(σ2) holds for any unit vector
v ∈Rd. We use c,C ,C ,... to denote constants that may differ from line to line.
0 1
52.2 Mixture model with factor structure
We consider the additive noise model with K clusters to describe the data x ,...,x ,
1 n
x ∈Rd, x =µ +e , y ∈[K], e =Bf +ε , i∈[n], (2)
i i yi i i i i i
where
• µ ,...,µ are the centroids corresponding to the K clusters,
1 K
• for each i∈[n], y denotes the cluster label of data x ,
i i
• B ∈Rd×r is the factor loading matrix and f ,...,f ∈Rr are the latent factors,
1 n
• the additive noises ε ,...,ε are independent mean zero random vectors in Rd.
1 n
Theadditivenoisemodelisstandardintheliteratureformodelingmixturesofsub-Gaussian
distributions (Lu & Zhou 2016, Balakrishnan et al. 2017, Klusowski & Brinda 2016). We
make the following distributional assumptions on the random quantities described above.
Assumption 1. The factor and error variables are distributed as follows.
(a) f ∈SG (c2) for some constant c >0 and E[f ]=0,E(cid:2) f fT(cid:3) =I ,
i r 1 1 i i i r
(b) {y }n i ∼id y with P[y =j] = p ,j ∈ [K]. We further assume that there is a constant
i i=1 j
c∈(0,1) such that min p > c and the model is centered, i.e., (cid:80)K p µ =0,
j∈[K] j K j=1 j j
(c) ε ∈SG (σ2) where 0<σ <c′ <1 for a sufficiently small constant c′ >0,
i d
(d) d→∞ as n→∞ and r,K ≤C for a large constant C.
Remark 1. We explain the above assumptions here. Assumption 1(a) is a standard reg-
ularity assumption in factor model literature. We assume the covariance of the factor f
i
is I so that the model is identifiable (we can always rotate B to satisfy this assumption).
r
Also, we require the factor f to be sub-Gaussian, which is also standard. Assumption 1
i
(b) essentially centralize the cluster centers so that the mean of the data is zero, see (Abbe
et al. 2022, Assumption 3.1) for similar conditions. Also, we do not want any of the cluster
sizes to be too small so that the clusters degenerate, therefore we require min p > c .
j∈[K] j K
Assumption 1 (c) is a standard sub-Gaussian assumption for the noise. Assumption 1 (d)
describes the regime we consider, where the number of factors and the number of clusters
are both at a constant level, which is common in factor model literature and cluster anal-
ysis literature. Also, we consider the scenario where both n and d go to infinity, which is
common in modern high-dimensional data.
Mislabeling rate The goal of this work is to construct an algorithm that produces esti-
mates y ={y }n of the true labels y ={y }n to guarantee minimum mislabeling error.
(cid:98) (cid:98)i i=1 i i=1
Notably the labels can only be learned up to a permutation ambiguity. Given the above,
we define the minimum mislabeling as
M(y,y)=n−1 min |{i∈[n]:y ̸=τ(y )}|.
(cid:98) (cid:98)i i
τ∈SK
where S is the set of all permutations of [K]. Our goal is to design an efficient algorithm
K
that obtains y from the unlabeled data {x }n with a small M(y,y).
(cid:98) i i=1 (cid:98)
63 Method: Factor Adjusted Spectral Clustering (FASC)
For clarity of presentation, we first introduce the vanilla spectral clustering method in
Algorithm 1. This algorithm can obtain a vanishing mislabeling error when the data is
weakly correlated (L¨offler et al. 2021).
Algorithm 1 Spectral clustering
1: Input: {x i}n i=1. The number of clusters K, target dimension of embedding k ≤ K,
error threshold ϵ>0.
2: Compute V(cid:98) := (ξ 1,··· ,ξ k), where ξ 1,··· ,ξ k are the top k right singular vectors of
(x ,··· ,x )T.
1 n
3: Conduct (1 + ϵ)-approximate K-means (Kumar et al. 2004) on {V(cid:98)Tx i}n i=1, getting
{y }n and {µ }K such that:
(cid:98)i i=1 (cid:98)j j=1
n n
(cid:88) (cid:88)
i=1∥V(cid:98)Tx i−µ (cid:98)y(cid:98)i∥2
2
≤(1+ϵ) {µ(cid:101)j}m
K
j=in 1∈Rk{ i=1∥V(cid:98)Tx i−µ (cid:101)y(cid:101)i∥2 2}.
{y(cid:101)i}n i=1∈[K]
4: Output: {y (cid:98)i}n i=1.
Totacklethecasewithstrongdatadependency,weleverageideasfromfactoranalysisto
propose the Factor Adjusted Spectral Clustering (FASC) algorithm (Algorithm 2). Under
model (2), we can write
x =Bf +u , u =µ +ε . (3)
i i i i yi i
Note that the idiosyncratic component u essentially retains the cluster information y of
i i
the data x via µ . Under the assumption that the covariance structure of the idiosyn-
i yi
cratic component is well-conditioned, an application of Algorithm 1 on {u }n would have
i i=1
guaranteed label outputs with a small mislabeling error, as the variance component from
Bf has been subtracted. Unfortunately, {u }n would be unknown in practice, and one
i i=1
of our contributions to the FASC algorithm is to estimate it. The approach of FASC is to
first learn the latent factor components Bf by using principal component analysis, esti-
i
mate the idiosyncratic components u = x −Bf , and then apply the spectral clustering
i i i
algorithm(Algorithm1)ontheestimatedidiosyncraticcomponentstoobtainthefinallabel
estimates. By subtracting the latent factor components, we not only approximately decor-
relate the variables but also reduce the noise in the data, enabling the spectral clustering
method to achieve the optimal mislabeling rate. The formal description of the FASC algo-
rithmisgivenbelow. Foreaseofprovingtheoreticalresults, lateron, weincorporateadata
splitting step. For simplicity of notations, we assume the sample size is 2n.
7Algorithm 2 Factor Adjusted Spectral Clustering (FASC)
1: Input: Dataset{x i}2 i=n 1. Thenumberoffactorsr,clustersK,andthetargetdimension
of embedding k ≤K.
2: Split the data {x i}2 i=n
1
into two halves, and use the second half of data to compute V(cid:98)r:
Let Σ(cid:98) := n1 (cid:80)2 i=n n+1x ixT i. Then we write the eigendecomposition of Σ(cid:98):
d
(cid:88)
Σ(cid:98) = λ(cid:98)jv (cid:98)jv (cid:98)jT,
j=1
where λ(cid:98)1 ≥λ(cid:98)2 ≥···≥λ(cid:98)d ≥0. Let
V(cid:98)r :=(v (cid:98)1,··· ,v (cid:98)r)∈Rd×r. (4)
3: Compute u (cid:98)i for i=1,··· ,n:
u
(cid:98)i
=x i−V(cid:98)rV(cid:98) rTx i, i=1,··· ,n. (5)
4: Compute {y (cid:98)i}n
i=1
as the output of applying Algorithm 1 on {u (cid:98)i}n
i=1
with projection
dimension k and number of clusters K.
5: Output: {y (cid:98)i}n i=1.
Remark 2. In Algorithm 2 we obtain the label estimates {y ,...,y }. By switching the
(cid:98)1 (cid:98)n
roleof{x ,...,x }toestimatethefactorcomponents,wecansimilarlyestimatethesecond
1 n
half of clustering labels as {y ,...,y }. However, it is expected that there might be a
(cid:98)n+1 (cid:98)2n
permutationambiguityoftheestimatedlabelsinthefirstandsecondhalfofthedata. This
canbeeasilyresolvedbymatchingthelabelscorrespondingtoclusterswithsimilarcentroid
estimates in the first and second halves. This is justified because the label estimates via
spectralclustering can leadto consistent estimationof thecluster centroids (Zhang& Zhou
2022, Proposition 3.1). However, the related technical details are beyond the scope of the
current work.
Remark3. Theprojectiondimensionkrequiredtoguaranteedesiredtheoreticalproperties
of Algorithm 1 (which extends to the guarantees of the FASC algorithm) depends on the
rank of the subspace containing the cluster centers and might not be known beforehand. In
such scenarios, one might want to choose k = K as the number of clusters to be extracted
is often known from practical experience or according to problem specifications. When the
underlying data-generating distributions consist of an unknown number of mixture com-
ponents, the problem of estimating K is well-defined in the literature. For example, see
Hamerly & Elkan (2003), Sugar & James (2003) and the references therein for a detailed
discussion. However, such analysis is beyond the scope of the current manuscript, and we
will assume that we know the values of k,K. See the previous work of Abbe et al. (2022)
for similar assumptions.
84 Theory
4.1 Main results
Fortherestofthepaper,wewillassumethefollowingregularityconditionsonthecentroids
µ ,...,µ of the underlying model (2).
1 K
Assumption 2 (Regularity). (a) There is a constant c such that n≥c dlogn.
1 1
(b)
E(cid:2)
µ
µT(cid:3)
is rank k and there exist constants c ,c >0, such that
yi yi 2 3
c ≥λ
(E(cid:2)
µ
µT(cid:3)
)≥λ
(E(cid:2)
µ
µT(cid:3)
)≥c .
2 1 yi yi k yi yi 3
(c) There exist constants c ,c >0 such that
4 5
c ≥max∥µ −µ ∥≥min∥µ −µ ∥≥c .
5 i j i j 4
i̸=j i̸=j
Remark 4. Assumption 2(b) is similar to (Abbe et al. 2022, Assumption 3.1) and ensures
that the spectral projection step in Algorithm 1 captures all the major directions in the
space spanned by the mean vectors µ ,...,µ . The inequality min ∥µ −µ ∥ ≥ c in
1 K i̸=j i j 4
Assumption2(c)isastandardseparationcriteriatoguaranteesmallmislabelingerrors. The
condition c ≥max ∥µ −µ ∥ in Assumption 2 (c) controls the variation in the data set
5 i̸=j i j
along the space spanned by the centroid vectors to ensure that the variation due to the
factor components can be separated easily. See Abbe et al. (2022), Zhang & Zhou (2022),
L¨offler et al. (2021) for a discussion on such comparable regularity conditions.
Next, we assume a set of general conditions on the factor loading matrix B.
Assumption 3 (Factor loading matrix). Let n ≥ Cd(logn)3 for a large constant C > 0.
Suppose that E(cid:2) µ µT(cid:3) and BBT obtain the following spectral decompositions
yi yi
E(cid:2) µ yiµT yi(cid:3) =MΛ(cid:101)MT, BBT =UΛUT, Λ(cid:101),Λ∈Rk×k are diagonal. (6)
Then there are constants {γ }4 such that the following holds true.
i i=1
(a) σ (B)2 ≥3(∥E(cid:2) µ µT(cid:3) ∥+σ2) and σ (B)≥1.
min yi yi max
(b) σ σm ma inx (( BB )) 22 (σ max(B)+√ d)≤γ 1(σ∨ √ lo1 gn)(cid:113) lon gn.
(cid:113)
(c) ∥UTM∥ ≤ γ and σmax(B)(∥UTM∥+σ2) ≤ γ (σ∨ 1 ). If the noise variances
2 σmin(B)2 3 logn
(cid:8)E(cid:2)
ε
εT(cid:3)(cid:9)n
have a common isotropic structure, then the final assumption can be
i i i=1
(cid:113)
weakened to σmax(B)∥UTM∥≤γ (σ∨ 1 ).
σmin(B)2 4 logn
Remark 5. Assumption 3(a) describes the scenario where the component of data vari-
ability due to the factors is much larger than the data variability due to the presence of
different clusters. This enables us to filter out the factor component before applying the
spectral method in Algorithm 1. Assumption 3(b) is a regularity condition that controls
the discrepancy between the different directions of data deviations resulting from the un-
derlying factors. Assumption 3(c) ensures that the space of factors and the space of the
9centroidslieinnearlyorthogonaldirectionssothattheireffectscanbeseparatedeasily. Our
assumptions are sufficient to guarantee a consistent estimation of clusters, and optimizing
theassumptionsisleftforfuturework. However,inthelatersections,wewillseethatthese
conditions are general enough to include various standard setups in factor modeling.
With the above assumptions, we are now able to state our first main theoretical result.
Denote the signal-to-noise ratio
min ∥µ −µ ∥2
SNR:= i̸=j i j , (7)
σ2
which we will use to describe the related mislabeling guarantees.
Theorem 1. Consider the model (2). Let y :=(y ,··· ,y ) be the output of Algorithm 2,
(cid:98) (cid:98)1 (cid:98)n
y := (y ,··· ,y ) be the true labels. Then, under Assumption 2 and Assumption 3 , there
1 n
exist constants C,C¯ and c such that
(a) If SNR>Clogn, then lim P[M(y,y)=0]=1.
n→∞ (cid:98)
(b) If C¯ ≤SNR≤Clogn, then E[M(y,y)]≤exp(−c·SNR) for all sufficiently large n.
(cid:98)
Theorem 1 shows that the FASC algorithm proposed in Algorithm 2 achieves the expo-
nentially small mislabeling rate exp(−c·SNR) with near linear (with respect to the data
dimension) sample complexity. The above mislabeling rate is significantly smaller than the
existing error bounds in the literature that are attained by the vanilla spectral clustering
method in Algorithm 1. For example, the mislabeling rates for the spectral clustering al-
gorithm presented in Abbe et al. (2022), Zhang & Zhou (2022) is given by exp(−Ω(S))
where S := mini̸=j∥µi−µj∥2 . Note that ∥BBT∥ ≥ max ∥B ∥2 is typically of order d,
∥BBT+σ2Id∥ j≤k j
where B is the jth column of B. Thus, S is typically an order of magnitude smaller than
j
SNR. This shows that our results are comparatively much stronger. The performance dif-
ference between FASC and spectral clustering will be further demonstrated by simulations
in Section 5.
4.2 Implications for factor model with the pervasiveness condition
Inthissubsection,weassumethefactorloadingmatrixB satisfiesthefollowing“pervasive-
ness” condition.
Assumption 4 (Pervasiveness). C d≤σ2 (B)≤σ2 (B)≤C d for constants C ,C >
1 min max 2 1 2
0.
This condition is common in the related literature (see Fan et al. (2021) for discus-
sions) regarding modeling purposes. For example, the condition is satisfied if we assume
that each of the entries B are generated iid from a SG(1) distribution. In addition, in
ij
high-dimensional scenarios, this assumption helps separate the factor component from the
idiosyncratic component in the data, enabling a consistent learning of the factors.
Assumption 5 (Approximate perpendicularity). There exists a small enough constant c
for which the eigen-decompositions of E(cid:2) µ µT(cid:3) and BBT as defined in (6) satisfy
yi yi
(cid:26)√ (cid:18) (cid:114)
1
(cid:19) (cid:27)
∥UTM∥≤min d σ∨ ,c .
logn
10Remark 6. Assumption 5 can be easily satisfied in the high-dimensional regime, which is
the area of interest here. A simple example that satisfies the above assumption is that the
space of the mean vectors and the space of the factor loadings are generated independently.
In particular, consider the Grassmannian manifold G that consists of all k-dimensional
d,k
subspacesofRd andletUnif(G )betheuniformdistributiononthecollectionofrandomly
d,k
chosenk-dimensionalsubspaceofRd (seeVershynin(2018)foradetaileddescription). Then
we have the following result (see Appendix B.1 for a proof) that shows that Assumption 5
holds with a high probability whenever d≥C logn for a large constant C >0.
1 1
Lemma2. LetB ∈Rd×r beafixedmatrix,U ∈Rd×r bethematrixofleftsingularvectors
of B, and M ∼Unif(G ). Then we have the following for some constants c,c>0.
d,k (cid:101)
• If d≥clogn then ∥UTM∥≤c with a probability at least 1−n−9.
(cid:101)
• If d>c(logn)2, then ∥UTM∥≤ √ (cid:101)c with a probability at least 1−n−9.
logn
As Assumption 4 and Assumption 5 are specialized versions of Assumption 3, we can
conclude the following.
Corollary 3. The outcomes of Theorem 1 hold under the pervasiveness condition in As-
sumption 4 and the approximate perpendicularity condition Assumption 5.
4.3 Implications for models with weak factors
Next, we describe the “weak factor” model that satisfies the following condition.
Assumption6(Weakfactorcondition).
ThereexistsaconstantC,suchthat3(∥E(cid:2)
µ
µT(cid:3)
∥+
yi yi
σ2)≤σ2 (B)≤σ2 (B)≤C.
min max
The above modeling assumption includes factor models previously studied in Onatski
(2012), Bryzgalova (2015), Lettau & Pelger (2020). For example, Lettau & Pelger (2020)
assumes that BTB is approximately I , which implies that σ (B) is of constant order.
r max
The weak factor scenario makes it harder to separately detect the variation due to factors
and cluster centroids, as both of them are now of constant order. As a result, we will
need a slightly stronger “approximate perpendicularity” condition. The approximate per-
pendicularity assumption we use here can be easily satisfied in the high-dimensional regime
whenever d≥Ω((logn)2) (Lemma 2).
Assumption 7 (Approximate perpendicularity for weak factor models). Assume
(cid:26) (cid:114) (cid:27)
1
∥UTM∥≤min σ∨ ,c .
logn
for some c>0, where we adopt the notation of eigen-decomposition in (6).
As the above assumptions satisfy Assumption 3, we obtain the following theoretical
guarantees for the FASC algorithm under the “weak factor” scenario.
Corollary 4. The outcomes of Theorem 1 hold under the conditions in weak factor condi-
tion in Assumption 6 and the corresponding perpendicularity condition Assumption 7.
114.4 Implications for models with disproportionate factor loadings
Finally, we analyze the scenario where the smallest and the largest singular values of the
factor loading matrix can scale with the dimension at a significantly different rate.
Assumption8(Disproportionatesingularvaluecondition). ThereexistsaconstantC such
that 3(∥E(cid:2) µ µT(cid:3) ∥+σ2)≤σ2 (B)≤σ2 (B)≤Cd and lim σmin(B) →0.
yi yi min max d→∞ σmax(B)
The above modeling assumption includes the sparsity-induced factor model previously
studied in the literature. For example, while studying the factor models in Uematsu &
Yamagata (2022), the authors assume that the eigenvalues of BBT satisfy λ ℓ(BBT)=dαℓ
where α > ··· > α are constants in (0,1). Under such a disproportionate eigenvalue
1 r
scenario, an even stronger perpendicularity assumption is needed to ensure that the factors
can be learned properly and the FASC algorithm can achieve the optimal mislabeling rate.
Thisassumptioncanbesatisfiedeasilybychoosingthecolumnsofthefactorloadingmatrix
from an orthogonal space with respect to the space of the true centroids. We note that it is
beyond the scope of the current work to see whether this condition can be satisfied with a
more classical data-generating setup, similar to the one mentioned in Lemma 2.
Assumption9(Approximateperpendicularity).
Lettheeigen-decompositionofE(cid:2)
µ
µT(cid:3)
yi yi
and BBT be defined in (6). We assume the following holds for some constant c:
(cid:26) (cid:27)
σ
∥UTM∥≤min √ ,c .
d
In view of the above assumptions, we have the following result.
Corollary 5. Consider the model (2) with E(cid:2) ε iεT i(cid:3) = σ2I
d
and let n ≥ C(cid:101)d3(logn)3 for
a large constant C(cid:101) > 0. Then the outcomes of Theorem 1 hold under the conditions in
Assumption 8 and Assumption 5.
Corollary 5 shows that under this scenario, when a strong version of “approximate per-
pendicularity” Assumption 9 holds, FASC can still achieve the optimal mislabeling rate.
However, the sample complexity needed is n=Ω(cid:101)(d3), which is much worse than the coun-
terpart in Corollary 3 and Corollary 4.
5 Simulation studies
5.1 Gaussian mixture models with correlated measurements
In the following simulations, we generate samples using the model (2) with the number
of clusters K = 5, the number of factors r = 3, and the data dimension d is varied over
the set {5,20,100,500}. For each such combination of d,K,r we repeat the following data
generation process 100 times, indexed by t=1,2,··· ,100, with sample size n=1000 each
• generatethefactorloadingmatrixB ∈Rd×r whoserowsaredrawnfromi.i.d. N(0,I )
r
• draw iid vectors {θ }K from N(0,1I ) and denote µ =θ − 1 (cid:80)K θ
j j=1 d d j j K i=1 i
• obtain class label {y }n i ∼id Uniform([K]), latent factors {f }n i ∼id N(0,I ) and
i i=1 i i=1 r
noise ε i ∼id N(0,σ2I ) with σ =0.01t
i d
12Figure 2: Mislabeling rates for FASC and vanilla spectral clustering with strong factors.
• produce the observed data data set {x }n with x =µ +Bf +ε .
i i=1 i yi i i
We then perform the FASC (Algorithm 2) and the vanilla spectral clustering (Algorithm 1)
on these generated data and compare their mislabeling proportions. For simulation pur-
poses, we use the entire data set to estimate the factors and perform the spectral step
rather than using data splitting as required in Algorithm 2.
In Fig. 2, we plot the mislabeling rate of FASC (red points) and spectral clustering
(blue points) with respect to σ = {0.01t}100 for different dimensions d = 5,20,100,500
t=1
respectively. From Fig. 2 it can be observed that vanilla spectral clustering fails for highly
correlated data and the FASC algorithm achieves improved mislabeling errors regardless of
the noise level. The explanation is that the “noise” Bf gets suppressed or removed by
i
the factor adjustment and hence increases the SNR. In the extreme case, FASC achieves
nearly exact recovery (mislabeling rate is close to 0) when σ is small. When σ grows, the
mislabelingrateofFASCgrowsuntilitreachesthelevelofspectralclustering. Whent=1,
evenwiththenoisefromthecommonpartBf isremoved,thesignalsarestilltooweakfor
i
goodlabeling. ThesimulationresultssupportTheorem1: whenSNRexceedssomeconstant
multiple of logn, Algorithm 2 is able to recover all the labels with high probability; when
SNR is not that large, FASC can still recover the cluster labels significantly better than the
spectral method, albeit with a non-zero mislabeling error.
Remark 7. We point out that for the cases d = 5, FASC does not perform as well as it
doeswhendislarger. Theapproximateperpendicularitymaynotholdwhendisverysmall,
andthereforethelatentfactorsanditsassociatedloadingmatrixcannotbewellestimated.
13Figure 3: Mislabeling rates for FASC on x versus spectral clustering on u , which is
i i
infeasible and ideal data after the factor adjustments.
Thiscanbejustifiedthroughfurtherexperiments: inthefollowingexperiments,weperform
spectralclusteringonu . Inthiscase,spectralclusteringshouldprovideoptimalmislabeling.
i
WecomparetheresultsofFASConx andspectralclusteringonu , whichistheinfeasible
i i
and ideal data after factor adjustments, in Fig. 3 with d = 5,20,100,500 respectively. We
canseethatwhend=5,FASCdoesnotperformaswellasthespectralclusteringusingthe
ideal data but still significantly outperform the case without factor adjustment as shown in
Fig.2. Ontheotherhand, whend=20,100,500, theperformanceofFASCarecomparable
withthespectralmethodwithidealdata. Thesephenomenonssupportourtheoryandshow
the effectiveness of Algorithm 2 under relatively high-dimension scenarios.
Different number of factors in algorithm In real datasets, the exact number r of
factors may be unknown. The following simulation study suggest that when applying Al-
gorithm 2, choosing an r slightly larger than the real number of factors can be helpful. In
Fig.4,theactualnumberofunderlyingfactorsis3,andweapplyFASCwithr =1,2,3,4,5
respectively and plot the mislabeling rate as a function of σ. When the chosen number of
factors in the algorithm is smaller than the actual number of factors (i.e., r = 1,2), FASC
performs poorly since the factor-adjusted data is still strongly dependent. On the other
hand, when the chosen number of factors in the algorithm is larger than the actual number
of factors (i.e., r =4,5), FASC provides much better performance. It performs worse than
thecasewithidealr =3duetothenoiseaccumulationaselucidatedinFan&Fan(2008).
14Figure 4: Mislabeling rates of FASC with a different number of factors.
5.1.1 High dimensional cases
Some experiments are also provided for the scenario that d is large compared to n with the
same data-generating process. We choose n = 100, and d = 500,2000 respectively. The
resultsareshowninFig.5,whereFASCstillperformsmuchbetterthanthevanillaspectral
clustering. However, when d = 2000, FASC can only achieve exact recovery when σ ≤ 0.1,
while in the previous section, e.g. Fig. 2, when n=1000,d=100, FASC can achieve exact
recovery for some σ ≥0.2. This phenomenon suggests that when d is much larger than the
sample size, the performance for FASC deteriorates. However, this deterioration is due to
the nature of spectral clustering, which we will show by comparing the results with those
of spectral clustering with infeasible ideal data. Similar to what we do in Fig. 3, in the
following experiments, we compare the results of FASC on x and spectral clustering on u
i i
in Fig. 6 with n = 100 and d = 500,2000 respectively. We can see that the performance
of FASC are still comparable with the spectral method with ideal data, even under the
scenario that d is large compared to n, showing the effectiveness of FASC.
5.2 Weak factor cases
In this section, we will investigate the “weak factor” cases. The data generating process
remains the same, except that we generate the factor loading matrix B ∈Rd×r whose rows
are drawn from iid √1 N(0,I r). The simulation results are provided in Fig. 7 for n=1000,
d
d = 100,500. Similar to Fig. 2, FASC still performs well under the weak factor scenario,
supporting our result in Corollary 4.
15Figure 5: The mislabeling rates for FASC and vanilla spectral clustering in high dimension.
Figure 6: FASC on x versus spectral clustering on u , in high dimension.
i i
Figure 7: FASC versus vanilla spectral clustering for the case with weak factors.
16Clustering algorithm Mislabeling rate for mice Mislabeling rate for DNA
protein expression data codon usage frequencies
data
K-means 0.659 0.610
Spectralclustering 0.657 0.608
FASC(r=1) 0.538 0.507
FASC(r=2) 0.569 0.565
FASC(r=3) 0.666 0.630
FASC(r=4) 0.645 0.700
Baseline(randomguess) 0.875 0.875
Table 1: Misclassification rate for different methods on mice protein expression data and
DNA codon usage frequencies data
6 Real data analysis
6.1 Mice protein expression
We analyze the mice protein expression data from UCI Machine Learning Repository:
https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression.
The dataset consists of the expression levels of 77 proteins measured in the cerebral cortex
of8classesofcontrol(whichweviewasthetrueclasslabel),containing1080measurements
per protein. Each measurement can be considered as an independent sample. We first
cleaned the missing values by dropping 6 columns “BAD N”, “BCL2 N”, “pCFOS N”,
“H3AcK18 N”,“EGR1 N”,“H3MeK4 N”andthenremovedallrowswithatleast1missing
value. After data cleaning and centralization, we get a dataset of 1047 samples each having
71attributes. WethenapplytheK-meansclustering(weusethestandardkmeansfunction
inRwhichrunstheHartigan-Wongalgorithm(Hartigan&Wong1979)),spectralclustering
and FASC to the unlabeled dataset with the number of clusters K = 8. We compare
the clustering results to the true class labels and calculate the mislabeling rate. We also
compare them with the “random guess” baseline, i.e. the case where we assign each data
point randomly to one of the eight clusters and then compute the mislabeling error. The
clustering results are stated in the second column of Table 1, where FASC with r = 1 or
2 performs much better than K-means and spectral clustering. However, for larger r, the
performance of FASC is not better than K-means or spectral clustering. This phenomenon
can be explained by the spectral structure of this dataset. We draw the scree plot Fig. 8a
which shows the eigenvalues of the dataset arranged in descending order. It is obvious that
thefirsttwoeigenvaluesaresignificantlylargerthanothers,suggestingthattheremayexist
two significant factors. Therefore it is appropriate to do factor adjustment on this dataset
with the number of factors r = 1 or 2, justifying the result that FASC performs well with
r =1 or 2.
6.2 DNA codon usage frequencies
WeanalyzetheDNAcodonusagefrequenciesdatafromUCIMachineLearningRepository:
https://archive.ics.uci.edu/dataset/577/codon+usage.
The dataset consists of codon usage frequencies in the genomic coding DNA of a large
sample of diverse organisms from different taxa tabulated in the CUTG database. Each
sample has 64 attributes that are the codon (including ‘UUU’, ‘UUA’, ‘UUG’, ‘CUU’, etc)
17(a) Scree plot of mice protein expression data (b) Scree plot of DNA codon frequency data
Figure 8: Scree plot of real data
frequency. The data are classified by their kingdom (which we view as the true class label):
‘bct’(bacteria), ‘pln’ (plant), ‘inv’ (invertebrate), ‘vrt’ (vertebrate), ‘mam’ (mammal), ‘rod’
(rodent), ‘pri’ (primate) and ‘vrl’(virus) (‘arc’,‘phg’,‘plm’ are removed since the number of
samples is very small). After data cleaning, we get a dataset of 12135 samples with 64 at-
tributes. WeapplytheK-meansclustering,spectralclusteringandFASCtothecentralized
unlabeled dataset with the number of clusters K = 8. We compare the clustering results
to the true class label and calculate the misclassification rate. The results of clustering are
stated in the third column of Table 1, where FASC with r = 1 or 2 performs much better
than K-means and spectral clustering. Similar to the mice protein expression analysis, this
phenomenon can also be explained by the spectral structure. In the scree plot Fig. 8b, the
first two eigenvalues are significantly larger than others.
7 Proof sketch of Theorem 1
Below we provide a brief argument to explain why the FASC algorithm can guarantee a
consistent clustering. On a high level, we can summarize Assumption 3 in the following
way:
• thecovariancecomponentcorrespondingtothefactorsisthemostsignificanttermfor
explaining the data variability cov(x ).
i
• thefactorcomponentsareapproximatelyorthogonaltotheidiosyncraticcomponents,
Our proof shows that these assumptions are sufficient to guarantee that the idiosyncratic
components can be extracted from the data. In view of (3) we can conclude that the
idiosyncratic components contains all the information regarding cluster memberships, so a
consistent estimation of the idiosyncratic components should suffice to extract a consistent
clustering.
First phase (extracting the idiosyncratic components): To estimate u we remove
i
the component of x that lies on the column space of B. We first estimate the matrix of
i
projectiononthecolumnspaceofB. ThepopulationcovariancematrixΣundermodel(2)
18is given by
Σ:=cov(x )=cov(Bf +u )=BBT+Σ , Σ =cov(u ). (8)
i i i u u i
To extract the factor component, in view of the above decomposition we use the first r
principle components of the sample covariance matrix
n
1 1 (cid:88)
Σ(cid:98) := nXTX =
n
x ixT i.
i=1
Let V(cid:98)r denote the top r eigenvectors of the eigen-decomposition of Σ(cid:98)
d
(cid:88)
V(cid:98)r :=(v (cid:98)1,··· ,v (cid:98)r), Σ(cid:98) = λ(cid:98)jv (cid:98)jv (cid:98)jT, λ(cid:98)1 ≥λ(cid:98)2 ≥···≥λ(cid:98)d ≥0.
j=1
Algorithm 2 uses the estimator of u
i
given by u
(cid:98)i
= (I −V(cid:98)rV(cid:98) rT)x i. In the case where all
the non-zero eigenvalues of BBT are significantly larger than the operator norm of Σ and
u
B ∈ Rd×r has a full column rank r, the matrix V(cid:98)rV(cid:98) rT should approximate the projection
matrix onto the column space of B. Therefore V(cid:98)rV(cid:98) rTx
i
is approximately equal to Bf i, and
hence, u is approximately equal to u .
(cid:98)i i
Second phase (spectral clustering on the estimated idiosyncratic components):
To show that an application of Algorithm 1 on {u }n will give us a consistent clustering,
(cid:98)i i=1
we will use the following guarantee for spectral clustering with sub-Gaussian data.
Lemma 6. (Zhang & Zhou 2022, Theorem 3.1) Consider the model
z =θ +ε , y ∈[K],i∈[n]. (9)
i yi i i
Let y := (y ,··· ,y ) be the output of Algorithm 1 on {z }n . Assume ε ∼ SG (σ2)
(cid:98) (cid:98)1 (cid:98)n i i=1 i d
independently with zero mean for each i ∈ [n]. Let β = K min |{i : y = a}| with
n a∈[K] i
βn/K2 >10. There exist constants C(cid:101),C(cid:101)′ >0 such that under the assumption that
min ∥θ −θ ∥ σ ([θ ,...,θ ]T)
ψ := i̸=j∈[K] (cid:113)i j >C(cid:101), ρ:= k √y1 √ yn >C(cid:101),
β−0.5K(1+ d)σ ( n+ d)σ
n
we have
min ∥θ −θ ∥2 n
E[M(y (cid:98),y)]≤exp(−(1−C(cid:101)′(ψ−1+ρ−2)) i̸=j∈[K 8]
σ2
i j )+exp(− 2).
The related proof details are provided in Appendix A.
8 Conclusion
We study the problem of clustering high-dimensional data generated from a mixture of
strongly correlated distributions. We propose FASC, which decorrelates the dependence
among variables and reduces the noise through factor adjustments. Theoretical analysis of
FASC is provided, showing FASC has a misclassification rate of e−Ω(SNR) with near linear
sample complexity n = Ω(cid:101)(d). We also show the applicability of FASC with real data
experiments and numerical studies. The experimental results suggest that FASC achieves
performance improvement compared with traditional methods, such as spectral clustering
and K-means.
19References
Abbe, E., Fan, J. & Wang, K. (2022), ‘An ℓ theory of pca and spectral clustering’, The
p
Annals of Statistics 50(4), 2359–2385.
Ahn, S. C. & Horenstein, A. R. (2013), ‘Eigenvalue ratio test for the number of factors’,
Econometrica 81(3), 1203–1227.
Bach,F.&Jordan,M.(2003),‘Learningspectralclustering’,Advancesinneuralinformation
processing systems 16.
Bai, J. & Ng, S. (2002), ‘Determining the number of factors in approximate factor models’,
Econometrica 70(1), 191–221.
Bakshi, A., Diakonikolas, I., Jia, H., Kane, D. M., Kothari, P. K. & Vempala, S. S. (2022),
Robustly learning mixtures of k arbitrary gaussians, in ‘Proceedings of the 54th Annual
ACM SIGACT Symposium on Theory of Computing’, STOC 2022, Association for Com-
puting Machinery, New York, NY, USA, p. 1234–1247.
Balakrishnan, S., Wainwright, M. J. & Yu, B. (2017), ‘Statistical guarantees for the em
algorithm: From population to sample-based analysis’.
Bickel,P.J.&Levina,E.(2004),‘Sometheoryforfisher’slineardiscriminantfunction,naive
bayes’, and some alternatives when there are many more variables than observations’,
Bernoulli 10(6), 989–1010.
Boucheron, S., Lugosi, G. & Bousquet, O. (2003), Concentration inequalities, in ‘Summer
school on machine learning’, Springer, pp. 208–240.
Brubaker,S.C.&Vempala,S.S.(2008),‘Isotropicpcaandaffine-invariantclustering’,2008
49th Annual IEEE Symposium on Foundations of Computer Science pp. 551–560.
Bryzgalova, S. (2015), ‘Spurious factors in linear asset pricing models’, LSE manuscript
1(3), 6.
Cai, F., Le-Khac, N.-A. & Kechadi, T. (2016), ‘Clustering approaches for financial data
analysis: a survey’, arXiv preprint arXiv:1609.08520 .
Chen, X. & Zhang, A. Y. (2021), ‘Optimal clustering in anisotropic gaussian mixture mod-
els’, arXiv preprint arXiv:2101.05402 .
Chen,Y.,Chi,Y.,Fan,J.&Ma,C.(2021),‘Spectralmethodsfordatascience: Astatistical
perspective’, Foundations and Trends® in Machine Learning 14(5), 566–806.
Daskalakis, C., Tzamos, C. & Zampetakis, M. (2017), Ten steps of em suffice for mixtures
of two gaussians, in S. Kale & O. Shamir, eds, ‘Proceedings of the 2017 Conference on
LearningTheory’,Vol.65ofProceedings of Machine Learning Research,PMLR,pp.704–
710.
Davis, D., D´ıaz, M. & Wang, K. (2021), ‘Clustering a mixture of gaussians with unknown
covariance’, arXiv preprint arXiv:2110.01602 .
Doss, N., Wu, Y., Yang, P. & Zhou, H. H. (2023), ‘Optimal estimation of high-dimensional
gaussian location mixtures’, The Annals of Statistics 51(1), 62–95.
20Fama,E.F.&French,K.R.(2015),‘Afive-factorassetpricingmodel’,Journal of financial
economics 116(1), 1–22.
Fan, J. & Fan, Y. (2008), ‘High dimensional classification using features annealed indepen-
dence rules’, Annals of statistics 36(6), 2605.
Fan, J. & Gu, Y. (2023), ‘Factor augmented sparse throughput deep relu neural networks
for high dimensional regression’, Journal of the American Statistical Association (just-
accepted), 1–28.
Fan, J., Guo, J. & Zheng, S. (2022), ‘Estimating number of factors by adjusted eigenvalues
thresholding’, Journal of the American Statistical Association 117(538), 852–861.
Fan, J., Han, F. & Liu, H. (2014), ‘Challenges of big data analysis’, National science review
1(2), 293–314.
Fan, J., Ke, Y., Sun, Q. & Zhou, W.-X. (2019), ‘Farmtest: Factor-adjusted robust multiple
testing with approximate false discovery control’, Journal of the American Statistical
Association 114(528), 1880–1893. PMID: 33033420.
Fan, J., Ke, Y. & Wang, K. (2020), ‘Factor-adjusted regularized model selection’, Journal
of Econometrics 216(1), 71–85.
Fan, J., Li, R., Zhang, C.-H. & Zou, H. (2020), Statistical foundations of data science,
Chapman and Hall/CRC.
Fan, J. & Liao, Y. (2019), ‘Learning latent factors from diversified projections and its
applications to over-estimated and weak factors’, Journal of the American Statistical
Association 117, 909 – 924.
Fan, J., Liao, Y. & Mincheva, M. (2013), ‘Large covariance estimation by thresholding
principalorthogonalcomplements’,Journal of the Royal Statistical Society Series B: Sta-
tistical Methodology 75(4), 603–680.
Fan, J., Liao, Y. & Wang, W. (2016), ‘Projected principal component analysis in factor
models’, The Annals of Statistics 44(1), 219 – 254.
Fan, J., Wang, K., Zhong, Y. & Zhu, Z. (2021), ‘Robust high dimensional factor models
with applications to statistical machine learning’, Statistical science: a review journal of
the Institute of Mathematical Statistics 36(2), 303.
Fan,J.&Wang, W.(2015), ‘Asymptoticsofempiricaleigen-structureforultra-highdimen-
sional spiked covariance model’, arXiv preprint arXiv:1502.04733 .
Fisher, R. A. (1936), ‘The use of multiple measurements in taxonomic problems’, Annals of
eugenics 7(2), 179–188.
Friedman, J. H. (1989), ‘Regularized discriminant analysis’, Journal of the American Sta-
tistical Association 84, 165–175.
Ge, R., Huang, Q. & Kakade, S. M. (2015), ‘Learning mixtures of gaussians in high dimen-
sions’, Proceedings of the forty-seventh annual ACM symposium on Theory of Computing
.
21Hallin, M. & Liˇska, R. (2007), ‘Determining the number of factors in the general dynamic
factor model’, Journal of the American Statistical Association 102(478), 603–617.
Hamerly,G.&Elkan,C.(2003),‘Learningthekink-means’,Advancesinneuralinformation
processing systems 16.
Hartigan,J.A.&Wong,M.A.(1979),‘Algorithmas136: Ak-meansclusteringalgorithm’,
Journal of the royal statistical society. series c (applied statistics) 28(1), 100–108.
Herwig, R., Poustka, A. J., Mu¨ller, C., Bull, C., Lehrach, H. & O’Brien, J. (1999), ‘Large-
scale clustering of cdna-fingerprinting data’, Genome research 9(11), 1093–1105.
Jana, S., Fan, J. & Kulkarni, S. (2024), ‘A general theory for robust clustering via trimmed
mean’, arXiv preprint arXiv:2401.05574 .
Jana, S., Yang, K. & Kulkarni, S. (2023), ‘Adversarially robust clustering with optimality
guarantees’, arXiv preprint arXiv:2306.09977 .
J.H.,S.&Watson,M.(2002),‘Forecastingusingprincipalcomponentsfromalargenumber
of predictors’, Journal of the American Statistical Association 97, 1167–1179.
Jin, C., Netrapalli, P., Ge, R., Kakade, S. M. & Jordan, M. I. (2019), ‘A short note on
concentration inequalities for random vectors with subgaussian norm’, arXiv preprint
arXiv:1902.03736 .
Jin, J., Ke, Z. T. & Wang, W. (2017), ‘Phase transitions for high dimensional clustering
and related problems’.
Johnstone, I. M. & Lu, A. Y. (2009), ‘On consistency and sparsity for principal com-
ponents analysis in high dimensions’, Journal of the American Statistical Association
104(486), 682–693.
Klusowski, J. M. & Brinda, W. (2016), ‘Statistical guarantees for estimating the centers of
a two-component gaussian mixture by em’, arXiv preprint arXiv:1608.02280 .
Kumar, A., Sabharwal, Y. & Sen, S. (2004), A simple linear time (1+ϵ)-approximation
algorithm for k-means clustering in any dimensions, in ‘45th Annual IEEE Symposium
on Foundations of Computer Science’, IEEE, pp. 454–462.
Kwon, J. & Caramanis, C. (2020), The em algorithm gives sample-optimality for learn-
ing mixtures of well-separated gaussians, in ‘Conference on Learning Theory’, PMLR,
pp. 2425–2487.
Lam, C. & Yao, Q. (2012), ‘Factor modeling for high-dimensional time series: inference for
the number of factors’, The Annals of Statistics pp. 694–726.
Lettau, M.&Pelger, M.(2020), ‘Estimatinglatentasset-pricingfactors’, Journal of Econo-
metrics 218(1), 1–31.
Li, X., Kao, B., Shan, C., Yin, D. & Ester, M. (2020), Cast: a correlation-based adap-
tive spectral clustering algorithm on multi-scale data, in ‘Proceedings of the 26th ACM
SIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining’,pp.439–449.
22Liu, A. & Moitra, A. (2021), Settling the robust learnability of mixtures of gaussians, in
‘Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing’,
pp. 518–531.
Liu, A. & Moitra, A. (2022), Learning gmms with nearly optimal robustness guarantees, in
‘Conference on Learning Theory’, PMLR, pp. 2815–2895.
L¨offler, M., Zhang, A. Y. & Zhou, H. H. (2021), ‘Optimality of spectral clustering in the
gaussian mixture model’, The Annals of Statistics 49(5), 2506–2530.
Longford, N. T. & Muth´en, B. O. (1992), ‘Factor analysis for clustered observations’, Psy-
chometrika 57, 581–597.
Lu, Y.&Zhou, H.H.(2016), ‘Statisticalandcomputationalguaranteesoflloyd’salgorithm
and its variants’, arXiv preprint arXiv:1612.02099 .
Luo, R., Wang, H. & Tsai, C.-L. (2009), ‘Contour projected dimension reduction’.
Ma, Y., Wu, Y. & Yang, P. (2023), On the best approximation by finite gaussian mixtures,
in‘2023IEEEInternationalSymposiumonInformationTheory(ISIT)’,IEEE,pp.2619–
2624.
Mamalis,B.,Gavalas,D.,Konstantopoulos,C.&Pantziou,G.(2009),Clusteringinwireless
sensor networks, in ‘RFID and sensor Networks’, CRC Press, pp. 343–374.
Ndaoud,M.(2018),‘Sharpoptimalrecoveryinthetwocomponentgaussianmixturemodel’,
The Annals of Statistics .
Ng,A.,Jordan,M.&Weiss,Y.(2001),‘Onspectralclustering: Analysisandanalgorithm’,
Advances in neural information processing systems 14.
Onatski, A. (2012), ‘Asymptotics of the principal components estimator of large factor
models with weakly influential factors’, Journal of Econometrics 168(2), 244–258.
Shen, D., Shen, H., Zhu, H. & Marron, J. (2016), ‘The statistics and mathematics of high
dimension low sample size asymptotics’, Statistica Sinica 26(4), 1747.
Subedi, S., Punzo, A., Ingrassia, S. & McNicholas, P. D. (2013), ‘Clustering and classifica-
tion via cluster-weighted factor analyzers’, Advances in Data Analysis and Classification
7(1), 5–40.
Sugar, C. A. & James, G. M. (2003), ‘Finding the number of clusters in a dataset:
An information-theoretic approach’, Journal of the American Statistical Association
98(463), 750–763.
Uematsu, Y. & Yamagata, T. (2022), ‘Estimation of sparsity-induced weak factor models’,
Journal of Business & Economic Statistics 41(1), 213–227.
Vempala,S.&Wang,G.(2004),‘Aspectralalgorithmforlearningmixturemodels’,Journal
of Computer and System Sciences 68(4), 841–860. Special Issue on FOCS 2002.
Vershynin, R. (2018), High-dimensional probability: An introduction with applications in
data science, Vol. 47, Cambridge university press.
23Viroli,C.(2010),‘Dimensionallyreducedmodel-basedclusteringthroughmixturesoffactor
mixture analyzers’, Journal of classification 27, 363–388.
Von Luxburg, U. (2007), ‘A tutorial on spectral clustering’, Statistics and computing
17, 395–416.
Wu, Y.&Zhou, H.H.(2021), ‘Randomlyinitializedemalgorithmfortwo-componentgaus-
√
sian mixture achieves near optimality in o( n) iterations’, Mathematical Statistics and
Learning 4(3).
Zhang, A. Y. & Zhou, H. H. (2022), ‘Leave-one-out singular subspace perturbation analysis
for spectral clustering’, arXiv preprint arXiv:2205.14855 .
Zhou,Y.,Xue,L.,Shi,Z.,Wu,L.&Fan,J.(2022),‘Measuringhousingvitalityfrommulti-
sourcebigdataandmachinelearnin’,JournalofAmericanStatisticalAssociation(invited
discussion) 117, 1045–1059.
24SUPPLEMENTARY MATERIAL to Factor Adjusted Spectral
Clustering for Mixture Models
A Proof details of Theorem 1
The proof of Theorem 1 will rely on the following major results.
Lemma 7. Let n ≥ dlog3n. Denote ∆ := V rV rT − V(cid:98)rV(cid:98) rT. Then there are constants
ξ >0,n ∈N+ such that for all n≥n
1 0 0
(cid:20) (cid:18) (cid:19)(cid:21)
ξ 1
P ∥∆∥≤ 1 σ∨ √ ≥1−n−10.
σ (B) logn
max
Lemma 8. There are constant ξ ,ξ >0 and ξ ∈(0,1) such that the following holds:
2 3 4
(cid:16) (cid:17)
(i) ∥(I−V rV rT)B∥≤ξ 2 σ∨ √ lo1 gn
(ii) λ ((I−V VT)E[µ µT](I−V VT))≥ξ
k r r yi yi r r 3
(iii) ∥(I−V VT)(µ −µ )∥≥ξ ∥µ −µ ∥ for any j ̸=j .
r r j1 j2 4 j1 j2 1 2
Now we get back to proving Theorem 1. Note that the final clustering is obtained
when we apply Algorithm 1 to {u }n . We will show that the estimates {u }n will
(cid:98)i i=1 (cid:98)i i=1
be distributed according to a mixture of approximately sub-Gaussian distribution and the
centroids of the mixture components will be well separated. To show the above, we note
the following decomposition
u
(cid:98)i
=µ
(cid:101)yi
+ε (cid:101)i, µ
(cid:101)j
:=(I−V(cid:98)rV(cid:98) rT)µ j, ε
(cid:101)i
:=(I−V(cid:98)rV(cid:98) rT)Bf i+(I−V(cid:98)rV(cid:98) rT)ε i. (10)
Next, we establish the following guarantees with a probability at least 1−n−10
(P1) min ∥µ −µ ∥≥ ξ4 min ∥µ −µ ∥
i̸=j∈[K] (cid:101)i (cid:101)j 8 i̸=j∈[K] i j
(cid:16) (cid:16) (cid:17)(cid:17)
(P2) ε ∈SG ξ σ2∨ 1 for a constant ξ >0.
(cid:101)i d 5 logn 5
We first prove the claim (P1). We have for j ̸=j ,
1 2
∥µ
(cid:101)j1
−µ (cid:101)j2∥=∥(I−V(cid:98)rV(cid:98) rT)(µ
j1
−µ j2)∥
≥∥(I−V VT)(µ −µ )∥−∥∆(µ −µ )∥
r r j1 j2 j1 j2
(a)
≥ ξ ∥µ −µ ∥−∥∆∥∥µ −µ ∥
4 j1 j2 j1 j2
(cid:18) (cid:19)
(b) ξ 1 (c) ξ
≥ ξ ∥µ −µ ∥− 1 σ∨ √ ∥µ −µ ∥ ≥ 4∥µ −µ ∥.
4 j1 j2 σ (B) logn j1 j2 2 j1 j2
max
(11)
where (a) follows from Lemma 8, (b) follows from Lemma 7, and (c) follows from Assump-
tion 3.
Next we will prove (P2). We note the following decomposition for ε
(cid:101)i
ε
(cid:101)i
=(I−V(cid:98)rV(cid:98) rT)Bf i+(I−V(cid:98)rV(cid:98) rT)ε
i
=(I−V rV rT)Bf i+∆Bf i+(I−V(cid:98)rV(cid:98) rT)ε i.
25Hence, conditioned on V(cid:98)r,B, and using the independence of f
i
and ε
i
we have that
ε ∈SG ((∥(I−V VT)B+∆B∥)2+σ2)⊆SG (2∥(I−V VT)B∥2+2∥∆B∥2+σ2).
(cid:101)i d r r d r r
(12)
For a large constant ξ > 0 we note that the bound ∥(I −V VT)B∥2 +∥∆B∥2 +σ2 ≤
5 r r
(cid:16) (cid:17)
ξ σ2∨ 1 holds with a probability at least 1−2n−10 as
5 logn
• f ∈SG (c ),ε ∈SG (σ2)
i r 1 i d
• Using ∥I−V(cid:98)rV(cid:98) rT∥≤1, Lemma 7, Lemma 8 we get with a probability 1−2n−10
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
∥(I−V VT)B∥2 ≤ξ2 σ2∨ , ∥∆B∥2 ≤∥∆∥2·σ2 (B)≤ξ2 σ2∨ .
r r 2 logn max 1 logn
In view of the above and (12) we get that
(cid:18) (cid:18) (cid:19)(cid:19)
1
ε ∈SG ξ σ2∨ with a probability at least 1−2n−10. (13)
(cid:101)i d 5 logn
Now we are ready to apply Lemma 6 with the vectors {u }n . From now on our argu-
(cid:98)i i=1
(cid:110) (cid:16) (cid:17)(cid:111)
mentwillbeconditionalonthevaluesofV(cid:98)rovertheevent V(cid:98)r :∥∆∥≤ σmaξ x1
(B)
σ∨ √ lo1
gn
for the constant ξ >0 as in Lemma 7, and note that the above event holds with a proba-
1
bility 1−n−10. We will show that given any such V(cid:98)r the final result will be the same, and
hence the result without any conditioning on V(cid:98)r will also hold.
Inviewof (13)itremainstocheckwhethertheconditionsonβ,ψ,ρrequiredbyLemma6
are satisfied. In our setup of (2) note that using a Chernoff type argument (Boucheron
(cid:12) (cid:12)
etal.2003, Section2.2)itcanbeshownthat(cid:12)min a∈[K]{a:y
i
=j}(cid:12)> (cid:101)cnwithaprobability
1−Kn−10 for a small constant c>0. The above implies
(cid:101)
K
c<β = min |{i:y =a}|≤1 for all large n. (14)
(cid:101) n a∈[K] i
Next we check whether the condition on ψ is satisfied. In view of (P1), (14), Assumption 2
and K <C(cid:101) for some constant C(cid:101) >0 we get that there is a large constant C(cid:101)1 >0 such that
for sufficiently large n and small enough σ
min ∥µ −µ ∥
i̸=j∈[K] (cid:101)i (cid:101)j
ψ :=
(cid:113)
>C(cid:101)1. (15)
β−0.5K(1+ d)σ
n
Next we check the condition on ρ in Lemma 6. For observations {u }n , ρ is defined as
(cid:98)i i=1
σ ([µ ,...,µ ]T)
ρ:= k √(cid:101)1 √(cid:101)n .
( n+ d)σ
Let M(cid:99) denote the matrix [µ (cid:101)1,...,µ (cid:101)n]T. To this end we note the following result
Lemma 9. Conditioned on V(cid:98)r, there are constants C(cid:101)2,C(cid:101)3,C(cid:101)4 > 0 such that for all large
enough n, with probability 1−C(cid:101)2/n,
n
1 (cid:88) logn
λ k(
n
µ (cid:101)yiµ (cid:101)T yi)≥λ k(E[µ (cid:101)yiµ (cid:101)T yi])−C(cid:101)3 √
n
≥C(cid:101)4.
i=1
26A proof of the above result is provided later. In view of the above we have
(cid:114) (cid:118) (cid:117) n (cid:115)
√1
nσ k(M(cid:99))=
n1
λ
k(M(cid:99)TM(cid:99))=(cid:117)
(cid:116)λ k(
n1 (cid:88)
µ (cid:101)yiµ (cid:101)T yi)≥ λ
k(E(cid:2)
µ (cid:101)yiµ (cid:101)T
yi(cid:3) )−C(cid:101)3lo √g nn
>C(cid:101)5.
i=1
for some constant C(cid:101)5 > 0 when σ is small enough, with probability 1−C(cid:101)2/n. In view of
the last display we get for a constant C(cid:101)6 >0, whenever σ is small
ρ=
σ k( ([ √µ
(cid:101)
n1, +.. √., dµ
(cid:101) )n
σ]T)
≥
C(cid:101)
σ5 >C(cid:101)6. (16)
In view of (15), (16), and the claims (P1),(P2) we apply Lemma 6 to get
 
min ∥µ −µ ∥2 n
E[M(y (cid:98),y)]≤exp−(1−C′(ψ−1+ρ−2)) i 8̸= (cid:16)j∈ σ[K 2]
∨
(cid:101) 1i (cid:17)(cid:101)j +exp(− 2).
logn
In view of (11), (15) and (16) we get that there exists a constant C(cid:101)7 >0 for which
E[M(y,y)]≤exp(cid:32) −C(cid:101)7·SNR2(cid:33)
+exp(−n
), SNR:=
min i̸=j∥µ i−µ j∥2
.
(cid:98) 1∨ 1 2 σ2
σlogn
Notice that all the above results holds are conditioned on a fixed ∆ that satisfies Lemma 7.
For each of those ∆, the constants remain the same. Therefore the above results hold
simlutaneously for any ∆ that satisfies Lemma 7. Therefore with probability greater than
1−n−10, for all sufficiently large n, the above results will hold without conditioning on ∆.
This concludes the proof of Theorem 1 as σ <1.
Remark8. Although(Zhang&Zhou2022,Theorem3.1andCorollary3.1)holdforspectral
clusteringwithexactK-means,theresultswillactuallyholdfor(1+ϵ)-approximateK-means
with only differences in constants. To prove this we use an argument similar to the proof of
(L¨offleretal.2021,Theorem2.2). Noticethattheproofof(Zhang&Zhou2022,Proposition
3.1) claims that
∥Θ(cid:98) −P(cid:98)∥
F
≤∥P −P(cid:98)∥
F
since Θ(cid:98) is the solution to the exact K-means. However if Θ(cid:98) is the solution to (1 + ϵ)-
approximate K-means, we can similarly get
∥Θ(cid:98) −P(cid:98)∥
F
≤(1+ϵ)∥P −P(cid:98)∥ F.
Using this inequality will not affect the analysis except for constant factors in the result
statement in Theorem 1.
B Technical results
Lemma 10. Consider the model defined in (2). Let Σ :=
E(cid:2)
x
xT(cid:3)
be the population
i i
covariancematrix,andΣ(cid:98) := n1 (cid:80)n i=1x ixT
i
betheempiricalcovariancematrixandletn≥d.
Then there are constants n ,C¯ ,C¯ >0 such that for all n≥n we have
0 1 2 0
(cid:32) (cid:114) (cid:114) (cid:114) (cid:33)
logn λ∗dlogn dlogn dlog2n
∥Σ(cid:98) −Σ∥≤C¯
1
λ∗
1 n
+ 1
n
+
n
+
n
27with probability greater than 1−C¯ n−9. Here λ∗ =λ (BBT)=σ (B)2.
2 1 max max
Proof. We will need the following result, which is a truncated version of matrix Bernstein
inequality:
Lemma 11. (Chen et al. 2021, Corollary 3.2) Let Z,Z ,··· ,Z be i.i.d. random matrices
1 n
with dimension m ×m . Suppose the followings hold:
1 2
P(∥Z−E[Z]∥≥L)≤q and ∥E[Z]−E[Z1{∥Z∥<L}]∥≤q . (17)
0 1
Define
V =max{∥E(cid:2) (Z−EZ)(Z−EZ)T(cid:3) ∥1/2,∥E(cid:2) (Z−EZ)T(Z−EZ)(cid:3) ∥1/2}.
Set m:=max{m ,m }. Then for any a≥2, with probability at least 1−2m−a+1−nq ,
1 2 0
n (cid:114)
1 (cid:88) 2alogm 2alogm
∥ (Z −EZ )∥≤V +L +q ,
n i i n 3n 1
i=1
where m=m +m .
1 2
Back to the proof of Lemma 10, we have the following decomposition:
n
∥Σ(cid:98) −Σ∥=∥ n1 (cid:88)(cid:0) x ixT
i
−E(cid:2) x ixT i(cid:3)(cid:1) ∥
i=1
n
=∥1 (cid:88)
(Bf +u )(Bf +u
)T−E(cid:2)
(Bf +u )(Bf +u
)T(cid:3)
∥
n i i i i i i i i
i=1
n n n
≤∥B∥2∥1 (cid:88)
f
fT−E(cid:2)
f
fT(cid:3) ∥+2∥B∥∥1 (cid:88)
f
uT∥+∥1 (cid:88)
u
uT−E(cid:2)
u
uT(cid:3)
∥.
n i i i i n i i n i i i i
i=1 i=1 i=1
(18)
WewillthenboundeachofthethreetermsbymatrixBernsteininequality. For∥1 (cid:80)n f fT−
E(cid:2) f fT(cid:3) ∥, we apply Lemma 11 by setting Z = f fT. We first choose propen r L,i= q1 ai ndi
i i i i i 0
q for (17) to hold. Recall from Assumption 1 (a) that f ∈SG (c2) and E[f ]=0 implies
1 i r 1 i
∥f ∥∈SG(rc2) for some constant c>0 (Jin et al. 2019, Lemma 1). Therefore
i (cid:101) (cid:101)
P(∥Z−E[Z]∥≥L)=P(∥f fT−E[f fT]∥≥L)
i i i i
≤P(∥f fT∥+∥E[f fT]∥≥L)
i i i i
√
=P(∥f fT∥≥L−1)≤P(∥f ∥2 ≥L−1)=P(∥f ∥≥ L−1).
i i i i
Let L=22c2rlogn+2. Then by the sub-Gaussian tail bound
(cid:101)
Y ∈SG(σ2), P[|Y −E[Y]|>y]≤2e− 2y σ2 2, (19)
28√
we have P(∥f ∥≥ L−1)≤2n−11 :=q . For determining q as in Lemma 11 we note
i 0 1
∥E[Z]−E[Z1{∥Z∥<L}]∥=∥E[Z1{∥Z∥≥L}]∥
≤E[∥Z∥1{∥Z∥≥L}]
=E[(∥Z∥−L)1{∥Z∥−L≥0}]+LP(∥Z∥≥L)
(cid:90) ∞
= P(∥Z∥−L≥x)dx+LP(∥Z∥≥L)
0
(cid:90) ∞
= P(∥f fT∥≥y)dy+LP(∥f fT∥≥L)
i i i i
L
(cid:90) ∞
≤ P(∥f ∥2 ≥y)dy+LP(∥f ∥2 ≥L)
i i
L
(cid:90) ∞
≤ 2e− 2ry (cid:101)c2dy+2Le− 2rL (cid:101)c2
L
=(4rc2+2L)e− 2rL (cid:101)c2 ≤3Le− 2rL (cid:101)c2 ≤n−10 :=q 1
for a sufficiently large n. Using the definition of V we have
V =max{∥E(cid:2) (Z−EZ)(Z−EZ)T(cid:3) ∥1/2,∥E(cid:2) (Z−EZ)T(Z−EZ)(cid:3) ∥1/2}
=∥E(cid:2) (f fT−E(cid:2) f fT(cid:3) )(f fT−E(cid:2) f fT(cid:3) )(cid:3) ∥1/2
i i i i i i i i
=∥E(cid:2) f fTf fT(cid:3) −(E(cid:2) f fT(cid:3) )2∥1/2 ≤∥E(cid:2) f fTf fT(cid:3) ∥1/2.
i i i i i i i i i i
The last inequality is due to the fact that E(cid:2) f fTf fT(cid:3) −(E(cid:2) f fT(cid:3) )2 and (E(cid:2) f fT(cid:3) )2 are
i i i i i i i i
both positive semidefinite. Next, we notice that for any v ∈Rr such that ∥v∥=1,
vT(E(cid:2) f fTf fT(cid:3) )v =E[(fTv)2fTf ]
i i i i i i i
( ≤a)(cid:113) E(cid:2) (f iTv)4(cid:3)E(cid:2) (f iTf i)2(cid:3)( ≤b) C(cid:101)1(cid:112) E[∥f i∥4]( ≤c) C(cid:101)2r,
forconstantsC(cid:101)1,C(cid:101)2 >0,where(a)followsusingtheCauchy-Schwarzinequality,(b)follows
(cid:113)
as f iTv ∈SG(c2 1), and (c) follows using ∥f i∥∈SG( (cid:101)c2r). This implies V ≤ C(cid:101)2r. Therefore
by Lemma 11, using a= 11 lol gog rn we get for a constant C(cid:101)3
∥
n1 (cid:88)n
f if
iT−E(cid:2)
f if
iT(cid:3) ∥≤C((cid:114) lo ngn
+
log n2n )≤C(cid:101)3(cid:114) lo ngn
(20)
i=1
with probability at least 1−3n−10 for all sufficiently large n, as r is at most a constant.
For ∥1 (cid:80)n f uT∥, we apply Lemma 11 by setting Z = f uT. We first choose proper
n i=1 i i i i i
L, q
0
and q
1
for (17) to hold. We claim that u
i
∈ SG d(C(cid:101) 42) for a constant C(cid:101)4 > 0. This
is because, u = µ +ε , so for any unit vector v ∈ Rd, vTu = vTµ +vTε . Since
i yi i i yi i
ε ∈SG (σ2), we have vTε ∈SG(σ2). On the other hand, notice that by Assumption 2 (c)
i d i
c ≥ max ∥µ −µ ∥, and by Assumption 1 (b), there is a constant c ∈ (0,1) such that
5 i̸=j i j
min p > c and (cid:80)K p µ =0. Therefore ∀j ∈[K],
j∈[K] j K j=1 j j
K K K K
(cid:88) (cid:88) (cid:88) (cid:88)
∥µ ∥=∥µ − p µ ∥=∥ p (µ −µ )∥≤ p ∥µ −µ ∥≤ p c =c . (21)
j j i i i j i i j i i 5 5
i=1 i=1 i=1 i=1
29Thus |vTµ yi| ≤ ∥µ yi∥ ≤ c 5. Then vTµ
yi
∈ SG(C(cid:101) 42) for some constant c. Since vTµ
yi
and
vTε
i
are independent, we have vTu
i
=vTµ
yi
+vTε
i
∈SG(C(cid:101) 42+σ2), which implies
u
i
∈SG d(C(cid:101) 52), ∥u i∥∈SG(dC(cid:101) 62) (22)
for a constant C(cid:101)6 >0 (Jin et al. 2019, Lemma 1). Recall ∥f i∥∈SG(rc2). Therefore √∥f iuT i∥
is sub-Exponential (Vershynin 2018, Lemma 2.7.7) with sub-Exponential norm C(cid:101)7 dr for
a constant C(cid:101)7 > 0 (Vershynin 2018, Definitio √n 2.7.5). In view of the properties of sub-
Exponential distributions with parameter C(cid:101)7 dr (Vershynin 2018, Proposition 2.7.1) we
get
P(∥Z−E[Z]∥≥L)=P(∥f uT∥≥L)≤2n−11 :=q
i i 0
For obtaining q we use the following computation,
1
∥E[Z]−E[Z1{∥Z∥<L}]∥=∥E[Z1{∥Z∥≥L}]∥
≤E[∥Z∥1{∥Z∥≥L}]
=E[(∥Z∥−L)1{∥Z∥−L≥0}]+LP(∥Z∥≥L)
(cid:90) ∞
= P(∥Z∥−L≥x)dx+LP(∥Z∥≥L)
0
(cid:90) ∞
= P(∥f uT∥≥y)dy+LP(∥f uT∥≥L)
i i i i
L
≤(cid:90) ∞ 2e− 2C(cid:101)7y √ drdy+2Le− 2C(cid:101)7L√
dr
L
√
=(4c2 dr+2L)e− 2C(cid:101)7L√ dr ≤3Le− 2C(cid:101)7L√ dr ≤n−10 :=q 1.
where the last inequality holds for sufficiently large n. For the variance statistic V, by
definition we have
V =max{∥E(cid:2) (Z−EZ)(Z−EZ)T(cid:3) ∥1/2,∥E(cid:2) (Z−EZ)T(Z−EZ)(cid:3) ∥1/2}
=max{∥E(cid:2) f uTu fT(cid:3) ∥1/2,∥E(cid:2) u fTf uT(cid:3) ∥1/2}
i i i i i i i i
( =a) max{∥E(cid:2) uTu (cid:3)E(cid:2) f fT(cid:3) ∥1/2,∥E(cid:2) u uT(cid:3)E(cid:2) fTf (cid:3) ∥1/2}
i i i i i i i i
=max{∥E(cid:2) ∥u ∥2(cid:3) I ∥1/2,∥E(cid:2) bu uT(cid:3) r∥1/2}
i r i i
(b) √ √ √
≤ C(cid:101)8max{ d, r}=C(cid:101)8 d,
for some constant C(cid:101)8 > 0, where (a) follows as u
i
and f
i
are independent and (b) follows
as ∥u i∥∈SG(dC(cid:101) 62) from (22) and
∥E(cid:2) u iuT i(cid:3) ∥≤C(cid:101)9 (23)
for a constant C(cid:101)9 >0. Hence, using Lemma 11 with a= 11 lol gog dn we get that for a constant
C(cid:101)9 >0
n (cid:114)
1 (cid:88) dlogn
∥
n
f iuT i∥≤C(cid:101)9
n
(24)
i=1
with probability at least 1−3n−10 for sufficiently large n.
30To bound ∥1 (cid:80)n u uT−E(cid:2) u uT(cid:3) ∥, we apply Lemma 11 by setting Z = u uT. We
n i=1 i i i i i i i
firstchooseproperL,q 0andq 1for(17)tohold. Recall∥u i∥∈SG(dC(cid:101) 62)from(22). Therefore
P(∥Z−E[Z]∥≥L)=P(∥u uT−E[u uT]∥≥L)
i i i i
≤P(∥u uT∥+∥E[u uT]∥≥L)
i i i i
(a) (cid:113)
≤ P(∥u iuT i∥≥L−C(cid:101)9)≤P(∥u i∥2 ≥L−C(cid:101)9)=P(∥u i∥≥ L−C(cid:101)9),
where (a) follows as ∥E[u iuT i]∥ ≤ C(cid:101)9 from (23). Choose L = 24C(cid:101) 62dlogn+C(cid:101)9, by sub-
(cid:113)
Gaussian tail bound, we have P(∥u i∥≥ L−C(cid:101)9)≤2n−12 :=q 0. For q 1,
∥E[Z]−E[Z1{∥Z∥<L}]∥=∥E[Z1{∥Z∥≥L}]∥
≤E[∥Z∥1{∥Z∥≥L}]
=E[(∥Z∥−L)1{∥Z∥−L≥0}]+LP(∥Z∥≥L)
(cid:90) ∞
= P(∥Z∥−L≥x)dx+LP(∥Z∥≥L)
0
(cid:90) ∞
= P(∥u uT∥≥y)dy+LP(∥u uT∥≥L)
i i i i
L
(cid:90) ∞
≤ P(∥u ∥2 ≥y)dy+LP(∥u ∥2 ≥L)
i i
L
(cid:90) ∞ − y − L
≤ 2e 2dC(cid:101)62dy+2Le 2dC(cid:101)62
L
− L − L
=(4dc2+2L)e 2rC(cid:101)62 ≤3Le 2dC(cid:101)62 ≤n−10 :=q
1
for sufficiently large n. Here we use the fact that n ≥ d. For the variance statistic V, by
definition we have
V =max{∥E(cid:2) (Z−EZ)(Z−EZ)T(cid:3) ∥1/2,∥E(cid:2) (Z−EZ)T(Z−EZ)(cid:3) ∥1/2}
=∥E(cid:2) u uTu uT(cid:3) −(E(cid:2) u uT(cid:3) )2∥1/2
i i i i i i
≤∥E(cid:2) u uTu uT(cid:3) ∥1/2.
i i i i
The last inequality is due to the fact that E(cid:2) u uTu uT(cid:3) −(E(cid:2) u uT(cid:3) )2 and (E(cid:2) u uT(cid:3) )2 are
i i i i i i i i
both positive semidefinite. To bound ∥E(cid:2) u uTu uT(cid:3) ∥, note that for any v ∈Rd,∥v∥=1,
i i i i
vT(E(cid:2) u iuT iu iuT i(cid:3) )v =E[(uT iv)2uT iu i]( ≤a)(cid:113) E(cid:2) (uT iv)4(cid:3)E(cid:2) (uT iu i)2(cid:3)( ≤b) C(cid:101)10(cid:112) E[∥u i∥4]( ≤c) C(cid:101)11d,
for constants C(cid:101)10,C(cid:101)11 >0, where (a) follows using the Cauchy-Schwarz inequality, (b) and
(cid:113)
(c) follow using (22). This implies V ≤ C(cid:101)12d. Therefore by Lemma 11, take a= 11 lol gog dn,
∥ n1
(cid:88)n
u iuT
i
−E(cid:2) u iuT i(cid:3)
∥≤C(cid:101)13(cid:32)(cid:114)
dlo ngn + dlo
ng2n(cid:33)
(25)
i=1
for a constant C(cid:101)13 >0 with probability at least 1−3n−10 for sufficiently large n.
31Finally, combine (18), (20), (24), (25), we have
(cid:32) (cid:114) (cid:114) (cid:114) (cid:33)
logn λ∗dlogn dlogn dlog2n
∥Σ(cid:98) −Σ∥≤C¯
1
λ∗
1 n
+ 1
n
+
n
+
n
with probability 1−C¯ n−10 for some constants C¯ ,C¯ >0, as required.
2 1 2
B.1 Proof for Lemma 2
Suppose that B is already given and fixed. Thus, U is also fixed. Consider the Grassman-
nian manifold G that consists of all k-dimensional subspaces of Rd. Let M ∼Unif(G )
d,k d,k
(see Vershynin (2018) for detailed discussion of the measure on Grassmannian manifold),
which means M represents a uniformly randomly chosen k-dimensional subspace of Rd.
Let the columns of U be (w ,··· ,w ). Then using (Vershynin 2018, Lemma 5.3.2) we can
1 r
conclude
(cid:32)(cid:12) (cid:114) (cid:12) (cid:33)
(cid:12) k(cid:12)
P (cid:12)∥wTM∥− (cid:12)≥t ≤2e−ξdt2 , 1≤i≤r,
(cid:12) i d(cid:12)
(cid:12) (cid:12)
(cid:113)
for some constant ξ > 0. Choosing t = 10logn, the above implies that for each i ∈ [r],
ξd
with probability no less than 1−2exp(−ξdt2)=1−2n−10,
(cid:114) (cid:115) (cid:114)
k 10logn k
∥wTM∥≤t+ = + .
i d ξd d
Using the union bound, we have with a probability at least 1−2rn−10 ≥ 1−n−9 (for
(cid:113) (cid:113)
sufficiently large n), for every i∈[r], we have ∥wTM∥≤ 10logn + k. Using
i ξd d
(cid:118) (cid:117) (cid:117)(cid:88)r √ (cid:32)(cid:115) 10logn (cid:114) k(cid:33) (cid:115) 11rlogn
∥UTM∥≤(cid:116) ∥wTM∥2 ≤ r + ≤
i ξd d ξd
i=1
forsufficientlylargen(recallr,karesmallerthansomeconstant)wegettherequiredresults.
B.2 Proof of Lemma 7
Proof. We will use the following lemma (Weyl’s inequality) for controlling the difference of
eigenvalues between two matrices.
Lemma 12 (Weyl’s inequality for eigenvalues). (Chen et al. 2021, Lemma 2.2) Let A,E ∈
Rn×n be two real symmetric matrices. For every 1 ≤ i ≤ n, the i-th largest eigenvalues of
A and A+E obey
|λ (A)−λ (A+E)|≤∥E∥.
i i
We will also use the Davis-Kahan’s Theorem (Chen et al. 2021, Corollary 2.8) for con-
trolling the distance between two the subspaces spanned by eigenvectors.
Lemma 13 (Davis-Kahan’s sinθ theorem). Let M∗ and M =M∗+E be two n×n real
positive semi-definite symmetric matrices. Let λ ≥···≥λ ≥0 be the eigenvalues of M,
1 n
λ∗ ≥ ··· ≥ λ∗ ≥ 0 be the eigenvalues of M∗, and u (resp. u∗) stands for the eigenvector
1 n i i
associated with the eigenvalue λ (resp. λ∗). We denote U := [u ,··· ,u ] ∈ Rn×r, U∗ :=
i √ i 1 r
[u∗,··· ,u∗]∈Rn×r. If ∥E∥≤(1−1/ 2)(λ∗−λ∗ ), then ∥UUT−U∗U∗T∥≤ 2∥E∥ .
1 r r r+1 λ∗−λ∗
r r+1
32By Lemma 13, since V(cid:98)r consists of the top r eigenvectors of Σ(cid:98) and V
r
consists of the
top r eigenvectors of Σ, we have
2∥Σ(cid:98) −Σ∥
∥∆∥≤ ,
λ (Σ)−λ (Σ)
r r+1
where λ (Σ) and λ (Σ) are the r-th and (r+1)-th largest eigenvalues of Σ respectively.
r r+1
Notice that Σ=BBT+Σ , therefore using Lemma 12 we have
u
λ (Σ)−λ (Σ)≥λ (BBT)−λ (BBT)−2∥Σ ∥=σ (B)2−2∥Σ ∥.
r r+1 r r+1 u min u
Therefore from Lemma 10 and the Assumption 3 gives us the result.
B.3 Proof of Lemma 8
B.3.1 Non-isotropic noise
Proof. We first consider the case where the noise is not isotropic. Using Assumption 1 and
the decomposition E[µ yiµT yi]=MΛ(cid:101)MT as in (6) we get
Σ:=E[x xT]=E[(Bf +µ +ε )(Bf +µ +ε )T]
i i i yi i i yi i
=BBT+E[µ µT]+E[ε εT]
yi yi i i
=UΛUT+MΛ(cid:101)MT+E[ε iεT i].
Let Σ(cid:101) denote the matrix UΛUT+(I−UUT)MΛ(cid:101)MT(I−UUT)T. Notice that
UT(I−UUT)M =(UT−UTUUT)M =(UT−UT)M =0.
This implies that the column space of (I −UUT)MΛ(cid:101)MT(I −UUT)T (which belongs to
the column space of (I −UUT)M) is orthogonal to the column space of U. Thus we can
write the spectral decomposition of (I−UUT)MΛ(cid:101)MT(I−UUT)T as
(I−UUT)MΛ(cid:101)MT(I−UUT)T =U ⊥Λ¯U ⊥T (26)
for a diagonal matrix Λ¯ and an orthogonal matrix U that satisfies U UT =0. In view of
⊥ ⊥
the last display we have
∥E[µ yiµT yi]∥=∥Λ(cid:101)∥≥∥(I−UUT)MΛ(cid:101)MT(I−UUT)T∥=∥Λ¯∥. (27)
In view of (26) we note that
 
Λ
Σ(cid:101) =UΛUT+(I−UUT)MΛ(cid:101)MT(I−UUT)T =(U,U ⊥,U 0) Λ¯ (U,U ⊥,U 0)T
0
where U is the orthogonal complement of (U,U ). In view of Assumption 3 and (27), as
0 ⊥
Λ is an r×r matrix we get
λ (Λ)=σ2 (B)≥3∥Λ¯∥ (28)
r min
33which implies that the set of top r eigenvectors of Σ(cid:101) is given by U. Since V
r
is the top r
eigenvectors of Σ, by Lemma 13,
2∥Σ−Σ(cid:101)∥
dist(U,V ):=∥UUT−V VT∥≤ . (29)
r r r
λ r(Σ(cid:101))−λ r+1(Σ(cid:101))
We bound the numerator and denominator of the right most term in the above display in
the following way. To bound the denominator, we use (28) to get via Lemma 13
1
λ r(Σ(cid:101))−λ r+1(Σ(cid:101))≥σ min(B)2−∥Λ¯∥≥ 2σ min(B)2 (30)
To bound the numerator ∥Σ−Σ(cid:101)∥ in (29) we note the decomposition
Σ−Σ(cid:101) =(UΛUT+MVMT+E[ε iεT i])−(UΛUT+(I−UUT)MVMT(I−UUT)T)
=UUTMVMT+MVMTUUT−UUTMVMTUUT+E[ε εT]. (31)
i i
Recallε ∼SG (σ2), whichmeansforanyv ∈Rd suchthat∥v∥=1, vTε ∼SG(σ2). Thus,
i d i
vTE[ε iεT i]v =E(cid:2) (vTε i)2(cid:3) ≤C(cid:101)1σ2,
forany∥v∥=1,whereC(cid:101)1 >0isaconstant. Therefore∥E[ε iεT i]∥≤C(cid:101)1σ2. Wethencontinue
(31) to have
∥Σ−Σ(cid:101)∥≤∥UUTMVMT∥+∥MVMTUUT∥+∥UUTMVMTUUT∥+∥E[ε iεT i]∥
≤2∥U∥∥UTM∥∥V∥∥MT∥+∥U∥∥UTM∥∥V∥MTU∥∥UT∥+cσ2
≤C(cid:101)2(∥UTM∥+σ2)
for a constant C(cid:101)2 >0. Using (29),
(cid:18) ∥UTM∥+σ2(cid:19) (cid:18)
1
(cid:18) (cid:114)
1
(cid:19)(cid:19)
dist(U,V r)≤C
σ (B)2
≤C(cid:101)3
σ (B)
σ∨
logn
, (32)
min max
for some constant C(cid:101)3 >0, where the last inequality follows from Assumption 3(c). We are
now in a position to proceed with the proof of our desired results.
• To show the first claim in Lemma 8, we note
(a)
∥(I−V VT)B∥ ≤ ∥(I−UUT)B∥+∥(UUT−V VT)B∥
r r r r
( =b) 0+∥(UUT−V VT)B∥
r r
(c)
≤ dist(U,V )∥B∥
r
(cid:18) (cid:114) (cid:19) (cid:18) (cid:114) (cid:19)
( ≤d)
σ
C(cid:101)4
(B)
σ∨
log1
n
×σ max(B)≤C(cid:101)4 σ∨
log1
n
,
max
where(a)followsfromtriangleinequality,(b)followsfromthespectraldecomposition
BBT = UΛUT, (c) follows from ∥(UUT−V VT)B∥ ≤ ∥(UUT−V VT)∥∥B∥ and
r r r r
∥UUT−V VT∥=dist(U,V ) as defined in (29) and (d) follows from (32).
r r r
34• For the third claim, we note using triangle inequality and (29) that for any j ̸=j
1 2
∥(I−V VT)(µ −µ )∥
r r j1 j2
≥∥(I−UUT)(µ −µ )∥−∥(UUT−V VT)(µ −µ )∥
j1 j2 r r j1 j2
≥∥µ −µ ∥−∥UUT(µ −µ )∥−dist(U,V )∥µ −µ ∥.
j1 j2 j1 j2 r j1 j2
Notice that µ −µ ∈Col(M). As M is an orthogonal matrix, there exists α such
j1 j2
that µ −µ =Mα, and ∥µ −µ ∥=∥α∥. Then we have
j1 j2 j1 j2
1 1
∥UUT(µ −µ )∥=∥UUTMα∥≤∥U∥∥UTM∥∥α∥≤ ∥α∥= ∥µ −µ ∥,
j1 j2 2 2 j1 j2
wherethelastinequalityfollowedusing∥UTM∥≤γ fromAssumption3andwecan
5
pick γ > 0 to be very small. Therefore, using (32), as σ < c for a sufficiently small
5
constant c>0 (Assumption 1), we get
∥(I−V VT)(µ −µ )∥
r r j1 j2
≥∥µ −µ ∥−∥UUT(µ −µ )∥−dist(U,V )∥µ −µ ∥
j1 j2 j1 j2 r j1 j2
1 1 1
≥∥µ −µ ∥− ∥µ −µ ∥− ∥µ −µ ∥≥ ∥µ −µ ∥.
j1 j2 2 j1 j2 4 j1 j2 4 j1 j2
• For the second claim, notice that by Lemma 12 we have,
λ ((I−V VT)E[µ µT](I−V VT))
k r r yi yi r r
≥λ ((I−UUT)E[µ µT](I−UUT))
k yi yi
−∥(I−UUT)E[µ µT](I−UUT)−(I−V VT)E[µ µT](I−V VT)∥
yi yi r r yi yi r r
≥λ (E[µ µT])−∥E[µ µT]−(I−UUT)E[µ µT](I−UUT)∥
k yi yi yi yi yi yi
−∥(I−UUT)E[µ µT](I−UUT)−(I−V VT)E[µ µT](I−V VT)∥. (33)
yi yi r r yi yi r r
We also have for a constant C(cid:101)5
∥E[µ µT]−(I−UUT)E[µ µT](I−UUT)∥
yi yi yi yi
=∥UUTMΛ(cid:101)MT+MΛ(cid:101)MTUUT−UUTMΛ(cid:101)MTUUT∥
(a) (b)
≤2∥UTM∥∥Λ(cid:101)∥+∥UTM∥2∥Λ(cid:101)∥ ≤ C(cid:101)5(2∥UTM∥+∥UTM∥2) ≤ γ, (34)
where(a)followedfromAssumption2and(b)holdsaverysmallconstantγ >0using
Assumption 3. We also have for a sufficiently small constant γ >0
(cid:101)
∥(I−UUT)E[µ µT](I−UUT)−(I−V VT)E[µ µT](I−V VT)∥
yi yi r r yi yi r r
=∥(UUTMΛ(cid:101)MTUUT−UUTMΛ(cid:101)MT−MΛ(cid:101)MTUUT)
−(V rV rTMΛ(cid:101)MTV rV rT−V rV rTMΛ(cid:101)MT−MΛ(cid:101)MTV rV rT)∥
≤∥UUTMΛ(cid:101)MTUUT−V rV rTMΛ(cid:101)MTV rV rT∥
+2∥UUTMΛ(cid:101)MT−V rV rTMΛ(cid:101)MT∥
≤2∥Λ(cid:101)∥∥UUT−V rV rT∥+2∥Λ(cid:101)∥∥UUT−V rV rT∥≤γ
(cid:101)
(35)
35wherethelastinequalityfollowedusing (31)and(32)withσ <γ fromAssumption1
5
and ∥Λ(cid:101)∥=λ 1(E[µ yiµT yi])≤C
3
from Assumption 2. Combining (33),(34),(35) we get
λ ((I−V VT)E[µ µT](I−V VT))≥λ (E[µ µT])−γ−γ ≥ξ
k r r yi yi r r k yi yi (cid:101) 4
for a constant ξ >0. Therefore we proved Lemma 8.
4
B.3.2 Isotropic noise
Proof. For the case that the noise is isotropic., i.e., E(cid:2) ε εT(cid:3) =σ2I , we will show that (32)
i i d
stillholdsundertheweakerversionofAssumption3(d). Thereforetheproofstillworks. For
showing(32)stillholds,weletΣ¯ denotethematrixΣ(cid:101)+σ2I d. Thesameasthenon-isotropic
case, we will use the decomposition
Σ=UΛUT+MΛ(cid:101)MT+E[ε iεT i]
=UΛUT+MΛ(cid:101)MT+σ2I d,
and
 
Λ
Σ¯ =(U,U ⊥,U 0) Λ¯ (U,U ⊥,U 0)T+σ2I d.
0
Recall in the proof for non-isotropic case, we show that the set of top r eigenvectors of Σ(cid:101) is
given by U. Therefore Σ¯ = Σ(cid:101) +σ2I
d
have the same set of top r eigenvectors as Σ(cid:101), which
is U (here we use σ2I is isotropic). Since V is the top r eigenvectors of Σ, by Lemma 13,
d r
2∥Σ−Σ¯∥
dist(U,V ):=∥UUT−V VT∥≤ . (36)
r r r λ (Σ¯)−λ (Σ¯)
r r+1
We bound the numerator and denominator of the right most term in the above display in
the following way. To bound the denominator, we use (28) to get
1
λ r(Σ¯)−λ r+1(Σ¯)=λ r(Σ(cid:101))−λ r+1(Σ(cid:101))≥σ min(B)2−∥Λ¯∥≥ 2σ min(B)2 (37)
To bound the numerator ∥Σ−Σ¯∥ in (36) we note the decomposition
Σ−Σ¯ =(UΛUT+MVMT+σ2I )−(UΛUT+(I−UUT)MVMT(I−UUT)T+σ2I )
d d
=UUTMVMT+MVMTUUT−UUTMVMTUUT
≤∥UUTMVMT∥+∥MVMTUUT∥+∥UUTMVMTUUT∥
≤2∥U∥∥UTM∥∥V∥∥MT∥+∥U∥∥UTM∥∥V∥MTU∥∥UT∥
≤C(cid:101)1(∥UTM∥) (38)
for a constant C(cid:101)1 >0. Using (36),
(cid:18) ∥UTM∥(cid:19) (cid:18)
1
(cid:18) (cid:114)
1
(cid:19)(cid:19)
dist(U,V r)≤C(cid:101)2
σ (B)2
≤C(cid:101)3
σ (B)
σ∨
logn
, (39)
min max
for constants C(cid:101)2,C(cid:101)3 > 0, where the last inequality follows from the weaker version of
Assumption 3(c). Therefore we show that (32) still holds for isotropic noise with weaker
assumption. The rest of the proofs remain the same as the non isotropic case.
36B.4 Proof of Lemma 9
Back to the proof of Lemma 9. For the first inequality in the lemma statement,
n n
1 (cid:88) (a) 1 (cid:88)
|λ (E[µ µT])−λ ( µ µT)| ≤ ∥E[µ µT]− µ µT∥
k (cid:101)yi(cid:101)yi k n (cid:101)yi(cid:101)yi (cid:101)yi(cid:101)yi n (cid:101)yi(cid:101)yi
i=1 i=1
K K K
=∥(cid:88) p jµ (cid:101)jµ (cid:101)T
j
−(cid:88)n njµ (cid:101)jµ (cid:101)T j∥≤(cid:88) |n nj −p j|∥µ (cid:101)jµ (cid:101)T j∥( ≤b) C(cid:101)1lo √g nn
j=1 j=1 j=1
withprobabilitygreaterthan1−C(cid:101)1/n,foraconstantC(cid:101)1 >0,wheren
j
=|{i∈[n]:y
i
=j}|.
(a) follows from Weyl’s inequality (Lemma 12), (b) holds since n is a binomial random
j
variable (so that it has sub-Gaussian tail bound), and ∥µ (cid:101)jµ (cid:101)T j∥ ≤ ∥µ (cid:101)j∥2 ≤ ∥µ j∥2 ≤ C(cid:101) 22 for
a constant C(cid:101)2 >0 by (21). To show the second inequality in the lemma statement we note
that (as we have already conditioned for V(cid:98)r)
λ (E[µ µT])
k (cid:101)yi(cid:101)yi
=λ k((I−V(cid:98)rV(cid:98) rT)E[µ yiµT yi](I−V(cid:98)rV(cid:98) rT))
(a)
≥ λ ((I−V VT)E[µ µT](I−V VT))
k r r yi yi r r
−∥(I−V rV rT)E[µ yiµT yi](I−V rV rT)−(I−V(cid:98)rV(cid:98) rT)E[µ yiµT yi](I−V(cid:98)rV(cid:98) rT)∥
(b)
≥ λ ((I−V VT)E[µ µT](I−V VT))−ξ∥∆∥
k r r yi yi r r
(cid:18) (cid:19)
(c) ξ 1
≥ ξ − 1 σ∨ √ ≥ξ ,
3 σ (B) logn 6
max
for constants ξ,ξ >0, where (a) follows using Lemma 12, (b) follows using (6) as
6
∥(I−V(cid:98)rV(cid:98) rT)E[µ yiµT yi](I−V(cid:98)rV(cid:98) rT)−(I−V rV rT)E[µ yiµT yi](I−V rV rT)∥
=∥(V(cid:98)rV(cid:98) rTMΛ(cid:101)MTV(cid:98)rV(cid:98) rT−V(cid:98)rV(cid:98) rTMΛ(cid:101)MT−MΛ(cid:101)MTV(cid:98)rV(cid:98) rT)
−(V rV rTMΛ(cid:101)MTV rV rT−V rV rTMΛ(cid:101)MT−MΛ(cid:101)MTV rV rT)∥
≤∥V(cid:98)rV(cid:98) rTMΛ(cid:101)MTV(cid:98)rV(cid:98) rT−V rV rTMΛ(cid:101)MTV rV rT∥
+2∥V(cid:98)rV(cid:98) rTMΛ(cid:101)MT−V rV rTMΛ(cid:101)MT∥
≤2∥Λ(cid:101)∥∥V(cid:98)rV(cid:98) rT−V rV rT∥+2∥Λ(cid:101)∥∥V(cid:98)rV(cid:98) rT−V rV rT∥≤ξ∥∆∥,
for a constant ξ > 0 and (c) follows as Lemma 7 and the last inequality follows whenever
σ max(B)>C(cid:101)3σ for a large enough constant C(cid:101)3 >0.
C Proofs for Corollary 5
In order to proof Corollary 5, we only need to directly check that Assumption 3 is satisfied
whenAssumption2,Assumption7andAssumption9andd3 ≤O( n )hold(noticethat
(logn)2
since the noise is isotropic, we only need the weaker version of Assumption 3(c)).
37