[
    {
        "title": "DreamCinema: Cinematic Transfer with Free Camera and 3D Character",
        "authors": "Weiliang ChenFangfu LiuDiankun WuHaowen SunHaixu SongYueqi Duan",
        "links": "http://arxiv.org/abs/2408.12601v1",
        "entry_id": "http://arxiv.org/abs/2408.12601v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12601v1",
        "summary": "We are living in a flourishing era of digital media, where everyone has the\npotential to become a personal filmmaker. Current research on cinematic\ntransfer empowers filmmakers to reproduce and manipulate the visual elements\n(e.g., cinematography and character behaviors) from classic shots. However,\ncharacters in the reimagined films still rely on manual crafting, which\ninvolves significant technical complexity and high costs, making it\nunattainable for ordinary users. Furthermore, their estimated cinematography\nlacks smoothness due to inadequate capturing of inter-frame motion and modeling\nof physical trajectories. Fortunately, the remarkable success of 2D and 3D AIGC\nhas opened up the possibility of efficiently generating characters tailored to\nusers' needs, diversifying cinematography. In this paper, we propose\nDreamCinema, a novel cinematic transfer framework that pioneers generative AI\ninto the film production paradigm, aiming at facilitating user-friendly film\ncreation. Specifically, we first extract cinematic elements (i.e., human and\ncamera pose) and optimize the camera trajectory. Then, we apply a character\ngenerator to efficiently create 3D high-quality characters with a human\nstructure prior. Finally, we develop a structure-guided motion transfer\nstrategy to incorporate generated characters into film creation and transfer it\nvia 3D graphics engines smoothly. Extensive experiments demonstrate the\neffectiveness of our method for creating high-quality films with free camera\nand 3D characters.",
        "updated": "2024-08-22 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12601v1"
    },
    {
        "title": "ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction",
        "authors": "Ziyu TangWeicai YeYifan WangDi HuangHujun BaoTong HeGuofeng Zhang",
        "links": "http://arxiv.org/abs/2408.12598v1",
        "entry_id": "http://arxiv.org/abs/2408.12598v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12598v1",
        "summary": "Neural implicit reconstruction via volume rendering has demonstrated its\neffectiveness in recovering dense 3D surfaces. However, it is non-trivial to\nsimultaneously recover meticulous geometry and preserve smoothness across\nregions with differing characteristics. To address this issue, previous methods\ntypically employ geometric priors, which are often constrained by the\nperformance of the prior models. In this paper, we propose ND-SDF, which learns\na Normal Ddeflection field to represent the angular deviation between the scene\nnormal and the prior normal. Unlike previous methods that uniformly apply\ngeometric priors on all samples, introducing significant bias in accuracy, our\nproposed normal deflection field dynamically learns and adapts the utilization\nof samples based on their specific characteristics, thereby improving both the\naccuracy and effectiveness of the model. Our method not only obtains smooth\nweakly textured regions such as walls and floors but also preserves the\ngeometric details of complex structures. In addition, we introduce a novel ray\nsampling strategy based on the deflection angle to facilitate the unbiased\nrendering process, which significantly improves the quality and accuracy of\nintricate surfaces, especially on thin structures. Consistent improvements on\nvarious challenging datasets demonstrate the superiority of our method.",
        "updated": "2024-08-22 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12598v1"
    },
    {
        "title": "Automating Deformable Gasket Assembly",
        "authors": "Simeon AdebolaTara SadjadpourKarim El-RefaiWill PanitchZehan MaRoy LinTianshuang QiuShreya GantiCharlotte LeJaimyn DrakeKen Goldberg",
        "links": "http://arxiv.org/abs/2408.12593v1",
        "entry_id": "http://arxiv.org/abs/2408.12593v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12593v1",
        "summary": "In Gasket Assembly, a deformable gasket must be aligned and pressed into a\nnarrow channel. This task is common for sealing surfaces in the manufacturing\nof automobiles, appliances, electronics, and other products. Gasket Assembly is\na long-horizon, high-precision task and the gasket must align with the channel\nand be fully pressed in to achieve a secure fit. To compare approaches, we\npresent 4 methods for Gasket Assembly: one policy from deep imitation learning\nand three procedural algorithms. We evaluate these methods with 100 physical\ntrials. Results suggest that the Binary+ algorithm succeeds in 10/10 on the\nstraight channel whereas the learned policy based on 250 human teleoperated\ndemonstrations succeeds in 8/10 trials and is significantly slower. Code, CAD\nmodels, videos, and data can be found at\nhttps://berkeleyautomation.github.io/robot-gasket/",
        "updated": "2024-08-22 17:57:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12593v1"
    },
    {
        "title": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations",
        "authors": "Can QinCongying XiaKrithika RamakrishnanMichael RyooLifu TuYihao FengManli ShuHonglu ZhouAnas AwadallaJun WangSenthil PurushwalkamLe XueYingbo ZhouHuan WangSilvio SavareseJuan Carlos NieblesZeyuan ChenRan XuCaiming Xiong",
        "links": "http://arxiv.org/abs/2408.12590v1",
        "entry_id": "http://arxiv.org/abs/2408.12590v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12590v1",
        "summary": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models.",
        "updated": "2024-08-22 17:55:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12590v1"
    },
    {
        "title": "Real-Time Video Generation with Pyramid Attention Broadcast",
        "authors": "Xuanlei ZhaoXiaolong JinKai WangYang You",
        "links": "http://arxiv.org/abs/2408.12588v1",
        "entry_id": "http://arxiv.org/abs/2408.12588v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12588v1",
        "summary": "We present Pyramid Attention Broadcast (PAB), a real-time, high quality and\ntraining-free approach for DiT-based video generation. Our method is founded on\nthe observation that attention difference in the diffusion process exhibits a\nU-shaped pattern, indicating significant redundancy. We mitigate this by\nbroadcasting attention outputs to subsequent steps in a pyramid style. It\napplies different broadcast strategies to each attention based on their\nvariance for best efficiency. We further introduce broadcast sequence parallel\nfor more efficient distributed inference. PAB demonstrates superior results\nacross three models compared to baselines, achieving real-time generation for\nup to 720p videos. We anticipate that our simple yet effective method will\nserve as a robust baseline and facilitate future research and application for\nvideo generation.",
        "updated": "2024-08-22 17:54:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12588v1"
    }
]