[
    {
        "title": "ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction",
        "authors": "Ziyu TangWeicai YeYifan WangDi HuangHujun BaoTong HeGuofeng Zhang",
        "links": "http://arxiv.org/abs/2408.12598v1",
        "entry_id": "http://arxiv.org/abs/2408.12598v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12598v1",
        "summary": "Neural implicit reconstruction via volume rendering has demonstrated its\neffectiveness in recovering dense 3D surfaces. However, it is non-trivial to\nsimultaneously recover meticulous geometry and preserve smoothness across\nregions with differing characteristics. To address this issue, previous methods\ntypically employ geometric priors, which are often constrained by the\nperformance of the prior models. In this paper, we propose ND-SDF, which learns\na Normal Ddeflection field to represent the angular deviation between the scene\nnormal and the prior normal. Unlike previous methods that uniformly apply\ngeometric priors on all samples, introducing significant bias in accuracy, our\nproposed normal deflection field dynamically learns and adapts the utilization\nof samples based on their specific characteristics, thereby improving both the\naccuracy and effectiveness of the model. Our method not only obtains smooth\nweakly textured regions such as walls and floors but also preserves the\ngeometric details of complex structures. In addition, we introduce a novel ray\nsampling strategy based on the deflection angle to facilitate the unbiased\nrendering process, which significantly improves the quality and accuracy of\nintricate surfaces, especially on thin structures. Consistent improvements on\nvarious challenging datasets demonstrate the superiority of our method.",
        "updated": "2024-08-22 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12598v1"
    },
    {
        "title": "Differentiable Logic Programming for Distant Supervision",
        "authors": "Akihiro TakemuraKatsumi Inoue",
        "links": "http://arxiv.org/abs/2408.12591v1",
        "entry_id": "http://arxiv.org/abs/2408.12591v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12591v1",
        "summary": "We introduce a new method for integrating neural networks with logic\nprogramming in Neural-Symbolic AI (NeSy), aimed at learning with distant\nsupervision, in which direct labels are unavailable. Unlike prior methods, our\napproach does not depend on symbolic solvers for reasoning about missing\nlabels. Instead, it evaluates logical implications and constraints in a\ndifferentiable manner by embedding both neural network outputs and logic\nprograms into matrices. This method facilitates more efficient learning under\ndistant supervision. We evaluated our approach against existing methods while\nmaintaining a constant volume of training data. The findings indicate that our\nmethod not only matches or exceeds the accuracy of other methods across various\ntasks but also speeds up the learning process. These results highlight the\npotential of our approach to enhance both accuracy and learning efficiency in\nNeSy applications.",
        "updated": "2024-08-22 17:55:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12591v1"
    },
    {
        "title": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations",
        "authors": "Can QinCongying XiaKrithika RamakrishnanMichael RyooLifu TuYihao FengManli ShuHonglu ZhouAnas AwadallaJun WangSenthil PurushwalkamLe XueYingbo ZhouHuan WangSilvio SavareseJuan Carlos NieblesZeyuan ChenRan XuCaiming Xiong",
        "links": "http://arxiv.org/abs/2408.12590v1",
        "entry_id": "http://arxiv.org/abs/2408.12590v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12590v1",
        "summary": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models.",
        "updated": "2024-08-22 17:55:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12590v1"
    },
    {
        "title": "Identifying the Best Arm in the Presence of Global Environment Shifts",
        "authors": "Phurinut SrisawadJuergen BrankeLong Tran-Thanh",
        "links": "http://arxiv.org/abs/2408.12581v1",
        "entry_id": "http://arxiv.org/abs/2408.12581v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12581v1",
        "summary": "This paper formulates a new Best-Arm Identification problem in the\nnon-stationary stochastic bandits setting, where the means of all arms are\nshifted in the same way due to a global influence of the environment. The aim\nis to identify the unique best arm across environmental change given a fixed\ntotal budget. While this setting can be regarded as a special case of\nAdversarial Bandits or Corrupted Bandits, we demonstrate that existing\nsolutions tailored to those settings do not fully utilise the nature of this\nglobal influence, and thus, do not work well in practice (despite their\ntheoretical guarantees). To overcome this issue, in this paper we develop a\nnovel selection policy that is consistent and robust in dealing with global\nenvironmental shifts. We then propose an allocation policy, LinLUCB, which\nexploits information about global shifts across all arms in each environment.\nEmpirical tests depict a significant improvement in our policies against other\nexisting methods.",
        "updated": "2024-08-22 17:47:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12581v1"
    },
    {
        "title": "RuleAlign: Making Large Language Models Better Physicians with Diagnostic Rule Alignment",
        "authors": "Xiaohan WangXiaoyan YangYuqi ZhuYue ShenJian WangPeng WeiLei LiangJinjie GuHuajun ChenNingyu Zhang",
        "links": "http://arxiv.org/abs/2408.12579v1",
        "entry_id": "http://arxiv.org/abs/2408.12579v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12579v1",
        "summary": "Large Language Models (LLMs) like GPT-4, MedPaLM-2, and Med-Gemini achieve\nperformance competitively with human experts across various medical benchmarks.\nHowever, they still face challenges in making professional diagnoses akin to\nphysicians, particularly in efficiently gathering patient information and\nreasoning the final diagnosis. To this end, we introduce the RuleAlign\nframework, designed to align LLMs with specific diagnostic rules. We develop a\nmedical dialogue dataset comprising rule-based communications between patients\nand physicians and design an alignment learning approach through preference\nlearning. Experimental results demonstrate the effectiveness of the proposed\napproach. We hope that our work can serve as an inspiration for exploring the\npotential of LLMs as AI physicians.",
        "updated": "2024-08-22 17:44:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12579v1"
    }
]