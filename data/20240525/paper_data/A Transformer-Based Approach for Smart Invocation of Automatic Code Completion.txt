A Transformer-Based Approach for Smart Invocation of
Automatic Code Completion
AraldeMoor ArievanDeursen MalihehIzadi
a.d.demoor@tudelft.nl arie.vandeursen@tudelft.nl m.izadi@tudelft.nl
DelftUniversityofTechnology DelftUniversityofTechnology DelftUniversityofTechnology
Delft,Netherlands Delft,Netherlands Delft,Netherlands
ABSTRACT
ofthesemodels,particularlythosebuiltontheTransformerar-
Transformer-basedlanguagemodelsarehighlyeffectiveforcode chitecture[38].TheseAI-driventoolstypicallyanalysethecode
completion,withmuchresearchdedicatedtoenhancingthecon- precedingthecursortosuggestthenextlinesofcode[4,18].Recent
tentofthesecompletions.Despitetheireffectiveness,thesemodels advancementshaveexpandedtheconsideredcontexttoinclude
comewithhighoperationalcostsandcanbeintrusive,especially notonlythesubsequentcode[3,15,19,26,35],butalsorelated
whentheysuggesttoooftenandinterruptdeveloperswhoarecon- snippetsfromotherfilestoenrichthepredictionaccuracy[27,45].
centratingontheirwork.Currentresearchlargelyoverlookshow Thisfocusonaugmentingthecontentqualityofcompletions
thesemodelsinteractwithdevelopersinpracticeandneglectsto hasinadvertentlyovershadowedavitalaspectoftheuserexperi-
addresswhenadevelopershouldreceivecompletionsuggestions. ence:theinteractiondynamicsbetweenthedevelopersandtheAI
Totacklethisissue,wedevelopedamachinelearningmodelthat tools[2,34,42].Whilethesemodelsgeneratehigh-qualitycode
canaccuratelypredictwhentoinvokeacodecompletiontoolgiven suggestions,theiroperationalandenvironmentalcostsposesignif-
thecodecontextandavailabletelemetrydata. icantchallenges[6,30].Moreover,duetotheirpotentialtodisrupt
Todoso,wecollectadatasetof200kdeveloperinteractionswith thecodingworkflowofdevelopers,thefrequencyandtimingof
ourcross-IDEcodecompletionpluginandtrainseveralinvocation thesesuggestionsiscriticalfortheoverallproductivitythetools
filteringmodels.Ourresultsindicatethatoursmall-scaletrans- aimtoboost[34].
formermodelsignificantlyoutperformsthebaselinewhilemain- Previouseffortsfocusondevelopingafilteringmodeldesigned
taininglowenoughlatency.Wefurtherexplorethesearchspacefor toshowacompletiononlywhenthereisahighconfidenceitwillbe
integratingadditionaltelemetrydataintoapre-trainedtransformer accepted[32,39].Thisreducesinferencecostandlikelyimproves
directlyandobtainpromisingresults. Tofurtherdemonstrateour developers’focus.However,Sunetal.[39]assumecompletionsare
approach’spracticalpotential,wedeployedthemodelinanonline rejectedbasedonthecontextbeforethecursoralone,ignoringthe
environmentwith34developersandprovidedreal-worldinsights interplaywithdevelopers’modeofthought.Mozannaretal.[32]
basedon74kactualinvocations. improvesonthisbyconsideringin-IDEtelemetrydata.However,
theyproposearelativelycomplexensemblemodelthatcanincur
CCSCONCEPTS additionallatencybyfilteringafteracompletionisgenerated;and
donotconsiderthatsomecompletions,despitebeingrejected,may
•Human-centeredcomputing;•Computingmethodologies;
helpguidetheuserintheirthinking.
KEYWORDS Inthisstudy,wetakeafurthersteptoproactivelypredictwhen
to invoke a code completion model based on code context and
IDE,CodeCompletion,Usability,Transformers,Interaction telemetrydata.Ourlightweight,transformer-basedfilteringmodel,
ACMReferenceFormat: JonBERTa,isdesignedtotriggeracodecompletionmodelonly
AraldeMoor,ArievanDeursen,andMalihehIzadi.2024. ATransformer- whenthere’sastronglikelihoodthatadeveloperrequiresassis-
BasedApproachforSmartInvocationofAutomaticCodeCompletion.In tanceorislikelytoacceptthesuggestedcompletion.Totrainour
Proceedingsofthe1stACMInternationalConferenceonAI-PoweredSoftware model,weleveragethedatawehavecollectedfromdevelopers’
(AIware’24),July15–16,2024,PortodeGalinhas,Brazil.ACM,NewYork, real-worldinteractionswithouropen-sourcecodecompletiontool
NY,USA,14pages.https://doi.org/10.1145/3664646.3664760 calledCode4Me1,availableforbothVSCodeandJetbrainsIDEs.
Wegathercodecontextandtelemetrydatafromuserinteractions
1 INTRODUCTION
withtheplugin,subjecttotheirconsent.Weusetwoindicators
Transformer-basedcodecompletionhasbecomeessentialinmod- togaugewhenadeveloperwouldprefertoreceiveasuggestion
ernsoftwaredevelopment[49].ThewidespreadadoptionofArti- basedonusagedata:(1)whentheyacceptamodel-suggestedcom-
ficialIntelligence(AI)toolsincodinghighlightsthesignificance pletion,and(2)whentheymanuallyinvokethemodel,irrespective
ofwhethertheyultimatelyacceptorrejectthesuggestion.
Permissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalor Everykeystrokemadebyadeveloperprovidestwokeytypesof
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed contextualinformationthatassistourinvocation-filteringmodel
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored. indecidingwhethertotriggertheLLM-basedcompletionsystem.
Forallotheruses,contacttheowner/author(s). Theseare:(1)thecodingcontextsurroundingthepointofinvoca-
AIware’24,July15–16,2024,PortodeGalinhas,Brazil
tion,and(2)telemetrydatagatheredviatheplugin,e.g.thetime
©2024Copyrightheldbytheowner/author(s).
ACMISBN979-8-4007-0685-1/24/07
https://doi.org/10.1145/3664646.3664760 1https://code4me.me
4202
yaM
32
]ES.sc[
1v35741.5042:viXraAIware’24,July15–16,2024,PortodeGalinhas,Brazil deMooretal.
since the last completion. First, we use code context alone and lackofpersonalisationwithdevelopers’modeofthought[2].Copi-
trainatransformer-basedclassifier.Next,weinvestigatehybrid lot’sauthorsthemselvesreportedthatabouttwo-thirdsofshown
transformerarchitecturesintegratingtelemetrydataasadditional completionsareignoredbytheirend-user[49].Furthermore,Git-
features.Basedonourpromisingresults,weevaluateourapproach Clearrecentlyreleasedareportempiricallydescribingastrong
inauser-studywith34developers,toinvestigatehowourfilters correlationbetweentheadoptionofAIcodecompletionandcode
performinpractice.Tothisend,wealsoproposeanewperformance churninindustry-gradesoftwareengineering[16],raisingques-
metrictohelpmitigateissuesfromoptimisingforjustacceptance tionsabouttheimpactofAIonsoftwaremaintainability.Other
rateinpreviouswork[32,49],whichweighsproxiesforthequality studiesfurtherfindthatthebugsintroducedbyLLM-poweredcode
andtimingofacompletionequallyasaharmonicmean.Wefind completionareoftenmoresubtle[4,34],anddifficulttorevert[42].
thatourproposedJonBERTa-headmodelscoreshighestinboth Barkeetal.[2]findthatdeveloperinteractionswithAI-powered
theofflineandonlineevaluation. codecompletionarebimodal:eitheraccelerative,wherethedevel-
Ourcontributionsareasfollows: operknowswhattheywantandusesthetooltogettherefaster;or
• Anofflineevaluationofatransformermodelwefine-tuned explorative,wherethedeveloperreliesonthetooltosuggestpos-
onourcollectedcodecompletiondataset,demonstratingthat sibleapproaches.Pratheretal.[34]observetwoadditionalmodes
codecontextcanconsiderablyimprovefilteringaccuracy amongnovices:shepherding,whereanoviceslowlyacceptsasug-
overabaselinetrainedontelemetryfeaturesonly. gestion;anddrifting,wheretheyareleddownacyclic‘debugging
• JonBERTa, a novel transformer architecture to show the rabbithole’.Notonlyisthisaninefficientuseofcomputational
potentialoftrainingjointlyoncodecontextandtelemetry resources,butthisalsomisalignsthetoolwithdeveloperintent.
data;aswellasacustomtokenisationstrategycentredon Somestudiesadvocateforallowinguserstoconfigurethetiming
thecursorposition. andcontextofcompletions[2,44]toaddresstheseconcerns.How-
• An online evaluation of our filters in a code-completion ever,weassumethatthemajorityofend-userswilllikelyexpect
pluginwith74kinvocations,spanning34users. suchtoolstoworkoutoftheboxandadapttotheirusagepatterns.
• For reproducibility purposes, we publish our replication Theissueoflanguagemodelalignmentisaspressingastherate
package2withanonlineappendix,aswellasourfine-tuned oftheirincreasingcapabilities.AsLLMsarebecomingmoreinte-
models3.However,incompliancewiththeGDPR,wecannot gratedintoend-userworkflows,itisnecessarytothinkbeyond
shareourtrainingdataset. aligningmerecontent;but,alsotheinteractionswiththiscontent.
2 BACKGROUNDANDRELATEDWORK
2.2 ExistingSolutions
Today,themostprominentAI-poweredcodecompletiontoolwith
Sunetal.[39]aimtoaddressthisproblembyfilteringoutcomple-
over one million active users is GitHub Copilot [13]. This tool
tionsthatarelikelytoberejected.Theytrainatransformer-based
ispoweredby atransformer-based [43]LLMtrainedonsource
classifieronthecodeprefixbeforeitissubmittedtothecompletion
code,originallywiththeobjectiveofpredictingafunctionbody
model,andfinditcanhide5%ofsuggestionsthatwouldhavebeen
givenitsdocumentation[4].However,giventhatthesuffixlines
rejectedwith94.5%accuracy.However,theauthorsrelyonthe
belowthefunctionareunavailabletothemodel,thecurrentmodel
assumptionthatrejectedcompletionsareduetoinsufficientcode
poweringCopilotispresumablytrainedwithaFill-In-the-Middle
contextalone.
(FIM)objective[3];wherethemodelistrainedtopredictaspanof
Mozannaretal.[32]trainafilterconsistingoftwomodelensem-
arbitrarylengthbetweentheprefixandsuffix.Severalalternatives
bles:oneensemblebeforeandoneafteracompletionisgenerated.
existtoCopilot,namelyAmazonCodeWhisperer[1],TabNine[40],
Theyciteitcanhide53%ofcompletionswiththeguarantee91%
Codeium[7],SourcegraphCody[37],JetBrainsAI[20],andGemini
wouldhavebeenrejectedbytheuser.However,despitedesigning
CodeAssist[12].
their tool for GitHub Copilot, they do not include a user-study.
Sinceitsinception,severaluserstudiesandsurveys[2,24,31,
33,34,42,44,49]havebeenperformedonGitHubCopilot4.They Additionally,theirfiltercandependonacompletionbeinggener-
ated,whichisguaranteedtoincuradditionallatencyandcompute
highlight that the transformer models backing such tools excel
inpractice.Moreover,bothofthesetoolsaimtomaximisetheac-
atprovidingcontextual suggestions,duetotheirsemanticalun-
ceptancerateoftheshowncompletions,whiletheneedforbetter
derstandingofcode.Asaresult,thisleadstoincreaseddeveloper
alignmentwithend-usershasbeenhighlighted[2,32,36,42].
satisfaction[42]andperceivedproductivity[49].
Furthermore,asdetailedinareverse-engineeringblog-post[41],
2.1 CodeCompletionPainPoints Copilotalsohasitsownlogistic-regressionclassifiertofilterout
completionsbasedontelemetrydata.Weanalysehowitweighs
Nonetheless,suchnewtechnologycomeswithnewquestionsabout
eachfeatureinmoredetailinouronlineappendix,anddesignour
itsusabilityanddesign,arisingfromdeveloperpainpoints,such
baselineinthisstudyassimilaraspossible.
as:distractionduetothealways-onnatureofsuggestions[34],out-
Ahead-runnerinthisfieldwouldbeGmailSmartCompose[5],
of-distributiongenerationleadingtohallucinatedterms[21],anda
consideringthesameprobleminanatural-languageemailsetting.
2https://github.com/ar4l/curating-code-completions Theauthorsdetailadeepconsiderationforlatency,scalability,and
3https://huggingface.co/collections/AISE-TUDelft/smart-invocation-of-code- personalisation. However, their approach uses legacy language
completion-66473ddf6fa6cf6e541f750c
4Asof29December2023,GitHubCopilot,andothers,alsoofferaconversational modelarchitectures;andtheirinsights,whilevaluable,maynot
interface;butwelimitthescopeofthispapertogenerativecodecompletiononly. entirelyapplytotheprogrammingprocess.ATransformer-BasedApproachforSmartInvocationofAutomaticCodeCompletion AIware’24,July15–16,2024,PortodeGalinhas,Brazil
3 PROBLEMDEFINITION
trigger(e.g.,fullstoporanopeningparenthesis).Though,
Codecompletionaimstoimprovedeveloperproductivitybysaving wedoallowuserstomanuallyinvokeacompletion.
themkeystrokesandkeepingthemintheirflow.Ziegleretal.[49]
• Modelselection.Ourline-completionsaregeneratedby
proposestheacceptancerateofsuggestionsasaproxyfordevelop- smallerlanguagemodelsthantheindustry-standard,allow-
ers’perceivedproductivity;however,thisiscritiquedbyMozannar ingtheusertopickoneofthreecompletionsprovidedby
etal.[32],whomfindthatoptimisingtheacceptancerateresultsin InCoder[11],UniXCoder[14],andCodeGPT[28].
shorterandtypicallylessusefulsuggestions.
Therefore, some completions, despite seemingly helpful, can Tohelpkeepdevelopersintheirflow,wewanttoleveragelan-
potentiallybedetrimentaltosoftwarequalityandprogramming guagemodels’abilitytocompletecodeevenwhennotataprede-
flow.And,conversely,somecompletions,despitebeingrejected, finedtriggerpoint.Inpractice,thismeansthatourpluginwillquery
arenotwhollyignoredbythedeveloperandarepotentiallyhelpful thecompletionsserveratanycursorposition,andwewouldlike
inguidingtheirthinking(e.g.,atip-of-the-tonguefunctioncall, tofilteroutthosepointswhereitmaynotbenecessarytogenerate
butwiththewrongarguments).Determiningtheactualquality
acompletion.Thus,thepluginshouldfilteroutthoseautomatic
ofacompletion,basedonitsfunctionalcorrectnessandhuman
suggestionsthathistoricallyendedupbeingrejectedorignoredby
preferencesforstyle,remainsanopenquestion,thoughresearchis theuser.And,additionally,theplugininfersthatauserwantsa
progressingintherightdirection[8,47,48].
suggestion,atahistoricallymanualtriggerpoint.
Consequently,wereframethisproblemtowhatwecanactually Asaddedcontext,ourfiltershouldprioritisefalse-positivesover
measure.Weconsidertwoplausiblereasonswhyausermayhave false-negatives.Weassumethatthisiswhereuserpreferenceslie,
rejected a suggestion, as depicted in Figure 1: (1) the model is giventheconstant-suggestionnatureofthemorepopularplugins.
incapableofgeneratingagoodsuggestion,whichismainlycode However,toavoidwastedcomputeanddeveloperdistraction,we
context-dependent;or(2)theuserdoesnotwanttoseeasuggestion, aimtominimisethosecompletionsthatarecertaintobeignored;
whichismainlyusertelemetry-dependent.Thesereasonsarenot e.g.,becausethedeveloperisactivelytyping,ortheirintentcannot
exclusiveandlikelyhaveconsiderableoverlap.Thus,wemotivate beaccuratelyinferredatthecurrentcursorposition.
theneedforintegratingthesefeaturetypestobetteralignwiththe Lastly,intermsofnon-functionalconstraints,wewouldlikeour
end-user’sflow. filtertotakeatmost10mstomakeadecision.Thisisinpreparation
forabackendmigrationtothevLLMengine[23].Foranyincoming
completionprompt,thislibrarywaitsabout10msfortoseeifitcan
Rejected Completions
bebatchedtogetherwithotherrequests.Ifourfiltertakeslessthan
10ms,itmeansweincur0additionallatencyoverthebasewaittime.
Model incapable of
And,eveniffilteringtakeslonger,itisstillpossibletoterminate
generating a worth-
thetoken-by-tokengenerationprocessearly,savingconsiderable
while completion. computeasourcompletionstakeabout300–400mstogenerate.
3.2 JointOptimisationObjective
User does not want
to see a completion Ourgoalistotrainafiltertoinvokethecompletionmodelonly
at this moment. attheinstantadeveloperwantstoseeacompletion.Wepartially
mitigate the issues arising from optimising for acceptance rate,
throughtheobservationthatwemayactuallywanttodisplaythe
rejectedcompletionsthatweremanuallyinvokedviaakey-bind.By
Figure1:ReasonsforRejectedCompletions. trainingafilterwiththisobjective,alongwithanycompletionsthat
wereaccepted,wehopetobetteralignthecompletionsthatpass
throughthefilterwithwhatend-userswantatthatmoment.Our
3.1 Code-CompletionDataandConstraints objectiveisthusperpendiculartoexistingworkinthisarea[32,39].
Theobjectivejointlyoptimisesthefollowing:(1)weaimtomin-
Throughoutthisstudy,weleverageCode4Me,anopen-sourcecode imisetheamountoftimesanend-userhastomanuallyinvokethe
completionpluginwitharound100monthlyactiveusersdeveloped model,and(2)weaimtominimisetheamountofrejectedcomple-
atourinstitution[19].Wenotethreekeydifferencescomparedwith tionsthatwereautomaticallytriggered.Inotherwords,weconsider
thepopularcodecompletionpluginsmentionedearlier(Section2): allmanualinvocationsandautomatic,acceptedcompletionstobe
• Line-completiononly.Othertoolstendtoprovidemulti- ourpositiveclass(notfilteredout),andrejectedautomaticcomple-
linesuggestionsinghost-textstyle;whileweprovidecom- tionstobeournegativeclass(shouldbefiltered).
pletions up to a newline character. Additionally, comple- Anaddedbonusofconsideringmanually-invoked,yetrejected
tionsaredisplayedinacompletionboxalongwithtypical completionsinthepositiveclassisthattheresultingfilterwillbe
language-serversuggestions. lessdependentonthecompletion-model’scapabilities.Weassume
• Restrictiveactivation.Contrarytoalternativeplugins,we thatamanualtriggerisastrongindicatorthatauserwouldlike
providecompletionsonlyonapredefinedsetoftriggerchar- to see a suggestion, and choose not to depend on whether the
acters;atwhichIDE-basedautocompletewouldtypically suggestionwasacceptedinthisscenario.AIware’24,July15–16,2024,PortodeGalinhas,Brazil deMooretal.
4 APPROACH
Output
Weaimtofilteroutsuggestionsbyteachingamodeltopredict,
Probabilities
atanycursorlocation,whethertoinvokethecompletionmodel RoBERTa
ornot.Todothis,weproposetoleverageourcollecteddatasetof
codecompletionsandaccompanyingin-IDEtelemetrytobetter
Linear (proj.)
discernthenatureofinteractions.Notingthestate-of-the-artcon-
textualunderstandingthattransformermodelsexhibit,weexplore Linear (dense)
architecturesforintegratingtelemetryfeatureswithcodecontext.
4.1 JonBERTaArchitecture Add & Norm
Weaugmentacode-pretrainedRoBERTaarchitecture[25],yielding Feed
thefollowingtwomodelsof84Mparameters.Astherearemany Forward
possibleextensionstoatransformermodel,welimitoursearch
space to parameter-efficient implementations. Specifically, both N×
ourmodelsincurlessthan1Madditionalparameters.Weassume
Add & Norm
readers’familiaritywiththetransformerarchitecture[43].
Multi-Head
• JonBERTa-headincorporatingtelemetryfeaturesdirectly Attention
intheclassificationhead.
• JonBERTa-attnattendingto(small)learnedfeatureembed-
dingsintheself-attentionmodules.
Norm
WeusetheJonprefixtorefertoJointlyoptimisedattention,
Positional
tobothcodecontextandtelemetrydata.Themotivationbehind
Encoding
thisapproachliesinthestate-of-the-artcontextualunderstanding
transformermodelsexhibit,whichwehypothesisecanalsoleverage Token
Embedding
contextualtelemetrydata.Wefurtherproposeanoveltokenisation
strategycentredonthecursor,tocapturethemostsignificantparts
ofcodecontext. Code Context Telemetry Features
(Prefix & Suffix) (Scalar-Valued)
4.1.1 ExtendedClassificationHead. WeexploreasimpleJonBERTa-
headmodeldepictedinFigure2.Giventhataclassificationhead
firstpoolstheoutputofthepreviouslayertothefirsttoken(<cls>,
Figure2:TelemetryFeatureDatainClassificationHead.
the classification token), it is trivial to extend this token’s one-
dimensionalembeddingwithadditionalfeaturedata.
Inthescopeofthispaper,weonlyconsideraJonBERTa-head furtherexploreavarietyoflayercombinationsandfeatureembed-
wheretheembeddingisconcatenatedbeforereachingthedense dingdimensionsintheonlineappendixofourreplicationpackage,
layer.Thedenselayerisamatrixofsize𝑐×𝑐,where𝑐isthelengthof
butcannotconclusivelystatewhichachievesbetterresults.We
atokenembedding.Denselayerscanhelpthemodellearnlow-rank useafeatureembeddingdimensionof204throughoutthisstudy
embeddings[17]ofbothfeatureandcodecontext(whichcanhelp tolimittheadditionalparameterstothemodel,whileensuring
withtrain/testgeneralisability),whiletheprojectionlayerafterward enoughexpressivity.
servesasalogisticclassifier.Weincreaseonlythedenselayer’s
size,alongoneaxis,toaccommodatetheconcatenatedfeatures, 4.1.3 Tokenisation Strategy. The code context provided to our
andreinitialiseit. JonBERTa-headandJonBERTa-attnmodelsconsistsoftheprefix
(beforethecursor),andthesuffix(afterthecursor).Commonly,
4.1.2 Extended Self-Attention. Figure 3 depicts our JonBERTa- tokenisers truncate such sequence pairs either both on the left
attnmodel,whichlearnsfeatureembeddingstobeattendedtoin orbothontherightinpairedsequence-classificationtasks(e.g.,
thepre-existingself-attentionmodule.Eachweightinthefeature question-answermatching).However,wehypothesisethatitisop-
embeddingmatrixislearnedasafunctionofthecorresponding timaltocentrethecontextwindowonthecursorlocation.Perhaps
scalarfeature.Giventhisembedding,keys,andvaluescanbepro- surprisingly,somethingwehavenotyetseeninpreviouswork.
ducedtobeattendedtobycodetokens.Inpractice,theattention ToachievethiswithintheJonBERTacontextwindowof512
moduleitselfisequivalenttotheoriginalmodel,exceptthatthe tokens,wefirsttokenisethesuffixwithright-truncationuptoa
keysemittedfromfeaturescandot-multiplywiththequeriesfrom maximumof128tokens.Denotethenumberoftokensinthesuffix
tokenembeddings,toproduceweightedscoresforhowmucha by𝑛 𝑠,whichmaybelessthan128ifthedeveloperisclosetothe
givenfeature’svalueshouldbeaddedtothattokenembedding. endofthefile.Wethentokenisetheprefix,uptoamaximumof
Byincludingtelemetryfeatureembeddingsinitsself-attention 512−1−𝑛 𝑠;subtractingoneoffthetotalcontextwindowhere,to
mechanism, we hypothesise the model is able to combine both allowustoinserta<sep>separatortokenatthecursorposition.If
modalitiestograspafirmerpictureofthecurrentuserintent.We thetotallengthhappenstobeshorterthanthecontextwindow,weATransformer-BasedApproachforSmartInvocationofAutomaticCodeCompletion AIware’24,July15–16,2024,PortodeGalinhas,Brazil
Table 1: Class Distribution in Our Code Completion
Output Train/ValidationandTestDatasets.
Probabilities
RoBERTa
Class Positive Negative
Linear (proj.) Completion Automatic
Manual
Type Accepted Rejected
Linear (dense)
Test 6118(27.8%) 431(1.9%) 15889(70.3%)
Train/Validation 3909(33.3%) 3909(33.3%) 3909(33.3%)
Add & Norm
Feed
Table1showsthe(sub-)classdistributionofourtrain/validation
Forward
andtestdataset.Wepurposefullyavoiddistinguishingbetween
N× manualaccepted,andmanualrejectedinvocationsofourtool,as
weconsiderbothtobepartofourpositiveclass.Asaddedcontext,
Add & Norm
however,themanualrejectedinvocationsconstituteatotalof47%of
Multi-Head ourpositiveclassinthereal-world(test)distribution.Unfortunately,
Attention weareunabletoshareourcollecteddatasetasitcontainsuser-
sensitivecodecontext.
5 EXPERIMENTALSETUP
Norm
Welaidoutthecodecompletionplugincontextandproblemcon-
Positional
Encoding Norm straintsinSection3.And,havingestablishedoursearchspacefor
thisprobleminSection4,wenowproposehowtonavigateit.
Token Feature
Embedding Embedding
5.1 ResearchQuestions
AsdefinedinSection3.2,theobjectiveofourmodelsistolabel
Code Context Telemetry Features
manualandautomaticinvocationswhichareacceptedaspositive
(Prefix & Suffix) (Scalar-Valued)
(helpful);and,considertheremainingautomaticinvocationsthat
arerejectedasthenegativeclass(unhelpful).Tothisend,wepose
thefollowingresearchquestions:
Figure3:Self-AttentionExtendedtoTelemetryFeatureData
RQ1 Howdoesatransformermodelcomparetoabaseline
logistic regression model at filtering out unhelpful
conventionallyright-padtheremainderofthesequence.Notethat suggestions(offlineevaluation)?Wefine-tuneacode-
thisdoesn’tperfectlycentrethecursorinthecontextwindow,and pretrainedRoBERTa[25]model(CodeBERTa)oncodecom-
assumestheprefixholdsmoreweightthanthesuffix.Thisdecision pletionsnippetscollectedfromourplugin,andevaluateit
issubstantiatedbytheresultsinouronlineappendix. againstalogisticregressionbaselinetrainedontelemetry
and selected textual features. The baseline is inspired by
4.2 Dataset
reverse-engineeringGitHubCopilotandreflectsthestate-
We train our models on data collected from a code completion of-the-art.
plugindevelopedatourinstitution(Section3.1).Anygivencode RQ2 Canapre-trainedtransformerbeextendedtoincor-
porateanadditionalmodalityconsistingoftelemetry
suggestioniseithermanuallyinvokedviaakey-bind,orautomati-
callyonapredefinedsetofcommontriggercharacters(e.g.,afull
featuredata?WetrainourJonBERTavariants(Section4.1)
stop,oropeningparanthesis)[19].AsstatedinourobjectiveSec- withtelemetryfeaturedataasanadditionalmodalitywhen
tion3.2,weaimtooptimiseforthosecompletionsthatareeither makingpredictions.
(1)manuallyinvoked,or(2)automaticallyinvokedandaccepted. RQ3 Howeffectivearetheaboveapproachesinareal-world
Whilewehaveover1Minvocationsofourtool,afterfiltering setting (online evaluation)? We deploy our filters in a
forhigh-qualitysamplescontainingcodecontext(collectedonan code-completionplugintoinvestigatewhetherthefilters’
opt-inbasis,∼200ksamples),andbalancingourdatasetbyunder- decisionsalignwith34usersinpractice.Wefurtherevalu-
sampling,wemaintainonlyabout10kcodesuggestionsfortraining. atethecomputationalfeasibilityofourapproach,andnote
Ourtestset,followingthereal-worlddistributionofmanualand discrepanciesbetweentheofflineandonlineenvironments.
automatic invocations, contains about 20k samples, completely
5.2 EvaluationSettingsandMetrics
separatefromthetrainingset.Weempiricallycomparedifferent
datasetdistributionsinouronlineappendix,motivatingtheunder- 5.2.1 MetricsforOfflineSettings(RQ1&RQ2). Toevaluatewhether
sampledtrainingdistributioninthisclassificationscenariowhere ourmodelscancapturethedifferentinvocationtypesthatdeter-
classesarenotequallyrepresentedinpractice. mineourclasses(seeSection3.2),wecomputeaccuracypermanual,AIware’24,July15–16,2024,PortodeGalinhas,Brazil deMooretal.
acceptedautomatic,andrejectedautomaticsubclass.Basedonthis, S Snippetastheprompttothecompletionmodel,truncated
wefurthercomputethemacroaverageaccuracy,toserveasasingle tothefiltermodel’scontextwindow(512tokens)usingour
metrictocomparemodelson. centred-on-cursorstrategy(Section4.1.3).
Wechoosemacroaverageaccuracy(acrossclasses),asopposed
tomicroaverage(acrossallsamples),asourpositiveclassesare Table2:FeaturesUsedinClassification,perFilterModel.
under-representedinourcode-completiondataset.Asaresult,itis
paramountthatcompletionsthedeveloperwantstoseearepriori-
tisedagainstthevastmajorityofcompletionsthatareignored.We Log.Reg. Code Jon
assumethatthemistakeoffilteringoutacompletionwhenadevel- T1 Timesincelastcompletion ✓ ✓
operwouldwanttoseeoneisworsethanshowingacompletion T2 Documentlength ✓ ✓
whenthedeveloperdoesnotwanttoseeone. T3 Cursoroffset ✓ ✓
AsshowninTable1,someofourclassesareconsiderablyunder- T4 Offsetaspercentage ✓ ✓
represented.Thevarianceduetosuchasmalldatasetcanbecome T5 language(20options) ✓ ✓
pronouncedwhentrainingtransformermodels.Tocapturethis T6 IDE(jetbrains/vscode) ✓ ✓
variance,wetrainfivemodelsonfivetrain/evalsplits(9:1).Then,we
C1 Lengthoflastprefixline ✓
bootstrapouraccuracyscoresonthetestsetbyalternatinglytaking
asamplefromeachofthefivemodels,foratotalof𝑛 = 10,000 C2 Above,withoutwhitespace ✓
C3 Whitespaceaftercursor ✓
samples.
C4 Lastprefixchar(ASCII32-125) ✓
5.2.2 MetricsforOnlineSetting. Forouronlineevaluation,weno C5 Above,withoutwhitespace ✓
longerhaveavaluabledistributionofmanual/automaticclassesas S1 Prefix ✓ ✓
weremovethepredefinedtrigger-pointinvocationrule.Toremedy S2 Suffix ✓ ✓
this,weevaluatecompletionsthatpassthefiltervia(1)acceptance
rateasaproxyfortheirtimingwithdevelopers’modeofthought;
and(2)scoreacceptedcompletionsusingCodeBERTScore[47]asa Thefeaturesforthelogisticregressionmodelareinspiredby
proxyfortheirquality.Wealsomeasurethelatencyinmilliseconds. reverse-engineeringCopilot[41],withtheexceptionofonefeature
CodeBERTScoreisarecently-proposedmeasurethatcorrelates thatdependsonapre-existingfilter(whichwedonothave).The
closestwithbothfunctionalcorrectnessandhumanpreference[47]. languagesarethesameasthe20consideredbyCopilot.Wechoose
Thisiscontrarytotheoft-seenCodeBLEU,METEOR,andROUGE-L tofollowCopilotas,toourknowledge,thisistheonlyfiltercur-
measureswhicharedesignedfornaturallanguagesanddonotwork rentlydeployedinpractice,andcontainsmostofthesignificant
wellwiththesyntacticstructureofprogramminglanguages[8]. featuresfoundinpreviouswork[32].Anextendedexplanationof
CodeBERTScoreiscomputedbypassingthecodecompletionand thesefeaturescanbefoundinouronlineappendix.
groundtruth(after30s)throughacode-pre-trainedencodermodel,
5.4 ConfigurationandImplementationDetails
andthencomputestheF3scorebasedonthesimilaritybetween
tokenembeddingsatalayerthatcorrelatesbestwithhumanpref- Wefine-tunealltransformermodelsforsixepochswitha2𝑒−5
erenceandfunctionalcorrectness. learningrateand16batchsizefromapublicCodeBERTa-base-v15
Wefurtherproposetheharmonicmeanofthesetwoasasingle checkpoint,witheachepochcontainingabout10ktrainingsamples.
metrictocomparemodelsby.Specifically,optimisingonlyaccep- Wealsouseitstokeniserforsnippetfeatures.FortheJonBERTa
tancerateresultsinworsecompletions[32,49].And,optimising models,wefine-tunefromthe3rd-epochCodeBERTacheckpoint,
justthecontentofacompletion,doesnotmakethefilteralignwell foranadditionalthreeepochs;asweobservethisresultsinstabler
withdevelopers,asevidencedbythenumberofmanualcomple- trainingthantrainingfromthepubliccheckpoint.
tionsweobserveinourdataset.Wehopethiscommunicatestothe AllofourtransformermodelsareimplementedwithPyTorch6,
readerhowthesetwomeasuresshouldbeweighedinourframing andtrainedonanNVIDIAGeForceRTX3080GPU,takingabout20
oftheproblem. minutespermodel.Allmetricsarecomputedusingthefunctions
providedbyscikit-learn7library.Forouronlineevaluation,we
5.3 FeatureEngineeringandBaselines use an inference server with an NVIDIA GeForce RTX 2080 Ti,
separatefromourtrainingsetup.AsthisisarelativelyolderGPU,
Thefeaturesextractedfromourcode-completiondatafortheLo-
weexpectslightlyhigherlatencyduringfilterinference.
gisticRegression,CodeBERTa,andJonBERTamodels,areshown
inTable2.WepurposelyavoidprovidingJonBERTawithfeatures 6 RESULTS
thatcanbeinferredfromthecodecontext(e.g.,whetherthereis
6.1 RQ1:ImpactofCodeContext
whitespaceafterthecursor),toassertitisabletoleveragethatdata
implicitly.Tothisend,wedefinethreetypesoffeaturedata: ToaddressRQ1,weevaluatethecontributionsfromtrainingon
snippetfeatures(S),againsttelemetry(T),andfixed-ruletextual
T Telemetryasthosefeaturesthatcannotbedirectlyextracted
fromasnippetofcode.
5https://huggingface.co/huggingface/CodeBERTa-small-v1
C Code context as those textual features that are explicitly 6https://pytorch.org
extractedbyfixedrules. 7http://scikit-learn.orgATransformer-BasedApproachforSmartInvocationofAutomaticCodeCompletion AIware’24,July15–16,2024,PortodeGalinhas,Brazil
features(C).Tothisend,wefirstconsiderlogisticregressionbase- equallyearlychanceastokenembeddingsatcommunicatinginthe
linestrainedoniterativelymoreTandC(seeSection5.3).Lastly, attentionmechanism;andchooseanembeddingdimensionof204
Code4Meiscross-application,soweincludeafeaturefortheIDE. tonotincurtoomanyadditionalparameters.
Inourdataset,JetBrainsuserstendtohaveahighersuggestion
acceptancerate,partiallybecausetheysupportonlythepopular Table4:FilteraccuracyforCodeBERTaandJonBERTaclassi-
languages8whichcodecompletionmodelstendtoperformbest ficationmodels,givenperinvocationsub-class.ErrorBounds
on[22]. arefor𝑝 <0.5viaBootstrapping𝑛=10000.
Wedonotdirectlyextracttheweightsfromitsplugincode.In-
stead,weretraintheCopilot-stylebaselinesonourowncompletion- Manual Auto/acc. Auto/rej. Avg.
requestdataset.becausethedatadistributionofourcompletion
pluginislikelydifferentduetoitsdifferentinvocationmethodsand CodeBERTa 98.5±0.4 74.7±4.6 73.1±1.2 82.1
completionstyleasexplainedinsection3.1. JonBERTa
head(dense) 98.6±0.4 78.0±6.2 71.4±5.1 82.7
Table3:FilteraccuracyforLogisticRegressionandCode- attn(0L) 98.6±0.4 75.0±6.8 72.5±3.5 82.0
BERTaclassificationmodels,givenperinvocationsub-class.
In Table 4, JonBERTa-head promisingly shows it can better
Manual Auto/acc. Auto/rej. Avg. discernbetweencompletionclasses,thoughourresultsareless
conclusivethanbefore.Whileincorporatingthetelemetryfeatures
LogisticRegr.
inthefirstattentionlayerof JonBERTa-attnmatchestheper-
TelemetryT1−5 99.6±0.3 99.1±0.7 1.4±0.7 66.7
formanceof JonBERTa-head,implyingitcanlearntointegrate
+TextualC1−4 98.6±0.3 66.8±4.5 61.9±1.2 75.8
telemetryfeatureswithitssemanticunderstandingofcode,this
+CopilotC5 98.6±0.3 66.1±4.6 63.9±0.9 76.2
mayequallywellbeattributabletovariance.Thiscouldbe,inpart,
+IDET6 98.5±0.3 66.1±4.6 65.0±0.9 76.5
duetothelimitedtrainingsetsizeasrevealedbyourbootstrap
CodeBERTa 98.5±0.4 74.7±4.6 73.1±1.2 82.1
strategyforcomputingerrorbounds(Section5.2),whichwefurther
discussasaninternalthreattovalidityinSection7.1.1.
AsshowninTable3,amodeltrainedontelemetryfeatures(T1– Regardless,wepresenttheseresultsmotivatedbythesameargu-
5)alone,whileattainingaverageaccuracyof66.7%,iscompletely mentasinSection3:Transformermodelsarebecomingincreasingly
unabletodistinguishautomaticinvocationsthatendupbeingre- integratedintosoftwareengineeringtasks,butalsowelloutsideof
jected(1.4%accuracy).Thus,itisnecessarytoincludesomeexplicit thefield.Asaresult,thereisaneedtointegrateadditionalmodali-
textualfeatures(C1–4)fordistinguishingtheseclasses.Thisinti- tiesintopre-trainedmodels,astokensarebutoneofmanysources
matesthatextendedcodecontextcanbeleveraged.Wereferto ofinformationthatcanbeleveragedinAI-poweredtools.
ourreplicationpackageforadditionalexperimentswithdifferent
6.3 RQ3:OnlineEvaluation
granularitiesoftextualfeatures.
Notably,CodeBERTa,trainedonsolelycodesnippet(S)features, Toevaluatehowourfiltersfareintherealworld,wedeploythemin
canoutperformthebestbaselineonautomaticallyacceptedand acodecompletionpluginwith34developersovera2weekperiod,
automaticallyrejectedqueriesby9.6and3.2absolutepercentage resultingin74krequests.Tothisend,wedisablethepredefined
points,respectively.Thisindicatesthatthesemanticunderstanding trigger-pointconstraintthatwasdescribedinSection3.1,tonowau-
ofcodesuchatransformermodelexhibitspropelsitpasttheclas- tomaticallyinvokethecompletionmodelatthehistoricallymanual
sificationbaseline.And,furthermore,thissnippetmodalitylikely trigger-points.
contributesorthogonallytothetelemetrydata,asbotharedistinct WeperformanA/Bstudybyassigningeachuseroneofthe
featuresthatcannotbeinferredfromeachother.Thismotivates followingfivefilterspercodingsession.Wedefineasessionas
ourarchitecturalexplorationtoattendtoboththesemodalitiesin asequenceofcompletionrequestswhereanytwoarenomore
oneclassificationmodel. than30minutesapart,toavoidend-userconfusionfromdifferent
completionbehaviouroneveryrequest.Forallfilters,requestswith
6.2 RQ2:HybridJonBERTaModels
aprompt(prefix+suffixaroundthecursor)lessthan10characters
ToaddressRQ2,wetrainJonBERTamodelsleveragingbothsnippet areautomaticallyrejected,astheydonothaveenoughcontextfor
andtelemetrymodalitiesandcomparethemtoournewCodeBERTa aworthwhilecompletion.
baseline.Thisarchitecturalsearchspaceisespeciallyvastforthe 1 None:allcompletionrequestspassthrough.
JonBERTa-attnmodel,duetotheinclusionoffeatureembeddings 2 Logistic regression using telemetry (T) and context (C)
withtuneableparameters.Assuch,wedefermostofourexperi- features.
mentstotheonlineappendixinourreplicationpackage,aswell 3 CodeBERTausingonlysnippet(S)features.
asafewJonBERTa-headexperiments.Wechoosetoonlydisplay 4&5 JonBERTa-headand-attnusingbothTandS.
thefirst-layerconfigurationhere9togivetelemetryembeddingan
Table5showsourresults.Usingourproposedharmonicmean
toconveythebalanceofsuggestionqualityandtiming,JonBERTa-
8Very recently, JetBrains did release a preview for a cross-language IDE, Fleet:
headperformsbest.Whileourproposedmetricmaintainsordi-
https://www.jetbrains.com/fleet/
9JonBERTa-attn0L:Ranking19/67,Medianaveragescoreis81.7. nalityamongtheCodeBERTaandJonBERTamodels,comparedAIware’24,July15–16,2024,PortodeGalinhas,Brazil deMooretal.
Table 5: Completion statistics for filters deployed in
in future research would likely offer a better understanding of
Code4Me.Therelativeacceptancerateiswithrespectto thesemodels’capabilities,enablingmoremeaningfulcomparisons
None.Latencyiscomputedasthemedianofallrequests. betweendifferentarchitectures.
7.1.2 External:GeneralisabilitytoOtherCode-CompletionTools.
JonBERTa Weutilizedacode-suggestionpluginthathasasmalleruserbase
Filters
None LogReg. Code head attn compared to larger production systems. This choice introduces
Requests severalfactorsthatmightimpacthowourfindingscanbeapplied
toothercodecompletiontools.Thesedifferenceshavebeendetailed
Received 13.5k 10k 20k 15k 13k
inSection3.1.Ourapproach,whilespecific,offersvaluableinsights
Filteredout 2.2% 29.4% 36.6% 34.4% 39.4%
Completions butwarrantscautionwhengeneralizingtoothercontexts.
Shown 97.8% 70.5% 63.2% 65.6% 60.5% 7.1.3 Construct:LimitationsoftheProxyMetrics. Inouronline
Accepted 1.42% 0.62% 1.20% 1.44% 1.03% evaluation,weusetheharmonicmeanofacceptancerateandCode-
Accepted
BERTScoreasourmetrictomeasureperformance.Thisapproach,
Relativerate 100.0% 45.0% 84.5% 101.0% 72.3% suggestedforfurtherexplorationinfuturestudies,allowsforadjust-
CodeBERTScore 0.76 0.94 0.85 0.82 0.88 mentsinhoweachcomponentisweighted.Consistentwithearlier
HarmonicMean 0.864 0.609 0.847 0.905 0.794 research[32,49],weacknowledgethattheseproxymetricsmight
Latency(ms) 0.0 0.2 25.0 20.8 24.1 notfullycapturetheusabilityoftheinteractionwithoutpotentially
compromisingitinsomeotherway.Togainadeeperunderstand-
totheofflineevaluation,thisalsohighlightsourproposedmetric ing,wesuggestthatfutureworkcouldbenefitfromqualitative
isnotperfect,aspresumably,allourfiltersshouldbeperforming studies,includinginterviewswithdevelopers,tocomplementthese
betterthantheno-filterbaseline.Wedeferdiscussiononalternate quantitativemeasures.
weightingsfortheharmonicmeantofuturework(Section7.1.3), 7.1.4 EthicalConsiderations. Ourresearchreceivedapprovalfrom
toavoidconflictinginterestsfromtuningourmetrichere. theinstitutionalethicalboardandexplicituserconsentfordata
Additionally,wedemonstratethefeasibilityofourapproachin use. Additionally, we have secured explicit consent from users
practice.Alightweighttransformermodelcanbedeployedserver- before collecting and using their information. We chose not to
sideasafilterforincomingrequests,withrelativelyminimallatency deeplyinvestigateprivacyissuesrelatedtodeveloperdatausage,
comparedtothecompletionmodelitself,whichtakes300-400msin asourinvocationfilteringmodels(classifiers)posefewerprivacy
ourcase.Futureworkcanconsiderfurtheroptimisingthesemodels, riskscomparedtogenerativemodels.However,tocomplywiththe
throughe.g.,modelcompression[46],forclient-sidedeployment. GDPR,weareunabletoshareourdatasetaswecannotguarantee
anonymity.
7 DISCUSSION
8 CONCLUSIONANDFUTUREWORK
Weanticipatethattheissueofredundantinvocationwillbecome
evenmorenoticeableassoftwareengineeringtasksincreasinglyin- Tosummarise,wetrainatransformer-basedinvocation-filtering
corporatebillion-parametertransformermodels.Ourresultsdemon- modelonadatasetwecollectedfromanopen-sourcecodecom-
stratetheeffectivenessofusingasmaller,lightweighttransformer pletionplugin,Code4Me.Weshowthatcodecontextisespecially
tocontrolwhenalarger,completionmodelisinvoked.Moreover, usefulinfilteringpredictions,andhighlightthepotentialofinte-
webelievesmartinvocationfilteringmodelssuchasoursnotonly gratingthisinformationwiththetelemetrydatacollectedinanIDE.
enhancecodecompletionbutalsoanyothertransformer-based Lastly,wedeployourfiltersinpracticeandshowtheirpractical
interactionwithusers. effectivenessinbothofflineandonlinesettings.
Toourknowledge,wearethefirstworktoaugmentapre-trained Futureworkcanmorethoroughlyexplorethesearchspacewe
transformerwithadditionalfeaturemodalities.Transformermodels haveestablished,byutilisingalargerdataset.Ourlimiteddataset
exhibitexceptionalcontextualunderstanding,yetarebottlenecked maynotfullyrepresentthediversebehavioursofdevelopers,andre-
bythetextualmedium.Especiallyconsideringthatin-apptelemetry latedworkshowspromisingresultsinpersonalisingtheinvocation-
dataisoftencollectedanyway,wehighlightthatitisfruitfulto filteringsystem[5,32].Wechoosenottoexplorethisavenuein
leveragethisadditionalinputdimension.Byshowingpromising thisstudytolimitourarchitecturalsearchspace,thoughstrongly
resultsinthissearchspace,wehopetoinspireotherstoventure advocateforfurtherexplorationinthisarea.
deeper. Lastly,wenotethatdeliveringcompletionsexactlywhenade-
veloperrequeststhemmightnotalwaysmatchwhatdevelopers
7.1 ThreatstoValidity
trulyneedinthelongrun.Trackingthelong-termimpactofthese
7.1.1 Internal:LimitedContextualUsageData. InRQ1andRQ2, completionspresentschallenges,yetunderstandingthisiscrucial,
we used a training dataset of just 10k samples. This size is not especiallyastheuseofAItoolsshowsalinktoincreasedcode
optimalforfullyexaminingthepotentialofhybridtransformer changes.Thisdomaindeservesfurtherinvestigationtobetteralign
modelsenhancedwithextrafeaturedata.Expandingthedataset forlastingdeveloperbenefits.ATransformer-BasedApproachforSmartInvocationofAutomaticCodeCompletion AIware’24,July15–16,2024,PortodeGalinhas,Brazil
REFERENCES
[20] JetbrainsAI2023. JetbrainsAIServiceanIn-IDEAssistant. Online. https:
[1] AmazonCodeWhisperer2023.AICodeGenerator.Online. https://aws.amazon. //www.jetbrains.com/ai/
com/codewhisperer/ [21] DanielD.Johnson,DanielTarlow,andChristianWalder.2023. R-U-SURE?
[2] ShraddhaBarke,MichaelB.James,andNadiaPolikarpova.2022. Grounded Uncertainty-AwareCodeSuggestionsByMaximizingUtilityAcrossRandom
Copilot:HowProgrammersInteractwithCode-GeneratingModels.Proceedings UserIntents.(2023). https://doi.org/10.48550/ARXIV.2303.00732Publisher:arXiv
oftheACMonProgrammingLanguages7,OOPSLA1(Oct.2022),85–111. https: VersionNumber:2.
//doi.org/10.1145/3586030 [22] JonathanKatzy,MalihehIzadi,andArievanDeursen.2023.OntheImpactof
[3] MohammadBavarian,HeewooJun,NikolasTezak,JohnSchulman,Christine LanguageSelectionforTrainingandEvaluatingProgrammingLanguageModels.
McLeavey,JerryTworek,andMarkChen.2022.EfficientTrainingofLanguage http://arxiv.org/abs/2308.13354arXiv:2308.13354[cs].
ModelstoFillintheMiddle. http://arxiv.org/abs/2207.14255arXiv:2207.14255 [23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,
[cs]. CodyHaoYu,JosephE.Gonzalez,HaoZhang,andIonStoica.2023. Efficient
[4] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveira MemoryManagementforLargeLanguageModelServingwithPagedAttention.
Pinto,JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman, http://arxiv.org/abs/2309.06180arXiv:2309.06180[cs].
AlexRay,RaulPuri,GretchenKrueger,MichaelPetrov,HeidyKhlaaf,Girish [24] JennyT.Liang,ChenyangYang,andBradA.Myers.2023. Understanding
Sastry,PamelaMishkin,BrookeChan,ScottGray,NickRyder,MikhailPavlov, theUsabilityofAIProgrammingAssistants. http://arxiv.org/abs/2303.17125
AletheaPower,LukaszKaiser,MohammadBavarian,ClemensWinter,Philippe arXiv:2303.17125[cs].
Tillet,FelipePetroskiSuch,DaveCummings,MatthiasPlappert,FotiosChantzis, [25] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer
ElizabethBarnes,ArielHerbert-Voss,WilliamHebgenGuss,AlexNichol,Alex Levy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.RoBERTa:A
Paino,NikolasTezak,JieTang,IgorBabuschkin,SuchirBalaji,ShantanuJain, RobustlyOptimizedBERTPretrainingApproach. http://arxiv.org/abs/1907.11692
WilliamSaunders,ChristopherHesse,AndrewN.Carr,JanLeike,JoshAchiam, arXiv:1907.11692[cs].
VedantMisra,EvanMorikawa,AlecRadford,MatthewKnight,MilesBrundage, [26] AntonLozhkov,RaymondLi,LoubnaBenAllal,FedericoCassano,JoelLamy-
MiraMurati,KatieMayer,PeterWelinder,BobMcGrew,DarioAmodei,Sam Poirier,NouamaneTazi,AoTang,DmytroPykhtar,JiaweiLiu,YuxiangWei,
McCandlish,IlyaSutskever,andWojciechZaremba.2021.EvaluatingLargeLan- TianyangLiu,MaxTian,DenisKocetkov,ArthurZucker,YounesBelkada,Zijian
guageModelsTrainedonCode. http://arxiv.org/abs/2107.03374arXiv:2107.03374 Wang,QianLiu,DmitryAbulkhanov,IndraneilPaul,ZhuangLi,Wen-Ding
[cs]. Li,MeganRisdal,JiaLi,JianZhu,TerryYueZhuo,EvgeniiZheltonozhskii,
[5] MiaXuChen,BenjaminN.Lee,GaganBansal,YuanCao,ShuyuanZhang,Justin NiiOsaeOsaeDade,WenhaoYu,LucasKrauß,NamanJain,YixuanSu,Xu-
Lu,JackieTsay,YinanWang,AndrewM.Dai,ZhifengChen,TimothySohn, anliHe,MananDey,EdoardoAbati,YekunChai,NiklasMuennighoff,Xiangru
andYonghuiWu.2019. GmailSmartCompose:Real-TimeAssistedWriting. Tang,MuhtashamOblokulov,ChristopherAkiki,MarcMarone,ChenghaoMou,
http://arxiv.org/abs/1906.00080arXiv:1906.00080[cs]. MayankMishra,AlexGu,BinyuanHui,TriDao,ArmelZebaze,OlivierDe-
[6] AndrewAChien,LiuzixuanLin,HaiNguyen,VarshaRao,TristanSharma,and haene,NicolasPatry,CanwenXu,JulianMcAuley,HanHu,TorstenScholak,
RajiniWijayawardana.2023. ReducingtheCarbonImpactofGenerativeAI SebastienPaquet,JenniferRobinson,CarolynJaneAnderson,NicolasChapa-
Inference(todayandin2035).InProceedingsofthe2ndWorkshoponSustainable dos,MostofaPatwary,NimaTajbakhsh,YacineJernite,CarlosMuñozFerrandis,
ComputerSystems.1–7. LingmingZhang,SeanHughes,ThomasWolf,ArjunGuha,LeandrovonWerra,
[7] Codeium2023.Codeium-FreeAICodeCompletions.Online. https://codeium. andHarmdeVries.2024.StarCoder2andTheStackv2:TheNextGeneration.
com/ http://arxiv.org/abs/2402.19173arXiv:2402.19173[cs].
[8] MikhailEvtikhiev,EgorBogomolov,YaroslavSokolov,andTimofeyBryksin. [27] ShuaiLu,NanDuan,HojaeHan,DayaGuo,Seung-wonHwang,andAlexeySvy-
2023.OutoftheBLEU:howshouldweassessqualityoftheCodeGeneration atkovskiy.2022.ReACC:ARetrieval-AugmentedCodeCompletionFramework.
models?JournalofSystemsandSoftware203(Sept.2023),111741. https://doi. http://arxiv.org/abs/2203.07722arXiv:2203.07722[cs].
org/10.1016/j.jss.2023.111741arXiv:2208.03133[cs]. [28] ShuaiLu,DayaGuo,ShuoRen,JunjieHuang,AlexeySvyatkovskiy,Ambrosio
[9] MatthiasFeurer,KatharinaEggensperger,StefanFalkner,MariusLindauer,and Blanco,ColinClement,DawnDrain,DaxinJiang,DuyuTang,GeLi,LidongZhou,
FrankHutter.2020.Auto-Sklearn2.0:Hands-freeAutoMLviaMeta-Learning. LinjunShou,LongZhou,MicheleTufano,MingGong,MingZhou,NanDuan,
arXiv:2007.04074[cs.LG](2020). NeelSundaresan,ShaoKunDeng,ShengyuFu,andShujieLiu.2021.CodeXGLUE:
[10] MatthiasFeurer,AaronKlein,KatharinaEggensperger,JostSpringenberg,Manuel AMachineLearningBenchmarkDatasetforCodeUnderstandingandGeneration.
Blum,andFrankHutter.2015.EfficientandRobustAutomatedMachineLearning. http://arxiv.org/abs/2102.04664arXiv:2102.04664[cs].
InAdvancesinNeuralInformationProcessingSystems28(2015).2962–2970. [29] BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,Bowen
[11] DanielFried,ArmenAghajanyan,JessyLin,SidaWang,EricWallace,FredaShi, Zhang,PhilippDufter,DhrutiShah,XianzhiDu,FutangPeng,FlorisWeers,
RuiqiZhong,Wen-tauYih,LukeZettlemoyer,andMikeLewis.2023.InCoder:A AntonBelyi,HaotianZhang,KaranjeetSingh,DougKang,AnkurJain,Hongyu
GenerativeModelforCodeInfillingandSynthesis. http://arxiv.org/abs/2204. Hè,MaxSchwarzer,TomGunter,XiangKong,AonanZhang,JianyuWang,Chong
05999arXiv:2204.05999[cs]. Wang,NanDu,TaoLei,SamWiseman,MarkLee,ZiruiWang,RuomingPang,
[12] GeminiCodeAssist2023.GeminCodeAssist.Online. https://cloud.google.com/ PeterGrasch,AlexanderToshev,andYinfeiYang.2024.MM1:Methods,Analysis
products/gemini/code-assist &InsightsfromMultimodalLLMPre-training. http://arxiv.org/abs/2403.09611
[13] GitHubCopilot2021.GitHubCopilot:YourAIPairProgrammer.Online. https: arXiv:2403.09611[cs].
//github.com/features/copilot [30] AaronMok.2024.EstimatedCostofChatGPT. https://www.businessinsider.
[14] DayaGuo,ShuaiLu,NanDuan,YanlinWang,MingZhou,andJianYin.2022. com/how-much-chatgpt-costs-openai-to-run-estimate-report-2023-
UniXcoder:UnifiedCross-ModalPre-trainingforCodeRepresentation. http: 4?international=true&r=US&IR=T
//arxiv.org/abs/2203.03850arXiv:2203.03850[cs]. [31] HusseinMozannar,GaganBansal,AdamFourney,andEricHorvitz.2023.Reading
[15] DayaGuo,QihaoZhu,DejianYang,ZhendaXie,KaiDong,WentaoZhang,Guant- BetweentheLines:ModelingUserBehaviorandCostsinAI-AssistedProgram-
ingChen,XiaoBi,Y.Wu,Y.K.Li,FuliLuo,YingfeiXiong,andWenfengLiang. ming. http://arxiv.org/abs/2210.14306arXiv:2210.14306[cs].
2024.DeepSeek-Coder:WhentheLargeLanguageModelMeetsProgramming– [32] HusseinMozannar,GaganBansal,AdamFourney,andEricHorvitz.2023.When
TheRiseofCodeIntelligence. http://arxiv.org/abs/2401.14196arXiv:2401.14196 toShowaSuggestion?IntegratingHumanFeedbackinAI-AssistedProgramming.
[cs]. http://arxiv.org/abs/2306.04930arXiv:2306.04930[cs].
[16] William Harding and Matthew Kloster. 2024. Coding on Copilot: 2023 [33] SidaPeng,EiriniKalliamvakou,PeterCihon,andMertDemirer.2023. The
DataShowsDownwardPressureonCodeQuality. Whitepaper.GitClear.24 ImpactofAIonDeveloperProductivity:EvidencefromGitHubCopilot. http:
pages. https://gitclear-public.s3.us-west-2.amazonaws.com/Coding-on-Copilot- //arxiv.org/abs/2302.06590arXiv:2302.06590[cs].
2024-Developer-Research.pdf [34] JamesPrather,BrentN.Reeves,PaulDenny,BrettA.Becker,JuhoLeinonen,
[17] MinyoungHuh,HosseinMobahi,RichardZhang,BrianCheung,PulkitAgrawal, AndrewLuxton-Reilly,GarrettPowell,JamesFinnie-Ansley,andEddieAntonio
andPhillipIsola.2023.TheLow-RankSimplicityBiasinDeepNetworks. http: Santos.2023."It’sWeirdThatitKnowsWhatIWant":UsabilityandInteractions
//arxiv.org/abs/2103.10427arXiv:2103.10427[cs]. withCopilotforNoviceProgrammers.(April2023). https://doi.org/10.48550/
[18] MalihehIzadi,RobertaGismondi,andGeorgiosGousios.2022.CodeFill:Multi- ARXIV.2304.02491Publisher:arXivVersionNumber:1.
tokenCodeCompletionbyJointlyLearningfromStructureandNamingSe- [35] BaptisteRozière,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,Xi-
quences.InProceedingsofthe44thInternationalConferenceonSoftwareEngi- aoqingEllenTan,YossiAdi,JingyuLiu,TalRemez,JérémyRapin,Artyom
neering.401–412. https://doi.org/10.1145/3510003.3510172 arXiv:2202.06689 Kozhevnikov,IvanEvtimov,JoannaBitton,ManishBhatt,CristianCantonFer-
[cs]. rer,AaronGrattafiori,WenhanXiong,AlexandreDéfossez,JadeCopet,Faisal
[19] MalihehIzadi,JonathanKatzy,TimvanDam,MarcOtten,RazvanMihaiPopescu, Azhar,HugoTouvron,LouisMartin,NicolasUsunier,ThomasScialom,and
andArievanDeursen.2024.LanguageModelsforCodeCompletion:APracti- Gabriel Synnaeve. 2023. Code Llama: Open FoundationModels for Code.
calEvaluation.In46thInternationalConferenceonSoftwareEngineering(ICSE). http://arxiv.org/abs/2308.12950arXiv:2308.12950[cs].
ACM/IEEE. http://arxiv.org/abs/2402.16197arXiv:2402.16197[cs]. [36] DanielRusso.2023.NavigatingtheComplexityofGenerativeAIAdoptionin
SoftwareEngineering. http://arxiv.org/abs/2307.06081arXiv:2307.06081[cs].AIware’24,July15–16,2024,PortodeGalinhas,Brazil deMooretal.
[37] SourceGraph Cody 2023. Cody - AI Coding Assistant. Online. https:
//sourcegraph.com/cody
[38] Stack Overflow. 2023. Stack Overflow Developer Survey 2023.
https://survey.stackoverflow.co/2023/#section-developer-tools-ai-in-the-
development-workflow
[39] ZhensuSun,XiaoningDu,FuSong,ShangwenWang,MingzeNi,andLiLi.
2023. Don’tCompleteIt!PreventingUnhelpfulCodeCompletionforProduc-
tiveandSustainableNeuralCodeCompletionSystems.In2023IEEE/ACM45th
InternationalConferenceonSoftwareEngineering:CompanionProceedings(ICSE-
Companion).IEEE,Melbourne,Australia,324–325. https://doi.org/10.1109/ICSE-
Companion58688.2023.00089
[40] Tabnine2023.TabnineAICodingAssistant.Online. https://www.tabnine.com/
[41] ParthThakkar.2023.CopilotInternals.https://thakkarparth007.github.io/copilot-
explorer/posts/copilot-internalsPublicationTitle:Copilot-Explorer.
[42] PriyanVaithilingam,TianyiZhang,andElenaL.Glassman.2022.Expectationvs.
Experience:EvaluatingtheUsabilityofCodeGenerationToolsPoweredbyLarge
LanguageModels.InCHIConferenceonHumanFactorsinComputingSystems
ExtendedAbstracts.ACM,NewOrleansLAUSA,1–7. https://doi.org/10.1145/
3491101.3519665
[43] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
AidanN.Gomez,LukaszKaiser,andIlliaPolosukhin.2023. AttentionIsAll
YouNeed. http://arxiv.org/abs/1706.03762arXiv:1706.03762[cs].
[44] RuotongWang,RuijiaCheng,DenaeFord,andThomasZimmermann.2023.
InvestigatingandDesigningforTrustinAI-poweredCodeGenerationTools.
http://arxiv.org/abs/2305.11248arXiv:2305.11248[cs].
[45] YueWang,HungLe,AkhileshDeepakGotmare,NghiD.Q.Bui,JunnanLi,and
StevenC.H.Hoi.2023.CodeT5+:OpenCodeLargeLanguageModelsforCode
UnderstandingandGeneration. http://arxiv.org/abs/2305.07922arXiv:2305.07922
[cs].
[46] XiaoxiaWu,ZheweiYao,MinjiaZhang,ConglongLi,andYuxiongHe.2022.
ExtremeCompressionforPre-trainedTransformersMadeSimpleandEfficient.
https://doi.org/10.48550/arXiv.2206.01859arXiv:2206.01859[cs].
[47] ShuyanZhou,UriAlon,SumitAgarwal,andGrahamNeubig.2023. Code-
BERTScore:EvaluatingCodeGenerationwithPretrainedModelsofCode. http:
//arxiv.org/abs/2302.05527arXiv:2302.05527[cs].
[48] TerryYueZhuo.2024.ICE-Score:InstructingLargeLanguageModelstoEvaluate
Code. http://arxiv.org/abs/2304.14317arXiv:2304.14317[cs].
[49] AlbertZiegler,EiriniKalliamvakou,ShawnSimister,GaneshSittampalam,Al-
iceLi,AndrewRice,DevonRifkin,andEdwardAftandilian.2022. Productiv-
ityAssessmentofNeuralCodeCompletion. http://arxiv.org/abs/2205.06537
arXiv:2205.06537[cs].ATransformer-BasedApproachforSmartInvocationofAutomaticCodeCompletion AIware’24,July15–16,2024,PortodeGalinhas,Brazil
A FEATURESUSEDINCOPILOT’SFILTER
PrecedingCharacter
Basedon[41],weinvestigatedthev1.57.7193(June2022)version PrecedingCharacter (WhitespaceTrimmed)
ofCopilotandlistthefeaturesusedintheirlogisticclassifierin !
Table 6. The weight is the coefficient in the logistic regression #”
$
model,andthescalingindicatesthetransformationappliedbefore %
&
multiplyingwiththeweight.Thelanguageandcharactermaps(last
0
(
threefeatures)areaone-hotencodingofafixedsetoflanguages )
andcharacters,giveninFigure4andFigure5respectively. +∗
,
Beforeapromptreachesthefilter,twohard-codedrulesprevent
−.
(1)promptscontainingfewerthan10characters,and(2)prompts /
0
wherethecursorisinthemiddleofaline,bycheckingwhether 1
2
thereiswhitespaceafterthecursor;exceptifthereisaclosing 3
4
characteronthatline,suchasaclosingbracket,quote,orsemicolon. 5
6
7
8
9
Table6:FeaturesUsedinCopilot’sFilter. ;
<
=
>
?
Feature Weight InputScaling @
A
Previousfilterlabel 0.997 none CB
Whitespaceaftercursor 0.700 none D E
Timesincelastlabel −0.174 log GF
Lengthoflastprefixline −0.230 log H I
J
Above,withoutwhitespace 0.134 log K
L
Documentlength −0.007 log M
N
Cursoroffset 0.005 log O
P
Offsetaspercentage 0.419 none Q
R
Documentlanguage(map) [−0.654,0.358] none S
T
Lastprefixchar(map) [−1.56,1.15] none U
V
Above,withoutwhitespace [−1.12,0.85] none W
X
Y
Z
[
}]
ˆ
`
a
c b
c
php d
cpp e f
java g
h
go i
j
csharp k
l
javascript m
n
javascriptreact o
p
python q
r
html s
vue ut
ruby v
w
typescript x
y
rust z
typescriptreact —{
dart }
json
css
scss 1.5 1.0 0.5 0.0 0.5 1.0
− − −
markdown
Figure 5: Copilot’s Prefix Character Map. Higher-scoring
0.6 0.4 0.2 0.0 0.2 0.4
− − − characters(directlybeforethecursor)aremorelikelytoget
acompletion.
Figure4:Copilot’sLanguageMap.Higher-scoringlanguages
aremorelikelytogetacompletion.AIware’24,July15–16,2024,PortodeGalinhas,Brazil deMooretal.
ThelanguagesinFigure4showthatmoreverboselanguageslike biasedtounbalanced(real-worlddistribution)tohelpthemodel
CandJavaarebiasedtoreceivemorecompletions,whiledesign- learnclassesquickly,andthengeneralisetodatamorediversein
orientedlanguageslikeCSS,SCSS,andTypeScript-Reactareless textualcontent.
likelytoreceivecompletions.Notably,Markdownisleastlikelyto
receivecompletions,likelybecausetheLLMsthatgeneratecom-
pletionsareprimarilytrainedoncode;and,itmaybeespecially
annoyingtodeveloperstoreceiveincorrectcompletionswhenthey
areinanatural-languageflow.
ThecharactersinFigure5revealsomespecificusagepatternstoo.
Manylettersfallcloseto0,indicatingthattheydonothavemuch
influenceonthefiltering;withtheexceptionof‘r’,maybeduetothe
commonprint-statementcompletionswhichtendtobeeasiertoin-
fer.Furthermore,ifthedeveloperiscurrentlytypinganumber,this
hasanegativeweight.Presumably,programmersdon’twantcom-
pletionswhiletheyarewritingsomeuninferrablenumber.Lastly,
perhapssurprisingly,thefull-stop‘.’hasnear-zeroweight,while
acomma‘,’followedbywhitespacehaspositiveweight,implying
developersmayrelyonCopilottocompletelist/objectstructures.
B COMPARISONOFTOKENISATION
STRATEGIES
Toinvestigatetheeffectivenessofourjointprefix-and-suffixtokeni-
sationstrategy,wecompareittothetwoalternatives:prefix-only
andsuffix-onlytokenisation.Tothisend,wetrainRoBERTamodels
fromthehuggingface/CodeBERTa-small-v1checkpoint10.We
usethesamehyperparametersforallmodels:alearningrateof
2×10−5,abatchsizeof8,andtrainfor3epochs.
Figure6:DatasetDistributions
Table7:ComparisonoftokenisationStrategies.
Table8:CodeBERTaModelsunderDifferentTrainingData
Tokenisation Manual Auto/acc. Auto/rej. Average Distributions.
Suffix-only 89.3 93.0 24.1 68.8
Prefix-only 97.6 61.0 74.7 77.8 Manual Auto/acc. Auto/rej. Average
Joint 98.6 73.5 71.7 81.3
Unbalanced 98.9 19.4 98.1 72.1
Classes 99.2 32.2 95.7 75.7
C THEEFFECTOFDATASETDISTRIBUTION Sub-Classes 98.2 71.6 74.4 81.3
ONMODELPERFORMANCE Biased 98.6 78.9 67.3 81.6
Ourcodecompletiondatasetdistributiondoesnotequallyrepre-
senttheinvocationtypeswewanttoclassify,yettheyshouldbe
Table 9: Logistic Regression Models (with full telemetry),
weighedroughlyequally.Toremedythis,weexperimentwithsev-
underDifferentTrainingDataDistributions.
eraldatadistributions(Figure6)fortrainingourmodels.Asshown
inTable8forCodeBERTa,andTable9fortheLogisticRegression
model,findthatthebiaseddistribution,givingroughlyequalweight Manual Auto/acc. Auto/rej. Average
toallsub-classesandundersamplingtobetterrepresentmanual,
Unbalanced 97.4 0.0 99.8 65.8
acceptedinvocations,yieldsthebestmacroaverageperformance
Classes 97.8 0.0 99.7 65.8
acrossthesub-classes.Weobserveasimilarpatternforallmodels
Sub-Classes 97.8 35.7 88.4 74.0
detailedinAppendixF.Thus,weusethebiaseddistributionasour Biased 98.5 66.1 65.0 76.5
training/validationdataset.
ForCodeBERTainTable8,thesuperiorperformanceonthe
biasedbalancingmayalsobeduetoourtraininghyperparameters.
Wefine-tunefor8epochs(asopposedto6intherestofthispaper),
andourmodelsarepronetooverfittingduetothesmalltrainingset
size.Futureworkcouldperhapsstudyadatacurriculum,goingfrom
10https://huggingface.co/huggingface/CodeBERTa-small-v1ATransformer-BasedApproachforSmartInvocationofAutomaticCodeCompletion AIware’24,July15–16,2024,PortodeGalinhas,Brazil
D JONBERTA-HEADARCHITECTURE Table 11: Performance of JonBERTa-attn Variants ini-
EXPERIMENTS tialisedfromRoBERTa-small-v1.
Welistthe3jonberta-headvariantsbelow,withthe2options
forwhethertoreinitialisethemoduleornot.Wehypothesisethat Layers Manual Auto/acc Auto/rej Avg.
thedensevariantperformsbestbecausedenselayersallowthe 1,4,5 98.4±0.4 75.9±6.4 69.9±5.8 81.4
modeltolearnlow-rankembeddings,andspeeduptrainingatthis 2,4,5 98.4±0.5 77.1±6.9 68.5±6.4 81.3
scale.Forcomparison,weprovidethebaselineCodeBERTamodel 0,4,5 98.2±0.7 76.4±8.0 69.1±5.0 81.3
trainedoncode-contextalone. 0,1,4 98.3±0.4 77.3±5.8 68.1±2.7 81.2
0,1,5 98.3±0.4 77.3±5.0 68.1±2.8 81.2
0,2,4 98.3±0.7 77.5±7.1 67.9±5.6 81.2
Table10:JonBERTa-headVariants.Rdenotesre-initialised
1,2,3 98.2±0.5 77.6±5.0 67.9±2.1 81.2
modules.
0,1,3 98.4±0.4 78.0±6.1 67.2±6.1 81.2
3,4,5 98.4±0.5 76.3±6.2 68.7±3.3 81.1
Manual Auto/acc Auto/rej Avg. 0,2,5 98.3±0.4 77.4±5.8 67.6±3.1 81.1
dense-R 98.6±0.4 78.0±6.2 71.4±5.1 82.7 1,2,4 98.4±0.5 77.3±7.0 67.6±5.0 81.1
0,1,2 98.5±0.6 80.6±5.2 64.1±6.2 81.1
proj-R 98.6±0.6 76.1±11.0 72.4±3.6 82.3
0,3,5 98.3±0.5 76.0±7.5 68.7±6.4 81.0
dense 98.6±0.5 75.6±6.8 72.6±4.4 82.3
2,3,4 98.3±0.5 76.2±6.7 68.4±8.2 81.0
dense-proj- 98.6±0.3 73.5±6.7 74.1±1.8 82.1
2 98.0±0.6 77.1±5.7 67.4±4.0 80.8
dense-proj-R 98.5±0.5 72.2±5.8 75.4±1.9 82.0
2,3,5 98.3±0.5 77.5±7.2 66.6±5.2 80.8
proj 98.4±0.5 72.9±7.0 74.7±5.0 82.0
0,3,4 98.4±0.5 76.6±5.9 67.2±4.3 80.7
CodeBERTa 98.5±0.4 74.7±4.6 73.1±1.2 82.1 0,2,3 98.3±0.7 74.9±10.6 68.8±5.9 80.7
1,2,5 98.3±0.3 76.8±6.3 66.8±6.1 80.6
1 98.2±0.5 76.6±7.7 66.9±3.4 80.5
0 98.1±0.7 76.5±10.0 66.8±5.8 80.5
1,3,5 98.2±0.6 74.7±10.2 68.3±7.2 80.4
E JONBERTA-ATTNARCHITECTURE
3 98.1±0.8 77.5±7.8 65.3±5.7 80.3
EXPERIMENTS 1,3,4 98.3±0.6 74.1±7.6 68.5±6.6 80.3
Itmayseemnaturaltoconsidertelemetrydataassimplyanother 4 98.2±0.4 77.7±6.4 64.5±5.3 80.1
modalitythatcanbefedtothetransformerakintomultimodal 5 98.2±0.5 78.7±8.2 63.4±5.9 80.1
largelanguagemodels[29].Wechoosenottoexplorethisapproach CodeBERTa 98.5±0.4 74.7±4.6 73.1±1.2 82.1
asitintroducestoomanynewparametersforthemodeltolearn
inourlimited-datasetting.Furthermore,asfarasweknow,there
arenostudiesinvestigatingwhetheranexistingnatural-language
transformercanbeextendedwithadditionalmodalitieswithout
reinitialising(all)itsweights.Tothisend,wetrylearninganem-
beddingforeachtelemetryfeatureasafunctionofthatfeature,and
extendtheattentionmoduletoattendtotheseembeddingsaswell.
AsshowninTable11,wetrainavarietyoflayerconfigurations,
fromindividuallayerstotripletsoflayers.Weinitialisethemodel
fromCodeBERTa-small-v1,providedbyHuggingFace11,andfine-
tunefor6epochsonourcollectedcode-completiondataset.We
notethatthefinalperformanceofallmodelsislowerthanthatof
CodeBERTa,andhypothesisethismaybeduetothedualtraining
objective;i.e.wearefine-tuningbothasacode-contextclassifica-
tiontask,andlearningtelemetryembeddingssimultaneously.
Tofurtherinvestigate,wefirstfine-tunetheCodeBERTamodel
oncode-completionclassificationforonly3epochs,andthenfine-
tuneanadditional3epochswiththeadditionaltelemetryfeatures.
TheresultsareshowninTable12,wheretheworst-performing
modelattainsclassificationaccuracyonparwiththebest-performing
modelfromTable11,indicatingthatbetterperformanceisachiev-
ableinthistrainingparadigm.
11https://huggingface.co/huggingface/CodeBERTa-small-v1AIware’24,July15–16,2024,PortodeGalinhas,Brazil deMooretal.
Table12:JonBERTa-attnvariantsinitialisedfromourfine-
ThealternativemodelsareshowninTable13.Wecompareby
tunedCodeBERTa. encoding𝑛 prefix and𝑛 suffix words/tokens/lines surrounding
thecursor.Itisnotfullyfairtocompareeverymethodwiththis
Layers Manual Auto/acc Auto/rej Avg. granularity,assomemethodsmayperformbetterwithe.g.3prefix
andtheentiresuffixencoded,butwedosoforconsistency.Ifwe
4,5 98.6±0.5 76.6±5.7 72.1±4.2 82.4
hadanyremarkableresults,thiswouldnotbeintheappendixin
2,3,4 98.5±0.4 74.8±5.9 73.8±2.4 82.4
thefirstplace.
0,1,4 98.5±0.4 75.2±7.6 73.1±4.1 82.3
AutosklearnandAutoSklearn2areensemblesconsistingof22
0,1,3 98.5±0.4 74.8±4.6 73.4±2.0 82.3
and3modelsrespectively.Weomitthesemodelsfromthebodyof
2,5 98.5±0.4 74.9±4.6 73.2±3.0 82.2
ourstudyasweprefersimplersolutionsovercomplexones.We
0,5 98.6±0.3 74.6±7.5 73.3±6.1 82.2
furtherwanttohighlightthepotentialofintegratingpre-trained
1,2,5 98.6±0.3 75.5±7.2 72.4±2.7 82.2
transformers’semanticalunderstandingandcontextualcapabilities
2 98.6±0.4 75.8±5.9 71.9±3.1 82.1
withadditionalmodalitiesliketelemetryfeaturedata.
3,4 98.6±0.4 75.7±5.3 72.0±2.5 82.1
3,4,5 98.4±0.4 74.4±5.0 73.5±2.3 82.1
Table13:FilteraccuracyforLogisticRegressionclassifica-
3,5 98.5±0.3 74.2±6.3 73.6±2.7 82.1
tionmodelswithdifferentcode-contextencodings,givenper
0,4,5 98.5±0.4 75.0±4.9 72.8±2.3 82.1
invocationsub-class.Macrodenotesthemacroaverageof
0,2 98.7±0.3 77.5±4.6 70.1±1.9 82.1
thethreesub-classes.
0,3,5 98.6±0.4 75.4±6.2 72.3±2.7 82.1
2,3,5 98.4±0.4 72.4±6.2 75.4±3.4 82.1
0,2,5 98.5±0.4 74.1±4.3 73.5±1.5 82.1 LogisticRegr.w/ Manual Auto/acc. Auto/rej. Macro
0,1 98.7±0.4 75.4±5.6 72.1±2.1 82.0 One-HotTokens(dim=50821)
0,3 98.7±0.3 76.7±5.0 70.7±3.0 82.0
3Tokens 99.0 77.7 54.0 77.1
0 98.6±0.4 75.0±6.8 72.5±3.5 82.0
2Tokens 99.1 77.5 53.0 76.7
1,2,3 98.6±0.4 74.4±4.4 73.1±1.3 82.0
1Token 99.1 75.2 50.7 73.9
1,2,4 98.5±0.4 73.1±7.8 74.4±2.9 82.0
1,5 98.6±0.6 75.3±9.1 72.1±4.3 82.0 Tok2Vec(dim=100)
1,4 98.6±0.5 75.8±7.1 71.6±3.0 82.0
3tokens 97.7 74.0 58.8 76.0
0,1,2 98.6±0.5 74.3±6.5 73.1±6.0 82.0
2tokens 98.3 75.4 56.2 76.3
0,2,4 98.5±0.4 73.6±4.6 73.7±3.5 81.9
1token 98.5 76.3 52.7 75.5
0,2,3 98.5±0.5 74.3±5.4 73.0±5.1 81.9
0,4 98.5±0.4 75.0±5.3 72.2±1.4 81.9 Word2Vec(dim=100)
0,3,4 98.6±0.4 76.6±7.9 70.5±4.2 81.9
3words 74.9 60.3 58.2 63.7
1,2 98.6±0.4 74.3±8.4 72.6±4.2 81.9
2words 77.5 60.6 56.3 64.2
1,3,4 98.5±0.4 74.6±6.8 72.6±3.4 81.9
1words 85.1 63.3 51.7 66.7
0,1,5 98.6±0.5 76.4±10.1 70.6±4.9 81.9
1,4,5 98.6±0.4 73.7±6.5 73.1±2.2 81.8 SetFit(dim=768)
1 98.5±0.4 73.8±7.3 73.0±7.5 81.8
1line 78.6 58.3 66.9 70.7
2,3 98.5±0.5 72.4±7.6 74.4±2.9 81.8
2lines 67.4 57.6 62.1 61.4
1,3,5 98.6±0.5 73.0±8.1 73.7±4.8 81.7
3lines 61.3 57.3 60.8 59.9
2,4,5 98.5±0.5 73.9±5.5 72.8±4.9 81.7
4 98.5±0.5 75.5±9.9 71.0±4.4 81.7 TF.IDF(dim∈ [15000,18500])
2,4 98.5±0.4 73.7±6.5 72.9±6.7 81.7
3tokens 93.0 84.5 41.6 72.9
3 98.5±0.3 75.0±8.6 71.4±6.5 81.6
2tokens 95.0 84.0 41.4 72.4
5 98.4±0.5 74.6±6.9 71.8±5.6 81.6
1token 97.0 80.5 38.4 69.4
1,3 98.5±0.6 73.8±7.3 71.9±5.8 81.4
1line 89.6 91.9 24.0 69.0
AutoSklearn[10] 98.9 75.9 64.6 79.4
F ALTERNATIVEMODELS
AutoSklearn2[9] 98.7 73.5 65.7 78.9
Wehaveexhaustivelyexploredalternativeapproachestoourcom-
pletionclassificationproblem,whichhaveguidedandmotivated
ForourOne-HotTokensandTok2Vecclassifiers,weusethe
our transformer-based approach. To this end, we train logistic-
TikTokenp50k-basetokeniserfromOpenAI’sCodexmodels12.
regressionmodelsontelemetrydata,andextendthemwithvarious
ForSetFit,weexperimentwiththreeembeddingmodels,andonly
methodsofincorporatingcodecontext;aswellasAutoMLmodels
reportall-mpnet-base-v213hereasitmarginallyoutperforms.
[9,10].Ourmainfinding,asstatedinthepaper,isthatcodecon-
textissignificantlymoreimportantforclassificationaccuracythan 12https://github.com/openai/tiktoken
whatcanberetrievedfromtelemetrydataalone. 13https://huggingface.co/sentence-transformers/all-mpnet-base-v2