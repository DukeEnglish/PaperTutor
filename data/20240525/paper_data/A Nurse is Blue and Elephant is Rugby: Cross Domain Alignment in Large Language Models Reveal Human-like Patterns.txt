A Nurse is Blue and Elephant is Rugby:
Cross Domain Alignment in Large Language Models Reveal Human-like Patterns
AsafYehudai∗ TaelinKaridi∗ GabrielStanovsky ArielGoldstein OmriAbend
HebrewUniversityofJerusalem
{asaf.yehudai, taelin.karidi, gabriel.stanovsky, ariel.goldstien, omri.abend}@mail.huji.ac.il
Abstract
Cross-domainalignmentreferstothetaskofmappingacon-
cept from one domain to another. For example, “If a doctor
wereacolor,whatcolorwoulditbe?”. Thisseeminglypecu-
liartaskisdesignedtoinvestigatehowpeoplerepresentcon-
crete and abstract concepts through their mappings between
categoriesandtheirreasoningprocessesoverthosemappings.
Inthispaper,weadaptthistaskfromcognitivesciencetoeval-
uatetheconceptualizationandreasoningabilitiesoflargelan-
guagemodels(LLMs)throughabehavioralstudy.Weexamine
several LLMs by prompting them with a cross-domain map-
pingtaskandanalyzingtheirresponsesatboththepopulation
andindividuallevels.Additionally,weassessthemodels’abil-
itytoreasonabouttheirpredictionsbyanalyzingandcatego-
rizingtheirexplanationsforthesemappings.Theresultsreveal
several similarities between humans’ and models’ mappings
and explanations, suggesting that models represent concepts
similarlytohumans. Thissimilarityisevidentnotonlyinthe
modelrepresentationbutalsointheirbehavior. Furthermore,
themodelsmostlyprovidevalidexplanationsanddeployrea- Figure 1: Cross-Domain Alignment: Mapping object from
soningpathsthataresimilartothoseofhumans.
domainAtodomainB.Here,doctorfromtheprofessiondo-
maintothecolordomain,andpianofromtheinstrumentdo-
Introduction
maintotheanimaldomain.
LargeLanguageModels(LLMs)havesignificantlyimproved
their ability to generate human-like text and tackle complex Understanding the basis of these mappings offers insights
tasks that require reasoning. However, the ability to explain intotheparticipants’conceptualrepresentationandorganiza-
orpresenttheirbehaviorinhuman-understandabletermshas tion,similartohowpsychologistsusementalassociationsor
remained a challenge (Doshi-Velez & Kim, 2017; Du et al., nonsensicalteststoassesshumanconceptualrepresentations
2021;Zhaoetal.,2023). Theabilitytointeractwiththemin (Greenwald,McGhee,&Schwartz,1998;Davis,Morrow,&
averysimilarwaytohumans,hasencouragedresearchersto Lupyan, 2019). We further investigate the reasoning behind
evaluatetheirunderstandingandreasoningabilitiesbycom- their mappings by analyzing their explanations of the map-
paringtheirbehaviortohumans(Futrelletal.,2019;Binz& pingresponses,makingthemmoreinterpretable.
Schulz, 2023b), drawing insights from fields such as cogni-
Our experiments are divided into two main parts: (1)
tive psychology, psycholinguistics, and neuroscience (Huth,
Cross-domain mapping task, and (2) Explanations of the
De Heer, Griffiths, Theunissen, & Gallant, 2016; Pereira et
mappings. We first ask whether LLMs can perform cross-
al.,2018;Futrelletal.,2019;Goldsteinetal.,2022).
domainmappingsandifso, whethertheyconvergewithhu-
In this paper, we perform a behavioral study on LLMs,
manbehavior. Toanswerthesequestions,weusehumandata
drawing inspiration from a recently created psychological
collected as part of cognitive experiments, to prompt sev-
task that has claimed to uncover aspects of how people rep-
eral LLMs with cross-domain mappings. Surprisingly, re-
resentconcretevs. abstractconcepts,andtheconceptualor-
sults show that models can perform such mappings, reach-
ganizationofmetaphoriclanguage(Q.Liu&Lupyan,2023,
ing a substantial agreement at the population level, which is
henceforthLL23). Inthistask,participantsareaskedtomap
much higher than a random chance guess. Moreover, some
concepts from one semantic domain to another (e.g., doc-
LLMssurpasstheindividuallevelagreementwiththepopu-
tor to color, and piano to animal, see Fig. 1) and explain
lationlevel(i.e. mostpopular)mappings, showingthattheir
their choices. Interestingly, it was shown that individuals
behavior is closer to the “typical” human behavior than that
perform these seemingly arbitrary mappings in predictable
ofarandomparticipant.
ways, relying on certain types of similarity such as percep-
To further interpret the models’ mappings and their abil-
tual similarity or word associations. For example, drum
ity to reason about them, we prompt the models to explain
was consistently mapped to thunder, clearly motivated by
the mappings. We use the predefined similarity categories
their sensory similarity, as they both make a similar noise.
(e.g., perceptual similarity) that were found to establish the
*Equalcontribution. basis of alignment for humans and train a classifier to clas-
4202
yaM
32
]LC.sc[
1v36841.5042:viXrasifythemodels’explanationsaccordingtothem. Weseethat iments (full details are in the Appendix). The data contains
the models’ explanation categories are distributed in a very 12 domains1 (see Table 2 in the Appendix), from which 32
similarwaytohumans,suggestingtheyrelyonsimilartypes domain-pairs were selected. For each domain pair, 2-3 ran-
of similarity in their representations. Moreover, we perform domstatementsoftheform:“Ifa(n)x(sourceitem)werea(n)
aqualitativeanalysisoftheirexplanations,showingtheycan y (target domain), what y would it be?” (e.g., “If a doctor
giveconciseargumentsforthecross-domainmappings. Our (sourceitem)wereacolor(targetdomain),whatcolorwould
findings can contribute to the recent discussion in the NLP it be?”) are constructed. Resulting in 75 statements, each
andcognitiveliteratureastowhetherwecanascribeconcepts answeredby20participants.
suchasconceptualizationtoLLMs,bydemonstratingthatat
leastatthebehaviorallevel,alignmentcanbefoundbetween
Models. We select seven robust LLMs, including variants
LLMsandhumansinconceptualizationtests(Bubecketal.,
of Flan language models (Wei et al., 2022) and Llama-
2023;Kosinski,2023;Binz&Schulz,2023a).
chat language models (Touvron et al., 2023) and Mistral-
7B2 (See Appendix for full details). These models, termed
Background
“instruction-following LLMs,” belong to a category of lan-
In recent years, the use of methods and experiments from
guagemodelsspecificallytrainedtofollowinstructions–an
the cognitive psychology literature in the field of NLP, has
importanttraitinourcontext.Weselectthesemodelsfortheir
become more ubiquitous (Dasgupta, Schulz, Tenenbaum, &
accessibilityandtheirhighperformance.
Gershman, 2020; Hagendorff, Fabi, & Kosinski, 2023; Ull-
man, 2023). LLMs are also used as cognitive models, as
Prompt Templates. Following recent work, to reduce the
they offer almost accurate representations of human behav-
noiseinthemodels’response,wemanuallyconstruct4tem-
ior,evenoutperformingtraditionalcognitivemodelsonsome
plates to prompt the models (Mizrahi et al., 2023; Rabi-
tasks (Binz & Schulz, 2023a; Suresh et al., 2023; Gold-
novich,Ackerman,Raz,Farchi,&Anaby-Tavor,2023). For
stein,Havin,Reichart,&Goldstein,2023). Moreover,itwas
example,“Ifa(n)x(sourceitem)werea(n)y(targetdomain),
shown that improving model-human alignment on psycho-
itwouldbea(n)”(seeAppendixforafulldescriptionofthe
logicaltasksresultsinmodelimprovementonvariousdown-
templates). Wechoosetheindefinitearticleprecedingx and
streamapplications(Sucholutsky&Griffiths,2023).
y to construct a grammatical sentence. For the Llama mod-
Recentpsychologicalresearchproposesanewtask,cross-
elsthatareorientedtowardsmorelengthyconversationalre-
domain alignment, to specifically investigate concepts’ se-
sponses,weadjustthemeta-prompttoencouragethemtopro-
manticsimilaritybetweendifferentdomains(LL23). Cross-
videshortanswersthatfittheformat.
domain alignment refers to the task of mapping a concept
fromonedomaintoanother(LL23). Comparingwords(con-
cept)withinasemanticdomain(e.g.dog/catornurse/doctor) Prompting Methodology. Prompting the model with 4
is relatively easy as they have a multitude of common fea- templates per statement, results in 4 responses. We use ma-
tures,andtendtosharesimilarfunctions. However,itisless jorityvotetoconsolidatetheresults,andhaveonemodelre-
intuitivetoalignconceptsfromdifferentsemanticdomains– sponseperstatement.3 Weusegreedydecodingasawayof
such as nurse (profession) to blue (color) or guitar (instru- approximatingthemodel’smostprobableresponse. Notably,
ment) to rain (weather). The research concludes that even different templates and different decoding schemes can lead
concreteconceptsarementallyorganizedalongmoreabstract to different responses that in turn affect the model’s behav-
dimensions. Anexampleofsuchadimensionisvalence;ex- ior. However, these interventions were designed to ensure
perimentsindicatethatindividualstendtoassignpositiveor responsesthatbettercapturethemodel’spredominantbehav-
negativevalencetoconcreteconceptsindomainslikecolors, ior.
professions, and beverages. Here, we use this task to get a
glimpse into LLMs’ conceptualization and reasoning abili-
Metrics. To score the model’s performance we use Match
ties.
at K metric, M@K, i.e., we check if the model’s answer is
within the first K most popular human answers. Formally,
Cross-DomainMapping
wedenotethemodel’sresponsesby{r ,...,r }andthehu-
1 N
In this section, we describe the cross-domain mapping ex- mans’ responses by {hj,...,hj} where N is the number of
periments. We perform two types of analysis – population- 1 N
statements, N =75, and j is the popularity of the answer,
levelanalysis,inwhichwecomparethemodel’sbehaviorto
with 1 representing the most popular answer. Accordingly,
a“typical”humanbehavior. Wethenperformanindividual-
levelanalysiswherewecomparethemodel’sbehavioranda 1Thedomainswereinitiallyselectedintheoriginalpaper,LL23.
singleparticipant’sbehavior. 2Mistralistheonlymodelweuseforgeneratingexplanationsfor
thereceivedcross-domainmappings. Thisdecisionisbasedonthe
ExperimentalSetup observationthatMistraldoesnotfollowthetask’sinstructions,such
as providing responses that are not concise or relevant to the task
Dataset. For our experiments, we use the cross-domain (thismaybeattributedtoitstrainingandalignmentprocess).
mappingdatacollectedfromhumansbyLL23intheirexper- 3Incaseofatiewerandomlychooseoneresponse.red
A.
white
Human Annotators
blue
white
If a doctor were a color, white
Domain Pair what color would it be? ………
white
green
Large language model
M@1 M@3 M@any
Please answer in one word: white
doctor color
If a doctor were a color,
what color would it be? red
B. C.
Llama prompt template
<s>[INST] <<SYS>>
Answer in one word, and one word only.
<</SYS>>
Question: If a doctor were a color, what
color would it be?[/INST]
Answer:
Figure2: A.Anillustrationofourevaluationpipeline,atthetop,isthehumanannotationprocessofLL23,andatthebottomis
ourLLMevaluationprocess. B.Model-humanAgreement: eachbarrepresentstheM@1score. Thedashedlinerepresentsthe
individual-levelnormforagreementwiththemostpopularanswer. C.Llamaprompttemplate. AnexampleofaLlamaprompt
template,forthedomain-pair(doctor,color).
M@K is defined as the average over the indicator δ repre- fromthemodeland∼20responsesfromhumanparticipants.
ij
sentingifforstatementi,themodel’sresponser isthesame WethencomputeM@1,M@3andM@any.
i
asthehumanresponsehj
. Forourexperiments,wewilluse,
i
M@1, M@3, and M@any, with M@any indicating that the
Results. Table1presentstheM@1,M@3,andM@anyfor
model’sresponseisoneofthehumans’answers.
theFlanandLlamamodels. TheM@1scorerangesbetween
We note that 20 human participants are a relatively small 8.1%−24.3%, and the M@3 ranges between 14.9−36.5.
grouptoestablishresponsesatthepopulationlevel,neverthe- Llama-7B scores the highest in both M@1 and M@3 with
less, LL23 found responses to converge in a nontrivial way, 24.3% and 36.5% respectively. The M@any shows further
indicating these responses are at least somewhat representa- increase and has higher than 50% of being a human answer
tive. ToovercomethiswechosetoincorporateM@anyand formostmodels. Itiseasytobeconvincedthatthesescores
assesifthemodelanswerisreasonableeveniflessprobable. aremuchhigherthanarandomchancebaselineofjustguess-
ingaconceptfromthetargetdomain.
Experiments&Results
Interestingly,weseethatlargermodelsdonotnecessarily
Sincetherearenoclear-cut“right”or“wrong”responsesfor score higher. A possible reason for this might be that their
cross-domain mappings (a characteristic typical to any pro- responsesarelesssimilartothoseofhumans,butstillaccept-
jective test), we conduct our analysis at both the population able or even semantically equivalent (e.g., answering latte
andindividuallevels.ThisallowsustoassessLLMsbehavior where humans answer with coffee). Verifying this requires
bycomparingittohumanbehavior.4 the definition of a metric that assesses semantic equivalence
ratherthanword-levelmatching,adirectionweleaveforfu-
turework. Anotherexplanationcanbethesizeofthetraining
Population-LevelAnalysis. Webeginbyevaluatingtheca-
data.5 We note that as the evaluation set is relatively small
pability of LLMs to perform cross-domain mappings, com-
theseconclusionswarrantfurtherinvestigation. Nonetheless,
paring their performance to that of ’typical’ or average hu-
man behavior For each domain pair, we have one response 5Flan-ul2waspre-trainedon1trilliontokens,Llamawastrained
on2trilliontokens,Flan-xland-xxlweretrainedforatmost350B
4Ourcodeandsupplementarymaterialsarepubliclyavailableat tokens, based on the T5, and T5-LM-adapt versions (Raffel et al.,
https://github.com/tai314159/XCategory 2023).statement’sscorebymultiplyingitbythenumberofannota-
Model Name M@1 M@3 M@any
torsthatgavethatresponse. Thenwenormalizeittoenable
Flan-xl 12.2 23.0 39.2 ameaningfulcomparisonwiththeM@1results. Wefindthat
the new score is higher in cases with higher human consen-
Flan-xxl 8.1 14.9 32.4
sus, and lower in cases with lower consensus. According to
Flan-ul2 18.9 28.4 55.4
thisadjustedscoringmethod,weobservea30%improvement
Llama-7B 24.3 36.5 54.1 over the M@1 results, suggesting that LLMs, tend to agree
Llama-13B 21.6 27.0 56.8 with humans more in cases with higher consensus. This is
Llama-70B 16.2 29.7 50.0 anotherpointofsimilaritybetweenLLMsandhumans.
Weturntoanalyzethecaseswheremodelsdivergeintheir
agreement with humans. For that, we offer a categorization
Table1: Cross-DomainAlignmentscoresforseveralmodels.
of their disagreement patterns, based on manual inspection
Here,M@nindicatesthatthemodelansweriswithinthetop
ofthedata: (1)Similarmeaning,(2)Acceptableanswer,(3)
nmostpopularhumananswers.
Wronganswer. Toillustrate,whilemostparticipantsmapped
drumtothunder,themodelmappedittostormy. Inessence,
ourmainclaimisthatLLMsperformcross-domainmappings
theirmappingsaresemanticallysimilar,butsincethescoreis
at an above-chance level,6 resembling the patterns observed
sensitivetotheformoftheresponses,suchcasesdonotcount
inhumancognitiveprocessesacrossallmodels.
asanagreement. Anexampleofthesecondcategoryiscases
in which the model gives an acceptable answer that differs
Individual-Level Analysis. Our task can be categorized from the human answer. For example, the model mapped
as a projective test, similar to the Word Association and a football to an orange, while the typical human answer is
Rorschach tests, in which the patient is expected to project a pineapple. This suggests that in such cases models rely
unconsciousperceptionsbytheteststimuli. Insuchtests,the on different features than humans. Lastly, the model might
popular answers at the population level are not expected to generatelesscommonorhypotheticalanswersthatcanarise
beproducedattheindividuallevel. Tofurtherunderstandthe fromthenonsensicalnatureofquestionsinthistask.
behaviorofLLMscomparedtohumans, beyondjusttheav-
eragehumanbehavior,weadditionallyconductananalysisat Explanations
theindividuallevel.
In this section, we focus on explanations of cross-domain
Inourtask,duetotheabsenceofpredefinedindividualbe-
mappings.WeaskwhattypeofsimilarityLLMsrelyonwhen
haviors in the experimental data, we employ a manipulation
performingcross-domainmappings, andwhetherLLMscan
at the single annotator level to infer individual-level norms
reasonabouttheirresponses. Forhumans, thecross-domain
for cross-domain mappings. To this end, we iteratively ex-
alignmentprocessreliesonvariousfactors,includingpercep-
clude one annotator from the pool of annotators and score
tual,associative,andphonologicalsimilarity. Thiselicitation
their answers compared to the popular answers defined by
hasshowntobevaluable,asitshedslightontheunderlying
the rest of the annotators. This procedure allows us to es-
representations of basic concepts (LL23). Motivated by this
tablish individual-level norms, however, as we sample from
findingwederiveexplanationsfortheLLMsmappings. We
the distribution defined by the annotations we can see this
perform a qualitative and quantitative analysis of the expla-
normasanupper-boundtotheactualindividual-levelnorm.
nationstheygeneratefortheirmappings.
Nonetheless,thisindividual-levelnormfacilitatesacompari-
sonbetweenindividualsandLLMs.
Dataset. For each cross-domain mapping, LL23 collected
Results. Figure 2Bpresents theM@1 results comparedto
20 distinct responses, each accompanied by a participant-
the individual-level norm we define. The individual-level
provided explanation outlining the reasoning behind their
normisabout17.4%.WecanseethatFlan-ul2andLlama-7B
choice. Subsequently, twoindependentratersperformedan-
outperform this baseline, and Llama-70B slightly underper-
notation by classifying each response into one or more of
forms it. These results indicate that some LLMs may align
seven predetermined categories: (1) Phonological associa-
morecloselytotheaveragehumanbehavior,thanindividual
tion, (2) Perceptual similarity, (3) Word associations, (4)
humansonthistask.
Commonmediators,(5)Abstractalignmentoncertaindimen-
Furthermore, we investigate whether models exhibit a
sions, (6)Thematicassociation, and(7)Guessing(orOther;
higher level of agreement with the most popular human an-
full details regarding these categories are in the Appendix).
swer in cases where humans also demonstrate a higher con-
As explanations tend to have few plausible categories, the
sensus on the most popular answer. To account for this, we
inter-rater agreement was substantial but not high, with Co-
propose a new score similar to M@1. We re-weight each hen’s kappa of 0.62 and agreement of 61.5%. Most expla-
nations, 93.5%, were classified into one category, and we
6The chance level for any response (which is uniquely defined
choosetofocusonthiscase. Theannotatedexplanationsin-
byadomain-pair)canbeinterpretedastheprobabilityofrandomly
choosingaconceptfromthetargetdomain. dicate that human explanations aren’t distributed uniformlyA. B.
Abstract Common Perceptual Thematic Word
Other
Science Green Alignment Mediators Similarity Association Association
Green symbolizes growth,
development, and life. In the
context of science, it represents
the natural world and the study
of various sciences like botany.
Truck Beer
It's rather a humorous way of
expressing the idea that truck drivers
spend a lot of time on the road and
may enjoy a beer or two while on
breaks.
Football Pineapple
The idea here is to think of a fruit
that has an irregular shape, and a
hard, textured outer layer with
small bumps, which resembles the
surface of a football.
Figure3: A.Afewexamplesofmodels’explanationsfromdifferentcategoriesB.Basisofcross-domainalignment,according
tothemodel’sexplanations.
acrossthesecategories,withcategories5,4,and2accounting conclude that both models exhibit a decent to high percent-
formostexplanations,with46.1%oftheexplanationsrelying ageofsensibleexplanations.
on(5)Abstractalignment,alone.
ExplanationCategories
ModelExplanations
We analyze model explanations by classifying them into
Formodelexplanations,weusetheLlamamodels(described
seven predefined categories (following the classification of
in the Appendix) with the addition of Mistral-7B, a strong
LL23). Initially, we use LLMs with a manually constructed
LLMthatisspecificallygearedtowardsexplanationandrea-
prompt with one example per category and test the perfor-
soningtasks(Jiangetal.,2023). Wepromptthemodelwith
mance on the human-annotated explanations data. Mistral-
the question template, the human response, and a prefix for
7Bperformsthebestat40.8%,considerablybelowtheinner-
theexplanation. Wealsoaddameta-instructionthatasksthe
annotator agreement of 61.5%. To improve the results, we
modeltoprovideaclearexplanationforhisanswer. Forthis
fine-tuneaRoBERTa-largeSetFitclassifierwith32examples
part,weuseatemperatureof0.7toelicitafewdifferentrea-
percategory,excluding224examplesfortraining,whichare
soning paths. For each statement, we have 4 templates, and
about 16% of the data (Tunstall et al., 2022; Y. Liu et al.,
for each statement we sample 4 explanations, leading to 16
2019). Wedeliberatelychoosethesamenumberofexamples
explanationsperstatementand1200permodel,intotal.
per category to minimize potential classification bias. The
fine-tuned classifier achieves a 60.1% accuracy, comparable
Explanation Quality To ensure the validity of the expla-
totheinner-annotatoragreement,soweusetheresultantclas-
nations, we conduct a small-scale qualitative analysis of the
sifiertoannotateourmodelexplanations.
model’s explanations. For this analysis, we randomly sam-
InFigure3,wepresentacomparisonbetweenthedistribu-
ple one explanation for each of the 75 statements, focus-
tionofexplanationcategoriesgeneratedbyLLMsandthose
ing on Mistral-7B and Llama-13B. We manually classify7
producedbyhumans. Tomakeitclearer,categoriesnotpre-
each explanation into (i) sensible or (ii) not sensible expla-
dicted by the models are excluded from the analysis. The
nations. Overall, Mistral-7B produces more sensible expla-
analysis reveals consistent trends in the dominant explana-
nationswith87%ofexplanationsclassifiedassensible,com-
tioncategoriesemployedbybothhumansandLLMs. Specif-
pared to Llama-13B, which scored 69%. Consequently, we
ically,theprimarycategoriesforbothgroupsinclude(5)Ab-
7Oneoftheauthorsmanuallyannotatedthedata. stractalignment,(4)Commonmediators,(2)Perceptualsim-ilarity, and (6) Thematic association, suggesting LLMs rely attributesintheirexplanations,andpossibly,whileperform-
on similar types of similarity as humans to perform cross- ingcross-domainmappings.
domainmappings. However,thishypothesisrequiresfurther
evaluation,asthelinkbetweenhowtheLLMsexplainagiven Discussion
mappingandthefactorstheyrelyonduringthemappingpro-
In this paper, we draw inspiration from the recently defined
cessisnotclear.
psychological task, cross-domain alignment (LL23). This
RegardingcategoriesOther,and(7)Guessing,wenotethat
task was designed to reveal how humans represent concrete
humansoftenuse“Guessing”,whereasLLMsconsistentlytry
andabstractconcepts,bydrawinginsightsregardingthetypes
to provide an explanation and abstain from guessing. This
of similarity humans tend to rely on while performing these
difference can lead to a larger percentage of explanations
mappings. Weusethistasktoperformabehavioralanalysis
assigned to the ”Other” category for LLMs. Additionally,
ofLLMs,promptingthemwithcross-domainmappings.
LLMs produce only a few (3) Word Association explana-
Our results reveal that, at the population level, there is
tionsanddon’tproduce(1)PhonologicalAssociationexpla-
a substantial agreement between LLMs’ and humans’ map-
nations. This may suggest that LLMs are less equipped to
pings. The top-performing model predicts the most popular
model less frequent, end-of-distribution behaviors. Further-
humananswermorethan25%ofthetimes,muchhigherthan
more,thelowerprevalenceofcategory(3)impliesthattheas-
a random baseline, and most models agree with at least one
sociativestrengthbetweenthesourcewordandtargetdomain
participantmorethanhalfofthetime. However,asourmet-
wordstendstobeweakeroverall. Consequently,similarlyto
ricsM@K relysolelyonword-matchinganddonotaccount
humans,LLMsarelesslikelytousethistypeofsimilarityin
for semantic equivalence, this approach might yield only a
theirexplanations.
lowerbound. Consequently,wedeferthistofuturework.
Theseresultsshowastrongsimilaritybetweenthedistribu-
We find that LLMs produce sensible explanations for the
tionofsimilaritytypesusedbyhumansandLLMsasthebasis
mappings. Their explanation categories distribute similarly
of their alignment. This may suggest that they both rely on
tothoseofhumans,whichmightimplythattheyrelyonsim-
similarprocessesorfactorswhileperformingthemappings.
ilardimensionsofsimilarityintheirmappings. Theconnec-
ExplanationsSimilarity tionbetweentheexplanationsprovidedforthemappingsand
We showed that LLMs and human explanations distribute the actual factors LLMs rely on during their performance is
similarly across the different categories. In this section we highlynon-trivial,andrequiresfurtherverification.
turntoassessingwhethertheexplanationsthatareproduced An intriguing finding is that, similarly to humans, LLMs
byLLMsaresimilartohumanexplanations,ratherthanjust tend to generate explanations that are based on perceptual
falling into the same category. To that end, we compare the similarity,althoughtheyweretrainedonlyontext.Forexam-
pairwisesimilarityofLLM-humanexplanationswiththesim- ple,mappingpineappletofootballduetotheirsharedtexture
ilarityofLLM-LMMexplanations. Wesampleoneexplana- andshape.Thisrelatestothebodyofworkthattriestoassess
tionperstatementfromthepairofLLMsorLLMandhuman, the ability of LLMs to learn perceptual knowledge, such as
and for each explanation pair (e,e ) we compute the com- visual,sensory,andphonologicalknowledgefromtextalone
i j
monlyusedBERTScoremetric(Zhang,Kishore,Wu,Wein- (van Paridon, Liu, & Lupyan, 2021; Winter, Lupyan, Perry,
berger, & Artzi, 2019) across all 75 explanation pairs. We Dingemanse, & Perlman, 2023; Q. Liu & Lupyan, 2023;
thenaveragethesescoresanddefinetheresultasthesimilar- Alper,Fiman,&Averbuch-Elor,2023;Marjieh,Sucholutsky,
itybetweenthetwomodels.8 Weaverageacrossallsimilarity Rijn,Jacoby,&Griffiths,2023).
scoresofLLMpairsandLLM-humanpairs. Wefindthatthe Toconclude, weshowthattheseeminglynonsensicaltest
similarityscorebetweenLLMsis85.7%,whilethesimilarity ofcross-domainmappingsrevealssimilarpatternsofbehav-
scorebetweenLLMandhumansis83.6%(foundsignificant ior as presented by humans. We also use this task to as-
byt-test(p<0.01). Thisindicatesthatthemodel’sexplana- sessLLM’sabilitytoreasonaboutthesemappings,similarto
tions tend to be more similar to each other than to those of howhumanscan“reverse-engineer”thereasonsbehindsuch
humans. Apossiblereasonforthiscouldbethetendencyof alignments. Thismotivatesafuturestudyontheimplications
modelstoprovideafullandlengthyexplanation,unlikemost of these findings, and an examination of whether this align-
annotators’explanations,whichareconcise. ment between LLMs and humans runs even deeper, namely,
Moreover,weusethesamesetuptocompareexplanations whether the behavioral correlates found between these tests
acrossdifferentcategories.Interestingly,weseethatexplana- andpersonalityandcognitivepatternsinhumanscanalsobe
tionsfromthePerceptualSimilaritycategory,aremuchmore observed in LLMs. Given the impressive abilities presented
similartoeachother(87.7%)thanothercategoriesofexpla- by LLMs, several lines of work construe them as cognitive
nations (4 with 84.7%, and 5 with 85.0%), suggesting that (Binz&Schulz,2023a; Kosinski,2023; Sap,LeBras,Fried,
both LLMs and humans tend to rely on the same perceptual &Choi,2022)andevenneural(Huthetal.,2016; Pereiraet
al.,2018;Goldsteinetal.,2022)models. Weviewthiswork
8For technical reasons the value itself doesn’t reflect the simi-
as providing empirical underpinnings that can help map the
larity, but can facilitate a comparison between different similarity
scores. strengthsandweaknessesofthisconstrual.References Kosinski, M. (2023). Theory of mind might have sponta-
neously emerged in large language models. Preprint at
Alper, M., Fiman, M., & Averbuch-Elor, H. (2023). Is bert
https://arxiv.org/abs/2302.02083.
blind? exploring the effect of vision-and-language pre-
Liu, Q., & Lupyan, G. (2023). Cross-domain semantic
trainingonvisuallanguageunderstanding.
alignment: concrete concepts are more abstract than you
Binz, M., & Schulz, E. (2023a). Turning large lan-
think. Philosophical Transactions of the Royal Society B,
guage models into cognitive models. arXiv preprint
378(1870).
arXiv:2306.03917.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ...
Binz, M., & Schulz, E. (2023b, February). Using cogni-
Stoyanov, V. (2019). Roberta: A robustly optimized bert
tive psychology to understand gpt-3. Proceedings of the
pretrainingapproach.
National Academy of Sciences, 120(6). Retrieved from
Marjieh, R., Sucholutsky, I., Rijn, P. v., Jacoby, N., & Grif-
http://dx.doi.org/10.1073/pnas.2218523120 doi:
fiths,T.L. (2023). Largelanguagemodelspredicthuman
10.1073/pnas.2218523120
sensory judgments across six modalities. arXiv preprint
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,
arXiv:2302.01308.
Horvitz, E., Kamar, E., ... others (2023). Sparks of ar-
Mizrahi, M., Kaplan, G., Malkin, D., Dror, R., Shahaf, D.,
tificialgeneralintelligence: Earlyexperimentswithgpt-4.
& Stanovsky, G. (2023). State of what art? a call for
arXivpreprintarXiv:2303.12712.
multi-promptllmevaluation.
Dasgupta,I.,Schulz,E.,Tenenbaum,J.B.,&Gershman,S.J.
Pereira,F.,Lou,B.,Pritchett,B.,Ritter,S.,Gershman,S.J.,
(2020).Atheoryoflearningtoinfer.Psychologicalreview,
Kanwisher, N., ... Fedorenko, E. (2018). Toward a uni-
127(3),412.
versaldecoderoflinguisticmeaningfrombrainactivation.
Davis, C. P., Morrow, H. M., & Lupyan, G. (2019). What
Naturecommunications,9(1).
doesahorgouslooklike?nonsensewordselicitmeaningful
Rabinovich,E.,Ackerman,S.,Raz,O.,Farchi,E.,&Anaby-
drawings. CognitiveScience,43(10),e12791.
Tavor, A. (2023). Predicting question-answering perfor-
Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous
manceoflargelanguagemodelsthroughsemanticconsis-
science of interpretable machine learning. arXiv preprint
tency.
arXiv:1702.08608.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Du, M., Manjunatha, V., Jain, R., Deshpande, R., Dernon-
Matena, M., ... Liu, P. J. (2023). Exploring the limits
court, F., Gu, J., ... Hu, X. (2021). Towards interpreting
oftransferlearningwithaunifiedtext-to-texttransformer.
and mitigating shortcut learning behavior of nlu models.
arXivpreprintarXiv:2103.06922. Sap, M., LeBras, R., Fried, D., & Choi, Y. (2022). Neural
theory-of-mind?onthelimitsofsocialintelligenceinlarge
Futrell, R., Wilcox, E., Morita, T., Qian, P., Ballesteros, M.,
lms. arXivpreprintarXiv:2210.13312.
&Levy,R. (2019). Neurallanguagemodelsaspsycholin-
guistic subjects: Representations of syntactic state. arXiv Sucholutsky, I., & Griffiths, T. L. (2023). Alignment with
preprintarXiv:1903.03260. human representations supports robust few-shot learning.
Goldstein, A., Havin, M., Reichart, R., & Goldstein, A. arXivpreprintarXiv:2301.11990.
(2023). Decoding stumpers: Large language models vs. Suresh,S.,Mukherjee,K.,Yu,X.,Huang,W.-C.,Padua,L.,
humanproblem-solvers.arXivpreprintarXiv:2310.16411. &Rogers,T. (2023). Conceptualstructurecoheresinhu-
Goldstein, A., Zada, Z., Buchnik, E., Schain, M., Price, A., man cognition but not in large language models. In Pro-
Aubrey,B.,... others (2022). Sharedcomputationalprin- ceedings of the 2023 conference on empirical methods in
ciples for language processing in humans and deep lan- naturallanguageprocessing.
guagemodels. Natureneuroscience,25(3). Tay,Y.,Dehghani,M.,Tran,V.Q.,Garcia,X.,Wei,J.,Wang,
Greenwald,A.G.,McGhee,D.E.,&Schwartz,J.L. (1998). X.,... Metzler,D. (2023). Ul2: Unifyinglanguagelearn-
Measuringindividualdifferencesinimplicitcognition: the ingparadigms.
implicitassociationtest. Journalofpersonalityandsocial Touvron,H.,Martin,L.,Stone,K.,Albert,P.,Almahairi,A.,
psychology,74(6),1464. Babaei, Y., ... Scialom, T. (2023). Llama2: Openfoun-
Hagendorff,T.,Fabi,S.,&Kosinski,M. (2023). Human-like dationandfine-tunedchatmodels.
intuitive behavior and reasoning biases emerged in large Tunstall, L., Reimers, N., Jo, U. E. S., Bates, L., Korat, D.,
languagemodelsbutdisappearedinchatgpt. NatureCom- Wasserblat, M., & Pereg, O. (2022). Efficient few-shot
putationalScience,3(10),833–838. learningwithoutprompts.
Huth, A. G., De Heer, W. A., Griffiths, T. L., Theunissen, Ullman, T. (2023). Large language models fail on trivial
F. E., & Gallant, J. L. (2016). Natural speech reveals alterationstotheory-of-mindtasks.arxiv.
thesemanticmapsthattilehumancerebralcortex. Nature, vanParidon,J.,Liu,Q.,&Lupyan,G. (2021). Howdoblind
532(7600). peopleknowthatblueiscold? distributionalsemanticsen-
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., code color-adjective associations. In Proceedings of the
Chaplot,D.S.,delasCasas,D.,... Sayed,W.E. (2023). annualmeetingofthecognitivesciencesociety(Vol.43).
Mistral7b. Wei,J.,Bosma,M.,Zhao,V.Y.,Guu,K.,Yu,A.W.,Lester,B., ... Le, Q. V. (2022). Finetuned language models are andthreeassentencecompilation:
zero-shotlearners.
’If a $x$ (source item) were a $y$
Winter, B., Lupyan, G., Perry, L. K., Dingemanse, M., &
(target domain), it would be a’
Perlman,M. (2023). Iconicityratingsfor14,000+english
words. Behaviorresearchmethods.
’If a $x$ (source item) were a $y$
Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi,
(target domain), it would be an’
Y. (2019). Bertscore: Evaluatingtextgenerationwithbert.
arXivpreprintarXiv:1904.09675.
’If a $x$ (source item) were a $y$
Zhao,H.,Chen,H.,Yang,F.,Liu,N.,Deng,H.,Cai,H.,...
(target domain), it would be’
Du,M. (2023). Explainabilityforlargelanguagemodels:
A survey. ACM Transactions on Intelligent Systems and Wechoosetheindefinitearticleprecedingxandytocon-
Technology. structagrammaticalsentence. Wealsochangetheindefinite
articleprecedingtheendoftheprompttoavoiddirectingthe
ExperimentalDetails modeltowardonetypeofanswermorethananother.
For the explanations experiment, we modify the meta in-
Data
structionandaddanexplanationprefix.
Domains. Forthedomain-pairmappingsweusethedataset
Answer in one word, and one word only.
created by LL23 as we describe in the paper. The domains
Then provide a clear explanation for your answer
are: cities,animals,instruments,beverages,colors,subjects,
Question: {instruction}
jobs,fruits,vehicles,sports,supernaturals,andweather.
Answer: {answer}
Dataset. Explanation:
Forourexperiments,weusethecross-domainmappingdata
Re-weightedM@1Score
collected fromhumans by LL23 intheir experiments. Here,
we describe the main steps for its construction that will be Table 3 presents the full results of the re-weighted M@1
relevantforus9. ThedatawascollectedthroughMturkfrom score, as defined in the paper. From the table, we also see
80 participants (40 females, 40 males, mean age = 40). Af- thatFlan-ul2surpassesLlama-7B,andachieves61.9%.
ter establishing 12 general domains, 32 domain pairs were
ExplanationsCategories
selected,suchthateachdomainismatchedwithtwoorthree
otherdomains(e.g.,animal(cid:55)→job/sports/beverage).Foreach The alignment explanations are categorized into seven pre-
domain pair, 2-3 random statements of the form:“If a(n) x definedcategories,torevealtheirunderlyinglogic,following
(source item) were a(n) y (target domain), what y would it the classification established by LL23. This classification is
be?”(e.g. “Ifadoctor (sourceitem)wereacolor (targetdo- derived from the manual examination of human annotators’
main),whatcolorwoulditbe?”)areconstructed. Resultingin rationale provided for their explanations. In cases where an
75statements,eachansweredby20participants. explanation fits into multiple categories, such as those ob-
served in both our experiments and the human experiments
Models.
conductedbyLL23,themostsuitablecategoryisselected.
Forthe cross-domainmappingexperiments, we selectafew
robust LLMs to evaluate. Flan language models (Wei et al., ElaboratingonPredefinedExplanationCategories
2022) are fine-tuned on a diverse range of natural language Weherebyelaborateoneachofthecategories10:
processing(NLP)tasksanddatasets,makingthemadaptable Abstract Alignment. The concepts require active abstrac-
forvarioustasksthroughprompting. Inourexperiments,we tiontogetthesimilarity. Forexample,“Ifacloudydaywere
usethreevariants:Flan-xl(3B),Flan-xxl(11B),andFlan-ul2 afruit itwouldbeabanana”(explanation: “cloudydaysare
(20B)(Tayetal.,2023). Llama-2-chat(Touvronetal.,2023) sadandmushylikearottenbanana.”).
isasetoflargelanguagemodelsdevelopedforconversational PerceptualSimilarity. Thesimilarityisbasedonperceptual
applicationsthatundergoalignmentwithhumanpreferences. features. For example, “If a thunder were an instrument it
We use llama-2-chat in three different sizes, referred to as wouldbeadrum”.
Llama-7B,Llama-13B,andLlama-70B. Common Mediators. Both concepts are associated with a
third concept. For example, “if a sunny day were a fruit, it
PromptTemplates
would be an apple.” (explanation: “Both apple and sunny
Inourmappingexperiment,weusedthenext4differenttem-
daysareassociatedwithsummer.”).
plates. Onepresentsthetaskasaquestion:
WordAssociations.Thealignmentisbasedonanassociative
’If a $x$ (source item) were a $y$ relation11 Forexample,“ifBeijingwereacolor,itwouldbe
(target domain), what would it be?
10WeusetheexamplesfromLL23
9FulldetailsforthedataconstructionarealsofoundinLL23’s 11As this may hold true for many of the predefined categories,
supplementarymaterialshttps://osf.io/tkc84/. in LL23, the human annotators were directed to choose this cate-Category Concepts
Subject History, Philosophy, Biology, Mathematics, Psychology, Science
Colors White, Red, Blue, Brown
Animals Bird, Cat, Horse, Dog, Cow, Bear, Elephant
Sports Baseball, Football, Golf, Bike, Volleyball
Weather Cloudy day, Snow, Rain, Sunny day
Supernaturals Zombie, Fairy, Wizard, Spell, Witch, Demon
Jobs Cashier, Carpenter, Doctor, Plumber
Fruits Banana, Pear, Strawberry, Apple, Grape
Cities Boston, New York, London, Beijing, Paris
Instruments Piano, Drum, Flute, Guitar, Harp
Beverages Water, Wine, Whisky, Milk, Tea, Juice
Vehicles Scooter, Bike, Truck, Motorcycle
Table2: Categoriesandtheirrespectiveconcepts.
DistributionOfExplanationsCategories
Model Name RW-M@1 @1 Diff
InTable4wepresentthenumberofexplanationsforLLMs
Flan-xl 42.3 12.2 30.1
and humans and their distribution across categories. These
Flan-xxl 38.6 8.1 30.5 values are the basis for Fig. 3B. In this Table, we use the
Flan-ul2 61.9 18.9 43.0 abbreviation Perceptual Sim. for Perceptual Similarity, and
T-AssociationandW-AssociationsforandThematicAssoci-
Llama-7B 55.4 24.3 31.1
ationandWordAssociationsrespectively.
Llama-13B 55.1 21.6 29.5
ExplanationsSimilarity
Table3: Cross-DomainAlignmentscoresforseveralmodels.
InTable5,wepresentthesimilaritybetweenexplanationsof
Here,wepresentRW-M@1,M@1andtheirdifferences.RW-
different LLMs, and human pairs, as calculated by the aver-
M@1takesintoaccountthehumanconsensusaboutthemost
ageBERT-Score.Abovethediagonal,wepresenttheaverage
popularanswer. ThehigherscoresofRW-M@1indicatethat
similarity, and below the diagonal, we present the standard
the models tend to produce the most popular answer more
deviation. WeseethatLLMstendtobemoresimilartoone
whenitisinhigherconsensus.
another, than to humans. This does not necessarily reveal
dissimilarity between human and LLM explanations in gen-
eralastheannotationsexplanationstendtoalsodifferentiate
red.” (explanation: “becauseitremindsmeof“redChina”.”).
in style; human explanations are shorter and are not always
PhonologicalSimilarity. Thesimilarityisbasedonphono-
structured as a full sentence. For instance, to explain map-
logicalsimilarity(e.g,bearandbeer).
pingBostontored,oneannotatorreplied: ”redsox”.
Thematic Association. The similarity is based on thematic
(non-linguistic)association12. Forexample,“Ifadogwerea Limitations
sport,itwouldbefrisbee”(explanation: “theylovegamesof
Despite the insights gained from our experimentation, sev-
fetchandfrisbee”).
erallimitationsshouldbeacknowledged. First,weworkwith
Guessing. Incaseswheretheexplanationreliesonintuition
strongLLMsthatarenotSOTA.WechosethissetupasSOTA
orspeculation. Forexample,“Ifmathematicswereacolor,it
LLMs are proprietary which prevents us from knowing the
wouldbeblack. ” (explanation: ”Iwasguessing.”).
process of prompting the models. Instead, we prefer to uti-
lize the best open-source models that allow us more control
gory when the associations were likely formed by frequent word
co-occurrence. ofourexperiment.
12ThehumanannotatorsinLL23weregiventheguideline“Iftwo Second, Our score M@k is sensitive to the surface form
thingsappearinthesamecontextthatdoesnotnecessarilyhaveany- of the responses, as we discuss in our analysis. The model
thingtodowithlanguage,itshouldgointothiscategory. Iftheas-
might perform a semantically equivalent mapping to that of
sociation is based on linguistic context it should go into the Word
Associationcategory.” humans,butiftheformoftheresponseisdifferent,itwillnotModelMetric AbstractAlignment CommonMediators PerceptualSim. T-Association Other W-Associations
Llama-7B 662 345 115 38 38 2
Llama-13B 760 269 108 48 14 1
Llama-70B 624 448 158 21 34 4
Mistral-7B 471 367 150 85 29 9
Human 659 221 192 102 38 29
Table4: Distributionofexplanationscategoriesfordifferentmodels.
Llama-7b Llama-13b Llama-70b Mistral-7b Human
Llama-7b 100.0 87.3 88.6 87.4 83.6
Llama-13b 10.4 100.0 87.7 86.0 84.1
Llama-70b 2.2 1.0 100.0 87.2 83.7
Mistral-7b 2.1 1.0 1.9 100.0 83.1
Human 1.3 1.4 1 1.6 100.0
Table5: SimilarityandstandarddeviationbetweenexplanationsofdifferentLLMsandhumanpairs.
be taken into account. This concern can be mitigated by in-
corporatingothertoolsthatcandetectsemanticequivalency.
For example, using word-embedding-based metrics. This is
somethingweintendtodoinfuturework.
Furthermore,thehumanexperimentdataweuseisofsmall
scale(1500datapoints)raisingafewconcerns. Oneconcern
involves the robustness of the data. Although this concern
holds,theclearpatternsfromourexperimentsacrossdifferent
modelscaneaseitsomewhat. Additionally,thelackofsocial
demographicdetailsaboutourparticipantsmakesourresults
susceptibletosocietalbiasesandmightnotaccuratelyreflect
thephenomenoninotherreal-worlddistributions. Toaddress
this,futureworkcanvalidateourresultsoverlargerandmore
diverseannotatorcohorts.