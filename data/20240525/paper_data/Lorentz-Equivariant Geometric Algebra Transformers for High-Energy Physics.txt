MIT-CTP/5723
Lorentz-Equivariant Geometric Algebra Transformers
for High-Energy Physics
JonasSpinner∗ VictorBresó∗
HeidelbergUniversity HeidelbergUniversity
j.spinner@thphys.uni-heidelberg.de v.breso@thphys.uni-heidelberg.de
PimdeHaan TilmanPlehn JesseThaler JohannBrehmer
QualcommAIResearch† HeidelbergUniversity MIT/IAIFI QualcommAIResearch†
Abstract
Extracting scientific understanding from particle-physics experiments requires
solvingdiverselearningproblemswithhighprecisionandgooddataefficiency.
WeproposetheLorentzGeometricAlgebraTransformer(L-GATr),anewmulti-
purposearchitectureforhigh-energyphysics. L-GATrrepresentshigh-energydata
inageometricalgebraoverfour-dimensionalspace-timeandisequivariantunder
Lorentz transformations, the symmetry group of relativistic kinematics. At the
sametime,thearchitectureisaTransformer,whichmakesitversatileandscalable
tolargesystems.L-GATrisfirstdemonstratedonregressionandclassificationtasks
fromparticlephysics. WethenconstructthefirstLorentz-equivariantgenerative
model: acontinuousnormalizingflowbasedonanL-GATrnetwork,trainedwith
Riemannianflowmatching. Acrossourexperiments, L-GATrisonparwithor
outperformsstrongdomain-specificbaselines.
1 Introduction
Inthequesttounderstandnatureonthemostfundamentallevel,machinelearningisomnipresent[22].
Takethemostcomplexmachineeverbuilt: atCERN’sLargeHadronCollider(LHC),protonsare
acceleratedtoclosetothespeedoflightandinteract;theirremnantsarerecordedbyvariousdetector
components,totallingaround1015bytesofdatapersecond[23]. Thesedataarefiltered,processed,
andcomparedtotheorypredictions,aswesketchinFig.1. Eachstepofthispipelinerequiresmaking
decisionsabouthigh-dimensionaldata. Moreoftenthannot,thesedecisionsarerootedinmachine
learning, increasinglyoftendeepneuralnetworks[11,12,19,31,43,48,51,64]. Thisapproach
powersmostmeasurementsinhigh-energyphysics,culminatingintheHiggsbosondiscoveryin
2012[4,29].
High-energyphysicsanalysesputstringentrequirementsonnetworkarchitectures. Theyneedto
be able to represent particle data and have to be expressive enough to learn complex relations
in high-dimensional spaces precisely. Moreover, training data often come from precise theory
computationsandcomplexdetectorsimulations,bothofwhichrequireaconsiderablecomputational
cost; architecturesthereforeneedtobedataefficient. Generativemodelsofparticle-physicsdata
faceadditionalchallenges: becauseofdetectorboundariesandselectioncuts,densitiesfrequently
featuresharpedges;atthesametime,itisoftenimportanttomodellow-densitytailsofdistributions
preciselyovermultipleordersofmagnitudeofprobabilitydensities.
∗Equalcontribution
†QualcommAIResearchisaninitiativeofQualcommTechnologies,Inc.
Preprint.Underreview.
4202
yaM
32
]na-atad.scisyhp[
1v60841.5042:viXraRecon- Reconstr. Event
struction particles selection
Nature Detector
Inference
Shower +
Scattering MC Detector Recon- Reconstr. Event Discoveries &
hadron.
Quantum amplitudes sampler sim sim struction particles selection measurements
Theory
4.1 Amplitude
surrogates
4.2 Top
4.3 Generative modelling tagging
Figure 1: Schematic view of the data-analysis workflow in high-energy physics. Measurements (top) are
processedinparallelwithsimulateddata(bottom);theircomparisonisultimatelythebasisformostscientific
conclusions.Inorange,weshowhowthethreeapplicationsofL-GATrweexperimentwithinthispaperfitinto
thisworkflow.Thearchitectureisalsoapplicableinseveralotherstages,includingreconstructionandinference.
Off-the-shelfarchitecturesoriginallydevelopedforvisionorlanguagearepopularstartingpointsfor
high-energyphysicsapplications[18,36],butdonotsatisfythesegoalsreliably. Wearguethatthisis
becausetheydonotmakesystematicuseoftherichstructureofthedata. Particleinteractionsare
governedbyquantumfieldtheoriesandrespecttheirsymmetries,notablytheLorentzsymmetryof
specialrelativity[38,61]. FirstLorentz-equivariantarchitectureshaverecentlybeenproposed[9,41,
66],buttheyarelimitedtospecificapplicationsandnotdesignedwithafocusonscalability.
Inthiswork,weintroducetheLorentzGeometricAlgebraTransformer(L-GATr),anewgeneral-
purpose network architecture for high-energy physics. It is based on three design choices. First,
L-GATrisequivariantwithrespecttotheLorentzsymmetry.3 Itsupportspartialandapproximate
symmetriesasfoundinsomehigh-energyphysicsapplicationsthroughsymmetry-breakinginputs.
Second,asrepresentations,L-GATrusesthegeometric(orClifford)algebraoverthefour-vectors
ofspecialrelativity. Thisalgebraisbasedonthescalarandfour-vectorpropertiesthatLHCdata
arenaturallyparameterizedinandextendsthemtohigherorders,increasingthenetworkcapacity.
Finally,L-GATrisaTransformer. Itsupportsvariable-lengthinputs,asfoundinmanyLHCproblems,
andevenlargemodelscanbetrainedefficiently. Becauseitcomputespairwiseinteractionsthrough
scaleddot-productattention,forwhichtherearehighlyoptimizedbackendslikeFlashAttention[32],
thearchitecturescalesparticularlywelltoproblemswithmanytokensorparticles.
L-GATrisbasedontheGeometricAlgebraTransformerarchitecture[13,35],whichwasdesigned
fornon-relativisticproblemsgovernedbytheEuclideansymmetryE(3)oftranslations,rotations,
andreflections. OurL-GATrarchitecturegeneralizesthattorelativisticscenariosandtheLorentz
symmetry. Tothisend,wedevelopseveralnewnetworklayers,includingamaximallyexpressive
Lorentz-equivariantlinearmap,aLorentz-equivariantattentionmechanism,andLorentz-equivariant
layernormalization.
Inadditiontothegeneral-purposearchitecture,wedevelopthefirst(tothebestofourknowledge)
Lorentz-equivariantgenerativemodel. WeconstructacontinuousnormalizingflowwithanL-GATr
denoisingnetworkandproposetrainingitwithaRiemannianflowmatchingapproach[24]. This
not only lets us train the model in a scalable way, but also allows us to encode more aspects of
theproblemgeometryintothemodel: wecanevenhard-codephase-spaceboundaries,whichare
commonplaceinhigh-energyphysics.
WedemonstrateL-GATronthreeparticle-physicsapplications. Wefirsttrainneuralsurrogatesfor
quantumfieldtheoreticamplitudes,aregressionproblemwithhighdemandsonprecision. Next,we
trainclassificationmodelsandevaluateL-GATronthepopularbenchmarkproblemoftoptagging.
Finally,weturntothegenerativemodellingofreconstructedparticles,whichcanmaketheentire
analysispipelinesubstantiallymoreefficient. Thethreeapplicationsdifferintheroletheyplayin
theLHCanalysispipeline(seeFig.1),data,andlearningobjective,highlightingtheversatilityof
L-GATr. WefindthatL-GATrisonparwithoroutperformsstrongdomain-specificbaselinesacross
allproblems,bothintermsofperformanceanddataefficiency.
3OnecouldextendL-GATrtothefullPoincarésymmetry,whichadditionallyincludesspace-timetranslations.
However,thisisnotnecessaryformostparticle-physicsapplications,asusuallyonlythemomentum,andnotthe
absoluteposition,ofparticlesisofinterest.Akeyexceptionisthestudyoflong-livedparticles.
22 Backgroundandrelatedwork
High-energy physics In Fig. 1 we sketch the typical data-analysis pipeline in particle physics.
Detectormeasurementsandtheorysimulationsareprocessedinparallelandultimatelycompared.
Bothuseasacentralrepresentationthenotionofparticles. Aparticleischaracterizedbyadiscrete
type label, an energy E ∈ R, and a spatial momentum p⃗ ∈ R3. Types include fundamental
particles like electrons, photons, quarks, and gluons, composite particles like protons, as well as
reconstructedobjectslike“jets”[68]or“particle-flowcandidates”[69], whicharetheoutputsof
complexreconstructionalgorithms. Theenergyandspatialmomentumofaparticleareconveniently
combinedintoafour-momentump=(E,p⃗).
The laws of fundamental physics [40, 73] are invariant with respect to the choice of an inertial
referenceframe[38,61]: theydonotchangeunderrotationsandboostsfromoneun-accelerated
reference frame into another.4 Together, these transformations form the special orthochronous
LorentzgroupSO+(1,3).5 Thisgroupistheconnectedcomponentoftheorthogonalgrouponthe
four-vectorspaceR1,3withMinkowskimetricdiag(+1,−1,−1,−1)[59]. Lorentztransformations
mix temporal and spatial components. Space and time should therefore not be considered as
separateconcepts,butratherascomponentsofafour-dimensionalspace-time. Particlefour-momenta
are another instance of this: they transform in the vector representation of the Lorentz group as
pµ → p′µ = (cid:80) Λµpν for Λ ∈ SO+(1,3), with the Lorentz transformation mixing energy and
ν ν
spatialmomentum.
Geometricdeeplearning Thecentraltenetofgeometricdeeplearning[14,30]istoembedthe
known structure of a problem into the architecture used to solve it, instead of having to learn it
completelyfromdata. Thekeyideaisthatofequivariancetosymmetrygroups: whentheinputs
xtoanetworkf aretransformedwithasymmetrytransformationg,theoutputsshouldtransform
underthesameelementofthesymmetrygroup,f(g·x)=g·f(x),where·denotesthegroupaction.
Whatisknownas“equivariance”inmachinelearningisoftencalled“covariance”inphysics[27].
GATr Our work is rooted in the Geometric Algebra Transformer (GATr) [13, 35], a network
architecture that is equivariant to E(3), the group of non-relativistic translations, rotations, and
reflections. GATrrepresentsinputs,hiddenstates,andoutputsinthegeometric(orClifford)algebra
G [28,42,65]. AgeometricalgebraextendsabasespacelikeR3 tohigherordersandaddsa
3,0,1
bilinearmapknownasthegeometricproduct. WeprovideaformalintroductioninAppendixA.What
mattersinpracticeisthatthisvectorspacecanrepresentvarious3Dgeometricobjects. Brehmer
et al. [13] develop different layers for this representation and combine them in a Transformer
architecture[71]. ForL-GATr,webuildontheGATrblueprint,butre-designallcomponentssuch
thattheycanrepresentfour-momentaandareequivariantwithrespecttoLorentztransformations.
Lorentz-equivariantarchitectures Recently,someLorentz-equivariantarchitectureshavebeen
proposed. Most closely related to this work is the Clifford Group Equivariant Neural Networks
(CGENN)byRuheetal.[66]. Likeus, theyusethegeometricalgebraoverfour-vectors. While
theyalsouseLorentz-equivariantlinearmapsandgeometricproducts,ourarchitecturesdifferina
numberofways. Inparticular,theyproposeamessage-passinggraphneuralnetwork,whilewebuild
aTransformerarchitecturebasedondot-productattention.
OtherLorentz-equivariantarchitecturesincludeLorentzNet[41]andthePermutationEquivariantand
LorentzInvariantorCovariantAggregatorNetwork(PELICAN)[9]. Botharemessage-passinggraph
neuralnetworkaswell. Givenasetoffour-vectors,PELICANcomputesallpairwiseinnerproducts,
whichareLorentzinvariants,andthenprocessesthemwithapermutation-equivariantarchitecture.
LorentzNetmaintainsscalarandfour-vectorrepresentationsandupdatesthemwithagraphattention
mechanismsimilartotheoneproposedbyVillaretal.[72].
4Allowingforacceleratingreferenceframeswouldbringustothegeneraltheoryofrelativity,whichis
irrelevantforparticlephysicsexperimentaslongastheyarenotperformedclosetoablackhole.
5“Special” and “orthochronous” here mean that spatial and temporal reflections are not considered as
symmetries. Infact,thefundamentallawsofnaturearenotinvariantunderthosetransformations,aneffect
knownasP-violationandT-violation.
3Flowmatching Continuousnormalizingflows[25]areaclassofgenerativemodelsthatpusha
samplefromabasedensitythroughatransformationdefinedbyanordinarydifferentialequation.
Specifically,theevolutionofapointx ∈ Rd ismodelledasatime-dependentflowψ : Rd → Rd
t
with dψ (x)=u (ψ (x)),ψ (x)=x,whereu isatime-dependentvectorfield.
dt t t t 0 t
Conditionalflowmatching[52]isasimpleandscalabletrainingalgorithmforcontinuousnormal-
izingflowsthatdoesnotrequirethesimulationoftrajectoriesduringtraining. Instead,theobjec-
tive is to match a vector field v (x), parametrized by a neural network, onto a conditional target
t
vectorfieldu (x|x )alongaconditionalprobabilitypathp (x|x ),minimizingthelossL =
t 1 t 1 CFM
E ∥v (x)−u (x|x )∥2,wherex ∼q(x )aresamplesfromthebasedis-
t∼U[0,1],x1∼q(x1),x∼pt(x|x1) t t 1 1 1
tribution.
Choosing a well-suited probability path and corresponding target vector field can substantially
improvethedataefficiencyandsamplingquality. AprincipledapproachtothischoiceisRiemannian
flowmatching(RFM)[24]. Insteadofconnectingtargetandlatentspacepointsbystraightlinesin
Euclideanspace, RFMproposestochooseprobabilitypathsbasedonthemetricofthemanifold
structureofthedataspace. Ifavailableinclosedform,theyproposetousegeodesicsasprobability
paths,whichcorrespondstooptimaltransportbetweenbaseanddatadensity.
3 TheLorentzGeometricAlgebraTransformer(L-GATr)
3.1 Lorentz-equivariantarchitecture
Geometricalgebrarepresentations Theinputs,hiddenstates,andoutputsofL-GATrarevariable-
sizesetsoftokens. EachtokenconsistsofncopiesofthegeometricalgebraG andmadditional
1,3
scalarchannels.
ThegeometricalgebraG isdefinedformallyinAppendixA.Inpractice,G isa16-dimensional
1,3 1,3
vector space that consists of multiple subspaces (or grades). The 0-th grade consists of scalars
thatdonottransformunderLorentztransformations,forinstanceembeddingsofparticletypesor
regressionamplitudes. Thefirstgradecontainsspace-timefour-vectorssuchasthefour-momenta
p=(E,p⃗). Theremaininggradesextendtheseobjectstohigherorders(i.e.antisymmetrictensors),
increasing expressivity. In addition, the geometric algebra defines a bilinear map, the geometric
productG ×G →G ,whichcontainsboththespace-timeinnerproductandageneralization
1,3 1,3 1,3
oftheEuclideancrossproduct.
ThisrepresentationnaturallyfitsmostLHCproblems,whicharecanonicallyrepresentedassetsof
particles,eachparameterizedwithtypeinformationandfour-momenta. Werepresenteachparticleas
atoken,storetheparticletypeasaone-hotembeddinginthescalarchannelsandthefour-momentum
inthefirstgradeofthegeometricalgebra.
Lorentz-equivariantlinearlayers WedefineseveralnewlayersthathavebothG andadditional
1,3
scalarrepresentationsasinputsandoutputs. Forreadability,wewillsuppressthescalarchannelsin
thefollowing. Werequireeachlayerf(x)tobeequivariantwithrespecttoLorentztransformations
Λ∈SO+(1,3):f(Λ·x)=Λ·f(x),where·denotestheactionoftheLorentzgrouponthegeometric
algebra(seeAppendixA).Lorentzequivariancestronglyconstrainslinearmapsbetweengeometric
algebrarepresentations:6
Proposition1. Anylinearmapϕ:G →G thatisequivarianttoSO+(1,3)isoftheform
1,3 1,3
4 4
(cid:88) (cid:88)
ϕ(x)= v ⟨x⟩ + w e ⟨x⟩ (1)
k k k 0123 k
k=0 k=0
forparametersv,w ∈R5. Heree isthepseudoscalar,theuniquehighest-gradebasiselementin
0123
G ;⟨x⟩ isthebladeprojectionofamultivector,whichsetsallnon-grade-kelementstozero.
1,3 k
WeshowthisinAppendixA.Inourarchitecture,linearlayersmapbetweenmultipleinputandoutput
channels. Therearethentenlearnableweightsv ,w foreachpairofinputandoutputG channels
k k 1,3
(plustheusualweightsforlinearmapsbetweentheadditionalscalarchannels).
6O(1,3)-equivariantlinearmapsarerestrictedtothefirstsum;theadditionalequivarianceunderreflections
forbidsthemultiplicationwiththepseudoscalar.
4Lorentz-equivariantnon-linearlayers Wedefinefouradditionallayers,allofwhicharemanifestly
Lorentz-equivariant. Thefirstisthescaleddot-productattention
(cid:32)(cid:80) (cid:33)
Attention(q,k,v)
=(cid:88)
Softmax
c√⟨q i′c,k ic⟩
v , (2)
i′c′ i ic′
16n
c
i
wheretheindicesi,i′labeltokens,c,c′labelchannels,and⟨·,·⟩istheG innerproduct. Thisinner
1,3
productcanberewrittenasapre-computedlistofsignsandaEuclideaninnerproduct, whichis
whywecancomputetheattentionmechanismwithefficientbackendsdevelopedfortheoriginal
Transformerarchitecture,forinstanceFlashAttention[32]. Thisiskeytothegoodscalabilityof
L-GATr,whichwewilldemonstratelater.
Whendefininganormalizationlayer,wehavetobecareful: intheG innerproduct,cancellations
1,3
betweenpositive-normdirectionsandnegative-normdirectionscanleadtonormvaluesmuchsmaller
thanthescaleoftheindividualcomponents;dividingbythenormthenrisksblowingupthedata.
Thesecancellationsareanunavoidableconsequenceofthegeometryofspace-time. Wemitigatethis
issuebyusingthegrade-wiseabsolutevalueoftheinnerproductinthenorm
(cid:118)
(cid:117) n 4 (cid:12) (cid:12)
(cid:46)(cid:117)1 (cid:88)(cid:88)(cid:12)(cid:68) (cid:69)(cid:12)
LayerNorm(x)=x (cid:116) n (cid:12) (cid:12) ⟨x c⟩ k,⟨x c⟩ k (cid:12) (cid:12)+ϵ, (3)
c=1k=0
applying an absolute value around each grade of each multivector channel ⟨x ⟩ . Here ϵ > 0 is
c k
a constant that further numerically stabilizes the operation. This normalization was proposed by
DeHaanetal.[35]forE(3)-invariantarchitectures,weadaptittotheLorentz-equivariantsetting.
WealsousethegeometricproductGP(x,y)=xydefinedbythegeometricalgebraG . Finally,
1,3
weusethescalar-gatedGELU[45]nonlinearitiesGatedGELU(x)=GELU(⟨x⟩ )x,asproposed
0
byBrehmeretal.[13].
Transformerarchitecture WecombinetheselayersintoaTransformerarchitecture[71,74]:
x¯=LayerNorm(x),
AttentionBlock(x)=Linear◦Attention(Linear(x¯),Linear(x¯),Linear(x¯))+x,
MLPBlock(x)=Linear◦GatedGELU◦Linear◦GP(Linear(x¯),Linear(x¯))+x,
Block(x)=MLPBlock◦AttentionBlock(x),
L-GATr(x)=Linear◦Block◦Block◦···◦Block◦Linear(x).
This L-GATr architecture is structurally similar to the original GATr architecture [13], but the
representations,linearlayers,attentionmechanism,geometricproduct,andnormalizationlayerare
differenttoaccommodatethedifferentnatureofthedataanddifferentsymmetrygroup.
Lorentzsymmetrybreaking Whilefundamentalphysicsis(tothebestofourknowledge)symmet-
ricunderLorentztransformations,theLHCmeasurementprocessisnot. Thedirectionoftheproton
beamspresentsthemostobviousviolationofthissymmetry. Smallerviolationsareduetothedetec-
torresolution: particleshittingthecentralpartofthedetector(orthogonaltothebeaminthedetector
restframe)aretypicallyreconstructedwithahigherprecisionthanthoseemergingatanarrowangle
tothebeam. Evensmallerviolationscome,forinstance,fromindividualdefunctdetectorelements.
SolvingsometasksmaythereforebenefitfromanetworkthatcanbreakLorentzequivariance.
L-GATrsupportssuchbrokenorapproximatesymmetriesbyincludingthesymmetry-breakingeffects
asadditionalinputsintothenetwork. Concretely,wheneverweanalyzereconstruction-leveldata,we
includethebeamdirections;seeAppendixC.Thisapproachcombinesthestronginductivebiasesof
aLorentz-equivariantarchitecturewiththeabilitytolearntobreakthesymmetrywhenrequired.
3.2 Lorentz-equivariantflowmatching
Inadditiontoregressionandclassificationmodels,weconstructagenerativemodelforparticledata.
Besidesthestrictrequirementsonprecision,flexibility,anddataefficiency,generativemodelsof
LHCdataneedtobeabletoaddresssharpedgesandlongtailsinhigh-dimensionaldistributions.
5We develop a continuous normalizing flow based on an L-GATr vector field and train it with
Riemannianflowmatching(RFM)[24]. Thisapproachhasseveralcompellingproperties: trainingis
simulation-freeandscalableandthegenerativemodelisLorentz-equivariant.7 Inaddition,theRFM
approachallowsustodealwithsharpedgesandlongtailsinageometricway: weparameterizethe
reachablefour-momentumspaceforeachparticleasamanifoldandusegeodesicsonthismanifold
asprobabilitypathsfrombasesamplestodatapoints.
Probabilitypathsperfectforparticles Concretely,reconstructedparticlesp=(E,p⃗)areoften
requiredtosatisfyconstraintsoftheformp2+p2 ≥p2 andp2 >0. FollowingRefs.[16,17,44,
1 2 Tmin
47],weparameterizethismanifoldwithphysicallymotivatedcoordinatesy =(y ,y ,η,ϕ). The
m p
maptofour-momentaisgivenby
(cid:32) (cid:33)
(cid:113)
p=(E,p ,p ,p )=f(y)= m2+p2 cosh2η, p cosϕ, p sinϕ, p sinhη , (4)
x y z T T T T
wherem2 = exp(y )andp = p +exp(y ). We
m T T,min p
60
defineaconstantdiagonalmetricinthecoordinatesyand
usethecorrespondinggeodesicsasprobabilitypaths. This
40
Riemannian manifold is geodesically convex, meaning
anytwopointsareconnectedbyauniquegeodesic,and
20
geodesicallycomplete,meaningthatpathsthusneverenter
four-momentumregionsforbiddenbythephase-spacecuts. 0 p T<p T,min
Byalsorunningtheordinarydifferentialequation(ODE)
solverinthesecoordinates,weguaranteethateachsample 20
−
satisfies the four-momentum constraints. As an added
benefit,thischoiceofmetriccompressesthehigh-energy 40
−
tails of typical particle distributions and thus simplifies
learningthemcorrectly. 60
− 25 0 25 50 75
InFig.2,weshowtargetprobabilitypathsgeneratedin − p x[GeV]
this way. Our approach ensures that none of the trajec- Figure2:TargetvectorfieldforRiemannian
toriespassthroughthephase-spaceregionp < p , flowmatching. Ourchoiceofmetricspace
T T,min
where the target density does not have support; instead, guaranteesthatthegenerativemodelrespects
thegeodesicsleadaroundthisproblematicregion. phase-spaceboundaries(redcircle).
4 Experiments
4.1 SurrogatesforQFTamplitudes
Problem WefirstdemonstrateL-GATrasaneuralsurrogateforquantumfieldtheoreticalampli-
tudes [5–7, 57, 58], the core of the theory predictions that LHC measurements are compared to.
Theseamplitudesdescribethe(un-normalized)probabilityofinteractionsoffundamentalparticles
asafunctionoftheirfour-momenta. Asthisisafundamentalinteractionanddoesnotincludethe
measurementprocess,itisexactlyLorentz-invariant. Evaluatingthemisexpensive,ontheonehand
because it requires solving complex integrals, on the other hand because the number of relevant
termscombinatoriallygrowswiththenumberofparticles. Neuralsurrogatescangreatlyspeedup
thisprocessandthusenablebettertheorypredictions,butaccuratelymodellingtheamplitudesof
high-multiplicityprocesseshasbeenchallenging.
As example processes, we study qq¯ → Z +ng, the production of a Z boson with n = 1,...,4
additional gluons from a quark-antiquark pair. For each gluon multiplicity, we train a L-GATr
7Strictlyspeaking,onlythemapfromthebasedensitytodataspaceisequivariantwithrespecttothefull
Lorentzgroup.Thebasedensityandthusalsothedensityofthegenerativemodelareonlyinvariantwithrespect
torotations.Thisisbecausethegroupofboostsisnotcompact:itisimpossibletodefineaproperlynormalized
densitythatassignsthesameprobabilitytoeveryboosteddatavariation. Intheory,onecoulddefineafully
Lorentz-invariantbasemeasure; thentheflowwoulddefineaLorentz-invariantmeasurethatwouldnotbe
normalizable—goodluckwiththat.Inpractice,compactsubsetsoftheorbits,forinstancecharacterizedbya
limitedrangeofthecenter-of-massmomentum,suffice.Allofthisisinanalogyto“E(3)-invariant”generative
models[46],whicharestrictlyonlyinvarianttorotations,butnotto(non-compact)translations.
6
]VeG[yp10 −1 MLP qq¯ Z+4g
→
Transformer
10 −2 DSI 10 −1
GAP
10 3 CGENN
− L-GATr 10 −2
10 4
−
10 3
10 5 − MLP
−
Transformer
10 −6 10 −4 D GS AI
P
CGENN
10 7
− 10 5 L-GATr
−
Z+1g Z+2g Z+3g Z+4g 103 104 105
Numberoftrainingsamples
Figure3: Amplitudesurrogates. Left: Surrogateerrorforprocessesofincreasingparticlemultiplicityand
complexity,trainingonthefulldatasetof4·105samples.L-GATroutperformsthebaselines,especiallyatmore
complexprocesses.Right:Surrogateerrorasafunctionofthetrainingdatasetsize.
Model Accuracy AUC 1/ϵ (ϵ =0.5) 1/ϵ (ϵ =0.3)
B S B S
TopoDNN[48] 0.916 0.972 – 295± 5
LoLa[15] 0.929 0.980 – 722± 17
P-CNN[1] 0.930 0.9803 201± 4 759± 24
N-subjettiness[60] 0.929 0.981 – 867± 15
PFN[50] 0.932 0.9819 247± 3 888± 17
TreeNiN[56] 0.933 0.982 – 1025± 11
ParticleNet[62] 0.940 0.9858 397± 7 1615± 93
ParT[63] 0.940 0.9858 413±16 1602± 81
LorentzNet*[41] 0.942 0.9868 498±18 2195±173
CGENN*[66] 0.942 0.9869 500 2172
PELICAN*[9] 0.9426±0.0002 0.9870±0.0001 – 2250± 75
L-GATr(ours)* 0.9417±0.0002 0.9868±0.0001 548±26 2148±106
Table 1: Top tagging. We compare accuracy, area under the ROC curve (AUC), and inverse background
acceptancerate1/ϵ attwodifferentsignalacceptancerates(orrecall)ϵ ∈ (0.3,0.5)forthetoptagging
B S
datasetfromKasieczkaetal.[49].Lorentz-equivariantmethodsareindicatedwithanasterisk*;thebestresults
foreachmetricareinbold. ForL-GATr, weshowthemeanandstandarddeviationoffiverandomseeds.
Baselineresultsaretakenfromtheliterature.
modeltopredicttheamplitudeasafunctionofthefour-momentaoftheinitialandfinalparticles.8
Thegenerationofthetrainingdataandtheprecisesetupofthelearningproblemaredescribedin
AppendixC.WecompareL-GATrtovariousbaselines,includingtheLorentz-equivariantmessage-
passingarchitectureCGENN[66],aTransformer[71],andDSI,abaselinebasedontheDeepSets
framework[75]thatwedevelopourselves;wedescribeitindetailinAppendixB.
Surrogatequality L-GATrconsistentlyapproximatestheamplitudeswithhighprecision,aswe
showintheleftpanelofFig.3. Forasmallnumberofparticles,itisslightlyworsethanourown
baselineDSI,butitscalesmuchbettertoalargenumberofparticles,whereitoutperformsallother
methods. Thisisexactlytheregioninwhichneuralsurrogatescouldhavethehighestimpact.
Dataefficiency IntherightpanelofFig.3westudythedataefficiencyofthedifferentarchitectures.
WefindthatL-GATriscompetitiveatanytrainingdatasize,combiningthesmall-dataadvantagesof
itsstronginductivebiasesandthebig-dataadvantagesofitsTransformerarchitecture.
7
sedutilpmadezilamronnoESM sedutilpmadezilamronnoESM10 −2
0.3
0.015
Groundtruth
MLP
Transformer
10 4 L-GATr
−
0.0 0.0
1.1 1.1 1.1
1.0 1.0 1.0
0.9 0.9 0.9
0 100 200 300 0 2 4 6 150 200 250
p T,t[GeV] ∆R j3,j4 m t[GeV]
Figure4:Generativemodelling:Marginaldistributionsofreconstructedparticlesinthepp→tt¯+4jetsprocess.
Wecomparetheground-truthdistribution(black)tothreegenerativemodels: continuousnormalizingflows
basedonaTransformer,MLP,orourL-GATrnetwork.Thethreemarginalsshownrepresentkinematicfeatures
thatareknowntobechallenging.TheL-GATrflowdescribesthemmostaccurately.
4.2 Toptagging
Problem Next,weturntotheproblemofclassifyingwhetherasprayofreconstructedhadrons
originatedfromthedecayofatopquarkoranyotherprocess. Thisproblemoftoptaggingisan
importantfilteringstepinanyanalysisthattargetsthephysicsoftopquarks,theheaviestelementary
particleintheStandardModel. WeusetheestablishedtoptaggingdatasetbyKasieczkaetal.[48,49]
asabenchmarkandcomparetothepublishedresultsformanyalgorithmsandarchitectures.
Results AsshowninTbl.1,L-GATrachievesaperformancethatisforallpracticalpurposeson
parwiththestrongestbaselines.
4.3 Generativemodelling
Problem Finally, we study the generative modelling of reconstructed events as an end-to-end
generationtask[16,17],bypassingthewholesimulationchainvisualizedinFig.1. Suchgenerative
models can obliterate the computational cost of both the theory computations and the detector
simulation at once. However, the high-dimensional distributions of reconstructed particles often
havenon-trivialkinematicfeaturesthatarechallengingforgenerativemodelstolearn,forinstance
thepropertiesofunstableresonancesandangularcorrelations. Wefocusontheprocessespp →
tt¯+njets,thegenerationoftoppairswithn=0...4additionaljets,wherethetopquarksdecay
hadronically,t→bq′q¯′′.
We train continuous normalizing flows based on an L-GATr network with the Riemannian flow
matchingobjectivedescribedinSec.3. Asbaselines,weconsidersimilarflowmatchingmodels,but
useMLPandTransformernetworksasscoremodels,asproposedbyRefs.[17,44].
Kinematicdistributions Webeginwithaqualitativeanalysisofthesamplesfromthegenerative
models. InFig.4weshowexamplemarginaldistributionsfromthedifferentmodelsandcompare
themtotheground-truthdistributioninthetestset. Weselectthreemarginalsthatarenotoriously
difficulttomodelcorrectlyforgenerativemodels. Whilethedifferencesaresubtleandonlyvisiblein
tailsandedgesofthedistributions,L-GATrmatchesthetruedistributionbetterthanthebaselines.
However,noneofthemodelsareabletocapturethekinematicsofthetopmasspeakatpercent-level
precisionyet.
Loglikelihood Next,weevaluatethegenerativemodelsquantitativelythroughtheloglikelihoodof
datasamplesunderthetrainedmodels;seeAppendixCfordetails. TheleftpanelofFig.5showsthat
theL-GATrmodelsoutperformbothbaselinesacrossalldifferentjetmultiplicities. Theymaintain
thisperformanceadvantagealsoforsmallertrainingdatasize,asshownintherightpanel.
Classifiertwo-sampletest Howclosetotheground-truthdistributionarethesegenerativemodels
really? Neithermarginaldistributionsnorloglikelihoodscoresfullyanswerthisquestion,asthe
8WealsoexperimentedwithtrainingasingleL-GATrmodeltolearntheamplitudesofallprocessesjointly,
findingasimilarperformance.
8
ledoM
ytisnedlanigraM
hturtdnuorG
ledoM
ytisnedlanigraM
hturtdnuorG
ledoM
ytisnedlanigraM
hturtdnuorGMLP MLP
Transformer Transformer
L-GATr L-GATr
26
−
5.4
− 28
−
30
−
32
− t¯t+0j
5.5
− t¯t+0j t¯t+1j t¯t+2j t¯t+3j t¯t+4j 104 105 106
Numberoftrainingsamples
Figure5: Generativemodelling: negativeloglikelihoodonthetestset(lowerisbetter). Left: Fordifferent
processes.Right:Asafunctionofthetrainingdatasetsize.Weshowthemeanandstandarddeviationofthree
randomseeds.TheL-GATrflowoutperformsthebaselinesinallprocessesandalltrainingsetsizes.
formerneglectmostofthehigh-dimensionalinformationandthelatterdonothaveaknownground-
truthvaluetocompareto. Wethereforeperformaclassifiertwo-sampletest[53]. WefindthatL-
GATrsamplesaredifficulttodistinguishfromtheground-truthdistribution: aclassifiertrainedto
discriminatethemachievesonlyaROCAUCofbetween0.51and0.56,dependingontheprocess.
Incontrast,TransformerandMLPdistributionsaremoreeasilydiscriminatedfromthebackground,
withROCAUCresultsbetween0.58and0.85. Fordetails,seeAppendixC.
EffectofRiemannianflowmatching Howimportantwas
Probabilitypaths NLL
ourchoiceofprobabilitypathsthroughRiemannianflowmatch-
ingfortheperformanceofthesemodels? InTbl.2wecompare Euclidean -30.11±0.98
theloglikelihoodofCFML-GATrmodelsthatdifferonlyin RFM -32.65±0.01
theprobabilitypaths. Clearly,theRiemannianflowmatching
Table 2: Benefit of Riemannian flow
approachthatallowsustoencodegeometricconstraintsiscru-
matching for generative models. We
cialforagoodperformance. Wefindsimilarlylargegainsfor
showthenegativeloglikelihoodonthe
allarchitectures. tt¯+0jtestset(lowerisbetter).
4.4 Computationalcostandscalability
Finally,webrieflycommentonL-GATr’scomputationalcost. ComparedtoavanillaTransformer,the
architecturehassomecomputationaloverheadbecauseofthemorecomplexlinearmaps. However,it
scalesexactlyinthesamewaytolargeparticlemultiplicities,wherebotharchitecturesarebottle-
neckedbythesamedot-productattentionmechanism. Atthesametime,L-GATrissubstantially
moreefficientthanequivariantarchitecturesbasedonmessagepassing,bothintermsofcomputeand
memory. Thisisbecausehigh-energyphysicsproblemsdonotlendthemselvestosparsegraphs,and
fordensegraphs,dot-productattentionismuchmoreefficient.SeeAppendixCforourmeasurements.
5 Discussion
Outofallareasofscience,high-energyphysicsisastrongcontenderforthefieldinwhichsymmetries
playthemostcentralrole. Surprisingly,whileparticlephysicistswerequicktoembracemachine
learning,architecturestailoredtothesymmetriesinherentinparticlephysicsproblemshavereceived
comparablylittleattention.
WeintroducedtheLorentzGeometricAlgebraTransformer(L-GATr),aversatilearchitecturewith
strong inductive biases for high-energy physics: its representations are based on particle four-
momenta,extendedtohigherordersinageometricalgebra,anditslayersareequivariantwithrespect
totheLorentzsymmetryofspecialrelativity. Atthesametime,L-GATrisaTransformer,andscales
favorablytolargecapacityandlargenumbersofinputtokens.
9
elcitraprepdoohilekilgolevitageN
doohilekilgolevitageNWedemonstratedL-GATr’sversatilityondiverseregression,classification,andgenerativemodelling
tasksfromtheLHCanalysisworkflow. Forthelatter,weconstructedthefirstLorentz-equivariant
generativemodelbasedonRiemannianflowmatching. Acrossallexperiments,L-GATrperformedas
wellasorbetterthanstrongbaselines.
Still, L-GATr has its limitations. While the architecture scales better than comparable message-
passingnetworks,ithassomecomputationaloverheadcomparedto,forinstance,efficientTransformer
implementations. AndwhileL-GATrshouldinprinciplebesuitableforpretrainingacrossmultiple
problems,wehavenotyetinvestigateditspotentialasafoundationmodel.
While the LHC is preparing for the high-luminosity runs and its legacy measurements, the high-
energyphysicscommunityisoptimizingallstepsoftheanalysispipeline. Deployingperformantand
data-efficientarchitecturessuchasL-GATrcouldimprovethispipelineinmanyplaces. Wehopethat
thiswillultimatelycontributetomoreprecisemeasurementsofnatureatitsmostfundamentallevel.
Acknowledgements
We would like to thank Taco Cohen for fruitful discussions and Nathan Hütsch for help with
conditionalflowmatching.
J.S.,V.B.andT.P.aresupportedbytheBaden-Württemberg-StiftungthroughtheprogramInterna-
tionaleSpitzenforschung,projectUncertainties—TeachingAIitsLimits (BWST_IF2020-010),the
DeutscheForschungsgemeinschaft(DFG,GermanResearchFoundation)undergrant396021762–
TRR257ParticlePhysicsPhenomenologyaftertheHiggsDiscovery,andthroughGermany’sEx-
cellenceStrategyEXC2181/1–390900948(theHeidelbergSTRUCTURESExcellenceCluster).
J.S.isfundedbytheCarl-Zeiss-StiftungthroughtheprojectModel-BasedAI:PhysicalModelsand
DeepLearningforImagingandCancerTreatment. V.B.issupportedbytheBMBFJuniorGroup
GenerativePrecisionNetworksforParticlePhysics (DLR01IS22079). J.T.issupportedbytheNa-
tionalScienceFoundationunderCooperativeAgreementPHY-2019786(TheNSFAIInstitutefor
ArtificialIntelligenceandFundamentalInteractions,http://iaifi.org/),bytheU.S.Department
ofEnergyOfficeofHighEnergyPhysicsundergrantnumberDE-SC0012567,andbytheSimons
FoundationthroughInvestigatorgrant929241.
10References
[1] Boosted jet identification using particle candidates and deep neural networks. 2017. URL
https://cds.cern.ch/record/2295725. (Citedonpage7)
[2] Georges Aad et al. Measurements of top-quark pair single- and double-differential cross-
√
sectionsintheall-hadronicchannelinppcollisionsat s=13TeVusingtheATLASdetector.
JHEP,01:033,2021. doi: 10.1007/JHEP01(2021)033. (Citedonpage19)
[3] JohanAlwall,MichelHerquet,FabioMaltoni,OlivierMattelaer,andTimStelzer. MadGraph5:
GoingBeyond. JHEP,06:128,2011. doi: 10.1007/JHEP06(2011)128. (Citedonpages17and19)
[4] ATLAScollaboration. ObservationofanewparticleinthesearchfortheStandardModelHiggs
bosonwiththeATLASdetectorattheLHC. PhysicsLettersB,716(1):1–29,2012. (Citedon
page1)
[5] JosephAylett-Bullock,SimonBadger,andRyanMoodie. Optimisingsimulationsfordiphoton
productionathadroncollidersusingamplitudeneuralnetworks.JournalofHighEnergyPhysics,
2021(8):1–30,2021. (Citedonpage6)
[6] Simon Badger and Joseph Bullock. Using neural networks for efficient evaluation of high
multiplicityscatteringamplitudes. JournalofHighEnergyPhysics,2020(6):1–26,2020. (Not
cited.)
[7] Simon Badger, Anja Butter, Michel Luchmann, Sebastian Pitz, and Tilman Plehn. Loop
amplitudesfromprecisionnetworks. SciPostPhysicsCore,6(2):034,2023. (Citedonpage6)
[8] EduardoBayro-Corrochano,SvenBuchholz,andGeraldSommer. Anewself-organizingneural
networkusinggeometricalgebra. InProceedingsof13thInternationalConferenceonPattern
Recognition,volume4,pages555–559.IEEE,1996. (Citedonpage16)
[9] Alexander Bogatskiy, Timothy Hoffman, David W. Miller, and Jan T. Offermann. Pelican:
Permutation equivariant and lorentz invariant or covariant aggregator network for particle
physics,2022. (Citedonpages2,3,and7)
[10] JohannesBrandstetter,RiannevandenBerg,MaxWelling,andJayeshKGupta. Cliffordneural
layersforPDEmodeling. arXiv:2209.04934,2022. (Citedonpage16)
[11] JohannBrehmerandKyleCranmer. Simulation-basedinferencemethodsforparticlephysics.
InArtificialIntelligenceforHighEnergyPhysics,pages579–611.WorldScientific,2022. (Cited
onpage1)
[12] JohannBrehmer,KyleCranmer,GillesLouppe,andJuanPavez. Constrainingeffectivefield
theorieswithmachinelearning.PhysicalReviewLetters,121(11):111801,2018.(Citedonpage1)
[13] JohannBrehmer,PimdeHaan,SönkeBehrends,andTacoCohen. GeometricAlgebraTrans-
former. InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,editors,Advances
inNeuralInformationProcessingSystems,volume37,2023. (Citedonpages2,3,5,16,and17)
[14] MichaelMBronstein,JoanBruna,TacoCohen,andPetarVelicˇkovic´. Geometricdeeplearning:
Grids,groups,graphs,geodesics,andgauges. 2021. (Citedonpage3)
[15] AnjaButter,GregorKasieczka,TilmanPlehn,andMichaelRussell. Deep-learnedtoptagging
withalorentzlayer. SciPostPhysics,5(3):028,2018. (Citedonpage7)
[16] AnjaButter,TheoHeimel,SanderHummerich,TobiasKrebs,TilmanPlehn,ArmandRousselot,
andSophiaVent. Generativenetworksforprecisionenthusiasts. SciPostPhys.,14(4):078,2023.
doi: 10.21468/SciPostPhys.14.4.078. (Citedonpages6and8)
[17] AnjaButter,NathanHuetsch,SofiaPalaciosSchweitzer,TilmanPlehn,PeterSorrenson,and
JonasSpinner. JetDiffusionversusJetGPT–ModernNetworksfortheLHC. 52023. (Citedon
pages6and8)
11[18] Anja Butter, Nathan Huetsch, Sofia Palacios Schweitzer, Tilman Plehn, Peter Sorrenson,
andJonasSpinner. Jetdiffusionversusjetgpt–modernnetworksforthelhc. arXivpreprint
arXiv:2305.10475,2023. (Citedonpage2)
[19] AnjaButter,TilmanPlehn,SteffenSchumann,SimonBadger,SaschaCaron,KyleCranmer,
FrancescoArmandoDiBello,EtienneDreyer,StefanoForte,SanmayGanguly,etal. Machine
learningandLHCeventgeneration. SciPostPhysics,14(4):079,2023. (Citedonpage1)
[20] Matteo Cacciari, Gavin P. Salam, and Gregory Soyez. The anti-k jet clustering algorithm.
t
JHEP,04:063,2008. doi: 10.1088/1126-6708/2008/04/063. (Citedonpage19)
[21] MatteoCacciari,GavinP.Salam,andGregorySoyez. FastJetUserManual. Eur.Phys.J.C,72:
1896,2012. doi: 10.1140/epjc/s10052-012-1896-2. (Citedonpage19)
[22] GiuseppeCarleo,IgnacioCirac,KyleCranmer,LaurentDaudet,MariaSchuld,NaftaliTishby,
Leslie Vogt-Maranto, and Lenka Zdeborová. Machine learning and the physical sciences.
ReviewsofModernPhysics,91(4):045002,2019. (Citedonpage1)
[23] CERNDataCentre. Keyfactsandfigures. URLhttps://information-technology.web.
cern.ch/sites/default/files/CERNDataCentre_KeyInformation_Nov2021V1.pdf.
(Citedonpage1)
[24] Ricky TQ Chen and Yaron Lipman. Riemannian flow matching on general geometries.
arXiv:2302.03660,2023. (Citedonpages2,4,and6)
[25] RickyTQChen,YuliaRubanova,JesseBettencourt,andDavidKDuvenaud. Neuralordinary
differentialequations. AdvancesinNeuralInformationProcessingSystems,31,2018. (Citedon
pages4and20)
[26] XiangningChen,ChenLiang,DaHuang,EstebanReal,KaiyuanWang,YaoLiu,HieuPham,
XuanyiDong,ThangLuong,Cho-JuiHsieh,YifengLu,andQuocV.Le. Symbolicdiscovery
ofoptimizationalgorithms,2023. (Citedonpage19)
[27] MCNCheng,VAnagiannis,MWeiler,andothers. Covarianceinphysicsandconvolutional
neuralnetworks. arXivpreprintarXiv,2019. URLhttps://arxiv.org/abs/1906.02481.
(Citedonpage3)
[28] WilliamKingdonClifford. ApplicationsofGrassmann’sExtensiveAlgebra. Amer.J.Math.,1
(4):350–358,1878. (Citedonpage3)
[29] CMScollaboration.Observationofanewbosonatamassof125GeVwiththeCMSexperiment
attheLHC. PhysicsLettersB,716(1):30–61,2012. (Citedonpage1)
[30] TacoCohen. EquivariantConvolutionalNetworks. PhDthesis,UniversityofAmsterdam,2021.
(Citedonpage3)
[31] KyleCranmer,JohannBrehmer,andGillesLouppe. Thefrontierofsimulation-basedinference.
ProceedingsoftheNationalAcademyofSciences,117(48):30055–30062,2020. (Citedonpage1)
[32] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast
and memory-efficient exact attention with IO-awareness. Advances in Neural Information
ProcessingSystems,35:16344–16359,2022. (Citedonpages2and5)
[33] Ranit Das, Luigi Favaro, Theo Heimel, Claudius Krause, Tilman Plehn, and David Shih.
Howtounderstandlimitationsofgenerativenetworks. SciPostPhys.,16(1):031,2024. doi:
10.21468/SciPostPhys.16.1.031. (Citedonpage21)
[34] J.deFavereau,C.Delaere,P.Demin,A.Giammanco,V.Lemaître,A.Mertens,andM.Selvaggi.
DELPHES3,Amodularframeworkforfastsimulationofagenericcolliderexperiment. JHEP,
02:057,2014. doi: 10.1007/JHEP02(2014)057. (Citedonpage19)
[35] PimDeHaan,TacoCohen,andJohannBrehmer. Euclidean,projective,conformal: Choosing
ageometricalgebraforequivarianttransformers. InSanjoyDasgupta, StephanMandt, and
YingzhenLi,editors,ProceedingsofThe27thInternationalConferenceonArtificialIntelligence
andStatistics,volume238ofProceedingsofMachineLearningResearch,pages3088–3096.
PMLR,02–04May2024. (Citedonpages2,3,5,and16)
12[36] LukedeOliveira,MichaelKagan,LesterMackey,BenjaminNachman,andArielSchwartzman.
Jet-images—deeplearningedition. JournalofHighEnergyPhysics,2016(7):1–32,2016. (Cited
onpage2)
[37] CDoranandALasenby. Geometricalgebraforphysicists. CambridgeUniversityPress,2003.
(Citedonpage16)
[38] AlbertEinstein. ZurElektrodynamikbewegterKörper. AnnalenderPhysik,4,1905. (Citedon
pages2and3)
[39] MarcFinzi,MaxWelling,andAndrewGordonWilson. Apracticalmethodforconstructing
equivariant multilayer perceptrons for arbitrary matrix groups. April 2021. URL http:
//arxiv.org/abs/2104.09459. (Citedonpage16)
[40] SheldonLGlashow. Partial-symmetriesofweakinteractions. Nuclearphysics,22(4):579–588,
1961. (Citedonpage3)
[41] Shiqi Gong, Qi Meng, Jue Zhang, Huilin Qu, Congqiao Li, Sitian Qian, Weitao Du, Zhi-
Ming Ma, and Tie-Yan Liu. An efficient lorentz equivariant graph neural network for jet
tagging. Journal of High Energy Physics, 2022(7), July 2022. ISSN 1029-8479. doi: 10.
1007/jhep07(2022)030. URLhttp://dx.doi.org/10.1007/JHEP07(2022)030. (Citedon
pages2,3,and7)
[42] HermannGrassmann. DielinealeAusdehnungslehre. OttoWigand,Leipzig,1844. (Citedon
page3)
[43] DanGuest,KyleCranmer,andDanielWhiteson. DeeplearninganditsapplicationtoLHC
physics. AnnualReviewofNuclearandParticleScience,68:161–181,2018. (Citedonpage1)
[44] TheoHeimel,NathanHuetsch,RamonWinterhalder,TilmanPlehn,andAnjaButter. Precision-
MachineLearningfortheMatrixElementMethod. 102023. (Citedonpages6and8)
[45] DanHendrycksandKevinGimpel. Gaussianerrorlinearunits(gelus). arXiv:1606.08415,2016.
(Citedonpage5)
[46] EmielHoogeboom,VíctorGarciaSatorras,ClémentVignac,andMaxWelling. Equivariantdif-
fusionformoleculegenerationin3D.InKamalikaChaudhuri,StefanieJegelka,LeSong,Csaba
Szepesvari,GangNiu,andSivanSabato,editors,Proceedingsofthe39thInternationalConfer-
enceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pages
8867–8887.PMLR,2022. URLhttps://proceedings.mlr.press/v162/hoogeboom22a.
html. (Citedonpage6)
[47] NathanHuetschetal. TheLandscapeofUnfoldingwithMachineLearning. 42024. (Citedon
page6)
[48] GregorKasieczka,TilmanPlehn,AnjaButter,KyleCranmer,DipsikhaDebnath,BarryMDillon,
MalcolmFairbairn,DariusAFaroughy,WojtekFedorko,ChristopheGay,etal. Themachine
learninglandscapeoftoptaggers. SciPostPhysics,7(1):014,2019. (Citedonpages1,7,8,and18)
[49] GregorKasieczka,TilmanPlehn,JenniferThompson,andMichaelRussel. Topquarktagging
referencedataset,March2019. URLhttps://doi.org/10.5281/zenodo.2603256. (Cited
onpages7,8,and18)
[50] PatrickTKomiske,EricMMetodiev,andJesseThaler. Energyflownetworks: deepsetsfor
particlejets. JournalofHighEnergyPhysics,2019(1):1–46,2019. (Citedonpage7)
[51] AndrewJLarkoski,IanMoult,andBenjaminNachman. JetsubstructureattheLargeHadron
Collider: areviewofrecentadvancesintheoryandmachinelearning. PhysicsReports,841:
1–63,2020. (Citedonpage1)
[52] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow
matchingforgenerativemodeling. arXiv:2210.02747,2022. (Citedonpage4)
[53] DavidLopez-PazandMaximeOquab. Revisitingclassifiertwo-sampletests. arXivpreprint
arXiv:1610.06545,2016. (Citedonpage9)
13[54] IlyaLoshchilovandFrankHutter. SGDR:stochasticgradientdescentwithrestarts. CoRR,
abs/1608.03983,2016.URLhttp://arxiv.org/abs/1608.03983. (Citedonpages18and19)
[55] PerttiLounesto. CliffordAlgebrasandSpinors. LondonMathematicalSocietyLectureNote.
CambridgeUniversityPress,2001. (Citedonpage16)
[56] SebastianMacalusoandKyleCranmer. TreeNetworkinNetwork(TreeNiN)forjetphysics,
2019. URL https://github.com/SebastianMacaluso/TreeNiN. DOI: 10.5281/zen-
odo.2582216. (Citedonpage7)
[57] D Maître and H Truong. One-loop matrix element emulation with factorisation awareness.
JournalofHighEnergyPhysics,2023(5):1–21,2023. (Citedonpage6)
[58] DanielMaîtreandHenryTruong. Afactorisation-awarematrixelementemulator. Journalof
HighEnergyPhysics,2021(11):1–24,2021. (Citedonpage6)
[59] HermannMinkowski.DieGrundgleichungenfürdieelektromagnetischenVorgängeinbewegten
Körpern. NachrichtenvonderGesellschaftderWissenschaftenzuGöttingen,Mathematisch-
PhysikalischeKlasse,1908:53–111,1908. (Citedonpage3)
[60] LiamMoore,KarlNordström,SreedeviVarma,andMalcolmFairbairn. Reportsofmydemise
aregreatlyexaggerated: n-subjettinesstaggerstakeonjetimages. SciPostphysics,7(3):036,
2019. (Citedonpage7)
[61] HenriPoincaré. Surladynamiquedel’électron. CircoloMatematicodiPalermo,1906. (Cited
onpages2and3)
[62] HuilinQuandLoukasGouskos. Jettaggingviaparticleclouds. PhysicalReviewD,101(5),
March2020. ISSN2470-0029. doi: 10.1103/physrevd.101.056019. URLhttp://dx.doi.
org/10.1103/PhysRevD.101.056019. (Citedonpage7)
[63] HuilinQu,CongqiaoLi,andSitianQian. Particletransformerforjettagging,2024. (Citedon
page7)
[64] Alexander Radovic, Mike Williams, David Rousseau, Michael Kagan, Daniele Bonacorsi,
AlexanderHimmel,AdamAurisano,KazuhiroTerao,andTaritreeWongjirad.Machinelearning
attheenergyandintensityfrontiersofparticlephysics. Nature,560(7716):41–48,2018. (Cited
onpage1)
[65] Martin Roelfs and Steven De Keninck. Graded symmetry groups: plane and simple.
arXiv:2107.03771,2021. (Citedonpage3)
[66] David Ruhe, Johannes Brandstetter, and Patrick Forré. Clifford group equivariant neural
networks. InAdvancesinNeuralInformationProcessingSystems,volume37,2023. (Citedon
pages2,3,7,16,17,18,and21)
[67] DavidRuhe, JayeshKGupta, StevendeKeninck, MaxWelling, andJohannesBrandstetter.
Geometriccliffordalgebranetworks. InInternationalConferenceonMachineLearning,2023.
(Citedonpage16)
[68] GavinP.Salam. TowardsJetography. Eur.Phys.J.C,67:637–686,2010. doi: 10.1140/epjc/
s10052-010-1314-6. (Citedonpage3)
[69] A.M.Sirunyanetal. Particle-flowreconstructionandglobaleventdescriptionwiththeCMS
detector. JINST,12(10):P10003,2017. doi: 10.1088/1748-0221/12/10/P10003. (Citedonpage3)
[70] TorbjörnSjöstrand,StefanAsk,JesperR.Christiansen,RichardCorke,NishitaDesai,Philip
Ilten, Stephen Mrenna, Stefan Prestel, Christine O. Rasmussen, and Peter Z. Skands. An
introductiontoPYTHIA8.2. Comput.Phys.Commun.,191:159–177,2015. doi: 10.1016/j.cpc.
2015.01.024. (Citedonpage19)
[71] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. AdvancesinNeuralInformation
ProcessingSystems,30,2017. (Citedonpages3,5,7,and19)
14[72] SoledadVillar,DavidWHogg,KateStorey-Fisher,WeichiYao,andBenBlum-Smith. Scalars
are universal: Equivariant machine learning, structured like classical physics. Advances in
NeuralInformationProcessingSystems,34:28848–28863,2021. (Citedonpage3)
[73] StevenWeinberg. Amodelofleptons. PhysicalReviewLetters,19(21):1264,1967. (Citedon
page3)
[74] RuibinXiong,YunchangYang,DiHe,KaiZheng,ShuxinZheng,ChenXing,HuishuaiZhang,
YanyanLan,LiweiWang,andTieyanLiu.Onlayernormalizationinthetransformerarchitecture.
InInternationalConferenceonMachineLearning,pages10524–10533.PMLR,2020. (Citedon
page5)
[75] ManzilZaheer,SatwikKottur,SiamakRavanbakhsh,BarnabasPoczos,RussRSalakhutdinov,
andAlexanderJSmola. Deepsets. Advancesinneuralinformationprocessingsystems,30,
2017. (Citedonpages7and17)
15A Geometricalgebra
Geometricalgebrasaremathematicalobjectsthatwereinitiallyusedforphysics. Althoughtheyhave
beenusedinmachinelearningfordecades[8],theyhaveseenarecentuptickinpopularity[10,13,
37,55,66,67]. Inthissection,wewillintroducegeometricalgebrasandtherelevantconcepts.
Analgebraisavectorspacethatisequippedwithanassociativebilinearproduct. Givenavector
space V with a symmetric bilinear inner product, we can construct an algebra G(V), called the
geometricorCliffordalgebra,inthefollowingway: chooseanorthogonalbasise oftheoriginal
i
d-dimensionalvectorspaceV. Then,thealgebrahas2ddimensionswithabasisgivenbyelements
e e ...e =: e ,with1 ≤ j < j < ... < j ≤ d,0 ≤ k ≤ d. Forexample,forV = R3,
j1 j2 jk j1j2...jk 1 2 k
withorthonormalbasise ,e ,e ,abasisforthealgebraG(R3)is
1 2 3
1,e ,e ,e ,e ,e ,e ,e . (5)
1 2 3 12 13 23 123
An algebra element spanned by basis elements with k indices is called a k-vector or a vector of
gradek. Agenericelementwhosebasiselementscanhavevaryinggradesiscalledamultivector. A
multivectorxcanbeprojectedtoak-vectorwiththegradeprojection⟨x⟩ .
k
Theproductonthealgebra,calledthegeometricproduct,isdefinedtosatisfye e =−e e ifi̸=j
i j j i
ande e =⟨e ,e ⟩,whichbybilinearityandassociativityfullyspecifiesthealgebra.Givenanalgebra
i i i i
G(V),thereisagroupPin(V)thatisgeneratedbythe1-vectorsinthealgebrawithnorm±1,and
whosegroupproductisthegeometricproduct. Thisgrouphasalinearactionρ:Pin(V)×G(V)→
G(V)onthealgebradefinedsuchthatforanyunit1-vectoru∈Pin(V)and1-vectorx∈G(V)
ρ(u,x)=−uxu−1. (6)
Theactionisdefinedtobeanalgebrahomomorphism, meaningthatforanyu ∈ Pin(V),x,y ∈
G(V),ρ(u,xy)=ρ(u,x)ρ(u,y).Also,itisagroupaction,meaningthatforanytwogroupelements
u,v ∈Pin(V),ρ(uv,x)=ρ(u,ρ(v,x)).AsthegroupPin(V)isgeneratedbyproductsof1-vectors,
andthealgebraG(V)isgeneratedbylinearcombinationsandgeometricproducts,thisfullyspecifies
theactionρ.
Space-timegeometricalgebra Inthispaper,weusethegeometricalgebraG =G(R1,3)based
1,3
onfour-dimensionalMinkowskispaceR1,3,whichhasanorthogonalbasiswithonebasisvectore
0
satisfying⟨e ,e ⟩=+1andfori=1,2,3abasisvectore satisfying⟨e ,e ⟩=−1. ThePingroup
0 0 i i i
Pin(R1,3)isadoublecoveroftheLorentzgroupO(1,3). Aswedonotrequireequivariancetotime
reversals or spatial mirrorings, we are only interested in equivariance to the connected subgroup
SO+(1,3).
Equivariance Thefactthatρisanalgebrahomomorphismisequivalenttosayingthatthegeomet-
ricproductisequivarianttoPin(V). Furthermore,thegradesinageometricalgebraformsubrep-
resentations[13,Prop.2]. Thus,thegradeprojectionsareequivariant. Thepseudoscalarisaone-
dimensionalrealrepresentation,andthusmustbeinvarianttoanyconnectedsubgroupofPin(V).
Therefore,multiplyingbythepseudoscalarisequivarianttotheconnectedgroupSO+(1,3). Hence,
thelinearlayerinEq.(1)isequivariant,foranyvalueoftheparametersv,w.
Toshowthatthisformsacompletebasisofallequivariantlinearmaps,weusethenumericalapproach
ofDeHaanetal.[35],basedonFinzietal.[39]. Numerically,wefinda10-dimensionalspaceof
equivariantmaps,indicatingthebasisinEq.(1)iscomplete,whichprovesProp.1.
Expressivity A fortiori, De Haan et al. [35] showed that for several geometric algebras, any
equivariantmapG(V)×...×G(V) → G(V)thatisapolynomialfunctionofthecoefficientsof
G(V),canbeexpressedaslinearcombinationsofgradeprojections,geometricproductsandinvariant
multivectors,andisthusexpressiblebyGATr. Thisargumentholdsforanygeometricalgebrathatis
basedonavectorspaceV withanon-degeneratemetric,whichisthecasefortheMinkowskispace
R1,3. Hence,thisexpressivityargumentcanbeextendedtoL-GATr: theoperationsinitsMLPsare
abletoexpressanypolyomialmapofG mutlivectors.
1,3
B Architectures
LorentzGeometricAlgebraTransformer(L-GATr) OurmaincontributionistheL-GATrarchi-
tecture,describedindetailinSec.3. OurimplementationisinpartbasedontheGeometricAlgebra
16Transformer[13]codeinversion1.0.0.9 Unlike[13],weusemulti-headattention,notmulti-query
attention.
Cliffordgroupequivariantneuralnetwork(CGENN) WeusetheCGENNarchitecture[66]as
abaseline. Weusetheofficialimplementation10andadapttheirtop-taggingcodetoouramplitude
regressionexperiments.
Deep Sets with invariants (DSI) We build a new architecture based on the Deep Sets frame-
work[75]fortacklingtheamplitudesurrogatetask. DeepSetsisapermutation-invariantarchitec-
ture that applies the same function to each element of an input set, aggregates the results with a
permutation-invariantoperationlikeasum,andprocessestheoutputswithanotherfunction.
OuradaptationfortheamplituderegressiontasksappliestheDeepSetsapproachtoeachsubsetof
identicalparticlesintheinputs,astheamplitudesaremanifestlyinvariantunderpermutationsofthe
four-momentaofparticlesofthesametype. Wethusapplyadifferentpreprocessingtoeachparticle
typeandaggregateseparatelyforeachparticletype. Inadditiontotheparticle-specificlatentspace
samples,theinputtoourmainnetworkalsoincludesthemomentuminvariantsforalltheparticles
involvedintheprocess. DSIthuscombinesaLorentz-equivariant,permutation-equivariantpathwith
anon-Lorentz-equivariantnon-permutation-equivariantpath,allowingthenetworktolearnwhether
torelyonequivariantornon-equivariantfeatures.
BothpreprocessingunitsandthemainnetworkareimplementedasMLPswithGELUnonlinearities.
Usingthismodel,weareabletoobtainoptimalperformanceforsimpleinteractions,butweobserve
apoorscalingbehaviorforpredictionqualityasweincreaseparticlemultiplicity.
GeometricAlgebraPerceptron(GAP) ToablatetowhatextentL-GATr’sperformanceisdue
tothegeometricalgebrarepresentationsanditsequivariancepropertiesandtowhatextentdueto
theTransformerarchitecture,weuseL-GATr’sMLPblockasastandalonenetwork. Wecallthis
“GeometricAlgebraPerceptron”(GAP).Insteadofstructuringourdataasasetoftokens,weformat
theparticledataasasinglelistofchannels. Thisimpliesthatinteractionsbetweenparticleswithinthe
modelwillbecarriedoutinthelinearlayers,asopposedtotheattentionmechanismweuseinL-GATr.
Transformer OrthogonallytoGAP,wealsouseavanillaTransformerasabaselineinourexperi-
ments. Weuseaper-LayerNormTransformerwithmulti-headattentionandGELUnonlinearities.
ThissetupmirrorsthatofL-GATrascloselyaspossible.
Multilayerperceptron(MLP) TheMLPrepresentsoursimplestbaseline,formulatedasastackof
linearlayerswithGELUnonlinearities.
C Experimentdetails
C.1 SurrogatesforQFTamplitudes
Dataset We generate training and evaluation data consisting of phase space inputs and their
corresponding interaction amplitudes for processes qq¯ → Z +ng, n = {1,4}, where an initial
quark-antiquarkpairinteracttoproduceaZ bosonandavariablenumberofgluons. Theamplitudes
are invariant under the permutation of any identical particles and under Lorentz transformations.
These datasets are generated by the MadGraph Monte Carlo event generator [3] in two steps.11
First, we use a standard run to generate the phase space distributions. This standard run applies
importancesamplingtoproduceunweightedsamples,thatis,eventsthataredistributedaccording
totheprobabilitydistributionthatdescribesthephysicalinteractions. Second,were-computethe
amplitudevaluescorrespondingtothesephase-spacesampleswithMadGraph’sstandalonemodule.
Weproducefourdatasets,eachwithadifferentnumberofgluons. Eachdatasetconsistsof4×105
9Available at https://github.com/Qualcomm-AI-research/geometric-algebra-transformer
underaBSD-3-Clause-Clearlicense.
10Availableathttps://github.com/DavidRuhe/clifford-group-equivariant-neural-networks
underaMITlicense.
11Availableathttps://launchpad.net/mg5amcnlounderaUoI-NCSAopensourcelicense.
17samplesfortraining,105forvalidation,and5×105fortesting. Oureventsetsfeaturekinematiccuts
inthetransversemomentumoftheoutgoingparticles(p >20GeV)andontheangulardistance
T
(cid:112)
betweenthegluons(∆R= ∆η2+∆ϕ2 >0.4).
Forthelearningproblem,weaffinelynormalizetheamplitudesytozeromeanandvarianceone:
log(y )−log(y )
yˆ = i i . (7)
i σ
log(yi)
Models For the L-GATr model, we embed eachparticle as a token. It ischaracterized with its
four-momentum,embeddedasagrade-1multivectorinthegeometricalgebra,aswellasaone-hot
embeddingoftheparticletype,embeddedasascalar. Westandardizethefour-momentuminputs
tounitvariance, usingthesamenormalizationforeachcomponentinordertonotbreakLorentz
equivariance. Inadditiontotheparticletokensweuseone“global”token,initializedtozero. After
processingtheseinputswithanL-GATrnetwork,weselectthescalarcomponentoftheglobaltoken
and identify it as the amplitude output. We use 8 attention blocks, 32 multivector and 32 scalar
channels,and8attentionheads,resultingin1.8×106learnableparameters.
FortheCGENN,weminimallyalterthegraphneuralnetworkversionofthemodelbuiltforthe
top tagging classification task so that it is able to perform amplitude regression. We keep the
hyperparameters proposed by [66] and use 72 hidden node features, 8 hidden edge features, and
4 blocks. This model features around 3.2×105 trainable parameters. Scaling up the size of the
CGENNsothatitmatchestheparametercountinourL-GATrbuilddoesnottranslateintoanyvisible
improvementintheamplitudepredictions.
FortheGAPweusethesameprocedureaswithL-GATrtoembed(andpreprocess)theinputsand
extracttheoutputs,theonlydifferenceisthatthedifferentparticlesinagiveneventaredistributed
asindividualchannelsintheinput. Thismodelconsistsof8blocks,96multivectorand96scalar
channels,resultingin2.5×106learnableparameters.
FortheTransformerbaseline,weagainincludeparticletokensthroughaone-hotembedding. Inthis
casetheinputsxarepreprocessedbyperformingstandarization,definedas
x −x
xˆ = i i, (8)
i σ
xi
wherethemeanandthestandarddeviationarecomputedovereachparticleinputseparately. Asfor
thenetworkstructure,weuse8attentionblocks,128hiddenchannelsand8attentionheads,resulting
in1.3×106learnableparameters.
FortheDSI,weimplementinputstandarizationinthesamewaywedowiththeTransformer,butwe
alsoapplythesametransformationtoeachofthemomentuminvariantinputsseparately. Asforthe
layerstructure,allMLPmoduleshave4layerswith128hiddenchannelseach,andwesetupthe
preprocessingunitssothattheyoutput64-dimensionallatentspacesamples. Allinall,weendup
with2.6×105parametersintotal.
For the MLP, we once again apply standarization, this time over the whole input. The network
consistsof5layersand128hiddenchannelsamountingto7×104learnableparameters.
Training Allmodelsaretrainedbyminimizingameansquarederror(MSE)lossontheprepro-
cessedamplitudetargetsandbymakinguseoftheAdamoptimizer. Weuseabatchsizeof256and
afixedlearningrateof10−4 forallbaselinesexceptCGENN,whichwetrainwithabatchsizeof
32duetomemoryrestrictionsandtheCosineAnnealingscheduler[54]withamaximumlearning
rateof10−4. Asforthenumberoftrainingsteps,MLPandDSIaretrainedforaround2.5×106
iterations,theTransformerforaround106iterationsandGAP,CGENNandGATrfor2.5×105itera-
tions. WeusenoregularizationmethodforanyofourbaselinesexceptforCGENN,whichincludes
bydefaultadropoutof0.2attheoutputlayer. Weuseearlystoppingacrossalltrainingruns.
C.2 Toptagging
Dataset WeusethereferencetopquarktaggingdatasetbyKasieczkaetal.[48,49].12 Thedata
samplesarestructuredaspointclouds,witheacheventsimulatingameasurementbytheATLAS
12Availableathttps://zenodo.org/records/2603256underaCC-BY4.0license.
18experimentatdetectorlevel. Signalsamplesoriginatefromthedecayofatopquark,whiletherestof
theeventsaregeneratedbystandardbackgroundprocesses. Thedatasetconsistsof1.2×106events
fortrainingand4×105eachforvalidationandtesting.
Models Asthetop-taggingdatasetsoperatewithreconstructedparticlesasmeasuredbyadetector,
weincludetheprotonbeamdirectionasaninputtothenetwork,whichpartiallybreakstheLorentz
equivarianceoftheprocess. Weuseadditionalchannelswithspacelikefour-momentum(0,0,0,1),
butfindthatotherchoicesleadtocomparableresults.
Otherwise,weusethesamesetupasintheamplituderegressiontask. Weuse12attentionblocks,16
multivectorand32scalarchannelsand8attentionheads,resultingin1.1×106learnableparameters.
Training L-GATristrainedbyminimizingabinarycrossentropy(BCE)lossonthetopquark
labels. Wetrainitfor4×105stepsusingtheEvoLvedSignMomentum(LION)[26]optimizerwith
aweightdecayof0.2andabatchsizeof128. WeuseaCosineAnnealingscheduler[54]withthe
maximumlearningratesetat3×10−4.
C.3 Generativemodelling
Dataset Thett¯+njets,n=0...4datasetissimulatedwiththeMadGraph3.5.1[3]eventgeneration
toolchain,consistingofMadEvent[3]fortheunderlyinghardprocess,Pythia8[70]fortheparton
shower,Delphes3[34]forafastdetectorsimulation,andtheanti-k jetreconstructionalgorithm[20]
T
withR=0.4asimplementedinFASTJET[21]. ThePythiasimulationdoesnotincludemulti-parton
interactions. WeusetheATLASdetectorcardfortheDelphesdetectorsimulation,applythestandard
(cid:112)
phasespacecutsp >20GeV,∆R= ∆ϕ2+∆η2 <0.4andrequire2b-taggedjets. Theevents
T
arereconstructedwithaχ2-basedreconstructionalgorithm[2],andidenticalparticlesareorderedby
p .
T
The sizes of the tt¯+n jets, n = 0...4 datasets reflect the frequency of the respective processes,
resultingin9.8×106(n=0),7.2×106(n=1),3.7×106(n=2),1.5×106(n=3)and4.8×105
(n = 4)events. Oneachdataset,1%ofthesamplesaresetasideasvalidationandtestsplit. We
rescalethefour-momentapbythestandarddeviationofallfivedatasets206.6GeVforprocessing
withneuralnetworks.
Models TheL-GATrscorenetworkoperatesinMinkowskispacep = (E,p ,p ,p ), whereas
x y z
flowmatchinghappensinthephysicallymotivatedcoordinatesy =(y ,y ,η,ϕ)definedinEq.(4).
m p
Aftertransformingyintop,weembedeachparticlepintogeometricalgebrarepresentations. We
use scalar channels for the one-hot-encoded particle type and the flow time, for which we use a
sinusoidalembeddingwith8channels[71]. Weaddextramultivectorchannelsassymmetry-breaking
inputs,inparticularthetimedirection(1,0,0,0)asarank-1multivector,andtheplaneorthogonal
to the beam direction encoded as a rank-2 multivector. The time direction is required to break
thenon-compactspecialorthochronousLorentzgroupSO+(1,3)downtothecompactsubgroup
SO(3),asdiscussedinSec.3. TheoutputoftheL-GATrnetworkisavectorfieldinMinkowski
space(v ,v ,v ,v ). Toobtainvectorfieldsinthemanifoldcoordinatesy,wemultiplywiththe
E px py pz
Jacobiansofthetransformationp→ytoobtain(v ,v ,v ,v ). Becauseofthelogarithminthe
ym yp η ϕ
changeofvariables,theJacobiansfory ,y cantakeonlargevalues,whichmakestrainingunstable.
m p
Toavoidthiscomplication,weextractv˜ ,v˜ directlyfromthescalaroutputchannelsofL-GATr
ym yp
andusethesevaluesasvectorfieldv ,v . Likethesymmetry-breakinginputs, thisprocedure
ym yp
breakstheLorentzsymmetrydowntotheresidualsymmetrygroupofthemeasurementprocess. We
finditbeneficialtoperformthetransformationy ↔pat64-bitfloating-pointprecision,whichhasno
noticableeffectonthecomputationalcost. WeuseanL-GATrnetworkwith16multivectorchannels,
32scalarchannels,6L-GATrblocks,and8attentionheads,totalling5.4×105learnableparameters.
TheTransformerscorenetworkdirectlyoperatesinthephysicallymotivatedyspace. Eachparticleis
embeddedintoonetoken,with4channelsforthecomponentsofy,8channelsfortheflowtimeina
sinusoidalembedding[71],andtheone-hot-encodedparticletype. Thenetworkhas108channels,6
Transformerblocks,and8attentionheads,totallingto5.7×105learnableparameters.
TheMLPscorenetworkalsooperatesinyspace. Eacheventisembeddedasalistofthecomponents
of y, together with the time embedded with Gaussian Fourier Projection using 8 channels. The
networkhas336channelsand6blocks,with5.9×105learnableparameters.
191.0 1.0
MLP MLP
Transformer Transformer
0.9 L-GATr 0.9 L-GATr
0.8 0.8
0.7 0.7
0.6 0.6
t¯t+0j
0.5 0.5
t¯t+0j t¯t+1j t¯t+2j t¯t+3j t¯t+4j 104 105 106
Numberoftrainingsamples
Figure6: Generativemodelling: classifiertwo-sampletests. Weshowhowwellaclassifiercandiscriminate
modelsamplesfromtestsamples,measuredthroughtheareaundertheROCcurve(lowerisbetter,0.5isideal).
Left:Fordifferentprocesses.Right:Asafunctionofthetrainingdatasetsize.Weshowthemeanandstandard
deviationofthreerandomseeds.TheL-GATrflowoutperformsthebaselinesinallprocessesandalltrainingset
sizes.
Training All networks are trained for 2×105 iterations with batchsize 2048 using the Adam
optimizerwithdefaultsettingsandaninitiallearningrateof0.001. Weevaluatethevalidationloss
every103 iterationsanddecreasethelearningratebyafactorof10afternoimprovementsfor20
validationsteps. Westoptrainingafterthevalidationlosshasnotimprovedafter50validationsteps,
andpickthebestnetworkwiththebestvalidationloss.
Base distribution The base distribution is defined in the rescaled Minkowski space discussed
above. WeuseunitGaussiansforthespatialmomentump ∼N(0,1)andthelog-transformed
x,y,z
squaredmassy =logm2 ∼N(0,1). Weensuretheconstraintsp >20GeV,∆R>0.4through
m T
rejectionsampling. Wehaveexperimentedwithotherbasedistributionsandfindsimilarperformance.
Probabilitypaths ThetargetprobabilitypathsforRCFMlinearlychangethephysicallymotivated
coordinatesy =(y ,y ,η,ϕ)definedinEq.(4). Weusepseudorapidityηinsteadoftruerapidity,
m p
since this is easier to implement given the cut on transverse momentum p . We use a constant
T
diagonal metric, with the squared inverse standard deviation of these coordinates in the training
datasetonthediagonal. Thisisequivalenttoastandardizationstep. Weconstructperiodictarget
vectorfieldsforangularcoordinatesϕbyaddingfactorsof2πuntilangularcoordinatesandangular
velocitiesendupintheinterval[−π,π].
Negativelog-likelihood(NLL)metric ForanysamplepinMinkowskispace,weevaluatethelog-
densityofthetransformedsampleyusingtheinstantaneouschangeofvariables[25]. Inotherwords,
wesolvetheODE
(cid:18) (cid:19) (cid:18) (cid:19)
d x v (x)
t = t (9)
dt f t(x) −div(v t)(x t)
withtheinitialconditionsx = y,f (x ) = 0. WeusetheHutchinsontraceestimatortoevaluate
1 1 1
thedivergenceofthevectorfield. UsingthebasedensityP ,wethenevaluatethedensityP of
0 model
CFM-generatedsamplesinMinkowskispaceas
∂p ∂y
−logP (p)=−logP (x )+f (x )+logdet 0 +logdet 1 . (10)
model 0 0 0 0 ∂y ∂p
0 1
Thelasttwotermsarethelogarithmsofthejacobiandeterminantsforthetransformationsbetween
Minkowskispaceandthephysicallymotivatedspace.
20
CUArefiissalclarueN CUArefiissalclarueNTransformer
GNN
L-GATr
102
101
100
100 101 102 103 104 105
Numberofitems
Figure7: Inferencecost(wall-timeperforwardpass)asafunctionofthenumberofparticles. Wecompare
L-GATr,aTransformer,andamessage-passinggraphneuralnetwork(weuseCGENN[66]butexpectsimilar
resultsforotherarchitectures).Thelatterrunsoutofmemorywhenevaluatingmorethanafewhundredparticles.
Whilewedoourbesttofindcomparablesettings,suchcomparisonsdependonalotofchoicesandshould
beinterpretedwithcare. Nevertheless,webelievetheyillustratethatL-GATrscalestolargesystemslikea
Transformer,thankstoitbeingbasedondot-productattention.
Classifier two-sample test We train a MLP classifier to distinguish generated events from the
groundtruth. Theclassifierinputsarefulleventsintheyrepresentation,togetherwithchallenging
correlations, in particular all pairwise ∆R values, and the y representations of the reconstructed
intermediate particles t,t¯,W+,W−. The classifier network has 256 channels and 3 layers. It is
trained for 500 epochs with batchsize 1024, a dropout rate of 0.1, and the Adam optimizer with
defaultsettings. Westartwithaninitiallearningrateof0.0003anddecreasethelearningrateafter
no improvements in the validation loss for 5 epochs. We stop training after 10 epochs without
improvementsinthevalidationlossandloadthebest-validationmodelafterwards. Weusethefull
groundtruthdatasetaswellas1Mgeneratedeventsandsplitinto80%fortrainingand10%eachfor
testingandvalidation.WeusetheAUCofthisclassifierevaluatedonthetestdatasetasascalarmetric,
withthevalue0.5foraperfectgenerator. Neuralclassifiersapproximatetheevent-wiselikelihood
ratiop (x)/p (x)ofsingleevents,whichisthemostpowerfulteststatisticaccordingtothe
data model
Neyman-Pearsonlemma,andopensmanywaystofurtherstudytheperformanceofthegenerator
beyondscalarmetrics[33].
OurresultsareshowninFig.6. Acrosstrainingdatasizesandprocesses,theL-GATrflowsaremore
difficulttodistinguishfromthetestsamplesthanthebaselines.
C.4 Computationalcostandscalability
InFig.7wecompareL-GATrtoamessage-passinggraphneuralnetwork(weuseCGENN[66])and
avanillaTransformerintermsoftheirtest-timecomputationalcosts. Forthiscomparison,weuse
smallversionsofallarchitectures,consistingofasinglemodelblockandaround9×104learnable
parameters. InthecaseoftheTransformerandL-GATr,wefixtheirlayerstructuresothatinputs
goingintotheattentionlayerconsistof72channels. Ourmeasurementsareperformedwithdatasets
madeupbyasinglesampleandallmodelsarerunonanH100GPU.
L-GATrinitscurrentimplementationisnotyetasefficientasaTransformerforsmallsystems: for
uptohundredsofparticles,L-GATrtakesanorderofmagnitudelongertoevaluate. Thisiscausedby
thelinearlayersinL-GATr,whicharemorecostlytoexecutethantheirnon-equivariantcounterparts
andrepresentaconstantcomputationaloverhead. However,becauseL-GATrisbasedonthesame
efficientbackendfordot-productattention,itscalesjustlikeaTransformertolargersystems,andwe
findthesamecomputationalcostfor5000particlesormore.
Comparedtoanequivariantgraphnetwork,L-GATrisclearlymoreefficientintermsofcompute
21
)sm(emitecnerefnItimeandmemory. Alreadyatsmallsystems,L-GATrcanbeevaluatedanorderofmagnitudefaster.
Thedifferenceisevenmorepronouncedintermsofmemory: thegraphnetworkranoutofmemory
formorethanafewhundredparticles. Thisislargelybecausegraphnetworkimplementationsare
often optimized for sparse computational graphs, but here we use fully connected graphs: LHC
problemsoftenbenefitfromafullyconnectedcomputationalgraph,becausepairwiseinteractions
donotusuallydecaywiththeMinkowskinormofthedistance. Transformer-basedapproacheslike
L-GATrcanthushavesubstantialcomputationaladvantagesinparticlephysicsproblemsthatinvolve
alargenumberofparticles.
22