Low-Energy Line Codes for On-Chip Networks
Beyza Dabak Major Glenn Jingyang Liu Alexander Buck
Duke University Duke University University of Toronto University of Toronto
beyza.dabak@duke.edu major.glenn@duke.edu leonjy2014@gmail.com alexander.buck@mail.utoronto.ca
Siyi Yang Robert Calderbank Natalie Enright Jerger Daniel J. Sorin
Duke University Duke University University of Toronto Duke University
siyi.yang@duke.edu robert.calderbank@duke.edu enright@ece.utoronto.ca sorin@ee.duke.edu
Abstract—Energy is a primary constraint in processor design, therefore,weseektoreducetransitions.WeuseNRZIsignaling;
and much of that energy is consumed in on-chip communication. thus, the goal is to reduce the number of 1s. Our LELCs
Communication can be intra-core (e.g., from a register file to
take advantage of the observation that communicated data
an ALU) or inter-core (e.g., over the on-chip network). In this
communicated on chip is not uniformly random (i.e., the k-bit
paper, we use the on-chip network (OCN) as a case study for
saving on-chip communication energy. We have identified a new datawords are not equiprobable), as assumed in much prior
way to reduce the OCN’s link energy consumption by using line work. Fig. 1 presents the distribution of 8-bit datawords sent
coding, a longstanding technique in information theory. Our line through the OCN for two representative CPU benchmarks
codes, called Low-Energy Line Codes (LELCs), reduce energy and one GPU benchmark.1 The all-0 dataword is by far the
by reducing the frequency of voltage transitions of the links, and
most common, the all-1 dataword is fairly common, and other
they achieve a range of energy/performance trade-offs.
datawordsarenotuniformlylikely.2Theseresultsareconsistent
I. INTRODUCTION with intuition and prior experimental results, including results
Architects strive to save energy consumption wherever they that show that all-0 datawords are very common [33]. We
can, and within the chip, one of the largest culprits is on-chip design codes that exploit redundancy in the data to produce
communication[12].Communicationoccursbothwithinacore codewords with fewer 1s.
(e.g., from register file to L1D cache, on pipeline bypass paths, While many data codes exist that enable us to reduce the
etc.) and between cores via the on-chip network (OCN). To number of 1s, we must consider their costs. Specifically, the
reduceon-chipcommunicationenergyconsumption,weexploit impact of coding on performance is represented by its rate,
and adapt ideas from information theory. Information theory which is the ratio of dataword bits to codeword bits. Parity, for
explores how best to send information over communication example, requires us to send n=k+1 codeword bits and thus
channels, and it is applicable to the communication channels has a rate of k/(k+1). Information theory governs the pareto
onprocessorchips.Inparticular,weusetheinformationtheory optimaltrade-offsbetweenenergyreductionandrate.However,
technique of line coding and apply it to processor architecture. just because a trade-off is theoretically possible does not imply
Line coding, which consists of two interrelated aspects— that it is practical; certain codes might require prohibitively
modulation and data coding—is a longstanding technique for complexorlargehardwareforencodinganddecoding.Wealso
transmitting data over a communication channel (i.e., wire). must ensure that any line code we develop does not exacerbate
Modulation is how we represent 0s and 1s with voltages, such crosstalk among the wires in a link.
as with the widely used NRZ (non-return-to-zero) and NRZI In this paper, we present several practically implementable
(non-return-to-zero inverted) signaling techniques. With NRZ line codes that correspond to a range of energy-rate trade-offs.
signaling, a logical 0 is mapped to low voltage and a logical To showcase the opportunity for LELCs, we focus on OCN
1 is mapped to high voltage. With NRZI signaling, a 0 is communication; the OCN takes up approximately 10% of the
mapped to an unchanged voltage, and a 1 is mapped to a area [14] and consumes up to 20% of the processor’s power
voltage transition. Coding is how we map the k-bit dataword budget [14,22,23,39]. Furthermore, OCN links contribute a
intothen-bitcodeword,soastoachievedesirabletransmission substantial fraction of OCN power–∼50% of OCN dynamic
properties.Asasimpleexample,aparitycodeaddsasinglebit power is spent on wires [5]. Transmitting a single bit on a link
to the end of the dataword to create a codeword that can detect consumes approximately 1−1.75pJ [45,47]. In the context of
single-bit errors. Other codes create codewords with other the OCN, having a range of trade-offs is attractive, because
properties, such as RLL codes which limit the length of a run of the over-provisioning of link bandwidth and the varying
of like symbols in any codeword [46]. Code constraints enable runtime demand for bandwidth. Furthermore, we provide a
us to shape the distribution of 0s and 1s in the codewords.
To reduce energy in on-chip communication, we introduce
1Forfulllistofworkloadsandmethodology,seeSectionVI.
line codes that we call Low-Energy Line Codes (LELCs).
2Otherbenchmarkshadsimilartrendsofall-0andall-1datawords,along
Energy is a function of the number of voltage transitions; withvariationsinthenextmostcommonsetofdatawords.
1
4202
yaM
32
]CH.sc[
1v38741.5042:viXraproportional to link length; this Manhattan distance between
coresisindependentofthenumberofroutertraversalsbetween
them (router traversals would be a function of topology).
OCN Traffic. OCN traffic includes two types of packets:
control and data. A control packet (e.g., coherence request
oracknowledgment)isshort(e.g.,8B),becauseitiseffectively
just a header. A data packet (e.g., coherence response) is long
because it carries a cache block of data (e.g., 64B) in addition
to a header. In this paper, we only perform coding on data
packets,becauseofthegreateropportunity.Eachdatapacketis
divided into multiple flits whose size corresponds to the width
of the link. Furthermore, we only encode the data payload and
not the header, because coding the header would introduce
routing complexity and latency (i.e., having to decode the
header at each router to determine the next hop).
OurOCNhasfixedlinkwidthsbetweenrouters,andwemust
map the bits being transmitted to those fixed number of wires.
Coding will change the rate; a change in rate can be accounted
for by increasing the number of flits per packet and/or by
increasing the link width. We conservatively assume that we
cannot widen links. For example, if our 64B data payloads
are broken up into 4 flits of 16B each and we reduce the rate
to 0.9, we would require one additional flit to accommodate
the coded data. Additional flits consume bandwidth on the
links and in the router crossbars and also increase serialization
latency (paid only once at the destination).
The increase in the number of flits is discretized because
of the fixed link width. Returning to our example with a link
width of 16B, any increase in data payload size from 1 bit to
Fig. 1: Dataword distributions for benchmarks (2 CPU and 1 GPU) 16B requires exactly one flit to accommodate it. Thus, a code
that adds 16B has the same performance impact as one that
mechanism for throttling coding when OCN utilization is high. adds anything less, and thus it often makes sense to use codes
This paper makes the following primary contributions: that add multiples (or near multiples) of the link width.
• IntroduceslinecodestoreduceOCNlinkenergy. Energy Consumption. Recent work demonstrates that
• Presents insight into how OCN architecture influences links consume ∼ 40-50% of the total energy in the net-
which codes are effective and which are less so. work [5,27,49]. In the context of the un-core (including L1,
• Implements low-cost hardware implementations of en- L2, OCN and DRAM), prior work reports that links consume
coders and decoders for the most promising LELCs. an average of ∼30% of uncore energy [29]. Link energy is
• Providesasimpledynamicstrategytoturnofflinecoding dominated by the energy consumed when the voltage on wires
when bandwidth demand is high. transitions (from low-to-high or high-to-low). The amount
• Experimentally shows that our LELCs provide a tunable of energy consumed during a transition depends on many
rangeofenergy-performancetrade-offswithenergyreduc- factors–wire length, resistivity, and parasitic capacitance and
tions (8.0%-36.7%) that greatly exceed runtime increases inductance–in this work, we focus on reducing the number of
(−1.25%-8.42%).OurLELCsalsoreducecrosstalkbyan transitions as a proxy for the energy savings.
average of 21.3-36.1%. Link Geometry. A link is a collection of wires, and these
• Demonstrates that codes designed for CPU workloads wires are arranged in some 3D geometry. For the purposes
provide significant energy improvements in GPUs. of this work, link geometry matters only insofar as it affects
crosstalk (discussed next). We assume the wires in a link are
II. SYSTEMMODEL arranged such that a cross-section of them is a 2D grid.
Crosstalk. Crosstalk is the phenomenon in which a signal
WefocusonthelinksinanOCN,andwearelargelyagnostic
transmitted through one information channel affects signals
to the routers and topology. Coding happens at the source and
on neighboring channels. Crosstalk results from capacitive or
destination, leaving routers unchanged. We assume a 2D mesh
inductive coupling between adjacent parallel wires, and trans-
but our insights generalize to other topologies.3 Link energy is
mission errors are more likely to occur when the interference
from neighboring wires is constructive. This happens when the
3WefocushereonOCNsforCPUs.InSec.VIII,wepresentlinkenergy
reductionsforGPUswhichtypicallyfeaturecrossbarnetworks. neighbors of a victim wire experience identical transitions and
2each of which contains at most H−1(R)N 1s, where H is the
binary entropy function. When the information rate R=1/10
(typical of our benchmarks) we have NR/2=(0.05)N versus
H−1(R)N≈(0.013)N, so compression requires more energy
to send the file. It is important to note that this ideal analysis
assumesperfectcompressionandperfectcoding,anditneglects
thepracticalconsiderationsofimplementingeithercompression
or coding on-chip: latency and area.
IV. LOW-ENERGYLINECODESFOROCNS
Fig. 2: Optimal trade-off between rate and energy reduction for
equiprobable input data. Our goal is to reduce OCN energy by using NRZI signaling
and by reducing the frequency of 1s transmitted. There are
the victim wire experiences no transition or a transition in the
several types of codes that we could use, adapt, or create to
opposite direction. Crosstalk manifests as a delay proportional
shape the frequency of 1s, and we exploit the rich information
to the effective capacitance experienced by the victim wire,
theory tradition of coding for other contexts and applications.
and it is a significant issue in on-chip links [15,42].
We call these codes Low-Energy Line Codes (LELCs). First,
III. THEPOTENTIALOFLINECODING we outline the properties we seek when developing our codes:
Theprinciplesofinformationtheoryenableustoanalytically • Reduce the fraction of 1s to a sufficient extent to achieve
substantial energy savings, even when accounting for
bound the potential of line coding. To do so, we must make an
additional overheads such as encoding/decoding circuitry.
assumption about the input datawords; specifically, we assume
they are equiprobable. Real-world inputs are not equiprobable, • Achieve a rate that is high enough to incur a penalty of
only one additional flit. This rate threshold depends on
but with lossless compression, if the input is compressed down
the link width of the target OCN system (e.g., it is 0.8 in
to its entropy, it can be made equiprobable.
our experimental system).
First, consider rate. The maximum achievable rate R of a
codethathasaprobability f thatanygivenbitinthecodeword • Facilitatelow-latencyhardwareforencodinganddecoding,
as well as the ability to easily parallelize the hardware for
is a 1 is determined by the binary entropy function H(f):
encoding (decoding) independent datawords (codewords).
R=H(f)=−flog 2f−(1−f)log 2(1−f). (1) • Generalizewelltodifferentbenchmarksandsystems;per
applicationcodesarepossiblebutwouldrequireconsider-
Intuitively, the rate is 0 at the extreme values of f; if we send
ableprofilingandreconfigurabilitytoachievegoodresults.
only 0s or only 1s, we convey no information. The maximum
achievable rate of 1 occurs when f = 1, i.e., when 0s and 1s • Avoid pathological scenarios that could increase energy.
are equally likely, as with uncoded dat2 a.4 • Reduce crosstalk as a result of reducing transitions; in the
worst case, avoid increasing crosstalk.
Our goal in this work is to save energy (i.e., decrease f)
while sacrificing as little rate as possible. We plot rate as a A. LELC Class 1: Flip-N-Write
functionofthepercentageofenergyreductioninFigure2.The
Asimplewaytoreducethefrequencyof1sinthecodeword
figure shows the trade-offs that are possible with line coding,
streamisbyinvertingdatawordswithmore1sthan0s.Thecost
with two caveats. First, recall that this assumes equiprobable
is an extra flag bit that denotes whether the codeword’s bits
input datawords, which is not true of real application data
were flipped or not; we refer to this extra bit as the IsFlipped
though it could be achieved via compression. Second, some of
bit. Thus, for k-bit datawords, we have k+1-bit codewords,
the points on the curve may correspond to codes that would
resulting in a rate of k/(k+1). This code is a degenerate case
be prohibitively expensive to implement, due to circuit latency
of coset coding [17,18]. The code is parameterized by k, and
and/or energy. Among the possible trade-offs, we believe that
it has the nice property that a codeword will never have more
there is a sweet spot for OCNs at a rate around 0.8, which has
than half of its bits equal to 1. This coding scheme has been
a corresponding theoretical energy reduction of 40%.
previously proposed for use in storage, under the name Flip-N-
It is natural to consider compression as an alternative to
Write [11], which we adopt. It has also been used for general
line coding because our benchmarks are very compressible,
data buses, under the name bus-invert [44]. While bus-invert is
and compression reduces the number of bits communicated.
also an application of the code for communication, that work
Consider communicating an N bit file that can be compressed
did not consider real-world data or any specific network, but
to NR bits, where the information rate R<1. If we assume
instead assumed equiprobable datawords.
perfectcompression(downtotheinformationrateR),thenbits
We extend Flip-N-Write (FnW) with a multi-level version
in the compressed file will be equiprobable, and so we will
that further trades rate for energy savings. Consider applying
transmit NR/2 1s. In a world without practical constraints, we
(single-level) FnW on a stream of datawords. The result is a
can design a line code comprising 2RN codewords of length N,
stream of codewords, each with its own IsFlipped bit. Now, if
4Theconverseisnottrue;onecancreatecodeswith f=1 thathaverate we consider each group of f IsFlipped bits (i.e., the IsFlipped
lessthantheupperboundof1. 2 bits corresponding to f codewords, where f may or may not
3Fig. 3: Illustration of 2-level Flip-N-Write (k=4).
Fig. 5: Tree Code 1 (TC1): Variable rate that is bounded between
3/4 and 5/4. 1 bit added redundancy for all 0s and compression
opportunity for strings of 1s.
Fig. 4: Huffman coding tree with codewords at leaves. Assumes the
frequency of input datawords is, in decreasing order: 00, 11, 01, and
10. These datawords are mapped to codewords 0, 11, 100, and 101.
equalk),wecouldtreatthemasa f-bitdatawordtobeencoded
with FnW. Thus, for every group of f datawords, we have f
codewords, each with k+1 bits, plus one extra bit to denote
whether the f IsFlipped bits are inverted or not. We illustrate Fig.6:TreeCode2(TC2):Variableratethatisboundedbetween3/4
and6/4.Noaddedredundancyforall0sandcompressionopportunity
an example in Figure 3. FnW is attractive in that encoding
for strings of 1s.
and decoding require only simple logic, thus adding minimal
complexity, latency, and energy consumption. We evaluate the
Figure 1, the distribution of datawords is not equiprobable.
circuitry and energy for encoding and decoding FnW (and all
We have developed several tree codes that provide different
other LELCs) in our experimental evaluation (Sec. VII).
energy/ratetrade-offsandrequiredifferentamountsofhardware
for encoding/decoding. In Figures 5 and 6, we show two of
B. LELC Class 2: Tree Codes
the tree codes we have developed. Both tree codes guarantee
One can create a code using a tree, in which the leaves
that every codeword has no more 1s than its corresponding
of the tree are the codewords, and the path taken to reach
dataword, and both target compression of strings of dataword
a leaf is the dataword. A famous example of a tree code is
1s (i.e., the rightmost parts of the trees). Compared to tree
a Huffman code [24]. A common use of a Huffman code is
code 1 (TC1), TC2 targets longer strings of dataword 1s. The
losslesscompression,anditachievescompressionbyassigning
essentialdesignpointinTC2isthatnobitredundancyisadded
theshortestcodewordstothemostcommondatawords.Figure4
to the all-zero dataword, which is the most common dataword
shows a simplified example which maps 2-bit data words to
in our benchmarks and very common, in general [33]. Thus,
code words that are 1 to 3 bits in length.5 In this example,
TC2 offers higher rate for the most frequent datawords and a
input00ismostcommonsoitismappedtotheshortest(single-
higher overall rate, but less energy savings than TC1.
bit) codeword. Uncommon datawords are mapped to longer
We designed TC1 and TC2 to fairly compete with FnW ,
k=3
codewords. Because a Huffman tree is designed to compress
such that the codeword length n=4 in both schemes, and
datawords with expected (e.g., profiled) input statistics, its rate
the lower bound on rate for TC1 and TC2 is 3/4. (We can
is greater than 1 when input statistics are similar to expected.
make a tree code that is exactly equivalent to FnW with
k=3
One can also construct tree codes that do not follow the
a balanced tree that has path length 3 and codeword length
Huffman algorithm. Instead of compression, our primary goal n=4.6) A small value of k limits the depth of the tree and
is to reduce the frequency of 1s in the codewords; nevertheless,
the logic required to encode the datawords. Using a similar
any compression we achieve benefits our code rate. Therefore,
methodology, we could design other tree codes to compete
weconstructtreesthatmapdatawordbitsequencestocodeword
against FnW with larger values of k. As we show in Section V,
sequences with fewer 1s. While doing so, it is often possible
though, even a shallow tree has a high encoding latency due
to retain some of the compression benefits of Huffman codes.
to variable length encodings; therefore, we do not consider
The construction of a tree code is informed by the expected
deeper trees further in this paper.
dataword distribution from PARSEC benchmarks. Recall from
Tree codes can be either fixed or variable rate (both TC1
and TC2 are variable rate), depending on whether the tree
5Inlosslesscompression,aHuffmancodeencodessourcestringsintovariable-
is perfectly balanced or not, respectively. Fixed-rate codes
length codewords. Source symbols label leaves of the Huffman tree, and
codewordslabelpathstotheroot.Tobeconsistentwithourothertreecodes,
wereversethisassignment,labelingleavesbycodewordsandpathsbydatawords. 6Infact,thebalancedleftpartofTC1isequivalenttoFnWk=3.
4TABLE I: Mapping Codes
but is mapped to 0110 when n=4 (rate-3/4), which provides
(a)Examplerate-1 (b)Examplerate-3/4 the aforementioned guarantee on energy consumption.
Dataword Codeword Dataword Codeword Themainchallengeindesigningamappingcodeischoosing
000 000 000 0000 themapping.Aswithourtreecodes,weuseprofilingsimilarto
111 001 111 0001
Fig. 1. We profile PARSEC benchmarks [6] independently and
100 010 100 0010
inaggregate,andweexplorehowwellper-benchmarkmappings
001 100 001 0100
perform on other benchmarks (i.e., how well does the mapping
010 011 110 0011
110 101 010 1000 based on the profile for benchmark X perform on benchmark
011 110 011 0101 Y) and how well the aggregate mapping performs on each
101 111 101 0110 benchmark. The similarities between benchmark distributions
enables a single aggregate mapping to work well; we quantify
generally enable simpler hardware for encoding/decoding, these results in Sec. VII. Note that, while FnW targets high-
but variable-rate codes are attractive when they offer better weight datawords, regardless of their observed frequency,
energy/rate trade-offs. We discuss how to overcome the mapping codes (like tree codes, to a somewhat lesser extent)
implementation challenges of variable-rate codes in Section V. exploit differences in observed frequencies. Mapping codes
also have the potential to be reconfigurable; we leave the study
C. LELC Class 3: Mapping Codes of this to future work.
Fundamentally, a code is a mapping from a dataword to
a codeword. Mappings are often based on math or trees that D. Compound LELCs
facilitate simple hardware. However, for short codes, we can
One can combine multiple codes to achieve multiple goals,
simply create a 1-to-1 mapping using a lookup table.
and we refer to such codes as compound codes. Many com-
We take advantage of this opportunity by explicitly mapping
pound codes are possible, and we now discuss one compound
the most common k-bit datawords to the n-bit codewords with
LELC which combines a simple compression code with a
the lowest weight, i.e., number of 1s. The rate of a mapping
code that is designed to save energy.7 The dataword is first
codeisunderourcontrol,becausewecanchoosentobeequal
converted by the compression scheme (discussed next) into an
tok(inwhichcaserateequals1)orgreater.Asnincreases,rate
intermediate codeword. At this step the objective is increasing
decreases,butwecanreduceenergymore.Considerdatawords
the code rate. Then the intermediate codeword is converted
of length k and codewords of length n=k+1, and assume k
by the rate-8/9 mapping code into the final codeword. At this
is even, for simplicity. Datawords can have a weight anywhere
step the objective is energy reduction.
from 0 to k. For codewords, we can choose to use the 2k n-bit
Our compression scheme exploits the observation that long
strings with weights from 0 to k/2. We will show later that
runs of 0s are very prevalent in typical software (including
mapping codes with length k+1 reduce energy significantly
our benchmarks). Our compression scheme is as follows: a
more than mapping codes with length k, at a relatively small
length-k dataword consisting of all 0s is mapped to ‘1’ and
loss in rate (i.e., rate k/(k+1) versus rate 1). The primary
all other length-k datawords are mapped to ‘0+dataword’. This
constraint on the size of a mapping code is k, because the
mapping table has 2k entries, each of which is n bits long. is similar to a standard Huffman compression scheme which
focuses on compressing the most frequent dataword. However,
Large tables may be too slow and energy-hungry to be viable.
as observed in Figure 4, even in the simplest Huffman code
In Table Ia, we illustrate a simple mapping code example
there are more than two different codeword lengths which
with 3-bit datawords and 3-bit codewords; because datawords
is a significant drawback during hardware implementation of
and codewords are the same length, the code rate equals 1.
decoders. Our compression scheme can be built for any choice
Datawordsaresortedindescendingorderaccordingtoobserved
of k, and the resulting codeword length is either 1 or k+1.
frequency in data packets, and the corresponding codewords
Although we are increasing the number of 1s when we encode
are in ascending order according to their weights.
the all-0 dataword to ‘1’ in the intermediate codeword, this is
Mapping codes offer the possibility of guaranteeing that
overcome by the subsequent mapping code.
every codeword has a weight that is no greater than its
Our compound code’s rate depends on k and the dataword
corresponding dataword. This feature could be desirable for
statistics. As k increases, both the maximum possible compres-
making guarantees about energy consumption, as well as
sion ratio for the all-0 dataword and the maximum possible
serving as a safety net against adversarial input data. A rate-1
rate for the other datawords increase. However, depending on
mapping code cannot provide this guarantee, but once rate is
the benchmark statistics, the frequency of the all-0 dataword
k/(k+1)orless,wecancreatemapswiththisguarantee.Such
variesatdifferentgranularities,andthusreal-worldresultsvary.
maps offer better worst-case behavior at the potential cost of
Furthermore, because the compound codes contain a code with
somewhat worse common-case behavior. We show a rate-3/4
high energy savings, they also offer good energy savings.
mapping code with this property in Table Ib. Comparing the
examples in Table I, note that dataword 101 is mapped to
codeword 111 which increases its energy when n=3 (rate-1) 7RecallfromSec.IIIthatcompressionaloneislesseffectivethanlinecoding.
5TABLEII:RatesandpercentageenergyreductionforproposedLELCs
coding is more (less) often disabled, so performance impacts
observed across 8 PARSEC benchmarks running on a CPU. Codes
aredecreased(increased)butenergysavingsarealsodecreased
above (below) the dashed line have fixed (variable) rate.
(increased). We evaluate several different thresholds.
Coding Scheme Rate Energy Reduction %
Flip-N-Write (k=3) 0.75 15.52–23.66 Dynamic estimation of OCN utilization can be done in
Flip-N-Write (k=8) 0.89 17.47–27.30 several ways, and we are agnostic as to how this is done. The
2-level FnW (k=4) 0.76 24.31–36.40 option that we use is a per-link scheme in which each router
Mapping1 0.89 21.91–36.67
observesitslocalutilizationatthegranularityof100kcycles.If
Mapping2 1 10.79–30.41
theutilizationisaboveagiventhreshold,codingisdisabledfor
TC1 0.76–0.78 18.30–27.88
thesubsequent100kcycles;otherwise,itisenabled.Locallink
TC1’ 0.76 18.94–27.91
TC2 0.92–0.97 11.10–20.83 utilization is tracked simply with a counter and a comparator,
TC2’ 0.86–0.88 10.65–20.52 resulting in minimal circuit overheads. The choice of interval
Compound1 (k=32) 1.07–1.54 15.90–25.74 length is a tunable parameter, but we did not find that our
FP Compression [2] 1.25–1.72 -23.38 – -10.74
results were very sensitive to it.
E. Summary of Promising LELCs V. ENCODER/DECODERCIRCUITRY
We presented four classes of LELCs. Moreover, all of these The hardware costs in terms of energy, latency, and area
LELCshavemultipleviabledesignpoints.FnWcanbeapplied are critical to determining the practicality of our codes. We
at different granularities. Tree codes can be designed with implemented the circuits for all our LELC encoders and
many different topologies and assignments of datawords to decoders in Verilog and synthesized them using Synposys
leaves. Mapping codes have many possible mappings and rates. Design Compiler with a 12nm library. For codes requiring
Compound codes can exploit compression at different rates. look-up tables, we use CACTI [4] with its smallest technology
In Table II, we list the ranges of rates and percentage (22nm) to estimate the energy and latency, due to difficulties
energy reductions we experimentally observe across our CPU with the Synposys memory compiler. We discuss circuit
benchmarks for some of our most promising LELCs. The implementation issues before presenting the synthesis results.
observed rate for a given benchmark is measured as the total
A. Fixed Rate Codes
number of dataword bits across all data payloads divided by
the total number of codeword bits across all data payloads. For our fixed-rate codes, we easily take advantage of
For variable rate codes (below the dashed line in the table), a parallelism. Assume d-bit data payloads (d = 512 in our
differentrateisobservedforeachbenchmark,becausedataword experiments) and k-bit datawords. We can encode (decode) all
statisticsdiffer.Thepercentageenergyreductionismeasuredby d/k datawords (codewords) in parallel, given enough replicas
normalizing the total number of 1s in codeword bits across all of the circuit that encodes (decodes) each k-bit dataword (n-
datapayloadsbythetotalnumberof1sindatawordbitsacross bit codeword). Replicas consume area and power (our power
all data payloads. We include two different entries for each of and area results in Sec. V account for the overhead of these
our tree codes. TC1 and TC2 denote the codes as presented replicas), but the concurrency is vital for performance. For
thus far, while TC1’ and TC2’ represent adjustments made to FnW,wehaveimplementedcircuitsforvariousvaluesofk.For
simplify their hardware implementations (see Section V). We mapping codes, we have evaluated lookup tables (i.e., ROMs)
also compare against Frequent Pattern Compression (FPC) [2]; for maps of different sizes.
as expected FPC improves rate, yet across all benchmarks, the
B. Variable Rate Codes
energyconsumedincreasesduetoanincreaseinthenumberof
ones. This is consistent with the theoretical analysis in Sec. III. Variable-rate codes pose implementation challenges that we
This list of codes is not exhaustive, but it shows the potential must overcome, because our two tree codes and our compound
of practical LELCs codes to reduce energy at differing rates. codes are variable-rate. With a variable-rate code, parallelism
is difficult because the boundaries between datawords and
F. Dynamic Code Throttling
codewords may not be determined until runtime. A variable-
Because many of our LELCs incur a cost in rate, they can rate code can have variable-length datawords (like our tree
potentially degrade performance. When OCN link utilization is codes)orcodewords(likeourcompoundcodes),bothofwhich
low (i.e., there is bandwidth slack in the OCN), a decrease in present implementation challenges. If datawords (codewords)
rateisunlikelytohavemuchimpactonperformance.However, are variable-length, then we cannot start encoding (decoding)
whenlinkutilizationhigh,itcanbemoreproblematictoaddto a dataword (codeword) until we have encoded (decoded) the
bandwidth demand. To limit the potential performance impact previousone.Withoutparallelism,bothencodinganddecoding
of coding, we explore disabling coding when OCN utilization would require more latency than we can comfortably tolerate.
is high. (A more complicated solution would seek to switch Parallelencodingwithvariable-lengthdatawords.Wedivide
between LELCs; we leave this option for future work.) To the data payload into fixed-length dataword chunks of size f
implementthrottling,wemustanswertwoquestions:(1)where and use independent modules to process each dataword chunk
to set the threshold and (2) how to measure OCN utilization. in parallel. As the dataword lengths differ, a different number
The choice of threshold is a trade-off. If it is low (high), then of datawords can fit into each fixed-length dataword chunk.
6TABLE III: Latency/Energy/Area for Encoder/Decoder Circuits for
Wepackthedatawordsthatfitintothedatawordchunkandthe
128-bit flits
remaining bits are sent unencoded. This solution lowers our
energy reduction (because unencoded bits do not benefit from Coding Latency (ps) Energy (pJ) Area (µm2)
our coding), but may improve the rate of the overall scheme as Scheme Enc. Dec. Enc. Dec. Enc. Dec.
FnW (k=3) 119.4 99.9 1.33 1.53 605 591
unencoded bits have a rate of 1. This solution requires some
FnW (k=8) 203.1 104.6 1.27 1.33 669 515
circuit complexity to “pack” the output of each module into
2L FnW (k=4) 168.5 147.9 1.32 1.06 633 452
flits, since the output length of each module is variable. Mapping1 74.9 85.7 0.32 0.65 682 1093
Padding variable-length codeword chunks into fixed-size Mapping2 73.5 73.5 0.14 0.14 262 262
codeword chunks. Although tree codes have fixed-length TC1’ 675.0 429.2 2.31 1.96 3775 2346
Comp1 (k=32) 324.9 261.5 1.50 1.45 1226 1579
codewords,parallelizingtheencodingprocesswithfixed-length
dataword chunks (described above) results in variable-length
TABLE IV: Simulation parameters
codeword chunks. To overcome the complexity of parallel
Cores 16 OoO x86 cores
decoding of variable-length codeword chunks (see below), the
L1 ICache/DCache I: 32KB, 2-way, D: 64KB, 2-way
encoder uses zero-padding to create fixed-length codeword
L2 shared, inclusive 2MB, 8-way
chunks. Assume that f dataword bits are encoded as at most t
Cache block size 64B
codeword bits. If any group of f dataword bits are encoded to Coherence protocol MESI
fewer than t bits, the remaining bits can be “padded” with 0s Topology 4×4 mesh
to get a fixed output length of t. The padding hurts rate but Routing Algorithm minimal XY
Router Latency 1 cycle for uncoded baseline
has no impact on energy, which is attractive for tree codes that
Link width 16B (same as flit size)
prioritizeenergy.Asaresultofthistechnique,ourimplemented
Flow Control 4 virt. nets, each with 3 virt. channels
treecodesdifferfromtheonesdescribedearlier.Wedividedata
payloads into 32-bit dataword chunks, and we use padding to
of energy, to provide context, transmitting a single bit on a
create fixed-length codeword chunks of length 42. We denote
link consumes approximately 1−1.75pJ [45,47], whereas our
the more implementation-friendly versions of TC1 and TC2
encoding and decoding circuitry requires 1.5pJ per 128-bit flit
as TC1’ and TC2’, respectively.
for ourcompound code. Each networkinterface will requirean
Decoding variable-length codewords. Assume we receive
encoder and decoder; each encoder/decoder circuit processes
a codeword string of c bits. If all codewords are the same
128-bits. In terms of area, all our designs are negligible.
length, n, or if we use zero-padding to create fixed-size
codeword chunks, then parallel decoding is easy; we can have VI. EXPERIMENTALMETHODOLOGY
parallel modules to decode each group of n codeword bits.
Byusinglinecoding,weseektoreduceenergyconsumption,
However, if codewords can be multiple possible lengths, as
while minimizing the potential impact on performance and
in our compound codes (without padding), then we do not
crosstalk. To evaluate these metrics, we use full-system simu-
immediately know where each codeword ends. Nevertheless,
lation and multithreaded benchmarks. To model a multicore
we can make the problem much more tractable by limiting
processor, we use the gem5 simulator [7]. We configure gem5
the number of possible codeword lengths. Specifically, if our
to model an x86 processor with 16 cores. To model the
codewords can be either of only two lengths n or n , then
1 2 OCN in detail, we use Garnet [1]. The configuration for our
the circuitry can, in parallel, consider all of the possible bit
simulator can be found in Table IV. Performance results are
positions where codewords can start. However, if codewords
produced directly by the simulator. The latencies in Table III
can have many different lengths, then the number of codeword
areincludedinoursimulationtoproperlyaccountforencoding
starting positions is too large for efficient circuitry. As such,
and decoding overheads. To estimate the energy savings for
ourcompoundcodehasonlytwocodewordlengths:onelength
the OCN links, we examine the bits in every message sent in
for a run of 0s and a second length for all other data words.
the simulated system and code them accordingly. Energy is
Despitetheavailabilityofthesethreetechniquesforsimplify-
directlyproportionaltothenumberof1s,andallenergyresults
ing hardware implementations, TC2 remained too complicated
are normalized to the number of 1s in the uncoded baseline.
and slow to be attractive. It requires padding for two different
We run applications from the PARSEC [6] benchmark suite
lengths to limit rate loss, and has more dataword lengths
each with 16 threads, except for a few that do not currently
than TC1. As TC1 is already our most complicated code to
run on our simulation infrastructure: dedup, facesim, raytrace,
implement (see next section), we did not further pursue TC2.
vips, and x264. Our use of benchmarks instead of assuming
C. Latency, Energy, and Area Results equiprobable dataword statistics distinguishes our work from
much of the prior work. To highlight the broad applicability
The latency, energy consumption, and area footprints of our
of our work, we also evaluate our LELCs on a discrete GPU
encoding and decoding circuits are summarized in Table III.
using gem5’s GCN3 GPU model [19] and running 6 Pannotia
These circuits encode/decode 128 bits. We target single-cycle
graph workloads [10].8 This system is configured with 32 CUs
latencyforencoding/decodingtominimizeperformanceimpact.
and uses a crossbar interconnect modeled with Garnet.
Allcircuitsfitwithinthelatencyofa1.5GHzclockperiod,and
all but the encoder for TC1’ fit within a 2GHz clock. In terms 8Floyd-Warshallwasomittedduetoerrorsinthebenchmark.
7case have sufficient spare bandwidth to accommodate a rate
reduction. Figure 7 plots performance as a function of code
rate. We abstract away the codes themselves and simply apply
a range of fixed rates, and we never throttle the coding. These
results include the addition of one cycle each for encoding
and decoding, as in the previous section. The results show
that adding one extra flit—which corresponds to a rate in
the range (0.8,1]—has an impact of about 4-7% (including
the roughly 2-3% for encoding/decoding latency). Adding
additional flits causes additional slowdown; unsurprisingly,
doubling the number of flits per payload causes significant
contentionintheOCNandleadstoexcessiveslowdowns.These
results motivate high-rate codes and code throttling.
B. Link Energy versus Performance
The fundamental trade-off with LELCs is link energy versus
performance. In Figure 8, we plot performance on the left y-
axis and link energy on the right y-axis, with both represented
as their percentage changes with respect to an uncoded
baseline (increased for performance and decreased for energy
consumption). Moreover, we plot results for both throttled and
unthrottled coding. Energy consumption reductions are reports
for OCN links; OCN links consume ∼40-50% of the total
energy in the network [5,27,49].
The graph reveals three general insights. First, on every
Fig. 7: Runtime impact of encoding/decoding latency (top) and code
rate (bottom). benchmark, every LELC provides a greater savings in energy
thanincreaseinruntime.Second,thetrendsacrossbenchmarks
VII. EXPERIMENTALEVALUATION are consistent, just with modest differences in absolute values.
Our primary goal is to evaluate the potential of LELCs to Third, dynamic throttling is effective in reducing performance
reduceOCNenergywithoutundueimpactonperformance.We impact;here,wethrottlecodingwhenutilizationexceeds16.5%
also evaluate the impact of LELCs on crosstalk. on a link (measured over 100K cycles). Dynamic throttling
roughly halves the performance loss but at the cost of more
A. OCN Tolerance to Latency and Bandwidth than half of the energy savings. Figure 9 sweeps the utilization
threshold values for ferret. The utilization marker provides a
WefirstexplorehowmuchtoleranceOCNshavetoincreases
knob to tune the performance-energy trade-off of our LELCs.
in latency (due to encoding/decoding time) and decreases in
bandwidth (due to lower rate). The graph also reveals trends across our LELCs:
1) LatencyToleranceofOCNs: Encodinganddecodingtake • Our rate-8/9 mapping code has the best energy reduction
some amount of time, and that added latency could impact across benchmarks (except ferret).
OCN and overall system performance. To quantify this impact, • Our compound code has the least impact on runtime
we ran simulations in which we added varying amounts of (−1.25-2.54%). Due to its high rate, little performance
latency for encoding and decoding. The latency is paid once degradation (and even small improvements for fluidani-
at sending (for encoding) and once at receiving (for decoding), mate and freqmine) is observed. Compound codes benefit
but not at intermediate routers. The results of this sensitivity significantlyfromnon-equiprobableapplicationdata;com-
analysis, shown in Fig. 7, reveal that the overall performance mon, long strings of 0s are compressed for high rate.
impact of a 1-cycle latency is only 2-3%, but at a 2-cycle • Our rate-1 mapping code achieves significant energy
latency the runtime increases by 8-11%.9 These results show savings, with its only cost being the latency for encoding
that we are practically constrained to simple hardware that can and decoding (i.e., there are no extra flits).
complete within a single cycle. • FnW k=8isparetooptimalwithrespecttoFnW k=3,because
2) BandwidthToleranceofOCNs: Aratereductionincreases it offers greater energy savings and less runtime impact.
bandwidth demand. Modern OCNs are over-provisioned to It has lower runtime as it requires only 1 extra flit per
tolerate bursty communication [16,21] but in the common data packet.
• Our new 2-level FnW k=4 has an almost identical rate
9Sincewedonotencodetheheadflit,1cycleofencodinglatencyforbody as FnW —and they both incur the same number of
k=3
flitscanbeperformedduringtheroutingoftheheadflit.Ourresults,which
additional flits, so they have the same performance—
assumeaminimum1cycleencoding,arepessimisticintermsofperformance
asoursimplecircuitrycanbefullyoverlappedwiththeheadflit. yet the 2-level scheme gets significantly better energy
8Fig. 8: Percentage increase in runtime (left y-axis, bars) and percentage decrease in link energy (right y-axis, points)
tree codes). Including different variants for our codes provides
insight into the size of the design space and the trade-offs that
canbeachieved.Inthefigure,weshadezonesbasedonwhether
the LELCs are fixed-rate or variable-rate. We observe that, in
general, fixed-rate codes provide greater energy reductions,
whereas variable-rate codes offer higher rates.
Variable-rate codes. These LELCs, including implementation-
friendly tree codes and compound codes, lie in the blue
rectangle and provide a wide range of rates. The datapoints
above rate-1 correspond to compound codes, with k=16 and
k=32, that combine a simple compression scheme with our
Fig. 9: Sweep of utilization markers for ferret
rate-8/9mappingcode.Adeeperexplorationoftheimportance
reduction. The same comparison is true for FnW (not of k on compound codes reveals that k<32 offer higher rates,
k=4
shown). but less energy benefit and more complicated hardware (e.g.,
• Our tree code (TC1’) is designed for fair comparison with see Table III for k =16). Choosing k >32 is unattractive
FnW . Designing a deeper tree, say to compete with because 32 bits represents a common datatype, and we would
k=3
FnW , would require complicated, slow circuitry. Com- not necessarily expect to have many runs of 0s at a granularity
k=8
paredtoFnW ,TC1haslowerrate,yetimplementation- greaterthan32.Thus,weconsiderk=32tobethebestchoice
k=3
friendly TC1’ offers greater energy savings with the same forourcompoundcodes.Variable-ratecodescanalsohaverate
performance impact. less than 1, shown by the shaded green (intersection) portion,
• In this work, we assume one fixed code for each bench- which is where almost all of our tree codes lie.
markrun;differentcodesmayprovidebenefitsfordifferent
phases of benchmark execution but come with added
Fixed rate codes. We explore a number of fixed-rate codes,
overhead of multiple encoders and decoders–we leave the
whichlieintheyellowrectangleandhaveratesof1,0.89,0.88,
exploration of this for future work.
0.8, 0.76, and 0.75. For performance, we strongly prefer codes
To further understand the differences between LELCs, we with rates above 0.8, since they add only a single extra flit in
plot rate (not end-to-end performance) versus link energy eachdatapacket.Inchoosingcodesbetween0.8−1,weprefer
reductioninFigure10.ThefigureincludesalloftheLELCswe those with the greatest energy reduction (furthest to the right)
have discussed and evaluated already, plus other variants (i.e., and the simplest circuit implementation. Among the fixed-rate
different values of k for Flip-N-Write and compound codes, LELCs, 2-level FnW and the rate-8/9 mapping code offer the
mapping codes with different maps, and different trees for best trade-off between energy savings and implementation cost.
9uncoded baseline. This result is intuitive, because (a) our
LELCsreducethenumberofvoltagetransitions(representedby
1s in NRZI signalling), and (b) crosstalk effects are dominated
by transitions (i.e., up or down arrows in Figure 11). The exact
amount of crosstalk reduction, which ranges from negligible
up to 45%, depends on the LELC and the benchmark. Given
thatourprimaryconcernwasavoidinganincreaseincrosstalk,
this result is a bonus. In addition, we observed that there is a
future opportunity to optimize the mapping code used by the
compound LELC to further reduce crosstalk.
E. Recommended LELCs
Fig. 10: Rate vs. Energy We recommend the use of Mapping1, 2-level FnW, and
Compound1 LELCs. These LELCs save energy up to 36.7%,
C. Choosing a Map for Mapping Codes
36.4%, and 25.7%, respectively, with less than 6.8%, 8.4%,
Our two mapping code results each use a single map for and 2.5% slowdown, respectively, on our benchmarks. The
all benchmarks. This map was based on profiling all of the latency, energy, and area overheads for the encoder/decoder
benchmarks;thisisnotoptimalforanyonespecificbenchmark. circuits are small for these LELCs (Table III).
The profiles provide the frequency of all length-8 datawords,
enabling us to map the most common datawords to the lowest- VIII. DISCUSSION
energy codewords (i.e,. the codewords with the fewest 1s). To
We have evaluated our LELCs in the context of general-
exploretheimpactofusingthisaggregatemap,ratherthanaper-
purpose multicore architectures; however, they have broader
benchmark map, we (a) profile each benchmark individually to
applicability, some of which we briefly discuss here.
generateamapthatisoptimalforthatbenchmark,(b)aggregate
GPUs. To assess the applicability of our codes beyond multi-
the per-benchmark profiles to create a single aggregate profile
core processors, we evaluated the link energy reduction for a
and single aggregate map, and (c) run every benchmark with
GPUrunningPannotia[10]applications.Wepresenttheresults
every per-benchmark map and the aggregate map. Results (not
in Table V. As observed in the Table, our LELCs designed for
shown due to space constraints) reveal that using the aggregate
CPUs provide substantial opportunities for link energy savings
map performs well. There is certainly a gap between using a
in graph workloads running on a GPU. In benchmarks such as
benchmark’s optimal map and using the aggregate map (2%-
color and mis, we note very high gains for some of our codes.
8%), but, even so, the mapping code with the aggregate map
We discovered that some benchmarks initialize matrices to -1
still achieves significant energy reductions.
(0xFFFFFFFF), which can be effectively coded for high gains.
D. Impact on Crosstalk Another property we see is matrices initialized to very large
positive values that also feature a large number of 1s. These
With LELCs, we want to ensure that we do not increase
results highlight a few key findings:
crosstalk. Since crosstalk is exacerbated by voltage transitions
and LELCs reduce transitions, we expect LELCs to actually • Common programmer behavior lends itself to non-
reduce crosstalk. Many models have been developed for equiprobable data; properly designed codes can transform
crosstalk, and we adopt the intuitive one from Patooghi et this data into more efficient values to be transmitted.
al. [37]. It models the crosstalk on a given victim wire in the • Ourcodes,whichweredesignedbasedonCPUworkloads,
middle of a 3×3 grid as a function of its 8 neighboring wires. still perform very well on GPU workloads, even though
Neighboring wires that are horizontally or vertically adjacent dataword characteristics may differ.
have more impact than wires that are diagonally adjacent, • One GPU benchmark (SSSP) shows lower gains–this
because they are closer to the victim, and this particular benchmark is dominated by zeros, leaving little oppor-
model ignores the impact of diagonal neighbors. The amount tunity for coding. The remaining GPU benchmarks out-
of crosstalk depends heavily on the voltage transitions on performtheCPUbenchmarksintermsofenergyreduction.
the victim and its neighbors. For example, if the victim is Chiplet-based Designs. Chiplet-based architectures have
transitioning in the same direction (e.g., low-to-high) as its emerged as a means to combat skyrocketing manufacturing
neighbors, that incurs less crosstalk than if they transition costs by manufacturing smaller dies and composing them into
in opposite directions. The model, illustrated in Figure 11, larger systems through interposers or package-level intercon-
measures crosstalk in terms of capacitance, normalized to the nects [34]. Links between chiplets consume more energy than
transition pattern with the least amount of capacitance. on-chip links; current efficiencies on inter-chiplet links are
Using this model, we evaluate the crosstalk of each of around 2pJ per bit [34,47]. Thus we can expect more benefit
our LELCs, normalized to that of an uncoded baseline. The from our LELCs in this context. No modifications are needed
results in Figure 12 show that, for every combination of to our design to support chiplets—inter-chiplet routers code
LELC and benchmark, crosstalk is actually less than the the data prior to sending and decode upon receipt.
10Fig. 11: Crosstalk model from Patooghi et al. [37]. There are 9 pattern types; for each pattern we show one possibility but not the equivalent
rotations or the “mirror” version in which the low-to-high and high-to-low transitions are flipped. Beneath each pattern we show the pattern’s
probability and its normalized crosstalk metric.
Fig. 12: The impact of LELCs on crosstalk
TABLEV:RatesandpercentageenergyreductionforproposedLELCs
CodingresearchinOCNshaslargelyappliedtechniquesfrom
observed across 6 Pannotia benchmarks running on a GPU. Codes
DRAM bus coding, which has a longer history of exploration.
above (below) the dashed line have fixed (variable) rate.
ModernDRAMsoftenemployDataBusInversion(DBI)based
Coding Scheme Rate Energy Reduction %
on Bus-Invert coding [44] to reduce energy and noise. DBI is
Flip-N-Write (k=3) 0.75 9.57–50.23
analogous to FnW discussed earlier in the paper. A bus coding
Flip-N-Write (k=8) 0.89 11.22–67.69
2-level FnW (k=4) 0.76 15.79–73.39 scheme that exploits value locality has been proposed [8].
Mapping1 0.89 19.24–68.75 Bitwise Difference Encoding [38] exploits data similarity to
Mapping2 1 7.38–65.53 reduce the Hamming weight of words on the DRAM bus.
TC1 0.77–0.88 10.75–59.50
Multiple schemes that XOR adjacent bits to reduce transitions
TC1’ 0.76 11.89–55.14
on a link have been proposed [30,31] An online clustering
TC2 0.96–1.06 5.62–49.28
TC2’ 0.89 5.66–41.53 method that XORs data with a common value to reduce the
Compound1 (k=32) 0.87–1.51 10.11–60.49 number of 1s transmitted improves on DBI [48]; this scheme
requires multiple cycles for encoding and would have high
Machine-learning accelerators. Prior work has observed overhead to synchronize the center values across the OCN.
that the distribution of activation and weight values in deep Primarily in the CAD/EDA community, there has been
convolutional neural networks are non-uniform [3,32]. The research on crosstalk-avoidance codes for OCNs and
non-uniform nature of values in ML models would make buses [15,28,40,41,43,51]. This prior work develops models
their communication highly-amenable to the types of LELCs (like the one we use from this paper [37]) that are a function
explored in this paper. of the geometry of the wires and the signals that are on each
wire. Certain signal patterns (e.g., a wire transitioning from
IX. RELATEDWORK low-to-high while its neighbors transition from high-to-low)
OCN line coding has been studied to reduce power and are worse than others, and the crosstalk-avoidance codes seek
crosstalk. A temporal coding scheme codes the current flit to eliminate the worst-case patterns of signals. This prior
with the flit transmitted ahead of it on the link [35,36] using work also assumes that inputs are equiprobable, which enables
a bus-invert style scheme; this works well for wormhole mathematical analysis but is not representative of real-world
routed OCNs where flits from different packets cannot be inputs. Unlike prior work, we reduce crosstalk by changing
interleaved. However, in modern OCNs, virtual channel flow the distribution of signal patterns to favor patterns with fewer
control prevails, which can disrupt the flow of flits from the 1s, rather than by eliminating a few select patterns.
samepacket,renderingsuchtechniquesineffective.Otherwork Compression schemes are similar but distinct from coding.
focuses on uniform random data [25] which leaves opportunity Compression has been studied in the OCN [13], including
on the table, as we demonstrate on benchmark data; that table-based compression [26], delta-compression [50], and
work also assumed hop-by-hop encoding which consumes approximate (lossy) compression [9]. As noted earlier, coding
more overhead. Finally, an XOR-coding scheme improves the is theoretically superior to compression so that is our focus; in
efficiency of speculative routers [20]. addition, we demonstrate that coding provides greater benefits
11when compared to frequent pattern compression [2]. Frequent [9] R. Boyapati, J. Huang, P. Majumdar, K. H. Yum, and E. J. Kim,
pattern compression requires multiple clock cycles, whereas “APPROX-NoC:Adataapproximationframeworkfornetwork-on-chip
architectures,” in International Symposium on Computer Architecture,
our codes can be implemented within a single cycle.
2017.
[10] S. Che, B. Beckmann, S. Reinhardt, and K. Skadron, “Pannotia:
X. CONCLUSIONS
Understanding irregular GPGPU graph applications,” in Proceedings
Communication in various forms consumes substantial on- of the IEEE International Symposium on Workload Characterization
(IISWC),2013.
chip energy. Fundamentally rethinking the representation of
[11] S. Cho and H. Lee, “Flip-n-write: A simple deterministic technique
data can reduce energy spent on communications. Line coding to improve PRAM write performance, energy and endurance,” in
originating in the information theory community offers insight Proceedingsofthe52ndAnnualIEEE/ACMInternationalSymposium
onMicroarchitecture(MICRO),2019.
into more efficient on-chip communication. In this paper, we
[12] B.Dally,“Challengesoffuturecomputing,”inHotPar,2013.
apply line coding to OCN communication. Two particular [13] R. Das, A. Mishra, C. Nicopoulos, D. Park, V. N. an dR. Iyer,
insights are gained from our results: 1) designing codes based M.Yousif,andC.Das,“Performanceandpoweroptimizationthrough
data compression in network-on-chip architectures,” in International
onequiprobabledatadoesnotachievethefullgainspossiblein
SymposiumonHighPerformanceComputerArchitecture,2008.
real applications and 2) the discrete flit sizes in OCNs impact [14] B. K. Daya, C.-H. Chen, S. Subramanian, W.-C. Kwon, S. Park,
the desirable line code rates. Furthermore, our line codes have T.Krishna,J.Holt,A.P.Chandrakasan,andL.-S.Peh,“SCORPIO:a36-
coreresearchchipdemonstratingsnoopycoherenceonascalablemesh
the added benefit of reducing crosstalk.
NoCwithin-networkordering,”inACM/IEEEInternationalSymposium
onComputerArchitecture,2014,pp.25–36.
ACKNOWLEDGMENTS [15] C.Duan,A.Tirumala,andS.Khatri,“Analysisandavoidanceofcross-
talkinon-chipbuses,”inHotInterconnects9,2001.
This material is based on work supported by the National
[16] H.Farrokhbakht,P.Gratz,T.Krishna,J.SanMiguel,andN.Enright
Science Foundation under grant CCF-171-7602. Any opinions, Jerger, “Stay in your lane: A NoC with low-overhead multi-packet
findings and conclusions or recommendations expressed in bypassing,”inInternationalSymposiumonHighPerformanceComputer
Architecture,2022.
this material are those of the authors and do not necessarily
[17] G.D.Forney,“Cosetcodes.i.introductionandgeometricclassification,”
reflect the views of the U.S. National Science Foundation. We IEEETransactionsonInformationTheory,vol.34,no.5,pp.1123–1151,
gratefullyacknowledgethesupportoftheNaturalSciencesand 1988.
[18] G.D.Forney,“Cosetcodes.ii.binarylatticesandrelatedcodes,”IEEE
Engineering Research Council of Canada (NSERC) Discovery
TransactionsonInformationTheory,vol.34,no.5,pp.1152–1187,1988.
Grant RGPIN-2020-04179. This research was undertaken, in [19] A. Gutierrez, B. M. Beckmann, A. Dutu, J. Gross, M. LeBeane,
part, thanks to funding from the Canada Research Chairs J.Kalamatianos,O.Kayiran,M.Poremba,B.Potter,S.Puthoor,M.D.
Sinclair,M.Wyse,J.Yin,X.Zhang,A.Jain,andT.Rogers,“Lostin
program and through the support of the University of Toronto
abstraction:Pitfallsofanalyzinggpusattheintermediatelanguagelevel,”
McLean Award. in2018IEEEInternationalSymposiumonHighPerformanceComputer
Architecture(HPCA),2018,pp.608–619.
REFERENCES [20] M. Hayenga and M. Lipasti, “The nox router,” in Proceedings of the
44thAnnualIEEE/ACMInternationalSymposiumonMicroarchitecture,
[1] N.Agarwal,T.Krishna,L.-S.Peh,andN.K.Jha,“GARNET:Adetailed
2011,pp.36–46.
on-chipnetworkmodelinsideafull-systemsimulator,”inProceedingsof
[21] R. Hesse, J. Nicholls, and N. Enright Jerger, “Fine-grained band-
theIEEEInternationalSymposiumonPerformanceAnalysisofSystems
widthadaptivityinnetworks-on-chipusingbidirectionalchannels,”in
andSoftware,2009.
IEEE/ACMSixthInternationalSymposiumonNetworks-on-Chip,2012,
[2] A. Alameldeen and D. Wood, “Frequent pattern compression: A
pp.132–141.
significance-based compression scheme for L2 caches,” University of
[22] Y.Hoskote,S.Vangal,A.Singh,N.Borkar,andS.Borkar,“A5-GHz
Wisconsin-Madison,Tech.Rep.1500,2004.
mesh interconnect for a teraflops processor,” IEEE Micro, pp. 51–61,
[3] J.Albericio,P.Judd,T.Hetherington,T.Aamodt,N.EnrightJerger,and
2007.
A.Moshovos,“Cnvlutin:Ineffectual-neuron-freedeepneuralnetwork
[23] J. Howard, S. Dighe, Y. Hoskote, S. Vangal, D. Finan, G. Ruhl,
computing,” in Proceedings of the 43rd International Symposium
D.Jenkins,H.Wilson,N.Borkar,G.Schrom,F.Pailet,S.Jain,T.Jacob,
on Computer Architecture, 2016, p. 1–13. [Online]. Available:
S.Yada,S.Marella,P.Salihundam,V.Erraguntla,M.Konow,M.Riepen,
https://doi.org/10.1109/ISCA.2016.11
G.Droege,J.Lindemann,M.Gries,T.Apel,K.Henriss,T.Lund-Larsen,
[4] R. Balasubramonian, A. B. Kahng, N. Muralimanohar, A. Shafiee,
S.Steibl,S.Borkar,V.De,R.VanderWijngaart,andT.Mattson,“A
and V. Srinivas, “Cacti 7: New tools for interconnect exploration in
48-coreIA-32message-passingprocessorwithDVFSin45nmCMOS,”
innovativeoff-chipmemories,”ACMTransactionsonArchitectureand
inProceedingsoftheInternationalSolid-StateCircuitsConference,2010.
CodeOptimization,vol.14,no.2,jun2017.
[24] D.A.Huffman,“Amethodfortheconstructionofminimumredundancy
[5] M.Besta,S.Hassan,S.Yalamanchili,R.Ausavarungnirun,O.Mutlu,
codes,”ProceedingsoftheIRE,vol.40,no.10,1952.
andT.Hoefler,“SlimNoC:Alow-diameteron-chipnetworktopology
[25] A.Jantsch,R.Lauter,andA.Vitkowski,“Poweranalysisoflinklevel
forhighenergyefficiencyandscalability,”inInternationalConference
and end-to-end data protection in networks on chip,” in 2005 IEEE
onArchitecturalSupportforProgrammingLanguagesandOperating
International Symposium on Circuits and Systems (ISCAS), 2005, pp.
Systems,2018.
1770–1773Vol.2.
[6] C.Bienia,S.Kumar,J.P.Singh,andK.Li,“ThePARSECbenchmark
[26] Y.Jin,K.H.Yum,andE.J.Kim,“Adaptivedatacompressionforhigh-
suite:Characterizationandarchitecturalimplications,”inProceedings
performancelow-poweron-chipnetworks,”inInternationalSymposium
of the 17th International Conference on Parallel Architectures and
onMicroarchitecture,2008.
CompilationTechniques,2008.
[27] M.KarandT.Krishna,“Acaseforlowfrequencysinglecyclemultihop
[7] N.Binkert,B.Beckmann,G.Black,S.Reinhardt,A.Saidi,A.Basu,
NoCsforenergyefficiencyandhighperformance,”in2017IEEE/ACM
J. Hestness, D. Hower, T. Krishna, S. Sardashti, R. Sen, K. Sewell,
InternationalConferenceonComputer-AidedDesign(ICCAD). IEEE,
M.Shoaib,N.Vaish,M.Hill,andD.Wood,“Thegem5simulator,”ACM
2017,pp.743–750.
SIGARCHComputerArchitectureNews,2011.
[28] R.KumarandS.P.Khatri,“Crosstalkavoidancecodesfor3dvlsi,”in
[8] B. Bishop and A. Bahuman, “A low-energy adaptive bus coding
ProceedingsofDesign,Automation&TestinEurope,2013.
scheme,” in Proceedings IEEE Computer Society Workshop on VLSI
[29] G.Kurian,S.Devadas,andO.Khan,“Locality-awaredatareplication
2001.EmergingTechnologiesforVLSISystems,2001,pp.118–122.
inthelast-levelcache,”inIEEE20thInternationalSymposiumonHigh
PerformanceComputerArchitecture(HPCA). IEEE,2014,pp.1–12.
12[30] D.Lee,M.O’Connor,andN.Chatterjee,“Reducingdatatransferenergy International Symposium on Reconfigurable Communication-Centric
byexploitingsimilaritywithinadatatransaction,”inIEEEInternational Systems-on-Chip,2015.
SymposiumonHighPerformanceComputerArchitecture(HPCA),2018, [41] K.Soleimani,A.Patooghy,N.Soltani,L.Bu,andM.Kinsy,“Crosstalk
pp.40–51. freecodingsystemstoprotectNoCchannelsagainstcrosstalkfaults,”in
[31] K. Lee, S.-J. Lee, and H.-J. Yoo, “SILENT: serialized low energy InternationalConferenceonComputerDesign,2017.
transmissioncodingforon-chipinterconnectionnetworks,”inIEEE/ACM [42] S. Sridhara and N. Shanbhag, “Coding for reliable on-chip buses: A
InternationalConferenceonComputerAidedDesign,2004,pp.448–451. classoffundamentalboundsandpracticalcodes,”IEEETransactions
[32] Y.Li,X.Dong,andW.Wang,“Additivepowers-of-twoquantization:An onComputer-AidedDesignofIntegratedCircuitsandSystems,vol.26,
efficientnon-uniformdiscretizationforneuralnetworks,”inInternational no.5,2007.
ConferenceonLearningRepresentations,2020. [43] S. Sridhara and N. Shanbhag, “Coding for reliable on-chip buses:
[33] M. H. Lipasti, C. B. Wilkerson, and J. P. Shen, “Value locality and fundamentallimitsandpracticalcodes,”in18thInternationalConference
loadvalueprediction,”inProceedingsoftheInternationalConference on VLSI Design held jointly with 4th International Conference on
onArchitecturalSupportforProgrammingLanguagesandOperating EmbeddedSystemsDesign,2005,pp.417–422.
Systems,2007. [44] M.StanandW.Burleson,“Bus-invertcodingforlow-powerI/O,”IEEE
[34] S.Naffziger,N.Beck,T.Burd,K.Lepak,G.H.Loh,M.Subramony,and Transactions on Very Large Scale Integration (VLSI) Systems, vol. 3,
S.White,“PioneeringchiplettechnologyanddesignfortheamdEPYC™ no.1,pp.49–58,1995.
andRyzen™processorfamilies:Industrialproduct,”in2021ACM/IEEE [45] C.Sun,C.O.Chen,G.Kurian,L.Wei,J.Miller,A.Agarwal,L.Peh,
48thAnnualInternationalSymposiumonComputerArchitecture(ISCA), and V. Stojanovic, “DSENT - a tool connecting emerging photonics
2021,pp.57–70. withelectronicsforopto-electronicnetworks-on-chipmodeling,”in2012
[35] M.Palesi,G.Ascia,F.Fazzino,andV.Catania,“Dataencodingschemes IEEE/ACMSixthInternationalSymposiumonNetworks-on-Chip,2012.
innetworksonchip,”IEEETransactionsonComputer-AidedDesignof [46] D.TangandR.Bahl,“Blockcodesforaclassofconstrainednoiseless
IntegratedCircuitsandSystems,vol.30,no.5,pp.774–786,2011. channels,”InformationandControl,vol.17,no.5,pp.436–461,1970.
[36] M.Palesi,F.Fazzino,G.Ascia,andV.Catania,“Dataencodingforlow- [47] R. Venkatesan, Y. Shao, B. Zimmer, J. Clemons, M. Fojtik, N. Jian,
powerinwormhole-switchednetworks-on-chip,”in200912thEuromicro B. Keller, A. Klinefelter, N. Pinckney, P. Raina, S. Tell, Y. Zhang,
ConferenceonDigitalSystemDesign,Architectures,MethodsandTools, B.Dally,J.Emer,T.Gray,S.Keckler,andB.Khailany,“A0.11pj/op,
2009,pp.119–126. 0.32-128tops,scalablemulti-chipmodule-baseddeepneuralnetwork
[37] A.Patooghi,M.F.Torkaman,andM.Elahi,“Yourhardwareisallwired acceleratordesignedwithahigh-producitivityvlsimethodology,”inHOT
up!attackingnetwork-on-chipsviacrosstalkchannel,”inProceedings CHIPS,2019.
ofthe12thInternationalWorkshoponNetworkonChipArchitectures, [48] S.WangandE.Ipek,“Reducingdatamovementenergyviaonlinedata
2019. clusteringandencoding,”in201649thAnnualIEEE/ACMInternational
[38] H. Seol, W. Shin, J. Jang, J. Choi, J. Suh, and L.-S. Kim, “Energy SymposiumonMicroarchitecture(MICRO). IEEE,2016,pp.1–13.
efficientdataencodingindramchannelsexploitingdatavaluesimilarity,” [49] S. Werner, J. Navaridas, and M. Luja´n, “Designing low-power, low-
in Proceedings of the 43rd International Symposium on Computer latencynetworks-on-chipbyoptimallycombiningelectricalandoptical
Architecture,2016,p.719–730. links,”inIEEEInternationalSymposiumonHighPerformanceComputer
[39] K. Sewell, R. G. Dreslinski, T. Manville, S. Satpathy, N. Pinckney, Architecture(HPCA). IEEE,2017,pp.265–276.
G.Blake,M.Cieslak,R.Das,T.F.Wenisch,D.Sylvester,D.Blaauw, [50] J. Zhan, M. Poremba, Y. Xu, and Y. Xie, “No∆: Leveraging delta
andT.Mudge,“Swizzle-switchnetworksformany-coresystems,”IEEE compression for end-to-end memory access in noc based multicores,”
JournalonEmergingandSelectedTopicsinCircuitsandSystems,vol.2, in 2014 19th Asia and South Pacific Design Automation Conference
no.2,pp.278–294,2012. (ASP-DAC),2014.
[40] Z.ShirmohammadiandS.Miremadi,“S2sap:Anefficientnumerical- [51] Q.Zou,D.Niu,Y.Cao,andY.Xie,“3DLAT:TSV-based3DICscrosstalk
based crosstalk avoidance code for reliable data transfer of nocs,” in minimizationutilizinglessadjacenttransitioncode,”inProceedingsof
theAsiaandSouthPacificDesignAutomationConference,2014.
13