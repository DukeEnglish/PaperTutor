RetAssist: Facilitating Vocabulary Learners with Generative
Images in Story Retelling Practices
QIAOYICHEN,
SunYat-senUniversity,China
SIYULIU,
SunYat-senUniversity,China
KAIHUIHUANG,
SunYat-senUniversity,China
XINGBOWANG,
CornellUniversity,UnitedStates
XIAOJUANMA, TheHongKongUniversityofScienceandTechnology,China
JUNKAIZHU,
GuangdongPolytechnicofIndustryandCommerce,China
ZHENHUIPENG∗,
SunYat-senUniversity,China
Readingandrepeatedlyretellingashortstoryisacommonandeffectiveapproachtolearningthemeanings
andusagesoftargetwords.However,learnersoftenstrugglewithcomprehending,recalling,andretellingthe
storycontextsofthesetargetwords.InspiredbytheCognitiveTheoryofMultimediaLearning,weproposea
computationalworkflowtogeneraterelevantimagespairedwithstories.Basedontheworkflow,wework
withlearnersandteacherstoiterativelydesignaninteractivevocabularylearningsystemnamedRetAssist.It
cangeneratesentence-levelimagesofastorytofacilitatetheunderstandingandrecallofthetargetwords
inthestoryretellingpractices.Ourwithin-subjectsstudy(N=24)showsthatcomparedtoabaselinesystem
withoutgenerativeimages,RetAssistsignificantlyimproveslearners’fluencyinexpressingwithtargetwords.
ParticipantsalsofeelthatRetAssisteasestheirlearningworkloadandismoreuseful. Wediscussinsightsinto
leveragingtext-to-imagegenerativemodelstosupportlearningtasks.
CCSConcepts:•Human-centeredcomputing→Interactivesystemsandtools;EmpiricalstudiesinHCI.
AdditionalKeyWordsandPhrases:Vocabularylearning,storyretelling,imagegeneration
ACMReferenceFormat:
QiaoyiChen,SiyuLiu,KaihuiHuang,XingboWang,XiaojuanMa,JunkaiZhu,andZhenhuiPeng.2024.
RetAssist:FacilitatingVocabularyLearnerswithGenerativeImagesinStoryRetellingPractices.InDesigning
InteractiveSystemsConference(DIS’24),July1–5,2024,ITUniversityofCopenhagen,Denmark.ACM,New
York,NY,USA,28pages.https://doi.org/10.1145/3643834.3661581
1 INTRODUCTION
Learningvocabularyinmeaningfulcontexts,suchasstoriesandimagesinlanguagelearningtext-
books,andvideoclipsfrommovies,isacommonandeffectivepracticeasitenablesdeepandactive
processingofvocabulary(e.g.,wordassociations,logic)[50].Human-ComputerInteraction(HCI)
researchershaveexploredvarioustechnologiestosupportvocabularylearnerswithmeaningful
contextsinvariouslearningactivities,e.g.,ViVoinwatchingvideos[74],VocabEncounter inreading
onlinearticles[3],andEnglishBot inconversingwithothers[61].Inthispaper,wefocusonthe
storyretellingactivitythatencouragesEnglish-as-the-Second-Languagevocabularylearnersto
integrate,reconstruct,anddemonstratethecontextualizeduseofthetargetwordsinashortstory
∗Correspondingauthor.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfee
providedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthe
fullcitationonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthantheauthor(s)mustbehonored.
Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires
priorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ACMISBN979-8-4007-0583-0/24/07...$15.00
https://doi.org/10.1145/3643834.3661581
1
4202
yaM
32
]CH.sc[
1v49741.5042:viXraDIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
[23,35,42,44].Thispracticetypicallyinvolvestwostages–storycomprehensionandrepeated
retelling[47],i.e.,thelearnerfirstreadsorlistenstoashortstorythatcontainsasetoftargetwords
tocomprehenditsmainideaandthenverballyretellsitformultiplerounds.Severalstudieson
languageeducationhavedemonstratedtheeffectivenessofstoryretellingforvocabularylearning
[16,23,43],especiallyinrememberingthemeaningsoftargetwordsandusingtheminverbal
expressions[16,61].Infact,storyretellinghasbeenincludedintheEnglishtestoftheCollege
EntranceExaminationinChina1.
However,thestoryretellingpracticeisoftenchallengingforlearnersofsecondlanguagevocab-
ulary. Foronething,inthestorycomprehensionstage,learnersneedtoassociatethemeaningsof
targetwordswiththestorycontextandmemorizethestoryflowforthelaterrepeatedretelling
practice[33,44,46].Foranother,intherepeatedretellingstage,theyshouldrepeatedlynarratethe
readstorywithrequirementsonthecorrectusageoftargetwordsandfluencyinspeakingthem
outinthestory[33,44,46].Inotherwords,itrequireslearnerstounderstand,memorize,recall,
organize,andspeakthetargetwordsandassociatedstory[33].Thisbecomesmorechallenging
whenthereisatimelimitforeachroundofrepeatedretelling,whichcouldhelptodeveloplanguage
fluencyunderpressure[46]. Imagesrelatedtothestorycanhelpvocabularylearnerscopewith
thesetwochallengesduringthestoryretellingpractice.AssuggestedbytheCognitiveTheoryof
MultimediaLearning(CTML)[53],buildingmentalrepresentationsfromtextandvisualelements
couldfacilitatecomprehensionandrecallofwordsandtheircontextualizedusage[19,41,49,65].
Inthecontextofsecondlanguageacquisition,individualstendtosubvocallyarticulatethetext
associatedwithvisualstimuliintheirnativelanguage[52].Thus,comparedtolearningwithout
visualaids,non-verbalmodalitiessuchasimagesbridgethegapbetweentwodifferentlanguages,
whichwouldenhancethelikelihoodofrecallingthesecondlanguage[51,52]. Giventhesebenefits,
languageeducatorswidelypreparerelevantimagesforthetextualstoriesincoursebooksoronline
resources,andHCIresearchershaveproposedvocabularylearningsupportsystemsinactivities
thatinvolvevisualelements[1,29,74].Forexample,CoSpeak[1]usesvoicerecognitiontechniques
tosupportstudentstocollaborativelyandverballycreateastorygivenanimageprompt.However,
itistime-consumingandoftenunavailabletopreparerelevantimagesforthestorywithanysetof
targetwordsthatuserswishtolearninthestoryretellingpractices,whileirrelevantimageswould
confuselearnersandreducevocabularylearningoutcome[26].
Inthiswork,weexplorethedesignandusageofgenerativeimagestofacilitatethelearningof
anytargetwordsetviareadingandrepeatedlyretellingashortstorythatcontainsthesewords.Our
focusismotivatedbythebenefitsofimagesforvocabularylearningasdescribedaboveandrecent
advancesintext-to-imagegenerativetechniques.Forexample,thepre-trainedLatentDiffusion
Model(LDM)[60]isabletogeneratehigh-qualityandcontent-relevantimagesgivenatextprompt.
Thesegenerativetechniqueshavebeenusedtosupportthecreationsofartworks[21],medical
images[20],andgamecharacters[17]. Nevertheless,littlework,ifany,hasexploredgenerative
images for supporting vocabulary learning in the story retelling practices where users should
mastertargetwords’meaningsandverbalexpressions.Questionsarisesuchas1)whetherandhow
text-to-imagegenerativetechniquescangeneraterelevantimagesofanystorythatcoversatarget
wordset,2)ifso,whatkindsofsupportthatthegenerativeimagescanofferinthestoryretelling
practices,and3)howwouldthesupportfromgenerativeimagesimpacttheusers’vocabulary
learningoutcomeandexperience.
To this end, we seek to provide insights into these questions by designing, developing, and
evaluatinganintelligentsystemprototype,RetAssist,thatcangeneraterelevantimagesforlearning
vocabularyinthestoryretellingpractices.Here,wetargetEnglish-as-the-Second-Language(ESL)
1https://gaokao.eol.cn/guang_dong/dongtai/201811/t20181101_1631228.shtml
2RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
Chineselearners,e.g.,high-schooloruniversitystudentsinChina.Wetakeaniterativedesign
approachwithinsightsfromeducationalliteratureandtheinvolvementofESLlearnersandEnglish
teachersinthisprocess.Wefirstdevelopatext-to-imagecomputationalworkflowandvalidateits
capabilityingeneratingaseriesofcoherentandrelevantsentence-levelimagesgivenanyshort
textualstorythatcontainsIELTS2targetwords. Wethenconductaninterviewstudywithseven
ESLlearnerstounderstandtheirchallengesandneedsinthestoryretellingpracticesandaskfor
theircommentsonthegenerativeimages.Basedontheinsightsfromtheinterviewsandeducational
literature,wedevelopaRetAssist prototypeandseekfeedbackfromanother18ESLlearnersand
twoEnglishteacherstorefineit.InthestoryretellingpracticewiththerefinedRetAssist,userscan
first readandlistentothestorywithgenerativeimagesalignedtoeachsentence.Then,during
eachroundofrepeatedretelling,userscanretellthestorybyviewingtheimages. Aftereachround,
userscanreviewtheirperformanceintheexpressionsoftargetwordsandre-readthestorywith
images.
Weconductawithin-subjectsstudywith24ESLvocabularylearnerstoevaluatetheimpactof
RetAssist’sfunctiononthegenerativeimageonthevocabularylearningoutcomeandexperience.
Theresultsshowthatcomparedwiththebaselinesystemwithoutgenerativeimages,participants
usingRetAssist significantlyoutperforminfluentlyusingthetargetwordsinverbalexpressions.
ParticipantsfavorthegenerativeimagesofRetAssist forreducinglearningworkloadandaiding
recallofthecontextualusageoftargetwordsinthestory. Basedonourfindings,wehighlightthe
valueoftext-to-imagegenerativetechniquesinofferingusefullearningmaterialsandenjoyable
learning experiences. We further discuss design considerations for future vocabulary learning
supportsystemsandtheimpactofourworkongenerativeAIsforeducation.
Ourworkmakesthreecontributions.First,wepresentavocabularylearningsystemRetAssist
thatusesgenerativeimagestofacilitateuserstomastertargetwords’meaningsandexpressionsvia
storyretellingpractices.Second,ourdesignandevaluationofRetAssist providefirst-handfindings
onthefeasibility,effectiveness,anduserexperienceofapplyingtext-to-imagegenerativemodels
tovocabularylearning.Third,weproposeastorytext-to-imagegenerationworkflowandoffer
designconsiderationsofleveraginggenerativemodelstosupportlearningtasks.
2 RELATEDWORK
Weintroducepriorstudiesthatmotivate,inspire,andsupportthedesignofRetAssist,including
storyretellingforvocabularylearning,vocabularylearningsystems,andtext-to-imagegeneration
techniques.
2.1 StoryRetellingforVocabularyLearning
Storyretellingisawell-recognizedapproachthathelpsstudentsacquirevocabularyandskills
likereading,listening,andspeakinginlanguagelearningandteaching[42,44,46,47].Astory
retellingpracticenormallyconsistsoftwostages,i.e.,storycomprehensioninwhichlearnerslisten
orreadagivenstory,andrepeatedretellinginwhichtheyspeakitoutforseveraltimeswithina
timelimit[46,47].AssuggestedbyNationetal.[46],itisapracticethatproperlyintegratesfour
typicalstrandsofactivities,i.e.,meaning-focusedinput,meaning-focusedoutput,language-focused
learning, and fluency development. First, in the story comprehension stage, learners focus on
understandingthegivenstorywithtargetwords–usinglanguagereceptively(meaning-focus
input)[46].Next,intherepeatedretellingstage,learnersarerequiredtocorrectlyandfluentlyspeak
thestoryout–usinglanguageproductively(meaning-focusoutput)[46].Moreover,inthewhole
2ShortforInternationalEnglishLanguageTestingSystem,agloballyrecognizedstandardizedtestdesignedtoassessthe
Englishlanguageproficiencyofindividuals.
3DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
practice,learnersshouldspecificallypayattentiontothemeanings,pronunciations,andcorrect
usagesofthetargetwords–deliberatelearningoflanguagefeatures(language-focusedlearning)
[46].Lastly,thepracticerequireslearnerstomakethebestuseofwhattheyalreadyknowtoperform
wellinretellingundertimepressure[6,30,33,44],whichisatypicalfluencydevelopmentlearning
activity[46].Inall,storyretellingencourageslearnerstointegrate,reconstruct,anddemonstrate
thecontextualuseofvocabulary[23,42,44].Theexpectedlearningoutcomeis,therefore,notonly
onmemorizationoftargetwords’meaningsbutalsoonthecapacityofusingthewordscorrectly
andfluentlyinlanguageexpressions[16,22].
Giventheserequirementsofreading,interpreting,memorizing,andspeakingthestorywith
targetwords[33,44,46],storyretellingisoftenchallengingforlearners.Traditionally,thereare
additionalmaterials(e.g.,imagesandprops)tothetextualstoryandin-situguidancefromteachers
(e.g.,promptingphrases)toassistlearnersinthestoryretellingpractices[14,16]. Imagesrelevantto
thestory,forexample,arebeneficialinthattheycanhelplearnersrememberthewords’meanings
and comprehend the story [2, 19, 49]. Images can also serve as the visual guidance that helps
learners recall the story and target words when they get stuck in the repeated retelling stage
[68]. AccordingtoCognitiveTheoryofMultimediaLearning(CTML)[19,41,49,65]andDual
CodingTheory(DCT)[51,53],buildingmentalrepresentationsfromtextandvisualelementscould
enhancetheencodingandretentionofinformationbyleveragingdualcoding,whichtapsinto
bothverbalandvisualprocessingsystemsinthebrain.AstheextensionofDCT,BilingualDual
CodingTheory(BDCT)[52]suggeststhatimagesenhancesecondlanguagelearningsincelearners
covertlypronouncethecontentoftheimagesintheirnativelanguageandthecontentandthe
imagesconvergeontheforeignlanguageresponses,increasingtheprobabilityofrecallrelativeto
theconditionwithoutimages.Furthermore,Mayeridentifiedthetwelvemultimediainstructional
principlestoaddresstheissueofhowtostructuremultimediainstructionalpracticesandemploy
moreeffectivecognitivestrategiestohelppeoplelearnefficiently[41].Forinstance,thespatial
contiguityprinciple[41]indicatesthatpeoplelearnbetterwhencorrespondingwordsandimages
areplacedneareachotherratherthanfarfromeachotheronthepageorscreen. Insummary,
theseprinciplessuggestthatrelevantvisuals(e.g.,images)cansignificantlyaidinrecallingtextual
information(e.g.,storyinourcase)ofvocabularytofacilitatevocabularylearning.Despitethe
clearbenefits,selectingappropriateimagesthataligncloselywiththetextualcontentremainsa
considerablechallenge[27].
Ourworkismotivatedbythebenefitsofstoryretellingpracticesforenhancingunderstanding
andexpressionoftargetwordsandthehelpfulnessofimagesforassistingusersinthesepractices.
Insteadofrequiringhumanefforttopreparetheimages,weproposetogeneraterelevantimagesto
anystorythatcoversthetargetwordsthatuserswishtolearn.
2.2 VocabularyLearningSystems
ExistingHCIresearchershaveexploredvariousintelligentsystemstosupportvocabularylearning.
Broadlyspeaking,theyareeitherbasedonwordlistsormeaningfulcontexts.Theformertypeof
vocabularylearningsystemaimstofacilitatequickmemorizationoftargetwordsinalist.Previous
workhasincorporatedmodelsofusers’memorycyclesandindividualizedlearningstylesintothese
systems,suchthattheycanrecommendasetoftargetwordswithappropriatelevelsofdifficulty
andrepetitionfrequency[9,48,73].Forexample,Chenetal.[9]proposedapersonalizedmobile
EnglishvocabularylearningsystembasedonItemResponseTheoryandthelearningmemory
cycle.
Context-basedvocabularylearningsystemsleveragevariousformsofmaterialssuchasstories
[1],videos[74],andonlinearticles[3]tohelpuserslearnvocabulary.Forexample,VocabEncounter
[3]encapsulatestargetvocabularyintothecontextofonlinearticles,whileARLang[7]visualizes
4RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
bilinguallabelsonphysicalobjectsoutdoorsinARenvironmenttosupportthemicro-learning
oflanguagewithinitsspatialcontext.Additionally,EnglishBot [61],alanguagelearningchatbot,
engagesstudentsininteractiveconversationsoncollege-relatedtopicstolearnEnglish.Learners
canclickasneededtoreceiveanswerpromptsprovidedintheirnativelanguage,ensuringsmooth
conversationswithEnglishBot [61].InlinewithEnglishBot’smethod,RetAssist allowsusersto
autonomouslyclickoncorrespondingimagesbasedontheircurrentprogresswhencomprehending
orretellingstories. Somestudiesuseimagesasvisualcontextstosupportvocabularylearning.
AIVAS[26]usesanimagererankingalgorithmtoselectimagesthatprominentlycontainrelevant
objectsinthemiddleground,thusaidinginrepresentingconcretenounseffectively.Furthermore,
FCAI [27]considersusers’personalinformation,learningtime,andlocationtorecommendcon-
textuallyappropriateimagesthatbestrepresentthetargetwords.BothAIVAS andFCAI focus
onsearchingforappropriateimagesfortargetwords,whereastheimagesinRetAssist needto
represent the story content, potentially favoring a generative approach. Story retelling also
facilitatescontextualizedvocabularylearning. CoSpeak[1]providesanapplicationforlearnersto
practicespeakingEnglishbypairingthemtogethertoco-createastorywithanimagepromptbased
ontheongoingtopicinclass.UnlikethefocusofourstudyonindividuallearnersusingRetAssist
forvocabularylearningthroughstoryretelling,CoSpeakconcentratesonenhancingEnglishoral
expressionthroughdialoguesbetweentwoindividualsinthematicstorysettings.
OurproposedRetAssist fallsintothecategoryofcontext-basedvocabularylearningsystems.
RetAssistnotonlyintegratesvocabularyintothestorytoprovidetextualcontext,butalsogenerates
asetofrelatedimagesforthestorythatserveasthevisualcontexttohelpindividualvocabulary
learnersacquirevocabularythroughstoryretelling.
2.3 Text-to-ImageGenerationTechniques
AssuggestedbytheDualCodingTheory[51,53],textualstoriespairedwithrelevantimagescan
facilitatevocabularylearning.Recentadvancesintext-to-imagegenerativetechniquesoffergreat
potentialforpreparingvisualaidsforanystorythatcoverslearners’interestedwords.Text-to-
imagegenerativemodelsnormallytakeatextpromptasinputandoutputoneormultipleimages
that are related to the text content. One of the early representatives is Diffusion Probabilistic
Model(DM)[64],whichachievedstate-of-the-artresultsindensityestimation(i.e.,howwellthe
modelcapturestheprobabilitydistributionofthedataset)[32]aswellasinsamplequality(i.e.,
howwellthemodelgeneratesdatasamplesthatcloselyresemblerealdatafromthatdistribution)
[13]. DMsusedeeplearningtechniquestogeneratehigh-qualityimagesfromtextpromptsbut
havethedownsideoflowinferencespeed.Toaddressthedrawback,recentapproacheswidely
leverageLatentDiffusionModels(LDMs)[60],whichworkonacompressedlatentspaceoflower
dimensionalityandspeedupinferencewithalmostnoreductioninimagesynthesisquality.Inthis
work,weuseastate-of-the-artLDMforgeneratingimagesfromtextprompts.
Comparedtothetraditionaltext-to-imagetasksthatgenerateimagesfromatextprompt,image
sequencegenerationforstoriesismorechallengingasitneedstogenerateasequenceofcoherent
andconsistentimagesforastorythatcontainsmultiplesentences.Toenhancetheimagequality
andtheconsistencyofthegeneratedsequences,StoryGAN consistsofadeepContextEncoderthat
dynamicallytracksthestoryflow,andtwodiscriminatorsatthestoryandimagelevels[37].Neural
StoryboardArtist visualizesthestoryintheformofacomicstripthroughtheretrievalofmultiple
relatedimagesfromthestorycontentandseveralimagerenderingstepslikesegmentingrelevant
regionsandconvertingtheimagesintocartoonstyle[10]. Nevertheless,previousstoryimage
generationtechniquesprioritizecoherenceofthewholestoryflow,whichmayoverlookthecontexts
(e.g.,sentences)containingtargetwordsforlearning. Tofacilitatevocabularylearningbasedon
story retelling practices, we segment the whole story into sentences to provide rich contexts
5DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
fortargetwords.Thesentencesareusedaspromptstogenerateasequenceofrelevantimages.
Specifically,weuseaStableDiffusionmodel[63]toconvertsentencesintoimagesandapplyacross-
modalmodel,CLIP[57],toselectthemostrelevantoneforeachsentence.Toimprovethevisual
consistencyandcoherenceoftheimagesequence,wefollow[10]anduseastyletransfermodel
[11]tounifytheimageswithcartoonstyles.Thereafter,ourproposedcomputationalworkflow
cangeneraterelevantandstyle-consistentimagesforstorysentencesasmeaningfulcontextsto
facilitatestoryretellingforvocabularylearning.
3 DESIGNPROCESS
Fig.1. OurdesignanddevelopmentprocessofRetAssistwithEnglishteachersandESLlearners.
Inthissection,weexplainhowwedesignanddevelopRetAssist tofacilitatevocabularylearners
toreadandrepeatedlyretellanystorythatcoverstheirinterestedtargetwords(Figure1).First,
weproposeacomputationalworkflowfortext-to-imagegenerationandvalidateitsfeasibilityin
generatingaseriesofcoherentandrelevantsentence-levelimagesgivenanyshorttextualstory
thatcontainstargetwords.Then,weworkwithvocabularylearnersandteacherstoderivethe
designprinciplesofRetAssist.
3.1 DevelopingaComputationalWorkflowforText-to-ImageGeneration
Toassistusersinthestoryretellingpractices,thegenerativeimagesofastoryshouldsatisfythe
followingtworequirements.First,theimagesshouldbesemanticallyrelevanttothetextualstory.As
suggestedbytheDualCodingTheory[51,53],thebrainprocessesvisualandverbalinformationin
distinctregions.Thevisualchannelhandlesvisualdata,generatingpictorialrepresentations,while
6RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
theverbalchannelprocessesverbalinformation,producingcorrespondingverbalrepresentations.
Whenthevisualandverbalinputsaresemanticallyrelevant,peopleestablishmentalconnections
thatorganizeinformationintocause-and-effectchains[28].Aftertheseconnectionsareformed,
thereisasignificantenhancementintheabilitytorememberinformation[54].Therefore,when
servinglanguagelearning,thevisualinformationintheimageshouldsemanticallymatchwiththe
textcontent.Second,theimagesthemselvesshouldbecoherentintheircontentandconsistentin
stylestodepictastory[41].Otherwise,theimagescouldconfuselearnersinthestoryretelling
practices. Thesetworequirementsguideourdesignchoicesinthecomputationalworkflow,as
detailedbelow(Figure2).
Sub-step1:PreprocesstheStory.Wechoosetogenerateoneimageforeachstorysentence
fortworeasons.First,aseriesofimagesratherthanoneimagecanbetterrevealthelogicofthe
story[53].Second,theSegmentingPrincipal[41]suggeststhatpreparinganimageforeachstory
segmentcanprovidenaturalpausesforlearnerstoabsorbthecontentbeforeproceedingtothenext
segment[31].WeusetheSpacypackageinPythontosplitthestoryintosentences.Tomaintain
thecoherenceamongthegenerativeimages,wefurtherresolvecoreferencesinthestorysentences,
e.g., pronouns like “he” and “it” refer to the objects mentioned earlier. Specifically, we adopt a
pretrainedcoreferenceresolutionmodelnamedNeuralCoref[71]toselectthereferencewordsin
thestorytoreplacethepronounineachsplitsentence.Forexample,fortheredtextinFigure3,
thepronounce“he”inthesecondandfourthsentencesoftheexamplestoryisreplacedby“anold
man”.
Sub-step2:Text-to-ImageGeneration.Afterpreprocessingthestory,weproceedtogenerate
multiple images for each story sentence. Specifically, we leverage a state-of-the-art pretrained
text-to-imagegenerationmodelnamedStable-Diffusion-v1-5,releasedbyRunwayMLandavailable
intheHuggingFacemodelhub[63],becauseofitsdemonstratedcapabilitytogeneratehigh-quality
imagesrelevanttothetext[60]. Themodeloutputsfiveimagesgivenapreprocessedinputstory
sentence.
Sub-step3:Postprocessgenerativeimages.Withthecandidategenerativeimages,wefurther
selectandpolishthemostrelevantimageforeachstorysentence.Theselectionisbasedonthe
semanticsimilaritybetweenthesentenceanditscandidateimages.Specifically,weuseapretrained
cross-modalmodelnamedCLIP[57]toencodethesentenceandimageintovectorsandcompute
thecosinesimilarityoftheimage.Afterselectingtheimages(e.g.,A1-A4inFigure3)withthe
highestsimilarityscoreswiththestorysentences,weseektomitigatethepotentialinconsistencies
amongtheselectedimages,e.g.,thesamehumancharactermaybevisuallyrepresenteddifferently
Fig.2. Ourcomputationalworkflowofgeneratingrelevantimagesforstories.
7DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
Fig.3. Givensentencesofanexamplestoryasinput,wecompareimagesgeneratedbyourcomputational
workflowwiththosegeneratedbytwoalternatives.[Ours(sentence-level,sentence-based)]A1-A4:Images
generatedusingthepreprocessedsentencesasprompts.B1-B4:CartoonstylizationofA1-A4.[Alternative-2
(sentence-level,keyword-based)]C1-C4:Imagesgeneratedusingthekeywords(boldwordsinthepreprocessed
sentencesoftheexamplestory)correspondingtothepreprocessedsentencesasprompts.D1-D4:Cartoon
stylizationofC1-C4.[Alternative-1(story-level)]E:Imagesgeneratedusingtheentirestoryasaprompt.F:
CartoonstylizationofE.
8RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
acrossimages,suchasvariationsinhairorfacialdetails. Weadoptacartoon-styletransfermodel
[11]thatcanconverteachimagetomatchacartoonstylewhilemaintainingtheoriginalstructures,
textures,andbasiccolorsoftheimage(e.g.,B1-B4inFigure3).
3.2 EvaluatingtheFeasibilityofGenerativeImagesforStoryRetellingSupport
Atthisstage,wewouldliketocomparethequalityoftheimagesgeneratedbyourworkflowwith
thosegeneratedbyalternativeapproachesgiventhesameshortstory.Thisevaluationaimsat
validatingifthegenerativeimagesarerelevanttothestory,haveacceptablevisualquality,and
areperceivedashelpfulinhelpinglearnerscomprehendandrecallthestory.Wewillassessthe
effectivenessanduserexperienceofgenerativeimagesinstoryretellinginthelaterexperiments
withvocabularylearners.Inspiredbypriorworkontext-to-imagegeneration[34],story-related
imagesgeneration[37],andtheusageofimagesinstoryretellingpractices[16],wederivethe
followingevaluationmetrics:relevance(Theimagesarerelevanttothestorydescription),visual
quality(Theimagesareclosetotherealscene),perceivedeffectivenessinaidingcomprehension
(Theimagesarehelpfulifyouaregoingtodostorycomprehension),andperceivedeffectivenessin
aidingrecall(Theimagesarehelpfulifyouaregoingtodorepeatedretelling).Eachitemisratedon
astandardfive-pointLikertScale(1for“Stronglydisagree”and5for“Stronglyagree”).
3.2.1 Alternative approaches. We compare our computational workflow with two alternative
approaches for text-to-image generation. The first one, noted as Alternative-1, generates ten
imagesbydirectlyinputtingtheoriginalstorytotheStable-Diffusion-v1-5modelandthenselects
andstylizesthemostrelevantone(e.g.,FinFigure3)similartothesub-step3inourworkflow.
Alternative-1producesasingleimagefortheentirestoryrefertoCoSpeak[1],whichprovidesa
singleimagetoassisttwoEnglishlearnerstoco-createastorythroughdialogue. Thecomparison
withAlternative-1(i.e.,sentence-levelvs.story-level)aimsatcheckingifgeneratingaseriesof
sentence-levelimagescouldbemorehelpfulthangeneratingonestory-levelimage.Thesecond
approach,notedasAlternative-2,usesTextRanktoextractkeywords(e.g.,theboldonesinFigure3)
aspromptstotheStable-Diffusion-v1-5modeltogeneratefiveimages[38].Itthenselectsand
polishesthemostrelevantimage(D1-5inFigure3)foreachsentenceusingthesamepostprocess
methodsinourproposedworkflow.BycomparingourworkflowtoAlternative-2(i.e.,sentence-
basedvs.keyword-based),weaimtoexamineifthesentence-basedpromptwouldbebetterthan
thekeyword-basedprompt,asarelatedworksuggeststhatthesetwopromptswerecomparablein
text-to-imagegenerationtasks[38].
3.2.2 Preparingtargetwordsetsandshortstories. Weprepare20shortstories,eachcontaining
agiventargetwordset,tocomparetheimagesgeneratedbyourproposedworkflowwiththose
generatedbyalternativeapproaches.Thetargetwordsarefromthevocabularypool(3,672wordsin
total)suggestedbytheInternationalEnglishLanguageTestingSystem(IELTS)[12].Threeauthors
ofthispaperrandomlyselectnon-easyIELTSwords(e.g.,notthewordslike“easy”and“general”)
thattheydidnotknowbefore,whicharerandomlyassignedto20sets,eachwithsixorsevenwords.
Thismanipulationsimulatesthecaseinwhichlearnerswouldliketolearnanyinterestedtarget
wordsetviastoryretelling.Toprepareastoryforeachtargetwordset,wefirstqueryChatGPT[5]
with“generateashortstorythathasnomorethan60wordsandmustcontainthewords‘[word1]’,
‘[word2]’,...,and‘[wordn]’”.Thisapproachleveragesthecapabilityoftherecentlargelanguage
modelstogenerateashortstorythatcontainsanytargetwordset[55].Comparedtousingexisting
shortstoriesvalidatedbyEnglishteachers,storiesgeneratedbyChatGPTcanbeflexiblyadapted
tolearners’needsandinterestsonmasteringanytargetwords. Thefirstauthorthenrefinesthe
generatedstoriestoimprovetheirreadability.Finally,weget20shortstories(averagewordlength:
9DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
60,averagenumberofsentences:5)thatcovertopicslikefunnyanimals,disasters,everydaylife,
andtravel.
3.2.3 ProcedureandResults.
Vs. Alternative-1. We prepare a document that lists the 20 stories; following each, there is a
series of images generated by our workflow, an image generated by Alternative-1, and spaces
forraterstoinputtheirscoresforeachmetric.Wedistributethisdocumenttofivehumanraters
(3 males, 2 females, age: 𝑀𝑒𝑎𝑛 = 20.6,𝑆𝐷 = 0.49) recruited from a local university. For each
metric of the generative image(s) for a story, we average the scores of five raters as the final
score.Next,weusepaired-sampleWilcoxonsignedrankteststoanalyzethedifferencesbetween
ourworkflowandAlternative-1oneachmetric.AsdepictedinFigure4,ourworkflowperforms
significantlybetteringeneratingrelevantimage(s)tothestorythantheAlternative-1(𝑝 <0.05,𝑧 =
2.023,Cohen’sd=1.208).Ratersalsoperceivethatourgenerativeimagesaresignificantlymore
effectiveinaidingcomprehension(𝑝 <0.05,𝑧 =2.023,Cohen’sd=1.417)andrecallofthestory
(𝑝 <0.05,𝑧 =2.023,Cohen’sd=1.537). Theseresultsindicatethatgeneratingaseriesofsentence-
levelimagesaboutastorycouldbemorehelpfulinstoryretellingthangeneratingonestory-level
image.
Fig.4. MeansandStandardErrorsofhumanratings Fig. 5. Means and Standard Errors of human rat-
onthequalityofgenerativeimages;1/5-stronglydis- ings on the quality of generative images; 1/5 -
agree/agree;*:𝑝<.05usingpairedsamplesWilcoxon strongly disagree/agree; *: 𝑝 < .05 using paired
signedranktests.WecompareAlternative-1(story- samples Wilcoxon signed rank tests. We com-
level)withOurs(sentence-level)ontheimages’rel- pare Alternative-2 (keyword-based) with Ours
evance(R)tothestory,visualquality(VQ),andef- (sentence-based)ontheimages’relevance(R)tothe
fectivenessinaidingstorycomprehension(E-1)and story,visualquality(VQ),andeffectivenessinaiding
recall(E-2). storycomprehension(E-1)andrecall(E-2).
Vs.Alternative-2.SimilartotheprocedureincomparingwithAlternative-1,werecruitanother
five human raters (3 males, 2 females, age: 𝑀𝑒𝑎𝑛 = 20.4,𝑆𝐷 = 0.27) from the local university
toscoretheimagesgeneratedbyourworkflowandAlternative-2oneachmetricandconduct
paired-sampleWilcoxonsignedranktests. Theorderofencounteringimagesofeachstoryin
theratingdocumentisrandomizedandblindtotheraters.AsshowninFigure5,comparedwith
theAlternative-2,imagesgeneratedbyourworkflowaresignificantlymorerelevanttothestory
(𝑝 < 0.05,𝑧 = 2.023,Cohen’sd = 1.809)andareperceivedsignificantlymoreeffectiveinaiding
comprehension(𝑝 <0.05,𝑧 =2.032,Cohen’sd=0.872)andrecall(𝑝 <0.05,𝑧 =2.032,Cohen’sd=
1.06)ofthestory. Theseresultsindicatethatgeneratingaseriesofsentence-levelimagesabouta
10RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
storyusingthesentence-basedpromptcouldbemorehelpfulinstoryretellingthangeneratingthe
imageseriesusingthekeyword-basedprompt.
Tosumup,theresultsoftheevaluationstudysupportourchoicestogeneratesentence-level
imagesusingsentence-basedprompts.Themeansofthefourmetricsontheimagesgeneratedby
ourcomputationalworkflowarealllargerthanorequalto4outof5points,indicatingitsfeasibility
forgeneratingimagesthatarerelevanttothestory,ofhighvisualquality,andpotentiallyhelpful
tosupportstoryretelling.Wethenproceedtoexplorehowourgenerativeimagescanbeusedto
supportvocabularylearnersintheirstoryretellingpractices.
3.3 ExploringDesignPrinciplesofRetAssist
Withourcomputationalworkflowfortext-to-imagegenerationasthebackboneofRetAssist,we
workwithvocabularylearnersandteacherstoderivedesignprinciplesofRetAssist.
3.3.1 Processofexploringdesignprinciples. Toputforwarddesignprinciplesonhowtobuilda
vocabularylearningsystemthatusesgenerativeimagesinstoryretellingpractices,wefirstconduct
a formative study with seven ESL (English-as-Second-Language) learners. Then, we develop a
workableprototypeofRetAssist.Next,weevaluatetheRetAssistprototypethroughawithin-subjects
studywith18ESLlearners.Accordingtouserfeedbackontheprototype,wepreparearevision
planonRetAssist andsolicitfeedbackfromtwoEnglishteachers.
FormativestudywithsevenESLlearners.Tounderstanduserneedsandrequirementsfora
systemthatprovidesgenerativeimagesinthestoryretellingpractices,weconductaformative
studywithsevenESLcollegestudents(S1-S7,1male,6females,age:𝑀𝑒𝑎𝑛 =20.57,𝑆𝐷 =0.82)in
China.FocusingongatheringthefeedbackandsuggestionsofESLlearners,wedonotspecifically
balancetheorderofretellingwithandwithoutgenerativeimagesinthisinstance. Wefirstinvite
themtoconductonestoryretellingpracticewithgenerativeimages3andtheotherwithoutimages.
Then,weaskquestionsabouttheirperceptionsofthepracticesandtheirexpectationsforasystem
usinggenerativeimagestosupportstoryretelling.ThefindingshereunderpinDP1,DP2,DP4and
DP5inSection3.3.2.
Fig.6. Thestructuredstoryretellingpracticeflowwiththestorycomprehensionandrepeatedretelling
stagesinRetAssistandbaselinesystems.IntheevaluationofRetAssistprototype,thebaselinesystemdoes
nothavefeaturesofspeechtranscript,generativeimages,andfeedback.Intheuserstudyofthefinalversion
ofRetAssist,thebaselinesystemdoesnothavethegenerativeimagesbuthasotherfeatureslikeRetAssist.
PrototypeofRetAssist.Basedontheresultsoftheformativestudy,wedevelopaworkable
prototypeofRetAssist.Thisprototypestructurestheprocedureofstoryretellingpracticeasused
inthefinalversionofRetAssist (Figure6,detailedinSection4)buthasseveralfeaturesdifferent
fromthefinalversionofRetAssist.Forexample,thegenerativeimagesaresequentiallyfixedin
theinterfaceandarenotinteractive.InspiredbythestudyofGuetal.[24],thisprototypewill
3ThestoriesandimagesarelistedinaWordfileandcomefromthematerialsusedintheevaluationofourworkflowin
Section3.2.2.
11DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
promptthenextsentencethatmasksthekeywordwhenusersgetstuckforfivesecondsduringthe
repeatedretellingstage.Besides,aftereachroundofrepeatedretelling,thisprototypeprovides
feedbackabouttheincorrectuseoftargetwordsandtheassociatedsentencebutdoesnotprovide
thestoryandgenerativeimagesforreviewbeforethenextroundofretelling.Thesefeaturesare
discardedorrefinedinthefinalversionofRetAssist basedonthefeedbackfromESLlearnersand
Englishteachers,asdiscussedinSection3.3.2.
EvaluationoftheRetAssistprototype.ToprobeuserexperienceoftheRetAssist prototype
andfeedbacktoimproveit,weconductawithin-subjectsstudywith18ESLlearners(L1-L18,14
females,4males,age:𝑀𝑒𝑎𝑛 =20.56,𝑆𝐷 =1.17).Thetaskandprocedurearesimilartothelateruser
studyofthefinalRetAssist (detailedinSection5.3),exceptthatwedonothavethepretestandthe
twoposttestsinthisstudy.Duringthewithin-subjectsstudy,wegettheirqualitativefeedbackon
howthefeaturesoftheRetAssist prototypeaffecttheirlearningprocess.WecompareourRetAssist
prototypeandabaselinesystemwithoutgenerativeimages,speechtranscription,adaptiveprompts,
andfeedbacktoexplorethenecessityofthesesystemfeatures.Consistentwiththeuserstudy
offinalRetAssist,wecounterbalancetheorderoftheusedsystemsandencounteredwordsets
usingLatinSquare. Afterthelearningsessions,weaskabouttheirexperienceinthestoryretelling
practices,theirperceptiontowardsthetwosystems,andsuggestionsforimprovement.Thefindings
hereunderpinDP1-DP5inSection3.3.2.
FeedbackfromtwoEnglishteachers.Basedontheuserfeedback,wepreparearevisionplan
inaPowerPointfilethatdrawspossibledesignsforfeaturesabouttheinteractionwithgenerative
images,promptsintheretellingstage,andfeedbackonuserperformance.Webringthisplanand
ourRetAssist prototypetotwoEnglishteachers(E1,female,age:27;E2,male,age:27)andaskfor
theircritiquesandsuggestions.ThefindingshereunderpinDP1-DP5inSection3.3.2.
3.3.2 Designprinciples. Wefinalizefivedesignprinciples(DPs)basedontheresultsfromthe
designprocess.
DP1:Inthestorycomprehensionstage,thesystemshouldprovidegenerativeimagesto
facilitateusersinunderstandingandrememberingthestoryline. Previouseducationalliter-
aturesuggeststhatimagesdepictingthestorycouldhelplearnerstounderstandstoriesefficiently
[19,49].Ourparticipantsintheformativestudyfavortheconditionwithimagesintheirstory
retellingpracticessincethegenerativeimagescanhelpthemquicklyandcorrectlyunderstand
thestory.“Theassociatedpictureswiththestoryhelpmeunderstandandrememberthestoryline,
enablingmoreefficientstoryretellingpractices”(S7).Also,alllearnersintheevaluationstudy
oftheRetAssist prototypeexpressedtheirfavorforthegenerativeimages.“Iliketoincorporate
imagestounderstandthestory”(L8).BothEnglishteachersbelievethatgenerativeimagesare
practicalmaterialstopromotestorycomprehension.
DP2:Duringeachroundoftherepeatedretellingstage,thesystemshouldofferthe
generativeimagestohelpusersrecallthestorylineyetnotpromptthenextsentence
when users get stuck. As suggested by the Cognitive Theory of Multimedia Learning [53],
visual elements (e.g., figures) associated with the story can facilitate recall of words and their
contextualizedusage[19,49].TheESLlearnersparticipatinginboththeformativestudyandthe
evaluationstudyindicatethattheimagescanassistthemrecallthestoryandorganizetheirretelling
flow.“Icaneasilyconnectthepicturesbacktothestoryplotwhenretellingthestory”(S2).“I
connecttheimagesprovidedwiththestory,whichhelpsmereflectonthestorylineinashorttime”
(L3).PreviouslearningsupportsystemEnglishBot [61]offersChinesepromptstousersintheir
conversationswithachatbot,andourparticipantsintheformativestudyraisesimilarexpectations
thattheintendedsystemcouldprovidein-situpromptsaboutthestorywhentheygetstuckin
therepeatedretellingstage.“Ratherthanre-readingthefullstory,I’dliketogethintsfromthe
12RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
systemaboutwhat’snextwhenIgetstuckintheretelling”(S3).Nevertheless,asindicatedby
eightparticipantsintheevaluationstudyofRetAssist prototype,theproactivesentenceprompts
duringtherepeatedretellingstageofteninterrupttheirretellingprocessandmayresultintheir
dependenceonthepromptstofinishtheretelling.Additionally,E1andE2agreethatgenerative
imagescanpromoteusers’recallintheretelling,whilesentencepromptsarenotnecessaryoreven
unhelpful.
DP3:Tohelpusersaligntheimagesandstorycontent,thesystemshouldenablethe
userstoselectandenlargeanimagewhilehighlightingtherelatedstorysentence. Asone
ofthetwelvemultimediainstructionalprinciples[41],thespatialcontiguityprinciplesuggeststhat
userscouldbemorefocusedonthelearningtaskswhenrelatedtextandimagearevisuallycloseto
eachother.WiththeRetAssist prototype,fivelearnersintheevaluationstudyalsosuggestthat
theimagesshouldalignwiththestorycontentinamoreclearway.“Ihavetoconsciouslyremind
myselftocombinetheimagestounderstandthetext.Showingalltheimagessimultaneouslyand
fixedlymakesitdifficulttofocusontextandimagesatthesametime”(L1).OurEnglishteachers
help us identify the proper design to visually align the images and story content. “Interaction
designfordisplayingimagesshouldstrikeabalancebetweenindividualimagesandtheoverall
narrative.Wecoulduseanimagesliderthathelpslearnersfocusononeimageatatimewhile
havinganoverviewoftheimagesequence”(E1).“Highlightingthecorrespondingstorysentence
whentheusersenlargeoneoftheimagescouldbeanintuitiveway”(E2).
DP4: In the story retelling stage, the system should provide a speech transcription
functiontorecordtheusers’retellingcontentandhelpthemkeeptrackoftheirprogress.
Asasimilarfeaturewithpreviousretelling-basedEnglishlearningsystemslikeCoSpeak[1]and
EnglishBot [61],ourparticipantsintheformativestudyexpresstheirwishtocheckwhattheyjust
spokeintheretellingexercise.“IwanttoseewhatIhavesaidsofarwhenretelling,whichcanhelp
meorganizewhatIwillsaynext”(S2).Ingeneral,allESLlearnersintheprototypeevaluationand
bothteachersfavorthecomponentofspeechtranscription.“Withspeechtranscription,Icouldpay
attentiontothepronunciationswhenspeaking”(L14).
DP5:Aftereachroundofrepeatedretelling,tohelplearnersreviewtheirperformance,
thesystemshouldofferfeedbackontheincorrectusageoftargetwords,togetherwiththe
storyandgenerativeimages. Providingfeedbackonusers’taskperformanceisacommonand
effectivefeatureinlearningsupporttoolslikeArgueTutor[69]andEnglishBot[61].Fiveparticipants
intheformativestudysuggestthattheywanttogetfeedbackontheirperformanceinpractice,e.g.,
aboutthecorrectnessofwords’expressions.“Itwillbebetterifthesystemcouldindicatewhether
Iwasusingthetargetwordcorrectlyinmyretellingpractice,whichcanhelpmemakeprogress
inthenextretelling”(S1).Moreimportantly,eightparticipantsintheevaluationstudyaccount
thefeedbackfromRetAssist prototypefortheirperceivedimprovementinthelearningoutcome.
“Unlikethebaselinesystem,RetAssist tellsmehowwellIdidinthelastexercise,whichhelpsme
recheckthetargetwords’meaningsandmakeprogressinthenextroundofretelling”(L11).E1and
E2concurontheroleofassessingthecorrectnessofsemanticusagethroughsimilaritymeasures
andagreethatitenableslearnerstoverifytheaccuracyoftheirsemanticexpressions. However,
intheevaluationstudy,sevenlearnerssuggestthatRetAssist wouldbetterpresentthefeedback
togetherwiththestoryandimages,sothattheycanbetterreviewtheirperformanceinthecurrent
roundofrepeatedretellingbeforeproceedingtothenextround.“Ihopetoreviewthestoryand
imagesagainbeforestartingthenextroundofretellingsinceithelpsmefillinsomeofthedetails
fortheretelling”(L13).E1andE2alsoagreethatthereviewofthestoryandimagesbetweentwo
roundsofrepeatedretellingishelpful.
13DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
4 RETASSISTSYSTEMDESIGNANDIMPLEMENTATION
Basedontheidentifieddesignprinciplesandproposedstorytext-to-imagegenerationworkflowin
thelastsection,wedevelopRetAssisttofacilitatevocabularylearnersinstoryretellingpractices.We
developRetAssistasawebappthatcanbeeasilyaccessedbylearnersontheircomputers.Asshown
inFigure6,inthestructuredprocedureofarepeatedretellingpractice,RetAssist providesusers
withgenerativeimagesalignedtothestorysentences(DP3)toassiststorycomprehension(DP1)
andrepeatedretelling(DP2),speechtranscriptionduringrepeatedretelling(DP4),andadaptive
feedbackaftereachroundofretellingpractice(DP5).Wedescribehowvocabularylearnerscanuse
RetAssist inastoryretellingpracticeasfollows.
Storycomprehension.Atthebeginningofastoryretellingpractice,usersneedtofirstacquaint
themselveswiththetargetwords’meaningsandreadthestorythatcontainsthetargetwords
(Figure7-A).Atthisstage,theycanlookupthebilingualdefinitionsandpronunciationofeach
targetwordintheleftpartoftheinterface(A1).Theycanreadthestorywiththetargetedwords
markedinbold(A2).Meanwhile,userscanclickthe“Play”buttontolistentotheaudioofthestory
andclickthe“Translation”icontocheckitsChinesemeanings(A2).Furthermore,userscanclick
eachimagepreviewtoswitchtheenlargedimage(A3).Sucha“sliding”interactiondesignwiththe
imagescouldhelpusersfocusonprocessingoneimageatatimeandcouldbeengaging[66].Users
canalsoseeahighlightedsentenceinthestory(A2)thatcorrespondingtotheenlargedimage.
Suchadesignfollowsthespatialcontiguityprinciple,whichstatesthatuserscanlearnbetterwhen
relatedtextandimageareclosetoeachother[41].
Fig.7. UserinterfacedesignofRetAssist.(A)Inthestorycomprehensionstage,userscan1)checkthetarget
words’meanings,2)readthestory,and3)seetherelevantimagesforeachstorysentence.(B)Intherepeated
retellingstage,userscanretellthestorywith1)thetargetwords,2)theretellingtranscription,and3)the
generativeimages.(C)Aftereachroundofretelling,userscancheckfeedbackontheirperformanceand
review1)thetargetwordswithincorrectmarks,2)thestory,and3)thegenerativeimages.
Repeatedretelling.Aftercomprehendingthetargetwordsandassociatedstory,userscanclick
“Retell”intheuppermenubartoproceedtotherepeatedretellingstage(Figure7-B).Theyneed
tocompletethreeroundsofretellingpracticeswithindecreasingtimelimits,e.g.,120,90,and60
secondsbasedonourtrialsintheformativestudyandevaluationstudyofRetAssistprototype.The
designofdecreasingtimelimitsinthelearningpracticescouldhelpusersdeveloplanguagefluency
14RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
[46].Userscanclick“Retell”tostarteachroundofretellingtrials(B2).Astheyspeak,RetAssist
willtranscribetheirspeechinreal-timeusingChrome’sspeechrecognitionAPI[59].Duringeach
roundofrepeatedretelling,userscanaccessthepronunciationanddefinitionoftargetwordsin
thewordlistanytimetheywant(B1).Thebackgroundcolorofthewordwillturn“blue”when
RetAssist detectsthattheuserspeaksit.Meanwhile,userscanswitchtheimagesliderandclick
eachgenerativeimagetoenlargeitwhenevertheywant(B3).Userscanstopthecurrentround
ofstoryretellingbyclicking“Retelling”again.Then,theycaneditthetranscribedsentencesto
correctspeechrecognitionerrorsinthetextboxiftheywant(B2).
Reviewaftereachroundofrepeatedretelling.Whenusersfinishoneroundofrepeated
retelling,theycanclickthe“Check”button(Figure7-B2)toviewRetAssist’sfeedbackonthetheir
performanceandreviewthestorymaterialwithgenerativeimages(C).Userscancheckwhich
targetwordshavebeencorrectlycontextualized(markedinblueinC1)andwhichwordsarenot
usedorincorrectlyusedintherepeatedretelling(markedinred).Theycanclickeachredwordto
viewitsmeaningsandtheassociatedsentencethattheuserspoke.Userscanalsoreadthestory
withthehighlightedsentencesthatcontainthetargetwordstheyincorrectlyuse(C2).Meanwhile,
theycanchecktheassociatedgenerativeimages(C3).Userscanclickthe“Retell’buttoninthe
upperbartostartthenextroundofrepeatedretelling.
Weusesemanticsimilaritytojudgewhethertheusercorrectlyusesthetargetwords,inspired
bythestudyofCaoetal.[8],whichverifiesthecorrectnessofmachinetranslationbychecking
semanticsimilaritybetweentheoriginalandthetranslatedsentences. Specifically,weconsidera
targetwordisnotcorrectlyusedifthespokensentencethatshouldcontainthiswordissemantically
differentfromtheoriginalstorysentencethatcontainsthisword[62].Wecalculatethesemantic
similarity(rangingfrom0to1)betweentheexpressionofeachtargetwordinthestoryandthatin
theusers’retelling.First,weidentifythesentencecontainingeachtargetwordintheuser’sretelling
andcalculatetheirsentenceembeddingsbySentence-BERT[58].Then,wecomputethecosine
similarity(rangingfrom0to1)betweenthisidentifiedsentenceandthecorrespondingsentence
fromtheoriginalstory.Iftheusermentionsthetargetwordinmultiplesentences,thesimilarity
isrecordedasthemaximumofsimilaritybetweenthemultiplesentencesandthecorresponding
sentencefromtheoriginalstory.Iftheuserdoesnotmentionthetargetword,thesimilarityis
recorded as 0. To decide the thresholds of similarity scores that differentiate the correct and
incorrectuseoftargetwords,threeauthorsmarkthecorrectness(i.e.,correctorincorrect)ofthe
wordusageineachsentenceoftherecordedretelledcontentoftheparticipantsinourformative
study.Aftermarking,weobtaintwosetsofsimilaritiesseparatelyrepresentingthecorrectuseof
wordmeaningsandtheincorrectuseofwordmeaningsbycalculatingthesemanticsimilarities
betweenstorysentencesandspokensentences. Finally,thethresholdisdeterminedtobe0.7based
ontheROCcurvefordifferentsimilarityscores[18].
5 USERSTUDY
ToevaluatehowthegenerativeimagesinRetAssistimpactusers’vocabularylearningoutcomeand
experienceinthestoryretellingpractices,weconductawithin-subjects(RetAssist vs.baseline)
studywith24ESL(English-as-the-Second-Language)universitystudentsinChina.Ourresearch
questionsare:
RQ1.HowwouldRetAssist’sgenerativeimagesaffectusers’learningoutcomesregardingthe
retentionandverbalexpressionoftargetwordsintheirstoryretellingpractices?
RQ2. How would RetAssist’s generative images affect users’ a) learning experience and b)
behaviorsintheirstoryretellingpractices?
RQ3.HowwouldusersperceivetheusefulnessofRetAssist’sgenerativeimagesintheirstory
retellingpractices?
15DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
Fig.8. Userinterfacedesignofthebaselinesystem.(A)Inthestorycomprehensionstage,userscan1)check
thetargetwords’meanings,and2)readthestory.(B)Intherepeatedretellingstage,userscanretellthestory
with1)thetargetwords,and2)theretellingtranscription.(C)Aftereachroundofretelling,userscancheck
feedbackontheirperformanceandreview1)thetargetwordswithincorrectmarks,and2)thestory.The
baselinesystemdiffersfromRetAssistinthatitdoesnotprovidegenerativeimages.
5.1 TheBaselineSystem
Thebaselinesystem(Figure8)supportsthesameuserworkflow(Figure6)instoryretellingpractices
asRetAssist.However,itdoesnotoffergenerativeimagesduringboththestorycomprehension
stageandtherepeatedretellingstage.Thebaselinesystemsimulatesthescenarioinwhichthe
user is required to learn target words via story retelling practices without generative images.
Specifically,thebaselinesystemoffersthewordlist(Figure8-A1)andstory(Figure8-A2)inthe
storycomprehensionstage,anditprovidesdecreasingtimelimitsaswellaswordlist(Figure8-B1)
andspeechtranscript(Figure8-B2)inusers’threeroundsofretellingpractices.Also,userscan
checkadaptivefeedbackregardingtheaccuracyofthetargetwordusage(Figure8-C1)insuch
roundsandreviewthestorytext(Figure8-C2). Suchabaselinesystemsatisfiesalldesignprinciples
withouttheinvolvementofgenerativeimages,specificallyreferringtoDP4andDP5.Insummary,
theonlydifferencebetweenRetAssistandthebaselinesystemliesintheincorporationorexclusion
ofgenerativeimages,whileallotherfunctionalitiesarepresentinbothconditionstomeetusers’
demands.
5.2 Participants
Werecruit24undergraduatestudents(P1-24,15females,9males,meanage:20(SD=1.67))froma
universityinmainlandChinaviaapostinthesocialmedia.Theymajorinvariousdomainssuch
asComputerScience,Historiography,Philosophy,Physics,Finance,Literature,andInternational
Relations.Twenty-threeparticipantshavepassedthenationalEnglishexamCET-4inChina,with
anaveragescoreof575(SD=48.04)4.Seventeenparticipantsadditionallyhavepassedahigher-
levelnationalexamCET-6inChina(Meanscore:523(SD=48.47)).Noneofourparticipantshave
takentheIELTSexam.However,theyexhibitastronginterestinlearningtheirunknownIELTS
vocabularyviathestorytellingpractices(M=5.75,SD=1.05;1-notinterestedatall,7-very
interested).
5.3 ProcedureandTasks
Figure9showstheprocedureandtaskofouruserstudyconductedremotely.Following[3,55],on
Day0,participantsfillinaconsentformandabackgroundsurveyandtakeavocabularypretest.
Werandomlyselect4storiesfromthe20preparedstoriesmentionedinSection3.2.2asthelearning
materialsforallparticipants,eachcontainingsixorseventargetwords. Intotal,thepretestconsists
4710isthefullmarkofbothCET-4andCET-6,and425istheminimumscoretopasstheexams.
16RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
Fig.9. Procedureofthewithin-subjects(RetAssistvs.thebaselinesystem)userstudy.Ineachtask,participants
learntwosetsoftargetwordswitheithertheRetAssistorthebaselinesystem.
ofthe26targetwordsthatparticipantswilllearninourlearningsessions.Foreachtargetword
inthepretest,participantsarerequiredtoselectoneoptionfromfivechoices,includingonethat
givesthecorrectmeaningsofthewordinChinese,threedistractors,andan“Idon’tknow”option.
Accordingtotheresultsofthepretest,theaveragenumberofcorrectlychosenoptionsamong24
participantsis8.62.Inotherwords,onaverage,participantsdonotknowthemeaningsof17.38
wordspriortothelearningsessions. Weinformthemnottolearnthewordsthatappearinthe
pretestbeforethelearningsessions.
OnDay2,participantsfirstwatchourpre-recordedvideothatdescribesthelearningtaskand
introducestheinterfacesofRetAssist andbaselinesystemswithblindnames.Theythenusetheir
laptops to log in to our systems. Each participant has two learning sessions. In each session,
participantshavetwostoryretellingpracticeswitheitherRetAssist orbaselinesystemtolearntwo
targetwordsetsthattheyencounteredinthepretest.Basedonthepilotstudywithtwoparticipants,
weallocate30minutesforeachlearningsession. Aftereachlearningsession,participantsrate
theirengagement,enjoyment,taskworkload,andperceptionsofthesysteminaquestionnaire.In
thequestionnaire,wealsoaskthemtowritedownresponsestosomeshortquestionssoastomake
senseoftheratings. Additionally,theyneedtoconductanimmediateposttestthatexaminestheir
learningoutcomeonrememberingthetargetwords’meaningsandbeingabletoverballyusethem
toretellastory.Uponcompletionoftwolearningsessions,participantsfillinaquestionnairethat
asksthemtowritedowntheirpreferencesontheinterfaces,commentsonthegenerativeimages,
andsuggestionsforimprovingRetAssist. Wecounterbalancetheorderoftheusedsystemsand
wordsetsusingLatinSquare,i.e.,sixparticipantsexperience“set1and2withRetAssist –>set3
and4withBaseline”,six“set1and2withBaseline–>set3and4withRetAssist”,six“set3and4
withRetAssist –>set1and2withBaseline”,andtherestsix“set3and4withBaseline–>set1and
2withRetAssist”.
OnDay9,theyconductadelayedposttestthathasthesameformatastheimmediateposttestto
examinetheirretentionandverbalexpressionoftargetwordslearnedonDay2.Theprocedureon
Day2andDay9isvideo-andaudio-recordedforfurtherdataanalyses.Overall,eachparticipant
spendsapproximatelyonehourandahalfinourstudyandreceives80RMBascompensation.
17DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
5.4 Measurements
5.4.1 RQ1.Learningoutcomes. Wemeasureparticipants’vocabularylearningoutcomesthrough
performanceonanimmediateposttestrightaftereachlearningsessionandadelayedposttestone
weeklater.Specifically,bothposttestsincludeamultiple-choicequizandanexpressivetest.The
multiple-choicequizisthesameasthepretestthatrequiresuserstoselectoneoffiveoptionsthat
isthecorrectChinesemeaningofthetargetword.Wecalculatethenumberofcorrectanswersto
themultiple-choicequestionstocapturethelearningoutcomeonthemeaningsoftargetwords.
Intheexpressiontest,participantsneedtoverballyretelleachstoryintheirlearningsessions
basedonthestorysynopsisinChineseandthetargetwordset.AssuggestedbyourtwoEnglish
teachersinthedesignprocess,wechoosetopresentthesynopsisinsteadofpresentingnothingor
providingthefullChinesetranslationoftheoriginalstorytobalancethedifficultyoftheexpression
test.WeadaptthemarkingschemeoftheIELTSspeakingtest[12]buthaveafocusontheverbal
expressionsoftargetwords.WithconfirmationfromourtwoEnglishteachersinthedesignprocess,
foreachexpressiontestoftwostorieswithinalearningsession,wecapture:
• Numberoftargetwordsused(range0-135).
• Numberoftargetwordspronouncedcorrectly,i.e.,thenumberoftargetwordscorrectly
pronounced.
• Numberoftargetwordsusedcorrectly,i.e.,thenumberoftargetwordsthathavebeenused
semanticallycorrectly.
• Fluency, which is determined by the expression of individual clauses and the lag between
sentences,rangingfrom0to9onascalereferencedtotheIELTSmarkingscheme.
Threeauthorsofourresearchteamfirstindependentlyscoresixrandomlyselectedaudiosamples,
eachconsistingoftworetellingstoriesinalearningsession.Theythenmeetanddiscusstogether
withoneofourtwoEnglishteachers(male,age:29)torefinetheirratingscheme.Forexample,
tofocusontheusageandexpressionoftargetwords,theratingschemeexcludesfactorslikethe
participants’volumeofvoice,intonation,oraccent.Thethreeauthorsthenapplytheratingscheme
toall192(24×2systems×2storiespersystem×2posttests)audiosamplesinashuffledorder.For
eachdimensionofthemeasuredperformanceontheverbalexpressionsoftargetwords,weaverage
thethreeauthors’scores(ICC=0.939)asthefinalscoreineachretellingstory.Foreachofthefirst
threedimensions,weaddthescoresoftwostorieswithinonelearningsessionasuserperformance
inverballyexpressingtargetwordslearnedinthatsession,whileforthelastdimensionoffluency,
weaveragethescoresofthetwostoriesasthefinalscore.
5.4.2 RQ2.Learningprocess. IneachlearningsessionwitheitherRetAssist orbaselinesystem,
wemeasureparticipants’engagementandenjoymentinthelearningprocessusingitemsadapted
from[69,72]:“Iwasabsorbedinusingthisinterfacetolearnvocabulary”and“Itisenjoyableto
learnvocabularywiththisinterface”.Besides,wemeasuretheperceivedtaskworkloadoflearning
sessionsusingitemsadaptedfromNASATaskLoadIndex[25](e.g.,“Irequiremuchmentaland
perceptualactivitysuchasthinkingandrememberingintheprocessofthestoryretellingpractice”).
Inadditiontothequestionnairedata,wealsomeasurehowlearnersperformineachofthethree
roundsofrepeatedretellingineachpractice.Foreachroundofrepeatedretelling,wemeasure:1)
spenttime,i.e.,thetimeperiodbetweenclickingthe“Check”andthe“Retell”buttoninthisround
ofrepeatedretelling;2)performanceinpractice,i.e.,howwelluserscanretellthestorycontent,
reflectedonthesemanticsimilaritybetweenlearners’retoldcontentandoriginalstory(ranging
from0to1,detailedinReviewaftereachroundofrepeatedretellinginSection4).Foreach
5Ineachlearningsession,participantslearnvocabularybasedontwostories.Onecontains6targetwords,andtheother
contains7targetwords.Themaximumscoreforonelearningsessionistherefore6+7=13.
18RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
learningsessionwithtwostoryretellingpractices,weaveragethespenttimeintwopracticesas
themeantimespentinoneroundofrepeatedretellinginthatsession.Similarly,weaveragethe
semanticsimilarityscoresoftwopracticestorevealuserperformanceinoneroundofrepeated
retellingineachlearningsession.
5.4.3 RQ3. Perceptions towards RetAssist. For each system interface, we adapt the technology
acceptancemodel[67,70]tomeasuretheperceivedusefulness(fouritems,e.g.,“Ifindthevocabulary
learningsupportsystemusefulinmyvocabularylearningprocessbystoryretelling”;Cronbach’s
𝛼 =0.921);easinesstouse(fouritems,e.g.,“Myinteractionwiththevocabularylearningsupport
systemisclearandunderstandable”;𝛼 =0.830);andintentiontouse(twoitems,e.g.,“Iintendtobea
heavyuserofthevocabularylearningsupportsystemwhenIwanttolearnvocabulary”;𝛼 =0.901).
Weaveragetheratingsofmultiplequestionsasthefinalscoreforeachaspect.Allstatementsin
thequestionnairesareratedonastandard7-pointLikertScale,with1-stronglydisagreeand7-
stronglyagree.
Fig.10. RQ1resultsregardingthe Fig.11. RQ1resultsregardingthe Fig.12. RQ1resultsregardingthe
numberofcorrectchoicesontar- numberoftargetwordsusedinex- fluencyinexpression.□ :𝑝 < .05
getwords’meanings.□ : 𝑝 < .05 pression(S1),thenumberoftarget fortimefactor(immediateposttest
fortimefactor(pretestvs.imme- wordspronouncedcorrectlyinex- vs. delayed posttest), △ : 𝑝 <
diateposttestvs.delayedposttest) pression (S2), and the number of .05forsystemfactor(RetAssistvs.
usingrepeatedmeasuresANOVA targetwordsusedcorrectlyinex- Baseline)usingrepeatedmeasures
withBonferronipost-hoctest. pression(S3).□:𝑝 < .05fortime ANOVA.
factor(immediateposttestvs.de-
layedposttest)usingrepeatedmea-
suresANOVA.
6 ANALYSESANDRESULTS
Fortherateditems,wefirstconductasetofmixedANOVAteststocheckwhethertheorderof
systemusageorthelearnedwordsetsassociatedwiththesystemsaffectedourresults(orderand
wordsetsasbetween-subjects,systemsaswithin-subjects).Theresultsindicatethatneitherthe
maineffectsoftheorderandwordsetsnortheirinteractionwiththesystemsaresignificant. For
themeasurementsforRQ1,weperformtwo-way(timeandsystem)repeatedmeasuresANOVA
toaccountforthedependenciesintime.AsforthemeasurementsforRQ2andRQ3,weperform
Shapiro-Wilknormalitytestsbeforerunningallthepairedsamplest-tests.Ifthehypothesisthat
thedatasatisfiesanormaldistributionisrejected,weusepairedsamplesWilcoxonsignedrank
testsinstead.Asaresult,forthespenttimeandperformanceineachroundofrepeatedretelling,
weperformpaired-samplet-teststocomparetheRetAssist andthebaselinesystem.Fortherest
19DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
measures,weperformpairedsamplesWilcoxonsignedranktests.Additionally,twoauthorsconduct
opencodingonparticipants’commentsandsuggestionsonbothvocabularylearningsystems.
Theyhavemultipleroundsofdiscussionsandfinallyreachanagreementonthecodes,whichare
incorporatedintothefollowingresultpresentation.
6.1 LearningOutcomes(RQ1)
6.1.1 Multiple-choiceQuiz. AsshowninFigure10,participantsdemonstratecomparableperfor-
mancewithRetAssist (𝑀 = 12.792,𝑆𝐷 = 0.644) andbaselinesystem (𝑀 = 12.625,𝑆𝐷 = 1.033)
regarding the number of correct answers to the multiple-choice questions in the immediate
posttest. In the delayed posttest, participants have better performance on average with RetAs-
sist (𝑀 =12.167,𝑆𝐷 =1.179)thanbaselinesystem(𝑀 =11.667,𝑆𝐷 =1.863)regardingthenumber
ofcorrectanswers.TheresultsofrepeatedmeasuresANOVAindicatethatneitherthesystemfactor
(RetAssist andBaseline)noritsinteractionwiththetimefactor(pretestvs.immediateposttest
vs.delayedposttest)significantlyaffectsparticipants’performanceinthemultiple-choicequiz
(𝑝 > 0.05).However,thetimefactorhassignificanteffectsonparticipants’performanceinthe
multiple-choicequiz(𝑝 <0.001,𝐹 =596.792,𝜂2 =0.912),andtheresultsoftheBonferronipost-hoc
test ensure the significant difference among the three quizzes (pretest vs. immediate posttest:
𝑝 <0.001;pretestvs.delayedposttest:𝑝 <0.001;immediateposttestvs.delayedposttest:𝑝 <0.05).
6.1.2 ExpressionTest. AsshowninFigure11,participantsgenerallyperformwellinusingthetarget
words,pronouncingthemcorrectly,andusingthemcorrectlyintheimmediateexpressionposttest
afterthelearningsessionwitheitherRetAssist orbaselinesystem;𝑀 >10and𝑝 >0.05inallthe
threedimensions.Thisfindingsuggeststhatthestoryretellingpractice,eitherwithorwithoutthe
involvementofgenerativeimages,isaneffectiveapproachtolearningtheverbalexpressionof
targetwordsintheshortterm.Inthedelayedposttestafteroneweekofthelearningsessions,the
userperformancewithbothsystemsnaturallydecreasescomparedtothatintheimmediateposttest.
WefindthatparticipantsareabletousemoretargetwordslearnedwithRetAssist (𝑀 =8.96,𝑆𝐷 =
3.77andusethemcorrectly (𝑀 = 7.5,𝑆𝐷 = 3.77) comparedtothebaselinesystem(usetarget
words:𝑀 = 7.83,𝑆𝐷 = 3.45,usethemcorrectly:𝑀 = 6.375,𝑆𝐷 = 3.89)inaverage.Theaverage
numberofcorrectlypronouncedtargetwordsisalsohigherinthelearningsessionwithRetAssist
(𝑀 =7.875,𝑆𝐷 =3.61)thanthatinthesessionwithbaselinesystem(𝑀 =6.375,𝑆𝐷 =3.89).With
therepeatedmeasuresANOVA,wefindthatneitherthesystemfactor(RetAssist andBaseline)nor
itsinteractionwiththetimefactor(immediateposttestvs.delayedposttest)significantlyaffects
thenumberoftargetwordsused(S1),thenumberoftargetwordspronouncedcorrectly(s2),and
thenumberoftargetwordsusedcorrectly(S3)inexpression(𝑝 > 0.05).Amongtheexpression
measurementsofS1-S3,thetimefactorhassignificanteffects(S1:𝑝 <0.001,𝐹 =49.127,𝜂2 =0.699;
S2:𝑝 <0.001,𝐹 =43.381,𝜂2 =0.712;S3:𝑝 <0.001,𝐹 =76.741,𝜂2 =0.827).
Asforthefluencyofparticipants’spokenEnglishintheimmediateposttest(Figure12),par-
ticipants can tell the story significantly more fluently after the learning session with RetAssist
(𝑀 = 6.604,𝑆𝐷 = 0.935) thatthatwiththebaselinesystem (𝑀 = 6.354,𝑆𝐷 = 0.872).Similarly,
in the delayed posttest, participants’ spoken English in telling the story with target words is
significantlymorefluentafterlearningwithRetAssist (𝑀 = 6.229,𝑆𝐷 = 0.901) comparedtothe
baseline system (𝑀 = 5.688,𝑆𝐷 = 0.966). The results of repeated measures ANOVA indicate
thatboththesystemfactor(RetAssist andBaseline)andthetimefactor(immediateposttestvs.
delayedposttest)significantlyaffectthefluencyofparticipants’spokenEnglish(systemfactor:
𝑝 <0.05,𝐹 =4.01,𝜂2 =0.041;timefactor:𝑝 <0.001,𝐹 =21.552,𝜂2 =0.238). Theseresultssuggest
thatRetAssist’sgenerativeimagescansignificantlyimprovethelearners’fluencyinusing
20RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
targetwordstotellastoryafterthestoryretellingpracticescomparedtothebaseline
system.
Fig.13. RQ2resultsregardingengagement,enjoyment,andworkloadinvocabularylearningsessions.∗:𝑝 <
0.05usingpairedsamplesWilcoxonsignedranktests.
6.2 LearningProcess(RQ2)
6.2.1 Engagement,enjoymentandworkload. AsshowninFigure13,participantsreportaslight
increase in engagement and enjoyment during the vocabulary learning process with RetAssist
comparedtothebaselinesystem,thoughthedifferencewasnotstatisticallysignificant.However,
22(outof24)participantscommendthequalityoftheimagesandfeelthattheimagesareclosely
alignedwiththetext.“Thepicturesareappealing,andIcaninteractwiththembyswitchingthe
pictureandcheckingitsrelatedsentence”(P12). Furthermore,participantsreportalowerlevelof
mentaldemand (𝑝 < 0.05,𝑧 = 2.066,Cohen’sd = 0.455) duringthevocabularylearningprocess
withRetAssist thanthatwiththebaselinesystem. 21participantsperceivethatpracticingstory
retellingwiththebaselinesystemisnotablymorechallenging,astheyneedtomentallyvisualize
andconstructthesceneofthestory.“Recallingthestory’sdetailsandscenarios(withthebaseline
system)takesupalotofmymentaleffort.Incontrast,RetAssist helpsmetorecallthestoryina
visualway”(P6).Fiveparticipantsfurtherreportthatthebaselinesystemismonotonouscompared
toRetAssist.“Idonotlikethe(baseline)interfaceasitismonotonousandinflexible”(P3).
6.2.2 Performance in each round of repeated retelling. Figure 14 shows the spent time in each
roundofrepeatedretellingwithRetAssist andthebaselinesystem.Participantsspendlesstime
withRetAssist inthesecond(𝑀(𝑆𝐷) :113(45.07)vs.137(61.02);𝑝 <0.05,𝑡 =−2.227,Cohen’sd=
0.321) and third (110(47.23) vs. 132(55.28);𝑝 < 0.05,𝑡 = −2.18,Cohen’sd = 0.315) rounds of
repeated retelling compared to the cases with the baseline system. “While using the baseline
system, I frequently run out of the limited time before finishing retelling the story; however,
when using RetAssist, I am more comfortable in the repeated retelling stage and can complete
theretellingontime”(P20).Meanwhile,asshowninFigure15,thesemanticsimilaritybetween
users’retellingcontentandoriginalstorysignificantlyincreasesduringthesecond(𝑝 <0.05,𝑡 =
2.397,Cohen’sd=0.346)andthethirdrounds(𝑝 <0.001,𝑡 =3.793,Cohen’sd=0.547)ofrepeated
retelling. Nineteenparticipantsattributetheirimprovementduringthepracticetotheprovided
images in RetAssist. “Images provided in RetAssist help me better connect my native language
expressionandEnglishexpressionofthetargetwords,whichhelpsmereflectonthedetailsand
storylineinashorttime”(P4).Theseresultsindicatethatcomparedtothebaselinesystem,
21DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
RetAssistcanreducelearners’workloadandimprovetheirefficiencyandperformanceof
eachroundofrepeatedretellingduringthestoryretellingpractices.
Fig.14. RQ2resultsregardingtime Fig.15. RQ2resultsregardingthe Fig.16. RQ3resultsregardinguser
spentbyusersinthreeroundsof semanticsimilaritybetweenusers’ perceptions of each interface. ∗ :
retelling.∗:𝑝 <0.05usingpaired- retellingcontentandstory.∗:𝑝 < 𝑝 < 0.05 using paired samples
samplet-tests. 0.05,∗∗∗:𝑝 <0.001usingpaired- Wilcoxonsignedranktests.
samplet-tests.
6.3 PerceptionstowardstheSystems(RQ3)
AsshowninFigure16,participantsfeelthatourRetAssist (𝑀 =5.365,𝑆𝐷 =1.031)issignificantly
moreusefulthanthebaselinesystem(𝑀 =4.74,𝑆𝐷 =0.996; 𝑝 <0.05,𝑧 =2.29,Cohen’sd=0.604).
NineteenparticipantsimpliedthattheimagesinRetAssist arethereasonforratingitmoreuseful.
“Withouttheimages,Ifinditdifficulttogothroughtherepeatedretellingstage.Theimagesare
especially useful when I am stuck” (P13). There is no significant difference between RetAssist
(𝑀 =4.708,𝑆𝐷 =1.156)andthebaselinesystem(𝑀 =4.604,𝑆𝐷 =1.141)regardingeasinessofuse.
Wehavecommentsfromtwenty-oneparticipantsthatpraisetheinteractiondesignofRetAssist.
“TheinterfaceofRetAssistisintuitive,andtheinteractionflowisclear.Icanlistentothestorywhile
easilyreadingthestorywithalignedimages”(P11).Lastly,participantsgenerallyhaveahigher
intentiontouseRetAssist (𝑀 =4.9,𝑆𝐷 =1.249)forvocabularylearninginthefuturecomparedto
baselinesystem(𝑀 =4.6,𝑆𝐷 =1.337).TwentyparticipantscommentthattheyprefertheRetAssist
forfuturevocabularylearning.“WithRetAssist,Icanexpressthelearnedwordsmorecorrectly
withlesspressure.Iwanttohaveitasmyweeklyusedvocabularylearningsystem”(P16).However,
fourparticipantspreferthebaselinesystem,becausetheyfeelitistime-consumingtoviewthe
imagesandmentallyconnectthemwiththestory.
7 DISCUSSION
Inthiswork,wedeveloptheRetAssist systemthataimstofacilitatevocabularylearnersintheir
storyretellingpractices.Itscorefeaturesarethegenerativeimagesrelevanttothestoryinthe
storycomprehensionandrepeatedretellingstages.Ourstudyshowsthatparticipantsusingeither
RetAssist orthe baselinesystem canmaster themeanings andexpressionsof thetarget words
right after a story retelling practice, supporting that story retelling is an effective approach to
vocabularylearning[16,23,42,44].However,oneweekafterthepractices,participantsbetterrecall
andverballyexpressthetargetwordslearnedwithRetAssist thanthosewiththebaselinesystem.
Thisprovesthevalueofourgenerativeimagesforsupportingvocabularylearningandprovides
empiricalevidenceforthebenefitsofvisualaidsforlanguagelearningstatedintheCognitive
TheoryofMultimediaLearning[53].
22RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
7.1 DesignConsiderations
Basedonourfindings,weprovidethreedesignconsiderationsforstory-basedvocabularylearning
tools.
Providemoretypesofvisualaids. ParticipantsgenerallyfavorRetAssist’simagesforhelping
themcomprehendandrecallstories.However,threeparticipantscommentthattheystillhave
difficulty in recalling the expression of target words with the generative images and suggest
thatitwouldbebettertovisualizestoriesthroughmindmapsorflowcharts[45,56].Moreover,
participantsexpectRetAssisttoincorporateshortvideos[4]ormotiongraphics[36]intovocabulary
learning.“Understandingtheimagesthemselvesisanadditionalburdenforme.Iwouldprefera
moreexplainableformofvisualaidstohelpmeunderstandsomeabstractstorylinesinthestory
comprehensionstage”(P18).We,therefore,suggestthatthegenerativetechniquecouldofferother
formsofvisualaidssuchasanextractedmindmapandarelevantvideoclip,andallowusersto
customizethembasedontheirinterests.
Offerpromptsthatareadaptivetousers’performance. RetAssist currentlyprovidesfixed
imagepromptsandwordprompts.However,twoparticipantssuggestthattheyneedmoreper-
sonalizedandinteractiveprompts.Forexample,thesystemcan“recognizemystucknessandgive
me corresponding hints based on the progress of my current retelling.” To provide timely and
personalizedsupportduringeachroundofrepeatedretelling,itwouldrequirefutureresearchersto
labelasetofstoryretellingaudioclipsfortrainingamodeltopredictusers’difficulttimingbased
ontheirtone,speed,andpausesinthecurrentpractice.
Providesuggestionstoimprove.ThefeedbackofferedbyRetAssist includesthecorrectness
of target words’ semantic usage as well as highlighting the incorrectly used target words and
thecorrespondingsentences. Fourparticipantsexpectthatitcanalsoexplicitlytellthemhowto
improveinthenextroundofpractice.Forinstance,thesystemcan“correctmispronunciations
ofwords,listthetargetword’sgrammaticalusageandprovideadditionalexamplesentences”to
enhancethecomprehensionofthetargetvocabulary.Wesuggestthatfuturevocabularylearning
toolsshouldoffernotonlyfeedbackonwhatandwhyatargetwordismisusedbutalsosuggestions
onhowtodeepentheunderstandingofthisword,e.g.,withmoreexamplesentences.
7.2 BroaderImpacttoGenerativeAIsforEducation
OurdesignanddevelopmentofRetAssist offersafeasibleexampleofleveraginggenerativeAIsto
supportlearningtasks.First,generativemodelscanoffermeaningfulandflexiblelearningmaterials,
e.g.,ChatGPT[5]thatgeneratesastorygivenanytargetwordsinourcase.Itisalsopromising
toapplythesemodelstopreparelisteningmaterials[59]andprovidecontextuallypersonalized
learningmaterials[15]forlanguagelearners.Second,generativemodelscansupportadditional
modalities of learning activities used in traditional instruction on a large scale. In addition to
servingasvisualaidsasinourcase,text-to-imageAIcanbeintegratedinto3DDesignWorkflow
toproducereferenceimages,preventdesignfixation,andinspiredesignconsiderations[39].Also,
theycanempoweraconversationalagent,whichactslikealecturer,tosociallyconversewiththe
learnerstopracticetheirspokenlanguage[61]onanytopic.
However,utilizinggenerativecontentaslearningmaterialsmayhavethepotentialtohinder
learninggainsincertainscenarios.OneconcernistheriskofgenerativeAIsintermsofaccuracy
andreliability.Learnersneedtotakeprecautionsagainstgeneratingerrorsorfalseinformation
whenadoptinggeneratedcontentaslearningmaterial,andthegenerativecontentmaybeone-sided
andoutdatedbecauseofthelimitationsofthetrainingdataforgenerativeAIs[40].Anotherconcern
isthattheassistanceofgenerativeAIsmaydiscourageusersfromputtinginenougheffortinthe
learningprocess.Forinstance,Pengetal.’sstudysuggeststhatlearnerscouldexperiencereduced
23DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
gainsinvocabularyacquisitionwhenengagedinwritingexerciseswithgenerativeAIscompared
tothosewithoutAIassistance[55].Thiscouldbeattributedtothefactthatparticipantsinvested
lesstimeinwritingandwrotesignificantlyfewerwordsinthestory,asitindicatesapreference
fordependenceonthegenerativemodelforassistance[55].Tomitigatethesepotentialnegative
impacts,wesuggestthatthedevelopersoflearningsupportsystemsshouldexaminethequalityof
generatedcontentbeforehandandworkwithtargetedlearnersandeducatorstoidentifyproper
designprinciples(Figure1).
AlthoughRetAssist isinitiallydesignedforindependentlearningoutsideoftheclassroom,it
canbeusefulindiverseeducationalsettingsbeyondindividualstudy.Forexample,teachersin
traditionalclassroomscanuseRetAssist toenrichvocabularyinstructionaroundstoryreading
or story retelling. In addition to assisting ESL learners in vocabulary acquisition using story
retelling,thesystem’sstorytext-to-imagegenerationworkflowisexpectedtobeusefulingeneral
educationscenariosthatcombineimageswithstories.Forexample,ourworkflowcangenerate
sentence-levelillustrationsforchildren’sstorybookstohelpthembetterunderstandthemeaning
oftextualdescriptions.Inaddition,forcultivatingchildren’sexpressivelanguageskills,ourstory
text-to-imagegenerationworkflowcanbeusedasaninteractiveandcreativewayforteachers
orparentstopracticeexpressivelanguageinchildren’seducation.Byretellingstorieswiththeir
illustrations,childrencandeveloptheabilitytoclearlyorganizetheirverbalexpressions,make
associationsbetweenvisualmaterialsandtextualmaterials,andcreativelyconceptualizetheplot
withtheillustrationdetails.Despitetheenormouspotentialofourproposedsystemandworkflow
ineducation,wemustapproachpotentialriskscautiouslytoensurethattheybringpositiveand
sustainableimpacts.Wemustensurethatthegenerativeimagesandstoriesconformtowidely
acceptededucationalstandardsandethicalnormstoavoidconveyingincorrectinformationor
inappropriatecontent.
7.3 LimitationsandFutureWork
Ourstudyhasseverallimitationsthaturgefuturework.First,asourprimaryfocusisonvocabulary
learningsupport,wedidnotexamineRetAssist’simpactonlearners’storyretellingskills.Learners
mayhaveoverrelianceongenerativeimagesinthestoryretellingpractices,whileintheEnglish
examsthatteststoryretellingperformance,theywouldnothavesuchassistance.Futureworkcan
extendRetAssist fortrainingstoryretellingskills.Second,weevaluateRetAssist withtwenty-four
English-as-second-languageundergraduateslearningIELTSwords,whocouldnotrepresentall
targetusergroups.Wewouldliketoextendthestudytoincludelearnersofdifferentagegroupsor
proficiencylevelsinourfuturework,andwealsoencouragefutureresearcherstocustomizeour
systemandevaluateittosupportvocabularylearnersofdifferentages,expertise,andcultures(e.g.,
middleorhigh-schoolstudentsandEnglishstudentslearningChinese). Third,weconducteda
short-termuserstudythatcanrevealRetAssist’suserexperienceandeffectivenessinourproposed
learningtasks. Toexamineitsusageinthewild,weneedalong-termfieldstudyinwhichusers
can specify any target words and take story retelling practices at any time they want. Fourth,
wedesignourcomputationalworkflowofgeneratingmultipleimagepromptsrelevanttoeach
story.Inourformativestudy,weindicatethatthestoriesareshortoneswithapproximately60
words. However,this studydesignmaynot applytoalluser groups.As thestorygetslonger,
our computational workflow will generate more sentence-level images that may decrease the
coherence among images and increase users’ cognitive workload to process them in the story
retellingpractices.Toalleviatethiscognitiveload,futureworkcouldconsiderwaystogenerate
imagesbasedonthesemanticsegmentsofthestory(i.e.,oneormultiplesentencesthatdescribe
oneimage).Fifth,futuredesigniterationsofRetAssistcouldincorporatemoreadvancedAIfeatures
likeadaptivelearningalgorithmsthattailorimageselectionorpresentationbasedonindividual
24RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
learnerperformance.Sixth,ourstudyexclusivelyutilizedgenerativeimagesasvisualaids,yet
alternativemediaformatsmightyielddifferentoutcomes.Inordertounderstandtheaffordances
ofstaticordynamicimagesforlearning,wewillconsidercomparingtheefficacyofgenerative
imageswithothermediatypes(e.g.,videosorinteractivegraphics)inourfuturework.
8 CONCLUSION
Inthispaper,basedoneducationalliteratureandworkingwithteachersaswellasESLlearners,
weiterativelydesignanddevelopaninteractivesystem,RetAssist,tofacilitatevocabularylearners
instoryretellingpractices.RetAssist equipsourproposedcomputationalworkflowthatgenerates
imagesrelevanttothestorytofosterusers’understandingandrecallofthestorythatcontainsa
setoftargetwords.Weconductawithin-subjectsstudywith24participantsincomparisontothe
baselinesystemwithoutgenerativeimages.OurresultsshowthatlearningwithRetAssist leads
tosignificantlybetterlearningoutcomesonmasteringmeaningsandexpressionsoftargetwords
thanlearningwiththebaselinesystem.Ourworkdemonstratesthefeasibilityandeffectivenessof
generativemodelstosupportlanguagelearningtasksandoffersimplicationsforfuturelearning
supporttools.
ACKNOWLEDGMENTS
This work is supported by the Young Scientists Fund of the National Natural Science Founda-
tion of China (NSFC) with Grant No.: 62202509, NSFC Grant No.: U22B2060, and the General
Projects Fund of the Natural Science Foundation of Guangdong Province in China with Grant
No.2024A1515012226.Also,thisworkissupportedinpartbyHKUST30for30withGrantNo.:
3030_003.Wearegratefultotheanonymousreviewersfortheirinsightfulsuggestions.
REFERENCES
[1] RichaAgrawalandRaviPoovaiah.2021.CoSpeak:PeerFeedbackonVoiceStoriestoInformLearningSpokenEnglish.
InCompanionPublicationofthe2021ConferenceonComputerSupportedCooperativeWorkandSocialComputing(Virtual
Event,USA)(CSCW’21).AssociationforComputingMachinery,NewYork,NY,USA,1–4. https://doi.org/10.1145/
3462204.3481750
[2] MunassirAlhamami.2016.Vocabularylearningthroughaudios,images,andvideos:Linkingtechnologieswithmemory.
Call-Ej17,2(2016),87–112.
[3] RikuArakawa,HiromuYakura,andSosukeKobayashi.2022.VocabEncounter:NMT-poweredVocabularyLearning
byPresentingComputer-GeneratedUsagesofForeignWordsintoUsers’DailyLives.InProceedingsofthe2022CHI
ConferenceonHumanFactorsinComputingSystems.1–21.
[4] BetülBal-Gezegin.2014. Aninvestigationofusingvideovs.audioforteachingvocabulary. Procedia-Socialand
BehavioralSciences143(2014),450–457.
[5] YejinBang,SamuelCahyawijaya,NayeonLee,WenliangDai,DanSu,BryanWilie,HolyLovenia,ZiweiJi,Tiezheng
Yu,WillyChung,etal.2023.Amultitask,multilingual,multimodalevaluationofchatgptonreasoning,hallucination,
andinteractivity.arXivpreprintarXiv:2302.04023(2023).
[6] FrankBoers.2014.Areappraisalofthe4/3/2activity.RELCJournal45,3(2014),221–235.
[7] ArthurCaetano,AlyssaLawson,YimengLiu,andMishaSra.2023.ARLang:Anoutdooraugmentedrealityapplication
forportuguesevocabularylearning.InProceedingsofthe2023ACMDesigningInteractiveSystemsConference.1224–1235.
[8] JialunCao,MeiziniuLi,YetingLi,MingWen,Shing-ChiCheung,andHaimingChen.2022.SemMT:asemantic-based
testingapproachformachinetranslationsystems.ACMTransactionsonSoftwareEngineeringandMethodology(TOSEM)
31,2(2022),1–36.
[9] Chih-MingChenandChing-JuChung.2008.PersonalizedmobileEnglishvocabularylearningsystembasedonitem
responsetheoryandlearningmemorycycle.Computers&Education51,2(2008),624–645.
[10] ShizheChen,BeiLiu,JianlongFu,RuihuaSong,QinJin,PingpingLin,XiaoyuQi,ChuntingWang,andJinZhou.
2019. Neuralstoryboardartist:Visualizingstorieswithcoherentimagesequences.InProceedingsofthe27thACM
InternationalConferenceonMultimedia.2236–2244.
[11] YangChen,Yu-KunLai,andYong-JinLiu.2018.Cartoongan:Generativeadversarialnetworksforphotocartoonization.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.9465–9474.
25DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
[12] PaulineCullen.2012.CambridgeVocabularyforIELTSAdvancedBand6.5+withAnswersandAudioCD.Vol.6.Cambridge
UniversityPress.
[13] PrafullaDhariwalandAlexanderNichol.2021.Diffusionmodelsbeatgansonimagesynthesis.AdvancesinNeural
InformationProcessingSystems34(2021),8780–8794.
[14] HDouglasandLEEBROWN.2001. Teachingbyprinciples:Aninteractiveapproachtolanguagepedagogy. PED
AUSTRALIA.
[15] FionaDraxler,AlbrechtSchmidt,andLewisLChuang.2023. Relevance,Effort,andPerceivedQuality:Language
Learners’ExperienceswithAI-GeneratedContextuallyPersonalizedLearningMaterial.InProceedingsofthe2023ACM
DesigningInteractiveSystemsConference.2249–2262.
[16] CarlJDunst,AndrewSimkus,andDeborahWHamby.2012. Children’sstoryretellingasaliteracyandlanguage
enhancementstrategy.CenterforEarlyLiteracyLearning5,2(2012),1–14.
[17] FerdaGülAydınEmekligilandİlkayÖksüz.2022.Gamecharactergenerationwithgenerativeadversarialnetworks.
In202230thSignalProcessingandCommunicationsApplicationsConference(SIU).IEEE,1–4.
[18] TomFawcett.2006.AnintroductiontoROCanalysis.Patternrecognitionletters27,8(2006),861–874.
[19] DiamantoFilippatouandPeterDPumfrey.1996. Pictures,titles,readingaccuracyandreadingcomprehension:a
researchreview(1973-95).EducationalResearch38,3(1996),259–291.
[20] MaayanFrid-Adar,EyalKlang,MichalAmitai,JacobGoldberger,andHayitGreenspan.2018.Syntheticdataaugmen-
tationusingGANforimprovedliverlesionclassification.In2018IEEE15thinternationalsymposiumonbiomedical
imaging(ISBI2018).IEEE,289–293.
[21] LeonAGatys,AlexanderSEcker,andMatthiasBethge.2015. Aneuralalgorithmofartisticstyle. arXivpreprint
arXiv:1508.06576(2015).
[22] MohammadRezaGhorbani.2014.StoryRetellingandtheEFLVocabularyLearningProcess.TheIranianEFLJournal
11(2014),398.
[23] AkimiGibson,JudithGold,andCharissaSgouros.2003.Thepowerofstoryretelling.Thetutor(2003),1–11.
[24] YongqiGuandRobertKeithJohnson.1996.Vocabularylearningstrategiesandlanguagelearningoutcomes.Language
learning46,4(1996),643–679.
[25] SandraGHart.2006. NASA-taskloadindex(NASA-TLX);20yearslater.InProceedingsofthehumanfactorsand
ergonomicssocietyannualmeeting,Vol.50.SagepublicationsSageCA:LosAngeles,CA,904–908.
[26] MohammadNehalHasnine,MasatoshiIshikawa,YukiHirai,HarukoMiyakoda,andKeiichiKaneko.2017. An
algorithmtoevaluateappropriatenessofstillimagesforlearningconcretenounsofanewforeignlanguage.IEICE
TRANSACTIONSonInformationandSystems100,9(2017),2156–2164.
[27] MohammadNehalHasnine,KousukeMouri,BrendanFlanagan,GokhanAkcapinar,NorikoUosaki,andHiroaki
Ogata.2018. Imagerecommendationforinformalvocabularylearninginacontext-awarelearningenvironment.
InProceedingsofthe26thInternationalConferenceonComputerinEducation.Asia-PacificSocietyforComputersin
EducationPhilippines,Asia,669–674.
[28] MohammadNehalHasnineandJunjiWu.2021.Wordhyve:Acontext-awarelanguagelearningappforvocabulary
enhancementthroughimagesandlearningcontexts.ProcediaComputerScience192(2021),3432–3439.
[29] AriHautasaari,TakeoHamada,KuntaroIshiyama,andShogoFukushima.2020.VocaBura:AMethodforSupporting
SecondLanguageVocabularyLearningWhileWalking.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.3,4,
Article135(sep2020),23pages. https://doi.org/10.1145/3369824
[30] ShinichiIzumi.2002. Output,inputenhancement,andthenoticinghypothesis:AnexperimentalstudyonESL
relativization.Studiesinsecondlanguageacquisition24,4(2002),541–577.
[31] SlavaKalyuga,PaulChandler,andJohnSweller.1999. Managingsplit-attentionandredundancyinmultimedia
instruction. AppliedCognitivePsychology:TheOfficialJournaloftheSocietyforAppliedResearchinMemoryand
Cognition13,4(1999),351–371.
[32] DiederikKingma,TimSalimans,BenPoole,andJonathanHo.2021.Variationaldiffusionmodels.Advancesinneural
informationprocessingsystems34(2021),21696–21707.
[33] WalterKintschandEileenKintsch.2005. Comprehension. InChildren’sreadingcomprehensionandassessment.
Routledge,89–110.
[34] JingYuKoh,JasonBaldridge,HonglakLee,andYinfeiYang.2021.Text-to-imagegenerationgroundedbyfine-grained
userattention.InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision.237–246.
[35] RaziyeKütük.2007. TheEffectofMnemonicVocabularyLearningStrategyandStoryTellingonYoungLearners’
VocabularyLearningandRetention.UnpublishedMAThesis)(1-101)(2007).
[36] VahidNorouziLarsariandRadkaWildová.2020.Thepsychologicaleffectofmotioninfographicsonreadingabilityof
primaryschoolstudents.(2020).
[37] YitongLi,ZheGan,YelongShen,JingjingLiu,YuCheng,YuexinWu,LawrenceCarin,DavidCarlson,andJianfeng
Gao.2019.Storygan:Asequentialconditionalganforstoryvisualization.InProceedingsoftheIEEE/CVFConferenceon
26RetAssist DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark
ComputerVisionandPatternRecognition.6329–6338.
[38] VivianLiuandLydiaBChilton.2022.Designguidelinesforpromptengineeringtext-to-imagegenerativemodels.In
Proceedingsofthe2022CHIConferenceonHumanFactorsinComputingSystems.1–23.
[39] VivianLiu,JoVermeulen,GeorgeFitzmaurice,andJustinMatejka.2023.3DALL-E:Integratingtext-to-imageAIin3D
designworkflows.InProceedingsofthe2023ACMdesigninginteractivesystemsconference.1955–1977.
[40] ChungKwanLo.2023.WhatIstheImpactofChatGPTonEducation?ARapidReviewoftheLiterature.Education
Sciences13,4(2023). https://doi.org/10.3390/educsci13040410
[41] RichardEMayer.2002.Multimedialearning.InPsychologyoflearningandmotivation.Vol.41.Elsevier,85–139.
[42] DonnaDiSegnaMerrittandBettyZLiles.1989.Narrativeanalysis:Clinicalapplicationsofstorygenerationandstory
retelling.JournalofSpeechandHearingDisorders54,3(1989),438–447.
[43] SaraMillerandLisaPennycuff.2008.Thepowerofstory:Usingstorytellingtoimproveliteracylearning.Journalof
Cross-DisciplinaryPerspectivesinEducation1,1(2008),36–43.
[44] LesleyMandelMorrow.1985.Retellingstories:Astrategyforimprovingyoungchildren’scomprehension,conceptof
storystructure,andorallanguagecomplexity.TheElementarySchoolJournal85,5(1985),647–661.
[45] ShobanaMusti,JesslynMSmith,andJohnCBegeny.2022.AVirtualTutoringProgramtoIncreaseStudents’Text
ReadingFluency.InterventioninSchoolandClinic(2022),10534512221140474.
[46] PaulNation.2007.Thefourstrands.InternationalJournalofInnovationinLanguageLearningandTeaching1,1(2007),
2–13.
[47] Chi-DucNguyenandFrankBoers.2019.TheeffectofcontentretellingonvocabularyuptakefromaTEDtalk.Tesol
Quarterly53,1(2019),5–29.
[48] AurélienNioche,Pierre-AlexandreMurena,CarlosdelaTorre-Ortiz,andAnttiOulasvirta.2021.Improvingartificial
teachersbyconsideringhowpeoplelearnandforget.In26thInternationalConferenceonIntelligentUserInterfaces.
445–453.
[49] PutuSantiOktarina,NiPutuLilaSriHari,andNiMadeWindaAmbarwati.2020.Theeffectivenessofusingpicture
booktomotivatestudentsespeciallyyounglearnersinreading.YavanaBhasha:JournalofEnglishLanguageEducation
1,1(2020),72–79.
[50] RebeccaLOxfordandRobinCScarcella.1994.Secondlanguagevocabularylearningamongadults:Stateoftheartin
vocabularyinstruction.System22,2(1994),231–243.
[51] AllanPaivio.1990.Mentalrepresentations:Adualcodingapproach.Oxforduniversitypress.
[52] AllanPaivio.2014.Bilingualdualcodingtheoryandmemory.Foundationsofbilingualmemory(2014),41–62.
[53] AllanPaivioandAlainDesrochers.1980.Adual-codingapproachtobilingualmemory.CanadianJournalofPsycholo-
gy/Revuecanadiennedepsychologie34,4(1980),388.
[54] HilalPeker,MicheleRegalla,andThomasDwightCox.2018.Teachingandlearningvocabularyincontext:Examining
engagementinthreeprekindergartenFrenchclassrooms. ForeignLanguageAnnals 51(2018),472–483. https:
//api.semanticscholar.org/CorpusID:149920259
[55] ZhenhuiPeng,XingboWang,QiushiHan,JunkaiZhu,XiaojuanMa,andHuaminQu.2023. Storyfier:Exploring
VocabularyLearningSupportwithTextGenerationModels. arXiv:2308.03864[cs.HC]
[56] SasitornPraneetponkrangandMalineePhaiboonnugulkij.2014.Theuseofretellingstoriestechniqueindeveloping
Englishspeakingabilityofgrade9students.AdvancesinLanguageandLiteraryStudies5,5(2014),141–154.
[57] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,Amanda
Askell,PamelaMishkin,JackClark,etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InInternationalconferenceonmachinelearning.PMLR,8748–8763.
[58] NilsReimersandIrynaGurevych.2019. Sentence-bert:Sentenceembeddingsusingsiamesebert-networks. arXiv
preprintarXiv:1908.10084(2019).
[59] YiRen,YangjunRuan,XuTan,TaoQin,ShengZhao,ZhouZhao,andTie-YanLiu.2019.FastSpeech:Fast,Robustand
ControllableTexttoSpeech. arXiv:1905.09263[cs.CL]
[60] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer.2022.High-ResolutionImage
SynthesisWithLatentDiffusionModels.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR).10684–10695.
[61] SherryRuan,LiweiJiang,QianyaoXu,ZhiyuanLiu,GlennMDavis,EmmaBrunskill,andJamesALanday.2021.
Englishbot:Anai-poweredconversationalsystemforsecondlanguagelearning.In26thinternationalconferenceon
intelligentuserinterfaces.434–444.
[62] HerbertRubensteinandJohnBGoodenough.1965.Contextualcorrelatesofsynonymy.Commun.ACM8,10(1965),
627–633.
[63] RunwayML.2021.Stable-diffusion-v1-5.https://huggingface.co/runwayml/stable-diffusion-v1-5. AccessedonMarch
25,2023.
27DIS’24,July1–5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
[64] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli.2015. Deepunsupervisedlearning
usingnonequilibriumthermodynamics.InInternationalConferenceonMachineLearning.PMLR,2256–2265.
[65] StephenDSorden.2012.Thecognitivetheoryofmultimedialearning.Handbookofeducationaltheories1,2012(2012),
1–22.
[66] SShyamSundar,SaraswathiBellur,JeeyunOh,QianXu,andHaiyanJia.2014.Userexperienceofon-screeninteraction
techniques:Anexperimentalinvestigationofclicking,sliding,zooming,hovering,dragging,andflipping.Human–
ComputerInteraction29,2(2014),109–152.
[67] ViswanathVenkateshandHillolBala.2008.Technologyacceptancemodel3andaresearchagendaoninterventions.
Decisionsciences39,2(2008),273–315.
[68] LevSemenovichVygotskyandMichaelCole.1978. Mindinsociety:Developmentofhigherpsychologicalprocesses.
Harvarduniversitypress.
[69] ThiemoWambsganss,TobiasKueng,MatthiasSoellner,andJanMarcoLeimeister.2021. ArgueTutor:Anadaptive
dialog-basedlearningsystemforargumentationskills.InProceedingsofthe2021CHIconferenceonhumanfactorsin
computingsystems.1–13.
[70] ThiemoWambsganss,ChristinaNiklaus,MatthiasCetto,MatthiasSöllner,SiegfriedHandschuh,andJanMarco
Leimeister.2020.AL:Anadaptivelearningsupportsystemforargumentationskills.InProceedingsofthe2020CHI
ConferenceonHumanFactorsinComputingSystems.1–14.
[71] ThomasWolf,JamesRavenscroft,JulienChaumond,andMaxwellRebo.2018.Neuralcoref:Coreferenceresolutionin
spacywithneuralnetworks.
[72] ZimingWu,YulunJiang,YidingLiu,andXiaojuanMa.2020.Predictinganddiagnosinguserengagementwithmobile
uianimationviaadata-drivenapproach.InProceedingsofthe2020CHIconferenceonhumanfactorsincomputing
systems.1–13.
[73] LirenZengandLingLin.2011.AninteractivevocabularylearningsystembasedonwordfrequencylistsandEbbinghaus’
curveofforgetting.In2011WorkshoponDigitalMediaandDigitalContentManagement.IEEE,313–317.
[74] YeshuangZhu,YuntaoWang,ChunYu,ShaoyunShi,YankaiZhang,ShuangHe,PeijunZhao,XiaojuanMa,and
YuanchunShi.2017. ViVo:Video-AugmentedDictionaryforVocabularyLearning.InProceedingsofthe2017CHI
ConferenceonHumanFactorsinComputingSystems(Denver,Colorado,USA)(CHI’17).AssociationforComputing
Machinery,NewYork,NY,USA,5568–5579. https://doi.org/10.1145/3025453.3025779
ReceivedFebruary2024;revisedApril2024;acceptedMay2024
28