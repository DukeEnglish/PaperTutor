[
    {
        "title": "ALTA: Compiler-Based Analysis of Transformers",
        "authors": "Peter ShawJames CohanJacob EisensteinKenton LeeJonathan BerantKristina Toutanova",
        "links": "http://arxiv.org/abs/2410.18077v1",
        "entry_id": "http://arxiv.org/abs/2410.18077v1",
        "pdf_url": "http://arxiv.org/pdf/2410.18077v1",
        "summary": "We propose a new programming language called ALTA and a compiler that can map\nALTA programs to Transformer weights. ALTA is inspired by RASP, a language\nproposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler\nfrom RASP programs to Transformer weights. ALTA complements and extends this\nprior work, offering the ability to express loops and to compile programs to\nUniversal Transformers, among other advantages. ALTA allows us to\nconstructively show how Transformers can represent length-invariant algorithms\nfor computing parity and addition, as well as a solution to the SCAN benchmark\nof compositional generalization tasks, without requiring intermediate\nscratchpad decoding steps. We also propose tools to analyze cases where the\nexpressibility of an algorithm is established, but end-to-end training on a\ngiven training set fails to induce behavior consistent with the desired\nalgorithm. To this end, we explore training from ALTA execution traces as a\nmore fine-grained supervision signal. This enables additional experiments and\ntheoretical analyses relating the learnability of various algorithms to data\navailability and modeling decisions, such as positional encodings. We make the\nALTA framework -- language specification, symbolic interpreter, and weight\ncompiler -- available to the community to enable further applications and\ninsights.",
        "updated": "2024-10-23 17:58:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.18077v1"
    },
    {
        "title": "TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts",
        "authors": "Yuxuan XieTianhua LiWenqi ShaoKaipeng Zhang",
        "links": "http://arxiv.org/abs/2410.18071v1",
        "entry_id": "http://arxiv.org/abs/2410.18071v1",
        "pdf_url": "http://arxiv.org/pdf/2410.18071v1",
        "summary": "Recently, multimodal large language models (MLLMs) have received much\nattention for their impressive capabilities. The evaluation of MLLMs is\nbecoming critical to analyzing attributes of MLLMs and providing valuable\ninsights. However, current benchmarks overlook the problem of prompt\nsensitivity - minor prompt variations may lead to significant performance\nfluctuations. Thus, inappropriate prompts may obscure the models' capabilities,\nunderestimating the models' performance. Moreover, different models have\ndifferent preferences for different prompts, and thus, using the same prompt\nfor all models will cause evaluation bias. This paper analyzes this deficiency\nin existing benchmarks and further introduces a new evaluation framework named\nTP-Eval, which introduces a prompt customization method to reduce evaluation\nbiases and tap models' potential. TP-Eval will rewrite the original prompts to\ndifferent customized prompts for different models. In particular, we propose\nsome well-designed modules for prompt customization tailored to the scenario of\nMLLM evaluation. Extensive experiments demonstrate the effectiveness of our\napproach to uncovering models' capabilities, and TP-Eval should benefit the\ncommunity in developing more comprehensive and convincing MLLM evaluation\nbenchmarks.",
        "updated": "2024-10-23 17:54:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.18071v1"
    },
    {
        "title": "CLEAR: Character Unlearning in Textual and Visual Modalities",
        "authors": "Alexey DontsovDmitrii KorzhAlexey ZhavoronkinBoris MikheevDenis BobkovAibek AlanovOleg Y. RogovIvan OseledetsElena Tutubalina",
        "links": "http://arxiv.org/abs/2410.18057v1",
        "entry_id": "http://arxiv.org/abs/2410.18057v1",
        "pdf_url": "http://arxiv.org/pdf/2410.18057v1",
        "summary": "Machine Unlearning (MU) is critical for enhancing privacy and security in\ndeep learning models, particularly in large multimodal language models (MLLMs),\nby removing specific private or hazardous information. While MU has made\nsignificant progress in textual and visual modalities, multimodal unlearning\n(MMU) remains significantly underexplored, partially due to the absence of a\nsuitable open-source benchmark. To address this, we introduce CLEAR, a new\nbenchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious\nindividuals and 3,700 images linked with corresponding question-answer pairs,\nenabling a thorough evaluation across modalities. We assess 10 MU methods,\nadapting them for MMU, and highlight new challenges specific to multimodal\nforgetting. We also demonstrate that simple $\\ell_1$ regularization on LoRA\nweights significantly mitigates catastrophic forgetting, preserving model\nperformance on retained data. The dataset is available at\nhttps://huggingface.co/datasets/therem/CLEAR",
        "updated": "2024-10-23 17:30:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.18057v1"
    },
    {
        "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering",
        "authors": "Qingfei ZhaoRuobing WangYukuo CenDaren ZhaShicheng TanYuxiao DongJie Tang",
        "links": "http://arxiv.org/abs/2410.18050v1",
        "entry_id": "http://arxiv.org/abs/2410.18050v1",
        "pdf_url": "http://arxiv.org/pdf/2410.18050v1",
        "summary": "Long-Context Question Answering (LCQA), a challenging task, aims to reason\nover long-context documents to yield accurate answers to questions. Existing\nlong-context Large Language Models (LLMs) for LCQA often struggle with the\n\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\nissue by providing external factual evidence. However, its chunking strategy\ndisrupts the global long-context information, and its low-quality retrieval in\nlong contexts hinders LLMs from identifying effective factual details due to\nsubstantial noise. To this end, we propose LongRAG, a general,\ndual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\nRAG's understanding of complex long-context knowledge (i.e., global information\nand factual details). We design LongRAG as a plug-and-play paradigm,\nfacilitating adaptation to various domains and LLMs. Extensive experiments on\nthree multi-hop datasets demonstrate that LongRAG significantly outperforms\nlong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\nmulti-dimensional analyses, highlighting the effectiveness of the system's\ncomponents and fine-tuning strategies. Data and code are available at\nhttps://github.com/QingFei1/LongRAG.",
        "updated": "2024-10-23 17:24:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.18050v1"
    },
    {
        "title": "Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for Russian Scientific Keyphrases",
        "authors": "Anna GlazkovaDmitry MorozovTimur Garipov",
        "links": "http://arxiv.org/abs/2410.18040v1",
        "entry_id": "http://arxiv.org/abs/2410.18040v1",
        "pdf_url": "http://arxiv.org/pdf/2410.18040v1",
        "summary": "Keyphrase selection is a challenging task in natural language processing that\nhas a wide range of applications. Adapting existing supervised and unsupervised\nsolutions for the Russian language faces several limitations due to the rich\nmorphology of Russian and the limited number of training datasets available.\nRecent studies conducted on English texts show that large language models\n(LLMs) successfully address the task of generating keyphrases. LLMs allow\nachieving impressive results without task-specific fine-tuning, using text\nprompts instead. In this work, we access the performance of prompt-based\nmethods for generating keyphrases for Russian scientific abstracts. First, we\ncompare the performance of zero-shot and few-shot prompt-based methods,\nfine-tuned models, and unsupervised methods. Then we assess strategies for\nselecting keyphrase examples in a few-shot setting. We present the outcomes of\nhuman evaluation of the generated keyphrases and analyze the strengths and\nweaknesses of the models through expert assessment. Our results suggest that\nprompt-based methods can outperform common baselines even using simple text\nprompts.",
        "updated": "2024-10-23 17:07:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.18040v1"
    }
]