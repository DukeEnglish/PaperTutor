[
    {
        "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
        "authors": "Max WilcoxsonQiyang LiKevin FransSergey Levine",
        "links": "http://arxiv.org/abs/2410.18076v1",
        "entry_id": "http://arxiv.org/abs/2410.18076v1",
        "pdf_url": "http://arxiv.org/pdf/2410.18076v1",
        "summary": "Unsupervised pretraining has been transformative in many supervised domains.\nHowever, applying such ideas to reinforcement learning (RL) presents a unique\nchallenge in that fine-tuning does not involve mimicking task-specific data,\nbut rather exploring and locating the solution through iterative\nself-improvement. In this work, we study how unlabeled prior trajectory data\ncan be leveraged to learn efficient exploration strategies. While prior data\ncan be used to pretrain a set of low-level skills, or as additional off-policy\ndata for online RL, it has been unclear how to combine these ideas effectively\nfor online exploration. Our method SUPE (Skills from Unlabeled Prior data for\nExploration) demonstrates that a careful combination of these ideas compounds\ntheir benefits. Our method first extracts low-level skills using a variational\nautoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an\noptimistic reward model, transforming prior data into high-level, task-relevant\nexamples. Finally, SUPE uses these transformed examples as additional\noff-policy data for online RL to learn a high-level policy that composes\npretrained low-level skills to explore efficiently. We empirically show that\nSUPE reliably outperforms prior strategies, successfully solving a suite of\nlong-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.",
        "updated": "2024-10-23 17:58:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.18076v1"
    },
    {
        "title": "Estimating the Spectral Moments of the Kernel Integral Operator from Finite Sample Matrices",
        "authors": "Chanwoo ChunSueYeon ChungDaniel D. Lee",
        "links": "http://arxiv.org/abs/2410.17998v2",
        "entry_id": "http://arxiv.org/abs/2410.17998v2",
        "pdf_url": "http://arxiv.org/pdf/2410.17998v2",
        "summary": "Analyzing the structure of sampled features from an input data distribution\nis challenging when constrained by limited measurements in both the number of\ninputs and features. Traditional approaches often rely on the eigenvalue\nspectrum of the sample covariance matrix derived from finite measurement\nmatrices; however, these spectra are sensitive to the size of the measurement\nmatrix, leading to biased insights. In this paper, we introduce a novel\nalgorithm that provides unbiased estimates of the spectral moments of the\nkernel integral operator in the limit of infinite inputs and features from\nfinitely sampled measurement matrices. Our method, based on dynamic\nprogramming, is efficient and capable of estimating the moments of the operator\nspectrum. We demonstrate the accuracy of our estimator on radial basis function\n(RBF) kernels, highlighting its consistency with the theoretical spectra.\nFurthermore, we showcase the practical utility and robustness of our method in\nunderstanding the geometry of learned representations in neural networks.",
        "updated": "2024-10-24 17:47:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17998v2"
    },
    {
        "title": "Semi-Implicit Functional Gradient Flow",
        "authors": "Shiyue ZhangZiheng ChengCheng Zhang",
        "links": "http://arxiv.org/abs/2410.17935v1",
        "entry_id": "http://arxiv.org/abs/2410.17935v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17935v1",
        "summary": "Particle-based variational inference methods (ParVIs) use non-parametric\nvariational families represented by particles to approximate the target\ndistribution according to the kernelized Wasserstein gradient flow for the\nKullback-Leibler (KL) divergence. Recent works introduce functional gradient\nflows to substitute the kernel for better flexibility. However, the\ndeterministic updating mechanism may suffer from limited exploration and\nrequire expensive repetitive runs for new samples. In this paper, we propose\nSemi-Implicit Functional Gradient flow (SIFG), a functional gradient ParVI\nmethod that uses perturbed particles as the approximation family. The\ncorresponding functional gradient flow, which can be estimated via denoising\nscore matching, exhibits strong theoretical convergence guarantee. We also\npresent an adaptive version of our method to automatically choose the suitable\nnoise magnitude. Extensive experiments demonstrate the effectiveness and\nefficiency of the proposed framework on both simulated and real data problems.",
        "updated": "2024-10-23 15:00:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17935v1"
    },
    {
        "title": "Deep learning for model correction of dynamical systems with data scarcity",
        "authors": "Caroline TatsuokaDongbin Xiu",
        "links": "http://arxiv.org/abs/2410.17913v1",
        "entry_id": "http://arxiv.org/abs/2410.17913v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17913v1",
        "summary": "We present a deep learning framework for correcting existing dynamical system\nmodels utilizing only a scarce high-fidelity data set. In many practical\nsituations, one has a low-fidelity model that can capture the dynamics\nreasonably well but lacks high resolution, due to the inherent limitation of\nthe model and the complexity of the underlying physics. When high resolution\ndata become available, it is natural to seek model correction to improve the\nresolution of the model predictions. We focus on the case when the amount of\nhigh-fidelity data is so small that most of the existing data driven modeling\nmethods cannot be applied. In this paper, we address these challenges with a\nmodel-correction method which only requires a scarce high-fidelity data set.\nOur method first seeks a deep neural network (DNN) model to approximate the\nexisting low-fidelity model. By using the scarce high-fidelity data, the method\nthen corrects the DNN model via transfer learning (TL). After TL, an improved\nDNN model with high prediction accuracy to the underlying dynamics is obtained.\nOne distinct feature of the propose method is that it does not assume a\nspecific form of the model correction terms. Instead, it offers an inherent\ncorrection to the low-fidelity model via TL. A set of numerical examples are\npresented to demonstrate the effectiveness of the proposed method.",
        "updated": "2024-10-23 14:33:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17913v1"
    },
    {
        "title": "Reinforcement Learning under Latent Dynamics: Toward Statistical and Algorithmic Modularity",
        "authors": "Philip AmortilaDylan J. FosterNan JiangAkshay KrishnamurthyZakaria Mhammedi",
        "links": "http://arxiv.org/abs/2410.17904v1",
        "entry_id": "http://arxiv.org/abs/2410.17904v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17904v1",
        "summary": "Real-world applications of reinforcement learning often involve environments\nwhere agents operate on complex, high-dimensional observations, but the\nunderlying (''latent'') dynamics are comparatively simple. However, outside of\nrestrictive settings such as small latent spaces, the fundamental statistical\nrequirements and algorithmic principles for reinforcement learning under latent\ndynamics are poorly understood.\n  This paper addresses the question of reinforcement learning under\n$\\textit{general}$ latent dynamics from a statistical and algorithmic\nperspective. On the statistical side, our main negative result shows that most\nwell-studied settings for reinforcement learning with function approximation\nbecome intractable when composed with rich observations; we complement this\nwith a positive result, identifying latent pushforward coverability as a\ngeneral condition that enables statistical tractability. Algorithmically, we\ndevelop provably efficient observable-to-latent reductions -- that is,\nreductions that transform an arbitrary algorithm for the latent MDP into an\nalgorithm that can operate on rich observations -- in two settings: one where\nthe agent has access to hindsight observations of the latent dynamics [LADZ23],\nand one where the agent can estimate self-predictive latent models [SAGHCB20].\nTogether, our results serve as a first step toward a unified statistical and\nalgorithmic theory for reinforcement learning under latent dynamics.",
        "updated": "2024-10-23 14:22:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17904v1"
    }
]