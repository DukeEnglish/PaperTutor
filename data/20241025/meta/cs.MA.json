[
    {
        "title": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration",
        "authors": "Xin LiQizhi ChuYubin ChenYang LiuYaoqi LiuZekai YuWeize ChenChen QianChuan ShiCheng Yang",
        "links": "http://arxiv.org/abs/2410.18032v1",
        "entry_id": "http://arxiv.org/abs/2410.18032v1",
        "pdf_url": "http://arxiv.org/pdf/2410.18032v1",
        "summary": "Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.",
        "updated": "2024-10-23 17:02:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.18032v1"
    },
    {
        "title": "Scalable Offline Reinforcement Learning for Mean Field Games",
        "authors": "Axel BrunnbauerJulian LemmelZahra BabaieeSophie NeubauerRadu Grosu",
        "links": "http://arxiv.org/abs/2410.17898v1",
        "entry_id": "http://arxiv.org/abs/2410.17898v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17898v1",
        "summary": "Reinforcement learning algorithms for mean-field games offer a scalable\nframework for optimizing policies in large populations of interacting agents.\nExisting methods often depend on online interactions or access to system\ndynamics, limiting their practicality in real-world scenarios where such\ninteractions are infeasible or difficult to model. In this paper, we present\nOffline Munchausen Mirror Descent (Off-MMD), a novel mean-field RL algorithm\nthat approximates equilibrium policies in mean-field games using purely offline\ndata. By leveraging iterative mirror descent and importance sampling\ntechniques, Off-MMD estimates the mean-field distribution from static datasets\nwithout relying on simulation or environment dynamics. Additionally, we\nincorporate techniques from offline reinforcement learning to address common\nissues like Q-value overestimation, ensuring robust policy learning even with\nlimited data coverage. Our algorithm scales to complex environments and\ndemonstrates strong performance on benchmark tasks like crowd exploration or\nnavigation, highlighting its applicability to real-world multi-agent systems\nwhere online experimentation is infeasible. We empirically demonstrate the\nrobustness of Off-MMD to low-quality datasets and conduct experiments to\ninvestigate its sensitivity to hyperparameter choices.",
        "updated": "2024-10-23 14:16:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17898v1"
    },
    {
        "title": "TranSPORTmer: A Holistic Approach to Trajectory Understanding in Multi-Agent Sports",
        "authors": "Guillem CapelleraLuis FerrazAntonio RubioAntonio AgudoFrancesc Moreno-Noguer",
        "links": "http://arxiv.org/abs/2410.17785v1",
        "entry_id": "http://arxiv.org/abs/2410.17785v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17785v1",
        "summary": "Understanding trajectories in multi-agent scenarios requires addressing\nvarious tasks, including predicting future movements, imputing missing\nobservations, inferring the status of unseen agents, and classifying different\nglobal states. Traditional data-driven approaches often handle these tasks\nseparately with specialized models. We introduce TranSPORTmer, a unified\ntransformer-based framework capable of addressing all these tasks, showcasing\nits application to the intricate dynamics of multi-agent sports scenarios like\nsoccer and basketball. Using Set Attention Blocks, TranSPORTmer effectively\ncaptures temporal dynamics and social interactions in an equivariant manner.\nThe model's tasks are guided by an input mask that conceals missing or\nyet-to-be-predicted observations. Additionally, we introduce a CLS extra agent\nto classify states along soccer trajectories, including passes, possessions,\nuncontrolled states, and out-of-play intervals, contributing to an enhancement\nin modeling trajectories. Evaluations on soccer and basketball datasets show\nthat TranSPORTmer outperforms state-of-the-art task-specific models in player\nforecasting, player forecasting-imputation, ball inference, and ball\nimputation. https://youtu.be/8VtSRm8oGoE",
        "updated": "2024-10-23 11:35:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17785v1"
    },
    {
        "title": "Markov Potential Game with Final-time Reach-Avoid Objectives",
        "authors": "Sarah H. Q. LiAbraham P. Vinod",
        "links": "http://arxiv.org/abs/2410.17690v1",
        "entry_id": "http://arxiv.org/abs/2410.17690v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17690v1",
        "summary": "We formulate a Markov potential game with final-time reach-avoid objectives\nby integrating potential game theory with stochastic reach-avoid control. Our\nfocus is on multi-player trajectory planning where players maximize the same\nmulti-player reach-avoid objective: the probability of all participants\nreaching their designated target states by a specified time, while avoiding\ncollisions with one another. Existing approaches require centralized\ncomputation of actions via a global policy, which may have prohibitively\nexpensive communication costs. Instead, we focus on approximations of the\nglobal policy via local state feedback policies. First, we adapt the recursive\nsingle player reach-avoid value iteration to the multi-player framework with\nlocal policies, and show that the same recursion holds on the joint state\nspace. To find each player's optimal local policy, the multi-player reach-avoid\nvalue function is projected from the joint state to the local state using the\nother players' occupancy measures. Then, we propose an iterative best response\nscheme for the multi-player value iteration to converge to a pure Nash\nequilibrium. We demonstrate the utility of our approach in finding\ncollision-free policies for multi-player motion planning in simulation.",
        "updated": "2024-10-23 09:13:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17690v1"
    },
    {
        "title": "Bridging Swarm Intelligence and Reinforcement Learning",
        "authors": "Karthik SomaYann BouteillerHeiko HamannGiovanni Beltrame",
        "links": "http://arxiv.org/abs/2410.17517v1",
        "entry_id": "http://arxiv.org/abs/2410.17517v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17517v1",
        "summary": "Swarm intelligence (SI) explores how large groups of simple individuals\n(e.g., insects, fish, birds) collaborate to produce complex behaviors,\nexemplifying that the whole is greater than the sum of its parts. A fundamental\ntask in SI is Collective Decision-Making (CDM), where a group selects the best\noption among several alternatives, such as choosing an optimal foraging site.\nIn this work, we demonstrate a theoretical and empirical equivalence between\nCDM and single-agent reinforcement learning (RL) in multi-armed bandit\nproblems, utilizing concepts from opinion dynamics, evolutionary game theory,\nand RL. This equivalence bridges the gap between SI and RL and leads us to\nintroduce a novel abstract RL update rule called Maynard-Cross Learning.\nAdditionally, it provides a new population-based perspective on common RL\npractices like learning rate adjustment and batching. Our findings enable\ncross-disciplinary fertilization between RL and SI, allowing techniques from\none field to enhance the understanding and methodologies of the other.",
        "updated": "2024-10-23 02:49:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17517v1"
    }
]