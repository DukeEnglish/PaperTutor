[
    {
        "title": "DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes",
        "authors": "Hengwei BianLingdong KongHaozhe XieLiang PanYu QiaoZiwei Liu",
        "links": "http://arxiv.org/abs/2410.18084v1",
        "entry_id": "http://arxiv.org/abs/2410.18084v1",
        "pdf_url": "http://arxiv.org/pdf/2410.18084v1",
        "summary": "LiDAR scene generation has been developing rapidly recently. However,\nexisting methods primarily focus on generating static and single-frame scenes,\noverlooking the inherently dynamic nature of real-world driving environments.\nIn this work, we introduce DynamicCity, a novel 4D LiDAR generation framework\ncapable of generating large-scale, high-quality LiDAR scenes that capture the\ntemporal evolution of dynamic environments. DynamicCity mainly consists of two\nkey models. 1) A VAE model for learning HexPlane as the compact 4D\nrepresentation. Instead of using naive averaging operations, DynamicCity\nemploys a novel Projection Module to effectively compress 4D LiDAR features\ninto six 2D feature maps for HexPlane construction, which significantly\nenhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we\nutilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in\nparallel, which improves both network training efficiency and reconstruction\naccuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x\ntraining speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model\nfor HexPlane generation. To make HexPlane feasible for DiT generation, a Padded\nRollout Operation is proposed to reorganize all six feature planes of the\nHexPlane as a squared 2D feature map. In particular, various conditions could\nbe introduced in the diffusion or sampling process, supporting versatile 4D\ngeneration applications, such as trajectory- and command-driven generation,\ninpainting, and layout-conditioned generation. Extensive experiments on the\nCarlaSC and Waymo datasets demonstrate that DynamicCity significantly\noutperforms existing state-of-the-art 4D LiDAR generation methods across\nmultiple metrics. The code will be released to facilitate future research.",
        "updated": "2024-10-23 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.18084v1"
    },
    {
        "title": "FIPER: Generalizable Factorized Fields for Joint Image Compression and Super-Resolution",
        "authors": "Yang-Che SunCheng Yu YeoErnie ChuJun-Cheng ChenYu-Lun Liu",
        "links": "http://arxiv.org/abs/2410.18083v1",
        "entry_id": "http://arxiv.org/abs/2410.18083v1",
        "pdf_url": "http://arxiv.org/pdf/2410.18083v1",
        "summary": "In this work, we propose a unified representation for Super-Resolution (SR)\nand Image Compression, termed **Factorized Fields**, motivated by the shared\nprinciples between these two tasks. Both SISR and Image Compression require\nrecovering and preserving fine image details--whether by enhancing resolution\nor reconstructing compressed data. Unlike previous methods that mainly focus on\nnetwork architecture, our proposed approach utilizes a basis-coefficient\ndecomposition to explicitly capture multi-scale visual features and structural\ncomponents in images, addressing the core challenges of both tasks. We first\nderive our SR model, which includes a Coefficient Backbone and Basis Swin\nTransformer for generalizable Factorized Fields. Then, to further unify these\ntwo tasks, we leverage the strong information-recovery capabilities of the\ntrained SR modules as priors in the compression pipeline, improving both\ncompression efficiency and detail reconstruction. Additionally, we introduce a\nmerged-basis compression branch that consolidates shared structures, further\noptimizing the compression process. Extensive experiments show that our unified\nrepresentation delivers state-of-the-art performance, achieving an average\nrelative improvement of 204.4% in PSNR over the baseline in Super-Resolution\n(SR) and 9.35% BD-rate reduction in Image Compression compared to the previous\nSOTA.",
        "updated": "2024-10-23 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.18083v1"
    },
    {
        "title": "FreeVS: Generative View Synthesis on Free Driving Trajectory",
        "authors": "Qitai WangLue FanYuqi WangYuntao ChenZhaoxiang Zhang",
        "links": "http://arxiv.org/abs/2410.18079v1",
        "entry_id": "http://arxiv.org/abs/2410.18079v1",
        "pdf_url": "http://arxiv.org/pdf/2410.18079v1",
        "summary": "Existing reconstruction-based novel view synthesis methods for driving scenes\nfocus on synthesizing camera views along the recorded trajectory of the ego\nvehicle. Their image rendering performance will severely degrade on viewpoints\nfalling out of the recorded trajectory, where camera rays are untrained. We\npropose FreeVS, a novel fully generative approach that can synthesize camera\nviews on free new trajectories in real driving scenes. To control the\ngeneration results to be 3D consistent with the real scenes and accurate in\nviewpoint pose, we propose the pseudo-image representation of view priors to\ncontrol the generation process. Viewpoint transformation simulation is applied\non pseudo-images to simulate camera movement in each direction. Once trained,\nFreeVS can be applied to any validation sequences without reconstruction\nprocess and synthesis views on novel trajectories. Moreover, we propose two new\nchallenging benchmarks tailored to driving scenes, which are novel camera\nsynthesis and novel trajectory synthesis, emphasizing the freedom of\nviewpoints. Given that no ground truth images are available on novel\ntrajectories, we also propose to evaluate the consistency of images synthesized\non novel trajectories with 3D perception models. Experiments on the Waymo Open\nDataset show that FreeVS has a strong image synthesis performance on both the\nrecorded trajectories and novel trajectories. Project Page:\nhttps://freevs24.github.io/",
        "updated": "2024-10-23 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.18079v1"
    },
    {
        "title": "UnCLe: Unsupervised Continual Learning of Depth Completion",
        "authors": "Suchisrit GangopadhyayXien ChenMichael ChuPatrick RimHyoungseob ParkAlex Wong",
        "links": "http://arxiv.org/abs/2410.18074v1",
        "entry_id": "http://arxiv.org/abs/2410.18074v1",
        "pdf_url": "http://arxiv.org/pdf/2410.18074v1",
        "summary": "We propose UnCLe, a standardized benchmark for Unsupervised Continual\nLearning of a multimodal depth estimation task: Depth completion aims to infer\na dense depth map from a pair of synchronized RGB image and sparse depth map.\nWe benchmark depth completion models under the practical scenario of\nunsupervised learning over continuous streams of data. Existing methods are\ntypically trained on a static, or stationary, dataset. However, when adapting\nto novel non-stationary distributions, they \"catastrophically forget\"\npreviously learned information. UnCLe simulates these non-stationary\ndistributions by adapting depth completion models to sequences of datasets\ncontaining diverse scenes captured from distinct domains using different visual\nand range sensors. We adopt representative methods from continual learning\nparadigms and translate them to enable unsupervised continual learning of depth\ncompletion. We benchmark these models for indoor and outdoor and investigate\nthe degree of catastrophic forgetting through standard quantitative metrics.\nFurthermore, we introduce model inversion quality as an additional measure of\nforgetting. We find that unsupervised continual learning of depth completion is\nan open problem, and we invite researchers to leverage UnCLe as a development\nplatform.",
        "updated": "2024-10-23 17:56:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.18074v1"
    },
    {
        "title": "WorldSimBench: Towards Video Generation Models as World Simulators",
        "authors": "Yiran QinZhelun ShiJiwen YuXijun WangEnshen ZhouLijun LiZhenfei YinXihui LiuLu ShengJing ShaoLei BaiWanli OuyangRuimao Zhang",
        "links": "http://arxiv.org/abs/2410.18072v1",
        "entry_id": "http://arxiv.org/abs/2410.18072v1",
        "pdf_url": "http://arxiv.org/pdf/2410.18072v1",
        "summary": "Recent advancements in predictive models have demonstrated exceptional\ncapabilities in predicting the future state of objects and scenes. However, the\nlack of categorization based on inherent characteristics continues to hinder\nthe progress of predictive model development. Additionally, existing benchmarks\nare unable to effectively evaluate higher-capability, highly embodied\npredictive models from an embodied perspective. In this work, we classify the\nfunctionalities of predictive models into a hierarchy and take the first step\nin evaluating World Simulators by proposing a dual evaluation framework called\nWorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and\nImplicit Manipulative Evaluation, encompassing human preference assessments\nfrom the visual perspective and action-level evaluations in embodied tasks,\ncovering three representative embodied scenarios: Open-Ended Embodied\nEnvironment, Autonomous, Driving, and Robot Manipulation. In the Explicit\nPerceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment\ndataset based on fine-grained human feedback, which we use to train a Human\nPreference Evaluator that aligns with human perception and explicitly assesses\nthe visual fidelity of World Simulators. In the Implicit Manipulative\nEvaluation, we assess the video-action consistency of World Simulators by\nevaluating whether the generated situation-aware video can be accurately\ntranslated into the correct control signals in dynamic environments. Our\ncomprehensive evaluation offers key insights that can drive further innovation\nin video generation models, positioning World Simulators as a pivotal\nadvancement toward embodied artificial intelligence.",
        "updated": "2024-10-23 17:56:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.18072v1"
    }
]