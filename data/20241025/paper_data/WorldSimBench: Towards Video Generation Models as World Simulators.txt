WORLDSIMBENCH: TOWARDS VIDEO GENERATION
MODELS AS WORLD SIMULATORS
YiranQin1,2∗,ZhelunShi3∗,JiwenYu4,XijunWang2,EnshenZhou3,LijunLi2,
ZhenfeiYin2‡,XihuiLiu4,LuSheng3,JingShao2†,LeiBai2†,WanliOuyang2,RuimaoZhang1†
1TheChineseUniversityofHongKong,Shenzhen2ShanghaiArtificialIntelligenceLaboratory
3BeihangUniversity 4TheUniversityofHongKong
ProjectPage: https://iranqin.github.io/WorldSimBench.github.io
ABSTRACT
Recent advancements in predictive models have demonstrated exceptional capa-
bilitiesinpredictingthefuturestateofobjectsandscenes. However, thelackof
categorization based on inherent characteristics continues to hinder the progress
of predictive model development. Additionally, existing benchmarks are unable
toeffectivelyevaluatehigher-capability,highlyembodiedpredictivemodelsfrom
an embodiedperspective. In thiswork, weclassify the functionalities of predic-
tive models into a hierarchy and take the first step in evaluating World Simula-
tors by proposing a dual evaluation framework called WorldSimBench. World-
SimBenchincludesExplicitPerceptualEvaluationandImplicitManipulative
Evaluation, encompassing human preference assessments from the visual per-
spectiveandaction-levelevaluationsinembodiedtasks,coveringthreerepresen-
tative embodied scenarios: Open-Ended Embodied Environment, Autonomous
Driving,andRobotManipulation.IntheExplicitPerceptualEvaluation,weintro-
ducetheHF-EmbodiedDataset,avideoassessmentdatasetbasedonfine-grained
humanfeedback,whichweusetotrainaHumanPreferenceEvaluatorthataligns
withhumanperceptionandexplicitlyassessesthevisualfidelityofWorldSimu-
lators. In the Implicit Manipulative Evaluation, we assess the video-action con-
sistencyofWorldSimulatorsbyevaluatingwhetherthegeneratedsituation-aware
videocanbeaccuratelytranslatedintothecorrectcontrolsignalsindynamicenvi-
ronments. Ourcomprehensiveevaluationofferskeyinsightsthatcandrivefurther
innovationinvideogenerationmodels,positioningWorldSimulatorsasapivotal
advancementtowardembodiedartificialintelligence.
1 INTRODUCTION
Before taking action, humans make predictions based on their objectives and observations of the
current environment. These predictions manifest in various forms, e.g., textual planning, visual
imagination of future scene changes, or even subconscious planning at the action level. With the
developmentofgenerativemodels,agentsdrivenbythesemodelsareexhibitingpredictivecapabili-
tiesthatenablethemtocompleteembodiedtasksbymakinghuman-likepredictions,e.g.,high-level
planning(Driessetal.,2023;Lietal.,2024),image-basedguidance(Laietal.,2023;Blacketal.,
2023), orfuturevideopredictiontodriveactions(Duetal.,2023;2024)). Werefertothesemod-
elsasPredictiveModels. Recently,thesemodelshavebeenwidelyappliedacrossvariousdomains
spanningfromdevelopingagentstosolveinferencetaskstoleveragingpredictionsfordrivingrobots
toperformspecificactions.
Nevertheless, the rich application scenarios and diverse model designs make predictive models a
broad family. However, without categorizing them based on their inherent characteristics, the ad-
vancementofpredictivemodeldevelopmentremainslimited. Thisleadstoourfirstquestion: Can
∗Equalcontribution †Correspondingauthor ‡Projectlead
1
4202
tcO
32
]VC.sc[
1v27081.0142:viXraFigure1: OverviewofthehierarchicalcapabilitiesofthePredictiveModels. Modelsathigher
stagesdemonstratemoreadvancedcapabilities.WetaketheinitialstepinevaluatingPredictiveGen-
erativeModelsuptotheS stage,knownasWorldSimulators,byintroducingaparallelevaluation
3
framework,WorldSimBench. WorldSimBenchassessesthemodelsbothExplicitPerceptualEvalu-
ationandImplicitManipulativeEvaluation,focusingonvideogenerationandactiontransformation
acrossthreecriticalembodiedscenarios.
we establish a reasonable hierarchical system for Predictive Models based on their degree of em-
bodiment? With a well-defined categorization, we can better target the evaluation of Predictive
Modelsfromdifferentperspectivesofembodiment,ensuringthattheirstrengthsandweaknessesare
adequatelyassessed.
In the literature, existing evaluations have typically focused on task planning capabilities by as-
sessing text outputs or evaluating visual outputs from an aesthetic perspective. However, such
approaches significantly limit the evaluation of highly embodied Predictive Models, as embodied
scenariosaremoreconcernedwithphysicalproperties(e.g.,perspectiveconsistency,objectbreak-
ability),whichthesemethodsfailtoeffectivelyassess. Thisbringsustooursecondquestion: Can
we conduct a more detailed evaluation of highly embodied Predictive Models from an embodied
perspective?
Toanswerthefirstquestion,wecategorizethefunctionalitiesofPredictiveModelsintoahierarchy
fromS toS ,definedbythemodel’scapabilitiesandlevelofembodiment,accompaniedbycorre-
0 3
spondingevaluationbenchmarksasillustratedinFig.1. Modelsareclassifiedbasedonthedegree
ofembodimentintheiroutputmodalities. Fromlowertohigherstages, themodelsarecapableof
generating: text, images, videos, and actionable videos (i.e., the videos that can be translated into
actions). It is worth noting that Predictive Models at S capable of generating actionable videos
3
integrate robust 3D scene understanding and physical rule priors to provide precise guidance for
generatingexecutableactions. Thesemodelsarecloselyalignedwiththerecentlyproposedconcept
ofWorldSimulators(Yangetal.,2023).
Toanswerthesecondquestion,wereviewtherelatedbenchmarks,aslistedinTab.1.Evaluationson
modelsinS thatgeneratetextprimarilyfocusonassessingtaskplanningcapabilities,whileS and
0 1
S assessmentsonvisualoutputmeasureaestheticqualitythroughfeaturesimilarityanalyseswith
2
ground truth data. With clearly defined evaluation dimensions and extensive annotated datasets,
both types of assessments can be effectively conducted. However, evaluating World Simulators
introducescomplexitiesduetotheintricatephysicaldefinitionsinvolved.Additionally,conventional
evaluationmethodsareinadequateforassessingtheactionabliltyofthegeneratedvideos,asthereis
nodefinitegroundtruthforactionablevideostowardscompletingaspecificembodiedtask. These
factorsposesignificantchallengestotheevaluationofWorldSimulators.
We argue that an evaluation aligned with human perception could provide a more intuitive and
accurate reflection of the characteristics of the synthesized videos, including their adherence to
physical rules. Besides, the actionability can be assessed through a closed-loop manner in simu-
lationsdeployedwithaunifiedvideo-to-actionpolicynetwork. Consideringtheseaspects,wetake
theveryfirststepinevaluatingWorldSimulatorsbyproposingadualevaluationframeworkcalled
WorldSimBench. As shown in Fig. 1, WorldSimBench assesses World Simulators through two
complementary approaches: Explicit Perceptual Evaluation, which focuses on the Visual Qual-
2Table1: ComparisonsbetweenexistingPredictiveModelbenchmarks. InteractiveEnvironment
refers to the interaction with the simulation environment during the prediction phase. Task-Level
Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the
frequencyofinteractionsthatoccurthroughthegenerationofactionsforcontrolpurposes.
Benchmark InputModality OutputModality BasedMethod Stage InteractiveEnv. EvaluationStrategy
AgentBench(Liuetal.,2023b) Text Text LLM S0 Task-Level HumanJudgement
EgoPlan-Bench(Chenetal.,2023) Text&Images Text MLLM S0 N/A Multi-choice
MMWorld(Heetal.,2024) Text&Images Text MLLM S0 N/A GPTJudgement
VAB(Liuetal.,2024a) Text&Images Text MLLM S0 Task-Level HumanJudgement
LEGO(Laietal.,2023) Text&Images Image IGM S1 Task-Level FeatureSimilarity
VBench(Huangetal.,2024) Text Video VGM S2 N/A FeatureSimilarity
EvalCrafter(Liuetal.,2024b) Text&Images Video VGM S2 N/A FeatureSimilarity
HumanPreferenceEvaluator
WorldSimBench Text&Images ActionableVideo VGM S3 Action-Level
EmbodiedMetric
ity, Condition consistency, and Embodiment of the generated content, and Implicit Manipulative
Evaluation, which measures the World Simulator’s performance through the conversion of video
into control signals. We present three representative embodied scenarios: Open-Ended Embodied
Environment(OE),AutonomousDriving(AD),andRobotManipulation(RM),tothoroughlyeval-
uatethecapabilityofWorldSimulatorsingeneratingandrepresentingscenario-specificattributes.
IntheExplicitPerceptualEvaluation,wefirstdefineevaluationcriteriawhichisusedtoconstructa
comprehensivesetofpromptsspecifictoeachscenario. Thepromptlistsarethenusedbyvarious
videogenerationmodelstoproducealargenumberofvideoclips.Followingextensivehumanfeed-
backandannotation,thesevideoclipsarecompiledintotheHF-Embodieddatasetwhichconsistsof
atotalof35,701tupleswithmulti-dimensionalscoresandfine-grainedhumanfeedback. Addition-
ally,wetrainHumanPreferenceEvaluator,usingtheHF-EmbodieddatasettoassessWorldSimu-
latorsattheperceptuallevel,offeringarobustevaluationofboththeirvisualfidelityandcontextual
accuracy. For the Implicit Manipulative Evaluation, we deploy three simulation environments for
the three embodied scenarios respectively. These environments are used to collect data and train
inversedynamicorgoal-basedvideo-to-actionmodelscapableofmappingfuturevideostoactions.
Ineachoftheseembodiedscenarios,theWorldSimulatoristaskedwithgeneratingsituation-aware
videosinreal-time, basedoncurrentobservationsandprovidedtextinstructions. Thesegenerated
videosarethenconvertedintoactionsusingthepre-trainedvideo-to-actionmodels. Theeffective-
nessoftheWorldSimulatorisimplicitlyevaluatedbymeasuringtheperformanceofthetasks,using
relevantmetricstoreflectthequalityandaccuracyofthegeneratedvideo.
Insummary,themaincontributionsareasfollows:(1)WecategorizethefunctionalitiesofPredictive
Models into a hierarchy, defined by the model’s capabilities and level of embodiment, to advance
research and development in the field and take the very first step in evaluating World Simulators.
(2)We propose a dual evaluation framework called WorldSimBench, through Explicit Perceptual
EvaluationandImplicitManipulativeEvaluation,weconductedacomprehensiveevaluationofthe
World Simulator’s capabilities from an embodied perspective, focusing on both the visual and ac-
tion levels. (3)We conducted extensive testing across multiple models and performed a thorough
analysisoftheexperimentalresults. Ourfindingshighlightthestrengthsandlimitationsofcurrent
World Simulators and provide actionable insights for improving future video generation models.
(4)WedevelopedHF-EmbodiedDataset,whichincludesfine-grainedhumanfeedbackacrossthree
scenarios and 20 dimensions, with a total of 35,701entries. This dataset, containing both human
ratingsandthereasonsbehindthem, notonlyenablestheevaluationofWorldSimulatorsbutalso
providesbroaderapplications(e.g.,alignment)forfuturevideogenerationmodels.
2 RELATED WORK
Predictive Models. Predictive models are capable of generating process representations that map
thecurrentstatetofuturestatesbyincorporatingcurrentstaterepresentationsandcontroloverfuture
trends. Predictive Text Model, built on LLMs (Radford et al., 2019; Touvron et al., 2023; Chiang
etal.,2023)andMLLMs(Achiametal.,2023;Teametal.,2023;Liuetal.,2023a;Yinetal.,2023),
generate future predictions in the text modality by accepting current state representations and text
instructions. Thesemodelshavedemonstratedimpressiveperformanceinhigh-levelplanningtasks
forembodiedagents(Driessetal.,2023;Lietal.,2024;Qinetal.,2024;Chenetal.,2024;Zhang
et al., 2024b; Lu et al., 2024). Similarly, image generation models (Brooks et al., 2023; Fu et al.,
2023)asPredictiveImageModel(Laietal.,2023;Blacketal.,2023;Zhouetal.,2024)canproduce
3futuregoalimages, showcasingstrongcapabilitiesduringthedecision-makingphaseofembodied
agents. PredictiveVideoModel(Duetal.,2024;2023),basedonvideogenerationmodels(Janner
etal.,2022),havemadesomeprogressinembodiedcontrol. However,duetolimitationsindataor
models, the generated videos often lack essential physical representations and logical consistency,
restrictingtheirapplicabilitytofixedscenariosandsingletasks.
With the advancement of diffusion transformer (Peebles & Xie, 2023) and the extensive utiliza-
tionoflarge-scaleinternetvideodatasets(Bainetal.,2021;Ebertetal.,2021;Goyaletal.,2017;
Graumanetal.,2022),certainPredictiveActionableVideoModel(Yangetal.,2023)models,also
known as World Simulators, have achieved more precise representations of physical laws and 3D
environments.
Evaluation of Predictive Models. With the advancement of predictive models, research has also
expanded to evaluate the capabilities of models at different stages. Liu et al. (2023b); Chen et al.
(2023); Shi et al. (2024); Liu et al. (2024a) conducted text-level and task completion evaluations
forPredictiveTextModelattheS stage. Laietal.(2023)performedscore-basedevaluationsfrom
0
anaestheticperspectiveforPredictiveImageModelattheS stage. Huangetal.(2024);Liuetal.
1
(2024b)alsoassessedtheaestheticqualityofvideosgeneratedbyPredictiveVideoModelattheS
2
stage. WetakethefirststepinevaluatingWorldSimulatorsthroughanembodiedperspective.
3 PREDICTIVE MODEL CATEGORY DEFINITION
In this section, we concretely categorize predictive models based on the model’s capabilities and
levelofembodiment. ThedetailedcategorizationstageofFig.1isillustratedbelow,
• Stage S : At this stage, predictive models can generate corresponding predictions based on in-
0
structions and observations but are limited to textual modality. Benchmarks at this stage conduct
text-levelandtask-completionevaluationsthroughoutputtextplanning.
• Stage S : At this stage, predictive models can generate visual predictions based on instructions
1
andobservations,butwithoutincorporatingtemporalinformation.Benchmarksatthisstageconduct
aestheticevaluationforgeneratedimages.
•StageS : Atthisstage,predictivemodelscangeneratecorrespondingvideopredictionsbasedon
2
bothinstructionsandobservations.Yet,duetolimitedmodelcapabilities,theevaluationatthislevel
focusessolelyontheaestheticqualityofthegeneratedoutputs.
•StageS : Atthisstage,predictivemodelscangeneratecorrespondingvideopredictionsbasedon
3
instructionsandobservations,withthepredictedvideocontentadheringtophysicalrulesandalign-
ingwiththeexecutedactions. ThesemodelsareknownasWorldSimulators(Ha&Schmidhuber,
2018;Yangetal.,2023),andWorldSimBenchisabenchmarkspecificallydesignedtoevaluatethese
WorldSimulators.
The rapidly evolving field of World Simulators offers exciting opportunities for advancing Artifi-
cial General Intelligence, with significant potential to enhance human productivity and creativity,
especially in embodied intelligence. Therefore, conducting a comprehensive embodied evaluation
ofWorldSimulatorsiscrucial.
4 WORLDSIMBENCH CONSTRUCTION
WorldSimBench evaluates the embodied capabilities of World Simulators across two distinct lev-
els. TheExplicitPerceptualEvaluationassessesthesimulatorsbasedonhuman-perceivedqual-
ity across different embodied scenarios, while the Implicit Manipulative Evaluation implicitly
evaluates the simulators’ capabilities by converting the generated videos into control signals and
observingtheirperformanceinvariousclosed-loopembodiedtasks.
The evaluation of World Simulators encompasses three critical embodied scenarios: Open-Ended
EmbodiedEnvironment(OE),AutonomousDriving(AD),andRobotManipulation(RM).Minecraft
servesasapopulartestbedforOE,providingachallengingplatformforagentstohandlecomplex,
unstructured tasks. In the context of AD, especially in outdoor settings, ensuring the stability and
robustness of the agent’s actions is crucial, making it an essential domain for assessing a World
4Figure 2: Overview of Explicit Perceptual Evaluation. (Top) Instruction Prompt Generation.
Weusealargecollectionofvideocaptionsfromtheinternetandourpredefinedembodiedevaluation
dimensions. TheseareexpandedusingGPTandmanuallyverifiedtocreateacorrespondingTask
InstructionPromptListfordatagenerationandevaluation. (Bottom)HF-EmbodiedDatasetGen-
eration. Massiveinternet-sourcedembodiedvideoswithcaptionsareusedtotraindatageneration
models. Fine-grainedHumanFeedbackAnnotationisthenappliedtotheembodiedvideosaccord-
ingtothecorrespondingTaskInstructionPromptList,coveringmultipleembodieddimensions.
Simulator’s capability in dynamic and uncertain environments. RM, a core task in embodied in-
telligence, demands precise and adaptive control, testing the world simulator’s ability to generate
actionable predictions that align with physical interactions. Together, these scenarios provide a
comprehensive benchmark for evaluating the effectiveness of World Simulators across a range of
real-worldtasks.
4.1 EXPLICITPERCEPTUALEVALUATION
InExplicitPerceptualEvaluation,weproposeHierarchicalEvaluationDimensions,basedonwhich
we build a video assessment dataset annotated through fine-grained human feedback, named HF-
EmbodiedDataset. Thedatasetisconstructedbasedonthreekeyresources,eachcorrespondingto
aspecificembodiedscenario: acurateddatasetofMinecraftvideosfromtheinternetforOE(Baker
etal.,2022),real-worlddrivingdataforAD(Caesaretal.,2020),andreal-worldrobotmanipulation
videos annotated with text instructions for RM (Chen et al., 2024). Using HF-Embodied Dataset,
wetrainaHumanPreferenceEvaluatortoperformperceptualevaluationsofWorldSimulators.
4.1.1 HIERARCHICALEVALUATIONDIMENSION
We develop a hierarchical evaluation dimension checklist for the three embodied scenarios, as il-
lustrated in Tab. 2, which can be categorized into three main aspects: Visual Quality, Condition
Consistency,andEmbodiment. (1)VisualQualityprimarilyassessestheoverallqualityofvideo
generation, includingAesthetics, BackgroundandForegroundConsistency. (2)ConditionConsis-
tency focuses on the alignment with the input instruction. For tasks in OE that involve distinct
scenarios,weadditionallydefineScenarioAlignmenttoassessthealignmenttothespecificscenar-
iosoutlinedintheinstruction. (3)Embodimenthasdifferentdefinitionsdependingonthescenario.
Asalltasksrequiremovementalongacertaintrajectory,weuniformlydefineTrajectorytoevaluate
therationalityofobjectmovementinthevideo(e.g.,whetheraroboticarmavoidsobstaclesduring
motion). InADandRM,wedefinePerspectivitytoassesswhetherthevideoexhibitsaclearsense
ofdepth. InOEandRM,wedefineEmbodiedInteractiontoevaluatetheplausibilityofinteractions
withobjects. WealsodefineVelocityinOEtodeterminewhetherspeedvariesappropriatelyacross
differentenvironments(e.g.,slowermovementinwater). InAD,wedefineKeyElementtoevaluate
therenderingqualityandconsistencyofcrucialembodiedelements, e.g., pedestrians. Wealsoin-
troduceSafetyinADtoassesswhethertheembodiedactionscomplywithtrafficrules.Moredetails
inSup.A.
5Table 2: Hierarchical Evaluation Dimension. The dimensions are categorized into three main
aspects: VisualQualityforevaluatingtheoverallquality,ConditionConsistencyforevaluatingthe
alignment to the input instruction, and Embodiment for evaluating embodied related factors like
physicalrules.
EmbodiedScenarios VisualQuality ConditionConsistency Embodiment
Velocity(VC)
BackgroundConsistency(BC) InstructionAlignment(IA)
Open-EndedEmbodiedEnvironment(OE) Trajectory(TJ)
ForegroundConsistency(FC) ScenarioAlignment(SA)
EmbodiedInteraction(EI)
Perspectivity(PV)
Trajectory(TJ)
AutonomousDriving(AD) Aesthetics(AE) InstructionAlignment(IA)
KeyElement(KE)
Safety(SF)
Aesthetics(AE) Perspectivity(PV)
RobotManipulation(RM) BackgroundConsistency(BC) InstructionAlignment(IA) Trajectory(TJ)
ForegroundConsistency(FC) EmbodiedInteraction(EI)
4.1.2 INSTRUCTIONPROMPTGENERATION
Using the Hierarchical Evaluation Dimension and massive video captions from the key resources,
we create a foundational but comprehensive prompt list. We utilize the knowledge of LLMs, i.e.
ChatGPT,toextendthemeta-promptsacrossawiderange.Aftermanualscreeningforrelevance,di-
versity,anddatadistribution,wecompiletheTaskInstructionPromptList,whichseparatesprompts
foreachcontent-embodiedscenarioandeachevaluationdimension,asshowninFig.2.
4.1.3 HF-EMBODIEDDATASETGENERATION
DataPreparation. Weselectmultiplevideogenerationmodelsandtrainthemusingalargecorpus
ofvideosandcorrespondingcaptionsfromthekeyresources. Duetothecapabilitiesoftheopen-
sourcevideogenerationmodel,weconducttargetedtrainingforeachofthethreedistinctembodied
scenarios individually, thereby developing several data generation models for different embodied
scenarios. Thesemodelsarethenusedtoproduceasubstantialamountofinstruction-followingem-
bodiedvideos,basedonthecorrespondingcaptions,andtheinitialimageconditionwhereapplicable
(firstframeconditionedtext-to-videotogeneratesituation-awarevideos).
HumanAnnotation. Weusehumanannotationtolabelthegeneratedvideos. BasedontheHierar-
chicalEvaluationDimension,weestablishspecificannotationguidelinesandnumerousin-conttext
examplesfortheannotators. Foreachdimension,annotatorsareinstructedtoscorethevideosolely
based on its performance within that particular dimension and provide corresponding reasoning.
For instance in RM, as illustrated in Fig. 2, under the dimension of Trajectory, annotators are re-
quired to evaluate the video exclusively on the generation quality of the motion trajectory. They
areinstructednottoconsiderotherelements(e.g., therenderingqualityoftherobotarm)orother
dimensions(e.g.,consistencywithinstructions). Additionally,annotatorsareaskedtoprovidefine-
grained feedback on any deficiencies, e.g., “inconsistent trajectory”. As a result, we construct the
HF-Embodied Dataset, which consists of a total of 35,701 tuples, each comprising a video, text
instruction,multi-dimensionalscores,andthepotentialreasons. MoredetailsinSup.B.1.
4.1.4 HUMANPREFERENCEEVALUATOR
Theobjectiveistodevelopavideoscoringmodelthatassessesvideosacrossmultipledimensions
aligning with human perception. The model takes a generated video and a prompt as input and
outputs a score ranging from 1 to n (n is defined specifically for each embodied scenario). The
promptincludesboththevideogenerationinstructionsandanexplanationoftheevaluationcriteria.
Leveraging the strong video understanding capabilities of multimodal large language models, we
fine-tuneFlash-VStream(Zhangetal.,2024a),aVideoLLM,aligningitwithhumanperceptionon
HF-EmbodiedDataset. OnlyLoRA(Huetal.,2021)parametersaretrained. Thisenablesthemodel
to effectively grasp the evaluation metrics for embodied tasks and produce accurate scores, while
maintaining its video perception and reasoning ability. We prove the effectiveness and generaliz-
abilityofourHumanPreferenceEvaluatorinSec.5.2.
6Figure 3: Overviewof Implicit Manipulative Evaluation. Embodiedtasks in different scenarios
are decomposed into executable sub-tasks. The video generation model generates corresponding
predicted videos based on the current instructions and real-time observations. Using a pre-trained
IDM or a goal-based policy, the agent executes the generated sequence of actions. After a fixed
timestep,thepredictedvideoisrefreshedbysamplingagainfromthevideogenerationmodel,and
thisprocessrepeats. Finally,thesuccessratesofvariousembodiedtasksareobtainedthroughmon-
itorsinthesimulationenvironment.
4.1.5 EVALUATIONMETRICS.
Theevaluationofavideogenerationmodelisbasedonthescoresassignedbytheevaluatoracross
various dimensions. For each dimension, the video generation model generates videos guided by
several carefully selected instructions sourced from Task Instruction Prompt List that are strongly
alignedwiththespecificevaluationcriteria,e.g.,“exploreonthebeach”forEmbodiedScenarioin
OE. The final metric for each model is computed as the average score across all dimensions. The
evaluateddimensionsforeachembodiedscenarioarelistedinTab.2.
4.2 IMPLICITMANIPULATIVEEVALUATION
TheImplicitManipulativeEvaluationassessesthecapabilitiesofWorld Simulatorsacrossvarious
embodiedscenariosbytreatingthesimulatorasalow-leveldecisionmakerforsituationalcontexts.
Usingpre-trainedvideo-to-actionmodels,weimplicitlyevaluatetheperformanceoftheWorldSim-
ulatorsbyobservingtheireffectivenessinclosed-loopembodiedtasktests.
4.2.1 SIMULATIONCONSTRUCTION
TheImplicitManipulativeEvaluationisconductedusingthefollowingthreesimulationplatforms,
forspecificsettings,pleaserefertotheSupplementaryMaterial.
OE We employ MineRL as the Minecraft simulator, with the observation space limited to RGB
imagesandtheactionspaceconfinedtokeyboardandmousecontrols. WeadopttheSteve-1bench-
marks(Lifshitzetal.,2024),withtaskdescriptionse.g.,”chopatree.”
ADWeconductstandardclosed-loopevaluationsusingtheCARLA(Dosovitskiyetal.,2017)sim-
ulatorontheLangAutoBenchmark(Shaoetal.,2024). Taskdescriptionsincludeinstructionslike
”donotdeviatefromthisroute.”
RMWeemployCALVIN(Meesetal.,2022)astherobotmanipulationsimulator,usingonlyRGB
imagesfortheobservationspaceandlimitingtheactionspacetothe7-DOF(degreesoffreedom)
oftherobotarm. Taskdescriptionsincludecommandse.g.,”pullthehandletoopenthedrawer.”
4.2.2 EMBODIEDTASKEVALUATION
EvaluationPipeline. AsillustratedinFig.3,wefirstleverageexistingorcustom-trainedvideo-to-
actionmodelsasintermediariesbetweentheWorldSimulatorandtheagentperformingclosed-loop
tasks, for the selected benchmarks across three simulation environments. This approach enables
thetransformationofthepredictedfuturevideosfromtheWorldSimulatorintoexecutablecontrol
signalsinreal-time,therebyindirectlyevaluatingtheWorldSimulator’scapabilitythroughthesuc-
7Table3:TheoverallperformancecomparisonbetweenHumanPreferenceEvaluatorandGPT-
4o. HPEindicatesHumanPreferenceEvaluator. HPE@LaviemeansthatHPEistrainedonvideos
exceptthosegeneratedbyLavie. ThevalidationisconductedonvideosgeneratedbyLaiveunder
zero-shotsetting.
EmbodiedScenario GPT-4o HPE GPT-4o@OpenSora HPE@OpenSora GPT-4o@Lavie HPE@Lavie
OE@Acc(↑) 72.8 89.4 66.5 71.6 78.5 87.9
AD@PLCC(↑) 0.28 0.60 0.03 0.34 -0.04 0.49
RM@PLCC(↑) 0.07 0.43 -0.06 0.47 0.17 0.44
cessful completion of embodied tasks. The evaluation process is tailored to the specific nature of
themodelsunderconsideration,establishingdistinctprotocolsforclosed-looptaskevaluation. We
fine-tunethemodelsonsimulationdatasetstailoredtoeachtask. Thesedatasets, derivedfromthe
threeaforementionedbenchmarks,includetaskinstructionsandcorrespondingvideos,ensuringthe
modelsarewell-adaptedtothespecificembodiedscenarios. Finally,theevaluatedWorldSimulator
is integrated with the video-to-action model to jointly form an embodied agent that performs the
giventasks. Theagent’sperformanceacrossvarioustasksservesasadirectmeasureoftheWorld
Simulator’seffectiveness.
EvaluationMetrics. InOE,wetracktheMineRL(Gussetal.,2019)environmentstatetocalculate
metricse.g.,traveldistanceandearly-gameitemcollection. Traveldistanceistheagent’smaximum
horizontal displacement (X-Z plane) from the spawn point, while dig depth is its maximum verti-
cal displacement (Y axis). We record the maximum number of logs, seeds, and dirt items in the
agent’s inventory during the episode. In AD, we employ eight widely used evaluation metrics in
Carla (Dosovitskiy et al., 2017), including Route Completion (RC), Infraction Score (IS), Driving
Score(DS),VehicleCollisions(VC),PedestrianCollisions(PC),LayoutCollisions(LC),RedLight
Violations (RV), and Offroad Infractions (OI). In RM, we evaluate the video generation model in
the CALVIN (Mees et al., 2022) setting (train on A, B, C → test on D) by running 20 trials and
calculatingtheaveragesuccessrate.
5 EXPERIMENTS
5.1 EXPERIMENTALSETUP
Weevaluate8popularvideogenerationmodels,includingOpen-Sora-Plan(T2V)(Lab&etc.,2024),
Lavie (Wang et al., 2023c), ModelScope (Wang et al., 2023b), OpenSora (Zheng et al., 2024),
AnimateDiff (Guo et al., 2023), Open-Sora-Plan(TI2V) (Lab & etc., 2024), Dynamicrafter (Xing
et al., 2023), EasyAnimate (Xu et al., 2024) through both Explicit Perceptual Evaluation and Im-
plicitManipulativeEvaluation,acrossthreedistinctscenarios:Open-EndedEmbodiedEnvironment
(OE),AutonomousDriving(AD),andRobotManipulation(RM).Allmodelsfinetunedonspecific
datasets corresponding to three embodied scenarios in Explicit Perceptual Evaluation and Implicit
ManipulativeEvaluation. Detailedinformationonthedatasets,training,andtestingconfigurations
canbefoundintheSupplementaryMaterial.
For Explicit Perceptual Evaluation, we extract five instructions from the Task Instruction Prompt
Listforeachdimensionacrossthethreeembodiedscenarios,ensuringtheystronglyalignwiththe
specificevaluationcriteria,asdiscussedinSec.4.1.5. Theselectedinstructionpromptseachmodel
to generate five videos, which are then scored by the Human Preference Evaluator to obtain an
averagescoreforthemodel’sperformance. Forthescoringrange1-n,nisset2forOE,andset5
for both AD and RM. We indicate that the generation quality in OE is perceived as binary from a
humanperspective,whiletheothertwoscenariosexhibitamorediverserangeofvideoquality.
For Implicit Manipulative Evaluation, we constructed three video-to-action models for embodied
simulationenvironments,followingthedesignsofSteve-1(Lifshitzetal.,2024),Susie(Blacketal.,
2023), andLMdrive(Shaoetal.,2024). Fortheevaluatedmodels, weusedthefollowingdatasets
forfine-tuning:(1)VPT(Bakeretal.,2022)andourowncollectedvideosalongwithcorresponding
taskdescriptionsasthetrainingsetfortheOE;(2)thefullCalvin(ABC D)(Meesetal.,2022)video
datasetandcorrespondingrobotarmcontrolinstructionsasthetrainingsetforRM;and(3)thefull
Carla (Dosovitskiy et al., 2017) video dataset and corresponding autonomous driving navigation
commands as the training set for AD. Since the video-to-action model in our OE setup utilizes a
8Figure 4: Result of Explicit Perceptual Evaluation aross three embodied scenarios. Scores in
eachembodiedscenarioarenormalizedto0-1. TheabbreviationsarelistedinTab.2.
goal-basedpolicy,whichinterpretsthegoalfromtheinputvideoandgeneratesactionsbasedonthe
currentobservationsandthegoal,itallowsustoadditionallyevaluatetext-to-videomodels.
5.2 EXPERIMENTSONHUMANPREFERENCEEVALUATOR
WedemonstratethestrongcapabilitiesandgeneralizationofHumanPreferenceEvaluatorbycom-
paringitwithGPT-4o(OpenAI,2024), showcasingitsapplicabilityforExplicitPerceptualEvalu-
ation,asshowninTab.3. Weuseaccuracy(Acc)inOEtoassessthealignmentofthemodelwith
humanpreferences,giventhescoringrangeof1-2.Incontrast,weemployPearsonlinearcorrelation
coefficient(PLCC)forADandRMastheirscoresrangefrom1-5.
After fine-tuning on HF-Embodied Dataset, our evaluator consistently surpasses the performance
of GPT-4o in terms of alignment with human preferences across all scenarios. Additionally, we
conducted zero-shot experiments with two challenging models, i.e. OpenSora and Lavie. GPT-4o
exhibits a negative correlation with human preferences in evaluating OpenSora in AD under zero-
shot setting, as well as evaluating Lavie in RM under zero-shot setting. Our evaluator’s zero-shot
performanceshowsahighcorrelationwithhumanpreferences,furtherdemonstratingitsrobustgen-
eralizationcapabilities. HumanPreferenceEvaluatorissuitableforExplicitPerceptualEvaluation,
and the HF-Embodied Dataset can be leveraged to train even more aligned models for assessing
videogenerationmodelstowardsWorldSimulators. MoredetailsinSup.B.3.
5.3 DESIGNFEATURESANDDISCUSSIONS
In this section, we discuss the Design features and corresponding observations we draw from our
comprehensiveevaluationexperiments. MoredetailscanbefoundintheSupplementaryMaterial.
Human Prefrence with Feedback. Given the complexity and diversity in the representation of
physicalrulesinvideos,evenaspecificdimensionmaymanifestinvariousways(forexample,both
illogical and discontinuous object motion fall under trajectory-related issues). This makes it chal-
lengingtoevaluateusingscore-basedmodelsorasinglefixedsetofevaluationcriteria. WorldSim-
Bench addresses this challenge effectively by employing a human preference scoring mechanism
and a fine-grained feedback system. Fig. 4 illustrates the evaluation results of Explicit Perceptual
Evaluation, more detail analyze could be found in Sup. C. In OE, most models struggle with Em-
bodied Interaction, particularly in generating plausible object deformations, e.g., block shattering,
duetothecomplexityofphysicalrules. InAD,thevariationbetweenmodelsisminimal,withhigh-
performingmodelsexcellingacrossalldimensions. Thesimplerinstructions,likemovingforward
orturning,leadtohighInstructionAlignment,butmanygeneratedvideossufferfrompoor3Ddepth
(Perspectivity)andfailtodepictrealisticembodiedelementslikepedestriansandvehicles,affecting
the overall Aesthetic. In RM, models perform uniformly well in static scene depiction, excelling
inPerspectivityandForeground/BackgroundConsistency. However,theystrugglewithInstruction
Alignment, often generating aimless actions. Despite this, the lack of unreasonable trajectories
resultsinrelativelyhighTrajectoryscores,thoughroboticmanipulationremainsasignificantchal-
lengeforcurrentmodels.
Close-loopInteractiveEvaluation. Giventhedynamicnatureandreal-timerequirementsofinter-
activeenvironments, evaluatingWorldSimulatorsthroughstaticbenchmarksoftenfailstocapture
thefullspectrumoftheircapabilities. Close-loopInteractiveEvaluationaddressesthisbyenabling
9Figure 5: Result of Implicit Manipulative Evaluation aross three embodied scenarios.The ab-
breviationsarelistedinSec.4.2.2.
continuousfeedbackandadaptation,ensuringthatthemodel’spredictionsandactionsevolveinre-
sponsetothechangingenvironment,thusprovidingamoreaccurateandrealisticassessmentofits
performance.Fig.5presentstheImplicitManipulativeEvaluationevaluationresults,showingsignif-
icantvariationintheperformanceofvideogenerationmodelsacrossdifferenttasks.IntheOE,video
generationmodelsconditionedonthefirstframehaveasignificantlylowersuccessratecompared
tothosewithoutimageconditioning. Thissuggeststhatmodelswithimageconditioningstruggleto
generate physical laws and 3D scene representations accurately. Tasks like travel, requiring high-
qualitytrajectoriesand3Drepresentation,showthegreatestvariationinmodelperformance,while
simplertaskslikecollectingwoodseesimilarperformanceacrossmodels,indicatingeffectivehan-
dlingofminimalbackgroundvariation. IntheAD,modelswithbettertrajectory(Open-Sora-Plan)
generationperformbetter. IntheRM,wherebackgroundvariationisminimal,modelsperformsim-
ilarlyonsimpletasks,butascomplexityincreases,morerobustmodelsachievehighersuccessrates.
Despitesomesuccessacrossscenarios,videogenerationmodelsstillneedsignificantimprovements
ingeneratingphysicallyconsistentcontenttobereliablefortrainingagentsorguidingactions.
AlignmentofPhysicalRulesandActions.EnsuringthatWorldSimulatorsadheretophysicallaws
whilegeneratingpredictionsiscrucialforpracticalapplication. Thealignmentofphysicalrulesand
actionsisessentialasitguaranteesthatthemodel’soutputsarenotonlyvisuallyplausiblebutalso
executableinreal-worldscenarios. Thisapproachallowsfortheseamlessintegrationofpredicted
actions with their physical environment, ensuring reliability and effectiveness in real-world tasks.
Basedonourexperimentalfindings,weobservethatmostconclusionsfromtheExplicitPerceptual
Evaluationand Implicit Manipulative Evaluationevaluations are consistent. Specifically, the visual
qualityacrossmostdimensionsalignswiththeresultsfromtheclosed-loopexperiments. e.g.,Dy-
namicrafter, which performs well in trajectory generation in Explicit Perceptual Evaluation, also
excelsintrajectory-focusedscenarioslikeADandRM.However,inothercases—suchastheOE,
which requires more frequent interactions, and long-sequence tasks (4, 5) in RM—Dynamicrafter
underperforms compared to Open-Sora-Plan. This differs from the Explicit Perceptual Evaluation
results,likelybecausethesetasksdemandstable,high-qualityvideogenerationforguidance,where
Open-Sora-Plan shows higher robustness. Therefore, a comprehensive evaluation of video gener-
ation models requires a combination of Explicit Perceptual Evaluation and Implicit Manipulative
Evaluationassessmentstoprovidethemostfairandaccuratejudgment.Finally,basedontheoverall
ExplicitPerceptualEvaluationandImplicitManipulativeEvaluationresults,weconcludethatcurrent
video generation models still fail to effectively capture many physical rules, indicating significant
improvementsareneededbeforetheycanfunctionastrueWorldSimulators.
6 CONCLUSION
Inthiswork, weclassifythefunctionalitiesofpredictivemodelsintoahierarchyandtakethefirst
step in evaluating World Simulators by proposing a dual evaluation framework called WorldSim-
10Bench. Weconductedacomprehensiveevaluationandanalysisofmultiplevideogenerationmodels
asWorldSimulatorsthroughbothExplicitPerceptualEvaluationandImplicitManipulativeEvalua-
tionprocesses. Wesummarizekeyfindingsfromtheevaluationandhopetheseinsightswillinspire
andguidefutureresearchonWorldSimulators.
Limitations. Althoughweevaluatephysicalrulesand3Dcontentfromtheperspectiveofembodied
intelligence, the World Simulator can be applied to more scenarios than just robots, and different
scenarioshavemorephysicalrepresentations,sohowtoeffectivelyevaluatetheWorldSimulatorin
otherscenariosrequiresmoreexploration.
REFERENCES
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technical
report. arXivpreprintarXiv:2303.08774,2023.
Max Bain, Arsha Nagrani, Gu¨l Varol, and Andrew Zisserman. Frozen in time: A joint video and
imageencoderforend-to-endretrieval.InProceedingsoftheIEEE/CVFinternationalconference
oncomputervision,pp.1728–1738,2021.
Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon
Houghton,RaulSampedro,andJeffClune. Videopretraining(vpt): Learningtoactbywatching
unlabeledonlinevideos. AdvancesinNeuralInformationProcessingSystems,35:24639–24654,
2022.
KevinBlack,MitsuhikoNakamoto,PranavAtreya,HomerWalke,ChelseaFinn,AviralKumar,and
SergeyLevine. Zero-shotroboticmanipulationwithpretrainedimage-editingdiffusionmodels.
arXivpreprintarXiv:2310.10639,2023.
TimBrooks,AleksanderHolynski,andAlexeiAEfros. Instructpix2pix: Learningtofollowimage
editinginstructions.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.18392–18402,2023.
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush
Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for
autonomousdriving.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pp.11621–11631,2020.
YiChen,YuyingGe,YixiaoGe,MingyuDing,BohaoLi,RuiWang,RuifengXu,YingShan,and
XihuiLiu. Egoplan-bench: Benchmarkingegocentricembodiedplanningwithmultimodallarge
languagemodels. arXivpreprintarXiv:2312.06722,2023.
ZerenChen,ZhelunShi,XiaoyaLu,LehanHe,SuchengQian,HaoShuFang,ZhenfeiYin,Wanli
Ouyang,JingShao,YuQiao,etal.Rh20t-p:Aprimitive-levelroboticdatasettowardscomposable
generalizationagents. arXivpreprintarXiv:2403.19622,2024.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot
impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April
2023),2023.
AlexeyDosovitskiy,GermanRos,FelipeCodevilla,AntonioLopez,andVladlenKoltun. Carla: An
openurbandrivingsimulator. InConferenceonrobotlearning,pp.1–16.PMLR,2017.
Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,
AyzaanWahid,JonathanTompson,QuanVuong,TianheYu,etal. Palm-e: Anembodiedmulti-
modallanguagemodel. arXivpreprintarXiv:2303.03378,2023.
Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet,
TianheYu,PieterAbbeel,JoshuaBTenenbaum,etal. Videolanguageplanning. arXivpreprint
arXiv:2310.10625,2023.
11YilunDu,SherryYang,BoDai,HanjunDai,OfirNachum,JoshTenenbaum,DaleSchuurmans,and
PieterAbbeel. Learninguniversalpoliciesviatext-guidedvideogeneration. AdvancesinNeural
InformationProcessingSystems,36,2024.
Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas
Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic
skillswithcross-domaindatasets. arXivpreprintarXiv:2109.13396,2021.
Hao-Shu Fang, Hongjie Fang, Zhenyu Tang, Jirong Liu, Junbo Wang, Haoyi Zhu, and Cewu Lu.
Rh20t: Aroboticdatasetforlearningdiverseskillsinone-shot. InRSS2023WorkshoponLearn-
ingforTaskandMotionPlanning,2023.
Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guid-
ing instruction-based image editing via multimodal large language models. arXiv preprint
arXiv:2309.17102,2023.
Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang,
and Hongyang Li. Vista: A generalizable driving world model with high fidelity and versatile
controllability. arXivpreprintarXiv:2405.17398,2024.
Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne West-
phal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al.
The”somethingsomething”videodatabaseforlearningandevaluatingvisualcommonsense. In
ProceedingsoftheIEEEinternationalconferenceoncomputervision,pp.5842–5850,2017.
KristenGrauman,AndrewWestbury,EugeneByrne,ZacharyChavis,AntoninoFurnari,RohitGird-
har, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in
3,000hoursofegocentricvideo.InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.18995–19012,2022.
Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh
Agrawala,DahuaLin,andBoDai. Animatediff: Animateyourpersonalizedtext-to-imagediffu-
sionmodelswithoutspecifictuning. arXivpreprintarXiv:2307.04725,2023.
William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela
Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.
arXivpreprintarXiv:1907.13440,2019.
DavidHaandJu¨rgenSchmidhuber. Worldmodels. arXivpreprintarXiv:1803.10122,2018.
XuehaiHe,WeixiFeng,KaizhiZheng,YujieLu,WanrongZhu,JiachenLi,YueFan,JianfengWang,
LinjieLi,ZhengyuanYang,etal.Mmworld:Towardsmulti-disciplinemulti-facetedworldmodel
evaluationinvideos. arXivpreprintarXiv:2406.08407,2024.
EdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
andWeizhuChen. Lora: Low-rankadaptationoflargelanguagemodels,2021.
ZiqiHuang,YinanHe,JiashuoYu,FanZhang,ChenyangSi,YumingJiang,YuanhanZhang,Tianx-
ingWu,QingyangJin,NattapolChanpaisit,etal. Vbench: Comprehensivebenchmarksuitefor
videogenerativemodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pp.21807–21818,2024.
Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for
flexiblebehaviorsynthesis. arXivpreprintarXiv:2205.09991,2022.
PKU-YuanLabandTuzhanAIetc. Open-sora-plan,April2024. URLhttps://doi.org/10.
5281/zenodo.10948109.
Bolin Lai, Xiaoliang Dai, Lawrence Chen, Guan Pang, James M Rehg, and Miao Liu. Lego:
Learning egocentric action frame generation via visual instruction tuning. arXiv preprint
arXiv:2312.03849,2023.
12Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang,
JiamingLiu,andHaoDong. Manipllm: Embodiedmultimodallargelanguagemodelforobject-
centricroboticmanipulation. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.18061–18070,2024.
ShalevLifshitz,KeiranPaster,HarrisChan,JimmyBa,andSheilaMcIlraith. Steve-1: Agenerative
modelfortext-to-behaviorinminecraft. AdvancesinNeuralInformationProcessingSystems,36,
2024.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,
2023a.
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,
Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint
arXiv:2308.03688,2023b.
XiaoLiu,TianjieZhang,YuGu,IatLongIong,YifanXu,XixuanSong,ShudanZhang,HanyuLai,
Xinyi Liu, Hanlin Zhao, et al. Visualagentbench: Towards large multimodal models as visual
foundationagents. arXivpreprintarXiv:2408.06327,2024a.
Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu,
TieyongZeng,RaymondChan,andYingShan. Evalcrafter: Benchmarkingandevaluatinglarge
videogenerationmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pp.22139–22149,2024b.
Chaochao Lu, Chen Qian, Guodong Zheng, Hongxing Fan, Hongzhi Gao, Jie Zhang, Jing Shao,
Jingyi Deng, Jinlan Fu, Kexin Huang, Kunchang Li, Lijun Li, Limin Wang, Lu Sheng, Meiqi
Chen,MingZhang,QibingRen,SiruiChen,TaoGui,WanliOuyang,YaliWang,YanTeng,Yaru
Wang, Yi Wang, Yinan He, Yingchun Wang, Yixu Wang, Yongting Zhang, Yu Qiao, Yujiong
Shen,YurongMou,YuxiChen,ZaibinZhang,ZhelunShi,ZhenfeiYin,andZhipinWang. From
gpt-4togeminiandbeyond:Assessingthelandscapeofmllmsongeneralizability,trustworthiness
andcausalitythroughfourmodalities,2024.
OierMees, LukasHermann, ErickRosete-Beas, andWolframBurgard. Calvin: Abenchmarkfor
language-conditionedpolicylearningforlong-horizonrobotmanipulationtasks. IEEERobotics
andAutomationLetters,7(3):7327–7334,2022.
OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/,2024.
WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision,pp.4195–4205,2023.
YiranQin,EnshenZhou,QichangLiu,ZhenfeiYin,LuSheng,RuimaoZhang,YuQiao,andJing
Shao. Mp5: Amulti-modalopen-endedembodiedsysteminminecraftviaactiveperception. In
2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16307–
16316.IEEE,2024.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
HaoShao,YuxuanHu,LetianWang,GuangluSong,StevenLWaslander,YuLiu,andHongsheng
Li. Lmdrive: Closed-loopend-to-enddrivingwithlargelanguagemodels. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.15120–15130,2024.
Zhelun Shi, Zhipin Wang, Hongxing Fan, Zaibin Zhang, Lijun Li, Yongting Zhang, Zhenfei Yin,
LuSheng,YuQiao,andJingShao.Assessmentofmultimodallargelanguagemodelsinalignment
withhumanvalues,2024.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
13Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,
andAnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels.
arXivpreprintarXiv:2305.16291,2023a.
Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Mod-
elscopetext-to-videotechnicalreport. arXivpreprintarXiv:2308.06571,2023b.
YaohuiWang,XinyuanChen,XinMa,ShangchenZhou,ZiqiHuang,YiWang,CeyuanYang,Yinan
He, JiashuoYu, PeiqingYang, etal. Lavie: High-qualityvideogenerationwithcascadedlatent
diffusionmodels. arXivpreprintarXiv:2309.15103,2023c.
Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe,
explain, plan and select: Interactive planning with large language models enables open-world
multi-taskagents. arXivpreprintarXiv:2302.01560,2023d.
Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying
Shan.Dynamicrafter:Animatingopen-domainimageswithvideodiffusionpriors.arXivpreprint
arXiv:2310.12190,2023.
Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun
Huang. Easyanimate: A high-performance long video generation method based on transformer
architecture. arXivpreprintarXiv:2405.18991,2024.
MengjiaoYang,YilunDu,KamyarGhasemipour,JonathanTompson,DaleSchuurmans,andPieter
Abbeel. Learninginteractivereal-worldsimulators. arXivpreprintarXiv:2310.06114,2023.
Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang,
Zhiyong Wang, Lu Sheng, Lei Bai, Jing Shao, and Wanli Ouyang. LAMM: language-assisted
multi-modalinstruction-tuningdataset,framework,andbenchmark. InNeurIPS,2023.
HaojiZhang,YiqinWang,YansongTang,YongLiu,JiashiFeng,JifengDai,andXiaojieJin. Flash-
vstream: Memory-basedreal-timeunderstandingforlongvideostreams,2024a.
Zaibin Zhang, Shiyu Tang, Yuanhang Zhang, Talas Fu, Yifan Wang, Yang Liu, Dong Wang, Jing
Shao,LijunWang,andHuchuanLu. Ad-h: Autonomousdrivingwithhierarchicalagents. arXiv
preprintarXiv:2406.03474,2024b.
Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun
Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all,
March2024. URLhttps://github.com/hpcaitech/Open-Sora.
Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and
JingShao. Minedreamer:Learningtofollowinstructionsviachain-of-imaginationforsimulated-
worldcontrol. arXivpreprintarXiv:2403.12037,2024.
14Part I
Appendix
Table of Contents
A TaxonomyinExplicitPerceptualEvaluation 16
A.1 Open-EndedEmbodiedEnvironment . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 AutonomousDriving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 RobotManipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B DetaildImplementationofExplicitPerceptualEvaluation 17
B.1 HF-EmbodiedDataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.2 VideoGenerationModelFinetuning. . . . . . . . . . . . . . . . . . . . . . . . 17
B.3 HumanPreferenceEvaluatorTraing. . . . . . . . . . . . . . . . . . . . . . . . 18
C DetailedResultofExplicitPerceptualEvaluation 19
D ImplicitManipulativeEvaluation-OE 20
D.1 DetailedDescription . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
D.2 Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D.3 FullResult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D.4 RollOut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
E ImplicitManipulativeEvaluation-AD 22
E.1 DetailedDescription . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
E.2 Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
E.3 FullResult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
E.4 RollOut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
F ImplicitManipulativeEvaluation-RM 24
F.1 DetailedDescription . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
F.2 Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
F.3 FullResult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
F.4 RollOut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
15A TAXONOMY IN EXPLICIT PERCEPTUAL EVALUATION
We outline the evaluation dimensions for each embodied scenario below, along with their corre-
spondingexplanations. Theseexplanationsareusedfordetailedhumanannotationdocumentation
andalsoserveastheexplanationoftheevaluationcriteriaininstructionsfortheHumanPreference
Evaluator.
A.1 OPEN-ENDEDEMBODIEDENVIRONMENT
Visual Quality. Background Consistency ensures the background remains consistent throughout
thevideo. ForegroundConsistencyverifiestheconsistencyoftheforegroundelements.
ConditionConsistency. InstructionAlignmentassesseswhetherthevideoalignswiththeprovided
inputinstruction. ScenarioAlignmentchecksiftheinputinstructiondefinesanembodiedscenario
andwhetherthevideoaccuratelyreflectsthisscenario.
Embodiment. Velocity evaluates if the velocity of the observed object is appropriate. Embodied
Interaction evaluates the embodied interaction’s appropriateness based on the interaction process
andtarget. Trajectoryevaluateswhetherthemotiontrajectoryinthevideoislogical.
A.2 AUTONOMOUSDRIVING
Visual Quality. Aesthetics evaluates whether the composition, color, lighting, and scene in the
videoalignwithhumanaesthetics.
ConditionConsistency. InstructionAlignmentassesseswhetherthevideoalignswiththeprovided
inputinstruction.
Embodiment. Perspectivity evaluates the video’s perspective, specifically assessing the 3D scene
relationships. This includes evaluating whether the video has a strong sense of depth and realism
(i.e., whether it feels three-dimensional). Additionally, assess the logic of lighting and shadows,
includingwhether theshadow positionsare consistentwith thelight sources. Trajectory evaluates
whetherthemovementandthetrajectoryofelementsinthevideoislogical. KeyElementassesses
thegeneratedqualityofembodiedelementse.g., roads, vehicles, pedestrians, bicycles, lanemark-
ings,sidewalks,trafficsigns,andtrafficlights. Safetyevaluateswhetherthebehaviorofthevehicles
complywithtrafficrules. Arethereanyinstancesofrunningredlights,speeding,ordrivingoutside
ofpermissibleareas.
A.3 ROBOTMANIPULATION
VisualQuality.Aestheticsevaluateswhetherthecomposition,color,lighting,andsceneinthevideo
align with human aesthetics. Background Consistency ensures the background remains consistent
throughoutthevideo,includethemanipulationtableandtheenvironment. ForegroundConsistency
verifiestheconsistencyoftheforegroundelements,includingtheroboticarmandtheobjectonthe
manipulationtable.
ConditionConsistency. InstructionAlignmentassesseswhethertheactionoftherobotarminthe
generatedvideoalignswiththeprovidedinputinstruction.
Embodiment. Perspectivity evaluates the video’s perspective, specifically assessing the 3D scene
relationships. This includes evaluating whether the video has a strong sense of depth and realism
(i.e.., whether it feels three-dimensional). Additionally, assess the logic of lighting and shadows,
includingwhethertheshadowpositionsareconsistentwiththelightsources. EmbodiedInteraction
judgeswhethertheobject’sshapeandpostureconformtotherulesduringthecollisionofobjectsand
theinteractionbetweentheroboticarmandtheobject. Trajectoryevaluateswhetherthetrajectory
oftheroboticarmisreasonableandinlinewithhumancognition.
16Table 4: Analysis of HF-Embodied Dataset. Samples scored higher than 3 in AD and RM are
consideredpositive.
EmbodiedScenario #instructions #videos #dims #actions #positive #negative
Open-EndedEmbodiedEnvironment 270 8401 7 11 121249 79965
AutonomousDriving 5 15870 6 5 56768 35044
RobotManipulation 2556 11430 7 26 70672 9338
B DETAILD IMPLEMENTATION OF EXPLICIT PERCEPTUAL EVALUATION
B.1 HF-EMBODIEDDATASET
Tab.4providesananalysisoftheHF-EmbodiedDataset.InAutonomousDrivingscenario,thereare
onlyfiveinstructions: moveforward,movebackward,turnleft,turnright,andstop. Theothertwo
scenariosincludeavarietyofinstructionsthatcombineactionswithtargetobjects.Giventhediverse
instructions,differentvideogenerationmodelsgeneratemultiplevideosafterfinetuningonspecific
datasets. To enhance the Human Preference Evaluator understanding of the autonomous driving
context, we also supplement the AD scenario with videos from real-world scenes. Additionally,
welistthequantitiesofpositiveandnegativesamplesacrossalldimensions. Sampleswithhuman
annotatedscoresof3orhigherinADandRMareconsideredpositive. LeveragingHF-Embodied
Dataset with comprehensive embodied dimensions, we train the Human Preference Evaluator to
enableefficientassessmentinExplicitPerceptualEvaluation.
B.2 VIDEOGENERATIONMODELFINETUNING
Table5: TrainingFramesofGenerationModels.
Model Open-Sora-Plan Lavie ModelScope OpenSora AnimateDiff DynamicCrafter EasyAnimate
ShortVideos(frames) 16 16 16 16 16 16 16
LongVideos(frames) 64 48 60 48 64 60 64
Weevaluate8popularvideogenerationmodel,includingOpen-Sora-Plan(T2V)(Lab&etc.,2024),
Lavie(Wangetal.,2023c),ModelScope(Wangetal.,2023b),OpenSora(Zhengetal.,2024),Ani-
mateDiff(Guoetal.,2023),Open-Sora-Plan(TI2V)(Lab&etc.,2024),DynamicCrafter(Xingetal.,
2023),EasyAnimate(Xuetal.,2024)throughbothExplicitPerceptualEvaluationandImplicitMa-
nipulative Evaluation, across three distinct scenarios: Open-Ended Embodied Environment (OE),
AutonomousDriving(AD),andRobotManipulation(RM).
In Open-Ended Embodied Environment, we use OpenAI Contractor Gameplay Dataset (Baker
et al., 2022) which is created by hiring human contractors to play Minecraft and complete tasks
like house building. Keypresses and mouse movements are recorded during gameplay. We apply
the same preprocessing steps as VPT, including filtering out null actions. Additionally, we create
asupplementarydatasetforthetask”Explore”bygeneratingtrajectoriesusingvariouspre-trained
Steve-1 agents. The distribution of this dataset is enhanced by randomly switching between mod-
els during trajectories, resetting the agent’s memory, and adjusting the agent’s orientation to face
newdirectionsatrandomintervals. Forspecificin-gameevents,e.g.,“mine block”,thetypeof
blockbrokenisloggedalongsideprecisetimestamps. Thesetimestampsallowforaccurateprogress
trackingandarealignedwiththecompletionofevent-relatedinstructions.
In Autonomous Driving, we fine-tune using the nuScenes training set (Caesar et al., 2020), and
followingtheapproachinVista(Gaoetal.,2024),wesamplevideoclipsconsistingof25framesata
frequencyof10Hz.Toclassifyactionsintotextualcommands,weadheretoestablishedconventions
inplanninganddefineego-vehiclecommandsas“turnright”,“turnleft”,“gostraight”,and“stop”,
consistentwiththedefinitionsinVista.
In Robot Manipulation, we use RH20T-P (Chen et al., 2024), a dataset based on RH20T (Fang
et al., 2023) and designed for primitive-level robotic manipulation that features meticulously de-
finedprimitiveskillsanddiverseprimitive-levelspatialknowledgeofmultipleforms. Weuseeach
primitive-level robotic manipulation instruction along with the corresponding video as input for
17OE@Acc(↑) BC FC IA SA VC TJ EI Overall
GPT-4o 60.5 70.4 70.9 67.3 79.6 83.7 85.9 72.8
HPE 81.2 87.5 87.5 96.4 94.5 93.8 88.8 89.4
GPT-4o@OpenSora 60 80 80 50 0.0 100 88.8 66.5
HPE@OpenSora 70 90 60 100 100 22.2 80 71.6
GPT-4o@Lavie 50 66.7 75 88.8 87.5 100 87.5 78.5
HPE@Lavie 80 80 80 100 100 75 100 87.9
AD@PLCC(↑) AE IA PV TJ KE SF Overall
GPT-4o 0.37 0.22 0.23 0.28 0.37 0.18 0.28
HPE 0.71 0.57 0.50 0.58 0.65 0.58 0.60
GPT-4o@OpenSora 0.22 -0.39 0.32 0.15 -0.03 -0.12 0.03
HPE@OpenSora 0.37 0.55 0.34 0.06 0.28 0.41 0.34
GPT-4o@Lavie 0.17 0.13 -0.34 0.06 -0.09 -0.15 -0.04
HPE@Lavie 0.28 1.0 0.49 0.37 0.12 0.69 0.49
RM@PLCC(↑) AE BC FC IA PV TJ EI Overall
GPT-4o 0.07 0.18 0.20 0.32 -0.14 -0.01 -0.14 0.07
HPE 0.52 0.43 0.43 0.43 0.20 0.56 0.44 0.43
GPT-4o@OpenSora -0.45 -0.03 0.08 0.0 0.04 -0.23 0.14 -0.06
HPE@OpenSora 0.25 0.35 0.05 0.42 0.89 0.89 0.44 0.47
GPT-4o@Lavie 0.11 -0.07 0.42 0.42 0.21 0.31 -0.21 0.17
HPE@Lavie 0.33 0.04 0.69 0.40 0.89 0.67 0.06 0.44
Table 6: Performance comparison between Human Preference Evaluatorand GPT-4o. HPE
indicatesHumanPreferenceEvaluator. TheotherabbreviationsarelistedinTab.2.
training. Additionally, since this dataset is designed for downstream tasks in specific scenarios,
some textual instructions include explicit coordinate information. To enhance the generalization
abilityofthevideomodel,weexcludedthesecoordinate-specificinstructionsduringtraining.
Atthemodelarchitecturelevel,wefollowedDynamicrafter(Xingetal.,2023)tomodifythetext-to-
videomodelofOpen-Sora-Plan(T2V)(Lab&etc.,2024)byreplacingthefirstframeandexpanding
thechanneldimensions, enablingthemodeltotakethefirstframeasacondition. Thisresultedin
theOpen-Sora-Plan(TI2V)model. Nostructuraladjustmentsweremadetoothermodels. During
training, we preprocessed the data according to each model’s default input format and performed
fine-tuning following the official implementation without changing the training settings. We fine-
tunedeachmodelusingtwodifferentvideolengthstoenhancethediversityofthevideoevaluation
set: shortvideoswithapproximately20framesandlongvideoswitharound60frames,depending
onthemodel’sdefaulttrainingvideolength. ThespecificlengthsaredetailedintheTab.5.
B.3 HUMANPREFERENCEEVALUATORTRAING
The Human Preference Evaluator is trained based on Flash-VStream (Zhang et al., 2024a), where
onlyLoRA(Huetal.,2021)parametersaretrained. Themodel’sinputconsistsofasampledvideo,
represented as multiple frames, along with a prompt. The prompt includes the current scenario,
theinstructioninputforvideogeneration,thedimensionbeingevaluated,andthedefinitionofthat
dimension. An example of such a prompt is illustrated in Fig. 6, while the details of the explana-
tion are discussed in Section 2. We don’t use the annotated reason during training for CoT of the
evaluator,asthereasonlabeledbydifferenthumanvariesalot,hardformodeltolearn.
Wemaintainconsistenttrainingsettingsinallthreescenarios,withavideosamplingfrequencyof4.
TheLoRAsettingsalignedwiththoseinFlash-VStream. WeuseAdamWastheoptimizer,employ
cosine decay for the learning rate scheduler. We train for 4 epochs with a learning rate of 2e-5
and a warmup ratio of 0.03. The training is conducted on 4 A100 80 GPUs. To avoid over-fitting
tospecificpromptsorvideosgeneratedbyparticularmodels,wecarefullyfiltertheHF-Embodied
Datasettoensurebalanceddistributionacrossvariousgenerationmodelsandevaluationdimensions.
18Figure6:PrompttemplateforAutonomousDriving.The{item}isreplacedwithspecificcontent.
Table7: EvaluationresultsinOE.TheabbreviationsarelistedinTab.2.
Model BC FC IA SA VC TJ EI Overall
Open-Sora-Plan 1.4 1.9 1.7 1.7 2.0 1.5 1.6 1.69
Lavie 1.3 2.0 1.7 1.7 2.0 2.0 1.8 1.79
ModelScope 1.9 2.0 2.0 1.7 2.0 2.0 1.75 1.91
OpenSora 1.6 1.9 1.6 1.8 2.0 2.0 1.6 1.79
AnimateDiff 1.3 1.3 1.2 1.7 1.4 1.38 1.55 1.40
DynamicCrafter 1.9 2.0 1.5 2.0 2.0 2.0 1.45 1.84
EasyAnimate 1.4 1.8 1.5 2.0 2.0 1.22 1.45 1.62
We prove the effectiveness and generalizability of through comparison with GPT-4o arcoss the
three embodied scenarios, under both finetuned and zero-shot setting, as shown in Tab. 6. After
fine-tuning,theHumanPreferenceEvaluatorsurpassesGPT4-oinaligningwithhumanpreferences
acrossalldimensionsineveryscenario. Thisisparticularlyevidentinchallengingdimensions,e.g.,
EmbodiedInteractionandTrajectoryinRM,whereGPT4-oshowsanegativecorrelation,whilethe
HumanPreferenceEvaluatorexhibitsastrongpositivecorrelation. Theseresultsdemonstratetheits
robustperformance,makingitsuitableforExplicitPerceptualEvaluation. Inzero-shotsettings,the
HumanPreferenceEvaluatoralsooutperformsGPT4-oinnearlyalldimensions,furtherprovingour
model’sailitytounderstandvideosgeneratedbydifferentmodels.
C DETAILED RESULT OF EXPLICIT PERCEPTUAL EVALUATION
Tabs. 7-9 present the comprehensive evaluation results for 7 video generation models across three
scenarios, including the scores for each dimension and the mean scores representing the overall
performanceofthemodels. InOE,althoughourscoringisbinary,wedisplayscoresonascaleof
1-2 for consistent comparison. In addition to the conclusions mentioned in the main text, we can
observethefollowingfindings.
InOE,mostmodelsachievehighscoresinVelocity,largelyduetothelimitedoccurrencesofobject
movementinthegeneratedvideos. Generatingdynamicembodiedenvironmentswithmovingob-
jectspresentsasignificantchallengeforcurrentmodels. Additionally,theconsistencybetweenthe
generatedvideosandthescenariosspecifiedintheinstructionsishigherthanthealignmentwiththe
task-orientedinstructions. Thisindicatesthatwhilethemodelscangeneratecorrespondingscenes,
theystruggletoreasonaboutthetemporalactionsnecessaryfortaskcompletion.
In AD, the quality of the generated videos significantly declines due to the complexity of outdoor
driving scenarios. The models must understand and generate various traffic elements, e.g., roads,
backgroundbuildings,pedestrians,andvehicles,whilealsoproducingdynamiccontent,witheach
elementrequiringreasonablespeed. Thispresentssubstantialchallenges. However,top-performing
models,e.g.,OpenSora,managetoachievethehighestscoresacrossallmetrics.
In RM, the primary issue lies in Instruction Alignment. The video generation models struggle to
comprehend the input instructions and generate appropriate actions to complete the tasks, instead
movingaimlesslywithoutclearobjectives. Thislackoftargetedmovementreducespotentialerrors
related to object interaction or penetration, resulting in artificially inflated scores in Embodied In-
teractionandTrajectory. Currentvideogenerationmodelsstruggleineffectivelyaddressingrobotic
manipulationtasks.
19Table8: EvaluationresultsinAD.TheabbreviationsarelistedinTab.2.
Model AE IA PV TJ KE SF Overall
Open-Sora-Plan 1.6 5.0 1.55 1.4 1.45 3.2 2.37
Lavie 2.15 5.0 2.2 2.8 2.1 5.0 3.21
ModelScope 2.8 5.0 3.35 4.0 3.0 5.0 3.86
OpenSora 3.55 5.0 4.4 4.8 3.65 5.0 4.40
AnimateDiff 1.55 5.0 1.55 1.0 1.3 3.8 2.37
DynamicCrafter 2.6 4.0 3.4 3.8 2.65 5.0 3.57
EasyAnimate 1.5 3.4 1.4 1.4 1.3 2.6 1.93
Table9: EvaluationresultsinRM.TheabbreviationsarelistedinTab.2.
Model AE BC FC IA PV TJ EI Overall
Open-Sora-Plan 4.0 4.0 4.0 1.0 4.9 5.0 4.0 3.84
Lavie 3.8 3.9 4.0 1.8 4.95 5.0 4.1 3.94
ModelScope 3.63 4.1 4.0 1.18 4.9 5.0 4.0 3.83
OpenSora 3.85 4.0 3.95 1.3 4.75 5.0 4.1 3.85
AnimateDiff 3.8 3.9 4.0 1.0 4.95 5.0 4.1 3.82
DynamicCrafter 3.97 4.08 4.0 2.6 5.0 5.0 4.31 4.14
EasyAnimate 3.55 3.45 3.65 1.2 4.8 4.3 3.45 3.49
D IMPLICIT MANIPULATIVE EVALUATION-OE
In this section, we provide additional details about Implicit Manipulative Evaluation-Open-Ended
EmbodiedEnvironmentthatarenotcoveredinthemainpaperduetospacelimitations. Minecraft
hasemergedasapopularopen-worldenvironmentfordevelopinggeneralistembodiedagents(Lif-
shitzetal.,2024;Qinetal.,2024;Zhouetal.,2024)duetoitsdiversetasks(e.g.,survival,harvest-
ing, crafting, combat, and creative tasks), varied environments, and interactive mobs, all of which
requiregeneralizedagentcapabilities. Previousworks(Qinetal.,2024;Wangetal.,2023d;a)have
primarilyfocusedonexploringthecapabilitiesofLLMsorMLLMsasPredictiveTextModelatthe
S stage. However,nopriorresearchhasconductedclosed-loopevaluationsofWorldSimulatorsat
1
the S stage within Minecraft. To address this gap, we leverage the Steve-1 pipeline to assess the
3
performanceofVideoGenerationModelsasWorldSimulatorsinOpen-EndedEmbodiedEnviron-
ment.
D.1 DETAILEDDESCRIPTION
InImplicitManipulativeEvaluation-Open-EndedEmbodiedEnvironment,weadapttheactionspace
ofSteve-1(Lifshitzetal.,2024)todevelopapipelinefortheVideoGenerationModel,enablingit
tofunctionasalow-levelembodiedcontroller. Additionally,weemployProgrammaticEvaluation
tobenchmarkthelow-levelembodiedcontrolcapabilitiesoftheVideoGenerationModelasWorld
Simulators. These tasks are comprehensive, requiring the combination of multiple atomic actions
andsmoothscenetransitions. Eachaspectrigorouslyteststhecoherenceofthegeneratedcontent,
theconsistencywithgiveninstructions,andthemodel’sabilitytointeracteffectivelywiththeenvi-
ronment.
Testing. WeevaluatedperformanceinOEusingfivetasks: collectingwood,collectingdirt,collect-
ing seeds, exploring the area, and vertical digging. To reduce evaluation randomness, we selected
the most suitable initialization environments for each task (e.g., the agent is initialized in a forest
for the wood collection task). During testing, for each task, we randomly select one description
from various task instructions and input it into the World Simulator to generate the corresponding
video. Thevideoisthencontinuouslytranslatedintoactionsbyapre-trainedgoal-basedvideo-to-
action model, which executes until the test time expires. Each task runs for 10 trials with distinct
environmentseeds,withalimitof3,000frames(i.e.,2.5minutesofgameplay).
20Training. Duetothelowvideoqualityproducedbytheopen-sourcevideogenerationmodelbased
on the provided instructions, we applied additional fine-tuning using data from the OE simula-
tionenvironment. ForVideoGenerationModelfine-tuning, weuseOpenAIContractorGameplay
Dataset(Bakeretal.,2022)whichisthesameasOEinExplicitPerceptualEvaluation. Thetrain-
ing setting could be found in Sup. B.2. For pre-trained goal-based video-to-action model, we use
pre-trainedSteve-1(visual)modelwithoutextrafine-tuning.
Metrics. WecalculateprogrammaticevaluationmetricsbytrackingtheMineRLenvironmentstate
throughout each evaluation episode. Several metrics are measured, including travel distance and
early-gameitemcollection. Traveldistanceisdefinedastheagent’smaximumdisplacementonthe
horizontal (X-Z) plane from its initial spawn point. Dig depth is defined as the agent’s maximum
displacementonthevertical(Y)axisfromitsinitialspawnpoint. Foranearly-gameinventory,we
record the maximum count of logs, seeds, and dirt items observed in the agent’s inventory during
theepisode.
D.2 ACTIONS
We use the part of the action space of (Baker et al., 2022) which encompasses nearly all actions
availabletohumanplayers,includingkeypresses,mousemovements,andclicks.Thespecificbinary
actionsusedinoursetuparelistedinTab10.
Table10: ActionSpaceofOE.
Behavior Action
forward Wkey
back Skey
left Akey
right Dkey
jump spacekey
inventory Ekey
sneak shiftkey
sprint ctrlkey
attack leftmousebutton
D.3 FULLRESULT
Tab. 11 presents the evaluation results of several models across five specific tasks (collect wood,
collect dirt, collect seeds, travel distance, and dig depth), along with the average (AVG) score for
each model. The models are evaluated under two different conditions: Text and Text & Image.
Notably, to ensure that each task falls within a similar score range, we divided the score for the
traveldistancetaskby10tocalculatetheAVGscore.
Performance of Models Under Text Condition. Open-Sora-Plan and Lavie demonstrate strong
performance under the text-only condition, especially in the collect dirt and travel distance tasks.
Theiraveragescores(26.38and26.06,respectively)areveryclose,indicatingconsistentandrobust
performance across tasks. ModelScope shows an excellent score in the collect dirt task (52.20),
but it performs poorly in tasks like collect wood (14.00) and travel distance (240.72), resulting in
anoverallloweraveragescore(21.050)comparedtoothertext-basedmodels. OpenSorastandsout
withthehighestoverallaveragescore(27.80),excellingparticularlyincollectdirt(70.20)andtravel
distance(339.87). Thissuggeststhatitiswell-adaptedtoavarietyoftasksandexhibitsstrongtask
performance. AnimateDiff shows the weakest performance across all tasks, especially in collect
wood(7.40)andcollectseeds(3.30),indicatingchallengesinhandlingsuchtasks.
Performance of Models Under Text & Image Condition. Open-Sora-Plan shows a significant
dropinaveragescoreunderthe”Text&Image”condition,demonstratingthataddingimageinput
reducesitsperformancecomparedtothetext-onlycondition. Inparticular,itstraveldistancescore
dropsfrom342.91to195.14,suggestingthatincorporatingimagedatamightinterferewithcertain
tasks. DynamICrafter and EasyAnimate exhibit poor performance across all tasks, especially in
collectwoodandcollectseeds,wheretheybarelycompletethetasks(withscoresof0.40and0.20,
21respectively). This may indicate a lack of generalization ability in these models when combining
imageinputwithtext.Comparingthe”Text”and”Text&Image”conditions,weobservethatadding
image input does not consistently improve task performance and, in some cases, even degrades it.
We also observed that the success rates of various tasks significantly decrease when an image is
added as an additional condition. This indicates that the current video generation models need
improvementinhandlingmultipleconditionalinputs.
Table11:DetailResultofOpen-EndedEmbodiedEnvironmentinImplicitManipulativeEvaluation.
SpecificTasks
Model Condition AVG
CollectWood CollectDirt CollectSeed TravelDis. DigDepth
Open-Sora-Plan 26.38 19.90 50.20 7.30 342.91 20.20
Lavie 26.06 23.50 56.00 11.60 270.20 12.20
ModelScope Text 21.050 14.00 52.20 6.30 240.72 8.70
OpenSora 27.80 21.20 70.20 10.40 339.87 3.20
AnimateDiff 13.10 7.40 22.90 3.30 274.19 4.50
Open-Sora-Plan 10.28 11.10 12.50 2.60 195.14 5.70
DynamiCrafter Text&Image 4.06 0.40 0.30 1.30 130.04 5.30
EasyAnimate 4.84 0.20 0.70 1.70 157.12 5.90
D.4 ROLLOUT
Fig. 7 illustrates the downstream execution process in the Open-Ended Embodied Environment,
alongwiththecorrespondingtextualinstructions.
Figure7: RolloutofOpen-EndedEmbodiedEnvironmentinImplicitManipulativeEvaluation.
E IMPLICIT MANIPULATIVE EVALUATION-AD
In this section, we provide additional details about Implicit Manipulative Evaluation-Autonomous
Drivingthatarenotcoveredinthemainpaperduetospacelimitations.
E.1 DETAILEDDESCRIPTION
In Implicit Manipulative Evaluation-Autonomous Driving, we adapt the action space of LM-
Drive (Shao et al., 2024) to develop a pipeline for the Video Generation Model, enabling it to
functionasalow-levelembodiedcontroller. Additionally,weemployLangAuto(Language-guided
AutonomousDriving)CARLAbenchmark,toevaluatethelow-levelembodiedcontrolcapabilities
22of the Video Generation Model as World Simulators. These tasks are designed to be comprehen-
sive,spanningall8publiclyavailabletownsinCARLA,coveringadiverserangeofscenariose.g.,
highways,intersections,androundabouts. Additionally,theyaccountfor16differentenvironmen-
tal conditions, combining 7 distinct weather settings (Clear, Cloudy, Wet, MidRain, WetCloudy,
HardRain,SoftRain)with3daylightconditions(Night,Noon,Sunset). Eachaspectrigorouslytests
the coherence of the generated content, the consistency with given instructions, and the model’s
abilitytointeracteffectivelywiththeenvironment.
Testing. We evaluated performance in Autonomous Driving using the LangAuto-Tiny benchmark
settingwheretheroutelengthisshorterthan150meters.Wepositthatshorterdrivingdistancespro-
videamoreeffectivetestofthelow-levelcontrolcapabilitiesofWorldSimulators. Longerroutes
typicallyinvolvemoreinstructions,whicharepronetomisalignmentwiththereal-timesimulation
environment. Therefore, we opt to evaluate performance on shorter routes to minimize these dis-
crepancies. Duringtesting, werandomlyselectonedescriptionfromvarioustaskinstructionsand
input it into the World Simulator to generate the corresponding video. The video is then continu-
ouslytranslatedintoactionsbyapre-trainedgoal-basedvideo-to-actionmodel,whichexecutesun-
tilthetesttimeexpires. WeusethecorrespondingLangAuto-Tinyinstructionsandthefirst-person
view rendered by the real-time CARLA simulation environment as input to the video generation
model.Thegeneratedvideoisthencontinuouslytransformedintodownstreamcontrolsignalsusing
apre-trainedvideo-to-actionmodeluntiltheagentreachesapredefinedsuccesszoneorthetaskis
terminatedduetofactorse.g.,timeoutsorcollisions.
Training. Duetothelowvideoqualityproducedbytheopen-sourcevideogenerationmodelbased
ontheprovidedinstructions, weappliedadditionalfine-tuningusingdatafromtheADsimulation
environment. ForVideoGenerationModeltraining,weuseLMDriveTrainingDataset(Shaoetal.,
2024). We preprocessed the training data according to each model’s default input format and per-
formedfine-tuningfollowingtheofficialimplementationwithoutchangingthetrainingsettings. We
fine-tunedeachmodelusingashortvideogenerationsettingwithapproximately20frames. Forthe
video-to-action model, we use pre-trained LMdrive model. Additional fine-tuning was conducted
basedonthetestrequirements. Weprovidedthemodelwitharbitrarytextinstructionsandreplaced
thevisualinputwiththefutureframewhilekeepingallothertrainingsettingsconsistentwithLM-
Drive.
Metrics. WeconsidereightkeymetricsintroducedbytheCARLALeaderboard(Dosovitskiyetal.,
2017):RouteCompletion(RC),InfractionScore(IS),DrivingScore(DS),VehicleCollisions(VC),
Pedestrian Collisions (PC), Layout Collisions (LC), Red Light Violations (RV), and Offroad In-
fractions(OI).RouteCompletionreferstothepercentageofthetotalroutelengththattheagenthas
completed.Thismetriconlyaccountsforthedistancetraveledalongthepredeterminedroute,where
each segment corresponds to a navigation instruction. If the agent strays too far from the route, it
isconsideredtohaveviolatedtheinstruction,resultingintheepisodebeingmarkedasafailureand
terminated. TheInfractionScoretracksanyinfractionscausedbytheagent,withpenaltiesapplied
forcollisionsortrafficviolationsthroughacorrespondingdiscountfactor. TheDrivingScoreisthe
productoftheroutecompletionratioandtheinfractionscore,reflectingbothdrivingprogressand
safety,andiswidelyregardedastheprimaryrankingmetric. Theprecisedefinitionsoftheresidual
metricscanbefoundintheCARLAdocumentation(Dosovitskiyetal.,2017).
E.2 ACTIONS
ThevideogeneratedbytheWorldSimulatoriscontinuouslyfedintothevideo-to-actionmodelto
obtainthecorrespondingwaypoints.Theagentthengeneratescontrolsignalsbasedonthegenerated
waypointsandtheconversionstrategyusedinCARLA.
E.3 FULLRESULT
Tab.12presentstheevaluationresultsofseveralmodelsacrosseightmetrics. Theevaluationresults
highlight significant differences in how video generation models perform in autonomous driving
tasks. Open-Sora-Plan stands out in trajectory generation, instruction following, and environment
perception, producinghigh-qualityvideosthateffectivelysupporttaskexecution. Incontrast, Dy-
namiCrafterandEasyAnimatestrugglewithgeneratingdetailedandconsistentvideocontent, par-
23ticularlywhenhandlingcomplexordynamicscenes. Thesemodelsrequireimprovementsinvideo
generationquality,sceneunderstanding,andtaskalignmenttoenhancetheirperformance.
From a video generation perspective, several key areas for future development are identified: Im-
proved Trajectory Generation: High-quality trajectory generation is essential for accurate control
signals. Models must focus on generating more coherent and precise trajectories, especially in
dynamic environments, to ensure vehicles follow instructions and avoid collisions. Enhanced In-
struction Following: Generated videos should closely align with task instructions, particularly in
changingenvironments,enablingvehiclestoadaptquicklywhilemaintainingtaskaccuracy. Better
EnvironmentPerception: Futuremodelsneedtogeneratevideosthataccuratelyrepresentcomplex
scenes, e.g., interactions with pedestrians, other vehicles, and varied terrains. More detailed and
realistic video generation will provide stronger input for real-time decision-making in the control
system.
Insummary, advancingtrajectoryaccuracy, instructionalignment, andenvironmentrepresentation
will be crucial for improving the overall performance of these video generation models in au-
tonomousdrivingtasks.
Table12: DetailResultofAutonomousDrivinginImplicitManipulativeEvaluation.
Model DS(↑) RC(↑) IS(↑) VC(↓) PC(↓) LC(↓) RV(↓) OI(↓)
Open-Sora-Plan 31.054 38.249 0.767 2.400 0.000 4.401 1.133 3.514
DynamiCrafter 24.491 37.189 0.599 5.030 0.000 4.896 0.937 3.221
EasyAnimate 17.414 28.475 0.607 0.000 0.000 29.344 0.000 1.690
E.4 ROLLOUT
Fig.8illustratesthedownstreamexecutionprocessintheAutonomousDriving,thecorresponding
textinstructionscanbefoundinthelowerleftcornerofeachframe.
Figure8: RolloutofAutonomousDrivinginImplicitManipulativeEvaluation.
F IMPLICIT MANIPULATIVE EVALUATION-RM
Inthissection,weprovideadditionaldetailsaboutImplicitManipulativeEvaluation-RobotManip-
ulationthatarenotcoveredinthemainpaperduetospacelimitations.
24F.1 DETAILEDDESCRIPTION
We primarily conduct our experiments on the CALVIN benchmark (Mees et al., 2022), which is
specificallydesignedforlong-horizon,language-conditionedmanipulationtasks.CALVINincludes
foursimulatedenvironments(labeledA,B,C,andD)thatdifferintexturesandobjectplacements.
EachenvironmentfeaturesaFrankaEmikaPandarobotpositionednexttoadeskwithvariousma-
nipulableobjects. Theevaluationprotocoltestsmodelperformanceacross1,000uniqueinstruction
chains, each consisting of five distinct tasks. By providing an extensive dataset paired with natu-
rallanguageannotations,theCALVINbenchmarkcanprovideaclose-loopevaluationplatformfor
evaluatingWorldSimulatortotestitsgenerationandgeneralizationcapabilities.
Testing. WeevaluatedperformanceinRobotManipulationusingtheCALVINbenchmarkbench-
mark,policymodelsaretrainedondemonstrationsfromenvironmentsA,B,andC,andevaluatedin
azero-shotmannerinenvironmentD.Duringthetestingphase,weleverageWorldSimulatorsand
apre-trainedvideo-to-actionmodeltotacklenovelmanipulationtasksguidedbyuser-specifiednat-
urallanguagecommands. Givenacurrentobservation, wegeneratefuturevideopredictionsusing
theWorldSimulatorforthemanipulationtaskwithtextinstruction. Oncethevideoissampled,we
thenexecutethevideo-to-actionpolicyconditionedonforktimesteps,wherekisatestinghyperpa-
rameter. Afterktimesteps,thevideopredictionisrefreshedbysamplingfromtheWorldSimulator
again,andtheprocessisrepeated.
Training. Duetothelowvideoqualityproducedbytheopen-sourcevideogenerationmodelbased
ontheprovidedinstructions,weappliedadditionalfine-tuningusingdatafromtheRMsimulation
environment. For Video Generation Model training, we use Calvin(ABC D) datset (Mees et al.,
2022). We preprocessed the training data according to each model’s default input format and per-
formedfine-tuningfollowingtheofficialimplementationwithoutchangingthetrainingsettings. We
fine-tunedeachmodelusingashortvideogenerationsettingwithapproximately20frames. Forthe
video-to-actionmodel,weuseapre-trainedSusiepolicywithoutextrafine-tuning.
Metrics. We report the success rates and the average task length completed (out of five tasks) for
eachevaluationsequence.
F.2 ACTIONS
Forlow-levelcontrol,weutilizethesameactionspaceasCalvin(Meesetal.,2022).
F.3 FULLRESULT
BasedontheresultsshowninTab.13,Open-Sora-Plandemonstratesconsistentperformance,with
an average task length of 2.95, indicating its ability to reliably complete task sequences. While
DynamiCrafter achieves a higher success rate of 0.95 on the initial task, its performance declines
as task complexity increases, suggesting limitations in handling longer manipulation sequences.
EasyAnimate,althoughmoderatelysuccessfulincompletingearlytasks,experiencesasharpdecline
inperformanceastaskdifficultyrises,reflectedinitsloweraveragetasklengthof2.05.
Overall, the models’ ability to consistently complete multiple tasks in succession showcases their
potential in downstream applications, with Open-Sora-Plan emerging as the most capable. How-
ever, the observed decrease in success rates as task complexity increases highlights the need for
furtherimprovementsinvideo-to-actiontranslation,particularlyinaddressingthechallengesposed
bylongerandmorecomplexmanipulationsequences.
Table13: DetailResultofRobotManipulationinImplicitManipulativeEvaluation.
Taskcompletedinarow(%)↑
Method Avg. Len. ↑
1 2 3 4 5
Open-Sora-Plan 0.85 0.70 0.60 0.40 0.40 2.95
DynamiCrafter 0.95 0.75 0.55 0.25 0.25 2.75
EasyAnimate 0.90 0.60 0.35 0.10 0.10 2.05
25F.4 ROLLOUT
Fig.9illustratesthedownstreamexecutionprocessintheRobotManipulation,alongwiththecor-
respondingtextualinstructions.
Figure9: RolloutofRobotManipulationinImplicitManipulativeEvaluation.
26